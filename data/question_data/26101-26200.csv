,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $r$ is the rank of the $n$x$n$ matrix $A\iff A$ has a nonsingular $r$x$r$ submatrix [duplicate],Show that  is the rank of the x matrix  has a nonsingular x submatrix [duplicate],r n n A\iff A r r,"This question already has an answer here : Proof that determinant rank equals row/column rank (1 answer) Closed 8 years ago . Show that $r$ is the rank of the $n$x$n$ matrix $A\iff A$ has a nonsingular $r$x$r$ submatrix, but any larger square submatrix of $A$ is singular. I know that to be nonsingular, det $ \neq 0$ I can see this to be true by writing out examples but I am unsure how to approach writing a proof for it.","This question already has an answer here : Proof that determinant rank equals row/column rank (1 answer) Closed 8 years ago . Show that $r$ is the rank of the $n$x$n$ matrix $A\iff A$ has a nonsingular $r$x$r$ submatrix, but any larger square submatrix of $A$ is singular. I know that to be nonsingular, det $ \neq 0$ I can see this to be true by writing out examples but I am unsure how to approach writing a proof for it.",,"['linear-algebra', 'matrices', 'proof-writing']"
1,"Text recommendations for linear algebra (tensors, jordan forms)","Text recommendations for linear algebra (tensors, jordan forms)",,"I'm having extreme difficulty trying to understand to topic of tensor products, freespaces, and jordan forms. Are there any text books that take an elementary approach to these topics that you may recommend? I am currently using advanced linear algebra by roman, but other sources would be appreciated. Thanks!","I'm having extreme difficulty trying to understand to topic of tensor products, freespaces, and jordan forms. Are there any text books that take an elementary approach to these topics that you may recommend? I am currently using advanced linear algebra by roman, but other sources would be appreciated. Thanks!",,"['linear-algebra', 'tensor-products', 'book-recommendation', 'jordan-normal-form']"
2,Number of $4\times3$ matrices of rank 3 over a field with 3 elements.,Number of  matrices of rank 3 over a field with 3 elements.,4\times3,I am finding number of $4\times3$ matrices of rank 3 over a field with 3 elements. If i count it as number of linearly independent columns i.e $3$ then its answer is $(3^{4}-1)(3^{4}-3)(3^{4}-3^{2}).$ But when i like to obtain the same formula as number of linearly independent rows my answer does not match. Please suggest me how to find the same formula as we look at number of linearly independent rows i.e. $3.$ Column wise already solved number of matrices of rank 3? . Thanks.,I am finding number of $4\times3$ matrices of rank 3 over a field with 3 elements. If i count it as number of linearly independent columns i.e $3$ then its answer is $(3^{4}-1)(3^{4}-3)(3^{4}-3^{2}).$ But when i like to obtain the same formula as number of linearly independent rows my answer does not match. Please suggest me how to find the same formula as we look at number of linearly independent rows i.e. $3.$ Column wise already solved number of matrices of rank 3? . Thanks.,,"['linear-algebra', 'permutations', 'combinations']"
3,Trace-Preserving Matrices,Trace-Preserving Matrices,,"Given two $n\times n$ matrices $A,B\in M_n(\mathbb{C})$ such that $B>0$, and $\text{tr}(B)=1$, if $A^{\dagger}BA=B$, does this necessarily imply that $A$ is unitary? How can I prove it?","Given two $n\times n$ matrices $A,B\in M_n(\mathbb{C})$ such that $B>0$, and $\text{tr}(B)=1$, if $A^{\dagger}BA=B$, does this necessarily imply that $A$ is unitary? How can I prove it?",,"['linear-algebra', 'matrices', 'matrix-equations']"
4,Symmetric Matrix with Positive Eigenvalues,Symmetric Matrix with Positive Eigenvalues,,"Not all matrix with positive eigenvalues is positive definite, i.e. $\mathbf{x}^\mathsf{T}A\mathbf{x}>0$ for all non zero vector $\mathbf{x}$. For example consider matrix $$A = \begin{bmatrix} 1 & -3 \\ 0 & 1 \end{bmatrix}.$$ How to prove that if we add symmetry into hypothesis then the assertion is true? That is, a symmetric matrix with positive eigenvalues is positive definite.","Not all matrix with positive eigenvalues is positive definite, i.e. $\mathbf{x}^\mathsf{T}A\mathbf{x}>0$ for all non zero vector $\mathbf{x}$. For example consider matrix $$A = \begin{bmatrix} 1 & -3 \\ 0 & 1 \end{bmatrix}.$$ How to prove that if we add symmetry into hypothesis then the assertion is true? That is, a symmetric matrix with positive eigenvalues is positive definite.",,"['linear-algebra', 'matrices']"
5,When does a real matrix have a real square root?,When does a real matrix have a real square root?,,"Given a real symmetric positive-definite matrix $A$, it is easy to show that there is a real square root: the spectral theorem says that there will be a real eigenbasis, and that reduces the problem to taking the square root of the diagonalization. In fact, we don't need symmetry, assuming we have some other way of determining that our matrix is diagonalizable with real and positive eigenvalues. However, there is no need for the eigenvalues to be real and positive.  For example, by using the Taylor series for $\sqrt{1+x}$, one can show that a real $2\times 2$ matrix $A$ will have a real square root if $\operatorname{tr}(A)>0$ and $0<\det(A)<\operatorname{tr}(A)^2/2$. Are there good general necessary or sufficient (or both) conditions under which a real square matrix has a real square root?","Given a real symmetric positive-definite matrix $A$, it is easy to show that there is a real square root: the spectral theorem says that there will be a real eigenbasis, and that reduces the problem to taking the square root of the diagonalization. In fact, we don't need symmetry, assuming we have some other way of determining that our matrix is diagonalizable with real and positive eigenvalues. However, there is no need for the eigenvalues to be real and positive.  For example, by using the Taylor series for $\sqrt{1+x}$, one can show that a real $2\times 2$ matrix $A$ will have a real square root if $\operatorname{tr}(A)>0$ and $0<\det(A)<\operatorname{tr}(A)^2/2$. Are there good general necessary or sufficient (or both) conditions under which a real square matrix has a real square root?",,"['linear-algebra', 'matrices']"
6,A Scalar times the Zero Vector,A Scalar times the Zero Vector,,"I'm reading Linear Algebra Done Right by Sheldon Axler and the proof given in the book is the same as the one in the answer provided for this question . I tried to solve this before looking at the solution and the way I did it was: Theorem: $a \cdot \vec0 = \vec 0 $ for every $a \in \mathbb F$ Proof $\ $Let $a \in \mathbb F$, then \begin{align}a \cdot \vec0     &= a \cdot \langle 0_1,0_2, \ldots ,0_n\rangle \tag{Def. of a vector}\\ &= \langle a \cdot0_1,a \cdot0_2, \ldots ,a \cdot0_n \rangle \tag{Def. of Scalar Multiplication} \\ &= \langle 0_1,0_2,...,0_n \rangle \\ &= \vec 0 \end{align} Hence,  $a \cdot \vec0 = \vec 0 $ , desired result. Is there anything wrong with this proof? For example, I didn't explain why $a \cdot 0_j = 0$, do I have to do so? Also doesn't this proof provide more insight in terms of using basic definitions rather than just vector algebra?* Is there a way to proof this result besides this and the one given in the link?","I'm reading Linear Algebra Done Right by Sheldon Axler and the proof given in the book is the same as the one in the answer provided for this question . I tried to solve this before looking at the solution and the way I did it was: Theorem: $a \cdot \vec0 = \vec 0 $ for every $a \in \mathbb F$ Proof $\ $Let $a \in \mathbb F$, then \begin{align}a \cdot \vec0     &= a \cdot \langle 0_1,0_2, \ldots ,0_n\rangle \tag{Def. of a vector}\\ &= \langle a \cdot0_1,a \cdot0_2, \ldots ,a \cdot0_n \rangle \tag{Def. of Scalar Multiplication} \\ &= \langle 0_1,0_2,...,0_n \rangle \\ &= \vec 0 \end{align} Hence,  $a \cdot \vec0 = \vec 0 $ , desired result. Is there anything wrong with this proof? For example, I didn't explain why $a \cdot 0_j = 0$, do I have to do so? Also doesn't this proof provide more insight in terms of using basic definitions rather than just vector algebra?* Is there a way to proof this result besides this and the one given in the link?",,"['linear-algebra', 'proof-verification', 'vector-spaces', 'alternative-proof']"
7,questions about Gerschgorin circle theorem.,questions about Gerschgorin circle theorem.,,"Q:if $A$ is a strictly diagonally dominant matrix, prove that $$|\det A|\ge \prod_{i=1}^n(|a_{ii}|-\sum_{j\neq i}|a_{ij}|)$$ the proof is: by  Gerschgorin circle theorem, the eigenvalue of $A$ lies in the union of the following circles: $$|z-a_{ii}|\le\sum_{j\neq i}|a_{ij}|\quad i=1,\cdots,n$$ so the eigenvalue $\lambda$ satisfy $$|\lambda|\ge|a_{ii}|-\sum_{j\neq i}|a_{ij}|$$ and we get the conclusion. my question is that the last equation only holds for some $i$, for example, all the eigenvalue can lie in the same circle, so the conclusion does not hold. What's the problem?","Q:if $A$ is a strictly diagonally dominant matrix, prove that $$|\det A|\ge \prod_{i=1}^n(|a_{ii}|-\sum_{j\neq i}|a_{ij}|)$$ the proof is: by  Gerschgorin circle theorem, the eigenvalue of $A$ lies in the union of the following circles: $$|z-a_{ii}|\le\sum_{j\neq i}|a_{ij}|\quad i=1,\cdots,n$$ so the eigenvalue $\lambda$ satisfy $$|\lambda|\ge|a_{ii}|-\sum_{j\neq i}|a_{ij}|$$ and we get the conclusion. my question is that the last equation only holds for some $i$, for example, all the eigenvalue can lie in the same circle, so the conclusion does not hold. What's the problem?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
8,2-Norm of a Submatrix is $\leq$ 2-Norm of Original Matrix,2-Norm of a Submatrix is  2-Norm of Original Matrix,\leq,"Say $A$ is a submatrix of $B$. How do I prove that the $\|A\|_2 \leq \|B\|_2$? I can easily show this for $\|\cdot\|_1, \|\cdot\|_\infty, $ and $\|\cdot\|_F$ and thought maybe the solution lies in relating the inequalities of these other norms to the 2-norm, but this path hasn't proved fruitful.","Say $A$ is a submatrix of $B$. How do I prove that the $\|A\|_2 \leq \|B\|_2$? I can easily show this for $\|\cdot\|_1, \|\cdot\|_\infty, $ and $\|\cdot\|_F$ and thought maybe the solution lies in relating the inequalities of these other norms to the 2-norm, but this path hasn't proved fruitful.",,['linear-algebra']
9,Determine the smallest disc in which all the eigen values of a given matrix lie,Determine the smallest disc in which all the eigen values of a given matrix lie,,"Let , $$A=\left[\begin{matrix}1&-2&3&-2\\1&1&0&3\\-1&1&1&-1\\0&-3&1&1\end{matrix}\right]$$Which of the following is the smallest disc in $\mathbb C$  which contains all eigen values of $A$. $|z-1|\le 7$ $|z-1|\le 6$ $|z-1|\le 4$. Characteristic polynomial of $A$ is $x^4-4x^3+21x^2-48x+46$. From this computing eigen values is very difficult in hand. Without computing eigen values how we can detect the required interval ? Does there any other process??","Let , $$A=\left[\begin{matrix}1&-2&3&-2\\1&1&0&3\\-1&1&1&-1\\0&-3&1&1\end{matrix}\right]$$Which of the following is the smallest disc in $\mathbb C$  which contains all eigen values of $A$. $|z-1|\le 7$ $|z-1|\le 6$ $|z-1|\le 4$. Characteristic polynomial of $A$ is $x^4-4x^3+21x^2-48x+46$. From this computing eigen values is very difficult in hand. Without computing eigen values how we can detect the required interval ? Does there any other process??",,"['linear-algebra', 'matrices']"
10,The Diagonal Elements Of A Special Symmetric Matrix,The Diagonal Elements Of A Special Symmetric Matrix,,"A $n \times n$ matrix $M$ is a symmetric matrix,where $n$ is odd($i.e.n=2k+1,k\in  \mathbb{Z}^{+}\cup{\{0\}}$). Every row of $M$ is a permutation of $\{1,2,\cdots,n\}$.   Show that the diagonal elements of $M$ is also a  permutation of $\{1,2,\cdots,n\}.$ $e.g.$ when $n=3, $ all possible matrices satisfying  $M$'s requirements as following: $$\begin{pmatrix} {\color{red} 3} & 2 & 1\\  2&  {\color{red}1}& 3\\  1 &  3& {\color{red}2} \end{pmatrix},\begin{pmatrix} {\color{red}3} & 1 & 2\\  1&  {\color{red}2}& 3\\  2 &  3& {\color{red}1} \end{pmatrix},\begin{pmatrix} \color{red}2 & 1 & 3\\  1&  \color{red}3& 2\\  3 &  2& \color{red}1 \end{pmatrix},\begin{pmatrix} \color{red}2 & 3 & 1\\  3&  \color{red}1& 2\\  1 &  2& \color{red}3 \end{pmatrix},\begin{pmatrix} {\color{red}1} & 2 & 3\\  2&  {\color{red}3}& 1\\  3 &  1& {\color{red}2} \end{pmatrix},\begin{pmatrix} {\color{red}1} & 3 & 2\\  3&  {\color{red}2}& 1\\  2 &  1& {\color{red}3} \end{pmatrix}.$$ Obviously,the diagonal elements of each one is a permutation of $\{1,2,3\}.$ For $n=2k+1,k\geq 2 ,k\in \mathbb{N}.$ I consider the characteristic polynomial of $M$: $$f_{\mathbf{M}}(\lambda)=(\lambda-\frac{n(n+1)}{2})(\lambda^{2}+a_{1}\lambda+b_{1})\cdots(\lambda^{2}+a_{k}\lambda+b_{k}).$$ If we can prove $a_{1}=a_{2}=\cdots=a_{k}=0,$then $\mathbb{trace}(M)=\frac{n(n+1)}{2}.$ Additionally,if we can prove the product of  diagonal elements $\prod_{i=1}^{n}a_{11} a_{22}\cdots a_{nn}=n!,$then the question will be sloved. But both of them  are not easy to be proved.If you have some good ideas, please give me some hints !","A $n \times n$ matrix $M$ is a symmetric matrix,where $n$ is odd($i.e.n=2k+1,k\in  \mathbb{Z}^{+}\cup{\{0\}}$). Every row of $M$ is a permutation of $\{1,2,\cdots,n\}$.   Show that the diagonal elements of $M$ is also a  permutation of $\{1,2,\cdots,n\}.$ $e.g.$ when $n=3, $ all possible matrices satisfying  $M$'s requirements as following: $$\begin{pmatrix} {\color{red} 3} & 2 & 1\\  2&  {\color{red}1}& 3\\  1 &  3& {\color{red}2} \end{pmatrix},\begin{pmatrix} {\color{red}3} & 1 & 2\\  1&  {\color{red}2}& 3\\  2 &  3& {\color{red}1} \end{pmatrix},\begin{pmatrix} \color{red}2 & 1 & 3\\  1&  \color{red}3& 2\\  3 &  2& \color{red}1 \end{pmatrix},\begin{pmatrix} \color{red}2 & 3 & 1\\  3&  \color{red}1& 2\\  1 &  2& \color{red}3 \end{pmatrix},\begin{pmatrix} {\color{red}1} & 2 & 3\\  2&  {\color{red}3}& 1\\  3 &  1& {\color{red}2} \end{pmatrix},\begin{pmatrix} {\color{red}1} & 3 & 2\\  3&  {\color{red}2}& 1\\  2 &  1& {\color{red}3} \end{pmatrix}.$$ Obviously,the diagonal elements of each one is a permutation of $\{1,2,3\}.$ For $n=2k+1,k\geq 2 ,k\in \mathbb{N}.$ I consider the characteristic polynomial of $M$: $$f_{\mathbf{M}}(\lambda)=(\lambda-\frac{n(n+1)}{2})(\lambda^{2}+a_{1}\lambda+b_{1})\cdots(\lambda^{2}+a_{k}\lambda+b_{k}).$$ If we can prove $a_{1}=a_{2}=\cdots=a_{k}=0,$then $\mathbb{trace}(M)=\frac{n(n+1)}{2}.$ Additionally,if we can prove the product of  diagonal elements $\prod_{i=1}^{n}a_{11} a_{22}\cdots a_{nn}=n!,$then the question will be sloved. But both of them  are not easy to be proved.If you have some good ideas, please give me some hints !",,"['linear-algebra', 'matrices']"
11,trace of symmetric matrix problems,trace of symmetric matrix problems,,I have the two problems below from a practice exam. I can prove them on their own but am not exactly sure if/how to show that they only hold for symmetric matrices and for '3)' showing that it only holds for a matrix with only positive eigenvalues. I know that if the eigenvalues are all positive the determinant will be positive and the trace but cant see how that affects whether '3)' is true or not. Show that $\operatorname{Tr}(A^2) \leq \operatorname{Tr}(A)^2$ holds for any symmetric matrix $A$ whose eigenvalues are all non-negative. Show that $\operatorname{Tr}(AB)^2 \le \operatorname{Tr}(A^2)\operatorname{Tr}(B^2)$ holds for any symmetric matrices $A$ and $B$.,I have the two problems below from a practice exam. I can prove them on their own but am not exactly sure if/how to show that they only hold for symmetric matrices and for '3)' showing that it only holds for a matrix with only positive eigenvalues. I know that if the eigenvalues are all positive the determinant will be positive and the trace but cant see how that affects whether '3)' is true or not. Show that $\operatorname{Tr}(A^2) \leq \operatorname{Tr}(A)^2$ holds for any symmetric matrix $A$ whose eigenvalues are all non-negative. Show that $\operatorname{Tr}(AB)^2 \le \operatorname{Tr}(A^2)\operatorname{Tr}(B^2)$ holds for any symmetric matrices $A$ and $B$.,,"['linear-algebra', 'matrices', 'trace']"
12,Prove reflection in a hyperplane is a linear map,Prove reflection in a hyperplane is a linear map,,"Let $\alpha \in \mathbb{R}^n$, $n \geq 2$, be a non-zero vector. Define a reflection in the hyperplane perpendicular to $\alpha$ by:   $$\sigma_{\alpha}(v) = v - \dfrac{2(v, \alpha)}{(\alpha, \alpha)} \cdot \alpha$$   ($(x, y)$ is the usual inner product on $\mathbb{R}^n$). 1) Show $\sigma_{\alpha}$ is a linear map that fixes the hyperplane orthogonal to $\alpha$ and sends $\alpha$ to $-\alpha$. 2) Given $\alpha, \beta$ non-zero vectors, determine when the subgroup $\langle \sigma_{\alpha}, \sigma_{\beta} \rangle$ is infinite. Find its order when it is finite. For 2) I don't understand what the group is. If $\sigma_{\alpha}$ and $\sigma_{\beta}$ are elements of a group, what other elements do they generate? Like for example, $\sigma_{\alpha}(\sigma_{\beta}(v)) = \left(v - \dfrac{2(v, \beta)}{(\beta, \beta)} \cdot \beta \right) - \dfrac{2\left(v - \dfrac{2(v, \beta)}{(\beta, \beta)} \cdot \beta, \beta \right)}{(\beta, \beta)} \cdot \beta$ which I guess makes sense (in the sense that dot products work in this function since the dot product is between vectors). But how do I know when there will be an infinite many number of these, and when there will be finitely many? I can't even find an identity function $\sigma$, because a composition of $\sigma_{\alpha}$ and $\sigma_{\beta}$ is $\sigma_{\beta}$ only when $\sigma_{\alpha} = v$, but this is a constant function and does not reflect $\alpha$ about the hyperplane to $-\alpha$, so this constant function cannot be in the group.","Let $\alpha \in \mathbb{R}^n$, $n \geq 2$, be a non-zero vector. Define a reflection in the hyperplane perpendicular to $\alpha$ by:   $$\sigma_{\alpha}(v) = v - \dfrac{2(v, \alpha)}{(\alpha, \alpha)} \cdot \alpha$$   ($(x, y)$ is the usual inner product on $\mathbb{R}^n$). 1) Show $\sigma_{\alpha}$ is a linear map that fixes the hyperplane orthogonal to $\alpha$ and sends $\alpha$ to $-\alpha$. 2) Given $\alpha, \beta$ non-zero vectors, determine when the subgroup $\langle \sigma_{\alpha}, \sigma_{\beta} \rangle$ is infinite. Find its order when it is finite. For 2) I don't understand what the group is. If $\sigma_{\alpha}$ and $\sigma_{\beta}$ are elements of a group, what other elements do they generate? Like for example, $\sigma_{\alpha}(\sigma_{\beta}(v)) = \left(v - \dfrac{2(v, \beta)}{(\beta, \beta)} \cdot \beta \right) - \dfrac{2\left(v - \dfrac{2(v, \beta)}{(\beta, \beta)} \cdot \beta, \beta \right)}{(\beta, \beta)} \cdot \beta$ which I guess makes sense (in the sense that dot products work in this function since the dot product is between vectors). But how do I know when there will be an infinite many number of these, and when there will be finitely many? I can't even find an identity function $\sigma$, because a composition of $\sigma_{\alpha}$ and $\sigma_{\beta}$ is $\sigma_{\beta}$ only when $\sigma_{\alpha} = v$, but this is a constant function and does not reflect $\alpha$ about the hyperplane to $-\alpha$, so this constant function cannot be in the group.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'finite-groups', 'linear-transformations']"
13,"Parallelepiped $P$ circumscribed around ellipsoid $S$: length of main diagonal of $P$ depends only on $S$, formula.","Parallelepiped  circumscribed around ellipsoid : length of main diagonal of  depends only on , formula.",P S P S,"Let $S$ be an ellipsoid in $\mathbb{R}^n$ (as in my previous question here ), and let $P$ be a parallelepiped such that $S$ is inscribed in $P$ (in particular, each face of $P$ is tangent to $S$). However, we do not require the edges of $P$ to be parallel to the axes of $S$; in particular, $S$ does not determine $P$ unique. What is the easiest way to see that the length of the main diagonal of $P$ depends only on $S$? What is a formula for this length in terms of the coefficients $a_1, \dots, a_n$ introduced by my aforementioned previous question here ?","Let $S$ be an ellipsoid in $\mathbb{R}^n$ (as in my previous question here ), and let $P$ be a parallelepiped such that $S$ is inscribed in $P$ (in particular, each face of $P$ is tangent to $S$). However, we do not require the edges of $P$ to be parallel to the axes of $S$; in particular, $S$ does not determine $P$ unique. What is the easiest way to see that the length of the main diagonal of $P$ depends only on $S$? What is a formula for this length in terms of the coefficients $a_1, \dots, a_n$ introduced by my aforementioned previous question here ?",,"['linear-algebra', 'euclidean-geometry']"
14,$O_{2n}(\mathbb{R}) \cap GL_{n}(\mathbb{C})=U(n)$,,O_{2n}(\mathbb{R}) \cap GL_{n}(\mathbb{C})=U(n),"During a lecture of a Lie Algebras yesterday, the professor of the class stated the following fact without proof $O_{2n}(\mathbb{R}) \cap GL_{n}(\mathbb{C})=U(n)$ Note that we are viewing $GL_{n}(\mathbb{C})$ as elements of $GL_{2n}(\mathbb{R})$ I was wondering if there was a quick way to see this ? I can see why this is so at $n=1$, where we need to show that  $O_{2}(\mathbb{R}) \cap GL_{1}(\mathbb{C})=U(1)$ We can identify elements of $GL_{1}(\mathbb{C})$ with $2 \times 2$ matrices of the form $\begin{bmatrix} x&-y\\ y&x\\ \end{bmatrix}$ where $x,y \in \mathbb{R}$ and whose determinant $x^{2}+y^{2}$ in nonzero. We can identify $U(1)$ with the rotation group of the plane $SO(2)$. For a matrix $A \in O_{2}(\mathbb{R}) \cap GL_{1}(\mathbb{C})$, we that $det(A)=x^{2}+y^{2}$ is either $1$ or $-1$ and the sum of squares implies that the determinant of $A$ is $1$ and hence $A$ must be in $SO(2)$. I realize now that this has to do with the 2 out of 3 property https://en.wikipedia.org/wiki/Unitary_group#2-out-of-3_property Thanks in advance.","During a lecture of a Lie Algebras yesterday, the professor of the class stated the following fact without proof $O_{2n}(\mathbb{R}) \cap GL_{n}(\mathbb{C})=U(n)$ Note that we are viewing $GL_{n}(\mathbb{C})$ as elements of $GL_{2n}(\mathbb{R})$ I was wondering if there was a quick way to see this ? I can see why this is so at $n=1$, where we need to show that  $O_{2}(\mathbb{R}) \cap GL_{1}(\mathbb{C})=U(1)$ We can identify elements of $GL_{1}(\mathbb{C})$ with $2 \times 2$ matrices of the form $\begin{bmatrix} x&-y\\ y&x\\ \end{bmatrix}$ where $x,y \in \mathbb{R}$ and whose determinant $x^{2}+y^{2}$ in nonzero. We can identify $U(1)$ with the rotation group of the plane $SO(2)$. For a matrix $A \in O_{2}(\mathbb{R}) \cap GL_{1}(\mathbb{C})$, we that $det(A)=x^{2}+y^{2}$ is either $1$ or $-1$ and the sum of squares implies that the determinant of $A$ is $1$ and hence $A$ must be in $SO(2)$. I realize now that this has to do with the 2 out of 3 property https://en.wikipedia.org/wiki/Unitary_group#2-out-of-3_property Thanks in advance.",,"['linear-algebra', 'abstract-algebra', 'lie-groups']"
15,Understanding degrees of freedom in relation to rank for $\sum_{i=1}^{n}(y_i-\bar{y})^2$,Understanding degrees of freedom in relation to rank for,\sum_{i=1}^{n}(y_i-\bar{y})^2,"So I'm looking at this website which states: One of the questions an instrutor [sic] dreads most from a mathematically unsophisticated audience is, ""What exactly is degrees of freedom?"" It's not that there's no answer. The mathematical answer is a single phrase, ""The rank of a quadratic form."" And near the end: Okay, so where's the quadratic form? Let's look at the variance of a single sample. If $y$ is an $n$ by $1$ vector of observations, then   $$\sum(y_i - \bar{y})^2 = y^{\prime}My\text{, where }M = \begin{pmatrix} 1-\frac{1}{n} & -1/n & \cdot & -1/n \\ -1/n          & 1-\frac{1}{n} & \cdot & -1/n \\ \cdot         & \cdot         & \cdot & \cdot \\ -1/n          & -1/n          & -1/n & 1-\frac{1}{n} \end{pmatrix}\text{.}$$   The number of degrees of freedom is equal to the rank of the $n$ by $n$ matrix $M$, which is $n-1$. I will use $r$ to denote the rank and $C(A)$ to denote the column space of a matrix $A$. From my notes, I have the definition $r(A) = r\left(C(A)\right)$, which is the number of vectors that create a basis for $C(A)$. Sure. Let's consider column one: $$\begin{pmatrix} 1-\frac{1}{n}\\ -1/n \\ \cdot \\ -1/n \end{pmatrix} = \dfrac{-1}{n} \begin{pmatrix} 1-n \\ 1 \\ \cdot \\ 1  \end{pmatrix} = \dfrac{-1}{n}\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix} + \dfrac{-1}{n}\begin{pmatrix} -n \\ 0 \\ \cdot \\ 0  \end{pmatrix} = \dfrac{-1}{n}\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}+\begin{pmatrix} 1 \\ 0 \\ \cdot \\ 0  \end{pmatrix}\text{.} $$ So based on what I'm seeing here, I'm guessing that a basis for $C(A)$ would be something like  $$\mathscr{V} = \left\{\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ \cdot \\ 0  \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \cdot \\ 0  \end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \cdot \\ 1  \end{pmatrix}\right\}$$ which consists of $n+1$ vectors. However, clearly $$\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}$$ needs to be removed, since it is the sum of the other $n$ vectors, so I get that there are $n$ vectors in $\mathscr{V}$, and therefore $r(\mathscr{V}) = r(C(M)) = n$. Where is the $n-1$ coming from?","So I'm looking at this website which states: One of the questions an instrutor [sic] dreads most from a mathematically unsophisticated audience is, ""What exactly is degrees of freedom?"" It's not that there's no answer. The mathematical answer is a single phrase, ""The rank of a quadratic form."" And near the end: Okay, so where's the quadratic form? Let's look at the variance of a single sample. If $y$ is an $n$ by $1$ vector of observations, then   $$\sum(y_i - \bar{y})^2 = y^{\prime}My\text{, where }M = \begin{pmatrix} 1-\frac{1}{n} & -1/n & \cdot & -1/n \\ -1/n          & 1-\frac{1}{n} & \cdot & -1/n \\ \cdot         & \cdot         & \cdot & \cdot \\ -1/n          & -1/n          & -1/n & 1-\frac{1}{n} \end{pmatrix}\text{.}$$   The number of degrees of freedom is equal to the rank of the $n$ by $n$ matrix $M$, which is $n-1$. I will use $r$ to denote the rank and $C(A)$ to denote the column space of a matrix $A$. From my notes, I have the definition $r(A) = r\left(C(A)\right)$, which is the number of vectors that create a basis for $C(A)$. Sure. Let's consider column one: $$\begin{pmatrix} 1-\frac{1}{n}\\ -1/n \\ \cdot \\ -1/n \end{pmatrix} = \dfrac{-1}{n} \begin{pmatrix} 1-n \\ 1 \\ \cdot \\ 1  \end{pmatrix} = \dfrac{-1}{n}\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix} + \dfrac{-1}{n}\begin{pmatrix} -n \\ 0 \\ \cdot \\ 0  \end{pmatrix} = \dfrac{-1}{n}\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}+\begin{pmatrix} 1 \\ 0 \\ \cdot \\ 0  \end{pmatrix}\text{.} $$ So based on what I'm seeing here, I'm guessing that a basis for $C(A)$ would be something like  $$\mathscr{V} = \left\{\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ \cdot \\ 0  \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \cdot \\ 0  \end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \cdot \\ 1  \end{pmatrix}\right\}$$ which consists of $n+1$ vectors. However, clearly $$\begin{pmatrix} 1 \\ 1 \\ \cdot \\ 1  \end{pmatrix}$$ needs to be removed, since it is the sum of the other $n$ vectors, so I get that there are $n$ vectors in $\mathscr{V}$, and therefore $r(\mathscr{V}) = r(C(M)) = n$. Where is the $n-1$ coming from?",,"['linear-algebra', 'statistics']"
16,Find trace of linear operator,Find trace of linear operator,,"Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$  by permutation of the basis vectors. Suppose we know its eigenvalues ( some roots of unity  ): $\lambda_1, \lambda_2, \ldots, \lambda_n.$ Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i<j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us expand the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$. It is clear that  $A^{(2)}$ permutes the basis vectors of   $V^{(2)}$ so $A^{(2)}$   is an endomorphism of $V^{(2)}$. Question. What is the trace  of the $A^{(2)}?$ By  method of trial and error I have found a formula for the trace $$ {\rm Tr}(A^{(2)})=\sum_{i=1}^n\lambda_i^2+\sum_{i<j}\lambda_i \lambda_j-\sum_{i=1}^n \lambda_i $$ but I can't prove it. Any ideas?","Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$  by permutation of the basis vectors. Suppose we know its eigenvalues ( some roots of unity  ): $\lambda_1, \lambda_2, \ldots, \lambda_n.$ Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i<j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us expand the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$. It is clear that  $A^{(2)}$ permutes the basis vectors of   $V^{(2)}$ so $A^{(2)}$   is an endomorphism of $V^{(2)}$. Question. What is the trace  of the $A^{(2)}?$ By  method of trial and error I have found a formula for the trace $$ {\rm Tr}(A^{(2)})=\sum_{i=1}^n\lambda_i^2+\sum_{i<j}\lambda_i \lambda_j-\sum_{i=1}^n \lambda_i $$ but I can't prove it. Any ideas?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tensor-products']"
17,Union of conjugacy classes of $O(n)$ is not a subgroup,Union of conjugacy classes of  is not a subgroup,O(n),"Let $O(n)$ be the standard orthogonal group of real matrices. I am trying to prove the following: $N = \bigcup_{g\in GL_n(\mathbb{R})}g\cdot O(n)\cdot g^{-1}$ is not a subgroup of $GL_n(\mathbb{R})$. I know that if it was a subgroup then it was equal to the normal closure of $O(n)$ but I do not know what that is... Motivation: It is proved here that a linear automorphism $T:V \rightarrow V$ preserves some inner product on $V$ if and only if the matrix of $T$ w.r.t an arbitrary basis is similar to an orthogonal matrix. I want to prove a composition of two transformation of this type is not necessarily also of that type. (Which amounts to proving $N$ is not a subgroup, since closure under taking inverses clearly holds).","Let $O(n)$ be the standard orthogonal group of real matrices. I am trying to prove the following: $N = \bigcup_{g\in GL_n(\mathbb{R})}g\cdot O(n)\cdot g^{-1}$ is not a subgroup of $GL_n(\mathbb{R})$. I know that if it was a subgroup then it was equal to the normal closure of $O(n)$ but I do not know what that is... Motivation: It is proved here that a linear automorphism $T:V \rightarrow V$ preserves some inner product on $V$ if and only if the matrix of $T$ w.r.t an arbitrary basis is similar to an orthogonal matrix. I want to prove a composition of two transformation of this type is not necessarily also of that type. (Which amounts to proving $N$ is not a subgroup, since closure under taking inverses clearly holds).",,"['linear-algebra', 'group-theory', 'normal-subgroups']"
18,Distance between point and plane - why use the dot product?,Distance between point and plane - why use the dot product?,,"So according to this , the signed distance between a point and a plane will be the dot product of the plane's normal vector (does it have to be a unit vector?) and the point-in-plane minus the point vector. I searched everywhere and I can't find a good explanation on why does the dot product give the correct answer. I even studied a little bit more about the dot product itself and I came to know that the dot product of a * b is like multiplying the magnitudes of the vectors that go on the same direction . This still doesn't help me understand my problem. If it matters, I encountered this problem as a programmer.","So according to this , the signed distance between a point and a plane will be the dot product of the plane's normal vector (does it have to be a unit vector?) and the point-in-plane minus the point vector. I searched everywhere and I can't find a good explanation on why does the dot product give the correct answer. I even studied a little bit more about the dot product itself and I came to know that the dot product of a * b is like multiplying the magnitudes of the vectors that go on the same direction . This still doesn't help me understand my problem. If it matters, I encountered this problem as a programmer.",,"['linear-algebra', 'vector-spaces', 'vectors']"
19,Inverse of a matrix having zeroes in diagonal and one elsewhere,Inverse of a matrix having zeroes in diagonal and one elsewhere,,"Could any one help me to find inverse of such matrix? I observed that $A=  J-I$, where J is a matrix having all entries 1. Thanks for helping.","Could any one help me to find inverse of such matrix? I observed that $A=  J-I$, where J is a matrix having all entries 1. Thanks for helping.",,['linear-algebra']
20,Explicit calculation of eigenvalues of banded Toeplitz matrix,Explicit calculation of eigenvalues of banded Toeplitz matrix,,"I recently found a paper which detailed a method of finding the eigenvalues of the $n\times n$ banded Toeplitz matrix $$ \left[ \begin{array}{ccccccc}  a_0 & a_1 & a_2 & \dots & a_s & 0 & \dots & 0 \\ a_{-1} & a_0 & \dots & & & & & 0 \\ a_{-2} & \dots & & & & & & 0 \\ \vdots &&&&&& & \vdots \\ a_{-s} &\dots &&&& && \vdots \\ 0 & \dots &&&&& & \vdots \\ \vdots &&&&&& a_0 & a_1\\ 0 & \dots & & & & & a_{-1} & a_0 \end{array} \right]_.$$ The problem was reduced to finding the zeros of a much smaller determinant, of dimension $(2s) \times (2s)$.  As an example the authors compute the well-known formula for eigenvalues of a tridiagonal Toeplitz matrix, and the associated determinant had a form along the lines of $$ \left[ \begin{array}{cc} a & b \\ a^{n+1} & b^{n+1} \end{array} \right] $$ That's just about all I can recall from the reading - I managed to misplace the paper/pdf and am having trouble finding any references that might point to it in the literature. I'm hoping a MSE reader recognizes this and can point me in the direction of the source.  Any leads would be much appreciated.","I recently found a paper which detailed a method of finding the eigenvalues of the $n\times n$ banded Toeplitz matrix $$ \left[ \begin{array}{ccccccc}  a_0 & a_1 & a_2 & \dots & a_s & 0 & \dots & 0 \\ a_{-1} & a_0 & \dots & & & & & 0 \\ a_{-2} & \dots & & & & & & 0 \\ \vdots &&&&&& & \vdots \\ a_{-s} &\dots &&&& && \vdots \\ 0 & \dots &&&&& & \vdots \\ \vdots &&&&&& a_0 & a_1\\ 0 & \dots & & & & & a_{-1} & a_0 \end{array} \right]_.$$ The problem was reduced to finding the zeros of a much smaller determinant, of dimension $(2s) \times (2s)$.  As an example the authors compute the well-known formula for eigenvalues of a tridiagonal Toeplitz matrix, and the associated determinant had a form along the lines of $$ \left[ \begin{array}{cc} a & b \\ a^{n+1} & b^{n+1} \end{array} \right] $$ That's just about all I can recall from the reading - I managed to misplace the paper/pdf and am having trouble finding any references that might point to it in the literature. I'm hoping a MSE reader recognizes this and can point me in the direction of the source.  Any leads would be much appreciated.",,"['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors', 'toeplitz-matrices']"
21,Let $trcA=0$.why $A=M+N$ where $M$ and $N$ are nilpotent matrices?,Let .why  where  and  are nilpotent matrices?,trcA=0 A=M+N M N,Let $A \in {M_n}$ and $trcA=0$.why $A=M+N$  where $M$ and $N$  are nilpotent matrices?,Let $A \in {M_n}$ and $trcA=0$.why $A=M+N$  where $M$ and $N$  are nilpotent matrices?,,"['linear-algebra', 'matrices']"
22,Expressing a line as a linear combination of two points on the line.,Expressing a line as a linear combination of two points on the line.,,"I'm currently reading Pugh's Analysis. He makes the statement that the line between two points x and y is the set of linear combinations $sx + ty$ where $s + t = 1$. I'm satisfied that this is true, as the line between two points $x$ and $y$ has the equation $y-x_2=\frac{x_2-y_2}{x_1-y_1}(x-x_1)$ and the points of form  $(sx_1+ty_1, sx_2+ty_2)$ are solutions of the equation. I still don't have any reasonable geometric intuition for why this is true though. Help?","I'm currently reading Pugh's Analysis. He makes the statement that the line between two points x and y is the set of linear combinations $sx + ty$ where $s + t = 1$. I'm satisfied that this is true, as the line between two points $x$ and $y$ has the equation $y-x_2=\frac{x_2-y_2}{x_1-y_1}(x-x_1)$ and the points of form  $(sx_1+ty_1, sx_2+ty_2)$ are solutions of the equation. I still don't have any reasonable geometric intuition for why this is true though. Help?",,['linear-algebra']
23,Do scalars commute across matrices?,Do scalars commute across matrices?,,"Do scalars commute across matrices? $A,B,C$ are matrices that work together, lets just assume they are all $n\times n$, and $a$ is a scalar. E.g. does $aABC=AaBC=ABaC=ABCa$, I imagine this is the case, but I wanted to verify, and maybe a quick reason why would be good. Can't really show working since I am asking for a property, so please don't downvote for that...","Do scalars commute across matrices? $A,B,C$ are matrices that work together, lets just assume they are all $n\times n$, and $a$ is a scalar. E.g. does $aABC=AaBC=ABaC=ABCa$, I imagine this is the case, but I wanted to verify, and maybe a quick reason why would be good. Can't really show working since I am asking for a property, so please don't downvote for that...",,"['linear-algebra', 'matrices']"
24,A combinatorial identity.,A combinatorial identity.,,"Let $n \in \mathbb N$ and $X_1,\ldots,X_n$ be subsets of $\{1,\ldots,n\}$ such that there is some $p$ such that $\forall i\in  \{1,\ldots,n\}, |X_i|=p$ . Suppose as well that there is some $q$ such that $i\neq j \implies |X_i \cap X_j|=q$ . Prove that $p^2=p+(n-1)q$ . I tried something with an incidence matrix and it boils down to proving that its determinant is $p^2(p-q)^{(n-1)/2}$ , but I can't prove that EDIT : The problem as it is stated is flawed. It must be added that each $i$ appears in exactly $p$ of the $X_j$","Let and be subsets of such that there is some such that . Suppose as well that there is some such that . Prove that . I tried something with an incidence matrix and it boils down to proving that its determinant is , but I can't prove that EDIT : The problem as it is stated is flawed. It must be added that each appears in exactly of the","n \in \mathbb N X_1,\ldots,X_n \{1,\ldots,n\} p \forall i\in  \{1,\ldots,n\}, |X_i|=p q i\neq j \implies |X_i \cap X_j|=q p^2=p+(n-1)q p^2(p-q)^{(n-1)/2} i p X_j","['linear-algebra', 'combinatorics']"
25,General linear group and special linear group,General linear group and special linear group,,"Consider the general linear group $$GL(n,\mathbb R)=\{g\in {\mathbb R}^{n\times n}\mid\det(g)\neq 0\}$$ Prove that the derivative of the function $f=\det:{\mathbb R}^{n\times n}\to\mathbb R$ is given by $$df(g)v=\det(g)\operatorname{trace}(g^{-1}v)$$ for every $g\in GL(n,\mathbb R)$ and every $v\in {\mathbb R}^{n\times n}$. Deduce that the special linear group $$SL(n,\mathbb R):=\{g\in GL(n,\mathbb R)\mid\det(g)=1\}$$ is a smooth submainfold of ${\mathbb R}^{n\times n}$ I have a bunch of questions to ask before I can solve it: Is the dimension of $GL(n,\mathbb R)$ $n^2$? And what is the dimension of the special linear group? What are the basis vectors of the special linear group? Is the general linear space is a metric space? I was trying to compute $df(g)v$, and by definition, $df(g)v=\lim_{t\to 0}\dfrac{\det(g+tv)-\det(g)}{t}$. But how can I expand $ \det(g+tv)$?","Consider the general linear group $$GL(n,\mathbb R)=\{g\in {\mathbb R}^{n\times n}\mid\det(g)\neq 0\}$$ Prove that the derivative of the function $f=\det:{\mathbb R}^{n\times n}\to\mathbb R$ is given by $$df(g)v=\det(g)\operatorname{trace}(g^{-1}v)$$ for every $g\in GL(n,\mathbb R)$ and every $v\in {\mathbb R}^{n\times n}$. Deduce that the special linear group $$SL(n,\mathbb R):=\{g\in GL(n,\mathbb R)\mid\det(g)=1\}$$ is a smooth submainfold of ${\mathbb R}^{n\times n}$ I have a bunch of questions to ask before I can solve it: Is the dimension of $GL(n,\mathbb R)$ $n^2$? And what is the dimension of the special linear group? What are the basis vectors of the special linear group? Is the general linear space is a metric space? I was trying to compute $df(g)v$, and by definition, $df(g)v=\lim_{t\to 0}\dfrac{\det(g+tv)-\det(g)}{t}$. But how can I expand $ \det(g+tv)$?",,"['linear-algebra', 'differential-geometry']"
26,Surface normal to point on displaced sphere,Surface normal to point on displaced sphere,,"I want to calculate the surface normal to a point on a deformed sphere. The surface of the sphere is displaced along its (original) normals by a function $f(\vec x)$. In mathematical terms: Let $\vec x$ be a unit length vector in 3D, i.e. any point on the unit sphere centered on the origin. And let $f:\vec x \to \mathbb{R}$ be a function with a well-defined and continuous gradient $\nabla f(\vec x)$ for every $\vec x$. Now a point on the surface of the resulting sphere is defined as: $\vec P(\vec x) = (R + s \cdot f(\vec x)) \cdot \vec x$ where $R$ and $s$ are constants for the radius and the modulation depth, respectively. The question now: Is this information sufficient to calculate the surface normal to the point $\vec P$? I feel like this should be possible, but it is entirely possible that it is not. More information: The function in my case is 3D Simplex Noise which gives values on the range $(-1,1)$ for any $\vec x$. The values for $R$ are about 10 times larger than those of $s$, probably more. Maybe this helps if only an approximation is possible. UPDATE: Yes, it is possible, though there is still some error in here: Calculate the gradient: $\vec g = \nabla f(\vec x)$ Then project it into the tangent plane to the sphere passing through the point: $\vec h = \vec g - (\vec g \cdot \vec x)\vec x$ The normal can then be calculated as $\vec n = \vec x - s \cdot \vec h$ This works as long as the radius is $R=1$ and something like $s \ll R$. I think for $R \ne 1$ the gradient has to be rescaled: $\vec g = \dfrac{1}{R} \nabla f(\vec x)$ But I have no idea why the error gets larger with increasing $s$. UPDATE 2: Alright, the rescaling factor was only half complete $\vec g = \dfrac{\nabla f(\vec x)}{R + s \cdot f(\vec x)}$ Now it works for all $R$ and $s$. Here is an image to give some intuition what this thing looks like (with approximated normals).","I want to calculate the surface normal to a point on a deformed sphere. The surface of the sphere is displaced along its (original) normals by a function $f(\vec x)$. In mathematical terms: Let $\vec x$ be a unit length vector in 3D, i.e. any point on the unit sphere centered on the origin. And let $f:\vec x \to \mathbb{R}$ be a function with a well-defined and continuous gradient $\nabla f(\vec x)$ for every $\vec x$. Now a point on the surface of the resulting sphere is defined as: $\vec P(\vec x) = (R + s \cdot f(\vec x)) \cdot \vec x$ where $R$ and $s$ are constants for the radius and the modulation depth, respectively. The question now: Is this information sufficient to calculate the surface normal to the point $\vec P$? I feel like this should be possible, but it is entirely possible that it is not. More information: The function in my case is 3D Simplex Noise which gives values on the range $(-1,1)$ for any $\vec x$. The values for $R$ are about 10 times larger than those of $s$, probably more. Maybe this helps if only an approximation is possible. UPDATE: Yes, it is possible, though there is still some error in here: Calculate the gradient: $\vec g = \nabla f(\vec x)$ Then project it into the tangent plane to the sphere passing through the point: $\vec h = \vec g - (\vec g \cdot \vec x)\vec x$ The normal can then be calculated as $\vec n = \vec x - s \cdot \vec h$ This works as long as the radius is $R=1$ and something like $s \ll R$. I think for $R \ne 1$ the gradient has to be rescaled: $\vec g = \dfrac{1}{R} \nabla f(\vec x)$ But I have no idea why the error gets larger with increasing $s$. UPDATE 2: Alright, the rescaling factor was only half complete $\vec g = \dfrac{\nabla f(\vec x)}{R + s \cdot f(\vec x)}$ Now it works for all $R$ and $s$. Here is an image to give some intuition what this thing looks like (with approximated normals).",,"['linear-algebra', 'geometry', 'differential-geometry']"
27,Solving $AB - BA = C$,Solving,AB - BA = C,"Suppose $C$ is an $n\times n$ matrix over complex numbers, with trace $0$. Are there always $n\times n$ matrices $A,B$ such that $AB - BA = C$? (Inspired by a recent question which asked for a trace free proof of non-existence of solutions for $C=I$).","Suppose $C$ is an $n\times n$ matrix over complex numbers, with trace $0$. Are there always $n\times n$ matrices $A,B$ such that $AB - BA = C$? (Inspired by a recent question which asked for a trace free proof of non-existence of solutions for $C=I$).",,"['linear-algebra', 'matrices', 'matrix-equations']"
28,Basis of the matrices with only non diagonalizable matrices,Basis of the matrices with only non diagonalizable matrices,,"Is it possible to find a basis of $M_n(\mathbb{R})$ that only has non diagonalisable matrices ? I'm looking for a rather easy example, or a proof of the (non-)existence.","Is it possible to find a basis of $M_n(\mathbb{R})$ that only has non diagonalisable matrices ? I'm looking for a rather easy example, or a proof of the (non-)existence.",,"['linear-algebra', 'matrices', 'diagonalization']"
29,Why is not a vector space isomorphic to its dual space?,Why is not a vector space isomorphic to its dual space?,,"Let $V$ be a finitely generated vector space with a basis $\mathcal{B}=\{\alpha_1,\cdots,\alpha_n\}$ and let $\mathcal{B}^*= \{f_1,\cdots,f_n\}$ be the dual basis of $\mathcal{B}$. In this situation, I defined a function $T:V\to V^*$ with $T(\alpha_i)=f_i$. I think this function is linear and bijective, thus an isomorphism. Please disprove my claim.","Let $V$ be a finitely generated vector space with a basis $\mathcal{B}=\{\alpha_1,\cdots,\alpha_n\}$ and let $\mathcal{B}^*= \{f_1,\cdots,f_n\}$ be the dual basis of $\mathcal{B}$. In this situation, I defined a function $T:V\to V^*$ with $T(\alpha_i)=f_i$. I think this function is linear and bijective, thus an isomorphism. Please disprove my claim.",,['linear-algebra']
30,"$P(AB=BA)$ , $A,B\in M_{3x3}(\mathbb Z/p\mathbb Z)$",",","P(AB=BA) A,B\in M_{3x3}(\mathbb Z/p\mathbb Z)","Let $A,B\in M_{3x3}(\mathbb Z/p\mathbb Z)$ ($p$ a prime number). Find the probability $P$ that $AB=BA$ that is $P(AB=BA)$ $$A=\begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \\ \end{pmatrix} $$ $$B=\begin{pmatrix} b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23} \\ b_{31} & b_{32} & b_{33} \\ \end{pmatrix} $$ Please I would really appreciate if you can help me with this problem. Any ideas or suggestions would be highly appreciated :)","Let $A,B\in M_{3x3}(\mathbb Z/p\mathbb Z)$ ($p$ a prime number). Find the probability $P$ that $AB=BA$ that is $P(AB=BA)$ $$A=\begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \\ \end{pmatrix} $$ $$B=\begin{pmatrix} b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23} \\ b_{31} & b_{32} & b_{33} \\ \end{pmatrix} $$ Please I would really appreciate if you can help me with this problem. Any ideas or suggestions would be highly appreciated :)",,"['linear-algebra', 'probability', 'combinatorics', 'matrices']"
31,Cholesky factor when adding a row and column to already factorized matrix,Cholesky factor when adding a row and column to already factorized matrix,,"I have a positive deifnite, symmetrical, $N\times N$ real matrix $A$ which has 1's on the diagonal and all off-diagonal elements positive and $<1$. Let $A=LL^t$ be the Cholesky decomposition of $A$. Suppose now that I extend $A$ as follows: $$\left( \begin{array}{cc} A & a \\ a^t & 1 \end{array} \right)$$ where $a$ is a $N\times1$ real vector with positive elements $<1$. Thus the extended matrix has the same structure as the original matrix $A$. I would like to prove that the Cholesky factor of the extended matrix has the form $$\left( \begin{array}{cc} L & 0 \\ c^t & d \end{array} \right)$$ where $L$ is, again, the Cholesky factor of $A$, $c$ an appropriate $N\times 1$ real vector, and $d$ an appropriate scalar, positive or 0. By definition of Cholesky factor, the following should hold: $$\left( \begin{array}{cc} A & a \\ a^t & 1 \end{array} \right) = \left( \begin{array}{cc} L & 0 \\ c^t & d \end{array} \right) \left( \begin{array}{cc} L^t & c \\ 0 & d \end{array} \right) = \left( \begin{array}{cc} LL^t & Lc \\ L^tc^t & c^t c + d^2 \end{array} \right)$$ where I just carried out the matrix product. This is promising, and means that we have to prove that we can choose $c$ and $d$ so that these two statements hold: $$a=Lc$$ $$1=c^tc+d^2$$ The first is easy because $L$ is invertible: $$c=L^{-1}a$$ Then second equation becomes $$1=a^t(L^{-1})^tL^{-1}a+d^2$$ or $$1=a^tA^{-1}a+d^2$$ or $$d=\sqrt{1 - a^tA^{-1}a }$$ which gives a real $d$ so long as $a^t A^{-1} a<1$, but I am not sure one can prove this. If that helps, the entries of $A$ and inner products of unit vectors. I have not been able to find numerical counterexamples, but the elements of $a$ are not necessarily small and in the worst case its norm is close to $N$. Is there something about the norm of $A^{-1}$ or of $L^-1$ that can help me out here? Thanks, Stefano","I have a positive deifnite, symmetrical, $N\times N$ real matrix $A$ which has 1's on the diagonal and all off-diagonal elements positive and $<1$. Let $A=LL^t$ be the Cholesky decomposition of $A$. Suppose now that I extend $A$ as follows: $$\left( \begin{array}{cc} A & a \\ a^t & 1 \end{array} \right)$$ where $a$ is a $N\times1$ real vector with positive elements $<1$. Thus the extended matrix has the same structure as the original matrix $A$. I would like to prove that the Cholesky factor of the extended matrix has the form $$\left( \begin{array}{cc} L & 0 \\ c^t & d \end{array} \right)$$ where $L$ is, again, the Cholesky factor of $A$, $c$ an appropriate $N\times 1$ real vector, and $d$ an appropriate scalar, positive or 0. By definition of Cholesky factor, the following should hold: $$\left( \begin{array}{cc} A & a \\ a^t & 1 \end{array} \right) = \left( \begin{array}{cc} L & 0 \\ c^t & d \end{array} \right) \left( \begin{array}{cc} L^t & c \\ 0 & d \end{array} \right) = \left( \begin{array}{cc} LL^t & Lc \\ L^tc^t & c^t c + d^2 \end{array} \right)$$ where I just carried out the matrix product. This is promising, and means that we have to prove that we can choose $c$ and $d$ so that these two statements hold: $$a=Lc$$ $$1=c^tc+d^2$$ The first is easy because $L$ is invertible: $$c=L^{-1}a$$ Then second equation becomes $$1=a^t(L^{-1})^tL^{-1}a+d^2$$ or $$1=a^tA^{-1}a+d^2$$ or $$d=\sqrt{1 - a^tA^{-1}a }$$ which gives a real $d$ so long as $a^t A^{-1} a<1$, but I am not sure one can prove this. If that helps, the entries of $A$ and inner products of unit vectors. I have not been able to find numerical counterexamples, but the elements of $a$ are not necessarily small and in the worst case its norm is close to $N$. Is there something about the norm of $A^{-1}$ or of $L^-1$ that can help me out here? Thanks, Stefano",,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-decomposition']"
32,Calculate the dimension of a space of operators,Calculate the dimension of a space of operators,,"This is a homework question. First, consider the ring of real polynomials in $n$ variables, $P_n=\mathbb{R}[x_1,\ldots,x_n]$ ($n\geq 2$), and let $S_n$ act on $P_n$ by automorphism (of algebra) by permutating the variables: $\sigma(x_i)=x_{\sigma(i)}$ For each $i=1,\ldots,n-1$, let $s_i$ be the transposition $(i\ i+1)$ (so $s_i(x_i)=x_{i+1}$ and $s_i(x_{i+1})=x_i$). Finally, for each $i$, consider the operator $\Delta_i:P_n\to P_n$ given by $$\Delta_i(f)=\frac{f-s_i(f)}{x_i-x_{i+1}}$$ (a) Show that $\Delta_i$ is a linear operator on $P_n$ that satisfies the rule   $$\Delta_i(fg)=\Delta_i(f)g+s_i(f)\Delta_i(g)$$   (b) Let $D_n$ be the subalgebra generated by $Id_P,\Delta_1,\ldots,\Delta_{n-1}$. Calculate the dimension of $D_3$. This is what I have so far: It is easy to check that $\Delta_i$ is a well-defined operator and that the rule above is satisfied. Moreover, in the basis $\left\{x_1^{\alpha_1}\cdots x_n^{\alpha_n}\right\}$, we have the following (let's write $x(\alpha)=x_1^{\alpha_1}\cdots x_n^{\alpha_n}$ for $\alpha=(\alpha_1,\ldots,\alpha_n)$) $$\Delta_i(x(\alpha))=x_1^{\alpha_1}\cdots x_{i-1}^{\alpha_{i-1}}x_i^{\min(\alpha_i,\alpha_{i+1})}x_{i+1}^{\min(\alpha_i,\alpha_{i+1})}\left(\sum_{k=0}^{|\alpha_i-\alpha_{i+1}|-1}x_i^kx_{i+1}^{|\alpha_i-\alpha_{i+1}|-1-k}\right)x_{i+1}^{\alpha_{i+2}}\cdots x_n^{\alpha_n}$$ (the sum is $0$ if $\alpha_i=\alpha_{i+1}$) The problem is finding the dimension of $D_3$. I checked that $\Delta_i^2=0$, so $D_3$ is generated (as a vector space) by $Id_P,\Delta_1,\Delta_1\Delta_2,\Delta_1\Delta_2\Delta_1,\ldots,\Delta_2,\Delta_2\Delta_1,\Delta_2\Delta_1\Delta_2,\ldots$, but I couldn`t find any other good relation between $\Delta_1$ and $\Delta_2$ so obtain an upper bound for the dimension of $D_3$ (which I'm guessing is finite).","This is a homework question. First, consider the ring of real polynomials in $n$ variables, $P_n=\mathbb{R}[x_1,\ldots,x_n]$ ($n\geq 2$), and let $S_n$ act on $P_n$ by automorphism (of algebra) by permutating the variables: $\sigma(x_i)=x_{\sigma(i)}$ For each $i=1,\ldots,n-1$, let $s_i$ be the transposition $(i\ i+1)$ (so $s_i(x_i)=x_{i+1}$ and $s_i(x_{i+1})=x_i$). Finally, for each $i$, consider the operator $\Delta_i:P_n\to P_n$ given by $$\Delta_i(f)=\frac{f-s_i(f)}{x_i-x_{i+1}}$$ (a) Show that $\Delta_i$ is a linear operator on $P_n$ that satisfies the rule   $$\Delta_i(fg)=\Delta_i(f)g+s_i(f)\Delta_i(g)$$   (b) Let $D_n$ be the subalgebra generated by $Id_P,\Delta_1,\ldots,\Delta_{n-1}$. Calculate the dimension of $D_3$. This is what I have so far: It is easy to check that $\Delta_i$ is a well-defined operator and that the rule above is satisfied. Moreover, in the basis $\left\{x_1^{\alpha_1}\cdots x_n^{\alpha_n}\right\}$, we have the following (let's write $x(\alpha)=x_1^{\alpha_1}\cdots x_n^{\alpha_n}$ for $\alpha=(\alpha_1,\ldots,\alpha_n)$) $$\Delta_i(x(\alpha))=x_1^{\alpha_1}\cdots x_{i-1}^{\alpha_{i-1}}x_i^{\min(\alpha_i,\alpha_{i+1})}x_{i+1}^{\min(\alpha_i,\alpha_{i+1})}\left(\sum_{k=0}^{|\alpha_i-\alpha_{i+1}|-1}x_i^kx_{i+1}^{|\alpha_i-\alpha_{i+1}|-1-k}\right)x_{i+1}^{\alpha_{i+2}}\cdots x_n^{\alpha_n}$$ (the sum is $0$ if $\alpha_i=\alpha_{i+1}$) The problem is finding the dimension of $D_3$. I checked that $\Delta_i^2=0$, so $D_3$ is generated (as a vector space) by $Id_P,\Delta_1,\Delta_1\Delta_2,\Delta_1\Delta_2\Delta_1,\ldots,\Delta_2,\Delta_2\Delta_1,\Delta_2\Delta_1\Delta_2,\ldots$, but I couldn`t find any other good relation between $\Delta_1$ and $\Delta_2$ so obtain an upper bound for the dimension of $D_3$ (which I'm guessing is finite).",,['linear-algebra']
33,Finding only first row in a matrix inverse,Finding only first row in a matrix inverse,,"Let's say I have a somewhat large matrix $M$ and I need to find its inverse $M^{-1}$, but I only care about the first row in that inverse, what's the best algorithm to use to calculate just this row? My matrix $M$ has the following properties: All its entries describe probabilities, i.e. take on values between $0$ and $1$ (inclusive) Many of the entries are $0$, but I don't know before hand which ones All entries in the same row sum to $1$ $M$'s size is on the order of $10\times10$ to $100\times100$ I need to solve this problem literally a trillion times, though, so I need an algorithm that is as efficient as possible for matrices of this size.","Let's say I have a somewhat large matrix $M$ and I need to find its inverse $M^{-1}$, but I only care about the first row in that inverse, what's the best algorithm to use to calculate just this row? My matrix $M$ has the following properties: All its entries describe probabilities, i.e. take on values between $0$ and $1$ (inclusive) Many of the entries are $0$, but I don't know before hand which ones All entries in the same row sum to $1$ $M$'s size is on the order of $10\times10$ to $100\times100$ I need to solve this problem literally a trillion times, though, so I need an algorithm that is as efficient as possible for matrices of this size.",,"['linear-algebra', 'matrices', 'inverse', 'numerical-linear-algebra']"
34,How to show $\exp(tX)\exp(tY)=\exp(t(X+Y)+tR(t))$ with $\displaystyle \lim_{t\to 0} R(t)=0$?,How to show  with ?,\exp(tX)\exp(tY)=\exp(t(X+Y)+tR(t)) \displaystyle \lim_{t\to 0} R(t)=0,"Let $X\in GL(n, \mathbb R)$. The exponential of $X$ is the matrix given by $$\exp(X)=\sum_{n=0}^\infty \frac{X^n}{n!}.$$ I need some help for showing the following result: $$\exp(tX)\exp(tY)=\exp(t(X+Y)+tR(t)),\quad t\in\mathbb R,$$ with $\displaystyle\lim_{t\to 0} R(t)=0$. Thanks Remark: I need this result for showing that if $H$ is a closed subgroup of a Lie group $G\leq GL(n, \mathbb R)$ then $H$ is itself a lie group.","Let $X\in GL(n, \mathbb R)$. The exponential of $X$ is the matrix given by $$\exp(X)=\sum_{n=0}^\infty \frac{X^n}{n!}.$$ I need some help for showing the following result: $$\exp(tX)\exp(tY)=\exp(t(X+Y)+tR(t)),\quad t\in\mathbb R,$$ with $\displaystyle\lim_{t\to 0} R(t)=0$. Thanks Remark: I need this result for showing that if $H$ is a closed subgroup of a Lie group $G\leq GL(n, \mathbb R)$ then $H$ is itself a lie group.",,"['linear-algebra', 'analysis', 'lie-groups', 'lie-algebras']"
35,composition sum of functions/sum of composition of functions,composition sum of functions/sum of composition of functions,,"I know it sounds really dumb, but is it true that $(f_1+f_2)\circ g=f_1\circ g+f_2\circ g$? I know it must be really elementary, but I don't recall seeing this being proved (or defined) explicitly.","I know it sounds really dumb, but is it true that $(f_1+f_2)\circ g=f_1\circ g+f_2\circ g$? I know it must be really elementary, but I don't recall seeing this being proved (or defined) explicitly.",,"['linear-algebra', 'functions']"
36,Problem on matrices : $\dim E\leq n^2-(n-r)^2-1$,Problem on matrices :,\dim E\leq n^2-(n-r)^2-1,"I have the following problem : Let $E$ be a subspace of $M_n(\mathbb{R})$ that contains no invertible matrix. Let $r=\max\{rank(M)\mid M\in E\}$ Show that $\dim E\leq n^2-(n-r)^2-1$ I don't know how to do this. We have obviously $\dim E=n^2$, and I guess that he $(n-r)^2$ is due to the rank of the space with invertible matrixes, but I'm kind of lost here.","I have the following problem : Let $E$ be a subspace of $M_n(\mathbb{R})$ that contains no invertible matrix. Let $r=\max\{rank(M)\mid M\in E\}$ Show that $\dim E\leq n^2-(n-r)^2-1$ I don't know how to do this. We have obviously $\dim E=n^2$, and I guess that he $(n-r)^2$ is due to the rank of the space with invertible matrixes, but I'm kind of lost here.",,"['linear-algebra', 'matrices']"
37,Find the Basis and Dimension of a Solution Space for homogeneous systems,Find the Basis and Dimension of a Solution Space for homogeneous systems,,I have the following system of equations: $$\left\{\begin{array}{c} x+2y-2z+2s-t=0\\ x+2y-z+3s-2t=0\\ 2x+4y-7z+s+t=0 \end{array}\right.$$ Which forms the following matrix $$\left[\begin{array}{ccccc|c} 1 & 2 & -2 & 2 & -1 & 0\\ 1 & 2 & -1 & 3 & -2 & 0\\ 2 & 4 & -7 & 1 & 1 & 0 \end{array}\right]$$ Which I then row reduced to the following form: $$\left[\begin{array}{ccccc|c} 1 & 2 & 0 & 4 & -3 & 0\\ 0 & 0 & 1 & 1 & -1 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 \end{array}\right]$$ I am unsure from this point how to find the basis for the solution set.  Any help of direction would be appreciated.  I know that the dimension would be $n-r$ where $n$ is the number of unknowns and $r$ is the rank of the matrix but I do not know how to find the basis.,I have the following system of equations: $$\left\{\begin{array}{c} x+2y-2z+2s-t=0\\ x+2y-z+3s-2t=0\\ 2x+4y-7z+s+t=0 \end{array}\right.$$ Which forms the following matrix $$\left[\begin{array}{ccccc|c} 1 & 2 & -2 & 2 & -1 & 0\\ 1 & 2 & -1 & 3 & -2 & 0\\ 2 & 4 & -7 & 1 & 1 & 0 \end{array}\right]$$ Which I then row reduced to the following form: $$\left[\begin{array}{ccccc|c} 1 & 2 & 0 & 4 & -3 & 0\\ 0 & 0 & 1 & 1 & -1 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 \end{array}\right]$$ I am unsure from this point how to find the basis for the solution set.  Any help of direction would be appreciated.  I know that the dimension would be $n-r$ where $n$ is the number of unknowns and $r$ is the rank of the matrix but I do not know how to find the basis.,,"['linear-algebra', 'matrices', 'systems-of-equations']"
38,expansion of matrix inverse,expansion of matrix inverse,,"I would like to invert a square matrix $L$. One can write it as a sum of two matrices, one containing the diagonal terms ($D$) and the other the off-diagonal ones ($A$). $$L = D+A$$ I would like to know, under which conditions can one make the following expansion of the inverse of $L$: $$L^{-1} = (D+A)^{-1} = (D(I+D^{-1}A))^{-1} \approx (I-D^{-1}A)D^{-1} = D^{-1}-D^{-1}AD^{-1}$$ Rephrasing the question, I realise that this somehow implies that the terms in the  off-diagonal part $A$ must be ""small"", but I do not know how to write that condition formally. When is one allowed to make such an expansion and what would be the error?","I would like to invert a square matrix $L$. One can write it as a sum of two matrices, one containing the diagonal terms ($D$) and the other the off-diagonal ones ($A$). $$L = D+A$$ I would like to know, under which conditions can one make the following expansion of the inverse of $L$: $$L^{-1} = (D+A)^{-1} = (D(I+D^{-1}A))^{-1} \approx (I-D^{-1}A)D^{-1} = D^{-1}-D^{-1}AD^{-1}$$ Rephrasing the question, I realise that this somehow implies that the terms in the  off-diagonal part $A$ must be ""small"", but I do not know how to write that condition formally. When is one allowed to make such an expansion and what would be the error?",,"['linear-algebra', 'matrices', 'inverse']"
39,An interesting linear algebra question,An interesting linear algebra question,,"Let $A$ and $u$ be $n\times n$ matrix and $n\times 1$ vector of $\mathbb{C}$. Denote $\overline{A}$ is the matrix $(\overline{A})_{ij}=A_{ij}^*$, the conjugate number; ($\overline{A}$ is not the conjugate transpose matrix) and similarly $\overline{u}$. Prove that if $\lambda$ is a nonnegative eigenvalue of $A\overline{A}$, ie $\exists v\ne 0:A\overline{A}v=\lambda v$, then $\exists u\ne 0$ such that: $$A\overline{u}=\sqrt{\lambda}u$$","Let $A$ and $u$ be $n\times n$ matrix and $n\times 1$ vector of $\mathbb{C}$. Denote $\overline{A}$ is the matrix $(\overline{A})_{ij}=A_{ij}^*$, the conjugate number; ($\overline{A}$ is not the conjugate transpose matrix) and similarly $\overline{u}$. Prove that if $\lambda$ is a nonnegative eigenvalue of $A\overline{A}$, ie $\exists v\ne 0:A\overline{A}v=\lambda v$, then $\exists u\ne 0$ such that: $$A\overline{u}=\sqrt{\lambda}u$$",,"['linear-algebra', 'matrices']"
40,Minimum vector sets span spaces cover problem,Minimum vector sets span spaces cover problem,,"Instance: a set of vectors $V=\{\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\}$ and $m$ vector sets $V_1,V_2,...,V_m$, each of which contains multiple vectors ($V_i$ may not be a subset of $V$). In our problem, the elements of each vector are integer numbers. The linear span space of $\bigcup\limits_{i=1}^mV_{i}$ contains each vector in $V$. Question : find minimum number of sets from $\{V_1,V_2,...,V_m\}$, denoted as $\{V_{i_1},...V_{i_k}\}$, such that the linear span space of $\bigcup\limits_{j=1}^kV_{i_j}$ contains each vector in $V$. There may exist a reduction to our problem from SET COVER, which is NP-complete. I try to find an approximate algorithm to solve this problem with a provable approximation ratio . One prossible algorithm is that at each step, we can remove one subset $V_i$ from $\{V_1,V_2,...,V_m\}$ under the condition that the span space of union of remainning subsets contains V. For this algorithm, the key issue is how to select a ""good"" $V_i$ at each step to make the final result is near to the optimal result. This approach may have no provable approximation ratio. The worst case that can be used to verify the approximiation ratio of the approximation algorithm is shown as following: $V_1=\{[1,0,\cdots,0]\}$, $V_2=\{[0,1,0,\cdots,0]\}$,...,$V_m=\{[0,0,\cdots,0,1]\}$, in which each set contains one row vector of a $m\times m$ identity matrix. $V={[0,0,\cdots,0,1,1]}$. Therefore, the optimal solution is $\{V_{m-1},V_{m}\}$ and the minimum number is 2. If you have ideas and suggestions, it's pleasure if you can share with me. Thanks very much!","Instance: a set of vectors $V=\{\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\}$ and $m$ vector sets $V_1,V_2,...,V_m$, each of which contains multiple vectors ($V_i$ may not be a subset of $V$). In our problem, the elements of each vector are integer numbers. The linear span space of $\bigcup\limits_{i=1}^mV_{i}$ contains each vector in $V$. Question : find minimum number of sets from $\{V_1,V_2,...,V_m\}$, denoted as $\{V_{i_1},...V_{i_k}\}$, such that the linear span space of $\bigcup\limits_{j=1}^kV_{i_j}$ contains each vector in $V$. There may exist a reduction to our problem from SET COVER, which is NP-complete. I try to find an approximate algorithm to solve this problem with a provable approximation ratio . One prossible algorithm is that at each step, we can remove one subset $V_i$ from $\{V_1,V_2,...,V_m\}$ under the condition that the span space of union of remainning subsets contains V. For this algorithm, the key issue is how to select a ""good"" $V_i$ at each step to make the final result is near to the optimal result. This approach may have no provable approximation ratio. The worst case that can be used to verify the approximiation ratio of the approximation algorithm is shown as following: $V_1=\{[1,0,\cdots,0]\}$, $V_2=\{[0,1,0,\cdots,0]\}$,...,$V_m=\{[0,0,\cdots,0,1]\}$, in which each set contains one row vector of a $m\times m$ identity matrix. $V={[0,0,\cdots,0,1,1]}$. Therefore, the optimal solution is $\{V_{m-1},V_{m}\}$ and the minimum number is 2. If you have ideas and suggestions, it's pleasure if you can share with me. Thanks very much!",,"['linear-algebra', 'matrices', 'np-complete', 'matrix-rank']"
41,Special solutions to Ax = 0,Special solutions to Ax = 0,,"I solved most of it, just not sure about one point. The problem statement, all given variables and data Suppose A is the matrix shown below: $$         \begin{pmatrix}         0 & 1 & 2 & 2 \\         0 & 3 & 8 & 7 \\         0 & 0 & 4 & 2 \\         \end{pmatrix} $$ Find all special solutions to Ax = 0. Attempt at a solution So after some elimination, I acquired the matrix below. $$         \begin{pmatrix}         0 & 1 & 0 & 1 \\         0 & 0 & 1 & -1/2 \\         0 & 0 & 0 & 0 \\         \end{pmatrix} $$ According to what I read online, there must be special solutions as many as the number of free variables. You go through the free variables one by one, making one of them equal to 1 and the rest equal to 0. So my first free solution is below: Let $x_4 = 1$. $\Rightarrow x_1 = ?$ (there is no $x_1$), $x_2 = -1$, $x_3 = -\frac{1}{2}$ and $x_4 = 1$ Let $x_1 = 1$ (but there is no $x_1$) $\Rightarrow x_1 = ?$, $x_2 = 0$, $x_3 = 0$, $x_4 = 0$ I am not entirely sure if I concluded the answer correctly. I would appreciate if someone could wrap it up. If I don't have a variable in my system at all, like $x_1$ here, then what exactly do I put in its place? How do I represent it?","I solved most of it, just not sure about one point. The problem statement, all given variables and data Suppose A is the matrix shown below: $$         \begin{pmatrix}         0 & 1 & 2 & 2 \\         0 & 3 & 8 & 7 \\         0 & 0 & 4 & 2 \\         \end{pmatrix} $$ Find all special solutions to Ax = 0. Attempt at a solution So after some elimination, I acquired the matrix below. $$         \begin{pmatrix}         0 & 1 & 0 & 1 \\         0 & 0 & 1 & -1/2 \\         0 & 0 & 0 & 0 \\         \end{pmatrix} $$ According to what I read online, there must be special solutions as many as the number of free variables. You go through the free variables one by one, making one of them equal to 1 and the rest equal to 0. So my first free solution is below: Let $x_4 = 1$. $\Rightarrow x_1 = ?$ (there is no $x_1$), $x_2 = -1$, $x_3 = -\frac{1}{2}$ and $x_4 = 1$ Let $x_1 = 1$ (but there is no $x_1$) $\Rightarrow x_1 = ?$, $x_2 = 0$, $x_3 = 0$, $x_4 = 0$ I am not entirely sure if I concluded the answer correctly. I would appreciate if someone could wrap it up. If I don't have a variable in my system at all, like $x_1$ here, then what exactly do I put in its place? How do I represent it?",,"['linear-algebra', 'matrices']"
42,Smallest linear combination of a set of vectors,Smallest linear combination of a set of vectors,,"I'm searching for an algorithm to accomplish a (hopefully) simple task. If I have a set of vectors, e.g., $\left( \begin{bmatrix} 0\\ 2\end{bmatrix}, \begin{bmatrix} 1\\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \end{bmatrix}, \begin{bmatrix} 1\\-1\end{bmatrix} \right)$ , and an arbitrary coordinate $\begin{bmatrix} x\\ y \end{bmatrix}$ , I want to find the smallest integral linear combination of these vectors that will sum to the point. I can interpret the vectors as a set of linear equations: $$\begin{aligned} b + 2c + d &= x \\                  2a +  b - d &= y \end{aligned} $$ and  look for the solution such that $|a| + |b| + |c| + |d|$ is minimized, or determine that there is no solution (for $x=2,y=1$ there isn't one, in this example). My problem is that I often end up doing this by drawing out a grid and eyeballing the solution, which isn't very mathematical. My hope is to find an algorithm that will allow me to solve this problem with any set of vectors and any target coordinate, or determine that the solution does not exist. I don't have a lot of formal education in math, so searching for help on this is tricky for me. I don't have the vocabulary I need to correctly describe this to Google or Wikipedia. I thought it would be worthwhile to ask here and see what I need to learn to solve this problem.","I'm searching for an algorithm to accomplish a (hopefully) simple task. If I have a set of vectors, e.g., , and an arbitrary coordinate , I want to find the smallest integral linear combination of these vectors that will sum to the point. I can interpret the vectors as a set of linear equations: and  look for the solution such that is minimized, or determine that there is no solution (for there isn't one, in this example). My problem is that I often end up doing this by drawing out a grid and eyeballing the solution, which isn't very mathematical. My hope is to find an algorithm that will allow me to solve this problem with any set of vectors and any target coordinate, or determine that the solution does not exist. I don't have a lot of formal education in math, so searching for help on this is tricky for me. I don't have the vocabulary I need to correctly describe this to Google or Wikipedia. I thought it would be worthwhile to ask here and see what I need to learn to solve this problem.","\left( \begin{bmatrix} 0\\ 2\end{bmatrix}, \begin{bmatrix} 1\\ 1 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \end{bmatrix}, \begin{bmatrix} 1\\-1\end{bmatrix} \right) \begin{bmatrix} x\\ y \end{bmatrix} \begin{aligned} b + 2c + d &= x \\
                 2a +  b - d &= y \end{aligned}  |a| + |b| + |c| + |d| x=2,y=1","['linear-algebra', 'optimization', 'algorithms', 'integer-lattices']"
43,Double dot product in Cylindrical Polar coordinates - Strain energy,Double dot product in Cylindrical Polar coordinates - Strain energy,,"I'm working with a problem in linear elasticity, and I have to calculate the strain energy function as follows: $$ 2W = _{ij}_{ij} $$ Where  and  are symmetric rank 2 tensors. For cartesian coordinates it is really easy because the metric is just the identity matrix, hence: $$ 2W = _{xx}_{xx} + _{yy}_{yy} + _{zz}_{zz} + 2 _{xy}_{xy} + 2 _{xz}_{xz} + 2 _{zy}_{zy} $$ My question is how the expression should be for cylindrical polar coordinates $(r,,z)$ Many thanks!","I'm working with a problem in linear elasticity, and I have to calculate the strain energy function as follows: $$ 2W = _{ij}_{ij} $$ Where  and  are symmetric rank 2 tensors. For cartesian coordinates it is really easy because the metric is just the identity matrix, hence: $$ 2W = _{xx}_{xx} + _{yy}_{yy} + _{zz}_{zz} + 2 _{xy}_{xy} + 2 _{xz}_{xz} + 2 _{zy}_{zy} $$ My question is how the expression should be for cylindrical polar coordinates $(r,,z)$ Many thanks!",,"['linear-algebra', 'matrices', 'tensor-products', 'tensors', 'tensor-rank']"
44,Usual convex combination and the one with measure,Usual convex combination and the one with measure,,"Let $X$ be a Borel measurable subset of $\Bbb R^n$ and let $\nu$ be a probability measure on $X$. Can we always find an integer $m$, points  $x_1,\dots,x_m\in X$ and coefficients $a_1,\dots,a_m \geq 0$ such that $$   \sum_{i=1}^m a_i = 1,\qquad \sum_{i=1}^m a_ix_i = \int_X x \;\nu(\mathrm dx). $$ If not, is that true for closed or compact $X$?","Let $X$ be a Borel measurable subset of $\Bbb R^n$ and let $\nu$ be a probability measure on $X$. Can we always find an integer $m$, points  $x_1,\dots,x_m\in X$ and coefficients $a_1,\dots,a_m \geq 0$ such that $$   \sum_{i=1}^m a_i = 1,\qquad \sum_{i=1}^m a_ix_i = \int_X x \;\nu(\mathrm dx). $$ If not, is that true for closed or compact $X$?",,"['linear-algebra', 'functional-analysis', 'measure-theory', 'probability-theory']"
45,An example of space $V$ such that $(V^{\perp})^{\perp} \neq V$,An example of space  such that,V (V^{\perp})^{\perp} \neq V,"I know that if $W$ is a vector space of finite dimension then for any subspace $V$ ,$(V^{\perp})^{\perp} = V$. But I have heard that this is not true for infinite dimensional vector spaces. So I tried to construct a counter example but I could not get any. I tried the vector space formed by infinite tuples but whatever subspace I took it was satisfying $(V^{\perp})^{\perp} = V$. So if any one could give a counter example it would be great.Thanks.","I know that if $W$ is a vector space of finite dimension then for any subspace $V$ ,$(V^{\perp})^{\perp} = V$. But I have heard that this is not true for infinite dimensional vector spaces. So I tried to construct a counter example but I could not get any. I tried the vector space formed by infinite tuples but whatever subspace I took it was satisfying $(V^{\perp})^{\perp} = V$. So if any one could give a counter example it would be great.Thanks.",,"['linear-algebra', 'vector-spaces']"
46,Determinant of sum of orthogonal matrix with rank-$1$ matrix,Determinant of sum of orthogonal matrix with rank- matrix,1,"What is the determinant of the sum of two matrices $$\det (G + S)$$ where $S$ is all zeros except for a single column of $1$'s? $$S = \begin{bmatrix} 0 & ... & 0 & 1 & 0 & ... & 0 \\ 0 & ... & 0 & 1 & 0 & ... & 0 \\ \vdots & & \vdots & \vdots & \vdots &  & \vdots \\ 0 & ... & 0 & 1 & 0 & ... & 0 \\ \end{bmatrix}$$ I understand this can be solved by breaking up the determinant into columns, but I am unsure of how to do this. Also, $S$ is clearly singular - is there a general rule for the determinant of the sum of a singular and non-singular matrix (i.e. $G$ orthogonal $S$ singular)? Any help greatly appreciated Im essentially asking what happens to the determinant of a matrix when you add $1$ to each entry in a column. Specifically, I am interested in the case where $G$ is orthogonal with $\det(G) = -1$. I would also be interested in the case where we add $1$ to just a single entry.","What is the determinant of the sum of two matrices $$\det (G + S)$$ where $S$ is all zeros except for a single column of $1$'s? $$S = \begin{bmatrix} 0 & ... & 0 & 1 & 0 & ... & 0 \\ 0 & ... & 0 & 1 & 0 & ... & 0 \\ \vdots & & \vdots & \vdots & \vdots &  & \vdots \\ 0 & ... & 0 & 1 & 0 & ... & 0 \\ \end{bmatrix}$$ I understand this can be solved by breaking up the determinant into columns, but I am unsure of how to do this. Also, $S$ is clearly singular - is there a general rule for the determinant of the sum of a singular and non-singular matrix (i.e. $G$ orthogonal $S$ singular)? Any help greatly appreciated Im essentially asking what happens to the determinant of a matrix when you add $1$ to each entry in a column. Specifically, I am interested in the case where $G$ is orthogonal with $\det(G) = -1$. I would also be interested in the case where we add $1$ to just a single entry.",,"['linear-algebra', 'matrices', 'determinant']"
47,rank of quadrics,rank of quadrics,,"Consider the quadric $xw-yz$ in $\mathbf{P}^3$ (all over $\mathbf{C}$), and the Klein quadric $x_0 x_5+x_1 x_4+x_2 x_3$ in $\mathbf{P}^5$. I want to determine the rank of these quadrics. For the first I see that this quadric can be written as $s^T A s$ with $s=(x,y,z,w)$ and $A$ having entries $a_{14}=-a_{23}=-a_{32}=a_{41}=1/2$ and all other zero. From this I can conclude, since $\mathrm{rank} \ A=4$, that the first quadric has rank $4$ and similarily that the second one has rank $6$. So is this correct, both the result and the approach? Talking from experience, what would you say is the shortest way to determine the rank of a quadric? (I should note that I am mostly interested in a computational, i.e. linear algebra approach.)","Consider the quadric $xw-yz$ in $\mathbf{P}^3$ (all over $\mathbf{C}$), and the Klein quadric $x_0 x_5+x_1 x_4+x_2 x_3$ in $\mathbf{P}^5$. I want to determine the rank of these quadrics. For the first I see that this quadric can be written as $s^T A s$ with $s=(x,y,z,w)$ and $A$ having entries $a_{14}=-a_{23}=-a_{32}=a_{41}=1/2$ and all other zero. From this I can conclude, since $\mathrm{rank} \ A=4$, that the first quadric has rank $4$ and similarily that the second one has rank $6$. So is this correct, both the result and the approach? Talking from experience, what would you say is the shortest way to determine the rank of a quadric? (I should note that I am mostly interested in a computational, i.e. linear algebra approach.)",,"['linear-algebra', 'algebraic-geometry', 'quadratic-forms']"
48,Help required in finding determinant using characteristic equation.,Help required in finding determinant using characteristic equation.,,"A problem in my textbook proceeds as follows: Find $\det(A)$ given that $A$ has $p(\lambda)$ as its characteristic polynomial a) $p(\lambda)=\lambda^3-2\lambda^2+\lambda+5$ b) $p(\lambda)=\lambda^4-\lambda^3+7$ What  I did was: a) Since $\det(\lambda I-A)=\lambda^3-2\lambda^2+\lambda+5$, then $\det(-A)=5$. Hence, $\det(A)=-5$. b) Since $\det(\lambda I-A)=\lambda^4-2\lambda^3+7$, then $\det(-A)=7$. Hence, $\det(A)=7$. Am I correct here?","A problem in my textbook proceeds as follows: Find $\det(A)$ given that $A$ has $p(\lambda)$ as its characteristic polynomial a) $p(\lambda)=\lambda^3-2\lambda^2+\lambda+5$ b) $p(\lambda)=\lambda^4-\lambda^3+7$ What  I did was: a) Since $\det(\lambda I-A)=\lambda^3-2\lambda^2+\lambda+5$, then $\det(-A)=5$. Hence, $\det(A)=-5$. b) Since $\det(\lambda I-A)=\lambda^4-2\lambda^3+7$, then $\det(-A)=7$. Hence, $\det(A)=7$. Am I correct here?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
49,Levi-Civita connection compatible with Riemaniann and Pseudo-riemaniann metric,Levi-Civita connection compatible with Riemaniann and Pseudo-riemaniann metric,,"Given a Pseudo-riemaniann metric on ${\cal{M}}$, is it possible to find a Riemaniann metric  on ${\cal{M}}$ with the same Levi-Civita connection? If in general this is not possible, what sufficient or necessary conditions are needed for the Pseudo-Riemaniann metric and the holonomy ? If I understood correcly from this post . The question can be rephrase: What conditions an indefinite symmetric quadratic form that is left invariant by the holonomy must satisfy to allow a positive definite quadratic form to be invariant under the same holonomy?","Given a Pseudo-riemaniann metric on ${\cal{M}}$, is it possible to find a Riemaniann metric  on ${\cal{M}}$ with the same Levi-Civita connection? If in general this is not possible, what sufficient or necessary conditions are needed for the Pseudo-Riemaniann metric and the holonomy ? If I understood correcly from this post . The question can be rephrase: What conditions an indefinite symmetric quadratic form that is left invariant by the holonomy must satisfy to allow a positive definite quadratic form to be invariant under the same holonomy?",,"['linear-algebra', 'differential-geometry']"
50,Exponential of matrices and bounded operators,Exponential of matrices and bounded operators,,"Let $A$ be a complex $n \times n$ matrix, such that the function $t\mapsto e^{tA}x$ is bounded on $\mathbb{R}$ and nonzero, for some vector $x\in \mathbb{C}$. How can we prove that $\inf_{t\in \mathbb{R}}|e^{tA}x|>0$ or in other words $|e^{tA}x|\geq c>0$ for all $t\in \mathbb{R}$. I can only prove this in the case $A=a \in \mathbb{C}$, because $|e^{ta}x|=e^{Re(a)t}|x|$, and the boundedness implies that $Re(a)=0$, so $|e^{ta}x|=|x|=c>0$ because $t\mapsto e^{ta}x$ is supposed to be nonzero. I also want to know if we have this property in the case $A$ is a bounded operator on an infinite dimensional Banach space $X$.","Let $A$ be a complex $n \times n$ matrix, such that the function $t\mapsto e^{tA}x$ is bounded on $\mathbb{R}$ and nonzero, for some vector $x\in \mathbb{C}$. How can we prove that $\inf_{t\in \mathbb{R}}|e^{tA}x|>0$ or in other words $|e^{tA}x|\geq c>0$ for all $t\in \mathbb{R}$. I can only prove this in the case $A=a \in \mathbb{C}$, because $|e^{ta}x|=e^{Re(a)t}|x|$, and the boundedness implies that $Re(a)=0$, so $|e^{ta}x|=|x|=c>0$ because $t\mapsto e^{ta}x$ is supposed to be nonzero. I also want to know if we have this property in the case $A$ is a bounded operator on an infinite dimensional Banach space $X$.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'operator-theory', 'banach-spaces']"
51,Multicollinearity and SVD,Multicollinearity and SVD,,"I compute the Singular Value Decomposition of a $n \times n$ matrix. If the matrix is not full rank, and I have 2 collinear columns, I end up with one singular value equal to 0. Is it possible to find out which columns of the original matrix are collinear from the result of the SVD ? This is a simple example. I would then like, for a more complicated matrix where a group of columns are multicollinear, to identify this group. Therefore I want to know if the SVD can go beyond just knowing the number of multicollinear columns and identifying the multicollinear group of columns.","I compute the Singular Value Decomposition of a $n \times n$ matrix. If the matrix is not full rank, and I have 2 collinear columns, I end up with one singular value equal to 0. Is it possible to find out which columns of the original matrix are collinear from the result of the SVD ? This is a simple example. I would then like, for a more complicated matrix where a group of columns are multicollinear, to identify this group. Therefore I want to know if the SVD can go beyond just knowing the number of multicollinear columns and identifying the multicollinear group of columns.",,"['linear-algebra', 'regression', 'svd']"
52,Existence of a basis such that $\|e_i\|=1$ and $\|e_i^{*}\|_*=1$. (dual),Existence of a basis such that  and . (dual),\|e_i\|=1 \|e_i^{*}\|_*=1,"Let $E$ a $n$ -finite dimensional normed vector space. Can we find a basis $e_1,e_2,\cdots,e_n$ of $E$ such that $\|e_i\|=1$ and $\|e_i^{*}\|_*=1$ for all $i$ ? where $\|\|_*$ is the dual norm. I know that $n=\dim(E)=\dim(E^*)$ and In the case of finite-dimensional vector spaces, the dual set is always a dual basis. Furthermore, $$\|f\|_{*}\ = \sup_{x\in E-\{0\}}\frac{|f(x)|}{\|x\|}=\sup_{||x||=1}|f(x)|$$ Anyway, I don't see how can I tackle this exercise. Any help will be very grateful, Thank you in advance for your time.","Let a -finite dimensional normed vector space. Can we find a basis of such that and for all ? where is the dual norm. I know that and In the case of finite-dimensional vector spaces, the dual set is always a dual basis. Furthermore, Anyway, I don't see how can I tackle this exercise. Any help will be very grateful, Thank you in advance for your time.","E n e_1,e_2,\cdots,e_n E \|e_i\|=1 \|e_i^{*}\|_*=1 i \|\|_* n=\dim(E)=\dim(E^*) \|f\|_{*}\ = \sup_{x\in E-\{0\}}\frac{|f(x)|}{\|x\|}=\sup_{||x||=1}|f(x)|",[]
53,Calculate determinant of Vandermonde using specified steps.,Calculate determinant of Vandermonde using specified steps.,,"$V_n(a_1,a_2\dots, a_n)$ is a $n\times n$ Vandermonde matrix =   $$\left|\begin{array}[cccc]  11&a_1&\cdots&a^{n-1}_1\\ 1&a_2&\cdots&a^{n-1}_2\\ 1&a_3&\cdots&a^{n-1}_3\\ \vdots&\vdots&\ddots&\vdots\\ 1&a_n&\cdots&a_n^{n-1} \end{array}\right|$$ Replace $a_1$ by $x$ so that the first row is $1,x, \dots ,x^{n-1}$ $$V_n(x,a_2\dots, a_n) = \left|\begin{array}[cccc]  11&x&\cdots&x^{n-1}\\ 1&a_2&\cdots&a^{n-1}_2\\ 1&a_3&\cdots&a^{n-1}_3\\ \vdots&\vdots&\ddots&\vdots\\ 1&a_n&\cdots&a_n^{n-1} \end{array}\right|$$ Let $P(x) = V_n(x,a_2\dots, a_n)$. (a) Show that $P(x)$ is a polynomial in $x$ of degree $\leq n-1$. $z = 1\det() -x\det() + x^2\det() -\cdots+ (-1)^{n-1}x^{n-1} \cdot \det()$, where each $\det()$ stands for the determinant of a smaller matrix after removing the appropriate columns, rows. Is this right? But isn't $z$ a polynomial of degree $n$? (b) Show that $P(x)$ has $n-1$ distinct roots $a_2, \dots a_n$ and therefore has factorization $P(x) = k \prod_{i=2}^n(x-a_i)$ where the constant factor $k$ is the coefficient of $x^{n-1}.$ I'll be honest, I have no clue how to do this. Not even sure where to start. Nor do I know where to start on the rest. (c) Show that $k = (-1)^{n-1}V_{n-1}(a_2,\dots a_n)$. (d) Use parts (b) and (c) to deduce the recursion formula $$V_n(a_1, \dots a_n) = \left(\prod_{i=2}^n(a_i-a_1)\right)V_{n-1}(a_2,\dots a_n)$$ (e) Use part (d) to deduce that $V_n(a_1,a_2,\dots a_n) = \prod^n_{1\leq i< j\leq n} (a_j - a_i)$","$V_n(a_1,a_2\dots, a_n)$ is a $n\times n$ Vandermonde matrix =   $$\left|\begin{array}[cccc]  11&a_1&\cdots&a^{n-1}_1\\ 1&a_2&\cdots&a^{n-1}_2\\ 1&a_3&\cdots&a^{n-1}_3\\ \vdots&\vdots&\ddots&\vdots\\ 1&a_n&\cdots&a_n^{n-1} \end{array}\right|$$ Replace $a_1$ by $x$ so that the first row is $1,x, \dots ,x^{n-1}$ $$V_n(x,a_2\dots, a_n) = \left|\begin{array}[cccc]  11&x&\cdots&x^{n-1}\\ 1&a_2&\cdots&a^{n-1}_2\\ 1&a_3&\cdots&a^{n-1}_3\\ \vdots&\vdots&\ddots&\vdots\\ 1&a_n&\cdots&a_n^{n-1} \end{array}\right|$$ Let $P(x) = V_n(x,a_2\dots, a_n)$. (a) Show that $P(x)$ is a polynomial in $x$ of degree $\leq n-1$. $z = 1\det() -x\det() + x^2\det() -\cdots+ (-1)^{n-1}x^{n-1} \cdot \det()$, where each $\det()$ stands for the determinant of a smaller matrix after removing the appropriate columns, rows. Is this right? But isn't $z$ a polynomial of degree $n$? (b) Show that $P(x)$ has $n-1$ distinct roots $a_2, \dots a_n$ and therefore has factorization $P(x) = k \prod_{i=2}^n(x-a_i)$ where the constant factor $k$ is the coefficient of $x^{n-1}.$ I'll be honest, I have no clue how to do this. Not even sure where to start. Nor do I know where to start on the rest. (c) Show that $k = (-1)^{n-1}V_{n-1}(a_2,\dots a_n)$. (d) Use parts (b) and (c) to deduce the recursion formula $$V_n(a_1, \dots a_n) = \left(\prod_{i=2}^n(a_i-a_1)\right)V_{n-1}(a_2,\dots a_n)$$ (e) Use part (d) to deduce that $V_n(a_1,a_2,\dots a_n) = \prod^n_{1\leq i< j\leq n} (a_j - a_i)$",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
54,Prove that $\det(A)=\det(A^T)$ algebraically,Prove that  algebraically,\det(A)=\det(A^T),"If we use row operations to turn matrix $A$ into an upper triangular matrix then the $\det(A)$ is equal to the product of the entries on its main diagonal. So if we transpose $A$, then those row operations can be made column operations and we would have the same upper triangular matrix where $\det(A^T)$ is equal to the product of the entries on its main diagonal. So $\det(A)=\det(A^T)$. Is there an algebraic way to show this.","If we use row operations to turn matrix $A$ into an upper triangular matrix then the $\det(A)$ is equal to the product of the entries on its main diagonal. So if we transpose $A$, then those row operations can be made column operations and we would have the same upper triangular matrix where $\det(A^T)$ is equal to the product of the entries on its main diagonal. So $\det(A)=\det(A^T)$. Is there an algebraic way to show this.",,"['linear-algebra', 'determinant']"
55,Are positive semi-definite matrices always covariance matrices?,Are positive semi-definite matrices always covariance matrices?,,"This may be trivial. While covariance matrices of random variables are positive semi-definite, does the converse hold true as well, that positive semi-definite matrices are also valid covariance matrices of random variables? Wikipedia says this is the case, however, I don't follow the argument: ... the covariance matrix of a multivariate probability distribution is always positive semi-definite. (...) Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution. Grateful for any explanations.","This may be trivial. While covariance matrices of random variables are positive semi-definite, does the converse hold true as well, that positive semi-definite matrices are also valid covariance matrices of random variables? Wikipedia says this is the case, however, I don't follow the argument: ... the covariance matrix of a multivariate probability distribution is always positive semi-definite. (...) Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution. Grateful for any explanations.",,"['linear-algebra', 'probability', 'matrices']"
56,"Let $U,V$ be subspaces of the vector space $W$. Show that if $U\nsubseteq V$ and $V\nsubseteq U$ then $U \cup V$ is not a subspace.",Let  be subspaces of the vector space . Show that if  and  then  is not a subspace.,"U,V W U\nsubseteq V V\nsubseteq U U \cup V","Let $U,V$ be subspaces of the vector space $W$.  Show that if $U\nsubseteq V$ and $V\nsubseteq U$ then $U \cup V$ is not a subspace. I know that in order to be considered a subspace, the matrix addition and scalar multiplication operations must hold.  However, I can define: $U = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $V = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$ They are not a subset of each other, but their union is a subspace.  Adding a matrix that spans $U\cup V$ or performing scalar multiplication seems valid in my constructed $U \cup V$.  Clearly I'm missing something important. Can anyone shed some light on this?","Let $U,V$ be subspaces of the vector space $W$.  Show that if $U\nsubseteq V$ and $V\nsubseteq U$ then $U \cup V$ is not a subspace. I know that in order to be considered a subspace, the matrix addition and scalar multiplication operations must hold.  However, I can define: $U = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $V = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$ They are not a subset of each other, but their union is a subspace.  Adding a matrix that spans $U\cup V$ or performing scalar multiplication seems valid in my constructed $U \cup V$.  Clearly I'm missing something important. Can anyone shed some light on this?",,['linear-algebra']
57,Proving that a set is/is not semilinear,Proving that a set is/is not semilinear,,"A subset $X$ of $\mathbb{N}^n$ is linear if it is in the form: $u_0 + \langle u_1,...,u_m \rangle = \{ u_0 + t_1 u_1 + ... + t_m u_m \mid t_1,...,t_n \in \mathbb{N}\}$ for some $u_0,...,u_m \in \mathbb{N}^n$ $X$ is semilinear if it is the union of finitely many linear subsets. What are the techniques used to prove that a set is not semilinear (or is semilinear)? For example I would like to know/prove if the following subset is not semilinear: $X = \{ \langle x, y_1, y_2, z_1, z_2, w \rangle \}$ in which: ($x \geq y_1+y_2 +z_1+z_2+w$) OR ($w \geq x + y_1+y_2 +z_1+z_2$) OR ($x+y_1 = y_2+z_1 +z_2+w$ AND $x\neq y_2$ AND $y_1 \neq w$) OR ($x + y_1 + y_2 + z_1 = z_2 +w$ AND $x \neq z_2$ AND $z_1 \neq w$) Any suggestions?","A subset $X$ of $\mathbb{N}^n$ is linear if it is in the form: $u_0 + \langle u_1,...,u_m \rangle = \{ u_0 + t_1 u_1 + ... + t_m u_m \mid t_1,...,t_n \in \mathbb{N}\}$ for some $u_0,...,u_m \in \mathbb{N}^n$ $X$ is semilinear if it is the union of finitely many linear subsets. What are the techniques used to prove that a set is not semilinear (or is semilinear)? For example I would like to know/prove if the following subset is not semilinear: $X = \{ \langle x, y_1, y_2, z_1, z_2, w \rangle \}$ in which: ($x \geq y_1+y_2 +z_1+z_2+w$) OR ($w \geq x + y_1+y_2 +z_1+z_2$) OR ($x+y_1 = y_2+z_1 +z_2+w$ AND $x\neq y_2$ AND $y_1 \neq w$) OR ($x + y_1 + y_2 + z_1 = z_2 +w$ AND $x \neq z_2$ AND $z_1 \neq w$) Any suggestions?",,['linear-algebra']
58,The group of invertible matrices with entries in Z,The group of invertible matrices with entries in Z,,"Let $G=GL_{n}(\mathbb{Z})$ , the group of invertible matrices with entries in $\mathbb{Z}$ . Then show that $G=\{A\in M_n(\mathbb{Z}) : det(A)=1 \,\,\,or -1\}$ Can you help me please?","Let , the group of invertible matrices with entries in . Then show that Can you help me please?","G=GL_{n}(\mathbb{Z}) \mathbb{Z} G=\{A\in M_n(\mathbb{Z}) : det(A)=1 \,\,\,or -1\}",['linear-algebra']
59,Nilpotent matrix and basis for $F^n$,Nilpotent matrix and basis for,F^n,"Let $A \in Mat_{n \times n}(F)$ a matrix which satisfies $A^n=0$ and $A^{n-1} \ne 0$ for some positive integer $n$. Let $v \in F^n$ be such that $A^{n-1}v \ne 0$.  Prove that $\{v, Av, A^2v,...,A^{n-1}v\}$ is a basis for $F^n$. So, from definition of $A$, I know that $A^iv \ne 0$ for all $0\le i \le n-1$. I know I need to prove the set is linearly independent and span $F^n$ to prove it's a basis, but I just don't know how.","Let $A \in Mat_{n \times n}(F)$ a matrix which satisfies $A^n=0$ and $A^{n-1} \ne 0$ for some positive integer $n$. Let $v \in F^n$ be such that $A^{n-1}v \ne 0$.  Prove that $\{v, Av, A^2v,...,A^{n-1}v\}$ is a basis for $F^n$. So, from definition of $A$, I know that $A^iv \ne 0$ for all $0\le i \le n-1$. I know I need to prove the set is linearly independent and span $F^n$ to prove it's a basis, but I just don't know how.",,"['linear-algebra', 'matrices']"
60,How do i prove that every linear operator between finite-dimensional Hilbert spaces is bounded?,How do i prove that every linear operator between finite-dimensional Hilbert spaces is bounded?,,"When I learned basic linear-algebra, ""adjoint"" was only defined for linear operator between finite-dimensional inner product spaces. Right now, I'm studying Hilbert spaces and I want the past definition consistent with a new definition. I have proved following theorem in basic linear-algebra: Let $V,W$ be inner product spaces over $\mathbb{F}$ . Let $T:V\rightarrow W$ be a linear operator. If $V$ is finite-dimensional, there exists a unique function $T^*$ such that $\langle T(x),y\rangle=\langle x,T^*(y)\rangle$ . So my question is; How do I prove that $T$ is bounded when $V$ and $W$ are finite-dimensional? Moreover, is it true when $V$ is finite-dimensional but $W$ is not?","When I learned basic linear-algebra, ""adjoint"" was only defined for linear operator between finite-dimensional inner product spaces. Right now, I'm studying Hilbert spaces and I want the past definition consistent with a new definition. I have proved following theorem in basic linear-algebra: Let be inner product spaces over . Let be a linear operator. If is finite-dimensional, there exists a unique function such that . So my question is; How do I prove that is bounded when and are finite-dimensional? Moreover, is it true when is finite-dimensional but is not?","V,W \mathbb{F} T:V\rightarrow W V T^* \langle T(x),y\rangle=\langle x,T^*(y)\rangle T V W V W",['linear-algebra']
61,Is the matrix $R$ in the $QR$ decomposition unique?,Is the matrix  in the  decomposition unique?,R QR,"I'd like to know for positive diagonal elements why is $R$  in $QR$ decomposition unique. My guess is it must have something to do with linearly independence of the column of $R$, but then I can think of a property that lead me to uniqueness of $R$.","I'd like to know for positive diagonal elements why is $R$  in $QR$ decomposition unique. My guess is it must have something to do with linearly independence of the column of $R$, but then I can think of a property that lead me to uniqueness of $R$.",,['linear-algebra']
62,Semigroup implies exponential,Semigroup implies exponential,,"Let $S :[0,1]\to \mathbb R^{n\times n}$ be a continuous function which satisfies $$ S(0)=I \quad\text{and}\quad S(s+t)=S(s)S(t), $$ for all $s,t\in[0,1]$ with $s+t\in[0,1]$. (Here $I$ is the identity matrix in $\mathbb R^{n\times n}$). Show that there exists a matrix $A\in\mathbb R^{n\times n}$, such that $S(t)=\mathrm{e}^{tA}$. Comment. Once we show that $S$ is differentiable at $0$, then setting $A=S'(0)$, it is not hard to prove the above, as we would then have $S'(t)=AS(t),\,\,S(0)=I$, which is a system of ODEs with unique solution the exponential.","Let $S :[0,1]\to \mathbb R^{n\times n}$ be a continuous function which satisfies $$ S(0)=I \quad\text{and}\quad S(s+t)=S(s)S(t), $$ for all $s,t\in[0,1]$ with $s+t\in[0,1]$. (Here $I$ is the identity matrix in $\mathbb R^{n\times n}$). Show that there exists a matrix $A\in\mathbb R^{n\times n}$, such that $S(t)=\mathrm{e}^{tA}$. Comment. Once we show that $S$ is differentiable at $0$, then setting $A=S'(0)$, it is not hard to prove the above, as we would then have $S'(t)=AS(t),\,\,S(0)=I$, which is a system of ODEs with unique solution the exponential.",,"['linear-algebra', 'ordinary-differential-equations', 'matrix-equations']"
63,"Finding the basis, difference between row space and column space","Finding the basis, difference between row space and column space",,"I'm confused in Linear Algebra when finding the basis. In my textbook there are two methods: Row space and Casting out In the Row Space algorithm I form the Matrix whose rows are the given vectors, then I reduce it to echelon and my basis are the non zero rows: In the Casting out method, I basically form the matrix whose columns are the given vectors.  Reduce to echelon and my entries with pivots form the basis. I would really appreciate if someone could tell me the difference between using the column and row interpretation (this part really blocks me in the subject), is it basically the same (the difference being how to interpret the echeloned matrix? If yes what is the point of having two distinct methods. And when do I know which one I need to use.","I'm confused in Linear Algebra when finding the basis. In my textbook there are two methods: Row space and Casting out In the Row Space algorithm I form the Matrix whose rows are the given vectors, then I reduce it to echelon and my basis are the non zero rows: In the Casting out method, I basically form the matrix whose columns are the given vectors.  Reduce to echelon and my entries with pivots form the basis. I would really appreciate if someone could tell me the difference between using the column and row interpretation (this part really blocks me in the subject), is it basically the same (the difference being how to interpret the echeloned matrix? If yes what is the point of having two distinct methods. And when do I know which one I need to use.",,['linear-algebra']
64,Generator of End(V),Generator of End(V),,"If V is a finite-dimensional vector space of dimension n and GEnd(V) such that G generates End(V) meaning that any element of End(V) is expressible as a linear combination of products of a number of elements of G, what is the minimal possible number of elements in G?","If V is a finite-dimensional vector space of dimension n and GEnd(V) such that G generates End(V) meaning that any element of End(V) is expressible as a linear combination of products of a number of elements of G, what is the minimal possible number of elements in G?",,"['linear-algebra', 'group-theory', 'vector-spaces']"
65,Matrices such that $A^2=A$ and $B^2=B$,Matrices such that  and,A^2=A B^2=B,"Let $A,B$ be two matrices of $M(n,\mathbb{R})$ such that $$A^2=A\quad\text{and}\quad B^2=B$$ Then $A$ and $B$ are similar if and only if $\operatorname{rk}A = \operatorname{rk}B$. The first implication is pretty easy because the rank is an invariant under matrix similarity. But the second one is kind of baffling me. I thought of reasoning about linear mappings instead of matrices. My reasoning was, basically, that if we consider the matrix as a linear mapping with respect to the canonical basis ($T(v)$ for the matrix $A$, $L(v)$ for the matrix $B$) then we have $$T(T(v))=T(v)\quad\text{and}\quad L(L(v))=L(v)$$ for all $v \in V$. Then the mapping must be either the $0$ function or the identity function (if this was the case, then the rest of the demonstration would be easy). But I soon realised that equating the arguments of the function, in general, doesn't work. Thanks in advance for your help.","Let $A,B$ be two matrices of $M(n,\mathbb{R})$ such that $$A^2=A\quad\text{and}\quad B^2=B$$ Then $A$ and $B$ are similar if and only if $\operatorname{rk}A = \operatorname{rk}B$. The first implication is pretty easy because the rank is an invariant under matrix similarity. But the second one is kind of baffling me. I thought of reasoning about linear mappings instead of matrices. My reasoning was, basically, that if we consider the matrix as a linear mapping with respect to the canonical basis ($T(v)$ for the matrix $A$, $L(v)$ for the matrix $B$) then we have $$T(T(v))=T(v)\quad\text{and}\quad L(L(v))=L(v)$$ for all $v \in V$. Then the mapping must be either the $0$ function or the identity function (if this was the case, then the rest of the demonstration would be easy). But I soon realised that equating the arguments of the function, in general, doesn't work. Thanks in advance for your help.",,"['linear-algebra', 'matrices']"
66,"Show that $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ form a basis of $\mathbb{R}^3$ iff $x\neq y, x \neq z, y \neq z$",Show that  form a basis of  iff,"(1,x,x^2),(1,y,y^2),(1,z,z^2) \mathbb{R}^3 x\neq y, x \neq z, y \neq z","I'm having some trouble with this one because I always get negated statements. If I try to prove both direction directly I get that three elements are all not equal to each other and the three vectors form a basis, or if I proof by contrapositive I get the statement that the vectors do not from a basis and therefore do not span $\mathbb{R}^3$ and are not linearly independent. Either way I end up with negated statements, which I find hard to use in proofs. What I have come up with thus far: $=>$ Proof by contraposition: Suppose $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ form a basis of $\mathbb{R}^3$ and assume that $x=y \vee x=z \vee y=z$.  In each case it follows that at least two of the three vectors are equal and thus all three vectors are not linearly independent, therefore $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ cannot be a basis of $\mathbb{R}^3$. $<=$ Proof by contraposition: Suppose $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ are not a basis of $\mathbb{R}^3$, then the vectors are not linearly independent or do not span $\mathbb{R}^3$. Case 1: One of the vectors is in the span of the other two. Then it follows that there is a linear combination of the following form : $1x + 0y = z$, where x is the vector $(1,x,x^2)$ and y and z the corresponding other two vectors. Then it follows that $(1,x,x^2) = (1,z,z^2)$ and therefore $x=z$ with $x,z \in \mathbb{R}$. The same result would hold in the other cases. Case 2: No idea. Honestly I don't really feel as if those proofs are correct. For the second direction in case 1, I'm not sure whether the fact that the vectors are not linearly independent, implies the fact that there is a linear combination of the form $1x + 0y = z$. Can anybody tell me what I have done wrong thus far and give some hints to help me prove this?","I'm having some trouble with this one because I always get negated statements. If I try to prove both direction directly I get that three elements are all not equal to each other and the three vectors form a basis, or if I proof by contrapositive I get the statement that the vectors do not from a basis and therefore do not span $\mathbb{R}^3$ and are not linearly independent. Either way I end up with negated statements, which I find hard to use in proofs. What I have come up with thus far: $=>$ Proof by contraposition: Suppose $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ form a basis of $\mathbb{R}^3$ and assume that $x=y \vee x=z \vee y=z$.  In each case it follows that at least two of the three vectors are equal and thus all three vectors are not linearly independent, therefore $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ cannot be a basis of $\mathbb{R}^3$. $<=$ Proof by contraposition: Suppose $(1,x,x^2),(1,y,y^2),(1,z,z^2)$ are not a basis of $\mathbb{R}^3$, then the vectors are not linearly independent or do not span $\mathbb{R}^3$. Case 1: One of the vectors is in the span of the other two. Then it follows that there is a linear combination of the following form : $1x + 0y = z$, where x is the vector $(1,x,x^2)$ and y and z the corresponding other two vectors. Then it follows that $(1,x,x^2) = (1,z,z^2)$ and therefore $x=z$ with $x,z \in \mathbb{R}$. The same result would hold in the other cases. Case 2: No idea. Honestly I don't really feel as if those proofs are correct. For the second direction in case 1, I'm not sure whether the fact that the vectors are not linearly independent, implies the fact that there is a linear combination of the form $1x + 0y = z$. Can anybody tell me what I have done wrong thus far and give some hints to help me prove this?",,['linear-algebra']
67,similar matrices over $\mathbb{Z}$,similar matrices over,\mathbb{Z},"Let $A,B$ be $2\times 2$ matrices over $\mathbb{Z}$. Suppose $x^2+x+1$ is the characteristic polynomial for both $A,B$. Determine whether $A,B$ are similar to each other over $\mathbb{Z}$. How to solve?","Let $A,B$ be $2\times 2$ matrices over $\mathbb{Z}$. Suppose $x^2+x+1$ is the characteristic polynomial for both $A,B$. Determine whether $A,B$ are similar to each other over $\mathbb{Z}$. How to solve?",,"['linear-algebra', 'matrices', 'number-theory', 'ring-theory', 'modules']"
68,Why do XOR and other operators on binary variables qualify as linear?,Why do XOR and other operators on binary variables qualify as linear?,,"I never fully understood why is the operation $\oplus: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ considered linear ? I am well aware of the definition of linearity on real numbers, and I understand the intuition behind this fact, but could one give me a sketch of proof why this is the case ? Is it also the case for other operations on binary data such as: bitwise-OR $\vee: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ truncated addition: $+: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ (i.e., $a+b \mod 2^n$) Hamming-weight: $H: \{0,1\}^n \mapsto \{0,\dots, n\}$","I never fully understood why is the operation $\oplus: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ considered linear ? I am well aware of the definition of linearity on real numbers, and I understand the intuition behind this fact, but could one give me a sketch of proof why this is the case ? Is it also the case for other operations on binary data such as: bitwise-OR $\vee: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ truncated addition: $+: \{0,1\}^n \times \{0,1\}^n \mapsto \{0,1\}^n$ (i.e., $a+b \mod 2^n$) Hamming-weight: $H: \{0,1\}^n \mapsto \{0,\dots, n\}$",,"['linear-algebra', 'binary', 'binary-operations']"
69,QR factorization of a special structured matrix,QR factorization of a special structured matrix,,"A friend asked me the following interesting question: Let $$A = \begin{bmatrix} R \\ \xi{\rm I} \end{bmatrix},$$ where $R \in \mathbb{R}^{n \times n}$ is an upper triangular and ${\rm I}$ is an identity matrix, both of order $n$ , and $\xi \in \mathbb{R}$ is a scalar. Is there an efficient way to compute a QR factorization of $A$ ? I have found this question with a very nice answer, but I'd like to avoid doing the SVD because it is computationally expensive and my $R$ is not a constant like $W$ in that other question. Also, my $R$ is already triangular, which I hope can somehow be used. Edit: There was a comment (turned into an answer while I was writing this edit) on using Givens rotations. Since this is a logical first idea, I'd like to explain why I don't like it. We could use Givens rotations to cancel out the elements of $\xi{\rm I}$ , but each Givens rotation is computing two linear combinations of two rows. That means that if I cancel out the first element of $\xi{\rm I}$ , I will also introduce a bunch of non-zeros to the rest of that row. This means that I would need to go through the whole upper triangle of the bottom block, same as I'd have to do if $\xi{\rm I}$ was a general upper triangular matrix. Given that it is a diagonal matrix (with all its diagonal elements being the same, although I suspect this doesn't help much), I am hoping to get more efficient than that.","A friend asked me the following interesting question: Let where is an upper triangular and is an identity matrix, both of order , and is a scalar. Is there an efficient way to compute a QR factorization of ? I have found this question with a very nice answer, but I'd like to avoid doing the SVD because it is computationally expensive and my is not a constant like in that other question. Also, my is already triangular, which I hope can somehow be used. Edit: There was a comment (turned into an answer while I was writing this edit) on using Givens rotations. Since this is a logical first idea, I'd like to explain why I don't like it. We could use Givens rotations to cancel out the elements of , but each Givens rotation is computing two linear combinations of two rows. That means that if I cancel out the first element of , I will also introduce a bunch of non-zeros to the rest of that row. This means that I would need to go through the whole upper triangle of the bottom block, same as I'd have to do if was a general upper triangular matrix. Given that it is a diagonal matrix (with all its diagonal elements being the same, although I suspect this doesn't help much), I am hoping to get more efficient than that.","A = \begin{bmatrix} R \\ \xi{\rm I} \end{bmatrix}, R \in \mathbb{R}^{n \times n} {\rm I} n \xi \in \mathbb{R} A R W R \xi{\rm I} \xi{\rm I} \xi{\rm I}","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
70,"topology on $\hom_{\mathbb C}(V,W)$",topology on,"\hom_{\mathbb C}(V,W)","Let's $V,W$ be finite-dimensional complex vector spaces. How to define topology on $\hom_{\mathbb C}(V,W)$? As I see we can define metric in this $(\dim_{\mathbb C}V\times\dim_{\mathbb C}W)-$dimensional vector space and induced topology from it. Is there any way to define topology in terms of topologies on $V$ and $W$? Or more ""canonical"" way, then by induction from metric? How much more complicated situation if I don't require finiteness of $\dim$'s? UPD: why the space  of all isomorphisms is open in $\hom_{\mathbb C}(V,W)$? Thanks a lot!","Let's $V,W$ be finite-dimensional complex vector spaces. How to define topology on $\hom_{\mathbb C}(V,W)$? As I see we can define metric in this $(\dim_{\mathbb C}V\times\dim_{\mathbb C}W)-$dimensional vector space and induced topology from it. Is there any way to define topology in terms of topologies on $V$ and $W$? Or more ""canonical"" way, then by induction from metric? How much more complicated situation if I don't require finiteness of $\dim$'s? UPD: why the space  of all isomorphisms is open in $\hom_{\mathbb C}(V,W)$? Thanks a lot!",,"['linear-algebra', 'general-topology', 'functional-analysis']"
71,Jordan Normal Form and eigenvalue 0,Jordan Normal Form and eigenvalue 0,,"I understand the processes of putting a matrix into Jordan normal form and forming the transformation matrix associated to ""diagonalizing"" the matrix. So here's my question: Why is it that when you have an eigenvalue x=0 with algebraic multiplicity greater than 1, that you don't put a 1 in the superdiagonal of the JNF matrix but when the eigenvalue is non-zero and satisfies the same properties, we put a 1 in the superdiagonal of the Jordan normal form? My professor posted solutions to an assignment involving finding a matrix exponential, but the JNF of a matrix had eigenvalue x=0 with algebraic multiplicity of 3,yet had no entries of 1 along the superdiagonal. In advance, I would like to thank you for your help.","I understand the processes of putting a matrix into Jordan normal form and forming the transformation matrix associated to ""diagonalizing"" the matrix. So here's my question: Why is it that when you have an eigenvalue x=0 with algebraic multiplicity greater than 1, that you don't put a 1 in the superdiagonal of the JNF matrix but when the eigenvalue is non-zero and satisfies the same properties, we put a 1 in the superdiagonal of the Jordan normal form? My professor posted solutions to an assignment involving finding a matrix exponential, but the JNF of a matrix had eigenvalue x=0 with algebraic multiplicity of 3,yet had no entries of 1 along the superdiagonal. In advance, I would like to thank you for your help.",,"['linear-algebra', 'ordinary-differential-equations', 'soft-question', 'jordan-normal-form']"
72,Why least square problem always has solution for arbitrary b? [duplicate],Why least square problem always has solution for arbitrary b? [duplicate],,"This question already has answers here : Existence of least squares solution to $Ax=b$ (7 answers) Closed 10 years ago . We know from linear algebra, the least square solution of linear equation system : $$Ax=b$$ always exists. That is, the equation  $$A^TAx=A^Tb$$ always has at least one solution. Most of us explain it existence by interpreting least square problem as a matter of orthogonal projection. It's correct, I know. But can we prove this by showing that the rank of matrix $$A^TA$$ is equal to the rank of the augmented matrix:$$[A^TA,A^Tb]$$for any given vector b? I've worked for hours but failed to show that. Although there is a similar thread in mathematics.SX, but I personally think there is no good answer presented there. So I address this problem again. Thanks at first! [update] Now I come up with a proof for this proposition: suppose that rank $A^TA$=k, and the matrix $[A^TA,A^Tb]$ has one more column than $[A^TA]$, so rank $[A^TA,A^Tb]\ge k$. But on the other hand, we have $[A^TA,A^Tb]=A^T[A,b]$, using the rank inequality, we have $rank [A^TA,A^Tb]\le rank A^T$. Since (we can prove that) $ rank A^TA= rank A= rank A^T$ for any given matrix A. So the inequality is actually $rank [A^TA,A^Tb]\le k$. Combining the two inequalities, we have $rank [A^TA,A^Tb]=rank A^TA$. Q.E.D.","This question already has answers here : Existence of least squares solution to $Ax=b$ (7 answers) Closed 10 years ago . We know from linear algebra, the least square solution of linear equation system : $$Ax=b$$ always exists. That is, the equation  $$A^TAx=A^Tb$$ always has at least one solution. Most of us explain it existence by interpreting least square problem as a matter of orthogonal projection. It's correct, I know. But can we prove this by showing that the rank of matrix $$A^TA$$ is equal to the rank of the augmented matrix:$$[A^TA,A^Tb]$$for any given vector b? I've worked for hours but failed to show that. Although there is a similar thread in mathematics.SX, but I personally think there is no good answer presented there. So I address this problem again. Thanks at first! [update] Now I come up with a proof for this proposition: suppose that rank $A^TA$=k, and the matrix $[A^TA,A^Tb]$ has one more column than $[A^TA]$, so rank $[A^TA,A^Tb]\ge k$. But on the other hand, we have $[A^TA,A^Tb]=A^T[A,b]$, using the rank inequality, we have $rank [A^TA,A^Tb]\le rank A^T$. Since (we can prove that) $ rank A^TA= rank A= rank A^T$ for any given matrix A. So the inequality is actually $rank [A^TA,A^Tb]\le k$. Combining the two inequalities, we have $rank [A^TA,A^Tb]=rank A^TA$. Q.E.D.",,"['linear-algebra', 'matrices', 'matrix-equations']"
73,Is there an algebraic method for hyperbolic rotations?,Is there an algebraic method for hyperbolic rotations?,,"Given a 2d vector, how do you rotate it in space? You could use a rotation matrix, $$\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta &\cos\theta \end{bmatrix} \begin{bmatrix} x\\y\end{bmatrix} $$ or you could represent it as a complex number, and multiply it by a complex exponential. $$x'+iy'=e^{i\theta}(x+iy) =(\cos\theta+i\sin\theta)(x+iy)$$ For a hyperbolic rotation $$\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix} \cosh\theta & \sinh\theta \\ \sinh\theta &\cosh\theta \end{bmatrix} \begin{bmatrix} x\\y\end{bmatrix} $$ is there an algebraic method of representing this transformation, in analogy to the complex numbers?","Given a 2d vector, how do you rotate it in space? You could use a rotation matrix, $$\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta &\cos\theta \end{bmatrix} \begin{bmatrix} x\\y\end{bmatrix} $$ or you could represent it as a complex number, and multiply it by a complex exponential. $$x'+iy'=e^{i\theta}(x+iy) =(\cos\theta+i\sin\theta)(x+iy)$$ For a hyperbolic rotation $$\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix} \cosh\theta & \sinh\theta \\ \sinh\theta &\cosh\theta \end{bmatrix} \begin{bmatrix} x\\y\end{bmatrix} $$ is there an algebraic method of representing this transformation, in analogy to the complex numbers?",,"['linear-algebra', 'complex-analysis', 'hyperbolic-geometry']"
74,"The dimension of the sum of subspaces $(U_1,\ldots,U_n)$",The dimension of the sum of subspaces,"(U_1,\ldots,U_n)","If $U_1$ and $U_2$ are subspaces of a finite dimensional vector space, then $$\dim(U_1+U_2) = \dim U_1+\dim U_2-\dim(U_1 \cap U_2).$$ How can one generalize this notion to a collection of $n$ subspaces $U_1,\ldots,U_n$? Or what does $\dim(U_1+\cdots+U_n)$ equal?","If $U_1$ and $U_2$ are subspaces of a finite dimensional vector space, then $$\dim(U_1+U_2) = \dim U_1+\dim U_2-\dim(U_1 \cap U_2).$$ How can one generalize this notion to a collection of $n$ subspaces $U_1,\ldots,U_n$? Or what does $\dim(U_1+\cdots+U_n)$ equal?",,['linear-algebra']
75,"How do you determine whether a given set of functions is a subspace of C[-1,1]?","How do you determine whether a given set of functions is a subspace of C[-1,1]?",,"I'm having a terrible time understanding subspaces (and, well, linear algebra in general). I'm presented with the problem: Determine whether the following are subspaces of C [-1,1]: a) The set of functions f in C [-1,1] such that f (-1)= f (1) e) The set of functions f in C [-1,1] such that f (-1)=0 or f (1)=0 I'm not sure that I even completely understand the question, let alone how to solve the problem. Before I go about ripping my hair out, can someone perhaps explain to me how to approach this problem?","I'm having a terrible time understanding subspaces (and, well, linear algebra in general). I'm presented with the problem: Determine whether the following are subspaces of C [-1,1]: a) The set of functions f in C [-1,1] such that f (-1)= f (1) e) The set of functions f in C [-1,1] such that f (-1)=0 or f (1)=0 I'm not sure that I even completely understand the question, let alone how to solve the problem. Before I go about ripping my hair out, can someone perhaps explain to me how to approach this problem?",,"['linear-algebra', 'vector-spaces']"
76,Composition of two reflections is a rotation,Composition of two reflections is a rotation,,"I have this problem that says: Prove that in the plane, every rotation about the origin is composition of two reflections in axis on the origin. First I have to say that this is a translation, off my own, about a problem written in spanish, second, this is the first time I write a geometry question in english. I don't know how to prove this, so I made a few drawings, but I believe I got more confused. I put a point P in the plane and then rotate it $\theta$ from the X axis and got $P_\theta$, I assume that what the problem wants is to get from P to the same $P_\theta$ but with two reflections, this is what I don't understand, why do we need two? if we bisect the angle that P and $P_\theta$ formed then we get an axis that works as the axis of reflection, then we don't need two, but one to get the same point. Please tell me what's going on here. I know that we can see rotations and reflections as matrix, should I try to multiply two reflections with different angles and then see if I can rewrite the result as a rotation?","I have this problem that says: Prove that in the plane, every rotation about the origin is composition of two reflections in axis on the origin. First I have to say that this is a translation, off my own, about a problem written in spanish, second, this is the first time I write a geometry question in english. I don't know how to prove this, so I made a few drawings, but I believe I got more confused. I put a point P in the plane and then rotate it $\theta$ from the X axis and got $P_\theta$, I assume that what the problem wants is to get from P to the same $P_\theta$ but with two reflections, this is what I don't understand, why do we need two? if we bisect the angle that P and $P_\theta$ formed then we get an axis that works as the axis of reflection, then we don't need two, but one to get the same point. Please tell me what's going on here. I know that we can see rotations and reflections as matrix, should I try to multiply two reflections with different angles and then see if I can rewrite the result as a rotation?",,"['linear-algebra', 'geometry', 'euclidean-geometry']"
77,On the difference of two positive semi-definite matrices,On the difference of two positive semi-definite matrices,,"I am relatively new to linear algebra, and have been struggling with a problem for a few days now. Say we have two positive semi-definite matrices $A$ and $B$, and further assume that $A$ and $B$ are such that $A - B$ is also positive semi-definite. Can it be shown that $det(A) \ge det(B)$? In my own attempts, I can see that $Tr(A) \ge Tr(B)$, but I do not think this is enough to prove the desired result. Perhaps there is something to be said about the relative magnitudes of the eigenvalues of $A$ and $B$, but I can't see it. In any case, I would appreciate any help. Thanks a lot.","I am relatively new to linear algebra, and have been struggling with a problem for a few days now. Say we have two positive semi-definite matrices $A$ and $B$, and further assume that $A$ and $B$ are such that $A - B$ is also positive semi-definite. Can it be shown that $det(A) \ge det(B)$? In my own attempts, I can see that $Tr(A) \ge Tr(B)$, but I do not think this is enough to prove the desired result. Perhaps there is something to be said about the relative magnitudes of the eigenvalues of $A$ and $B$, but I can't see it. In any case, I would appreciate any help. Thanks a lot.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
78,Example of modules over PID's,Example of modules over PID's,,"From Advanced Modern Algebra (Rotman): Definition If $M$ is a finitely generated torsion $R$ -module, where $R$ is a PID, and $$M= R/(c_1) \oplus R/(c_2) \oplus ... \oplus R/(c_t),$$ where $t \geq 1$ and $c_1 | c_2 | ... | c_t$ , then the ideals $(c_1), (c_2), ... , (c_t)$ are called the invariant factors of $M$ . Corollary 8.20 If $M$ is a finitely generated torsion module over a PID $R$ , then $$(c_t) = \{r \in R: rM=\{0\}\},$$ where $(c_t)$ is teh last ideal occuring in the decomposition of $M$ [in the definition]. In particular, if $R=k[x]$ , where $k$ is a field, then $c_t$ is the monic polynoial of least degree for which $c_tM=\{0\}$ . I understand how these work for abelain groups. Let $G = J(c_1) \oplus ... \oplus J(c_r),$ where $J(c_i)$ is a cyclic group of order $c_i$ , and $c_1 | c_2 | ... |c_r$ . Then, since $c_iJ(c_i) = 0$ , (if we let $c_r = ac_i$ for some $a$ ) we have $c_rJ(c_i) = a(c_iJ(c_i)) = 0$ . It also makes intuitive sense to me. But when I think about modules, it kind of gets confusing. How can you annhilate a cylic module by multiplying it by a polynomial ? In other words, I thought that it would be clearer in my head if I saw an example, so I tried to search the internet but couldn't really find anything useful. So I was wondering if anybody could give me an example (even if its a simple one), just so I can see how multiplying by a polynomial can annihilate a cyclic module. Thank you in advance","From Advanced Modern Algebra (Rotman): Definition If is a finitely generated torsion -module, where is a PID, and where and , then the ideals are called the invariant factors of . Corollary 8.20 If is a finitely generated torsion module over a PID , then where is teh last ideal occuring in the decomposition of [in the definition]. In particular, if , where is a field, then is the monic polynoial of least degree for which . I understand how these work for abelain groups. Let where is a cyclic group of order , and . Then, since , (if we let for some ) we have . It also makes intuitive sense to me. But when I think about modules, it kind of gets confusing. How can you annhilate a cylic module by multiplying it by a polynomial ? In other words, I thought that it would be clearer in my head if I saw an example, so I tried to search the internet but couldn't really find anything useful. So I was wondering if anybody could give me an example (even if its a simple one), just so I can see how multiplying by a polynomial can annihilate a cyclic module. Thank you in advance","M R R M= R/(c_1) \oplus R/(c_2) \oplus ... \oplus R/(c_t), t \geq 1 c_1 | c_2 | ... | c_t (c_1), (c_2), ... , (c_t) M M R (c_t) = \{r \in R: rM=\{0\}\}, (c_t) M R=k[x] k c_t c_tM=\{0\} G = J(c_1) \oplus ... \oplus J(c_r), J(c_i) c_i c_1 | c_2 | ... |c_r c_iJ(c_i) = 0 c_r = ac_i a c_rJ(c_i) = a(c_iJ(c_i)) = 0",['linear-algebra']
79,Need help calculating this determinant using induction,Need help calculating this determinant using induction,,This is the determinant of a matrix of ($n \times n$) that needs to be calculated: \begin{pmatrix} 3 &2 &0 &0 &\cdots &0 &0 &0 &0\\ 1 &3 &2 &0 &\cdots &0 &0 &0 &0\\ 0 &1 &3 &2 &\cdots &0 &0 &0 &0\\ 0 &0 &1 &3 &\cdots &0 &0 &0 &0\\ \vdots &\vdots &\vdots&\ddots &\ddots &\ddots&\vdots &\vdots&\vdots\\ 0 &0 &0 &0 &\cdots &3 &2 &0 &0\\ 0 &0 &0 &0 &\cdots &1 &3 &2 &0\\ 0 &0 &0 &0 &\cdots &0 &1 &3 &2\\ 0 &0 &0 &0 &\cdots &0 &0 &1 &3\\ \end{pmatrix} The matrix follows the pattern as showed. I have to calculate it using induction (we haven't learnt recursion so far). Thanks,This is the determinant of a matrix of ($n \times n$) that needs to be calculated: \begin{pmatrix} 3 &2 &0 &0 &\cdots &0 &0 &0 &0\\ 1 &3 &2 &0 &\cdots &0 &0 &0 &0\\ 0 &1 &3 &2 &\cdots &0 &0 &0 &0\\ 0 &0 &1 &3 &\cdots &0 &0 &0 &0\\ \vdots &\vdots &\vdots&\ddots &\ddots &\ddots&\vdots &\vdots&\vdots\\ 0 &0 &0 &0 &\cdots &3 &2 &0 &0\\ 0 &0 &0 &0 &\cdots &1 &3 &2 &0\\ 0 &0 &0 &0 &\cdots &0 &1 &3 &2\\ 0 &0 &0 &0 &\cdots &0 &0 &1 &3\\ \end{pmatrix} The matrix follows the pattern as showed. I have to calculate it using induction (we haven't learnt recursion so far). Thanks,,"['linear-algebra', 'matrices', 'induction', 'determinant']"
80,How to prove positive semi-definiteness using eigenvalues,How to prove positive semi-definiteness using eigenvalues,,"I'm reading a very short paper by Peter G. Casazza but I cannot understand the proof in page 2. Suppose $\mathbf A$ is a positive semi-definite matrix with rank $k$; that is, some of its eigenvalues $\lambda_i$ may be zero, but all the positive eigenvalues are larger than $b$ and $b'$. This is what we want to prove: $$(\mathbf A - b\mathbf I)^{-1} - (\mathbf A - b'\mathbf I)^{-1} \succeq \frac{\delta}{2}(\mathbf A - b'\mathbf I)^{-2}$$ where $b' = b-\delta > \delta > 0$ and $\mathbf X \succeq \mathbf Y$ means $\mathbf X - \mathbf Y$ is positive semi-definite. He finished his proof by showing the following two things: $$\frac{-1}{b} - \frac{-1}{b'} = \frac{b-b'}{bb'} =  \frac{\delta}{b'(b'+\delta)} \geq \frac{\delta}{2(b')^2}$$ and $$\frac{1}{\lambda_i - b} - \frac{1}{\lambda_i-b'} = \frac{b-b'}{(\lambda_i - b)(\lambda_i - b')} \geq \frac{\delta}{(\lambda_i - b')^2}.$$ My question is: To prove $\mathbf X - \mathbf Y \succeq \mathbf Z$, is it enough to check their eigenvalues and find some relation between them? Can anyone explain why those two things above are enough to show the desired positive semi-definiteness?","I'm reading a very short paper by Peter G. Casazza but I cannot understand the proof in page 2. Suppose $\mathbf A$ is a positive semi-definite matrix with rank $k$; that is, some of its eigenvalues $\lambda_i$ may be zero, but all the positive eigenvalues are larger than $b$ and $b'$. This is what we want to prove: $$(\mathbf A - b\mathbf I)^{-1} - (\mathbf A - b'\mathbf I)^{-1} \succeq \frac{\delta}{2}(\mathbf A - b'\mathbf I)^{-2}$$ where $b' = b-\delta > \delta > 0$ and $\mathbf X \succeq \mathbf Y$ means $\mathbf X - \mathbf Y$ is positive semi-definite. He finished his proof by showing the following two things: $$\frac{-1}{b} - \frac{-1}{b'} = \frac{b-b'}{bb'} =  \frac{\delta}{b'(b'+\delta)} \geq \frac{\delta}{2(b')^2}$$ and $$\frac{1}{\lambda_i - b} - \frac{1}{\lambda_i-b'} = \frac{b-b'}{(\lambda_i - b)(\lambda_i - b')} \geq \frac{\delta}{(\lambda_i - b')^2}.$$ My question is: To prove $\mathbf X - \mathbf Y \succeq \mathbf Z$, is it enough to check their eigenvalues and find some relation between them? Can anyone explain why those two things above are enough to show the desired positive semi-definiteness?",,"['linear-algebra', 'matrices']"
81,Is it possible to derive $B$ from $AB=C$ when $B$ is not invertible?,Is it possible to derive  from  when  is not invertible?,B AB=C B,"Let $A$ be an $m\times n$ matrix, $B$ be an $n\times k$ non-invertible matrix (where $n<k$) and $C=AB$. If we have $C$ and $A$, can we derive $B$?","Let $A$ be an $m\times n$ matrix, $B$ be an $n\times k$ non-invertible matrix (where $n<k$) and $C=AB$. If we have $C$ and $A$, can we derive $B$?",,"['linear-algebra', 'matrices']"
82,Examples of how to calculate $e^A$,Examples of how to calculate,e^A,"I'm trying to learn the process to discover $e^A$ For example, if $A$ is diagonalizable is easy: $$A =\begin{pmatrix}         5 & -6 \\         3 & -4 \\         \end{pmatrix}$$ Then we have the canonical form $$J_A =\begin{pmatrix}         2 & 0 \\         0 & -1 \\         \end{pmatrix}$$ because the auto-values are $2$ and $-1$. Am I right? so I continue $$e^A =P\begin{pmatrix}         e^2 & 0 \\         0 & e^{-1} \\         \end{pmatrix}P^{-1}$$ Where $$P=\begin{pmatrix}         2 & 1 \\         1 & 1 \\         \end{pmatrix}$$ Because the auto-vectors are (2,1) and (1,1). If the auto-values the things become more complicated: For example: $$B =\begin{pmatrix}         0 & 1 \\        -2 & -2 \\         \end{pmatrix}$$ The auto-values are $-1+i$ and $-1-i$, then the canonical form is: $$J_B =\begin{pmatrix}         -1 & -1 \\         1 & -1 \\         \end{pmatrix}$$ I don't know how to discover $e^B$ in the complex case. How do I have to proceed in this case? I would like to know also if there are some pdfs or books with examples or problems with solutions about this subject. I really need help Thanks a lot. EDIT I found another example of a matrix whose some auto-values are complexes: $$         C=\begin{pmatrix}         1 & 0 & -2 \\         -5 & 6 & 11 \\         5 & -5 & -10 \\         \end{pmatrix} $$ Its canonical form $$         J_C=\begin{pmatrix}         1 & 0 & 0 \\         0 & -2 & 1 \\         0 & -1 & -2 \\         \end{pmatrix} $$ Why $J_A$ is the matrix $A$ in the canonical form? the author doesn't use complexes numbers, why? How do I find $e^A$ in this case? in the same way? EDIT 2 The book I'm using says that the complex Jordan block related to the auto-value $a+bi$ is $$         \begin{pmatrix}         a & b \\         -b & a \\         \end{pmatrix} $$ The book also says that it's using the real Jordan canonical form in contrast with the complex Jordan canonical form (see answer below).","I'm trying to learn the process to discover $e^A$ For example, if $A$ is diagonalizable is easy: $$A =\begin{pmatrix}         5 & -6 \\         3 & -4 \\         \end{pmatrix}$$ Then we have the canonical form $$J_A =\begin{pmatrix}         2 & 0 \\         0 & -1 \\         \end{pmatrix}$$ because the auto-values are $2$ and $-1$. Am I right? so I continue $$e^A =P\begin{pmatrix}         e^2 & 0 \\         0 & e^{-1} \\         \end{pmatrix}P^{-1}$$ Where $$P=\begin{pmatrix}         2 & 1 \\         1 & 1 \\         \end{pmatrix}$$ Because the auto-vectors are (2,1) and (1,1). If the auto-values the things become more complicated: For example: $$B =\begin{pmatrix}         0 & 1 \\        -2 & -2 \\         \end{pmatrix}$$ The auto-values are $-1+i$ and $-1-i$, then the canonical form is: $$J_B =\begin{pmatrix}         -1 & -1 \\         1 & -1 \\         \end{pmatrix}$$ I don't know how to discover $e^B$ in the complex case. How do I have to proceed in this case? I would like to know also if there are some pdfs or books with examples or problems with solutions about this subject. I really need help Thanks a lot. EDIT I found another example of a matrix whose some auto-values are complexes: $$         C=\begin{pmatrix}         1 & 0 & -2 \\         -5 & 6 & 11 \\         5 & -5 & -10 \\         \end{pmatrix} $$ Its canonical form $$         J_C=\begin{pmatrix}         1 & 0 & 0 \\         0 & -2 & 1 \\         0 & -1 & -2 \\         \end{pmatrix} $$ Why $J_A$ is the matrix $A$ in the canonical form? the author doesn't use complexes numbers, why? How do I find $e^A$ in this case? in the same way? EDIT 2 The book I'm using says that the complex Jordan block related to the auto-value $a+bi$ is $$         \begin{pmatrix}         a & b \\         -b & a \\         \end{pmatrix} $$ The book also says that it's using the real Jordan canonical form in contrast with the complex Jordan canonical form (see answer below).",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
83,Jordan form and an invertible $P$ for $A =\left( \begin{smallmatrix} 1&1&1 \\ 0 & 2 & 2 \\ 0 & 0 & 2 \end{smallmatrix}\right)$,Jordan form and an invertible  for,P A =\left( \begin{smallmatrix} 1&1&1 \\ 0 & 2 & 2 \\ 0 & 0 & 2 \end{smallmatrix}\right),"$A = \begin{pmatrix} 1&1&1 \\ 0 & 2 & 2 \\ 0 & 0 & 2 \end{pmatrix}$, find the jordan form and the invertible $P$ such that: $A = P J P^{-1}$. Now I found the characteristic polynomial and minimal polynomials: $P_A(x) = (x-1)(x-2)^2 = m_A(x)$. And from the minimal polynomial I found out that the maximal block size for the eigenvalue $1$ is $1$ so we have one block of size $1$ for that eigenvalue. And in the same way that the maximal jordan block size for eigenvalue $2$ is $2$ and I calculated $N=A-2I$ and figured that there is only one block of size $2$ for eigenvalue $2$. And so I found the Jordan Form: $$J_A = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \end{pmatrix}$$ Now what I am having trouble with is finding $P$. I know that $Ker(N) = Ker(N-2I) = (1,1,0$ and $Ker(Z) = Ker(A-I) = (1,0,0)$ But how do I exactly calculate the spans to know the basis for the Jordan form if I have two eigenvalues? This is an algorithm that I was not taught! Any help will be appreciated","$A = \begin{pmatrix} 1&1&1 \\ 0 & 2 & 2 \\ 0 & 0 & 2 \end{pmatrix}$, find the jordan form and the invertible $P$ such that: $A = P J P^{-1}$. Now I found the characteristic polynomial and minimal polynomials: $P_A(x) = (x-1)(x-2)^2 = m_A(x)$. And from the minimal polynomial I found out that the maximal block size for the eigenvalue $1$ is $1$ so we have one block of size $1$ for that eigenvalue. And in the same way that the maximal jordan block size for eigenvalue $2$ is $2$ and I calculated $N=A-2I$ and figured that there is only one block of size $2$ for eigenvalue $2$. And so I found the Jordan Form: $$J_A = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \end{pmatrix}$$ Now what I am having trouble with is finding $P$. I know that $Ker(N) = Ker(N-2I) = (1,1,0$ and $Ker(Z) = Ker(A-I) = (1,0,0)$ But how do I exactly calculate the spans to know the basis for the Jordan form if I have two eigenvalues? This is an algorithm that I was not taught! Any help will be appreciated",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
84,Elementary Row Matrices,Elementary Row Matrices,,"Let $A$ = $$ \begin{align} \begin{bmatrix} -4 & 3\\ 1 & 0 \end{bmatrix} \end{align} $$ Find $2 \times 2$ elementary matrices $E_1$,$E_2$,$E_3$ such that $A$ = $E_1 E_2 E_3$ I figured out the operations which need to be performed which are; $E_1$ = $R_2 \leftrightarrow R_1$ $E_2$ = $R_2$ = $R_2$ + $4R_1$ $E_3$ = $R_2$ * $\frac{1}{3}$ My question is how would I go about writing the elementary matrices? The solution says that they are; $E_1$ = $ \begin{align} \begin{bmatrix} 1 & -4\\ 0 & 1 \end{bmatrix} \end{align} $ $E_2$ = $ \begin{align} \begin{bmatrix} 3 & 0\\ 0 & 1 \end{bmatrix} \end{align} $ $E_3$ = $ \begin{align} \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} \end{align} $","Let $A$ = $$ \begin{align} \begin{bmatrix} -4 & 3\\ 1 & 0 \end{bmatrix} \end{align} $$ Find $2 \times 2$ elementary matrices $E_1$,$E_2$,$E_3$ such that $A$ = $E_1 E_2 E_3$ I figured out the operations which need to be performed which are; $E_1$ = $R_2 \leftrightarrow R_1$ $E_2$ = $R_2$ = $R_2$ + $4R_1$ $E_3$ = $R_2$ * $\frac{1}{3}$ My question is how would I go about writing the elementary matrices? The solution says that they are; $E_1$ = $ \begin{align} \begin{bmatrix} 1 & -4\\ 0 & 1 \end{bmatrix} \end{align} $ $E_2$ = $ \begin{align} \begin{bmatrix} 3 & 0\\ 0 & 1 \end{bmatrix} \end{align} $ $E_3$ = $ \begin{align} \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} \end{align} $",,"['linear-algebra', 'matrices']"
85,"if $\mathbf x$ is sampled randomly from a hypercube on $R^n$, what is the probability density for $|\mathbf x| = d$","if  is sampled randomly from a hypercube on , what is the probability density for",\mathbf x R^n |\mathbf x| = d,"if the vector $\mathbf x$ is sampled randomly from a uniform distribution on $[0, 1]^d$, what is the probability density function for $|\mathbf x|$? Is it easy to scale for $[0, n]^d$?","if the vector $\mathbf x$ is sampled randomly from a uniform distribution on $[0, 1]^d$, what is the probability density function for $|\mathbf x|$? Is it easy to scale for $[0, n]^d$?",,"['linear-algebra', 'probability']"
86,Multiplication of determinants,Multiplication of determinants,,"Show that for any vectors $\bf{a}$,$\bf{b}$,$\bf{c}$,$\bf{u}$,$\bf{v}$,$\bf{w}$ in $\mathbb{R}^3$, $$[\bf{a},\bf{b},\bf{c}][\bf{u},\bf{v},\bf{w}]=\begin{vmatrix}\bf{a}\cdot\bf{u}&\bf{a}\cdot\bf{v}&\bf{a}\cdot\bf{w}\\ \bf{b}\cdot\bf{u}&\bf{b}\cdot\bf{v}&\bf{b}\cdot\bf{w}\\ \bf{c}\cdot\bf{u}&\bf{c}\cdot\bf{v}&\bf{c}\cdot\bf{w}\\ \end{vmatrix}$$ My question: is there a more elegant way to prove that without actually delving into tedious direct approach by definition? I was considering that ""summing"" approach, using the fact that $$[\textbf{a},\textbf{b},\textbf{c}][\textbf{u},\textbf{v},\textbf{w}]=\bigg(\sum^3_{i,j,k=1}\epsilon_{ijk}a_ib_jc_k\bigg)\bigg(\sum^3_{i,j,k=1}\epsilon_{ijk}u_iv_jw_k\bigg)$$ But I am very weak with manipulations of sums and can't get much further. Any hints? Note This problem occured when the text book formalized the corkscrew rule and discussed orientation of vectors in $\mathbb{R}^3$, meaning even matrices has not been introduced yet, as the chapter itself was analyzing vectors in the given space.","Show that for any vectors $\bf{a}$,$\bf{b}$,$\bf{c}$,$\bf{u}$,$\bf{v}$,$\bf{w}$ in $\mathbb{R}^3$, $$[\bf{a},\bf{b},\bf{c}][\bf{u},\bf{v},\bf{w}]=\begin{vmatrix}\bf{a}\cdot\bf{u}&\bf{a}\cdot\bf{v}&\bf{a}\cdot\bf{w}\\ \bf{b}\cdot\bf{u}&\bf{b}\cdot\bf{v}&\bf{b}\cdot\bf{w}\\ \bf{c}\cdot\bf{u}&\bf{c}\cdot\bf{v}&\bf{c}\cdot\bf{w}\\ \end{vmatrix}$$ My question: is there a more elegant way to prove that without actually delving into tedious direct approach by definition? I was considering that ""summing"" approach, using the fact that $$[\textbf{a},\textbf{b},\textbf{c}][\textbf{u},\textbf{v},\textbf{w}]=\bigg(\sum^3_{i,j,k=1}\epsilon_{ijk}a_ib_jc_k\bigg)\bigg(\sum^3_{i,j,k=1}\epsilon_{ijk}u_iv_jw_k\bigg)$$ But I am very weak with manipulations of sums and can't get much further. Any hints? Note This problem occured when the text book formalized the corkscrew rule and discussed orientation of vectors in $\mathbb{R}^3$, meaning even matrices has not been introduced yet, as the chapter itself was analyzing vectors in the given space.",,"['linear-algebra', 'determinant']"
87,Showing no non-trivial t-invariant subspace has a t-invariant complement.,Showing no non-trivial t-invariant subspace has a t-invariant complement.,,"The question is from Hoffman and Kunze Let $T$ be a linear operator on a finite-dimensional vector space $V$ . Suppose that: (a) the minimal polynomial for $T$ is a power of an irreducible polynomial ; (b) the minimal polynomial is equal to the characteristic polynomial. Show that no non-trivial $T$ -invariant subspace has a complementary $T$ -invariant subspace I know from a,b that $T$ is not diagonalizable; possible irrelevant. I know that every $T$ -admissible subspace has a complementary subspace which is also invariant under $T$ . So I basically want to show that $W=\{0\}$ and its complement are the only $T$ -admissible subspaces. Not sure how to do this as $T$ -admissible requires $T$ -invariant. Can somebody point me in the right direction for how to solve this problem? (preferable without posting a solution.)","The question is from Hoffman and Kunze Let be a linear operator on a finite-dimensional vector space . Suppose that: (a) the minimal polynomial for is a power of an irreducible polynomial ; (b) the minimal polynomial is equal to the characteristic polynomial. Show that no non-trivial -invariant subspace has a complementary -invariant subspace I know from a,b that is not diagonalizable; possible irrelevant. I know that every -admissible subspace has a complementary subspace which is also invariant under . So I basically want to show that and its complement are the only -admissible subspaces. Not sure how to do this as -admissible requires -invariant. Can somebody point me in the right direction for how to solve this problem? (preferable without posting a solution.)",T V T T T T T T W=\{0\} T T T,['linear-algebra']
88,Determinant inequality about positive definite matrices.,Determinant inequality about positive definite matrices.,,"Assume $A \in M_n(\Bbb{R})$ (not necessarily symmetric), and for $\forall \alpha\not=0$, $\alpha^TA\alpha>0$. Show that $$\det\left(\frac{A+A^T}{2}\right)\leq \det A.$$","Assume $A \in M_n(\Bbb{R})$ (not necessarily symmetric), and for $\forall \alpha\not=0$, $\alpha^TA\alpha>0$. Show that $$\det\left(\frac{A+A^T}{2}\right)\leq \det A.$$",,"['linear-algebra', 'matrices', 'inequality']"
89,Prove by using diagonalization,Prove by using diagonalization,,Can anyone give me some hints on how to prove this question? Q:  Use diagonalization to prove that if $A \subset B$ are lattices then $[B:A ]=\frac{\Delta(A)}{\Delta(B)}$. Added: Definition:  A lattice $A$ in the plane $\mathbb{R}^2$ is generated or spanned by a set $B$ if every element of $A$ can be written as an integer combination of elements of $B$.,Can anyone give me some hints on how to prove this question? Q:  Use diagonalization to prove that if $A \subset B$ are lattices then $[B:A ]=\frac{\Delta(A)}{\Delta(B)}$. Added: Definition:  A lattice $A$ in the plane $\mathbb{R}^2$ is generated or spanned by a set $B$ if every element of $A$ can be written as an integer combination of elements of $B$.,,['linear-algebra']
90,A sufficient condition for diagonalization,A sufficient condition for diagonalization,,"How to prove that if $A,B,M\in \mathcal{M_n}(\mathbb{C})$ and $\lambda,\mu\in\mathbb{C}$ so $$   \begin{cases}     M&=& \lambda A+\mu B \\     M^2&=& \lambda^2 A+\mu^2 B \\     M^3&=& \lambda^3 A +\mu^3 B   \end{cases} \Rightarrow M\ \text{is diagonalizable}.$$","How to prove that if $A,B,M\in \mathcal{M_n}(\mathbb{C})$ and $\lambda,\mu\in\mathbb{C}$ so $$   \begin{cases}     M&=& \lambda A+\mu B \\     M^2&=& \lambda^2 A+\mu^2 B \\     M^3&=& \lambda^3 A +\mu^3 B   \end{cases} \Rightarrow M\ \text{is diagonalizable}.$$",,[]
91,The rank of a block matrix as a function of the rank of its submatrices.,The rank of a block matrix as a function of the rank of its submatrices.,,"I would like to post this problem here in this forum. Having the following block matrix: \begin{equation} M=\begin{bmatrix} S_1 &C\\ C^T &S_2\\ \end{bmatrix} \end{equation} I would like to find the function $f$ that holds $\operatorname{rank}(M)=f( S_1, S_2,C)$ $S_1$ and $S_2$ are covariance matrices $\implies$ symmetric and positive semi-definite. $C$ is the cross covariance that may be positive semi-definite. Can you help me? I sincerely thank you! :) All the best GoodSpirit","I would like to post this problem here in this forum. Having the following block matrix: \begin{equation} M=\begin{bmatrix} S_1 &C\\ C^T &S_2\\ \end{bmatrix} \end{equation} I would like to find the function $f$ that holds $\operatorname{rank}(M)=f( S_1, S_2,C)$ $S_1$ and $S_2$ are covariance matrices $\implies$ symmetric and positive semi-definite. $C$ is the cross covariance that may be positive semi-definite. Can you help me? I sincerely thank you! :) All the best GoodSpirit",,"['linear-algebra', 'matrices', 'matrix-rank', 'block-matrices']"
92,Trace of a matrix times outer product,Trace of a matrix times outer product,,$\DeclareMathOperator{\trace}{tr}$Is there any relationship between $\trace(Sxx^T)$ and $x^TSx$?  Is there a nice way to write the set of quadratic functions of the components of a vector $x$ given coefficients in some matrix $S$?,$\DeclareMathOperator{\trace}{tr}$Is there any relationship between $\trace(Sxx^T)$ and $x^TSx$?  Is there a nice way to write the set of quadratic functions of the components of a vector $x$ given coefficients in some matrix $S$?,,['linear-algebra']
93,A particular ILP where the existence of a relaxed solution implies the existence of an integer solution,A particular ILP where the existence of a relaxed solution implies the existence of an integer solution,,"This question emerged from a discussion on my previous question Determining quickly whether this Integer Linear Program has any solution at all , which I would like to elaborate separately. I am trying to determine whether a solution exists for $$ \begin{aligned} A\mathbf{x} &\geq \mathbf{b} &\text{where}\quad \mathbf{x} &\in \mathbb{Z}^m,\ \mathbf{x}\geq\mathbf{0},\ \\ && \mathbf{b} &\in \mathbb{R}^n,\ \mathbf{b}\geq\mathbf{0},\ \\ && A&\in \mathbb{R}^{n\times m}\ . \end{aligned} $$ I postulate that in this particular case, if a solution exists for the relaxed constraint $\mathbf{x} \in \mathbb{R}^m,\ \mathbf{x}\geq\mathbf{0}$, an integer solution always exists as well. The proof would go somewhat like this: $A\mathbf{x}$ spans a pointed convex cone $C_a$ from the origin.  The criterion $\mathbf{y} \geq \mathbf{b}$ implies a cone $C_b$ originating at $\mathbf{b}$, with the particular trait that if the two cones intersect, every ray in $C_a$ which intersects $C_b$ will intersect exactly one side of $C_b$, i.e. it will enter $C_b$ but never leave it.  This should, in turn, imply that there must exist an $\mathbf{x} \in \mathbb{Z}^m,\ \mathbf{x}\geq\mathbf{0}$ so that $A\mathbf{x} \geq \mathbf{b}$. Obviously that is not a proof, and anything but formal. But maybe someone could enlighten me on how such a formal proof could be constructed  or maybe, in case I'm altogether wrong, provide a counterexample?  Thanks in advance!","This question emerged from a discussion on my previous question Determining quickly whether this Integer Linear Program has any solution at all , which I would like to elaborate separately. I am trying to determine whether a solution exists for $$ \begin{aligned} A\mathbf{x} &\geq \mathbf{b} &\text{where}\quad \mathbf{x} &\in \mathbb{Z}^m,\ \mathbf{x}\geq\mathbf{0},\ \\ && \mathbf{b} &\in \mathbb{R}^n,\ \mathbf{b}\geq\mathbf{0},\ \\ && A&\in \mathbb{R}^{n\times m}\ . \end{aligned} $$ I postulate that in this particular case, if a solution exists for the relaxed constraint $\mathbf{x} \in \mathbb{R}^m,\ \mathbf{x}\geq\mathbf{0}$, an integer solution always exists as well. The proof would go somewhat like this: $A\mathbf{x}$ spans a pointed convex cone $C_a$ from the origin.  The criterion $\mathbf{y} \geq \mathbf{b}$ implies a cone $C_b$ originating at $\mathbf{b}$, with the particular trait that if the two cones intersect, every ray in $C_a$ which intersects $C_b$ will intersect exactly one side of $C_b$, i.e. it will enter $C_b$ but never leave it.  This should, in turn, imply that there must exist an $\mathbf{x} \in \mathbb{Z}^m,\ \mathbf{x}\geq\mathbf{0}$ so that $A\mathbf{x} \geq \mathbf{b}$. Obviously that is not a proof, and anything but formal. But maybe someone could enlighten me on how such a formal proof could be constructed  or maybe, in case I'm altogether wrong, provide a counterexample?  Thanks in advance!",,"['linear-algebra', 'euclidean-geometry', 'convex-analysis', 'linear-programming', 'integer-programming']"
94,Use row reduction to prove that $\det(\mathbf{A})=\det(\mathbf{A}^{T})$,Use row reduction to prove that,\det(\mathbf{A})=\det(\mathbf{A}^{T}),"I need to prove that the determinant of a matrix is equal to the determinant of its transpose. This fact is obviously easy to prove using the definition of the determinant, but the question stipulates that the proof must be by row reduction. It would never occur to me to do it this way. How do you even get to the transpose using row reduction? I've always just swapped the rows and columns. Any advice would be appreciated.","I need to prove that the determinant of a matrix is equal to the determinant of its transpose. This fact is obviously easy to prove using the definition of the determinant, but the question stipulates that the proof must be by row reduction. It would never occur to me to do it this way. How do you even get to the transpose using row reduction? I've always just swapped the rows and columns. Any advice would be appreciated.",,"['linear-algebra', 'determinant']"
95,Problem related to a square matrix,Problem related to a square matrix,,"Let $A$ be an $n\times n$ matrix with real entries such that $A^{2}+I=\mathbf{0}$. Then: (A) $n$ is an odd integer. (B) $n$ is an even integer. (C) $n$ has to be $2$ (D) $n$ could be any positive integer. I was thinking about the problem.I noticed for a  $2\times 2$ matrix $A$ of the form  $$\begin{pmatrix} 1 &-2 \\   1& -1 \end{pmatrix},$$ the given condition holds good.So option (C) is a possibility.But I am not sure about other options. Is there any convenient way to tackle it?With regards..","Let $A$ be an $n\times n$ matrix with real entries such that $A^{2}+I=\mathbf{0}$. Then: (A) $n$ is an odd integer. (B) $n$ is an even integer. (C) $n$ has to be $2$ (D) $n$ could be any positive integer. I was thinking about the problem.I noticed for a  $2\times 2$ matrix $A$ of the form  $$\begin{pmatrix} 1 &-2 \\   1& -1 \end{pmatrix},$$ the given condition holds good.So option (C) is a possibility.But I am not sure about other options. Is there any convenient way to tackle it?With regards..",,['linear-algebra']
96,unit length vector in kernel of matrix,unit length vector in kernel of matrix,,"congratulation  you all   passed  festival(new year,christmas),guys i have question related kernel of matrix,namely suppose we have  following matrix $$ A=   \begin{bmatrix}       1 & -1 & 0 \\       0 & 0 & 1  \\     0 & 0 & 0  \\     \\   \end{bmatrix} $$ we should find  unit-length  vector of kernel of matrix,for kernel  i think  we should find such null space  or  vector  $x$  for which $A*x=0$ as it is indicated  on following  wikepedia  site [http://en.wikipedia.org/wiki/Kernel_(matrix)][1] but i think it has  many solution because  if we interpret  $x$  as  $x=[x_1,  x_2,  x_3]$ we get $x_1-x_2+0*x_3=0$ $x_3=0$ so we have $x_1=x_2$ $x_3=0$ how can i continue?please help me","congratulation  you all   passed  festival(new year,christmas),guys i have question related kernel of matrix,namely suppose we have  following matrix $$ A=   \begin{bmatrix}       1 & -1 & 0 \\       0 & 0 & 1  \\     0 & 0 & 0  \\     \\   \end{bmatrix} $$ we should find  unit-length  vector of kernel of matrix,for kernel  i think  we should find such null space  or  vector  $x$  for which $A*x=0$ as it is indicated  on following  wikepedia  site [http://en.wikipedia.org/wiki/Kernel_(matrix)][1] but i think it has  many solution because  if we interpret  $x$  as  $x=[x_1,  x_2,  x_3]$ we get $x_1-x_2+0*x_3=0$ $x_3=0$ so we have $x_1=x_2$ $x_3=0$ how can i continue?please help me",,['linear-algebra']
97,How to solve this linear equations?,How to solve this linear equations?,,"I am dealing with the following example: $$ \pmatrix{     -1 & -2& -3 \\      3 & 2 & 1 \\ 1& 1 & 1 \\     2 & 4& 1      }*  \pmatrix{    x_1\\ x_2 \\ x_3 \\     }= \pmatrix{    -6\\ 6 \\ 3 \\ 7 \\   }$$ I know I should solve that matrix with gau' elimination, but my problem is that there are only $x_1 $ to $x_3 $, so I do not know how to interprete this example?","I am dealing with the following example: $$ \pmatrix{     -1 & -2& -3 \\      3 & 2 & 1 \\ 1& 1 & 1 \\     2 & 4& 1      }*  \pmatrix{    x_1\\ x_2 \\ x_3 \\     }= \pmatrix{    -6\\ 6 \\ 3 \\ 7 \\   }$$ I know I should solve that matrix with gau' elimination, but my problem is that there are only $x_1 $ to $x_3 $, so I do not know how to interprete this example?",,"['linear-algebra', 'matrices']"
98,Invariants in a second order equation,Invariants in a second order equation,,"For $Ax^2+2Bxy+Cy^2+2Dx+2Ey+F=0$, why are $\begin{vmatrix} A &B \\  B &C  \end{vmatrix}$ and $\begin{vmatrix} A & B & D\\  B & C & E\\  D & E & F \end{vmatrix}$ invariant under an orthogonal transformation? I was considering simply convincing myself of its self-evidence by through looking at the mechanics of the possible transformations, but the fact that 2 invariants are expressible in determinant form makes it look as if there's a far more elegant scheme underneath. What is the 'book proof' of their invariance (if there is an elegant one beyond mechanics), and how is it proved that they (and $A+C$) are the only possible invariants for a second order equation (for orthogonal transformations)? The answer to the following will probably be implicit in the main answer, but how would this proof be extendable into an $n$-ordered equation?","For $Ax^2+2Bxy+Cy^2+2Dx+2Ey+F=0$, why are $\begin{vmatrix} A &B \\  B &C  \end{vmatrix}$ and $\begin{vmatrix} A & B & D\\  B & C & E\\  D & E & F \end{vmatrix}$ invariant under an orthogonal transformation? I was considering simply convincing myself of its self-evidence by through looking at the mechanics of the possible transformations, but the fact that 2 invariants are expressible in determinant form makes it look as if there's a far more elegant scheme underneath. What is the 'book proof' of their invariance (if there is an elegant one beyond mechanics), and how is it proved that they (and $A+C$) are the only possible invariants for a second order equation (for orthogonal transformations)? The answer to the following will probably be implicit in the main answer, but how would this proof be extendable into an $n$-ordered equation?",,"['linear-algebra', 'linear-transformations', 'determinant', 'quadratic-forms', 'invariance']"
99,Question with inner product,Question with inner product,,"This is a question that I'm trying to solve since yesterday. Let $T:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be a linear transformation such that $$\begin{equation}     \langle u,v\rangle = 0, \langle Tu,v\rangle >0\quad\Rightarrow\quad \langle u,Tv\rangle >0 \end{equation}$$ We have to proof the following 1) $\langle u,v\rangle = 0$, $\langle Tu,v\rangle =0\quad\Rightarrow\quad \langle u,Tv\rangle =0$; 2) There exists an orthonormal basis for $T$; 3) $T$ is symmetric. Im stuck in the first item...it really looks easy but i just cant prove this.  I used Cauchy-Schwarz inequality to see that if we have $\langle u,v\rangle=0$ and $\langle Tu,v\rangle=0$, then $|\langle u,Tv\rangle + \langle v,Tv\rangle| = 0$, in that case I want to show that $\langle v,Tv\rangle=0$ so I this implies $\langle u,Tv\rangle=0$. Also, with this $u$ and $v$, I have that $\langle u,v+T^\ast v\rangle=0$. I dont know what to do from here...I`m out of ideas, any help is very welcome.","This is a question that I'm trying to solve since yesterday. Let $T:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be a linear transformation such that $$\begin{equation}     \langle u,v\rangle = 0, \langle Tu,v\rangle >0\quad\Rightarrow\quad \langle u,Tv\rangle >0 \end{equation}$$ We have to proof the following 1) $\langle u,v\rangle = 0$, $\langle Tu,v\rangle =0\quad\Rightarrow\quad \langle u,Tv\rangle =0$; 2) There exists an orthonormal basis for $T$; 3) $T$ is symmetric. Im stuck in the first item...it really looks easy but i just cant prove this.  I used Cauchy-Schwarz inequality to see that if we have $\langle u,v\rangle=0$ and $\langle Tu,v\rangle=0$, then $|\langle u,Tv\rangle + \langle v,Tv\rangle| = 0$, in that case I want to show that $\langle v,Tv\rangle=0$ so I this implies $\langle u,Tv\rangle=0$. Also, with this $u$ and $v$, I have that $\langle u,v+T^\ast v\rangle=0$. I dont know what to do from here...I`m out of ideas, any help is very welcome.",,"['linear-algebra', 'inner-products']"
