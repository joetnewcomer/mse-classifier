,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof of same two-parameter family curves in $\mathbb R^3$.,Proof of same two-parameter family curves in .,\mathbb R^3,"Exercise : If $V = (x,y,z)$ and $\Omega = \{(x,y,z) \in \mathbb R^3 : x>0, y>0, z>0\}$, show that $yz/x^2$ and $(y^2-z^2)/x^2$ are functionally independent first integrals of $V$ in $\Omega$. Verify directly that the equations  $$\frac{yz}{x^2} = c_1, \quad \frac{(y^2-z^2)}{x^2}=c_2$$ and the equations of the example (1.21) $$\frac{y}{x} = c_1, \quad \frac{z}{x} = c_2$$ describe the same two-parameter family of curves in $\mathbb R^3$. Attempt : Let : $$u_1 = \frac{yz}{x^2}, \quad u_2 = \frac{y^2-z^2}{x^2}$$ The functionals $u_1, u_2$ indeed are first integrals of the vector field $V$, since it is $u_1, u_2 \in C^1(\Omega)$ and also  $$xu_{1_x} + yu_{1_y} + zu_{1_z} = x\frac{-2yz}{x^3} + y\frac{z}{x^2} + z \frac{y}{x^2} = 0$$ $$xu_{2_x} + yu_{2_y} + zu_{2_z} = x \frac{-2(y^2-z^2)}{x^3} + y\frac{2y}{x^2} + z \frac{-2z}{x^2}= 0 $$ Finally, we have that $$\nabla u_1 = \bigg(-\frac{2yz}{x^3}, \frac{z}{x^2}, \frac{y}{x^2}\bigg), \quad \nabla u_2 = \bigg(-\frac{2(y^2-z^2)}{x^3}, \frac{2y}{x^2}, -\frac{2z}{x^2}\bigg) $$ $$\text{and}$$ $$\nabla u_1 \times \nabla u_2 = \begin{vmatrix} i & j & k \\ -2yz/x^3 & z/x^2 & y/x^2 \\ -2(y^2-z^2)/x^3 & 2y/x^2 & -2z/x^2 \end{vmatrix} = \dots = 0$$ from where we conclude the linear independence. Question : Now, for the last part regarding proving that the equations describe the same two parameter family of curves in $\mathbb R^3$, is where I am at a loss at continuing. I can observe that, the second given equations from the example (1.21) yield the first ones if you multiply them and if you take the difference of their squares, but how would I conclude that they describe the same two-parameter family of curves ?","Exercise : If $V = (x,y,z)$ and $\Omega = \{(x,y,z) \in \mathbb R^3 : x>0, y>0, z>0\}$, show that $yz/x^2$ and $(y^2-z^2)/x^2$ are functionally independent first integrals of $V$ in $\Omega$. Verify directly that the equations  $$\frac{yz}{x^2} = c_1, \quad \frac{(y^2-z^2)}{x^2}=c_2$$ and the equations of the example (1.21) $$\frac{y}{x} = c_1, \quad \frac{z}{x} = c_2$$ describe the same two-parameter family of curves in $\mathbb R^3$. Attempt : Let : $$u_1 = \frac{yz}{x^2}, \quad u_2 = \frac{y^2-z^2}{x^2}$$ The functionals $u_1, u_2$ indeed are first integrals of the vector field $V$, since it is $u_1, u_2 \in C^1(\Omega)$ and also  $$xu_{1_x} + yu_{1_y} + zu_{1_z} = x\frac{-2yz}{x^3} + y\frac{z}{x^2} + z \frac{y}{x^2} = 0$$ $$xu_{2_x} + yu_{2_y} + zu_{2_z} = x \frac{-2(y^2-z^2)}{x^3} + y\frac{2y}{x^2} + z \frac{-2z}{x^2}= 0 $$ Finally, we have that $$\nabla u_1 = \bigg(-\frac{2yz}{x^3}, \frac{z}{x^2}, \frac{y}{x^2}\bigg), \quad \nabla u_2 = \bigg(-\frac{2(y^2-z^2)}{x^3}, \frac{2y}{x^2}, -\frac{2z}{x^2}\bigg) $$ $$\text{and}$$ $$\nabla u_1 \times \nabla u_2 = \begin{vmatrix} i & j & k \\ -2yz/x^3 & z/x^2 & y/x^2 \\ -2(y^2-z^2)/x^3 & 2y/x^2 & -2z/x^2 \end{vmatrix} = \dots = 0$$ from where we conclude the linear independence. Question : Now, for the last part regarding proving that the equations describe the same two parameter family of curves in $\mathbb R^3$, is where I am at a loss at continuing. I can observe that, the second given equations from the example (1.21) yield the first ones if you multiply them and if you take the difference of their squares, but how would I conclude that they describe the same two-parameter family of curves ?",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'vector-spaces']"
1,Understanding solution to $y' = y$ and exponential distribution,Understanding solution to  and exponential distribution,y' = y,"My Understanding: I would derive the exponential random variable as follows: I consider an experiment which consists of a continuum of trials on an interval $[0,t)$. The result of the experiment takes the form of an ordered $n$-tuple $\forall n \in \mathbb{N}$ containing distinct points on the interval. Every outcome is equally likely and I measure the size of the set containing tuples of $n$ different points by $I_n$ as: $$ I_n = \int_0^{t}  \int_0^{x_{n}} \int_0^{x_{n-1}} \cdots \int_0^{ x_2 } dx_1 dx_{2} dx_{3} \dots dx_{n-1} dx_{n} = \frac{ t^n } { n! }$$ Since the size of the set where no success occurs, $I_0 = 1$, the ratio of these sets gives the probability that no event occurs and its complement yields the c.d.f: $$ \begin{align} P(\text{no successful trial in } [0,t)) &= \big(\sum_{n = 0}^{\infty} I_n(t)\big)^{-1} = e^{-t} \\ P(X \leq t) &= 1 - e^{-t} \\ \end{align} $$ Where I Lose Intuition: It's easy to arrive at a power series solution to the ODE: $$y' = y \text{ with } y'(0) = 1$$ $$ \boxed{ y = \sum_{n = 0}^{\infty} \frac{ t^n }{ n ! } = e^{t}} $$ My problem is that I do not understand the role of each term in the expansion. Substitution by power series is an attractive idea, but I have no deep intuition as to why we'd do this and hence, I'm having trouble putting it all together to understand the solution. Question How do I interpret the power series solution of the ODE? Hopefully this will allow me to reconcile my understanding of both these processes. I am not willing to accept this as pure coincidence .","My Understanding: I would derive the exponential random variable as follows: I consider an experiment which consists of a continuum of trials on an interval $[0,t)$. The result of the experiment takes the form of an ordered $n$-tuple $\forall n \in \mathbb{N}$ containing distinct points on the interval. Every outcome is equally likely and I measure the size of the set containing tuples of $n$ different points by $I_n$ as: $$ I_n = \int_0^{t}  \int_0^{x_{n}} \int_0^{x_{n-1}} \cdots \int_0^{ x_2 } dx_1 dx_{2} dx_{3} \dots dx_{n-1} dx_{n} = \frac{ t^n } { n! }$$ Since the size of the set where no success occurs, $I_0 = 1$, the ratio of these sets gives the probability that no event occurs and its complement yields the c.d.f: $$ \begin{align} P(\text{no successful trial in } [0,t)) &= \big(\sum_{n = 0}^{\infty} I_n(t)\big)^{-1} = e^{-t} \\ P(X \leq t) &= 1 - e^{-t} \\ \end{align} $$ Where I Lose Intuition: It's easy to arrive at a power series solution to the ODE: $$y' = y \text{ with } y'(0) = 1$$ $$ \boxed{ y = \sum_{n = 0}^{\infty} \frac{ t^n }{ n ! } = e^{t}} $$ My problem is that I do not understand the role of each term in the expansion. Substitution by power series is an attractive idea, but I have no deep intuition as to why we'd do this and hence, I'm having trouble putting it all together to understand the solution. Question How do I interpret the power series solution of the ODE? Hopefully this will allow me to reconcile my understanding of both these processes. I am not willing to accept this as pure coincidence .",,"['probability', 'ordinary-differential-equations', 'exponential-function', 'intuition']"
2,Qualitative solutions of KDV,Qualitative solutions of KDV,,"I'm reading Solitons: An introduction [1] , and in the chapter two (§2.2 p. 21), the author make a qualitative analysis of KDV: $u_t -6uu_x + u_{xxx} = 0$, then he try find solutions on this form $u(x,t)=f(x-ct)$, and then make the following substitution $\xi =x-ct$ and then we have: $-cf'-6ff'+f'''=0$, integrating twice we have: $$\frac{1}{2}(f')^2 = f^3 + \frac{1}{2}c f^2 + Af +B \equiv F(f)$$ where $A$ and $B$ are constants. So I don't understand this step (cf. book [1] p. 23). He wrote the Taylor series for $F(f)$ around $f_1$, where $f(\xi_1) = f_1$ and $F(f_1) = 0$ $$(f')^2 = 2(f-f_1)F'(f_1) + O((f-f_1)^2)) \tag{1}$$ And then: $$f = f_1 + \frac{1}{2}(\xi - \xi_1)^2F'(f_1) + O((\xi - \xi_1)^4) \tag{2}$$ I did not understand what he did in p. 23 from $(\text{1})$ to $(\text{2})$, someone can explain to me? [1] P.G. Drazin, R.S. Johnson, Solitons: An Introduction , Cambridge University Press, 1989","I'm reading Solitons: An introduction [1] , and in the chapter two (§2.2 p. 21), the author make a qualitative analysis of KDV: $u_t -6uu_x + u_{xxx} = 0$, then he try find solutions on this form $u(x,t)=f(x-ct)$, and then make the following substitution $\xi =x-ct$ and then we have: $-cf'-6ff'+f'''=0$, integrating twice we have: $$\frac{1}{2}(f')^2 = f^3 + \frac{1}{2}c f^2 + Af +B \equiv F(f)$$ where $A$ and $B$ are constants. So I don't understand this step (cf. book [1] p. 23). He wrote the Taylor series for $F(f)$ around $f_1$, where $f(\xi_1) = f_1$ and $F(f_1) = 0$ $$(f')^2 = 2(f-f_1)F'(f_1) + O((f-f_1)^2)) \tag{1}$$ And then: $$f = f_1 + \frac{1}{2}(\xi - \xi_1)^2F'(f_1) + O((\xi - \xi_1)^4) \tag{2}$$ I did not understand what he did in p. 23 from $(\text{1})$ to $(\text{2})$, someone can explain to me? [1] P.G. Drazin, R.S. Johnson, Solitons: An Introduction , Cambridge University Press, 1989",,"['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'soliton-theory']"
3,Equality at equilibrium of a ODE system arising in immunology,Equality at equilibrium of a ODE system arising in immunology,,"Consider the following system of ODE's [1] : \begin{align} \dot x & = \lambda - d_1 x - \frac{k_1 x v}{x + v}\\ \dot y & = \frac{k_1 x v}{x + v} - d_2 y - k_2 y \\ \dot s &= k_2 y - d_3s \\ \dot v &= as - d_4 v \end{align} where $x(t)$, $y(t)$, $s(t)$ and $v(t)$ represent the concentrations of uninfected CD4+ T cells, infected CD4+ T cells in latent stage, productively infected CD4+ T cells and free virus (HIV), respectively. Under some condition, this system has unique strictly positive equilibrium $Q_2 = (x^*, y^*, s^*, v^*)$ that is locally asymptotically stable. Assume this condition. Then at $Q_2$, the system becomes: \begin{align} 0 & = \lambda - d_1 x^* - \frac{k_1 x^* v^*}{x^* + v^*}\\ 0 & = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* \\ 0 &= k_2 y^* - d_3 s^* \\ 0 &= a s^* - d_4 v^* \end{align} My question is: what justified the following equality? (cf. left side of p. 99 in [1] ) $$ k_1 = \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$ Specifically, how are the following true? $$ \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = 1 $$ and  $$ \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$ I understand that the equation $ 0 = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* $ is the key, but I cannot figure out how the above two equalities are true. [1] Q. Sun, L. Min, Y. Kuang: ""Global stability of infection-free state and endemic infection state of a modified human immunodeficiency virus infection model"", IET Syst. Biol. 9 -3 (2015), 95–103 doi:10.1049/iet-syb.2014.0046","Consider the following system of ODE's [1] : \begin{align} \dot x & = \lambda - d_1 x - \frac{k_1 x v}{x + v}\\ \dot y & = \frac{k_1 x v}{x + v} - d_2 y - k_2 y \\ \dot s &= k_2 y - d_3s \\ \dot v &= as - d_4 v \end{align} where $x(t)$, $y(t)$, $s(t)$ and $v(t)$ represent the concentrations of uninfected CD4+ T cells, infected CD4+ T cells in latent stage, productively infected CD4+ T cells and free virus (HIV), respectively. Under some condition, this system has unique strictly positive equilibrium $Q_2 = (x^*, y^*, s^*, v^*)$ that is locally asymptotically stable. Assume this condition. Then at $Q_2$, the system becomes: \begin{align} 0 & = \lambda - d_1 x^* - \frac{k_1 x^* v^*}{x^* + v^*}\\ 0 & = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* \\ 0 &= k_2 y^* - d_3 s^* \\ 0 &= a s^* - d_4 v^* \end{align} My question is: what justified the following equality? (cf. left side of p. 99 in [1] ) $$ k_1 = \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$ Specifically, how are the following true? $$ \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = 1 $$ and  $$ \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$ I understand that the equation $ 0 = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* $ is the key, but I cannot figure out how the above two equalities are true. [1] Q. Sun, L. Min, Y. Kuang: ""Global stability of infection-free state and endemic infection state of a modified human immunodeficiency virus infection model"", IET Syst. Biol. 9 -3 (2015), 95–103 doi:10.1049/iet-syb.2014.0046",,"['ordinary-differential-equations', 'dynamical-systems', 'mathematical-modeling', 'biology']"
4,What is wrong with my Laplace transformation with cover-up method?,What is wrong with my Laplace transformation with cover-up method?,,"I have a two-compartment ODE system defined as: $\begin{cases} \frac{dC_1(t)}{dt}=\frac{Rate}{V_1}-\frac{Q\cdot C_1(t)}{V_1}+\frac{Q\cdot C_2(t)}{V_1}-\frac{CL\cdot C_1(t)}{V_1} \quad\quad...(1)\\ \frac{dC_2(t)}{dt}=\frac{Q\cdot C_1(t)}{V_2}-\frac{Q\cdot C_2(t)}{V_2} \quad\quad...(2)\end{cases}$    where $C_1(0)=C_2(0)=0$ I applied the Laplace transformation to each equation and got, respectively: $\begin{cases} s \cdot L_{C_1}(s)=\frac{Rate}{V_1 \cdot s}-\frac{Q\cdot L_{C_1}(s)}{V_1}+\frac{Q\cdot L_{C_2}(s)}{V_1}-\frac{CL\cdot L_{C_1}(s)}{V_1} \quad\quad...(3)\\ s \cdot L_{C_2}(s)=\frac{Q\cdot L_{C_1}(s)}{V_2}-\frac{Q\cdot L_{C_2}(s)}{V_2} \quad\quad...(4)\end{cases}$ Solving (3) and (4) gives the following form for $C_1$: $L_{C_1}(s)=\frac{(Rate \cdot V_2 + V_1 \cdot Q)s+Rate \cdot Q}{s(s-\alpha)(s-\beta)}\quad\quad\quad...(5)$ where $\{\alpha,\beta\}=-\frac{1}{2}[\frac{Q}{V_2}+\frac{Q}{V_1}+\frac{CL}{V_1}\pm\sqrt{(\frac{Q}{V_2}+\frac{Q}{V_1}+\frac{CL}{V_1})^2-4 \frac{CL \cdot Q}{V_1 \cdot V_2}}]$ Using the cover-up method for (5) gives: $C_1(t)=\frac{Rate \cdot Q}{\alpha\beta}+\frac{(Rate \cdot V_2 + V_1 \cdot Q)\alpha+Rate\cdot Q}{\alpha(\alpha-\beta)}e^{\alpha t}+\frac{(Rate \cdot V_2 + V_1 \cdot Q)\beta+Rate\cdot Q}{\beta(\beta-\alpha)}e^{\beta t}\quad\quad...(6)$ However, when I compare the results of numerical estimation of $C_1(t)$ against the analytical solution for $C_1(t)$ that I derived (i.e. equation 6), my version consistently gives larger values that differs by a constant ratio of $V_1V_2$. Can anyone point out my mistake in the derivation? =========================================================== EDIT: I am able to reach $L_{C_{1}}(s)=\frac{\text{Rate}\,(Q+s V_2)}{s(CL(Q+s V_2)+s(s V_1 V_2+Q(V_1+V_2)))}$ On using the cover-up method, this time I got: $C_1(t)=Rate(\frac{Q}{\alpha\beta}+\frac{Q-V_2 \alpha}{\alpha(\alpha-\beta)}e^{- \alpha t}+\frac{Q-V_2 \beta}{\beta(\beta-\alpha)}e^{-\beta t})\quad\quad...(6)$ But it is giving me the same problem: The value is inflated than the expected values by a ratio of $V_1 V_2$... I rearrange the analytic solution from a reference source and confirm that this observation is correct - the solution in the reference contains the $\frac{1}{V_1 V_2}$ term which is missing from my equation. Anyway, let me put down the R codes that can illustrate the above discrepancies I have been describing: CL <- 5 V1 <- 19 Rate <- 1000 Q <- 0.05 V2 <- 5.263157895 time <- seq(0, 48, 1)  library(deSolve) library(dplyr)  #deSolve function comp2Inf <- function(t, state, parameters) {   with(as.list(c(state, parameters)), {     #Differential equations     dC1 <- Rate/V1 - C1*(CL/V1) - C1*(Q/V1) + C2*(Q/V1)     dC2 <- C1*(Q/V2) - C2*(Q/V2)     #return rate of change     list(c(dC1, dC2))   }) }  ref_fcn <- function(times, Rate, CL, V1, Q, V2) {   b = (Q/V1+Q/V2+CL/V1-sqrt((Q/V1+Q/V2+CL/V1)**2-4*Q*CL/V1/V2))/2   a = Q*CL/V1/V2/b   A = (a-Q/V2)/V1/(a-b)   B = (b-Q/V2)/V1/(b-a)   return(sapply(times, function(t) {     Rate*(A/a*(1-exp(-a*t))+B/b*(1-exp(-b*t)))   })) }  my_fcn <- function(times, Rate, CL, V1, Q, V2) {   a = (Q/V2+Q/V1+CL/V1-sqrt((Q/V2+Q/V1+CL/V1)**2-4*CL*Q/V1/V2))/2   b = (Q/V2+Q/V1+CL/V1+sqrt((Q/V2+Q/V1+CL/V1)**2-4*CL*Q/V1/V2))/2   return(sapply(times, function(t) {     Rate*(Q/a/b+(Q-(V2*a))*exp(-a*t)/a/(a-b)+(Q-(V2*b))*exp(-b*t)/b/(b-a))   })) }  tab_deS <- ode(y = c(C1=0,C2=0), times = time, func = comp2Inf, parms = list(Rate=Rate, CL=CL, V1=V1, Q=Q, V2=V2)) %>% as.matrix arr_ref <- ref_fcn(time, Rate, CL, V1, Q, V2) arr_my <- my_fcn(time, Rate, CL, V1, Q, V2)  data.frame(time=time, deS=tab_deS[,2], ref=arr_ref, my=arr_my) In the code, I generated 3 arrays of values Using the numerical estimation solution by R package deSolve Using a reference source Using my derivation the code outputs a table for comparison.","I have a two-compartment ODE system defined as: $\begin{cases} \frac{dC_1(t)}{dt}=\frac{Rate}{V_1}-\frac{Q\cdot C_1(t)}{V_1}+\frac{Q\cdot C_2(t)}{V_1}-\frac{CL\cdot C_1(t)}{V_1} \quad\quad...(1)\\ \frac{dC_2(t)}{dt}=\frac{Q\cdot C_1(t)}{V_2}-\frac{Q\cdot C_2(t)}{V_2} \quad\quad...(2)\end{cases}$    where $C_1(0)=C_2(0)=0$ I applied the Laplace transformation to each equation and got, respectively: $\begin{cases} s \cdot L_{C_1}(s)=\frac{Rate}{V_1 \cdot s}-\frac{Q\cdot L_{C_1}(s)}{V_1}+\frac{Q\cdot L_{C_2}(s)}{V_1}-\frac{CL\cdot L_{C_1}(s)}{V_1} \quad\quad...(3)\\ s \cdot L_{C_2}(s)=\frac{Q\cdot L_{C_1}(s)}{V_2}-\frac{Q\cdot L_{C_2}(s)}{V_2} \quad\quad...(4)\end{cases}$ Solving (3) and (4) gives the following form for $C_1$: $L_{C_1}(s)=\frac{(Rate \cdot V_2 + V_1 \cdot Q)s+Rate \cdot Q}{s(s-\alpha)(s-\beta)}\quad\quad\quad...(5)$ where $\{\alpha,\beta\}=-\frac{1}{2}[\frac{Q}{V_2}+\frac{Q}{V_1}+\frac{CL}{V_1}\pm\sqrt{(\frac{Q}{V_2}+\frac{Q}{V_1}+\frac{CL}{V_1})^2-4 \frac{CL \cdot Q}{V_1 \cdot V_2}}]$ Using the cover-up method for (5) gives: $C_1(t)=\frac{Rate \cdot Q}{\alpha\beta}+\frac{(Rate \cdot V_2 + V_1 \cdot Q)\alpha+Rate\cdot Q}{\alpha(\alpha-\beta)}e^{\alpha t}+\frac{(Rate \cdot V_2 + V_1 \cdot Q)\beta+Rate\cdot Q}{\beta(\beta-\alpha)}e^{\beta t}\quad\quad...(6)$ However, when I compare the results of numerical estimation of $C_1(t)$ against the analytical solution for $C_1(t)$ that I derived (i.e. equation 6), my version consistently gives larger values that differs by a constant ratio of $V_1V_2$. Can anyone point out my mistake in the derivation? =========================================================== EDIT: I am able to reach $L_{C_{1}}(s)=\frac{\text{Rate}\,(Q+s V_2)}{s(CL(Q+s V_2)+s(s V_1 V_2+Q(V_1+V_2)))}$ On using the cover-up method, this time I got: $C_1(t)=Rate(\frac{Q}{\alpha\beta}+\frac{Q-V_2 \alpha}{\alpha(\alpha-\beta)}e^{- \alpha t}+\frac{Q-V_2 \beta}{\beta(\beta-\alpha)}e^{-\beta t})\quad\quad...(6)$ But it is giving me the same problem: The value is inflated than the expected values by a ratio of $V_1 V_2$... I rearrange the analytic solution from a reference source and confirm that this observation is correct - the solution in the reference contains the $\frac{1}{V_1 V_2}$ term which is missing from my equation. Anyway, let me put down the R codes that can illustrate the above discrepancies I have been describing: CL <- 5 V1 <- 19 Rate <- 1000 Q <- 0.05 V2 <- 5.263157895 time <- seq(0, 48, 1)  library(deSolve) library(dplyr)  #deSolve function comp2Inf <- function(t, state, parameters) {   with(as.list(c(state, parameters)), {     #Differential equations     dC1 <- Rate/V1 - C1*(CL/V1) - C1*(Q/V1) + C2*(Q/V1)     dC2 <- C1*(Q/V2) - C2*(Q/V2)     #return rate of change     list(c(dC1, dC2))   }) }  ref_fcn <- function(times, Rate, CL, V1, Q, V2) {   b = (Q/V1+Q/V2+CL/V1-sqrt((Q/V1+Q/V2+CL/V1)**2-4*Q*CL/V1/V2))/2   a = Q*CL/V1/V2/b   A = (a-Q/V2)/V1/(a-b)   B = (b-Q/V2)/V1/(b-a)   return(sapply(times, function(t) {     Rate*(A/a*(1-exp(-a*t))+B/b*(1-exp(-b*t)))   })) }  my_fcn <- function(times, Rate, CL, V1, Q, V2) {   a = (Q/V2+Q/V1+CL/V1-sqrt((Q/V2+Q/V1+CL/V1)**2-4*CL*Q/V1/V2))/2   b = (Q/V2+Q/V1+CL/V1+sqrt((Q/V2+Q/V1+CL/V1)**2-4*CL*Q/V1/V2))/2   return(sapply(times, function(t) {     Rate*(Q/a/b+(Q-(V2*a))*exp(-a*t)/a/(a-b)+(Q-(V2*b))*exp(-b*t)/b/(b-a))   })) }  tab_deS <- ode(y = c(C1=0,C2=0), times = time, func = comp2Inf, parms = list(Rate=Rate, CL=CL, V1=V1, Q=Q, V2=V2)) %>% as.matrix arr_ref <- ref_fcn(time, Rate, CL, V1, Q, V2) arr_my <- my_fcn(time, Rate, CL, V1, Q, V2)  data.frame(time=time, deS=tab_deS[,2], ref=arr_ref, my=arr_my) In the code, I generated 3 arrays of values Using the numerical estimation solution by R package deSolve Using a reference source Using my derivation the code outputs a table for comparison.",,"['integration', 'ordinary-differential-equations', 'laplace-transform']"
5,Second order autonomous system,Second order autonomous system,,"I am looking for a periodic solution (with period $2\pi m$, for some integer $m$) to the following ODE $$\ddot{x}+x-\frac{1}{x}=0.$$ ($x(t)=\pm 1$ trivially solves this equation). Note that this system is integrable, so the quantity $$E=\dot{x}^2+x^2-\log x^2$$ is constant for any solution. Separating variables and integrating gives $$t=\int\frac{dx}{\sqrt{E-x^2+\log x^2}}.$$ However, I haven't been able to solve the integral. Maybe thinking of this as a physical system with potential $V(x)=x^2-\log x^2$ would be useful (though I haven't been able to employ this fact). I'd appreciate any suggestions.","I am looking for a periodic solution (with period $2\pi m$, for some integer $m$) to the following ODE $$\ddot{x}+x-\frac{1}{x}=0.$$ ($x(t)=\pm 1$ trivially solves this equation). Note that this system is integrable, so the quantity $$E=\dot{x}^2+x^2-\log x^2$$ is constant for any solution. Separating variables and integrating gives $$t=\int\frac{dx}{\sqrt{E-x^2+\log x^2}}.$$ However, I haven't been able to solve the integral. Maybe thinking of this as a physical system with potential $V(x)=x^2-\log x^2$ would be useful (though I haven't been able to employ this fact). I'd appreciate any suggestions.",,"['ordinary-differential-equations', 'dynamical-systems']"
6,I lose a big class of solutions when making a differential equation homogenous substituting $y=z^m$,I lose a big class of solutions when making a differential equation homogenous substituting,y=z^m,"Solve $2y+(x^2 y + 1)xy'=0$. The book that contains this problem suggests a way of solving such differential equations - do a substitution $y=z^m$, $dy=m z^{m-1}dz$ with such $m$ that the differential equation becomes homogenous. Here $m=-2$ works. However if I substitute $y=z^{-2}$ I lose all information about solutions with $y \leq 0$. Let's say I do this substitution nonetheless. $$2z^{-2} + (x^2z^{-2}+1)x(-2)z^{-3}z'=0$$ $$z^{-2}dx = (x^3z^{-5} +x z^{-3})dz$$ Now it's homogenous. Substitute $t=\frac{x}{z}, dx=tdz+zdt$ $$tdz+zdt = (t^3+t)dz$$ $$\frac{dz}{z}=\frac{dt}{t^3}$$ Note that we lost $t=x=0$, but it's not a solution. $$\ln{|z|} = -\frac{1}{2t^2} + C,$$ Substitute back: $$\ln{\frac{1}{\sqrt{y}}} = -\frac{1}{x^2 y} + C.$$ Check whether $y=0$ is a solution - it is. But the book containing this problem says the solutions are $x^2 y \ln{Cy}=1$ and $y=0$. It allows for $y$ to be negative, but our solution does not. How to go about it?","Solve $2y+(x^2 y + 1)xy'=0$. The book that contains this problem suggests a way of solving such differential equations - do a substitution $y=z^m$, $dy=m z^{m-1}dz$ with such $m$ that the differential equation becomes homogenous. Here $m=-2$ works. However if I substitute $y=z^{-2}$ I lose all information about solutions with $y \leq 0$. Let's say I do this substitution nonetheless. $$2z^{-2} + (x^2z^{-2}+1)x(-2)z^{-3}z'=0$$ $$z^{-2}dx = (x^3z^{-5} +x z^{-3})dz$$ Now it's homogenous. Substitute $t=\frac{x}{z}, dx=tdz+zdt$ $$tdz+zdt = (t^3+t)dz$$ $$\frac{dz}{z}=\frac{dt}{t^3}$$ Note that we lost $t=x=0$, but it's not a solution. $$\ln{|z|} = -\frac{1}{2t^2} + C,$$ Substitute back: $$\ln{\frac{1}{\sqrt{y}}} = -\frac{1}{x^2 y} + C.$$ Check whether $y=0$ is a solution - it is. But the book containing this problem says the solutions are $x^2 y \ln{Cy}=1$ and $y=0$. It allows for $y$ to be negative, but our solution does not. How to go about it?",,"['ordinary-differential-equations', 'homogeneous-equation']"
7,Existence of solution for ODE,Existence of solution for ODE,,"The Question: Consider the inhomogeneous ODE $$y''(x)+y(x)=f(x) \; \; \; \; \; \; y(0)=0 \;,\;y(\pi)=1 \; \; \; \; \; \; 0<x<\pi$$ Give a condition on $f$ such that a solution exists My Thoughts: I get how if, for example, $f(x) = 0$ then the general solution is $y(x)=A\cos (x)+B\sin (x)$ so then no solutions exist. By the Fredholm Alternative Theorem, the homogeneous adjoint problem $$w''(x)+w(x)=0 \; \; \; \; \; \; w(0)=0 \;,\;w(\pi)=0 \; \; \; \; \; \; 0<x<\pi$$ has non-trivial solutions (namely $w(x)=C\sin (x)$), so that the original problem either has no solutions or non-unique solutions. How should I proceed?","The Question: Consider the inhomogeneous ODE $$y''(x)+y(x)=f(x) \; \; \; \; \; \; y(0)=0 \;,\;y(\pi)=1 \; \; \; \; \; \; 0<x<\pi$$ Give a condition on $f$ such that a solution exists My Thoughts: I get how if, for example, $f(x) = 0$ then the general solution is $y(x)=A\cos (x)+B\sin (x)$ so then no solutions exist. By the Fredholm Alternative Theorem, the homogeneous adjoint problem $$w''(x)+w(x)=0 \; \; \; \; \; \; w(0)=0 \;,\;w(\pi)=0 \; \; \; \; \; \; 0<x<\pi$$ has non-trivial solutions (namely $w(x)=C\sin (x)$), so that the original problem either has no solutions or non-unique solutions. How should I proceed?",,['ordinary-differential-equations']
8,Why is the solution of Clairaut's Differential Equation valid?,Why is the solution of Clairaut's Differential Equation valid?,,"Clairaut's Differential Equation is: $$y=xy^\prime+f(y^\prime)$$ where $f$ is supposed to be continuously differentiable. Every proof for the solution of this equation that I have seen starts by differentiating both sides of this equation; however, this makes $y^{\prime\prime}$ appear. Why can we assume that $y$ has a second derivative?","Clairaut's Differential Equation is: where is supposed to be continuously differentiable. Every proof for the solution of this equation that I have seen starts by differentiating both sides of this equation; however, this makes appear. Why can we assume that has a second derivative?",y=xy^\prime+f(y^\prime) f y^{\prime\prime} y,['ordinary-differential-equations']
9,Differential equation $x''+a(t)x'+b(t)x=0$ and wronskian,Differential equation  and wronskian,x''+a(t)x'+b(t)x=0,"Let this differential equation : $$(H)\qquad x''+tx'-(1+t)x=0$$ As $\phi_a=e^t$ , is a solution of $(H)$ , we can deduce $\phi_b$ from the wronskian $W(t)=\begin{array}{|ll|}\phi_a(t)&\phi_b(t) \\ \phi_a'(t) & \phi_b'(t)\end{array}$ Thus $\left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{W(t)}{(\phi_a(t))^2}$ As $W(t)$ is a solution of this equation $y'(t)=-t\cdot y(t)$ we have got $\displaystyle W(t)=\exp\bigg[-\int_{t_0}^t s\cdot ds\bigg]=e^{-t^2/2+t_0^2/2}=\lambda e^{-t^2/2}\qquad \lambda :=e^{t_0^2/2}$ So $\left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{\lambda e^{-t^2/2}}{e^{2t}}=\lambda\cdot e^{-t^2/2-2t}$ And now I'am blocked, because if $\displaystyle \gamma(t)=\int\lambda\cdot e^{-t^2/2-2t}\cdot dt$ , we have got $\dfrac{\phi_b(t)}{\phi_a(t)}=\gamma(t)$ So $\phi_b(t)=\phi_a(t)\cdot \gamma(t)$ , how to calculate $\gamma(t)$ ???","Let this differential equation : As , is a solution of , we can deduce from the wronskian Thus As is a solution of this equation we have got So And now I'am blocked, because if , we have got So , how to calculate ???",(H)\qquad x''+tx'-(1+t)x=0 \phi_a=e^t (H) \phi_b W(t)=\begin{array}{|ll|}\phi_a(t)&\phi_b(t) \\ \phi_a'(t) & \phi_b'(t)\end{array} \left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{W(t)}{(\phi_a(t))^2} W(t) y'(t)=-t\cdot y(t) \displaystyle W(t)=\exp\bigg[-\int_{t_0}^t s\cdot ds\bigg]=e^{-t^2/2+t_0^2/2}=\lambda e^{-t^2/2}\qquad \lambda :=e^{t_0^2/2} \left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{\lambda e^{-t^2/2}}{e^{2t}}=\lambda\cdot e^{-t^2/2-2t} \displaystyle \gamma(t)=\int\lambda\cdot e^{-t^2/2-2t}\cdot dt \dfrac{\phi_b(t)}{\phi_a(t)}=\gamma(t) \phi_b(t)=\phi_a(t)\cdot \gamma(t) \gamma(t),['ordinary-differential-equations']
10,Series solution to $e^xy''+xy=0$,Series solution to,e^xy''+xy=0,"Question Find the first two non-zero terms of each of two power series solution about the origin of $$e^xy''+xy=0$$ Attempt Let $$y= \sum_{n=0}^\infty a_nx^n$$ Substituting $y$ in the given equation gives. $$e^xy''+xy = \sum_{n=0}^\infty \frac{x^n}{n!}\sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n +\sum_{n=0}^\infty a_nx^{n+1}=0$$ Setting the coefficients to $0$ gives us  $$a_2 = 0;~~~ a_3 = -\frac{1}{6}a_0;~~~ a_4 = \frac{1}{12}\left(a_0-a_1\right)$$ Substituting it back into $y$ gives us  $$y = a_0 +a_1x-\frac{1}{6}a_0 x^3  +\frac{1}{12}\left(a_0-a_1\right) x^4$$ However the provided solution is  $$y = 1 -\frac{1}{6} x^3  +\frac{1}{12} x^4+...$$ $$y = x  +\frac{1}{12} x^4+...$$ Why are we supposed to assume $(a_0,a_1) = (1,0)$ for one solution and $(a_0,a_1) = (0,1)$? Is it because the solution set is a vector space with the basis $ (1,0), (0,1)$? Moreover, would such assumption (that the solution set is a vector space and hence can be written as linear combination of $y_1$ and $y_2$) be feasible if there was a value of $n$ for which $a_n$ was non-linear in $a_0$ or $a_1$ or at the very least, was $ca_0a_1$? (Since the coefficient would no longer remain and hence would not be a vector space","Question Find the first two non-zero terms of each of two power series solution about the origin of $$e^xy''+xy=0$$ Attempt Let $$y= \sum_{n=0}^\infty a_nx^n$$ Substituting $y$ in the given equation gives. $$e^xy''+xy = \sum_{n=0}^\infty \frac{x^n}{n!}\sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n +\sum_{n=0}^\infty a_nx^{n+1}=0$$ Setting the coefficients to $0$ gives us  $$a_2 = 0;~~~ a_3 = -\frac{1}{6}a_0;~~~ a_4 = \frac{1}{12}\left(a_0-a_1\right)$$ Substituting it back into $y$ gives us  $$y = a_0 +a_1x-\frac{1}{6}a_0 x^3  +\frac{1}{12}\left(a_0-a_1\right) x^4$$ However the provided solution is  $$y = 1 -\frac{1}{6} x^3  +\frac{1}{12} x^4+...$$ $$y = x  +\frac{1}{12} x^4+...$$ Why are we supposed to assume $(a_0,a_1) = (1,0)$ for one solution and $(a_0,a_1) = (0,1)$? Is it because the solution set is a vector space with the basis $ (1,0), (0,1)$? Moreover, would such assumption (that the solution set is a vector space and hence can be written as linear combination of $y_1$ and $y_2$) be feasible if there was a value of $n$ for which $a_n$ was non-linear in $a_0$ or $a_1$ or at the very least, was $ca_0a_1$? (Since the coefficient would no longer remain and hence would not be a vector space",,['ordinary-differential-equations']
11,Linear Differential Equation for Performance Level,Linear Differential Equation for Performance Level,,"I'm solving a question from my Calculus textbook that asks the following: Let $P(t)$ be the performance level of someone learning a skill as a function of the training time $t.$ The graph of $P$ is called a learning curve. In Exercise 9.1.15 we proposed the differential equation $\frac{dP}{dt} = k[M-P(t)]$ as a reasonable model for learning, where $k$ is a positive constant. Solve it as a linear differential equation and use your solution to graph the learning curve. My solution was to use the fact that a linear differential equation can be written in the following form: $\frac{dy}{dx} + yP(x) = Q(x)$ where $t$ is the independent variable, and $P$ is the dependent variable (so $P(t)$ is the like 'y' in this case.) I rewrote the given differential equation for the performance level as such: $\frac{dP}{dt} + kP(t) = kM$ Thus, my $k$ is like my ""function of x"", and my $P(t)$ is my ""y."" By setting $e$ to the power of the integral of $k$ , I got $e^{kt}$ as my integration factor. I multiplied both sides of my differential equation by $e^{kt}$ , and then rewrote the left side as $(e^{kt} · P(t))'$ . I integrated both sides, and factored out $k$ and $M$ since they're both constants. The answer I got for $P(t)$ is $M + ce^{-kt}.$ So for my actual question: Is this correct? I'm not very sure about my solution to this question. Also, is my logic for each of my steps sound and reasonable? Thank you so much for reading!","I'm solving a question from my Calculus textbook that asks the following: Let be the performance level of someone learning a skill as a function of the training time The graph of is called a learning curve. In Exercise 9.1.15 we proposed the differential equation as a reasonable model for learning, where is a positive constant. Solve it as a linear differential equation and use your solution to graph the learning curve. My solution was to use the fact that a linear differential equation can be written in the following form: where is the independent variable, and is the dependent variable (so is the like 'y' in this case.) I rewrote the given differential equation for the performance level as such: Thus, my is like my ""function of x"", and my is my ""y."" By setting to the power of the integral of , I got as my integration factor. I multiplied both sides of my differential equation by , and then rewrote the left side as . I integrated both sides, and factored out and since they're both constants. The answer I got for is So for my actual question: Is this correct? I'm not very sure about my solution to this question. Also, is my logic for each of my steps sound and reasonable? Thank you so much for reading!",P(t) t. P \frac{dP}{dt} = k[M-P(t)] k \frac{dy}{dx} + yP(x) = Q(x) t P P(t) \frac{dP}{dt} + kP(t) = kM k P(t) e k e^{kt} e^{kt} (e^{kt} · P(t))' k M P(t) M + ce^{-kt}.,"['ordinary-differential-equations', 'applications']"
12,Differential equations -- exact solution to nonlinear second order ODE,Differential equations -- exact solution to nonlinear second order ODE,,"I get a problem with the following nonlinear second order ODE: $y^2\dfrac{d^2u(y)}{dy^2}+(\Sigma-\rho y^\lambda-\Omega y)y\dfrac{du(y)}{dy}-(\alpha\rho y^\lambda+\beta \Omega y) u(y)=0$, where $\Sigma, \rho,\Omega,\alpha,\beta,\lambda$ are constants, and $\lambda$ is positive real number. I want to ask for the exact solution to it. For example, for the special case that $\rho=0$, one of the exact solutions is $M(\beta,\Sigma,\Omega y)$ in the form of Kummer function.","I get a problem with the following nonlinear second order ODE: $y^2\dfrac{d^2u(y)}{dy^2}+(\Sigma-\rho y^\lambda-\Omega y)y\dfrac{du(y)}{dy}-(\alpha\rho y^\lambda+\beta \Omega y) u(y)=0$, where $\Sigma, \rho,\Omega,\alpha,\beta,\lambda$ are constants, and $\lambda$ is positive real number. I want to ask for the exact solution to it. For example, for the special case that $\rho=0$, one of the exact solutions is $M(\beta,\Sigma,\Omega y)$ in the form of Kummer function.",,"['ordinary-differential-equations', 'hypergeometric-function']"
13,On proving that $f(x) = f'(x)\iff f(x) = e^x$. (Not aware of a possible duplicate.) [duplicate],On proving that . (Not aware of a possible duplicate.) [duplicate],f(x) = f'(x)\iff f(x) = e^x,"This question already has answers here : Are the any non-trivial functions where $f(x)=f'(x)$ not of the form $Ae^x$ (6 answers) Closed 6 years ago . This is just a curious question, but is the following true? $$f(x) = f'(x)\iff f(x) = e^x.$$ I can prove that $\dfrac{\mathrm d}{\mathrm dx}\left(e^x\right) = e^x$ from using the the formula, $e^x := \operatorname*{\lim}\limits_{n\to 0}(1+n)^{1/n}.$ For those who are not familiar with the proof, it can be found here . Or, you can go here for a similar approach. However, my question is asking whether or not $f(x) = e^x$ is the only function equal to its own deriv. I suspect it is true, but how can we prove that $e^x$ is the only value equal to its derivative, for any $x$? I consider it very likely that there exists another question out there, perhaps exactly like this. If so, please comment the link below, and I will go straight to it, delete this post, and give you a muffin. I do not intend on trolling or wasting anyone's time. Thank you in advance.","This question already has answers here : Are the any non-trivial functions where $f(x)=f'(x)$ not of the form $Ae^x$ (6 answers) Closed 6 years ago . This is just a curious question, but is the following true? $$f(x) = f'(x)\iff f(x) = e^x.$$ I can prove that $\dfrac{\mathrm d}{\mathrm dx}\left(e^x\right) = e^x$ from using the the formula, $e^x := \operatorname*{\lim}\limits_{n\to 0}(1+n)^{1/n}.$ For those who are not familiar with the proof, it can be found here . Or, you can go here for a similar approach. However, my question is asking whether or not $f(x) = e^x$ is the only function equal to its own deriv. I suspect it is true, but how can we prove that $e^x$ is the only value equal to its derivative, for any $x$? I consider it very likely that there exists another question out there, perhaps exactly like this. If so, please comment the link below, and I will go straight to it, delete this post, and give you a muffin. I do not intend on trolling or wasting anyone's time. Thank you in advance.",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'proof-writing', 'exponential-function']"
14,finding a particular solution of $\frac{dy}{dx}=\frac{3y}{x}$,finding a particular solution of,\frac{dy}{dx}=\frac{3y}{x},"I need a help with the solving the differential equation explicitly. $\frac{dy}{dx}=\frac{3y}{x}$, $y(1)=1$ and here is what I have so far: $\frac{1}{y}dy=\frac{3}{x}dx$ (separate the variables) $\int\frac{1}{y}\,dy=\int\frac{3}{x}\,dx$ (integrate) $\ln\vert y\vert =3\ln\vert x\vert +C$ substitute the initial condition and get $C=0$ $\ln\vert y\vert =3\ln\vert x\vert$ $e^{\ln\vert y\vert}=e^{3\ln\vert x\vert}$ $\vert y\vert=\vert x\vert ^3$ $y=\pm\vert x\vert ^3$ $y=\vert x\vert ^3$ (choosing + judging by the initial condition) and this is where I'm stuck. The answer key says it's $y=x^3$. -EDIT- now the next part of the question is asking what the domain of the particular solution is. The answer says it's $(0,\,\infty)$. I understand the initial condition of $x=1$ is in there, but I don't understand why I need to choose it, ignoring $(-\infty,\,0)$. If the problem stated the particular solution has no removable discontinuity, then I agree with their answer. But for now, I keep getting the domain of $x\neq 0$. -EDIT2- After more thorough research about this, I found that a solution to a differentiable equation is supposed to be ""continuously differentiable function"". Thus, only choosing a continuous interval that contains the initial condition. This is well-discussed in DE textbooks, but rarely in Calculus textbooks (The Chair of AP Calculus Development Committee agrees on this). For example, Stewart's Calculus textbooks don't even mention about the domain of solutions (granted, those textbooks are not designed to prep for AP exams). But I just hope that this is discussed by most instructors. Source: http://ecademy.agnesscott.edu/~lriddle/apcalculus/apcentral-domain.pdf","I need a help with the solving the differential equation explicitly. $\frac{dy}{dx}=\frac{3y}{x}$, $y(1)=1$ and here is what I have so far: $\frac{1}{y}dy=\frac{3}{x}dx$ (separate the variables) $\int\frac{1}{y}\,dy=\int\frac{3}{x}\,dx$ (integrate) $\ln\vert y\vert =3\ln\vert x\vert +C$ substitute the initial condition and get $C=0$ $\ln\vert y\vert =3\ln\vert x\vert$ $e^{\ln\vert y\vert}=e^{3\ln\vert x\vert}$ $\vert y\vert=\vert x\vert ^3$ $y=\pm\vert x\vert ^3$ $y=\vert x\vert ^3$ (choosing + judging by the initial condition) and this is where I'm stuck. The answer key says it's $y=x^3$. -EDIT- now the next part of the question is asking what the domain of the particular solution is. The answer says it's $(0,\,\infty)$. I understand the initial condition of $x=1$ is in there, but I don't understand why I need to choose it, ignoring $(-\infty,\,0)$. If the problem stated the particular solution has no removable discontinuity, then I agree with their answer. But for now, I keep getting the domain of $x\neq 0$. -EDIT2- After more thorough research about this, I found that a solution to a differentiable equation is supposed to be ""continuously differentiable function"". Thus, only choosing a continuous interval that contains the initial condition. This is well-discussed in DE textbooks, but rarely in Calculus textbooks (The Chair of AP Calculus Development Committee agrees on this). For example, Stewart's Calculus textbooks don't even mention about the domain of solutions (granted, those textbooks are not designed to prep for AP exams). But I just hope that this is discussed by most instructors. Source: http://ecademy.agnesscott.edu/~lriddle/apcalculus/apcentral-domain.pdf",,"['calculus', 'ordinary-differential-equations']"
15,"$n$th order linear IVP with explicit time dependent coefficients, existence and uniqueness","th order linear IVP with explicit time dependent coefficients, existence and uniqueness",n,"I am looking for the proof that the following $n$th order ODE $$ y^{(n)}(t)+p_{n-1}(t)y^{(n-1)}(t)+\cdots+p_1(t)y'(t)+p_0(t)y(t)=g(t) $$ for $t\in (a,b)$ and $p_i$ and $g$ continuous on $(a,b)$ with  $$ y(t_0)=y_0,y'(t_0)=y_0',\dots,y^{(n-1)}(t_0)=y_0^{(n-1)} $$ for some $t_0\in (a,b)$ has a unique solution on the entire interval $(a,b)$. This is as stated in Boyce and De Prima. However, a proof is not offered. Furthermore, I dug up the referenced text and couldn't find a proof there either (the edition was different, so it is possible I missed it). Hirsch Smale and Devaney just asserts a result for non explicitly $t$ dependent coefficients and then moves on to fully nonlinear equations and dynamics. I am hoping for a more nitty gritty proof. And for convenience, I would be fine with taking $n=2$ if it extrapolates nicely. My ideas to proceed; we can certainly reduce this to a system of first order ode's (and I think it suffices to examine the homogeneous version by variation of parameters) and try and prove existence and uniqueness in the first order cases. However, what troubles me is I don't see how to use Picard's theorem to get existence and uniqueness on the whole interval where all the $p_i$ are continuous. Wouldn't we need to restrict to a (sufficiently small) compact subset to make sure the integral operator we define is a contraction for the Banach fixed point theorem argument to go through? I confess I haven't worked out all the details myself, so I apologize if I missed something obvious. I am also not that familiar with the Wronskian, and maybe that's where the solution will come from.","I am looking for the proof that the following $n$th order ODE $$ y^{(n)}(t)+p_{n-1}(t)y^{(n-1)}(t)+\cdots+p_1(t)y'(t)+p_0(t)y(t)=g(t) $$ for $t\in (a,b)$ and $p_i$ and $g$ continuous on $(a,b)$ with  $$ y(t_0)=y_0,y'(t_0)=y_0',\dots,y^{(n-1)}(t_0)=y_0^{(n-1)} $$ for some $t_0\in (a,b)$ has a unique solution on the entire interval $(a,b)$. This is as stated in Boyce and De Prima. However, a proof is not offered. Furthermore, I dug up the referenced text and couldn't find a proof there either (the edition was different, so it is possible I missed it). Hirsch Smale and Devaney just asserts a result for non explicitly $t$ dependent coefficients and then moves on to fully nonlinear equations and dynamics. I am hoping for a more nitty gritty proof. And for convenience, I would be fine with taking $n=2$ if it extrapolates nicely. My ideas to proceed; we can certainly reduce this to a system of first order ode's (and I think it suffices to examine the homogeneous version by variation of parameters) and try and prove existence and uniqueness in the first order cases. However, what troubles me is I don't see how to use Picard's theorem to get existence and uniqueness on the whole interval where all the $p_i$ are continuous. Wouldn't we need to restrict to a (sufficiently small) compact subset to make sure the integral operator we define is a contraction for the Banach fixed point theorem argument to go through? I confess I haven't worked out all the details myself, so I apologize if I missed something obvious. I am also not that familiar with the Wronskian, and maybe that's where the solution will come from.",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'reference-request']"
16,Relation between independent/dependent variable for substitution in ODE,Relation between independent/dependent variable for substitution in ODE,,"I have the following ODE: $$ y'=y^2\cos t-\frac{1}{10}y $$ substituting $z=1/y$, I understand this is $z(t)=1/y(t)$. I know $t$ is the independent variable and $y$ is dependent variable. What is the role of $z$ here? Does it change the dependent variable when $y(t)=1/z(t)$? I saw in the book that it is written: $$ y'=\frac{-z'}{z^2} $$ does this mean that: $$ y'(t)=\frac{dz}{dy}\frac{dy}{dt} $$ is the relation $z(y(t))$? Can someone explain the implicit differentiation in this expression? What are the dependent and independent variables here? I understand how to finish this, but I want to know the intuition behind. I always confuse what are the dependent and independent variables when doing substitution, and to what variable I need to implicitly differentiate.","I have the following ODE: $$ y'=y^2\cos t-\frac{1}{10}y $$ substituting $z=1/y$, I understand this is $z(t)=1/y(t)$. I know $t$ is the independent variable and $y$ is dependent variable. What is the role of $z$ here? Does it change the dependent variable when $y(t)=1/z(t)$? I saw in the book that it is written: $$ y'=\frac{-z'}{z^2} $$ does this mean that: $$ y'(t)=\frac{dz}{dy}\frac{dy}{dt} $$ is the relation $z(y(t))$? Can someone explain the implicit differentiation in this expression? What are the dependent and independent variables here? I understand how to finish this, but I want to know the intuition behind. I always confuse what are the dependent and independent variables when doing substitution, and to what variable I need to implicitly differentiate.",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'chain-rule']"
17,Solve the wave equation in an infinite sector,Solve the wave equation in an infinite sector,,"Consider the following wave equation:   $$\begin{align} u_{tt}-u_{xx} &=0, \quad 0<t<tx, k>1\\ u|_{t=0}&=\phi_0(x),\quad x\ge 0\\ u_t|_{t=0}&=\phi_1(x),\quad x\ge 0\\ u|_{t=kx}&=\psi(x)\end{align} $$   In which $\phi_0(0)=\psi(0)$. The problem is that on part of the sector where $t>x$ d'Alembert's formula isn't applicable. It seems we will have to do some sort of extension or reflection, but how to start?","Consider the following wave equation:   $$\begin{align} u_{tt}-u_{xx} &=0, \quad 0<t<tx, k>1\\ u|_{t=0}&=\phi_0(x),\quad x\ge 0\\ u_t|_{t=0}&=\phi_1(x),\quad x\ge 0\\ u|_{t=kx}&=\psi(x)\end{align} $$   In which $\phi_0(0)=\psi(0)$. The problem is that on part of the sector where $t>x$ d'Alembert's formula isn't applicable. It seems we will have to do some sort of extension or reflection, but how to start?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
18,"Proving Sturm's separation theorem: if $y_1, y_2$ are fundamental solutions of $y''+py+q=0$, their zeros alternate.","Proving Sturm's separation theorem: if  are fundamental solutions of , their zeros alternate.","y_1, y_2 y''+py+q=0","This is a problem from Braun's Differential Equations book. Let $p(t)$ and $q(t)$ continuous functions and $y_{1}$, $y_{2}$ a fundamental set of solutions of the ODE \begin{equation} y''+p(t)y'+q(t)y=0 \end{equation} in the interval $t\in(-\infty,\infty)$. Prove that there is only one zero of $y_{1}$ between two consequtive zeros of $y_{2}$. Hint: Differentiate the expression $y_{2}/y_{1}$ and use Rolle's theorem. This is what I have. Let $a,b$ two consecutive zeros of $y_{2}$ with $a<b$. Since $y_{1}$, $y_{2}$ are a fundamental set of solutions, their Wronskian \begin{equation} W(y_{1},y_{2})=y_{1}y_{2}'-y_{1}'y_{2} \end{equation}  is not zero for all $t$. This implies that neither $a$ or $b$ are zeros of $y_{1}$. Then, I tried to use the hint computing the derivative of $y_{2}/y_{1}$, \begin{equation} \dfrac{d}{dt}(y_{2}/y_{1})=W(y_{1},y_{2})/y_{1}^{2} \end{equation} but then I have no idea of what to do, thanks for the help.","This is a problem from Braun's Differential Equations book. Let $p(t)$ and $q(t)$ continuous functions and $y_{1}$, $y_{2}$ a fundamental set of solutions of the ODE \begin{equation} y''+p(t)y'+q(t)y=0 \end{equation} in the interval $t\in(-\infty,\infty)$. Prove that there is only one zero of $y_{1}$ between two consequtive zeros of $y_{2}$. Hint: Differentiate the expression $y_{2}/y_{1}$ and use Rolle's theorem. This is what I have. Let $a,b$ two consecutive zeros of $y_{2}$ with $a<b$. Since $y_{1}$, $y_{2}$ are a fundamental set of solutions, their Wronskian \begin{equation} W(y_{1},y_{2})=y_{1}y_{2}'-y_{1}'y_{2} \end{equation}  is not zero for all $t$. This implies that neither $a$ or $b$ are zeros of $y_{1}$. Then, I tried to use the hint computing the derivative of $y_{2}/y_{1}$, \begin{equation} \dfrac{d}{dt}(y_{2}/y_{1})=W(y_{1},y_{2})/y_{1}^{2} \end{equation} but then I have no idea of what to do, thanks for the help.",,"['calculus', 'ordinary-differential-equations', 'roots']"
19,Finding Streamlines: How to solve this ODE? Is there a easier way?,Finding Streamlines: How to solve this ODE? Is there a easier way?,,"I was given the velocity vector field $\mathbf{\vec{v}}(x, y, z)$ and was asked to find the streamlines $\mathbf{\vec{r}}(t)$ in my exam: $$\mathbf{\vec{v}}(x, y, z) = -\dfrac yx \hat i + 4x \hat j + 0 \hat k$$ This is what I did: $$z(t) = \int 0 dt = C_1$$ $$\dfrac{dx}{dt} = -\dfrac {y(t)}{x(t)};\; \dfrac{dy}{dt} = 4x(t)$$ But these two equations are coupled. So, I tried to decouple them like this: $$y = -x\dfrac{dx}{dt}$$ $$\implies \dfrac{dy}{dt} = -\left(\frac{dx}{dt}\right)^2 - x\dfrac{d^2x}{dt^2}$$ $$\implies x\dfrac{d^2x}{dt^2} + \left(\frac{dx}{dt}\right)^2 + 4x = 0$$ Now, what do I do? Is there a easier way?","I was given the velocity vector field $\mathbf{\vec{v}}(x, y, z)$ and was asked to find the streamlines $\mathbf{\vec{r}}(t)$ in my exam: $$\mathbf{\vec{v}}(x, y, z) = -\dfrac yx \hat i + 4x \hat j + 0 \hat k$$ This is what I did: $$z(t) = \int 0 dt = C_1$$ $$\dfrac{dx}{dt} = -\dfrac {y(t)}{x(t)};\; \dfrac{dy}{dt} = 4x(t)$$ But these two equations are coupled. So, I tried to decouple them like this: $$y = -x\dfrac{dx}{dt}$$ $$\implies \dfrac{dy}{dt} = -\left(\frac{dx}{dt}\right)^2 - x\dfrac{d^2x}{dt^2}$$ $$\implies x\dfrac{d^2x}{dt^2} + \left(\frac{dx}{dt}\right)^2 + 4x = 0$$ Now, what do I do? Is there a easier way?",,"['ordinary-differential-equations', 'physics']"
20,Solutions with same points of inflection,Solutions with same points of inflection,,"Given $y'' + p(t)y'+ q(t)y = 0$, where the functions $p(t)$ and $q(t)$ are continuous on the whole real line and suppose that $p(t) > 0$ and $q(t) > 0$ for all values of $t$. Is it possible that its two linearly independent solutions have a point of inflection for the same value of $t$?","Given $y'' + p(t)y'+ q(t)y = 0$, where the functions $p(t)$ and $q(t)$ are continuous on the whole real line and suppose that $p(t) > 0$ and $q(t) > 0$ for all values of $t$. Is it possible that its two linearly independent solutions have a point of inflection for the same value of $t$?",,['ordinary-differential-equations']
21,Solve the following differential equations 2x2 with complex eigenvert,Solve the following differential equations 2x2 with complex eigenvert,,"Solve the following equation:   $$X'=\pmatrix { 2 & 5   \\ -5 & 8}X$$   $$x(0)=(5,5)^t$$ I already got out the followings: $$\lambda_{1,2} = 5 \pm  4i$$ $$v_{1} = \begin{pmatrix} 3 \\ 5 \end{pmatrix} - i\cdot \begin{pmatrix} 4 \\ 0 \end{pmatrix}$$ $$v_{1} = \begin{pmatrix} 3 \\ 5 \end{pmatrix} + i\cdot \begin{pmatrix} 4 \\ 0 \end{pmatrix}$$ And so far I have this(Sry its messy): $$\begin{align*}\vec{x}_{1}(t) = (3-4i)e^{5t} \begin{pmatrix}a\cos(4t)+b\sin(4t)\end{pmatrix}\end{align*}$$ $$\begin{align*}\vec{x}_{2}(t) = 5\cdot e^{5t} \begin{pmatrix}c\cos(4t)+d\sin(4t)\end{pmatrix}\end{align*}$$ I dont know if this is right so far... but I also tried to sum up x_1: My attend to solve x_1 This is the right answer in the end:$$\begin{align*}\vec{x}(t) = e^{5t} \begin{pmatrix}5\cos(4t)+\frac{5}{2}\sin(4t)\\ 5\cos(4t)-\frac{5}{2}\sin(4t)\end{pmatrix}\end{align*}$$ tyx","Solve the following equation:   $$X'=\pmatrix { 2 & 5   \\ -5 & 8}X$$   $$x(0)=(5,5)^t$$ I already got out the followings: $$\lambda_{1,2} = 5 \pm  4i$$ $$v_{1} = \begin{pmatrix} 3 \\ 5 \end{pmatrix} - i\cdot \begin{pmatrix} 4 \\ 0 \end{pmatrix}$$ $$v_{1} = \begin{pmatrix} 3 \\ 5 \end{pmatrix} + i\cdot \begin{pmatrix} 4 \\ 0 \end{pmatrix}$$ And so far I have this(Sry its messy): $$\begin{align*}\vec{x}_{1}(t) = (3-4i)e^{5t} \begin{pmatrix}a\cos(4t)+b\sin(4t)\end{pmatrix}\end{align*}$$ $$\begin{align*}\vec{x}_{2}(t) = 5\cdot e^{5t} \begin{pmatrix}c\cos(4t)+d\sin(4t)\end{pmatrix}\end{align*}$$ I dont know if this is right so far... but I also tried to sum up x_1: My attend to solve x_1 This is the right answer in the end:$$\begin{align*}\vec{x}(t) = e^{5t} \begin{pmatrix}5\cos(4t)+\frac{5}{2}\sin(4t)\\ 5\cos(4t)-\frac{5}{2}\sin(4t)\end{pmatrix}\end{align*}$$ tyx",,"['matrices', 'ordinary-differential-equations', 'trigonometry', 'complex-numbers', 'differential']"
22,Lower bound on integral of coefficient in an ODE,Lower bound on integral of coefficient in an ODE,,"Suppose that $\phi$ solves $x'' + A \left( t \right) x = 0$, is positive on $\left(0, b \right)$, and $\phi \left( 0 \right) = \phi \left( b \right) = 0$. ($A(t)$ is continuous) Is it true that $$\int_{0}^{b} \left| A \left( t \right) \right| dt > \frac{4}{b}$$ Any help or hints appreciated. One attempt: By the MVT for integrals, for some $0 < c < b$, $$\int_0^b \left| A \left( t \right) \right| dt = b \left| A \left( c \right) \right| \geq b \inf_{0 \leq t \leq c} \left| A \left( t \right) \right| $$ However, I think this is too restrictive. Another attempt: since $\int_0^b \left| A \left( t \right) \right| \phi \left( t \right) dt \leq M \int_0^b \left| A \left( t \right) \right| dt$, where $M = \sup \left| \phi \left( t \right) \right|$, $$\int_0^b \left| A \left( t \right) \right| dt \geq \frac{1}{M} \int_0^b \left| A \left( t \right) \right| \phi \left( t \right) dt = \frac{1}{M} \int_0^b \left| A \left( t \right) \phi \left( t \right) \right| dt = \frac{1}{M} \int_0^b \left| \phi'' \left( t \right) \right| dt \geq \frac{1}{M} \left| \int_0^b \phi'' \left( t \right) dt \right| = \frac{1}{M} \left| \phi' \left( b \right) - \phi' \left( 0 \right) \right| $$ or, again by the MVT for integrals, $$ = \frac{b}{M} \left| \phi'' \left( c \right) \right| $$ for some $0 < c < t$. Again, I'm not sure how to turn this into a solution. FINAL EDIT: To anyone finding this result in the future, this is a famous result by Lyapunov. One may complete my second attempt as follows: Apply Rolle's theorem to obtain $\alpha$ and $\beta$, $s$ and $t$ such that $x'(t) = -M / \alpha$ and $x' (s) = M / \beta$, and $\beta + \alpha = b$. An application of the AGM inequality finishes the job.","Suppose that $\phi$ solves $x'' + A \left( t \right) x = 0$, is positive on $\left(0, b \right)$, and $\phi \left( 0 \right) = \phi \left( b \right) = 0$. ($A(t)$ is continuous) Is it true that $$\int_{0}^{b} \left| A \left( t \right) \right| dt > \frac{4}{b}$$ Any help or hints appreciated. One attempt: By the MVT for integrals, for some $0 < c < b$, $$\int_0^b \left| A \left( t \right) \right| dt = b \left| A \left( c \right) \right| \geq b \inf_{0 \leq t \leq c} \left| A \left( t \right) \right| $$ However, I think this is too restrictive. Another attempt: since $\int_0^b \left| A \left( t \right) \right| \phi \left( t \right) dt \leq M \int_0^b \left| A \left( t \right) \right| dt$, where $M = \sup \left| \phi \left( t \right) \right|$, $$\int_0^b \left| A \left( t \right) \right| dt \geq \frac{1}{M} \int_0^b \left| A \left( t \right) \right| \phi \left( t \right) dt = \frac{1}{M} \int_0^b \left| A \left( t \right) \phi \left( t \right) \right| dt = \frac{1}{M} \int_0^b \left| \phi'' \left( t \right) \right| dt \geq \frac{1}{M} \left| \int_0^b \phi'' \left( t \right) dt \right| = \frac{1}{M} \left| \phi' \left( b \right) - \phi' \left( 0 \right) \right| $$ or, again by the MVT for integrals, $$ = \frac{b}{M} \left| \phi'' \left( c \right) \right| $$ for some $0 < c < t$. Again, I'm not sure how to turn this into a solution. FINAL EDIT: To anyone finding this result in the future, this is a famous result by Lyapunov. One may complete my second attempt as follows: Apply Rolle's theorem to obtain $\alpha$ and $\beta$, $s$ and $t$ such that $x'(t) = -M / \alpha$ and $x' (s) = M / \beta$, and $\beta + \alpha = b$. An application of the AGM inequality finishes the job.",,['ordinary-differential-equations']
23,"When either $\frac{\partial M}{\partial y}$, or $ \frac{\partial N}{\partial x}$ do not exists, how to check whether given ODE is exact or not","When either , or  do not exists, how to check whether given ODE is exact or not",\frac{\partial M}{\partial y}  \frac{\partial N}{\partial x},"For a given ODE, such as $$M(x,y) dx + N(x,y) dy = 0$$ if $\frac{\partial M}{\partial y} \not = \frac{\partial N}{\partial   x}$, how can we determine whether the given ODE is exact or not ?   Moreover, how can we find $F(x,y)$ ? To be clear, (I guess) we can always integrate $M$ wrt $x$, and then take the derivative of the resulting function wrt $y$, and look for a possible $g(y)$, where $g(y)$ is the intagration ""constant"" coming from the integration of $M$ wrt $x$, but my question is that is there any other methods ? Moreover, to check whether there exists a $g(y)$ satisfying the above conditions leads an integration, which might not be possible or easy all the time, so in such cases, what can we do ? Edit: I'm not talking about non-exact ODEs, I'm basically asking what to do when either $M$ or $N$ is not belong to $C^1$, as I have explain in the comments of the answer of @dezdichado.","For a given ODE, such as $$M(x,y) dx + N(x,y) dy = 0$$ if $\frac{\partial M}{\partial y} \not = \frac{\partial N}{\partial   x}$, how can we determine whether the given ODE is exact or not ?   Moreover, how can we find $F(x,y)$ ? To be clear, (I guess) we can always integrate $M$ wrt $x$, and then take the derivative of the resulting function wrt $y$, and look for a possible $g(y)$, where $g(y)$ is the intagration ""constant"" coming from the integration of $M$ wrt $x$, but my question is that is there any other methods ? Moreover, to check whether there exists a $g(y)$ satisfying the above conditions leads an integration, which might not be possible or easy all the time, so in such cases, what can we do ? Edit: I'm not talking about non-exact ODEs, I'm basically asking what to do when either $M$ or $N$ is not belong to $C^1$, as I have explain in the comments of the answer of @dezdichado.",,['ordinary-differential-equations']
24,When is the Euler-Lagrange equation trivially satisfied?,When is the Euler-Lagrange equation trivially satisfied?,,"In calculus of variations, we seek the functions $y(x)$ extremising the quantity $$I = \int_{x_1}^{x_2} F(x, y, y') \, dx.$$ To find these functions, we solve the Euler-Lagrange equation $$\frac{\partial F}{\partial y} = \frac{d}{dx}\left[\frac{\partial F}{\partial y'} \right].$$ Sometimes the E-L equation is trivially satisfied. This means that given boundary conditions $y(x_1) = y_1$ and $y(x_2)=y_2$, all functions $y(x)$ are extremals, i.e. $I$ is independent of the path $y$. In such a situation, I understand that the Lagrangian $F$ simplifies to a total derivative. Here are two examples: 1) $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx$. We compute $\frac{\partial F}{\partial y} = 2y + 2xy'$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ 2xy \right] = 2y + 2xy'$. We then show path-independence by writing $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ xy^2 \right] dx = \left[ xy^2 \right]_{x_1}^{x_2}$. 2) $I = \int_{x_1}^{x_2} (\frac{y'}{y}) dx$. We compute $\frac{\partial F}{\partial y} = -\frac{y'}{y^2}$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ \frac{1}{y} \right] = -\frac{y'}{y^2}$. We then show path-independence by writing $I = \int_{x_1}^{x_2} \frac{y'}{y} dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ \log |y| \right] dx = \left[ \log |y| \right]_{x_1}^{x_2}$. I want to know how to figure out what the Lagrangian is the total derivative of. In the two examples above I got the total derivatives by trial and error, but I would like to know what the answer is in general.","In calculus of variations, we seek the functions $y(x)$ extremising the quantity $$I = \int_{x_1}^{x_2} F(x, y, y') \, dx.$$ To find these functions, we solve the Euler-Lagrange equation $$\frac{\partial F}{\partial y} = \frac{d}{dx}\left[\frac{\partial F}{\partial y'} \right].$$ Sometimes the E-L equation is trivially satisfied. This means that given boundary conditions $y(x_1) = y_1$ and $y(x_2)=y_2$, all functions $y(x)$ are extremals, i.e. $I$ is independent of the path $y$. In such a situation, I understand that the Lagrangian $F$ simplifies to a total derivative. Here are two examples: 1) $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx$. We compute $\frac{\partial F}{\partial y} = 2y + 2xy'$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ 2xy \right] = 2y + 2xy'$. We then show path-independence by writing $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ xy^2 \right] dx = \left[ xy^2 \right]_{x_1}^{x_2}$. 2) $I = \int_{x_1}^{x_2} (\frac{y'}{y}) dx$. We compute $\frac{\partial F}{\partial y} = -\frac{y'}{y^2}$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ \frac{1}{y} \right] = -\frac{y'}{y^2}$. We then show path-independence by writing $I = \int_{x_1}^{x_2} \frac{y'}{y} dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ \log |y| \right] dx = \left[ \log |y| \right]_{x_1}^{x_2}$. I want to know how to figure out what the Lagrangian is the total derivative of. In the two examples above I got the total derivatives by trial and error, but I would like to know what the answer is in general.",,"['calculus', 'ordinary-differential-equations', 'partial-derivative', 'calculus-of-variations', 'euler-lagrange-equation']"
25,Jacobian matrix of time dependant system,Jacobian matrix of time dependant system,,"Suppose we have a differential equation of the form $x^{'}(t)=(x^{'}_{1}(t),x^{'}_{2}(t))=f((x_{1}(t),x_{2}(t))$  where $f : R^2 \rightarrow R^2$ Does the Jacobian matrix of $f$ get affected by $t$?","Suppose we have a differential equation of the form $x^{'}(t)=(x^{'}_{1}(t),x^{'}_{2}(t))=f((x_{1}(t),x_{2}(t))$  where $f : R^2 \rightarrow R^2$ Does the Jacobian matrix of $f$ get affected by $t$?",,['ordinary-differential-equations']
26,Wave equation with Robin and Neumann boundary conditions,Wave equation with Robin and Neumann boundary conditions,,"What are the periodic solutions of the following problem: Governing equation (wave equation): $u_{tt} - c^2 u_{xx} = 0 $, $\forall x\in[0;L]$, $\forall t\geq 0$ Robin boundary conditions at $x=0$: $u_x(0,t) = k u(0,t)$, $\forall t\geq 0$ Neumann boundary conditions at $x=L$: $u_x(L,t)=0$, $\forall t\geq 0$ where $k$ is a constant and $c=\sqrt{E/\rho}$.","What are the periodic solutions of the following problem: Governing equation (wave equation): $u_{tt} - c^2 u_{xx} = 0 $, $\forall x\in[0;L]$, $\forall t\geq 0$ Robin boundary conditions at $x=0$: $u_x(0,t) = k u(0,t)$, $\forall t\geq 0$ Neumann boundary conditions at $x=L$: $u_x(L,t)=0$, $\forall t\geq 0$ where $k$ is a constant and $c=\sqrt{E/\rho}$.",,"['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
27,solving third order nonlinear ordinary differential equation.,solving third order nonlinear ordinary differential equation.,,"Can anybody give any hints how to solve any kind of particular solution involving at least one arbitrary constant of this ode: $p^2y^{\prime \prime \prime}+ yy^{\prime}+y+ax+b=0$, where $a$,$b$,$p$ are constants.","Can anybody give any hints how to solve any kind of particular solution involving at least one arbitrary constant of this ode: $p^2y^{\prime \prime \prime}+ yy^{\prime}+y+ax+b=0$, where $a$,$b$,$p$ are constants.",,['ordinary-differential-equations']
28,Inhomogeneous biharmonic equation on $\mathbb{R}^d$,Inhomogeneous biharmonic equation on,\mathbb{R}^d,"I have been tasked with solving $\Delta(\Delta(u)) = f$ in $\mathbb{R}^d$ for $f \in C_c^4(\mathbb{R}^d)$. Frankly, I don't know where to begin. The ""naive"" thing to do would be to say we can solve $\Delta v=f$ and then $\Delta u =v$, but the solution to $\Delta v=f$ is $$\int_{\mathbb{R}^d} \Phi(x-y)f(y) \ dy$$ (where $\Phi$ is the fundamental solution to Laplace's equation) which isn't $C_c^2$, which is all we know how to solve for Poisson's equation for (in Evans; I know that there are more general sufficient conditions). So then a hands-on approach seems like the only way to go. Indeed, my professor said to find a $\Phi$ whose convolution with $f$ gives the solution. Yet, even in the case of  $$\Delta(\Delta(u)) = 0$$  in $\mathbb{R}^2$, the problem seems unapproachable. I do not know how to solve $u_{xxxx} + 2u_{xxyy} +u_{yyyy} =0$. I have tried looking up results about this equation, but nothing seems helpful at all for the inhomogeneous $\mathbb{R}^d$ case. Can anyone point me in the right direction? Thanks","I have been tasked with solving $\Delta(\Delta(u)) = f$ in $\mathbb{R}^d$ for $f \in C_c^4(\mathbb{R}^d)$. Frankly, I don't know where to begin. The ""naive"" thing to do would be to say we can solve $\Delta v=f$ and then $\Delta u =v$, but the solution to $\Delta v=f$ is $$\int_{\mathbb{R}^d} \Phi(x-y)f(y) \ dy$$ (where $\Phi$ is the fundamental solution to Laplace's equation) which isn't $C_c^2$, which is all we know how to solve for Poisson's equation for (in Evans; I know that there are more general sufficient conditions). So then a hands-on approach seems like the only way to go. Indeed, my professor said to find a $\Phi$ whose convolution with $f$ gives the solution. Yet, even in the case of  $$\Delta(\Delta(u)) = 0$$  in $\mathbb{R}^2$, the problem seems unapproachable. I do not know how to solve $u_{xxxx} + 2u_{xxyy} +u_{yyyy} =0$. I have tried looking up results about this equation, but nothing seems helpful at all for the inhomogeneous $\mathbb{R}^d$ case. Can anyone point me in the right direction? Thanks",,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
29,Solving a two function differential equation,Solving a two function differential equation,,I came across this question and I have tried a lot but still cannot get it.Can someone help?? $$y'(x)\phi(x)-y(x)\phi'(x)+y^2(x)=9$$ Given that $y(1)=1$. We need to find $y(2)$. Also we need to calculate $$\int_1^2 \frac{\phi(x) dx}{y(x)x^2\sqrt{x^2+(\frac{\phi(x)}{y(x)})^2}}$$ I tried $$y\phi'-y'\phi=y^2-9$$ On dividing by $y^2$ we get $$d(\frac{\phi}{y})=dx -\frac{9dx}{y^2}$$.The first two terms are integrable but I cannot understand how to deal with $\frac{9dx}{y^2}$. I even tried differentiating the given equation to get$$y''\phi+y'\phi'-y'\phi'-y\phi''+2yy'=0$$ .After cancelling we can rearrange to get$$\frac{d(\frac{\phi}{y})}{dx}=\frac{2dy}{y}.$$Unaware of how to handle such equation. Please help.Thanks.,I came across this question and I have tried a lot but still cannot get it.Can someone help?? $$y'(x)\phi(x)-y(x)\phi'(x)+y^2(x)=9$$ Given that $y(1)=1$. We need to find $y(2)$. Also we need to calculate $$\int_1^2 \frac{\phi(x) dx}{y(x)x^2\sqrt{x^2+(\frac{\phi(x)}{y(x)})^2}}$$ I tried $$y\phi'-y'\phi=y^2-9$$ On dividing by $y^2$ we get $$d(\frac{\phi}{y})=dx -\frac{9dx}{y^2}$$.The first two terms are integrable but I cannot understand how to deal with $\frac{9dx}{y^2}$. I even tried differentiating the given equation to get$$y''\phi+y'\phi'-y'\phi'-y\phi''+2yy'=0$$ .After cancelling we can rearrange to get$$\frac{d(\frac{\phi}{y})}{dx}=\frac{2dy}{y}.$$Unaware of how to handle such equation. Please help.Thanks.,,"['ordinary-differential-equations', 'definite-integrals']"
30,"Solve $(8p^3-27)x=12p^2y$, $p=\frac{dy}{dx}$","Solve ,",(8p^3-27)x=12p^2y p=\frac{dy}{dx},"Solve $F(x,y,p)=(8p^3-27)x-12p^2y=0$ My attempt: $F=0, \frac{\partial F}{\partial p}=0,$ Solving them i got $p=\frac{y}{x}$ Now subsituting this in above, we get $\frac{x(8y^3-27x^3)}{x^3}=12 \frac{y^2}{x^2}y \implies 4y^3+27x^3=0 \rightarrow$ this is my p-discriminant. But p-descriminant solution is given as $x(4y^3+27x^3)=0$. Pls clarify","Solve $F(x,y,p)=(8p^3-27)x-12p^2y=0$ My attempt: $F=0, \frac{\partial F}{\partial p}=0,$ Solving them i got $p=\frac{y}{x}$ Now subsituting this in above, we get $\frac{x(8y^3-27x^3)}{x^3}=12 \frac{y^2}{x^2}y \implies 4y^3+27x^3=0 \rightarrow$ this is my p-discriminant. But p-descriminant solution is given as $x(4y^3+27x^3)=0$. Pls clarify",,['ordinary-differential-equations']
31,"How do we know that every function of a DE we ""solved"" satifies the equation?","How do we know that every function of a DE we ""solved"" satifies the equation?",,"Suppose we have a D.E.  $\frac{dy}{dx}=f(y,x)$, which we solve by taking functions of both sides, dividing integrateing etc. How do we know that every function of a  DE we solved satifies the equation? And how do we know that we found all solutions? For example, suppose we use some function on both sides which is not bijective on the range of expression then we surley loose some solution right? How would one keep track of which solutions one looses or even introduce using different kinds of operations?","Suppose we have a D.E.  $\frac{dy}{dx}=f(y,x)$, which we solve by taking functions of both sides, dividing integrateing etc. How do we know that every function of a  DE we solved satifies the equation? And how do we know that we found all solutions? For example, suppose we use some function on both sides which is not bijective on the range of expression then we surley loose some solution right? How would one keep track of which solutions one looses or even introduce using different kinds of operations?",,[]
32,Event-based rather than flow-based systems?,Event-based rather than flow-based systems?,,"This is a very vague question, so I don’t know if people will understand it (I’m not even sure I understand the question myself), but here it goes: Dynamical systems are formulated (in continuous time) as $$\begin {align} &\dot x_1=f_1(x_1,...,x_n)\\ &...\\ &\dot x_n=f_n(x_1,...,x_n) \end {align}$$ These are flows over time . In other words, we have a stock for each variable, and a specified flow at each point in time, from which we will deduce the stock over time. Situations in applied science that are captured well by this system are for example: There is an amount of water $x$ in a cup, and we know that every second, an amount $a$ drips through a hole, so that $\dot x=a$. An object is moving with $x$ meters/s towards the earth, and we know that every second, $x$ increases by $9.8\,\mathrm{m}$. We can of course do this in discrete time as well, which doesn’t fundamentally change the idea: $$\begin {align} &\Delta x_1=f_1(x_1,...,x_n)\\ &...\\ &\Delta x_n=f_n(x_1,...,x_n) \end {align}$$ However, sometimes our logical analysis in an applied setting doesn’t start with knowledge of a flow per unit time, but with knowledge of events . For example, in biology, a birth can occur, which would cause various things to happen (among other things, an increase of the population by $1$). As we do in most of science, we also generally implement births in population models as a flow. Instead of starting with a formal definition of a birth as an event, we simply state that in a given time interval, the population increases. In other words, the analysis starts with the variable population, and with a formula denoting its change over time. In fact the whole concept of a birth, being an event, never enters the mathematical model, and is only a kind of informal interpretation that we stick to the model. I am wondering whether this informal leap from the event of a birth to a flow can somehow be formalized. So my questions are: Is there a (sub)branch of mathematics that starts its analysis with a formalization of an event, rather than of a flow? Is there any mathematical analysis of events at all? I tend to think of events as function calls in the programming sense, i.e., when a birth occurs, a “birth” function is called and this causes various variables to change, and possibly other events.","This is a very vague question, so I don’t know if people will understand it (I’m not even sure I understand the question myself), but here it goes: Dynamical systems are formulated (in continuous time) as $$\begin {align} &\dot x_1=f_1(x_1,...,x_n)\\ &...\\ &\dot x_n=f_n(x_1,...,x_n) \end {align}$$ These are flows over time . In other words, we have a stock for each variable, and a specified flow at each point in time, from which we will deduce the stock over time. Situations in applied science that are captured well by this system are for example: There is an amount of water $x$ in a cup, and we know that every second, an amount $a$ drips through a hole, so that $\dot x=a$. An object is moving with $x$ meters/s towards the earth, and we know that every second, $x$ increases by $9.8\,\mathrm{m}$. We can of course do this in discrete time as well, which doesn’t fundamentally change the idea: $$\begin {align} &\Delta x_1=f_1(x_1,...,x_n)\\ &...\\ &\Delta x_n=f_n(x_1,...,x_n) \end {align}$$ However, sometimes our logical analysis in an applied setting doesn’t start with knowledge of a flow per unit time, but with knowledge of events . For example, in biology, a birth can occur, which would cause various things to happen (among other things, an increase of the population by $1$). As we do in most of science, we also generally implement births in population models as a flow. Instead of starting with a formal definition of a birth as an event, we simply state that in a given time interval, the population increases. In other words, the analysis starts with the variable population, and with a formula denoting its change over time. In fact the whole concept of a birth, being an event, never enters the mathematical model, and is only a kind of informal interpretation that we stick to the model. I am wondering whether this informal leap from the event of a birth to a flow can somehow be formalized. So my questions are: Is there a (sub)branch of mathematics that starts its analysis with a formalization of an event, rather than of a flow? Is there any mathematical analysis of events at all? I tend to think of events as function calls in the programming sense, i.e., when a birth occurs, a “birth” function is called and this causes various variables to change, and possibly other events.",,"['ordinary-differential-equations', 'analysis', 'discrete-mathematics', 'dynamical-systems']"
33,First Order Time-Variant System : Picard Method,First Order Time-Variant System : Picard Method,,"I looking at a time variant first order system. I am trying to prove that a sequence of functions $x^{[k]}(t)$ generated using Picard iterations converges uniformly on some interval $[0,T]$. Given: $\dot{x}(t)=A(t)x(t)$ where $x(0)=x_0$ $x(t)=x∈R^n$ where $A(t)$ is a $n×n$ matrix Assume there exists $M$ such that $||A(t)||\le M$ for all $t\ge 0$ and that $A(t)$ is a piecewise continuous function. Show that, for any $T > 0$, a solution exists in the interval $[0,T]$. I know that $x(t)=x_0+\int^t_0A(τ)x(τ) dτ$ must be satisfied if the solution does indeed exist. I am supposed to use Picard iterations to prove this, which I am not familiar with yet. But I know that they are of the form: $x^{[k+1]}=x_0+\int^t_0A(τ)x^{[k]}(τ) dτ$. Where $x^{[0]}(t)=x_0$ Can I use the standard Picard proof to prove this or do I need to start somewhere else? I was also asked to show that this method satisfies $|x^{[k+1]}(t)-x^{[k]}(t)|\le |x_0| $$\frac{π^{k+1}(t)}{(k+1)!}$ where $π(t)=\int^t_0||A(τ)||dτ$. I have no idea where to start with this, but hopefully it will become clearer once I finish the first part.","I looking at a time variant first order system. I am trying to prove that a sequence of functions $x^{[k]}(t)$ generated using Picard iterations converges uniformly on some interval $[0,T]$. Given: $\dot{x}(t)=A(t)x(t)$ where $x(0)=x_0$ $x(t)=x∈R^n$ where $A(t)$ is a $n×n$ matrix Assume there exists $M$ such that $||A(t)||\le M$ for all $t\ge 0$ and that $A(t)$ is a piecewise continuous function. Show that, for any $T > 0$, a solution exists in the interval $[0,T]$. I know that $x(t)=x_0+\int^t_0A(τ)x(τ) dτ$ must be satisfied if the solution does indeed exist. I am supposed to use Picard iterations to prove this, which I am not familiar with yet. But I know that they are of the form: $x^{[k+1]}=x_0+\int^t_0A(τ)x^{[k]}(τ) dτ$. Where $x^{[0]}(t)=x_0$ Can I use the standard Picard proof to prove this or do I need to start somewhere else? I was also asked to show that this method satisfies $|x^{[k+1]}(t)-x^{[k]}(t)|\le |x_0| $$\frac{π^{k+1}(t)}{(k+1)!}$ where $π(t)=\int^t_0||A(τ)||dτ$. I have no idea where to start with this, but hopefully it will become clearer once I finish the first part.",,"['real-analysis', 'ordinary-differential-equations', 'convergence-divergence', 'uniform-convergence', 'picard-scheme']"
34,What is the inverse laplace transform of an inverted function?,What is the inverse laplace transform of an inverted function?,,I have $\mathcal{L}\{y'\}+4\mathcal{L}\{y\circledast e^{-4t}\}=\mathcal{L}\{1\}$ with $y(0)=0$ So we get: $sY+4Y\frac{1}{1+4}=\frac{1}{s}$ $\iff Y = \frac{1}{s^2+\frac{4s}{s+4}}$ Now I could rewrite this as $Y=\frac{1}{F(S)}$ but I am not aware of any identity for $\mathcal{L}^{-1}\{\frac{1}{F(s)}\}$. How can I find the Laplace inverse of this expression?,I have $\mathcal{L}\{y'\}+4\mathcal{L}\{y\circledast e^{-4t}\}=\mathcal{L}\{1\}$ with $y(0)=0$ So we get: $sY+4Y\frac{1}{1+4}=\frac{1}{s}$ $\iff Y = \frac{1}{s^2+\frac{4s}{s+4}}$ Now I could rewrite this as $Y=\frac{1}{F(S)}$ but I am not aware of any identity for $\mathcal{L}^{-1}\{\frac{1}{F(s)}\}$. How can I find the Laplace inverse of this expression?,,"['ordinary-differential-equations', 'laplace-transform', 'convolution']"
35,Solving second order differential equation 1,Solving second order differential equation 1,,"I want to solve the initial value problem \begin{align*} \frac{d^2c}{dx^2} = -\pi^2 x^2c -\pi\frac{s}{\sqrt{c^2+s^2}} \\ \frac{d^2s}{dx^2} = -\pi^2 x^2s +\pi\frac{c}{\sqrt{c^2+s^2}} \end{align*} with inital value $ c(0)=1, \frac{dc}{dx}(0)=0, s(0)=0, \frac{ds}{dx}(0)=0.$ I know that the exact solution is \begin{align*} c(x)=\cos(\frac{\pi}{2}x^2), s(x)=\sin(\frac{\pi}{2}x^2). \end{align*} But, I couldn't derive the solution... Any help is appreciated!! Thank you!","I want to solve the initial value problem \begin{align*} \frac{d^2c}{dx^2} = -\pi^2 x^2c -\pi\frac{s}{\sqrt{c^2+s^2}} \\ \frac{d^2s}{dx^2} = -\pi^2 x^2s +\pi\frac{c}{\sqrt{c^2+s^2}} \end{align*} with inital value $ c(0)=1, \frac{dc}{dx}(0)=0, s(0)=0, \frac{ds}{dx}(0)=0.$ I know that the exact solution is \begin{align*} c(x)=\cos(\frac{\pi}{2}x^2), s(x)=\sin(\frac{\pi}{2}x^2). \end{align*} But, I couldn't derive the solution... Any help is appreciated!! Thank you!",,['ordinary-differential-equations']
36,How to solve a system of linear differential equations using laplace transformation when one of the initial condition is not at zero?,How to solve a system of linear differential equations using laplace transformation when one of the initial condition is not at zero?,,"A new user here! For specifics, the problem is: $$x' - y'' = - 2 \cosh t$$ $$y' - x'' = 2 \sinh t$$ $$y(0) = y'(0) = y''(0) = 1$$ $$x(1) = - x'(1) =x'' (1) = 1/e$$ From what I know, I need to change $x(1)$ to $x(0)$, by letting a variable $u = t-1$. But if I do that, $y(0)$ would change to $y(-1)$. I am at a loss to what to do. No need to solve the whole problem, just point me to the right direction. Thanks!","A new user here! For specifics, the problem is: $$x' - y'' = - 2 \cosh t$$ $$y' - x'' = 2 \sinh t$$ $$y(0) = y'(0) = y''(0) = 1$$ $$x(1) = - x'(1) =x'' (1) = 1/e$$ From what I know, I need to change $x(1)$ to $x(0)$, by letting a variable $u = t-1$. But if I do that, $y(0)$ would change to $y(-1)$. I am at a loss to what to do. No need to solve the whole problem, just point me to the right direction. Thanks!",,"['ordinary-differential-equations', 'laplace-transform']"
37,Solving a first order differential equation in terms of Lambert W-function Continued:,Solving a first order differential equation in terms of Lambert W-function Continued:,,"I have had some excellent answers in response to a previous question here: Solving a first order differential equation in terms of Lambert W-function However I would like to show my work some more since I am still stuck, and terribly lost, in hopes for some further guidance. Thank you all. So I was left with an equation of the following form: $$ t + K = \frac{bx^2}{2a} + \frac{c\ln(x)}{a}   $$ Multiplying through by $$ \frac{2a}{c}$$ Gives $$ \frac{2a(t + K)}{c} = \frac{b}{c} x^2 + 2\ln(x)   $$ Following by: $$ exp(\frac{2a(t + K)}{c}) = x^2 exp(bx^2/c)   $$ It is at this point here I am stuck. I actually am not 100 percent sure the final step I made is valid. However I belive here that the function is now in the form:  $$ y = x exp(x) $$ Which is the form needed to use the Lambert function $$ W(y) = x $$ Any help is appreciated. Thanks all.","I have had some excellent answers in response to a previous question here: Solving a first order differential equation in terms of Lambert W-function However I would like to show my work some more since I am still stuck, and terribly lost, in hopes for some further guidance. Thank you all. So I was left with an equation of the following form: $$ t + K = \frac{bx^2}{2a} + \frac{c\ln(x)}{a}   $$ Multiplying through by $$ \frac{2a}{c}$$ Gives $$ \frac{2a(t + K)}{c} = \frac{b}{c} x^2 + 2\ln(x)   $$ Following by: $$ exp(\frac{2a(t + K)}{c}) = x^2 exp(bx^2/c)   $$ It is at this point here I am stuck. I actually am not 100 percent sure the final step I made is valid. However I belive here that the function is now in the form:  $$ y = x exp(x) $$ Which is the form needed to use the Lambert function $$ W(y) = x $$ Any help is appreciated. Thanks all.",,"['ordinary-differential-equations', 'lambert-w']"
38,Reeb flow of the (2n-1)-sphere,Reeb flow of the (2n-1)-sphere,,"Let $S^{2n-1}$ be the unit sphere in $(\mathbb R^{2n}, \omega = \text d \underline x \wedge \text d\underline y)$, where $(x_1, y_1, ..., x_n, y_n)$ are cartesian coordinates and $\omega$ is the standard symplectic form. Show that   $$ \alpha = \frac 12 \sum\limits_{j=1}^n (x_j \text dy_j - y_j \text dx_j) $$   defines a contact structure on $S^{2n-1}$ and compute the Reeb flow for $\alpha$. I have already proven that $\alpha = \iota_Y \omega$ is a contact form where $Y = \frac 12 \sum\limits_{j=1}^n (x_j \partial x_j + y_j \partial y_j)$ is the radial vector field on the sphere. Obviously $Y$ is a Liouville vector field, so one can show easily $\iota_Y \omega$ defines a contact structure. I also proved that the Reeb vector field is $R = 2 \sum_j (x_j \partial y_j - y_j \partial x_j)$ since $\alpha(R) = 1$ and $\iota_R \text d\alpha = 0$. Now I need to calculate the Reeb flow $\Phi$, which is the solution to the differential equation $$R (\Phi) = \dot\Phi.$$ How on earth do I solve this thing? A little tip would really help me. Edit: I think the solution locally is $\Phi(t) = (\cos t - 1, \sin t, ..., \cos t - 1, \sin t)$ for $\Phi(0) = (0)$.","Let $S^{2n-1}$ be the unit sphere in $(\mathbb R^{2n}, \omega = \text d \underline x \wedge \text d\underline y)$, where $(x_1, y_1, ..., x_n, y_n)$ are cartesian coordinates and $\omega$ is the standard symplectic form. Show that   $$ \alpha = \frac 12 \sum\limits_{j=1}^n (x_j \text dy_j - y_j \text dx_j) $$   defines a contact structure on $S^{2n-1}$ and compute the Reeb flow for $\alpha$. I have already proven that $\alpha = \iota_Y \omega$ is a contact form where $Y = \frac 12 \sum\limits_{j=1}^n (x_j \partial x_j + y_j \partial y_j)$ is the radial vector field on the sphere. Obviously $Y$ is a Liouville vector field, so one can show easily $\iota_Y \omega$ defines a contact structure. I also proved that the Reeb vector field is $R = 2 \sum_j (x_j \partial y_j - y_j \partial x_j)$ since $\alpha(R) = 1$ and $\iota_R \text d\alpha = 0$. Now I need to calculate the Reeb flow $\Phi$, which is the solution to the differential equation $$R (\Phi) = \dot\Phi.$$ How on earth do I solve this thing? A little tip would really help me. Edit: I think the solution locally is $\Phi(t) = (\cos t - 1, \sin t, ..., \cos t - 1, \sin t)$ for $\Phi(0) = (0)$.",,['ordinary-differential-equations']
39,Getting singular solution from parametric solutions of an ODE,Getting singular solution from parametric solutions of an ODE,,"How to obtain the singular solution for this ODE $$y=2xy'+\frac{1}{y'}$$ After getting the general solution (by calling y' as p and differentiating the equation by x) , I tried to get the envelope but I failed because I got two parametric equations representing the general solution. here are the equations: $$x=\frac{ln(p)}{p^2}+\frac{c}{p^2}$$ $$y=2\frac{ln(p)}{p}+\frac{2c+1}{p}$$ So which equation do I have to differentiate wrt c ?  If I differentiate any of them wrt c and equating by zero , I got 1=0 !","How to obtain the singular solution for this ODE After getting the general solution (by calling y' as p and differentiating the equation by x) , I tried to get the envelope but I failed because I got two parametric equations representing the general solution. here are the equations: So which equation do I have to differentiate wrt c ?  If I differentiate any of them wrt c and equating by zero , I got 1=0 !",y=2xy'+\frac{1}{y'} x=\frac{ln(p)}{p^2}+\frac{c}{p^2} y=2\frac{ln(p)}{p}+\frac{2c+1}{p},"['ordinary-differential-equations', 'singular-solution']"
40,Integrating matrix differential equation with linearly varying anti-symmetric matrix,Integrating matrix differential equation with linearly varying anti-symmetric matrix,,"I have the following 3-dimensional matrix initial value problem: $$\left\{\begin{matrix}\textbf{x}'(t) &=& (A + Bt)\:\textbf{x}(t) \\ \textbf{x}(0) &=& \textbf{x}_0\end{matrix}\right.$$ The matrices $A$ and $B$ do not depend on $t$, are anti-symmetric and have the following sparsity pattern: $$\begin{bmatrix} 0 & \times & 0 \\ \times & 0 & \times \\ 0 & \times & 0 \end{bmatrix}$$ where the crosses indicate the non-zero elements. I am wondering whether this initial value problem can be solved analytically. I am aware that both the special cases $A = 0$ and $B = 0$ can be solved analytically, with solutions $\textbf{x}(t) = e^{\frac12Bt^2}\textbf{x}_0$ and $\textbf{x}(t) = e^{At}\textbf{x}_0$, respectively. $A$ and $B$ are very likely not to commute, so the solution $\textbf{x}(t) = e^{At + \frac12Bt^2}\textbf{x}_0$ is probably false. Does anyone have an idea how to tackle this problem?","I have the following 3-dimensional matrix initial value problem: $$\left\{\begin{matrix}\textbf{x}'(t) &=& (A + Bt)\:\textbf{x}(t) \\ \textbf{x}(0) &=& \textbf{x}_0\end{matrix}\right.$$ The matrices $A$ and $B$ do not depend on $t$, are anti-symmetric and have the following sparsity pattern: $$\begin{bmatrix} 0 & \times & 0 \\ \times & 0 & \times \\ 0 & \times & 0 \end{bmatrix}$$ where the crosses indicate the non-zero elements. I am wondering whether this initial value problem can be solved analytically. I am aware that both the special cases $A = 0$ and $B = 0$ can be solved analytically, with solutions $\textbf{x}(t) = e^{\frac12Bt^2}\textbf{x}_0$ and $\textbf{x}(t) = e^{At}\textbf{x}_0$, respectively. $A$ and $B$ are very likely not to commute, so the solution $\textbf{x}(t) = e^{At + \frac12Bt^2}\textbf{x}_0$ is probably false. Does anyone have an idea how to tackle this problem?",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'initial-value-problems']"
41,About the general solution of $x'(t)+x(t)=\int_0^{x(t)}\frac{du}{\cosh u}$,About the general solution of,x'(t)+x(t)=\int_0^{x(t)}\frac{du}{\cosh u},"I wondered as curiosity about the general solution of the nonlinear ordinary differential equation  $$x'(t)+x(t)=\int_0^{x(t)}\frac{du}{\cosh u},\tag{1}$$ for $t\geq 0$. See the article of Wikipedia or MathWorld dedicated to the Gudermannian function. Of couse I know using Euler's trick the solution of the homogeneous equation $x'(t)+x(t)=0$ as $Ce^{-t}$ for a real constant $C$ depending of the value $x(0)$. Or also (for real variables $C_1$ and $C_2$ we obtain the solution as a vectorial space of dimension $2$) the solution of the homogeneous equation associated to the derivative of $(1)$ $$x''(t)+x'(t)=\frac{2x(t)}{e^{x(t)}+e^{-x(t)}},\tag{2}$$  as $x_h(t)=C_1+C_2e^{-t}$. Also I've tried to combine with facts that I know of hyperbolic functions, but I believe that it doesn't solve the problem, that is that our equation $(1)$ is nonlinear and thus is difficult, and in this exercise I believe that impossible to compute explicitely the particular solution $x_p(t)$, that I need, to express the general solution of $(1)$ using the superposition principle. Question. I want to solve $(1)$, well explicitely or well numerically. I don't know if has mathematical meaning the linearization to explore the shape of the solutions of this non-linear equation. Then I would like to refresh topics from differential equations and numerical analysis, learn more from your answer. The question is thus do calculations and reasonins to state the solution of $(1)$. Thanks in advance. See the output that was calculated by Wolfram Alpha online calculator from my code solve x'+x=gd(x) the plot of the solution family sampling $x(0)$. Then, what are your ideas to present the solutions of my Question? I've updated $(2)$ since was a mistake, see the comments. We can see also that there was a mistake in my original $(2)$ using the second fundamental theorem of calculus.","I wondered as curiosity about the general solution of the nonlinear ordinary differential equation  $$x'(t)+x(t)=\int_0^{x(t)}\frac{du}{\cosh u},\tag{1}$$ for $t\geq 0$. See the article of Wikipedia or MathWorld dedicated to the Gudermannian function. Of couse I know using Euler's trick the solution of the homogeneous equation $x'(t)+x(t)=0$ as $Ce^{-t}$ for a real constant $C$ depending of the value $x(0)$. Or also (for real variables $C_1$ and $C_2$ we obtain the solution as a vectorial space of dimension $2$) the solution of the homogeneous equation associated to the derivative of $(1)$ $$x''(t)+x'(t)=\frac{2x(t)}{e^{x(t)}+e^{-x(t)}},\tag{2}$$  as $x_h(t)=C_1+C_2e^{-t}$. Also I've tried to combine with facts that I know of hyperbolic functions, but I believe that it doesn't solve the problem, that is that our equation $(1)$ is nonlinear and thus is difficult, and in this exercise I believe that impossible to compute explicitely the particular solution $x_p(t)$, that I need, to express the general solution of $(1)$ using the superposition principle. Question. I want to solve $(1)$, well explicitely or well numerically. I don't know if has mathematical meaning the linearization to explore the shape of the solutions of this non-linear equation. Then I would like to refresh topics from differential equations and numerical analysis, learn more from your answer. The question is thus do calculations and reasonins to state the solution of $(1)$. Thanks in advance. See the output that was calculated by Wolfram Alpha online calculator from my code solve x'+x=gd(x) the plot of the solution family sampling $x(0)$. Then, what are your ideas to present the solutions of my Question? I've updated $(2)$ since was a mistake, see the comments. We can see also that there was a mistake in my original $(2)$ using the second fundamental theorem of calculus.",,"['integration', 'analysis']"
42,Neumann boundary condition problem,Neumann boundary condition problem,,"Say I have a long hallway in an office building. If I assume that any cigarette smoke, mixes across the width of the hallway and vertically through the depth of the hallway much faster than it mixes along the hallway, I wrote the diﬀusion of cigarette smoke as an equation $$\frac{\partial S}{\partial t}= κ \frac{\partial^2 S}{\partial x^2} -γS +α(x)$$ where $S$ is the concentration of smoke, $κ$ is the rate of diﬀusion of smoke, $γ$ is the rate at which the smoke sticks to the walls or otherwise leaves the system, $α(x)$ is the sources of smoke, t is the time and x is distance along the hallway. How can I discretize the hall into N segments and write the equation for the steady state as a matrix equation? How do I find the condition number of the matrix? Any help will be appreciated!","Say I have a long hallway in an office building. If I assume that any cigarette smoke, mixes across the width of the hallway and vertically through the depth of the hallway much faster than it mixes along the hallway, I wrote the diﬀusion of cigarette smoke as an equation $$\frac{\partial S}{\partial t}= κ \frac{\partial^2 S}{\partial x^2} -γS +α(x)$$ where $S$ is the concentration of smoke, $κ$ is the rate of diﬀusion of smoke, $γ$ is the rate at which the smoke sticks to the walls or otherwise leaves the system, $α(x)$ is the sources of smoke, t is the time and x is distance along the hallway. How can I discretize the hall into N segments and write the equation for the steady state as a matrix equation? How do I find the condition number of the matrix? Any help will be appreciated!",,"['linear-algebra', 'ordinary-differential-equations', 'finite-differences']"
43,2nd order ODE most general solution and Taylor's theorem,2nd order ODE most general solution and Taylor's theorem,,"Context : The 2nd order ODE of the form $$ \frac{d^{2}y}{dx^{2}}+p\frac{dy}{dx}+qy=0, $$ which has general solution of the form (assuming distinct $\gamma_{1}$ and $\gamma_{2}$): $$ y= A_{1}e^{\gamma_{1}x}+A_{2}e^{\gamma_{2}x}, \ \ \ \ \ \ \ \ \ \ \ \ (1)$$ where $A_{1}$ and $A_{2}$ are two arbitrary constants. In the book ""Applied Mathematics"" by Schelkunoff, p.194, the author also gives the following exposition (note that primed symbols denote derivative with $x$): ""The equation gives merely a relationship between the function and its first two derivatives... Since the coefficient of the second-order derivative is different from zero, $y$ and $y'$ may be assigned arbitrarily at the point in question without any exceptions; $y''$ is then determined uniquely. By successive differentiation of the ODE we may express all higher derivatives in terms of $y$ and $y'$ at a given point. Applying Taylor's theorem we shall get the most general analytic solution of the ODE. ""We shall be able to claim that the solution in (1) is the most general analytic solution if we can show that at some point, $x=0$ for instance, we can express $A_{1}$ and $A_{2}$ in terms of arbitrarily assigned values of $y(0)$ and $y'(0)$. We leave the proof to the reader."" Question: I don't have a problem in proving that $A_{1}$ and $A_{2}$ are expressible in terms of $y(x_{0})$ and $y'(x_{0})$ at some point $x_{0}$, but I am not sure why we can make such general claim (in the second quoted paragraph) if we simply proved such connection? Why would that mean that the two solution types ($y$ given in (1) above and Taylor's expansion) be equivalent at all other points, as to make $y$ in (1) the most general solution? When I tried to expand for Taylor's solution in terms of $y(0)$ and $y'(0)$, I got: $$ y(x)=y(0)\left[ 1-\frac{x^{2}}{2}q +\frac{x^{3}}{3}pq+\cdots\right]+y'(0)\left[ x-\frac{x^{2}}{2}p +\frac{x^{3}}{3}(p^{2}-q)+\cdots\right],\ \ \ \ (2)$$ which can be clearly used to give $A_{1},A_{2}$ at $x=0$ in terms of $y(0),y'(0)$, without problem. But what is not clear is that proving such connection at a point basis (such as $x=0$ here) would be enough to conclude that the two solutions are equivalent in gerenal, and therefore for the form $y$ in (1) to become the most general solution according to the author's argument. [Incidentally, if I expand the exponents in (1) as series, it is not straightforward how they can be grouped together to exactly map to those is (2).]","Context : The 2nd order ODE of the form $$ \frac{d^{2}y}{dx^{2}}+p\frac{dy}{dx}+qy=0, $$ which has general solution of the form (assuming distinct $\gamma_{1}$ and $\gamma_{2}$): $$ y= A_{1}e^{\gamma_{1}x}+A_{2}e^{\gamma_{2}x}, \ \ \ \ \ \ \ \ \ \ \ \ (1)$$ where $A_{1}$ and $A_{2}$ are two arbitrary constants. In the book ""Applied Mathematics"" by Schelkunoff, p.194, the author also gives the following exposition (note that primed symbols denote derivative with $x$): ""The equation gives merely a relationship between the function and its first two derivatives... Since the coefficient of the second-order derivative is different from zero, $y$ and $y'$ may be assigned arbitrarily at the point in question without any exceptions; $y''$ is then determined uniquely. By successive differentiation of the ODE we may express all higher derivatives in terms of $y$ and $y'$ at a given point. Applying Taylor's theorem we shall get the most general analytic solution of the ODE. ""We shall be able to claim that the solution in (1) is the most general analytic solution if we can show that at some point, $x=0$ for instance, we can express $A_{1}$ and $A_{2}$ in terms of arbitrarily assigned values of $y(0)$ and $y'(0)$. We leave the proof to the reader."" Question: I don't have a problem in proving that $A_{1}$ and $A_{2}$ are expressible in terms of $y(x_{0})$ and $y'(x_{0})$ at some point $x_{0}$, but I am not sure why we can make such general claim (in the second quoted paragraph) if we simply proved such connection? Why would that mean that the two solution types ($y$ given in (1) above and Taylor's expansion) be equivalent at all other points, as to make $y$ in (1) the most general solution? When I tried to expand for Taylor's solution in terms of $y(0)$ and $y'(0)$, I got: $$ y(x)=y(0)\left[ 1-\frac{x^{2}}{2}q +\frac{x^{3}}{3}pq+\cdots\right]+y'(0)\left[ x-\frac{x^{2}}{2}p +\frac{x^{3}}{3}(p^{2}-q)+\cdots\right],\ \ \ \ (2)$$ which can be clearly used to give $A_{1},A_{2}$ at $x=0$ in terms of $y(0),y'(0)$, without problem. But what is not clear is that proving such connection at a point basis (such as $x=0$ here) would be enough to conclude that the two solutions are equivalent in gerenal, and therefore for the form $y$ in (1) to become the most general solution according to the author's argument. [Incidentally, if I expand the exponents in (1) as series, it is not straightforward how they can be grouped together to exactly map to those is (2).]",,"['analysis', 'ordinary-differential-equations']"
44,"Looking for a family of functions that satisfy $f\left(\frac1 x\right)+f(x)=1$ and maps $[0,\infty) → [0, 1)$ one-to-one",Looking for a family of functions that satisfy  and maps  one-to-one,"f\left(\frac1 x\right)+f(x)=1 [0,\infty) → [0, 1)","Title has it all.  The faster the better, it's for collision calculations in a game.  I like math but haven't had diffy, so I'm stumped.  Thanks! If it helps, x is a ratio of two positive real numbers. I was messing with arctan of the difference of the two positive real numbers, but it didn't seem to be working quite right.  I figure I need to work it more, since all my scribbles thus far seem to be point towards arctan as my solution.","Title has it all.  The faster the better, it's for collision calculations in a game.  I like math but haven't had diffy, so I'm stumped.  Thanks! If it helps, x is a ratio of two positive real numbers. I was messing with arctan of the difference of the two positive real numbers, but it didn't seem to be working quite right.  I figure I need to work it more, since all my scribbles thus far seem to be point towards arctan as my solution.",,"['ordinary-differential-equations', 'functions', 'asymptotics']"
45,Common quadratic Lyapunov function with all convex combinations Hurwitz,Common quadratic Lyapunov function with all convex combinations Hurwitz,,"I have two $n\times n$ Hurwitz stable matrices, so $A_i\in H_n$ for $i=1,2$ .  I also know that every possible convex combination of these two matrices is Hurwitz, i.e., \begin{equation} C(A_1,A_2) = cA_1+(1-c)A_2 \in H_n \hspace{0.5cm} \forall c\in[0,1]. \end{equation} I would like to determine tractable conditions under which a common quadratic Lyapunov function exists for the unforced systems $\dot{x}(t)=A_i x(t)$ such that \begin{equation} A_iP+PA_i^T = -Q_i \hspace{0.5cm} i=1,2 \end{equation} with $P=P^T>0$ and $Q_i=Q_i^T>0$ . I am familiar with some results that might help here, but so far nothing is applicable.  The matrices do not commute.  I cannot establish that they are simultaneously triangularizable.  They are generated from nonminimal systems and it is necessary that they are not expressed in companion form.  They are of order $n$ which will certainly be greater than 2. Perhaps this is not possible. However, I was hoping that I could exploit the known stability of all convex combinations somehow. I also have a related formulation that might be more feasible.  Say the problem is now framed as an unforced time-varying system $\dot{x}(t)=A(t)x(t)$ with \begin{equation} A(t) = c(t)A_1 +(1-c(t))A_2 \hspace{0.5cm} c(t)\in[0,1]. \end{equation} The eigenvalues of $A(t)$ are always in the open LHP and $c(t)$ behaves nice enough to ensure that the system is exponentially stable.  I would like to determine conditions under which a constant Lyapunov matrix exists \begin{equation} A(t)P+PA^T(t) = -Q(t). \end{equation} I view this as somewhat akin to finding a common Lyapunov function for a switched system that can switch between an infinite number of modes.  Maybe I could restrict the time variation of $c(t)$ to help make this possible? Edit: Looking at this from the time-varying system perspective, I think a converse Lyapunov theorem could help. If the system is exponentially stable for all permissible $c(t)$ trajectories then can I claim there exists a Lyapunov function satisfying \begin{equation} c_1\|x\|^2 \le V(x,t) \le c_2 \|x\|^2 \end{equation} \begin{equation} \dot{V}(x,t) \le -c_3 \|x\|^2 \end{equation} with $c_1,c_2,c_3>0$ ? Could I further claim that the form of such a function is \begin{equation} V(x,t) = x^TP(t)x \end{equation} with a smooth $P(t)=P^T(t)>0$ for an appropriate $c(t)$ ?  Would there be any loss of generality in assuming this $P(t)$ to be a specific $P(t)$ , i.e. one satisfying properties of another result I wish to use? Edit 2: Is there any tractable way to relate the state transition matrices for $A_1$ , $A_2$ , and $A(t)$ ?  I believe that I would have to work directly from the $\dot{\Phi} = A(t)\Phi$ statement to determine its transition matrix which seems daunting.","I have two Hurwitz stable matrices, so for .  I also know that every possible convex combination of these two matrices is Hurwitz, i.e., I would like to determine tractable conditions under which a common quadratic Lyapunov function exists for the unforced systems such that with and . I am familiar with some results that might help here, but so far nothing is applicable.  The matrices do not commute.  I cannot establish that they are simultaneously triangularizable.  They are generated from nonminimal systems and it is necessary that they are not expressed in companion form.  They are of order which will certainly be greater than 2. Perhaps this is not possible. However, I was hoping that I could exploit the known stability of all convex combinations somehow. I also have a related formulation that might be more feasible.  Say the problem is now framed as an unforced time-varying system with The eigenvalues of are always in the open LHP and behaves nice enough to ensure that the system is exponentially stable.  I would like to determine conditions under which a constant Lyapunov matrix exists I view this as somewhat akin to finding a common Lyapunov function for a switched system that can switch between an infinite number of modes.  Maybe I could restrict the time variation of to help make this possible? Edit: Looking at this from the time-varying system perspective, I think a converse Lyapunov theorem could help. If the system is exponentially stable for all permissible trajectories then can I claim there exists a Lyapunov function satisfying with ? Could I further claim that the form of such a function is with a smooth for an appropriate ?  Would there be any loss of generality in assuming this to be a specific , i.e. one satisfying properties of another result I wish to use? Edit 2: Is there any tractable way to relate the state transition matrices for , , and ?  I believe that I would have to work directly from the statement to determine its transition matrix which seems daunting.","n\times n A_i\in H_n i=1,2 \begin{equation}
C(A_1,A_2) = cA_1+(1-c)A_2 \in H_n \hspace{0.5cm} \forall c\in[0,1].
\end{equation} \dot{x}(t)=A_i x(t) \begin{equation}
A_iP+PA_i^T = -Q_i \hspace{0.5cm} i=1,2
\end{equation} P=P^T>0 Q_i=Q_i^T>0 n \dot{x}(t)=A(t)x(t) \begin{equation}
A(t) = c(t)A_1 +(1-c(t))A_2 \hspace{0.5cm} c(t)\in[0,1].
\end{equation} A(t) c(t) \begin{equation}
A(t)P+PA^T(t) = -Q(t).
\end{equation} c(t) c(t) \begin{equation}
c_1\|x\|^2 \le V(x,t) \le c_2 \|x\|^2
\end{equation} \begin{equation}
\dot{V}(x,t) \le -c_3 \|x\|^2
\end{equation} c_1,c_2,c_3>0 \begin{equation}
V(x,t) = x^TP(t)x
\end{equation} P(t)=P^T(t)>0 c(t) P(t) P(t) A_1 A_2 A(t) \dot{\Phi} = A(t)\Phi","['linear-algebra', 'ordinary-differential-equations', 'control-theory', 'stability-theory', 'hurwitz-matrices']"
46,Existence and Uniqueness Theorem of $y'=1+2\sqrt{x-y}$,Existence and Uniqueness Theorem of,y'=1+2\sqrt{x-y},"I solved the following differential equation: $$y'=1+2\sqrt{x-y}$$ the solution I found is $\sqrt{x-y}=-x+C$ and $x=y(y\ne0)$ after that I was asked the following questions: a) Find a solution that that maintain $y(1)=3$ and find it's domain. when I input $y(1)=3$ into the equation I get $(c-1)^2=-2$ and that's impossible. I don't understand why it is considered a solution and what is the domain. b) Find two different solutions that maintain $y(1)=1$ and explain why it does not contradict the Existence and Uniqueness Theorem. When I input $y(1)=1$ I get $C=1$ I think that the equation is continuous for $y\gt0$ and $y\lt 0$ so for each we have one solution. Is this true. Also, I don't fully understand what does it mean that a differential equation has a ""solution"". I would like a small explanation. Answer for any of the questions above would be highly appreciated.","I solved the following differential equation: $$y'=1+2\sqrt{x-y}$$ the solution I found is $\sqrt{x-y}=-x+C$ and $x=y(y\ne0)$ after that I was asked the following questions: a) Find a solution that that maintain $y(1)=3$ and find it's domain. when I input $y(1)=3$ into the equation I get $(c-1)^2=-2$ and that's impossible. I don't understand why it is considered a solution and what is the domain. b) Find two different solutions that maintain $y(1)=1$ and explain why it does not contradict the Existence and Uniqueness Theorem. When I input $y(1)=1$ I get $C=1$ I think that the equation is continuous for $y\gt0$ and $y\lt 0$ so for each we have one solution. Is this true. Also, I don't fully understand what does it mean that a differential equation has a ""solution"". I would like a small explanation. Answer for any of the questions above would be highly appreciated.",,['ordinary-differential-equations']
47,Solving differential equation with finite boundary condition,Solving differential equation with finite boundary condition,,"This is a follow-up to my previous question , as I try to understand how to solve the differential equation under less conventional boundaries. Consider a finite diffusion in the boundary of $0 < x < l$ where $l$ is a rigid wall (the diffusing species do to pass through). $$\frac{\partial c}{\partial t} = D\frac{\partial^2c}{\partial x^2}$$ with boundary conditions of: $c(0,t) = A$ $c(l,t) = 0$ $c(x,0) = 0    \quad (l > x > 0)$ How can we solve it with the condition that causes the concentration to increase by $t$ at $x=l$. Example: $c(0,t)=A$ is the concentration at the membrane. $l$ is a rigid wall, and the concentration increases by time. I believe the concentration profile should be something like","This is a follow-up to my previous question , as I try to understand how to solve the differential equation under less conventional boundaries. Consider a finite diffusion in the boundary of $0 < x < l$ where $l$ is a rigid wall (the diffusing species do to pass through). $$\frac{\partial c}{\partial t} = D\frac{\partial^2c}{\partial x^2}$$ with boundary conditions of: $c(0,t) = A$ $c(l,t) = 0$ $c(x,0) = 0    \quad (l > x > 0)$ How can we solve it with the condition that causes the concentration to increase by $t$ at $x=l$. Example: $c(0,t)=A$ is the concentration at the membrane. $l$ is a rigid wall, and the concentration increases by time. I believe the concentration profile should be something like",,"['ordinary-differential-equations', 'partial-differential-equations']"
48,Solving an ODE in the complex plane using a MatLab ODE solver,Solving an ODE in the complex plane using a MatLab ODE solver,,"In Matlab, I mostly use the solver ode45 and it seems that I cannot use complex numbers when specifying the interval of integration. An oversimple example would be if I wanted to solve numerically the initial value problem $$\frac{{\mbox{d}}y}{{\mbox{d}}t}=y,\;\;y(0)=3,$$ in the complex plane on the straight line between $t=0$ and $t=3+2i$. There does not seem to be a way to use an ODE solver in MatLab that would let me specify an interval in the complex plane. My question is: is there a way to use a Matlab ODE solver to solve an ODE in the complex plane?","In Matlab, I mostly use the solver ode45 and it seems that I cannot use complex numbers when specifying the interval of integration. An oversimple example would be if I wanted to solve numerically the initial value problem $$\frac{{\mbox{d}}y}{{\mbox{d}}t}=y,\;\;y(0)=3,$$ in the complex plane on the straight line between $t=0$ and $t=3+2i$. There does not seem to be a way to use an ODE solver in MatLab that would let me specify an interval in the complex plane. My question is: is there a way to use a Matlab ODE solver to solve an ODE in the complex plane?",,"['ordinary-differential-equations', 'numerical-methods', 'matlab']"
49,How to find points of extrema of $f(x)=\cos x+ \cos\left(\sqrt{2}x\right)$,How to find points of extrema of,f(x)=\cos x+ \cos\left(\sqrt{2}x\right),"How to find points of extrema of $f(x)=\cos x+ \cos\left(\sqrt{2}x\right)$ we have $$f'(x)=-\sin x - \sqrt{2}\sin\left(\sqrt{2}x \right)=0 \tag{1}$$ $x=0$ is one of the solutions at which $f$ attains local Maximum since $$f''(x)=-\left(\cos x+2 \cos\left(\sqrt{2}x\right)\right)$$ we have$f''(0) \lt 0$ But $1$ has infinite solutions right, how to check whether they are points of Local Maxima and Minima?","How to find points of extrema of $f(x)=\cos x+ \cos\left(\sqrt{2}x\right)$ we have $$f'(x)=-\sin x - \sqrt{2}\sin\left(\sqrt{2}x \right)=0 \tag{1}$$ $x=0$ is one of the solutions at which $f$ attains local Maximum since $$f''(x)=-\left(\cos x+2 \cos\left(\sqrt{2}x\right)\right)$$ we have$f''(0) \lt 0$ But $1$ has infinite solutions right, how to check whether they are points of Local Maxima and Minima?",,"['algebra-precalculus', 'ordinary-differential-equations', 'derivatives', 'convergence-divergence', 'continuity']"
50,differential equation on a Lie group,differential equation on a Lie group,,"I'm struggling to reproduce Ex. 2 from the paper by Lafferriere and Sussmann. In short, we have a nilpotent Lie algebra of vector fields generated by two elements: $X$ and $Y$. Its basis is $B_1=X$, $B_2=Y$, $B_3=[X,Y]$, $B_4=[X,[X,Y]]$, and $B_5=[Y,[X,Y]]$ (all higher order brackets are zero). The authors write an arbitrary element of the Lie group as $$S(t)=e^{h_5(t)B_5}\circ\dots\circ e^{h_1(t)B_1}$$ and assert that $S(t)$ can be obtaned as a solution to the DE  $$\dot{S}(t)=S(t)(v_1(t)B_1 + v_2(t)B_2 + v_3(t)B_3 + v_4(t)B_4)\quad [\mbox{ note } v_5=0!],$$ with $S(0)=Id$ and where $h_i$'s are related to $v_i$'s by the DEs \begin{align*}\dot{h}_1={ }&v_1,\\ \dot{h}_2={ }&v_2,\\  \dot{h}_3={ }&h_1v_2+v_3\\ \dot{h}_4={ }&\frac{1}{2}h^2_1v_2+h_1v_3+v_4\\ \dot{h}_5={ }&h_2v_3+h_1h_2v_2\end{align*} with zero initial conditions.  I was trying to differentiate $S(t)$, but failed to obtain the relations between $h_i$'s and $v_i$'s as shown above. Could somebody give me a hint at how to approach this? I'll also appreciate a reference to the literature where such differential equations on Lie groups are treated.","I'm struggling to reproduce Ex. 2 from the paper by Lafferriere and Sussmann. In short, we have a nilpotent Lie algebra of vector fields generated by two elements: $X$ and $Y$. Its basis is $B_1=X$, $B_2=Y$, $B_3=[X,Y]$, $B_4=[X,[X,Y]]$, and $B_5=[Y,[X,Y]]$ (all higher order brackets are zero). The authors write an arbitrary element of the Lie group as $$S(t)=e^{h_5(t)B_5}\circ\dots\circ e^{h_1(t)B_1}$$ and assert that $S(t)$ can be obtaned as a solution to the DE  $$\dot{S}(t)=S(t)(v_1(t)B_1 + v_2(t)B_2 + v_3(t)B_3 + v_4(t)B_4)\quad [\mbox{ note } v_5=0!],$$ with $S(0)=Id$ and where $h_i$'s are related to $v_i$'s by the DEs \begin{align*}\dot{h}_1={ }&v_1,\\ \dot{h}_2={ }&v_2,\\  \dot{h}_3={ }&h_1v_2+v_3\\ \dot{h}_4={ }&\frac{1}{2}h^2_1v_2+h_1v_3+v_4\\ \dot{h}_5={ }&h_2v_3+h_1h_2v_2\end{align*} with zero initial conditions.  I was trying to differentiate $S(t)$, but failed to obtain the relations between $h_i$'s and $v_i$'s as shown above. Could somebody give me a hint at how to approach this? I'll also appreciate a reference to the literature where such differential equations on Lie groups are treated.",,"['group-theory', 'ordinary-differential-equations', 'differential-geometry', 'lie-groups', 'lie-algebras']"
51,Does this system of non-linear differential equations have an analytical solution?,Does this system of non-linear differential equations have an analytical solution?,,"I have this system of linear differential equations: \begin{align*} \frac{dA}{dt} & = k_1 - k_2AB - k_3A\\ \frac{dB}{dt} & = -k_2AB - k_4B\\ \frac{dC}{dt} & = k_2AB - k_5C\\ \frac{dD}{dt} & = k_6C \end{align*} Where $k_{i}$ are constants for $i\in\{1,2,3,4,5,6\}$. Will an analytic solution to these equations exist? Is there an easy way to tell? Thank you in advance","I have this system of linear differential equations: \begin{align*} \frac{dA}{dt} & = k_1 - k_2AB - k_3A\\ \frac{dB}{dt} & = -k_2AB - k_4B\\ \frac{dC}{dt} & = k_2AB - k_5C\\ \frac{dD}{dt} & = k_6C \end{align*} Where $k_{i}$ are constants for $i\in\{1,2,3,4,5,6\}$. Will an analytic solution to these equations exist? Is there an easy way to tell? Thank you in advance",,"['calculus', 'ordinary-differential-equations', 'nonlinear-system']"
52,Is it possible to find a solution to this differential equation?,Is it possible to find a solution to this differential equation?,,"Suppose that $f_1(x,y), f_2(x,y), f_3(x,y)$ are known real valued functions. I am wondering if it is always possible to find another function $A(x,y)$ that satisfies: $f_1 \frac{\partial A}{\partial x} + f_2 \frac{\partial A}{\partial y} = f_3$. I am not even sure how to begin here, how should I proceed? Note that it isn't so important to actually find the function $A$, it would be enough to show that it exists.","Suppose that $f_1(x,y), f_2(x,y), f_3(x,y)$ are known real valued functions. I am wondering if it is always possible to find another function $A(x,y)$ that satisfies: $f_1 \frac{\partial A}{\partial x} + f_2 \frac{\partial A}{\partial y} = f_3$. I am not even sure how to begin here, how should I proceed? Note that it isn't so important to actually find the function $A$, it would be enough to show that it exists.",,['ordinary-differential-equations']
53,Heaviside Step Function,Heaviside Step Function,,"In studying properties of 1/x, its derivatives and its integral,  I came across the following ""apparent"" identity. Plot it, and found that it appears to be the same as the Heaviside function.  Any post with reference to the RHS, or verifying/disproving would be appreciated. $$\frac{\arctan(1/x)+\arctan(x+1/2)+\arctan(2(x^2+1)+x)}{\pi} = H(x)$$ Thanks in advance.","In studying properties of 1/x, its derivatives and its integral,  I came across the following ""apparent"" identity. Plot it, and found that it appears to be the same as the Heaviside function.  Any post with reference to the RHS, or verifying/disproving would be appreciated. $$\frac{\arctan(1/x)+\arctan(x+1/2)+\arctan(2(x^2+1)+x)}{\pi} = H(x)$$ Thanks in advance.",,"['calculus', 'ordinary-differential-equations', 'proof-verification', 'vector-analysis']"
54,Runge-Kutta 4th order,Runge-Kutta 4th order,,"I am not sure if it's a mathematics question or coding part although I am pretty sure that the code I used is right,so do excuse me if I a wrong. I have a set of three first order ode and I am trying to numerically integrate them in python using RK4 method. The problem arises with the exponential term where python rounds it to $0$ and the values thereafter are returned ""NAN"". I tried various packages to deal with large values still no use. I am thinking if I modify the equations or scale them so that the value is smaller might help, but I am not sure how to do that. Any help or the reference to that would be really helpful as I am kind of in a really stuck up situation. Here's the code I have used for this purpose: #simple exponential potential # u = K*phi'/H0; v = omega_matter**(1/3)*(1+z); w = l*K*phi' - ln((K**2)*V0/H0**2),z from 1100(initial) to z=0(final) # f,g,h are functions of derivation of u,v,w respectively derived w.r.t t*H0 = T  import matplotlib.pyplot as plt import numpy as np import math  def f(u,v,w):     return -3*u*((v**3 + (u**2)/6 + np.exp(-w)/3)**0.5) + l*np.exp(-w)  def g(u,v,w):     return -v*(v**3 + (u**2)/6 + np.exp(-w)/3)**0.5  def h(u):     return l*u  z = np.linspace(start=0.0,stop=1.0,num = 10001) print (z)     p = 0.1 q = 1.0 n = 10000.0 dh = (q-p)/n u = [0.0] v = [1101] w = [1.8] l = 1.0   for i in range(0,int(n)): k1 = f(u[i],v[i],w[i]) r1 = g(u[i],v[i],w[i]) s1 = h(u[i]) k2 = f(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2) r2 = g(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2) s2 = h(u[i] + k1*dh/2) k3 = f(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2) r3 = g(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2) s3 = h(u[i] + k2*dh/2) k4 = f(u[i] + dh*k3,v[i] + dh*r3,w[i] + s3*dh) r4 = g(u[i] + k3*dh,v[i] + dh*r3,w[i] + s3*dh) s4 = h(u[i] + dh*k3) u == u.append(u[i] + (dh/6)*(k1 + 2.0*k2 + 2.0*k3 + k4)) v == v.append(v[i] + (dh/6)*(r1 + 2.0*r2 + 2.0*r3 + r4)) w == w.append(w[i] + (dh/6)*(s1 + 2.0*s2 + 2.0*s3 + s4))  plt.plot(z,u, '-b') plt.plot(z,v, '-r') plt.plot(z,w, '-g')  #plt.plot(u,v) #plt.plot(u,w)   plt.title('quintessence cosmological model') plt.show()","I am not sure if it's a mathematics question or coding part although I am pretty sure that the code I used is right,so do excuse me if I a wrong. I have a set of three first order ode and I am trying to numerically integrate them in python using RK4 method. The problem arises with the exponential term where python rounds it to $0$ and the values thereafter are returned ""NAN"". I tried various packages to deal with large values still no use. I am thinking if I modify the equations or scale them so that the value is smaller might help, but I am not sure how to do that. Any help or the reference to that would be really helpful as I am kind of in a really stuck up situation. Here's the code I have used for this purpose: #simple exponential potential # u = K*phi'/H0; v = omega_matter**(1/3)*(1+z); w = l*K*phi' - ln((K**2)*V0/H0**2),z from 1100(initial) to z=0(final) # f,g,h are functions of derivation of u,v,w respectively derived w.r.t t*H0 = T  import matplotlib.pyplot as plt import numpy as np import math  def f(u,v,w):     return -3*u*((v**3 + (u**2)/6 + np.exp(-w)/3)**0.5) + l*np.exp(-w)  def g(u,v,w):     return -v*(v**3 + (u**2)/6 + np.exp(-w)/3)**0.5  def h(u):     return l*u  z = np.linspace(start=0.0,stop=1.0,num = 10001) print (z)     p = 0.1 q = 1.0 n = 10000.0 dh = (q-p)/n u = [0.0] v = [1101] w = [1.8] l = 1.0   for i in range(0,int(n)): k1 = f(u[i],v[i],w[i]) r1 = g(u[i],v[i],w[i]) s1 = h(u[i]) k2 = f(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2) r2 = g(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2) s2 = h(u[i] + k1*dh/2) k3 = f(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2) r3 = g(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2) s3 = h(u[i] + k2*dh/2) k4 = f(u[i] + dh*k3,v[i] + dh*r3,w[i] + s3*dh) r4 = g(u[i] + k3*dh,v[i] + dh*r3,w[i] + s3*dh) s4 = h(u[i] + dh*k3) u == u.append(u[i] + (dh/6)*(k1 + 2.0*k2 + 2.0*k3 + k4)) v == v.append(v[i] + (dh/6)*(r1 + 2.0*r2 + 2.0*r3 + r4)) w == w.append(w[i] + (dh/6)*(s1 + 2.0*s2 + 2.0*s3 + s4))  plt.plot(z,u, '-b') plt.plot(z,v, '-r') plt.plot(z,w, '-g')  #plt.plot(u,v) #plt.plot(u,w)   plt.title('quintessence cosmological model') plt.show()",,"['ordinary-differential-equations', 'numerical-methods', 'exponential-function', 'runge-kutta-methods']"
55,Why do we use eigenvalues and eigenvectors to build Solutions of ODE?,Why do we use eigenvalues and eigenvectors to build Solutions of ODE?,,"I was wondering about why we use eigenvalues and eigenvectors to build solutions of systems of ODE... What is the reasoning behind this procedure? When building approximated solutions near to fixed points, why do we use eigenvalues and eigenvectors to do this? Hugs!","I was wondering about why we use eigenvalues and eigenvectors to build solutions of systems of ODE... What is the reasoning behind this procedure? When building approximated solutions near to fixed points, why do we use eigenvalues and eigenvectors to do this? Hugs!",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
56,Solution of $y''-y'=xe^x$ using Method of Undetermined Coefficients,Solution of  using Method of Undetermined Coefficients,y''-y'=xe^x,"The problem gives $y''-y'=xe^x$ with conditions $y(0)=2$ and $y'\left(0\right)=1$. My issue is when calculating the particular solution of the equation. I assumed that it should take the form $y_p=(Ax+B)e^x$. After substituting into the 2nd ODE, it becomes: $(2A+Ax+B)e^x-(A+Ax+B)e^x=xe^x$ This simplifies to $Ae^x=xe^x$. I'm not sure how to progress. Is $A=0$? This does not seem like the correct way to proceed with solving the problem but I am not sure how to proceed.","The problem gives $y''-y'=xe^x$ with conditions $y(0)=2$ and $y'\left(0\right)=1$. My issue is when calculating the particular solution of the equation. I assumed that it should take the form $y_p=(Ax+B)e^x$. After substituting into the 2nd ODE, it becomes: $(2A+Ax+B)e^x-(A+Ax+B)e^x=xe^x$ This simplifies to $Ae^x=xe^x$. I'm not sure how to progress. Is $A=0$? This does not seem like the correct way to proceed with solving the problem but I am not sure how to proceed.",,['ordinary-differential-equations']
57,Proof verification for differential equation for certain theorem!,Proof verification for differential equation for certain theorem!,,"Theorem Hypothesis: Suppose $u$ is any solution to $n$-th order homogeneous differential equation whereas $v$ is any solution to the $n$-th order non homogeneous differential equation. Then, we can conclude that $y=u+v$ is also a solution to the non homogeneous differential equation Prove that $u$ is any solution to the following second order homogeneous differential equation. $$a_{0}(x)\frac{d^2y}{dx^2}+a_{1}(x)\frac{dy}{dx}+a_{2}(x)y=0$$ and $v$ is any solution to the following second order non-homogeneous differential equation, $$a_{0}(x)\frac{d^2y}{dx^2}+a_{1}(x)\frac{dy}{dx}+a_{2}(x)y=F(x)$$ then, $u+v$ is also a solution to the second order non-homogeneous differential equation! My work is then, let $y=u$ ,$\frac{dy}{dx}=\frac{du}{dx}$, $\frac{d^2y}{dx^2}=\frac{d^2u}{dx^2}$ So, $$a_{0}(x)\frac{d^2u}{dx^2}+a_{1}(x)\frac{du}{dx}+a_{2}(x)u=0$$ Suppose $y=v$ ,$\frac{dy}{dx}=\frac{dv}{dx}$, $\frac{d^2y}{dx^2}=\frac{d^2v}{dx^2}$ $$a_{0}(x)\frac{d^2v}{dx^2}+a_{1}(x)\frac{dv}{dx}+a_{2}(x)v=F(x)$$ Adding these two solutions together we would have $y=u+v$ as the solution to the non homogeneous differential equation! Is this the right way to do the proof?","Theorem Hypothesis: Suppose $u$ is any solution to $n$-th order homogeneous differential equation whereas $v$ is any solution to the $n$-th order non homogeneous differential equation. Then, we can conclude that $y=u+v$ is also a solution to the non homogeneous differential equation Prove that $u$ is any solution to the following second order homogeneous differential equation. $$a_{0}(x)\frac{d^2y}{dx^2}+a_{1}(x)\frac{dy}{dx}+a_{2}(x)y=0$$ and $v$ is any solution to the following second order non-homogeneous differential equation, $$a_{0}(x)\frac{d^2y}{dx^2}+a_{1}(x)\frac{dy}{dx}+a_{2}(x)y=F(x)$$ then, $u+v$ is also a solution to the second order non-homogeneous differential equation! My work is then, let $y=u$ ,$\frac{dy}{dx}=\frac{du}{dx}$, $\frac{d^2y}{dx^2}=\frac{d^2u}{dx^2}$ So, $$a_{0}(x)\frac{d^2u}{dx^2}+a_{1}(x)\frac{du}{dx}+a_{2}(x)u=0$$ Suppose $y=v$ ,$\frac{dy}{dx}=\frac{dv}{dx}$, $\frac{d^2y}{dx^2}=\frac{d^2v}{dx^2}$ $$a_{0}(x)\frac{d^2v}{dx^2}+a_{1}(x)\frac{dv}{dx}+a_{2}(x)v=F(x)$$ Adding these two solutions together we would have $y=u+v$ as the solution to the non homogeneous differential equation! Is this the right way to do the proof?",,"['ordinary-differential-equations', 'proof-verification']"
58,"Prob. 27, Chap. 5, in Baby Rudin: The initial-value problem $y^\prime = \phi(x,y)$, $y(a)=c$","Prob. 27, Chap. 5, in Baby Rudin: The initial-value problem ,","y^\prime = \phi(x,y) y(a)=c","Here is Prob. 27, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\phi$ be a real function defined on a rectangle $R$ in the plane, given by $a \leq x \leq b$, $\alpha \leq y \leq \beta$. A solution of the initial-value problem    $$ y^\prime = \phi(x, y), \qquad y(a) = c \qquad (\alpha \leq c \leq \beta) $$   is, by definition, a differentiable function $f$ on $[a, b]$ such that $f(a) = c$, $\alpha \leq f(x) \leq \beta$, and    $$ f^\prime(x) = \phi\left( x, f(x) \right) \qquad (a \leq x \leq b). $$    Prove that such a problem has at most one solution if there is a constant $A$ such that    $$ \left| \phi \left(x, y_2 \right) - \phi \left(x, y_1 \right) \right| \leq A \left| y_2 - y_1 \right| $$   whenever $\left( x, y_1 \right) \in R$ and $\left( x, y_2 \right) \in R$. Hint: Apply Exercise 26 to the difference of two solutions. Note that this uniqueness theorem does not hold for the initial-value problem    $$ y^\prime = y^{1/2}, \qquad y(0) = 0, $$    which has two solutions: $f(x) = 0$ and $f(x) = x^2/4$. Find all other solutions. Here is the link to my post here on Math SE on Prob. 26, Chap. 5, in Baby Rudin, 3rd edition: Prob. 26, Chap. 5 in Baby Rudin: If $\left| f^\prime(x) \right| \leq A \left| f(x) \right|$ on $[a, b]$, then $f = 0$ My Effort: Let $f_1$ and $f_2$ be any two solutions of the given initial-value problem, and let $g = f_1 - f_2$. Then as $f_1$ and $f_2$ are solutions to the given IVP, so $f_1$ and $f_2$ are differentiable functions on $[a, b]$ such that $f_1(a) = f_2(a) = c$; $\alpha \leq f_1(x) \leq \beta$ and $\alpha \leq f_2(x) \leq \beta$; and    $$ f_1^\prime(x) = \phi \left( x, f_1(x) \right) \ \mbox{ and } \ f_2^\prime(x) = \phi \left( x, f_2(x) \right) $$   for all $x \in [a, b]$. For all $x \in [a, b]$, since  $\left( x, f_1(x) \right) \in R$ and $\left( x, f_2(x) \right) \in R$, therefore we can conclude that $$ \left| g^\prime(x) \right| = \left| f_1^\prime(x) - f_2^\prime(x) \right| = \left| \phi \left( x, f_1(x) \right) - \phi \left( x, f_2(x) \right) \right| \leq A \left| f_1(x) - f_2(x) \right| = A \left| g(x) \right| $$   for all $x \in [a, b]$. Moreover,    $$ g(a) = f_1(a) - f_2(a) = c - c = 0.$$   So, by the conclusion in Prob. 26, we have $g(x) = 0$ for all $x \in [a, b]$; that is, $f_1(x) = f_2(x)$ for all $x \in [a, b]$; that is, $f_1 = f_2$, and so the given initial-value problem has at most one solution. Is this proof correct? Now for the IVP $$ y^\prime = y^{1/2}, \qquad y(0) = 0. \tag{1}$$    Let $b$ be any real number such that $b > 0$. If $f(x) = 0$ for all $x \in [0, b]$, then   $$f^\prime(x) = 0 = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$   Thus $f$ is a solution to the initial-value problem (1). And, if $f(x) = x^2/4$ for all $x \in [0, b]$, then we have    $$ f^\prime(x) = \frac{x}{2} = \left( \frac{x^2}{4} \right)^{1/2} = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$   Thus this $f$ is also a solution to (1). Now if $y \neq \hat{0}$, the zero function, then from (1) we obtain    $$ \frac{1}{y^{1/2}} y^\prime = 1,$$   and so, upon integrating both sides, we get   $$ 2 y^{1/2} = x + c, $$   where $c$ is an arbitrary constant of integration.    Therefore, $$ y(x) = { (x+c)^2 \over 4 } \tag{2}$$   for all $x \in [0, b]$. As $y(0) = 0$, so from (2) we obtain    $$ c^2/4 = 0,$$   which implies that $c = 0$. So (2) becomes   $$ y(x) = x^2/4$$   for all $x \in [0, b]$. Thus, for any real number $b > 0$, the initial-value problem (2) has only the following two solutions: $f_1(x) = 0$ and $f_2(x) = x^2/4$ for all $x \in [0, b]$. Is my reasoning correct? If so, then have I arrived at the correct conclusion as well?","Here is Prob. 27, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\phi$ be a real function defined on a rectangle $R$ in the plane, given by $a \leq x \leq b$, $\alpha \leq y \leq \beta$. A solution of the initial-value problem    $$ y^\prime = \phi(x, y), \qquad y(a) = c \qquad (\alpha \leq c \leq \beta) $$   is, by definition, a differentiable function $f$ on $[a, b]$ such that $f(a) = c$, $\alpha \leq f(x) \leq \beta$, and    $$ f^\prime(x) = \phi\left( x, f(x) \right) \qquad (a \leq x \leq b). $$    Prove that such a problem has at most one solution if there is a constant $A$ such that    $$ \left| \phi \left(x, y_2 \right) - \phi \left(x, y_1 \right) \right| \leq A \left| y_2 - y_1 \right| $$   whenever $\left( x, y_1 \right) \in R$ and $\left( x, y_2 \right) \in R$. Hint: Apply Exercise 26 to the difference of two solutions. Note that this uniqueness theorem does not hold for the initial-value problem    $$ y^\prime = y^{1/2}, \qquad y(0) = 0, $$    which has two solutions: $f(x) = 0$ and $f(x) = x^2/4$. Find all other solutions. Here is the link to my post here on Math SE on Prob. 26, Chap. 5, in Baby Rudin, 3rd edition: Prob. 26, Chap. 5 in Baby Rudin: If $\left| f^\prime(x) \right| \leq A \left| f(x) \right|$ on $[a, b]$, then $f = 0$ My Effort: Let $f_1$ and $f_2$ be any two solutions of the given initial-value problem, and let $g = f_1 - f_2$. Then as $f_1$ and $f_2$ are solutions to the given IVP, so $f_1$ and $f_2$ are differentiable functions on $[a, b]$ such that $f_1(a) = f_2(a) = c$; $\alpha \leq f_1(x) \leq \beta$ and $\alpha \leq f_2(x) \leq \beta$; and    $$ f_1^\prime(x) = \phi \left( x, f_1(x) \right) \ \mbox{ and } \ f_2^\prime(x) = \phi \left( x, f_2(x) \right) $$   for all $x \in [a, b]$. For all $x \in [a, b]$, since  $\left( x, f_1(x) \right) \in R$ and $\left( x, f_2(x) \right) \in R$, therefore we can conclude that $$ \left| g^\prime(x) \right| = \left| f_1^\prime(x) - f_2^\prime(x) \right| = \left| \phi \left( x, f_1(x) \right) - \phi \left( x, f_2(x) \right) \right| \leq A \left| f_1(x) - f_2(x) \right| = A \left| g(x) \right| $$   for all $x \in [a, b]$. Moreover,    $$ g(a) = f_1(a) - f_2(a) = c - c = 0.$$   So, by the conclusion in Prob. 26, we have $g(x) = 0$ for all $x \in [a, b]$; that is, $f_1(x) = f_2(x)$ for all $x \in [a, b]$; that is, $f_1 = f_2$, and so the given initial-value problem has at most one solution. Is this proof correct? Now for the IVP $$ y^\prime = y^{1/2}, \qquad y(0) = 0. \tag{1}$$    Let $b$ be any real number such that $b > 0$. If $f(x) = 0$ for all $x \in [0, b]$, then   $$f^\prime(x) = 0 = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$   Thus $f$ is a solution to the initial-value problem (1). And, if $f(x) = x^2/4$ for all $x \in [0, b]$, then we have    $$ f^\prime(x) = \frac{x}{2} = \left( \frac{x^2}{4} \right)^{1/2} = \left( f(x) \right)^{1/2}, \ \mbox{ and } \ f(0) = 0.$$   Thus this $f$ is also a solution to (1). Now if $y \neq \hat{0}$, the zero function, then from (1) we obtain    $$ \frac{1}{y^{1/2}} y^\prime = 1,$$   and so, upon integrating both sides, we get   $$ 2 y^{1/2} = x + c, $$   where $c$ is an arbitrary constant of integration.    Therefore, $$ y(x) = { (x+c)^2 \over 4 } \tag{2}$$   for all $x \in [0, b]$. As $y(0) = 0$, so from (2) we obtain    $$ c^2/4 = 0,$$   which implies that $c = 0$. So (2) becomes   $$ y(x) = x^2/4$$   for all $x \in [0, b]$. Thus, for any real number $b > 0$, the initial-value problem (2) has only the following two solutions: $f_1(x) = 0$ and $f_2(x) = x^2/4$ for all $x \in [0, b]$. Is my reasoning correct? If so, then have I arrived at the correct conclusion as well?",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'derivatives', 'initial-value-problems']"
59,Solving ode similar to Adler's equation,Solving ode similar to Adler's equation,,"I would like to find the exact solution of the following form: $$f'(t) = ae^{-bt} - K \sin(f(t)) $$ It is similar to Adler's equation, but it can not be solved by the method for finding exact solution of Adler's equation. Is there any way to solve this?","I would like to find the exact solution of the following form: $$f'(t) = ae^{-bt} - K \sin(f(t)) $$ It is similar to Adler's equation, but it can not be solved by the method for finding exact solution of Adler's equation. Is there any way to solve this?",,['ordinary-differential-equations']
60,How to solve this ODE? How to evaluate this integral?,How to solve this ODE? How to evaluate this integral?,,"How to solve the following ODE? $$ \frac{ \mathrm{d} x(t) }{ \mathrm{d} t} = \alpha x(t) \left( 1 -  \frac{ x(t) }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{- \gamma (t - \lambda) }}  } \right),$$ where $x = x(t)$ is the unknown function, whereas $\alpha$, $\beta$, $\gamma$, and $\lambda$ are constants. My Attempt: We can write the given ODE as    $$ -\frac{1}{x^2} x^\prime + \frac{\alpha}{x} = \frac{ \alpha }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }. $$   Or,    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{1}{x} \right) + \frac{\alpha}{x} = \frac{ \alpha }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }. $$   Multiplying both sides of the last ODE by $\mathrm{e}^{\alpha t}$, we get    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{1}{x} \right) \mathrm{e}^{\alpha t} + \frac{\alpha \mathrm{e}^{\alpha t}}{x} = \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }, $$   which can be written as    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{\mathrm{e}^{\alpha t} }{x} \right) = \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }, $$   and so    $$ \begin{align} \frac{\mathrm{e}^{\alpha t} }{x} &= \int \ \left(  \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  } \right) \ \mathrm{d} t. \\ &= \alpha \int \ \left( \frac{ \mathrm{e}^{\alpha t} }{ \beta + \frac{\gamma \mathrm{e}^{\gamma(t-\lambda)}}{ \mathrm{e}^{\gamma(t-\lambda)} +1  } } \right) \ \mathrm{d} t \\ &= \alpha \int \ \left( \frac{\mathrm{e}^{\alpha t} \left( \mathrm{e}^{\gamma(t-\lambda) } + 1 \right) }{\beta \left( \mathrm{e}^{\gamma(t-\lambda) } + 1 \right) + \gamma \mathrm{e}^{\gamma(t-\lambda) } } \right) \ \mathrm{d} t \\ &= \alpha  \mathrm{e}^{-\gamma \lambda} \int \ \left( \frac{ \mathrm{e}^{ (\alpha+\gamma)t } + \mathrm{e}^{\alpha t} }{ (\beta + \gamma)  \mathrm{e}^{-\gamma \lambda} \mathrm{e}^{\gamma t} + \beta } \right) \ \mathrm{d} t \\ &=  \end{align} $$ What next? Is what I've done so far correct? If so, then can we analytically evaluate the integral? I'm afraid I can't think of any trick that will work for sure.","How to solve the following ODE? $$ \frac{ \mathrm{d} x(t) }{ \mathrm{d} t} = \alpha x(t) \left( 1 -  \frac{ x(t) }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{- \gamma (t - \lambda) }}  } \right),$$ where $x = x(t)$ is the unknown function, whereas $\alpha$, $\beta$, $\gamma$, and $\lambda$ are constants. My Attempt: We can write the given ODE as    $$ -\frac{1}{x^2} x^\prime + \frac{\alpha}{x} = \frac{ \alpha }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }. $$   Or,    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{1}{x} \right) + \frac{\alpha}{x} = \frac{ \alpha }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }. $$   Multiplying both sides of the last ODE by $\mathrm{e}^{\alpha t}$, we get    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{1}{x} \right) \mathrm{e}^{\alpha t} + \frac{\alpha \mathrm{e}^{\alpha t}}{x} = \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }, $$   which can be written as    $$ \frac{ \mathrm{d} }{ \mathrm{d} t} \left( \frac{\mathrm{e}^{\alpha t} }{x} \right) = \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  }, $$   and so    $$ \begin{align} \frac{\mathrm{e}^{\alpha t} }{x} &= \int \ \left(  \frac{ \alpha \mathrm{e}^{\alpha t} }{ \beta + \frac{ \gamma }{ 1 + \mathrm{e}^{-\gamma(t-\lambda) }}  } \right) \ \mathrm{d} t. \\ &= \alpha \int \ \left( \frac{ \mathrm{e}^{\alpha t} }{ \beta + \frac{\gamma \mathrm{e}^{\gamma(t-\lambda)}}{ \mathrm{e}^{\gamma(t-\lambda)} +1  } } \right) \ \mathrm{d} t \\ &= \alpha \int \ \left( \frac{\mathrm{e}^{\alpha t} \left( \mathrm{e}^{\gamma(t-\lambda) } + 1 \right) }{\beta \left( \mathrm{e}^{\gamma(t-\lambda) } + 1 \right) + \gamma \mathrm{e}^{\gamma(t-\lambda) } } \right) \ \mathrm{d} t \\ &= \alpha  \mathrm{e}^{-\gamma \lambda} \int \ \left( \frac{ \mathrm{e}^{ (\alpha+\gamma)t } + \mathrm{e}^{\alpha t} }{ (\beta + \gamma)  \mathrm{e}^{-\gamma \lambda} \mathrm{e}^{\gamma t} + \beta } \right) \ \mathrm{d} t \\ &=  \end{align} $$ What next? Is what I've done so far correct? If so, then can we analytically evaluate the integral? I'm afraid I can't think of any trick that will work for sure.",,"['analysis', 'ordinary-differential-equations']"
61,Reformulate the initial value problem as an equivalent integral equation,Reformulate the initial value problem as an equivalent integral equation,,"Reformulate this initial value problem as an equivalent integral equation. $$\dfrac{d^2y}{dx^2}+y=x,  \ \ \ \ 0\leq x \leq 1, \ \ \ \ y(0)=1, y'(1)=0 $$ I'm not sure how to do this. I think I start by letting $\phi=\dfrac{d^2y}{dx^2}$ and integrating over $[0,1]$ which gives: $$y'(1)-y'(0)=\int^1_0 \phi(t) \ dt $$ Then using $y'(1)=0$, this gives: $$y'(0)=-\int^1_0 \phi(t) \ dt$$ Do I then integrate this over $[0,x]$? This is one example of a few inital value problems that I'm not sure how to reformulate so is there a method I follow so that I can apply this to the others?","Reformulate this initial value problem as an equivalent integral equation. $$\dfrac{d^2y}{dx^2}+y=x,  \ \ \ \ 0\leq x \leq 1, \ \ \ \ y(0)=1, y'(1)=0 $$ I'm not sure how to do this. I think I start by letting $\phi=\dfrac{d^2y}{dx^2}$ and integrating over $[0,1]$ which gives: $$y'(1)-y'(0)=\int^1_0 \phi(t) \ dt $$ Then using $y'(1)=0$, this gives: $$y'(0)=-\int^1_0 \phi(t) \ dt$$ Do I then integrate this over $[0,x]$? This is one example of a few inital value problems that I'm not sure how to reformulate so is there a method I follow so that I can apply this to the others?",,"['ordinary-differential-equations', 'integral-equations', 'initial-value-problems']"
62,Using approximate solution for system,Using approximate solution for system,,"So I have a system: \begin{align*} x' &= (2y+cx)(z+1)\\ y' &= (cy-x)(z+1)\\ z' &= -z^3 \end{align*} Solving for $z$ yields $$z = \frac{1}{\sqrt{2t+c_1}}$$ for $z(0)>0$. I want to show that if $c<0$ every solution converges to the origin. The question is, since $z$ approaches $0$ for t large, can I just set $z = 0$ and can go from there? Do I need to show that $x$ and $y$ are bounded first? I think $x$ and $y$ must be bounded because of the way the derivatives are set up(if x is large and postive its derivative becomes negative), but I'm not sure how to formally prove this.","So I have a system: \begin{align*} x' &= (2y+cx)(z+1)\\ y' &= (cy-x)(z+1)\\ z' &= -z^3 \end{align*} Solving for $z$ yields $$z = \frac{1}{\sqrt{2t+c_1}}$$ for $z(0)>0$. I want to show that if $c<0$ every solution converges to the origin. The question is, since $z$ approaches $0$ for t large, can I just set $z = 0$ and can go from there? Do I need to show that $x$ and $y$ are bounded first? I think $x$ and $y$ must be bounded because of the way the derivatives are set up(if x is large and postive its derivative becomes negative), but I'm not sure how to formally prove this.",,['ordinary-differential-equations']
63,Question about a 3rd order linear PDE,Question about a 3rd order linear PDE,,"Consider $f(x,y,t)$ whose spatial second order derivatives evolve over time based on the following pair of PDEs: $$\begin{align} \frac{\partial^3}{\partial x^2 \partial t}f\,&=\,\frac{1}{2}\left(\frac{\partial^2}{\partial y^2}f-\frac{\partial^2}{\partial x^2}f\right)\\ \\ \frac{\partial^3}{\partial y^2 \partial t}f\,&=\,\frac{1}{2}\left(\frac{\partial^2}{\partial x^2}f-\frac{\partial^2}{\partial y^2}f\right) \end{align}$$ The process is subject to initial condition $f(x,y,t=0)=f_0(x,y)$ for some given $f_0(x,y)$. Is there an analytical expression for the solution $f$ in terms of $f_0$? Help would be greatly appreciated. Golabi","Consider $f(x,y,t)$ whose spatial second order derivatives evolve over time based on the following pair of PDEs: $$\begin{align} \frac{\partial^3}{\partial x^2 \partial t}f\,&=\,\frac{1}{2}\left(\frac{\partial^2}{\partial y^2}f-\frac{\partial^2}{\partial x^2}f\right)\\ \\ \frac{\partial^3}{\partial y^2 \partial t}f\,&=\,\frac{1}{2}\left(\frac{\partial^2}{\partial x^2}f-\frac{\partial^2}{\partial y^2}f\right) \end{align}$$ The process is subject to initial condition $f(x,y,t=0)=f_0(x,y)$ for some given $f_0(x,y)$. Is there an analytical expression for the solution $f$ in terms of $f_0$? Help would be greatly appreciated. Golabi",,"['ordinary-differential-equations', 'partial-differential-equations']"
64,Given a solution of a Differential Equation. How to find the another solution in this exercise?,Given a solution of a Differential Equation. How to find the another solution in this exercise?,,"$y'''-x^2y'+xy=0$, solve the differential equation if $y_1=x$ is a solution. I've tryed using the sustitution $y=u(x)y_1(x)=u(x) \cdot x$. Taking the derivates, the final form is like this: $$(3u''+xu''')-x^2(u+xu')+x(u \cdot x)=0$$ $$3u''+xu'''-x^2u-x^3u'+ux^2=0$$ $$xu'''+3u''-x^3u'=0$$ But, now i have antoher D.E of degree 3. How i can find the another solution?.","$y'''-x^2y'+xy=0$, solve the differential equation if $y_1=x$ is a solution. I've tryed using the sustitution $y=u(x)y_1(x)=u(x) \cdot x$. Taking the derivates, the final form is like this: $$(3u''+xu''')-x^2(u+xu')+x(u \cdot x)=0$$ $$3u''+xu'''-x^2u-x^3u'+ux^2=0$$ $$xu'''+3u''-x^3u'=0$$ But, now i have antoher D.E of degree 3. How i can find the another solution?.",,['ordinary-differential-equations']
65,The derivative of a moving L2 norm,The derivative of a moving L2 norm,,"Consider the class of $C^0$ signals $x(t)$ such that $$ \left|x\left(t+T\right)-x\left(t\right)\right|\leq\epsilon  $$ I would like to find a uniform bound on the derivative of its moving rms, which is defined by $$ J\left(t\right)=\left(\frac{1}{T}\int_{t-T}^{t}\left|x\left(\tau\right)\right|^{p}d\tau\right)^{\frac{1}{p}} $$ with $p=2$. Apparently, when $\epsilon=0$, $J$ is always constant and the derivative is uniformly 0; so I wonder whether $ \left|\frac{dJ}{dt}\right| $ is uniformly small when $\epsilon$ is small. It might be worth noting that a uniform bound could be found when $p=1$ (in the case of moving average), see https://math.stackexchange.com/q/57307 . It would also be interesting to find out whether it holds for other $p\in[1,\infty]$.","Consider the class of $C^0$ signals $x(t)$ such that $$ \left|x\left(t+T\right)-x\left(t\right)\right|\leq\epsilon  $$ I would like to find a uniform bound on the derivative of its moving rms, which is defined by $$ J\left(t\right)=\left(\frac{1}{T}\int_{t-T}^{t}\left|x\left(\tau\right)\right|^{p}d\tau\right)^{\frac{1}{p}} $$ with $p=2$. Apparently, when $\epsilon=0$, $J$ is always constant and the derivative is uniformly 0; so I wonder whether $ \left|\frac{dJ}{dt}\right| $ is uniformly small when $\epsilon$ is small. It might be worth noting that a uniform bound could be found when $p=1$ (in the case of moving average), see https://math.stackexchange.com/q/57307 . It would also be interesting to find out whether it holds for other $p\in[1,\infty]$.",,"['calculus', 'functional-analysis', 'ordinary-differential-equations', 'signal-processing']"
66,Reformulating a discrete model into a continuous model?,Reformulating a discrete model into a continuous model?,,"Past Paper Question: Given a model of a ﬁsh population undergoing constant effort harvesting, where $N_i$ is the total population at time point $i$: $$N_{i+1}=N_{i}+BN_{i}-DN_{i}^{2}-EN_{i},$$ where $BN_{i}$ are the number of births, $DN_{i}^{2}$ are the number of death, and $EN_{i}$ is the harvesting rate. Reformulate the above model as a discrete model for constant yield ﬁshing, then take a small time limit to ﬁnd the equivalent continuous model. My Attempt: Not sure about the first part, but for the second part I think you make the substitution $N_{i+1}=N\left( t+\delta t\right)$ to get the form:   $$\dfrac {N\left( t+\delta t\right) -N\left( t\right) } {\delta t}=BN-DN^{2}-EN,$$   let $\delta t\rightarrow 0$ to give:   $$\dfrac {dN } {dt}=BN-DN^{2}-EN$$ My Question: Is this the correct approach and what is the final answer?","Past Paper Question: Given a model of a ﬁsh population undergoing constant effort harvesting, where $N_i$ is the total population at time point $i$: $$N_{i+1}=N_{i}+BN_{i}-DN_{i}^{2}-EN_{i},$$ where $BN_{i}$ are the number of births, $DN_{i}^{2}$ are the number of death, and $EN_{i}$ is the harvesting rate. Reformulate the above model as a discrete model for constant yield ﬁshing, then take a small time limit to ﬁnd the equivalent continuous model. My Attempt: Not sure about the first part, but for the second part I think you make the substitution $N_{i+1}=N\left( t+\delta t\right)$ to get the form:   $$\dfrac {N\left( t+\delta t\right) -N\left( t\right) } {\delta t}=BN-DN^{2}-EN,$$   let $\delta t\rightarrow 0$ to give:   $$\dfrac {dN } {dt}=BN-DN^{2}-EN$$ My Question: Is this the correct approach and what is the final answer?",,"['calculus', 'ordinary-differential-equations', 'mathematical-modeling', 'biology']"
67,Solve $x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{\tan t}$,Solve,x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{\tan t},"Solve $$x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{\tan t}$$ Hint is to find and use substitution $u(x)=\dots$ I try $u=\ln x$, then $x=e^u, x'=e^uu'$ And we obtain: $$u'+\frac{u}{\cos^2t}=\frac{\sin t}{\cos^2t}te^{\tan t}$$ So first we solve corresponding homogeneous equation and obtain $u=c\cdot e^{-\tan t}$. We assume that $c=c(t)$ and put it into inhomogeneous equation. The problem is that now we need to solve integral $$c=\int\frac{\sin t}{\cos^2t}te^{2\tan t}$$ and I do not know if it is even solvable. It would be easier if our equation to solve was $$x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{-\tan t}$$. But it isn't. So I do not know how to proceed.","Solve $$x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{\tan t}$$ Hint is to find and use substitution $u(x)=\dots$ I try $u=\ln x$, then $x=e^u, x'=e^uu'$ And we obtain: $$u'+\frac{u}{\cos^2t}=\frac{\sin t}{\cos^2t}te^{\tan t}$$ So first we solve corresponding homogeneous equation and obtain $u=c\cdot e^{-\tan t}$. We assume that $c=c(t)$ and put it into inhomogeneous equation. The problem is that now we need to solve integral $$c=\int\frac{\sin t}{\cos^2t}te^{2\tan t}$$ and I do not know if it is even solvable. It would be easier if our equation to solve was $$x'\cos^2t+x\ln{x}=x\sin{t}\cdot te^{-\tan t}$$. But it isn't. So I do not know how to proceed.",,['ordinary-differential-equations']
68,Qualitatively analyzing the local behavior of equilibria,Qualitatively analyzing the local behavior of equilibria,,"I have the system $$r' = r-r^3$$ $$\theta' = \sin(\theta)^2-\frac{1}{4}$$ I know the equilibria are $$(0,0) \space (1,\frac{\pi}{6}) \space (1,-\frac{\pi}{6}) \space (1, \frac{5\pi}{6}) \space (1, -\frac{5\pi}{6})$$ but I do not know how to analyze the local behavior of each one without using the hartman-grobman theorem. I want to be able to figure out if each one is a source or a sink, straight line solutions, spirals, centers, etc. All with minimal computation. Also try to try to draw the whole phase portrait using qualitative methods (I know this is possible because I saw a professor do it in thirty seconds in his head, writing on a blackboard. He did not, however, explain his thinking).  Any help is appreciated. If you give me hints that lead me to the answer , I will accept and post my solution in the question.","I have the system $$r' = r-r^3$$ $$\theta' = \sin(\theta)^2-\frac{1}{4}$$ I know the equilibria are $$(0,0) \space (1,\frac{\pi}{6}) \space (1,-\frac{\pi}{6}) \space (1, \frac{5\pi}{6}) \space (1, -\frac{5\pi}{6})$$ but I do not know how to analyze the local behavior of each one without using the hartman-grobman theorem. I want to be able to figure out if each one is a source or a sink, straight line solutions, spirals, centers, etc. All with minimal computation. Also try to try to draw the whole phase portrait using qualitative methods (I know this is possible because I saw a professor do it in thirty seconds in his head, writing on a blackboard. He did not, however, explain his thinking).  Any help is appreciated. If you give me hints that lead me to the answer , I will accept and post my solution in the question.",,['ordinary-differential-equations']
69,Solving hyperbolic partial differential equation,Solving hyperbolic partial differential equation,,"I'm having hard time solving this pde. $$U_{xx} - 2\cos(x) U_{xy} - (15+\sin^2(x)) U_{yy} + \sin(x) U_{y} = 0$$ With following initial conditions $$U(x,y)|_{y = -\sin(x)} = 4\sin(x)$$ $$\frac{\partial U(x,y)}{\partial y}|_{y = -\sin(x)} = 2 + 2\sin(x)$$ Solving the characteristic equation we get. $$\frac{dy}{dx} = - \cos(x) \pm 4x$$ So we need to use this kind of substitution $\xi = y + \sin(x) - 4x $ $\eta = y + \sin(x) + 4x $ and then computing partial derivitives we get $U_{x} = (\cos(x) - 4)U_{\xi} + (\cos(x) + 4)U_{\eta}$ $U_{y} = (U_{\xi} + U_{\eta})$ $U_{xx} = (\cos(x) - 4)^2 U_{\xi\xi} +2(\cos^2(x) - 16)U_{\xi\eta} + (\cos(x) + 4)^2U_{\eta\eta} - \sin(x)U_{\xi} - \sin(x)U_{\eta}$ $U_{xy} = (\cos(x) - 4)U_{\xi\xi} + 2\cos(x)U_{\xi\eta} + (\cos(x) + 4)U_{\eta\eta}$ $U_{yy} = U_{\xi\xi} + 2U_{\xi\eta} + U_{\eta\eta}$ After substituting them into original equation we get. $$U_{\xi\eta} = 0$$ After integrating wrt $\xi$ and wrt $\eta$ $$U(\xi,\eta) = f(\xi) + g(\eta)$$ After substituting $x$ and $y$ $$U(x,y) = f(y + \sin(x) - 4x) + g(y + \sin(x) + 4x )$$ $$U_{y}(x,y) = f'_{y}(y + \sin(x) - 4x) + g'_{y}(y + \sin(x) + 4x )$$ Now it's time to solve the Cauchy problem. I've got this system, and this is where I'm stuck $\left\{\begin{matrix} f(-4x) + g(+ 4x ) = 4\sin(x) \\  f'_{y}(- 4x) + g'_{y}(+ 4x ) = 2 + 2 \sin(x) \end{matrix}\right.$ Any hints how to proceed? Thank you :)","I'm having hard time solving this pde. $$U_{xx} - 2\cos(x) U_{xy} - (15+\sin^2(x)) U_{yy} + \sin(x) U_{y} = 0$$ With following initial conditions $$U(x,y)|_{y = -\sin(x)} = 4\sin(x)$$ $$\frac{\partial U(x,y)}{\partial y}|_{y = -\sin(x)} = 2 + 2\sin(x)$$ Solving the characteristic equation we get. $$\frac{dy}{dx} = - \cos(x) \pm 4x$$ So we need to use this kind of substitution $\xi = y + \sin(x) - 4x $ $\eta = y + \sin(x) + 4x $ and then computing partial derivitives we get $U_{x} = (\cos(x) - 4)U_{\xi} + (\cos(x) + 4)U_{\eta}$ $U_{y} = (U_{\xi} + U_{\eta})$ $U_{xx} = (\cos(x) - 4)^2 U_{\xi\xi} +2(\cos^2(x) - 16)U_{\xi\eta} + (\cos(x) + 4)^2U_{\eta\eta} - \sin(x)U_{\xi} - \sin(x)U_{\eta}$ $U_{xy} = (\cos(x) - 4)U_{\xi\xi} + 2\cos(x)U_{\xi\eta} + (\cos(x) + 4)U_{\eta\eta}$ $U_{yy} = U_{\xi\xi} + 2U_{\xi\eta} + U_{\eta\eta}$ After substituting them into original equation we get. $$U_{\xi\eta} = 0$$ After integrating wrt $\xi$ and wrt $\eta$ $$U(\xi,\eta) = f(\xi) + g(\eta)$$ After substituting $x$ and $y$ $$U(x,y) = f(y + \sin(x) - 4x) + g(y + \sin(x) + 4x )$$ $$U_{y}(x,y) = f'_{y}(y + \sin(x) - 4x) + g'_{y}(y + \sin(x) + 4x )$$ Now it's time to solve the Cauchy problem. I've got this system, and this is where I'm stuck $\left\{\begin{matrix} f(-4x) + g(+ 4x ) = 4\sin(x) \\  f'_{y}(- 4x) + g'_{y}(+ 4x ) = 2 + 2 \sin(x) \end{matrix}\right.$ Any hints how to proceed? Thank you :)",,"['ordinary-differential-equations', 'partial-differential-equations']"
70,Is it possible for a 3 by 3 matrix to use Trace-Determinants plane to see convergency?,Is it possible for a 3 by 3 matrix to use Trace-Determinants plane to see convergency?,,"I am showing in a report the trace det graph if solutions to different equations for linear systems will convert (attractor) or divert (repulsor). Is it possible to draw a 3D graph showing the correlation between the Trace(A) and Det(A) for a 3 by 3 linear system to show if the system will converge or not, in the same way you would for a 2 by 2 system.","I am showing in a report the trace det graph if solutions to different equations for linear systems will convert (attractor) or divert (repulsor). Is it possible to draw a 3D graph showing the correlation between the Trace(A) and Det(A) for a 3 by 3 linear system to show if the system will converge or not, in the same way you would for a 2 by 2 system.",,['ordinary-differential-equations']
71,How to obtain error bound on Runge-Kutta 4th order method?,How to obtain error bound on Runge-Kutta 4th order method?,,"I am using Runge-Kutta 4th order method for numerical simulation of system of ODEs $x'=f(t,x)$ where $x$ has around 20 dimensions. I wish to give a bound on the global error. I know that the global accumulated error is $O(h^4)$, but I need an actual bound, ie what is the coefficient of $h^4$ and a bound on the terms that comes after $h^4$. I can obtain a bound on both $|x|$ and $|\frac{dx}{dt}|$ (by appealing to physics, since my system is modelling some real-life things), but it's hard to compute bounds for higher order derivatives. I'm sure there are many methods for error estimation, but I just need one. It doesn't have to be asymptotically tight, just an upper bound is enough.","I am using Runge-Kutta 4th order method for numerical simulation of system of ODEs $x'=f(t,x)$ where $x$ has around 20 dimensions. I wish to give a bound on the global error. I know that the global accumulated error is $O(h^4)$, but I need an actual bound, ie what is the coefficient of $h^4$ and a bound on the terms that comes after $h^4$. I can obtain a bound on both $|x|$ and $|\frac{dx}{dt}|$ (by appealing to physics, since my system is modelling some real-life things), but it's hard to compute bounds for higher order derivatives. I'm sure there are many methods for error estimation, but I just need one. It doesn't have to be asymptotically tight, just an upper bound is enough.",,"['ordinary-differential-equations', 'numerical-methods']"
72,What differential equation does this function satisfy?,What differential equation does this function satisfy?,,"Given the equation: $$\nabla f(x) \times \nabla g(x) = 0$$ for two scalar fields $f$ and $g$. It follows that when this is satisfied $h(f,g)=0$ for some function $h$. The question is to find a smooth function $h$ given functions $f$ and $g$ such $h(s,t)=0$ if and only if there exists a solution to: $s=f(x), t=g(x), \nabla f(x) \times \nabla g(x) = 0$ and otherwise $h(s,t)\neq 0$. Can we infer a differential equation that is satisfied by $h$? Or any other properties? If we write $h$ as: $$h(f,g) = \sum a_{nm} f^n g^m$$ is there a way to find values of $a_{nm}$ knowing the functions $f(x)$ and $g(x)$ ? My first attempt at trying to write a function which if it converges (which it probably doesn't) seems to have the right properties $$ \frac{1}{h(u,v)} = \int \frac{1}{\left((u-f(x))^2+(v-g(x))^2 + |\nabla f(x) \times \nabla g(x)|^2\right)^2 } dx^3 $$ Theoretically this has the right properties but I can't simplify it. e.g. whenever the top equation is satisfied the RHS becomes infinite which implies the function h is zero as required.","Given the equation: $$\nabla f(x) \times \nabla g(x) = 0$$ for two scalar fields $f$ and $g$. It follows that when this is satisfied $h(f,g)=0$ for some function $h$. The question is to find a smooth function $h$ given functions $f$ and $g$ such $h(s,t)=0$ if and only if there exists a solution to: $s=f(x), t=g(x), \nabla f(x) \times \nabla g(x) = 0$ and otherwise $h(s,t)\neq 0$. Can we infer a differential equation that is satisfied by $h$? Or any other properties? If we write $h$ as: $$h(f,g) = \sum a_{nm} f^n g^m$$ is there a way to find values of $a_{nm}$ knowing the functions $f(x)$ and $g(x)$ ? My first attempt at trying to write a function which if it converges (which it probably doesn't) seems to have the right properties $$ \frac{1}{h(u,v)} = \int \frac{1}{\left((u-f(x))^2+(v-g(x))^2 + |\nabla f(x) \times \nabla g(x)|^2\right)^2 } dx^3 $$ Theoretically this has the right properties but I can't simplify it. e.g. whenever the top equation is satisfied the RHS becomes infinite which implies the function h is zero as required.",,"['ordinary-differential-equations', 'differential-geometry', 'vector-analysis']"
73,Showing that a function is Frechet Differentiable,Showing that a function is Frechet Differentiable,,"If I have a map $K\times K\rightarrow K$, which maps $(x,y)$ to the product $xy$. How would i prove by direct caculation that the map is Frechet Differentiable without using the product rule. I've been trying for a while but haven't really got anywhere. Any help would be greatly appreciated, thanks","If I have a map $K\times K\rightarrow K$, which maps $(x,y)$ to the product $xy$. How would i prove by direct caculation that the map is Frechet Differentiable without using the product rule. I've been trying for a while but haven't really got anywhere. Any help would be greatly appreciated, thanks",,"['linear-algebra', 'ordinary-differential-equations']"
74,Prove an integral inequality ($e^{\alpha x}u(x) \le e^{\alpha y}u(y) + \int_y^x e^{\alpha \xi} f(\xi) d\xi$) under certain hypotheses,Prove an integral inequality () under certain hypotheses,e^{\alpha x}u(x) \le e^{\alpha y}u(y) + \int_y^x e^{\alpha \xi} f(\xi) d\xi,"Let $A\in \mathbb{R}$ and $A>0$, $\alpha \in \mathbb{R}$, and  $u,f \in C([0,A])$. Suppose that for every $g \in C^\infty((0,A))$ we have  $$g'(x_0) + \alpha u(x_0) \le f(x_0)$$ if $x_0$ is a local maximum of $u-g$. How can I prove that  $$e^{\alpha x}u(x) \le e^{\alpha y}u(y) + \int_y^x e^{\alpha \xi} f(\xi) d\xi$$ for any $x,y \in [0,A]?$ My failed attempts were mostly manipulations that included integrations by parts and several attempts at reasoning by contradiction.","Let $A\in \mathbb{R}$ and $A>0$, $\alpha \in \mathbb{R}$, and  $u,f \in C([0,A])$. Suppose that for every $g \in C^\infty((0,A))$ we have  $$g'(x_0) + \alpha u(x_0) \le f(x_0)$$ if $x_0$ is a local maximum of $u-g$. How can I prove that  $$e^{\alpha x}u(x) \le e^{\alpha y}u(y) + \int_y^x e^{\alpha \xi} f(\xi) d\xi$$ for any $x,y \in [0,A]?$ My failed attempts were mostly manipulations that included integrations by parts and several attempts at reasoning by contradiction.",,"['calculus', 'real-analysis']"
75,Can we ignore terms of differential equation if coefficients go to zero at a point?,Can we ignore terms of differential equation if coefficients go to zero at a point?,,"Given a differential equation with variable coefficients, if at a given point some coefficients go to zero, can I take them to be zero and assume that the answer is correct for that point? I mean, let me give a concrete example: considering the following differential equation: $$ a(x)\frac{dy}{dx}+b(x)\left(\frac{dy}{dx}\right)^{-1/2}+c(x)\left(\frac{dy}{dx}\right)^{1/2}+d(x)\left(\frac{dy}{dx}\right)^{-1}=0~.  $$ if at a given point, say $x=0$, the coefficients $c(x)$ and $d(x)$ are zero, can I solve the simplified equation: $$ a(x)\frac{dy}{dx}+b(x)\left(\frac{dy}{dx}\right)^{-1/2}=0~,  $$ and get a valid result for $y(0)$? Moreover, does the validity of the result depends of, by solving the simplified equation, we get $\frac{dy}{dx}(0)=0$, $\frac{dy}{dx}(0)=\infty$ or $\frac{dy}{dx}(0)=\text{const.}$?","Given a differential equation with variable coefficients, if at a given point some coefficients go to zero, can I take them to be zero and assume that the answer is correct for that point? I mean, let me give a concrete example: considering the following differential equation: $$ a(x)\frac{dy}{dx}+b(x)\left(\frac{dy}{dx}\right)^{-1/2}+c(x)\left(\frac{dy}{dx}\right)^{1/2}+d(x)\left(\frac{dy}{dx}\right)^{-1}=0~.  $$ if at a given point, say $x=0$, the coefficients $c(x)$ and $d(x)$ are zero, can I solve the simplified equation: $$ a(x)\frac{dy}{dx}+b(x)\left(\frac{dy}{dx}\right)^{-1/2}=0~,  $$ and get a valid result for $y(0)$? Moreover, does the validity of the result depends of, by solving the simplified equation, we get $\frac{dy}{dx}(0)=0$, $\frac{dy}{dx}(0)=\infty$ or $\frac{dy}{dx}(0)=\text{const.}$?",,['ordinary-differential-equations']
76,How to solve the non-linear second order ODE $y''=\frac{x}{y}-1$,How to solve the non-linear second order ODE,y''=\frac{x}{y}-1,"How can I find a general solution to the non-linear second order ODE $$ y'' = \frac{x}{y}-1, $$ if there is one expressible in closed form? So far I have only found the particular solution $y(x)=x$.","How can I find a general solution to the non-linear second order ODE $$ y'' = \frac{x}{y}-1, $$ if there is one expressible in closed form? So far I have only found the particular solution $y(x)=x$.",,"['calculus', 'ordinary-differential-equations']"
77,The general solution of $y''=-\sin y$,The general solution of,y''=-\sin y,"When I asked Mathematica to solve the ODE $$y''=-\sin y \tag{1} $$ I got the solutions $$y=\pm 2 \text{am}\left(\frac{1}{2} \sqrt{\left(c_1+2\right) \left(t+c_2\right){}^2}|\frac{4}{c_1+2}\right), \tag{2} $$ where $\text{am}(u|m)$ is the Jacobi Amplitude function . I wonder why there is a $\pm$ ambiguity here (I believe it's related to the square root), since the equation $(1)$ is explicit. P.S. Using the rules $c_1 \mapsto -2+4c_1^2,c_2 \mapsto c_2/c_1$ one gets the equivalent form $$y= \pm2 \text{am}\left(\sqrt{\left(t c_1+c_2\right){}^2}|\frac{1}{c_1^2}\right)$$ and if one cancels the square root with the square, noticing that am is odd in the first argument one gets an even nicer form $$y=\pm2 \text{am}\left(c_1t +c_2|\frac{1}{c_1^2}\right).$$ However, my question still remains: Why is there an ugly $\pm$ in the solution if the ODE is written explicitly in the form $y''=f(x,y,y')$? Is there a nicer form for the general solution? Thank you!","When I asked Mathematica to solve the ODE $$y''=-\sin y \tag{1} $$ I got the solutions $$y=\pm 2 \text{am}\left(\frac{1}{2} \sqrt{\left(c_1+2\right) \left(t+c_2\right){}^2}|\frac{4}{c_1+2}\right), \tag{2} $$ where $\text{am}(u|m)$ is the Jacobi Amplitude function . I wonder why there is a $\pm$ ambiguity here (I believe it's related to the square root), since the equation $(1)$ is explicit. P.S. Using the rules $c_1 \mapsto -2+4c_1^2,c_2 \mapsto c_2/c_1$ one gets the equivalent form $$y= \pm2 \text{am}\left(\sqrt{\left(t c_1+c_2\right){}^2}|\frac{1}{c_1^2}\right)$$ and if one cancels the square root with the square, noticing that am is odd in the first argument one gets an even nicer form $$y=\pm2 \text{am}\left(c_1t +c_2|\frac{1}{c_1^2}\right).$$ However, my question still remains: Why is there an ugly $\pm$ in the solution if the ODE is written explicitly in the form $y''=f(x,y,y')$? Is there a nicer form for the general solution? Thank you!",,"['ordinary-differential-equations', 'elliptic-functions']"
78,Is recursion a type of differential equations [closed],Is recursion a type of differential equations [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Unfortunately, this question has a very bland answer; however, it occurred to me that recursive functions, like ones to program and calculate factorials for instance, are very similar to differential equations whose results depend on initial conditions. Is recursion essentially just discrete DEs, versus the typical continuous DEs learned at school?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Unfortunately, this question has a very bland answer; however, it occurred to me that recursive functions, like ones to program and calculate factorials for instance, are very similar to differential equations whose results depend on initial conditions. Is recursion essentially just discrete DEs, versus the typical continuous DEs learned at school?",,"['ordinary-differential-equations', 'recursion']"
79,Saturated damped harmonic oscillator,Saturated damped harmonic oscillator,,"Consider a critically damped harmonic oscillator: $$x' = v$$ $$v' = -2v - x$$ Such a system has the property that if $x(0) < 0$ and $x(0)+v(0)\leq 0$, then $x(t) < 0$ for all $t$. Now suppose the system is modified so that the acceleration is subject to some minimum value, i.e. $v' \geq a_{min}$. Is there some way of maintaining the above property, i.e. $x(t) < 0$ for all $t$? More precisely, is it possible to do one or both of the following: Add additional constraints on the initial state ($x(0)$, $v(0))$ so that $v'(t) = -2v(t) - x(t) \geq a_{min}$ for all $t$. Add a term to the differential equations, i.e. $v' = -2v-x + g(x,v)$ so that $v'(t) = -2v-x + g(x,v) \geq a_{min}$, perhaps also under additional constraints on the initial state. In other words, I want to make a small tweak to the critically damped harmonic oscillator to take into account saturation of acceleration.","Consider a critically damped harmonic oscillator: $$x' = v$$ $$v' = -2v - x$$ Such a system has the property that if $x(0) < 0$ and $x(0)+v(0)\leq 0$, then $x(t) < 0$ for all $t$. Now suppose the system is modified so that the acceleration is subject to some minimum value, i.e. $v' \geq a_{min}$. Is there some way of maintaining the above property, i.e. $x(t) < 0$ for all $t$? More precisely, is it possible to do one or both of the following: Add additional constraints on the initial state ($x(0)$, $v(0))$ so that $v'(t) = -2v(t) - x(t) \geq a_{min}$ for all $t$. Add a term to the differential equations, i.e. $v' = -2v-x + g(x,v)$ so that $v'(t) = -2v-x + g(x,v) \geq a_{min}$, perhaps also under additional constraints on the initial state. In other words, I want to make a small tweak to the critically damped harmonic oscillator to take into account saturation of acceleration.",,"['ordinary-differential-equations', 'control-theory', 'linear-control']"
80,Solving set of Killing equations,Solving set of Killing equations,,"I'm trying to find Killing vectors of the metric, $$ds^2 = \frac{1}{x^2}(dt^2-dx^2)$$ i.e. the half-space on anti-de Sitter space. The Ricci scalar is a constant and thus motivates one to consider a full general vector, $\vec v = (v_t, v_x)$, for which the Killing equations give, $$\partial_t v_t + \frac{1}{x}v_x= 0, \quad \partial_x v_ x + \frac{1}{x}v_x = 0,$$ $$\partial_t u_x + \partial_x v_t + \frac{2}{x}v_t = 0.$$ The second is easy to solve, we have, $$v_x = \frac{f(t)}{x}$$ for arbitrary $f(t)$ and then we have, $$v_t = -\frac{1}{x^2}\int f(t) \, dt + g(x).$$ for arbitrary $g(x)$. The third equation gives the constraint, denoting $F(t) = \int f(t) \, dt$: $$F''(t) + \left( 1-\frac{2}{x^2}\right)F(t) + xg'(x) + 2g(x) = 0.$$ I'm unsure how to proceed from here, as I don't think there's a way to solve for $F$ and $g$ separately. Though clearly if $F = 0$, one solution is $g(x) = c/x^2$. Any idea how to proceed?","I'm trying to find Killing vectors of the metric, $$ds^2 = \frac{1}{x^2}(dt^2-dx^2)$$ i.e. the half-space on anti-de Sitter space. The Ricci scalar is a constant and thus motivates one to consider a full general vector, $\vec v = (v_t, v_x)$, for which the Killing equations give, $$\partial_t v_t + \frac{1}{x}v_x= 0, \quad \partial_x v_ x + \frac{1}{x}v_x = 0,$$ $$\partial_t u_x + \partial_x v_t + \frac{2}{x}v_t = 0.$$ The second is easy to solve, we have, $$v_x = \frac{f(t)}{x}$$ for arbitrary $f(t)$ and then we have, $$v_t = -\frac{1}{x^2}\int f(t) \, dt + g(x).$$ for arbitrary $g(x)$. The third equation gives the constraint, denoting $F(t) = \int f(t) \, dt$: $$F''(t) + \left( 1-\frac{2}{x^2}\right)F(t) + xg'(x) + 2g(x) = 0.$$ I'm unsure how to proceed from here, as I don't think there's a way to solve for $F$ and $g$ separately. Though clearly if $F = 0$, one solution is $g(x) = c/x^2$. Any idea how to proceed?",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations']"
81,Hopf bifurcation in a simple system,Hopf bifurcation in a simple system,,"Given the system: $$  \dot{r} = -\mu r + r^3, \\ \dot{\theta} = r $$ There is clearly one single node at $r=0$. The Jacobian is then: $$  \begin{pmatrix}         -\mu + 3r^2 & 0 \\         1 & 0         \end{pmatrix}$$ Setting $r=0$ and finding the eigenvalues I get: $\lambda = 0 , \lambda = -\mu $. The problem statement says ""show that a subcritical Hopf bifurcation occurs at the parameter value $\mu = 0$ "". I don't see how a Hopf bifurcation appears here when all my eigenvalues are all real and I am failing to interpret $\lambda = 0$","Given the system: $$  \dot{r} = -\mu r + r^3, \\ \dot{\theta} = r $$ There is clearly one single node at $r=0$. The Jacobian is then: $$  \begin{pmatrix}         -\mu + 3r^2 & 0 \\         1 & 0         \end{pmatrix}$$ Setting $r=0$ and finding the eigenvalues I get: $\lambda = 0 , \lambda = -\mu $. The problem statement says ""show that a subcritical Hopf bifurcation occurs at the parameter value $\mu = 0$ "". I don't see how a Hopf bifurcation appears here when all my eigenvalues are all real and I am failing to interpret $\lambda = 0$",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems']"
82,Solution of $f(3x)=\frac{f(x)f(4x)}{2f(2x)}+\frac{f^2(2x)}{2f(x)}$?,Solution of ?,f(3x)=\frac{f(x)f(4x)}{2f(2x)}+\frac{f^2(2x)}{2f(x)},What are the analytic solutions to $f(3x)=\dfrac{f(x)f(4x)}{2f(2x)}+\dfrac{f^2(2x)}{2f(x)}$? Are there solutions that do not satisfy a linear first or second ODE ? See also : About the addition formula $f(x+y) = f(x)g(y)+f(y)g(x)$,What are the analytic solutions to $f(3x)=\dfrac{f(x)f(4x)}{2f(2x)}+\dfrac{f^2(2x)}{2f(x)}$? Are there solutions that do not satisfy a linear first or second ODE ? See also : About the addition formula $f(x+y) = f(x)g(y)+f(y)g(x)$,,"['ordinary-differential-equations', 'arithmetic', 'functional-equations']"
83,I don't know what to call this in English! Constant Value? Lyapunov function?,I don't know what to call this in English! Constant Value? Lyapunov function?,,"first of all let me apologise. I am English but I'm studying in Germany and haven't been able to find a translation of what this is called. I have been given the dynamic system $$\dot{x}=1$$ $$\dot{y}=-y$$ and told to find an Erhaltungsgröße . This will be a little difficult to explain because I don't understand it properly (hence being here). My understanding is that I need to find a function $E(x,y)$ such that for a solution curve $\begin{pmatrix}x(t)\\y(t)\end{pmatrix}$ $E(x(t), y(t))=const$. We've been given the starting point that $E(x,y)=X(x)\cdot Y(y)$ I've attempted to solve this (and not come very far) by: $$0=\frac{d}{dt}E(x(t), y(t))=\frac{\partial E}{\partial x}\dot{x}+ \frac{\partial E}{\partial y}\dot{y}$$ $$\frac{d}{dt}E(x,y)=X´(x)\dot{x}+ Y´(y)\dot{y}=X´(x)+Y´(y)(-1)=0$$ I only came to this idea because of a similar problem we were given with the starting point $E(r,v)=R(r)+V(v)$ but I'm unsure if I can apply it here. So, what is this called in English (so that I can maybe look up some info on it)? And how do I proceed? Edit: Perhaps a little more explanation may help. Anything which is conserved is an Erhaltungsgröße - such as energy. Edit II: I'm taking Differential Equations for Engineers so we aren't going too deeply into most of these problems. The wiki page for a Lyapunov function seems to be in the right direction but there is also a lot on there I don't recognise or understand.","first of all let me apologise. I am English but I'm studying in Germany and haven't been able to find a translation of what this is called. I have been given the dynamic system $$\dot{x}=1$$ $$\dot{y}=-y$$ and told to find an Erhaltungsgröße . This will be a little difficult to explain because I don't understand it properly (hence being here). My understanding is that I need to find a function $E(x,y)$ such that for a solution curve $\begin{pmatrix}x(t)\\y(t)\end{pmatrix}$ $E(x(t), y(t))=const$. We've been given the starting point that $E(x,y)=X(x)\cdot Y(y)$ I've attempted to solve this (and not come very far) by: $$0=\frac{d}{dt}E(x(t), y(t))=\frac{\partial E}{\partial x}\dot{x}+ \frac{\partial E}{\partial y}\dot{y}$$ $$\frac{d}{dt}E(x,y)=X´(x)\dot{x}+ Y´(y)\dot{y}=X´(x)+Y´(y)(-1)=0$$ I only came to this idea because of a similar problem we were given with the starting point $E(r,v)=R(r)+V(v)$ but I'm unsure if I can apply it here. So, what is this called in English (so that I can maybe look up some info on it)? And how do I proceed? Edit: Perhaps a little more explanation may help. Anything which is conserved is an Erhaltungsgröße - such as energy. Edit II: I'm taking Differential Equations for Engineers so we aren't going too deeply into most of these problems. The wiki page for a Lyapunov function seems to be in the right direction but there is also a lot on there I don't recognise or understand.",,['ordinary-differential-equations']
84,Solving this diffusion equation,Solving this diffusion equation,,"I have the following diffusion equation  \begin{equation} \frac{\partial{P}}{\partial N}=\frac{1}{2}\frac{\partial^2P}{\partial x^2}-a\frac{\partial P}{\partial x}\,, \quad\mbox{where}\ a > 0\ \mbox{is some real parameter.} \end{equation} I am given with two boundary conditions: $P\left(x = 1, N\right) = 0$ $P\left(x, N = 0\right) = \delta\left(x\right)$, where $\delta\left(x\right)$ is the Dirac delta function. Does any one know how to proceed to solve for $P\left(x,N\right)$ ?. The boundary condition is making the whole problem difficult.","I have the following diffusion equation  \begin{equation} \frac{\partial{P}}{\partial N}=\frac{1}{2}\frac{\partial^2P}{\partial x^2}-a\frac{\partial P}{\partial x}\,, \quad\mbox{where}\ a > 0\ \mbox{is some real parameter.} \end{equation} I am given with two boundary conditions: $P\left(x = 1, N\right) = 0$ $P\left(x, N = 0\right) = \delta\left(x\right)$, where $\delta\left(x\right)$ is the Dirac delta function. Does any one know how to proceed to solve for $P\left(x,N\right)$ ?. The boundary condition is making the whole problem difficult.",,"['calculus', 'ordinary-differential-equations', 'probability-distributions', 'partial-differential-equations']"
85,How to find the Fourier series for limacons?,How to find the Fourier series for limacons?,,"In paper Shadows of rotating black holes approximated by Durer-Pascal limacons , the author tries to find an approximation to the equations $$ x=\frac{r ∆+r Q^2 −M(r^2 −a^2)}{a(r −M)\sin θ} $$  and  $$ y^2=\frac{4r^2∆}{(r −M)^2}−(x+a\sinθ)^2 $$  where $∆ = r ^2 − 2Mr + a^ 2 + Q^ 2$ Note: $r$ does not represent the radius here, i.e $r\neq\sqrt{x^{2}+y^{2}}$. The author shows that the above equations can be approximated using limacon equation  $$ (x ^2 +y^ 2 −Ax)^ 2 =B ^2 (x ^2 +y^ 2 ) $$ for $Q=0$ and $\theta=\frac{\pi}{2}$. Then it was stated (without proof) that for different values of $\theta$ the ""paramters A and B are given by the following simple Fourier series"" $$ A_a(θ) = A_a \sin θ +0.2 \,a \sin^ 3 θ \cos ^2 θ $$ $$ B_a(θ) = B_a +0.23M\left( 1− \sqrt{ 1-\frac{ a^4}{ M^4}}\right) \cos^4 θ $$ My question is how did the author use the fourier series to arrive at the above equations for the paramters A and B?","In paper Shadows of rotating black holes approximated by Durer-Pascal limacons , the author tries to find an approximation to the equations $$ x=\frac{r ∆+r Q^2 −M(r^2 −a^2)}{a(r −M)\sin θ} $$  and  $$ y^2=\frac{4r^2∆}{(r −M)^2}−(x+a\sinθ)^2 $$  where $∆ = r ^2 − 2Mr + a^ 2 + Q^ 2$ Note: $r$ does not represent the radius here, i.e $r\neq\sqrt{x^{2}+y^{2}}$. The author shows that the above equations can be approximated using limacon equation  $$ (x ^2 +y^ 2 −Ax)^ 2 =B ^2 (x ^2 +y^ 2 ) $$ for $Q=0$ and $\theta=\frac{\pi}{2}$. Then it was stated (without proof) that for different values of $\theta$ the ""paramters A and B are given by the following simple Fourier series"" $$ A_a(θ) = A_a \sin θ +0.2 \,a \sin^ 3 θ \cos ^2 θ $$ $$ B_a(θ) = B_a +0.23M\left( 1− \sqrt{ 1-\frac{ a^4}{ M^4}}\right) \cos^4 θ $$ My question is how did the author use the fourier series to arrive at the above equations for the paramters A and B?",,"['geometry', 'ordinary-differential-equations', 'fourier-series', 'approximation']"
86,Solutions of $y''+y'+by=\cos x$ that satisfy $\lim\limits_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$,Solutions of  that satisfy,y''+y'+by=\cos x \lim\limits_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0,"Find for which values of $b$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ My attempt : Since the difference of every two solutions of the original equation is a solution of the homogeneous equation $y''+y'+by=0$ we are interested only in the solutions of the latter. The characteristic roots are: $$m_{1,2}=\frac{-1\pm\sqrt{1-4b}}{2}$$ If $b=1/4$ then $m_{1,2}=-1/2$ so the general homogeneous solution is $y_h(x)=(C_1+C_2x)e^{-x/2}$ and $$\lim_{x\to\infty}\frac{(C_1+C_2x)e^{-x/2}}{e^x}=\lim_{x\to\infty}\frac{C_1+C_2x}{e^{3x/2}}=0$$ If $0 \leq b < \frac{1}{4}$ then $0<\sqrt{1-4b}\leq 1$ and so we have two distinct nonpositive real roots (at least one is strictly negative). Therefore the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+\frac{C_2}{e^{|m_2|x}}$$ So $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+\frac{C_2}{e^{(|m_2|+1)x}} \right )=0$$ If $b<0$ then $\sqrt{1-4b}>1$ we have two distinct real roots (with opposite signs). If $m_1<0$ and $m_2>0$ then the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+C_2e^{m_2 x}$$ Therefore $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+C_2e^{(m_2-1) x} \right )=0 \iff m_2<1 \iff b>-2$$ If $b>\frac{1}{4}$ then there are two complex roots of the form $-\frac{1}{2}\pm i\beta$ so the general homogeneous solution is $$y_h(x)=e^{-x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x))$$ Hence $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( e^{-3x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x)) \right )=0$$ In conclusion for all $b>-2$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ Is it correct? If yes, is there an easier way to solve this problem?","Find for which values of $b$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ My attempt : Since the difference of every two solutions of the original equation is a solution of the homogeneous equation $y''+y'+by=0$ we are interested only in the solutions of the latter. The characteristic roots are: $$m_{1,2}=\frac{-1\pm\sqrt{1-4b}}{2}$$ If $b=1/4$ then $m_{1,2}=-1/2$ so the general homogeneous solution is $y_h(x)=(C_1+C_2x)e^{-x/2}$ and $$\lim_{x\to\infty}\frac{(C_1+C_2x)e^{-x/2}}{e^x}=\lim_{x\to\infty}\frac{C_1+C_2x}{e^{3x/2}}=0$$ If $0 \leq b < \frac{1}{4}$ then $0<\sqrt{1-4b}\leq 1$ and so we have two distinct nonpositive real roots (at least one is strictly negative). Therefore the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+\frac{C_2}{e^{|m_2|x}}$$ So $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+\frac{C_2}{e^{(|m_2|+1)x}} \right )=0$$ If $b<0$ then $\sqrt{1-4b}>1$ we have two distinct real roots (with opposite signs). If $m_1<0$ and $m_2>0$ then the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+C_2e^{m_2 x}$$ Therefore $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+C_2e^{(m_2-1) x} \right )=0 \iff m_2<1 \iff b>-2$$ If $b>\frac{1}{4}$ then there are two complex roots of the form $-\frac{1}{2}\pm i\beta$ so the general homogeneous solution is $$y_h(x)=e^{-x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x))$$ Hence $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( e^{-3x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x)) \right )=0$$ In conclusion for all $b>-2$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ Is it correct? If yes, is there an easier way to solve this problem?",,"['ordinary-differential-equations', 'proof-verification']"
87,Separable Differential Equation -- Last step,Separable Differential Equation -- Last step,,"I have separable ODE that is: $$\frac{dy}{dt}=\frac{5y(100-y)}{100}$$ I get stuck towards the end and maybe my answer is even correct,but can't see that it looks like Wolfram's answer (which I know is sometimes strange), but wanted to ask a few questions. I separate this into: $$dy \frac{100}{y (100-y)}=5dt$$ instead of solving the left-hand side by partial fractions, I re-write it as $$dy \frac{100}{y^2\left(\frac{100}{y} -1\right)}$$ so that I can use the u substitution of $$u=\frac{100}{y}-1$$ $$du = -100y^{-2} dy$$ so my left hand integral becomes $$- \int\frac1u du$$ thus my $2$ integrals are: $$-\int \frac1u du  = \int 5 dt$$ Solving these integrals $$-\ln\left|\frac{100}{y} -1\right| = 5t +C.$$ Multiply by $-1$: $$\ln \left|\frac{100}{y} -1\right| = -5t +C.$$ Here is where I'm not sure of best next step (assuming my math is correct so far) Should I put the $\ln$ stuff into: $$\ln\left|\frac{100-y}{y}\right|?$$ Then I would have $$\ln (100-y) - \ln y = -5t +C$$","I have separable ODE that is: $$\frac{dy}{dt}=\frac{5y(100-y)}{100}$$ I get stuck towards the end and maybe my answer is even correct,but can't see that it looks like Wolfram's answer (which I know is sometimes strange), but wanted to ask a few questions. I separate this into: $$dy \frac{100}{y (100-y)}=5dt$$ instead of solving the left-hand side by partial fractions, I re-write it as $$dy \frac{100}{y^2\left(\frac{100}{y} -1\right)}$$ so that I can use the u substitution of $$u=\frac{100}{y}-1$$ $$du = -100y^{-2} dy$$ so my left hand integral becomes $$- \int\frac1u du$$ thus my $2$ integrals are: $$-\int \frac1u du  = \int 5 dt$$ Solving these integrals $$-\ln\left|\frac{100}{y} -1\right| = 5t +C.$$ Multiply by $-1$: $$\ln \left|\frac{100}{y} -1\right| = -5t +C.$$ Here is where I'm not sure of best next step (assuming my math is correct so far) Should I put the $\ln$ stuff into: $$\ln\left|\frac{100-y}{y}\right|?$$ Then I would have $$\ln (100-y) - \ln y = -5t +C$$",,['ordinary-differential-equations']
88,Clarification on the Wronskian,Clarification on the Wronskian,,"I am getting a few ""contradicting"" conclusions from the Wronskian and I just wanted to clarify, but assume $y_1, y_2$ are two solutions to a second order differential equation that is homogeneous. According to my textbook: If the Wronskian, $W\neq 0$ at some point $t_0$ on the interval $I$ , then $y_1, y_2$ are linearly independent and form the fundamental set of solutions But the solution to the question ""Show that if $y_1, y_2$ are solutions for some second order differential equation, but they have a maxima/minima at the same point in $I$ , then they cannot form the fundamental set of solutions"" seems to contradict this. Their explanation is that: Pick $t_0$ , then the $W(y_1, y_2)(t_0) = 0$ at $t_0$ , so they cannot be linearly independent, so they don't form the set of fundamental solutions But this doesn't prove that $W=0$ across the entire interval $I$ , so I'm confused. Could someone please enlighten me?","I am getting a few ""contradicting"" conclusions from the Wronskian and I just wanted to clarify, but assume are two solutions to a second order differential equation that is homogeneous. According to my textbook: If the Wronskian, at some point on the interval , then are linearly independent and form the fundamental set of solutions But the solution to the question ""Show that if are solutions for some second order differential equation, but they have a maxima/minima at the same point in , then they cannot form the fundamental set of solutions"" seems to contradict this. Their explanation is that: Pick , then the at , so they cannot be linearly independent, so they don't form the set of fundamental solutions But this doesn't prove that across the entire interval , so I'm confused. Could someone please enlighten me?","y_1, y_2 W\neq 0 t_0 I y_1, y_2 y_1, y_2 I t_0 W(y_1, y_2)(t_0) = 0 t_0 W=0 I","['ordinary-differential-equations', 'wronskian']"
89,Can we model finite propagation speeds through parabolic PDEs?,Can we model finite propagation speeds through parabolic PDEs?,,"In physics, phenomena involving a finite propagation speed are typically expressed through hyperbolic (second-order) partial differential equations, such as, for instance, the well-known wave equation $$f_{tt}=c^2\nabla^2f$$ where $c$ is the wave propagation speed. What I'm interested to know is, could we find a parabolic PDE that models this kind of finite velocity limits, or are we automatically being thrown back to second-order equations? Is there a general mathematical theorem at play here about the solutions of parabolic vs hyperbolic PDEs? (This question is a follow-up to this one , which I posted some months ago in Physics SE, and I consider wasn't fully answered by that time).","In physics, phenomena involving a finite propagation speed are typically expressed through hyperbolic (second-order) partial differential equations, such as, for instance, the well-known wave equation $$f_{tt}=c^2\nabla^2f$$ where $c$ is the wave propagation speed. What I'm interested to know is, could we find a parabolic PDE that models this kind of finite velocity limits, or are we automatically being thrown back to second-order equations? Is there a general mathematical theorem at play here about the solutions of parabolic vs hyperbolic PDEs? (This question is a follow-up to this one , which I posted some months ago in Physics SE, and I consider wasn't fully answered by that time).",,"['ordinary-differential-equations', 'partial-differential-equations', 'physics']"
90,Solving an implicit differential equation?,Solving an implicit differential equation?,,"Consider solving the differential equation $y' = f(t,y)$ at time points $t_i$ with the respective solutions $y_i$. The following multi-step method is based on the integral relation: $y(t_{i+1}) = y(t_{i-1}) + \int^{t_{i+1}}_{t_{i-1}}f(t,y)dt$ Derive an implicit multi-step method by approximating the integrand $f(t,y)$ with the polynomial interpolant using function values at $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}$. OK, so I know this multi-step method is called the Nystrom method. I also know that the method I come up with needs to have the following form: $y_{i+1} = y_{i-1} + h(\alpha f_{i-2} + \beta f_{i-1} + \gamma f_{i} + \phi f_{i+1})$ Here $\alpha, \beta, \gamma$, and $\phi$ are constants I have to find. $h$ is just the timestep interval size, and $f_p = f(t_p,y_p)$ for some arbitrary $p$. Beyond this, I'm completely stuck, and don't know how to even get started. Through only a few examples I found online, it seems what sometimes works is to use Taylor series to expand about the point $t_{i-1}$ and then somehow construct a system of equations for the constants to solve for them. Unfortunately the examples I've found haven't been really illustrative though. So when I tried this I got nowhere helpful pretty fast. I'd really appreciate any help I can get with this.","Consider solving the differential equation $y' = f(t,y)$ at time points $t_i$ with the respective solutions $y_i$. The following multi-step method is based on the integral relation: $y(t_{i+1}) = y(t_{i-1}) + \int^{t_{i+1}}_{t_{i-1}}f(t,y)dt$ Derive an implicit multi-step method by approximating the integrand $f(t,y)$ with the polynomial interpolant using function values at $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}$. OK, so I know this multi-step method is called the Nystrom method. I also know that the method I come up with needs to have the following form: $y_{i+1} = y_{i-1} + h(\alpha f_{i-2} + \beta f_{i-1} + \gamma f_{i} + \phi f_{i+1})$ Here $\alpha, \beta, \gamma$, and $\phi$ are constants I have to find. $h$ is just the timestep interval size, and $f_p = f(t_p,y_p)$ for some arbitrary $p$. Beyond this, I'm completely stuck, and don't know how to even get started. Through only a few examples I found online, it seems what sometimes works is to use Taylor series to expand about the point $t_{i-1}$ and then somehow construct a system of equations for the constants to solve for them. Unfortunately the examples I've found haven't been really illustrative though. So when I tried this I got nowhere helpful pretty fast. I'd really appreciate any help I can get with this.",,"['ordinary-differential-equations', 'numerical-methods', 'taylor-expansion', 'computational-mathematics', 'implicit-differentiation']"
91,First order ODE with Dirac delta funtcion,First order ODE with Dirac delta funtcion,,"I am looking for a direct method to solve this first order ODE with Dirac delta funtcion $$\frac{dU(t)}{dt}+k^2U(t)=\frac{1}{\sqrt{2\pi}}\delta(t)$$ with the initial condition $U(0)=\frac{1}{\sqrt{2\pi}}$. The solution to this problem is $$U(t)=\frac{1}{\sqrt{2\pi}}e^{-k^2t}$$ My try The integrating factor for this ode is $$I=e^{\int k^2 dt }=e^{ k^2 t  }$$ then multiplying both sides of the differential equation by  $\,\,e^{ k^2 t  }$, we get $$\frac{d }{d t}\left(e^{ k^2 t  } U(t)\right)=\frac{1}{\sqrt{2\pi}}e^{ k^2 t  } \delta(t)$$ Integrating both sides, we have $$e^{ k^2 t  } U(t)=\frac{1}{\sqrt{2\pi}}\int{e^{ k^2 t  } \delta(t)}dt+C$$ From here on, I am lost. Any suggestions? Back ground of this problem The above ode we got after applying Fourier transform to the following PDE $$u_{t}=u_{xx}+\delta{(x)}\delta{(t)}$$ with $u(x,0)=\delta(x)$.","I am looking for a direct method to solve this first order ODE with Dirac delta funtcion $$\frac{dU(t)}{dt}+k^2U(t)=\frac{1}{\sqrt{2\pi}}\delta(t)$$ with the initial condition $U(0)=\frac{1}{\sqrt{2\pi}}$. The solution to this problem is $$U(t)=\frac{1}{\sqrt{2\pi}}e^{-k^2t}$$ My try The integrating factor for this ode is $$I=e^{\int k^2 dt }=e^{ k^2 t  }$$ then multiplying both sides of the differential equation by  $\,\,e^{ k^2 t  }$, we get $$\frac{d }{d t}\left(e^{ k^2 t  } U(t)\right)=\frac{1}{\sqrt{2\pi}}e^{ k^2 t  } \delta(t)$$ Integrating both sides, we have $$e^{ k^2 t  } U(t)=\frac{1}{\sqrt{2\pi}}\int{e^{ k^2 t  } \delta(t)}dt+C$$ From here on, I am lost. Any suggestions? Back ground of this problem The above ode we got after applying Fourier transform to the following PDE $$u_{t}=u_{xx}+\delta{(x)}\delta{(t)}$$ with $u(x,0)=\delta(x)$.",,"['ordinary-differential-equations', 'dirac-delta']"
92,Where can I find data on which to base mathematical modeling with differential equaitons?,Where can I find data on which to base mathematical modeling with differential equaitons?,,"Where can I find data and models on which to base mathematical modeling with differential equations? We are building a community of colleagues interested in using modeling to motivate teaching differential equations and we seek good sources of data and models to offer colleagues. Any leads, examples, pointers, engagements, etc. would be appreciated. Brian Winkel, Director SIMIODE at www.simiode.org.  Thank you.","Where can I find data and models on which to base mathematical modeling with differential equations? We are building a community of colleagues interested in using modeling to motivate teaching differential equations and we seek good sources of data and models to offer colleagues. Any leads, examples, pointers, engagements, etc. would be appreciated. Brian Winkel, Director SIMIODE at www.simiode.org.  Thank you.",,['ordinary-differential-equations']
93,"I.V.P. $y'=max\{1,y\}, y(0)=1$",I.V.P.,"y'=max\{1,y\}, y(0)=1","Prove that I.V.P. $y'=max\{1,y\}, y(0)=1$ have a unique solution then find this solution. I know I should to prove i) f(x,y) continues on R. ii) f(x,y) bounded: There exist M constant such that: $|f(x,y)|\leq M$ iii) f(x,y) satisfy lipshtize condition $|f(x,y_2)-f(x,y_1)|\leq L|y_2-y_1|$ L is lipshtize constant. But i don't know how can I find $f(x,y)$ here I say: $f(x,y)=1, \ if  \ y\leq 1$ Or $f(x,y)=y, \ if \ y>1$ But my teacher say no it's wrong answer. I know how can i prove the three above conditions but i only want $f(x,y)$ Thanks.","Prove that I.V.P. $y'=max\{1,y\}, y(0)=1$ have a unique solution then find this solution. I know I should to prove i) f(x,y) continues on R. ii) f(x,y) bounded: There exist M constant such that: $|f(x,y)|\leq M$ iii) f(x,y) satisfy lipshtize condition $|f(x,y_2)-f(x,y_1)|\leq L|y_2-y_1|$ L is lipshtize constant. But i don't know how can I find $f(x,y)$ here I say: $f(x,y)=1, \ if  \ y\leq 1$ Or $f(x,y)=y, \ if \ y>1$ But my teacher say no it's wrong answer. I know how can i prove the three above conditions but i only want $f(x,y)$ Thanks.",,"['ordinary-differential-equations', 'initial-value-problems']"
94,Deriving the integrating factor for exact equations.,Deriving the integrating factor for exact equations.,,"Show that if $\frac{N_x - M_y}{M} = Q$, where Q  is a function of $y$ only, the the differential equation $M + Ny' = 0$      (*) has an integrating factor of the form $u(y) = exp$$\int Q(y) \, dy$. (The subscripts in the first equation are partial derivatives). I've done quite a few examples and see that it indeed works, but I'm not sure how to prove this general formula. (So we need to show that multiplying through by $u(y)$ makes (*) exact.)","Show that if $\frac{N_x - M_y}{M} = Q$, where Q  is a function of $y$ only, the the differential equation $M + Ny' = 0$      (*) has an integrating factor of the form $u(y) = exp$$\int Q(y) \, dy$. (The subscripts in the first equation are partial derivatives). I've done quite a few examples and see that it indeed works, but I'm not sure how to prove this general formula. (So we need to show that multiplying through by $u(y)$ makes (*) exact.)",,['ordinary-differential-equations']
95,How can I write this differential equation system in MATLAB?,How can I write this differential equation system in MATLAB?,,"I am newbie MATLAB user. How to write this equation system in MATLAB? $$\frac{dN_{ik}}{dt}=\sum_{j=i}^{n}\sum_{l=1}^{n}\beta_{jl}b_{ikjl}S_{jl}N_{jl} - S_{ik}N_{ik}$$ Here, I want to know if I take $n=3$ or any constant number what kind of equation system will I have? How can i run this loop without entering $\beta$, $b$ and $S$ functions. I just want to see matrix form of this system. Sorry for my bad English,  thanks for help.","I am newbie MATLAB user. How to write this equation system in MATLAB? $$\frac{dN_{ik}}{dt}=\sum_{j=i}^{n}\sum_{l=1}^{n}\beta_{jl}b_{ikjl}S_{jl}N_{jl} - S_{ik}N_{ik}$$ Here, I want to know if I take $n=3$ or any constant number what kind of equation system will I have? How can i run this loop without entering $\beta$, $b$ and $S$ functions. I just want to see matrix form of this system. Sorry for my bad English,  thanks for help.",,"['ordinary-differential-equations', 'matlab']"
96,Continuous solution of ODE,Continuous solution of ODE,,"The question is to find a continuous solution to the ODE: $$(1+x^{2})y' +2xy = f(x)$$ with the intial condition $y(0)=0$, and where $$f(x)= \begin{cases} x & 0 \leq x<1 \\ -x & x \geq 1. \end{cases} $$ I split up this question into cases. For case 1, $f(x)=x$ and case 2 $f(x)=-x$. I then solved the ODE for each separate case. Case 1 I solved using integrating factors to get: $$y_1=\frac{x^2}{2(1+x^2)}.$$ Case 2 I used the same method to get: $$y_2=\frac{-x^2}{2(1+x^2)}.$$ For both the cases my c-value calculated from the initial condition was 0. However when I look at the graph of these two functions for the interval $0$ to infinity, they are discontinuous. I don't think I did the ODE wrong as I double checked. Whats up here? If I use the condition of $y_1(1)=y_2(1)$ Then I will get $y_1(1)=1/4$. However just looking at the equation of y2, it doesn't seem like its possible to get a positive number.","The question is to find a continuous solution to the ODE: $$(1+x^{2})y' +2xy = f(x)$$ with the intial condition $y(0)=0$, and where $$f(x)= \begin{cases} x & 0 \leq x<1 \\ -x & x \geq 1. \end{cases} $$ I split up this question into cases. For case 1, $f(x)=x$ and case 2 $f(x)=-x$. I then solved the ODE for each separate case. Case 1 I solved using integrating factors to get: $$y_1=\frac{x^2}{2(1+x^2)}.$$ Case 2 I used the same method to get: $$y_2=\frac{-x^2}{2(1+x^2)}.$$ For both the cases my c-value calculated from the initial condition was 0. However when I look at the graph of these two functions for the interval $0$ to infinity, they are discontinuous. I don't think I did the ODE wrong as I double checked. Whats up here? If I use the condition of $y_1(1)=y_2(1)$ Then I will get $y_1(1)=1/4$. However just looking at the equation of y2, it doesn't seem like its possible to get a positive number.",,['ordinary-differential-equations']
97,Is there an exact solution to the differential equation: $\frac{d^2x}{dt^2} = -a\sinh(x)$?,Is there an exact solution to the differential equation: ?,\frac{d^2x}{dt^2} = -a\sinh(x),"I was wondering if the equation $\frac{d^2x}{dt^2} = -a\sinh(x)$ had an exact solution because the differential equation $\frac{d^2x}{dt^2} = -a\sin(x)$ has an exact solution by way of the elliptical integral. This leads me to believe that, since $\sinh(x) = -i\sin(ix)$, $\frac{d^2x}{dt^2} = -a\sinh(x)$ should have a similar solution. Is this so?","I was wondering if the equation $\frac{d^2x}{dt^2} = -a\sinh(x)$ had an exact solution because the differential equation $\frac{d^2x}{dt^2} = -a\sin(x)$ has an exact solution by way of the elliptical integral. This leads me to believe that, since $\sinh(x) = -i\sin(ix)$, $\frac{d^2x}{dt^2} = -a\sinh(x)$ should have a similar solution. Is this so?",,"['ordinary-differential-equations', 'elliptic-integrals']"
98,Where are the other solutions to this ODE disappearing in this analysis?,Where are the other solutions to this ODE disappearing in this analysis?,,"I am asked to solve the following ODE involving constants $\alpha, L, V_0 > 0$ and $E < 0$: $$-\psi'' - \alpha V_0\psi\cdot [\delta(x)+\delta(x-L)] = \alpha E\psi.$$ In particular, we want solutions $\psi:\mathbb{R}\to \mathbb{C}$ that are: Continuous $L^2$ We are given that the solutions look something like this (shifted): Now pretend we didn't know this. Here is my attempt at arriving at these functions: Away from $0$ and $L$, the ODE is just the wave equation, whose solutions are always of the form $a_1e^{-i\sqrt{\alpha E}x}+a_2e^{i\sqrt{\alpha E}x}.$ So we know $\psi$ is also of this form piece-wise:   $$\psi(x)=\left\lbrace \begin{array}{ll}     A_1e^{\sqrt{|\alpha E|}x}+A_2e^{-\sqrt{|\alpha E|}x} & x<0 \\     B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\     C_1e^{\sqrt{|\alpha E|}x}+C_2e^{-\sqrt{|\alpha E|}x} & x\geq  L \\ \end{array} \right.$$   (I expanded a little here, since $E<0\Rightarrow \sqrt{\alpha E}=i\sqrt{|\alpha E|}$) For $\psi$ to be integrable, we also need the exponentials with infinite > support be integrable. Then $A_2=C_1=0.$ Then we can simplify it:   $$\psi(x)=\left\lbrace \begin{array}{ll}     A_1e^{\sqrt{|\alpha E|}x} & x<0 \\     B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\     C_2e^{-\sqrt{|\alpha E|}x}& x\geq L \\ \end{array} \right.$$   All that's left to do is to find the space of coefficients $A_i,B_i,C_i$ so that it is continuous and satisfies the DE. For $\psi$ to be continuous, $\lim_{x\to 0^+}\psi(x)=\lim_{x\to 0^-}\psi(x)$ and likewise at $L$. So $A_1=B_1+B_2,$ and a similar equation at $L.$ To satisfty the ODE at $0$ and $L$, because of the deltas, we know that the derivative must have a discintinuity of size $-\alpha V_0 \psi$ there. In particular: $(\lim_{x\to 0^+}\psi'(x))-(\lim_{x\to 0^-}\psi'(x))=-\alpha V_0 \psi(0),$ so computing these terms, $(\sqrt{|\alpha E|}B_1 - \sqrt{|\alpha E|}B_2)- (\sqrt{|\alpha E|}A_1) = -\alpha V_0A_1$. We have a similar equation at $L$. Now we have 4 equations relating all the coefficients to each other. I solved this system and plotted the solutions. The problem is, varying values of $\alpha, L, V_0, E$, I can only get solutions that look like the 'even' graph. Where did the odd solutions disappear in this analysis and how do you get them back?","I am asked to solve the following ODE involving constants $\alpha, L, V_0 > 0$ and $E < 0$: $$-\psi'' - \alpha V_0\psi\cdot [\delta(x)+\delta(x-L)] = \alpha E\psi.$$ In particular, we want solutions $\psi:\mathbb{R}\to \mathbb{C}$ that are: Continuous $L^2$ We are given that the solutions look something like this (shifted): Now pretend we didn't know this. Here is my attempt at arriving at these functions: Away from $0$ and $L$, the ODE is just the wave equation, whose solutions are always of the form $a_1e^{-i\sqrt{\alpha E}x}+a_2e^{i\sqrt{\alpha E}x}.$ So we know $\psi$ is also of this form piece-wise:   $$\psi(x)=\left\lbrace \begin{array}{ll}     A_1e^{\sqrt{|\alpha E|}x}+A_2e^{-\sqrt{|\alpha E|}x} & x<0 \\     B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\     C_1e^{\sqrt{|\alpha E|}x}+C_2e^{-\sqrt{|\alpha E|}x} & x\geq  L \\ \end{array} \right.$$   (I expanded a little here, since $E<0\Rightarrow \sqrt{\alpha E}=i\sqrt{|\alpha E|}$) For $\psi$ to be integrable, we also need the exponentials with infinite > support be integrable. Then $A_2=C_1=0.$ Then we can simplify it:   $$\psi(x)=\left\lbrace \begin{array}{ll}     A_1e^{\sqrt{|\alpha E|}x} & x<0 \\     B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\     C_2e^{-\sqrt{|\alpha E|}x}& x\geq L \\ \end{array} \right.$$   All that's left to do is to find the space of coefficients $A_i,B_i,C_i$ so that it is continuous and satisfies the DE. For $\psi$ to be continuous, $\lim_{x\to 0^+}\psi(x)=\lim_{x\to 0^-}\psi(x)$ and likewise at $L$. So $A_1=B_1+B_2,$ and a similar equation at $L.$ To satisfty the ODE at $0$ and $L$, because of the deltas, we know that the derivative must have a discintinuity of size $-\alpha V_0 \psi$ there. In particular: $(\lim_{x\to 0^+}\psi'(x))-(\lim_{x\to 0^-}\psi'(x))=-\alpha V_0 \psi(0),$ so computing these terms, $(\sqrt{|\alpha E|}B_1 - \sqrt{|\alpha E|}B_2)- (\sqrt{|\alpha E|}A_1) = -\alpha V_0A_1$. We have a similar equation at $L$. Now we have 4 equations relating all the coefficients to each other. I solved this system and plotted the solutions. The problem is, varying values of $\alpha, L, V_0, E$, I can only get solutions that look like the 'even' graph. Where did the odd solutions disappear in this analysis and how do you get them back?",,"['linear-algebra', 'ordinary-differential-equations', 'physics', 'quantum-mechanics', 'eigenfunctions']"
99,First order differential equation concept issue,First order differential equation concept issue,,"Find differential equation  $y^{\prime} = f(t, y)$ satisfied by $y(t) = 4 \, e^{2 t} + 3$. Solution : Compute derivative of $y$, $y^{\prime} = 8 \, e^{2 t}$ Write right hand side above, in terms of the original function $y$, that is, $y = 4 \, e^{2 t} +3$ ----> $y - 3 = 4 \, e^{2 t}$ ----> $2(y-3)= 8 \, e^{2 t}$ Get a differential equation satisfied by $y$, namely $y^{\prime} = 2y - 6$ So my issue with that last answer. How is this a solution? Does it mean that if you somehow take an integral of $2y - 6$ you should end up with the original $y(t) = 4 \, e^{2 t} + 3$ ??? It seems that there two different derivatives of $y(t)$ one is: $y^{\prime} = 8 \, e^{2 t}$ the other is: $y^{\prime} = 2y - 6$ and I don't get it, can someone explain? Also a bit offtopic, but the way $y^{\prime} = f(t, y)$ is written kinda bugs me. Shouldn't it be written like $y^{\prime} = f(t, y(t))$ to show that the function $f$ contains t as an independent variable and the function y(t) which contains variable t as an input to itself (dependent variable t) ??? That's kinda an essential information, so surprised it's omitted in the writings.","Find differential equation  $y^{\prime} = f(t, y)$ satisfied by $y(t) = 4 \, e^{2 t} + 3$. Solution : Compute derivative of $y$, $y^{\prime} = 8 \, e^{2 t}$ Write right hand side above, in terms of the original function $y$, that is, $y = 4 \, e^{2 t} +3$ ----> $y - 3 = 4 \, e^{2 t}$ ----> $2(y-3)= 8 \, e^{2 t}$ Get a differential equation satisfied by $y$, namely $y^{\prime} = 2y - 6$ So my issue with that last answer. How is this a solution? Does it mean that if you somehow take an integral of $2y - 6$ you should end up with the original $y(t) = 4 \, e^{2 t} + 3$ ??? It seems that there two different derivatives of $y(t)$ one is: $y^{\prime} = 8 \, e^{2 t}$ the other is: $y^{\prime} = 2y - 6$ and I don't get it, can someone explain? Also a bit offtopic, but the way $y^{\prime} = f(t, y)$ is written kinda bugs me. Shouldn't it be written like $y^{\prime} = f(t, y(t))$ to show that the function $f$ contains t as an independent variable and the function y(t) which contains variable t as an input to itself (dependent variable t) ??? That's kinda an essential information, so surprised it's omitted in the writings.",,"['calculus', 'ordinary-differential-equations']"
