,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Prove that if each row of a matrix sums to zero, then it has no inverse.","Prove that if each row of a matrix sums to zero, then it has no inverse.",,"Could anyone help me with this proof without using determinant? I tried two ways. Let $A$ be a matrix. If $A$ has the property that each row sums to zero, then there does not exist any matrix $X$ such that $AX=I$, where $I$ denotes the identity matrix. I then get stuck. The other way was to prove by contradiction, and I failed too.","Could anyone help me with this proof without using determinant? I tried two ways. Let $A$ be a matrix. If $A$ has the property that each row sums to zero, then there does not exist any matrix $X$ such that $AX=I$, where $I$ denotes the identity matrix. I then get stuck. The other way was to prove by contradiction, and I failed too.",,"['linear-algebra', 'matrices', 'inverse']"
1,"Proving $(\bf x\times y\cdot N)\ z+(y\times z\cdot N)\ x+(z\times x \cdot N)\ y= 0$ when $\bf x,y,z$ are coplanar and $\bf N$ is a unit normal vector",Proving  when  are coplanar and  is a unit normal vector,"(\bf x\times y\cdot N)\ z+(y\times z\cdot N)\ x+(z\times x \cdot N)\ y= 0 \bf x,y,z \bf N","Prove that if $\mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{R}^3$ are coplanar vectors and $\mathbf{N}$ is a unit normal vector to the plane then $$(\mathbf{x}\times\mathbf{y} \cdot \mathbf{N})\ \mathbf{z} + (\mathbf{y}\times\mathbf{z} \cdot \mathbf{N})\ \mathbf{x} + (\mathbf{z}\times\mathbf{x} \cdot \mathbf{N})\ \mathbf{y}=\mathbf{0}.$$ This is an elementary identity involving cross products which is used in the proof of the Gauss-Bonnet Theorem and whose proof was left as an exercise. I've tried it unsuccessfully. Initially I tried writing $\mathbf{N}=\frac{\mathbf{x}\times\mathbf{y}}{\| \mathbf{x}\times\mathbf{y}\|}=\frac{\mathbf{y}\times\mathbf{z}}{\| \mathbf{y}\times\mathbf{z}\|}=\frac{\mathbf{z}\times\mathbf{x}}{\| \mathbf{z}\times\mathbf{x}\|}$ and substituting into the equation to get $\| \mathbf{x}\times\mathbf{y}\|z +\| \mathbf{y}\times\mathbf{z}\|\mathbf{x}+\| \mathbf{z}\times\mathbf{x}\|\mathbf{y}=\mathbf{0}$ but then I realised these terms are only correct up to $\pm$ signs. You could write the norms in terms of sines of angles and divide by norms to get unit vectors with coefficients $\sin\theta,\sin\psi,\sin(\theta+\psi)$ (or $2\pi -(\theta+\psi)$ I suppose) but I don't know what to do from there, especially when the terms are only correct up to sign. Any hints how to prove this identity? Perhaps there is a clever trick to it but I can't see it. Edit: Maybe writing $\mathbf{z}=\lambda\mathbf{x}+\mu\mathbf{y}$ will help.","Prove that if are coplanar vectors and is a unit normal vector to the plane then This is an elementary identity involving cross products which is used in the proof of the Gauss-Bonnet Theorem and whose proof was left as an exercise. I've tried it unsuccessfully. Initially I tried writing and substituting into the equation to get but then I realised these terms are only correct up to signs. You could write the norms in terms of sines of angles and divide by norms to get unit vectors with coefficients (or I suppose) but I don't know what to do from there, especially when the terms are only correct up to sign. Any hints how to prove this identity? Perhaps there is a clever trick to it but I can't see it. Edit: Maybe writing will help.","\mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{R}^3 \mathbf{N} (\mathbf{x}\times\mathbf{y} \cdot \mathbf{N})\ \mathbf{z} + (\mathbf{y}\times\mathbf{z} \cdot \mathbf{N})\ \mathbf{x} + (\mathbf{z}\times\mathbf{x} \cdot \mathbf{N})\ \mathbf{y}=\mathbf{0}. \mathbf{N}=\frac{\mathbf{x}\times\mathbf{y}}{\| \mathbf{x}\times\mathbf{y}\|}=\frac{\mathbf{y}\times\mathbf{z}}{\| \mathbf{y}\times\mathbf{z}\|}=\frac{\mathbf{z}\times\mathbf{x}}{\| \mathbf{z}\times\mathbf{x}\|} \| \mathbf{x}\times\mathbf{y}\|z +\| \mathbf{y}\times\mathbf{z}\|\mathbf{x}+\| \mathbf{z}\times\mathbf{x}\|\mathbf{y}=\mathbf{0} \pm \sin\theta,\sin\psi,\sin(\theta+\psi) 2\pi -(\theta+\psi) \mathbf{z}=\lambda\mathbf{x}+\mu\mathbf{y}","['linear-algebra', 'vectors', 'cross-product']"
2,Calculating matrix exponential,Calculating matrix exponential,,Given matrix $$M = \begin{pmatrix} 7i& -6-2i\\6-2i&-7i\end{pmatrix}$$ how do I calculate matrix exponential $e^M$ ? I know I can use that $e^A=Pe^DP^{-1}$ where $D=P^{-1}AP$ . I computed the characteristic polynomial of the above matrix as $$P(\lambda)=\lambda^2+89$$ Is there an easier way to do this than trying to compute the diagonalized matrix?,Given matrix how do I calculate matrix exponential ? I know I can use that where . I computed the characteristic polynomial of the above matrix as Is there an easier way to do this than trying to compute the diagonalized matrix?,M = \begin{pmatrix} 7i& -6-2i\\6-2i&-7i\end{pmatrix} e^M e^A=Pe^DP^{-1} D=P^{-1}AP P(\lambda)=\lambda^2+89,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-exponential']"
3,Diagonalized matrix - Is each column an eigenvector?,Diagonalized matrix - Is each column an eigenvector?,,"Everything below is correct. But I got a question at the bottom (yellow). Given is matrix $A=\begin{pmatrix} 11 &  0 &  -6\\   0 &  5 &  -6\\  -6 & -6 &  -2 \end{pmatrix}$ which is diagonalizable . The eigenvalues are $$\lambda_1= -7\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \lambda_2= 7\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \lambda_3=14$$ The eigenspaces are $E_{A}(\lambda_1) = \left\{ \begin{pmatrix} \frac{z}{3}\\  \frac{z}{2}\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}, E_{A}(\lambda_2) = \left\{ \begin{pmatrix} \frac{3}{2}z\\  -3z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ $E_{A}(\lambda_3) = \left\{ \begin{pmatrix} -2z\\  -\frac{2}{3}z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ Let's say I was supposed to to determine an orthonormal basis   $(v_1,v_2,v_3) \in \mathbb{R}^{3}$ of the eigenvectors of $A$, can I   just write the matrix in its diagonalized form and continue there   determining an orthonormal basis? It should be possible iff each   column is an eigenvector of the eigenspace, right? Is it? So matrix A diagonalized is $A_D= \begin{pmatrix} -7 & 0 & 0\\   0 & 7 & 0\\   0 & 0 & 14 \end{pmatrix}$ Now can I say that $v_1= \begin{pmatrix} -7\\  0\\  0 \end{pmatrix}, v_2= \begin{pmatrix} 0\\  7\\  0 \end{pmatrix}, v_3= \begin{pmatrix} 0\\  0\\  14 \end{pmatrix}$ and   determine the orthonormal basis with these? Sorry for making this question so long, I didn't know how to keep it short. I hope you can help me anyway.","Everything below is correct. But I got a question at the bottom (yellow). Given is matrix $A=\begin{pmatrix} 11 &  0 &  -6\\   0 &  5 &  -6\\  -6 & -6 &  -2 \end{pmatrix}$ which is diagonalizable . The eigenvalues are $$\lambda_1= -7\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \lambda_2= 7\text{ }\text{ }\text{ }\text{ }\text{ }\text{ } \lambda_3=14$$ The eigenspaces are $E_{A}(\lambda_1) = \left\{ \begin{pmatrix} \frac{z}{3}\\  \frac{z}{2}\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}, E_{A}(\lambda_2) = \left\{ \begin{pmatrix} \frac{3}{2}z\\  -3z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ $E_{A}(\lambda_3) = \left\{ \begin{pmatrix} -2z\\  -\frac{2}{3}z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ Let's say I was supposed to to determine an orthonormal basis   $(v_1,v_2,v_3) \in \mathbb{R}^{3}$ of the eigenvectors of $A$, can I   just write the matrix in its diagonalized form and continue there   determining an orthonormal basis? It should be possible iff each   column is an eigenvector of the eigenspace, right? Is it? So matrix A diagonalized is $A_D= \begin{pmatrix} -7 & 0 & 0\\   0 & 7 & 0\\   0 & 0 & 14 \end{pmatrix}$ Now can I say that $v_1= \begin{pmatrix} -7\\  0\\  0 \end{pmatrix}, v_2= \begin{pmatrix} 0\\  7\\  0 \end{pmatrix}, v_3= \begin{pmatrix} 0\\  0\\  14 \end{pmatrix}$ and   determine the orthonormal basis with these? Sorry for making this question so long, I didn't know how to keep it short. I hope you can help me anyway.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
4,What exactly is a basis in linear algebra?,What exactly is a basis in linear algebra?,,"I have a brief understanding of bases. But I don't know if it is right or not. So, I just need someone to correct me if it's not. When we look for the basis of the image of a matrix, we simply remove all the redundant vectors from the matrix, and keep the linearly independent column vectors. When we look for the basis of the kernel of a matrix, we remove all the redundant column vectors from the kernel, and keep the linearly independent column vectors. Therefore, a basis is just a combination of all the linearly independent vectors. By the way, is basis just the plural form of base? Let me know if I am right.","I have a brief understanding of bases. But I don't know if it is right or not. So, I just need someone to correct me if it's not. When we look for the basis of the image of a matrix, we simply remove all the redundant vectors from the matrix, and keep the linearly independent column vectors. When we look for the basis of the kernel of a matrix, we remove all the redundant column vectors from the kernel, and keep the linearly independent column vectors. Therefore, a basis is just a combination of all the linearly independent vectors. By the way, is basis just the plural form of base? Let me know if I am right.",,"['linear-algebra', 'matrices', 'vectors']"
5,How do I prove that $x^TAy = y^TAx$ if A is symmetric?,How do I prove that  if A is symmetric?,x^TAy = y^TAx,"Ok this is for a HW but I'm not looking for a handout...just a hint to get me on the right track. I have no idea where to begin proving this: Show that if A is a symmetric matrix, then $$x^TAy = y^TAx$$","Ok this is for a HW but I'm not looking for a handout...just a hint to get me on the right track. I have no idea where to begin proving this: Show that if A is a symmetric matrix, then $$x^TAy = y^TAx$$",,['linear-algebra']
6,Basis for $\mathbb R$ over $\mathbb Q$,Basis for  over,\mathbb R \mathbb Q,Give me some examples of basis for $\mathbb R$ (as vector space over field $\mathbb F=\mathbb Q$ ). Thanks.,Give me some examples of basis for (as vector space over field ). Thanks.,\mathbb R \mathbb F=\mathbb Q,"['linear-algebra', 'linear-transformations']"
7,How to tell if two 3D vectors are in the same direction?,How to tell if two 3D vectors are in the same direction?,,Given: $$AB=\left( \begin{array}{ccc}2\\1\\3\end{array} \right) \;\;\;\; \text{and}\;\;\;\; CD=\left( \begin{array}{ccc}4\\3\\6\end{array} \right).$$ Justify if $AB$ has the same direction as $CD$ -- give the explanation.,Given: $$AB=\left( \begin{array}{ccc}2\\1\\3\end{array} \right) \;\;\;\; \text{and}\;\;\;\; CD=\left( \begin{array}{ccc}4\\3\\6\end{array} \right).$$ Justify if $AB$ has the same direction as $CD$ -- give the explanation.,,"['linear-algebra', 'vectors']"
8,Dot product of two vectors without a common origin,Dot product of two vectors without a common origin,,"Given two unit vectors $v_1, v_2\in R^n$, their dot product is defined as $v_1^Tv_2=\|v_1\|\cdot\|v_2\|\cdot\cos(\alpha)=\cos(\alpha)$. Now, suppose the vectors are in a relation $v_2=v_1+a\cdot1_n$, i.e., the vectors are parallel (one is the shifted version of another), where $a\in \mathbb{R}$ is a constant and $1_n=[1 \dots 1]\in\mathbb{R}^n$ is a vector of all ones.  Dot product would now be $$v_1^Tv_2=v_1^T(v_1+a\cdot1_n)=1+a\cdot v_1^T1_n \tag 1.$$ This implies that by shifting the vectors, the dot product changes, but still $v_1v_2=\cos(\alpha)$, where the angle now has no meaning. Does that imply that, to perform the proper angle check between two vectors one has to center them (average of vector entries is zero for both vectors, which would be one option)? If so, what does this imply geometrically? Are vectors brought to the same origin this way? Another, more interesting observation is with $v_1, v_2$ being unit eigenvectors of two diffrent matrices. From (1), it can be concluded that when the vector entry sum is zero (vectors are centered), the origin is shared (?). For which matrices is it the case that the eigenvectors are centered, ie., with the eigenvector entry sum of zero? For instance, with the matrices having $1_n$ as an eigenvector, by virtue of orthogonality, the sum of entries for each other eigenvector is 0. Are these the only kind of matrices with this kind of the eigenvector property?","Given two unit vectors $v_1, v_2\in R^n$, their dot product is defined as $v_1^Tv_2=\|v_1\|\cdot\|v_2\|\cdot\cos(\alpha)=\cos(\alpha)$. Now, suppose the vectors are in a relation $v_2=v_1+a\cdot1_n$, i.e., the vectors are parallel (one is the shifted version of another), where $a\in \mathbb{R}$ is a constant and $1_n=[1 \dots 1]\in\mathbb{R}^n$ is a vector of all ones.  Dot product would now be $$v_1^Tv_2=v_1^T(v_1+a\cdot1_n)=1+a\cdot v_1^T1_n \tag 1.$$ This implies that by shifting the vectors, the dot product changes, but still $v_1v_2=\cos(\alpha)$, where the angle now has no meaning. Does that imply that, to perform the proper angle check between two vectors one has to center them (average of vector entries is zero for both vectors, which would be one option)? If so, what does this imply geometrically? Are vectors brought to the same origin this way? Another, more interesting observation is with $v_1, v_2$ being unit eigenvectors of two diffrent matrices. From (1), it can be concluded that when the vector entry sum is zero (vectors are centered), the origin is shared (?). For which matrices is it the case that the eigenvectors are centered, ie., with the eigenvector entry sum of zero? For instance, with the matrices having $1_n$ as an eigenvector, by virtue of orthogonality, the sum of entries for each other eigenvector is 0. Are these the only kind of matrices with this kind of the eigenvector property?",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
9,Linear Algebra - Linear transformation question,Linear Algebra - Linear transformation question,,"Let $ b \in \mathbb{R}^4, \space A\in M_{4\times4} (\mathbb{R}).  $ Suppose $$ \begin{bmatrix}  0  \\ 1 \\ 1 \\ 1\end{bmatrix}, \begin{bmatrix}  1  \\ 0 \\ 1 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 0 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 1 \\ 0\end{bmatrix}, \begin{bmatrix}  1  \\ 1 \\ 1 \\ 1\end{bmatrix}$$ are all solutions of the equation $Ax=b$ . Prove that $$b = \begin{bmatrix}  0  \\ 0 \\ 0 \\ 0\end{bmatrix}.$$ I had an intuiton that the given vectors are linear-dependent since there are five of them and the dim of the vector space is four. The professor solved that question after the exam using linear transformation which seemed much easier and clean solution however he didn't explain the intuition behind. I  would like to get an explanation regarding that. Thanks",Let Suppose are all solutions of the equation . Prove that I had an intuiton that the given vectors are linear-dependent since there are five of them and the dim of the vector space is four. The professor solved that question after the exam using linear transformation which seemed much easier and clean solution however he didn't explain the intuition behind. I  would like to get an explanation regarding that. Thanks,"
b \in \mathbb{R}^4, \space A\in M_{4\times4} (\mathbb{R}). 
 
\begin{bmatrix}  0  \\ 1 \\ 1 \\ 1\end{bmatrix}, \begin{bmatrix}  1  \\ 0 \\ 1 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 0 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 1 \\ 0\end{bmatrix}, \begin{bmatrix}  1  \\ 1 \\ 1 \\ 1\end{bmatrix} Ax=b b = \begin{bmatrix}  0  \\ 0 \\ 0 \\ 0\end{bmatrix}.","['linear-algebra', 'abstract-algebra', 'linear-transformations']"
10,What are some alternative definitions of vector addition and scalar multiplication?,What are some alternative definitions of vector addition and scalar multiplication?,,"While teaching the concept of vector spaces, my professor mentioned that addition and multiplication aren't necessarily what we normally call addition and multiplication, but any other function that complies with the eight axioms needed by the definition of a vector space (for instance, associativity, commutativity of addition, etc.). Is there any widely used vector space in which alternative functions are used as addition/multiplication?","While teaching the concept of vector spaces, my professor mentioned that addition and multiplication aren't necessarily what we normally call addition and multiplication, but any other function that complies with the eight axioms needed by the definition of a vector space (for instance, associativity, commutativity of addition, etc.). Is there any widely used vector space in which alternative functions are used as addition/multiplication?",,"['linear-algebra', 'vector-spaces']"
11,is $\det(A^2 + I)$ always non negative?,is  always non negative?,\det(A^2 + I),"Obviously $\det(A^2)$ is (casework), but is the above matrix non-negative? $\det(A)\det(A) \geq 0$ as $\det(A) > 0$ or $\det(A) < 0$ yields positive when squared. However, I am not sure that when adding the identity matrix that it is also positive.","Obviously is (casework), but is the above matrix non-negative? as or yields positive when squared. However, I am not sure that when adding the identity matrix that it is also positive.",\det(A^2) \det(A)\det(A) \geq 0 \det(A) > 0 \det(A) < 0,['linear-algebra']
12,Where does the Jordan canonical form show up in more advanced mathematics?,Where does the Jordan canonical form show up in more advanced mathematics?,,"My bounty for this question expires soon :) Edit: in regards to the bounty offered, what current research trends use the Jordan canonical form ? If one takes a second course in Linear Algebra — or a graduate level Linear Algebra course — one typically learns about non-diagonalizable operators and the Jordan canonical form. However, where does the Jordan canonical form show up again in later, more advanced mathematics?  All I hear from applied mathematicians is that the Jordan canonical form is useless in practice (in academic research).  If it's not useful in applied mathematics, is it an important tool in pure mathematics?  If so, in which areas of pure mathematics?","My bounty for this question expires soon :) Edit: in regards to the bounty offered, what current research trends use the Jordan canonical form ? If one takes a second course in Linear Algebra — or a graduate level Linear Algebra course — one typically learns about non-diagonalizable operators and the Jordan canonical form. However, where does the Jordan canonical form show up again in later, more advanced mathematics?  All I hear from applied mathematicians is that the Jordan canonical form is useless in practice (in academic research).  If it's not useful in applied mathematics, is it an important tool in pure mathematics?  If so, in which areas of pure mathematics?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'jordan-normal-form', 'research']"
13,If $AB=I_n$ and $BA=I_m$ then $n=m$.,If  and  then .,AB=I_n BA=I_m n=m,"Let $A, B$ matrices $n\times m$ and $m\times n$ respectively so that $AB=I_n$ and $BA=I_m$. Show that $n=m$.","Let $A, B$ matrices $n\times m$ and $m\times n$ respectively so that $AB=I_n$ and $BA=I_m$. Show that $n=m$.",,"['linear-algebra', 'matrices']"
14,Proof if $a \vec v = 0$ then $a = 0$ or $\vec v = 0$,Proof if  then  or,a \vec v = 0 a = 0 \vec v = 0,"I'm kicking myself over this one, but I just can't seem to make the argument rigorous. From Axler's Linear Algebra Done Right : for a vector space $V$ with an underlying field $F$: Take an element $a$ from $F$ and $\vec{v}$ from $V$. $a\vec{v}=\vec{0}\implies a=0 $ or $ \vec{v}=\vec{0}$ After only being able to come up with half of a direct proof, I tried doing this by proving the contrapositive $a\neq 0 \wedge \vec{v} \neq \vec{0} \implies a\vec{v}\neq \vec{0}$ Say $a\vec{v}=\vec{u}$. Since $a$ is non-zero, we can divide both sides by $a$. $$\vec{v}=\frac 1 a \vec{u}$$ If $\vec{u}$ were $\vec{0}$ then by $$\frac 1 a \vec{0}=\frac 1 a (\vec{0}+\vec{0})\implies\frac 1 a \vec{0}=\vec{0}$$ $v$ would be $0$ as well. Since it isn't by assumption, $\frac 1 a \vec{u}$ cannot be zero and so $\vec{u}$ cannot be as well. Is this fully rigorous? It seems like a very simple question, but I'm not sure about it. Namely, the last step of $\frac 1 a \vec{u}\neq 0 \implies \vec{u}\neq 0$ doesn't seem obvious. I think I need to use the $1\vec{v}=\vec{v}$ axiom, but I'm not sure how. Is there a more direct proof? This whole contrapositive business seems a bit clunky for something so simple.","I'm kicking myself over this one, but I just can't seem to make the argument rigorous. From Axler's Linear Algebra Done Right : for a vector space $V$ with an underlying field $F$: Take an element $a$ from $F$ and $\vec{v}$ from $V$. $a\vec{v}=\vec{0}\implies a=0 $ or $ \vec{v}=\vec{0}$ After only being able to come up with half of a direct proof, I tried doing this by proving the contrapositive $a\neq 0 \wedge \vec{v} \neq \vec{0} \implies a\vec{v}\neq \vec{0}$ Say $a\vec{v}=\vec{u}$. Since $a$ is non-zero, we can divide both sides by $a$. $$\vec{v}=\frac 1 a \vec{u}$$ If $\vec{u}$ were $\vec{0}$ then by $$\frac 1 a \vec{0}=\frac 1 a (\vec{0}+\vec{0})\implies\frac 1 a \vec{0}=\vec{0}$$ $v$ would be $0$ as well. Since it isn't by assumption, $\frac 1 a \vec{u}$ cannot be zero and so $\vec{u}$ cannot be as well. Is this fully rigorous? It seems like a very simple question, but I'm not sure about it. Namely, the last step of $\frac 1 a \vec{u}\neq 0 \implies \vec{u}\neq 0$ doesn't seem obvious. I think I need to use the $1\vec{v}=\vec{v}$ axiom, but I'm not sure how. Is there a more direct proof? This whole contrapositive business seems a bit clunky for something so simple.",,"['linear-algebra', 'vector-spaces']"
15,About positive semidefiniteness of one matrix,About positive semidefiniteness of one matrix,,"It is not clear how to prove that the matrix $(\min(i,j))_{i,j=1,\dots,n}$ is (or is not) positive semidefinite. There are some facts from Horn and Johnson's book Matrix Analysis : if $A \in M_n$ is positive semidefinite, then $a_{ii}a_{jj} \ge a_{ij}^2, i,j=1,2,\dots, n$ ( source ). It seems that this condition is not enough for the matrix to be positive semidefinite. Are there any techniques to check that all eigenvalues of the matrix are positive, or that the minor determinants are non-negative?","It is not clear how to prove that the matrix $(\min(i,j))_{i,j=1,\dots,n}$ is (or is not) positive semidefinite. There are some facts from Horn and Johnson's book Matrix Analysis : if $A \in M_n$ is positive semidefinite, then $a_{ii}a_{jj} \ge a_{ij}^2, i,j=1,2,\dots, n$ ( source ). It seems that this condition is not enough for the matrix to be positive semidefinite. Are there any techniques to check that all eigenvalues of the matrix are positive, or that the minor determinants are non-negative?",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
16,Matrix Representation of Inner Product,Matrix Representation of Inner Product,,"In my readings I have on several occasions encountered references to a linear algebra theorem that runs as follows: Let $g$ be a non-degenerate inner product on the real vector space $V$. Then, there exists a basis $e_1, \dots, e_n$ such that the matrix of $g$ is diagonal and whose diagonal entries are all either $-1$ or $1$ Despite having encountered this result several times, unfortunately, I have not had the luck of finding a precise reference to where this theorem is stated/proved. Can anyone provide a reference that proves this claim?","In my readings I have on several occasions encountered references to a linear algebra theorem that runs as follows: Let $g$ be a non-degenerate inner product on the real vector space $V$. Then, there exists a basis $e_1, \dots, e_n$ such that the matrix of $g$ is diagonal and whose diagonal entries are all either $-1$ or $1$ Despite having encountered this result several times, unfortunately, I have not had the luck of finding a precise reference to where this theorem is stated/proved. Can anyone provide a reference that proves this claim?",,"['linear-algebra', 'reference-request']"
17,If $A$ is an antisymmetric matrix then $A+I$ is invertible [closed],If  is an antisymmetric matrix then  is invertible [closed],A A+I,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $A$ be an antisymmetric matrix with real entries. How can I show that $A+I$ is invertible?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $A$ be an antisymmetric matrix with real entries. How can I show that $A+I$ is invertible?",,"['linear-algebra', 'matrices']"
18,Orthogonal Projection onto the $ {L}_{2} $ Unit Ball,Orthogonal Projection onto the  Unit Ball, {L}_{2} ,"On an article I'm reading, I find that: if $v$ is a vector, the projection of of $v$ on the unit ball is: $$p(v)=\frac{v}{\max\{1,\|v\|\}}$$ I know that a projection of a point $v$ into a space is the nearest point to $v$ inside the space..why the expression above?","On an article I'm reading, I find that: if $v$ is a vector, the projection of of $v$ on the unit ball is: $$p(v)=\frac{v}{\max\{1,\|v\|\}}$$ I know that a projection of a point $v$ into a space is the nearest point to $v$ inside the space..why the expression above?",,"['linear-algebra', 'geometry', 'optimization', 'convex-optimization', 'matlab']"
19,Apostol proof divides by zero?,Apostol proof divides by zero?,,"The following is a proof from Apostol's linear algebra book that $\{1,t,t^{2},...\}$ is independent. To my eye, he's dividing by zero repeatedly. Is this really as huge an error as it seems, or are there missing details that would make this rigorous? It suffices to show that for each $n$ the $n+1$ polynomials $1,t,...,t^{n}$ are independent. If these polynomials span the zero polynomial we have $$\sum_{k=0}^{n}c_{k}t^{k}=0$$ for all real $t$. Putting $t=0$ we see that $c_{0}=0$. Now divide by $t$ and put $t=0$ again to find $c_{1}=0$. Repeating the process we find every $c_{k}$ is $0$, so $1,t,...,t^{n}$ are independent. Edit : I'm treating ""polynomial"" as a garden-variety function from $\mathbb{R}$ to $\mathbb{R}$, and it seems like that may be part of my problem.","The following is a proof from Apostol's linear algebra book that $\{1,t,t^{2},...\}$ is independent. To my eye, he's dividing by zero repeatedly. Is this really as huge an error as it seems, or are there missing details that would make this rigorous? It suffices to show that for each $n$ the $n+1$ polynomials $1,t,...,t^{n}$ are independent. If these polynomials span the zero polynomial we have $$\sum_{k=0}^{n}c_{k}t^{k}=0$$ for all real $t$. Putting $t=0$ we see that $c_{0}=0$. Now divide by $t$ and put $t=0$ again to find $c_{1}=0$. Repeating the process we find every $c_{k}$ is $0$, so $1,t,...,t^{n}$ are independent. Edit : I'm treating ""polynomial"" as a garden-variety function from $\mathbb{R}$ to $\mathbb{R}$, and it seems like that may be part of my problem.",,['linear-algebra']
20,Why is the exterior algebra a bi-algebra (and even a Hopf algebra)?,Why is the exterior algebra a bi-algebra (and even a Hopf algebra)?,,"According to the wikipedia, the exterior algebra of a $\Bbbk$-vector space $V$ is initial with respect to being unital and there existing a $\Bbbk$-linear map $j\colon V\to A$ such that $j(v)^2=0$ for all $v\in V$. This is a reasonable algebra to consider if one is interested in measuring $k$-dimensional volumes, which are specified by $k$ linearly independent vectors, and which are degenerate if the vectors are not linearly independent (equivalently, for $char \Bbbk\neq 2$, measuring $k$-dimensional signed volumes). Then multiplication consists simply of throwing in extra vectors. My question, inspired by the recent question on the meaning of addition in the exterior algebra , is about the meaning of co-multiplication. I happen to know virtually nothing about co-algebras outside of their formalism, so in part I am looking for answers that will help me build some intuition, geometric and otherwise, about what's happening. What is a categorical argument that the exterior algebra satisfying the above universal property has co-multiplication (and is thus a bi-algbera)? Is there a (heurisitc) geometric interpretation of the exterior algebra's co-multiplication similar to the $k$-dimensional volume tracking I sketched above? (Bonus question): Is there any geometric significance to the anti-pode that makes the exterior (bi-)algebra into a Hopf algebra?","According to the wikipedia, the exterior algebra of a $\Bbbk$-vector space $V$ is initial with respect to being unital and there existing a $\Bbbk$-linear map $j\colon V\to A$ such that $j(v)^2=0$ for all $v\in V$. This is a reasonable algebra to consider if one is interested in measuring $k$-dimensional volumes, which are specified by $k$ linearly independent vectors, and which are degenerate if the vectors are not linearly independent (equivalently, for $char \Bbbk\neq 2$, measuring $k$-dimensional signed volumes). Then multiplication consists simply of throwing in extra vectors. My question, inspired by the recent question on the meaning of addition in the exterior algebra , is about the meaning of co-multiplication. I happen to know virtually nothing about co-algebras outside of their formalism, so in part I am looking for answers that will help me build some intuition, geometric and otherwise, about what's happening. What is a categorical argument that the exterior algebra satisfying the above universal property has co-multiplication (and is thus a bi-algbera)? Is there a (heurisitc) geometric interpretation of the exterior algebra's co-multiplication similar to the $k$-dimensional volume tracking I sketched above? (Bonus question): Is there any geometric significance to the anti-pode that makes the exterior (bi-)algebra into a Hopf algebra?",,"['linear-algebra', 'abstract-algebra']"
21,"Showing $1,e^{x}$ and $\sin{x}$ are linearly independent in $\mathcal{C}[0,1]$",Showing  and  are linearly independent in,"1,e^{x} \sin{x} \mathcal{C}[0,1]","How do i show that $f_{1}(x)=1$, $f_{2}(x)=e^{x}$ and $f_{3}(x)=\sin{x}$ are linearly independent, as elements of the vector space, of continuous functions $\mathcal{C}[0,1]$. So for showing these elements are linearly independent, one needs to show that if $$ a_{1} \cdot 1 + a_{2} \cdot e^{x} + a_{3} \cdot \sin{x}=0$$ then from this we should conclude that $a_{1}=a_{2}=a_{3}=0$. But i am not being able to deduce this.","How do i show that $f_{1}(x)=1$, $f_{2}(x)=e^{x}$ and $f_{3}(x)=\sin{x}$ are linearly independent, as elements of the vector space, of continuous functions $\mathcal{C}[0,1]$. So for showing these elements are linearly independent, one needs to show that if $$ a_{1} \cdot 1 + a_{2} \cdot e^{x} + a_{3} \cdot \sin{x}=0$$ then from this we should conclude that $a_{1}=a_{2}=a_{3}=0$. But i am not being able to deduce this.",,['linear-algebra']
22,Direct product vs direct sum of infinite dimensional vector spaces?,Direct product vs direct sum of infinite dimensional vector spaces?,,"These seems like simple questions, but I cannot seem to find straightforward answers: (1)  If $V$ and $W$ are infinite (possibly uncountable) dimensional vector spaces, does $V \bigoplus W = V \times W$ ?  That is, is there any difference between the direct sum and direct product of infinite dimensional vector spaces? (2)  If $\{ V_i \}_{i \in I}$ is an infinite (possibly uncountable) collection of finite dimensional vector spaces, does $ \bigoplus_{i \in I} V_i = \times_{i \in I} V_i$ ?  Is the direct product product of infinitely many vector spaces even defined since vectors are supposed to consist of finite linear combinations of basis vectors (I realize there are subtleties in what ""basis"" means in the case of an infinite dimensional vector space)? The inspiration for this question comes from the study of Banach spaces.  Namely, that the product (or direct sum?) of Banach spaces can be made into a Banach space.","These seems like simple questions, but I cannot seem to find straightforward answers: (1)  If and are infinite (possibly uncountable) dimensional vector spaces, does ?  That is, is there any difference between the direct sum and direct product of infinite dimensional vector spaces? (2)  If is an infinite (possibly uncountable) collection of finite dimensional vector spaces, does ?  Is the direct product product of infinitely many vector spaces even defined since vectors are supposed to consist of finite linear combinations of basis vectors (I realize there are subtleties in what ""basis"" means in the case of an infinite dimensional vector space)? The inspiration for this question comes from the study of Banach spaces.  Namely, that the product (or direct sum?) of Banach spaces can be made into a Banach space.",V W V \bigoplus W = V \times W \{ V_i \}_{i \in I}  \bigoplus_{i \in I} V_i = \times_{i \in I} V_i,"['linear-algebra', 'abstract-algebra', 'functional-analysis', 'vector-spaces']"
23,what is the difference between linear transformation and affine transformation?,what is the difference between linear transformation and affine transformation?,,"Recently, I am struglling with the difference between linear transformation and affine transformation. Are they the same ?  I found an interesting question on the difference between the functions. But still, not very clear. What is the difference between linear and affine function http://mathworld.wolfram.com/AffineTransformation.html","Recently, I am struglling with the difference between linear transformation and affine transformation. Are they the same ?  I found an interesting question on the difference between the functions. But still, not very clear. What is the difference between linear and affine function http://mathworld.wolfram.com/AffineTransformation.html",,"['linear-algebra', 'discrete-mathematics', 'transformation']"
24,Center of the Orthogonal Group and Special Orthogonal Group,Center of the Orthogonal Group and Special Orthogonal Group,,"How can I prove that the center of $\operatorname{O}_n$ is $\pm I_n$ ? I have that $AM = MA$ , $\forall M \in \operatorname{O}_n$ and $A^{-1} = A^T$ , $M^{-1} = M^T$ . Then $M = A^{-1}MA = A^{T}MA$ . I see that since conjugating by $A$ must leave the determinant of $M$ unchanged then the result of conjugation must be a rotation. But now I'm stuck. How to proceed? Thanks in advance.","How can I prove that the center of is ? I have that , and , . Then . I see that since conjugating by must leave the determinant of unchanged then the result of conjugation must be a rotation. But now I'm stuck. How to proceed? Thanks in advance.",\operatorname{O}_n \pm I_n AM = MA \forall M \in \operatorname{O}_n A^{-1} = A^T M^{-1} = M^T M = A^{-1}MA = A^{T}MA A M,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'orthogonal-matrices']"
25,Prove concurrency of triangle altitudes with vector algebra?,Prove concurrency of triangle altitudes with vector algebra?,,"I know how to do it in normal Euclid geometry, but is it possible to do it with vector algebra?","I know how to do it in normal Euclid geometry, but is it possible to do it with vector algebra?",,"['linear-algebra', 'geometry']"
26,Can an underdetermined system have a unique solution?,Can an underdetermined system have a unique solution?,,"In my case, I am calling an underdetermined system as a system of linear equations where there are fewer equations than variables (unknowns). My textbook says the answer is false, however the internet says otherwise. On the outside, it makes sense that an underdetermined system can't have a unique solution, but I am leaning towards trusting the internet on this one. Can someone explain how a unique solution exists? A proof  might be helpful too, although hopefully not too complex.","In my case, I am calling an underdetermined system as a system of linear equations where there are fewer equations than variables (unknowns). My textbook says the answer is false, however the internet says otherwise. On the outside, it makes sense that an underdetermined system can't have a unique solution, but I am leaning towards trusting the internet on this one. Can someone explain how a unique solution exists? A proof  might be helpful too, although hopefully not too complex.",,"['linear-algebra', 'matrices']"
27,Determining If a Function is a Linear Transformation,Determining If a Function is a Linear Transformation,,"I thought I understood this concept, as it's pretty similar to other ones I've done in linear algebra. Perhaps I'm tired, and just not seeing it right now. Here's what I know: For the vector spaces $V$ and $W$, the function $T: V\to W$  is a linear transformation of $V$ mapping into $W$ when two properties are true (for all vectors $u,v$ and any scalar $c$): $T(u+v) = T(u) + T(v)$   - Addition in $V$ to addition in $W$ $T(cu) = cT(u)$ - Scalar multiplication in $V$ to SM in $W$ My book gives an example of proving $T(v_1, v_2) = (v_1 - v_2, v_1+2v_2)$ is a linear transformation. I won't copy the whole thing, but I actually follow it. However, for practice, I have to determine if this function is a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$... $T:\mathbb{R}^2 \to \mathbb{R}^2$ , $T(x,y) = (x,1)$ I tried doing it, but I honestly wasn't going anywhere or really understanding what I was doing. Can anyone please explain this problem to me? Appreciate any help.","I thought I understood this concept, as it's pretty similar to other ones I've done in linear algebra. Perhaps I'm tired, and just not seeing it right now. Here's what I know: For the vector spaces $V$ and $W$, the function $T: V\to W$  is a linear transformation of $V$ mapping into $W$ when two properties are true (for all vectors $u,v$ and any scalar $c$): $T(u+v) = T(u) + T(v)$   - Addition in $V$ to addition in $W$ $T(cu) = cT(u)$ - Scalar multiplication in $V$ to SM in $W$ My book gives an example of proving $T(v_1, v_2) = (v_1 - v_2, v_1+2v_2)$ is a linear transformation. I won't copy the whole thing, but I actually follow it. However, for practice, I have to determine if this function is a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$... $T:\mathbb{R}^2 \to \mathbb{R}^2$ , $T(x,y) = (x,1)$ I tried doing it, but I honestly wasn't going anywhere or really understanding what I was doing. Can anyone please explain this problem to me? Appreciate any help.",,"['linear-algebra', 'linear-transformations']"
28,How to evaluate the derivatives of matrix inverse?,How to evaluate the derivatives of matrix inverse?,,"Cliff Taubes wrote in his differential geometry book that: We now calculate the directional derivatives of the map $$M\rightarrow M^{-1}$$ Let $\alpha\in M(n,\mathbb{R})$ denote any given matrix. Then the directional derivatives of the coordinates of the map $M\rightarrow M^{-1}$ in the drection $\alpha$ are the entries of the matrix $$-M^{-1}\alpha M^{-1}$$ Consider, for example, the coordinate given by the $(i,j)$th entry, $(M^{-1})_{ij}$. The directional derivative in the drection $\alpha$ of this function on $GL(n,\mathbb{R})$ is $$-(M^{-1}\alpha M^{-1})_{ij}$$ In particular, the partial derivative of the function $M\rightarrow (M^{-1})_{ij}$ with respect to the coordinate $M_{rs}$ is $-(M^{-1})_{ir}(M^{-1})_{sj}$. I am wondering why this is true. He did not give any deduction of this formula, and all the formulas I know for matrix inverse does not generate anything similar to his result. So I venture to ask.","Cliff Taubes wrote in his differential geometry book that: We now calculate the directional derivatives of the map $$M\rightarrow M^{-1}$$ Let $\alpha\in M(n,\mathbb{R})$ denote any given matrix. Then the directional derivatives of the coordinates of the map $M\rightarrow M^{-1}$ in the drection $\alpha$ are the entries of the matrix $$-M^{-1}\alpha M^{-1}$$ Consider, for example, the coordinate given by the $(i,j)$th entry, $(M^{-1})_{ij}$. The directional derivative in the drection $\alpha$ of this function on $GL(n,\mathbb{R})$ is $$-(M^{-1}\alpha M^{-1})_{ij}$$ In particular, the partial derivative of the function $M\rightarrow (M^{-1})_{ij}$ with respect to the coordinate $M_{rs}$ is $-(M^{-1})_{ir}(M^{-1})_{sj}$. I am wondering why this is true. He did not give any deduction of this formula, and all the formulas I know for matrix inverse does not generate anything similar to his result. So I venture to ask.",,"['linear-algebra', 'differential-geometry', 'lie-groups']"
29,"Let $A\in M_{n\times n}(\Bbb R)$ so that $I\notin span(A,A^2,...,A^n)$. Prove that $\det(A)=0$.",Let  so that . Prove that .,"A\in M_{n\times n}(\Bbb R) I\notin span(A,A^2,...,A^n) \det(A)=0","Let $A\in M_{n\times n}(\Bbb R)$ so that $I\notin span(A,A^2,...,A^n)$. Prove that $\det(A)=0$. I was thinking of showing $A$ is not invertible, meaning it has an eigenvalue of $0$. Since no matter the power you give to $A$, it is still not the identity, you can deduce that at least one eigenvalue is indeed $0$. However this doesn't work if you have different eigenvalues in the Matrix, so I got stuck. Help will be most appreciated.","Let $A\in M_{n\times n}(\Bbb R)$ so that $I\notin span(A,A^2,...,A^n)$. Prove that $\det(A)=0$. I was thinking of showing $A$ is not invertible, meaning it has an eigenvalue of $0$. Since no matter the power you give to $A$, it is still not the identity, you can deduce that at least one eigenvalue is indeed $0$. However this doesn't work if you have different eigenvalues in the Matrix, so I got stuck. Help will be most appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
30,The set of all linear maps T:V->W is a vector space,The set of all linear maps T:V->W is a vector space,,"I have to show that the set $L$ of all linear maps $T: V \to W$ is a vector space w.r.t the addition $$(T_1 + T_2)(\vec{v}) = T_1(\vec{v}) + T_2(\vec{v})$$ and scalar multiplication $$(xT)(\vec{v}) = xT(\vec{v})$$ such that $T_1,T_2,T \in L$, $\vec{v} \in V$, and $x \in \mathbb{R}$ I'm asked to detail all 10 properties required for $L$ to be a vector space as follows; $ \forall \vec{u}, \vec{v}, \vec{w} \in V$  & $\forall x,y \in \mathbb{R}$ i) Closure:  $$(T_1 + T_2)(\vec{v} + \vec{w}) = T_1(\vec{v} + \vec{w})) + T_2(\vec{v} + \vec{w}) \in L$$ ii) Associativity:  $$(T_1 + T_2)(\vec{u} + \vec{v} + \vec{w}) =$$  $$((T_1 + T_2)(\vec{u}) + (T_1 + T_2)(\vec{v})) + (T_1 + T_2)(\vec{w}) =$$ $$ (T_1 + T_2)(\vec{u}) + ((T_1 + T_2)(\vec{v}) + (T_1 + T_2)(\vec{w}))$$ iii) Commutativity:  $$(T_1 + T_2)(\vec{u} + \vec{v}) = (T_1 + T_2)(\vec{u}) + (T_1 + T_2)(\vec{v}) = (T_1 + T_2)(\vec{v}) + (T_1 + T_2)(\vec{u})$$ iv) Identity:  $$(T_1 + T_2)(\vec{0}) = T_1(\vec{0}) + T_2(\vec{0}) = 0 *T_1(\vec{0}) + 0*T_2(\vec{0}) = \vec{0} + \vec{0} = \vec{0} \in L$$ v) Inverse: $$(T_1+T_2)(-\vec{v}) = T_1(-\vec{v}) + T_2(-\vec{v}) = -T_1(\vec{v}) - T_2(\vec{v}) = -(T_1(\vec{v}) + T_2(\vec{v})) =$$  $$-(T_1 + T_2)(\vec{v}) \in L$$ For scalar multiplication vi) Closure:  $$(xT)(\vec{v}) = xT(\vec{v}) \in L$$ vii) Compatibility: $$ (xT)(\vec{u} + \vec{v}) = xT(\vec{u} + \vec{v}) = x(T(\vec{u}) + T(\vec{v})) =$$ $$ xT(\vec{u}) + xT(\vec{v})$$ viii) Compatibility 2: $$((xy)T)(\vec{v}) = x(yT(\vec{v}))$$ ix) Distributivity:  $$((x + y)T)(\vec{v}) = (x + y)T(\vec{v}) = xT(\vec{v}) + yT(\vec{v})$$ x) Identity:  $$(1*T)(\vec{v}) = 1*T(\vec{v}) = T(\vec{v})$$ Have I done enough to show L is a vector space? Must I expand on the definitions for instance in ii I didn't fully apply the transformation b/c it would have been very time consuming. What is the most efficient way to show this?","I have to show that the set $L$ of all linear maps $T: V \to W$ is a vector space w.r.t the addition $$(T_1 + T_2)(\vec{v}) = T_1(\vec{v}) + T_2(\vec{v})$$ and scalar multiplication $$(xT)(\vec{v}) = xT(\vec{v})$$ such that $T_1,T_2,T \in L$, $\vec{v} \in V$, and $x \in \mathbb{R}$ I'm asked to detail all 10 properties required for $L$ to be a vector space as follows; $ \forall \vec{u}, \vec{v}, \vec{w} \in V$  & $\forall x,y \in \mathbb{R}$ i) Closure:  $$(T_1 + T_2)(\vec{v} + \vec{w}) = T_1(\vec{v} + \vec{w})) + T_2(\vec{v} + \vec{w}) \in L$$ ii) Associativity:  $$(T_1 + T_2)(\vec{u} + \vec{v} + \vec{w}) =$$  $$((T_1 + T_2)(\vec{u}) + (T_1 + T_2)(\vec{v})) + (T_1 + T_2)(\vec{w}) =$$ $$ (T_1 + T_2)(\vec{u}) + ((T_1 + T_2)(\vec{v}) + (T_1 + T_2)(\vec{w}))$$ iii) Commutativity:  $$(T_1 + T_2)(\vec{u} + \vec{v}) = (T_1 + T_2)(\vec{u}) + (T_1 + T_2)(\vec{v}) = (T_1 + T_2)(\vec{v}) + (T_1 + T_2)(\vec{u})$$ iv) Identity:  $$(T_1 + T_2)(\vec{0}) = T_1(\vec{0}) + T_2(\vec{0}) = 0 *T_1(\vec{0}) + 0*T_2(\vec{0}) = \vec{0} + \vec{0} = \vec{0} \in L$$ v) Inverse: $$(T_1+T_2)(-\vec{v}) = T_1(-\vec{v}) + T_2(-\vec{v}) = -T_1(\vec{v}) - T_2(\vec{v}) = -(T_1(\vec{v}) + T_2(\vec{v})) =$$  $$-(T_1 + T_2)(\vec{v}) \in L$$ For scalar multiplication vi) Closure:  $$(xT)(\vec{v}) = xT(\vec{v}) \in L$$ vii) Compatibility: $$ (xT)(\vec{u} + \vec{v}) = xT(\vec{u} + \vec{v}) = x(T(\vec{u}) + T(\vec{v})) =$$ $$ xT(\vec{u}) + xT(\vec{v})$$ viii) Compatibility 2: $$((xy)T)(\vec{v}) = x(yT(\vec{v}))$$ ix) Distributivity:  $$((x + y)T)(\vec{v}) = (x + y)T(\vec{v}) = xT(\vec{v}) + yT(\vec{v})$$ x) Identity:  $$(1*T)(\vec{v}) = 1*T(\vec{v}) = T(\vec{v})$$ Have I done enough to show L is a vector space? Must I expand on the definitions for instance in ii I didn't fully apply the transformation b/c it would have been very time consuming. What is the most efficient way to show this?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
31,What does a positive definite matrix have to do with Cauchy-Schwarz Inequality?,What does a positive definite matrix have to do with Cauchy-Schwarz Inequality?,,"In my text book, Cauchy-Schwarz Inequality is extended to a positive definite matrix. But I neither understand what the relationship between Cauchy-Schwarz Inequality and a positive definite matrix nor the sentence underlined in red, I am not a strong linear algebra person, I have just started studying it. I'd like post the page here so that I might get a better explanation. Thank you.","In my text book, Cauchy-Schwarz Inequality is extended to a positive definite matrix. But I neither understand what the relationship between Cauchy-Schwarz Inequality and a positive definite matrix nor the sentence underlined in red, I am not a strong linear algebra person, I have just started studying it. I'd like post the page here so that I might get a better explanation. Thank you.",,['linear-algebra']
32,Prove that the rank of the block matrix is rank $A$ + rank $B$,Prove that the rank of the block matrix is rank  + rank,A B,"Assume $A,B$ are $n \times n$ matrices. Prove that the rank of the block matrix that is defined as the following $$ \pmatrix{A& AB\\B&B+B^2} $$  is equal to $ \mbox{rank} A +\mbox{rank} B$. I have absolutely no clue...","Assume $A,B$ are $n \times n$ matrices. Prove that the rank of the block matrix that is defined as the following $$ \pmatrix{A& AB\\B&B+B^2} $$  is equal to $ \mbox{rank} A +\mbox{rank} B$. I have absolutely no clue...",,"['linear-algebra', 'matrices', 'block-matrices']"
33,Determinant of a special skew-symmetric matrix,Determinant of a special skew-symmetric matrix,,"Simple calculation show that: $$ \begin{align} \det(A_2)=\begin{vmatrix}     0& 1  \\    -1& 0   \end{vmatrix}&=1\\ \det(A_4)=\begin{vmatrix}     0& 1 &1 &1 \\    -1& 0 &1&1\\    -1& -1& 0&1\\    -1& -1& -1&0  \end{vmatrix}&=1 \end{align} $$ Here is my question : Is it true that $\det(A_{2n})=1$ for all $n\in{\mathbb Z_+}$? With MAPLE, I tried some large $n$. And I guess it is true. But temporarily I have no idea how to show it.","Simple calculation show that: $$ \begin{align} \det(A_2)=\begin{vmatrix}     0& 1  \\    -1& 0   \end{vmatrix}&=1\\ \det(A_4)=\begin{vmatrix}     0& 1 &1 &1 \\    -1& 0 &1&1\\    -1& -1& 0&1\\    -1& -1& -1&0  \end{vmatrix}&=1 \end{align} $$ Here is my question : Is it true that $\det(A_{2n})=1$ for all $n\in{\mathbb Z_+}$? With MAPLE, I tried some large $n$. And I guess it is true. But temporarily I have no idea how to show it.",,['linear-algebra']
34,Finite sum of eigenspaces (with distinct eigenvalues) is a direct sum,Finite sum of eigenspaces (with distinct eigenvalues) is a direct sum,,"I'm trying to show that a finite sum of eigenspaces (with distinct eigenvalues) is a direct sum. I have $ \alpha : V \to V $. The eigenspaces are $ V_{\lambda_i} = \ker(\alpha - \lambda_i id_V )$ for $ 1 \leq i \leq n $. My attempt at a proof: $ A + B $ is a direct sum iff $ A \cap B = \{0\} $. If $ v \neq 0 \in V_{\lambda_i} \cap V_{\lambda_j} $ for some $i,j, i \neq j $, then $ \alpha(v) = \lambda_i v $ and $ \alpha(v) = \lambda_j v $. So $(\lambda_i - \lambda_j)v = 0 $, and so $ \lambda_i = \lambda_j $. This is a contradiction, so any pair of the eigenspaces have trivial intersection. Therefore $ \cap_{i=1}^n V_{\lambda_i} = \{0\} $, and so we have a direct sum. Is this ok? Thanks","I'm trying to show that a finite sum of eigenspaces (with distinct eigenvalues) is a direct sum. I have $ \alpha : V \to V $. The eigenspaces are $ V_{\lambda_i} = \ker(\alpha - \lambda_i id_V )$ for $ 1 \leq i \leq n $. My attempt at a proof: $ A + B $ is a direct sum iff $ A \cap B = \{0\} $. If $ v \neq 0 \in V_{\lambda_i} \cap V_{\lambda_j} $ for some $i,j, i \neq j $, then $ \alpha(v) = \lambda_i v $ and $ \alpha(v) = \lambda_j v $. So $(\lambda_i - \lambda_j)v = 0 $, and so $ \lambda_i = \lambda_j $. This is a contradiction, so any pair of the eigenspaces have trivial intersection. Therefore $ \cap_{i=1}^n V_{\lambda_i} = \{0\} $, and so we have a direct sum. Is this ok? Thanks",,['linear-algebra']
35,Let $A$ be a $2 \times 2$ matrix such that $A^2=I$. Find trace and determinant,Let  be a  matrix such that . Find trace and determinant,A 2 \times 2 A^2=I,"Suppose $A$ is a $2 \times 2$ matrix, $A^2=I$ . If $A \ne I,-I$ , find $\mbox{tr}(A)$ and $\det(A)$ . My solution: There are $3$ cases for the eigenvalues of A. case(i) $1,1$ . case(ii) $-1,-1$ . case(iii) $1,-1$ . I guess only the case(iii) holds. But I have no idea whether case(i) and (ii) hold. I know that $tr(A)$ is the sum of the sum of eigenvalues and $det(A)$ is the product of eigenvalues. So the answer is $tr(A)=0$ , $det(A)=-1$ ? Thanks.","Suppose is a matrix, . If , find and . My solution: There are cases for the eigenvalues of A. case(i) . case(ii) . case(iii) . I guess only the case(iii) holds. But I have no idea whether case(i) and (ii) hold. I know that is the sum of the sum of eigenvalues and is the product of eigenvalues. So the answer is , ? Thanks.","A 2 \times 2 A^2=I A \ne I,-I \mbox{tr}(A) \det(A) 3 1,1 -1,-1 1,-1 tr(A) det(A) tr(A)=0 det(A)=-1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'trace']"
36,Are eigen spaces orthogonal?,Are eigen spaces orthogonal?,,Let $A$ be a $N$ x $N$ matrix which has $k < N$ distinct eigenvalues. Are eigenspaces corresponding to different eigenvalues orthogonal in general ? I know it is true if $A$ is normal matrix. But can't prove in general.,Let $A$ be a $N$ x $N$ matrix which has $k < N$ distinct eigenvalues. Are eigenspaces corresponding to different eigenvalues orthogonal in general ? I know it is true if $A$ is normal matrix. But can't prove in general.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
37,Are all rank-1 positive semidefinite matrices the result of an outer product $vv^T$?,Are all rank-1 positive semidefinite matrices the result of an outer product ?,vv^T,"If $v$ is a column vector in $\mathbb{R}^n$, then it is obvious that $V = vv^T$ is a rank-1 positive semidefinite matrix.  I am wondering if the converse is true: if $V$ is a square real rank-1 positive semidefinite matrix, can it be written in the form $vv^T$?","If $v$ is a column vector in $\mathbb{R}^n$, then it is obvious that $V = vv^T$ is a rank-1 positive semidefinite matrix.  I am wondering if the converse is true: if $V$ is a square real rank-1 positive semidefinite matrix, can it be written in the form $vv^T$?",,"['linear-algebra', 'matrices']"
38,Prove $\det(kA)=k^n\det A$ [closed],Prove  [closed],\det(kA)=k^n\det A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $A$ be an $n \times n$ matrix and $k$ be a scalar. Prove that $\det(kA)=k^n\det A$ . I really don't know where to start. Can someone give me a hint for this proof?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let be an matrix and be a scalar. Prove that . I really don't know where to start. Can someone give me a hint for this proof?",A n \times n k \det(kA)=k^n\det A,"['linear-algebra', 'matrices', 'determinant']"
39,Does the product of two invertible matrix remain invertible?,Does the product of two invertible matrix remain invertible?,,"If $A$ and $B$ are two invertible $5 \times 5$ matrices, does $B^{T}A$ remain invertible? I cannot find out is there any properties of invertible matrix to my question. Thank you!","If $A$ and $B$ are two invertible $5 \times 5$ matrices, does $B^{T}A$ remain invertible? I cannot find out is there any properties of invertible matrix to my question. Thank you!",,"['linear-algebra', 'matrices', 'inverse', 'products']"
40,"Is a matrix diagonalizable, if one of its eigenvalues is zero?","Is a matrix diagonalizable, if one of its eigenvalues is zero?",,"I checked weather the following matrix is diagonalizable. $$A=\begin{bmatrix} 4 & 0 & 4\\ 0 & 4 & 4\\ 4 & 4 & 8 \end{bmatrix}$$ And the corresponding eigenvalues were $0$, $4$, $12$. Now if we write the similar diagonal matrix to $A$ it would be, $$D=\begin{bmatrix} 0 & 0 & 0\\ 0 & 4 & 0\\ 0 & 0 & 12 \end{bmatrix}$$ which is theoretically not a diagonal matrix. Now is $A$ diagonalizable?","I checked weather the following matrix is diagonalizable. $$A=\begin{bmatrix} 4 & 0 & 4\\ 0 & 4 & 4\\ 4 & 4 & 8 \end{bmatrix}$$ And the corresponding eigenvalues were $0$, $4$, $12$. Now if we write the similar diagonal matrix to $A$ it would be, $$D=\begin{bmatrix} 0 & 0 & 0\\ 0 & 4 & 0\\ 0 & 0 & 12 \end{bmatrix}$$ which is theoretically not a diagonal matrix. Now is $A$ diagonalizable?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
41,How to prove polynomials with degree $n$ does not form a vector space?,How to prove polynomials with degree  does not form a vector space?,n,"This is one of my linear algebra problems: Prove that polynomials of degree $n$ does not (The professor made these words bold intentionally) form a vector space. From what I read, the set of polynomials of degree $n$ should be a vector space, because: There is an ""One"" and a ""Zero"" in this set; We can find inverse for addition and multiplication from this set; It follows all the axioms of addition. It follows all the axioms of scalar multiplication. Then can someone give me some hints to prove it does not form a vector space?","This is one of my linear algebra problems: Prove that polynomials of degree $n$ does not (The professor made these words bold intentionally) form a vector space. From what I read, the set of polynomials of degree $n$ should be a vector space, because: There is an ""One"" and a ""Zero"" in this set; We can find inverse for addition and multiplication from this set; It follows all the axioms of addition. It follows all the axioms of scalar multiplication. Then can someone give me some hints to prove it does not form a vector space?",,"['linear-algebra', 'polynomials', 'vector-spaces', 'field-theory', 'vectors']"
42,Is $y=mx+b$ linear?,Is  linear?,y=mx+b,"Consider $f(x) = mx+b$. Let $b\ne 0$ If $f$ is linear, $f(0)$ should yield $0$ $f(0) = m(0)+b = b$ Therefore $f(x)=mx+b$ is nonlinear. Question : Why is $y=mx+b$ called a "" linear equation ""? Remark: Consider $f(x,y) = y + -mx = b$ \begin{align} \alpha f(x,y) &= \alpha b \\ f(\alpha x,\alpha y) &= \alpha y +-\alpha mx = \alpha (y +-mx) =\alpha b \end{align} \begin{align} f(x+\beta,y+\beta) &= y + \beta+ - m (x +\beta) \\&= (y+-mx)+(\beta +-m\beta)\\ &= f(x,y)+ f(\beta,\beta)  \end{align} Therefore, $f(x,y) = y +-mx$ and $f(x,y) =b$ are linear. Question: Why is this not a contradiction?","Consider $f(x) = mx+b$. Let $b\ne 0$ If $f$ is linear, $f(0)$ should yield $0$ $f(0) = m(0)+b = b$ Therefore $f(x)=mx+b$ is nonlinear. Question : Why is $y=mx+b$ called a "" linear equation ""? Remark: Consider $f(x,y) = y + -mx = b$ \begin{align} \alpha f(x,y) &= \alpha b \\ f(\alpha x,\alpha y) &= \alpha y +-\alpha mx = \alpha (y +-mx) =\alpha b \end{align} \begin{align} f(x+\beta,y+\beta) &= y + \beta+ - m (x +\beta) \\&= (y+-mx)+(\beta +-m\beta)\\ &= f(x,y)+ f(\beta,\beta)  \end{align} Therefore, $f(x,y) = y +-mx$ and $f(x,y) =b$ are linear. Question: Why is this not a contradiction?",,"['linear-algebra', 'linear-transformations', 'terminology']"
43,Finding the projection matrix of $\mathbb R^3$ onto the plane $x-y-z=0$,Finding the projection matrix of  onto the plane,\mathbb R^3 x-y-z=0,"Find the matrix of the projection of $\mathbb{R}^3$ onto the plane $x-y-z = 0.$ I can find a normal unit vector of the plane, which is $\vec{n}=(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}})^T$ And then the vectors $\vec{u}=(1,1,0)^T, \ \vec{v} = (1,0,1)^T$ form a basis of $\mathbb{R}^3$. but why would the solution be $$A = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}?$$","Find the matrix of the projection of $\mathbb{R}^3$ onto the plane $x-y-z = 0.$ I can find a normal unit vector of the plane, which is $\vec{n}=(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}})^T$ And then the vectors $\vec{u}=(1,1,0)^T, \ \vec{v} = (1,0,1)^T$ form a basis of $\mathbb{R}^3$. but why would the solution be $$A = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}?$$",,"['linear-algebra', 'projection-matrices']"
44,"For given $n\times n$ matrix $A$ singular matrix, prove that $\operatorname{rank}(\operatorname{adj}A) \leq 1$","For given  matrix  singular matrix, prove that",n\times n A \operatorname{rank}(\operatorname{adj}A) \leq 1,"For given $n\times n$ matrix $A$ singular matrix, prove that $\operatorname{rank}(\operatorname{adj}A) \leq 1$ So from the properties of the adjugate matrix we know that $$ A \cdot \operatorname{adj}(A) = \operatorname{det}(A)\cdot I$$ Since $A$ is singular we know that $\operatorname{det}(A) = 0$, thus $$ A \cdot \operatorname{adj}(A) = 0$$ This is where I'm getting lost, I think I should say that for the above to happen one of the two, $A$ or $\operatorname{adj}(A)$ would have to be the $0$ matrix, but if $A = 0$ then $\operatorname{adj}(A) = 0$ for sure, which means I said nothing. A leading hint is needed.","For given $n\times n$ matrix $A$ singular matrix, prove that $\operatorname{rank}(\operatorname{adj}A) \leq 1$ So from the properties of the adjugate matrix we know that $$ A \cdot \operatorname{adj}(A) = \operatorname{det}(A)\cdot I$$ Since $A$ is singular we know that $\operatorname{det}(A) = 0$, thus $$ A \cdot \operatorname{adj}(A) = 0$$ This is where I'm getting lost, I think I should say that for the above to happen one of the two, $A$ or $\operatorname{adj}(A)$ would have to be the $0$ matrix, but if $A = 0$ then $\operatorname{adj}(A) = 0$ for sure, which means I said nothing. A leading hint is needed.",,"['linear-algebra', 'matrices']"
45,How to calculate the eigenvector corresponding to zero eigenvalue,How to calculate the eigenvector corresponding to zero eigenvalue,,"How can the eigenvector corresponding to zero eigenvalue be found out? I was trying with the following simple matrix in Matlab: $$A=\left[\begin{array}{ccc}1 & -2 & 3 \\ 2 & -3 & 4 \\ 3 & -4 & 5 \end{array}\right] \; .$$ In matlab computations, the matrix seemed nearly singular with one of the eigenvalues very close to zero (3e-15). That means the usual shifted inverse power methods for finding out the unit eigenvector corresponding to an eigenvalue won't work. But Matlab returns an eigenvector corresponding to 0. How? Basically, I would like to develop a program to compute this eigenvector given any singular matrix. What algorithm should I use? Edit: (1) Edited to reflect that the 'nearly singular' comment was corresponding to Matlab calculation. (2) Edited to specify the actual question.","How can the eigenvector corresponding to zero eigenvalue be found out? I was trying with the following simple matrix in Matlab: $$A=\left[\begin{array}{ccc}1 & -2 & 3 \\ 2 & -3 & 4 \\ 3 & -4 & 5 \end{array}\right] \; .$$ In matlab computations, the matrix seemed nearly singular with one of the eigenvalues very close to zero (3e-15). That means the usual shifted inverse power methods for finding out the unit eigenvector corresponding to an eigenvalue won't work. But Matlab returns an eigenvector corresponding to 0. How? Basically, I would like to develop a program to compute this eigenvector given any singular matrix. What algorithm should I use? Edit: (1) Edited to reflect that the 'nearly singular' comment was corresponding to Matlab calculation. (2) Edited to specify the actual question.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Are there finite dimensional matrices for which $e^{A+B}=e^Ae^B$ when $A$ and $B$ do not commute?,Are there finite dimensional matrices for which  when  and  do not commute?,e^{A+B}=e^Ae^B A B,"If we have square matrices $A$ and $B$ that commute (i.e. $AB=BA$ ), then we have $e^{A+B} = e^Ae^B$ . In general this isn't true without the condition that $A$ and $B$ commute. I would like to know if this is ""if and only if"", or whether $A$ commuting with $B$ is just a necessary condition. In other words, do there exist square matrices $A$ and $B$ such that $AB\ne BA$ but $e^{A+B} = e^A e^B$ ? Not that it really matters because this question already got a very good answer, but I don't think this should be considered a duplicate of Does $e^{a+b}=e^{a}e^{b}$ implies that $ab=ba$ for banach algebras? , because that question is about Banach algebras whereas mine is only about ordinary finite-dimensional matrices. Neither is it a duplicate of If $e^A$ and $e^B$ commute, do $A$ and $B$ commute for finite dimensional matrices? (of which Does $e^{a+b}=e^{a}e^{b}$ implies that $ab=ba$ for banach algebras? itself is closed as a duplicate), because that is clearly just a different question.","If we have square matrices and that commute (i.e. ), then we have . In general this isn't true without the condition that and commute. I would like to know if this is ""if and only if"", or whether commuting with is just a necessary condition. In other words, do there exist square matrices and such that but ? Not that it really matters because this question already got a very good answer, but I don't think this should be considered a duplicate of Does $e^{a+b}=e^{a}e^{b}$ implies that $ab=ba$ for banach algebras? , because that question is about Banach algebras whereas mine is only about ordinary finite-dimensional matrices. Neither is it a duplicate of If $e^A$ and $e^B$ commute, do $A$ and $B$ commute for finite dimensional matrices? (of which Does $e^{a+b}=e^{a}e^{b}$ implies that $ab=ba$ for banach algebras? itself is closed as a duplicate), because that is clearly just a different question.",A B AB=BA e^{A+B} = e^Ae^B A B A B A B AB\ne BA e^{A+B} = e^A e^B,"['linear-algebra', 'matrices', 'matrix-exponential']"
47,eigenvalues of a matrix $A$ plus $cI$ for some constant $c$,eigenvalues of a matrix  plus  for some constant,A cI c,"If $A$ is a $n \times n$ real matrix with eigenvalues $\lambda_1,\lambda_2,...\lambda_n$, how does one get the eigenvalues of the matrix $A$ + c$I$, where $I$ is the identity matrix and $c$ is a non-zero real constant? I tried to work out the characteristic polynomials, but I am wondering if there is a way to quickly get the eigenvalues.","If $A$ is a $n \times n$ real matrix with eigenvalues $\lambda_1,\lambda_2,...\lambda_n$, how does one get the eigenvalues of the matrix $A$ + c$I$, where $I$ is the identity matrix and $c$ is a non-zero real constant? I tried to work out the characteristic polynomials, but I am wondering if there is a way to quickly get the eigenvalues.",,['linear-algebra']
48,What exactly are pseudovectors and pseudoscalars? And where could I read about them?,What exactly are pseudovectors and pseudoscalars? And where could I read about them?,,"I can't find good information on the internet. In my mathematical physics class the definition of a vector was given as: That object with magnitude and direction which doesn't change under inversions. Pseudovectors do change. Scalars are that magnitude that don't change with inversions. Pseudoscalars do change. Inversions were loosely defined as inverting all components of a vector. The examples were given as: Say $\vec{A}=\vec{B}\times\vec{C}$; under the inversion it becomes $\vec{-B}\times\vec{-C}$ which is again, $\vec{A}$ which then has to be a pseudovector. Say $A=\vec{a}\cdot\vec{b}\times\vec{c}$ then under inversion $\vec{-a}\cdot\vec{-b}\times\vec{-c}$, which is $-A$ and has to be a pseudoscalar. I don't get it though. Why are inversions done on the individual vectors and not to the whole product? What really is an inversion? Where could I read more about this? Thanks.","I can't find good information on the internet. In my mathematical physics class the definition of a vector was given as: That object with magnitude and direction which doesn't change under inversions. Pseudovectors do change. Scalars are that magnitude that don't change with inversions. Pseudoscalars do change. Inversions were loosely defined as inverting all components of a vector. The examples were given as: Say $\vec{A}=\vec{B}\times\vec{C}$; under the inversion it becomes $\vec{-B}\times\vec{-C}$ which is again, $\vec{A}$ which then has to be a pseudovector. Say $A=\vec{a}\cdot\vec{b}\times\vec{c}$ then under inversion $\vec{-a}\cdot\vec{-b}\times\vec{-c}$, which is $-A$ and has to be a pseudoscalar. I don't get it though. Why are inversions done on the individual vectors and not to the whole product? What really is an inversion? Where could I read more about this? Thanks.",,"['linear-algebra', 'vector-analysis', 'mathematical-physics']"
49,Dimension of $\Bbb R$ over $\Bbb Q$ without cardinality argument. [duplicate],Dimension of  over  without cardinality argument. [duplicate],\Bbb R \Bbb Q,"This question already has answers here : Is there a quick proof as to why the vector space of $\mathbb{R}$ over $\mathbb{Q}$ is infinite-dimensional? (7 answers) Closed 9 years ago . I am looking for the easiest (elementary) proof that $\mathbb R$ is infinite dimensional as a $\mathbb Q$-vector space, without using cardinality. It should be understandable at highschool level. So I guess the question could be reformulated as: what is the easiest infinite family of reals that can be showed to be independent ? So far square roots of primes seems a good candidate, but the proof is still a little intricate, is it the easiest possible ? The goal of this is to show students that we can prove the result by different ways, and see that understanding cardinals is useful. But I still want the students to be able to understand the other proof.","This question already has answers here : Is there a quick proof as to why the vector space of $\mathbb{R}$ over $\mathbb{Q}$ is infinite-dimensional? (7 answers) Closed 9 years ago . I am looking for the easiest (elementary) proof that $\mathbb R$ is infinite dimensional as a $\mathbb Q$-vector space, without using cardinality. It should be understandable at highschool level. So I guess the question could be reformulated as: what is the easiest infinite family of reals that can be showed to be independent ? So far square roots of primes seems a good candidate, but the proof is still a little intricate, is it the easiest possible ? The goal of this is to show students that we can prove the result by different ways, and see that understanding cardinals is useful. But I still want the students to be able to understand the other proof.",,"['linear-algebra', 'vector-spaces', 'irrational-numbers', 'real-numbers']"
50,Is a projector matrix the inverse of itself?,Is a projector matrix the inverse of itself?,,"I want to confirm if a projector matrix is its own inverse. I have $x=Px$ and $Px=P^2x$, so premultiplying the second equation with $P^{-1}$ twice, I get $P^{-1}x=Px$ for all x, implying $P^{-1}=P$. Is this reasoning correct? So are all projection matrices orthogonal too?","I want to confirm if a projector matrix is its own inverse. I have $x=Px$ and $Px=P^2x$, so premultiplying the second equation with $P^{-1}$ twice, I get $P^{-1}x=Px$ for all x, implying $P^{-1}=P$. Is this reasoning correct? So are all projection matrices orthogonal too?",,['linear-algebra']
51,How to prove there exists a linear transformation?,How to prove there exists a linear transformation?,,"I've been given the following problem as homework: Prove that there exists a linear transformation $T: \mathbb{R}^2 \to \mathbb{R}^3$ such that $T(1,1) = (1,0,2)$ and $T(2,3) = (1,-1,4)$. Since it just says prove that one exists, I'm guessing I'm not supposed to actually identify the transformation. One thing I tried is showing that it holds under addition/multiplication in the sense of: 1) $T(x + y) = T(x) + T(y)$ 2) $T(cx) = cT(x)$ 3) $T((1,1) + (2,3)) = T(1,1) + T(2,3)$? But that's not necessarily true. I don't know how I'd accomplish this given my limited knowledge. What's the approach to solving a problem like this?","I've been given the following problem as homework: Prove that there exists a linear transformation $T: \mathbb{R}^2 \to \mathbb{R}^3$ such that $T(1,1) = (1,0,2)$ and $T(2,3) = (1,-1,4)$. Since it just says prove that one exists, I'm guessing I'm not supposed to actually identify the transformation. One thing I tried is showing that it holds under addition/multiplication in the sense of: 1) $T(x + y) = T(x) + T(y)$ 2) $T(cx) = cT(x)$ 3) $T((1,1) + (2,3)) = T(1,1) + T(2,3)$? But that's not necessarily true. I don't know how I'd accomplish this given my limited knowledge. What's the approach to solving a problem like this?",,['linear-algebra']
52,I don't understand why the inverse is this?,I don't understand why the inverse is this?,,"my question is related to matrix inverting and Hill cipher(you don't have to know what it is to help me) My teacher gave me an example. First we have a matrix (the key matrix) that multiplied by a vector of letters is another vector with the previous letters encrypted. To decrypt it you need the inverse of the key matrix and then multiply it by the vector of the encrypted letters, thus you get the vector of the decrypted letters (the real message) Well, this is the matrix, the key matrix that I have to multiply the vectors of letters by and get the encrypted message. $$ \left[   \begin{array}{ c c }      22&27&18 \\      18&28&5  \\      4&17&1   \end{array} \right] $$ However, when I try to invert it using multiple calculators on the internet and even programming languages (e.g. Ruby) I get a matrix (the inverted one) with a lot of 0.decimals numbers. Not whole numbers Why am I expecting to get whole numbers? Because my teacher gave me the inverse. This is it: $$ \left[   \begin{array}{ c c }      1&18&8 \\      2&8&11  \\      20&24&14   \end{array} \right] $$ I don't get something like this one. I know the inverse matrix is unique, but then who is wrong? Calculators bring on the same matrix, however, the matrix my teacher gave is the right matrix, because it can decrypt the encrypted message well, so it must be the good one. Not to forget to tell you, that the inverse is the matrix mod 29 . Any idea on how I could get to the same matrix as my teacher? Thanks a lot.","my question is related to matrix inverting and Hill cipher(you don't have to know what it is to help me) My teacher gave me an example. First we have a matrix (the key matrix) that multiplied by a vector of letters is another vector with the previous letters encrypted. To decrypt it you need the inverse of the key matrix and then multiply it by the vector of the encrypted letters, thus you get the vector of the decrypted letters (the real message) Well, this is the matrix, the key matrix that I have to multiply the vectors of letters by and get the encrypted message. $$ \left[   \begin{array}{ c c }      22&27&18 \\      18&28&5  \\      4&17&1   \end{array} \right] $$ However, when I try to invert it using multiple calculators on the internet and even programming languages (e.g. Ruby) I get a matrix (the inverted one) with a lot of 0.decimals numbers. Not whole numbers Why am I expecting to get whole numbers? Because my teacher gave me the inverse. This is it: $$ \left[   \begin{array}{ c c }      1&18&8 \\      2&8&11  \\      20&24&14   \end{array} \right] $$ I don't get something like this one. I know the inverse matrix is unique, but then who is wrong? Calculators bring on the same matrix, however, the matrix my teacher gave is the right matrix, because it can decrypt the encrypted message well, so it must be the good one. Not to forget to tell you, that the inverse is the matrix mod 29 . Any idea on how I could get to the same matrix as my teacher? Thanks a lot.",,"['linear-algebra', 'matrices', 'modular-arithmetic', 'inverse']"
53,Characteristic polynomial of companion matrix [duplicate],Characteristic polynomial of companion matrix [duplicate],,"This question already has answers here : The characteristic and minimal polynomial of a companion matrix (5 answers) Closed 10 years ago . I have a matrix in companion form, $$A=\begin{pmatrix}  0 & \cdots & 0& -a_{0} \\  1 & \cdots & 0 & -a_{1}\\ \vdots &\ddots & \vdots &\vdots \\ 0 &\cdots & 1 & -a_{n-1} \end{pmatrix}$$ where $A \in M_{n}$ . I want to prove by induction that the characteristic polynomial is $$p_{A}=t^{n}+a_{n-1}t^{n-1}+\cdots +a_{0}$$ The part that is confusing me is if we assume this hold for $A_{1}\in M_{n-1}$ how do we transform, or ""add"" to $A_{1}$ to get the n by n matrix $A\in M_{n}$ ? I hope this makes sense.","This question already has answers here : The characteristic and minimal polynomial of a companion matrix (5 answers) Closed 10 years ago . I have a matrix in companion form, where . I want to prove by induction that the characteristic polynomial is The part that is confusing me is if we assume this hold for how do we transform, or ""add"" to to get the n by n matrix ? I hope this makes sense.",A=\begin{pmatrix}  0 & \cdots & 0& -a_{0} \\  1 & \cdots & 0 & -a_{1}\\ \vdots &\ddots & \vdots &\vdots \\ 0 &\cdots & 1 & -a_{n-1} \end{pmatrix} A \in M_{n} p_{A}=t^{n}+a_{n-1}t^{n-1}+\cdots +a_{0} A_{1}\in M_{n-1} A_{1} A\in M_{n},"['linear-algebra', 'matrices', 'characteristic-polynomial', 'companion-matrices']"
54,How do you extend a basis?,How do you extend a basis?,,"I recently posted a question about this; now that I've acquired some new info, I have some follow up questions about extending a basis (I'm not too sure if this is actually the name for it, so my apologies if it isn't.) Let's say I have a set $\{(1,1,1)\}$ and wanted to add vectors such that the set is a basis of $\mathbb R^3$ . Tt's rather obvious that I could easily find a couple of vectors that would make the set linearly dependent, but I'm looking for a method that I could depend on. I've seen a couple of ways of doing this, neither of which I understand fully. The first is using dot products. Let's call the vectors I want to create $a,b \in \mathbb R$ . I want an $a$ and a $b$ such that $a \cdot (1,1,1) = 0$ and that $b \cdot (1,1,1) = 0$ . If I've understood things correctly, this is because if the dot product of two vectors is $0$ , then they are orthogonal. If $a$ and $b$ are both orthogonal to $(1,1,1)$ , then $\{(1,1,1), a, b)\}$ is linearly independent, and therefore a basis on $\mathbb R$$^3$ . Is that all true? If so, then I suppose I understand this solution conceptually, but I'm not sure how to go about it otherwise. The second method I've found is by creating a matrix $A$ out of my vectors, create a basis of the nullspace $N(A)$ , and then go from there. Honestly, I don't understand much of anything here. I know how to create a matrix from my 3 vectors (2 of which are unknown). I also understand that the nullspace of A is the set of vectors such that, if multiplied with $A$ , the product is the zero vector (if this is also wrong, please correct me.) I also understand that if my matrix is built of linearly independent vectors, the nullspace is simply the zero vector. However, I don't understand what it means to create a basis of the nullspace, and thus don't know how to carry on. The last thing that bothers me is, I don't see how these two methods are any more powerful/helpful than proving that having any linear combination of the vectors equaling zero implies that the scalars multiplying the vectors are also zero. Any help is greatly appreciated, thank you.","I recently posted a question about this; now that I've acquired some new info, I have some follow up questions about extending a basis (I'm not too sure if this is actually the name for it, so my apologies if it isn't.) Let's say I have a set and wanted to add vectors such that the set is a basis of . Tt's rather obvious that I could easily find a couple of vectors that would make the set linearly dependent, but I'm looking for a method that I could depend on. I've seen a couple of ways of doing this, neither of which I understand fully. The first is using dot products. Let's call the vectors I want to create . I want an and a such that and that . If I've understood things correctly, this is because if the dot product of two vectors is , then they are orthogonal. If and are both orthogonal to , then is linearly independent, and therefore a basis on . Is that all true? If so, then I suppose I understand this solution conceptually, but I'm not sure how to go about it otherwise. The second method I've found is by creating a matrix out of my vectors, create a basis of the nullspace , and then go from there. Honestly, I don't understand much of anything here. I know how to create a matrix from my 3 vectors (2 of which are unknown). I also understand that the nullspace of A is the set of vectors such that, if multiplied with , the product is the zero vector (if this is also wrong, please correct me.) I also understand that if my matrix is built of linearly independent vectors, the nullspace is simply the zero vector. However, I don't understand what it means to create a basis of the nullspace, and thus don't know how to carry on. The last thing that bothers me is, I don't see how these two methods are any more powerful/helpful than proving that having any linear combination of the vectors equaling zero implies that the scalars multiplying the vectors are also zero. Any help is greatly appreciated, thank you.","\{(1,1,1)\} \mathbb R^3 a,b \in \mathbb R a b a \cdot (1,1,1) = 0 b \cdot (1,1,1) = 0 0 a b (1,1,1) \{(1,1,1), a, b)\} \mathbb R^3 A N(A) A","['linear-algebra', 'vector-spaces']"
55,How to determine if a point lies on the same side of the plane that origin does?,How to determine if a point lies on the same side of the plane that origin does?,,"I'm puzzled by this problem - determine whether the point $A = (2, -1, 1)$ lies on the same side of the plane $5x-3y+z-18=0$  that the origin point $O$ = (0, 0, 0) does. How to approach this?","I'm puzzled by this problem - determine whether the point $A = (2, -1, 1)$ lies on the same side of the plane $5x-3y+z-18=0$  that the origin point $O$ = (0, 0, 0) does. How to approach this?",,"['linear-algebra', 'analytic-geometry']"
56,Why do we call a vector space in terms of vector space over some field?,Why do we call a vector space in terms of vector space over some field?,,"I am getting a bit confused with the terminology here. I understand that a field means some set of scalars like real numbers but why do we need a field for a vector space? Are not the numerical values used to define a vector are inherent properties of the vector space? Why do we term it like ""vector space over real field""? If not numbers, what are the other fields possible for a vector space because you obviously numbers to define the value of magnitude and direction of the vectors?","I am getting a bit confused with the terminology here. I understand that a field means some set of scalars like real numbers but why do we need a field for a vector space? Are not the numerical values used to define a vector are inherent properties of the vector space? Why do we term it like ""vector space over real field""? If not numbers, what are the other fields possible for a vector space because you obviously numbers to define the value of magnitude and direction of the vectors?",,"['linear-algebra', 'terminology']"
57,"For two vectors $a$ and $b$, why does $\cos(θ)$ equal the dot product of $a$ and $b$ divided by the product of the vectors' magnitudes?","For two vectors  and , why does  equal the dot product of  and  divided by the product of the vectors' magnitudes?",a b \cos(θ) a b,"While watching a video about dot products ( https://www.youtube.com/watch?v=WDdR5s0C4cY ), the following formula is presented for finding the angle between two vectors: For vectors $a$, and $b$,   $$\cos( \theta ) =  \frac{(a, b)}{  \|  a  \|   \| b \| }$$   where $(a,b)$ is the dot product of $a$ and $b$. How is this formula derived?","While watching a video about dot products ( https://www.youtube.com/watch?v=WDdR5s0C4cY ), the following formula is presented for finding the angle between two vectors: For vectors $a$, and $b$,   $$\cos( \theta ) =  \frac{(a, b)}{  \|  a  \|   \| b \| }$$   where $(a,b)$ is the dot product of $a$ and $b$. How is this formula derived?",,"['linear-algebra', 'vectors']"
58,"Prove that for a real matrix $A$, $\ker(A) = \ker(A^TA)$","Prove that for a real matrix ,",A \ker(A) = \ker(A^TA),"So clearly the kernel of $A$ is contained within the kernel of $A^TA$ , since $$\forall x\in \ker A\Rightarrow Ax=0\Rightarrow (A^TA)x=A^T(Ax) = A^T(0) = 0\Rightarrow x\in \ker (A^TA)$$ Now we need to show that the kernel of $A^TA$ is contained within the kernel of $A$ . So suppose we have an arbitrary $x \in \ker(A^TA)$ , then we have $(A^TA)x = 0$ . How can we show that $Ax=0$ ?","So clearly the kernel of is contained within the kernel of , since Now we need to show that the kernel of is contained within the kernel of . So suppose we have an arbitrary , then we have . How can we show that ?",A A^TA \forall x\in \ker A\Rightarrow Ax=0\Rightarrow (A^TA)x=A^T(Ax) = A^T(0) = 0\Rightarrow x\in \ker (A^TA) A^TA A x \in \ker(A^TA) (A^TA)x = 0 Ax=0,"['linear-algebra', 'matrices']"
59,how to find maximal linearly independent subsets,how to find maximal linearly independent subsets,,"Given a set of vectors, we can compute the number of independent vectors by calculating the rank of the set, but my question is how to find a maximal linearly independent subset. Thanks!","Given a set of vectors, we can compute the number of independent vectors by calculating the rank of the set, but my question is how to find a maximal linearly independent subset. Thanks!",,['linear-algebra']
60,How to prove the Pythagoras theorem using vectors,How to prove the Pythagoras theorem using vectors,,"I've got a question concerning how to proof the Pythagoras theorem using the following assumption: $x$ is perpendicular to $y$ (if and only if) $||x+y||^2 = ||x||^2 + ||y||^2$, where $x$ and $y$ are vectors. I have a basic understanding of linear algebra, however I'm a beginner with this. The question provides hints how to prove the above mentioned equation. Namely that I should use the properties of the dot product and the definition of the norm of a vector. Those being symmetry, scaling and distributivity as the dot product properties and the norm of a vector being the squared root of the dot product between the same vector. I was thinking about using the fact that if a vector is perpendicular to another vector the dot product between those vectors should be 0. But that is not provided as a hint so I'm not sure. I know the under lying thought behind it is the cosine rule for vectors, that being: $$x\cdot y = ||x||\,||y|| \cos(\theta)$$ If the angle between the two vectors is perpendicular you should use $\cos(\pi/2)$ which is $0$ and $||x||\cdot 0 = 0$ and $||y||\cdot 0 = 0$ with the vectors not necessarily being $0$. Thus $x\cdot y = 0$. How would I apply this to the equation I first mentioned to prove the Pythagoras theorem? I have a few more thoughts on how I could prove this but I'm not sure if they're correct.  I hope someone could point me in the right direction.","I've got a question concerning how to proof the Pythagoras theorem using the following assumption: $x$ is perpendicular to $y$ (if and only if) $||x+y||^2 = ||x||^2 + ||y||^2$, where $x$ and $y$ are vectors. I have a basic understanding of linear algebra, however I'm a beginner with this. The question provides hints how to prove the above mentioned equation. Namely that I should use the properties of the dot product and the definition of the norm of a vector. Those being symmetry, scaling and distributivity as the dot product properties and the norm of a vector being the squared root of the dot product between the same vector. I was thinking about using the fact that if a vector is perpendicular to another vector the dot product between those vectors should be 0. But that is not provided as a hint so I'm not sure. I know the under lying thought behind it is the cosine rule for vectors, that being: $$x\cdot y = ||x||\,||y|| \cos(\theta)$$ If the angle between the two vectors is perpendicular you should use $\cos(\pi/2)$ which is $0$ and $||x||\cdot 0 = 0$ and $||y||\cdot 0 = 0$ with the vectors not necessarily being $0$. Thus $x\cdot y = 0$. How would I apply this to the equation I first mentioned to prove the Pythagoras theorem? I have a few more thoughts on how I could prove this but I'm not sure if they're correct.  I hope someone could point me in the right direction.",,['linear-algebra']
61,Why do row operations not change the column rank?,Why do row operations not change the column rank?,,"From this question link , I got to know that row operation (row subtraction and row permutation) do change column space. But still it seems that it does not change the column rank. I am trying to prove that row rank == columns rank, but for that I need confirm the statement above. I am referring to this note Have any intuitive explanation or proof for that row operation does not change the column rank?","From this question link , I got to know that row operation (row subtraction and row permutation) do change column space. But still it seems that it does not change the column rank. I am trying to prove that row rank == columns rank, but for that I need confirm the statement above. I am referring to this note Have any intuitive explanation or proof for that row operation does not change the column rank?",,"['linear-algebra', 'matrix-rank']"
62,Sufficient and necessary conditions for matrix to be diagonalize or triangular,Sufficient and necessary conditions for matrix to be diagonalize or triangular,,There are many sufficient and necessary conditions that derive from the fact that a matrix can be diagonalize means it is similar to a diagonal matrix. Is there a list of all of conditions?,There are many sufficient and necessary conditions that derive from the fact that a matrix can be diagonalize means it is similar to a diagonal matrix. Is there a list of all of conditions?,,"['linear-algebra', 'matrices', 'diagonalization']"
63,Product of positive-definite matrices has positive trace,Product of positive-definite matrices has positive trace,,"In his paper ""On the existence of a connection with curvature zero"", Milnor makes the following claim.  Let $X$ and $Y$ be positive-definite $n \times n$ matrices.  Then the trace of $X Y$ is positive.  I can't figure out how to prove this.  Can someone help me?","In his paper ""On the existence of a connection with curvature zero"", Milnor makes the following claim.  Let $X$ and $Y$ be positive-definite $n \times n$ matrices.  Then the trace of $X Y$ is positive.  I can't figure out how to prove this.  Can someone help me?",,"['linear-algebra', 'matrices', 'trace']"
64,Are three matrices linearly independent and form a basis of $M_2(\mathbb R)$?,Are three matrices linearly independent and form a basis of ?,M_2(\mathbb R),"I know how to prove whether or not vectors are linearly independent, but can't apply the same thing to matrices it seems. Given three 2x2 matrices, for example: $$A = \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix}$$  $$B = \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix}$$  $$C = \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix}$$ I want to test whether or not these are linearly dependent. So with vectors I would do something like: $$ c_1A + c_2B + c_3C = 0$$ Where the $c_i$'s are some scalar constants, and prove that the only solution of that is when $$c_1 = c_2 = c_3 = 0$$ So how do I go about solving this: $$ c_1 \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix} + c_2 \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix} + c_3 \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix} = 0$$ Or I am going about this completely the wrong way? Any help would be hugely appreciated.","I know how to prove whether or not vectors are linearly independent, but can't apply the same thing to matrices it seems. Given three 2x2 matrices, for example: $$A = \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix}$$  $$B = \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix}$$  $$C = \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix}$$ I want to test whether or not these are linearly dependent. So with vectors I would do something like: $$ c_1A + c_2B + c_3C = 0$$ Where the $c_i$'s are some scalar constants, and prove that the only solution of that is when $$c_1 = c_2 = c_3 = 0$$ So how do I go about solving this: $$ c_1 \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix} + c_2 \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix} + c_3 \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix} = 0$$ Or I am going about this completely the wrong way? Any help would be hugely appreciated.",,['linear-algebra']
65,Problem in skew-symmetric matrix,Problem in skew-symmetric matrix,,"Let $A$ be a real skew-symmetric matrix. Prove that $I+A$ is non-singular, where $I$ is the identity matrix.","Let be a real skew-symmetric matrix. Prove that is non-singular, where is the identity matrix.",A I+A I,"['linear-algebra', 'matrices', 'skew-symmetric-matrices']"
66,Cayley Hamilton Theorem Intuition,Cayley Hamilton Theorem Intuition,,"Why should, intuitive (not a formal proof, just motivations ) be true that the square matrix satisfy its own characteristic equation?","Why should, intuitive (not a formal proof, just motivations ) be true that the square matrix satisfy its own characteristic equation?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'cayley-hamilton']"
67,Is the matrix square root uniformly continuous?,Is the matrix square root uniformly continuous?,,"Let $\operatorname{Psym}_n$ be the cone of symmetric positive-definite matrices of size $n \times n$. How to prove the positive square root function $\sqrt{\cdot}:\operatorname{Psym}_n \to \operatorname{Psym}_n$ is uniformly continuous? I am quite sure this is true, since on any compact ball this clearly holds, and far enough from the origin, I think the rate of change should decrease (analogous to the one-dimensional case where $(\sqrt{x})'=\frac{1}{2\sqrt{x}}$ tends to zero when $x \to \infty$). A naive approach is to try to use the mean value inequality: For that we need to show the norm $\|d(\sqrt{\cdot})_A\|$ is bounded for $\|A\|$ large enough. We know the derivative satisfies: $$d(\sqrt{\cdot})_A(B) \cdot \sqrt{A} + \sqrt{A} \cdot (\sqrt{\cdot})_A(B)=B$$ for every $B \in \operatorname{sym}_n$. Thus, $$ \|B\| \le \| d(\sqrt{\cdot})_A(B) \cdot \sqrt{A}\| +\| \sqrt{A} \cdot (\sqrt{\cdot})_A(B) \|\le 2 \| \sqrt{A}\| \| d(\sqrt{\cdot})_A(B)\|,$$ so we only get a bound from below : $$\|d(\sqrt{\cdot})_A\|_{op} \ge \frac{1}{2\|\sqrt{A}\|}$$","Let $\operatorname{Psym}_n$ be the cone of symmetric positive-definite matrices of size $n \times n$. How to prove the positive square root function $\sqrt{\cdot}:\operatorname{Psym}_n \to \operatorname{Psym}_n$ is uniformly continuous? I am quite sure this is true, since on any compact ball this clearly holds, and far enough from the origin, I think the rate of change should decrease (analogous to the one-dimensional case where $(\sqrt{x})'=\frac{1}{2\sqrt{x}}$ tends to zero when $x \to \infty$). A naive approach is to try to use the mean value inequality: For that we need to show the norm $\|d(\sqrt{\cdot})_A\|$ is bounded for $\|A\|$ large enough. We know the derivative satisfies: $$d(\sqrt{\cdot})_A(B) \cdot \sqrt{A} + \sqrt{A} \cdot (\sqrt{\cdot})_A(B)=B$$ for every $B \in \operatorname{sym}_n$. Thus, $$ \|B\| \le \| d(\sqrt{\cdot})_A(B) \cdot \sqrt{A}\| +\| \sqrt{A} \cdot (\sqrt{\cdot})_A(B) \|\le 2 \| \sqrt{A}\| \| d(\sqrt{\cdot})_A(B)\|,$$ so we only get a bound from below : $$\|d(\sqrt{\cdot})_A\|_{op} \ge \frac{1}{2\|\sqrt{A}\|}$$",,"['linear-algebra', 'matrices', 'matrix-calculus', 'uniform-continuity', 'positive-definite']"
68,Why is rotation about the y axis in $\mathbb{R^3}$ different from rotation about the x and y axis.,Why is rotation about the y axis in  different from rotation about the x and y axis.,\mathbb{R^3},In my textbook for a counterclockwise rotation about the x-axis we have $\begin{pmatrix}  1 & 0 & 0\\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta  \end{pmatrix}$ For rotation about the z-axis we have $\begin{pmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1  \end{pmatrix} $ . Now for rotation about the y-axis it's listed as $\begin{pmatrix} \cos\theta & 0 & \sin\theta \\ 0 & 1 & 0 \\ -\sin\theta & 0 & \cos\theta \end{pmatrix} $ . I can see that they changed it to the rotational matrix for a clockwise matrix but it says right infront of these 3 matrices that they are all for counterclockwise rotations so I'm not entirely sure what's going on.,In my textbook for a counterclockwise rotation about the x-axis we have For rotation about the z-axis we have . Now for rotation about the y-axis it's listed as . I can see that they changed it to the rotational matrix for a clockwise matrix but it says right infront of these 3 matrices that they are all for counterclockwise rotations so I'm not entirely sure what's going on.,"\begin{pmatrix} 
1 & 0 & 0\\
0 & \cos\theta & -\sin\theta \\
0 & \sin\theta & \cos\theta 
\end{pmatrix} \begin{pmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1 
\end{pmatrix}
 \begin{pmatrix}
\cos\theta & 0 & \sin\theta \\
0 & 1 & 0 \\
-\sin\theta & 0 & \cos\theta
\end{pmatrix} ",['linear-algebra']
69,The identity det(A) = exp(Tr(ln(A)) for A general,The identity det(A) = exp(Tr(ln(A)) for A general,,"I understand the proof of the identity in the title for $A$ Hermitian. One uses that any Hermitian matrix can be diagonalized as $A = X \Lambda X^{-1}$, such that $$ \det{A} = \prod_i \lambda_i, $$ and we have $$ \exp(Tr(\log(A)) = \exp(Tr(X\log\Lambda X^ {-1}) = \exp(\sum_i\log(\lambda_i)) = \prod_i \lambda_i. $$ However, is it possible to show the identity for $A$ not Hermitian? My motivation for this question is that in physics the identity is often used without it being clear that $A$ is Hermitian.","I understand the proof of the identity in the title for $A$ Hermitian. One uses that any Hermitian matrix can be diagonalized as $A = X \Lambda X^{-1}$, such that $$ \det{A} = \prod_i \lambda_i, $$ and we have $$ \exp(Tr(\log(A)) = \exp(Tr(X\log\Lambda X^ {-1}) = \exp(\sum_i\log(\lambda_i)) = \prod_i \lambda_i. $$ However, is it possible to show the identity for $A$ not Hermitian? My motivation for this question is that in physics the identity is often used without it being clear that $A$ is Hermitian.",,"['linear-algebra', 'matrices']"
70,Polar decomposition of real matrices,Polar decomposition of real matrices,,"Can we decompose real matrices in a way that is analogous to polar decomposition in the complex case? What I mean is: given an invertible real matrix $M$, can we always write: $$ M = OP, $$ maybe uniquely, where $O$ is orthogonal, and $P$ is symmetric positive-definite? Thanks.","Can we decompose real matrices in a way that is analogous to polar decomposition in the complex case? What I mean is: given an invertible real matrix $M$, can we always write: $$ M = OP, $$ maybe uniquely, where $O$ is orthogonal, and $P$ is symmetric positive-definite? Thanks.",,"['linear-algebra', 'matrix-decomposition']"
71,is it true every left inverse of a matrix is also right inverse of it?,is it true every left inverse of a matrix is also right inverse of it?,,"I am wondering that, consider there are $m$ linear equations with $n$ unknowns. We can represent it as $AX=B$. Let $L$ is the left inverse of $A$ therefore $LA=I$. Again from $AX=B$, we get $LAX=LB$ implies $X=LB$. Till this I have no problem but from $X=LB$, multiplying it by $A$ we get $AX=ALB$ implies $B=ALB$. So does it imply also $AL=I$ ?","I am wondering that, consider there are $m$ linear equations with $n$ unknowns. We can represent it as $AX=B$. Let $L$ is the left inverse of $A$ therefore $LA=I$. Again from $AX=B$, we get $LAX=LB$ implies $X=LB$. Till this I have no problem but from $X=LB$, multiplying it by $A$ we get $AX=ALB$ implies $B=ALB$. So does it imply also $AL=I$ ?",,"['linear-algebra', 'matrices', 'inverse']"
72,Proving a subset is a subspace of a Vector Space,Proving a subset is a subspace of a Vector Space,,"To prove a subset is a subspace of a vector space we have to prove that the same operations (closed under vector addition and closed under scalar multiplication) on the Vector space apply to the subset. Fine, I get this. But I am having trouble with the subspace tests. For example, if the question is: Let $W = \{(a,b,c)|a\geq b\}$ be a subset of the vector space $V$. Show that $W$ is a subspace of $V$. I can do upto this: Let $w$ belong to $V$, and $w_1 = (a_1,b_1,c_1)$ and $w_2 = (a_2,b_2,c_2)$ belong to $w$. We have to show that $w_1 + w_2$ is closed under vector addition. Therefore, $w_1 + w_2 = (a_1 + a_2,b_1 + b_2,c_1 +c_2)$. My question is, considering what the question is asking, basically how do I solve this? How to test whether it is closed under scalar multiplication when there are two vectors involved? How do I bring in $a\geq b$ in the answer? Thank you.","To prove a subset is a subspace of a vector space we have to prove that the same operations (closed under vector addition and closed under scalar multiplication) on the Vector space apply to the subset. Fine, I get this. But I am having trouble with the subspace tests. For example, if the question is: Let $W = \{(a,b,c)|a\geq b\}$ be a subset of the vector space $V$. Show that $W$ is a subspace of $V$. I can do upto this: Let $w$ belong to $V$, and $w_1 = (a_1,b_1,c_1)$ and $w_2 = (a_2,b_2,c_2)$ belong to $w$. We have to show that $w_1 + w_2$ is closed under vector addition. Therefore, $w_1 + w_2 = (a_1 + a_2,b_1 + b_2,c_1 +c_2)$. My question is, considering what the question is asking, basically how do I solve this? How to test whether it is closed under scalar multiplication when there are two vectors involved? How do I bring in $a\geq b$ in the answer? Thank you.",,"['linear-algebra', 'vector-spaces']"
73,Prove that if $\operatorname{rank}(T) = \operatorname{rank}(T^2)$ then $R(T) \cap N(T) = \{0\}$,Prove that if  then,\operatorname{rank}(T) = \operatorname{rank}(T^2) R(T) \cap N(T) = \{0\},"Let $V$ be a finite-dimensional vector space and let $T:V\to V$ be linear. Prove that if $\operatorname{rank}(T) = \operatorname{rank}(T^2)$ , then $R(T) \cap N(T) = \{0\}$ . I don't see this implication, at all. Please give hints and explain conceptually.","Let be a finite-dimensional vector space and let be linear. Prove that if , then . I don't see this implication, at all. Please give hints and explain conceptually.",V T:V\to V \operatorname{rank}(T) = \operatorname{rank}(T^2) R(T) \cap N(T) = \{0\},['linear-algebra']
74,What are the requirements for a rotation matrix?,What are the requirements for a rotation matrix?,,"Generally speaking, what are the necessary and sufficient properties of a matrix to make it a rotation matrix ? Is det(A) = 1 enough?","Generally speaking, what are the necessary and sufficient properties of a matrix to make it a rotation matrix ? Is det(A) = 1 enough?",,"['linear-algebra', 'matrices', 'rigid-transformation']"
75,On the generators of the Modular Group,On the generators of the Modular Group,,"The modular group is the group $G$ consisting of all linear fractional transformations $\phi$ of the form $$\phi(z)=\frac{az+b}{cz+d}$$ where $a,b,c,d$ are integers and $ad-bc=1$. I have read that $G$ is generated by the transformations $\tau(z)=z+1$ and $\sigma(z)=-1/z$. Is there an easy way to prove this? In particular, is there a proof that uses the relation between linear fractional transformations and matrices? Any good reference would be helpful. Thank you, Malik","The modular group is the group $G$ consisting of all linear fractional transformations $\phi$ of the form $$\phi(z)=\frac{az+b}{cz+d}$$ where $a,b,c,d$ are integers and $ad-bc=1$. I have read that $G$ is generated by the transformations $\tau(z)=z+1$ and $\sigma(z)=-1/z$. Is there an easy way to prove this? In particular, is there a proof that uses the relation between linear fractional transformations and matrices? Any good reference would be helpful. Thank you, Malik",,"['linear-algebra', 'group-theory']"
76,Derivative as a matrix: $\mathbf{D}=\dfrac{\mathrm{d}}{\mathrm{d}x}$,Derivative as a matrix:,\mathbf{D}=\dfrac{\mathrm{d}}{\mathrm{d}x},"I have a strange question, it is possible to consider the derivative as a matrix? (Both are linear transformation technically). I thought about this example, since $1, x,x^2,...,x^n$ can be thought of as the basis of a vector space, I can consider a polynomial as a vector of its coefficients: Let $p_n(x)=a_0+a_1 x+a_2 x^2+...+a_n x^n$ Its derivative is $p'_n(x)=\dfrac{\mathrm{d}}{\mathrm{d}x}p_n(x)=a_1+2a_2 x+...+n a_n x^{n-1}$ (and so far nothing new) But if I consider $p_n(x)$ as a vector of his coefficients: $(a_0,a_1,...,a_n)$ and I want to create a matrix that transforms $p_n(x)$ into $p'_n(x)$ I have: $$\begin{pmatrix}0&1&0&\cdots&0\\ 0&0&2&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&0\\ 0&0&0&\cdots&n\\ 0&0&0&\cdots&0 \end{pmatrix}\begin{pmatrix}a_0\\ a_1\\ a_2\\ \vdots\\ a_n\end{pmatrix}=\begin{pmatrix}a_1\\ 2a_2\\\vdots\\ n a_n\\ 0\\ \end{pmatrix}$$ So technically $\mathbf{D}=\begin{pmatrix}0&1&0&\cdots&0\\ 0&0&2&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&0\\ 0&0&0&\cdots&n\\ 0&0&0&\cdots&0 \end{pmatrix}$ represents the derivative. After this I tried to do some operations on it, and it came out that: $\mathbf{D}^n$ represents $\dfrac{\mathrm{d}^n}{\mathrm{d}x^n}$ $\det(\mathbf{D})=0$ , this means that $\mathbf{D}^{-1}$ is not defined (I interpreted this fact as the fact that the inverse operation of the derivative is the integral, which in general is not unique since there are infinitely many that vary for arbitrary constants, EVEN IF it is possible to calculate the pseudoinverse and it gives the integral) $\text{trace}(\mathbf{D})=0$ (I don't know how to interpret that), idem for $\mathbf{D}^{\top}$ In general, now I was working on a polynomial, so the dimension of the matrix is $n\times n$ with rank $n-1$ , but for a general function I suppose it is $""\infty\times\infty""$ (I don't know if it can be defined the rank for an infinite matrix) $\exp(\mathbf{D})$ gives the upper Pascal Matrix $\mathbf{D}=\text{diag}(1,1,2,...,n!)^{-1}\begin{pmatrix}0&1&0&\cdots&0\\ 0&0&1&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&0\\ 0&0&0&\cdots&1\\ 0&0&0&\cdots&0 \end{pmatrix}\text{diag}(1,1,2,...,n!)$ Are these things correct? I'm curious to see if it also has applications for partial derivatives or fractional calculus. (I tried to search on the internet ""derivative as a matrix"" but the main result was the Jacobian, so tell me if this has a name)","I have a strange question, it is possible to consider the derivative as a matrix? (Both are linear transformation technically). I thought about this example, since can be thought of as the basis of a vector space, I can consider a polynomial as a vector of its coefficients: Let Its derivative is (and so far nothing new) But if I consider as a vector of his coefficients: and I want to create a matrix that transforms into I have: So technically represents the derivative. After this I tried to do some operations on it, and it came out that: represents , this means that is not defined (I interpreted this fact as the fact that the inverse operation of the derivative is the integral, which in general is not unique since there are infinitely many that vary for arbitrary constants, EVEN IF it is possible to calculate the pseudoinverse and it gives the integral) (I don't know how to interpret that), idem for In general, now I was working on a polynomial, so the dimension of the matrix is with rank , but for a general function I suppose it is (I don't know if it can be defined the rank for an infinite matrix) gives the upper Pascal Matrix Are these things correct? I'm curious to see if it also has applications for partial derivatives or fractional calculus. (I tried to search on the internet ""derivative as a matrix"" but the main result was the Jacobian, so tell me if this has a name)","1, x,x^2,...,x^n p_n(x)=a_0+a_1 x+a_2 x^2+...+a_n x^n p'_n(x)=\dfrac{\mathrm{d}}{\mathrm{d}x}p_n(x)=a_1+2a_2 x+...+n a_n x^{n-1} p_n(x) (a_0,a_1,...,a_n) p_n(x) p'_n(x) \begin{pmatrix}0&1&0&\cdots&0\\
0&0&2&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&n\\
0&0&0&\cdots&0
\end{pmatrix}\begin{pmatrix}a_0\\ a_1\\ a_2\\ \vdots\\ a_n\end{pmatrix}=\begin{pmatrix}a_1\\ 2a_2\\\vdots\\ n a_n\\ 0\\ \end{pmatrix} \mathbf{D}=\begin{pmatrix}0&1&0&\cdots&0\\
0&0&2&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&n\\
0&0&0&\cdots&0
\end{pmatrix} \mathbf{D}^n \dfrac{\mathrm{d}^n}{\mathrm{d}x^n} \det(\mathbf{D})=0 \mathbf{D}^{-1} \text{trace}(\mathbf{D})=0 \mathbf{D}^{\top} n\times n n-1 ""\infty\times\infty"" \exp(\mathbf{D}) \mathbf{D}=\text{diag}(1,1,2,...,n!)^{-1}\begin{pmatrix}0&1&0&\cdots&0\\
0&0&1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&0\\
0&0&0&\cdots&1\\
0&0&0&\cdots&0
\end{pmatrix}\text{diag}(1,1,2,...,n!)","['linear-algebra', 'matrices', 'derivatives', 'polynomials', 'infinite-matrices']"
77,Sum of vector subspaces,Sum of vector subspaces,,"I am trying to self study linear algebra and am stuck on a problem. It comes from Axler's Linear Algebra done right example 1.38 and I don't understand the solution that I could find online. Suppose that $U=\{(x,x,y,y)\in F^4 : x,y \in F\}$ and $W=\{(x,x,x,y) \in F^4 : x,y \in F\}$, show that $U+W=\{(x,x,y,z) \in F^4 : x,y,z \in F\}$. Its stated that the sum of vector subspaces is the set of all possible sums of the elements but how does $x+y=y$ and $y+y=z$","I am trying to self study linear algebra and am stuck on a problem. It comes from Axler's Linear Algebra done right example 1.38 and I don't understand the solution that I could find online. Suppose that $U=\{(x,x,y,y)\in F^4 : x,y \in F\}$ and $W=\{(x,x,x,y) \in F^4 : x,y \in F\}$, show that $U+W=\{(x,x,y,z) \in F^4 : x,y,z \in F\}$. Its stated that the sum of vector subspaces is the set of all possible sums of the elements but how does $x+y=y$ and $y+y=z$",,['linear-algebra']
78,Can a Matrix with positive entries have a negative eigenvalue?,Can a Matrix with positive entries have a negative eigenvalue?,,"It seems intuitive to me that the answer is no, but I can't prove it.","It seems intuitive to me that the answer is no, but I can't prove it.",,"['linear-algebra', 'matrices']"
79,what does the set containing only the zero vector actually span?,what does the set containing only the zero vector actually span?,,"I apologize if this sounds stupid but I am struggling to grasp the following concept. I understand that the span of the empty set is the zero vector. However, what does the set only containing the zero vector span? The zero vector as well? Also, are the following two phrases logically equivalent, ""The span of the empty set is the zero vector"" and ""The empty set spans the zero vector"". I believe they do but I don't want to make any assumptions since I'm not entirely sure myself. Thank you.","I apologize if this sounds stupid but I am struggling to grasp the following concept. I understand that the span of the empty set is the zero vector. However, what does the set only containing the zero vector span? The zero vector as well? Also, are the following two phrases logically equivalent, ""The span of the empty set is the zero vector"" and ""The empty set spans the zero vector"". I believe they do but I don't want to make any assumptions since I'm not entirely sure myself. Thank you.",,"['linear-algebra', 'span']"
80,Differentiation with respect to a matrix (residual sum of squares)?,Differentiation with respect to a matrix (residual sum of squares)?,,"I've never heard of differentiating with respect to a matrix. Let $\mathbf{y}$ be a $N \times 1$ vector, $\mathbf{X}$ be a $N \times p$ matrix, and $\beta$ be a $p \times 1$ vector. Then the residual sum of squares is defined by $$\text{RSS}(\beta) = \left(\mathbf{y}-\mathbf{X}\beta\right)^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)\text{.}$$ The Elements of Statistical Learning , 2nd ed., p. 45, states that when we differentiate this with respect to $\beta$, we get $$\begin{align} &\dfrac{\partial\text{RSS}}{\partial \beta} = -2\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right) \\ &\dfrac{\partial^2\text{RSS}}{\partial \beta\text{ }\partial \beta^{T}} = 2\mathbf{X}^{T}\mathbf{X}\text{.} \end{align}$$ I mean, I could look at $\mathbf{y}$ and $\mathbf{X}$ as ""constants"" and $\beta$ as a variable, but it's unclear to me where the $-2$ in $\dfrac{\partial\text{RSS}}{\partial \beta}$ comes from, and why we would use $\beta^T$ for the second partial. Any textbooks that cover this topic would be appreciated as well. Side note : this is not homework. Please note that I graduated with an undergrad degree only, so assume that I've seen undergraduate real analysis, abstract algebra, and linear algebra for my pure mathematics background.","I've never heard of differentiating with respect to a matrix. Let $\mathbf{y}$ be a $N \times 1$ vector, $\mathbf{X}$ be a $N \times p$ matrix, and $\beta$ be a $p \times 1$ vector. Then the residual sum of squares is defined by $$\text{RSS}(\beta) = \left(\mathbf{y}-\mathbf{X}\beta\right)^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)\text{.}$$ The Elements of Statistical Learning , 2nd ed., p. 45, states that when we differentiate this with respect to $\beta$, we get $$\begin{align} &\dfrac{\partial\text{RSS}}{\partial \beta} = -2\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right) \\ &\dfrac{\partial^2\text{RSS}}{\partial \beta\text{ }\partial \beta^{T}} = 2\mathbf{X}^{T}\mathbf{X}\text{.} \end{align}$$ I mean, I could look at $\mathbf{y}$ and $\mathbf{X}$ as ""constants"" and $\beta$ as a variable, but it's unclear to me where the $-2$ in $\dfrac{\partial\text{RSS}}{\partial \beta}$ comes from, and why we would use $\beta^T$ for the second partial. Any textbooks that cover this topic would be appreciated as well. Side note : this is not homework. Please note that I graduated with an undergrad degree only, so assume that I've seen undergraduate real analysis, abstract algebra, and linear algebra for my pure mathematics background.",,"['linear-algebra', 'matrices', 'matrix-calculus']"
81,Determinant of $4\times4$ Matrix,Determinant of  Matrix,4\times4,"I tried to solve for a $4 \times 4$ matrix, but I'm unsure if I did this properly, can anyone tell me if I did this correct? Or if there were any mistakes where at? Also, I know this is an inefficient method for finding the determinant, however I want to get practice with solving like so: $$A= \begin{bmatrix} 2 & 4 & 0 & 1 \\ 0 & 8 & 0 & 2 \\ 0 & 3 & 0 & 5 \\ 1 & 2 & 1 & 1 \end{bmatrix} $$ $$\begin{align*} \det(A)&=2 \begin{vmatrix} 8 & 0 & 2 \\ 3 & 0 & 5 \\ 2 & 1 & 1 \end{vmatrix} -4 \begin{vmatrix} 0 & 0 & 2 \\ 0 & 0 & 5 \\ 1 & 1 & 1 \end{vmatrix} +0 -1 \begin{vmatrix} 0 & 8 & 0 \\ 0 & 3 & 0 \\ 1 & 2 & 1 \end{vmatrix}\\[0.3in] &=2\left(8 \begin{vmatrix} 0 & 5 \\ 1 & 1 \end{vmatrix}-0+2 \begin{vmatrix} 3 & 0\\ 2 & 1 \end{vmatrix}\right)\\[0.1in] &\quad{}-4\left(0-0+2 \begin{vmatrix} 0 & 0\\ 1 & 1 \end{vmatrix}\right)\\[0.1in] &\quad{}+0\\[0.1in] &\quad{}-1\left(0-8 \begin{vmatrix} 0 & 0\\ 1 & 1 \end{vmatrix}+0\right)\\[0.3in] &=2(8(0-5)-0+2(3-0))\\[0.1in] &\quad{}-4(0-0+2(0))\\[0.1in] &\quad{}+0\\[0.1in] &\quad{}-1(0-8(0)+0)\\[0.3in] &= 2(8(-5)-0+2(3))\\[0.3in] &=2(-45+6)\\[0.3in] &=2(-39)\\[0.3in] &=-78 \end{align*}$$ Sorry for the long post, I tried to make the readability easy for everyone.","I tried to solve for a $4 \times 4$ matrix, but I'm unsure if I did this properly, can anyone tell me if I did this correct? Or if there were any mistakes where at? Also, I know this is an inefficient method for finding the determinant, however I want to get practice with solving like so: $$A= \begin{bmatrix} 2 & 4 & 0 & 1 \\ 0 & 8 & 0 & 2 \\ 0 & 3 & 0 & 5 \\ 1 & 2 & 1 & 1 \end{bmatrix} $$ $$\begin{align*} \det(A)&=2 \begin{vmatrix} 8 & 0 & 2 \\ 3 & 0 & 5 \\ 2 & 1 & 1 \end{vmatrix} -4 \begin{vmatrix} 0 & 0 & 2 \\ 0 & 0 & 5 \\ 1 & 1 & 1 \end{vmatrix} +0 -1 \begin{vmatrix} 0 & 8 & 0 \\ 0 & 3 & 0 \\ 1 & 2 & 1 \end{vmatrix}\\[0.3in] &=2\left(8 \begin{vmatrix} 0 & 5 \\ 1 & 1 \end{vmatrix}-0+2 \begin{vmatrix} 3 & 0\\ 2 & 1 \end{vmatrix}\right)\\[0.1in] &\quad{}-4\left(0-0+2 \begin{vmatrix} 0 & 0\\ 1 & 1 \end{vmatrix}\right)\\[0.1in] &\quad{}+0\\[0.1in] &\quad{}-1\left(0-8 \begin{vmatrix} 0 & 0\\ 1 & 1 \end{vmatrix}+0\right)\\[0.3in] &=2(8(0-5)-0+2(3-0))\\[0.1in] &\quad{}-4(0-0+2(0))\\[0.1in] &\quad{}+0\\[0.1in] &\quad{}-1(0-8(0)+0)\\[0.3in] &= 2(8(-5)-0+2(3))\\[0.3in] &=2(-45+6)\\[0.3in] &=2(-39)\\[0.3in] &=-78 \end{align*}$$ Sorry for the long post, I tried to make the readability easy for everyone.",,"['linear-algebra', 'matrices', 'determinant']"
82,Relating Linear Independence to Affine Independence,Relating Linear Independence to Affine Independence,,"Question: (From an Introduction to Convex Polytopes) Let $(x_{1},...,x_{n})$ be an $n$-family of points from $\mathbb{R}^d$, where $x_{i} = (\alpha_{1i},...,\alpha_{di})$, and $\bar{x_{i}} =(1,\alpha_{1i},...,\alpha_{di})$, where $i=1,...,n$. Show that the $n$-family $(x_{1},...,x_{n})$ is affinely independent if and only if the $n$-family $(\bar{x_{1}},...,\bar{x_{n}})$ of vectors from $\mathbb{R}^{d+1}$ is linearly independent. - Here is what I have so far, it is mostly just writing out definitions, if you can give me some hints towards how I can start the problem that would be great. $(\Rightarrow)$ Assume that for $x_{i} = (\alpha_{1i},...,\alpha_{di})$, the $n$-family $(x_{1},...,x_{n})$ is affinely independent. Then, a linear combination $\lambda_{1}x_{1} + ... + \lambda_{n}x_{n} = 0$ can only equal the zero vector when $\lambda_{1} + ... + \lambda_{n} = 0$. An equivalent characterization of affine independence is that the $(n-1)$-families $(x_{1}-x_{i},...,x_{i-1}-x_{i},x_{i+1}-x_{i},...,x_{n}-x_{i})$ are linearly independent. We want to prove that for $\bar{x_{i}}=(1,\alpha_{1i},...,\alpha_{di})$, the $n$-family $(\bar{x}_{1},...,\bar{x}_{n})$ of vectors from $\mathbb{R}^{d+1}$ is linearly independent.","Question: (From an Introduction to Convex Polytopes) Let $(x_{1},...,x_{n})$ be an $n$-family of points from $\mathbb{R}^d$, where $x_{i} = (\alpha_{1i},...,\alpha_{di})$, and $\bar{x_{i}} =(1,\alpha_{1i},...,\alpha_{di})$, where $i=1,...,n$. Show that the $n$-family $(x_{1},...,x_{n})$ is affinely independent if and only if the $n$-family $(\bar{x_{1}},...,\bar{x_{n}})$ of vectors from $\mathbb{R}^{d+1}$ is linearly independent. - Here is what I have so far, it is mostly just writing out definitions, if you can give me some hints towards how I can start the problem that would be great. $(\Rightarrow)$ Assume that for $x_{i} = (\alpha_{1i},...,\alpha_{di})$, the $n$-family $(x_{1},...,x_{n})$ is affinely independent. Then, a linear combination $\lambda_{1}x_{1} + ... + \lambda_{n}x_{n} = 0$ can only equal the zero vector when $\lambda_{1} + ... + \lambda_{n} = 0$. An equivalent characterization of affine independence is that the $(n-1)$-families $(x_{1}-x_{i},...,x_{i-1}-x_{i},x_{i+1}-x_{i},...,x_{n}-x_{i})$ are linearly independent. We want to prove that for $\bar{x_{i}}=(1,\alpha_{1i},...,\alpha_{di})$, the $n$-family $(\bar{x}_{1},...,\bar{x}_{n})$ of vectors from $\mathbb{R}^{d+1}$ is linearly independent.",,['linear-algebra']
83,Is there a formula for the determinant of the wedge product of two matrices?,Is there a formula for the determinant of the wedge product of two matrices?,,I was going over the Wikipedia page for exterior products of vector spaces and we can define the determinant as the coefficient of the exterior product of vectors with respect to the standard basis when the vectors are elements in $\mathbb{R}^n$.  I was wondering if there was a way to deduce the formula for the determinant of the exterior (wedge) product of two matrices from this definition. In particular let $V$ be a finite vector space and let $\wedge^k V$ be the $k$-th exterior power of $V$ that is $T^k(V)/A^k(V)$ where $A(V)$ is the ideal generated by all $v \otimes v$ for $v \in V$ and $T^k(V) = V \otimes V \otimes \cdots \otimes V$ is tensor product of $k$ vector spaces. Let $M$ be a square $m\times m$ matrix.  Is there a known formula for $\det(M \wedge M)?$ I was thinking there must be some nice formula like $\det(M \wedge M) = \det(M)\det(M)$ but I have a feeling this does not generalize to higher powers of wedge products.,I was going over the Wikipedia page for exterior products of vector spaces and we can define the determinant as the coefficient of the exterior product of vectors with respect to the standard basis when the vectors are elements in $\mathbb{R}^n$.  I was wondering if there was a way to deduce the formula for the determinant of the exterior (wedge) product of two matrices from this definition. In particular let $V$ be a finite vector space and let $\wedge^k V$ be the $k$-th exterior power of $V$ that is $T^k(V)/A^k(V)$ where $A(V)$ is the ideal generated by all $v \otimes v$ for $v \in V$ and $T^k(V) = V \otimes V \otimes \cdots \otimes V$ is tensor product of $k$ vector spaces. Let $M$ be a square $m\times m$ matrix.  Is there a known formula for $\det(M \wedge M)?$ I was thinking there must be some nice formula like $\det(M \wedge M) = \det(M)\det(M)$ but I have a feeling this does not generalize to higher powers of wedge products.,,"['linear-algebra', 'abstract-algebra', 'determinant', 'exterior-algebra']"
84,Are complex numbers two dimensional or one dimensional?,Are complex numbers two dimensional or one dimensional?,,"Complex numbers are represented as: z = x + yi This gives the impression that complex numbers are a real component plus an imaginary component. However, when doing math with complex numbers, they are represented as 2-D vectors like in this picture: Complex Vector Are complex numbers one or two dimensional? If they are one dimensional, then why do we do math with them and represent them as vectors? If they are two dimensional, then why are they represented as one number: ""x + yi"" instead of a coordinate pair: ""(x, yi)"" I know this has been asked a zillion times, but most of the answers I've found online don't explain the subject well. The best explanation I've found so far was here on Quora: https://www.quora.com/Are-complex-numbers-2-dimensional","Complex numbers are represented as: z = x + yi This gives the impression that complex numbers are a real component plus an imaginary component. However, when doing math with complex numbers, they are represented as 2-D vectors like in this picture: Complex Vector Are complex numbers one or two dimensional? If they are one dimensional, then why do we do math with them and represent them as vectors? If they are two dimensional, then why are they represented as one number: ""x + yi"" instead of a coordinate pair: ""(x, yi)"" I know this has been asked a zillion times, but most of the answers I've found online don't explain the subject well. The best explanation I've found so far was here on Quora: https://www.quora.com/Are-complex-numbers-2-dimensional",,"['linear-algebra', 'complex-numbers']"
85,Why is $v/\|v\|$ not a unit vector?,Why is  not a unit vector?,v/\|v\|,"I have a homework question that is seriously stumping me. It was a true/false statement. The statement is ""If $v$ is any vector in an inner product space $V$, then $v/\|v\|$ is a unit vector"" and according to my professor, the statement is false. Isn't dividing a vector by its length the very definition of normalizing a vector? Edit: Thanks everyone! I didn't even consider the null vector.","I have a homework question that is seriously stumping me. It was a true/false statement. The statement is ""If $v$ is any vector in an inner product space $V$, then $v/\|v\|$ is a unit vector"" and according to my professor, the statement is false. Isn't dividing a vector by its length the very definition of normalizing a vector? Edit: Thanks everyone! I didn't even consider the null vector.",,"['linear-algebra', 'vectors']"
86,Dimension of vector space of matrices with zero row and column sum.,Dimension of vector space of matrices with zero row and column sum.,,Let $V(\mathbb{R})$ be the vector space of $m\times  n$ real  matrices such that each row sum and each column sum is zero. What is the dimension of $V(\mathbb{R})$? I know by General thinking that its dimension is $(m-1)(n-1)$. But I don't know what is the method to find its dimension. Please tell me how to think about its dimension. Thanks a lot.,Let $V(\mathbb{R})$ be the vector space of $m\times  n$ real  matrices such that each row sum and each column sum is zero. What is the dimension of $V(\mathbb{R})$? I know by General thinking that its dimension is $(m-1)(n-1)$. But I don't know what is the method to find its dimension. Please tell me how to think about its dimension. Thanks a lot.,,['linear-algebra']
87,Show that trace is a unique linear functional,Show that trace is a unique linear functional,,"If $W=M_{n\times n}(\mathbb F)$ and $f$ is a linear functional on $W$ such that $f(AB)=f(BA)\;\forall A,B\in W$ , and $f(I)=n$ , then $f$ is the trace function. I have tried to generate useful matrices that can be represented in the form $AB-BA$ for some $A$ and $B$ . My idea was to generate a basis for $W$ of the form $AB-BA$ plus the identity.","If and is a linear functional on such that , and , then is the trace function. I have tried to generate useful matrices that can be represented in the form for some and . My idea was to generate a basis for of the form plus the identity.","W=M_{n\times n}(\mathbb F) f W f(AB)=f(BA)\;\forall A,B\in W f(I)=n f AB-BA A B W AB-BA","['linear-algebra', 'matrices', 'trace']"
88,Fourier Transform: Understanding change of basis property with ideas from linear algebra,Fourier Transform: Understanding change of basis property with ideas from linear algebra,,"The notion of Fourier transform was always a little bit mysterious to me and recently I was introduced to functional analysis. I am a beginner in this field but still I am almost seeing that the Fourier transform can be viewed as a change of basis in a space of functions. I read the following article here which tries to build an intuition: https://sites.google.com/site/butwhymath/fourier-analysis/the-fourier-transform Now, I can see that the Fourier and Inverse Fourier tranforms are projecting and projecting back a function $f(x)$ onto and from the basis of complex exponentials, $e^{i2\pi sx}$, respectively: $$ F(s) = \int_{-\infty}^{\infty}f(x)e^{-i2\pi sx}dx$$ $$ f(x) = \int_{-\infty}^{\infty}F(s)e^{i2\pi sx}ds$$ Here are my questions about this view to make it more clear: 1)If I understand correctly, this operation is akin to regular linear algebra change of basis operations $a=Mb$ and $b=M^{-1}a$. Roughly, in this case $M$ is a matrix of uncountable many rows and columns where each row is $e^{-i2\pi sx}$, a function of $x$, and similarly $M^{-1}$ has rows as $e^{i2\pi sx}$, functions of $s$. Is this interpretation correct? 2)I don't have the exact rigour for this but intuitively think, if 1) is a correct interpretation then we should obtain from the ""infinite dimensional"" matrix multiplication $MM^{-1}$ something which resembles an infinite dimensional identity matrix. To test that, I built the inner product where $s$ is held fixed and equal in both terms from two objects $M$ and $M^{-1}$, which should correspond to a ""diagonal"" element of $MM^{-1}$: $\int_{-\infty}^{\infty}e^{i2\pi sx}e^{-i2\pi sx}dx = \int_{-\infty}^{\infty}e^{0}dx=\infty$. So this is not $1$ as expected from an identity matrix. What is the reason of that?","The notion of Fourier transform was always a little bit mysterious to me and recently I was introduced to functional analysis. I am a beginner in this field but still I am almost seeing that the Fourier transform can be viewed as a change of basis in a space of functions. I read the following article here which tries to build an intuition: https://sites.google.com/site/butwhymath/fourier-analysis/the-fourier-transform Now, I can see that the Fourier and Inverse Fourier tranforms are projecting and projecting back a function $f(x)$ onto and from the basis of complex exponentials, $e^{i2\pi sx}$, respectively: $$ F(s) = \int_{-\infty}^{\infty}f(x)e^{-i2\pi sx}dx$$ $$ f(x) = \int_{-\infty}^{\infty}F(s)e^{i2\pi sx}ds$$ Here are my questions about this view to make it more clear: 1)If I understand correctly, this operation is akin to regular linear algebra change of basis operations $a=Mb$ and $b=M^{-1}a$. Roughly, in this case $M$ is a matrix of uncountable many rows and columns where each row is $e^{-i2\pi sx}$, a function of $x$, and similarly $M^{-1}$ has rows as $e^{i2\pi sx}$, functions of $s$. Is this interpretation correct? 2)I don't have the exact rigour for this but intuitively think, if 1) is a correct interpretation then we should obtain from the ""infinite dimensional"" matrix multiplication $MM^{-1}$ something which resembles an infinite dimensional identity matrix. To test that, I built the inner product where $s$ is held fixed and equal in both terms from two objects $M$ and $M^{-1}$, which should correspond to a ""diagonal"" element of $MM^{-1}$: $\int_{-\infty}^{\infty}e^{i2\pi sx}e^{-i2\pi sx}dx = \int_{-\infty}^{\infty}e^{0}dx=\infty$. So this is not $1$ as expected from an identity matrix. What is the reason of that?",,"['linear-algebra', 'functional-analysis', 'fourier-analysis']"
89,Why is cross product not commutative?,Why is cross product not commutative?,,"Why, conceptually, is the cross product not commutative? Obviously I could simply take a look at the formula for computing cross product from vector components to prove this, but I'm interested in why it makes logical sense for the resultant vector to be either going in the negative or positive direction depending on the order of the cross operation. I don't have any formal experience in linear algebra, so I would appreciate if an answer would take that into account (I'm merely learning vectors as part of a 3D game math education).","Why, conceptually, is the cross product not commutative? Obviously I could simply take a look at the formula for computing cross product from vector components to prove this, but I'm interested in why it makes logical sense for the resultant vector to be either going in the negative or positive direction depending on the order of the cross operation. I don't have any formal experience in linear algebra, so I would appreciate if an answer would take that into account (I'm merely learning vectors as part of a 3D game math education).",,"['linear-algebra', 'vectors', 'cross-product']"
90,Finding Jordan Basis of a matrix,Finding Jordan Basis of a matrix,,"Having trouble finding the Jordan base (and hence $P$) for this matrix $A = \begin{pmatrix} 15&-4\\ 49&-13 \end{pmatrix}$ I know that the eigenvalue is $1$, this gives an eigenvector $\begin{pmatrix} 2\\ 7 \end{pmatrix} $ Now to create the Jordan basis and find $P$ (of which its columns will consist of the two basis vectors) I'm aware that I need to find $v_1$ s.t $(A-I)v_1 = 0$, so $v_1 = \begin{pmatrix} 2\\ 7 \end{pmatrix}$. Now to find $v_2$ I need to  do $ker(A-I)^2$ but $(A-I)^2 = 0$ so any non zero vector is in the kernel right$?$ So why doesn't it work if I choose $v_2 = \begin{pmatrix} 1\\ 0 \end{pmatrix}$ $?$","Having trouble finding the Jordan base (and hence $P$) for this matrix $A = \begin{pmatrix} 15&-4\\ 49&-13 \end{pmatrix}$ I know that the eigenvalue is $1$, this gives an eigenvector $\begin{pmatrix} 2\\ 7 \end{pmatrix} $ Now to create the Jordan basis and find $P$ (of which its columns will consist of the two basis vectors) I'm aware that I need to find $v_1$ s.t $(A-I)v_1 = 0$, so $v_1 = \begin{pmatrix} 2\\ 7 \end{pmatrix}$. Now to find $v_2$ I need to  do $ker(A-I)^2$ but $(A-I)^2 = 0$ so any non zero vector is in the kernel right$?$ So why doesn't it work if I choose $v_2 = \begin{pmatrix} 1\\ 0 \end{pmatrix}$ $?$",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
91,An operator that commutes with another operator $T$ with distinct characteristic values is a polynomial in $T$,An operator that commutes with another operator  with distinct characteristic values is a polynomial in,T T,"I'm trying to solve some problems in Hoffman and Kunze and I'm kind of stuck on this one. This is 6.5.3 on Hoffman and Kunze. Here is the question: Let $T$ be a a linear operator on an $n$-dimensional space, and suppose that $T$ has $n$ distinct characteristic values. Prove that any linear operator $U$ which commutes with $T$ is a polynomial in $T$. My work so far: Since $T$ has $n$ distinct characteristic values and since the space it acts on is also $n$ dimensional, $T$ must be diagonalisable. Now using the fact $UT=TU$ I can show that $U$ must also be diagonalizable. So these two operators must be simultaneously diagonalizable. Now I'm stuck with where this is going. I'm thinking of simultaneous diagonalizability because this is section 6.5 of Hoffman and Kunze and it deals with simultaneous diagonalizability. Am I on the right track with this?. Can anybody help? Thanks so much for your time and your answers.","I'm trying to solve some problems in Hoffman and Kunze and I'm kind of stuck on this one. This is 6.5.3 on Hoffman and Kunze. Here is the question: Let $T$ be a a linear operator on an $n$-dimensional space, and suppose that $T$ has $n$ distinct characteristic values. Prove that any linear operator $U$ which commutes with $T$ is a polynomial in $T$. My work so far: Since $T$ has $n$ distinct characteristic values and since the space it acts on is also $n$ dimensional, $T$ must be diagonalisable. Now using the fact $UT=TU$ I can show that $U$ must also be diagonalizable. So these two operators must be simultaneously diagonalizable. Now I'm stuck with where this is going. I'm thinking of simultaneous diagonalizability because this is section 6.5 of Hoffman and Kunze and it deals with simultaneous diagonalizability. Am I on the right track with this?. Can anybody help? Thanks so much for your time and your answers.",,"['linear-algebra', 'matrices', 'diagonalization']"
92,Average area of the shadow of a convex shape [closed],Average area of the shadow of a convex shape [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question What is the average area of the shadow of a convex shape taken over all possible orientations? If we take a sphere, its surface area is exactly 4 times the area of its shadow. How can it be generalised for any convex shape? I know there are a lot of books like ""Introduction to Geometric Probability"", but I would appreciate an almost-intuitive explanation.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question What is the average area of the shadow of a convex shape taken over all possible orientations? If we take a sphere, its surface area is exactly 4 times the area of its shadow. How can it be generalised for any convex shape? I know there are a lot of books like ""Introduction to Geometric Probability"", but I would appreciate an almost-intuitive explanation.",,"['linear-algebra', 'geometry', 'linear-transformations', 'polyhedra', 'geometric-probability']"
93,Intuition behind speciality of symmetric matrices,Intuition behind speciality of symmetric matrices,,"What is the geometric intuition behind the fact that only matrices that are similar to a symmetric matrix are diagonizable? So e.g. why is it important that the multiplier of the the first component of the last basis vector be the same as the multiplier of the the last component of the first basis vector(i.e. that in an $n*n $ matrix $(n,1)$ be the same as $(1,n)$)?","What is the geometric intuition behind the fact that only matrices that are similar to a symmetric matrix are diagonizable? So e.g. why is it important that the multiplier of the the first component of the last basis vector be the same as the multiplier of the the last component of the first basis vector(i.e. that in an $n*n $ matrix $(n,1)$ be the same as $(1,n)$)?",,['linear-algebra']
94,The number of subspaces over a finite field [duplicate],The number of subspaces over a finite field [duplicate],,This question already has answers here : How many k-dimensional subspaces there are in n-dimensional vector space over $\mathbb F_p$? (3 answers) Closed 2 years ago . How to prove this conclusion If $V$ is a vector space of dimension $n$ and $F$ is a finite field with $q$ elements then number of subspaces of dim $k$ is,This question already has answers here : How many k-dimensional subspaces there are in n-dimensional vector space over $\mathbb F_p$? (3 answers) Closed 2 years ago . How to prove this conclusion If is a vector space of dimension and is a finite field with elements then number of subspaces of dim is,V n F q k,"['linear-algebra', 'finite-fields']"
95,$T$ is diagonalizable if $T^n$ is identity for some $n$,is diagonalizable if  is identity for some,T T^n n,"Suppose $T$ is a linear operator on a $\mathbb{C}$-vector space $V$. Further, assume $T^n$ is the identity operator, for some $n$. Then, $T$ is diagonalizable. I think there is a proof using Jordan theory, but I wish to find one without using it.","Suppose $T$ is a linear operator on a $\mathbb{C}$-vector space $V$. Further, assume $T^n$ is the identity operator, for some $n$. Then, $T$ is diagonalizable. I think there is a proof using Jordan theory, but I wish to find one without using it.",,['linear-algebra']
96,Injectivity of the dual map,Injectivity of the dual map,,"Suppose V and W are vector spaces of  possibly finite and infinite dimension over a field K. Show that if a linear map $L : V → W$ is surjective the its dual is injective. Also prove the converse of the last implication. Well when V,W are finite spaces i can prove it and i understand that dimension is not necessary if i want prove surjective implies injective. Other way injective implies surjective when is finite i take a basis ${e_1,….,e_n}$ of V then ${Le_1,….,Le_n}$ is l.i. in W so we can extend a basis ${Le_1,….,Le_n,w_1,…,w_k}$ of W and define:$f: W → K$ by $f(Le_i) = g(w_i)$ and $f(w_i)=0$ then $L^*: V → K$ and$L^*(f)(e_i) = (fL)(e_i) = f(Le_i)=g(e_i)$ then $L^*(f)=g$ But what happen if V and W is infinite dimension?.","Suppose V and W are vector spaces of  possibly finite and infinite dimension over a field K. Show that if a linear map $L : V → W$ is surjective the its dual is injective. Also prove the converse of the last implication. Well when V,W are finite spaces i can prove it and i understand that dimension is not necessary if i want prove surjective implies injective. Other way injective implies surjective when is finite i take a basis ${e_1,….,e_n}$ of V then ${Le_1,….,Le_n}$ is l.i. in W so we can extend a basis ${Le_1,….,Le_n,w_1,…,w_k}$ of W and define:$f: W → K$ by $f(Le_i) = g(w_i)$ and $f(w_i)=0$ then $L^*: V → K$ and$L^*(f)(e_i) = (fL)(e_i) = f(Le_i)=g(e_i)$ then $L^*(f)=g$ But what happen if V and W is infinite dimension?.",,"['linear-algebra', 'vector-spaces', 'category-theory']"
97,Density of Diagonalizable matrices,Density of Diagonalizable matrices,,"Suppose that the matrix $A$ is not diagonalizable. Show that for all $\epsilon>0$, there exists a diagonalizable matrix $A_{\epsilon}$ such that $||A-A_{\epsilon}||_{2}<\epsilon$. Can anyone help me prove this or give a hint?","Suppose that the matrix $A$ is not diagonalizable. Show that for all $\epsilon>0$, there exists a diagonalizable matrix $A_{\epsilon}$ such that $||A-A_{\epsilon}||_{2}<\epsilon$. Can anyone help me prove this or give a hint?",,"['linear-algebra', 'matrices']"
98,Prove that if $A$ is diagonalizable then there is a matrix $B$ such that $B^{2012} = A$,Prove that if  is diagonalizable then there is a matrix  such that,A B B^{2012} = A,"Given: $$A \in M_{n\times n} (\mathbb C) \; , \; A \; \text{is diagonalizable}$$ We need to prove that: $$ \exists B \in M_{n\times n} (\mathbb C) \; : B^{2012} = A$$ What I said so far: If $A$ is diagonalizable, then $\exists P$ and $D$ such that $A = P D P^{-1}$   where $D = \begin{pmatrix} \lambda_1 & 0 &\cdots& 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0  & \cdots & \lambda_n \end{pmatrix}$ , $P = [[v_1], [v_2], \dotsc ,[v_n]]$, $\det(P) \neq 0$ and also $\lambda_1 , \lambda_2 , \dotsc , \lambda_n$ are all distinct. also, $A^{2012} = P D^{2012} P^{-1}$ But where do I exactly go from there?","Given: $$A \in M_{n\times n} (\mathbb C) \; , \; A \; \text{is diagonalizable}$$ We need to prove that: $$ \exists B \in M_{n\times n} (\mathbb C) \; : B^{2012} = A$$ What I said so far: If $A$ is diagonalizable, then $\exists P$ and $D$ such that $A = P D P^{-1}$   where $D = \begin{pmatrix} \lambda_1 & 0 &\cdots& 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0  & \cdots & \lambda_n \end{pmatrix}$ , $P = [[v_1], [v_2], \dotsc ,[v_n]]$, $\det(P) \neq 0$ and also $\lambda_1 , \lambda_2 , \dotsc , \lambda_n$ are all distinct. also, $A^{2012} = P D^{2012} P^{-1}$ But where do I exactly go from there?",,['linear-algebra']
99,Condition number of a rectangular matrix,Condition number of a rectangular matrix,,"From what I understand, the condition number of a rectangular matrix $A$ is its largest singular value divided by its smallest nonzero singular value $$\kappa(A) := \frac{\sigma_1 (A)}{\sigma_n (A)}$$ Where $\sigma_1 (A)$ is the operator norm of $A$ and $\sigma_n (A)$ is the operator norm of $A^\dagger$, the pseudoinverse of A. Is this correct and is it generally accepted? I have not been able to find much on the subject online. Is this used in any notable applications?","From what I understand, the condition number of a rectangular matrix $A$ is its largest singular value divided by its smallest nonzero singular value $$\kappa(A) := \frac{\sigma_1 (A)}{\sigma_n (A)}$$ Where $\sigma_1 (A)$ is the operator norm of $A$ and $\sigma_n (A)$ is the operator norm of $A^\dagger$, the pseudoinverse of A. Is this correct and is it generally accepted? I have not been able to find much on the subject online. Is this used in any notable applications?",,"['linear-algebra', 'matrices', 'condition-number', 'singular-values']"
