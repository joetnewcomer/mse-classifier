,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Show that $f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2$ is coercive",Show that  is coercive,"f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2","I am trying to show that the function $$f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2$$ is coercive on $\mathbb{R}^2$ . To show the function is coercive, we require $\|(x_1,x_2)\|\rightarrow+\infty\implies f(x_1,x_2)\rightarrow +\infty.$ We proceed by using polar coordinates. This gives \begin{align} f(r,\theta)&=2r\cos\theta+(r\sin\theta-r^2\cos^2\theta)^2+(1-r\cos\theta)^2 \\ &=r^2+1+r^4\cos^4\theta-2r^3\sin\theta\cos^2\theta\\ &\geq r^2+1-2r^3\sin\theta\cos^2\theta. \\ \end{align} However, I am unsure on how to proceed. I require $r\rightarrow +\infty\implies f(r,\theta)\rightarrow +\infty$ , but I am unable to show how $r^2+1-2r^3\sin\theta\cos^2\theta\rightarrow +\infty$ .","I am trying to show that the function is coercive on . To show the function is coercive, we require We proceed by using polar coordinates. This gives However, I am unsure on how to proceed. I require , but I am unable to show how .","f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2 \mathbb{R}^2 \|(x_1,x_2)\|\rightarrow+\infty\implies f(x_1,x_2)\rightarrow +\infty. \begin{align}
f(r,\theta)&=2r\cos\theta+(r\sin\theta-r^2\cos^2\theta)^2+(1-r\cos\theta)^2 \\
&=r^2+1+r^4\cos^4\theta-2r^3\sin\theta\cos^2\theta\\
&\geq r^2+1-2r^3\sin\theta\cos^2\theta. \\
\end{align} r\rightarrow +\infty\implies f(r,\theta)\rightarrow +\infty r^2+1-2r^3\sin\theta\cos^2\theta\rightarrow +\infty","['multivariable-calculus', 'coercive']"
1,"Prove $ \int_{\mathbb{R}^d} \frac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2 $",Prove," \int_{\mathbb{R}^d} \frac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2 ","My question is: Prove that there exists some constant $c_d$ such that for any $\xi \in \mathbb{R}^d$ : $$\displaystyle \int_{\mathbb{R}^d} \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2 $$ where $\langle \xi, x \rangle = \displaystyle \sum_{j=1}^{d} \xi_j y_j$ . I have $$ \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}} = \dfrac{|2\cos\langle \xi, y \rangle - 2|^2 }{|y|^{d+2}} = \dfrac{16|\sin^2\dfrac{\langle \xi, y \rangle}{2} |^2 }{|y|^{d+2}} \leq \dfrac{16}{|y|^{d+2}}$$ integrable since $d+2 > d$ so the mapping $y \mapsto \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}$ is integrable in $\mathbb{R}^d$ . But I don't know how to find prove the equality above. Are there any ideas for this problem? Thank you so much.",My question is: Prove that there exists some constant such that for any : where . I have integrable since so the mapping is integrable in . But I don't know how to find prove the equality above. Are there any ideas for this problem? Thank you so much.,"c_d \xi \in \mathbb{R}^d \displaystyle \int_{\mathbb{R}^d} \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2  \langle \xi, x \rangle = \displaystyle \sum_{j=1}^{d} \xi_j y_j  \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}} = \dfrac{|2\cos\langle \xi, y \rangle - 2|^2 }{|y|^{d+2}} = \dfrac{16|\sin^2\dfrac{\langle \xi, y \rangle}{2} |^2 }{|y|^{d+2}} \leq \dfrac{16}{|y|^{d+2}} d+2 > d y \mapsto \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}} \mathbb{R}^d","['integration', 'multivariable-calculus', 'fourier-analysis', 'fourier-transform', 'multiple-integral']"
2,The optimal value is continuous in the parameter,The optimal value is continuous in the parameter,,"Let $f:(0,\infty) \to [0,\infty)$ be a continuous function satisfying $f(1)=0$ , which is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Define $$ F(s)=\min_{xy=s,x,y>0} f(x)+ f(y), \, \,  \, \, \text{for } \, \, s \in (0,\infty). $$ Claim: $F$ is continuous. I am looking for a reference for such a claim. (not that claim exactly, but perhaps a slightly more general claim which implies it , or is similar to it). I think that it follows from a result in the book ""Perturbation Analysis of Optimization Problems"" by Bonnans and Shapiro, but that book phrases things in much more abstract setting than I find necessary. BTW, here is my proof: Suppose that $s \le 1$ . Define $g(x,y)=f(x)+f(y)$ . Suppose that $s_n \to s$ . Write $F(s_n)=g(x_n,y_n)$ for some $x_n,y_n \in [s_n,1]^2, x_ny_n=s_n$ . By compactness we may assume that $x_{n_{k}} \to x, y_{n_{k}} \to y$ . Thus $$ F(s) \le g(x,y)=\lim_{k \to \infty} g(x_{n_k},x_{y_k})=\lim_{k \to \infty}F(s_{n_k}) \le \liminf F(s_n). $$ On the other hand, take $(x,y) \in (0,\infty)^2$ such that $xy=s$ and $F(s)=g(x,y)$ . Now take $x_n,y_n$ such that $x_ny_n=s_n$ , and $(x_n,y_n) \to (x,y)$ . Then $$ F(s_n) \le g(x_n,y_n) \Rightarrow \limsup F(s_n) \le \lim_{n \to \infty}g(x_n,y_n)=g(x,y)=F(s). $$","Let be a continuous function satisfying , which is strictly increasing on , and strictly decreasing on . Define Claim: is continuous. I am looking for a reference for such a claim. (not that claim exactly, but perhaps a slightly more general claim which implies it , or is similar to it). I think that it follows from a result in the book ""Perturbation Analysis of Optimization Problems"" by Bonnans and Shapiro, but that book phrases things in much more abstract setting than I find necessary. BTW, here is my proof: Suppose that . Define . Suppose that . Write for some . By compactness we may assume that . Thus On the other hand, take such that and . Now take such that , and . Then","f:(0,\infty) \to [0,\infty) f(1)=0 [1,\infty) (0,1] 
F(s)=\min_{xy=s,x,y>0} f(x)+ f(y), \, \,  \, \, \text{for } \, \, s \in (0,\infty).
 F s \le 1 g(x,y)=f(x)+f(y) s_n \to s F(s_n)=g(x_n,y_n) x_n,y_n \in [s_n,1]^2, x_ny_n=s_n x_{n_{k}} \to x, y_{n_{k}} \to y 
F(s) \le g(x,y)=\lim_{k \to \infty} g(x_{n_k},x_{y_k})=\lim_{k \to \infty}F(s_{n_k}) \le \liminf F(s_n).
 (x,y) \in (0,\infty)^2 xy=s F(s)=g(x,y) x_n,y_n x_ny_n=s_n (x_n,y_n) \to (x,y) 
F(s_n) \le g(x_n,y_n) \Rightarrow \limsup F(s_n) \le \lim_{n \to \infty}g(x_n,y_n)=g(x,y)=F(s).
","['real-analysis', 'calculus', 'multivariable-calculus', 'optimization', 'reference-request']"
3,Prove that the boundary and the interior of a manifold are disjoint.,Prove that the boundary and the interior of a manifold are disjoint.,,"What shown below is a reference from Analysis on Manifolds by James Munkres. So using the last lemma I ask to prove that the boundary and the interior of a manifold are disjoint. In particular I attempted to show this using reductio ad adsurdum but it seems I didn't be able to do this. Anyway reading the above definition it seems that the boundary and the interior are disjoint by them own definition. Indeed there are two possible cases: either there exist a coordinate patch $\alpha$ about $p$ with domain an open set in $\Bbb R^k$ or there not exist a such coordinate patch. Perhaps is my last argument incorrect? So could someone help me, please?","What shown below is a reference from Analysis on Manifolds by James Munkres. So using the last lemma I ask to prove that the boundary and the interior of a manifold are disjoint. In particular I attempted to show this using reductio ad adsurdum but it seems I didn't be able to do this. Anyway reading the above definition it seems that the boundary and the interior are disjoint by them own definition. Indeed there are two possible cases: either there exist a coordinate patch about with domain an open set in or there not exist a such coordinate patch. Perhaps is my last argument incorrect? So could someone help me, please?",\alpha p \Bbb R^k,"['calculus', 'general-topology', 'multivariable-calculus', 'differential-geometry', 'manifolds']"
4,"What Are the $x_i^*,y_j^*$ in the Riemann Sum Definition of the Double Integral?",What Are the  in the Riemann Sum Definition of the Double Integral?,"x_i^*,y_j^*","the double integral $$\iint \limits_{[a,b] \times [c,d]} f(x,y) \, dxdy$$ can be represented with a Riemann Sum as $$\lim_{(\Delta x , \Delta y) \to (0,0)} \sum_{i=1}^n\sum_{j=1}^mf(x_i^*,y_j^*)\Delta x \Delta y$$ What are $x_i^*$ and $y_j^*$ in this case?",the double integral can be represented with a Riemann Sum as What are and in this case?,"\iint \limits_{[a,b] \times [c,d]} f(x,y) \, dxdy \lim_{(\Delta x , \Delta y) \to (0,0)} \sum_{i=1}^n\sum_{j=1}^mf(x_i^*,y_j^*)\Delta x \Delta y x_i^* y_j^*","['calculus', 'integration', 'multivariable-calculus', 'notation', 'riemann-sum']"
5,Changing the direction of integration,Changing the direction of integration,,"I need to change the direction of the integral: $$ \int_0^1 dy \int_{0.5y^2}^{\sqrt{3-y^2}} fdx$$ From what I know, I first need to find the shapes: $0.5y^2 = x$ and $\sqrt{3-y^2} =x$ Shape I is a parabola: $y^2 = 2x$ Shape II is a circle $x^2 + y^2 = 3$ (radius of $\sqrt{3}$ ) So we basically draw horizontal arrows from the parabola to the circle while we keep $0 \leq y \leq 1$ . Something that looks very similar to this picture: We need to draw vertical lines, so it looks like this, but we have 3 areas: Where we hit the parabola (red) Where we hit the line $y=1$ (green) Where we hit the circle (blue) And so my final answer is: $$ \int_0^{0.5} dx \int_0^{\sqrt{2x}} fdy + \int_{0.5}^{\sqrt{2}} dx \int_0^1 fdy + \int_{\sqrt{2}}^{\sqrt{3}} dx \int_0^{\sqrt{3-x^2}} fdy$$ Am I right so far? If I am not, then how do I fix it? I feel stuck as I have no idea how to keep going... I would appreciate your help! Thanks!","I need to change the direction of the integral: From what I know, I first need to find the shapes: and Shape I is a parabola: Shape II is a circle (radius of ) So we basically draw horizontal arrows from the parabola to the circle while we keep . Something that looks very similar to this picture: We need to draw vertical lines, so it looks like this, but we have 3 areas: Where we hit the parabola (red) Where we hit the line (green) Where we hit the circle (blue) And so my final answer is: Am I right so far? If I am not, then how do I fix it? I feel stuck as I have no idea how to keep going... I would appreciate your help! Thanks!", \int_0^1 dy \int_{0.5y^2}^{\sqrt{3-y^2}} fdx 0.5y^2 = x \sqrt{3-y^2} =x y^2 = 2x x^2 + y^2 = 3 \sqrt{3} 0 \leq y \leq 1 y=1  \int_0^{0.5} dx \int_0^{\sqrt{2x}} fdy + \int_{0.5}^{\sqrt{2}} dx \int_0^1 fdy + \int_{\sqrt{2}}^{\sqrt{3}} dx \int_0^{\sqrt{3-x^2}} fdy,"['integration', 'multivariable-calculus']"
6,Center of mass on an unbounded interval [closed],Center of mass on an unbounded interval [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question I am having a hard time locating the center of mass of $y=\frac{1}{x}$ on $(1,\infty)$ . I used the formula but I get a value of infinity for the mass of the lamina alone. I was able to get an improper integral after using the formula but then it just gave me an infinity as value at the end. I tried to check it with the graph and it seems that it isn't at infinity. How do we solve center of mass when the interval is unbounded? Any idea is greatly appreciated. Thank you so much.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question I am having a hard time locating the center of mass of on . I used the formula but I get a value of infinity for the mass of the lamina alone. I was able to get an improper integral after using the formula but then it just gave me an infinity as value at the end. I tried to check it with the graph and it seems that it isn't at infinity. How do we solve center of mass when the interval is unbounded? Any idea is greatly appreciated. Thank you so much.","y=\frac{1}{x} (1,\infty)","['calculus', 'algebra-precalculus', 'multivariable-calculus']"
7,change of variables and partial derivatives in thermodynamics,change of variables and partial derivatives in thermodynamics,,"In thermodynamics one works with state functions, e.g., the energy function $U(Y_1,\dotsc,Y_n)$ , where $Y_i>0$ are the so called extensive variables. This function is $1$ st order homogeneous. Sometimes one uses the specific energy function $$u(y_1,\dots,y_{n-1})=U(y_1,\dots,y_{n-1},1),$$ where $y_i=\frac{Y_i}{Y_n}$ . The relation between $u$ and $U$ is $$Y_n\cdot u(y_1,\dotsc,y_n)=U(Y_1,\dots,Y_n).$$ I want to understand how to express second order partial derivatives $\frac{\partial^2u}{\partial y_i\partial y_j}$ in terms of $\frac{\partial^2U}{\partial Y_i\partial Y_j}$ and $Y_i$ . Let's start with the first order derivatives. Using the chain rule we can write $$\frac{\partial u}{\partial y_i}=\sum_{k=1}^n \frac{\partial U}{\partial Y_k}\frac{\partial Y_k}{\partial y_i}.$$ But now I get stuck. What is $\frac{\partial Y_k}{\partial y_i}$ ? Can we simply write it as $\frac{\partial Y_k}{\partial y_i}=1/\frac{\partial y_i}{\partial Y_k}$ ? Apparently not as otherwise, e.g., $\frac{\partial y_1}{\partial Y_2}=0$ and $\frac{\partial Y_2}{\partial y_1}=\infty$ . How can we deal with that? Technically, it must hold that $\frac{\partial U}{\partial Y_i}=\frac{\partial u}{\partial y_i}$ because the first order partial derivatives of a $1$ st order homogeneous function are $0$ th order homogeneous. But I do not see how to show that formally using mathematical arguments.","In thermodynamics one works with state functions, e.g., the energy function , where are the so called extensive variables. This function is st order homogeneous. Sometimes one uses the specific energy function where . The relation between and is I want to understand how to express second order partial derivatives in terms of and . Let's start with the first order derivatives. Using the chain rule we can write But now I get stuck. What is ? Can we simply write it as ? Apparently not as otherwise, e.g., and . How can we deal with that? Technically, it must hold that because the first order partial derivatives of a st order homogeneous function are th order homogeneous. But I do not see how to show that formally using mathematical arguments.","U(Y_1,\dotsc,Y_n) Y_i>0 1 u(y_1,\dots,y_{n-1})=U(y_1,\dots,y_{n-1},1), y_i=\frac{Y_i}{Y_n} u U Y_n\cdot u(y_1,\dotsc,y_n)=U(Y_1,\dots,Y_n). \frac{\partial^2u}{\partial y_i\partial y_j} \frac{\partial^2U}{\partial Y_i\partial Y_j} Y_i \frac{\partial u}{\partial y_i}=\sum_{k=1}^n \frac{\partial U}{\partial Y_k}\frac{\partial Y_k}{\partial y_i}. \frac{\partial Y_k}{\partial y_i} \frac{\partial Y_k}{\partial y_i}=1/\frac{\partial y_i}{\partial Y_k} \frac{\partial y_1}{\partial Y_2}=0 \frac{\partial Y_2}{\partial y_1}=\infty \frac{\partial U}{\partial Y_i}=\frac{\partial u}{\partial y_i} 1 0","['multivariable-calculus', 'derivatives', 'partial-derivative', 'mathematical-modeling', 'change-of-variable']"
8,Where is the Schatten norm differentiable?,Where is the Schatten norm differentiable?,,"Let $M_n$ be the space of real $n \times n$ matrices, and let $|\cdot|_p$ be the $p$ - Schatten norm on $M_n$ , for $p \neq 2$ . For $A \in M_n$ , $|A|_p=\big(\sum \sigma_i^p(A)\big)^{1/p}$ , where $\sigma_i(A)$ are the singular values of $A$ . Is $| \cdot |_p$ differentiable at any non-zero point $A \in M_n$ ? Is it smooth there? If not, at which points is it differentiable? I think that $| \cdot |_p$ is $C^{\infty}$ when restricted to $GL_n(\mathbb R)$ , since $|A|_p^p=\text{tr}(|A|^p)$ : The map $A \to |A|$ is smooth on $GL_n(\mathbb R)$ , I guess that the map $A \to A^p$ is smooth on the domain of symmetric matrices. (Is it really? I am not sure).","Let be the space of real matrices, and let be the - Schatten norm on , for . For , , where are the singular values of . Is differentiable at any non-zero point ? Is it smooth there? If not, at which points is it differentiable? I think that is when restricted to , since : The map is smooth on , I guess that the map is smooth on the domain of symmetric matrices. (Is it really? I am not sure).",M_n n \times n |\cdot|_p p M_n p \neq 2 A \in M_n |A|_p=\big(\sum \sigma_i^p(A)\big)^{1/p} \sigma_i(A) A | \cdot |_p A \in M_n | \cdot |_p C^{\infty} GL_n(\mathbb R) |A|_p^p=\text{tr}(|A|^p) A \to |A| GL_n(\mathbb R) A \to A^p,"['real-analysis', 'multivariable-calculus', 'matrix-calculus', 'matrix-decomposition']"
9,Question about proof of interchanging integral and derivative using uniform convergence.,Question about proof of interchanging integral and derivative using uniform convergence.,,"Theorem: Assume that $\varphi(x,t):[a,b]\times[c,d]\to \Bbb R$ $\varphi(x,t)\in R[a,b]$ ( $\varphi(x,t)$ is Riemann integrable on $[a,b]$ ) $\ \ \forall t\in [c,d]$ . i.e. $$\int_a^b\varphi(x,t)dx$$ exists $\ \forall t\in [c,d]$ . Let $c\lt s\lt d $ and $\varepsilon \gt 0$ , there exists $\delta \gt 0 $ s.t. $$\lvert \; \frac{\partial\varphi}{\partial t}(x,t) - \frac{\partial\varphi}{\partial t}(x,s)\;\rvert \lt \epsilon$$ for all $x\in [a,b]$ , $t\in (s-\delta,s+\delta)$ . Define $$f(t)=\int_a^b\varphi(x,t)dx$$ , $t\in[c,d]$ . Then $$\frac{\partial\varphi}{\partial t}(x,s)\in R[a,b]$$ and $$f'(s)=\int_a^b\frac{\partial\varphi}{\partial t}(x,s)dx$$ My attempt: Define $$\Psi(x,t)=\frac{\varphi(x,t)-\varphi(x,s)}{t-s}$$ for $0\lt\lvert t-s \rvert\lt\delta$ . By MVT, $\exists u$ between $s$ and $t$ s.t. $$\Psi(x,t)=\frac{\partial\varphi}{\partial t}(x,u)$$ So $$\lvert \Psi(x,t)-\frac{\partial\varphi}{\partial t}(x,s)  \rvert=\lvert\frac{\partial\varphi}{\partial t}(x,u)-\frac{\partial\varphi}{\partial t}(x,s)  \rvert\lt\varepsilon\ \forall x\in [a,b] \;\; \color{blue}{(\star)}$$ by assumption 3. $\color{darkred}{\textrm{My question}}$ : By $\color{blue}{(\star)}$ , can I conclude that $$\Psi(x,t)\to\frac{\partial\varphi}{\partial t}(x,s) \text{ uniformly on } [a,b] \text{ as } t\to s  \;\;\color{darkorange}{(\star)}\text{ ? }$$ be the limit function. I have learned uniform convergence for sequence of functions like $\{ f_n\}\to f$ uniformly means for any $\varepsilon \gt 0$ , there exist $N\in \Bbb N$ s.t.  for any $n\gt N$ $\lvert f_n(x)-f(x) \rvert\lt\varepsilon\ \forall x\in X$ ( if $f$ is defined on $X$ ). How can I find such $N$ in $\color{darkorange}{(\star)}$ ? Suppose $$\varepsilon=\frac1n$$ $(\forall n\in\Bbb N)\ \exists\delta_n$ in assumption 3 s.t. $$t\in (s-\delta_n,s+\delta_n)$$ which means $t$ getting closer and closer to $s$ as $n$ getting greater. i.e. $t\to s$ as $n\to\infty$ Can I let $$\Psi(x,t)=f_n(x)$$ in $\color{darkorange}{(\star)}$ ,where $n$ depends on $\delta_n$ related to $t$ . Let $$\frac{\partial\varphi}{\partial t}(x,s)=f(x)$$ Then $$f_n(x)\to f(x)\text{ uniformly on }[a,b]\text{ as }t\to s$$ Is my proof correct? Thanks for helping.","Theorem: Assume that ( is Riemann integrable on ) . i.e. exists . Let and , there exists s.t. for all , . Define , . Then and My attempt: Define for . By MVT, between and s.t. So by assumption 3. : By , can I conclude that be the limit function. I have learned uniform convergence for sequence of functions like uniformly means for any , there exist s.t.  for any ( if is defined on ). How can I find such in ? Suppose in assumption 3 s.t. which means getting closer and closer to as getting greater. i.e. as Can I let in ,where depends on related to . Let Then Is my proof correct? Thanks for helping.","\varphi(x,t):[a,b]\times[c,d]\to \Bbb R \varphi(x,t)\in R[a,b] \varphi(x,t) [a,b] \ \ \forall t\in [c,d] \int_a^b\varphi(x,t)dx \ \forall t\in [c,d] c\lt s\lt d  \varepsilon \gt 0 \delta \gt 0  \lvert \; \frac{\partial\varphi}{\partial t}(x,t) - \frac{\partial\varphi}{\partial t}(x,s)\;\rvert \lt \epsilon x\in [a,b] t\in (s-\delta,s+\delta) f(t)=\int_a^b\varphi(x,t)dx t\in[c,d] \frac{\partial\varphi}{\partial t}(x,s)\in R[a,b] f'(s)=\int_a^b\frac{\partial\varphi}{\partial t}(x,s)dx \Psi(x,t)=\frac{\varphi(x,t)-\varphi(x,s)}{t-s} 0\lt\lvert t-s \rvert\lt\delta \exists u s t \Psi(x,t)=\frac{\partial\varphi}{\partial t}(x,u) \lvert \Psi(x,t)-\frac{\partial\varphi}{\partial t}(x,s)  \rvert=\lvert\frac{\partial\varphi}{\partial t}(x,u)-\frac{\partial\varphi}{\partial t}(x,s)  \rvert\lt\varepsilon\ \forall x\in [a,b] \;\; \color{blue}{(\star)} \color{darkred}{\textrm{My question}} \color{blue}{(\star)} \Psi(x,t)\to\frac{\partial\varphi}{\partial t}(x,s) \text{ uniformly on } [a,b] \text{ as } t\to s  \;\;\color{darkorange}{(\star)}\text{ ? } \{ f_n\}\to f \varepsilon \gt 0 N\in \Bbb N n\gt N \lvert f_n(x)-f(x) \rvert\lt\varepsilon\ \forall x\in X f X N \color{darkorange}{(\star)} \varepsilon=\frac1n (\forall n\in\Bbb N)\ \exists\delta_n t\in (s-\delta_n,s+\delta_n) t s n t\to s n\to\infty \Psi(x,t)=f_n(x) \color{darkorange}{(\star)} n \delta_n t \frac{\partial\varphi}{\partial t}(x,s)=f(x) f_n(x)\to f(x)\text{ uniformly on }[a,b]\text{ as }t\to s","['real-analysis', 'multivariable-calculus', 'uniform-convergence']"
10,Limit of Darboux sums in $\mathbb{R}^n$,Limit of Darboux sums in,\mathbb{R}^n,"Let $Q\subseteq\mathbb{R}^n$ be a rectangle $f:Q\to\mathbb{R}$ be a bounded function. Then for any $\varepsilon>0$ there exists a $\delta>0$ such that $U(f;P)\le \overline{\int}_Qf+\varepsilon$ for any partition $P$ of $Q$ with width (the maximum length of the intervals composing each subrectangle determined by $P$ ) lesser than $\delta$ . I'm trying to generalize a step from the case $n=1$ : Let $f(x)\ge 0~\forall x\in Q$ . Given $\varepsilon>0$ there exists $P_0\in\Pi_Q$ such that $U(f;P_0) < \overline{\int}_Q f + \varepsilon/2$ , because the upper integral is the infimum of the upper sums. Let $(R_i)_{1\leq i \leq k}$ be the family of the $k$ rectangles of $P_0$ . Choose a $\delta$ such that $0<\delta<{\varepsilon/2kM}$ , and take a partition $P$ of $Q$ with width lesser than $\delta$ . Denote by $R_\alpha$ the rectangles of $P$ that lie in some $R_i$ of $P_0$ , and by $R_\beta$ the remaining rectangles of $P$ . From here, if $n=1$ we could conclude that there are at most $k$ of the rectangles $R_\beta$ because each one of these should have, in their interior, a point of the partition $P_0=\{t_0,\dots t_k\}$ of the closed interval $Q$ (and their interiors are disjoint). And with this we could bound the upper sum the way we want. But in the general case I can't find a way to do it. I tried considering the border $\partial R_i$ of each rectangle of $P$ , and indeed each $R_\beta$ must have points of them in their interior, but the borders infinite sets so I can't find a bound for the number of rectangles $R_\beta$ . Any ideas?","Let be a rectangle be a bounded function. Then for any there exists a such that for any partition of with width (the maximum length of the intervals composing each subrectangle determined by ) lesser than . I'm trying to generalize a step from the case : Let . Given there exists such that , because the upper integral is the infimum of the upper sums. Let be the family of the rectangles of . Choose a such that , and take a partition of with width lesser than . Denote by the rectangles of that lie in some of , and by the remaining rectangles of . From here, if we could conclude that there are at most of the rectangles because each one of these should have, in their interior, a point of the partition of the closed interval (and their interiors are disjoint). And with this we could bound the upper sum the way we want. But in the general case I can't find a way to do it. I tried considering the border of each rectangle of , and indeed each must have points of them in their interior, but the borders infinite sets so I can't find a bound for the number of rectangles . Any ideas?","Q\subseteq\mathbb{R}^n f:Q\to\mathbb{R} \varepsilon>0 \delta>0 U(f;P)\le \overline{\int}_Qf+\varepsilon P Q P \delta n=1 f(x)\ge 0~\forall x\in Q \varepsilon>0 P_0\in\Pi_Q U(f;P_0) < \overline{\int}_Q f + \varepsilon/2 (R_i)_{1\leq i \leq k} k P_0 \delta 0<\delta<{\varepsilon/2kM} P Q \delta R_\alpha P R_i P_0 R_\beta P n=1 k R_\beta P_0=\{t_0,\dots t_k\} Q \partial R_i P R_\beta R_\beta","['real-analysis', 'multivariable-calculus', 'riemann-sum']"
11,Continuity of multivariable function $\frac{xy}{\sqrt{16-x^2-y^2}}$,Continuity of multivariable function,\frac{xy}{\sqrt{16-x^2-y^2}},"How can I define where is the following function continuous? The function being: $$f(x,y) = \frac{xy}{\sqrt{16-x^2-y^2}}$$ Is it enough with getting the domain by doing $16-x^2-y^2>0$ . How does it change if instead of $xy$ I have $\frac{y}{\sqrt{16-x^2-y^2}}$ or $\frac{x^2+y^2}{\sqrt{16-x^2-y^2}}$ ? Thanks for the help.",How can I define where is the following function continuous? The function being: Is it enough with getting the domain by doing . How does it change if instead of I have or ? Thanks for the help.,"f(x,y) = \frac{xy}{\sqrt{16-x^2-y^2}} 16-x^2-y^2>0 xy \frac{y}{\sqrt{16-x^2-y^2}} \frac{x^2+y^2}{\sqrt{16-x^2-y^2}}","['calculus', 'multivariable-calculus']"
12,Jacobian determinant equal to one?,Jacobian determinant equal to one?,,"I would like to characterize the set of all continuous $G:[0,1]^n \rightarrow [0,1]^n$ such that the Jacobian determinant is one: $\mathcal{G}=\{G:[0,1]^n \rightarrow [0,1]^n: |DG(x)|=1\forall x \}$ . Is it true that $\mathcal{G}$ corresponds to the set of all linear transformations $G(x)=Ax+b$ with orthogonal matrices $A$ ? Indeed we have $DG= A$ in this case. And taking a determinant equal to one should imply that $A$ is orthogonal.",I would like to characterize the set of all continuous such that the Jacobian determinant is one: . Is it true that corresponds to the set of all linear transformations with orthogonal matrices ? Indeed we have in this case. And taking a determinant equal to one should imply that is orthogonal.,"G:[0,1]^n \rightarrow [0,1]^n \mathcal{G}=\{G:[0,1]^n \rightarrow [0,1]^n: |DG(x)|=1\forall x \} \mathcal{G} G(x)=Ax+b A DG= A A","['real-analysis', 'multivariable-calculus', 'linear-transformations', 'determinant', 'orthogonal-matrices']"
13,Difference between Directional Derivative x Chain Rule for Scalar Fields,Difference between Directional Derivative x Chain Rule for Scalar Fields,,"could you guys help me out with an issue I am having. What's the difference between the ""Directional Derivative"" and ""Chain Rule for Scalar Fields""? In meaning and the formulae ? I don't know if I got it right but both of them have the same formula: $$ g'(\vec{r(t)}) = \nabla(g(t)) \cdot  \vec{r'(t)}    \ for \ the \ Chain \ Rule$$ And $$ Derivative = \nabla (g(t)) \cdot \vec{r(t)}  \ for \ the \ Directional\ Derivative$$ And to me they both seem to have the same meaning, since when we use the chain rule, we are using a vector whose direction is defined by the parameters.","could you guys help me out with an issue I am having. What's the difference between the ""Directional Derivative"" and ""Chain Rule for Scalar Fields""? In meaning and the formulae ? I don't know if I got it right but both of them have the same formula: And And to me they both seem to have the same meaning, since when we use the chain rule, we are using a vector whose direction is defined by the parameters.", g'(\vec{r(t)}) = \nabla(g(t)) \cdot  \vec{r'(t)}    \ for \ the \ Chain \ Rule  Derivative = \nabla (g(t)) \cdot \vec{r(t)}  \ for \ the \ Directional\ Derivative,"['calculus', 'multivariable-calculus', 'derivatives', 'chain-rule', 'scalar-fields']"
14,Find the area enclosed by $r = 1 + \sin\theta$ and $r = 1 - \sin\theta$,Find the area enclosed by  and,r = 1 + \sin\theta r = 1 - \sin\theta,"Find the area enclosed by $r = 1 + \sin\theta$ and $r = 1 - \sin\theta$ So, the curves are given by the following parametrizations: $$ f_1(\theta) = ((1 + \sin \theta) \cos \theta,(1 + \sin \theta) \sin \theta)$$ $$  f_2(\theta) = ((1 - \sin \theta) \cos \theta,(1 - \sin \theta) \sin \theta)$$ It looks logical that I have to find the intersections. How can I find the integral enclosed by the curves?","Find the area enclosed by and So, the curves are given by the following parametrizations: It looks logical that I have to find the intersections. How can I find the integral enclosed by the curves?","r = 1 + \sin\theta r = 1 - \sin\theta  f_1(\theta) = ((1 + \sin \theta) \cos \theta,(1 + \sin \theta) \sin \theta)   f_2(\theta) = ((1 - \sin \theta) \cos \theta,(1 - \sin \theta) \sin \theta)",['calculus']
15,"Equivalence of the two directional derivative definitions, without multivariable chain rule","Equivalence of the two directional derivative definitions, without multivariable chain rule",,"I am currently trying to nail down a good proof of the multivariable chain rule. So far, I have that for a function $ f: \mathbb{R}^n \to \mathbb{R}$ and a function $v: \mathbb{R} \to \mathbb{R}^n$ , $$\dfrac{d}{dt} (f \circ v)(t) = \lim_{h \to 0} \dfrac{f(v(t)+hv'(t))-f(v(t))}{h}$$ This is, by definition, the directional derivative $\nabla_{v'(t)} f(v(t))$ , which using the second definition of the directional derivative, is nothing but $\nabla f(v(t)) \cdot v'(t)$ , which yields the multivariable chain rule. However, I cannot find a proof of the equivalence of these two definitions which does not use the multivariable chain rule in some way; so I ask: 1) Does a proof of this equivalence exist without the multivariable chain rule, or 2) If this equivalence cannot be shown without the multivariable chain rule, are there any nice proofs of which you know (I have yet to find one I like, besides this one if it does work out).","I am currently trying to nail down a good proof of the multivariable chain rule. So far, I have that for a function and a function , This is, by definition, the directional derivative , which using the second definition of the directional derivative, is nothing but , which yields the multivariable chain rule. However, I cannot find a proof of the equivalence of these two definitions which does not use the multivariable chain rule in some way; so I ask: 1) Does a proof of this equivalence exist without the multivariable chain rule, or 2) If this equivalence cannot be shown without the multivariable chain rule, are there any nice proofs of which you know (I have yet to find one I like, besides this one if it does work out).", f: \mathbb{R}^n \to \mathbb{R} v: \mathbb{R} \to \mathbb{R}^n \dfrac{d}{dt} (f \circ v)(t) = \lim_{h \to 0} \dfrac{f(v(t)+hv'(t))-f(v(t))}{h} \nabla_{v'(t)} f(v(t)) \nabla f(v(t)) \cdot v'(t),['multivariable-calculus']
16,If the Hessian matrix is symmetric is $\mathscr C^2$?,If the Hessian matrix is symmetric is ?,\mathscr C^2,"I know that, if $ f (x)$ is a $\mathscr {C}^2$ function, then the Hessian matrix is symmetric. But if the matrix is symmetric could not be $\mathscr {C}^2$ ; can someone give me an example?","I know that, if is a function, then the Hessian matrix is symmetric. But if the matrix is symmetric could not be ; can someone give me an example?", f (x) \mathscr {C}^2 \mathscr {C}^2,"['multivariable-calculus', 'hessian-matrix']"
17,$|x|^p/p^p\le e^{-x}+e^{x}$ for all $p$ positive and $x\in \mathbb R$,for all  positive and,|x|^p/p^p\le e^{-x}+e^{x} p x\in \mathbb R,"I wish to prove the following inequality $$|x|^p/p^p\le e^{-x}+e^{x}$$ for all $p$ positive and $x\in \mathbb R$ . My partial progress: Taking the $p$ -th root and $\log$ , it suffices to show $$f(x):=(1/p)\log(e^{-x}+e^{x})- \log (|x|/p)$$ is non-negative. But I have trouble in analyzing the derivative and minimal value of this function. Maybe there is some other trick to approach this? Maybe we should consider this as a two-variable problem?","I wish to prove the following inequality for all positive and . My partial progress: Taking the -th root and , it suffices to show is non-negative. But I have trouble in analyzing the derivative and minimal value of this function. Maybe there is some other trick to approach this? Maybe we should consider this as a two-variable problem?",|x|^p/p^p\le e^{-x}+e^{x} p x\in \mathbb R p \log f(x):=(1/p)\log(e^{-x}+e^{x})- \log (|x|/p),"['calculus', 'multivariable-calculus', 'inequality']"
18,"Find the directional derivative of the function $\phi=x^2-y^2+2$ at the point $P(1,2,3)$ .",Find the directional derivative of the function  at the point  .,"\phi=x^2-y^2+2 P(1,2,3)","Question: Find the directional derivative of the function $\phi=x^2-y^2+2$ at the point $P(1,2,3)$ in the direction of the st.line $PQ$ , where $Q$ is the point $(5,0,4)$ . $\dfrac{d\phi}{ds}=(l\hat{i}+m\hat{j}+n\hat{k})\cdot \left( \dfrac{\partial \phi_1}{\partial x}\hat{i}+ \dfrac{\partial \phi_2}{\partial y}\hat{j}+ \dfrac{\partial \phi_3}{\partial z}\hat{k}\right)=(\dfrac{4}{21}\hat{i}+\dfrac{-2}{21}\hat{j}+\dfrac{1}{21}\hat{k})\cdot (2x\hat{i}+-2y\hat{j})=\dfrac{4(2x+y)}{21}\implies \left[ \dfrac{d\phi}{ds}\right]_Q=\dfrac{4(10+0+y)}{21}=\dfrac{40}{21}$ . Is my approach correct?","Question: Find the directional derivative of the function at the point in the direction of the st.line , where is the point . . Is my approach correct?","\phi=x^2-y^2+2 P(1,2,3) PQ Q (5,0,4) \dfrac{d\phi}{ds}=(l\hat{i}+m\hat{j}+n\hat{k})\cdot \left( \dfrac{\partial \phi_1}{\partial x}\hat{i}+ \dfrac{\partial \phi_2}{\partial y}\hat{j}+ \dfrac{\partial \phi_3}{\partial z}\hat{k}\right)=(\dfrac{4}{21}\hat{i}+\dfrac{-2}{21}\hat{j}+\dfrac{1}{21}\hat{k})\cdot (2x\hat{i}+-2y\hat{j})=\dfrac{4(2x+y)}{21}\implies \left[ \dfrac{d\phi}{ds}\right]_Q=\dfrac{4(10+0+y)}{21}=\dfrac{40}{21}","['multivariable-calculus', 'derivatives']"
19,Generalize Clairaut-Schwarz theorem to arbitrary order of mixed partial derivatives,Generalize Clairaut-Schwarz theorem to arbitrary order of mixed partial derivatives,,"After reading the answer here to understand how to apply difference operator, I've figured out how to generalize my proof of Clairaut-Schwarz theorem here to arbitrary order of mixed partial derivatives. Could you please verify if my proof looks fine or contains logical gaps/errors? Thank you so much for your help! $\textbf{Generalized Clairaut-Schwarz Theorem:}$ Let $X$ be open in $\mathbb R^n$ , $f:X \to F$ , and $m \in \mathbb N$ . Suppose $j_1, j_2, \ldots, j_m \in\{1,\ldots,n\}$ and $\sigma$ is a permutation of $\{1, \ldots, m\}$ . If $\partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f$ is continuous at $a$ and $\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f$ exists in a neighborhood of $a$ , then $$\partial_{j_1} \cdots \partial_{j_m} f (a)= \partial_{j_{\sigma(1)}}  \cdots \partial_{j_{\sigma(m)}} f(a)$$ In my proof, I utilize two below lemmas: Let $\{e_1,\ldots, e_n\}$ be the standard basis of $\mathbb R^n$ . For $h \in \mathbb  R$ and $j \in \{1,\ldots,n\}$ , we define a map $\Delta_j^h f$ by $$\Delta_j^h f: X \to  F, \quad x \mapsto f(x+he_j)-f(x)$$ $\textbf{Lemma 1:}$ $$\partial_{j_1} \cdots \partial_{j_m} f (a) = \lim_{h_1 \to 0} \left ( \lim_{h_2 \to 0} \left( \cdots \left ( \lim_{h_m \to 0} \left( \frac{ \Delta_{j_1}^{h_1}  \cdots\Delta_{j_m}^{h_m} f (a)}{h_1 \cdots h_m} \right ) \right ) \cdots \right ) \right)$$ $\textbf{Lemma 2:}$ The finite difference operator is commutative, i.e. $$ \Delta_{j_1}^{h_1} \cdots\Delta_{j_m}^{h_m} f (a) = \Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)$$ $\textbf{My attempt:}$ By Mean Value Theorem, we have $$\begin{aligned} & \quad \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}}\\ =& \quad \frac{\Delta_{j_1}^{h_1} \cdots\Delta_{j_{m}}^{h_{m}} f (a)}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{Lemma 2} \\ =& \quad \frac{\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) h_1 \cdots h_{m}}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{MVT} \\ =& \quad\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) \end{aligned}$$ in which $$\begin{aligned} \min\{0,h_1\} < t_1 < \max\{0,h_1\} \\  \vdots\quad\quad\quad\quad\quad\quad \,\,\, \\ \min\{0,h_m\} < t_1 < \max\{0,h_m\} \end{aligned}$$ Hence $$\begin{aligned} & \quad \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\\ =& \quad \left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\end{aligned}$$ Let $t = |t_1| + \cdots+|t_{m}|$ and $h= |h_1| + \cdots+|h_{m}|$ . It follows from the continuity of $\partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f$ at $a$ that for all $\delta > 0$ there is $\epsilon > 0$ such that $$\left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| <\ \delta$$ for all $t < \epsilon$ . As such, for all $h <\ \epsilon$ , we have $$ \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| < \delta$$ Take the limit $h_{\sigma(m)} \to 0$ , we have $$\lim_{h_{\sigma(m)} \to 0} \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ and consequently $$ \left \| \lim_{h_{\sigma(m)} \to 0} \left (\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} \right ) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ and consequently $$\left \| \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m-1)}}^{h_{\sigma(m-1)}} \partial_{j_{m}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m-1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta  \quad \text{by} \,\, \textbf{Lemma 1}$$ Iterating this process of taking limit, we get $$\left \| \lim_{h_{\sigma(1)} \to 0} \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ or equivalently $$\left \| \lim_{h_{\sigma(1)} \to 0} \frac{ \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a + h_{\sigma(1)} e_{\sigma(1)}) - \left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ For all $\delta >0$ , there is $\epsilon >0$ such that for all $|h_{\sigma(1)}| < \epsilon$ , the last inequality holds. It follows that $$\partial_{j_{\sigma(1)}}\left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a)$$ and consequently $$\partial_{j_{\sigma(1)}} \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f (a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a)$$ This completes the proof.","After reading the answer here to understand how to apply difference operator, I've figured out how to generalize my proof of Clairaut-Schwarz theorem here to arbitrary order of mixed partial derivatives. Could you please verify if my proof looks fine or contains logical gaps/errors? Thank you so much for your help! Let be open in , , and . Suppose and is a permutation of . If is continuous at and exists in a neighborhood of , then In my proof, I utilize two below lemmas: Let be the standard basis of . For and , we define a map by The finite difference operator is commutative, i.e. By Mean Value Theorem, we have in which Hence Let and . It follows from the continuity of at that for all there is such that for all . As such, for all , we have Take the limit , we have and consequently and consequently Iterating this process of taking limit, we get or equivalently For all , there is such that for all , the last inequality holds. It follows that and consequently This completes the proof.","\textbf{Generalized Clairaut-Schwarz Theorem:} X \mathbb R^n f:X \to F m \in \mathbb N j_1, j_2, \ldots, j_m \in\{1,\ldots,n\} \sigma \{1, \ldots, m\} \partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f a \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f a \partial_{j_1} \cdots \partial_{j_m} f (a)= \partial_{j_{\sigma(1)}}  \cdots \partial_{j_{\sigma(m)}} f(a) \{e_1,\ldots, e_n\} \mathbb R^n h \in \mathbb  R j \in \{1,\ldots,n\} \Delta_j^h f \Delta_j^h f: X \to  F, \quad x \mapsto f(x+he_j)-f(x) \textbf{Lemma 1:} \partial_{j_1} \cdots \partial_{j_m} f (a) = \lim_{h_1 \to 0} \left ( \lim_{h_2 \to 0} \left( \cdots \left ( \lim_{h_m \to 0} \left( \frac{ \Delta_{j_1}^{h_1}  \cdots\Delta_{j_m}^{h_m} f (a)}{h_1 \cdots h_m} \right ) \right ) \cdots \right ) \right) \textbf{Lemma 2:}  \Delta_{j_1}^{h_1} \cdots\Delta_{j_m}^{h_m} f (a) = \Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a) \textbf{My attempt:} \begin{aligned} & \quad \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}}\\
=& \quad \frac{\Delta_{j_1}^{h_1} \cdots\Delta_{j_{m}}^{h_{m}} f (a)}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{Lemma 2} \\
=& \quad \frac{\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) h_1 \cdots h_{m}}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{MVT} \\ =& \quad\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) \end{aligned} \begin{aligned} \min\{0,h_1\} < t_1 < \max\{0,h_1\} \\  \vdots\quad\quad\quad\quad\quad\quad \,\,\, \\ \min\{0,h_m\} < t_1 < \max\{0,h_m\} \end{aligned} \begin{aligned} & \quad \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\\
=& \quad \left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\end{aligned} t = |t_1| + \cdots+|t_{m}| h= |h_1| + \cdots+|h_{m}| \partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f a \delta > 0 \epsilon > 0 \left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| <\ \delta t < \epsilon h <\ \epsilon  \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| < \delta h_{\sigma(m)} \to 0 \lim_{h_{\sigma(m)} \to 0} \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta  \left \| \lim_{h_{\sigma(m)} \to 0} \left (\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} \right ) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta \left \| \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m-1)}}^{h_{\sigma(m-1)}} \partial_{j_{m}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m-1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta  \quad \text{by} \,\, \textbf{Lemma 1} \left \| \lim_{h_{\sigma(1)} \to 0} \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta \left \| \lim_{h_{\sigma(1)} \to 0} \frac{ \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a + h_{\sigma(1)} e_{\sigma(1)}) - \left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta \delta >0 \epsilon >0 |h_{\sigma(1)}| < \epsilon \partial_{j_{\sigma(1)}}\left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \partial_{j_{\sigma(1)}} \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f (a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a)","['real-analysis', 'multivariable-calculus', 'partial-derivative']"
20,"Derivative of $f(x,y)=x-y$, where $x,y\in\mathbb R^2$","Derivative of , where","f(x,y)=x-y x,y\in\mathbb R^2","Suppose that $f:\mathbb R^2\times\mathbb R^2\to\mathbb R^2$ is given by $f(x,y)=x-y$ . How can I calculate the first and the second order derivative of $f$ ? If $g:\mathbb R^n\to\mathbb R$ , the gradient vector and the Hessian matrix of $g$ are given by $$ \nabla g(x)_i=\frac{\partial g(x)}{\partial x_i} \quad\text{and}\quad \nabla^2g(x)_{ij}=\frac{\partial^2g(x)}{\partial x_i\partial x_j} $$ for $i,j=1,\ldots,n$ . But the function $f$ is defined on the Cartesian product $\mathbb R^2\times\mathbb R^2$ . Is the Cartesian product $\mathbb R^2\times\mathbb R^2$ the same as $\mathbb R^4$ in some sense? Any help is much appreciated!","Suppose that is given by . How can I calculate the first and the second order derivative of ? If , the gradient vector and the Hessian matrix of are given by for . But the function is defined on the Cartesian product . Is the Cartesian product the same as in some sense? Any help is much appreciated!","f:\mathbb R^2\times\mathbb R^2\to\mathbb R^2 f(x,y)=x-y f g:\mathbb R^n\to\mathbb R g 
\nabla g(x)_i=\frac{\partial g(x)}{\partial x_i}
\quad\text{and}\quad
\nabla^2g(x)_{ij}=\frac{\partial^2g(x)}{\partial x_i\partial x_j}
 i,j=1,\ldots,n f \mathbb R^2\times\mathbb R^2 \mathbb R^2\times\mathbb R^2 \mathbb R^4","['real-analysis', 'multivariable-calculus', 'derivatives']"
21,Schwarz's theorem,Schwarz's theorem,,"We were provided the following example in the context of Schwarz's theorem: Let be $f:\mathbb{R}^2 \to \mathbb{R}$ , given by: $$ f{x \choose y}=\left\{\begin{array}{ll} \frac {xy^3}{x^2+y^2}, & {x \choose y}\neq {0 \choose 0} \\          0, & {x \choose y}={0 \choose 0}\end{array}\right. . $$ If we calculate the second order partial derivatives for each ${x \choose y} \neq {0 \choose 0}$ we get: $$ D_{12}f{x \choose y} = D_{21}f{x \choose y}. $$ However, at point ${0 \choose 0}$ we have: $$ D_{12}f{0 \choose 0} =1 \neq 0 = D_{21}f{0 \choose 0}. $$ Our lecture notes say that we now can deduce only from Schwarz's theorem, that both second order derivatives $D_{12}f{x \choose y}$ and $D_{21}f{x \choose y}$ cannot be continuous at point ${0 \choose 0} $ . I don't understand how we can deduce this fact only from Schwarz's theorem? Couldn't it be possible that only one of the two second order derivatives is continuous?","We were provided the following example in the context of Schwarz's theorem: Let be , given by: If we calculate the second order partial derivatives for each we get: However, at point we have: Our lecture notes say that we now can deduce only from Schwarz's theorem, that both second order derivatives and cannot be continuous at point . I don't understand how we can deduce this fact only from Schwarz's theorem? Couldn't it be possible that only one of the two second order derivatives is continuous?","f:\mathbb{R}^2 \to \mathbb{R} 
f{x \choose y}=\left\{\begin{array}{ll} \frac {xy^3}{x^2+y^2}, & {x \choose y}\neq {0 \choose 0} \\
         0, & {x \choose y}={0 \choose 0}\end{array}\right. .
 {x \choose y} \neq {0 \choose 0} 
D_{12}f{x \choose y} = D_{21}f{x \choose y}.
 {0 \choose 0} 
D_{12}f{0 \choose 0} =1 \neq 0 = D_{21}f{0 \choose 0}.
 D_{12}f{x \choose y} D_{21}f{x \choose y} {0 \choose 0} ","['multivariable-calculus', 'continuity', 'partial-derivative']"
22,Volume integration,Volume integration,,"Let $$D = \left\{(x,y,z)\in\mathbb{R}^{3}\mid x\ge0,0\le y\le x, x^2+y^2\le {16}, 0\le z\le {5}\right\}.$$ I want to integrate $$\displaystyle\iiint\limits_{D}\left({-4\,z+y^2+x^2}\right)\,\mathrm{d}V $$ We can see that $x^2+y^2=r^2$ so $r^2=16$ . $r\to[0,16]$ and $\theta\to[0,2\pi]$ and $z \to[0,5]$ And then integration $$\int_0^{2\pi}\int_0^5\int_0^{16}(-4z+y^2+x^2)\,dV= \int_0^{2\pi}\int_0^5\int_0^{16}(-4z+r^2)\,drdzd\theta=\frac{36160\pi}{3}$$ That is wrong answer and I don't know where I have done mistake.",Let I want to integrate We can see that so . and and And then integration That is wrong answer and I don't know where I have done mistake.,"D = \left\{(x,y,z)\in\mathbb{R}^{3}\mid x\ge0,0\le y\le x, x^2+y^2\le {16}, 0\le z\le {5}\right\}. \displaystyle\iiint\limits_{D}\left({-4\,z+y^2+x^2}\right)\,\mathrm{d}V  x^2+y^2=r^2 r^2=16 r\to[0,16] \theta\to[0,2\pi] z \to[0,5] \int_0^{2\pi}\int_0^5\int_0^{16}(-4z+y^2+x^2)\,dV= \int_0^{2\pi}\int_0^5\int_0^{16}(-4z+r^2)\,drdzd\theta=\frac{36160\pi}{3}","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
23,Finding absolute maximum and minimum values on circular bounded region,Finding absolute maximum and minimum values on circular bounded region,,"Find the absolute maximum and minimum values of $f(x,y)=4x^2y$ on the set $S=\{(x,y):x^2+y^2\le1\}$ . I am confused as to how to check the boundary of the circular region. I tried subtracting the formulas, i.e. $z = 4(x^2)y - 9 = x^2 + y^2$ and got the critical points of $(\frac{1}{2\sqrt 2} , \frac{1}{4})$ and $(-\frac{1}{2\sqrt 2} , \frac{1}{4})$ but this seems to be incorrect? Any help would be much appreciated, preferably building on the method I used (unless it is a completely wrong approach). Thank you so much!","Find the absolute maximum and minimum values of on the set . I am confused as to how to check the boundary of the circular region. I tried subtracting the formulas, i.e. and got the critical points of and but this seems to be incorrect? Any help would be much appreciated, preferably building on the method I used (unless it is a completely wrong approach). Thank you so much!","f(x,y)=4x^2y S=\{(x,y):x^2+y^2\le1\} z = 4(x^2)y - 9 = x^2 + y^2 (\frac{1}{2\sqrt 2} , \frac{1}{4}) (-\frac{1}{2\sqrt 2} , \frac{1}{4})","['multivariable-calculus', 'derivatives', 'optimization', 'maxima-minima']"
24,"$\iint_A F\cdot n\, dS$ region bounded by $x^2+y^2=1$ and $-1\le z\le 2$",region bounded by  and,"\iint_A F\cdot n\, dS x^2+y^2=1 -1\le z\le 2","This is the question: we need to find the outward flux and verify it using divergence theorem The main query is that i solved it normally first using $\iint F\cdot n\,dS$ and i got incorrect answer as $3\pi$ and when i solve it using divergence theorem i got the correct answer as $6\pi$ . Can yu please find my mistake? Here is my solution: Now one more thing : what is the condition to use green's theorem and stoke's theorem why are they not valid here as i am getting curl 0 so answer will be 0 according to stoke's theorem. The question is how to know when do we use stoke's green's or divergence theorem ?",This is the question: we need to find the outward flux and verify it using divergence theorem The main query is that i solved it normally first using and i got incorrect answer as and when i solve it using divergence theorem i got the correct answer as . Can yu please find my mistake? Here is my solution: Now one more thing : what is the condition to use green's theorem and stoke's theorem why are they not valid here as i am getting curl 0 so answer will be 0 according to stoke's theorem. The question is how to know when do we use stoke's green's or divergence theorem ?,"\iint F\cdot n\,dS 3\pi 6\pi","['calculus', 'integration', 'multivariable-calculus', 'surface-integrals', 'divergence-operator']"
25,Find $d$ for which $f$ gets its minimal value,Find  for which  gets its minimal value,d f,"Let $\beta>\alpha>0$ and let $A,B,C,D$ be the vertices of the rectangle $[\alpha,\beta]\times[\alpha,\beta]$ , such that $A$ is the closest one to the origin, and the path $A\to B\to C\to D\to A$ is oriented clockwise. Let $\vec{F}(x,y)=(y,x)$ . A particle is located at the vertex $B$ and is moving along a curve $\gamma$ which is given by the following parametrization: $$\left\{\begin{aligned}\gamma(t)=\bigg(\beta-(\beta-\alpha)\cos(t)+e^{-t^2}\sin^3 (2t)& ,\beta-(\beta-\alpha)\sin(t)+(dt^2-\pi t)^3\bigg)\\ t\in[0,\frac \pi 2]\end{aligned}\right.$$ When $d\in\mathbb{R}$ . Let $f(d)$ be the function: $$f(d)=\left|\int_\gamma \vec{F}\cdot d\vec{r}\right|$$ Find $d$ for which $f$ gets its minimal value. Where does the particle stop, given the value of $d$ you found? Hint : What does the given rectangle has to do with the problem? Note : I usually expand on what I tried to do and how I approached the problem. However, for this particular problem, I really could not think of a way to approach it. Calculating the integral and getting an expression that is dependent on $d$ seemed the straightforward solution, but it seems impossible given the above parametrization. Thank you very much!","Let and let be the vertices of the rectangle , such that is the closest one to the origin, and the path is oriented clockwise. Let . A particle is located at the vertex and is moving along a curve which is given by the following parametrization: When . Let be the function: Find for which gets its minimal value. Where does the particle stop, given the value of you found? Hint : What does the given rectangle has to do with the problem? Note : I usually expand on what I tried to do and how I approached the problem. However, for this particular problem, I really could not think of a way to approach it. Calculating the integral and getting an expression that is dependent on seemed the straightforward solution, but it seems impossible given the above parametrization. Thank you very much!","\beta>\alpha>0 A,B,C,D [\alpha,\beta]\times[\alpha,\beta] A A\to B\to C\to D\to A \vec{F}(x,y)=(y,x) B \gamma \left\{\begin{aligned}\gamma(t)=\bigg(\beta-(\beta-\alpha)\cos(t)+e^{-t^2}\sin^3 (2t)& ,\beta-(\beta-\alpha)\sin(t)+(dt^2-\pi t)^3\bigg)\\ t\in[0,\frac \pi 2]\end{aligned}\right. d\in\mathbb{R} f(d) f(d)=\left|\int_\gamma \vec{F}\cdot d\vec{r}\right| d f d d",['multivariable-calculus']
26,The solution of the Euler-Bernoulli equation via separation of variables,The solution of the Euler-Bernoulli equation via separation of variables,,"I'm trying to solve the Euler-Bernoulli equation for a beam for a construction with a solution in the form of $w(\zeta,t) = f(\zeta)g(t)$ (separation of variables). However, it is mentioned that this can still be tricky, but since the beam is undamped, we can assume $ g(t) = e^{i \lambda t}$ with $\lambda$ real. My first question is; why is the above choice logical to assume? (does this have anything to do with $e^{i \lambda t} = cos(\lambda t) + i sin(\lambda t)$ for $\lambda \in \mathbb{R}$ ) I made a beginning with the separation of variable technique: The Euler-Bernoulli equation: $\rho\frac{\partial^{2}w}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}w}{\partial \zeta^{4}}(\zeta,t),\text{ }\text{ }\text{ }\zeta \in [0,1], t \geq 0$ With boundary conditions: $w(0,t) = w(1,t), \frac{\partial w}{\partial \zeta}(0,t) = \frac{\partial w}{\partial \zeta}(1,t)$ Substituting $w(\zeta,t) = f(\zeta)g(t)$ in the Euler-Bernoulli equation gives: $\rho\frac{\partial^{2}f(\zeta)g(t)}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}f(\zeta)g(t)}{\partial \zeta^{4}}(\zeta,t)$ $f(\zeta)\rho\frac{\partial^{2}g(t)}{\partial t^{2}}(\zeta,t) = -EIg(t)\frac{\partial^{4}f(\zeta)}{\partial \zeta^{4}}(\zeta,t)$ Since space and time are in this case independent of each other, both sides have to be equal to the same constant - $\lambda$ . This gives us a set of three qualities: $ 1. \frac{d^2g(t)}{dt^2} = -g(t)\lambda \\ 2. \frac{d^4f(\zeta)}{d\zeta^4} = \rho f(t)\frac{\lambda}{EI} \\ 3. w(0,t) = w(1,t) \rightarrow f(0) = f(1) $ The general solution of g(t) (if positive) is: $ g(t) = C_{1} * e^{-i \sqrt{\lambda}t} + C_{2} * e^{i \sqrt{\lambda}t} $ Using Euler's formula, g(t) becomes: $ g(t) = C_1 cos(\sqrt{\lambda}t) + C_2 sin(\sqrt{\lambda}t) $ From here, I am pretty much stuck to find a proposed solution for $f(\zeta)$ and to determine the solution to this problem. The second question now is how to determine a proposed solution for $f(\zeta)$ and how to determine the 3 lowest (strictly positive) $\lambda$ 's such that $w(\zeta,t) = f(\zeta)e^{i\lambda t}$ is a non-zero solution of the Euler-Bernoulli equation? Thank you for all of your help.","I'm trying to solve the Euler-Bernoulli equation for a beam for a construction with a solution in the form of (separation of variables). However, it is mentioned that this can still be tricky, but since the beam is undamped, we can assume with real. My first question is; why is the above choice logical to assume? (does this have anything to do with for ) I made a beginning with the separation of variable technique: The Euler-Bernoulli equation: With boundary conditions: Substituting in the Euler-Bernoulli equation gives: Since space and time are in this case independent of each other, both sides have to be equal to the same constant - . This gives us a set of three qualities: The general solution of g(t) (if positive) is: Using Euler's formula, g(t) becomes: From here, I am pretty much stuck to find a proposed solution for and to determine the solution to this problem. The second question now is how to determine a proposed solution for and how to determine the 3 lowest (strictly positive) 's such that is a non-zero solution of the Euler-Bernoulli equation? Thank you for all of your help.","w(\zeta,t) = f(\zeta)g(t)  g(t) = e^{i \lambda t} \lambda e^{i \lambda t} = cos(\lambda t) + i sin(\lambda t) \lambda \in \mathbb{R} \rho\frac{\partial^{2}w}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}w}{\partial \zeta^{4}}(\zeta,t),\text{ }\text{ }\text{ }\zeta \in [0,1], t \geq 0 w(0,t) = w(1,t), \frac{\partial w}{\partial \zeta}(0,t) = \frac{\partial w}{\partial \zeta}(1,t) w(\zeta,t) = f(\zeta)g(t) \rho\frac{\partial^{2}f(\zeta)g(t)}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}f(\zeta)g(t)}{\partial \zeta^{4}}(\zeta,t) f(\zeta)\rho\frac{\partial^{2}g(t)}{\partial t^{2}}(\zeta,t) = -EIg(t)\frac{\partial^{4}f(\zeta)}{\partial \zeta^{4}}(\zeta,t) \lambda 
1. \frac{d^2g(t)}{dt^2} = -g(t)\lambda \\
2. \frac{d^4f(\zeta)}{d\zeta^4} = \rho f(t)\frac{\lambda}{EI} \\
3. w(0,t) = w(1,t) \rightarrow f(0) = f(1)
 
g(t) = C_{1} * e^{-i \sqrt{\lambda}t} + C_{2} * e^{i \sqrt{\lambda}t}
 
g(t) = C_1 cos(\sqrt{\lambda}t) + C_2 sin(\sqrt{\lambda}t)
 f(\zeta) f(\zeta) \lambda w(\zeta,t) = f(\zeta)e^{i\lambda t}","['calculus', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'boundary-value-problem']"
27,"Showing that $f(u,v) = (u,v,u^2-v^2)$ is a parametrization.",Showing that  is a parametrization.,"f(u,v) = (u,v,u^2-v^2)","Exercise : Let $f(u,v) = (u,v,u^2-v^2), \; (u,v) \in U$ where $U$ is a coherent and open subset of $\mathbb R^2$ . Show that $f$ is a parametrization of a $C^\infty$ patch (local surface) of a surface $\Phi$ . Attempt/Question : I know that in order to show that $f$ is a regular parametrization (I don't know and cannot find the cases just for ""parametrization""), one needs to show that the function $f: U \to \Phi$ is injective and onto, while also $f_u \times f_v \neq 0 $ everywhere in $U$ . It is $f_u \times f_v = (-2u,2v,1) \neq \mathbf{0} \; \forall (u,v) \in U$ . I am having trouble showing that $f$ is injective and onto though, any guidance will be appreciated. Though, the following question arises : Question : When is $f$ just simply called a parametrization ? I assume, that when $f$ is injective and onto, while also smooth. That takes the $f_u \times f_v \neq \mathbf{0}$ out. As a starter in Differential Geometry I would really appreciate any thorough explanations. P.S. : I apologise if any term is mistakenly stated, as I am trying to properly translate the terms and the exercise from Greek.","Exercise : Let where is a coherent and open subset of . Show that is a parametrization of a patch (local surface) of a surface . Attempt/Question : I know that in order to show that is a regular parametrization (I don't know and cannot find the cases just for ""parametrization""), one needs to show that the function is injective and onto, while also everywhere in . It is . I am having trouble showing that is injective and onto though, any guidance will be appreciated. Though, the following question arises : Question : When is just simply called a parametrization ? I assume, that when is injective and onto, while also smooth. That takes the out. As a starter in Differential Geometry I would really appreciate any thorough explanations. P.S. : I apologise if any term is mistakenly stated, as I am trying to properly translate the terms and the exercise from Greek.","f(u,v) = (u,v,u^2-v^2), \; (u,v) \in U U \mathbb R^2 f C^\infty \Phi f f: U \to \Phi f_u \times f_v \neq 0  U f_u \times f_v = (-2u,2v,1) \neq \mathbf{0} \; \forall (u,v) \in U f f f f_u \times f_v \neq \mathbf{0}","['multivariable-calculus', 'differential-geometry', 'differential-topology', 'parametrization']"
28,Computing the gradient of quadratic form [duplicate],Computing the gradient of quadratic form [duplicate],,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . If $v$ is an $m$ -vector and $X$ is an $m \times n$ full rank matrix, how can I find the value for $n$ -vector $s$ that satisfies the following? $$ \frac{\partial}{\partial s} \|Xs - v\|^2 = 0 $$","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . If is an -vector and is an full rank matrix, how can I find the value for -vector that satisfies the following?",v m X m \times n n s  \frac{\partial}{\partial s} \|Xs - v\|^2 = 0 ,"['multivariable-calculus', 'derivatives', 'quadratic-forms', 'least-squares', 'quadratic-programming']"
29,Surface area inside cylinder,Surface area inside cylinder,,"Find the surface area of the part $\sigma$ : $x^2+y^2+z^2=4$ that lies inside the cylinder $x^2+y^2=2y$ So, the surface is a sphere of $R=2$ . It looks there should be double integral to calculate the surface, but how, which way?","Find the surface area of the part : that lies inside the cylinder So, the surface is a sphere of . It looks there should be double integral to calculate the surface, but how, which way?",\sigma x^2+y^2+z^2=4 x^2+y^2=2y R=2,"['integration', 'multivariable-calculus']"
30,Is reversing the order of integration possible here. If so where do I make a mistake?,Is reversing the order of integration possible here. If so where do I make a mistake?,,"I've been looking to solve this problem for a few hours now. But I don't seem to progress to the give solution at all. Maybe you could be of some help to me. The question is as follows: ""Reverse the order of integration and evaluate the resulting iterated integral"" $$\int_{-2}^{4}\int_{\frac{x_2^2}{2}-3}^{x_2+1}(x_1)dx_1dx_2$$ So what I did is this: $$x_{1}=x_{2}+1 \Rightarrow x_2=x_1-1$$ $$x_1=\frac{(x^2)}{2}-3 \Rightarrow x_2= -\sqrt{2x_1+6}$$ Given that where $$x_2=4 \Rightarrow x_1 = 4+1 = 5 $$ Then for, $$x_2=-2 \Rightarrow x_1=\frac{-2^2}{2}-3=-1$$ Which if I'm right gives me the new intervals for the reverse iterated interval. Namely, $$\int_{-1}^{5}\left(\int_{-\sqrt{2x_1+6}}^{x_1-1}(x_1)dx_2\right)dx_1$$ Then integrated to: $$\int_{-1}^{5}\left[x_1*(x_1-1)-(x_1*-\sqrt{2x_1+6})\right] dx_1 $$ According to my teachers solution manual the solution should be should be (18/5). Which is the solution to the integral when not reversing the order of integration.","I've been looking to solve this problem for a few hours now. But I don't seem to progress to the give solution at all. Maybe you could be of some help to me. The question is as follows: ""Reverse the order of integration and evaluate the resulting iterated integral"" So what I did is this: Given that where Then for, Which if I'm right gives me the new intervals for the reverse iterated interval. Namely, Then integrated to: According to my teachers solution manual the solution should be should be (18/5). Which is the solution to the integral when not reversing the order of integration.",\int_{-2}^{4}\int_{\frac{x_2^2}{2}-3}^{x_2+1}(x_1)dx_1dx_2 x_{1}=x_{2}+1 \Rightarrow x_2=x_1-1 x_1=\frac{(x^2)}{2}-3 \Rightarrow x_2= -\sqrt{2x_1+6} x_2=4 \Rightarrow x_1 = 4+1 = 5  x_2=-2 \Rightarrow x_1=\frac{-2^2}{2}-3=-1 \int_{-1}^{5}\left(\int_{-\sqrt{2x_1+6}}^{x_1-1}(x_1)dx_2\right)dx_1 \int_{-1}^{5}\left[x_1*(x_1-1)-(x_1*-\sqrt{2x_1+6})\right] dx_1 ,"['real-analysis', 'multivariable-calculus']"
31,Integral over a surface area $\int_{S^{n-1}}x_{1}^{2}dS$,Integral over a surface area,\int_{S^{n-1}}x_{1}^{2}dS,"I want to evaluate the following integral: $$\int_{S^{n-1}}x_{1}^{2}dS$$ And I think i'm supposed to use the fact that $$ \int_{S^{n-1}}dS=\frac{2\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}\right)} $$ Attempt: As $S^{n-1}=\left\{ \left(x_{1},\ldots,x_{n}\right)\in\mathbb{R}^{n}\mid x_{1}^{2}+\ldots+x_{n}^{2}=1\right\} \subset\mathbb{R}^{n}$ we can parametrize the hemisphere using $r\colon B^{n-1}\subset\mathbb{R}^{n-1}\to\mathbb{R}^{n}$ defined by $$r\left(x\right)=\left(f\left(x_{2},\ldots,x_{n}\right),x_{2},\ldots,x_{n}\right) $$ where $f\colon\mathbb{R}^{n-1}\to\mathbb{R}$ is defined by \begin{align*}  & f\left(x\right)=\sqrt{1-\left(x_{2}^{2}+\ldots+x_{n}^{2}\right)}=\sqrt{1-\left|x\right|^{2}}\\ \Rightarrow\quad & \left|\nabla f\right|^{2}=\frac{\left|x\right|^{2}}{1-\left|x\right|^{2}} \end{align*} Now for $g\colon\mathbb{R}^{n}\to\mathbb{R}$ defined by $g\left(x_{1},\ldots,x_{n}\right)=x_{1}^{2}$ we can get \begin{align*} \int_{S^{n-1}}g\left(x\right)dS & =\int_{B^{n-1}}\left(g\circ r\right)\left(x\right)\sqrt{1+\left|\nabla f\right|^{2}}dx_{2}\ldots dx_{n}=\\  & =\int_{B^{n-1}}f\left(x_{2},\ldots,x_{n}\right)^{2}\sqrt{1+\left|\nabla f\right|^{2}}dx_{2}\ldots dx_{n}=\\  & =\int_{B^{n-1}}\left(1-\left|x\right|^{2}\right)\sqrt{\frac{1}{1-\left|x\right|^{2}}}dx_{2}\ldots dx_{n}=\\  & =\int_{B^{n-1}}\sqrt{1-\left|x\right|^{2}}dx_{2}\ldots dx_{n} \end{align*} Is this correct? I don't even know if it helps. How can I evaluate the initial integral?",I want to evaluate the following integral: And I think i'm supposed to use the fact that Attempt: As we can parametrize the hemisphere using defined by where is defined by Now for defined by we can get Is this correct? I don't even know if it helps. How can I evaluate the initial integral?,"\int_{S^{n-1}}x_{1}^{2}dS 
\int_{S^{n-1}}dS=\frac{2\pi^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}\right)}
 S^{n-1}=\left\{ \left(x_{1},\ldots,x_{n}\right)\in\mathbb{R}^{n}\mid x_{1}^{2}+\ldots+x_{n}^{2}=1\right\} \subset\mathbb{R}^{n} r\colon B^{n-1}\subset\mathbb{R}^{n-1}\to\mathbb{R}^{n} r\left(x\right)=\left(f\left(x_{2},\ldots,x_{n}\right),x_{2},\ldots,x_{n}\right)
 f\colon\mathbb{R}^{n-1}\to\mathbb{R} \begin{align*}
 & f\left(x\right)=\sqrt{1-\left(x_{2}^{2}+\ldots+x_{n}^{2}\right)}=\sqrt{1-\left|x\right|^{2}}\\
\Rightarrow\quad & \left|\nabla f\right|^{2}=\frac{\left|x\right|^{2}}{1-\left|x\right|^{2}}
\end{align*} g\colon\mathbb{R}^{n}\to\mathbb{R} g\left(x_{1},\ldots,x_{n}\right)=x_{1}^{2} \begin{align*}
\int_{S^{n-1}}g\left(x\right)dS & =\int_{B^{n-1}}\left(g\circ r\right)\left(x\right)\sqrt{1+\left|\nabla f\right|^{2}}dx_{2}\ldots dx_{n}=\\
 & =\int_{B^{n-1}}f\left(x_{2},\ldots,x_{n}\right)^{2}\sqrt{1+\left|\nabla f\right|^{2}}dx_{2}\ldots dx_{n}=\\
 & =\int_{B^{n-1}}\left(1-\left|x\right|^{2}\right)\sqrt{\frac{1}{1-\left|x\right|^{2}}}dx_{2}\ldots dx_{n}=\\
 & =\int_{B^{n-1}}\sqrt{1-\left|x\right|^{2}}dx_{2}\ldots dx_{n}
\end{align*}","['integration', 'multivariable-calculus', 'surface-integrals']"
32,On an example of a subet of $\mathbb{R}^2$ that is not a submanifold of $\mathbb{R}^2$,On an example of a subet of  that is not a submanifold of,\mathbb{R}^2 \mathbb{R}^2,"I am struggling with the proof by contradiction that the following subset $X$ is not a submanifold of $\mathbb{R}^2$ and I would appreciate any explanation. Let $$ X = ((-1,0] \times \{0\}) \cup (\{0\} \times [0, 1)) \subseteq \mathbb{R}^2. $$ Suppose $X$ is a submanifold of $\mathbb{R}^2$ .  Suppose $(U, \phi)$ be a local chart of $\mathbb{R}^2$ around $(0,0)$ such that $\phi|_{U \cap X}$ maps $U \cap X$ homeomorphically onto $\phi(U) \cap (\mathbb{R} \times \{0\})$ . WLOG let $\phi(0,0) = 0$ . Let now $(U, id)$ be a local chart of $\mathbb{R}^2$ for its $C^{\infty}$ structure. Since $D(\phi \circ id^{-1})|_0$ is singular, it follows that $\phi \circ id^{-1}$ is not a diffeomorphism. Hence the local chart $(U, \phi)$ and $(U, id)$ are not $C^{\infty}$ compatible. Contradiction. $X$ is not a $C^{\infty}$ submanifold of $\mathbb{R}^2$ . 1) How come we only need to consider the dimension $1$ case? How do we know that $X$ will not be of dim $0$ and $2$ submanifold? 2) How do I prove that $D(\phi \circ id^{-1})|_0$ ? I can see it, because it's turning an edge to a straight line, but I was wondering how can I prove this?","I am struggling with the proof by contradiction that the following subset is not a submanifold of and I would appreciate any explanation. Let Suppose is a submanifold of .  Suppose be a local chart of around such that maps homeomorphically onto . WLOG let . Let now be a local chart of for its structure. Since is singular, it follows that is not a diffeomorphism. Hence the local chart and are not compatible. Contradiction. is not a submanifold of . 1) How come we only need to consider the dimension case? How do we know that will not be of dim and submanifold? 2) How do I prove that ? I can see it, because it's turning an edge to a straight line, but I was wondering how can I prove this?","X \mathbb{R}^2 
X = ((-1,0] \times \{0\}) \cup (\{0\} \times [0, 1)) \subseteq \mathbb{R}^2.
 X \mathbb{R}^2 (U, \phi) \mathbb{R}^2 (0,0) \phi|_{U \cap X} U \cap X \phi(U) \cap (\mathbb{R} \times \{0\}) \phi(0,0) = 0 (U, id) \mathbb{R}^2 C^{\infty} D(\phi \circ id^{-1})|_0 \phi \circ id^{-1} (U, \phi) (U, id) C^{\infty} X C^{\infty} \mathbb{R}^2 1 X 0 2 D(\phi \circ id^{-1})|_0","['calculus', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
33,How to simplify the the binomial coefficient in the binomial series?,How to simplify the the binomial coefficient in the binomial series?,,"Use binomial series to expand the function $\frac{5}{(6+x)^3}$ as a power series. I understand the process to get the following summation: $\frac{5}{6^3}\sum_{n=0}^{\infty} {-3 \choose n} (\frac{x}{6})^n $ However, I am stuck on seeing what's going on with ${-3 \choose n}$ . From the Stewart Calculus textbook, it says that ${k \choose n} = \frac{k(k-1)(k-2)...(k-n+1)}{n!}$ . By applying that, I would get: ${-3 \choose n}=\frac {(-3)(-4)(-5)...[-(n+2)]}{n!}$ . I think the next step would be to extract out the negative, so I will get $(-1)^n$ . The summation would then be: $\frac{5}{6^3}\sum_{n=0}^{\infty} {\frac {(-1)^n(3)(4)(5)...[(n+2)]}{n!}} (\frac{x}{6})^n $ The solution to this problem is $\frac{5}{2}\sum_{n=0}^{\infty} {\frac {(-1)^n(n+1)(n+2)x^n}{6^{n+3}}}$ I am not sure what has happened to the factorial. Was it cancelled out due to the (3)(4)(5)... in the numerator? Where did (n+1) come from?","Use binomial series to expand the function as a power series. I understand the process to get the following summation: However, I am stuck on seeing what's going on with . From the Stewart Calculus textbook, it says that . By applying that, I would get: . I think the next step would be to extract out the negative, so I will get . The summation would then be: The solution to this problem is I am not sure what has happened to the factorial. Was it cancelled out due to the (3)(4)(5)... in the numerator? Where did (n+1) come from?",\frac{5}{(6+x)^3} \frac{5}{6^3}\sum_{n=0}^{\infty} {-3 \choose n} (\frac{x}{6})^n  {-3 \choose n} {k \choose n} = \frac{k(k-1)(k-2)...(k-n+1)}{n!} {-3 \choose n}=\frac {(-3)(-4)(-5)...[-(n+2)]}{n!} (-1)^n \frac{5}{6^3}\sum_{n=0}^{\infty} {\frac {(-1)^n(3)(4)(5)...[(n+2)]}{n!}} (\frac{x}{6})^n  \frac{5}{2}\sum_{n=0}^{\infty} {\frac {(-1)^n(n+1)(n+2)x^n}{6^{n+3}}},"['multivariable-calculus', 'power-series']"
34,Why is the Lagrange multiplier considered to be a variable?,Why is the Lagrange multiplier considered to be a variable?,,"I'm learning the math behind SVMs (Support Vector Machines) from here . I understood the intuition behind Lagrange multipliers. But what I don't understand is that, why Lagrange multiplier is considered as a variable in Lagrangian? Isn't it just a constant by which the gradient vectors of the objective function (for which the optimization is to be done) and the subject function (constraint function) vary in length? Here is the equation of the Lagrangian: $$L(x, y, \lambda) = f(x,y) - \lambda g(x, y)$$ where $f(x, y)$ is objective function and $g(x, y)$ is constraint function. Now applying gradient both sides: $$\nabla L(x, y, \lambda) = \nabla f(x,y) - \lambda \nabla g(x, y)$$ Here while applying $\nabla$ , we've considered $\lambda$ as constant, even if we are considering Lagrangian ( $L$ ) as a function of $x$ , $y$ and $\lambda$ . What I'm missing here? Any intuitive explanation will help. Thanks! EDIT In Wikipedia page of this topic, $\lambda$ is considered as constant. Here is the direct quote from Wikipedia page. The constant  is required because although the two gradient vectors are parallel, the magnitudes of the gradient vectors are generally not equal. This constant is called the Lagrange multiplier. (In some conventions  is preceded by a minus sign).","I'm learning the math behind SVMs (Support Vector Machines) from here . I understood the intuition behind Lagrange multipliers. But what I don't understand is that, why Lagrange multiplier is considered as a variable in Lagrangian? Isn't it just a constant by which the gradient vectors of the objective function (for which the optimization is to be done) and the subject function (constraint function) vary in length? Here is the equation of the Lagrangian: where is objective function and is constraint function. Now applying gradient both sides: Here while applying , we've considered as constant, even if we are considering Lagrangian ( ) as a function of , and . What I'm missing here? Any intuitive explanation will help. Thanks! EDIT In Wikipedia page of this topic, is considered as constant. Here is the direct quote from Wikipedia page. The constant  is required because although the two gradient vectors are parallel, the magnitudes of the gradient vectors are generally not equal. This constant is called the Lagrange multiplier. (In some conventions  is preceded by a minus sign).","L(x, y, \lambda) = f(x,y) - \lambda g(x, y) f(x, y) g(x, y) \nabla L(x, y, \lambda) = \nabla f(x,y) - \lambda \nabla g(x, y) \nabla \lambda L x y \lambda \lambda","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
35,Finding the expected value of $\sqrt{x^2 + y^2}$,Finding the expected value of,\sqrt{x^2 + y^2},"Below is a problem from the Schaum book on Probability and Statistics. I did part a right but my answer for part b is wrong. I am hoping somebody can tell me where I went wrong. Thanks, Bob Problem: Let $X$ and $Y$ have join density function: $$f(x,y) = \begin{cases} cxy & \text{for } 0 < x < 1, 0 < y < 1\\ 0 & otherwise \\ \end{cases} \\ $$ Find (a) $E(X^2 + Y^2)$ , (b) $E(\sqrt{X^2+Y^2})$ . Answer: (a) The first thing we need to do is find $c$ . \begin{align*} \int_0^1 \int_0^1 cxy \, dy \, dx &= 1 \\ \int_0^1 \frac{cx}{2} \, dx &= 1 \\ \frac{c}{4} &= 1 \\ c &= 4 \\ E(X^2 + Y^2) &= \int_0^1 \int_0^1 4xy(x^2+y^2) \, dy \, dx \\ E(X^2 + Y^2) &= \int_0^1 \int_0^1 4x^3y + 4xy^3 \, dy \, dx \\ E(X^2 + Y^2) &= \int_0^1  2x^3y^2 + xy^4 \,\Big{|}_{y = 0}^{y=1} dx \\ E(X^2 + Y^2) &= \int_0^1 2x^3 + x \, dx = \frac{2x^4}{4} + \frac{x^2}{2} \Big{|}_0^1 = \frac{2}{4} + \frac{1}{2} \\ E(X^2 + Y^2) &= 1 \\ \end{align*} Part (b) \begin{align*} E(\sqrt{X^2+Y^2}) &= \int_0^1 \int_0^1 4xy \sqrt{x^2+y^2} \, dy \, dx \\ \end{align*} Now we use the substitution $u_1 = x^2 + y^2$ with $du_1 = 2y_1 dy$ . \begin{align*} E(\sqrt{X^2+Y^2}) &= \int_0^1 \int_{x^2}^{x^2+1} 2xu_1 \sqrt{u_1} \, du_1 \, dx = 	\int_0^1 \int_{x^2}^{x^2+1} 2xu_1^{\frac{3}{2}} \, du_1 \, dx \\ E(\sqrt{X^2+Y^2}) &=  \int_0^1 \frac{2xu_1^{ \frac{5}{2} }}{\frac{5}{2}} \Big{|}_{u_1 = x^2}^{u_1 = x^2 + 1} \, dx = 	 \int_0^1 \frac{4xu_1^{ \frac{5}{2} }}{5} \Big{|}_{u_1 = x^2}^{u_1 = x^2 + 1} \, dx \\ E(\sqrt{X^2+Y^2}) &= \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} - \frac{4x(x^2)^\frac{5}{2}}{5} \, dx \\ \int_0^1 \frac{4x(x^2)^\frac{5}{2}}{5} \, dx  &= \int_0^1 \frac{4x^6}{5} \, dx = \frac{4x^7}{35} \Big|_0^1 \\ \int_0^1 \frac{4x(x^2)^\frac{5}{2}}{5} \, dx  &= \frac{4}{35} \\ \end{align*} Now we need to perform the following integration: $$ \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  $$ \newline To perform this integration, we use the substitution $u_2 = x^2 + 1$ . \begin{align*} \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \int_1^2 \frac{ 2u^{\frac{5}{2} } }{5} \, du_2 \\ \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{2u^{ \frac{7}{2} }}{ \frac{5(7)} {2 }} \Big{|}_1^2 \\ \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{4u^{ \frac{7}{2} }}{ 35 } \Big{|}_1^2 \\ \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{4 (2^\frac{7}{2})}{35} - \frac{4}{35} \\ E(\sqrt{X^2+Y^2}) &= \frac{4 (2^\frac{7}{2})}{35} - \frac{4}{35} - \frac{4}{35} \\ E(\sqrt{X^2+Y^2}) &= \frac{ 32 \sqrt{2} - 8 }{35}  \\ E(\sqrt{X^2+Y^2}) &= \frac{8(4 \sqrt{2} - 1)}{35}  \\ \end{align*} However, the book's answer is: $$ \frac{8(2 \sqrt{2} - 1)} {15} $$","Below is a problem from the Schaum book on Probability and Statistics. I did part a right but my answer for part b is wrong. I am hoping somebody can tell me where I went wrong. Thanks, Bob Problem: Let and have join density function: Find (a) , (b) . Answer: (a) The first thing we need to do is find . Part (b) Now we use the substitution with . Now we need to perform the following integration: \newline To perform this integration, we use the substitution . However, the book's answer is:","X Y f(x,y) = \begin{cases}
cxy & \text{for } 0 < x < 1, 0 < y < 1\\
0 & otherwise \\
\end{cases} \\
 E(X^2 + Y^2) E(\sqrt{X^2+Y^2}) c \begin{align*}
\int_0^1 \int_0^1 cxy \, dy \, dx &= 1 \\
\int_0^1 \frac{cx}{2} \, dx &= 1 \\
\frac{c}{4} &= 1 \\
c &= 4 \\
E(X^2 + Y^2) &= \int_0^1 \int_0^1 4xy(x^2+y^2) \, dy \, dx \\
E(X^2 + Y^2) &= \int_0^1 \int_0^1 4x^3y + 4xy^3 \, dy \, dx \\
E(X^2 + Y^2) &= \int_0^1  2x^3y^2 + xy^4 \,\Big{|}_{y = 0}^{y=1} dx \\
E(X^2 + Y^2) &= \int_0^1 2x^3 + x \, dx = \frac{2x^4}{4} + \frac{x^2}{2} \Big{|}_0^1 = \frac{2}{4} + \frac{1}{2} \\
E(X^2 + Y^2) &= 1 \\
\end{align*} \begin{align*}
E(\sqrt{X^2+Y^2}) &= \int_0^1 \int_0^1 4xy \sqrt{x^2+y^2} \, dy \, dx \\
\end{align*} u_1 = x^2 + y^2 du_1 = 2y_1 dy \begin{align*}
E(\sqrt{X^2+Y^2}) &= \int_0^1 \int_{x^2}^{x^2+1} 2xu_1 \sqrt{u_1} \, du_1 \, dx =
	\int_0^1 \int_{x^2}^{x^2+1} 2xu_1^{\frac{3}{2}} \, du_1 \, dx \\
E(\sqrt{X^2+Y^2}) &=  \int_0^1 \frac{2xu_1^{ \frac{5}{2} }}{\frac{5}{2}} \Big{|}_{u_1 = x^2}^{u_1 = x^2 + 1} \, dx =
	 \int_0^1 \frac{4xu_1^{ \frac{5}{2} }}{5} \Big{|}_{u_1 = x^2}^{u_1 = x^2 + 1} \, dx \\
E(\sqrt{X^2+Y^2}) &= \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} - \frac{4x(x^2)^\frac{5}{2}}{5} \, dx \\
\int_0^1 \frac{4x(x^2)^\frac{5}{2}}{5} \, dx  &= \int_0^1 \frac{4x^6}{5} \, dx = \frac{4x^7}{35} \Big|_0^1 \\
\int_0^1 \frac{4x(x^2)^\frac{5}{2}}{5} \, dx  &= \frac{4}{35} \\
\end{align*}  \int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx   u_2 = x^2 + 1 \begin{align*}
\int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \int_1^2 \frac{ 2u^{\frac{5}{2} } }{5} \, du_2 \\
\int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{2u^{ \frac{7}{2} }}{ \frac{5(7)} {2 }} \Big{|}_1^2 \\
\int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{4u^{ \frac{7}{2} }}{ 35 } \Big{|}_1^2 \\
\int_0^1 \frac{4x(x^2+1)^\frac{5}{2}}{5} \, dx  &= \frac{4 (2^\frac{7}{2})}{35} - \frac{4}{35} \\
E(\sqrt{X^2+Y^2}) &= \frac{4 (2^\frac{7}{2})}{35} - \frac{4}{35} - \frac{4}{35} \\
E(\sqrt{X^2+Y^2}) &= \frac{ 32 \sqrt{2} - 8 }{35}  \\
E(\sqrt{X^2+Y^2}) &= \frac{8(4 \sqrt{2} - 1)}{35}  \\
\end{align*}  \frac{8(2 \sqrt{2} - 1)} {15} ","['probability', 'integration', 'multivariable-calculus']"
36,"Stationary points of $z(x, y)=\cos{x}e^{-x^2-y^2}$",Stationary points of,"z(x, y)=\cos{x}e^{-x^2-y^2}","This is a problem from one of past exams from my course. Consider a function $z(x, y)=\cos{x}e^{-x^2-y^2}$ . Show that stationary points of $z$ lie along the $x$ -axis, and satisfy $\tan{x}=k(x)$ . Give an expression for $k(x)$ . This one I managed to do, I got that for $z_x=z_y=0$ we have $y=0$ and $\tan{x}=-2x$ . Determine the nature of these stationary points of $z(x,y)$ . This one I have problems with. I know the conditions for maxima, minima etc. in two variables ( $D=z_{xx}z_{yy}-z_{xy}^2$ test), but I got into some horrible algebraic mess when trying to determine $D$ . Is there any ""cleverer"" method to determine the nature of the stationary points? For what it's worth, $z_x = -e^{-x^2-y^2}(2x\cos{x}+\sin{x})$ and $z_y = -2y\cos{x}e^{-x^2-y^2}.$","This is a problem from one of past exams from my course. Consider a function . Show that stationary points of lie along the -axis, and satisfy . Give an expression for . This one I managed to do, I got that for we have and . Determine the nature of these stationary points of . This one I have problems with. I know the conditions for maxima, minima etc. in two variables ( test), but I got into some horrible algebraic mess when trying to determine . Is there any ""cleverer"" method to determine the nature of the stationary points? For what it's worth, and","z(x, y)=\cos{x}e^{-x^2-y^2} z x \tan{x}=k(x) k(x) z_x=z_y=0 y=0 \tan{x}=-2x z(x,y) D=z_{xx}z_{yy}-z_{xy}^2 D z_x = -e^{-x^2-y^2}(2x\cos{x}+\sin{x}) z_y = -2y\cos{x}e^{-x^2-y^2}.",['calculus']
37,Reparametrization of a curve in opposite direction and find intersection of two particles travelling in opossite directions on the same curve.,Reparametrization of a curve in opposite direction and find intersection of two particles travelling in opossite directions on the same curve.,,"Hi I need help with this problem: Given $$r(t)=(e^t,e^t\cos t, e^t \sin t),\quad t\in[0,2\pi]$$ It represents the trajectory of a particle $P$ . Draw the curve. Reparametrize $r$ such that it represents a particle $Q$ moving  in the opposite direction. If $P$ starts at one end of the curve and $Q$ starts at the other end (at the same time), find the point of the trajectory when they both crash. 1.- I first tried to draw the curve, but I don't know how to exactly do it with those $e^t$ in there. I tried by factoring them out like $e^t(1, \cos t, \sin t)$ , so I think that the that it would be like an spiral, because of $e^t$ , but I really don't know how to draw it. 2.- Then I tried to reparametrize r, so I put $-t$ instead of $t$ on $r$ , but I don't know if I'm correct. 3.- I don't know how to find the point when both particles collide, I was thinking of setting both parametrizations as equal and then finding the time $t$ that satisfies that, but I'm not sure.","Hi I need help with this problem: Given It represents the trajectory of a particle . Draw the curve. Reparametrize such that it represents a particle moving  in the opposite direction. If starts at one end of the curve and starts at the other end (at the same time), find the point of the trajectory when they both crash. 1.- I first tried to draw the curve, but I don't know how to exactly do it with those in there. I tried by factoring them out like , so I think that the that it would be like an spiral, because of , but I really don't know how to draw it. 2.- Then I tried to reparametrize r, so I put instead of on , but I don't know if I'm correct. 3.- I don't know how to find the point when both particles collide, I was thinking of setting both parametrizations as equal and then finding the time that satisfies that, but I'm not sure.","r(t)=(e^t,e^t\cos t, e^t \sin t),\quad t\in[0,2\pi] P r Q P Q e^t e^t(1, \cos t, \sin t) e^t -t t r t","['multivariable-calculus', 'vector-analysis', 'curves']"
38,Implicit multivariable differentiation with Jacobian matrix,Implicit multivariable differentiation with Jacobian matrix,,"Two functions $u = u(x,y)$ and $v = v(x,y)$ are defined by the system of equations: $$u-v=x-y$$ $$yu-xv=1$$ The problem asks for the partial derivatives $\frac {\partial u} {\partial x},~ \frac {\partial u} {\partial y},~ \frac {\partial v} {\partial x},~\frac {\partial v} {\partial y}$ and the added picture below is showing my solution using a Jacobian matrix. We have received the our professor's solution and he solved the system manually without the Jacobian by solving the system simultaneously. It seems I have got the correct answer for both partials with respect to x. But the partials with respect to y are seemingly not. In his solution $\frac {\partial u}{\partial y}=\frac {u-x}{x+y}$ and $\frac {\partial v}{\partial y}=\frac{u+y}{x+y}$ I don't understand why my method worked for the partials with respect to x, but not with respect to y. How can that be? If both are wrong, then fine, there must be some mistake, but one is correct one is not with the same method? What am I missing? Thanks in advance for any help on this. $$\\$$","Two functions and are defined by the system of equations: The problem asks for the partial derivatives and the added picture below is showing my solution using a Jacobian matrix. We have received the our professor's solution and he solved the system manually without the Jacobian by solving the system simultaneously. It seems I have got the correct answer for both partials with respect to x. But the partials with respect to y are seemingly not. In his solution and I don't understand why my method worked for the partials with respect to x, but not with respect to y. How can that be? If both are wrong, then fine, there must be some mistake, but one is correct one is not with the same method? What am I missing? Thanks in advance for any help on this.","u = u(x,y) v = v(x,y) u-v=x-y yu-xv=1 \frac {\partial u} {\partial x},~ \frac {\partial u} {\partial y},~ \frac {\partial v} {\partial x},~\frac {\partial v} {\partial y} \frac {\partial u}{\partial y}=\frac {u-x}{x+y} \frac {\partial v}{\partial y}=\frac{u+y}{x+y} \\","['matrices', 'multivariable-calculus']"
39,Showing volume and surface integration is unaffected by the singularity at $\mathbf{r'}=\mathbf{r}$,Showing volume and surface integration is unaffected by the singularity at,\mathbf{r'}=\mathbf{r},"This question is not entirely similar to the question here . Please read this question and the reader will see it is obviously not the same. $\mathbf{M'}$ is a continuous vector field in volume $V'$ and $P$ be any point on the surface of $V'$ with position vector $\mathbf {r}$ PART I: Consider the expression: $\displaystyle \iiint_{V'} \mathbf{M(r')}.\nabla'  \left( \dfrac{1}{\left|   \mathbf{r}-\mathbf{r'}  \right|}       \right) dV'$ Take the origin of our coordinate system at $P$ (see the diagram) and write $dV'$ as spherical volume element. Then the above expression can be written as: $\displaystyle \iiint_{V'} \mathbf{M(r')}.\dfrac{\hat{r'}}{{r'}^2} {r'}^2\ \sin\theta\ d\theta\ d\phi\ dr' = \iiint_{V'} \left[ \mathbf{M(r')}.{\hat{r'}} \right] \sin\theta\ d\theta\ d\phi\ dr' \tag{1}$ The integrand is defined everywhere except at point $P$ where $r'=0$ and the integrand is $\frac{0}{0}$ . Since $\mathbf{M(r')}$ is finite everywhere, there is no blowing up of the integrand at any point. Therefore we can directly integrate equation $(1)$ just like ordinary integrals. PART II: Using the vector identity $\nabla.(\psi \mathbf{A})=\mathbf{A}.(\nabla \psi)+\psi (\nabla.\mathbf{A})$ : $\displaystyle \iiint_{V'}    \left[     \nabla' . \left( \dfrac{\mathbf{M'}}{\left|   \mathbf{r}-\mathbf{r'}  \right|} \right) \right]   dV' = \iiint_{V'} \mathbf{M'}.\nabla'  \left(  \dfrac{1}{\left|   \mathbf{r}-\mathbf{r'}  \right|}       \right) dV' + \iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{\left|   \mathbf{r}-\mathbf{r'} \right|}  dV'\tag{2}$ Now for simplicity, let's take the origin of our coordinate system at $P$ (see the diagram). Thus equation $(2)$ becomes: $\displaystyle \iiint_{V'}    \left[     \nabla' . \left( \dfrac{\mathbf{M'}}{r'} \right)      \right]   dV' = \iiint_{V'} \mathbf{M'}.\nabla'  \left(     \dfrac{1}{r'}       \right) dV' + \iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{r'}  dV' \tag{3}$ Now by writing $dV'$ as spherical volume element, equation $(3)$ becomes: \begin{align} \iiint_{V'}    \left[     \nabla' . \left( \dfrac{\mathbf{M'}}{r'} \right)      \right]   {r'}^2\ \sin\theta\ d\theta\ d\phi\ dr &=\iiint_{V'} \mathbf{M'}.\nabla'  \left(  \dfrac{1}{r'}  \right) {r'}^2\ \sin\theta\ d\theta\ d\phi\ dr'\\ &+\iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{r'}  {r'}^2\ \sin\theta\ d\theta\ d\phi\ dr'\\ &= \iiint_{V'} (\mathbf{M'}. \hat{r'}) \sin\theta\ d\theta\ d\phi\ dr' \\ &+ \iiint_{V'} (\nabla' . \mathbf{M'})\ r'\ \sin\theta\ d\theta\ d\phi\ dr' \tag{4} \end{align} In both terms: The integrands are defined everywhere except at point $P$ where $r'=0$ and the integrand $\frac{0}{0}$ . Since $\mathbf{M(r')}$ is finite everywhere, there is no blowing up of the integrands at any point. Therefore we can directly integrate both terms in the $RHS$ of equation $(4)$ just like ordinary integrals. Therefore we can directly integrate $LHS$ of equation $(4)$ just like ordinary integrals. Now my question is: Is Gauss divergence theorem applicable to $LHS$ of equation $(4)$ ? If it is applicable, then, since point $P$ lies on the surface, there would be a singularity in the equation: $\unicode{x222F}_{S'}    \left[    \left(\dfrac{\mathbf{M'}}{\left|  \mathbf{r}-\mathbf{r'}  \right|} \right)   . \hat{n}      \right]   dS'$ How to deal with it?","This question is not entirely similar to the question here . Please read this question and the reader will see it is obviously not the same. is a continuous vector field in volume and be any point on the surface of with position vector PART I: Consider the expression: Take the origin of our coordinate system at (see the diagram) and write as spherical volume element. Then the above expression can be written as: The integrand is defined everywhere except at point where and the integrand is . Since is finite everywhere, there is no blowing up of the integrand at any point. Therefore we can directly integrate equation just like ordinary integrals. PART II: Using the vector identity : Now for simplicity, let's take the origin of our coordinate system at (see the diagram). Thus equation becomes: Now by writing as spherical volume element, equation becomes: In both terms: The integrands are defined everywhere except at point where and the integrand . Since is finite everywhere, there is no blowing up of the integrands at any point. Therefore we can directly integrate both terms in the of equation just like ordinary integrals. Therefore we can directly integrate of equation just like ordinary integrals. Now my question is: Is Gauss divergence theorem applicable to of equation ? If it is applicable, then, since point lies on the surface, there would be a singularity in the equation: How to deal with it?","\mathbf{M'} V' P V' \mathbf {r} \displaystyle \iiint_{V'} \mathbf{M(r')}.\nabla'  \left(
\dfrac{1}{\left|   \mathbf{r}-\mathbf{r'}  \right|}       \right) dV' P dV' \displaystyle \iiint_{V'} \mathbf{M(r')}.\dfrac{\hat{r'}}{{r'}^2}
{r'}^2\ \sin\theta\ d\theta\ d\phi\ dr' = \iiint_{V'} \left[
\mathbf{M(r')}.{\hat{r'}} \right] \sin\theta\ d\theta\ d\phi\ dr' \tag{1} P r'=0 \frac{0}{0} \mathbf{M(r')} (1) \nabla.(\psi \mathbf{A})=\mathbf{A}.(\nabla \psi)+\psi (\nabla.\mathbf{A}) \displaystyle \iiint_{V'}    \left[     \nabla' . \left(
\dfrac{\mathbf{M'}}{\left|   \mathbf{r}-\mathbf{r'}  \right|} \right)
\right]   dV' = \iiint_{V'} \mathbf{M'}.\nabla'  \left(
 \dfrac{1}{\left|   \mathbf{r}-\mathbf{r'}  \right|}       \right) dV' +
\iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{\left|   \mathbf{r}-\mathbf{r'}
\right|}  dV'\tag{2} P (2) \displaystyle \iiint_{V'}    \left[     \nabla' . \left(
\dfrac{\mathbf{M'}}{r'} \right)      \right]   dV' = \iiint_{V'}
\mathbf{M'}.\nabla'  \left(     \dfrac{1}{r'}       \right) dV' +
\iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{r'}  dV' \tag{3} dV' (3) \begin{align}
\iiint_{V'}    \left[     \nabla' . \left(
\dfrac{\mathbf{M'}}{r'} \right)      \right]   {r'}^2\ \sin\theta\ d\theta\
d\phi\ dr
&=\iiint_{V'} \mathbf{M'}.\nabla'  \left(
 \dfrac{1}{r'}  \right) {r'}^2\ \sin\theta\ d\theta\ d\phi\ dr'\\
&+\iiint_{V'} \dfrac{\nabla' . \mathbf{M'}}{r'}  {r'}^2\ \sin\theta\ d\theta\
d\phi\ dr'\\
&= \iiint_{V'} (\mathbf{M'}. \hat{r'}) \sin\theta\
d\theta\ d\phi\ dr' \\
&+ \iiint_{V'} (\nabla' . \mathbf{M'})\ r'\ \sin\theta\
d\theta\ d\phi\ dr'
\tag{4}
\end{align} P r'=0 \frac{0}{0} \mathbf{M(r')} RHS (4) LHS (4) LHS (4) P \unicode{x222F}_{S'}    \left[    \left(\dfrac{\mathbf{M'}}{\left| 
\mathbf{r}-\mathbf{r'}  \right|} \right)   . \hat{n}      \right]   dS'","['multivariable-calculus', 'volume', 'surface-integrals', 'divergence-operator', 'singularity']"
40,Minimum value of expression having $2$ variables,Minimum value of expression having  variables,2,"Minimum value of $$\bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2$$ for real $x\geq 0,y\in[-2,2]$ Try: Using Partial derivative $$f(x,y) = \bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2$$ $$\frac{df(x,y)}{dx}=2\bigg(x-4-\sqrt{4-y^2}\bigg)+\frac{4}{\sqrt{x}}\bigg(4\sqrt{x}-y\bigg)$$ $$\frac{df(x,y)}{dy}=-2y\frac{\bigg(x-4-\sqrt{4-y^2}\bigg)}{\sqrt{4-y^2}}-2\bigg(4\sqrt{x}-y\bigg)$$ Put $\displaystyle \frac{df}{dx} =0$ and $\displaystyle \frac{df}{dy}=0$ But these $2$ equation is tedious work Could some help me how to solve it , Thanks in advance","Minimum value of for real Try: Using Partial derivative Put and But these equation is tedious work Could some help me how to solve it , Thanks in advance","\bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2 x\geq 0,y\in[-2,2] f(x,y) = \bigg(x-4-\sqrt{4-y^2}\bigg)^2+\bigg(4\sqrt{x}-y\bigg)^2 \frac{df(x,y)}{dx}=2\bigg(x-4-\sqrt{4-y^2}\bigg)+\frac{4}{\sqrt{x}}\bigg(4\sqrt{x}-y\bigg) \frac{df(x,y)}{dy}=-2y\frac{\bigg(x-4-\sqrt{4-y^2}\bigg)}{\sqrt{4-y^2}}-2\bigg(4\sqrt{x}-y\bigg) \displaystyle \frac{df}{dx} =0 \displaystyle \frac{df}{dy}=0 2",['multivariable-calculus']
41,Double integral of $xe^y$ over the area inside $x^2 + y^2 = 1$ but outside $x^2 + y^2 = 2y$,Double integral of  over the area inside  but outside,xe^y x^2 + y^2 = 1 x^2 + y^2 = 2y,"My question goes like this: Let R be the area inside $x^2 + y^2 = 1$ and outside $x^2 + y^2 = 2y$ . Calculate $\int\int_R xe^y dA$ . How sould I approach this question? I tried to use integration with polar cordinates, but then I end up with the following very complicated integral when inserting $x = r \cos\theta$ and $y = r \sin \theta$ : $$\int_0^{\frac{\pi}{4}} \int_1^{2\sin\theta} r^2 \cos\theta e^{r \sin\theta} dr d\theta$$ Is this a wrong approach, or have I simply done something wrong? Should i use substitution of variables instead? If yes, any suggestions as to which substitution? All help is very much apprectiated!","My question goes like this: Let R be the area inside and outside . Calculate . How sould I approach this question? I tried to use integration with polar cordinates, but then I end up with the following very complicated integral when inserting and : Is this a wrong approach, or have I simply done something wrong? Should i use substitution of variables instead? If yes, any suggestions as to which substitution? All help is very much apprectiated!",x^2 + y^2 = 1 x^2 + y^2 = 2y \int\int_R xe^y dA x = r \cos\theta y = r \sin \theta \int_0^{\frac{\pi}{4}} \int_1^{2\sin\theta} r^2 \cos\theta e^{r \sin\theta} dr d\theta,"['calculus', 'integration', 'multivariable-calculus', 'indefinite-integrals']"
42,Invertibility of a function (Surjective/everywhere frechet),Invertibility of a function (Surjective/everywhere frechet),,"Let $U,V \subseteq \mathbb{R^n}$ be open. Suppose that $f: U \rightarrow V$ is surjective and is everywhere Frechet differentiable with $df(x)$ invertible for all $x \in U$ . Is $f$ invertible? I know that one of the conditions for the inverse function theorem is that $f$ is continuously differentiable, which we don't necessarily have. If the question didn't give us that $f$ was surjective, I'd have concluded that $f$ doesn't have to be invertible, but I'm just wondering what the question is trying to tell us by mentioning surjectivity of $f$ , if it implies that $f$ is continuously differentiable I don't see how.","Let be open. Suppose that is surjective and is everywhere Frechet differentiable with invertible for all . Is invertible? I know that one of the conditions for the inverse function theorem is that is continuously differentiable, which we don't necessarily have. If the question didn't give us that was surjective, I'd have concluded that doesn't have to be invertible, but I'm just wondering what the question is trying to tell us by mentioning surjectivity of , if it implies that is continuously differentiable I don't see how.","U,V \subseteq \mathbb{R^n} f: U \rightarrow V df(x) x \in U f f f f f f","['multivariable-calculus', 'functions', 'derivatives']"
43,give 5 other equivalent iterated triple integrals,give 5 other equivalent iterated triple integrals,,"I am given the following integral: $$\int_0^2\int_0^{y^3}\int_0^{y^2}f(x,y,z) dzdxdy $$ I was successfully able to rewrite this in its dzdydx, dxdzdy, and dxdydz forms, but I'm having a hard time understanding the iterated triple integrals where dy is the inner integral. The solution for dydzdx is as follows: $$\int_0^8\int_0^{x^{2/3}}\int_{x^{1/3}}^2f(x,y,z)dydzdx\,+\,\int_0^8\int_{x^{2/3}}^4\int_{z^{1/2}}^2f(x,y,z)dydzdx $$ I don't understand why the integral was split. How do I figure out what this shape looks like in the zx plane in order to even know that it needed to be split?","I am given the following integral: I was successfully able to rewrite this in its dzdydx, dxdzdy, and dxdydz forms, but I'm having a hard time understanding the iterated triple integrals where dy is the inner integral. The solution for dydzdx is as follows: I don't understand why the integral was split. How do I figure out what this shape looks like in the zx plane in order to even know that it needed to be split?","\int_0^2\int_0^{y^3}\int_0^{y^2}f(x,y,z) dzdxdy  \int_0^8\int_0^{x^{2/3}}\int_{x^{1/3}}^2f(x,y,z)dydzdx\,+\,\int_0^8\int_{x^{2/3}}^4\int_{z^{1/2}}^2f(x,y,z)dydzdx ","['calculus', 'integration', 'multivariable-calculus']"
44,Creating an integral to represent the volume of the intersection of two balls in cartesian coordinates,Creating an integral to represent the volume of the intersection of two balls in cartesian coordinates,,The question states: Let $A$ be the intersection of the balls $x^2+y^2+z^2\leq 9$ and $x^2+y^2+(z-8)^2\leq 49$ I am asked to just set up the iterated triple integral that represents the volume of $A$ in cartesian coordinates. What I am able to determine so far is that for the equation $x^2+y^2+z^2\leq 9 $ : $-\sqrt{9-x^2-y^2}\leq z \leq \sqrt{9-x^2-y^2}$ $-\sqrt{9-x^2}\leq y \leq \sqrt{9-x^2}$ $-3\leq x \leq 3$ If I were to set up this integral it would be: $\int_{-3}^{3}\int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \int_{-\sqrt{9-x^2-y^2}}^{\sqrt{9-x^2-y^2}} dzdydx$ But I don't know how I'm supposed to set up the intersection of the two balls? I was thinking of setting the two equations equal to each other so that $x^2+y^2\leq 9-z^2$ and $x^2+y^2\leq 49-(z-8)^2$ so then I have $9-z^2=49-(z-8)^2$ Solving for $z$ I get $z=3/2$ but I don't know what to do with this information.,The question states: Let be the intersection of the balls and I am asked to just set up the iterated triple integral that represents the volume of in cartesian coordinates. What I am able to determine so far is that for the equation : If I were to set up this integral it would be: But I don't know how I'm supposed to set up the intersection of the two balls? I was thinking of setting the two equations equal to each other so that and so then I have Solving for I get but I don't know what to do with this information.,A x^2+y^2+z^2\leq 9 x^2+y^2+(z-8)^2\leq 49 A x^2+y^2+z^2\leq 9  -\sqrt{9-x^2-y^2}\leq z \leq \sqrt{9-x^2-y^2} -\sqrt{9-x^2}\leq y \leq \sqrt{9-x^2} -3\leq x \leq 3 \int_{-3}^{3}\int_{-\sqrt{9-x^2}}^{\sqrt{9-x^2}} \int_{-\sqrt{9-x^2-y^2}}^{\sqrt{9-x^2-y^2}} dzdydx x^2+y^2\leq 9-z^2 x^2+y^2\leq 49-(z-8)^2 9-z^2=49-(z-8)^2 z z=3/2,"['multivariable-calculus', 'multiple-integral']"
45,"Trying to solve this triple integral: $\iiint (x-1)(y-1) \,dx\,dy\,dz$",Trying to solve this triple integral:,"\iiint (x-1)(y-1) \,dx\,dy\,dz","Here's the question $$\iiint (x-1)(y-1) \,dx\,dy\,dz.$$ I am asked to evaluate this integral over the region $$D:=\left \{ (x,y,z) \in\mathbb{R}^3 :x^2+y^2 \leq z \leq 2x+2y+2 \right \}.$$ There are the bounds of integration in set D (the variable $z$ is isolated) and well I tried to find the solution of this integral : \begin{align*} &\iint_{Pr_{(y,x)}(D)}\int_{x^2+y^2}^{2x+2y+2}(x-1)(y-1) \,dx\, dy\, dz \\ =&\iint_{Pr_{(y,x)}(D)}\int_{x^2+y^2}^{2x+2y+2}(xy-x-y+1) \,dx\, dy\, dz, \end{align*} and integrate only with respect to $z.$ I have that: \begin{align*} \int_{x^2+y^2}^{2x+2y+2}(xy-x-y+1) \,dz&=(xy-x-y+1)*(2x+2y+2-(x^2+y^2)) \\ &=3x^2y+3xy^2-2x^3y-2xy^3-3x^2+x^3-2xy-3y^2+y^3+2. \end{align*} It looks like this way is too long. The second thing that came to mind when I saw the set $D$ was to apply cylindrical coordinates, but this doesn't make easier the left member of the set $D.$ What can I do or what have I done wrong up until now? Any support for this question would be appreciated.","Here's the question I am asked to evaluate this integral over the region There are the bounds of integration in set D (the variable is isolated) and well I tried to find the solution of this integral : and integrate only with respect to I have that: It looks like this way is too long. The second thing that came to mind when I saw the set was to apply cylindrical coordinates, but this doesn't make easier the left member of the set What can I do or what have I done wrong up until now? Any support for this question would be appreciated.","\iiint (x-1)(y-1) \,dx\,dy\,dz. D:=\left \{ (x,y,z) \in\mathbb{R}^3 :x^2+y^2 \leq z \leq 2x+2y+2 \right \}. z \begin{align*}
&\iint_{Pr_{(y,x)}(D)}\int_{x^2+y^2}^{2x+2y+2}(x-1)(y-1) \,dx\, dy\, dz \\
=&\iint_{Pr_{(y,x)}(D)}\int_{x^2+y^2}^{2x+2y+2}(xy-x-y+1) \,dx\, dy\, dz,
\end{align*} z. \begin{align*}
\int_{x^2+y^2}^{2x+2y+2}(xy-x-y+1) \,dz&=(xy-x-y+1)*(2x+2y+2-(x^2+y^2)) \\
&=3x^2y+3xy^2-2x^3y-2xy^3-3x^2+x^3-2xy-3y^2+y^3+2.
\end{align*} D D.","['integration', 'multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
46,Find any local max or min of $x^2+y^2+z^2$ s.t $x+y+z=1$ and $3x+y+z=5$,Find any local max or min of  s.t  and,x^2+y^2+z^2 x+y+z=1 3x+y+z=5,"Find any local max or min of \begin{align} f(x,y,z)=x^2+y^2+z^2 && (1) \end{align} such that \begin{align} x+y+z=1 && (2)\\ 3x+y+z=5 && (3) \end{align} My attempt . Let $L(x,y,z,\lambda_1, \lambda_2)= f(x,y,z)+\lambda_2 (x+y+z-1) + \lambda_1 (3x+y+z-5)$ $L_x=2x+ 3 \lambda_1 + \lambda_2 =0$ $L_y=2y+ \lambda_1 + \lambda_2=0$ $L_z=2z+\lambda_1 + \lambda_2=0$ Solve for $x,y,z$ we get: $x=\frac{-3 \lambda_1 - \lambda_2}{2}$ $z=y=\frac{-\lambda_1 - \lambda_2}{2}$ with the use of $(2)$ and $(3)$ $\implies$ $x=2$ $y=z= \frac{-1}{2}$ so the stationary point is $(x,y,z)=(2, \frac{-1}{2},\frac{-1}{2})$ The Hessian of $L$ gives a postive definite matrix for all $(x,y,z)$ thus $(x,y,z)=(2, \frac{-1}{2},\frac{-1}{2})$ is the only local minimizer of $f$ and there is no maximizors of $f$ . Is this correct?",Find any local max or min of such that My attempt . Let Solve for we get: with the use of and so the stationary point is The Hessian of gives a postive definite matrix for all thus is the only local minimizer of and there is no maximizors of . Is this correct?,"\begin{align}
f(x,y,z)=x^2+y^2+z^2 && (1)
\end{align} \begin{align}
x+y+z=1 && (2)\\
3x+y+z=5 && (3)
\end{align} L(x,y,z,\lambda_1, \lambda_2)= f(x,y,z)+\lambda_2 (x+y+z-1) + \lambda_1 (3x+y+z-5) L_x=2x+ 3 \lambda_1 + \lambda_2 =0 L_y=2y+ \lambda_1 + \lambda_2=0 L_z=2z+\lambda_1 + \lambda_2=0 x,y,z x=\frac{-3 \lambda_1 - \lambda_2}{2} z=y=\frac{-\lambda_1 - \lambda_2}{2} (2) (3) \implies x=2 y=z= \frac{-1}{2} (x,y,z)=(2, \frac{-1}{2},\frac{-1}{2}) L (x,y,z) (x,y,z)=(2, \frac{-1}{2},\frac{-1}{2}) f f","['calculus', 'multivariable-calculus', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier']"
47,Transformation Jacobian for integration by substitution in single and multiple variables?,Transformation Jacobian for integration by substitution in single and multiple variables?,,"I wonder how to reconcile the transformation rules for integration by substitution in a single variable vs several variables . In the single-variable case the Jacobian factor $\varphi'(x)$ is just a derivative and may take on negative as well as positive values depending on the particular function and integration range. However, in the multi-variable case the Jacobian determinant factor Det $(D\varphi)(u)$ is taken in absolute value, which allows this transformation Jacobian to take on positive values only. If we take the multi-variable expression and consider the single-variable special case, then the partial derivative determinant again reduces to simply $\varphi'(x)$ . However, the additional absolute value of the determinant seems to introduce a discrepancy between the purely single-variable treatment and this special case. Moreover, it is easy to find examples showing that the absolute value indeed should not be there in the single-variable case. How to reconcile the two? Is the absolute value truly needed and correct in the multi-variable case? EDIT: Since some doubt was expressed in the comments on whether $\varphi'(x)$ may or may not be negative in the single-variable case, consider the following example: $$\int_0^1 du\,u^2=\frac{1}{3}$$ Now take $u\equiv\varphi(x)=-x$ . If we consider the absolute value of the Jacobian to be true, we get: $$\int_{0}^{-1}\left|\frac{d\varphi(x)}{dx}\right|dx\,(-x)^2=\int_0^{-1}dx\,x^2=-\frac{1}{3}$$ which gives a wrong result; while if we do not take the absolute value, we recover: $$\int_{0}^{-1} \frac{d\varphi(x)}{dx}dx\,(-x)^2=-\int_0^{-1}dx\,x^2=\frac{1}{3}$$ which is correct.","I wonder how to reconcile the transformation rules for integration by substitution in a single variable vs several variables . In the single-variable case the Jacobian factor is just a derivative and may take on negative as well as positive values depending on the particular function and integration range. However, in the multi-variable case the Jacobian determinant factor Det is taken in absolute value, which allows this transformation Jacobian to take on positive values only. If we take the multi-variable expression and consider the single-variable special case, then the partial derivative determinant again reduces to simply . However, the additional absolute value of the determinant seems to introduce a discrepancy between the purely single-variable treatment and this special case. Moreover, it is easy to find examples showing that the absolute value indeed should not be there in the single-variable case. How to reconcile the two? Is the absolute value truly needed and correct in the multi-variable case? EDIT: Since some doubt was expressed in the comments on whether may or may not be negative in the single-variable case, consider the following example: Now take . If we consider the absolute value of the Jacobian to be true, we get: which gives a wrong result; while if we do not take the absolute value, we recover: which is correct.","\varphi'(x) (D\varphi)(u) \varphi'(x) \varphi'(x) \int_0^1 du\,u^2=\frac{1}{3} u\equiv\varphi(x)=-x \int_{0}^{-1}\left|\frac{d\varphi(x)}{dx}\right|dx\,(-x)^2=\int_0^{-1}dx\,x^2=-\frac{1}{3} \int_{0}^{-1} \frac{d\varphi(x)}{dx}dx\,(-x)^2=-\int_0^{-1}dx\,x^2=\frac{1}{3}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'substitution']"
48,Finding parametric equations of the tangent line to a curve of intersection,Finding parametric equations of the tangent line to a curve of intersection,,"The question asks to find the parametric equations of the tangent line to the curve of intersection of the surface $z=2\sqrt{9-\frac{x^2}{2}-y^2}$ and the plane $x=2$ at the point $(2,\sqrt{3},4)$ I've seen solutions to questions similar to this one but they use the gradient notation (the triangle) which I am unfamiliar with. How do you do this question? I was thinking of starting with the partial derivatives w.r.t $x$ and $y$ but I'm not sure where to go from there. Then I thought of ignoring the partial derivative of $x$ since the intersection involves $x=2$ so $x$ is unchanging, so there's that. Thanks!","The question asks to find the parametric equations of the tangent line to the curve of intersection of the surface and the plane at the point I've seen solutions to questions similar to this one but they use the gradient notation (the triangle) which I am unfamiliar with. How do you do this question? I was thinking of starting with the partial derivatives w.r.t and but I'm not sure where to go from there. Then I thought of ignoring the partial derivative of since the intersection involves so is unchanging, so there's that. Thanks!","z=2\sqrt{9-\frac{x^2}{2}-y^2} x=2 (2,\sqrt{3},4) x y x x=2 x","['calculus', 'multivariable-calculus', 'parametric', 'curves']"
49,"flux of $\langle x,y,z^2\rangle$ across unit sphere",flux of  across unit sphere,"\langle x,y,z^2\rangle","I'm trying to compute $\iint_S F\cdot$ d S where $F=\langle x,y,z^2\rangle$ and $S$ is the unit sphere centered at the origin. Here's my attempt: On the sphere we can describe any point by $r(\phi,\theta)=\langle \sin\phi \cos\theta,\sin\phi \cos\theta, \cos\theta\rangle$ and the outward normal vector to $S$ is given by $n=r.$ Therefore, $$\iint_S F\cdot dS = \iint_D F(r(\phi,\theta))\cdot n dA$$ $$=\iint_D \langle \sin\phi\cos\theta,\sin\phi\sin\theta,\cos^2\theta\rangle\cdot \langle \sin\phi\cos\theta,\sin\phi\sin\theta,\cos\theta\rangle dA$$ $$=\iint_D \sin^2\phi(\cos^2\theta + \sin^2\theta)+\cos^3d\theta dA = \int_0^{\pi}\int_0^{2\pi} \sin^2\phi +\cos^3\theta d\theta d\phi = \pi^2$$ however the answer is $\frac{8}{3}\pi$ and the last equals sign is correct (used Wolfram to confirm), so I guess I did something wrong in the setup.","I'm trying to compute d S where and is the unit sphere centered at the origin. Here's my attempt: On the sphere we can describe any point by and the outward normal vector to is given by Therefore, however the answer is and the last equals sign is correct (used Wolfram to confirm), so I guess I did something wrong in the setup.","\iint_S F\cdot F=\langle x,y,z^2\rangle S r(\phi,\theta)=\langle \sin\phi \cos\theta,\sin\phi \cos\theta, \cos\theta\rangle S n=r. \iint_S F\cdot dS = \iint_D F(r(\phi,\theta))\cdot n dA =\iint_D \langle \sin\phi\cos\theta,\sin\phi\sin\theta,\cos^2\theta\rangle\cdot \langle
\sin\phi\cos\theta,\sin\phi\sin\theta,\cos\theta\rangle dA =\iint_D \sin^2\phi(\cos^2\theta + \sin^2\theta)+\cos^3d\theta dA = \int_0^{\pi}\int_0^{2\pi} \sin^2\phi +\cos^3\theta d\theta d\phi = \pi^2 \frac{8}{3}\pi","['multivariable-calculus', 'proof-verification', 'vector-analysis']"
50,Are line integral function of a function differentiable?,Are line integral function of a function differentiable?,,"I was reading Tom Apostol calculus volume 2 and came across theorem 10.4 FIRST FUNDAMENTAL THEOREM FOR LINE INTEGRALS (page 338) The theorem roughly stated: $\vec f$ be a vector field that is continuous on an open connected set S in $R^n$ and assume that the line integral of $\vec f$ is independent of path in S . $\vec a$ be  a fixed point of S and define a scalar field $\phi$ on S by the equation $$\phi(\vec x)=\int_{\vec a}^\vec x\vec f.d\vec \alpha$$ Where $\vec\alpha$ is any piecewise smooth path in S joining $\vec a $ and $\vec x $ .Then the gradient of $\phi$ exists and is equal to $\vec f $ . My question is in the proof it was proved that the $D_t \phi(\vec x)=f_t$ for all k in {1,2,3,4,5,....,n}, Whereas the theorem states gradient should exist that is the function $\phi(\vec x)$ should be differentiable, if so how to PROVE IT ?","I was reading Tom Apostol calculus volume 2 and came across theorem 10.4 FIRST FUNDAMENTAL THEOREM FOR LINE INTEGRALS (page 338) The theorem roughly stated: be a vector field that is continuous on an open connected set S in and assume that the line integral of is independent of path in S . be  a fixed point of S and define a scalar field on S by the equation Where is any piecewise smooth path in S joining and .Then the gradient of exists and is equal to . My question is in the proof it was proved that the for all k in {1,2,3,4,5,....,n}, Whereas the theorem states gradient should exist that is the function should be differentiable, if so how to PROVE IT ?",\vec f R^n \vec f \vec a \phi \phi(\vec x)=\int_{\vec a}^\vec x\vec f.d\vec \alpha \vec\alpha \vec a  \vec x  \phi \vec f  D_t \phi(\vec x)=f_t \phi(\vec x),"['real-analysis', 'calculus', 'functional-analysis', 'multivariable-calculus', 'vector-analysis']"
51,Looking for another way to calculate the integral $\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A$,Looking for another way to calculate the integral,\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A,"Here, I have a little unpleasant way to calculate the following double integral $$\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A$$ where $D$ is the square area $D=\{(x,y)\in\mathbb{R}^2: 0 \le x \le \pi/2, 0 \le y \le \pi/2\}$ my attempt: with the symmetry of the area we know $$I=\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A=\iint_{D}{\sin(y)e^{\sin(x)\sin(y)}}\text{d}A$$ thus $$I=\frac1{2}\iint_{D}{(\sin(x)+\sin(y))e^{\sin(x)\sin(y)}}\text{d}A$$ with the substitution $u=\sin(x)$ and $v=\sin(y)$ , we have $$I=\frac1{2}\iint_{D^{*}}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A$$ and now the integral area is $D^{*}=\{(u,v)\in\mathbb{R}^2: 0 \le u \le 1, 0 \le v \le 1\}$ notice that the function $\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}$ also holds a symmetric form, thus $$I=\iint_{D^{*}\cap(u\le v)}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A=\iint_{D^{*}\cap(u\ge v)}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A$$ so I can only foucs on area $D^{*}\cap(u\ge v)$ , set another substitution $$u+v=\alpha$$ $$uv=\beta$$ under the condition $u\ge v$ , the determinant of Jacobian matrix writes $$|J(\alpha,\beta)|=\frac1{\sqrt{\alpha^2-4\beta}}$$ and $$\frac1{\sqrt{(1-u^2)(1-v^2)}}=\frac1{\sqrt{(1+\beta)^2-\alpha^2}}$$ the integral area change to $\{(\alpha,\beta)\in\mathbb{R}^2: 2\sqrt{\beta} \le \alpha \le 1+\beta, 0 \le \beta \le 1\}$ , so $$\begin{align} I&=\int_{0}^{1}\int_{2\sqrt{\beta}}^{1+\beta}{\frac{\alpha}{\sqrt{((1+\beta)^2-\alpha^2)(\alpha^2-4\beta)}}e^{\beta}}\text{d}\alpha \text{d}\beta \\&=\int_{0}^{1}\left(-\tan^{-1}\sqrt{\frac{(1+\beta)^2-\alpha^2}{\alpha^2-4\beta}}\right)\biggr|_{\alpha=2\sqrt{\beta}}^{1+\beta} e^{\beta}\text{d}\beta \\&=\frac{\pi}{2}(e-1) \end{align}$$ obviously, I choose an inconvenient way in the second half of this calculation, but I still can not come up with another good method. thanks in advance for any suggestion!","Here, I have a little unpleasant way to calculate the following double integral where is the square area my attempt: with the symmetry of the area we know thus with the substitution and , we have and now the integral area is notice that the function also holds a symmetric form, thus so I can only foucs on area , set another substitution under the condition , the determinant of Jacobian matrix writes and the integral area change to , so obviously, I choose an inconvenient way in the second half of this calculation, but I still can not come up with another good method. thanks in advance for any suggestion!","\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A D D=\{(x,y)\in\mathbb{R}^2: 0 \le x \le \pi/2, 0 \le y \le \pi/2\} I=\iint_{D}{\sin(x)e^{\sin(x)\sin(y)}}\text{d}A=\iint_{D}{\sin(y)e^{\sin(x)\sin(y)}}\text{d}A I=\frac1{2}\iint_{D}{(\sin(x)+\sin(y))e^{\sin(x)\sin(y)}}\text{d}A u=\sin(x) v=\sin(y) I=\frac1{2}\iint_{D^{*}}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A D^{*}=\{(u,v)\in\mathbb{R}^2: 0 \le u \le 1, 0 \le v \le 1\} \frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv} I=\iint_{D^{*}\cap(u\le v)}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A=\iint_{D^{*}\cap(u\ge v)}{\frac{u+v}{\sqrt{(1-u^2)(1-v^2)}}e^{uv}}\text{d}A D^{*}\cap(u\ge v) u+v=\alpha uv=\beta u\ge v |J(\alpha,\beta)|=\frac1{\sqrt{\alpha^2-4\beta}} \frac1{\sqrt{(1-u^2)(1-v^2)}}=\frac1{\sqrt{(1+\beta)^2-\alpha^2}} \{(\alpha,\beta)\in\mathbb{R}^2: 2\sqrt{\beta} \le \alpha \le 1+\beta, 0 \le \beta \le 1\} \begin{align}
I&=\int_{0}^{1}\int_{2\sqrt{\beta}}^{1+\beta}{\frac{\alpha}{\sqrt{((1+\beta)^2-\alpha^2)(\alpha^2-4\beta)}}e^{\beta}}\text{d}\alpha \text{d}\beta
\\&=\int_{0}^{1}\left(-\tan^{-1}\sqrt{\frac{(1+\beta)^2-\alpha^2}{\alpha^2-4\beta}}\right)\biggr|_{\alpha=2\sqrt{\beta}}^{1+\beta} e^{\beta}\text{d}\beta
\\&=\frac{\pi}{2}(e-1)
\end{align}","['calculus', 'multivariable-calculus']"
52,Check the differentiability of the given function.,Check the differentiability of the given function.,,"Let $M_n(\mathbb{R})$ denote the space  of  all $n\times n $ real   matrices  identified  with  Euclidean space $\mathbb{R^{n^2}}$ . Fixed   a column vector $x \neq 0$ in $\mathbb{ R^n}$ . Define $f :  M_n(\mathbb{R})  \rightarrow \mathbb{R}$ by $f(A) = \langle A^2x,x  \rangle$ . Check whether given function is differntiable or not? When I took $    A=   \left[ {\begin{array}{cc}    x_1 & x_2 \\    x_3 & x_4\\   \end{array} } \right]  $ and $x=\left[ {\begin{array}{cc}    a \\    b\\   \end{array} } \right]$ . I got $f(A)$ as a polynomial of four variables. I know that the polynomial function is always differentiable. How do I prove it for $n\times n$ matrix case?Without expanding the inner product How do I prove the given function is differentiable?",Let denote the space  of  all real   matrices  identified  with  Euclidean space . Fixed   a column vector in . Define by . Check whether given function is differntiable or not? When I took and . I got as a polynomial of four variables. I know that the polynomial function is always differentiable. How do I prove it for matrix case?Without expanding the inner product How do I prove the given function is differentiable?,"M_n(\mathbb{R}) n\times n  \mathbb{R^{n^2}} x \neq 0 \mathbb{ R^n} f :
 M_n(\mathbb{R})  \rightarrow \mathbb{R} f(A) = \langle A^2x,x
 \rangle 
   A=
  \left[ {\begin{array}{cc}
   x_1 & x_2 \\
   x_3 & x_4\\
  \end{array} } \right]
  x=\left[ {\begin{array}{cc}
   a \\
   b\\
  \end{array} } \right] f(A) n\times n",['multivariable-calculus']
53,Equivalent Definition of Strong Convexity,Equivalent Definition of Strong Convexity,,"Let $f \in \mathcal{C}^2(\mathbb{R}^n).$ Recall that we defined $f$ to be strongly convex if there exists $\beta > 0$ such that $\langle D^{2}f|_{x}y,y\rangle\ge\beta$ for every $x, y \in \mathbb{R}^{n}$ or equivalently if the function $g(x)=f(x)-\frac{\beta}{2}\|x\|^{2}$ is convex. Show that if there exists $\gamma>0$ such that $$f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)-\gamma t(1-t)\|x-y\|^{2}\tag{*}$$ for all $x,y \in \mathbb{R}^{n}, t \in [0,1]$ then the function $g(x)=f(x)-\gamma\|x\|^2$ is convex. I tried showing that the function $g(x)=f(x)-\gamma \|x\|^2$ is convex using the above assumption but did not strike any luck. Any hints on how to proceed for this problem are appreciated. Than you for you help.",Let Recall that we defined to be strongly convex if there exists such that for every or equivalently if the function is convex. Show that if there exists such that for all then the function is convex. I tried showing that the function is convex using the above assumption but did not strike any luck. Any hints on how to proceed for this problem are appreciated. Than you for you help.,"f \in \mathcal{C}^2(\mathbb{R}^n). f \beta > 0 \langle D^{2}f|_{x}y,y\rangle\ge\beta x, y \in \mathbb{R}^{n} g(x)=f(x)-\frac{\beta}{2}\|x\|^{2} \gamma>0 f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)-\gamma t(1-t)\|x-y\|^{2}\tag{*} x,y \in \mathbb{R}^{n}, t \in [0,1] g(x)=f(x)-\gamma\|x\|^2 g(x)=f(x)-\gamma \|x\|^2","['real-analysis', 'multivariable-calculus', 'convex-analysis', 'convex-optimization']"
54,Looking for a counter-example in Multi-variable Calculus,Looking for a counter-example in Multi-variable Calculus,,"I looking for a proof or a counterexample (more likely) for the following Claim: Claim. Let $\,\boldsymbol{f}:U\to\mathbb R^n$ , where $U\subset\mathbb R^m$ open, and $\boldsymbol{a}\in U$ , be a function with the property that, there exists a matrix $A\in\mathbb R^{n\times m}$ , such that for every smooth curve $\boldsymbol{\gamma}: I\to U$ , where $I$ is an open interval and $0\in I$ , with $\boldsymbol{\gamma}(0)=\boldsymbol{a}$ and $\boldsymbol{\gamma}'(0)\ne \boldsymbol{0}$ , the composition $h(t)=\boldsymbol{f}\big(\boldsymbol{\gamma}(t)\big)$ is differentiable at $t=0$ and $h'(0)=A\boldsymbol{\gamma}'(0)$ . Then $\boldsymbol{f}$ is differentiable at $\boldsymbol{x}=\boldsymbol{a}$ . So far, available are the following examples (which do not consist counterexamples of the above): A. $f(x,y)=\left\{\begin{array}{ccc}\displaystyle \frac{xy^2}{x^2+y^6} & if & (x,y)\ne (0,0), \\ 0 & if & (x,y)=(0,0). \end{array}\right.$ For this $f$ all the directional derivatives exist, but $f$ is not even continuous at $(x,y)=(0,0)$ B. $f(x,y)=\left\{\begin{array}{ccc}\displaystyle \frac{x^3}{x^2+y^2} & if & (x,y)\ne (0,0), \\ 0 & if & (x,y)=(0,0). \end{array}\right.$ For this $f$ , the composition $h=f\circ\boldsymbol\gamma$ is differentiable at $t=0$ and it is a function of $\boldsymbol\gamma'(0)$ , but it does not depend linearly on $\boldsymbol\gamma'(0)$ . In particular, if $\boldsymbol\gamma'(0)=(a,b)\ne (0,0)$ , then $h'(0)=\displaystyle\frac{a^3}{a^2+b^2}$ .","I looking for a proof or a counterexample (more likely) for the following Claim: Claim. Let , where open, and , be a function with the property that, there exists a matrix , such that for every smooth curve , where is an open interval and , with and , the composition is differentiable at and . Then is differentiable at . So far, available are the following examples (which do not consist counterexamples of the above): A. For this all the directional derivatives exist, but is not even continuous at B. For this , the composition is differentiable at and it is a function of , but it does not depend linearly on . In particular, if , then .","\,\boldsymbol{f}:U\to\mathbb R^n U\subset\mathbb R^m \boldsymbol{a}\in U A\in\mathbb R^{n\times m} \boldsymbol{\gamma}: I\to U I 0\in I \boldsymbol{\gamma}(0)=\boldsymbol{a} \boldsymbol{\gamma}'(0)\ne \boldsymbol{0} h(t)=\boldsymbol{f}\big(\boldsymbol{\gamma}(t)\big) t=0 h'(0)=A\boldsymbol{\gamma}'(0) \boldsymbol{f} \boldsymbol{x}=\boldsymbol{a} f(x,y)=\left\{\begin{array}{ccc}\displaystyle
\frac{xy^2}{x^2+y^6} & if & (x,y)\ne (0,0), \\
0 & if & (x,y)=(0,0).
\end{array}\right. f f (x,y)=(0,0) f(x,y)=\left\{\begin{array}{ccc}\displaystyle
\frac{x^3}{x^2+y^2} & if & (x,y)\ne (0,0), \\
0 & if & (x,y)=(0,0).
\end{array}\right. f h=f\circ\boldsymbol\gamma t=0 \boldsymbol\gamma'(0) \boldsymbol\gamma'(0) \boldsymbol\gamma'(0)=(a,b)\ne (0,0) h'(0)=\displaystyle\frac{a^3}{a^2+b^2}","['calculus', 'multivariable-calculus', 'differential-geometry', 'examples-counterexamples']"
55,"(a) Can $\sin(x+y)/(x+y)$ be made continuous by suitably defining it at (0,0)?","(a) Can  be made continuous by suitably defining it at (0,0)?",\sin(x+y)/(x+y),"(a) Can $\sin(x+y)/(x+y)$ be made continuous by suitably defining it at (0,0)? (b) Can $xy/(x^2 + y^2)$ be made continuous by suitably defining it at (0,0)? (c) Prove that $f: \mathbb R^2 \to \mathbb R$ , $(x, y) \to ye^x + \sin(x) + (xy)^4$ is continuous. Attempt (a) let $t = x + y$ $\lim_{(x, y) \to (0,0)} \frac{sin(x+y)}{x+y} = \lim_{t \to 0} \frac{sin(t)}{t} = 1$ is continuous at $0,0$ (b) Using $y = mx$ $\frac{x \cdot mx}{x^2 + (mx)^2} = \frac{m}{1+m^2}$ Not continuous since it's dependent on value of m (c) let $y = x, x = 0$ $\lim_{(x, y) \to (0,0)} ye^x + sin(x) + xy^4 = 0$ therefore continuous Am I right?","(a) Can be made continuous by suitably defining it at (0,0)? (b) Can be made continuous by suitably defining it at (0,0)? (c) Prove that , is continuous. Attempt (a) let is continuous at (b) Using Not continuous since it's dependent on value of m (c) let therefore continuous Am I right?","\sin(x+y)/(x+y) xy/(x^2 + y^2) f: \mathbb R^2 \to \mathbb R (x, y) \to ye^x + \sin(x) + (xy)^4 t = x + y \lim_{(x, y) \to (0,0)} \frac{sin(x+y)}{x+y} = \lim_{t \to 0} \frac{sin(t)}{t} = 1 0,0 y = mx \frac{x \cdot mx}{x^2 + (mx)^2} = \frac{m}{1+m^2} y = x, x = 0 \lim_{(x, y) \to (0,0)} ye^x + sin(x) + xy^4 = 0",['multivariable-calculus']
56,Calculation of flux through sphere when the vector field is not defined at the origin,Calculation of flux through sphere when the vector field is not defined at the origin,,"I am trying to calculate the flux through the unit sphere centered at the origin given a vector field $F:\mathbb{R}^3 \setminus \{(0,0,0)\} \rightarrow \mathbb{R}^3$ with $\operatorname{div} F=1/(x^2+y^2+z^2)$. I can't apply the divergence theorem directly because of the discontinuity so what I have done instead is to consider an inner sphere of infinitesimal radius $\epsilon>0$ and write $\iiint_V \operatorname{div} F dV$= (flux through unit sphere)-(flux through inner sphere). However, I really don't know how to go on from here. Can I prove that the flux through the sphere of radius $\epsilon$ is $0$ or is this not even true? I would appreciate some help.","I am trying to calculate the flux through the unit sphere centered at the origin given a vector field $F:\mathbb{R}^3 \setminus \{(0,0,0)\} \rightarrow \mathbb{R}^3$ with $\operatorname{div} F=1/(x^2+y^2+z^2)$. I can't apply the divergence theorem directly because of the discontinuity so what I have done instead is to consider an inner sphere of infinitesimal radius $\epsilon>0$ and write $\iiint_V \operatorname{div} F dV$= (flux through unit sphere)-(flux through inner sphere). However, I really don't know how to go on from here. Can I prove that the flux through the sphere of radius $\epsilon$ is $0$ or is this not even true? I would appreciate some help.",,"['integration', 'multivariable-calculus', 'vector-analysis', 'divergence-operator']"
57,"What are the gradient, divergence and curl of the three-dimensional delta function?","What are the gradient, divergence and curl of the three-dimensional delta function?",,The three-dimensional delta function is defined as follows: $$\delta(\mathbf{r}-\mathbf{r'})= 0 \;\; \mathrm{for} \;\;\mathbf{r}\neq\mathbf{r'} $$ $$\delta(\mathbf{r}-\mathbf{r'})= \infty \;\; \mathrm{for} \;\;\mathbf{r}=\mathbf{r'} $$ $$\int_V\delta(\mathbf{r}-\mathbf{r'})\;dV= 1 .$$ By definition also: $$\int_V f(\mathbf{r})\delta(\mathbf{r}-\mathbf{r'})\;dV= f(\mathbf{r'}) $$. I am wondering what the vector calculus operators are for the delta function. For example: Curl: $\nabla \times \delta(\mathbf{r}-\mathbf{r'})$ Divergence: $\nabla \cdot \delta(\mathbf{r}-\mathbf{r'})$ Gradient: $\nabla \delta(\mathbf{r}-\mathbf{r'})$ Any insight or references are appreciated. Please note that I am a geophysicist by training and it has been a few years since I have taken any vector calculus classes!,The three-dimensional delta function is defined as follows: $$\delta(\mathbf{r}-\mathbf{r'})= 0 \;\; \mathrm{for} \;\;\mathbf{r}\neq\mathbf{r'} $$ $$\delta(\mathbf{r}-\mathbf{r'})= \infty \;\; \mathrm{for} \;\;\mathbf{r}=\mathbf{r'} $$ $$\int_V\delta(\mathbf{r}-\mathbf{r'})\;dV= 1 .$$ By definition also: $$\int_V f(\mathbf{r})\delta(\mathbf{r}-\mathbf{r'})\;dV= f(\mathbf{r'}) $$. I am wondering what the vector calculus operators are for the delta function. For example: Curl: $\nabla \times \delta(\mathbf{r}-\mathbf{r'})$ Divergence: $\nabla \cdot \delta(\mathbf{r}-\mathbf{r'})$ Gradient: $\nabla \delta(\mathbf{r}-\mathbf{r'})$ Any insight or references are appreciated. Please note that I am a geophysicist by training and it has been a few years since I have taken any vector calculus classes!,,"['multivariable-calculus', 'vector-analysis', 'dirac-delta', 'divergence-operator', 'curl']"
58,"How to evaluate $\int_S(x^4+y^4+z^4) \, dS$ over surface of the unit sphere.",How to evaluate  over surface of the unit sphere.,"\int_S(x^4+y^4+z^4) \, dS","Question. Let $S$ denote the unit sphere in $\mathbb{R}^3$. Evaluate: $$\int_S (x^4+y^4+z^4) \, dS$$ My Solution. First I parametrize $S$ by $$r(u,v)=(\cos v \cos u, \cos v \sin u, \sin v)$$ $0\le u \le 2 \pi;~-\frac{\pi}{2}\le v \le \frac{\pi}{2}$ Let $~f(x,y,z)=x^4+y^4+z^4$. Here $~|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}|=|\cos v|$ Then $\displaystyle \int_S(x^4+y^4+z^4)\,dS = \int_{-\pi/2}^{\pi/2} \int_0^{2\pi} f[r(u,v)] \left|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}\right|~du~dv$ Thus I try to calculate this integral directly using the definition of surface integral. But I had so much calculations in this way. Does this particular problem can be solved using any theorems e.g-Gauss' Divergence (By writing $f$ as $F\cdot n$ for some vector field $F$? Thank you.","Question. Let $S$ denote the unit sphere in $\mathbb{R}^3$. Evaluate: $$\int_S (x^4+y^4+z^4) \, dS$$ My Solution. First I parametrize $S$ by $$r(u,v)=(\cos v \cos u, \cos v \sin u, \sin v)$$ $0\le u \le 2 \pi;~-\frac{\pi}{2}\le v \le \frac{\pi}{2}$ Let $~f(x,y,z)=x^4+y^4+z^4$. Here $~|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}|=|\cos v|$ Then $\displaystyle \int_S(x^4+y^4+z^4)\,dS = \int_{-\pi/2}^{\pi/2} \int_0^{2\pi} f[r(u,v)] \left|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}\right|~du~dv$ Thus I try to calculate this integral directly using the definition of surface integral. But I had so much calculations in this way. Does this particular problem can be solved using any theorems e.g-Gauss' Divergence (By writing $f$ as $F\cdot n$ for some vector field $F$? Thank you.",,"['multivariable-calculus', 'surface-integrals']"
59,Understanding this very generic divergence theorem where the open set have border $C^k$,Understanding this very generic divergence theorem where the open set have border,C^k,"I'm at a PDE class and my teacher gave a very generic definition of the divergence theorem. I can't find it anywhere. It's something like this: Definition: let $k\in \{1,2,\cdots,\infty\}$, $N\ge 2$ An open $\Omega\in\mathbb{R}^N$ has border of class $C^k$ if: $\Omega$ is bounded and given any $x_0\in\partial\Omega$, there is an   open $U_0$ containing $x_0$ and a diffeomorphism $\psi:U_0\to Q$,   where $Q = ]-1,1[^N$ such that $\psi(\Omega\cap U_0) = \{t\in Q: t_N>0\}$ $\psi(x_0) = 0$ $\psi(\partial \Omega\cap U_0) = \{t\in Q: t_N = 0\}$ Now he wrote these things: $X(t) = \psi^{-1}(t_1,\cdots,t_{N-1}, 0)$ $X:\{t'\in\mathbb{R}^{N-1}:|t_j|<1, j=1,\cdots,N-1\}\to U_0\cap\partial\Omega$ $$\vec{\theta}(t') = \left(\frac{\partial(x_2,\cdots,x_N)}{\partial(t_1,\cdots,t_{N-1)}}(t'),\cdots,\frac{\partial(x_1,\cdots,x_N)}{\partial(t_1,\cdots,t_{N-1)}}(t')\right)$$ then $\vec{\theta}(t')$ is normal to $\partial\Omega\cap U_0$ at $X(t')$ What are those derivatives? I don't recognize this notation and I don't know how it can be normal to anything. And why he picked the inverse image of $\psi$? The surface element $d\sigma$ of $\partial\Omega$ is represented in $U_0\cap \partial \Omega$ by $$|\vec{\theta}(t')|dt_1\cdots dt_{N-1}$$ finally we can define the exterior unitary normal in $\partial \Omega$: $$\vec{n}:\partial\Omega\to\mathbb{R}^N$$ $$|\vec{n}(p)| = 1, \forall p\in \partial \Omega$$ Divergence Theorem Let $\Omega$ be an open with boundary class $C^k$ and $\vec{X}(x) =  (X_1(x), \cdots, X_N(x))$ with $X_j\in C^1(\overline{\Omega}),  j=1,\cdots, N$ then $$\int_{\Omega}(\mbox{div} \vec{X})(x)dx =  \int_{\partial\Omega}\vec{X}(y)\cdot\vec{n}(y)d\sigma(y)$$ This seems to be a very generic view of the divergence theorem. And it seems that I need to understand what is an open set with a border and what does it mean for it to have class $C^k$. I also need to understand the surface element $d\sigma$. Is there a place where I can learn about those things but not too deep? They're just tools to solve PDEs Also, what does $C^n(\Omega)$ means? Also, the integral on $\Omega$ is supposed to be on an open of $\mathbb{R}^n$, but $div$ applied to $x$ is a real number, and $dx$ looks like a one dimensional thing. UPDATE: I'm downloading some books on vector calculus but I only find ones with a lot of wedge products and differential forms. I needed only a vector explanation of all this. Can someone point me a gook book?","I'm at a PDE class and my teacher gave a very generic definition of the divergence theorem. I can't find it anywhere. It's something like this: Definition: let $k\in \{1,2,\cdots,\infty\}$, $N\ge 2$ An open $\Omega\in\mathbb{R}^N$ has border of class $C^k$ if: $\Omega$ is bounded and given any $x_0\in\partial\Omega$, there is an   open $U_0$ containing $x_0$ and a diffeomorphism $\psi:U_0\to Q$,   where $Q = ]-1,1[^N$ such that $\psi(\Omega\cap U_0) = \{t\in Q: t_N>0\}$ $\psi(x_0) = 0$ $\psi(\partial \Omega\cap U_0) = \{t\in Q: t_N = 0\}$ Now he wrote these things: $X(t) = \psi^{-1}(t_1,\cdots,t_{N-1}, 0)$ $X:\{t'\in\mathbb{R}^{N-1}:|t_j|<1, j=1,\cdots,N-1\}\to U_0\cap\partial\Omega$ $$\vec{\theta}(t') = \left(\frac{\partial(x_2,\cdots,x_N)}{\partial(t_1,\cdots,t_{N-1)}}(t'),\cdots,\frac{\partial(x_1,\cdots,x_N)}{\partial(t_1,\cdots,t_{N-1)}}(t')\right)$$ then $\vec{\theta}(t')$ is normal to $\partial\Omega\cap U_0$ at $X(t')$ What are those derivatives? I don't recognize this notation and I don't know how it can be normal to anything. And why he picked the inverse image of $\psi$? The surface element $d\sigma$ of $\partial\Omega$ is represented in $U_0\cap \partial \Omega$ by $$|\vec{\theta}(t')|dt_1\cdots dt_{N-1}$$ finally we can define the exterior unitary normal in $\partial \Omega$: $$\vec{n}:\partial\Omega\to\mathbb{R}^N$$ $$|\vec{n}(p)| = 1, \forall p\in \partial \Omega$$ Divergence Theorem Let $\Omega$ be an open with boundary class $C^k$ and $\vec{X}(x) =  (X_1(x), \cdots, X_N(x))$ with $X_j\in C^1(\overline{\Omega}),  j=1,\cdots, N$ then $$\int_{\Omega}(\mbox{div} \vec{X})(x)dx =  \int_{\partial\Omega}\vec{X}(y)\cdot\vec{n}(y)d\sigma(y)$$ This seems to be a very generic view of the divergence theorem. And it seems that I need to understand what is an open set with a border and what does it mean for it to have class $C^k$. I also need to understand the surface element $d\sigma$. Is there a place where I can learn about those things but not too deep? They're just tools to solve PDEs Also, what does $C^n(\Omega)$ means? Also, the integral on $\Omega$ is supposed to be on an open of $\mathbb{R}^n$, but $div$ applied to $x$ is a real number, and $dx$ looks like a one dimensional thing. UPDATE: I'm downloading some books on vector calculus but I only find ones with a lot of wedge products and differential forms. I needed only a vector explanation of all this. Can someone point me a gook book?",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'divergence-operator']"
60,Strange form of the chain rule I keep seeing in differential geometry material?,Strange form of the chain rule I keep seeing in differential geometry material?,,"Since I've started reading up on differential geometry I keep coming across something that's bothering me, which is the chain rule. The standard chain rule from calculus is $$ \frac{df}{dt}(g(t)) = f'(g(t))\cdot \frac{dg}{dt}(t), $$ which I could also write as $$ \frac{d f}{dt}(g(t)) = \frac{\partial f}{\partial g(t)}(g(t))\cdot \frac{dg}{dt}(t). \quad \quad (1) $$ Now when I'm reading these differential geometry texts I keep seeing the following strange approach to the chain rule (I'll stay in 1 dimension for simplicity). The function $g$ is specified explicitly and in the simplest case it could be $g(t) = tx$. Then the chain rule is always given as $$ \begin{align} \frac{d f}{dt}(tx) = \frac{\partial f}{\partial x}(tx) \frac{d (tx)}{dt}. \end{align} $$ Replacing $tx$ by $g(t)$ (so we can compare it with the usual chain rule above) we have $$ \begin{align} \frac{d}{dt}f(g(t)) = \frac{\partial f}{\partial \color{red}{ \textbf{x}}}(g(t)) \cdot \frac{dg}{dt}(t), \quad \quad (2) \end{align} $$ where I have highlighted the problematic issue. Why have we $x$ in the denominator here instead of $g(t)$ as in the standard chain rule in (1) above? Some places, among many others, where I have seen this are: nLab - Hadamard lemma Second answer in this m.se tread The book Introduction to Manifolds by Loring Tu. So what is going on, is this some 'convention' in which this $x$ actually refers to $g(t) = tx$ or have I misinterpreted something and does the chain rule in (2) somehow agree with the standard chain rule (1)?","Since I've started reading up on differential geometry I keep coming across something that's bothering me, which is the chain rule. The standard chain rule from calculus is $$ \frac{df}{dt}(g(t)) = f'(g(t))\cdot \frac{dg}{dt}(t), $$ which I could also write as $$ \frac{d f}{dt}(g(t)) = \frac{\partial f}{\partial g(t)}(g(t))\cdot \frac{dg}{dt}(t). \quad \quad (1) $$ Now when I'm reading these differential geometry texts I keep seeing the following strange approach to the chain rule (I'll stay in 1 dimension for simplicity). The function $g$ is specified explicitly and in the simplest case it could be $g(t) = tx$. Then the chain rule is always given as $$ \begin{align} \frac{d f}{dt}(tx) = \frac{\partial f}{\partial x}(tx) \frac{d (tx)}{dt}. \end{align} $$ Replacing $tx$ by $g(t)$ (so we can compare it with the usual chain rule above) we have $$ \begin{align} \frac{d}{dt}f(g(t)) = \frac{\partial f}{\partial \color{red}{ \textbf{x}}}(g(t)) \cdot \frac{dg}{dt}(t), \quad \quad (2) \end{align} $$ where I have highlighted the problematic issue. Why have we $x$ in the denominator here instead of $g(t)$ as in the standard chain rule in (1) above? Some places, among many others, where I have seen this are: nLab - Hadamard lemma Second answer in this m.se tread The book Introduction to Manifolds by Loring Tu. So what is going on, is this some 'convention' in which this $x$ actually refers to $g(t) = tx$ or have I misinterpreted something and does the chain rule in (2) somehow agree with the standard chain rule (1)?",,"['multivariable-calculus', 'partial-derivative', 'chain-rule']"
61,Find absolute maximum and minimum values by parametrizing the boundaries,Find absolute maximum and minimum values by parametrizing the boundaries,,"$f(x,y) = 2\cos x + 3\sin y$  $; R= {(x , y): 0 \leq x \leq 2\pi \\\mbox{and}\\ 0 \leq y \leq \pi} $ I need to find the absolute maximum value and absolute minimum value in the region $R$, and I do have to parametrize the boundary pieces of $R$ to find critical points there. I tried taking $(x,y) = (r\cos(\theta),r\sin(\theta))$ for $\theta \in [0,2\pi]$ and then I got $h(\theta) = 2\cos(r\cos(\theta))+3\sin(r\sin(\theta))$ After that $h'(\theta)=2r\sin(\theta)\cdot\sin(r\cos(\theta))+3r\cos(\theta)\cdot(\cos(r\sin(\theta))$. I can't find values of $\theta$ for which $h'(\theta)=0$. How should I proceed from here?","$f(x,y) = 2\cos x + 3\sin y$  $; R= {(x , y): 0 \leq x \leq 2\pi \\\mbox{and}\\ 0 \leq y \leq \pi} $ I need to find the absolute maximum value and absolute minimum value in the region $R$, and I do have to parametrize the boundary pieces of $R$ to find critical points there. I tried taking $(x,y) = (r\cos(\theta),r\sin(\theta))$ for $\theta \in [0,2\pi]$ and then I got $h(\theta) = 2\cos(r\cos(\theta))+3\sin(r\sin(\theta))$ After that $h'(\theta)=2r\sin(\theta)\cdot\sin(r\cos(\theta))+3r\cos(\theta)\cdot(\cos(r\sin(\theta))$. I can't find values of $\theta$ for which $h'(\theta)=0$. How should I proceed from here?",,['multivariable-calculus']
62,Curve of Intersection between a Surface and a Plane,Curve of Intersection between a Surface and a Plane,,"The following terms are defined as in the book Differential Geometry by Do Carmo. Let $S$ be a regular surface, $p \in S,$ $N$ be a normal vector at $p$ and $v \in T_pS$ . Let $P$ be the plane parallel to the plane spanned by $v$ and $N$ and passes through $p$ . Now, I want to show that $P$ intersects with $S$ at a regular curve. I first tried to work on projections but that does not bring me to the conclusion. After that, I used the fact that a regular surface is locally a graph. Without loss of generality, I assumed that $S$ is a graph of a function of the form $z=f(x,y)$ . The equation of the plane $P$ is $(\vec{x}-p)\bullet(N \times v)=0$ , so I put in $\vec{x}=(x,y,f(x,y))$ and get $((x,y,f(x,y))\bullet(u\times v)=0$ . I try to use Implicit Function Theorem on this equation but I have no idea to proceed. Can anyone give me the idea of solving such problem? Thank you.","The following terms are defined as in the book Differential Geometry by Do Carmo. Let be a regular surface, be a normal vector at and . Let be the plane parallel to the plane spanned by and and passes through . Now, I want to show that intersects with at a regular curve. I first tried to work on projections but that does not bring me to the conclusion. After that, I used the fact that a regular surface is locally a graph. Without loss of generality, I assumed that is a graph of a function of the form . The equation of the plane is , so I put in and get . I try to use Implicit Function Theorem on this equation but I have no idea to proceed. Can anyone give me the idea of solving such problem? Thank you.","S p \in S, N p v \in T_pS P v N p P S S z=f(x,y) P (\vec{x}-p)\bullet(N \times v)=0 \vec{x}=(x,y,f(x,y)) ((x,y,f(x,y))\bullet(u\times v)=0","['multivariable-calculus', 'differential-geometry', 'surfaces', 'curves', 'implicit-function-theorem']"
63,Multivariable chain rule - notation?,Multivariable chain rule - notation?,,"Assume we have four functions $\in C^\infty$ from $\mathbb R^2 $ to $\mathbb R$, written as follows: $$ x=x(u,v) , \quad u=u(r,s) $$ $$ y=y(u,v) , \quad v=v(r,s) $$ then, assuming the composition $x(u(r,s),v(r,s))$ is properly defined, $$ \frac{\partial x}{\partial r} = \frac{\partial x}{\partial u} \frac{\partial u}{\partial r} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial r}. $$ But, in the particular case where $(r,s)=(x,y)$, we have that $\frac{\partial x}{\partial x} =1 $, but the right hand side gives us $$  \frac{\partial x}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial x} $$ which equals 2? What am I getting wrong here? Does my misunderstanding follows from a notation error? Thanks","Assume we have four functions $\in C^\infty$ from $\mathbb R^2 $ to $\mathbb R$, written as follows: $$ x=x(u,v) , \quad u=u(r,s) $$ $$ y=y(u,v) , \quad v=v(r,s) $$ then, assuming the composition $x(u(r,s),v(r,s))$ is properly defined, $$ \frac{\partial x}{\partial r} = \frac{\partial x}{\partial u} \frac{\partial u}{\partial r} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial r}. $$ But, in the particular case where $(r,s)=(x,y)$, we have that $\frac{\partial x}{\partial x} =1 $, but the right hand side gives us $$  \frac{\partial x}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial x} $$ which equals 2? What am I getting wrong here? Does my misunderstanding follows from a notation error? Thanks",,"['multivariable-calculus', 'chain-rule']"
64,Area between two spirals,Area between two spirals,,"I need to find the area between these two spirals given in the polar coordinates:  $$r = e^{5 \theta}$$ $$r = e^{10 \theta}$$ $$0 \le \theta \le 3\pi$$ This seems to be quite simple, yet this problem is marked as ""hard"" and so I am  not sure of the solution. I think that it is enough to find the area between these two exp curves and then 'convert' this area into the polar coordinates. The Jacobian is $r$. The are will be $$\int_{\theta = 0}^{\theta = 3 \pi} \int_{r = e^{5 \theta}}^{r = e^{10 \theta} } r drd\theta$$ Is my method a correct way to solve this? If not, please, tell me where the mistake is before posting your own, different solution.","I need to find the area between these two spirals given in the polar coordinates:  $$r = e^{5 \theta}$$ $$r = e^{10 \theta}$$ $$0 \le \theta \le 3\pi$$ This seems to be quite simple, yet this problem is marked as ""hard"" and so I am  not sure of the solution. I think that it is enough to find the area between these two exp curves and then 'convert' this area into the polar coordinates. The Jacobian is $r$. The are will be $$\int_{\theta = 0}^{\theta = 3 \pi} \int_{r = e^{5 \theta}}^{r = e^{10 \theta} } r drd\theta$$ Is my method a correct way to solve this? If not, please, tell me where the mistake is before posting your own, different solution.",,"['calculus', 'integration', 'multivariable-calculus']"
65,"Find the extrema of the implicit function $f(x,y,z(x,y)) = x^2 + y^2 - z^2 = 0$",Find the extrema of the implicit function,"f(x,y,z(x,y)) = x^2 + y^2 - z^2 = 0","Find the extrema of the implicit function $f(x,y,z(x,y)) = x^2 + y^2 -  z^2$ Of course, I start with calculating the partial derivatives by implicit differentiation. $$\frac{\partial z}{\partial x} = \frac x z$$ $$\frac{\partial z}{\partial y} = \frac y z$$ Which yields that the only feasible stationary point is $(0,0)$. By the general formula of the function, $z = 0 $. But now we have a problem. Since $z = 0$, no partial derivatives at this point exist. We could try to check if $(0,0,0)$ is an extremum straight from the definition, but we don't have the formula for $z$. I need to find the extremum of $z$. How do I proceed form here?","Find the extrema of the implicit function $f(x,y,z(x,y)) = x^2 + y^2 -  z^2$ Of course, I start with calculating the partial derivatives by implicit differentiation. $$\frac{\partial z}{\partial x} = \frac x z$$ $$\frac{\partial z}{\partial y} = \frac y z$$ Which yields that the only feasible stationary point is $(0,0)$. By the general formula of the function, $z = 0 $. But now we have a problem. Since $z = 0$, no partial derivatives at this point exist. We could try to check if $(0,0,0)$ is an extremum straight from the definition, but we don't have the formula for $z$. I need to find the extremum of $z$. How do I proceed form here?",,"['calculus', 'multivariable-calculus', 'derivatives', 'implicit-differentiation']"
66,"Taking the derivative of $A(t)\,x(t).$",Taking the derivative of,"A(t)\,x(t).","Can someone tell me how to interpret $\dfrac{d}{dt}(A(t)\,x(t))$ for some matrix $A$ and a vector $x$? I can't find this sort of derivative anywhere.","Can someone tell me how to interpret $\dfrac{d}{dt}(A(t)\,x(t))$ for some matrix $A$ and a vector $x$? I can't find this sort of derivative anywhere.",,['multivariable-calculus']
67,Relation between line integral of scalar function and surface integral,Relation between line integral of scalar function and surface integral,,"I've seen the following identity on a book: $$\int_{\partial S} f \, d\vec{\ell} = \iint_S d\vec{S} \times \nabla f$$ where $f$ is a scalar function and $\partial S$ is a closed curve. I've been trying to prove it but I don't know where to start.","I've seen the following identity on a book: $$\int_{\partial S} f \, d\vec{\ell} = \iint_S d\vec{S} \times \nabla f$$ where $f$ is a scalar function and $\partial S$ is a closed curve. I've been trying to prove it but I don't know where to start.",,['multivariable-calculus']
68,Show that these circles are linked (their linking number is $\pm 1$),Show that these circles are linked (their linking number is ),\pm 1,"There are two circles given in 3 dimensions: $$c_0(\phi) = (\cos(\phi), \sin(\phi),0)\ \ \ \phi \in [0,2\pi[$$ $$c_1(\phi) = \frac{1}{\sqrt{2}-\sin(\phi)}(\cos(\phi), \sin(\phi), \cos(\phi))\ \ \ \phi \in [0,2\pi[$$ Show that their linking number is $\pm 1$, where the linking number is defined by: $$L(c_0,c_1) = \frac{1}{4\pi}\int_0^{2\pi}\int_0^{2\pi}\frac{(c_0(t)-c_1(s),\dot c_0(t),\dot c_1(s))}{|c_0(t)-c_1(s)|^3}\,dt\ ds$$ Where $$\text{if } \ a,b,c:\Bbb{R} \to \Bbb{R}^3$$ $$\text{then } \ (a(t),b(t),c(t)) = \begin{align}\begin{vmatrix} a_1(t) & b_1(t) & c_1(t) \\  a_2(t) & b_2(t) & c_2(t) \\  a_3(t) & b_3(t) & c_3(t) \\ \end{vmatrix} \end{align}$$ And $$\text{if } \ c: \Bbb{R} \to \Bbb{R}^3, \ c(t) = (c_1(t),c_2(t),c_3(t))$$ $$\text{then } \ \dot c(t) = (c_1'(t),c_2'(t),c_3'(t))$$ Usually the integral for the linking number is very difficult to calculate, so there are other options too. For example, since we know that our curves are circles, their linking number can only be $L(c_0,c_1) \in \{-1,0,+1\}$. Showing that an integral is equal to a specific value can be done with this following technique: $$L(c_0,c_1) \stackrel{?}{=} a \in \{-1,0,+1\} \iff \\  \iff \forall \varepsilon \in \Bbb{R}^+: a - \varepsilon \stackrel{?}{\le} L(c_0,c_1) \stackrel{?}{\le} a + \varepsilon $$ So far I've shown that the circles don't intersect, meaning that there are no $\phi \in \ [0,2\pi[$ values for which $|c_0(\phi)-c_1(\phi)| = 0$. That was relatively easy to show. But the big question is: $$L(c_0,c_1) \stackrel{?}{=} \pm 1$$ I would appreciate any insight on how I could possibly show this. In other words, how can we show that the above integral is approximately equal to $1$, with perhaps numerical methods?","There are two circles given in 3 dimensions: $$c_0(\phi) = (\cos(\phi), \sin(\phi),0)\ \ \ \phi \in [0,2\pi[$$ $$c_1(\phi) = \frac{1}{\sqrt{2}-\sin(\phi)}(\cos(\phi), \sin(\phi), \cos(\phi))\ \ \ \phi \in [0,2\pi[$$ Show that their linking number is $\pm 1$, where the linking number is defined by: $$L(c_0,c_1) = \frac{1}{4\pi}\int_0^{2\pi}\int_0^{2\pi}\frac{(c_0(t)-c_1(s),\dot c_0(t),\dot c_1(s))}{|c_0(t)-c_1(s)|^3}\,dt\ ds$$ Where $$\text{if } \ a,b,c:\Bbb{R} \to \Bbb{R}^3$$ $$\text{then } \ (a(t),b(t),c(t)) = \begin{align}\begin{vmatrix} a_1(t) & b_1(t) & c_1(t) \\  a_2(t) & b_2(t) & c_2(t) \\  a_3(t) & b_3(t) & c_3(t) \\ \end{vmatrix} \end{align}$$ And $$\text{if } \ c: \Bbb{R} \to \Bbb{R}^3, \ c(t) = (c_1(t),c_2(t),c_3(t))$$ $$\text{then } \ \dot c(t) = (c_1'(t),c_2'(t),c_3'(t))$$ Usually the integral for the linking number is very difficult to calculate, so there are other options too. For example, since we know that our curves are circles, their linking number can only be $L(c_0,c_1) \in \{-1,0,+1\}$. Showing that an integral is equal to a specific value can be done with this following technique: $$L(c_0,c_1) \stackrel{?}{=} a \in \{-1,0,+1\} \iff \\  \iff \forall \varepsilon \in \Bbb{R}^+: a - \varepsilon \stackrel{?}{\le} L(c_0,c_1) \stackrel{?}{\le} a + \varepsilon $$ So far I've shown that the circles don't intersect, meaning that there are no $\phi \in \ [0,2\pi[$ values for which $|c_0(\phi)-c_1(\phi)| = 0$. That was relatively easy to show. But the big question is: $$L(c_0,c_1) \stackrel{?}{=} \pm 1$$ I would appreciate any insight on how I could possibly show this. In other words, how can we show that the above integral is approximately equal to $1$, with perhaps numerical methods?",,"['multivariable-calculus', 'differential-geometry', 'definite-integrals', 'circles', 'knot-theory']"
69,How do I show that partial derivatives are continuous?,How do I show that partial derivatives are continuous?,,"The theorem says that for $f$ to be differentiable, partial derivatives of $f$ exist and are continuous. For example, let $f(x,y)=x^2+2xy+y^2.$ Let $(a,b)\in R^2.$ Then, I know that partial derivatives exist and $f_x(a,b)=2a+b,$ and $f_y(a,b) = a+2b$. In order to test the continuity, $$\lim_{(x,y)\to(a,b)}f_x(x,y)=\lim_{(x,y)\to(a,b)}2x+y=2a+b=f_x(a,b). $$ $$\lim_{(x,y)\to(a,b)}f_y(x,y)=\lim_{(x,y)\to(a,b)}x+2y=a+2b=f_y(a,b).$$ Is this the right way to check whether partial derivatives are continuous on $(a,b)$ for the multivariable case?","The theorem says that for $f$ to be differentiable, partial derivatives of $f$ exist and are continuous. For example, let $f(x,y)=x^2+2xy+y^2.$ Let $(a,b)\in R^2.$ Then, I know that partial derivatives exist and $f_x(a,b)=2a+b,$ and $f_y(a,b) = a+2b$. In order to test the continuity, $$\lim_{(x,y)\to(a,b)}f_x(x,y)=\lim_{(x,y)\to(a,b)}2x+y=2a+b=f_x(a,b). $$ $$\lim_{(x,y)\to(a,b)}f_y(x,y)=\lim_{(x,y)\to(a,b)}x+2y=a+2b=f_y(a,b).$$ Is this the right way to check whether partial derivatives are continuous on $(a,b)$ for the multivariable case?",,"['real-analysis', 'multivariable-calculus']"
70,Why is this integral equal to zero?,Why is this integral equal to zero?,,"The other day, I was helping some friend to study maths for an exam when we came across with this exercise: Let $\Omega\subset\mathbb R^3$ be a connected bounded subset with differentiable boundary $\partial\Omega$ . If the divergence of $F:\mathbb R^3\rightarrow \mathbb R^3$ is zero in $\Omega$ , then $$ \int_\Omega F^TJ^T F dx = 0 , $$ where $J$ means the jacobian of $F$ and $J^T$ the transpose matrix of $J$ . The exercise also suggests use integration by parts. To be honest, I have never seen some identity like such a one. I have looked at the classical divergence theorem, but I don't know how to apply it here. Also it seems that the identities for the curl and the divergence  can't work here. We have develop the integral expression, which can be written compactly as follows: $$F^TJ^TF=  \sum_i F_i^2\frac{\partial F_i}{\partial x_i} + \sum_{i,j\\i<j} F_iF_j\left(\frac{\partial F_i}{\partial x_j} + \frac{\partial F_j}{\partial x_i}\right) $$ The first sum is ( $1$ over $3$ times) the divergence of the vector field $F^3=(F_1^3,F_2^3,F_3^3)$ , so in this case we can use the divergence theorem. But I can't see how to use the fact that the divergence of $F$ is zero (clearly it doesn't imply that the divergence of $F^3$ is also $0$ ). May you give us some clue please? To be honest I'm very lost with that (and it is supposed I'm the savant of both...) PD: There is no tag for homework or something similar :(","The other day, I was helping some friend to study maths for an exam when we came across with this exercise: Let be a connected bounded subset with differentiable boundary . If the divergence of is zero in , then where means the jacobian of and the transpose matrix of . The exercise also suggests use integration by parts. To be honest, I have never seen some identity like such a one. I have looked at the classical divergence theorem, but I don't know how to apply it here. Also it seems that the identities for the curl and the divergence  can't work here. We have develop the integral expression, which can be written compactly as follows: The first sum is ( over times) the divergence of the vector field , so in this case we can use the divergence theorem. But I can't see how to use the fact that the divergence of is zero (clearly it doesn't imply that the divergence of is also ). May you give us some clue please? To be honest I'm very lost with that (and it is supposed I'm the savant of both...) PD: There is no tag for homework or something similar :(","\Omega\subset\mathbb R^3 \partial\Omega F:\mathbb R^3\rightarrow \mathbb R^3 \Omega  \int_\Omega F^TJ^T F dx = 0 ,  J F J^T J F^TJ^TF=  \sum_i F_i^2\frac{\partial F_i}{\partial x_i} + \sum_{i,j\\i<j} F_iF_j\left(\frac{\partial F_i}{\partial x_j} + \frac{\partial F_j}{\partial x_i}\right)  1 3 F^3=(F_1^3,F_2^3,F_3^3) F F^3 0","['multivariable-calculus', 'definite-integrals']"
71,Double Integral in Polar Coordinates - Confused by angle,Double Integral in Polar Coordinates - Confused by angle,,"I need to compute the integral $$\int_{-1}^{0} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2} }x dy dx $$ by converting to polar coordinates. Now, since $-\sqrt{1-x^{2}}\leq y \leq \sqrt{1-x^2}$, we are integrating over the unit circle. However, since $-1 \leq x \leq 0$ - i.e., $-1 \leq \cos \theta \leq 0$, I was under the impression that $\theta$ should range from $\displaystyle \frac{\pi}{2}$ to $\pi$. But, in fact, I was informed that in this problem, we are integrating over the left half of the unit circle (so $\theta$ should range from $\pi/2$ to $3 \pi/2$ [and backwards??]), and not just the upper left-hand corner of the circle. Could somebody please explain to me why that is/how I should be able to tell that from the integral? Thank you.","I need to compute the integral $$\int_{-1}^{0} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2} }x dy dx $$ by converting to polar coordinates. Now, since $-\sqrt{1-x^{2}}\leq y \leq \sqrt{1-x^2}$, we are integrating over the unit circle. However, since $-1 \leq x \leq 0$ - i.e., $-1 \leq \cos \theta \leq 0$, I was under the impression that $\theta$ should range from $\displaystyle \frac{\pi}{2}$ to $\pi$. But, in fact, I was informed that in this problem, we are integrating over the left half of the unit circle (so $\theta$ should range from $\pi/2$ to $3 \pi/2$ [and backwards??]), and not just the upper left-hand corner of the circle. Could somebody please explain to me why that is/how I should be able to tell that from the integral? Thank you.",,"['calculus', 'integration']"
72,Formulating a function $f$ such that $\nabla f = F$,Formulating a function  such that,f \nabla f = F,"Example: Let $F = (2xy)i+(x^2-\cos{z})j+(y\sin{z})k$. Find a function $f$ so that $\nabla f=F$. I understand that I can essentially go backwards and determine a solution from guesswork. My question is, what is a more systematic process that can be followed to get to the same solution? Thanks!","Example: Let $F = (2xy)i+(x^2-\cos{z})j+(y\sin{z})k$. Find a function $f$ so that $\nabla f=F$. I understand that I can essentially go backwards and determine a solution from guesswork. My question is, what is a more systematic process that can be followed to get to the same solution? Thanks!",,['multivariable-calculus']
73,Properties of the 1-d Heat Kernel,Properties of the 1-d Heat Kernel,,"Let $H$ be the one dimensional heat kernel, i.e the function $H(t,x,y)$ such that the Dirichlet problem: $$ \begin{cases} u_t - u_{xx} = 0 & x\in(0,1) , t \in (0,\infty)  \\ u(t,1) = u(t,0) =0 & t>0  \\ u(0,x)  = f(x) \end{cases} $$ is solved by: $$ u(t,x) = \int_0^1 f(y)  H(t,x,y) \mathrm{d}y $$ Prove that: $H$ is nonnegative The integral of $H$ with respect to $y$ is non-increasing with respect to $t$. I believe that these properties of $H$ can be derived without using the explicit (Fourier sine series) representation of $H$. I am not sure however how to derive the first (I have not started on the second). Would anyone be able to provide a hint (I am not looking for a complete answer, at least not right now). I know that $H$ itself solves the heat equation, but I am not entirely sure how to use this property, as taking derivatives of $H$ without using the explicit representation seems to be an arduous task.","Let $H$ be the one dimensional heat kernel, i.e the function $H(t,x,y)$ such that the Dirichlet problem: $$ \begin{cases} u_t - u_{xx} = 0 & x\in(0,1) , t \in (0,\infty)  \\ u(t,1) = u(t,0) =0 & t>0  \\ u(0,x)  = f(x) \end{cases} $$ is solved by: $$ u(t,x) = \int_0^1 f(y)  H(t,x,y) \mathrm{d}y $$ Prove that: $H$ is nonnegative The integral of $H$ with respect to $y$ is non-increasing with respect to $t$. I believe that these properties of $H$ can be derived without using the explicit (Fourier sine series) representation of $H$. I am not sure however how to derive the first (I have not started on the second). Would anyone be able to provide a hint (I am not looking for a complete answer, at least not right now). I know that $H$ itself solves the heat equation, but I am not entirely sure how to use this property, as taking derivatives of $H$ without using the explicit representation seems to be an arduous task.",,"['multivariable-calculus', 'partial-differential-equations', 'heat-equation']"
74,Continuity of potential function at an interior point,Continuity of potential function at an interior point,,"Suppose $\Omega \subset \mathbb{R}^3$ is closed, bounded, with smooth boundary and the density $\rho: \Omega \to \mathbb{R}$ is continuous.  The potential function is $$\phi(x) = \int_\Omega \frac{\rho(x')}{|x - x'|} dx',$$ I already showed it continuous for $x \notin \Omega$ and am trying to show it continuous for $x \in \Omega$. My effort: I take a ball $B_\delta(x_0)$ around a fixed point $x_0$ with $|x-x_0| < \delta$ and look at $$|\phi(x) - \phi(x_0)| \leq \left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right|+ \left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| + \left|\int_{ B_\delta(x_0)} \frac{\rho(x')}{|x-x'|} dx' \right|$$ Since the integrand in the first integral is continuous there exists $\delta_0$ such that if $|x - x_0| < \delta_0$ then $$\left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right| < \frac{\epsilon}{3}$$ Also convergence of the potential integral means if $\delta$ is small enough then $$\left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| < \frac{\epsilon}{3}$$ My question is how to show the third integral can be made $< \frac{\epsilon}{3}$ since $B_\delta(x_0)$ is not centered at $x$.","Suppose $\Omega \subset \mathbb{R}^3$ is closed, bounded, with smooth boundary and the density $\rho: \Omega \to \mathbb{R}$ is continuous.  The potential function is $$\phi(x) = \int_\Omega \frac{\rho(x')}{|x - x'|} dx',$$ I already showed it continuous for $x \notin \Omega$ and am trying to show it continuous for $x \in \Omega$. My effort: I take a ball $B_\delta(x_0)$ around a fixed point $x_0$ with $|x-x_0| < \delta$ and look at $$|\phi(x) - \phi(x_0)| \leq \left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right|+ \left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| + \left|\int_{ B_\delta(x_0)} \frac{\rho(x')}{|x-x'|} dx' \right|$$ Since the integrand in the first integral is continuous there exists $\delta_0$ such that if $|x - x_0| < \delta_0$ then $$\left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right| < \frac{\epsilon}{3}$$ Also convergence of the potential integral means if $\delta$ is small enough then $$\left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| < \frac{\epsilon}{3}$$ My question is how to show the third integral can be made $< \frac{\epsilon}{3}$ since $B_\delta(x_0)$ is not centered at $x$.",,"['multivariable-calculus', 'potential-theory']"
75,Double integral of maximum function.,Double integral of maximum function.,,"Let $D:= \lbrace (x,y) \in [0,\infty)^2: 1 \le x^2+y^2\le9 \rbrace$. Determine the integral : $$\int\int_D \max(3x^2,y^2)\;dx\,dy.$$ I have a little problem, because I'm not sure where the maximum is $3x^2$ or $y^2$.","Let $D:= \lbrace (x,y) \in [0,\infty)^2: 1 \le x^2+y^2\le9 \rbrace$. Determine the integral : $$\int\int_D \max(3x^2,y^2)\;dx\,dy.$$ I have a little problem, because I'm not sure where the maximum is $3x^2$ or $y^2$.",,"['integration', 'multivariable-calculus', 'maxima-minima']"
76,"Inverse of $f(x,y)=(x^2+y^2,x^2-y^2$)",Inverse of ),"f(x,y)=(x^2+y^2,x^2-y^2","Let $f:[0,1]\times[0,1]\to\mathbb{R}^2$ be given by $f(x,y)=(x^2+y^2,x^2-y^2)$. Am I correct in thinking that $f$ has no inverse? I can show that $f$ is one-to-one but $f$ is not onto since for $(1,2)$ in the codomain, there is no $(x,y)$ in the domain such that $f(x,y)=(1,2)$. So $f$ does not have any inverse.","Let $f:[0,1]\times[0,1]\to\mathbb{R}^2$ be given by $f(x,y)=(x^2+y^2,x^2-y^2)$. Am I correct in thinking that $f$ has no inverse? I can show that $f$ is one-to-one but $f$ is not onto since for $(1,2)$ in the codomain, there is no $(x,y)$ in the domain such that $f(x,y)=(1,2)$. So $f$ does not have any inverse.",,[]
77,Clarification for the meaning of arc lenght and area forms,Clarification for the meaning of arc lenght and area forms,,"Let $C$ be an oriented regular curve and $ds$ its arc lenght. If $\alpha:I\to C$ is a parametrization compatible with the orientation of $C$, then $$\alpha^*(ds)=\|\alpha'\|dt$$   Let $S\subset\mathbb{R}^3$ be an oriented regular surface and $dA$ its area form. If $\psi:U\subset\mathbb{R}^2\to S$ is a parametrization compatible with the orientation of S then $$\psi^*(dA)=\|\psi_u\psi_v\|du \wedge dv$$ I (more or less) understand forms on open subsets of $\mathbb{R}^n$ and their pullback and I just started with forms on manifolds. I don't think I understand what is the meaning of $ds$ and $dA$ in those statements. I thought $ds$ should be the 1-form of $C$ whose $p-$component ($p\in C$) is $(\alpha'(q))^*$, where $p=\alpha(q)$. So, the pullback of $ds$ by $\alpha$ should be a 1-form on $I$ whose $q-$component acting on a vector of the tangent space of $I$, which in this case is just some $x \in \mathbb{R}$, is $ds_p=(\alpha'(q))^*$ acting on $\alpha'(q)x$, which is simply $$(\alpha'(q))^*[\alpha'(q)x]=x(\alpha'(q))^*[\alpha'(q)]=x$$ So maybe I'm missunderstanding the meaning of $ds$ (and similarly with $dA$) How should I interpret those differential forms?","Let $C$ be an oriented regular curve and $ds$ its arc lenght. If $\alpha:I\to C$ is a parametrization compatible with the orientation of $C$, then $$\alpha^*(ds)=\|\alpha'\|dt$$   Let $S\subset\mathbb{R}^3$ be an oriented regular surface and $dA$ its area form. If $\psi:U\subset\mathbb{R}^2\to S$ is a parametrization compatible with the orientation of S then $$\psi^*(dA)=\|\psi_u\psi_v\|du \wedge dv$$ I (more or less) understand forms on open subsets of $\mathbb{R}^n$ and their pullback and I just started with forms on manifolds. I don't think I understand what is the meaning of $ds$ and $dA$ in those statements. I thought $ds$ should be the 1-form of $C$ whose $p-$component ($p\in C$) is $(\alpha'(q))^*$, where $p=\alpha(q)$. So, the pullback of $ds$ by $\alpha$ should be a 1-form on $I$ whose $q-$component acting on a vector of the tangent space of $I$, which in this case is just some $x \in \mathbb{R}$, is $ds_p=(\alpha'(q))^*$ acting on $\alpha'(q)x$, which is simply $$(\alpha'(q))^*[\alpha'(q)x]=x(\alpha'(q))^*[\alpha'(q)]=x$$ So maybe I'm missunderstanding the meaning of $ds$ (and similarly with $dA$) How should I interpret those differential forms?",,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'differential-forms']"
78,Prove that $x^4 + y^4 - 3xy = 2$ is compact,Prove that  is compact,x^4 + y^4 - 3xy = 2,"The exercise consists of showing that the function $f(x,y)=x^4 + y^4$ has a global minimum and maximum under the constraint $x^4 + y^4 - 2xy = 2$. In the solution to the exercise, it it follows that the constraint is compact if we can show that $\lim_{x^2 + y^2 \rightarrow \infty} x^4 + y^4 - 3xy - 2 \rightarrow \infty$.Why this is the case? My intuition tells me that this is because the $x^4$ and $y^4$ terms dominates the other two terms when $x$ and $y$ gets large. This would then imply that $x$ and $y$ cannot get arbitrarily big without violating the constraint. Does this imply that if the limit of the constraint was $0$, that the domain would not be compact? Is my reasoning valid? Many thanks,","The exercise consists of showing that the function $f(x,y)=x^4 + y^4$ has a global minimum and maximum under the constraint $x^4 + y^4 - 2xy = 2$. In the solution to the exercise, it it follows that the constraint is compact if we can show that $\lim_{x^2 + y^2 \rightarrow \infty} x^4 + y^4 - 3xy - 2 \rightarrow \infty$.Why this is the case? My intuition tells me that this is because the $x^4$ and $y^4$ terms dominates the other two terms when $x$ and $y$ gets large. This would then imply that $x$ and $y$ cannot get arbitrarily big without violating the constraint. Does this imply that if the limit of the constraint was $0$, that the domain would not be compact? Is my reasoning valid? Many thanks,",,"['multivariable-calculus', 'optimization']"
79,How many cells are expected?,How many cells are expected?,,"Suppose in the beginning, one cell was present in a Petri dish, and   nutrients for cell division were provided in abundance. It is given   that the probability for a cell to divide is $\sin(t^\circ)$, where   $t$ (in minutes) is the time elapsed since its last division. What is   the expected number of cells in the dish after $t$ minutes? Assume the   cells won't die, or, cease to exist. A troubled friend of mine asked me this question. I became as troubled as him afterwards. I limited the discussion to $0 \leq t < 90$ for simplicity's sake. My approach Denote $p_m$ as the probability of having $m$ cells after $t$ minutes. Expected number of cells = $\sum^{\infty}_{m=1}mp_m$ Consider $m=1$. If there was still 1 cell in the dish after $t$ minutes, then that one cell must not have divided itself at the first place, so $p_1=1-\sin(t)$. Looks innocent enough. Consider $m=2$. The only case is that the 1 cell initially present had divided into 2 daughter cells at some time $x$, where $x\leq t$, AND that both the 2 daughter cells did not further divide themselves from time $x$ to $t$. So the probability is $$p_2=\int_0^t \sin(x)[1-\sin(t-x)]^2 \text dx$$ Consider $m=3$. Now either one of the daughter cells must have divided itself. Say that this second division occurs $y$ minutes since the first division occured. Also taking the symmetry between the two daughter cells into consideration, the core of the integral is $$2\sin x \sin y [1-\sin(t-x)][1-\sin(t-x-y)]^2$$ My queries When $m=3$, from/to where should I integrate $\text d x$ and $\text d y$? Can I write something like the following, if not, how should I formulate $p_3$? $$p_3 = \int_0^t\int_0^{t-t_1}\int_0^{t_1} 2\sin x \sin y [1-\sin(t-x)][1-\sin(t-x-y)]^2 \text dx \text dy \text dt_1$$ The probability tree gets really complicated when $m \geq 4$, and the integral itself requires $m-1$ variables. Is there a much simpler way of thinking this problem? (Is it even logically sound to find the probabilities using integration...?) Can a computer program help predict the expected number of cells? What would the results be? Any help is heartily appreciated... :(","Suppose in the beginning, one cell was present in a Petri dish, and   nutrients for cell division were provided in abundance. It is given   that the probability for a cell to divide is $\sin(t^\circ)$, where   $t$ (in minutes) is the time elapsed since its last division. What is   the expected number of cells in the dish after $t$ minutes? Assume the   cells won't die, or, cease to exist. A troubled friend of mine asked me this question. I became as troubled as him afterwards. I limited the discussion to $0 \leq t < 90$ for simplicity's sake. My approach Denote $p_m$ as the probability of having $m$ cells after $t$ minutes. Expected number of cells = $\sum^{\infty}_{m=1}mp_m$ Consider $m=1$. If there was still 1 cell in the dish after $t$ minutes, then that one cell must not have divided itself at the first place, so $p_1=1-\sin(t)$. Looks innocent enough. Consider $m=2$. The only case is that the 1 cell initially present had divided into 2 daughter cells at some time $x$, where $x\leq t$, AND that both the 2 daughter cells did not further divide themselves from time $x$ to $t$. So the probability is $$p_2=\int_0^t \sin(x)[1-\sin(t-x)]^2 \text dx$$ Consider $m=3$. Now either one of the daughter cells must have divided itself. Say that this second division occurs $y$ minutes since the first division occured. Also taking the symmetry between the two daughter cells into consideration, the core of the integral is $$2\sin x \sin y [1-\sin(t-x)][1-\sin(t-x-y)]^2$$ My queries When $m=3$, from/to where should I integrate $\text d x$ and $\text d y$? Can I write something like the following, if not, how should I formulate $p_3$? $$p_3 = \int_0^t\int_0^{t-t_1}\int_0^{t_1} 2\sin x \sin y [1-\sin(t-x)][1-\sin(t-x-y)]^2 \text dx \text dy \text dt_1$$ The probability tree gets really complicated when $m \geq 4$, and the integral itself requires $m-1$ variables. Is there a much simpler way of thinking this problem? (Is it even logically sound to find the probabilities using integration...?) Can a computer program help predict the expected number of cells? What would the results be? Any help is heartily appreciated... :(",,"['probability', 'integration', 'multivariable-calculus', 'programming']"
80,Find maximizing region for triple integral,Find maximizing region for triple integral,,"Given the following triple integral: $-\iiint_D (x^2 + y^2 + z^2 -4) \,dV$ How can I find the closed surface out of which the above integral is maximal? I am using the divergence theorem to calculate flux through a surface.","Given the following triple integral: $-\iiint_D (x^2 + y^2 + z^2 -4) \,dV$ How can I find the closed surface out of which the above integral is maximal? I am using the divergence theorem to calculate flux through a surface.",,"['integration', 'multivariable-calculus']"
81,Understanding a notation chain rule for multivariable functions,Understanding a notation chain rule for multivariable functions,,"I can't understand the meaning of partial derivative times differential. I was reading wikipedia and poped to Total derivative article, Where I saw this: The total derivative of $ {\displaystyle f(t,x(t),y(t))}$ with respect to ${\displaystyle t}$ is $\frac{df}{dt} = \frac{\partial f}{\partial t}\frac{dt}{dt} + \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$ which can be simplefied to: ${\displaystyle \operatorname {d} f={\frac {\partial f}{\partial t}}\operatorname {d} t+{\frac {\partial f}{\partial x}}\operatorname {d} x+{\frac {\partial f}{\partial y}}\operatorname {d} y}$ What does it mean to take a partial derivative ${\frac {\partial f}{\partial t}}$ (which is a new function by itself) and multiply it by a differential $\operatorname {d}t$. I'm in high school currently and I read what interests me. I don't have an comprehensive knowledge, therefore it might have been taught in topics which I have'nt learned. So any question will be welcomed :)","I can't understand the meaning of partial derivative times differential. I was reading wikipedia and poped to Total derivative article, Where I saw this: The total derivative of $ {\displaystyle f(t,x(t),y(t))}$ with respect to ${\displaystyle t}$ is $\frac{df}{dt} = \frac{\partial f}{\partial t}\frac{dt}{dt} + \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$ which can be simplefied to: ${\displaystyle \operatorname {d} f={\frac {\partial f}{\partial t}}\operatorname {d} t+{\frac {\partial f}{\partial x}}\operatorname {d} x+{\frac {\partial f}{\partial y}}\operatorname {d} y}$ What does it mean to take a partial derivative ${\frac {\partial f}{\partial t}}$ (which is a new function by itself) and multiply it by a differential $\operatorname {d}t$. I'm in high school currently and I read what interests me. I don't have an comprehensive knowledge, therefore it might have been taught in topics which I have'nt learned. So any question will be welcomed :)",,"['calculus', 'multivariable-calculus', 'chain-rule']"
82,"Evaluate $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{1}{\cosh(x)+\cosh(y)} \, dy \, dx$",Evaluate,"\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\frac{1}{\cosh(x)+\cosh(y)} \, dy \, dx","My friend gave me this problem, and I'm not too sure where to start. I know single integrals relatively well, but I'm pretty new to double integrals. If someone could show a step-by-step solution, that would be great. $$\int_{-\infty}^{\infty}\int_{-\infty}^\infty \frac 1 {\cosh(x)+\cosh(y)} \, dy \, dx$$ I've tried hyperbolic tangent half-angle substitution and normal substitution, but I have no idea if that applies to double integrals as well.","My friend gave me this problem, and I'm not too sure where to start. I know single integrals relatively well, but I'm pretty new to double integrals. If someone could show a step-by-step solution, that would be great. $$\int_{-\infty}^{\infty}\int_{-\infty}^\infty \frac 1 {\cosh(x)+\cosh(y)} \, dy \, dx$$ I've tried hyperbolic tangent half-angle substitution and normal substitution, but I have no idea if that applies to double integrals as well.",,"['multivariable-calculus', 'hyperbolic-functions']"
83,Directional Derivatives (2 var),Directional Derivatives (2 var),,"Compute the directional derivatives of the following functions along unit vectors at the indicated points in directions parallel to the given vector. $f(x, y) = x^y$ $(x_0, y_0)$ = (e, e) d = 3 i + 4 j The formula for directional derivatives = gradient f(e,e)  v I got the answer $3ee^{e-1}+4\ln \left(e\right)e^e$ which is incorrect. What is the correct way of doing this problem? I can show my steps if necessary. MY STEPS: find gradient f = $(yx^y-1 , ln(x)x^y)$ gradient f(e,e) = $(ee^{e-1},ln(e)e^e)$ d = 3 i + 4 j $(ee^{e-1},ln(e)e^e)$  (3,4) = $3ee^{e-1}+4\ln \left(e\right)e^e$","Compute the directional derivatives of the following functions along unit vectors at the indicated points in directions parallel to the given vector. $f(x, y) = x^y$ $(x_0, y_0)$ = (e, e) d = 3 i + 4 j The formula for directional derivatives = gradient f(e,e)  v I got the answer $3ee^{e-1}+4\ln \left(e\right)e^e$ which is incorrect. What is the correct way of doing this problem? I can show my steps if necessary. MY STEPS: find gradient f = $(yx^y-1 , ln(x)x^y)$ gradient f(e,e) = $(ee^{e-1},ln(e)e^e)$ d = 3 i + 4 j $(ee^{e-1},ln(e)e^e)$  (3,4) = $3ee^{e-1}+4\ln \left(e\right)e^e$",,"['multivariable-calculus', 'partial-derivative']"
84,Applying Leibniz Rule,Applying Leibniz Rule,,"I found the following problem on a comprehensive exam: Let $f : \mathbb{R}^2 \to \mathbb{R}$ be a continuous function and consider the function $F: \mathbb{R}^2 \to \mathbb{R}$ given by $$F(x,y) = \int_{D_{x,y}} f(u,v)\,du\,dv, \qquad D_{x,y} = \left\{(u,v) \in \mathbb{R}^2 \,\middle|\, u^2 + v^2 \leq x^2 + y^2 \right\}$$ Is $F(x,y)$ differentiable? If yes, find the differential $DF$. To me, it seems that this is a straight forward application of Leibniz Rule, but I've never applied it in such a setting: Treating $y$ as a constant, we can describe F(x,y) (somewhat inelegantly) as $$\int_{h_1(x)}^{h_2(x)} \int_{g_1(x,v)}^{g_2(x,v)} f\,du\,dv$$ where $h_1,h_2$ are differentiable.  Then if we let  $$G(v) = \int_{g_1(x,v)}^{g_2(x,v)} f\,du\,dv$$ We get $\frac{d}{dx}F(x,y) = G(h_2(x)h_2'(x) - G(h_1(x))h_1'(x)$.  Does this seem correct?  This does not seem like a very adequate answer. I'd like to ask for some more insight into problems of this form, verification for if the my attempt had any validity, and a more complete answer if possible. Thanks.","I found the following problem on a comprehensive exam: Let $f : \mathbb{R}^2 \to \mathbb{R}$ be a continuous function and consider the function $F: \mathbb{R}^2 \to \mathbb{R}$ given by $$F(x,y) = \int_{D_{x,y}} f(u,v)\,du\,dv, \qquad D_{x,y} = \left\{(u,v) \in \mathbb{R}^2 \,\middle|\, u^2 + v^2 \leq x^2 + y^2 \right\}$$ Is $F(x,y)$ differentiable? If yes, find the differential $DF$. To me, it seems that this is a straight forward application of Leibniz Rule, but I've never applied it in such a setting: Treating $y$ as a constant, we can describe F(x,y) (somewhat inelegantly) as $$\int_{h_1(x)}^{h_2(x)} \int_{g_1(x,v)}^{g_2(x,v)} f\,du\,dv$$ where $h_1,h_2$ are differentiable.  Then if we let  $$G(v) = \int_{g_1(x,v)}^{g_2(x,v)} f\,du\,dv$$ We get $\frac{d}{dx}F(x,y) = G(h_2(x)h_2'(x) - G(h_1(x))h_1'(x)$.  Does this seem correct?  This does not seem like a very adequate answer. I'd like to ask for some more insight into problems of this form, verification for if the my attempt had any validity, and a more complete answer if possible. Thanks.",,"['real-analysis', 'integration', 'multivariable-calculus']"
85,Total derivative not unique?,Total derivative not unique?,,"I probably did something wrong as I get two different total derivatives, but I don't see what. I use this definition: https://en.wikipedia.org/wiki/Total_derivative#The_total_derivative_as_a_linear_map Let $f(x, y) = (x + y, x^2 + y^2, xy)$. Then $f(\xi + h) - f(\xi) = (h_1 + h_2, 2\xi_1h_1 + 2\xi_2h_2 + h_1^2 + h_2^2, \xi_1h_2 + \xi_2h_1 + h_1h_2) = (h_1 + h_2, 2\xi_1h_1 + 2\xi_2h_2, \xi_1h_2 + \xi_2h_1) + (0, ||h||^2, h_1h_2)$ but also $= (0, 2\xi_1h_1 + 2\xi_2h_2, \xi_1h_2 + \xi_2h_1) + (h_1 + h_2, ||h||^2, h_1h_2)$. In both cases, if we fill it in the definition with the first vector the linear map aka total derivative and call the second $R(h)$ we get in both cases $0 < \frac{||R(h)||}{||h||} < \frac{||h||^2}{||h||} = ||h||$, so both tend to 0 as h tends to 0 by the squeeze theorem. But that would mean we have two different total differentials which is impossible. Where do I go wrong?","I probably did something wrong as I get two different total derivatives, but I don't see what. I use this definition: https://en.wikipedia.org/wiki/Total_derivative#The_total_derivative_as_a_linear_map Let $f(x, y) = (x + y, x^2 + y^2, xy)$. Then $f(\xi + h) - f(\xi) = (h_1 + h_2, 2\xi_1h_1 + 2\xi_2h_2 + h_1^2 + h_2^2, \xi_1h_2 + \xi_2h_1 + h_1h_2) = (h_1 + h_2, 2\xi_1h_1 + 2\xi_2h_2, \xi_1h_2 + \xi_2h_1) + (0, ||h||^2, h_1h_2)$ but also $= (0, 2\xi_1h_1 + 2\xi_2h_2, \xi_1h_2 + \xi_2h_1) + (h_1 + h_2, ||h||^2, h_1h_2)$. In both cases, if we fill it in the definition with the first vector the linear map aka total derivative and call the second $R(h)$ we get in both cases $0 < \frac{||R(h)||}{||h||} < \frac{||h||^2}{||h||} = ||h||$, so both tend to 0 as h tends to 0 by the squeeze theorem. But that would mean we have two different total differentials which is impossible. Where do I go wrong?",,"['real-analysis', 'multivariable-calculus', 'frechet-derivative']"
86,"Show that if $f(tx)=tf(x)$ for all $t>0,x\in E\setminus\{0\}$ and $f\in C^1(E,F)$ then $f\in\mathcal L(E,F)$",Show that if  for all  and  then,"f(tx)=tf(x) t>0,x\in E\setminus\{0\} f\in C^1(E,F) f\in\mathcal L(E,F)","Can someone confirm if this exercise is correctly done? The context of the exercise is multivariable calculus, that is, $E$ and $F$ are Banach spaces of unknown dimension. What I did: I must show that if $f\in C^1(E,F)$ and $f(tx)=tf(x)$ for all $t>0$ for all $x\in E\setminus\{0\}$ then $f\in\mathcal L(E,F)$. Using the chain rule I know that $$\partial[f(tx)]=t\partial f(tx)=t\partial f(x)=\partial[t f(x)]\implies \partial f(tx)=\partial f(x),\quad \forall t>0$$ Thus, by the continuity of $\partial f$, $\partial f(x)$ is a constant function for each $x\in E$, and by differentiation it can be seen that $$f(x)=\partial f(x)x+v\quad\text{and}\quad f(tx)=tf(x)\implies v=0$$ Thus $f$ is linear, as required. UPDATE: I now think that the above is not totally correct. I solved the exercise in a different way (with a more explicit result) using the fact that $f$ is differentiable at zero. From the first attempt above we knows that $\partial f$ is a constant function in the sets $\{tx:t>0\text{ and } x\in E\setminus\{0\}\}$. And because $f$ is differentiable at zero by assumption then using the directional derivatives of $f$ at zero we can see that $$D_vf(0):=\lim_{t\to 0}\frac{f(0+tv)-f(0)}t\in F\implies\matrix{f(0)=0\;\text{ and }\\D_vf(0)=f(v)=-f(v),\forall v\in E\setminus\{0\}}$$ Thus $f$ is a constant function and because $f(0)=0$ then $f=0$.","Can someone confirm if this exercise is correctly done? The context of the exercise is multivariable calculus, that is, $E$ and $F$ are Banach spaces of unknown dimension. What I did: I must show that if $f\in C^1(E,F)$ and $f(tx)=tf(x)$ for all $t>0$ for all $x\in E\setminus\{0\}$ then $f\in\mathcal L(E,F)$. Using the chain rule I know that $$\partial[f(tx)]=t\partial f(tx)=t\partial f(x)=\partial[t f(x)]\implies \partial f(tx)=\partial f(x),\quad \forall t>0$$ Thus, by the continuity of $\partial f$, $\partial f(x)$ is a constant function for each $x\in E$, and by differentiation it can be seen that $$f(x)=\partial f(x)x+v\quad\text{and}\quad f(tx)=tf(x)\implies v=0$$ Thus $f$ is linear, as required. UPDATE: I now think that the above is not totally correct. I solved the exercise in a different way (with a more explicit result) using the fact that $f$ is differentiable at zero. From the first attempt above we knows that $\partial f$ is a constant function in the sets $\{tx:t>0\text{ and } x\in E\setminus\{0\}\}$. And because $f$ is differentiable at zero by assumption then using the directional derivatives of $f$ at zero we can see that $$D_vf(0):=\lim_{t\to 0}\frac{f(0+tv)-f(0)}t\in F\implies\matrix{f(0)=0\;\text{ and }\\D_vf(0)=f(v)=-f(v),\forall v\in E\setminus\{0\}}$$ Thus $f$ is a constant function and because $f(0)=0$ then $f=0$.",,"['multivariable-calculus', 'proof-verification', 'frechet-derivative']"
87,Wronskian for multivariate functions,Wronskian for multivariate functions,,"I've been reading about the wronskian and I got stuck in the following: Suppose we are given a multivariate function describing e.g. a plane: $z = m_1 x + m_2 y + b$. How is the wronskian computed? We have here to two variables ($x,y$). How is this case dealt with the wronskian? Best regards","I've been reading about the wronskian and I got stuck in the following: Suppose we are given a multivariate function describing e.g. a plane: $z = m_1 x + m_2 y + b$. How is the wronskian computed? We have here to two variables ($x,y$). How is this case dealt with the wronskian? Best regards",,"['multivariable-calculus', 'wronskian']"
88,Difference Between Double vs Single Integration in Polar Coordinates,Difference Between Double vs Single Integration in Polar Coordinates,,"I am familiar with integrating polar equations in single variable calculus to find area. For example, to find the area enclosed by one loop of the four-leaved rose $r= \cos(2\theta)$ I would preform the integral $\frac {1}{2}\int^{\pi/4}_{-\pi/4}\cos^{2}\left(2\theta \right) d\theta$. Now, I am learning to do double integrals on polar coordinates, but I am conceptually stuck. A problem given by my textbook is to evaluate $$\int^{2\pi}_{\pi}\int^{7}_{4}rdrd\theta$$ the answer to the question is found here. The answer states that the region described looks like this: I am used to thinking that an integral essentially takes a function in one dimension makes its antiderivative, which is described in one higher dimension. So, when performing a double integral I expect a volume, although the answer is an area. Because it is an area, it is no different than a single integral, but this must be wrong. I am thinking that the inter integral creates a line from $r=4$ to $r=7$, and then the outer integral sweeps this line from $\pi$ to $2\pi$ to create an area. I do not understand what the purpose of this would be, when we could essentially use single variable calculus to find the areas of two half circles and subtract the smaller one. What is the purpose and conceptual background of double integrals like the one above?","I am familiar with integrating polar equations in single variable calculus to find area. For example, to find the area enclosed by one loop of the four-leaved rose $r= \cos(2\theta)$ I would preform the integral $\frac {1}{2}\int^{\pi/4}_{-\pi/4}\cos^{2}\left(2\theta \right) d\theta$. Now, I am learning to do double integrals on polar coordinates, but I am conceptually stuck. A problem given by my textbook is to evaluate $$\int^{2\pi}_{\pi}\int^{7}_{4}rdrd\theta$$ the answer to the question is found here. The answer states that the region described looks like this: I am used to thinking that an integral essentially takes a function in one dimension makes its antiderivative, which is described in one higher dimension. So, when performing a double integral I expect a volume, although the answer is an area. Because it is an area, it is no different than a single integral, but this must be wrong. I am thinking that the inter integral creates a line from $r=4$ to $r=7$, and then the outer integral sweeps this line from $\pi$ to $2\pi$ to create an area. I do not understand what the purpose of this would be, when we could essentially use single variable calculus to find the areas of two half circles and subtract the smaller one. What is the purpose and conceptual background of double integrals like the one above?",,"['multivariable-calculus', 'polar-coordinates']"
89,Surface element of inverted cone in cylindrical coordinates.,Surface element of inverted cone in cylindrical coordinates.,,"I have to integrate over the curved surface of a inverted cone, since the surface is of a cone I think all three $s, \phi$ and $z$ are varying under the integral sign. Now this causes the problem, I don't know surface element when all three are varying nor I can find a solution on internet,  I thought I will take $\displaystyle|d\vec a| = s\ ds\ d\phi\ dz$ but that is volume element. What should I do ? Drop $s$ from $\displaystyle |d\vec a| = s\ ds\ d\phi \ dz$ ?","I have to integrate over the curved surface of a inverted cone, since the surface is of a cone I think all three $s, \phi$ and $z$ are varying under the integral sign. Now this causes the problem, I don't know surface element when all three are varying nor I can find a solution on internet,  I thought I will take $\displaystyle|d\vec a| = s\ ds\ d\phi\ dz$ but that is volume element. What should I do ? Drop $s$ from $\displaystyle |d\vec a| = s\ ds\ d\phi \ dz$ ?",,"['calculus', 'integration', 'multivariable-calculus']"
90,Evaluate the line integral $\int_C zdx+xdy+ydz $,Evaluate the line integral,\int_C zdx+xdy+ydz ,"Calculate $\int_C zdx+xdy+ydz $ where $C= \{(x,y,z) \;| \;x=t, y=t^2, z=t^3, 0 \leq t \leq 1 \} $ So $$x=t $$ $$dx=1 $$ $$y=t^2 $$ $$dy=2t $$ $$z=t^3 $$ $$dz=3t^2 $$ $$\int_{0}^{1} [t^3(1)+t(2t)+t^2(3t^2)]dt $$ $$\int_{0}^{1} (t^3+2t^2+3t^4)dt $$ $$[\frac{t^4}{4}+\frac{2t^3}{3}+\frac{3t^5}{5}]_{0}^{1} $$ $$=\frac{91}{60} $$","Calculate $\int_C zdx+xdy+ydz $ where $C= \{(x,y,z) \;| \;x=t, y=t^2, z=t^3, 0 \leq t \leq 1 \} $ So $$x=t $$ $$dx=1 $$ $$y=t^2 $$ $$dy=2t $$ $$z=t^3 $$ $$dz=3t^2 $$ $$\int_{0}^{1} [t^3(1)+t(2t)+t^2(3t^2)]dt $$ $$\int_{0}^{1} (t^3+2t^2+3t^4)dt $$ $$[\frac{t^4}{4}+\frac{2t^3}{3}+\frac{3t^5}{5}]_{0}^{1} $$ $$=\frac{91}{60} $$",,"['multivariable-calculus', 'line-integrals']"
91,Calculate line integral $\oint _C d \overrightarrow{r } \times \overrightarrow{a }$,Calculate line integral,\oint _C d \overrightarrow{r } \times \overrightarrow{a },"Calculate line integral: $$\oint_C d \overrightarrow{r}  \times \overrightarrow{a},$$ where $\overrightarrow{a} = -yz\overrightarrow{i} + xz \overrightarrow{j} +xy \overrightarrow{k}$ , and curve $C$ is intersection of surfaces given by $ x^2+y^2+z^2 = 1$, $y=x^2$, positively oriented looked from the positive part of axis $Oy$. I first tried to calculate the intersection vector $\overrightarrow{r }$ by substituting $y=x^2$ into $ x^2+y^2+z^2 = 1$, and completing the square with $y+1/2$ and got the following : $ \overrightarrow{r}= \pm\sqrt{ \sqrt{5/4} \cos(t) - 1/2}\overrightarrow{i}, (\sqrt{5/4}\cos(t) - 1/2 )\overrightarrow{j}, (\sqrt{5/4}\sin(t) )\overrightarrow{k}$, $  -\cos^{-1} (1/\sqrt 5)<t< + \cos^{-1}(1/\sqrt 5)$ After that i tried to vector multiply the two vectors, after differentiating $\overrightarrow{r}$ first, but I got some difficult expressions to integrate. Is there an easier way to do this?","Calculate line integral: $$\oint_C d \overrightarrow{r}  \times \overrightarrow{a},$$ where $\overrightarrow{a} = -yz\overrightarrow{i} + xz \overrightarrow{j} +xy \overrightarrow{k}$ , and curve $C$ is intersection of surfaces given by $ x^2+y^2+z^2 = 1$, $y=x^2$, positively oriented looked from the positive part of axis $Oy$. I first tried to calculate the intersection vector $\overrightarrow{r }$ by substituting $y=x^2$ into $ x^2+y^2+z^2 = 1$, and completing the square with $y+1/2$ and got the following : $ \overrightarrow{r}= \pm\sqrt{ \sqrt{5/4} \cos(t) - 1/2}\overrightarrow{i}, (\sqrt{5/4}\cos(t) - 1/2 )\overrightarrow{j}, (\sqrt{5/4}\sin(t) )\overrightarrow{k}$, $  -\cos^{-1} (1/\sqrt 5)<t< + \cos^{-1}(1/\sqrt 5)$ After that i tried to vector multiply the two vectors, after differentiating $\overrightarrow{r}$ first, but I got some difficult expressions to integrate. Is there an easier way to do this?",,"['calculus', 'multivariable-calculus', 'line-integrals']"
92,Definition of differential of multivariable function,Definition of differential of multivariable function,,"I have this definition: $f:R^n  R^m$ is differentiable at $aR^n$, if there exists a linear transformation $:R^nR^m$ such that $\lim_{h \to 0} \frac{|f(a+h)-f(a)-\mu(h)|}{|h|} = 0$. My questions are what's the linear transformation $(h)$ for? What does it mean and where does it come from? Why is it necessary? Can anyone explain the definition to me a bit better? Thanks","I have this definition: $f:R^n  R^m$ is differentiable at $aR^n$, if there exists a linear transformation $:R^nR^m$ such that $\lim_{h \to 0} \frac{|f(a+h)-f(a)-\mu(h)|}{|h|} = 0$. My questions are what's the linear transformation $(h)$ for? What does it mean and where does it come from? Why is it necessary? Can anyone explain the definition to me a bit better? Thanks",,"['multivariable-calculus', 'differential']"
93,Product rule of Curl,Product rule of Curl,,"I know how to do a to d.  For e, i let $\phi = \frac{1}{|r|^3}$ and $A = a \times r$, tried to simplify but did not reach the answer. Anything can help. THank you","I know how to do a to d.  For e, i let $\phi = \frac{1}{|r|^3}$ and $A = a \times r$, tried to simplify but did not reach the answer. Anything can help. THank you",,['multivariable-calculus']
94,Computing Hessian of a particular function,Computing Hessian of a particular function,,"Let $x \in \mathbb{R}^k$, $D \in \mathbb{R}^{k \times k}$, and $b \in \mathbb{R}$. Assume $D$ is symmetric. Consider the function $f : \mathbb{R}^k \rightarrow \mathbb{R}$ defined by $$f(x) = (x^TDx - b^2)^2.$$ I want to compute the Hessian $\nabla^2f$. The gradient can be computed via the chain rule: \begin{align*} \nabla f(x) & = 2(x^TDx - b^2)Dx. \\ & = 2[(x^TDx)Dx - b^2Dx].\end{align*} If we differentiate the second term again, we get $\nabla(b^2Dx) = b^2D$. However, I am not sure how to differentiate the first term. Can anyone help with this?","Let $x \in \mathbb{R}^k$, $D \in \mathbb{R}^{k \times k}$, and $b \in \mathbb{R}$. Assume $D$ is symmetric. Consider the function $f : \mathbb{R}^k \rightarrow \mathbb{R}$ defined by $$f(x) = (x^TDx - b^2)^2.$$ I want to compute the Hessian $\nabla^2f$. The gradient can be computed via the chain rule: \begin{align*} \nabla f(x) & = 2(x^TDx - b^2)Dx. \\ & = 2[(x^TDx)Dx - b^2Dx].\end{align*} If we differentiate the second term again, we get $\nabla(b^2Dx) = b^2D$. However, I am not sure how to differentiate the first term. Can anyone help with this?",,"['multivariable-calculus', 'derivatives']"
95,"Determine differentiability of $f(x,y)=\frac{x^3y}{x^4+y^2}, f(0,0)=0$ at the origin",Determine differentiability of  at the origin,"f(x,y)=\frac{x^3y}{x^4+y^2}, f(0,0)=0","I want to determine the differentiability of the following function at the origin $(0,0)$: \begin{equation} f(x,y)=\begin{cases} \frac{x^3y}{x^4+y^2} & x \neq 0, y\neq 0 \\ 0, & x=y=0\end{cases} \end{equation} I proved that the partial derivatives of the function, as well as its directional derivatives, at $(0,0)$  all equal zero using the limit definition. The definition for differentiable that I use is: $$\lim_{h\rightarrow 0}\frac{f(a+h) - f(a)-Ah}{||h||}=0$$ I haven't studied the polar notation yet, so if possible please refrain from using it. Because of the partial derivatives, $A=(0,0)$ although I couldn't get very far with that: $$\lim_{h\rightarrow 0}\frac{f((0,0)+(h_x,h_y)) - 0-0}{||h||}=\lim_{h\rightarrow 0}\frac{h_x^3\cdot h_y}{(h_x^4+h_y^2)(h_x^2+h_y^2)^{1/2}}$$ And because of the coefficients, I couldn't find a path that would help me disprove it (as is done often,$(t,t),(t-t)(1/n,1/n^2),(t,t^2)$). So I looked at the two pages of questions regarding this function here on MO. Most of them focused on showing that it is continuous at the origin, which is nice but insufficient. This says that it is not differentiable, but the path doesn't seem right $$\frac{t^3t^2}{t^4+t^4}=\frac{t^5}{2t^4}=\frac{t}{2}$$ I didn't find it in the book available online. Looking at the graph however, it is more similar to the non-diffetiable exampe from math-insight but going by the drawing is not enough. So, can you help me either prove or disprove its differntiablity at the origin? Don't do my homework though, just a little help with the next step or pointing to an error I made would be much appreciated. Thanks.","I want to determine the differentiability of the following function at the origin $(0,0)$: \begin{equation} f(x,y)=\begin{cases} \frac{x^3y}{x^4+y^2} & x \neq 0, y\neq 0 \\ 0, & x=y=0\end{cases} \end{equation} I proved that the partial derivatives of the function, as well as its directional derivatives, at $(0,0)$  all equal zero using the limit definition. The definition for differentiable that I use is: $$\lim_{h\rightarrow 0}\frac{f(a+h) - f(a)-Ah}{||h||}=0$$ I haven't studied the polar notation yet, so if possible please refrain from using it. Because of the partial derivatives, $A=(0,0)$ although I couldn't get very far with that: $$\lim_{h\rightarrow 0}\frac{f((0,0)+(h_x,h_y)) - 0-0}{||h||}=\lim_{h\rightarrow 0}\frac{h_x^3\cdot h_y}{(h_x^4+h_y^2)(h_x^2+h_y^2)^{1/2}}$$ And because of the coefficients, I couldn't find a path that would help me disprove it (as is done often,$(t,t),(t-t)(1/n,1/n^2),(t,t^2)$). So I looked at the two pages of questions regarding this function here on MO. Most of them focused on showing that it is continuous at the origin, which is nice but insufficient. This says that it is not differentiable, but the path doesn't seem right $$\frac{t^3t^2}{t^4+t^4}=\frac{t^5}{2t^4}=\frac{t}{2}$$ I didn't find it in the book available online. Looking at the graph however, it is more similar to the non-diffetiable exampe from math-insight but going by the drawing is not enough. So, can you help me either prove or disprove its differntiablity at the origin? Don't do my homework though, just a little help with the next step or pointing to an error I made would be much appreciated. Thanks.",,['multivariable-calculus']
96,How to find $\theta$ bounds when calculating the volume enclosed between sphere $x^2+y^2+z^2=4a^2$ and cylinder $x^2+(y-a)^2=a^2$?,How to find  bounds when calculating the volume enclosed between sphere  and cylinder ?,\theta x^2+y^2+z^2=4a^2 x^2+(y-a)^2=a^2,"I need to find $\theta$ bounds when calculating the volume enclosed between sphere $x^2+y^2+z^2=4a^2$ and cylinder $x^2+(y-a)^2=a^2$. The final answer must be that volume=$\frac{48\pi-64}{9}$. Here's an illustration: Let $$V=\int\int\int_Bf(\theta,r,z)dV$$ Because of the symmetry of volume relative of the $y$ axis we can calculate the volume of the upper half then multiply it by 2. Then: $$ 0\le z\le \sqrt{4a^2-x^2-y^2} \le \sqrt{4a^2-r^2} $$ Because our the given surfaces don't have their center at the origin we need to adjust the $r$. We can retrieve it from the cylinder equation, $r=2a\sin\theta$. So: $$ 0\le r\le 2a\sin\theta $$ Finally, the projection of the surface onto the $xy$ plane is a circle. So I think that $0\le \theta \le 2\pi$. But I don't get the correct answer with this angle range. If we evaluate the triple integral we'll get to this expression in the end: $$ E=-\frac{8a^3}{3}(\frac{1}{3}\cos^2\theta \sin\theta+\frac{2}{3}\sin\theta-\theta) $$ For example: $$ \int_0^{2\pi}E=\frac{16\pi}{3}a^3 $$ which is not the answer I should've got. However if I choose the following bounds for $\theta$: $$ \int_0^{\frac{\pi}{2}}E=\frac{12\pi-16}{9}a^3 $$ The last bounds give the volume of the quarter of the original volume so if we multiply it by $4$ we get the desired answer: $$ \frac{12\pi-16}{9}a^3\cdot 4=\frac{48\pi-64}{9} $$ Please explain why my bounds are not good.","I need to find $\theta$ bounds when calculating the volume enclosed between sphere $x^2+y^2+z^2=4a^2$ and cylinder $x^2+(y-a)^2=a^2$. The final answer must be that volume=$\frac{48\pi-64}{9}$. Here's an illustration: Let $$V=\int\int\int_Bf(\theta,r,z)dV$$ Because of the symmetry of volume relative of the $y$ axis we can calculate the volume of the upper half then multiply it by 2. Then: $$ 0\le z\le \sqrt{4a^2-x^2-y^2} \le \sqrt{4a^2-r^2} $$ Because our the given surfaces don't have their center at the origin we need to adjust the $r$. We can retrieve it from the cylinder equation, $r=2a\sin\theta$. So: $$ 0\le r\le 2a\sin\theta $$ Finally, the projection of the surface onto the $xy$ plane is a circle. So I think that $0\le \theta \le 2\pi$. But I don't get the correct answer with this angle range. If we evaluate the triple integral we'll get to this expression in the end: $$ E=-\frac{8a^3}{3}(\frac{1}{3}\cos^2\theta \sin\theta+\frac{2}{3}\sin\theta-\theta) $$ For example: $$ \int_0^{2\pi}E=\frac{16\pi}{3}a^3 $$ which is not the answer I should've got. However if I choose the following bounds for $\theta$: $$ \int_0^{\frac{\pi}{2}}E=\frac{12\pi-16}{9}a^3 $$ The last bounds give the volume of the quarter of the original volume so if we multiply it by $4$ we get the desired answer: $$ \frac{12\pi-16}{9}a^3\cdot 4=\frac{48\pi-64}{9} $$ Please explain why my bounds are not good.",,"['integration', 'multivariable-calculus', 'polar-coordinates', 'volume']"
97,Understanding a computation of a surface integral on a cube,Understanding a computation of a surface integral on a cube,,"Calculate the flow of the vector field $$\mathbf{A} = \nabla \dfrac{\mathbf{a}\cdot\mathbf{r}}{r^3}$$ from a cube with side length $1$, centered at origin and with one space diagonal parallel with the constant vector $\mathbf{a}$. Attempted solution Let the $z$-axis run parallel to $\mathbf{a}$ in a cartesian coordinate system. Introduce spherical coordinates so that $$\mathbf{A} = \nabla \dfrac{\mathbf{a}\cdot\mathbf{r}}{r^3} = \nabla \dfrac{a\cos\theta}{r^2} = \sum_{i} \dfrac{1}{h_i} \dfrac{\partial (\dfrac{a\cos\theta}{r^2})}{\partial u_i} \hat{e}_i = \dfrac{-2a\cos\theta}{r^3}\hat{e}_r - \dfrac{a\sin\theta}{r^3}\hat{e}_{\theta}$$  Now what my book does is saying that $\nabla \cdot\mathbf{A}=0$ for $r\neq0$ and that the flow out of the cube is the same as the flow out ofa sphere centered at origin. It then goes to show that $$\iint_{r=R}\mathbf{A}\cdot \hat{e}_r dS = -\dfrac{2a}{R}2\pi \int_{0}^{\pi}\cos\theta\sin\theta d\theta=0$$ I understand every step except for: 1) Why did they calculate the divergence? (If Gauss' theorem is applicable here (which it should be) then that would give the result instantly). 2) How can they claim the flow is the same for the cube as it is for some sphere with radius $R$?","Calculate the flow of the vector field $$\mathbf{A} = \nabla \dfrac{\mathbf{a}\cdot\mathbf{r}}{r^3}$$ from a cube with side length $1$, centered at origin and with one space diagonal parallel with the constant vector $\mathbf{a}$. Attempted solution Let the $z$-axis run parallel to $\mathbf{a}$ in a cartesian coordinate system. Introduce spherical coordinates so that $$\mathbf{A} = \nabla \dfrac{\mathbf{a}\cdot\mathbf{r}}{r^3} = \nabla \dfrac{a\cos\theta}{r^2} = \sum_{i} \dfrac{1}{h_i} \dfrac{\partial (\dfrac{a\cos\theta}{r^2})}{\partial u_i} \hat{e}_i = \dfrac{-2a\cos\theta}{r^3}\hat{e}_r - \dfrac{a\sin\theta}{r^3}\hat{e}_{\theta}$$  Now what my book does is saying that $\nabla \cdot\mathbf{A}=0$ for $r\neq0$ and that the flow out of the cube is the same as the flow out ofa sphere centered at origin. It then goes to show that $$\iint_{r=R}\mathbf{A}\cdot \hat{e}_r dS = -\dfrac{2a}{R}2\pi \int_{0}^{\pi}\cos\theta\sin\theta d\theta=0$$ I understand every step except for: 1) Why did they calculate the divergence? (If Gauss' theorem is applicable here (which it should be) then that would give the result instantly). 2) How can they claim the flow is the same for the cube as it is for some sphere with radius $R$?",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
98,Vector calculus identities and theorems to move derivatives over,Vector calculus identities and theorems to move derivatives over,,"Let $\Omega \subset \mathbb{R}^2$. Then we have \begin{align*} \int_{\Omega} \textbf{v} \cdot \nabla (\nabla\cdot \textbf{u})dX &= \int_{\Omega} \begin{bmatrix} v_1 \\ v_2\end{bmatrix} \cdot \nabla (u_{1,x} + u_{2,y}) dX, \\ &= \int_{\Omega} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} \cdot \begin{bmatrix} u_{1,xx} + u_{2,yx} \\ u_{1,xy} + u_{2,yy}\end{bmatrix} dX, \\ &= \int_{\Omega} v_1 (u_{1,xx} + u_{2,xy}) + v_2 (u_{1,xy} + u_{2,yy}) dX. \end{align*} Is there an easy way to ""move the derivatives"" over to the components of $\textbf{v}$ (by using the Divergence theorem or Integration by parts, perhaps)? I want to rewrite my starting integral as the sum of an integral over $\Omega$ and an integral over the boundary $\partial \Omega$. The motivation for this is in deriving the weak form a BVP involving linear elasticity. Are there any vector calculus identities that give me what I want immediately? See here: Deriving the weak form for linear elasticity equation","Let $\Omega \subset \mathbb{R}^2$. Then we have \begin{align*} \int_{\Omega} \textbf{v} \cdot \nabla (\nabla\cdot \textbf{u})dX &= \int_{\Omega} \begin{bmatrix} v_1 \\ v_2\end{bmatrix} \cdot \nabla (u_{1,x} + u_{2,y}) dX, \\ &= \int_{\Omega} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} \cdot \begin{bmatrix} u_{1,xx} + u_{2,yx} \\ u_{1,xy} + u_{2,yy}\end{bmatrix} dX, \\ &= \int_{\Omega} v_1 (u_{1,xx} + u_{2,xy}) + v_2 (u_{1,xy} + u_{2,yy}) dX. \end{align*} Is there an easy way to ""move the derivatives"" over to the components of $\textbf{v}$ (by using the Divergence theorem or Integration by parts, perhaps)? I want to rewrite my starting integral as the sum of an integral over $\Omega$ and an integral over the boundary $\partial \Omega$. The motivation for this is in deriving the weak form a BVP involving linear elasticity. Are there any vector calculus identities that give me what I want immediately? See here: Deriving the weak form for linear elasticity equation",,"['multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations']"
99,Prove that $f$ is an Affine function,Prove that  is an Affine function,f,"Definition: An affine function is a function composed of a linear function + a constant and its graph is a straight line. A topological space $X$ is said to be disconnected if it is the union of two disjoint nonempty open sets. Otherwise, $X$ is said to be connected. Question: Assume that $U \subseteq \mathbb R^n$ is a connected set and $f : U  \to \mathbb R^m$ is a function such that we have $\forall x \in U  \quad Df(x)=A$   Such that $A: \mathbb R^n \to \mathbb R^m$ is a   linear transformation. Prove that $f$ is an Affine function. Note: We just know that the derivative is a linear transformation. I know that its like the case of working with normal derivaties in $\mathbb R$. But, It's not the same. I can't for example do integration! Any idea on reaching the statement?  (Please include more details so that i can understand it)","Definition: An affine function is a function composed of a linear function + a constant and its graph is a straight line. A topological space $X$ is said to be disconnected if it is the union of two disjoint nonempty open sets. Otherwise, $X$ is said to be connected. Question: Assume that $U \subseteq \mathbb R^n$ is a connected set and $f : U  \to \mathbb R^m$ is a function such that we have $\forall x \in U  \quad Df(x)=A$   Such that $A: \mathbb R^n \to \mathbb R^m$ is a   linear transformation. Prove that $f$ is an Affine function. Note: We just know that the derivative is a linear transformation. I know that its like the case of working with normal derivaties in $\mathbb R$. But, It's not the same. I can't for example do integration! Any idea on reaching the statement?  (Please include more details so that i can understand it)",,"['multivariable-calculus', 'derivatives']"
