,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Convergence of an alternating harmonic series,Convergence of an alternating harmonic series,,"Consider the series $$\sum_{n=1}^\infty c_n \cdot \tfrac 1n$$ where $c_n$ is either $-1$ or $1$. In my case I have $$c_n = \begin{cases} 1&; \lceil np \rceil - \lceil (n-1)p \rceil = 1 \\ -1 &; \lceil np \rceil - \lceil (n-1)p \rceil = 0 \end{cases}$$ with $p\in(0,1)$. My question: For which $p$ does the series converges? I know that the series converges for $p=\tfrac 12$ because of the alternating series test . What about the cases $p \neq \tfrac 12$?","Consider the series $$\sum_{n=1}^\infty c_n \cdot \tfrac 1n$$ where $c_n$ is either $-1$ or $1$. In my case I have $$c_n = \begin{cases} 1&; \lceil np \rceil - \lceil (n-1)p \rceil = 1 \\ -1 &; \lceil np \rceil - \lceil (n-1)p \rceil = 0 \end{cases}$$ with $p\in(0,1)$. My question: For which $p$ does the series converges? I know that the series converges for $p=\tfrac 12$ because of the alternating series test . What about the cases $p \neq \tfrac 12$?",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
1,Convergence of $\int_0^\infty $sin$ (x^p) dx$,Convergence of sin,\int_0^\infty   (x^p) dx,"Consider the $\displaystyle \int_0^\infty $sin$ (x^p) dx$. Does it converge when $p<0$? Does it converge when $p>1$? My Work: Let $x^p=y$, then $\displaystyle \int_0^\infty $sin$ \displaystyle (x^p) dx=\frac{1}{p}\sum_{n=1}^\infty \int_{(n-1)\pi}^{n\pi} \frac{\text{sin}  \; y}{y^r} dy$ , where $r=\frac{p-1}{p}>1 $when $p<0$ and $0<r=\frac{p-1}{p}\leq 1 $when $p>1$. I know that $\displaystyle \int_0^\infty $sin$ (\frac{1}{x}) dx$ diverges which is a special case of first case. But failed to show it generally. Can anyone please give me a hint to preceed?","Consider the $\displaystyle \int_0^\infty $sin$ (x^p) dx$. Does it converge when $p<0$? Does it converge when $p>1$? My Work: Let $x^p=y$, then $\displaystyle \int_0^\infty $sin$ \displaystyle (x^p) dx=\frac{1}{p}\sum_{n=1}^\infty \int_{(n-1)\pi}^{n\pi} \frac{\text{sin}  \; y}{y^r} dy$ , where $r=\frac{p-1}{p}>1 $when $p<0$ and $0<r=\frac{p-1}{p}\leq 1 $when $p>1$. I know that $\displaystyle \int_0^\infty $sin$ (\frac{1}{x}) dx$ diverges which is a special case of first case. But failed to show it generally. Can anyone please give me a hint to preceed?",,"['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals']"
2,Inequality between $L^2$- and $L^1$-norms for functions,Inequality between - and -norms for functions,L^2 L^1,"In the vector space $\mathbb{R}^n$, we have the inequality $$ ||x||_2 \leq ||x||_1 $$ where $x$ is a vector. I am wondering that we have similar inequality for function's norm. The $L^1$-norm of function $f$ is given by $$ ||f||_1=\int |f| d\mu,  $$ the $L^2$-norm is  $$ ||f||_2=\left( \int |f|^2 d\mu \right)^{1/2} $$ then, is this right? $$ ||f||_2\leq ||f||_1 $$ If so, how to prove it? And tell me what I should study. e.g. real analysis or Lebesgue integral.","In the vector space $\mathbb{R}^n$, we have the inequality $$ ||x||_2 \leq ||x||_1 $$ where $x$ is a vector. I am wondering that we have similar inequality for function's norm. The $L^1$-norm of function $f$ is given by $$ ||f||_1=\int |f| d\mu,  $$ the $L^2$-norm is  $$ ||f||_2=\left( \int |f|^2 d\mu \right)^{1/2} $$ then, is this right? $$ ||f||_2\leq ||f||_1 $$ If so, how to prove it? And tell me what I should study. e.g. real analysis or Lebesgue integral.",,"['real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure', 'functional-inequalities']"
3,Prove that the exponential function is differentiable,Prove that the exponential function is differentiable,,"Imagine that you are writing a book on the foundations of analysis. You have already proved that for each $a > 1$ there is a unique function $f_a(x) = a^x$ satisfying the following: $f_a$ is an isomorphism of ordered groups between $(\mathbb{R},+)$ and $(\mathbb{R}_{+},\cdot)$; $f_a(1) = a$. It follows from the monotonicity and bijectivity of $f_a$ that it is continuous. Now you would like to prove that $f_a$ is differentiable. At this point, you don't know anything about integration, differential equations or power series. What is the simplest or most elegant way of doing this?","Imagine that you are writing a book on the foundations of analysis. You have already proved that for each $a > 1$ there is a unique function $f_a(x) = a^x$ satisfying the following: $f_a$ is an isomorphism of ordered groups between $(\mathbb{R},+)$ and $(\mathbb{R}_{+},\cdot)$; $f_a(1) = a$. It follows from the monotonicity and bijectivity of $f_a$ that it is continuous. Now you would like to prove that $f_a$ is differentiable. At this point, you don't know anything about integration, differential equations or power series. What is the simplest or most elegant way of doing this?",,"['real-analysis', 'exponential-function', 'education']"
4,Can $\sum_1^n 1/k$ be arranged so that it is an integer for infinitely many $n?$,Can  be arranged so that it is an integer for infinitely many,\sum_1^n 1/k n?,"It's well known that when $n>1:$ $$\sum_{k=1}^n \frac{1}{k}\not\in \mathbb{N}$$ But if we are allowed to rearrange the series, we can for instance can get: $$\frac{1}{1}\in\mathbb{N},\quad\frac{1}{1}+\frac{1}{2}+\frac{1}{3}+\frac{1}{6}\in\mathbb{N},\quad \dots\;?$$ Maybe there is another obvious one that I didn't see. Question: Can the harmonic series be rearranged so that its partial sums hit infinitely many integers?","It's well known that when $n>1:$ $$\sum_{k=1}^n \frac{1}{k}\not\in \mathbb{N}$$ But if we are allowed to rearrange the series, we can for instance can get: $$\frac{1}{1}\in\mathbb{N},\quad\frac{1}{1}+\frac{1}{2}+\frac{1}{3}+\frac{1}{6}\in\mathbb{N},\quad \dots\;?$$ Maybe there is another obvious one that I didn't see. Question: Can the harmonic series be rearranged so that its partial sums hit infinitely many integers?",,"['real-analysis', 'number-theory', 'harmonic-numbers']"
5,"A closed-form of $\frac{1}{2}\int_0^\infty\left[\frac{x^2\cos x}{\cosh 2x-\cos x}-\frac{2x^2}{e^{4x}-2e^{2x}\cos x+1}\right]\,dx$",A closed-form of,"\frac{1}{2}\int_0^\infty\left[\frac{x^2\cos x}{\cosh 2x-\cos x}-\frac{2x^2}{e^{4x}-2e^{2x}\cos x+1}\right]\,dx","I am looking for a closed-form of this integral \begin{equation} \frac{1}{2}\int_0^\infty\left[\frac{x^2\cos x}{\cosh 2x-\cos x}-\frac{2x^2}{e^{4x}-2e^{2x}\cos x+1}\right]\,dx \end{equation} I can rewrite the integral into \begin{equation} \int_0^\infty\frac{x^2(e^{2x}\cos x-1)}{e^{4x}-2e^{2x}\cos x+1}\,dx \end{equation} But I am stuck for the next step. I have a strong feeling the integral involving gamma or beta function but I am unable to prove it. Could anyone here please help me? Any help would be greatly appreciated. Thank you.","I am looking for a closed-form of this integral \begin{equation} \frac{1}{2}\int_0^\infty\left[\frac{x^2\cos x}{\cosh 2x-\cos x}-\frac{2x^2}{e^{4x}-2e^{2x}\cos x+1}\right]\,dx \end{equation} I can rewrite the integral into \begin{equation} \int_0^\infty\frac{x^2(e^{2x}\cos x-1)}{e^{4x}-2e^{2x}\cos x+1}\,dx \end{equation} But I am stuck for the next step. I have a strong feeling the integral involving gamma or beta function but I am unable to prove it. Could anyone here please help me? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
6,"Does $\sum_{p\in\mathbb P}\frac {( - 1)^{[\sqrt p\,]}}{p}$ converges?",Does  converges?,"\sum_{p\in\mathbb P}\frac {( - 1)^{[\sqrt p\,]}}{p}","Does $$\sum_{p\in\mathbb P}\frac {( - 1)^{[\sqrt p\,]}}{p}$$ converges ? I know that the following $\sum_{p\in\mathbb P} \frac{1}{p}$ diverges, we can find proofs on Wikepedia Divergence of the sum of the reciprocals of the primes Unfortunately this one is more complicated. I think one needs to use some non-trivial facts about their distribution.","Does $$\sum_{p\in\mathbb P}\frac {( - 1)^{[\sqrt p\,]}}{p}$$ converges ? I know that the following $\sum_{p\in\mathbb P} \frac{1}{p}$ diverges, we can find proofs on Wikepedia Divergence of the sum of the reciprocals of the primes Unfortunately this one is more complicated. I think one needs to use some non-trivial facts about their distribution.",,"['real-analysis', 'sequences-and-series']"
7,Calculate $\int_0^1 e^x dx$ as a limit of a sum?,Calculate  as a limit of a sum?,\int_0^1 e^x dx,"As for now, I've been doing the opposite thing. For a given sum in terms of $n\in\mathbb{N}$ I had to calculate the limit (as $n$ approaches infinity) of that sum by applying: http://en.wikibooks.org/wiki/Calculus/Integration_techniques/Infinite_Sums , more precisely, a special case: $$[a, b] = [0, 1] , \space x_k = \frac{k}{n} \implies \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n f\bigg(\frac{k}{n}\bigg) = \int_0^1f(x)\,dx$$ Example: $$ \lim_{n\to\infty} \bigg( \frac{1}{n+1} + \frac{1}{n+2} + \cdots + \frac{1}{2n}\bigg) = \lim_{n\to\infty} \sum_{k=1}^n\frac{1}{n+k} = \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n\frac{1}{1 + \frac{k}{n}}$$ = integral sums ( I literally translated the term we use for this in my Analysis class, I don't know how it's called in English ) for $f(x) = \frac{1}{1+x}$ on $[0, 1]$ $$ = \int_0^1 \frac{1}{1+x} \,dx = \ln(1+x)\bigg\vert_0^1 = \ln2 - \ln1 = \ln2 $$ so the limit of the sum as $n$ approaches infinity is $\ln2$. The following exercise asks to reverse the process (i.e. to use integral sums to calculate the limit), so I have: $$\int_0^1 e^x \,dx$$ The regular approach gives me: $\int_0^1 e^x \,dx = $ Leibniz-Newton $ = e^x \vert_0^1 = e^1 - e^0 = e -1$ If this integral can be expressed as a limit of a sum, then it should (if I am correct) have the following form: $$\int_0^1 e^x \,dx = \lim_{n \to \infty}\frac{1}{n}\sum_{k=1}^ne^\frac{k}{n}= \lim_{n \to \infty}\sum_{k=1}^n\frac{e^\frac{k}{n}}{n}$$ And since one definition of $e$ is: $$e = \lim_{n\to\infty}\bigg(1+\frac{1}{n}\bigg)^n$$ I have: $$\int_0^1 e^x \, dx = \lim_{n \to \infty}\sum_{k=1}^n\frac{((1+\frac{1}{n})^n)^\frac{k}{n}}{n} = \lim_{n \to \infty}\sum_{k=1}^n\frac{(1+\frac{1}{n})^k}{n} =$$ $$\lim_{n \to \infty} \bigg[ \frac{(1+\frac{1}{n})^1}{n} + \frac{(1+\frac{1}{n})^2}{n} + \cdots + \frac{(1+\frac{1}{n})^{n-1}}{n} + \frac{(1+\frac{1}{n})^n}{n}\bigg]$$ ... which doesn't seem to be what I am looking for (or I am not seeing it). I would appreciate a hint (or two :))! EDIT: By using Ron Gordons big hint we have: $$\int_0^1 e^x\,dx = \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n e^\frac{k}{n} = \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{2}{n} + e^\frac{3}{n} + \cdots + e^\frac{n}{n} \bigg)$$ $$\lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{1}{n}\cdot e^\frac{1}{n} + e^\frac{1}{n} \cdot e^\frac{1}{n} \cdot e^\frac{1}{n} + \cdots + \underbrace{e^\frac{1}{n} \cdot e^\frac{1}{n} \cdots e^\frac{1}{n}}_{n \space factors} \bigg) =$$ $$ \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{1}{n} \cdot e^\frac{1}{n} + e^\frac{2}{n} \cdot e^\frac{1}{n} + e^\frac{3}{n} \cdot e^\frac{1}{n} + \cdots + e^\frac{n-1}{n} \cdot e^\frac{1}{n} \bigg) = $$ $$ \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^1}_q \cdot e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^2}_{q^2} \cdot e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^3}_{q^3} \cdot e^\frac{1}{n} + \cdots + \underbrace{(e^\frac{1}{n})^{n-1}}_{q^{n-1}} \cdot e^\frac{1}{n} \bigg)$$ The expression inside the parentheses can be interpreted as a sum of a geometric series: $a_1 + a_2 + a_3 + a_4 + \cdots + a_n = a_1 + q\cdot a_1 + q^2\cdot a_1 + q^3\cdot a_1 +  \cdots + q^{n-1}\cdot a_1$ where $a_1 = e^\frac{1}{n}$ and $q = e^\frac{1}{n}$ so $a_1 = q$ and by using the formula $S_n = a_1 \cdot \frac{q^n - 1}{q - 1}$ we get: $$ S_n = e^\frac{1}{n} \cdot \frac{(e^\frac{1}{n})^n - 1}{e^\frac{1}{n} - 1} = e^\frac{1}{n} \cdot \frac{e^\frac{n}{n} - 1}{e^\frac{1}{n} - 1} = e^\frac{1}{n} \cdot \frac{e - 1}{e^\frac{1}{n} - 1}$$ so our integral is actually the following limit: $$ \int_0^1 e^x\,dx = \lim_{n\to\infty} \frac{1}{n} \cdot S_n = \lim_{n\to\infty} \frac{1}{n} \cdot e^\frac{1}{n} \frac{e - 1}{e^\frac{1}{n} - 1} = $$ ... by using some properties of limits we get: $$ = \lim_{n\to\infty} e^\frac{1}{n} \cdot  \lim_{n\to\infty}\frac{1}{n}\frac{e - 1}{e^\frac{1}{n} - 1} = \lim_{n\to\infty} e^\frac{1}{n} \cdot \frac{\lim_{n\to\infty} (e - 1)}{\lim_{n\to\infty} n\cdot (e^\frac{1}{n} - 1)}$$ $$\lim_{n\to\infty} e^\frac{1}{n} = e^\frac{1}{+\infty} = e^0 = 1$$ $$\lim_{n\to\infty} (e - 1) = e - 1$$ so we have: $$\int_0^1 e^x\,dx = 1\cdot \frac{e - 1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)} = \frac{e - 1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)}$$ The only thing left is to show: $\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = 1$ Approach 1: $$\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = \begin{bmatrix} t = \frac{1}{n} \\ n \to \infty \implies t\to 0 \end{bmatrix} = \lim_{t\to 0} \frac{1}{t}(e^t - 1) = \lim_{t\to 0}\frac{e^t - 1}{t} = 1$$ Aproach 2: $$\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = [\infty \cdot 0] = \lim_{n\to\infty}\frac{(e^\frac{1}{n} - 1)}{\frac{1}{n}} = \bigg[\frac{0}{0}\bigg] = L'Hospital = \lim_{n\to\infty}\frac{(e^\frac{1}{n}\cdot (-\frac{1}{n^2}) - 0)}{-\frac{1}{n^2}} = \lim_{n\to\infty}\frac{e^\frac{1}{n}\cdot (-\frac{1}{n^2})}{-\frac{1}{n^2}} = \lim_{n\to\infty} e^\frac{1}{n} = e^\frac{1}{+\infty} = e^0 = 1$$ finally: $$\int_0^1 e^x\,dx = \frac{e-1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)} = \frac{e-1}{1} = e - 1$$","As for now, I've been doing the opposite thing. For a given sum in terms of $n\in\mathbb{N}$ I had to calculate the limit (as $n$ approaches infinity) of that sum by applying: http://en.wikibooks.org/wiki/Calculus/Integration_techniques/Infinite_Sums , more precisely, a special case: $$[a, b] = [0, 1] , \space x_k = \frac{k}{n} \implies \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n f\bigg(\frac{k}{n}\bigg) = \int_0^1f(x)\,dx$$ Example: $$ \lim_{n\to\infty} \bigg( \frac{1}{n+1} + \frac{1}{n+2} + \cdots + \frac{1}{2n}\bigg) = \lim_{n\to\infty} \sum_{k=1}^n\frac{1}{n+k} = \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n\frac{1}{1 + \frac{k}{n}}$$ = integral sums ( I literally translated the term we use for this in my Analysis class, I don't know how it's called in English ) for $f(x) = \frac{1}{1+x}$ on $[0, 1]$ $$ = \int_0^1 \frac{1}{1+x} \,dx = \ln(1+x)\bigg\vert_0^1 = \ln2 - \ln1 = \ln2 $$ so the limit of the sum as $n$ approaches infinity is $\ln2$. The following exercise asks to reverse the process (i.e. to use integral sums to calculate the limit), so I have: $$\int_0^1 e^x \,dx$$ The regular approach gives me: $\int_0^1 e^x \,dx = $ Leibniz-Newton $ = e^x \vert_0^1 = e^1 - e^0 = e -1$ If this integral can be expressed as a limit of a sum, then it should (if I am correct) have the following form: $$\int_0^1 e^x \,dx = \lim_{n \to \infty}\frac{1}{n}\sum_{k=1}^ne^\frac{k}{n}= \lim_{n \to \infty}\sum_{k=1}^n\frac{e^\frac{k}{n}}{n}$$ And since one definition of $e$ is: $$e = \lim_{n\to\infty}\bigg(1+\frac{1}{n}\bigg)^n$$ I have: $$\int_0^1 e^x \, dx = \lim_{n \to \infty}\sum_{k=1}^n\frac{((1+\frac{1}{n})^n)^\frac{k}{n}}{n} = \lim_{n \to \infty}\sum_{k=1}^n\frac{(1+\frac{1}{n})^k}{n} =$$ $$\lim_{n \to \infty} \bigg[ \frac{(1+\frac{1}{n})^1}{n} + \frac{(1+\frac{1}{n})^2}{n} + \cdots + \frac{(1+\frac{1}{n})^{n-1}}{n} + \frac{(1+\frac{1}{n})^n}{n}\bigg]$$ ... which doesn't seem to be what I am looking for (or I am not seeing it). I would appreciate a hint (or two :))! EDIT: By using Ron Gordons big hint we have: $$\int_0^1 e^x\,dx = \lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n e^\frac{k}{n} = \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{2}{n} + e^\frac{3}{n} + \cdots + e^\frac{n}{n} \bigg)$$ $$\lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{1}{n}\cdot e^\frac{1}{n} + e^\frac{1}{n} \cdot e^\frac{1}{n} \cdot e^\frac{1}{n} + \cdots + \underbrace{e^\frac{1}{n} \cdot e^\frac{1}{n} \cdots e^\frac{1}{n}}_{n \space factors} \bigg) =$$ $$ \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + e^\frac{1}{n} \cdot e^\frac{1}{n} + e^\frac{2}{n} \cdot e^\frac{1}{n} + e^\frac{3}{n} \cdot e^\frac{1}{n} + \cdots + e^\frac{n-1}{n} \cdot e^\frac{1}{n} \bigg) = $$ $$ \lim_{n\to\infty}\frac{1}{n} \bigg( e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^1}_q \cdot e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^2}_{q^2} \cdot e^\frac{1}{n} + \underbrace{(e^\frac{1}{n})^3}_{q^3} \cdot e^\frac{1}{n} + \cdots + \underbrace{(e^\frac{1}{n})^{n-1}}_{q^{n-1}} \cdot e^\frac{1}{n} \bigg)$$ The expression inside the parentheses can be interpreted as a sum of a geometric series: $a_1 + a_2 + a_3 + a_4 + \cdots + a_n = a_1 + q\cdot a_1 + q^2\cdot a_1 + q^3\cdot a_1 +  \cdots + q^{n-1}\cdot a_1$ where $a_1 = e^\frac{1}{n}$ and $q = e^\frac{1}{n}$ so $a_1 = q$ and by using the formula $S_n = a_1 \cdot \frac{q^n - 1}{q - 1}$ we get: $$ S_n = e^\frac{1}{n} \cdot \frac{(e^\frac{1}{n})^n - 1}{e^\frac{1}{n} - 1} = e^\frac{1}{n} \cdot \frac{e^\frac{n}{n} - 1}{e^\frac{1}{n} - 1} = e^\frac{1}{n} \cdot \frac{e - 1}{e^\frac{1}{n} - 1}$$ so our integral is actually the following limit: $$ \int_0^1 e^x\,dx = \lim_{n\to\infty} \frac{1}{n} \cdot S_n = \lim_{n\to\infty} \frac{1}{n} \cdot e^\frac{1}{n} \frac{e - 1}{e^\frac{1}{n} - 1} = $$ ... by using some properties of limits we get: $$ = \lim_{n\to\infty} e^\frac{1}{n} \cdot  \lim_{n\to\infty}\frac{1}{n}\frac{e - 1}{e^\frac{1}{n} - 1} = \lim_{n\to\infty} e^\frac{1}{n} \cdot \frac{\lim_{n\to\infty} (e - 1)}{\lim_{n\to\infty} n\cdot (e^\frac{1}{n} - 1)}$$ $$\lim_{n\to\infty} e^\frac{1}{n} = e^\frac{1}{+\infty} = e^0 = 1$$ $$\lim_{n\to\infty} (e - 1) = e - 1$$ so we have: $$\int_0^1 e^x\,dx = 1\cdot \frac{e - 1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)} = \frac{e - 1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)}$$ The only thing left is to show: $\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = 1$ Approach 1: $$\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = \begin{bmatrix} t = \frac{1}{n} \\ n \to \infty \implies t\to 0 \end{bmatrix} = \lim_{t\to 0} \frac{1}{t}(e^t - 1) = \lim_{t\to 0}\frac{e^t - 1}{t} = 1$$ Aproach 2: $$\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1) = [\infty \cdot 0] = \lim_{n\to\infty}\frac{(e^\frac{1}{n} - 1)}{\frac{1}{n}} = \bigg[\frac{0}{0}\bigg] = L'Hospital = \lim_{n\to\infty}\frac{(e^\frac{1}{n}\cdot (-\frac{1}{n^2}) - 0)}{-\frac{1}{n^2}} = \lim_{n\to\infty}\frac{e^\frac{1}{n}\cdot (-\frac{1}{n^2})}{-\frac{1}{n^2}} = \lim_{n\to\infty} e^\frac{1}{n} = e^\frac{1}{+\infty} = e^0 = 1$$ finally: $$\int_0^1 e^x\,dx = \frac{e-1}{\lim_{n\to\infty}n\cdot (e^\frac{1}{n} - 1)} = \frac{e-1}{1} = e - 1$$",,"['real-analysis', 'integration', 'definite-integrals']"
8,Integrating $ \int_2^4 \frac{ \sqrt{\ln(9-x)} }{ \sqrt{\ln(9-x)}+\sqrt{\ln(x+3)} } dx. $,Integrating, \int_2^4 \frac{ \sqrt{\ln(9-x)} }{ \sqrt{\ln(9-x)}+\sqrt{\ln(x+3)} } dx. ,Compute $$ \int_2^4 \frac{ \sqrt{\ln(9-x)}      }{ \sqrt{\ln(9-x)}+\sqrt{\ln(x+3)}        } dx. $$ I am not sure how to start this one...I am thinking of a substitution to get started.,Compute $$ \int_2^4 \frac{ \sqrt{\ln(9-x)}      }{ \sqrt{\ln(9-x)}+\sqrt{\ln(x+3)}        } dx. $$ I am not sure how to start this one...I am thinking of a substitution to get started.,,"['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
9,Help with epsilon delta definition,Help with epsilon delta definition,,"$$\lim_{x\to 3} x^2 = 9$$ Hi, I'm new to this community (and to calculus) and English isn't my first language, so I'm sorry if I'm not able to properly convey this problem I have. If I'm doing anything wrong, could someone please correct me? So, I know there has to be a $\delta>0$ for all $\epsilon>0$ given so that $0<|x-3|<\delta \implies |x^2-9|<\epsilon$ And I also know that $|x^2-9|<\epsilon$ can be written as $|x-3||x+3|<\epsilon$ The author (Stewart) used this method before, but I'm kind of confused... First, he used a positive constant $C$ so that $|x+3|<C$ I'd imagine that he's trying to ""restrict"" $|x+3|$ by restricting the interval... $|x-3|<1 \implies 2<x<4 \implies 5<x+3<7$ Then he chose $C = 7$ , which I guess comes from the fact that it's the upper bound of $x+3$ , so that $|x-3||x+3| < 7|x-3| < \epsilon \implies |x-3| < \frac{\epsilon}{7}$ And that $\delta = \min(1,\frac{\epsilon}{7})$ so that if $\epsilon > 7$ , it still works. I think I understand each individual part... But, for some reason, I feel something is missing. Why is that? Even after doing most of the exercises, this still feels alien to me. Maybe I should try reading another book as well? I've bought Spivak book (and it might take a while for it to arrive), does it teaches the $\epsilon - \delta$ differently? What should I do so that I get comfortable with it? Thanks in advance!","Hi, I'm new to this community (and to calculus) and English isn't my first language, so I'm sorry if I'm not able to properly convey this problem I have. If I'm doing anything wrong, could someone please correct me? So, I know there has to be a for all given so that And I also know that can be written as The author (Stewart) used this method before, but I'm kind of confused... First, he used a positive constant so that I'd imagine that he's trying to ""restrict"" by restricting the interval... Then he chose , which I guess comes from the fact that it's the upper bound of , so that And that so that if , it still works. I think I understand each individual part... But, for some reason, I feel something is missing. Why is that? Even after doing most of the exercises, this still feels alien to me. Maybe I should try reading another book as well? I've bought Spivak book (and it might take a while for it to arrive), does it teaches the differently? What should I do so that I get comfortable with it? Thanks in advance!","\lim_{x\to 3} x^2 = 9 \delta>0 \epsilon>0 0<|x-3|<\delta \implies |x^2-9|<\epsilon |x^2-9|<\epsilon |x-3||x+3|<\epsilon C |x+3|<C |x+3| |x-3|<1 \implies 2<x<4 \implies 5<x+3<7 C = 7 x+3 |x-3||x+3| < 7|x-3| < \epsilon \implies |x-3| < \frac{\epsilon}{7} \delta = \min(1,\frac{\epsilon}{7}) \epsilon > 7 \epsilon - \delta","['calculus', 'real-analysis']"
10,Showing $\frac{x}{1+x}<\log(1+x)<x$ for all $x>0$ using the mean value theorem [duplicate],Showing  for all  using the mean value theorem [duplicate],\frac{x}{1+x}<\log(1+x)<x x>0,"This question already has answers here : Intuition behind logarithm inequality: $1 - \frac1x \leq \log x \leq x-1$ (4 answers) Closed 6 years ago . I want to show that $$\frac{x}{1+x}<\log(1+x)<x$$ for all $x>0$ using the mean value theorem. I tried to prove the two inequalities separately. $$\frac{x}{1+x}<\log(1+x) \Leftrightarrow \frac{x}{1+x} -\log(1+x) <0$$ Let $$f(x) = \frac{x}{1+x} -\log(1+x).$$ Since $$f(0)=0$$ and $$f'(x)= \frac{1}{(1+x)^2}-\frac{1}{1+x}<0$$ for all $x > 0$,  $f(x)<0$ for all $x>0$.  Is this correct so far? I go on with the second part: Let $f(x) = \log(x+1)$. Choose $a=0$ and $x>0$ so that there is, according to the mean value theorem, an $x_0$ between $a$ and $x$ with $f'(x_0)=\frac{f(x)-f(a)}{x-a} \Leftrightarrow \frac{1}{x_0+1}=\frac{ \log(x+1)}{x}$. Since $$x_0>0 \Rightarrow  \frac{1}{x_0+1}<1.$$ $$\Rightarrow 1 > \frac{1}{x_0+1}= \frac{ \log(x+1)}{x} \Rightarrow x> \log(x+1)$$","This question already has answers here : Intuition behind logarithm inequality: $1 - \frac1x \leq \log x \leq x-1$ (4 answers) Closed 6 years ago . I want to show that $$\frac{x}{1+x}<\log(1+x)<x$$ for all $x>0$ using the mean value theorem. I tried to prove the two inequalities separately. $$\frac{x}{1+x}<\log(1+x) \Leftrightarrow \frac{x}{1+x} -\log(1+x) <0$$ Let $$f(x) = \frac{x}{1+x} -\log(1+x).$$ Since $$f(0)=0$$ and $$f'(x)= \frac{1}{(1+x)^2}-\frac{1}{1+x}<0$$ for all $x > 0$,  $f(x)<0$ for all $x>0$.  Is this correct so far? I go on with the second part: Let $f(x) = \log(x+1)$. Choose $a=0$ and $x>0$ so that there is, according to the mean value theorem, an $x_0$ between $a$ and $x$ with $f'(x_0)=\frac{f(x)-f(a)}{x-a} \Leftrightarrow \frac{1}{x_0+1}=\frac{ \log(x+1)}{x}$. Since $$x_0>0 \Rightarrow  \frac{1}{x_0+1}<1.$$ $$\Rightarrow 1 > \frac{1}{x_0+1}= \frac{ \log(x+1)}{x} \Rightarrow x> \log(x+1)$$",,"['calculus', 'real-analysis', 'inequality', 'logarithms']"
11,"A proof that $C[0,1]$ is separable",A proof that  is separable,"C[0,1]","Im reading Chapter11 of Carothers' Real Analysis, 1ed. Here is a proof of Separable C[0,1], I can understand ""Why"" from his figure11.1. But I do not how to prove it? I mean since g is a polygonal function and |f(x) - f(y)|<ε whenever |x-y|<$\frac{1}{n}$, how to prove that $||f-g||_∞$ =< ε on each interval ($\frac{k}{n}$,$\frac{k+1}{n}$) will be guaranteed?","Im reading Chapter11 of Carothers' Real Analysis, 1ed. Here is a proof of Separable C[0,1], I can understand ""Why"" from his figure11.1. But I do not how to prove it? I mean since g is a polygonal function and |f(x) - f(y)|<ε whenever |x-y|<$\frac{1}{n}$, how to prove that $||f-g||_∞$ =< ε on each interval ($\frac{k}{n}$,$\frac{k+1}{n}$) will be guaranteed?",,"['real-analysis', 'analysis']"
12,Open map as a corollary of the inverse function theorem,Open map as a corollary of the inverse function theorem,,"Let $U \in \mathbb{R}^n$ be a open set and $f:U \rightarrow \mathbb{R}^n$ be a continuously differentiable function such that $Df(x_0)$ is an isomorphism for every $x_0 \in U$. I'm trying to use the inverse function theorem to show that $f(U)$ is a open set. What I've got so far: we have that $det(Df(x_0)) \neq 0$ so, by the inverse function theorem, I know that there are open sets $V_{x_0} \subset U$ and $W_{x_0} \subset \mathbb{R}^n$ such that $x_0 \in V_{x_0}$, $f(x_0) \in W_{x_0}$ and the restriction $f|V_{x_0} \rightarrow W_{x_0}$ is invertible and the inverse continuously differentiable. So, $W_{x_0}$ is an open set as it is the inverse image of an open set ($U_{x_0}$) by continuous function ($f^{-1}$). But how can I prove that the all set $f(U)$ must be open? Thank you!","Let $U \in \mathbb{R}^n$ be a open set and $f:U \rightarrow \mathbb{R}^n$ be a continuously differentiable function such that $Df(x_0)$ is an isomorphism for every $x_0 \in U$. I'm trying to use the inverse function theorem to show that $f(U)$ is a open set. What I've got so far: we have that $det(Df(x_0)) \neq 0$ so, by the inverse function theorem, I know that there are open sets $V_{x_0} \subset U$ and $W_{x_0} \subset \mathbb{R}^n$ such that $x_0 \in V_{x_0}$, $f(x_0) \in W_{x_0}$ and the restriction $f|V_{x_0} \rightarrow W_{x_0}$ is invertible and the inverse continuously differentiable. So, $W_{x_0}$ is an open set as it is the inverse image of an open set ($U_{x_0}$) by continuous function ($f^{-1}$). But how can I prove that the all set $f(U)$ must be open? Thank you!",,['real-analysis']
13,Norm of Fredholm integral operator equals norm of its kernel?,Norm of Fredholm integral operator equals norm of its kernel?,,"Let $T_k(f)(s):=\int_0^1 k(s,t) f(t) dt $, where $k \in L^2([0,1]^2)$ and $f \in L^2([0,1])$. Then it was fairly easy to see that $||T_k|| \le ||k||_{L^2}$, but now I was wondering how to show that we have $||T_k|| = ||k||_{L^2}$, or is there a lower bound?","Let $T_k(f)(s):=\int_0^1 k(s,t) f(t) dt $, where $k \in L^2([0,1]^2)$ and $f \in L^2([0,1])$. Then it was fairly easy to see that $||T_k|| \le ||k||_{L^2}$, but now I was wondering how to show that we have $||T_k|| = ||k||_{L^2}$, or is there a lower bound?",,"['real-analysis', 'analysis']"
14,Set of discontinuous points,Set of discontinuous points,,Suppose $f$ is function from $\mathbb{R}$ to $\mathbb{R}$. Let be the set $\mathbf{A}$ that contains all the discontinuous points of $f$.Is  $\mathbf{A}$ Borel Measureable?,Suppose $f$ is function from $\mathbb{R}$ to $\mathbb{R}$. Let be the set $\mathbf{A}$ that contains all the discontinuous points of $f$.Is  $\mathbf{A}$ Borel Measureable?,,"['real-analysis', 'analysis', 'measure-theory']"
15,Showing a recursive sequence is Cauchy,Showing a recursive sequence is Cauchy,,"let $a_1=1$, $a_2=2$, and define $a_n=\frac{1}{2}(a_{n-1}+a_{n-2})$. How can I show that this sequence is Cauchy? I began with $|a_n-a_{n-1}|=\frac{1}{2}(a_n-a_{n-1})$ which goes to $\frac{1}{2}(a_{n-1}+a_{n-2})-\frac{1}{2}(a_{n-2}+a_{n-3})$ I am have been toying with this for quite some time and it is just not clicking. Can somebody please help!","let $a_1=1$, $a_2=2$, and define $a_n=\frac{1}{2}(a_{n-1}+a_{n-2})$. How can I show that this sequence is Cauchy? I began with $|a_n-a_{n-1}|=\frac{1}{2}(a_n-a_{n-1})$ which goes to $\frac{1}{2}(a_{n-1}+a_{n-2})-\frac{1}{2}(a_{n-2}+a_{n-3})$ I am have been toying with this for quite some time and it is just not clicking. Can somebody please help!",,['real-analysis']
16,"If $x_1<x_2$ are arbitrary real numbers, and $x_n=\frac{1}{2}(x_{n-2}+x_{n-1})$ for $n>2$, show that $(x_n)$ is convergent.","If  are arbitrary real numbers, and  for , show that  is convergent.",x_1<x_2 x_n=\frac{1}{2}(x_{n-2}+x_{n-1}) n>2 (x_n),"If $x_1<x_2$ are arbitrary real numbers, and $x_n=\frac{1}{2}(x_{n-2}+x_{n-1})$ for $n>2$, show that $(x_n)$ is convergent. What is the limit? The back of my textbook says that $\lim(x_n)=\frac{1}{3}x_1+\frac{2}{3}x_2$. I was thinking that if I show that the sequence is monotone increasing by induction, then I can ""guess"" (from the back of my textbook) that there is an upperbound of the limit and show by induction that all elements in $x_n$ are between $x_1$ and the upperbound and so it's bounded. Then by the monotone convergence theorem say it's convergent. I'm not sure how to show that it is monotone, and then after showing convergence finding the limit, without magically guessing it.","If $x_1<x_2$ are arbitrary real numbers, and $x_n=\frac{1}{2}(x_{n-2}+x_{n-1})$ for $n>2$, show that $(x_n)$ is convergent. What is the limit? The back of my textbook says that $\lim(x_n)=\frac{1}{3}x_1+\frac{2}{3}x_2$. I was thinking that if I show that the sequence is monotone increasing by induction, then I can ""guess"" (from the back of my textbook) that there is an upperbound of the limit and show by induction that all elements in $x_n$ are between $x_1$ and the upperbound and so it's bounded. Then by the monotone convergence theorem say it's convergent. I'm not sure how to show that it is monotone, and then after showing convergence finding the limit, without magically guessing it.",,"['real-analysis', 'sequences-and-series', 'recurrence-relations']"
17,Open set as a countable union of open bounded intervals,Open set as a countable union of open bounded intervals,,"Can every nonempty open set be written as a countable union of bounded open intervals of the form $(a_k,b_k)$, where $a_k$ and $b_k$ are real numbers (not $\pm\infty$)? If yes, can someone point me toward a proof? If not, counterexample? Note that this is not the same question as the property ""every nonempty open set is the disjoint union of a countable collection of open intervals.""","Can every nonempty open set be written as a countable union of bounded open intervals of the form $(a_k,b_k)$, where $a_k$ and $b_k$ are real numbers (not $\pm\infty$)? If yes, can someone point me toward a proof? If not, counterexample? Note that this is not the same question as the property ""every nonempty open set is the disjoint union of a countable collection of open intervals.""",,['real-analysis']
18,Is this a Cantor Set?,Is this a Cantor Set?,,"Suppose $\alpha:[0,1]\rightarrow\mathbb{R}^2$ is a continuous curve. Let $K\subset \alpha([0,1])$ be a cantor set, i.e. there exist a homeomorphism between K and the standard Cantor set in $\mathbb{R}$. Consider the set $$D=\{|x-y|:\ x,y\in K\}\subset\mathbb{R}$$ Is $D$ a Cantor set?","Suppose $\alpha:[0,1]\rightarrow\mathbb{R}^2$ is a continuous curve. Let $K\subset \alpha([0,1])$ be a cantor set, i.e. there exist a homeomorphism between K and the standard Cantor set in $\mathbb{R}$. Consider the set $$D=\{|x-y|:\ x,y\in K\}\subset\mathbb{R}$$ Is $D$ a Cantor set?",,"['real-analysis', 'general-topology']"
19,Do zeros of uniformly convergent function sequences also converge?,Do zeros of uniformly convergent function sequences also converge?,,"Assume the following: $f_n{(x)}$ is a sequence of continuous functions, each with a unique zero $x_n^*$ $f_n\to f$ uniformly $f$ has a unique zero at $x$ Does it then follow that $x_n^*\to x$? If this claim is false, what are the minimum additional assumptions needed in order to make it true (for example, do we need to assume that all of the $f_n$'s are analytic)?","Assume the following: $f_n{(x)}$ is a sequence of continuous functions, each with a unique zero $x_n^*$ $f_n\to f$ uniformly $f$ has a unique zero at $x$ Does it then follow that $x_n^*\to x$? If this claim is false, what are the minimum additional assumptions needed in order to make it true (for example, do we need to assume that all of the $f_n$'s are analytic)?",,"['real-analysis', 'sequences-and-series', 'functions']"
20,inequality proof,inequality proof,,"how could we show that the following holds for $k \ge 2$, $k \in \mathbb{N}$: $$ 1 < \left( k - \frac{1}{2}\right) \log{\left(\frac{k}{k-1}\right)} $$ or equivalently, how can we show that $$ e < \left( \frac{k}{k-1} \right)^{k-1/2} $$ Thanks!","how could we show that the following holds for $k \ge 2$, $k \in \mathbb{N}$: $$ 1 < \left( k - \frac{1}{2}\right) \log{\left(\frac{k}{k-1}\right)} $$ or equivalently, how can we show that $$ e < \left( \frac{k}{k-1} \right)^{k-1/2} $$ Thanks!",,"['calculus', 'real-analysis', 'inequality']"
21,Is the Laplacian surjective on $C_0^{\infty}$?,Is the Laplacian surjective on ?,C_0^{\infty},"Let $M := C_0^{\infty}(\mathbb{R}^n)$ denote the smooth maps with compact support. Then we have a map $\Delta:M\rightarrow M,\,\, f\mapsto \Delta f$, where $\Delta f = \sum_{i=1}^{n} \frac{\partial^2}{\partial x_i^2}f$ is the Laplacian. I am wondering if $\Delta$ is surjective, i.e. if for any $f\in M$ there exists an $F\in M$ with $\Delta F = f$. Is that true? Thanks for your help!","Let $M := C_0^{\infty}(\mathbb{R}^n)$ denote the smooth maps with compact support. Then we have a map $\Delta:M\rightarrow M,\,\, f\mapsto \Delta f$, where $\Delta f = \sum_{i=1}^{n} \frac{\partial^2}{\partial x_i^2}f$ is the Laplacian. I am wondering if $\Delta$ is surjective, i.e. if for any $f\in M$ there exists an $F\in M$ with $\Delta F = f$. Is that true? Thanks for your help!",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
22,Lebesgue's criterion for Riemann-integrability of Banach-space-valued functions?,Lebesgue's criterion for Riemann-integrability of Banach-space-valued functions?,,"Lebesgue's criterion for Riemann-integrability says that a function $f:[a,b]\to\mathbb{R}$ is Riemann-integrable iff it is bounded and the set of points at which it is not continuous has measure zero. This can be easily extended to functions with values in $\mathbb{R}^n$. However, is there an equivalent criterion for functions taking values in a Banach space or, making some more assumptions, in a (separable) Hilbert space? To avoid confusion: The Riemann integral for Banach space valued functions is (obviously) not defined via upper and lower sums, but as described in http://en.wikipedia.org/wiki/Riemann_integral#Definition for real-valued functions","Lebesgue's criterion for Riemann-integrability says that a function $f:[a,b]\to\mathbb{R}$ is Riemann-integrable iff it is bounded and the set of points at which it is not continuous has measure zero. This can be easily extended to functions with values in $\mathbb{R}^n$. However, is there an equivalent criterion for functions taking values in a Banach space or, making some more assumptions, in a (separable) Hilbert space? To avoid confusion: The Riemann integral for Banach space valued functions is (obviously) not defined via upper and lower sums, but as described in http://en.wikipedia.org/wiki/Riemann_integral#Definition for real-valued functions",,"['real-analysis', 'integration']"
23,Find the limit of $x_n^3/n^2$ if $x_{n+1}=x_{n}+1/\sqrt{x_n}$,Find the limit of  if,x_n^3/n^2 x_{n+1}=x_{n}+1/\sqrt{x_n},"Consider a sequence $(x_n)$, $n \geq 0$, with $x_0>0$, and for any $n$ natural number, $$x_{n+1}=x_{n}+\frac{1}{\sqrt{x_{n}}}.$$ I am required to calculate   $$ \lim_{n\rightarrow\infty} \frac{x_n^3}{n^2}$$","Consider a sequence $(x_n)$, $n \geq 0$, with $x_0>0$, and for any $n$ natural number, $$x_{n+1}=x_{n}+\frac{1}{\sqrt{x_{n}}}.$$ I am required to calculate   $$ \lim_{n\rightarrow\infty} \frac{x_n^3}{n^2}$$",,"['real-analysis', 'sequences-and-series', 'limits']"
24,Positivity of a determinant,Positivity of a determinant,,"I'm stuck to prove the following exercise : Given real numbers $x_1,\ldots,x_n$ and $y_1,\ldots,y_n$, show that  $$ \det(e^{\large{x_iy_j}})_{i,j=1}^n>0 $$ provided that $x_1<\cdots<x_n$ and $y_1<\cdots<y_n$. Any idea ?","I'm stuck to prove the following exercise : Given real numbers $x_1,\ldots,x_n$ and $y_1,\ldots,y_n$, show that  $$ \det(e^{\large{x_iy_j}})_{i,j=1}^n>0 $$ provided that $x_1<\cdots<x_n$ and $y_1<\cdots<y_n$. Any idea ?",,"['linear-algebra', 'real-analysis', 'determinant']"
25,"Show that if $\{x_{n}\}$ and $\{y_{n}\}$ are Cauchy sequences in X then $d(x_{n},y_{n})$ converges in $\mathbb{R}$.",Show that if  and  are Cauchy sequences in X then  converges in .,"\{x_{n}\} \{y_{n}\} d(x_{n},y_{n}) \mathbb{R}","Could someone help me through this problem? Let $X$ be a metric space. Show that if $\{x_{n}\}$ and $\{y_{n}\}$ are Cauchy sequences in $X$ then $d(x_{n},y_{n})$ converges in $\mathbb{R}$. Does this follow from the fact that every Cauchy sequence in $\mathbb{R}$ is convergent?","Could someone help me through this problem? Let $X$ be a metric space. Show that if $\{x_{n}\}$ and $\{y_{n}\}$ are Cauchy sequences in $X$ then $d(x_{n},y_{n})$ converges in $\mathbb{R}$. Does this follow from the fact that every Cauchy sequence in $\mathbb{R}$ is convergent?",,"['real-analysis', 'sequences-and-series']"
26,Subset of the preimage of a semicontinuous real function is Borel,Subset of the preimage of a semicontinuous real function is Borel,,"I'm in a jam with this problem: Let $ f: \mathbb{R} \to [-\infty,\infty] $ be lower semicontinuous, and let $ A = \{ x:f(x)\ge a \}  $. Is $A$ necessarily a Borel set in $ \mathbb{R} $? I've actually managed to prove that if $A$ has no excluded points, then it is Borel, and it's easy to use that to show that it's also Borel if there are only countably many such points, but I wasn't able to do so, so I guess a hint in this direction would be appreciated. I do feel, however, that there's a much less roundabout way to resolve this problem, so if someone could gently hint me to the right direction I would be most grateful.","I'm in a jam with this problem: Let $ f: \mathbb{R} \to [-\infty,\infty] $ be lower semicontinuous, and let $ A = \{ x:f(x)\ge a \}  $. Is $A$ necessarily a Borel set in $ \mathbb{R} $? I've actually managed to prove that if $A$ has no excluded points, then it is Borel, and it's easy to use that to show that it's also Borel if there are only countably many such points, but I wasn't able to do so, so I guess a hint in this direction would be appreciated. I do feel, however, that there's a much less roundabout way to resolve this problem, so if someone could gently hint me to the right direction I would be most grateful.",,"['real-analysis', 'measure-theory', 'borel-sets', 'semicontinuous-functions']"
27,Lusin's theorem,Lusin's theorem,,"Lusin's theorem states that for every $\varepsilon$, for every borel measure $\mu$, for every function $f:\mathbb{R}^n\to\mathbb{R}^m$, for every open set $A$ of finite measure, there exists a compact set $K$ such that $\mu(A-K)<\varepsilon$ and $f$ restricted to $K$ is continue. Now, I'm wondering about what's the form of the compact $K$ in the case of the Dirichlet's function. Can you help me?","Lusin's theorem states that for every $\varepsilon$, for every borel measure $\mu$, for every function $f:\mathbb{R}^n\to\mathbb{R}^m$, for every open set $A$ of finite measure, there exists a compact set $K$ such that $\mu(A-K)<\varepsilon$ and $f$ restricted to $K$ is continue. Now, I'm wondering about what's the form of the compact $K$ in the case of the Dirichlet's function. Can you help me?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
28,Determine if the convergence of $\sum|a_{n+1} - a_n|$ implies the convergence of $\sum |a_n|$.,Determine if the convergence of  implies the convergence of .,\sum|a_{n+1} - a_n| \sum |a_n|,"Let $(a_n)$ a sequence of reals with the property that $\sum a_n$ is finite. Determine if the convergence of $\sum|a_{n+1} - a_n|$ implies the convergence of $\sum |a_n|$ . After contemplating it, I assume it is indeed true. My attempts to prove it failed where the hardest part for me is using the convergence of $\sum a_n$ . So far my main observation I suppose is that for each $m > n > 1$ we have $ |a_n| \leq \sum_{k=n}^{m} |a_k - a_{k-1}| $ Any idea will be appreciated","Let a sequence of reals with the property that is finite. Determine if the convergence of implies the convergence of . After contemplating it, I assume it is indeed true. My attempts to prove it failed where the hardest part for me is using the convergence of . So far my main observation I suppose is that for each we have Any idea will be appreciated",(a_n) \sum a_n \sum|a_{n+1} - a_n| \sum |a_n| \sum a_n m > n > 1  |a_n| \leq \sum_{k=n}^{m} |a_k - a_{k-1}| ,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
29,Are convex polytopes closed in arbitrary metric spaces?,Are convex polytopes closed in arbitrary metric spaces?,,"Let $(X,d)$ be a metric space. For all points $x,y \in X$ we define the metric segment between them as the following set: $$\left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \}$$ We then say that a set $S\subseteq X$ is convex if for all $x,y \in S$ it holds true that $\left [ x,y \right ] \subseteq S$ . It can be easily shown that arbitrary intersection of convex sets in metric spaces is a convex set. Therefore, for each subset $S \subseteq X$ of a metric space $(X,d)$ we define its convex hull as the set $\mathrm{conv}(S)=\bigcap_{}^{} \left \{ U  \supseteq S : U \; \mathrm{convex}  \right \}$ . We say that a set is a convex polytope if it is a convex hull of a finite set. My question is are convex polytopes in metric spaces closed sets?","Let be a metric space. For all points we define the metric segment between them as the following set: We then say that a set is convex if for all it holds true that . It can be easily shown that arbitrary intersection of convex sets in metric spaces is a convex set. Therefore, for each subset of a metric space we define its convex hull as the set . We say that a set is a convex polytope if it is a convex hull of a finite set. My question is are convex polytopes in metric spaces closed sets?","(X,d) x,y \in X \left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \} S\subseteq X x,y \in S \left [ x,y \right ] \subseteq S S \subseteq X (X,d) \mathrm{conv}(S)=\bigcap_{}^{} \left \{ U  \supseteq S : U \; \mathrm{convex}  \right \}","['real-analysis', 'general-topology', 'metric-spaces', 'convexity-spaces']"
30,Having trouble understanding the generalized chain rule for multivariable functions,Having trouble understanding the generalized chain rule for multivariable functions,,"Determine the derivative of $F\circ G$ where $F:\mathbb{R^3}\rightarrow \mathbb{R^2}$ and $G:\mathbb{R^2}\rightarrow \mathbb{R^3}$ are given by $$F(x,y,z)=\begin {pmatrix} \cos(x)-z \\xe^{y}\end {pmatrix}, G(u,v)=\begin {pmatrix} v^2\\uv+v^3 \\ u^2+v\end {pmatrix} $$ I'm having trouble applying the generalized chain rule for this question. $F\circ G=F(G(u,v)) = \begin {pmatrix} \cos(v^2)-(u^2+v)\\v^2e^{uv+v^3}\end {pmatrix}.$ Unless I'm mistaken we can treat the composition of the functions as a different function such that $F\circ G =h:\mathbb{R^2}\rightarrow \mathbb{R^2}$ and then differentiating this function would simply be taking the partial derivatives of the two expressions: $$D=\begin {pmatrix}\frac{\partial}{\partial u} \big(\cos(v^2)-u^2-v \big)& \frac{\partial}{\partial v} (\cos(v^2)-u^2-v) \\ \frac{\partial}{\partial u} (v^2\exp({uv+v^3})) & \frac{\partial}{\partial v}(v^2\exp(uv+v^3))\end {pmatrix}$$ But I honestly don't know if any of this is correct and would appreciate if someone could help.",Determine the derivative of where and are given by I'm having trouble applying the generalized chain rule for this question. Unless I'm mistaken we can treat the composition of the functions as a different function such that and then differentiating this function would simply be taking the partial derivatives of the two expressions: But I honestly don't know if any of this is correct and would appreciate if someone could help.,"F\circ G F:\mathbb{R^3}\rightarrow \mathbb{R^2} G:\mathbb{R^2}\rightarrow \mathbb{R^3} F(x,y,z)=\begin {pmatrix} \cos(x)-z \\xe^{y}\end {pmatrix}, G(u,v)=\begin {pmatrix} v^2\\uv+v^3 \\ u^2+v\end {pmatrix}  F\circ G=F(G(u,v)) = \begin {pmatrix} \cos(v^2)-(u^2+v)\\v^2e^{uv+v^3}\end {pmatrix}. F\circ G =h:\mathbb{R^2}\rightarrow \mathbb{R^2} D=\begin {pmatrix}\frac{\partial}{\partial u} \big(\cos(v^2)-u^2-v \big)& \frac{\partial}{\partial v} (\cos(v^2)-u^2-v) \\ \frac{\partial}{\partial u} (v^2\exp({uv+v^3})) & \frac{\partial}{\partial v}(v^2\exp(uv+v^3))\end {pmatrix}","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
31,"Books for upper-undergraduate, higher level real analysis","Books for upper-undergraduate, higher level real analysis",,"My university does not offer a real analysis course for math major students in the later stages of their undergrad; these are reserved for honours students. I had an excellent professor in my final introductory analysis course and I wish to study some higher level real analysis on my own. I was looking for a book that might help me in this endeavour. In my analysis courses, we covered sequences and series, limits including $\limsup$ and $\liminf$ , continuity, differentiability, and Riemann integration (Darboux's approach only). Our main textbook was Abbott's Understanding Analysis , though I found Bartle's Introduction to Real Analysis much more helpful. I remember reading a bit of Rudin's Principles of Mathematical Analysis , but it went a bit too fast for me. I looked at the syllabi for the honours courses and they involve topics like point-set topology, some introductory measure theory, and Lebesgue integration, with a brief foray into functional analysis and Fourier analysis only at the end of the year. Some potential books that I looked at were Royden's Real Analysis , Carothers' Real Analysis , and Axler's Measure, Integration, & Real Analysis . Royden seems to be used by my university, but upon a quick glance I may need a bit more mathematical maturity before I attempt to self-study from it. Carothers and Axler both seem to match my pace, but I am interested as to your thoughts on either the books I mentioned or some other books outside of these three that you believe might suit me best.","My university does not offer a real analysis course for math major students in the later stages of their undergrad; these are reserved for honours students. I had an excellent professor in my final introductory analysis course and I wish to study some higher level real analysis on my own. I was looking for a book that might help me in this endeavour. In my analysis courses, we covered sequences and series, limits including and , continuity, differentiability, and Riemann integration (Darboux's approach only). Our main textbook was Abbott's Understanding Analysis , though I found Bartle's Introduction to Real Analysis much more helpful. I remember reading a bit of Rudin's Principles of Mathematical Analysis , but it went a bit too fast for me. I looked at the syllabi for the honours courses and they involve topics like point-set topology, some introductory measure theory, and Lebesgue integration, with a brief foray into functional analysis and Fourier analysis only at the end of the year. Some potential books that I looked at were Royden's Real Analysis , Carothers' Real Analysis , and Axler's Measure, Integration, & Real Analysis . Royden seems to be used by my university, but upon a quick glance I may need a bit more mathematical maturity before I attempt to self-study from it. Carothers and Axler both seem to match my pace, but I am interested as to your thoughts on either the books I mentioned or some other books outside of these three that you believe might suit me best.",\limsup \liminf,"['real-analysis', 'book-recommendation']"
32,"Proving that the outer measure of a closed interval $[a,b]$ is $b-a$",Proving that the outer measure of a closed interval  is,"[a,b] b-a","In Sheldon Axler's book, Measure Integration, and Real Analysis , he defines outer measure of a set as $|A| = \inf\big\{\sum_{k=1}^\infty \ell(I_k): I_1, I_2, \dots \text{are open intervals such that} A\subset \bigcup_{k=1}^\infty I_k\big\}$ , where $\ell(I)$ for an open interval $(a,b)$ is just $b-a$ . He later proves that outer measure preserves order, i.e. $A\subset B \Rightarrow |A| \le |B|$ . Later, we are trying to prove that the outer measure of the closed interval $[a,b]$ is $b-a$ . We bound it from above by saying for $\varepsilon > 0$ , $(a-\varepsilon, b+\varepsilon), \varnothing, \varnothing,\dots$ is a sequence of open intervals whose union contains $[a,b]$ , so $|[a,b]|\le b-a+2\varepsilon$ which with the definition of outer measure implies $|[a,b]| \le b-a$ . The next section is confusing to me: Is the inequality in the other direction obviously true to you? If so, think again, because a proof of the inequality in the other direction requires that the completeness of $\mathbf{R}$ is used in some form...Thus something deeper than you might suspect is going on with the ingredients needed to prove that $|[a, b]| ≥ b − a$ . He then goes onto prove it using the Heine-Borel theorem. However, because outer measure preserves order and $(a,b)$ is a subset of $[a,b]$ , couldn't we easily bound it from below with that? Is the open interval not thought of as a subset? I don't quite understand the reasoning and feel I'm missing something obvious. Any help would be appreciated.","In Sheldon Axler's book, Measure Integration, and Real Analysis , he defines outer measure of a set as , where for an open interval is just . He later proves that outer measure preserves order, i.e. . Later, we are trying to prove that the outer measure of the closed interval is . We bound it from above by saying for , is a sequence of open intervals whose union contains , so which with the definition of outer measure implies . The next section is confusing to me: Is the inequality in the other direction obviously true to you? If so, think again, because a proof of the inequality in the other direction requires that the completeness of is used in some form...Thus something deeper than you might suspect is going on with the ingredients needed to prove that . He then goes onto prove it using the Heine-Borel theorem. However, because outer measure preserves order and is a subset of , couldn't we easily bound it from below with that? Is the open interval not thought of as a subset? I don't quite understand the reasoning and feel I'm missing something obvious. Any help would be appreciated.","|A| = \inf\big\{\sum_{k=1}^\infty \ell(I_k): I_1, I_2, \dots \text{are open intervals such that} A\subset \bigcup_{k=1}^\infty I_k\big\} \ell(I) (a,b) b-a A\subset B \Rightarrow |A| \le |B| [a,b] b-a \varepsilon > 0 (a-\varepsilon, b+\varepsilon), \varnothing, \varnothing,\dots [a,b] |[a,b]|\le b-a+2\varepsilon |[a,b]| \le b-a \mathbf{R} |[a, b]| ≥ b − a (a,b) [a,b]","['real-analysis', 'outer-measure']"
33,"If $f$ is Lebesgue integrable and $f''$ exists and is Lebesgue integrable, what can we say about the integrability of $f'$?","If  is Lebesgue integrable and  exists and is Lebesgue integrable, what can we say about the integrability of ?",f f'' f',"Let $\newcommand{\RR}{\mathbb{R}}f: \RR \to \RR$ be a twice differentiable function that is in $\mathcal{L}^1$ . Suppose that also $f'' \in \mathcal{L}^1$ . Can we conclude that $f' \in \mathcal{L}^1$ ? What about the case where we know $f \in \mathcal{L}^1$ and $f^{(m)} \in \mathcal{L}^1$ , can we say something about the integrability of $f^{(k)}$ for $k \leq m$ ? In general we can't say that the derivative of a smooth integrable function is integrable, but maybe the integrability of $f''$ helps?","Let be a twice differentiable function that is in . Suppose that also . Can we conclude that ? What about the case where we know and , can we say something about the integrability of for ? In general we can't say that the derivative of a smooth integrable function is integrable, but maybe the integrability of helps?",\newcommand{\RR}{\mathbb{R}}f: \RR \to \RR \mathcal{L}^1 f'' \in \mathcal{L}^1 f' \in \mathcal{L}^1 f \in \mathcal{L}^1 f^{(m)} \in \mathcal{L}^1 f^{(k)} k \leq m f'',"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
34,Calculating $\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx$ without using Beta function and Euler sum.,Calculating  without using Beta function and Euler sum.,\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx,"Is it possible to show that $$\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=-\frac12\zeta(4)$$ without using the Beta function $$\text{B}(a,b)=\int_0^1 x^{a-1}(1-x)^{b-1}dx=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$ and the generalized Euler sum $$\sum_{n=1}^\infty \frac{H_n}{n^q}= \left(1+\frac{q}{2} \right)\zeta(q+1)-\frac{1}{2}\sum_{k=1}^{q-2}\zeta(k+1)\zeta(q-k),\quad q\ge 2\ ?$$ By integration by parts I found $$\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=\color{blue}{\int_0^1\frac{\ln^2(1-x)\ln x}{x}dx}$$ Setting $1-x\to x$ gives the blue integral again. This integral seems tough under such restrictions. All approaches are welcome. thanks.",Is it possible to show that without using the Beta function and the generalized Euler sum By integration by parts I found Setting gives the blue integral again. This integral seems tough under such restrictions. All approaches are welcome. thanks.,"\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=-\frac12\zeta(4) \text{B}(a,b)=\int_0^1 x^{a-1}(1-x)^{b-1}dx=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} \sum_{n=1}^\infty \frac{H_n}{n^q}= \left(1+\frac{q}{2} \right)\zeta(q+1)-\frac{1}{2}\sum_{k=1}^{q-2}\zeta(k+1)\zeta(q-k),\quad q\ge 2\ ? \int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=\color{blue}{\int_0^1\frac{\ln^2(1-x)\ln x}{x}dx} 1-x\to x","['real-analysis', 'integration', 'complex-analysis', 'logarithms', 'alternative-proof']"
35,Evaluate $\int _0^1\frac{\ln x\ln (1+x^2)}{1+x^2}\:dx$ without trigonometric and complex functions,Evaluate  without trigonometric and complex functions,\int _0^1\frac{\ln x\ln (1+x^2)}{1+x^2}\:dx,"Earlier, I posted the sum evaluation that has the following integral representation. $$\sum _{k=1}^{\infty }\frac{\left(-1\right)^kH_k}{\left(2k+1\right)^2}=\int _0^1\frac{\ln \left(x\right)\ln \left(1+x^2\right)}{1+x^2}\:\mathrm{d}x$$ Ali Shather managed to prove here $$\int _0^1\frac{\ln \left(x\right)\ln \left(1+x^2\right)}{1+x^2}\:\mathrm{d}x=\frac3{32}\pi^3+\frac{\pi}8\ln^22-\ln2~G-2\text{Im}\operatorname{Li_3}(1+i)$$ which relies on trigonometric substitutions as well as complex methods. Question : Can this integral be evaluated without the trig functions? and is it possible to evaluate it without complex methods? I tried using certain substitutions, but ended up with similar or even harder integrals. I'm not sure how to approach this.","Earlier, I posted the sum evaluation that has the following integral representation. Ali Shather managed to prove here which relies on trigonometric substitutions as well as complex methods. Question : Can this integral be evaluated without the trig functions? and is it possible to evaluate it without complex methods? I tried using certain substitutions, but ended up with similar or even harder integrals. I'm not sure how to approach this.",\sum _{k=1}^{\infty }\frac{\left(-1\right)^kH_k}{\left(2k+1\right)^2}=\int _0^1\frac{\ln \left(x\right)\ln \left(1+x^2\right)}{1+x^2}\:\mathrm{d}x \int _0^1\frac{\ln \left(x\right)\ln \left(1+x^2\right)}{1+x^2}\:\mathrm{d}x=\frac3{32}\pi^3+\frac{\pi}8\ln^22-\ln2~G-2\text{Im}\operatorname{Li_3}(1+i),['real-analysis']
36,Challenging limit: $\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right)$,Challenging limit:,\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right),Here is a challenging limit proposed by a friend: $$\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right)$$ and he claims that the closed form for this limit is really pleasant. I am not good at limits so I am not going to show any work and just leave it to those who find it interesting. Addendum: A similar problem proposed by the same person: $$\lim_{\alpha\to0^{+}}\left(\frac{2}{3\alpha^3}-\frac{4\pi}{3\alpha}\int_1^\infty\frac{x\cosh(\pi\alpha x)}{\sinh^2(\pi\alpha x)\sqrt{x^2-1}}dx\right)$$,Here is a challenging limit proposed by a friend: and he claims that the closed form for this limit is really pleasant. I am not good at limits so I am not going to show any work and just leave it to those who find it interesting. Addendum: A similar problem proposed by the same person:,\lim_{\alpha\to0^{+}}\left(\frac{1}{2\alpha}-\int_1^\infty\frac{dx}{\sinh(\pi\alpha x)\sqrt{x^2-1}}\right) \lim_{\alpha\to0^{+}}\left(\frac{2}{3\alpha^3}-\frac{4\pi}{3\alpha}\int_1^\infty\frac{x\cosh(\pi\alpha x)}{\sinh^2(\pi\alpha x)\sqrt{x^2-1}}dx\right),"['real-analysis', 'integration', 'limits', 'improper-integrals', 'closed-form']"
37,Approximating reals with rationals,Approximating reals with rationals,,"I don't have any particular motivation for this question, it just popped into my head. We play the following game: you give me a real number between 0 and 1, and I have to do my best to approximate it as a rational number with a limit on the size of the denominator. More formally, you choose $x \in [0,1]$ , and I have to come up with a pair of integers $p,q \leq N$ that minimize the error: $$ E = \bigg| x - \frac{p}{q} \bigg| $$ For a given $N$ , how should you choose $x$ to make me incur the largest possible error? I made a plot to see how all the possible fractions fall on the number line for each $N$ between 1 and 100. It has a rather interesting pattern which seems to have a fractal nature. For any $N$ , the hardest choices of $x$ seem to be at the extremes of the interval: between 0 and the smallest possible fraction larger than 0, and between 1 and the largest possible fraction smaller than $1$ .","I don't have any particular motivation for this question, it just popped into my head. We play the following game: you give me a real number between 0 and 1, and I have to do my best to approximate it as a rational number with a limit on the size of the denominator. More formally, you choose , and I have to come up with a pair of integers that minimize the error: For a given , how should you choose to make me incur the largest possible error? I made a plot to see how all the possible fractions fall on the number line for each between 1 and 100. It has a rather interesting pattern which seems to have a fractal nature. For any , the hardest choices of seem to be at the extremes of the interval: between 0 and the smallest possible fraction larger than 0, and between 1 and the largest possible fraction smaller than .","x \in [0,1] p,q \leq N 
E = \bigg| x - \frac{p}{q} \bigg|
 N x N N x 1","['real-analysis', 'approximation', 'fractions', 'irrational-numbers', 'rational-numbers']"
38,The calculation of the series $\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^4}$,The calculation of the series,\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^4},"Happy New Year 2020, Romania In this recent post, Evaluate $\int_0^1 \frac{\arctan x\ln^2 x}{1+x^2}\,dx$ , the proposed integral reduces to the calculation of $\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^3}$ which is known in the literature. Now, what can we say about the more advanced version of it, the one with $n^4$ in the denominator? $$\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^4}$$ Can we do it by series manipulations? It looks like a new series in the literature. A good note : With the previous result in hand which we combine with some of the Cornel 's work, we immediately arrive at two delightful series results, $$i) \ \sum _{n=1}^{\infty }(-1)^{n-1} \frac{ H_{2 n}^{(2)}}{n^3}=\frac{61 }{192}\pi ^2 \zeta (3)+\frac{1973 }{128}\zeta (5)+\frac{\pi ^5}{16}-\frac{1}{128} \pi  \psi ^{(3)}\left(\frac{1}{4}\right);$$ $$ii) \ \sum _{n=1}^{\infty } (-1)^{n-1} \frac{ H_{2 n}^{(3)}}{n^2}=\frac{\pi ^3 G}{8}+\frac{1}{64}\pi ^2 \zeta (3)-\frac{2997 }{256}\zeta (5)-\frac{\pi ^5}{32}+\frac{1}{256} \pi  \psi ^{(3)}\left(\frac{1}{4}\right).$$ How would you go proving these last two results? Another good note : Remaining on the ground with alternating harmonic series of weight $5$ and harmonic numbers of the type $H_{2n}$ , we might be curious to know what the value of the series $\displaystyle \sum _{n=1}^{\infty }(-1)^{n-1} \frac{ H_{2 n}^{(4)}}{n}$ is. $$\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}^{(4)}}{n}$$ $$=4\zeta(5)-\frac{3}{128}\zeta(2)\zeta(3)-\frac{7}{128}\log(2)\zeta(4)+\frac{\pi^5}{192}-\frac{\pi^3}{16}G-\frac{\pi}{1536}\psi^{(3)}\left(\frac{1}{4}\right).$$ This last series is elegantly calculated in A simple strategy of calculating two alternating harmonic series generalizations where one may also find its generalization with respect to the order of the harmonic number.","Happy New Year 2020, Romania In this recent post, Evaluate $\int_0^1 \frac{\arctan x\ln^2 x}{1+x^2}\,dx$ , the proposed integral reduces to the calculation of which is known in the literature. Now, what can we say about the more advanced version of it, the one with in the denominator? Can we do it by series manipulations? It looks like a new series in the literature. A good note : With the previous result in hand which we combine with some of the Cornel 's work, we immediately arrive at two delightful series results, How would you go proving these last two results? Another good note : Remaining on the ground with alternating harmonic series of weight and harmonic numbers of the type , we might be curious to know what the value of the series is. This last series is elegantly calculated in A simple strategy of calculating two alternating harmonic series generalizations where one may also find its generalization with respect to the order of the harmonic number.",\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^3} n^4 \sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n^4} i) \ \sum _{n=1}^{\infty }(-1)^{n-1} \frac{ H_{2 n}^{(2)}}{n^3}=\frac{61 }{192}\pi ^2 \zeta (3)+\frac{1973 }{128}\zeta (5)+\frac{\pi ^5}{16}-\frac{1}{128} \pi  \psi ^{(3)}\left(\frac{1}{4}\right); ii) \ \sum _{n=1}^{\infty } (-1)^{n-1} \frac{ H_{2 n}^{(3)}}{n^2}=\frac{\pi ^3 G}{8}+\frac{1}{64}\pi ^2 \zeta (3)-\frac{2997 }{256}\zeta (5)-\frac{\pi ^5}{32}+\frac{1}{256} \pi  \psi ^{(3)}\left(\frac{1}{4}\right). 5 H_{2n} \displaystyle \sum _{n=1}^{\infty }(-1)^{n-1} \frac{ H_{2 n}^{(4)}}{n} \sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}^{(4)}}{n} =4\zeta(5)-\frac{3}{128}\zeta(2)\zeta(3)-\frac{7}{128}\log(2)\zeta(4)+\frac{\pi^5}{192}-\frac{\pi^3}{16}G-\frac{\pi}{1536}\psi^{(3)}\left(\frac{1}{4}\right).,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'harmonic-numbers']"
39,If $a^2>b^2$ prove that $\int\limits_0^{\pi} \frac{dx}{(a+b\cos x)^3}=\frac{\pi (2a^2+b^2)}{2(a^2-b^2)^{5/2}}$.,If  prove that .,a^2>b^2 \int\limits_0^{\pi} \frac{dx}{(a+b\cos x)^3}=\frac{\pi (2a^2+b^2)}{2(a^2-b^2)^{5/2}},"Problem: If $a^2>b^2$ prove that $\int\limits_0^\pi \dfrac{dx}{(a+b\cos x)^3} = \dfrac{\pi (2a^2+b^2)}{2(a^2-b^2)^{5/2}}$ . My effort: If we choose $$x=\tan\frac{\theta}{2}\Longrightarrow d\theta=\frac{2}{x^2+1} \, dx\;\;,\;\;\cos\theta=\frac{1-x^2}{1+x^2}$$ then the integral becomes critical. What is the simplest way to solve?",Problem: If prove that . My effort: If we choose then the integral becomes critical. What is the simplest way to solve?,"a^2>b^2 \int\limits_0^\pi \dfrac{dx}{(a+b\cos x)^3} = \dfrac{\pi (2a^2+b^2)}{2(a^2-b^2)^{5/2}} x=\tan\frac{\theta}{2}\Longrightarrow d\theta=\frac{2}{x^2+1} \, dx\;\;,\;\;\cos\theta=\frac{1-x^2}{1+x^2}","['real-analysis', 'integration', 'definite-integrals']"
40,Positive derivative implies increasing without Mean Value Theorem,Positive derivative implies increasing without Mean Value Theorem,,"The result below is usually proven by using the Mean Value Theorem (see e.g. ProofWiki ). But can we also prove it more directly (and fairly elegantly) without resort to the MVT? Suppose $f:[a,b]\rightarrow \mathbb{R}$ is differentiable. If $f'(x)\geq0$ for all $x\in[a,b]$ , then $f$ is increasing on $[a,b]$ . If $f'(x)>0$ for all $x\in[a,b]$ , then $f$ is strictly increasing on $[a,b]$ . If $f'(x)\leq0$ for all $x\in[a,b]$ , then $f$ is decreasing on $[a,b]$ . If $f'(x)<0$ for all $x\in[a,b]$ , then $f$ is strictly decreasing on $[a,b]$ .","The result below is usually proven by using the Mean Value Theorem (see e.g. ProofWiki ). But can we also prove it more directly (and fairly elegantly) without resort to the MVT? Suppose is differentiable. If for all , then is increasing on . If for all , then is strictly increasing on . If for all , then is decreasing on . If for all , then is strictly decreasing on .","f:[a,b]\rightarrow \mathbb{R} f'(x)\geq0 x\in[a,b] f [a,b] f'(x)>0 x\in[a,b] f [a,b] f'(x)\leq0 x\in[a,b] f [a,b] f'(x)<0 x\in[a,b] f [a,b]","['real-analysis', 'calculus']"
41,Two challenging sums $\sum_{n=1}^\infty\frac{(-1)^nH_n^{(2)}}{n^3}$ and $\sum_{n=1}^\infty\frac{(-1)^nH_n^2}{n^3}$,Two challenging sums  and,\sum_{n=1}^\infty\frac{(-1)^nH_n^{(2)}}{n^3} \sum_{n=1}^\infty\frac{(-1)^nH_n^2}{n^3},"where $H_n$ is the harmonic number and can be defined as: $H_n=1+\frac12+\frac13+...+\frac1n$ $H_n^{(2)}=1+\frac1{2^2}+\frac1{3^2}+...+\frac1{n^2}$ again, my goal of posting these two challenging sums is to use them as a reference. I will provide my solutions soon. I would like to mention that these two sums can also be found in Cornel's book "" almost impossible integrals, sums, and series"".","where is the harmonic number and can be defined as: again, my goal of posting these two challenging sums is to use them as a reference. I will provide my solutions soon. I would like to mention that these two sums can also be found in Cornel's book "" almost impossible integrals, sums, and series"".",H_n H_n=1+\frac12+\frac13+...+\frac1n H_n^{(2)}=1+\frac1{2^2}+\frac1{3^2}+...+\frac1{n^2},"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'harmonic-numbers']"
42,Dominating function for derivative of moment generating function,Dominating function for derivative of moment generating function,,"Let $X$ be a random variable and the moment generating function $$\psi_X:(-\varepsilon,\varepsilon)\rightarrow \mathbb{R}_+,\quad \psi_X(t):=E[e^{tX}]$$ be defined, such that $\psi_X(t)<\infty$ for all $t\in(-\varepsilon,\varepsilon)$ . According my textbook we have $$\psi^{(n)}_X(0)=\frac{d^n}{dt^n}E[e^{tX}]\bigg|_{t=0}=E[\frac{d^n}{dt^n}e^{tX}\bigg|_{t=0}]=E[X^n],$$ where differentiation and integration can be interchanged by using dominated convergence. However what is the dominating function here? It is clear that for all $t\in(-\varepsilon,\varepsilon)$ , we have $$\frac{d^n}{dt^n}e^{tX}=X^n\ e^{tX}$$ So I need to find a integrable dominating function $h:\Omega\rightarrow\mathbb{R}_+$ , such that $$\forall t\in(-\varepsilon,\varepsilon): |X^n\ e^{tX}|\le h$$ Does someone has a hint on this one? Thanks a lot in advance!","Let be a random variable and the moment generating function be defined, such that for all . According my textbook we have where differentiation and integration can be interchanged by using dominated convergence. However what is the dominating function here? It is clear that for all , we have So I need to find a integrable dominating function , such that Does someone has a hint on this one? Thanks a lot in advance!","X \psi_X:(-\varepsilon,\varepsilon)\rightarrow \mathbb{R}_+,\quad \psi_X(t):=E[e^{tX}] \psi_X(t)<\infty t\in(-\varepsilon,\varepsilon) \psi^{(n)}_X(0)=\frac{d^n}{dt^n}E[e^{tX}]\bigg|_{t=0}=E[\frac{d^n}{dt^n}e^{tX}\bigg|_{t=0}]=E[X^n], t\in(-\varepsilon,\varepsilon) \frac{d^n}{dt^n}e^{tX}=X^n\ e^{tX} h:\Omega\rightarrow\mathbb{R}_+ \forall t\in(-\varepsilon,\varepsilon): |X^n\ e^{tX}|\le h","['real-analysis', 'probability-theory', 'characteristic-functions', 'moment-generating-functions']"
43,Is the limit of $f/f'$ necessarily $0$?,Is the limit of  necessarily ?,f/f' 0,"Let $f:[a,b]\to\mathbb{R}$ be differentiable on $[a,b]$ , with $\lim\limits_{x\to a}f(x)=\lim\limits_{x\to a}f^\prime(x)=0$ , and $f^\prime(x)\ne 0$ in a neighborhood of $a$ . Is it necessarily true that $$\lim_{x\to a}\frac{f(x)}{f^\prime(x)}=0$$ It doesn't seem necessarily clear to me that this limit should always exist, and that there's not some pathological function for which the limit is nonzero.","Let be differentiable on , with , and in a neighborhood of . Is it necessarily true that It doesn't seem necessarily clear to me that this limit should always exist, and that there's not some pathological function for which the limit is nonzero.","f:[a,b]\to\mathbb{R} [a,b] \lim\limits_{x\to a}f(x)=\lim\limits_{x\to a}f^\prime(x)=0 f^\prime(x)\ne 0 a \lim_{x\to a}\frac{f(x)}{f^\prime(x)}=0","['real-analysis', 'calculus']"
44,Algebraic proof that two statements of the fundamental theorem of algebra are equivalent,Algebraic proof that two statements of the fundamental theorem of algebra are equivalent,,"Students studying the fundamental theorem of algebra in high school are probably familiar with the statement that goes something like the following. Every non-zero, single-variable, degree $n$ polynomial with complex coefficients has, counted with multiplicity, exactly $n$ complex roots. However, another statement I often see (and comes first in the Wikipedia article) is the following. The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. These statements are equivalent and Wikipedia says that this can be proven by successive polynomial long division. However, Wikipedia does not give this proof. It is absent from Wolfram MathWorld as well. My question is, what is the proof that these statements are equivalent? I know that the fundamental theorem of algebra cannot be proven algebraically, but can this equivalence be proven with only algebra, i.e., polynomial division?","Students studying the fundamental theorem of algebra in high school are probably familiar with the statement that goes something like the following. Every non-zero, single-variable, degree polynomial with complex coefficients has, counted with multiplicity, exactly complex roots. However, another statement I often see (and comes first in the Wikipedia article) is the following. The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. These statements are equivalent and Wikipedia says that this can be proven by successive polynomial long division. However, Wikipedia does not give this proof. It is absent from Wolfram MathWorld as well. My question is, what is the proof that these statements are equivalent? I know that the fundamental theorem of algebra cannot be proven algebraically, but can this equivalence be proven with only algebra, i.e., polynomial division?",n n,"['real-analysis', 'complex-analysis', 'algebra-precalculus', 'polynomials', 'proof-explanation']"
45,"If $E$ is Banach and $E^*$ is its dual, is every $T:E^*\rightarrow E^*$ an adjoint?","If  is Banach and  is its dual, is every  an adjoint?",E E^* T:E^*\rightarrow E^*,"If $E$ is a (complex) Banach and $E^*$ is it's dual, is every bounded $T:E^*\rightarrow E^*$ an adjoint? I am mostly interested in when $T$ is an automorphism. If not, is $\{T: E^*\rightarrow E^* \ | \ \mbox{T is an adjoint}\}$ dense in $B(E^*)$ ? Thanks in advance!","If is a (complex) Banach and is it's dual, is every bounded an adjoint? I am mostly interested in when is an automorphism. If not, is dense in ? Thanks in advance!",E E^* T:E^*\rightarrow E^* T \{T: E^*\rightarrow E^* \ | \ \mbox{T is an adjoint}\} B(E^*),"['real-analysis', 'functional-analysis']"
46,"$L^1$ norm equivalent to weak topology of $W^{1,1}$?",norm equivalent to weak topology of ?,"L^1 W^{1,1}","Let's consider a weakly compact set $S\subset W^{1,1}(\Omega)$ , where $\Omega$ is a domain in $\Bbb R^m$ with smooth boundary. It turns out that $(S,w)$ is metrizable. Is the topology induced by the metric $d(u,v):=\int_{\Omega} |u(x)-v(x)|\, dx$ on $S$ equivalent to the weak topology $(S,w)$ that $S$ inherits from $W^{1,1}$ ? We know that the map $T:(S,w)\to L^1(\Omega)$ defined by $$ Tu:=u $$ is a bijection from a compact space into a Hausdorff space, hence if we manage to show continuity of $T$ then $T$ is a homeomorphism onto its image. An element $u\in S$ can be identified with $(u,\nabla u)\in L^1(\Omega;\Bbb R\times \Bbb R^m)$ so we can view $T$ as a projection of the first coordinate. It is clearly continuous but I don't know if it is weakly continuous or not. Am I missing something obvious or is the statement simply not true?","Let's consider a weakly compact set , where is a domain in with smooth boundary. It turns out that is metrizable. Is the topology induced by the metric on equivalent to the weak topology that inherits from ? We know that the map defined by is a bijection from a compact space into a Hausdorff space, hence if we manage to show continuity of then is a homeomorphism onto its image. An element can be identified with so we can view as a projection of the first coordinate. It is clearly continuous but I don't know if it is weakly continuous or not. Am I missing something obvious or is the statement simply not true?","S\subset W^{1,1}(\Omega) \Omega \Bbb R^m (S,w) d(u,v):=\int_{\Omega} |u(x)-v(x)|\, dx S (S,w) S W^{1,1} T:(S,w)\to L^1(\Omega) 
Tu:=u
 T T u\in S (u,\nabla u)\in L^1(\Omega;\Bbb R\times \Bbb R^m) T","['real-analysis', 'general-topology', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
47,uniformly continuous function $f$ such that $\sum 1/f(n)$ is convergent?,uniformly continuous function  such that  is convergent?,f \sum 1/f(n),"Does there exist a uniformly continuous function $f:[1,\infty)\to \mathbb R$ such that $\sum_{n=1}^\infty 1/f(n)$ is convergent ? I know that $\exists M>0$ such that $|f(x)|< Mx, \forall x\in [1,\infty)$ , so $|1/f(n)|>1/(Mn) ,\forall n \ge 1$ , thus $\sum_{n=1}^\infty |1/f(n)|$ is divergent. But I don't know what happens with $\sum_{n=1}^\infty 1/f(n)$ . Please help","Does there exist a uniformly continuous function such that is convergent ? I know that such that , so , thus is divergent. But I don't know what happens with . Please help","f:[1,\infty)\to \mathbb R \sum_{n=1}^\infty 1/f(n) \exists M>0 |f(x)|< Mx, \forall x\in [1,\infty) |1/f(n)|>1/(Mn) ,\forall n \ge 1 \sum_{n=1}^\infty |1/f(n)| \sum_{n=1}^\infty 1/f(n)","['real-analysis', 'sequences-and-series', 'uniform-continuity']"
48,Real world application of Lebesgue measure as opposed to Jordan measure [closed],Real world application of Lebesgue measure as opposed to Jordan measure [closed],,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question Are there real world applications of Lebesgue measure? I think Jordan measure is sufficient to solve real world problems.,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question Are there real world applications of Lebesgue measure? I think Jordan measure is sufficient to solve real world problems.,,"['real-analysis', 'measure-theory', 'soft-question', 'lebesgue-measure', 'applications']"
49,Laplace method (or other integral asymptotic) with near-corner,Laplace method (or other integral asymptotic) with near-corner,,"Consider the integral $$\int_{-\infty}^\infty \exp(-\sqrt{h^2+M^2x^2}) dx.$$ Here $h$ is a small positive parameter and $M$ is a large positive parameter. I would like to obtain a ""reasonably uniform"" asymptotic approximation for this integral in the limit of large $M$ and small $h$, specifically when $h$ goes to zero before $M$ goes to infinity. The difficulty is that the leading order part of the Laplace method sees $\sqrt{h^2+M^2 x^2}$ as $h+\frac{M^2}{2h} x^2$, a quadratic function, but in fact this approximation is only any good where $|x| \ll h/M$. By contrast there is a significant contribution to the integration over an interval of length on the order of $1/M$, which is much larger. Higher order Taylor approximations never see this because they just keep on assuming that $|x| \ll h/M$ and thus proceed to divide by larger and larger powers of $h$. An obvious alternative is to sacrifice accuracy in this $O(h/M)$ vicinity of $0$, for example by suppressing $h^2$ altogether, but this obviously does not achieve $o(h)$ accuracy, which is required for my application. Is there another workaround for this situation? Perhaps by ""matching"" the two approximations which are valid in different regimes?","Consider the integral $$\int_{-\infty}^\infty \exp(-\sqrt{h^2+M^2x^2}) dx.$$ Here $h$ is a small positive parameter and $M$ is a large positive parameter. I would like to obtain a ""reasonably uniform"" asymptotic approximation for this integral in the limit of large $M$ and small $h$, specifically when $h$ goes to zero before $M$ goes to infinity. The difficulty is that the leading order part of the Laplace method sees $\sqrt{h^2+M^2 x^2}$ as $h+\frac{M^2}{2h} x^2$, a quadratic function, but in fact this approximation is only any good where $|x| \ll h/M$. By contrast there is a significant contribution to the integration over an interval of length on the order of $1/M$, which is much larger. Higher order Taylor approximations never see this because they just keep on assuming that $|x| \ll h/M$ and thus proceed to divide by larger and larger powers of $h$. An obvious alternative is to sacrifice accuracy in this $O(h/M)$ vicinity of $0$, for example by suppressing $h^2$ altogether, but this obviously does not achieve $o(h)$ accuracy, which is required for my application. Is there another workaround for this situation? Perhaps by ""matching"" the two approximations which are valid in different regimes?",,"['real-analysis', 'integration', 'asymptotics']"
50,$f(x+1)=f(x)$ almost everywhere,almost everywhere,f(x+1)=f(x),"Let $f:\mathbb{R}\to\mathbb{C}$ be a function such that $f(x+1)=f(x)$ almost everywhere. I want to prove that exists a function $F$ such that $F(x+1)=F(x)$ always holds and $F(x)=f(x)$ almost everywhere. I think the intuition here is simple: let $x_0$ be a real number, if every $x\in x_0\mathbb{Z}$ satisfies $f(x+1)=f(x)$, then we define $F(x)=f(x)$. If there are a couple exceptions, we fix them and keep going for every $x_0\in \mathbb{R}$. However I am failling to formalize this. Let $A=\{x\in\mathbb{R} : f(x+1)\neq f(x)\}$. Since $\mathbb{R}\setminus A$ is dense, for every $x\in\mathbb{R}$ there is a sequence in $\mathbb{R}\setminus A$ that converges to $x$. Then maybe we could define $F(x)$ as $$\lim_{n\to\infty}f(x_n).$$ I am lost.","Let $f:\mathbb{R}\to\mathbb{C}$ be a function such that $f(x+1)=f(x)$ almost everywhere. I want to prove that exists a function $F$ such that $F(x+1)=F(x)$ always holds and $F(x)=f(x)$ almost everywhere. I think the intuition here is simple: let $x_0$ be a real number, if every $x\in x_0\mathbb{Z}$ satisfies $f(x+1)=f(x)$, then we define $F(x)=f(x)$. If there are a couple exceptions, we fix them and keep going for every $x_0\in \mathbb{R}$. However I am failling to formalize this. Let $A=\{x\in\mathbb{R} : f(x+1)\neq f(x)\}$. Since $\mathbb{R}\setminus A$ is dense, for every $x\in\mathbb{R}$ there is a sequence in $\mathbb{R}\setminus A$ that converges to $x$. Then maybe we could define $F(x)$ as $$\lim_{n\to\infty}f(x_n).$$ I am lost.",,"['real-analysis', 'lebesgue-measure']"
51,Does taking a fractional derivative remove a fractional amount of Holder regularity?,Does taking a fractional derivative remove a fractional amount of Holder regularity?,,"We define the space $C^{n+\alpha}$ as functions who are $n$ times differentiable whose $n$th derivative is $\alpha$ Holder. That is, each time we take a derivative we remove one number of regularity. When functions have no more regularity to give to take a derivative we get to negative $C$ spaces. I.e. it can be shown that a function that is $C^{-.5}$ is the weak/distributional derivative of a $C^{.5}$ function. How far can this be generalized? Can this be generalized to fractional derivatives? Meaning if we take a $.5$ derivative of a $C^{.9}$ function do we get a $C^{.4}$ function? Recall that we define $D^\alpha f(x)=\frac{1}{\Gamma(1-\alpha)}\frac{d}{dx}\int_0^x\frac{f(t)}{(x-t)^\alpha}dt$. This is true for $f(x)=x^{\alpha}$, which is $C^{\alpha}$. Fractional derivatives give $cx^{\alpha-\beta}$ which is $C^{\alpha-\beta}$.","We define the space $C^{n+\alpha}$ as functions who are $n$ times differentiable whose $n$th derivative is $\alpha$ Holder. That is, each time we take a derivative we remove one number of regularity. When functions have no more regularity to give to take a derivative we get to negative $C$ spaces. I.e. it can be shown that a function that is $C^{-.5}$ is the weak/distributional derivative of a $C^{.5}$ function. How far can this be generalized? Can this be generalized to fractional derivatives? Meaning if we take a $.5$ derivative of a $C^{.9}$ function do we get a $C^{.4}$ function? Recall that we define $D^\alpha f(x)=\frac{1}{\Gamma(1-\alpha)}\frac{d}{dx}\int_0^x\frac{f(t)}{(x-t)^\alpha}dt$. This is true for $f(x)=x^{\alpha}$, which is $C^{\alpha}$. Fractional derivatives give $cx^{\alpha-\beta}$ which is $C^{\alpha-\beta}$.",,['real-analysis']
52,Partial Sums of Geometric Series,Partial Sums of Geometric Series,,"This may be a simple question, but I was slightly confused.  I was looking at the second line $S_n(x)=1-x^{n+1}/(1-x)$.  I was confused how they derived this.  I know the infinite sum of a geometric series is $1/(1-x)$.  I just can't figure out how the partial sums, $S_n(x)$, have $1-x^{n+1}$ on the numerator.  How was this derived? Thank you. Example 5.20. The geometric series   $$     \sum_{n=0}^\infty x^n   = 1 + x + x^2 + x^3 + \dotsb $$   has partial sums   $$     S_n(x)   = \sum_{k=0}^n x^k   = \frac{1 - x^{n+1}}{1 - x} \cdotp $$   Thus, $S_n(x) \to 1/(1-x)$ as $n \to \infty$ if $|x| < 1$ and diverges if $|x| \geq 1$, meaning that   $$     \sum_{n=0}^\infty x^n   = \frac{1}{1-x}   \qquad   \text{pointwise on $(-1,1)$}. $$   (Original image here .)","This may be a simple question, but I was slightly confused.  I was looking at the second line $S_n(x)=1-x^{n+1}/(1-x)$.  I was confused how they derived this.  I know the infinite sum of a geometric series is $1/(1-x)$.  I just can't figure out how the partial sums, $S_n(x)$, have $1-x^{n+1}$ on the numerator.  How was this derived? Thank you. Example 5.20. The geometric series   $$     \sum_{n=0}^\infty x^n   = 1 + x + x^2 + x^3 + \dotsb $$   has partial sums   $$     S_n(x)   = \sum_{k=0}^n x^k   = \frac{1 - x^{n+1}}{1 - x} \cdotp $$   Thus, $S_n(x) \to 1/(1-x)$ as $n \to \infty$ if $|x| < 1$ and diverges if $|x| \geq 1$, meaning that   $$     \sum_{n=0}^\infty x^n   = \frac{1}{1-x}   \qquad   \text{pointwise on $(-1,1)$}. $$   (Original image here .)",,"['real-analysis', 'sequences-and-series']"
53,Taylor series of $\tan x - \tan (\sin x)$ has all coefficients positive. Why?,Taylor series of  has all coefficients positive. Why?,\tan x - \tan (\sin x),"It's well known that $x > \sin x$ for $x> 0$. The Taylor series of $ x - \sin x$ is also well known, and the coefficients are alternating. However, it appears that the Taylor coefficients of the function $\tan x - \tan (\sin x)$ are all positive ( and this implies  that $x > \sin x$ on $(0, \pi/2)$, as it should). It's not clear for me why this is true. In fact, one can go further as follows. It is known that we have inequalities of the form $$\sum_{k=0}^{2 l} (-1)^k\frac{x^{2k+1}}{(2k+1)!}< \sin x < \sum_{k=0}^{2 m + 1}(-1)^k \frac{x^{2k+1}}{(2k+1)!}$$ for $x > 0$. Let's consider for instance the inequality  $$x< \sin x + \frac{x^3}{6}$$ for $x>0$. Now, it appears again that the function $\tan ( \sin x + x^3/6) - \tan x$ has a ""positive"" Taylor expansion. Similarly for  $\tan ( x + \frac{x^5}{120}) - \tan( \sin x + x^3/6)$, and so on, for any inequality with positive coefficients obtained from the above by switching sides of terms. One can substitute the function $\sec$ for the function $\tan$. I am aware of the Taylor expansions of the functions $\tan$ and $\sec$ ( see the wikipedia article on trigonometric functions), they are all positive, and have a combinatorial interpretation. One can do some testing with WolframAlpha or any computer algebra system.","It's well known that $x > \sin x$ for $x> 0$. The Taylor series of $ x - \sin x$ is also well known, and the coefficients are alternating. However, it appears that the Taylor coefficients of the function $\tan x - \tan (\sin x)$ are all positive ( and this implies  that $x > \sin x$ on $(0, \pi/2)$, as it should). It's not clear for me why this is true. In fact, one can go further as follows. It is known that we have inequalities of the form $$\sum_{k=0}^{2 l} (-1)^k\frac{x^{2k+1}}{(2k+1)!}< \sin x < \sum_{k=0}^{2 m + 1}(-1)^k \frac{x^{2k+1}}{(2k+1)!}$$ for $x > 0$. Let's consider for instance the inequality  $$x< \sin x + \frac{x^3}{6}$$ for $x>0$. Now, it appears again that the function $\tan ( \sin x + x^3/6) - \tan x$ has a ""positive"" Taylor expansion. Similarly for  $\tan ( x + \frac{x^5}{120}) - \tan( \sin x + x^3/6)$, and so on, for any inequality with positive coefficients obtained from the above by switching sides of terms. One can substitute the function $\sec$ for the function $\tan$. I am aware of the Taylor expansions of the functions $\tan$ and $\sec$ ( see the wikipedia article on trigonometric functions), they are all positive, and have a combinatorial interpretation. One can do some testing with WolframAlpha or any computer algebra system.",,['real-analysis']
54,Convergence of a series depending on a parameter.,Convergence of a series depending on a parameter.,,"I was studying the convergence of a series with a parameter and I want to ask you if my conclusion is correct and if there is a better method to do it. $$\sum_{n=1}^{\infty}\left(\frac{\pi}{2}-\arcsin\frac{n}{n+4} \right)^{\alpha}$$ I'm asked to say for which $\alpha$ the series is convergent. I tried to do this: $$\sum_{n=1}^{\infty}\left(\frac{\pi}{2}-\arcsin\frac{n}{n+4} \right)^{\alpha}= \sum_{n=1}^{\infty}\left(\arccos\frac{n}{n+4} \right)^{\alpha}=\sum_{n=1}^{\infty}\left(\arccos\left(1-\frac{4}{n+4}\right)\right)^{\alpha}$$ Now I tried to see the Taylor Series of $\arccos(1-y)$ as $y\rightarrow 0$, $y=\frac{4}{n+4}$. By deriving $g(y)=\arccos(1-y)$ I obtain: $$g'(y)=\frac{1}{\sqrt{1-(1-y)^2}}=(-x^2+2x)^{\frac{1}{2}}=(2x)^{\frac{1}{2}}(1-\frac{x}{2})^{\frac{1}{2}}=\sqrt{2y}+o(\sqrt{2y})$$ So I see that: $$g(y)=\int\frac{1}{\sqrt{2y}}+o\left(\frac{1}{\sqrt{2x}}\right)dx=\sqrt{2x}+o(\sqrt{2x})$$ From this, I can conclude that my series is: $$\sum_{n=1}^{\infty}\left(\arccos\left(1-\frac{4}{n+4}\right)\right)^{\alpha} \sim \left(\sqrt{\frac{8}{n+4}}\right)^{\alpha}$$ So, for asymptotic comparison to $\left(\frac{1}{n}\right)^{\frac{\alpha}{2}}$ the series converges if and only if $\alpha > 2$. Am I right? If yes, may I ask you if there is a better method to do this computation? And, expecially, if there is a better method (if possible without involving integrals) to compute the Taylor series of inverse trigonometric functions! Thanks in advance.","I was studying the convergence of a series with a parameter and I want to ask you if my conclusion is correct and if there is a better method to do it. $$\sum_{n=1}^{\infty}\left(\frac{\pi}{2}-\arcsin\frac{n}{n+4} \right)^{\alpha}$$ I'm asked to say for which $\alpha$ the series is convergent. I tried to do this: $$\sum_{n=1}^{\infty}\left(\frac{\pi}{2}-\arcsin\frac{n}{n+4} \right)^{\alpha}= \sum_{n=1}^{\infty}\left(\arccos\frac{n}{n+4} \right)^{\alpha}=\sum_{n=1}^{\infty}\left(\arccos\left(1-\frac{4}{n+4}\right)\right)^{\alpha}$$ Now I tried to see the Taylor Series of $\arccos(1-y)$ as $y\rightarrow 0$, $y=\frac{4}{n+4}$. By deriving $g(y)=\arccos(1-y)$ I obtain: $$g'(y)=\frac{1}{\sqrt{1-(1-y)^2}}=(-x^2+2x)^{\frac{1}{2}}=(2x)^{\frac{1}{2}}(1-\frac{x}{2})^{\frac{1}{2}}=\sqrt{2y}+o(\sqrt{2y})$$ So I see that: $$g(y)=\int\frac{1}{\sqrt{2y}}+o\left(\frac{1}{\sqrt{2x}}\right)dx=\sqrt{2x}+o(\sqrt{2x})$$ From this, I can conclude that my series is: $$\sum_{n=1}^{\infty}\left(\arccos\left(1-\frac{4}{n+4}\right)\right)^{\alpha} \sim \left(\sqrt{\frac{8}{n+4}}\right)^{\alpha}$$ So, for asymptotic comparison to $\left(\frac{1}{n}\right)^{\frac{\alpha}{2}}$ the series converges if and only if $\alpha > 2$. Am I right? If yes, may I ask you if there is a better method to do this computation? And, expecially, if there is a better method (if possible without involving integrals) to compute the Taylor series of inverse trigonometric functions! Thanks in advance.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
55,"is $\int_1^x \sin(\sin(t)+\cos(t\sqrt{2}))\,dt$ a bounded function?",is  a bounded function?,"\int_1^x \sin(\sin(t)+\cos(t\sqrt{2}))\,dt","The function $f(x)=\int_1^x \sin(\sin(t)+\cos(t\sqrt{2})) \, dt$ seems bounded, so I plot it in maple and I deduce that $f$ is always between $-1$ and $1$. I tried to prove it by searching the max and min of this function, but this doesn't help me at all. Do you any idea why $f$ is bounded?","The function $f(x)=\int_1^x \sin(\sin(t)+\cos(t\sqrt{2})) \, dt$ seems bounded, so I plot it in maple and I deduce that $f$ is always between $-1$ and $1$. I tried to prove it by searching the max and min of this function, but this doesn't help me at all. Do you any idea why $f$ is bounded?",,"['calculus', 'real-analysis', 'analysis']"
56,Riccati Comparison Principle,Riccati Comparison Principle,,"This is a proposition in a book I'm reading whose proof I am unsure of how to fill in the details Proposition Suppose we have two smooth functions $\rho_{1,2} : (0,b) \to \mathbb{R}$ such that $$\rho_1'+ \rho_1^2 \leq \rho_2'+ \rho_2^2 $$ then $$\rho_2 - \rho_1 \geq \limsup_{t \to 0}\,(\, \rho_2(t) - \rho_1(t)\,)$$ Question The author says that this follows from the easily verified fact that the function $(\rho_2 - \rho_1) e^F$ is increasing where $F$ is the antiderivative of $\rho_2 + \rho_1$ on $(0, b)$ . This would be true if $\rho_2 - \rho_1$ is increasing but I don't think that this is the case. How does the lim sup inequality follow from the fact that $(\rho_2 - \rho_1) e^F$ is increasing?",This is a proposition in a book I'm reading whose proof I am unsure of how to fill in the details Proposition Suppose we have two smooth functions such that then Question The author says that this follows from the easily verified fact that the function is increasing where is the antiderivative of on . This would be true if is increasing but I don't think that this is the case. How does the lim sup inequality follow from the fact that is increasing?,"\rho_{1,2} : (0,b) \to \mathbb{R} \rho_1'+ \rho_1^2 \leq \rho_2'+ \rho_2^2  \rho_2 - \rho_1 \geq \limsup_{t \to 0}\,(\, \rho_2(t) - \rho_1(t)\,) (\rho_2 - \rho_1) e^F F \rho_2 + \rho_1 (0, b) \rho_2 - \rho_1 (\rho_2 - \rho_1) e^F","['real-analysis', 'ordinary-differential-equations']"
57,Improper Integral $\int_0^\infty\log(x)e^{-x^2}dx$ [closed],Improper Integral  [closed],\int_0^\infty\log(x)e^{-x^2}dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Why is: $$\int_0^\infty\log(x)e^{-x^2}dx=-\frac{\sqrt\pi}4(\gamma+\log4)$$ And does anybody have a reference?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Why is: $$\int_0^\infty\log(x)e^{-x^2}dx=-\frac{\sqrt\pi}4(\gamma+\log4)$$ And does anybody have a reference?",,"['calculus', 'real-analysis', 'definite-integrals', 'improper-integrals', 'closed-form']"
58,Lebesgue $n$-dimensional measure of a hyperplane,Lebesgue -dimensional measure of a hyperplane,n,Show that every $n-1$-dimensional hyperplane in $\mathbb R^n$ has zero $n$-dimensional Lebesgue measure. My problem is that I'm stuck on how to cover the hyperplane.,Show that every $n-1$-dimensional hyperplane in $\mathbb R^n$ has zero $n$-dimensional Lebesgue measure. My problem is that I'm stuck on how to cover the hyperplane.,,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
59,Necessary and sufficient condition for a curve to have infinite length,Necessary and sufficient condition for a curve to have infinite length,,"What is the necessary and sufficient condition for a curve to have infinite length in a compact interval?  Say the curve is restricted to $[0, 1]$. I vaguely remember that it is related to the boundedness of the total variation. I checked already the answers here but they are related to specific examples.","What is the necessary and sufficient condition for a curve to have infinite length in a compact interval?  Say the curve is restricted to $[0, 1]$. I vaguely remember that it is related to the boundedness of the total variation. I checked already the answers here but they are related to specific examples.",,"['real-analysis', 'bounded-variation']"
60,"How to calculate $\int_0^{1} \frac{\arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$?",How to calculate ?,"\int_0^{1} \frac{\arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx","What is your best idea for this integral? No need for any full solution, this is optional. I'm curious about the core idea you might like to use to make all very simple . Calculate in closed-form $$\int_0^{1} \frac{\displaystyle \arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ Supplementary questions: $$a) \ \int_0^{1} \frac{\displaystyle \arctan^3\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ $$b) \ \int_0^{1} \frac{\displaystyle \arctan^3\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)\log(x)}{x} \, dx.$$ EDIT: Maybe it helps to write down the closed-form I got for the first integral, which is $$\int_0^{1} \frac{\displaystyle \arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ $$=\frac{1}{216} \left(\sqrt{3} \pi  \left(\psi ^{(1)}\left(\frac{1}{3}\right)-\psi ^{(1)}\left(\frac{2}{3}\right)+\psi ^{(1)}\left(\frac{1}{6}\right)-\psi ^{(1)}\left(\frac{5}{6}\right)\right)-144 \zeta (3)\right).$$","What is your best idea for this integral? No need for any full solution, this is optional. I'm curious about the core idea you might like to use to make all very simple . Calculate in closed-form $$\int_0^{1} \frac{\displaystyle \arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ Supplementary questions: $$a) \ \int_0^{1} \frac{\displaystyle \arctan^3\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ $$b) \ \int_0^{1} \frac{\displaystyle \arctan^3\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)\log(x)}{x} \, dx.$$ EDIT: Maybe it helps to write down the closed-form I got for the first integral, which is $$\int_0^{1} \frac{\displaystyle \arctan^2\left(\frac{\sqrt{x}}{\sqrt{4-x}}\right)}{x} \, dx$$ $$=\frac{1}{216} \left(\sqrt{3} \pi  \left(\psi ^{(1)}\left(\frac{1}{3}\right)-\psi ^{(1)}\left(\frac{2}{3}\right)+\psi ^{(1)}\left(\frac{1}{6}\right)-\psi ^{(1)}\left(\frac{5}{6}\right)\right)-144 \zeta (3)\right).$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
61,Heine definition of limit of a function at infinity using sequences,Heine definition of limit of a function at infinity using sequences,,"I couldn't find the answer neither on Google, nor this website, so decided to ask. The Heine definition of limit: from Wikipedia $\lim_{x\to a}f(x)=L$ if and only if for all sequences $x_n$ (with $x_n$ not equal to $a$ for all $n$) converging to $a$ the sequence $f(x_n)$ converges to $L$. Does it work with limit of a function at infinity, using a sequence that converges to infinity? (for example: $x_n = n$) Thank you.","I couldn't find the answer neither on Google, nor this website, so decided to ask. The Heine definition of limit: from Wikipedia $\lim_{x\to a}f(x)=L$ if and only if for all sequences $x_n$ (with $x_n$ not equal to $a$ for all $n$) converging to $a$ the sequence $f(x_n)$ converges to $L$. Does it work with limit of a function at infinity, using a sequence that converges to infinity? (for example: $x_n = n$) Thank you.",,"['real-analysis', 'limits', 'definition']"
62,limit of the sequence $a_n=1+\frac{1}{a_{n-1}}$ and $a_1=1$,limit of the sequence  and,a_n=1+\frac{1}{a_{n-1}} a_1=1,"Problem: Find with proof limit of the sequence $a_n=1+\frac{1}{a_{n-1}}$ with  $a_1=1$  or show that the limit does not exist. My attempt: I have failed to determine the existence. However if the limit exists then it is easy to find it. The sequence is not  monotonic and I have failed to find any monotonic subsequence subsequence. The sequence is clearly bounded below by $1$. I have observed that the sequence is a  continued fraction so it alternatively increases and decreases. So, please help me.","Problem: Find with proof limit of the sequence $a_n=1+\frac{1}{a_{n-1}}$ with  $a_1=1$  or show that the limit does not exist. My attempt: I have failed to determine the existence. However if the limit exists then it is easy to find it. The sequence is not  monotonic and I have failed to find any monotonic subsequence subsequence. The sequence is clearly bounded below by $1$. I have observed that the sequence is a  continued fraction so it alternatively increases and decreases. So, please help me.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'recurrence-relations']"
63,Show that the set of points where a real-valued continuous sequence of functions converges is $F_{\sigma\delta}$,Show that the set of points where a real-valued continuous sequence of functions converges is,F_{\sigma\delta},"By $F_{\sigma\delta}$ , I mean that the set can be expressed as a countable intersection of $F_\sigma$ sets. Let this sequence of functions be $f_n$ , and the set of points where $f_n$ converges be $C$ . Since $f_n$ must be Cauchy, I can define $C$ as $$C = \{x \in \mathbb{R}\:|\: \forall\epsilon\in\mathbb{N} \; \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}$$ . Looking at this set, I'm trying to prove $F_{\sigma\delta}$ by firstly trying to find some countable intersection. I try to rewrite $C$ as $$C = \bigcap\limits_{\epsilon \in \mathbb{N}}\{x \in \mathbb{R}\:|\: \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}$$ . Now I need to express each set being intersected as a countable union of closed sets to prove that it is $F_\sigma$ . Then the countable intersection of these must be $F_{\sigma\delta}$ . I tried to pick $N$ to describe this union, and rewrote $C$ as $$C = \bigcap\limits_{\epsilon \in \mathbb{N}}\bigg[\;\bigcup\limits_{N \in \mathbb{N}}\{x \in \mathbb{R}\:|\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}\;\bigg]$$ . But this option confuses me because $N$ depends on $\epsilon$ , so I don't know whether it would make sense to let both vary freely like that. Am I going in the wrong direction?","By , I mean that the set can be expressed as a countable intersection of sets. Let this sequence of functions be , and the set of points where converges be . Since must be Cauchy, I can define as . Looking at this set, I'm trying to prove by firstly trying to find some countable intersection. I try to rewrite as . Now I need to express each set being intersected as a countable union of closed sets to prove that it is . Then the countable intersection of these must be . I tried to pick to describe this union, and rewrote as . But this option confuses me because depends on , so I don't know whether it would make sense to let both vary freely like that. Am I going in the wrong direction?","F_{\sigma\delta} F_\sigma f_n f_n C f_n C C = \{x \in \mathbb{R}\:|\: \forall\epsilon\in\mathbb{N} \; \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\} F_{\sigma\delta} C C = \bigcap\limits_{\epsilon \in \mathbb{N}}\{x \in \mathbb{R}\:|\: \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\} F_\sigma F_{\sigma\delta} N C C = \bigcap\limits_{\epsilon \in \mathbb{N}}\bigg[\;\bigcup\limits_{N \in \mathbb{N}}\{x \in \mathbb{R}\:|\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}\;\bigg] N \epsilon",['real-analysis']
64,Function in $L^p$ but not in $L^{\infty}$,Function in  but not in,L^p L^{\infty},"Show that if $$f(x) = \ln\left({1\over x}\right),\quad 0<x\le 1$$ then $f\in L^p((0, 1])$ for all $1 \le p < \infty$ but $f\not\in L^{\infty}((0, 1])$. Intuitively I know I have to show that $\int_{[0,1]}|f|^p <\infty $ but $f$ is not essentialy bounded, that means $\exists M >0$, s.t. $|f(x)|>M$ but I forget how to compute the integral and I am not sure how to find $M$. Any help, Thanks.","Show that if $$f(x) = \ln\left({1\over x}\right),\quad 0<x\le 1$$ then $f\in L^p((0, 1])$ for all $1 \le p < \infty$ but $f\not\in L^{\infty}((0, 1])$. Intuitively I know I have to show that $\int_{[0,1]}|f|^p <\infty $ but $f$ is not essentialy bounded, that means $\exists M >0$, s.t. $|f(x)|>M$ but I forget how to compute the integral and I am not sure how to find $M$. Any help, Thanks.",,"['real-analysis', 'measure-theory', 'lp-spaces']"
65,"The Radon-Nikodym derivative of a measure such that $|\int f'\,d\mu|\le \|f\|_{L^2}$ for $f\in C^1$",The Radon-Nikodym derivative of a measure such that  for,"|\int f'\,d\mu|\le \|f\|_{L^2} f\in C^1","Suppose that $\mu$ is a measure on the Borel $\sigma$-algebra on $[0,1]$ and for every $f$ that is real-valued and continuously differentiable we have $$ \left|\int f'(x)~\mu(\text{d}x)\right|  \leqslant \left(\int_0^1 f^2(x) ~\text{d}x\right) ^{1/2}. $$ (1) Show that $\mu$ is absolutely continuous with respect to Lebesgue measure on [0,1]. (2) If $g$ is the Radon-Nikodym derivative of $\mu$ with respect to Lebesgue measure, prove that there exists a constant $c>0$ such that $$ \left|g(x)-g(y)\right|\le c\left|x-y\right|^{1/2},~~~~~~x,y\in[0,1]. $$ Attempt The inequality given reminds me of Jensen's inequality but I've no idea how to use it on (1).","Suppose that $\mu$ is a measure on the Borel $\sigma$-algebra on $[0,1]$ and for every $f$ that is real-valued and continuously differentiable we have $$ \left|\int f'(x)~\mu(\text{d}x)\right|  \leqslant \left(\int_0^1 f^2(x) ~\text{d}x\right) ^{1/2}. $$ (1) Show that $\mu$ is absolutely continuous with respect to Lebesgue measure on [0,1]. (2) If $g$ is the Radon-Nikodym derivative of $\mu$ with respect to Lebesgue measure, prove that there exists a constant $c>0$ such that $$ \left|g(x)-g(y)\right|\le c\left|x-y\right|^{1/2},~~~~~~x,y\in[0,1]. $$ Attempt The inequality given reminds me of Jensen's inequality but I've no idea how to use it on (1).",,"['real-analysis', 'integration', 'functional-analysis', 'derivatives']"
66,"Step functions on $[a,b]$ are dense in $\mathcal C^0([a,b])$.",Step functions on  are dense in .,"[a,b] \mathcal C^0([a,b])","Let $\|f \|=\sup_{[a,b]}|f|$. We consider ($\mathcal C^0([a,b]),\|\cdot \|)$ and $(\mathcal E([a,b]),\|\cdot \|)$ where $\mathcal E([a,b])$ is a set of the step functions on $[a,b]$. I have to show that $\mathcal E([a,b])$ is dense in $\mathcal C^0([a,b])$. There is my proof: Let $f\in\mathcal C^0([a,b])$ and $\varepsilon>0$. Since $f$ is continuous on $[a,b]$ it's also uniformly continuous and thus, there is a $\delta>0$ s.t. $|f(x)-f(y)|<\varepsilon$ for all $x,y\in[a,b]$ s.t. $|x-y|<\delta$. Let $a=x_0<x_1<...<x_n=b$ such that $x_{i+1}-x_i<\delta$ for all $i$. We set $\varphi(x)=f(x_i)$ for all $i=0,...,n-1$ and $\varphi(x_n)=f(x_n)$. Therefore, if $x\in[a,b[$, there is a $i$ such that $x_i\leq x<x_{i+1}$ and thus $$|f(x)-\varphi(x)|=|f(x)-\varphi(x_i)|<\varepsilon$$ since $|x-x_i|<\delta$. Finally, since $f(b)=\varphi(b)$ we get that  $$\forall x\in[a,b], |f(x)-\varphi(x)|<\varepsilon$$ and thus $$\|f-\varphi\|=\sup_{[a,b]}|f-\varphi|<\varepsilon$$ what prove the claim. Q1) Is it correct ? Q2) Something is strange to me. If $A$ is dense in $B$, in particular $A\subset B$, but here, how can $\mathcal E([a,b])\subset \mathcal C^0([a,b])$ since an element of $\mathcal E([a,b])$ is not necessarily continuous ?","Let $\|f \|=\sup_{[a,b]}|f|$. We consider ($\mathcal C^0([a,b]),\|\cdot \|)$ and $(\mathcal E([a,b]),\|\cdot \|)$ where $\mathcal E([a,b])$ is a set of the step functions on $[a,b]$. I have to show that $\mathcal E([a,b])$ is dense in $\mathcal C^0([a,b])$. There is my proof: Let $f\in\mathcal C^0([a,b])$ and $\varepsilon>0$. Since $f$ is continuous on $[a,b]$ it's also uniformly continuous and thus, there is a $\delta>0$ s.t. $|f(x)-f(y)|<\varepsilon$ for all $x,y\in[a,b]$ s.t. $|x-y|<\delta$. Let $a=x_0<x_1<...<x_n=b$ such that $x_{i+1}-x_i<\delta$ for all $i$. We set $\varphi(x)=f(x_i)$ for all $i=0,...,n-1$ and $\varphi(x_n)=f(x_n)$. Therefore, if $x\in[a,b[$, there is a $i$ such that $x_i\leq x<x_{i+1}$ and thus $$|f(x)-\varphi(x)|=|f(x)-\varphi(x_i)|<\varepsilon$$ since $|x-x_i|<\delta$. Finally, since $f(b)=\varphi(b)$ we get that  $$\forall x\in[a,b], |f(x)-\varphi(x)|<\varepsilon$$ and thus $$\|f-\varphi\|=\sup_{[a,b]}|f-\varphi|<\varepsilon$$ what prove the claim. Q1) Is it correct ? Q2) Something is strange to me. If $A$ is dense in $B$, in particular $A\subset B$, but here, how can $\mathcal E([a,b])\subset \mathcal C^0([a,b])$ since an element of $\mathcal E([a,b])$ is not necessarily continuous ?",,"['real-analysis', 'functional-analysis']"
67,Show that $\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty$ implies $f_n \to f$ a.e.,Show that  implies  a.e.,\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty f_n \to f,"Show that $\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty$ implies $f_n \to f$ a.e, where $f_n$ and $f$ are measurable functions. My attempt: The Borel Cantelli lemma gives us that almost all $x$ are in at most finitely many $A_n = \{x : |f_n(x) - f(x)| > \epsilon\}$, so, for each $x$, there exists some $N$ such that for all $n \ge N$, $|f_n(x) - f(x)| \le \epsilon$. Since this is true for all $\epsilon$, we let $\epsilon$ go to zero, and we are done. This is what I have, but someone told me that I have an issue with countability, and that in order to fix it, I need to consider $\epsilon = 1/k$. But, I don't see why my proof is incorrect, so I don't see how that would fix it.","Show that $\sum^\infty_{n=1} \mu(\{x : |f_n(x) - f(x)| > \epsilon\}) < \infty$ implies $f_n \to f$ a.e, where $f_n$ and $f$ are measurable functions. My attempt: The Borel Cantelli lemma gives us that almost all $x$ are in at most finitely many $A_n = \{x : |f_n(x) - f(x)| > \epsilon\}$, so, for each $x$, there exists some $N$ such that for all $n \ge N$, $|f_n(x) - f(x)| \le \epsilon$. Since this is true for all $\epsilon$, we let $\epsilon$ go to zero, and we are done. This is what I have, but someone told me that I have an issue with countability, and that in order to fix it, I need to consider $\epsilon = 1/k$. But, I don't see why my proof is incorrect, so I don't see how that would fix it.",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
68,The limit inferior of Borel functions [closed],The limit inferior of Borel functions [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Suppose $X$ is a separable metric space and $F \colon X \times ℝ_+→[0,1]$ is Borel. Let $f(x) = \liminf_{ε→0} F(x,ε)$. Is $f$ Borel?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Suppose $X$ is a separable metric space and $F \colon X \times ℝ_+→[0,1]$ is Borel. Let $f(x) = \liminf_{ε→0} F(x,ε)$. Is $f$ Borel?",,"['real-analysis', 'descriptive-set-theory']"
69,An infinite series in polygamma function,An infinite series in polygamma function,,"I'm interested in $2$ things: $1)$ if you're used to such series and when you met before such series and $2)$ the tools you might like to employ $3)$ I don't ask for a solution . Calculating in closed form $$\sum _{n=1}^{\infty } (24 \psi ^{(-3)}(n)-24 \psi ^{(-3)}(n+1)+24 \psi ^{(-2)}(n+1)-12 \text{log$\Gamma $}(n+1)+4 \psi ^{(0)}(n+1)-\psi ^{(1)}(n+1))$$ EDIT: I conjecture the beautiful closed form  $$36 \log (A)+3 \gamma+\zeta (2) -6 \log (2 \pi )-\frac{3 \zeta (3)}{2 \zeta (2)}$$ Here is another example with conjuctured closed form $$\sum _{n=1}^{\infty } (90 \text{log$\Gamma $}(n+1)-3628800 \psi ^{(-9)}(n)+3628800 \psi ^{(-9)}(n+1)-3628800 \psi ^{(-8)}(n+1)+1814400 \psi ^{(-7)}(n+1)-604800 \psi ^{(-6)}(n+1)+151200 \psi ^{(-5)}(n+1)-30240 \psi ^{(-4)}(n+1)+5040 \psi ^{(-3)}(n+1)-720 \psi ^{(-2)}(n+1)-10 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$ $$=-810 \log (A)+3240 \zeta '(-7)+11340 \zeta '(-5)+7560 \zeta '(-3)+\frac{810 \zeta (3)}{\pi ^2}+\frac{42525 \zeta (7)}{\pi ^6}-\frac{8505 \zeta (5)}{\pi ^4}-\frac{127575 \zeta (9)}{2 \pi ^8}-\frac{\pi ^2}{6}+\frac{13371}{280}-9 \gamma +55 \log (2)-2 \log (32)+45 \log (\pi )$$ And again, a last one $$\sum _{n=1}^{\infty }(380 \text{log$\Gamma $}(n+1)-2432902008176640000 \psi ^{(-19)}(n)+2432902008176640000 \psi ^{(-19)}(n+1)-2432902008176640000 \psi ^{(-18)}(n+1)+1216451004088320000 \psi ^{(-17)}(n+1)-405483668029440000 \psi ^{(-16)}(n+1)+101370917007360000 \psi ^{(-15)}(n+1)-20274183401472000 \psi ^{(-14)}(n+1)+3379030566912000 \psi ^{(-13)}(n+1)-482718652416000 \psi ^{(-12)}(n+1)+60339831552000 \psi ^{(-11)}(n+1)-6704425728000 \psi ^{(-10)}(n+1)+670442572800 \psi ^{(-9)}(n+1)-60949324800 \psi ^{(-8)}(n+1)+5079110400 \psi ^{(-7)}(n+1)-390700800 \psi ^{(-6)}(n+1)+27907200 \psi ^{(-5)}(n+1)-1860480 \psi ^{(-4)}(n+1)+116280 \psi ^{(-3)}(n+1)-6840 \psi ^{(-2)}(n+1)-20 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$  $$=-7220 \log (A)+64980 \zeta '(-17)+1472880 \zeta '(-15)+10310160 \zeta '(-13)+28721160 \zeta '(-11)+35103640 \zeta '(-9)+19147440 \zeta '(-7)+4418640 \zeta '(-5)+368220 \zeta '(-3)+\frac{16245 \zeta (3)}{\pi ^2}+\frac{57994650 \zeta (7)}{\pi ^6}+\frac{62199262125 \zeta (11)}{\pi ^{10}}+\frac{11755660541625 \zeta (15)}{\pi ^{14}}+\frac{176334908124375 \zeta (19)}{2 \pi ^{18}}-\frac{1104660 \zeta (5)}{\pi ^4}-\frac{2261791350 \zeta (9)}{\pi ^8}-\frac{1119586718250 \zeta (13)}{\pi ^{12}}-\frac{58778302708125 \zeta (17)}{\pi ^{16}}-\frac{\pi ^2}{6}+\frac{17504273203}{16081065}-19 \gamma +210 \log (2)-2 \log (1024)+190 \log (\pi )$$ I already said the previous is going to be the last one, but this one is the last one $$\sum_{n=1}^{\infty}(870 \text{log$\Gamma $}(n+1)-265252859812191058636308480000000 \psi ^{(-29)}(n)+265252859812191058636308480000000 \psi ^{(-29)}(n+1)-265252859812191058636308480000000 \psi ^{(-28)}(n+1)+132626429906095529318154240000000 \psi ^{(-27)}(n+1)-44208809968698509772718080000000 \psi ^{(-26)}(n+1)+11052202492174627443179520000000 \psi ^{(-25)}(n+1)-2210440498434925488635904000000 \psi ^{(-24)}(n+1)+368406749739154248105984000000 \psi ^{(-23)}(n+1)-52629535677022035443712000000 \psi ^{(-22)}(n+1)+6578691959627754430464000000 \psi ^{(-21)}(n+1)-730965773291972714496000000 \psi ^{(-20)}(n+1)+73096577329197271449600000 \psi ^{(-19)}(n+1)-6645143393563388313600000 \psi ^{(-18)}(n+1)+553761949463615692800000 \psi ^{(-17)}(n+1)-42597073035662745600000 \psi ^{(-16)}(n+1)+3042648073975910400000 \psi ^{(-15)}(n+1)-202843204931727360000 \psi ^{(-14)}(n+1)+12677700308232960000 \psi ^{(-13)}(n+1)-745747076954880000 \psi ^{(-12)}(n+1)+41430393164160000 \psi ^{(-11)}(n+1)-2180547008640000 \psi ^{(-10)}(n+1)+109027350432000 \psi ^{(-9)}(n+1)-5191778592000 \psi ^{(-8)}(n+1)+235989936000 \psi ^{(-7)}(n+1)-10260432000 \psi ^{(-6)}(n+1)+427518000 \psi ^{(-5)}(n+1)-17100720 \psi ^{(-4)}(n+1)+657720 \psi ^{(-3)}(n+1)-24360 \psi ^{(-2)}(n+1)-30 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$ $$=-25230 \log (A)+353220 \zeta '(-27)+20663370 \zeta '(-25)+413267400 \zeta '(-23)+3734166150 \zeta '(-21)+17426108700 \zeta '(-19)+45149463450 \zeta '(-17)+67476121200 \zeta '(-15)+59041606050 \zeta '(-13)+30099642300 \zeta '(-11)+8713054350 \zeta '(-9)+1357878600 \zeta '(-7)+103316850 \zeta '(-5)+3178980 \zeta '(-3)+\frac{88305 \zeta (3)}{\pi ^2}+\frac{2324629125 \zeta (7)}{\pi ^6}+\frac{61753772705625 \zeta (11)}{2 \pi ^{10}}+\frac{179518217255251875 \zeta (15)}{\pi ^{14}}+\frac{735127099660256428125 \zeta (19)}{2 \pi ^{18}}+\frac{363887914331826931921875 \zeta (23)}{2 \pi ^{22}}+\frac{38208231004841827851796875 \zeta (27)}{4 \pi ^{26}}-\frac{30995055 \zeta (5)}{2 \pi ^4}-\frac{588131168625 \zeta (9)}{2 \pi ^8}-\frac{10559895132661875 \zeta (13)}{4 \pi ^{12}}-\frac{18849412811801446875 \zeta (17)}{2 \pi ^{16}}-\frac{40431990481314103546875 \zeta (21)}{4 \pi ^{20}}-\frac{7641646200968365570359375 \zeta (25)}{4 \pi ^{24}}-\frac{114624693014525483555390625 \zeta (29)}{8 \pi ^{28}}-\frac{\pi ^2}{6}-\frac{16571275403939220313}{40156716600}-29 \gamma +435 \log (2)+435 \log (\pi )$$","I'm interested in $2$ things: $1)$ if you're used to such series and when you met before such series and $2)$ the tools you might like to employ $3)$ I don't ask for a solution . Calculating in closed form $$\sum _{n=1}^{\infty } (24 \psi ^{(-3)}(n)-24 \psi ^{(-3)}(n+1)+24 \psi ^{(-2)}(n+1)-12 \text{log$\Gamma $}(n+1)+4 \psi ^{(0)}(n+1)-\psi ^{(1)}(n+1))$$ EDIT: I conjecture the beautiful closed form  $$36 \log (A)+3 \gamma+\zeta (2) -6 \log (2 \pi )-\frac{3 \zeta (3)}{2 \zeta (2)}$$ Here is another example with conjuctured closed form $$\sum _{n=1}^{\infty } (90 \text{log$\Gamma $}(n+1)-3628800 \psi ^{(-9)}(n)+3628800 \psi ^{(-9)}(n+1)-3628800 \psi ^{(-8)}(n+1)+1814400 \psi ^{(-7)}(n+1)-604800 \psi ^{(-6)}(n+1)+151200 \psi ^{(-5)}(n+1)-30240 \psi ^{(-4)}(n+1)+5040 \psi ^{(-3)}(n+1)-720 \psi ^{(-2)}(n+1)-10 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$ $$=-810 \log (A)+3240 \zeta '(-7)+11340 \zeta '(-5)+7560 \zeta '(-3)+\frac{810 \zeta (3)}{\pi ^2}+\frac{42525 \zeta (7)}{\pi ^6}-\frac{8505 \zeta (5)}{\pi ^4}-\frac{127575 \zeta (9)}{2 \pi ^8}-\frac{\pi ^2}{6}+\frac{13371}{280}-9 \gamma +55 \log (2)-2 \log (32)+45 \log (\pi )$$ And again, a last one $$\sum _{n=1}^{\infty }(380 \text{log$\Gamma $}(n+1)-2432902008176640000 \psi ^{(-19)}(n)+2432902008176640000 \psi ^{(-19)}(n+1)-2432902008176640000 \psi ^{(-18)}(n+1)+1216451004088320000 \psi ^{(-17)}(n+1)-405483668029440000 \psi ^{(-16)}(n+1)+101370917007360000 \psi ^{(-15)}(n+1)-20274183401472000 \psi ^{(-14)}(n+1)+3379030566912000 \psi ^{(-13)}(n+1)-482718652416000 \psi ^{(-12)}(n+1)+60339831552000 \psi ^{(-11)}(n+1)-6704425728000 \psi ^{(-10)}(n+1)+670442572800 \psi ^{(-9)}(n+1)-60949324800 \psi ^{(-8)}(n+1)+5079110400 \psi ^{(-7)}(n+1)-390700800 \psi ^{(-6)}(n+1)+27907200 \psi ^{(-5)}(n+1)-1860480 \psi ^{(-4)}(n+1)+116280 \psi ^{(-3)}(n+1)-6840 \psi ^{(-2)}(n+1)-20 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$  $$=-7220 \log (A)+64980 \zeta '(-17)+1472880 \zeta '(-15)+10310160 \zeta '(-13)+28721160 \zeta '(-11)+35103640 \zeta '(-9)+19147440 \zeta '(-7)+4418640 \zeta '(-5)+368220 \zeta '(-3)+\frac{16245 \zeta (3)}{\pi ^2}+\frac{57994650 \zeta (7)}{\pi ^6}+\frac{62199262125 \zeta (11)}{\pi ^{10}}+\frac{11755660541625 \zeta (15)}{\pi ^{14}}+\frac{176334908124375 \zeta (19)}{2 \pi ^{18}}-\frac{1104660 \zeta (5)}{\pi ^4}-\frac{2261791350 \zeta (9)}{\pi ^8}-\frac{1119586718250 \zeta (13)}{\pi ^{12}}-\frac{58778302708125 \zeta (17)}{\pi ^{16}}-\frac{\pi ^2}{6}+\frac{17504273203}{16081065}-19 \gamma +210 \log (2)-2 \log (1024)+190 \log (\pi )$$ I already said the previous is going to be the last one, but this one is the last one $$\sum_{n=1}^{\infty}(870 \text{log$\Gamma $}(n+1)-265252859812191058636308480000000 \psi ^{(-29)}(n)+265252859812191058636308480000000 \psi ^{(-29)}(n+1)-265252859812191058636308480000000 \psi ^{(-28)}(n+1)+132626429906095529318154240000000 \psi ^{(-27)}(n+1)-44208809968698509772718080000000 \psi ^{(-26)}(n+1)+11052202492174627443179520000000 \psi ^{(-25)}(n+1)-2210440498434925488635904000000 \psi ^{(-24)}(n+1)+368406749739154248105984000000 \psi ^{(-23)}(n+1)-52629535677022035443712000000 \psi ^{(-22)}(n+1)+6578691959627754430464000000 \psi ^{(-21)}(n+1)-730965773291972714496000000 \psi ^{(-20)}(n+1)+73096577329197271449600000 \psi ^{(-19)}(n+1)-6645143393563388313600000 \psi ^{(-18)}(n+1)+553761949463615692800000 \psi ^{(-17)}(n+1)-42597073035662745600000 \psi ^{(-16)}(n+1)+3042648073975910400000 \psi ^{(-15)}(n+1)-202843204931727360000 \psi ^{(-14)}(n+1)+12677700308232960000 \psi ^{(-13)}(n+1)-745747076954880000 \psi ^{(-12)}(n+1)+41430393164160000 \psi ^{(-11)}(n+1)-2180547008640000 \psi ^{(-10)}(n+1)+109027350432000 \psi ^{(-9)}(n+1)-5191778592000 \psi ^{(-8)}(n+1)+235989936000 \psi ^{(-7)}(n+1)-10260432000 \psi ^{(-6)}(n+1)+427518000 \psi ^{(-5)}(n+1)-17100720 \psi ^{(-4)}(n+1)+657720 \psi ^{(-3)}(n+1)-24360 \psi ^{(-2)}(n+1)-30 \psi ^{(0)}(n+1)+\psi ^{(1)}(n+1))$$ $$=-25230 \log (A)+353220 \zeta '(-27)+20663370 \zeta '(-25)+413267400 \zeta '(-23)+3734166150 \zeta '(-21)+17426108700 \zeta '(-19)+45149463450 \zeta '(-17)+67476121200 \zeta '(-15)+59041606050 \zeta '(-13)+30099642300 \zeta '(-11)+8713054350 \zeta '(-9)+1357878600 \zeta '(-7)+103316850 \zeta '(-5)+3178980 \zeta '(-3)+\frac{88305 \zeta (3)}{\pi ^2}+\frac{2324629125 \zeta (7)}{\pi ^6}+\frac{61753772705625 \zeta (11)}{2 \pi ^{10}}+\frac{179518217255251875 \zeta (15)}{\pi ^{14}}+\frac{735127099660256428125 \zeta (19)}{2 \pi ^{18}}+\frac{363887914331826931921875 \zeta (23)}{2 \pi ^{22}}+\frac{38208231004841827851796875 \zeta (27)}{4 \pi ^{26}}-\frac{30995055 \zeta (5)}{2 \pi ^4}-\frac{588131168625 \zeta (9)}{2 \pi ^8}-\frac{10559895132661875 \zeta (13)}{4 \pi ^{12}}-\frac{18849412811801446875 \zeta (17)}{2 \pi ^{16}}-\frac{40431990481314103546875 \zeta (21)}{4 \pi ^{20}}-\frac{7641646200968365570359375 \zeta (25)}{4 \pi ^{24}}-\frac{114624693014525483555390625 \zeta (29)}{8 \pi ^{28}}-\frac{\pi ^2}{6}-\frac{16571275403939220313}{40156716600}-29 \gamma +435 \log (2)+435 \log (\pi )$$",,"['calculus', 'real-analysis', 'sequences-and-series']"
70,Does the series $\sum_{n=1}^{\infty} \frac{(-1)^n}{n(\sin(n)+2)}$ converge or diverge?,Does the series  converge or diverge?,\sum_{n=1}^{\infty} \frac{(-1)^n}{n(\sin(n)+2)},I was thinking about it and was stumped. Mathematica claims it converges.,I was thinking about it and was stumped. Mathematica claims it converges.,,['real-analysis']
71,Complex exponential has $1$ as Lipschitz constant.,Complex exponential has  as Lipschitz constant.,1,"(In the following, Lipschitz constant does not mean ""best Lipschitz constant"".) I've just read this in a book that I highly regard: Moreover, by mean value theorem, $u\to e^{iu}$ is Lipschitz continuous with Lipschitz constant $1$. How does the author infer this from the mean value theorem ? The theorem applies only for real-valued functions. The mean value theorem shows nontheless that $\sin$ and $\cos$ have Lipschitz constant $1$, and therefore $u\to e^{iu}$ has Lipschitz constant $2$. How can this be lowered  to $1$ ?","(In the following, Lipschitz constant does not mean ""best Lipschitz constant"".) I've just read this in a book that I highly regard: Moreover, by mean value theorem, $u\to e^{iu}$ is Lipschitz continuous with Lipschitz constant $1$. How does the author infer this from the mean value theorem ? The theorem applies only for real-valued functions. The mean value theorem shows nontheless that $\sin$ and $\cos$ have Lipschitz constant $1$, and therefore $u\to e^{iu}$ has Lipschitz constant $2$. How can this be lowered  to $1$ ?",,"['real-analysis', 'lipschitz-functions']"
72,Approximation of conditional expectation,Approximation of conditional expectation,,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a complete probability space. Let $\mathcal{A}$ be a complete sub-$\sigma$-algebra of $\mathcal{F}$. For the moment assume that $X$ is a random variable with finite variance. Then we have the nice property: $$\mathbb{E}[X|\mathcal{A}]=\arg \min_Y \mathbb{E}((X-Y)^2).$$ Here we take the minimum over $Y$ which are $\mathcal{A}$-measurable and have finite variance. In the general situation, $X$ only has finite mean, and in this case this approach does not work. I like this property because it makes conditional expectation relatively concrete, even in a relatively general situation: the conditional expectation of $X$ is just the orthogonal projection onto an appropriate subspace of the Hilbert space of random variables with finite variance. So I would like to try to extend a weaker version of it to the most general situation. My idea is as follows. Suppose $X$ has only finite mean, and take a sequence of random variables $X_n$ which converge in mean to $X$, such that each $X_n$ has finite variance. Of course they do not converge in mean square, but I do not require this here. Now consider the sequence $\mathbb{E}[X_n | \mathcal{A}]$. Does this sequence converge to $\mathbb{E}[X|\mathcal{A}]$? If so, in which senses does it converge (a.s., in mean, etc.)? If not, can I add a hypothesis to get this result (for instance, assume in addition that $X_n \to X$ a.s.)?","Let $(\Omega,\mathcal{F},\mathbb{P})$ be a complete probability space. Let $\mathcal{A}$ be a complete sub-$\sigma$-algebra of $\mathcal{F}$. For the moment assume that $X$ is a random variable with finite variance. Then we have the nice property: $$\mathbb{E}[X|\mathcal{A}]=\arg \min_Y \mathbb{E}((X-Y)^2).$$ Here we take the minimum over $Y$ which are $\mathcal{A}$-measurable and have finite variance. In the general situation, $X$ only has finite mean, and in this case this approach does not work. I like this property because it makes conditional expectation relatively concrete, even in a relatively general situation: the conditional expectation of $X$ is just the orthogonal projection onto an appropriate subspace of the Hilbert space of random variables with finite variance. So I would like to try to extend a weaker version of it to the most general situation. My idea is as follows. Suppose $X$ has only finite mean, and take a sequence of random variables $X_n$ which converge in mean to $X$, such that each $X_n$ has finite variance. Of course they do not converge in mean square, but I do not require this here. Now consider the sequence $\mathbb{E}[X_n | \mathcal{A}]$. Does this sequence converge to $\mathbb{E}[X|\mathcal{A}]$? If so, in which senses does it converge (a.s., in mean, etc.)? If not, can I add a hypothesis to get this result (for instance, assume in addition that $X_n \to X$ a.s.)?",,"['real-analysis', 'probability-theory', 'convergence-divergence']"
73,Indefinite integral which is not differentiable,Indefinite integral which is not differentiable,,"I would like to know if there exists an indefinite integral which is not differentiable? Is this possible? That is, I want to know if there exists a real function F defined in a interval $[a,b]$ by the equation $$F(x)=F(a)+\int_a^x f(t) dt $$ where $f:[a,b]\rightarrow \mathbb{R}$ is only Riemann integrable, that $F$ is not differentiable of such a function? Could you give an example?","I would like to know if there exists an indefinite integral which is not differentiable? Is this possible? That is, I want to know if there exists a real function F defined in a interval by the equation where is only Riemann integrable, that is not differentiable of such a function? Could you give an example?","[a,b] F(x)=F(a)+\int_a^x f(t) dt  f:[a,b]\rightarrow \mathbb{R} F","['real-analysis', 'derivatives', 'indefinite-integrals']"
74,Distance is (uniformly) continuous,Distance is (uniformly) continuous,,"I've been told that $$d(x,A) = \inf_{y \in A} |x-y|$$ is uniformly continuous, but I don't understand why? Is there a short proof of this statement or is this a slightly deeper result? This was a result discussed in my analysis lecture.","I've been told that $$d(x,A) = \inf_{y \in A} |x-y|$$ is uniformly continuous, but I don't understand why? Is there a short proof of this statement or is this a slightly deeper result? This was a result discussed in my analysis lecture.",,['real-analysis']
75,Folland's ''Real Analysis'' Exercise 3.17: Missing hypothesis?,Folland's ''Real Analysis'' Exercise 3.17: Missing hypothesis?,,"The following exercise is 3.17 in Folland's Real Analysis, Second Edition: ''Let $(X, \scr{M}, \mu)$ be a $\sigma$-finite measure space, $\scr{N}$ a sub-$\sigma$-algebra of $\scr{M}$, and $\nu = \mu|\scr{N}$.  If $f \in L^1(\mu)$, there exists $g \in L^1(\nu)$ (thus $g$ is $\scr{N}$-measurable) such that $\int_E f d\mu = \int_E g d\nu$ for all $E \in \scr{N}$; if $g'$ is another such function then $g = g'$ $\nu$-a.e.  (In probability theory, $g$ is called the conditional expectation of $f$ on $\scr{N}$.)'' My question is this :  Is Folland missing a hypothesis that $\nu$ is $\sigma$-finite?  This is not necessarily true even if $\mu$ is $\sigma$-finite.  For example, if $\mu(X) = \infty$ and $\scr{N} = \{X, \emptyset\}$, then $\nu$ is not $\sigma$-finite. The obvious way to go about solving this exercise is to define a measure $\lambda$ on $(X, \scr{N})$ by $\lambda(E) = \int_E f d\mu$.  Then $\lambda <\!\!< \nu$.  So if $\nu$ is assumed to be $\sigma$-finite , one may apply the Radon-Nikodym theorem in the form of Exercise 3.14, which I quote here, to obtain the desired result (using Exercise 3.14 eliminates the need to worry about whether $\lambda$ is $\sigma$-finite): ''If $\nu$ is an arbitrary signed measure and $\mu$ is a $\sigma$-finite measure on $(X, \scr{M})$ such that $\nu <\!\!< \mu$, there exists an extended $\mu$-integrable function $f \colon X \to [-\infty, \infty]$ such that $d\nu = f d\mu$.'' Of course, in my proof of Exercise 3.17, I am making the replacements $\nu \to \lambda$, $\mu \to \nu$, and $\scr{M} \to \scr{N}$ in the statement of Exercise 3.14. Thanks for your help.","The following exercise is 3.17 in Folland's Real Analysis, Second Edition: ''Let $(X, \scr{M}, \mu)$ be a $\sigma$-finite measure space, $\scr{N}$ a sub-$\sigma$-algebra of $\scr{M}$, and $\nu = \mu|\scr{N}$.  If $f \in L^1(\mu)$, there exists $g \in L^1(\nu)$ (thus $g$ is $\scr{N}$-measurable) such that $\int_E f d\mu = \int_E g d\nu$ for all $E \in \scr{N}$; if $g'$ is another such function then $g = g'$ $\nu$-a.e.  (In probability theory, $g$ is called the conditional expectation of $f$ on $\scr{N}$.)'' My question is this :  Is Folland missing a hypothesis that $\nu$ is $\sigma$-finite?  This is not necessarily true even if $\mu$ is $\sigma$-finite.  For example, if $\mu(X) = \infty$ and $\scr{N} = \{X, \emptyset\}$, then $\nu$ is not $\sigma$-finite. The obvious way to go about solving this exercise is to define a measure $\lambda$ on $(X, \scr{N})$ by $\lambda(E) = \int_E f d\mu$.  Then $\lambda <\!\!< \nu$.  So if $\nu$ is assumed to be $\sigma$-finite , one may apply the Radon-Nikodym theorem in the form of Exercise 3.14, which I quote here, to obtain the desired result (using Exercise 3.14 eliminates the need to worry about whether $\lambda$ is $\sigma$-finite): ''If $\nu$ is an arbitrary signed measure and $\mu$ is a $\sigma$-finite measure on $(X, \scr{M})$ such that $\nu <\!\!< \mu$, there exists an extended $\mu$-integrable function $f \colon X \to [-\infty, \infty]$ such that $d\nu = f d\mu$.'' Of course, in my proof of Exercise 3.17, I am making the replacements $\nu \to \lambda$, $\mu \to \nu$, and $\scr{M} \to \scr{N}$ in the statement of Exercise 3.14. Thanks for your help.",,"['real-analysis', 'measure-theory', 'probability-theory']"
76,Supremum of a Continuous Function is Continuous,Supremum of a Continuous Function is Continuous,,"I'm working on this problem from Elementary Analysis by Ross which is intuitive when sketched but keeps stymieing me when I try to write it out. Let $f$ be a continuous function on $[a,b] \subset \mathbb{R}$. Define $f^\star (x)$ as: $$ f^\star(x) = \sup \{f(y)\mid a \leq y \leq x \} $$ Prove that $f^\star$ is a continuous increasing function on $[a,b]$. Things I've figured out: (these are mostly trivial) $f$ is uniformly continuous on $[a,b]$. Since $[a,b]$ is closed and bounded there exists some $c \in [a,b]$ such that $f(c) \geq f(x)\ \forall x \in [a,b]$. (In words: the supremum is actually attained.) If $c \in [a,b]$ is as above, then $f^\star$ is constant (and hence continuous) on $[c,b]$ (which is possibly a singleton). This seems completely obvious when you actually draw a continuous function but translating that to a formal proof eludes me...","I'm working on this problem from Elementary Analysis by Ross which is intuitive when sketched but keeps stymieing me when I try to write it out. Let $f$ be a continuous function on $[a,b] \subset \mathbb{R}$. Define $f^\star (x)$ as: $$ f^\star(x) = \sup \{f(y)\mid a \leq y \leq x \} $$ Prove that $f^\star$ is a continuous increasing function on $[a,b]$. Things I've figured out: (these are mostly trivial) $f$ is uniformly continuous on $[a,b]$. Since $[a,b]$ is closed and bounded there exists some $c \in [a,b]$ such that $f(c) \geq f(x)\ \forall x \in [a,b]$. (In words: the supremum is actually attained.) If $c \in [a,b]$ is as above, then $f^\star$ is constant (and hence continuous) on $[c,b]$ (which is possibly a singleton). This seems completely obvious when you actually draw a continuous function but translating that to a formal proof eludes me...",,['real-analysis']
77,"When is a vector space of polynomials dense in $C([0,1])$?",When is a vector space of polynomials dense in ?,"C([0,1])","Weierstrass' theorem states, in particular, that the set of polynomials with real coefficients is dense (with the supremum norm) in the set of continuous function on $[0,1]$. Using the Stone-Weierstrass theorem, one can specialize further: the set of polynomials in which each monomial is of even degree (e.g., $x^6+3x^4+7$ is one, but $x^2+x$ is not) satisfied the conditions, and is once again dense in $C([0,1])$. Moving further, the algebra generated by $x^n$ and $1$ is again dense for the same reasons. But how sparse can the dense sets be? How important is the algebra condition? Given a subset $S \subset \Bbb N$, when is the vector space $P_S$ generated by $\{x^n : n \in S\}$ dense in $C([0,1])$? In particular, what if $S$ is the odd positive integers (and 0)? Of course, if S contains all the multiples of some $n$, $P_S$ is dense by the argument above; and if it's finite, $P_S$ is not dense. But I'm unable to see anything else.","Weierstrass' theorem states, in particular, that the set of polynomials with real coefficients is dense (with the supremum norm) in the set of continuous function on $[0,1]$. Using the Stone-Weierstrass theorem, one can specialize further: the set of polynomials in which each monomial is of even degree (e.g., $x^6+3x^4+7$ is one, but $x^2+x$ is not) satisfied the conditions, and is once again dense in $C([0,1])$. Moving further, the algebra generated by $x^n$ and $1$ is again dense for the same reasons. But how sparse can the dense sets be? How important is the algebra condition? Given a subset $S \subset \Bbb N$, when is the vector space $P_S$ generated by $\{x^n : n \in S\}$ dense in $C([0,1])$? In particular, what if $S$ is the odd positive integers (and 0)? Of course, if S contains all the multiples of some $n$, $P_S$ is dense by the argument above; and if it's finite, $P_S$ is not dense. But I'm unable to see anything else.",,['real-analysis']
78,limits using $ \epsilon - \delta $ to prove two variable function,limits using  to prove two variable function, \epsilon - \delta ,"I'm trying to use the $ \epsilon - \delta $ argument to prove $\lim_{(x,y) \rightarrow (1.1)}  \frac{2xy}{x^2+y^2} =1$. I know that I need to show that  $\forall  \epsilon>0,  \exists \delta>0$ s.t. for all (x,y) in the domain of f,  $| \frac{2xy}{x^2+y^2} -1| <  \epsilon$  whenever $0<  \sqrt{(x-1)^2 + (y-1)^2} <\delta$. I've made $| \frac{2xy}{x^2+y^2} -1|$ to $\frac{(x-y)^2}{x^2 + y^2} $, but I can't get it to look like $\sqrt{(x-1)^2 + (y-1)^2}$..... what should I do from now? :(","I'm trying to use the $ \epsilon - \delta $ argument to prove $\lim_{(x,y) \rightarrow (1.1)}  \frac{2xy}{x^2+y^2} =1$. I know that I need to show that  $\forall  \epsilon>0,  \exists \delta>0$ s.t. for all (x,y) in the domain of f,  $| \frac{2xy}{x^2+y^2} -1| <  \epsilon$  whenever $0<  \sqrt{(x-1)^2 + (y-1)^2} <\delta$. I've made $| \frac{2xy}{x^2+y^2} -1|$ to $\frac{(x-y)^2}{x^2 + y^2} $, but I can't get it to look like $\sqrt{(x-1)^2 + (y-1)^2}$..... what should I do from now? :(",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus', 'epsilon-delta']"
79,Is there a generalization of the fundamental theorem of algebra for power series?,Is there a generalization of the fundamental theorem of algebra for power series?,,"Given the similarity between polynomials and power series, I was wondering if there is any generalization of the fundamental theorem of algebra for power series. I understand that it doesn't make much sense to talk of multiplicity when the roots are supposed to be infinite, but maybe there is something like this :P","Given the similarity between polynomials and power series, I was wondering if there is any generalization of the fundamental theorem of algebra for power series. I understand that it doesn't make much sense to talk of multiplicity when the roots are supposed to be infinite, but maybe there is something like this :P",,"['calculus', 'real-analysis', 'power-series']"
80,Show annihilator is closed subspace of dual space.,Show annihilator is closed subspace of dual space.,,"Let $X$ be normed vector space and $M$ be subspace. Let $M^{0} = \{ \lambda \in X^{*} : \lambda(x)= 0, \forall x \in M\}$, where $X^{*}$ is dual space of $X$. I want to show that $M^{0}$ is closed subspace. How can do I show that? I'm tried to show that let $\{\lambda_{n}\}_{n=1}^{\infty}$ be cauchy sequence and it converges to some element in $M^{0}$, but failed. Anyone give me some hint? Update : I think I found the answer. For fixed $x \in i(M)$, where $i: M \to M^{**}$ by natural map, intersection of all $x^{-1}(\{0 \})$ is $M^{0}$. Hence it is closed. Is it right?","Let $X$ be normed vector space and $M$ be subspace. Let $M^{0} = \{ \lambda \in X^{*} : \lambda(x)= 0, \forall x \in M\}$, where $X^{*}$ is dual space of $X$. I want to show that $M^{0}$ is closed subspace. How can do I show that? I'm tried to show that let $\{\lambda_{n}\}_{n=1}^{\infty}$ be cauchy sequence and it converges to some element in $M^{0}$, but failed. Anyone give me some hint? Update : I think I found the answer. For fixed $x \in i(M)$, where $i: M \to M^{**}$ by natural map, intersection of all $x^{-1}(\{0 \})$ is $M^{0}$. Hence it is closed. Is it right?",,"['real-analysis', 'functional-analysis']"
81,"homomorphism $f: \mathbb{C}^* \rightarrow \mathbb{R}^*$ with multiplicative groups, prove that kernel of $f$ is infinite.","homomorphism  with multiplicative groups, prove that kernel of  is infinite.",f: \mathbb{C}^* \rightarrow \mathbb{R}^* f,Let $f: \mathbb{C}^* \rightarrow \mathbb{R}^*$ be a homomorphism of the multiplicative group of complex numbers to the multiplicative group of real numbers. I need to show that the kernel of $f$ must be infinite. I do know that $\mathbb{C}^*$ and $\mathbb{R}^*$ are not isomorphic to each other from here . So does that mean $f$ is not onto? But how will I be able to show that the kernel is infinite? Thanks in advance.,Let $f: \mathbb{C}^* \rightarrow \mathbb{R}^*$ be a homomorphism of the multiplicative group of complex numbers to the multiplicative group of real numbers. I need to show that the kernel of $f$ must be infinite. I do know that $\mathbb{C}^*$ and $\mathbb{R}^*$ are not isomorphic to each other from here . So does that mean $f$ is not onto? But how will I be able to show that the kernel is infinite? Thanks in advance.,,"['real-analysis', 'group-theory', 'complex-analysis', 'abstract-algebra']"
82,"Continuous $f$ satisfying $f(2x)=f(x-1/4)+f(x+1/4)$ on $(-1/2,1/2)$",Continuous  satisfying  on,"f f(2x)=f(x-1/4)+f(x+1/4) (-1/2,1/2)","What are the continuous functions $f\colon (-\frac{1}{2},\frac{1}{2}) \to \mathbb{C}$ that satisfy the following functional equation, and how are they derived? $$f(2x)=f(x-\frac{1}{4})+f(x+\frac{1}{4})\,\,\,\,\,\,\,\,\text{for }x \in (-\frac{1}{4},\frac{1}{4})$$ I think this could be interesting because in addition to the obvious solutions $f(x)=zx$, this is also satisfied by $f(x)=\ln (2\cos(\pi x))$.","What are the continuous functions $f\colon (-\frac{1}{2},\frac{1}{2}) \to \mathbb{C}$ that satisfy the following functional equation, and how are they derived? $$f(2x)=f(x-\frac{1}{4})+f(x+\frac{1}{4})\,\,\,\,\,\,\,\,\text{for }x \in (-\frac{1}{4},\frac{1}{4})$$ I think this could be interesting because in addition to the obvious solutions $f(x)=zx$, this is also satisfied by $f(x)=\ln (2\cos(\pi x))$.",,"['real-analysis', 'functions', 'functional-equations']"
83,Showing convergence of a series,Showing convergence of a series,,"Let $a_0,a_1\in\mathbb{R}$,$a_n=a_{n-1}-\cfrac{2}{n}a_{n-2}$ for $n\ge2$. How to show $\sum_0^\infty|a_n|<\infty$","Let $a_0,a_1\in\mathbb{R}$,$a_n=a_{n-1}-\cfrac{2}{n}a_{n-2}$ for $n\ge2$. How to show $\sum_0^\infty|a_n|<\infty$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
84,A continuous mapping is determined by its values on a dense set,A continuous mapping is determined by its values on a dense set,,"Let f and g be continuous mappings of a metric space $X$ into a metric space $Y$ and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p)=f(p)$ for all $p \in E$, prove that $g(p)=f(p)$ for all $p \in X$.","Let f and g be continuous mappings of a metric space $X$ into a metric space $Y$ and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p)=f(p)$ for all $p \in E$, prove that $g(p)=f(p)$ for all $p \in X$.",,"['real-analysis', 'general-topology', 'continuity']"
85,The measure of the diagonal of a unit square in an alternative measure.,The measure of the diagonal of a unit square in an alternative measure.,,"Usually, we say that the measure of the diagonal of a unit square is 0, but that's with the preassumption the measure is Lebesgue measure in $\mathbb{R}^2$. But what if we are talking about a strange measure where its the product of a 1-dimensional Lebesgue measure and a counting measure? To be precise: Let $X=Y=[0,1]$, $\mathcal{M}=\mathcal{N}=\mathcal{B}_{[0,1]}$, $\mu=$Lebesgue measure, and $\nu=$counting measure. If $D=\{(x,x):x\in[0,1]\}$ is the diagonal in $X\times Y$, then what is $(\mu\times\nu)(D)$? According to the definition, it should be the infimum of the measure sum of all the possible ""rectangle"" cover of $D$. But in this measure $\mu\times\nu$, the only rectangles that have finite measures are those with either finite ""height"", or with Lebesgue-null ""width"", but clearly you cannot cover the diagonal with those thin strips (or rather 1-d line segments in the former case). Does that mean this diagonal has a measure $\infty$?","Usually, we say that the measure of the diagonal of a unit square is 0, but that's with the preassumption the measure is Lebesgue measure in $\mathbb{R}^2$. But what if we are talking about a strange measure where its the product of a 1-dimensional Lebesgue measure and a counting measure? To be precise: Let $X=Y=[0,1]$, $\mathcal{M}=\mathcal{N}=\mathcal{B}_{[0,1]}$, $\mu=$Lebesgue measure, and $\nu=$counting measure. If $D=\{(x,x):x\in[0,1]\}$ is the diagonal in $X\times Y$, then what is $(\mu\times\nu)(D)$? According to the definition, it should be the infimum of the measure sum of all the possible ""rectangle"" cover of $D$. But in this measure $\mu\times\nu$, the only rectangles that have finite measures are those with either finite ""height"", or with Lebesgue-null ""width"", but clearly you cannot cover the diagonal with those thin strips (or rather 1-d line segments in the former case). Does that mean this diagonal has a measure $\infty$?",,['real-analysis']
86,"Is $(-\infty,\infty)$ a closed **interval**?",Is  a closed **interval**?,"(-\infty,\infty)","Note that we are working in the reals, not the extended reals. Now consider two opposing claims: $(-\infty,\infty)$ is a closed interval , because a closed interval is an interval that is a closed set. $(-\infty,\infty)$ is not a closed interval , because a closed interval is an interval that includes both its endpoints. Recently, I was very surprised to be informed by several mathematicians that $(-\infty,\infty)$ ought to be called a closed interval, not an open interval. Perhaps I could get some consensus from this community, if possible. p.s. To be very clear, I am not asking if $(-\infty,\infty)$ is a closed set —it sure is. Update For what it's worth, the ISO 80000-2:2009 document explicitly calls $(a,b)$ an open interval and $[a,b]$ a closed interval.","Note that we are working in the reals, not the extended reals. Now consider two opposing claims: is a closed interval , because a closed interval is an interval that is a closed set. is not a closed interval , because a closed interval is an interval that includes both its endpoints. Recently, I was very surprised to be informed by several mathematicians that ought to be called a closed interval, not an open interval. Perhaps I could get some consensus from this community, if possible. p.s. To be very clear, I am not asking if is a closed set —it sure is. Update For what it's worth, the ISO 80000-2:2009 document explicitly calls an open interval and a closed interval.","(-\infty,\infty) (-\infty,\infty) (-\infty,\infty) (-\infty,\infty) (a,b) [a,b]","['real-analysis', 'analysis', 'soft-question', 'terminology']"
87,$\mathbb{R}^n\times\{0\}$ has measure zero in $\mathbb{R}^{n+1}$,has measure zero in,\mathbb{R}^n\times\{0\} \mathbb{R}^{n+1},"I want to show that $\mathbb{R}^n\times\{0\}$ has measure zero in $\mathbb{R}^{n+1}$. For example, take $n=1$. I want to show that the $x$-axis has measure zero in the plane. I cover it with the sets $[-1,1]\times[-\epsilon/8,\epsilon/8]$, $[-2,2]\times[-\epsilon/32,\epsilon/32]$, $\ldots$ The goal is to have the volumes be $\epsilon/2, \epsilon/4, \ldots$ so that they sum to $\epsilon$. I think this method generalizes to $\mathbb{R}^n$, simply by choosing $[-1,1]^{n-1}\times[-\epsilon/2^{n+2},\epsilon/2^{n+2}],\ldots$. I don't think it is very elegant though. Is there a ""better"" way to do this?","I want to show that $\mathbb{R}^n\times\{0\}$ has measure zero in $\mathbb{R}^{n+1}$. For example, take $n=1$. I want to show that the $x$-axis has measure zero in the plane. I cover it with the sets $[-1,1]\times[-\epsilon/8,\epsilon/8]$, $[-2,2]\times[-\epsilon/32,\epsilon/32]$, $\ldots$ The goal is to have the volumes be $\epsilon/2, \epsilon/4, \ldots$ so that they sum to $\epsilon$. I think this method generalizes to $\mathbb{R}^n$, simply by choosing $[-1,1]^{n-1}\times[-\epsilon/2^{n+2},\epsilon/2^{n+2}],\ldots$. I don't think it is very elegant though. Is there a ""better"" way to do this?",,"['real-analysis', 'measure-theory']"
88,"General case integral $\int^1_0 f(x) \, f'(1-x) \, dx $",General case integral,"\int^1_0 f(x) \, f'(1-x) \, dx ","Suppose we get the following general case $$\int^1_0 \, f(x) \, f'(1-x) \, dx $$ where $f$ is differentiable on $[0,1]$ . Can we have a general formula for the integral ? Of cousre we can generalize for $$\int^a_0 \, f(x) \, f'(a-x) \, dx $$ Interesting examples would be $$\int^{\frac{\pi}{2}}_0 \sin^n(x) dx $$ Which is easily found by beta function or integration be parts .","Suppose we get the following general case $$\int^1_0 \, f(x) \, f'(1-x) \, dx $$ where $f$ is differentiable on $[0,1]$ . Can we have a general formula for the integral ? Of cousre we can generalize for $$\int^a_0 \, f(x) \, f'(a-x) \, dx $$ Interesting examples would be $$\int^{\frac{\pi}{2}}_0 \sin^n(x) dx $$ Which is easily found by beta function or integration be parts .",,"['calculus', 'real-analysis', 'integration']"
89,"Show that if the sequence$(x_n)$ is bounded, then $(x_n)$ converges iff $\lim\sup(x_n)=\lim\inf(x_n)$.","Show that if the sequence is bounded, then  converges iff .",(x_n) (x_n) \lim\sup(x_n)=\lim\inf(x_n),"Show that if the sequence$(x_n)$ is bounded, then $(x_n)$ converges iff $\lim\sup_{x\to\infty}(x_n)=\lim\inf_{x\to\infty}(x_n)$. The definitions that I’m using: $$\begin{align*} &\liminf_{n\to\infty}x_n=\lim_{n\to\infty}\inf_{m\ge n}x_m\\ &\limsup_{n\to\infty}x_n=\lim_{n\to\infty}\sup_{m\ge n}x_m \end{align*}$$ This’s the first time I deal with $\lim\inf$ things, can someone give me help? Thank you.","Show that if the sequence$(x_n)$ is bounded, then $(x_n)$ converges iff $\lim\sup_{x\to\infty}(x_n)=\lim\inf_{x\to\infty}(x_n)$. The definitions that I’m using: $$\begin{align*} &\liminf_{n\to\infty}x_n=\lim_{n\to\infty}\inf_{m\ge n}x_m\\ &\limsup_{n\to\infty}x_n=\lim_{n\to\infty}\sup_{m\ge n}x_m \end{align*}$$ This’s the first time I deal with $\lim\inf$ things, can someone give me help? Thank you.",,"['real-analysis', 'limits', 'limsup-and-liminf']"
90,analysis problem proof with derivative,analysis problem proof with derivative,,"$f:[a,b] \to \mathbb R$ is a continuous function and $0 < a < b$ and $f$ is differentiable in $(a,b)$ and $\dfrac{f(a)}{a} = \dfrac{f(b)}{b}$. Prove that there exists $x \in (a,b)$ so that $xf'(x) = f(x)$.","$f:[a,b] \to \mathbb R$ is a continuous function and $0 < a < b$ and $f$ is differentiable in $(a,b)$ and $\dfrac{f(a)}{a} = \dfrac{f(b)}{b}$. Prove that there exists $x \in (a,b)$ so that $xf'(x) = f(x)$.",,"['real-analysis', 'analysis']"
91,Wikipedia's definition of isolated point.,Wikipedia's definition of isolated point.,,"Wikipedia defines an isolated point of a subset $S \subseteq X$ to be a point $x \in S$ such that there exists a neighborhood $U$ of $x$ not containing any other points of $S$.  Furthermore, it claims that this is equivalent to saying $\{x\}$ is open in $X$. Question: How is the last sentence true?  This seems to be false since for example $1$  is an isolated point of $\{1\} \cup (3, 4)$, but $\{1\} \subseteq \mathbb{R}$ is not open.","Wikipedia defines an isolated point of a subset $S \subseteq X$ to be a point $x \in S$ such that there exists a neighborhood $U$ of $x$ not containing any other points of $S$.  Furthermore, it claims that this is equivalent to saying $\{x\}$ is open in $X$. Question: How is the last sentence true?  This seems to be false since for example $1$  is an isolated point of $\{1\} \cup (3, 4)$, but $\{1\} \subseteq \mathbb{R}$ is not open.",,"['real-analysis', 'general-topology']"
92,Intermediate value property and closedness of rational level sets implies continuity,Intermediate value property and closedness of rational level sets implies continuity,,"Suppose $f$ satisfies the intermediate value property, i.e. if $f(a)<c<f(b)$, then there exists $a<x<b$ such that $f(x)=c$ and for every rational $r$, $S_r$ such that $f(x)=r$ is a closed set. Prove $f$ is continuous. This looks pretty daunting. I am guessing it uses some sort of sequential continuity argument, but I am somewhat lost. Hints would be most appreciated.","Suppose $f$ satisfies the intermediate value property, i.e. if $f(a)<c<f(b)$, then there exists $a<x<b$ such that $f(x)=c$ and for every rational $r$, $S_r$ such that $f(x)=r$ is a closed set. Prove $f$ is continuous. This looks pretty daunting. I am guessing it uses some sort of sequential continuity argument, but I am somewhat lost. Hints would be most appreciated.",,"['real-analysis', 'general-topology', 'analysis', 'continuity']"
93,Find a smooth function $f:\mathbb{R}\to\mathbb{R}$ such that $|f'(x)| < 1$ and $f(x) \neq x$ for all $x\in\mathbb{R}$,Find a smooth function  such that  and  for all,f:\mathbb{R}\to\mathbb{R} |f'(x)| < 1 f(x) \neq x x\in\mathbb{R},"Exercise : Find a smooth function $f:\mathbb{R}\to\mathbb{R}$ such that $|f'(x)| < 1$ and $f(x) \neq x$ for all $x\in\mathbb{R}$ I got this exercise from the book ""Curso de Análise: volume 1"", by Elon Lages Lima. (In Portuguese). My attempts include $1$) integrate $\frac{2\text{arctan}(x)}{\pi}$, but I get this . (Adding larger constants doesn't seem to help.) $2$) $f(x) = \text{sin}(x/2) + x + 2$ but its derivative gets too large. Any ideas?","Exercise : Find a smooth function $f:\mathbb{R}\to\mathbb{R}$ such that $|f'(x)| < 1$ and $f(x) \neq x$ for all $x\in\mathbb{R}$ I got this exercise from the book ""Curso de Análise: volume 1"", by Elon Lages Lima. (In Portuguese). My attempts include $1$) integrate $\frac{2\text{arctan}(x)}{\pi}$, but I get this . (Adding larger constants doesn't seem to help.) $2$) $f(x) = \text{sin}(x/2) + x + 2$ but its derivative gets too large. Any ideas?",,"['real-analysis', 'derivatives']"
94,Young's inequality without using convexity,Young's inequality without using convexity,,"I was doing some problems from Rudin's Principles of Mathematical Analysis and came across a problem in which he asks you to prove Hölder's inequality via Young's inequality: If $u$ and $v$ are nonnegative real numbers, and $p$ and $q$ are positive real numbers such that $\displaystyle \frac{1}{p}+\frac{1}{q}=1$, then $\displaystyle uv \leq \frac{1}{p}u^p+\frac{1}{q}v^q$. I'm familiar with the proof using convexity of the $\log$ function and Jensen's inequality, but Rudin hasn't defined the $\log$ function by chapter $6$ (where this problem originates) and hasn't done anything with convexity. Usually he gives everything necessary for a problem before he poses one, so this seems to be something of an omission. Perhaps he wants us to read Chapter $8$ to learn about $\log$ and prove Jensen's inequality before attacking this problem? But then why put it in Chapter $6$? My question: is there a proof of Young's inequality that does not use convexity of $\log$ or something similar? If one exists, can it be done using only the material from chapters $1-6$ of Principles ? (For clarity, chapter 1-6 essentially cover the real number system, metric space topology, sequences and series, continuity, differentiability, and the Riemann-Stieltjes integral.)","I was doing some problems from Rudin's Principles of Mathematical Analysis and came across a problem in which he asks you to prove Hölder's inequality via Young's inequality: If $u$ and $v$ are nonnegative real numbers, and $p$ and $q$ are positive real numbers such that $\displaystyle \frac{1}{p}+\frac{1}{q}=1$, then $\displaystyle uv \leq \frac{1}{p}u^p+\frac{1}{q}v^q$. I'm familiar with the proof using convexity of the $\log$ function and Jensen's inequality, but Rudin hasn't defined the $\log$ function by chapter $6$ (where this problem originates) and hasn't done anything with convexity. Usually he gives everything necessary for a problem before he poses one, so this seems to be something of an omission. Perhaps he wants us to read Chapter $8$ to learn about $\log$ and prove Jensen's inequality before attacking this problem? But then why put it in Chapter $6$? My question: is there a proof of Young's inequality that does not use convexity of $\log$ or something similar? If one exists, can it be done using only the material from chapters $1-6$ of Principles ? (For clarity, chapter 1-6 essentially cover the real number system, metric space topology, sequences and series, continuity, differentiability, and the Riemann-Stieltjes integral.)",,"['real-analysis', 'inequality', 'young-inequality']"
95,graph is dense in $\mathbb{R}^2$,graph is dense in,\mathbb{R}^2,"I was asked in a exam: does there exist a function(need not be continous) $f:\mathbb{R}\rightarrow \mathbb{R}$ whose graph is dense in $\mathbb{R}^2$? I proved that graph of a discontinuous linear map is dense but did not provide explicit example, could any one give me one such? thank you","I was asked in a exam: does there exist a function(need not be continous) $f:\mathbb{R}\rightarrow \mathbb{R}$ whose graph is dense in $\mathbb{R}^2$? I proved that graph of a discontinuous linear map is dense but did not provide explicit example, could any one give me one such? thank you",,['real-analysis']
96,Prove that $\int_0^1[f''(x)]^2dx\ge4.$,Prove that,\int_0^1[f''(x)]^2dx\ge4.,"Let $f$ be a $C^2$ function on $[0,1]$ such that $f(0)=f(1)=f'(0)=0,f'(1)=1.$ Prove that $$\int_0^1[f''(x)]^2dx\ge4.$$ Find all $f$ for equality to occur.",Let be a function on such that Prove that Find all for equality to occur.,"f C^2 [0,1] f(0)=f(1)=f'(0)=0,f'(1)=1. \int_0^1[f''(x)]^2dx\ge4. f","['real-analysis', 'calculus-of-variations']"
97,Proof that operator is compact,Proof that operator is compact,,"Prove that the operator $T:\ell^1\rightarrow\ell^1$ which maps $x=(x_1,x_2,\dots)$ to $\left(x_1,\frac{x_2}{2},\frac{x_3}{3},\dots\right)$ is compact. For an arbitrary sequence $x^{(N)}\in\ell^1$ one would have extract a convergent subsequence of $T x^{(N)}$. Maybe via the diagonal argument?","Prove that the operator $T:\ell^1\rightarrow\ell^1$ which maps $x=(x_1,x_2,\dots)$ to $\left(x_1,\frac{x_2}{2},\frac{x_3}{3},\dots\right)$ is compact. For an arbitrary sequence $x^{(N)}\in\ell^1$ one would have extract a convergent subsequence of $T x^{(N)}$. Maybe via the diagonal argument?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'compact-operators']"
98,Changing the order of $\lim$ and $\sup$,Changing the order of  and,\lim \sup,"Suppose that $f_n:X\to [0,1]$ where $X$ is some arbitrary set. Suppose that  $$   f_n(x)\geq f_{n+1}(x) $$ for all $x\in X$ and all $n = 0,1,2,\dots$ so there exists $\lim_n f_n(x)$ point-wise, let's call it $f(x)$. Define $f^*_n:=\sup\limits_{x\in X}f_n(x)$, $f^*:=\sup\limits_{x\in X}f(x)$ and $\hat f:= \lim\limits_n f^*_n$. I wonder when $f^* = \hat f$, i.e. $$   \lim\limits_n \sup\limits_{x\in X}f_n(x) = \sup\limits_{x\in X}\lim\limits_n f_n(x). $$ I was googling the topic, but strangely have not found any information, strangely because I expected it to be available as for changing the order of limits or of integration. Some simple facts: $\hat f\geq f^*$ and the reverse is true at least when $f_n$ converges uniformly to $f$. This does not hold in general, e.g. when $f_n = 1_{[n,\infty)}$. I would appreciate any other ideas that you can advise me. Also related to this . A similar question was asked here .","Suppose that $f_n:X\to [0,1]$ where $X$ is some arbitrary set. Suppose that  $$   f_n(x)\geq f_{n+1}(x) $$ for all $x\in X$ and all $n = 0,1,2,\dots$ so there exists $\lim_n f_n(x)$ point-wise, let's call it $f(x)$. Define $f^*_n:=\sup\limits_{x\in X}f_n(x)$, $f^*:=\sup\limits_{x\in X}f(x)$ and $\hat f:= \lim\limits_n f^*_n$. I wonder when $f^* = \hat f$, i.e. $$   \lim\limits_n \sup\limits_{x\in X}f_n(x) = \sup\limits_{x\in X}\lim\limits_n f_n(x). $$ I was googling the topic, but strangely have not found any information, strangely because I expected it to be available as for changing the order of limits or of integration. Some simple facts: $\hat f\geq f^*$ and the reverse is true at least when $f_n$ converges uniformly to $f$. This does not hold in general, e.g. when $f_n = 1_{[n,\infty)}$. I would appreciate any other ideas that you can advise me. Also related to this . A similar question was asked here .",,['real-analysis']
99,"If $f$ is positive and increasing, then $f'(x)x^2$ is increasing?","If  is positive and increasing, then  is increasing?",f f'(x)x^2,"If $f: (0, \infty)\to (0, \infty)$ is increasing, is it true that the function $x\longmapsto f'(x) \cdot x^2 $ is increasing? We can assume that $f$ is twice differentiable. Can someone provide a counter-example, a function $f$ which is increasing and positive, but $f'(a)\cdot a^2 < f'(b)\cdot b^2$, for some $ a>b $ ?","If $f: (0, \infty)\to (0, \infty)$ is increasing, is it true that the function $x\longmapsto f'(x) \cdot x^2 $ is increasing? We can assume that $f$ is twice differentiable. Can someone provide a counter-example, a function $f$ which is increasing and positive, but $f'(a)\cdot a^2 < f'(b)\cdot b^2$, for some $ a>b $ ?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
