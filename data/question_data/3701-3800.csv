,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Closed form of $\int_{0}^{\pi/2}x\cot\left(x\right)\cos\left(x\right)\log\left(\sin\left(x\right)\right)dx$,Closed form of,\int_{0}^{\pi/2}x\cot\left(x\right)\cos\left(x\right)\log\left(\sin\left(x\right)\right)dx,I would like to know if there exists a closed form for this integral $$\int_{0}^{\pi/2}x\cot\left(x\right)\cos\left(x\right)\log\left(\sin\left(x\right)\right)dx.$$ I tried the relation $$\log\left(\sin\left(x\right)\right)=-\log\left(2\right)-\sum_{n=1}^{\infty}\frac{\cos\left(2nx\right)}{n}$$ but it seems useless.,I would like to know if there exists a closed form for this integral $$\int_{0}^{\pi/2}x\cot\left(x\right)\cos\left(x\right)\log\left(\sin\left(x\right)\right)dx.$$ I tried the relation $$\log\left(\sin\left(x\right)\right)=-\log\left(2\right)-\sum_{n=1}^{\infty}\frac{\cos\left(2nx\right)}{n}$$ but it seems useless.,,"['calculus', 'real-analysis', 'integration', 'closed-form']"
1,Real Analysis : Self Studying vs Doing a Course,Real Analysis : Self Studying vs Doing a Course,,"I am an engineering graduate student. Recently I got interested in studying Maths. So, I have started self-studying Real Analysis(let's call it RA) using a few books. I will also be using problem books to supplement my learning. I told my adviser that I'd be devoting a certain amount of time in studying Maths, apart from the time that I devote to my engineering thesis. He told me that self-studying RA would not be a good option. RA is something which requires the discipline of a coursework. You might get some feel of it but the rigorousness can only be achieved through a course. He said that pure mathematics is not something like programming in which you can become master by self-studying. I did not debate him on this. But I wanted to ask you this: If I am extremely dedicated to studying RA, devoting 4-5 hours daily and doing problems given in the textbook or a separate problem book, trying to understand every theorem/concept which the author says is important, and honestly attempting problems, will I be able to understand Real Analysis in the same manner as doing a course. My target is to learn RA up to the level of an Undergraduate Mathematics Major. Your answer should answer some or all of the following questions: Is it possible at all to achieve the same level of understanding through self-studying as compared to a course? What is the essential difference (the ""gist"") between self-studying and doing a coursework in RA? How much more time should it take in self-studying, comparatively? Can honestly attempting problems in the books be an alternative to exams and assignments? PS: I'm in my masters right now but will be going for PhD which will also be in Engineering. However, during PhD also I'm thinking of honing my Maths skills and might go for a separate major later on if I so desire. So, essentially I am hell-bent in understanding Mathematics which has intimidated me for so long. And as it is turning out, I am actually enjoying it thoroughly!! So that's that.","I am an engineering graduate student. Recently I got interested in studying Maths. So, I have started self-studying Real Analysis(let's call it RA) using a few books. I will also be using problem books to supplement my learning. I told my adviser that I'd be devoting a certain amount of time in studying Maths, apart from the time that I devote to my engineering thesis. He told me that self-studying RA would not be a good option. RA is something which requires the discipline of a coursework. You might get some feel of it but the rigorousness can only be achieved through a course. He said that pure mathematics is not something like programming in which you can become master by self-studying. I did not debate him on this. But I wanted to ask you this: If I am extremely dedicated to studying RA, devoting 4-5 hours daily and doing problems given in the textbook or a separate problem book, trying to understand every theorem/concept which the author says is important, and honestly attempting problems, will I be able to understand Real Analysis in the same manner as doing a course. My target is to learn RA up to the level of an Undergraduate Mathematics Major. Your answer should answer some or all of the following questions: Is it possible at all to achieve the same level of understanding through self-studying as compared to a course? What is the essential difference (the ""gist"") between self-studying and doing a coursework in RA? How much more time should it take in self-studying, comparatively? Can honestly attempting problems in the books be an alternative to exams and assignments? PS: I'm in my masters right now but will be going for PhD which will also be in Engineering. However, during PhD also I'm thinking of honing my Maths skills and might go for a separate major later on if I so desire. So, essentially I am hell-bent in understanding Mathematics which has intimidated me for so long. And as it is turning out, I am actually enjoying it thoroughly!! So that's that.",,"['real-analysis', 'analysis', 'self-learning']"
2,$g(x)$ is continuous on $\mathbb{R}$ st $g(x)=g(x^2)$. Prove that $g(x)$ is constant.,is continuous on  st . Prove that  is constant.,g(x) \mathbb{R} g(x)=g(x^2) g(x),"For $x>0$ , $g(\sqrt{x})=g(x)$ and similarly $g(x^{\frac{1}{2^n}})=g(\sqrt{x})=g(x)$ for $n \in \mathbb{N}$ Thus taking $\lim_{n\to \infty}$ both sides we get g(1)=g(x) $\forall x>0$ and as $g(x)=g(-x)$ thus $g(x)=g(1) \forall x\in \mathbb{R}$ thus $g(x)$ is constant. Is this proof correct?","For , and similarly for Thus taking both sides we get g(1)=g(x) and as thus thus is constant. Is this proof correct?",x>0 g(\sqrt{x})=g(x) g(x^{\frac{1}{2^n}})=g(\sqrt{x})=g(x) n \in \mathbb{N} \lim_{n\to \infty} \forall x>0 g(x)=g(-x) g(x)=g(1) \forall x\in \mathbb{R} g(x),"['real-analysis', 'continuity']"
3,Integral$\int_0^{\pi/4} \log \tan \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}=\pm\frac{\pi^2}{16}$,Integral,\int_0^{\pi/4} \log \tan \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}=\pm\frac{\pi^2}{16},"Hi I am trying to prove $$ \int_0^{\pi/4} \log \tan \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}=\pm\frac{\pi^2}{16}. $$ What an amazing result and a clever one this is.    I tried writing $$ \int_0^{\pi/4} \log \sin \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}-\int_0^{\pi/4} \log \cos \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}. $$ Changing variables $y=2x$ I obtained $$ \frac{1}{2}\int_0^{\pi/2} \log \sin \left(\frac{\pi}{4}\pm \frac{y}{2}\right)\frac{dy}{\tan y}-\frac{1}{2}\int_0^{\pi/2} \log \cos \left(\frac{\pi}{4}\pm \frac{y}{2}\right)\frac{dy}{\tan y}. $$ I would rather work with the log sine/cosines for $y\in [0,\pi/2]$ since we can use $\int_0^{\pi/2} \log \sin x dx=-\frac{\pi}{2} \ln 2.$  But I am stuck here.  Thanks","Hi I am trying to prove $$ \int_0^{\pi/4} \log \tan \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}=\pm\frac{\pi^2}{16}. $$ What an amazing result and a clever one this is.    I tried writing $$ \int_0^{\pi/4} \log \sin \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}-\int_0^{\pi/4} \log \cos \left(\frac{\pi}{4}\pm x\right)\frac{dx}{\tan 2x}. $$ Changing variables $y=2x$ I obtained $$ \frac{1}{2}\int_0^{\pi/2} \log \sin \left(\frac{\pi}{4}\pm \frac{y}{2}\right)\frac{dy}{\tan y}-\frac{1}{2}\int_0^{\pi/2} \log \cos \left(\frac{\pi}{4}\pm \frac{y}{2}\right)\frac{dy}{\tan y}. $$ I would rather work with the log sine/cosines for $y\in [0,\pi/2]$ since we can use $\int_0^{\pi/2} \log \sin x dx=-\frac{\pi}{2} \ln 2.$  But I am stuck here.  Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
4,"Prove that : $|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)}$ with $(a,b) \in \mathbb{R}^{2}$",Prove that :  with,"|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)} (a,b) \in \mathbb{R}^{2}","Let $(a,b) \in \mathbb{R}^{2}$ such that $a<b$ and $f\in C^2([a,b],\mathbb{R})$ such that   $f'\neq 0$ and $f''/f'$ is decreasing. Prove that : $$|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)}$$ My attempt: By the MVT theorem, we have to prove : $$ |f'(c)| \geq \sqrt{f'(a)f'(b)}, \quad c\in (a,b) $$ Without loss of generality we can assume that $f'$ is stricly positive. Thus, by the fact that $\ln(f')$ is concave we get  $$ \frac{\ln(f'(c))-\ln(f'(a))}{c-a}\geqslant\frac{\ln(f'(b)-\ln(f'(c))}{b-c} $$ So, $$ \frac{\ln \frac{f'(c)}{f'(a)}}{c-a}\geqslant\frac{\ln \frac{f'(b)}{f'(c)}}{b-c} $$ Notice that I haven't use  $\frac{f''(a)}{f'(a)}\geq \frac{f''(b)}{f'(b)}$, So I think what I am doing it's not enough. Any ideas? Thank you in advance for your help,","Let $(a,b) \in \mathbb{R}^{2}$ such that $a<b$ and $f\in C^2([a,b],\mathbb{R})$ such that   $f'\neq 0$ and $f''/f'$ is decreasing. Prove that : $$|f(b)-f(a)|\geqslant (b-a) \sqrt{f'(a) f'(b)}$$ My attempt: By the MVT theorem, we have to prove : $$ |f'(c)| \geq \sqrt{f'(a)f'(b)}, \quad c\in (a,b) $$ Without loss of generality we can assume that $f'$ is stricly positive. Thus, by the fact that $\ln(f')$ is concave we get  $$ \frac{\ln(f'(c))-\ln(f'(a))}{c-a}\geqslant\frac{\ln(f'(b)-\ln(f'(c))}{b-c} $$ So, $$ \frac{\ln \frac{f'(c)}{f'(a)}}{c-a}\geqslant\frac{\ln \frac{f'(b)}{f'(c)}}{b-c} $$ Notice that I haven't use  $\frac{f''(a)}{f'(a)}\geq \frac{f''(b)}{f'(b)}$, So I think what I am doing it's not enough. Any ideas? Thank you in advance for your help,",,['calculus']
5,$0<a_n<\frac{a_{n-1}+a_{n-2}}{2} \Longrightarrow a_n\ $ converges,converges,0<a_n<\frac{a_{n-1}+a_{n-2}}{2} \Longrightarrow a_n\ ,Let $a_n$ be a sequence of positive real numbers such that $$a_n<\frac{a_{n-1}+a_{n-2}}{2}$$ Show that $a_n$ converges.,Let $a_n$ be a sequence of positive real numbers such that $$a_n<\frac{a_{n-1}+a_{n-2}}{2}$$ Show that $a_n$ converges.,,"['real-analysis', 'sequences-and-series']"
6,Limit of maximum of $f_{n}(x)=\frac{1}{n}(\sin{x}+\sin{(2x)}+\cdots+\sin{(nx)})$,Limit of maximum of,f_{n}(x)=\frac{1}{n}(\sin{x}+\sin{(2x)}+\cdots+\sin{(nx)}),"let $$f_{n}(x)=\dfrac{1}{n}(\sin{x}+\sin{(2x)}+\sin{(3x)}+\cdots+\sin{(nx)}),x\in R,n\in N$$ let $$a_{n}=\max_{x\in R}{(f_{n}(x))}$$ Find this limit $$\lim_{n\to\infty}a_{n}$$ My try: since $$\sin{x}+\sin{(2x)}+\sin{(3x)}+\cdots+\sin{(nx)}=\dfrac{\sin{\dfrac{nx}{2}}\sin{\dfrac{(n+1)x}{2}}}{\sin{\dfrac{x}{2}}}$$ so $$f_{n}(x)=\dfrac{\sin{\dfrac{nx}{2}}\sin{\dfrac{(n+1)x}{2}}}{n\sin{\dfrac{x}{2}}}$$ so follow wa have find the $f_{n}(x)$ maximum.But I can't. and I can't find this limit.Thank you very much!","let $$f_{n}(x)=\dfrac{1}{n}(\sin{x}+\sin{(2x)}+\sin{(3x)}+\cdots+\sin{(nx)}),x\in R,n\in N$$ let $$a_{n}=\max_{x\in R}{(f_{n}(x))}$$ Find this limit $$\lim_{n\to\infty}a_{n}$$ My try: since $$\sin{x}+\sin{(2x)}+\sin{(3x)}+\cdots+\sin{(nx)}=\dfrac{\sin{\dfrac{nx}{2}}\sin{\dfrac{(n+1)x}{2}}}{\sin{\dfrac{x}{2}}}$$ so $$f_{n}(x)=\dfrac{\sin{\dfrac{nx}{2}}\sin{\dfrac{(n+1)x}{2}}}{n\sin{\dfrac{x}{2}}}$$ so follow wa have find the $f_{n}(x)$ maximum.But I can't. and I can't find this limit.Thank you very much!",,"['real-analysis', 'sequences-and-series', 'limits', 'fourier-analysis', 'trigonometric-series']"
7,Why does the volume of a hypersphere decrease in higher dimensions? [duplicate],Why does the volume of a hypersphere decrease in higher dimensions? [duplicate],,"This question already has answers here : Why does the volume of the unit sphere go to zero? (7 answers) Closed 10 years ago . First let us define an $n$-ball as the euclidean sphere in $\mathbb{R}^n$ including its interior and its surface where $n$ refers to the number of coordinates needed to describe the object (the geometer's notation), NOT the topologist's notation which refers to the dimension of the manifold. All $n$-balls considered here are of radius 1 centered at the origin. So an $n$-ball is the set of points $$\{x=(x_1,x_2,...,x_n)\in \mathbb{R}^n : \sum_{i=1}^n x_i^2 \leq 1\}.$$ It is well known that the volume of the unit $n$-ball is given by $$V(n)=\frac{\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}$$ and before anyone points it out, we will consider all volumes to be unit-less so that they can be compared with each other. So for example $$\pi=V(2)<V(3)=\frac{4}{3}\pi.$$ Now, my question is why does $V(n)\rightarrow 0$ as $n\rightarrow\infty$? This behavior is independent of the radius. Depending on the radius, $V(n)$ may (or may not) increase at first, hit a peak at some $n$, and then monotonically decrease and converge to zero. Considering $n\in\mathbb{N}$ to take on only discrete values, for the unit $n$-ball, the max volume is achieved at $n=5$. I have read all I could find (here/wikipedia and elsewhere) and I do see that the $n$-ball occupies a smaller and smaller portion of the circumscribing cube $[-1,1]^n$ and I have seen the arguments that the diameter should be used as the fundamental quantity instead of the radius. This way, the volume monotonically decreases to zero for all $n$ at least for the unit ball. I also see the analytic reason why the function converges to zero. The gamma function in the denominator grows much faster than the numerator and eventually the whole fraction converges to zero even if the radius is a googol. But my question is, intuitively speaking (as if intuition is a good guide in higher dimensions), $V(n)$ should be monotonically increasing or at the very least non-decreasing. The way I see it, a 2-ball is contained in a 3-ball and a 2-ball can be rotated in $\mathbb{R}^3$ to create a 3-ball. Similarly, you can rotate any $(n-1)$-ball in $\mathbb{R}^n$ around the appropriate axis to create an $n$-ball. I know that any $(n-1)$-ball in $\mathbb{R}^n$ has Lebesgue measure zero but inside any $n$-ball, an $(n-1)$-ball can be rotated around any of the $n$ axes so $V(n)$ should be larger than $V(n-1)$. This specific ""argument"" hasn't been addressed on any of the previous questions that I could find. Any geometric interpretation of what's happening? Bonus points for something intuitive and/or 1-2-3 dimensional examples to show me the fallacy of my argument. Thanks! Addendum: I have seen this thread (and even this one and many others on stack exchange and math overflow) and like I said, they don't address my argument presented in this question. They do mention other arguments such as comparison with the circumscribing cube or analytically looking at fraction to see why it goes to zero. But those I already know...namely by reading these very threads.","This question already has answers here : Why does the volume of the unit sphere go to zero? (7 answers) Closed 10 years ago . First let us define an $n$-ball as the euclidean sphere in $\mathbb{R}^n$ including its interior and its surface where $n$ refers to the number of coordinates needed to describe the object (the geometer's notation), NOT the topologist's notation which refers to the dimension of the manifold. All $n$-balls considered here are of radius 1 centered at the origin. So an $n$-ball is the set of points $$\{x=(x_1,x_2,...,x_n)\in \mathbb{R}^n : \sum_{i=1}^n x_i^2 \leq 1\}.$$ It is well known that the volume of the unit $n$-ball is given by $$V(n)=\frac{\pi^{n/2}}{\Gamma(\frac{n}{2}+1)}$$ and before anyone points it out, we will consider all volumes to be unit-less so that they can be compared with each other. So for example $$\pi=V(2)<V(3)=\frac{4}{3}\pi.$$ Now, my question is why does $V(n)\rightarrow 0$ as $n\rightarrow\infty$? This behavior is independent of the radius. Depending on the radius, $V(n)$ may (or may not) increase at first, hit a peak at some $n$, and then monotonically decrease and converge to zero. Considering $n\in\mathbb{N}$ to take on only discrete values, for the unit $n$-ball, the max volume is achieved at $n=5$. I have read all I could find (here/wikipedia and elsewhere) and I do see that the $n$-ball occupies a smaller and smaller portion of the circumscribing cube $[-1,1]^n$ and I have seen the arguments that the diameter should be used as the fundamental quantity instead of the radius. This way, the volume monotonically decreases to zero for all $n$ at least for the unit ball. I also see the analytic reason why the function converges to zero. The gamma function in the denominator grows much faster than the numerator and eventually the whole fraction converges to zero even if the radius is a googol. But my question is, intuitively speaking (as if intuition is a good guide in higher dimensions), $V(n)$ should be monotonically increasing or at the very least non-decreasing. The way I see it, a 2-ball is contained in a 3-ball and a 2-ball can be rotated in $\mathbb{R}^3$ to create a 3-ball. Similarly, you can rotate any $(n-1)$-ball in $\mathbb{R}^n$ around the appropriate axis to create an $n$-ball. I know that any $(n-1)$-ball in $\mathbb{R}^n$ has Lebesgue measure zero but inside any $n$-ball, an $(n-1)$-ball can be rotated around any of the $n$ axes so $V(n)$ should be larger than $V(n-1)$. This specific ""argument"" hasn't been addressed on any of the previous questions that I could find. Any geometric interpretation of what's happening? Bonus points for something intuitive and/or 1-2-3 dimensional examples to show me the fallacy of my argument. Thanks! Addendum: I have seen this thread (and even this one and many others on stack exchange and math overflow) and like I said, they don't address my argument presented in this question. They do mention other arguments such as comparison with the circumscribing cube or analytically looking at fraction to see why it goes to zero. But those I already know...namely by reading these very threads.",,"['real-analysis', 'geometry', 'multivariable-calculus', 'differential-geometry']"
8,When do we have $\liminf_{n\to\infty}(a_n+b_n)=\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n)$?,When do we have ?,\liminf_{n\to\infty}(a_n+b_n)=\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n),"It's not hard to show that  $$\liminf_{n\to\infty}(a_n+b_n)\geq\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n)$$ for any $\{a_n\},\{b_n\}\subset{\Bbb R}$ such that the right hand side is defined (i.e. no $\infty-\infty$ or $-\infty+\infty$). Also, if both $\lim a_n$ and $\lim b_n$ exist, then we have $$ \liminf_{n\to\infty}(a_n+b_n)=\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n).  $$ The wikipedia article about limit inferior and limit superior gives a sufficient conditions for ""$=$"" to hold which I don't see how to prove it: if one of $\lim a_n$ and $\lim b_n$ exists, then we have ""$=$"". Here are my questions: How can I show the statement above? Is this condition also necesarry? Assume for example $\lim a_n=a$. To show $$ \liminf_{n\to\infty}(a_n+b_n)=a+\liminf_{n\to\infty}(b_n), $$ it suffices to show that $$ \liminf_{n\to\infty}(a_n+b_n)\leq\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n) $$ I think somehow I would need to use $\liminf_{n\to\infty}(a_n)=\limsup_{n\to\infty}(a_n)=a$. But I don't see how this works.","It's not hard to show that  $$\liminf_{n\to\infty}(a_n+b_n)\geq\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n)$$ for any $\{a_n\},\{b_n\}\subset{\Bbb R}$ such that the right hand side is defined (i.e. no $\infty-\infty$ or $-\infty+\infty$). Also, if both $\lim a_n$ and $\lim b_n$ exist, then we have $$ \liminf_{n\to\infty}(a_n+b_n)=\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n).  $$ The wikipedia article about limit inferior and limit superior gives a sufficient conditions for ""$=$"" to hold which I don't see how to prove it: if one of $\lim a_n$ and $\lim b_n$ exists, then we have ""$=$"". Here are my questions: How can I show the statement above? Is this condition also necesarry? Assume for example $\lim a_n=a$. To show $$ \liminf_{n\to\infty}(a_n+b_n)=a+\liminf_{n\to\infty}(b_n), $$ it suffices to show that $$ \liminf_{n\to\infty}(a_n+b_n)\leq\liminf_{n\to\infty}(a_n)+\liminf_{n\to\infty}(b_n) $$ I think somehow I would need to use $\liminf_{n\to\infty}(a_n)=\limsup_{n\to\infty}(a_n)=a$. But I don't see how this works.",,['real-analysis']
9,Abel summability of integrals,Abel summability of integrals,,"In Introduction to Fourier Analysis on Euclidean Spaces by Stein and Weiss, there is the following footnote on page 5 (the integral referred to is the integral of $\sin(x)/x$) on $[0,\infty)$): As is well known, in this case the limit $\lim_{p\rightarrow\infty} \int_0^p f(x)\ dx$ exists. It is an easy exercise to show that whenever $f$ is locally integrable and such a limit, $l$, exists the Abel means $A_\epsilon =\int_0^\infty e^{-\epsilon x}f(x) \ dx$ converge to $l$. The limit is of course taken as $\epsilon$ tends to $0$, and we are using Lesbegue integrals. Unfortunately, I am not finding this to be an easy exercise. Break each interval $[n,n+1]$ into $2^n$ pieces. Consider the function that is alternately $2^{n/2}$ and $-2^{n/2}$ on these intervals. This is conditionally convergent, but the Abel means don't converge for small $\epsilon$. Either I made a mistake with this counterexample, or the theorem requires more hypotheses. Could I please get a few hints?","In Introduction to Fourier Analysis on Euclidean Spaces by Stein and Weiss, there is the following footnote on page 5 (the integral referred to is the integral of $\sin(x)/x$) on $[0,\infty)$): As is well known, in this case the limit $\lim_{p\rightarrow\infty} \int_0^p f(x)\ dx$ exists. It is an easy exercise to show that whenever $f$ is locally integrable and such a limit, $l$, exists the Abel means $A_\epsilon =\int_0^\infty e^{-\epsilon x}f(x) \ dx$ converge to $l$. The limit is of course taken as $\epsilon$ tends to $0$, and we are using Lesbegue integrals. Unfortunately, I am not finding this to be an easy exercise. Break each interval $[n,n+1]$ into $2^n$ pieces. Consider the function that is alternately $2^{n/2}$ and $-2^{n/2}$ on these intervals. This is conditionally convergent, but the Abel means don't converge for small $\epsilon$. Either I made a mistake with this counterexample, or the theorem requires more hypotheses. Could I please get a few hints?",,"['real-analysis', 'integration', 'improper-integrals']"
10,Can all subseries of an infinite series be pairwise independent over $\mathbb{Q}$?,Can all subseries of an infinite series be pairwise independent over ?,\mathbb{Q},"I'm wondering about a simple question that has multiple possible variants depending on a few parameters. The prototypical one would be: Does there exist an infinite series such that any two subseries  are linearly independent over $\mathbb{Q}$? Assume that the two subseries (sums of subsets of terms from the original series) in question are to be summed in order of increasing index - this allows one to put in place a restriction on absolute or conditional convergence. One can add a combination of restrictions from {finite, cofinite, infinite} to each of the two subseries, e.g. ""any finite subseries $\circ$ and cofinite subseries $\bullet$."" Lastly, one can replace linear independence with algebraic independence - much tougher, I imagine - or replace indepenence with de $\text{}$pendence. Are there any general results on these sorts of questions?","I'm wondering about a simple question that has multiple possible variants depending on a few parameters. The prototypical one would be: Does there exist an infinite series such that any two subseries  are linearly independent over $\mathbb{Q}$? Assume that the two subseries (sums of subsets of terms from the original series) in question are to be summed in order of increasing index - this allows one to put in place a restriction on absolute or conditional convergence. One can add a combination of restrictions from {finite, cofinite, infinite} to each of the two subseries, e.g. ""any finite subseries $\circ$ and cofinite subseries $\bullet$."" Lastly, one can replace linear independence with algebraic independence - much tougher, I imagine - or replace indepenence with de $\text{}$pendence. Are there any general results on these sorts of questions?",,"['real-analysis', 'sequences-and-series', 'analysis', 'reference-request']"
11,Special subset of an Euclidean space,Special subset of an Euclidean space,,"This is a nice problem that I found it somewhere and thought to share it with everyone! Does there exist a subset $S \subset \mathbb{R}^n$ s.t. for every non-zero $t \in \mathbb{R}^n\;, \; S \cap (S+t)$ has precisely one element?","This is a nice problem that I found it somewhere and thought to share it with everyone! Does there exist a subset $S \subset \mathbb{R}^n$ s.t. for every non-zero $t \in \mathbb{R}^n\;, \; S \cap (S+t)$ has precisely one element?",,['real-analysis']
12,Asymptotic difference between a function and its binomial average,Asymptotic difference between a function and its binomial average,,"The origin of this question is the identity $$\sum_{k=0}^n \binom{n}{k} H_k = 2^n \left(H_n - \sum_{k=1}^n \frac{1}{k 2^k}\right),$$ where $H_n$ is the $n$th harmonic number. Dividing by $2^n$, we have $$2^{-n} \sum_{k=0}^n \binom{n}{k} H_k = H_n - \sum_{k=1}^n \frac{1}{k 2^k}.$$ The sum on the left can now be interpreted as a weighted average of the harmonic numbers through $H_n$ -- where the weights, of course, are the binomial coefficients.  Thus the difference between $H_n$ and its ""binomial average"" (I'm guessing there's no term for this) is $$H_n - 2^{-n} \sum_{k=0}^n \binom{n}{k} H_k =  \sum_{k=1}^n \frac{1}{k 2^k}.$$ The sum on the right is known to converge to $\ln 2$ as $n \to \infty$.  (Substitute $-\frac{1}{2}$ into the Maclaurin series for $\ln (1+x)$.) This leads me to my question: Can we classify nonnegative functions $f(n)$ for which    $$\lim_{n \to \infty} \left(f(n) - 2^{-n} \sum_{k=0}^n \binom{n}{k} f(k) \right)$$ is finite and nonzero? It would seem that if $f$ increases sufficiently rapidly, then the limit would be $\infty$.  This is the case with both $f(n) = a^n$ and $f(n) = n$.  If $f$ decreases or is constant, then the limit is zero.  If $f$ has basically logarithmic growth, then it seems the limit would behave as $H_n$.  But can this be proved?  And what about other sublinear, increasing functions?","The origin of this question is the identity $$\sum_{k=0}^n \binom{n}{k} H_k = 2^n \left(H_n - \sum_{k=1}^n \frac{1}{k 2^k}\right),$$ where $H_n$ is the $n$th harmonic number. Dividing by $2^n$, we have $$2^{-n} \sum_{k=0}^n \binom{n}{k} H_k = H_n - \sum_{k=1}^n \frac{1}{k 2^k}.$$ The sum on the left can now be interpreted as a weighted average of the harmonic numbers through $H_n$ -- where the weights, of course, are the binomial coefficients.  Thus the difference between $H_n$ and its ""binomial average"" (I'm guessing there's no term for this) is $$H_n - 2^{-n} \sum_{k=0}^n \binom{n}{k} H_k =  \sum_{k=1}^n \frac{1}{k 2^k}.$$ The sum on the right is known to converge to $\ln 2$ as $n \to \infty$.  (Substitute $-\frac{1}{2}$ into the Maclaurin series for $\ln (1+x)$.) This leads me to my question: Can we classify nonnegative functions $f(n)$ for which    $$\lim_{n \to \infty} \left(f(n) - 2^{-n} \sum_{k=0}^n \binom{n}{k} f(k) \right)$$ is finite and nonzero? It would seem that if $f$ increases sufficiently rapidly, then the limit would be $\infty$.  This is the case with both $f(n) = a^n$ and $f(n) = n$.  If $f$ decreases or is constant, then the limit is zero.  If $f$ has basically logarithmic growth, then it seems the limit would behave as $H_n$.  But can this be proved?  And what about other sublinear, increasing functions?",,"['real-analysis', 'binomial-coefficients', 'asymptotics', 'average']"
13,sum $\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right)$,sum,\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right),"Let for $a>0$ , $\displaystyle S_n(a)=\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right)$ . I conjecture that there exist a number $c_a\in\mathbb R$ , such that : $$ \displaystyle S_n(a)\sim c_a  \big(\sum_{k=1}^{n-1}{1\over k^a}\big)n^{a-2} \ln n\quad (n\to +\infty).$$ I found by elementary calculus that When $a=1$ , $ \displaystyle S_n(1)\sim \frac 32   \frac{\ln^2 n}n$ , so $c_1=\frac 32$ and when $a=2$ , $ \displaystyle S_n(2)\sim   \frac{\pi^2}6\ln n$ , so $c_2=1$ I need some help for général case I give this non-rigorous proof $\displaystyle S_n(a) = \sum_{k=1}^{n-1} \Big(1-{k \over n}\Big)^{-a} \Big({\ln k \over k} - {\ln (k+1) \over k+1} \Big). $ with change of index : $\displaystyle S_n (a)= \sum_{k=1}^{n-1} \Big({n \over k}\Big)^a \Big({\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} \Big).$ I believe we have this asymptotic development : $\displaystyle {\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} = {\ln n -1\over n^2}\Big(1+ {(2k-1)\over n } {2 \ln n -3 \over 2 \ln n-2} + \big({1\over n^3}\big)\epsilon_{k,n}\Big)$ , Thus ? $\displaystyle S_n (a) \sim c_a \Big(\sum_{k=1}^{n-1}\Big({n \over k}\Big)^a \Big) {\ln n \over n^{2}}$ with a constant $c_a$ to be determined Explanations at the request of Dr. Wolfgang Hintze A- Let us establish that Thank's for my friend Lou $\displaystyle \lim_{n\to + \infty}S_n(a)=\left\{\begin{array}{cl} 0 & \text { si } \: 0\leqslant a <2\\+\infty &\:\text {si } \: a\geqslant 2. \end{array}\right.$ Let $f: x\mapsto \dfrac{\log x}x - \dfrac {\log(x+1)}{x+1}.\qquad f(x)\underset{x\to + \infty}\sim \dfrac {\log x}{x^2}.$ $1)\quad\forall a\geqslant 2, \quad S_n(a)\geqslant \dfrac {n^{a}}{(n-1)^{a}}f(1)+\dfrac {n^{a}}{(n-2)^{a}}f(2) +0+0+\dots +0+\dfrac {n^{a}}{(1)^{a}}f(n-1)\underset{n\to + \infty}\sim n^{a-2}\log n.$ $2)\quad $ It's easy to prove that : $\:\:\forall x\in [3;n], \:\: 0<f(x)<\dfrac{ \log n}{x^2} \quad(1).$ If $a\in [0;2[.\quad S_n(a) =-\dfrac{\log n}n+ \displaystyle \sum _{k=1}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) f(k).\quad$ write : $\:T_n(a):=\log n\displaystyle \sum _{k=3}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) \dfrac 1{k^2}.$ From $ (1) $ , it therefore suffices to prove that : $ \displaystyle  \lim_{n\to + \infty} T_n(a) =0.$ $\displaystyle T_n(a)=\log n  \sum_{k=1}^{n-3}\dfrac{n^{a} - k^{a}}{k^{a}(n-k)^2} =\dfrac{\log n}{n^2}  \displaystyle \sum_{k=1}^{n-3}(n^{a}- k^{a})\left( k^{-a}+ \dfrac {2 k^{1-a}}{n-k} + \dfrac {k^{2-a}}{(n-k)^2}\right).\quad $ We also have inequalities: $\:\:\forall a \in \mathbb R^+,\: \forall k\in [1;n]: $ $0 \leqslant a<2 \implies 0\leqslant n^{a}- k^{a}\leqslant an^{a-1}(n-k).\quad (2), \quad 0\leqslant a \leqslant1 \implies 0\leqslant n^{a}- k^{a}\leqslant ak^{a-1}(n-k).\quad (3).$ They lead to: $\bullet \:\text{Si }1\leqslant a<2, \:\text {alors}\:\:0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{a-3}\sum_{k=1}^{n-3}k^{1-a}+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right). \qquad (4)$ $\bullet \:\text{Si }0\leqslant a \leqslant1, \:\text {alors}\:\: 0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{-2}\sum_{k=1}^{n-3}1+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right).\qquad (5)$ We check that the right-hand side of the inequalities (4) and (5) have a zero limit when $ n \to + \infty.$ B-  Show that $\:\:\boxed{S_n(1) \underset{n\to + \infty}\sim \dfrac {3(\log n)^2}{2n}.}\qquad$ Let $g: x\mapsto x^2f(x) -\log x.\quad $ Then : $\:\:\displaystyle \lim_{+\infty} g=-1, \quad g \text { is bounded on  } [1;+\infty[\:\: (1).\qquad$ Let $\:\:T_n:= \displaystyle \sum_{k=1}^{n-1} \dfrac k{n-k} f(k). $ Then: $ \:\: S_n(1) = T_n- \dfrac {\log n}n\:\:(2).\:\:\quad T_n=\dfrac 1n \displaystyle \sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) k^2f(k) =U_n +V_n +W_n\:\:$ with $U_n,V_n,W_n $ are defined by  : $\displaystyle U_n:=\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}k, \quad V_n:= \dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}{n-k}, \quad W_n:= \dfrac 1n\sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) g(k).\:\:\:$ According to $(1): \quad W_n=\mathcal O\left (\dfrac{\log n}n \right)\:\:(3).\quad \:\:U_n \underset{n\to + \infty}\sim \dfrac{(\log n)^2}{2n}\:\: (4). \qquad V_n =\displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} +\dfrac 1{n^2}\displaystyle\sum_{k=1}^{n-1}\dfrac {\log(k/n)}{1-(k/n)}. $ The Function $h:x\mapsto \dfrac{\ln x}{1-x}\text{ is continuous, monotonic, integrable on  }\:]0;1[,\quad $ so $\:\:\displaystyle \lim_{n\to +\infty}\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log(k/n)}{1-(k/n)} =\int _0 ^1 h .$ This information, combined with the fact that: $\: \displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} \underset{n\to + \infty}\sim \dfrac {(\log n)^2}n,\:$ leads to: $\:\:V_n\underset{n\to + \infty}\sim  \dfrac {(\log n)^2}n.\quad (5)$ $(2), (3), (4) $ et $(5)$ provide the equivalent of $ S_n (1) $ announced. I can explain the case a = 2 if needed","Let for , . I conjecture that there exist a number , such that : I found by elementary calculus that When , , so and when , , so I need some help for général case I give this non-rigorous proof with change of index : I believe we have this asymptotic development : , Thus ? with a constant to be determined Explanations at the request of Dr. Wolfgang Hintze A- Let us establish that Thank's for my friend Lou Let It's easy to prove that : If write : From , it therefore suffices to prove that : We also have inequalities: They lead to: We check that the right-hand side of the inequalities (4) and (5) have a zero limit when B-  Show that Let Then : Let Then: with are defined by  : According to The Function so This information, combined with the fact that: leads to: et provide the equivalent of announced. I can explain the case a = 2 if needed","a>0 \displaystyle S_n(a)=\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right) c_a\in\mathbb R  \displaystyle S_n(a)\sim c_a  \big(\sum_{k=1}^{n-1}{1\over k^a}\big)n^{a-2} \ln n\quad (n\to +\infty). a=1  \displaystyle S_n(1)\sim \frac 32   \frac{\ln^2 n}n c_1=\frac 32 a=2  \displaystyle S_n(2)\sim   \frac{\pi^2}6\ln n c_2=1 \displaystyle S_n(a) = \sum_{k=1}^{n-1} \Big(1-{k \over n}\Big)^{-a} \Big({\ln k \over k} - {\ln (k+1) \over k+1} \Big).  \displaystyle S_n (a)= \sum_{k=1}^{n-1} \Big({n \over k}\Big)^a \Big({\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} \Big). \displaystyle {\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} = {\ln n -1\over n^2}\Big(1+ {(2k-1)\over n } {2 \ln n -3 \over 2 \ln n-2} + \big({1\over n^3}\big)\epsilon_{k,n}\Big) \displaystyle S_n (a) \sim c_a \Big(\sum_{k=1}^{n-1}\Big({n \over k}\Big)^a \Big) {\ln n \over n^{2}} c_a \displaystyle \lim_{n\to + \infty}S_n(a)=\left\{\begin{array}{cl} 0 & \text { si } \: 0\leqslant a <2\\+\infty &\:\text {si } \: a\geqslant 2. \end{array}\right. f: x\mapsto \dfrac{\log x}x - \dfrac {\log(x+1)}{x+1}.\qquad f(x)\underset{x\to + \infty}\sim \dfrac {\log x}{x^2}. 1)\quad\forall a\geqslant 2, \quad S_n(a)\geqslant \dfrac {n^{a}}{(n-1)^{a}}f(1)+\dfrac {n^{a}}{(n-2)^{a}}f(2) +0+0+\dots +0+\dfrac {n^{a}}{(1)^{a}}f(n-1)\underset{n\to + \infty}\sim n^{a-2}\log n. 2)\quad  \:\:\forall x\in [3;n], \:\: 0<f(x)<\dfrac{ \log n}{x^2} \quad(1). a\in [0;2[.\quad S_n(a) =-\dfrac{\log n}n+ \displaystyle \sum _{k=1}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) f(k).\quad \:T_n(a):=\log n\displaystyle \sum _{k=3}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) \dfrac 1{k^2}.  (1)   \displaystyle  \lim_{n\to + \infty} T_n(a) =0. \displaystyle T_n(a)=\log n  \sum_{k=1}^{n-3}\dfrac{n^{a} - k^{a}}{k^{a}(n-k)^2} =\dfrac{\log n}{n^2}  \displaystyle \sum_{k=1}^{n-3}(n^{a}- k^{a})\left( k^{-a}+ \dfrac {2 k^{1-a}}{n-k} + \dfrac {k^{2-a}}{(n-k)^2}\right).\quad  \:\:\forall a \in \mathbb R^+,\: \forall k\in [1;n]:  0 \leqslant a<2 \implies 0\leqslant n^{a}- k^{a}\leqslant an^{a-1}(n-k).\quad (2), \quad 0\leqslant a \leqslant1 \implies 0\leqslant n^{a}- k^{a}\leqslant ak^{a-1}(n-k).\quad (3). \bullet \:\text{Si }1\leqslant a<2, \:\text {alors}\:\:0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{a-3}\sum_{k=1}^{n-3}k^{1-a}+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right). \qquad (4) \bullet \:\text{Si }0\leqslant a \leqslant1, \:\text {alors}\:\: 0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{-2}\sum_{k=1}^{n-3}1+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right).\qquad (5)  n \to + \infty. \:\:\boxed{S_n(1) \underset{n\to + \infty}\sim \dfrac {3(\log n)^2}{2n}.}\qquad g: x\mapsto x^2f(x) -\log x.\quad  \:\:\displaystyle \lim_{+\infty} g=-1, \quad g \text { is bounded on  } [1;+\infty[\:\: (1).\qquad \:\:T_n:= \displaystyle \sum_{k=1}^{n-1} \dfrac k{n-k} f(k).   \:\: S_n(1) = T_n- \dfrac {\log n}n\:\:(2).\:\:\quad T_n=\dfrac 1n \displaystyle \sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) k^2f(k) =U_n +V_n +W_n\:\: U_n,V_n,W_n  \displaystyle U_n:=\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}k, \quad V_n:= \dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}{n-k}, \quad W_n:= \dfrac 1n\sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) g(k).\:\:\: (1): \quad W_n=\mathcal O\left (\dfrac{\log n}n \right)\:\:(3).\quad \:\:U_n \underset{n\to + \infty}\sim \dfrac{(\log n)^2}{2n}\:\: (4). \qquad V_n =\displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} +\dfrac 1{n^2}\displaystyle\sum_{k=1}^{n-1}\dfrac {\log(k/n)}{1-(k/n)}.  h:x\mapsto \dfrac{\ln x}{1-x}\text{ is continuous, monotonic, integrable on  }\:]0;1[,\quad  \:\:\displaystyle \lim_{n\to +\infty}\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log(k/n)}{1-(k/n)} =\int _0 ^1 h . \: \displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} \underset{n\to + \infty}\sim \dfrac {(\log n)^2}n,\: \:\:V_n\underset{n\to + \infty}\sim  \dfrac {(\log n)^2}n.\quad (5) (2), (3), (4)  (5)  S_n (1) ","['real-analysis', 'sequences-and-series', 'asymptotics']"
14,The domino curve,The domino curve,,"Consider a set of domino tiles with zero thickness evenly spaced apart. Now let them fall but hold the last one upright. The arangement could now look like this: Call the distance between two tiles $d$ and the coordinates of the top of each domino $(p_n,q_n)$ , so the $n$ -th tile is described by $y_n=\frac{q_n(nd-x)}{nd-p_n}$ . However, these equations get complicated fast: $$y_1 = \sqrt{1-d^2}\left(1-\frac{x}{d}\right) \qquad  y_2 = \frac{ \sqrt{1-d^2} \left( d^2 - \sqrt{d^4-d^2+1} \right) (2d-x)}{d \left( d^2 - \sqrt{d^4-d^2+1} -1 \right)}$$ Is there an easy formula for $(p_n,q_n)$ ? Since they all have the same length and are equally spaced it might be easier to look at it from a point-slope perspective. I obtained $(p_n,q_n)$ by finding the intercept between $y_{n-1}$ and $y=\sqrt{1-(x-nd)}$ . Maybe there is also a nice representation for the angle at the bottom of the $n$ -th tile (depending on $d$ ) but I wasn't able to find it. What curve are all dominoes tangent to? This also depends on $d$ and if it's too hard, consider only the case $\lim_\limits{d \to 0}$ . Such a function would have the following properties: $$\lim_\limits{x \to \infty} f(x) = \lim_\limits{x \to \infty} f'(x) = 0 \qquad  \lim_\limits{x \to 0^+} f(x) = 1 \qquad \lim_\limits{x \to 0^+} f'(x) = -\infty$$ Thanks in advance.","Consider a set of domino tiles with zero thickness evenly spaced apart. Now let them fall but hold the last one upright. The arangement could now look like this: Call the distance between two tiles and the coordinates of the top of each domino , so the -th tile is described by . However, these equations get complicated fast: Is there an easy formula for ? Since they all have the same length and are equally spaced it might be easier to look at it from a point-slope perspective. I obtained by finding the intercept between and . Maybe there is also a nice representation for the angle at the bottom of the -th tile (depending on ) but I wasn't able to find it. What curve are all dominoes tangent to? This also depends on and if it's too hard, consider only the case . Such a function would have the following properties: Thanks in advance.","d (p_n,q_n) n y_n=\frac{q_n(nd-x)}{nd-p_n} y_1 = \sqrt{1-d^2}\left(1-\frac{x}{d}\right) \qquad 
y_2 = \frac{ \sqrt{1-d^2} \left( d^2 - \sqrt{d^4-d^2+1} \right) (2d-x)}{d \left( d^2 - \sqrt{d^4-d^2+1} -1 \right)} (p_n,q_n) (p_n,q_n) y_{n-1} y=\sqrt{1-(x-nd)} n d d \lim_\limits{d \to 0} \lim_\limits{x \to \infty} f(x) = \lim_\limits{x \to \infty} f'(x) = 0 \qquad 
\lim_\limits{x \to 0^+} f(x) = 1 \qquad \lim_\limits{x \to 0^+} f'(x) = -\infty","['real-analysis', 'geometry', 'limits', 'analytic-geometry']"
15,"On which subspace $W\subset C^{\infty}[0,1]$ is $D(f)=xf'(x)$ a bounded operator provided all functions in $W$ are flat functions?",On which subspace  is  a bounded operator provided all functions in  are flat functions?,"W\subset C^{\infty}[0,1] D(f)=xf'(x) W","First we introduce the concept ""Flat function"": Definition: A smooth function $f:\mathbb{R}\to \mathbb{R}$ is called a flat function at origin if $f^{(k)}(0)=0$ for all $k=0,1,2,\ldots $ . Let $V=\{f\in C^{\infty}[0,1]\mid \text{$f$ is flat at origin}\}$ . We equip $V$ with $\|\cdot\|_{\infty}$ . Is there an infinite dimensional subspace $W\subseteq V$ which is invariant under the differential operator $D(f)=xf'(x)$ and $D$ is a bounded operator on $W$ ? The motivation comes from page 43 remark 2 of the following paper: http://mcs.qut.ac.ir/article_243944.html","First we introduce the concept ""Flat function"": Definition: A smooth function is called a flat function at origin if for all . Let . We equip with . Is there an infinite dimensional subspace which is invariant under the differential operator and is a bounded operator on ? The motivation comes from page 43 remark 2 of the following paper: http://mcs.qut.ac.ir/article_243944.html","f:\mathbb{R}\to \mathbb{R} f^{(k)}(0)=0 k=0,1,2,\ldots  V=\{f\in C^{\infty}[0,1]\mid \text{f is flat at origin}\} V \|\cdot\|_{\infty} W\subseteq V D(f)=xf'(x) D W","['real-analysis', 'functional-analysis', 'operator-theory', 'differential-operators']"
16,Prove two series are equal,Prove two series are equal,,"Prove $I=J$ , where: $$I=\left\{\frac{4}{\pi^6}\displaystyle\sum_{n=1}^{\infty}\displaystyle\sum_{m=1}^{\infty}\frac{1}{n^2m^2\sqrt{n^2+m^2}}\left(\pi\frac{e^{\pi\sqrt{n^2+m^2}}+e^{-\pi\sqrt{n^2+m^2}}}{e^{\pi\sqrt{n^2+m^2}}-e^{-\pi\sqrt{n^2+m^2}}}-\frac{1}{\sqrt{n^2+m^2}}\right)\right\}^{-1},$$ and $$J=12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}.$$ My atempt :  We have \begin{align*} \displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}+\frac{1}{(3n+2)^2}\right\}&=\displaystyle\sum_{n=1}^{\infty}\left\{\frac{1}{(3n-2)^2}+\frac{1}{(3n-1)^2}+\frac{1}{(3n)^2}\right\}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\ &=\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\ &=\frac{8}{9}\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{4\pi^2}{27}\\ \end{align*} and: \begin{align*} \displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+2)^2}&=4\displaystyle\sum_{n=0}^{\infty}\frac{1}{(6n+4)^2}\\ &=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(6n+1)^2}+\frac{1}{(6n+4)^2}\right\}-4\displaystyle\sum_{n=1}^{\infty}\frac{1}{(6n+1)^2}\\ &=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right\}\\ \end{align*} So : \begin{align*} \frac{4\pi^2}{27}&=\displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+1)^2}+4\displaystyle\sum_{n=0}^{\infty}\left(\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right)\\ &=\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\\ &\implies \frac{1}{12\pi^2}\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}=\frac{1}{81}\\ &\implies 12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}=81\\ \end{align*} Finally $ J=81$ , but how to prove $I=81$ ? Any help for how to calculate $I$ ? Thanks in advance","Prove , where: and My atempt :  We have and: So : Finally , but how to prove ? Any help for how to calculate ? Thanks in advance","I=J I=\left\{\frac{4}{\pi^6}\displaystyle\sum_{n=1}^{\infty}\displaystyle\sum_{m=1}^{\infty}\frac{1}{n^2m^2\sqrt{n^2+m^2}}\left(\pi\frac{e^{\pi\sqrt{n^2+m^2}}+e^{-\pi\sqrt{n^2+m^2}}}{e^{\pi\sqrt{n^2+m^2}}-e^{-\pi\sqrt{n^2+m^2}}}-\frac{1}{\sqrt{n^2+m^2}}\right)\right\}^{-1}, J=12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}. \begin{align*}
\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}+\frac{1}{(3n+2)^2}\right\}&=\displaystyle\sum_{n=1}^{\infty}\left\{\frac{1}{(3n-2)^2}+\frac{1}{(3n-1)^2}+\frac{1}{(3n)^2}\right\}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\
&=\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\
&=\frac{8}{9}\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{4\pi^2}{27}\\
\end{align*} \begin{align*}
\displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+2)^2}&=4\displaystyle\sum_{n=0}^{\infty}\frac{1}{(6n+4)^2}\\
&=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(6n+1)^2}+\frac{1}{(6n+4)^2}\right\}-4\displaystyle\sum_{n=1}^{\infty}\frac{1}{(6n+1)^2}\\
&=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right\}\\
\end{align*} \begin{align*}
\frac{4\pi^2}{27}&=\displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+1)^2}+4\displaystyle\sum_{n=0}^{\infty}\left(\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right)\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\\
&\implies \frac{1}{12\pi^2}\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}=\frac{1}{81}\\
&\implies 12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}=81\\
\end{align*}  J=81 I=81 I","['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
17,Regarding evaluation of the limit of the sequence $\Bigl(\frac{1}{n}\Bigr)^n+\Bigl(\frac{2}{n}\Bigr)^n+ \cdots \Bigl(\frac{n}{n}\Bigr)^n$ [duplicate],Regarding evaluation of the limit of the sequence  [duplicate],\Bigl(\frac{1}{n}\Bigr)^n+\Bigl(\frac{2}{n}\Bigr)^n+ \cdots \Bigl(\frac{n}{n}\Bigr)^n,"This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 4 years ago . The problem states as, Prove the sequence $$S_n= \Bigl(\frac{1}{n}\Bigr)^n+\Bigl(\frac{2}{n}\Bigr)^n+ \cdots \Bigl(\frac{n}{n}\Bigr)^n$$ converges to $\frac{e}{e-1}$ . A simple (possibly wrong) solution: We can  also write the sequence as $$S_n=\Bigl(\frac{n}{n}\Bigr)^n+ \Bigl(\frac{n-1}{n}\Bigr)^n+\Bigl(\frac{n-2}{n}\Bigr)^n+ \cdots +\Bigl(\frac{2}{n}\Bigr)^n+\Bigl(\frac{1}{n}\Bigr)^n$$ Where we just rearranged the terms of the sequence. Since the series $\lim \limits_{n \to \infty} S_n$ is absolutely convergent, the rearrangement will not affect the value of the limit. Now by passing the limit to $S_n$ $$\lim \limits_{n \to \infty}S_n=\lim \limits_{n \to \infty}\Bigl(\frac{n}{n}\Bigr)^n+ \lim \limits_{n \to \infty}\Bigl(\frac{n-1}{n}\Bigr)^n+\lim \limits_{n \to \infty}\Bigl(\frac{n-2}{n}\Bigr)^n+ \cdots $$ or, $$\lim \limits_{n \to \infty}S_n=1+e^{-1}+e^{-2}+\cdots$$ or, $$\lim \limits_{n \to \infty}S_n= \frac{e}{e-1}$$ Why I'm considering it wrong: I know the basic limit theorem $\lim(X_1+X_2)=\lim X_1+\lim X_2$ and by mathematical induction, the theorem is true for sum of any finite number of $X_n$ 's. However, the theorem does not hold good if $n \to \infty$ . For example, let us take a sequence $$a_n= \frac{1}{n^2}+\frac{2}{n^2}+\cdots \frac{n}{n^2}$$ Where $\lim\limits_{n \to \infty}a_n=\frac{1}{2}$ , although if we take limit term by term in the summand (i.e expression of $a_n$ ), we would get $\lim\limits_{n \to \infty}a_n=0$ . Now coming back to our original sequence of concern, it is fairly easy to show that $\lim\limits_{n \to \infty}S_n \leq \frac{e}{e-1}$ , since the function $f(x)=\Bigl(1-\frac{k}{x} \Bigr)^x$ is monotonically increasing $\forall x \geq 1$ and $\forall k \in \Bbb N$ . Now we got somehow to show $\lim\limits_{n \to \infty}S_n \geq \frac{e}{e-1}$ , in order to use the squeeze theorem to get the desired result.  However, I'm unable to make any significant progress to find the same. So my questions are, i) How to show $\lim\limits_{n \to \infty}S_n \geq \frac{e}{e-1}$ or what can be some alternate method to find the desired limit of the sequence? ii) Moreover, are there any general results when $\lim(X_1+X_2+ \cdots) = \lim X_1+ \lim X_2+ \cdots$ holds even when the number of terms in the summand are infinity?","This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 4 years ago . The problem states as, Prove the sequence converges to . A simple (possibly wrong) solution: We can  also write the sequence as Where we just rearranged the terms of the sequence. Since the series is absolutely convergent, the rearrangement will not affect the value of the limit. Now by passing the limit to or, or, Why I'm considering it wrong: I know the basic limit theorem and by mathematical induction, the theorem is true for sum of any finite number of 's. However, the theorem does not hold good if . For example, let us take a sequence Where , although if we take limit term by term in the summand (i.e expression of ), we would get . Now coming back to our original sequence of concern, it is fairly easy to show that , since the function is monotonically increasing and . Now we got somehow to show , in order to use the squeeze theorem to get the desired result.  However, I'm unable to make any significant progress to find the same. So my questions are, i) How to show or what can be some alternate method to find the desired limit of the sequence? ii) Moreover, are there any general results when holds even when the number of terms in the summand are infinity?",S_n= \Bigl(\frac{1}{n}\Bigr)^n+\Bigl(\frac{2}{n}\Bigr)^n+ \cdots \Bigl(\frac{n}{n}\Bigr)^n \frac{e}{e-1} S_n=\Bigl(\frac{n}{n}\Bigr)^n+ \Bigl(\frac{n-1}{n}\Bigr)^n+\Bigl(\frac{n-2}{n}\Bigr)^n+ \cdots +\Bigl(\frac{2}{n}\Bigr)^n+\Bigl(\frac{1}{n}\Bigr)^n \lim \limits_{n \to \infty} S_n S_n \lim \limits_{n \to \infty}S_n=\lim \limits_{n \to \infty}\Bigl(\frac{n}{n}\Bigr)^n+ \lim \limits_{n \to \infty}\Bigl(\frac{n-1}{n}\Bigr)^n+\lim \limits_{n \to \infty}\Bigl(\frac{n-2}{n}\Bigr)^n+ \cdots  \lim \limits_{n \to \infty}S_n=1+e^{-1}+e^{-2}+\cdots \lim \limits_{n \to \infty}S_n= \frac{e}{e-1} \lim(X_1+X_2)=\lim X_1+\lim X_2 X_n n \to \infty a_n= \frac{1}{n^2}+\frac{2}{n^2}+\cdots \frac{n}{n^2} \lim\limits_{n \to \infty}a_n=\frac{1}{2} a_n \lim\limits_{n \to \infty}a_n=0 \lim\limits_{n \to \infty}S_n \leq \frac{e}{e-1} f(x)=\Bigl(1-\frac{k}{x} \Bigr)^x \forall x \geq 1 \forall k \in \Bbb N \lim\limits_{n \to \infty}S_n \geq \frac{e}{e-1} \lim\limits_{n \to \infty}S_n \geq \frac{e}{e-1} \lim(X_1+X_2+ \cdots) = \lim X_1+ \lim X_2+ \cdots,"['real-analysis', 'sequences-and-series', 'limits']"
18,"$\int_a^bf^2(x)\,dx\le \frac{2}{3}\int_a^bf(x)\,dx$ for a convex differentiable function",for a convex differentiable function,"\int_a^bf^2(x)\,dx\le \frac{2}{3}\int_a^bf(x)\,dx","If $f:[a,b] \to \mathbb{R}, f(a)=0,f(b)=1$ is a convex increasing differentiable function on the interval $[a,b]$ . Prove that $$\int_a^bf^2(x)\,dx\le \frac{2}{3}\int_a^bf(x)\,dx$$ Since f is convex and increasing so $f''(x)\ge 0 $ and $f'(x)\ge 0$ . Then I consider a function $g:[a,b]\to \mathbb{R}$ , $g(x)=\frac{2}{3}\int_a^xf(t)\,dt-\int_a^xf^2(t)\,dt$ . Now $f$ is differentiable implies $g$ is also but can't conclude $g'(x)\ge 0$ .","If is a convex increasing differentiable function on the interval . Prove that Since f is convex and increasing so and . Then I consider a function , . Now is differentiable implies is also but can't conclude .","f:[a,b] \to \mathbb{R}, f(a)=0,f(b)=1 [a,b] \int_a^bf^2(x)\,dx\le \frac{2}{3}\int_a^bf(x)\,dx f''(x)\ge 0  f'(x)\ge 0 g:[a,b]\to \mathbb{R} g(x)=\frac{2}{3}\int_a^xf(t)\,dt-\int_a^xf^2(t)\,dt f g g'(x)\ge 0","['real-analysis', 'inequality', 'convex-analysis', 'integral-inequality']"
19,Maclaurin expansion of $\arctan(x)/(1 − x).$,Maclaurin expansion of,\arctan(x)/(1 − x).,"How was this Maclaurin expansion derived? For each $|x|<1,$ \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k,\;\text{where}\; D_k=\{j\in \Bbb{N}:0\leq j\leq (k-1)/2\}.\end{align} HERE'S MY TRIAL \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum^{k}_{j=0} x^j\dfrac{ (-1)^{(k-j)} x^{2(k-j)+1}}{2(k-j)+1}\right),\;\text{for}\; k\in \Bbb{N}\\&\stackrel{\text{how?}}{=} \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k.\end{align}",How was this Maclaurin expansion derived? For each HERE'S MY TRIAL,"|x|<1, \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k,\;\text{where}\; D_k=\{j\in \Bbb{N}:0\leq j\leq (k-1)/2\}.\end{align} \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum^{k}_{j=0} x^j\dfrac{ (-1)^{(k-j)} x^{2(k-j)+1}}{2(k-j)+1}\right),\;\text{for}\; k\in \Bbb{N}\\&\stackrel{\text{how?}}{=} \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k.\end{align}","['real-analysis', 'calculus', 'sequences-and-series', 'analysis', 'taylor-expansion']"
20,Equivalent definitions of Schwartz Space,Equivalent definitions of Schwartz Space,,"I'm trying to show the following but have no idea how to begin. I'm quite new to analysis and multi-index notation. $$ f \in \mathcal{S} \quad \Longleftrightarrow \quad \forall N \in \mathbb{N}, \alpha \in \mathbb N_0 \text{ a multi-index} \ \exists C_{N,\alpha} > 0: \ \vert \partial^{\alpha} f(x) \vert \leq \frac{C_{N,\alpha}}{(1+|x|)^{N}} $$ where $\mathcal{S}$ denotes the Schwartz space. For the forward direction I've written down the definition of being in Schwartz space i.e. $$ \sup_{x \in \mathbb{R}^{n}}|x^{\alpha}\partial^{\beta}f(x)| \leq C_{\alpha,\beta} $$ where $ \alpha, \beta \in \mathbb N_0$ are multi-indices","I'm trying to show the following but have no idea how to begin. I'm quite new to analysis and multi-index notation. $$ f \in \mathcal{S} \quad \Longleftrightarrow \quad \forall N \in \mathbb{N}, \alpha \in \mathbb N_0 \text{ a multi-index} \ \exists C_{N,\alpha} > 0: \ \vert \partial^{\alpha} f(x) \vert \leq \frac{C_{N,\alpha}}{(1+|x|)^{N}} $$ where $\mathcal{S}$ denotes the Schwartz space. For the forward direction I've written down the definition of being in Schwartz space i.e. $$ \sup_{x \in \mathbb{R}^{n}}|x^{\alpha}\partial^{\beta}f(x)| \leq C_{\alpha,\beta} $$ where $ \alpha, \beta \in \mathbb N_0$ are multi-indices",,"['real-analysis', 'fourier-analysis', 'schwartz-space']"
21,"How to solve $(y)^{y'}=(y')^{y+c},c \in \mathbb{R}$",How to solve,"(y)^{y'}=(y')^{y+c},c \in \mathbb{R}","In the case when $ c=0 $ this ode will be  $(y)^{y'}=(y')^{y}$ , let's assume that $y$ and $y'$ are  strictly positive functions so: $$(y)^{y'}=(y')^{y} \iff e^{y' \log(y)}= e^{y \log(y')} \iff {y' \log(y)}= {y \log(y')} \iff \frac{y'}{y}=\frac{\log(y')}{\log(y)}$$ I have no idea what to do now?  Any help please ?","In the case when $ c=0 $ this ode will be  $(y)^{y'}=(y')^{y}$ , let's assume that $y$ and $y'$ are  strictly positive functions so: $$(y)^{y'}=(y')^{y} \iff e^{y' \log(y)}= e^{y \log(y')} \iff {y' \log(y)}= {y \log(y')} \iff \frac{y'}{y}=\frac{\log(y')}{\log(y)}$$ I have no idea what to do now?  Any help please ?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'change-of-variable']"
22,Combinatorial proof that$|ma-\sqrt{p_n}|$ get's arbitrarily small for infinitely many $n$,Combinatorial proof that get's arbitrarily small for infinitely many,|ma-\sqrt{p_n}| n,"Given a sequence of positive numbers, $\varepsilon_n$, which converge to zero, there is some a>0 such that for infinitely many n there is an m for which $$|ma-\sqrt{p_n}|<\varepsilon_n$$ where $p_n$ is the nth prime number. I have a proof of this fact produced below, but it comes as an edge case of a lemma based on Baire's theorem. Is there a more direct approach which uses number theory or combinatorics? Lemma: Let $f\colon \mathbb R \to \mathbb R$ be a continuous function such that for all $a>0$ we have $\lim\limits_{n\to \infty} f(na)=0$. Then $\lim\limits_{x\to \infty} f(x)=0$. We prove this as follows: Choose any $\varepsilon>0$ and set $$E_n = \{x\colon |f(mx)|\leq \varepsilon \text{ for all }m\geq n\}=\bigcap\limits_{m\in \mathbb N} \{x\colon mx\in f^{-1}([-\varepsilon,\varepsilon])\}.$$ Each of these sets will be closed by the continuity of $f$, and by the definition of our function we have that $\mathbb R_+ = \bigcup\limits_{n\in \mathbb N} E_n$, so that by Baire's theorem at least $E_n$ has nonempty interior. Thus we have an interval $I=[a,b]$, and some positive integer $n_0$ such that for all $n>n_0$, $f(nI)\subset [-\varepsilon,\varepsilon]$. We also have that if $n$ is big enough that $na<(n-1)b$ so that integer multiples of this interval will eventually cover the tail of the $\mathbb R_+$. All $x$ large enough will then be in a large integer multiple of $I$, and so we have $f(x)\in[-\varepsilon,\varepsilon]$. QED Now to prove the statement in the question, take $f$ to be $1$ at the square root of prime numbers, $0$ on $\mathbb R_+ \setminus \bigcup (\sqrt{p_n}-\varepsilon_n,\sqrt{p_n}+\varepsilon_n)$ and linear elsewhere. This function does not go to $0$ as $x\to \infty$ so we must have the existence of at least one point $a$ such that $$\lim_{m\to \infty}f(ma)\not\to 0.$$ It follows that $ma$ must wind up in infinitely many of the peaks centered about $\sqrt{p_n}$.","Given a sequence of positive numbers, $\varepsilon_n$, which converge to zero, there is some a>0 such that for infinitely many n there is an m for which $$|ma-\sqrt{p_n}|<\varepsilon_n$$ where $p_n$ is the nth prime number. I have a proof of this fact produced below, but it comes as an edge case of a lemma based on Baire's theorem. Is there a more direct approach which uses number theory or combinatorics? Lemma: Let $f\colon \mathbb R \to \mathbb R$ be a continuous function such that for all $a>0$ we have $\lim\limits_{n\to \infty} f(na)=0$. Then $\lim\limits_{x\to \infty} f(x)=0$. We prove this as follows: Choose any $\varepsilon>0$ and set $$E_n = \{x\colon |f(mx)|\leq \varepsilon \text{ for all }m\geq n\}=\bigcap\limits_{m\in \mathbb N} \{x\colon mx\in f^{-1}([-\varepsilon,\varepsilon])\}.$$ Each of these sets will be closed by the continuity of $f$, and by the definition of our function we have that $\mathbb R_+ = \bigcup\limits_{n\in \mathbb N} E_n$, so that by Baire's theorem at least $E_n$ has nonempty interior. Thus we have an interval $I=[a,b]$, and some positive integer $n_0$ such that for all $n>n_0$, $f(nI)\subset [-\varepsilon,\varepsilon]$. We also have that if $n$ is big enough that $na<(n-1)b$ so that integer multiples of this interval will eventually cover the tail of the $\mathbb R_+$. All $x$ large enough will then be in a large integer multiple of $I$, and so we have $f(x)\in[-\varepsilon,\varepsilon]$. QED Now to prove the statement in the question, take $f$ to be $1$ at the square root of prime numbers, $0$ on $\mathbb R_+ \setminus \bigcup (\sqrt{p_n}-\varepsilon_n,\sqrt{p_n}+\varepsilon_n)$ and linear elsewhere. This function does not go to $0$ as $x\to \infty$ so we must have the existence of at least one point $a$ such that $$\lim_{m\to \infty}f(ma)\not\to 0.$$ It follows that $ma$ must wind up in infinitely many of the peaks centered about $\sqrt{p_n}$.",,"['real-analysis', 'combinatorics', 'number-theory']"
23,Elementary proof of $f>0$ implies $\int f>0$?,Elementary proof of  implies ?,f>0 \int f>0,"The question (Abbott, Understanding Analysis 2ed, 7.4.4) is: Show that if $f(x)>0$ for all $x\in[a,b]$ and $f$ is integrable, then $\int_a^b f>0$. I can show it using Baire's theorem (the sets $E_n=\{x: f(x)>1/n\}$ can't all be nowhere dense...), but that's optional in this book, and the Lebesgue characterization of integrable functions is two sections ahead.  Is there a way using not much more than the definition of Riemann integral?","The question (Abbott, Understanding Analysis 2ed, 7.4.4) is: Show that if $f(x)>0$ for all $x\in[a,b]$ and $f$ is integrable, then $\int_a^b f>0$. I can show it using Baire's theorem (the sets $E_n=\{x: f(x)>1/n\}$ can't all be nowhere dense...), but that's optional in this book, and the Lebesgue characterization of integrable functions is two sections ahead.  Is there a way using not much more than the definition of Riemann integral?",,['real-analysis']
24,To find the minimum value of $|z+1|+|z-1|+|z-i|$ where $z\in \Bbb C$.,To find the minimum value of  where .,|z+1|+|z-1|+|z-i| z\in \Bbb C,"To find the minimum value of $|z+1|+|z-1|+|z-i|$ where $z\in \Bbb C$. Options: $(A) \ 2$ $(B) \ 2\sqrt2$ $(C) \ 1+\sqrt3$ $(D) \ \sqrt5$ My logic is the sum will be minimum iff $z\in \Bbb C$ is any one of the three fixed points $1,-1,i$. And by calculation we see that the sum is min when take $z=i$.Is the solution correct? Know that its not a good solution to the problem....searching for an elegant one...Suggestion reqd.. One can apply Fermat-Torricelli point as given in solution below by Quang Hoang and it is a good solution to the problem geometrically.....but this can be applied only if I know the result...searching a solution of this from known basic results of analysis...","To find the minimum value of $|z+1|+|z-1|+|z-i|$ where $z\in \Bbb C$. Options: $(A) \ 2$ $(B) \ 2\sqrt2$ $(C) \ 1+\sqrt3$ $(D) \ \sqrt5$ My logic is the sum will be minimum iff $z\in \Bbb C$ is any one of the three fixed points $1,-1,i$. And by calculation we see that the sum is min when take $z=i$.Is the solution correct? Know that its not a good solution to the problem....searching for an elegant one...Suggestion reqd.. One can apply Fermat-Torricelli point as given in solution below by Quang Hoang and it is a good solution to the problem geometrically.....but this can be applied only if I know the result...searching a solution of this from known basic results of analysis...",,"['calculus', 'real-analysis', 'complex-analysis', 'optimization']"
25,How do i evaluate this sum $\sum\limits_{n=1}^{\infty} \frac{(-1)^{n+1}}{n^2n!}$?,How do i evaluate this sum ?,\sum\limits_{n=1}^{\infty} \frac{(-1)^{n+1}}{n^2n!},How do I evaluate this sum: $$\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n^2n!}$$ Note : The series converges by the ratio test. I have tried to use this sum:$$ \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}= \ln (2) $$ but I didn't succeed. Might there be others techniques which I don't know? Thank you for any help,How do I evaluate this sum: $$\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n^2n!}$$ Note : The series converges by the ratio test. I have tried to use this sum:$$ \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}= \ln (2) $$ but I didn't succeed. Might there be others techniques which I don't know? Thank you for any help,,"['real-analysis', 'sequences-and-series', 'summation']"
26,Example of distinctions between multiple integral and iterated integrals.,Example of distinctions between multiple integral and iterated integrals.,,"The following question is asked in the book of Analysis On Manifolds by Munkres, and given at page 103 as question 3. $Q=A\times B$,where $A$ is a rectangle in $\mathbb{R^k}$ and $B$ is a rectangle in $\mathbb{R^n}$. Give an example where $\int_{Q}f$ exists and one of the iterated integral $$\int_{x\in A}\int_{y\in B} f(x,y) \; \text{and} \; \int_{y\in B}\int_{x\in A} f(x,y)$$ exists, but the other does not. Find an example where both the iterated integrals of 1. exist, but the integral $\int_Q f$ does not. [Hint: One approach is to find a subset $S$ of $Q$ whose closure equals $Q$, such that $S$ contains at most one point on each vertical line and at most one point on each horizontal line.] I'm having difficulty coming up with examples for the second case using the hint. For the first one, I found $f(x,y)= 1/n$ if $y$ is rational and $x=m/n$, where $(m,n)=1$. However, I have no clue about the second one, and what the hint even means. I'd appreciate it if anyone can help me with this problem.","The following question is asked in the book of Analysis On Manifolds by Munkres, and given at page 103 as question 3. $Q=A\times B$,where $A$ is a rectangle in $\mathbb{R^k}$ and $B$ is a rectangle in $\mathbb{R^n}$. Give an example where $\int_{Q}f$ exists and one of the iterated integral $$\int_{x\in A}\int_{y\in B} f(x,y) \; \text{and} \; \int_{y\in B}\int_{x\in A} f(x,y)$$ exists, but the other does not. Find an example where both the iterated integrals of 1. exist, but the integral $\int_Q f$ does not. [Hint: One approach is to find a subset $S$ of $Q$ whose closure equals $Q$, such that $S$ contains at most one point on each vertical line and at most one point on each horizontal line.] I'm having difficulty coming up with examples for the second case using the hint. For the first one, I found $f(x,y)= 1/n$ if $y$ is rational and $x=m/n$, where $(m,n)=1$. However, I have no clue about the second one, and what the hint even means. I'd appreciate it if anyone can help me with this problem.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'riemann-integration']"
27,Modified Doob's $L^1$ inequality,Modified Doob's  inequality,L^1,"Let $X_n$ be a non-negative submartingale. Show that for all $\lambda >0$ $$ P(\sup_{k\leq n} X_n \geq 2\lambda) \leq \frac{1}{\lambda} \int_{X_n \geq \lambda} X_n dP$$ In Doob's weak $L^1$ inequality, the right hand side integrates over $\left\lbrace \sup_{k\leq n} X_n \geq 2\lambda \right\rbrace$. I have been stuck on part (1) already. Does anyone have idea how to solve this problem? Thanks! EDIT: Hint added, consider $Y_k = E(X_n 1_{X_n \geq \lambda} | F_k)$ At present I'm not quite sure how to apply this hint.","Let $X_n$ be a non-negative submartingale. Show that for all $\lambda >0$ $$ P(\sup_{k\leq n} X_n \geq 2\lambda) \leq \frac{1}{\lambda} \int_{X_n \geq \lambda} X_n dP$$ In Doob's weak $L^1$ inequality, the right hand side integrates over $\left\lbrace \sup_{k\leq n} X_n \geq 2\lambda \right\rbrace$. I have been stuck on part (1) already. Does anyone have idea how to solve this problem? Thanks! EDIT: Hint added, consider $Y_k = E(X_n 1_{X_n \geq \lambda} | F_k)$ At present I'm not quite sure how to apply this hint.",,"['real-analysis', 'probability', 'inequality', 'stochastic-processes']"
28,Evaluation of a tough double integral,Evaluation of a tough double integral,,"This is an integral coming from personal research, and very important to me, but it does not seem an easy job to do. If a solution is not possible then I'd be glad with a closed form only. $$\int_{[0,1]^2} \frac{(1-x-y+x y+x \log(x)-x y\log(x)+y \log(y)- x y\log(y)+x y\log(x)\log(y))\log(1+x y)}{x y (1-x) (1-y)\log(x)\log(y)} \ dx \ dy$$ Hopefully this will be seen by Cleo too and maybe we'll find its closed form.","This is an integral coming from personal research, and very important to me, but it does not seem an easy job to do. If a solution is not possible then I'd be glad with a closed form only. $$\int_{[0,1]^2} \frac{(1-x-y+x y+x \log(x)-x y\log(x)+y \log(y)- x y\log(y)+x y\log(x)\log(y))\log(1+x y)}{x y (1-x) (1-y)\log(x)\log(y)} \ dx \ dy$$ Hopefully this will be seen by Cleo too and maybe we'll find its closed form.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
29,"It is easy to show that $S_m=\sum_{n=1}^\infty \frac{n}{2^n + m}$ converges for any natural$\ m$, but what is its value?","It is easy to show that  converges for any natural, but what is its value?",S_m=\sum_{n=1}^\infty \frac{n}{2^n + m} \ m,"In fact the series would converge even if$\ m$ were not natural, I just wanted to state that it is natural in my case. I have found the partial sum formula of$\ S_0$,$\displaystyle \sum_{n=1}^k \frac{n}{2^n} =\frac{2^{k+1}-k-2}{2^k}$, thus easily obtaining$\ S_0=2$. Then, since $\displaystyle \frac{1}{2^n+m}=\frac{1}{2^n}-\frac{m}{2^n\left(2^n+m\right)}$, I know $\displaystyle S_m= 2-\sum_{n=1}^\infty \frac{mn}{2^n\left(2^n+m\right)}$, though I'm not sure it is a convenient path to study the last series.","In fact the series would converge even if$\ m$ were not natural, I just wanted to state that it is natural in my case. I have found the partial sum formula of$\ S_0$,$\displaystyle \sum_{n=1}^k \frac{n}{2^n} =\frac{2^{k+1}-k-2}{2^k}$, thus easily obtaining$\ S_0=2$. Then, since $\displaystyle \frac{1}{2^n+m}=\frac{1}{2^n}-\frac{m}{2^n\left(2^n+m\right)}$, I know $\displaystyle S_m= 2-\sum_{n=1}^\infty \frac{mn}{2^n\left(2^n+m\right)}$, though I'm not sure it is a convenient path to study the last series.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'proof-writing']"
30,"If the integral of a non-negative function is $0$, then the function is $0$","If the integral of a non-negative function is , then the function is",0 0,"Suppose that $f$ is a continuous function on $[a,b]$ and that $f(x)\geq0$ for all $x\in [a,b]$. Show that if $\int_a^bf(x)=0$, then $f(x)=0$ for all $x\in[a,b]$. Let $F(x)=\int_a^xf(x)$. Since $\int_a^xf(x)+\int_x^bf(x)=\int_a^bf(x)=0$ for every $x\in[a,b]$, we have $\int_a^xf(x)=-\int_x^bf(x)$. But since $f(x)\geq0$ for all $x\in[a,b]$, both $\int_a^xf(x)$ and $\int_x^bf(x)$ are positive. So $F(x)=0$ for all $x\in[a,b]$ and by the fundamental theorem of calculus, $f(x)=F'(x)=0$ for all $x\in[a,b]$. Is this a valid proof?","Suppose that $f$ is a continuous function on $[a,b]$ and that $f(x)\geq0$ for all $x\in [a,b]$. Show that if $\int_a^bf(x)=0$, then $f(x)=0$ for all $x\in[a,b]$. Let $F(x)=\int_a^xf(x)$. Since $\int_a^xf(x)+\int_x^bf(x)=\int_a^bf(x)=0$ for every $x\in[a,b]$, we have $\int_a^xf(x)=-\int_x^bf(x)$. But since $f(x)\geq0$ for all $x\in[a,b]$, both $\int_a^xf(x)$ and $\int_x^bf(x)$ are positive. So $F(x)=0$ for all $x\in[a,b]$ and by the fundamental theorem of calculus, $f(x)=F'(x)=0$ for all $x\in[a,b]$. Is this a valid proof?",,"['real-analysis', 'definite-integrals', 'proof-verification', 'solution-verification']"
31,Integral in $n-$dimensional euclidean space,Integral in dimensional euclidean space,n-,"I want to calculate this integral in $n$-dimensional euclidean space. $$I(x)=\int_{\mathbb{R}^n}\frac{d^n k}{(2\pi)^n}\frac{e^{i(k\cdot x)}}{k^2+a^2},$$ where $k^2=(k\cdot k)$, $k=(k_1,\ldots,k_n)\in\mathbb{R}^n$, $x=(x_1,\ldots,x_n)\in\mathbb{R}^n$,$a\in \mathbb{R}$. I've done this integral for $n=3$ by spherical coordinates and residue theorem. I have $$I(r)=\frac{1}{4\pi r}e^{-ar},$$ where $r=|x|$ But in $n$- dimensions I failed in using spherical coordinates , because I have never done it before. Also I see that this integral is Fourier transform of $\frac{1}{k^2+a^2}$, but I failed here too, because I can't find Fourier pair in my reference books. If someone could guide me in this integration it would be great.","I want to calculate this integral in $n$-dimensional euclidean space. $$I(x)=\int_{\mathbb{R}^n}\frac{d^n k}{(2\pi)^n}\frac{e^{i(k\cdot x)}}{k^2+a^2},$$ where $k^2=(k\cdot k)$, $k=(k_1,\ldots,k_n)\in\mathbb{R}^n$, $x=(x_1,\ldots,x_n)\in\mathbb{R}^n$,$a\in \mathbb{R}$. I've done this integral for $n=3$ by spherical coordinates and residue theorem. I have $$I(r)=\frac{1}{4\pi r}e^{-ar},$$ where $r=|x|$ But in $n$- dimensions I failed in using spherical coordinates , because I have never done it before. Also I see that this integral is Fourier transform of $\frac{1}{k^2+a^2}$, but I failed here too, because I can't find Fourier pair in my reference books. If someone could guide me in this integration it would be great.",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'special-functions']"
32,On continuity of roots of a polynomial depending on a real parameter,On continuity of roots of a polynomial depending on a real parameter,,"Problem Suppose $f^{(t)}(z)=a_0^{(t)}+\dotsb+a_{n-1}^{(t)}z^{n-1}+z^n\in\mathbb C[z]$ for all $t\in\mathbb R$, where $a_0^{(t)},\dotsc,a_{n-1}^{(t)}\colon\mathbb R\to\mathbb C$ are continuous on $t$.   Is it true that there is always a complex-valued continuous function $\phi^{(t)}\colon\mathbb R\to\mathbb C$ such that $f^{(t)}(\phi^{(t)})=0\;(\forall t\in\mathbb R)$? Topologically, Let $S=\big\{\,(a_0,\dotsc,a_{n-1},z)\in\mathbb C^{n+1}\,\big\vert\,a_0+a_1z+\dotsb+a_{n-1}z^{n-1}+z^n=0\,\big\}$ and $\pi\colon S\to\mathbb C^n,(a_0,\dotsc,a_{n-1},z)\mapsto(a_0,\dotsc,a_{n-1})$. Is it true that $\pi$ has path lifting property (i.e. for each continuous map $p\colon[0,1]\to\mathbb C^n$, there exists a continuous map $\tilde p\colon[0,1]\to S$ such that $p=\pi\circ\tilde p$? (Sorry, I cannot find a good reference for that term. In my definition, there's no assumption of uniqueness.) Or algebraically, Let $R=\mathcal C(\mathbb R,\mathbb C)$ denote the ring of complexed-valued real continuous functions. Is it true that any monic polynomial over $R$ has a root in $R$? Discussion It's certainly true that there is a function $\phi_{t_0}^{(t)}$ continuous at $t=t_0$ such that $f(\phi_{t_0}^{(t)})=0\;(\forall t\in\mathbb R)$, no matter whether $t$ is a real or complex parameter, or a parameter from some Hausdorff space. It follows directly from, say, Rouché's theorem . For an elementary proof, see Michael Artin's Algebra , proposition 5.2.1(b). A sharper proposition of the original problem , i.e. replacing the real parameter $t\in\mathbb R$ with a complex parameter $w\in\mathbb C$, is generally demonstrably false, i.e. there could be no continuous function to be a root of the polynomial. Here's a simple counterexample: $f^{(w)}(z)=z^2-w$. Note that there's a branch point at $w=0$, and if we draw an arbitrary circle around the origin in the $w$-plane, we'll see that a root shouldn't be continuous on the whole $w$-plane $\mathbb C$, for otherwise letting $w$ travel the circle would lead to a contradiction. It seems true when $t$ is a real parameter, since the dimension is lower. I have no idea how to attack this. I expect your grateful ideas or hints. Thanks! Postscript There's an old post related, inequivalent but informative and interesting.","Problem Suppose $f^{(t)}(z)=a_0^{(t)}+\dotsb+a_{n-1}^{(t)}z^{n-1}+z^n\in\mathbb C[z]$ for all $t\in\mathbb R$, where $a_0^{(t)},\dotsc,a_{n-1}^{(t)}\colon\mathbb R\to\mathbb C$ are continuous on $t$.   Is it true that there is always a complex-valued continuous function $\phi^{(t)}\colon\mathbb R\to\mathbb C$ such that $f^{(t)}(\phi^{(t)})=0\;(\forall t\in\mathbb R)$? Topologically, Let $S=\big\{\,(a_0,\dotsc,a_{n-1},z)\in\mathbb C^{n+1}\,\big\vert\,a_0+a_1z+\dotsb+a_{n-1}z^{n-1}+z^n=0\,\big\}$ and $\pi\colon S\to\mathbb C^n,(a_0,\dotsc,a_{n-1},z)\mapsto(a_0,\dotsc,a_{n-1})$. Is it true that $\pi$ has path lifting property (i.e. for each continuous map $p\colon[0,1]\to\mathbb C^n$, there exists a continuous map $\tilde p\colon[0,1]\to S$ such that $p=\pi\circ\tilde p$? (Sorry, I cannot find a good reference for that term. In my definition, there's no assumption of uniqueness.) Or algebraically, Let $R=\mathcal C(\mathbb R,\mathbb C)$ denote the ring of complexed-valued real continuous functions. Is it true that any monic polynomial over $R$ has a root in $R$? Discussion It's certainly true that there is a function $\phi_{t_0}^{(t)}$ continuous at $t=t_0$ such that $f(\phi_{t_0}^{(t)})=0\;(\forall t\in\mathbb R)$, no matter whether $t$ is a real or complex parameter, or a parameter from some Hausdorff space. It follows directly from, say, Rouché's theorem . For an elementary proof, see Michael Artin's Algebra , proposition 5.2.1(b). A sharper proposition of the original problem , i.e. replacing the real parameter $t\in\mathbb R$ with a complex parameter $w\in\mathbb C$, is generally demonstrably false, i.e. there could be no continuous function to be a root of the polynomial. Here's a simple counterexample: $f^{(w)}(z)=z^2-w$. Note that there's a branch point at $w=0$, and if we draw an arbitrary circle around the origin in the $w$-plane, we'll see that a root shouldn't be continuous on the whole $w$-plane $\mathbb C$, for otherwise letting $w$ travel the circle would lead to a contradiction. It seems true when $t$ is a real parameter, since the dimension is lower. I have no idea how to attack this. I expect your grateful ideas or hints. Thanks! Postscript There's an old post related, inequivalent but informative and interesting.",,"['real-analysis', 'complex-analysis', 'algebraic-topology', 'continuity']"
33,Does the implicit function theorem imply Peano existence theorem,Does the implicit function theorem imply Peano existence theorem,,"In The implicit function theorem written by Krantz & Parks, it's said that the implicit function theorem implies the following existence theorem of ODE: Theorem 4.1.1 If $F(t,x)$, $(t,x)\in\mathbb R\times\mathbb R^N$, is continuous in the $(N+1)$-dimensional region $(t_0-a,t_0+a)\times B(x_0,r)$, then there exists a solution $x(t)$ of   $$\frac{dx}{dt}=F(t,x),\qquad x(t_0)=x_0$$   defined over an interval $(t_0-h,t_0+h)$. It's Peano existence theorem, I think. However, there seems a gap in their proof. WLOG, suppose $t_0=0$. They constructed $\mathcal H\colon[0,1]\times\mathcal B_1\to\mathcal B_0\times\mathbb R$, where $\mathcal B_0$ is the space of bounded continuous $\mathbb R^N$-valued functions on $(-a,a)$ normed canonically, and $\mathcal B_1$ is the space of bounded continuously differentiable $\mathbb R^N$-valued functions on $(-a,a)$ that also have a bounded derivative, normed canonically by $\sup\lvert f\rvert+\sup\lvert\dot f\rvert$, as follows: $$\mathcal H[\alpha,X(\tau)]=[X'(\tau)-\alpha F(\alpha\tau,X(\tau)),X(0)-x_0]$$. Note that $\mathcal H[0,x_0]=[0,0]$, where $x_0$ on the left side denotes the constant function. Then they claim that the existence theorem follows from the implicit function theorem. However, under the only condition that $F$ is continuous, there's no evidence that $\mathcal H$ is partially differentiable with respect to $X$ for $\alpha\in(0,x_0)$. Can we fix the preceding proof in some extent? PS: I posted the question not only because I want to comprehend such a proof, but also want to understand the relation between ODE and implicit functions. It seems certain that such a proof cannot be alright, since the canonical implicit function theorem is also a uniqueness theorem, which implies the local uniqueness of a solution of ODE. However, I want to know how to fix it. I doubt it might rely on a more general implicit function theorem.","In The implicit function theorem written by Krantz & Parks, it's said that the implicit function theorem implies the following existence theorem of ODE: Theorem 4.1.1 If $F(t,x)$, $(t,x)\in\mathbb R\times\mathbb R^N$, is continuous in the $(N+1)$-dimensional region $(t_0-a,t_0+a)\times B(x_0,r)$, then there exists a solution $x(t)$ of   $$\frac{dx}{dt}=F(t,x),\qquad x(t_0)=x_0$$   defined over an interval $(t_0-h,t_0+h)$. It's Peano existence theorem, I think. However, there seems a gap in their proof. WLOG, suppose $t_0=0$. They constructed $\mathcal H\colon[0,1]\times\mathcal B_1\to\mathcal B_0\times\mathbb R$, where $\mathcal B_0$ is the space of bounded continuous $\mathbb R^N$-valued functions on $(-a,a)$ normed canonically, and $\mathcal B_1$ is the space of bounded continuously differentiable $\mathbb R^N$-valued functions on $(-a,a)$ that also have a bounded derivative, normed canonically by $\sup\lvert f\rvert+\sup\lvert\dot f\rvert$, as follows: $$\mathcal H[\alpha,X(\tau)]=[X'(\tau)-\alpha F(\alpha\tau,X(\tau)),X(0)-x_0]$$. Note that $\mathcal H[0,x_0]=[0,0]$, where $x_0$ on the left side denotes the constant function. Then they claim that the existence theorem follows from the implicit function theorem. However, under the only condition that $F$ is continuous, there's no evidence that $\mathcal H$ is partially differentiable with respect to $X$ for $\alpha\in(0,x_0)$. Can we fix the preceding proof in some extent? PS: I posted the question not only because I want to comprehend such a proof, but also want to understand the relation between ODE and implicit functions. It seems certain that such a proof cannot be alright, since the canonical implicit function theorem is also a uniqueness theorem, which implies the local uniqueness of a solution of ODE. However, I want to know how to fix it. I doubt it might rely on a more general implicit function theorem.",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'calculus-of-variations', 'implicit-function-theorem']"
34,Operator norm of a convolution,Operator norm of a convolution,,"Consider the operator on $L^2(\Bbb R)$, $f\rightarrow f*g$, where $g\geq 0$ is some $L^1$ function. Show the operator is a bounded linear operator with operator norm equal to $||g||_1$. Showing this actually is a linear operator was not hard, and the operator norm is less than $||g||_1$ by Young's inequality, but I have had no luck trying to show the operator norm is actually $||g||_1$.","Consider the operator on $L^2(\Bbb R)$, $f\rightarrow f*g$, where $g\geq 0$ is some $L^1$ function. Show the operator is a bounded linear operator with operator norm equal to $||g||_1$. Showing this actually is a linear operator was not hard, and the operator norm is less than $||g||_1$ by Young's inequality, but I have had no luck trying to show the operator norm is actually $||g||_1$.",,"['real-analysis', 'normed-spaces', 'lp-spaces']"
35,Exercise 6.9 in Rudin's RCA (Real and Complex Analysis),Exercise 6.9 in Rudin's RCA (Real and Complex Analysis),,"The following is an exercise 6.9 in Rudin's Real and Complex Analysis: Suppose that $\{ g_n \}$ is a sequence of positive continuous functions on $I=[0,1]$, that $\mu$ is a positive Borel measure on $I$, and that (i) lim$_{n\to \infty}$ $g_n (x) = 0$ a.e. [m], (ii) $\int_I g_n dm = 1$ for all $n$, (iii) lim$_{n\to \infty}$ $\int_I fg_n dm = \int_I f d\mu$ for every $f\in C(I)$. Does it follow that $\mu \perp m$? I think that the answer is positive. $\{g_n\}$ seems to be something similar to good kernel. I tried to use Egoroff's theorem and then derive something useful, but couldn't. Would you please give me some help?","The following is an exercise 6.9 in Rudin's Real and Complex Analysis: Suppose that $\{ g_n \}$ is a sequence of positive continuous functions on $I=[0,1]$, that $\mu$ is a positive Borel measure on $I$, and that (i) lim$_{n\to \infty}$ $g_n (x) = 0$ a.e. [m], (ii) $\int_I g_n dm = 1$ for all $n$, (iii) lim$_{n\to \infty}$ $\int_I fg_n dm = \int_I f d\mu$ for every $f\in C(I)$. Does it follow that $\mu \perp m$? I think that the answer is positive. $\{g_n\}$ seems to be something similar to good kernel. I tried to use Egoroff's theorem and then derive something useful, but couldn't. Would you please give me some help?",,"['real-analysis', 'analysis']"
36,Evaluating $\int_0^\infty \frac{1}{x+1-u}\cdot \frac{\mathrm{d}x}{\log^2 x+\pi^2}$ using real methods.,Evaluating  using real methods.,\int_0^\infty \frac{1}{x+1-u}\cdot \frac{\mathrm{d}x}{\log^2 x+\pi^2},"By reading a german wikipedia ( see here ) about integrals, i stumpled upon this entry 27 1.5 $$ \color{black}{ \int_0^\infty \frac{1}{x+1-u}\cdot \frac{\mathrm{d}x}{\log^2 x+\pi^2} =\frac{1}{u}+\frac{1}{\log(1-u)}\,, \qquad u \in (0,1)} $$ ( Click for the source ) Where the result was proven using complex analysis. Is there any method to show the equality using real methods? Any help will be appreciated =)","By reading a german wikipedia ( see here ) about integrals, i stumpled upon this entry 27 1.5 $$ \color{black}{ \int_0^\infty \frac{1}{x+1-u}\cdot \frac{\mathrm{d}x}{\log^2 x+\pi^2} =\frac{1}{u}+\frac{1}{\log(1-u)}\,, \qquad u \in (0,1)} $$ ( Click for the source ) Where the result was proven using complex analysis. Is there any method to show the equality using real methods? Any help will be appreciated =)",,"['real-analysis', 'integration', 'improper-integrals']"
37,$L^{p}$ functions from Rudin Exercises 3.5,functions from Rudin Exercises 3.5,L^{p},"I am attempting a question from Rudin's ""Real and Complex Analysis"" Chapter 3 question 5. I shall summarise the question as below: Suppose that $f$ is a complex measurable function on $X$, $\mu$ a positive measure on $X$. Also, assume that $\mu(X)=1$. 1) If $0<r<s\leq\infty$, when does $||f||_{r}=||f||_{s}<\infty$ hold? My hunch is that this happens when $f$ is a constant function, but I can't prove it. 2) Assume that $||f||_{r}<\infty$ for some $r>0$, prove that \begin{eqnarray} \lim_{p\rightarrow 0}||f||_{p}=\exp\left\{\int_{X}\log|f|d\mu\right\} \end{eqnarray} where $\exp(-\infty)$ is defined to be $0$. For this, I have proven that $||f||_{p}$ is bounded below by the right hand side expression, but I am stuck here. Any help is appreciated! Thanks!","I am attempting a question from Rudin's ""Real and Complex Analysis"" Chapter 3 question 5. I shall summarise the question as below: Suppose that $f$ is a complex measurable function on $X$, $\mu$ a positive measure on $X$. Also, assume that $\mu(X)=1$. 1) If $0<r<s\leq\infty$, when does $||f||_{r}=||f||_{s}<\infty$ hold? My hunch is that this happens when $f$ is a constant function, but I can't prove it. 2) Assume that $||f||_{r}<\infty$ for some $r>0$, prove that \begin{eqnarray} \lim_{p\rightarrow 0}||f||_{p}=\exp\left\{\int_{X}\log|f|d\mu\right\} \end{eqnarray} where $\exp(-\infty)$ is defined to be $0$. For this, I have proven that $||f||_{p}$ is bounded below by the right hand side expression, but I am stuck here. Any help is appreciated! Thanks!",,"['real-analysis', 'probability-theory', 'lebesgue-integral', 'lp-spaces']"
38,Asymptotic Expansion of an Oscillating Integral,Asymptotic Expansion of an Oscillating Integral,,"Let $g(x):\mathbb{R}_{\geq0}\rightarrow\mathbb{R}$ be real analytic s.t. $g(0)\neq 0$ and $g(x)=O(x^{-2})$ as $x\rightarrow\infty$. What is the leading order in $\lambda$ as $\lambda\rightarrow 0$ of the following integral? $$ I(\lambda) = \int_0^{\infty}dx \cos\left(\frac{x}{\lambda}\right)x\log(x)g(x) $$ I think it should be $I(\lambda)\sim -\lambda^2(\log\lambda) g(0)$ based on integration by parts, but I haven't got a complete argument. Edit: Integrating by parts and throwing away subleading terms easily shows that it suffices to prove that $$ J(\lambda) = \int_0^{\infty}dy \sin\left(y\right)\log(y)g(\lambda y) $$ is bounded as $\lambda\rightarrow 0$ under the above conditions on $g$. Numerical experiments suggest this to be the case. Can somebody prove it, please?","Let $g(x):\mathbb{R}_{\geq0}\rightarrow\mathbb{R}$ be real analytic s.t. $g(0)\neq 0$ and $g(x)=O(x^{-2})$ as $x\rightarrow\infty$. What is the leading order in $\lambda$ as $\lambda\rightarrow 0$ of the following integral? $$ I(\lambda) = \int_0^{\infty}dx \cos\left(\frac{x}{\lambda}\right)x\log(x)g(x) $$ I think it should be $I(\lambda)\sim -\lambda^2(\log\lambda) g(0)$ based on integration by parts, but I haven't got a complete argument. Edit: Integrating by parts and throwing away subleading terms easily shows that it suffices to prove that $$ J(\lambda) = \int_0^{\infty}dy \sin\left(y\right)\log(y)g(\lambda y) $$ is bounded as $\lambda\rightarrow 0$ under the above conditions on $g$. Numerical experiments suggest this to be the case. Can somebody prove it, please?",,"['real-analysis', 'integration', 'asymptotics']"
39,Does continuous convergence imply uniform convergence?,Does continuous convergence imply uniform convergence?,,"Question Related to a nice problem I met yesterday, a question arises: Suppose $\{f_n\}$ is a sequence of mappings from a connected complete metric space $X$ to a metric space $Y$. Given $f\colon X\to Y$ such that $f_n(x_n)\to f(x)$ whenever a real sequence $x_n\to x$. Must $\{f_n\}$ converge uniformly? According to Canez's hint , it's true that $f_n\to f$ pointwise and $f$ is continuous (modifying my ugly answer , replace $+1/N_k$ with $+0$) even if $X$ is an arbitrary metric space (without assuming that $X$ is connected or complete). If it's too abstract, we can consider a concrete one, such as $X=Y=\mathbb R$ or $X=Y=\mathbb C$. Explanations If $X$ is compact, it's true that $f_n\to f$ uniformly. Prove by contradiction. Suppose $\exists\epsilon_0>0,\forall N,\exists n\ge N$ such that $$\sup_{x\in X}d_Y(f_n(x),f(x))>\epsilon_0$$ We can choose a increasing sequence $\{m_k\}\subset\mathbb N$, and a sequence $\{x_{m_k}\}\subset X$, such that $d_Y(f_{m_k}(x_{m_k}),f(x_{m_k}))>\epsilon_0$ for all $k$. Since $X$ is compact, there's a subsequence $\{n_k\}$ of $\{m_k\}$ such that $x_{n_k}\to x$ for some $x\in X$, then we can show that $f_{n_k}(x_{n_k})\to f(x)$ by constructing a sequence with duplicated $x_{n_k}$'s, just like this . On the other hand, since $f$ is continuous, we have $f(x_{n_k})\to f(x)$ contradicting to $d_Y(f_n(x),f(x))>\epsilon_0$. In contrary to the preceding statement, if $X$ isn't connected (or at least, with finite connected components), even if $X$ is complete, $f_n\to f$ needn't be uniform, such as $X=\mathbb Z$. Generalization I don't know whether $X$ could be replaced with an arbitrary connected metric space, or even $X$, $Y$ are just Hausdorff spaces. I don't know whether the preceding compact metric space $X$ could be replaced with a compact Hausdorff space. It's out of my ability and I can't discuss more.","Question Related to a nice problem I met yesterday, a question arises: Suppose $\{f_n\}$ is a sequence of mappings from a connected complete metric space $X$ to a metric space $Y$. Given $f\colon X\to Y$ such that $f_n(x_n)\to f(x)$ whenever a real sequence $x_n\to x$. Must $\{f_n\}$ converge uniformly? According to Canez's hint , it's true that $f_n\to f$ pointwise and $f$ is continuous (modifying my ugly answer , replace $+1/N_k$ with $+0$) even if $X$ is an arbitrary metric space (without assuming that $X$ is connected or complete). If it's too abstract, we can consider a concrete one, such as $X=Y=\mathbb R$ or $X=Y=\mathbb C$. Explanations If $X$ is compact, it's true that $f_n\to f$ uniformly. Prove by contradiction. Suppose $\exists\epsilon_0>0,\forall N,\exists n\ge N$ such that $$\sup_{x\in X}d_Y(f_n(x),f(x))>\epsilon_0$$ We can choose a increasing sequence $\{m_k\}\subset\mathbb N$, and a sequence $\{x_{m_k}\}\subset X$, such that $d_Y(f_{m_k}(x_{m_k}),f(x_{m_k}))>\epsilon_0$ for all $k$. Since $X$ is compact, there's a subsequence $\{n_k\}$ of $\{m_k\}$ such that $x_{n_k}\to x$ for some $x\in X$, then we can show that $f_{n_k}(x_{n_k})\to f(x)$ by constructing a sequence with duplicated $x_{n_k}$'s, just like this . On the other hand, since $f$ is continuous, we have $f(x_{n_k})\to f(x)$ contradicting to $d_Y(f_n(x),f(x))>\epsilon_0$. In contrary to the preceding statement, if $X$ isn't connected (or at least, with finite connected components), even if $X$ is complete, $f_n\to f$ needn't be uniform, such as $X=\mathbb Z$. Generalization I don't know whether $X$ could be replaced with an arbitrary connected metric space, or even $X$, $Y$ are just Hausdorff spaces. I don't know whether the preceding compact metric space $X$ could be replaced with a compact Hausdorff space. It's out of my ability and I can't discuss more.",,"['real-analysis', 'general-topology', 'metric-spaces']"
40,Derivative and constant function,Derivative and constant function,,"Problem Suppose that $f:(a,b)\to\Bbb R$ is differentiable and $f^\prime(x)=0$ on $D$ , where $D$ is dense on $(a,b)$ . Can we conclude that $f$ is a constant function? Background In calculus course, I'm told that there's a theorem stated when $D=(a,b)$ . In fact, it's reducible. For example, if $D=(a,b)\backslash C$ where $C$ is at most countable, we have $f^\prime(x)=0$ for all $x\in(a,b)$ , because if $k_0=f^\prime(x_0)\neq0$ for some $x_0\in C$ , there's some $\xi\in C$ such that $f^\prime(\xi)=\eta$ for all $0\le\eta\le k_0$ , so $C$ is uncountable.","Problem Suppose that is differentiable and on , where is dense on . Can we conclude that is a constant function? Background In calculus course, I'm told that there's a theorem stated when . In fact, it's reducible. For example, if where is at most countable, we have for all , because if for some , there's some such that for all , so is uncountable.","f:(a,b)\to\Bbb R f^\prime(x)=0 D D (a,b) f D=(a,b) D=(a,b)\backslash C C f^\prime(x)=0 x\in(a,b) k_0=f^\prime(x_0)\neq0 x_0\in C \xi\in C f^\prime(\xi)=\eta 0\le\eta\le k_0 C","['calculus', 'real-analysis']"
41,Gradient nonzero extensions of a vector field on the circle,Gradient nonzero extensions of a vector field on the circle,,"Let $\mathbf{v}=(a,b)$ be a smooth vector field on the unit circle $\mathbb{S}^{1}$ such that $a^{2}+b^{2}\neq0$ everywhere in $\mathbb{S}^{1}$ with degree $\deg\mathbf{v}=0$. Suppose also that $\int\limits_{\mathbb{S} ^{1}}a\mathtt{dx}+b\mathtt{dy}=0$. My question is whether the field $\mathbf{v}$ may be extended to a nonzero gradient vector field $\overline {\mathbf{v}}=(A,B)$ on the unit disk $\mathbb{D}$, i.e. whether there exist smooth functions $A=A(x,y)$, $B=B(x,y)$, $\ (x,y)\in\mathbb{D}$, such that $A|_{\mathbb{S}^{1}}=a$, $B|_{\mathbb{S}^{1}}=b$,  $A^{2}+B^{2}\neq0$ everywhere in $\mathbb{D}$ and finally $\frac{\partial B}{\partial x} =\frac{\partial A}{\partial y}$ in $\mathbb{D}$. Let me make some remarks. The condition $\deg\mathbf{v}=0$ is necessary, for the field $\mathbf{v}$ to have an everywhere nonzero extension in the unit disk. The degree is defined as usually as the degree of  $\mathbf{v/}\left\Vert \mathbf{v} \right\Vert $ considered as a map $\mathbb{S}^{1}\rightarrow\mathbb{S}^{1}$. The condition $\int\limits_{\mathbb{S}^{1}}a\mathtt{dx}+b\mathtt{dy}=0$ is also necessary for $\mathbf{v}$ to have a gradient extension $\overline {\mathbf{v}}$, as following by Green's Theorem. I suppose that this proposition should have some elegant proof (if true :)) and may be probably something well-known, but I have only few examples, not a proof. So, any references are welcome as well. Note also that this is somehow a ""global"" proposition, not a ""local"" one. Thanks in advance.","Let $\mathbf{v}=(a,b)$ be a smooth vector field on the unit circle $\mathbb{S}^{1}$ such that $a^{2}+b^{2}\neq0$ everywhere in $\mathbb{S}^{1}$ with degree $\deg\mathbf{v}=0$. Suppose also that $\int\limits_{\mathbb{S} ^{1}}a\mathtt{dx}+b\mathtt{dy}=0$. My question is whether the field $\mathbf{v}$ may be extended to a nonzero gradient vector field $\overline {\mathbf{v}}=(A,B)$ on the unit disk $\mathbb{D}$, i.e. whether there exist smooth functions $A=A(x,y)$, $B=B(x,y)$, $\ (x,y)\in\mathbb{D}$, such that $A|_{\mathbb{S}^{1}}=a$, $B|_{\mathbb{S}^{1}}=b$,  $A^{2}+B^{2}\neq0$ everywhere in $\mathbb{D}$ and finally $\frac{\partial B}{\partial x} =\frac{\partial A}{\partial y}$ in $\mathbb{D}$. Let me make some remarks. The condition $\deg\mathbf{v}=0$ is necessary, for the field $\mathbf{v}$ to have an everywhere nonzero extension in the unit disk. The degree is defined as usually as the degree of  $\mathbf{v/}\left\Vert \mathbf{v} \right\Vert $ considered as a map $\mathbb{S}^{1}\rightarrow\mathbb{S}^{1}$. The condition $\int\limits_{\mathbb{S}^{1}}a\mathtt{dx}+b\mathtt{dy}=0$ is also necessary for $\mathbf{v}$ to have a gradient extension $\overline {\mathbf{v}}$, as following by Green's Theorem. I suppose that this proposition should have some elegant proof (if true :)) and may be probably something well-known, but I have only few examples, not a proof. So, any references are welcome as well. Note also that this is somehow a ""global"" proposition, not a ""local"" one. Thanks in advance.",,"['real-analysis', 'differential-topology']"
42,Let a real measurable function $f$ map every open set to the whole real line. Is there always a restriction to a set of measure zero doing the same?,Let a real measurable function  map every open set to the whole real line. Is there always a restriction to a set of measure zero doing the same?,f,"Given $f:\mathbb{R}\rightarrow\mathbb{R}$ mapping each non-empty open set to the whole real line, is there always a set $A$ of measure zero such that the function $g:\mathbb{R}\rightarrow\mathbb{R}$ defined as $g(x)=f(x)$ on $A$ and $0$ everywhere else also maps every non-empty open set to the whole real line? Edit: My idea is to use the sigmoid function to convert it into the equivalent problem about functions with domain $(0,1)$ and work in binary. Consider the set $B \subseteq (0,1)$ defined as the union over all $n$ of values in $(0,1)$ whose $n$ th to $n+k$ th binary places are 0, for some $k$ depending on $n$ . This strategy lets us restrict to a set of arbirtrarily small positive measure. Can this idea be extended by incorporating the actual $f$ to get a let us restrict to a set of measure $0$ ?","Given mapping each non-empty open set to the whole real line, is there always a set of measure zero such that the function defined as on and everywhere else also maps every non-empty open set to the whole real line? Edit: My idea is to use the sigmoid function to convert it into the equivalent problem about functions with domain and work in binary. Consider the set defined as the union over all of values in whose th to th binary places are 0, for some depending on . This strategy lets us restrict to a set of arbirtrarily small positive measure. Can this idea be extended by incorporating the actual to get a let us restrict to a set of measure ?","f:\mathbb{R}\rightarrow\mathbb{R} A g:\mathbb{R}\rightarrow\mathbb{R} g(x)=f(x) A 0 (0,1) B \subseteq (0,1) n (0,1) n n+k k n f 0","['real-analysis', 'measure-theory', 'lebesgue-measure']"
43,Are these two function spaces identical?,Are these two function spaces identical?,,"Let the function space $A$ denote all functions $f : [0, 1) \to [0, 1)$ such that, for some set $Z$ of Lebesgue measure zero, the derivative $f'$ exists on $[0, 1) \setminus Z$ and $|f'| = 1$ there. Let the function space $B$ denote all functions $f : [0, 1) \to [0, 1)$ such that for all $a < b$ in $[0, 1)$ , the portion $G_f(a,b) = \{(x,f(x)) ∈ \mathbb R^2 \mid a \le x < b\}$ of the graph of $f$ that lies above $[a, b)$ has Hausdorff $1$ -measure $H_1(G_f(a,b))$ equal to $\sqrt 2(b - a)$ . (Note that we do not assume functions to be continuous.) Does $A = B$ ? Edit: The following is false, as was shown to me by Christian Remling by an easy application of the Cantor function: ""It is clear to me that $A$ is a subset of $B$ ."" (Wrong!) So the real question is whether the reverse inclusion holds.","Let the function space denote all functions such that, for some set of Lebesgue measure zero, the derivative exists on and there. Let the function space denote all functions such that for all in , the portion of the graph of that lies above has Hausdorff -measure equal to . (Note that we do not assume functions to be continuous.) Does ? Edit: The following is false, as was shown to me by Christian Remling by an easy application of the Cantor function: ""It is clear to me that is a subset of ."" (Wrong!) So the real question is whether the reverse inclusion holds.","A f : [0, 1) \to [0, 1) Z f' [0, 1) \setminus Z |f'| = 1 B f : [0, 1) \to [0, 1) a < b [0, 1) G_f(a,b) = \{(x,f(x)) ∈ \mathbb R^2 \mid a \le x < b\} f [a, b) 1 H_1(G_f(a,b)) \sqrt 2(b - a) A = B A B","['real-analysis', 'measure-theory', 'geometric-measure-theory']"
44,For what real values of $a$ does $\sum\limits_{k=1}^\infty k^{-(1+|\sin k|^a)}$ converge? [duplicate],For what real values of  does  converge? [duplicate],a \sum\limits_{k=1}^\infty k^{-(1+|\sin k|^a)},"This question already has an answer here : Does $\sum_{n=1}^\infty n^{-1-|\sin n|^a}$ converge for some $a\in(0,1)$? (1 answer) Closed 11 months ago . I was thinking about the $p$ -series , and came up with this question: For what real values of $a$ does $S=\sum\limits_{k=1}^\infty \dfrac{1}{k^{1+|\sin k|^a}}$ converge? My attempt: I know that if $a\le 0 $ , then $S\le\sum\limits_{k=1}^\infty \dfrac{1}{k^{2}}=\pi^2/6$ , so $S$ converges. If $a>0$ , the standard tests do not work: The $n$ th term divergence test is inconclusive, because the terms approach $0$ . The comparison test with a $p$ -series test does not apply, because the exponent, $1+|\sin k|^a$ , is not a constant. ( $1+|\sin k|^a$ is always greater than $1$ since $k\in\mathbb{N}$ , but this does not imply that the series converges. For example, $\sum\limits_{k=2}^\infty \dfrac{1}{k^{1+\frac{1}{\ln k}}}=\sum\limits_{k=2}^\infty \dfrac{1}{ek}$ diverges) The integral test does not apply, because $\dfrac{1}{x^{1+|\sin x|^a}}$ is not a decreasing function. The ratio test does not apply, because the limit of the ratio does not exist. The root test is inconclusive, because the limit of the root is $1$ . Numerical investigation: If $a=1$ , numerical investigation suggests that, for large $n$ , $\sum\limits_{k=1}^n \dfrac{1}{k^{1+|\sin k|^a}}\approx c_1\ln (\ln n)+c_2$ where $c_1$ and $c_2$ are constants. If this approximation is valid, then $S$ would diverge when $a\ge 1$ . If $0<a<1$ , numerical investigation does not elucidate what happens.","This question already has an answer here : Does $\sum_{n=1}^\infty n^{-1-|\sin n|^a}$ converge for some $a\in(0,1)$? (1 answer) Closed 11 months ago . I was thinking about the -series , and came up with this question: For what real values of does converge? My attempt: I know that if , then , so converges. If , the standard tests do not work: The th term divergence test is inconclusive, because the terms approach . The comparison test with a -series test does not apply, because the exponent, , is not a constant. ( is always greater than since , but this does not imply that the series converges. For example, diverges) The integral test does not apply, because is not a decreasing function. The ratio test does not apply, because the limit of the ratio does not exist. The root test is inconclusive, because the limit of the root is . Numerical investigation: If , numerical investigation suggests that, for large , where and are constants. If this approximation is valid, then would diverge when . If , numerical investigation does not elucidate what happens.",p a S=\sum\limits_{k=1}^\infty \dfrac{1}{k^{1+|\sin k|^a}} a\le 0  S\le\sum\limits_{k=1}^\infty \dfrac{1}{k^{2}}=\pi^2/6 S a>0 n 0 p 1+|\sin k|^a 1+|\sin k|^a 1 k\in\mathbb{N} \sum\limits_{k=2}^\infty \dfrac{1}{k^{1+\frac{1}{\ln k}}}=\sum\limits_{k=2}^\infty \dfrac{1}{ek} \dfrac{1}{x^{1+|\sin x|^a}} 1 a=1 n \sum\limits_{k=1}^n \dfrac{1}{k^{1+|\sin k|^a}}\approx c_1\ln (\ln n)+c_2 c_1 c_2 S a\ge 1 0<a<1,"['real-analysis', 'calculus', 'sequences-and-series', 'trigonometry', 'convergence-divergence']"
45,"How to solve this estimate $m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}}$",How to solve this estimate,"m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}}","(a) Assume that the function $f:[a, b] \rightarrow \mathcal{R}$ with $a<b$ is differentiable and satisfies $\left|f^{\prime}(t)\right| \geq \beta$ for all $t \in[a, b]$ for some $\beta>0 .$ Prove the following estimate $$ m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq \frac{2 \varepsilon}{\beta} \quad \text { for } \varepsilon>0 $$ where $m\{B\}$ denotes the Lebesgue measure of set $B$ . (b) Assume that the function $f:[a, b] \rightarrow \mathcal{R}$ with $a<b$ is $q$ -times continuously differentiable and satisfies $\left|f^{(q)}(t)\right| \geq \beta$ for all $t \in[a, b]$ for some positive integer $q$ and $\beta>0 .$ Prove the following estimate $$ m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}} \quad \text { for } \varepsilon>0 $$ where $m\{B\}$ denotes the Lebesgue measure of set $B$ . I have proved $(a)$ . Noting that we can assume that $f^{\prime}(t)\geq \beta$ and $a= 0$ , so $f(t)\geq \beta t+f(0)  $ $(*)$ . However, it's easy to show that for any linear function with slope $\beta$ , $(a)$ is correct. By $ (*)$ one can prove $(a)$ for all differentiable $f$ with $\left|f^{\prime}(t)\right| \geq \beta$ . For (b), the method we use in (a) is out of work, since we cannot get the inequality like $(*)$ . I also tried to prove it by induction but falied. May anyone give me some hint or correct solution? Thanks in advance!!!","(a) Assume that the function with is differentiable and satisfies for all for some Prove the following estimate where denotes the Lebesgue measure of set . (b) Assume that the function with is -times continuously differentiable and satisfies for all for some positive integer and Prove the following estimate where denotes the Lebesgue measure of set . I have proved . Noting that we can assume that and , so . However, it's easy to show that for any linear function with slope , is correct. By one can prove for all differentiable with . For (b), the method we use in (a) is out of work, since we cannot get the inequality like . I also tried to prove it by induction but falied. May anyone give me some hint or correct solution? Thanks in advance!!!","f:[a, b] \rightarrow \mathcal{R} a<b \left|f^{\prime}(t)\right| \geq \beta t \in[a, b] \beta>0 . 
m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq \frac{2 \varepsilon}{\beta} \quad \text { for } \varepsilon>0
 m\{B\} B f:[a, b] \rightarrow \mathcal{R} a<b q \left|f^{(q)}(t)\right| \geq \beta t \in[a, b] q \beta>0 . 
m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}} \quad \text { for } \varepsilon>0
 m\{B\} B (a) f^{\prime}(t)\geq \beta a= 0 f(t)\geq \beta t+f(0)   (*) \beta (a)  (*) (a) f \left|f^{\prime}(t)\right| \geq \beta (*)","['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-measure']"
46,Second derivative of mollification at local maximum,Second derivative of mollification at local maximum,,"Let $u:[-1,1] \mapsto \mathbf{R}$ be a continuous function with a local maximum at 0. That is, there is a ball $B_\delta(0)$ with $u(x) \leq u(0)$ for all $x \in B_\delta(0)$ . Let $\eta(x)$ be the standard mollifier defined by $$ \eta(x) = \begin{cases} C \exp\left(\frac{1}{|x|^2-1}\right) &\text{if } |x|< 1\\ 0 &\text{if } |x| \geq 1,\end{cases} $$ where $C$ is chosen so that $\int_{\mathbf{R}^n} \eta = 1$ . For $x \in (-1+\epsilon, 1 - \epsilon)$ define the mollified function $u_\epsilon$ by $$ u_\epsilon(x) = \int_{B(0,\epsilon)} \epsilon^{-n}\eta\left(\frac{y}{\epsilon}\right)u(x-y) \ dy. $$ Is it true that $$ \liminf_{\epsilon \rightarrow 0} u_\epsilon ''(0) \leq 0. $$ This would be true if the mollified function also had a local maximum at 0 — however this is not necessarily the case. As some further intuition it should be true that the mollification has a local max near 0, and this local max approaches the origin as $\epsilon \rightarrow 0$ . However without something resembling uniform convergence I've been unable to turn this into a solution. Note also the lim inf can not be replaced by a limit as we can construct a function for  for which $ \lim_{\epsilon \rightarrow 0} u_\epsilon ''(0)$ does not exist. The idea behind the construction is to take any positive function for which the limit of the mollifications don't exist at 0 then integrate twice. More precisely take $h \in L^\infty[0,1]$ with $h \geq 0$ for which $\lim_{\epsilon \rightarrow 0} h_{\epsilon}(0)$ does not exist. Set $g(x) = \int_0^x h(x)$ and $u(x) = \int_0^x g(x)$ . Note $u(0)=0$ and $u \leq 0$ . Then $h$ bounded implies $u,g$ are Lipschitz, and differentiable a.e with $u′=g$ and $g′=h$ . Since the derivative of the mollification is the mollification of the derivative we have $(f_\epsilon)'' = u_\epsilon$ which we recall was chosen to not have a limit at 0.","Let be a continuous function with a local maximum at 0. That is, there is a ball with for all . Let be the standard mollifier defined by where is chosen so that . For define the mollified function by Is it true that This would be true if the mollified function also had a local maximum at 0 — however this is not necessarily the case. As some further intuition it should be true that the mollification has a local max near 0, and this local max approaches the origin as . However without something resembling uniform convergence I've been unable to turn this into a solution. Note also the lim inf can not be replaced by a limit as we can construct a function for  for which does not exist. The idea behind the construction is to take any positive function for which the limit of the mollifications don't exist at 0 then integrate twice. More precisely take with for which does not exist. Set and . Note and . Then bounded implies are Lipschitz, and differentiable a.e with and . Since the derivative of the mollification is the mollification of the derivative we have which we recall was chosen to not have a limit at 0.","u:[-1,1] \mapsto \mathbf{R} B_\delta(0) u(x) \leq u(0) x \in B_\delta(0) \eta(x)  \eta(x) = \begin{cases} C \exp\left(\frac{1}{|x|^2-1}\right) &\text{if } |x|< 1\\ 0 &\text{if } |x| \geq 1,\end{cases}  C \int_{\mathbf{R}^n} \eta = 1 x \in (-1+\epsilon, 1 - \epsilon) u_\epsilon  u_\epsilon(x) = \int_{B(0,\epsilon)} \epsilon^{-n}\eta\left(\frac{y}{\epsilon}\right)u(x-y) \ dy.   \liminf_{\epsilon \rightarrow 0} u_\epsilon ''(0) \leq 0.  \epsilon \rightarrow 0  \lim_{\epsilon \rightarrow 0} u_\epsilon ''(0) h \in L^\infty[0,1] h \geq 0 \lim_{\epsilon \rightarrow 0} h_{\epsilon}(0) g(x) = \int_0^x h(x) u(x) = \int_0^x g(x) u(0)=0 u \leq 0 h u,g u′=g g′=h (f_\epsilon)'' = u_\epsilon","['real-analysis', 'calculus', 'partial-differential-equations']"
47,How should I prove $\lim_{x \to \infty} \frac{1}{x^3} = 0$,How should I prove,\lim_{x \to \infty} \frac{1}{x^3} = 0,"Use the definition of the limit to prove the following limit. $$\lim_{x \to \infty} \frac{1}{x^3} = 0$$ This is my attempt at solving this question Suppose $\epsilon > 0$ , choose $M = \frac{1}{^3\sqrt{\epsilon}}$ Suppose $x>M$ $$\frac{1}{x}<\frac{1}{M}\ \text{(taking the reciprocal)}$$ $$\frac{1}{x^3}<\frac{1}{M^3}\ \text{(cubing both sides)}$$ Assuming $x > 0$ as the limit is as $x$ approaches $\infty$ : $$\left|\frac{1}{x^3}\right|<\frac{1}{M^3}$$ $$\implies \left|\frac{1}{x^3} - 0\right|<\epsilon$$ I am unsure whether this is the right way to do so. I thought of this method after watching videos and reading up on limit proofs. I am self-learning all these topics purely for interest. Any corrections to my working will be greatly appreciated! Thank you.","Use the definition of the limit to prove the following limit. This is my attempt at solving this question Suppose , choose Suppose Assuming as the limit is as approaches : I am unsure whether this is the right way to do so. I thought of this method after watching videos and reading up on limit proofs. I am self-learning all these topics purely for interest. Any corrections to my working will be greatly appreciated! Thank you.",\lim_{x \to \infty} \frac{1}{x^3} = 0 \epsilon > 0 M = \frac{1}{^3\sqrt{\epsilon}} x>M \frac{1}{x}<\frac{1}{M}\ \text{(taking the reciprocal)} \frac{1}{x^3}<\frac{1}{M^3}\ \text{(cubing both sides)} x > 0 x \infty \left|\frac{1}{x^3}\right|<\frac{1}{M^3} \implies \left|\frac{1}{x^3} - 0\right|<\epsilon,"['real-analysis', 'limits', 'epsilon-delta']"
48,Proving monotonicity of this ratio of Hypergeometric functions,Proving monotonicity of this ratio of Hypergeometric functions,,"Let $n\in\Bbb N$ , $\omega=0,1,\dots,n$ , and $\nu,z>0$ . We define $$ \tilde g_{n,\omega}(z,\nu):=\frac{z^{n-\omega}\partial_z^n z^\omega {_1F_0}(1;-;z)_\nu}{{_1F_0}(1;-;z)_\nu}, $$ where $$ {_1F_0}(1;-;z)_\nu:=\frac{1-z^{\nu+1}}{1-z}=(\nu+1)z^{\nu+1}\mathbf F(1,\nu+2,2,1-z), $$ which is a continuous interpolation of the truncated geometric series and $\mathbf F(a,b,c,z)=F(a,b,c,z)/\Gamma(c)$ is the regularized Gauss hypergeometric function. Conjecture: Under the specified restrictions on the parameters, $|\tilde g_{n,\omega}(z,\nu)|$ is nondecreasing in $z$ and is strictly increasing in $z$ when $n-\omega-\nu\notin\Bbb N$ . I am seeking to solve this conjecture but have been unsuccessful so far and am turning to the SE math community for help in finishing the proof.  See below for what I have tried as a potential path forward.","Let , , and . We define where which is a continuous interpolation of the truncated geometric series and is the regularized Gauss hypergeometric function. Conjecture: Under the specified restrictions on the parameters, is nondecreasing in and is strictly increasing in when . I am seeking to solve this conjecture but have been unsuccessful so far and am turning to the SE math community for help in finishing the proof.  See below for what I have tried as a potential path forward.","n\in\Bbb N \omega=0,1,\dots,n \nu,z>0 
\tilde g_{n,\omega}(z,\nu):=\frac{z^{n-\omega}\partial_z^n z^\omega {_1F_0}(1;-;z)_\nu}{{_1F_0}(1;-;z)_\nu},
 
{_1F_0}(1;-;z)_\nu:=\frac{1-z^{\nu+1}}{1-z}=(\nu+1)z^{\nu+1}\mathbf F(1,\nu+2,2,1-z),
 \mathbf F(a,b,c,z)=F(a,b,c,z)/\Gamma(c) |\tilde g_{n,\omega}(z,\nu)| z z n-\omega-\nu\notin\Bbb N","['real-analysis', 'gamma-function', 'interpolation', 'hypergeometric-function', 'monotone-functions']"
49,Can we approximate a.e. invertible matrices with everywhere invertible matrices in $L^2$ sense?,Can we approximate a.e. invertible matrices with everywhere invertible matrices in  sense?,L^2,"Let $\mathbb{D}^n=\{ x \in \mathbb{R}^n \, | \, |x| \le 1\}$ be the closed unit ball, and let $A:\mathbb{D}^n \to \mathbb{R}^{n^2}$ be real-analytic on the interior $(\mathbb{D}^n)^o$ and smooth on the entire closed ball $\mathbb{D}^n$ . Suppose that $n \ge 2$ , and that $\det A >0$ a.e. on $\mathbb{D}^n$ . Are there smooth maps $A_k: \mathbb{D}^n \to \mathbb{R}^{n^2}$ , such that $A_k \to A$ in $L^2(\mathbb{D}^n , \mathbb{R}^{n^2})$ and $\det A_k >0$ everywhere on $\mathbb{D}^n$ ? Edit: In Vogel's elegant answer, it is proved that we can approximate $A$ via continuous maps. Can we approximate it with smooth maps? I think the answer should be positive, but I am having trouble with the details: Using mollifiers, we can approximate any continuous $A_k \in L^2(\mathbb{D}^n , \mathbb{R}^{n^2})$ with a smooth version $\tilde A_k$ in such a way to ensure that $\tilde A_k$ will converge to $A_k$ uniformly on compact subsets of $(\mathbb{D}^n)^o$ . Since $$s_k=\min_{x \in \mathbb{D}^n}\text{dist}(A_k(x), {\det}^{-1}(0))>0,$$ then if $\| \tilde A_k(y) -A_k(y)\| < s_k$ we have $\det(\tilde A_k(y))>0$ as we wanted. The problem is that we only have uniform convergence $\tilde A_k \to A_k$ on compact subsets of the interior of $\mathbb{D}^n$ , so I think we might have a problem on the boundary... Any ideas about how to finish this reduction?","Let be the closed unit ball, and let be real-analytic on the interior and smooth on the entire closed ball . Suppose that , and that a.e. on . Are there smooth maps , such that in and everywhere on ? Edit: In Vogel's elegant answer, it is proved that we can approximate via continuous maps. Can we approximate it with smooth maps? I think the answer should be positive, but I am having trouble with the details: Using mollifiers, we can approximate any continuous with a smooth version in such a way to ensure that will converge to uniformly on compact subsets of . Since then if we have as we wanted. The problem is that we only have uniform convergence on compact subsets of the interior of , so I think we might have a problem on the boundary... Any ideas about how to finish this reduction?","\mathbb{D}^n=\{ x \in \mathbb{R}^n \, | \, |x| \le 1\} A:\mathbb{D}^n \to \mathbb{R}^{n^2} (\mathbb{D}^n)^o \mathbb{D}^n n \ge 2 \det A >0 \mathbb{D}^n A_k: \mathbb{D}^n \to \mathbb{R}^{n^2} A_k \to A L^2(\mathbb{D}^n , \mathbb{R}^{n^2}) \det A_k >0 \mathbb{D}^n A A_k \in L^2(\mathbb{D}^n , \mathbb{R}^{n^2}) \tilde A_k \tilde A_k A_k (\mathbb{D}^n)^o s_k=\min_{x \in \mathbb{D}^n}\text{dist}(A_k(x), {\det}^{-1}(0))>0, \| \tilde A_k(y) -A_k(y)\| < s_k \det(\tilde A_k(y))>0 \tilde A_k \to A_k \mathbb{D}^n","['real-analysis', 'measure-theory', 'determinant', 'lp-spaces', 'perturbation-theory']"
50,On $\int_0^\infty \frac{\exp(-x^2)}{1+x^2}dx=\frac{\pi e}2\text{erfc}(1)$,On,\int_0^\infty \frac{\exp(-x^2)}{1+x^2}dx=\frac{\pi e}2\text{erfc}(1),"I was attempting to answer this question , but then I came across a question of my own involving my attempt. Task: Prove $$\int_0^\infty\frac{\exp(-x^2)}{1+x^2}\mathrm dx=\frac{\pi e}2\text{erfc}(1)$$ Attempt: $$I=\int_0^{\infty}\frac{\exp(-x^2)}{1+x^2}\mathrm dx$$ We then use the Taylor series for the exponential function to find that $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^\infty\frac{x^{2n}}{1+x^2}\mathrm dx$$ Setting $x=\tan u$ , $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\tan(u)^{2n}\mathrm{d}u$$ $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\sin(u)^{2n}\cos(u)^{-2n}\mathrm{d}u$$ And using $$\int_0^{\pi/2}\sin(t)^a\cos(t)^b\mathrm{d}t=\frac{\Gamma(\frac{a+1}{2})\Gamma(\frac{b+1}{2})}{2\Gamma(\frac{a+b}{2}+1)}$$ We have $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac{1+2n}{2}\bigg)\Gamma\bigg(\frac{1-2n}{2}\bigg)$$ $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac12+n\bigg)\Gamma\bigg(\frac12-n\bigg)$$ Recall the Gamma reflection formula: $$\Gamma(s)\Gamma(1-s)=\pi\csc\pi s\ ,\qquad s\not\in\Bbb Z$$ Since $n\in\Bbb N_0$ , we have $\frac12+n\not\in\Bbb Z$ , which means we may plug in $s=\frac12+n$ : $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\pi\csc\bigg(\frac\pi2+\pi n\bigg)$$ $$I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\csc\bigg(\frac\pi2(2n+1)\bigg)$$ Then we recall that $$\sin\bigg(\frac\pi2(2n+1)\bigg)=(-1)^n,\qquad n\in\Bbb Z$$ So we have $$I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\frac1{(-1)^n}$$ $$I=\frac\pi2\sum_{n\geq0}\frac1{n!}$$ $$I=\frac{\pi e}2$$ But $$\frac{\pi e}2\neq \frac{\pi e}2\text{erfc}(1)$$ What did I do wrong? Thanks. Edit: I see that $$\int_{\Bbb R^+}\frac{x^{2n}}{1+x^2}\mathrm dx$$ diverges, and as was pointed out in the comments, I can't interchange the $\sum$ and $\int$ , but why? The Taylor series converges for all $x\in\Bbb R_0^+$ , so what's wrong with the swappage?","I was attempting to answer this question , but then I came across a question of my own involving my attempt. Task: Prove Attempt: We then use the Taylor series for the exponential function to find that Setting , And using We have Recall the Gamma reflection formula: Since , we have , which means we may plug in : Then we recall that So we have But What did I do wrong? Thanks. Edit: I see that diverges, and as was pointed out in the comments, I can't interchange the and , but why? The Taylor series converges for all , so what's wrong with the swappage?","\int_0^\infty\frac{\exp(-x^2)}{1+x^2}\mathrm dx=\frac{\pi e}2\text{erfc}(1) I=\int_0^{\infty}\frac{\exp(-x^2)}{1+x^2}\mathrm dx I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^\infty\frac{x^{2n}}{1+x^2}\mathrm dx x=\tan u I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\tan(u)^{2n}\mathrm{d}u I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\sin(u)^{2n}\cos(u)^{-2n}\mathrm{d}u \int_0^{\pi/2}\sin(t)^a\cos(t)^b\mathrm{d}t=\frac{\Gamma(\frac{a+1}{2})\Gamma(\frac{b+1}{2})}{2\Gamma(\frac{a+b}{2}+1)} I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac{1+2n}{2}\bigg)\Gamma\bigg(\frac{1-2n}{2}\bigg) I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac12+n\bigg)\Gamma\bigg(\frac12-n\bigg) \Gamma(s)\Gamma(1-s)=\pi\csc\pi s\ ,\qquad s\not\in\Bbb Z n\in\Bbb N_0 \frac12+n\not\in\Bbb Z s=\frac12+n I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\pi\csc\bigg(\frac\pi2+\pi n\bigg) I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\csc\bigg(\frac\pi2(2n+1)\bigg) \sin\bigg(\frac\pi2(2n+1)\bigg)=(-1)^n,\qquad n\in\Bbb Z I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\frac1{(-1)^n} I=\frac\pi2\sum_{n\geq0}\frac1{n!} I=\frac{\pi e}2 \frac{\pi e}2\neq \frac{\pi e}2\text{erfc}(1) \int_{\Bbb R^+}\frac{x^{2n}}{1+x^2}\mathrm dx \sum \int x\in\Bbb R_0^+","['real-analysis', 'integration', 'sequences-and-series', 'improper-integrals', 'error-function']"
51,$f$ is uniformly continuity,is uniformly continuity,f,"Let $f:\mathbb{R}\to \mathbb{R}$ be a bounded  continuous function. Suppose for any $y\in \mathbb{R}$, $f^{-1}(\{y\})$ is the empty set or a finite set. Then $f$ is uniformly continuous. My idea :If $f$ is not uniformly continuous, there exists $a_n, b_n$ and $\epsilon $ s.t.,  $|a_n-b_n|\leq \frac{1}{n}$ $|f(a_n)-f(b_n)|>\epsilon$ . $f$ is bounded, so I can take convergence subsequence. But I don't know how to use cardinality of $f^{-1}(\{y\})$","Let $f:\mathbb{R}\to \mathbb{R}$ be a bounded  continuous function. Suppose for any $y\in \mathbb{R}$, $f^{-1}(\{y\})$ is the empty set or a finite set. Then $f$ is uniformly continuous. My idea :If $f$ is not uniformly continuous, there exists $a_n, b_n$ and $\epsilon $ s.t.,  $|a_n-b_n|\leq \frac{1}{n}$ $|f(a_n)-f(b_n)|>\epsilon$ . $f$ is bounded, so I can take convergence subsequence. But I don't know how to use cardinality of $f^{-1}(\{y\})$",,"['real-analysis', 'continuity', 'uniform-continuity']"
52,An inverse question inspired by Cauchy–Schwarz inequality [duplicate],An inverse question inspired by Cauchy–Schwarz inequality [duplicate],,"This question already has answers here : If $(a_n)$ is such that $\sum_{n=1}^\infty a_nb_n$ converges for every $b\in\ell_2$, then $a\in\ell_2$ (2 answers) Closed 6 years ago . I have the following question, whose inverse question can be done by the well-known Cauchy–Schwarz inequality. But I do not know how to solve this question: Suppose that $\{a_n\}_{n=1}^\infty$ is a sequence of real numbers such that \begin{equation} \sum_{n=1}^\infty a_n b_n \quad \text{is a convergent series whenever}\quad\sum_{n=1}^\infty b_n^2<\infty. \end{equation} Show that $\sum_{n=1}^\infty a_n^2<\infty$.","This question already has answers here : If $(a_n)$ is such that $\sum_{n=1}^\infty a_nb_n$ converges for every $b\in\ell_2$, then $a\in\ell_2$ (2 answers) Closed 6 years ago . I have the following question, whose inverse question can be done by the well-known Cauchy–Schwarz inequality. But I do not know how to solve this question: Suppose that $\{a_n\}_{n=1}^\infty$ is a sequence of real numbers such that \begin{equation} \sum_{n=1}^\infty a_n b_n \quad \text{is a convergent series whenever}\quad\sum_{n=1}^\infty b_n^2<\infty. \end{equation} Show that $\sum_{n=1}^\infty a_n^2<\infty$.",,"['real-analysis', 'sequences-and-series', 'inequality']"
53,"If $f$ is integrable, then I can bring continuous functions from above or below","If  is integrable, then I can bring continuous functions from above or below",f,"$\def\d{\mathrm{d}}$Let $f$ be integrable. I want to show there exist two functions $g$ and $h$ that are continuous under a closed interval $[a,b]$ s.t $h\leq f\leq g$ and at the same time$$\int_{a}^{b} (g(x)-h(x)) \,\d x <ε.$$ I know that because $f$ is integrable there exist two steps functions $h\leq f\leq g$ such that$$\int_{a}^{b}g(x) \,\d x - \int_{a}^{b} h(x) \,\d x <ε,$$  but I'm having trouble in the continuity part My intuition: I'm thinking of ""joining"" the steps using straight lines in order to have a continuous function. but I have no idea how to formalize it. Thanks in advance!","$\def\d{\mathrm{d}}$Let $f$ be integrable. I want to show there exist two functions $g$ and $h$ that are continuous under a closed interval $[a,b]$ s.t $h\leq f\leq g$ and at the same time$$\int_{a}^{b} (g(x)-h(x)) \,\d x <ε.$$ I know that because $f$ is integrable there exist two steps functions $h\leq f\leq g$ such that$$\int_{a}^{b}g(x) \,\d x - \int_{a}^{b} h(x) \,\d x <ε,$$  but I'm having trouble in the continuity part My intuition: I'm thinking of ""joining"" the steps using straight lines in order to have a continuous function. but I have no idea how to formalize it. Thanks in advance!",,"['real-analysis', 'integration', 'riemann-integration']"
54,"Proof of Karlin-Rubin's theorem, detail about a real analysis fact.","Proof of Karlin-Rubin's theorem, detail about a real analysis fact.",,"Although the setting of this question is statistics, the question actually asks for a real analysis fact (monotone functions). Karlin-Rubin's theorem states conditions under which we can find a uniformly most powerful test (UMPT) for a statistical hypothesis: Suppose a family of density or mass functions $\{f(\vec{x}|\theta):\,\theta\in\Theta\}$ and we want to test $$\begin{cases} H_0:\,\theta\leq\theta_0 \\ H_A:\,\theta>\theta_0.\end{cases}$$If the likelihood ratio is monotone on a statistic $T(\vec{x})$ (that is, for every fixed $\theta_1<\theta_2$ in $\Theta$, the ratio $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}$ is nondecreasing on $\{\vec{x}:\,f(\vec{x}|\theta_2)>0\text{ or }f(\vec{x}|\theta_1)>0\}$ as a function of $T(\vec{x})$), then the test of critical region $\text{CR}=\{\vec{x}:\,T(\vec{x})\geq k\}$, where $k$ is chosen so that $\alpha=P(\text{CR}|\theta=\theta_0)$, is the UMPT of size $\alpha$. In all the proofs I have read (for instance, in page 22 here or in "" Statistical inference "" by Casella-Berger, 2n edition, page 391), it is (more or less) said: ""we can find $k_1$ such that, if $T(\vec{x})\geq k$, then $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}\geq k_1$, and if $T(\vec{x})<k$, then  $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}< k_1$"". I would understand that statement if the likehood ratio were strictly increasing, but what about the case in which it is constant? For example, if $X\sim U(0,\theta)$, the likelihood ratio is monotone on $T(\vec{x})=\max_{1\leq i\leq n}x_i$ ($n$ is the length of the sample $\vec{x}$), but not strictly increasing. EDIT: My questions are: Is the assertion between quotation marks true for every density or mass function with (not strictly) monotone likelihood ratio on $T$? And what about in the case of the uniform distribution? The second question has an answer below. I would like an answer for the first question , with claims based on real-analysis.","Although the setting of this question is statistics, the question actually asks for a real analysis fact (monotone functions). Karlin-Rubin's theorem states conditions under which we can find a uniformly most powerful test (UMPT) for a statistical hypothesis: Suppose a family of density or mass functions $\{f(\vec{x}|\theta):\,\theta\in\Theta\}$ and we want to test $$\begin{cases} H_0:\,\theta\leq\theta_0 \\ H_A:\,\theta>\theta_0.\end{cases}$$If the likelihood ratio is monotone on a statistic $T(\vec{x})$ (that is, for every fixed $\theta_1<\theta_2$ in $\Theta$, the ratio $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}$ is nondecreasing on $\{\vec{x}:\,f(\vec{x}|\theta_2)>0\text{ or }f(\vec{x}|\theta_1)>0\}$ as a function of $T(\vec{x})$), then the test of critical region $\text{CR}=\{\vec{x}:\,T(\vec{x})\geq k\}$, where $k$ is chosen so that $\alpha=P(\text{CR}|\theta=\theta_0)$, is the UMPT of size $\alpha$. In all the proofs I have read (for instance, in page 22 here or in "" Statistical inference "" by Casella-Berger, 2n edition, page 391), it is (more or less) said: ""we can find $k_1$ such that, if $T(\vec{x})\geq k$, then $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}\geq k_1$, and if $T(\vec{x})<k$, then  $\frac{f(\vec{x}|\theta_2)}{f(\vec{x}|\theta_1)}< k_1$"". I would understand that statement if the likehood ratio were strictly increasing, but what about the case in which it is constant? For example, if $X\sim U(0,\theta)$, the likelihood ratio is monotone on $T(\vec{x})=\max_{1\leq i\leq n}x_i$ ($n$ is the length of the sample $\vec{x}$), but not strictly increasing. EDIT: My questions are: Is the assertion between quotation marks true for every density or mass function with (not strictly) monotone likelihood ratio on $T$? And what about in the case of the uniform distribution? The second question has an answer below. I would like an answer for the first question , with claims based on real-analysis.",,"['real-analysis', 'probability', 'statistical-inference', 'hypothesis-testing', 'monotone-functions']"
55,Parallelogram / Polarization Inequality,Parallelogram / Polarization Inequality,,"Dear Math Experts, as a stepping stone for another result I am interested in the following generalization of the parallelogram / polarization equality for Hilbert spaces. Denote by $H$ a Hilbert space over the real numbers $R$ and by $B = \{ u \in H : \Vert u \Vert \leq 1\}$ the closed unit ball in $H$. The inner product is denoted by $\langle , \rangle$. I am now after the following statement: For any fixed vectors $x,y \in H$ we have \begin{align} -\Vert x - y \Vert^2 \leq 2\Vert x \Vert \langle u, x \rangle + 2\Vert y \Vert \langle u, y \rangle - \Vert x + y \Vert \langle u, x + y \rangle \leq \Vert x - y \Vert^2 \end{align} for all $u \in B$. For $u$ such that $\Vert u \Vert < 1$ equality holds if and only if $x = y$. Any ideas related to my own proof strategy (see below) or otherwise are very welcome. My own attempt so far: I can deal with the cases $x=y$ and $x = 0 \neq y$ and I therefore assume $0 \neq x \neq y \neq 0$ in the following. As a further observation we can regard the part in the middle as function of $u$ and rewrite in terms of the inner product as $f_{x,y}(u) = \langle u, (2\Vert x\Vert - \Vert x+y \Vert)x + (2\Vert y\Vert - \Vert x+y \Vert)y \rangle$. Using Cauchy-Schwarz and $\Vert u \Vert \leq 1$ we then have  \begin{align} -L(x,y) \leq f_{x,y}(u) \leq L(x,y) \end{align} with $L(x,y) = \Vert (2\Vert x\Vert - \Vert x+y \Vert)x + (2\Vert y\Vert - \Vert x+y \Vert)y \Vert$. The statement is then equivalent to $L(x,y) \leq \Vert x - y \Vert^2$. Of course a result for general $H$ would be very interesting, but my own attempts have so far focused on the finite dimensional case $H = R^d$ with $\langle x,y \rangle = x^T y$ which is fine for my particular application. We can now use that for any positive scalar $s$ and any rotation matrix $A$ we have $L(sAx,sAy) = s^2 L(x,y)$ as well as $\Vert sAx - sAy \Vert^2 = s^2 \Vert x - y\Vert^2$. Assuming $0 \neq x \neq y \neq 0$ we then find a rotation matrix $A$ and the strictly positive scaling factor $s = 1 / \Vert x \Vert$ such that $sAx = e = (1,0,\ldots,0)$. Wlog we then only consider $L(e,y) \leq \Vert e - y \Vert^2$ for any $y \in R^d$. Therefore I need to show that $h(y) = \Vert e - y \Vert^2 - L(e,y) \geq 0$ at which point I am stuck. I know that for $\alpha \geq 0$ we have $h(\alpha e) = 0$ which I believe is the lowest function value.","Dear Math Experts, as a stepping stone for another result I am interested in the following generalization of the parallelogram / polarization equality for Hilbert spaces. Denote by $H$ a Hilbert space over the real numbers $R$ and by $B = \{ u \in H : \Vert u \Vert \leq 1\}$ the closed unit ball in $H$. The inner product is denoted by $\langle , \rangle$. I am now after the following statement: For any fixed vectors $x,y \in H$ we have \begin{align} -\Vert x - y \Vert^2 \leq 2\Vert x \Vert \langle u, x \rangle + 2\Vert y \Vert \langle u, y \rangle - \Vert x + y \Vert \langle u, x + y \rangle \leq \Vert x - y \Vert^2 \end{align} for all $u \in B$. For $u$ such that $\Vert u \Vert < 1$ equality holds if and only if $x = y$. Any ideas related to my own proof strategy (see below) or otherwise are very welcome. My own attempt so far: I can deal with the cases $x=y$ and $x = 0 \neq y$ and I therefore assume $0 \neq x \neq y \neq 0$ in the following. As a further observation we can regard the part in the middle as function of $u$ and rewrite in terms of the inner product as $f_{x,y}(u) = \langle u, (2\Vert x\Vert - \Vert x+y \Vert)x + (2\Vert y\Vert - \Vert x+y \Vert)y \rangle$. Using Cauchy-Schwarz and $\Vert u \Vert \leq 1$ we then have  \begin{align} -L(x,y) \leq f_{x,y}(u) \leq L(x,y) \end{align} with $L(x,y) = \Vert (2\Vert x\Vert - \Vert x+y \Vert)x + (2\Vert y\Vert - \Vert x+y \Vert)y \Vert$. The statement is then equivalent to $L(x,y) \leq \Vert x - y \Vert^2$. Of course a result for general $H$ would be very interesting, but my own attempts have so far focused on the finite dimensional case $H = R^d$ with $\langle x,y \rangle = x^T y$ which is fine for my particular application. We can now use that for any positive scalar $s$ and any rotation matrix $A$ we have $L(sAx,sAy) = s^2 L(x,y)$ as well as $\Vert sAx - sAy \Vert^2 = s^2 \Vert x - y\Vert^2$. Assuming $0 \neq x \neq y \neq 0$ we then find a rotation matrix $A$ and the strictly positive scaling factor $s = 1 / \Vert x \Vert$ such that $sAx = e = (1,0,\ldots,0)$. Wlog we then only consider $L(e,y) \leq \Vert e - y \Vert^2$ for any $y \in R^d$. Therefore I need to show that $h(y) = \Vert e - y \Vert^2 - L(e,y) \geq 0$ at which point I am stuck. I know that for $\alpha \geq 0$ we have $h(\alpha e) = 0$ which I believe is the lowest function value.",,"['real-analysis', 'functional-analysis', 'multivariable-calculus', 'inequality', 'hilbert-spaces']"
56,"Arithmetic-quadratic mean and other ""means by limits of means""","Arithmetic-quadratic mean and other ""means by limits of means""",,"For $x,y$ positive real numbers, and $p\neq 0$ real, define the Hölder $p$-mean $$M_p(x,y) := \left(\frac{x^p+y^p}{2}\right)^{1/p}$$ whereas $$M_0(x,y) := \sqrt{xy}$$ is the limit of $M_p(x,y)$ when $p\to 0$, i.e., the geometric mean. Furthermore, when $p,q$ are two real numbers, define $L_{p,q}(x,y)$ as the obvious fixed point satisfying the equation $$L_{p,q}(x,y) = L_{p,q}(M_p(x,y),M_q(x,y))$$ —meaning that $L_{p,q}(x,y)$ is defined as the common limit of the sequences $(x_n)$ and $(y_n)$ such that $(x_0,y_0) = (x,y)$ and $(x_{n+1},y_{n+1}) = (M_p(x,y),M_q(x,y))$.  (Showing convergence is easy, and evidently, $L_{p,q} = L_{q,p}$.) Thus, $L_{0,1}(x,y)$ is the famous arithmetic-geometric mean . Among other things, all these functions satisfy $\min(x,y) \leq F(x,y) \leq \max(x,y)$, as well as $F(y,x) = F(x,y)$ and $F(\lambda x, \lambda y) = \lambda F(x,y)$ — so we could just define them from $F(1,x)$, which is continuous and monotonically increasing with value $1$ at $1$.  It is well-known that $M_p(x,y) < M_q(x,y)$ whenever $x\neq y$ and $p<q$, so of course $M_p(x,y) < L_{p,q}(x,y) < M_q(x,y)$ under those circumstances. Surprisingly, I can find no reference to the $L_{p,q}$ in the literature except for $(p,q)=(0,1)$ (mentioned above), and $(p,q)=(-1,1)$ which this page points out is just $M_0$.  More generally, we have $$L_{p,-p}(x,y) = M_0(x,y)$$ and $$L_{0,p}(x,y) = \big(L_{0,1}(x^p,y^p)\big)^{1/p}$$ —both are easy, and $L_{p,p}(x,x) = M_p(x,y)$ is trivial. I couldn't find anything intelligent to say about $L_{1,2}$ (the ""arithmetic-quadratic mean""), however, let alone the other $L_{p,q}$.  Experimentally, it satisfies $L_{1,2}(x,y) \leq M_{3/2}(x,y)$, something which is probably easy to prove and not interesting enough to try, so that's not my question.  Instead, my question will be: Can $L_{1,2}$, or more generally $L_{p,q}$ (with $0<p<q$), be expressed in closed form using elementary functions and the arithmetic-geometric mean $L_{0,1}$?  Or at least using ""standard"" special functions (e.g. elliptic integrals)? (I realize that the answer ""no"" might be exceedingly difficult to prove, so I'll be liberal in what I accept as an answer: perhaps the question should really be ""please say something interesting about $L_{1,2}$ or $L_{p,q}$ in general"".)","For $x,y$ positive real numbers, and $p\neq 0$ real, define the Hölder $p$-mean $$M_p(x,y) := \left(\frac{x^p+y^p}{2}\right)^{1/p}$$ whereas $$M_0(x,y) := \sqrt{xy}$$ is the limit of $M_p(x,y)$ when $p\to 0$, i.e., the geometric mean. Furthermore, when $p,q$ are two real numbers, define $L_{p,q}(x,y)$ as the obvious fixed point satisfying the equation $$L_{p,q}(x,y) = L_{p,q}(M_p(x,y),M_q(x,y))$$ —meaning that $L_{p,q}(x,y)$ is defined as the common limit of the sequences $(x_n)$ and $(y_n)$ such that $(x_0,y_0) = (x,y)$ and $(x_{n+1},y_{n+1}) = (M_p(x,y),M_q(x,y))$.  (Showing convergence is easy, and evidently, $L_{p,q} = L_{q,p}$.) Thus, $L_{0,1}(x,y)$ is the famous arithmetic-geometric mean . Among other things, all these functions satisfy $\min(x,y) \leq F(x,y) \leq \max(x,y)$, as well as $F(y,x) = F(x,y)$ and $F(\lambda x, \lambda y) = \lambda F(x,y)$ — so we could just define them from $F(1,x)$, which is continuous and monotonically increasing with value $1$ at $1$.  It is well-known that $M_p(x,y) < M_q(x,y)$ whenever $x\neq y$ and $p<q$, so of course $M_p(x,y) < L_{p,q}(x,y) < M_q(x,y)$ under those circumstances. Surprisingly, I can find no reference to the $L_{p,q}$ in the literature except for $(p,q)=(0,1)$ (mentioned above), and $(p,q)=(-1,1)$ which this page points out is just $M_0$.  More generally, we have $$L_{p,-p}(x,y) = M_0(x,y)$$ and $$L_{0,p}(x,y) = \big(L_{0,1}(x^p,y^p)\big)^{1/p}$$ —both are easy, and $L_{p,p}(x,x) = M_p(x,y)$ is trivial. I couldn't find anything intelligent to say about $L_{1,2}$ (the ""arithmetic-quadratic mean""), however, let alone the other $L_{p,q}$.  Experimentally, it satisfies $L_{1,2}(x,y) \leq M_{3/2}(x,y)$, something which is probably easy to prove and not interesting enough to try, so that's not my question.  Instead, my question will be: Can $L_{1,2}$, or more generally $L_{p,q}$ (with $0<p<q$), be expressed in closed form using elementary functions and the arithmetic-geometric mean $L_{0,1}$?  Or at least using ""standard"" special functions (e.g. elliptic integrals)? (I realize that the answer ""no"" might be exceedingly difficult to prove, so I'll be liberal in what I accept as an answer: perhaps the question should really be ""please say something interesting about $L_{1,2}$ or $L_{p,q}$ in general"".)",,"['real-analysis', 'closed-form', 'means']"
57,Does equation $f(g(x))=a f(x)$ have a solution?,Does equation  have a solution?,f(g(x))=a f(x),"Suppose $g: [0,1] \to [0,1]$ is a strictly increasing, continuously differentiable function with $g(0)=0$ and $g(1) < 1$, and $a \in (0,1)$. Is there a function $f:[0,1] \to \mathbb{R}$ such that $f(g(x))=a f(x)$ for all $x \in [0,1]$? I know that when $g$ is linear, then there are many such functions $f$. But how about arbitrary (sufficiently smooth) function? Background: it is a part of larger project and I have already shown that $g$ is a certain function with nice properties. I have strong intuition that $f$ function should exists. If not generally, then with some additional properties that $g$ probably has. Also, if it helps, then in all cases relevant to my project, $a$ and $g(1)$ are small.","Suppose $g: [0,1] \to [0,1]$ is a strictly increasing, continuously differentiable function with $g(0)=0$ and $g(1) < 1$, and $a \in (0,1)$. Is there a function $f:[0,1] \to \mathbb{R}$ such that $f(g(x))=a f(x)$ for all $x \in [0,1]$? I know that when $g$ is linear, then there are many such functions $f$. But how about arbitrary (sufficiently smooth) function? Background: it is a part of larger project and I have already shown that $g$ is a certain function with nice properties. I have strong intuition that $f$ function should exists. If not generally, then with some additional properties that $g$ probably has. Also, if it helps, then in all cases relevant to my project, $a$ and $g(1)$ are small.",,"['real-analysis', 'functions', 'recurrence-relations']"
58,Separation in compact spaces,Separation in compact spaces,,"There was recently a question that I cannot find about separation in compact spaces. The answer to that question was no for trivial reasons. Motivated by that, let me ask a less trivial version of this question. Given two points $x,y$ in a (separable) compact Hausdorff space $X$ , can we find two open sets $V,U$ in $X$ and a continuous function $f\colon X\to \mathbb{R}$ such that $x\in V, y\in U$ $f(t)=1$ for all $t\in V$ $f(t)<0$ for all $t\in U$ $f^{-1}(\{0\})$ has empty interior My feeling is that in general the answer should be no . Yet for familiar compact metric compacta such open sets and function $f$ can be constructed. (Of course this is not a proof.) EDIT : I modified the question by removing one of the requirements; I believe now it will be more tractable. It seems that the answer is yes if $X$ is perfectly normal. Indeed, take $f$ that satisfies the first three clauses and by perfect normality take a function $g$ that is positive on the interior of $f^{-1}(\{0\})$ and $0$ otherwise. Then consider simply $f+g$ .","There was recently a question that I cannot find about separation in compact spaces. The answer to that question was no for trivial reasons. Motivated by that, let me ask a less trivial version of this question. Given two points in a (separable) compact Hausdorff space , can we find two open sets in and a continuous function such that for all for all has empty interior My feeling is that in general the answer should be no . Yet for familiar compact metric compacta such open sets and function can be constructed. (Of course this is not a proof.) EDIT : I modified the question by removing one of the requirements; I believe now it will be more tractable. It seems that the answer is yes if is perfectly normal. Indeed, take that satisfies the first three clauses and by perfect normality take a function that is positive on the interior of and otherwise. Then consider simply .","x,y X V,U X f\colon X\to \mathbb{R} x\in V, y\in U f(t)=1 t\in V f(t)<0 t\in U f^{-1}(\{0\}) f X f g f^{-1}(\{0\}) 0 f+g","['real-analysis', 'general-topology', 'proof-verification', 'continuity']"
59,Multivariate Weierstrass theorem?,Multivariate Weierstrass theorem?,,"The Weierstrass theorem states that for any continuous function $f$ of one variable there is a sequence of polynomials that uniformly converge to $f$. To my surprise, I couldn't find any reference to similar results (either positive or negative) for the multivariate case, i.e. when $f \in C([0, 1]^n), n > 1$. I know about Kolmogorov's theorem but I can't see how can it apply in this case (I don't if there is a version in which the ""inner"" functions are just polynomials; approximating them would produce hard to quantify errors).","The Weierstrass theorem states that for any continuous function $f$ of one variable there is a sequence of polynomials that uniformly converge to $f$. To my surprise, I couldn't find any reference to similar results (either positive or negative) for the multivariate case, i.e. when $f \in C([0, 1]^n), n > 1$. I know about Kolmogorov's theorem but I can't see how can it apply in this case (I don't if there is a version in which the ""inner"" functions are just polynomials; approximating them would produce hard to quantify errors).",,"['real-analysis', 'continuity', 'uniform-continuity', 'approximation-theory']"
60,"0 to the power of 0, what does the essential discontinuity actually look like?","0 to the power of 0, what does the essential discontinuity actually look like?",,"So having watch this clip by Numberphile which explains why $0^0$ is undefined https://www.youtube.com/watch?v=BRRolKTlF6Q And also this Link And also this Zero to the zero power – is $0^0=1$? I understand how when given a function of the form $x^y$ then you have the following results $$\lim_{x\rightarrow 0} x^0=1,$$ $$\lim_{y\rightarrow 0^+} 0^y=0,$$ and both the video and the mathforum link (and numerous others) mentioned about how when approaching form different directions you get different answers However, both google and mathematica failed to actually show what the discontinuity look like While a similar problematic case of "" $\frac{0}{0}$ "" indeterminate form gives rather consistently the shape of the essential singularity attempt to do so for the case $x^y$ in google and mathematica does not really illuminate the shape of the essential singularity By approaching zero from the x axis, and also analysing the x and y partial derivatives of the function $x^y$ I can clearly see a jump However I don't quite get what the result for the x^y cases as you approach from directions that are not x or y axis geometrically look like For example, for the "" $\frac{0}{0}$ "" case it is easy to see why Numberphile said the value of $\frac{0}{0}$ depends on the angle you approach the origin But for $0^0$ ... While algebraically the limits clearly evaluated to different values depending on how we approach it, geometrically it does not seeemed to agree with what the limits said (the curve look smooth and continuous...probably due to the limitation of the graphing programs) ""==============================================="" So to sum up my question, what does the discontinuity of $0^0$ actually look like, is it like an oscillation, a jump, a dot or something more complicated? ""==============================================="" UPDATE to ask for clarification, which should be in the comment section had it has a ""posting image function"") Using the answers of Aes and Meelo, and examining the plots of the curves and parametric curves used by them to investigate the limiting behavior, I got the following So the singularity has a shape of the vertical line interval (0,1) as mentioned by Aes and its neighborhood is as mentioned by Meelo has a very steep but non vertical ""fault like structure""near the x axis which using the general form of a curve that can give limits that are between 0 and 1, as shown by Aes, explains why we need to travel along these curves (so that part of our journey is on that steep fault) in order not not get dragged into the value of 1 Or in short, based on the answers, is my understanding as shown below the correct way to understand it?","So having watch this clip by Numberphile which explains why is undefined https://www.youtube.com/watch?v=BRRolKTlF6Q And also this Link And also this Zero to the zero power – is $0^0=1$? I understand how when given a function of the form then you have the following results and both the video and the mathforum link (and numerous others) mentioned about how when approaching form different directions you get different answers However, both google and mathematica failed to actually show what the discontinuity look like While a similar problematic case of "" "" indeterminate form gives rather consistently the shape of the essential singularity attempt to do so for the case in google and mathematica does not really illuminate the shape of the essential singularity By approaching zero from the x axis, and also analysing the x and y partial derivatives of the function I can clearly see a jump However I don't quite get what the result for the x^y cases as you approach from directions that are not x or y axis geometrically look like For example, for the "" "" case it is easy to see why Numberphile said the value of depends on the angle you approach the origin But for ... While algebraically the limits clearly evaluated to different values depending on how we approach it, geometrically it does not seeemed to agree with what the limits said (the curve look smooth and continuous...probably due to the limitation of the graphing programs) ""==============================================="" So to sum up my question, what does the discontinuity of actually look like, is it like an oscillation, a jump, a dot or something more complicated? ""==============================================="" UPDATE to ask for clarification, which should be in the comment section had it has a ""posting image function"") Using the answers of Aes and Meelo, and examining the plots of the curves and parametric curves used by them to investigate the limiting behavior, I got the following So the singularity has a shape of the vertical line interval (0,1) as mentioned by Aes and its neighborhood is as mentioned by Meelo has a very steep but non vertical ""fault like structure""near the x axis which using the general form of a curve that can give limits that are between 0 and 1, as shown by Aes, explains why we need to travel along these curves (so that part of our journey is on that steep fault) in order not not get dragged into the value of 1 Or in short, based on the answers, is my understanding as shown below the correct way to understand it?","0^0 x^y \lim_{x\rightarrow 0} x^0=1, \lim_{y\rightarrow 0^+} 0^y=0, \frac{0}{0} x^y x^y \frac{0}{0} \frac{0}{0} 0^0 0^0","['calculus', 'real-analysis', 'visualization']"
61,An interesting series $\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(2k+3)^n-1}$,An interesting series,\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(2k+3)^n-1},"Here is an interesting series that unexpectedly (at least to me) evaluates to a nice value, that is $$\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(2k+3)^n-1}=1-\log(2)$$ As you can see, it's not one of the series we meet here daily, and I suppose the ways to approach it might be a good lesson to learn. Any hint, clue for a good starting point is welcome. Here is a supplementary question: Find the closed form of $$\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(3k+4)^n-1}$$","Here is an interesting series that unexpectedly (at least to me) evaluates to a nice value, that is $$\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(2k+3)^n-1}=1-\log(2)$$ As you can see, it's not one of the series we meet here daily, and I suppose the ways to approach it might be a good lesson to learn. Any hint, clue for a good starting point is welcome. Here is a supplementary question: Find the closed form of $$\sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{1}{(3k+4)^n-1}$$",,"['calculus', 'real-analysis', 'sequences-and-series']"
62,Convergence of $\sum_{n=1}^{\infty}\frac{1}{3^n\ \sin(n)}$,Convergence of,\sum_{n=1}^{\infty}\frac{1}{3^n\ \sin(n)},Does this series converge? Root test and ratio test are inconclusive.,Does this series converge? Root test and ratio test are inconclusive.,,"['real-analysis', 'sequences-and-series']"
63,"$f$ continuous, monotone, what do we know about differentiability?","continuous, monotone, what do we know about differentiability?",f,"I am interested in knowing what we can say in general about when a continuous function $f:\mathbb{R} \to \mathbb{R}$ is differentiable. To my mind, there are various ways a continuous function can fail to be differentiable. It could have a corner (i.e. its left-derivative is not equal to its right-derivative, but both exist). It could oscillate wildly, like $x\sin \dfrac{1}{x}$ at $x=0$. I'm not really sure if there are other options. For instance, suppose we eliminate the oscillation by saying that $f$ is monotone on $[a,b]$. Can we then say for instance that its left-derivative exists almost everywhere?","I am interested in knowing what we can say in general about when a continuous function $f:\mathbb{R} \to \mathbb{R}$ is differentiable. To my mind, there are various ways a continuous function can fail to be differentiable. It could have a corner (i.e. its left-derivative is not equal to its right-derivative, but both exist). It could oscillate wildly, like $x\sin \dfrac{1}{x}$ at $x=0$. I'm not really sure if there are other options. For instance, suppose we eliminate the oscillation by saying that $f$ is monotone on $[a,b]$. Can we then say for instance that its left-derivative exists almost everywhere?",,"['real-analysis', 'derivatives']"
64,Measure of image of critical points set is equal 0,Measure of image of critical points set is equal 0,,"Let $f:\mathbb{R}\to \mathbb{R}$ be $C^1$ function and $K = \{x : f'(x) = 0 \} $.   Show that $\mu \left(f\left(K\right)\right) = 0$, where $\mu$ is Lebesgue measure. My attempt was following: $$\mu \left(f\left(K\right)\right)= \int_{f(k)} 1 dy \stackrel{(*)}{=} \int_{K}f'(x) dx = \mu\left(K\right) \cdot 0 = 0$$ but we cannot substitute $y = f(x)$ at $(*)$ like that. I was told that there exists quite elementary proof (not using Sard's theorem) so I'm looking for it.","Let $f:\mathbb{R}\to \mathbb{R}$ be $C^1$ function and $K = \{x : f'(x) = 0 \} $.   Show that $\mu \left(f\left(K\right)\right) = 0$, where $\mu$ is Lebesgue measure. My attempt was following: $$\mu \left(f\left(K\right)\right)= \int_{f(k)} 1 dy \stackrel{(*)}{=} \int_{K}f'(x) dx = \mu\left(K\right) \cdot 0 = 0$$ but we cannot substitute $y = f(x)$ at $(*)$ like that. I was told that there exists quite elementary proof (not using Sard's theorem) so I'm looking for it.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
65,Is there a smooth compact set between any two compact sets?,Is there a smooth compact set between any two compact sets?,,"today I saw the following statement and of course believe that this is true but I don't know how to prove it rigorously (and neither do my colleagues). Let $\Omega \subset \mathbb{R}^n$ be open and bounded and $K\subset \Omega$ compact. Then there is a set $A$ such that $K\subset A\subset \Omega$ with $\partial A$ smooth (smooth means at least $C^1$). However, my first ansatz was: Mollify the characteristic function of $K$ such that its support is in $\Omega$ and denote this functions by $g$. Now my intention was to use the regular value theorem on $g$. Like $g^{-1}(1/2)$ should be a smooth manifold. But I can't show whether this point is a regular point nor do I know that $\{x :g(x)\geq 1/2\} \supset K$. Does anyone have a different or additional idea on this?","today I saw the following statement and of course believe that this is true but I don't know how to prove it rigorously (and neither do my colleagues). Let $\Omega \subset \mathbb{R}^n$ be open and bounded and $K\subset \Omega$ compact. Then there is a set $A$ such that $K\subset A\subset \Omega$ with $\partial A$ smooth (smooth means at least $C^1$). However, my first ansatz was: Mollify the characteristic function of $K$ such that its support is in $\Omega$ and denote this functions by $g$. Now my intention was to use the regular value theorem on $g$. Like $g^{-1}(1/2)$ should be a smooth manifold. But I can't show whether this point is a regular point nor do I know that $\{x :g(x)\geq 1/2\} \supset K$. Does anyone have a different or additional idea on this?",,"['real-analysis', 'general-topology', 'functional-analysis']"
66,Does $f'$ analytic imply $f$ analytic?,Does  analytic imply  analytic?,f' f,"If $f'$ is known to be analytic, does it mean that $f$ is analytic as well? I've tried to expand $f$ and then to replace the tail of it by the expansion of $f'$, yet the factorials don't add up. I also tried to start with the known-to-converge expansion of $f'$ yet it was unclear how to move to $f$ (I didn't have integration yet). If the statement isn't true then how does one prove, for example, that $f(x)=-\log\cos(x)$ is analytic in zero by using the fact that its derivative $\tan(x) = \sum_{n=1}^\infty (-1)^{n-1} 2^{2n}(2^{2n}-1) B_{2n}x^{2n-1}/(2n)!$ is analytic in zero?","If $f'$ is known to be analytic, does it mean that $f$ is analytic as well? I've tried to expand $f$ and then to replace the tail of it by the expansion of $f'$, yet the factorials don't add up. I also tried to start with the known-to-converge expansion of $f'$ yet it was unclear how to move to $f$ (I didn't have integration yet). If the statement isn't true then how does one prove, for example, that $f(x)=-\log\cos(x)$ is analytic in zero by using the fact that its derivative $\tan(x) = \sum_{n=1}^\infty (-1)^{n-1} 2^{2n}(2^{2n}-1) B_{2n}x^{2n-1}/(2n)!$ is analytic in zero?",,"['calculus', 'real-analysis', 'analyticity']"
67,Is this function completely monotone?,Is this function completely monotone?,,"Background Long ago I bumped into an exercise in ordinary differential equations, which asks to find a solution to the differential equation: $$h'(x)=\frac{1}{2(1+xh(x))}$$ It turns out that $h(x)$ is the inverse function of $$G(x)=2e^{x^2}\int_0^xe^{-u^2}\,du=\sqrt{\pi}e^{x^2}\hbox{erf}(x)$$ where $\hbox{erf}(x)$ is the standard error function. This function $G(x)$ (multiplied by $1/2$ ), appears in the problem set of the second chapter of the book ""Special functions and their applications"", by Lebedev, where the reader is asked to prove that it satisfies a certain differential equation, namely, $$G'(x)=2(1+xG(x))$$ whence it easily follows that the inverse $G^{-1}(x)=h(x)$ satisfies the ODE above for $h$ . In Lebedev's book the reader is asked to use the differential equation for $G$ to derive a series expansion for the error function, valid in the entire complex plain. In terms of $G$ , the series is as follows: $$G(z)=\sum_{k=0}^{\infty}\frac{2^kz^{2k+1}}{(2k+1)!!}\qquad (z\in\mathbb{C})$$ Lebedev leaves it at that, but some further comments may be of interest. A simple calculation involving Stirling's formula shows that this series has an infinite radius of convergence, hence it defines an entire function in the complex plain. Moreover, restricting to positive real values only, the series expansion shows that all derivatives of $G$ are positive in $(0,\infty)$ , i.e., $G(x)$ is an absolutely monotone function in $(0,\infty)$ . Moreover, since $G'(0)\neq 0$ , there is a disc around zero where the inverse function $G^{-1}(z)=h(z)$ is analytic, and its series expansion can be computed from that of $G(z)$ . However, it turns out that the radius of convergence of the series for $G^{-1}(z)$ around zero is finite. This perhaps is connected to the fact that $G(z)$ is not one-to-one in the whole complex plain, i.e., does not have a global inverse. The problem Looking into various properties of the inverse function $h(x)=G^{-1}(x)$ , I came across the function $g(x)=\exp(G^{-1}(x)^2)=\exp(h^2(x))$ , where $x\geq 0$ . Recall that a function $f(x)$ defined on $(0,\infty)$ is called completely monotone if it has derivatives of all order and $(-1)^nf^{(n)}(x)\geq 0$ for all $n=0,1,2,\dots$ . The problem is this: Is the function $g''(\sqrt{x})$ completely monotone in $[0,\infty)$ ? Using the differential equation for $h$ above, the second derivative is $$g''(x)=\frac{\exp(h^2(x))}{2(1+xh(x))^3}$$ so $$g''(\sqrt{x})=\frac{\exp(h^2(\sqrt{x}))}{2(1+\sqrt{x}h(\sqrt{x}))^3}$$ Since this is a composition of $g''(x)$ and the square root function $\sqrt{x}$ , and since the derivative of $\sqrt{x}$ is completely monotone, then if we knew that $g''(x)$ is itself completely monotone, we would be able to deduce that $g''(\sqrt{x})$ is completely monotone as well. But that is not the case: $g''(x)$ is not completely monotone, because, for example, it is not convex. Its second derivative, the fourth derivative of $g(x)$ , is not a positive function. A partial result With $f(x)=g''(\sqrt{x})$ , the following holds. For every $n$ , there is some $x_n>0$ , such that for all $x>x_n$ we have $(-1)^nf^{(n)}(x)>0$ . My proof is neither elegant nor short. Moreover, I still don't know whether $(-1)^nf^{(n)}(x)>0$ holds for all $x$ . There is lots of information and literature involving completely monotone functions. For example, the property of being completely monotone is equivalent to the property of being the Laplace transform of a positive measure, and in our case, since $g''(0)=\frac{1}{2}<\infty$ , we would have a bounded positive measure. There are lots of tricks and special cases that help determine whether a function is completely monotone, but as far as my research has reached, I could not settle the case for this specific function. It might be interesting to discover new methods of proving -- or disproving -- that a certain function is completely monotone. Any insights, comments, remarks and proofs are welcome.","Background Long ago I bumped into an exercise in ordinary differential equations, which asks to find a solution to the differential equation: It turns out that is the inverse function of where is the standard error function. This function (multiplied by ), appears in the problem set of the second chapter of the book ""Special functions and their applications"", by Lebedev, where the reader is asked to prove that it satisfies a certain differential equation, namely, whence it easily follows that the inverse satisfies the ODE above for . In Lebedev's book the reader is asked to use the differential equation for to derive a series expansion for the error function, valid in the entire complex plain. In terms of , the series is as follows: Lebedev leaves it at that, but some further comments may be of interest. A simple calculation involving Stirling's formula shows that this series has an infinite radius of convergence, hence it defines an entire function in the complex plain. Moreover, restricting to positive real values only, the series expansion shows that all derivatives of are positive in , i.e., is an absolutely monotone function in . Moreover, since , there is a disc around zero where the inverse function is analytic, and its series expansion can be computed from that of . However, it turns out that the radius of convergence of the series for around zero is finite. This perhaps is connected to the fact that is not one-to-one in the whole complex plain, i.e., does not have a global inverse. The problem Looking into various properties of the inverse function , I came across the function , where . Recall that a function defined on is called completely monotone if it has derivatives of all order and for all . The problem is this: Is the function completely monotone in ? Using the differential equation for above, the second derivative is so Since this is a composition of and the square root function , and since the derivative of is completely monotone, then if we knew that is itself completely monotone, we would be able to deduce that is completely monotone as well. But that is not the case: is not completely monotone, because, for example, it is not convex. Its second derivative, the fourth derivative of , is not a positive function. A partial result With , the following holds. For every , there is some , such that for all we have . My proof is neither elegant nor short. Moreover, I still don't know whether holds for all . There is lots of information and literature involving completely monotone functions. For example, the property of being completely monotone is equivalent to the property of being the Laplace transform of a positive measure, and in our case, since , we would have a bounded positive measure. There are lots of tricks and special cases that help determine whether a function is completely monotone, but as far as my research has reached, I could not settle the case for this specific function. It might be interesting to discover new methods of proving -- or disproving -- that a certain function is completely monotone. Any insights, comments, remarks and proofs are welcome.","h'(x)=\frac{1}{2(1+xh(x))} h(x) G(x)=2e^{x^2}\int_0^xe^{-u^2}\,du=\sqrt{\pi}e^{x^2}\hbox{erf}(x) \hbox{erf}(x) G(x) 1/2 G'(x)=2(1+xG(x)) G^{-1}(x)=h(x) h G G G(z)=\sum_{k=0}^{\infty}\frac{2^kz^{2k+1}}{(2k+1)!!}\qquad (z\in\mathbb{C}) G (0,\infty) G(x) (0,\infty) G'(0)\neq 0 G^{-1}(z)=h(z) G(z) G^{-1}(z) G(z) h(x)=G^{-1}(x) g(x)=\exp(G^{-1}(x)^2)=\exp(h^2(x)) x\geq 0 f(x) (0,\infty) (-1)^nf^{(n)}(x)\geq 0 n=0,1,2,\dots g''(\sqrt{x}) [0,\infty) h g''(x)=\frac{\exp(h^2(x))}{2(1+xh(x))^3} g''(\sqrt{x})=\frac{\exp(h^2(\sqrt{x}))}{2(1+\sqrt{x}h(\sqrt{x}))^3} g''(x) \sqrt{x} \sqrt{x} g''(x) g''(\sqrt{x}) g''(x) g(x) f(x)=g''(\sqrt{x}) n x_n>0 x>x_n (-1)^nf^{(n)}(x)>0 (-1)^nf^{(n)}(x)>0 x g''(0)=\frac{1}{2}<\infty",['real-analysis']
68,"Find a permutation $x_{\sigma(1)},\ldots,x_{\sigma(n)}$ of $x_1,\ldots,x_n$ that maximises $\sum_{k=1}^{n-1}\vert x_{\sigma(k)}-x_{\sigma(k+1)}\vert.$",Find a permutation  of  that maximises,"x_{\sigma(1)},\ldots,x_{\sigma(n)} x_1,\ldots,x_n \sum_{k=1}^{n-1}\vert x_{\sigma(k)}-x_{\sigma(k+1)}\vert.","Suppose $\ x_1,\ x_2,\ \ldots,\ x_n\ $ are real numbers with $\ x_1 < x_2 <\ldots < x_n.$ Is there an efficient way to find a permutation $\ x_{\sigma(1)},\ x_{\sigma(2)},\ \ldots,\ x_{\sigma(n)}\ $ of $\ x_1,\ x_2,\ \ldots,\ x_n\ $ which maximises $\ f\left(\ \left( x_{\sigma(1)},\ x_{\sigma(2),\ \ldots,\ x_{\sigma(k)} } \right)\ \right) = \displaystyle\sum_{k=1}^{n-1} \left\vert x_{\sigma(k)} - x_{\sigma(k+1)} \right\vert\ ? $ My intuition tells me that either $\ f(\ \left(x_1,\ x_n,\ x_2,\ x_{n-1},\ x_3,\ x_{n-2},\ \ldots)\ \right)\ $ or $\ f(\ \left(x_n,\ x_1,\ x_{n-1},\ x_2,\ x_{n-2},\ x_3,\ \ldots\ \right)\ )\ $ might be a maximum, but I am not sure about this or how to prove if this is true. Edit: my intuition is wrong. For example, take $\ x_1= 1,\ x_2 = 2,\ x_3=3,\ x_4=4.$ Then neither $\ x_1\to x_4\to x_2 \to x_3,\ $ nor $\ x_4\to x_1 \to x_3\to x_2\ $ are the longest route (both have length $6$ ), since $\ x_3\to x_1\to x_4\to x_2\ $ is longer with length $7$ .","Suppose are real numbers with Is there an efficient way to find a permutation of which maximises My intuition tells me that either or might be a maximum, but I am not sure about this or how to prove if this is true. Edit: my intuition is wrong. For example, take Then neither nor are the longest route (both have length ), since is longer with length .","\ x_1,\ x_2,\ \ldots,\ x_n\  \ x_1 < x_2 <\ldots < x_n. \ x_{\sigma(1)},\ x_{\sigma(2)},\ \ldots,\ x_{\sigma(n)}\  \ x_1,\ x_2,\ \ldots,\ x_n\  \ f\left(\ \left( x_{\sigma(1)},\ x_{\sigma(2),\ \ldots,\ x_{\sigma(k)} } \right)\ \right) = \displaystyle\sum_{k=1}^{n-1} \left\vert x_{\sigma(k)} - x_{\sigma(k+1)} \right\vert\ ?  \ f(\ \left(x_1,\ x_n,\ x_2,\ x_{n-1},\ x_3,\ x_{n-2},\ \ldots)\ \right)\  \ f(\ \left(x_n,\ x_1,\ x_{n-1},\ x_2,\ x_{n-2},\ x_3,\ \ldots\ \right)\ )\  \ x_1= 1,\ x_2 = 2,\ x_3=3,\ x_4=4. \ x_1\to x_4\to x_2 \to x_3,\  \ x_4\to x_1 \to x_3\to x_2\  6 \ x_3\to x_1\to x_4\to x_2\  7","['real-analysis', 'algorithms', 'discrete-optimization', 'rearrangement-inequality']"
69,Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function that satisfies $f(q+1/n)=f(q)$ for every $q\in\mathbb{Q}$ and for every $n\in\mathbb{N}$.,Let  be a continuous function that satisfies  for every  and for every .,f:\mathbb{R}\to\mathbb{R} f(q+1/n)=f(q) q\in\mathbb{Q} n\in\mathbb{N},"Question: Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function that satisfies $f(q+\frac{1}{n})=f(q)$ for every $q\in\mathbb{Q}$ and for every $n\in\mathbb{N}$ . Show that $f$ must be a constant function. Solution: Substituting $q-\frac{1}{n}$ for $q$ in $$f(q+\frac{1}{n})=f(q),$$ we have $$f(q)=f(q-\frac{1}{n})$$ for all $q\in\mathbb{Q}$ and for all $n\in\mathbb{N}$ . Let the original relation be denoted by $(*)$ and the derived relation be denoted by $(**)$ . Now select any positive rational number $q=\frac{r}{s},$ where $r,s\in\mathbb{N}$ . Thus, by repeated application of $(**)$ , we have $$f\left(\frac{r}{s}\right)=f\left(\frac{r-1}{s}\right)=f\left(\frac{r-2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0).$$ Next select any negative rational number $q=\frac{r}{s}$ , where $r\in\mathbb{Z}_{\le 1}$ and $s\in\mathbb{N}.$ Thus, by repeated application of $(*),$ we have $$f\left(\frac{r}{s}\right)=f\left(\frac{r+1}{s}\right)=f\left(\frac{r+2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0).$$ Thus, for all $q\in\mathbb{Q}$ , we have $f(q)=f(0)$ . Next select any irrational number $q'$ . We know that there exists a convergent sequence $(x_n)_{n\ge 1}$ of rational numbers such that it converges to $q'$ . Now since $f$ is continuous on $\mathbb{R}$ , implies that it is continuous at $q'$ . Thus, by the sequential definition of limit we can conclude that the sequence $f(x_n)$ converges to $f(q').$ Now note that $f(x_n)=f(0)$ , for all $n\in\mathbb{N}$ . This implies that $f(x_n)$ converges to $f(0)$ , which in turn implies that $f(q')=f(0)$ . Now since $q'$ is arbitrary, therefore, $f(q')=f(0)$ for all irrational numbers $q'$ . Thus, we can conclude that $f(x)=f(0)$ for all $x\in\mathbb{R}$ , that is $f$ is a constant function. Is this solution correct and rigorous enough and is there any other way to solve the problem?","Question: Let be a continuous function that satisfies for every and for every . Show that must be a constant function. Solution: Substituting for in we have for all and for all . Let the original relation be denoted by and the derived relation be denoted by . Now select any positive rational number where . Thus, by repeated application of , we have Next select any negative rational number , where and Thus, by repeated application of we have Thus, for all , we have . Next select any irrational number . We know that there exists a convergent sequence of rational numbers such that it converges to . Now since is continuous on , implies that it is continuous at . Thus, by the sequential definition of limit we can conclude that the sequence converges to Now note that , for all . This implies that converges to , which in turn implies that . Now since is arbitrary, therefore, for all irrational numbers . Thus, we can conclude that for all , that is is a constant function. Is this solution correct and rigorous enough and is there any other way to solve the problem?","f:\mathbb{R}\to\mathbb{R} f(q+\frac{1}{n})=f(q) q\in\mathbb{Q} n\in\mathbb{N} f q-\frac{1}{n} q f(q+\frac{1}{n})=f(q), f(q)=f(q-\frac{1}{n}) q\in\mathbb{Q} n\in\mathbb{N} (*) (**) q=\frac{r}{s}, r,s\in\mathbb{N} (**) f\left(\frac{r}{s}\right)=f\left(\frac{r-1}{s}\right)=f\left(\frac{r-2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0). q=\frac{r}{s} r\in\mathbb{Z}_{\le 1} s\in\mathbb{N}. (*), f\left(\frac{r}{s}\right)=f\left(\frac{r+1}{s}\right)=f\left(\frac{r+2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0). q\in\mathbb{Q} f(q)=f(0) q' (x_n)_{n\ge 1} q' f \mathbb{R} q' f(x_n) f(q'). f(x_n)=f(0) n\in\mathbb{N} f(x_n) f(0) f(q')=f(0) q' f(q')=f(0) q' f(x)=f(0) x\in\mathbb{R} f","['real-analysis', 'sequences-and-series', 'solution-verification']"
70,Solving used Real Based Methods: $\int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt$,Solving used Real Based Methods:,\int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt,"In working on integrals for the past couple of months, I've come across different cases of the following integral: \begin{equation} I\left(x,a,k,n,m\right) = \int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt \end{equation} Where $x,a\in \mathbb{R}^{+}$ . Here the method that I've taken is rather simple and I was curious as to other 'Real' Based methods could be employed with this integral? I also believe that with the conditions I've set on the parameters that it is convergent. If I'm able to expand those conditions, could you please advise. Interested in special cases too! The method I took: First I wanted to bring the 'a' out the front: \begin{equation} I(x,a,k,n,m) =  \int_0^x \frac{t^k}{\left(a\left[\left(a^{-\frac{1}{n}}t\right)^n + 1\right]\right)^m}\:dt = \frac{1}{a^m} \int_0^x \frac{t^k}{\left(\left(a^{-\frac{1}{n}}t\right)^n + 1\right)^m}\:dt \end{equation} Here let $u = a^{-\frac{1}{n}}t$ Thus, \begin{equation}  I(x,a,k,n,m) = \frac{1}{a^m} \int_0^{a^{-\frac{1}{n}}x} \frac{\left(a^{\frac{1}{n}}u\right)^k}{\left(u^n + 1\right)^m}a^{\frac{1}{n}}\:du = a^{\frac{k + 1}{n} - m}\int_0^{a^{-\frac{1}{n}}x} \frac{u^k}{\left(u^n + 1\right)^m}\:du = a^{\frac{k + 1}{n} - m}I(a^{-\frac{1}{n}}x,1,k,n,m) \end{equation} From here I will use $I$ in place of $I(x,a,k,n,m)$ for ease of typing. The next step is to make the substitution $w = u^n$ to yield: \begin{equation}  I = a^{\frac{k + 1}{n} - m}\int_0^{ax^n} \frac{w^\frac{k}{n}}{\left(w + 1\right)^m}\frac{\:dw}{nw^{\frac{n - 1}{n}}} = \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_0^{ax^n} \frac{w^{\frac{k + 1}{n} - 1}}{\left(w + 1\right)^m}\:dw \end{equation} Here make the substitution $z = \frac{1}{1 + w}$ to yield: \begin{align} I &= \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_1^{\frac{1}{1 + ax^n}} z^m \left(\frac{1 - z}{z}\right)^{\frac{k + 1}{n} - 1}\left(-\frac{1}{z^2}\right) \:dz = \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_{\frac{1}{1 + ax^n}}^1 z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz \\ &= \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[\int_0^1 z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz - \int_0^{\frac{1}{1 + ax^n}} z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz \ \right] \\ &= \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right) -  B\left( \frac{1}{1 + ax^n}; m - \frac{k + 1}{n}, \frac{k + 1}{n}  \right)\right] \end{align} Where $B(a,b)$ is the Beta Function and $B(x; a,b)$ is the Incomplete Beta Function . And so, we arrive at: \begin{equation}  \int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt = \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right) -  B\left(\frac{1}{1 + ax^n}; m - \frac{k + 1}{n}, \frac{k + 1}{n}  \right)\right] \end{equation} Here we observe that for convergence: \begin{equation}  m - \frac{k + 1}{n} \gt 0,\quad  \frac{k + 1}{n} \gt 0,\quad n \neq 0 \end{equation} Note: when $x \rightarrow \infty$ we have: \begin{equation}  \int_0^\infty \frac{t^k}{\left(t^n + a\right)^m}\:dt = \frac{1}{n}a^{\frac{k + 1}{n} - m} B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right)  \end{equation} Update: Today I realised that we can use this result for another integral: \begin{equation}  \int_0^\infty \frac{\ln(t)}{\left(t^n + 1\right)^m}\:dt \end{equation} This is achieved through a simple use of Feynman's Trick. Here we  consider the case when $x \rightarrow \infty$ and $a = 1$ . We see that \begin{align} \frac{d}{dk}\left[ \int_0^\infty \frac{t^k}{\left(t^n + 1\right)^m}\:dt \right]&= \frac{d}{dk}\left[\frac{1}{n}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\right] \\  \int_0^\infty \frac{t^k \ln(t)}{\left(t^n + 1\right)^m}\:dt &= \frac{1}{n^2}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\left[\psi^{(0)}\left(\frac{k + 1}{n}\right) - \psi^{(0)}\left(m - \frac{k + 1}{n}\right) \right] \end{align} Thus, \begin{equation} \lim_{k \rightarrow 0} \int_0^\infty \frac{t^k \ln(t)}{\left(t^n + 1\right)^m}\:dt = \lim_{k \rightarrow 0}\frac{1}{n^2}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\left[\psi^{(0)}\left(\frac{k + 1}{n}\right) - \psi^{(0)}\left(m - \frac{k + 1}{n}\right) \right] \end{equation} And finally: \begin{equation}  \int_0^\infty \frac{ \ln(t)}{\left(t^n + 1\right)^m}\:dt = \frac{1}{n^2}B\left(m - \frac{1}{n}, \frac{1}{n} \right)\left[\psi^{(0)}\left(\frac{1}{n}\right) - \psi^{(0)}\left(m - \frac{1}{n}\right) \right] \end{equation} Note: In the case where $m = 1$ we arrive: \begin{align}  \int_0^\infty \frac{ \ln(t)}{\left(t^n + 1\right)^1}\:dt &= \frac{1}{n^2}B\left(1 - \frac{1}{n}, \frac{1}{n} \right)\left[\psi^{(0)}\left(\frac{1}{n}\right) - \psi^{(0)}\left(1 - \frac{1}{n}\right) \right] \\ &= \frac{1}{n^2} \Gamma\left(\frac{1}{n} \right)\Gamma\left(1 - \frac{1}{n} \right) \cdot -\pi\cot\left(\frac{\pi}{n}\right) \\ &= \frac{1}{n^2} \frac{\pi}{\sin\left(\frac{\pi}{n}\right)}\cdot -\pi\cot\left(\frac{\pi}{n}\right) \end{align} Thus: \begin{equation}  \int_0^\infty \frac{ \ln(t)}{t^n + 1}\:dt = -\frac{\pi^2}{n^2} \operatorname{cosec}\left(\frac{\pi}{n} \right)\cot\left(\frac{\pi}{n}\right) \end{equation}","In working on integrals for the past couple of months, I've come across different cases of the following integral: Where . Here the method that I've taken is rather simple and I was curious as to other 'Real' Based methods could be employed with this integral? I also believe that with the conditions I've set on the parameters that it is convergent. If I'm able to expand those conditions, could you please advise. Interested in special cases too! The method I took: First I wanted to bring the 'a' out the front: Here let Thus, From here I will use in place of for ease of typing. The next step is to make the substitution to yield: Here make the substitution to yield: Where is the Beta Function and is the Incomplete Beta Function . And so, we arrive at: Here we observe that for convergence: Note: when we have: Update: Today I realised that we can use this result for another integral: This is achieved through a simple use of Feynman's Trick. Here we  consider the case when and . We see that Thus, And finally: Note: In the case where we arrive: Thus:","\begin{equation}
I\left(x,a,k,n,m\right) = \int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt
\end{equation} x,a\in \mathbb{R}^{+} \begin{equation}
I(x,a,k,n,m) =  \int_0^x \frac{t^k}{\left(a\left[\left(a^{-\frac{1}{n}}t\right)^n + 1\right]\right)^m}\:dt = \frac{1}{a^m} \int_0^x \frac{t^k}{\left(\left(a^{-\frac{1}{n}}t\right)^n + 1\right)^m}\:dt
\end{equation} u = a^{-\frac{1}{n}}t \begin{equation}
 I(x,a,k,n,m) = \frac{1}{a^m} \int_0^{a^{-\frac{1}{n}}x} \frac{\left(a^{\frac{1}{n}}u\right)^k}{\left(u^n + 1\right)^m}a^{\frac{1}{n}}\:du = a^{\frac{k + 1}{n} - m}\int_0^{a^{-\frac{1}{n}}x} \frac{u^k}{\left(u^n + 1\right)^m}\:du = a^{\frac{k + 1}{n} - m}I(a^{-\frac{1}{n}}x,1,k,n,m)
\end{equation} I I(x,a,k,n,m) w = u^n \begin{equation}
 I = a^{\frac{k + 1}{n} - m}\int_0^{ax^n} \frac{w^\frac{k}{n}}{\left(w + 1\right)^m}\frac{\:dw}{nw^{\frac{n - 1}{n}}} = \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_0^{ax^n} \frac{w^{\frac{k + 1}{n} - 1}}{\left(w + 1\right)^m}\:dw
\end{equation} z = \frac{1}{1 + w} \begin{align}
I &= \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_1^{\frac{1}{1 + ax^n}} z^m \left(\frac{1 - z}{z}\right)^{\frac{k + 1}{n} - 1}\left(-\frac{1}{z^2}\right) \:dz = \frac{1}{n}a^{\frac{k + 1}{n} - m}\int_{\frac{1}{1 + ax^n}}^1 z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz \\
&= \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[\int_0^1 z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz - \int_0^{\frac{1}{1 + ax^n}} z^{m - \frac{k + 1}{n} - 1}\left(1 - z\right)^{\frac{k + 1}{n} - 1}\:dz \ \right] \\
&= \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right) -  B\left( \frac{1}{1 + ax^n}; m - \frac{k + 1}{n}, \frac{k + 1}{n}  \right)\right]
\end{align} B(a,b) B(x; a,b) \begin{equation}
 \int_0^x \frac{t^k}{\left(t^n + a\right)^m}\:dt = \frac{1}{n}a^{\frac{k + 1}{n} - m} \left[B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right) -  B\left(\frac{1}{1 + ax^n}; m - \frac{k + 1}{n}, \frac{k + 1}{n}  \right)\right]
\end{equation} \begin{equation}
 m - \frac{k + 1}{n} \gt 0,\quad  \frac{k + 1}{n} \gt 0,\quad n \neq 0
\end{equation} x \rightarrow \infty \begin{equation}
 \int_0^\infty \frac{t^k}{\left(t^n + a\right)^m}\:dt = \frac{1}{n}a^{\frac{k + 1}{n} - m} B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right) 
\end{equation} \begin{equation}
 \int_0^\infty \frac{\ln(t)}{\left(t^n + 1\right)^m}\:dt
\end{equation} x \rightarrow \infty a = 1 \begin{align}
\frac{d}{dk}\left[ \int_0^\infty \frac{t^k}{\left(t^n + 1\right)^m}\:dt \right]&= \frac{d}{dk}\left[\frac{1}{n}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\right] \\
 \int_0^\infty \frac{t^k \ln(t)}{\left(t^n + 1\right)^m}\:dt &= \frac{1}{n^2}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\left[\psi^{(0)}\left(\frac{k + 1}{n}\right) - \psi^{(0)}\left(m - \frac{k + 1}{n}\right) \right]
\end{align} \begin{equation}
\lim_{k \rightarrow 0} \int_0^\infty \frac{t^k \ln(t)}{\left(t^n + 1\right)^m}\:dt = \lim_{k \rightarrow 0}\frac{1}{n^2}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)\left[\psi^{(0)}\left(\frac{k + 1}{n}\right) - \psi^{(0)}\left(m - \frac{k + 1}{n}\right) \right]
\end{equation} \begin{equation}
 \int_0^\infty \frac{ \ln(t)}{\left(t^n + 1\right)^m}\:dt = \frac{1}{n^2}B\left(m - \frac{1}{n}, \frac{1}{n} \right)\left[\psi^{(0)}\left(\frac{1}{n}\right) - \psi^{(0)}\left(m - \frac{1}{n}\right) \right]
\end{equation} m = 1 \begin{align}
 \int_0^\infty \frac{ \ln(t)}{\left(t^n + 1\right)^1}\:dt &= \frac{1}{n^2}B\left(1 - \frac{1}{n}, \frac{1}{n} \right)\left[\psi^{(0)}\left(\frac{1}{n}\right) - \psi^{(0)}\left(1 - \frac{1}{n}\right) \right] \\
&= \frac{1}{n^2} \Gamma\left(\frac{1}{n} \right)\Gamma\left(1 - \frac{1}{n} \right) \cdot -\pi\cot\left(\frac{\pi}{n}\right) \\
&= \frac{1}{n^2} \frac{\pi}{\sin\left(\frac{\pi}{n}\right)}\cdot -\pi\cot\left(\frac{\pi}{n}\right)
\end{align} \begin{equation}
 \int_0^\infty \frac{ \ln(t)}{t^n + 1}\:dt = -\frac{\pi^2}{n^2} \operatorname{cosec}\left(\frac{\pi}{n} \right)\cot\left(\frac{\pi}{n}\right)
\end{equation}","['real-analysis', 'integration']"
71,Is there a simpler proof of this fact in analysis?,Is there a simpler proof of this fact in analysis?,,"Suppose that $f:(0,1)\to\mathbb{R}$ is differentiable, and that $f(x_1)=f(x_2)=0$ and $f’(x_1)>0$ and $f’(x_2)>0$ for some $0 <x_1<x_2<1$ . Then there must exist an $x_0\in(x_1,x_2)$ such that $f(x_0)=0$ and $f’(x_0)\leq0$ , as follows: Let $A=\{x\in(x_1,x_2):f(x)\geq0\}$ , and note that $A$ is nonempty, since the condition $f’(x_1)>0$ guarantees that there exist points that exceed $x_1$ by arbitrarily small amounts, at which $f$ is strictly positive. Also note that $A$ is bounded above, by $x_2$ . Therefore, let $x_0=\sup A$ . Note that $x_0 > x_1$ . Since $f’(x_2)>0$ , there exist points that $x_2$ exceeds by arbitrarily small positive amounts, at which $f$ is strictly negative. It follows that $x_1 < x_0 < x_2$ . By continuity of $f$ , we have that $f(x_0)=0$ . Since $f(x)<0$ for all $x\in(x_0,x_2)$ , it follows also that $f’(x_0)\leq0$ , as required. Is this proof correct, and is there a simpler proof, perhaps using a ready-made theorem such as the Intermediate Value Theorem ?","Suppose that is differentiable, and that and and for some . Then there must exist an such that and , as follows: Let , and note that is nonempty, since the condition guarantees that there exist points that exceed by arbitrarily small amounts, at which is strictly positive. Also note that is bounded above, by . Therefore, let . Note that . Since , there exist points that exceeds by arbitrarily small positive amounts, at which is strictly negative. It follows that . By continuity of , we have that . Since for all , it follows also that , as required. Is this proof correct, and is there a simpler proof, perhaps using a ready-made theorem such as the Intermediate Value Theorem ?","f:(0,1)\to\mathbb{R} f(x_1)=f(x_2)=0 f’(x_1)>0 f’(x_2)>0 0 <x_1<x_2<1 x_0\in(x_1,x_2) f(x_0)=0 f’(x_0)\leq0 A=\{x\in(x_1,x_2):f(x)\geq0\} A f’(x_1)>0 x_1 f A x_2 x_0=\sup A x_0 > x_1 f’(x_2)>0 x_2 f x_1 < x_0 < x_2 f f(x_0)=0 f(x)<0 x\in(x_0,x_2) f’(x_0)\leq0","['real-analysis', 'calculus', 'limits', 'proof-verification', 'alternative-proof']"
72,Are functions satisfying $|f(x)-f(y)|\le L \|\nabla f(x)-\nabla f(y)\|^{1+s}$ constant?,Are functions satisfying  constant?,|f(x)-f(y)|\le L \|\nabla f(x)-\nabla f(y)\|^{1+s},"Let $f:\mathbb R^n\to \mathbb R$ be continuously differentiable. Suppose that there is $L>0,s>0$ such $$ |f(x)-f(y)|\le L \|\nabla f(x)-\nabla f(y)\|^{1+s} \quad \forall x,y\in\mathbb R^n. $$ Does this imply that $f$ is constant? Clearly if $\nabla f$ is Lipschitz continuous (or $\alpha$ -Hölder continuous with $\alpha(1+s)>1$ ) then $\nabla f=0$ follows immediately. The question was inspired by this question Question about strong convexity , and my subsequent answer. There I show that also convexity of $f$ implies that $f$ is constant. So the question is: are there non-constant $C^1$ -functions satisfying the above inequality? Or is there a proof to show that a $C^1$ function satisfying the inequality is constant?","Let be continuously differentiable. Suppose that there is such Does this imply that is constant? Clearly if is Lipschitz continuous (or -Hölder continuous with ) then follows immediately. The question was inspired by this question Question about strong convexity , and my subsequent answer. There I show that also convexity of implies that is constant. So the question is: are there non-constant -functions satisfying the above inequality? Or is there a proof to show that a function satisfying the inequality is constant?","f:\mathbb R^n\to \mathbb R L>0,s>0 
|f(x)-f(y)|\le L \|\nabla f(x)-\nabla f(y)\|^{1+s} \quad \forall x,y\in\mathbb R^n.
 f \nabla f \alpha \alpha(1+s)>1 \nabla f=0 f f C^1 C^1",['real-analysis']
73,Is there any way to systematically do all epsilon delta proofs?,Is there any way to systematically do all epsilon delta proofs?,,"If you want to prove that the limit of $f(x)$ as $x$ to $a$ is equal to $L$ using the epsilon-delta definition of the limit, you need to solve the inequality $$|f(x)-L|<\epsilon$$ for $x$, getting it into the form $$|x-a|<\delta$$ for some $\delta$, which will in general be a function of $\epsilon$. My question is, is there some way to calculate the function $\delta(\epsilon)$, short of solving the inequality above using the function $f$ you have? Is it at least possible if $f$ is sufficiently well behaved?  Like if $f$ is differentiable, can you calculate $\delta(\epsilon)$ using the derivative of $f$? EDIT: This journal paper shows a formula for polynomials.  If $f(x) = \sum_{n=0}^{k} a_n (x-a)^n$, then to prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$, we can let $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{k} |a_n|})$. Can this be generalized to Taylor series?  If $f(x) = \sum_{n=0}^{\infty} a_n (x-a)^n$, then can we prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$ by letting $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{\infty} |a_n|})$ ?","If you want to prove that the limit of $f(x)$ as $x$ to $a$ is equal to $L$ using the epsilon-delta definition of the limit, you need to solve the inequality $$|f(x)-L|<\epsilon$$ for $x$, getting it into the form $$|x-a|<\delta$$ for some $\delta$, which will in general be a function of $\epsilon$. My question is, is there some way to calculate the function $\delta(\epsilon)$, short of solving the inequality above using the function $f$ you have? Is it at least possible if $f$ is sufficiently well behaved?  Like if $f$ is differentiable, can you calculate $\delta(\epsilon)$ using the derivative of $f$? EDIT: This journal paper shows a formula for polynomials.  If $f(x) = \sum_{n=0}^{k} a_n (x-a)^n$, then to prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$, we can let $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{k} |a_n|})$. Can this be generalized to Taylor series?  If $f(x) = \sum_{n=0}^{\infty} a_n (x-a)^n$, then can we prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$ by letting $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{\infty} |a_n|})$ ?",,"['calculus', 'real-analysis', 'limits', 'proof-writing', 'epsilon-delta']"
74,Riesz representation theorem in measure theory,Riesz representation theorem in measure theory,,"Let $\Omega\subseteq\mathbb{R}^{n}$ be an open set and let $C_{c}\left(\Omega\rightarrow\mathbb{R}\right)$ be the space of continuous, compactly supported functions on $\Omega.$ We now consider two different topologies on $C_{c}\left(\Omega\rightarrow\mathbb{R}\right):$ $$ T_{1}:=\left[C_{c}\left(\Omega\rightarrow\mathbb{R}\right);\left|\cdot\right|_{L^{\infty}\left(\Omega\right)}\right]\quad and\quad T_{2}:=\left[C_{c}\left(\Omega\rightarrow\mathbb{R}\right);\textrm{the inductive topology}\right]. $$ We have $T_{1}$ a normed space but $T_{2}$ just a topological vector space in general. We now consider ""continuous"" (w.r.t these topologies) linear functionals on $T_{1}$ and $T_{2}:$  $$ F_{1}:T_{1}\rightarrow\mathbb{\mathbb{R}}\quad and\quad F_{2}:T_{2}\rightarrow\mathbb{R}. $$ (By definition, the statement of continuity is equivalent to: there exists $C>0$ such that $\left|\left\langle F_{1},f\right\rangle \right|\leq C\left|f\right|_{L^{\infty}\left(\Omega\right)}$; $\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right)$; and for any compact subset $K\subseteq\Omega,$ there exists $C_{K}>0$ such that $\left|\left\langle F_{2},f\right\rangle \right|\leq C_{K}\left|f\right|_{L^{\infty}\left(\Omega\right)};$ $\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right),suppf\subseteq K.$) From Exercise 16, [ https://terrytao.wordpress.com/2009/03/02/245b-notes-12-continuous-functions-on-locally-compact-hausdorff-spaces/#rrt] there exists a unique, finite (signed) Radon measure $\mu$ on $\Omega$ such that  $$ \left\langle F_{1},f\right\rangle =\intop_{\Omega}fd\mu;\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right). $$ My question is : is it true that  $$ \left\langle F_{2},f\right\rangle =\intop_{\Omega}fd\eta;\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right); $$ for some (unique, but may not be finite) signed Radon measure $\eta$ on $\Omega?$ Attempt : I think it's true. First, in the case of ""positive linear functional"", we already know it is the case [Theorem 8, https://terrytao.wordpress.com/2009/03/02/245b-notes-12-continuous-functions-on-locally-compact-hausdorff-spaces/#rrt] . Therefore, we firstly prove that: any continuous linear functional $F_{2}$ can be represented as $F_{2}=H-G,$ with $H:T_{2}\rightarrow\mathbb{R}$ and $G:T_{2}\rightarrow\mathbb{R}$ positive linear functionals. But what is the point of using the inductive topology in this stage?","Let $\Omega\subseteq\mathbb{R}^{n}$ be an open set and let $C_{c}\left(\Omega\rightarrow\mathbb{R}\right)$ be the space of continuous, compactly supported functions on $\Omega.$ We now consider two different topologies on $C_{c}\left(\Omega\rightarrow\mathbb{R}\right):$ $$ T_{1}:=\left[C_{c}\left(\Omega\rightarrow\mathbb{R}\right);\left|\cdot\right|_{L^{\infty}\left(\Omega\right)}\right]\quad and\quad T_{2}:=\left[C_{c}\left(\Omega\rightarrow\mathbb{R}\right);\textrm{the inductive topology}\right]. $$ We have $T_{1}$ a normed space but $T_{2}$ just a topological vector space in general. We now consider ""continuous"" (w.r.t these topologies) linear functionals on $T_{1}$ and $T_{2}:$  $$ F_{1}:T_{1}\rightarrow\mathbb{\mathbb{R}}\quad and\quad F_{2}:T_{2}\rightarrow\mathbb{R}. $$ (By definition, the statement of continuity is equivalent to: there exists $C>0$ such that $\left|\left\langle F_{1},f\right\rangle \right|\leq C\left|f\right|_{L^{\infty}\left(\Omega\right)}$; $\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right)$; and for any compact subset $K\subseteq\Omega,$ there exists $C_{K}>0$ such that $\left|\left\langle F_{2},f\right\rangle \right|\leq C_{K}\left|f\right|_{L^{\infty}\left(\Omega\right)};$ $\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right),suppf\subseteq K.$) From Exercise 16, [ https://terrytao.wordpress.com/2009/03/02/245b-notes-12-continuous-functions-on-locally-compact-hausdorff-spaces/#rrt] there exists a unique, finite (signed) Radon measure $\mu$ on $\Omega$ such that  $$ \left\langle F_{1},f\right\rangle =\intop_{\Omega}fd\mu;\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right). $$ My question is : is it true that  $$ \left\langle F_{2},f\right\rangle =\intop_{\Omega}fd\eta;\forall f\in C_{c}\left(\Omega\rightarrow\mathbb{R}\right); $$ for some (unique, but may not be finite) signed Radon measure $\eta$ on $\Omega?$ Attempt : I think it's true. First, in the case of ""positive linear functional"", we already know it is the case [Theorem 8, https://terrytao.wordpress.com/2009/03/02/245b-notes-12-continuous-functions-on-locally-compact-hausdorff-spaces/#rrt] . Therefore, we firstly prove that: any continuous linear functional $F_{2}$ can be represented as $F_{2}=H-G,$ with $H:T_{2}\rightarrow\mathbb{R}$ and $G:T_{2}\rightarrow\mathbb{R}$ positive linear functionals. But what is the point of using the inductive topology in this stage?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'distribution-theory', 'riesz-representation-theorem']"
75,Why does Rudin define $k = \frac{y^n-x}{n y^{n-1}}$ or $h < \frac{x - y^n}{n(y+1)^{n-1}}$ when he tries to prove that every real x has a nth root?,Why does Rudin define  or  when he tries to prove that every real x has a nth root?,k = \frac{y^n-x}{n y^{n-1}} h < \frac{x - y^n}{n(y+1)^{n-1}},"tl;dr (Too Long To Read): What is the intuition /conceptual idea to why Rudin used the number: $$ k = \frac{y^n-x}{n y^{n-1}} $$ in his proof and not some other number? It seems that that number is not random, so how could he have come up with it? My attempt: I was trying to prove theorem 1.21 from Rudin's real analysis book on my own without looking at Rudin's proof. The way I attempted to prove it was to try to prove what seemed true from my already collected intuition on the real numbers before I started to study real analysis. So I drew lots of picture which lead me to think of defining $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$: Thus, I knew that $E = \{ \bar{y} \in R_{>0} : \bar y^n < x \}$ is just the same as $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$. So it was obvious that what I had to show was that the $\alpha = supE = y = x^{1/n}$ (also the reason for defining sets like that is cuz we probably need to use the Least Upper Bound (LUB) property cuz its one of the few things we are suppose to know about analysis so far). Thus, I proceed to show $E$ is bounded and non-empty so that I was guaranteed that the sup existed since we assumed the Least Upper Bound (LUB) property (a.k.a. the completeness axiom). Then I thought I want $y = \alpha$ so one option was to try showing $y < \alpha$ AND $y > \alpha$ are false, so by trichotomy $y=\alpha$. Since I didn't have direct access to $y$ I decided to take the same strategy except with $y^n = x$ instead of $y$ and $\alpha^n$ instead of $\alpha$. Intuitively I thought well lets assume $\alpha^n < x$ and $ x < \alpha^n $. The first one is too small so hopefully it should lead to some contradiction and perhaps show $\alpha < y$ is false. Similarly the other one $ x < \alpha^n $ should be too large somehow. Then maybe we can use trichotomy to get $\alpha^n = x$ which completes the proof. I attempted the first one $\alpha^n < x$. The only other thing I knew about $\alpha$ was that $\forall \bar y \in E, \bar y^n < x$. Then I decided to combine both equation (since I was hinted to use $a^n - b^n$ cuz I saw that identity in the soln when I check for my solution that E was bounded and non-empty): $$ \alpha^n - \bar y^n < x - \bar y ^n < 0$$ then because of the hint (that I probably wouldn't had realized I needed to use) I was extremely lucky and decided to subtract $\bar y^n$ from both sides (notice that if I would have decided to subtract by $\alpha^n$ it wouldn't have worked): $$  \alpha^n - \bar y^n = (\alpha - \bar y)\left(\sum_{0\leq i+j \leq n-1} \bar y^i \alpha^j \right) = (\alpha - \bar y)K < 0$$ where I noticed that $K > 0$ since every element of E is greater than zero and so is its least upper bound $\alpha$. Thus $K > 0$ and getting rid of it gets me: $$ (\alpha - \bar y) < 0 \implies \alpha < \bar y$$ which is obviously false since that would imply that $ \alpha$ is not an upper bound. Thus $\alpha^n < x$ is false. Now assume $x < \alpha^n $. One can't use the same argument as in my previous attempt because this time we are trying to create an element that is an upper bound smaller than $\alpha$ and its not clear how elements form $E$ are useful. Its clear to me we have to choose an $h$ such that: $$ x < (\alpha - h)^n < \alpha^n$$ I have given it a proper attempt (see at the end of my question) but I am unable to prove the desired result no matter how much I play with the algebra and the known facts I have. Thus, my question is how did Rudin come up with the following: $$k = \frac{y^n-x}{n y^{n-1}}$$ from his explanation it seems it just came out of a hat. I am sure if I plugged it in I would see that ""it works"" however, I wanted to know/see how to come up with it myself. Similarly I don't see how/why he came up with this one: $$h < \frac{x - y^n}{n(y+1)^{n-1}}$$ it seems that its not even required considering my first proof/argument, but I assume it must use the same idea considering it seems it used the same identity $b^n - a^n = (b-a)(b^{n-1}+b^{n-2}a+ \cdots + b a^{n-2} + a^{n-1})$. Anyone care to share what is the trick I missed? Is there some way to understand how one would have come up with using that? Is there some conceptual idea for the proof that he did not make explicit that I missed? I am hoping to get a more satisfying proof than just feeling I played around with symbols until I forced the paper to tell me the truth. It seems I missed some insights because even with the hints (like using the identity) didn't yield me a solution. What I tried: If we have: $$ x < \alpha^n$$ then at least intuitively, that must imply that there must be some element $y_{BAD}$ smaller than our supremum $\alpha$ that is still an upper bound (this intuition is because we are going under the assumption that $\alpha^n = x \iff \alpha = x^{1/n}$). Therefore it seems reasonable to try to decrease $\alpha$ the right amount $h$ such that: $$ x < (\alpha - h)^n < \alpha^n $$ then since $h$ is some distance that we go down from $\alpha$ we probably don't need to go down further than $\alpha$ so it seems reasonable to require $0 < h < \alpha$. With that we have using algebra: $$ x < (\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} \right) = (\alpha - h) K < \alpha^n $$ the reason we did that factorization is so that we can hopefully get some inequality for $h$ (intuitively, think that we are trying to make $h$ the subject so that we can choose the right one to get the contradiction we need). Therefore lets try to remove all the nasty exponents with $h$ by assuming $ h < \alpha$ (otherwise $\alpha$ decreases by too much): $$ K = \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1}$$ so lets plug it into: $$ x < (\alpha - h) K < \alpha^n $$ after plugging the inequality and leaving $h$ alone and some algebra I skipped I got: $$ \alpha - \frac{ \alpha^n }{K} < h < \alpha - \frac{x}{n \alpha^{n-1}}$$ unfortunately I don't I didn't manage to plug in $K < n \alpha^{n-1}$ successfully to both sides so I got stuck with the above (which is still in terms of $h^j, j>1$) which still unfortunately has high order terms for $h$...so close it feels... (note I've also tried more things but it would be ridiculous to put it all in here). so I feel got some of the main insights: Require the constraint $x < (\alpha - y)^n < \alpha^n$ using $x<\alpha^n$ and $h>0$. use $(\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1} \right)$ to get $h$ alone. Use the upper bound on $K < n \alpha^{n-1}$ to remove the higher order terms of $h$ that are annoying (since we are assuming we don't know how to take roots). i.e. (\alpha - h)K < (\alpha - h)n \alpha^{n-1} those seem the main ingredients but when I tried putting them together it seemed I was still missing something because playing with the algebra didn't lead to the answer. Someone know what it is?","tl;dr (Too Long To Read): What is the intuition /conceptual idea to why Rudin used the number: $$ k = \frac{y^n-x}{n y^{n-1}} $$ in his proof and not some other number? It seems that that number is not random, so how could he have come up with it? My attempt: I was trying to prove theorem 1.21 from Rudin's real analysis book on my own without looking at Rudin's proof. The way I attempted to prove it was to try to prove what seemed true from my already collected intuition on the real numbers before I started to study real analysis. So I drew lots of picture which lead me to think of defining $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$: Thus, I knew that $E = \{ \bar{y} \in R_{>0} : \bar y^n < x \}$ is just the same as $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$. So it was obvious that what I had to show was that the $\alpha = supE = y = x^{1/n}$ (also the reason for defining sets like that is cuz we probably need to use the Least Upper Bound (LUB) property cuz its one of the few things we are suppose to know about analysis so far). Thus, I proceed to show $E$ is bounded and non-empty so that I was guaranteed that the sup existed since we assumed the Least Upper Bound (LUB) property (a.k.a. the completeness axiom). Then I thought I want $y = \alpha$ so one option was to try showing $y < \alpha$ AND $y > \alpha$ are false, so by trichotomy $y=\alpha$. Since I didn't have direct access to $y$ I decided to take the same strategy except with $y^n = x$ instead of $y$ and $\alpha^n$ instead of $\alpha$. Intuitively I thought well lets assume $\alpha^n < x$ and $ x < \alpha^n $. The first one is too small so hopefully it should lead to some contradiction and perhaps show $\alpha < y$ is false. Similarly the other one $ x < \alpha^n $ should be too large somehow. Then maybe we can use trichotomy to get $\alpha^n = x$ which completes the proof. I attempted the first one $\alpha^n < x$. The only other thing I knew about $\alpha$ was that $\forall \bar y \in E, \bar y^n < x$. Then I decided to combine both equation (since I was hinted to use $a^n - b^n$ cuz I saw that identity in the soln when I check for my solution that E was bounded and non-empty): $$ \alpha^n - \bar y^n < x - \bar y ^n < 0$$ then because of the hint (that I probably wouldn't had realized I needed to use) I was extremely lucky and decided to subtract $\bar y^n$ from both sides (notice that if I would have decided to subtract by $\alpha^n$ it wouldn't have worked): $$  \alpha^n - \bar y^n = (\alpha - \bar y)\left(\sum_{0\leq i+j \leq n-1} \bar y^i \alpha^j \right) = (\alpha - \bar y)K < 0$$ where I noticed that $K > 0$ since every element of E is greater than zero and so is its least upper bound $\alpha$. Thus $K > 0$ and getting rid of it gets me: $$ (\alpha - \bar y) < 0 \implies \alpha < \bar y$$ which is obviously false since that would imply that $ \alpha$ is not an upper bound. Thus $\alpha^n < x$ is false. Now assume $x < \alpha^n $. One can't use the same argument as in my previous attempt because this time we are trying to create an element that is an upper bound smaller than $\alpha$ and its not clear how elements form $E$ are useful. Its clear to me we have to choose an $h$ such that: $$ x < (\alpha - h)^n < \alpha^n$$ I have given it a proper attempt (see at the end of my question) but I am unable to prove the desired result no matter how much I play with the algebra and the known facts I have. Thus, my question is how did Rudin come up with the following: $$k = \frac{y^n-x}{n y^{n-1}}$$ from his explanation it seems it just came out of a hat. I am sure if I plugged it in I would see that ""it works"" however, I wanted to know/see how to come up with it myself. Similarly I don't see how/why he came up with this one: $$h < \frac{x - y^n}{n(y+1)^{n-1}}$$ it seems that its not even required considering my first proof/argument, but I assume it must use the same idea considering it seems it used the same identity $b^n - a^n = (b-a)(b^{n-1}+b^{n-2}a+ \cdots + b a^{n-2} + a^{n-1})$. Anyone care to share what is the trick I missed? Is there some way to understand how one would have come up with using that? Is there some conceptual idea for the proof that he did not make explicit that I missed? I am hoping to get a more satisfying proof than just feeling I played around with symbols until I forced the paper to tell me the truth. It seems I missed some insights because even with the hints (like using the identity) didn't yield me a solution. What I tried: If we have: $$ x < \alpha^n$$ then at least intuitively, that must imply that there must be some element $y_{BAD}$ smaller than our supremum $\alpha$ that is still an upper bound (this intuition is because we are going under the assumption that $\alpha^n = x \iff \alpha = x^{1/n}$). Therefore it seems reasonable to try to decrease $\alpha$ the right amount $h$ such that: $$ x < (\alpha - h)^n < \alpha^n $$ then since $h$ is some distance that we go down from $\alpha$ we probably don't need to go down further than $\alpha$ so it seems reasonable to require $0 < h < \alpha$. With that we have using algebra: $$ x < (\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} \right) = (\alpha - h) K < \alpha^n $$ the reason we did that factorization is so that we can hopefully get some inequality for $h$ (intuitively, think that we are trying to make $h$ the subject so that we can choose the right one to get the contradiction we need). Therefore lets try to remove all the nasty exponents with $h$ by assuming $ h < \alpha$ (otherwise $\alpha$ decreases by too much): $$ K = \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1}$$ so lets plug it into: $$ x < (\alpha - h) K < \alpha^n $$ after plugging the inequality and leaving $h$ alone and some algebra I skipped I got: $$ \alpha - \frac{ \alpha^n }{K} < h < \alpha - \frac{x}{n \alpha^{n-1}}$$ unfortunately I don't I didn't manage to plug in $K < n \alpha^{n-1}$ successfully to both sides so I got stuck with the above (which is still in terms of $h^j, j>1$) which still unfortunately has high order terms for $h$...so close it feels... (note I've also tried more things but it would be ridiculous to put it all in here). so I feel got some of the main insights: Require the constraint $x < (\alpha - y)^n < \alpha^n$ using $x<\alpha^n$ and $h>0$. use $(\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1} \right)$ to get $h$ alone. Use the upper bound on $K < n \alpha^{n-1}$ to remove the higher order terms of $h$ that are annoying (since we are assuming we don't know how to take roots). i.e. (\alpha - h)K < (\alpha - h)n \alpha^{n-1} those seem the main ingredients but when I tried putting them together it seemed I was still missing something because playing with the algebra didn't lead to the answer. Someone know what it is?",,"['real-analysis', 'proof-verification', 'proof-writing', 'proof-explanation', 'alternative-proof']"
76,Union of Balls around Rationals,Union of Balls around Rationals,,"Let $(r_n)_{n \ge 1}$ be an enumeration of the rationals. Consider the union $A := \cup_n (r_n-\frac{1}{n^2},r_n+\frac{1}{n^2})$ . It is unclear a-priori whether $A$ covers the real line, since although the rationals are dense in the reals, the $\frac{1}{n^2}$ 's might shrink too fast. However, using measure theory, it is very easy to see $A$ does not cover much: indeed, $m(A) \le \sum_n m((r_n-\frac{1}{n^2},r_n+\frac{1}{n^2})) = \sum_n \frac{2}{n^2} = \frac{\pi^2}{3}$ . Since this argument relies much on the convergence of $\sum_n \frac{1}{n^2}$ , I am wondering whether $B := \cup_n (r_n-\frac{1}{n},r_n+\frac{1}{n})$ covers the whole real line, or what portion of it? Does the amount covered depend on the enumeration we choose?","Let be an enumeration of the rationals. Consider the union . It is unclear a-priori whether covers the real line, since although the rationals are dense in the reals, the 's might shrink too fast. However, using measure theory, it is very easy to see does not cover much: indeed, . Since this argument relies much on the convergence of , I am wondering whether covers the whole real line, or what portion of it? Does the amount covered depend on the enumeration we choose?","(r_n)_{n \ge 1} A := \cup_n (r_n-\frac{1}{n^2},r_n+\frac{1}{n^2}) A \frac{1}{n^2} A m(A) \le \sum_n m((r_n-\frac{1}{n^2},r_n+\frac{1}{n^2})) = \sum_n \frac{2}{n^2} = \frac{\pi^2}{3} \sum_n \frac{1}{n^2} B := \cup_n (r_n-\frac{1}{n},r_n+\frac{1}{n})","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
77,axiomatic definition of trigonometric functions,axiomatic definition of trigonometric functions,,"A friend told me that in addition to the axioms for the real numbers, it can be proved (without appeal to sine and cosine) that a function exists satisfying the following conditions: $C(a-b)=C(a)C(b)+S(a)S(b)$ $ S(x) \geq 0 ,\forall x \in [0,p]$ $ S(p)=1$ This would allow an alternative definition of sine, cosine and even $\pi$, without using geometry, calculus or non-elementary arguments. See Timothy Gowers blogpost for a discussion of how difficult it can be to define sine. Now, using the conditions as 'axioms', I managed to show that: $C(x)$ and $S(x)$ were both periodic with period $4p$ $C^2(x)+S^2(x)=1$ $C(x+p)=-S(x)$ $S(x+p)=C(x)$ And, I found that if I defined $ \alpha_n= S(\frac{p}{2^n})$ and $\epsilon := \frac{p}{2^n}$, then I could show that $ S(x)$ could be defined as a function for countably infinite points $B = \{k \in \mathbb{Z},n \in \mathbb{N}:n\epsilon+kp\} \subset \mathbb{R}$, and simultaneously show that $\alpha_n$ was strictly decreasing. However, after this point I got stuck. I didn't manage to show the existence and uniqueness of $ S(x), \forall x \in \mathbb{R}_+\setminus B$. Can this be done without using geometry? Note: The fact that $S$ is a function is something to be proven. Writing $S(x)$ assumes functionness. So we should really be careful that we don't give circular arguments.","A friend told me that in addition to the axioms for the real numbers, it can be proved (without appeal to sine and cosine) that a function exists satisfying the following conditions: $C(a-b)=C(a)C(b)+S(a)S(b)$ $ S(x) \geq 0 ,\forall x \in [0,p]$ $ S(p)=1$ This would allow an alternative definition of sine, cosine and even $\pi$, without using geometry, calculus or non-elementary arguments. See Timothy Gowers blogpost for a discussion of how difficult it can be to define sine. Now, using the conditions as 'axioms', I managed to show that: $C(x)$ and $S(x)$ were both periodic with period $4p$ $C^2(x)+S^2(x)=1$ $C(x+p)=-S(x)$ $S(x+p)=C(x)$ And, I found that if I defined $ \alpha_n= S(\frac{p}{2^n})$ and $\epsilon := \frac{p}{2^n}$, then I could show that $ S(x)$ could be defined as a function for countably infinite points $B = \{k \in \mathbb{Z},n \in \mathbb{N}:n\epsilon+kp\} \subset \mathbb{R}$, and simultaneously show that $\alpha_n$ was strictly decreasing. However, after this point I got stuck. I didn't manage to show the existence and uniqueness of $ S(x), \forall x \in \mathbb{R}_+\setminus B$. Can this be done without using geometry? Note: The fact that $S$ is a function is something to be proven. Writing $S(x)$ assumes functionness. So we should really be careful that we don't give circular arguments.",,['real-analysis']
78,Examples 3.35 (a) and (b) in Baby Rudin: Limit Superior and limit inferior of a couple of sequences,Examples 3.35 (a) and (b) in Baby Rudin: Limit Superior and limit inferior of a couple of sequences,,"This question is related to Examples 3.35 (a) and (b) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition, p. 67. Let us consider the series  $$ \frac 1 2 + \frac 1 3 + \frac{1}{2^2} + \frac{1}{3^2} + \frac{1}{2^3} + \frac{1}{3^3} + \frac{1}{2^4} + \frac{1}{3^4} + \cdots$$  for which the formula for the general term $a_n$ is given by  $$a_n = \begin{cases} \frac{1}{2^k} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{3^k} \ \mbox{ if } \ n = 2k \end{cases} $$  for $k = 1, 2, 3, \ldots$, and the series  $$\frac 1 2 + 1 + \frac 1 8 + \frac 1 4 + \frac{1}{32} + \frac{1}{16} + \cdots$$  for which the formula for the general term is given by  $$b_n =  \begin{cases} \frac{1}{2^n} = \frac{1}{2^{2k-1}} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{2^{n-2}} = \frac{1}{2^{2k-2}} \ \mbox{ if } \ n = 2k \end{cases} $$ for $k = 1, 2, 3, \ldots$. Now Rudin states that  $$\liminf_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \left( \frac 2 3 \right)^n = 0,$$ $$\liminf_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[2n]{\frac{1}{3^n}} = \frac{1}{\sqrt{3}},$$ $$\limsup_{n\to\infty} \sqrt[n]{a_n} = \sqrt[2n]{\frac{1}{2^n}} = \frac{1}{\sqrt{2}},$$ $$\limsup_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \frac 1 2 \left( \frac 3 2 \right)^n = +\infty.$$ And, Rudin also states that  $$ \liminf_{n\to\infty} \frac{b_{n+1}}{b_n} = \frac 1 8,$$ $$ \limsup_{n\to\infty} \frac{b_{n+1}}{b_n} = 2,$$  $$\lim_{n\to\infty} \sqrt[n]{b_n} = \frac 1 2.$$ How to rigorously verify these statements using machinery (i.e. the definitions and theorems ) developed by Rudin up to this point? I know that $\liminf$ and $\limsup$ are the infimum and supremum, resp., of the set of all the subsequential limits (in the extended real number system) of a sequence, and there is a subsequence each converging to $\liminf$ and $\limsup$. Moreover, for each $k \in \mathbb{N}$, we have  $$\frac{a_{2k} }{a_{2k-1}} = \frac{ \frac{1}{3^k} }{ \frac{1}{2^k} } = \left( \frac 2 3 \right)^k \to 0 \ \mbox{ as } \ k \to \infty,$$ $$\frac{ a_{2k+1} }{ a_{2k} } = \frac{ \frac{1}{2^{k+1}}}{ \frac{1}{3^k} } =\frac 1 2 \left( \frac 3 2 \right)^k   \to +\infty \ \mbox{ as } \ k \to \infty,$$ $$\sqrt[2k]{a_{2k}} = \sqrt[2k]{ \frac{1}{3^k}} = \sqrt{\frac 1 3} \to \sqrt{\frac 1 3} \ \mbox{ as } \ k \to \infty,$$  $$\sqrt[2k-1]{a_{2k-1}} = \sqrt[2k-1]{ \frac{1}{2^k}} = \frac{1}{2^{\frac{k}{2k-1}}} \to ? \ \mbox{ as } \ k \to \infty.$$ How to find $\lim_{k \to \infty} \sqrt[2k-1]{2^k} $ using what Rudin has established (in Theorem 3.20)? Also, $$\frac{ b_{2k} }{ b_{2k-1} } = \frac{ \frac{1}{ 2^{2k-2} } }{ \frac{1}{ 2^{2k-1} } } = 2 \to 2 \ \mbox{ as } \ k \to \infty,$$ $$\frac{ b_{2k+1} }{ b_{2k} } = \frac{ \frac{1}{ 2^{2k+1} } }{ \frac{1}{ 2^{2k-2} } } = \frac 1 8 \to \frac 1 8  \ \mbox{ as } \ k \to \infty,$$ $$\sqrt[2k]{b_{2k}} = \sqrt[2k]{\frac{1}{2^{2k-2}}} = \frac{ \sqrt[2k]{4} }{ 2 } \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$  $$\sqrt[2k-1]{b_{2k-1}} = \sqrt[2k-1]{\frac{1}{2^{2k-1}}} = \frac 1 2 \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$","This question is related to Examples 3.35 (a) and (b) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition, p. 67. Let us consider the series  $$ \frac 1 2 + \frac 1 3 + \frac{1}{2^2} + \frac{1}{3^2} + \frac{1}{2^3} + \frac{1}{3^3} + \frac{1}{2^4} + \frac{1}{3^4} + \cdots$$  for which the formula for the general term $a_n$ is given by  $$a_n = \begin{cases} \frac{1}{2^k} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{3^k} \ \mbox{ if } \ n = 2k \end{cases} $$  for $k = 1, 2, 3, \ldots$, and the series  $$\frac 1 2 + 1 + \frac 1 8 + \frac 1 4 + \frac{1}{32} + \frac{1}{16} + \cdots$$  for which the formula for the general term is given by  $$b_n =  \begin{cases} \frac{1}{2^n} = \frac{1}{2^{2k-1}} \ \mbox{ if } \ n = 2k-1 \\ \frac{1}{2^{n-2}} = \frac{1}{2^{2k-2}} \ \mbox{ if } \ n = 2k \end{cases} $$ for $k = 1, 2, 3, \ldots$. Now Rudin states that  $$\liminf_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \left( \frac 2 3 \right)^n = 0,$$ $$\liminf_{n\to\infty} \sqrt[n]{a_n} = \lim_{n\to\infty} \sqrt[2n]{\frac{1}{3^n}} = \frac{1}{\sqrt{3}},$$ $$\limsup_{n\to\infty} \sqrt[n]{a_n} = \sqrt[2n]{\frac{1}{2^n}} = \frac{1}{\sqrt{2}},$$ $$\limsup_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim_{n\to\infty} \frac 1 2 \left( \frac 3 2 \right)^n = +\infty.$$ And, Rudin also states that  $$ \liminf_{n\to\infty} \frac{b_{n+1}}{b_n} = \frac 1 8,$$ $$ \limsup_{n\to\infty} \frac{b_{n+1}}{b_n} = 2,$$  $$\lim_{n\to\infty} \sqrt[n]{b_n} = \frac 1 2.$$ How to rigorously verify these statements using machinery (i.e. the definitions and theorems ) developed by Rudin up to this point? I know that $\liminf$ and $\limsup$ are the infimum and supremum, resp., of the set of all the subsequential limits (in the extended real number system) of a sequence, and there is a subsequence each converging to $\liminf$ and $\limsup$. Moreover, for each $k \in \mathbb{N}$, we have  $$\frac{a_{2k} }{a_{2k-1}} = \frac{ \frac{1}{3^k} }{ \frac{1}{2^k} } = \left( \frac 2 3 \right)^k \to 0 \ \mbox{ as } \ k \to \infty,$$ $$\frac{ a_{2k+1} }{ a_{2k} } = \frac{ \frac{1}{2^{k+1}}}{ \frac{1}{3^k} } =\frac 1 2 \left( \frac 3 2 \right)^k   \to +\infty \ \mbox{ as } \ k \to \infty,$$ $$\sqrt[2k]{a_{2k}} = \sqrt[2k]{ \frac{1}{3^k}} = \sqrt{\frac 1 3} \to \sqrt{\frac 1 3} \ \mbox{ as } \ k \to \infty,$$  $$\sqrt[2k-1]{a_{2k-1}} = \sqrt[2k-1]{ \frac{1}{2^k}} = \frac{1}{2^{\frac{k}{2k-1}}} \to ? \ \mbox{ as } \ k \to \infty.$$ How to find $\lim_{k \to \infty} \sqrt[2k-1]{2^k} $ using what Rudin has established (in Theorem 3.20)? Also, $$\frac{ b_{2k} }{ b_{2k-1} } = \frac{ \frac{1}{ 2^{2k-2} } }{ \frac{1}{ 2^{2k-1} } } = 2 \to 2 \ \mbox{ as } \ k \to \infty,$$ $$\frac{ b_{2k+1} }{ b_{2k} } = \frac{ \frac{1}{ 2^{2k+1} } }{ \frac{1}{ 2^{2k-2} } } = \frac 1 8 \to \frac 1 8  \ \mbox{ as } \ k \to \infty,$$ $$\sqrt[2k]{b_{2k}} = \sqrt[2k]{\frac{1}{2^{2k-2}}} = \frac{ \sqrt[2k]{4} }{ 2 } \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$  $$\sqrt[2k-1]{b_{2k-1}} = \sqrt[2k-1]{\frac{1}{2^{2k-1}}} = \frac 1 2 \to \frac 1 2 \ \mbox{ as } \ k \to \infty,$$",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'limsup-and-liminf']"
79,average of maximal function is less than its infimum?,average of maximal function is less than its infimum?,,"Let M be the dyadic Hardy-Littlewood maximal operator. Prove the following: there is a constant $C$ such that for any $f$, $$  \inf_{x\in I}Mf(x)\le C 2^k\inf_{x\in J} Mf(x)  $$ where $I$ and $J$ are dyadic intervals with $I\subset J$ and $2^k|I|=|J|$ for some positive integer $k$ (i.e. $I$ is the $k$-th generation of $J$). I noticed that this theorem follows from $$\frac{1}{|J|}\int_JMf\le C\inf_{x\in J}Mf(x). $$ This is an interesting phenomenon, but I am unable to prove it.","Let M be the dyadic Hardy-Littlewood maximal operator. Prove the following: there is a constant $C$ such that for any $f$, $$  \inf_{x\in I}Mf(x)\le C 2^k\inf_{x\in J} Mf(x)  $$ where $I$ and $J$ are dyadic intervals with $I\subset J$ and $2^k|I|=|J|$ for some positive integer $k$ (i.e. $I$ is the $k$-th generation of $J$). I noticed that this theorem follows from $$\frac{1}{|J|}\int_JMf\le C\inf_{x\in J}Mf(x). $$ This is an interesting phenomenon, but I am unable to prove it.",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'harmonic-analysis']"
80,"If $f:[a,b]\to \mathbb{R}$ satisfies $|f'(x)|<1, \forall x\in [a,b]$, is $f$ necessarily a contraction?","If  satisfies , is  necessarily a contraction?","f:[a,b]\to \mathbb{R} |f'(x)|<1, \forall x\in [a,b] f","If $f:[a,b]\to \mathbb{R}$, $f'(x)$ exists for all $x\in [a,b]$ (derivatives at endpoints $a,b$ are one-sided) and satisfies $|f'(x)|<1, \forall x\in [a,b]$, is $f$ necessarily a contraction (i.e. $|f(x)-f(y)|\leq c|x-y|$, for some $0<c<1$)? I've tried to prove it by contradiction. Define $E=\left\{\dfrac{|f(x)-f(y)|}{|x-y|}: x\neq y\in [a,b]\right\}\neq \emptyset$. By mean value theorem, $|f(x)-f(y)|<|x-y|,\forall x\neq y\in [a,b]$. Therefore, $E$ has an upper bound $1$, hence the least upper bound $s=\sup E\leq 1$. Suppose that $s=1$, take $\epsilon_n=\dfrac{1}{n}$, we can find $a_n=\dfrac{|f(x_n)-f(y_n)|}{|x_n-y_n|}\in E$ such that $1-\dfrac{1}{n}<a_n<1$, thus a sequence $\{a_n\}$ with limit $1$. For the two sequences $\{x_n\},\{y_n\}$, with $x_n<y_n$. According to Bolzano-Weierstrass Theorem, there exist subsequences $\{x_{n_k}\},\{y_{n_k}\}$ converging to $x_0,y_0$,respectively. If $x_0\neq y_0$，then since $a_{n_k}$ converges to $1$, we can obtain $\dfrac{|f(x_0)-f(y_0)|}{|x_0-y_0|}=1$, which is a contradiction. The proof gets stuck at the case $x_0=y_0$. Since the following proposition may fail to hold if $x_n<x_0=y_0<y_n$ doesn't hold. If $x_n<x_0<y_n$, both $\{x_n\},\{y_n\}$ converge to $x_0$ and $f'(x_0)$ exists, then $\lim\limits_{n\to\infty}\dfrac{f(x_n)-f(y_n)}{x_n-y_n}=f'(x_0)$. And now I don't know whether the original proposition holds. If we add the condition that the derivative $f'$ is continuous on $[a,b]$, then by the maximum value theorem, $f$ is surely a contraction. So, if there is any counterexample, then $f'$ must be discontinuous. Since some text requires a contraction maps a space into itself, how about adding this as a condition, i.e. consider $f:[a,b]\to [a,b]$?","If $f:[a,b]\to \mathbb{R}$, $f'(x)$ exists for all $x\in [a,b]$ (derivatives at endpoints $a,b$ are one-sided) and satisfies $|f'(x)|<1, \forall x\in [a,b]$, is $f$ necessarily a contraction (i.e. $|f(x)-f(y)|\leq c|x-y|$, for some $0<c<1$)? I've tried to prove it by contradiction. Define $E=\left\{\dfrac{|f(x)-f(y)|}{|x-y|}: x\neq y\in [a,b]\right\}\neq \emptyset$. By mean value theorem, $|f(x)-f(y)|<|x-y|,\forall x\neq y\in [a,b]$. Therefore, $E$ has an upper bound $1$, hence the least upper bound $s=\sup E\leq 1$. Suppose that $s=1$, take $\epsilon_n=\dfrac{1}{n}$, we can find $a_n=\dfrac{|f(x_n)-f(y_n)|}{|x_n-y_n|}\in E$ such that $1-\dfrac{1}{n}<a_n<1$, thus a sequence $\{a_n\}$ with limit $1$. For the two sequences $\{x_n\},\{y_n\}$, with $x_n<y_n$. According to Bolzano-Weierstrass Theorem, there exist subsequences $\{x_{n_k}\},\{y_{n_k}\}$ converging to $x_0,y_0$,respectively. If $x_0\neq y_0$，then since $a_{n_k}$ converges to $1$, we can obtain $\dfrac{|f(x_0)-f(y_0)|}{|x_0-y_0|}=1$, which is a contradiction. The proof gets stuck at the case $x_0=y_0$. Since the following proposition may fail to hold if $x_n<x_0=y_0<y_n$ doesn't hold. If $x_n<x_0<y_n$, both $\{x_n\},\{y_n\}$ converge to $x_0$ and $f'(x_0)$ exists, then $\lim\limits_{n\to\infty}\dfrac{f(x_n)-f(y_n)}{x_n-y_n}=f'(x_0)$. And now I don't know whether the original proposition holds. If we add the condition that the derivative $f'$ is continuous on $[a,b]$, then by the maximum value theorem, $f$ is surely a contraction. So, if there is any counterexample, then $f'$ must be discontinuous. Since some text requires a contraction maps a space into itself, how about adding this as a condition, i.e. consider $f:[a,b]\to [a,b]$?",,"['real-analysis', 'examples-counterexamples']"
81,Finding an integer $n$ such that $\sin(n)$ is close to 1,Finding an integer  such that  is close to 1,n \sin(n),"Given some $\epsilon>0$, is there an efficient way to find an integer $n$ such that  $$1-\sin(n)<\epsilon$$ We all know there is always one (and many), and so I can test all $n$ from $0$ until I find a good candidate, but I ask for some efficient algorithm that given some $\epsilon$, computes quickly such an $n$.","Given some $\epsilon>0$, is there an efficient way to find an integer $n$ such that  $$1-\sin(n)<\epsilon$$ We all know there is always one (and many), and so I can test all $n$ from $0$ until I find a good candidate, but I ask for some efficient algorithm that given some $\epsilon$, computes quickly such an $n$.",,['real-analysis']
82,Show that $f(x) = x\cos^3(x)$ is not uniformly continuous on $\mathbb{R}$,Show that  is not uniformly continuous on,f(x) = x\cos^3(x) \mathbb{R},"Show that $f(x) = x\cos^3(x)$ is not uniformly continuous on $\mathbb{R}$. I tried $x_n = \pi/2 + n\pi$ and $y_n = \pi/2 + n\pi + 1/(n\pi)$. Since $\cos^3(x) = \frac14 (\cos(3x)+3\cos x)$, $f(x_n) = 0$.  So,  $$\left|\frac34(\pi/2 + n\pi + 1/(n\pi))\cos(\pi/2 + n\pi + 1/(n\pi)) + \frac14(\pi/2 + n\pi + 1/(n\pi))\cos(3\pi/2 + n\pi + 1/(n\pi))\right| > \frac12|(\pi/2 + n\pi + 1/(n\pi))\sin(1/(n\pi))| > \frac12 n \pi\sin(1/(n\pi)) \to \frac12$$ so it is not uniformly continuous. Is this correct?","Show that $f(x) = x\cos^3(x)$ is not uniformly continuous on $\mathbb{R}$. I tried $x_n = \pi/2 + n\pi$ and $y_n = \pi/2 + n\pi + 1/(n\pi)$. Since $\cos^3(x) = \frac14 (\cos(3x)+3\cos x)$, $f(x_n) = 0$.  So,  $$\left|\frac34(\pi/2 + n\pi + 1/(n\pi))\cos(\pi/2 + n\pi + 1/(n\pi)) + \frac14(\pi/2 + n\pi + 1/(n\pi))\cos(3\pi/2 + n\pi + 1/(n\pi))\right| > \frac12|(\pi/2 + n\pi + 1/(n\pi))\sin(1/(n\pi))| > \frac12 n \pi\sin(1/(n\pi)) \to \frac12$$ so it is not uniformly continuous. Is this correct?",,"['real-analysis', 'continuity', 'solution-verification', 'uniform-continuity']"
83,How to derive this interesting identity for $\log(\sin(x))$ [duplicate],How to derive this interesting identity for  [duplicate],\log(\sin(x)),"This question already has answers here : Fourier series of Log sine and Log cos (2 answers) Closed 9 years ago . I saw on SE that: $$\log(\sin x)=-\log(2)-\sum_{n=1}^{\infty}\frac{\cos(2nx)}{n} \phantom{a} (0<x<\pi)$$ This is an extremely useful identity, as it helps solve: $$\int_{0}^{\pi} \log(\sin(x)) dx$$ But how is it derived? From Taylor series, power series? How do I get this? Even if someone can start me off that would be great.","This question already has answers here : Fourier series of Log sine and Log cos (2 answers) Closed 9 years ago . I saw on SE that: $$\log(\sin x)=-\log(2)-\sum_{n=1}^{\infty}\frac{\cos(2nx)}{n} \phantom{a} (0<x<\pi)$$ This is an extremely useful identity, as it helps solve: $$\int_{0}^{\pi} \log(\sin(x)) dx$$ But how is it derived? From Taylor series, power series? How do I get this? Even if someone can start me off that would be great.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'power-series']"
84,Maxima of the function $\left \vert \int_{-1}^1 e^{i(ax+bx^2)}dx \right \vert$,Maxima of the function,\left \vert \int_{-1}^1 e^{i(ax+bx^2)}dx \right \vert,"I am looking for extrema of the function $$g(a,b):=\left \vert \int_{-1}^1 e^{i(ax+bx^2)}dx \right \vert$$ where $a,b >0$ are real parameters. I already plotted this function and got the impression that there are periodically local extrema along some particular curve, but it was hard to be more precise. My question is: Can we say anything about extrema from the function itself analytically? In particular, I am interested in the asymptotic distribution of the maxima (if we cannot catch them analytically) for $a,b>0$ that you can see in the plot above. Apparently, they start with ""more or less"" $a = b$ , but how are they distributed for large $a,b$ ? They also seem to have some periodicity, can we say anything about this? You might also want to look at the new representations given in the comments :-) If anything is unclear about this, please let me know. NOTICE: Anything( also periodicity etc.) you could say about the extrema is probably useful, so I deliberately want this question to be somewhat broad.","I am looking for extrema of the function where are real parameters. I already plotted this function and got the impression that there are periodically local extrema along some particular curve, but it was hard to be more precise. My question is: Can we say anything about extrema from the function itself analytically? In particular, I am interested in the asymptotic distribution of the maxima (if we cannot catch them analytically) for that you can see in the plot above. Apparently, they start with ""more or less"" , but how are they distributed for large ? They also seem to have some periodicity, can we say anything about this? You might also want to look at the new representations given in the comments :-) If anything is unclear about this, please let me know. NOTICE: Anything( also periodicity etc.) you could say about the extrema is probably useful, so I deliberately want this question to be somewhat broad.","g(a,b):=\left \vert \int_{-1}^1 e^{i(ax+bx^2)}dx \right \vert a,b >0 a,b>0 a = b a,b","['calculus', 'real-analysis']"
85,"The integral on $[0,1]\times[0,1]$",The integral on,"[0,1]\times[0,1]","Here I have a problem. $p$ and $q$ are positive numbers. the integral $$\int_0^1\int_0^1 \frac{1}{x^p+y^q}\;dx\;dy< \infty \Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1$$ Here is my try, maybe we can change variable. When $p>1:$ $$\int_0^1\int_0^1 \frac{1}{x^p+y^q}dxdy=\int_0^1\int_0^1 \frac{1}{(\frac{x}{y^{\frac{q}{p}}})^p+1}\cdot \frac{1}{y^q}dxdy=\int_0^1\int_0^{y^{-\frac{q}{p}}} \frac{y^{\frac{q}{p}-q}}{t^p+1}dtdy$$ So when $p>1,$ integrable$\Longleftrightarrow \frac{q}{p}-q>-1\Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1 $ When $0<p \leq 1, \ \text{we have}\frac{1}{x^p}\geq \frac{1}{x^p+y^q}\geq \frac{1}{(x+y^\frac{q}{p})^p}$ So integrable$\Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1$","Here I have a problem. $p$ and $q$ are positive numbers. the integral $$\int_0^1\int_0^1 \frac{1}{x^p+y^q}\;dx\;dy< \infty \Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1$$ Here is my try, maybe we can change variable. When $p>1:$ $$\int_0^1\int_0^1 \frac{1}{x^p+y^q}dxdy=\int_0^1\int_0^1 \frac{1}{(\frac{x}{y^{\frac{q}{p}}})^p+1}\cdot \frac{1}{y^q}dxdy=\int_0^1\int_0^{y^{-\frac{q}{p}}} \frac{y^{\frac{q}{p}-q}}{t^p+1}dtdy$$ So when $p>1,$ integrable$\Longleftrightarrow \frac{q}{p}-q>-1\Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1 $ When $0<p \leq 1, \ \text{we have}\frac{1}{x^p}\geq \frac{1}{x^p+y^q}\geq \frac{1}{(x+y^\frac{q}{p})^p}$ So integrable$\Longleftrightarrow \frac{1}{p}+\frac{1}{q}>1$",,"['real-analysis', 'integration', 'convergence-divergence']"
86,Additive functional inequality,Additive functional inequality,,"The function $f:R_+\to R_+$ is continuously differentiable and  increasing. Also, $f(0)=0$ and $f(\infty)=\infty$.   Continuity and  differentiability of higher orders can be assumed if necessary.   The  proposition on hand is the following: If for all integers $t>0$ and     for all  $x> 0$, $f((t+1)x)<f(tx)+f(x)$, then for all     integers $m,n>0$, $f(mx)+f(nx)\geq f(mx+nx)$. Does there exist a proof or a counter example?","The function $f:R_+\to R_+$ is continuously differentiable and  increasing. Also, $f(0)=0$ and $f(\infty)=\infty$.   Continuity and  differentiability of higher orders can be assumed if necessary.   The  proposition on hand is the following: If for all integers $t>0$ and     for all  $x> 0$, $f((t+1)x)<f(tx)+f(x)$, then for all     integers $m,n>0$, $f(mx)+f(nx)\geq f(mx+nx)$. Does there exist a proof or a counter example?",,"['real-analysis', 'functions', 'puzzle', 'functional-equations']"
87,An Increasing Function Discontinuous at All Rational Numbers,An Increasing Function Discontinuous at All Rational Numbers,,"Let $q: \mathbb{N}\rightarrow \mathbb{Q}$ be a bijective map and let $g: \mathbb{Q}\rightarrow \mathbb{R}$ define $g(q(n))=2^{-n}$. Show that  $\sum_{r\in \mathbb{Q}}g(r)$ is absolutely convergent. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be define by the formula $f(x):=\sum_{r\in \mathbb{Q}:r<x}g(r)$, show that. (1) $f$ is well-defined. (2) $f$ is strictly monotonic increasing. (3) $f$ is discontinuous at every rational number. (4) $f$ is continuous at any irrational number $x$ (hint: first demonstrate that the functions $f_n(x):=\sum_{r\in \mathbb{Q}:r<x,\,g(r)\ge 2^{-n}}g(r)$ are continuous at $x$, and that $|f(x)-f_n(x)|\le 2^{-n}$). Sketch-proof First we have to show that $\sum_{r\in \mathbb{Q}}g(r)$ converges absolutely (I know that is obvious but I'd like to do a more elaborated argument of this fact). It suffice to show that for any finite subset of $\mathbb{Q}$ say $F$, the series $\sum _{r\in F}|g(r)|$ is bounded. Let $F\subset \mathbb{Q}$ and $\#F< \infty$. Then we have that the sequence $(q^{-1}(x))_{x\in F}$ is finite and therefore bounded by some $M\in \mathbb{N}$. Setting $\{q(m):m\in \mathbb{N} \text{ and } m\le M\}$ we know that it contains $F$. Thus \begin{align} \sum _{r\in F}|g(r)|\le \sum _{\{q(m):m\in \mathbb{N},\,  m\le M\}}|g(r)| = \sum _{m=0}^M|g(\,q(m)\,)|\\ =\sum _{m=0}^M 2^{-m} \le \sum _{m=0}^\infty 2^{-m}<\infty \end{align} Now since $F$ was an arbitrary subset of $\mathbb{Q}$, for any finite subset the upper bound works in other words we have $$ \sup \bigg\{\sum _{x\in F}|f(x)|:F\subset X; \#F<\infty \bigg\}<\infty $$ which shows that the series converges absolutely as desired. (1) $f$ is well-defined. we have to show that $f(x)$ is defined for every $x\in \mathbb{R}$. Let $x\in \mathbb{R}$, then we have $$\sum_{r\in \mathbb{Q}:r<x}g(r)\le \sum_{r\in \mathbb{Q}}g(r)$$ Since $g(r)$ is always non-negative for any rational number and since the latter is  convergent then the former must converges also. Hence $f(x)$ always exists. (2) $f$ is strictly monotonic increasing. Let $x,y\in \mathbb{R}$ such that $x<y$. Then there exists some rational number in between, let call it $s$, i.e., $x<s<y$. Since $s$ is rational then there exists some $n\in \mathbb{N}$ for which $q(n)=s$. So $$f(y)=\sum_{r\in \mathbb{Q}:r<y}g(r)=\sum_{r\in \mathbb{Q}:r<x}g(r) +\sum_{r\in X}g(r)$$ where $X = \{r\in \mathbb{Q}:r<y \} \backslash \{ r\in \mathbb{Q}:r<x\}$, clearly $s\in X$ so is a non-empty set, and since $g(r)$ is always positive for any rational. Then $$\sum_{r\in X}g(r)\ge g(s)=g(q(n))= 2^{-n}$$ Thus putting this together we can conclude that $f(y) > f(x)$, as desired. (3) $f$ is discontinuous to every rational number. Let $s$ be an arbitrary rational number, then there exists a $n\in \mathbb{N}$ so that $q(n)=s$. Now for what we have shown in (2), for any real number $x$ such that $x>s$, we have $f(x)\ge f(s)+2^{-n}$. We argue by contradiction, suppose that $f$ is continuous at $s$. Let $\varepsilon$ be a positive real such that $\varepsilon < 2^{-n}$. Since $f$ is continuous then there is some $\delta >0$ such that $|f(y)-f(s)|<\varepsilon$ whenever $y$ is a real number which is $\delta$- close to $s$, i.e., $|y-s|<\delta$. But there for any $x\in (s,s +\delta)$  we have $f(x)-f(s)\ge 2^{-n}>\varepsilon$. Contradiction. Hence $f$ cannot be continuous at any rational number as claimed. (4) $f$ is continuous at any irrational number $x$. Let $x$ be an irrational number and let define the functions $f_n: \mathbb{R} \rightarrow \mathbb{R}$ by setting  $$f_n(x):=\sum_{r\in \mathbb{Q}:r<x,\,g(r)\ge 2^{-n}}g(r)$$ Let define $Q_n= \{r\in \mathbb{Q}: g(r)\ge 2^{-n}\}$. We claim that $Q_n$ is finite. If $r \in Q_n$ so $g(r)\ge 2^{-n}$ and since $r\in \mathbb{Q}$, there is some $m$ such that $q(m)=r$. Thus $g(r)=g(q(m))= 2^{-m}\ge 2^{-n}$, which means that $m\le n$, in other words $Q_n= \{q(i): 0\le i \le n \}$, which clearly contains a finite number of rational numbers. We shall show that  $f_n$ is continuous at $x$. Given $\varepsilon>0$ and $n\in \mathbb{N}$. We set $\delta = \min \{\, |r-x|: r\in Q_n  \,\}$, $\delta$ is well-defined since $Q_n$ is non-empty (at least contain $q(0)$) and all  $|r-x|$ are different to zero since $x$ is irrational, then $\delta>0$. Let $\{z\in \mathbb{R}: |z-x|<\delta \}$, we claim that $|f_n(z)-f_n(x)|< \varepsilon$. $$|f_n(z)-f_n(x)| = \Bigg|\sum_{r\in \mathbb{Q}:r<z,\,r\in Q_n}g(r)\, -\sum_{r\in \mathbb{Q}:r<x,\,r\in Q_n}g(r) \Bigg|$$ If we can show that $\{r\in \mathbb{Q}:r<z,\,r\in Q_n\}=\{r\in \mathbb{Q}:r<x,\,r\in Q_n\}$, then both series are the same and the difference is zero. $LHS\Rightarrow RHS$ By contradiction suppose that there is some element $r$ in $LHS$ that is not in the $RHS$. So $r$ is rational, $r<z$ and $r\in Q_n$. Since $r$ is not in the $RHS$ then we would have $r\ge x$. Clearly the equality is not possible since $x$ is irrational. In the case $r>x$, we have $x<r<z$. So, $z-x>r-x$, but $z-x< \delta \le r-x$ since $r\in Q_n$, contradiction. $RHS\Rightarrow LHS$ Again for contradiction suppose there is some $r$ in $RHS$ that is not in the $LHS$. So this is only possible if $r\ge z$. If $z=r$, we have $|z-x|<\delta \le |r-x|=|z-x|$, a contradiction. The case when $r>z$ is similar to the last case in $LHS\Rightarrow RHS$, just with the roles of $z$ and $x$ interchanged. Thus both sets are equal and then the series are the same. Hence its difference is zero  and $f_n$ is continuous at $x$ as claimed. Claim: $|f(x)-f_n(x)|\le 2^{-n}$. Since $f(x)=\sum_{r\in \mathbb{Q}:r<x}g(r)$, then we have $$f(x)=\sum_{r\in \mathbb{Q}:r<x, r\in Q_n}g(r)+\sum_{r\in X}g(r)$$ where $X=\{r\in \mathbb{Q}:r<x\}-\{r\in \mathbb{Q}:r<x, r\in Q_n\}$. Now since $X\subset \{r\in \mathbb{Q}: g(r)< 2^{-n}\} $, and $g(r)$ is non-negative for all the rational numbers then \begin{align} \sum_{r\in X}g(r)\le \sum_{r\in \mathbb{Q}: g(r)< 2^{-n}}g(r) = \sum_{i=n+1}^\infty2^{-i}\\ =2^{-n} \end{align} as desired. To conclude we have to show that $f$ is continuous at $x$. Since \begin{align}|f(y)-f(x)|\le |f(y)-f_n(y)|+|f_n(y)-f_n(x)|+|f_n(x)-f(x)| \\ \le 2^{-n} +|f_n(y)-f_n(x)|+2^{-n} \end{align} Given $\varepsilon>0$, we set $N$ such that $2^{-N}<\varepsilon$ and we choose $\delta = \min \{\, |r-x|: r\in Q_N  \,\}$. Then as we have proven, for all $y\in (x-\delta,x+\delta)$ we must have  $|f_N(y)-f_N(x)|\le \varepsilon$. So, \begin{align}|f(y)-f(x)|\le |f(y)-f_N(y)|+|f_N(y)-f_N(x)|+|f_N(x)-f(x)| \\ \le 2^{-N} +|f_N(y)-f_N(x)|+2^{-N} \\  \le 3\varepsilon  \end{align} which proves that $f$ is continuous at any irrational as desired. Hopefully this work :)","Let $q: \mathbb{N}\rightarrow \mathbb{Q}$ be a bijective map and let $g: \mathbb{Q}\rightarrow \mathbb{R}$ define $g(q(n))=2^{-n}$. Show that  $\sum_{r\in \mathbb{Q}}g(r)$ is absolutely convergent. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be define by the formula $f(x):=\sum_{r\in \mathbb{Q}:r<x}g(r)$, show that. (1) $f$ is well-defined. (2) $f$ is strictly monotonic increasing. (3) $f$ is discontinuous at every rational number. (4) $f$ is continuous at any irrational number $x$ (hint: first demonstrate that the functions $f_n(x):=\sum_{r\in \mathbb{Q}:r<x,\,g(r)\ge 2^{-n}}g(r)$ are continuous at $x$, and that $|f(x)-f_n(x)|\le 2^{-n}$). Sketch-proof First we have to show that $\sum_{r\in \mathbb{Q}}g(r)$ converges absolutely (I know that is obvious but I'd like to do a more elaborated argument of this fact). It suffice to show that for any finite subset of $\mathbb{Q}$ say $F$, the series $\sum _{r\in F}|g(r)|$ is bounded. Let $F\subset \mathbb{Q}$ and $\#F< \infty$. Then we have that the sequence $(q^{-1}(x))_{x\in F}$ is finite and therefore bounded by some $M\in \mathbb{N}$. Setting $\{q(m):m\in \mathbb{N} \text{ and } m\le M\}$ we know that it contains $F$. Thus \begin{align} \sum _{r\in F}|g(r)|\le \sum _{\{q(m):m\in \mathbb{N},\,  m\le M\}}|g(r)| = \sum _{m=0}^M|g(\,q(m)\,)|\\ =\sum _{m=0}^M 2^{-m} \le \sum _{m=0}^\infty 2^{-m}<\infty \end{align} Now since $F$ was an arbitrary subset of $\mathbb{Q}$, for any finite subset the upper bound works in other words we have $$ \sup \bigg\{\sum _{x\in F}|f(x)|:F\subset X; \#F<\infty \bigg\}<\infty $$ which shows that the series converges absolutely as desired. (1) $f$ is well-defined. we have to show that $f(x)$ is defined for every $x\in \mathbb{R}$. Let $x\in \mathbb{R}$, then we have $$\sum_{r\in \mathbb{Q}:r<x}g(r)\le \sum_{r\in \mathbb{Q}}g(r)$$ Since $g(r)$ is always non-negative for any rational number and since the latter is  convergent then the former must converges also. Hence $f(x)$ always exists. (2) $f$ is strictly monotonic increasing. Let $x,y\in \mathbb{R}$ such that $x<y$. Then there exists some rational number in between, let call it $s$, i.e., $x<s<y$. Since $s$ is rational then there exists some $n\in \mathbb{N}$ for which $q(n)=s$. So $$f(y)=\sum_{r\in \mathbb{Q}:r<y}g(r)=\sum_{r\in \mathbb{Q}:r<x}g(r) +\sum_{r\in X}g(r)$$ where $X = \{r\in \mathbb{Q}:r<y \} \backslash \{ r\in \mathbb{Q}:r<x\}$, clearly $s\in X$ so is a non-empty set, and since $g(r)$ is always positive for any rational. Then $$\sum_{r\in X}g(r)\ge g(s)=g(q(n))= 2^{-n}$$ Thus putting this together we can conclude that $f(y) > f(x)$, as desired. (3) $f$ is discontinuous to every rational number. Let $s$ be an arbitrary rational number, then there exists a $n\in \mathbb{N}$ so that $q(n)=s$. Now for what we have shown in (2), for any real number $x$ such that $x>s$, we have $f(x)\ge f(s)+2^{-n}$. We argue by contradiction, suppose that $f$ is continuous at $s$. Let $\varepsilon$ be a positive real such that $\varepsilon < 2^{-n}$. Since $f$ is continuous then there is some $\delta >0$ such that $|f(y)-f(s)|<\varepsilon$ whenever $y$ is a real number which is $\delta$- close to $s$, i.e., $|y-s|<\delta$. But there for any $x\in (s,s +\delta)$  we have $f(x)-f(s)\ge 2^{-n}>\varepsilon$. Contradiction. Hence $f$ cannot be continuous at any rational number as claimed. (4) $f$ is continuous at any irrational number $x$. Let $x$ be an irrational number and let define the functions $f_n: \mathbb{R} \rightarrow \mathbb{R}$ by setting  $$f_n(x):=\sum_{r\in \mathbb{Q}:r<x,\,g(r)\ge 2^{-n}}g(r)$$ Let define $Q_n= \{r\in \mathbb{Q}: g(r)\ge 2^{-n}\}$. We claim that $Q_n$ is finite. If $r \in Q_n$ so $g(r)\ge 2^{-n}$ and since $r\in \mathbb{Q}$, there is some $m$ such that $q(m)=r$. Thus $g(r)=g(q(m))= 2^{-m}\ge 2^{-n}$, which means that $m\le n$, in other words $Q_n= \{q(i): 0\le i \le n \}$, which clearly contains a finite number of rational numbers. We shall show that  $f_n$ is continuous at $x$. Given $\varepsilon>0$ and $n\in \mathbb{N}$. We set $\delta = \min \{\, |r-x|: r\in Q_n  \,\}$, $\delta$ is well-defined since $Q_n$ is non-empty (at least contain $q(0)$) and all  $|r-x|$ are different to zero since $x$ is irrational, then $\delta>0$. Let $\{z\in \mathbb{R}: |z-x|<\delta \}$, we claim that $|f_n(z)-f_n(x)|< \varepsilon$. $$|f_n(z)-f_n(x)| = \Bigg|\sum_{r\in \mathbb{Q}:r<z,\,r\in Q_n}g(r)\, -\sum_{r\in \mathbb{Q}:r<x,\,r\in Q_n}g(r) \Bigg|$$ If we can show that $\{r\in \mathbb{Q}:r<z,\,r\in Q_n\}=\{r\in \mathbb{Q}:r<x,\,r\in Q_n\}$, then both series are the same and the difference is zero. $LHS\Rightarrow RHS$ By contradiction suppose that there is some element $r$ in $LHS$ that is not in the $RHS$. So $r$ is rational, $r<z$ and $r\in Q_n$. Since $r$ is not in the $RHS$ then we would have $r\ge x$. Clearly the equality is not possible since $x$ is irrational. In the case $r>x$, we have $x<r<z$. So, $z-x>r-x$, but $z-x< \delta \le r-x$ since $r\in Q_n$, contradiction. $RHS\Rightarrow LHS$ Again for contradiction suppose there is some $r$ in $RHS$ that is not in the $LHS$. So this is only possible if $r\ge z$. If $z=r$, we have $|z-x|<\delta \le |r-x|=|z-x|$, a contradiction. The case when $r>z$ is similar to the last case in $LHS\Rightarrow RHS$, just with the roles of $z$ and $x$ interchanged. Thus both sets are equal and then the series are the same. Hence its difference is zero  and $f_n$ is continuous at $x$ as claimed. Claim: $|f(x)-f_n(x)|\le 2^{-n}$. Since $f(x)=\sum_{r\in \mathbb{Q}:r<x}g(r)$, then we have $$f(x)=\sum_{r\in \mathbb{Q}:r<x, r\in Q_n}g(r)+\sum_{r\in X}g(r)$$ where $X=\{r\in \mathbb{Q}:r<x\}-\{r\in \mathbb{Q}:r<x, r\in Q_n\}$. Now since $X\subset \{r\in \mathbb{Q}: g(r)< 2^{-n}\} $, and $g(r)$ is non-negative for all the rational numbers then \begin{align} \sum_{r\in X}g(r)\le \sum_{r\in \mathbb{Q}: g(r)< 2^{-n}}g(r) = \sum_{i=n+1}^\infty2^{-i}\\ =2^{-n} \end{align} as desired. To conclude we have to show that $f$ is continuous at $x$. Since \begin{align}|f(y)-f(x)|\le |f(y)-f_n(y)|+|f_n(y)-f_n(x)|+|f_n(x)-f(x)| \\ \le 2^{-n} +|f_n(y)-f_n(x)|+2^{-n} \end{align} Given $\varepsilon>0$, we set $N$ such that $2^{-N}<\varepsilon$ and we choose $\delta = \min \{\, |r-x|: r\in Q_N  \,\}$. Then as we have proven, for all $y\in (x-\delta,x+\delta)$ we must have  $|f_N(y)-f_N(x)|\le \varepsilon$. So, \begin{align}|f(y)-f(x)|\le |f(y)-f_N(y)|+|f_N(y)-f_N(x)|+|f_N(x)-f(x)| \\ \le 2^{-N} +|f_N(y)-f_N(x)|+2^{-N} \\  \le 3\varepsilon  \end{align} which proves that $f$ is continuous at any irrational as desired. Hopefully this work :)",,"['real-analysis', 'self-learning']"
88,Evaluate limit of a sequence... NBHM $2013$,Evaluate limit of a sequence... NBHM,2013,"Question is to Evaluate $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin(2n\pi + \frac{1}{2n\pi}))$$ All I could do was to see that $$\sin(2n\pi + \frac{1}{2n\pi}))=\sin( \frac{1}{2n\pi})$$ Just because $\sin(2n\pi+\theta)=\sin(\theta)$.. So, we now have $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin( \frac{1}{2n\pi}))$$ Now, as $\lim _{x\rightarrow \infty}x\sin(\frac{1}{x})=1$ we would have  $$\lim_{n\rightarrow \infty}2n\pi \sin( \frac{1}{2n\pi})=1$$ So, we now have $$\lim_{n\rightarrow \infty} \sin(1 + \frac{1}{2n\pi} \sin( \frac{1}{2n\pi}))$$ Now, as $\sin(x)$ is bounded and $\frac{1}{2n\pi} \rightarrow 0$ we would have $$\lim_{n\rightarrow \infty}\frac{1}{2n\pi} \sin( \frac{1}{2n\pi})=0$$ So, we would now left with : $$\lim_{n\rightarrow \infty} \sin(1)=\sin(1)$$ After all i would like to say that as $\sin (x)$ is continuous I can take limits inside. So, we have $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin(2n\pi + \frac{1}{2n\pi}))=\sin 1$$ I would like somebody to check if I have done correctly and I would be thankful if any body can let me know if there is anything more to be specified to do so. Thank you... :)","Question is to Evaluate $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin(2n\pi + \frac{1}{2n\pi}))$$ All I could do was to see that $$\sin(2n\pi + \frac{1}{2n\pi}))=\sin( \frac{1}{2n\pi})$$ Just because $\sin(2n\pi+\theta)=\sin(\theta)$.. So, we now have $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin( \frac{1}{2n\pi}))$$ Now, as $\lim _{x\rightarrow \infty}x\sin(\frac{1}{x})=1$ we would have  $$\lim_{n\rightarrow \infty}2n\pi \sin( \frac{1}{2n\pi})=1$$ So, we now have $$\lim_{n\rightarrow \infty} \sin(1 + \frac{1}{2n\pi} \sin( \frac{1}{2n\pi}))$$ Now, as $\sin(x)$ is bounded and $\frac{1}{2n\pi} \rightarrow 0$ we would have $$\lim_{n\rightarrow \infty}\frac{1}{2n\pi} \sin( \frac{1}{2n\pi})=0$$ So, we would now left with : $$\lim_{n\rightarrow \infty} \sin(1)=\sin(1)$$ After all i would like to say that as $\sin (x)$ is continuous I can take limits inside. So, we have $$\lim_{n\rightarrow \infty} \sin((2n\pi + \frac{1}{2n\pi}) \sin(2n\pi + \frac{1}{2n\pi}))=\sin 1$$ I would like somebody to check if I have done correctly and I would be thankful if any body can let me know if there is anything more to be specified to do so. Thank you... :)",,['real-analysis']
89,Does it exist a function for which the derivative changes sign more than countably many times?,Does it exist a function for which the derivative changes sign more than countably many times?,,"Does there exist any function $f \in C^2[0,1]$; $f: [0,1] \mapsto [0,1]$, for which the derivative changes sign more than countably many times?","Does there exist any function $f \in C^2[0,1]$; $f: [0,1] \mapsto [0,1]$, for which the derivative changes sign more than countably many times?",,"['calculus', 'real-analysis']"
90,a limit involving derivative,a limit involving derivative,,"Find the following limit for a smooth convex function $f:[0,\infty)\to\mathbb{R}^{\geq0}$ such that $f(1)=1$ and $f'(0)=f(0)=0$ $$\lim_{x\to0^{+}}\frac{f'(f^{-1}(x)a)}{f'(f^{-1}(x))}=L.$$ Is $L$ equal to $\frac{f(a)}{a}?$","Find the following limit for a smooth convex function $f:[0,\infty)\to\mathbb{R}^{\geq0}$ such that $f(1)=1$ and $f'(0)=f(0)=0$ $$\lim_{x\to0^{+}}\frac{f'(f^{-1}(x)a)}{f'(f^{-1}(x))}=L.$$ Is $L$ equal to $\frac{f(a)}{a}?$",,['calculus']
91,Bounded sequence and every convergent subsequence converges to L [duplicate],Bounded sequence and every convergent subsequence converges to L [duplicate],,"This question already has answers here : Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. (4 answers) Closed 6 years ago . Let $\{x_n\}$ be a bounded sequence such that every convergent subsequence converges to $L$. Prove that $$\lim_{n\to\infty}x_n = L.$$ The following is my proof. Please let me know what you think. Prove by contradiction: ($A \wedge  \lnot B$) Let {$x_n$} be bounded, and every convergent sub-sequence converges to $L$. Assume that $$\lim_{n\to\infty}x_n\ne L$$ Then there exists an $\epsilon>0$ such that $|x_n - L|\ge \epsilon$ for infinitely many n. Now, there exists a sub-sequence $\{x_{n_{k}}\}$ such that $|x_{n_{k}} - L| \ge\epsilon$. By Bolzano-Weierstrass Theorem $x{_{n{_k}}}$ has a convergent subsequence $x_{n_{k{_{l}}}}$ that does not converge to $L$. $x_{n_{k_{l}}}$ is also a sub-sequence of the original sequence $x_n$, then this is a contradiction since every convergent sub-sequence of $x_n$ converges to $L$. Hence the assumption is wrong.  So $\lim_{n\to\infty}x_n = L.$","This question already has answers here : Every subsequence of $x_n$ has a further subsequence which converges to $x$. Then the sequence $x_n$ converges to $x$. (4 answers) Closed 6 years ago . Let $\{x_n\}$ be a bounded sequence such that every convergent subsequence converges to $L$. Prove that $$\lim_{n\to\infty}x_n = L.$$ The following is my proof. Please let me know what you think. Prove by contradiction: ($A \wedge  \lnot B$) Let {$x_n$} be bounded, and every convergent sub-sequence converges to $L$. Assume that $$\lim_{n\to\infty}x_n\ne L$$ Then there exists an $\epsilon>0$ such that $|x_n - L|\ge \epsilon$ for infinitely many n. Now, there exists a sub-sequence $\{x_{n_{k}}\}$ such that $|x_{n_{k}} - L| \ge\epsilon$. By Bolzano-Weierstrass Theorem $x{_{n{_k}}}$ has a convergent subsequence $x_{n_{k{_{l}}}}$ that does not converge to $L$. $x_{n_{k_{l}}}$ is also a sub-sequence of the original sequence $x_n$, then this is a contradiction since every convergent sub-sequence of $x_n$ converges to $L$. Hence the assumption is wrong.  So $\lim_{n\to\infty}x_n = L.$",,"['real-analysis', 'convergence-divergence']"
92,Old versus New enunciation of Taylor's Theorem.,Old versus New enunciation of Taylor's Theorem.,,"I am studying from Spivak' Calculus, and he states Taylor's Theorem as follows: THEOREM Let $f',\cdots,f^{(n+1)}$ be defined on $[a,x]$ and let $R_{n,a}(x)$ be defined by $$R_{n,a}(x)=f(x)-\sum_{k=0}^n \frac{f^{(k)}(a)}{k!}x^k$$ Then, for some $t\in (a,x)$ $$\eqalign{   & {R_{n,a}}(x) = \frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{n!}}{\left( {x - t} \right)^n}\left( {x - a} \right)  \cr    & {R_{n,a}}(x) = \frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{\left( {n + 1} \right)!}}{\left( {x - a} \right)^{n + 1}} \cr} $$ Moreover, if $f^{(n+1)}$ is integrable over $[a,x]$; then $${R_{n,a}}(x) = \int\limits_a^x {\frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{n!}}{{\left( {x - t} \right)}^n}} dt$$ On the other hand, Landau's older textbook states: THEOREM Let $h>0$. Suppose $f^{(n)}$ continuous for $0\leq x\leq h$ and differentiable on $0<x<h$. Let $$\Phi=f(h)-\sum_{k=0}^n \frac{f^{(k)}(0)}{k!}h^k$$ Then, there exists a $t$ in $(0,h)$ such that $$\Phi=f^{(n+1)}(t)\frac{h^{n+1}}{(n+1)!}$$ and then uses this to prove. THEOREM (Taylor's Theorem) Let $h>0$.  Suppose $f^{(n)}$ continuous for $\mu \leq x\leq \mu+h$ and differentiable on $\mu <x<\mu+h$. Then there exists an $t$ such that $\mu<t<\mu +h $ and$$f(\mu+h)=\sum\limits_{v = 0}^n {\frac{{{f^{\left( v \right)}}\left( \mu  \right)}}{{v!}}} {h^v} + \frac{{{h^{n+1}}}}{{(n+1)!}}{f^{\left( n+1 \right)}}\left( t \right)$$ Now: I see the hypothesis are both the same, and they both address the remainder, but I can't see why Landau fixes $h>0$ and then gives a formula for this fixed $h$ and the $t$ (he actually names this $x$, but I found it a little conflicting) , while Spivak gives the remainder as a function of $x$ and a fixed $t$. Maybe it is just to make his (Landau's) proof simpler? I see how to go from Spivak's result to Landau's, but not the other way around.","I am studying from Spivak' Calculus, and he states Taylor's Theorem as follows: THEOREM Let $f',\cdots,f^{(n+1)}$ be defined on $[a,x]$ and let $R_{n,a}(x)$ be defined by $$R_{n,a}(x)=f(x)-\sum_{k=0}^n \frac{f^{(k)}(a)}{k!}x^k$$ Then, for some $t\in (a,x)$ $$\eqalign{   & {R_{n,a}}(x) = \frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{n!}}{\left( {x - t} \right)^n}\left( {x - a} \right)  \cr    & {R_{n,a}}(x) = \frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{\left( {n + 1} \right)!}}{\left( {x - a} \right)^{n + 1}} \cr} $$ Moreover, if $f^{(n+1)}$ is integrable over $[a,x]$; then $${R_{n,a}}(x) = \int\limits_a^x {\frac{{{f^{\left( {n + 1} \right)}}\left( t \right)}}{{n!}}{{\left( {x - t} \right)}^n}} dt$$ On the other hand, Landau's older textbook states: THEOREM Let $h>0$. Suppose $f^{(n)}$ continuous for $0\leq x\leq h$ and differentiable on $0<x<h$. Let $$\Phi=f(h)-\sum_{k=0}^n \frac{f^{(k)}(0)}{k!}h^k$$ Then, there exists a $t$ in $(0,h)$ such that $$\Phi=f^{(n+1)}(t)\frac{h^{n+1}}{(n+1)!}$$ and then uses this to prove. THEOREM (Taylor's Theorem) Let $h>0$.  Suppose $f^{(n)}$ continuous for $\mu \leq x\leq \mu+h$ and differentiable on $\mu <x<\mu+h$. Then there exists an $t$ such that $\mu<t<\mu +h $ and$$f(\mu+h)=\sum\limits_{v = 0}^n {\frac{{{f^{\left( v \right)}}\left( \mu  \right)}}{{v!}}} {h^v} + \frac{{{h^{n+1}}}}{{(n+1)!}}{f^{\left( n+1 \right)}}\left( t \right)$$ Now: I see the hypothesis are both the same, and they both address the remainder, but I can't see why Landau fixes $h>0$ and then gives a formula for this fixed $h$ and the $t$ (he actually names this $x$, but I found it a little conflicting) , while Spivak gives the remainder as a function of $x$ and a fixed $t$. Maybe it is just to make his (Landau's) proof simpler? I see how to go from Spivak's result to Landau's, but not the other way around.",,"['calculus', 'real-analysis', 'math-history', 'taylor-expansion']"
93,"Are integrations on forms ""different"" from Riemann integrations?","Are integrations on forms ""different"" from Riemann integrations?",,"I was amazed by the power of integration on forms when I learned that the Stokes' theorem can be written in a beautiful way (don't assume that I know more than this fact itself): $$ \int_{\Omega}d\omega=\int_{\partial\Omega}\omega. $$ from which Green's theorem, the divergence theorem, and the fundamental theorem of calculus follow. I learned the definition (it might not be the most general one) from Loring W. Tu's An Introduction to Manifolds : Let $\omega=f(x)dx^1\wedge \cdots\wedge dx^n$ be a $C^{\infty}$ $n$-form on an open subset $U\subset{\mathbb R}^n$, with standard coordinates $x^1,\cdots,x^n$. Its integral over a subset $A\subset U$ is defined to be the Riemann integral of $f(x)$:   $$ \int_{A}\omega=\int_{A}f(x)dx^1\wedge\cdots\wedge dx^n:=\int_{A}f(x)dx^1\cdots dx^n $$   if the Riemann integral exists. As I understand, since ""integration on forms"" is defined by the Riemann integral, it does not provide a new kind of integrals (e.g. Lebesgue integrals in measure theory, Itō integrals in stochastic analysis, etc.). Instead of doing so, it provides a new view of Riemann integral, in which for example $f(x)dx$ has its new meaning, $1$-form. Here are my questions: Is what I understand above correct? Or what's the fundamental difference between these two kinds of integrals? Can I say that this new integration provides a new way to prove the theorems in Riemann integral theory? [EDIT: Is the above definition the only way to define ""integration on forms""?] I feel that my questions might be vague. Any suggestions to improve it will be really appreciated.","I was amazed by the power of integration on forms when I learned that the Stokes' theorem can be written in a beautiful way (don't assume that I know more than this fact itself): $$ \int_{\Omega}d\omega=\int_{\partial\Omega}\omega. $$ from which Green's theorem, the divergence theorem, and the fundamental theorem of calculus follow. I learned the definition (it might not be the most general one) from Loring W. Tu's An Introduction to Manifolds : Let $\omega=f(x)dx^1\wedge \cdots\wedge dx^n$ be a $C^{\infty}$ $n$-form on an open subset $U\subset{\mathbb R}^n$, with standard coordinates $x^1,\cdots,x^n$. Its integral over a subset $A\subset U$ is defined to be the Riemann integral of $f(x)$:   $$ \int_{A}\omega=\int_{A}f(x)dx^1\wedge\cdots\wedge dx^n:=\int_{A}f(x)dx^1\cdots dx^n $$   if the Riemann integral exists. As I understand, since ""integration on forms"" is defined by the Riemann integral, it does not provide a new kind of integrals (e.g. Lebesgue integrals in measure theory, Itō integrals in stochastic analysis, etc.). Instead of doing so, it provides a new view of Riemann integral, in which for example $f(x)dx$ has its new meaning, $1$-form. Here are my questions: Is what I understand above correct? Or what's the fundamental difference between these two kinds of integrals? Can I say that this new integration provides a new way to prove the theorems in Riemann integral theory? [EDIT: Is the above definition the only way to define ""integration on forms""?] I feel that my questions might be vague. Any suggestions to improve it will be really appreciated.",,"['real-analysis', 'soft-question']"
94,About the limit of a uniformly converging function sequence,About the limit of a uniformly converging function sequence,,"Let $\phi_n\colon [a,b]\to \mathbb R$ be a sequence of continuous functions. Assuming there is an $A\subset [a,b]$ such that $\phi_n|_A$ converges uniformly to $\phi\colon A\to \mathbb R$, I have already proven that for all $x\in \bar A$ the closure of $A$ the sequence $(\phi_n(x))$ converges. Defining $\bar\phi$ on $\bar A$ by the pointwise limit of $\phi_n$ it seems to me, $\bar\phi$ should be continuous on $\bar A$ but I cannot bring a proof together. My idea: I have already proven, that for any finite set $B\subset \bar A$ the function $\bar\phi$ is continuous on $B\cup A$, and I was now trying to extend this to the entire $\bar A$ with Zorn's Lemma, but showing that, if $A\subset B_1\subset B_2 \subset \dots\subset \bar A$ and $\bar\phi$ is continuous on each $B_k$ then it is also on $\bigcup B_k$, is making some problems. Note that could also assume uniform continuity everywhere. Note the thoughts of Continuity on a union .","Let $\phi_n\colon [a,b]\to \mathbb R$ be a sequence of continuous functions. Assuming there is an $A\subset [a,b]$ such that $\phi_n|_A$ converges uniformly to $\phi\colon A\to \mathbb R$, I have already proven that for all $x\in \bar A$ the closure of $A$ the sequence $(\phi_n(x))$ converges. Defining $\bar\phi$ on $\bar A$ by the pointwise limit of $\phi_n$ it seems to me, $\bar\phi$ should be continuous on $\bar A$ but I cannot bring a proof together. My idea: I have already proven, that for any finite set $B\subset \bar A$ the function $\bar\phi$ is continuous on $B\cup A$, and I was now trying to extend this to the entire $\bar A$ with Zorn's Lemma, but showing that, if $A\subset B_1\subset B_2 \subset \dots\subset \bar A$ and $\bar\phi$ is continuous on each $B_k$ then it is also on $\bigcup B_k$, is making some problems. Note that could also assume uniform continuity everywhere. Note the thoughts of Continuity on a union .",,"['real-analysis', 'general-topology']"
95,density of roots of a family of polynomials: $(1-x^2)^{v+n}$,density of roots of a family of polynomials:,(1-x^2)^{v+n},"My research has brought me to the following, very general problem. Given a fixed, but arbitrary, natural number, $\displaystyle v$, consider the following family of polynomials: The $\displaystyle (n-1)^{th}$ derivative of $$\displaystyle (1-x^2)^{v+n} \ \ \forall n \in \mathbb{N} $$ I would like to prove (or disprove) that the roots of this entire family of polynomials forms a dense subset of the interval $\displaystyle [0,1]$ for any value of $\displaystyle v$ (I am not interested in roots outside the interval $\displaystyle [0,1]$). In other words, given any subinterval, $\displaystyle [a,b]$,no mater how small, at least one of these polynomials has at least one root in the interval $\displaystyle [a,b]$ (for any fixed value of $\displaystyle v$). I realize my question is very general and will happily accept any partial solutions.","My research has brought me to the following, very general problem. Given a fixed, but arbitrary, natural number, $\displaystyle v$, consider the following family of polynomials: The $\displaystyle (n-1)^{th}$ derivative of $$\displaystyle (1-x^2)^{v+n} \ \ \forall n \in \mathbb{N} $$ I would like to prove (or disprove) that the roots of this entire family of polynomials forms a dense subset of the interval $\displaystyle [0,1]$ for any value of $\displaystyle v$ (I am not interested in roots outside the interval $\displaystyle [0,1]$). In other words, given any subinterval, $\displaystyle [a,b]$,no mater how small, at least one of these polynomials has at least one root in the interval $\displaystyle [a,b]$ (for any fixed value of $\displaystyle v$). I realize my question is very general and will happily accept any partial solutions.",,"['real-analysis', 'polynomials', 'special-functions', 'roots']"
96,Extracting a subsequence common to infinitely many sets from an uncountable collection with uniform positive upper density,Extracting a subsequence common to infinitely many sets from an uncountable collection with uniform positive upper density,,"Let $\{a_n\},\{b_n\}$ be strictly increasing sequence of positive integers satisfying $a_1<b_1<a_2<b_2<a_3<b_3<\ldots$ and $(b_n-a_n) \to \infty$ . Define $I_n:= [a_n,b_n]$ , meaning the set of all consecutive integers from $a_n$ to $b_n$ .. Let $A_{\alpha},\alpha \in \mathcal{A}$ be an uncountable family of infinite subsets of $\mathbb{N}$ that satisfies the following: There exists some $c \in (0,1)$ so that for all $\alpha \in \mathcal{A}$ there exists some $n(\alpha) \in \mathbb{N}$ so that $|A_{\alpha} \cap I_n|\geq c(b_n-a_n)$ for all $n\geq n(\alpha)$ . (Here $|B|$ is the number of elements of a finite subset $B \subset \mathbb{N}$ ) Then my question is the following: Does there exist a strictly increasing sequence of positive integers $\{l_n\} \to \infty$ for which there exists an infinite subset $\mathcal{B}\subset \mathcal{A}$ so that $l_n \in A_{\alpha}$ for all $n \in \mathbb{N}$ , $\alpha \in \mathcal{B}$ ?","Let be strictly increasing sequence of positive integers satisfying and . Define , meaning the set of all consecutive integers from to .. Let be an uncountable family of infinite subsets of that satisfies the following: There exists some so that for all there exists some so that for all . (Here is the number of elements of a finite subset ) Then my question is the following: Does there exist a strictly increasing sequence of positive integers for which there exists an infinite subset so that for all , ?","\{a_n\},\{b_n\} a_1<b_1<a_2<b_2<a_3<b_3<\ldots (b_n-a_n) \to \infty I_n:= [a_n,b_n] a_n b_n A_{\alpha},\alpha \in \mathcal{A} \mathbb{N} c \in (0,1) \alpha \in \mathcal{A} n(\alpha) \in \mathbb{N} |A_{\alpha} \cap I_n|\geq c(b_n-a_n) n\geq n(\alpha) |B| B \subset \mathbb{N} \{l_n\} \to \infty \mathcal{B}\subset \mathcal{A} l_n \in A_{\alpha} n \in \mathbb{N} \alpha \in \mathcal{B}","['real-analysis', 'sequences-and-series', 'set-theory', 'descriptive-set-theory', 'extremal-combinatorics']"
97,"Does there exist a function $\mathbb{R}^2 \rightarrow \mathbb{R}$ whose directional derivatives are infinity in almost every direction, at all points?","Does there exist a function  whose directional derivatives are infinity in almost every direction, at all points?",\mathbb{R}^2 \rightarrow \mathbb{R},"For $\mathbf{x}\in \mathbb{R}^2$ and $\mathbf{r}\in S^1$ (here $S^1=\{\mathbf{r}\in \mathbb{R}^2: \|\mathbf{r}\|=1\}$ ), let us write $f'(\mathbf{x};\mathbf{r})=\infty$ if $\liminf\limits_{t\rightarrow 0}\frac{f(\mathbf{x}+t\mathbf{r})-f(\mathbf{x})}{t}=\infty$ . My question is the following: does there exist a (not necessarily measurable) function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ , such that for all $\mathbf{x}\in \mathbb{R}^2$ , the set $$\{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$$ has full measure as a subset of $S^1$ ? Here $S^1$ is equipped with the ""arc length"" measure. Can a function $f:\mathbb{R} \rightarrow \mathbb{R}$ be ""infinitely steep"" on a set with non-zero Lebesgue outer measure? tells us that on any line parallell to $\mathbf{r}$ , the set of points $\mathbf{x}$ for which $f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty$ , is a null set. One is then tempted to argue that by Tonelli's, the set $$A=\{(\mathbf{r}, \mathbf{x})\in S^1\times \mathbb{R}^2: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$$ is a null set. On the other hand, Tonelli's seems to imply that when $\{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$ has full measure for all $\mathbf{x}\in \mathbb{R}^2$ , then $A$ has full measure, so there appears to be a contradiction. However, none of these applications of Tonelli are valid, since Tonelli only works when one integrates over a measurable set and a set may be non-measurable even if all of its sections are null sets. Indeed, there are non-measurable subsets of $\mathbb{R}^2$ for which all sections are singletons (see https://mathoverflow.net/questions/89375/sections-measure-zero-imply-set-is-measure-zero ). All of this is to say, I cannot straightforwardly deduce from the "" $1$ -dimensional case"", i.e. the first link, that that there cannot exist a function with the properties as in my question. My hope is that I have not overlooked some obvious argument here and that either the answer to my question is negative for some ""deeper"" reason, or, even better, that the answer is affirmative!","For and (here ), let us write if . My question is the following: does there exist a (not necessarily measurable) function , such that for all , the set has full measure as a subset of ? Here is equipped with the ""arc length"" measure. Can a function $f:\mathbb{R} \rightarrow \mathbb{R}$ be ""infinitely steep"" on a set with non-zero Lebesgue outer measure? tells us that on any line parallell to , the set of points for which , is a null set. One is then tempted to argue that by Tonelli's, the set is a null set. On the other hand, Tonelli's seems to imply that when has full measure for all , then has full measure, so there appears to be a contradiction. However, none of these applications of Tonelli are valid, since Tonelli only works when one integrates over a measurable set and a set may be non-measurable even if all of its sections are null sets. Indeed, there are non-measurable subsets of for which all sections are singletons (see https://mathoverflow.net/questions/89375/sections-measure-zero-imply-set-is-measure-zero ). All of this is to say, I cannot straightforwardly deduce from the "" -dimensional case"", i.e. the first link, that that there cannot exist a function with the properties as in my question. My hope is that I have not overlooked some obvious argument here and that either the answer to my question is negative for some ""deeper"" reason, or, even better, that the answer is affirmative!","\mathbf{x}\in \mathbb{R}^2 \mathbf{r}\in S^1 S^1=\{\mathbf{r}\in \mathbb{R}^2: \|\mathbf{r}\|=1\} f'(\mathbf{x};\mathbf{r})=\infty \liminf\limits_{t\rightarrow 0}\frac{f(\mathbf{x}+t\mathbf{r})-f(\mathbf{x})}{t}=\infty f:\mathbb{R}^2\rightarrow \mathbb{R} \mathbf{x}\in \mathbb{R}^2 \{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\} S^1 S^1 \mathbf{r} \mathbf{x} f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty A=\{(\mathbf{r}, \mathbf{x})\in S^1\times \mathbb{R}^2: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\} \{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\} \mathbf{x}\in \mathbb{R}^2 A \mathbb{R}^2 1","['real-analysis', 'measure-theory', 'derivatives']"
98,Why are monotonicity and convexity so intensely studied in comparison to their higher-order analogies?,Why are monotonicity and convexity so intensely studied in comparison to their higher-order analogies?,,"I've been recently contemplating what can be said about functions $f:\Bbb R \to \Bbb R$ with positive third-order derivative and their properties: It puzzles me why monotonicity and convexity , i.e. the properties associate with the first- and second-order derivatives of a function being positive, are frequently used, whilst  their higher-order alternatives are so rarely referred to. I found that the third-order derivative at a point is called jerk in physics and its meaning is discussed here . I found also a discussion on the topic why are third-order concepts so rare . However, what I'm interested in are the uses of the fact that third-order derivative of $f$ is positive in analysis. For example, whilst strict convexity of $f$ implies that every critical point of $f$ is the unique global minimum of $f$ and that $f$ has at most two roots, having strictly positive third-order derivative would guarantee that every point with $f''(x)=0$ is the unique inflection point of $f$ , that the function has at most one local minimum and one local maximum, and no more than 3 roots. One could argue that it would be informative enough to know that there is $x_2\in \Bbb R$ such that $f''(x_2)=0$ and that $f$ is concave on $(-\infty,x_2]$ and convex on $[x_2,\infty)$ . However, analogously one could argue that convexity is not needed for analyzing function's minima because it is enough to know that there is $x_1\in \Bbb R$ such that $f'(x_1)=0$ and that $f$ is decreasing on $(-\infty,x_1]$ and increasing on $[x_1,\infty)$ . The concept of $n$ -th order convexity (having positive $n$ -th order derivative) is essential for the analysis of the number of roots of one-variable functions: A function with positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra. Consider the question on what is the maximum number of strict local minima that a degree $k$ polynomial $p(x,y)$ in two variables can have ? In case of quadratic polynomial one can readily answer: The polynomial has a strict local minima only if it is strictly convex, and then the minimum must be unique. In case of cubic polynomial , there is also at most one strict local minimum because if there were two, say at points $a$ and $b$ , then the third degree polynomial $q(t)=p\big((1-t)a+tb\big)$ would need to have two strict local minima – impossible. [1] In case of quartic polynomial , none of the above arguments apply and the only ready estimate follows from applying Bézout's theorem to the partial derivatives of $p$ , and so we can be sure that $p$ has is no more than $3\times 3$ isolated critical points. [2] I expected that the above analysis would be trivial for the case of quadratic polynomials thanks to the concept of convexity. However, the analysis is equally trivial in case of cubic polynomial, the only difference is that there is no name for the ""third-order convexity"" that gives the result. In fact, the analysis becomes difficult as late as in the case of quartic polynomials. This suggests that third-order convexity has practical applications, only the higher-order alternatives would be less practical for the analysis of functions in two or more variables. Does importance of $n$ -th derivative drop? Let me compare the count of search results of terms a) convex b) monotone or monotonic (sum up the count) \begin{array} {|r|r|r|}\hline   & \text{Google} & \text{site:SME} & \text{site:mathoverflow} & \text{G Scholar} \\ \hline  \text{a)} & 148M & 79K & 229 & 4.3M  \\ \hline  \text{b)} & 98M & 77K & 54 & 2.5M \\ \hline   \end{array} Why is the second-order property (convexity) more prevalent than the first-oder one (monotonicity) and yet the third-order one is almost never heard of? Questions: Is there a standardized name for functions with $f^{(3)}>0$ ? Is the main reason that $f^{(3)}$ is so rarely analyzed the fact that for most problems it is enough to determine on which regions $f$ is convex/concave and the effort needed to analyze $f^{(3)}$ would typically not be justified? Is there a known set property of $\mathop{epi}(f)$ for $f$ with $f^{(3)}>0$ , akin to strict convexity of $\mathop{epi}(f)$ for $f$ with $f^{(2)}>0$ ? Related Posts: Let me share some observations and conjectures that I came up with when contemplating on this topic: Functions not necessarily differentiable that behave like those with $f^{(3)}\geq 0$ : Geometric characterization of functions with positive third derivative . ""Quasi"" generalization of the condition that $f^{(3)}>0$ : Is there a third-order analogy of quasi-convexity? An attempt to find the geometric property whose special case $\mathop{epi}(f)$ of $f$ with $f^{(3)}\geq 0$ satisfies: Second-order star convex set: A set whose intersection with any conics passing through two given points consists of at most two connected curves.","I've been recently contemplating what can be said about functions with positive third-order derivative and their properties: It puzzles me why monotonicity and convexity , i.e. the properties associate with the first- and second-order derivatives of a function being positive, are frequently used, whilst  their higher-order alternatives are so rarely referred to. I found that the third-order derivative at a point is called jerk in physics and its meaning is discussed here . I found also a discussion on the topic why are third-order concepts so rare . However, what I'm interested in are the uses of the fact that third-order derivative of is positive in analysis. For example, whilst strict convexity of implies that every critical point of is the unique global minimum of and that has at most two roots, having strictly positive third-order derivative would guarantee that every point with is the unique inflection point of , that the function has at most one local minimum and one local maximum, and no more than 3 roots. One could argue that it would be informative enough to know that there is such that and that is concave on and convex on . However, analogously one could argue that convexity is not needed for analyzing function's minima because it is enough to know that there is such that and that is decreasing on and increasing on . The concept of -th order convexity (having positive -th order derivative) is essential for the analysis of the number of roots of one-variable functions: A function with positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra. Consider the question on what is the maximum number of strict local minima that a degree polynomial in two variables can have ? In case of quadratic polynomial one can readily answer: The polynomial has a strict local minima only if it is strictly convex, and then the minimum must be unique. In case of cubic polynomial , there is also at most one strict local minimum because if there were two, say at points and , then the third degree polynomial would need to have two strict local minima – impossible. [1] In case of quartic polynomial , none of the above arguments apply and the only ready estimate follows from applying Bézout's theorem to the partial derivatives of , and so we can be sure that has is no more than isolated critical points. [2] I expected that the above analysis would be trivial for the case of quadratic polynomials thanks to the concept of convexity. However, the analysis is equally trivial in case of cubic polynomial, the only difference is that there is no name for the ""third-order convexity"" that gives the result. In fact, the analysis becomes difficult as late as in the case of quartic polynomials. This suggests that third-order convexity has practical applications, only the higher-order alternatives would be less practical for the analysis of functions in two or more variables. Does importance of -th derivative drop? Let me compare the count of search results of terms a) convex b) monotone or monotonic (sum up the count) Why is the second-order property (convexity) more prevalent than the first-oder one (monotonicity) and yet the third-order one is almost never heard of? Questions: Is there a standardized name for functions with ? Is the main reason that is so rarely analyzed the fact that for most problems it is enough to determine on which regions is convex/concave and the effort needed to analyze would typically not be justified? Is there a known set property of for with , akin to strict convexity of for with ? Related Posts: Let me share some observations and conjectures that I came up with when contemplating on this topic: Functions not necessarily differentiable that behave like those with : Geometric characterization of functions with positive third derivative . ""Quasi"" generalization of the condition that : Is there a third-order analogy of quasi-convexity? An attempt to find the geometric property whose special case of with satisfies: Second-order star convex set: A set whose intersection with any conics passing through two given points consists of at most two connected curves.","f:\Bbb R \to \Bbb R f f f f f f''(x)=0 f x_2\in \Bbb R f''(x_2)=0 f (-\infty,x_2] [x_2,\infty) x_1\in \Bbb R f'(x_1)=0 f (-\infty,x_1] [x_1,\infty) n n k p(x,y) a b q(t)=p\big((1-t)a+tb\big) p p 3\times 3 n \begin{array} {|r|r|r|}\hline 
 & \text{Google} & \text{site:SME} & \text{site:mathoverflow} & \text{G Scholar} \\ \hline 
\text{a)} & 148M & 79K & 229 & 4.3M  \\ \hline 
\text{b)} & 98M & 77K & 54 & 2.5M \\ \hline  
\end{array} f^{(3)}>0 f^{(3)} f f^{(3)} \mathop{epi}(f) f f^{(3)}>0 \mathop{epi}(f) f f^{(2)}>0 f^{(3)}\geq 0 f^{(3)}>0 \mathop{epi}(f) f f^{(3)}\geq 0","['real-analysis', 'soft-question', 'convex-analysis', 'maxima-minima', 'convex-geometry']"
99,On the convergence of $\sum_n 1 /(n\sin(2^nx))$,On the convergence of,\sum_n 1 /(n\sin(2^nx)),"Find all values of $x$ such that $\displaystyle\sum_{n=1}^\infty\frac{1}{n\sin(2^nx)}$ converges. I've been attempting to solve this problem without much success. Firstly, it is defined for all $x\in\mathbb R$ such that $\forall n\in\mathbb N$ , $2^nx\not\in\pi\mathbb Z$ . Then, if $x$ is such that $\sin(2^nx)$ converges then by comparison to $\sum\frac1n$ the series diverges. If $x\in\pi(\mathbb R\backslash\mathbb Q)$ , then $(\sin(2^nx))_{n\in\mathbb N}$ is dense in $[-1,1]$ and so it feels like the series should diverge. On the other hand, I believe that if $x\in\pi\mathbb Q$ , then $(\sin(2^nx))_{n\in\mathbb N}$ should be periodic (or not far from) and the series should converge. However, I can't prove any of this. Any help is appreciated!","Find all values of such that converges. I've been attempting to solve this problem without much success. Firstly, it is defined for all such that , . Then, if is such that converges then by comparison to the series diverges. If , then is dense in and so it feels like the series should diverge. On the other hand, I believe that if , then should be periodic (or not far from) and the series should converge. However, I can't prove any of this. Any help is appreciated!","x \displaystyle\sum_{n=1}^\infty\frac{1}{n\sin(2^nx)} x\in\mathbb R \forall n\in\mathbb N 2^nx\not\in\pi\mathbb Z x \sin(2^nx) \sum\frac1n x\in\pi(\mathbb R\backslash\mathbb Q) (\sin(2^nx))_{n\in\mathbb N} [-1,1] x\in\pi\mathbb Q (\sin(2^nx))_{n\in\mathbb N}","['real-analysis', 'sequences-and-series', 'trigonometry']"
