,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving Linear Systems with LU Decomposition and complete pivoting; stupid question,Solving Linear Systems with LU Decomposition and complete pivoting; stupid question,,"Given a matrix A and vector B, solve $Ax=B$ Using LU Decomposition with full Pivoting; $PAQ=LU$ where P and Q are row and column permutation vectors (correct me if I'm wrong) What I don't understand is what to do with the permutation matrices to finish the solution. I know in partial pivoting, its simple $Lz=PB$ $Ux=z$ But what do I do with Q? PS If anyone is a C head, you're help would be appreciated in the implementation","Given a matrix A and vector B, solve $Ax=B$ Using LU Decomposition with full Pivoting; $PAQ=LU$ where P and Q are row and column permutation vectors (correct me if I'm wrong) What I don't understand is what to do with the permutation matrices to finish the solution. I know in partial pivoting, its simple $Lz=PB$ $Ux=z$ But what do I do with Q? PS If anyone is a C head, you're help would be appreciated in the implementation",,"['linear-algebra', 'matrices']"
1,How to find matrices with given commutator,How to find matrices with given commutator,,"Consider $M_2(\mathbb{Z})$. Is it possible to find two matrices A,B such that their commutator AB - BA equals a given matrix C? Is there any chance to characterize all possible occuring commutators in a given set of matrices over some fixed ring? One specific example: C the unit matrix. Are there such A,B? To find them i would try to solve a system of equations. But is there are more systematic way to say theoretically that we cannot or can find A,B, such that AB-BA = 1?","Consider $M_2(\mathbb{Z})$. Is it possible to find two matrices A,B such that their commutator AB - BA equals a given matrix C? Is there any chance to characterize all possible occuring commutators in a given set of matrices over some fixed ring? One specific example: C the unit matrix. Are there such A,B? To find them i would try to solve a system of equations. But is there are more systematic way to say theoretically that we cannot or can find A,B, such that AB-BA = 1?",,"['matrices', 'ring-theory']"
2,What is the best way to show an idempotent matrix whose nullspace and range are orthogonal must be symmetric?,What is the best way to show an idempotent matrix whose nullspace and range are orthogonal must be symmetric?,,"and in the other way, orthogonal nullspace and range require symmetry? Thanks, R","and in the other way, orthogonal nullspace and range require symmetry? Thanks, R",,"['linear-algebra', 'matrices']"
3,How many positive elements in a diagonal matrix $D$ do we need to make $A+D$ positive definite for a real symmetric $A$ with $m$ negative eigenvalues,How many positive elements in a diagonal matrix  do we need to make  positive definite for a real symmetric  with  negative eigenvalues,D A+D A m,"Consider a $n$ -dimensional real symmetric matrix $A\in\mathbb{R}^{n\times n}$ having exactly $m$ negative eigenvalues and $n-m$ positive eigenvalues ( $1\leq m\leq n$ ). Let $D\in\mathbb{R}^{n\times n}$ be a diagonal matrix with non-negative elements. My question is: To make $A+D\succ0$ , i.e. positive definite, at least how many positive elements of $D$ do we need? For $n=1, 2$ , it is easy to show that the answer is exactly $m$ . Does this conclusion still hold for $n\geq3$ ? Any ideas and suggestions are welcomed. Thanks in advance!","Consider a -dimensional real symmetric matrix having exactly negative eigenvalues and positive eigenvalues ( ). Let be a diagonal matrix with non-negative elements. My question is: To make , i.e. positive definite, at least how many positive elements of do we need? For , it is easy to show that the answer is exactly . Does this conclusion still hold for ? Any ideas and suggestions are welcomed. Thanks in advance!","n A\in\mathbb{R}^{n\times n} m n-m 1\leq m\leq n D\in\mathbb{R}^{n\times n} A+D\succ0 D n=1, 2 m n\geq3","['matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'hessian-matrix']"
4,index of $Z\mathrm{SL}_2(\mathbb{Q}_p)$ in $\mathrm{GL}_2(\mathbb{Q}_p)$,index of  in,Z\mathrm{SL}_2(\mathbb{Q}_p) \mathrm{GL}_2(\mathbb{Q}_p),"Let $p>2$ , consider the subgroup $Z\mathrm{SL}_2(\mathbb{Q}_p)$ of $\mathrm{GL}_2(\mathbb{Q}_p)$ where $Z\cong\mathbb{Q}_p^\times$ is the centre of $\mathrm{GL}_2(\mathbb{Q}_p)$ . Then I'd like to know how to prove that the index of $Z\mathrm{SL}_2(\mathbb{Q}_p)$ in $\mathrm{GL}_2(\mathbb{Q}_p)$ is $4$ . By the determinant map, I know that there is an isomorphism of groups $\mathrm{GL}_2(\mathbb{Q}_p)/\mathrm{SL}_2(\mathbb{Q}_p)\cong\mathbb{Q}_p^\times$ . But I do know how to continue. The motivation is that I want to know the dimension of the induced representation $\mathrm{Ind}_{Z\mathrm{SL}_2(\mathbb{Q}_p)}^{\mathrm{GL}_2(\mathbb{Q}_p)}$ of a character.","Let , consider the subgroup of where is the centre of . Then I'd like to know how to prove that the index of in is . By the determinant map, I know that there is an isomorphism of groups . But I do know how to continue. The motivation is that I want to know the dimension of the induced representation of a character.",p>2 Z\mathrm{SL}_2(\mathbb{Q}_p) \mathrm{GL}_2(\mathbb{Q}_p) Z\cong\mathbb{Q}_p^\times \mathrm{GL}_2(\mathbb{Q}_p) Z\mathrm{SL}_2(\mathbb{Q}_p) \mathrm{GL}_2(\mathbb{Q}_p) 4 \mathrm{GL}_2(\mathbb{Q}_p)/\mathrm{SL}_2(\mathbb{Q}_p)\cong\mathbb{Q}_p^\times \mathrm{Ind}_{Z\mathrm{SL}_2(\mathbb{Q}_p)}^{\mathrm{GL}_2(\mathbb{Q}_p)},"['matrices', 'group-theory', 'number-theory', 'p-adic-number-theory']"
5,Rearrangement property of diagonal matrices [closed],Rearrangement property of diagonal matrices [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question Is it true that for a diagonal matrix $B\in\mathbb{R}^{n\times n}$ , a matrix $A\in\mathbb{R}^{n\times p}$ , the following property holds: $$ A^\top B A = A^\top A B, $$ where in essence I am asking this: Can diagonal matrices be rearranged in matrix multiplications (we know this is not true in general from the basic laws of linear algebra)? The motivation for knowing this is that it helps to simplify the analysis of machine learning algorithms that uses singular/eigen value decomposition. Thanks.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question Is it true that for a diagonal matrix , a matrix , the following property holds: where in essence I am asking this: Can diagonal matrices be rearranged in matrix multiplications (we know this is not true in general from the basic laws of linear algebra)? The motivation for knowing this is that it helps to simplify the analysis of machine learning algorithms that uses singular/eigen value decomposition. Thanks.","B\in\mathbb{R}^{n\times n} A\in\mathbb{R}^{n\times p} 
A^\top B A = A^\top A B,
","['linear-algebra', 'matrices', 'machine-learning']"
6,Trace of matrix which equals to number of columns,Trace of matrix which equals to number of columns,,"While studying linear algebra, I found that: Let $A$ be an $m \times n$ Matrix (with $m>n$ ) then the trace $tr(A(A^T A)^{-1} A^T)$ equals the number of columns of $A$ . Does this hold in general and if so: why? If the number of columns is 1, the trace seems to be 1, too. I think that each columns acts like some kind of adding 1, but I can't explain why.","While studying linear algebra, I found that: Let be an Matrix (with ) then the trace equals the number of columns of . Does this hold in general and if so: why? If the number of columns is 1, the trace seems to be 1, too. I think that each columns acts like some kind of adding 1, but I can't explain why.",A m \times n m>n tr(A(A^T A)^{-1} A^T) A,"['linear-algebra', 'matrices', 'trace']"
7,How do you analyse the rank of a matrix depending on a parameter,How do you analyse the rank of a matrix depending on a parameter,,"I have the matrix $$ \begin{pmatrix} 3-t & 3 & 2t \\ -2 & 0 & -1 \\ 1 & 3 & 2+t \\ t+2 & 0 &t \end{pmatrix} $$ And I'm asked to evaluate the matrix's rank depending of $t$ , using the determinants and minors of the matrix. However, the solution I see in the answer book is too short and only addresses one minor $$ \begin{pmatrix} 3-t & 3 & 2t \\ -2 & 0 & -1 \\ 1 & 3 & 2+t \end{pmatrix} $$ and its determinant equals $0$ . But I've tried different minors and they dont give always the same answer. Is it because I'm doing it wrong or I should think of something else? The answer book is this page 85 ex 30","I have the matrix And I'm asked to evaluate the matrix's rank depending of , using the determinants and minors of the matrix. However, the solution I see in the answer book is too short and only addresses one minor and its determinant equals . But I've tried different minors and they dont give always the same answer. Is it because I'm doing it wrong or I should think of something else? The answer book is this page 85 ex 30","
\begin{pmatrix}
3-t & 3 & 2t \\
-2 & 0 & -1 \\
1 & 3 & 2+t \\
t+2 & 0 &t
\end{pmatrix}
 t 
\begin{pmatrix}
3-t & 3 & 2t \\
-2 & 0 & -1 \\
1 & 3 & 2+t
\end{pmatrix}
 0","['matrices', 'determinant', 'matrix-rank']"
8,Trace inequality for $\operatorname{Tr}(ABAB)$?,Trace inequality for ?,\operatorname{Tr}(ABAB),"Is it true that for square matrices $A,B$ it holds that $$\operatorname{Tr}(A^2)\operatorname{Tr}(ABAB)\le (\operatorname{Tr}(A^2 B))^2$$ maybe under some additional assumptions on $A,B$ like positive semidefinite? Background: In a more general setting I obtained $$\operatorname{Tr}(A\bar A)\operatorname{Tr}(AB\bar A \bar B)+\lvert\operatorname{Tr}(AB\bar A)\rvert^2+\cdots\ge 0$$ for complex-symmetric $A$ and general $B$ , where $\bar A$ denotes the entry-wise complex conjugate. I am wondering whether the sum of the two terms above is always non-negative on their own without the rest of the sum. But then I noticed that even in the setting of real-valued matrices I do not know whether such an inequality might hold true.","Is it true that for square matrices it holds that maybe under some additional assumptions on like positive semidefinite? Background: In a more general setting I obtained for complex-symmetric and general , where denotes the entry-wise complex conjugate. I am wondering whether the sum of the two terms above is always non-negative on their own without the rest of the sum. But then I noticed that even in the setting of real-valued matrices I do not know whether such an inequality might hold true.","A,B \operatorname{Tr}(A^2)\operatorname{Tr}(ABAB)\le (\operatorname{Tr}(A^2 B))^2 A,B \operatorname{Tr}(A\bar A)\operatorname{Tr}(AB\bar A \bar B)+\lvert\operatorname{Tr}(AB\bar A)\rvert^2+\cdots\ge 0 A B \bar A","['linear-algebra', 'matrices', 'inequality', 'trace']"
9,Derivative of $\varphi({\bf X}) = \sum_{i=1}^n \lambda_i({\bf X}) \log \lambda_i({\bf X})$,Derivative of,\varphi({\bf X}) = \sum_{i=1}^n \lambda_i({\bf X}) \log \lambda_i({\bf X}),"Let $\mathbb{S}_+^n$ denote the set of $n \times n$ symmetric positive definite matrices. Let scalar field $\varphi : \mathbb{S}_+^n \to \Bbb R$ be defined by $$\varphi({\bf X}) := \sum_{i=1}^n \lambda_i({\bf X}) \log \lambda_i({\bf X})$$ where $\lambda_1({\bf X}), \ldots, \lambda_n({\bf X})$ are the eigenvalues of the matrix ${\bf X} \in \mathbb{S}_+^n$ . What is the gradient of $\varphi$ ?",Let denote the set of symmetric positive definite matrices. Let scalar field be defined by where are the eigenvalues of the matrix . What is the gradient of ?,"\mathbb{S}_+^n n \times n \varphi : \mathbb{S}_+^n \to \Bbb R \varphi({\bf X}) := \sum_{i=1}^n \lambda_i({\bf X}) \log \lambda_i({\bf X}) \lambda_1({\bf X}), \ldots, \lambda_n({\bf X}) {\bf X} \in \mathbb{S}_+^n \varphi","['matrices', 'derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus', 'scalar-fields']"
10,"Proving $\dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}=\frac{n(n-1)}{2}$ where $A\in {\rm O}(n)$ [duplicate]",Proving  where  [duplicate],"\dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}=\frac{n(n-1)}{2} A\in {\rm O}(n)","This question already has answers here : Show that an orthogonal group is a $ \frac{n(n−1)}2 $-dim. $ C^{\infty} $-Manifold and find its tangent space (2 answers) Closed last year . I'm reading Lee's Introduction to Smooth Manifolds and trying to show that $$T_A {\rm O}(n)=\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}.$$ Here is my attempt about this question: Take any smooth curve $\gamma(t)$ in ${\rm O}(n)$ satisfying $\gamma(0)=A$ , then $$\gamma(t)\cdot\gamma(t)^T=I_n.$$ By taking the derivative of the above formula with respect to $t$ , we can get $$\gamma'(t)\cdot\gamma(t)^T+\gamma(t)\cdot(\gamma(t)^T)'=0,$$ then $$\gamma'(0)\cdot\gamma(0)^T=-\gamma(0)\cdot(\gamma(t)^T)'|_{t=0}=-\gamma(0)\cdot\gamma'(0)^T.$$ Since $\gamma(0)=A$ , we obtain that $\gamma'(0)$ satisfies $$\gamma'(0)\cdot A^T=-A\cdot\gamma'(0)^T,$$ and $\gamma'(0)\in T_A {\rm O}(n)$ is arbitrary, therefore $$T_A {\rm O}(n)\subset\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}.$$ Now it suffices to prove that $$\dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-(BA^T)^T\}=\frac{n(n-1)}{2}$$ where $A\in {\rm O}(n)$ , because $\dim T_A {\rm O}(n)=\dim{\rm O}(n)=\frac{n(n-1)}{2}$ , but I'm stuck here. I know that the dimension of the vector space composed of all skew-symmetric matrices is $\frac{n(n-1)}{2}$ , and the above set is very similar to the vector space composed of all skew-symmetric matrices in form, but I don't know what the connection between them is and how to connect them. Any help would be great appreciated!","This question already has answers here : Show that an orthogonal group is a $ \frac{n(n−1)}2 $-dim. $ C^{\infty} $-Manifold and find its tangent space (2 answers) Closed last year . I'm reading Lee's Introduction to Smooth Manifolds and trying to show that Here is my attempt about this question: Take any smooth curve in satisfying , then By taking the derivative of the above formula with respect to , we can get then Since , we obtain that satisfies and is arbitrary, therefore Now it suffices to prove that where , because , but I'm stuck here. I know that the dimension of the vector space composed of all skew-symmetric matrices is , and the above set is very similar to the vector space composed of all skew-symmetric matrices in form, but I don't know what the connection between them is and how to connect them. Any help would be great appreciated!","T_A {\rm O}(n)=\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}. \gamma(t) {\rm O}(n) \gamma(0)=A \gamma(t)\cdot\gamma(t)^T=I_n. t \gamma'(t)\cdot\gamma(t)^T+\gamma(t)\cdot(\gamma(t)^T)'=0, \gamma'(0)\cdot\gamma(0)^T=-\gamma(0)\cdot(\gamma(t)^T)'|_{t=0}=-\gamma(0)\cdot\gamma'(0)^T. \gamma(0)=A \gamma'(0) \gamma'(0)\cdot A^T=-A\cdot\gamma'(0)^T, \gamma'(0)\in T_A {\rm O}(n) T_A {\rm O}(n)\subset\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}. \dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-(BA^T)^T\}=\frac{n(n-1)}{2} A\in {\rm O}(n) \dim T_A {\rm O}(n)=\dim{\rm O}(n)=\frac{n(n-1)}{2} \frac{n(n-1)}{2}","['linear-algebra', 'matrices', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
11,A proof that the null linear mapping is the only one whose matrix representation does not depend on the basis [FALSE],A proof that the null linear mapping is the only one whose matrix representation does not depend on the basis [FALSE],,"I would like to show the fact that the linear mapping $$ L : E\to E $$ $$ x\mapsto0_{E} $$ is the unique linear mapping whose matrix representation does not depend on the choice of the basis. My attempt : Consider $\mathcal{V}$ and $\mathcal{\hat{V}}$ two basis of $E$ . First we show that $A$ , the matrix representation of $L$ , does not depend on the basis. To do so just consider the image of any vector of the basis $\mathcal{V}$ : $$ L(v_i) = 0 $$ clearly its coordinates in the basis $\mathcal{V}$ are all zeros. This gives us the null matrix of size $n\times n$ . Now consider the coordinate of $L(v_i)$ in the basis $\mathcal{\hat{V}}$ , by linear independance of the $\hat{v}_i$ 's its coordinates are also zeros, this yields the null matrix. An analogous reasoning with the base $\mathcal{\hat{V}}$ as starting point allows to conclude. Now to prove the uniqueness, consider $F$ a linear mapping which is not the null linear mapping and does not depend on the choice of the basis $\mathcal{V}$ and $\mathcal{\hat{V}}$ . So what happens if I consider the image $F(v_i)$ and $F(\hat{v_i})$ ? No reason to be the same at first since it is the coordinates of the image $F(v_i)$ and $F(\hat{v}_i)$ which should be the same no matter which base is chosen. But if it does not depend on the choice of the basis, I should have the same matrix wether I consider the basis $\mathcal{V}$ in the space of arrival or the basis $\mathcal{\hat{V}}$ , but if I do so and decide to take the same basis on the space of arrival, then if their coordinates coincide, they are the same vector. Thus, we should have $$ F(v_i) = F(\hat{v}_i),\quad\forall i \in\{1,...,n\} $$ which implies $$ F(v_i) - F(\hat{v}_i) = 0_{E}\implies F(v_i-\hat{v}_i) = 0_{E}\implies v_i = \hat{v}_i,\quad\forall i \in\{1,...,n\} $$ but the bases $\mathcal{V}$ and $\mathcal{\hat{V}}$ are not the same, which yields the contradiction. I would like to know if the proof is correct please. Thank you a lot ! EDIT This is false , thanks to Federico Fallucca for his answer. My mistake is at the last implication.","I would like to show the fact that the linear mapping is the unique linear mapping whose matrix representation does not depend on the choice of the basis. My attempt : Consider and two basis of . First we show that , the matrix representation of , does not depend on the basis. To do so just consider the image of any vector of the basis : clearly its coordinates in the basis are all zeros. This gives us the null matrix of size . Now consider the coordinate of in the basis , by linear independance of the 's its coordinates are also zeros, this yields the null matrix. An analogous reasoning with the base as starting point allows to conclude. Now to prove the uniqueness, consider a linear mapping which is not the null linear mapping and does not depend on the choice of the basis and . So what happens if I consider the image and ? No reason to be the same at first since it is the coordinates of the image and which should be the same no matter which base is chosen. But if it does not depend on the choice of the basis, I should have the same matrix wether I consider the basis in the space of arrival or the basis , but if I do so and decide to take the same basis on the space of arrival, then if their coordinates coincide, they are the same vector. Thus, we should have which implies but the bases and are not the same, which yields the contradiction. I would like to know if the proof is correct please. Thank you a lot ! EDIT This is false , thanks to Federico Fallucca for his answer. My mistake is at the last implication.","
L : E\to E
 
x\mapsto0_{E}
 \mathcal{V} \mathcal{\hat{V}} E A L \mathcal{V} 
L(v_i) = 0
 \mathcal{V} n\times n L(v_i) \mathcal{\hat{V}} \hat{v}_i \mathcal{\hat{V}} F \mathcal{V} \mathcal{\hat{V}} F(v_i) F(\hat{v_i}) F(v_i) F(\hat{v}_i) \mathcal{V} \mathcal{\hat{V}} 
F(v_i) = F(\hat{v}_i),\quad\forall i \in\{1,...,n\}
 
F(v_i) - F(\hat{v}_i) = 0_{E}\implies F(v_i-\hat{v}_i) = 0_{E}\implies v_i = \hat{v}_i,\quad\forall i \in\{1,...,n\}
 \mathcal{V} \mathcal{\hat{V}}","['linear-algebra', 'matrices', 'solution-verification', 'linear-transformations', 'hamel-basis']"
12,Fast way to check linear independence of matrix,Fast way to check linear independence of matrix,,"Say we suspect the columns of a matrix are independent and want to verify that fact quickly by hand.  What is the best way to do it? I'm currently studying MITx 18.033 where they recommend checking if the nullspace is $\{\mathbf 0\}$ by reducing into row echelon form, but how can I check quickly (ideally in my head for a small matrix such as below)? $$ \begin{pmatrix} 2 & 1 & 9 \\ 3 & 2 & 11 \\ -1 & -3 & 8 \\ 4 & 6 & -4 \end{pmatrix} $$ I suspect columns are independent iff any 3 $2 \times 2$ submatrices with distinct rows and distinct columns are independent.  This allows us to prove a matrix is independent by finding those submatrices: $$ \begin{pmatrix} 2 & 1 \\ 3 & 2 \end{pmatrix}, \  \begin{pmatrix} 2 & 11 \\ -3 & 8 \end{pmatrix}, \  \begin{pmatrix} 2 & 9 \\ -1 & 8 \end{pmatrix} $$ Is this correct?  If so, why?  If not, how do I prove a matrix's columns are independent quickly?","Say we suspect the columns of a matrix are independent and want to verify that fact quickly by hand.  What is the best way to do it? I'm currently studying MITx 18.033 where they recommend checking if the nullspace is by reducing into row echelon form, but how can I check quickly (ideally in my head for a small matrix such as below)? I suspect columns are independent iff any 3 submatrices with distinct rows and distinct columns are independent.  This allows us to prove a matrix is independent by finding those submatrices: Is this correct?  If so, why?  If not, how do I prove a matrix's columns are independent quickly?","\{\mathbf 0\} 
\begin{pmatrix}
2 & 1 & 9 \\
3 & 2 & 11 \\
-1 & -3 & 8 \\
4 & 6 & -4
\end{pmatrix}
 2 \times 2 
\begin{pmatrix}
2 & 1 \\
3 & 2
\end{pmatrix}, \ 
\begin{pmatrix}
2 & 11 \\
-3 & 8
\end{pmatrix}, \ 
\begin{pmatrix}
2 & 9 \\
-1 & 8
\end{pmatrix}
","['linear-algebra', 'matrices', 'linear-independence']"
13,"Solving a system of $2n-2$ equations, what is the relationship of solutions between successive $n$?","Solving a system of  equations, what is the relationship of solutions between successive ?",2n-2 n,"I am trying to solve a system of $2n-2$ equations.  The first two rows of the matrix representation are always $$\begin{pmatrix}-8&1&1&0&\ldots& 0\end{pmatrix} \text{and}\begin{pmatrix}-1&-7&2&1&0&\ldots& 0\end{pmatrix}.$$ The bottom rows are always $$\begin{pmatrix}0&\ldots& 0&1&2&-6&2\end{pmatrix} \text{and}\begin{pmatrix}0&\ldots& 0&1&2&-6\end{pmatrix}.$$ The rows inbetween are $\begin{pmatrix}1&2&-6&2&1&0&\ldots& 0\end{pmatrix}$ and shifting over one to the right as you go down.  In my particular problem $x_1$ is always $1$ so that’s why there’s only $2n-2$ variables. For example, for $n=4$ , the matrix equation is: $$ M_4\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0   \\   1 &-7&  2&  1 & 0 & 0   \\   1 & 2 &-6 & 2 & 1 & 0   \\   0 & 1 & 2 &-6 & 2 & 1   \\   0 & 0 & 1 & 2 &-6 & 2  \\   0 & 0 & 0 & 1 & 2 &-6 \\ \end{pmatrix}_{6\times6}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\end{pmatrix} $$ For $n=5$ $$ M_5\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\   1 &-7&  2&  1 & 0 & 0 & 0 & 0 \\   1 & 2 &-6 & 2 & 1 & 0 & 0 & 0 \\   0 & 1 & 2 &-6 & 2 & 1 & 0 & 0 \\   0 & 0 & 1 & 2 &-6 & 2 & 1 & 0\\   0 & 0 & 0 & 1 & 2 &-6 & 2 & 1\\   0 & 0 & 0 & 0 & 1 & 2 &-6 & 2\\   0& 0 & 0 & 0 & 0 & 1 & 2 &-6\\ \end{pmatrix}_{8\times 8}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\\x_9\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\\g\\h\end{pmatrix} $$ My particular problem is that I want to solve thousands (possibly millions) of such systems but it is computationally demanding.  You’ll notice that you can step down - the $6\times6$ is simply the $8\times8$ matrix with the last two rows and columns removed.  Similarly, you can step up by extending the previous matrix with two rows and columns. The core of my question is this: is there some relationship between the inverse of $M_n$ and $M_{n+1}$ that I can utilize to make solving more efficient? Specifically, I am interested in $x_n$ for each $n$ if it is more efficient to only calculate that.","I am trying to solve a system of equations.  The first two rows of the matrix representation are always The bottom rows are always The rows inbetween are and shifting over one to the right as you go down.  In my particular problem is always so that’s why there’s only variables. For example, for , the matrix equation is: For My particular problem is that I want to solve thousands (possibly millions) of such systems but it is computationally demanding.  You’ll notice that you can step down - the is simply the matrix with the last two rows and columns removed.  Similarly, you can step up by extending the previous matrix with two rows and columns. The core of my question is this: is there some relationship between the inverse of and that I can utilize to make solving more efficient? Specifically, I am interested in for each if it is more efficient to only calculate that.","2n-2 \begin{pmatrix}-8&1&1&0&\ldots& 0\end{pmatrix} \text{and}\begin{pmatrix}-1&-7&2&1&0&\ldots& 0\end{pmatrix}. \begin{pmatrix}0&\ldots& 0&1&2&-6&2\end{pmatrix} \text{and}\begin{pmatrix}0&\ldots& 0&1&2&-6\end{pmatrix}. \begin{pmatrix}1&2&-6&2&1&0&\ldots& 0\end{pmatrix} x_1 1 2n-2 n=4 
M_4\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0   \\
  1 &-7&  2&  1 & 0 & 0   \\
  1 & 2 &-6 & 2 & 1 & 0   \\
  0 & 1 & 2 &-6 & 2 & 1   \\
  0 & 0 & 1 & 2 &-6 & 2  \\
  0 & 0 & 0 & 1 & 2 &-6 \\
\end{pmatrix}_{6\times6}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\end{pmatrix}
 n=5 
M_5\bf{x}=\begin{pmatrix}-8 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
  1 &-7&  2&  1 & 0 & 0 & 0 & 0 \\
  1 & 2 &-6 & 2 & 1 & 0 & 0 & 0 \\
  0 & 1 & 2 &-6 & 2 & 1 & 0 & 0 \\
  0 & 0 & 1 & 2 &-6 & 2 & 1 & 0\\
  0 & 0 & 0 & 1 & 2 &-6 & 2 & 1\\
  0 & 0 & 0 & 0 & 1 & 2 &-6 & 2\\
  0& 0 & 0 & 0 & 0 & 1 & 2 &-6\\
\end{pmatrix}_{8\times 8}\begin{pmatrix}x_2\\x_3\\x_4\\x_5\\x_6\\x_7\\x_8\\x_9\end{pmatrix}=\begin{pmatrix}a\\b\\c\\d\\e\\f\\g\\h\end{pmatrix}
 6\times6 8\times8 M_n M_{n+1} x_n n","['linear-algebra', 'matrices', 'systems-of-equations']"
14,How to show two equivalent projection in a $C^*$ algebra are not homotopic,How to show two equivalent projection in a  algebra are not homotopic,C^*,"Show that two equivalent projections need not be homotopic. HINT: Let $P=\begin{pmatrix} 1&0\\0&0\end{pmatrix}$ and $Q=\begin{pmatrix}         t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t     \end{pmatrix}\in M_2(C[0,1])$ be two projections. Show they're contained in a common $C^*$ subalgebra of $M_2(C[0,1])$ in which they're equivalent but not  homotopic. My attempt: for $X=\begin{pmatrix}         \sqrt{t}&\sqrt{1-t}\\0&0     \end{pmatrix}$ we have that: $$XX^*=\begin{pmatrix}         \sqrt{t}&\sqrt{1-t}\\0&0     \end{pmatrix}\cdot \begin{pmatrix}         \sqrt{t}&0\\\sqrt{1-t}&0     \end{pmatrix}=\begin{pmatrix}         1&0\\0&0     \end{pmatrix}=P$$ and $$X^*X=\begin{pmatrix}         \sqrt{t}&0\\\sqrt{1-t}&0     \end{pmatrix}\cdot \begin{pmatrix}         \sqrt{t}&\sqrt{1-t}\\0&0     \end{pmatrix}=\begin{pmatrix}          t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t     \end{pmatrix}=Q $$ so $P,Q\in C^*(X)$ are equivalent. However I'm not sure how to show they're not homotopic as projections in $C^*(X)$ . Any help would be appreciated.",Show that two equivalent projections need not be homotopic. HINT: Let and be two projections. Show they're contained in a common subalgebra of in which they're equivalent but not  homotopic. My attempt: for we have that: and so are equivalent. However I'm not sure how to show they're not homotopic as projections in . Any help would be appreciated.,"P=\begin{pmatrix} 1&0\\0&0\end{pmatrix} Q=\begin{pmatrix}
        t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t
    \end{pmatrix}\in M_2(C[0,1]) C^* M_2(C[0,1]) X=\begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix} XX^*=\begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix}\cdot \begin{pmatrix}
        \sqrt{t}&0\\\sqrt{1-t}&0
    \end{pmatrix}=\begin{pmatrix}
        1&0\\0&0
    \end{pmatrix}=P X^*X=\begin{pmatrix}
        \sqrt{t}&0\\\sqrt{1-t}&0
    \end{pmatrix}\cdot \begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix}=\begin{pmatrix}
         t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t
    \end{pmatrix}=Q  P,Q\in C^*(X) C^*(X)","['matrices', 'homotopy-theory', 'equivalence-relations', 'c-star-algebras', 'projection-matrices']"
15,"If $A< C$ and $B<D$, is it true that $\|AB\|_F \leq \|CD\|_F$ where $A, B, C, D$ are symmetric positive definite matrices?","If  and , is it true that  where  are symmetric positive definite matrices?","A< C B<D \|AB\|_F \leq \|CD\|_F A, B, C, D","I have symmetric positive definite matrices $A, B, C, D$ with \begin{equation} A \leq C \quad\text{and}\quad B \leq D \end{equation} Is it true that $\|AB\|_F \leq \|CD\|_F$ where $\|\cdot\|_F$ denotes the Frobenius norm? The closest question I could find was: Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices? . Based on this we have $\|A\|_F \leq \|C\|_F$ and $\|B\|_F \leq \|D\|_F$ .",I have symmetric positive definite matrices with Is it true that where denotes the Frobenius norm? The closest question I could find was: Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices? . Based on this we have and .,"A, B, C, D \begin{equation}
A \leq C \quad\text{and}\quad
B \leq D
\end{equation} \|AB\|_F \leq \|CD\|_F \|\cdot\|_F \|A\|_F \leq \|C\|_F \|B\|_F \leq \|D\|_F","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
16,What is the prerequisite trigonometry knowledge required to understand 2D vector rotation?,What is the prerequisite trigonometry knowledge required to understand 2D vector rotation?,,"I am trying to understand the why of $\,2$ D vector rotation. I know that I can use the following matrix to rotate a vector: $$ \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \\ \end{bmatrix} $$ I do not understand why this works; I know that trigonometry is involved (where my level of knowledge is basically to fallback to ""SOH CAH TOA"" to remember which function to use). I can identify the first column as being similar to how a point on a circle is calculated i.e. $$r\!\cdot\!\cos\theta\,,\;r\!\cdot\!\sin\theta$$ But as for the second column I am lost. I have been reading up on trigonometric addition which seems like the area I need to focus on, but I have not been able to map the formula to the matrix form.","I am trying to understand the why of D vector rotation. I know that I can use the following matrix to rotate a vector: I do not understand why this works; I know that trigonometry is involved (where my level of knowledge is basically to fallback to ""SOH CAH TOA"" to remember which function to use). I can identify the first column as being similar to how a point on a circle is calculated i.e. But as for the second column I am lost. I have been reading up on trigonometric addition which seems like the area I need to focus on, but I have not been able to map the formula to the matrix form.","\,2 
\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta \\
\end{bmatrix}
 r\!\cdot\!\cos\theta\,,\;r\!\cdot\!\sin\theta","['matrices', 'trigonometry', 'linear-transformations']"
17,What class of matrices permutes matrix entries?,What class of matrices permutes matrix entries?,,"Let's start with the $2$ by $ 2$ case: We're given a matrix A $$\begin{pmatrix} a & b \\ c & d. \end{pmatrix}$$ What class of matrices ""rotates"" or ""permutes"" the entries upon left-multiplication, such that we obtain, for example, $$BA = \\ B\begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{pmatrix} b & c \\ d & a \end{pmatrix}? $$ As another example, let's consider a $3$ by $3$ \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \\ \end{pmatrix} . What matrix $B$ would permute these entries, such as, for instance, into \begin{pmatrix} d & e & a \\ c & h & i \\ f & g & b \\ \end{pmatrix} . Does there exist a general class of matrices that permutes the entries of an $n$ by $n$ matrix to any desired result?","Let's start with the by case: We're given a matrix A What class of matrices ""rotates"" or ""permutes"" the entries upon left-multiplication, such that we obtain, for example, As another example, let's consider a by . What matrix would permute these entries, such as, for instance, into . Does there exist a general class of matrices that permutes the entries of an by matrix to any desired result?","2  2 \begin{pmatrix}
a & b \\
c & d.
\end{pmatrix} BA = \\
B\begin{pmatrix}
a & b \\
c & d
\end{pmatrix} = \begin{pmatrix}
b & c \\
d & a
\end{pmatrix}?
 3 3 \begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i \\
\end{pmatrix} B \begin{pmatrix}
d & e & a \\
c & h & i \\
f & g & b \\
\end{pmatrix} n n",['matrices']
18,Is this a known matrix function?,Is this a known matrix function?,,"Consider a linear $n$ -dimensional system $\mathbf{y} = A\mathbf{x}$ , where the $n$ variables $x_i$ and the $n$ variables $y_i$ are related linearly by the matrix $A$ and $\det(A)\neq 0$ . For example let's say we have $2+2$ variables: $$ \begin{pmatrix} y_0\\ y_1 \end{pmatrix}  = \begin{pmatrix} A_{00} & A_{01}\\ A_{10} & A_{11} \end{pmatrix} \begin{pmatrix} x_0\\ x_1 \end{pmatrix} $$ If we want to write $\mathbf{x}$ as a linear function of $\mathbf{y}$ , we would use the inverse of $A$ , i.e. $\mathbf{x} = A^{-1}\mathbf{y}$ . So far all pretty standard. But also other partitions of the set of $2n$ variables in two halves can be related linearly, e.g. $$ \begin{pmatrix} x_0\\ y_1 \end{pmatrix} = f(A) \begin{pmatrix} x_1\\ y_0 \end{pmatrix} $$ could be well-defined. Is $f$ a known thing? For an $n$ -dimensional system, what is $f$ given two halves (if it exists)? EDIT: I've noticed using block inversion that the solution given by @Fishbane should be equivalent to $$ f(A) = (V_4-AV_2)^{-1}(AV_1-V_3) $$ where the $V_i$ blocks are defined in the answer. This is a matrix version of the Möbius transform.","Consider a linear -dimensional system , where the variables and the variables are related linearly by the matrix and . For example let's say we have variables: If we want to write as a linear function of , we would use the inverse of , i.e. . So far all pretty standard. But also other partitions of the set of variables in two halves can be related linearly, e.g. could be well-defined. Is a known thing? For an -dimensional system, what is given two halves (if it exists)? EDIT: I've noticed using block inversion that the solution given by @Fishbane should be equivalent to where the blocks are defined in the answer. This is a matrix version of the Möbius transform.","n \mathbf{y} = A\mathbf{x} n x_i n y_i A \det(A)\neq 0 2+2 
\begin{pmatrix}
y_0\\
y_1
\end{pmatrix} 
=
\begin{pmatrix}
A_{00} & A_{01}\\
A_{10} & A_{11}
\end{pmatrix}
\begin{pmatrix}
x_0\\
x_1
\end{pmatrix}
 \mathbf{x} \mathbf{y} A \mathbf{x} = A^{-1}\mathbf{y} 2n 
\begin{pmatrix}
x_0\\
y_1
\end{pmatrix} = f(A)
\begin{pmatrix}
x_1\\
y_0
\end{pmatrix}
 f n f 
f(A) = (V_4-AV_2)^{-1}(AV_1-V_3)
 V_i","['linear-algebra', 'matrices', 'matrix-equations']"
19,Conjugacy of p-subgroups in $GL_{5}(\mathbb{F}_{p})$?.,Conjugacy of p-subgroups in ?.,GL_{5}(\mathbb{F}_{p}),"Let $U_{5}$ denote the unitriangular group of $5\times 5$ upper triangular matrices with ones on the diagonal, over the finite field $\mathbb{F}_{p}$ . Let $H=\left. \left\{ A=\begin{pmatrix} 1 & 0 & 0 & a &d \\ 0 & 1 & 0 & b &e \\ 0 & 0 & 1 & c &f \\ 0 & 0 & 0 & 1 &0 \\ 0 & 0 & 0 & 0 &1% \end{pmatrix}% \right| a,b,c,d,e,f \in \mathbb{F}_{p}\right\}$ . and $K=\left. \left\{B= \begin{pmatrix} 1 & 0 & a' & b' &c' \\ 0 & 1 & d' & e' &f' \\ 0 & 0 & 1 & 0 &0 \\ 0 & 0 & 0 & 1 &0 \\ 0 & 0 & 0 & 0 &1% \end{pmatrix}% \right| a',b',c',d',e',f' \in \mathbb{F}_{p}\right\}$ be two subgroups of $GL_{5}(\mathbb{F}_{p})$ . The subgroups $H$ and $K$ are maximal abelian normal in $U_{5}$ (See for example Exercise $3$ p. $94$ of the Book {M. Suzuki, Group theory I}). Does the subgroups $H$ and $K$ conjugate in $GL_{5}(\mathbb{F}_{p})$ ?. I think the answer is No but I don't sure what to do about it. My try to this question: Let $V$ be a vector of $\mathbb{F}_{p}^{5}$ . H and K are not conjugate since $I(\mathbb{F}_{p}[H])V$ is a 3-dimensional vector space but $I(\mathbb{F}_{p}[K])V$ is just a 2-dimensional. Here, $I$ denotes the augmentation ideal. Could anyone please tell me if my try is correct or provide a defferent approche? Thank you in advance.","Let denote the unitriangular group of upper triangular matrices with ones on the diagonal, over the finite field . Let . and be two subgroups of . The subgroups and are maximal abelian normal in (See for example Exercise p. of the Book {M. Suzuki, Group theory I}). Does the subgroups and conjugate in ?. I think the answer is No but I don't sure what to do about it. My try to this question: Let be a vector of . H and K are not conjugate since is a 3-dimensional vector space but is just a 2-dimensional. Here, denotes the augmentation ideal. Could anyone please tell me if my try is correct or provide a defferent approche? Thank you in advance.","U_{5} 5\times 5 \mathbb{F}_{p} H=\left. \left\{
A=\begin{pmatrix}
1 & 0 & 0 & a &d \\
0 & 1 & 0 & b &e \\
0 & 0 & 1 & c &f \\
0 & 0 & 0 & 1 &0 \\
0 & 0 & 0 & 0 &1%
\end{pmatrix}%
\right| a,b,c,d,e,f \in \mathbb{F}_{p}\right\} K=\left. \left\{B=
\begin{pmatrix}
1 & 0 & a' & b' &c' \\
0 & 1 & d' & e' &f' \\
0 & 0 & 1 & 0 &0 \\
0 & 0 & 0 & 1 &0 \\
0 & 0 & 0 & 0 &1%
\end{pmatrix}%
\right| a',b',c',d',e',f' \in \mathbb{F}_{p}\right\} GL_{5}(\mathbb{F}_{p}) H K U_{5} 3 94 H K GL_{5}(\mathbb{F}_{p}) V \mathbb{F}_{p}^{5} I(\mathbb{F}_{p}[H])V I(\mathbb{F}_{p}[K])V I","['matrices', 'group-theory', 'finite-groups', 'similar-matrices']"
20,Solving recurrence relation $a_n = a_{n-1} - a_{n-2}$,Solving recurrence relation,a_n = a_{n-1} - a_{n-2},"I am given a sequence of determinants of matrices $M_n$ , where the matrix elements $(M_n)_{ij}$ of $M_n$ are $0$ whenever $|i-j|>1$ and $1$ whenever $|i-j| ≤ 1$ . Writing out the first five matrices, it becomes apparent that $\det(M_n) = \det(M_{n-1}) - \det(M_{n-2})$ . I want a formula for the mapping $n ↦ \det(M_n)$ , which I believe to be $$a_n = \begin{cases} 0, & n ≡ 2 \mod 6 \,\, \vee n ≡ 5 \mod 6, \\ 1, & n ≡ 0 \mod 6 \,\, \vee n ≡ 1 \mod 6, \\ -1, & n ≡ 3 \mod 6 \,\, \vee n ≡ 4 \mod 6. \end{cases} $$ This can quite readily be seen from the first 15 or so terms. Of course, this doesn't constitute a proof , which most likely will have to be performed by induction. I just fear that I am to embark on a six-piece proof by exhaustion, which I would like to avoid if there is a (much) quicker way to do it!","I am given a sequence of determinants of matrices , where the matrix elements of are whenever and whenever . Writing out the first five matrices, it becomes apparent that . I want a formula for the mapping , which I believe to be This can quite readily be seen from the first 15 or so terms. Of course, this doesn't constitute a proof , which most likely will have to be performed by induction. I just fear that I am to embark on a six-piece proof by exhaustion, which I would like to avoid if there is a (much) quicker way to do it!","M_n (M_n)_{ij} M_n 0 |i-j|>1 1 |i-j| ≤ 1 \det(M_n) = \det(M_{n-1}) - \det(M_{n-2}) n ↦ \det(M_n) a_n = \begin{cases} 0, & n ≡ 2 \mod 6 \,\, \vee n ≡ 5 \mod 6, \\ 1, & n ≡ 0 \mod 6 \,\, \vee n ≡ 1 \mod 6, \\ -1, & n ≡ 3 \mod 6 \,\, \vee n ≡ 4 \mod 6. \end{cases}
","['linear-algebra', 'matrices', 'recurrence-relations', 'determinant']"
21,What does $\mbox{diag}(A)$ denote?,What does  denote?,\mbox{diag}(A),"Let $A$ be a $2 \times 2$ matrix. What does $\mbox{diag}(A)$ denote? It can't refer to a block-diagonal matrix, so does it basically mean $A$ with anything but the diagonal set to $0$ ?","Let be a matrix. What does denote? It can't refer to a block-diagonal matrix, so does it basically mean with anything but the diagonal set to ?",A 2 \times 2 \mbox{diag}(A) A 0,"['linear-algebra', 'matrices', 'notation']"
22,"For all $ m$ and $ n,$ there exists an $n \times m$ matrix $ A$ such that $ A^TA=\frac{n}{m}\,I$ and every element of the diagonal of $ AA^T$ is $ 1$?",For all  and  there exists an  matrix  such that  and every element of the diagonal of  is ?," m  n, n \times m  A  A^TA=\frac{n}{m}\,I  AA^T  1","The following speculation comes from this thread , which was inspired from a Putnam competition problem. For all $ m$ and $ n,$ there exists an $n \times m$ matrix $ A$ such that $ A^TA=\frac{n}{m}\,I_m$ and every element of the diagonal of $ AA^T$ is $ 1.$ I personally care more about the cases when $m<n$ . I have tried a couple of cases for small $n$ 's and tried using induction to prove it in general but was not successful. Solving matrix equations is very foreign to me. Update: I added the tag algebraic geometry. If anyone can show the existence of solution (essentially polynomial equations) with any advanced theory of algebraic geometry, I am also happy to know, although I prefer to see elementary solutions.","The following speculation comes from this thread , which was inspired from a Putnam competition problem. For all and there exists an matrix such that and every element of the diagonal of is I personally care more about the cases when . I have tried a couple of cases for small 's and tried using induction to prove it in general but was not successful. Solving matrix equations is very foreign to me. Update: I added the tag algebraic geometry. If anyone can show the existence of solution (essentially polynomial equations) with any advanced theory of algebraic geometry, I am also happy to know, although I prefer to see elementary solutions."," m  n, n \times m  A  A^TA=\frac{n}{m}\,I_m  AA^T  1. m<n n","['linear-algebra', 'matrices', 'algebraic-geometry']"
23,Counterexample for a convex problem,Counterexample for a convex problem,,"The convex optimization problem is as follows: \begin{align}     \underset{\mathbb{X},\mathbb{Y}\in\mathbb{S}_n^+}{\min}\quad &\operatorname{Tr}(X)+ \operatorname{Tr}\left(D Y \right)\nonumber\\ \text{s.t.}\;\; &AX+XA^T+BB^T\geq 0 \nonumber\\  &\begin{bmatrix} YA+A^\top Y -\gamma I & YB \\ B^\top Y & -I\end{bmatrix} \preceq 0\nonumber\\ &\begin{bmatrix} X&I\\I& Y \end{bmatrix}\geq 0\nonumber \end{align} I feel at optimality $XY$ might not be equal to I. Any counterexamples",The convex optimization problem is as follows: I feel at optimality might not be equal to I. Any counterexamples,"\begin{align}
    \underset{\mathbb{X},\mathbb{Y}\in\mathbb{S}_n^+}{\min}\quad &\operatorname{Tr}(X)+ \operatorname{Tr}\left(D Y \right)\nonumber\\
\text{s.t.}\;\; &AX+XA^T+BB^T\geq 0 \nonumber\\
 &\begin{bmatrix} YA+A^\top Y -\gamma I & YB \\ B^\top Y & -I\end{bmatrix} \preceq 0\nonumber\\
&\begin{bmatrix}
X&I\\I& Y
\end{bmatrix}\geq 0\nonumber
\end{align} XY","['matrices', 'optimization', 'convex-optimization', 'matrix-equations', 'control-theory']"
24,Is there general formula for the inverse of this matrices?,Is there general formula for the inverse of this matrices?,,For some natural number $\alpha>2$ we set $\zeta=e^{\frac{2i\pi}{\alpha}}$ . Let's consider $\alpha$ -by- $\alpha$ matrix and choose its entries to be $\zeta^{(i-1)(j-1)}$ in the $i$ -th row and $j$ -th column. $$\begin{bmatrix} \zeta^{0} & \zeta^{0} & \zeta^{0} & \dots &\zeta^{0}\\ \zeta^{0} & \zeta^{1} & \zeta^{2} & \dots &\zeta^{\alpha-1}\\ \zeta^{0} & \zeta^{2} & \zeta^{4} & \dots &\zeta^{2(\alpha-1)}\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \zeta^{0} & \zeta^{(\alpha-1)} & \zeta^{2(\alpha-1)} & \dots &\zeta^{(\alpha-1)^2} \end{bmatrix}$$ I would like to find the inverse of this matrix for given $\alpha$ . However I have very little experience with matrices and I don't even know if there is a chance of finding a solution to this problem. The only thing I noticed is that the inverse matrix will always have $\frac{1}{\alpha}$ in first row and first column since $\sum_{m=0}^{\alpha-1}\zeta^{nm}=0$ for $n \in \mathbb{N}_+$ I would be very thankful for any help.,For some natural number we set . Let's consider -by- matrix and choose its entries to be in the -th row and -th column. I would like to find the inverse of this matrix for given . However I have very little experience with matrices and I don't even know if there is a chance of finding a solution to this problem. The only thing I noticed is that the inverse matrix will always have in first row and first column since for I would be very thankful for any help.,"\alpha>2 \zeta=e^{\frac{2i\pi}{\alpha}} \alpha \alpha \zeta^{(i-1)(j-1)} i j \begin{bmatrix}
\zeta^{0} & \zeta^{0} & \zeta^{0} & \dots &\zeta^{0}\\
\zeta^{0} & \zeta^{1} & \zeta^{2} & \dots &\zeta^{\alpha-1}\\
\zeta^{0} & \zeta^{2} & \zeta^{4} & \dots &\zeta^{2(\alpha-1)}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\zeta^{0} & \zeta^{(\alpha-1)} & \zeta^{2(\alpha-1)} & \dots &\zeta^{(\alpha-1)^2}
\end{bmatrix} \alpha \frac{1}{\alpha} \sum_{m=0}^{\alpha-1}\zeta^{nm}=0 n \in \mathbb{N}_+","['linear-algebra', 'matrices', 'complex-numbers']"
25,How to find the coefficients of the second eigenvector?,How to find the coefficients of the second eigenvector?,,"I have a $2\times 2$ real symmetric matrix: $$\begin{pmatrix} A & C \\ C & B  \end{pmatrix} $$ and I know that the eigenvalues are: $$\lambda_{\pm} = \frac{1}{2}(A+B)\pm \frac{1}{2}\sqrt{(A-B)^{2}+C^{2}}$$ Define $x$ so that: $\lambda_{\pm} = e^{\pm x}$ . I know its first eigenvector, associated to $\lambda_{+}$ . It is given by: $$v = \cos \theta v_{1}+ \sin\theta v_{2}$$ where $v_{1}$ and $v_{2}$ form a basis for this two-dimensional space and the constant $\theta$ is defined by: $$\tan\theta = \frac{C}{e^{x}-A}.$$ Here is my problem. If I want to find an (orthogonal) eigenvector of the matrix associated to $\lambda_{-}$ , I would simply say it is given by: $$w = \cos \Delta v_{1} + \sin\Delta v_{2}$$ where now: $$\tan\Delta = \frac{C}{e^{-x}-A}$$ However, the result is supposed to be: $$w = -\sin\theta v_{1} + \cos\theta v_{2}.$$ Why is that? I am trying to prove that the change $x \to -x$ implies $\cos\theta \to -\sin\theta$ , but I really cannot prove it. Any help is useful!","I have a real symmetric matrix: and I know that the eigenvalues are: Define so that: . I know its first eigenvector, associated to . It is given by: where and form a basis for this two-dimensional space and the constant is defined by: Here is my problem. If I want to find an (orthogonal) eigenvector of the matrix associated to , I would simply say it is given by: where now: However, the result is supposed to be: Why is that? I am trying to prove that the change implies , but I really cannot prove it. Any help is useful!","2\times 2 \begin{pmatrix}
A & C \\
C & B 
\end{pmatrix}
 \lambda_{\pm} = \frac{1}{2}(A+B)\pm \frac{1}{2}\sqrt{(A-B)^{2}+C^{2}} x \lambda_{\pm} = e^{\pm x} \lambda_{+} v = \cos \theta v_{1}+ \sin\theta v_{2} v_{1} v_{2} \theta \tan\theta = \frac{C}{e^{x}-A}. \lambda_{-} w = \cos \Delta v_{1} + \sin\Delta v_{2} \tan\Delta = \frac{C}{e^{-x}-A} w = -\sin\theta v_{1} + \cos\theta v_{2}. x \to -x \cos\theta \to -\sin\theta","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
26,For what $\beta$ does the geometric series $I + A + A^2 + A^3 + \cdots$ converge to $(I-A)^{-1}$,For what  does the geometric series  converge to,\beta I + A + A^2 + A^3 + \cdots (I-A)^{-1},"Problem. Consider the matrix $A$ below. $$A = \begin{bmatrix} {\beta\over 2} & 0\\ {\beta\over 2} & \beta  \end{bmatrix}$$ For what $\beta$ does the sequence $I + A + A^n + \cdots$ converge to $(I-A)^{-1}$ ? How quickly does the sequence converge, as a function of $\beta$ ? As we're only in the second week of an undergraduate course and haven't yet covered them, I'm not able to make use of matrix norms (in which case clearly it must be that $lim_{n\to\infty} |A_n| = lim_{n\to\infty} A_n = 0$ , yielding $0<\beta<2$ by examination of the general form of $A^n$ ), and I'm not quite sure how else to approach this. Here's what I have so far. Notice first that the general form of $A^n$ seems to be given by: $$A^n = \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix}$$ Notice also that $(I-A^n)^{-1}$ is then given by: $$\Bigg(\begin{bmatrix}1 & 0\\0 & 1\end{bmatrix} - \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix}\Bigg)^{-1} = \begin{bmatrix}1 + {\beta^n\over 2^n} & 0 \\ ({\beta\over 2})^n(2^n-1) & \beta^n+1\end{bmatrix}^{-1} = \begin{bmatrix} {2^n\over {2^\beta +\beta^n}} & 0\\ {{\beta^n(1-2^n)}\over (2^n\beta^n)(\beta^n+1)} & {1\over \beta^n +1}\end{bmatrix}$$ Now, if we look at the sum $A + A^2 + A^3 + \ldots$ , it looks like: $$I+A = \begin{bmatrix} {\beta+2\over 2} & 0\\ {\beta\over 2} & \beta+1 \end{bmatrix}\;\;, \;\; I+A+A^2 =  \begin{bmatrix} {\beta^2 + 2\beta +4\over 4} & 0\\ {\beta(3\beta+2)\over 4} & \beta^2+\beta+1\\ \end{bmatrix}$$ It isn't clear to me that this series is approaching $(I-A^n)^{-1}$ , nor how one would determine for what $\beta$ this holds. Moreover, it isn't clear what is meant by ""how quickly"" the series converges, nor how one would determine that as a function of $\beta$ . Where do I go from here?","Problem. Consider the matrix below. For what does the sequence converge to ? How quickly does the sequence converge, as a function of ? As we're only in the second week of an undergraduate course and haven't yet covered them, I'm not able to make use of matrix norms (in which case clearly it must be that , yielding by examination of the general form of ), and I'm not quite sure how else to approach this. Here's what I have so far. Notice first that the general form of seems to be given by: Notice also that is then given by: Now, if we look at the sum , it looks like: It isn't clear to me that this series is approaching , nor how one would determine for what this holds. Moreover, it isn't clear what is meant by ""how quickly"" the series converges, nor how one would determine that as a function of . Where do I go from here?","A A = \begin{bmatrix} {\beta\over 2} & 0\\ {\beta\over 2} & \beta
 \end{bmatrix} \beta I + A + A^n + \cdots (I-A)^{-1} \beta lim_{n\to\infty} |A_n| = lim_{n\to\infty} A_n = 0 0<\beta<2 A^n A^n A^n = \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix} (I-A^n)^{-1} \Bigg(\begin{bmatrix}1 & 0\\0 & 1\end{bmatrix} - \begin{bmatrix}{\beta^n\over 2^n} &0\\ {\beta^n \over 2^n}(2^n-1)& \beta^n\end{bmatrix}\Bigg)^{-1} = \begin{bmatrix}1 + {\beta^n\over 2^n} & 0 \\ ({\beta\over 2})^n(2^n-1) & \beta^n+1\end{bmatrix}^{-1} = \begin{bmatrix} {2^n\over {2^\beta +\beta^n}} & 0\\ {{\beta^n(1-2^n)}\over (2^n\beta^n)(\beta^n+1)} & {1\over \beta^n +1}\end{bmatrix} A + A^2 + A^3 + \ldots I+A = \begin{bmatrix}
{\beta+2\over 2} & 0\\
{\beta\over 2} & \beta+1
\end{bmatrix}\;\;, \;\; I+A+A^2 = 
\begin{bmatrix}
{\beta^2 + 2\beta +4\over 4} & 0\\
{\beta(3\beta+2)\over 4} & \beta^2+\beta+1\\
\end{bmatrix} (I-A^n)^{-1} \beta \beta","['linear-algebra', 'sequences-and-series', 'matrices', 'matrix-equations']"
27,Find eigenvalues of linear operator $X \mapsto AX^TA$.,Find eigenvalues of linear operator .,X \mapsto AX^TA,"Matrix $A$ has $n$ distinct non-zero eigenvalues, $\lambda_1, \dots, \lambda_n$ . Find the eigenvalues and eigenvectors of the linear operator $$ L : X \mapsto AX^{T}A $$ I tried to use a decomposition: $A = P\Lambda P^{-1}$ , where $\Lambda$ is diagonal matrix with $\lambda_1 \dots \lambda_n$ on its diagonal. Then, I wrote equation $L : (PXP^{-1})^{T} \rightarrow P\Lambda X\Lambda P^{-1} = \lambda (PXP^{-1})^{T}$ . So, I've got $C\Lambda X\Lambda C^{-1} = \lambda X^{T}$ , where $C= P^{T}P$ and stuck there. Edited: $X, A \in \mathbb C^{n \times n}$","Matrix has distinct non-zero eigenvalues, . Find the eigenvalues and eigenvectors of the linear operator I tried to use a decomposition: , where is diagonal matrix with on its diagonal. Then, I wrote equation . So, I've got , where and stuck there. Edited:","A n \lambda_1, \dots, \lambda_n  L : X \mapsto AX^{T}A  A = P\Lambda P^{-1} \Lambda \lambda_1 \dots \lambda_n L : (PXP^{-1})^{T} \rightarrow P\Lambda X\Lambda P^{-1} = \lambda (PXP^{-1})^{T} C\Lambda X\Lambda C^{-1} = \lambda X^{T} C= P^{T}P X, A \in \mathbb C^{n \times n}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
28,Is function $X \mapsto \mbox{Tr}\left(X X^T X X^T\right)$ convex?,Is function  convex?,X \mapsto \mbox{Tr}\left(X X^T X X^T\right),"Is $\mbox{Tr}\left(X X^T X X^T\right)$ a convex function of arbitrary real matrix $X$ ? More generally, is $\mbox{Tr}\left(\left(X X^{\dagger}\right)^m\right)$ a convex function of arbitrary complex matrix $X$ for any integer $m \ge 1$ ? Any advice or suggestions would be greatly appreciated. The proof hint: Let us apply SVD to a matrix $X$ : $X$ = $U D V^{\dagger}$ . Every matrix has SVD with non-negative singular values on the main diagonal of $D$ . Next: $\left(X X^{\dagger}\right)^m$ = $U D V^{\dagger} V D U^{\dagger} U D V^{\dagger} V D U^{\dagger} \ldots U D V^{\dagger} V D U^{\dagger}$ = $U D^{2 m} U^{\dagger}$ . Note, all unitary matrices are cancelled in between, because $U^{\dagger}U = V^{\dagger}V = I$ . $\mbox{Tr} \left(\left(X X^{\dagger}\right)^m\right)$ = $\mbox{Tr} \left(U D^{2 m} U^{\dagger}\right)$ = $\mbox{Tr} \left(D^{2 m} U^{\dagger} U\right)$ = $\mbox{Tr} \left(D^{2 m}\right)$ = $\sum_i \sigma_i^{2 m}$ , where we used the cyclic property of trace operation, and $\{\sigma_i\}$ are the singular values of matrix $X$ . Let $\{x_i\}$ , $\{y_i\}$ and $\{z_i\}$ be the singular values of arbitrary matrices $X$ , $Y$ and their convex combination $Z$ = $\alpha X + (1 - \alpha) Y$ respectively. It was shown below by @PSL that for the Frobenius norm ( $m = 1$ ) the following holds true: $\alpha \sum_i x_i^2 + (1 - \alpha) \sum_i y_i^2 \ge \sum_i z_i^2$ . Considering that the function $\phi: x \rightarrow x^m, x \in R^+$ is convex, would it be possible to show that the case $m > 1$ is also satisfied: $\alpha \sum_i x_i^{2 m} + (1 - \alpha) \sum_i y_i^{2 m} \ge \sum_i z_i^{2 m}$ ? Update : by numerical simulation I found that $\sum_i x_i^{2} \ge \sum_i z_i^{2}$ does not necessarily entails $\sum_i x_i^{4} \ge \sum_i z_i^{4}$ on roughly 9% of random configurations. Seems like this line of thoughts does not work. However, the extended brute force simulation still succeeds for $m$ 2 to 5. Brute force approach to answer the questions. Here I literally check convexity on random matrices. The Python code speaks for itself: import numpy as np  tol = 10.0 * np.finfo(float).eps count_ok, count_fail = int(0), int(0)  for m in range(2, 5 + 1):     print(""m:"", m)     for dim in range(2, 10 + 1):         print(f""matrix size: {dim}x{dim}"")         for test in range(100000):             X = 2 * np.random.rand(dim, dim) - 1             Y = 2 * np.random.rand(dim, dim) - 1             XXt = X @ X.T             YYt = Y @ Y.T             for t in np.linspace(0.01, 0.99, 20):                 Z = X * t + Y * (1 - t)                 ZZt = Z @ Z.T                 ok = (np.trace(np.linalg.matrix_power(ZZt, m)) <=                       np.trace(np.linalg.matrix_power(XXt, m)) * t +                       np.trace(np.linalg.matrix_power(YYt, m)) * (1 - t) + tol)                 if ok:                     count_ok += 1                 else:                     count_fail += 1  print(f""succeeded: {count_ok} times"") print(f""failed: {count_fail} times"") print("""") ....... succeeded: 72,000,000 times failed: 0 times","Is a convex function of arbitrary real matrix ? More generally, is a convex function of arbitrary complex matrix for any integer ? Any advice or suggestions would be greatly appreciated. The proof hint: Let us apply SVD to a matrix : = . Every matrix has SVD with non-negative singular values on the main diagonal of . Next: = = . Note, all unitary matrices are cancelled in between, because . = = = = , where we used the cyclic property of trace operation, and are the singular values of matrix . Let , and be the singular values of arbitrary matrices , and their convex combination = respectively. It was shown below by @PSL that for the Frobenius norm ( ) the following holds true: . Considering that the function is convex, would it be possible to show that the case is also satisfied: ? Update : by numerical simulation I found that does not necessarily entails on roughly 9% of random configurations. Seems like this line of thoughts does not work. However, the extended brute force simulation still succeeds for 2 to 5. Brute force approach to answer the questions. Here I literally check convexity on random matrices. The Python code speaks for itself: import numpy as np  tol = 10.0 * np.finfo(float).eps count_ok, count_fail = int(0), int(0)  for m in range(2, 5 + 1):     print(""m:"", m)     for dim in range(2, 10 + 1):         print(f""matrix size: {dim}x{dim}"")         for test in range(100000):             X = 2 * np.random.rand(dim, dim) - 1             Y = 2 * np.random.rand(dim, dim) - 1             XXt = X @ X.T             YYt = Y @ Y.T             for t in np.linspace(0.01, 0.99, 20):                 Z = X * t + Y * (1 - t)                 ZZt = Z @ Z.T                 ok = (np.trace(np.linalg.matrix_power(ZZt, m)) <=                       np.trace(np.linalg.matrix_power(XXt, m)) * t +                       np.trace(np.linalg.matrix_power(YYt, m)) * (1 - t) + tol)                 if ok:                     count_ok += 1                 else:                     count_fail += 1  print(f""succeeded: {count_ok} times"") print(f""failed: {count_fail} times"") print("""") ....... succeeded: 72,000,000 times failed: 0 times","\mbox{Tr}\left(X X^T X X^T\right) X \mbox{Tr}\left(\left(X X^{\dagger}\right)^m\right) X m \ge 1 X X U D V^{\dagger} D \left(X X^{\dagger}\right)^m U D V^{\dagger} V D U^{\dagger} U D V^{\dagger} V D U^{\dagger} \ldots U D V^{\dagger} V D U^{\dagger} U D^{2 m} U^{\dagger} U^{\dagger}U = V^{\dagger}V = I \mbox{Tr} \left(\left(X X^{\dagger}\right)^m\right) \mbox{Tr} \left(U D^{2 m} U^{\dagger}\right) \mbox{Tr} \left(D^{2 m} U^{\dagger} U\right) \mbox{Tr} \left(D^{2 m}\right) \sum_i \sigma_i^{2 m} \{\sigma_i\} X \{x_i\} \{y_i\} \{z_i\} X Y Z \alpha X + (1 - \alpha) Y m = 1 \alpha \sum_i x_i^2 + (1 - \alpha) \sum_i y_i^2 \ge \sum_i z_i^2 \phi: x \rightarrow x^m, x \in R^+ m > 1 \alpha \sum_i x_i^{2 m} + (1 - \alpha) \sum_i y_i^{2 m} \ge \sum_i z_i^{2 m} \sum_i x_i^{2} \ge \sum_i z_i^{2} \sum_i x_i^{4} \ge \sum_i z_i^{4} m","['matrices', 'convex-analysis', 'trace']"
29,Do nested commutators show up when commuting more than two matrices?,Do nested commutators show up when commuting more than two matrices?,,"If I want to commute two matrices $A$ and $B$ , I obtain their commutator as an error term: $$AB = BA + [A,B].$$ I would think that, to generalize this to $n$ matrices, I would obtain nested commutators in the error term, but I cannot seem to work this out (if it is indeed true). For example, for $3$ matrices: $$ABC = BCA + [A, BC],$$ which was obtained by commuting $A$ with $BC$ . Alternatively: $$ABC = (BA + [A,B])C = BAC + [A,B]C = B(CA + [A, C]) + [A,B]C = BCA + B[A,C] + [A,B]C,$$ which was obtained by commuting $A$ with $B$ and then $A$ with $C$ . Clearly the two end results above are equal. However, after trying to do some algebra, I cannot seem to write this as a nested commutator. My questions are: Is there a way to write $$A X_1 X_2 \cdots X_{n-1} = X_1 X_2 \cdots X_{n-1} A + E,$$ where $E$ contains a term with $n-1$ nested commutators containing all of the matrices $A$ , $X_i$ ? If the above is not possible, is it possible when all the $X_i$ commute?","If I want to commute two matrices and , I obtain their commutator as an error term: I would think that, to generalize this to matrices, I would obtain nested commutators in the error term, but I cannot seem to work this out (if it is indeed true). For example, for matrices: which was obtained by commuting with . Alternatively: which was obtained by commuting with and then with . Clearly the two end results above are equal. However, after trying to do some algebra, I cannot seem to write this as a nested commutator. My questions are: Is there a way to write where contains a term with nested commutators containing all of the matrices , ? If the above is not possible, is it possible when all the commute?","A B AB = BA + [A,B]. n 3 ABC = BCA + [A, BC], A BC ABC = (BA + [A,B])C = BAC + [A,B]C = B(CA + [A, C]) + [A,B]C = BCA + B[A,C] + [A,B]C, A B A C A X_1 X_2 \cdots X_{n-1} = X_1 X_2 \cdots X_{n-1} A + E, E n-1 A X_i X_i","['linear-algebra', 'abstract-algebra', 'matrices', 'lie-algebras']"
30,Matrix Equality - Bellman,Matrix Equality - Bellman,,"Question : Use the relation $|AB|=|A||B|$ to show that $$(a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2.$$ Attempt : It's easy to expand out the two sides to verify they are equivalent, but I don't understand how the hint comes into play here? Bellman denoted matrices by capital letters in the exposition, but I would assume here that we'd want $A$ and $B$ to be vectors in $\mathbb{R}^2$ ? Any help appreciated - thanks. Source : Bellman - ""Introduction to Matrix Analysis""","Question : Use the relation to show that Attempt : It's easy to expand out the two sides to verify they are equivalent, but I don't understand how the hint comes into play here? Bellman denoted matrices by capital letters in the exposition, but I would assume here that we'd want and to be vectors in ? Any help appreciated - thanks. Source : Bellman - ""Introduction to Matrix Analysis""",|AB|=|A||B| (a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2. A B \mathbb{R}^2,['linear-algebra']
31,"If $A$ and $B$ are orthogonal projection matrices, how can I show that trace$(AB) \le $ rank$(AB)$?","If  and  are orthogonal projection matrices, how can I show that trace rank?",A B (AB) \le  (AB),"If $A$ and $B$ are orthogonal projection matrices, how can I show that trace $(AB) \le $ rank $(AB)$ ? I was using C-S inequality to get tr $(AB) \le \sqrt{tr(A^2)tr(B^2)}$ and I know that $tr(A^2)=$ rank $(A)$ . But I can't get the rank of $AB$ .","If and are orthogonal projection matrices, how can I show that trace rank ? I was using C-S inequality to get tr and I know that rank . But I can't get the rank of .",A B (AB) \le  (AB) (AB) \le \sqrt{tr(A^2)tr(B^2)} tr(A^2)= (A) AB,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank', 'trace']"
32,Lyapunov equation with semidefinite right-hand side,Lyapunov equation with semidefinite right-hand side,,Consider the Lyapunov equation $$A^TX+XA=-Q$$ with Hurwitz matrix $A$ and positive semi-definite matrix $Q\succeq0$ . When is its solution $X$ strictly positive definite?,Consider the Lyapunov equation with Hurwitz matrix and positive semi-definite matrix . When is its solution strictly positive definite?,A^TX+XA=-Q A Q\succeq0 X,"['linear-algebra', 'matrices', 'matrix-equations', 'linear-control', 'lyapunov-functions']"
33,Matrix with only positive entries whose inverse has only positive entries,Matrix with only positive entries whose inverse has only positive entries,,I'm looking for a class of matrices such that if it contains a matrix with only positive entries then the inverse of said matrix also has only positive entries. I imagine an example of such a class would be the class of orthogonal matrices where the inverse is the transpose but i'm looking for a more general class if possible.,I'm looking for a class of matrices such that if it contains a matrix with only positive entries then the inverse of said matrix also has only positive entries. I imagine an example of such a class would be the class of orthogonal matrices where the inverse is the transpose but i'm looking for a more general class if possible.,,['matrices']
34,"Given matrix $X$, how to find elementary matrices $E_1$, $E_2$ and $E_3$ such that $X = E_1 E_2 E_3$?","Given matrix , how to find elementary matrices ,  and  such that ?",X E_1 E_2 E_3 X = E_1 E_2 E_3,"Given $$X = \begin{bmatrix} 0 & 1\\ -2 & -18\end{bmatrix}$$ find elementary matrices $E_1$ , $E_2$ and $E_3$ such that $X = E_1 E_2 E_3$ . My attempt I did 3 row operations from $X$ to get to $I_2$ Swapping row 1 and row 2 Row 1 becomes $-\frac12$ of row 1 Row 1 becomes Row 1 - 9 Row 2 So then $$E_1 = \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}, \qquad  E_2 = \begin{bmatrix} -1/2 & 0\\ 0 & 1 \end{bmatrix}, \qquad  E_3 = \begin{bmatrix} 1 & -9\\ 0 & 1 \end{bmatrix}$$ However, when I multiply the $E_1$ , $E_2$ and $E_3$ it doesn't give $X$ . Can someone please tell me where I have made a mistake or if I've approached this question incorrectly?","Given find elementary matrices , and such that . My attempt I did 3 row operations from to get to Swapping row 1 and row 2 Row 1 becomes of row 1 Row 1 becomes Row 1 - 9 Row 2 So then However, when I multiply the , and it doesn't give . Can someone please tell me where I have made a mistake or if I've approached this question incorrectly?","X = \begin{bmatrix} 0 & 1\\ -2 & -18\end{bmatrix} E_1 E_2 E_3 X = E_1 E_2 E_3 X I_2 -\frac12 E_1 = \begin{bmatrix}
0 & 1\\
1 & 0
\end{bmatrix}, \qquad 
E_2 = \begin{bmatrix}
-1/2 & 0\\
0 & 1
\end{bmatrix}, \qquad 
E_3 = \begin{bmatrix}
1 & -9\\
0 & 1
\end{bmatrix} E_1 E_2 E_3 X","['linear-algebra', 'matrices', 'gaussian-elimination']"
35,Question about projection onto subspaces,Question about projection onto subspaces,,"Suppose I have an $n\times n$ (complex) matrix $Q$ viewed as a linear transformation from $\mathbb{C}^{n}$ to $\mathbb{C}^{n}$ . I'm struggling to understand the difference between the following two scenarios. For simplicity, let us consider $n = 4$ . Scenario 1: Let $S$ be a subspace of $\mathbb{C}^{4}$ of dimension $d=2$ . Let us think of $S$ as the subspace generated by two eigenvectors $v_{1}$ and $v_{2}$ of $Q$ .One can construct a projection matrix $P_{S}$ . According to this post , this projection should be given by: $$P_{S} = A(A^{T}A)^{-1}A^{T}$$ where $A = [v_{1} \hspace{0.1cm} v_{2}]$ the a $4\times 2$ matrix. This is a $4\times 4$ matrix . Scenario 2: Let us write $\mathbb{C}^{4} = S \oplus S^{\perp}$ . Since $Q$ is a linear operator on $\mathbb{C}^{4}$ , it might be possible to decompose $Q = Q_{1}\oplus Q_{2}$ so that, if $u = u_{1}+u_{2}$ and $u_{1}\in S$ and $u_{2} \in S^{\perp}$ : $$Qu = Q_{1}u_{1}\oplus Q_{2}u_{2}$$ Each $Q_{i}$ should be a $2\times 2$ matrix. Here is my problem. I would expect that $Q_{1} = QP_{S}$ , since $Q_{1}$ should be the restriction of $Q$ to $S$ . However, as I stressed before, both $Q$ and $P_{S}$ are $4\times 4$ matrices. So, where is the gap? If $Q_{1}$ is not $QP_{S}$ , how can I obtain $Q_{1}$ ? And finally, why is $P_{S}$ a $4\times 4$ matrix if it projects into a subspace of dimension $2$ ? (I would expect it to be a $2\times 4$ matrix instead).","Suppose I have an (complex) matrix viewed as a linear transformation from to . I'm struggling to understand the difference between the following two scenarios. For simplicity, let us consider . Scenario 1: Let be a subspace of of dimension . Let us think of as the subspace generated by two eigenvectors and of .One can construct a projection matrix . According to this post , this projection should be given by: where the a matrix. This is a matrix . Scenario 2: Let us write . Since is a linear operator on , it might be possible to decompose so that, if and and : Each should be a matrix. Here is my problem. I would expect that , since should be the restriction of to . However, as I stressed before, both and are matrices. So, where is the gap? If is not , how can I obtain ? And finally, why is a matrix if it projects into a subspace of dimension ? (I would expect it to be a matrix instead).",n\times n Q \mathbb{C}^{n} \mathbb{C}^{n} n = 4 S \mathbb{C}^{4} d=2 S v_{1} v_{2} Q P_{S} P_{S} = A(A^{T}A)^{-1}A^{T} A = [v_{1} \hspace{0.1cm} v_{2}] 4\times 2 4\times 4 \mathbb{C}^{4} = S \oplus S^{\perp} Q \mathbb{C}^{4} Q = Q_{1}\oplus Q_{2} u = u_{1}+u_{2} u_{1}\in S u_{2} \in S^{\perp} Qu = Q_{1}u_{1}\oplus Q_{2}u_{2} Q_{i} 2\times 2 Q_{1} = QP_{S} Q_{1} Q S Q P_{S} 4\times 4 Q_{1} QP_{S} Q_{1} P_{S} 4\times 4 2 2\times 4,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-decomposition']"
36,How to represent a matrix in index notation when it's a combination of more than two multiplications?,How to represent a matrix in index notation when it's a combination of more than two multiplications?,,Let's say there is an arbitrary matrix $A$ . $A$ is $a_{ij}$ in index notation. $A^2$ can be written as $\sum_{ij}a_{ij}a_{ji}$ . But I have no idea how to represent $A^3$ or $A^4$ . I tried to find a way by computing every element of $A^3$ and $A^4$ but I didn't catch any rule to condense each element in index notation in index notation. Can you give me any insight?,Let's say there is an arbitrary matrix . is in index notation. can be written as . But I have no idea how to represent or . I tried to find a way by computing every element of and but I didn't catch any rule to condense each element in index notation in index notation. Can you give me any insight?,A A a_{ij} A^2 \sum_{ij}a_{ij}a_{ji} A^3 A^4 A^3 A^4,"['matrices', 'notation']"
37,The existence of a closeness matrix,The existence of a closeness matrix,,"A matrix $U$ is a closeness matrix to $A$ if for every $\epsilon\ne0$ , $A+\epsilon U$ is invertible. For example, for $0$ , $I$ is a closeness matrix as $\epsilon I$ is invertible, and for $I$ a matrix with a single non-diagnal 1 is  a closeness matrix. The general question is does a closeness matrix alway exist? If $A$ is diagnalizable, $A=SDS^{-1}$ and $P$ is a matrix with a single non-diagonal 1, then $SPS^{-1}$ is a closeness matrix. A specific example I've not managed to show has a closeness matrix is $$\begin{pmatrix}1&1\\0&1\end{pmatrix}.$$","A matrix is a closeness matrix to if for every , is invertible. For example, for , is a closeness matrix as is invertible, and for a matrix with a single non-diagnal 1 is  a closeness matrix. The general question is does a closeness matrix alway exist? If is diagnalizable, and is a matrix with a single non-diagonal 1, then is a closeness matrix. A specific example I've not managed to show has a closeness matrix is",U A \epsilon\ne0 A+\epsilon U 0 I \epsilon I I A A=SDS^{-1} P SPS^{-1} \begin{pmatrix}1&1\\0&1\end{pmatrix}.,"['linear-algebra', 'matrices']"
38,Stability of the null solution in an autonomous system,Stability of the null solution in an autonomous system,,"I am supossed to study the instalibity of the null solution of the following (autonomous) system: \begin{equation*} \begin{cases} x' = -x+y-x^2 \\[5pt] y' =3x-x^2-2y \end{cases} \end{equation*} My resolution so far and my problems. Clearly we are working with an autonomous system with the form $z' = f(z)$ , where $z' = [x'\hspace{.3cm} y']^T$ and $f(z) = f(x,y) = (-x+y-x^2,3x-x^2-2y)$ . Obviously, $f \in C^1$ in $\mathbb{R^2}$ and $f(0,0)=(0,0)$ ( $(0,0)$ is an equilibruim point). Let's compute the Jacobian of $f$ . \begin{equation*} J_f(x,y) = \begin{pmatrix}-1-2x \quad \quad 1 \\[5pt] 3-2x \quad \quad -2 \end{pmatrix} \Rightarrow J_f(0,0) = \begin{pmatrix}-1 \quad \quad1 \\[5pt] 3 \quad \quad -2 \end{pmatrix} \end{equation*} Now we compute the eigenvalues of $J_f(0,0)$ and we get the following result: \begin{equation*} \lambda_1 = \frac{-3+\sqrt{13}}{2} \quad \vee \quad \lambda_2 = \frac{-3-\sqrt{13}}{2}   \end{equation*} Which make us conclude that $J_f(0,0)$ isn't hurwitz (it has a positive real eigenvalue). So, I can't conclude nothing about the stability of the null solution using this method. I would now try to find a Lupyanov function and use its direct method since these are the two methods I've been taught. I have tried multiple Lupyanov functions and I can't figure one out that actually works for this case. So this is my question, if someone can help me finding a Lupyanov function I would be really thankfull.","I am supossed to study the instalibity of the null solution of the following (autonomous) system: My resolution so far and my problems. Clearly we are working with an autonomous system with the form , where and . Obviously, in and ( is an equilibruim point). Let's compute the Jacobian of . Now we compute the eigenvalues of and we get the following result: Which make us conclude that isn't hurwitz (it has a positive real eigenvalue). So, I can't conclude nothing about the stability of the null solution using this method. I would now try to find a Lupyanov function and use its direct method since these are the two methods I've been taught. I have tried multiple Lupyanov functions and I can't figure one out that actually works for this case. So this is my question, if someone can help me finding a Lupyanov function I would be really thankfull.","\begin{equation*}
\begin{cases}
x' = -x+y-x^2 \\[5pt]
y' =3x-x^2-2y
\end{cases}
\end{equation*} z' = f(z) z' = [x'\hspace{.3cm} y']^T f(z) = f(x,y) = (-x+y-x^2,3x-x^2-2y) f \in C^1 \mathbb{R^2} f(0,0)=(0,0) (0,0) f \begin{equation*}
J_f(x,y) = \begin{pmatrix}-1-2x \quad \quad 1 \\[5pt]
3-2x \quad \quad -2
\end{pmatrix}
\Rightarrow J_f(0,0) = \begin{pmatrix}-1 \quad \quad1 \\[5pt] 3 \quad \quad -2
\end{pmatrix}
\end{equation*} J_f(0,0) \begin{equation*}
\lambda_1 = \frac{-3+\sqrt{13}}{2} \quad \vee \quad \lambda_2 = \frac{-3-\sqrt{13}}{2}  
\end{equation*} J_f(0,0)","['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'stability-theory']"
39,A square matrix (XX^T) made from column vector X always has an eigen vector as X,A square matrix (XX^T) made from column vector X always has an eigen vector as X,,"I was asked in an interview that Given a column vector $X$ of order $n\times1$ , when we obtain the matrix $XX^T$ (order $n\times n$ ), give at least one of the eigenvectors of the matrix $XX^T$ without any calculation, just by looking Observing that all the columns of the matrix $XX^T$ are dependent on the column vector $X$ , thus there is only one independent vector basis in the column space of $XX^T$ , thus rank of $XX^T$ is $1$ . Hence in row echelon form of $XX^T$ , there is only one row and thus, $XX^T$ has $n-1$ eigenvalues as $0$ and one eigenvalue is non-zero. With their help I was able to conclude from trace property of a matrix that the non-zero eigenvalue was (sum of square of each element in column vector $X$ ) . But eigenvector I could not found during interview but by their surprised look, it was evident that The eigenvector was hiding in plain sight and I could not see! Later taking examples, I found $X$ to be an eigenvector. I could prove it by brute force taking general $(n\times1)$ column vector and then constructing $XX^T$ with all columns dependent then the product $XX^T\times X$ , will have in first row of product a multiple of first row of any of the column and similarly in all rows, and I could show that the eigenvector $X$ is corresponding to non-zero eigenvalue But, is there a CLEAN method, like from vector spaces geometric interpretation or property of $XX^T$ or anything much more conceptual than brute force general example taking?","I was asked in an interview that Given a column vector of order , when we obtain the matrix (order ), give at least one of the eigenvectors of the matrix without any calculation, just by looking Observing that all the columns of the matrix are dependent on the column vector , thus there is only one independent vector basis in the column space of , thus rank of is . Hence in row echelon form of , there is only one row and thus, has eigenvalues as and one eigenvalue is non-zero. With their help I was able to conclude from trace property of a matrix that the non-zero eigenvalue was (sum of square of each element in column vector ) . But eigenvector I could not found during interview but by their surprised look, it was evident that The eigenvector was hiding in plain sight and I could not see! Later taking examples, I found to be an eigenvector. I could prove it by brute force taking general column vector and then constructing with all columns dependent then the product , will have in first row of product a multiple of first row of any of the column and similarly in all rows, and I could show that the eigenvector is corresponding to non-zero eigenvalue But, is there a CLEAN method, like from vector spaces geometric interpretation or property of or anything much more conceptual than brute force general example taking?",X n\times1 XX^T n\times n XX^T XX^T X XX^T XX^T 1 XX^T XX^T n-1 0 X X (n\times1) XX^T XX^T\times X X XX^T,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-rank', 'transpose']"
40,What is crossMatrix and what does it do?,What is crossMatrix and what does it do?,,"I read a code about 3x3 matrix related calculation, and there is no comment for all the code. There is a crossMatrix function, which, I don't understand. It is defined as: void Matrix3x3::v31xv31_crossMatrix(Vector inV, double diag, Matrix3x3 *poutM) {       poutM->m33_setZero(poutM);       poutM->m[0] = diag;       poutM->m[4] = diag;       poutM->m[8] = diag;       poutM->m[3] = inV.m[2];       poutM->m[1] = -inV.m[2];       poutM->m[6] = -inV.m[1];       poutM->m[2] = inV.m[1];       poutM->m[7] = inV.m[0];       poutM->m[5] = -inV.m[0];   } By writing elements as matrix in math, it looks like this: diag    -m[2]    m[1]    m[2]     diag   -m[0]   -m[1]     m[0]    diag I'm not familiar with this form. Any idea what this matrix is called? What does it do?","I read a code about 3x3 matrix related calculation, and there is no comment for all the code. There is a crossMatrix function, which, I don't understand. It is defined as: void Matrix3x3::v31xv31_crossMatrix(Vector inV, double diag, Matrix3x3 *poutM) {       poutM->m33_setZero(poutM);       poutM->m[0] = diag;       poutM->m[4] = diag;       poutM->m[8] = diag;       poutM->m[3] = inV.m[2];       poutM->m[1] = -inV.m[2];       poutM->m[6] = -inV.m[1];       poutM->m[2] = inV.m[1];       poutM->m[7] = inV.m[0];       poutM->m[5] = -inV.m[0];   } By writing elements as matrix in math, it looks like this: diag    -m[2]    m[1]    m[2]     diag   -m[0]   -m[1]     m[0]    diag I'm not familiar with this form. Any idea what this matrix is called? What does it do?",,['matrices']
41,Is the quotient map locally bi-Lipschitz?,Is the quotient map locally bi-Lipschitz?,,"Suppose $G$ is a Lie group of matrices with a subgroup $H$ and a metric $d_G$ , and then define the induced metric on $G/H$ as $d_H(g_1 H, g_2 H) = \inf_{h_1, h_2 \in H} d_G(g_1h_1, g_2h_2)$ . I've seen in sources online that the quotient map $\pi: G \to G/H; g \mapsto gH$ is differentiable. I want to know if it is locally bi-Lipschitz. It would be enough (I think) to show that the derivative is bounded on compact sets, but I can't find any information on how to practically compute the derivative of this map. Any help would be appreciated.","Suppose is a Lie group of matrices with a subgroup and a metric , and then define the induced metric on as . I've seen in sources online that the quotient map is differentiable. I want to know if it is locally bi-Lipschitz. It would be enough (I think) to show that the derivative is bounded on compact sets, but I can't find any information on how to practically compute the derivative of this map. Any help would be appreciated.","G H d_G G/H d_H(g_1 H, g_2 H) = \inf_{h_1, h_2 \in H} d_G(g_1h_1, g_2h_2) \pi: G \to G/H; g \mapsto gH","['linear-algebra', 'matrices', 'derivatives', 'lie-groups']"
42,Is there a closed form expression for the eigenvectors of a 2x2 matrix?,Is there a closed form expression for the eigenvectors of a 2x2 matrix?,,"Is there a closed form expression for the eigenvalues/eigenvectors of an arbitrary 2x2 matrix $  \begin{bmatrix}     a & b \\     c & d   \end{bmatrix} $ ? Wolfram|Alpha tries to provide an expression, but it seems wrong since it produces undefined results when $c = 0$ , but all matrices have at least one (possibly complex) eigenvector. Is there an actual closed form expression for the eigenvectors and eigenvalues of a matrix?","Is there a closed form expression for the eigenvalues/eigenvectors of an arbitrary 2x2 matrix ? Wolfram|Alpha tries to provide an expression, but it seems wrong since it produces undefined results when , but all matrices have at least one (possibly complex) eigenvector. Is there an actual closed form expression for the eigenvectors and eigenvalues of a matrix?","  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}
 c = 0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'closed-form', 'wolfram-alpha']"
43,"$AB=BA$ from $e^{A+B} = e^A e^B$, given Hermitian matrices","from , given Hermitian matrices",AB=BA e^{A+B} = e^A e^B,"Let $A$ and $B$ be Hermitian matrices. If $AB=BA$ , we know that $e^{A+B} = e^A e^B$ . In this paper , the author showed that $\text{Tr } e^{A+B} = \text{Tr } e^A e^B$ iff. $AB=BA$ . As such, $e^{A+B} = e^A e^B$ is equivalent to $\text{Tr } e^{A+B} = \text{Tr } e^A e^B$ in the context of Hermitian matrices. My question is how we can derive the commutation relation between $A$ and $B$ directly from $e^{A+B}=e^A e^B$ without bringing in the Golden-Thompson inequality (as in the paper I linked). Since the condition $e^{A+B} = e^A e^B$ has a simpler form than that involving the trace, I think there should be some way. Edit: rephrase the question","Let and be Hermitian matrices. If , we know that . In this paper , the author showed that iff. . As such, is equivalent to in the context of Hermitian matrices. My question is how we can derive the commutation relation between and directly from without bringing in the Golden-Thompson inequality (as in the paper I linked). Since the condition has a simpler form than that involving the trace, I think there should be some way. Edit: rephrase the question",A B AB=BA e^{A+B} = e^A e^B \text{Tr } e^{A+B} = \text{Tr } e^A e^B AB=BA e^{A+B} = e^A e^B \text{Tr } e^{A+B} = \text{Tr } e^A e^B A B e^{A+B}=e^A e^B e^{A+B} = e^A e^B,"['matrices', 'trace']"
44,Does the matrix product $X^T X$ have a special meaning?,Does the matrix product  have a special meaning?,X^T X,"I have come across this specific matrix product several times, lately in the context of stochastic linear models where it is an integral part of the Least Squares Estimator (LSE). Often times in linear algebra there is some beautiful intuition hidden behind recurring formulae and since I don't see the one behind this one I'm asking for help. Is there a geometric interpretation or special meaning for the matrix product $$X^T X$$ for a matrix $X \in \mathbb{R}^{n \times p}$ with $p \leq n$ ?","I have come across this specific matrix product several times, lately in the context of stochastic linear models where it is an integral part of the Least Squares Estimator (LSE). Often times in linear algebra there is some beautiful intuition hidden behind recurring formulae and since I don't see the one behind this one I'm asking for help. Is there a geometric interpretation or special meaning for the matrix product for a matrix with ?",X^T X X \in \mathbb{R}^{n \times p} p \leq n,"['linear-algebra', 'matrices', 'statistics']"
45,"$Φ : V → V$ all such linear mappings have same determinant, what does it mean?","all such linear mappings have same determinant, what does it mean?",Φ : V → V,"In the Book that I'm reading (Mathematics for Machine Learning), the following para is given, while listing the properties of a matrix determinant: Similar matrices (Definition 2.22) possess the same determinant. Therefore, for a linear mapping $Φ : V → V$ all transformation matrices $A_Φ$ of $Φ$ have the same determinant. Thus, the determinant is invariant to the choice of basis of a linear mapping. I know that matrices $A$ and $B$ are similar if they satisfy $B=C^{-1}AC$ . I can prove that determinants of such $A$ and $B$ are equal using other properties of a determinant. But beyond that I don't understand what this paragraph is saying. I can understand all matrices $Y$ such that $Y=X^{-1}AX$ have the same determinant as $A$ , for varying $X$ s. But how do I connect this to linear mappings of the form $Φ : V → V$ . What does $Φ : V → V$ mean here? Maybe someone can give me an example. EDIT: This video is pretty basic, but it helped me understand better https://www.youtube.com/watch?v=s4c5LQ5a4ek","In the Book that I'm reading (Mathematics for Machine Learning), the following para is given, while listing the properties of a matrix determinant: Similar matrices (Definition 2.22) possess the same determinant. Therefore, for a linear mapping all transformation matrices of have the same determinant. Thus, the determinant is invariant to the choice of basis of a linear mapping. I know that matrices and are similar if they satisfy . I can prove that determinants of such and are equal using other properties of a determinant. But beyond that I don't understand what this paragraph is saying. I can understand all matrices such that have the same determinant as , for varying s. But how do I connect this to linear mappings of the form . What does mean here? Maybe someone can give me an example. EDIT: This video is pretty basic, but it helped me understand better https://www.youtube.com/watch?v=s4c5LQ5a4ek",Φ : V → V A_Φ Φ A B B=C^{-1}AC A B Y Y=X^{-1}AX A X Φ : V → V Φ : V → V,"['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'machine-learning']"
46,How to get eigenvectors using QR algorithm?,How to get eigenvectors using QR algorithm?,,"From everything I've heard, this matlab code ought to spit out a matrix where each row is the same. So why doesn't it? A = [ 5  2  0  0;       3  9  4  0;       0  9  5 -2;       0  0 -3  4 ]; B=A; QQQ=eye(4);  %QR algorithm for i=1:100     [Q,R] = qr(B);     B=R*Q;     QQQ = QQQ*Q; end  (A*QQQ)./QQQ %should have constant rows, but doesn't","From everything I've heard, this matlab code ought to spit out a matrix where each row is the same. So why doesn't it? A = [ 5  2  0  0;       3  9  4  0;       0  9  5 -2;       0  0 -3  4 ]; B=A; QQQ=eye(4);  %QR algorithm for i=1:100     [Q,R] = qr(B);     B=R*Q;     QQQ = QQQ*Q; end  (A*QQQ)./QQQ %should have constant rows, but doesn't",,"['matrices', 'eigenvalues-eigenvectors', 'matlab', 'matrix-decomposition', 'unitary-matrices']"
47,Eigenvalues of an almost diagonal matrix [duplicate],Eigenvalues of an almost diagonal matrix [duplicate],,"This question already has answers here : How to find the eigenvalues of a block-diagonal matrix? (2 answers) Closed 3 years ago . I know that the eigenvalue of a diagonal matrix is simply the values in the diagonal. However, if I have a matrix of the following form: $$ \begin{bmatrix} a & b & 0 & 0 \\ b & c & 0  & 0 \\ 0 & 0 & d & e \\ 0 & 0 & e & f \end{bmatrix}. $$ Is there a closed form way to express the eigenvalues of this matrix? I can derive the eigenvalue of the smaller blocks along the diagonal, but how does it relate to the overall matrix?","This question already has answers here : How to find the eigenvalues of a block-diagonal matrix? (2 answers) Closed 3 years ago . I know that the eigenvalue of a diagonal matrix is simply the values in the diagonal. However, if I have a matrix of the following form: Is there a closed form way to express the eigenvalues of this matrix? I can derive the eigenvalue of the smaller blocks along the diagonal, but how does it relate to the overall matrix?","
\begin{bmatrix}
a & b & 0 & 0 \\
b & c & 0  & 0 \\
0 & 0 & d & e \\
0 & 0 & e & f
\end{bmatrix}.
","['matrices', 'eigenvalues-eigenvectors', 'eigenfunctions', 'block-matrices']"
48,How to solve this equation with matrices,How to solve this equation with matrices,,"can you please give me some hints to solve the following? I really don't know how to start. $$X^2= \begin{pmatrix} 6 & 2 \\ 3 & 7 \end{pmatrix}.$$ I tried to express this matrix as $4\cdot I + \begin{pmatrix} 2 & 2 \\ 3 & 3 \end{pmatrix}$ And somehow solve it, but I really have no clue. Please some help.","can you please give me some hints to solve the following? I really don't know how to start. I tried to express this matrix as And somehow solve it, but I really have no clue. Please some help.","X^2= \begin{pmatrix}
6 & 2 \\ 3 & 7
\end{pmatrix}. 4\cdot I + \begin{pmatrix}
2 & 2 \\ 3 & 3
\end{pmatrix}","['linear-algebra', 'matrices', 'matrix-equations']"
49,Is $\operatorname{SL}_2(\mathcal O_K)$ dense in $\operatorname{SL}_2(\mathbb R)$?,Is  dense in ?,\operatorname{SL}_2(\mathcal O_K) \operatorname{SL}_2(\mathbb R),"Let $K$ be a real quadratic number field of discriminant $D>0$ with $\mathcal O_K$ being its ring of integers. There are two embeddings $K \hookrightarrow \mathbb R$ and using them $\mathcal O_K \hookrightarrow \mathbb R^2$ can be reagarded as discrete subgroup of $\mathbb R^2$ . But when you regard only one embedding $\mathcal O_K \hookrightarrow \mathbb R$ is a dense subgroup of $\mathbb R$ . Now let us consider $\operatorname{SL}_2(\mathcal O_K)$ . Embedded into $\operatorname{SL}_2(\mathbb R)^2$ this is still discrete. But if you regard only one embedding $\operatorname{SL}_2(\mathcal O_K) \hookrightarrow \operatorname{SL}_2(\mathbb R)$ this is not discrete anymore. For example you have $$\left\{\left(\begin{matrix} 1 & \lambda\\ 0 & 1\\ \end{matrix}\right) \in \operatorname{SL}_2(\mathcal O_K) : \lambda \in \mathcal O_K\right\} $$ whose closure is $$\left\{\left(\begin{matrix} 1 & x\\ 0 & 1\\ \end{matrix}\right) \in \operatorname{SL}_2(\mathbb R) : x \in \mathbb R\right\}. $$ On the other hand I'm not sure if for all $x \in \mathbb R$ the matrix $\operatorname{diag}(x,x^{-1})$ is part of the closure of $\operatorname{SL}_2(\mathcal O_K)$ . So my question is : What is the closure of $\operatorname{SL}_2(\mathcal O_K)$ in $\operatorname{SL}_2(\mathbb R)$ ? Is $\operatorname{SL}_2(\mathcal O_K)$ maybe dense in $\operatorname{SL}_2(\mathbb R)$ ?",Let be a real quadratic number field of discriminant with being its ring of integers. There are two embeddings and using them can be reagarded as discrete subgroup of . But when you regard only one embedding is a dense subgroup of . Now let us consider . Embedded into this is still discrete. But if you regard only one embedding this is not discrete anymore. For example you have whose closure is On the other hand I'm not sure if for all the matrix is part of the closure of . So my question is : What is the closure of in ? Is maybe dense in ?,"K D>0 \mathcal O_K K \hookrightarrow \mathbb R \mathcal O_K \hookrightarrow \mathbb R^2 \mathbb R^2 \mathcal O_K \hookrightarrow \mathbb R \mathbb R \operatorname{SL}_2(\mathcal O_K) \operatorname{SL}_2(\mathbb R)^2 \operatorname{SL}_2(\mathcal O_K) \hookrightarrow \operatorname{SL}_2(\mathbb R) \left\{\left(\begin{matrix}
1 & \lambda\\
0 & 1\\
\end{matrix}\right) \in \operatorname{SL}_2(\mathcal O_K) : \lambda \in \mathcal O_K\right\}
 \left\{\left(\begin{matrix}
1 & x\\
0 & 1\\
\end{matrix}\right) \in \operatorname{SL}_2(\mathbb R) : x \in \mathbb R\right\}.
 x \in \mathbb R \operatorname{diag}(x,x^{-1}) \operatorname{SL}_2(\mathcal O_K) \operatorname{SL}_2(\mathcal O_K) \operatorname{SL}_2(\mathbb R) \operatorname{SL}_2(\mathcal O_K) \operatorname{SL}_2(\mathbb R)","['matrices', 'group-theory', 'number-theory', 'real-numbers']"
50,"Need help regarding intuition of rows in a coordinate/basis matrix, where the columns are vectors.","Need help regarding intuition of rows in a coordinate/basis matrix, where the columns are vectors.",,"Given a matrix M such that its columns are the vectors of a new basis with respect to another basis B. To find the coordinates of v in the other basis, we can simply take $M[v]_M = [v]_B$ . Let me give an example of M $$\begin{bmatrix}1&2\\ 4&3\end{bmatrix}$$ I believe they are linearly independent(i just pulled out some random number off my head and tested), but the numbers aren't that important. What i am confused about is we know that the columns of M form a set of basis vectors but when doing $M[v]_m$ matrix multiplication, we iterate within each $row_i$ of M for each value in the corresponding row of the output vector instead. Now, i learn that, in my school's materials convention, we represent linear functionals as row vectors instead, since column vectors are for things like coordinate vectors and this makes sense to me at least here, but above, i am using a basis matrix's rows like linear functionals? So yeah, is it just ""it is how it is because matrix multiplication rules"", or is there some special property or something about rows in matrices.","Given a matrix M such that its columns are the vectors of a new basis with respect to another basis B. To find the coordinates of v in the other basis, we can simply take . Let me give an example of M I believe they are linearly independent(i just pulled out some random number off my head and tested), but the numbers aren't that important. What i am confused about is we know that the columns of M form a set of basis vectors but when doing matrix multiplication, we iterate within each of M for each value in the corresponding row of the output vector instead. Now, i learn that, in my school's materials convention, we represent linear functionals as row vectors instead, since column vectors are for things like coordinate vectors and this makes sense to me at least here, but above, i am using a basis matrix's rows like linear functionals? So yeah, is it just ""it is how it is because matrix multiplication rules"", or is there some special property or something about rows in matrices.",M[v]_M = [v]_B \begin{bmatrix}1&2\\ 4&3\end{bmatrix} M[v]_m row_i,"['linear-algebra', 'matrices', 'linear-transformations']"
51,Prove that $\det(\overline M)=\overline {\det(M)}$,Prove that,\det(\overline M)=\overline {\det(M)},"For $M \in M_{n×n}(\mathbb C)$ , let $\overline M$ be the matrix such that $(\overline M)_{ij}=\overline {M_{ij}}$ for all $i,j$ , where $\overline {M_{ij}}$ is the complex conjugate of $M_{ij}$ . Prove that $\det(\overline M)=\overline {\det(M)}$ . My attempt: We will use induction on $n$ . Base case $n=1$ : Let $M$ be the one-by-one matrix with entry $a+bi$ \begin{align*} \det(\overline M) &= \det(a-bi)=a-bi \\ \overline {\det(M)} &=\overline {a+bi}=a-bi \end{align*} Induction hypothesis: assume holds for $n$ , NTS it holds for $n+1$ : \begin{align*} \det(\overline M) &= \sum_{j=1}^{n+1} {(-1)^{1+j} \overline M_{1j} \cdot \det(\overline {\tilde M_{1j}}}) \\&=(-1)^{1+n+1} \overline {M_{1,n+1}}\cdot \det(\overline {\tilde M_{1,n+1})} + \sum_{j=1}^{n} {(-1)^{1+j} \overline M_{1j} \cdot \det(\overline {\tilde M_{1j}}}) \\&=(-1)^{2+n}\overline {M_{1,n+1}}\cdot \overline{\det(\tilde M_{1,n+1})}+\overline{\det(M')} \end{align*} where $M'$ is $n$ -by- $n$ . I feel like the proof is almost complete but am not sure how to proceed. Any help is greatly appreciated.","For , let be the matrix such that for all , where is the complex conjugate of . Prove that . My attempt: We will use induction on . Base case : Let be the one-by-one matrix with entry Induction hypothesis: assume holds for , NTS it holds for : where is -by- . I feel like the proof is almost complete but am not sure how to proceed. Any help is greatly appreciated.","M \in M_{n×n}(\mathbb C) \overline M (\overline M)_{ij}=\overline {M_{ij}} i,j \overline {M_{ij}} M_{ij} \det(\overline M)=\overline {\det(M)} n n=1 M a+bi \begin{align*}
\det(\overline M) &= \det(a-bi)=a-bi \\
\overline {\det(M)} &=\overline {a+bi}=a-bi
\end{align*} n n+1 \begin{align*}
\det(\overline M) &= \sum_{j=1}^{n+1} {(-1)^{1+j} \overline M_{1j} \cdot \det(\overline {\tilde M_{1j}}})
\\&=(-1)^{1+n+1} \overline {M_{1,n+1}}\cdot \det(\overline {\tilde M_{1,n+1})} + \sum_{j=1}^{n} {(-1)^{1+j} \overline M_{1j} \cdot \det(\overline {\tilde M_{1j}}})
\\&=(-1)^{2+n}\overline {M_{1,n+1}}\cdot \overline{\det(\tilde M_{1,n+1})}+\overline{\det(M')}
\end{align*} M' n n","['linear-algebra', 'matrices', 'determinant']"
52,Derivative of Matrix with respect to matrix,Derivative of Matrix with respect to matrix,,"Let $x \in \mathbb{R}^d,W \in \mathbb{R}^{dxd}$ $\frac {\partial{}}{\partial{W_{i,j}}}(Wx+b)$ What I have done so far is $W_{i,j}.x_j =  \begin{pmatrix} \sum_{i} W_{1,i}.x_i \\ \vdots \\ \vdots   \\ \sum_{i} W_{d,i}.x_i \\ \end{pmatrix}$ Now if I take the derivative of the product mentioned above, Theoretically it should mean that all the entries of $x$ should be in the answer and the answer would be $x_i$ ?","Let What I have done so far is Now if I take the derivative of the product mentioned above, Theoretically it should mean that all the entries of should be in the answer and the answer would be ?","x \in \mathbb{R}^d,W \in \mathbb{R}^{dxd} \frac {\partial{}}{\partial{W_{i,j}}}(Wx+b) W_{i,j}.x_j = 
\begin{pmatrix}
\sum_{i} W_{1,i}.x_i \\
\vdots \\
\vdots   \\
\sum_{i} W_{d,i}.x_i \\
\end{pmatrix} x x_i","['matrices', 'multivariable-calculus', 'partial-derivative', 'matrix-calculus']"
53,Space of Matrices up to Congruence.,Space of Matrices up to Congruence.,,"I am trying to understand congruent matrices and Sylvester's Law of Inertia, and came upon the following space. Let $X$ be the space $M_n(\mathbb{R})$ mod congruence of matrices. Question 1. : Is this space studied in any part of mathematics (if so, does it have a name)? I assume it must be important since it is the space of quadratic forms classified by their indices. Question 2. Is it a manifold? Are there notable topological properties about this space? (Anything interesting about its homology, cohomology, fundamental group etc.)?","I am trying to understand congruent matrices and Sylvester's Law of Inertia, and came upon the following space. Let be the space mod congruence of matrices. Question 1. : Is this space studied in any part of mathematics (if so, does it have a name)? I assume it must be important since it is the space of quadratic forms classified by their indices. Question 2. Is it a manifold? Are there notable topological properties about this space? (Anything interesting about its homology, cohomology, fundamental group etc.)?",X M_n(\mathbb{R}),"['linear-algebra', 'matrices', 'algebraic-geometry', 'manifolds']"
54,The orbit of a non zero vector $v$ in $\mathbb{R}^n$ is $\mathbb{R}^n \setminus \{0\}$,The orbit of a non zero vector  in  is,v \mathbb{R}^n \mathbb{R}^n \setminus \{0\},"Prove that given any non zero vectors $v$ and $w$ in $\mathbb{R}^n$ , there exists an invertible $n\times n$ matrix $A$ such that $Av=w$ . (I don't know where to start. Any hints will be appreciated)","Prove that given any non zero vectors and in , there exists an invertible matrix such that . (I don't know where to start. Any hints will be appreciated)",v w \mathbb{R}^n n\times n A Av=w,"['linear-algebra', 'matrices', 'group-theory']"
55,"Given matrix $A^2$, how to find matrix $A$?","Given matrix , how to find matrix ?",A^2 A,"Let $$A^2 = \begin{pmatrix} 3 & 1 \\ 2 & 2 \end{pmatrix}$$ Knowing that $A$ has positive eigenvalues, what is $A$ ? What I did was the following: $$A = \begin{pmatrix} a & b \\ c & d  \end{pmatrix}$$ so $$A^2 = \begin{pmatrix} a^2 + bc & ab+bd \\ ac+cd & bc+d^2  \end{pmatrix}$$ I got stuck here after trying to solve the 4 equations. Can someone help, please?","Let Knowing that has positive eigenvalues, what is ? What I did was the following: so I got stuck here after trying to solve the 4 equations. Can someone help, please?","A^2 = \begin{pmatrix} 3 & 1 \\ 2 & 2 \end{pmatrix} A A A = \begin{pmatrix}
a & b \\
c & d 
\end{pmatrix} A^2 = \begin{pmatrix}
a^2 + bc & ab+bd \\
ac+cd & bc+d^2 
\end{pmatrix}","['linear-algebra', 'matrices', 'matrix-equations']"
56,How can I construct a nilpotent matrix of order 100 and index 98?,How can I construct a nilpotent matrix of order 100 and index 98?,,"I know to construct a nilpotent matrix of order $n$ with index of nilpotency $n$ , but how to construct a nilpotent matrix of order $n$ but index of nilpotency $(n-2)$ ? Is there any general rule for the same?","I know to construct a nilpotent matrix of order with index of nilpotency , but how to construct a nilpotent matrix of order but index of nilpotency ? Is there any general rule for the same?",n n n (n-2),"['matrices', 'examples-counterexamples', 'nilpotence']"
57,"Prove the existence of a principal submatrix of order $r$ in $M\in\Bbb F^{n\times n}, M=-M^T,\ \operatorname{rank}(M)=r$",Prove the existence of a principal submatrix of order  in,"r M\in\Bbb F^{n\times n}, M=-M^T,\ \operatorname{rank}(M)=r","Let $M$ be a skew-symmetric matrix of $\operatorname{rank}(M)=r$ , prove that there exists a principal submatrix of order $r$ . I have a solution for the version which doesn't require the submatrix to be principal (this is not my solution) take away all but $r$ linearly independent columns of $M$ , call the matrix $P$ since $\operatorname{rank}M=r$ this is possible then if $P$ is $r \times r$ , done, invertible if $P$ is $n \times r$ where $n < r$ then $\operatorname{rank}(M)\leqslant n < r$ so this case can't happen so assume $P$ is $n \times r$ where $n > r$ , now we look at $P^T = -P$ , $\operatorname{rank}(P) = \operatorname{rank}(-P) = r$ then you can take away all but $r$ columns of $P^T$ obtaining $Q^T=$ taking all but $r$ rows of $P$ obtaining $Q$ then $Q$ is $r \times r$ and all columns are linearly independent So I am looking for a solution which proves the existence of a principal matrix.","Let be a skew-symmetric matrix of , prove that there exists a principal submatrix of order . I have a solution for the version which doesn't require the submatrix to be principal (this is not my solution) take away all but linearly independent columns of , call the matrix since this is possible then if is , done, invertible if is where then so this case can't happen so assume is where , now we look at , then you can take away all but columns of obtaining taking all but rows of obtaining then is and all columns are linearly independent So I am looking for a solution which proves the existence of a principal matrix.",M \operatorname{rank}(M)=r r r M P \operatorname{rank}M=r P r \times r P n \times r n < r \operatorname{rank}(M)\leqslant n < r P n \times r n > r P^T = -P \operatorname{rank}(P) = \operatorname{rank}(-P) = r r P^T Q^T= r P Q Q r \times r,"['linear-algebra', 'matrices']"
58,"Solve the matrix equation $X ^ 3 = A$, with $X \in M_2(\mathbb{R})$ and given $A$.","Solve the matrix equation , with  and given .",X ^ 3 = A X \in M_2(\mathbb{R}) A,"I have to solve the matrix equation: $$X^3 = A$$ where $$A = \begin{pmatrix} 2 & 1\\ 1 & 2 \end{pmatrix}$$ So in the end I have to solve: $$X^3 =  \begin{pmatrix} 2 & 1\\ 1 & 2 \end{pmatrix}$$ Since we have $2$ x $2$ matrices I tried the following: $$X ^ 3 = A$$ $$\det(X^3) = \det(A)$$ $$\det(X)^3 = 3$$ $$\det(X) = \sqrt[3]{3}$$ I did this in hopes that it would result in the determinant of $X$ being $0$ so then I could've used the Cayley-Hamilton identity : $$X^2 - tr(X) X + det(X) I_2 = O_2$$ where $tr(x) = (a + d)$ (trace of the matrix). But since the determinant is not $0$ , I can't abuse that, it looks complicated since that last term does not get canceled. Then I tried to use the notation: $$X = \begin{pmatrix} a & b\\ c & d \end{pmatrix}$$ and to do the multiplication $X \cdot X \cdot X$ and then to equate it to $A$ and try to find $a, b, c, d$ but the algebra got very ugly very fast and I lost myself in the calculations. So how should I approach this?","I have to solve the matrix equation: where So in the end I have to solve: Since we have x matrices I tried the following: I did this in hopes that it would result in the determinant of being so then I could've used the Cayley-Hamilton identity : where (trace of the matrix). But since the determinant is not , I can't abuse that, it looks complicated since that last term does not get canceled. Then I tried to use the notation: and to do the multiplication and then to equate it to and try to find but the algebra got very ugly very fast and I lost myself in the calculations. So how should I approach this?","X^3 = A A = \begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix} X^3 =  \begin{pmatrix}
2 & 1\\
1 & 2
\end{pmatrix} 2 2 X ^ 3 = A \det(X^3) = \det(A) \det(X)^3 = 3 \det(X) = \sqrt[3]{3} X 0 X^2 - tr(X) X + det(X) I_2 = O_2 tr(x) = (a + d) 0 X = \begin{pmatrix}
a & b\\
c & d
\end{pmatrix} X \cdot X \cdot X A a, b, c, d",['matrices']
59,continuous surjection from $GL_2(\Bbb R)$ to closed unit disc in the complex plane,continuous surjection from  to closed unit disc in the complex plane,GL_2(\Bbb R),Does there exist a continuous surjective map from $G = GL_2(\Bbb R)$ to the closed unit disc $\{z \in \Bbb C: |z| \le 1\}$ in $\Bbb C$ ? I know that $C^{*}$ sits inside $G$ as a subgroup. But it didn't help me. So kindly share some thoughts. Thank you.,Does there exist a continuous surjective map from to the closed unit disc in ? I know that sits inside as a subgroup. But it didn't help me. So kindly share some thoughts. Thank you.,G = GL_2(\Bbb R) \{z \in \Bbb C: |z| \le 1\} \Bbb C C^{*} G,"['matrices', 'continuity']"
60,"Calculating determinant of a symmetric matrix where the $k$th row is given by $[a_{k-1},a_k,...,a_0,a_1,...,a_{n-(k-1)}]$",Calculating determinant of a symmetric matrix where the th row is given by,"k [a_{k-1},a_k,...,a_0,a_1,...,a_{n-(k-1)}]","For $j = 0,...,n$ set $a_{j} = a_{0} + jd$ , where $a_{0}, d$ are fixed real numbers. Calculate the determinant of the $(n+1)\times (n+1)$ matrix $$A = \begin{pmatrix}     a_{0}   & a_{1} & a_{2} &  \dots & a_{n}\\     a_{1} & a_{0}  & a_{1} & \dots & a_{n-1}\\     a_{2} & a_{1}  & a_{0} & \dots & a_{n-2}\\     \ldots & \ldots & \ldots & \ldots & \ldots\\     a_{n} & a_{n-1}  & a_{n-2} & \dots & a_{0}  \end{pmatrix}.$$ How to calculate that? I haven't found any property of determinant of symmetric matrix which could help. I've tried to use Gaussian elimination (subtracting each row from the row above it), but it didn't work Gaussian elimination(subtracting each row from the row above it)  brings to the matrix: $$\begin{pmatrix}     -d   & d & d &  ... & d\\     -d & -d  & d & ... & d\\     -d & -d  & -d & .... & d\\     \ldots & \ldots & \ldots & \ldots & \ldots\\     a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0}  \end{pmatrix} = d^{n-1} \cdot \begin{pmatrix}     -1   & 1 & 1 &  ... & 1\\     -1 & -1  & 1 & ... & 1\\     -1 & -1  & -1 & .... & 1\\     \ldots & \ldots & \ldots & \ldots & \ldots\\     a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0}  \end{pmatrix}$$","For set , where are fixed real numbers. Calculate the determinant of the matrix How to calculate that? I haven't found any property of determinant of symmetric matrix which could help. I've tried to use Gaussian elimination (subtracting each row from the row above it), but it didn't work Gaussian elimination(subtracting each row from the row above it)  brings to the matrix:","j = 0,...,n a_{j} = a_{0} + jd a_{0}, d (n+1)\times (n+1) A = \begin{pmatrix}
    a_{0}   & a_{1} & a_{2} &  \dots & a_{n}\\
    a_{1} & a_{0}  & a_{1} & \dots & a_{n-1}\\
    a_{2} & a_{1}  & a_{0} & \dots & a_{n-2}\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & \dots & a_{0} 
\end{pmatrix}. \begin{pmatrix}
    -d   & d & d &  ... & d\\
    -d & -d  & d & ... & d\\
    -d & -d  & -d & .... & d\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0} 
\end{pmatrix} = d^{n-1} \cdot \begin{pmatrix}
    -1   & 1 & 1 &  ... & 1\\
    -1 & -1  & 1 & ... & 1\\
    -1 & -1  & -1 & .... & 1\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0} 
\end{pmatrix}","['linear-algebra', 'matrices', 'determinant']"
61,Can I multiply 2 determinants of square matrices when they are of different in sizes?,Can I multiply 2 determinants of square matrices when they are of different in sizes?,,"In the image attached, you can see the problem. (I am supposed to compute iii ) I know that any equally sized square matrices' determinants can be multiplied directly. $$det(AB) = det(A)det(B)$$ Here however one of the matrices is 4x4 and the other is 5x5 so I am not sure if that rule holds or if this value can be computed.","In the image attached, you can see the problem. (I am supposed to compute iii ) I know that any equally sized square matrices' determinants can be multiplied directly. Here however one of the matrices is 4x4 and the other is 5x5 so I am not sure if that rule holds or if this value can be computed.",det(AB) = det(A)det(B),"['matrices', 'determinant']"
62,What is a matrix with no Jordan canonical form?,What is a matrix with no Jordan canonical form?,,Could somebody give me an example of a matrix that doesn’t admit a Jordan Canonical Form over $\mathbb R$ and explain why it does not?,Could somebody give me an example of a matrix that doesn’t admit a Jordan Canonical Form over and explain why it does not?,\mathbb R,"['linear-algebra', 'matrices', 'jordan-normal-form']"
63,Filling an $8\times 8$ grid with the numbers $1$ to $64$ such that every $3\times 3$ subsquare has a sum less than $256$,Filling an  grid with the numbers  to  such that every  subsquare has a sum less than,8\times 8 1 64 3\times 3 256,Can you help me construct an $8 \times 8$ square filled with numbers from 1 to 64 (each cell has a different number obviously) such that every $3 \times 3$ subsquare has sum of numbers less than $256$ ? I have tried to fill up the corners with the big numbers but I have failed to balance the $3 \times 3$ squares. I have also find some symmetries in the configuration but I still haven't succeed to stay lower than $256$ . The question is a subtask from a problem I invented myself. I don't post the whole problem because I want to protect my work and use it for future projects.,Can you help me construct an square filled with numbers from 1 to 64 (each cell has a different number obviously) such that every subsquare has sum of numbers less than ? I have tried to fill up the corners with the big numbers but I have failed to balance the squares. I have also find some symmetries in the configuration but I still haven't succeed to stay lower than . The question is a subtask from a problem I invented myself. I don't post the whole problem because I want to protect my work and use it for future projects.,8 \times 8 3 \times 3 256 3 \times 3 256,"['matrices', 'combinatorial-designs']"
64,Eigenvalues of block Toeplitz matrix with Toeplitz blocks,Eigenvalues of block Toeplitz matrix with Toeplitz blocks,,"Consider integers $m,n$ and a $m \times m$ -block Toeplitz matrix $A$ consisting of two different types of blocks as follows \begin{align}	 A_{mn \times mn}  &=  \begin{bmatrix}  B & C & C & \cdots & \cdots & C \\  C & B & C & C & \cdots & C \\  C & C & B  & C & \ddots & \vdots \\  \vdots & \ddots & \ddots & \ddots &  \ddots & C \\  C & \cdots & \cdots & C & B & C \\  C & \cdots & \cdots & \cdots & C & B  \end{bmatrix} _{mn \times mn}  , \end{align} where $B$ 's are diagonal blocks with $B=\frac{1}{m}I_n$ and $C$ 's are multiples of the all-ones matrix $J_n$ , specifically $C=\frac{1}{mn}J_n$ . I want to compute the eigenvalues of $A$ (I am mainly interested in the value of the 2nd largest eigenvalue since it has a special meaning in graph expansion applications). Note that in my problem the following conditions also hold for $m,n$ : $m$ is odd. $n$ is prime. $m<n$ . I have experimented with such matrices on the computer and I have observed a trend for the spectrum of $A$ which consists of the following eigenvalues: $\lambda_1=0$ with algebraic multiplicity $m-1$ . $\lambda_2=1/m$ with algebraic multiplicity $m(n-1)$ . $\lambda_3=1$ with algebraic multiplicity $1$ . I do not claim that this is necessarily the answer but at least it was consistent for the pairs of $m,n$ I tried. Can you suggest how one can go and prove the above claim (if correct) or pinpoint other known results? EDIT After Omnomnomnom's note that \begin{equation} A = \frac 1{mn}\underbrace{\pmatrix{ 0&1&\cdots & 1\\ 1&0&\ddots&\vdots\\ \vdots&\ddots&\ddots&1\\ 1&\cdots&1&0}}_{= C_{m \times m}} \otimes J_n + \frac 1m I_{mn} \end{equation} I did some computation of the spectrums of the individual matrices. First, the characteristic polynomial of the all-ones $J_n$ is $(\lambda-n)\lambda^{n-1}$ and hence its spectrum (with the multiplicities) is \begin{equation} \sigma(J_n)=\{(n,1),(0,n-1)\}. \end{equation} For $C$ , assume that $\lambda_1,\dots,\lambda_m$ are its eigenvalues. By the facts that $\mathrm{det}(C-(-1)I_m)=det(J_m)=0$ , $C\mathbf{1}_m=(m-1)\mathbf{1}_m$ and $\mathrm{trace}(C)=\sum_i\lambda_i=0$ it turns out that \begin{equation} \sigma(C)=\{(m-1,1),(-1,m-1)\}. \end{equation} Suppose that $\mu_1,\dots,\mu_n$ are the eigenvalues of $J_n$ then by the Kronecker product's properties the spectrum of $CJ_n$ consists of the pairwise products $\lambda_i\mu_j, \forall i,j$ .","Consider integers and a -block Toeplitz matrix consisting of two different types of blocks as follows where 's are diagonal blocks with and 's are multiples of the all-ones matrix , specifically . I want to compute the eigenvalues of (I am mainly interested in the value of the 2nd largest eigenvalue since it has a special meaning in graph expansion applications). Note that in my problem the following conditions also hold for : is odd. is prime. . I have experimented with such matrices on the computer and I have observed a trend for the spectrum of which consists of the following eigenvalues: with algebraic multiplicity . with algebraic multiplicity . with algebraic multiplicity . I do not claim that this is necessarily the answer but at least it was consistent for the pairs of I tried. Can you suggest how one can go and prove the above claim (if correct) or pinpoint other known results? EDIT After Omnomnomnom's note that I did some computation of the spectrums of the individual matrices. First, the characteristic polynomial of the all-ones is and hence its spectrum (with the multiplicities) is For , assume that are its eigenvalues. By the facts that , and it turns out that Suppose that are the eigenvalues of then by the Kronecker product's properties the spectrum of consists of the pairwise products .","m,n m \times m A \begin{align}	
A_{mn \times mn} 
&= 
\begin{bmatrix} 
B & C & C & \cdots & \cdots & C \\ 
C & B & C & C & \cdots & C \\ 
C & C & B  & C & \ddots & \vdots \\ 
\vdots & \ddots & \ddots & \ddots &  \ddots & C \\ 
C & \cdots & \cdots & C & B & C \\ 
C & \cdots & \cdots & \cdots & C & B 
\end{bmatrix}
_{mn \times mn} 
,
\end{align} B B=\frac{1}{m}I_n C J_n C=\frac{1}{mn}J_n A m,n m n m<n A \lambda_1=0 m-1 \lambda_2=1/m m(n-1) \lambda_3=1 1 m,n \begin{equation}
A = \frac 1{mn}\underbrace{\pmatrix{
0&1&\cdots & 1\\
1&0&\ddots&\vdots\\
\vdots&\ddots&\ddots&1\\
1&\cdots&1&0}}_{= C_{m \times m}} \otimes J_n + \frac 1m I_{mn}
\end{equation} J_n (\lambda-n)\lambda^{n-1} \begin{equation}
\sigma(J_n)=\{(n,1),(0,n-1)\}.
\end{equation} C \lambda_1,\dots,\lambda_m \mathrm{det}(C-(-1)I_m)=det(J_m)=0 C\mathbf{1}_m=(m-1)\mathbf{1}_m \mathrm{trace}(C)=\sum_i\lambda_i=0 \begin{equation}
\sigma(C)=\{(m-1,1),(-1,m-1)\}.
\end{equation} \mu_1,\dots,\mu_n J_n CJ_n \lambda_i\mu_j, \forall i,j","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-theory', 'toeplitz-matrices']"
65,Origin of the formula $\partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right) $,Origin of the formula,\partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right) ,I found that for any first order differential operator $\partial $ acting on a square matrix $X$ we have $$\partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right) $$ I'm quite sure it origins from $$ \det(\exp (X))=\exp (\operatorname{Tr}(X)) $$ But I get lost with the operator. I don't get how we can insert the differential inside the trace. Can anybody help?,I found that for any first order differential operator acting on a square matrix we have I'm quite sure it origins from But I get lost with the operator. I don't get how we can insert the differential inside the trace. Can anybody help?,\partial  X \partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right)   \det(\exp (X))=\exp (\operatorname{Tr}(X)) ,"['linear-algebra', 'matrices', 'analysis', 'lie-groups', 'matrix-calculus']"
66,What does a subscript F represent?,What does a subscript F represent?,,"On page 11 of the slide , Sum-of-least-square loss: $$ \ell\left(\mathbf{\tilde W}\right)  = \sum_{n=1}^N \left\| \mathbf{\tilde W}^T\mathbf{\tilde x^{(n)}}                        -\mathbf{t}^{(n})                 \right\|^2  = \left\|\mathbf{\tilde X\tilde W-T}\right\|^2_F $$ the $n$ -th row of $\mathbf{\tilde X}$ is $\left[\mathbf{\tilde x}^{(n)}\right]^T$ the $n$ -th row of $\mathbf{T}$ is $\left[\mathbf{t}^{(n)}\right]^T$ I don't get what the subscript $F$ means in the equation. I don't even know what tags I should put. Any modification would be appreciated!","On page 11 of the slide , Sum-of-least-square loss: the -th row of is the -th row of is I don't get what the subscript means in the equation. I don't even know what tags I should put. Any modification would be appreciated!","
\ell\left(\mathbf{\tilde W}\right)
 = \sum_{n=1}^N \left\| \mathbf{\tilde W}^T\mathbf{\tilde x^{(n)}}
                       -\mathbf{t}^{(n})
                \right\|^2
 = \left\|\mathbf{\tilde X\tilde W-T}\right\|^2_F
 n \mathbf{\tilde X} \left[\mathbf{\tilde x}^{(n)}\right]^T n \mathbf{T} \left[\mathbf{t}^{(n)}\right]^T F","['matrices', 'notation', 'normed-spaces']"
67,"Matrices - given $AB$, how to find determinant of $BA$ ??","Matrices - given , how to find determinant of  ??",AB BA,"Let A $_{3×2}$ and B $_{2×3}$ be matrices such that their product $AB$ is $$AB=\begin{pmatrix} 8&2 & -2\\ 2&5&4 \\ -2&4&5 \\ \end{pmatrix}$$ And $BA$ is nonsingular Find the determinant of $BA$ . I have no idea , how to solve this type of question. All I could notice is that $|AB| = 0$ and it's a symmetric matrix. I tried assuming a general matrix , but I get simply too many unknowns and very few equations.","Let A and B be matrices such that their product is And is nonsingular Find the determinant of . I have no idea , how to solve this type of question. All I could notice is that and it's a symmetric matrix. I tried assuming a general matrix , but I get simply too many unknowns and very few equations.","_{3×2} _{2×3} AB AB=\begin{pmatrix}
8&2 & -2\\
2&5&4 \\
-2&4&5 \\
\end{pmatrix} BA BA |AB| = 0",['matrices']
68,What is dot product attention actually doing?,What is dot product attention actually doing?,,"I've read multiple papers on machine learning attention mechanisms, but I fail to really understand what is going on from a basic level. I've never seen anything that deeply explains the concept of queries, keys, and values with real examples. Below are some examples of papers... [1] - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf [2] - https://arxiv.org/pdf/1901.05761.pdf An equation that is in both papers is the one below which is described as dot product attention. How can I really understand what the result of this means for the involved $Q$ , $K$ , and $V$ ?... $$ \mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{d_k}\right)V $$ There is also another variant which they called Laplacian attention which is defined as.. $$ \mathrm{Laplace}(Q, K, V) = WV \in \mathbb{R}^{n\times d_k}, \;\;\;\;\; W_i = \mathrm{softmax}((-|| Q - K ||_1)_{j=1}^n) \in \mathbb{R}^n $$ I understand all of the processes involved, but I don't understand what the end result of these is and I feel like it is interfering with my ability to grasp papers that deal with attention","I've read multiple papers on machine learning attention mechanisms, but I fail to really understand what is going on from a basic level. I've never seen anything that deeply explains the concept of queries, keys, and values with real examples. Below are some examples of papers... [1] - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf [2] - https://arxiv.org/pdf/1901.05761.pdf An equation that is in both papers is the one below which is described as dot product attention. How can I really understand what the result of this means for the involved , , and ?... There is also another variant which they called Laplacian attention which is defined as.. I understand all of the processes involved, but I don't understand what the end result of these is and I feel like it is interfering with my ability to grasp papers that deal with attention","Q K V 
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{d_k}\right)V
 
\mathrm{Laplace}(Q, K, V) = WV \in \mathbb{R}^{n\times d_k}, \;\;\;\;\; W_i = \mathrm{softmax}((-|| Q - K ||_1)_{j=1}^n) \in \mathbb{R}^n
","['linear-algebra', 'matrices', 'machine-learning']"
69,Moore-Penrose pseudoinverse and the Euclidean norm,Moore-Penrose pseudoinverse and the Euclidean norm,,"Section 2.9 The Moore-Penrose Pseudoinverse of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says the following: Matrix inversion is not defined for matrices that are not square. Suppose we want to make a left-inverse $\mathbf{B}$ of a matrix $\mathbf{A}$ so that we can solve a linear equation $$\mathbf{A} \mathbf{x} = \mathbf{y} \tag{2.44}$$ by left-multiplying each side to obtain $$\mathbf{x} = \mathbf{B} \mathbf{y}. \tag{2.45}$$ Depending on the structure of the problem, it may not be possible to design a unique mapping from $\mathbf{A}$ to $\mathbf{B}$ . If $\mathbf{A}$ is taller than it is wide, then it is possible for this equation to have no solution. If $\mathbf{A}$ is wider than it is tall, then there could be multiple possible solutions. The Moore-Penrose pseudoinverse enables us to make some headway in these cases. The pseudoinverse of $\mathbf{A}$ is defined as a matrix $$\mathbf{A}^+ = \lim_{\alpha \searrow 0^+}(\mathbf{A}^T \mathbf{A} + \alpha \mathbf{I} )^{-1} \mathbf{A}^T. \tag{2.46}$$ Practical algorithms for computing the pseudoinverse are based not on this definition, but rather on the formula $$\mathbf{A}^+ = \mathbf{V} \mathbf{D}^+ \mathbf{U}^T, \tag{2.47}$$ where $\mathbf{U}$ , $\mathbf{D}$ and $\mathbf{V}$ are the singular value decomposition of $\mathbf{A}$ , and the pseudoinverse $\mathbf{D}^+$ of a diagonal matrix $\mathbf{D}$ is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix. When $\mathbf{A}$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides  the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions. When $\mathbf{A}$ has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ . It's this last part that I'm wondering about: When $\mathbf{A}$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions. When $\mathbf{A}$ has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ . What I found confusing here is that the Euclidean norms $\vert \vert \mathbf{x} \vert \vert_2$ and $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ seemingly come out of nowhere. Prior to this section, there is no discussion of the Euclidean norm -- only of the mechanics of the Moore-Penrose Pseudoinverse. And the authors then just assert this part without explanation. So I am left wondering the following: Why is it that, when $\mathbf{A}$ has more columns than rows, then using the pseudoinverse gives us the solution $\mathbf{x} = \mathbf{A}^+ \mathbf{y}$ with minimal Euclidean norm $\vert \vert \mathbf{x} \vert \vert_2$ among all possible solutions? Why is it that, when $\mathbf{A}$ has more rows than columns, then using the pseudoinverse gives us the $\mathbf{x}$ for which $\mathbf{A} \mathbf{x}$ is as close as  possible to $\mathbf{y}$ in terms of Euclidean norm $\vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2$ ? And what are the mechanics involved here? I would greatly appreciate it if people would please take the time to clarify this.","Section 2.9 The Moore-Penrose Pseudoinverse of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says the following: Matrix inversion is not defined for matrices that are not square. Suppose we want to make a left-inverse of a matrix so that we can solve a linear equation by left-multiplying each side to obtain Depending on the structure of the problem, it may not be possible to design a unique mapping from to . If is taller than it is wide, then it is possible for this equation to have no solution. If is wider than it is tall, then there could be multiple possible solutions. The Moore-Penrose pseudoinverse enables us to make some headway in these cases. The pseudoinverse of is defined as a matrix Practical algorithms for computing the pseudoinverse are based not on this definition, but rather on the formula where , and are the singular value decomposition of , and the pseudoinverse of a diagonal matrix is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix. When has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides  the solution with minimal Euclidean norm among all possible solutions. When has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the for which is as close as  possible to in terms of Euclidean norm . It's this last part that I'm wondering about: When has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution with minimal Euclidean norm among all possible solutions. When has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the for which is as close as  possible to in terms of Euclidean norm . What I found confusing here is that the Euclidean norms and seemingly come out of nowhere. Prior to this section, there is no discussion of the Euclidean norm -- only of the mechanics of the Moore-Penrose Pseudoinverse. And the authors then just assert this part without explanation. So I am left wondering the following: Why is it that, when has more columns than rows, then using the pseudoinverse gives us the solution with minimal Euclidean norm among all possible solutions? Why is it that, when has more rows than columns, then using the pseudoinverse gives us the for which is as close as  possible to in terms of Euclidean norm ? And what are the mechanics involved here? I would greatly appreciate it if people would please take the time to clarify this.","\mathbf{B} \mathbf{A} \mathbf{A} \mathbf{x} = \mathbf{y} \tag{2.44} \mathbf{x} = \mathbf{B} \mathbf{y}. \tag{2.45} \mathbf{A} \mathbf{B} \mathbf{A} \mathbf{A} \mathbf{A} \mathbf{A}^+ = \lim_{\alpha \searrow 0^+}(\mathbf{A}^T \mathbf{A} + \alpha \mathbf{I} )^{-1} \mathbf{A}^T. \tag{2.46} \mathbf{A}^+ = \mathbf{V} \mathbf{D}^+ \mathbf{U}^T, \tag{2.47} \mathbf{U} \mathbf{D} \mathbf{V} \mathbf{A} \mathbf{D}^+ \mathbf{D} \mathbf{A} \mathbf{x} = \mathbf{A}^+ \mathbf{y} \vert \vert \mathbf{x} \vert \vert_2 \mathbf{A} \mathbf{x} \mathbf{A} \mathbf{x} \mathbf{y} \vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2 \mathbf{A} \mathbf{x} = \mathbf{A}^+ \mathbf{y} \vert \vert \mathbf{x} \vert \vert_2 \mathbf{A} \mathbf{x} \mathbf{A} \mathbf{x} \mathbf{y} \vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2 \vert \vert \mathbf{x} \vert \vert_2 \vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2 \mathbf{A} \mathbf{x} = \mathbf{A}^+ \mathbf{y} \vert \vert \mathbf{x} \vert \vert_2 \mathbf{A} \mathbf{x} \mathbf{A} \mathbf{x} \mathbf{y} \vert \vert \mathbf{A} \mathbf{x} − \mathbf{y} \vert \vert_2","['linear-algebra', 'matrices', 'normed-spaces', 'machine-learning', 'pseudoinverse']"
70,Do eigenvalues depend smoothly on the matrix elements of a diagonalizable matrix?,Do eigenvalues depend smoothly on the matrix elements of a diagonalizable matrix?,,Suppose I have a matrix $M(t)$ whose matrix elements depend smoothly on a real parameter $t$ . I also know that this matrix is diagonalizable for the $t$ s I'm interested in. Can I say that its eigenvalues depend smoothly on $t$ ? Thanks very much!,Suppose I have a matrix whose matrix elements depend smoothly on a real parameter . I also know that this matrix is diagonalizable for the s I'm interested in. Can I say that its eigenvalues depend smoothly on ? Thanks very much!,M(t) t t t,"['real-analysis', 'linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-analysis']"
71,Prove/disprove that we cannot have $V = U_1 \oplus U_2$,Prove/disprove that we cannot have,V = U_1 \oplus U_2,"$V$ vector space of upper triangular $3\times3$ matrices. $𝑈1$ and $𝑈2$ are subspaces of $𝑉$ . Every non-zero member of $𝑈1$ is invertible. Every member of $𝑈2$ is non-invertible. Prove/disprove that we cannot have $𝑉=𝑈1⊕𝑈2$ I think that this is correct and My idea is to prove that $U1 \cap U2 \ne {\{0}\}$ But I""m having hard time to write General member for $U1$ and $U2$ any hint how to prove this ? thanks","vector space of upper triangular matrices. and are subspaces of . Every non-zero member of is invertible. Every member of is non-invertible. Prove/disprove that we cannot have I think that this is correct and My idea is to prove that But I""m having hard time to write General member for and any hint how to prove this ? thanks",V 3\times3 𝑈1 𝑈2 𝑉 𝑈1 𝑈2 𝑉=𝑈1⊕𝑈2 U1 \cap U2 \ne {\{0}\} U1 U2,"['linear-algebra', 'matrices', 'vector-spaces', 'inverse']"
72,Each eigenvalue of $e^X$ is the exponential of an eigenvalue of $X$,Each eigenvalue of  is the exponential of an eigenvalue of,e^X X,"Let $X \in M(n)$ , I was able to show that if $v$ is an eigenvector of $X$ with corresponding eigenvalue $\lambda$ , that $v$ is also an eigenvector of $e^X$ with eigenvalue $e^{\lambda}$ , but I couldn't show that each eigenvalue of $e^X$ is the exponential of an eigenvalue of $X$ .","Let , I was able to show that if is an eigenvector of with corresponding eigenvalue , that is also an eigenvector of with eigenvalue , but I couldn't show that each eigenvalue of is the exponential of an eigenvalue of .",X \in M(n) v X \lambda v e^X e^{\lambda} e^X X,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-exponential']"
73,Implication of Matrices in $AX=0$,Implication of Matrices in,AX=0,"I'm not sure how to phrase my question and this is the first time I'm posting on math exchange but basically I'm looking for some feedback on my answer to this question: Let $A$ denote an $m×n$ matrix. If $AX=0$ for every $n×1$ matrix $X$ , show that $A=0$ . I claim that since $X$ is every column vector in the set $R^n$ , and $AX=0$ , it is implied that $A=0$ by considering the constituents of the products of $AX$ : $a_j×x_i$ where $a_j$ represents the $j$ th column of $A$ and $x_i$ represents the $i$ th row of $X$ , and $i=j$ . If $a_j×x_i=0$ and vector $X$ is every possible column vector (where $x_i=n$ ), then it's implied that $A$ must be a zero matrix (where $a_j=0$ ) to satisfy the equation $AX=0$ . I feel like I'm running in circles with this argument...","I'm not sure how to phrase my question and this is the first time I'm posting on math exchange but basically I'm looking for some feedback on my answer to this question: Let denote an matrix. If for every matrix , show that . I claim that since is every column vector in the set , and , it is implied that by considering the constituents of the products of : where represents the th column of and represents the th row of , and . If and vector is every possible column vector (where ), then it's implied that must be a zero matrix (where ) to satisfy the equation . I feel like I'm running in circles with this argument...",A m×n AX=0 n×1 X A=0 X R^n AX=0 A=0 AX a_j×x_i a_j j A x_i i X i=j a_j×x_i=0 X x_i=n A a_j=0 AX=0,"['linear-algebra', 'matrices', 'logic']"
74,What is the $B^{-1}$ if $B^2 -5B + I = 0$ [duplicate],What is the  if  [duplicate],B^{-1} B^2 -5B + I = 0,This question already has an answer here : Proving Invertibility and Eigenvalues (1 answer) Closed 4 years ago . I think how I did it was that I did $B (B -5I) = -I$ and then divided both sides by $-1$ to get $B (-B+5BI) = I$ and then said $(-B+5I) = B^{-1}$ . Is this correct or is there a better way of doing it/answer?,This question already has an answer here : Proving Invertibility and Eigenvalues (1 answer) Closed 4 years ago . I think how I did it was that I did and then divided both sides by to get and then said . Is this correct or is there a better way of doing it/answer?,B (B -5I) = -I -1 B (-B+5BI) = I (-B+5I) = B^{-1},"['linear-algebra', 'matrices']"
75,Finding the minimal polynomial of an $n \times n$ matrix,Finding the minimal polynomial of an  matrix,n \times n,How would one find the minimal polynomial of $$ A= \begin{pmatrix}0&-1&-2&\cdots&1-n\\1&0&-1&\cdots&2-n\\2&1&0&\cdots&3-n\\\vdots&\vdots&\vdots&\ddots&\vdots\\n-1&n-2&n-3&\cdots&0\end{pmatrix}$$ Where $A$ is an $n \times n$ matrix with $n\ge 3$ ?,How would one find the minimal polynomial of Where is an matrix with ?, A= \begin{pmatrix}0&-1&-2&\cdots&1-n\\1&0&-1&\cdots&2-n\\2&1&0&\cdots&3-n\\\vdots&\vdots&\vdots&\ddots&\vdots\\n-1&n-2&n-3&\cdots&0\end{pmatrix} A n \times n n\ge 3,"['matrices', 'minimal-polynomials']"
76,Is Loewner order equivalent to inequality of eigenvalues?,Is Loewner order equivalent to inequality of eigenvalues?,,"I know from this previous question that if $A \geq B$ then $\lambda_k(A) \geq \lambda_k(B)$ , where $\lambda_k$ denotes the $k$ th ordered eigenvalue. Now I would like to know if the reverse holds, i.e., if the Loewner order can be defined in terms of inequalities between eigenvalues. Any book or reference where this issue is explained in detail would be appreciated.","I know from this previous question that if then , where denotes the th ordered eigenvalue. Now I would like to know if the reverse holds, i.e., if the Loewner order can be defined in terms of inequalities between eigenvalues. Any book or reference where this issue is explained in detail would be appreciated.",A \geq B \lambda_k(A) \geq \lambda_k(B) \lambda_k k,"['linear-algebra', 'matrices', 'inequality', 'reference-request', 'eigenvalues-eigenvectors']"
77,Prove that two given matrices are strong shift equivalent.,Prove that two given matrices are strong shift equivalent.,,"Two square matrices $A$ and $B$ are called elementary strong shift equivalent (denote $A\sim B$ ) iff there exist non-negative integer matrices $U$ and $V$ (not necessarily square), such that $A=UV$ and $B=VU$ . Two square matrices $A$ and $B$ are called strong shift equivalent iff there are square matrices $A=M_{1},M_{2},\ldots,M_{n}=B$ such that $M_i\sim M_{i+1}$ for all $i$ . Given are the two following matrices: $$A:=\begin{pmatrix}1&1&0\\1&1&1\\2&2&1\end{pmatrix},\qquad B:=\begin{pmatrix}3\end{pmatrix}.$$ It is not hard to prove that $A$ and $B$ are not elementary strong shift equivalent. However, I have no idea how to prove that they are strong shift equivalent. Any suggestions are greatly appreciated! This is part of an exercise from the book Introduction to Dynamical Systems by Michael Brin and Garrett Stuck.","Two square matrices and are called elementary strong shift equivalent (denote ) iff there exist non-negative integer matrices and (not necessarily square), such that and . Two square matrices and are called strong shift equivalent iff there are square matrices such that for all . Given are the two following matrices: It is not hard to prove that and are not elementary strong shift equivalent. However, I have no idea how to prove that they are strong shift equivalent. Any suggestions are greatly appreciated! This is part of an exercise from the book Introduction to Dynamical Systems by Michael Brin and Garrett Stuck.","A B A\sim B U V A=UV B=VU A B A=M_{1},M_{2},\ldots,M_{n}=B M_i\sim M_{i+1} i A:=\begin{pmatrix}1&1&0\\1&1&1\\2&2&1\end{pmatrix},\qquad B:=\begin{pmatrix}3\end{pmatrix}. A B","['linear-algebra', 'matrices', 'dynamical-systems']"
78,Show that a reflection matrix is given by $\begin{bmatrix}\cos2\theta&\sin2\theta \\ \sin2\theta&-\cos2\theta\end{bmatrix}$,Show that a reflection matrix is given by,\begin{bmatrix}\cos2\theta&\sin2\theta \\ \sin2\theta&-\cos2\theta\end{bmatrix},"Reflection matrix: $$ \text{Reflection}(\theta) =  \begin{bmatrix} \cos2\theta & \sin2\theta \\ \sin2\theta & -\cos2\theta \end{bmatrix}$$ Attempt: Inspiration: Speaking non-rigorously, it seems like the angle between the reflected vector and the original vector will be $2\theta$ . Armed with this, let's consider how $e_1 = \begin{bmatrix}1\\0\end{bmatrix}$ and $e_2 = \begin{bmatrix}0\\1\end{bmatrix}$ change when we reflect them across an arbitrary line. Let $$ \text{Reflection}(\theta) =  \begin{bmatrix} a & b \\ c & d \end{bmatrix}$$ Then, $$\begin{align} \text{Reflection}(\theta) \cdot e_1 &=  \begin{bmatrix} a & b \\ c & d \end{bmatrix} \cdot \begin{bmatrix}1\\0\end{bmatrix} \\ &= \begin{bmatrix}a\\c\end{bmatrix} \end{align}$$ Using my assumption that reflected vectors have angle of $2\theta$ between itself and the original vector, $(\text{Reflection}(\theta)\cdot e_1) \cdot e_1 =  \begin{bmatrix}a\\c\end{bmatrix}\cdot \begin{bmatrix}1\\0\end{bmatrix} = a = ||\text{Reflection}(\theta)\cdot e_1||\cdot||e_1|| \cos(2\theta)$ (dot product). Simplifying the other side of the equation, we get: $$a = 1\cdot 1 \cos(2\theta) = \cos(2\theta)$$ Doing it similarly for $e_2$ yields $d=\cos(2\theta)$ which means our reflection matrix currently looks like this: $$ \text{Reflection}(\theta) =  \begin{bmatrix} \cos2\theta & b \\ c & \cos2\theta \end{bmatrix}$$ which is not correct. Questions: I suspect my implicit assumption that a reflection can be represented as a rotation of $2\theta$ , where $\theta$ is the angle between the original vector and reflection line, is where I went wrong. Why is this wrong? $$ \text{Rotation}(2\theta) =  \begin{bmatrix} \cos2\theta & -\sin2\theta \\ \sin2\theta & \cos2\theta \end{bmatrix} \text{ for reference. }$$ What's the correct way to do this? I would normally ask one question per SE question but I think my two questions are tightly coupled.","Reflection matrix: Attempt: Inspiration: Speaking non-rigorously, it seems like the angle between the reflected vector and the original vector will be . Armed with this, let's consider how and change when we reflect them across an arbitrary line. Let Then, Using my assumption that reflected vectors have angle of between itself and the original vector, (dot product). Simplifying the other side of the equation, we get: Doing it similarly for yields which means our reflection matrix currently looks like this: which is not correct. Questions: I suspect my implicit assumption that a reflection can be represented as a rotation of , where is the angle between the original vector and reflection line, is where I went wrong. Why is this wrong? What's the correct way to do this? I would normally ask one question per SE question but I think my two questions are tightly coupled."," \text{Reflection}(\theta) = 
\begin{bmatrix}
\cos2\theta & \sin2\theta \\
\sin2\theta & -\cos2\theta
\end{bmatrix} 2\theta e_1 = \begin{bmatrix}1\\0\end{bmatrix} e_2 = \begin{bmatrix}0\\1\end{bmatrix} 
\text{Reflection}(\theta) = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \begin{align}
\text{Reflection}(\theta) \cdot e_1 &= 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \cdot \begin{bmatrix}1\\0\end{bmatrix} \\
&= \begin{bmatrix}a\\c\end{bmatrix}
\end{align} 2\theta (\text{Reflection}(\theta)\cdot e_1) \cdot e_1 =  \begin{bmatrix}a\\c\end{bmatrix}\cdot \begin{bmatrix}1\\0\end{bmatrix} = a = ||\text{Reflection}(\theta)\cdot e_1||\cdot||e_1|| \cos(2\theta) a = 1\cdot 1 \cos(2\theta) = \cos(2\theta) e_2 d=\cos(2\theta)  \text{Reflection}(\theta) = 
\begin{bmatrix}
\cos2\theta & b \\
c & \cos2\theta
\end{bmatrix} 2\theta \theta  \text{Rotation}(2\theta) = 
\begin{bmatrix}
\cos2\theta & -\sin2\theta \\
\sin2\theta & \cos2\theta
\end{bmatrix} \text{ for reference. }","['linear-algebra', 'matrices', 'proof-verification', 'proof-writing']"
79,Finding the Eigenvectors,Finding the Eigenvectors,,"Consider the matrix given below: $\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}.$ The eigenvalues for this matrix are $\dfrac{1+\sqrt 5}{2},\dfrac{1-\sqrt 5}{2}.$ I am facing trouble finding the eigenvectors. Please help.",Consider the matrix given below: The eigenvalues for this matrix are I am facing trouble finding the eigenvectors. Please help.,"\begin{pmatrix}
1 & 1 \\
1 & 0
\end{pmatrix}. \dfrac{1+\sqrt 5}{2},\dfrac{1-\sqrt 5}{2}.","['matrices', 'eigenvalues-eigenvectors']"
80,Is this a family of similar matrices $\left(\begin{smallmatrix} 0&x\\ 0&0 \end{smallmatrix}\right)$?,Is this a family of similar matrices ?,\left(\begin{smallmatrix} 0&x\\ 0&0 \end{smallmatrix}\right),"Is matrix $A = \begin{pmatrix} 0&1\\ 0&0 \end{pmatrix}$ similar to matrix $B=\begin{pmatrix} 0&2\\ 0&0 \end{pmatrix}$ ? If so, how do I prove this? I came here from following the answer to this question: Do similar matrices have equal singular values?","Is matrix similar to matrix ? If so, how do I prove this? I came here from following the answer to this question: Do similar matrices have equal singular values?",A = \begin{pmatrix} 0&1\\ 0&0 \end{pmatrix} B=\begin{pmatrix} 0&2\\ 0&0 \end{pmatrix},"['linear-algebra', 'matrices', 'jordan-normal-form']"
81,Symmetric linear least squares solution,Symmetric linear least squares solution,,"Given an overdetermined linear system $AX=Y$ with known $A$ and $Y$ , how would I go about finding the least squares solution $X$ under the constraint that it is symmetric ( $X=X^T$ )?","Given an overdetermined linear system with known and , how would I go about finding the least squares solution under the constraint that it is symmetric ( )?",AX=Y A Y X X=X^T,"['linear-algebra', 'matrices', 'least-squares', 'symmetric-matrices', 'pseudoinverse']"
82,Trace of matrix $A^{\ast}A$,Trace of matrix,A^{\ast}A,"Given a $n \times n$ matrix $A$ with complex entries. And $A^{\ast}$ represents the  conjugate transpose of $A$ .Then If $\left | tr{\left ( A^{\ast}A \right )}\right | <n^2$ , then $\left |a_{ij} \right| < 1$ for some $i,j$ If $A$ is invertible ,then $ tr{\left ( A^{\ast}A \right )}$ $\neq0$ Solution i tried- The Matrix $A^{\ast}A$ is a Hermitian matrix  so all  of its eigenvalues will be real. but further i am not getting what to do ,while thinking about this question some other questions are also coming to my mind, like is $A^{\ast}A$ is a positive definite?  how eigenvalues of $A$ is related to eigenvalues of $A^{\ast}A$ ?. please help Thankyou?","Given a matrix with complex entries. And represents the  conjugate transpose of .Then If , then for some If is invertible ,then Solution i tried- The Matrix is a Hermitian matrix  so all  of its eigenvalues will be real. but further i am not getting what to do ,while thinking about this question some other questions are also coming to my mind, like is is a positive definite?  how eigenvalues of is related to eigenvalues of ?. please help Thankyou?","n \times n A A^{\ast} A \left | tr{\left ( A^{\ast}A \right )}\right | <n^2 \left |a_{ij} \right| < 1 i,j A  tr{\left ( A^{\ast}A \right )} \neq0 A^{\ast}A A^{\ast}A A A^{\ast}A","['linear-algebra', 'matrices', 'trace', 'matrix-norms']"
83,Showing that there is unique matrix $B$ such that $B^k=A$ for some $A$,Showing that there is unique matrix  such that  for some,B B^k=A A,"Let $A$ be a $n$ by $n$ real matrix with distinct positive eigenvalues $\lambda_1$ ,..., $\lambda_n$ . And let $k$ be an odd integer. Then, I was able to show that there exists a real matrix $B$ such that $B^k=A$ . However, it is not so easy to show that such $B$ is unique. How do I exclude the possibility that $B$ has some complex eigenvalues and still the entries of $B$ are all real? Could anyone please explain?","Let be a by real matrix with distinct positive eigenvalues ,..., . And let be an odd integer. Then, I was able to show that there exists a real matrix such that . However, it is not so easy to show that such is unique. How do I exclude the possibility that has some complex eigenvalues and still the entries of are all real? Could anyone please explain?",A n n \lambda_1 \lambda_n k B B^k=A B B B,"['linear-algebra', 'matrices']"
84,Do $3\times3$ matrices with this multiplication property exist?,Do  matrices with this multiplication property exist?,3\times3,"I'm doing some novice exploration in what I think is basic abstract algebra and for curiosity reasons, I'm trying to find a matrix representation similar to $a+bi\cong\bigl(\begin{smallmatrix}a&-b\\b&a\end{smallmatrix}\bigr)$ for the complex numbers, for some elements that multiply like this: $$\mathbf{e}_a\cdot\mathbf{e}_b = \mathbf{e}_{(2b-a)\text{ mod }3}$$ That is, there are 3 basis elements that we could call $\mathbf{i}, \mathbf{j}, \mathbf{k}$ , each of which are idempotent, but when one is multiplied by another, it results in the third. For example, $\mathbf{i}^2 = \mathbf{i} = \mathbf{jk}$ . I've tried a couple of different matrix ""representations"" but the product hasn't matched my expectation for the equivalent entries. Does such a representation exist?","I'm doing some novice exploration in what I think is basic abstract algebra and for curiosity reasons, I'm trying to find a matrix representation similar to for the complex numbers, for some elements that multiply like this: That is, there are 3 basis elements that we could call , each of which are idempotent, but when one is multiplied by another, it results in the third. For example, . I've tried a couple of different matrix ""representations"" but the product hasn't matched my expectation for the equivalent entries. Does such a representation exist?","a+bi\cong\bigl(\begin{smallmatrix}a&-b\\b&a\end{smallmatrix}\bigr) \mathbf{e}_a\cdot\mathbf{e}_b = \mathbf{e}_{(2b-a)\text{ mod }3} \mathbf{i}, \mathbf{j}, \mathbf{k} \mathbf{i}^2 = \mathbf{i} = \mathbf{jk}","['abstract-algebra', 'matrices']"
85,Square root of matrix product,Square root of matrix product,,"My linear algebra abilities are somewhat limited, so this may be a very basic question. Suppose we have two positive definite matrices A and B. Is $(AB)^{1/2}=A^{1/2}B^{1/2}$ ? Furthermore, is $(AB^{-1})^{1/2}=A^{1/2}B^{-1/2}$ ?","My linear algebra abilities are somewhat limited, so this may be a very basic question. Suppose we have two positive definite matrices A and B. Is ? Furthermore, is ?",(AB)^{1/2}=A^{1/2}B^{1/2} (AB^{-1})^{1/2}=A^{1/2}B^{-1/2},"['linear-algebra', 'matrices', 'radicals']"
86,Derivative of a Rotation Matrix with changing rotation axis,Derivative of a Rotation Matrix with changing rotation axis,,"Just to introduce the background of this question: As many of you know a Rotation Matrix can transform a point $^{B}\textrm{p}$ described in a rotated Coordinate Frame {B} into the point $^{A}\textrm{p}$ described in the Coordinate Frame {A} by: $^{A}\textrm{p}$ = $^{A}\textrm{R}_B \ ^{B}\textrm{p}$ The Rotation Matrix's $^{A}\textrm{R}_B$ columns are the unit vectors of {B}'s axis described in Frame {A}. Also the Rotation about a given axis can be given by: $^{A}\textrm{R}_B$ = $e^{[\hat{w}]_x\theta}$ , whereas $[\hat{w}]_x$ is the skew-symmetric 3x3 matrix of the unit vector of $\hat{w}$ (deschribed in Frame A), around which the Frame is being rotated. $\theta$ is the rotation angle (and a scalar). Now my question: Almost every book and paper i found states that the time-derivative of the Rotation Matrix is the following: $\frac{d}{dt}(^{A}\textrm{R}_B)$ = $[w]_x \  ^{A}\textrm{R}_B \qquad (1)$ Does the solution require that the direction of $\hat{w}$ remains constant at all times? Because if we use the chain rule on with a (1): $\frac{d}{dt}e^{[\hat{w}]_x\theta}$ = $\frac{d}{dt}([\hat{w}(t)]_x\theta(t)) \cdot e^{[\hat{w}]_x\theta}$ = $\frac{d}{dt}([\hat{w}(t)]_x)\theta(t) e^{[\hat{w}(t)]_x\theta(t)} + [\hat{w}(t)]_x)\dot{\theta}(t) e^{[\hat{w}(t)]_x\theta(t)}$ which can be further simplified to: $\frac{d}{dt}e^{[\hat{w}]_x\theta}$ = $(\frac{d}{dt}([\hat{w}(t)]_x)\theta(t)  + [w(t)]_x)^{A}\textrm{R}_B$ Whereas $[w(t)]_x=[\hat{w}(t)]_x\theta(t)$ Thus this is not equal to (1) and an addition term is generated. So which of these equations is true now, when the rotation is not being done around a constant axis? Or did i do something wrong? Greetings, 1lc","Just to introduce the background of this question: As many of you know a Rotation Matrix can transform a point described in a rotated Coordinate Frame {B} into the point described in the Coordinate Frame {A} by: = The Rotation Matrix's columns are the unit vectors of {B}'s axis described in Frame {A}. Also the Rotation about a given axis can be given by: = , whereas is the skew-symmetric 3x3 matrix of the unit vector of (deschribed in Frame A), around which the Frame is being rotated. is the rotation angle (and a scalar). Now my question: Almost every book and paper i found states that the time-derivative of the Rotation Matrix is the following: = Does the solution require that the direction of remains constant at all times? Because if we use the chain rule on with a (1): = = which can be further simplified to: = Whereas Thus this is not equal to (1) and an addition term is generated. So which of these equations is true now, when the rotation is not being done around a constant axis? Or did i do something wrong? Greetings, 1lc",^{B}\textrm{p} ^{A}\textrm{p} ^{A}\textrm{p} ^{A}\textrm{R}_B \ ^{B}\textrm{p} ^{A}\textrm{R}_B ^{A}\textrm{R}_B e^{[\hat{w}]_x\theta} [\hat{w}]_x \hat{w} \theta \frac{d}{dt}(^{A}\textrm{R}_B) [w]_x \  ^{A}\textrm{R}_B \qquad (1) \hat{w} \frac{d}{dt}e^{[\hat{w}]_x\theta} \frac{d}{dt}([\hat{w}(t)]_x\theta(t)) \cdot e^{[\hat{w}]_x\theta} \frac{d}{dt}([\hat{w}(t)]_x)\theta(t) e^{[\hat{w}(t)]_x\theta(t)} + [\hat{w}(t)]_x)\dot{\theta}(t) e^{[\hat{w}(t)]_x\theta(t)} \frac{d}{dt}e^{[\hat{w}]_x\theta} (\frac{d}{dt}([\hat{w}(t)]_x)\theta(t)  + [w(t)]_x)^{A}\textrm{R}_B [w(t)]_x=[\hat{w}(t)]_x\theta(t),"['linear-algebra', 'matrices', 'robotics']"
87,Irreducible representations of $SO(2)$ on 2x2 matrices.,Irreducible representations of  on 2x2 matrices.,SO(2),"I'm having trouble verifying my understanding of the representation theory of Lie groups (which is minimal) with my experience playing around with the rotations of 2x2 matrices. Specifically, if we consider the space of 2x2 matrices $M_2$ being acted on by the group of rotations $SO(2)$ , it's pretty easy to figure out what the irreducible representations are. $SO(2)$ is generated by $$R(\theta)=\left(\begin{array}{cc}\cos\theta&\sin\theta\\-\sin\theta&\cos\theta\end{array}\right)$$ and under the rotation $R(\theta)M_2R^{-1}(\theta)$ , there are three irreducible subspaces: $$\left\{\left(\begin{array}{cc}1&0\\0&1\end{array}\right)\right\},\left\{\left(\begin{array}{cc}0&1\\-1&0\end{array}\right)\right\},\left\{\left(\begin{array}{cc}0&1\\1&0\end{array}\right),\left(\begin{array}{cc}1&0\\0&-1\end{array}\right)\right\}$$ You can get this just by playing around with Mathematica for a while. I would called this $1\oplus 1\oplus 2$ . However, if I want to try to figure this out ""in the usual way"", I would do the following: Determine the irreducible representations on a single vector space $V$ . Take the tensor product of those $V^*\otimes V$ . To do the first step, I would normally do something like look for a Cartan Subalgebra to construct a diagonal representation, and then look at the action of the other generators to find the irreducible ones. This group is generated by a single element, but it seems like the essence of the process should still work. So let's find a diagonal representation, by just looking for eigenvectors of that rotation matrix. The answer is $$\lambda_{\pm}=\cos\theta\pm i\sin\theta$$ So there are two eigenvectors $v_+$ and $v_-$ , and no other generators, so the irreducible representation is $V=v_+\oplus v_-$ . (that seems right - no vector in $\mathbb{R}^2$ should be left invariant by this rotation) So now the tensor product is generated by $$(v_+\oplus v_-)\otimes(v_+\oplus v_-),$$ with eigenvectors $$R(\theta)(v_+\otimes v_+)=\lambda_+^2(v_+\otimes v_+)$$ $$R(\theta)(v_+\otimes v_-)=\lambda_+\lambda_-(v_+\otimes v_-)$$ $$R(\theta)(v_-\otimes v_+)=\lambda_+\lambda_-(v_-\otimes v_+)$$ $$R(\theta)(v_+\otimes v_-)=\lambda_-^2(v_-\otimes v_-)$$ So I might now say that the three invariant subspaces above are generated by $v_+\otimes v_+$ , $v_-\otimes v_-$ , and $v_+\otimes v_-+v_-\otimes v_+$ , respectively. But for example, $v_+\otimes v_+$ sure looks more like $$\left(\begin{array}{cc}a&a\\b&b\end{array}\right)$$ not $$\left(\begin{array}{cc}1&0\\0&1\end{array}\right)\sim\left(\begin{array}{cc}a&0\\0&a\end{array}\right)$$ Is there something here I am missing?","I'm having trouble verifying my understanding of the representation theory of Lie groups (which is minimal) with my experience playing around with the rotations of 2x2 matrices. Specifically, if we consider the space of 2x2 matrices being acted on by the group of rotations , it's pretty easy to figure out what the irreducible representations are. is generated by and under the rotation , there are three irreducible subspaces: You can get this just by playing around with Mathematica for a while. I would called this . However, if I want to try to figure this out ""in the usual way"", I would do the following: Determine the irreducible representations on a single vector space . Take the tensor product of those . To do the first step, I would normally do something like look for a Cartan Subalgebra to construct a diagonal representation, and then look at the action of the other generators to find the irreducible ones. This group is generated by a single element, but it seems like the essence of the process should still work. So let's find a diagonal representation, by just looking for eigenvectors of that rotation matrix. The answer is So there are two eigenvectors and , and no other generators, so the irreducible representation is . (that seems right - no vector in should be left invariant by this rotation) So now the tensor product is generated by with eigenvectors So I might now say that the three invariant subspaces above are generated by , , and , respectively. But for example, sure looks more like not Is there something here I am missing?","M_2 SO(2) SO(2) R(\theta)=\left(\begin{array}{cc}\cos\theta&\sin\theta\\-\sin\theta&\cos\theta\end{array}\right) R(\theta)M_2R^{-1}(\theta) \left\{\left(\begin{array}{cc}1&0\\0&1\end{array}\right)\right\},\left\{\left(\begin{array}{cc}0&1\\-1&0\end{array}\right)\right\},\left\{\left(\begin{array}{cc}0&1\\1&0\end{array}\right),\left(\begin{array}{cc}1&0\\0&-1\end{array}\right)\right\} 1\oplus 1\oplus 2 V V^*\otimes V \lambda_{\pm}=\cos\theta\pm i\sin\theta v_+ v_- V=v_+\oplus v_- \mathbb{R}^2 (v_+\oplus v_-)\otimes(v_+\oplus v_-), R(\theta)(v_+\otimes v_+)=\lambda_+^2(v_+\otimes v_+) R(\theta)(v_+\otimes v_-)=\lambda_+\lambda_-(v_+\otimes v_-) R(\theta)(v_-\otimes v_+)=\lambda_+\lambda_-(v_-\otimes v_+) R(\theta)(v_+\otimes v_-)=\lambda_-^2(v_-\otimes v_-) v_+\otimes v_+ v_-\otimes v_- v_+\otimes v_-+v_-\otimes v_+ v_+\otimes v_+ \left(\begin{array}{cc}a&a\\b&b\end{array}\right) \left(\begin{array}{cc}1&0\\0&1\end{array}\right)\sim\left(\begin{array}{cc}a&0\\0&a\end{array}\right)","['matrices', 'representation-theory', 'lie-groups', 'rotations', 'orthogonal-matrices']"
88,How many $2\times2$ orthogonal matrices $A$ with $A^3=I$ and $A^2=A^T$ are there?,How many  orthogonal matrices  with  and  are there?,2\times2 A A^3=I A^2=A^T,"How many $2 × 2$ matrices $A$ satisfy both $A^3 = I_2$ and $A^ 2 = A^t$ , where $I_2$ denotes the $2 × 2$ identity   matrix and $A^ t$ denotes the transpose of $A$ ? A bit of manipulation gives me $AA^t=A^tA=I_2$ .So this is orthogonal. Now is there a fixed number of $2\times2$ orthogonal matrices? I have no idea. Please help.","How many matrices satisfy both and , where denotes the identity   matrix and denotes the transpose of ? A bit of manipulation gives me .So this is orthogonal. Now is there a fixed number of orthogonal matrices? I have no idea. Please help.","2 × 2 A A^3 = I_2 A^
2 = A^t I_2 2 × 2 A^
t A AA^t=A^tA=I_2 2\times2",['matrices']
89,Decomposing a symmetric matrix as a sum of nilpotent matrices,Decomposing a symmetric matrix as a sum of nilpotent matrices,,"Assume that a real-valued symmetric matrix $M$ with trace zero can be written as $$ M = A + A^T, $$ with $A^2=0$ . Given that $M$ is known, how (if possible) can $A$ be found? The diagonal elements of $A$ are just half those of $M$ , that is $$ A_{ii}=M_{ii}/2. $$ But for the non diagonal ones, considering only the decomposition above, the number of unknowns is the double of the number of equations: $$ M_{ij}=A_{ij}+A_{ji}, $$ with $i\neq j$ , implying that is not possible to get an unique solution. Using the nilpotency property of $A$ , nonlinear equations such as $M^2=AA^T+A^TA$ or $A^TMA=0$ can be generated, but I can't see how using these nonlinear relations can lead to an unique solution.","Assume that a real-valued symmetric matrix with trace zero can be written as with . Given that is known, how (if possible) can be found? The diagonal elements of are just half those of , that is But for the non diagonal ones, considering only the decomposition above, the number of unknowns is the double of the number of equations: with , implying that is not possible to get an unique solution. Using the nilpotency property of , nonlinear equations such as or can be generated, but I can't see how using these nonlinear relations can lead to an unique solution.","M 
M = A + A^T,
 A^2=0 M A A M 
A_{ii}=M_{ii}/2.
 
M_{ij}=A_{ij}+A_{ji},
 i\neq j A M^2=AA^T+A^TA A^TMA=0","['linear-algebra', 'matrices', 'nonlinear-system', 'nilpotence']"
90,Factoring Constants out of Matrix Products,Factoring Constants out of Matrix Products,,"This seems like a rather trivial question, but I just want to know first if I am correct that this step is valid and, if so, if there's a name to it. It surely isn't linearity, unless I am mistaken. Suppose we have matrices A and B. Further, B is a scalar multiple of some other matrix, so perhaps we have $B = 3C$ . My question is, would it be valid in multiplying $A$ and $B$ to pull out the constant $3$ and then multiply it by the product of $A$ and $B$ ? In  other words: $$AB = A(3C) = 3(AC)$$ In other words, it is necessarily the case that the above must hold? Is there a name for such a property? Thanks. I apologize for how elementary this surely is.","This seems like a rather trivial question, but I just want to know first if I am correct that this step is valid and, if so, if there's a name to it. It surely isn't linearity, unless I am mistaken. Suppose we have matrices A and B. Further, B is a scalar multiple of some other matrix, so perhaps we have . My question is, would it be valid in multiplying and to pull out the constant and then multiply it by the product of and ? In  other words: In other words, it is necessarily the case that the above must hold? Is there a name for such a property? Thanks. I apologize for how elementary this surely is.",B = 3C A B 3 A B AB = A(3C) = 3(AC),[]
91,Understanding the constraints to find a $2\times 2$ non-zero matrix $A$ such that $A^2=0$,Understanding the constraints to find a  non-zero matrix  such that,2\times 2 A A^2=0,"Using the rules for matrix multiplication, I have found four algebraic equations as ""constraints"" for getting every element in the resulting matrix to be zero. Assuming that the elements in the resulting matrix are $a, b, c$ and $d$ ( $a, b$ are the elements in the first row and $c, d$ are the elements in the second). The equations are therefore; $a^2 + bc = 0$ $ab + bd = 0$ $ac + cd = 0$ $d^2 + bc = 0$ I have found from the four equations that $a=-d$ and that $bc=-a^2$ hence $bc$ must be  a negative quantity thus $b$ and $c$ have opposite signs but that does not seem to be enough to guarantee that the resulting matrix is always zero. Now, I do not know how to think about this problem; How do I find more constraints from these equations? And how do I know that I have found all possible constraints? Why is that when I square one equation, some how I get a new constraint? Shouldn't the four equations be enough to determine the exact conditions for $a, b, c$ and $d$ ?","Using the rules for matrix multiplication, I have found four algebraic equations as ""constraints"" for getting every element in the resulting matrix to be zero. Assuming that the elements in the resulting matrix are and ( are the elements in the first row and are the elements in the second). The equations are therefore; I have found from the four equations that and that hence must be  a negative quantity thus and have opposite signs but that does not seem to be enough to guarantee that the resulting matrix is always zero. Now, I do not know how to think about this problem; How do I find more constraints from these equations? And how do I know that I have found all possible constraints? Why is that when I square one equation, some how I get a new constraint? Shouldn't the four equations be enough to determine the exact conditions for and ?","a, b, c d a, b c, d a^2 + bc = 0 ab + bd = 0 ac + cd = 0 d^2 + bc = 0 a=-d bc=-a^2 bc b c a, b, c d","['linear-algebra', 'matrices', 'analysis']"
92,What is actually a Determinant?,What is actually a Determinant?,,"The way I've been introduced to determinants is that if there is a system of  two linear equations then we can represent the coefficients of the variables and the constants in the form of a matrix. Now if we plot the matrices on the coordinate system then we will get a parallelogram and if we calculate the area of the parallelogram then we will get the determinant of the given matrix. For eg if A is the matrix then its determinant will be: $ad-cb$ . i.e. |A|= $ad-cb$ . if A= $\begin{bmatrix}a & b\\c & d\end{bmatrix}$ Now the questions I want to ask: 1)What is a determinant actually what does it tells us about a system of equations? 2)The area found by the formula $ad-cb$ , how is it telling us a determinant? Basically how the area of parallelogram telling the value of determinant? 3)In my book its given that: system of equations has a unique solution or not is determined by the number of ab-cd .What does this mean?","The way I've been introduced to determinants is that if there is a system of  two linear equations then we can represent the coefficients of the variables and the constants in the form of a matrix. Now if we plot the matrices on the coordinate system then we will get a parallelogram and if we calculate the area of the parallelogram then we will get the determinant of the given matrix. For eg if A is the matrix then its determinant will be: . i.e. |A|= . if A= Now the questions I want to ask: 1)What is a determinant actually what does it tells us about a system of equations? 2)The area found by the formula , how is it telling us a determinant? Basically how the area of parallelogram telling the value of determinant? 3)In my book its given that: system of equations has a unique solution or not is determined by the number of ab-cd .What does this mean?",ad-cb ad-cb \begin{bmatrix}a & b\\c & d\end{bmatrix} ad-cb,"['matrices', 'systems-of-equations', 'determinant']"
93,Show a specially defined matrix is positive definite,Show a specially defined matrix is positive definite,,"Let $E_1, ..., E_n$ be non empty finite sets. Show that the matrix $A = (A_{ij})_{1 \leq i, j \leq n}$ defined by $A_{ij} = \dfrac{|E_i \cap E_j|}{|E_i \cup E_j|}$ , is positive semi-definite. This is part 5 of Exercise 1 in http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/homework/2019mva/hw.pdf . I start with the definition but it doesn't seem very promising. Any hint, please?","Let be non empty finite sets. Show that the matrix defined by , is positive semi-definite. This is part 5 of Exercise 1 in http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/homework/2019mva/hw.pdf . I start with the definition but it doesn't seem very promising. Any hint, please?","E_1, ..., E_n A = (A_{ij})_{1 \leq i, j \leq n} A_{ij} = \dfrac{|E_i \cap E_j|}{|E_i \cup E_j|}","['linear-algebra', 'combinatorics', 'matrices', 'positive-definite', 'symmetric-matrices']"
94,Kernel and image of matrix: What are they? Why do they exist?,Kernel and image of matrix: What are they? Why do they exist?,,"I've been trying to get an understanding of the Kernel of image of matrices. I'm studying them in college right now, but the problem is, while I can find a ton of resources on how to find them given a matrix by following steps, I haven't been able to find anything that explains what they are intuitively. Also why have them in the first place? I hope someone who understands these concepts better can help me.","I've been trying to get an understanding of the Kernel of image of matrices. I'm studying them in college right now, but the problem is, while I can find a ton of resources on how to find them given a matrix by following steps, I haven't been able to find anything that explains what they are intuitively. Also why have them in the first place? I hope someone who understands these concepts better can help me.",,"['linear-algebra', 'matrices', 'definition', 'intuition', 'motivation']"
95,A bizarre matrix product -- does it have a name?,A bizarre matrix product -- does it have a name?,,"Let $\mathbb{F}$ be a field, let $A\in\mathbb{F}^{n\times \ell m}$ , and let $B \in\mathbb{F}^{m\times p}$ . Let $A_j$ be the submatrix of $A$ consisting of columns $(j-1)m+1$ through $jm$ and define $$ A*B = \begin{bmatrix}A_1B \ | \ \dots \ | \ A_\ell B\end{bmatrix} \in \mathbb{F}^{n\times \ell p} $$ For example, $$ \begin{bmatrix} 1 & 2 & 3 & 4\\ 5 & 6 & 7 & 8 \end{bmatrix}*\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}\begin{bmatrix} 1 & 2 \\ 5 & 6 \end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\ | \ \begin{bmatrix} 3 & 4\\ 7 & 8 \end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} \end{bmatrix}=\begin{bmatrix}2 & 1 & 4 & 3\\ 6 & 5 & 8 & 7 \end{bmatrix} $$ Does this bizarre matrix product have a name? I would like to use it to write something like $XY = C*\text{vec}(X)$ where $C = [I\otimes \text{col}_1(Y)^T\dots I\otimes \text{col}_p(Y)^T]$","Let be a field, let , and let . Let be the submatrix of consisting of columns through and define For example, Does this bizarre matrix product have a name? I would like to use it to write something like where","\mathbb{F} A\in\mathbb{F}^{n\times \ell m} B \in\mathbb{F}^{m\times p} A_j A (j-1)m+1 jm 
A*B = \begin{bmatrix}A_1B \ | \ \dots \ | \ A_\ell B\end{bmatrix} \in \mathbb{F}^{n\times \ell p}
 
\begin{bmatrix}
1 & 2 & 3 & 4\\ 5 & 6 & 7 & 8
\end{bmatrix}*\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix}\begin{bmatrix}
1 & 2 \\ 5 & 6
\end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}\ | \ \begin{bmatrix}
3 & 4\\ 7 & 8
\end{bmatrix}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix} \end{bmatrix}=\begin{bmatrix}2 & 1 & 4 & 3\\ 6 & 5 & 8 & 7 \end{bmatrix}
 XY = C*\text{vec}(X) C = [I\otimes \text{col}_1(Y)^T\dots I\otimes \text{col}_p(Y)^T]","['linear-algebra', 'matrices']"
96,Is $[F(x)]^{-1}=F(-x)$ obvious for the given matrix $F(x)$?,Is  obvious for the given matrix ?,[F(x)]^{-1}=F(-x) F(x),$$F(x)=\begin{bmatrix}\cos x&-\sin x&0\\\sin x&\cos x&0\\0&0&1\end{bmatrix}$$ Is it very obvious (in the sense that without any calculations) that $[F(x)]^{-1}=F(-x)$ ? My book directly writes this without any explanation. How is this evident without calculation?,Is it very obvious (in the sense that without any calculations) that ? My book directly writes this without any explanation. How is this evident without calculation?,F(x)=\begin{bmatrix}\cos x&-\sin x&0\\\sin x&\cos x&0\\0&0&1\end{bmatrix} [F(x)]^{-1}=F(-x),['matrices']
97,A notion of adjacency-matrix symmetry?,A notion of adjacency-matrix symmetry?,,"I have a set of adjacency matrices that have a certain property, and I am trying to figure out what features of the adjacency matrix deliver that property and whether this property has a name. Here is the property: Let $A$ be an adjacency matrix for an undirected graph. Such a matrix has Property $X$ if all the diagonal elements of $A$ are the same (which of course they are trivially), all the diagonal elements of $A^2$ are the same, all the diagonal elements of $A^3$ are the same, and so on for all powers of $A$ . Another way of describing this property is that for each node $i$ , there are the same number of walks of length $k$ from node $i$ to itself for all $k$ . Thanks!","I have a set of adjacency matrices that have a certain property, and I am trying to figure out what features of the adjacency matrix deliver that property and whether this property has a name. Here is the property: Let be an adjacency matrix for an undirected graph. Such a matrix has Property if all the diagonal elements of are the same (which of course they are trivially), all the diagonal elements of are the same, all the diagonal elements of are the same, and so on for all powers of . Another way of describing this property is that for each node , there are the same number of walks of length from node to itself for all . Thanks!",A X A A^2 A^3 A i k i k,"['linear-algebra', 'matrices', 'graph-theory']"
98,Number of solutions $X$ to $AX=XB$ in $\mathbb F_2$,Number of solutions  to  in,X AX=XB \mathbb F_2,"It is a well-known theorem that in an arbitrary field $F$ , if $A$ is an $m\times m$ square matrix and $B$ is an $n\times n$ square matrix, then there is a unique $m\times n$ solution $X$ to the equation $$AX=XB$$ if and only if $A$ and $B$ share no eigenvalues. My question is this: what can be said about the number of solutions $X\in \mathbb F_2$ to the above equation? Is the number of nonzero solutions equal to the number of common eigenvalues? If so, how can one prove this?","It is a well-known theorem that in an arbitrary field , if is an square matrix and is an square matrix, then there is a unique solution to the equation if and only if and share no eigenvalues. My question is this: what can be said about the number of solutions to the above equation? Is the number of nonzero solutions equal to the number of common eigenvalues? If so, how can one prove this?",F A m\times m B n\times n m\times n X AX=XB A B X\in \mathbb F_2,"['linear-algebra', 'matrices', 'matrix-equations']"
99,Minimal polynomial of a matrix having only 1s on the counter diagonal,Minimal polynomial of a matrix having only 1s on the counter diagonal,,"Consider the matrix $A=a_{ij}$ where $$a_{ij}=\begin{cases}1\ \ \text{if}\ \ i+j=n+1\\0\ \ \text{otherwise}\end{cases}$$ . Then, what can be said about the minimal polynomial of the matrix $A$ . Note that one eigenvalue is easily found by taking the eigenvector $\begin{pmatrix}1\\1\\1\\\ldots\\\ldots\\\ldots\\1\end{pmatrix}$ . Any hints. Thanks beforehand.","Consider the matrix where . Then, what can be said about the minimal polynomial of the matrix . Note that one eigenvalue is easily found by taking the eigenvector . Any hints. Thanks beforehand.",A=a_{ij} a_{ij}=\begin{cases}1\ \ \text{if}\ \ i+j=n+1\\0\ \ \text{otherwise}\end{cases} A \begin{pmatrix}1\\1\\1\\\ldots\\\ldots\\\ldots\\1\end{pmatrix},"['linear-algebra', 'matrices', 'minimal-polynomials']"
