,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Alternate Definition of Derivative,Alternate Definition of Derivative,,"Looking at the ""secant line $\to$ tangent line"" motivation for the derivative I can't see any reason why one point on the graph of the function $f$ should be nailed down at the point in question.  Analytically, it's probably usually easier, but to me this alternate definition (which I think is equivalent) is more likely how I would have defined it: $$f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x-h)}{2h}$$  Is this alternate definition ever used?  Would it ever be more useful than the standard definition? EDIT: So this is a thing and it's called the symmetric derivative (thanks Ricky).  New question: can someone explain intuitively why this isn't equivalent to the usual definition of the derivative?","Looking at the ""secant line $\to$ tangent line"" motivation for the derivative I can't see any reason why one point on the graph of the function $f$ should be nailed down at the point in question.  Analytically, it's probably usually easier, but to me this alternate definition (which I think is equivalent) is more likely how I would have defined it: $$f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x-h)}{2h}$$  Is this alternate definition ever used?  Would it ever be more useful than the standard definition? EDIT: So this is a thing and it's called the symmetric derivative (thanks Ricky).  New question: can someone explain intuitively why this isn't equivalent to the usual definition of the derivative?",,"['calculus', 'derivatives']"
1,Conservative vector fields and Energy,Conservative vector fields and Energy,,"If one knows that $\vec{F}=m\vec{a}$ is a conservative force, which means that $\vec{F}=\nabla\phi$, and that energy $E(t) = \frac{1}{2}mv(t)^2 + \phi(\vec{x}(t))$, where $v(t) = \|\vec{v}(t)\|$ then it can be deduced that $E'(t) = m\frac{d}{dt}(v\cdot v) + \frac{d\phi}{dt} = m(\vec{v}\cdot \vec{a} - \vec{a}\cdot \vec{v}) = 0$. (Corrections were made as the discussion went).","If one knows that $\vec{F}=m\vec{a}$ is a conservative force, which means that $\vec{F}=\nabla\phi$, and that energy $E(t) = \frac{1}{2}mv(t)^2 + \phi(\vec{x}(t))$, where $v(t) = \|\vec{v}(t)\|$ then it can be deduced that $E'(t) = m\frac{d}{dt}(v\cdot v) + \frac{d\phi}{dt} = m(\vec{v}\cdot \vec{a} - \vec{a}\cdot \vec{v}) = 0$. (Corrections were made as the discussion went).",,"['derivatives', 'physics']"
2,Partial differentiation of the absolute value of a function containing complex coefficients.,Partial differentiation of the absolute value of a function containing complex coefficients.,,"I have a function, $H$, which is a dependent on a number of parameters, $\theta_{i=1,\ldots,n}$ and a number of complex coefficients. The function hence gives a complex quantity. $H$ is relatively simple to differentiate w.r.t $\theta_i$. What I am interested in finding is, \begin{equation} \sum_i \left| \frac{\partial|H|}{\partial \theta_i} \right|^2 \end{equation} Here is where I am unsure of myself: Because H is a function of theta \begin{equation} \frac{\partial|H|}{\partial \theta_i} = \frac{\partial|H|}{\partial H}\frac{\partial H}{\partial \theta_i} \end{equation} My question is: How do I calculate the differential of $|H|$ w.r.t $H$, given that H is complex (I think that this differential is undefined everywhere except zero?) Is there another way of expanding this in a form where I can calculate the differential. Note: although I can calculate dH/dtheta, it is difficult to directly calculate d(abs(H))/dtheta. Many thanks in advance. Edit If you will indulge me for one more question, Am I right in saying: \begin{equation} \frac{\partial H_R}{\partial \theta_i} = 0.5 \left(\frac{\partial H}{\partial \theta_i}  + \frac{\partial \overline{H}}{\partial \theta_i}  \right) \end{equation} So that I can use the partial differentials I have already calculated?","I have a function, $H$, which is a dependent on a number of parameters, $\theta_{i=1,\ldots,n}$ and a number of complex coefficients. The function hence gives a complex quantity. $H$ is relatively simple to differentiate w.r.t $\theta_i$. What I am interested in finding is, \begin{equation} \sum_i \left| \frac{\partial|H|}{\partial \theta_i} \right|^2 \end{equation} Here is where I am unsure of myself: Because H is a function of theta \begin{equation} \frac{\partial|H|}{\partial \theta_i} = \frac{\partial|H|}{\partial H}\frac{\partial H}{\partial \theta_i} \end{equation} My question is: How do I calculate the differential of $|H|$ w.r.t $H$, given that H is complex (I think that this differential is undefined everywhere except zero?) Is there another way of expanding this in a form where I can calculate the differential. Note: although I can calculate dH/dtheta, it is difficult to directly calculate d(abs(H))/dtheta. Many thanks in advance. Edit If you will indulge me for one more question, Am I right in saying: \begin{equation} \frac{\partial H_R}{\partial \theta_i} = 0.5 \left(\frac{\partial H}{\partial \theta_i}  + \frac{\partial \overline{H}}{\partial \theta_i}  \right) \end{equation} So that I can use the partial differentials I have already calculated?",,"['complex-analysis', 'derivatives', 'complex-numbers', 'partial-derivative']"
3,Chain rule for a function of a multivariable function,Chain rule for a function of a multivariable function,,"I've got confused in something which should not be too confusing. But... If we have some function  $f(r)$, where $r=\sqrt{x^2+y^2+z^2}$, then what is $f'(r) = \frac{df}{dr}$? I thought it would be this: $\frac{df}{dr} = \frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x}$, but something is not right here. What is confusing me so much is the absense of the part differentiating the variable $r$ itself. Say, if I had $f(r) = r^2$, I would say $f'(r) = 2r(\frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x})$. But $2r$ is $\frac{df}{dr}$... Please help :)","I've got confused in something which should not be too confusing. But... If we have some function  $f(r)$, where $r=\sqrt{x^2+y^2+z^2}$, then what is $f'(r) = \frac{df}{dr}$? I thought it would be this: $\frac{df}{dr} = \frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x}$, but something is not right here. What is confusing me so much is the absense of the part differentiating the variable $r$ itself. Say, if I had $f(r) = r^2$, I would say $f'(r) = 2r(\frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x})$. But $2r$ is $\frac{df}{dr}$... Please help :)",,['derivatives']
4,Even-order derivative of $y = x\sin (x)$,Even-order derivative of,y = x\sin (x),"How do I find the general formula for the even-order derivative of $y = x\sin (x)$? I tried using integration by parts and separation followed by mathematical induction, but I failed to obtain the correct answer, which is: $$\frac {\mathsf d^{2n}y}{\mathsf dx^{2n}} = (−1)^n(x\sin (x) − (2n\cos (x))).$$","How do I find the general formula for the even-order derivative of $y = x\sin (x)$? I tried using integration by parts and separation followed by mathematical induction, but I failed to obtain the correct answer, which is: $$\frac {\mathsf d^{2n}y}{\mathsf dx^{2n}} = (−1)^n(x\sin (x) − (2n\cos (x))).$$",,"['calculus', 'derivatives', 'induction']"
5,Application of Differentiation,Application of Differentiation,,"Prove that if the curve $y = x^3 + px + q$ is tangent to the x-axis, then $$4p^3 + 27q^2 = 0$$ I differentiated $y$ and obtained the value $3x^2 + p$. If the curve is tangent to the x-axis, it implies that $x=0$ (or is it $y = 0$?). How do I continue to prove the above statement? Thanks. If I substitute in $x=0$, I will obtain $y= q$? Are my above steps correct? Please guide me. Thank you so much!","Prove that if the curve $y = x^3 + px + q$ is tangent to the x-axis, then $$4p^3 + 27q^2 = 0$$ I differentiated $y$ and obtained the value $3x^2 + p$. If the curve is tangent to the x-axis, it implies that $x=0$ (or is it $y = 0$?). How do I continue to prove the above statement? Thanks. If I substitute in $x=0$, I will obtain $y= q$? Are my above steps correct? Please guide me. Thank you so much!",,['derivatives']
6,Simple Question about Derivative property,Simple Question about Derivative property,,"Suppose $f:[-1,1] \to \mathbb{R}$ is twice differentiable and $f(-1) = f(1) = 0$ and $f(0) = 1$. Prove that there exists $x_0 \in (-1,1)$ with $f''(x_0) = -2$. I tried establishing this with multiple applications of the MVT, and the intermediate value property of derivatives, but to no avail. I also tried with Taylor's Theorem to no avail. Trying to avoid integration of any sort. Any hints or suggestions welcome.","Suppose $f:[-1,1] \to \mathbb{R}$ is twice differentiable and $f(-1) = f(1) = 0$ and $f(0) = 1$. Prove that there exists $x_0 \in (-1,1)$ with $f''(x_0) = -2$. I tried establishing this with multiple applications of the MVT, and the intermediate value property of derivatives, but to no avail. I also tried with Taylor's Theorem to no avail. Trying to avoid integration of any sort. Any hints or suggestions welcome.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
7,Reference for gradient expression of a function on matrices,Reference for gradient expression of a function on matrices,,"I'm looking for a reference (I suppose the statement is correct) for the following formula: $$ \langle\nabla f(\rho)^\dagger,V\rangle=\left.\frac d{dt} f(\rho+tV)\right|_{t=0} $$ for any direction $V \in \mathbb{C}^{n \times n}$ . For a differentiable function $f : \mathbb{C}^{n \times n} \to \mathbb{C}$ and a matrix $\rho = (\rho_{ij})_{ij} \in \mathbb{C}^{n \times n}$ we define the gradient of $f$ in $\rho$ as $$ \nabla f(\rho)=\Big(\frac{d}{d\rho_{ij}} f(\rho)\Big)_{ij}. $$ I used the trace inner product $$ \langle A,B\rangle = \text{tr} \left(A^\dagger B\right). $$ Please tell me, if I'm wrong or give me a book, that I can cite in my thesis.","I'm looking for a reference (I suppose the statement is correct) for the following formula: for any direction . For a differentiable function and a matrix we define the gradient of in as I used the trace inner product Please tell me, if I'm wrong or give me a book, that I can cite in my thesis.","
\langle\nabla f(\rho)^\dagger,V\rangle=\left.\frac d{dt} f(\rho+tV)\right|_{t=0}
 V \in \mathbb{C}^{n \times n} f : \mathbb{C}^{n \times n} \to \mathbb{C} \rho = (\rho_{ij})_{ij} \in \mathbb{C}^{n \times n} f \rho 
\nabla f(\rho)=\Big(\frac{d}{d\rho_{ij}} f(\rho)\Big)_{ij}.
 
\langle A,B\rangle = \text{tr} \left(A^\dagger B\right).
","['analysis', 'derivatives', 'matrix-calculus', 'trace']"
8,Appling Jensen's inequality,Appling Jensen's inequality,,"I have to prove that for every $a,b,c \in \mathbb{R}$ $$1+\sqrt[3]{e^{2a}}\sqrt[5]{e^b}\sqrt[15]{e^{2c}}\le \sqrt[3]{(1+e^a)^2}\sqrt[5]{(1+e^b)}\sqrt[15]{(1+e^c)^2}.$$ We can prove that  $$\sqrt[3]{e^{2a}}\sqrt[5]{e^b}\sqrt[15]{e^{2c}}\le \sqrt[3]{(1+e^a)^2}\sqrt[5]{(1+e^b)}\sqrt[15]{(1+e^c)^2}$$ by appling $\ln$ function to both sides of inequality but how to prove the original one?","I have to prove that for every $a,b,c \in \mathbb{R}$ $$1+\sqrt[3]{e^{2a}}\sqrt[5]{e^b}\sqrt[15]{e^{2c}}\le \sqrt[3]{(1+e^a)^2}\sqrt[5]{(1+e^b)}\sqrt[15]{(1+e^c)^2}.$$ We can prove that  $$\sqrt[3]{e^{2a}}\sqrt[5]{e^b}\sqrt[15]{e^{2c}}\le \sqrt[3]{(1+e^a)^2}\sqrt[5]{(1+e^b)}\sqrt[15]{(1+e^c)^2}$$ by appling $\ln$ function to both sides of inequality but how to prove the original one?",,"['inequality', 'derivatives']"
9,Derivative of $\frac{x}{\|x\|}$ w.r.t. x where $x\in \mathbb{R}$ ($x \neq \theta_n$),Derivative of  w.r.t. x where  (),\frac{x}{\|x\|} x\in \mathbb{R} x \neq \theta_n,"I want to find the Hessian of a function. I have already computed the gradient of the function. So, I have to again differentiate it w.r.t. $x \in \mathbb{R}^n$ to get the hessian, but I am facing a bit difficulty to do that. My gradient is- $\nabla_x f(x) = \frac{x}{\|x\|}$, given $\|x\| \neq \theta_n$. Please help me to find the hessian. Advance thanks for your help.","I want to find the Hessian of a function. I have already computed the gradient of the function. So, I have to again differentiate it w.r.t. $x \in \mathbb{R}^n$ to get the hessian, but I am facing a bit difficulty to do that. My gradient is- $\nabla_x f(x) = \frac{x}{\|x\|}$, given $\|x\| \neq \theta_n$. Please help me to find the hessian. Advance thanks for your help.",,"['calculus', 'derivatives']"
10,Finding Tangent Line Using Limit Definition,Finding Tangent Line Using Limit Definition,,"I'm supposed to get the equation of the tangent line to the graph of $f(x)= \frac{8}{x}$ at the point $(2,4)$. I started with $$\frac{\frac{8}{x+h} - \frac{8}{x}}{h},$$ then I cross multiplied: $$\frac{8x - 8(x+h)}{ x(x+h)}$$ Where do I go from here?","I'm supposed to get the equation of the tangent line to the graph of $f(x)= \frac{8}{x}$ at the point $(2,4)$. I started with $$\frac{\frac{8}{x+h} - \frac{8}{x}}{h},$$ then I cross multiplied: $$\frac{8x - 8(x+h)}{ x(x+h)}$$ Where do I go from here?",,"['calculus', 'limits', 'derivatives']"
11,"If $f’(x) = \sin x + (\sin4x)(\cos x)$, then $f’(2x^2 + \pi/2) $is?","If , then is?",f’(x) = \sin x + (\sin4x)(\cos x) f’(2x^2 + \pi/2) ,"If $$f'(x) = \sin x + \sin4x \cdot \cos x,$$ then $$f'(2x^2 + \pi/2)$$ is? Given answer: $$4x\cos(2x^2) – 4x\sin(8x^2) \sin(2x^2)$$ I tried and I'm getting the answer as $\cos(2x^2) - \sin(8x^2)\sin(2x^2)$","If $$f'(x) = \sin x + \sin4x \cdot \cos x,$$ then $$f'(2x^2 + \pi/2)$$ is? Given answer: $$4x\cos(2x^2) – 4x\sin(8x^2) \sin(2x^2)$$ I tried and I'm getting the answer as $\cos(2x^2) - \sin(8x^2)\sin(2x^2)$",,"['trigonometry', 'derivatives']"
12,Check Differentiability,Check Differentiability,,"chech whether the function is differentiable at $x=0$ $$f(x)=\left\lbrace \begin{array}{cl} \arctan\frac{1}{\left | x \right |}, & x\neq 0 \\  \frac{\pi}{2}, &  x=0\\   \end{array}\right.$$ I feel that this is differentable at given point but I am unable to proceed with it.","chech whether the function is differentiable at $x=0$ $$f(x)=\left\lbrace \begin{array}{cl} \arctan\frac{1}{\left | x \right |}, & x\neq 0 \\  \frac{\pi}{2}, &  x=0\\   \end{array}\right.$$ I feel that this is differentable at given point but I am unable to proceed with it.",,[]
13,"The behavior of $f(x)=\alpha x+x^2\operatorname{sin}1/x$ for $x\neq 0$ near $0$, where $\alpha \ge 1$.","The behavior of  for  near , where .",f(x)=\alpha x+x^2\operatorname{sin}1/x x\neq 0 0 \alpha \ge 1,"Consider $\alpha \ge 1$. Let $f(x)=\alpha x+x^2\operatorname{sin}1/x$ for $x\neq 0$ and let $f(0)=0.$ In order to find the sign of $f'(x)$ when $\alpha \ge 1$ it is necessary to decide if $2x\operatorname{sin}1/x-\operatorname{cos}1/x$ is $\lt -1$ for any numbers $x$ close to $0$. It is a little more convenient to consider the function $g(y)=2(\operatorname{sin}y)/y-\operatorname{cos}y$ for $y\neq 0$; we want to know if $g(y)\lt -1$ for large $y$. This question is quite delicate; the most significant part of $g(y)$ is $-\operatorname{cos}y$, which does reach the value $-1$, but this happens only when $\operatorname{sin}y=0,$ and it is not at all clear whether $g$ itself can have values $\lt -1$. The obvious approach to this problem is to find the local minimum values of $g$. Unfortunately, it is impossible to solve the equation $g'(y)=0$ explicitly, so more ingenuity is required. (a) Show that if $g'(y)=0,$ then $$\operatorname{cos}y=(\operatorname{sin}y)(\frac{2-y^2}{2y}),$$ and conclude that $$g(y)=(\operatorname{sin}y)(\frac{2+y^2}{2y}).$$ (b) Now show that if $g'(y)=0, then $$\operatorname{sin}^2y=\frac{4y^2}{4+y^2},$$ and conclude that $$|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}.$$ (c) Using the fact that $(2+y^2)/\sqrt{4+y^4}\gt 1$, show that if $\alpha=1$, then $f$ is not increasing in any interval around $0$. (d) Using the fact that $\lim_{y\to \infty}(2+y^2)/\sqrt{4+y^4}=1,$ show that if $\alpha \gt 1$, then $f$ is increasing in some interval around $0$. I've worked through (a) and (b) but I have trouble solving (c) and (d). In fact, I can't even understand what the solution is saying. Below is what is written on the solution to this problem. (c) We have $$f'(x)=1+g(1/x).$$ Now we clearly have $g(y)\lt 0$ for arbitrarily large $y$ (since $g(y)$ is practically $-\operatorname{cos}y$ for large $y$), so for arbitrarily large $y$ we have $$g(y)\lt -\frac{2+y^2}{\sqrt{4+y^4}}\lt -1$$ by part (b). Thus $f'(x)\lt 0$ for arbitrarily small $x$, while we also have $f'(x)\gt 0$ for arbitrarily small $x$. First of all, I have confusion over the statement $g(y)\lt 0$ for arbitrarily large $y$. Usually I would interpret this as saying $g(y)\lt 0$ for all $y\lt a$ for some $a$, but I think in this case that is not true and the author merely means that there are negative values for very large $y$. More importantly, I don't understand why the strict inequality below follows. I think what the author suggests is since for points where $g'(y)=0$, $|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$, and at local minima we have $g'(y)=0$, there should be arbitrarily large values less than $-\frac{2+y^2}{\sqrt{4+y^4}}$. However, how do we know that there are even points with $g'(y)=0$?Also, even if there are such points, how do we know that they are local minimum points, or that local minimum of this function even exists anywhere? Moreover, why is the first inequality strict? Finally, the solution to (d) is We have $$f'(x)=\alpha + g(1/x).$$ For sufficiently large $y$ we have $g(y)\gt -\alpha.$ So for sufficiently small $x$ we have $f'(x)\gt 0.$ My understanding of this proof is that for large $y$ g(y) is practically $-cosy$ which is bounded below by $-1$ and since $-\alpha\lt -1$, we have $g(y)\gt -\alpha.$ From here the rest follows easily, but I don't see where the fact that $\lim_{y\to \infty}(2+y^2)/\sqrt{4+y^4}=1$ is used as the the problem said. I would greatly appreciate it if anyone could explain these points.","Consider $\alpha \ge 1$. Let $f(x)=\alpha x+x^2\operatorname{sin}1/x$ for $x\neq 0$ and let $f(0)=0.$ In order to find the sign of $f'(x)$ when $\alpha \ge 1$ it is necessary to decide if $2x\operatorname{sin}1/x-\operatorname{cos}1/x$ is $\lt -1$ for any numbers $x$ close to $0$. It is a little more convenient to consider the function $g(y)=2(\operatorname{sin}y)/y-\operatorname{cos}y$ for $y\neq 0$; we want to know if $g(y)\lt -1$ for large $y$. This question is quite delicate; the most significant part of $g(y)$ is $-\operatorname{cos}y$, which does reach the value $-1$, but this happens only when $\operatorname{sin}y=0,$ and it is not at all clear whether $g$ itself can have values $\lt -1$. The obvious approach to this problem is to find the local minimum values of $g$. Unfortunately, it is impossible to solve the equation $g'(y)=0$ explicitly, so more ingenuity is required. (a) Show that if $g'(y)=0,$ then $$\operatorname{cos}y=(\operatorname{sin}y)(\frac{2-y^2}{2y}),$$ and conclude that $$g(y)=(\operatorname{sin}y)(\frac{2+y^2}{2y}).$$ (b) Now show that if $g'(y)=0, then $$\operatorname{sin}^2y=\frac{4y^2}{4+y^2},$$ and conclude that $$|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}.$$ (c) Using the fact that $(2+y^2)/\sqrt{4+y^4}\gt 1$, show that if $\alpha=1$, then $f$ is not increasing in any interval around $0$. (d) Using the fact that $\lim_{y\to \infty}(2+y^2)/\sqrt{4+y^4}=1,$ show that if $\alpha \gt 1$, then $f$ is increasing in some interval around $0$. I've worked through (a) and (b) but I have trouble solving (c) and (d). In fact, I can't even understand what the solution is saying. Below is what is written on the solution to this problem. (c) We have $$f'(x)=1+g(1/x).$$ Now we clearly have $g(y)\lt 0$ for arbitrarily large $y$ (since $g(y)$ is practically $-\operatorname{cos}y$ for large $y$), so for arbitrarily large $y$ we have $$g(y)\lt -\frac{2+y^2}{\sqrt{4+y^4}}\lt -1$$ by part (b). Thus $f'(x)\lt 0$ for arbitrarily small $x$, while we also have $f'(x)\gt 0$ for arbitrarily small $x$. First of all, I have confusion over the statement $g(y)\lt 0$ for arbitrarily large $y$. Usually I would interpret this as saying $g(y)\lt 0$ for all $y\lt a$ for some $a$, but I think in this case that is not true and the author merely means that there are negative values for very large $y$. More importantly, I don't understand why the strict inequality below follows. I think what the author suggests is since for points where $g'(y)=0$, $|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$, and at local minima we have $g'(y)=0$, there should be arbitrarily large values less than $-\frac{2+y^2}{\sqrt{4+y^4}}$. However, how do we know that there are even points with $g'(y)=0$?Also, even if there are such points, how do we know that they are local minimum points, or that local minimum of this function even exists anywhere? Moreover, why is the first inequality strict? Finally, the solution to (d) is We have $$f'(x)=\alpha + g(1/x).$$ For sufficiently large $y$ we have $g(y)\gt -\alpha.$ So for sufficiently small $x$ we have $f'(x)\gt 0.$ My understanding of this proof is that for large $y$ g(y) is practically $-cosy$ which is bounded below by $-1$ and since $-\alpha\lt -1$, we have $g(y)\gt -\alpha.$ From here the rest follows easily, but I don't see where the fact that $\lim_{y\to \infty}(2+y^2)/\sqrt{4+y^4}=1$ is used as the the problem said. I would greatly appreciate it if anyone could explain these points.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
14,The sum of the lengths of the hypotenuse and another side of a right angled triangle is given.,The sum of the lengths of the hypotenuse and another side of a right angled triangle is given.,,"The sum of the lengths of the hypotenuse and another side of a right angled triangle is given.The area of the triangle will be maximum if the angle between them is: $(A)\frac{\pi}{6}\hspace{1cm}(B)\frac{\pi}{4}\hspace{1cm}(C)\frac{\pi}{3}\hspace{1cm}(D)\frac{5\pi}{12}$ Let $a,b$ are sides of a right triangle other than hypotenuse.Then given that $\sqrt{a^2+b^2}+a$=constant=$k$ Area of triangle=$\frac{1}{2}ab=\frac{1}{2}a\sqrt{(k-a)^2-a^2}$ and then?","The sum of the lengths of the hypotenuse and another side of a right angled triangle is given.The area of the triangle will be maximum if the angle between them is: $(A)\frac{\pi}{6}\hspace{1cm}(B)\frac{\pi}{4}\hspace{1cm}(C)\frac{\pi}{3}\hspace{1cm}(D)\frac{5\pi}{12}$ Let $a,b$ are sides of a right triangle other than hypotenuse.Then given that $\sqrt{a^2+b^2}+a$=constant=$k$ Area of triangle=$\frac{1}{2}ab=\frac{1}{2}a\sqrt{(k-a)^2-a^2}$ and then?",,"['calculus', 'geometry', 'derivatives', 'optimization']"
15,Finding the limit $\lim_{x\rightarrow 0^{+}}\frac{\int_{1}^{+\infty}\frac{e^{-xy}\quad-1}{y^3}dy}{\ln(1+x)}.$,Finding the limit,\lim_{x\rightarrow 0^{+}}\frac{\int_{1}^{+\infty}\frac{e^{-xy}\quad-1}{y^3}dy}{\ln(1+x)}.,"Finding the following limit:$$\lim_{x\rightarrow 0^{+}}\frac{\int_{1}^{+\infty}\frac{e^{-xy}\quad-1}{y^3}dy}{\ln(1+x)}.$$ To my way of thinking,L'Hopital's rule is useful to this question.Then $\frac{d}{dx}\int_{1}^{+\infty }\frac{e^{-xy}\quad-1}{y^3}dy=\int_{1}^{+\infty }\frac{\partial}{\partial x}(\frac{e^{-xy}\quad-1}{y^3})dy,\forall x\in [0,1],$ but it seems hopeless to  get the answer.I need some help,thanks.","Finding the following limit:$$\lim_{x\rightarrow 0^{+}}\frac{\int_{1}^{+\infty}\frac{e^{-xy}\quad-1}{y^3}dy}{\ln(1+x)}.$$ To my way of thinking,L'Hopital's rule is useful to this question.Then $\frac{d}{dx}\int_{1}^{+\infty }\frac{e^{-xy}\quad-1}{y^3}dy=\int_{1}^{+\infty }\frac{\partial}{\partial x}(\frac{e^{-xy}\quad-1}{y^3})dy,\forall x\in [0,1],$ but it seems hopeless to  get the answer.I need some help,thanks.",,"['calculus', 'real-analysis', 'integration', 'limits', 'derivatives']"
16,Minimizing long equation with hyperbolic functions,Minimizing long equation with hyperbolic functions,,In physics book that I am reading it is said that minimizing the expression $$\phi =  - N T k \log (2 \cosh(H \beta)) - \frac{J N}{2} z \tanh^2(H \beta) + H N \tanh(H \beta) $$ with respect to $H$ gives self-consistent expression: $$H_0=Jz\tanh(\beta H_0)$$ where $\beta=\frac{1}{kT}$. I want to obtain this by myself. My attempt is: $\frac{\partial \phi}{\partial H}=0$ where  \begin{align} \frac{\partial \phi}{\partial H} &= - N T \beta k \tanh(H \beta)  + J N \beta z (\tanh^3(H \beta) - \tanh(H \beta)) \\ & \hspace{5mm} + N \left(\frac{H \beta}{\cosh^{2}{\left (H \beta \right )}} + \tanh (H \beta ) \right) \end{align} Further I don't know what to do. Expression just seems too complicated. Maybe someone can advice me what to do next :).,In physics book that I am reading it is said that minimizing the expression $$\phi =  - N T k \log (2 \cosh(H \beta)) - \frac{J N}{2} z \tanh^2(H \beta) + H N \tanh(H \beta) $$ with respect to $H$ gives self-consistent expression: $$H_0=Jz\tanh(\beta H_0)$$ where $\beta=\frac{1}{kT}$. I want to obtain this by myself. My attempt is: $\frac{\partial \phi}{\partial H}=0$ where  \begin{align} \frac{\partial \phi}{\partial H} &= - N T \beta k \tanh(H \beta)  + J N \beta z (\tanh^3(H \beta) - \tanh(H \beta)) \\ & \hspace{5mm} + N \left(\frac{H \beta}{\cosh^{2}{\left (H \beta \right )}} + \tanh (H \beta ) \right) \end{align} Further I don't know what to do. Expression just seems too complicated. Maybe someone can advice me what to do next :).,,"['real-analysis', 'derivatives', 'optimization']"
17,third derivative of inverse function,third derivative of inverse function,,"Is my way of solving and my answer correct? Let $f(x)=x+\frac{x^2}{2}+\frac{x^3}{3}+\frac{x^4}{4}+\frac{x^5}{5}$ And $g(x)=f^{-1}(x)$ Find $g'''(0)$ My attempt: We know that $g'(x)=\frac{1}{f'(g(x))}$ $f'(x)=1+x+x^2+x^3+x^4=\frac{x^5-1}{x-1}$ $f'(g(x))=\frac{(g(x))^5-1}{(g(x))-1}$ $\Rightarrow g'(x)=\frac{(g(x))-1}{(g(x))^5-1}$ $\Rightarrow g'(x)[(g(x))^5-1]=g(x)-1$ Similarly differentiating again, $\Rightarrow g'(x)[5(g(x))^4]+[(g(x))^5-1]g''(x)=g'(x)$ Similarly differentiating again, $\Rightarrow g'(x)[20(g(x))^3g'(x)]+[5(g(x))^4]g''(x)+[(g(x))^5-1]g'''(x)+g''(x)[5(g(x))^4]=g''(x)$ Putting $x=0$, $g(0)=f^{-1}(0)=0$ $g'(0)=\frac{1}{f'(g(0))}=\frac{1}{f'(0)}=1$ similarly,$g''(0)=-1$ $\Rightarrow g'''(0)=1$","Is my way of solving and my answer correct? Let $f(x)=x+\frac{x^2}{2}+\frac{x^3}{3}+\frac{x^4}{4}+\frac{x^5}{5}$ And $g(x)=f^{-1}(x)$ Find $g'''(0)$ My attempt: We know that $g'(x)=\frac{1}{f'(g(x))}$ $f'(x)=1+x+x^2+x^3+x^4=\frac{x^5-1}{x-1}$ $f'(g(x))=\frac{(g(x))^5-1}{(g(x))-1}$ $\Rightarrow g'(x)=\frac{(g(x))-1}{(g(x))^5-1}$ $\Rightarrow g'(x)[(g(x))^5-1]=g(x)-1$ Similarly differentiating again, $\Rightarrow g'(x)[5(g(x))^4]+[(g(x))^5-1]g''(x)=g'(x)$ Similarly differentiating again, $\Rightarrow g'(x)[20(g(x))^3g'(x)]+[5(g(x))^4]g''(x)+[(g(x))^5-1]g'''(x)+g''(x)[5(g(x))^4]=g''(x)$ Putting $x=0$, $g(0)=f^{-1}(0)=0$ $g'(0)=\frac{1}{f'(g(0))}=\frac{1}{f'(0)}=1$ similarly,$g''(0)=-1$ $\Rightarrow g'''(0)=1$",,['derivatives']
18,Number of points of discontinuity,Number of points of discontinuity,,"Find the number of points where  $$f(\theta)=\int_{-1}^{1}\frac{\sin\theta dx}{1-2x\cos\theta +x^2}$$ is discontinuous where $\theta \in [0,2\pi]$ I am not able to find $f(\theta)$ in terms of $\theta$,$sin\theta$ in the numerator i can take out but $cos\theta$ in the denominator is troublesome.Can someone tell a good way to integate it or some other way to solve it.","Find the number of points where  $$f(\theta)=\int_{-1}^{1}\frac{\sin\theta dx}{1-2x\cos\theta +x^2}$$ is discontinuous where $\theta \in [0,2\pi]$ I am not able to find $f(\theta)$ in terms of $\theta$,$sin\theta$ in the numerator i can take out but $cos\theta$ in the denominator is troublesome.Can someone tell a good way to integate it or some other way to solve it.",,"['calculus', 'derivatives']"
19,Matrix Differentiation,Matrix Differentiation,,"Consider a differentiable function $f: \mathbb R \to \mathbb R$ and two $p\times 1$ vectors $x$ and $\theta$. Then define a new function as follows. $$ f\left( x^T\theta \right)x. $$ Now we want to find the derivative of the new function with respect to $\theta$. $$ \frac{d}{d\theta}f\left( x^T\theta \right)x = f'\left( x^T\theta \right) \frac{d}{d\theta}\left( x^T\theta \right) x = f'\left( x^T\theta \right)  x x. $$ So $f'\left( x^T\theta \right)$ is a scalar. $\frac{d}{d\theta}\left( x^T\theta \right)$ should a column vector. However, this clearly is not right since it does not make sense to have $xx$. The correct answer is $xx^T$. However, I cannot see why this should be the case. Is this some kind of convention? Could anyone help me, please? Thank you!","Consider a differentiable function $f: \mathbb R \to \mathbb R$ and two $p\times 1$ vectors $x$ and $\theta$. Then define a new function as follows. $$ f\left( x^T\theta \right)x. $$ Now we want to find the derivative of the new function with respect to $\theta$. $$ \frac{d}{d\theta}f\left( x^T\theta \right)x = f'\left( x^T\theta \right) \frac{d}{d\theta}\left( x^T\theta \right) x = f'\left( x^T\theta \right)  x x. $$ So $f'\left( x^T\theta \right)$ is a scalar. $\frac{d}{d\theta}\left( x^T\theta \right)$ should a column vector. However, this clearly is not right since it does not make sense to have $xx$. The correct answer is $xx^T$. However, I cannot see why this should be the case. Is this some kind of convention? Could anyone help me, please? Thank you!",,"['derivatives', 'self-learning']"
20,limits of integration and derivative,limits of integration and derivative,,"I have an integral that gives $$\left[\frac{d}{dx}[f(x)]\right]_a^b$$ is it possible in general to claim that this is equal to  $$\frac{d}{dx}\left(\left[f(x)\right]_a^b\right)\text{ ?}$$ If not in general, are there any sufficient conditions to claim this? Thanks!!","I have an integral that gives $$\left[\frac{d}{dx}[f(x)]\right]_a^b$$ is it possible in general to claim that this is equal to  $$\frac{d}{dx}\left(\left[f(x)\right]_a^b\right)\text{ ?}$$ If not in general, are there any sufficient conditions to claim this? Thanks!!",,"['integration', 'derivatives', 'definite-integrals']"
21,How to solve $x=\sin^{-1}(\frac{1}{2\sqrt{x}})$,How to solve,x=\sin^{-1}(\frac{1}{2\sqrt{x}}),"I was setting a question when I came across a problem. The question was: Suppose I have a function $y=e^{1+\cos(x)+\sqrt{x}}$. (A) Locate its turning points by taking derivatives and sketch its graph in the interval $[0, 20]$ (B) Given that I randomly choose 2 points on the graph such that their $x$-coordinates are 2 units apart. Find the probability that within that 2 points contains an extremum point (maxima/minima). I realise part (B) is relatively easy to solve once we have found the extremum points and identify the range of values of the interval that contains the extremum, but my problem is to solve and find its extremum. $$\frac{dy}{dx}=e^{1+\cos(x)+\sqrt{x}}(-\sin(x)+\frac{1}{2\sqrt{x}})=0$$ $$x=\sin^{-1}(\frac{1}{2\sqrt{x}})$$ My problem is that I am unable to solve this equation. Does anyone know how to solve such equations in general without the help of any software? Thank you!","I was setting a question when I came across a problem. The question was: Suppose I have a function $y=e^{1+\cos(x)+\sqrt{x}}$. (A) Locate its turning points by taking derivatives and sketch its graph in the interval $[0, 20]$ (B) Given that I randomly choose 2 points on the graph such that their $x$-coordinates are 2 units apart. Find the probability that within that 2 points contains an extremum point (maxima/minima). I realise part (B) is relatively easy to solve once we have found the extremum points and identify the range of values of the interval that contains the extremum, but my problem is to solve and find its extremum. $$\frac{dy}{dx}=e^{1+\cos(x)+\sqrt{x}}(-\sin(x)+\frac{1}{2\sqrt{x}})=0$$ $$x=\sin^{-1}(\frac{1}{2\sqrt{x}})$$ My problem is that I am unable to solve this equation. Does anyone know how to solve such equations in general without the help of any software? Thank you!",,"['calculus', 'probability', 'trigonometry', 'derivatives', 'inverse']"
22,Is a function a derivative?,Is a function a derivative?,,"I'm reading introductory calculus and I find that 'function' tends to be defined by what it does rather than what it is. If $y = f(x)$, then surely the value of $y$ is dependent on that of $x$, i.e. derived from that of $x$; hence $y$ is a derivative and thus 'function of' equals 'derivative of'. And yet the following chapters appear to deal with ""derivative of a function"", as if the word ""derivative"" is reserved for a particular type or phase of ""derivation"". Can someone please clarify this for me?","I'm reading introductory calculus and I find that 'function' tends to be defined by what it does rather than what it is. If $y = f(x)$, then surely the value of $y$ is dependent on that of $x$, i.e. derived from that of $x$; hence $y$ is a derivative and thus 'function of' equals 'derivative of'. And yet the following chapters appear to deal with ""derivative of a function"", as if the word ""derivative"" is reserved for a particular type or phase of ""derivation"". Can someone please clarify this for me?",,"['calculus', 'derivatives', 'terminology']"
23,A mean value theorem involving two functions [duplicate],A mean value theorem involving two functions [duplicate],,"This question already has answers here : Generalized mean value theorem (3 answers) Closed 8 years ago . Let $f,g:[a,b] \rightarrow \mathbb{R}$ be continuous in $[a,b]$ and differentiable in $(a,b)$. Prove that there is a point $c \in (a,b)$ such that:   $$[f(b)-f(a)]g'(c) = [g(b)-g(a)]f'(c).$$ I think this is a straightforward application of the mean value theorem to a clever function. Am I right? If so, which function? Thanks :)","This question already has answers here : Generalized mean value theorem (3 answers) Closed 8 years ago . Let $f,g:[a,b] \rightarrow \mathbb{R}$ be continuous in $[a,b]$ and differentiable in $(a,b)$. Prove that there is a point $c \in (a,b)$ such that:   $$[f(b)-f(a)]g'(c) = [g(b)-g(a)]f'(c).$$ I think this is a straightforward application of the mean value theorem to a clever function. Am I right? If so, which function? Thanks :)",,"['calculus', 'real-analysis', 'derivatives']"
24,Calculate the derivative,Calculate the derivative,,"I'm asked to find the derivative of the following: $$ \sqrt[4]{x} + \sqrt[3]{3x} $$ I attempted to solve the problem and got the following result, but my book says I am wrong. $$ \frac 14x^{-\frac 34} + x^{-\frac 23} $$ When I checked I got the following as the correct answer. Why is it 3x and not x? What am I doing wrong here? $$ \frac 14x^{-\frac 34} + 3x^{-\frac 23} $$","I'm asked to find the derivative of the following: $$ \sqrt[4]{x} + \sqrt[3]{3x} $$ I attempted to solve the problem and got the following result, but my book says I am wrong. $$ \frac 14x^{-\frac 34} + x^{-\frac 23} $$ When I checked I got the following as the correct answer. Why is it 3x and not x? What am I doing wrong here? $$ \frac 14x^{-\frac 34} + 3x^{-\frac 23} $$",,['calculus']
25,Solving for x using two derivatives and algebra.,Solving for x using two derivatives and algebra.,,"There are two things I don't understand about the following: "" Set these derivatives equal to each other and solve the resulting equation. $2\sqrt3\cos(x) = 2\sin(x)$ $= \sqrt3 = \tan(x)$ (since $\cos(x)$ can't be 0) On the specified domain $[0, 2\pi]$ that means that: $x = \frac13\pi$ and $x = \frac43\pi$ "" Why can't $\cos(x)$ be zero - surely it is 0 at $\frac12\pi$ and $\frac34\pi$, which are both in the domain. Why $x = \frac{\pi}3$ and $x = \frac{4\pi}3$ - this is a non-calculator problem$\ldots$","There are two things I don't understand about the following: "" Set these derivatives equal to each other and solve the resulting equation. $2\sqrt3\cos(x) = 2\sin(x)$ $= \sqrt3 = \tan(x)$ (since $\cos(x)$ can't be 0) On the specified domain $[0, 2\pi]$ that means that: $x = \frac13\pi$ and $x = \frac43\pi$ "" Why can't $\cos(x)$ be zero - surely it is 0 at $\frac12\pi$ and $\frac34\pi$, which are both in the domain. Why $x = \frac{\pi}3$ and $x = \frac{4\pi}3$ - this is a non-calculator problem$\ldots$",,"['algebra-precalculus', 'derivatives']"
26,Why is 1 not a critical point for this function?,Why is 1 not a critical point for this function?,,"For the function $f(x) = \frac{x^2}{x-1}$, why is $1$ not a critical point, along with $0$ and $2$? Don't critical points include discontinuities?","For the function $f(x) = \frac{x^2}{x-1}$, why is $1$ not a critical point, along with $0$ and $2$? Don't critical points include discontinuities?",,"['calculus', 'derivatives']"
27,Derivative of a lemniscate at the left hand side,Derivative of a lemniscate at the left hand side,,How does $${d \over{dx}}(3(x^2+y^2)^2)$$ turn into $$12y(x^2+y^2){{dy} \over{dx}}+12x(x^2+y^2)$$? I'm having a hard time solving it algebraically without it turning into a huge polynomial.,How does $${d \over{dx}}(3(x^2+y^2)^2)$$ turn into $$12y(x^2+y^2){{dy} \over{dx}}+12x(x^2+y^2)$$? I'm having a hard time solving it algebraically without it turning into a huge polynomial.,,"['calculus', 'derivatives']"
28,How to apply fundamental theorem of calculus to multiple integrals,How to apply fundamental theorem of calculus to multiple integrals,,"I have the following problem at hand: $$\lim_{\epsilon \to 0}\dfrac{1}{4\epsilon^2}\int_{z-\epsilon}^{z+\epsilon}f(e_1,y) \int_{\frac{z+y}{2}-\epsilon}^{\frac{z+y}{2}+\epsilon}g(e_2,x) dx dy$$ $f$ and $g$ are well behaving, continuous functions. I have the following intuitive thought but I can't show it rigorously: I shift $1/2\epsilon$ inside the first integral. Then we have: $$\lim_{\epsilon \to 0}\dfrac{1}{2\epsilon}\int_{z-\epsilon}^{z+\epsilon}f(e_1,y) \dfrac{1}{2\epsilon}\int_{\frac{z+y}{2}-\epsilon}^{\frac{z+y}{2}+\epsilon}g(e_2,x) dx dy$$ If it would be valid to apply Fundamental theorem of calculus both for inner and outer integrals, we would have obtain $f(e_1,z)g(e_2,z)$ as the result. But I have no idea how to show to in a proper way. What should be done in this case?","I have the following problem at hand: $$\lim_{\epsilon \to 0}\dfrac{1}{4\epsilon^2}\int_{z-\epsilon}^{z+\epsilon}f(e_1,y) \int_{\frac{z+y}{2}-\epsilon}^{\frac{z+y}{2}+\epsilon}g(e_2,x) dx dy$$ $f$ and $g$ are well behaving, continuous functions. I have the following intuitive thought but I can't show it rigorously: I shift $1/2\epsilon$ inside the first integral. Then we have: $$\lim_{\epsilon \to 0}\dfrac{1}{2\epsilon}\int_{z-\epsilon}^{z+\epsilon}f(e_1,y) \dfrac{1}{2\epsilon}\int_{\frac{z+y}{2}-\epsilon}^{\frac{z+y}{2}+\epsilon}g(e_2,x) dx dy$$ If it would be valid to apply Fundamental theorem of calculus both for inner and outer integrals, we would have obtain $f(e_1,z)g(e_2,z)$ as the result. But I have no idea how to show to in a proper way. What should be done in this case?",,"['calculus', 'integration', 'limits', 'derivatives']"
29,Are the extrema of this function global or local?,Are the extrema of this function global or local?,,"Last question about this function, I promise. The function $f: \mathbb R \rightarrow \mathbb R$ is given by $$f(x) = \begin{cases}  \frac{x^2+5x+7}{x+3} & \mathrm{for} \; x < -3 \\ 0 & \mathrm{for} \; x = -3 \\ \frac{x^2+5x+7}{x+3} & \mathrm{for} \; -3 < x < -2 \\ 1 & \mathrm{for} \; x = -2 \\ -x-e^{-x}+e^2-1 & \mathrm{for} \; x > -2  \end{cases}$$ Find all extrema of the function, and also determine, without the use of a calculator, whether they are global or local. My Attempt: The derivative is given by: $$f'(x) = \begin{cases}  \frac{x^2+6x+8}{(x+3)^2} & \mathrm{for} \; x < -3 \\ \mathrm{undefined} & \mathrm{for} \; x = -3 \\ \frac{x^2+6x+8}{(x+3)^2} & \mathrm{for} \; -3 < x < -2 \\ \mathrm{undefined} & \mathrm{for} \; x = -2 \\ -1+e^{-x} & \mathrm{for} \; x > -2  \end{cases}$$ Solve the first one: \begin{align*} \frac{x^2+6x+8}{x^2+6x+9} &= 0 \\ x^2+6x+8 &= 0 \\ (x+2)(x+4) &= 0 \\ x=-2 &\vee x=-4 \end{align*} Only $x=-4$ is in the domain. The second derivative is $$f''(x)=\frac{2x+6}{(x+3)^4}$$ Substitute in $x=-4$ to get $f''(x)=-2$, so it is a minimum. Solve the second one: \begin{align*} -1+e^{-x} &= 0 \\ e^{-x} &= 1 \\ -x&=0 \\ x &= 0 \end{align*} Which is in the domain. Taking the second derivative gives $-e^{-x}$, and $-e^{-0}=-1$, so it is a minimum. How to determine whether the minima are global or local? Thank you in advance. Edit : They are maxima, not minima.","Last question about this function, I promise. The function $f: \mathbb R \rightarrow \mathbb R$ is given by $$f(x) = \begin{cases}  \frac{x^2+5x+7}{x+3} & \mathrm{for} \; x < -3 \\ 0 & \mathrm{for} \; x = -3 \\ \frac{x^2+5x+7}{x+3} & \mathrm{for} \; -3 < x < -2 \\ 1 & \mathrm{for} \; x = -2 \\ -x-e^{-x}+e^2-1 & \mathrm{for} \; x > -2  \end{cases}$$ Find all extrema of the function, and also determine, without the use of a calculator, whether they are global or local. My Attempt: The derivative is given by: $$f'(x) = \begin{cases}  \frac{x^2+6x+8}{(x+3)^2} & \mathrm{for} \; x < -3 \\ \mathrm{undefined} & \mathrm{for} \; x = -3 \\ \frac{x^2+6x+8}{(x+3)^2} & \mathrm{for} \; -3 < x < -2 \\ \mathrm{undefined} & \mathrm{for} \; x = -2 \\ -1+e^{-x} & \mathrm{for} \; x > -2  \end{cases}$$ Solve the first one: \begin{align*} \frac{x^2+6x+8}{x^2+6x+9} &= 0 \\ x^2+6x+8 &= 0 \\ (x+2)(x+4) &= 0 \\ x=-2 &\vee x=-4 \end{align*} Only $x=-4$ is in the domain. The second derivative is $$f''(x)=\frac{2x+6}{(x+3)^4}$$ Substitute in $x=-4$ to get $f''(x)=-2$, so it is a minimum. Solve the second one: \begin{align*} -1+e^{-x} &= 0 \\ e^{-x} &= 1 \\ -x&=0 \\ x &= 0 \end{align*} Which is in the domain. Taking the second derivative gives $-e^{-x}$, and $-e^{-0}=-1$, so it is a minimum. How to determine whether the minima are global or local? Thank you in advance. Edit : They are maxima, not minima.",,"['calculus', 'derivatives', 'proof-verification']"
30,Derivatives of real part of function,Derivatives of real part of function,,My physics textbook gives me the complex form equation of simple harmonic motion as: $$z = Ae^{i(\omega _{o}t+\phi )}$$ and then defines $$ x = Re (z) $$ From there they argue that $$  \frac{\partial x}{\partial t} = Re  \frac{\partial z}{\partial t}$$ and then they go on to state that:  $$ \frac{\partial Z}{\partial t} = i\omega _oz $$  Can someone explain how they are going from the 3rd equation to the 4th equation?,My physics textbook gives me the complex form equation of simple harmonic motion as: $$z = Ae^{i(\omega _{o}t+\phi )}$$ and then defines $$ x = Re (z) $$ From there they argue that $$  \frac{\partial x}{\partial t} = Re  \frac{\partial z}{\partial t}$$ and then they go on to state that:  $$ \frac{\partial Z}{\partial t} = i\omega _oz $$  Can someone explain how they are going from the 3rd equation to the 4th equation?,,"['derivatives', 'complex-numbers']"
31,L'Hôpital's rule exercise with natural log function,L'Hôpital's rule exercise with natural log function,,"I'm looking for some advice on the following exercise: $$\lim_{x \to 0^+}{\ln{(\frac{1}{x}})}^x$$ This is my work so far: $$\lim_{x \to 0^+}{\ln{(\frac{1}{x}})}^x = \lim_{x \to 0^+}{x\cdot\ln{(\frac{1}{x}})} = \lim_{x \to 0^+}{\frac{\ln{\frac{1}{x}}}{\frac{1}{x}}}$$ Taking the derivatives of the top and bottom: $$\frac{d}{dx}\ln{\frac{1}{x}}=-\frac{1}{x}\,\,\text{and}\,\,\frac{d}{dx}\frac{1}{x}=-\frac{1}{x^2}$$ So: $$\lim_{x \to 0^+}{\frac{-\frac{1}{x}}{-\frac{1}{x^2}}} = \lim_{x \to 0^+}{\frac{x^2}{x}}=\lim_{x \to 0^+}{x} = 0$$ ... Which doesn't quite add up.  I'm thinking, I should rewrite the equation in the form of $e^x$ as the limit would then approach $1$, which is the correct answer.  However, I'm a bit stuck on that front. Any suggestions would be greatly appreciated!","I'm looking for some advice on the following exercise: $$\lim_{x \to 0^+}{\ln{(\frac{1}{x}})}^x$$ This is my work so far: $$\lim_{x \to 0^+}{\ln{(\frac{1}{x}})}^x = \lim_{x \to 0^+}{x\cdot\ln{(\frac{1}{x}})} = \lim_{x \to 0^+}{\frac{\ln{\frac{1}{x}}}{\frac{1}{x}}}$$ Taking the derivatives of the top and bottom: $$\frac{d}{dx}\ln{\frac{1}{x}}=-\frac{1}{x}\,\,\text{and}\,\,\frac{d}{dx}\frac{1}{x}=-\frac{1}{x^2}$$ So: $$\lim_{x \to 0^+}{\frac{-\frac{1}{x}}{-\frac{1}{x^2}}} = \lim_{x \to 0^+}{\frac{x^2}{x}}=\lim_{x \to 0^+}{x} = 0$$ ... Which doesn't quite add up.  I'm thinking, I should rewrite the equation in the form of $e^x$ as the limit would then approach $1$, which is the correct answer.  However, I'm a bit stuck on that front. Any suggestions would be greatly appreciated!",,"['calculus', 'limits', 'derivatives', 'logarithms']"
32,Derivatives and Linear transformations,Derivatives and Linear transformations,,"Let G be a non-empty open connected set in $R^n$, $f$ be a differentiable function from $G$ into $R$, and $A$ be a linear transformation from $R^n$ to $R$. If $f$ '($a$)=$A$ for all $a$ in $G$, find $f$ and prove your answer. I thought of $f$ as being the same as the linear transformation, i.e. $f(x)$=$A(x)$. Is this true?","Let G be a non-empty open connected set in $R^n$, $f$ be a differentiable function from $G$ into $R$, and $A$ be a linear transformation from $R^n$ to $R$. If $f$ '($a$)=$A$ for all $a$ in $G$, find $f$ and prove your answer. I thought of $f$ as being the same as the linear transformation, i.e. $f(x)$=$A(x)$. Is this true?",,"['derivatives', 'linear-transformations']"
33,Modulus differentiation,Modulus differentiation,,"For a Java project, I need to find a way to compute the derivate of a modulus function like $$f(x) = g(x) \pmod{h(x)}$$ for any value of $x$. I know that the modulus function is discontinuous. If there is no way to compute that, do you have any suggestion to obtain a coherent result?","For a Java project, I need to find a way to compute the derivate of a modulus function like $$f(x) = g(x) \pmod{h(x)}$$ for any value of $x$. I know that the modulus function is discontinuous. If there is no way to compute that, do you have any suggestion to obtain a coherent result?",,"['derivatives', 'modular-arithmetic']"
34,Total derivative for a polynomial,Total derivative for a polynomial,,"I refer to Rudin's (Principles of Mathematical analysis, 3rd ed.) definition of differentiability: Suppose E is an open set in $R^n$ and f maps E into $R^m$ and $x \in E$. If there exists a linear transformation A of $R^n$ into $R^m$ such that  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x) -Ah|  } {|h|} = 0,$$ then we say that f is differentiable at x and we write  $f'(x) = A$. With this definition and assuming the $l_2$ norm, $f'(x)$ for $(x_1^2 + x_2^2)$ is given by $(2x_1, 2x_2)$.  since  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1h_1 + x_2 h_2)}{|h|} = \lim_{|h| \rightarrow 0} \frac{(2x_1, 2x_2)'(h_1, h_2)}{|h|}.$$ However I could not find $f'(x)$ for $(x_1 + x_2)^2$ with this definition.  I get  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1 + x_2)(h_1 + h_2) + h_1h_2}{|h|}$$ The cross term $h_1 h_2$ becomes a problem, if it were not present, I would write $f'(x) = (2(x_1+x_2), 2(x_1+x_2))$. I have read elsewhere on this forum that polynomials are differentiable. I would be very thankful I someone were to tell me how do I proceed to find the total derivative, $f'(x)$ for a given polynomial function.","I refer to Rudin's (Principles of Mathematical analysis, 3rd ed.) definition of differentiability: Suppose E is an open set in $R^n$ and f maps E into $R^m$ and $x \in E$. If there exists a linear transformation A of $R^n$ into $R^m$ such that  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x) -Ah|  } {|h|} = 0,$$ then we say that f is differentiable at x and we write  $f'(x) = A$. With this definition and assuming the $l_2$ norm, $f'(x)$ for $(x_1^2 + x_2^2)$ is given by $(2x_1, 2x_2)$.  since  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1h_1 + x_2 h_2)}{|h|} = \lim_{|h| \rightarrow 0} \frac{(2x_1, 2x_2)'(h_1, h_2)}{|h|}.$$ However I could not find $f'(x)$ for $(x_1 + x_2)^2$ with this definition.  I get  $$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1 + x_2)(h_1 + h_2) + h_1h_2}{|h|}$$ The cross term $h_1 h_2$ becomes a problem, if it were not present, I would write $f'(x) = (2(x_1+x_2), 2(x_1+x_2))$. I have read elsewhere on this forum that polynomials are differentiable. I would be very thankful I someone were to tell me how do I proceed to find the total derivative, $f'(x)$ for a given polynomial function.",,"['derivatives', 'polynomials']"
35,"Find when the population is growing the fastest, under the logistic model","Find when the population is growing the fastest, under the logistic model",,The population $P$ of an island $y$ years after colonization is given by the function: $\displaystyle P = \frac{250}{1 + 4e^{-0.01y}}$. After how many years was the population growing the fastest? I tried taking the second derivative and setting it equal to zero. From there I solved for $y$ but kept getting $y = 0$. Which doesn't make too much sense so I was wondering how anyone would go about answering this problem.,The population $P$ of an island $y$ years after colonization is given by the function: $\displaystyle P = \frac{250}{1 + 4e^{-0.01y}}$. After how many years was the population growing the fastest? I tried taking the second derivative and setting it equal to zero. From there I solved for $y$ but kept getting $y = 0$. Which doesn't make too much sense so I was wondering how anyone would go about answering this problem.,,"['calculus', 'derivatives', 'logarithms', 'exponential-function']"
36,"Proving that a positive derivative means the function is smaller ""to the left"" and larger ""to the right"" for certain values","Proving that a positive derivative means the function is smaller ""to the left"" and larger ""to the right"" for certain values",,"I was trying to prove that if $g$ is differentiable on an open interval $I$ with $a\in I$ and $g'(a)>0$ then we can find $x<a$  for which $g(x)<g(a)$ and $y>a$ for which $g(y)>g(a)$, I think I understand limits correctly but it confuses me when it comes to derivative so I just wanted to make sure I understand it correctly. My proof: Let $g'(a)=K>0$, that means that $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=K$$ Therefore given $\epsilon=\frac{k}{2}$ we can find $\delta >0$ such that for $x\in I$ and $x\in (a-\delta , a+\delta )\setminus \{a\}$ $$\left|\frac{f(x)-f(a)}{x-a}-K\right|<\frac{K}{2}$$ (This is the part I'm not sure about).. hence $$-\frac{K}{2}<\frac{f(x)-f(a)}{x-a}-K<\frac{K}{2}$$ hence, $$\frac{K}{2}(x-a)<f(x)-f(a)<\frac{3K}{2}(x-a)$$ hence, $$\frac{K}{2}(x-a)+f(a)<f(x)<\frac{3K}{2}(x-a)+f(a)$$ Therefore if we choose $x\in (a-\delta ,a)$ and $y\in (a,a+\delta)$ (which we can find since $I$ is open) we get that $$f(x)<\frac{3K}{2}(x-a)+f(a)<f(a)\qquad \text{As }x-a<0$$ $$f(a)<\frac{K}{2}(y-a)+f(a)<f(y)\qquad \text{As }y-a>0$$ Is this proof correct? Also, I really got lost in trying to write the title :P","I was trying to prove that if $g$ is differentiable on an open interval $I$ with $a\in I$ and $g'(a)>0$ then we can find $x<a$  for which $g(x)<g(a)$ and $y>a$ for which $g(y)>g(a)$, I think I understand limits correctly but it confuses me when it comes to derivative so I just wanted to make sure I understand it correctly. My proof: Let $g'(a)=K>0$, that means that $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=K$$ Therefore given $\epsilon=\frac{k}{2}$ we can find $\delta >0$ such that for $x\in I$ and $x\in (a-\delta , a+\delta )\setminus \{a\}$ $$\left|\frac{f(x)-f(a)}{x-a}-K\right|<\frac{K}{2}$$ (This is the part I'm not sure about).. hence $$-\frac{K}{2}<\frac{f(x)-f(a)}{x-a}-K<\frac{K}{2}$$ hence, $$\frac{K}{2}(x-a)<f(x)-f(a)<\frac{3K}{2}(x-a)$$ hence, $$\frac{K}{2}(x-a)+f(a)<f(x)<\frac{3K}{2}(x-a)+f(a)$$ Therefore if we choose $x\in (a-\delta ,a)$ and $y\in (a,a+\delta)$ (which we can find since $I$ is open) we get that $$f(x)<\frac{3K}{2}(x-a)+f(a)<f(a)\qquad \text{As }x-a<0$$ $$f(a)<\frac{K}{2}(y-a)+f(a)<f(y)\qquad \text{As }y-a>0$$ Is this proof correct? Also, I really got lost in trying to write the title :P",,"['real-analysis', 'derivatives', 'proof-verification']"
37,A question involving Frechet differentiability,A question involving Frechet differentiability,,"Let $X, Y$ be real normed spaces and $U \subset X$ open subset. In ""Nonlinear functional analysis and applications"" edited by Louis B. Rall, we have the followint definition (page 115) A map $F : U \to Y$ is said to be $\textbf{Frechet differentiable}$ at $x_0 \in U$ if there exists a continuous linear operator $L(x_0) : X \to Y$ such that the following representation holds for every $h \in X$ with $x_0 + h \in U$, $$F(x_0 + h) - F(x_0) = L(x_0) h + r(x_0; h) $$ where $$\underset{h \to 0}{lim} \frac{\| r(x_0; h) \|}{\| h \|} =0.$$ How can I calculate $r(x_0; h)$? For example, at page 118 (Example 1.6) we have the following function: $$f(x_1, x_2) = \frac{x_1^3 x_2}{x_1^4 + x_2^2} \; \; \text{if} \; \; (x_1, x_2) \neq (0, 0) \;\; \text{and} \;\; f(x_1, x_2) = 0, \; \; \text{if} \; \; (x_1, x_2) = (0, 0)$$ and $$r(0; h) = \frac{h_1^3 h_2}{h_1^4 + h_2^2} \;\; \text{if} \;\; h \neq 0.$$ Why does $r(0; h)$ have this form? Thank you!","Let $X, Y$ be real normed spaces and $U \subset X$ open subset. In ""Nonlinear functional analysis and applications"" edited by Louis B. Rall, we have the followint definition (page 115) A map $F : U \to Y$ is said to be $\textbf{Frechet differentiable}$ at $x_0 \in U$ if there exists a continuous linear operator $L(x_0) : X \to Y$ such that the following representation holds for every $h \in X$ with $x_0 + h \in U$, $$F(x_0 + h) - F(x_0) = L(x_0) h + r(x_0; h) $$ where $$\underset{h \to 0}{lim} \frac{\| r(x_0; h) \|}{\| h \|} =0.$$ How can I calculate $r(x_0; h)$? For example, at page 118 (Example 1.6) we have the following function: $$f(x_1, x_2) = \frac{x_1^3 x_2}{x_1^4 + x_2^2} \; \; \text{if} \; \; (x_1, x_2) \neq (0, 0) \;\; \text{and} \;\; f(x_1, x_2) = 0, \; \; \text{if} \; \; (x_1, x_2) = (0, 0)$$ and $$r(0; h) = \frac{h_1^3 h_2}{h_1^4 + h_2^2} \;\; \text{if} \;\; h \neq 0.$$ Why does $r(0; h)$ have this form? Thank you!",,"['real-analysis', 'functional-analysis', 'derivatives']"
38,Gradient of the Fourier transform of a function,Gradient of the Fourier transform of a function,,"I am wondering if there is a simple way to express the first variation of the Fourier transform of a function as a function of said function. In other words, if $g:x\mapsto F(f)(x)$, where $F(f)$ is the Fourier transform of f, is there a simple expression for $\dfrac{\partial g}{\partial f} $? Thanks in advance for any insight or reference!","I am wondering if there is a simple way to express the first variation of the Fourier transform of a function as a function of said function. In other words, if $g:x\mapsto F(f)(x)$, where $F(f)$ is the Fourier transform of f, is there a simple expression for $\dfrac{\partial g}{\partial f} $? Thanks in advance for any insight or reference!",,"['functional-analysis', 'derivatives', 'fourier-analysis', 'calculus-of-variations']"
39,Proving the Derivative of cosine and sine functions,Proving the Derivative of cosine and sine functions,,"In the proof of the derivatives of cosine and sine functions, we used the facts that: $$\lim\limits_{\Delta x \to 0} \frac{\cos \Delta x - 1}{\Delta x} = 0$$ and $$\lim\limits_{\Delta x \to 0} \frac{\sin \Delta x}{\Delta x} = 1.$$ I saw the proof of these two facts but it's said that $x$ here must be in radians, so why it must be measured in radians?","In the proof of the derivatives of cosine and sine functions, we used the facts that: $$\lim\limits_{\Delta x \to 0} \frac{\cos \Delta x - 1}{\Delta x} = 0$$ and $$\lim\limits_{\Delta x \to 0} \frac{\sin \Delta x}{\Delta x} = 1.$$ I saw the proof of these two facts but it's said that $x$ here must be in radians, so why it must be measured in radians?",,"['calculus', 'trigonometry', 'derivatives']"
40,How do I show that a directional derivative is defined in every direction (for every vector) for a function $f: \mathbb{R}^2 \to \mathbb{R}$?,How do I show that a directional derivative is defined in every direction (for every vector) for a function ?,f: \mathbb{R}^2 \to \mathbb{R},"I'm working through some analysis books, and while working through the section on directional derivatives, I searched here and found this answer , which states Let $f: R^2 \to R$ be defined by $f(x,y)= \frac{x^3y}{x^4+y^2}$ for $(x,y) \neq (0,0)$ and $f(0,0) = 0$.  This function is continuous; its directional derivative is defined at each point in every direction; and at each point its directional derivative is a linear function of the direction. I set about trying to show this at the origin, and here's what I have so far: We have $g(t) = (x, y) + (v, w)t$, so $g(0) = (x, y)$. To find the directional derivative, I want to find $(f \circ g)'(0) = G'(0)$. Taking the composition and doing the algreba, I get \begin{align} G(t) &= (f \circ g)(t) \\ &= \frac{(x+vt)^3 (y+wt)}{(x+vt)^4+(y+wt)^2} \\ &= (x+vt)^3 (y+wt) \left[ (x+vt)^4+(y+wt)^2 \right]^{-1} \end{align} From the Chain Rule wrt $t$: \begin{align} G'(t) &= \frac{3v(x+vt)^2 (y+wt) + w(x+vt)^3}{(x+vt)^4+(y+wt)^2} - \frac{(4v(x+vt)^3 + 2w(y+wt)) (x+vt)^3 (y+wt) } {((x+vt)^4+(y+wt)^2 )^2} \\ \implies G'(0) &= \frac{3vx^2y + wx^3}{x^4+y^2} - \frac{(4vx^3 + 2wy) x^3 y } {(x^4+y^2)^2} \end{align} Assuming I did all that algebra correctly, I don't see where this gets me. Do I then have to use the definition of a derivative in one dimension and show that $G'(0)$ satisfies differentiability at $0$, i.e. \begin{equation} G(t) = G(0) + G'(0)(t-0) + X(t-0) \end{equation} and $\lim_{t\to0} \frac{X(t-0)}{t-0} = 0$? edit: I found this answer that takes another approach, so using that method, where $y$ is the direction vector and $a$ is the point at which we take the derivative, gives me this: \begin{align} \lim_{h \to 0}\frac{f(\mathbf{a} + h\mathbf{y}) - f(\mathbf{a})}{h} &= \lim_{h \to 0}\frac{f((0,0) + h(y_{1},y_{2})) - f(\mathbf{0})}{h} \\ &= \lim_{h \to 0}\frac{f(hy_{1},hy_{2})}{h} \\ &= \lim_{h \to 0}\frac{(hy_1)^3 (hy_2)}{h((hy_1)^4 + (hy_2)^2)} \\ &= \lim_{h \to 0}\frac{h^4 y_1^3 y_2}{h(h^4y_1^4 + h^2y_2^2)} \\ &= \lim_{h \to 0}\frac{h^4 y_1^3 y_2}{h^3(h^2y_1^4 + y_2^2)} \\ &= \lim_{h \to 0}\frac{h y_1^3 y_2}{(h^2y_1^4 + y_2^2)} \\ &= 0 \end{align} Since this limit exists, regardless of the values for the vector $\mathbf{y}$ (assuming that $\mathbf{y}$ isn't the zero vector, can I conclude that the directional derivative exists for all nonzero vectors?","I'm working through some analysis books, and while working through the section on directional derivatives, I searched here and found this answer , which states Let $f: R^2 \to R$ be defined by $f(x,y)= \frac{x^3y}{x^4+y^2}$ for $(x,y) \neq (0,0)$ and $f(0,0) = 0$.  This function is continuous; its directional derivative is defined at each point in every direction; and at each point its directional derivative is a linear function of the direction. I set about trying to show this at the origin, and here's what I have so far: We have $g(t) = (x, y) + (v, w)t$, so $g(0) = (x, y)$. To find the directional derivative, I want to find $(f \circ g)'(0) = G'(0)$. Taking the composition and doing the algreba, I get \begin{align} G(t) &= (f \circ g)(t) \\ &= \frac{(x+vt)^3 (y+wt)}{(x+vt)^4+(y+wt)^2} \\ &= (x+vt)^3 (y+wt) \left[ (x+vt)^4+(y+wt)^2 \right]^{-1} \end{align} From the Chain Rule wrt $t$: \begin{align} G'(t) &= \frac{3v(x+vt)^2 (y+wt) + w(x+vt)^3}{(x+vt)^4+(y+wt)^2} - \frac{(4v(x+vt)^3 + 2w(y+wt)) (x+vt)^3 (y+wt) } {((x+vt)^4+(y+wt)^2 )^2} \\ \implies G'(0) &= \frac{3vx^2y + wx^3}{x^4+y^2} - \frac{(4vx^3 + 2wy) x^3 y } {(x^4+y^2)^2} \end{align} Assuming I did all that algebra correctly, I don't see where this gets me. Do I then have to use the definition of a derivative in one dimension and show that $G'(0)$ satisfies differentiability at $0$, i.e. \begin{equation} G(t) = G(0) + G'(0)(t-0) + X(t-0) \end{equation} and $\lim_{t\to0} \frac{X(t-0)}{t-0} = 0$? edit: I found this answer that takes another approach, so using that method, where $y$ is the direction vector and $a$ is the point at which we take the derivative, gives me this: \begin{align} \lim_{h \to 0}\frac{f(\mathbf{a} + h\mathbf{y}) - f(\mathbf{a})}{h} &= \lim_{h \to 0}\frac{f((0,0) + h(y_{1},y_{2})) - f(\mathbf{0})}{h} \\ &= \lim_{h \to 0}\frac{f(hy_{1},hy_{2})}{h} \\ &= \lim_{h \to 0}\frac{(hy_1)^3 (hy_2)}{h((hy_1)^4 + (hy_2)^2)} \\ &= \lim_{h \to 0}\frac{h^4 y_1^3 y_2}{h(h^4y_1^4 + h^2y_2^2)} \\ &= \lim_{h \to 0}\frac{h^4 y_1^3 y_2}{h^3(h^2y_1^4 + y_2^2)} \\ &= \lim_{h \to 0}\frac{h y_1^3 y_2}{(h^2y_1^4 + y_2^2)} \\ &= 0 \end{align} Since this limit exists, regardless of the values for the vector $\mathbf{y}$ (assuming that $\mathbf{y}$ isn't the zero vector, can I conclude that the directional derivative exists for all nonzero vectors?",,"['calculus', 'real-analysis', 'derivatives']"
41,Finding Equation of tangent line,Finding Equation of tangent line,,"Can someone double check my work to see if I'm doing it correctly? Find the equation of the line tangent to the graph of $(2,1)$ where $f$ is given by $f(x) = 2x^3 - 2x^2 + 1$ 1) $f'(x) = 6x^2-4x$ (First I found derivative) 2) $f'(2) = 6(2)^2-4(2) = 16$ (Then found slope by plugging $x$ coordinate into derivative) 3) $y-1 = 16(x-2) =$ (Then I plugged slope, $x$, and $y$ into point slope formula and solved) $y = 16x - 31$","Can someone double check my work to see if I'm doing it correctly? Find the equation of the line tangent to the graph of $(2,1)$ where $f$ is given by $f(x) = 2x^3 - 2x^2 + 1$ 1) $f'(x) = 6x^2-4x$ (First I found derivative) 2) $f'(2) = 6(2)^2-4(2) = 16$ (Then found slope by plugging $x$ coordinate into derivative) 3) $y-1 = 16(x-2) =$ (Then I plugged slope, $x$, and $y$ into point slope formula and solved) $y = 16x - 31$",,"['calculus', 'derivatives']"
42,Parametric differentiation,Parametric differentiation,,The parametric equations of a curve are $$\begin{cases}x(t)=e^{-t}\cos t\\y(t)=e^{-t}\sin t\end{cases}$$ Show that  $$\frac{dy}{dx}= \tan\left(t-\frac{\pi}{4}\right)$$ I did the differentiation correct which is $$\frac{\sin t-\cos t}{\cos t+\sin t}$$ but I don't know how can I reach the final answer? how can this be changed to $\tan\left(t-\dfrac{\pi}{4}\right)$,The parametric equations of a curve are $$\begin{cases}x(t)=e^{-t}\cos t\\y(t)=e^{-t}\sin t\end{cases}$$ Show that  $$\frac{dy}{dx}= \tan\left(t-\frac{\pi}{4}\right)$$ I did the differentiation correct which is $$\frac{\sin t-\cos t}{\cos t+\sin t}$$ but I don't know how can I reach the final answer? how can this be changed to $\tan\left(t-\dfrac{\pi}{4}\right)$,,"['derivatives', 'parametric']"
43,What is the greatest value of b for which and real valued function f that satisfies the following properties must also satisfy f(1)<5,What is the greatest value of b for which and real valued function f that satisfies the following properties must also satisfy f(1)<5,,"The properties listed are: 1) f is infinitely differentiable on the real numbers 2) $f(0) = 1, f'(0) = 1$, and $f''(0) = 2$ 3) $|f'''(x)| < b$ for all x in $[0,1]$ This is question 42 from the GRE math subject test 9367. My idea was to integrate both sides of the inequality in property three, and plug in the info from property 2 for the value of c (post integration). Is this a valid approach? I get the correct answer for b, which is 12, but I don't know if this is mathematically valid to integrate a magnitude, and integrate both sides of an inequality. Please help! Thank you.","The properties listed are: 1) f is infinitely differentiable on the real numbers 2) $f(0) = 1, f'(0) = 1$, and $f''(0) = 2$ 3) $|f'''(x)| < b$ for all x in $[0,1]$ This is question 42 from the GRE math subject test 9367. My idea was to integrate both sides of the inequality in property three, and plug in the info from property 2 for the value of c (post integration). Is this a valid approach? I get the correct answer for b, which is 12, but I don't know if this is mathematically valid to integrate a magnitude, and integrate both sides of an inequality. Please help! Thank you.",,"['calculus', 'integration', 'derivatives', 'gre-exam']"
44,Find the Critical Points: $f(x) =(x^2-1)^3$,Find the Critical Points:,f(x) =(x^2-1)^3,"This question probably has more to do with my Algebra skills than Calculus. Nonetheless, can someone explain why the factored ""term"" is not set to zero (0) [second picture]. Thanks in advance.","This question probably has more to do with my Algebra skills than Calculus. Nonetheless, can someone explain why the factored ""term"" is not set to zero (0) [second picture]. Thanks in advance.",,"['calculus', 'derivatives']"
45,Differentiation of the function $\operatorname{li}(x) = \int_2^x \frac{dt}{\ln(t)} $,Differentiation of the function,\operatorname{li}(x) = \int_2^x \frac{dt}{\ln(t)} ,"I have to differentiate with respect to x: $$\operatorname{li}(x) = \int_2^x \frac{dt}{\ln(t)} $$ I havn't come across this before, so my idea is to integrate it first? (Backward right?). let $$\ln(t) = x$$ on differentiation with respect to t $$\frac{1}{t} = \frac{dx}{dt} \implies dt = dx\cdot t$$ I would continue my attempt but it gets so messy i just don't see an end to it. Could someone shed some light on this please.","I have to differentiate with respect to x: $$\operatorname{li}(x) = \int_2^x \frac{dt}{\ln(t)} $$ I havn't come across this before, so my idea is to integrate it first? (Backward right?). let $$\ln(t) = x$$ on differentiation with respect to t $$\frac{1}{t} = \frac{dx}{dt} \implies dt = dx\cdot t$$ I would continue my attempt but it gets so messy i just don't see an end to it. Could someone shed some light on this please.",,['calculus']
46,Unbounded derivatives on a bounded set,Unbounded derivatives on a bounded set,,"Suppose that $f$ is differentiable on a finite interval $(a,b)$ and $$ \lim_{x\to a^+}f(x)=\lim_{x\to b^-}f(x)=\infty. $$ Prove that, for any $r\in\mathbb{R}$ there exists $c\in (a,b)$ such that $f'(c)=r$. MY THOUGHTS: This theorem seems painfully obvious on an intuitive level, but I am not sure how to begin formalizing this. The Mean Value Theorem seems likely to be used here. Let $r\in\mathbb{R}$ and then there is some $c\in(s,t)\subset(a,b)$ such that $$ f'(c)=\frac{f(t)-f(s)}{t-s}=r. $$ It isn't immediately clear to me how to choose the subinterval $(s,t)$. Perhaps there is a better way to proceed?","Suppose that $f$ is differentiable on a finite interval $(a,b)$ and $$ \lim_{x\to a^+}f(x)=\lim_{x\to b^-}f(x)=\infty. $$ Prove that, for any $r\in\mathbb{R}$ there exists $c\in (a,b)$ such that $f'(c)=r$. MY THOUGHTS: This theorem seems painfully obvious on an intuitive level, but I am not sure how to begin formalizing this. The Mean Value Theorem seems likely to be used here. Let $r\in\mathbb{R}$ and then there is some $c\in(s,t)\subset(a,b)$ such that $$ f'(c)=\frac{f(t)-f(s)}{t-s}=r. $$ It isn't immediately clear to me how to choose the subinterval $(s,t)$. Perhaps there is a better way to proceed?",,['derivatives']
47,Converting prime notation of derivatives to Leibniz notation.Resources needed,Converting prime notation of derivatives to Leibniz notation.Resources needed,,"I have been studying calculus for past few months and through the time I have been using the so called prime notation.I have been studying from Spivaks Calculus for those of you who are familiar with the book. My problem now is that I want to study some differential equations so I can study Lagrangian and Hamiltonian mechanics eventually,but the books that teach differential equations mostly use Leibniz notation which I am not familiar with. Could some of you point me in direction where I can learn to use Leibniz notation and convert it to prime notation,and other way around as well? Also I would appreciate if someone would layout examples of chain rule,multiplication rule and other basic differentiation rules expressed in Leibniz notation in comparision to prime notation. Thanks in advance","I have been studying calculus for past few months and through the time I have been using the so called prime notation.I have been studying from Spivaks Calculus for those of you who are familiar with the book. My problem now is that I want to study some differential equations so I can study Lagrangian and Hamiltonian mechanics eventually,but the books that teach differential equations mostly use Leibniz notation which I am not familiar with. Could some of you point me in direction where I can learn to use Leibniz notation and convert it to prime notation,and other way around as well? Also I would appreciate if someone would layout examples of chain rule,multiplication rule and other basic differentiation rules expressed in Leibniz notation in comparision to prime notation. Thanks in advance",,"['derivatives', 'notation']"
48,"Examples and graphs of functions that are once, twice, three times differentiable, etc.","Examples and graphs of functions that are once, twice, three times differentiable, etc.",,"I'm trying to deepen my understanding of differentiation and this idea of infinitely differentiable functions as being ""smooth"" -- i.e., the more a function is differentiable, the smoother it gets. I saw the plot of a function that was everywhere continuous but nowhere differentiable, and it looked really...hairy... Anyway, to help me understand this concept of ""smoothness"" intuitively, I was hoping someone could give me examples (and hopefully plots, too) of functions from $\mathbb{R}$ to $\mathbb{R}$ that are: a) differentiable only once with continuous derivative (an a plot to see smoothness) b) differentiable only twice (and a plot for smoothness) c) differentiable 10 times (and a plot for smoothness) d) infinitely differentiable (and a plot for smoothness)","I'm trying to deepen my understanding of differentiation and this idea of infinitely differentiable functions as being ""smooth"" -- i.e., the more a function is differentiable, the smoother it gets. I saw the plot of a function that was everywhere continuous but nowhere differentiable, and it looked really...hairy... Anyway, to help me understand this concept of ""smoothness"" intuitively, I was hoping someone could give me examples (and hopefully plots, too) of functions from $\mathbb{R}$ to $\mathbb{R}$ that are: a) differentiable only once with continuous derivative (an a plot to see smoothness) b) differentiable only twice (and a plot for smoothness) c) differentiable 10 times (and a plot for smoothness) d) infinitely differentiable (and a plot for smoothness)",,"['real-analysis', 'derivatives']"
49,Prove that if $\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=L$ then $f'(0)=L$.,Prove that if  then .,\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=L f'(0)=L,"$f$ is differentiable. Prove that if $\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=L$ then $f'(0)=L$. I tried L'Hopital: $\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=\lim_{n \rightarrow \infty} \frac {f(\frac{1}{n})-f(0)}{\frac{1}{n}}$, but it didn't get me far... Any assistance would be much appreciated!","$f$ is differentiable. Prove that if $\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=L$ then $f'(0)=L$. I tried L'Hopital: $\lim_{n \rightarrow \infty}n(f(\frac{1}{n})-f(0))=\lim_{n \rightarrow \infty} \frac {f(\frac{1}{n})-f(0)}{\frac{1}{n}}$, but it didn't get me far... Any assistance would be much appreciated!",,"['calculus', 'derivatives']"
50,Norm in the space of Riemann-Liouville integrals/derivatives? (aka fractional integrals/derivatives),Norm in the space of Riemann-Liouville integrals/derivatives? (aka fractional integrals/derivatives),,"What is the natural norm to consider in the space of Riemann-Liouville integrals/derivatives of a function? Context: Let $f\in L^2 ([a,b])$. Define the Riemann-Liouville fractional integral of order $\alpha\geq 0$ (respectively fractional derivative) as: $$I_{a^+}^{\alpha} (f)(x) = \frac{1}{\Gamma (\alpha)} \int_a^x (x-y)^{\alpha-1} f(y)dy$$ respectively, $$D_{a^+}^{\alpha} (f)(x) = \frac{1}{\Gamma (1-\alpha)} \frac{d}{dx}\int_a^x \frac{f(y)}{(x-y)^{\alpha}}dy.$$ It follows that $D_{a^+}^{\alpha} I_{a^+}^{\alpha} (f) = f$ and viceversa. Denote by $I_{a^+}^{\alpha} (L^2)$ the imagine of $L^2([a,b])$ by the operator $I_{a^+}^{\alpha}$. Question: if I have an operator $T_{\alpha}:I_{a^+}^{\alpha} (L^2)\rightarrow L^2([a,b]) $ which is a linear isomorphism. What is the norm considered on the imagine space? That is, $$\|T_{\alpha}f\|_{L^2([a,b])} \leq \|T_{\alpha}\|_{op} \|f\|_{?}$$ where $\|T_{\alpha}\|_{op}$ denotes the operator norm.","What is the natural norm to consider in the space of Riemann-Liouville integrals/derivatives of a function? Context: Let $f\in L^2 ([a,b])$. Define the Riemann-Liouville fractional integral of order $\alpha\geq 0$ (respectively fractional derivative) as: $$I_{a^+}^{\alpha} (f)(x) = \frac{1}{\Gamma (\alpha)} \int_a^x (x-y)^{\alpha-1} f(y)dy$$ respectively, $$D_{a^+}^{\alpha} (f)(x) = \frac{1}{\Gamma (1-\alpha)} \frac{d}{dx}\int_a^x \frac{f(y)}{(x-y)^{\alpha}}dy.$$ It follows that $D_{a^+}^{\alpha} I_{a^+}^{\alpha} (f) = f$ and viceversa. Denote by $I_{a^+}^{\alpha} (L^2)$ the imagine of $L^2([a,b])$ by the operator $I_{a^+}^{\alpha}$. Question: if I have an operator $T_{\alpha}:I_{a^+}^{\alpha} (L^2)\rightarrow L^2([a,b]) $ which is a linear isomorphism. What is the norm considered on the imagine space? That is, $$\|T_{\alpha}f\|_{L^2([a,b])} \leq \|T_{\alpha}\|_{op} \|f\|_{?}$$ where $\|T_{\alpha}\|_{op}$ denotes the operator norm.",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'derivatives']"
51,Points at which a function is holomorphic,Points at which a function is holomorphic,,$f=\frac{1}{z^5-1}$ $z=1$ ofcourse makes it non-holomorphic. What other z's would make it non-holomorphic? Is it only $z=1$ ?,$f=\frac{1}{z^5-1}$ $z=1$ ofcourse makes it non-holomorphic. What other z's would make it non-holomorphic? Is it only $z=1$ ?,,"['complex-analysis', 'derivatives']"
52,Prove this alternative formula for derivative $f'(x)$,Prove this alternative formula for derivative,f'(x),"Show that: $$f'(x) = \frac{f(x + h) - f(x - h)}{2h} \tag 1$$ Proof: If $(1)$ is true then $f'(x) = \displaystyle \frac{f(x + h) - f(x) + f(x) - f(x - h)}{2h} = \frac{f(x + h) - f(x)}{2h} - \frac{f(x - h) - f(x)}{2h}$ But here is the confusion, how do I modify the $f(x- h)$ on the RHS? PLEASE ONLY SMALL HINTS! I want to do this mostly on my own! Thanks =)","Show that: $$f'(x) = \frac{f(x + h) - f(x - h)}{2h} \tag 1$$ Proof: If $(1)$ is true then $f'(x) = \displaystyle \frac{f(x + h) - f(x) + f(x) - f(x - h)}{2h} = \frac{f(x + h) - f(x)}{2h} - \frac{f(x - h) - f(x)}{2h}$ But here is the confusion, how do I modify the $f(x- h)$ on the RHS? PLEASE ONLY SMALL HINTS! I want to do this mostly on my own! Thanks =)",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'proof-writing']"
53,Deriving logarithm in exponent,Deriving logarithm in exponent,,"Im attempting to take the derivative of $n^{log_2(n)}$, but the answer I'm getting is different from http://www.derivative-calculator.net/ .. this isnt highschool math homework, I'm trying to use L'hospitals algorithm on a series of equations to determine their ordering(in terms of lowest to highest limit) for an algorithms analysis class. I haven't touched calculus in a few years now and I cant word this question properly to get a helpful google search. The first step I do is convert $log_2(n)$ to $\frac{ln(n)}{ln(2)}$, and perform u-substitution. $u = \frac{ln(n)}{ln(2)} $ and    $du = \frac{1}{n*ln(2)}$ this leaves with me with $\frac{d}{du}(n^u)$ =  $ u*n^{u-1}*du $ substituting the original values back in =  $\frac{ln(n)}{ln(2)} * n^{\frac{ln(n)}{ln(2)} - 1} * \frac{1}{n*ln(2)}$ =  $\frac{n^{\frac{ln(n)}{ln(2)}-1}*ln(n)}{n * ln(2)^2}$ The online calculator claims the solution is: = $\frac{2n^{\frac{ln(n)}{ln(2)}-1}* ln(n)}{ln(2)} $ What is it that I'm doing wrong? my algorithms homework is loaded with questions like this. -Thanks in advance","Im attempting to take the derivative of $n^{log_2(n)}$, but the answer I'm getting is different from http://www.derivative-calculator.net/ .. this isnt highschool math homework, I'm trying to use L'hospitals algorithm on a series of equations to determine their ordering(in terms of lowest to highest limit) for an algorithms analysis class. I haven't touched calculus in a few years now and I cant word this question properly to get a helpful google search. The first step I do is convert $log_2(n)$ to $\frac{ln(n)}{ln(2)}$, and perform u-substitution. $u = \frac{ln(n)}{ln(2)} $ and    $du = \frac{1}{n*ln(2)}$ this leaves with me with $\frac{d}{du}(n^u)$ =  $ u*n^{u-1}*du $ substituting the original values back in =  $\frac{ln(n)}{ln(2)} * n^{\frac{ln(n)}{ln(2)} - 1} * \frac{1}{n*ln(2)}$ =  $\frac{n^{\frac{ln(n)}{ln(2)}-1}*ln(n)}{n * ln(2)^2}$ The online calculator claims the solution is: = $\frac{2n^{\frac{ln(n)}{ln(2)}-1}* ln(n)}{ln(2)} $ What is it that I'm doing wrong? my algorithms homework is loaded with questions like this. -Thanks in advance",,"['derivatives', 'logarithms']"
54,"Proving that $\sup f'\left( \left( 0,\infty \right) \right)=0$ under a certain set of conditions.",Proving that  under a certain set of conditions.,"\sup f'\left( \left( 0,\infty \right) \right)=0","Let $f$ be a twice differentiable function on $\left( 0,\infty  \right)$ s.t. $f''(x)>0$ for all $x\in \left( 0,\infty  \right)$. Prove, that if the following conditions are satisfied: $\underset{x\to \infty }{\mathop{\lim }}\,f\left( x \right)=L<\infty $. $\forall x\in \left( 0,\infty  \right).f'\left( x \right)<0$. Then $\sup f'\left( \left( 0,\infty  \right) \right)=0$ . How would you go about proving this?","Let $f$ be a twice differentiable function on $\left( 0,\infty  \right)$ s.t. $f''(x)>0$ for all $x\in \left( 0,\infty  \right)$. Prove, that if the following conditions are satisfied: $\underset{x\to \infty }{\mathop{\lim }}\,f\left( x \right)=L<\infty $. $\forall x\in \left( 0,\infty  \right).f'\left( x \right)<0$. Then $\sup f'\left( \left( 0,\infty  \right) \right)=0$ . How would you go about proving this?",,"['derivatives', 'supremum-and-infimum']"
55,"Exercise 3.40 from Folland, Real Analysis","Exercise 3.40 from Folland, Real Analysis",,"Let $F$ denote the Cantor function on $[0, 1]$ (see $§1.5$), and set $F(x)= 0$ for $x<0$ and $F(x)=1$ for $x>1$. Let ${[a_n, b_n]}$ be an enumeration of the closed subintervals of $[0,1]$ with rational endpoints, and let $F_n{(x)}=F((x-a_n)/(b_n-a_n))$. Then let $G$ be the sum of ${2^{-n}F_n}$ as $n$ goes from $1$ to $\infty$, then $G$ is continuous and strictly increasing on $[0,1]$, and $G'=0$ a.e. (Use Exercise 39.) Source : Folland, Real Analysis , exercise $3.40$. Can anybody help please? I have no idea how to do this problem. Thanks a lot.","Let $F$ denote the Cantor function on $[0, 1]$ (see $§1.5$), and set $F(x)= 0$ for $x<0$ and $F(x)=1$ for $x>1$. Let ${[a_n, b_n]}$ be an enumeration of the closed subintervals of $[0,1]$ with rational endpoints, and let $F_n{(x)}=F((x-a_n)/(b_n-a_n))$. Then let $G$ be the sum of ${2^{-n}F_n}$ as $n$ goes from $1$ to $\infty$, then $G$ is continuous and strictly increasing on $[0,1]$, and $G'=0$ a.e. (Use Exercise 39.) Source : Folland, Real Analysis , exercise $3.40$. Can anybody help please? I have no idea how to do this problem. Thanks a lot.",,"['calculus', 'real-analysis', 'analysis', 'measure-theory', 'derivatives']"
56,"$\int_\Omega |\nabla u^+|^2 \, dx$ is not differentiable with respect to $u$ in $W_0^{1,2}(\Omega)$",is not differentiable with respect to  in,"\int_\Omega |\nabla u^+|^2 \, dx u W_0^{1,2}(\Omega)","Let $u \in W_0^{1,2}(\Omega)$, where $\Omega$ is some domain in $\mathbb{R}^N$, $N \geq 1$. Denote $u^+ := \max\{u, 0\}$. (It is know that $u^+$ also belongs to $W_0^{1,2}(\Omega)$ (see, e.g., Theorem A.1 in Kinderlehrer-Stampacchia ). Consider now the functional $$ u \mapsto \int_\Omega |\nabla u^+|^2 \, dx. $$ It is noted in the article of M. Clapp & T. Weth , p.4, that this functional is not differentiable (in the Fréchet sense) in $W_0^{1,2}(\Omega)$. Unfortunately, they don't provide any reference for a counterexample. I reinvented the wheel, and constructed a simple 1D counterexample, using the function $u(x) \approx x^\alpha$ near $0$, where $\alpha \in (0,1)$.  But, obviously, somewhere should be a published result concerning this non-differentiability, which I can cite. Maybe somebody met the same question and can provide the reference? Thanks! P.S. This functional is differentiable for $u \in W_0^{1,2}(\Omega) \cap W^{2,2}(\Omega)$ (see T. Bartsch, T. Weth , Lemma 3.1, p.7).","Let $u \in W_0^{1,2}(\Omega)$, where $\Omega$ is some domain in $\mathbb{R}^N$, $N \geq 1$. Denote $u^+ := \max\{u, 0\}$. (It is know that $u^+$ also belongs to $W_0^{1,2}(\Omega)$ (see, e.g., Theorem A.1 in Kinderlehrer-Stampacchia ). Consider now the functional $$ u \mapsto \int_\Omega |\nabla u^+|^2 \, dx. $$ It is noted in the article of M. Clapp & T. Weth , p.4, that this functional is not differentiable (in the Fréchet sense) in $W_0^{1,2}(\Omega)$. Unfortunately, they don't provide any reference for a counterexample. I reinvented the wheel, and constructed a simple 1D counterexample, using the function $u(x) \approx x^\alpha$ near $0$, where $\alpha \in (0,1)$.  But, obviously, somewhere should be a published result concerning this non-differentiability, which I can cite. Maybe somebody met the same question and can provide the reference? Thanks! P.S. This functional is differentiable for $u \in W_0^{1,2}(\Omega) \cap W^{2,2}(\Omega)$ (see T. Bartsch, T. Weth , Lemma 3.1, p.7).",,"['reference-request', 'derivatives', 'partial-differential-equations', 'sobolev-spaces']"
57,Find the point where the slope changes drastically,Find the point where the slope changes drastically,,"I have a distribution for which I have to find the point where the slope changes drastically. In visual terms, I have to find this point: I though I could use derivatives, but for the following equation: $$ y = -0.255ln(x) + 1.6889 $$ It seems I can't. How can I get what I need?","I have a distribution for which I have to find the point where the slope changes drastically. In visual terms, I have to find this point: I though I could use derivatives, but for the following equation: $$ y = -0.255ln(x) + 1.6889 $$ It seems I can't. How can I get what I need?",,"['calculus', 'derivatives', 'logarithms']"
58,"Does $g'$ need to be continuous for $g(x_0) = 0$, $g'(x_0) \neq 0$ to imply $g$ changes sign in a neighborhood of $x_0$","Does  need to be continuous for ,  to imply  changes sign in a neighborhood of",g' g(x_0) = 0 g'(x_0) \neq 0 g x_0,"The following theorem holds: Theorem: Let $g:\mathcal{A} \rightarrow \mathbb{R}$ be differentiable and let $x_0 \in \mathcal{A} $. If $g(x_0)=0, \; g'(x_0)\neq 0$ then $g$ changes sign at a neighbourhood of $x_0$. Questions: Do we need $g'$ to be continous on the interval $\mathcal{A}$? If yes, then give a counter example of a $g$ function that satisfies all the hypothesis but not the conclusion. I suspect that $g'$ should be continous, although I cannot find a counter example. I know a proof of the theorem which come from MVT (mean value theorem) and Taylor , but I don't know if we do need $g'$ to be continuous.","The following theorem holds: Theorem: Let $g:\mathcal{A} \rightarrow \mathbb{R}$ be differentiable and let $x_0 \in \mathcal{A} $. If $g(x_0)=0, \; g'(x_0)\neq 0$ then $g$ changes sign at a neighbourhood of $x_0$. Questions: Do we need $g'$ to be continous on the interval $\mathcal{A}$? If yes, then give a counter example of a $g$ function that satisfies all the hypothesis but not the conclusion. I suspect that $g'$ should be continous, although I cannot find a counter example. I know a proof of the theorem which come from MVT (mean value theorem) and Taylor , but I don't know if we do need $g'$ to be continuous.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
59,"$\frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\nu(x))}{g'(\nu(x))} $ ,the value of the limit: $\lim_{x \to 0^+} \frac{\nu(x)}{x} $",",the value of the limit:",\frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\nu(x))}{g'(\nu(x))}  \lim_{x \to 0^+} \frac{\nu(x)}{x} ,"Good evening, I thought a lot about this issue. I think I have to apply Lagrange, Taylor. Can someone  help me to calculate this limit? $$f,g \in C^2 [0,1]:   \\ f'(0)g''(0) \ne f''(0) g'(0) \\ g'(x) \ne 0, \forall x \in (0,1)  \\ \nu(x) \text{ is a real number }: \\ \frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\nu(x))}{g'(\nu(x))} \\ \lim_{x -> 0^+} \frac {\nu(x)}{x} $$ My reasoning, using Taylor: $ f(x)=f(0)+xf'(0)+x^2 \frac{f''(0)}{2} \\  g(x)=g(0)+xg'(0)+x^2 \frac{g''(0)}{2} \\ \frac{f'(0)+\frac{f''(0)}{2}x}{g'(0)+\frac{g''(0)}{2}x}=\frac{f'(\nu(x))}{g'(\nu(x)) } \\ \frac{f'(0)+\frac{f''(0)}{2}x}{g'(0)+\frac{g''(0)}{2}x}=\frac{f'(\nu(0))+\nu'(0)f''(\nu(0))x}{g'(\nu(0))+\nu'(0)g''(\nu(0))x } $ Can you give me a hint to continue the reasoning? Is there any mistake? Thanks.","Good evening, I thought a lot about this issue. I think I have to apply Lagrange, Taylor. Can someone  help me to calculate this limit? $$f,g \in C^2 [0,1]:   \\ f'(0)g''(0) \ne f''(0) g'(0) \\ g'(x) \ne 0, \forall x \in (0,1)  \\ \nu(x) \text{ is a real number }: \\ \frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\nu(x))}{g'(\nu(x))} \\ \lim_{x -> 0^+} \frac {\nu(x)}{x} $$ My reasoning, using Taylor: $ f(x)=f(0)+xf'(0)+x^2 \frac{f''(0)}{2} \\  g(x)=g(0)+xg'(0)+x^2 \frac{g''(0)}{2} \\ \frac{f'(0)+\frac{f''(0)}{2}x}{g'(0)+\frac{g''(0)}{2}x}=\frac{f'(\nu(x))}{g'(\nu(x)) } \\ \frac{f'(0)+\frac{f''(0)}{2}x}{g'(0)+\frac{g''(0)}{2}x}=\frac{f'(\nu(0))+\nu'(0)f''(\nu(0))x}{g'(\nu(0))+\nu'(0)g''(\nu(0))x } $ Can you give me a hint to continue the reasoning? Is there any mistake? Thanks.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
60,Understanding the definition of a pullback of a differential $k$-form and applying it in $1-d$,Understanding the definition of a pullback of a differential -form and applying it in,k 1-d,"I am having trouble understanding the definition of a pullback of a differential k-form in a basic course in differentiable geometry. This is the definition I am given. I believe it is easier to use what I hope is an equivalent identity of $$ (F^*\beta)(x;u_{(1)},\dots,u_{(n)})=\beta(F(x); \ dF(x)u_{(1)},\dots,dF(x)u_{(n)}) $$ I believe that the definition of algebraic k-form is not widely used so here it is below. Now in the following example you are asked to apply this definition of a pullback in 1-d. I am struggling to understand how $\beta(F(x); F'(x)e_{(i)})=\beta_j(F(x))(F'(x)e_{(i)})^j$. (I believe there was a typo there. I get that $\begin{align} \alpha(x;e_{(i)}) &= F^*\beta(x;e_{(i)}) \\ &= \beta(F(x);dF(x)e_{(i)}) \end{align}$ Now as $\beta(y)=\beta_j(y)dy^j$ we could either do $y=F(x)$ and then get $\beta_j(F(x))d(F(x))^j$ which would be only half right. Not sure why this doesnt work. or $\begin{align} \beta(F(x);dF(x)e_{(i)}) &= (\beta_j(F(x))dy^j)(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))dy^j)(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))d(y^j(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))d(dF(x)e_{(i)}))^j \end{align}$ Again this is wrong but I am not sure how or why. Also is $\displaystyle \alpha=F^*\beta=\frac{\partial F^j}{\partial x^i}\beta_j \circ F dx^i$ equivalent to $\displaystyle \alpha=F^*\beta= \beta_j \circ F \frac{\partial F^j}{\partial x^i} dx^i$ Please keep answers simple as this is only a basic course. I am not familiar with tensors or tangent-anything.","I am having trouble understanding the definition of a pullback of a differential k-form in a basic course in differentiable geometry. This is the definition I am given. I believe it is easier to use what I hope is an equivalent identity of $$ (F^*\beta)(x;u_{(1)},\dots,u_{(n)})=\beta(F(x); \ dF(x)u_{(1)},\dots,dF(x)u_{(n)}) $$ I believe that the definition of algebraic k-form is not widely used so here it is below. Now in the following example you are asked to apply this definition of a pullback in 1-d. I am struggling to understand how $\beta(F(x); F'(x)e_{(i)})=\beta_j(F(x))(F'(x)e_{(i)})^j$. (I believe there was a typo there. I get that $\begin{align} \alpha(x;e_{(i)}) &= F^*\beta(x;e_{(i)}) \\ &= \beta(F(x);dF(x)e_{(i)}) \end{align}$ Now as $\beta(y)=\beta_j(y)dy^j$ we could either do $y=F(x)$ and then get $\beta_j(F(x))d(F(x))^j$ which would be only half right. Not sure why this doesnt work. or $\begin{align} \beta(F(x);dF(x)e_{(i)}) &= (\beta_j(F(x))dy^j)(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))dy^j)(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))d(y^j(dF(x)e_{(i)}) \\ &= (\beta_j(F(x))d(dF(x)e_{(i)}))^j \end{align}$ Again this is wrong but I am not sure how or why. Also is $\displaystyle \alpha=F^*\beta=\frac{\partial F^j}{\partial x^i}\beta_j \circ F dx^i$ equivalent to $\displaystyle \alpha=F^*\beta= \beta_j \circ F \frac{\partial F^j}{\partial x^i} dx^i$ Please keep answers simple as this is only a basic course. I am not familiar with tensors or tangent-anything.",,"['calculus', 'differential-geometry', 'derivatives', 'differential-forms']"
61,proof of chain rule,proof of chain rule,,"Is my proof correct? show: $(g\circ f)'(x_0)=g'(y_0)f'(x_0)$ Since $f'(x_0) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$ and Since $g'(y_0) = \lim_{y\to y_0} \frac{g(y)-g(y_0)}{y-y_0}$ Multiply them to get: $$g'(y_0)f'(x_0) = \lim_{y\to y_0} \frac{g(y)-g(y_0)}{y-y_0} \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$$ since $y=f(x)$ and $y_0 = f(x_0)$, we have:  $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} \lim_{x\to x_0}  \frac{g(f(x))-g(f(x_0))}{f(x)-f(x_0)} \frac{f(x)-f(x_0)}{x-x_0}$$ $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} \lim_{x\to x_0}  \frac{g(f(x))-g(f(x_0))}{x-x_0}$$ $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} (g\circ f)'(x_0)$$ not sure how to get rid of that limit. proof: (2nd try) To prove the chain rule, I use Newton's approximation. We need to show that: $$\forall \epsilon >0\text{   }\exists \delta \text{  such that  } |g(f(x))-g(f(x_0))-f'(x_0)g'(y_0)(x-x_0)-|\leq \epsilon |x-x_0| \text{ whenever }|x-x_0|\leq \delta\text{and} x\in X$$ Since $g(y)$ is differentiable at $y_0$, we Know: $$\forall \epsilon_1>0 \text{   }\exists \delta_1 \text{such that}  |g(y)-g(y_0)-g'(y_0)(y-y_0)|\leq\epsilon_{1}|y-y_0| \text{ whenever }|y-y_0|\leq \delta_1\text{and} y\in Y$$ for $x\neq x_0$ choose $\delta_1 =f'(x_0)(x-x_0)$ and $\epsilon_1 = \frac{\epsilon }{f'(x_0)(x-x_0)} $ Proof (3rd try): $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{g(y)-g(y_0)}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{f(x)-f(x_0)}{x-x_0}$$  $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{g(y)-g(y_0)}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{y-y_0}{x-x_0}$$ $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{y-y_0}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{g(f(x))-g(f(x_0))}{x-x_0}$$  $$RHS = lim_{x\to x_0,x\in X-\{x_o\}}\frac{g(f(x))-g(f(x_0))}{x-x_0}=LHS$$ Is this correct?","Is my proof correct? show: $(g\circ f)'(x_0)=g'(y_0)f'(x_0)$ Since $f'(x_0) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$ and Since $g'(y_0) = \lim_{y\to y_0} \frac{g(y)-g(y_0)}{y-y_0}$ Multiply them to get: $$g'(y_0)f'(x_0) = \lim_{y\to y_0} \frac{g(y)-g(y_0)}{y-y_0} \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$$ since $y=f(x)$ and $y_0 = f(x_0)$, we have:  $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} \lim_{x\to x_0}  \frac{g(f(x))-g(f(x_0))}{f(x)-f(x_0)} \frac{f(x)-f(x_0)}{x-x_0}$$ $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} \lim_{x\to x_0}  \frac{g(f(x))-g(f(x_0))}{x-x_0}$$ $$g'(f(x_0))f'(x_0) = \lim_{f(x)\to f(x_0)} (g\circ f)'(x_0)$$ not sure how to get rid of that limit. proof: (2nd try) To prove the chain rule, I use Newton's approximation. We need to show that: $$\forall \epsilon >0\text{   }\exists \delta \text{  such that  } |g(f(x))-g(f(x_0))-f'(x_0)g'(y_0)(x-x_0)-|\leq \epsilon |x-x_0| \text{ whenever }|x-x_0|\leq \delta\text{and} x\in X$$ Since $g(y)$ is differentiable at $y_0$, we Know: $$\forall \epsilon_1>0 \text{   }\exists \delta_1 \text{such that}  |g(y)-g(y_0)-g'(y_0)(y-y_0)|\leq\epsilon_{1}|y-y_0| \text{ whenever }|y-y_0|\leq \delta_1\text{and} y\in Y$$ for $x\neq x_0$ choose $\delta_1 =f'(x_0)(x-x_0)$ and $\epsilon_1 = \frac{\epsilon }{f'(x_0)(x-x_0)} $ Proof (3rd try): $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{g(y)-g(y_0)}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{f(x)-f(x_0)}{x-x_0}$$  $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{g(y)-g(y_0)}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{y-y_0}{x-x_0}$$ $$RHS = lim_{y\to y_0,y\in Y-\{y_o\}}\frac{y-y_0}{y-y_0}lim_{x\to x_0,x\in X-\{x_o\}}\frac{g(f(x))-g(f(x_0))}{x-x_0}$$  $$RHS = lim_{x\to x_0,x\in X-\{x_o\}}\frac{g(f(x))-g(f(x_0))}{x-x_0}=LHS$$ Is this correct?",,"['real-analysis', 'analysis', 'derivatives', 'proof-verification']"
62,"How many critical values does $f$ have on $(0,10)$, given $f '(x)=\frac{\cos^2 x}{x} -\frac{1}{5}$?","How many critical values does  have on , given ?","f (0,10) f '(x)=\frac{\cos^2 x}{x} -\frac{1}{5}","How many critical values does $f$ have on open interval $(0, 10)$ given $$f'(x) = \frac{\cos^2x}{x} - \frac{1}{5}$$ I'm in calculus AB and this is a review question. I think the next step is to make it $$\cos x = \sqrt{\frac{x}{5}}$$ But I am not really sure.","How many critical values does $f$ have on open interval $(0, 10)$ given $$f'(x) = \frac{\cos^2x}{x} - \frac{1}{5}$$ I'm in calculus AB and this is a review question. I think the next step is to make it $$\cos x = \sqrt{\frac{x}{5}}$$ But I am not really sure.",,"['calculus', 'derivatives']"
63,Evaluate $f^{\prime}(0)$ where $f(x) = \begin{cases} g(x) \sin(1/x)& \mbox{if} \quad x \neq 0 \\ 0 &\mbox{if} \quad x = 0 \end{cases}$,Evaluate  where,f^{\prime}(0) f(x) = \begin{cases} g(x) \sin(1/x)& \mbox{if} \quad x \neq 0 \\ 0 &\mbox{if} \quad x = 0 \end{cases},"I am trying to evaluate $f^\prime (0)$, where $$ f(x) = \begin{cases} g(x) \sin(1/x)& \mbox{if} \quad x \neq 0 \\ 0 &\mbox{if} \quad  x = 0, \end{cases} $$ and $g(0) = g^{\prime}(0) = 0$. I started by noting that $g$ is differentiable at $x = 0$, hence, $g$ is continuous at $x = 0$. Therefore, $$\lim_{x \to 0} g(x) = g(0) = 0.$$ Now, $$ f^\prime (0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x-0} \stackrel{*}{=} \lim_{x \to 0} \frac{g(x) \sin (1/x)}{x} = \lim_{x \to 0} g^\prime (x) \frac{\sin(1/x)}{x}. $$ I'm not too sure how to proceed. I've already tried using L'Hospital's rule at $*$.","I am trying to evaluate $f^\prime (0)$, where $$ f(x) = \begin{cases} g(x) \sin(1/x)& \mbox{if} \quad x \neq 0 \\ 0 &\mbox{if} \quad  x = 0, \end{cases} $$ and $g(0) = g^{\prime}(0) = 0$. I started by noting that $g$ is differentiable at $x = 0$, hence, $g$ is continuous at $x = 0$. Therefore, $$\lim_{x \to 0} g(x) = g(0) = 0.$$ Now, $$ f^\prime (0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x-0} \stackrel{*}{=} \lim_{x \to 0} \frac{g(x) \sin (1/x)}{x} = \lim_{x \to 0} g^\prime (x) \frac{\sin(1/x)}{x}. $$ I'm not too sure how to proceed. I've already tried using L'Hospital's rule at $*$.",,"['real-analysis', 'derivatives']"
64,Find the $n^{th}$ order derivative of $x^n \ln x$,Find the  order derivative of,n^{th} x^n \ln x,"I'm doing it completely wrong, I'm sure, but I'll still show my attempt: $n^\text{th}$ order derivative of $x^n$ is $n!$ and of $\ln x$ is $(-1)^{(n-1)} (n-1)! x^{-n}$ So, using Leibnitz rule I got $n^\text{th}$ order derivative of $x^n \ln x$ as $$n!\ln x + n(n-1) \cdots 1 \cdot x \frac1x + \cdots + x^n \frac{1}{x^n} (-1)^{(n-1)} $$ But the form I'm supposed to get is $n! \left[ \ln x +1 + \frac12 + \cdots + \frac1n\right]$ Please help!","I'm doing it completely wrong, I'm sure, but I'll still show my attempt: $n^\text{th}$ order derivative of $x^n$ is $n!$ and of $\ln x$ is $(-1)^{(n-1)} (n-1)! x^{-n}$ So, using Leibnitz rule I got $n^\text{th}$ order derivative of $x^n \ln x$ as $$n!\ln x + n(n-1) \cdots 1 \cdot x \frac1x + \cdots + x^n \frac{1}{x^n} (-1)^{(n-1)} $$ But the form I'm supposed to get is $n! \left[ \ln x +1 + \frac12 + \cdots + \frac1n\right]$ Please help!",,['derivatives']
65,How the derivative might fail to exist,How the derivative might fail to exist,,Can a function have both a vertical tangent and cusp? Does The function $3x^{1/3}(x+2)$ have a vertical tangent and if so why? I believe that it has a cusp.,Can a function have both a vertical tangent and cusp? Does The function $3x^{1/3}(x+2)$ have a vertical tangent and if so why? I believe that it has a cusp.,,[]
66,Computing derivative of parametric equation,Computing derivative of parametric equation,,"This is probably a silly question but I am just not sure if I understand what to do. So I have the parametric equations: $x=6\cos (t)-2\\   y=5\sin (t)+3$ I am asked to compute $\dfrac{dy}{dx}$ at the point $(3\sqrt{2}-2, 3-\frac{5}{\sqrt{2}})$ So  $$\frac{dy}{dx}=\frac{\frac{dy}{dt}}{\frac{dx}{dt}}\implies\frac{dy}{dx}=-\frac{5\cos(t)}{6\sin(t)}$$ Now where do I plug the above numbers in to find $\dfrac{dy}{dx}$? Am I on the right track?","This is probably a silly question but I am just not sure if I understand what to do. So I have the parametric equations: $x=6\cos (t)-2\\   y=5\sin (t)+3$ I am asked to compute $\dfrac{dy}{dx}$ at the point $(3\sqrt{2}-2, 3-\frac{5}{\sqrt{2}})$ So  $$\frac{dy}{dx}=\frac{\frac{dy}{dt}}{\frac{dx}{dt}}\implies\frac{dy}{dx}=-\frac{5\cos(t)}{6\sin(t)}$$ Now where do I plug the above numbers in to find $\dfrac{dy}{dx}$? Am I on the right track?",,"['derivatives', 'parametric']"
67,Finding a tangent of an algebraic curve: $xy = 1$ [Well written explanation],Finding a tangent of an algebraic curve:  [Well written explanation],xy = 1,"I want to find, using (easy) calculus, the slope of a tangent to the algebraic curve $xy = 1$. The tangent I want to find is the tangent that passes through the point $(x_i,y_i) = (0,f(t))$. $f$ is defined as $f(t) = 4\cdot2e^{-t/10}$. I know the general way of finding the slope of a tangent to a function that passes through a point $P$ outside of the function is to put: $$y-y_i = y'(x-x_i)$$ The problem for me is then to find $x_i$. I've tried the following: Implicit differentiation of $xy=1$ gives me:  $$y' = -\frac{y}{x}$$ I then attempt to use the general method of finding tangents that pass through a point. $y-0 = -\dfrac{y}{x}\left(x-4\cdot2^{t/10}\right) \iff 1 = -1+\dfrac{1}{x}\cdot4\cdot2^{t/10} \iff x=2\cdot2^{t/10}$ I'm not sure if I even chose the right approach. Any help appreciated, thank you in advance.","I want to find, using (easy) calculus, the slope of a tangent to the algebraic curve $xy = 1$. The tangent I want to find is the tangent that passes through the point $(x_i,y_i) = (0,f(t))$. $f$ is defined as $f(t) = 4\cdot2e^{-t/10}$. I know the general way of finding the slope of a tangent to a function that passes through a point $P$ outside of the function is to put: $$y-y_i = y'(x-x_i)$$ The problem for me is then to find $x_i$. I've tried the following: Implicit differentiation of $xy=1$ gives me:  $$y' = -\frac{y}{x}$$ I then attempt to use the general method of finding tangents that pass through a point. $y-0 = -\dfrac{y}{x}\left(x-4\cdot2^{t/10}\right) \iff 1 = -1+\dfrac{1}{x}\cdot4\cdot2^{t/10} \iff x=2\cdot2^{t/10}$ I'm not sure if I even chose the right approach. Any help appreciated, thank you in advance.",,"['calculus', 'derivatives']"
68,Richardson Extrapolation - problems understanding how it works,Richardson Extrapolation - problems understanding how it works,,"I'm doing homework, and I am stumped on the first problem. I'm given this: Apply the extrapolation process described in Example 1 to determine $N_3(h)$, an approximation to $f(x_0)$, for the following functions and stepsizes. Where $f(x) = ln(x), x_0 = 1, h = 0.4$ So I go to build the table. In the reference example they use forward difference, so I will use that here. $$ N_1(0.4) = \frac{f(1.4)-f(1)}{0.4}=0.841181 \\ N_1(0.2) = \frac{f(1.2)-f(1)}{0.2}=0.911608 \\ N_1(0.1) = \frac{f(1.1)-f(1)}{0.1}=0.953102 \\ N_2(0.4) = N_1(0.2)+(N_1(0.2) - N_1(0.4)) = .982035 \\ N_2(0.2) = N_1(0.1)+(N_1(0.1) - N_1(0.2)) = .994596 \\ N_3(0.4) = N_2(0.2) + (N_2(0.2) - N_2(0.4)) = 1.077584 $$ This is wrong, according to the book this approximation should be around 1.0000109. Can anyone give me a in-depth explanation of where I'm going wrong here? I'm trying to understand this, but it doesn't help when your professor just does book examples and doesn't explain much even when asked. Thanks!","I'm doing homework, and I am stumped on the first problem. I'm given this: Apply the extrapolation process described in Example 1 to determine $N_3(h)$, an approximation to $f(x_0)$, for the following functions and stepsizes. Where $f(x) = ln(x), x_0 = 1, h = 0.4$ So I go to build the table. In the reference example they use forward difference, so I will use that here. $$ N_1(0.4) = \frac{f(1.4)-f(1)}{0.4}=0.841181 \\ N_1(0.2) = \frac{f(1.2)-f(1)}{0.2}=0.911608 \\ N_1(0.1) = \frac{f(1.1)-f(1)}{0.1}=0.953102 \\ N_2(0.4) = N_1(0.2)+(N_1(0.2) - N_1(0.4)) = .982035 \\ N_2(0.2) = N_1(0.1)+(N_1(0.1) - N_1(0.2)) = .994596 \\ N_3(0.4) = N_2(0.2) + (N_2(0.2) - N_2(0.4)) = 1.077584 $$ This is wrong, according to the book this approximation should be around 1.0000109. Can anyone give me a in-depth explanation of where I'm going wrong here? I'm trying to understand this, but it doesn't help when your professor just does book examples and doesn't explain much even when asked. Thanks!",,"['derivatives', 'numerical-methods', 'extrapolation']"
69,Solving $x\sin(\frac 1x)$ via limit definition,Solving  via limit definition,x\sin(\frac 1x),"I'm trying to show that the derivative of $x\sin(\frac 1x)$ exists and is equal to $\sin(\frac 1x)-\frac {\cos(\frac 1x)}x$ for every point in its domain via the limit definition (I can of course just use the product and chain rules, but I want to solve it via the definition). So I have the limit: $$\lim_{x\to c} \frac {x\sin(\frac 1x) - c\sin(\frac 1c)}{x-c}$$ Here's what I've got so far: I know I need to get that $x-c$ to cancel on bottom and that I need a $\sin(\frac 1x)$ or $\sin(\frac 1c)$ (either will evaluate to $\sin(\frac 1c)$ after I apply the limit) in my answer so I added $c\sin(\frac 1x)-c\sin(\frac 1x)$ to the numerator.  Then I get: $$\lim_{x\to c} \frac {(x-c)\sin(\frac 1x)+c(\sin(\frac 1x)-\sin(\frac 1c))}{x-c}$$ $$= \lim_{x\to c} \left[\sin(\frac 1x)+c\frac {\sin(\frac 1x)-\sin(\frac 1c)}{x-c}\right]$$ So I see that that second term should evaluate to $-\frac{\cos(\frac 1c)}c$, but I can't see how to get there algebraically. I asked an upper-division friend of mine and he recommended that I try to use a subsequential limit, but I thought that only applied when $x\to \infty$. Is this algebraic way the best way to do this problem?  And if so, what's the next step?  If not, should I be trying to find a subsequential limit?","I'm trying to show that the derivative of $x\sin(\frac 1x)$ exists and is equal to $\sin(\frac 1x)-\frac {\cos(\frac 1x)}x$ for every point in its domain via the limit definition (I can of course just use the product and chain rules, but I want to solve it via the definition). So I have the limit: $$\lim_{x\to c} \frac {x\sin(\frac 1x) - c\sin(\frac 1c)}{x-c}$$ Here's what I've got so far: I know I need to get that $x-c$ to cancel on bottom and that I need a $\sin(\frac 1x)$ or $\sin(\frac 1c)$ (either will evaluate to $\sin(\frac 1c)$ after I apply the limit) in my answer so I added $c\sin(\frac 1x)-c\sin(\frac 1x)$ to the numerator.  Then I get: $$\lim_{x\to c} \frac {(x-c)\sin(\frac 1x)+c(\sin(\frac 1x)-\sin(\frac 1c))}{x-c}$$ $$= \lim_{x\to c} \left[\sin(\frac 1x)+c\frac {\sin(\frac 1x)-\sin(\frac 1c)}{x-c}\right]$$ So I see that that second term should evaluate to $-\frac{\cos(\frac 1c)}c$, but I can't see how to get there algebraically. I asked an upper-division friend of mine and he recommended that I try to use a subsequential limit, but I thought that only applied when $x\to \infty$. Is this algebraic way the best way to do this problem?  And if so, what's the next step?  If not, should I be trying to find a subsequential limit?",,"['real-analysis', 'derivatives']"
70,how to maximize weekly revenue using profit function and derivatives,how to maximize weekly revenue using profit function and derivatives,,p=45-0.01q where p is price of each product sold and q is the quantity of products sold. a) find the quantity that maximizes the weekly revenue of the company b) what price should the company sell each product for in order to maximize the weekly revenue c) what is the maximum weekly revenue for the company? I found the revenue function R=45q-0.01q^2 and the derivative of it R'=45-0.02q but don't know what to do next  or how to apply those,p=45-0.01q where p is price of each product sold and q is the quantity of products sold. a) find the quantity that maximizes the weekly revenue of the company b) what price should the company sell each product for in order to maximize the weekly revenue c) what is the maximum weekly revenue for the company? I found the revenue function R=45q-0.01q^2 and the derivative of it R'=45-0.02q but don't know what to do next  or how to apply those,,"['calculus', 'derivatives']"
71,Partial derivative of convolution,Partial derivative of convolution,,"I have a convolution: $$g(x,\alpha) = \int_D \phi(t)f(x-t,\alpha)dt,$$ where $D$ is compact. I need to calculate $\frac{\partial}{\partial \alpha}g(x,\alpha)$. Under what conditions: $$\frac{\partial}{\partial \alpha}g(x,\alpha) = \int_D \phi(t)\frac{\partial}{\partial \alpha}f(x-t,\alpha)dt\,?$$ Do I need to walk the tortuous road of finding an integrable upper bound for the partial derivative of $f$ in order to appeal to the dominated convergence theorem?","I have a convolution: $$g(x,\alpha) = \int_D \phi(t)f(x-t,\alpha)dt,$$ where $D$ is compact. I need to calculate $\frac{\partial}{\partial \alpha}g(x,\alpha)$. Under what conditions: $$\frac{\partial}{\partial \alpha}g(x,\alpha) = \int_D \phi(t)\frac{\partial}{\partial \alpha}f(x-t,\alpha)dt\,?$$ Do I need to walk the tortuous road of finding an integrable upper bound for the partial derivative of $f$ in order to appeal to the dominated convergence theorem?",,"['derivatives', 'partial-derivative', 'convolution']"
72,"Verify Lagrange's Mean Value Theorem for $f(x)=4-(6-x)^{\frac23}$ on $[5,7]$",Verify Lagrange's Mean Value Theorem for  on,"f(x)=4-(6-x)^{\frac23} [5,7]","$f(x)=4-(6-x)^{\frac23}$ Well, $f(5)=3, f(7)=3$, so, $\frac{f(7)-f(5)}{7-5}=0$ And $f'(x)= \frac23 (6-x)^{-\frac13}$ So, by MVT, $\exists c\in (5,7)$ such that $\frac{f(7)-f(5)}{7-5}=f'(c)$, i.e. $\frac23 (6-c)^{-\frac13}=0$, which doesn't yield any solution. Where did I go wrong? Note: One possibility that comes to my mind is that the conditions of MVT are not satisfied here, but if so, I cannot identify that. Please help.","$f(x)=4-(6-x)^{\frac23}$ Well, $f(5)=3, f(7)=3$, so, $\frac{f(7)-f(5)}{7-5}=0$ And $f'(x)= \frac23 (6-x)^{-\frac13}$ So, by MVT, $\exists c\in (5,7)$ such that $\frac{f(7)-f(5)}{7-5}=f'(c)$, i.e. $\frac23 (6-c)^{-\frac13}=0$, which doesn't yield any solution. Where did I go wrong? Note: One possibility that comes to my mind is that the conditions of MVT are not satisfied here, but if so, I cannot identify that. Please help.",,['derivatives']
73,Existence of $\mathbb{C}$-differentiable function in neighbourhood of $0$,Existence of -differentiable function in neighbourhood of,\mathbb{C} 0,"I have a problem with the following: Is there any $\mathbb{C}$-differentiable function in neighbourhood of $0$ such that a) $f(1/n)=(-1)^n \frac{1}{n}$ for $n=1,2,\dots$ b) $f(1/n)=\frac{1}{n^2-1}$ for $n=2,3,\dots$ c) $|f(1/n)|\le e^{-n}$, and there is a point at which $f$ is not $0$. I completely dont have idea how can i tackle such problems. Thank you for any help. I need it asap... Thank you a lot.","I have a problem with the following: Is there any $\mathbb{C}$-differentiable function in neighbourhood of $0$ such that a) $f(1/n)=(-1)^n \frac{1}{n}$ for $n=1,2,\dots$ b) $f(1/n)=\frac{1}{n^2-1}$ for $n=2,3,\dots$ c) $|f(1/n)|\le e^{-n}$, and there is a point at which $f$ is not $0$. I completely dont have idea how can i tackle such problems. Thank you for any help. I need it asap... Thank you a lot.",,"['calculus', 'complex-analysis', 'analysis', 'derivatives']"
74,$\frac{\mathrm d}{\mathrm{d}x}3ax^3$ not equal to $9x^2$?,not equal to ?,\frac{\mathrm d}{\mathrm{d}x}3ax^3 9x^2,"Okay, so I'm doing some Khan Academy stuff and they ask me to take $\frac{\mathrm d}{\mathrm{d}x}$ of this function: $f(x)=3ax^3+ax^2$ where $f''(0.5)=3$. They say that the derivative of $f(x)$ is $9ax^2 + 2ax$. This doesn't make sense to me though. The constant rule states that $\frac{\mathrm d}{\mathrm{d}x}(cf)=cf'$. The product rule states that $\frac{\mathrm d}{\mathrm{d}x}(fg)=fg'+f'g$. $3$ is a constant.  $a$ is not.  $\frac{\mathrm d}{\mathrm{d}x}(x^3)=3x^2$. So $\frac{\mathrm d}{\mathrm{d}x}(3ax^3)=3\cdot\frac{\mathrm d}{\mathrm{d}x}(a)\cdot\frac{\mathrm d}{\mathrm{d}x}(x^3)=3\cdot 1\cdot\frac{\mathrm d}{\mathrm{d}x}(x^3)$, where $3$ is a constant and $x^3$ is not. $3\frac{\mathrm d}{\mathrm{d}x}(x^3)=3\cdot3x^2=9x^2$. Khan Academy says that $\frac{\mathrm d}{\mathrm{d}x}(3ax^3+ax^2)=9ax^2+2ax$.  Why all the $a$'s? So where am I going wrong?  Is $3$ a constant of $a$, and so when you take the derivative of $3a$ (which is $3$) and multiply it by $x^3$ you need to use the product rule? Or do you do $\frac{\mathrm d}{\mathrm{d}x}(3)\cdot\frac{\mathrm d}{\mathrm{d}x}(ax^3)$? $\frac{\mathrm d}{\mathrm{d}x}(ax^3)$ with the product rule? Moreover, they say that $f''(0.5)=18a(0.5)+2a$. Where did the $x$ go?  If $x=0.5$ then $2ax$ should be equal to $a\cdot 2\cdot 0.5=a$. I'll copy the entire problem down for you below. If $f(x)=3ax^3+ax^2$ and $f′′(0.5)=3$, then what is the value of $a$? a. $3/11$, b. $1/4$, c. $3/7$, d. $1/2$, e. None of the above We need to find the second derivative of the function $3ax^3+ax^2$.  The power rule is given by $\frac{\mathrm d}{\mathrm{d}x}(x^n)=nx^{n−1}$.  Using the power rule, we find the first derivative of $f(x)$. $f'(x)=9ax^2+2ax$ Using the power rule again, we find the second derivative of $f(x)$. $f''(x)=18ax+2a$ Finally, since we know that $f′′(0.5)=3$, we set $x=0.5$ and solve for $a$. $f''(0.5)=18a(0.5)+2a$ $3=11a$ $a=3/11$","Okay, so I'm doing some Khan Academy stuff and they ask me to take $\frac{\mathrm d}{\mathrm{d}x}$ of this function: $f(x)=3ax^3+ax^2$ where $f''(0.5)=3$. They say that the derivative of $f(x)$ is $9ax^2 + 2ax$. This doesn't make sense to me though. The constant rule states that $\frac{\mathrm d}{\mathrm{d}x}(cf)=cf'$. The product rule states that $\frac{\mathrm d}{\mathrm{d}x}(fg)=fg'+f'g$. $3$ is a constant.  $a$ is not.  $\frac{\mathrm d}{\mathrm{d}x}(x^3)=3x^2$. So $\frac{\mathrm d}{\mathrm{d}x}(3ax^3)=3\cdot\frac{\mathrm d}{\mathrm{d}x}(a)\cdot\frac{\mathrm d}{\mathrm{d}x}(x^3)=3\cdot 1\cdot\frac{\mathrm d}{\mathrm{d}x}(x^3)$, where $3$ is a constant and $x^3$ is not. $3\frac{\mathrm d}{\mathrm{d}x}(x^3)=3\cdot3x^2=9x^2$. Khan Academy says that $\frac{\mathrm d}{\mathrm{d}x}(3ax^3+ax^2)=9ax^2+2ax$.  Why all the $a$'s? So where am I going wrong?  Is $3$ a constant of $a$, and so when you take the derivative of $3a$ (which is $3$) and multiply it by $x^3$ you need to use the product rule? Or do you do $\frac{\mathrm d}{\mathrm{d}x}(3)\cdot\frac{\mathrm d}{\mathrm{d}x}(ax^3)$? $\frac{\mathrm d}{\mathrm{d}x}(ax^3)$ with the product rule? Moreover, they say that $f''(0.5)=18a(0.5)+2a$. Where did the $x$ go?  If $x=0.5$ then $2ax$ should be equal to $a\cdot 2\cdot 0.5=a$. I'll copy the entire problem down for you below. If $f(x)=3ax^3+ax^2$ and $f′′(0.5)=3$, then what is the value of $a$? a. $3/11$, b. $1/4$, c. $3/7$, d. $1/2$, e. None of the above We need to find the second derivative of the function $3ax^3+ax^2$.  The power rule is given by $\frac{\mathrm d}{\mathrm{d}x}(x^n)=nx^{n−1}$.  Using the power rule, we find the first derivative of $f(x)$. $f'(x)=9ax^2+2ax$ Using the power rule again, we find the second derivative of $f(x)$. $f''(x)=18ax+2a$ Finally, since we know that $f′′(0.5)=3$, we set $x=0.5$ and solve for $a$. $f''(0.5)=18a(0.5)+2a$ $3=11a$ $a=3/11$",,"['calculus', 'derivatives']"
75,Nowhere differentiability of Weierstrass function,Nowhere differentiability of Weierstrass function,,"It's again from Tao's book. Let  $f:\mathbb{R}\rightarrow \mathbb{R}$ be the function $$f:= \sum_{n=1}^\infty 4^{-n} \sin(8^n\pi x)$$ Show that for every 8-dyadic interval $[\frac{j}{8^k},\frac{j+1}{8^k}]$ with $k\geq 1$, one has $|f(\frac{j+1}{8^k})-f(\frac{j}{8^k})| \geq C4^{-k}$ for some absolute constant $C > 0$.For $n>k, |f(\frac{j+1}{8^k})-f(\frac{j}{8^k})|=0$. But how to get the inequality when $n<k$?","It's again from Tao's book. Let  $f:\mathbb{R}\rightarrow \mathbb{R}$ be the function $$f:= \sum_{n=1}^\infty 4^{-n} \sin(8^n\pi x)$$ Show that for every 8-dyadic interval $[\frac{j}{8^k},\frac{j+1}{8^k}]$ with $k\geq 1$, one has $|f(\frac{j+1}{8^k})-f(\frac{j}{8^k})| \geq C4^{-k}$ for some absolute constant $C > 0$.For $n>k, |f(\frac{j+1}{8^k})-f(\frac{j}{8^k})|=0$. But how to get the inequality when $n<k$?",,"['real-analysis', 'derivatives', 'examples-counterexamples', 'trigonometric-series']"
76,Using the limit definition to find the derivative of this function.,Using the limit definition to find the derivative of this function.,,f(x)=sin3x you have to use the limit definition to find it and I plugged x+h in and everything and I used the sin sum formula but I just can't get rid of the h on the bottom!,f(x)=sin3x you have to use the limit definition to find it and I plugged x+h in and everything and I used the sin sum formula but I just can't get rid of the h on the bottom!,,"['limits', 'derivatives']"
77,"If $\lim_{t\to\infty}\varphi(t)=x_0$, does this imply that $\lim_{t\to\infty}\varphi'(t)=0$?","If , does this imply that ?",\lim_{t\to\infty}\varphi(t)=x_0 \lim_{t\to\infty}\varphi'(t)=0,"Let $\phi:\mathbb{R} \to \mathbb{R}^n$ and $\lim_{t \to \infty} \phi(t) = X_0$, where $X_0$ is a constant in $\mathbb{R}^n$ then $\lim_{t\to \infty} \phi'(t) = 0$. I search everywhere and I need to know if this is true. Thanks!","Let $\phi:\mathbb{R} \to \mathbb{R}^n$ and $\lim_{t \to \infty} \phi(t) = X_0$, where $X_0$ is a constant in $\mathbb{R}^n$ then $\lim_{t\to \infty} \phi'(t) = 0$. I search everywhere and I need to know if this is true. Thanks!",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
78,Incongruencies with derivatives and differencials,Incongruencies with derivatives and differencials,,"I read in Piskunov that the increment $\Delta y$ of a function can be written as: $\Delta y = f'(x) \Delta x + \alpha \Delta x$ And, when ${\Delta x\to 0}$ , $dy=f'(x)dx$ The problem is, doesn't that mean that it is possible to write the derivative of $f$ as the quocient $f'(x)=dy/dx$   ? It is my understanding that a derivative can be written as the limit of a quocient $\lim\limits_{h\to 0} \frac {f(x+h)-f(x)}{h}$ But not as a quocient of limits, because the denominator would be zero. I started this month my first calculus course, and I am very confused by some of these details, specially with those that have to do with notions of infinitesimals. I also have a lot of doubts in regards to Leibniz notation, as it is repeatedly used as a quocient when, according to every source, $\frac {df(x)}{dx}$ is just notation for $f'(x)$, and not a quocient. An example of this is the following deduction from my physics course (in portuguese) Where de ""denominator"" is transfared. Any light shed on this would be greatly appreciated.","I read in Piskunov that the increment $\Delta y$ of a function can be written as: $\Delta y = f'(x) \Delta x + \alpha \Delta x$ And, when ${\Delta x\to 0}$ , $dy=f'(x)dx$ The problem is, doesn't that mean that it is possible to write the derivative of $f$ as the quocient $f'(x)=dy/dx$   ? It is my understanding that a derivative can be written as the limit of a quocient $\lim\limits_{h\to 0} \frac {f(x+h)-f(x)}{h}$ But not as a quocient of limits, because the denominator would be zero. I started this month my first calculus course, and I am very confused by some of these details, specially with those that have to do with notions of infinitesimals. I also have a lot of doubts in regards to Leibniz notation, as it is repeatedly used as a quocient when, according to every source, $\frac {df(x)}{dx}$ is just notation for $f'(x)$, and not a quocient. An example of this is the following deduction from my physics course (in portuguese) Where de ""denominator"" is transfared. Any light shed on this would be greatly appreciated.",,"['calculus', 'derivatives', 'notation']"
79,Can someone explain how to calculate the third order partial derivative of $f.$,Can someone explain how to calculate the third order partial derivative of,f.,"$f(x,y)=\sin(xy).$ I calculated that $ \dfrac{\partial^2f}{\partial x\,\partial y}=\dfrac{\partial^2f}{\partial y\,\partial x}=\cos(xy)-xy\sin(xy)$. I also calculated $$ \frac{\partial^3f}{\partial x^2\partial y}= -2y\sin(xy)-xy^2\cos(xy)$$ and  $$\frac{\partial^3f}{\partial y^2\partial x}=-2x\sin(xy)-x^2y\cos(xy).$$ However I am not sure whether this is the correct method to calculate the third order partial derivative because $$ \frac{\partial^3f}{\partial x^2\partial y} \neq \frac{\partial^3f}{\partial y^2\partial x}.$$ Can someone explain how to calculate the third order partial derivative of $f.$","$f(x,y)=\sin(xy).$ I calculated that $ \dfrac{\partial^2f}{\partial x\,\partial y}=\dfrac{\partial^2f}{\partial y\,\partial x}=\cos(xy)-xy\sin(xy)$. I also calculated $$ \frac{\partial^3f}{\partial x^2\partial y}= -2y\sin(xy)-xy^2\cos(xy)$$ and  $$\frac{\partial^3f}{\partial y^2\partial x}=-2x\sin(xy)-x^2y\cos(xy).$$ However I am not sure whether this is the correct method to calculate the third order partial derivative because $$ \frac{\partial^3f}{\partial x^2\partial y} \neq \frac{\partial^3f}{\partial y^2\partial x}.$$ Can someone explain how to calculate the third order partial derivative of $f.$",,"['derivatives', 'partial-derivative']"
80,Minimum volume cone.,Minimum volume cone.,,"What would be the radius and the altitude of a right circular cone that circumscribes a sphere with a radius 8 cm if the volume of the cone is to be minimized? Here is my rough sketch; My idea is to write some characteristics of the cone as a function of the radius of the circle, minimize it with differential calculus and connect those characteristics to the base radius and height of the cone, however I'm stuck at step 1.","What would be the radius and the altitude of a right circular cone that circumscribes a sphere with a radius 8 cm if the volume of the cone is to be minimized? Here is my rough sketch; My idea is to write some characteristics of the cone as a function of the radius of the circle, minimize it with differential calculus and connect those characteristics to the base radius and height of the cone, however I'm stuck at step 1.",,"['calculus', 'geometry', 'derivatives', 'optimization']"
81,Logarithmic Differentiation - when to use?,Logarithmic Differentiation - when to use?,,"Sorry if this is an ignorant or uninformed question, but I would like to know when I can (or should use) logarithmic differentiation. I haven't taken calculus in a while so I'm quite rusty. So, let's say I have the following equation: $$y = \sqrt[3]{x^2 +3} /  \sqrt[]{x^2 +1}$$ I know how to solve this using logarithmic differentiation, but I'm also wondering if it'd be acceptable, or plausible, to solve using the quotient rule. Similarly, for equations that I can solve using various rules (like chain rule, product rule, etc), am I also allowed to used logarithmic differentiation instead? My understanding of it remains vague. Thanks!","Sorry if this is an ignorant or uninformed question, but I would like to know when I can (or should use) logarithmic differentiation. I haven't taken calculus in a while so I'm quite rusty. So, let's say I have the following equation: $$y = \sqrt[3]{x^2 +3} /  \sqrt[]{x^2 +1}$$ I know how to solve this using logarithmic differentiation, but I'm also wondering if it'd be acceptable, or plausible, to solve using the quotient rule. Similarly, for equations that I can solve using various rules (like chain rule, product rule, etc), am I also allowed to used logarithmic differentiation instead? My understanding of it remains vague. Thanks!",,"['derivatives', 'logarithms']"
82,How to determine whether a piecewise function has a derivative?,How to determine whether a piecewise function has a derivative?,,"Could someone show me a worked example of showing whether a piecewise function is differentiable at some $x=a$? I can show that it is continuous at $a$, as the limit as $x\to a$ (from both sides) equals the value of the function at that point. The problem I have is what I do next. Is there any other method than taking the limit as $h\to 0$ of $( f(a+h) - f(a) )/ h$, that is simpler/easier? Also, if I use the $h\to 0$ method.... well what does $f(a+h)$ mean for a piecewise function? What is its value?","Could someone show me a worked example of showing whether a piecewise function is differentiable at some $x=a$? I can show that it is continuous at $a$, as the limit as $x\to a$ (from both sides) equals the value of the function at that point. The problem I have is what I do next. Is there any other method than taking the limit as $h\to 0$ of $( f(a+h) - f(a) )/ h$, that is simpler/easier? Also, if I use the $h\to 0$ method.... well what does $f(a+h)$ mean for a piecewise function? What is its value?",,"['calculus', 'derivatives']"
83,What is the derivative of a Radial Basis Interpolation function?,What is the derivative of a Radial Basis Interpolation function?,,"A radial basis interpolation function is described as: $ f(\textbf{x})=\sum_{k=1}^N c_k \phi(\lVert \textbf{x}-\textbf{x}_k \rVert_2), \ \textbf{x}\in\mathbb{R}^s $ where $\textbf{x}_k$ are the $N$ scattered points and $c_k$ are the coefficients of the function obtained by solving the linear system: $ \begin{bmatrix} f(\textbf{x}_1) \\ f(\textbf{x}_2) \\ \vdots \\ f(\textbf{x}_N)  \end{bmatrix} = \begin{bmatrix} \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  &  \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right)  & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)   \\  \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right) & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)  \\ \vdots & \vdots & \ddots & \vdots \\ \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right)  & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)  \\ \end{bmatrix} \begin{bmatrix} c_1 \\ c2 \\ \vdots \\ c_N \end{bmatrix} $ The radial basis function $\phi$ can be a Gaussian, an inverse multiquadric, etc. For a Gaussian, we get: $ \phi(x) = e^{-(\epsilon x)^2} $ for some free parameter $\epsilon$ How do you find the derivative of the function and is it well-defined on all values or do we get into zero-denominator situations? for example, what is $\frac{df(x)}{dy}$ if $\textbf{x}\in\mathbb{R}^2$?","A radial basis interpolation function is described as: $ f(\textbf{x})=\sum_{k=1}^N c_k \phi(\lVert \textbf{x}-\textbf{x}_k \rVert_2), \ \textbf{x}\in\mathbb{R}^s $ where $\textbf{x}_k$ are the $N$ scattered points and $c_k$ are the coefficients of the function obtained by solving the linear system: $ \begin{bmatrix} f(\textbf{x}_1) \\ f(\textbf{x}_2) \\ \vdots \\ f(\textbf{x}_N)  \end{bmatrix} = \begin{bmatrix} \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  &  \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right)  & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_1\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)   \\  \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right) & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_2\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)  \\ \vdots & \vdots & \ddots & \vdots \\ \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{1}\rVert} \right\rvert\right)  & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{2}\rVert} \right\rvert\right)  & \ldots & \varphi\left(\left\lvert \frac{1}{\lVert\textbf{x}_N\rVert} - \frac{1}{\lVert\textbf{x}_{N}\rVert} \right\rvert\right)  \\ \end{bmatrix} \begin{bmatrix} c_1 \\ c2 \\ \vdots \\ c_N \end{bmatrix} $ The radial basis function $\phi$ can be a Gaussian, an inverse multiquadric, etc. For a Gaussian, we get: $ \phi(x) = e^{-(\epsilon x)^2} $ for some free parameter $\epsilon$ How do you find the derivative of the function and is it well-defined on all values or do we get into zero-denominator situations? for example, what is $\frac{df(x)}{dy}$ if $\textbf{x}\in\mathbb{R}^2$?",,"['derivatives', 'partial-derivative', 'interpolation']"
84,Estimate value using Lagrange's MVT,Estimate value using Lagrange's MVT,,"Estimate the value of $51^{1/2}$ using Lagrange's MVT. Answer both in terms of inequalities and approximately estimated value. My method: Let $f(x)=x^{1/2}$ defined in $[49,51]$ and $f'(x)=\frac{x^{-3/2}}{2}$ which also exits in $(49,51)$ Applying Lagrange's MVT, let $c$ belong to $(49,51)$ We have $f'(c)=\frac{f(51)-f(49)}{2}$ and finally $f(51)=7+c^{-3/2}$ I am stuck at this step, how do I get an inequality as well as the value of $f(51)$?","Estimate the value of $51^{1/2}$ using Lagrange's MVT. Answer both in terms of inequalities and approximately estimated value. My method: Let $f(x)=x^{1/2}$ defined in $[49,51]$ and $f'(x)=\frac{x^{-3/2}}{2}$ which also exits in $(49,51)$ Applying Lagrange's MVT, let $c$ belong to $(49,51)$ We have $f'(c)=\frac{f(51)-f(49)}{2}$ and finally $f(51)=7+c^{-3/2}$ I am stuck at this step, how do I get an inequality as well as the value of $f(51)$?",,"['calculus', 'real-analysis', 'derivatives', 'linear-approximation']"
85,A polynomial agreeing with a function and its derivatives,A polynomial agreeing with a function and its derivatives,,"If we want  $$p(x_i)=a_i, \qquad x_1 < \dotsb < x_{n+1},$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this (Lagrange interpolation). If we want  $$p(x_i)=a_i, \qquad x_1<\dotsb < x_n\\ p'(\xi)=\alpha,$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this, except we can run into problems (for instance, suppose $a_i=0$ for all $i$ and $\xi$ is a zero of $\frac{d}{dx}(x\mapsto \prod_{i=1}^n (x-x_i))$, but $\alpha \neq 0$. Then this is impossible.) If we want  $$p(x)=a \\ p'(\xi_i)=\alpha_i,\qquad \xi_1< \dotsb < \xi_n$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this: take $q$ such that $q(x_i)=a_i$, and let $p(x):=\left(\int_0^xq\right) + c$ for the appropriate constant $c$. We've taken care of cases $(n+1,0)$, $(n,1)$, and $(1,n)$. It seems clear that cases like $(0,n+1)$ are in general not possible. What about intermediate cases, or prescribing values of the second or greater derivatives? What would be the method here? What is the general rule? Edit: @RobertIsrael, I wanted to try to modify your linear algebraic characterization to show that the normal Hermite interpolation exists uniquely. I.e. I want to show that a matrix such as  $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] $$ is invertible. I could try using your characterization of the determinant and show that  $$ \frac{\partial^4}{\partial x_2 \partial x_3^2 \partial x_5}\left((x_1, \dotsc, x_5)\mapsto \prod_{1\leq i < j <5}(x_i - x_j)\right) \left. \right|_{\substack{x_1=x_2=x_3=\alpha_1\\ x_4=x_5=\alpha_2}} \neq 0. $$ But so far I've been unable to do so in a way that extends more generally. I'm also trying to do column manipulations on matrices of the above type and come up with an induction proof (as with showing the determinant of the Vandermonde matrix) to show that they must be invertible. Any ideas? Edit 2: I have been able to show this, at another post: There is a unique polynomial interpolating $f$ and its derivatives","If we want  $$p(x_i)=a_i, \qquad x_1 < \dotsb < x_{n+1},$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this (Lagrange interpolation). If we want  $$p(x_i)=a_i, \qquad x_1<\dotsb < x_n\\ p'(\xi)=\alpha,$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this, except we can run into problems (for instance, suppose $a_i=0$ for all $i$ and $\xi$ is a zero of $\frac{d}{dx}(x\mapsto \prod_{i=1}^n (x-x_i))$, but $\alpha \neq 0$. Then this is impossible.) If we want  $$p(x)=a \\ p'(\xi_i)=\alpha_i,\qquad \xi_1< \dotsb < \xi_n$$ then there is a unique polynomial of degree $\leq n$ that accomplishes this: take $q$ such that $q(x_i)=a_i$, and let $p(x):=\left(\int_0^xq\right) + c$ for the appropriate constant $c$. We've taken care of cases $(n+1,0)$, $(n,1)$, and $(1,n)$. It seems clear that cases like $(0,n+1)$ are in general not possible. What about intermediate cases, or prescribing values of the second or greater derivatives? What would be the method here? What is the general rule? Edit: @RobertIsrael, I wanted to try to modify your linear algebraic characterization to show that the normal Hermite interpolation exists uniquely. I.e. I want to show that a matrix such as  $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] $$ is invertible. I could try using your characterization of the determinant and show that  $$ \frac{\partial^4}{\partial x_2 \partial x_3^2 \partial x_5}\left((x_1, \dotsc, x_5)\mapsto \prod_{1\leq i < j <5}(x_i - x_j)\right) \left. \right|_{\substack{x_1=x_2=x_3=\alpha_1\\ x_4=x_5=\alpha_2}} \neq 0. $$ But so far I've been unable to do so in a way that extends more generally. I'm also trying to do column manipulations on matrices of the above type and come up with an induction proof (as with showing the determinant of the Vandermonde matrix) to show that they must be invertible. Any ideas? Edit 2: I have been able to show this, at another post: There is a unique polynomial interpolating $f$ and its derivatives",,"['calculus', 'polynomials', 'derivatives', 'interpolation']"
86,"If $f$ has derivative at $1$ and $\lim_{h \to 0} {\frac{f(1+h)}{h} }=1$, then $f'(1)=0=f(1)$","If  has derivative at  and , then",f 1 \lim_{h \to 0} {\frac{f(1+h)}{h} }=1 f'(1)=0=f(1),"I need to prove that if $f(x)$ has derivative at $x=1$ and if $\lim_{h \to 0} {\frac{f(1+h)}{h} }=1$. then I need to prove that $f'(1)=0$ $f(1)=0$. It's pretty obovious if using arithmetic of limits, but it's impossible here. Any clues or general mindset how to prove it?","I need to prove that if $f(x)$ has derivative at $x=1$ and if $\lim_{h \to 0} {\frac{f(1+h)}{h} }=1$. then I need to prove that $f'(1)=0$ $f(1)=0$. It's pretty obovious if using arithmetic of limits, but it's impossible here. Any clues or general mindset how to prove it?",,"['calculus', 'limits', 'derivatives']"
87,Show that the value of $\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}}$ when $x=0$ is $\frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2$,Show that the value of  when  is,\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}} x=0 \frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2,"The question originally asks you to prove that if $y=\sin^{-1}(x)+(\sin^{-1}(x))^2$ that: $(1-x^2)y''-x y'$ is independent of $x$. I get that $(1-x^2)y''-x y'=2$ hence proving the first part. The second part asks you prove that for $n>1$ that $$(1-x^2)\frac{\text{d}^{n+2}y}{\text{d}x^{n+2}}-x(2n+1)\frac{\text{d}^{n+1}y}{\text{d}x^{n+1}}-n^2\frac{\text{d}^{n}y}{\text{d}x^{n}}=0$$  which again is fine (I did it using General Leibniz rule ). The problem I'm having is in the next part, it says show that the value of $\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}}$ when $x=0$ is: $$\frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2$$ I can't seem to be able to show it, I'm sure it must be something with a double factorial in since it the formula seems very similar to it but I'm not too sure how to go about it.","The question originally asks you to prove that if $y=\sin^{-1}(x)+(\sin^{-1}(x))^2$ that: $(1-x^2)y''-x y'$ is independent of $x$. I get that $(1-x^2)y''-x y'=2$ hence proving the first part. The second part asks you prove that for $n>1$ that $$(1-x^2)\frac{\text{d}^{n+2}y}{\text{d}x^{n+2}}-x(2n+1)\frac{\text{d}^{n+1}y}{\text{d}x^{n+1}}-n^2\frac{\text{d}^{n}y}{\text{d}x^{n}}=0$$  which again is fine (I did it using General Leibniz rule ). The problem I'm having is in the next part, it says show that the value of $\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}}$ when $x=0$ is: $$\frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2$$ I can't seem to be able to show it, I'm sure it must be something with a double factorial in since it the formula seems very similar to it but I'm not too sure how to go about it.",,"['calculus', 'algebra-precalculus', 'derivatives']"
88,Does my proof of $|x+y| \le |x| + |y|$ make sense? How do I conclude a proof?,Does my proof of  make sense? How do I conclude a proof?,|x+y| \le |x| + |y|,"Thank you for reading it. I know I made a lot of mistakes. This is my first ever proof that I have attempted. Another note is that I only have been studying proofs for about a week. Any advice will be helpful. prove: $|x+y| ≤ |x| + |y|$ Case 1: ∀ values of x<0 and y<0, the function will decrease:  $|x+y| \overset{x<0}= |y\pm x|$ $|x+y| \overset{y<0}= |-y+x)|$ $A=|-x+y|$ –-—-> $∂A/∂X=-1$ $B=|-y+x|$           $∂B/∂Y=-1$ Case 2: In the case of (x,y)>0, the two functions opposite of the inequalities are equal. {|x+y|⇔ |x|+|y|: x>0 and y>0} This is a normal property of the absolute value theorem. Notation: {|x+y|∀ values of x and y = |x|+|y| ∀ for all values of x and y} Case 3: Case 3 proves that the values of |x|+|y| are unaffected by values less than zero $|x| =  \begin{cases} x,&\text{if }x\ge 0\\ -x,&\text{if }x<0 \end{cases}$ $|y| =  \begin{cases} y,&\text{if }y\ge 0\\ -y&\text{if }y<0 \end{cases}$ ⇔$|X|+|y|>0$ when $(x,y)≠0$ Note: I don’t know if I properly stated the ∀correctly; however, I meant it as “for all“ Thank you for reading it. I know I made a lot of mistakes. This is my first ever proof that I have attempted. Another note is that I only have been studying proofs for about a week. Any advice will be helpful. ***Edited","Thank you for reading it. I know I made a lot of mistakes. This is my first ever proof that I have attempted. Another note is that I only have been studying proofs for about a week. Any advice will be helpful. prove: $|x+y| ≤ |x| + |y|$ Case 1: ∀ values of x<0 and y<0, the function will decrease:  $|x+y| \overset{x<0}= |y\pm x|$ $|x+y| \overset{y<0}= |-y+x)|$ $A=|-x+y|$ –-—-> $∂A/∂X=-1$ $B=|-y+x|$           $∂B/∂Y=-1$ Case 2: In the case of (x,y)>0, the two functions opposite of the inequalities are equal. {|x+y|⇔ |x|+|y|: x>0 and y>0} This is a normal property of the absolute value theorem. Notation: {|x+y|∀ values of x and y = |x|+|y| ∀ for all values of x and y} Case 3: Case 3 proves that the values of |x|+|y| are unaffected by values less than zero $|x| =  \begin{cases} x,&\text{if }x\ge 0\\ -x,&\text{if }x<0 \end{cases}$ $|y| =  \begin{cases} y,&\text{if }y\ge 0\\ -y&\text{if }y<0 \end{cases}$ ⇔$|X|+|y|>0$ when $(x,y)≠0$ Note: I don’t know if I properly stated the ∀correctly; however, I meant it as “for all“ Thank you for reading it. I know I made a lot of mistakes. This is my first ever proof that I have attempted. Another note is that I only have been studying proofs for about a week. Any advice will be helpful. ***Edited",,"['inequality', 'derivatives', 'proof-verification', 'absolute-value']"
89,Understanding the notation of a book when derivating,Understanding the notation of a book when derivating,,"I'm trying to understand the notation that the book  uses. The book says $(1)$ $y=a\cdot \sin x$ and then the derivate of $(1)$ is $(2)$ $\frac{d^2y}{dx^2}=-a \cdot \sin x$ I don't get what to do when derivating to get $(2)$, and what  exactly $\frac{d^2y}{dx^2}$ means.","I'm trying to understand the notation that the book  uses. The book says $(1)$ $y=a\cdot \sin x$ and then the derivate of $(1)$ is $(2)$ $\frac{d^2y}{dx^2}=-a \cdot \sin x$ I don't get what to do when derivating to get $(2)$, and what  exactly $\frac{d^2y}{dx^2}$ means.",,"['calculus', 'derivatives', 'notation']"
90,"Is $\tan(x)$ differentiable for $x\in ( -\pi/2 , \pi/2 )$",Is  differentiable for,"\tan(x) x\in ( -\pi/2 , \pi/2 )","This is an assignment question and in class we taught the definition that: A function $f(x)$ is differentiable if we can find $f(x+h) - f(x) = Kh +h E(x,h)$, where $K=f'(x)$ and $E(x,h) \rightarrow 0$   as $h \rightarrow 0$ E.g. $f(x) = x^3 \Rightarrow f(x+h) - f(x) = (x+h)^3 - x^3 = 3x^2h+2xh^2 +h^3$   and so $3x^2h+2xh^2+h^3 =Kh +h E(x,h) $. Thus we can conclude that $K = 3x^2$ and $E(x,h) = 2xh + h^2 $, and thus $f$ is differentiable since $E(x,h) = 2xh + h^2 \rightarrow 0$   as $h \rightarrow 0$ But for proving that $tan(x)$ is differentiable I started and got stuck: $\tan(x+h) - \tan(x) = \dfrac{\tan(h)(1+\tan^2(x))}{1-\tan(x)\tan(h)}$, I really want to find $K = 1+ \tan^2(x)$ but I can't find a way to get $(1+\tan^2(x))h$ without a fraction term, and $E(x,h) = 0 , h \rightarrow 0$. Any hints or help would be great, thanks.","This is an assignment question and in class we taught the definition that: A function $f(x)$ is differentiable if we can find $f(x+h) - f(x) = Kh +h E(x,h)$, where $K=f'(x)$ and $E(x,h) \rightarrow 0$   as $h \rightarrow 0$ E.g. $f(x) = x^3 \Rightarrow f(x+h) - f(x) = (x+h)^3 - x^3 = 3x^2h+2xh^2 +h^3$   and so $3x^2h+2xh^2+h^3 =Kh +h E(x,h) $. Thus we can conclude that $K = 3x^2$ and $E(x,h) = 2xh + h^2 $, and thus $f$ is differentiable since $E(x,h) = 2xh + h^2 \rightarrow 0$   as $h \rightarrow 0$ But for proving that $tan(x)$ is differentiable I started and got stuck: $\tan(x+h) - \tan(x) = \dfrac{\tan(h)(1+\tan^2(x))}{1-\tan(x)\tan(h)}$, I really want to find $K = 1+ \tan^2(x)$ but I can't find a way to get $(1+\tan^2(x))h$ without a fraction term, and $E(x,h) = 0 , h \rightarrow 0$. Any hints or help would be great, thanks.",,"['calculus', 'limits', 'derivatives']"
91,How to find $g'(0)$ if $g(x)=\sin(f(x^2+x)-2x)$ and $f$ satisfies $|f(x)|\le x^2$ for all $x$?,How to find  if  and  satisfies  for all ?,g'(0) g(x)=\sin(f(x^2+x)-2x) f |f(x)|\le x^2 x,"The problem goes as follows: Let $f(x)$ be a function such that $|{f(x)}|\le x^2$ $\forall x \in [-1,1/7]$. The first part of the problem is prove that $\lim_{x\rightarrow0} \frac{f(x)}{x}=0$, which a simple $\delta-\epsilon$ argument (set $\delta=\epsilon$) is enough to do so. However, I cannot do the second part of the problem: Let $g(x)=\sin(f(x^2+x)-2x)$ and find $g'(0)$. My attempt: It seems I should use the definition of the derivative of $g(x)$, hence I obtain $$\lim_{x\rightarrow0}\frac{\sin(f(x^2+x)-2x)-\sin(f(0))}{x}$$ but I don't know how to proceed from here.","The problem goes as follows: Let $f(x)$ be a function such that $|{f(x)}|\le x^2$ $\forall x \in [-1,1/7]$. The first part of the problem is prove that $\lim_{x\rightarrow0} \frac{f(x)}{x}=0$, which a simple $\delta-\epsilon$ argument (set $\delta=\epsilon$) is enough to do so. However, I cannot do the second part of the problem: Let $g(x)=\sin(f(x^2+x)-2x)$ and find $g'(0)$. My attempt: It seems I should use the definition of the derivative of $g(x)$, hence I obtain $$\lim_{x\rightarrow0}\frac{\sin(f(x^2+x)-2x)-\sin(f(0))}{x}$$ but I don't know how to proceed from here.",,"['calculus', 'limits', 'derivatives']"
92,Are there standard parameters for the Weierstrass nowhere differentiable function?,Are there standard parameters for the Weierstrass nowhere differentiable function?,,"On Wikipedia , the Weierstass non-differentiable function is defined as: $$f(x)=\sum^{\infty}_{n=0}a^n\cos(b^n\pi x)$$ where $0<a<1$, $0<b$, and $ab>1+\frac 32 \pi$ Since it seems like, for any $a$ and $b$ that satisfy the above conditions, the resulting function will be continuous everywhere and differentiable nowhere, would it be more accurate to refer to the Weierstrass ""function"" as a family of functions? Are there a standard $a$ and $b$ that are used in discussing this function?","On Wikipedia , the Weierstass non-differentiable function is defined as: $$f(x)=\sum^{\infty}_{n=0}a^n\cos(b^n\pi x)$$ where $0<a<1$, $0<b$, and $ab>1+\frac 32 \pi$ Since it seems like, for any $a$ and $b$ that satisfy the above conditions, the resulting function will be continuous everywhere and differentiable nowhere, would it be more accurate to refer to the Weierstrass ""function"" as a family of functions? Are there a standard $a$ and $b$ that are used in discussing this function?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'continuity', 'examples-counterexamples']"
93,Uniform convergence result in proof of second-derivative formula,Uniform convergence result in proof of second-derivative formula,,"This is a fairly basic analysis question. Consider a continuous function $f: \mathbb{R} \to \mathbb{R}$ which is twice differentiable at a point $x$. If necessary, also assume that $f \in C^2(\mathbb{R})$. Chasing through the definitions, we have $$f''(x) := \lim_{h\to 0} \frac{f'(x+h) - f'(x)}{h} = \lim_{h\to 0}\left(\lim_{k\to 0} \frac{f(x+h+k) - 2f(x+k) + f(x)}{hk}\right).$$ We have two limit processes - one with $h$ and one with $k$, but perhaps we can sort of ""set $h=k$"" and combine these limit processes, as in: $$f''(x) \overset{?}= \lim_{h\to 0}\frac{f(x+2h) - 2f(x+h) +f(x)}{h^2}.$$ I am trying to convert this into a Baby Rudin-type problem as follows: Define (continuous) functions $F_n(k)$ for $k\in[0,a]$ by $$F_n(k) = \begin{cases} \frac{f(x+\frac1n+k) - 2f(x+k) + f(x)}{k/n}&k \in(0,a]\\ \lim_{k\to 0}\frac{f(x+\frac1n+k) - 2f(x+k) + f(x)}{k/n}&k=0\end{cases}.$$ Showing that the $F_n$ converge uniformly to $F_{\infty} := \lim_{n\to \infty}F_n$ for some $a >0$ seems to suffice for our problem. (How to prove this is my question.) With uniform convergence, then by Baby Rudin Theorem 7.11, $F_{\infty}$ is continuous at $0$ so that we may find a $\delta$ such that if $k < \delta$, then $|F_{\infty}(0) - F_{\infty}(k)| < \varepsilon/2$, and also there is an $M$ such that for $n\geq M$ we have $|F_n(k) - F_{\infty}(k)| < \varepsilon/2$ for all $k$. So we have $|F_n(k) - F_{\infty}(0)| < \varepsilon$ when both $k < \delta$ and $n \geq M$. We can find $N$ (such as $\max(M,\lceil\delta^{-1}\rceil)$ I think) for which we can set $k = \frac1n$ if $n \geq N$, which is essentially what we wanted. So my questions is: How can we show that the $F_n$ converge uniformly to $F_{\infty}$, or if we need some extra conditions, then what are they? I would also be interested in easier (still rigorous) ways of concluding that we may ""set $h=k$"", or obviously any mistakes in my approach. Finally, I should mention that I became interested while on the page for the somewhat related question Second derivative ""formula derivation"" in which the current issue was brought up in a comment by Peter S. to an answer provided by Mhenni Benghorbal.","This is a fairly basic analysis question. Consider a continuous function $f: \mathbb{R} \to \mathbb{R}$ which is twice differentiable at a point $x$. If necessary, also assume that $f \in C^2(\mathbb{R})$. Chasing through the definitions, we have $$f''(x) := \lim_{h\to 0} \frac{f'(x+h) - f'(x)}{h} = \lim_{h\to 0}\left(\lim_{k\to 0} \frac{f(x+h+k) - 2f(x+k) + f(x)}{hk}\right).$$ We have two limit processes - one with $h$ and one with $k$, but perhaps we can sort of ""set $h=k$"" and combine these limit processes, as in: $$f''(x) \overset{?}= \lim_{h\to 0}\frac{f(x+2h) - 2f(x+h) +f(x)}{h^2}.$$ I am trying to convert this into a Baby Rudin-type problem as follows: Define (continuous) functions $F_n(k)$ for $k\in[0,a]$ by $$F_n(k) = \begin{cases} \frac{f(x+\frac1n+k) - 2f(x+k) + f(x)}{k/n}&k \in(0,a]\\ \lim_{k\to 0}\frac{f(x+\frac1n+k) - 2f(x+k) + f(x)}{k/n}&k=0\end{cases}.$$ Showing that the $F_n$ converge uniformly to $F_{\infty} := \lim_{n\to \infty}F_n$ for some $a >0$ seems to suffice for our problem. (How to prove this is my question.) With uniform convergence, then by Baby Rudin Theorem 7.11, $F_{\infty}$ is continuous at $0$ so that we may find a $\delta$ such that if $k < \delta$, then $|F_{\infty}(0) - F_{\infty}(k)| < \varepsilon/2$, and also there is an $M$ such that for $n\geq M$ we have $|F_n(k) - F_{\infty}(k)| < \varepsilon/2$ for all $k$. So we have $|F_n(k) - F_{\infty}(0)| < \varepsilon$ when both $k < \delta$ and $n \geq M$. We can find $N$ (such as $\max(M,\lceil\delta^{-1}\rceil)$ I think) for which we can set $k = \frac1n$ if $n \geq N$, which is essentially what we wanted. So my questions is: How can we show that the $F_n$ converge uniformly to $F_{\infty}$, or if we need some extra conditions, then what are they? I would also be interested in easier (still rigorous) ways of concluding that we may ""set $h=k$"", or obviously any mistakes in my approach. Finally, I should mention that I became interested while on the page for the somewhat related question Second derivative ""formula derivation"" in which the current issue was brought up in a comment by Peter S. to an answer provided by Mhenni Benghorbal.",,"['real-analysis', 'derivatives', 'uniform-convergence']"
94,"if $f([a,b])=[c,d]$ and $[c,d] \subset [a,b]$, is there $x \in [c,d]$ such that $f(x)=x$?","if  and , is there  such that ?","f([a,b])=[c,d] [c,d] \subset [a,b] x \in [c,d] f(x)=x","I'm trying to prove something that I'm not sure is correct. Let $f$ be a continuous, differentiable and monotonic function $f:[a,b] \to [c,d]$, where $[c,d] \subset [a,b]$. Is there an $x \in [c,d]$ such that $f(x)=x$?","I'm trying to prove something that I'm not sure is correct. Let $f$ be a continuous, differentiable and monotonic function $f:[a,b] \to [c,d]$, where $[c,d] \subset [a,b]$. Is there an $x \in [c,d]$ such that $f(x)=x$?",,"['calculus', 'derivatives', 'continuity']"
95,A calculus problem,A calculus problem,,"Question: Suppose that $u(x,t)$ is continuous, together with its first and second partial derivatives; suppose that $u$ and its first partial derivatives are periodic in $x$ of period $1,$ and suppose that $u_{tt}=u_{xx}.$ Prove that $$E(t)=\frac{1}{2}\int_0^1\left(u_t^2(x,t)+u_x^2(x,t)\right)\text{d}x$$ is a constant, independent of $t.$ Here is my solution: Since $u(x,t)$ and its $t-$partial derivative is continuous, we can differentiate as follows. \begin{equation} \begin{aligned} E'(t)&=\frac{1}{2}\int_0^12u_tu_{tt}+2u_xu_{xt}\text{d}x\\ &=\int_0^1u_tu_{xx}+u_xu_{xt}\text{d}x\\ &=u_xu_t\bigg|^1_0-\int_0^1u_xu_{tx}\text{d}x+u_tu_x\bigg|_0^1-\int_0^1u_tu_{xx}\text{d}x\\ &=-\int_0^1u_xu_{tx}\text{d}x-\int_0^1u_tu_{xx}\text{d}x \end{aligned} \end{equation} Combining the first and third line from the bottom to see that $$E'(t)=0\quad \forall t.$$ And thus $E(t)$ is a constant. Then I just realised that I haven't used the periodic condition. Was I wrong somewhere?","Question: Suppose that $u(x,t)$ is continuous, together with its first and second partial derivatives; suppose that $u$ and its first partial derivatives are periodic in $x$ of period $1,$ and suppose that $u_{tt}=u_{xx}.$ Prove that $$E(t)=\frac{1}{2}\int_0^1\left(u_t^2(x,t)+u_x^2(x,t)\right)\text{d}x$$ is a constant, independent of $t.$ Here is my solution: Since $u(x,t)$ and its $t-$partial derivative is continuous, we can differentiate as follows. \begin{equation} \begin{aligned} E'(t)&=\frac{1}{2}\int_0^12u_tu_{tt}+2u_xu_{xt}\text{d}x\\ &=\int_0^1u_tu_{xx}+u_xu_{xt}\text{d}x\\ &=u_xu_t\bigg|^1_0-\int_0^1u_xu_{tx}\text{d}x+u_tu_x\bigg|_0^1-\int_0^1u_tu_{xx}\text{d}x\\ &=-\int_0^1u_xu_{tx}\text{d}x-\int_0^1u_tu_{xx}\text{d}x \end{aligned} \end{equation} Combining the first and third line from the bottom to see that $$E'(t)=0\quad \forall t.$$ And thus $E(t)$ is a constant. Then I just realised that I haven't used the periodic condition. Was I wrong somewhere?",,"['calculus', 'analysis', 'derivatives', 'periodic-functions']"
96,Differentiate the function into the simplest form,Differentiate the function into the simplest form,,"My question: $y=\sin^{2}(x)$ My attempt: Is $\sin^{2}x$ the same as $(\sin(x))^2$? By rearranging the function I came up with the following. $$ \begin{align} u = \sin(x), \ & y=u^2 \\ \dfrac{du}{dx}= \cos(x), \ &\dfrac{dy}{dx}=2u \\ \end{align} $$ $$\dfrac{dy}{dx} = 2 \cos(x)$$ Is this correct? I am not sure if I'm going the correct way with the form.","My question: $y=\sin^{2}(x)$ My attempt: Is $\sin^{2}x$ the same as $(\sin(x))^2$? By rearranging the function I came up with the following. $$ \begin{align} u = \sin(x), \ & y=u^2 \\ \dfrac{du}{dx}= \cos(x), \ &\dfrac{dy}{dx}=2u \\ \end{align} $$ $$\dfrac{dy}{dx} = 2 \cos(x)$$ Is this correct? I am not sure if I'm going the correct way with the form.",,"['calculus', 'derivatives']"
97,The Fundamental Theorem of Calculus and Derivatives,The Fundamental Theorem of Calculus and Derivatives,,"How do I show this in a convincing manner? I know I need to use the Fundamental Theorem of Calculus, but I find it difficult to show any steps in between, as it appears obvious?","How do I show this in a convincing manner? I know I need to use the Fundamental Theorem of Calculus, but I find it difficult to show any steps in between, as it appears obvious?",,"['calculus', 'integration', 'derivatives', 'proof-writing']"
98,Second derivative of $\frac{\ln t}{\sqrt t}$ and derivative of $\arccos(1-2x^2)$,Second derivative of  and derivative of,\frac{\ln t}{\sqrt t} \arccos(1-2x^2),"$f(t)=\dfrac{\ln t}{\sqrt t}$ I'm stuck on the algebra of finding the second derivative. For the first derivative, I got: $f'(t)=\dfrac{t^{\frac{-1}{2}}(1-\frac{1}{2}\ln t)}{t^2}$ For the second derivative, I'm stuck on the algebra... If someone could differentiate this and show me the steps, I'd really appreciate it. Also: Differentiate $y=\arccos(1-2x^2)$ with respect to x, and simplify your answer. So far I have: $\dfrac{-4x}{\sqrt{4x^2-4x^4}}$ Am I on the right lines?","$f(t)=\dfrac{\ln t}{\sqrt t}$ I'm stuck on the algebra of finding the second derivative. For the first derivative, I got: $f'(t)=\dfrac{t^{\frac{-1}{2}}(1-\frac{1}{2}\ln t)}{t^2}$ For the second derivative, I'm stuck on the algebra... If someone could differentiate this and show me the steps, I'd really appreciate it. Also: Differentiate $y=\arccos(1-2x^2)$ with respect to x, and simplify your answer. So far I have: $\dfrac{-4x}{\sqrt{4x^2-4x^4}}$ Am I on the right lines?",,['derivatives']
99,How to derive $\frac{d}{dx}\left(x+1\right)^{\sin\left(x\right)}$,How to derive,\frac{d}{dx}\left(x+1\right)^{\sin\left(x\right)},"I need help to find derivative of: $\frac{d}{dx}(x+1)^{\sin x}$ i tried to do something like this.. $$(x+1)^{\sin x}\cdot \ln\left(x+1\right)=\sin x(x+1)^{\sin\left(x\right)-1}\cdot \ln(x+1)-(x+1)^{\sin(x)}\cdot \frac{1}{x+1}\:=\sin x(x+1)^{\sin\left(x\right)-1} \ln(x+1)-\frac{(x+1)^{\sin x}}{x+1}$$ I tried another way: $\left(x+1\right)^{sinx}\:=\:ln\left(x+1\right)^{sinx}\:=\:sin\left(x\right)ln\left(x+1\right)\:=\:cos\:\cdot \:ln\left(x+1\right)+sin\left(x\right)\cdot \frac{1}{x+1}\:=\:cos\left(x\right)ln\left(x+1\right)\:+\:\frac{sin\left(x\right)}{x+1}$ ok i got the solution! I put it there, maybe this will help someone! i used this rule : $e^a=e^{a\cdot ln\cdot e}$ $\left(x+1\right)^{sinx}=\:e^{sinx\cdot ln\left(x+1\right)}=\:e^{sinx\cdot ln\left(x+1\right)}\cdot cosx\:\cdot \:ln\left(x+1\right)\:+\:sinx\cdot \frac{1}{x+1}\:=\:\left(x+1\right)\left(cosxln\left(x+1\right)\:+\:\frac{sinx}{x+1}\right)$","I need help to find derivative of: $\frac{d}{dx}(x+1)^{\sin x}$ i tried to do something like this.. $$(x+1)^{\sin x}\cdot \ln\left(x+1\right)=\sin x(x+1)^{\sin\left(x\right)-1}\cdot \ln(x+1)-(x+1)^{\sin(x)}\cdot \frac{1}{x+1}\:=\sin x(x+1)^{\sin\left(x\right)-1} \ln(x+1)-\frac{(x+1)^{\sin x}}{x+1}$$ I tried another way: $\left(x+1\right)^{sinx}\:=\:ln\left(x+1\right)^{sinx}\:=\:sin\left(x\right)ln\left(x+1\right)\:=\:cos\:\cdot \:ln\left(x+1\right)+sin\left(x\right)\cdot \frac{1}{x+1}\:=\:cos\left(x\right)ln\left(x+1\right)\:+\:\frac{sin\left(x\right)}{x+1}$ ok i got the solution! I put it there, maybe this will help someone! i used this rule : $e^a=e^{a\cdot ln\cdot e}$ $\left(x+1\right)^{sinx}=\:e^{sinx\cdot ln\left(x+1\right)}=\:e^{sinx\cdot ln\left(x+1\right)}\cdot cosx\:\cdot \:ln\left(x+1\right)\:+\:sinx\cdot \frac{1}{x+1}\:=\:\left(x+1\right)\left(cosxln\left(x+1\right)\:+\:\frac{sinx}{x+1}\right)$",,"['calculus', 'trigonometry', 'derivatives']"
