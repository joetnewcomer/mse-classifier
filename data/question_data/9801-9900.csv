,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Show:if $u=u_t=0$, then $u\equiv0$ on $\Omega$ for damped wave equation","Show:if , then  on  for damped wave equation",u=u_t=0 u\equiv0 \Omega,"Let $\phi(x)$ be a function in $C_{0}^{\infty}(\mathbb R^3)$ and consider a damped wave equation: $$u_{tt} -\Delta u +\phi u=0$$ $u(x,0)=f$ and $u_t(x,0)=g$. Now for fixed $(x_0, t_0) \in \mathbb R^3 \times(0, \infty)$, let $\Omega = \{(x,t): t \in [0,t_0] ~\text{and}~ |x-x_0|\leq|t-t_0| \}$ be the cone of dependence  and define $U_{\tau}=\Omega \cap \{t=\tau\}$. Show:if $u=u_t=0$, then $u\equiv0$ on $\Omega$. I'm not sure how to use cone of dependence here for damped wave equation.","Let $\phi(x)$ be a function in $C_{0}^{\infty}(\mathbb R^3)$ and consider a damped wave equation: $$u_{tt} -\Delta u +\phi u=0$$ $u(x,0)=f$ and $u_t(x,0)=g$. Now for fixed $(x_0, t_0) \in \mathbb R^3 \times(0, \infty)$, let $\Omega = \{(x,t): t \in [0,t_0] ~\text{and}~ |x-x_0|\leq|t-t_0| \}$ be the cone of dependence  and define $U_{\tau}=\Omega \cap \{t=\tau\}$. Show:if $u=u_t=0$, then $u\equiv0$ on $\Omega$. I'm not sure how to use cone of dependence here for damped wave equation.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
1,$f(x)>0\implies\int_a^bf(x)\mathop{dx}>0$,,f(x)>0\implies\int_a^bf(x)\mathop{dx}>0,"Suppose $f$ is Riemann-integrable on $[a,b]$ such that $f(x)>0, \forall x \in [a,b].$ Prove: $$\int_a^bf(x)\mathop{dx}>0$$ Provided solution: Suppose by contradiction that $I=\int_a^bf(x)\mathop{dx}=0.$ Let us take a sequence of normal partitions $T_n$ of $[a,b],$ that is partitons of $n$ intervals of length $\frac{b-a}{n}.$ Then for $n$ large enough, upper Darboux sum of $f$ is small as we wish. So there exists $n_0$ such that for all $n>n_0:$ $$\tag{1}\sum_{i=0}^{n-1}\sup_{[x_i,x_{i+1}]}f\cdot\Delta x_i=\frac{b-a}{n}\sum_{i=0}^{n-1}\sup_{[x_i,x_{i+1}]}f<\frac{b-a}{2}$$ Therefore, there exists an interval $I_1:=[x_i,x_{i+1}],$ such that: $$\tag{2}\sup_{[x_i,x_{i+1}]}f<\frac{1}{2}$$ By the integral monotonicity and positiveness of $f:$ $$\tag{3}0=\int_a^bf(x)\mathop{dx}\geq \int_{I_1}f(x)\mathop{dx} \geq 0$$ Hence, $\int_{I_1}f(x)\mathop{dx}=0.$ Repeating the process on $I_1,$ let us take a sequence of normal partitions of $[x_i,x_{i+1},$ that is intervals $[y_i,y_{i+1}]$ of length $\frac{x_{i+1}-x_i}{n}.$ Therefore, there exists $n_1$ such that for all $n>n_1:$ $$\tag{4}\sum_{i=0}^{n-1}\sup_{[y_i,y_{i+1}]}f\cdot\Delta x_i=\frac{x_{i+1}-x_i}{n}\sum_{i=0}^{n-1}\sup_{[y_i,y_{i+1}]}f<\frac{x_{i+1}-x_i}{4}$$ So there exists an interval $I_2:=[y_i,y_{i+1}] \subset I_1,$ such that: $$\tag{5} \sup_{I_2}f<\frac{1}{4}$$ Continuing like that, we get a sequence of intervals: $\ \dots \subseteq I_2 \subseteq I_1,$ such that: $$\tag{6} \sup_{I_n}f \leq \frac{1}{2^n}$$ By Cantor's intersection theorem: $$\tag{7} \bigcap_{n=0}^\infty I_n \neq \emptyset  $$ But, if $x \in \bigcap_{n=0}^\infty I_n,$ then $f(x) \leq \frac{1}{2^n},$ for all $n$ , hence $f(x)=0,$ contradicting $f(x)>0.$ My questions: $(a)$ At $(1),$ I know $\sum_{i=0}^{n-1} \Delta x_i = b-a,$ but why does it equal $\frac{b-a}{n}?$ And why is the inequality true? $(a)$ At $(3),$ how is the monotonicity is used? Any help is appreciated.","Suppose is Riemann-integrable on such that Prove: Provided solution: Suppose by contradiction that Let us take a sequence of normal partitions of that is partitons of intervals of length Then for large enough, upper Darboux sum of is small as we wish. So there exists such that for all Therefore, there exists an interval such that: By the integral monotonicity and positiveness of Hence, Repeating the process on let us take a sequence of normal partitions of that is intervals of length Therefore, there exists such that for all So there exists an interval such that: Continuing like that, we get a sequence of intervals: such that: By Cantor's intersection theorem: But, if then for all , hence contradicting My questions: At I know but why does it equal And why is the inequality true? At how is the monotonicity is used? Any help is appreciated.","f [a,b] f(x)>0, \forall x \in [a,b]. \int_a^bf(x)\mathop{dx}>0 I=\int_a^bf(x)\mathop{dx}=0. T_n [a,b], n \frac{b-a}{n}. n f n_0 n>n_0: \tag{1}\sum_{i=0}^{n-1}\sup_{[x_i,x_{i+1}]}f\cdot\Delta x_i=\frac{b-a}{n}\sum_{i=0}^{n-1}\sup_{[x_i,x_{i+1}]}f<\frac{b-a}{2} I_1:=[x_i,x_{i+1}], \tag{2}\sup_{[x_i,x_{i+1}]}f<\frac{1}{2} f: \tag{3}0=\int_a^bf(x)\mathop{dx}\geq \int_{I_1}f(x)\mathop{dx} \geq 0 \int_{I_1}f(x)\mathop{dx}=0. I_1, [x_i,x_{i+1}, [y_i,y_{i+1}] \frac{x_{i+1}-x_i}{n}. n_1 n>n_1: \tag{4}\sum_{i=0}^{n-1}\sup_{[y_i,y_{i+1}]}f\cdot\Delta x_i=\frac{x_{i+1}-x_i}{n}\sum_{i=0}^{n-1}\sup_{[y_i,y_{i+1}]}f<\frac{x_{i+1}-x_i}{4} I_2:=[y_i,y_{i+1}] \subset I_1, \tag{5} \sup_{I_2}f<\frac{1}{4} \ \dots \subseteq I_2 \subseteq I_1, \tag{6} \sup_{I_n}f \leq \frac{1}{2^n} \tag{7} \bigcap_{n=0}^\infty I_n \neq \emptyset   x \in \bigcap_{n=0}^\infty I_n, f(x) \leq \frac{1}{2^n}, n f(x)=0, f(x)>0. (a) (1), \sum_{i=0}^{n-1} \Delta x_i = b-a, \frac{b-a}{n}? (a) (3),","['real-analysis', 'riemann-integration', 'riemann-sum']"
2,Proving that a continuous $\frac{1}{\sqrt{n}}-$periodic function is constant,Proving that a continuous periodic function is constant,\frac{1}{\sqrt{n}}-,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ a continuous function with the property $f(x)=f(x+\frac{1}{\sqrt{n}}),\forall x \in \mathbb{R},\forall n \in \mathbb{N}$.Prove that    $f$ is constant. Here is my attempt: Let $x \in \mathbb{R}$. Firstly we notice that $\forall n \in \mathbb{N}$ we have   $f(-\frac{1}{\sqrt{n}})=f(0)=f(\frac{1}{\sqrt{n}})$ From this we can deduce that $$f(\frac{m}{\sqrt{n}})=f(0),\forall n \in \mathbb{N},\forall m \in \mathbb{Z}$$ Now $x=\frac{x \sqrt{n}}{\sqrt{n}}$ and $x-\frac{1}{\sqrt{n}}=\frac{x \sqrt{n}-1}{\sqrt{n}} \leqslant \frac{[x \sqrt{n}]}{\sqrt{n}} \leqslant x$ The sequence $x_n=\frac{m_n}{\sqrt{n}} \rightarrow x$ where $m_n=[x\sqrt{n}]$ and $f(x_n)=f(0), \forall n \in \mathbb{N}$ From continuity by taking limits we have that $f(x)=f(0)$ Thus $f(x)=f(0),\forall x \in \mathbb{R}$ proving that $f$ is constant. Is my argument correct? If not can someone provide me a hint? Thank you in advance!","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ a continuous function with the property $f(x)=f(x+\frac{1}{\sqrt{n}}),\forall x \in \mathbb{R},\forall n \in \mathbb{N}$.Prove that    $f$ is constant. Here is my attempt: Let $x \in \mathbb{R}$. Firstly we notice that $\forall n \in \mathbb{N}$ we have   $f(-\frac{1}{\sqrt{n}})=f(0)=f(\frac{1}{\sqrt{n}})$ From this we can deduce that $$f(\frac{m}{\sqrt{n}})=f(0),\forall n \in \mathbb{N},\forall m \in \mathbb{Z}$$ Now $x=\frac{x \sqrt{n}}{\sqrt{n}}$ and $x-\frac{1}{\sqrt{n}}=\frac{x \sqrt{n}-1}{\sqrt{n}} \leqslant \frac{[x \sqrt{n}]}{\sqrt{n}} \leqslant x$ The sequence $x_n=\frac{m_n}{\sqrt{n}} \rightarrow x$ where $m_n=[x\sqrt{n}]$ and $f(x_n)=f(0), \forall n \in \mathbb{N}$ From continuity by taking limits we have that $f(x)=f(0)$ Thus $f(x)=f(0),\forall x \in \mathbb{R}$ proving that $f$ is constant. Is my argument correct? If not can someone provide me a hint? Thank you in advance!",,"['real-analysis', 'proof-verification', 'continuity']"
3,"Is the fourier orthonormal system special? If I chose some arbitary orthonormal system, could I hope for pointwise convergence for nice functions?","Is the fourier orthonormal system special? If I chose some arbitary orthonormal system, could I hope for pointwise convergence for nice functions?",,"As the title says, I would like to understand and get intution for fourier expanstion converging pointwise for nice functions. In particular, I might start by asking how often (of course this isn't well defined), when I pick an orthonormal system with respect to the usual integral inner product, does it give pointwise convergence for nice functions. Let's define nice functions as analytic (of course I'll be much happer if things hold for just diffrentiable functions for instance). I'm not sure how to define an arbtiary orthonormal system, so I leave this open to the answerer that is more knowledgeable than me. Thanks","As the title says, I would like to understand and get intution for fourier expanstion converging pointwise for nice functions. In particular, I might start by asking how often (of course this isn't well defined), when I pick an orthonormal system with respect to the usual integral inner product, does it give pointwise convergence for nice functions. Let's define nice functions as analytic (of course I'll be much happer if things hold for just diffrentiable functions for instance). I'm not sure how to define an arbtiary orthonormal system, so I leave this open to the answerer that is more knowledgeable than me. Thanks",,"['real-analysis', 'convergence-divergence', 'fourier-analysis', 'fourier-series']"
4,A power series involving binomial coefficients,A power series involving binomial coefficients,,"I've been playing around with infinite sums for quite a while now, but recently, I've come across the following question in an Under-Graduate Mathematics Book that is specifically targeted at problem solving. The problem is as follows, Evaluate the following: $$\sum_{r=2}^\infty \Biggl(\binom{2r}{r}{\biggl(\frac{x}{4}\biggr)^r}\Biggr)^2$$ where x is strictly less than unity. I've thoroughly checked that the sum is, in fact, convergent, however, I am completely stumped as to how I am to evaluate it. I am guessing that the final expression is one involving the variable 'x' since I do not see any way for it to be eliminated somehow. Any kind of hint/solution/explanation to the problem would be highly appreciated.","I've been playing around with infinite sums for quite a while now, but recently, I've come across the following question in an Under-Graduate Mathematics Book that is specifically targeted at problem solving. The problem is as follows, Evaluate the following: $$\sum_{r=2}^\infty \Biggl(\binom{2r}{r}{\biggl(\frac{x}{4}\biggr)^r}\Biggr)^2$$ where x is strictly less than unity. I've thoroughly checked that the sum is, in fact, convergent, however, I am completely stumped as to how I am to evaluate it. I am guessing that the final expression is one involving the variable 'x' since I do not see any way for it to be eliminated somehow. Any kind of hint/solution/explanation to the problem would be highly appreciated.",,"['real-analysis', 'sequences-and-series', 'summation', 'binomial-coefficients']"
5,Extension of Poincaré inequality: $\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)}$ when $u$ vanishes in $\Gamma\subseteq\partial\Omega$.,Extension of Poincaré inequality:  when  vanishes in .,\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)} u \Gamma\subseteq\partial\Omega,"Recall Poincaré inequality: Let $\Omega$ be a bounded open set in $\mathbb{R}^n$. Then there is a $C=C(\Omega,n)>0$ such that  $\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)}$ for all $u\in C_c^1(\Omega)$ (or $u\in H_0^1(\Omega)$). I want to prove the following version: Let $\Omega$ be a bounded connected open set in $\mathbb{R}^n$ with smooth boundary $\partial\Omega$. Let $\Gamma\subseteq\partial\Omega$ be a relatively open set. Then there is a $C=C(\Omega,n,\Gamma)>0$ such that $\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)}$ for all $u\in C^1(\bar{\Omega})$ with $u=0$ on $\Gamma$. This can be proved by contradiction and using Rellich compactness theorem in $H^1(\Omega)$. But I am more interested on a proof giving some idea of a possible $C$ (not necessarily the optimal one, just a $C$). Here is my attempt for a convex $\Omega$. I was not able to finish the proof. Maybe the whole idea is wrong, I do not know... Given $z\in\Gamma$ and $\sigma\in \mathbb{S}^{n-1}$, let $g(r)=u^2(z+r\sigma)$ defined on $A_{z,\sigma}=\{r>0:\,z+r\sigma\in\Omega\}$. For all $\bar{r}\in A_{z,\sigma}$ $$ u^2(z+\bar{r}\sigma)=g(\bar{r})=\underbrace{g(0)}_{=0}+\int_0^{\bar{r}}g'(r)\,dr=2\int_0^{\bar{r}}u(z+r\sigma)\,\nabla u(z+r\sigma)\cdot\sigma\,dr.$$ Then $$u^2(z+\bar{r}\sigma)\leq 2\int_0^{\bar{r}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr\leq 2\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr,$$ so  \begin{align*}\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr \leq {} & 2|A_{z,\sigma}|\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr\\ \leq {} &  2\,\text{diam}(\Omega)\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr. \end{align*} Now use the well-known inequality $2ab\leq a^2/\epsilon+b^2\epsilon$ with $a=|u(z+r\sigma)|$, $b=|\nabla u(z+r\sigma)|$ and $\epsilon=2\,\text{diam}(\Omega)$, so that $$\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr\leq 4\,\text{diam}(\Omega)^2\int_{A_{z,\sigma}}|\nabla u(z+r\sigma)|^2\,dr.$$ Integrating on $\mathbb{S}^{n-1}$ with respect to $d\sigma$ and using $r^{n-1}\,dr\,d\sigma=dy$, \begin{align*}\int_\Omega u^2(y)\,\frac{1}{|y-z|^{n-1}}\,dy= {} &\int_{\mathbb{S}^{n-1}}\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr\,d\sigma \\ \leq {} &  4\,\text{diam}(\Omega)^2\int_{\mathbb{S}^{n-1}}\int_{A_{z,\sigma}}|\nabla u(z+r\sigma)|^2\,dr\,d\sigma \\= {} & 4\,\text{diam}(\Omega)^2\int_\Omega |\nabla u(y)|^2\,\frac{1}{|y-z|^{n-1}}\,dy. \end{align*} Since $|y-z|\leq \text{diam}(\Omega)$, $$\int_\Omega u^2(y)\,dy\leq 4\,\text{diam}(\Omega)^{n+1}\int_\Omega |\nabla u(y)|^2\,\frac{1}{|y-z|^{n-1}}\,dy,$$ for every $z\in\Gamma$. My problem : I would like to get rid of $1/|y-z|^{n-1}$. I thought of integrating on $\Gamma$ with respect to $d\sigma$ at both sides, and to bound $$\int_\Gamma \frac{1}{|y-z|^{n-1}}\,d\sigma(z).$$ If this integral were a Lebesgue integral, since $n-1<n$ we would have the boundedness of the integral. But we are dealing with a surface integral. As $\Gamma$ is relatively open and $\partial\Omega$ is smooth, there is a relatively open subset $V\subseteq\Gamma$ such that $V$ is the graph of a function $\varphi:W\subseteq\mathbb{R}^{n-1}\rightarrow\mathbb{R}$, so (denote $y=(y',y_n)$): $$\int_V\frac{1}{|y-z|^{n-1}}\,d\sigma(z)=\int_W\frac{\sqrt{1+|\nabla \varphi(z')|^2}}{(|y'-z'|^2+|y_n-\varphi(z')|^2)^{\frac{n-1}{2}}}\,dz'\leq C\int_W \frac{1}{|y'-z'|^{n-1}}\,dz',$$ but this last integral may not be finite. EDIT : Another (unsuccessful) attempt of proof is the following, with no need of using radius $r$ or angles $\sigma$. It uses the idea of an answer below to prove that Poincaré inequality holds when the hypothesis is that the mean of the function over the domain is $0$. If $x\in\Omega$ and $z\in\Gamma$, then  $$ u(x)=u(x)-u(z)=\int_0^1 \nabla u((1-t)z+tx)\,dt\cdot (x-z),$$ so $$|u(x)|\leq\text{diam}(\Omega) \int_0^1|\nabla u((1-t)z+tx)|\,dt,$$ and by Jensen's inequality $$u(x)^2\leq\text{diam}(\Omega)^2 \int_0^1|\nabla u((1-t)z+tx)|^2\,dt.$$ Integration over $\Gamma$ and $\Omega$, \begin{align*} \|u\|_{L^2(\Omega)}\leq {} & \frac{\text{diam}(\Omega)^2}{|\Gamma|}\int_{\Omega}\int_\Gamma\int_0^1 |\nabla u((1-t)z+tx)|^2\,dt\,d\sigma(z)\,dx \\ = {} & \frac{\text{diam}(\Omega)^2}{|\Gamma|}\bigg[\underbrace{\int_\Gamma\int_{1/2}^1\int_\Omega |\nabla u((1-t)z+tx)|^2\,dx\,dt\,d\sigma(z)}_{(I)} + \\ + {} & \underbrace{\int_\Omega\int_{0}^{1/2}\int_\Gamma |\nabla u((1-t)z+tx)|^2\,d\sigma(z)\,dt\,dx}_{(II)}\bigg].\end{align*} For (I), make the change $(1-t)z+tx=y$, $dx=dy/t^n$, so that $$(I)=|\Gamma|\left(\int_{1/2}^1 \frac{1}{t^n}\,dy\right)\|\nabla u\|_{L^2(\Omega)}^2\leq |\Gamma|2^n \|\nabla u\|_{L^2(\Omega)}^2.$$ For (II) one could try to do a similar thing, but I do not know how to make the change of variable in $\Gamma$.","Recall Poincaré inequality: Let $\Omega$ be a bounded open set in $\mathbb{R}^n$. Then there is a $C=C(\Omega,n)>0$ such that  $\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)}$ for all $u\in C_c^1(\Omega)$ (or $u\in H_0^1(\Omega)$). I want to prove the following version: Let $\Omega$ be a bounded connected open set in $\mathbb{R}^n$ with smooth boundary $\partial\Omega$. Let $\Gamma\subseteq\partial\Omega$ be a relatively open set. Then there is a $C=C(\Omega,n,\Gamma)>0$ such that $\|u\|_{L^2(\Omega)}\leq C\|\nabla u\|_{L^2(\Omega)}$ for all $u\in C^1(\bar{\Omega})$ with $u=0$ on $\Gamma$. This can be proved by contradiction and using Rellich compactness theorem in $H^1(\Omega)$. But I am more interested on a proof giving some idea of a possible $C$ (not necessarily the optimal one, just a $C$). Here is my attempt for a convex $\Omega$. I was not able to finish the proof. Maybe the whole idea is wrong, I do not know... Given $z\in\Gamma$ and $\sigma\in \mathbb{S}^{n-1}$, let $g(r)=u^2(z+r\sigma)$ defined on $A_{z,\sigma}=\{r>0:\,z+r\sigma\in\Omega\}$. For all $\bar{r}\in A_{z,\sigma}$ $$ u^2(z+\bar{r}\sigma)=g(\bar{r})=\underbrace{g(0)}_{=0}+\int_0^{\bar{r}}g'(r)\,dr=2\int_0^{\bar{r}}u(z+r\sigma)\,\nabla u(z+r\sigma)\cdot\sigma\,dr.$$ Then $$u^2(z+\bar{r}\sigma)\leq 2\int_0^{\bar{r}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr\leq 2\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr,$$ so  \begin{align*}\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr \leq {} & 2|A_{z,\sigma}|\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr\\ \leq {} &  2\,\text{diam}(\Omega)\int_{A_{z,\sigma}}|u(z+r\sigma)|\,|\nabla u(z+r\sigma)|\,dr. \end{align*} Now use the well-known inequality $2ab\leq a^2/\epsilon+b^2\epsilon$ with $a=|u(z+r\sigma)|$, $b=|\nabla u(z+r\sigma)|$ and $\epsilon=2\,\text{diam}(\Omega)$, so that $$\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr\leq 4\,\text{diam}(\Omega)^2\int_{A_{z,\sigma}}|\nabla u(z+r\sigma)|^2\,dr.$$ Integrating on $\mathbb{S}^{n-1}$ with respect to $d\sigma$ and using $r^{n-1}\,dr\,d\sigma=dy$, \begin{align*}\int_\Omega u^2(y)\,\frac{1}{|y-z|^{n-1}}\,dy= {} &\int_{\mathbb{S}^{n-1}}\int_{A_{z,\sigma}}u^2(z+r\sigma)\,dr\,d\sigma \\ \leq {} &  4\,\text{diam}(\Omega)^2\int_{\mathbb{S}^{n-1}}\int_{A_{z,\sigma}}|\nabla u(z+r\sigma)|^2\,dr\,d\sigma \\= {} & 4\,\text{diam}(\Omega)^2\int_\Omega |\nabla u(y)|^2\,\frac{1}{|y-z|^{n-1}}\,dy. \end{align*} Since $|y-z|\leq \text{diam}(\Omega)$, $$\int_\Omega u^2(y)\,dy\leq 4\,\text{diam}(\Omega)^{n+1}\int_\Omega |\nabla u(y)|^2\,\frac{1}{|y-z|^{n-1}}\,dy,$$ for every $z\in\Gamma$. My problem : I would like to get rid of $1/|y-z|^{n-1}$. I thought of integrating on $\Gamma$ with respect to $d\sigma$ at both sides, and to bound $$\int_\Gamma \frac{1}{|y-z|^{n-1}}\,d\sigma(z).$$ If this integral were a Lebesgue integral, since $n-1<n$ we would have the boundedness of the integral. But we are dealing with a surface integral. As $\Gamma$ is relatively open and $\partial\Omega$ is smooth, there is a relatively open subset $V\subseteq\Gamma$ such that $V$ is the graph of a function $\varphi:W\subseteq\mathbb{R}^{n-1}\rightarrow\mathbb{R}$, so (denote $y=(y',y_n)$): $$\int_V\frac{1}{|y-z|^{n-1}}\,d\sigma(z)=\int_W\frac{\sqrt{1+|\nabla \varphi(z')|^2}}{(|y'-z'|^2+|y_n-\varphi(z')|^2)^{\frac{n-1}{2}}}\,dz'\leq C\int_W \frac{1}{|y'-z'|^{n-1}}\,dz',$$ but this last integral may not be finite. EDIT : Another (unsuccessful) attempt of proof is the following, with no need of using radius $r$ or angles $\sigma$. It uses the idea of an answer below to prove that Poincaré inequality holds when the hypothesis is that the mean of the function over the domain is $0$. If $x\in\Omega$ and $z\in\Gamma$, then  $$ u(x)=u(x)-u(z)=\int_0^1 \nabla u((1-t)z+tx)\,dt\cdot (x-z),$$ so $$|u(x)|\leq\text{diam}(\Omega) \int_0^1|\nabla u((1-t)z+tx)|\,dt,$$ and by Jensen's inequality $$u(x)^2\leq\text{diam}(\Omega)^2 \int_0^1|\nabla u((1-t)z+tx)|^2\,dt.$$ Integration over $\Gamma$ and $\Omega$, \begin{align*} \|u\|_{L^2(\Omega)}\leq {} & \frac{\text{diam}(\Omega)^2}{|\Gamma|}\int_{\Omega}\int_\Gamma\int_0^1 |\nabla u((1-t)z+tx)|^2\,dt\,d\sigma(z)\,dx \\ = {} & \frac{\text{diam}(\Omega)^2}{|\Gamma|}\bigg[\underbrace{\int_\Gamma\int_{1/2}^1\int_\Omega |\nabla u((1-t)z+tx)|^2\,dx\,dt\,d\sigma(z)}_{(I)} + \\ + {} & \underbrace{\int_\Omega\int_{0}^{1/2}\int_\Gamma |\nabla u((1-t)z+tx)|^2\,d\sigma(z)\,dt\,dx}_{(II)}\bigg].\end{align*} For (I), make the change $(1-t)z+tx=y$, $dx=dy/t^n$, so that $$(I)=|\Gamma|\left(\int_{1/2}^1 \frac{1}{t^n}\,dy\right)\|\nabla u\|_{L^2(\Omega)}^2\leq |\Gamma|2^n \|\nabla u\|_{L^2(\Omega)}^2.$$ For (II) one could try to do a similar thing, but I do not know how to make the change of variable in $\Gamma$.",,"['real-analysis', 'partial-differential-equations', 'vector-analysis', 'sobolev-spaces', 'surface-integrals']"
6,Fixed points of difference equations – stability/limits,Fixed points of difference equations – stability/limits,,"Suppose I have the difference equation $x_{n+1} = f(x_n)$. The point $x^{\ast}$ is called a fixed point of the equation if $x^{\ast}=f(x^{\ast})$. The fixed point is stable if $\,\left\lvert\, f'(x^{\ast})\right\rvert < 1$ and unstable if $\,\left\lvert\, f'(x^{\ast})\right\rvert > 1$. This is all from my differential equations notes. But could someone give a proof of these or explain why they are true? Thanks.","Suppose I have the difference equation $x_{n+1} = f(x_n)$. The point $x^{\ast}$ is called a fixed point of the equation if $x^{\ast}=f(x^{\ast})$. The fixed point is stable if $\,\left\lvert\, f'(x^{\ast})\right\rvert < 1$ and unstable if $\,\left\lvert\, f'(x^{\ast})\right\rvert > 1$. This is all from my differential equations notes. But could someone give a proof of these or explain why they are true? Thanks.",,"['real-analysis', 'ordinary-differential-equations', 'stability-in-odes']"
7,Integral inequality with two increasing functions,Integral inequality with two increasing functions,,"Let $f,g:[0,1]\rightarrow\mathbb{R}^+$ be increasing functions such that $f\leq g$. Is there a constant $c>0$ (independent of $f,g$) for which there exists some $r\geq 0$ (possibly dependent on $f,g$) such that $$\int_{x:f(x)\leq r\leq g(x)}g(x)dx+\int_{f(x)\geq r}f(x)dx\geq c\int_0^1g(x)dx ?$$ As an example, let $g(x)=x$ and $f(x)=x^2$. The integral $\int_0^1g(x)dx$ is $\frac{1}{2}$. On the left-hand side, for fixed $r$, the first integral is from $x=r$ to $x=\sqrt{r}$ and amounts to $\frac{1}{2}(r-r^2)$. The second integral is from $x=\sqrt{r}$ to $x=1$ and amounts to $\frac{1}{3}(1-r\sqrt{r})$. Suppose $g(x)=x^t$ and $f(x)=x^s$ for $s\geq t\geq 1$. Then the right-hand side is $\frac{1}{t+1}$. The left-hand side is $$\frac{r^{\frac{t+1}{s}}}{t+1}-\frac{r^{\frac{t+1}{t}}}{t+1}+\frac{1}{s+1}-\frac{r^{\frac{s+1}{s}}}{s+1}.$$","Let $f,g:[0,1]\rightarrow\mathbb{R}^+$ be increasing functions such that $f\leq g$. Is there a constant $c>0$ (independent of $f,g$) for which there exists some $r\geq 0$ (possibly dependent on $f,g$) such that $$\int_{x:f(x)\leq r\leq g(x)}g(x)dx+\int_{f(x)\geq r}f(x)dx\geq c\int_0^1g(x)dx ?$$ As an example, let $g(x)=x$ and $f(x)=x^2$. The integral $\int_0^1g(x)dx$ is $\frac{1}{2}$. On the left-hand side, for fixed $r$, the first integral is from $x=r$ to $x=\sqrt{r}$ and amounts to $\frac{1}{2}(r-r^2)$. The second integral is from $x=\sqrt{r}$ to $x=1$ and amounts to $\frac{1}{3}(1-r\sqrt{r})$. Suppose $g(x)=x^t$ and $f(x)=x^s$ for $s\geq t\geq 1$. Then the right-hand side is $\frac{1}{t+1}$. The left-hand side is $$\frac{r^{\frac{t+1}{s}}}{t+1}-\frac{r^{\frac{t+1}{t}}}{t+1}+\frac{1}{s+1}-\frac{r^{\frac{s+1}{s}}}{s+1}.$$",,"['real-analysis', 'integration']"
8,Understanding how a proof was developed,Understanding how a proof was developed,,"Basically, my question is about methods of approaching a given proof. Let me elaborate on that with an example. Proposition: Suppose $a$ and $b$ $\in R$ and $a < b$. There is always a rational number, say $x$, in the range $a < x < b$. Proof: Chose N $\in N$ sufficiently large that $\frac{1}{N} < (b-a)$. Chose the least integer $z$ such that $b$N $\leq z$. $z$ being the least such integer, $(z-1) < b$N. Thus $\frac{(z-1)}{N} < b$. From then on, proof applies some inequality manipulation to show that this number is also greater than $a$. At this step, we conclude by noting that $\frac{(z-1)}{N}$ is a rational number. Now I picked this example since it is simple but the following things I'll say plagues me in most analysis proofs, since I started studying it. I consider a proof understood if I can fully see how the mathematician might have come up with the proof. Here, for example, it took me a lot of time to make sense why we chose N in such a way. Verifying the deductive steps is easy most of the time but uncovering the intention behind each step takes significant time. Studying rigorous real analysis for the first time, I feel like my approach to understanding proofs makes me waste a lot of time. Should I skim faster and skip details at first reading, should I abandon my obsession with justifying each step? I expected studying rigorous mathematics to take more time then my past engineering math efforts but this feels painful at times and makes you feel like giving up. Is there a book that teaches the skills needed, if there is any such skills, to better uncover the intention behind proofs?","Basically, my question is about methods of approaching a given proof. Let me elaborate on that with an example. Proposition: Suppose $a$ and $b$ $\in R$ and $a < b$. There is always a rational number, say $x$, in the range $a < x < b$. Proof: Chose N $\in N$ sufficiently large that $\frac{1}{N} < (b-a)$. Chose the least integer $z$ such that $b$N $\leq z$. $z$ being the least such integer, $(z-1) < b$N. Thus $\frac{(z-1)}{N} < b$. From then on, proof applies some inequality manipulation to show that this number is also greater than $a$. At this step, we conclude by noting that $\frac{(z-1)}{N}$ is a rational number. Now I picked this example since it is simple but the following things I'll say plagues me in most analysis proofs, since I started studying it. I consider a proof understood if I can fully see how the mathematician might have come up with the proof. Here, for example, it took me a lot of time to make sense why we chose N in such a way. Verifying the deductive steps is easy most of the time but uncovering the intention behind each step takes significant time. Studying rigorous real analysis for the first time, I feel like my approach to understanding proofs makes me waste a lot of time. Should I skim faster and skip details at first reading, should I abandon my obsession with justifying each step? I expected studying rigorous mathematics to take more time then my past engineering math efforts but this feels painful at times and makes you feel like giving up. Is there a book that teaches the skills needed, if there is any such skills, to better uncover the intention behind proofs?",,"['real-analysis', 'analysis', 'proof-writing']"
9,Show that a functional has no minimum in a given set,Show that a functional has no minimum in a given set,,"This is a problem from my calculus of variations class: Let $X=\{v:[0,1]\rightarrow\mathbb{R}$ of class $C^1$ , $v(0)=1$ , $\ v(1)=0\}$ , let $F:X\rightarrow\mathbb{R}$ be the functional $$F(v):=\int_0^1 (e^{v'(x)}+v^2(x))dx.$$ Integrate the Euler-Lagrange equation with conditions $v(0)=1$ , $\ v'(0)=a$ , $a\in\mathbb{R}$ and show that $F$ has no minimum on $X$ . The professor also said that we could restrict ourselves to the case $a<0$ . Ok, I did my homework: first we define $X_a:=\{v:[0,1]\rightarrow\mathbb{R}$ of class $C^1$ , $v(0)=1$ , $\ v'(0)=a\}$ . The Euler-Lagrange equation for $F$ is $v''e^{v'}=2v$ . Since the functional $F$ doesn't depend explicitely on $x$ the quantity $v'e^{v'}-e^{v'}-v^2$ is some constant, say $k$ , thus imposing the boundary conditions at $x=0$ we get $e^{v'}(v'-1)-v^2=e^{a}(a-1)-1$ (which is the du Bois-Reymond equation and it is exactly what we would have got integrating $v'(v''e^{v'}-2v)=0$ ). Now, if a minumum exists in $X$ it must have bounded derivative in $x=0$ , so it must be in $X_a$ for some $a<0$ . Let's see if the boundary conditions of $X$ are compatible with the boundary conditions of $X_a$ . In $x=1$ we have $e^{v'(1)}(v'(1)-1)=e^{a}(a-1)-1$ . Since $v'(1)$ is something in $\mathbb{R}$ , we reduced the problem to the study of the zeros of the function $g(y):=e^{y}(y-1)-e^{a}(a-1)+1$ . It turns out that $g$ has no zeros for $a<0$ , so we are done. Let's get to the question: why can we restrict ourselves to $a<0$ ? For $a\geq0$ g has one zero, so there could be a  candidate minimum in $X\cap X_a$ . Graphically, I can believe that the function $v$ has to be decreasing to minimize the functional, but this is not a proof, nor a satisfactory justification. I thought that if we can bound from below the values that $F$ assumes on $X\cap X_a$ , $a\geq0$ with $F(w)$ for some $w$ not in $X\cap X_a$ , this will be enough to show that $F$ has no minimum in $X$ . Nevertheless this seems not so simple, since we don't have an explicit formula for the candidate minimum. $\boldsymbol{Edit}$ : A colleague of mine told me that for the case $a>0$ I have to look at the sign of the second derivative. From the Euler-Lagrange equation we get $sign(v''(x))=sign(v(x))$ $\forall x\in(0,1)$ , which implies that the first derivative is increasing in $(0,1)$ . By continuity and positivity at 0 of the first derivative we get that $v$ is increasing in $[0,1]$ , thus it cannot reach the point 0 at $x=1$ . My question is: does this argument cover also the case $a=0$ ?","This is a problem from my calculus of variations class: Let of class , , , let be the functional Integrate the Euler-Lagrange equation with conditions , , and show that has no minimum on . The professor also said that we could restrict ourselves to the case . Ok, I did my homework: first we define of class , , . The Euler-Lagrange equation for is . Since the functional doesn't depend explicitely on the quantity is some constant, say , thus imposing the boundary conditions at we get (which is the du Bois-Reymond equation and it is exactly what we would have got integrating ). Now, if a minumum exists in it must have bounded derivative in , so it must be in for some . Let's see if the boundary conditions of are compatible with the boundary conditions of . In we have . Since is something in , we reduced the problem to the study of the zeros of the function . It turns out that has no zeros for , so we are done. Let's get to the question: why can we restrict ourselves to ? For g has one zero, so there could be a  candidate minimum in . Graphically, I can believe that the function has to be decreasing to minimize the functional, but this is not a proof, nor a satisfactory justification. I thought that if we can bound from below the values that assumes on , with for some not in , this will be enough to show that has no minimum in . Nevertheless this seems not so simple, since we don't have an explicit formula for the candidate minimum. : A colleague of mine told me that for the case I have to look at the sign of the second derivative. From the Euler-Lagrange equation we get , which implies that the first derivative is increasing in . By continuity and positivity at 0 of the first derivative we get that is increasing in , thus it cannot reach the point 0 at . My question is: does this argument cover also the case ?","X=\{v:[0,1]\rightarrow\mathbb{R} C^1 v(0)=1 \ v(1)=0\} F:X\rightarrow\mathbb{R} F(v):=\int_0^1 (e^{v'(x)}+v^2(x))dx. v(0)=1 \ v'(0)=a a\in\mathbb{R} F X a<0 X_a:=\{v:[0,1]\rightarrow\mathbb{R} C^1 v(0)=1 \ v'(0)=a\} F v''e^{v'}=2v F x v'e^{v'}-e^{v'}-v^2 k x=0 e^{v'}(v'-1)-v^2=e^{a}(a-1)-1 v'(v''e^{v'}-2v)=0 X x=0 X_a a<0 X X_a x=1 e^{v'(1)}(v'(1)-1)=e^{a}(a-1)-1 v'(1) \mathbb{R} g(y):=e^{y}(y-1)-e^{a}(a-1)+1 g a<0 a<0 a\geq0 X\cap X_a v F X\cap X_a a\geq0 F(w) w X\cap X_a F X \boldsymbol{Edit} a>0 sign(v''(x))=sign(v(x)) \forall x\in(0,1) (0,1) v [0,1] x=1 a=0","['real-analysis', 'calculus-of-variations', 'euler-lagrange-equation', 'variational-analysis']"
10,Limit of a sequence satisfying $(2-a_n)a_{n+1} \rightarrow 1$ as $n\rightarrow\infty$,Limit of a sequence satisfying  as,(2-a_n)a_{n+1} \rightarrow 1 n\rightarrow\infty,"This was in one of the calculus textbook exercise problem. Now, I am pretty sure that there was a typo in the problem. Problem Let $a_n$ be a sequence of real numbers satisfying the following: $$ \lim_{n\rightarrow\infty} (2-a_n)a_{n+1} = 1. $$ Prove that $\lim\limits_{n\rightarrow\infty} a_n= 1$. My tries The conclusion is true under assumption of $0<a_n<2$ for all but finitely many $n$. The conclusion is false without any further assumption. So, the problem is incorrect as it is stated. Question Is the conclusion true under an additional assumption of boundedness of $a_n$? Remarks I was able to obtain a sequence $a_n$ that the conclusion is false. However, under the assumption of boundedness, I am wondering if we can prove the conclusion or give a counterexample. Proof of My try 1 Let $\overline{a} = \limsup a_n$ and $\underline{a} = \liminf a_n$. Let $\{n_k\}_{k=1}^{\infty}$ be a subsequence of natural numbers such that $a_{n_k}\rightarrow \overline{a}$. Then we have  $$ (2-a_{n_k}) a_{n_k +1} \rightarrow 1. $$ Taking a convergent subsequence of $\{a_{n_k +1}\}_{k=1}^{\infty}$ which converges to $b$, we have  $$(2-\overline{a}) b = 1.$$ This gives $b=1/(2-\overline{a})$. Since $\overline{a}$ is the limsup, we have $$ \frac1{2-\overline{a}}\leq \overline{a}. $$ By assumption $0<a_n<2$ for all but finitely many $n$, we have $2-\overline{a} >0$. Thus,  $$1\leq (2-\overline{a})\overline{a}.$$ Then it follows that $(\overline{a}-1)^2 \leq 0$, which yields $\overline{a}=1$. Similar argument for $\underline{a}$ gives $\underline{a}=1$. Therefore, $a_n\rightarrow 1$. Proof of My try 2 Since $a_n=0$ or $a_n=2$ do not happen infinitely often, we may assume that $a_n\neq 0$, $a_n\neq 2$ for all $n$. Consider $\{1/k \}_{k=1}^{\infty}$, and the sequence given by the recurrence: $$ a_{n+1} = \frac{1+\frac1 1}{2-a_n}. \ \ \textrm{(Round 1)} $$ Then there exists $n$ such that $a_n>2$. We will put the first time that it happens as $n=n_1$, and we will say that we exit the Round 1. We have $a_{n_1+1}<0$ by the recurrence. Once we exit the Round 1, we enter into the Round 2 which starts with $n=n_1+1$ and the recurrence:  $$ a_{n+1} = \frac{1+\frac12}{2-a_n}. \ \ \textrm{(Round 2)} $$ Then there exists $n$ such that $a_n>2$. We put the first time $n\geq n_1+1$ that it happens as $n=n_2$, and we exit the Round 2. Then $a_{n_2+1}<0$. Continue this process with using the recurrence  $$ a_{n+1} = \frac{1+\frac1k}{2-a_n}. \ \ \textrm{(Round $k$)} $$ Then we have the sequcne $a_n$ such that $a_n>2$ infinitely many often and $(2-a_n)a_{n+1}\rightarrow 1$.","This was in one of the calculus textbook exercise problem. Now, I am pretty sure that there was a typo in the problem. Problem Let $a_n$ be a sequence of real numbers satisfying the following: $$ \lim_{n\rightarrow\infty} (2-a_n)a_{n+1} = 1. $$ Prove that $\lim\limits_{n\rightarrow\infty} a_n= 1$. My tries The conclusion is true under assumption of $0<a_n<2$ for all but finitely many $n$. The conclusion is false without any further assumption. So, the problem is incorrect as it is stated. Question Is the conclusion true under an additional assumption of boundedness of $a_n$? Remarks I was able to obtain a sequence $a_n$ that the conclusion is false. However, under the assumption of boundedness, I am wondering if we can prove the conclusion or give a counterexample. Proof of My try 1 Let $\overline{a} = \limsup a_n$ and $\underline{a} = \liminf a_n$. Let $\{n_k\}_{k=1}^{\infty}$ be a subsequence of natural numbers such that $a_{n_k}\rightarrow \overline{a}$. Then we have  $$ (2-a_{n_k}) a_{n_k +1} \rightarrow 1. $$ Taking a convergent subsequence of $\{a_{n_k +1}\}_{k=1}^{\infty}$ which converges to $b$, we have  $$(2-\overline{a}) b = 1.$$ This gives $b=1/(2-\overline{a})$. Since $\overline{a}$ is the limsup, we have $$ \frac1{2-\overline{a}}\leq \overline{a}. $$ By assumption $0<a_n<2$ for all but finitely many $n$, we have $2-\overline{a} >0$. Thus,  $$1\leq (2-\overline{a})\overline{a}.$$ Then it follows that $(\overline{a}-1)^2 \leq 0$, which yields $\overline{a}=1$. Similar argument for $\underline{a}$ gives $\underline{a}=1$. Therefore, $a_n\rightarrow 1$. Proof of My try 2 Since $a_n=0$ or $a_n=2$ do not happen infinitely often, we may assume that $a_n\neq 0$, $a_n\neq 2$ for all $n$. Consider $\{1/k \}_{k=1}^{\infty}$, and the sequence given by the recurrence: $$ a_{n+1} = \frac{1+\frac1 1}{2-a_n}. \ \ \textrm{(Round 1)} $$ Then there exists $n$ such that $a_n>2$. We will put the first time that it happens as $n=n_1$, and we will say that we exit the Round 1. We have $a_{n_1+1}<0$ by the recurrence. Once we exit the Round 1, we enter into the Round 2 which starts with $n=n_1+1$ and the recurrence:  $$ a_{n+1} = \frac{1+\frac12}{2-a_n}. \ \ \textrm{(Round 2)} $$ Then there exists $n$ such that $a_n>2$. We put the first time $n\geq n_1+1$ that it happens as $n=n_2$, and we exit the Round 2. Then $a_{n_2+1}<0$. Continue this process with using the recurrence  $$ a_{n+1} = \frac{1+\frac1k}{2-a_n}. \ \ \textrm{(Round $k$)} $$ Then we have the sequcne $a_n$ such that $a_n>2$ infinitely many often and $(2-a_n)a_{n+1}\rightarrow 1$.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
11,Showing that the Lebesgue-Stieltjes measure is absolutely continuous with respect to Lebesgue measure (and one more thing).,Showing that the Lebesgue-Stieltjes measure is absolutely continuous with respect to Lebesgue measure (and one more thing).,,"I aim to show the following result: Let $f:\mathbb{R}\to\mathbb{R}$ be a nondecreasing, continuously differentiable function and let $\lambda_f$ be the corresponding Lebesgue-Stieltjes measure generated by $f$. Prove: (a) $\lambda_f <<\lambda$ (that is, $\lambda_f$ is absolutely continuous with respect to Lebesgue-measure $\lambda$); (b) $\frac{d\lambda_f}{d\lambda}=f'$ (that is, $f'$ is the Radon-Nikodym derivative of $\lambda_f$). -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- For item (a), I tried consider the case when all the things happen in a compact interval $[a,b]$ (the general case easily follow from this one). So $f$ is uniformly continuous in $[a,b]$. Let $E\subset[a,b]$ with $\lambda(E)=0$ and let $\epsilon>0$ arbitrary. We want to find a cover $\{(a_k, b_k]\}_{k=1}^\infty$ of $E$ such that $$\sum_{k=1}^\infty [f(b_k)-f(a_k)]<\epsilon.\tag{1}$$ If we find $\{(a_k, b_k]\}_{k=1}^\infty$ sufficiently fine so that $f(b_k)-f(a_k)<\epsilon/2^k$ for all $k\in \mathbb{N}$, then (1) follows. Here we have a problem: $\epsilon/2^k$ depends on $k$. Since $\lambda(E)=0$, there exists a cover $\{(a_k, b_k]\}_{k=1}^\infty$ of $E$ such that $$\sum_{k=1}^\infty(b_k-a_k) < \frac{\epsilon}{\star},$$ where $\star\in\mathbb{R}^+$ I can control. But the number $\star$ cannot depends on $k$. Now, what should I do? Right now I will try to use the Mean Value Theorem in each interval and see what happen. -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- For item (b) (considering that item (a) is valid), the things become complicated to write. I just solve a half of this item. My attempt was to show that $\lambda_f(E)=\int_Ef'd\lambda$ for all $E$ measurable, because from Radon-Nikodym theorem, the derivative is $\lambda$-a.e. unique. Since $f'\geq0$ ($f$ is nondecreasing), if we set $$\mathcal{A} = \left\{\sum_{k=1}^\infty[f(b_k)-f(a_k)]:E\subset\bigcup_{k=1}^\infty(a_k, b_k]\right\}=\left\{\sum_{k=1}^\infty f'(c_k)(b_k-a_k):E\subset\bigcup_{k=1}^\infty(a_k, b_k]\right\}$$ (we used the Mean Value Theorem) and $$\mathcal{B}=\left\{\int_E sd\lambda:0\leq s\leq f'~\text{is simple}\right\}=$$ $$=\sup\left\{\sum_{k=1}^n\alpha_k\lambda(A_k):E =\sum_{k=1}^n A_i~\text{and}~0\leq\alpha_k\leq f', \forall k\in A_k, k\in\{1,2,...,n\}\right\},$$ from the definition we get $$\lambda_f(E) = \inf\mathcal{A}~~~~~~~~\text{and}~~~~~~~~\int_E f' d\lambda=\sup\mathcal{B}.$$ We need to prove that $\inf\mathcal{A}=\sup\mathcal{B}$. Let $x\in \mathcal{A}$ and $y\in\mathcal{B}$ arbitraries. Then there exists a cover $\{(a_k, b_k]\}_{k=1}^\infty$ and a partition $\{A_r\}_{r=1}^n$ for $E$ and numbers $\alpha_1, ..., \alpha_n \geq 0$ with $\alpha_r\leq f'$ in $A_r$, $r=1, ..., n$, such that $$x = \sum_{k=1}^\infty f'(c_k)(b_k-a_k)~~~~~~~~\text{and}~~~~~~~~y=\sum_{r=1}^n\alpha_r\lambda(A_r).$$ We can assume that the family $\{(a_k, b_k]\}_{k=1}^\infty$ is disjoint (in fact, we can split it into more sets and split the expression of $x$ into more parcels). So each set $A_r$ in contained in some of the sets $\{(a_k, b_k]\}_{k=1}^\infty$. Call $I_r\subset\mathbb{N}$ the minimal set of indices such that $A_r\subset \sum_{k\in I_r}(a_k,b_k].$ Since $\{(a_k, b_k]\}_{k=1}^\infty$ is disjoint, $\{I_r\}_{r=1}^n$ is a partition of $\mathbb{N}$. Then $\alpha_r\leq f'$ in $A_r$ and $\lambda(A_r)\leq\sum_{k\in I_r}(b_k-a_k)$, what implies $\alpha_r\lambda(A_r)\leq\sum_{k\in I_r} f'(c_k)(b_k-a_k)$. From this we concludes $$y=\sum_{r=1}^n\alpha_r\lambda(A_r)\leq\sum_{r=1}^n\sum_{k\in I_r} f'(c_k)(b_k-a_k) = \sum_{k=1}^\infty f'(c_k)(b_k-a_k) = x.$$ Since $x, y$ are arbitrary, we have $\inf\mathcal{A}\geq\sup\mathcal{B}$. How can I proceed to show that $\inf\mathcal{A}\leq\sup\mathcal{B}$? Any hint will be really appreciated.","I aim to show the following result: Let $f:\mathbb{R}\to\mathbb{R}$ be a nondecreasing, continuously differentiable function and let $\lambda_f$ be the corresponding Lebesgue-Stieltjes measure generated by $f$. Prove: (a) $\lambda_f <<\lambda$ (that is, $\lambda_f$ is absolutely continuous with respect to Lebesgue-measure $\lambda$); (b) $\frac{d\lambda_f}{d\lambda}=f'$ (that is, $f'$ is the Radon-Nikodym derivative of $\lambda_f$). -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- For item (a), I tried consider the case when all the things happen in a compact interval $[a,b]$ (the general case easily follow from this one). So $f$ is uniformly continuous in $[a,b]$. Let $E\subset[a,b]$ with $\lambda(E)=0$ and let $\epsilon>0$ arbitrary. We want to find a cover $\{(a_k, b_k]\}_{k=1}^\infty$ of $E$ such that $$\sum_{k=1}^\infty [f(b_k)-f(a_k)]<\epsilon.\tag{1}$$ If we find $\{(a_k, b_k]\}_{k=1}^\infty$ sufficiently fine so that $f(b_k)-f(a_k)<\epsilon/2^k$ for all $k\in \mathbb{N}$, then (1) follows. Here we have a problem: $\epsilon/2^k$ depends on $k$. Since $\lambda(E)=0$, there exists a cover $\{(a_k, b_k]\}_{k=1}^\infty$ of $E$ such that $$\sum_{k=1}^\infty(b_k-a_k) < \frac{\epsilon}{\star},$$ where $\star\in\mathbb{R}^+$ I can control. But the number $\star$ cannot depends on $k$. Now, what should I do? Right now I will try to use the Mean Value Theorem in each interval and see what happen. -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#- For item (b) (considering that item (a) is valid), the things become complicated to write. I just solve a half of this item. My attempt was to show that $\lambda_f(E)=\int_Ef'd\lambda$ for all $E$ measurable, because from Radon-Nikodym theorem, the derivative is $\lambda$-a.e. unique. Since $f'\geq0$ ($f$ is nondecreasing), if we set $$\mathcal{A} = \left\{\sum_{k=1}^\infty[f(b_k)-f(a_k)]:E\subset\bigcup_{k=1}^\infty(a_k, b_k]\right\}=\left\{\sum_{k=1}^\infty f'(c_k)(b_k-a_k):E\subset\bigcup_{k=1}^\infty(a_k, b_k]\right\}$$ (we used the Mean Value Theorem) and $$\mathcal{B}=\left\{\int_E sd\lambda:0\leq s\leq f'~\text{is simple}\right\}=$$ $$=\sup\left\{\sum_{k=1}^n\alpha_k\lambda(A_k):E =\sum_{k=1}^n A_i~\text{and}~0\leq\alpha_k\leq f', \forall k\in A_k, k\in\{1,2,...,n\}\right\},$$ from the definition we get $$\lambda_f(E) = \inf\mathcal{A}~~~~~~~~\text{and}~~~~~~~~\int_E f' d\lambda=\sup\mathcal{B}.$$ We need to prove that $\inf\mathcal{A}=\sup\mathcal{B}$. Let $x\in \mathcal{A}$ and $y\in\mathcal{B}$ arbitraries. Then there exists a cover $\{(a_k, b_k]\}_{k=1}^\infty$ and a partition $\{A_r\}_{r=1}^n$ for $E$ and numbers $\alpha_1, ..., \alpha_n \geq 0$ with $\alpha_r\leq f'$ in $A_r$, $r=1, ..., n$, such that $$x = \sum_{k=1}^\infty f'(c_k)(b_k-a_k)~~~~~~~~\text{and}~~~~~~~~y=\sum_{r=1}^n\alpha_r\lambda(A_r).$$ We can assume that the family $\{(a_k, b_k]\}_{k=1}^\infty$ is disjoint (in fact, we can split it into more sets and split the expression of $x$ into more parcels). So each set $A_r$ in contained in some of the sets $\{(a_k, b_k]\}_{k=1}^\infty$. Call $I_r\subset\mathbb{N}$ the minimal set of indices such that $A_r\subset \sum_{k\in I_r}(a_k,b_k].$ Since $\{(a_k, b_k]\}_{k=1}^\infty$ is disjoint, $\{I_r\}_{r=1}^n$ is a partition of $\mathbb{N}$. Then $\alpha_r\leq f'$ in $A_r$ and $\lambda(A_r)\leq\sum_{k\in I_r}(b_k-a_k)$, what implies $\alpha_r\lambda(A_r)\leq\sum_{k\in I_r} f'(c_k)(b_k-a_k)$. From this we concludes $$y=\sum_{r=1}^n\alpha_r\lambda(A_r)\leq\sum_{r=1}^n\sum_{k\in I_r} f'(c_k)(b_k-a_k) = \sum_{k=1}^\infty f'(c_k)(b_k-a_k) = x.$$ Since $x, y$ are arbitrary, we have $\inf\mathcal{A}\geq\sup\mathcal{B}$. How can I proceed to show that $\inf\mathcal{A}\leq\sup\mathcal{B}$? Any hint will be really appreciated.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
12,"Prob. 11, Chap. 4 in Baby Rudin: uniformly continuous extension from a dense subset to the entire space","Prob. 11, Chap. 4 in Baby Rudin: uniformly continuous extension from a dense subset to the entire space",,"Here is Prob. 11, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a uniformly continuous mapping of a metric space $X$ into a    metric space $Y$ and prove that $\left\{ f\left( x_n \right) \right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence    $\left\{ x_n \right\}$ in $X$. Use this result to give an alternative proof of the theorem stated in Exercise 13. Now here is Prob. 13, Chap. 4 in Baby Rudin, 3rd edition: Let $E$ be a dense subset of a metric space $X$, and let $f$ be a unifromly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$. ... (Uniqueness follows from Exercise 4.) ... And, here is Prob. 4, Chap. 4: Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$, and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p) = f(p)$ for all $p \in E$, prove that $g(p) = f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.) My effort: Since $f$ is uniformly continuous on $X$, corresponding to every real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that $$d_Y\left(f(x), f(y)\right) < \varepsilon$$ for all points $x, y \in X$ which satisfy $$d_X(x,y)<\delta.$$ Now since $\left\{ x_n \right\}$ is a Cauchy sequence in $\left( X, d_X \right)$, therefore corresponding to the real number $\delta$ in the preceding paragraph we can find a natural number $N$ such that $$d_X\left(x_m, x_n \right)< \delta \ \mbox{ for any natural numbers } m \mbox{ and } n \mbox{ such that } m > N \mbox{ and } n > N.$$ So from the preceding two paragraphs we can conclude that $$d_Y\left( f\left(x_m\right), f\left(x_n\right) \right) < \varepsilon \ \mbox{ for any natural numbers } m \mbox{ and } n \mbox{ such that } m > N \mbox{ and } n > N,$$    from which it follows that $\left\{ f\left(x_n\right) \right\}$ is a Cauchy sequence in $\left( Y, d_Y \right)$. Now let $E$ be a dense set in $\left(X, d_X \right)$, $\left( Y, d_Y \right)$ be a complete metric space, and $f$ be a uniformly continuous mapping of $E$ into $Y$. We now show that there exists a unique uniformly continuous mapping $g$ of $X$ into $Y$ such that $g(p) = f(p)$ for all $p \in E$. Let $p \in X$. Since $E$ is dense in $X$, we can find a sequence $\left\{p_n\right\}$ in $E$ converging to the point $p$ in $\left( X, d_X \right)$. Now as the sequence $\left\{ p_n \right\}$ is a Cauchy sequence in $E$ and as $f$ is uniformly continuous, so the image sequence $\left\{ f\left(p_n \right)\right\}$ is a Cauchy sequence in the complete metric space $Y$, and so this sequence converges to some point $q$ in $Y$. We now show that this point $q$ is independent of the particular sequence $\left\{ p_n \right\}$ in $E$ which converges to $p$. For this, let $\left\{ p^\prime_n \right\}$ be another sequence in $E$ converging in $X$ to the point $p$. Then as before the image sequence $\left\{ f\left(p^\prime_n\right)\right\}$ converges in $Y$ to some point $q^\prime$. Now consider the sequence $\left\{ x_n \right\}$ in $E$ defined as follows: Let $$x_n = \begin{cases} p_{\frac{n}{2}} \ \mbox{ if $n$ is even}; \\ p^\prime_{\frac{n+1}{2}} \ \mbox{ if $n$ is odd}. \end{cases}$$ This sequence too converges to point $p$ in $X$ and is therefore Cauchy and so its image sequence $\left\{ f\left( x_n \right) \right\}$ is also a Cauchy sequence in $Y$, which is a complete metric space; so the sequence $\left\{ f\left( x_n \right) \right\}$ converges in $Y$ to some point $y$. Therefore every subsequence of $\left\{ f\left( x_n \right) \right\}$ also converges to $y$. But we note that,    $$p_n = x_{2n} \ \mbox{ and } p^\prime_n = x_{2n-1} \ \mbox{ for all } n \in \mathbb{N}.$$ So    $$q = \lim_{n \to \infty} f\left(p_n \right) = \lim_{n \to \infty} f\left( x_{2n}\right) = y,$$   where the limit is in $Y$, and similarly $$q^\prime = \lim_{n\to\infty} f\left(p_{2n-1}\right) = y.$$ Thus $q^\prime = q$. Now let's define the mapping $g$ of $X$ into $Y$ as follows: For any point $p \in X$, let  $$\tag{1} g(p) = \lim_{n \to \infty} f \left(p_n \right),$$ the limit being in the metric space $\left(Y, d_Y \right)$, where $\left\{ p_n \right\}$ is any sequence in $E$ such that $$\tag{2} \lim_{n \to \infty} p_n = p$$ in $\left( X, d_X \right)$. If $p \in E$, then we can take $p_n = p$ for all $n$ so that $$g(p) = \lim_{n \to \infty} f\left(p_n\right) = f(p).$$    Thus $g$ is an extension of $f$ from $E$ to $X$. Let $\varepsilon > 0$ be a given real number. Since $f$ is uniformly continuous on $E$, we can find a real number $\delta > 0$ such that $$d_Y\left( f(p), f(q) \right) < \frac{\varepsilon}{3}$$ for all points $p$ and $q$ in $E$ for which $$d_X\left(p, q\right) < \delta. $$ Now let $u$ and $v$ be any two points of $X$ which satisfy $$d_X\left(u, v\right) < \frac{\delta}{3}.$$    Then since $E$ is dense in $X$, we can find points $u^\prime$ and $v^\prime$ in $E$ such that    $$\tag{3} d_X \left( u, u^\prime \right) < \frac{\delta}{3} \mbox{ and } d_Y\left(g(u), f(u^\prime) \right) < \frac{\varepsilon}{3},$$   and also    $$\tag{4} d_X \left( v, v^\prime \right) < \frac{\delta}{3} \mbox{ and } d_Y\left( g(v), f(v^\prime) \right) < \frac{\varepsilon}{3}.$$   This is possible in view of (1) and (2) above. Then we note that    $$d_X \left( u^\prime, v^\prime \right) \leq d_X\left(u^\prime, u \right) + d_X\left(u, v\right) + d_X\left(v, v^\prime\right) < \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3} = \delta,$$   which implies that    $$ \tag{5} d_Y\left( f\left(u^\prime \right), f\left(v^\prime\right) \right) < \frac{\varepsilon}{3},$$   and therefore from (3), (4), and (5) we can conclude that    $$ \begin{align}  d_Y\left( g\left(u\right), g\left(v\right) \right) &\leq d_Y\left( g\left(u \right), f\left(u^\prime \right) \right) + d_Y\left( f\left(u^\prime\right), f\left(v^\prime\right) \right) + d_Y\left( f\left(v^\prime\right), g\left(v\right) \right) \\ &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\ &= \varepsilon. \end{align}  $$ Let us take a real number $\eta$ such that $$0 < \eta < \frac{\delta}{3}.$$ Thus, corresponding to every real number $\varepsilon > 0$, we can find a positive real number $\eta$ such that $$d_Y \left( g(u), g(v) \right) < \varepsilon$$   for all points $u$ and $v$ in $X$ for which $$ d_X \left(u, v \right) < \eta.$$ Hence $g$ is uniformly continuous on $X$. Let $g^\prime$ be (also)  a (uniformly) continuous extension of $f$ from $E$ to $X$. Since $g^\prime(p) = f(p) = g(p)$ for all $p \in E$ and since $E$ is dense in $X$, therefore by virtue of the conclusion in Prob. 4 we can conclude that $g^\prime(p) = g(p)$ for all $p \in X$. Now I know that the first part of the above solution, where we are required to show that $\left\{ f\left(x_n\right) \right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence $\left\{ x_n \right\}$ in $E$, is correct, isn't it? What about the second part? Is the result I've stated correct? If so, then is my proof correct also?","Here is Prob. 11, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a uniformly continuous mapping of a metric space $X$ into a    metric space $Y$ and prove that $\left\{ f\left( x_n \right) \right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence    $\left\{ x_n \right\}$ in $X$. Use this result to give an alternative proof of the theorem stated in Exercise 13. Now here is Prob. 13, Chap. 4 in Baby Rudin, 3rd edition: Let $E$ be a dense subset of a metric space $X$, and let $f$ be a unifromly continuous real function defined on $E$. Prove that $f$ has a continuous extension from $E$ to $X$. ... (Uniqueness follows from Exercise 4.) ... And, here is Prob. 4, Chap. 4: Let $f$ and $g$ be continuous mappings of a metric space $X$ into a metric space $Y$, and let $E$ be a dense subset of $X$. Prove that $f(E)$ is dense in $f(X)$. If $g(p) = f(p)$ for all $p \in E$, prove that $g(p) = f(p)$ for all $p \in X$. (In other words, a continuous mapping is determined by its values on a dense subset of its domain.) My effort: Since $f$ is uniformly continuous on $X$, corresponding to every real number $\varepsilon > 0$, we can find a real number $\delta > 0$ such that $$d_Y\left(f(x), f(y)\right) < \varepsilon$$ for all points $x, y \in X$ which satisfy $$d_X(x,y)<\delta.$$ Now since $\left\{ x_n \right\}$ is a Cauchy sequence in $\left( X, d_X \right)$, therefore corresponding to the real number $\delta$ in the preceding paragraph we can find a natural number $N$ such that $$d_X\left(x_m, x_n \right)< \delta \ \mbox{ for any natural numbers } m \mbox{ and } n \mbox{ such that } m > N \mbox{ and } n > N.$$ So from the preceding two paragraphs we can conclude that $$d_Y\left( f\left(x_m\right), f\left(x_n\right) \right) < \varepsilon \ \mbox{ for any natural numbers } m \mbox{ and } n \mbox{ such that } m > N \mbox{ and } n > N,$$    from which it follows that $\left\{ f\left(x_n\right) \right\}$ is a Cauchy sequence in $\left( Y, d_Y \right)$. Now let $E$ be a dense set in $\left(X, d_X \right)$, $\left( Y, d_Y \right)$ be a complete metric space, and $f$ be a uniformly continuous mapping of $E$ into $Y$. We now show that there exists a unique uniformly continuous mapping $g$ of $X$ into $Y$ such that $g(p) = f(p)$ for all $p \in E$. Let $p \in X$. Since $E$ is dense in $X$, we can find a sequence $\left\{p_n\right\}$ in $E$ converging to the point $p$ in $\left( X, d_X \right)$. Now as the sequence $\left\{ p_n \right\}$ is a Cauchy sequence in $E$ and as $f$ is uniformly continuous, so the image sequence $\left\{ f\left(p_n \right)\right\}$ is a Cauchy sequence in the complete metric space $Y$, and so this sequence converges to some point $q$ in $Y$. We now show that this point $q$ is independent of the particular sequence $\left\{ p_n \right\}$ in $E$ which converges to $p$. For this, let $\left\{ p^\prime_n \right\}$ be another sequence in $E$ converging in $X$ to the point $p$. Then as before the image sequence $\left\{ f\left(p^\prime_n\right)\right\}$ converges in $Y$ to some point $q^\prime$. Now consider the sequence $\left\{ x_n \right\}$ in $E$ defined as follows: Let $$x_n = \begin{cases} p_{\frac{n}{2}} \ \mbox{ if $n$ is even}; \\ p^\prime_{\frac{n+1}{2}} \ \mbox{ if $n$ is odd}. \end{cases}$$ This sequence too converges to point $p$ in $X$ and is therefore Cauchy and so its image sequence $\left\{ f\left( x_n \right) \right\}$ is also a Cauchy sequence in $Y$, which is a complete metric space; so the sequence $\left\{ f\left( x_n \right) \right\}$ converges in $Y$ to some point $y$. Therefore every subsequence of $\left\{ f\left( x_n \right) \right\}$ also converges to $y$. But we note that,    $$p_n = x_{2n} \ \mbox{ and } p^\prime_n = x_{2n-1} \ \mbox{ for all } n \in \mathbb{N}.$$ So    $$q = \lim_{n \to \infty} f\left(p_n \right) = \lim_{n \to \infty} f\left( x_{2n}\right) = y,$$   where the limit is in $Y$, and similarly $$q^\prime = \lim_{n\to\infty} f\left(p_{2n-1}\right) = y.$$ Thus $q^\prime = q$. Now let's define the mapping $g$ of $X$ into $Y$ as follows: For any point $p \in X$, let  $$\tag{1} g(p) = \lim_{n \to \infty} f \left(p_n \right),$$ the limit being in the metric space $\left(Y, d_Y \right)$, where $\left\{ p_n \right\}$ is any sequence in $E$ such that $$\tag{2} \lim_{n \to \infty} p_n = p$$ in $\left( X, d_X \right)$. If $p \in E$, then we can take $p_n = p$ for all $n$ so that $$g(p) = \lim_{n \to \infty} f\left(p_n\right) = f(p).$$    Thus $g$ is an extension of $f$ from $E$ to $X$. Let $\varepsilon > 0$ be a given real number. Since $f$ is uniformly continuous on $E$, we can find a real number $\delta > 0$ such that $$d_Y\left( f(p), f(q) \right) < \frac{\varepsilon}{3}$$ for all points $p$ and $q$ in $E$ for which $$d_X\left(p, q\right) < \delta. $$ Now let $u$ and $v$ be any two points of $X$ which satisfy $$d_X\left(u, v\right) < \frac{\delta}{3}.$$    Then since $E$ is dense in $X$, we can find points $u^\prime$ and $v^\prime$ in $E$ such that    $$\tag{3} d_X \left( u, u^\prime \right) < \frac{\delta}{3} \mbox{ and } d_Y\left(g(u), f(u^\prime) \right) < \frac{\varepsilon}{3},$$   and also    $$\tag{4} d_X \left( v, v^\prime \right) < \frac{\delta}{3} \mbox{ and } d_Y\left( g(v), f(v^\prime) \right) < \frac{\varepsilon}{3}.$$   This is possible in view of (1) and (2) above. Then we note that    $$d_X \left( u^\prime, v^\prime \right) \leq d_X\left(u^\prime, u \right) + d_X\left(u, v\right) + d_X\left(v, v^\prime\right) < \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3} = \delta,$$   which implies that    $$ \tag{5} d_Y\left( f\left(u^\prime \right), f\left(v^\prime\right) \right) < \frac{\varepsilon}{3},$$   and therefore from (3), (4), and (5) we can conclude that    $$ \begin{align}  d_Y\left( g\left(u\right), g\left(v\right) \right) &\leq d_Y\left( g\left(u \right), f\left(u^\prime \right) \right) + d_Y\left( f\left(u^\prime\right), f\left(v^\prime\right) \right) + d_Y\left( f\left(v^\prime\right), g\left(v\right) \right) \\ &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\ &= \varepsilon. \end{align}  $$ Let us take a real number $\eta$ such that $$0 < \eta < \frac{\delta}{3}.$$ Thus, corresponding to every real number $\varepsilon > 0$, we can find a positive real number $\eta$ such that $$d_Y \left( g(u), g(v) \right) < \varepsilon$$   for all points $u$ and $v$ in $X$ for which $$ d_X \left(u, v \right) < \eta.$$ Hence $g$ is uniformly continuous on $X$. Let $g^\prime$ be (also)  a (uniformly) continuous extension of $f$ from $E$ to $X$. Since $g^\prime(p) = f(p) = g(p)$ for all $p \in E$ and since $E$ is dense in $X$, therefore by virtue of the conclusion in Prob. 4 we can conclude that $g^\prime(p) = g(p)$ for all $p \in X$. Now I know that the first part of the above solution, where we are required to show that $\left\{ f\left(x_n\right) \right\}$ is a Cauchy sequence in $Y$ for every Cauchy sequence $\left\{ x_n \right\}$ in $E$, is correct, isn't it? What about the second part? Is the result I've stated correct? If so, then is my proof correct also?",,"['real-analysis', 'analysis', 'metric-spaces', 'cauchy-sequences', 'uniform-continuity']"
13,checking Inner product,checking Inner product,,"Consider the vector space of polynomials $\mathbb{R}[x]$. Let $p=\displaystyle \sum_{k=0}^{n} a_kx^k$ and $q= \displaystyle \sum_{j=0}^{n} b_jx^j$ be two polynomials each of degree $n$ in $\mathbb{R}[x]$. Assume that the sequence $\{s_n\}_{n=0}^\infty$ is positive definite and Let $L: \mathbb{R}[x] \rightarrow \mathbb{R}$ be a psoitive semi-definite linear functional suh that $L(x^n) =s_n , n\ge 0$. This implies that $L(p^2)> 0$ for all $p \in \mathbb{R}[x], p\neq 0$. Then $$\left<p,q \right>:= L(pq), \quad p,q \in \mathbb{R}[x]$$ defines an inner product $\left<.,.\right>$ on $\mathbb{R}[x]$ since $$ \left<p,q \right> =  \left< \sum_{k=0}^{n} a_kx^k, \sum_{j=0}^{n} b_jx^j\right> =L(pq) = \sum_{k,j=0}^{n}s_{k+j}a_kb_j.$$ I was checking that indeed $\left<.,.\right>$ is an inner product. First  $$\left<p,p \right> = L(p^2)\ge 0, \forall p \in \mathbb{R}[x].$$ Secondly  $$\left<p,q \right>= L(pq)=L(qp) =\left<q,p \right>$$ I checked other properties of an inner product but I am a bit confused if this last property is true $\left<p,p \right>=0$ if and only if $p=0$ since $\{s_n\}_{n=0}^{\infty}$ is positive definite by assumption. It is clear to me that if p=0 then $\left<p,p\right>=0$ but I am not convinced with the reverse direction. Thanks for helping me.","Consider the vector space of polynomials $\mathbb{R}[x]$. Let $p=\displaystyle \sum_{k=0}^{n} a_kx^k$ and $q= \displaystyle \sum_{j=0}^{n} b_jx^j$ be two polynomials each of degree $n$ in $\mathbb{R}[x]$. Assume that the sequence $\{s_n\}_{n=0}^\infty$ is positive definite and Let $L: \mathbb{R}[x] \rightarrow \mathbb{R}$ be a psoitive semi-definite linear functional suh that $L(x^n) =s_n , n\ge 0$. This implies that $L(p^2)> 0$ for all $p \in \mathbb{R}[x], p\neq 0$. Then $$\left<p,q \right>:= L(pq), \quad p,q \in \mathbb{R}[x]$$ defines an inner product $\left<.,.\right>$ on $\mathbb{R}[x]$ since $$ \left<p,q \right> =  \left< \sum_{k=0}^{n} a_kx^k, \sum_{j=0}^{n} b_jx^j\right> =L(pq) = \sum_{k,j=0}^{n}s_{k+j}a_kb_j.$$ I was checking that indeed $\left<.,.\right>$ is an inner product. First  $$\left<p,p \right> = L(p^2)\ge 0, \forall p \in \mathbb{R}[x].$$ Secondly  $$\left<p,q \right>= L(pq)=L(qp) =\left<q,p \right>$$ I checked other properties of an inner product but I am a bit confused if this last property is true $\left<p,p \right>=0$ if and only if $p=0$ since $\{s_n\}_{n=0}^{\infty}$ is positive definite by assumption. It is clear to me that if p=0 then $\left<p,p\right>=0$ but I am not convinced with the reverse direction. Thanks for helping me.",,"['real-analysis', 'linear-algebra', 'abstract-algebra', 'functional-analysis', 'inner-products']"
14,Possible road-maps for proving $\lim_{x\to 0}\frac{\sin x}{x}=1$ in a non-circular way,Possible road-maps for proving  in a non-circular way,\lim_{x\to 0}\frac{\sin x}{x}=1,"$$\lim_{x\to 0}\frac{\sin x}{x}=1\tag{1}$$ Poofs for the limit above have been asked many many times in MSE. Here are a few of them: How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? Finding functions for the squeeze theorem for $\lim_{x \to 0}{\frac {x}{\sin x}}$ Non-circular proof of $\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1$ Here is my question : What are possible ""road maps"" for developing a proof for (1) in a non-circular rigorous way from a few axioms? One possible ""road map"" is as follows: ZFC --- natural numbers --- rational numbers --- real numbers --- limits and derivatives --- power series --- power series definition of the sine function  --- proof of (1) (In such an approach, one can derive the basic properties of the trigonometric functions from the power series definition without any appeal to the geometric notion of angle.) [Added] My question is essential this: are there other ""road maps"" than the one given above? If one is going to give an geometric argument about (1) like most introductory calculus textbooks do, then I would like to add one more question : What axioms in geometry would you need in order to develop a satisfactory ""rigorous"" definition of the sine function?","$$\lim_{x\to 0}\frac{\sin x}{x}=1\tag{1}$$ Poofs for the limit above have been asked many many times in MSE. Here are a few of them: How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? Finding functions for the squeeze theorem for $\lim_{x \to 0}{\frac {x}{\sin x}}$ Non-circular proof of $\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1$ Here is my question : What are possible ""road maps"" for developing a proof for (1) in a non-circular rigorous way from a few axioms? One possible ""road map"" is as follows: ZFC --- natural numbers --- rational numbers --- real numbers --- limits and derivatives --- power series --- power series definition of the sine function  --- proof of (1) (In such an approach, one can derive the basic properties of the trigonometric functions from the power series definition without any appeal to the geometric notion of angle.) [Added] My question is essential this: are there other ""road maps"" than the one given above? If one is going to give an geometric argument about (1) like most introductory calculus textbooks do, then I would like to add one more question : What axioms in geometry would you need in order to develop a satisfactory ""rigorous"" definition of the sine function?",,['calculus']
15,"If $f \in L^1$ and $f$ is everywhere strictly positive, then does it follow that $|\widehat{f}(y)| < \widehat{f}(0)$ for $y \neq 0$?","If  and  is everywhere strictly positive, then does it follow that  for ?",f \in L^1 f |\widehat{f}(y)| < \widehat{f}(0) y \neq 0,"As the question title suggests, if $f \in L^1$ and $f$ is everywhere strictly positive, then does it follow that $|\widehat{f}(y)| < \widehat{f}(0)$ for $y \neq 0$?","As the question title suggests, if $f \in L^1$ and $f$ is everywhere strictly positive, then does it follow that $|\widehat{f}(y)| < \widehat{f}(0)$ for $y \neq 0$?",,['real-analysis']
16,How to construct the set E invoving an almost constant function?,How to construct the set E invoving an almost constant function?,,"Assume that $f$ is a function from $\mathbb{R}$ to $\mathbb{R}$, and for all $h\in \mathbb{R}$, the set $E_h=\{x:f(x+h)-f(x)\neq 0,x\in \mathbb{R}\}$ is a finite set which has no more than 2016 elements.  Prove that there exists a set $E$ which has no more than $1008$ elements, such that $f$ is a constant in $\mathbb{R}\backslash E$. To solve this problem, I think it needs a keen observation.  What I have thought is first to prove $f(\mathbb{R})$ have no more than 1008 elements, but it is hard for me.  How can I do this?","Assume that $f$ is a function from $\mathbb{R}$ to $\mathbb{R}$, and for all $h\in \mathbb{R}$, the set $E_h=\{x:f(x+h)-f(x)\neq 0,x\in \mathbb{R}\}$ is a finite set which has no more than 2016 elements.  Prove that there exists a set $E$ which has no more than $1008$ elements, such that $f$ is a constant in $\mathbb{R}\backslash E$. To solve this problem, I think it needs a keen observation.  What I have thought is first to prove $f(\mathbb{R})$ have no more than 1008 elements, but it is hard for me.  How can I do this?",,"['real-analysis', 'functions']"
17,Is there any difference between a parametric equation and a vector function?,Is there any difference between a parametric equation and a vector function?,,"Perhaps it's due to the fact that I'm currently tackling introductory material on parametric equations, but it seems to me at the moment that there is no real difference between a set of parametric equations and a vector function. I'll give the standard example: Let's say I have a parametric curve defined by the parametric equations $x=\cos(t)$ and $y=\sin(t)$. What stops me from defining $r(t) = \left\langle  x, \  y   \right\rangle  = \left\langle  \cos(t), \  \sin(t)   \right\rangle$, which describes the exact same parametric curve (the unit circle) as the parametric equations for $x$ and $y$? Now I know assume that there has to be some difference between parametric equations and vector functions, but with the material I'm currently working with I can't seem to find a counter-example, or cases where they differ. I also realize that the concept of parameterization is critical to fields like Differential Geometry (based on what I've read so far in do Carmo's book ), and proofs of the big Integral Theorems (generalized Stokes' Theorem etc) rely on it, and this concept is something I want to understand rock solid. Can anyone give an example, as to why a set of parametric equations are different from vector functions of the form $f : \mathbb{R} \to \mathbb{R^m}$, and furthermore as to why they are so important to theorems in higher-dimensions?","Perhaps it's due to the fact that I'm currently tackling introductory material on parametric equations, but it seems to me at the moment that there is no real difference between a set of parametric equations and a vector function. I'll give the standard example: Let's say I have a parametric curve defined by the parametric equations $x=\cos(t)$ and $y=\sin(t)$. What stops me from defining $r(t) = \left\langle  x, \  y   \right\rangle  = \left\langle  \cos(t), \  \sin(t)   \right\rangle$, which describes the exact same parametric curve (the unit circle) as the parametric equations for $x$ and $y$? Now I know assume that there has to be some difference between parametric equations and vector functions, but with the material I'm currently working with I can't seem to find a counter-example, or cases where they differ. I also realize that the concept of parameterization is critical to fields like Differential Geometry (based on what I've read so far in do Carmo's book ), and proofs of the big Integral Theorems (generalized Stokes' Theorem etc) rely on it, and this concept is something I want to understand rock solid. Can anyone give an example, as to why a set of parametric equations are different from vector functions of the form $f : \mathbb{R} \to \mathbb{R^m}$, and furthermore as to why they are so important to theorems in higher-dimensions?",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'vectors', 'parametric']"
18,Fixed-point iteration and continuity of parameters,Fixed-point iteration and continuity of parameters,,"Let $X$ a compact set and $A\subseteq \mathbb{R}$. Consider a continuous function $f\colon X\times A\to X$ and construct a fixed-point iteration as follows $$ x_{k+1}=f(x_k,a),\quad x_0\in X, a \in A.\quad (\star) $$ My question : If $(\star)$ admits a unique fixed point, denoted by $\mathrm{Fix}(f_a)$, for all $a\in A$, can we conclude that $\mathrm{Fix}(f_a)$ is a continuous function of $a$? What can be said in case $(\star)$ admits a set of fixed points for all $a\in A$? Thanks for your help. Comments. This question is different from this one . Indeed, here $f$ is not assumed to be contractive.","Let $X$ a compact set and $A\subseteq \mathbb{R}$. Consider a continuous function $f\colon X\times A\to X$ and construct a fixed-point iteration as follows $$ x_{k+1}=f(x_k,a),\quad x_0\in X, a \in A.\quad (\star) $$ My question : If $(\star)$ admits a unique fixed point, denoted by $\mathrm{Fix}(f_a)$, for all $a\in A$, can we conclude that $\mathrm{Fix}(f_a)$ is a continuous function of $a$? What can be said in case $(\star)$ admits a set of fixed points for all $a\in A$? Thanks for your help. Comments. This question is different from this one . Indeed, here $f$ is not assumed to be contractive.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'fixed-point-theorems']"
19,"If $f(x/n)\to0$ when $n\to\infty$, for every $x$ and $f$ is continuous, then $f(x)\to0$ when $x\to0$, or not?","If  when , for every  and  is continuous, then  when , or not?",f(x/n)\to0 n\to\infty x f f(x)\to0 x\to0,"Let $f:(0,\infty)\to \Bbb R$ be continuous and such that for each $x>0$ the sequence $\{f(\frac{x}{n})\}\to 0$. Does it imply that $\lim_{x\to 0^+} f(x)=0$? My try :Fix $x>0$.Since $f$ is continuous then $\lim _{n\to \infty}f(\frac{x}{n})=f(\lim_{n\to \infty} \frac{x}{n})=f(0)\implies f(0)=0$ Since $f$ is continuous and $f(0)=0\implies \lim _{x\to 0^+} f(x)=0$ Is the solution correct?In the book it is  given to use the Baire Category Theorem.","Let $f:(0,\infty)\to \Bbb R$ be continuous and such that for each $x>0$ the sequence $\{f(\frac{x}{n})\}\to 0$. Does it imply that $\lim_{x\to 0^+} f(x)=0$? My try :Fix $x>0$.Since $f$ is continuous then $\lim _{n\to \infty}f(\frac{x}{n})=f(\lim_{n\to \infty} \frac{x}{n})=f(0)\implies f(0)=0$ Since $f$ is continuous and $f(0)=0\implies \lim _{x\to 0^+} f(x)=0$ Is the solution correct?In the book it is  given to use the Baire Category Theorem.",,"['real-analysis', 'limits', 'continuity']"
20,A Convergence Result Conjecture,A Convergence Result Conjecture,,"When I was doing my own research, I am tempted to prove the following statement. Suppose $\{a_n\}$ and $\{b_n\}$ are sequences of real numbers such that $\frac{1}{n}\sum_{i = 1}^n a_ib_i \to G \neq 0$ . $f$ is a continuous and bounded function on $\mathbb{R}^1$ . Fix $x_0 \in \mathbb{R}^1$ , then $$\frac{1}{n}\sum_{i = 1}^n a_ib_i\int_0^1\left[f(x_0) - f\left(x_0 + sn^{-1/2}b_i\right)\right]ds \to 0 \tag{1}$$ as $n \to \infty$ . PS: If necessary, one may also assume $\max_{1 \leq i \leq n}|b_i| = O(n^{1/4})$ . I tend to believe this is true and spent some time to prove it. However, the difficulty comes from when I tried to bound the left side of $(1)$ by triangle inequality, although the integral is controlled by arbitrarily small positive number, the absolute value was also imposed on $a_ib_i$ , which makes it difficult to apply the non-absolute-value condition $\frac{1}{n}\sum_{i = 1}^n a_ib_i \to G$ (note we do not have any information about whether the summands are positive or negative.). Can someone give me a clear proof of $(1)$ if it is true? Or construct a counter example to overthrow it? Edit: I am happy to see this question gets much attention. In fact, the background of this problem comes from some theoretical proof under quantile regression settings. The above conjecture is my own abstraction. The thing I feel confusing are the proofs from some publications. The missing details seems hard to fix. In the following, I will list the original statements from some papers: For example, in the proof of Gutenbrunnner, Jureckova (1992) , Lemma $1$ , the author claims directly (I simplified the case to homoscedastic case so that $\sigma_{ni} \equiv 1$ ): \begin{align*} \sup_{\|t\| \leq K, \varepsilon \leq \alpha \leq 1 - \varepsilon} & \left\|\frac{1}{n}\sum_{i = 1}^n x_{ni}x_{ni}'t\int_0^1\left[f\left(F^{-1}(\alpha) + n^{-1/2}x_{ni}'t\right) - f(F^{-1}(\alpha))\right]ds\right\| = o(1). \tag{2} \end{align*} Under the assumptions: $f$ is the continuous density of some distribution function $F$ , which is positive and finite on $\{t: 0 < F(t) <1\}$ . $x_{ni}$ are rows of an $n \times p$ design matrix $X_n$ , where $p$ is fixed and $n \to \infty$ . The first column of $X_n$ consists of ones and the other columns are orthogonal to the first one. $\|X_n\|_\infty = o(n^{1/2})$ . $Q_n = \frac{1}{n}X_n^TX_n \to Q$ where $Q$ is a positive definite $p \times p$ matrix. I think $(2)$ and $(1)$ bear some resemblance, so if $(1)$ were not true, could $(2)$ be true? The problem $(2)$ may be even a little more challenging since for which we are actually dealing with the convergence of a sequence of matrices. Another even more ambitious claim is Lemma A.2 of Koenker, Zhao (1996) , which states (both the statement and proof have many confusing typos, here I presented the version I corrected): If $\{g_t\}$ and $\{H_t\}$ are sequences of random $p$ -vectors such that $E\|g_t\|^{2 + \delta} \leq S < \infty$ , $E\|H_t\|^{2 + \delta} \leq S < \infty$ for some $\delta > 0$ . $\{u_t\}$ is a sequence of i.i.d. random variables with continuous and bounded density $f$ . $g_t$ and $H_t$ are independent of $(u_t, u_{t - 1}, \ldots)$ and $$n^{-1}\sum_{t = 1}^n g_tH_t' \to_P G$$ for a nonrandom, nonsingular matrix. Then, $$V(\Delta) = n^{-1/2}\sum_{t = 1}^n g_t\psi_\tau(u_t - F^{-1}(\tau) - n^{-1/2}H_t'\Delta)$$ satisfies $$\sup_{\|\Delta\| \leq M} \|V(\Delta) - V(0) + f(F^{-1}(\tau))G\Delta\| = o_P(1)$$ for fixed $M$ , $0 < M < \infty$ . Here $\psi_\tau(x) = \tau - I(x < 0)$ , $\tau \in (0, 1)$ , $I$ is indicator function. The last step to complete the proof of this lemma turns out to be a similar claim as $(1)$ and $(2)$ , that is $$\sup_{\|\Delta\| \leq M}\left\|n^{-1/2}\sum_{1}^n g_t(F(F^{-1}(\tau)) - F(F^{-1}(\tau) + n^{-1/2}H_t'\Delta)) + f(F^{-1}(\tau))G\Delta\right\| = o_P(1). \tag{3}$$ $(3)$ holds if the following form like $(1)$ $$\sup_{\|\Delta\| \leq M}\left\|n^{-1}\sum_{1}^n g_tH_t'\Delta\int_0^1\left[f(F^{-1}(\tau)) - f(F^{-1}(\tau) + sn^{-1/2}H_t'\Delta)\right]ds\right\| = o_P(1) \tag{4}$$ holds. But to prove $(4)$ , we probably encounter the same problem we must handle in proving $(1)$ , so if $(1)$ were wrong, would $(4)$ be true? Of course, it is also very welcome if someone can provide me with a direct proof of $(2)$ and $(3)$ (without linking them to $(1)$ ).","When I was doing my own research, I am tempted to prove the following statement. Suppose and are sequences of real numbers such that . is a continuous and bounded function on . Fix , then as . PS: If necessary, one may also assume . I tend to believe this is true and spent some time to prove it. However, the difficulty comes from when I tried to bound the left side of by triangle inequality, although the integral is controlled by arbitrarily small positive number, the absolute value was also imposed on , which makes it difficult to apply the non-absolute-value condition (note we do not have any information about whether the summands are positive or negative.). Can someone give me a clear proof of if it is true? Or construct a counter example to overthrow it? Edit: I am happy to see this question gets much attention. In fact, the background of this problem comes from some theoretical proof under quantile regression settings. The above conjecture is my own abstraction. The thing I feel confusing are the proofs from some publications. The missing details seems hard to fix. In the following, I will list the original statements from some papers: For example, in the proof of Gutenbrunnner, Jureckova (1992) , Lemma , the author claims directly (I simplified the case to homoscedastic case so that ): Under the assumptions: is the continuous density of some distribution function , which is positive and finite on . are rows of an design matrix , where is fixed and . The first column of consists of ones and the other columns are orthogonal to the first one. . where is a positive definite matrix. I think and bear some resemblance, so if were not true, could be true? The problem may be even a little more challenging since for which we are actually dealing with the convergence of a sequence of matrices. Another even more ambitious claim is Lemma A.2 of Koenker, Zhao (1996) , which states (both the statement and proof have many confusing typos, here I presented the version I corrected): If and are sequences of random -vectors such that , for some . is a sequence of i.i.d. random variables with continuous and bounded density . and are independent of and for a nonrandom, nonsingular matrix. Then, satisfies for fixed , . Here , , is indicator function. The last step to complete the proof of this lemma turns out to be a similar claim as and , that is holds if the following form like holds. But to prove , we probably encounter the same problem we must handle in proving , so if were wrong, would be true? Of course, it is also very welcome if someone can provide me with a direct proof of and (without linking them to ).","\{a_n\} \{b_n\} \frac{1}{n}\sum_{i = 1}^n a_ib_i \to G \neq 0 f \mathbb{R}^1 x_0 \in \mathbb{R}^1 \frac{1}{n}\sum_{i = 1}^n a_ib_i\int_0^1\left[f(x_0) - f\left(x_0 + sn^{-1/2}b_i\right)\right]ds \to 0 \tag{1} n \to \infty \max_{1 \leq i \leq n}|b_i| = O(n^{1/4}) (1) a_ib_i \frac{1}{n}\sum_{i = 1}^n a_ib_i \to G (1) 1 \sigma_{ni} \equiv 1 \begin{align*}
\sup_{\|t\| \leq K, \varepsilon \leq \alpha \leq 1 - \varepsilon} & \left\|\frac{1}{n}\sum_{i = 1}^n x_{ni}x_{ni}'t\int_0^1\left[f\left(F^{-1}(\alpha) + n^{-1/2}x_{ni}'t\right) - f(F^{-1}(\alpha))\right]ds\right\| = o(1).
\tag{2}
\end{align*} f F \{t: 0 < F(t) <1\} x_{ni} n \times p X_n p n \to \infty X_n \|X_n\|_\infty = o(n^{1/2}) Q_n = \frac{1}{n}X_n^TX_n \to Q Q p \times p (2) (1) (1) (2) (2) \{g_t\} \{H_t\} p E\|g_t\|^{2 + \delta} \leq S < \infty E\|H_t\|^{2 + \delta} \leq S < \infty \delta > 0 \{u_t\} f g_t H_t (u_t,
u_{t - 1}, \ldots) n^{-1}\sum_{t = 1}^n g_tH_t' \to_P G V(\Delta) = n^{-1/2}\sum_{t = 1}^n g_t\psi_\tau(u_t - F^{-1}(\tau) - n^{-1/2}H_t'\Delta) \sup_{\|\Delta\| \leq M} \|V(\Delta) - V(0) + f(F^{-1}(\tau))G\Delta\| = o_P(1) M 0 < M < \infty \psi_\tau(x) = \tau - I(x < 0) \tau \in (0, 1) I (1) (2) \sup_{\|\Delta\| \leq M}\left\|n^{-1/2}\sum_{1}^n g_t(F(F^{-1}(\tau)) - F(F^{-1}(\tau) + n^{-1/2}H_t'\Delta)) + f(F^{-1}(\tau))G\Delta\right\| = o_P(1). \tag{3} (3) (1) \sup_{\|\Delta\| \leq M}\left\|n^{-1}\sum_{1}^n g_tH_t'\Delta\int_0^1\left[f(F^{-1}(\tau)) - f(F^{-1}(\tau) + sn^{-1/2}H_t'\Delta)\right]ds\right\| = o_P(1) \tag{4} (4) (1) (1) (4) (2) (3) (1)","['real-analysis', 'probability', 'convergence-divergence', 'asymptotics']"
21,"Solving or knowing something about a non-linear PDE which is ""almost"" linear?","Solving or knowing something about a non-linear PDE which is ""almost"" linear?",,"Let $a>0$ be fixed. I have the following PDE: $u=u(t,x)$, $t\in [0,1]$, $x\in \mathbb{R}$, $$-\partial_t u = |\partial_x u| + \frac{1}{2}\partial_x^2 u, \quad u(1,x)=\delta_a(x)-\delta_{-a}(x),\quad \lim_{|x|\to \infty}u(t,x)=0.$$ Observe the absolute value. If there was no absolute value the PDE would be linear parabolic and a solution is possible. The equation is not linear but ""almost"". Is there any trick or way to find or have an intuition of the solution? Actually, I am only interested in the behaviour of $u$ in the region $\mathbb{R}\setminus [-a,a]$. I would like to know whether $\partial_x u(t,x)<0$ on $\mathbb{R}\setminus [-a,a]$ for every $t\in [0,1]$. In other words, whether $x\mapsto u(t,x)$ is decreasing on $\mathbb{R}\setminus [-a,a]$ for every $t\in [0,1]$. Any ideas? Thanks :D","Let $a>0$ be fixed. I have the following PDE: $u=u(t,x)$, $t\in [0,1]$, $x\in \mathbb{R}$, $$-\partial_t u = |\partial_x u| + \frac{1}{2}\partial_x^2 u, \quad u(1,x)=\delta_a(x)-\delta_{-a}(x),\quad \lim_{|x|\to \infty}u(t,x)=0.$$ Observe the absolute value. If there was no absolute value the PDE would be linear parabolic and a solution is possible. The equation is not linear but ""almost"". Is there any trick or way to find or have an intuition of the solution? Actually, I am only interested in the behaviour of $u$ in the region $\mathbb{R}\setminus [-a,a]$. I would like to know whether $\partial_x u(t,x)<0$ on $\mathbb{R}\setminus [-a,a]$ for every $t\in [0,1]$. In other words, whether $x\mapsto u(t,x)$ is decreasing on $\mathbb{R}\setminus [-a,a]$ for every $t\in [0,1]$. Any ideas? Thanks :D",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'proof-verification']"
22,"Prove that $\lim_{x\to\infty}\frac{1}{x}\int_{0}^xf(t)\,dt=a$ if $f$ is continuous and $\lim_{x\to\infty}f(x)=a$",Prove that  if  is continuous and,"\lim_{x\to\infty}\frac{1}{x}\int_{0}^xf(t)\,dt=a f \lim_{x\to\infty}f(x)=a",Suppose $f$ is continuous and $\displaystyle \lim_{x \to \infty} f(x) = a$. Prove that $$\displaystyle \lim_{x \to \infty}\dfrac{1}{x} \int_{0}^x f(t) dt = a.$$ I don't see why in the solution below they take the integral from $1$ to $N$ instead of $0$ to $N$ and how that proves the result. Also is $M$ considered to be positive? Book solution:,Suppose $f$ is continuous and $\displaystyle \lim_{x \to \infty} f(x) = a$. Prove that $$\displaystyle \lim_{x \to \infty}\dfrac{1}{x} \int_{0}^x f(t) dt = a.$$ I don't see why in the solution below they take the integral from $1$ to $N$ instead of $0$ to $N$ and how that proves the result. Also is $M$ considered to be positive? Book solution:,,"['calculus', 'real-analysis']"
23,Part of proof of the set of continuous integrable functions is dense in $L^1(\Bbb R)$,Part of proof of the set of continuous integrable functions is dense in,L^1(\Bbb R),"I want to prove: If $g$ belongs to $L(\Bbb R, \Bbb B, \lambda)$ and $\epsilon\gt 0$, then there exists a continuous function $f$ such that $\Vert g-f\Vert_1=\int \lvert g-f\rvert \,\text{d}\lambda \lt \epsilon$. I know this then implies the set of continuous integrable functions is dense in $L^1(\Bbb R)$  in the metric $d(f,g)=\int_{\Bbb R}|f-g|\,\text{d}\mu$. Note that $\Bbb B$ denotes the Borel set; $\lambda$ denotes the Lebesgue Measure on $\Bbb R$; $L^1$ denotes space of lebesgue integrable functions. I'm not sure if Show that there exists a continuous function $f$ such that $\int |\chi_A-f| d\lambda\lt \epsilon$ could help deducing the proof. Could someone show how to prove the above statement? Thanks.","I want to prove: If $g$ belongs to $L(\Bbb R, \Bbb B, \lambda)$ and $\epsilon\gt 0$, then there exists a continuous function $f$ such that $\Vert g-f\Vert_1=\int \lvert g-f\rvert \,\text{d}\lambda \lt \epsilon$. I know this then implies the set of continuous integrable functions is dense in $L^1(\Bbb R)$  in the metric $d(f,g)=\int_{\Bbb R}|f-g|\,\text{d}\mu$. Note that $\Bbb B$ denotes the Borel set; $\lambda$ denotes the Lebesgue Measure on $\Bbb R$; $L^1$ denotes space of lebesgue integrable functions. I'm not sure if Show that there exists a continuous function $f$ such that $\int |\chi_A-f| d\lambda\lt \epsilon$ could help deducing the proof. Could someone show how to prove the above statement? Thanks.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
24,Theorem 3.7 in Baby Rudin: The subsequential limits of a sequence in a metric space form a closed set,Theorem 3.7 in Baby Rudin: The subsequential limits of a sequence in a metric space form a closed set,,"Here's Theorem 3.7 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. The subsequential limits of a sequence $(p_n)$ in a metric space $X$ form a closed subset of $X$. And, here's Rudin's proof. Let $E^*$ be the set of all subsequential limits of $(p_n)$ and let $q$ be a limit point of $E^*$. We have to show that $q \in E^*$. Choose $n_1$ so that $p_{n_1} \neq q$. (If no such $n_1$ exists, then $E^*$ has only one point, and there is nothing to prove. ) Put $\delta = d(q, p_{n_1})$. Suppose $n_1, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E^*$, there is an $x \in E^*$ with $d(x, q) < 2^{-i} \delta$. Since $x \in E^*$, there is an $n_i > n_{i-1}$ such that $d(x, p_{n_i}) < 2^{-i} \delta$. Thus $$d(q, p_{n_i}) \leq 2^{1-i} \delta$$ for $i = 1, 2, 3, \ldots$. This says that $(p_{n_i})$ converges to $q$. Hence $q \in E^*$. Now here's my reading of Rudin's proof. If the set $E^*$ of all the subsequential limits of the sequence $(p_n)$ has no limit points, then the set of all the limit points of the set  $E^*$ is empty and is therefore contained in $E^*$. So let's suppose that $q$ is a limit point of the set $E^*$. If $p_n = q$ for all $n \in \mathbb{N}$, then the sequence $(p_n)$, being a constant sequence, converges to $q$, and so every subsequence of $(p_n)$ also converges to $q$; therefore the set $E^*$ consists of a single point $q$ and thus cannot have limit points. So there is a natural number $n$ for which $p_n \neq q$. Let $n_1$ be the smallest such natural number. Let's put $\delta = d\left(q, p_{n_1}\right)$. Then $\delta > 0$. Now since $q$ is a limit point of the set $E^*$, there exists a point $x_1 \in E^*$ such that $$0 < d\left(q, x_1\right) < \frac{\delta}{4}.$$ Now since $x_1$ is a subsequential limit of the sequence $(p_n)$, there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n \to \infty} p_{\varphi_1(n)}.$$ So there exists a natural number $N_1$ such that $$d\left( \ x_1\ ,\  p_{\varphi_1(n)} \ \right) < \frac{\delta}{4} $$ for all natural numbers $n$ such that $n > N_1$. We note that, for each $n \in \mathbb{N}$, the inequality $n \leq \varphi_1(n)$ holds. Let $n_2$ be the natural number defined as $$n_2 \colon= \max \left( \ \varphi_1(n_1 + 1)\ , \ \varphi_1(N_1 + 1) \ \right).$$ Then $n_2 > N_1$ and so $$ d\left( \ q \ , \  p_{n_2} \ \right) \leq d\left( \ q,\  x_1 \ \right) + d\left(\ x_1\ , \  p_{n_2} \ \right) < \frac{\delta}{4} + \frac{\delta}{4} = \frac{\delta}{2}.$$ Now as $q$ is a limit point of $E^*$, there exists a point $x_2 \in E^*$ such that $$0 < d(q, x_2) < \frac{ \delta}{8}.$$ Moreover, since $x_2$ is a subsequential limit of the sequence $(p_n)$, there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n \to \infty} p_{\varphi_2(n)}.$$ So there is a natural number $N_2$ such that $$ d\left( \ x_2 \ , \  p_{\varphi_2(n)} \ \right) < \frac{\delta}{8}$$ for all natural numbers $n > N_2$. Note that, for each $n \in \mathbb{N}$, we have $n \leq \varphi_2 (n)$. Now let $$n_3 \colon= \max \left( \ \varphi_2(n_2 + 1) \ , \  \varphi_2 ( N_2 + 1) \  \right).$$ Then $n_3$ is a natural number greater than $N_2$ and so we have  $$d\left( \ q\ ,\  p_{n_3} \ \right) \leq d\left( \ q \ , \  x_2 \ \right) + d\left( \ x_2 \ , \  p_{n_3} \ \right) < \frac{\delta}{8} + \frac{\delta}{8} = \frac{\delta}{4}.$$ We note that $$n_3 \geq \varphi_2(n_2 + 1) > \varphi_2(n_2) \geq n_2 \geq \varphi_1 (n_1 + 1) > \varphi_1 (n_1) \geq n_1.$$ That is, $n_1, n_2, n_3$ are all natural numbers such that $$n_1 < n_2 < n_3.$$ Continuing in this way, we obtain a subsequence $(p_{n_i})$, where $n_1, n_2, n_3, \ldots \in \mathbb{N}$  and $n_1 < n_2 < n_3 < \cdots$, such that $$d\left(\ q \ , \  p_{n_i} \ \right) \leq \frac{2\delta}{2^i}$$ for all $i \in \mathbb{N}$. We now show that this  subsequence $(p_{n_i})$ converges to $q$. Let $\varepsilon > 0$ be given. Then we can find a natural number $$K > \frac{2\delta}{\varepsilon}.$$ Then $$2^K > K > \frac{2\delta}{\varepsilon}.$$ So for any natural number $i > K$, we have $$2^i > 2^K > \frac{2\delta}{\varepsilon}$$ and so $$ d\left( \ q \ , \  p_{n_i} \ \right) \leq \frac{2 \delta}{2^i} < \varepsilon. $$ Thus every limit point $q$ of the set $E^*$ is also an element of $E^*$. Hence  $E^*$ is closed. Is my reading of Rudin's proof correct? If not, what is it that I'm missing?","Here's Theorem 3.7 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. The subsequential limits of a sequence $(p_n)$ in a metric space $X$ form a closed subset of $X$. And, here's Rudin's proof. Let $E^*$ be the set of all subsequential limits of $(p_n)$ and let $q$ be a limit point of $E^*$. We have to show that $q \in E^*$. Choose $n_1$ so that $p_{n_1} \neq q$. (If no such $n_1$ exists, then $E^*$ has only one point, and there is nothing to prove. ) Put $\delta = d(q, p_{n_1})$. Suppose $n_1, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E^*$, there is an $x \in E^*$ with $d(x, q) < 2^{-i} \delta$. Since $x \in E^*$, there is an $n_i > n_{i-1}$ such that $d(x, p_{n_i}) < 2^{-i} \delta$. Thus $$d(q, p_{n_i}) \leq 2^{1-i} \delta$$ for $i = 1, 2, 3, \ldots$. This says that $(p_{n_i})$ converges to $q$. Hence $q \in E^*$. Now here's my reading of Rudin's proof. If the set $E^*$ of all the subsequential limits of the sequence $(p_n)$ has no limit points, then the set of all the limit points of the set  $E^*$ is empty and is therefore contained in $E^*$. So let's suppose that $q$ is a limit point of the set $E^*$. If $p_n = q$ for all $n \in \mathbb{N}$, then the sequence $(p_n)$, being a constant sequence, converges to $q$, and so every subsequence of $(p_n)$ also converges to $q$; therefore the set $E^*$ consists of a single point $q$ and thus cannot have limit points. So there is a natural number $n$ for which $p_n \neq q$. Let $n_1$ be the smallest such natural number. Let's put $\delta = d\left(q, p_{n_1}\right)$. Then $\delta > 0$. Now since $q$ is a limit point of the set $E^*$, there exists a point $x_1 \in E^*$ such that $$0 < d\left(q, x_1\right) < \frac{\delta}{4}.$$ Now since $x_1$ is a subsequential limit of the sequence $(p_n)$, there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n \to \infty} p_{\varphi_1(n)}.$$ So there exists a natural number $N_1$ such that $$d\left( \ x_1\ ,\  p_{\varphi_1(n)} \ \right) < \frac{\delta}{4} $$ for all natural numbers $n$ such that $n > N_1$. We note that, for each $n \in \mathbb{N}$, the inequality $n \leq \varphi_1(n)$ holds. Let $n_2$ be the natural number defined as $$n_2 \colon= \max \left( \ \varphi_1(n_1 + 1)\ , \ \varphi_1(N_1 + 1) \ \right).$$ Then $n_2 > N_1$ and so $$ d\left( \ q \ , \  p_{n_2} \ \right) \leq d\left( \ q,\  x_1 \ \right) + d\left(\ x_1\ , \  p_{n_2} \ \right) < \frac{\delta}{4} + \frac{\delta}{4} = \frac{\delta}{2}.$$ Now as $q$ is a limit point of $E^*$, there exists a point $x_2 \in E^*$ such that $$0 < d(q, x_2) < \frac{ \delta}{8}.$$ Moreover, since $x_2$ is a subsequential limit of the sequence $(p_n)$, there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n \to \infty} p_{\varphi_2(n)}.$$ So there is a natural number $N_2$ such that $$ d\left( \ x_2 \ , \  p_{\varphi_2(n)} \ \right) < \frac{\delta}{8}$$ for all natural numbers $n > N_2$. Note that, for each $n \in \mathbb{N}$, we have $n \leq \varphi_2 (n)$. Now let $$n_3 \colon= \max \left( \ \varphi_2(n_2 + 1) \ , \  \varphi_2 ( N_2 + 1) \  \right).$$ Then $n_3$ is a natural number greater than $N_2$ and so we have  $$d\left( \ q\ ,\  p_{n_3} \ \right) \leq d\left( \ q \ , \  x_2 \ \right) + d\left( \ x_2 \ , \  p_{n_3} \ \right) < \frac{\delta}{8} + \frac{\delta}{8} = \frac{\delta}{4}.$$ We note that $$n_3 \geq \varphi_2(n_2 + 1) > \varphi_2(n_2) \geq n_2 \geq \varphi_1 (n_1 + 1) > \varphi_1 (n_1) \geq n_1.$$ That is, $n_1, n_2, n_3$ are all natural numbers such that $$n_1 < n_2 < n_3.$$ Continuing in this way, we obtain a subsequence $(p_{n_i})$, where $n_1, n_2, n_3, \ldots \in \mathbb{N}$  and $n_1 < n_2 < n_3 < \cdots$, such that $$d\left(\ q \ , \  p_{n_i} \ \right) \leq \frac{2\delta}{2^i}$$ for all $i \in \mathbb{N}$. We now show that this  subsequence $(p_{n_i})$ converges to $q$. Let $\varepsilon > 0$ be given. Then we can find a natural number $$K > \frac{2\delta}{\varepsilon}.$$ Then $$2^K > K > \frac{2\delta}{\varepsilon}.$$ So for any natural number $i > K$, we have $$2^i > 2^K > \frac{2\delta}{\varepsilon}$$ and so $$ d\left( \ q \ , \  p_{n_i} \ \right) \leq \frac{2 \delta}{2^i} < \varepsilon. $$ Thus every limit point $q$ of the set $E^*$ is also an element of $E^*$. Hence  $E^*$ is closed. Is my reading of Rudin's proof correct? If not, what is it that I'm missing?",,"['real-analysis', 'sequences-and-series', 'analysis', 'proof-verification', 'metric-spaces']"
25,A commutation between curl and integral,A commutation between curl and integral,,"I have been struggling to understand the only derivation of Ampère's law from the Biot-Savart law for a tridimensional distribution of current (which, needless to say, is not the case of a linear distribution of current discussed here ) that I have been able to find, i.e. Wikipedia's outline of proof , for more than a month with no result. I was not excluding that Wikipedia's outline of proof is one of those cases, whose set I have been told to be non-empy, where physics, at least at some level, renounces the rigour of mathematics, until I was told that the proof is indeed rigourous . Although I have recently asked for an alternative proof of the same entailment, the fact that it is not to be excluded that Wikipedia's outline of proof is rigourous has made me decide, as it has been suggested to me in these comments , to ask for an explanation of the single steps that I do not understand of that outline of proof . My first doubt starts at the first commutation between the integral and curl signs. I know that $$\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\left(\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}\right)$$provided that we read the components of $\nabla_r$ as ordinary derivatives (see below) in the sense of elementary multivariable calculus. The outline of proof says that $$\mathbf{B}(\mathbf{r}):=\iiint_V\,d^3l\,\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\iiint_V\,d^3l\,\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}.$$What mathematical result justifies that commutation between integral and curl? I heartily thank you for any answer. In order to take the problem on, it is obviously needed to understand what the integral and the derivatives in $\nabla$ are , but I am not sure about what they are. Since theorems such as Stokes' are usually applied when integrating $\nabla\times\mathbf{B}$, I would tempted to believe that the components of $\nabla_r$ are ordinary derivatives of elementary multivariable /vector calculus , but Dirac's $\delta$, which is a tool of the theory of distributions, ""pops out"" at a certain point in the outline of proof , and in the theory of distributions there exist derivatives of distributions which are a very different thing, although they are taken, as far as I know, with respect to the variables written as ""variables of integration"" in the distribution integral notation, while, here, we start with $\nabla_r\times \mathbf{B}$ with $r$, while the integral is $\iiint_V d^3l$ with $l$. As to the integral sign, I would tend to interpretate it as the Lebesgue integral $$\int_V\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,d\mu_{\mathbf{l}}$$ or as the limit of a Riemann integral (where $B(\mathbf{r},\varepsilon)$ is the ball centred in $\mathbf{r}$ with radius $\varepsilon$) $$\lim_{\varepsilon\to 0}\iiint_{V\setminus B(\mathbf{r},\varepsilon)}\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,dl_1dl_2dl_3$$but, then, if the commutations between integral and differential operators used Wikipedia's outline of proof were licit for this interpretation, we would then conclude that $\iiint_V\,d^3l\, \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)$ $=\int_V \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}$ $=\int_{V\setminus{\{\mathbf{r}}\}} \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}=\mathbf{0}$ even where $\mathbf{J}(\mathbf{r})\ne\mathbf{0}$, which must not be the case.","I have been struggling to understand the only derivation of Ampère's law from the Biot-Savart law for a tridimensional distribution of current (which, needless to say, is not the case of a linear distribution of current discussed here ) that I have been able to find, i.e. Wikipedia's outline of proof , for more than a month with no result. I was not excluding that Wikipedia's outline of proof is one of those cases, whose set I have been told to be non-empy, where physics, at least at some level, renounces the rigour of mathematics, until I was told that the proof is indeed rigourous . Although I have recently asked for an alternative proof of the same entailment, the fact that it is not to be excluded that Wikipedia's outline of proof is rigourous has made me decide, as it has been suggested to me in these comments , to ask for an explanation of the single steps that I do not understand of that outline of proof . My first doubt starts at the first commutation between the integral and curl signs. I know that $$\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\left(\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}\right)$$provided that we read the components of $\nabla_r$ as ordinary derivatives (see below) in the sense of elementary multivariable calculus. The outline of proof says that $$\mathbf{B}(\mathbf{r}):=\iiint_V\,d^3l\,\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}=\nabla_r\times\iiint_V\,d^3l\,\frac{\mathbf{J}(\mathbf{l})}{|\mathbf{r}-\mathbf{l}|}.$$What mathematical result justifies that commutation between integral and curl? I heartily thank you for any answer. In order to take the problem on, it is obviously needed to understand what the integral and the derivatives in $\nabla$ are , but I am not sure about what they are. Since theorems such as Stokes' are usually applied when integrating $\nabla\times\mathbf{B}$, I would tempted to believe that the components of $\nabla_r$ are ordinary derivatives of elementary multivariable /vector calculus , but Dirac's $\delta$, which is a tool of the theory of distributions, ""pops out"" at a certain point in the outline of proof , and in the theory of distributions there exist derivatives of distributions which are a very different thing, although they are taken, as far as I know, with respect to the variables written as ""variables of integration"" in the distribution integral notation, while, here, we start with $\nabla_r\times \mathbf{B}$ with $r$, while the integral is $\iiint_V d^3l$ with $l$. As to the integral sign, I would tend to interpretate it as the Lebesgue integral $$\int_V\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,d\mu_{\mathbf{l}}$$ or as the limit of a Riemann integral (where $B(\mathbf{r},\varepsilon)$ is the ball centred in $\mathbf{r}$ with radius $\varepsilon$) $$\lim_{\varepsilon\to 0}\iiint_{V\setminus B(\mathbf{r},\varepsilon)}\mathbf{J}(\mathbf{l})\times\frac{\mathbf{r}-\mathbf{l}}{|\mathbf{r}-\mathbf{l}|^3}\,dl_1dl_2dl_3$$but, then, if the commutations between integral and differential operators used Wikipedia's outline of proof were licit for this interpretation, we would then conclude that $\iiint_V\,d^3l\, \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)$ $=\int_V \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}$ $=\int_{V\setminus{\{\mathbf{r}}\}} \mathbf{J}(\mathbf{l})\nabla_l^2\left(\frac{1}{|\mathbf{r}-\mathbf{l}|}\right)\,d\mu_{\mathbf{l}}=\mathbf{0}$ even where $\mathbf{J}(\mathbf{r})\ne\mathbf{0}$, which must not be the case.",,"['real-analysis', 'multivariable-calculus', 'physics', 'vector-analysis', 'distribution-theory']"
26,"$f \in L^1 ((0,1))$, decreasing on $(0,1)$ implies $x f(x)\rightarrow 0$ as $x \rightarrow 0$",", decreasing on  implies  as","f \in L^1 ((0,1)) (0,1) x f(x)\rightarrow 0 x \rightarrow 0","Suppose $f\in L^1((0,1),dm)$. Suppose $f$ is also a decreasing function on $(0,1)$. Prove that $$\lim _\limits{x \to 0} xf(x) = 0.$$ My attempt at a solution We may assume without loss of generality that $f$ is positive, and unbounded near zero; thus we can write $$\infty >\int_0^1 f\,dm=\sum_{n=1}^{\infty} \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm.$$ Then $$ \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm \rightarrow 0 \quad \text{as}\quad n\rightarrow \infty. $$ Also since $f$ is decreasing, we have $$f \left(\frac{1}{n}\right)\left( \frac 1n - \frac{1}{n+1}\right) \leq \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm \leq f \left(\frac{1}{n+1}\right)\left( \frac 1n - \frac{1}{n+1}\right).$$ I want to say $$ \lim _\limits{x \to 0} xf(x) = \lim _\limits{n \to \infty} \frac 1n f \left( \frac 1n \right) = 0,$$ But that's not exactly what I have in my inequalities. Added as pointed out by @Hetebrij below, Note that by showing $\lim _\limits{n \to \infty} \frac1n f\left( \frac 1n  \right) =0$ you have not shown $\lim_\limits{x\to 0}xf(x)=0$ unless you know   (1) the limit exist or (2) $xf(x)$ is decreasing or increasing. Any hints or suggestions? Would contradiction be better suited for this problem? UPDATE Another attempt: Take any $n>1$. Then $$\frac {1}{2n}f \left(\frac 1n\right) = \left(\frac 1n-\frac {1}{2n}\right)f \left(\frac 1n\right) \leq \int_{[0,1]}f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)dx.$$ Now the sequence of functions $$f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)$$ is dominated by the integrable function $f$, and converges to $\infty\cdot \chi_{\{0\}}$. Thus by the Lebesgue Dominated Convergence Theorem, we have $$\int_{[0,1]}f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)dx \rightarrow 0.$$","Suppose $f\in L^1((0,1),dm)$. Suppose $f$ is also a decreasing function on $(0,1)$. Prove that $$\lim _\limits{x \to 0} xf(x) = 0.$$ My attempt at a solution We may assume without loss of generality that $f$ is positive, and unbounded near zero; thus we can write $$\infty >\int_0^1 f\,dm=\sum_{n=1}^{\infty} \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm.$$ Then $$ \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm \rightarrow 0 \quad \text{as}\quad n\rightarrow \infty. $$ Also since $f$ is decreasing, we have $$f \left(\frac{1}{n}\right)\left( \frac 1n - \frac{1}{n+1}\right) \leq \int_{\frac{1}{n+1}}^{\frac 1n}f\,dm \leq f \left(\frac{1}{n+1}\right)\left( \frac 1n - \frac{1}{n+1}\right).$$ I want to say $$ \lim _\limits{x \to 0} xf(x) = \lim _\limits{n \to \infty} \frac 1n f \left( \frac 1n \right) = 0,$$ But that's not exactly what I have in my inequalities. Added as pointed out by @Hetebrij below, Note that by showing $\lim _\limits{n \to \infty} \frac1n f\left( \frac 1n  \right) =0$ you have not shown $\lim_\limits{x\to 0}xf(x)=0$ unless you know   (1) the limit exist or (2) $xf(x)$ is decreasing or increasing. Any hints or suggestions? Would contradiction be better suited for this problem? UPDATE Another attempt: Take any $n>1$. Then $$\frac {1}{2n}f \left(\frac 1n\right) = \left(\frac 1n-\frac {1}{2n}\right)f \left(\frac 1n\right) \leq \int_{[0,1]}f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)dx.$$ Now the sequence of functions $$f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)$$ is dominated by the integrable function $f$, and converges to $\infty\cdot \chi_{\{0\}}$. Thus by the Lebesgue Dominated Convergence Theorem, we have $$\int_{[0,1]}f(x)\chi_{\left[\frac{1}{2n},\frac 1n\right]}(x)dx \rightarrow 0.$$",,"['real-analysis', 'integration', 'measure-theory', 'convergence-divergence', 'lebesgue-measure']"
27,"Show that this integral converges,","Show that this integral converges,",,"Show that $$\int_1^{\infty}\int_1^{\infty}\cdots\int_1^{\infty} \frac{{\rm d}x_1\cdots {\rm d}x_n}{x_1^{\alpha_1} + \ldots + x_n^{\alpha_n}} < \infty$$ if $\frac{1}{\alpha_1} + \ldots + \frac{1}{\alpha_n} < 1 $ I'm guessing that, to make use of the inequality assumption given in the problem statement, I should look to make a change of variables. Any ideas are welcome. Thanks!","Show that $$\int_1^{\infty}\int_1^{\infty}\cdots\int_1^{\infty} \frac{{\rm d}x_1\cdots {\rm d}x_n}{x_1^{\alpha_1} + \ldots + x_n^{\alpha_n}} < \infty$$ if $\frac{1}{\alpha_1} + \ldots + \frac{1}{\alpha_n} < 1 $ I'm guessing that, to make use of the inequality assumption given in the problem statement, I should look to make a change of variables. Any ideas are welcome. Thanks!",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'convergence-divergence']"
28,Changing order of sum,Changing order of sum,,We know that  $$\sum_{n=1}^{\infty}a_n = \mbox{convergent}$$ Does that imply that  $$a_3 + a_1 + a_2 + a_6 + a_4 + a_5 + \cdots = \mbox{convergent?}$$ We don't know whether our series is absolutely convergent.,We know that  $$\sum_{n=1}^{\infty}a_n = \mbox{convergent}$$ Does that imply that  $$a_3 + a_1 + a_2 + a_6 + a_4 + a_5 + \cdots = \mbox{convergent?}$$ We don't know whether our series is absolutely convergent.,,"['real-analysis', 'sequences-and-series', 'analysis']"
29,Continuous function taking each of its values twice,Continuous function taking each of its values twice,,"I read that a continuous function can't take each of it's values exactly twice. But I don't understand why e.g. take the function $x^2$ and add one point to it very close to $0$ that also has $0$ value (and shift the rest of the function with the difference between the $x$-coordinate of the added point and $0$), then it takes each value twice. Or is it not continuous in this case? Also could you give me a function in R that takes each of its values three times?","I read that a continuous function can't take each of it's values exactly twice. But I don't understand why e.g. take the function $x^2$ and add one point to it very close to $0$ that also has $0$ value (and shift the rest of the function with the difference between the $x$-coordinate of the added point and $0$), then it takes each value twice. Or is it not continuous in this case? Also could you give me a function in R that takes each of its values three times?",,['real-analysis']
30,"Prove that if the sequence of increasing functions $(f_n)$ converges to $f$, then $f$ is increasing","Prove that if the sequence of increasing functions  converges to , then  is increasing",(f_n) f f,"Let $a,b\in \Bbb R$, $a<b$ and for $n=1,2,3,\ldots$ let $f_n\colon [a,b]\to \Bbb R$ be an increasing function (i.e. $f_n(x)\leq f_n(y)$ if $x\leq y$). Prove that if the sequence $f_1,f_2,f_3,\ldots$ converges to $f$ then $f$ is increasing, and that if $f$ is continuous then the convergence is uniform. Let $(f_n)$ be a sequence of increasing functions from $[a,b]$ to $\mathbb{R}$ and $f$ be its pointwise limit. Choose $x$ and $y$ such that $x\ge y$. There are $\epsilon>0$ and $N,M$ such that $|f_n(x)-f(x)|<\epsilon$ and $|f_m(y)-f(y)|<\epsilon$ whenever $n>N$ and $m>M$. If we choose $k>\min\{N,M\}$  then we have $$f_k(x)-f(x)<\epsilon$$ $$-f_k(y)+f(y)<\epsilon$$ thus $0\le f_k(x)-f_k(y)< f(x)-f(y)+2\epsilon$. Since we can make $\epsilon$ arbitrarly small, $f(x)\ge f(y)$. I find it difficult to prove the second part. Any hint please?","Let $a,b\in \Bbb R$, $a<b$ and for $n=1,2,3,\ldots$ let $f_n\colon [a,b]\to \Bbb R$ be an increasing function (i.e. $f_n(x)\leq f_n(y)$ if $x\leq y$). Prove that if the sequence $f_1,f_2,f_3,\ldots$ converges to $f$ then $f$ is increasing, and that if $f$ is continuous then the convergence is uniform. Let $(f_n)$ be a sequence of increasing functions from $[a,b]$ to $\mathbb{R}$ and $f$ be its pointwise limit. Choose $x$ and $y$ such that $x\ge y$. There are $\epsilon>0$ and $N,M$ such that $|f_n(x)-f(x)|<\epsilon$ and $|f_m(y)-f(y)|<\epsilon$ whenever $n>N$ and $m>M$. If we choose $k>\min\{N,M\}$  then we have $$f_k(x)-f(x)<\epsilon$$ $$-f_k(y)+f(y)<\epsilon$$ thus $0\le f_k(x)-f_k(y)< f(x)-f(y)+2\epsilon$. Since we can make $\epsilon$ arbitrarly small, $f(x)\ge f(y)$. I find it difficult to prove the second part. Any hint please?",,"['calculus', 'real-analysis', 'convergence-divergence', 'proof-verification', 'uniform-convergence']"
31,"Let $f(x,y)$ be a continuous function from $R^2$ to $R$. If $f(x,y)=0$, does $y$ vary continuously as $x$ varies?","Let  be a continuous function from  to . If , does  vary continuously as  varies?","f(x,y) R^2 R f(x,y)=0 y x","Let $f(x,y)$ be a continuous function from $R^2$ to $R$. Suppose further that for every $x$, there exists a unique $y$ such that $f(x,y)=0$. Thus we can define the function $g(x)=y$ such that $f(x,y)=0$. Is $g$ is continuous? Hello all, I am working on a project and I have found myself stuck on the question in the title. I am trying to find out if the roots of a continuous function $f(x,y)$ vary continuously as the argument $x$ is allowed to vary. I have a particular function $f$ in my project, but I cannot solve it for $x$ or $y$ so I am trying to do things in the abstract. We can suppose further that there exists a unique $y$ for every $x$ such that $f(x,y)=0$. Thus we can define the function $g(x)=y$ such that $f(x,y)=0$. I want to know if $g$ is continuous. From my attempts to solve this, it seems like the implicit function theorem might point the way, but I am not familiar with that result and it seems like it only gives a continuous function in a subset of the domain. I was hoping to find an elementary answer but having gone though my old topology and analysis textbooks I was not able to find anything helpful.","Let $f(x,y)$ be a continuous function from $R^2$ to $R$. Suppose further that for every $x$, there exists a unique $y$ such that $f(x,y)=0$. Thus we can define the function $g(x)=y$ such that $f(x,y)=0$. Is $g$ is continuous? Hello all, I am working on a project and I have found myself stuck on the question in the title. I am trying to find out if the roots of a continuous function $f(x,y)$ vary continuously as the argument $x$ is allowed to vary. I have a particular function $f$ in my project, but I cannot solve it for $x$ or $y$ so I am trying to do things in the abstract. We can suppose further that there exists a unique $y$ for every $x$ such that $f(x,y)=0$. Thus we can define the function $g(x)=y$ such that $f(x,y)=0$. I want to know if $g$ is continuous. From my attempts to solve this, it seems like the implicit function theorem might point the way, but I am not familiar with that result and it seems like it only gives a continuous function in a subset of the domain. I was hoping to find an elementary answer but having gone though my old topology and analysis textbooks I was not able to find anything helpful.",,"['real-analysis', 'general-topology']"
32,Let $f$ be integrable over $\mathbb{R}$. Show that the following four assertions are equivalent:,Let  be integrable over . Show that the following four assertions are equivalent:,f \mathbb{R},"Let $f$ be integrable over $\mathbb{R}$. Show that the following four assertions are equivalent: $f = 0$ a.e. on $\mathbb{R}$. $\int_{\mathbb{R}}fg=0$ for every   bounded measurable function $g$ on $\mathbb{R}$. $\int_A f = 0$ for    every measurable set $A$. $\int_{\mathcal{O}} f = 0$ for every open       set $\mathcal{O}$. $(1) \Rightarrow (2)$ If $f = 0$ a.e. on $\mathbb{R} $, then $m(E_0) = 0$, where $$E_0 =\{x \in \mathbb{R} : f(x) \neq 0\}.$$ Now, by the previous exercise, we shown that for $g: \mathbb{R} \rightarrow \mathbb{R}$ being bounded and measurable, then $fg$ is integrable and $$\int_{\mathbb{R}}fg = \int_{\mathbb{R}\sim E_0}fg + \int_{E_0}fg,$$ but since $$f = 0 \text{ a.e  and } m(E_0) = 0,$$ we have $$\int_{\mathbb{R}\sim E_0}fg =0,$$ and $$\int_{E_0}fg = 0,$$ thus, $$\int_{\mathbb{R}}fg = 0,$$ as required. $(2) \Rightarrow (3)$ For a measurable set $A$, let $g = \chi_A$. Clearly, $g$ is measurable and bounded. Therefore, $$0= \int_{\mathbb{R}}fg =  \int_{\mathbb{R}}f \chi_A =  \int_A f = 0,$$ as required. $(3) \Rightarrow (4)$ Since every open set is measurable, then if $(3)$ holds, we have $$\int_{\mathcal{O}}f = 0 \text{ for every open set $\mathcal{O}$, }$$ as required. $(4) \Rightarrow (1)$ Let $\{U_n\}_{n \in \mathbb{N}}$ be a countable collection of open sets. Then, $$G = \bigcap_{n \in \mathbb{N}} \mathcal{O}_n,$$ is a $G_\delta$ set with $\mathcal{O}_n = \bigcap_{k \leq n}U_k,$ where each $U_k$ is an open set. Thus, $\{\mathcal{O}_n\}_{n \in \mathbb{N}}$ is a countable descending collection of open sets, i.e. $$\forall i \in \mathbb{N}: \mathcal{O}_i \supset \mathcal{O}_{i+1}.$$ Now, for $\mathcal{O}_n$ being open, we have $$\int_{\mathcal{O}_n} f = 0,$$ and by the continuity of integration,  $$\int_{G} f  =\int_{\bigcap_{n \in \mathbb{N}} \mathcal{O}_n} f = \lim_{n \to \infty} \int_{\mathcal{O}_n} f = 0.$$ Since every measurable set $E \subset \mathbb{R}$ is of the form $G \sim E_0$, where $G$ is a $G_{\delta}$ subset of $\mathbb{R}$ and $m(E_0) = 0$, so that  $$\int_{E} f = 0 \text{ for each measurable set } E \subset \mathbb{R}.$$ Now, define $$E^+ = \{x \in \mathbb{R}: f(x) \geq 0 \} \text{ and } E^- = \{x \in \mathbb{R}: f(x) \leq 0 \},$$ then $E^+$ and $E^-$ are measurable subsets of $\mathbb{R}$ and thus,  $$\int_{\mathbb{R}} f^+ = \int_{E^+} f = 0,$$ and $$\int_{\mathbb{R}} (-f^-) = -\int_{E^-} f = 0,$$ where $f^+ = \max\{f,0\}$ and $f^- = \max\{-f,0\}$. Finally we use the fact (proposition $9$ chapter $4$) that $$\int_E f = 0 \text{ if and only if } f = 0 \text{ a.e}$$ so that $f^+$ and $f^-$ vanishes almost everywhere in $\mathbb{R}$ and so does $f$ as required.","Let $f$ be integrable over $\mathbb{R}$. Show that the following four assertions are equivalent: $f = 0$ a.e. on $\mathbb{R}$. $\int_{\mathbb{R}}fg=0$ for every   bounded measurable function $g$ on $\mathbb{R}$. $\int_A f = 0$ for    every measurable set $A$. $\int_{\mathcal{O}} f = 0$ for every open       set $\mathcal{O}$. $(1) \Rightarrow (2)$ If $f = 0$ a.e. on $\mathbb{R} $, then $m(E_0) = 0$, where $$E_0 =\{x \in \mathbb{R} : f(x) \neq 0\}.$$ Now, by the previous exercise, we shown that for $g: \mathbb{R} \rightarrow \mathbb{R}$ being bounded and measurable, then $fg$ is integrable and $$\int_{\mathbb{R}}fg = \int_{\mathbb{R}\sim E_0}fg + \int_{E_0}fg,$$ but since $$f = 0 \text{ a.e  and } m(E_0) = 0,$$ we have $$\int_{\mathbb{R}\sim E_0}fg =0,$$ and $$\int_{E_0}fg = 0,$$ thus, $$\int_{\mathbb{R}}fg = 0,$$ as required. $(2) \Rightarrow (3)$ For a measurable set $A$, let $g = \chi_A$. Clearly, $g$ is measurable and bounded. Therefore, $$0= \int_{\mathbb{R}}fg =  \int_{\mathbb{R}}f \chi_A =  \int_A f = 0,$$ as required. $(3) \Rightarrow (4)$ Since every open set is measurable, then if $(3)$ holds, we have $$\int_{\mathcal{O}}f = 0 \text{ for every open set $\mathcal{O}$, }$$ as required. $(4) \Rightarrow (1)$ Let $\{U_n\}_{n \in \mathbb{N}}$ be a countable collection of open sets. Then, $$G = \bigcap_{n \in \mathbb{N}} \mathcal{O}_n,$$ is a $G_\delta$ set with $\mathcal{O}_n = \bigcap_{k \leq n}U_k,$ where each $U_k$ is an open set. Thus, $\{\mathcal{O}_n\}_{n \in \mathbb{N}}$ is a countable descending collection of open sets, i.e. $$\forall i \in \mathbb{N}: \mathcal{O}_i \supset \mathcal{O}_{i+1}.$$ Now, for $\mathcal{O}_n$ being open, we have $$\int_{\mathcal{O}_n} f = 0,$$ and by the continuity of integration,  $$\int_{G} f  =\int_{\bigcap_{n \in \mathbb{N}} \mathcal{O}_n} f = \lim_{n \to \infty} \int_{\mathcal{O}_n} f = 0.$$ Since every measurable set $E \subset \mathbb{R}$ is of the form $G \sim E_0$, where $G$ is a $G_{\delta}$ subset of $\mathbb{R}$ and $m(E_0) = 0$, so that  $$\int_{E} f = 0 \text{ for each measurable set } E \subset \mathbb{R}.$$ Now, define $$E^+ = \{x \in \mathbb{R}: f(x) \geq 0 \} \text{ and } E^- = \{x \in \mathbb{R}: f(x) \leq 0 \},$$ then $E^+$ and $E^-$ are measurable subsets of $\mathbb{R}$ and thus,  $$\int_{\mathbb{R}} f^+ = \int_{E^+} f = 0,$$ and $$\int_{\mathbb{R}} (-f^-) = -\int_{E^-} f = 0,$$ where $f^+ = \max\{f,0\}$ and $f^- = \max\{-f,0\}$. Finally we use the fact (proposition $9$ chapter $4$) that $$\int_E f = 0 \text{ if and only if } f = 0 \text{ a.e}$$ so that $f^+$ and $f^-$ vanishes almost everywhere in $\mathbb{R}$ and so does $f$ as required.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
33,"Give an example of a function which is in $L^2 (\mathbb{R})$ but not in $L^p(\mathbb{R})$ for any $p \in [1, 2) \cup (2, \infty]$.",Give an example of a function which is in  but not in  for any .,"L^2 (\mathbb{R}) L^p(\mathbb{R}) p \in [1, 2) \cup (2, \infty]","This question was on a problem set regarding $L^p$ spaces in an undergraduate-level real analysis course. I actually used an answer on StackExchange to help me provide an example, but I couldn't provide adequate justification for why it works. The answer I used is linked here . I modified this function a little to make it so that it only converges if $p = 2$, namely I defined my function as follows: $$f(x)=\frac{1}{x^{2/p} \ln^2(x^{2/p})}$$ But I can't really explain why it works (and to be honest, I'm not really sure it does work). My professor hinted in class that there is a solution that involves a continuous piecewise function agreeing at $x=1$ that is basically a generalization of the observation that for the following functions: $$g(x)=\begin{cases} \frac{1}{\sqrt{x}} & 0<x<1 \\ 0 & \text{otherwise}\end{cases}$$ $$h(x)=\begin{cases} \frac{1}{x} & x>1 \\ 0 & \text{otherwise}\end{cases}$$ we can easily show that $g \in L^1(\mathbb{R}), \, g \not\in L^2(\mathbb{R})$ and $h \in L^2(\mathbb{R}), \, h \not\in L^1(\mathbb{R})$. So I would imagine that the solution based on this hint would involve some sort of clever manipulation of powers to incorporate all $p \in [1, 2) \cup (2, \infty]$ somehow, but I have no idea how to do that. I'm pretty lost with this problem. What is the best way to go about it?","This question was on a problem set regarding $L^p$ spaces in an undergraduate-level real analysis course. I actually used an answer on StackExchange to help me provide an example, but I couldn't provide adequate justification for why it works. The answer I used is linked here . I modified this function a little to make it so that it only converges if $p = 2$, namely I defined my function as follows: $$f(x)=\frac{1}{x^{2/p} \ln^2(x^{2/p})}$$ But I can't really explain why it works (and to be honest, I'm not really sure it does work). My professor hinted in class that there is a solution that involves a continuous piecewise function agreeing at $x=1$ that is basically a generalization of the observation that for the following functions: $$g(x)=\begin{cases} \frac{1}{\sqrt{x}} & 0<x<1 \\ 0 & \text{otherwise}\end{cases}$$ $$h(x)=\begin{cases} \frac{1}{x} & x>1 \\ 0 & \text{otherwise}\end{cases}$$ we can easily show that $g \in L^1(\mathbb{R}), \, g \not\in L^2(\mathbb{R})$ and $h \in L^2(\mathbb{R}), \, h \not\in L^1(\mathbb{R})$. So I would imagine that the solution based on this hint would involve some sort of clever manipulation of powers to incorporate all $p \in [1, 2) \cup (2, \infty]$ somehow, but I have no idea how to do that. I'm pretty lost with this problem. What is the best way to go about it?",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
34,Exercise 3.9 in rudin's real and complex analysis,Exercise 3.9 in rudin's real and complex analysis,,"Problem Suppose $f$ is Lebesgue measurable on $(0,1)$ and not essentially bounded. By Exercise $4(e)$ $\|f\|_{p}\to \infty $ as $p\to \infty $.Can $\|f\|_{p}$ tend to $\infty $ arbitrarily slowly? More precisely ,is it true that to every positive function $\Phi$ on $(0,\infty)$ such that $\Phi(p)\to \infty$ as $p\to \infty$ one can find an $f$ such that $\|f\|_{p}\to \infty$ as $p\to\infty$,but $\|f\|_{p}\leq \Phi(p)$ for all sufficiently large $p$ ? I don't have much idea about it, I try to build such $f$ relate to $\Phi$ but failed dut to $\|f\|_{p}=\infty $ Can someone help me ? Thank you in advance!","Problem Suppose $f$ is Lebesgue measurable on $(0,1)$ and not essentially bounded. By Exercise $4(e)$ $\|f\|_{p}\to \infty $ as $p\to \infty $.Can $\|f\|_{p}$ tend to $\infty $ arbitrarily slowly? More precisely ,is it true that to every positive function $\Phi$ on $(0,\infty)$ such that $\Phi(p)\to \infty$ as $p\to \infty$ one can find an $f$ such that $\|f\|_{p}\to \infty$ as $p\to\infty$,but $\|f\|_{p}\leq \Phi(p)$ for all sufficiently large $p$ ? I don't have much idea about it, I try to build such $f$ relate to $\Phi$ but failed dut to $\|f\|_{p}=\infty $ Can someone help me ? Thank you in advance!",,['real-analysis']
35,A characterisation of functions of $\mathbb{N} \rightarrow \mathbb{R}$ of the form $f(n) = \sum_k a_k k^n$,A characterisation of functions of  of the form,\mathbb{N} \rightarrow \mathbb{R} f(n) = \sum_k a_k k^n,"I'm looking for a characterisation of functions $f : \mathbb{N} \rightarrow \mathbb{R}$ for which there exists $(a_k) \in \mathbb{R}^\mathbb{N}$ such as $f(n) = \sum_k a_k k^n $ for all $n \in \mathbb{N}$. I don't think that every functions of $\mathbb{N} \rightarrow \mathbb{R}$ can be written like this. An easy calculus (using Vandermonde matrix) show that for $N \in \mathbb{N}$, one can find a function $f_N$ such as $f_N(N) = 1$ and $f_N(n) = 0$ if $n < N$. So for a given function $f$, we can find a function $g$ of the form we want such as $f(n) = g(n)$ for $n < N$, with of course no information after $N$ on $g$. I'm looking for a condition on $\lim f(n)$ as $n$ go to infinite for $f$ to be of the given form (by the result before, it's the only kind of condition we can put on $f$).","I'm looking for a characterisation of functions $f : \mathbb{N} \rightarrow \mathbb{R}$ for which there exists $(a_k) \in \mathbb{R}^\mathbb{N}$ such as $f(n) = \sum_k a_k k^n $ for all $n \in \mathbb{N}$. I don't think that every functions of $\mathbb{N} \rightarrow \mathbb{R}$ can be written like this. An easy calculus (using Vandermonde matrix) show that for $N \in \mathbb{N}$, one can find a function $f_N$ such as $f_N(N) = 1$ and $f_N(n) = 0$ if $n < N$. So for a given function $f$, we can find a function $g$ of the form we want such as $f(n) = g(n)$ for $n < N$, with of course no information after $N$ on $g$. I'm looking for a condition on $\lim f(n)$ as $n$ go to infinite for $f$ to be of the given form (by the result before, it's the only kind of condition we can put on $f$).",,"['real-analysis', 'sequences-and-series', 'power-series']"
36,"$\mathbb{Q} \cap [0, 1]$, infinite union and boundary, measure zero, not integrable on $[0, 1]$.",", infinite union and boundary, measure zero, not integrable on .","\mathbb{Q} \cap [0, 1] [0, 1]","I've considered the set $A = \mathbb{Q} \cap [0, 1]$ and showed that it has measure zero. In particular, I showed that it was countable, that is, I could write it as $$A = \{a_0, a_1, a_2, a_3, \dots\}.$$Given an $\epsilon > 0$, I then covered it with rectangles$$I_i = \left[ a_i - {\epsilon\over{2^{i+1}}}, a_i + {\epsilon\over{2^{i+1}}}\right],\text{ }\forall\,i \in \mathbb{N},$$so that$$v\left( \bigcup_{i=0}^\infty I_i\right) \le \sum_{i=0}^\infty v(I_i) = \epsilon.$$Fix $\epsilon = 1/2$, and let$$J_i = \left( a_i - {1\over{2^{i+3}}}, a_i + {1\over{2^{i+3}}}\right)$$be the open rectangle equal to the interior of the corresponding $I_i$ defined above. Let$$B = \bigcup_{i=2}^\infty J_i.$$Note that I am purposely omitting the first two sets, which cover $0$ and $1$, respectively, so that each $J_i \subset [0, 1]$. Show that $\partial B = [0, 1] \setminus B$. Show that $\partial B$ does not have measure zero. Let $\chi_B$ be the characteristic function of $B$. Show that $\chi_B$ is not Riemann integrable on $[0, 1]$.","I've considered the set $A = \mathbb{Q} \cap [0, 1]$ and showed that it has measure zero. In particular, I showed that it was countable, that is, I could write it as $$A = \{a_0, a_1, a_2, a_3, \dots\}.$$Given an $\epsilon > 0$, I then covered it with rectangles$$I_i = \left[ a_i - {\epsilon\over{2^{i+1}}}, a_i + {\epsilon\over{2^{i+1}}}\right],\text{ }\forall\,i \in \mathbb{N},$$so that$$v\left( \bigcup_{i=0}^\infty I_i\right) \le \sum_{i=0}^\infty v(I_i) = \epsilon.$$Fix $\epsilon = 1/2$, and let$$J_i = \left( a_i - {1\over{2^{i+3}}}, a_i + {1\over{2^{i+3}}}\right)$$be the open rectangle equal to the interior of the corresponding $I_i$ defined above. Let$$B = \bigcup_{i=2}^\infty J_i.$$Note that I am purposely omitting the first two sets, which cover $0$ and $1$, respectively, so that each $J_i \subset [0, 1]$. Show that $\partial B = [0, 1] \setminus B$. Show that $\partial B$ does not have measure zero. Let $\chi_B$ be the characteristic function of $B$. Show that $\chi_B$ is not Riemann integrable on $[0, 1]$.",,['real-analysis']
37,$|g(x)| \leq K \int_a^x|g| \ \ \forall x \in I$ [duplicate],[duplicate],|g(x)| \leq K \int_a^x|g| \ \ \forall x \in I,"This question already has answers here : Inequality of continuous functions (3 answers) Closed 8 years ago . Let $I:=[a,b]$ and let $g: I \to \Bbb R$ be continuous on $I$. Suppose that there exists $K > 0$ such that $$|g(x)| \leq K \int_a^x|g| \ \ \forall x \in I.$$  Then $g(x) = 0\ \ \forall x \in I $. I am stuck with the problem please help!","This question already has answers here : Inequality of continuous functions (3 answers) Closed 8 years ago . Let $I:=[a,b]$ and let $g: I \to \Bbb R$ be continuous on $I$. Suppose that there exists $K > 0$ such that $$|g(x)| \leq K \int_a^x|g| \ \ \forall x \in I.$$  Then $g(x) = 0\ \ \forall x \in I $. I am stuck with the problem please help!",,['calculus']
38,Well defined measure,Well defined measure,,"Let $X$ be a nonempty set and let $\mathcal{M}$ be the $\sigma$-algebra of countable subsets and cocountable subsets of $X$.  Let $\mu(A) = 0$ if $A$ is countable and 1 if $A$ is cocountable.  I want to prove that $\mu$ is a measure, but I'm worried that it might not be well defined since $X$ can be countable, and thus any subset of $X$ is both countable and cocountable.  Any ideas?","Let $X$ be a nonempty set and let $\mathcal{M}$ be the $\sigma$-algebra of countable subsets and cocountable subsets of $X$.  Let $\mu(A) = 0$ if $A$ is countable and 1 if $A$ is cocountable.  I want to prove that $\mu$ is a measure, but I'm worried that it might not be well defined since $X$ can be countable, and thus any subset of $X$ is both countable and cocountable.  Any ideas?",,['real-analysis']
39,"Accumulation points of $\{ 2^{-n} + 5^{-m} : n,m \geq 1 \}$",Accumulation points of,"\{ 2^{-n} + 5^{-m} : n,m \geq 1 \}","Determine all of the accumulation points of the following sets in $\mathbb{R}^1$ and decide whether the sets are open or closed or neither. The set $X$ of all numbers of the form $2^{-n} + 5^{-m}$ where $m,n = 1,2,\ldots$ . Claim: The set of accumulation points is the following: $$\left\{\frac{1}{2^n}: n \in \mathbb{N} \right\} \cup \left\{\frac{1}{5^n}: n \in \mathbb{N}\right\} \cup \{0\}.$$ Let $n \in \Bbb{N}$ and $r>0$ , and consider the open neighbourhood $B\left(\frac{1}{2^n},r\right) = \left(\frac{1}{2^n} - r,\frac{1}{2^n} + r\right)$ . For the given $r$ we can find $m$ large enough such that $5^{-m} < r \implies \frac{1}{2^n} + \frac{1}{5^m} < \frac{1}{2^n} + r$ , and $\frac{1}{2^n} - \frac{1}{5^m} > \frac{1}{2^n} - r$ ; therefore $5^{-m} + 2^{-n} \in B\left(\frac{1}{2^n},r\right)$ . A similar argument shows that each element of $\{5^{-m}: m \in \mathbb{N}\} \cup \{0\}$ is an accumulation point of $X$ . Now consider $y \in X$ . $B(y,r)$ isn't a subset of $X$ since for all $r > 0$ , $B(y,r) \subset (\mathbb{R} - \mathbb{Q}) \not\subset \Bbb{Q}$ , hence $B(y,r) \not\subset X$ .","Determine all of the accumulation points of the following sets in and decide whether the sets are open or closed or neither. The set of all numbers of the form where . Claim: The set of accumulation points is the following: Let and , and consider the open neighbourhood . For the given we can find large enough such that , and ; therefore . A similar argument shows that each element of is an accumulation point of . Now consider . isn't a subset of since for all , , hence .","\mathbb{R}^1 X 2^{-n} + 5^{-m} m,n = 1,2,\ldots \left\{\frac{1}{2^n}: n \in \mathbb{N} \right\} \cup \left\{\frac{1}{5^n}: n \in \mathbb{N}\right\} \cup \{0\}. n \in \Bbb{N} r>0 B\left(\frac{1}{2^n},r\right) = \left(\frac{1}{2^n} - r,\frac{1}{2^n} + r\right) r m 5^{-m} < r \implies \frac{1}{2^n} + \frac{1}{5^m} < \frac{1}{2^n} + r \frac{1}{2^n} - \frac{1}{5^m} > \frac{1}{2^n} - r 5^{-m} + 2^{-n} \in B\left(\frac{1}{2^n},r\right) \{5^{-m}: m \in \mathbb{N}\} \cup \{0\} X y \in X B(y,r) X r > 0 B(y,r) \subset (\mathbb{R} - \mathbb{Q}) \not\subset \Bbb{Q} B(y,r) \not\subset X",['real-analysis']
40,Matrix with non-negative eigenvalues (and additional assumption),Matrix with non-negative eigenvalues (and additional assumption),,"Let $A \in \mathbb{R}^{n \times n}$ be positive semi-definite ($A = A^\top \succcurlyeq 0$) and with positive diagonal elements ($A_{i,i} > 0$ for all $i$). Assume that both the column and the row sum of $A$ is $0$, i.e., for all $i$, $\sum_{j} A_{i,j} = 0$, and for all $j$, $\sum_{i} A_{i,j} = 0$. Also assume that $A$ has exactly one eigenvalue equal to $0$. Let $b, c \in \mathbb{R}^n$ be element-wise positive and non-negative vectors respectively, i.e., $b > 0$ and $c \geq 0$. Assume that $c^\top b > 0$. Prove that the $(n+1)$-dimensional matrix $$ \left[ \begin{matrix} A & b \\ c^\top A & c^\top b\end{matrix} \right]$$ has eigenvalues with non-negative real part. Comments. Numerics show that the claim is true. In here it is shown that $A \succcurlyeq 0$ is not enough for the claim to be true, thus the additional assumption on $A$ would be needed.","Let $A \in \mathbb{R}^{n \times n}$ be positive semi-definite ($A = A^\top \succcurlyeq 0$) and with positive diagonal elements ($A_{i,i} > 0$ for all $i$). Assume that both the column and the row sum of $A$ is $0$, i.e., for all $i$, $\sum_{j} A_{i,j} = 0$, and for all $j$, $\sum_{i} A_{i,j} = 0$. Also assume that $A$ has exactly one eigenvalue equal to $0$. Let $b, c \in \mathbb{R}^n$ be element-wise positive and non-negative vectors respectively, i.e., $b > 0$ and $c \geq 0$. Assume that $c^\top b > 0$. Prove that the $(n+1)$-dimensional matrix $$ \left[ \begin{matrix} A & b \\ c^\top A & c^\top b\end{matrix} \right]$$ has eigenvalues with non-negative real part. Comments. Numerics show that the claim is true. In here it is shown that $A \succcurlyeq 0$ is not enough for the claim to be true, thus the additional assumption on $A$ would be needed.",,"['real-analysis', 'linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
41,"If $E \subset\mathbb R$ is compact and nonempty. Prove $\sup (E)$, $\inf (E) \in E$","If  is compact and nonempty. Prove ,",E \subset\mathbb R \sup (E) \inf (E) \in E,"Suppose that $E \subset \mathbb R$ is compact and nonempty. Prove $\sup (E)$, $\inf (E)\in E$. attempt: Suppose $E$ is compact, then $E$ is closed and bounded. Thus $\sup(E)$ and $\inf (E)$ exist.  let $a = \sup(E)$, then there is a sequence $x_n \in E$ such that $x_n \to a$. Since $E$ is closed, then $a \in E$. Thus $\sup (E) \in E$. Is this correct? Any feedback would really help. Thank you.","Suppose that $E \subset \mathbb R$ is compact and nonempty. Prove $\sup (E)$, $\inf (E)\in E$. attempt: Suppose $E$ is compact, then $E$ is closed and bounded. Thus $\sup(E)$ and $\inf (E)$ exist.  let $a = \sup(E)$, then there is a sequence $x_n \in E$ such that $x_n \to a$. Since $E$ is closed, then $a \in E$. Thus $\sup (E) \in E$. Is this correct? Any feedback would really help. Thank you.",,"['real-analysis', 'proof-verification', 'metric-spaces']"
42,Equivalence of 2 definitions of Differentiability,Equivalence of 2 definitions of Differentiability,,"Let $X,Y$ be Banach spaces. I would like to prove the equivalence of the following definitions of differentiability. Let $f:X\to Y$ and $a\in X$ There is a map $\Delta : X \to L(X,Y)$ continuous at $a$, s.t. $$f(x)=f(a)+\Delta(x)(x-a)$$ There is a map $D_af\in L(X,Y)$ s.t. $$\lim_{x\to a}\frac{f(x)-f(a)-D_af(x-a)}{\|x-a\|_X}=0$$ The implication 1 => 2 is easy by picking $D_af=\Delta(a)$. Im however stuck on the other direction, do I have to assume that $X,Y$ are finite dimensional? 1 => 2: Assume 1 holds, then $$\begin{align*}\frac{f(x)-f(a)-\Delta(a)(x-a)}{\|x-a\|_X}&=\frac{\Delta(x)(x-a)-\Delta(a)(x-a)}{\|x-a\|_x}\\ &=[\Delta(x)-\Delta(a)]\left(\frac{x-a}{\|x-a\|_X}\right)\to0\end{align*}$$ since  $\Delta$ is continuous at a. For 2=>1 I can do the 1-dimensional case. Then the map $D_af$ is just multiplication with the element $f'(a)$. If I let $$R(x):= \frac{f(x)-f(a)-D_af(x-a)}{x-a}$$ And define the map $\Delta(x)$ to be multiplication with the element $(f'(a) + R(x))$ (for $x\neq a$ and $D_af$ else) everything works out fine. So I thought the general finite dimensional case should work similar by defining $\Delta(x)$ to be the map given by the Jacobi Matrix of $D_af$ with every entry increased by $R(x):=\frac{\|f(x)-f(a)-D_af(x-a)\|_Y}{\|x-a\|_X}$. This surely gives continuity at $a$ but I don't see how the equality in 1 follows. For the infinite dimensional case I have no idea how to proceed.","Let $X,Y$ be Banach spaces. I would like to prove the equivalence of the following definitions of differentiability. Let $f:X\to Y$ and $a\in X$ There is a map $\Delta : X \to L(X,Y)$ continuous at $a$, s.t. $$f(x)=f(a)+\Delta(x)(x-a)$$ There is a map $D_af\in L(X,Y)$ s.t. $$\lim_{x\to a}\frac{f(x)-f(a)-D_af(x-a)}{\|x-a\|_X}=0$$ The implication 1 => 2 is easy by picking $D_af=\Delta(a)$. Im however stuck on the other direction, do I have to assume that $X,Y$ are finite dimensional? 1 => 2: Assume 1 holds, then $$\begin{align*}\frac{f(x)-f(a)-\Delta(a)(x-a)}{\|x-a\|_X}&=\frac{\Delta(x)(x-a)-\Delta(a)(x-a)}{\|x-a\|_x}\\ &=[\Delta(x)-\Delta(a)]\left(\frac{x-a}{\|x-a\|_X}\right)\to0\end{align*}$$ since  $\Delta$ is continuous at a. For 2=>1 I can do the 1-dimensional case. Then the map $D_af$ is just multiplication with the element $f'(a)$. If I let $$R(x):= \frac{f(x)-f(a)-D_af(x-a)}{x-a}$$ And define the map $\Delta(x)$ to be multiplication with the element $(f'(a) + R(x))$ (for $x\neq a$ and $D_af$ else) everything works out fine. So I thought the general finite dimensional case should work similar by defining $\Delta(x)$ to be the map given by the Jacobi Matrix of $D_af$ with every entry increased by $R(x):=\frac{\|f(x)-f(a)-D_af(x-a)\|_Y}{\|x-a\|_X}$. This surely gives continuity at $a$ but I don't see how the equality in 1 follows. For the infinite dimensional case I have no idea how to proceed.",,"['real-analysis', 'multivariable-calculus']"
43,"For what values of t, the solution for this equation exist","For what values of t, the solution for this equation exist",,"I need help in finding maximal solution for the problem:  $$ \cases {{\dot{x} = x^2+t}\\{x(0)=0}}$$ I know that because $x(1) \geq \frac{1}{2}$ and that every solution $x(t)$ of the problem is greater than or equal to $\tan (t+x(1) - 1)$ gives that $$\displaystyle \lim_{t \rightarrow \frac{\pi +1}{2}}x(t) = \infty$$ Is this enough or can I tighten this? What do I do with negative $t$'s ? (I also tried to bound the absolute value of the integral equation and use Gronwell inequality i.e: $$\lvert x \left( t\right)\rvert = \rvert \int_{0}^{t}x^2+t\ dt\lvert < something$$ as well as $$|\int_0 ^t \frac{dx}{x^2}| = |\int_0 ^ t 1+\frac{t}{x^2}dt|< anything$$  but no success.) An other attempt with Pickard iterations: $\varphi_{0} = 0$ and $\varphi_{n} = \int_0 ^t \varphi^2_{n-1}(s) + s\ ds$, after some calculations, (and I need help in verifying correctness) I got  $$\varphi_n(t) = \sum _{i=0}^n \frac{t^{a_i}}{a_i \cdot {a_{i-1}^2 \cdot... \cdot {a_{0}^{\log_2{i}}}}} = \frac{t^{a_n}}{a_n \cdot a_{n-1}^2 \cdot ... \cdot a_{0}^{\log n}} + O( \frac{t^{a_{n-1}}}{{a_{n-1}}^{n-1}} )$$ When $a_n = 3\cdot 2^{n-1}-1$ By ratio test - the series diverges when t>1, but $ \frac{1}{2} \leq x(1) \leq \tan(1)$ (previous lines) gives that the solution can be extended for $t$'s larger than 1. What is going on Here?","I need help in finding maximal solution for the problem:  $$ \cases {{\dot{x} = x^2+t}\\{x(0)=0}}$$ I know that because $x(1) \geq \frac{1}{2}$ and that every solution $x(t)$ of the problem is greater than or equal to $\tan (t+x(1) - 1)$ gives that $$\displaystyle \lim_{t \rightarrow \frac{\pi +1}{2}}x(t) = \infty$$ Is this enough or can I tighten this? What do I do with negative $t$'s ? (I also tried to bound the absolute value of the integral equation and use Gronwell inequality i.e: $$\lvert x \left( t\right)\rvert = \rvert \int_{0}^{t}x^2+t\ dt\lvert < something$$ as well as $$|\int_0 ^t \frac{dx}{x^2}| = |\int_0 ^ t 1+\frac{t}{x^2}dt|< anything$$  but no success.) An other attempt with Pickard iterations: $\varphi_{0} = 0$ and $\varphi_{n} = \int_0 ^t \varphi^2_{n-1}(s) + s\ ds$, after some calculations, (and I need help in verifying correctness) I got  $$\varphi_n(t) = \sum _{i=0}^n \frac{t^{a_i}}{a_i \cdot {a_{i-1}^2 \cdot... \cdot {a_{0}^{\log_2{i}}}}} = \frac{t^{a_n}}{a_n \cdot a_{n-1}^2 \cdot ... \cdot a_{0}^{\log n}} + O( \frac{t^{a_{n-1}}}{{a_{n-1}}^{n-1}} )$$ When $a_n = 3\cdot 2^{n-1}-1$ By ratio test - the series diverges when t>1, but $ \frac{1}{2} \leq x(1) \leq \tan(1)$ (previous lines) gives that the solution can be extended for $t$'s larger than 1. What is going on Here?",,"['real-analysis', 'ordinary-differential-equations', 'proof-verification']"
44,Generalization of Minkowski inequality,Generalization of Minkowski inequality,,"I am wondering if the following is true: Suppose continuous function $g: [0, \infty) \to [0, \infty)$ satisfying $g(0)=0$ is increasing and strictly convex and (therefore) invertible. Let $||f ||^g$ denote $g^{-1}(\int_X g \circ |f| d \mu)$ where $f$ is measurable for measurable space $(X, M, \mu)$. Then for any measurable functions $f_1$ and $f_2$ we have $$ || f_1 +f_2||^g \le ||f_1||^g + ||f_2||^g$$ I think this is true, but am particularly interested if it is true for when $g(x) = (x+1)^p -1$ with $(X, M, \mu)$ being the usual Lebesgue measure space with $X=R^n$. Any insights, ideas, or references would be appreciated.","I am wondering if the following is true: Suppose continuous function $g: [0, \infty) \to [0, \infty)$ satisfying $g(0)=0$ is increasing and strictly convex and (therefore) invertible. Let $||f ||^g$ denote $g^{-1}(\int_X g \circ |f| d \mu)$ where $f$ is measurable for measurable space $(X, M, \mu)$. Then for any measurable functions $f_1$ and $f_2$ we have $$ || f_1 +f_2||^g \le ||f_1||^g + ||f_2||^g$$ I think this is true, but am particularly interested if it is true for when $g(x) = (x+1)^p -1$ with $(X, M, \mu)$ being the usual Lebesgue measure space with $X=R^n$. Any insights, ideas, or references would be appreciated.",,"['real-analysis', 'measure-theory', 'reference-request', 'lp-spaces']"
45,Topological Invariance of Unique Ergodicity,Topological Invariance of Unique Ergodicity,,"Show that unique ergodicity is a topological invariant. Is arguing as follows an overkill (hopefully if the logic is correct — I have a feeling that there has to be a way a $T$ -invariant measure has to depend on the $S$ -invariant measure and vice-versa and so that they are unique together)? Let $T$ be a uniquely ergodic transformation acting on the probability space $(X, \mathfrak{A}, \mu)$ and $S$ a transformation acting on $(Y, \mathfrak{B}, \nu)$ . Let $h: (X, \mathfrak{A}) \to (Y, \mathfrak{B})$ be a homeomorphism such that $h \circ T = S \circ h$ . Note that $T^{k}  = h^{-1} \circ S^{k} \circ h$ .  Suppose that $S$ is uniquely ergodic.  We wish to show that $T$ is uniquely ergodic. Let $\phi \in C(X)$ .By one equivalence characterizations of unique ergodicity,we must show that $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))$ converges to a constant pointwise on $X$ . Since $S$ is uniquely ergodic. We have in particular for a continuous function $\psi = \phi \circ h^{-1} \in C(Y)$ and each point $h(x) = y$ , the time average $\frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) )$ converges pointwise to a constant on $Y$ . But then so is $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))$ because $\frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))= \frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) )$ .","Show that unique ergodicity is a topological invariant. Is arguing as follows an overkill (hopefully if the logic is correct — I have a feeling that there has to be a way a -invariant measure has to depend on the -invariant measure and vice-versa and so that they are unique together)? Let be a uniquely ergodic transformation acting on the probability space and a transformation acting on . Let be a homeomorphism such that . Note that .  Suppose that is uniquely ergodic.  We wish to show that is uniquely ergodic. Let .By one equivalence characterizations of unique ergodicity,we must show that converges to a constant pointwise on . Since is uniquely ergodic. We have in particular for a continuous function and each point , the time average converges pointwise to a constant on . But then so is because .","T S T (X, \mathfrak{A}, \mu) S (Y, \mathfrak{B}, \nu) h: (X, \mathfrak{A}) \to (Y, \mathfrak{B}) h \circ T = S \circ h T^{k}  = h^{-1} \circ S^{k} \circ h S T \phi \in C(X) \frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x)) X S \psi = \phi \circ h^{-1} \in C(Y) h(x) = y \frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) ) Y \frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x)) \frac{1}{n}\sum_{k=0}^{n-1}\phi(T^{k}(x))= \frac{1}{n}\sum_{k=0}^{n-1} (\phi \circ h^{-1})(S^{k}(h(x)) )","['real-analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
46,"If $x_n \rightarrow 0$ and $\{y_n\}$ is a bounded sequence, then $x_ny_n \rightarrow 0$.","If  and  is a bounded sequence, then .",x_n \rightarrow 0 \{y_n\} x_ny_n \rightarrow 0,"Let $(x_n)$ be a sequence such that $x_n \to 0 $ and let $(y_n)$ be a sequence such that $(y_n)$ is bounded. Show that $(x_ny_n) \to 0 $ My try Since $(y_n)$ is bounded, we can find some $\alpha \in \mathbb{R}$ such that $|y_n| < \alpha $ for all $n$. Next, let $\epsilon > 0 $ be given and find some $N > 0$ such that $|x_n| < \frac{ \epsilon}{ \alpha } $ for all $n > N $. Notice, $$|x_ny_n| = |x_n||y_n| < \frac{ \epsilon}{\alpha} \cdot \alpha = \epsilon$$ To show that $(x_ny_n) \to 0 $, is it enough to take the same $N$ as before? so that $|x_ny_n| < \epsilon $ for all $n > N $","Let $(x_n)$ be a sequence such that $x_n \to 0 $ and let $(y_n)$ be a sequence such that $(y_n)$ is bounded. Show that $(x_ny_n) \to 0 $ My try Since $(y_n)$ is bounded, we can find some $\alpha \in \mathbb{R}$ such that $|y_n| < \alpha $ for all $n$. Next, let $\epsilon > 0 $ be given and find some $N > 0$ such that $|x_n| < \frac{ \epsilon}{ \alpha } $ for all $n > N $. Notice, $$|x_ny_n| = |x_n||y_n| < \frac{ \epsilon}{\alpha} \cdot \alpha = \epsilon$$ To show that $(x_ny_n) \to 0 $, is it enough to take the same $N$ as before? so that $|x_ny_n| < \epsilon $ for all $n > N $",,['real-analysis']
47,If $F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is continuous and $\| F(x) - F(y)\| \geq \lambda \| x - y \|$ is $F$ a surjection?,If  is continuous and  is  a surjection?,F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m} \| F(x) - F(y)\| \geq \lambda \| x - y \| F,"In my real analysis class my professor gave us the problem of proving that if $F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is continuous and satisfies $\| F(x) - F(y)\| \geq \lambda \| x - y \|$ then $F$ is a bijection with continuous inverse. ($∥⋅∥$ is the Eucliden norm and $\lambda$ is some positive real number.) The problem of injectivity is easy enough since if $x \neq y$ then $\|F(x) - F(y)\| \geq \lambda \|x-y\| > 0$. Also given that F is a continuous bijection then the continuity of the  inverse $g$ is also obvious since fixing $x = F(u)$ and $y = F(v)$ we have that $\|g(x) - g(y)\| = \|u-v\| \leq \frac{1}{\lambda}\|x-y\|$  so g is Lipschitz and therefore continuous. My question is, how exactly is one supposed to prove surjectivity? It seems easy enough by intermediate value theorem if we restrict $F:\mathbb{R} \rightarrow \mathbb{R}$. But I can't seem to figure it out more generally. any hints would be much appreciated!","In my real analysis class my professor gave us the problem of proving that if $F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is continuous and satisfies $\| F(x) - F(y)\| \geq \lambda \| x - y \|$ then $F$ is a bijection with continuous inverse. ($∥⋅∥$ is the Eucliden norm and $\lambda$ is some positive real number.) The problem of injectivity is easy enough since if $x \neq y$ then $\|F(x) - F(y)\| \geq \lambda \|x-y\| > 0$. Also given that F is a continuous bijection then the continuity of the  inverse $g$ is also obvious since fixing $x = F(u)$ and $y = F(v)$ we have that $\|g(x) - g(y)\| = \|u-v\| \leq \frac{1}{\lambda}\|x-y\|$  so g is Lipschitz and therefore continuous. My question is, how exactly is one supposed to prove surjectivity? It seems easy enough by intermediate value theorem if we restrict $F:\mathbb{R} \rightarrow \mathbb{R}$. But I can't seem to figure it out more generally. any hints would be much appreciated!",,[]
48,Continuous function with non-negative second derivative in the weak sense is convex,Continuous function with non-negative second derivative in the weak sense is convex,,"I am currently working through a section of Peter Petersen's Riemannian Geometry in which he talks about weak second derivatives of functions. I am trying to work through the details of why a function on a Riemannian manifold $(M,g)$ with non-negative Hessian in the weak sense is convex. I understand how one can reduce the problem to the case where $M$ is the real line with the Euclidean metric by precomposing with unit speed geodesics, so I have been able to reduce the problem to the following: Proposition: Let $f$ be a continuous function defined on an open interval in $\mathbf{R}$ . We say that $f''(p)\geq 0$ in the weak sense if for every $\varepsilon>0$ there exists a smooth function $f_\varepsilon$ defined in a neighborhood of $p$ such that (1) $f(p)=f_\varepsilon(p)$ (2) $f\geq f_\varepsilon$ (3) $f_\varepsilon''(p)\geq -\varepsilon$ . Then if $f''\geq 0$ everywhere in the weak sense, then $f$ is convex. I have tried for a while to come up with a direction to move in from here. My ideas keep running into the problem that the definition of $f_\varepsilon$ only necessarily has nice properties in a small neighborhood of $p$ . This makes the naive approach of trying to directly apply the definition of convexity more difficult (compared to when we assume our function is actually twice differentiable). Given $x_1$ , $x_2$ and $t\in(0,1)$ , for example, we cannot guarantee the function $f_\varepsilon$ defined around $p=tx_1+(1-t)x_2$ is even defined at $x_1$ or $x_2$ . I have a feeling I am over-thinking this problem. I would appreciate any input/hints anyone would be willing to provide as to how I might proceed from here.","I am currently working through a section of Peter Petersen's Riemannian Geometry in which he talks about weak second derivatives of functions. I am trying to work through the details of why a function on a Riemannian manifold with non-negative Hessian in the weak sense is convex. I understand how one can reduce the problem to the case where is the real line with the Euclidean metric by precomposing with unit speed geodesics, so I have been able to reduce the problem to the following: Proposition: Let be a continuous function defined on an open interval in . We say that in the weak sense if for every there exists a smooth function defined in a neighborhood of such that (1) (2) (3) . Then if everywhere in the weak sense, then is convex. I have tried for a while to come up with a direction to move in from here. My ideas keep running into the problem that the definition of only necessarily has nice properties in a small neighborhood of . This makes the naive approach of trying to directly apply the definition of convexity more difficult (compared to when we assume our function is actually twice differentiable). Given , and , for example, we cannot guarantee the function defined around is even defined at or . I have a feeling I am over-thinking this problem. I would appreciate any input/hints anyone would be willing to provide as to how I might proceed from here.","(M,g) M f \mathbf{R} f''(p)\geq 0 \varepsilon>0 f_\varepsilon p f(p)=f_\varepsilon(p) f\geq f_\varepsilon f_\varepsilon''(p)\geq -\varepsilon f''\geq 0 f f_\varepsilon p x_1 x_2 t\in(0,1) f_\varepsilon p=tx_1+(1-t)x_2 x_1 x_2","['real-analysis', 'riemannian-geometry']"
49,"Convergence of $n^{-\gamma}T$ where $T$ a hitting time for uniform rvs, can I use CLT?","Convergence of  where  a hitting time for uniform rvs, can I use CLT?",n^{-\gamma}T T,"Let $X_1,X_2,\dots$ be iid uniform on $\{1,\dots,n\}$ and define $T=\inf\{k:X_k=X_r \text{ for some }r<k\}$. The objective is to figure out when $n^{-\gamma} T$ converges weakly to some non-degenerate distribution. So $\gamma$ is a parameter that can be chosen? My work: $P(T=k)=\frac{n-1}{n}\times \cdots \times \frac{n-(k-2)}{n} \times \frac{k-1}{n}$ so then I constructed the characteristic function. The question then reduces to the following. As $n\rightarrow \infty$, what happens to $$\sum_{k=2}^{n+1} \left( \prod_{j=1}^{k-2}\frac{n-j}{n} \right)\frac{k-1}{n} e^{it\frac{k}{n^\gamma}}.$$ Just plugging this into wolfram, it looks complicated. Should I recognize this, or did I do something incorrectly? Edit: Is this actually a simple application of CLT? For any $n$, the mean and variance are finite, so this converges to some kind of normal?","Let $X_1,X_2,\dots$ be iid uniform on $\{1,\dots,n\}$ and define $T=\inf\{k:X_k=X_r \text{ for some }r<k\}$. The objective is to figure out when $n^{-\gamma} T$ converges weakly to some non-degenerate distribution. So $\gamma$ is a parameter that can be chosen? My work: $P(T=k)=\frac{n-1}{n}\times \cdots \times \frac{n-(k-2)}{n} \times \frac{k-1}{n}$ so then I constructed the characteristic function. The question then reduces to the following. As $n\rightarrow \infty$, what happens to $$\sum_{k=2}^{n+1} \left( \prod_{j=1}^{k-2}\frac{n-j}{n} \right)\frac{k-1}{n} e^{it\frac{k}{n^\gamma}}.$$ Just plugging this into wolfram, it looks complicated. Should I recognize this, or did I do something incorrectly? Edit: Is this actually a simple application of CLT? For any $n$, the mean and variance are finite, so this converges to some kind of normal?",,"['real-analysis', 'probability-theory', 'probability-distributions']"
50,Assume that $f$ is $2\pi$ continuous and $C^1$ such that $\int_{-\pi}^{\pi} f(x) dx=0$.,Assume that  is  continuous and  such that .,f 2\pi C^1 \int_{-\pi}^{\pi} f(x) dx=0,"Show that $\int_{-\pi}^{\pi} (f(x))^2 dx \leq \int_{-\pi}^{\pi} (f'(x))^2 dx$. So here's my approach to this question: Assume that $f$ was $2\pi$ continuous and $C^1$. Therefore, we have that $s_n(f)$ converges uniformly to $f$. So let  $f(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty} (a_n \cos (nx) + b_n \sin (nx))$. Since we are given $\int_{-\pi}^{\pi} f(x) dx = 0$, we have that $a_0 = 0$. Now we find $f'$ in terms of coefficients of $f$: $$f'(x) = \sum_{n = 1}^{\infty} (n b_n \cos(nx) - n a_n \sin (nx))$$ We have that all the eigenfunctions are orthogonal on $[-\pi, \pi]$ and  $\int_{-\pi}^{\pi} \sin^2(nx) dx = \pi = \int_{-\pi}^{\pi} \cos^2(nx) dx$ So by Parseval's identity, we have that: $$\int_{-\pi}^{\pi} (f(x))^2 dx = \pi \sum_{n=1}^{\infty} (a^2_n +b^2_n)$$ $$\int_{-\pi}^{\pi} (f'(x))^2 dx = \pi \sum_{n=1}^{\infty} n^2 (a^2_n +b^2_n)$$ It is clear that $n^2 \geq 1$ for all $n \in \mathbb{N}$, so we have that: $$\int_{-\pi}^{\pi} (f(x))^2 dx = \pi \sum_{n=1}^{\infty} (a^2_n +b^2_n)\leq  \pi \sum_{n=1}^{\infty} n^2 (a^2_n +b^2_n) = \int_{-\pi}^{\pi} (f'(x))^2 dx $$ Some reason I feel like there's something wrong with this. But I can't put my finger on it. Is there a better approach to this?","Show that $\int_{-\pi}^{\pi} (f(x))^2 dx \leq \int_{-\pi}^{\pi} (f'(x))^2 dx$. So here's my approach to this question: Assume that $f$ was $2\pi$ continuous and $C^1$. Therefore, we have that $s_n(f)$ converges uniformly to $f$. So let  $f(x) = \frac{a_0}{2} + \sum_{n = 1}^{\infty} (a_n \cos (nx) + b_n \sin (nx))$. Since we are given $\int_{-\pi}^{\pi} f(x) dx = 0$, we have that $a_0 = 0$. Now we find $f'$ in terms of coefficients of $f$: $$f'(x) = \sum_{n = 1}^{\infty} (n b_n \cos(nx) - n a_n \sin (nx))$$ We have that all the eigenfunctions are orthogonal on $[-\pi, \pi]$ and  $\int_{-\pi}^{\pi} \sin^2(nx) dx = \pi = \int_{-\pi}^{\pi} \cos^2(nx) dx$ So by Parseval's identity, we have that: $$\int_{-\pi}^{\pi} (f(x))^2 dx = \pi \sum_{n=1}^{\infty} (a^2_n +b^2_n)$$ $$\int_{-\pi}^{\pi} (f'(x))^2 dx = \pi \sum_{n=1}^{\infty} n^2 (a^2_n +b^2_n)$$ It is clear that $n^2 \geq 1$ for all $n \in \mathbb{N}$, so we have that: $$\int_{-\pi}^{\pi} (f(x))^2 dx = \pi \sum_{n=1}^{\infty} (a^2_n +b^2_n)\leq  \pi \sum_{n=1}^{\infty} n^2 (a^2_n +b^2_n) = \int_{-\pi}^{\pi} (f'(x))^2 dx $$ Some reason I feel like there's something wrong with this. But I can't put my finger on it. Is there a better approach to this?",,"['real-analysis', 'fourier-series']"
51,"Every $\sigma-$finite measure is semifinite. $(X, \mathcal{M}, \mu)$ is a measure space.",Every finite measure is semifinite.  is a measure space.,"\sigma- (X, \mathcal{M}, \mu)","Definition 1: Say $X = \bigcup_{n=1}^{\infty} E_n $ where $E_n \in \mathcal{M}$ and $\mu( E_n ) < \infty $ for all $n$ , we call $\mu$ $\sigma$ -finite. More generally, if $E = \bigcup^{\infty} E_n $ where $E_n \in \mathcal{M}$ for all $n$ and $\mu(E_n) < \infty $ for all $n$ then $E$ is said to be $\sigma-$ finite for $\mu$ . Definition 2: If for each $E \in \mathcal{M}$ with $\mu(E) = \infty$ , there exists $F \in \mathcal{M}$ with $F \subset E $ and $0 < \mu(F) < \infty$ , then we cal $\mu$ semifinite. Problem: Every $\sigma-$ finite measure is semifinite. Attempt Let $\mu$ be a $\sigma-$ finite measure on $X$ . Take $E \in \mathcal{M}$ arbitrary with $\mu(E) = \infty $ . Write $$ E = \bigcup^{\infty} E_n \; \; \; \; E_n \in \mathcal{M} \; \; forall \; \; n $$ (Here is where I am not sure I am doing the problem correctly. Can I assume that I can write $E$ in such a form? ) Next, there is some $k$ such that $E_k \subset E $ . Hence by monotonicity, $$ 0 \leq \mu(E_k) < \mu(E) = \infty $$","Definition 1: Say where and for all , we call -finite. More generally, if where for all and for all then is said to be finite for . Definition 2: If for each with , there exists with and , then we cal semifinite. Problem: Every finite measure is semifinite. Attempt Let be a finite measure on . Take arbitrary with . Write (Here is where I am not sure I am doing the problem correctly. Can I assume that I can write in such a form? ) Next, there is some such that . Hence by monotonicity,",X = \bigcup_{n=1}^{\infty} E_n  E_n \in \mathcal{M} \mu( E_n ) < \infty  n \mu \sigma E = \bigcup^{\infty} E_n  E_n \in \mathcal{M} n \mu(E_n) < \infty  n E \sigma- \mu E \in \mathcal{M} \mu(E) = \infty F \in \mathcal{M} F \subset E  0 < \mu(F) < \infty \mu \sigma- \mu \sigma- X E \in \mathcal{M} \mu(E) = \infty   E = \bigcup^{\infty} E_n \; \; \; \; E_n \in \mathcal{M} \; \; forall \; \; n  E k E_k \subset E   0 \leq \mu(E_k) < \mu(E) = \infty ,"['real-analysis', 'measure-theory', 'proof-verification']"
52,"Bounded Measurable Functions on [0,1]^2","Bounded Measurable Functions on [0,1]^2",,"Suppose $f(x,y),g(x,y)$ are two functions on $[0,1]^2$ that are bounded and measurable, such that: $$ \int_0^1 f(x,u)g(y,u) du \leq 1 $$ for almost every $(x,y) \in [0,1]^2$. Show that $$\int_0^1 f(x,u)g(x,u) du \leq 1 $$ holds for almost every $x \in [0,1]$. Apparently there is a way to solve this question using martingales instead of conventional real analysis. But after trying for several hours I cannot seem to pick up on this question. If I can have some kind of general idea or hint to help me on the right track it would be most appreciated.","Suppose $f(x,y),g(x,y)$ are two functions on $[0,1]^2$ that are bounded and measurable, such that: $$ \int_0^1 f(x,u)g(y,u) du \leq 1 $$ for almost every $(x,y) \in [0,1]^2$. Show that $$\int_0^1 f(x,u)g(x,u) du \leq 1 $$ holds for almost every $x \in [0,1]$. Apparently there is a way to solve this question using martingales instead of conventional real analysis. But after trying for several hours I cannot seem to pick up on this question. If I can have some kind of general idea or hint to help me on the right track it would be most appreciated.",,"['real-analysis', 'probability-theory']"
53,$n$ linearly independent rows of Vandermonde matrix,linearly independent rows of Vandermonde matrix,n,"Consider the ""infinite"" Vandermonde matrix  $$ V (x_1, x_2, \ldots , x_n) = \begin{pmatrix}   1      & x_1    & x_1^2  & \cdots & x_1^{n-1} & x_1^n & x_1^{n+1} & \cdots \\   1      & x_2    & x_2^2  & \cdots & x_2^{n-1} & x_2^n & x_1^{n+1} & \cdots \\   1      & x_3    & x_3^2  & \cdots & x_3^{n-1} & x_3^n & x_1^{n+1} & \cdots \\   \vdots & \vdots & \vdots & \ddots & \vdots    & \vdots & \vdots & \cdots \\   1      & x_n    & x_n^2  & \cdots & x_n^{n-1} & x_n^n & x_n^{n+1} & \cdots \end{pmatrix} $$ with distinct $x_1, \dots, x_n$. It is well-known that if we pick the first $n$ columns, then they span the whole space. I was wondering, if the same holds true if one picks arbitrary (not necessary consecutive) $n$ columns in the above ""infinite"" Vandermonde matrix. I looked at simple examples and they suggest that this is true. Is there a nice way of proving this?","Consider the ""infinite"" Vandermonde matrix  $$ V (x_1, x_2, \ldots , x_n) = \begin{pmatrix}   1      & x_1    & x_1^2  & \cdots & x_1^{n-1} & x_1^n & x_1^{n+1} & \cdots \\   1      & x_2    & x_2^2  & \cdots & x_2^{n-1} & x_2^n & x_1^{n+1} & \cdots \\   1      & x_3    & x_3^2  & \cdots & x_3^{n-1} & x_3^n & x_1^{n+1} & \cdots \\   \vdots & \vdots & \vdots & \ddots & \vdots    & \vdots & \vdots & \cdots \\   1      & x_n    & x_n^2  & \cdots & x_n^{n-1} & x_n^n & x_n^{n+1} & \cdots \end{pmatrix} $$ with distinct $x_1, \dots, x_n$. It is well-known that if we pick the first $n$ columns, then they span the whole space. I was wondering, if the same holds true if one picks arbitrary (not necessary consecutive) $n$ columns in the above ""infinite"" Vandermonde matrix. I looked at simple examples and they suggest that this is true. Is there a nice way of proving this?",,"['real-analysis', 'linear-algebra']"
54,"Consider the sequence $f_n(x) = (\sin(πnx))^n , n = 1, 2, ...,$ on the interval $[0,1].$",Consider the sequence  on the interval,"f_n(x) = (\sin(πnx))^n , n = 1, 2, ..., [0,1].","Consider the sequence $f_n(x) = (\sin(πnx))^n , n = 1, 2, ...,$ on the interval $[0,1].$ Prove that for any $δ > 0$ there is a set $E ⊂ [0,1]$ with $m(E) > 1−δ,$ and a subsequence $f_{n_k} (x), k = 1, 2, 3...,$ such that $\lim_{k→∞} f_{n_k} (x) = 0$ for $x ∈ E.$ Not sure what to do.  Want to construct a set $E$ as above with $f_n$ going to zero in $L_1$ norm.  Then it would follow.","Consider the sequence $f_n(x) = (\sin(πnx))^n , n = 1, 2, ...,$ on the interval $[0,1].$ Prove that for any $δ > 0$ there is a set $E ⊂ [0,1]$ with $m(E) > 1−δ,$ and a subsequence $f_{n_k} (x), k = 1, 2, 3...,$ such that $\lim_{k→∞} f_{n_k} (x) = 0$ for $x ∈ E.$ Not sure what to do.  Want to construct a set $E$ as above with $f_n$ going to zero in $L_1$ norm.  Then it would follow.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
55,"Limit everywhere, limit function is continuous, specific proof.","Limit everywhere, limit function is continuous, specific proof.",,"Suppose $f:[a,b] \to R$ is a function such that $\lim_{t\to x} f(t) = g(x)$ exists $\forall x \in [a,b]$. It can be shown that $g(x)$ is a continuous function. I seem to remember that there was a proof using Riemann Integrability. Did anyone come across such a proof? If so, what was it?","Suppose $f:[a,b] \to R$ is a function such that $\lim_{t\to x} f(t) = g(x)$ exists $\forall x \in [a,b]$. It can be shown that $g(x)$ is a continuous function. I seem to remember that there was a proof using Riemann Integrability. Did anyone come across such a proof? If so, what was it?",,"['real-analysis', 'reference-request']"
56,The sequences $x_n$ and $y_n$ converges,The sequences  and  converges,x_n y_n,"Let $\quad2{x}_{n+1}=1+{y}^{2}_{n}，\quad 2{y}_{n+1}=2{x}_{n}-{x}^2_{n},\quad n\in\mathbb{N};\quad 0\leq {y}_{0}\leq \frac{1}{2}\leq {x}_{0}\leq 2.$ Prove that the sequences $ \begin{Bmatrix} {x}_{n}\end{Bmatrix},\begin{Bmatrix} {y}_{n}\end{Bmatrix} $ converges in $\mathbb{R}.$ Note that: Of course, there must be more than one way to solve this,but I want to use the Limit superior and limit inferior of the sequences $ \{{x}_{n}\}$ and $\{y_n\}$ to get the conclusion. Can someone help me to solve this question in my way of thinking?","Let $\quad2{x}_{n+1}=1+{y}^{2}_{n}，\quad 2{y}_{n+1}=2{x}_{n}-{x}^2_{n},\quad n\in\mathbb{N};\quad 0\leq {y}_{0}\leq \frac{1}{2}\leq {x}_{0}\leq 2.$ Prove that the sequences $ \begin{Bmatrix} {x}_{n}\end{Bmatrix},\begin{Bmatrix} {y}_{n}\end{Bmatrix} $ converges in $\mathbb{R}.$ Note that: Of course, there must be more than one way to solve this,but I want to use the Limit superior and limit inferior of the sequences $ \{{x}_{n}\}$ and $\{y_n\}$ to get the conclusion. Can someone help me to solve this question in my way of thinking?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
57,"Is $ L^{\infty} $ a direct limit or inverse limit of the directed system $ (L^p , i_{p}^q )_{p,q \in [1 , + \infty [ } $?",Is  a direct limit or inverse limit of the directed system ?," L^{\infty}   (L^p , i_{p}^q )_{p,q \in [1 , + \infty [ } ","Let $X$ be a finite measure space. Then, for any $ 1≤p<q≤+∞ $ : $ L^q(X,B,m)⊂L^p(X,B,m) $. I would like to know if the space $ L^{\infty} ( X , B , m ) $ is the direct limit or the inverse limit of the direct system or the inverse system $ ( L^p ( X , B , m ), i_{p}^q )_{p \in [1 , + \infty [ } $ with $ i_{p}^{q} : L^q ( X , B , m ) \to L^p ( X , B , m ) $ an embedding. Thanks a lot for your help.","Let $X$ be a finite measure space. Then, for any $ 1≤p<q≤+∞ $ : $ L^q(X,B,m)⊂L^p(X,B,m) $. I would like to know if the space $ L^{\infty} ( X , B , m ) $ is the direct limit or the inverse limit of the direct system or the inverse system $ ( L^p ( X , B , m ), i_{p}^q )_{p \in [1 , + \infty [ } $ with $ i_{p}^{q} : L^q ( X , B , m ) \to L^p ( X , B , m ) $ an embedding. Thanks a lot for your help.",,"['real-analysis', 'abstract-algebra', 'measure-theory', 'category-theory', 'lp-spaces']"
58,Is the function $f(x)=1^x=1$ considered an exponential function?,Is the function  considered an exponential function?,f(x)=1^x=1,"I am confused about the following: The exponential function (by definition) is a function of the form $f(x)=a^x$ where $a>0$. However, when $a=1$, we get the constant function $f(x)=1^x=1$. Is the constant function $f(x)=1^x=1$ still considered an exponential function even though it does not have behave like an exponential function? Is the definition of the exponential function that I gave above (that I read in many textbooks) not entirely correct? Should we define the exponential function by:""a function of the form $f(x)=a^x$ where $a>0$ and $a\neq 1$""? I welcome any answer. Thanks!","I am confused about the following: The exponential function (by definition) is a function of the form $f(x)=a^x$ where $a>0$. However, when $a=1$, we get the constant function $f(x)=1^x=1$. Is the constant function $f(x)=1^x=1$ still considered an exponential function even though it does not have behave like an exponential function? Is the definition of the exponential function that I gave above (that I read in many textbooks) not entirely correct? Should we define the exponential function by:""a function of the form $f(x)=a^x$ where $a>0$ and $a\neq 1$""? I welcome any answer. Thanks!",,"['calculus', 'real-analysis', 'algebra-precalculus', 'definition', 'exponential-function']"
59,$L^1$ function unbounded on every interval,function unbounded on every interval,L^1,"[My question concerns part of Exercise 2.25 in Folland's Real Analysis text.] I'm looking at the function $g(x)=\sum_{n=1}^\infty 2^{-n}f(x-r_n)$, where $f(x)=x^{-1/2}$ for $x\in (0,1)$ and $f(x)=0$ elsewhere, and $\{r_n\}$ is some enumeration of the rational numbers.  I am trying to prove that $g$ is discontinuous at every point.  It is easy enough to see that $g$ is discontinuous wherever it is finite (since it is unbounded on every interval), and that it is finite almost everywhere (since it is in particular in $L^1$).  However, it seems to me that to show that $g$ is discontinuous at a point $x_0$ with $g(x_0)=+\infty$, one would have to exhibit a sequence $\{x_n\}$ converging to $x_0$ such that $\{g(x_n)\}$ is bounded.  It seems intuitively obvious to me that such a sequence exists, but I have not been able to construct one.  Any suggestions? (Hints preferred to answers.)","[My question concerns part of Exercise 2.25 in Folland's Real Analysis text.] I'm looking at the function $g(x)=\sum_{n=1}^\infty 2^{-n}f(x-r_n)$, where $f(x)=x^{-1/2}$ for $x\in (0,1)$ and $f(x)=0$ elsewhere, and $\{r_n\}$ is some enumeration of the rational numbers.  I am trying to prove that $g$ is discontinuous at every point.  It is easy enough to see that $g$ is discontinuous wherever it is finite (since it is unbounded on every interval), and that it is finite almost everywhere (since it is in particular in $L^1$).  However, it seems to me that to show that $g$ is discontinuous at a point $x_0$ with $g(x_0)=+\infty$, one would have to exhibit a sequence $\{x_n\}$ converging to $x_0$ such that $\{g(x_n)\}$ is bounded.  It seems intuitively obvious to me that such a sequence exists, but I have not been able to construct one.  Any suggestions? (Hints preferred to answers.)",,['real-analysis']
60,"Characterization of lim sup, lim inf","Characterization of lim sup, lim inf",,"If $(a_n)$ is a real sequence, in lecture we had: $$\begin{align}\limsup_{n\to\infty} a_n=a \iff &(i)\forall \epsilon >0 \,\exists n_0\in \mathbb{N} :a_n<a+\epsilon  \forall n\ge n_0\\\text{ and }&(ii) \forall \epsilon >0 \, \forall m\in \mathbb{N} \, \exists n\ge m :a_n>a-\epsilon\end{align}$$ and such a analogue characterization for lim inf. I know the definition of $ \limsup\limits_{n\to\infty} a_n=\sup H(a_n)$ with $H(a_n)$ set of all the limitpoints of $(a_n)$. I don't understand the epsilon-characterization of lim sup an, maybe you can draw a picture,  explain it in words why it is equivalent (or prove it directly, but I only want to understand it). Later I want to do an example if I have understand this. Maybe you want to help? Regards","If $(a_n)$ is a real sequence, in lecture we had: $$\begin{align}\limsup_{n\to\infty} a_n=a \iff &(i)\forall \epsilon >0 \,\exists n_0\in \mathbb{N} :a_n<a+\epsilon  \forall n\ge n_0\\\text{ and }&(ii) \forall \epsilon >0 \, \forall m\in \mathbb{N} \, \exists n\ge m :a_n>a-\epsilon\end{align}$$ and such a analogue characterization for lim inf. I know the definition of $ \limsup\limits_{n\to\infty} a_n=\sup H(a_n)$ with $H(a_n)$ set of all the limitpoints of $(a_n)$. I don't understand the epsilon-characterization of lim sup an, maybe you can draw a picture,  explain it in words why it is equivalent (or prove it directly, but I only want to understand it). Later I want to do an example if I have understand this. Maybe you want to help? Regards",,"['real-analysis', 'definition', 'limsup-and-liminf']"
61,Generalization of Hahn-Banach Theorem,Generalization of Hahn-Banach Theorem,,"Let $Y$ be a subspace of a locally convex topological vector space $X$. Suppose $T:Y\longrightarrow l^\infty$ is a continuous linear operator. Prove that $T$ can be extended to a continuous linear operator $\tilde T:X\longrightarrow l^\infty$ such that $\tilde T|_Y=T$. We can extend each coordinate linear functional to a continuous linear functional on $X$. But I do not know how to prove the series is still in $l^\infty$. Note that the question gives locally convex topological vector spaces but not normed space. Hence I am not able to extend the coordinate functionals preserving their norms, which are not well-defined. How to prove? Thank you a lot :)","Let $Y$ be a subspace of a locally convex topological vector space $X$. Suppose $T:Y\longrightarrow l^\infty$ is a continuous linear operator. Prove that $T$ can be extended to a continuous linear operator $\tilde T:X\longrightarrow l^\infty$ such that $\tilde T|_Y=T$. We can extend each coordinate linear functional to a continuous linear functional on $X$. But I do not know how to prove the series is still in $l^\infty$. Note that the question gives locally convex topological vector spaces but not normed space. Hence I am not able to extend the coordinate functionals preserving their norms, which are not well-defined. How to prove? Thank you a lot :)",,"['real-analysis', 'analysis', 'functional-analysis', 'locally-convex-spaces']"
62,Limit of Riemann integral,Limit of Riemann integral,,"The task is to show that $$\lim_{h \to 0} \int_a^b \lvert f(x + h) - f(x) \rvert\,dx  = 0$$ The function $f$ is Riemann integrable on $[a,b]$. I can show this in case $f$ is continuous (then $f$ is uniformly continuous) by bounding the difference $f(x+h)-f(x)$. But in case $f$ is only Riemann integrable I don't know how to show this.","The task is to show that $$\lim_{h \to 0} \int_a^b \lvert f(x + h) - f(x) \rvert\,dx  = 0$$ The function $f$ is Riemann integrable on $[a,b]$. I can show this in case $f$ is continuous (then $f$ is uniformly continuous) by bounding the difference $f(x+h)-f(x)$. But in case $f$ is only Riemann integrable I don't know how to show this.",,"['calculus', 'real-analysis', 'integration', 'limits']"
63,Real Analysis : uniform convergence of sequence,Real Analysis : uniform convergence of sequence,,"I was working on a real-analysis problem, but I got stuck, so could anybody please help me with this question? Give an example of a sequence of continuous functions $\{f_n\}_{n\in\mathbb{N}}$ on the interval $[0,1]$ such that $f_n(x)\xrightarrow[n\to\infty]{}0$ for all $x\in[0,1]$, but the supremum of $f_n(x)$ is $1$ for all $n\in\mathbb{N}$.","I was working on a real-analysis problem, but I got stuck, so could anybody please help me with this question? Give an example of a sequence of continuous functions $\{f_n\}_{n\in\mathbb{N}}$ on the interval $[0,1]$ such that $f_n(x)\xrightarrow[n\to\infty]{}0$ for all $x\in[0,1]$, but the supremum of $f_n(x)$ is $1$ for all $n\in\mathbb{N}$.",,"['real-analysis', 'convergence-divergence', 'uniform-convergence']"
64,Some questions about Banach's proof of the existence of continuous nowhere differentiable functions,Some questions about Banach's proof of the existence of continuous nowhere differentiable functions,,"Im reading Continuity and Category of Chapter11, Carothers' Real Analysis, 1ed. Here is a reading material at the end of this chapter, talking about Banach's proof of the existence of continuous nowhere differentiable functions, I have 4 questions here, I cannot understand the sentence in paragraph3 that is "" In particular, any f∈$C$[0,1] having a right-hand derivative at most n in magnitude at even one point in [0, 1-(1/n)] is in $E_n$"". I mean what is ""at most n in magnitude ""? Im not an english native speaker. How did he guarantee that |f(x+h)-f(x)| =< nh for all 0 < h < 1-x ? Why does the proof merely focus on right-hand derivatives? What is the quotients involved in?","Im reading Continuity and Category of Chapter11, Carothers' Real Analysis, 1ed. Here is a reading material at the end of this chapter, talking about Banach's proof of the existence of continuous nowhere differentiable functions, I have 4 questions here, I cannot understand the sentence in paragraph3 that is "" In particular, any f∈$C$[0,1] having a right-hand derivative at most n in magnitude at even one point in [0, 1-(1/n)] is in $E_n$"". I mean what is ""at most n in magnitude ""? Im not an english native speaker. How did he guarantee that |f(x+h)-f(x)| =< nh for all 0 < h < 1-x ? Why does the proof merely focus on right-hand derivatives? What is the quotients involved in?",,"['real-analysis', 'analysis']"
65,Continuous linear map,Continuous linear map,,"I am trying to understand how to start with this exercise. Let $\Omega \subset \mathbb{R}^n$, $n\ge 3$, open and bounded and  $$ C^{1,b}(\Omega)= \{\,f\in C^1(\Omega): \text{$f$ and all its partial  derivatives $D_if$ are bounded} \} $$ with the norm $\,\|f\|=\|f\|_{\infty} + \sum_i \|D_if\|_\infty$. Now let $g:\Omega \times \Omega \rightarrow \mathbb{R}$, such that, $$ x \neq y \,\,\Longrightarrow\,\, |g(x,y)| \le C\,|x-y|^2 $$  for some constant $C>0$,  and  $$ \int _{\Omega} g(.,y)\,f(y)\, dy \in C^{1,b}(\Omega)\quad \text{for all $\,f\in C^b(\Omega)$.} $$  Then show that there is a constant $A$ such that  $$ \Big|\,D_i \int_{\Omega} g(x,y) \,f(y)\, dy\,\Big|\le A\|f\|_{\infty},  \quad\text{for all $\,\,f \in C^b(\Omega),\, x \in \Omega$ and $i=1,...,n$.} $$ Okay let's summarize this. What I probably need to show is that the map $V: C^b(\Omega) \rightarrow C^{1,b}(\Omega)$, $f \mapsto \int_{\Omega} g(.,y)f(y) dy$ is a continuous map(it is linear). If I have this, then this solves the excercise. What's the problem here?- The thing is, we do not know anything about the nature of $g$. Furthermore, I notice that all spaces that appear here are Banach spaces! By the way, in this excercise is a hint given that we shall use the following theorem: $X,Y,Z$ Banach spaces and $T:X\rightarrow Y$ linear and $J:Y \rightarrow Z$ linear, injective and continuous as well as $J \circ T$ be continuous, then $T$ is also continuous. I do not see how this is helpful cause I do not see any injective operator that appears here. If anything is unclear, please let me know.","I am trying to understand how to start with this exercise. Let $\Omega \subset \mathbb{R}^n$, $n\ge 3$, open and bounded and  $$ C^{1,b}(\Omega)= \{\,f\in C^1(\Omega): \text{$f$ and all its partial  derivatives $D_if$ are bounded} \} $$ with the norm $\,\|f\|=\|f\|_{\infty} + \sum_i \|D_if\|_\infty$. Now let $g:\Omega \times \Omega \rightarrow \mathbb{R}$, such that, $$ x \neq y \,\,\Longrightarrow\,\, |g(x,y)| \le C\,|x-y|^2 $$  for some constant $C>0$,  and  $$ \int _{\Omega} g(.,y)\,f(y)\, dy \in C^{1,b}(\Omega)\quad \text{for all $\,f\in C^b(\Omega)$.} $$  Then show that there is a constant $A$ such that  $$ \Big|\,D_i \int_{\Omega} g(x,y) \,f(y)\, dy\,\Big|\le A\|f\|_{\infty},  \quad\text{for all $\,\,f \in C^b(\Omega),\, x \in \Omega$ and $i=1,...,n$.} $$ Okay let's summarize this. What I probably need to show is that the map $V: C^b(\Omega) \rightarrow C^{1,b}(\Omega)$, $f \mapsto \int_{\Omega} g(.,y)f(y) dy$ is a continuous map(it is linear). If I have this, then this solves the excercise. What's the problem here?- The thing is, we do not know anything about the nature of $g$. Furthermore, I notice that all spaces that appear here are Banach spaces! By the way, in this excercise is a hint given that we shall use the following theorem: $X,Y,Z$ Banach spaces and $T:X\rightarrow Y$ linear and $J:Y \rightarrow Z$ linear, injective and continuous as well as $J \circ T$ be continuous, then $T$ is also continuous. I do not see how this is helpful cause I do not see any injective operator that appears here. If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
66,Why is the derivative the tangent vector?,Why is the derivative the tangent vector?,,"I'm trying to understand, at least intuitively why the derivative of a function at a point is the tangent vector at this point. If we see the functions of this form $f:\mathbb R\to \mathbb R$ we see clearly that $$f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$$ is the slope of the tangent of $f$ at the point $a\in \mathbb R$, because the angular coefficient of a line is $\frac{\Delta y}{\Delta x}$. However I couldn't understand why the derivative is the tangent vector in higher dimensions so clearly as we see in my example above. In order to illustrate what I said above, let's take for example the helix  $$\alpha(t):\mathbb R\to \mathbb R^3,\ \alpha(t)=(a\cos t,a\sin t,bt)$$ If we take the definition of derivative in higher dimensions in Spivak's book we have: A function $f:\mathbb R^n\to \mathbb R^m$ is differentiable at $a\in \mathbb R^m$ if there is a linear transformation $\lambda: \mathbb R^n\to \mathbb R^m$ such that $$\lim_{h\to 0}\frac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0$$ I can't see why the $\alpha'(t)$ is the tangent at the point $t$ and I tried also see the derivative as the Jacobian without success. So my question is why does the derivative is the tangent vector? Thanks in advance.","I'm trying to understand, at least intuitively why the derivative of a function at a point is the tangent vector at this point. If we see the functions of this form $f:\mathbb R\to \mathbb R$ we see clearly that $$f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}$$ is the slope of the tangent of $f$ at the point $a\in \mathbb R$, because the angular coefficient of a line is $\frac{\Delta y}{\Delta x}$. However I couldn't understand why the derivative is the tangent vector in higher dimensions so clearly as we see in my example above. In order to illustrate what I said above, let's take for example the helix  $$\alpha(t):\mathbb R\to \mathbb R^3,\ \alpha(t)=(a\cos t,a\sin t,bt)$$ If we take the definition of derivative in higher dimensions in Spivak's book we have: A function $f:\mathbb R^n\to \mathbb R^m$ is differentiable at $a\in \mathbb R^m$ if there is a linear transformation $\lambda: \mathbb R^n\to \mathbb R^m$ such that $$\lim_{h\to 0}\frac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0$$ I can't see why the $\alpha'(t)$ is the tangent at the point $t$ and I tried also see the derivative as the Jacobian without success. So my question is why does the derivative is the tangent vector? Thanks in advance.",,"['calculus', 'real-analysis', 'analysis', 'differential-geometry']"
67,Lebesgue's Dominated Convergence Theorem problem,Lebesgue's Dominated Convergence Theorem problem,,"I am having trouble using DCT for the following Prove  $$\lim_{n\to\infty}\int_0^\infty \frac{n}{(1+y)^n(ny)^\frac{1}{n}}dy = 1$$ I think most of the mass of the integral lies beneath $(e-1)/n$ but now dont believe this is true, so I was hoping to break up the integral into two parts, but that didn't seem to help. So I seem to be stuck at the moment. Any help or suggestions would be greatly appreciated. It would be more helpful to me if you gave me a general suggestion rather than a specific one. Such as how to find the dominating function for integrals which become tidal waves as $n \rightarrow \infty$ or how to chose the point to split the integral into two bits. Thanks","I am having trouble using DCT for the following Prove  $$\lim_{n\to\infty}\int_0^\infty \frac{n}{(1+y)^n(ny)^\frac{1}{n}}dy = 1$$ I think most of the mass of the integral lies beneath $(e-1)/n$ but now dont believe this is true, so I was hoping to break up the integral into two parts, but that didn't seem to help. So I seem to be stuck at the moment. Any help or suggestions would be greatly appreciated. It would be more helpful to me if you gave me a general suggestion rather than a specific one. Such as how to find the dominating function for integrals which become tidal waves as $n \rightarrow \infty$ or how to chose the point to split the integral into two bits. Thanks",,"['real-analysis', 'convergence-divergence', 'lebesgue-integral']"
68,Direct proof that $\sum_{n\geq 0}\frac{x^n}{n!}=\lim_{n\rightarrow\infty}(1+\frac{x}{n})^n$,Direct proof that,\sum_{n\geq 0}\frac{x^n}{n!}=\lim_{n\rightarrow\infty}(1+\frac{x}{n})^n,Is there a direct proof that $$\sum_{n\geq 0}\frac{x^n}{n!}=\lim_{n\rightarrow\infty}(1+\frac{x}{n})^n?$$ We dont know what logarithms or exponentials are.,Is there a direct proof that $$\sum_{n\geq 0}\frac{x^n}{n!}=\lim_{n\rightarrow\infty}(1+\frac{x}{n})^n?$$ We dont know what logarithms or exponentials are.,,"['real-analysis', 'sequences-and-series', 'algebra-precalculus', 'exponential-function', 'binomial-theorem']"
69,True or False? Continuous Functions,True or False? Continuous Functions,,"If the function $f+g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous, then the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ are also continuous. False; Let $f(x)=\begin{cases} -1 \text{ if } x<0 \\ 1 \text{ if } x\ge 0 \end{cases}$ $\hspace{10pt}$ and $\hspace{10pt}$ $g(x)=\begin{cases} 1 \text{ if } x<0 \\ -1 \text{ if } x\ge 0 \end{cases}$ Then $(f+g)(x)=0 \hspace{10pt}\forall x$. Here, $f+g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous, but the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ are not continuous. Is this a good example?","If the function $f+g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous, then the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ are also continuous. False; Let $f(x)=\begin{cases} -1 \text{ if } x<0 \\ 1 \text{ if } x\ge 0 \end{cases}$ $\hspace{10pt}$ and $\hspace{10pt}$ $g(x)=\begin{cases} 1 \text{ if } x<0 \\ -1 \text{ if } x\ge 0 \end{cases}$ Then $(f+g)(x)=0 \hspace{10pt}\forall x$. Here, $f+g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous, but the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ are not continuous. Is this a good example?",,"['real-analysis', 'continuity']"
70,a problem about liminf/ limsup with a continuous function,a problem about liminf/ limsup with a continuous function,,"My Mathematical Analysis III professor gave me this problem: Let $f:(0,1) \rightarrow f((0,1))$ be a continuous function in the standard euclidean metric space $($$\Bbb R$,$d_2$$)$ and let $\liminf_{x\rightarrow0} f(x)<\limsup_{x\rightarrow0} f(x)$, then prove that for every L $\in$ $(\liminf_{x\rightarrow0} f(x),\limsup_{x\rightarrow0} f(x))$, exists a sequence $x_{n}$ in $(0,1)$ that converges to $0$ and such that $\lim_{n\rightarrow \infty} f(x_{n}) = L$ I truly don't know how to prove it, if someone could help me i would be grateful.","My Mathematical Analysis III professor gave me this problem: Let $f:(0,1) \rightarrow f((0,1))$ be a continuous function in the standard euclidean metric space $($$\Bbb R$,$d_2$$)$ and let $\liminf_{x\rightarrow0} f(x)<\limsup_{x\rightarrow0} f(x)$, then prove that for every L $\in$ $(\liminf_{x\rightarrow0} f(x),\limsup_{x\rightarrow0} f(x))$, exists a sequence $x_{n}$ in $(0,1)$ that converges to $0$ and such that $\lim_{n\rightarrow \infty} f(x_{n}) = L$ I truly don't know how to prove it, if someone could help me i would be grateful.",,"['real-analysis', 'limsup-and-liminf']"
71,Addition inequalities with lim sup and lim inf,Addition inequalities with lim sup and lim inf,,"$\lim\inf a_n+\lim\sup b_n\le\lim\sup(a_n+b_n)\le\lim\sup a_n+\lim\sup b_n$ For the right inequality, I assume $A=\lim\sup a_n, B = \lim\sup B_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_{n}<A+\varepsilon$ for all $n\ge n_0$. Similarly for $b_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_n+b_n<A+B+\varepsilon$ for all $n\ge n_0$. Therefore $\lim\sup(a_n+b_n)\leq A+B$. Now for the left inequality, assume $A=\lim\inf a_n, B = \lim\sup B_n$. How can this be compared to $\lim\sup(a_n+b_n)$?","$\lim\inf a_n+\lim\sup b_n\le\lim\sup(a_n+b_n)\le\lim\sup a_n+\lim\sup b_n$ For the right inequality, I assume $A=\lim\sup a_n, B = \lim\sup B_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_{n}<A+\varepsilon$ for all $n\ge n_0$. Similarly for $b_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_n+b_n<A+B+\varepsilon$ for all $n\ge n_0$. Therefore $\lim\sup(a_n+b_n)\leq A+B$. Now for the left inequality, assume $A=\lim\inf a_n, B = \lim\sup B_n$. How can this be compared to $\lim\sup(a_n+b_n)$?",,['real-analysis']
72,Is my proof correct?,Is my proof correct?,,"I'm a new user so if my question is inappropriate, non-uniform or ill-shaped; please comment (or edit maybe). First of all, I'm asking that if my proof is correct or not. Of course, you can prove it nicer and this helps me but my main reason for asking this question is a check for my proof. Let $a \in V \subseteq X \subseteq \mathbb{R} $. If for an open interval $I$, $I \cap X \subseteq V$,  we call $V$ as ""$X$-neighbourhood of $a$"". We will prove following statement: ""Let $X \subseteq \mathbb{R}$ and assume that for all $x \in X$ there exist a finite $X$-neighbourhood of $x$. Then, $X$ must be countably infinite."" EDIT: It doesn't have to be infinite, it just must be countable. My proof : Because of the fact that $X \subseteq \mathbb{R} $, we can just show that $X$ is not uncountably infinite. But$_*$ also, we should show that $X$ cannot be finite. Let's assume that $X$ is uncountably infinite and let's define a set $A$ as $A = \{Y$: The set $X \setminus Y$ is still uncountably infinite$ \} $. Now$_{**}$ we will pick a maximal$_{***}$ element $B$ in $A$. Then, if $x \in X\setminus B$ there must be a finite $X$-neighbourhood of $x$. Let's call this neighbourhood as $W$ and call the open interval as $I$ (there might be more than one open interval, just pick one of them) which make $W$ a $X$-neighbourhood of $x$ .Then, obviously $(X \setminus B) \cap I \subseteq W $. But, then $(X \setminus B) \cap I \in A $ and if we take $J = (X \setminus B) \cap I$, $(X \setminus B) \setminus J  = X \setminus (B \cup J)$ must be in $A$ and that is a contradiction because we take $B$ as a maximal element. $_*$: With edit, this part is wrong. $_{**}$: We need to show that A is well-defined and there exists a maximal element in A. $_{***}$: If for a $k_1 \in A$, there exists no $k_2 \in A$ such that $k_2 \subset k_1$, we call $k_1$ as the maximal element. For completion we should prove that $_{**}$ but I think you can easily show it. Also, there are little things, for example, $B$ must be non-empty. Thanks for any help and please bear in mind that I cannot understand high-level topological explanations.","I'm a new user so if my question is inappropriate, non-uniform or ill-shaped; please comment (or edit maybe). First of all, I'm asking that if my proof is correct or not. Of course, you can prove it nicer and this helps me but my main reason for asking this question is a check for my proof. Let $a \in V \subseteq X \subseteq \mathbb{R} $. If for an open interval $I$, $I \cap X \subseteq V$,  we call $V$ as ""$X$-neighbourhood of $a$"". We will prove following statement: ""Let $X \subseteq \mathbb{R}$ and assume that for all $x \in X$ there exist a finite $X$-neighbourhood of $x$. Then, $X$ must be countably infinite."" EDIT: It doesn't have to be infinite, it just must be countable. My proof : Because of the fact that $X \subseteq \mathbb{R} $, we can just show that $X$ is not uncountably infinite. But$_*$ also, we should show that $X$ cannot be finite. Let's assume that $X$ is uncountably infinite and let's define a set $A$ as $A = \{Y$: The set $X \setminus Y$ is still uncountably infinite$ \} $. Now$_{**}$ we will pick a maximal$_{***}$ element $B$ in $A$. Then, if $x \in X\setminus B$ there must be a finite $X$-neighbourhood of $x$. Let's call this neighbourhood as $W$ and call the open interval as $I$ (there might be more than one open interval, just pick one of them) which make $W$ a $X$-neighbourhood of $x$ .Then, obviously $(X \setminus B) \cap I \subseteq W $. But, then $(X \setminus B) \cap I \in A $ and if we take $J = (X \setminus B) \cap I$, $(X \setminus B) \setminus J  = X \setminus (B \cup J)$ must be in $A$ and that is a contradiction because we take $B$ as a maximal element. $_*$: With edit, this part is wrong. $_{**}$: We need to show that A is well-defined and there exists a maximal element in A. $_{***}$: If for a $k_1 \in A$, there exists no $k_2 \in A$ such that $k_2 \subset k_1$, we call $k_1$ as the maximal element. For completion we should prove that $_{**}$ but I think you can easily show it. Also, there are little things, for example, $B$ must be non-empty. Thanks for any help and please bear in mind that I cannot understand high-level topological explanations.",,"['real-analysis', 'proof-verification']"
73,Question on monotonicity and differentiability,Question on monotonicity and differentiability,,"Let $f:[0,1]\rightarrow \Re$ be continuous. Assume $f$ is differentiable almost everywhere and $f(0)>f(1)$. Does this imply that there exists an $x\in(0,1)$ such that $f$ is differentiable at $x$ and $f'(x)<0$? My gut feeling is yes but I do not see a way to prove it. Any thoughts (proof/counterexample)? Thanks!","Let $f:[0,1]\rightarrow \Re$ be continuous. Assume $f$ is differentiable almost everywhere and $f(0)>f(1)$. Does this imply that there exists an $x\in(0,1)$ such that $f$ is differentiable at $x$ and $f'(x)<0$? My gut feeling is yes but I do not see a way to prove it. Any thoughts (proof/counterexample)? Thanks!",,['real-analysis']
74,A problem from Spivak's Calculus on Manifolds,A problem from Spivak's Calculus on Manifolds,,"Notation As Spivak suggests, given $A\subset\mathbb R^n$, boundary $A$ denotes the topological boundary of $A$, i.e. $\overline A\cap\overline{A^c}$. Problem 5-3(a): Let $A\subset\mathbb R^n$ be an open set such that boundary $A$ is an $n-1$ dimensional manifold. Show that $N=A\cup$ boundary $A$ is an $n$ dimensional manifold-with-boundary. Thoughts It seems that we only need to show that there's a neighborhood $V_x$ of $x\in$ boundary $A$ such that $V_x\cap N$ is diffeomorphic to $\mathbb R^n$ or $[0,\infty)\times \mathbb R^{n-1}$. The first situation is met when $A=\{x:0<\lVert x\rVert<1\}$. The condition of boundary $A$ is indispensable, but I don't know how to make use of it. At first, I found a counterexample , eventually pointed out by somebody that it is against the condition of boundary $A$: $$A=\bigcup_{n=1}^\infty\left(\frac1{2^{n+1}},\frac1{2^n}\right)\times\mathbb R$$ It seems that, the material of manifolds is not that manageable.","Notation As Spivak suggests, given $A\subset\mathbb R^n$, boundary $A$ denotes the topological boundary of $A$, i.e. $\overline A\cap\overline{A^c}$. Problem 5-3(a): Let $A\subset\mathbb R^n$ be an open set such that boundary $A$ is an $n-1$ dimensional manifold. Show that $N=A\cup$ boundary $A$ is an $n$ dimensional manifold-with-boundary. Thoughts It seems that we only need to show that there's a neighborhood $V_x$ of $x\in$ boundary $A$ such that $V_x\cap N$ is diffeomorphic to $\mathbb R^n$ or $[0,\infty)\times \mathbb R^{n-1}$. The first situation is met when $A=\{x:0<\lVert x\rVert<1\}$. The condition of boundary $A$ is indispensable, but I don't know how to make use of it. At first, I found a counterexample , eventually pointed out by somebody that it is against the condition of boundary $A$: $$A=\bigcup_{n=1}^\infty\left(\frac1{2^{n+1}},\frac1{2^n}\right)\times\mathbb R$$ It seems that, the material of manifolds is not that manageable.",,"['calculus', 'real-analysis', 'manifolds']"
75,Monotone Decreasing Sequences of Functions: Is the Dominated Convergence Thm applicable?,Monotone Decreasing Sequences of Functions: Is the Dominated Convergence Thm applicable?,,"Reading through Rudin's Real and Complex Analysis, I came across the following exercise: Suppose $(f_n: X \to [0,\infty])$ is a monotone decreasing sequence of   measurable functions such that $\lim\limits_{n \to \infty} f_n(x) =  f(x)$ for all $x \in X$. Prove that if  $f_1 \in L^1(\mu)$, then $$\lim\limits_{n \to \infty}\int\limits_{X} f_n \, \mathrm{d}\mu =  \int\limits_X f \, \mathrm{d}\mu.$$ It seems like this should be a trivial application of the Dominated Convergence Theorem, taking $f_1$ to be the dominating function. But it seems like an exercise would not be so trivial as to merit basically a one line proof. Is there a reason that DCT fails to be applicable here?","Reading through Rudin's Real and Complex Analysis, I came across the following exercise: Suppose $(f_n: X \to [0,\infty])$ is a monotone decreasing sequence of   measurable functions such that $\lim\limits_{n \to \infty} f_n(x) =  f(x)$ for all $x \in X$. Prove that if  $f_1 \in L^1(\mu)$, then $$\lim\limits_{n \to \infty}\int\limits_{X} f_n \, \mathrm{d}\mu =  \int\limits_X f \, \mathrm{d}\mu.$$ It seems like this should be a trivial application of the Dominated Convergence Theorem, taking $f_1$ to be the dominating function. But it seems like an exercise would not be so trivial as to merit basically a one line proof. Is there a reason that DCT fails to be applicable here?",,"['real-analysis', 'analysis', 'integration', 'lebesgue-integral']"
76,"If $\lim\limits_{x \to \infty} f'(x) = L$ and $\lim\limits_{n \to \infty} f(n) = A$ exists, prove that $L = 0$.","If  and  exists, prove that .",\lim\limits_{x \to \infty} f'(x) = L \lim\limits_{n \to \infty} f(n) = A L = 0,"Here is the homework problem I am stuck on: Let $f$ be differentiable on $(0,\infty)$.  If $\lim\limits_{x \to \infty} f'(x) = L$ exists in $\mathbb{R}$ and $\lim\limits_{n \to \infty} f(n) = A$ exists in $\mathbb{R}$, prove that $L = 0$. From the given information, I know that we get to assume: $f$ is continuous at every point $s \in (0,\infty)$. We can now apply the MVT. So far this is what I'm thinking: First, I think this describes a function which increases towards a horizontal asymptote. I have created a strictly increasing sequence $\{x_n\} = \{x_1, x_2, \dots, x_n \}$ to serve as the $x$ values in $(0, \infty)$.  This gives me a sequence of intervals basically. I can apply the MVT on each of these intervals, getting a sequence of $c_n \in (x_{n-1}, x_n)$. But I don't see where to go from here.  I am guessing I will need the squeeze theorem later, but there's a gap in between. Perhaps I'm doing something wrong? Thanks for your help.","Here is the homework problem I am stuck on: Let $f$ be differentiable on $(0,\infty)$.  If $\lim\limits_{x \to \infty} f'(x) = L$ exists in $\mathbb{R}$ and $\lim\limits_{n \to \infty} f(n) = A$ exists in $\mathbb{R}$, prove that $L = 0$. From the given information, I know that we get to assume: $f$ is continuous at every point $s \in (0,\infty)$. We can now apply the MVT. So far this is what I'm thinking: First, I think this describes a function which increases towards a horizontal asymptote. I have created a strictly increasing sequence $\{x_n\} = \{x_1, x_2, \dots, x_n \}$ to serve as the $x$ values in $(0, \infty)$.  This gives me a sequence of intervals basically. I can apply the MVT on each of these intervals, getting a sequence of $c_n \in (x_{n-1}, x_n)$. But I don't see where to go from here.  I am guessing I will need the squeeze theorem later, but there's a gap in between. Perhaps I'm doing something wrong? Thanks for your help.",,['real-analysis']
77,Discreteness of eigenvalues for certain operators - can this approach be made rigorous?,Discreteness of eigenvalues for certain operators - can this approach be made rigorous?,,"I was idly thinking about why one might naïvely expect a discrete spectrum of eigenvalues for a linear operator $L$ when I dreamt up the following argument (which I expect isn't new instead - references?). Can it be made rigorous , with any necessary added restrictions, or does it at least offer some genuine informal insight? (Or did I just mess something up?) Suppose $L$ is a linear, self-adjoint operator acting on (sufficiently) continuously differentiable functions  $\phi:[0,1]\to\mathbb{R}$ with boundary conditions $\phi(0)=\phi(1)=0$. An eigenfunction $\phi_\lambda(x)$ is a non-zero such function such that $$\left[L \phi_\lambda\right](x) = \lambda \phi_\lambda(x)$$ We expect that such functions occur only for 'discrete' $\lambda$, in particular not for an interval $(a,b)$ of eigenvalues. Clarification : I'm thinking of $L$ being a 'nice' integral/differential operator in my head. What I'm interested in is what further restrictions are necessary to make the spectrum simple. Here's a reason to suspect this is the case: suppose $\phi\equiv \phi(x;\lambda)$ is a (sufficiently) continuously differentiable function $[0,1]\times(a,b)\to\mathbb{R}$ such that $L \phi = \lambda\phi$. Suppose further that $L,\phi$ is sufficiently nicely behaved that $L (\partial_\lambda \phi) \equiv \partial_\lambda(L\phi)$. Note that we expect $\partial_\lambda \phi|_{x=0,1}=0$ since all $\phi$ obey the boundary conditions, so $L$ should be self-adjoint on this function too. Then, with a simple $L^2$ inner product $\left<\cdot,\cdot\right>$ say, $$\boxed{\lambda \left<\phi,\partial_\lambda \phi\right> = \left<L\phi,\partial_\lambda \phi\right> = \left<\phi,L\partial_\lambda \phi\right> = \left<\phi,\partial_\lambda L\phi\right> = \left<\phi,\partial_\lambda (\lambda\phi)\right> = \left<\phi,\phi\right> + \lambda \left<\phi,\partial_\lambda \phi\right>}$$ and hence $$\left<\phi,\phi\right> = 0$$ Hence there is no continuously differentiable family of solutions which vary the eigenvalue throughout some interval. Example By contrast, if we dropped our boundary conditions and worked on the fully infinite domain $x\in(-\infty,\infty)$ the operator $Lf=f''(x)$ acquires all real eigenvalues on the smooth families $\cos kx,\exp kx$. But for s finite interval with homogeneous BCs, we only get e.g. $0,\pi,2\pi,\ldots$ Any interesting thoughts are welcome! I haven't thought about this in great detail, so it may be that this just leads to a standard spectral theory argument when one tries to make it rigorous, but I'm curious as to whether it has an interesting interpretation, and whether you think it would be a good, informal justification to teach someone first coming to Sturm-Liouville theory. Mainly, I am interested in whether this can be tweaked to a proof without losing all the simple spirit of the boxed equation above.","I was idly thinking about why one might naïvely expect a discrete spectrum of eigenvalues for a linear operator $L$ when I dreamt up the following argument (which I expect isn't new instead - references?). Can it be made rigorous , with any necessary added restrictions, or does it at least offer some genuine informal insight? (Or did I just mess something up?) Suppose $L$ is a linear, self-adjoint operator acting on (sufficiently) continuously differentiable functions  $\phi:[0,1]\to\mathbb{R}$ with boundary conditions $\phi(0)=\phi(1)=0$. An eigenfunction $\phi_\lambda(x)$ is a non-zero such function such that $$\left[L \phi_\lambda\right](x) = \lambda \phi_\lambda(x)$$ We expect that such functions occur only for 'discrete' $\lambda$, in particular not for an interval $(a,b)$ of eigenvalues. Clarification : I'm thinking of $L$ being a 'nice' integral/differential operator in my head. What I'm interested in is what further restrictions are necessary to make the spectrum simple. Here's a reason to suspect this is the case: suppose $\phi\equiv \phi(x;\lambda)$ is a (sufficiently) continuously differentiable function $[0,1]\times(a,b)\to\mathbb{R}$ such that $L \phi = \lambda\phi$. Suppose further that $L,\phi$ is sufficiently nicely behaved that $L (\partial_\lambda \phi) \equiv \partial_\lambda(L\phi)$. Note that we expect $\partial_\lambda \phi|_{x=0,1}=0$ since all $\phi$ obey the boundary conditions, so $L$ should be self-adjoint on this function too. Then, with a simple $L^2$ inner product $\left<\cdot,\cdot\right>$ say, $$\boxed{\lambda \left<\phi,\partial_\lambda \phi\right> = \left<L\phi,\partial_\lambda \phi\right> = \left<\phi,L\partial_\lambda \phi\right> = \left<\phi,\partial_\lambda L\phi\right> = \left<\phi,\partial_\lambda (\lambda\phi)\right> = \left<\phi,\phi\right> + \lambda \left<\phi,\partial_\lambda \phi\right>}$$ and hence $$\left<\phi,\phi\right> = 0$$ Hence there is no continuously differentiable family of solutions which vary the eigenvalue throughout some interval. Example By contrast, if we dropped our boundary conditions and worked on the fully infinite domain $x\in(-\infty,\infty)$ the operator $Lf=f''(x)$ acquires all real eigenvalues on the smooth families $\cos kx,\exp kx$. But for s finite interval with homogeneous BCs, we only get e.g. $0,\pi,2\pi,\ldots$ Any interesting thoughts are welcome! I haven't thought about this in great detail, so it may be that this just leads to a standard spectral theory argument when one tries to make it rigorous, but I'm curious as to whether it has an interesting interpretation, and whether you think it would be a good, informal justification to teach someone first coming to Sturm-Liouville theory. Mainly, I am interested in whether this can be tweaked to a proof without losing all the simple spirit of the boxed equation above.",,"['real-analysis', 'functional-analysis', 'reference-request', 'intuition']"
78,Sum of the roots equation,Sum of the roots equation,,"Need help! how to prove that equation have two roots on $(0,\frac{\pi}{2})$ and calculate $x_1+x_2$ $$\tan(x)^{\cos^2x}=\frac{\tan(x)^{\sin^2x}}{e}$$ That's what I tried : $ \tan(x)=t $ $\cos^2x=\frac{1}{1+t^2} $ $\sin^2x=1-\frac{1}{1+t^2} $ And have to solve :$$ \frac{\ln(t)}{1+t^2} = (1- \frac{1}{1+t^2})\ln(t)-1$$","Need help! how to prove that equation have two roots on $(0,\frac{\pi}{2})$ and calculate $x_1+x_2$ $$\tan(x)^{\cos^2x}=\frac{\tan(x)^{\sin^2x}}{e}$$ That's what I tried : $ \tan(x)=t $ $\cos^2x=\frac{1}{1+t^2} $ $\sin^2x=1-\frac{1}{1+t^2} $ And have to solve :$$ \frac{\ln(t)}{1+t^2} = (1- \frac{1}{1+t^2})\ln(t)-1$$",,"['real-analysis', 'trigonometry']"
79,From the area of the inscribed/circumscribed $2^n$-gons to the area of the unit circle,From the area of the inscribed/circumscribed -gons to the area of the unit circle,2^n,"Say, we have two $2^n$-gons: one inscribed in a unit circle and another one circumscribed around the unit circle. After using some basic geometry and limits we arrive at the following result: $$ \begin{align}  \lim_{n\to \infty} \mbox{(perimeter of the inscribed $2^n$-gon)} &= \lim_{n\to \infty} \mbox{(perimeter of the circumscribed $2^n$-gon)} \\ &:= 2\pi \end{align} $$ Yet now (at least according to my professor) we are not allowed to conclude that the perimeter of the unit circle itself equals $2\pi$, because we have only showed it for $2^n$-gons and maybe some other approximations would give us different result. Now what if we are interested in the areas? $$ \begin{align}  \lim_{n\to \infty} \mbox{(area of the circumscribed $2^n$-gon)} &= \lim_{n\to \infty} \mbox{($2^n \times $ area of the triangle)}    \\ &=  \lim_{n\to \infty} (2^n \frac{1 \times s_n}{2})  \\ &=  \lim_{n\to \infty} \mbox{($\frac{1}{2} \times$ perimeter of the circumscribed $2^n$-gon)}  \\ &=  \frac{1}{2} \times 2\pi \mbox{ (using the result above)}  \\ &=  \pi \end{align} $$ Similarly (but with slightly more geometry involved): $$\lim_{n\to \infty} \mbox{(area of the inscribed $2^n$-gon)} = \pi $$ My question is: are we now allowed to conclude that the area of the unit circle equals $\pi$? Or do we have a similar problem as the one (with perimeters) described above? Any help is much appreciated.","Say, we have two $2^n$-gons: one inscribed in a unit circle and another one circumscribed around the unit circle. After using some basic geometry and limits we arrive at the following result: $$ \begin{align}  \lim_{n\to \infty} \mbox{(perimeter of the inscribed $2^n$-gon)} &= \lim_{n\to \infty} \mbox{(perimeter of the circumscribed $2^n$-gon)} \\ &:= 2\pi \end{align} $$ Yet now (at least according to my professor) we are not allowed to conclude that the perimeter of the unit circle itself equals $2\pi$, because we have only showed it for $2^n$-gons and maybe some other approximations would give us different result. Now what if we are interested in the areas? $$ \begin{align}  \lim_{n\to \infty} \mbox{(area of the circumscribed $2^n$-gon)} &= \lim_{n\to \infty} \mbox{($2^n \times $ area of the triangle)}    \\ &=  \lim_{n\to \infty} (2^n \frac{1 \times s_n}{2})  \\ &=  \lim_{n\to \infty} \mbox{($\frac{1}{2} \times$ perimeter of the circumscribed $2^n$-gon)}  \\ &=  \frac{1}{2} \times 2\pi \mbox{ (using the result above)}  \\ &=  \pi \end{align} $$ Similarly (but with slightly more geometry involved): $$\lim_{n\to \infty} \mbox{(area of the inscribed $2^n$-gon)} = \pi $$ My question is: are we now allowed to conclude that the area of the unit circle equals $\pi$? Or do we have a similar problem as the one (with perimeters) described above? Any help is much appreciated.",,"['real-analysis', 'geometry', 'limits']"
80,Basic exercise about Sobolev spaces,Basic exercise about Sobolev spaces,,"let $\Omega  \subset R^n$  a open set .let $\varphi  \in H^{1}_{0} (\Omega)$ . Suppose that the  suport of $\varphi$ is compact. By definition , there exists a sequence of functions $  \varphi_i   , i \in N$   in $C_{0}^{\infty}(\Omega)$ converging to $\varphi$ in $H^{1}(\Omega)$ . Consider an open set ${\Omega}^{'}$  satisfying (this set exists because the suport of $\varphi$ is compact) :  $ \operatorname{supp}\varphi \subset {\Omega}^{'}  $ and $\overline{{\Omega}^{'}}  \subset \Omega$ The affirmation "" there exists $i_0 \in N$ such that   $i  \geq i_0 $ implies $ \operatorname{supp}\varphi_i \subset {\Omega}^{'}$  "" is true       ? My professor said that the affirmation is true . the hint of my professor is: Suppose the affirmation is not true. since $\varphi_i   , i \in N$ converges to $\varphi$ in $H^{1}(\Omega)$, then $\varphi_i \rightarrow    \varphi$ a.e. the negation of your thesis contradicts this $""\varphi_i \rightarrow    \varphi$ a.e."" I dont know how to prove the affirmation..... someone can give me a hint ? Thank you ( my english is terrible , sorry )","let $\Omega  \subset R^n$  a open set .let $\varphi  \in H^{1}_{0} (\Omega)$ . Suppose that the  suport of $\varphi$ is compact. By definition , there exists a sequence of functions $  \varphi_i   , i \in N$   in $C_{0}^{\infty}(\Omega)$ converging to $\varphi$ in $H^{1}(\Omega)$ . Consider an open set ${\Omega}^{'}$  satisfying (this set exists because the suport of $\varphi$ is compact) :  $ \operatorname{supp}\varphi \subset {\Omega}^{'}  $ and $\overline{{\Omega}^{'}}  \subset \Omega$ The affirmation "" there exists $i_0 \in N$ such that   $i  \geq i_0 $ implies $ \operatorname{supp}\varphi_i \subset {\Omega}^{'}$  "" is true       ? My professor said that the affirmation is true . the hint of my professor is: Suppose the affirmation is not true. since $\varphi_i   , i \in N$ converges to $\varphi$ in $H^{1}(\Omega)$, then $\varphi_i \rightarrow    \varphi$ a.e. the negation of your thesis contradicts this $""\varphi_i \rightarrow    \varphi$ a.e."" I dont know how to prove the affirmation..... someone can give me a hint ? Thank you ( my english is terrible , sorry )",,"['real-analysis', 'sobolev-spaces']"
81,Show that the upper semicontinuous has a maximum,Show that the upper semicontinuous has a maximum,,"I found a proof of showing, given that $D$ is compact, if the function $f$ is upper semicontinuous then f achieves a maximum on $D$. But I have a question about the very last sentence: and thus $M<\infty$. How could this conclusion is obtained, since just before two lines it is said that ""It may be that $M=\infty$""? Could anyone help me with this? Thanks!","I found a proof of showing, given that $D$ is compact, if the function $f$ is upper semicontinuous then f achieves a maximum on $D$. But I have a question about the very last sentence: and thus $M<\infty$. How could this conclusion is obtained, since just before two lines it is said that ""It may be that $M=\infty$""? Could anyone help me with this? Thanks!",,"['real-analysis', 'compactness', 'semicontinuous-functions']"
82,Sequences not in $l^p$,Sequences not in,l^p,I am wondering if there is an easy sequence $x_n \in \mathbb R$ with $x_n \to 0$ and $x_n \notin l^p$ for all $1 \le p < \infty$. I found $x_n = (\log n)^{-1}$ satisfies $x_n \to 0$ and $x_n \notin l^p$ because $\sum_{n=2}^\infty |x_n|^p \ge \sum_{n=2}^\infty |x_{n+1}|^p \ge \int_2^\infty (\log x)^{-p} dx \ge \int_2^\infty (\log x)^{-1} dx = (x(\log x -1))_2^\infty = \infty$. But it is complicated. Is there an easier example?,I am wondering if there is an easy sequence $x_n \in \mathbb R$ with $x_n \to 0$ and $x_n \notin l^p$ for all $1 \le p < \infty$. I found $x_n = (\log n)^{-1}$ satisfies $x_n \to 0$ and $x_n \notin l^p$ because $\sum_{n=2}^\infty |x_n|^p \ge \sum_{n=2}^\infty |x_{n+1}|^p \ge \int_2^\infty (\log x)^{-p} dx \ge \int_2^\infty (\log x)^{-1} dx = (x(\log x -1))_2^\infty = \infty$. But it is complicated. Is there an easier example?,,['real-analysis']
83,Power Functions on the Integers,Power Functions on the Integers,,"Suppose $f:\mathbb{R}\to\mathbb{R}$ is of the form $f(x)=x^a$ for some $a\in\mathbb{R}^{+}$. If $f(\mathbb{Z})\subset\mathbb{Z}$, show that $a\in\mathbb{Z}$. Source: A friend posed this problem; not sure if it is well known.","Suppose $f:\mathbb{R}\to\mathbb{R}$ is of the form $f(x)=x^a$ for some $a\in\mathbb{R}^{+}$. If $f(\mathbb{Z})\subset\mathbb{Z}$, show that $a\in\mathbb{Z}$. Source: A friend posed this problem; not sure if it is well known.",,"['real-analysis', 'integer-lattices']"
84,How to prove that $\lim\limits_{x\to\infty}f(x)^{1/x}=g$ if $\lim\limits_{x\to\infty}\frac{f(x+1)}{f(x)}=g$,How to prove that  if,\lim\limits_{x\to\infty}f(x)^{1/x}=g \lim\limits_{x\to\infty}\frac{f(x+1)}{f(x)}=g,"$$ f: (0,\infty)$$ is bounded and$$ f(x)>0 $$  How to prove that: If $$\lim_{x\to \infty} \frac{f(x+1)}{f(x)} = g$$,  Then $$\lim_{x\to \infty} f(x)^{1/x} = g $$ I remember a similary proof for a strings using inequality between average","$$ f: (0,\infty)$$ is bounded and$$ f(x)>0 $$  How to prove that: If $$\lim_{x\to \infty} \frac{f(x+1)}{f(x)} = g$$,  Then $$\lim_{x\to \infty} f(x)^{1/x} = g $$ I remember a similary proof for a strings using inequality between average",,"['real-analysis', 'limits']"
85,"Demonstrate that $F(x,y) = f(x,y)\sin(x^2 + y^2)$ is differentiable at $(0,0)$",Demonstrate that  is differentiable at,"F(x,y) = f(x,y)\sin(x^2 + y^2) (0,0)","Let $f : \mathbb{R^2} \rightarrow \mathbb{R}$ be a bounded function and $F(x,y) = f(x,y)\sin(x^2 + y^2)$. How can we demonstrate that $F$ is differentiable at $(0,0)$? I don't how to do this. I already tried, but I don't get it.","Let $f : \mathbb{R^2} \rightarrow \mathbb{R}$ be a bounded function and $F(x,y) = f(x,y)\sin(x^2 + y^2)$. How can we demonstrate that $F$ is differentiable at $(0,0)$? I don't how to do this. I already tried, but I don't get it.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
86,Show that $\sum_{k=1}^\infty \frac{1}{k!}$ converges without the Ratio Test,Show that  converges without the Ratio Test,\sum_{k=1}^\infty \frac{1}{k!},"Show that $\sum_{k=1}^\infty \frac{1}{k!}$ converges without the Ratio Test. (hint: show first that $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$) \begin{align} & k!\geq 2^{k-1} \\ \leftrightarrow\quad & k!\geq 2^k\cdot2^{-1} \\ \leftrightarrow\quad & 2\cdot k!\geq 2^k \\ \text{For }k=1:\quad& 2\cdot1!=2^1 \\ \text{For }k=2:\quad& 2\cdot2!=2^2 \\ \text{For }k=3:\quad& 2\cdot3!=2^3 \\ \leftrightarrow \quad& 12 \geq 8 \\ \text{For }k+1:\quad & 2(k+1)!=2(k+1)k! \geq 2^{k+1}=2\cdot2^k \\ \leftrightarrow \quad & (k+1)k! \geq 2^k \end{align} that's true because we assume that $k!\geq 2^{k-1}\mid \forall k \in \mathbb{N}$ So it is proved by induction that  $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$ is true. $s_n=\sum_{k=1}^\infty \frac{1}{k!}=1+1/2+1/6+1/24+\cdots$ if $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$) then  $(k!)^{-1}\leq 2^{1-k}$ $\leftrightarrow \sum_{k=1}^\infty \frac{1}{k!} \leq \sum_{k=1}^\infty 2^{1-k}$ with $\sum_{k=1}^\infty 2^{1-k}$ is a geometric serie, which converges $\leftrightarrow |\frac{1}{k!}| \leq 2^{1-k}$ by the comparison test: and $s_n$ converges. Is that prove correct? Or should I try to prove it with the connection to e?Or is there a faster way to prove it?","Show that $\sum_{k=1}^\infty \frac{1}{k!}$ converges without the Ratio Test. (hint: show first that $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$) \begin{align} & k!\geq 2^{k-1} \\ \leftrightarrow\quad & k!\geq 2^k\cdot2^{-1} \\ \leftrightarrow\quad & 2\cdot k!\geq 2^k \\ \text{For }k=1:\quad& 2\cdot1!=2^1 \\ \text{For }k=2:\quad& 2\cdot2!=2^2 \\ \text{For }k=3:\quad& 2\cdot3!=2^3 \\ \leftrightarrow \quad& 12 \geq 8 \\ \text{For }k+1:\quad & 2(k+1)!=2(k+1)k! \geq 2^{k+1}=2\cdot2^k \\ \leftrightarrow \quad & (k+1)k! \geq 2^k \end{align} that's true because we assume that $k!\geq 2^{k-1}\mid \forall k \in \mathbb{N}$ So it is proved by induction that  $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$ is true. $s_n=\sum_{k=1}^\infty \frac{1}{k!}=1+1/2+1/6+1/24+\cdots$ if $k!\geq 2^{k-1}|\forall k \in \mathbb{N}$) then  $(k!)^{-1}\leq 2^{1-k}$ $\leftrightarrow \sum_{k=1}^\infty \frac{1}{k!} \leq \sum_{k=1}^\infty 2^{1-k}$ with $\sum_{k=1}^\infty 2^{1-k}$ is a geometric serie, which converges $\leftrightarrow |\frac{1}{k!}| \leq 2^{1-k}$ by the comparison test: and $s_n$ converges. Is that prove correct? Or should I try to prove it with the connection to e?Or is there a faster way to prove it?",,"['real-analysis', 'convergence-divergence']"
87,How to apply Borel-Cantelli Lemma?,How to apply Borel-Cantelli Lemma?,,"Assume that we are given a sequence of continuous functions $f_n(x)$ on $[0,1]$. How to show the existence of a sequence $a_n$ and a set $A$ with $\mu(A^c)=0$ so that $$  \lim_{ n\to \infty} \frac{f_n(x)}{a_n}=0, ~~ \forall x\in A. $$ I choose a sequence $a_n$ such that $ \mu (\phi_n) \leq 1/2^n$ where $$  \phi_n := \left\{ x: \frac{|f_n(x)|}{a_n} \geq \frac{1}{n} \right\}. $$ Since $\sum_n \mu (\phi_n) < \infty$, using Borel-Cantelli Lemma we have $\mu(\limsup_n \phi_n)=0$. It seems okay if we say that $\limsup_n \phi_n = A^c$. How can we write it clearly in full details? Also, how can we assure the existence of $a_n$, how to construct such a sequence by means of $f_n(x)$? Thanks!","Assume that we are given a sequence of continuous functions $f_n(x)$ on $[0,1]$. How to show the existence of a sequence $a_n$ and a set $A$ with $\mu(A^c)=0$ so that $$  \lim_{ n\to \infty} \frac{f_n(x)}{a_n}=0, ~~ \forall x\in A. $$ I choose a sequence $a_n$ such that $ \mu (\phi_n) \leq 1/2^n$ where $$  \phi_n := \left\{ x: \frac{|f_n(x)|}{a_n} \geq \frac{1}{n} \right\}. $$ Since $\sum_n \mu (\phi_n) < \infty$, using Borel-Cantelli Lemma we have $\mu(\limsup_n \phi_n)=0$. It seems okay if we say that $\limsup_n \phi_n = A^c$. How can we write it clearly in full details? Also, how can we assure the existence of $a_n$, how to construct such a sequence by means of $f_n(x)$? Thanks!",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
88,"Why the ""distances"" satisfy the triangle inequality?","Why the ""distances"" satisfy the triangle inequality?",,"This is a excercise in Shiryaev's Probability On Page 139: Show that the ""distance"" $\rho_1(A, B)$ and $\rho_2(A, B)$ defined by $$\rho_1(A, B)=P(A\triangle B),$$ $$\rho_2(A, B)=\begin{cases} \frac{P(A\triangle B)}{P(A\bigcup B)} & \text{if } P(A\bigcup B)\ne 0,\\ \quad\quad\ 0 & \text{if } P(A\bigcup B)=0 \end{cases}$$ satisfy the triangle inequality. I have proved the case $\rho_1$ , but don't know how to prove for $\rho_2$ . Thanks! I got an answer myself: $$\frac{P(A\Delta C)}{P(A\cup C)}=\frac{P(A\Delta C)}{P(A\Delta C)+P(A\cap C)}\leq \frac{P(A\Delta B)+P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}$$ Because $(A\Delta B)\cup (B\Delta C) \cup (A\cap C)\supseteq A\cup B\cup C$ , so $$\frac{P(A\Delta B)+P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}=\frac{P(A\Delta B)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}+\frac{P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}\\\leq \frac{P(A\Delta B)}{P(A\cup B)}+\frac{P(B\Delta C)}{P(B\cup C)}$$","This is a excercise in Shiryaev's Probability On Page 139: Show that the ""distance"" and defined by satisfy the triangle inequality. I have proved the case , but don't know how to prove for . Thanks! I got an answer myself: Because , so","\rho_1(A, B) \rho_2(A, B) \rho_1(A, B)=P(A\triangle B), \rho_2(A, B)=\begin{cases}
\frac{P(A\triangle B)}{P(A\bigcup B)} & \text{if } P(A\bigcup B)\ne 0,\\
\quad\quad\ 0 & \text{if } P(A\bigcup B)=0
\end{cases} \rho_1 \rho_2 \frac{P(A\Delta C)}{P(A\cup C)}=\frac{P(A\Delta C)}{P(A\Delta C)+P(A\cap C)}\leq \frac{P(A\Delta B)+P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)} (A\Delta B)\cup (B\Delta C) \cup (A\cap C)\supseteq A\cup B\cup C \frac{P(A\Delta B)+P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}=\frac{P(A\Delta B)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}+\frac{P(B\Delta C)}{P(A\Delta B)+P(B\Delta C)+P(A\cap C)}\\\leq \frac{P(A\Delta B)}{P(A\cup B)}+\frac{P(B\Delta C)}{P(B\cup C)}",['real-analysis']
89,Prove that $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k = \int \limits _E f$,Prove that,\lim \limits _{k\rightarrow \infty} \int \limits _E f_k = \int \limits _E f,"Show that if $f : E \rightarrow [0,\infty]$, $\lim \limits _{k\rightarrow \infty} f_k = f$ on $E$, and $f_k \leq f$ on $E$ for each $k \in N$, then $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k = \int \limits _E f$ An idea was to show that $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \leq \int \limits _E f$ and $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \geq \int \limits _E f$. I am able to prove $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \leq \int \limits _E f$ but am struggling to prove the second condition. My idea was to use fatou's lemma to get  $$ \int \limits _E \liminf \limits _{k\rightarrow \infty} f_k = \int \limits _E \lim \limits _{k\rightarrow \infty} f_k = \int \limits _E f \leq \liminf \limits _{k\rightarrow \infty} \int \limits _E f_k = \lim \limits _{k\rightarrow \infty} \int \limits _E f_k $$ but I don't know how to show $\liminf \limits _{k\rightarrow \infty} f_k = \lim \limits _{k\rightarrow \infty} f_k$, besides showing $\liminf \limits _{k\rightarrow \infty} f_k = \limsup \limits _{k\rightarrow \infty} f_k$ . Any ideas on how I could finish this? Also, is my approach wrong? Could I do it a better way?","Show that if $f : E \rightarrow [0,\infty]$, $\lim \limits _{k\rightarrow \infty} f_k = f$ on $E$, and $f_k \leq f$ on $E$ for each $k \in N$, then $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k = \int \limits _E f$ An idea was to show that $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \leq \int \limits _E f$ and $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \geq \int \limits _E f$. I am able to prove $\lim \limits _{k\rightarrow \infty} \int \limits _E f_k \leq \int \limits _E f$ but am struggling to prove the second condition. My idea was to use fatou's lemma to get  $$ \int \limits _E \liminf \limits _{k\rightarrow \infty} f_k = \int \limits _E \lim \limits _{k\rightarrow \infty} f_k = \int \limits _E f \leq \liminf \limits _{k\rightarrow \infty} \int \limits _E f_k = \lim \limits _{k\rightarrow \infty} \int \limits _E f_k $$ but I don't know how to show $\liminf \limits _{k\rightarrow \infty} f_k = \lim \limits _{k\rightarrow \infty} f_k$, besides showing $\liminf \limits _{k\rightarrow \infty} f_k = \limsup \limits _{k\rightarrow \infty} f_k$ . Any ideas on how I could finish this? Also, is my approach wrong? Could I do it a better way?",,['real-analysis']
90,"Find the interior, accumulation points, closure, and boundary of the set","Find the interior, accumulation points, closure, and boundary of the set",,"I need to find the interior, accumulation points, closure, and boundary of the set $$ A = \left\{ \frac1n + \frac1k \in \mathbb{R} \mid n,k \in \mathbb{N} \right\} $$   and use the information to determine whether the set is bounded, closed, or compact. So far, I have that the interior is empty, but not sure how to prove it. My thoughts are to fix $n$ and then the accumulation points would be $\left\{ \frac 1n \mid n \in \mathbb{N} \right\}$. But I'm not sure if that is correct. Then, I believe that the boundary is $[0,2]$. Can someone confirm that? Any help would be appreciated.","I need to find the interior, accumulation points, closure, and boundary of the set $$ A = \left\{ \frac1n + \frac1k \in \mathbb{R} \mid n,k \in \mathbb{N} \right\} $$   and use the information to determine whether the set is bounded, closed, or compact. So far, I have that the interior is empty, but not sure how to prove it. My thoughts are to fix $n$ and then the accumulation points would be $\left\{ \frac 1n \mid n \in \mathbb{N} \right\}$. But I'm not sure if that is correct. Then, I believe that the boundary is $[0,2]$. Can someone confirm that? Any help would be appreciated.",,['real-analysis']
91,When is the boundary of a subset of $\mathbb{R^n}$ compact?,When is the boundary of a subset of  compact?,\mathbb{R^n},"In Euclidean space, the boundary of a subset (Closure - Interior) is always closed, but I would like to know when it's compact as well.  Is it when the subset is bounded?","In Euclidean space, the boundary of a subset (Closure - Interior) is always closed, but I would like to know when it's compact as well.  Is it when the subset is bounded?",,"['real-analysis', 'compactness']"
92,Claims in Pinchover's textbook's proof of existence and uniqueness theorem for first order PDEs,Claims in Pinchover's textbook's proof of existence and uniqueness theorem for first order PDEs,,"The reference here is Pinchover & Rubinstein's An introduction to partial differential equations , pages 36-37. It's about the existence and uniqueness of a solution to the equation $a(x,y,u)u_x + b(x,y,u)u_y = c(x,y,u)$ with initial condition (curve on the integral surface) $x(0,s)=x_0 (s), y(0,s)=y_0(s), u(0,s)=u_0(s)$. It involves the construction of a surface, by solving a family of systems of ODEs (whose solutions $x(t,s), y(t,s), u(t,s)$) are curves on the desired surface and thus ""knit it together""). They claim that ""the transversality condition ($x_t(0,s)y_s(0,s)-y_t(0,s)x_s(0,s)\neq 0$) implies that the parametric representation provides a smooth surface"". Why is it enough to check this along the initial curve? Then they check that this surface satisfies the PDE. On they go: ""to show that there are no further integral surfaces, we prove that the characteristic curves we constructed must lie on an integral surface"". I don't know what this means, because I thought that was precisely what they had just done. Most likely I'm just completely clueless and their writing is not to blame. I appreciate any comments that may enlighten me on understanding this proof.","The reference here is Pinchover & Rubinstein's An introduction to partial differential equations , pages 36-37. It's about the existence and uniqueness of a solution to the equation $a(x,y,u)u_x + b(x,y,u)u_y = c(x,y,u)$ with initial condition (curve on the integral surface) $x(0,s)=x_0 (s), y(0,s)=y_0(s), u(0,s)=u_0(s)$. It involves the construction of a surface, by solving a family of systems of ODEs (whose solutions $x(t,s), y(t,s), u(t,s)$) are curves on the desired surface and thus ""knit it together""). They claim that ""the transversality condition ($x_t(0,s)y_s(0,s)-y_t(0,s)x_s(0,s)\neq 0$) implies that the parametric representation provides a smooth surface"". Why is it enough to check this along the initial curve? Then they check that this surface satisfies the PDE. On they go: ""to show that there are no further integral surfaces, we prove that the characteristic curves we constructed must lie on an integral surface"". I don't know what this means, because I thought that was precisely what they had just done. Most likely I'm just completely clueless and their writing is not to blame. I appreciate any comments that may enlighten me on understanding this proof.",,"['real-analysis', 'analysis', 'partial-differential-equations']"
93,$C^1$ approximation of a continuous curve.,approximation of a continuous curve.,C^1,"Suppose I have two points $\alpha,\beta \in \Bbb{R}^n$. Define  $$ X=\{\gamma \in C^1([0,1] , \Bbb{R}^n),\ \gamma(0)=\alpha,\gamma(1)=\beta ,0 <|\gamma'|<K\}$$ parametrized curves joining $\alpha$ and $\beta$. If I have a sequence $(\gamma_n) \subset X$ such that the paths $\gamma_n$ are all contained in a compact set $K \subset \Bbb{R}^n$ then this sequence is equi-bounded and equi-continuous (in $C([0,1],\Bbb{R}^n$) and by Ascoli-Arzela theorem there is a path $\gamma :[0,1] \to \Bbb{R}^n$ such that $\gamma_n$ converges uniformly to $\gamma$. The limit $\gamma$ is continuous, but may not be of class $C^1$. In my case I can prove that $\gamma([0,1])\subset [\alpha,\beta]$ (the line segment joining $\alpha$ and $\beta$). My question is: Can I find a sequence of paths $\theta_n \in C^1([0,1],[\alpha,\beta])$ for which $|\theta_n'| \leq K$ and $(\theta_n)$ converges uniformly (or maybe pointwise) to $\gamma$? My intuition says that if I can approximate $\gamma$ with a sequence of $C^1$ paths joining $\alpha$ and $\beta$ then it is sufficiently regular such that I can approximate $\gamma$ with $C^1$ curves with $\theta_n([0,1])\subset \gamma([0,1])$. One thought was to use projections of $\gamma_n$ on the segment $[\alpha,\beta]$, but I think that in some cases the projection can destroy differentiability.","Suppose I have two points $\alpha,\beta \in \Bbb{R}^n$. Define  $$ X=\{\gamma \in C^1([0,1] , \Bbb{R}^n),\ \gamma(0)=\alpha,\gamma(1)=\beta ,0 <|\gamma'|<K\}$$ parametrized curves joining $\alpha$ and $\beta$. If I have a sequence $(\gamma_n) \subset X$ such that the paths $\gamma_n$ are all contained in a compact set $K \subset \Bbb{R}^n$ then this sequence is equi-bounded and equi-continuous (in $C([0,1],\Bbb{R}^n$) and by Ascoli-Arzela theorem there is a path $\gamma :[0,1] \to \Bbb{R}^n$ such that $\gamma_n$ converges uniformly to $\gamma$. The limit $\gamma$ is continuous, but may not be of class $C^1$. In my case I can prove that $\gamma([0,1])\subset [\alpha,\beta]$ (the line segment joining $\alpha$ and $\beta$). My question is: Can I find a sequence of paths $\theta_n \in C^1([0,1],[\alpha,\beta])$ for which $|\theta_n'| \leq K$ and $(\theta_n)$ converges uniformly (or maybe pointwise) to $\gamma$? My intuition says that if I can approximate $\gamma$ with a sequence of $C^1$ paths joining $\alpha$ and $\beta$ then it is sufficiently regular such that I can approximate $\gamma$ with $C^1$ curves with $\theta_n([0,1])\subset \gamma([0,1])$. One thought was to use projections of $\gamma_n$ on the segment $[\alpha,\beta]$, but I think that in some cases the projection can destroy differentiability.",,"['real-analysis', 'differential-geometry', 'approximation']"
94,Understanding Proof About an Immersion,Understanding Proof About an Immersion,,"I am studying the following proof for which an excerpt is provided below: Update: I have written out a fully-detailed proof of an argument that seeks verify the claim that $\partial \psi$ is invertible. (1) I am unclear on what is special about the point $(x_0, 0)$ as the proof seems to goes through irrespective of the value of the particular point and (2) The author's logic at the end seems reversed to me. It would be helpful if someone could critique the proof below and indicate what step, if any, is incorrect. Also, I apologize for not including the actual TEX; it was formulated locally and I made use of many macros that mathjax wouldn't understand:","I am studying the following proof for which an excerpt is provided below: Update: I have written out a fully-detailed proof of an argument that seeks verify the claim that $\partial \psi$ is invertible. (1) I am unclear on what is special about the point $(x_0, 0)$ as the proof seems to goes through irrespective of the value of the particular point and (2) The author's logic at the end seems reversed to me. It would be helpful if someone could critique the proof below and indicate what step, if any, is incorrect. Also, I apologize for not including the actual TEX; it was formulated locally and I made use of many macros that mathjax wouldn't understand:",,"['real-analysis', 'multivariable-calculus', 'manifolds']"
95,Is it true that $ \left| \int_{-\pi}^\pi f(x) \sin nx dx\right| \leq \frac{f(-\pi)-f(\pi)}{n}$?,Is it true that ?, \left| \int_{-\pi}^\pi f(x) \sin nx dx\right| \leq \frac{f(-\pi)-f(\pi)}{n},"Let $f: [-\pi, \pi] \rightarrow \mathbb{R}$ be nonincreasing. Is it true  that $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq \frac{f(-\pi)-f(\pi)}{n}.$$ (Please, without Stieltjes integrals.) I obtain something similar by using the second mean value theorem for integrals but with right hand side equal $2 \frac{f(-\pi)-f(\pi)}{n}$. Thanks. Added. Sorry,  I  mistaked and this inequality is generally not true. It holds the following inequality $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq 2 \frac{f(-\pi)-f(\pi)}{n}.$$ The proof goes in the followig way. By the second mean value theorem  there exists a $c\in [-\pi,\pi]$  such that  $$\int_{-\pi}^\pi f(x) \sin nxdx=f(-\pi)\int_{-\pi}^c \sin nx dx+f(\pi) \int_c^\pi \sin nx dx$$       $$=-\frac{f(-\pi)}{n} (\cos nc-\cos n\pi)-\frac{f(\pi)}{n} (\cos n\pi-\cos nc)$$      $$=\frac{f(-\pi)-f(\pi)}{n} (\cos n\pi-\cos nc).$$ Since $|(\cos n\pi-\cos nc)| \leq 2$ we obtain  $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq 2 \frac{f(-\pi)-f(\pi)}{n}.$$","Let $f: [-\pi, \pi] \rightarrow \mathbb{R}$ be nonincreasing. Is it true  that $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq \frac{f(-\pi)-f(\pi)}{n}.$$ (Please, without Stieltjes integrals.) I obtain something similar by using the second mean value theorem for integrals but with right hand side equal $2 \frac{f(-\pi)-f(\pi)}{n}$. Thanks. Added. Sorry,  I  mistaked and this inequality is generally not true. It holds the following inequality $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq 2 \frac{f(-\pi)-f(\pi)}{n}.$$ The proof goes in the followig way. By the second mean value theorem  there exists a $c\in [-\pi,\pi]$  such that  $$\int_{-\pi}^\pi f(x) \sin nxdx=f(-\pi)\int_{-\pi}^c \sin nx dx+f(\pi) \int_c^\pi \sin nx dx$$       $$=-\frac{f(-\pi)}{n} (\cos nc-\cos n\pi)-\frac{f(\pi)}{n} (\cos n\pi-\cos nc)$$      $$=\frac{f(-\pi)-f(\pi)}{n} (\cos n\pi-\cos nc).$$ Since $|(\cos n\pi-\cos nc)| \leq 2$ we obtain  $$ \left| \int_{-\pi}^\pi f(x) \sin (nx) dx \right| \leq 2 \frac{f(-\pi)-f(\pi)}{n}.$$",,"['real-analysis', 'integration']"
96,Weaker assumption to ensure $f_{n}^\prime \to f'$,Weaker assumption to ensure,f_{n}^\prime \to f',"I learned in my real analysis class that if $f_n:[a,b] \to \mathbb{R}$ is a sequence of differentiable functions such that $f_n \to f$ uniformly and $f_{n}^\prime \to g$ uniformly then $f$ is differentiable and $f' = g$. Can the assumptions be weakened? In particular I'd like to dispense of the condition that $f_{n}^\prime$ converges uniformly to some function $g$. This question is motivated by the following: Let $K = \{f:[a,b] \to \mathbb{R}: f \text{ is differentiable and } \|f\|_{\infty} + \|f'\|_{\infty} \le 1\}$. By the Arzelà-Ascoli theorem, the closure of this set is compact in $C([a,b])$. But I want it to actually be compact possibly after imposing some additional conditions on the possible functions $f$. This happens when $K$ is closed, hence the question. Thanks in advance.","I learned in my real analysis class that if $f_n:[a,b] \to \mathbb{R}$ is a sequence of differentiable functions such that $f_n \to f$ uniformly and $f_{n}^\prime \to g$ uniformly then $f$ is differentiable and $f' = g$. Can the assumptions be weakened? In particular I'd like to dispense of the condition that $f_{n}^\prime$ converges uniformly to some function $g$. This question is motivated by the following: Let $K = \{f:[a,b] \to \mathbb{R}: f \text{ is differentiable and } \|f\|_{\infty} + \|f'\|_{\infty} \le 1\}$. By the Arzelà-Ascoli theorem, the closure of this set is compact in $C([a,b])$. But I want it to actually be compact possibly after imposing some additional conditions on the possible functions $f$. This happens when $K$ is closed, hence the question. Thanks in advance.",,['real-analysis']
97,Non-vanishing of derivative hypothesis in l'Hospital's rule,Non-vanishing of derivative hypothesis in l'Hospital's rule,,"I'm teaching first semester calculus and trying to find a way to explain why each hypothesis in l'Hostpial's rule is needed. If $f$ and $g$ are real differentiable functions on an interval containing a point $c$ then there are three hypotheses one needs to check in order to apply l'Hospitals rule to compute $\lim_{x \to c} f(x)/g(x)$ : $g'(x) \neq 0$ on a neighborhood of $c$, (the wikipedia page misses this) $\lim_{x \to c} f'(x)/g'(x)$ exists (in the extended sense including $\pm \infty$), Either $\lim_{x \to c} g(x) = \pm \infty$ or $\lim_{x \to c} f(x) = \lim_{x \to c} g(x) = 0$. What I am looking for is a pair of functions $f$ and $g$ so that 2 and 3 both hold and you can reasonably compute $\lim_{x \to c} f(x)/g(x)$ but get a different answer than computing the limit of the quotient of the derivatives. Bonus points if you can make $\lim_{x \to c} f(x) = \pm \infty$ as well in the case that $g(x) \to \pm \infty$. I was shown an example like this a long time ago in my analysis sequence but it would be lost on the students I think: Let $f(x) = 2x + \sin(2x)$ and let $g(x) = (2x+\sin(2x))e^{-\sin(x)}$. Then $f/g = e^{\sin x}$ on $(0,\infty)$ so $\lim_{x \to \infty} f(x)/g(x)$ does not exist. Both functions will go to $\infty$ as $x \to \infty$ and the limit of the quotients of the derivatives actually goes to zero. The issue is that $g'(x)$ has infinitely many zeros as $x \to \infty$ so you cannot use l'Hospital. Any clue on how to make this more inviting to a freshman calculus student?","I'm teaching first semester calculus and trying to find a way to explain why each hypothesis in l'Hostpial's rule is needed. If $f$ and $g$ are real differentiable functions on an interval containing a point $c$ then there are three hypotheses one needs to check in order to apply l'Hospitals rule to compute $\lim_{x \to c} f(x)/g(x)$ : $g'(x) \neq 0$ on a neighborhood of $c$, (the wikipedia page misses this) $\lim_{x \to c} f'(x)/g'(x)$ exists (in the extended sense including $\pm \infty$), Either $\lim_{x \to c} g(x) = \pm \infty$ or $\lim_{x \to c} f(x) = \lim_{x \to c} g(x) = 0$. What I am looking for is a pair of functions $f$ and $g$ so that 2 and 3 both hold and you can reasonably compute $\lim_{x \to c} f(x)/g(x)$ but get a different answer than computing the limit of the quotient of the derivatives. Bonus points if you can make $\lim_{x \to c} f(x) = \pm \infty$ as well in the case that $g(x) \to \pm \infty$. I was shown an example like this a long time ago in my analysis sequence but it would be lost on the students I think: Let $f(x) = 2x + \sin(2x)$ and let $g(x) = (2x+\sin(2x))e^{-\sin(x)}$. Then $f/g = e^{\sin x}$ on $(0,\infty)$ so $\lim_{x \to \infty} f(x)/g(x)$ does not exist. Both functions will go to $\infty$ as $x \to \infty$ and the limit of the quotients of the derivatives actually goes to zero. The issue is that $g'(x)$ has infinitely many zeros as $x \to \infty$ so you cannot use l'Hospital. Any clue on how to make this more inviting to a freshman calculus student?",,"['calculus', 'real-analysis']"
98,Show that $f=0$ $\mu$ a.e.,Show that   a.e.,f=0 \mu,"Let $([0,1], \mathcal{B}([0,1]), \mu)$ be a measure space with Lebesgue measure $\mu$ . Let $f\in L_1([0,1])$ and a fixed constant $b\in (0,1)$ so that $$ \int_B fd\mu=0 $$ for all $B\in \mathcal{B}([0,1])$ with $\mu(B)=b$ . Show that $f=0$ $\mu$ a.e. My proof is as follows: (that is different from Want to show that $f=0$ a.e. ). W.L.O.G, assume that for any open set $G$ with $\mu(G)=b$ we have $ \int_G fd\mu=0$ . Then for every $a\in \mathbb{R}$ , we have $$\int_a^{a+b}f(x)dx=0$$ Then for every $c>0$ we have $$ \frac{1}{c}\int_a^{a+c}f(x)dx=\frac{1}{c}\int_{a+b}^{a+b+c}f(x)dx $$ As $c\to 0$ , we get $f(a)=f(a+b)$ a.e. for $a\in \mathbb{R}$ . Define $$E_n:=\{a\in\mathbb{R}: f(a)=f(a+b)=\dots=f(a+nb)\neq f(a+(n+1)b)\}$$ with $\mu(E_n)=0$ for $n\in\mathbb{N}$ . Let $S:=\mathbb{R}\setminus \cup_{n=1}^\infty E_n$ . we have $$ \lim_{n\to\infty}\frac{1}{b/n}\int_a^{a+b/n}f(x)dx=f(a) $$ for all $a\in S\setminus\mathbb{Z}$ . Let $G=(a,a+b/n)\cup(a+b/n, a+2b/n)\cup\dots\cup (a+n-1, a+n-1+b/n)$ . $G$ is open and $m(G)=1$ . So $$ \int_G fd\mu=0=\frac{1}{b/n}\int_a^{a+b/n}f(x)dx\to f(a) $$ So $f(a)=0$ a.e. for every $a\in\mathbb{R}$ .","Let be a measure space with Lebesgue measure . Let and a fixed constant so that for all with . Show that a.e. My proof is as follows: (that is different from Want to show that $f=0$ a.e. ). W.L.O.G, assume that for any open set with we have . Then for every , we have Then for every we have As , we get a.e. for . Define with for . Let . we have for all . Let . is open and . So So a.e. for every .","([0,1], \mathcal{B}([0,1]), \mu) \mu f\in L_1([0,1]) b\in (0,1) 
\int_B fd\mu=0
 B\in \mathcal{B}([0,1]) \mu(B)=b f=0 \mu G \mu(G)=b 
\int_G fd\mu=0 a\in \mathbb{R} \int_a^{a+b}f(x)dx=0 c>0 
\frac{1}{c}\int_a^{a+c}f(x)dx=\frac{1}{c}\int_{a+b}^{a+b+c}f(x)dx
 c\to 0 f(a)=f(a+b) a\in \mathbb{R} E_n:=\{a\in\mathbb{R}: f(a)=f(a+b)=\dots=f(a+nb)\neq f(a+(n+1)b)\} \mu(E_n)=0 n\in\mathbb{N} S:=\mathbb{R}\setminus \cup_{n=1}^\infty E_n 
\lim_{n\to\infty}\frac{1}{b/n}\int_a^{a+b/n}f(x)dx=f(a)
 a\in S\setminus\mathbb{Z} G=(a,a+b/n)\cup(a+b/n, a+2b/n)\cup\dots\cup (a+n-1, a+n-1+b/n) G m(G)=1 
\int_G fd\mu=0=\frac{1}{b/n}\int_a^{a+b/n}f(x)dx\to f(a)
 f(a)=0 a\in\mathbb{R}",['real-analysis']
99,"Finding a closed form for $ \int_0^1 \frac1x \ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x $",Finding a closed form for," \int_0^1 \frac1x \ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x ","I want a closed form for the following integral $$ \int_0^1 \frac1x\;\ln\left(\frac {\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x $$ An integration by parts attempt on my end led to the following integral $$ \int_0^1 \frac1x \;\ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right) \, \mathrm{d}x  = \int_0^1 \ln(x)\left(\frac{1}{(1-x)\ln\left(\frac{1-x}{2}\right)}  + \frac{1}{(1+x)\ln\left(\frac{1+x}{2}\right)}\right) \, \mathrm{d}x $$ I have seen variations of these integrals, and usually they are solved by Feynman's trick.  I think the first one admits to a nice result, but I am not sure on how to handle the second one. If anyone has any ideas please let me know, there might be a substitution I am not seeing? Apologies for the small integral sign for such a big integrand, I am unsure on how to make them bigger.","I want a closed form for the following integral An integration by parts attempt on my end led to the following integral I have seen variations of these integrals, and usually they are solved by Feynman's trick.  I think the first one admits to a nice result, but I am not sure on how to handle the second one. If anyone has any ideas please let me know, there might be a substitution I am not seeing? Apologies for the small integral sign for such a big integrand, I am unsure on how to make them bigger.","
\int_0^1 \frac1x\;\ln\left(\frac {\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right)\, \mathrm{d}x
 
\int_0^1 \frac1x \;\ln\left(\frac{\ln\left(\frac{1-x}{2}\right)}{\ln\left(\frac{x+1}{2}\right)}\right) \, \mathrm{d}x 
= \int_0^1 \ln(x)\left(\frac{1}{(1-x)\ln\left(\frac{1-x}{2}\right)} 
+ \frac{1}{(1+x)\ln\left(\frac{1+x}{2}\right)}\right) \, \mathrm{d}x
","['real-analysis', 'calculus', 'complex-analysis', 'definite-integrals', 'logarithms']"
