,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Shortest proof for showing $\mathbb{Z}[\frac{1+\sqrt{-19}}{2}]$ is a PID.,Shortest proof for showing  is a PID.,\mathbb{Z}[\frac{1+\sqrt{-19}}{2}],"I'm looking for an easy proof for that $\mathbb{Z}[\frac{1+\sqrt{-19}}{2}]$ is a PID. One proof I know is to show that the field norm is a Dedekind-Hasse norm, but this proof is quite dirty( that it proves case by case) and technical. Since any ring is a PID iff it admits a Dedekind-Hasse norm, I think every proof would look similar to the one I know. However, since this problem has been on qualifying exam quite several times , I'm curious if there is one which is less technical. Thank you in advance.","I'm looking for an easy proof for that $\mathbb{Z}[\frac{1+\sqrt{-19}}{2}]$ is a PID. One proof I know is to show that the field norm is a Dedekind-Hasse norm, but this proof is quite dirty( that it proves case by case) and technical. Since any ring is a PID iff it admits a Dedekind-Hasse norm, I think every proof would look similar to the one I know. However, since this problem has been on qualifying exam quite several times , I'm curious if there is one which is less technical. Thank you in advance.",,"['abstract-algebra', 'alternative-proof', 'principal-ideal-domains']"
1,Proving that $-1$ has no square root in $\mathbb Z_3$ (3-adic integers),Proving that  has no square root in  (3-adic integers),-1 \mathbb Z_3,"I want to prove that $-1$ has no square root in $\mathbb Z_3$ (3-adic integers). By the definition of inverse limit, we know that there is a ring homomorphism $\phi$ from $\mathbb Z_3$ to $\mathbb Z/3 \mathbb Z$. So, if $\alpha$ is the square root of $-1$ in  $\mathbb Z_3$ then $\phi(\alpha)$ must be the square root of $-1$ in $\mathbb Z/3 \mathbb Z$. Since, there is no square root of $-1$ in $\mathbb Z/3 \mathbb Z$ this should imply $-1$ has no square root in $\mathbb Z_3$ (3-adic integers). Is the above argument correct? or am I missing something?","I want to prove that $-1$ has no square root in $\mathbb Z_3$ (3-adic integers). By the definition of inverse limit, we know that there is a ring homomorphism $\phi$ from $\mathbb Z_3$ to $\mathbb Z/3 \mathbb Z$. So, if $\alpha$ is the square root of $-1$ in  $\mathbb Z_3$ then $\phi(\alpha)$ must be the square root of $-1$ in $\mathbb Z/3 \mathbb Z$. Since, there is no square root of $-1$ in $\mathbb Z/3 \mathbb Z$ this should imply $-1$ has no square root in $\mathbb Z_3$ (3-adic integers). Is the above argument correct? or am I missing something?",,"['abstract-algebra', 'number-theory', 'p-adic-number-theory']"
2,"If $a^{-1} b^2 a=b^3$ and $b^{-1}a^2 b=a^3$ with $a,b\in G$, $G$ a group, then $a=b=1$. [duplicate]","If  and  with ,  a group, then . [duplicate]","a^{-1} b^2 a=b^3 b^{-1}a^2 b=a^3 a,b\in G G a=b=1","This question already has answers here : A Particular Two-Variable System in a Group (4 answers) Closed 8 years ago . I had this problem. Let $G$ be a group and $a,b\in G$ such that $a^{-1} b^2 a=b^3$ and $b^{-1}a^2 b=a^3$. Prove that $a=b=1$. Somehow I solved it, but it was a bit tedious (for example in some place I got to something like $b^3 =a^{-3}b^{-1}a^2a^{-3}b^{-1}a^2a^{-3}b^{-1}a^2=a^{-3}b^{-1}a^{-1}b^{-1}a^{-1}b^{-1}a^{2}$). I first proved that $b=a^{-2}ba^3$ (It's also true that $b=a^2ba^{-3}$) and then (Details not included) that $b^3=ab^6a^3$. Then (Details not included) that $b^3 a^3=a^{-1}b$, then $b^{-1}=a^4$ and from that $a^2=a^3$, from which $a=e$ (There are a lot of lines covering these details but there may be a mistake in these lines). It's the same to get $b=e$. So, is there any shorter, or less tedious way to prove it?","This question already has answers here : A Particular Two-Variable System in a Group (4 answers) Closed 8 years ago . I had this problem. Let $G$ be a group and $a,b\in G$ such that $a^{-1} b^2 a=b^3$ and $b^{-1}a^2 b=a^3$. Prove that $a=b=1$. Somehow I solved it, but it was a bit tedious (for example in some place I got to something like $b^3 =a^{-3}b^{-1}a^2a^{-3}b^{-1}a^2a^{-3}b^{-1}a^2=a^{-3}b^{-1}a^{-1}b^{-1}a^{-1}b^{-1}a^{2}$). I first proved that $b=a^{-2}ba^3$ (It's also true that $b=a^2ba^{-3}$) and then (Details not included) that $b^3=ab^6a^3$. Then (Details not included) that $b^3 a^3=a^{-1}b$, then $b^{-1}=a^4$ and from that $a^2=a^3$, from which $a=e$ (There are a lot of lines covering these details but there may be a mistake in these lines). It's the same to get $b=e$. So, is there any shorter, or less tedious way to prove it?",,"['abstract-algebra', 'group-theory']"
3,"A ""new"" formula relating the quotients of the upper central series, method of proof and background information","A ""new"" formula relating the quotients of the upper central series, method of proof and background information",,"For a finite group $G$ the upper central series is defined inductively by  $$  Z_1(G) := Z(G), \qquad Z_{i+1}(G) / Z_i(G) = Z( G / Z_i(G) ). $$ Now I am interested in generalising this formula, i.e. for $j > i$ do we have $$  Z_j(G) / Z_i(G) = Z_{j - i}( G / Z_i(G) ). $$ Of course for $j = i + 1$ this is just the definition. I have two proofs, the first is by direct computation, i.e. using $$  Z_{i+1}(G) = \{ x \in G : [x,y] \in Z_i(G) \mbox{ for all } y \in G \}. $$ Suppose inductively that $Z_{j-1}(G)/Z_i(G) = Z_{(j-1)-i}(G/Z_i(G))$ and $j > i+1$, then \begin{align*}  xZ_i(G) \in Z_j(G) / Z_i(G)    & \Leftrightarrow x \in Z_j(G) \\   & \Leftrightarrow \forall y \in G : [x,y] \in Z_{j-1}(G) \\   & \Leftrightarrow \forall y \in G : [x,y]Z_i(G) \in Z_{j-1}(G) / Z_i(G) \\   & \Leftrightarrow \forall y \in G : [x,y]Z_i(G) \in Z_{(j-1)-i}(G / Z_i(G)) \\   & \Leftrightarrow \forall y \in G : [xZ_i(G), yZ_i(G)] \in Z_{(j-1)-i}(G / Z_i(G)) \\   & \Leftrightarrow xZ_i(G) \in Z_{j-1}( G / Z_i(G) ). \end{align*} But I do not like that approach. As for subgroup series, using properties of subgroups like the Dedekind identity or isomorphism theorem are preferable, as they lead more easily to generalisations for other subgroup series, and I guess it might be possible to give such a proof here, without breaking it down to computing with commutators. I tried, and arrived at a second proof. Suppose inductively that for $j > i + 1$ $$  Z_{j-1}(G) / Z_i(G) = Z_{(j-1)-i}( G / Z_i(G) ) $$ then by taking the quotient of both sides of the formula we want to prove, the isomorphism theorems and using the definition for $j = i + 1$ $$  (Z_j(G) / Z_i(G)) / ( Z_{j-1}(G) / Z_i(G) ) \cong Z_j(G) / Z_{j-1}(G) = Z(G / Z_{j-1}(G))  $$ and by induction hypothesis and using the definition \begin{align*}  Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G)   & = Z_{j-i}( G / Z_i(G) ) / Z_{(j-1)-i}( G / Z_i(G) ) \\   & = Z( ( G / Z_i(G) ) / Z_{(j-1)-i}( G / Z_i(G) ) ) \\   & = Z( G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G) ) )  \\ \end{align*} and as $G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G)) \cong G / Z_{j-1}(G)$ we have $$  Z(G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G) ) ) ) \cong Z( G / Z_{j-1}(G) ). $$ So we have that both subgroups $$  ( Z_j(G) / Z_i(G) ) / ( Z_{j-1}(G) / Z_i(G) ) $$ and $$  Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) $$ are isomorphic to the same group, so in general we just get that they are isomorphic. But to conclude $(Z_j(G) / Z_i(G)) = Z_{j-i}( G / Z_i(G) )$, i.e. that their inverse images under the quotient are equal, we need more than isomorphism, we need equality. Looking at the proof of the isomorphism theorems, we see that a map realising the mentioned isomorphisms is $\varphi : G / Z_i(G) \to G / Z_{j-1}(G)$ with $$  xZ_i(G) \mapsto xZ_{j-1}(G). $$ Then the map $\overline \varphi(x \cdot \mbox{ker}(\varphi)) := \varphi(x)$ with $x \in G / Z_i(G)$ gives the isomorphism, and as $$  \overline{\varphi}( (Z_j(G) / Z_i(G)) / (Z_{j-1}(G) / Z_i(G) ) )  = \overline{\varphi}( Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) )  $$ and as $\overline{\varphi}$ is an isomorphism we have $$  (Z_j(G) / Z_i(G)) / (Z_{j-1}(G) / Z_i(G) )  = Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) $$ hence $Z_j(G) / Z_i(G) = Z_{j-i}( G / Z_i(G) )$. Okay, both proofs make me at least confident that the formula holds. But I do not like them both. The first is commutator computation, which I guess could be avoided by reasoning on the ""higher level"" of the subgroup series, in the other a specific map used in the proof of the isomorphism theorem is involved, which also takes some flavour of generality as the isomorphism theorem are not used plainly, but with respect to some arbitrary choice, i.e. a map realising the stated isomorphisms. So is there any proof of this formula which does not has these drawbacks? I also looked up the literature, but I nowhere found this formula. So maybe most authors consider it to trivial to state or do not mind it, I do not know but at least if it is trivial there must be some (preferable elegant) short proof. So specifically I am asking about an elegant proof of this formula, and maybe some background information. Is it known, used somewhere?","For a finite group $G$ the upper central series is defined inductively by  $$  Z_1(G) := Z(G), \qquad Z_{i+1}(G) / Z_i(G) = Z( G / Z_i(G) ). $$ Now I am interested in generalising this formula, i.e. for $j > i$ do we have $$  Z_j(G) / Z_i(G) = Z_{j - i}( G / Z_i(G) ). $$ Of course for $j = i + 1$ this is just the definition. I have two proofs, the first is by direct computation, i.e. using $$  Z_{i+1}(G) = \{ x \in G : [x,y] \in Z_i(G) \mbox{ for all } y \in G \}. $$ Suppose inductively that $Z_{j-1}(G)/Z_i(G) = Z_{(j-1)-i}(G/Z_i(G))$ and $j > i+1$, then \begin{align*}  xZ_i(G) \in Z_j(G) / Z_i(G)    & \Leftrightarrow x \in Z_j(G) \\   & \Leftrightarrow \forall y \in G : [x,y] \in Z_{j-1}(G) \\   & \Leftrightarrow \forall y \in G : [x,y]Z_i(G) \in Z_{j-1}(G) / Z_i(G) \\   & \Leftrightarrow \forall y \in G : [x,y]Z_i(G) \in Z_{(j-1)-i}(G / Z_i(G)) \\   & \Leftrightarrow \forall y \in G : [xZ_i(G), yZ_i(G)] \in Z_{(j-1)-i}(G / Z_i(G)) \\   & \Leftrightarrow xZ_i(G) \in Z_{j-1}( G / Z_i(G) ). \end{align*} But I do not like that approach. As for subgroup series, using properties of subgroups like the Dedekind identity or isomorphism theorem are preferable, as they lead more easily to generalisations for other subgroup series, and I guess it might be possible to give such a proof here, without breaking it down to computing with commutators. I tried, and arrived at a second proof. Suppose inductively that for $j > i + 1$ $$  Z_{j-1}(G) / Z_i(G) = Z_{(j-1)-i}( G / Z_i(G) ) $$ then by taking the quotient of both sides of the formula we want to prove, the isomorphism theorems and using the definition for $j = i + 1$ $$  (Z_j(G) / Z_i(G)) / ( Z_{j-1}(G) / Z_i(G) ) \cong Z_j(G) / Z_{j-1}(G) = Z(G / Z_{j-1}(G))  $$ and by induction hypothesis and using the definition \begin{align*}  Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G)   & = Z_{j-i}( G / Z_i(G) ) / Z_{(j-1)-i}( G / Z_i(G) ) \\   & = Z( ( G / Z_i(G) ) / Z_{(j-1)-i}( G / Z_i(G) ) ) \\   & = Z( G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G) ) )  \\ \end{align*} and as $G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G)) \cong G / Z_{j-1}(G)$ we have $$  Z(G / Z_i(G) / ( Z_{j-1}(G) / Z_i(G) ) ) ) \cong Z( G / Z_{j-1}(G) ). $$ So we have that both subgroups $$  ( Z_j(G) / Z_i(G) ) / ( Z_{j-1}(G) / Z_i(G) ) $$ and $$  Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) $$ are isomorphic to the same group, so in general we just get that they are isomorphic. But to conclude $(Z_j(G) / Z_i(G)) = Z_{j-i}( G / Z_i(G) )$, i.e. that their inverse images under the quotient are equal, we need more than isomorphism, we need equality. Looking at the proof of the isomorphism theorems, we see that a map realising the mentioned isomorphisms is $\varphi : G / Z_i(G) \to G / Z_{j-1}(G)$ with $$  xZ_i(G) \mapsto xZ_{j-1}(G). $$ Then the map $\overline \varphi(x \cdot \mbox{ker}(\varphi)) := \varphi(x)$ with $x \in G / Z_i(G)$ gives the isomorphism, and as $$  \overline{\varphi}( (Z_j(G) / Z_i(G)) / (Z_{j-1}(G) / Z_i(G) ) )  = \overline{\varphi}( Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) )  $$ and as $\overline{\varphi}$ is an isomorphism we have $$  (Z_j(G) / Z_i(G)) / (Z_{j-1}(G) / Z_i(G) )  = Z_{j-i}( G / Z_i(G) ) / Z_{j-1}(G) / Z_i(G) $$ hence $Z_j(G) / Z_i(G) = Z_{j-i}( G / Z_i(G) )$. Okay, both proofs make me at least confident that the formula holds. But I do not like them both. The first is commutator computation, which I guess could be avoided by reasoning on the ""higher level"" of the subgroup series, in the other a specific map used in the proof of the isomorphism theorem is involved, which also takes some flavour of generality as the isomorphism theorem are not used plainly, but with respect to some arbitrary choice, i.e. a map realising the stated isomorphisms. So is there any proof of this formula which does not has these drawbacks? I also looked up the literature, but I nowhere found this formula. So maybe most authors consider it to trivial to state or do not mind it, I do not know but at least if it is trivial there must be some (preferable elegant) short proof. So specifically I am asking about an elegant proof of this formula, and maybe some background information. Is it known, used somewhere?",,"['group-theory', 'finite-groups', 'abstract-algebra']"
4,Historical notes on the Jordan-Hölder program,Historical notes on the Jordan-Hölder program,,"I'm looking for any material (books, articles..) documenting the historical process of the formulation, partial work and/or the actual stage of the Jordan-Hölder program. I'm not sure if there is any ambiguity in defining the program, but I mean the program that had as its objectives: Classifying all finite simple groups Classifying all the possible ways to put up simple groups together (The actual 'Extension Problem') I'm currently studying the elementary topics which build up to the extension problem and I'm looking for the context of what I'm studying. Most of my material on math history (even those on algebra history) don't cover this topic. Actually, those who cover group theory focus mostly on Galois theory and some combinatorics. I have a feeling this is a very specific topic to document, and even though Math Exchange is a English board, for broadening sake foreing language material is also welcome (I would be especially grateful for material in English or French, which I can easily read, though if none is avaible I would gladly take some recess time to try and read other languages, math history and the program itself interest me much) Many thanks in advance.","I'm looking for any material (books, articles..) documenting the historical process of the formulation, partial work and/or the actual stage of the Jordan-Hölder program. I'm not sure if there is any ambiguity in defining the program, but I mean the program that had as its objectives: Classifying all finite simple groups Classifying all the possible ways to put up simple groups together (The actual 'Extension Problem') I'm currently studying the elementary topics which build up to the extension problem and I'm looking for the context of what I'm studying. Most of my material on math history (even those on algebra history) don't cover this topic. Actually, those who cover group theory focus mostly on Galois theory and some combinatorics. I have a feeling this is a very specific topic to document, and even though Math Exchange is a English board, for broadening sake foreing language material is also welcome (I would be especially grateful for material in English or French, which I can easily read, though if none is avaible I would gladly take some recess time to try and read other languages, math history and the program itself interest me much) Many thanks in advance.",,"['abstract-algebra', 'group-theory', 'reference-request', 'group-cohomology', 'simple-groups']"
5,"What is the intuition behind the name ""Flat modules""?","What is the intuition behind the name ""Flat modules""?",,"I am studying Atiyah and MacDonald's book ""Introduction to Commutative Algebra"" and I have just read the definition of a flat module. It seems to me that if they have called that kind of modules ""flat"" there must be some sort of geometric intuition  or something like that. Why are they called so? Or is it just a meaningless name? Thanks","I am studying Atiyah and MacDonald's book ""Introduction to Commutative Algebra"" and I have just read the definition of a flat module. It seems to me that if they have called that kind of modules ""flat"" there must be some sort of geometric intuition  or something like that. Why are they called so? Or is it just a meaningless name? Thanks",,"['commutative-algebra', 'definition', 'intuition']"
6,Finding and classifying all groups of order 12,Finding and classifying all groups of order 12,,"I was working on classifying all the groups of order 12. I dug around at some of the previous questions here and while they address the idea, none of them were entirely satisfactory: Classifying groups of order 12. (doesn't explain how classification is derived) Group of order 12 (doesn't address how to generate classification) Nonisomorphic groups of order 12. (shows they aren't isomorphic, but doesn't actually show the derivation for the 4 groups listed) So I wanted to ask, the specific question not yet presented, of how to derive that there must be 5 non isomorphic groups of order 12, and which groups those are. Work So Far: To begin with we have $|G|=12= 2^2 \times 3$ for any group of order 12. Let $n_3$ be the number of sylow-3 subgroups and let $n_2$ be the number of sylow 2 subgroups. We have by the third sylow theorem that $$n_3 | 4, n_3 \equiv 1 \mod 3$$ $$n_2 | 3, n_2 \equiv 1 \mod 2$$ So the groups can have either 1 or 4 sylow 3 subgroups of order 3, and either 1 or 3 sylow 2 subgroups of order 4. Furthermore coprime sylow-p groups only share the identity element in common, so we can rule out 4 sylow-3 groups and 3 sylow-2 groups, as the presence of either rules out the existence of a single copy of the other. Now we have established our groups must have a SINGLE sylow 2 subgroup, and a SINGLE sylow 3 subgroup, and that means that each is a normal subgroup of the entire group. The sylow-2 subgroup can be either $\Bbb{Z}_2 \times \Bbb{Z}_2$ or $\Bbb{Z}_4$. And the sylow-3 subgroup has a single contender $\Bbb{Z}_3$. Naturally then we can list out two groups $$\Bbb{Z}_2 \times \Bbb{Z}_2 \times \Bbb{Z}_3$$ $$\Bbb{Z}_4 \times \Bbb{Z}_3$$ But now the question remains, how to discover any remaining groups, and show that the remaining set covers all possible groups.a","I was working on classifying all the groups of order 12. I dug around at some of the previous questions here and while they address the idea, none of them were entirely satisfactory: Classifying groups of order 12. (doesn't explain how classification is derived) Group of order 12 (doesn't address how to generate classification) Nonisomorphic groups of order 12. (shows they aren't isomorphic, but doesn't actually show the derivation for the 4 groups listed) So I wanted to ask, the specific question not yet presented, of how to derive that there must be 5 non isomorphic groups of order 12, and which groups those are. Work So Far: To begin with we have $|G|=12= 2^2 \times 3$ for any group of order 12. Let $n_3$ be the number of sylow-3 subgroups and let $n_2$ be the number of sylow 2 subgroups. We have by the third sylow theorem that $$n_3 | 4, n_3 \equiv 1 \mod 3$$ $$n_2 | 3, n_2 \equiv 1 \mod 2$$ So the groups can have either 1 or 4 sylow 3 subgroups of order 3, and either 1 or 3 sylow 2 subgroups of order 4. Furthermore coprime sylow-p groups only share the identity element in common, so we can rule out 4 sylow-3 groups and 3 sylow-2 groups, as the presence of either rules out the existence of a single copy of the other. Now we have established our groups must have a SINGLE sylow 2 subgroup, and a SINGLE sylow 3 subgroup, and that means that each is a normal subgroup of the entire group. The sylow-2 subgroup can be either $\Bbb{Z}_2 \times \Bbb{Z}_2$ or $\Bbb{Z}_4$. And the sylow-3 subgroup has a single contender $\Bbb{Z}_3$. Naturally then we can list out two groups $$\Bbb{Z}_2 \times \Bbb{Z}_2 \times \Bbb{Z}_3$$ $$\Bbb{Z}_4 \times \Bbb{Z}_3$$ But now the question remains, how to discover any remaining groups, and show that the remaining set covers all possible groups.a",,"['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
7,Solving particular type of system of equations in $\mathbb R/\mathbb Z$,Solving particular type of system of equations in,\mathbb R/\mathbb Z,"I apologize in advance for the long post. You can freely skip to the last paragraph. I was motivated by this question given in 5th grade mathematics competition that I was solving with my advanced students: Three satellites are orbiting the Earth. The first one circles around the Earth in $75$ minutes, the second one in $105$ minutes and the third one in $135$ minutes. How much time is needed, counting from the moment they simultaneously passed over South Pole, to meet again at the same spot. Now, obviously, the answer is $\operatorname{lcm}(75,105,135) = 4725$. Let us now assume the orbits are coplanar and we are trying to determine when the satellites are going to first align. Like in previous example let us assume they start at the same point and assume uniform circular motion.  It seems natural to work inside $\mathbb R/\mathbb Z$, and convert time to frequency. So, we need to find the smallest $T\in\mathbb R_{>0}$ such that $$f_1T +\mathbb Z = f_2T +\mathbb Z =\ldots =f_nT + \mathbb Z\tag{1}$$ This is equivalent to finding the smallest $T\in\mathbb R_{>0}$ such that $$(f_k - f_1)T\in\mathbb Z,\quad\forall k=2,3,\ldots,n$$ or even more conveniently, equivalent to finding integers $m_k\neq 0$ such that $$\frac{m_2}{f_2 - f_1} = \frac{m_3}{f_3 - f_1} = \ldots = \frac{m_n}{f_n - f_1}\tag{2}$$ This is pretty nice as it will lead to computing $T$. But, before I continue, let me also note that now we have that $T$ is the generator of $\displaystyle\bigcap_{k=2}^n \frac 1{f_k-f_1}\mathbb Z.$ This is not surprising at all, actually, as satellites $1$ and $k$ will meet (align) exactly at times $$T = \frac m{f_k-f_1},\, m\in\mathbb Z$$ so above intersection will exactly correspond to the times all satellites meet simultaneously. Returning to $(2)$, we can conclude that $$\frac{f_k-f_1}{f_2-f_1}=\frac{m_k}{m_2}\in\mathbb Q,\quad\forall k = 2,\ldots,n$$ Thus, $(2)$ can be solved if and only if $$\frac{f_k-f_1}{f_2-f_1}\in\mathbb Q,\quad\forall k = 2,\ldots,n\tag{3}$$ This actually gives us computational method, as I mentioned earlier: 1) Calculate $\frac{f_k-f_1}{f_2-f_1} = \frac {a_k}{b_k}$ for all $k$, where $a_k$ and $b_k$ are relatively prime. 2) Set $T = \frac{l}{f_2 - f_1}$, where $l = \operatorname{lcm}(b_3,\ldots,b_n)$ This works because we have $$\frac{1}{f_2 - f_1} = \frac{\frac{a_3}{b_3}}{f_3 - f_1} = \ldots = \frac{\frac{a_n}{b_n}}{f_n - f_1}\implies \frac{l}{f_2 - f_1} = \frac{m_3}{f_3 - f_1} = \ldots = \frac{m_n}{f_n - f_1}$$ where $m_k = l\frac{a_k}{b_k},\, k>2$, and such $T$ is really the smallest because $m_3,\ldots,m_n$ are relatively prime. Also, note that we have nice necessary condition to have a solution in the first place: $$\frac{f_k-f_1}{f_2-f_1} = q_k \implies f_k = (1-q_k)f_1 + q_kf_2\implies f_k\in\mathbb Q(f_1,f_2),\quad k >2$$ so we can immediately conclude that, for example, satellites with frequencies $$f_1 = 1,\ f_2=\sqrt 2,\ f_3=\sqrt 3$$ will never simultaneously meet after starting from the same point. I was positively surprised that such a problem (in physics?) can be completely characterized (and computed) algebraically. I tried to generalize this to the problem when satellites do not necessarily start at the same point, but had no luck. So, after the long introduction, my question is: For fixed positive constants $f_1,\ldots,f_n$ (such that $f_i\neq f_j$ for $i\neq j$) and $\alpha_1,\ldots,\alpha_n$, can we find $T\in\mathbb R_{>0}$ such that $$f_1T + \alpha_1 + \mathbb Z = f_2T + \alpha_2 + \mathbb Z = \ldots = f_nT + \alpha_n + \mathbb Z$$ in general or at least find necessary conditions for existence? Examples: 1) For $n=2$ and $\alpha_1 = \alpha_2 = 0$ $$fT + \mathbb Z = gT+\mathbb Z$$ has solutions $T = \frac m{f-g}$ for any $m\in\mathbb Z$. 2) For $\alpha_1 = \alpha_2 = \alpha_3 = 0$ and $f_1 = 1$, $f_2 = \sqrt 2$, $f_3 = \sqrt 3$ there are no solutions to $$T+\mathbb Z = T\sqrt 2+\mathbb Z = T\sqrt 3+\mathbb Z$$ as previously noted. 3) Another example might be $$T + \mathbb Z = T\sqrt 2 +\sqrt 3+\mathbb Z = T\sqrt 3 +\frac 1 2+\mathbb Z$$ This kind of system interests me most, as (I think) I have solved the problem for trivial $\alpha$'s. P.S. If you read my long introduction, feel free to correct any mistakes, or suggest better method.","I apologize in advance for the long post. You can freely skip to the last paragraph. I was motivated by this question given in 5th grade mathematics competition that I was solving with my advanced students: Three satellites are orbiting the Earth. The first one circles around the Earth in $75$ minutes, the second one in $105$ minutes and the third one in $135$ minutes. How much time is needed, counting from the moment they simultaneously passed over South Pole, to meet again at the same spot. Now, obviously, the answer is $\operatorname{lcm}(75,105,135) = 4725$. Let us now assume the orbits are coplanar and we are trying to determine when the satellites are going to first align. Like in previous example let us assume they start at the same point and assume uniform circular motion.  It seems natural to work inside $\mathbb R/\mathbb Z$, and convert time to frequency. So, we need to find the smallest $T\in\mathbb R_{>0}$ such that $$f_1T +\mathbb Z = f_2T +\mathbb Z =\ldots =f_nT + \mathbb Z\tag{1}$$ This is equivalent to finding the smallest $T\in\mathbb R_{>0}$ such that $$(f_k - f_1)T\in\mathbb Z,\quad\forall k=2,3,\ldots,n$$ or even more conveniently, equivalent to finding integers $m_k\neq 0$ such that $$\frac{m_2}{f_2 - f_1} = \frac{m_3}{f_3 - f_1} = \ldots = \frac{m_n}{f_n - f_1}\tag{2}$$ This is pretty nice as it will lead to computing $T$. But, before I continue, let me also note that now we have that $T$ is the generator of $\displaystyle\bigcap_{k=2}^n \frac 1{f_k-f_1}\mathbb Z.$ This is not surprising at all, actually, as satellites $1$ and $k$ will meet (align) exactly at times $$T = \frac m{f_k-f_1},\, m\in\mathbb Z$$ so above intersection will exactly correspond to the times all satellites meet simultaneously. Returning to $(2)$, we can conclude that $$\frac{f_k-f_1}{f_2-f_1}=\frac{m_k}{m_2}\in\mathbb Q,\quad\forall k = 2,\ldots,n$$ Thus, $(2)$ can be solved if and only if $$\frac{f_k-f_1}{f_2-f_1}\in\mathbb Q,\quad\forall k = 2,\ldots,n\tag{3}$$ This actually gives us computational method, as I mentioned earlier: 1) Calculate $\frac{f_k-f_1}{f_2-f_1} = \frac {a_k}{b_k}$ for all $k$, where $a_k$ and $b_k$ are relatively prime. 2) Set $T = \frac{l}{f_2 - f_1}$, where $l = \operatorname{lcm}(b_3,\ldots,b_n)$ This works because we have $$\frac{1}{f_2 - f_1} = \frac{\frac{a_3}{b_3}}{f_3 - f_1} = \ldots = \frac{\frac{a_n}{b_n}}{f_n - f_1}\implies \frac{l}{f_2 - f_1} = \frac{m_3}{f_3 - f_1} = \ldots = \frac{m_n}{f_n - f_1}$$ where $m_k = l\frac{a_k}{b_k},\, k>2$, and such $T$ is really the smallest because $m_3,\ldots,m_n$ are relatively prime. Also, note that we have nice necessary condition to have a solution in the first place: $$\frac{f_k-f_1}{f_2-f_1} = q_k \implies f_k = (1-q_k)f_1 + q_kf_2\implies f_k\in\mathbb Q(f_1,f_2),\quad k >2$$ so we can immediately conclude that, for example, satellites with frequencies $$f_1 = 1,\ f_2=\sqrt 2,\ f_3=\sqrt 3$$ will never simultaneously meet after starting from the same point. I was positively surprised that such a problem (in physics?) can be completely characterized (and computed) algebraically. I tried to generalize this to the problem when satellites do not necessarily start at the same point, but had no luck. So, after the long introduction, my question is: For fixed positive constants $f_1,\ldots,f_n$ (such that $f_i\neq f_j$ for $i\neq j$) and $\alpha_1,\ldots,\alpha_n$, can we find $T\in\mathbb R_{>0}$ such that $$f_1T + \alpha_1 + \mathbb Z = f_2T + \alpha_2 + \mathbb Z = \ldots = f_nT + \alpha_n + \mathbb Z$$ in general or at least find necessary conditions for existence? Examples: 1) For $n=2$ and $\alpha_1 = \alpha_2 = 0$ $$fT + \mathbb Z = gT+\mathbb Z$$ has solutions $T = \frac m{f-g}$ for any $m\in\mathbb Z$. 2) For $\alpha_1 = \alpha_2 = \alpha_3 = 0$ and $f_1 = 1$, $f_2 = \sqrt 2$, $f_3 = \sqrt 3$ there are no solutions to $$T+\mathbb Z = T\sqrt 2+\mathbb Z = T\sqrt 3+\mathbb Z$$ as previously noted. 3) Another example might be $$T + \mathbb Z = T\sqrt 2 +\sqrt 3+\mathbb Z = T\sqrt 3 +\frac 1 2+\mathbb Z$$ This kind of system interests me most, as (I think) I have solved the problem for trivial $\alpha$'s. P.S. If you read my long introduction, feel free to correct any mistakes, or suggest better method.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
8,"Tensor product of $k$-algebras, center, isomorphism.","Tensor product of -algebras, center, isomorphism.",k,"Let $A$, $B$ be two $k$-algebras of finite dimension, where $k$ is a field. Here, $A$ and $B$ are not necessarily commutative. Do we have that$$Z(A \otimes_k B) \cong Z(A) \otimes_k Z(B),$$where $Z(-)$ is the center?","Let $A$, $B$ be two $k$-algebras of finite dimension, where $k$ is a field. Here, $A$ and $B$ are not necessarily commutative. Do we have that$$Z(A \otimes_k B) \cong Z(A) \otimes_k Z(B),$$where $Z(-)$ is the center?",,"['abstract-algebra', 'homological-algebra']"
9,The Battleship Problem: sequences $x$ such that $\{tv+x_t\pmod n:x_t\in x\}=\mathbb{Z}_n$ for all $v \in \mathbb{Z}_n$.,The Battleship Problem: sequences  such that  for all .,x \{tv+x_t\pmod n:x_t\in x\}=\mathbb{Z}_n v \in \mathbb{Z}_n,"A coworker posed this problem as ""The Battleship Problem"": A battleship starts at some unknown initial location $i\in\mathbb{Z}_n$ and moves at a constant velocity $v$ each turn. Each turn $t$ you may drop a bomb on one predetermined location, and the ship is destroyed if it occupies that position (that is, if $x_t=tv+i\pmod n$). What is the fewest number of turns required to guarantee that you destroy the ship? Trivially it can be done by dropping $n$ bombs on each location, because after $n$ turns the ship is always back where it started. It turns out that it can actually be done in $2n-1$ (or less) turns for all $n<35$ (this was discovered by brute-force search). Is this true for all $n$? I can show that it is true for primes as follows: $x_t=0$ for $0\leq t<n$ (all $v\neq 0$ are generators of $\mathbb{Z}_n$). $x_t=t$ for $n\leq t<2n$ (all locations for $v=0$). The zero common to both parts can be eliminated for a total length of $2n-1$. And similar logic shows that the same sequence works for prime powers. I have not been able to come up with anything for odd composite numbers. What I'm hoping for is an algorithm to generate sequences given $n$, or keywords I can research to make progress on that.","A coworker posed this problem as ""The Battleship Problem"": A battleship starts at some unknown initial location $i\in\mathbb{Z}_n$ and moves at a constant velocity $v$ each turn. Each turn $t$ you may drop a bomb on one predetermined location, and the ship is destroyed if it occupies that position (that is, if $x_t=tv+i\pmod n$). What is the fewest number of turns required to guarantee that you destroy the ship? Trivially it can be done by dropping $n$ bombs on each location, because after $n$ turns the ship is always back where it started. It turns out that it can actually be done in $2n-1$ (or less) turns for all $n<35$ (this was discovered by brute-force search). Is this true for all $n$? I can show that it is true for primes as follows: $x_t=0$ for $0\leq t<n$ (all $v\neq 0$ are generators of $\mathbb{Z}_n$). $x_t=t$ for $n\leq t<2n$ (all locations for $v=0$). The zero common to both parts can be eliminated for a total length of $2n-1$. And similar logic shows that the same sequence works for prime powers. I have not been able to come up with anything for odd composite numbers. What I'm hoping for is an algorithm to generate sequences given $n$, or keywords I can research to make progress on that.",,['abstract-algebra']
10,Proving $\mathrm{Aut}(S_{n}) = S_{n}$ for $n > 6$,Proving  for,\mathrm{Aut}(S_{n}) = S_{n} n > 6,"This is a question for a school assignment. We are being asked to prove that  $\mathrm{Aut}(S_{n}) = S_{n}$ for $n > 6$. These are the steps we are supposed to follow in our proof. Prove that an automorphism of $Sn$ takes an element of order $2$ to an element of order $2$. My attempt: Let $x$ be an element of order $2$, now we want to show that $f(x)$ is also an element of order $2$. So $x^2=e$, $f(x)^2=f(x)f(x)=f(x^2)=f(e)=e$ . So we know $f(x)$ is of order $2$. For $n > 6$ use an argument involving centralizers to show that an automorphism of $S_{n}$ takes a transposition to a transposition. Can we just consider that a transposition is an element of order $2$ and this would follow from part 1? Prove that every automorphism has the effect $(1\;2) \mapsto (a\;b_{2})$, $(1\;3) \mapsto (a\;b_{3})$, $...$, $(1\;n) \mapsto(a\;b_{n})$, for some distinct $a,b_{2}, ..., b_{n} ∈ {1, 2, ..., n}$. Conclude that $|\mathrm{Aut}(S_{n})| \le n!$. Show that for $n > 6$ there is an isomorphism $S_{n} \cong \mathrm{Aut}(S_{n})$. For parts 3 and 4 I'm very stumped. Any help would be appreciated. Or even just to point me in the right direction. Thank you.","This is a question for a school assignment. We are being asked to prove that  $\mathrm{Aut}(S_{n}) = S_{n}$ for $n > 6$. These are the steps we are supposed to follow in our proof. Prove that an automorphism of $Sn$ takes an element of order $2$ to an element of order $2$. My attempt: Let $x$ be an element of order $2$, now we want to show that $f(x)$ is also an element of order $2$. So $x^2=e$, $f(x)^2=f(x)f(x)=f(x^2)=f(e)=e$ . So we know $f(x)$ is of order $2$. For $n > 6$ use an argument involving centralizers to show that an automorphism of $S_{n}$ takes a transposition to a transposition. Can we just consider that a transposition is an element of order $2$ and this would follow from part 1? Prove that every automorphism has the effect $(1\;2) \mapsto (a\;b_{2})$, $(1\;3) \mapsto (a\;b_{3})$, $...$, $(1\;n) \mapsto(a\;b_{n})$, for some distinct $a,b_{2}, ..., b_{n} ∈ {1, 2, ..., n}$. Conclude that $|\mathrm{Aut}(S_{n})| \le n!$. Show that for $n > 6$ there is an isomorphism $S_{n} \cong \mathrm{Aut}(S_{n})$. For parts 3 and 4 I'm very stumped. Any help would be appreciated. Or even just to point me in the right direction. Thank you.",,"['abstract-algebra', 'group-theory', 'group-homomorphism', 'automorphism-group']"
11,How many tubes can you balance in a centrifuge?,How many tubes can you balance in a centrifuge?,,"I recently learned that if you have a centrifuge whose number of holes $n$ is divisible by $6$, then you can balance any number of tubes except for $1$ and $n-1$. If $k$, the number of tubes you want to balance, is even, just put them in $k/2$ pairs of opposite holes. If $k$ is odd and bigger than $1$ and less than $n-1$, then you can place $3$ tubes in an equilateral triangle, leaving $n/2-3$ unoccupied pairs of holes for the remaining $(k-1)/2$ tubes. I was wondering if we could solve the problem for a general $n$. To make this more formal, let $\zeta_n$ be a primitive $n$th root of unity. We can balance $k$ tubes in a centrifuge of $n$ holes if there exists a subset of $\{1, \zeta_n,\ldots, \zeta_n^{n-1}\}$ whose sum is $0$. If $n=p$ is prime, then you can only balance $0$ or $p$ tubes, since given a nonempty proper subset of the $p$th root of unity WLOG we can choose $\zeta_p$ so that $\zeta_p^{p-1}$ is not in the subset (unless the subset is everything except $1$, which clearly doesn't sum to $0$), and the zero sum gives a nonzero polynomial in $\zeta_p$ of degree less than $p-1$, contradicting $1+x+\cdots+x^{p-1}$ being the minimal polynomial of $\zeta_p$. I've made a few more observations: in general, if $k$ and $n$ have a common factor greater than $1$, then you can balance $k$ tubes in a centrifuge with $n$ holes (just take the disjoint union of $n/k$ cyclically symmetric $k$-tuples). I suspect this is the only possibility if $n=p^m$ is a prime power (i.e., $k$ must be divisible by $p$). Furthermore, I suspect that if $n=pq$ is the product of distinct primes, then only multiples of $p$ and multiples of $q$ work: I couldn't do $k=p+q$ as a disjoint union, since if you take the $q$ roots whose powers are the multiples of $p$, then each is unique mod $q$, so any cyclically symmetric $p$-tuple will intersect with the $q$-tuple. In general, I suspect that the only way to balance tubes is to start with cyclically symmetric arrangements and take successive disjoint unions and set differences. Has this been shown?","I recently learned that if you have a centrifuge whose number of holes $n$ is divisible by $6$, then you can balance any number of tubes except for $1$ and $n-1$. If $k$, the number of tubes you want to balance, is even, just put them in $k/2$ pairs of opposite holes. If $k$ is odd and bigger than $1$ and less than $n-1$, then you can place $3$ tubes in an equilateral triangle, leaving $n/2-3$ unoccupied pairs of holes for the remaining $(k-1)/2$ tubes. I was wondering if we could solve the problem for a general $n$. To make this more formal, let $\zeta_n$ be a primitive $n$th root of unity. We can balance $k$ tubes in a centrifuge of $n$ holes if there exists a subset of $\{1, \zeta_n,\ldots, \zeta_n^{n-1}\}$ whose sum is $0$. If $n=p$ is prime, then you can only balance $0$ or $p$ tubes, since given a nonempty proper subset of the $p$th root of unity WLOG we can choose $\zeta_p$ so that $\zeta_p^{p-1}$ is not in the subset (unless the subset is everything except $1$, which clearly doesn't sum to $0$), and the zero sum gives a nonzero polynomial in $\zeta_p$ of degree less than $p-1$, contradicting $1+x+\cdots+x^{p-1}$ being the minimal polynomial of $\zeta_p$. I've made a few more observations: in general, if $k$ and $n$ have a common factor greater than $1$, then you can balance $k$ tubes in a centrifuge with $n$ holes (just take the disjoint union of $n/k$ cyclically symmetric $k$-tuples). I suspect this is the only possibility if $n=p^m$ is a prime power (i.e., $k$ must be divisible by $p$). Furthermore, I suspect that if $n=pq$ is the product of distinct primes, then only multiples of $p$ and multiples of $q$ work: I couldn't do $k=p+q$ as a disjoint union, since if you take the $q$ roots whose powers are the multiples of $p$, then each is unique mod $q$, so any cyclically symmetric $p$-tuple will intersect with the $q$-tuple. In general, I suspect that the only way to balance tubes is to start with cyclically symmetric arrangements and take successive disjoint unions and set differences. Has this been shown?",,"['abstract-algebra', 'elementary-number-theory', 'roots-of-unity']"
12,What is number of group homomorphisms from $D_{12}$ to $D_{18}$?,What is number of group homomorphisms from  to ?,D_{12} D_{18},"I am willing to find out the number of group homomorphisms from $D_{12}$ to $D_{18}$ where $D_m:=\langle r_m, f_m: r_m^m=f_m^2=(r_mf_m)^2=e_m \rangle$ is the standrard dihedral group of order $2m$. Basically I am motivated from this paper . I have studied and understood the first two cases. But for the third case, I am not getting any clue. SO this is why I have started numerical example. Let $\rho:D_{12}\rightarrow D_{18}$ be a group homomoprhism. So we can say that $\#$Hom$(D_{12}, D_{18})=$ trivial + non-trivial group homomoprhisms. Here non-trivial group homomorphisms = number of possible choice for $\rho(f_m)\times$ number of possible choices for $\rho(r_m)$. here according to rule it is supposed to be $4+4\times 18+18\times (12,18)$. BUt I want to understand the answer step by step. So I started from the begining. Here $\rho(r_{12})$ has the possibilities $e_{18}, r_{18}^\alpha$ with $1\leq \alpha\leq 17$. BUt I dont understand why in the paper it is said another possibilities $r_{18}^\beta f_{18}$. How to count the homomorphisms ? Please enlighten me. thanks in advance My Attempt:  Hom$(D_{12}, D_{18})=Hom(D_{12}/[D_{12}, D_{12}], D_{18})=Hom(\mathbb Z_2, D_{18})$. And we know that $\#Hom(\mathbb Z_n, G)=\#\{x\in G: x^n = e_G\}$. SO if we use this theorem, we then can conclude that required number of nothing but $\#\{x\in D_{18}: x^2=e\}$. Is it correct ? Not sure if this will give me correct answer according to the formula provided in the paper stated above","I am willing to find out the number of group homomorphisms from $D_{12}$ to $D_{18}$ where $D_m:=\langle r_m, f_m: r_m^m=f_m^2=(r_mf_m)^2=e_m \rangle$ is the standrard dihedral group of order $2m$. Basically I am motivated from this paper . I have studied and understood the first two cases. But for the third case, I am not getting any clue. SO this is why I have started numerical example. Let $\rho:D_{12}\rightarrow D_{18}$ be a group homomoprhism. So we can say that $\#$Hom$(D_{12}, D_{18})=$ trivial + non-trivial group homomoprhisms. Here non-trivial group homomorphisms = number of possible choice for $\rho(f_m)\times$ number of possible choices for $\rho(r_m)$. here according to rule it is supposed to be $4+4\times 18+18\times (12,18)$. BUt I want to understand the answer step by step. So I started from the begining. Here $\rho(r_{12})$ has the possibilities $e_{18}, r_{18}^\alpha$ with $1\leq \alpha\leq 17$. BUt I dont understand why in the paper it is said another possibilities $r_{18}^\beta f_{18}$. How to count the homomorphisms ? Please enlighten me. thanks in advance My Attempt:  Hom$(D_{12}, D_{18})=Hom(D_{12}/[D_{12}, D_{12}], D_{18})=Hom(\mathbb Z_2, D_{18})$. And we know that $\#Hom(\mathbb Z_n, G)=\#\{x\in G: x^n = e_G\}$. SO if we use this theorem, we then can conclude that required number of nothing but $\#\{x\in D_{18}: x^2=e\}$. Is it correct ? Not sure if this will give me correct answer according to the formula provided in the paper stated above",,"['abstract-algebra', 'group-theory']"
13,Milnor patching for general modules,Milnor patching for general modules,,"The Milnor patching theorem for projective modules is the following statement. Given a pullback diagram of rings  $$ \begin{array}{} R  & \xrightarrow{f_2} & R_2  \\ \downarrow{f_1} & & \downarrow{j_2} \\ R_1 & \xrightarrow{j_1} & R_3  \end{array} $$ with $j_1$ or $j_2$ surjective, and projective modules $P_1$ and $P_2$ over $R_1$ and $R_2$ respectively together with an isomorphism $h : R_3 \otimes_{R_2} P_2 \to R_3 \otimes_{R_1} P_1$, then 1) the $R$-module $P$ as the pullback of $$ \begin{array}{} P  & \xrightarrow{} & P_2  \\ \downarrow{} & & \downarrow{h(1 \otimes \text{id})} \\ P_1 & \xrightarrow{1 \otimes \text{id}} & R_3 \otimes_{R_1} P_1 \end{array} $$ is projective. If $P_1$ and $P_2$ are finitely generated over $R_1$ and $R_2$ respectively, then $P$ is finitely generated. 2) There are natural isomorphisms $P \otimes_R R_1 \to P_1$ and $P \otimes_R R_2 \to P_2$. 3) All projective $R$-modules arise for appropriately chosen $P_1$, $P_2$ and $h$. This theorem gives an equivalence of categories of projective modules on $R$, and ""patching data"" of $P_1$, $P_2$ and $h$ above. Specifically, the equivalence may be described as follows. If we let $(P_1,P_2,h)$, $(P_1',P_2',h')$ be such patching data, a morphism of such triples are homorphisms $\phi_1 : P_1 \to P_1'$ and $\phi_2 : P_2 \to P_2'$ such that  $$ \begin{array}{} R_3 \otimes_{R_2} P_2  & \xrightarrow{1 \otimes \phi_2} & R_3 \otimes_{R_2} P_2'  \\ \downarrow{h} & & \downarrow{h'}\\ R_3 \otimes_{R_1} P_1 & \xrightarrow{1 \otimes \phi_1} & R_3 \otimes_{R_1} P_1' \end{array} $$ commutes. A projective module $P$ on $R$ yields a triple $(R_1 \otimes_R P, R_2 \otimes_R P, \text{id})$, where $\text{id}$ denotes the identity map $R_3 \otimes_R P \to R_3 \otimes P$. A morphism of modules $P \to P'$ induces a morphism of triples in the obvious way. My question is whether there exists a similar result for finitely generated modules (and modules in general), that is, not assuming $P_1$ and $P_2$ projective, with reasonable assumptions on $j_1$ and $j_2$. Optimally, no stronger conditions though. I would like an equivalence of categories of finitely generated $R$-modules and a certain notion of ""patching data"" in this context. For example, perhaps by allowing $h$ to be any homomorphism would yield the corresponding result for finitely generated modules over $R$?","The Milnor patching theorem for projective modules is the following statement. Given a pullback diagram of rings  $$ \begin{array}{} R  & \xrightarrow{f_2} & R_2  \\ \downarrow{f_1} & & \downarrow{j_2} \\ R_1 & \xrightarrow{j_1} & R_3  \end{array} $$ with $j_1$ or $j_2$ surjective, and projective modules $P_1$ and $P_2$ over $R_1$ and $R_2$ respectively together with an isomorphism $h : R_3 \otimes_{R_2} P_2 \to R_3 \otimes_{R_1} P_1$, then 1) the $R$-module $P$ as the pullback of $$ \begin{array}{} P  & \xrightarrow{} & P_2  \\ \downarrow{} & & \downarrow{h(1 \otimes \text{id})} \\ P_1 & \xrightarrow{1 \otimes \text{id}} & R_3 \otimes_{R_1} P_1 \end{array} $$ is projective. If $P_1$ and $P_2$ are finitely generated over $R_1$ and $R_2$ respectively, then $P$ is finitely generated. 2) There are natural isomorphisms $P \otimes_R R_1 \to P_1$ and $P \otimes_R R_2 \to P_2$. 3) All projective $R$-modules arise for appropriately chosen $P_1$, $P_2$ and $h$. This theorem gives an equivalence of categories of projective modules on $R$, and ""patching data"" of $P_1$, $P_2$ and $h$ above. Specifically, the equivalence may be described as follows. If we let $(P_1,P_2,h)$, $(P_1',P_2',h')$ be such patching data, a morphism of such triples are homorphisms $\phi_1 : P_1 \to P_1'$ and $\phi_2 : P_2 \to P_2'$ such that  $$ \begin{array}{} R_3 \otimes_{R_2} P_2  & \xrightarrow{1 \otimes \phi_2} & R_3 \otimes_{R_2} P_2'  \\ \downarrow{h} & & \downarrow{h'}\\ R_3 \otimes_{R_1} P_1 & \xrightarrow{1 \otimes \phi_1} & R_3 \otimes_{R_1} P_1' \end{array} $$ commutes. A projective module $P$ on $R$ yields a triple $(R_1 \otimes_R P, R_2 \otimes_R P, \text{id})$, where $\text{id}$ denotes the identity map $R_3 \otimes_R P \to R_3 \otimes P$. A morphism of modules $P \to P'$ induces a morphism of triples in the obvious way. My question is whether there exists a similar result for finitely generated modules (and modules in general), that is, not assuming $P_1$ and $P_2$ projective, with reasonable assumptions on $j_1$ and $j_2$. Optimally, no stronger conditions though. I would like an equivalence of categories of finitely generated $R$-modules and a certain notion of ""patching data"" in this context. For example, perhaps by allowing $h$ to be any homomorphism would yield the corresponding result for finitely generated modules over $R$?",,"['abstract-algebra', 'commutative-algebra', 'modules', 'projective-module']"
14,$R$ is semisimple iff it is Artinian and $J(R) = 0$,is semisimple iff it is Artinian and,R J(R) = 0,"Let $R$ be a ring with identity. The ring $R$ is semisimple if it is semisimple as a left $R$ module. A module $M$ is semisimple if it can be expressed as a direct sum of simple submodules. The Jacobson radical of $R$ , denoted by $J(R)$ , is the intersection of all maximal left ideals of $R$ . I am asked to prove this: The ring $R$ is semisimple if and only if it is Artinian and $J(R) = 0$ . I have proved the $\implies$ part. How to prove the converse? Since $R$ is Artinian every collection of left ideals of $R$ has a minimal element. How to go from here?","Let be a ring with identity. The ring is semisimple if it is semisimple as a left module. A module is semisimple if it can be expressed as a direct sum of simple submodules. The Jacobson radical of , denoted by , is the intersection of all maximal left ideals of . I am asked to prove this: The ring is semisimple if and only if it is Artinian and . I have proved the part. How to prove the converse? Since is Artinian every collection of left ideals of has a minimal element. How to go from here?",R R R M R J(R) R R J(R) = 0 \implies R R,"['abstract-algebra', 'ring-theory', 'artinian', 'semi-simple-rings']"
15,How to find the Galois group of a given polynomial from Galois viewpoint?,How to find the Galois group of a given polynomial from Galois viewpoint?,,"Excerpt from Page 3 Consider the polynomial equation   \begin{equation*}   x^4 - 2 = 0, \end{equation*}   with integer coefficients. We have four solutions:   \begin{equation*}   \alpha = \sqrt[4]{2},\quad    \beta = i \sqrt[4]{2},\quad   \gamma = -\sqrt[4]{2},\quad   \delta = -i \sqrt[4]{2},  \end{equation*}   Note that two of these solutions are not real. Now let us list some equations   that these four roots satisfy:   \begin{equation*}   \alpha + \gamma = 0,\quad   \alpha \beta \gamma \delta = -2,\quad   \alpha \beta - \gamma \delta = 0, \dotsc \end{equation*} What happens if we swap $ \alpha $ and $ \gamma $ in this list? We get   \begin{equation*}   \gamma + \alpha = 0, \quad   \gamma \beta \alpha \delta = -2,\quad   \gamma \beta - \alpha \delta = 0, \dotsc \end{equation*}   which are all still valid. Likewise we could permute the variables as follows   \begin{equation*}  \alpha \mapsto \beta \mapsto \gamma \mapsto \delta  \end{equation*}   You can check that the equations above remain valid. So, does any such   permutation work? For any such equation? The answer is ""no"": try, for   instance, swapping $ \beta $ and $ \gamma $,   and you find that the first of the equations above becomes false. The set of permutations of the set $ \{\alpha, \beta, \gamma, \delta \} $   that preserve the validity of all polynomial equations (with coefficients in    $ \mathbb{Q} $) in these variables is called the Galois group of the   equation $ x^4 = 2 $. This group is the symmetric group of the square (which   makes sense if you look at the location of $ \alpha, \beta, \gamma $,    and $ \delta $ in the Complex plane. It is also known as $ D_8 $,   the dihedral group of order 8. Question 1 : In order to determine the permutations of the set $ \{ \alpha, \beta, \gamma, \delta \} $ that preserve the validity of all polynomial equations (with coefficients in Q) in these variables, do I need to list out all such equations (how would I do it), and how do tell the list is exhaustive and then only determine such permutations? Question 2 : Following Question 1, the coefficients of a polynomial form  symmetric polynomials as functions of the roots. How this help in determining the correct permutations? Using this as example, the Galois group is isomorphic to the Klein 4-group: $ x^4 - 5x + 6 = 0 $ factorize as $ (x^2 - 3)(x^2 - 2) = (x^2 - r_1 r_2)(x^2 - r_3 r_4) $ and the permutation that leave the polynomial unchanged is the identity, $ (12), (34) $ and $ (12)(34) $. Other permutations turn it into different polynomials and do not belong the Galois group of the permutation? Question 3 : Am I correct to say that a permutation belongs to the Galois group of the polynomial if it permutes the roots in such a way that when we multiply out, we obtain the polynomial that we started with? Question 4 : How would Galois himself have determined the ""Galois group"" of a given polynomial? Question 5 : Could you please point out where I could find materials on how Galois solved the insolvability of quintic using materials available during his time, instead of the automorphism of splitting field approach?  My google search has been unsatisfatory. The textbooks by Cox, Tignol, Rotman, Swallow are not easy to me and do not have the discussions I am comfortable with.","Excerpt from Page 3 Consider the polynomial equation   \begin{equation*}   x^4 - 2 = 0, \end{equation*}   with integer coefficients. We have four solutions:   \begin{equation*}   \alpha = \sqrt[4]{2},\quad    \beta = i \sqrt[4]{2},\quad   \gamma = -\sqrt[4]{2},\quad   \delta = -i \sqrt[4]{2},  \end{equation*}   Note that two of these solutions are not real. Now let us list some equations   that these four roots satisfy:   \begin{equation*}   \alpha + \gamma = 0,\quad   \alpha \beta \gamma \delta = -2,\quad   \alpha \beta - \gamma \delta = 0, \dotsc \end{equation*} What happens if we swap $ \alpha $ and $ \gamma $ in this list? We get   \begin{equation*}   \gamma + \alpha = 0, \quad   \gamma \beta \alpha \delta = -2,\quad   \gamma \beta - \alpha \delta = 0, \dotsc \end{equation*}   which are all still valid. Likewise we could permute the variables as follows   \begin{equation*}  \alpha \mapsto \beta \mapsto \gamma \mapsto \delta  \end{equation*}   You can check that the equations above remain valid. So, does any such   permutation work? For any such equation? The answer is ""no"": try, for   instance, swapping $ \beta $ and $ \gamma $,   and you find that the first of the equations above becomes false. The set of permutations of the set $ \{\alpha, \beta, \gamma, \delta \} $   that preserve the validity of all polynomial equations (with coefficients in    $ \mathbb{Q} $) in these variables is called the Galois group of the   equation $ x^4 = 2 $. This group is the symmetric group of the square (which   makes sense if you look at the location of $ \alpha, \beta, \gamma $,    and $ \delta $ in the Complex plane. It is also known as $ D_8 $,   the dihedral group of order 8. Question 1 : In order to determine the permutations of the set $ \{ \alpha, \beta, \gamma, \delta \} $ that preserve the validity of all polynomial equations (with coefficients in Q) in these variables, do I need to list out all such equations (how would I do it), and how do tell the list is exhaustive and then only determine such permutations? Question 2 : Following Question 1, the coefficients of a polynomial form  symmetric polynomials as functions of the roots. How this help in determining the correct permutations? Using this as example, the Galois group is isomorphic to the Klein 4-group: $ x^4 - 5x + 6 = 0 $ factorize as $ (x^2 - 3)(x^2 - 2) = (x^2 - r_1 r_2)(x^2 - r_3 r_4) $ and the permutation that leave the polynomial unchanged is the identity, $ (12), (34) $ and $ (12)(34) $. Other permutations turn it into different polynomials and do not belong the Galois group of the permutation? Question 3 : Am I correct to say that a permutation belongs to the Galois group of the polynomial if it permutes the roots in such a way that when we multiply out, we obtain the polynomial that we started with? Question 4 : How would Galois himself have determined the ""Galois group"" of a given polynomial? Question 5 : Could you please point out where I could find materials on how Galois solved the insolvability of quintic using materials available during his time, instead of the automorphism of splitting field approach?  My google search has been unsatisfatory. The textbooks by Cox, Tignol, Rotman, Swallow are not easy to me and do not have the discussions I am comfortable with.",,"['abstract-algebra', 'galois-theory']"
16,Elementary divisors theorem for Dedekind domains (Exercise in Lang's Algebra),Elementary divisors theorem for Dedekind domains (Exercise in Lang's Algebra),,"Exercise 13 (b) of Chapter III in Lang's Algebra is as follows. Let $M$ be a finitely generated projective module over the Dedekind ring $\mathfrak{o}$. Then there exists free modules $F$ and $F^\prime$ such that $F \supset M \supset F^\prime$ were $F$ and $F^\prime$ have the same rank $n$. Prove that there exists a basis $\{e_1, \cdots, e_n \}$ of $F$ and ideals $\mathfrak{a}_1, \cdots, \mathfrak{a}_n$ such that $M = \mathfrak{a_1} e_1 + \cdots + \mathfrak{a_n} e_n$, or in other words, $M \approx \oplus \; \mathfrak{a}_i$. I understand the proof of the structure theorem for finitely generated modules over Dedekind domains, but I do not see how to prove the existence of a basis of $F$ if we follow the same kind of arguments.","Exercise 13 (b) of Chapter III in Lang's Algebra is as follows. Let $M$ be a finitely generated projective module over the Dedekind ring $\mathfrak{o}$. Then there exists free modules $F$ and $F^\prime$ such that $F \supset M \supset F^\prime$ were $F$ and $F^\prime$ have the same rank $n$. Prove that there exists a basis $\{e_1, \cdots, e_n \}$ of $F$ and ideals $\mathfrak{a}_1, \cdots, \mathfrak{a}_n$ such that $M = \mathfrak{a_1} e_1 + \cdots + \mathfrak{a_n} e_n$, or in other words, $M \approx \oplus \; \mathfrak{a}_i$. I understand the proof of the structure theorem for finitely generated modules over Dedekind domains, but I do not see how to prove the existence of a basis of $F$ if we follow the same kind of arguments.",,"['abstract-algebra', 'modules', 'dedekind-domain']"
17,Are complex split-octonions isomorphic to a more easily-defined algebra?,Are complex split-octonions isomorphic to a more easily-defined algebra?,,"I write fiction and nonfiction, both which are mathy.  My fiction is not usually supermathy but I'm working on a fictional story that has some math in it, and I prefer accuracy to mathbabble.  I'm stepping outside my particular math domain so what I am asking may be incredibly naive, but here it is. I have two questions. $0)$ Are complex split-octonions (that is, split-octonions with complex coefficients) isomorphic to another algebra more easily defined or described?  I mention this because the Muses's so-called ""conic sedenions"" are isomorphic to complex octonions, and for all I know complex split-octonions can be reduced to some other algebra. $1)$ Can complex split-octonions be represented as a simple subalgebra (I am using this word very loosely, obv.) of trigintaduonions?  If not, could they be represented as a simple subalgebra of a higher Cayley construction? Thanks for your time, and if this question is ludicrous in some way, let me know so I can fix or remove it.","I write fiction and nonfiction, both which are mathy.  My fiction is not usually supermathy but I'm working on a fictional story that has some math in it, and I prefer accuracy to mathbabble.  I'm stepping outside my particular math domain so what I am asking may be incredibly naive, but here it is. I have two questions. $0)$ Are complex split-octonions (that is, split-octonions with complex coefficients) isomorphic to another algebra more easily defined or described?  I mention this because the Muses's so-called ""conic sedenions"" are isomorphic to complex octonions, and for all I know complex split-octonions can be reduced to some other algebra. $1)$ Can complex split-octonions be represented as a simple subalgebra (I am using this word very loosely, obv.) of trigintaduonions?  If not, could they be represented as a simple subalgebra of a higher Cayley construction? Thanks for your time, and if this question is ludicrous in some way, let me know so I can fix or remove it.",,"['abstract-algebra', 'octonions', 'sedenions']"
18,Inner automorphism group as the kernel of a homomorphism,Inner automorphism group as the kernel of a homomorphism,,"Is there a homomorphism $\psi : \text{Aut}(G) \to \mathcal G$ with $\ker \psi = \text{Inn}(G)$ ? (Besides mapping each automorphism to its corresponding coset in $\text{Aut}(G) / \text{Inn}(G)$ .) Any group $G$ acts on itself via conjugation: $g * h = ghg^{-1}$ . So there is a corresponding homomorphism $\varphi : G \to \text{Sym}(G)$ defined by $\varphi(g) = (h \mapsto g*h)$ . The kernel of this action is clearly $Z(G)$ , so $Z(G) \trianglelefteq G$ . The image of $\varphi$ is clearly $\text{Inn}(G)$ , the set of all conjugation automorphisms of $G$ , so $\text{Inn}(G) \leq \text{Sym}(G)$ . By the first isomorphism theorem, $G / Z(G) \cong \text{Inn}(G)$ . It follows that $\text{Inn}(G) \leq \text{Aut}(G)$ . What we DON'T get from this argument is that $\text{Inn}(G)$ is normal in $\text{Aut}(G)$ . So far I've only seen proofs that analyze what happens when you conjugate an inner automorphism by an automorphism: Inner automorphisms form a normal subgroup of $\operatorname{Aut}(G)$ , Set of all inner automorphisms is a normal subgroup . But is there a homomorphism $\psi : \text{Aut}(G) \to \mathcal G$ (for some other group $\mathcal G$ ) with $\ker \psi = \text{Inn}(G)$ ? An obvious choice is the canonical map $\pi : \text{Aut}(G) \to \text{Aut}(G)/\text{Inn}(G) = \text{Out}(G)$ that maps each element to its corresponding coset. But its codomain will not be a group unless we first prove that $\text{Inn}(G) \trianglelefteq \text{Aut}(G)$ . EDIT: To be clear, I am not asking for any arbitrary proof that $\text{Inn}(G)$ is normal. I am looking for a homomorphism with kernel $\text{Inn}(G)$ besides the obvious one.","Is there a homomorphism with ? (Besides mapping each automorphism to its corresponding coset in .) Any group acts on itself via conjugation: . So there is a corresponding homomorphism defined by . The kernel of this action is clearly , so . The image of is clearly , the set of all conjugation automorphisms of , so . By the first isomorphism theorem, . It follows that . What we DON'T get from this argument is that is normal in . So far I've only seen proofs that analyze what happens when you conjugate an inner automorphism by an automorphism: Inner automorphisms form a normal subgroup of $\operatorname{Aut}(G)$ , Set of all inner automorphisms is a normal subgroup . But is there a homomorphism (for some other group ) with ? An obvious choice is the canonical map that maps each element to its corresponding coset. But its codomain will not be a group unless we first prove that . EDIT: To be clear, I am not asking for any arbitrary proof that is normal. I am looking for a homomorphism with kernel besides the obvious one.",\psi : \text{Aut}(G) \to \mathcal G \ker \psi = \text{Inn}(G) \text{Aut}(G) / \text{Inn}(G) G g * h = ghg^{-1} \varphi : G \to \text{Sym}(G) \varphi(g) = (h \mapsto g*h) Z(G) Z(G) \trianglelefteq G \varphi \text{Inn}(G) G \text{Inn}(G) \leq \text{Sym}(G) G / Z(G) \cong \text{Inn}(G) \text{Inn}(G) \leq \text{Aut}(G) \text{Inn}(G) \text{Aut}(G) \psi : \text{Aut}(G) \to \mathcal G \mathcal G \ker \psi = \text{Inn}(G) \pi : \text{Aut}(G) \to \text{Aut}(G)/\text{Inn}(G) = \text{Out}(G) \text{Inn}(G) \trianglelefteq \text{Aut}(G) \text{Inn}(G) \text{Inn}(G),"['abstract-algebra', 'group-theory', 'group-homomorphism', 'automorphism-group']"
19,The difference between the ring version and module version of Chinese Remainder Thereom.,The difference between the ring version and module version of Chinese Remainder Thereom.,,"Chinese Remainder Theorem for Commutative Rings If $R$ is a commutative ring with $1$ and $I, J$ are ideals of $R$ that are pairwise coprime or comaximal (meaning $I + J = R$), then $IJ = I \cap J$, and the quotient ring $R/I \cap J$ is isomorphic to $R/I \times R/J$. I will first prove that $IJ = I \cap J$. If $x \in IJ$, then $x = \sum a_i b_i$ where $a_i \in I$ and $b_j \in J$. Thus for any fixed $i$, we have that $a_i b_i \in I$ since $a_i \in I$ and $a_i b_i \in J$ since $b_i \in J$. Thus, $\sum a_i b_i \in I$ and $\sum a_i b_i \in J$, which means that x = $\sum a_i b_i \in I \cap J$. Therefore, $IJ \in I \cap J$. Note that since $R$ contains 1, we can write $I \cap J = R(I \cap J)$. Therefore, $I \cap J = R(I \cap J) = (I+J)(I \cap J) = I(I \cap J) + J(I \cap J) \subseteq IJ + JI = IJ$. Now, I will prove that $R/IJ \cong R/I \times R/J$. Consider the ring homomorphism $\phi: R \rightarrow R/I \times R/J$ defined by $\phi(r) = (r+ I, r+J)$ with kernel $I \cap J$. If we can prove that $\phi$ is surjective, then we can apply the first ring isomorphism theorem to show that $R/(I \cap J) \cong R/I \times R/J$. Since $I + J = R$, for any $r_1, r_2 \in R$, we can write $r_1 = i_1 + j_1$ and $r_2 = i_2 + j_2$, where $i_1, i_2 \in I$ and $j_1, j_2 \in J$. Let $x = i_1 + j_2$. This gives us $x - r_1 = j_2 - j_1 \in J$ and $x - r_2 = i_1 - i_2 \in I$. In particular, we have $x + I = r_2 + I$ and $x + J = r_1 + J$. Hence, for all $(r_1 + I, r_2 + J)$, we can find an element $x$ such that $\phi(x) = ( x+ I, x+ J) = (r_2 +I, r_1 + J)$. This shows that $\phi$ is surjective. Hence, we can conclude that $\phi: R/(I\cap J) \cong R/I \times R/J$. (Can someone verify if the proof is correct?) Using module homomorphism, the result also hold (I think.) So, now my concern is proving the converse. Converse to Chinese Remainder Theorem According to this post the converse is false if the isomorphism is of rings. I don't quite understand how the link given shows this. I will put the link here: Can $R \times R$ be isomorphic to $R$ as rings? Can someone explain to me how the result in this link shows that it is only true when the isomorphism is of $R$-modules? If so, am I correct to say that chinese remainder theorem shows that $R/I \cap J \cong R/I \times R/J$ both as rings and $R$-modules, but the converse is only true when the isomorphism is of $R$-modules? What about this, Show that $R/(I \cap J) \cong (R/I) \times (R/J) $ Can it be used to show that the converse is true by saying that since $\phi: R \rightarrow R/I \times R/J$ is surjective if and only if $I+J = R$, so $R/I \cap J \cong R/I \times R/J$ if and only if $I+J = R$ I think we cannot because in the post the $\phi$ is predefined, but here we are talking about the general case, $R/I \cap J \cong R/I \times R/J$ and the homomorphism might not be the same. Am I right? I am very confused about this. Note that this is only my first algebra course, so please don't use stuff that is too advanced to explain to me. Thanks.","Chinese Remainder Theorem for Commutative Rings If $R$ is a commutative ring with $1$ and $I, J$ are ideals of $R$ that are pairwise coprime or comaximal (meaning $I + J = R$), then $IJ = I \cap J$, and the quotient ring $R/I \cap J$ is isomorphic to $R/I \times R/J$. I will first prove that $IJ = I \cap J$. If $x \in IJ$, then $x = \sum a_i b_i$ where $a_i \in I$ and $b_j \in J$. Thus for any fixed $i$, we have that $a_i b_i \in I$ since $a_i \in I$ and $a_i b_i \in J$ since $b_i \in J$. Thus, $\sum a_i b_i \in I$ and $\sum a_i b_i \in J$, which means that x = $\sum a_i b_i \in I \cap J$. Therefore, $IJ \in I \cap J$. Note that since $R$ contains 1, we can write $I \cap J = R(I \cap J)$. Therefore, $I \cap J = R(I \cap J) = (I+J)(I \cap J) = I(I \cap J) + J(I \cap J) \subseteq IJ + JI = IJ$. Now, I will prove that $R/IJ \cong R/I \times R/J$. Consider the ring homomorphism $\phi: R \rightarrow R/I \times R/J$ defined by $\phi(r) = (r+ I, r+J)$ with kernel $I \cap J$. If we can prove that $\phi$ is surjective, then we can apply the first ring isomorphism theorem to show that $R/(I \cap J) \cong R/I \times R/J$. Since $I + J = R$, for any $r_1, r_2 \in R$, we can write $r_1 = i_1 + j_1$ and $r_2 = i_2 + j_2$, where $i_1, i_2 \in I$ and $j_1, j_2 \in J$. Let $x = i_1 + j_2$. This gives us $x - r_1 = j_2 - j_1 \in J$ and $x - r_2 = i_1 - i_2 \in I$. In particular, we have $x + I = r_2 + I$ and $x + J = r_1 + J$. Hence, for all $(r_1 + I, r_2 + J)$, we can find an element $x$ such that $\phi(x) = ( x+ I, x+ J) = (r_2 +I, r_1 + J)$. This shows that $\phi$ is surjective. Hence, we can conclude that $\phi: R/(I\cap J) \cong R/I \times R/J$. (Can someone verify if the proof is correct?) Using module homomorphism, the result also hold (I think.) So, now my concern is proving the converse. Converse to Chinese Remainder Theorem According to this post the converse is false if the isomorphism is of rings. I don't quite understand how the link given shows this. I will put the link here: Can $R \times R$ be isomorphic to $R$ as rings? Can someone explain to me how the result in this link shows that it is only true when the isomorphism is of $R$-modules? If so, am I correct to say that chinese remainder theorem shows that $R/I \cap J \cong R/I \times R/J$ both as rings and $R$-modules, but the converse is only true when the isomorphism is of $R$-modules? What about this, Show that $R/(I \cap J) \cong (R/I) \times (R/J) $ Can it be used to show that the converse is true by saying that since $\phi: R \rightarrow R/I \times R/J$ is surjective if and only if $I+J = R$, so $R/I \cap J \cong R/I \times R/J$ if and only if $I+J = R$ I think we cannot because in the post the $\phi$ is predefined, but here we are talking about the general case, $R/I \cap J \cong R/I \times R/J$ and the homomorphism might not be the same. Am I right? I am very confused about this. Note that this is only my first algebra course, so please don't use stuff that is too advanced to explain to me. Thanks.",,"['abstract-algebra', 'ring-theory', 'modules', 'ideals', 'chinese-remainder-theorem']"
20,Deformations of associative algebras and Hochschild cohomology.,Deformations of associative algebras and Hochschild cohomology.,,"I am studying the deformation theory of associative algebras (and Poisson algebras) and came across a question for which I cannot find an answer: Let $(A, \mu)$ be a commutative associative algebra over a field $\mathbb{F}$ . The first order deformations of $\mu$ are classified up to equivalence by the second Hochschild cohomology group $\mathrm{HH}_{\mu}^{2}(A)$ . I’m wondering if this can be generalized in the following sense: Given a $k$ th order deformation $\mu_{(k)} = \mu + \mu_{1} h + \dotsb + \mu_{k} h^k$ of $\mu$ , in some cases it is possible to extend it to a $(k+1)$ th order deformation $\mu_{(k+1)} = \mu_{(k)} + \mu_{k+1}h^{k+1}$ , where $\mu_{k+1}$ is a bilinear map $A \times A \to A$ . In this case it is natural to ask what are all the possible extensions up to equivalence. It turns out that the set of all possible deformations consists of the affine space $ \mu_{k+1} + \ker(\delta_{\mu}^2), $ where $\delta_{\mu}^2 \colon \mathrm{HC}^{2}(A) \to \mathrm{HC}^{3}(A)$ is the Hochschild coboundary operator. Furthermore, if two extensions correspond to cohomologous elements of $\mathrm{HC}^2(A)$ , it turns out that the extensions are equivalent (in the sense that the resulting deformed algebras are isomorphic). My question is whether the converse holds, which is to say do equivalent extensions correspond to cohomologous elements? (And hence $\mathrm{HH}_{\mu}^2(A)$ would classify the $(k+1)$ th order deformations extending a particular $k$ th order deformation of $\mu$ .) Here is what I have tried so far (in the case of $k = 2$ ): consider two equivalent $2$ nd order deformations of $\mu$ : $$   \mu_{(2)} = \mu + \mu_{1} h + \eta h^2 \,,   \quad   \mu_{(2)}' = \mu + \mu_{1} h + \eta' h^2 \,, $$ with isomorphism $\Phi = 1 + \phi h + \psi h^2$ . Then from the requirement that $$   \Phi(\mu_{(2)}(F, G)) = \mu_{(2)}'(\Phi(F), \Phi(G)) $$ for $F, G \in A$ we get that $$   \delta_{\mu}^{1}(\phi) = 0 \,, $$ and $$   \eta - \eta'   = \delta_{\mu}^{1}(\psi) + \delta_{\mu_{1}}^{1}(\phi) + \mu \circ \phi \otimes \phi \,. $$ Note that I use the notation $\delta_{\mu_{1}}^{1}(\phi)(F, G) = \mu_{1}(F, \phi(G)) + \mu_{1}(\phi(F), G) - \phi(\mu_{1}(F, G))$ . I need to end up with a formula like $\eta - \eta' = \delta_{\mu}^1(\gamma)$ for some $\gamma \in \mathrm{Hom}_{\mathbb{F}}(A, A)$ . It doesn’t seem like its going to work, but I haven’t been able to think of a counterexample, and I have been assured that the result should be true. P.S. I’d like to know if the analogous statement holds for deformations of a Poisson algebra.","I am studying the deformation theory of associative algebras (and Poisson algebras) and came across a question for which I cannot find an answer: Let be a commutative associative algebra over a field . The first order deformations of are classified up to equivalence by the second Hochschild cohomology group . I’m wondering if this can be generalized in the following sense: Given a th order deformation of , in some cases it is possible to extend it to a th order deformation , where is a bilinear map . In this case it is natural to ask what are all the possible extensions up to equivalence. It turns out that the set of all possible deformations consists of the affine space where is the Hochschild coboundary operator. Furthermore, if two extensions correspond to cohomologous elements of , it turns out that the extensions are equivalent (in the sense that the resulting deformed algebras are isomorphic). My question is whether the converse holds, which is to say do equivalent extensions correspond to cohomologous elements? (And hence would classify the th order deformations extending a particular th order deformation of .) Here is what I have tried so far (in the case of ): consider two equivalent nd order deformations of : with isomorphism . Then from the requirement that for we get that and Note that I use the notation . I need to end up with a formula like for some . It doesn’t seem like its going to work, but I haven’t been able to think of a counterexample, and I have been assured that the result should be true. P.S. I’d like to know if the analogous statement holds for deformations of a Poisson algebra.","(A, \mu) \mathbb{F} \mu \mathrm{HH}_{\mu}^{2}(A) k \mu_{(k)} = \mu + \mu_{1} h + \dotsb + \mu_{k} h^k \mu (k+1) \mu_{(k+1)} = \mu_{(k)} + \mu_{k+1}h^{k+1} \mu_{k+1} A \times A \to A 
\mu_{k+1} + \ker(\delta_{\mu}^2),
 \delta_{\mu}^2 \colon \mathrm{HC}^{2}(A) \to \mathrm{HC}^{3}(A) \mathrm{HC}^2(A) \mathrm{HH}_{\mu}^2(A) (k+1) k \mu k = 2 2 \mu 
  \mu_{(2)} = \mu + \mu_{1} h + \eta h^2 \,,
  \quad
  \mu_{(2)}' = \mu + \mu_{1} h + \eta' h^2 \,,
 \Phi = 1 + \phi h + \psi h^2 
  \Phi(\mu_{(2)}(F, G)) = \mu_{(2)}'(\Phi(F), \Phi(G))
 F, G \in A 
  \delta_{\mu}^{1}(\phi) = 0 \,,
 
  \eta - \eta'
  = \delta_{\mu}^{1}(\psi) + \delta_{\mu_{1}}^{1}(\phi) + \mu \circ \phi \otimes \phi \,.
 \delta_{\mu_{1}}^{1}(\phi)(F, G) = \mu_{1}(F, \phi(G)) + \mu_{1}(\phi(F), G) - \phi(\mu_{1}(F, G)) \eta - \eta' = \delta_{\mu}^1(\gamma) \gamma \in \mathrm{Hom}_{\mathbb{F}}(A, A)","['abstract-algebra', 'homology-cohomology', 'deformation-theory', 'hochschild-cohomology']"
21,chain rule for derivations,chain rule for derivations,,"Off we go. So let $b:X\rightarrow Y$ be a function from $X$ to $Y$ endowed with as much structure as it needs to make sense of the question :) and $a:Y\rightarrow \mathbb R$ a function into the reals. Then $a\circ b$ is a function from $X$ into the reals. It is an element of the algebra $\mathcal A$ of functions on $X$. Let $D$ be a derivation on $\mathcal A$, i.e. it is a linear map satisfying the Leibniz rule. But how does it act on $a\circ b$? Or is there simply no general rule? The naive guess for the chain rule would be: \begin{equation} D(a\circ b)=(D'a\circ b)\cdot D''(b) \end{equation} Where $D'a$ should be some sort of derivation on the algebra $A'$ of functions from $Y$ into $\mathbb R$. More worrying even is $D''(b)$ what is that supposed to be? Do those functions even have an algebra structure? So unless we know what the derivations are on $ A'$ and what $D''$ is, we can't say anything about the way $D$ acts on $(a\circ b)$? Of course i have diff geo in mind, where $X$ and $Y$ are manifolds. In my case $Y=TM$ is the tangent bundle for some mfld $M$ and $X$ is actually the mfld $\mathbb R$ in my case. So very simple. Thanks for reading. and looking forward to the moment where i notice the simplicity of the answer :)","Off we go. So let $b:X\rightarrow Y$ be a function from $X$ to $Y$ endowed with as much structure as it needs to make sense of the question :) and $a:Y\rightarrow \mathbb R$ a function into the reals. Then $a\circ b$ is a function from $X$ into the reals. It is an element of the algebra $\mathcal A$ of functions on $X$. Let $D$ be a derivation on $\mathcal A$, i.e. it is a linear map satisfying the Leibniz rule. But how does it act on $a\circ b$? Or is there simply no general rule? The naive guess for the chain rule would be: \begin{equation} D(a\circ b)=(D'a\circ b)\cdot D''(b) \end{equation} Where $D'a$ should be some sort of derivation on the algebra $A'$ of functions from $Y$ into $\mathbb R$. More worrying even is $D''(b)$ what is that supposed to be? Do those functions even have an algebra structure? So unless we know what the derivations are on $ A'$ and what $D''$ is, we can't say anything about the way $D$ acts on $(a\circ b)$? Of course i have diff geo in mind, where $X$ and $Y$ are manifolds. In my case $Y=TM$ is the tangent bundle for some mfld $M$ and $X$ is actually the mfld $\mathbb R$ in my case. So very simple. Thanks for reading. and looking forward to the moment where i notice the simplicity of the answer :)",,"['abstract-algebra', 'differential-geometry', 'derivatives', 'differential-algebra']"
22,Defining multiplication on the tensor product of $R$-algebras.,Defining multiplication on the tensor product of -algebras.,R,"If $M$ and $N$ are $R$-algebras, then one can define a multiplication of elementary tensors as follows; $(m \otimes n) \cdot (m' \otimes n') = mm' \otimes nn'$. My question is how can we show, using the universal property of the tensor product, that this is a well defined operation? My understanding is that we would be need a 4-linear map out of $(M \times N)\times (M \times N)$ into $M \otimes N$, that would factor through $(M \otimes N) \otimes (M \otimes N)$ giving us the required linear map. Am I on the right track? I can't seem to get my head around the details in the proof. Any pointers would be of great help to me.","If $M$ and $N$ are $R$-algebras, then one can define a multiplication of elementary tensors as follows; $(m \otimes n) \cdot (m' \otimes n') = mm' \otimes nn'$. My question is how can we show, using the universal property of the tensor product, that this is a well defined operation? My understanding is that we would be need a 4-linear map out of $(M \times N)\times (M \times N)$ into $M \otimes N$, that would factor through $(M \otimes N) \otimes (M \otimes N)$ giving us the required linear map. Am I on the right track? I can't seem to get my head around the details in the proof. Any pointers would be of great help to me.",,"['abstract-algebra', 'tensor-products']"
23,What is an importance of Gaussian and Eisenstein rings?,What is an importance of Gaussian and Eisenstein rings?,,"Among quadratic integer rings, $\mathbb{Z}[i]$ and $\mathbb{Z}[\frac{1+\sqrt{-3}}{2}]$ have their special names, namely Gaussian integers and Eisenstein integers respectively. I guess this is named so because these rings are particularly more important than other quadratic rings. Moreover, one can completely characterize prime elements of these rings. I asked my professor (her major is number theory) and she said one reason why these rings are important is that $\mathbb{Z}$,Gaussian and Eisenstein rings are related to elliptic curves. However, it is not very clear to me how. Why are Gaussian & Eisenstein integers are important? And what is a use of their prime elements?","Among quadratic integer rings, $\mathbb{Z}[i]$ and $\mathbb{Z}[\frac{1+\sqrt{-3}}{2}]$ have their special names, namely Gaussian integers and Eisenstein integers respectively. I guess this is named so because these rings are particularly more important than other quadratic rings. Moreover, one can completely characterize prime elements of these rings. I asked my professor (her major is number theory) and she said one reason why these rings are important is that $\mathbb{Z}$,Gaussian and Eisenstein rings are related to elliptic curves. However, it is not very clear to me how. Why are Gaussian & Eisenstein integers are important? And what is a use of their prime elements?",,"['abstract-algebra', 'examples-counterexamples', 'quadratic-integer-rings']"
24,Quotient of a category by equality in Grothendieck group,Quotient of a category by equality in Grothendieck group,,"I'm currently studying a question in the area of categorification. The situation is that we have an abelian category $C$ and endofunctors $F, G$ on $C$ we really expected to be naturally isomorphic. In fact, the induced homomorphisms $[F]$ and $[G]$ agree on the Grothendieck group $K_0(C)$, but $F$ and $G$ are not naturally isomorphic. Is there an easy way to ""fix"" $C$, say by considering some sort of quotient of $C$ by the relation $X \, \tilde{} \, Y$ if $[X] = [Y]$ in $K_0(C)$, so that the functors $F$ and $G$ become naturally isomorphic? The problem seems to be that $[X] = [Y]$ in $K_0$ apparently does not give much information on how $X$ relates to $Y$ in $C$.","I'm currently studying a question in the area of categorification. The situation is that we have an abelian category $C$ and endofunctors $F, G$ on $C$ we really expected to be naturally isomorphic. In fact, the induced homomorphisms $[F]$ and $[G]$ agree on the Grothendieck group $K_0(C)$, but $F$ and $G$ are not naturally isomorphic. Is there an easy way to ""fix"" $C$, say by considering some sort of quotient of $C$ by the relation $X \, \tilde{} \, Y$ if $[X] = [Y]$ in $K_0(C)$, so that the functors $F$ and $G$ become naturally isomorphic? The problem seems to be that $[X] = [Y]$ in $K_0$ apparently does not give much information on how $X$ relates to $Y$ in $C$.",,"['abstract-algebra', 'category-theory']"
25,Group of order 112 [closed],Group of order 112 [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $G$ be a finite group of order $2^4\times 7$ and Sylow $7$-subgroup of $G$ is not normal. Prove that Sylow $2$-subgroup of $G$ is abelian. I am very grateful for any help in this problem.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $G$ be a finite group of order $2^4\times 7$ and Sylow $7$-subgroup of $G$ is not normal. Prove that Sylow $2$-subgroup of $G$ is abelian. I am very grateful for any help in this problem.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups', 'sylow-theory']"
26,Module product and coproduct,Module product and coproduct,,"Completely lost when reading this: ""Defintion: If the index set $T$ is finite, the coproduct and product are the same module. If $T$ is infinite, the coproduct or sum $$\coprod_{t\in T}M_t=\bigoplus_{t\in T}M_t=\oplus M_t$$ is the submodule of $\prod M_t$ consisting of all sequences $\{m_t\}$ with only a finite number of non-zero terms ..."" I think I understand what the product of module is. It's just like $n$-tuple $\{m_1,m_2,\cdots,m_n\}$ when $T={1,2,\cdots,n}$, but generalized to general index set $T$, right? But what are coproducts? Submodule with finite number of non-zero terms only? Why need to restrict it to finite number? Why it is also called a sum?","Completely lost when reading this: ""Defintion: If the index set $T$ is finite, the coproduct and product are the same module. If $T$ is infinite, the coproduct or sum $$\coprod_{t\in T}M_t=\bigoplus_{t\in T}M_t=\oplus M_t$$ is the submodule of $\prod M_t$ consisting of all sequences $\{m_t\}$ with only a finite number of non-zero terms ..."" I think I understand what the product of module is. It's just like $n$-tuple $\{m_1,m_2,\cdots,m_n\}$ when $T={1,2,\cdots,n}$, but generalized to general index set $T$, right? But what are coproducts? Submodule with finite number of non-zero terms only? Why need to restrict it to finite number? Why it is also called a sum?",,"['abstract-algebra', 'modules']"
27,Factorizations in the symmetric group,Factorizations in the symmetric group,,"Notation notes: The cycle $(i,i+1,\dots,j)\in S_n$ is the permutation $(1)(2)...(i,i+1,\dots,j)\dots(n)$ in cycle notation. Motivation Given a factorization of a permutation into certain cycles with non negative exponents, is it possible to find a factorization of the permutation to the same cycle factors (in the same order) but with bounds on the exponents of the factors? For example (factorizing $(1,2)$ ): $(1,2) = (1,2)^1(1,2,3)^0(2,3)^0$ But if the exponent of $(12)$ must be zero we also have $(1,2) = (1,2)^0(1,2,3)^1(2,3)^1$ The Cycles We are given the cycles $\{\sigma_1,\dots,\sigma_{2n-1}\}$ in $S_n$ , where $\sigma_1=(1)$ , $\sigma_{2n-1}=(n)$ (the identity element). for every $m$ , $\sigma_m=(i,i+1,\dots,j)$ such that For every two consecutive cycles $\sigma_m=(i,i+1,\dots,j)$ and $\sigma_{m+1}=(k,k+1,\dots,l)$ either $(k=i$ and $l=j+1)$ or $(k=i+1$ and $l=j)$ allowing a special case $(i,i-1)$ to mean $Id$ . In other words, the cycle brackets keep moving one by one to the right from $(1)$ to $(n)$ : Example in $S_4$ : $\bbox[yellow]{(1)}(2)(3)(4)$ $\quad\sigma_1=(1)$ $\bbox[yellow]{(1)(2)}(3)(4)$ $\quad\sigma_2=(1,2)$ $\bbox[yellow]{(1)(2)(3)}(4)$ $\quad\sigma_3=(1,2,3)$ $(1)\bbox[yellow]{(2)(3)}(4)$ $\quad\sigma_4=(2,3)$ $(1)(2)\bbox[yellow]{(3)}(4)$ $\quad\sigma_5=(3)$ $(1)(2)(3)(4)$ $\quad\sigma_6=()\;\;(=Id)$ $(1)(2)(3)\bbox[yellow]{(4)}$ $\quad\sigma_7=(4)$ Equal Factorizations Now we are given $\pi\in S_n$ such that $$ \pi = \sigma_1^{\alpha_1}\dots\sigma_{2n-1}^{\alpha_{2n-1}}.$$ where $\{\alpha_1,\dots,\alpha_{2n-1}\}$ are non negative integer exponents. We are also given $2n-1$ non negative integer bounds on the exponents $\{m_1,\dots,m_{2n-1}\}$ . We can assume that each bound $m_i$ is also smaller than the length of the cycle $\sigma_i$ . Are there non negative integer exponents $\{\beta_1,\dots,\beta_{2n-1}\}$ such that $\beta_i\leq m_i$ for every $i$ and $ \pi = \sigma_1^{\beta_1}\dots\sigma_{2n-1}^{\beta_{2n-1}}$ ? Comments It is not important what are the values of $\beta_i$ , just whether they exist. One could go over every possible combination of values of $\beta_i$ , which is $\prod m_i$ options, is there a faster way? Empirical The following was generated going over all possible exponents: $\pi=(1,5)(2,4)$ in $S_5$ can be presented as $\pi=(1)^{\alpha_1}(1,2)^{\alpha_2}(1,2,3)^{\alpha_3}(1,2,3,4)^{\alpha_4}(1,2,3,4,5)^{\alpha_5}(2,3,4,5)^{\alpha_6}(3,4,5)^{\alpha_7}(4,5)^{\alpha_8}(5)^{\alpha_9}$ for the following values of $(\alpha_1,\dots,\alpha_9)$ : (0, 0, 0, 0, 4, 3, 2, 1, 0) (0, 0, 0, 1, 4, 2, 2, 1, 0) (0, 0, 0, 2, 4, 1, 2, 1, 0) (0, 0, 0, 3, 4, 0, 2, 1, 0) (0, 0, 1, 0, 4, 3, 1, 1, 0) (0, 0, 1, 1, 4, 2, 1, 1, 0) (0, 0, 1, 2, 4, 1, 1, 1, 0) (0, 0, 1, 3, 4, 0, 1, 1, 0) (0, 0, 2, 0, 4, 3, 0, 1, 0) (0, 0, 2, 1, 4, 2, 0, 1, 0) (0, 0, 2, 2, 4, 1, 0, 1, 0) (0, 0, 2, 3, 4, 0, 0, 1, 0) (0, 1, 0, 0, 4, 3, 2, 0, 0) (0, 1, 0, 1, 4, 2, 2, 0, 0) (0, 1, 0, 2, 4, 1, 2, 0, 0) (0, 1, 0, 3, 4, 0, 2, 0, 0) (0, 1, 1, 0, 4, 3, 1, 0, 0) (0, 1, 1, 1, 4, 2, 1, 0, 0) (0, 1, 1, 2, 4, 1, 1, 0, 0) (0, 1, 1, 3, 4, 0, 1, 0, 0) (0, 1, 2, 0, 4, 3, 0, 0, 0) (0, 1, 2, 1, 4, 2, 0, 0, 0) (0, 1, 2, 2, 4, 1, 0, 0, 0) (0, 1, 2, 3, 4, 0, 0, 0, 0) In this example it stands out that the largest cycle $(1,2,3,4,5)$ has an exponent $4$ in all solutions and the sum of exponents is a constant $10$ (which happens to be the number of inversions in $\pi$ ).","Notation notes: The cycle is the permutation in cycle notation. Motivation Given a factorization of a permutation into certain cycles with non negative exponents, is it possible to find a factorization of the permutation to the same cycle factors (in the same order) but with bounds on the exponents of the factors? For example (factorizing ): But if the exponent of must be zero we also have The Cycles We are given the cycles in , where , (the identity element). for every , such that For every two consecutive cycles and either and or and allowing a special case to mean . In other words, the cycle brackets keep moving one by one to the right from to : Example in : Equal Factorizations Now we are given such that where are non negative integer exponents. We are also given non negative integer bounds on the exponents . We can assume that each bound is also smaller than the length of the cycle . Are there non negative integer exponents such that for every and ? Comments It is not important what are the values of , just whether they exist. One could go over every possible combination of values of , which is options, is there a faster way? Empirical The following was generated going over all possible exponents: in can be presented as for the following values of : (0, 0, 0, 0, 4, 3, 2, 1, 0) (0, 0, 0, 1, 4, 2, 2, 1, 0) (0, 0, 0, 2, 4, 1, 2, 1, 0) (0, 0, 0, 3, 4, 0, 2, 1, 0) (0, 0, 1, 0, 4, 3, 1, 1, 0) (0, 0, 1, 1, 4, 2, 1, 1, 0) (0, 0, 1, 2, 4, 1, 1, 1, 0) (0, 0, 1, 3, 4, 0, 1, 1, 0) (0, 0, 2, 0, 4, 3, 0, 1, 0) (0, 0, 2, 1, 4, 2, 0, 1, 0) (0, 0, 2, 2, 4, 1, 0, 1, 0) (0, 0, 2, 3, 4, 0, 0, 1, 0) (0, 1, 0, 0, 4, 3, 2, 0, 0) (0, 1, 0, 1, 4, 2, 2, 0, 0) (0, 1, 0, 2, 4, 1, 2, 0, 0) (0, 1, 0, 3, 4, 0, 2, 0, 0) (0, 1, 1, 0, 4, 3, 1, 0, 0) (0, 1, 1, 1, 4, 2, 1, 0, 0) (0, 1, 1, 2, 4, 1, 1, 0, 0) (0, 1, 1, 3, 4, 0, 1, 0, 0) (0, 1, 2, 0, 4, 3, 0, 0, 0) (0, 1, 2, 1, 4, 2, 0, 0, 0) (0, 1, 2, 2, 4, 1, 0, 0, 0) (0, 1, 2, 3, 4, 0, 0, 0, 0) In this example it stands out that the largest cycle has an exponent in all solutions and the sum of exponents is a constant (which happens to be the number of inversions in ).","(i,i+1,\dots,j)\in S_n (1)(2)...(i,i+1,\dots,j)\dots(n) (1,2) (1,2) = (1,2)^1(1,2,3)^0(2,3)^0 (12) (1,2) = (1,2)^0(1,2,3)^1(2,3)^1 \{\sigma_1,\dots,\sigma_{2n-1}\} S_n \sigma_1=(1) \sigma_{2n-1}=(n) m \sigma_m=(i,i+1,\dots,j) \sigma_m=(i,i+1,\dots,j) \sigma_{m+1}=(k,k+1,\dots,l) (k=i l=j+1) (k=i+1 l=j) (i,i-1) Id (1) (n) S_4 \bbox[yellow]{(1)}(2)(3)(4) \quad\sigma_1=(1) \bbox[yellow]{(1)(2)}(3)(4) \quad\sigma_2=(1,2) \bbox[yellow]{(1)(2)(3)}(4) \quad\sigma_3=(1,2,3) (1)\bbox[yellow]{(2)(3)}(4) \quad\sigma_4=(2,3) (1)(2)\bbox[yellow]{(3)}(4) \quad\sigma_5=(3) (1)(2)(3)(4) \quad\sigma_6=()\;\;(=Id) (1)(2)(3)\bbox[yellow]{(4)} \quad\sigma_7=(4) \pi\in S_n  \pi = \sigma_1^{\alpha_1}\dots\sigma_{2n-1}^{\alpha_{2n-1}}. \{\alpha_1,\dots,\alpha_{2n-1}\} 2n-1 \{m_1,\dots,m_{2n-1}\} m_i \sigma_i \{\beta_1,\dots,\beta_{2n-1}\} \beta_i\leq m_i i  \pi = \sigma_1^{\beta_1}\dots\sigma_{2n-1}^{\beta_{2n-1}} \beta_i \beta_i \prod m_i \pi=(1,5)(2,4) S_5 \pi=(1)^{\alpha_1}(1,2)^{\alpha_2}(1,2,3)^{\alpha_3}(1,2,3,4)^{\alpha_4}(1,2,3,4,5)^{\alpha_5}(2,3,4,5)^{\alpha_6}(3,4,5)^{\alpha_7}(4,5)^{\alpha_8}(5)^{\alpha_9} (\alpha_1,\dots,\alpha_9) (1,2,3,4,5) 4 10 \pi","['abstract-algebra', 'group-theory', 'symmetric-groups', 'permutations']"
28,Do coalgebras arise outside the study of bi/Hopf-algebras?,Do coalgebras arise outside the study of bi/Hopf-algebras?,,"Hopefully the title is fairly self explanatory.  I'm curious as to whether the coalgebra structure (that is, a vector space with a comultiplication and counit) comes up an any area of mathematics not specifically because it is the arrow reversal of a unital algebra.","Hopefully the title is fairly self explanatory.  I'm curious as to whether the coalgebra structure (that is, a vector space with a comultiplication and counit) comes up an any area of mathematics not specifically because it is the arrow reversal of a unital algebra.",,"['abstract-algebra', 'category-theory', 'motivation', 'hopf-algebras', 'coalgebras']"
29,Counterexamples in $R$-modules products and $R$-modules direct sums and $R$-homomorphisms (Exemplification),Counterexamples in -modules products and -modules direct sums and -homomorphisms (Exemplification),R R R,"Q: (Exemplification) Do examples family of $R$-modules like $\{M_i\} , \{N_i\}$  and $R$-modules like $M,N$ such that every of four  question in underline is true. (in other word for every question Do Examples separately (Exemplification) $${Q1: \operatorname{Hom}_R\left ( \prod M_{i} ,N\right )\not \cong_{\Bbb Z} \bigoplus_{i\in I} \operatorname{Hom}_R\left ( M_{i} ,N\right )}$$   $${Q2: \operatorname{Hom}_R\left ( \prod M_{i} ,N\right )\not \cong_{\Bbb Z} \prod_{i\in I} Hom_R\left ( M_{i} ,N\right )}$$ $${Q3: \operatorname{Hom}_R\left ( M ,\bigoplus N_{i}\right )\not \cong_{\Bbb Z} \bigoplus_{i\in I} \operatorname{Hom}_R\left ( M ,N_{i}\right )}$$ $${Q4:\operatorname{Hom}_R\left ( M ,\bigoplus N_{i}\right )\not \cong_{\Bbb Z} \prod_{i\in I} \operatorname{Hom}_R\left ( M ,N_{i}\right )}$$ in other word this question is four separate question but similar(may be in one answer is existed for all) . i can't any answer for every case(Q1-Q4). if you can help me or hint until to aid to solve them (or one)  for this question we just $\Bbb Z_p, \Bbb Q, \Bbb R, \Bbb C$ $R$-modules. and I think we must work with  this $R$-modules. but is not work for example (for Q1 ): I let $M_i =\Bbb Z_{p_i} , N=\Bbb R$ then $\operatorname{Hom}_R(\prod \Bbb Z_{p_i}, \Bbb R)=\{ \phi \mid \phi : \prod \Bbb Z_{p_i} \rightarrow \Bbb R$ is $R$-homomorphism $\}$ $\operatorname{Hom}_R\left ( \prod M_{i} ,N\right )=\{ \phi \mid \phi : \prod M_{i} \rightarrow N$ is $R$-homomorphism $\}$ $\operatorname{Hom}_R\left (  M_{i} ,N\right )=\{ \phi \mid \phi :  M_{i} \rightarrow N$ is $R$-homomorphism $\}$ $\cong_{\Bbb Z}$ is $\Bbb Z$-isomorphism. $\bigoplus M$ in this statement means submodules of product. In other words it's product word such that ${""""""\bigoplus_{i\in I} M_i=\{ (a_i) \in \prod_{i \in I}M_i : a_i=0}$  for all $i \in I$ except finitely elements $\}""""""$ I cannot find the true notation for this operation in help center, so i define this operation under question and in $"""""","""""" $in fact $\bigoplus$ in this statement is not true and just is used for notation in this way definition. I must be find for every i to iv statement, examples to show...","Q: (Exemplification) Do examples family of $R$-modules like $\{M_i\} , \{N_i\}$  and $R$-modules like $M,N$ such that every of four  question in underline is true. (in other word for every question Do Examples separately (Exemplification) $${Q1: \operatorname{Hom}_R\left ( \prod M_{i} ,N\right )\not \cong_{\Bbb Z} \bigoplus_{i\in I} \operatorname{Hom}_R\left ( M_{i} ,N\right )}$$   $${Q2: \operatorname{Hom}_R\left ( \prod M_{i} ,N\right )\not \cong_{\Bbb Z} \prod_{i\in I} Hom_R\left ( M_{i} ,N\right )}$$ $${Q3: \operatorname{Hom}_R\left ( M ,\bigoplus N_{i}\right )\not \cong_{\Bbb Z} \bigoplus_{i\in I} \operatorname{Hom}_R\left ( M ,N_{i}\right )}$$ $${Q4:\operatorname{Hom}_R\left ( M ,\bigoplus N_{i}\right )\not \cong_{\Bbb Z} \prod_{i\in I} \operatorname{Hom}_R\left ( M ,N_{i}\right )}$$ in other word this question is four separate question but similar(may be in one answer is existed for all) . i can't any answer for every case(Q1-Q4). if you can help me or hint until to aid to solve them (or one)  for this question we just $\Bbb Z_p, \Bbb Q, \Bbb R, \Bbb C$ $R$-modules. and I think we must work with  this $R$-modules. but is not work for example (for Q1 ): I let $M_i =\Bbb Z_{p_i} , N=\Bbb R$ then $\operatorname{Hom}_R(\prod \Bbb Z_{p_i}, \Bbb R)=\{ \phi \mid \phi : \prod \Bbb Z_{p_i} \rightarrow \Bbb R$ is $R$-homomorphism $\}$ $\operatorname{Hom}_R\left ( \prod M_{i} ,N\right )=\{ \phi \mid \phi : \prod M_{i} \rightarrow N$ is $R$-homomorphism $\}$ $\operatorname{Hom}_R\left (  M_{i} ,N\right )=\{ \phi \mid \phi :  M_{i} \rightarrow N$ is $R$-homomorphism $\}$ $\cong_{\Bbb Z}$ is $\Bbb Z$-isomorphism. $\bigoplus M$ in this statement means submodules of product. In other words it's product word such that ${""""""\bigoplus_{i\in I} M_i=\{ (a_i) \in \prod_{i \in I}M_i : a_i=0}$  for all $i \in I$ except finitely elements $\}""""""$ I cannot find the true notation for this operation in help center, so i define this operation under question and in $"""""","""""" $in fact $\bigoplus$ in this statement is not true and just is used for notation in this way definition. I must be find for every i to iv statement, examples to show...",,"['commutative-algebra', 'modules', 'products', 'abstract-algebra']"
30,Determining if an extension is a normal extension [MORANDI'S BOOK],Determining if an extension is a normal extension [MORANDI'S BOOK],,"I'm dealing with the following problem: Let $K$ be a field and suppose that $\sigma\in\mbox{Aut}(K)$ has infinite order. Let $F$ be the fixed field of $\sigma$. If $K/F$ is algebraic, show that $K$ is normal over $F$. ${\it Proof. }$ Let $\alpha\in K$, I want to show that $f(x)=\mbox{min}(F,\alpha)$ splits over $K$. Let $i\in \mathbb{N}$, notice that $f(\sigma^i(\alpha))=\sigma^i(f(\alpha))=\sigma^i(0)=0$, thus $\sigma^i(\alpha)$ is a root of $f(x)$ for all $i\in \mathbb{N}$. Consider $n$ the minimal integer such that $\sigma^n(\alpha)=\alpha$ ($n$ exists since we have a finite number of roots of $f$), then we can factor $f$ over $K$ as $f(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))g(x)$ where $g(x)\in K$, moreover, since $f=\sigma(f)$, we have $$(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))g(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))\sigma(g(x))$$ and therefore $\sigma(g)=g$, thus $g(x)\in F[x]$, this implies that the factorization of $f$ is a factorization over $F$, hence $g(x)=1$ since $f$ is irreducible over $F$ and so $f(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))$, i.e., $f$ splits over $K$. Is this proof correct? If that's the case... Where did I use the infinite order of $\sigma$? Because if the proof is correct, it seems I've proved something like ""There are not irreducible polynomials of order $n$ in a fixed field of an automorphism of order less than $n$"". Thanks for your answers.","I'm dealing with the following problem: Let $K$ be a field and suppose that $\sigma\in\mbox{Aut}(K)$ has infinite order. Let $F$ be the fixed field of $\sigma$. If $K/F$ is algebraic, show that $K$ is normal over $F$. ${\it Proof. }$ Let $\alpha\in K$, I want to show that $f(x)=\mbox{min}(F,\alpha)$ splits over $K$. Let $i\in \mathbb{N}$, notice that $f(\sigma^i(\alpha))=\sigma^i(f(\alpha))=\sigma^i(0)=0$, thus $\sigma^i(\alpha)$ is a root of $f(x)$ for all $i\in \mathbb{N}$. Consider $n$ the minimal integer such that $\sigma^n(\alpha)=\alpha$ ($n$ exists since we have a finite number of roots of $f$), then we can factor $f$ over $K$ as $f(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))g(x)$ where $g(x)\in K$, moreover, since $f=\sigma(f)$, we have $$(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))g(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))\sigma(g(x))$$ and therefore $\sigma(g)=g$, thus $g(x)\in F[x]$, this implies that the factorization of $f$ is a factorization over $F$, hence $g(x)=1$ since $f$ is irreducible over $F$ and so $f(x)=(x-\alpha)(x-\sigma(\alpha))...(x-\sigma^n(\alpha))$, i.e., $f$ splits over $K$. Is this proof correct? If that's the case... Where did I use the infinite order of $\sigma$? Because if the proof is correct, it seems I've proved something like ""There are not irreducible polynomials of order $n$ in a fixed field of an automorphism of order less than $n$"". Thanks for your answers.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
31,"Why are centers, centralizers and normalizers called that way?","Why are centers, centralizers and normalizers called that way?",,"I know what they are, but where do the names come from?","I know what they are, but where do the names come from?",,"['abstract-algebra', 'group-theory', 'terminology']"
32,Presentation of an object in an Eilenberg Moore category by generators and relations,Presentation of an object in an Eilenberg Moore category by generators and relations,,"Let $\cal C$ be a category and let $T \colon \cal C \longrightarrow C$ be a monad. An $T$-algebra $A$ is presented by generators $G \in \cal C$ and relations $R \in \cal C$ if it is the coequaliser of two morphisms $TR \longrightarrow TG$. See here for more information. Unfortunately, the site does not go into much detail. I am a bit confused about how this works in practice though. For example, consider the group $$ A = \langle x,y,z \mid xy=yx \rangle $$ Let $G = \{x,y,z\}$ and let $R = \{ * \}$ be a singleton. Let $F \colon \mathsf{Set} \longrightarrow \mathsf{Set}$ be the free group monad. Define $s(*) = xy \in FG$ and $t(*) = yx \in FG$. Then $s,t$ extend to group homomorphisms $FR \longrightarrow FG$, and I think that $A$ is the coequaliser of these. Is that the correct way to think of this?","Let $\cal C$ be a category and let $T \colon \cal C \longrightarrow C$ be a monad. An $T$-algebra $A$ is presented by generators $G \in \cal C$ and relations $R \in \cal C$ if it is the coequaliser of two morphisms $TR \longrightarrow TG$. See here for more information. Unfortunately, the site does not go into much detail. I am a bit confused about how this works in practice though. For example, consider the group $$ A = \langle x,y,z \mid xy=yx \rangle $$ Let $G = \{x,y,z\}$ and let $R = \{ * \}$ be a singleton. Let $F \colon \mathsf{Set} \longrightarrow \mathsf{Set}$ be the free group monad. Define $s(*) = xy \in FG$ and $t(*) = yx \in FG$. Then $s,t$ extend to group homomorphisms $FR \longrightarrow FG$, and I think that $A$ is the coequaliser of these. Is that the correct way to think of this?",,"['abstract-algebra', 'category-theory']"
33,Analysis or (abstract) algebra first?,Analysis or (abstract) algebra first?,,Which one would you recommend? I only know calculus and linear algebra when it comes to university-level mathematics. Is one required to understand the other?,Which one would you recommend? I only know calculus and linear algebra when it comes to university-level mathematics. Is one required to understand the other?,,"['real-analysis', 'abstract-algebra', 'self-learning', 'advice', 'learning']"
34,Airy differential equation and Galois group,Airy differential equation and Galois group,,"Consider the Airy equation $y^{(2)}=ry$ where $r \in \Bbb{C}(z)$ but not constant. How do you show that $G^0=G$, where $G$ is the galois group of the picard vessiot extension of solutions over $\Bbb{C}(z)$ and $G^0$ is its connected component. It would be helpful if someone could give me some reference for this fact.","Consider the Airy equation $y^{(2)}=ry$ where $r \in \Bbb{C}(z)$ but not constant. How do you show that $G^0=G$, where $G$ is the galois group of the picard vessiot extension of solutions over $\Bbb{C}(z)$ and $G^0$ is its connected component. It would be helpful if someone could give me some reference for this fact.",,"['abstract-algebra', 'group-theory', 'ordinary-differential-equations', 'algebraic-topology', 'algebraic-groups']"
35,"If $a,b$ is an $R$-sequence, then $ax-b$ is prime (Eisenbud, Exercise 10.4)","If  is an -sequence, then  is prime (Eisenbud, Exercise 10.4)","a,b R ax-b","This is the exercise mentioned above: Let $a,b$ be regular sequence over a domain $R$. Prove that $ax-b$ is a prime of $R[x]$. Thank you for your answer!","This is the exercise mentioned above: Let $a,b$ be regular sequence over a domain $R$. Prove that $ax-b$ is a prime of $R[x]$. Thank you for your answer!",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
36,Is there an online database somewhere that lists identities for algebraic structures with two binary operators?,Is there an online database somewhere that lists identities for algebraic structures with two binary operators?,,"I'm working on an abstract algebra library in Python, and I'm trying to include as many functions that analyze algebraic structures, returning true or false based on whether or not the algebra satisfies a certain identity or not. I've found a good list of these in the Magma package provided by Maple, but I want something similar but for algebraic structures with two binary operators. Of course, I'll include the obvious: isDistributive(), isRing(), isField(), etc... but I want to be as complete as possible, so if there was a reference somewhere online with a large number of properties/identities/different types of algebras with two binary operators, or possibly an existing CAS that implements something like this, that would be very helpful.","I'm working on an abstract algebra library in Python, and I'm trying to include as many functions that analyze algebraic structures, returning true or false based on whether or not the algebra satisfies a certain identity or not. I've found a good list of these in the Magma package provided by Maple, but I want something similar but for algebraic structures with two binary operators. Of course, I'll include the obvious: isDistributive(), isRing(), isField(), etc... but I want to be as complete as possible, so if there was a reference somewhere online with a large number of properties/identities/different types of algebras with two binary operators, or possibly an existing CAS that implements something like this, that would be very helpful.",,"['abstract-algebra', 'ring-theory', 'field-theory', 'online-resources']"
37,Abelization of symmetric groups and its subgroups of bounded support,Abelization of symmetric groups and its subgroups of bounded support,,"For $X$ a finite set one can show easily that the abelization of the symmetric group on $X$ is given by the group of order $2$ and that the commutator subgroup of $\operatorname{Sym}X$ is given by the group of even permutations: Proof: Consider the $\operatorname{sign}\colon\operatorname{Sym}X\longrightarrow\{\pm1\}$. Since the two-element-group is commutative, we find $[\operatorname{Sym}X,\operatorname{Sym}X]\subseteq\ker\operatorname{sign}=\operatorname{Alt}X$. Since the symmetric group is not commutative, the commutator subgroup is a nontrivial normal subgroup of the alternating group and by simplicity of the latter, we find that the commutator subgroup is equal to $\operatorname{Alt}X$. (The cases $\operatorname{card}X=1,2,4$ of course need a slight variation because of lacking noncommutativity or simplicity of the alternating group). Question 1: For $X$ infinite, I have read that the abelization of $\operatorname{Sym}X$ is trivial. Is there an elementary way to see this? Elementary in particular means that the argument makes no use of the classification of normal subgroups of the symmetric group. Question 2: Let $I$ be the category asssociated to the preordered set of finite cardinals and denote by $F\colon I\longrightarrow\mathbf{Grp}$ the functor which associates to each cardinal $\alpha$ the symmetric group on $\alpha$. The colimit of this functor is then isomorphic to the group of permutations with support stricitly less then $\aleph_0$, which is usually called the finitary symmetric group and which I denote by $\operatorname{Sym}_{\aleph_0}X$ for $X$ a set of cardinality $\ge\aleph_0$. Since the abelization functor is left adjoint to the natural fucntor $\mathbf{Ab}\longrightarrow\mathbf{Grp}$ and thus commutes with colimits, the abelization of the finitary symmetric group is given by the group of order $2$, aswell. My second question is, wether this argument can be generalized to arbitrary cardinal numbers. Maybe my questions are simple to answer, but since I don't know a lot of either category theory, or set theory or algebra, I need some help.","For $X$ a finite set one can show easily that the abelization of the symmetric group on $X$ is given by the group of order $2$ and that the commutator subgroup of $\operatorname{Sym}X$ is given by the group of even permutations: Proof: Consider the $\operatorname{sign}\colon\operatorname{Sym}X\longrightarrow\{\pm1\}$. Since the two-element-group is commutative, we find $[\operatorname{Sym}X,\operatorname{Sym}X]\subseteq\ker\operatorname{sign}=\operatorname{Alt}X$. Since the symmetric group is not commutative, the commutator subgroup is a nontrivial normal subgroup of the alternating group and by simplicity of the latter, we find that the commutator subgroup is equal to $\operatorname{Alt}X$. (The cases $\operatorname{card}X=1,2,4$ of course need a slight variation because of lacking noncommutativity or simplicity of the alternating group). Question 1: For $X$ infinite, I have read that the abelization of $\operatorname{Sym}X$ is trivial. Is there an elementary way to see this? Elementary in particular means that the argument makes no use of the classification of normal subgroups of the symmetric group. Question 2: Let $I$ be the category asssociated to the preordered set of finite cardinals and denote by $F\colon I\longrightarrow\mathbf{Grp}$ the functor which associates to each cardinal $\alpha$ the symmetric group on $\alpha$. The colimit of this functor is then isomorphic to the group of permutations with support stricitly less then $\aleph_0$, which is usually called the finitary symmetric group and which I denote by $\operatorname{Sym}_{\aleph_0}X$ for $X$ a set of cardinality $\ge\aleph_0$. Since the abelization functor is left adjoint to the natural fucntor $\mathbf{Ab}\longrightarrow\mathbf{Grp}$ and thus commutes with colimits, the abelization of the finitary symmetric group is given by the group of order $2$, aswell. My second question is, wether this argument can be generalized to arbitrary cardinal numbers. Maybe my questions are simple to answer, but since I don't know a lot of either category theory, or set theory or algebra, I need some help.",,['abstract-algebra']
38,"What is the isomorphism between the fields $(Z_2[x]^{<3},+_{x^3+x^2+1},\times_{x^3+x^2+1})$ and $(Z_2[x]^{<3},+_{x^3+x+1},\times_{x^3+x+1})$? [closed]",What is the isomorphism between the fields  and ? [closed],"(Z_2[x]^{<3},+_{x^3+x^2+1},\times_{x^3+x^2+1}) (Z_2[x]^{<3},+_{x^3+x+1},\times_{x^3+x+1})","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question They are both Galois fields of order 8. I'm not exactly sure what the question means - how does one determine/describe an isomorphism?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question They are both Galois fields of order 8. I'm not exactly sure what the question means - how does one determine/describe an isomorphism?",,"['abstract-algebra', 'number-theory', 'field-theory', 'finite-fields', 'group-isomorphism']"
39,Simple module over matrix rings [duplicate],Simple module over matrix rings [duplicate],,"This question already has answers here : Simple $M_n(D)$-module with $D$ a division ring (3 answers) Closed 6 years ago . I'm trying to prove that if $M$ is a simple module over $M_n(D)$ where $D$ is a division algebra, then $M\cong D^n$. I know that if $M$ is a simple module over $R$ then it is isomorphic to $Rv=\{rv|r\in R\}$ for all $v\in M$. I'm trying to find a $M_n(D)$ module isomorphism $f:M_n(D)v\rightarrow D^n$. I've been told to try the map that maps $Xv\mapsto Xe_1$, where $e_1$ is the column vector with 1st entry 1 and the rest 0. I can prove that this is a homomorphism but I'm stuck on the injectivity. Any help would be well appreciated!","This question already has answers here : Simple $M_n(D)$-module with $D$ a division ring (3 answers) Closed 6 years ago . I'm trying to prove that if $M$ is a simple module over $M_n(D)$ where $D$ is a division algebra, then $M\cong D^n$. I know that if $M$ is a simple module over $R$ then it is isomorphic to $Rv=\{rv|r\in R\}$ for all $v\in M$. I'm trying to find a $M_n(D)$ module isomorphism $f:M_n(D)v\rightarrow D^n$. I've been told to try the map that maps $Xv\mapsto Xe_1$, where $e_1$ is the column vector with 1st entry 1 and the rest 0. I can prove that this is a homomorphism but I'm stuck on the injectivity. Any help would be well appreciated!",,"['abstract-algebra', 'matrices', 'modules']"
40,action of symmetry group of cube on pairs of opposite faces.,action of symmetry group of cube on pairs of opposite faces.,,"I want to solve the following problem from Dummit & Foote's Abstract Algebra: Explain why the action of the group of rigid motions of a cube on the set of three pairs of opposite faces is not faithful. Find the kernel of this action. My attempt: If the action is faithful, we would have an injective group homomorphism $G \to S_A$ where $A$ is the set of three pairs of opposite faces of the cube. That means $|G| \leq |S_A|$, but $|G|=24,|S_A|=6$. Thus this action cannot be faithful. In order to see what elements of $G$ are in the kernel, we list them all explicitly: There are 3 rotations around a line through centres of two opposite faces, each of order 4. There are 4 rotations around a line through opposite vertices, each of order 3. There are 6 rotations around a line through midpoints of opposite edges, each of order 2 Thus we have $3(4-1)+4(3-1)+6(2-1)=23$ nonidentity rotations. If we add the identity we see that we indeed have listed all elements of $G$. It is now geometrically evident that the kernel of the action consists of the squares of rotations in  part $1$, as well as the identity. Is my solution correct? If not, please help me fix it. Thanks!","I want to solve the following problem from Dummit & Foote's Abstract Algebra: Explain why the action of the group of rigid motions of a cube on the set of three pairs of opposite faces is not faithful. Find the kernel of this action. My attempt: If the action is faithful, we would have an injective group homomorphism $G \to S_A$ where $A$ is the set of three pairs of opposite faces of the cube. That means $|G| \leq |S_A|$, but $|G|=24,|S_A|=6$. Thus this action cannot be faithful. In order to see what elements of $G$ are in the kernel, we list them all explicitly: There are 3 rotations around a line through centres of two opposite faces, each of order 4. There are 4 rotations around a line through opposite vertices, each of order 3. There are 6 rotations around a line through midpoints of opposite edges, each of order 2 Thus we have $3(4-1)+4(3-1)+6(2-1)=23$ nonidentity rotations. If we add the identity we see that we indeed have listed all elements of $G$. It is now geometrically evident that the kernel of the action consists of the squares of rotations in  part $1$, as well as the identity. Is my solution correct? If not, please help me fix it. Thanks!",,"['abstract-algebra', 'geometry', 'solution-verification']"
41,"Number of common zeros of two quadratic polynomials in ${\Bbb C}[t,x]$",Number of common zeros of two quadratic polynomials in,"{\Bbb C}[t,x]","The following theorem is in Artin's Algebra (2nd edition): Theorem 11.9.10 Two nonzero polynomials $f(t,x)$ and $g(t,x)$ in two variables have only finitely many common zeros in ${\Bbb C}^2$, unless they have a common nonconstant factor in ${\Bbb C}[t,x]$. A comment after this theorem says that "" It is harder to prove the Bezout bound than the finiteness. We won't need that bound, so we won't prove it "". Then there is an exercise in the 1st edition: Prove that two quadratic polynomials $f, g$ in two variables have at most four common zeros, unless they have a nonconstant factor in common. Then in the 2nd edition, this exercise has been changed to the following one: Let $C_1$ and $C_2$ be the zeros of quadratic polynomials $f_1$ and $f_2$ respectively that don't have a common linear factor. (a) Let $p$ and $q$ be distinct points of intersection of $C_1$ and $C_2$, and let $L$ be the (complex) line through $p$ and $q$. Prove that there are constants $c_1$ and $c_2$, not both zero, so that $g=c_1f_1+c_2f_2$ vanishes identically on $L$. Prove also that $g$ is the product of linear polynomials. (b) Prove that $C_1$ and $C_2$ have at most $4$ points in common. I think (a) is supposed to be a hint to (b). But I don't see how I can make the implication. I guess I have to show that there cannot be too many $(c_1,c_2)$ as in (a), while such pairs might correspond to zeros of a polynomial of small degree. Here is my question : How can I prove (b) using (a)?","The following theorem is in Artin's Algebra (2nd edition): Theorem 11.9.10 Two nonzero polynomials $f(t,x)$ and $g(t,x)$ in two variables have only finitely many common zeros in ${\Bbb C}^2$, unless they have a common nonconstant factor in ${\Bbb C}[t,x]$. A comment after this theorem says that "" It is harder to prove the Bezout bound than the finiteness. We won't need that bound, so we won't prove it "". Then there is an exercise in the 1st edition: Prove that two quadratic polynomials $f, g$ in two variables have at most four common zeros, unless they have a nonconstant factor in common. Then in the 2nd edition, this exercise has been changed to the following one: Let $C_1$ and $C_2$ be the zeros of quadratic polynomials $f_1$ and $f_2$ respectively that don't have a common linear factor. (a) Let $p$ and $q$ be distinct points of intersection of $C_1$ and $C_2$, and let $L$ be the (complex) line through $p$ and $q$. Prove that there are constants $c_1$ and $c_2$, not both zero, so that $g=c_1f_1+c_2f_2$ vanishes identically on $L$. Prove also that $g$ is the product of linear polynomials. (b) Prove that $C_1$ and $C_2$ have at most $4$ points in common. I think (a) is supposed to be a hint to (b). But I don't see how I can make the implication. I guess I have to show that there cannot be too many $(c_1,c_2)$ as in (a), while such pairs might correspond to zeros of a polynomial of small degree. Here is my question : How can I prove (b) using (a)?",,"['abstract-algebra', 'algebraic-geometry']"
42,Extension of Euclidean Domain in which irreducibles have minimal norm,Extension of Euclidean Domain in which irreducibles have minimal norm,,"For the ring of polynomials $F[x]$ over a field $F$, there exists a larger ring $\bar{F}[x]$, the ring of polynomials over the closure of $F$, in which irreducibles are linear polynomials -- that is, polynomials of minimal non-zero degree. I'm wondering if this situation generalises to any Euclidean Domain $R$. Specifically, if $R$ is a Euclidean Domain with norm $M$, does there exist a Euclidean Domain $S$ with norm $N$ containing $R$ such that $N$ restricted to $R$ is $M$ and the irreducibles in $S$ have minimal non-zero norm? (In the above example we had $R = F[x]$, $S = \bar{F}[x]$, and $M = N = $degree of polynomials.) If $R$ is a field, the problem is trivial: take $S = R$. Otherwise, we could choose $S$ to be the field of fractions of $R$, but that involves eliminating the existence of irreducibles. Can we do better, or is the field of fractions the ""minimal"" choice for $S$ in some sense?","For the ring of polynomials $F[x]$ over a field $F$, there exists a larger ring $\bar{F}[x]$, the ring of polynomials over the closure of $F$, in which irreducibles are linear polynomials -- that is, polynomials of minimal non-zero degree. I'm wondering if this situation generalises to any Euclidean Domain $R$. Specifically, if $R$ is a Euclidean Domain with norm $M$, does there exist a Euclidean Domain $S$ with norm $N$ containing $R$ such that $N$ restricted to $R$ is $M$ and the irreducibles in $S$ have minimal non-zero norm? (In the above example we had $R = F[x]$, $S = \bar{F}[x]$, and $M = N = $degree of polynomials.) If $R$ is a field, the problem is trivial: take $S = R$. Otherwise, we could choose $S$ to be the field of fractions of $R$, but that involves eliminating the existence of irreducibles. Can we do better, or is the field of fractions the ""minimal"" choice for $S$ in some sense?",,"['abstract-algebra', 'ring-theory']"
43,A few questions about a specific ring,A few questions about a specific ring,,"My question is kinda long, so please bear with me... And  I only need hints to get me started. So, I'm working on the ring  $R  =\left(  \begin{matrix} \mathbb{Z} & \mathbb{Q}  \\ 0 & \mathbb{Q}   \end{matrix} \right)$ . $\textbf{1.}$  I  consider the left ideal $I_1= \left(  \begin{matrix} 0& \mathbb{Q}  \\ 0 & 0   \end{matrix} \right)$ ; I am trying to show that it is not projective, any help with that ? $\textbf{2.} $    I also  wanna show that if $M$ is a projective right $R$-module then any submodule of $M$ is also projective. I also need a nudge in the right direction to prove this claim. Thanks","My question is kinda long, so please bear with me... And  I only need hints to get me started. So, I'm working on the ring  $R  =\left(  \begin{matrix} \mathbb{Z} & \mathbb{Q}  \\ 0 & \mathbb{Q}   \end{matrix} \right)$ . $\textbf{1.}$  I  consider the left ideal $I_1= \left(  \begin{matrix} 0& \mathbb{Q}  \\ 0 & 0   \end{matrix} \right)$ ; I am trying to show that it is not projective, any help with that ? $\textbf{2.} $    I also  wanna show that if $M$ is a projective right $R$-module then any submodule of $M$ is also projective. I also need a nudge in the right direction to prove this claim. Thanks",,"['abstract-algebra', 'ring-theory', 'modules', 'noncommutative-algebra', 'projective-module']"
44,Order of the Normalizer of a Sylow $p$-subgroup in $S_{p}$,Order of the Normalizer of a Sylow -subgroup in,p S_{p},"Question: Let $p$ be a prime and $P \leq S_{p}$ with $|P| = p$. Prove that $|N_{S_{p}}(P)| = p(p-1)$. I have already solved this problem, but I have come across a proof that is much more elegant than my own. However, I'm not really convinced by it. The proof is as follows. Any permutation $\sigma$ that normalizes $P$ should send $1$ to one of $\{1, \dots , p\}$ and $2 \mapsto \{1, \dots , p\} \setminus \{\sigma(1)\}$. The conclusion is that these $p(p-1)$ choices determine the permutation completely, but I do not see why this is necessarily the case.","Question: Let $p$ be a prime and $P \leq S_{p}$ with $|P| = p$. Prove that $|N_{S_{p}}(P)| = p(p-1)$. I have already solved this problem, but I have come across a proof that is much more elegant than my own. However, I'm not really convinced by it. The proof is as follows. Any permutation $\sigma$ that normalizes $P$ should send $1$ to one of $\{1, \dots , p\}$ and $2 \mapsto \{1, \dots , p\} \setminus \{\sigma(1)\}$. The conclusion is that these $p(p-1)$ choices determine the permutation completely, but I do not see why this is necessarily the case.",,"['abstract-algebra', 'group-theory', 'proof-verification', 'sylow-theory']"
45,How many compatible group structures does a topological space admit?,How many compatible group structures does a topological space admit?,,"Suppose I have a topological space $(G,\tau)$ and am interested in whether there exists a topological group $(G,*,\tau)$. In other words, can we assign a binary operation $*$ to $(G,\tau)$ which satisfies the group axioms, and under which the functions $x \mapsto x^{-1}$ and $(x,y) \mapsto x*y$ are continuous? If so, how many non equivalent ways can we do this? And what would a sensible form of equivalence be? For example we can give $\mathbb R$ the standard additive group structure, but we could equally inter it ""backwards"" in the group and define $x*y = \begin{cases} x \cdot y & \mbox{if } x,y \le 0\\ -(x \cdot y) & \mbox{if } x \le 0 \mbox{ and } y \ge 0 \\ -(x \cdot y) & \mbox{if } x \ge 0 \mbox{ and } y \le 0\\ x \cdot y, & \mbox{if } x, y \ge 0 \end{cases}$ Here negation is used in the context of $\mathbb R$. It does not represent the inverse with respect to $*$. Clearly $\tau$ has to satisfy a lot of conditions for this to even stand a chance at being possible. In a topological group the map $x \mapsto a*x$ is a homeomorphism for each $a \in G$ and a result of this is the topology ""looks the same"" at any given point. If this is not true then there definitely is no $(G,*,\tau)$. But I'm sure this condition isn't enough, so are there any properties of the topology that can be reasonable computed which necessitate a compatible structure to exist? There is a similar question, which I know has a lot of theory behind it: Can we determine whether a topological group $(G,*,\tau)$ admits a Lie group structure by only looking at $(G,*,\tau)$? There is yet another question I found on StackExchange which is asking the opposite of me: How many agreable topologies can we assign an abstract group which make the group operations continuous? I would guess that problem is easier than mine. But I am not interested in either of these at the moment.","Suppose I have a topological space $(G,\tau)$ and am interested in whether there exists a topological group $(G,*,\tau)$. In other words, can we assign a binary operation $*$ to $(G,\tau)$ which satisfies the group axioms, and under which the functions $x \mapsto x^{-1}$ and $(x,y) \mapsto x*y$ are continuous? If so, how many non equivalent ways can we do this? And what would a sensible form of equivalence be? For example we can give $\mathbb R$ the standard additive group structure, but we could equally inter it ""backwards"" in the group and define $x*y = \begin{cases} x \cdot y & \mbox{if } x,y \le 0\\ -(x \cdot y) & \mbox{if } x \le 0 \mbox{ and } y \ge 0 \\ -(x \cdot y) & \mbox{if } x \ge 0 \mbox{ and } y \le 0\\ x \cdot y, & \mbox{if } x, y \ge 0 \end{cases}$ Here negation is used in the context of $\mathbb R$. It does not represent the inverse with respect to $*$. Clearly $\tau$ has to satisfy a lot of conditions for this to even stand a chance at being possible. In a topological group the map $x \mapsto a*x$ is a homeomorphism for each $a \in G$ and a result of this is the topology ""looks the same"" at any given point. If this is not true then there definitely is no $(G,*,\tau)$. But I'm sure this condition isn't enough, so are there any properties of the topology that can be reasonable computed which necessitate a compatible structure to exist? There is a similar question, which I know has a lot of theory behind it: Can we determine whether a topological group $(G,*,\tau)$ admits a Lie group structure by only looking at $(G,*,\tau)$? There is yet another question I found on StackExchange which is asking the opposite of me: How many agreable topologies can we assign an abstract group which make the group operations continuous? I would guess that problem is easier than mine. But I am not interested in either of these at the moment.",,"['abstract-algebra', 'general-topology', 'topological-groups', 'group-theory']"
46,What is the injective hull of $k_R$ where $k$ is a field and $R$ is the free $k$-algebra?,What is the injective hull of  where  is a field and  is the free -algebra?,k_R k R k,"I found the injective hull of $k_R$ where $k$ is a field and $R$ is the ring of polynomials in $n$ commutative indeterminates in Lectures on modules and rings of T.Y. Lam. I don't know if there exists some result in case $R=k\langle x_1,...,x_n\rangle$ is the free $k$-algebra generated by $\{x_1,...,x_n\}$. Here is my question: What is the injective hull of $k_R$ where $k$ is a field and $R=k\langle x_1,...,x_n\rangle$ is the free $k$-algebra generated by $\{x_1,...,x_n\}$?","I found the injective hull of $k_R$ where $k$ is a field and $R$ is the ring of polynomials in $n$ commutative indeterminates in Lectures on modules and rings of T.Y. Lam. I don't know if there exists some result in case $R=k\langle x_1,...,x_n\rangle$ is the free $k$-algebra generated by $\{x_1,...,x_n\}$. Here is my question: What is the injective hull of $k_R$ where $k$ is a field and $R=k\langle x_1,...,x_n\rangle$ is the free $k$-algebra generated by $\{x_1,...,x_n\}$?",,"['abstract-algebra', 'injective-module']"
47,Orbits of affine group scheme actions,Orbits of affine group scheme actions,,"Motivation Let $F$ and $K$ be distinct algebraically closed fields.  Then $GL_n(F)$ acts on $M_n(F)$ by conjugation, and the nilpotent orbits of this action can be characterized by the Jordan normal form.  Similarly for $GL_n(K)$ acting on $M_n(K)$, so that we have a bijection between the nilpotent orbits of $M_n(F)$ and $M_n(K)$ under their respective actions. Background Let $G$ be an affine group scheme over $\mathbb{Z}$, and let $X$ be an affine $\mathbb{Z}$-scheme on which $G$ acts via a map of schemes: $$G\times X\to X$$ or equivalently via a map of rings $$\varphi:\mathbb{Z}[X]\to \mathbb{Z}[G]\otimes\mathbb{Z}[X]$$ For a commutative ring $R$, we obtain an action of the $R$-points of $G$ on the $R$-points of $X$.  Specifically, given $\rho:\mathbb{Z}[X]\to R$ and $\tau:\mathbb{Z}[G]\to R$, we can define $\tau\cdot\rho$ to be the $R$-point given by the composition $$\mathbb{Z}[X]\xrightarrow{\varphi}\mathbb{Z}[G]\otimes\mathbb{Z}[X]\xrightarrow{\tau\otimes\rho}R\otimes R\to R$$ where the last map is multiplication in the ring.  With this notion of an action, we can similarly define $R$-orbits (of $G$ on $X$). Question For two commutative rings $R$ and $S$, when can we expect there to be a bijection between the $R$-orbits and $S$-orbits?  What are some reasonable hypotheses for $R$, $S$, $G$, $X$, and/or $\varphi$ to obtain such a bijection? Edit: As stated in the comments, a bijection is a rather weak result to seek; there should be something richer.  However, I don't know enough about group scheme actions to even hypothesize something else about two orbit spaces coming from the same action. In general, I'd like to learn more about the similarities between different orbit spaces coming from the same group scheme action.","Motivation Let $F$ and $K$ be distinct algebraically closed fields.  Then $GL_n(F)$ acts on $M_n(F)$ by conjugation, and the nilpotent orbits of this action can be characterized by the Jordan normal form.  Similarly for $GL_n(K)$ acting on $M_n(K)$, so that we have a bijection between the nilpotent orbits of $M_n(F)$ and $M_n(K)$ under their respective actions. Background Let $G$ be an affine group scheme over $\mathbb{Z}$, and let $X$ be an affine $\mathbb{Z}$-scheme on which $G$ acts via a map of schemes: $$G\times X\to X$$ or equivalently via a map of rings $$\varphi:\mathbb{Z}[X]\to \mathbb{Z}[G]\otimes\mathbb{Z}[X]$$ For a commutative ring $R$, we obtain an action of the $R$-points of $G$ on the $R$-points of $X$.  Specifically, given $\rho:\mathbb{Z}[X]\to R$ and $\tau:\mathbb{Z}[G]\to R$, we can define $\tau\cdot\rho$ to be the $R$-point given by the composition $$\mathbb{Z}[X]\xrightarrow{\varphi}\mathbb{Z}[G]\otimes\mathbb{Z}[X]\xrightarrow{\tau\otimes\rho}R\otimes R\to R$$ where the last map is multiplication in the ring.  With this notion of an action, we can similarly define $R$-orbits (of $G$ on $X$). Question For two commutative rings $R$ and $S$, when can we expect there to be a bijection between the $R$-orbits and $S$-orbits?  What are some reasonable hypotheses for $R$, $S$, $G$, $X$, and/or $\varphi$ to obtain such a bijection? Edit: As stated in the comments, a bijection is a rather weak result to seek; there should be something richer.  However, I don't know enough about group scheme actions to even hypothesize something else about two orbit spaces coming from the same action. In general, I'd like to learn more about the similarities between different orbit spaces coming from the same group scheme action.",,"['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'schemes']"
48,torsion subgroup of a finitely generated nilpotent group is finite.,torsion subgroup of a finitely generated nilpotent group is finite.,,"Prove that the torsion subgroup of a finitely generated nilpotent group is finite. More generally, in any group with ""almost"" no torsion all periodic subgroups are finite. Here ""almost"" means that there is a subgroup of finite index which has no torsion elements. I tried to ""apply"" the main theorem that a f.g. NP group is polycyclic. That is probably the first step, but I don't see how to proceed from there.","Prove that the torsion subgroup of a finitely generated nilpotent group is finite. More generally, in any group with ""almost"" no torsion all periodic subgroups are finite. Here ""almost"" means that there is a subgroup of finite index which has no torsion elements. I tried to ""apply"" the main theorem that a f.g. NP group is polycyclic. That is probably the first step, but I don't see how to proceed from there.",,"['abstract-algebra', 'group-theory']"
49,Where in the proof did Herstein use the fact that $A$ is a two-sided ideal of $R$?,Where in the proof did Herstein use the fact that  is a two-sided ideal of ?,A R,"I'm reading Noncommutative Rings by I. N. Herstein . The theorem I'm having trouble with is 1.2.5, on page 16 of the book. Some definition 1. Regular ideal An ideal $\rho \subset R$ is called a regular ideal right ideal of $R$ iff There exists a $r \in R$, such that $x - rx \in \rho, \forall x \in R$. 2. Right quasi-regular element $a$ is called the right-quasi element of $R$, is we can find $r \in R$, such that $a + r + ar = 0$. Such $r$ is called a right-quasi inverse of $a$. 3. Right quasi-regular ideal An ideal $\rho \subset R$ is called a right-quasi regular ideal iff every element of it is right-quasi regular. 3. Simple module A right $R-$module $M$ is called simple iff the two requirements below hold: $MR \neq 0$. $M$ has no non-trivial submodule. 4. Jacobson radical The Jacobson radical is the set of all elements in $R$ that annihilate all simple $R-$modules. Some properties $J(R) = \bigcap\limits_{M \text{ simple $R-$module}}\text{Ann}(M) = \bigcap\limits_{\rho \text{ regular, maximal right ideal of } R} \rho$ $M$ is a right, simple $R-$module iff there's some maximal, regular right ideal $\rho \subset R$, such that $M \cong R/\rho$. Every right-quasi regular ideal is contained in $J(R)$, and $J(R)$ is the maximal ideal amongst the set of right-quasi ideals of $R$. And this is a theorem I'm having trouble with. Theorem 1.2.5 (page 16) If $A$ is a two-sided ideal of $R$, then $J(A) = A \cap J(R)$. Proof We'll now prove that $A \cap J(R) \subset J(A)$: Let $a \in A \cap J(R)$, since $a \in J(R)$, as an element of $J(R)$, $a$ is right-quasi, hence there exists an $a'$, such that $a + a' + aa' = 0$, so $a' = -a -aa' \in A$, since $A$ is an ideal of $R$. Being a right-quasi ideal of $A$, $A \cap J(R) \subset J(A)$. We'll now prove that $A \cap J(R) \supset J(A)$: For every maximal, regular right ideal $\rho$ of $R$, let $\rho_A = A \cap \rho$. Now, there can only be 2 cases: $A \not \subset \rho$, since $\rho$ is maximal, $A + \rho = R$, and combining the two, we'll have: $$R/\rho \cong (A + \rho) / \rho \cong A /(\rho \cap A) = A/\rho_A$$ Now, since $R / \rho$ is $R-$simple, we get that $\rho_A$ is a maximal right ideal of $A$ (?) . Since $\rho$ is regular, there exists some $b \in R$, such that $x - bx \in \rho, \forall x \in R$. Now $A + \rho = R$, hence $b = a + r$, for $a \in A$, and $r \in \rho$. So, we'll have $x - bx = x - (a + r)x = a - ax -rx \in \rho, \forall x \in R$. Since $r \in \rho$, we must have $rx \in \rho$. Hence $x - ax \in \rho, \forall x \in R$, hence $x - ax \in \rho_A, \forall x \in A$, which makes $\rho_A$ $A-$regular. Since $J(A)$ is the intersection of all maximal, regular ideal of $A$, we'll have $J(A) \subset \rho_A$. If $A \subset \rho$, then $\rho_A = A \cap \rho = A$, so obviously $J(A) \subset A =  \rho_A$. Combining the two cases above, we'll have $J(A) \subset \bigcap \rho_A = \left(\bigcap \rho \right) \cap A = J(R) \cap A$. Hence, yielding the desired result. After this theorem, Herstein point out that if $A$ is not two-sided, then the theorem's result will fail. But as far as I can see, there's no place in the theorem that Herstein actually used $A$ as a two-sided ideal of $R$. And there's one thing that I cannot get, it's the (?) part. I know $R / \rho$ is $R-$simple, hence $A / \rho_A$ is also $R-$simple, which means that there is no right ideal of $R$ ( not $A$) lies in between $\rho_A$, and $A$. How come he concluded that $\rho_A$ is a maximal ideal of $A$? Is it where I must use the fact that $A$ is two-sided to prove? Thank you guys a lot, And have a good day.","I'm reading Noncommutative Rings by I. N. Herstein . The theorem I'm having trouble with is 1.2.5, on page 16 of the book. Some definition 1. Regular ideal An ideal $\rho \subset R$ is called a regular ideal right ideal of $R$ iff There exists a $r \in R$, such that $x - rx \in \rho, \forall x \in R$. 2. Right quasi-regular element $a$ is called the right-quasi element of $R$, is we can find $r \in R$, such that $a + r + ar = 0$. Such $r$ is called a right-quasi inverse of $a$. 3. Right quasi-regular ideal An ideal $\rho \subset R$ is called a right-quasi regular ideal iff every element of it is right-quasi regular. 3. Simple module A right $R-$module $M$ is called simple iff the two requirements below hold: $MR \neq 0$. $M$ has no non-trivial submodule. 4. Jacobson radical The Jacobson radical is the set of all elements in $R$ that annihilate all simple $R-$modules. Some properties $J(R) = \bigcap\limits_{M \text{ simple $R-$module}}\text{Ann}(M) = \bigcap\limits_{\rho \text{ regular, maximal right ideal of } R} \rho$ $M$ is a right, simple $R-$module iff there's some maximal, regular right ideal $\rho \subset R$, such that $M \cong R/\rho$. Every right-quasi regular ideal is contained in $J(R)$, and $J(R)$ is the maximal ideal amongst the set of right-quasi ideals of $R$. And this is a theorem I'm having trouble with. Theorem 1.2.5 (page 16) If $A$ is a two-sided ideal of $R$, then $J(A) = A \cap J(R)$. Proof We'll now prove that $A \cap J(R) \subset J(A)$: Let $a \in A \cap J(R)$, since $a \in J(R)$, as an element of $J(R)$, $a$ is right-quasi, hence there exists an $a'$, such that $a + a' + aa' = 0$, so $a' = -a -aa' \in A$, since $A$ is an ideal of $R$. Being a right-quasi ideal of $A$, $A \cap J(R) \subset J(A)$. We'll now prove that $A \cap J(R) \supset J(A)$: For every maximal, regular right ideal $\rho$ of $R$, let $\rho_A = A \cap \rho$. Now, there can only be 2 cases: $A \not \subset \rho$, since $\rho$ is maximal, $A + \rho = R$, and combining the two, we'll have: $$R/\rho \cong (A + \rho) / \rho \cong A /(\rho \cap A) = A/\rho_A$$ Now, since $R / \rho$ is $R-$simple, we get that $\rho_A$ is a maximal right ideal of $A$ (?) . Since $\rho$ is regular, there exists some $b \in R$, such that $x - bx \in \rho, \forall x \in R$. Now $A + \rho = R$, hence $b = a + r$, for $a \in A$, and $r \in \rho$. So, we'll have $x - bx = x - (a + r)x = a - ax -rx \in \rho, \forall x \in R$. Since $r \in \rho$, we must have $rx \in \rho$. Hence $x - ax \in \rho, \forall x \in R$, hence $x - ax \in \rho_A, \forall x \in A$, which makes $\rho_A$ $A-$regular. Since $J(A)$ is the intersection of all maximal, regular ideal of $A$, we'll have $J(A) \subset \rho_A$. If $A \subset \rho$, then $\rho_A = A \cap \rho = A$, so obviously $J(A) \subset A =  \rho_A$. Combining the two cases above, we'll have $J(A) \subset \bigcap \rho_A = \left(\bigcap \rho \right) \cap A = J(R) \cap A$. Hence, yielding the desired result. After this theorem, Herstein point out that if $A$ is not two-sided, then the theorem's result will fail. But as far as I can see, there's no place in the theorem that Herstein actually used $A$ as a two-sided ideal of $R$. And there's one thing that I cannot get, it's the (?) part. I know $R / \rho$ is $R-$simple, hence $A / \rho_A$ is also $R-$simple, which means that there is no right ideal of $R$ ( not $A$) lies in between $\rho_A$, and $A$. How come he concluded that $\rho_A$ is a maximal ideal of $A$? Is it where I must use the fact that $A$ is two-sided to prove? Thank you guys a lot, And have a good day.",,"['abstract-algebra', 'ring-theory', 'ideals', 'rngs']"
50,"Showing $\mathbb{Z}_{2} * \mathbb{Z}_{3} \cong\ (a, b\ |\ a^2 = b^3 = e)$",Showing,"\mathbb{Z}_{2} * \mathbb{Z}_{3} \cong\ (a, b\ |\ a^2 = b^3 = e)","Let $G = (a, b\ |\ a^2  = b^3 = e)$. I recognize there must be an epimorphism $\phi : G \rightarrow \mathbb{Z}_{2} * \mathbb{Z}_{3}$ (the free product) by the Van Dyck theorem, but I must show an isomorphism. Essentially, I believe we can do $\psi : G \rightarrow \mathbb{Z}_{2} * \mathbb{Z}_{3}$, $\psi(a^{p_{1}} b^{q_{1}} ... a^{p_{n}} b^{q_{n}}) = (1_{2})^{p_{1}} (1_{3})^{q_{1}} ... (1_{2})^{p_{n}} (1_{3})^{q_{n}}$ where $1_{2}$ is the generator of $\mathbb{Z}_{2}$ and $1_{3}$ the generator of $\mathbb{Z}_{3}$. But I'm having difficulty showing that this works, i.e. it is well-defined and an isomorphism. If we could say that $\psi = \phi$, i.e. it's an epimorphism, I believe seeing injectiveness is easy (by triviality of the kernel). But the proof of Van Dyck is not explicit, so I can't tell what $\phi$ is.","Let $G = (a, b\ |\ a^2  = b^3 = e)$. I recognize there must be an epimorphism $\phi : G \rightarrow \mathbb{Z}_{2} * \mathbb{Z}_{3}$ (the free product) by the Van Dyck theorem, but I must show an isomorphism. Essentially, I believe we can do $\psi : G \rightarrow \mathbb{Z}_{2} * \mathbb{Z}_{3}$, $\psi(a^{p_{1}} b^{q_{1}} ... a^{p_{n}} b^{q_{n}}) = (1_{2})^{p_{1}} (1_{3})^{q_{1}} ... (1_{2})^{p_{n}} (1_{3})^{q_{n}}$ where $1_{2}$ is the generator of $\mathbb{Z}_{2}$ and $1_{3}$ the generator of $\mathbb{Z}_{3}$. But I'm having difficulty showing that this works, i.e. it is well-defined and an isomorphism. If we could say that $\psi = \phi$, i.e. it's an epimorphism, I believe seeing injectiveness is easy (by triviality of the kernel). But the proof of Van Dyck is not explicit, so I can't tell what $\phi$ is.",,"['abstract-algebra', 'group-theory', 'group-presentation', 'free-product']"
51,Purely inseparable extension of algebraic function field,Purely inseparable extension of algebraic function field,,"Let $K$ be a field with $\operatorname{char}(K) = p > 0$ and with the property that $[K:K^p] < \infty$. Let $F/K$ be an algebraic function field, i.e there is an element $x \in F$ which is transcendental over $K$ and $F/K(x)$ is a finite field extension. Assume that $F/K(x)$ is separable. It can be shown that $F/F^{p^n}$ is a finite, purely inseparable field extension with $[F:F^{p^n}] \geq p^n$. If $K$ is perfect, then $[F:F^{p^n}] = p^n$. When $K$ is not perfect, one can consider $F' := F^{p^n} K$. Using the fact that $F/K$ is separably generated, one can prove that $[F:F'] \leq p^n$. But do we have $[F:F'] = p^n$? Update: I may have a way to prove it, but I need to know if there is a mistake. Since $F/K(x)$ is separable by the assumption, we can write $F = K(x, y)$. Consider the subfield $K(x, y^{p^n})$ of $F$. It contains $K(x)$, hence $F/K(x, y^{p^n})$ is separable. On the other hand, it is easy to see that $F/K(x, y^{p^n})$ is purely inseparable (of degree $\leq p^n$). Hence $F = K(x, y^{p^n})$. Now $F^{p^n} = K^{p^n}(x^{p^n}, y^{p^n})$ and $F' = F^{p^n}K = K(x^{p^n}, y^{p^n})$. We have $$[F:F'] = \frac{[F:K(x^{p^n})]}  {[F': K(x^{p^n})]} = \frac{[F:K(x)] \cdot [K(x) : K(x^{p^n})]} {[F': K(x^{p^n})]} = \frac{[F:K(x)] \cdot p^n} {[F': K(x^{p^n})]} = \frac{[K(x, y^{p^n}):K(x)] \cdot p^n} {[K(x^{p^n}, y^{p^n}): K(x^{p^n})]} \cdot$$  In order to see $[F:F'] = p^n$, we only need to show $[K(x, y^{p^n}):K(x)] \geq [K(x^{p^n}, y^{p^n}): K(x^{p^n})]$, because it is easy to see $[F:F'] = [K(x, y^{p^n}) :K(x^{p^n}, y^{p^n})] \leq p^n$. Now let $f(T) = \sum_{i=0}^n f_i(x)T^i \in K(x)[T] $ be the minimal polynomial of $y$ over $K(x)$. Then $f(y) = \sum_{i=0}^n f_i(x)y^i = 0$. Hence $\sum_{i=0}^n \tilde{f_i}(x^{p^n})(y^{p^n})^i = 0$, here $\tilde{f_i}$'s are obtained from $f_i$'s by raising their coefficients to the $p^n$ power. This shows that $[K(x^{p^n}, y^{p^n}): K(x^{p^n})] \leq n = [K(x, y):K(x)] = [K(x, y^{p^n}):K(x)]$ and the proof is done.","Let $K$ be a field with $\operatorname{char}(K) = p > 0$ and with the property that $[K:K^p] < \infty$. Let $F/K$ be an algebraic function field, i.e there is an element $x \in F$ which is transcendental over $K$ and $F/K(x)$ is a finite field extension. Assume that $F/K(x)$ is separable. It can be shown that $F/F^{p^n}$ is a finite, purely inseparable field extension with $[F:F^{p^n}] \geq p^n$. If $K$ is perfect, then $[F:F^{p^n}] = p^n$. When $K$ is not perfect, one can consider $F' := F^{p^n} K$. Using the fact that $F/K$ is separably generated, one can prove that $[F:F'] \leq p^n$. But do we have $[F:F'] = p^n$? Update: I may have a way to prove it, but I need to know if there is a mistake. Since $F/K(x)$ is separable by the assumption, we can write $F = K(x, y)$. Consider the subfield $K(x, y^{p^n})$ of $F$. It contains $K(x)$, hence $F/K(x, y^{p^n})$ is separable. On the other hand, it is easy to see that $F/K(x, y^{p^n})$ is purely inseparable (of degree $\leq p^n$). Hence $F = K(x, y^{p^n})$. Now $F^{p^n} = K^{p^n}(x^{p^n}, y^{p^n})$ and $F' = F^{p^n}K = K(x^{p^n}, y^{p^n})$. We have $$[F:F'] = \frac{[F:K(x^{p^n})]}  {[F': K(x^{p^n})]} = \frac{[F:K(x)] \cdot [K(x) : K(x^{p^n})]} {[F': K(x^{p^n})]} = \frac{[F:K(x)] \cdot p^n} {[F': K(x^{p^n})]} = \frac{[K(x, y^{p^n}):K(x)] \cdot p^n} {[K(x^{p^n}, y^{p^n}): K(x^{p^n})]} \cdot$$  In order to see $[F:F'] = p^n$, we only need to show $[K(x, y^{p^n}):K(x)] \geq [K(x^{p^n}, y^{p^n}): K(x^{p^n})]$, because it is easy to see $[F:F'] = [K(x, y^{p^n}) :K(x^{p^n}, y^{p^n})] \leq p^n$. Now let $f(T) = \sum_{i=0}^n f_i(x)T^i \in K(x)[T] $ be the minimal polynomial of $y$ over $K(x)$. Then $f(y) = \sum_{i=0}^n f_i(x)y^i = 0$. Hence $\sum_{i=0}^n \tilde{f_i}(x^{p^n})(y^{p^n})^i = 0$, here $\tilde{f_i}$'s are obtained from $f_i$'s by raising their coefficients to the $p^n$ power. This shows that $[K(x^{p^n}, y^{p^n}): K(x^{p^n})] \leq n = [K(x, y):K(x)] = [K(x, y^{p^n}):K(x)]$ and the proof is done.",,"['abstract-algebra', 'field-theory']"
52,Question about solvable groups,Question about solvable groups,,"How to prove the following: Groups of order less than 60, are solvable? I tried to do this by showing that groups of order $p^n$, $p q$, $p^2 q$, $p q r$ for primes $p$, $q$, $r$ are solvable. In this way, almost every group of order less than 60 is eliminated. Precisely, it only remains to show that groups of order 24, 40, 48, 54, 56 are solvable. It seems to me that this is too complicated way of solving, so I would like to know is there any other more elegant (and shorter) solution to the problem. Thanks in advance.","How to prove the following: Groups of order less than 60, are solvable? I tried to do this by showing that groups of order $p^n$, $p q$, $p^2 q$, $p q r$ for primes $p$, $q$, $r$ are solvable. In this way, almost every group of order less than 60 is eliminated. Precisely, it only remains to show that groups of order 24, 40, 48, 54, 56 are solvable. It seems to me that this is too complicated way of solving, so I would like to know is there any other more elegant (and shorter) solution to the problem. Thanks in advance.",,"['abstract-algebra', 'solvable-groups']"
53,Transcendental elements in $k[[x]]$ over the field $k(x)$,Transcendental elements in  over the field,k[[x]] k(x),I have a hard time to prove that $k[[x]]$ contains an element which is transcendental over $k(x)$. Could you please explain the some idea how to do that?,I have a hard time to prove that $k[[x]]$ contains an element which is transcendental over $k(x)$. Could you please explain the some idea how to do that?,,"['abstract-algebra', 'field-theory']"
54,"Deciding whether or not a class of modules is ""big enough""","Deciding whether or not a class of modules is ""big enough""",,"For the last few days I'm pondering the following question. The situation is this: $R$ is a commutative ring and $A$ a (noncommutative) $R$-algebra. I have a class $\mathcal{C}\subseteq\coprod_{S} SA-\textbf{Mod}$ of $A$-modules where $S$ runs over all commutative $R$-algebras. I want to know if $\mathcal{C}$ is ""big enough"". More precisely: I want to know if $I:=\bigcap_{V\in\mathcal{C}} \ker(A\to \mathrm{End}_{S-\textbf{Mod}}(V))$ is the zero ideal. I already know the following facts from the construction of $\mathcal{C}$ and $A$: $\mathcal{C}$ is closed under extension of scalars: If $V\in SA-\textbf{Mod}$ is in $\mathcal{C}$, then $TV$ is in $\mathcal{C}$ for any $S\to T$. $\mathcal{C}$ is closed under direct sums: If $V_i\in SA-\textbf{Mod}$ are in $\mathcal{C}$, then so is $\bigoplus_i V_i$. If $S$ is a field, then $SA-\textbf{Mod}\subseteq\mathcal{C}$. More generally: If $V\in SA-\textbf{Mod}$ is free as an $S$-module and $S$ has the property that projective $S$-modules are always free (like principal ideal domains or local rings) then $V\in\mathcal{C}$. All $V\in\mathcal{C}\cap SA-\textbf{Mod}$ are free as $S$-modules. This suggests to me that $I$ might be contained in $nil(R)A$. Is this true? If it helps, $R$ is an integral domain. More specifically $R=k[v^{\pm 1}]$ oder $R=k[v^{\pm 1}, u^{\pm 1}]$ with $k$ either $\mathbb{Z}[2\cos(\frac{2\pi}{m})]$ for some $m\in\mathbb{N}$ or some localization of these rings at a small set of prime numbers. $A$ is given by generators and relations (specifically it is a quotient of a path algebra) but I do not know much more about $A$ itself. In particular, I do not know, if $A$ is free as an $R$-module (in which case the answer to my question would be ""yes""). EDIT: Since I was asked: $SA$ denotes the scalar extension $S \otimes_R A$, $TV$ analogously means $T \otimes_S V$. Johannes Hahn.","For the last few days I'm pondering the following question. The situation is this: $R$ is a commutative ring and $A$ a (noncommutative) $R$-algebra. I have a class $\mathcal{C}\subseteq\coprod_{S} SA-\textbf{Mod}$ of $A$-modules where $S$ runs over all commutative $R$-algebras. I want to know if $\mathcal{C}$ is ""big enough"". More precisely: I want to know if $I:=\bigcap_{V\in\mathcal{C}} \ker(A\to \mathrm{End}_{S-\textbf{Mod}}(V))$ is the zero ideal. I already know the following facts from the construction of $\mathcal{C}$ and $A$: $\mathcal{C}$ is closed under extension of scalars: If $V\in SA-\textbf{Mod}$ is in $\mathcal{C}$, then $TV$ is in $\mathcal{C}$ for any $S\to T$. $\mathcal{C}$ is closed under direct sums: If $V_i\in SA-\textbf{Mod}$ are in $\mathcal{C}$, then so is $\bigoplus_i V_i$. If $S$ is a field, then $SA-\textbf{Mod}\subseteq\mathcal{C}$. More generally: If $V\in SA-\textbf{Mod}$ is free as an $S$-module and $S$ has the property that projective $S$-modules are always free (like principal ideal domains or local rings) then $V\in\mathcal{C}$. All $V\in\mathcal{C}\cap SA-\textbf{Mod}$ are free as $S$-modules. This suggests to me that $I$ might be contained in $nil(R)A$. Is this true? If it helps, $R$ is an integral domain. More specifically $R=k[v^{\pm 1}]$ oder $R=k[v^{\pm 1}, u^{\pm 1}]$ with $k$ either $\mathbb{Z}[2\cos(\frac{2\pi}{m})]$ for some $m\in\mathbb{N}$ or some localization of these rings at a small set of prime numbers. $A$ is given by generators and relations (specifically it is a quotient of a path algebra) but I do not know much more about $A$ itself. In particular, I do not know, if $A$ is free as an $R$-module (in which case the answer to my question would be ""yes""). EDIT: Since I was asked: $SA$ denotes the scalar extension $S \otimes_R A$, $TV$ analogously means $T \otimes_S V$. Johannes Hahn.",,"['abstract-algebra', 'ring-theory', 'representation-theory', 'modules']"
55,What is the correspondence between combinatorial problems and the location of the zeroes of polynomials called?,What is the correspondence between combinatorial problems and the location of the zeroes of polynomials called?,,"In the wikipedia article on the Italian-born American mathematician and philosopher Gian-Carlo Rota, it is stated that the one combinatorial idea he would like to be remembered for ""... is the correspondence between combinatorial problems and problems of the location of the zeroes of polynomials."" Also, a refence [1] is given for this quote. Upon reading through the interview, though, I didn't discover any more about this correspondence, nor did I find a lot by searching for it on the web. Question 1 : What is this correspondence called? I am also interested in how these two (which seem to me) disparate problems in mathematics relate to one another, so: Question 2 : How does this correspondence work? Any references? Finally: 3 more questions : To what extent has this correspondence been developed any further since Rota's discovery? Are there any other connections between zeroes of polynomials and combinatorics? I know algebraic geometry is concerned with the study of zeroes of polynomials, so is there any connection between (algebraic) combinatorics and algebraic geometry? Reference: [1] http://web.archive.org/web/20070811172343/http://www.rota.org/hotair/rotasharp.html","In the wikipedia article on the Italian-born American mathematician and philosopher Gian-Carlo Rota, it is stated that the one combinatorial idea he would like to be remembered for ""... is the correspondence between combinatorial problems and problems of the location of the zeroes of polynomials."" Also, a refence [1] is given for this quote. Upon reading through the interview, though, I didn't discover any more about this correspondence, nor did I find a lot by searching for it on the web. Question 1 : What is this correspondence called? I am also interested in how these two (which seem to me) disparate problems in mathematics relate to one another, so: Question 2 : How does this correspondence work? Any references? Finally: 3 more questions : To what extent has this correspondence been developed any further since Rota's discovery? Are there any other connections between zeroes of polynomials and combinatorics? I know algebraic geometry is concerned with the study of zeroes of polynomials, so is there any connection between (algebraic) combinatorics and algebraic geometry? Reference: [1] http://web.archive.org/web/20070811172343/http://www.rota.org/hotair/rotasharp.html",,"['abstract-algebra', 'combinatorics', 'algebraic-geometry', 'polynomials']"
56,Free module such that $R=R\oplus R$ [duplicate],Free module such that  [duplicate],R=R\oplus R,"This question already has answers here : Isomorphism of an endomorphism ring, how can $R\cong R^2$? (2 answers) Closed 8 years ago . I was wondering if the following is correct: Let $V=\bigoplus_{i\in\mathbb N}\mathbb Z$ and $R=\text{Hom}_{\ \mathbb Z}(V,V)$. Regard $R$ as an $R$-module. Then $R$ is free of rank $1$ with basis $\{\text{id}_{V}\}$. Now define a map $\phi$ as follows: where $f_i: V \to \mathbb Z$ for each $f\in R$ are linear maps. Then $\phi$ is an isomorphism of $R$-modules. Can I conclude as follows? (1) Since $\phi$ is an isomorphism of $R$-modules we have $R\cong R^2$. (2) Due to (1) we have $R^n\cong R^m$ for any $n,m\in\mathbb N$. (3) Due to (2) we have that $R$ as an $R$-module is free of rank $k$ for any $k\in\mathbb N$.","This question already has answers here : Isomorphism of an endomorphism ring, how can $R\cong R^2$? (2 answers) Closed 8 years ago . I was wondering if the following is correct: Let $V=\bigoplus_{i\in\mathbb N}\mathbb Z$ and $R=\text{Hom}_{\ \mathbb Z}(V,V)$. Regard $R$ as an $R$-module. Then $R$ is free of rank $1$ with basis $\{\text{id}_{V}\}$. Now define a map $\phi$ as follows: where $f_i: V \to \mathbb Z$ for each $f\in R$ are linear maps. Then $\phi$ is an isomorphism of $R$-modules. Can I conclude as follows? (1) Since $\phi$ is an isomorphism of $R$-modules we have $R\cong R^2$. (2) Due to (1) we have $R^n\cong R^m$ for any $n,m\in\mathbb N$. (3) Due to (2) we have that $R$ as an $R$-module is free of rank $k$ for any $k\in\mathbb N$.",,"['abstract-algebra', 'modules']"
57,Fun problems with binary operations.,Fun problems with binary operations.,,"Does anyone know of a book / internet resource containing lots of problems relating to properties of binary operations? An example of the sort of problem I'm looking for is: Let * be a binary operation on a set $A$ such that  $$\text{for all } x, y \in A, \, x*(y*x)=y.$$ Prove that  $$\text{for all } x, y \in A, \, (x*y)*x=y.$$","Does anyone know of a book / internet resource containing lots of problems relating to properties of binary operations? An example of the sort of problem I'm looking for is: Let * be a binary operation on a set $A$ such that  $$\text{for all } x, y \in A, \, x*(y*x)=y.$$ Prove that  $$\text{for all } x, y \in A, \, (x*y)*x=y.$$",,"['abstract-algebra', 'reference-request']"
58,Transcendental extension of field.,Transcendental extension of field.,,"Let $F(a)$ be a transcendental extension of the field $F$.  Given an element $b \in F(a)$ such that $b \notin F$, I would like to show that $F(a)$ is an algebraic extension of $F(b)$. My idea of the proof is that if $b = \dfrac{f(a)}{g(a)}$, where $f(x), g(x) \in F[x]$.  Then $a $ is a root of $$ bg(x)  - f(x)$$ which is a member of $F(b)[x]$. Is this correct?","Let $F(a)$ be a transcendental extension of the field $F$.  Given an element $b \in F(a)$ such that $b \notin F$, I would like to show that $F(a)$ is an algebraic extension of $F(b)$. My idea of the proof is that if $b = \dfrac{f(a)}{g(a)}$, where $f(x), g(x) \in F[x]$.  Then $a $ is a root of $$ bg(x)  - f(x)$$ which is a member of $F(b)[x]$. Is this correct?",,"['abstract-algebra', 'field-theory']"
59,Quotient-lifting properties,Quotient-lifting properties,,"I borrowed this terminology from K. Conrad's article on series of subgroups , in which he discusses solvability of groups. This property of certain groups satisfies Let $N\triangleleft G$. Then $G$ is solvable if and only if both $N$ and $G/N$ are solvable. This says solvability is a quotient-lifting property in the sense that it can be passed down to substructures and quotients, and it can also be lifted up. I think it is natural to abstract this a little bit. Say we are studying some structures $\mathcal{A}$, with a substructure $J$ and the quotient $\mathcal{A}/J$, and P is a certain property. We say P is quotient-lifting if $\mathcal{A}$ has P if and only if both $J$ and $\mathcal{A}/J$ have P . Apart from solvability of groups, being Noetherian is a quotient-lifting property of modules. Quotient-lifting properties allow us to break a large structure to smaller parts, and probably would make life easier. Also, whenever we have a quotient-lifting property P , it allows us to define a maximal object with property P inside a given $\mathcal{A}$. This follows from similar argument in Let $N,H\triangleleft G$. If both $N$ and $H$ are solvable then $NH$ is solvable. (So in particular, we can take all solvable normal subgroup $\{N_{\alpha}\}$ in $G$ then $\Pi N_{\alpha}$ is the maximal solvable normal subgroup in $G$.) I hope to learn more about these properties: 1) do we have some general results concerning quotient-lifting properties? 2) what are other nice examples of quotient-lifting properties? 3) apart from abstract algebra, do we have such things in other parts of mathematics? (Analysis? Geometry? ) Thanks!","I borrowed this terminology from K. Conrad's article on series of subgroups , in which he discusses solvability of groups. This property of certain groups satisfies Let $N\triangleleft G$. Then $G$ is solvable if and only if both $N$ and $G/N$ are solvable. This says solvability is a quotient-lifting property in the sense that it can be passed down to substructures and quotients, and it can also be lifted up. I think it is natural to abstract this a little bit. Say we are studying some structures $\mathcal{A}$, with a substructure $J$ and the quotient $\mathcal{A}/J$, and P is a certain property. We say P is quotient-lifting if $\mathcal{A}$ has P if and only if both $J$ and $\mathcal{A}/J$ have P . Apart from solvability of groups, being Noetherian is a quotient-lifting property of modules. Quotient-lifting properties allow us to break a large structure to smaller parts, and probably would make life easier. Also, whenever we have a quotient-lifting property P , it allows us to define a maximal object with property P inside a given $\mathcal{A}$. This follows from similar argument in Let $N,H\triangleleft G$. If both $N$ and $H$ are solvable then $NH$ is solvable. (So in particular, we can take all solvable normal subgroup $\{N_{\alpha}\}$ in $G$ then $\Pi N_{\alpha}$ is the maximal solvable normal subgroup in $G$.) I hope to learn more about these properties: 1) do we have some general results concerning quotient-lifting properties? 2) what are other nice examples of quotient-lifting properties? 3) apart from abstract algebra, do we have such things in other parts of mathematics? (Analysis? Geometry? ) Thanks!",,"['abstract-algebra', 'analysis', 'reference-request', 'intuition', 'examples-counterexamples']"
60,Is there a purely algebraic proof of the Fundamental Theorem of Algebra?,Is there a purely algebraic proof of the Fundamental Theorem of Algebra?,,"Among the many techniques available at our disposal to prove FTA, is there any purely algebraic proof of the theorem? That seems reasonably unexpected, because somehow or the other we are depending on the topological nature of $R$, and Wikipedia supports  the claim in these statements: ""In spite of its name, there is no purely algebraic proof of the theorem, since any proof must use the completeness of the reals (or some other equivalent formulation of completeness), which is not an algebraic concept."" Is it a proven fact that no pure algebraic proof is possible? And so if I assume we are relying on the topological properties of $R$ and $C$ to prove the theorem, then given any arbitrary field how can one test whether it is algebraically closed or not? Again in Wikipedia I found this result stated, ""The classic example is the theory of algebraically closed fields of a given characteristic. Categoricity does not say that all algebraically closed fields of characteristic 0 as large as the complex numbers C are the same as C; it only asserts that they are isomorphic as fields to C. It follows that although the completed p-adic closures Cp are all isomorphic as fields to C, they may (and in fact do) have completely different topological and analytic properties."" so i now want to rephrase my question as, given any field with different topological properties than C and which is in no simple way isomorphic to $C$ how can we generalize the proof of FTA to check whether FTA is valid on those fields? And what about characteristic p fields? i can see an example that algebraic closure of $F_p((t))$ is an example of infinite, characteristic p field that is by construction closed, but if we had in our arsenal other ways to describe the field and suppress the fact that it is algebraic closure of another field then how can one prove that it is algebraically closed? I don't understand anything about categoricity and such things, and I was only interested in the result taken, and I am sorry if it is a repost.","Among the many techniques available at our disposal to prove FTA, is there any purely algebraic proof of the theorem? That seems reasonably unexpected, because somehow or the other we are depending on the topological nature of $R$, and Wikipedia supports  the claim in these statements: ""In spite of its name, there is no purely algebraic proof of the theorem, since any proof must use the completeness of the reals (or some other equivalent formulation of completeness), which is not an algebraic concept."" Is it a proven fact that no pure algebraic proof is possible? And so if I assume we are relying on the topological properties of $R$ and $C$ to prove the theorem, then given any arbitrary field how can one test whether it is algebraically closed or not? Again in Wikipedia I found this result stated, ""The classic example is the theory of algebraically closed fields of a given characteristic. Categoricity does not say that all algebraically closed fields of characteristic 0 as large as the complex numbers C are the same as C; it only asserts that they are isomorphic as fields to C. It follows that although the completed p-adic closures Cp are all isomorphic as fields to C, they may (and in fact do) have completely different topological and analytic properties."" so i now want to rephrase my question as, given any field with different topological properties than C and which is in no simple way isomorphic to $C$ how can we generalize the proof of FTA to check whether FTA is valid on those fields? And what about characteristic p fields? i can see an example that algebraic closure of $F_p((t))$ is an example of infinite, characteristic p field that is by construction closed, but if we had in our arsenal other ways to describe the field and suppress the fact that it is algebraic closure of another field then how can one prove that it is algebraically closed? I don't understand anything about categoricity and such things, and I was only interested in the result taken, and I am sorry if it is a repost.",,"['abstract-algebra', 'field-theory']"
61,Dimension of An Ultraproduct Field as a Vector Space over Another Ultraproduct Field,Dimension of An Ultraproduct Field as a Vector Space over Another Ultraproduct Field,,"Suppose $F \subseteq K$ are fields with $G$ an ultrafilter on an infinite set $X$.  If $F^{\ast}$ and $K^{\ast}$ represent the ultraproducts respectively of $F$ and $K$, it is easy to see that $[K : F]$ is finite if and only if $[K^{\ast} : F^{\ast}]$ is finite.  Denote a typical element of $K^{\ast}$ by $a^{\ast}$, where $a$ is a member of $\prod_{i \in X}K$.  Also if $a \in K$, denote by $a^{\ast}$ the equivalence class in $K^{\ast}$ of the constant $a$-sequence in $\prod_{i \in X}K$. If $[K : F]$ is finite, let $v_1, ... , v_n$ be a basis for $K$ over $F$.  It then follows that $v_1^{\ast}, ... , v_n^{\ast}$ is a basis for $K^{\ast}$.  For if $a^{\ast} \in K^{\ast}$, then for every $i \in X$, $a(i) \in K$ so there exist scalars $c_{i1}, ... , c_{in} \in F$ such that $a(i) = c_{i1}v_1 + ... + c_{in}v_n$.  For each $1 \leq j \leq n$ we define $c_j \in \prod_{ i \in X}F$ by $c_j(i) = c_{ij}$.  It follows that, for every $i \in X$, $a(i) = c_1(i)v_1 + ... + c_n(i)v_n$, which implies $a^{\ast} = c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast}$.  Thus $v_1^{\ast}, ... , v_n^{\ast}$ span $K^{\ast}$.  To show linear independence, suppose $c_1^{\ast}, ... , c_n^{\ast} \in F^{\ast}$ are such that $c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast} = 0$.  Then $Q = \{ i \in X : c_1(i)v_1 + ... + c_n(i)v_n = 0 \} \in G$.  But for any such $i \in Q$, by the linear independence of $v_1, ... , v_n$ we get that $c_1(i), ... , c_n(i) = 0$.  In other words for every $1 \leq j \leq n$, $Q \subseteq \{ i \in X : c_j(i) = 0\}$.  But this just means that $c_1^{\ast}, ... , c_n^{\ast} = 0$.  This means that $v_1^{\ast}, ... , v_n^{\ast}$ form a basis for $K^{\ast}$ over $F^{\ast}$. Conversely if $[K : F]$ is infinite, let $B$ be a basis for $K$ over $F$.  Let $B'$ be the set with the elements $v \in B$ replaced by the equivalence class of their constant sequence in $\prod_{i \in X}K$, as $v^{\ast}$.  Then $B'$ is a linearly independent set, i.e. any finite subset thereof is linearly independent.  For if $v_1^{\ast}, ... , v_n^{\ast}$ are finitely many members of $B'$ with $c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast} = 0$ for some $c_1^{\ast}, c_2^{\ast}$ etc. $\in F^{\ast}$, then we can apply the same argument in the previous paragraph to show that $c_1^{\ast}, ... , c_n^{\ast}$ are all $0$.  Thus $B'$ is a linearly independent subset.  But any basis of $K^{\ast}$ would have cardinality at least as great as that of $B'$.  But there are as many elements in $B'$ as there are in $B$, which by supposition is infinite.  Therefore $[K^{\ast} : F^{\ast}]$ must be infinite. Now my question is, what can be said about the cardinality of $[K^{\ast} : F^{\ast}]$ in the infinite dimensional case, besides the fact that it is at least as great as $[K : F]$? For every set containing $B'$ I have tried, for example $\pi(\prod_{i \in X}B)$ (where $\pi: \prod_{i \in X}K \rightarrow K^{\ast}$ is the canonical homomorphism), linear independence or span are each too much to hope for.  An explicit basis for $K^{\ast}$ may be just about as easy to find as an explicit ultrafilter on $\mathbb{N}$.","Suppose $F \subseteq K$ are fields with $G$ an ultrafilter on an infinite set $X$.  If $F^{\ast}$ and $K^{\ast}$ represent the ultraproducts respectively of $F$ and $K$, it is easy to see that $[K : F]$ is finite if and only if $[K^{\ast} : F^{\ast}]$ is finite.  Denote a typical element of $K^{\ast}$ by $a^{\ast}$, where $a$ is a member of $\prod_{i \in X}K$.  Also if $a \in K$, denote by $a^{\ast}$ the equivalence class in $K^{\ast}$ of the constant $a$-sequence in $\prod_{i \in X}K$. If $[K : F]$ is finite, let $v_1, ... , v_n$ be a basis for $K$ over $F$.  It then follows that $v_1^{\ast}, ... , v_n^{\ast}$ is a basis for $K^{\ast}$.  For if $a^{\ast} \in K^{\ast}$, then for every $i \in X$, $a(i) \in K$ so there exist scalars $c_{i1}, ... , c_{in} \in F$ such that $a(i) = c_{i1}v_1 + ... + c_{in}v_n$.  For each $1 \leq j \leq n$ we define $c_j \in \prod_{ i \in X}F$ by $c_j(i) = c_{ij}$.  It follows that, for every $i \in X$, $a(i) = c_1(i)v_1 + ... + c_n(i)v_n$, which implies $a^{\ast} = c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast}$.  Thus $v_1^{\ast}, ... , v_n^{\ast}$ span $K^{\ast}$.  To show linear independence, suppose $c_1^{\ast}, ... , c_n^{\ast} \in F^{\ast}$ are such that $c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast} = 0$.  Then $Q = \{ i \in X : c_1(i)v_1 + ... + c_n(i)v_n = 0 \} \in G$.  But for any such $i \in Q$, by the linear independence of $v_1, ... , v_n$ we get that $c_1(i), ... , c_n(i) = 0$.  In other words for every $1 \leq j \leq n$, $Q \subseteq \{ i \in X : c_j(i) = 0\}$.  But this just means that $c_1^{\ast}, ... , c_n^{\ast} = 0$.  This means that $v_1^{\ast}, ... , v_n^{\ast}$ form a basis for $K^{\ast}$ over $F^{\ast}$. Conversely if $[K : F]$ is infinite, let $B$ be a basis for $K$ over $F$.  Let $B'$ be the set with the elements $v \in B$ replaced by the equivalence class of their constant sequence in $\prod_{i \in X}K$, as $v^{\ast}$.  Then $B'$ is a linearly independent set, i.e. any finite subset thereof is linearly independent.  For if $v_1^{\ast}, ... , v_n^{\ast}$ are finitely many members of $B'$ with $c_1^{\ast}v_1^{\ast} + ... + c_n^{\ast}v_n^{\ast} = 0$ for some $c_1^{\ast}, c_2^{\ast}$ etc. $\in F^{\ast}$, then we can apply the same argument in the previous paragraph to show that $c_1^{\ast}, ... , c_n^{\ast}$ are all $0$.  Thus $B'$ is a linearly independent subset.  But any basis of $K^{\ast}$ would have cardinality at least as great as that of $B'$.  But there are as many elements in $B'$ as there are in $B$, which by supposition is infinite.  Therefore $[K^{\ast} : F^{\ast}]$ must be infinite. Now my question is, what can be said about the cardinality of $[K^{\ast} : F^{\ast}]$ in the infinite dimensional case, besides the fact that it is at least as great as $[K : F]$? For every set containing $B'$ I have tried, for example $\pi(\prod_{i \in X}B)$ (where $\pi: \prod_{i \in X}K \rightarrow K^{\ast}$ is the canonical homomorphism), linear independence or span are each too much to hope for.  An explicit basis for $K^{\ast}$ may be just about as easy to find as an explicit ultrafilter on $\mathbb{N}$.",,"['abstract-algebra', 'field-theory', 'filters']"
62,Notation for n-ary exponentiation,Notation for n-ary exponentiation,,"We have $n$-ary sums ($\sum$) and products ($\prod$). Is there an $n$-ary exponentiation operator? $$\underset{i=1}{\overset{n}{\LARGE{\text{E}}}}\, x_i = x_1 \text{^} (x_2 \text{^} (\cdots \text{^} (x_{n-1} \text{^} x_n)))$$ How about an $n$-ary tetration operator? $$\underset{i=1}{\overset{n}{\boxed{\underset{\leftarrow}{\LARGE{\text{4}}}}}}\, x_i = x_1 \uparrow\uparrow (x_2 \uparrow\uparrow (\cdots \uparrow\uparrow (x_{n-1} \uparrow\uparrow x_n)))$$ $$\underset{i=1}{\overset{n}{\boxed{\underset{\rightarrow}{\LARGE{\text{4}}}}}}\, x_i = (((x_1 \uparrow\uparrow x_2) \uparrow\uparrow \cdots) \uparrow\uparrow x_{n-1}) \uparrow\uparrow x_n$$ What are the correct symbols for these operations, if they exist?","We have $n$-ary sums ($\sum$) and products ($\prod$). Is there an $n$-ary exponentiation operator? $$\underset{i=1}{\overset{n}{\LARGE{\text{E}}}}\, x_i = x_1 \text{^} (x_2 \text{^} (\cdots \text{^} (x_{n-1} \text{^} x_n)))$$ How about an $n$-ary tetration operator? $$\underset{i=1}{\overset{n}{\boxed{\underset{\leftarrow}{\LARGE{\text{4}}}}}}\, x_i = x_1 \uparrow\uparrow (x_2 \uparrow\uparrow (\cdots \uparrow\uparrow (x_{n-1} \uparrow\uparrow x_n)))$$ $$\underset{i=1}{\overset{n}{\boxed{\underset{\rightarrow}{\LARGE{\text{4}}}}}}\, x_i = (((x_1 \uparrow\uparrow x_2) \uparrow\uparrow \cdots) \uparrow\uparrow x_{n-1}) \uparrow\uparrow x_n$$ What are the correct symbols for these operations, if they exist?",,"['abstract-algebra', 'notation', 'exponentiation', 'tetration', 'hyperoperation']"
63,On the Nakayama functor,On the Nakayama functor,,"Let $A$ be a finite dimensional $k$-algebra with 1. Denote by $_AP$ the category of projective left $A$-modules finite dimensional. And with $_AI$ the category of injective left $A$-modules finite dimensional. Denote by $D$ the duality with respect to $k$, i.e. $D(M)=\mathrm{Hom}_k(M,k)$. Then the following functor is a functor from $_AP$ to $_AI$: $\nu=D\mathrm{Hom}_A(\;.\;,_AA)$ how can I prove it? Define also $\nu^{-1}=\mathrm{Hom}_A(D(A_A),\;.\;)$ how can I prove that this functor goes from $_AI$ into $_AP$? I want to prove that $\nu$ and $\nu^{-1}$ are quasi-inverse, how can I do it? Here they say there is an invertible natural transformation $\alpha_P:D\mathrm{Hom}(P,\;.\;)\rightarrow\mathrm{Hom}(\;.\;,\nu P)$ but I don't understand why it exists, why it is invertible and why this implies $\nu$ and $\nu^{-1}$ are quasi-inverse. Any help?","Let $A$ be a finite dimensional $k$-algebra with 1. Denote by $_AP$ the category of projective left $A$-modules finite dimensional. And with $_AI$ the category of injective left $A$-modules finite dimensional. Denote by $D$ the duality with respect to $k$, i.e. $D(M)=\mathrm{Hom}_k(M,k)$. Then the following functor is a functor from $_AP$ to $_AI$: $\nu=D\mathrm{Hom}_A(\;.\;,_AA)$ how can I prove it? Define also $\nu^{-1}=\mathrm{Hom}_A(D(A_A),\;.\;)$ how can I prove that this functor goes from $_AI$ into $_AP$? I want to prove that $\nu$ and $\nu^{-1}$ are quasi-inverse, how can I do it? Here they say there is an invertible natural transformation $\alpha_P:D\mathrm{Hom}(P,\;.\;)\rightarrow\mathrm{Hom}(\;.\;,\nu P)$ but I don't understand why it exists, why it is invertible and why this implies $\nu$ and $\nu^{-1}$ are quasi-inverse. Any help?",,"['abstract-algebra', 'ring-theory', 'category-theory', 'modules', 'noncommutative-algebra']"
64,"Prove $(\mathbb Z \times \mathbb Z, \Sigma)$ to be a partial order and tell if its subset $T'$ is a lattice",Prove  to be a partial order and tell if its subset  is a lattice,"(\mathbb Z \times \mathbb Z, \Sigma) T'","Let $T = (\mathbb Z\times\mathbb Z, \Sigma) $ be defined as follows: $$\begin{aligned} (a,b) \text{ } \Sigma  \text { } (c,d) \Leftrightarrow (a,b) = (c,d) \text{ or } a^2b^2<c^2d^2\end{aligned}$$ check $\Sigma $ is a partial order and it's not total; search for $\min(T)$, $\max(T)$, minimal and maximal elements, if there're any. Take $T' = \{(1,6), (-1,1), (0,1), (-1,-1), (2,-3)\} \subset \mathbb Z\times\mathbb Z$ then: draw an Hasse diagram for $(T', \Sigma)$, search for $\min(T')$, $\max(T')$, minimal and maximal elements, supremum and infimum, upper and lower bounds; tell if $(T', \Sigma)$ a totally ordered set; tell if it is a lattice. In order to prove $(\mathbb Z \times \mathbb Z,\Sigma)$ a partial order set, $\Sigma$ reflexivity, anti-symmetry and transitivity has to be shown. Reflexivity $\forall (a,b) \in \mathbb Z\times \mathbb Z$ $(a,b) \text { } \Sigma \text { } (a,b) $ as $(a,b) = (a,b)$. Anti-symmetry Let $(a,b),(c,d) \in \mathbb Z \times \mathbb Z$, then $\Sigma$ is anti-symmetric if $(a,b) \text { } \Sigma \text{ }(c,d)$ and $(c,d) \text { } \Sigma \text{ }(a,b) \Rightarrow (a,b) = (c,d)$ hence $\forall (a,b),(c,d)$ should be valid the following $a^2b^2<c^2d^2$ and $c^2d^2<a^2b^2 \Rightarrow (a,b) = (c,d)$ which clearly can't be, so anti-symmetry is only valid if $(a,b) = (c,d)$. Transitivity $\forall (a,b),(c,d),(e,f) \in \mathbb Z \times \mathbb Z$ $(a,b) \text { }  \Sigma \text { }  (c,d)$ and $(c,d) \text { }  \Sigma \text { }  (e,f) \Rightarrow (a,b) \text { }  \Sigma \text { }  (e,f)$ which is true, because (assuming all pairs being distinct): $a^2b^2 < c^2d^2$ and $c^2d^2<e^2f^2$ then $a^2b^2<e^2f^2$ Conclusion $(\mathbb Z \times \mathbb Z,\Sigma)$ is actually a partial order set and not total. In $(\mathbb Z \times \mathbb Z)$ we can find all $(a,b) : a = 0 \text { or } b = 0$ are minimal elements. However no maximum, minimum or massimal elements have been found. The Hasse diagram for the above mentioned set is: $(T', \Sigma)$ is not an ordered set, because $\forall (a,b),(c,d) \in T'$ it has to happen $(a,b) \text { } \Sigma \text{ } (c,d)$ or $(c,d) \text { } \Sigma \text{ } (a,b)$, but for the elements $(-1,1),(-1,-1)$ the $\Sigma$ relation doesn't apply. $\sup(T') = (1,6)$, whereas $(1,6)$ is also a maximal. $\inf(T') = (0,1)$, whereas $(0,1)$ is also a minimal. upper bounds are $\{(a,b) \in \mathbb Z \times \mathbb Z : a^2b^2 > 36\}$. no maximum or minumum or lower bounds found. My question would be: does everything hold? How about the anti-symmetry that is valid only if two pairs are equal? Does it mean in $T'$ the relation $\Sigma$ desn't really apply?  How do I prove if this is a lattice?","Let $T = (\mathbb Z\times\mathbb Z, \Sigma) $ be defined as follows: $$\begin{aligned} (a,b) \text{ } \Sigma  \text { } (c,d) \Leftrightarrow (a,b) = (c,d) \text{ or } a^2b^2<c^2d^2\end{aligned}$$ check $\Sigma $ is a partial order and it's not total; search for $\min(T)$, $\max(T)$, minimal and maximal elements, if there're any. Take $T' = \{(1,6), (-1,1), (0,1), (-1,-1), (2,-3)\} \subset \mathbb Z\times\mathbb Z$ then: draw an Hasse diagram for $(T', \Sigma)$, search for $\min(T')$, $\max(T')$, minimal and maximal elements, supremum and infimum, upper and lower bounds; tell if $(T', \Sigma)$ a totally ordered set; tell if it is a lattice. In order to prove $(\mathbb Z \times \mathbb Z,\Sigma)$ a partial order set, $\Sigma$ reflexivity, anti-symmetry and transitivity has to be shown. Reflexivity $\forall (a,b) \in \mathbb Z\times \mathbb Z$ $(a,b) \text { } \Sigma \text { } (a,b) $ as $(a,b) = (a,b)$. Anti-symmetry Let $(a,b),(c,d) \in \mathbb Z \times \mathbb Z$, then $\Sigma$ is anti-symmetric if $(a,b) \text { } \Sigma \text{ }(c,d)$ and $(c,d) \text { } \Sigma \text{ }(a,b) \Rightarrow (a,b) = (c,d)$ hence $\forall (a,b),(c,d)$ should be valid the following $a^2b^2<c^2d^2$ and $c^2d^2<a^2b^2 \Rightarrow (a,b) = (c,d)$ which clearly can't be, so anti-symmetry is only valid if $(a,b) = (c,d)$. Transitivity $\forall (a,b),(c,d),(e,f) \in \mathbb Z \times \mathbb Z$ $(a,b) \text { }  \Sigma \text { }  (c,d)$ and $(c,d) \text { }  \Sigma \text { }  (e,f) \Rightarrow (a,b) \text { }  \Sigma \text { }  (e,f)$ which is true, because (assuming all pairs being distinct): $a^2b^2 < c^2d^2$ and $c^2d^2<e^2f^2$ then $a^2b^2<e^2f^2$ Conclusion $(\mathbb Z \times \mathbb Z,\Sigma)$ is actually a partial order set and not total. In $(\mathbb Z \times \mathbb Z)$ we can find all $(a,b) : a = 0 \text { or } b = 0$ are minimal elements. However no maximum, minimum or massimal elements have been found. The Hasse diagram for the above mentioned set is: $(T', \Sigma)$ is not an ordered set, because $\forall (a,b),(c,d) \in T'$ it has to happen $(a,b) \text { } \Sigma \text{ } (c,d)$ or $(c,d) \text { } \Sigma \text{ } (a,b)$, but for the elements $(-1,1),(-1,-1)$ the $\Sigma$ relation doesn't apply. $\sup(T') = (1,6)$, whereas $(1,6)$ is also a maximal. $\inf(T') = (0,1)$, whereas $(0,1)$ is also a minimal. upper bounds are $\{(a,b) \in \mathbb Z \times \mathbb Z : a^2b^2 > 36\}$. no maximum or minumum or lower bounds found. My question would be: does everything hold? How about the anti-symmetry that is valid only if two pairs are equal? Does it mean in $T'$ the relation $\Sigma$ desn't really apply?  How do I prove if this is a lattice?",,"['abstract-algebra', 'lattice-orders']"
65,Quotient of a Clifford algebra by its radical is a Clifford algebra?,Quotient of a Clifford algebra by its radical is a Clifford algebra?,,"I'm fumbling a bit in my reading on Clifford algebras. I'm hoping someone can shed some light on the following isomorphism. Suppose you have a symmetric bilinear form $G$ over a vector space $V$, and let $\mathrm{Cl}_G(V)$ be the corresponding Clifford algebra. I'll denote it by $C_G$ for short when the vector space is clear. Now $G$ induces a form $\hat{G}$ on $V/\ker(G)$, and apparently $C_G/\mathrm{rad}(C_G)\cong C_{\hat{G}}$, where $\mathrm{rad}(C_G)$ is the radical of $C_G$. I thought this will fall out easily from some application of the isomorphism theorems, but some epimorphism $C_G\to C_{\hat{G}}$ whose kernel conveniently happens to be $\mathrm{rad}(C_G)$. However, I can't quite find such a map. Does anyone see what the trick is here? Thanks. Later. I believe I now understand that there is a surjective map $p:C(\beta)\to C(\bar{\beta})$ induced by the quotient map $V\to V/\ker\beta$ described below. If $I$ is the ideal generated by $\ker\beta$, then since $p(\ker\beta)=0$ in $C(\bar{\beta})$, it follows that $I\subset\ker p$. However, I don't understand why $I$ is nilpotent. What is the explanation for this last bit?","I'm fumbling a bit in my reading on Clifford algebras. I'm hoping someone can shed some light on the following isomorphism. Suppose you have a symmetric bilinear form $G$ over a vector space $V$, and let $\mathrm{Cl}_G(V)$ be the corresponding Clifford algebra. I'll denote it by $C_G$ for short when the vector space is clear. Now $G$ induces a form $\hat{G}$ on $V/\ker(G)$, and apparently $C_G/\mathrm{rad}(C_G)\cong C_{\hat{G}}$, where $\mathrm{rad}(C_G)$ is the radical of $C_G$. I thought this will fall out easily from some application of the isomorphism theorems, but some epimorphism $C_G\to C_{\hat{G}}$ whose kernel conveniently happens to be $\mathrm{rad}(C_G)$. However, I can't quite find such a map. Does anyone see what the trick is here? Thanks. Later. I believe I now understand that there is a surjective map $p:C(\beta)\to C(\bar{\beta})$ induced by the quotient map $V\to V/\ker\beta$ described below. If $I$ is the ideal generated by $\ker\beta$, then since $p(\ker\beta)=0$ in $C(\bar{\beta})$, it follows that $I\subset\ker p$. However, I don't understand why $I$ is nilpotent. What is the explanation for this last bit?",,"['abstract-algebra', 'clifford-algebras']"
66,Is there an upper bound to the number of rings that can be obtained from a semigroup with zero by defining an additive operation?,Is there an upper bound to the number of rings that can be obtained from a semigroup with zero by defining an additive operation?,,"Let $\mathscr S$ be the class of all semigroups with zero. For $(S,\times,0)\in\mathscr S,$ I want to count additive operations $+$ on $S$ such that $(S,+,\times,0)$ is a ring (possibly without unity). For $S\in\mathscr S,$ let $\Sigma_S$ be the set of all such additive operations on $S$. Let, for $+_1,+_2\in \Sigma_S,$ define  $+_1\sim +_2$ to mean that $$(S,+_1,\times,0)\cong (S,+_2,\times,0).$$ Let $$\Sigma'_S:=\Sigma_S/\sim.$$ Let $$\kappa_S:=\operatorname{card}(\Sigma_S)$$ and $$\kappa'_S:=\operatorname{card}(\Sigma'_S).$$ Are there upper bounds to the values of $\kappa_S$ and $\kappa'_S$ for $S\in \mathscr S?$ Do they admit all non-negative integer values? If not, which non-negative integer values do they admit? These questions are not similar to anything I know and I don't even know how to search for answers in literature. I'm especially curious what the behavior of $\kappa_S$ and $\kappa'_S$ is for finite semigroups $S.$ EDIT I have found this question on MO. Arturo Magidin gives an example there of two non-isomorphic rings having isomorphic multiplicative structures. If I understand correctly, this is proven by using the unique factorization property in polynomial rings over rings with the unique factorization property. So my question isn't trivial, which I didn't know at the time of asking: there is a semigroup with zero for which there are at least two additive operations making the semigroup two non-isomorphic rings. EDIT An answer to the following questions wouldn't be off-topic: Is there a semigroup $S$ with $0$ such that $\kappa_S$ is infinite? Is there a semigroup $S$ with $0$ such that $\kappa'_S$ is infinite? (If the answer is ""yes"", this doesn't answer any of my previous questions, but I would be interested in knowing it nonetheless.)","Let $\mathscr S$ be the class of all semigroups with zero. For $(S,\times,0)\in\mathscr S,$ I want to count additive operations $+$ on $S$ such that $(S,+,\times,0)$ is a ring (possibly without unity). For $S\in\mathscr S,$ let $\Sigma_S$ be the set of all such additive operations on $S$. Let, for $+_1,+_2\in \Sigma_S,$ define  $+_1\sim +_2$ to mean that $$(S,+_1,\times,0)\cong (S,+_2,\times,0).$$ Let $$\Sigma'_S:=\Sigma_S/\sim.$$ Let $$\kappa_S:=\operatorname{card}(\Sigma_S)$$ and $$\kappa'_S:=\operatorname{card}(\Sigma'_S).$$ Are there upper bounds to the values of $\kappa_S$ and $\kappa'_S$ for $S\in \mathscr S?$ Do they admit all non-negative integer values? If not, which non-negative integer values do they admit? These questions are not similar to anything I know and I don't even know how to search for answers in literature. I'm especially curious what the behavior of $\kappa_S$ and $\kappa'_S$ is for finite semigroups $S.$ EDIT I have found this question on MO. Arturo Magidin gives an example there of two non-isomorphic rings having isomorphic multiplicative structures. If I understand correctly, this is proven by using the unique factorization property in polynomial rings over rings with the unique factorization property. So my question isn't trivial, which I didn't know at the time of asking: there is a semigroup with zero for which there are at least two additive operations making the semigroup two non-isomorphic rings. EDIT An answer to the following questions wouldn't be off-topic: Is there a semigroup $S$ with $0$ such that $\kappa_S$ is infinite? Is there a semigroup $S$ with $0$ such that $\kappa'_S$ is infinite? (If the answer is ""yes"", this doesn't answer any of my previous questions, but I would be interested in knowing it nonetheless.)",,"['abstract-algebra', 'ring-theory']"
67,Computing contractions of ideals in Macaulay2,Computing contractions of ideals in Macaulay2,,"Does Macaulay2 compute contractions of ideals under ring homomorphisms. Specifically, if $R\subseteq S$ is a ring extension (say polynomial rings over $\mathbb{Q}$ which can be specified in M2) and $I$ is an ideal in $S$ given by generators, is there a command to compute $I\cap R$? EDIT: The eliminate command is supposed to do what I want, except when I use it the output is an ideal in the original ring.","Does Macaulay2 compute contractions of ideals under ring homomorphisms. Specifically, if $R\subseteq S$ is a ring extension (say polynomial rings over $\mathbb{Q}$ which can be specified in M2) and $I$ is an ideal in $S$ given by generators, is there a command to compute $I\cap R$? EDIT: The eliminate command is supposed to do what I want, except when I use it the output is an ideal in the original ring.",,"['abstract-algebra', 'commutative-algebra', 'math-software', 'symbolic-computation', 'macaulay2']"
68,Examples of Noncommutative Noetherian Rings in which Lasker-Noether Fails?,Examples of Noncommutative Noetherian Rings in which Lasker-Noether Fails?,,"I'm writing a paper on Emmy Noether for my introductory Abstract Algebra class, and I'm looking for examples of noncommutative Noetherian rings in which the Lasker-Noether theorem fails to hold. According to the wikipedia article, Noether herself presented a counterexample, although it does not link to any information regarding an example. I can't read German, but maybe it's listed in her 1921 article somewhere? I'm really just looking one or two simple examples, even if they weren't proposed by Noether herself. All help is greatly appreciated. Thank you. Here is the wikipedia article: http://en.wikipedia.org/wiki/Lasker%E2%80%93Noether_theorem Here is her original paper: Link","I'm writing a paper on Emmy Noether for my introductory Abstract Algebra class, and I'm looking for examples of noncommutative Noetherian rings in which the Lasker-Noether theorem fails to hold. According to the wikipedia article, Noether herself presented a counterexample, although it does not link to any information regarding an example. I can't read German, but maybe it's listed in her 1921 article somewhere? I'm really just looking one or two simple examples, even if they weren't proposed by Noether herself. All help is greatly appreciated. Thank you. Here is the wikipedia article: http://en.wikipedia.org/wiki/Lasker%E2%80%93Noether_theorem Here is her original paper: Link",,"['abstract-algebra', 'noncommutative-algebra']"
69,Homogeneous polynomial in $n$ variables of degree greater than $n(n-1)$ is in the ideal generated by the elementary symmetric polynomials,Homogeneous polynomial in  variables of degree greater than  is in the ideal generated by the elementary symmetric polynomials,n n(n-1),"Let $f(x_1,...,x_n) \in \mathbb{Z}[x_1,...,x_n]$ be homogeneous of degree  $d>n(n-1)$, i.e. $f$ is the sum of monomials of degree $d$. I am looking for a hint to prove that $f$ is in the ideal generated by the elementary symmetric polynomials $s_1,...,s_n$.","Let $f(x_1,...,x_n) \in \mathbb{Z}[x_1,...,x_n]$ be homogeneous of degree  $d>n(n-1)$, i.e. $f$ is the sum of monomials of degree $d$. I am looking for a hint to prove that $f$ is in the ideal generated by the elementary symmetric polynomials $s_1,...,s_n$.",,['abstract-algebra']
70,Identifying a certain subgroup of a free group,Identifying a certain subgroup of a free group,,"Let $F$ be the free group on the generators $a_1,\ldots,a_n$. Define homomorphisms $\phi_i:F\to F$ by $\phi_i(a_j)=a_j^{1-\delta_{ij}}$, where $\delta_{ij}$ is the Kronecker delta; basically, $\phi_i$ kills the $i$-th generator and fixes the others. Denote $N=\bigcap_{i=1}^n\ker\phi_i$. I want to identify the subgroup $N$ or at least find its index in $F$. Clearly we have $F'\subseteq N$, where $F'$ is the commutator subgroup. Is it in fact the case that $N=F'$? Edited: it has been pointed out in the comments that I was very hasty in the previous paragraph. The inclusion I stated holds only if $n\leq2$. However, $N$ is not trivial as it contains the element $[\ldots[a_1,a_2],a_3]\ldots]$","Let $F$ be the free group on the generators $a_1,\ldots,a_n$. Define homomorphisms $\phi_i:F\to F$ by $\phi_i(a_j)=a_j^{1-\delta_{ij}}$, where $\delta_{ij}$ is the Kronecker delta; basically, $\phi_i$ kills the $i$-th generator and fixes the others. Denote $N=\bigcap_{i=1}^n\ker\phi_i$. I want to identify the subgroup $N$ or at least find its index in $F$. Clearly we have $F'\subseteq N$, where $F'$ is the commutator subgroup. Is it in fact the case that $N=F'$? Edited: it has been pointed out in the comments that I was very hasty in the previous paragraph. The inclusion I stated holds only if $n\leq2$. However, $N$ is not trivial as it contains the element $[\ldots[a_1,a_2],a_3]\ldots]$",,"['abstract-algebra', 'group-theory', 'free-groups']"
71,"Since $K/F$ is Galois, every embedding of $K$ fixing $F$ is an automorphism of $K$?","Since  is Galois, every embedding of  fixing  is an automorphism of ?",K/F K F K,"I'm trying to read a proof in Dummit and Foote of the statement Suppose $K/F$ is a Galois extension and $F'/F$ is any extension.  Then $KF'/F'$ is a Galois extension, and $Gal(KF'/F') \cong Gal(K/K \cap F')$. One line I am confused about is Since $K/F$ is Galois, every embedding of $K$ fixing $F$ is an automorphism of $K$, so the map $\varphi: Gal(KF'/F') \to Gal(K/F$), $\sigma \mapsto \sigma\vert_K$ defined by restricting an automorphism $\sigma$ to the subfield $K$ is well-defined. I take it this means automorphisms of $KF'$ fixing $F'$ (thus fixing $F$) also send $K$ to $K$?  If that's what it means, why is it true? Thanks!","I'm trying to read a proof in Dummit and Foote of the statement Suppose $K/F$ is a Galois extension and $F'/F$ is any extension.  Then $KF'/F'$ is a Galois extension, and $Gal(KF'/F') \cong Gal(K/K \cap F')$. One line I am confused about is Since $K/F$ is Galois, every embedding of $K$ fixing $F$ is an automorphism of $K$, so the map $\varphi: Gal(KF'/F') \to Gal(K/F$), $\sigma \mapsto \sigma\vert_K$ defined by restricting an automorphism $\sigma$ to the subfield $K$ is well-defined. I take it this means automorphisms of $KF'$ fixing $F'$ (thus fixing $F$) also send $K$ to $K$?  If that's what it means, why is it true? Thanks!",,['abstract-algebra']
72,Is there an integral domain with a lot of residue fields of the same characteristic?,Is there an integral domain with a lot of residue fields of the same characteristic?,,"Is there a commutative integral domain $R$ in which: every nonzero prime ideal $Q$ is maximal, and for every prime power $q\equiv  3 \bmod 8$, there is a maximal ideal $Q$ of $R$ such that $R/Q$ is a field of size $q$? I am looking for a ring where ""$q\equiv  3 \bmod 8$"" describes finite fields, rather than just finite local rings like in the ring $\mathbb{Z}$.  The only examples I can think of that satisfy the first condition have a finite number of residue fields of each characteristic.  The only examples I can think of that satisfy the second condition have a ton of non-maximal non-proper prime ideals with all sorts of bizarre fields associated to them.","Is there a commutative integral domain $R$ in which: every nonzero prime ideal $Q$ is maximal, and for every prime power $q\equiv  3 \bmod 8$, there is a maximal ideal $Q$ of $R$ such that $R/Q$ is a field of size $q$? I am looking for a ring where ""$q\equiv  3 \bmod 8$"" describes finite fields, rather than just finite local rings like in the ring $\mathbb{Z}$.  The only examples I can think of that satisfy the first condition have a finite number of residue fields of each characteristic.  The only examples I can think of that satisfy the second condition have a ton of non-maximal non-proper prime ideals with all sorts of bizarre fields associated to them.",,"['abstract-algebra', 'commutative-algebra', 'algebraic-number-theory']"
73,A ring in which the two operations are equal is {0},A ring in which the two operations are equal is {0},,"Let R be a ring in which the two operations are equal, i.e., $ a + b = ab \mbox{  }\forall a,b \in R $. Prove that $R = \{0 \}$. I tried to prove that $R \subset \{0 \} $ and $ \{0 \} \subset R $. For the second inclusion, we have $ 0 + 0 = 0 = 0 \cdot 0 $. So $\{0 \} \subset R $.  However, I can't figure out a way of showing that $R \subset \{0 \} $. Any tips?","Let R be a ring in which the two operations are equal, i.e., $ a + b = ab \mbox{  }\forall a,b \in R $. Prove that $R = \{0 \}$. I tried to prove that $R \subset \{0 \} $ and $ \{0 \} \subset R $. For the second inclusion, we have $ 0 + 0 = 0 = 0 \cdot 0 $. So $\{0 \} \subset R $.  However, I can't figure out a way of showing that $R \subset \{0 \} $. Any tips?",,"['abstract-algebra', 'ring-theory']"
74,How could I know that $X^4+1$ is $(X^2+\sqrt 2X+1)(X^2-\sqrt 2X+1)$?,How could I know that  is ?,X^4+1 (X^2+\sqrt 2X+1)(X^2-\sqrt 2X+1),"I thought that $X^4+1$ was irreducible, but in fact, $$X^4+1=(X^2+\sqrt 2X+1)(X^2-\sqrt 2X+1).$$ In general, how can I have the intuition of such a factorisation if I don't know it ?","I thought that $X^4+1$ was irreducible, but in fact, $$X^4+1=(X^2+\sqrt 2X+1)(X^2-\sqrt 2X+1).$$ In general, how can I have the intuition of such a factorisation if I don't know it ?",,"['abstract-algebra', 'polynomials']"
75,how do I prove that $1 > 0$ in an ordered field?,how do I prove that  in an ordered field?,1 > 0,"I've started studying calculus.  As part of studies I've encountered a question. How does one prove that $1 > 0$? I tried proving it by contradiction by saying that $1 < 0$, but I can't seem to contradict this hypothesis. Any help will be welcomed.","I've started studying calculus.  As part of studies I've encountered a question. How does one prove that $1 > 0$? I tried proving it by contradiction by saying that $1 < 0$, but I can't seem to contradict this hypothesis. Any help will be welcomed.",,['abstract-algebra']
76,Showing two polynomial rings over $\mathbb{C}$ aren't isomorphic,Showing two polynomial rings over  aren't isomorphic,\mathbb{C},"Im trying to show that the ring of polynomials in one variable over the complex numbers is not isomorphic to the ring over $\mathbb C$ with two variables $x$ and $y$ modulo $\langle x^2-y^3\rangle$. I've shown previously that if the relationship $p^2=q^3$ holds for some $p$ and $q$ in one variable, there exists $r$ such that $p=r^3$ and $q=r^2$. I'm assuming this helps in some way but I'm not precisely sure how.","Im trying to show that the ring of polynomials in one variable over the complex numbers is not isomorphic to the ring over $\mathbb C$ with two variables $x$ and $y$ modulo $\langle x^2-y^3\rangle$. I've shown previously that if the relationship $p^2=q^3$ holds for some $p$ and $q$ in one variable, there exists $r$ such that $p=r^3$ and $q=r^2$. I'm assuming this helps in some way but I'm not precisely sure how.",,"['abstract-algebra', 'polynomials', 'ring-theory']"
77,Showing that $7+\sqrt[3]{2}$ is an algebraic number,Showing that  is an algebraic number,7+\sqrt[3]{2},"How do I go about showing that $7+\sqrt[3]{2}$ is an algebraic number? I need to show that it is the root of an integer valued formal polynomial? How do I solve these problems in general? I haven't a clue where to start on this one, so it is hard to give context. The problem is perhaps solved using methods of ring theory.","How do I go about showing that $7+\sqrt[3]{2}$ is an algebraic number? I need to show that it is the root of an integer valued formal polynomial? How do I solve these problems in general? I haven't a clue where to start on this one, so it is hard to give context. The problem is perhaps solved using methods of ring theory.",,['abstract-algebra']
78,"Show that $(\mathbb{Q}^*,\cdot)$ and $(\mathbb{R}^*,\cdot)$ aren't cyclic",Show that  and  aren't cyclic,"(\mathbb{Q}^*,\cdot) (\mathbb{R}^*,\cdot)","I'm reading a book about abstract algebra, but I'm having trouble solving this excercise: ""Show that $(\mathbb{Q}^*,\cdot)$ and $(\mathbb{R}^*,\cdot)$ aren't cyclic"" Where $(\mathbb{Q}^*,\cdot)$ is the group of nonzero rational numbers under multiplication and $(\mathbb{R}^*,\cdot)$ is the group of nonzero real numbers under multiplication. Here is my attempt for the first. Suppose $(\mathbb{Q}^*,\cdot)$ is cyclic, then $\mathbb{Q}^*=\langle\frac{p}{q}\rangle=\{(\frac{p}{q})^n,n\in\mathbb{Z}\}$, where $p$ and $q$ are coprime. $\frac{2p}{q}$ is also in $\mathbb{Q}^*$ so it must be equal to $(\frac{p}{q})^n$ for some $n\in\mathbb{Z}$. To solve $\frac{2p}{q}=(\frac{p}{q})^n$, I take a logarithm of both sides and end up with $1+\log_\frac{p}{q}(2)=n$, since $n$ is an integer $\log_\frac{p}{q}(2)$ must be an integer too, but it is possible only when $\frac{p}{q}=2^{\frac{1}{k}}, k\in\mathbb{N}$, (i.e. $\frac{p}{q}$ is a k-th root of $2$), but $k$ must be $1$ for $2^\frac{1}{k}$ to be rational so $\frac{p}{q}=2$ contradicting the hypothesis of $p$ and $q$ being coprime. However I don't know whether this is a proper proof and the same reasoning cannot be applied to $\mathbb{R}^*$, I'd like you to just give me an hint towards a proof, without telling me the whole proof, if possible.","I'm reading a book about abstract algebra, but I'm having trouble solving this excercise: ""Show that $(\mathbb{Q}^*,\cdot)$ and $(\mathbb{R}^*,\cdot)$ aren't cyclic"" Where $(\mathbb{Q}^*,\cdot)$ is the group of nonzero rational numbers under multiplication and $(\mathbb{R}^*,\cdot)$ is the group of nonzero real numbers under multiplication. Here is my attempt for the first. Suppose $(\mathbb{Q}^*,\cdot)$ is cyclic, then $\mathbb{Q}^*=\langle\frac{p}{q}\rangle=\{(\frac{p}{q})^n,n\in\mathbb{Z}\}$, where $p$ and $q$ are coprime. $\frac{2p}{q}$ is also in $\mathbb{Q}^*$ so it must be equal to $(\frac{p}{q})^n$ for some $n\in\mathbb{Z}$. To solve $\frac{2p}{q}=(\frac{p}{q})^n$, I take a logarithm of both sides and end up with $1+\log_\frac{p}{q}(2)=n$, since $n$ is an integer $\log_\frac{p}{q}(2)$ must be an integer too, but it is possible only when $\frac{p}{q}=2^{\frac{1}{k}}, k\in\mathbb{N}$, (i.e. $\frac{p}{q}$ is a k-th root of $2$), but $k$ must be $1$ for $2^\frac{1}{k}$ to be rational so $\frac{p}{q}=2$ contradicting the hypothesis of $p$ and $q$ being coprime. However I don't know whether this is a proper proof and the same reasoning cannot be applied to $\mathbb{R}^*$, I'd like you to just give me an hint towards a proof, without telling me the whole proof, if possible.",,"['abstract-algebra', 'proof-verification']"
79,"If $N$ and $M$ are normal subgroups and $N$ and $M$ have no common element other than $e$ then prove that for all $m \in M$ and $n\in N$, $mn=nm$.","If  and  are normal subgroups and  and  have no common element other than  then prove that for all  and , .",N M N M e m \in M n\in N mn=nm,My approach ; I proved the $MN$ to be a normal sub group whence $mn=nm$.,My approach ; I proved the $MN$ to be a normal sub group whence $mn=nm$.,,"['abstract-algebra', 'group-theory']"
80,Elegant way to show that N is a normal subgroup of G,Elegant way to show that N is a normal subgroup of G,,"Claim : Let $G$ be the set of all real $2 \times 2$ matrices $\left( \begin{array}{cc} a & b \\ 0 & d  \end{array} \right)$ such that $ad \not = 0$, with matrix multiplication as the operation. Let $N$ be the subset where  $a = d = 1$. Then $N$ is a normal subgroup of $G$. Showing that $N$ is a subgroup of $G$ is easy because $\left( \begin{array}{cc} 1 & b_1 \\ 0 & 1  \end{array} \right) \left( \begin{array}{cc} 1 & b_2 \\ 0 & 1  \end{array} \right) = \left( \begin{array}{cc} 1 & b_1 + b_2 \\ 0 & 1  \end{array} \right)$. However, I cannot think of a nice way to show that $N$ is a normal subgroup. It would be simple to do out all the computations, but also tedious. Is there a nice way to do this?","Claim : Let $G$ be the set of all real $2 \times 2$ matrices $\left( \begin{array}{cc} a & b \\ 0 & d  \end{array} \right)$ such that $ad \not = 0$, with matrix multiplication as the operation. Let $N$ be the subset where  $a = d = 1$. Then $N$ is a normal subgroup of $G$. Showing that $N$ is a subgroup of $G$ is easy because $\left( \begin{array}{cc} 1 & b_1 \\ 0 & 1  \end{array} \right) \left( \begin{array}{cc} 1 & b_2 \\ 0 & 1  \end{array} \right) = \left( \begin{array}{cc} 1 & b_1 + b_2 \\ 0 & 1  \end{array} \right)$. However, I cannot think of a nice way to show that $N$ is a normal subgroup. It would be simple to do out all the computations, but also tedious. Is there a nice way to do this?",,['abstract-algebra']
81,How do two conjugate elements of a group have the same order?,How do two conjugate elements of a group have the same order?,,"I'm reading group action in textbook Algebra by Saunders MacLane and Garrett Birkhoff. I have a problem of understanding the last sentence: Since conjugation is an automorphism, any two conjugate elements have the same order. Assume $x,y \in G$ are conjugate, then they are equivalent. As such, $gxg^{-1} = y$ for some $g \in G$ . This means $gx = yg$ . From here, I could not get how $x,y$ have the same order. Could you please elaborate on this point?","I'm reading group action in textbook Algebra by Saunders MacLane and Garrett Birkhoff. I have a problem of understanding the last sentence: Since conjugation is an automorphism, any two conjugate elements have the same order. Assume are conjugate, then they are equivalent. As such, for some . This means . From here, I could not get how have the same order. Could you please elaborate on this point?","x,y \in G gxg^{-1} = y g \in G gx = yg x,y","['abstract-algebra', 'group-theory']"
82,Are there fields and ordered fields of every infinite cardinality?,Are there fields and ordered fields of every infinite cardinality?,,"On the top of my head, I cannot think of any fields of cardinality more than that of the reals. (It is known that the process of algebraic closure does not increase the cardinality of an infinite field.) What is the simplest way to give an example of a field (and an ordered field) of a specific cardinality $\alpha$? I see there is the ""Field"" of surreal numbers, but it is a proper class rather than a set (and hence do not have a cardinality as such). However, there seems to be some modified construction which gives proper fields with the cardinality of some strongly inaccessible cardinal.","On the top of my head, I cannot think of any fields of cardinality more than that of the reals. (It is known that the process of algebraic closure does not increase the cardinality of an infinite field.) What is the simplest way to give an example of a field (and an ordered field) of a specific cardinality $\alpha$? I see there is the ""Field"" of surreal numbers, but it is a proper class rather than a set (and hence do not have a cardinality as such). However, there seems to be some modified construction which gives proper fields with the cardinality of some strongly inaccessible cardinal.",,"['abstract-algebra', 'field-theory', 'cardinals', 'ordered-fields']"
83,Roots of $x^2 + 2x + 2$,Roots of,x^2 + 2x + 2,I'm trying to show that there are infinitely many values of $p$ such that $x^2 + 2x + 2$ has no roots over $\mathbb{F}_p$.  Is this easily solvable?  (I kind of came up with it myself so I don't know.) Thanks!,I'm trying to show that there are infinitely many values of $p$ such that $x^2 + 2x + 2$ has no roots over $\mathbb{F}_p$.  Is this easily solvable?  (I kind of came up with it myself so I don't know.) Thanks!,,"['abstract-algebra', 'polynomials', 'finite-groups', 'finite-fields', 'quadratic-reciprocity']"
84,there is no injective group homomorphism from $\mathbb Z\times\mathbb Z$ into $\mathbb Z$,there is no injective group homomorphism from  into,\mathbb Z\times\mathbb Z \mathbb Z,there is no injective group homomorphism from $\mathbb Z\times\mathbb Z$ into $\mathbb Z$ But i don't know why it is true. should i investigate all group homomorphisms from $\mathbb Z\times\mathbb Z$ into $\mathbb Z$? Do you have any idea to help me? :(,there is no injective group homomorphism from $\mathbb Z\times\mathbb Z$ into $\mathbb Z$ But i don't know why it is true. should i investigate all group homomorphisms from $\mathbb Z\times\mathbb Z$ into $\mathbb Z$? Do you have any idea to help me? :(,,"['abstract-algebra', 'abelian-groups']"
85,Show that this ring has no identity.,Show that this ring has no identity.,,"Let $R=\left \{ g:\Bbb{R}\to \Bbb{R} \mid g \text{ is continuous and } g(1)=0 \right \}$ be a ring. Show that $R$ has no identity. The answer says there does not exist a function $h(x)\in R$ such that $h(x)=1$, which I don't understand why, since the only condition in $R$ is $g(1)=0$. Please help me understand what I am missing.","Let $R=\left \{ g:\Bbb{R}\to \Bbb{R} \mid g \text{ is continuous and } g(1)=0 \right \}$ be a ring. Show that $R$ has no identity. The answer says there does not exist a function $h(x)\in R$ such that $h(x)=1$, which I don't understand why, since the only condition in $R$ is $g(1)=0$. Please help me understand what I am missing.",,['abstract-algebra']
86,Proving specific unital rings have maximal ideals without Zorn's Lemma,Proving specific unital rings have maximal ideals without Zorn's Lemma,,"In Dummit & Foote's Abstract Algebra , they make the comment on the fact that unital rings have maximal ideals: $\dots$ the proof relies on Zorn's Lemma (see Appendix I). In many specific rings, however, the presence of maximal ideals is often obvious, independent of Zorn's Lemma. Are there specific rings that require use of Zorn's Lemma to show they have maximal ideals?","In Dummit & Foote's Abstract Algebra , they make the comment on the fact that unital rings have maximal ideals: $\dots$ the proof relies on Zorn's Lemma (see Appendix I). In many specific rings, however, the presence of maximal ideals is often obvious, independent of Zorn's Lemma. Are there specific rings that require use of Zorn's Lemma to show they have maximal ideals?",,"['abstract-algebra', 'ring-theory', 'axiom-of-choice']"
87,$\mathbb{Z}_n$ is not a subring of $\mathbb{Z}$,is not a subring of,\mathbb{Z}_n \mathbb{Z},"I come across an example stating that ' $\mathbb{Z}_n$ is not a subring of $\mathbb{Z},n \geq2, n \in \mathbb{Z}$' in the book ' Dummit and Foote , abstract algebra'. Can anyone explain to me why the statement is true?","I come across an example stating that ' $\mathbb{Z}_n$ is not a subring of $\mathbb{Z},n \geq2, n \in \mathbb{Z}$' in the book ' Dummit and Foote , abstract algebra'. Can anyone explain to me why the statement is true?",,['abstract-algebra']
88,Proving that an integral domain has at most two elements that satisfy the equation $x^2 = 1$.,Proving that an integral domain has at most two elements that satisfy the equation .,x^2 = 1,"I like to be thorough, but if you feel confident you can skip the first paragraph. Review: A ring is a set $R$ endowed with two operations of + and $\cdot$ such that $(G,+)$ is an additive abelian group, multiplication is associative, $R$ contains the multiplicative identity (denoted with 1), and the distributive law holds.  If multiplication is also commutative, we say $R$ is a commutative ring. A ring that has no zero divisors (non-zero elements whose product is zero) is called an integral domain, or just a domain. We want to show that for a domain, the equation $x^2 = 1$ has at most 2 solutions in $R$ (one of which is the trivial solution 1). Here's what I did: For simplicity let $1,a,b$ and $c$ be distinct non-zero elements in $R$.  Assume $a^2 = 1$.  We want to show that letting $b^2 = 1$ as well will lead to a contradiction.  So suppose $b^2 = 1$, then it follows that $a^2b^2 = (ab)^2 = 1$, so $ab$ is a solution as well, but is it a new solution?  If $ab = 1$, then $abb = 1b \Rightarrow a = b$ which is a contradiction.  If $ab = a$, then $aab = aa \Rightarrow b = 1$ which is also a contradiction.  Similarly, $ab = b$ won't work either.  So it must be that $ab = c$.  So by ""admitting"" $b$ as a solution, we're forced to admit $c$ as well. So far we have $a^2 = b^2 = c^2 = 1$ and $ab = c$.  We can procede as before as say that $(abc)^2 = 1$, so $abc$ is a solution, but once again we should check if it is a new solution.  From $ab = c$, we get $a = cb$ and $b = ac$, so $abc = (cb)(ac)(ab) = (abc)^2 = 1$.  So $abc$ is not a new solution; it's just one. At this point I'm stuck.  I've shown that it is in fact possible to have a ring with 4 distinct elements, namely $1,a,b$ and $c$ such that each satisfies the equation $x^2 = 1$ and $abc = 1$.  What am I missing?","I like to be thorough, but if you feel confident you can skip the first paragraph. Review: A ring is a set $R$ endowed with two operations of + and $\cdot$ such that $(G,+)$ is an additive abelian group, multiplication is associative, $R$ contains the multiplicative identity (denoted with 1), and the distributive law holds.  If multiplication is also commutative, we say $R$ is a commutative ring. A ring that has no zero divisors (non-zero elements whose product is zero) is called an integral domain, or just a domain. We want to show that for a domain, the equation $x^2 = 1$ has at most 2 solutions in $R$ (one of which is the trivial solution 1). Here's what I did: For simplicity let $1,a,b$ and $c$ be distinct non-zero elements in $R$.  Assume $a^2 = 1$.  We want to show that letting $b^2 = 1$ as well will lead to a contradiction.  So suppose $b^2 = 1$, then it follows that $a^2b^2 = (ab)^2 = 1$, so $ab$ is a solution as well, but is it a new solution?  If $ab = 1$, then $abb = 1b \Rightarrow a = b$ which is a contradiction.  If $ab = a$, then $aab = aa \Rightarrow b = 1$ which is also a contradiction.  Similarly, $ab = b$ won't work either.  So it must be that $ab = c$.  So by ""admitting"" $b$ as a solution, we're forced to admit $c$ as well. So far we have $a^2 = b^2 = c^2 = 1$ and $ab = c$.  We can procede as before as say that $(abc)^2 = 1$, so $abc$ is a solution, but once again we should check if it is a new solution.  From $ab = c$, we get $a = cb$ and $b = ac$, so $abc = (cb)(ac)(ab) = (abc)^2 = 1$.  So $abc$ is not a new solution; it's just one. At this point I'm stuck.  I've shown that it is in fact possible to have a ring with 4 distinct elements, namely $1,a,b$ and $c$ such that each satisfies the equation $x^2 = 1$ and $abc = 1$.  What am I missing?",,['abstract-algebra']
89,"In how many ways can the group $\mathbb Z_5$ act on the set $\{1,2,3,4,5\}$ $?$",In how many ways can the group  act on the set,"\mathbb Z_5 \{1,2,3,4,5\} ?","$\mathbb Z_{5}$ is the group to act on the set $\{ 1,2,3,4,5\}$ . In how many ways is that possible? Now $0$ will give the identity map. $1$ will give a  bijection in $5!$ ways so will the others and the number of possible bijections being $5!$ the bijections given by $1$ will coincide with those given by others. All get mixed up here.",is the group to act on the set . In how many ways is that possible? Now will give the identity map. will give a  bijection in ways so will the others and the number of possible bijections being the bijections given by will coincide with those given by others. All get mixed up here.,"\mathbb Z_{5} \{ 1,2,3,4,5\} 0 1 5! 5! 1","['abstract-algebra', 'combinatorics', 'group-theory', 'group-actions', 'cyclic-groups']"
90,Understanding the quotient ring $\mathbb{R}[x]/(x^3)$.,Understanding the quotient ring .,\mathbb{R}[x]/(x^3),"I am having difficulty in understanding exactly the elements of the set $\mathbb{R}[x]/(x^3)$. I'll explain my thought process. The Quotient Ring is the set of additive cosets, so we have that $$\mathbb{R}[x]/(x^3) = \{f+(x^3) : f\in\mathbb{R}[x]\}.$$ So we have the relation $$f-g\equiv 0 \mbox{ mod } x^3,$$ hence $x^3|f-g$. Now, right now I understand this as a whole bunch of notation taken strictly from the definition. But what exactly are the elements of this quotient ring?","I am having difficulty in understanding exactly the elements of the set $\mathbb{R}[x]/(x^3)$. I'll explain my thought process. The Quotient Ring is the set of additive cosets, so we have that $$\mathbb{R}[x]/(x^3) = \{f+(x^3) : f\in\mathbb{R}[x]\}.$$ So we have the relation $$f-g\equiv 0 \mbox{ mod } x^3,$$ hence $x^3|f-g$. Now, right now I understand this as a whole bunch of notation taken strictly from the definition. But what exactly are the elements of this quotient ring?",,"['abstract-algebra', 'ring-theory', 'ideals']"
91,Set-theoretical description of the free product?,Set-theoretical description of the free product?,,"There is something in the definition of the free product of two groups that annoys me, and it's this ""word"" thing: If $G$ and $H$ are groups, a word in $G$ and $H$ is a product of the form $$     s_1 s_2 \dots s_m, $$ where each $s_i$ is either an element of $G$ or an element of $H$ . So what is this ""word"" guy? Does it come out of the blue? Does it come from some sort of new operation that I can perform with the two sets $G$ and $H$ -in addition to the well-known ones of union, intersection, Cartesian product...? Fortunatelly, I think there is nothing new under the sun of set operations: it's easy to realise that words can be identified with elements of some Cartesian product (see below): $$ (s_1, s_2, \dots , s_m )  \ . $$ And Cartesian product is a well-established set-theoretical operation. So I tried to translate the rest of Wikipedia's definition Such a word may be reduced using the following operations: Remove an instance of the identity element (of either $G$ or $H$ ).   Replace a pair of the form $g_1g_2$ by its product in $G$ , or a pair $h_1h_2$ by its product in $H$ . Every reduced word is an alternating product of elements of $G$ and elements of $H$ , e.g. $$  g_1 h_1 g_2 h_2 \dots g_r h_r.  $$ The free product $G ∗ H$ is the group whose elements are the reduced words in $G$ and $H$ , under the operation of concatenation followed by reduction. in an elementary set setting. First, consider the set of ""unreduced"" tuples of elements of $G$ and $H$ $$ U =  G \sqcup H \sqcup (G\times G) \times (G\times H) \sqcup (H\times G) \sqcup (H\times H) \sqcup (G\times G \times G) \sqcup \dots  $$ More concisely: EDIT: I think the following formula may be less messier than the one I wrote previously: $$ U = \bigsqcup_{r \geq 1} (S_1 \times \cdots \times S_r), $$ where $S_i = G$ or $S_i = H$ . So, elements of $U$ are ordered tuples ( unreduced ones) $$ (s_1, s_2, \dots , s_m), $$ where each $s_i$ is either an element of $G$ or an element of $H$ . The product of two unreduced tuples is defined by concatenation $$ (s_1, \dots , s_m) \cdot (t_1,  \dots , t_n) = (s_1, \dots , s_m, t_1 , \dots , t_n) \ . $$ Now, consider the following equivalence relation in the set of unreduced tuples $U$ : $$ (s_1, s_2, \dots , s_{i-1}, 1, s_{i+1}, \dots , s_n) \sim (s_1, s_2, \dots, s_{i-1}, s_i, \dots , s_n) \ , $$ where $1$ is either the unit element of $G$ or the one of $H$ . And $$ (s_1, s_2, \dots , s_i,s_{i+1}, \dots , s_r) \sim (s_1, s_2, \dots , s_is_{i+1}, \dots , s_r ) $$ whenever two adjacent $s_i, s_{i+1} \in G$ or $s_i, s_{i+1} \in H$ at the same time. If you want, you may call the equivalence class of a tuple under this equivalence relation a reduced tuple. So every reduced tuple is an alternating one, $$ (g_1, h_1, \dots , g_r , h_r) \ , $$ with $g_i \in G$ and $h_i \in H$ for all $i = 1, \dots , r$ . Define the free product of $G$ and $H$ as the quotient: $$ G*H = U/\sim \ . $$ Finally, one verifies that concatenation is well-defined on unreduced tuples and gives $G*H$ a group structure. After performing this elementary exercise I understand perfectly well why nobody defines the free product in this way, but I still wanted to ask: Is this correct? Is it written somewhere?","There is something in the definition of the free product of two groups that annoys me, and it's this ""word"" thing: If and are groups, a word in and is a product of the form where each is either an element of or an element of . So what is this ""word"" guy? Does it come out of the blue? Does it come from some sort of new operation that I can perform with the two sets and -in addition to the well-known ones of union, intersection, Cartesian product...? Fortunatelly, I think there is nothing new under the sun of set operations: it's easy to realise that words can be identified with elements of some Cartesian product (see below): And Cartesian product is a well-established set-theoretical operation. So I tried to translate the rest of Wikipedia's definition Such a word may be reduced using the following operations: Remove an instance of the identity element (of either or ).   Replace a pair of the form by its product in , or a pair by its product in . Every reduced word is an alternating product of elements of and elements of , e.g. The free product is the group whose elements are the reduced words in and , under the operation of concatenation followed by reduction. in an elementary set setting. First, consider the set of ""unreduced"" tuples of elements of and More concisely: EDIT: I think the following formula may be less messier than the one I wrote previously: where or . So, elements of are ordered tuples ( unreduced ones) where each is either an element of or an element of . The product of two unreduced tuples is defined by concatenation Now, consider the following equivalence relation in the set of unreduced tuples : where is either the unit element of or the one of . And whenever two adjacent or at the same time. If you want, you may call the equivalence class of a tuple under this equivalence relation a reduced tuple. So every reduced tuple is an alternating one, with and for all . Define the free product of and as the quotient: Finally, one verifies that concatenation is well-defined on unreduced tuples and gives a group structure. After performing this elementary exercise I understand perfectly well why nobody defines the free product in this way, but I still wanted to ask: Is this correct? Is it written somewhere?","G H G H 
    s_1 s_2 \dots s_m,
 s_i G H G H 
(s_1, s_2, \dots , s_m )  \ .
 G H g_1g_2 G h_1h_2 H G H 
 g_1 h_1 g_2 h_2 \dots g_r h_r.
  G ∗ H G H G H 
U =  G \sqcup H \sqcup (G\times G) \times (G\times H) \sqcup (H\times G) \sqcup (H\times H) \sqcup (G\times G \times G) \sqcup \dots 
 
U = \bigsqcup_{r \geq 1} (S_1 \times \cdots \times S_r),
 S_i = G S_i = H U 
(s_1, s_2, \dots , s_m),
 s_i G H 
(s_1, \dots , s_m) \cdot (t_1,  \dots , t_n) = (s_1, \dots , s_m, t_1 , \dots , t_n) \ .
 U 
(s_1, s_2, \dots , s_{i-1}, 1, s_{i+1}, \dots , s_n) \sim (s_1, s_2, \dots, s_{i-1}, s_i, \dots , s_n) \ ,
 1 G H 
(s_1, s_2, \dots , s_i,s_{i+1}, \dots , s_r) \sim (s_1, s_2, \dots , s_is_{i+1}, \dots , s_r )
 s_i, s_{i+1} \in G s_i, s_{i+1} \in H 
(g_1, h_1, \dots , g_r , h_r) \ ,
 g_i \in G h_i \in H i = 1, \dots , r G H 
G*H = U/\sim \ .
 G*H",['abstract-algebra']
92,Cardinality of algebraic extensions of an infinite field.,Cardinality of algebraic extensions of an infinite field.,,"An exercise in Lang's algebra book is: let $k$ an infinite field, and $E$ an algebraic extension of $k$. Then $E$ has the same cardinality as $k$. How can one can prove this?","An exercise in Lang's algebra book is: let $k$ an infinite field, and $E$ an algebraic extension of $k$. Then $E$ has the same cardinality as $k$. How can one can prove this?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'cardinals']"
93,Can an element in a ring with unity be both a unit and zero-divisor? [closed],Can an element in a ring with unity be both a unit and zero-divisor? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I know this cannot be true for $\Bbb Z_n$, but is this also true in general rings? If so, how can you prove this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I know this cannot be true for $\Bbb Z_n$, but is this also true in general rings? If so, how can you prove this?",,"['abstract-algebra', 'ring-theory']"
94,Number of irreducible quadratic polynomials over a finite field [duplicate],Number of irreducible quadratic polynomials over a finite field [duplicate],,This question already has answers here : Number of monic irreducible polynomials of prime degree $p$ over finite fields (2 answers) Closed 8 years ago . To  find  the  number  of  irreducible  polynomials of  the  form $x^{2} + ax+b$ over  the  field $\Bbb{F}_{7}$ I  manually  checked  all  the  possibilities  and  thus  found  the  answer  to  be $21.$ Like $b=0$ is  not  possible  or  taking $b=1$ and  checking  what  values  of $a$ works  here  etc. Surely it  was   tedious  and  time-consuming  process  and  I  don't  think  it  was  appropriate  either. For  what  should  be  done for  a much  larger  field $?$ Are  there  some  basic  theories  that  I  should  use. Hints  please.,This question already has answers here : Number of monic irreducible polynomials of prime degree $p$ over finite fields (2 answers) Closed 8 years ago . To  find  the  number  of  irreducible  polynomials of  the  form over  the  field I  manually  checked  all  the  possibilities  and  thus  found  the  answer  to  be Like is  not  possible  or  taking and  checking  what  values  of works  here  etc. Surely it  was   tedious  and  time-consuming  process  and  I  don't  think  it  was  appropriate  either. For  what  should  be  done for  a much  larger  field Are  there  some  basic  theories  that  I  should  use. Hints  please.,x^{2} + ax+b \Bbb{F}_{7} 21. b=0 b=1 a ?,"['abstract-algebra', 'combinatorics', 'finite-fields', 'quadratics', 'irreducible-polynomials']"
95,Irrationality of $\sqrt[n]2$ [duplicate],Irrationality of  [duplicate],\sqrt[n]2,"This question already has answers here : Is $n^{th}$ root of $2$ an irrational number? [duplicate] (2 answers) Closed 8 years ago . I know how to prove the result for $n=2$ by contradiction, but does anyone know a proof for general integers $n$ ? Thank you for your answers. Marcus","This question already has answers here : Is $n^{th}$ root of $2$ an irrational number? [duplicate] (2 answers) Closed 8 years ago . I know how to prove the result for $n=2$ by contradiction, but does anyone know a proof for general integers $n$ ? Thank you for your answers. Marcus",,"['abstract-algebra', 'elementary-number-theory', 'radicals', 'rationality-testing']"
96,Can $R \times R$ be isomorphic to $R$ as rings?,Can  be isomorphic to  as rings?,R \times R R,"I know from this question that $R \times R$ can be isomorphic to $R$, as $R$-modules. But can they ever be isomorphic as rings?","I know from this question that $R \times R$ can be isomorphic to $R$, as $R$-modules. But can they ever be isomorphic as rings?",,"['abstract-algebra', 'ring-theory']"
97,"If $G$ is abelian, then the set of all $g \in G$ such that $g = g^{-1}$ is a subgroup of $G$","If  is abelian, then the set of all  such that  is a subgroup of",G g \in G g = g^{-1} G,"Prove that if $G$ is abelian then the set $H$ of all elements of $G$ that are their own inverses is a subgroup of $G$. Naturally in an abelian group, $ab = ba$ for $a, b \in G$, however I'm not sure how to show the set elements that are their own inverses is a subgroup of $G$ using arbitrary elements.","Prove that if $G$ is abelian then the set $H$ of all elements of $G$ that are their own inverses is a subgroup of $G$. Naturally in an abelian group, $ab = ba$ for $a, b \in G$, however I'm not sure how to show the set elements that are their own inverses is a subgroup of $G$ using arbitrary elements.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
98,Question about proving $\mathbb{Z}[i]$ is not UFD,Question about proving  is not UFD,\mathbb{Z}[i],"I know is a silly question, but why $10=\left(3+i\right)\left(3-i\right)=2\cdot5$ is not enough to prove that $\mathbb{Z}[i]$ is not a UFD. Thanks in advance!","I know is a silly question, but why is not enough to prove that is not a UFD. Thanks in advance!",10=\left(3+i\right)\left(3-i\right)=2\cdot5 \mathbb{Z}[i],"['abstract-algebra', 'ring-theory', 'unique-factorization-domains']"
99,$\mathbb Z_{p^n}$ is not the direct product of any family of its proper subgroups,is not the direct product of any family of its proper subgroups,\mathbb Z_{p^n},"I'm trying to solve this question of Hungerford's Algebra book: $S_3$ is not the direct product of any family of its proper subgroups.   The same is true of $\mathbb Z_{p^n}$ . The first claim is easy: we note that every subgroup of $S_3$ is of order $2$ or $3$ by Lagrange, since its subgroups are of prime order, they are cyclic, then abelian, but $S_3$ is not abelian, contradiction because $S_3$ is not abelian while the direct product of abelian groups has to be abelian. My problem is with the second claim, I need help.","I'm trying to solve this question of Hungerford's Algebra book: is not the direct product of any family of its proper subgroups.   The same is true of . The first claim is easy: we note that every subgroup of is of order or by Lagrange, since its subgroups are of prime order, they are cyclic, then abelian, but is not abelian, contradiction because is not abelian while the direct product of abelian groups has to be abelian. My problem is with the second claim, I need help.",S_3 \mathbb Z_{p^n} S_3 2 3 S_3 S_3,"['abstract-algebra', 'group-theory']"
