,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Curious convergence domain for $x_n$ defined by $(a_1 n+ b_1) x_{n+2} = (a_2 n + b_2) x_{n+1} - (a_3 n + b_3) x_n$,Curious convergence domain for  defined by,x_n (a_1 n+ b_1) x_{n+2} = (a_2 n + b_2) x_{n+1} - (a_3 n + b_3) x_n,"What are the conditions on $a_1, b_1, a_2, b_2, a_3, b_3$ and on the initial values $x_1, x_2$ for $x_n$ to converge to a value different from zero? We can assume that $a_1=1$ . Also we need $a_1 = a_2 - a_3$ and $b_1 = b_2 - b_3$ , but this is not enough to guarantee convergence. We can even assume that $x_1=0, x_2 = 1$ , see detailed discussion on these recurrences in my previous question, here . In short, there is only 3 free parameters, characterizing all the recurrences that do converge. As a starting point, see the chart below for $$2(n+2)x_{n+2}=(r (n+2) + s)x_{n+1} + ((2-r)(n+2)-s)x_n$$ with $x_0=0, x_1=1$ . This 2-dimensional plot represents one slice of all the possible 3-dimensional parameter representations leading to divergence. The X-axis represents $r$ , the Y-axis represents $s$ . I tried with 400,000 values of $(r, s)$ to check which ones result in actual convergence. The blue dots represent the points $(r,s)$ such that $|x_{40} - x_{39}|<0.000001$ . The blue area is full of holes because I only used 400,000 values of $(r, s)$ in this experiment. If you use 10,000,000 values, the boundaries will be smoother, and the holes in the blue area will vanish. These recurrences can be represented by generalized hypergeometric functions , and according to Wikipedia, the convergence status is typically studied separately for each recurrence. Yet my chart suggests that there is a general law governing the convergence (or lack of) for these recurrences. It is an interesting math problem, and also of interest to statisticians who are interested in estimating the boundaries (in the parameter space) of the convergence region, and test whether the boundaries are parallel lines. More cases Here we look at the general case, which can be written as: $$2(n+q)x_{n+2}=(r (n+q) + s)x_{n+1} + ((2-r)(n+q)-s)x_n$$ The charts below give some insights about the shape of the convergence regions / boundaries in the full 3-dimensional parameter space. Case $q=1$ : Case $q=5$ : Time permitting, I will try to create a 3-D picture of the boundary, maybe a rotating one so that it can be viewed under different angles.","What are the conditions on and on the initial values for to converge to a value different from zero? We can assume that . Also we need and , but this is not enough to guarantee convergence. We can even assume that , see detailed discussion on these recurrences in my previous question, here . In short, there is only 3 free parameters, characterizing all the recurrences that do converge. As a starting point, see the chart below for with . This 2-dimensional plot represents one slice of all the possible 3-dimensional parameter representations leading to divergence. The X-axis represents , the Y-axis represents . I tried with 400,000 values of to check which ones result in actual convergence. The blue dots represent the points such that . The blue area is full of holes because I only used 400,000 values of in this experiment. If you use 10,000,000 values, the boundaries will be smoother, and the holes in the blue area will vanish. These recurrences can be represented by generalized hypergeometric functions , and according to Wikipedia, the convergence status is typically studied separately for each recurrence. Yet my chart suggests that there is a general law governing the convergence (or lack of) for these recurrences. It is an interesting math problem, and also of interest to statisticians who are interested in estimating the boundaries (in the parameter space) of the convergence region, and test whether the boundaries are parallel lines. More cases Here we look at the general case, which can be written as: The charts below give some insights about the shape of the convergence regions / boundaries in the full 3-dimensional parameter space. Case : Case : Time permitting, I will try to create a 3-D picture of the boundary, maybe a rotating one so that it can be viewed under different angles.","a_1, b_1, a_2, b_2, a_3, b_3 x_1, x_2 x_n a_1=1 a_1 = a_2 - a_3 b_1 = b_2 - b_3 x_1=0, x_2 = 1 2(n+2)x_{n+2}=(r (n+2) + s)x_{n+1} + ((2-r)(n+2)-s)x_n x_0=0, x_1=1 r s (r, s) (r,s) |x_{40} - x_{39}|<0.000001 (r, s) 2(n+q)x_{n+2}=(r (n+q) + s)x_{n+1} + ((2-r)(n+q)-s)x_n q=1 q=5","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'recurrence-relations']"
1,Relation between $\lim_{x ‎\to ‎\infty} ‎\frac{f f''}{(f')^2}$ and $‎‏‌‎\lim_{x ‎\to ‎\infty} ‎\frac{f' f'''}{(f'')^2}$.,Relation between  and .,\lim_{x ‎\to ‎\infty} ‎\frac{f f''}{(f')^2} ‎‏‌‎\lim_{x ‎\to ‎\infty} ‎\frac{f' f'''}{(f'')^2},"Let assume ‎ $‎f ‎\in ‎C^3((0,\infty))‎$ ‎‎‏ and ‎ $‎‎‎f,f',f'' >0‎$ . If $$ ‎‎‏‌‎\lim_{x ‎\to ‎\infty} ‎‎\dfrac{f' f'''}{(f'')^2} =‎ c‎ \neq ‎1‎ $$ ‏show that $$ \lim_{x ‎\to ‎\infty} ‎‎\dfrac{f f''}{(f')^2} = ‎\dfrac{1}{2-c}. $$ My attempt: By Taylor Theorem we know that: $$f(x+h)=f(x)+f'(x)h+f''(x) \frac{h^2}{2} + f'''(\xi) \frac{h^3}{6}, \qquad \text{for} \,\,\, \text{some} \qquad \xi \in (x,x+h).$$ Multiplying both sides in $f'(x)$ and dividing by $(f'(x))^2$ we obtain: $$ \frac{f(x)f(x+h)}{(f'(x))^2}= \Big(\frac{f(x)}{f'(x)} \Big)^2+\frac{f(x)}{f'(x)}h+\frac{f(x)f''(x)}{(f'(x))^2} \frac{h^2}{2} + \frac{f(x) f'''(\xi)}{(f'(x))^2} \frac{h^3}{6}.$$ Also, multiplying both sides in $f'(x)$ and dividing by $(f''(x))^2$ we obtain: $$ \frac{f'(x)f(x+h)}{(f''(x))^2}= \frac{f'(x)f(x)}{(f''(x))^2} + \Big(\frac{f'(x)}{f''(x)} \Big)^2 h +\frac{f'(x)}{f''(x)} \frac{h^2}{2} + \frac{f'(x) f'''(\xi)}{(f''(x))^2} \frac{h^3}{6}.$$ I don't know how to continue.","Let assume ‎ ‎‎‏ and ‎ . If ‏show that My attempt: By Taylor Theorem we know that: Multiplying both sides in and dividing by we obtain: Also, multiplying both sides in and dividing by we obtain: I don't know how to continue.","‎f ‎\in ‎C^3((0,\infty))‎ ‎‎‎f,f',f'' >0‎  ‎‎‏‌‎\lim_{x ‎\to ‎\infty} ‎‎\dfrac{f' f'''}{(f'')^2} =‎ c‎ \neq ‎1‎   \lim_{x ‎\to ‎\infty} ‎‎\dfrac{f f''}{(f')^2} = ‎\dfrac{1}{2-c}.  f(x+h)=f(x)+f'(x)h+f''(x) \frac{h^2}{2} + f'''(\xi) \frac{h^3}{6}, \qquad \text{for} \,\,\, \text{some} \qquad \xi \in (x,x+h). f'(x) (f'(x))^2  \frac{f(x)f(x+h)}{(f'(x))^2}= \Big(\frac{f(x)}{f'(x)} \Big)^2+\frac{f(x)}{f'(x)}h+\frac{f(x)f''(x)}{(f'(x))^2} \frac{h^2}{2} + \frac{f(x) f'''(\xi)}{(f'(x))^2} \frac{h^3}{6}. f'(x) (f''(x))^2  \frac{f'(x)f(x+h)}{(f''(x))^2}= \frac{f'(x)f(x)}{(f''(x))^2} + \Big(\frac{f'(x)}{f''(x)} \Big)^2 h +\frac{f'(x)}{f''(x)} \frac{h^2}{2} + \frac{f'(x) f'''(\xi)}{(f''(x))^2} \frac{h^3}{6}.","['real-analysis', 'calculus', 'limits', 'analysis']"
2,"If $f$ is a bounded non-decreasing function, then it converges as $x \to \infty$ and as $x \to -\infty$. (Obvious?)","If  is a bounded non-decreasing function, then it converges as  and as . (Obvious?)",f x \to \infty x \to -\infty,"Here's a dumb one. This is a result that seems intuitively true, but I cannot find an actual statement of it anywhere, which leads me to believe there is some weird counterexample out there somewhere. Remark. I am aware of the monotone convergence theorem for sequences , but I do not see a result anywhere for functions. Rather than try to modify the proof of the MCT for sequences, I'm just going to try to prove the result directly. Proposition. Let $f: \mathbb{R} \to \mathbb{R}$ be a bounded, non-decreasing function. Then $$\lim_{x\to -\infty}f(x) \qquad \text{ and } \qquad \lim_{x \to \infty}f(x) $$ exist. Proof of Proposition. Let $\alpha = \inf\{f(\mathbb{R})\}$ and $\beta = \sup\{f(\mathbb{R})\}$ . Since $f$ is bounded, both of these extrema exist and, clearly, $f(\mathbb{R}) \subset [\alpha, \beta]$ . Since $f$ is non-decreasing, $$ x \le y \implies \alpha \le f(x) \le f(y) \le \beta, $$ for all $x, y \in \mathbb{R}$ (so $f$ cannot exhibit any ""oscillating"" behavior""). So assume WLOG that $\beta$ is positive. Then, given $\epsilon > 0$ , there is a $b \in \mathbb{R}$ such that $f(x) > \beta - \epsilon$ whenever $x \ge b$ . Therefore $$ \lim_{x \to \infty}f(x) = \beta. $$ A similar proof shows that $f(x) \to \alpha$ as $x$ tends to $-\infty$ . Eh?","Here's a dumb one. This is a result that seems intuitively true, but I cannot find an actual statement of it anywhere, which leads me to believe there is some weird counterexample out there somewhere. Remark. I am aware of the monotone convergence theorem for sequences , but I do not see a result anywhere for functions. Rather than try to modify the proof of the MCT for sequences, I'm just going to try to prove the result directly. Proposition. Let be a bounded, non-decreasing function. Then exist. Proof of Proposition. Let and . Since is bounded, both of these extrema exist and, clearly, . Since is non-decreasing, for all (so cannot exhibit any ""oscillating"" behavior""). So assume WLOG that is positive. Then, given , there is a such that whenever . Therefore A similar proof shows that as tends to . Eh?","f: \mathbb{R} \to \mathbb{R} \lim_{x\to -\infty}f(x) \qquad \text{ and } \qquad \lim_{x \to \infty}f(x)  \alpha = \inf\{f(\mathbb{R})\} \beta = \sup\{f(\mathbb{R})\} f f(\mathbb{R}) \subset [\alpha, \beta] f  x \le y \implies \alpha \le f(x) \le f(y) \le \beta,  x, y \in \mathbb{R} f \beta \epsilon > 0 b \in \mathbb{R} f(x) > \beta - \epsilon x \ge b  \lim_{x \to \infty}f(x) = \beta.  f(x) \to \alpha x -\infty","['real-analysis', 'calculus', 'limits']"
3,"Limit of integral, show limit exists and compute it","Limit of integral, show limit exists and compute it",,"Suppose $f : [0,\infty) \rightarrow \mathbb{R}$ is continuous and such that $|f(x)| \leq 1 + x^{2}, \forall x \geq 0$ . Then how do I show that: $L = \lim_{n \rightarrow \infty} \int_{0}^{n} \frac{f(x/n)}{(1+x)^{4}} dx$ exists and how do I compute it? If I give a bound for it, I obtain that $|L| \leq \lim \int_{0}^{n} \frac{1+(x/n)^{2}}{(1+x)^{4}} dx = \lim_{n \rightarrow \infty} \frac{1}{3} - \frac{1}{3(n+1)^{3}} + \frac{-3n^{2} - 3n - 1 + (n+1)^{3} }{3n^{2}(n+1)^{3}}$ . Taking the limit as $n$ goes to infinity gives that $|L| \leq \frac{1}{3}$ . However, this bound does not hold for all $f(x)$ , as $f(x) = 0$ gives $L = 0$ . I am assuming that the limit depends on $f$ , but I don't know how to proceed.","Suppose is continuous and such that . Then how do I show that: exists and how do I compute it? If I give a bound for it, I obtain that . Taking the limit as goes to infinity gives that . However, this bound does not hold for all , as gives . I am assuming that the limit depends on , but I don't know how to proceed.","f : [0,\infty) \rightarrow \mathbb{R} |f(x)| \leq 1 + x^{2}, \forall x \geq 0 L = \lim_{n \rightarrow \infty} \int_{0}^{n} \frac{f(x/n)}{(1+x)^{4}} dx |L| \leq \lim \int_{0}^{n} \frac{1+(x/n)^{2}}{(1+x)^{4}} dx = \lim_{n \rightarrow \infty} \frac{1}{3} - \frac{1}{3(n+1)^{3}} + \frac{-3n^{2} - 3n - 1 + (n+1)^{3} }{3n^{2}(n+1)^{3}} n |L| \leq \frac{1}{3} f(x) f(x) = 0 L = 0 f","['real-analysis', 'integration', 'sequences-and-series', 'limits']"
4,Limit of $\frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1}$ as $n\to\infty$,Limit of  as,\frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1} n\to\infty,I've tried to solve the limit $$     \lim_{n \to \infty} \frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1}$$ but I'm not sure. $$ \frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1} = \frac { 2^{\sqrt{ (\ln n)^2+ 2\ln n}}}{n^2+1} =  \frac { 2^{\ln n \sqrt{ 1+ \frac {2}{\ln n}}}}{n^2+1} \sim  \frac { 2^{\ln n }}{n^2+1}   \rightarrow 0$$ Is it right? I have another exercize that ends similarly with $$ \frac { 10^{\ln n }}{n^2+1}   \rightarrow 0$$ But the book says that the result is $+\infty$ .,I've tried to solve the limit but I'm not sure. Is it right? I have another exercize that ends similarly with But the book says that the result is .,     \lim_{n \to \infty} \frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1}  \frac { 2^{\sqrt{ (\ln n)^2+ \ln n^2}}}{n^2+1} = \frac { 2^{\sqrt{ (\ln n)^2+ 2\ln n}}}{n^2+1} =  \frac { 2^{\ln n \sqrt{ 1+ \frac {2}{\ln n}}}}{n^2+1} \sim  \frac { 2^{\ln n }}{n^2+1}   \rightarrow 0  \frac { 10^{\ln n }}{n^2+1}   \rightarrow 0 +\infty,"['sequences-and-series', 'limits']"
5,Help with multivariable limit and differentiability.,Help with multivariable limit and differentiability.,,"I have to determine differentiability at $(0,1)$ of the following function: $$f(x,y)=\frac{|x| y \sin(\frac{\pi x}{2})}{x^2+y^2}$$ The partial derivatives both have value $0$ at $(0,1),$ and both are continuous on that point (I think I've got this part right), so the function must be differentiable at $(0,1).$ But when I checked for differentiability using the definition, the limit that should be $0$ doesn't exist, so I assume I'm doing something wrong when computing the limit. The following limit has to be $0$ if the function is differentiable at that point $$\lim_{x,y\to(0,1)} \frac{|f(x,y)|}{\|(x,y)-(0,1)\|}$$ Doing the change $w=y-1$ we have: $$\lim_{x,y\to(0,0)} \frac{|x (w+1)\sin(\frac{\pi x}{2})|}{(x^2+(w+1)^2)\sqrt{x^2+w^2}}$$ and then computing the limit along the line $x=w,$ it has the value $\pi /2\sqrt{2}$ , which contradicts that the limit is $0.$ What am I doing wrong?","I have to determine differentiability at of the following function: The partial derivatives both have value at and both are continuous on that point (I think I've got this part right), so the function must be differentiable at But when I checked for differentiability using the definition, the limit that should be doesn't exist, so I assume I'm doing something wrong when computing the limit. The following limit has to be if the function is differentiable at that point Doing the change we have: and then computing the limit along the line it has the value , which contradicts that the limit is What am I doing wrong?","(0,1) f(x,y)=\frac{|x| y \sin(\frac{\pi x}{2})}{x^2+y^2} 0 (0,1), (0,1). 0 0 \lim_{x,y\to(0,1)} \frac{|f(x,y)|}{\|(x,y)-(0,1)\|} w=y-1 \lim_{x,y\to(0,0)} \frac{|x (w+1)\sin(\frac{\pi x}{2})|}{(x^2+(w+1)^2)\sqrt{x^2+w^2}} x=w, \pi /2\sqrt{2} 0.","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'derivatives']"
6,"Equivalent convergent conditions in $C([0,1])$",Equivalent convergent conditions in,"C([0,1])","I am doing problems in my instructor's notes. I am stuck at the following problem. ""Let $f_n\in C([0,1])$ for $n=1,2,\cdots$ . Show that the   following two statements are equivalent: 1) For every $\lambda\in C([0,1])^*$ we have $\lambda(f_n)\to       0$ as $n\to\infty$ , 2) $f_n(x)\to 0$ for every $x\in [0,1]$ and $\sup         ||f_n||_\infty<\infty$ ."" $(1) \implies (2):$ It turns out I did this part wrong. I used Uniform Boundedness Principle for $f_n$ , which is not linear. $(2) \implies (1):$ I am stuck at this one. I want to prove that $f_n\to 0$ and the rest would be easy. we have $$ lim_{n\to \infty} ||f_n|| = lim_{n\to \infty} \sup_{x\in [0,1]} \{|f_n(x)| \} $$ here I wish I can interchange lim and sup and we are done. Is this approach true and how do we do it? Thanks in advance","I am doing problems in my instructor's notes. I am stuck at the following problem. ""Let for . Show that the   following two statements are equivalent: 1) For every we have as , 2) for every and ."" It turns out I did this part wrong. I used Uniform Boundedness Principle for , which is not linear. I am stuck at this one. I want to prove that and the rest would be easy. we have here I wish I can interchange lim and sup and we are done. Is this approach true and how do we do it? Thanks in advance","f_n\in C([0,1]) n=1,2,\cdots \lambda\in C([0,1])^* \lambda(f_n)\to
      0 n\to\infty f_n(x)\to 0 x\in [0,1] \sup
        ||f_n||_\infty<\infty (1) \implies (2): f_n (2) \implies (1): f_n\to 0 
lim_{n\to \infty} ||f_n|| = lim_{n\to \infty} \sup_{x\in [0,1]} \{|f_n(x)| \}
","['functional-analysis', 'limits', 'convergence-divergence', 'banach-spaces']"
7,Statement with limits of sequences,Statement with limits of sequences,,"I have some statements with limits. We can have arbirary $a(n)$ . *First $\implies$ if limit of $n*a(n)$ is zero, then $a(n)$ must be zero, because if we multiply something with no-zero value and it is still zero, then it must be zero. If limit of $a(n)$ is zero, then $a(n)$ is limited. $\impliedby$ counter: $a(n)=1/n$ is limited, but $n*(1/n)$ is one, so it is not correct *Second $\implies$ counter: if we have limit of $((n)^{1/2})/n$ , it is zero, but $(n)^{1/2}$ is not limited, so it is not correct $\impliedby$ I am not quite sure about this one, dont know it is correct for all sequences, especially sin and so on, can you please explain? *Third Both are true, because if limit of $a(n)$ was $-1$ , multiplied by $3$ , it would have been $-1$ , but it is $1$ , so a(n) must be one Thank you for your feedback and please give me hand especially with that second one "" $\impliedby$ "".","I have some statements with limits. We can have arbirary . *First if limit of is zero, then must be zero, because if we multiply something with no-zero value and it is still zero, then it must be zero. If limit of is zero, then is limited. counter: is limited, but is one, so it is not correct *Second counter: if we have limit of , it is zero, but is not limited, so it is not correct I am not quite sure about this one, dont know it is correct for all sequences, especially sin and so on, can you please explain? *Third Both are true, because if limit of was , multiplied by , it would have been , but it is , so a(n) must be one Thank you for your feedback and please give me hand especially with that second one "" "".",a(n) \implies n*a(n) a(n) a(n) a(n) \impliedby a(n)=1/n n*(1/n) \implies ((n)^{1/2})/n (n)^{1/2} \impliedby a(n) -1 3 -1 1 \impliedby,"['sequences-and-series', 'limits']"
8,Derivation of information entropy using Stirling's approximation,Derivation of information entropy using Stirling's approximation,,"I'm currently studying the textbook Pattern Recognition and Machine Learning (Bishop, 2006) and am studying the information entropy in chapter 1. In the book, the author derives the definition of information entropy for discrete random variable cases by giving the analogy of ""considering a set of $N$ objects that are to be divided amongst a set of bins, such that there are $n_i$ objects in the $i$ th bin. The total number of ways to allocate the $N$ objects is: $$W = \frac{N!}{\prod_i n_i !}$$ and the entropy is based on the logarithm of this scaled by a constant: $$H = \frac{1}{N} \ln (W) $$ The author then uses Stirling's approximation to make a derivation, but I'm getting stuck midway from how the author derived the final result. Here's my derivation: $$ \begin{align} H & = \frac{1}{N} \ln (W) \\ & = \frac{1}{N} \ln \left( \frac{N!}{\prod_i n_i !}\right) \\ & = \frac{1}{N} \left( \ln(N!) - \ln \left( \prod_i n_i! \right) \right) \\ & = \frac{1}{N} \left( \ln(N!) - \sum_i \ln (n_i!) \right) \\ & = \frac{1}{N} \left( (N \ln(N) - N) - \left( \sum_i (n_i \ln (n_i) - n_i )\right) \right) \\ & = \left( \ln(N) - 1 \right) - \frac{1}{N} \left( \sum_i n_i\ln(n_i) - \sum_in_i \right) \\ & = (\ln(N) - 1) -\frac{1}{N} \sum_i n_i \ln(n_i)  + 1 \\ & = \ln(N) - \frac{1}{N} \sum_i n_i \ln(n_i) \end{align} $$ The author's final result states that: $$ \begin{align} H & = -\lim_{N \rightarrow \infty} \sum_i \left( \frac{n_i}{N} \right) \ln \left( \frac{n_i}{N} \right) \\ & = -\sum_i p_i \ln(p_i) \end{align} $$ I'm a bit confused as to how the author jumped from the derivation to suddenly using a limit, and also how using the limit on those values gives the values of $p_i$ . My initial thought was that $p_i = \frac{n_i}{N}$ and so I'm not sure what purpose the limit serves here. Would anybody be kind enough to give me some tips or pointers as to how this result was derived? Thanks in advance.","I'm currently studying the textbook Pattern Recognition and Machine Learning (Bishop, 2006) and am studying the information entropy in chapter 1. In the book, the author derives the definition of information entropy for discrete random variable cases by giving the analogy of ""considering a set of objects that are to be divided amongst a set of bins, such that there are objects in the th bin. The total number of ways to allocate the objects is: and the entropy is based on the logarithm of this scaled by a constant: The author then uses Stirling's approximation to make a derivation, but I'm getting stuck midway from how the author derived the final result. Here's my derivation: The author's final result states that: I'm a bit confused as to how the author jumped from the derivation to suddenly using a limit, and also how using the limit on those values gives the values of . My initial thought was that and so I'm not sure what purpose the limit serves here. Would anybody be kind enough to give me some tips or pointers as to how this result was derived? Thanks in advance.","N n_i i N W = \frac{N!}{\prod_i n_i !} H = \frac{1}{N} \ln (W)  
\begin{align}
H & = \frac{1}{N} \ln (W) \\
& = \frac{1}{N} \ln \left( \frac{N!}{\prod_i n_i !}\right) \\
& = \frac{1}{N} \left( \ln(N!) - \ln \left( \prod_i n_i! \right) \right) \\
& = \frac{1}{N} \left( \ln(N!) - \sum_i \ln (n_i!) \right) \\
& = \frac{1}{N} \left( (N \ln(N) - N) - \left( \sum_i (n_i \ln (n_i) - n_i )\right) \right) \\
& = \left( \ln(N) - 1 \right) - \frac{1}{N} \left( \sum_i n_i\ln(n_i) - \sum_in_i \right) \\
& = (\ln(N) - 1) -\frac{1}{N} \sum_i n_i \ln(n_i)  + 1 \\
& = \ln(N) - \frac{1}{N} \sum_i n_i \ln(n_i)
\end{align}
 
\begin{align}
H & = -\lim_{N \rightarrow \infty} \sum_i \left( \frac{n_i}{N} \right) \ln \left( \frac{n_i}{N} \right) \\
& = -\sum_i p_i \ln(p_i)
\end{align}
 p_i p_i = \frac{n_i}{N}","['limits', 'information-theory']"
9,Find the infimum and supremum of $\{x\in Q \mid x² < 9\}$,Find the infimum and supremum of,\{x\in Q \mid x² < 9\},"I have to find (and prove) the infimum and supremum of the following set: $M_1:=\{x\in\mathbb{Q} \mid x^2 < 9\}$ On first glance, I would say: $\inf M_1=-3 $ $\sup M_1=3$ Now I have to prove that these really are the infimum and supremum of the set, and that's the point where I'm having problems. According to the definition of $\inf$ and $\sup$ , this means, that $-3$ is the biggest lower bound and 3 is the lowest upper bound: $\forall x\in\mathbb(M_1): -3 \leq x \leq 3$ We can see, that -3 and 3 are not elements of M1, which means: $\forall x\in\mathbb(M_1):-3<x<3$ But how can I show that -3 and 3 are the $\textbf{biggest / smallest}$ bound? I mean, for example, what if there is a number bigger than -3 that acts like a lower bound to the set? Obviously there isn't a bigger lower bound, but how can I mathematically show it? Do you guys have any advice? Thanks in advance, and sorry for my English :D","I have to find (and prove) the infimum and supremum of the following set: On first glance, I would say: Now I have to prove that these really are the infimum and supremum of the set, and that's the point where I'm having problems. According to the definition of and , this means, that is the biggest lower bound and 3 is the lowest upper bound: We can see, that -3 and 3 are not elements of M1, which means: But how can I show that -3 and 3 are the bound? I mean, for example, what if there is a number bigger than -3 that acts like a lower bound to the set? Obviously there isn't a bigger lower bound, but how can I mathematically show it? Do you guys have any advice? Thanks in advance, and sorry for my English :D",M_1:=\{x\in\mathbb{Q} \mid x^2 < 9\} \inf M_1=-3  \sup M_1=3 \inf \sup -3 \forall x\in\mathbb(M_1): -3 \leq x \leq 3 \forall x\in\mathbb(M_1):-3<x<3 \textbf{biggest / smallest},"['calculus', 'limits', 'analysis', 'supremum-and-infimum']"
10,Evaluating $\lim_{n\to \infty}\frac{n!}{n^n}\left(\sum_{k=0}^{n}\frac{n^k}{k!}-\sum_{k=n+1}^{\infty}\frac{n^k}{k!}\right)$ [closed],Evaluating  [closed],\lim_{n\to \infty}\frac{n!}{n^n}\left(\sum_{k=0}^{n}\frac{n^k}{k!}-\sum_{k=n+1}^{\infty}\frac{n^k}{k!}\right),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How do you evaluate the following limit: $$\lim_{n\to \infty}\frac{n!}{n^n}\left(\sum_{k=0}^{n}\frac{n^k}{k!}-\sum_{k=n+1}^{\infty}\frac{n^k}{k!}\right)$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How do you evaluate the following limit:",\lim_{n\to \infty}\frac{n!}{n^n}\left(\sum_{k=0}^{n}\frac{n^k}{k!}-\sum_{k=n+1}^{\infty}\frac{n^k}{k!}\right),"['limits', 'infinity']"
11,Condition for $ \text{rank}(A) = \text{rank}(A^2) $,Condition for, \text{rank}(A) = \text{rank}(A^2) ,"Let $ A $ be an $ n \times n $ complex matrix. Prove that $ \text{rank}(A) = \text{rank}(A^2) $ if and only if $ \lim_{\lambda \to 0} (A + \lambda I)^{-1}A $ exists. I'm not exactly sure how to approach this sort of question. I've thought about putting $ A $ in Jordan form and working with Jordan blocks, but it seems messy.","Let be an complex matrix. Prove that if and only if exists. I'm not exactly sure how to approach this sort of question. I've thought about putting in Jordan form and working with Jordan blocks, but it seems messy.", A   n \times n   \text{rank}(A) = \text{rank}(A^2)   \lim_{\lambda \to 0} (A + \lambda I)^{-1}A   A ,"['linear-algebra', 'limits']"
12,What happens to $f(x) = \left\lceil \dfrac{x}{a} \right\rceil \cdot a$ as $x \rightarrow \infty$?,What happens to  as ?,f(x) = \left\lceil \dfrac{x}{a} \right\rceil \cdot a x \rightarrow \infty,"Consider the function $f(x) = \left\lceil \dfrac{x}{a} \right\rceil \cdot a~~~$ where $a \in R$ and $a \neq 0 $ . Now let us say we are interested in the behavior of $f(x)$ as $x \rightarrow \infty$ . It seems like $f(x) \sim x$ , but I'm trying to come up with a formal proof. For an illustration, refer here .","Consider the function where and . Now let us say we are interested in the behavior of as . It seems like , but I'm trying to come up with a formal proof. For an illustration, refer here .",f(x) = \left\lceil \dfrac{x}{a} \right\rceil \cdot a~~~ a \in R a \neq 0  f(x) x \rightarrow \infty f(x) \sim x,['limits']
13,Is my interpretation of three dimensional improper integral correct?,Is my interpretation of three dimensional improper integral correct?,,"In Physics/Electrostatics textbook, I am in a situation where we have to find the electric field at a point inside the volume charge distribution. In Cartesian coordinates, we can't do it the usual way because of the integrand singularity. So we use the three dimensional improper integral. $$\mathbf{E}=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}} \rho'\ \dfrac{\mathbf{r}-\mathbf{r'}}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag1$$ where: $\mathbf{r'}=(x',y',z')$ is coordinates of source points $\mathbf{r}=(x,y,z)$ is coordinates of field points $V'$ is the volume occupied by the charge $\delta_{\epsilon}$ is an arbitrary volume contained in $V'$ around the singular point $\mathbf{r}=\mathbf{r'}$ with $\epsilon$ being its greatest chord. $\rho'$ is the charge density and is continuous throughout the volume $V'-\delta_{\epsilon}$ While taking the limit the shape of $\delta_{\epsilon}$ is kept unaltered From equation $(1)$ , we can get the $x$ -component of $\mathbf{E}$ : $$E_x=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}} \rho'\ \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag2$$ I view the steps of solving $E_x$ as follows: Make a $\delta_{\epsilon}$ cavity (with $\epsilon=a$ ) contained in $V'$ around the singular point $\mathbf{r}=\mathbf{r'}$ . Then take the Riemann integral over $V'-\delta_{\epsilon}$ Find the function which relates ""Riemann integral over $V'-\delta_{\epsilon}$ "" and "" $\epsilon$ "" over the interval $(0,a]$ . For the sake of clarity we can also make a graph of "" $\epsilon$ "" ( $x$ -axis) and ""Riemann integral over $V'-\delta_{\epsilon}$ "" ( $y$ -axis) over the interval $(0,a]$ Find $l$ $\ni$ $\forall \varepsilon > 0, \exists \delta \ni \text{when} |x-0|<\delta, |y-l|< \varepsilon$ Thus $l$ is the solution for $E_x$ in equation $(2)$ I think this is the correct interpretation of equation $(2)$ (as I confirmed with one of my teachers) I know steps $(1)$ and $(3)$ can be done. Please explain a way to execute step $(2)$ .","In Physics/Electrostatics textbook, I am in a situation where we have to find the electric field at a point inside the volume charge distribution. In Cartesian coordinates, we can't do it the usual way because of the integrand singularity. So we use the three dimensional improper integral. where: is coordinates of source points is coordinates of field points is the volume occupied by the charge is an arbitrary volume contained in around the singular point with being its greatest chord. is the charge density and is continuous throughout the volume While taking the limit the shape of is kept unaltered From equation , we can get the -component of : I view the steps of solving as follows: Make a cavity (with ) contained in around the singular point . Then take the Riemann integral over Find the function which relates ""Riemann integral over "" and "" "" over the interval . For the sake of clarity we can also make a graph of "" "" ( -axis) and ""Riemann integral over "" ( -axis) over the interval Find Thus is the solution for in equation I think this is the correct interpretation of equation (as I confirmed with one of my teachers) I know steps and can be done. Please explain a way to execute step .","\mathbf{E}=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}}
\rho'\ \dfrac{\mathbf{r}-\mathbf{r'}}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag1 \mathbf{r'}=(x',y',z') \mathbf{r}=(x,y,z) V' \delta_{\epsilon} V' \mathbf{r}=\mathbf{r'} \epsilon \rho' V'-\delta_{\epsilon} \delta_{\epsilon} (1) x \mathbf{E} E_x=\lim\limits_{\epsilon\to 0} \int_{V'-\delta_{\epsilon}}
\rho'\ \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3} dV' \tag2 E_x \delta_{\epsilon} \epsilon=a V' \mathbf{r}=\mathbf{r'} V'-\delta_{\epsilon} V'-\delta_{\epsilon} \epsilon (0,a] \epsilon x V'-\delta_{\epsilon} y (0,a] l \ni \forall \varepsilon > 0, \exists \delta \ni \text{when} |x-0|<\delta, |y-l|< \varepsilon l E_x (2) (2) (1) (3) (2)","['limits', 'multivariable-calculus', 'improper-integrals', 'physics', 'riemann-integration']"
14,Help trying to prove the existence of sequence limit. [duplicate],Help trying to prove the existence of sequence limit. [duplicate],,"This question already has answers here : Proving the general limit $\lim_{n\to\infty} \frac{an+c}{bn+d} = \frac{a}{b}$ (4 answers) Closed 4 years ago . So I'm trying to prove that $$\lim_{n \to +\infty} \frac{2n - 4}{3n - 7} = \frac{2}{3}$$ using the formal definition of the limit of a sequence. Applying it I've got $$\left|\frac{2n - 4}{3n - 7} - \frac{2}{3}\right| \lt \varepsilon,$$ $$\frac{2}{\left|9n-21\right|} \lt \varepsilon.$$ From here I don't know how to continue, I guess I have to get ripped of the absolute value but I don't know how should I do it. Thank you so much.","This question already has answers here : Proving the general limit $\lim_{n\to\infty} \frac{an+c}{bn+d} = \frac{a}{b}$ (4 answers) Closed 4 years ago . So I'm trying to prove that using the formal definition of the limit of a sequence. Applying it I've got From here I don't know how to continue, I guess I have to get ripped of the absolute value but I don't know how should I do it. Thank you so much.","\lim_{n \to +\infty} \frac{2n - 4}{3n - 7} = \frac{2}{3} \left|\frac{2n - 4}{3n - 7} - \frac{2}{3}\right| \lt \varepsilon, \frac{2}{\left|9n-21\right|} \lt \varepsilon.","['real-analysis', 'sequences-and-series', 'limits', 'epsilon-delta']"
15,alternative definitions for limit of a sequence,alternative definitions for limit of a sequence,,"In several proofs I noticed that authors consider slightly different inequalities to prove that a sequence $(a_n)$ converges to a limit $l$ , for example: $$\forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | \le \epsilon$$ and $$\forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | < k\epsilon$$ where k is a constant. The aforementioned versions are different from the following traditional definition: $$\forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | < \epsilon$$ Why can we consider them as equivalent? Thanks a lot.","In several proofs I noticed that authors consider slightly different inequalities to prove that a sequence converges to a limit , for example: and where k is a constant. The aforementioned versions are different from the following traditional definition: Why can we consider them as equivalent? Thanks a lot.",(a_n) l \forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | \le \epsilon \forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | < k\epsilon \forall \epsilon>0 \: \exists N \: \forall n \ge N \: |a_n - l | < \epsilon,['limits']
16,Parametric limit with sine,Parametric limit with sine,,"I'm trying to solve this limit problem which asks to find what happens to the following limits as $x$ varies: $$\lim_{n\to+\infty} \left((\sin x -1) + {1\over{n^2 +1}}\right)^{n^2}$$ My steps so far: use exponential rule and rewrite as $$\lim_{n\to +\infty} \exp\left(n^2 \ln \frac{n^2\sin x+\sin x-n^2}{n^2+1}\right)$$ Divide both the numerator and denominator in the argument of the logarithm by $n^2$ to get $$\frac{\sin x+\frac{\sin x}{n^2}-1}{1+\frac{1}{n^2}}$$ Then, as $n \to +\infty$ everything should behave like: $$\lim_{n\to +\infty} e^{{n^2} \ln(\sin x -1)}$$ Hence we should have: $$\lim_{n\to +\infty} (\sin x -1)^{n^2}$$","I'm trying to solve this limit problem which asks to find what happens to the following limits as varies: My steps so far: use exponential rule and rewrite as Divide both the numerator and denominator in the argument of the logarithm by to get Then, as everything should behave like: Hence we should have:",x \lim_{n\to+\infty} \left((\sin x -1) + {1\over{n^2 +1}}\right)^{n^2} \lim_{n\to +\infty} \exp\left(n^2 \ln \frac{n^2\sin x+\sin x-n^2}{n^2+1}\right) n^2 \frac{\sin x+\frac{\sin x}{n^2}-1}{1+\frac{1}{n^2}} n \to +\infty \lim_{n\to +\infty} e^{{n^2} \ln(\sin x -1)} \lim_{n\to +\infty} (\sin x -1)^{n^2},['limits']
17,Please help me understand the following transition in the limit,Please help me understand the following transition in the limit,,"I was trying to figure out how a limit was calculated and got stuck when trying to understand one of the proposed solutions: (note that this is just a small part of the solution, but the one that got me in trouble) $$\lim_{n\to\infty}\frac{1}{\sqrt{n}}\left|\sum\limits_{k=1}^n (-1)^k\sqrt{k}\right|    = \lim_{n\to\infty}\frac{1}{\sqrt{2n}}\sum\limits_{k=1}^n \frac{1}{\sqrt{2k-1}+\sqrt{2k}} $$ In my opinion, whether $n$ is odd or even has an impact on the sum. Plugging a few random $n$ -s doesn't help to prove the validity of the formula for me. I guess this is one of the cases when I am puzzled and can't see something obvious. If someone could clarify it for me, that would be great. Thanks!","I was trying to figure out how a limit was calculated and got stuck when trying to understand one of the proposed solutions: (note that this is just a small part of the solution, but the one that got me in trouble) In my opinion, whether is odd or even has an impact on the sum. Plugging a few random -s doesn't help to prove the validity of the formula for me. I guess this is one of the cases when I am puzzled and can't see something obvious. If someone could clarify it for me, that would be great. Thanks!","\lim_{n\to\infty}\frac{1}{\sqrt{n}}\left|\sum\limits_{k=1}^n (-1)^k\sqrt{k}\right|   
= \lim_{n\to\infty}\frac{1}{\sqrt{2n}}\sum\limits_{k=1}^n \frac{1}{\sqrt{2k-1}+\sqrt{2k}}  n n","['sequences-and-series', 'limits', 'summation']"
18,"$\lim\limits_{y \rightarrow 0}3y\int_{y^2}^{y}\frac{x^2\sin(x^2y^3)}{x^2+\cos(x^2y^3)}dx\quad x,y \in R$",,"\lim\limits_{y \rightarrow 0}3y\int_{y^2}^{y}\frac{x^2\sin(x^2y^3)}{x^2+\cos(x^2y^3)}dx\quad x,y \in R","I would like to compute the following limit : $$\lim\limits_{y\rightarrow 0}{}g(y)=\lim\limits_{y \rightarrow 0}3y\int_{y^2}^{y}\frac{x^2\sin(x^2y^3)}{x^2+\cos(x^2y^3)}dx\quad x,y \in R, \quad \mid y \mid <1$$ My attempt :  We can use this theorem :  If $f :[a,b] \times I \rightarrow R$ is continuous ( $I$ is open) and $\frac{\partial f}{\partial y}$ exists and is continuous, then , let $a<b$ , then $$g(y):=\int_{a}^{b}f(x,y)dx$$ is $C^1(I)$ and $$g'(y):=\int_{a}^{b}\frac{\partial f}{\partial y}(x,y)dx$$ It means that $\lim\limits_{y\rightarrow 0}g(y)=g(0)$ So here, since $\mid y \mid <1$ , we get $$\lim\limits_{y\rightarrow 0}{}g(y)= 0$$ ? I am not sure about that...","I would like to compute the following limit : My attempt :  We can use this theorem :  If is continuous ( is open) and exists and is continuous, then , let , then is and It means that So here, since , we get ? I am not sure about that...","\lim\limits_{y\rightarrow 0}{}g(y)=\lim\limits_{y \rightarrow 0}3y\int_{y^2}^{y}\frac{x^2\sin(x^2y^3)}{x^2+\cos(x^2y^3)}dx\quad x,y \in R, \quad \mid y \mid <1 f :[a,b] \times I \rightarrow R I \frac{\partial f}{\partial y} a<b g(y):=\int_{a}^{b}f(x,y)dx C^1(I) g'(y):=\int_{a}^{b}\frac{\partial f}{\partial y}(x,y)dx \lim\limits_{y\rightarrow 0}g(y)=g(0) \mid y \mid <1 \lim\limits_{y\rightarrow 0}{}g(y)= 0","['real-analysis', 'integration', 'limits']"
19,Prove that $\sum_{n=1}^{\infty}\log \cos \left (\frac{1}{n}\right )$ converges absolutely.,Prove that  converges absolutely.,\sum_{n=1}^{\infty}\log \cos \left (\frac{1}{n}\right ),Prove that $$\sum_{n=1}^{\infty}\log \cos \left (\frac{1}{n}\right )$$ converges absolutely. The answer here suggests to use the Limit Comparison Test but it works for $a_n \geq 0$ while $\ln(\cos (1/n))<0$ . Also the limit given in the answer is $-\frac{1}{2}$ while the test gives results for positive limit values only. That post is $6$ years old so I didn't leave this as a comment.,Prove that converges absolutely. The answer here suggests to use the Limit Comparison Test but it works for while . Also the limit given in the answer is while the test gives results for positive limit values only. That post is years old so I didn't leave this as a comment.,\sum_{n=1}^{\infty}\log \cos \left (\frac{1}{n}\right ) a_n \geq 0 \ln(\cos (1/n))<0 -\frac{1}{2} 6,"['sequences-and-series', 'limits', 'trigonometry', 'summation']"
20,Convergence of moving point inside of unit disk.,Convergence of moving point inside of unit disk.,,"Suppose Set D : $$D=\left\{(x, y)\vert x^2 + y^2 \leq 1 \right\}$$ And Point $P(0, 0)$ on coordinate plane. Define 'Movement' : For a point P, select any direction and move $\frac{1}{2^n}$ to straight (Sorry, my english is poor.) When 'Movement' Execute $n$ -th times For example : $1$ st time, Suppose I selected positive-x-axis direction Then, Point $P(0,0)$ moves to $P\left(\frac{1}{2},0\right)$ $2$ nd time, Suppose I selected Positive-y-axis direction Then, Point $P\left(\frac{1}{2}, 0\right)$ moves to $P\left(\frac{1}{2},\frac{1}{4} \right)$ If I take $n\longrightarrow\infty$ , Arbitrary point $A(x, y)\in D$ can be expressed by 'Movement'? I Know that perimeter of $x^2+y^2=1$ can be expressed by 'Movement' because $$\sum_{n=1}^\infty \frac{1}{2^n}=1$$ But how about inside of $x^2 + y^2 =1$ ? Example The point with $1, 2, 3, 4, \cdots$ is a point which moved by $n$ -th 'Movement' The angle(direction) is free. the problem is : If $n \longrightarrow\infty$ , arbitrary points in $D$ can be expressed by 'Movement'?","Suppose Set D : And Point on coordinate plane. Define 'Movement' : For a point P, select any direction and move to straight (Sorry, my english is poor.) When 'Movement' Execute -th times For example : st time, Suppose I selected positive-x-axis direction Then, Point moves to nd time, Suppose I selected Positive-y-axis direction Then, Point moves to If I take , Arbitrary point can be expressed by 'Movement'? I Know that perimeter of can be expressed by 'Movement' because But how about inside of ? Example The point with is a point which moved by -th 'Movement' The angle(direction) is free. the problem is : If , arbitrary points in can be expressed by 'Movement'?","D=\left\{(x, y)\vert x^2 + y^2 \leq 1 \right\} P(0, 0) \frac{1}{2^n} n 1 P(0,0) P\left(\frac{1}{2},0\right) 2 P\left(\frac{1}{2}, 0\right) P\left(\frac{1}{2},\frac{1}{4} \right) n\longrightarrow\infty A(x, y)\in D x^2+y^2=1 \sum_{n=1}^\infty \frac{1}{2^n}=1 x^2 + y^2 =1 1, 2, 3, 4, \cdots n n \longrightarrow\infty D","['real-analysis', 'sequences-and-series', 'limits', 'euclidean-geometry']"
21,Calculate limit in use of integrals,Calculate limit in use of integrals,,"Calculate limit in use of integrals $$ \lim_{n \rightarrow \infty} \sum_{k=1}^{n} \frac{1+n}{3k^2+n^2} $$ My attempt: $$\sum_{k=1}^{n} \frac{1+n}{3k^2+n^2} = \frac{1}{n} \sum_{k=1}^{n} \frac{\frac{1}{n}+1}{3(k/n)^2+1} = \\ \frac{1}{n}\cdot (1/n + 1) \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}  $$ Ok, I know that when I am taking limit I should replace (from aproximation theorem) $$ \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}$$ with $$ \int_{0}^{1} \frac{1}{1+3x^2}$$ but I still don't know what have I do (and why) with $$ \frac{1}{n}\cdot (1/n + 1) $$ part. In many solutions we just ignore part $\frac{1}{n}$ but I don't know why and there where I have little more 'difficult' expression like $ \frac{1}{n}\cdot (1/n + 1) $ I completely don't know what should I do... $$  $$","Calculate limit in use of integrals My attempt: Ok, I know that when I am taking limit I should replace (from aproximation theorem) with but I still don't know what have I do (and why) with part. In many solutions we just ignore part but I don't know why and there where I have little more 'difficult' expression like I completely don't know what should I do..."," \lim_{n \rightarrow \infty} \sum_{k=1}^{n} \frac{1+n}{3k^2+n^2}  \sum_{k=1}^{n} \frac{1+n}{3k^2+n^2} = \frac{1}{n} \sum_{k=1}^{n} \frac{\frac{1}{n}+1}{3(k/n)^2+1} = \\
\frac{1}{n}\cdot (1/n + 1) \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}    \sum_{k=1}^{n} \frac{1}{3(k/n)^2+1}  \int_{0}^{1} \frac{1}{1+3x^2}  \frac{1}{n}\cdot (1/n + 1)  \frac{1}{n}  \frac{1}{n}\cdot (1/n + 1)    ","['integration', 'limits', 'summation']"
22,Functions of Bounded Variation Have Left and Right Limits,Functions of Bounded Variation Have Left and Right Limits,,"I have a proof of this for a real valued function https://www.encyclopediaofmath.org/index.php/Function_of_bounded_variation#Generalizations based on the Jordan decomposition into monotonic functions. I am looking for a more general proof in the case of a function $f: I = [a, b] \subset \mathbb R \to X$ where $X$ is a complete normed vector space (or even more generally, a complete metric space $(X, d)$ in which case substitute $||f(s) - f(t)||$ by $d(f(s) - f(t))$ in what follows) . My own attempt follows and I would appreciate feedback on this. Definitions: Let $f: I = [a, b] \subset \mathbb R \to X$ where $X$ is a normed vector space and $||.||$ its norm. A partition of a $[a, b]$ is a finite set of points $P = \{p_0 = a < p_1 < p_2 ... < p_n = b\}$ $V(f, P):= \sum_{i=1}^n ||f(p_i) - f(p_{i-1}||$ is a real non-negative number being the variation of $f$ on the partition $P$ (of the interval $I$ ). Note that the variation is defined for any function and any partition. $V(f):= sup\{V(f, P): P $ is a partition of I } is an extended real (i.e. can be $+\infty$ ) being the (total) variation of $f$ on the interval $I$ . If $V(f)$ is finite then $f$ is a function of bounded variation . $f$ has a left limit at $x \in (a, b]$ if given $\epsilon > 0$ there is $w \in [a, x)$ such that for all $y, z \in (w, x)$ then $||f(y) - f(z)|| < \epsilon$ . This is the Cauchy condition for the existence of this limit in a complete space. The right limit is similarly defined for $x \in [a, b)$ . Proof: Assume that $f$ does not have a left limit at some point $x \in (a, b]$ and construct a sequence of partitions of $[a, x]$ of increasing (unbounded) variation. This shows that in such cases $f$ cannot be of bounded variation (on $[a, x]$ and therefore also on $[a, b]$ ). The case for not having a right limit is analogous, so it follows by contra-positive that if $f$ is of bounded variation these limits must exist. Express the absence of a left limit by negation of the Cauchy definition....... If $f$ has no left limit at $x \in (a, b]$ then there is some $\epsilon > 0$ where for all $w \in [a, x)$ there is some $y, z \in (w, x)$ with $||f(y) - f(z)|| \ge \epsilon$ . Wlg assume $y < z$ . Firstly, this applies for $w = a$ and there is $a < y_1 < z_1 < x$ with $||f(y_1) - f(z_1)|| \ge \epsilon$ . Now define the partition $P_1 = \{a, y_1, z_1, x\}$ and it follows that $V(f, P_1) \ge \epsilon$ . As a second iteration, the condition applies for $w = z_1$ so there is $z_1 < y_2 < z_2 < x$ with $||f(y_2) - f(z_2)|| \ge \epsilon$ . Define the partition $P_2$ by adding $y_2, z_2$ to $P_1$ (in sequence) $ = \{a, y_1, z_1, y_2, z_2, x\}$ and  it follows that $V(f, P_2) \ge 2\epsilon$ . Then one can continue and define a sequence of partitions of $[a, x]$ $P_1, P_2, .....$ and for $P_n$ we have $V(f, P_n) \ge n.\epsilon$ so $sup\{V(f, P): P $ is a partition of $[a, x]\} = \infty$ , I.e. $f $ is not of bounded variation on $[a, x]$ .","I have a proof of this for a real valued function https://www.encyclopediaofmath.org/index.php/Function_of_bounded_variation#Generalizations based on the Jordan decomposition into monotonic functions. I am looking for a more general proof in the case of a function where is a complete normed vector space (or even more generally, a complete metric space in which case substitute by in what follows) . My own attempt follows and I would appreciate feedback on this. Definitions: Let where is a normed vector space and its norm. A partition of a is a finite set of points is a real non-negative number being the variation of on the partition (of the interval ). Note that the variation is defined for any function and any partition. is a partition of I } is an extended real (i.e. can be ) being the (total) variation of on the interval . If is finite then is a function of bounded variation . has a left limit at if given there is such that for all then . This is the Cauchy condition for the existence of this limit in a complete space. The right limit is similarly defined for . Proof: Assume that does not have a left limit at some point and construct a sequence of partitions of of increasing (unbounded) variation. This shows that in such cases cannot be of bounded variation (on and therefore also on ). The case for not having a right limit is analogous, so it follows by contra-positive that if is of bounded variation these limits must exist. Express the absence of a left limit by negation of the Cauchy definition....... If has no left limit at then there is some where for all there is some with . Wlg assume . Firstly, this applies for and there is with . Now define the partition and it follows that . As a second iteration, the condition applies for so there is with . Define the partition by adding to (in sequence) and  it follows that . Then one can continue and define a sequence of partitions of and for we have so is a partition of , I.e. is not of bounded variation on .","f: I = [a, b] \subset \mathbb R \to X X (X, d) ||f(s) - f(t)|| d(f(s) - f(t)) f: I = [a, b] \subset \mathbb R \to X X ||.|| [a, b] P = \{p_0 = a < p_1 < p_2 ... < p_n = b\} V(f, P):= \sum_{i=1}^n ||f(p_i) - f(p_{i-1}|| f P I V(f):= sup\{V(f, P): P  +\infty f I V(f) f f x \in (a, b] \epsilon > 0 w \in [a, x) y, z \in (w, x) ||f(y) - f(z)|| < \epsilon x \in [a, b) f x \in (a, b] [a, x] f [a, x] [a, b] f f x \in (a, b] \epsilon > 0 w \in [a, x) y, z \in (w, x) ||f(y) - f(z)|| \ge \epsilon y < z w = a a < y_1 < z_1 < x ||f(y_1) - f(z_1)|| \ge \epsilon P_1 = \{a, y_1, z_1, x\} V(f, P_1) \ge \epsilon w = z_1 z_1 < y_2 < z_2 < x ||f(y_2) - f(z_2)|| \ge \epsilon P_2 y_2, z_2 P_1  = \{a, y_1, z_1, y_2, z_2, x\} V(f, P_2) \ge 2\epsilon [a, x] P_1, P_2, ..... P_n V(f, P_n) \ge n.\epsilon sup\{V(f, P): P  [a, x]\} = \infty f  [a, x]","['real-analysis', 'limits', 'proof-verification', 'bounded-variation']"
23,Why is this limit evaluated like so?,Why is this limit evaluated like so?,,"Question: If $$\lim_{x \to 0}{\frac{-1 + \sqrt{(\tan x - \sin x) + \sqrt{(\tan x - \sin x) + \sqrt{(\tan x - \sin x) + \cdots \infty}}}}{-1 + \sqrt{x^3 + \sqrt{x^3 + \sqrt{x^3 + \cdots \infty}}}}} = \frac{1}{k}$$ Then find the value of $k$ . The way I approached the problem was by substituting $x = 0$ in the limit: $$\frac{-1 + \sqrt{(\tan 0 - \sin 0) + \sqrt{(\tan 0 - \sin 0) + \sqrt{(\tan 0 - \sin 0) + \cdots \infty}}}}{-1 + \sqrt{0^3 + \sqrt{0^3 + \sqrt{0^3 + \cdots \infty}}}} = \frac{1}{k}$$ $$\implies \frac{-1 + \sqrt{0 + \sqrt{0 + \sqrt{0 + \cdots \infty}}}}{-1 + \sqrt{0 + \sqrt{0 + \sqrt{0 + \cdots \infty}}}} = \frac{1}{k}$$ $$\implies \frac{-1}{-1} = \frac{1}{k}$$ $$\implies \frac{1}{1} = \frac{1}{k}$$ $$\implies k = 1$$ But according to the given solution, the answer is 2. I did find this question, but I do not understand why I cannot just put $x = 0$ in the limit. Any help is appreciated.","Question: If Then find the value of . The way I approached the problem was by substituting in the limit: But according to the given solution, the answer is 2. I did find this question, but I do not understand why I cannot just put in the limit. Any help is appreciated.",\lim_{x \to 0}{\frac{-1 + \sqrt{(\tan x - \sin x) + \sqrt{(\tan x - \sin x) + \sqrt{(\tan x - \sin x) + \cdots \infty}}}}{-1 + \sqrt{x^3 + \sqrt{x^3 + \sqrt{x^3 + \cdots \infty}}}}} = \frac{1}{k} k x = 0 \frac{-1 + \sqrt{(\tan 0 - \sin 0) + \sqrt{(\tan 0 - \sin 0) + \sqrt{(\tan 0 - \sin 0) + \cdots \infty}}}}{-1 + \sqrt{0^3 + \sqrt{0^3 + \sqrt{0^3 + \cdots \infty}}}} = \frac{1}{k} \implies \frac{-1 + \sqrt{0 + \sqrt{0 + \sqrt{0 + \cdots \infty}}}}{-1 + \sqrt{0 + \sqrt{0 + \sqrt{0 + \cdots \infty}}}} = \frac{1}{k} \implies \frac{-1}{-1} = \frac{1}{k} \implies \frac{1}{1} = \frac{1}{k} \implies k = 1 x = 0,"['sequences-and-series', 'limits', 'trigonometry']"
24,Find the limit $\lim_{x\to 0} ((9+x)^x-9^x)^x$,Find the limit,\lim_{x\to 0} ((9+x)^x-9^x)^x,Find the limit $$\lim_{x\to 0} \Big((9+x)^x-9^x\Big)^x$$ I simply cannot solve this limit. L'Hospital rule is useless(if you extract for example $9^x$ and rewrite) and there is nothing to gain if you consider $x$ as $1/n$ in order to somehow use the known limit $(1+1/n)^n \rightarrow e$,Find the limit I simply cannot solve this limit. L'Hospital rule is useless(if you extract for example and rewrite) and there is nothing to gain if you consider as in order to somehow use the known limit,\lim_{x\to 0} \Big((9+x)^x-9^x\Big)^x 9^x x 1/n (1+1/n)^n \rightarrow e,"['real-analysis', 'calculus', 'limits']"
25,limit of a series homework question,limit of a series homework question,,"I need some help with this question. The question: Prove or disprove the following statement: If $$a_n\cdot a_{n+1} \rightarrow 0$$ and $a_n > 0$ for all $n$ , then $$a_n \rightarrow 0$$ Solution attempt: The solution I was given disproves this statement using an example (which I understand): $$a_n = \left\{\begin{matrix} 1\quad & {n\quad odd} \\  \frac{1}{n}\quad &  n \quad even \end{matrix}\right.$$ but I don't understand what is wrong with this proof: For all $\epsilon>0$ , there exists an $N$ so that for all $n\geq N$ , $a_n\cdot a_{n+1} < \epsilon$ . Therefore (because $a_n > 0$ for all $n$ ): $$a_n<\frac{\epsilon}{a_{n+1}}<\epsilon$$ which prooves the statemant. Can anyone explain what I am doing wrong?","I need some help with this question. The question: Prove or disprove the following statement: If and for all , then Solution attempt: The solution I was given disproves this statement using an example (which I understand): but I don't understand what is wrong with this proof: For all , there exists an so that for all , . Therefore (because for all ): which prooves the statemant. Can anyone explain what I am doing wrong?","a_n\cdot a_{n+1} \rightarrow 0 a_n > 0 n a_n \rightarrow 0 a_n = \left\{\begin{matrix}
1\quad & {n\quad odd} \\ 
\frac{1}{n}\quad &  n \quad even
\end{matrix}\right. \epsilon>0 N n\geq N a_n\cdot a_{n+1} < \epsilon a_n > 0 n a_n<\frac{\epsilon}{a_{n+1}}<\epsilon","['calculus', 'limits']"
26,How do you prove that $\binom{n}{d} = \Theta(n^d)$?,How do you prove that ?,\binom{n}{d} = \Theta(n^d),"I am stuck on proving that that $\binom{n}{d} = \Theta(n^d)$ for any positive fixed integer d. I tried using the fact that if this is true, it means that for some integers c $_1$ and c $_2$ , $c_1n^d \le |\frac{n!}{(n-d)!d!}| \le c_2n^d$ for any $n \ge n_0$ where $n_0$ is an integer. I tried using the fact that since n is greater than d and d is positive, then $|\frac{n!}{(n-d)!d!}|$ = $\frac{n!}{(n-d)!d!}$ . If the above equation is true in which the binomial expansion is bounded, then it would mean that $c_2 \ge \frac{n!}{n^d(n-d)!d!}$ , and similarly, $c_1 \le \frac{n!}{n^d(n-d)!d!}$ So I have absolutely no idea how to find these constants $c_1$ and $c_2$ , let alone find what $n_0$ is. Can someone please help me with this in a way that allows me to at least understand more of what's going on here? Thank you in advance.","I am stuck on proving that that for any positive fixed integer d. I tried using the fact that if this is true, it means that for some integers c and c , for any where is an integer. I tried using the fact that since n is greater than d and d is positive, then = . If the above equation is true in which the binomial expansion is bounded, then it would mean that , and similarly, So I have absolutely no idea how to find these constants and , let alone find what is. Can someone please help me with this in a way that allows me to at least understand more of what's going on here? Thank you in advance.",\binom{n}{d} = \Theta(n^d) _1 _2 c_1n^d \le |\frac{n!}{(n-d)!d!}| \le c_2n^d n \ge n_0 n_0 |\frac{n!}{(n-d)!d!}| \frac{n!}{(n-d)!d!} c_2 \ge \frac{n!}{n^d(n-d)!d!} c_1 \le \frac{n!}{n^d(n-d)!d!} c_1 c_2 n_0,"['number-theory', 'limits', 'polynomials', 'binomial-coefficients']"
27,"Find $k\in\Bbb R$ if $\lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\int_0^\infty xe^{-x}\,\mathrm dx$",Find  if,"k\in\Bbb R \lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\int_0^\infty xe^{-x}\,\mathrm dx","Find $k\in\Bbb R$ if $$\lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\int_0^\infty xe^{-x}\,\mathrm dx.$$ $$\lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\lim_{x\to0}\frac{k\sin(kx)}{2x}=\lim_{x\to0}\frac{k^2}{2}\frac{\sin(kx)}{kx}=\frac{k^2}2.$$ To find $\int_0^\infty xe^{-x}\,\mathrm dx$ we can add a limit: $$\lim_{b\to\infty}\int_0^bxe^{-x}\,\mathrm dx.$$ Now we can integrate by parts. Thus $u=x$ , so $u'=1$ , and $\mathrm dv=e^{-x}$ , so $v=-e^{-x}$ , thus $$\int xe^{-x}\,\mathrm dx=uv-\int u'v\,\mathrm dx=-xe^{-x}+\int e^{-x}\,\mathrm dx=-xe^{-x}-e^{-x}=-e^{-x}(x+1),$$ hence $$\lim_{b\to\infty}\left[-e^{-x}(x+1)\right]_0^b=-\lim_{b\to\infty}(e^{-b}(b+1)-1)=-(0-1)=1.$$ Hence, $$\frac{k^2}2=1\implies\boxed{k=\sqrt2\vee k=-\sqrt2}.$$ Is it correct? Thanks!!","Find if To find we can add a limit: Now we can integrate by parts. Thus , so , and , so , thus hence Hence, Is it correct? Thanks!!","k\in\Bbb R \lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\int_0^\infty xe^{-x}\,\mathrm dx. \lim_{x\to0}\frac{1-\cos(kx)}{x^2}=\lim_{x\to0}\frac{k\sin(kx)}{2x}=\lim_{x\to0}\frac{k^2}{2}\frac{\sin(kx)}{kx}=\frac{k^2}2. \int_0^\infty xe^{-x}\,\mathrm dx \lim_{b\to\infty}\int_0^bxe^{-x}\,\mathrm dx. u=x u'=1 \mathrm dv=e^{-x} v=-e^{-x} \int xe^{-x}\,\mathrm dx=uv-\int u'v\,\mathrm dx=-xe^{-x}+\int e^{-x}\,\mathrm dx=-xe^{-x}-e^{-x}=-e^{-x}(x+1), \lim_{b\to\infty}\left[-e^{-x}(x+1)\right]_0^b=-\lim_{b\to\infty}(e^{-b}(b+1)-1)=-(0-1)=1. \frac{k^2}2=1\implies\boxed{k=\sqrt2\vee k=-\sqrt2}.","['calculus', 'limits', 'proof-verification', 'improper-integrals']"
28,"$\lim_{(x,y) \to (0,0)} {\frac{x^2y}{x+xy+y^2}}$",,"\lim_{(x,y) \to (0,0)} {\frac{x^2y}{x+xy+y^2}}","I was playing around with limits of two real variables, when I came up with the one on the title. I tried all directional limits and they were all $0.$ So then I tried a few relative limits like $y=ax^2$ , $y=x^3-x^2$ and $0\leq x=\sqrt{y}$ , but still got $0.$ I tried then with $y=\cos(x)-1,$ and it gave a complicated expression which I plugged into symbolab, and it gave $0$ too. So I tried to prove that it exists and is $0.$ So I'd need to find an inequality between the expression below and a norm $ \vert \vert (x,y) \vert \vert.$ I don't see how to manipulate it however, since the things in the denominator can be negative, and the only inequality I know can be applied here is $|xy| \leq x^2+y^2$ , which would get: $$\left|\frac{x^2y}{x+xy+y^2}\right| \leq \left| \frac{x^2+x^2y^2}{x+xy+y^2} \right|.$$ I don't know how to proceed from that, nor have any other ideas to try to disprove it exists. If possible give just a hint please, and I haven't yet learned polar coordinates nor partial derivatives.","I was playing around with limits of two real variables, when I came up with the one on the title. I tried all directional limits and they were all So then I tried a few relative limits like , and , but still got I tried then with and it gave a complicated expression which I plugged into symbolab, and it gave too. So I tried to prove that it exists and is So I'd need to find an inequality between the expression below and a norm I don't see how to manipulate it however, since the things in the denominator can be negative, and the only inequality I know can be applied here is , which would get: I don't know how to proceed from that, nor have any other ideas to try to disprove it exists. If possible give just a hint please, and I haven't yet learned polar coordinates nor partial derivatives.","0. y=ax^2 y=x^3-x^2 0\leq x=\sqrt{y} 0. y=\cos(x)-1, 0 0.  \vert \vert (x,y) \vert \vert. |xy| \leq x^2+y^2 \left|\frac{x^2y}{x+xy+y^2}\right| \leq \left| \frac{x^2+x^2y^2}{x+xy+y^2} \right|.","['limits', 'multivariable-calculus']"
29,Does the following limit exist $\lim_{x \to 0} \frac{\cos x^{-2}}{ \cos x^{-2}}$,Does the following limit exist,\lim_{x \to 0} \frac{\cos x^{-2}}{ \cos x^{-2}},"$$\lim_{x \to 0} \frac{\cos x^{-2}}{ \cos x^{-2}}$$ I had nothing better to do than come up with a pathological function.  The cos(x) function has an infinite number of zeroes which are constantly spaced. cos(x^2) likewise has an infinity of zeroes, but the spacing between them decreases as one goes out to either infinity. cos(x^-2) maps that infinity of zeroes into a finite interval, namely [-1,1]. I understand my quest for a pathological function might end here, but I felt the fact that its limit at zero didn't exist was more intuitive, since it had an infinite number of waves within any interval around x=0. So anyway, to generate a flat line with a infinite number of gaps on a finite interval around zero, I simply divided cos(x^-2) by itself to get cos(x^-2)/cos(x^-2) Clearly $${cos(x^{-2})\over{cos(x^{-2})}}=1;x \neq \sqrt[\scriptstyle-2]{{\pi\over2}+2\pi k} \enspace or \enspace \sqrt[\scriptstyle-2]{{3\pi\over2}+2\pi k}, \enspace k \in \mathbb{N} $$ I asked my classmates about this limit, and they thought little of it, even when I warned them about its odd neighborhood behavior. They were all convinced it was 1, since the function is 1 ""almost everywhere"" near x=0. I'll be honest, I'm not even sure if it exists. This pathology prevents one from defining f for an interval of any sort around 0, which I thought might preclude the existence of a limit because you lose some oppurtunities to find a delta within for every epsilon within an interval, since an infinity of epsilons is missing. I looked at wikipedia's (ε, δ)-definition of a limit line by line (I've never taken a real analysis course). Let ${\displaystyle f}$ be a real-valued function defined on a subset ${\displaystyle D}$ of the real numbers. Let ${\displaystyle c}$ be a limit point of ${\displaystyle D}$ and let ${\displaystyle L}$ be a real number. So f is defined on a subset of the real numbers, though an odd subset at that. When I speak of D, it can be the interval [-1,1] or any subinterval including x=0, since they all are pesky. I have a feeling that the issue here is the condition that c, here 0, has to be a limit point of the subset D of $\mathbb{R}$ . To be honest, I only have an intuitive grasp of what a limit point is. I know that the ends of an open interval are its limit points for example, but I have no Idea what it is in general. For a normal space, I know you can just show that a point is a limit point if it is the limit of some sequence of points in the subset at hand. So back to this example, I think you can come up with any other sequence that approaches zero as long as you dodge the gaps. So, straight to the chase... $${\displaystyle \lim _{x\to c}f(x)=L\iff (\forall \varepsilon >0,\,\exists \ \delta >0,\,\forall x\in D,\,0<|x-c|<\delta \ \Rightarrow \ |f(x)-L|<\varepsilon )}$$ I'll be honest, I don't fully understand this definition. In fact I've never done an epsilon delta proof before. If you can respond to some of my reasonings, that'd be great, but any airtight proof/disproof of the existence of the limit is appreciated.","I had nothing better to do than come up with a pathological function.  The cos(x) function has an infinite number of zeroes which are constantly spaced. cos(x^2) likewise has an infinity of zeroes, but the spacing between them decreases as one goes out to either infinity. cos(x^-2) maps that infinity of zeroes into a finite interval, namely [-1,1]. I understand my quest for a pathological function might end here, but I felt the fact that its limit at zero didn't exist was more intuitive, since it had an infinite number of waves within any interval around x=0. So anyway, to generate a flat line with a infinite number of gaps on a finite interval around zero, I simply divided cos(x^-2) by itself to get cos(x^-2)/cos(x^-2) Clearly I asked my classmates about this limit, and they thought little of it, even when I warned them about its odd neighborhood behavior. They were all convinced it was 1, since the function is 1 ""almost everywhere"" near x=0. I'll be honest, I'm not even sure if it exists. This pathology prevents one from defining f for an interval of any sort around 0, which I thought might preclude the existence of a limit because you lose some oppurtunities to find a delta within for every epsilon within an interval, since an infinity of epsilons is missing. I looked at wikipedia's (ε, δ)-definition of a limit line by line (I've never taken a real analysis course). Let be a real-valued function defined on a subset of the real numbers. Let be a limit point of and let be a real number. So f is defined on a subset of the real numbers, though an odd subset at that. When I speak of D, it can be the interval [-1,1] or any subinterval including x=0, since they all are pesky. I have a feeling that the issue here is the condition that c, here 0, has to be a limit point of the subset D of . To be honest, I only have an intuitive grasp of what a limit point is. I know that the ends of an open interval are its limit points for example, but I have no Idea what it is in general. For a normal space, I know you can just show that a point is a limit point if it is the limit of some sequence of points in the subset at hand. So back to this example, I think you can come up with any other sequence that approaches zero as long as you dodge the gaps. So, straight to the chase... I'll be honest, I don't fully understand this definition. In fact I've never done an epsilon delta proof before. If you can respond to some of my reasonings, that'd be great, but any airtight proof/disproof of the existence of the limit is appreciated.","\lim_{x \to 0} \frac{\cos x^{-2}}{ \cos x^{-2}} {cos(x^{-2})\over{cos(x^{-2})}}=1;x \neq \sqrt[\scriptstyle-2]{{\pi\over2}+2\pi k} \enspace or \enspace \sqrt[\scriptstyle-2]{{3\pi\over2}+2\pi k}, \enspace k \in \mathbb{N}  {\displaystyle f} {\displaystyle D} {\displaystyle c} {\displaystyle D} {\displaystyle L} \mathbb{R} {\displaystyle \lim _{x\to c}f(x)=L\iff (\forall \varepsilon >0,\,\exists \ \delta >0,\,\forall x\in D,\,0<|x-c|<\delta \ \Rightarrow \ |f(x)-L|<\varepsilon )}","['limits', 'epsilon-delta']"
30,Intuition for a limit found using L'Hôpital (geometry),Intuition for a limit found using L'Hôpital (geometry),,"$OPR$ is a sector with central angle $\theta$ . $A(\theta)$ is the area of the segment bounded by the line $PR$ and the arc $PR$ and $B(\theta)$ is the area of the triangle $PQR$ . The ratio $$\frac{A(\theta)}{B(\theta)} = \frac{r^2(\theta-\sin\theta)}{2\frac{r^2(1-\cos\theta)\sin\theta}{2}}$$ If I use L'Hôpital to find $\lim_{\theta \rightarrow 0} \frac{A(\theta)}{B(\theta)}$ , then the answer is $\frac{1}{3}$ . I was wondering if there is any intuition for this limit and if this should be the answer you 'expect'... I originally thought it would be $0$ and also not sure why this is wrong. As a smaller note, most books with this example write $\theta \rightarrow 0^+$ , but is it OK to still write $\theta \rightarrow 0$ ?","is a sector with central angle . is the area of the segment bounded by the line and the arc and is the area of the triangle . The ratio If I use L'Hôpital to find , then the answer is . I was wondering if there is any intuition for this limit and if this should be the answer you 'expect'... I originally thought it would be and also not sure why this is wrong. As a smaller note, most books with this example write , but is it OK to still write ?",OPR \theta A(\theta) PR PR B(\theta) PQR \frac{A(\theta)}{B(\theta)} = \frac{r^2(\theta-\sin\theta)}{2\frac{r^2(1-\cos\theta)\sin\theta}{2}} \lim_{\theta \rightarrow 0} \frac{A(\theta)}{B(\theta)} \frac{1}{3} 0 \theta \rightarrow 0^+ \theta \rightarrow 0,"['geometry', 'limits', 'intuition']"
31,"Study convergence of $x_{n+1} = a\left(x_n + {1\over x_n}\right)$, for $x_1 = a$ and $a \in (0, 1)$","Study convergence of , for  and","x_{n+1} = a\left(x_n + {1\over x_n}\right) x_1 = a a \in (0, 1)","Given a recurrence relation: $$ x_{n+1} = a\left(x_n + {1\over x_n}\right) \\ x_1 = a\\ n\in\Bbb N $$ Show that: $$ \begin{align*} a \ge 1 &\implies \lim_{n\to\infty} x_n =+\infty \tag1\\ a \in (0, 1) &\implies \lim_{n\to\infty} x_n =\sqrt{\frac{a}{1-a}} \tag2\\ \end{align*} $$ I think I've been able to prove case $(1)$ but faced difficulties in case $(2)$ . Here is what i've done for $a\ge 1$ . Suppose a limit exists, then it must equal to one of the fixed points: $$ \exists \lim_{n\to\infty}x_n = L \implies L = aL + {a\over L}\\ \iff L = \frac{\pm \sqrt{4a(1-a)}}{2(a-1)} = \mp \sqrt{\frac{a}{1-a}} $$ Since $x_n > 0$ the only possible finite limit is $L = \sqrt{\frac{a}{1-a}}$ . Consider the following expression: $$ x_{n+1} - x_n = a\left(x_n - {1\over x_n}\right) - x_n =\frac{x_n^2(a-1)+a}{x_n} > 0 $$ Therefore $x_n$ in monotonically increasing: $$ x_{n+1} \ge x_n $$ Also for $a\ge1$ : $$ L = \sqrt{\frac{a}{1-a}}\notin \Bbb R $$ Thus by monotonicity of $x_n$ the only possible options left is: $$ \lim_{n\to\infty} x_n = +\infty $$ I've tried to apply a similar reasoning to $(2)$ but unfortunately it lead nowhere. The difficulty is also in the fact that the sequence is not necessarily monotone. Here are two sandboxes i've been using while solving the problem. How can I show that the sequence has a limit when $a\in(0,1)$ ?","Given a recurrence relation: Show that: I think I've been able to prove case but faced difficulties in case . Here is what i've done for . Suppose a limit exists, then it must equal to one of the fixed points: Since the only possible finite limit is . Consider the following expression: Therefore in monotonically increasing: Also for : Thus by monotonicity of the only possible options left is: I've tried to apply a similar reasoning to but unfortunately it lead nowhere. The difficulty is also in the fact that the sequence is not necessarily monotone. Here are two sandboxes i've been using while solving the problem. How can I show that the sequence has a limit when ?","
x_{n+1} = a\left(x_n + {1\over x_n}\right) \\
x_1 = a\\
n\in\Bbb N
 
\begin{align*}
a \ge 1 &\implies \lim_{n\to\infty} x_n =+\infty \tag1\\
a \in (0, 1) &\implies \lim_{n\to\infty} x_n =\sqrt{\frac{a}{1-a}} \tag2\\
\end{align*}
 (1) (2) a\ge 1 
\exists \lim_{n\to\infty}x_n = L \implies L = aL + {a\over L}\\
\iff L = \frac{\pm \sqrt{4a(1-a)}}{2(a-1)} = \mp \sqrt{\frac{a}{1-a}}
 x_n > 0 L = \sqrt{\frac{a}{1-a}} 
x_{n+1} - x_n = a\left(x_n - {1\over x_n}\right) - x_n =\frac{x_n^2(a-1)+a}{x_n} > 0
 x_n 
x_{n+1} \ge x_n
 a\ge1 
L = \sqrt{\frac{a}{1-a}}\notin \Bbb R
 x_n 
\lim_{n\to\infty} x_n = +\infty
 (2) a\in(0,1)","['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
32,"Proving ${u_k}\to u$ given $\lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle$ for $u\in \mathbb{R}^n,\forall v\in \mathbb{R}^n$",Proving  given  for,"{u_k}\to u \lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle u\in \mathbb{R}^n,\forall v\in \mathbb{R}^n","I'm having trouble solving the following problem. Problem. Prove ${u_k}\to u$ given $\lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle$ for $u\in \mathbb{R}^n$ , $\forall v\in \mathbb{R}^n$ The textbook introduces the $i^{\text{th}}$ component function $p_{i} : \mathbb{R}^{n} \to \mathbb{R}$ for $1 \leq i \leq n$ by $p_{i}(u) = u_{i}$ , where $u \in \mathbb{R}^{n}$ . Using this definition, I can express any vector $u \in \mathbb{R}^{n}$ by $u = (p_{1}(u), \ldots, p_{n}(u))$ . I know that this function is linear. The book also tells us that a sequence $\{u_{k}\}$ converges to $u$ in $\mathbb{R}^{n}$ if and only if it converges componentwise (i.e. for each $1 \leq i \leq n$ , $\lim_{k\to\infty} p_{i}(u_{k}) = p_{i}(u)).$ The book provides a hint to define the point $e_{i} \in \mathbb{R}^{n}$ whose $i^{\text{ith}}$ component is equal to $1$ and every other component equals $0$ . This way, $p_{i}(u) = \langle u, e_{i}\rangle$ for each point $u \in \mathbb{R}^{n}$ . I've been working with this component function and don't seem to be making any progress. Any help is appreciated Note: $\langle u,v\rangle$ denotes $u\cdot v$ , u and v are points in $\mathbb{R}^n$ , $u_k$ is a sequence in $\mathbb{R}^n$ . My attempt: Letting $u_k=(u_1^k,u_2^k,...u_n^k)$ with the $k$ representing the $k$ -th term in the sequence $u=(u_1,u_2,...u_n)$ . Then $\lim_{k\to\infty}u_i^k=u_i$ $\forall i$ . $\lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle$ for any given $v\in\mathbb{R}$ , let $v=e_i$ then $\lim_{k\to\infty}\langle u_k,e_i\rangle=\langle u,e_i\rangle$ . Before proceeding we establish $\langle e_i, e_i\rangle=1$ and $\langle e_i,e_j\rangle=0$ assuming $i\neq j$ . \begin{align*} \lim_{k\to\infty} \langle u_k,e_i\rangle &= \Big< \lim_{k\to\infty} u_k, e_i \Big>  = \Big< \lim_{k\to\infty}(u_1^k, u_2^k, \cdots, u_n^k),e_i \Big> \\ &= \Big< \Big( \lim_{k\to\infty}u_1^k, \lim_{k\to\infty}u_2^k, \cdots, \lim_{k\to\infty}u_n^k \Big),e_i \Big>  =\lim_{k\to\infty} u_i^k \end{align*} Now we have $$\lim_{k\to\infty}\langle u_k,e_i\rangle = \lim_{k\to\infty}u_i^k = \langle u,e_i\rangle=\langle (u_1,u_2,...u_n),e_i\rangle = u_i, \ \forall i \quad \Box$$","I'm having trouble solving the following problem. Problem. Prove given for , The textbook introduces the component function for by , where . Using this definition, I can express any vector by . I know that this function is linear. The book also tells us that a sequence converges to in if and only if it converges componentwise (i.e. for each , The book provides a hint to define the point whose component is equal to and every other component equals . This way, for each point . I've been working with this component function and don't seem to be making any progress. Any help is appreciated Note: denotes , u and v are points in , is a sequence in . My attempt: Letting with the representing the -th term in the sequence . Then . for any given , let then . Before proceeding we establish and assuming . Now we have","{u_k}\to u \lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle u\in \mathbb{R}^n \forall v\in \mathbb{R}^n i^{\text{th}} p_{i} : \mathbb{R}^{n} \to \mathbb{R} 1 \leq i \leq n p_{i}(u) = u_{i} u \in \mathbb{R}^{n} u \in \mathbb{R}^{n} u = (p_{1}(u), \ldots, p_{n}(u)) \{u_{k}\} u \mathbb{R}^{n} 1 \leq i \leq n \lim_{k\to\infty} p_{i}(u_{k}) = p_{i}(u)). e_{i} \in \mathbb{R}^{n} i^{\text{ith}} 1 0 p_{i}(u) = \langle u, e_{i}\rangle u \in \mathbb{R}^{n} \langle u,v\rangle u\cdot v \mathbb{R}^n u_k \mathbb{R}^n u_k=(u_1^k,u_2^k,...u_n^k) k k u=(u_1,u_2,...u_n) \lim_{k\to\infty}u_i^k=u_i \forall i \lim_{k\to\infty}\langle u_k,v\rangle=\langle u,v\rangle v\in\mathbb{R} v=e_i \lim_{k\to\infty}\langle u_k,e_i\rangle=\langle u,e_i\rangle \langle e_i, e_i\rangle=1 \langle e_i,e_j\rangle=0 i\neq j \begin{align*}
\lim_{k\to\infty} \langle u_k,e_i\rangle
&= \Big< \lim_{k\to\infty} u_k, e_i \Big>
 = \Big< \lim_{k\to\infty}(u_1^k, u_2^k, \cdots, u_n^k),e_i \Big> \\
&= \Big< \Big( \lim_{k\to\infty}u_1^k, \lim_{k\to\infty}u_2^k, \cdots, \lim_{k\to\infty}u_n^k \Big),e_i \Big>
 =\lim_{k\to\infty} u_i^k
\end{align*} \lim_{k\to\infty}\langle u_k,e_i\rangle
= \lim_{k\to\infty}u_i^k
= \langle u,e_i\rangle=\langle (u_1,u_2,...u_n),e_i\rangle
= u_i, \ \forall i \quad \Box","['real-analysis', 'linear-algebra', 'sequences-and-series', 'limits', 'convergence-divergence']"
33,Is this way of finding $\lim\limits_{x\to +\infty}(x-\ln(x^2+1))$ valid?,Is this way of finding  valid?,\lim\limits_{x\to +\infty}(x-\ln(x^2+1)),"I needed to find: $$\lim\limits_{x\to +\infty}(x-\ln(x^2+1))$$ So here are the steps I took: Step 1: Replace $x$ with $\ln(e^x)$ : $$\lim\limits_{x\to +\infty}\left(\ln(e^x)-\ln(x^2+1)\right)$$ $$\lim\limits_{x\to +\infty}\ln\left(\frac{e^x}{x^2+1}\right)$$ Step 2: Bring the limit inside of the natural log function since it is continuous on the required interval. $$\ln\left(\lim_{x\to +\infty}\frac{e^x}{x^2+1}\right)$$ Step 3: Apply L'Hospital's rule twice and evaluate: $$\ln\left(\lim_{x\to +\infty}e^x\right)$$ $$\ln(+\infty) = +\infty$$ My question is whether step 2 is valid here because $\lim\limits_{x\to \infty}\frac{e^x}{x^2 + 1}$ doesn't exist (its $+\infty$ ), and in order to move the limit operator inside the function the limit $\lim\limits_{x\to \infty}\frac{e^x}{x^2 + 1}$ must exist according to this theorem in a book about Calculus (ISBN 978-0-470-64769-1): If it's not valid, what would be a valid way to find the limit?","I needed to find: So here are the steps I took: Step 1: Replace with : Step 2: Bring the limit inside of the natural log function since it is continuous on the required interval. Step 3: Apply L'Hospital's rule twice and evaluate: My question is whether step 2 is valid here because doesn't exist (its ), and in order to move the limit operator inside the function the limit must exist according to this theorem in a book about Calculus (ISBN 978-0-470-64769-1): If it's not valid, what would be a valid way to find the limit?",\lim\limits_{x\to +\infty}(x-\ln(x^2+1)) x \ln(e^x) \lim\limits_{x\to +\infty}\left(\ln(e^x)-\ln(x^2+1)\right) \lim\limits_{x\to +\infty}\ln\left(\frac{e^x}{x^2+1}\right) \ln\left(\lim_{x\to +\infty}\frac{e^x}{x^2+1}\right) \ln\left(\lim_{x\to +\infty}e^x\right) \ln(+\infty) = +\infty \lim\limits_{x\to \infty}\frac{e^x}{x^2 + 1} +\infty \lim\limits_{x\to \infty}\frac{e^x}{x^2 + 1},"['real-analysis', 'calculus', 'limits', 'continuity']"
34,Proof of a.s. uniqueness of probability convergence limit [closed],Proof of a.s. uniqueness of probability convergence limit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Prove that the limit of a random variable sequence (converges in probability) is almost surely uniuque. Let $X, Y$ random variables and let a random variable sequence $(X_n)_n$ such that $X_n$ converges in probability to $X$ and $X_n$ converges in probability to $Y$ (i.e. $X_n \overset{P}{\rightarrow} X, X_n\overset{P}{\rightarrow} Y$ ). Prove that $\mathbb{P}(X=Y)=1$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Prove that the limit of a random variable sequence (converges in probability) is almost surely uniuque. Let random variables and let a random variable sequence such that converges in probability to and converges in probability to (i.e. ). Prove that .","X, Y (X_n)_n X_n X X_n Y X_n \overset{P}{\rightarrow} X, X_n\overset{P}{\rightarrow} Y \mathbb{P}(X=Y)=1","['probability', 'limits', 'proof-verification']"
35,$\lim \int_1^2 \ln^n x dx$,,\lim \int_1^2 \ln^n x dx,"a) show that $\lim \int_1^2 \ln^n x dx$ goes to $0$ as $n$ goes to $\infty$ b) show that $\lim \int_2^3 \ln^n x dx$ goes to $\infty$ as $n$ goes to $\infty$ a) on $1<x<2$ , $\ln(x) < 1$ , so $ln^n(x)$ goes to $0$ b) on $2<x<e$ ; $\ln(x) < 1$ but in $e\le x \le 3$ , $\ln (x) \ge 1$ so $\lim \int_2^3 \ln^n x dx = \lim (\int_2^e \ln^n x dx + \int_e^3 \ln^n x dx)$ $|\int_2^e \ln^n x dx + \int_e^3 \ln^n x dx|\le |\int_2^e \ln^n x dx| + |\int_e^3 \ln^n x dx|$ Now the problem for me is that  for all $\epsilon > 0$ $|\int_2^e \ln^n x dx| < \epsilon$ , and for all $M > 0$ , $|\int_e^3 \ln^n x dx| > M$ . What should I do next? (Computing integrals is not allowed)","a) show that goes to as goes to b) show that goes to as goes to a) on , , so goes to b) on ; but in , so Now the problem for me is that  for all , and for all , . What should I do next? (Computing integrals is not allowed)",\lim \int_1^2 \ln^n x dx 0 n \infty \lim \int_2^3 \ln^n x dx \infty n \infty 1<x<2 \ln(x) < 1 ln^n(x) 0 2<x<e \ln(x) < 1 e\le x \le 3 \ln (x) \ge 1 \lim \int_2^3 \ln^n x dx = \lim (\int_2^e \ln^n x dx + \int_e^3 \ln^n x dx) |\int_2^e \ln^n x dx + \int_e^3 \ln^n x dx|\le |\int_2^e \ln^n x dx| + |\int_e^3 \ln^n x dx| \epsilon > 0 |\int_2^e \ln^n x dx| < \epsilon M > 0 |\int_e^3 \ln^n x dx| > M,"['integration', 'limits']"
36,Proof verification for $\lim_{n\to\infty}(\sqrt{n^2-1} - \sqrt n) = +\infty$,Proof verification for,\lim_{n\to\infty}(\sqrt{n^2-1} - \sqrt n) = +\infty,"Show that: $$ \lim_{n\to\infty}\left(\sqrt{n^2-1} - \sqrt n\right) = +\infty $$ I've started it this way. Lemma : Let $x_n$ and $y_n$ be two sequences. Claim: If: $$ \begin{cases} &\lim_{n\to\infty} x_n =+\infty \\ &\exists N\in \Bbb N, \ \forall n >N:y_n\ge c > 0 \end{cases} $$ Then: $$ \lim_{n\to\infty}(x_ny_n) = +\infty $$ Proof : $\Box$ Start with definition of limit for this case: $$ \forall\varepsilon>0,\ \exists N_1\in\Bbb N: \forall n > N_1 \implies x_n >\varepsilon $$ Also: $$ \exists N_2\in\Bbb N:\forall n>N_2 \implies y_n \ge c > 0 $$ Let: $$ N = \max\{N_1, N_2\} $$ Then starting from this $N$ we obtain: $$ x_n\cdot y_n > c\cdot \varepsilon $$ And we have that: $$ \forall\varepsilon>0,\ \exists N =\max\{N_1, N_2\}\in\Bbb N: \forall n > N \implies x_n y_n > c\varepsilon $$ Thus: $$ \lim_{n\to\infty}(x_ny_n) = +\infty \ \Box $$ Now back to the initial problem. Let: $$ z_n = \sqrt{n^2-1} - \sqrt n = \frac{n^2 - n - 1}{\sqrt{n^2 - 1} + \sqrt{n}} $$ Define: $$ x_n = n - 1 - {1\over n} \\ y_n = \frac{n}{\sqrt{n^2 - 1} + \sqrt{n}} $$ Obviously $y_n \ge c > 0$ for some $N$ and $n>N$ . Also $x_n \to +\infty$ , then by lemma: $$ \lim_{n\to\infty}z_n = \lim_{n\to\infty}{x_ny_n} = +\infty $$ I know this is a bit overkill, but i wanted to use that exact lemma for the proof. Apart from that, is it valid? BTW here is a visualization for $x_n, y_n$ Update Since it is not clear where the lemma comes from here is the problem from the problem book right before the limit. Let: $$ \lim_{n\to\infty}x_n = a\ , \text{where}\ a = +\infty \ \text{or} \ a = -\infty $$ Prove that if for all $n$ starting from some $N$ $y_n \ge c > 0$ then $$ \lim_{n\to\infty}x_ny_n = a $$ And if for all $n$ starting from some $N$ $y_n \le c < 0$ then $$ \lim_{n\to\infty}x_ny_n = -a $$ No other constraints are given.","Show that: I've started it this way. Lemma : Let and be two sequences. Claim: If: Then: Proof : Start with definition of limit for this case: Also: Let: Then starting from this we obtain: And we have that: Thus: Now back to the initial problem. Let: Define: Obviously for some and . Also , then by lemma: I know this is a bit overkill, but i wanted to use that exact lemma for the proof. Apart from that, is it valid? BTW here is a visualization for Update Since it is not clear where the lemma comes from here is the problem from the problem book right before the limit. Let: Prove that if for all starting from some then And if for all starting from some then No other constraints are given.","
\lim_{n\to\infty}\left(\sqrt{n^2-1} - \sqrt n\right) = +\infty
 x_n y_n 
\begin{cases}
&\lim_{n\to\infty} x_n =+\infty \\
&\exists N\in \Bbb N, \ \forall n >N:y_n\ge c > 0
\end{cases}
 
\lim_{n\to\infty}(x_ny_n) = +\infty
 \Box 
\forall\varepsilon>0,\ \exists N_1\in\Bbb N: \forall n > N_1 \implies x_n >\varepsilon
 
\exists N_2\in\Bbb N:\forall n>N_2 \implies y_n \ge c > 0
 
N = \max\{N_1, N_2\}
 N 
x_n\cdot y_n > c\cdot \varepsilon
 
\forall\varepsilon>0,\ \exists N =\max\{N_1, N_2\}\in\Bbb N: \forall n > N \implies x_n y_n > c\varepsilon
 
\lim_{n\to\infty}(x_ny_n) = +\infty \ \Box
 
z_n = \sqrt{n^2-1} - \sqrt n = \frac{n^2 - n - 1}{\sqrt{n^2 - 1} + \sqrt{n}}
 
x_n = n - 1 - {1\over n} \\
y_n = \frac{n}{\sqrt{n^2 - 1} + \sqrt{n}}
 y_n \ge c > 0 N n>N x_n \to +\infty 
\lim_{n\to\infty}z_n = \lim_{n\to\infty}{x_ny_n} = +\infty
 x_n, y_n 
\lim_{n\to\infty}x_n = a\ , \text{where}\ a = +\infty \ \text{or} \ a = -\infty
 n N y_n \ge c > 0 
\lim_{n\to\infty}x_ny_n = a
 n N y_n \le c < 0 
\lim_{n\to\infty}x_ny_n = -a
","['calculus', 'limits', 'proof-verification', 'epsilon-delta']"
37,$\lim_{x\to 0^+} (\sin x)^ {\tan x}$,,\lim_{x\to 0^+} (\sin x)^ {\tan x},"I have stumbled on this question in one of my problem sets from Cal I and I'm not sure how to proceed after the last step. $$\lim_{x\to 0^+} (\sin x)^ {\tan x}$$ // Applying exponential rule $$x=e^{\ln(x)}$$ $$\lim_{x\to 0^+} exp[\,\ln((\sin x)^ {\tan x})\,]$$ // Using natural logarithm property to bring the exponent to the front $$\lim_{x\to 0^+} exp[\,(\tan x)\ln(\sin x)\,]$$ // Using an algebra trick where : $$x= \frac{1}{\frac{1}{x}}$$ $$\lim_{x\to 0^+} exp\left[\,\frac{\ln(\sin x)}{\frac{1}{\tan x}}\,\right]$$ // After this step, we were taught to check for Hospital's rule, however in this case when you plug the value $0$ in the numerator, you get $\ln(\sin 0)$ which is equal to $\ln(0)$ . This is the part that is confusing me since $\ln(0)$ is undefined. // Could someone please explain to me why  this whole limit is equal to $1$ ? Does it have to do with the fact that $x$ is approaching $0$ from the right side?","I have stumbled on this question in one of my problem sets from Cal I and I'm not sure how to proceed after the last step. // Applying exponential rule // Using natural logarithm property to bring the exponent to the front // Using an algebra trick where : // After this step, we were taught to check for Hospital's rule, however in this case when you plug the value in the numerator, you get which is equal to . This is the part that is confusing me since is undefined. // Could someone please explain to me why  this whole limit is equal to ? Does it have to do with the fact that is approaching from the right side?","\lim_{x\to 0^+} (\sin x)^ {\tan x} x=e^{\ln(x)} \lim_{x\to 0^+} exp[\,\ln((\sin x)^ {\tan x})\,] \lim_{x\to 0^+} exp[\,(\tan x)\ln(\sin x)\,] x= \frac{1}{\frac{1}{x}} \lim_{x\to 0^+} exp\left[\,\frac{\ln(\sin x)}{\frac{1}{\tan x}}\,\right] 0 \ln(\sin 0) \ln(0) \ln(0) 1 x 0",['limits']
38,Find $\lim\limits_{n \to \infty} \sum\limits_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}\left(\frac{n \pi}{k}\right)$,Find,\lim\limits_{n \to \infty} \sum\limits_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}\left(\frac{n \pi}{k}\right),Find $$\lim\limits_{n \to \infty} \sum\limits_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}\left(\frac{n \pi}{k}\right)$$ This is the first time that I am operating with $\lim_{n\to \infty}\lim_{k \to \infty}$ so I am unsure. My first idea would be to look at: $\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k})$ where $n \in \mathbb N$ is constant. $\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k})\leq \frac{1}{k^{2}\sqrt[k]{n}}\leq\frac{1}{k^{2}\sqrt{n}}$ and $\sum_{k=1}^{\infty}\frac{1}{k^{2}\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{k=1}^{\infty}\frac{1}{k^{2}}$ and we know $\sum_{k=1}^{\infty}\frac{1}{k^{2}} < \infty$ and taking $n \to \infty$ we get $\lim_{n\to \infty}\frac{1}{\sqrt{n}}\sum_{k=1}^{\infty}\frac{1}{k^{2}}=0=\lim_{n \to \infty} \sum_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k})$ I assume this is incorrect. Help/Corrections would be greatly appreciated.,Find This is the first time that I am operating with so I am unsure. My first idea would be to look at: where is constant. and and we know and taking we get I assume this is incorrect. Help/Corrections would be greatly appreciated.,\lim\limits_{n \to \infty} \sum\limits_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}\left(\frac{n \pi}{k}\right) \lim_{n\to \infty}\lim_{k \to \infty} \frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k}) n \in \mathbb N \frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k})\leq \frac{1}{k^{2}\sqrt[k]{n}}\leq\frac{1}{k^{2}\sqrt{n}} \sum_{k=1}^{\infty}\frac{1}{k^{2}\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{k=1}^{\infty}\frac{1}{k^{2}} \sum_{k=1}^{\infty}\frac{1}{k^{2}} < \infty n \to \infty \lim_{n\to \infty}\frac{1}{\sqrt{n}}\sum_{k=1}^{\infty}\frac{1}{k^{2}}=0=\lim_{n \to \infty} \sum_{k=1}^{\infty}\frac{1}{k^{2}\sqrt[k]{n}}\sin^{2}(\frac{n \pi}{k}),"['real-analysis', 'sequences-and-series', 'limits']"
39,How to solve the limit $\lim_{k \to \infty} \frac{(2k)!}{2^{2k} (k!)^2}$.,How to solve the limit .,\lim_{k \to \infty} \frac{(2k)!}{2^{2k} (k!)^2},"How to solve this limit?? $$\lim_{k \to \infty} \frac{(2k)!}{2^{2k} (k!)^2}$$ It's a limit, not a series","How to solve this limit?? It's a limit, not a series",\lim_{k \to \infty} \frac{(2k)!}{2^{2k} (k!)^2},"['calculus', 'limits', 'factorial']"
40,Why this random walk can't go on forever?,Why this random walk can't go on forever?,,"$A$ starts with $i$ coins, $B$ with $N-i$ . At each trial, $A$ gives one coin to $B$ with probability $p$ or $B$ gives one coin to $A$ with probability $q$ where $p+q=1$ . This can be modeled as a 2D random walk starting from $i$ where probability of moving right = $p$ , left = $q$ , and the walk ends at reaching either $0$ or $N$ Nothing in this statement seems to say that oscillating around i and never reaching either $0$ or $N$ is not a possibility. However, doing the following calculation, something seems off. Let $p_i$ be the probability that A will end up with all money, that is, the object will reach N, when starting position is $i$ . $$p_i = p*p_{i+1} + q*p_{i-1}$$ Solving this difference equation gives $$p_i = \frac{1-(\frac{q}{p})^i}{1-(\frac{q}{p})^N} $$ for $p\neq q$ Now, P(reaching N starting from $i$ ) = $p_i$ By symmetry, P(reaching 0 starting from $i$ ) = $\frac{1-(\frac{p}{q})^{N-i}}{1-(\frac{p}{q})^N}$ Adding those together equals 1. Which means either A wins or B wins. That is no probability left for just oscillating around $i$ and never reaching either $0$ or $N$ . Why is that so? The probability for the event, going from i to i+1, then back to i, then i+1 and so on = $p*q*p*q*...  = (pq)^n$ where n can go up to infinity. However small, this is a positive number. And unless n goes to infinity, it is greater than zero. I understand that $\lim_{n \to \infty} (pq)^n = 0$ . But how is that applicable here. As $n \to \infty$ , probability $\to 0$ . But $n$ is always less than $\infty$ , so probability is always greater than $0$ . Please tell me if I am wrong with my interpretation of limits. Is it correct that either A or B has to win? Why?","starts with coins, with . At each trial, gives one coin to with probability or gives one coin to with probability where . This can be modeled as a 2D random walk starting from where probability of moving right = , left = , and the walk ends at reaching either or Nothing in this statement seems to say that oscillating around i and never reaching either or is not a possibility. However, doing the following calculation, something seems off. Let be the probability that A will end up with all money, that is, the object will reach N, when starting position is . Solving this difference equation gives for Now, P(reaching N starting from ) = By symmetry, P(reaching 0 starting from ) = Adding those together equals 1. Which means either A wins or B wins. That is no probability left for just oscillating around and never reaching either or . Why is that so? The probability for the event, going from i to i+1, then back to i, then i+1 and so on = where n can go up to infinity. However small, this is a positive number. And unless n goes to infinity, it is greater than zero. I understand that . But how is that applicable here. As , probability . But is always less than , so probability is always greater than . Please tell me if I am wrong with my interpretation of limits. Is it correct that either A or B has to win? Why?",A i B N-i A B p B A q p+q=1 i p q 0 N 0 N p_i i p_i = p*p_{i+1} + q*p_{i-1} p_i = \frac{1-(\frac{q}{p})^i}{1-(\frac{q}{p})^N}  p\neq q i p_i i \frac{1-(\frac{p}{q})^{N-i}}{1-(\frac{p}{q})^N} i 0 N p*q*p*q*...  = (pq)^n \lim_{n \to \infty} (pq)^n = 0 n \to \infty \to 0 n \infty 0,"['probability', 'limits', 'infinity', 'random-walk']"
41,Using the sequential definition of a limit to show $\lim_{x\to 0} \frac{x^2}{x} = 0.$,Using the sequential definition of a limit to show,\lim_{x\to 0} \frac{x^2}{x} = 0.,"I have the following definition for a limit: Definition : Given a function $f : D \rightarrow \mathbb{R}$ and a limit point $x_{0}$ of its domain $D$ , for a number $\ell$ , we write $$ \lim_{x\to x_{0}} f(x) = \ell$$ provided that whenever $\{x_{n}\}$ is a sequence in $D \ - \{x_{0}\}$ that converges to $x_{0}$ , $$\lim_{n\to\infty} f(x_{n}) = \ell. $$ Using this definition, I want to show that $\lim_{x\to 0} x^2/x = 0$ . Here is my attempt: Let $\{x_{n}\}$ be a sequence in $\mathbb{R} - \{0\}$ such that $\{x_{n}\}$ converges to $0$ . This means for all $\epsilon > 0$ , there exists an index $N$ such that $$|x_{n} - 0| < \epsilon $$ for all $n \geq N$ . To prove the original claim, we need to show for all $\epsilon > 0$ , there is an index $N'$ such that $$|\frac{x_{n}^{2}}{x_{n}} - 0| < \epsilon $$ for all $n \geq N'$ . But, note that $$|\frac{x_{n}^{2}}{x_{n}} - 0| = |\frac{x_{n}^{2}}{x_{n}}| = |x_{n}| = |x_{n} - 0|,$$ so setting $N' = N$ suffices. $\blacksquare$ Is my proof correct? Is there anything that can be made better?","I have the following definition for a limit: Definition : Given a function and a limit point of its domain , for a number , we write provided that whenever is a sequence in that converges to , Using this definition, I want to show that . Here is my attempt: Let be a sequence in such that converges to . This means for all , there exists an index such that for all . To prove the original claim, we need to show for all , there is an index such that for all . But, note that so setting suffices. Is my proof correct? Is there anything that can be made better?","f : D \rightarrow \mathbb{R} x_{0} D \ell  \lim_{x\to x_{0}} f(x) = \ell \{x_{n}\} D \ - \{x_{0}\} x_{0} \lim_{n\to\infty} f(x_{n}) = \ell.  \lim_{x\to 0} x^2/x = 0 \{x_{n}\} \mathbb{R} - \{0\} \{x_{n}\} 0 \epsilon > 0 N |x_{n} - 0| < \epsilon  n \geq N \epsilon > 0 N' |\frac{x_{n}^{2}}{x_{n}} - 0| < \epsilon  n \geq N' |\frac{x_{n}^{2}}{x_{n}} - 0| = |\frac{x_{n}^{2}}{x_{n}}| = |x_{n}| = |x_{n} - 0|, N' = N \blacksquare",['real-analysis']
42,Evalutation of the limit $\;\lim\limits_{x \to+ \infty}\left[x - x^2 \cdot \ln\left(1+ \frac 1 x\right)\right]$,Evalutation of the limit,\;\lim\limits_{x \to+ \infty}\left[x - x^2 \cdot \ln\left(1+ \frac 1 x\right)\right],"I was trying to evaluate the limit $$\lim_{x \to+ \infty}\left[x - x^2 \cdot  \ln\left(1+ \frac 1 x\right)\right]$$ without using neither Taylor series nor De L'Hopital rule, but just with notable limits such as $\;\lim\limits_{x \rightarrow 0} \dfrac {e^x - 1} x = 0\;$ or substitution. I tried for a lot of times with different substitutions and notable limits, but I couldn't find any solution. Can you give me some hints. Thanks in advance.","I was trying to evaluate the limit without using neither Taylor series nor De L'Hopital rule, but just with notable limits such as or substitution. I tried for a lot of times with different substitutions and notable limits, but I couldn't find any solution. Can you give me some hints. Thanks in advance.",\lim_{x \to+ \infty}\left[x - x^2 \cdot  \ln\left(1+ \frac 1 x\right)\right] \;\lim\limits_{x \rightarrow 0} \dfrac {e^x - 1} x = 0\;,"['calculus', 'limits', 'limits-without-lhopital']"
43,Find $c$ and $n$ such that $\frac{x^3 \arctan x}{x^4 + \cos x +3} \sim cx^n$ as $x \to 0$,Find  and  such that  as,c n \frac{x^3 \arctan x}{x^4 + \cos x +3} \sim cx^n x \to 0,"Where is my mistake in the below: $$\frac{1}{c} \lim_{x \to 0} \frac{x^3 \arctan x}{x^{4+n} + x^n \cos x + 3x^n} = \frac{1}{c} \lim_{x \to 0} \frac{x^4}{x^{4+n}+x^n \cos x + 3x^n} \\ =\frac{1}{c} \lim_{x \to 0} \frac{1}{x^n + x^{n-4} \cos x + 3x^{n-4}}$$ Looking at it now it feels like it is the step that I am about to do, because it appears as though the denominator becomes smaller and smaller going to infinity. However, my reasoning was that since it is given that there can be an equivalence between $cx^n$ and our function, i.e. the limit is finite and goes to $1$ as $x \to 0$ , then it also must be true that if we invert the fraction, then the limit is still finite and is $1/1=1$ as $x \to 0$ . So, next step: $$c \lim_{x \to 0} \left(x^n + x^{n-4}\cos x + 3x^{n-4} \right)$$ $$c \lim_{x \to 0} (x^{n-4}(\cos x -1))=1$$ $$-c \lim_{x \to 0} \left(x^{n-4} \frac{x^2}{2}\right) = 1$$ We need $x^{n-4} = x^{-2}$ $\implies n=2$ $$-c \frac{1}{2} =1 \implies c=-2$$","Where is my mistake in the below: Looking at it now it feels like it is the step that I am about to do, because it appears as though the denominator becomes smaller and smaller going to infinity. However, my reasoning was that since it is given that there can be an equivalence between and our function, i.e. the limit is finite and goes to as , then it also must be true that if we invert the fraction, then the limit is still finite and is as . So, next step: We need",\frac{1}{c} \lim_{x \to 0} \frac{x^3 \arctan x}{x^{4+n} + x^n \cos x + 3x^n} = \frac{1}{c} \lim_{x \to 0} \frac{x^4}{x^{4+n}+x^n \cos x + 3x^n} \\ =\frac{1}{c} \lim_{x \to 0} \frac{1}{x^n + x^{n-4} \cos x + 3x^{n-4}} cx^n 1 x \to 0 1/1=1 x \to 0 c \lim_{x \to 0} \left(x^n + x^{n-4}\cos x + 3x^{n-4} \right) c \lim_{x \to 0} (x^{n-4}(\cos x -1))=1 -c \lim_{x \to 0} \left(x^{n-4} \frac{x^2}{2}\right) = 1 x^{n-4} = x^{-2} \implies n=2 -c \frac{1}{2} =1 \implies c=-2,"['limits', 'functions']"
44,Evaluate $\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x}$ [closed],Evaluate  [closed],\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have just learned limits and series by myself, but I'm stuck with this limit: $$\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x}$$ I would like to evaluate that limit with $a\in\mathbb R$ . I would also like to understand in detail the steps involved in order to solve that. Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have just learned limits and series by myself, but I'm stuck with this limit: I would like to evaluate that limit with . I would also like to understand in detail the steps involved in order to solve that. Thanks.",\lim_{x\to0}\frac{\sqrt{a^2+x}-|a|}{\sin^a2x\ln\cos x} a\in\mathbb R,"['calculus', 'real-analysis', 'limits']"
45,Inequality on the exponential function,Inequality on the exponential function,,"By playing around, I seem to have come across the following inequality, valid for all $x$ : $$x-(1-e^{-x}) \ge e^{-\frac{2}{x}} x$$ (The constant $2$ is not necessarily the tightest one possible.) Is there an easy way to prove this, and if so, is this inequality known in the literature?  The closest I have been able to come across is $$x-(1-e^{-x}) \ge e^{-\frac{1}{x}} x$$ valid for $x\in[0,1]$ (see e.g., Mond and Paciric, ""Inequalities for exponential functions and means, II"", NAW, 2000. www.nieuwarchief.nl/serie5/pdf/naw5-2000-01-1-057.pdf )","By playing around, I seem to have come across the following inequality, valid for all : (The constant is not necessarily the tightest one possible.) Is there an easy way to prove this, and if so, is this inequality known in the literature?  The closest I have been able to come across is valid for (see e.g., Mond and Paciric, ""Inequalities for exponential functions and means, II"", NAW, 2000. www.nieuwarchief.nl/serie5/pdf/naw5-2000-01-1-057.pdf )","x x-(1-e^{-x}) \ge e^{-\frac{2}{x}} x 2 x-(1-e^{-x}) \ge e^{-\frac{1}{x}} x x\in[0,1]","['real-analysis', 'limits', 'derivatives', 'inequality', 'exponential-function']"
46,Limit Epsilon Delta,Limit Epsilon Delta,,"everybody! I am a new user here. Please correct me if I make any mistakes. Show that for any $\epsilon$ >0 there exists N such that for all n $\geqq$ N it is true that $|x^n - 0|$ < $\epsilon$ x $\in$ (-1,1) $x^n \to$ 0 as n $\to$ ∞ I tried to solve this problem $lim_{x\to ∞}$ $x^n$ = 0 | $x^n$ -0|< $\epsilon$ $x^n$ < $\epsilon$ But, I am not sure how to continue the proof. I also have another question: $x_n$ = $\frac {a^n - b^n}{a - b}$ $(\frac{b}{a})^n$ $\to$ 0 as n $\to$ ∞ Show that for any integer k $\geqq$ 1 $\frac {x_n+_k}{x_n}$ $\to$ $a^k$ as n $\to$ ∞ This is what I did: And I got $lim_{n\to ∞}$ $\frac {a^n{^+}^k - b^n{^+}^k}{a^n - b^n}$ Again, I am stuck as I don't know how to finish it. Can someone please direct me step-by-step? I need to understand this topic well. Thank you very much.","everybody! I am a new user here. Please correct me if I make any mistakes. Show that for any >0 there exists N such that for all n N it is true that < x (-1,1) 0 as n ∞ I tried to solve this problem = 0 | -0|< < But, I am not sure how to continue the proof. I also have another question: = 0 as n ∞ Show that for any integer k 1 as n ∞ This is what I did: And I got Again, I am stuck as I don't know how to finish it. Can someone please direct me step-by-step? I need to understand this topic well. Thank you very much.",\epsilon \geqq |x^n - 0| \epsilon \in x^n \to \to lim_{x\to ∞} x^n x^n \epsilon x^n \epsilon x_n \frac {a^n - b^n}{a - b} (\frac{b}{a})^n \to \to \geqq \frac {x_n+_k}{x_n} \to a^k \to lim_{n\to ∞} \frac {a^n{^+}^k - b^n{^+}^k}{a^n - b^n},"['calculus', 'limits', 'proof-writing']"
47,"Prove $\frac{\sqrt{x}}{x-1}$ is continuous on (0,1)","Prove  is continuous on (0,1)",\frac{\sqrt{x}}{x-1},"Use the epsilon-delta definition to prove that $$ \dfrac{\sqrt{x}}{x-1}  $$ is continuous on the interval $(0,1)$ . I've tried a lot but I just can't seem to see the first step. I don't want the answer just a hint to get me rolling. I know it should be continuous because I looked at the graph, but after $$ |\dfrac{\sqrt{x}}{x-1} - \dfrac{\sqrt{x_0}}{x_0-1}| < \varepsilon $$ I get stuck. I've attempted bringing everything to a common denominator. Subtracting the $x_0$ term to the other side, but to no avail. Any help appreciated","Use the epsilon-delta definition to prove that is continuous on the interval . I've tried a lot but I just can't seem to see the first step. I don't want the answer just a hint to get me rolling. I know it should be continuous because I looked at the graph, but after I get stuck. I've attempted bringing everything to a common denominator. Subtracting the term to the other side, but to no avail. Any help appreciated","
\dfrac{\sqrt{x}}{x-1} 
 (0,1) 
|\dfrac{\sqrt{x}}{x-1} - \dfrac{\sqrt{x_0}}{x_0-1}| < \varepsilon
 x_0","['real-analysis', 'limits', 'analysis', 'continuity', 'epsilon-delta']"
48,Find constant $a$ in way that $\lim_{x\rightarrow -2} \frac{3x^2+ax+a+3}{x^2+x-2}$ has limit,Find constant  in way that  has limit,a \lim_{x\rightarrow -2} \frac{3x^2+ax+a+3}{x^2+x-2},Problem If there exists $a \in \mathbb{R}$ such that: $$ \lim_{x\rightarrow -2} \frac{3x^2+ax+a+3}{x^2+x-2} $$ has limit in $-2$ . If such $a$ exists what is limit in $-2$ ? Attempt to solve My idea was first to try factorize denominator and then find factor of nominator in a way that these two cancel each other. factorizing denominator gives: $$ \lim_{x \rightarrow -2}\frac{3x^2+ax+a+3}{(x-1)(x+2)} $$ Now if it is possible to find solution to $3x^2+ax+a+3=0$ when $x=-2$ $$ 3(-2)^2+a(-2)+(-2)+3=0 $$ $$ 12-2a-2+3=0 $$ $$ 2a=13 \iff a = \frac{13}{2} $$ factorizing nominator gives: $$ 3x^2+\frac{13x}{2}+\frac{19}{2}=0 $$ $$ x= \frac{-\frac{13}{2}\pm \sqrt{(\frac{13}{2})^2-4\cdot 3 \cdot (\frac{19}{2})}}{2\cdot 3} $$ Only problem is this equation is never zero with $x\in \mathbb{R}$ since there is negative value under square root. Now there is contradiction between $a=\frac{13}{2}$ and that equation $3x^2+ax+a+3=0$ when $a=\frac{13}{2}$ and $x=-2$ $$ 3(-2)^2+\frac{13}{2}\cdot (-2)+\frac{13}{2}+3 \neq 0 $$ There is clearly something wrong but i cannot see where this went wrong.,Problem If there exists such that: has limit in . If such exists what is limit in ? Attempt to solve My idea was first to try factorize denominator and then find factor of nominator in a way that these two cancel each other. factorizing denominator gives: Now if it is possible to find solution to when factorizing nominator gives: Only problem is this equation is never zero with since there is negative value under square root. Now there is contradiction between and that equation when and There is clearly something wrong but i cannot see where this went wrong.,a \in \mathbb{R}  \lim_{x\rightarrow -2} \frac{3x^2+ax+a+3}{x^2+x-2}  -2 a -2  \lim_{x \rightarrow -2}\frac{3x^2+ax+a+3}{(x-1)(x+2)}  3x^2+ax+a+3=0 x=-2  3(-2)^2+a(-2)+(-2)+3=0   12-2a-2+3=0   2a=13 \iff a = \frac{13}{2}   3x^2+\frac{13x}{2}+\frac{19}{2}=0   x= \frac{-\frac{13}{2}\pm \sqrt{(\frac{13}{2})^2-4\cdot 3 \cdot (\frac{19}{2})}}{2\cdot 3}  x\in \mathbb{R} a=\frac{13}{2} 3x^2+ax+a+3=0 a=\frac{13}{2} x=-2  3(-2)^2+\frac{13}{2}\cdot (-2)+\frac{13}{2}+3 \neq 0 ,['limits']
49,"If a sum of $L$ positive integers grows like $L^d$, how does the summand grow?","If a sum of  positive integers grows like , how does the summand grow?",L L^d,"Suppose that $(a_N)_{N \in \mathbb{N}}$ is a sequence of (strictly) positive integers which satisfies the following property, namely there exists $C \in (0, \infty)$ and  an integer $d \in \mathbb{N}$, $d > 1$, such that for any $L \in \mathbb{N}$, $$ \sum\limits_{N=0}^{L} a_N \leq C \, \,  L^d. $$ Does this imply that there exists $C^{\prime} \in (0, \infty)$ such that the following inequality is fulfilled for any $N$? $$ a_N \leq C^{\prime} N^{d-1} $$ Of course an obvious bound is $a_N \leq C^{\prime}N^{d}$, but it seems to me that it can be improved.","Suppose that $(a_N)_{N \in \mathbb{N}}$ is a sequence of (strictly) positive integers which satisfies the following property, namely there exists $C \in (0, \infty)$ and  an integer $d \in \mathbb{N}$, $d > 1$, such that for any $L \in \mathbb{N}$, $$ \sum\limits_{N=0}^{L} a_N \leq C \, \,  L^d. $$ Does this imply that there exists $C^{\prime} \in (0, \infty)$ such that the following inequality is fulfilled for any $N$? $$ a_N \leq C^{\prime} N^{d-1} $$ Of course an obvious bound is $a_N \leq C^{\prime}N^{d}$, but it seems to me that it can be improved.",,"['real-analysis', 'sequences-and-series', 'limits', 'power-series']"
50,Ratio of Perimeter^3 to the Area of an Isoceles Triangle.,Ratio of Perimeter^3 to the Area of an Isoceles Triangle.,,"I am in trouble with the following question: QUESTION ABC is an isosceles triangle inscribed in a circle of radius $r$. If $AB=AC$ and $h$ is the altitude from $A$ to $BC$ and $p$ be the perimeter of $ABC$ then find out:  $$\lim_{h\to0} \frac{\Delta}{p^3}$$ (where $\Delta$ is the area of triangle ) MY ATTEMPT First, let us try to understand what's happening during the limiting process $h\to0$: The base $BC$ is translated vertically upwards to the apex $A$, all the while staying horizontal. If I describe the various quantities playing a role here in terms of the half angle $\alpha$ at $A$ (which tends to ${\pi\over2}$ in the process). Then I obtained an expression $${\Delta \over p^3}=\Psi(\alpha)\ ,$$ and now I have to determine $\lim_{\alpha\nearrow{\pi\over2}}\Psi(\alpha)$, whereby $r$ is considered constant and $\Psi(\alpha)$ is a function of half-angle. HENCE applying trigonometric identities will yield: $$\lim_{\alpha\to\frac{\pi}{2}}  \frac{h^2\tan\frac{\alpha}{2}}{[2h(\sec\frac{\alpha}{2}+\tan\frac{\alpha}{2})]^3}=\frac{h^2}{8h^3} \lim_{\alpha\to\frac{\pi}{2}}\frac{\tan\frac{\alpha}{2}}{[\sec\frac{\alpha}{2}+\tan\frac{\alpha}{2}]^3}$$ $$=\frac{1}{8h}\frac{1}{(\sqrt{2}+1)^3}=\frac{1}{h(56+40\sqrt{2})}$$ at $\lim_{h\to0}$ i can replace h by r (from figure h=AO=radius of circle) HELP Please let me know whether it is the correct answer or not. I am kind of worried as the answer given in my book is $\frac{1}{128r}$. PLEASE EXPLAIN HOW TO GET THIS ANSWER *I am sorry for any kind of mistake, I am in high school and this is my first week on MSE.","I am in trouble with the following question: QUESTION ABC is an isosceles triangle inscribed in a circle of radius $r$. If $AB=AC$ and $h$ is the altitude from $A$ to $BC$ and $p$ be the perimeter of $ABC$ then find out:  $$\lim_{h\to0} \frac{\Delta}{p^3}$$ (where $\Delta$ is the area of triangle ) MY ATTEMPT First, let us try to understand what's happening during the limiting process $h\to0$: The base $BC$ is translated vertically upwards to the apex $A$, all the while staying horizontal. If I describe the various quantities playing a role here in terms of the half angle $\alpha$ at $A$ (which tends to ${\pi\over2}$ in the process). Then I obtained an expression $${\Delta \over p^3}=\Psi(\alpha)\ ,$$ and now I have to determine $\lim_{\alpha\nearrow{\pi\over2}}\Psi(\alpha)$, whereby $r$ is considered constant and $\Psi(\alpha)$ is a function of half-angle. HENCE applying trigonometric identities will yield: $$\lim_{\alpha\to\frac{\pi}{2}}  \frac{h^2\tan\frac{\alpha}{2}}{[2h(\sec\frac{\alpha}{2}+\tan\frac{\alpha}{2})]^3}=\frac{h^2}{8h^3} \lim_{\alpha\to\frac{\pi}{2}}\frac{\tan\frac{\alpha}{2}}{[\sec\frac{\alpha}{2}+\tan\frac{\alpha}{2}]^3}$$ $$=\frac{1}{8h}\frac{1}{(\sqrt{2}+1)^3}=\frac{1}{h(56+40\sqrt{2})}$$ at $\lim_{h\to0}$ i can replace h by r (from figure h=AO=radius of circle) HELP Please let me know whether it is the correct answer or not. I am kind of worried as the answer given in my book is $\frac{1}{128r}$. PLEASE EXPLAIN HOW TO GET THIS ANSWER *I am sorry for any kind of mistake, I am in high school and this is my first week on MSE.",,"['calculus', 'geometry', 'limits', 'circles', 'triangles']"
51,Please help me with this trigonometric limit without using L'Hopital's rule,Please help me with this trigonometric limit without using L'Hopital's rule,,"I need to solve the following limit without using L'Hopital's rule: $$\lim _{x\to 0}\left(1+\sin\left(x\right)\right)^{\frac{1}{x}}$$ The thing is that I can not figure out what to do. One of my ideas was to apply this rule: $a^x=e^{\ln \left(a^x\right)}=e^{x\cdot \ln \left(a\right)}$, getting this: $$\lim _{x\to 0}e^{\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)}$$ I already know that the answer is $e$, so the exponent is definitely 1. However, I tried everything I could but have no idea how to solve $\lim _{x\to 0}\left(\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)\right)$ which needs to be 1. I would  really appreciate your help, and if you find a totally different way to solve the limit without using L'Hospital's rule it will be good as well.","I need to solve the following limit without using L'Hopital's rule: $$\lim _{x\to 0}\left(1+\sin\left(x\right)\right)^{\frac{1}{x}}$$ The thing is that I can not figure out what to do. One of my ideas was to apply this rule: $a^x=e^{\ln \left(a^x\right)}=e^{x\cdot \ln \left(a\right)}$, getting this: $$\lim _{x\to 0}e^{\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)}$$ I already know that the answer is $e$, so the exponent is definitely 1. However, I tried everything I could but have no idea how to solve $\lim _{x\to 0}\left(\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)\right)$ which needs to be 1. I would  really appreciate your help, and if you find a totally different way to solve the limit without using L'Hospital's rule it will be good as well.",,"['limits', 'trigonometry', 'limits-without-lhopital']"
52,Prove by induction $|u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \epsilon$,Prove by induction,|u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \epsilon,"Specifically, show that $\forall u \in \mathbb{R}$, $\forall n \in \mathbb{N}$, $\forall \epsilon > 0$, $\exists \delta > 0$ such that $\forall y \in \mathbb{R} \quad |u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \epsilon$ I am trying to see if I am understanding this correctly. I have written out a proof in as much detail as I think is necessary.  I feel shaky when it comes to using the $P(n)$ case to help prove $P(n+1)$. Proof. Base case n=1 is trivial.  We will only proceed with the inductive step n+1. For convenience suppose $|u| > |y|$.  We expand $P(n+1)$ \begin{align*} |u^{n+1} - y^{n+1}| & = |(u-y)(u^{n} + u^{n-1}y + ... + y^{n})| \\ & \leq |u-y|(|u^{n}| + |u^{n-1}y| + ... + |y^{n}|) \\ & \leq |u-y| \cdot (n+1) \cdot \max(\{|u|,|y|\})^{n} \\ & < \delta \cdot (n+1) \cdot |u|^{n} \end{align*} For $P(n)$ the statement holds true so lets first expand on this case to be able to proceed with case $P(n+1)$. \begin{align*} |u^{n} - y^{n}| & = |(u-y)(u^{n-1} + u^{n-2}y + ... + y^{n-1})| \\ & \leq |u-y|(|u^{n-1}| + |u^{n-2}y| + ... + |y^{n-1}|) \\ & \leq |u-y| \cdot n \cdot \max(\{|u|,|y|\})^{n-1} \\ & < \delta \cdot n \cdot |u|^{n-1} \\ & < \delta \cdot (n+1) \cdot |u|^{n-1} \end{align*} Our choice of delta will depend on the value of $|u|$ and $\epsilon$.  If $|u| \leq 1 \Rightarrow |u|^{n} \leq 1$ for any $n \in \mathbb{N}$ (it does not matter if it is $n-1$ or $n$), such that \begin{align*} \delta \cdot (n+1) \cdot |u|^{n} \leq \delta \cdot (n+1) \cdot 1 \end{align*} and so we pick $\delta = \frac{\epsilon}{n+1}$.  If $|u| > 1 \Rightarrow |u|^{n} > |u|^{n-1}$ such that \begin{align*} \delta \cdot (n+1) \cdot |u|^{n-1} < \delta \cdot (n+1) \cdot |u|^{n} \end{align*} and so we pick $\delta = \frac{\epsilon}{(n+1) \cdot |u|^{n}}$. In any of the two cases we can choose a delta such that. \begin{align*} |u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \delta \cdot (n+1) \cdot |u|^{n} = \epsilon \end{align*} a statement that holds true since $P(n)$ holds true. Having found the necessary delta, we return to $P(n+1)$.  We choose $\delta = \frac{\epsilon}{n+1}$ if $|u| \leq 1$ or $\delta = \frac{\epsilon}{(n+1) \cdot |u|^{n}}$ if $|u| > 1$ such that the following is holds true \begin{align*} |u^{n+1} - y^{n+1}| & < \delta \cdot (n+1) \cdot |u|^{n} = \epsilon \\ |u^{n+1} - y^{n+1}| & < \epsilon \end{align*} Any feedback is appreciated!  Thanks!","Specifically, show that $\forall u \in \mathbb{R}$, $\forall n \in \mathbb{N}$, $\forall \epsilon > 0$, $\exists \delta > 0$ such that $\forall y \in \mathbb{R} \quad |u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \epsilon$ I am trying to see if I am understanding this correctly. I have written out a proof in as much detail as I think is necessary.  I feel shaky when it comes to using the $P(n)$ case to help prove $P(n+1)$. Proof. Base case n=1 is trivial.  We will only proceed with the inductive step n+1. For convenience suppose $|u| > |y|$.  We expand $P(n+1)$ \begin{align*} |u^{n+1} - y^{n+1}| & = |(u-y)(u^{n} + u^{n-1}y + ... + y^{n})| \\ & \leq |u-y|(|u^{n}| + |u^{n-1}y| + ... + |y^{n}|) \\ & \leq |u-y| \cdot (n+1) \cdot \max(\{|u|,|y|\})^{n} \\ & < \delta \cdot (n+1) \cdot |u|^{n} \end{align*} For $P(n)$ the statement holds true so lets first expand on this case to be able to proceed with case $P(n+1)$. \begin{align*} |u^{n} - y^{n}| & = |(u-y)(u^{n-1} + u^{n-2}y + ... + y^{n-1})| \\ & \leq |u-y|(|u^{n-1}| + |u^{n-2}y| + ... + |y^{n-1}|) \\ & \leq |u-y| \cdot n \cdot \max(\{|u|,|y|\})^{n-1} \\ & < \delta \cdot n \cdot |u|^{n-1} \\ & < \delta \cdot (n+1) \cdot |u|^{n-1} \end{align*} Our choice of delta will depend on the value of $|u|$ and $\epsilon$.  If $|u| \leq 1 \Rightarrow |u|^{n} \leq 1$ for any $n \in \mathbb{N}$ (it does not matter if it is $n-1$ or $n$), such that \begin{align*} \delta \cdot (n+1) \cdot |u|^{n} \leq \delta \cdot (n+1) \cdot 1 \end{align*} and so we pick $\delta = \frac{\epsilon}{n+1}$.  If $|u| > 1 \Rightarrow |u|^{n} > |u|^{n-1}$ such that \begin{align*} \delta \cdot (n+1) \cdot |u|^{n-1} < \delta \cdot (n+1) \cdot |u|^{n} \end{align*} and so we pick $\delta = \frac{\epsilon}{(n+1) \cdot |u|^{n}}$. In any of the two cases we can choose a delta such that. \begin{align*} |u - y| < \delta \Rightarrow |u^{n} - y^{n}| < \delta \cdot (n+1) \cdot |u|^{n} = \epsilon \end{align*} a statement that holds true since $P(n)$ holds true. Having found the necessary delta, we return to $P(n+1)$.  We choose $\delta = \frac{\epsilon}{n+1}$ if $|u| \leq 1$ or $\delta = \frac{\epsilon}{(n+1) \cdot |u|^{n}}$ if $|u| > 1$ such that the following is holds true \begin{align*} |u^{n+1} - y^{n+1}| & < \delta \cdot (n+1) \cdot |u|^{n} = \epsilon \\ |u^{n+1} - y^{n+1}| & < \epsilon \end{align*} Any feedback is appreciated!  Thanks!",,"['real-analysis', 'limits', 'induction', 'epsilon-delta']"
53,"Help with a limit, function to the power of a function","Help with a limit, function to the power of a function",,I have the following limit: $$y=\lim_{x\to\infty}       \left(x\ln\left(1+\frac{1}{x}\right)\right)^{x^2\sin(1/x)}$$ From here I do the following: $$\ln(y)=\lim_{x\to\infty}x^2\sin(\frac{1}{x})ln(x\ln(1+\frac{1}{x}))=\lim_{x\to\infty}\frac{\sin(\frac{1}{x})}{\frac{1}{x^2}}\ln(\frac{\ln(1+\frac{1}{x})}{\frac{1}{x}})$$ And from here on I'm stuck with no obvious way to apply L'hopital's rule. Any tips? According to limit calculators $\ln(y)$ should have a value of $-\frac{1}{2}$ and then $y=\frac{1}{\sqrt{e}}$,I have the following limit: $$y=\lim_{x\to\infty}       \left(x\ln\left(1+\frac{1}{x}\right)\right)^{x^2\sin(1/x)}$$ From here I do the following: $$\ln(y)=\lim_{x\to\infty}x^2\sin(\frac{1}{x})ln(x\ln(1+\frac{1}{x}))=\lim_{x\to\infty}\frac{\sin(\frac{1}{x})}{\frac{1}{x^2}}\ln(\frac{\ln(1+\frac{1}{x})}{\frac{1}{x}})$$ And from here on I'm stuck with no obvious way to apply L'hopital's rule. Any tips? According to limit calculators $\ln(y)$ should have a value of $-\frac{1}{2}$ and then $y=\frac{1}{\sqrt{e}}$,,"['calculus', 'limits']"
54,Limits applications in geometric problem,Limits applications in geometric problem,,"We have the following situation The goal is to find $$\lim_{a \to b } \frac{a-b}{c-d} $$ Thought As $a $ tends to $b$ then we see we are gonna have a rectangle which means that $2 \alpha $ is gonna tend to $ \frac{\pi}{2}$ . In other words, $\alpha $ is gonna tend to $\frac{ \pi }{4} $ . Now notice $$ a = \frac{ x }{ \tan \alpha }, b = \frac{ y }{ \tan \alpha} $$ and $$ d = \frac{ y}{\cos \alpha}, c = \frac{x}{\sin \alpha } $$ now $$ \lim_{a-b } \frac{ a-b}{c-d} = \lim_{\alpha \to \frac{\pi}{4} } = \frac{\frac{ x }{ \tan \alpha }-  \frac{ y }{ \tan \alpha}}{\frac{ y}{\cos \alpha}- \frac{x}{\sin \alpha }} = \frac{x-y}{\frac{2}{\sqrt{2}}x - \frac{2}{\sqrt{2}} y } = \frac{ \sqrt{2} }{2}  $$ Is this a correct solution? My","We have the following situation The goal is to find Thought As tends to then we see we are gonna have a rectangle which means that is gonna tend to . In other words, is gonna tend to . Now notice and now Is this a correct solution? My","\lim_{a \to b } \frac{a-b}{c-d}  a  b 2 \alpha   \frac{\pi}{2} \alpha  \frac{ \pi }{4}   a = \frac{ x }{ \tan \alpha }, b = \frac{ y }{ \tan \alpha}   d = \frac{ y}{\cos \alpha}, c = \frac{x}{\sin \alpha }   \lim_{a-b } \frac{ a-b}{c-d} = \lim_{\alpha \to \frac{\pi}{4} } = \frac{\frac{ x }{ \tan \alpha }-  \frac{ y }{ \tan \alpha}}{\frac{ y}{\cos \alpha}- \frac{x}{\sin \alpha }} = \frac{x-y}{\frac{2}{\sqrt{2}}x - \frac{2}{\sqrt{2}} y } = \frac{ \sqrt{2} }{2}  ","['calculus', 'geometry', 'limits']"
55,Calculating Multivariable Limits,Calculating Multivariable Limits,,"I am teaching myself multivariable real analysis from Zorich's Math Analysis II. I am trying to prove that $$f(x)=\begin{cases} x+y\sin(1/x), \text{if } x\neq0\\ 0, \text{if } x=0 \end{cases}$$ satisfies $\lim_{(x,y)\to(0,0)} \; f(x,y)=0$. My proof: Let $\epsilon>0$ and consider the ball of radius epsilon $B_{\epsilon}(0)$. Choose $\delta>0$ such that $\delta<\epsilon/2$.  By norm equivalence, we may consider $0<\|(x,y)\|_{\infty}<\delta$. For all such $(x,y)$, we have $$\|f(x,y)-0\|=\|x+y\sin(1/x)\|\leq\|x\|+\|y\|\|\sin(1/x)\| \leq 2\|(x,y)\|_{\infty} < 2\delta <\epsilon)$$ since $\|\sin(1/x)\|\leq 1$ for all $x$. My only problem is justifying why I can use the sup-norm or max-norm instead of the Euclidean or $p=2$ norm. Please verify my proof if possible, as self-study is difficult.","I am teaching myself multivariable real analysis from Zorich's Math Analysis II. I am trying to prove that $$f(x)=\begin{cases} x+y\sin(1/x), \text{if } x\neq0\\ 0, \text{if } x=0 \end{cases}$$ satisfies $\lim_{(x,y)\to(0,0)} \; f(x,y)=0$. My proof: Let $\epsilon>0$ and consider the ball of radius epsilon $B_{\epsilon}(0)$. Choose $\delta>0$ such that $\delta<\epsilon/2$.  By norm equivalence, we may consider $0<\|(x,y)\|_{\infty}<\delta$. For all such $(x,y)$, we have $$\|f(x,y)-0\|=\|x+y\sin(1/x)\|\leq\|x\|+\|y\|\|\sin(1/x)\| \leq 2\|(x,y)\|_{\infty} < 2\delta <\epsilon)$$ since $\|\sin(1/x)\|\leq 1$ for all $x$. My only problem is justifying why I can use the sup-norm or max-norm instead of the Euclidean or $p=2$ norm. Please verify my proof if possible, as self-study is difficult.",,"['real-analysis', 'limits', 'multivariable-calculus', 'normed-spaces']"
56,Is there a proof for $\lim_{x \to a} \frac{1}{x-a} = \infty$?,Is there a proof for ?,\lim_{x \to a} \frac{1}{x-a} = \infty,"I am an adult software developer who is trying to do a math reboot. I am working through the exercises in the following book. Ayres, Frank , Jr. and Elliott Mendelson. 2013. Schaum's Outlines Calculus Sixth Edition (1,105 fully solved problems, 30 problem-solving videos online) . New York: McGraw Hill. ISBN 978-0-07-179553-1. So far as I can tell, the following question either has a misprint or the book does not cover the material.  It is entirely possible that I failed to grasp a key important sentence. Chapter 7 Limits, problem 24. Use the precise definition to prove: $$ \text{a)  }\lim_{x \to 0} \frac{1}{x} = \infty \\ \text{b)  }\lim_{x \to 1} \frac{x}{x-1} = \infty \\ $$ My understanding. It is possible to prove $\lim_{x \to 0^+} \frac{1}{x} = +\infty$ or $\lim_{x \to 0^-} \frac{1}{x} = -\infty$, but not $\lim_{x \to 0} \frac{1}{x} = \infty$ because $\frac{1}{x}$ is a hyperbola with no limit at 0.  A similar argument can be made for $\frac{x}{x-1}$ at 1. Is there a proof for $\lim_{x \to a} \frac{1}{x-a} = \infty$?","I am an adult software developer who is trying to do a math reboot. I am working through the exercises in the following book. Ayres, Frank , Jr. and Elliott Mendelson. 2013. Schaum's Outlines Calculus Sixth Edition (1,105 fully solved problems, 30 problem-solving videos online) . New York: McGraw Hill. ISBN 978-0-07-179553-1. So far as I can tell, the following question either has a misprint or the book does not cover the material.  It is entirely possible that I failed to grasp a key important sentence. Chapter 7 Limits, problem 24. Use the precise definition to prove: $$ \text{a)  }\lim_{x \to 0} \frac{1}{x} = \infty \\ \text{b)  }\lim_{x \to 1} \frac{x}{x-1} = \infty \\ $$ My understanding. It is possible to prove $\lim_{x \to 0^+} \frac{1}{x} = +\infty$ or $\lim_{x \to 0^-} \frac{1}{x} = -\infty$, but not $\lim_{x \to 0} \frac{1}{x} = \infty$ because $\frac{1}{x}$ is a hyperbola with no limit at 0.  A similar argument can be made for $\frac{x}{x-1}$ at 1. Is there a proof for $\lim_{x \to a} \frac{1}{x-a} = \infty$?",,"['calculus', 'limits']"
57,How is this proof for the scalar product rule of limits valid?,How is this proof for the scalar product rule of limits valid?,,"If we let $K=\lim\limits_{x \to a} f(x),$ and let $c$ be a constant, Then in order to show that $\lim\limits_{x \to a} cf(x) = cK$, we must show that there is an $\epsilon$ for every $\delta$ such that $\lvert cf(x)-cK \rvert < \epsilon$ whenever $\lvert x-a \rvert < \delta$. The proof claims to prove this by staing: $$\lvert cf(x)-cK \rvert = \lvert c \rvert \lvert f(x)-K \rvert < \lvert c \rvert  \frac{\epsilon}{\lvert c \rvert} = \epsilon.$$ However, I don't see how this proves anything other than basic manipulation of terms, and I don't see how it relates $\epsilon$ to $\delta$ in any way.","If we let $K=\lim\limits_{x \to a} f(x),$ and let $c$ be a constant, Then in order to show that $\lim\limits_{x \to a} cf(x) = cK$, we must show that there is an $\epsilon$ for every $\delta$ such that $\lvert cf(x)-cK \rvert < \epsilon$ whenever $\lvert x-a \rvert < \delta$. The proof claims to prove this by staing: $$\lvert cf(x)-cK \rvert = \lvert c \rvert \lvert f(x)-K \rvert < \lvert c \rvert  \frac{\epsilon}{\lvert c \rvert} = \epsilon.$$ However, I don't see how this proves anything other than basic manipulation of terms, and I don't see how it relates $\epsilon$ to $\delta$ in any way.",,"['limits', 'proof-explanation']"
58,Proving $\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) $ doesn't exist.,Proving  doesn't exist.,\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) ,"How can I show that $$\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) $$ doesn't exist? I used the fact that $$\arctan{x}\ge x-\frac{x^3} {3}, $$ so the initial limit is less than $$\lim_{x\to\infty} \frac{x^5}{3} +O(x^4),$$ therefore the limit tends to infinity. Is this enough? If not, then how can I show this rigorously?","How can I show that $$\lim_{x\to\infty} (x^2 +1)(\frac{\pi}{2} - \arctan{x}) $$ doesn't exist? I used the fact that $$\arctan{x}\ge x-\frac{x^3} {3}, $$ so the initial limit is less than $$\lim_{x\to\infty} \frac{x^5}{3} +O(x^4),$$ therefore the limit tends to infinity. Is this enough? If not, then how can I show this rigorously?",,[]
59,Why is $\lim\frac{x^2\sqrt{1-x}-18}{(x^2-9)\log(x+4)}$ different for $x\to -3^{+}$ and $x\to -3^{-}$?,Why is  different for  and ?,\lim\frac{x^2\sqrt{1-x}-18}{(x^2-9)\log(x+4)} x\to -3^{+} x\to -3^{-},"$$\lim_{x\to-3}\frac{x^2\sqrt{1-x}-18}{(x^2-9)\log(x+4)}$$ I used Taylor expansion of $\sqrt{1-x}$ and $\log(x+4)$ centered in $x=-3$: $$\lim_{x\to-3}\frac{x^2(2)-18}{(x^2-9)(x+3)}=\lim_{x\to-3}2\frac{x^2-9}{(x^2-9)(x+3)}=\frac2{x+3}=+\infty$$ I thought that the two-sided limit $=+\infty$, but Wolfram Alpha gives: $$\lim_{x\to-3^-}f(x)=-\infty,\qquad \lim_{x\to-3^+}f(x)=+\infty$$ I thought of the following as an explanation for the different values: $$\text{if } x\to-3^-, x=-3-\varepsilon\implies \frac2{-3-\varepsilon+3}=-\frac2{\varepsilon}=-\infty\\\text{if } x\to-3^+, x=-3+\varepsilon\implies \frac2{-3+\varepsilon+3}=+\frac2{\varepsilon}=+\infty$$ where $\varepsilon$ is an infinitesimal. Is my explanation correct?","$$\lim_{x\to-3}\frac{x^2\sqrt{1-x}-18}{(x^2-9)\log(x+4)}$$ I used Taylor expansion of $\sqrt{1-x}$ and $\log(x+4)$ centered in $x=-3$: $$\lim_{x\to-3}\frac{x^2(2)-18}{(x^2-9)(x+3)}=\lim_{x\to-3}2\frac{x^2-9}{(x^2-9)(x+3)}=\frac2{x+3}=+\infty$$ I thought that the two-sided limit $=+\infty$, but Wolfram Alpha gives: $$\lim_{x\to-3^-}f(x)=-\infty,\qquad \lim_{x\to-3^+}f(x)=+\infty$$ I thought of the following as an explanation for the different values: $$\text{if } x\to-3^-, x=-3-\varepsilon\implies \frac2{-3-\varepsilon+3}=-\frac2{\varepsilon}=-\infty\\\text{if } x\to-3^+, x=-3+\varepsilon\implies \frac2{-3+\varepsilon+3}=+\frac2{\varepsilon}=+\infty$$ where $\varepsilon$ is an infinitesimal. Is my explanation correct?",,"['calculus', 'real-analysis', 'limits']"
60,Is there a convex function tending to $\infty$ and bounded above by $(\log x)^2$?,Is there a convex function tending to  and bounded above by ?,\infty (\log x)^2,"I know that there is no convex function $f$ on $(1,\infty)$ such that $f(x) \rightarrow \infty $ as $x \rightarrow \infty$ and at the same time $f(x)<\log x$ for all $x \in (1,\infty)$ because convex function is a supremum of some affine functions. What I am wondering is: Can we extend this result to powers of logarithms? In particular, is there a convex function $g$ on $(1,\infty)$ such that $g(x) \rightarrow \infty$ as $x \rightarrow \infty$ and $g<\log ^2 $? Any hint would be really appreciated! Thanks and regards.","I know that there is no convex function $f$ on $(1,\infty)$ such that $f(x) \rightarrow \infty $ as $x \rightarrow \infty$ and at the same time $f(x)<\log x$ for all $x \in (1,\infty)$ because convex function is a supremum of some affine functions. What I am wondering is: Can we extend this result to powers of logarithms? In particular, is there a convex function $g$ on $(1,\infty)$ such that $g(x) \rightarrow \infty$ as $x \rightarrow \infty$ and $g<\log ^2 $? Any hint would be really appreciated! Thanks and regards.",,"['real-analysis', 'limits', 'logarithms', 'asymptotics', 'convex-analysis']"
61,A problem of limit: $\lim\limits_{n\to \infty}\left\{\frac{(n+1)^{n}}{n!} \right\}^{(1/n)}$ [duplicate],A problem of limit:  [duplicate],\lim\limits_{n\to \infty}\left\{\frac{(n+1)^{n}}{n!} \right\}^{(1/n)},"This question already has answers here : Why does $\;\lim_{n\to \infty }\frac{n}{n!^{1/n}}=e$? (3 answers) Closed 6 years ago . I need to find the limit of $$\lim\limits_{n\to \infty}\left\{\left(\frac{2}1\right) \left(\frac{3}2\right)^{2} \left(\frac{4}3\right)^{3} ... \left(\frac{n+1}n\right)^{n} \right\}^{(1/n)}$$ Cancelling out the same quantities in numerators and denominators, I have reached the step: $$\lim\limits_{n\to \infty}\left\{\frac{(n+1)^{n}}{n!} \right\}^{\frac1n}$$ Now I am stuck here. I have searched google for help, but found only the result for $\lim\limits_{n\to\infty}\left\{\frac{1}{n!} \right\}^{(\frac1n)}$. Which formulae/properties should I use now to proceed from this stage? Edit: The options for the answer are: a) $e$, b) $1/e$, c) $\pi$, d) $1/\pi$","This question already has answers here : Why does $\;\lim_{n\to \infty }\frac{n}{n!^{1/n}}=e$? (3 answers) Closed 6 years ago . I need to find the limit of $$\lim\limits_{n\to \infty}\left\{\left(\frac{2}1\right) \left(\frac{3}2\right)^{2} \left(\frac{4}3\right)^{3} ... \left(\frac{n+1}n\right)^{n} \right\}^{(1/n)}$$ Cancelling out the same quantities in numerators and denominators, I have reached the step: $$\lim\limits_{n\to \infty}\left\{\frac{(n+1)^{n}}{n!} \right\}^{\frac1n}$$ Now I am stuck here. I have searched google for help, but found only the result for $\lim\limits_{n\to\infty}\left\{\frac{1}{n!} \right\}^{(\frac1n)}$. Which formulae/properties should I use now to proceed from this stage? Edit: The options for the answer are: a) $e$, b) $1/e$, c) $\pi$, d) $1/\pi$",,"['real-analysis', 'limits']"
62,Applying the ratio test and uniform convergence,Applying the ratio test and uniform convergence,,"I have been trying to apply the ratio test onto $\dfrac{z^n}{1+z^n}$. After the usual initial steps. I need to show that $$\lim_{n \to \infty} \left|\dfrac{z(1+z^n)}{1+z^{n+1}}\right|<1$$ I am unsure of how to make further progress, and so what is the trick from here? Once this is shown, we can therefore say that the series  $$\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$$ converges absolutely. Before I can deduce $\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$ is holomorphic on $D=\{z \in \mathbb{C} \mid |z| <1\},$ would I have to show uniform convergence? If so, what is the simplest way?","I have been trying to apply the ratio test onto $\dfrac{z^n}{1+z^n}$. After the usual initial steps. I need to show that $$\lim_{n \to \infty} \left|\dfrac{z(1+z^n)}{1+z^{n+1}}\right|<1$$ I am unsure of how to make further progress, and so what is the trick from here? Once this is shown, we can therefore say that the series  $$\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$$ converges absolutely. Before I can deduce $\displaystyle\sum\limits^\infty_{n=0} \dfrac{z^n}{1+z^n}$ is holomorphic on $D=\{z \in \mathbb{C} \mid |z| <1\},$ would I have to show uniform convergence? If so, what is the simplest way?",,"['calculus', 'limits', 'convergence-divergence', 'summation', 'uniform-convergence']"
63,Find $ \lim_{ \epsilon \to 0} \frac{1}{R^n-(R-\epsilon)^n} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx$,Find, \lim_{ \epsilon \to 0} \frac{1}{R^n-(R-\epsilon)^n} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx,"How to find the following limit \begin{align}  \lim_{ \epsilon \to 0} \frac{1}{R^n-(R-\epsilon)^n} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx. \end{align} For the case of $n=1, $ I was able to compute this and the limit is given by  $e^{-\frac{(\mu -R)^2}{2}}+e^{-\frac{(\mu +R)^2}{2}}$. However, not sure how to do it in general. Reasoning that I had (which is wrong) Intuitively for a very small epsilon  \begin{align} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx&= e^{-\frac{\|R-\mu\|^2}{2}}  \int_{R-\epsilon\le \|x\| \le R}  dx\\ &= e^{-\frac{\|R-\mu\|^2}{2}}  V_n  (R^n-(R-\epsilon)^n), \end{align} where $V_n$ is the volume of a unit ball.  So, my guess for the limit was \begin{align} e^{-\frac{\|R-\mu\|^2}{2}}  V_n . \end{align} However, this does not appear to be correct as it does not agree with the $n=1$ case.  As Did pointed out my assumption is not correct. This question looks possibly related to  \begin{align} \lim_{\epsilon \to 0} \frac{1}{Vol(B_c(\epsilon))} \int_{B_c(\epsilon)} f(x) dx , \end{align} where  $B_c(\epsilon)$ is a ball of radius $\epsilon$ centered at $c$. The difference is that we are looking at the annulus instead of a ball.","How to find the following limit \begin{align}  \lim_{ \epsilon \to 0} \frac{1}{R^n-(R-\epsilon)^n} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx. \end{align} For the case of $n=1, $ I was able to compute this and the limit is given by  $e^{-\frac{(\mu -R)^2}{2}}+e^{-\frac{(\mu +R)^2}{2}}$. However, not sure how to do it in general. Reasoning that I had (which is wrong) Intuitively for a very small epsilon  \begin{align} \int_{R-\epsilon\le \|x\| \le R} e^{-\frac{\|x-\mu\|^2}{2}} dx&= e^{-\frac{\|R-\mu\|^2}{2}}  \int_{R-\epsilon\le \|x\| \le R}  dx\\ &= e^{-\frac{\|R-\mu\|^2}{2}}  V_n  (R^n-(R-\epsilon)^n), \end{align} where $V_n$ is the volume of a unit ball.  So, my guess for the limit was \begin{align} e^{-\frac{\|R-\mu\|^2}{2}}  V_n . \end{align} However, this does not appear to be correct as it does not agree with the $n=1$ case.  As Did pointed out my assumption is not correct. This question looks possibly related to  \begin{align} \lim_{\epsilon \to 0} \frac{1}{Vol(B_c(\epsilon))} \int_{B_c(\epsilon)} f(x) dx , \end{align} where  $B_c(\epsilon)$ is a ball of radius $\epsilon$ centered at $c$. The difference is that we are looking at the annulus instead of a ball.",,"['real-analysis', 'integration', 'limits']"
64,"If $a_{n+1}+1=(na_n+1)^{\frac{1}{n}}$, prove that $a_{n+1}/a_n \to 1$ and $na_n \to 0$","If , prove that  and",a_{n+1}+1=(na_n+1)^{\frac{1}{n}} a_{n+1}/a_n \to 1 na_n \to 0,"Let $(a_n)_{\geq 2}$ with $a_2 > 0$ and $$a_{n+1}+1=(na_n+1)^{\frac{1}{n}}$$   Prove that $\displaystyle\frac{a_{n+1}}{a_n}\to 1$ and $na_n \to 0$, eventually using the inequality $$\ln(1+x)>\frac{x}{x+1}, \: \forall x>0$$ I managed to prove that $a_n \to 0$. Using Bernoulli's inequality we get that $(a_n)$ is decreasing and it is obviously bounded by 0. If it were convergent to $l \neq 0$, we yould have  $$\lim_{n \to \infty}\frac{\ln(na_n+1)}{n}=\lim_{n \to \infty}\frac{\ln(na_n+1)}{na_n}\cdot a_n=0\cdot l=0$$ since $na_n \to \infty$ and so $\lim_{n \to \infty}(na_n+1)^{\frac{1}{n}}=1$ hence $$l+1=\lim_{n \to \infty}(a_{n+1}+1)=\lim_{n \to \infty}(na_n+1)^{\frac{1}{n}}=1$$ which is a contradiction, so $l=0$. This is where I got stuck. I don't know how to approach either of these two limits, but I'm pretty sure that their hint with the $\ln(x+1)$ inequality must be used somehow.","Let $(a_n)_{\geq 2}$ with $a_2 > 0$ and $$a_{n+1}+1=(na_n+1)^{\frac{1}{n}}$$   Prove that $\displaystyle\frac{a_{n+1}}{a_n}\to 1$ and $na_n \to 0$, eventually using the inequality $$\ln(1+x)>\frac{x}{x+1}, \: \forall x>0$$ I managed to prove that $a_n \to 0$. Using Bernoulli's inequality we get that $(a_n)$ is decreasing and it is obviously bounded by 0. If it were convergent to $l \neq 0$, we yould have  $$\lim_{n \to \infty}\frac{\ln(na_n+1)}{n}=\lim_{n \to \infty}\frac{\ln(na_n+1)}{na_n}\cdot a_n=0\cdot l=0$$ since $na_n \to \infty$ and so $\lim_{n \to \infty}(na_n+1)^{\frac{1}{n}}=1$ hence $$l+1=\lim_{n \to \infty}(a_{n+1}+1)=\lim_{n \to \infty}(na_n+1)^{\frac{1}{n}}=1$$ which is a contradiction, so $l=0$. This is where I got stuck. I don't know how to approach either of these two limits, but I'm pretty sure that their hint with the $\ln(x+1)$ inequality must be used somehow.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
65,If $u_n = (1+\frac{1}{n^2})\ldots(1+\frac{n}{n^2})$ then $\lim u_n=\sqrt{e}$? [duplicate],If  then ? [duplicate],u_n = (1+\frac{1}{n^2})\ldots(1+\frac{n}{n^2}) \lim u_n=\sqrt{e},"This question already has answers here : How to calculate $\lim_{n\to\infty}(1+1/n^2)(1+2/n^2)\cdots(1+n/n^2)$? (4 answers) Closed 4 years ago . I'm solving an exercise and I'm asked to prove that if $$u_n = \left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\left(1+\frac{3}{n^2}\right)\ldots\left(1+\frac{n}{n^2}\right)$$ then $\lim u_n = \sqrt{e}$. This is after it asks me to show that $\forall x>0,$ $$x-\frac{1}{2}x^2<\ln(1+x)<x$$ which I did, but even with this result I'm not being able to even get how to use it to show $\lim u_n = \sqrt{e}$.","This question already has answers here : How to calculate $\lim_{n\to\infty}(1+1/n^2)(1+2/n^2)\cdots(1+n/n^2)$? (4 answers) Closed 4 years ago . I'm solving an exercise and I'm asked to prove that if $$u_n = \left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\left(1+\frac{3}{n^2}\right)\ldots\left(1+\frac{n}{n^2}\right)$$ then $\lim u_n = \sqrt{e}$. This is after it asks me to show that $\forall x>0,$ $$x-\frac{1}{2}x^2<\ln(1+x)<x$$ which I did, but even with this result I'm not being able to even get how to use it to show $\lim u_n = \sqrt{e}$.",,['limits']
66,Limit at odd integer for $x$,Limit at odd integer for,x,"$$\lim_{x\to a}\frac{1}{(a^2-x^2)^2}\cdot\left(\frac{a^2+x^2}{ax}-2\sin\frac{a\pi}{2}\sin\frac{\pi x}2\right)=?$$ if $a$ is an odd integer. The way I set out is first assuming $a=1$ and seeing if I can spot some pattern. Now, if I rewrite this limit out as: $$\lim_{x\to 1}\frac{1}{(1-x^2)^2}\cdot\left(\frac{1+x^2}{x}-2\sin\frac{\pi x}2\right)=?$$ Now, you can clearly see that the denominator is of the form $0^4$ (due to the $(1-x^2)^2$), whereas the numerator is, at max, $0^2$ (due to the $x^2$). Since denominator has a higher power of zero than the numerator, I believe that the limit won't exist. However, my textbook says this limit exists and has a finite value. So, I wish to ask what is the fault in my reasoning.","$$\lim_{x\to a}\frac{1}{(a^2-x^2)^2}\cdot\left(\frac{a^2+x^2}{ax}-2\sin\frac{a\pi}{2}\sin\frac{\pi x}2\right)=?$$ if $a$ is an odd integer. The way I set out is first assuming $a=1$ and seeing if I can spot some pattern. Now, if I rewrite this limit out as: $$\lim_{x\to 1}\frac{1}{(1-x^2)^2}\cdot\left(\frac{1+x^2}{x}-2\sin\frac{\pi x}2\right)=?$$ Now, you can clearly see that the denominator is of the form $0^4$ (due to the $(1-x^2)^2$), whereas the numerator is, at max, $0^2$ (due to the $x^2$). Since denominator has a higher power of zero than the numerator, I believe that the limit won't exist. However, my textbook says this limit exists and has a finite value. So, I wish to ask what is the fault in my reasoning.",,['limits']
67,how to find the value of the $k$ to make $\lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A$ exist?,how to find the value of the  to make  exist?,k \lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A,"the question described as follow: $$\lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A$$ the $A$ is constant and $A\not=0$ and find the $k$ to make this limit exist. and I did this: $$let\space t\space be\space x^{1/15}\space, then \space x=t^{15}.$$ $$then \space \lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=\lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}.$$ $$then \space \lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}=\lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}.$$ use taylor expantion: $$then \space \lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}=\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}}{t^{15k}}=\lim_{x\to 0} \frac{1}{5t^{15k-11}}=A.$$ since A is a non-zero constant, so the  $t^{15k-11}$ should be $t^{0}=1$. then we get $15k-11=0$ and finally, we find $k=\frac{11}{15}$. Am I right? suppose I was right. but unfortunately I found the image of $f(x)=\frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{\frac{11}{15}}}$ in quick graph app. the value of $f(0)$ goes to $\infty$ instead any constant. which is wrong, the app or me? if I was wrong, how to find the right k?","the question described as follow: $$\lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A$$ the $A$ is constant and $A\not=0$ and find the $k$ to make this limit exist. and I did this: $$let\space t\space be\space x^{1/15}\space, then \space x=t^{15}.$$ $$then \space \lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=\lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}.$$ $$then \space \lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}=\lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}.$$ use taylor expantion: $$then \space \lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}=\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}}{t^{15k}}=\lim_{x\to 0} \frac{1}{5t^{15k-11}}=A.$$ since A is a non-zero constant, so the  $t^{15k-11}$ should be $t^{0}=1$. then we get $15k-11=0$ and finally, we find $k=\frac{11}{15}$. Am I right? suppose I was right. but unfortunately I found the image of $f(x)=\frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{\frac{11}{15}}}$ in quick graph app. the value of $f(0)$ goes to $\infty$ instead any constant. which is wrong, the app or me? if I was wrong, how to find the right k?",,['limits']
68,Asymptotic behavior of zeroes of $\ln(t)-t+n$,Asymptotic behavior of zeroes of,\ln(t)-t+n,"This is related to this question. Consider the sequence of functions $(f_n)_{n\geq 2}$ defined on $[1,\infty)$ by $$f_n(t)=\ln(t)-t+n.$$ Those functions are decreasing on $[1,\infty)$ with $f_n(1)=n-1$ and $\lim_{t\to \infty}f_n(t)=-\infty$. So by intermediate value theorem, there exists a sequence of zeroes located in $[1,\infty)$, that is, $t_n>1$ and $f_n (t_n)=0$ for all $n\geq2$. I am interested in the asymptotic behavior of the sequence $(t_n)_n$, specially finding an equivalent of this sequence at infinity. Here's what I tried: $\ln(t_n)-t_n+n=0$ and $t_n>1$ is equivalent to  $$t_n=e^{t_n-n}>1$$ which implies that $t_n>n$, so already the sequence diverges to infinity. Now again $$t_n=e^{t_n-n}>n$$ which implies that $$t_n>n+\ln(n)$$ repeating this argument we can find that $$t_n>n+\ln(n+\ln(n+\ln(n+\dots)))$$ I don't know if this path leads to something. I still feel I need an upper bound to find an equivalent.","This is related to this question. Consider the sequence of functions $(f_n)_{n\geq 2}$ defined on $[1,\infty)$ by $$f_n(t)=\ln(t)-t+n.$$ Those functions are decreasing on $[1,\infty)$ with $f_n(1)=n-1$ and $\lim_{t\to \infty}f_n(t)=-\infty$. So by intermediate value theorem, there exists a sequence of zeroes located in $[1,\infty)$, that is, $t_n>1$ and $f_n (t_n)=0$ for all $n\geq2$. I am interested in the asymptotic behavior of the sequence $(t_n)_n$, specially finding an equivalent of this sequence at infinity. Here's what I tried: $\ln(t_n)-t_n+n=0$ and $t_n>1$ is equivalent to  $$t_n=e^{t_n-n}>1$$ which implies that $t_n>n$, so already the sequence diverges to infinity. Now again $$t_n=e^{t_n-n}>n$$ which implies that $$t_n>n+\ln(n)$$ repeating this argument we can find that $$t_n>n+\ln(n+\ln(n+\ln(n+\dots)))$$ I don't know if this path leads to something. I still feel I need an upper bound to find an equivalent.",,"['calculus', 'sequences-and-series', 'limits', 'logarithms', 'asymptotics']"
69,Find $f'(0)$ if $\frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64$,Find  if,f'(0) \frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64,Assume f is a differentiable function with $f(0)=0$ satisfying the equation $$\frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64$$ then find $f'(0)$. This is easy using L'Hopital rule. How can one compute this without using L'Hopital?,Assume f is a differentiable function with $f(0)=0$ satisfying the equation $$\frac{f(4h)+f(2h)+f(h)+f\left(\frac{h}{2}\right)+f\left(\frac{h}{4}\right)+f\left(\frac{h}{8}\right)+\cdots}{h}=64$$ then find $f'(0)$. This is easy using L'Hopital rule. How can one compute this without using L'Hopital?,,"['calculus', 'real-analysis', 'analysis', 'limits', 'limits-without-lhopital']"
70,How to evaluate $\lim_{x\rightarrow 0^-} \sec^{-1}(1+x)$?,How to evaluate ?,\lim_{x\rightarrow 0^-} \sec^{-1}(1+x),"How to do the following $$\lim_{x\rightarrow 0^-} \sec^{-1}(1+x)$$ I am confused that it seems now $(1+x)<1$ when $x\rightarrow 0^-$, which is not valid for the domain of $\sec^{-1}$. Can anyone give me kick on this?","How to do the following $$\lim_{x\rightarrow 0^-} \sec^{-1}(1+x)$$ I am confused that it seems now $(1+x)<1$ when $x\rightarrow 0^-$, which is not valid for the domain of $\sec^{-1}$. Can anyone give me kick on this?",,"['limits', 'trigonometry']"
71,"Continuity of the function $f(x)=\lim\limits_{n \to \infty}[\lim\limits_{t\to 0}[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]] $, $x \in \mathbb R$","Continuity of the function ,",f(x)=\lim\limits_{n \to \infty}[\lim\limits_{t\to 0}[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]]  x \in \mathbb R,"Consider the Question.  I think that f(x)=1 $\forall x \in \mathbb {R}$. Reason: As t $\to$ 0 , $[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]$   $\to[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)}]= 1$. So it is differentiable in $\mathbb R$. But the explanation given is the following. Now my questions are as follows: 1) Am I right? if not, in what way? 2) Whether the explanation given in the picture is correct? 3)Can I interchange Limits? if So under what conditions. Thanks in advance.","Consider the Question.  I think that f(x)=1 $\forall x \in \mathbb {R}$. Reason: As t $\to$ 0 , $[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)+t^2}]$   $\to[\frac{\sin^2 (n!\pi x)}{\sin^2(n! \pi x)}]= 1$. So it is differentiable in $\mathbb R$. But the explanation given is the following. Now my questions are as follows: 1) Am I right? if not, in what way? 2) Whether the explanation given in the picture is correct? 3)Can I interchange Limits? if So under what conditions. Thanks in advance.",,"['real-analysis', 'limits', 'derivatives', 'continuity']"
72,Convergent arithmetic mean implies convergent sub-sequence?,Convergent arithmetic mean implies convergent sub-sequence?,,"We have a non-negative sequence $\{a_n\},~n\in\mathbb{N}$ and its arithmetic mean $\frac{\sum_{i=0}^{N-1}a_i}{N}$ goes to 0 as $N\to\infty$. Can we claim that in this case, there exists a subsequence $\{a_{k_n}\}$ such that $\lim_{n\to\infty}a_{k_n}=0$? I have a proof of yes as follows: We prove it by contradiction. Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$. However, in this case, $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=\epsilon>0$, which contradicts the fact that $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=0$. My question is: “Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$.” Is this statement correct?","We have a non-negative sequence $\{a_n\},~n\in\mathbb{N}$ and its arithmetic mean $\frac{\sum_{i=0}^{N-1}a_i}{N}$ goes to 0 as $N\to\infty$. Can we claim that in this case, there exists a subsequence $\{a_{k_n}\}$ such that $\lim_{n\to\infty}a_{k_n}=0$? I have a proof of yes as follows: We prove it by contradiction. Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$. However, in this case, $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=\epsilon>0$, which contradicts the fact that $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=0$. My question is: “Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$.” Is this statement correct?",,"['real-analysis', 'sequences-and-series', 'limits']"
73,Prove that $\lim_{x \to 2} x^3 = 8$ by using epsilon-delta,Prove that  by using epsilon-delta,\lim_{x \to 2} x^3 = 8,"Prove that $$\lim_{x \to 2} x^3 = 8$$ My attempt, Given $\epsilon>0$, $\exists \space \delta>0$ such that if $$|x^3-8|<\epsilon \space \text{if}  \space 0<|x-2|<\delta$$ $$|(x-2)(x^2+2x+4)|<\epsilon$$ I'm stuck here. Hope someone could continue the solution and explain it for me. Thanks in advance.","Prove that $$\lim_{x \to 2} x^3 = 8$$ My attempt, Given $\epsilon>0$, $\exists \space \delta>0$ such that if $$|x^3-8|<\epsilon \space \text{if}  \space 0<|x-2|<\delta$$ $$|(x-2)(x^2+2x+4)|<\epsilon$$ I'm stuck here. Hope someone could continue the solution and explain it for me. Thanks in advance.",,"['limits', 'proof-verification', 'epsilon-delta']"
74,Does this converge to $\int f \phi '$?,Does this converge to ?,\int f \phi ',"Let $f \in L^1(\mathbb R)$ and $\phi \in C_c^{\infty} (\mathbb R)$. I am pretty certain that the following is true: $$\lim_{h \to 0} \int_{\mathbb R} f(x) \frac{\phi(x-h) - \phi(x)}{h} dx = - \int_{\mathbb R} f(x) \phi'(x) dx$$ However, I can't prove it. I want to use dominated convergence, but I can't seem to find a dominating function. Any help?","Let $f \in L^1(\mathbb R)$ and $\phi \in C_c^{\infty} (\mathbb R)$. I am pretty certain that the following is true: $$\lim_{h \to 0} \int_{\mathbb R} f(x) \frac{\phi(x-h) - \phi(x)}{h} dx = - \int_{\mathbb R} f(x) \phi'(x) dx$$ However, I can't prove it. I want to use dominated convergence, but I can't seem to find a dominating function. Any help?",,['limits']
75,Computing: $\lim\limits_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx$,Computing:,\lim\limits_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx,Let $f:\Bbb R \to \Bbb R_+$ be measurable function such that $$\int_\Bbb Rf(x)dx = c$$ Then compute $$\lim_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx$$ Where $a>0 $ is  a parameter. My feeling is that this limit should be $\int_\Bbb Rf(x)dx = c$ but i don't have any good justification so far. any help?,Let $f:\Bbb R \to \Bbb R_+$ be measurable function such that $$\int_\Bbb Rf(x)dx = c$$ Then compute $$\lim_{n\to \infty} \int_\Bbb R n\ln\left(1+\left(\frac{f(x)}{n}\right)^a\right) dx$$ Where $a>0 $ is  a parameter. My feeling is that this limit should be $\int_\Bbb Rf(x)dx = c$ but i don't have any good justification so far. any help?,,"['calculus', 'real-analysis', 'integration', 'limits', 'measure-theory']"
76,Evaluate $\lim_{x \to 0} \frac{1-(x^2/2) -\cos (x/(1-x^2))}{x^4}$,Evaluate,\lim_{x \to 0} \frac{1-(x^2/2) -\cos (x/(1-x^2))}{x^4},find the limits with Using : $\lim_{x \to 0} \frac{1-\cos x}{x^2}=\frac12$ $$\lim_{x \to 0} \frac{1-\dfrac{x^2}2-\cos (\dfrac{x}{1-x^2})}{x^4}$$ My Try : $$\lim_{x \to 0} \frac{1-\dfrac{x^2}2-\cos (\dfrac{x}{1-x^2})}{x^4}=\lim_{x \to 0} \frac{(1-\cos (\dfrac{x}{1-x^2}))+(-\dfrac{x^2}2)}{x^4}$$ $$\lim_{x \to 0} \frac{(\dfrac{(1-\cos (\dfrac{x}{1-x^2}))}{(\dfrac{x}{1-x^2})^2})(\dfrac{x}{1-x^2})^2+(-\dfrac{x^2}2)}{x^4}$$ now what ?,find the limits with Using : My Try : now what ?,\lim_{x \to 0} \frac{1-\cos x}{x^2}=\frac12 \lim_{x \to 0} \frac{1-\dfrac{x^2}2-\cos (\dfrac{x}{1-x^2})}{x^4} \lim_{x \to 0} \frac{1-\dfrac{x^2}2-\cos (\dfrac{x}{1-x^2})}{x^4}=\lim_{x \to 0} \frac{(1-\cos (\dfrac{x}{1-x^2}))+(-\dfrac{x^2}2)}{x^4} \lim_{x \to 0} \frac{(\dfrac{(1-\cos (\dfrac{x}{1-x^2}))}{(\dfrac{x}{1-x^2})^2})(\dfrac{x}{1-x^2})^2+(-\dfrac{x^2}2)}{x^4},"['limits', 'limits-without-lhopital']"
77,What is $\int_0^{\infty}e^{-x^{1.5}+\theta x}dx=?$,What is,\int_0^{\infty}e^{-x^{1.5}+\theta x}dx=?,"Is there a way to obtain an expression for $$\int_0^{\infty}e^{-x^{1.5}+\theta x}dx=?$$ If $\theta=0$, we know the above is the same as $\frac{\Gamma(2/1.5)}{\Gamma(1/1.5)}$ from a generalized Gamma function. Also, I am interested in $$\int_0^{\infty}xe^{-x^{1.5}+\theta x}dx=?$$ or, $$\int_0^{\infty}\sqrt xe^{-x^{1.5}+\theta x}dx=?$$ It seems the 2nd integral above has to do with the generalized normal density, but coudn't figure out the exact connection. I am particularly interested in the limiting behavior of the above integrals when $\theta$ gets very large? Any help or intuitions would be very appreciated!","Is there a way to obtain an expression for $$\int_0^{\infty}e^{-x^{1.5}+\theta x}dx=?$$ If $\theta=0$, we know the above is the same as $\frac{\Gamma(2/1.5)}{\Gamma(1/1.5)}$ from a generalized Gamma function. Also, I am interested in $$\int_0^{\infty}xe^{-x^{1.5}+\theta x}dx=?$$ or, $$\int_0^{\infty}\sqrt xe^{-x^{1.5}+\theta x}dx=?$$ It seems the 2nd integral above has to do with the generalized normal density, but coudn't figure out the exact connection. I am particularly interested in the limiting behavior of the above integrals when $\theta$ gets very large? Any help or intuitions would be very appreciated!",,"['real-analysis', 'probability', 'limits', 'definite-integrals', 'gamma-distribution']"
78,"Proving $\lim\limits_{x\to a}\frac{1}{x^2-2x}\sin\left(\frac{1}{x}-\frac{1}{x-2}\right)$ does not exist at $a=0,2$",Proving  does not exist at,"\lim\limits_{x\to a}\frac{1}{x^2-2x}\sin\left(\frac{1}{x}-\frac{1}{x-2}\right) a=0,2","How would I go about proving that given $a=0$ or $a=2$ the following limit does not exist, $$\lim_{x\to a}\frac{\sin\left(\frac{1}{x}-\frac{1}{x-2}\right)}{x^2-2x}$$ Conceptually, the sine component oscillates wildly around that interval and the fractional component $x^2-2x$ seems to approach infinity around $a=0$ and $a=2$. However, I am having difficulty formulating a rigorous proof that the limit does not exist.","How would I go about proving that given $a=0$ or $a=2$ the following limit does not exist, $$\lim_{x\to a}\frac{\sin\left(\frac{1}{x}-\frac{1}{x-2}\right)}{x^2-2x}$$ Conceptually, the sine component oscillates wildly around that interval and the fractional component $x^2-2x$ seems to approach infinity around $a=0$ and $a=2$. However, I am having difficulty formulating a rigorous proof that the limit does not exist.",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
79,Using epsilon-delta to prove $\lim_{x\to a}\frac{x+3}{x^2+5}$ exists.,Using epsilon-delta to prove  exists.,\lim_{x\to a}\frac{x+3}{x^2+5},How can I formulate a rigorous epsilon-delta proof that $$\lim_{x\to a}\frac{x+3}{x^2+5}$$ exists for any a without the use of any helper theorems. I'm quite stumped and don't know where to begin.,How can I formulate a rigorous epsilon-delta proof that $$\lim_{x\to a}\frac{x+3}{x^2+5}$$ exists for any a without the use of any helper theorems. I'm quite stumped and don't know where to begin.,,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
80,evaluating limits using polar coordinates,evaluating limits using polar coordinates,,"For the following limit, why can you not convert to polar form and evaluate e.g. $$\lim_{(x,y)\to 0}\frac{(x^3+y^3)}{(x^2-y^2)}\to\lim_{r \to 0}\frac{r(\sin^3(t)+\cos^3(t))}{\sin^2(t)-\cos^2(t)} = 0$$ However the real limit does not exist. Why is this?","For the following limit, why can you not convert to polar form and evaluate e.g. $$\lim_{(x,y)\to 0}\frac{(x^3+y^3)}{(x^2-y^2)}\to\lim_{r \to 0}\frac{r(\sin^3(t)+\cos^3(t))}{\sin^2(t)-\cos^2(t)} = 0$$ However the real limit does not exist. Why is this?",,"['limits', 'multivariable-calculus']"
81,"Prove that $f(x,y)$ is totally differentiable in $ (0,0)$",Prove that  is totally differentiable in,"f(x,y)  (0,0)","I have the following function: $f(x,y)=\frac{x^{2}y}{\sqrt{x^{2}+y^{2}}}$, when $(x,y)\neq (0,0)$ and $f(x,y) =(0,0)$, when $(x,y)=(0,0)$. I have to prove that f is totally differentiable, I tried doing this using the the theorem that $f$ is totally differentiable in the point $\xi $ if there exists a linear image $A$ such that: $lim \frac{\| f(x)-f(\xi)-A(x-\xi)\|}{\|x-\xi\|}=0$, when $x\rightarrow \xi$. I now substitute $A$ by the jacobi matrix of $f(\xi _{1},\xi _{2})$ to show that indeed this is true where the jacobi matrix is equal to: $A=(\frac{\partial}{\partial \xi _{1}}f(\xi _{1},\xi _{2}),\frac{\partial}{\partial \xi _{2}}f(\xi _{1},\xi _{2}))=(\frac{\xi _{2}(\xi _{1}^{3}+2\xi _{1}\xi _{2}^{2})}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}},\frac{\xi _{1}^{4}}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}})$ and thus it schould hold that: $lim \frac{\| f(x,y)-f(\xi_{1},\xi_{2})-A(\xi_{1},\xi_{2}).((x,y)-(\xi_{1},\xi_{2}))^{T}\|}{\|(x,y)-(\xi_{1},\xi_{2})\|}=0$, when $(x,y)\rightarrow (\xi_{1},\xi_{2})$ where we have that $(\xi_{1},\xi_{2})=(0,0)$, my problem is that $A(0,0)$ doesn't exist. I don't really know where to go from here, am I doing something wrong?","I have the following function: $f(x,y)=\frac{x^{2}y}{\sqrt{x^{2}+y^{2}}}$, when $(x,y)\neq (0,0)$ and $f(x,y) =(0,0)$, when $(x,y)=(0,0)$. I have to prove that f is totally differentiable, I tried doing this using the the theorem that $f$ is totally differentiable in the point $\xi $ if there exists a linear image $A$ such that: $lim \frac{\| f(x)-f(\xi)-A(x-\xi)\|}{\|x-\xi\|}=0$, when $x\rightarrow \xi$. I now substitute $A$ by the jacobi matrix of $f(\xi _{1},\xi _{2})$ to show that indeed this is true where the jacobi matrix is equal to: $A=(\frac{\partial}{\partial \xi _{1}}f(\xi _{1},\xi _{2}),\frac{\partial}{\partial \xi _{2}}f(\xi _{1},\xi _{2}))=(\frac{\xi _{2}(\xi _{1}^{3}+2\xi _{1}\xi _{2}^{2})}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}},\frac{\xi _{1}^{4}}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}})$ and thus it schould hold that: $lim \frac{\| f(x,y)-f(\xi_{1},\xi_{2})-A(\xi_{1},\xi_{2}).((x,y)-(\xi_{1},\xi_{2}))^{T}\|}{\|(x,y)-(\xi_{1},\xi_{2})\|}=0$, when $(x,y)\rightarrow (\xi_{1},\xi_{2})$ where we have that $(\xi_{1},\xi_{2})=(0,0)$, my problem is that $A(0,0)$ doesn't exist. I don't really know where to go from here, am I doing something wrong?",,"['limits', 'multivariable-calculus']"
82,How to prove that this limit doesn't exits?,How to prove that this limit doesn't exits?,,"The limit is $$ \lim_{(x,y)\to(0,0)} \frac{x\sin(y)-y\sin(x)}{x^2 + y^2}$$ My calculations: I substitute $y=mx$ \begin{align}\lim_{x\to 0} \frac{x\sin(mx)-mx\sin(x)}{x^2 + (mx)^2} &= \lim_{x\to 0} \frac{x(\sin(mx)-m\sin(x)}{x^2(1 + m^2)}\\ &= \lim_{x\to 0} \frac{1}{1+m^2}\bigg[\frac{\sin(mx)}{x}- \frac{m\sin(x)}{x}\bigg]\end{align} Can I say that the limit $$ \lim_{x\to 0}\frac{\sin(mx)}{x}$$ doesn't exist because it depends on $m$, so the entire limit doesn't exist?","The limit is $$ \lim_{(x,y)\to(0,0)} \frac{x\sin(y)-y\sin(x)}{x^2 + y^2}$$ My calculations: I substitute $y=mx$ \begin{align}\lim_{x\to 0} \frac{x\sin(mx)-mx\sin(x)}{x^2 + (mx)^2} &= \lim_{x\to 0} \frac{x(\sin(mx)-m\sin(x)}{x^2(1 + m^2)}\\ &= \lim_{x\to 0} \frac{1}{1+m^2}\bigg[\frac{\sin(mx)}{x}- \frac{m\sin(x)}{x}\bigg]\end{align} Can I say that the limit $$ \lim_{x\to 0}\frac{\sin(mx)}{x}$$ doesn't exist because it depends on $m$, so the entire limit doesn't exist?",,"['limits', 'multivariable-calculus']"
83,How would one calculate $\lim_{n\to\infty}n((1+\frac{1}{n})^n-e)$? [duplicate],How would one calculate ? [duplicate],\lim_{n\to\infty}n((1+\frac{1}{n})^n-e),"This question already has answers here : Find $\lim_{n \to \infty} n[(1+\frac{1}{n})^n - e]$ [duplicate] (2 answers) Closed 6 years ago . What I have thought about this is: we may use L'Hopstal's rule to calculate $\lim_{n\to\infty}\frac{((1+\frac{1}{n})^n-e)}{\frac{1}{n}}$, both the numerator and denominator goes to 0 as n goes to infinity. But calculating the derivative of $(1+1/n)^n$ seems to be very complicated. Using Taylor series to calculate the dominant terms of $(1+1/n)^n$, but I'm not really sure if it makes sense to let $""n=\infty""$. Equivalently maybe we can expand $(1+x)^{1/x}$ at $x=0$, but it's not defined. Maybe I wasn't on the right track. Even if the solution uses a different approach, I would still love to know how to expand $(1+x)^{1/x}$. Thanks for any suggestions.","This question already has answers here : Find $\lim_{n \to \infty} n[(1+\frac{1}{n})^n - e]$ [duplicate] (2 answers) Closed 6 years ago . What I have thought about this is: we may use L'Hopstal's rule to calculate $\lim_{n\to\infty}\frac{((1+\frac{1}{n})^n-e)}{\frac{1}{n}}$, both the numerator and denominator goes to 0 as n goes to infinity. But calculating the derivative of $(1+1/n)^n$ seems to be very complicated. Using Taylor series to calculate the dominant terms of $(1+1/n)^n$, but I'm not really sure if it makes sense to let $""n=\infty""$. Equivalently maybe we can expand $(1+x)^{1/x}$ at $x=0$, but it's not defined. Maybe I wasn't on the right track. Even if the solution uses a different approach, I would still love to know how to expand $(1+x)^{1/x}$. Thanks for any suggestions.",,"['calculus', 'analysis', 'limits', 'taylor-expansion']"
84,How to prove L'Hospital's rule using $\varepsilon$-$\delta$ method? [duplicate],How to prove L'Hospital's rule using - method? [duplicate],\varepsilon \delta,"This question already has answers here : Understanding the Proof of L'Hopital's Rule (2 answers) Closed 4 years ago . For $\varepsilon$ - $\delta$ proofs, basically we need to find a $\delta$ such that $|F(x)-L|<\epsilon$ whenever, $0<|x-a|<\delta$ (for a small positive number $\epsilon$ ). To prove L'Hospital's rule (for when numerator and denominator function both tend to $0$ as $x\rightarrow a^{+}$ )  let us assume $F(x)=\frac{f(x)}{g(x)}$ . Where, $\lim_{x\rightarrow a^{+}}f(x)=0$ and $\lim_{x\rightarrow a^{+}}g(x)=0$ . I claim that $L=\lim_{x \rightarrow a^{+}}\frac{f'(x)}{g'(x)}$ . Now I need to prove this $L$ is indeed the limit. $$\left|\frac{f(x)}{g(x)}-\lim_{x \rightarrow a^{+}}\frac{f'(x)}{g'(x)}\right|<\epsilon.$$ But after this I cannot understand how to find $\delta$ as a function of $\epsilon$ , so that I can complete the proof. How should I proceed?","This question already has answers here : Understanding the Proof of L'Hopital's Rule (2 answers) Closed 4 years ago . For - proofs, basically we need to find a such that whenever, (for a small positive number ). To prove L'Hospital's rule (for when numerator and denominator function both tend to as )  let us assume . Where, and . I claim that . Now I need to prove this is indeed the limit. But after this I cannot understand how to find as a function of , so that I can complete the proof. How should I proceed?",\varepsilon \delta \delta |F(x)-L|<\epsilon 0<|x-a|<\delta \epsilon 0 x\rightarrow a^{+} F(x)=\frac{f(x)}{g(x)} \lim_{x\rightarrow a^{+}}f(x)=0 \lim_{x\rightarrow a^{+}}g(x)=0 L=\lim_{x \rightarrow a^{+}}\frac{f'(x)}{g'(x)} L \left|\frac{f(x)}{g(x)}-\lim_{x \rightarrow a^{+}}\frac{f'(x)}{g'(x)}\right|<\epsilon. \delta \epsilon,['real-analysis']
85,Can't solve Improper Integral $\int_{0}^{\infty} \frac{\sqrt{x}\sin(x)}{1+x^2} dx$,Can't solve Improper Integral,\int_{0}^{\infty} \frac{\sqrt{x}\sin(x)}{1+x^2} dx,"Whilst checking for the existence of improper integrals, I came across this one: $$\int_{0}^{\infty} \frac{\sqrt{x}\sin(x)}{1+x^2} dx$$ So in order to check its existence I simply have to see if the limit: $$\lim_{a\to\infty} \int_{0}^{a} \frac{\sqrt{x}\sin(x)}{1+x^2} dx $$ Is a number or not. However I seem unable to find a way to solve this particular Integral, and neither any online calculator can. I have tried all substitutions that I could think of, as well as partial integration and using any helpful trigonometric identities but they were all in vain.","Whilst checking for the existence of improper integrals, I came across this one: $$\int_{0}^{\infty} \frac{\sqrt{x}\sin(x)}{1+x^2} dx$$ So in order to check its existence I simply have to see if the limit: $$\lim_{a\to\infty} \int_{0}^{a} \frac{\sqrt{x}\sin(x)}{1+x^2} dx $$ Is a number or not. However I seem unable to find a way to solve this particular Integral, and neither any online calculator can. I have tried all substitutions that I could think of, as well as partial integration and using any helpful trigonometric identities but they were all in vain.",,"['integration', 'limits', 'improper-integrals']"
86,Does $\lim_{x \to +\infty} 2g(2x) - g(x) = 0$ imply $\lim_{x \to +\infty} g(x) = 0$?,Does  imply ?,\lim_{x \to +\infty} 2g(2x) - g(x) = 0 \lim_{x \to +\infty} g(x) = 0,"Suppose I have a function $g : [0, \infty) \to [0, \infty)$ such that  $$ \lim_{x \to +\infty} 2g(2x) - g(x) = 0, $$ and for every $M>0$, the restriction $g\vert_{[0,M]}$ is bounded (for instance, this is the case if $g$ is continuous). Does it follow that $ \lim_{x \to +\infty} g(x) = 0$ ? I know that $$4g(4x)-g(x) = 2(2g(4x) - g(2x)) + (2g(2x)-g(x)) \to 0, \quad x \to \infty$$ so $2^n g(2^n x) - g(x) \to 0$ for any $n \geq 1$. If $g$ is bounded from above by $B>0$, then $|g(2^n x)| \leq \left| \dfrac{2^n g(2^n x) - g(x)}{2^n} \right| + \dfrac{B}{2^n}$, so I guess by taking $x$ and $n$ large enough, we can get $g(x) \to 0$. What about the general case?","Suppose I have a function $g : [0, \infty) \to [0, \infty)$ such that  $$ \lim_{x \to +\infty} 2g(2x) - g(x) = 0, $$ and for every $M>0$, the restriction $g\vert_{[0,M]}$ is bounded (for instance, this is the case if $g$ is continuous). Does it follow that $ \lim_{x \to +\infty} g(x) = 0$ ? I know that $$4g(4x)-g(x) = 2(2g(4x) - g(2x)) + (2g(2x)-g(x)) \to 0, \quad x \to \infty$$ so $2^n g(2^n x) - g(x) \to 0$ for any $n \geq 1$. If $g$ is bounded from above by $B>0$, then $|g(2^n x)| \leq \left| \dfrac{2^n g(2^n x) - g(x)}{2^n} \right| + \dfrac{B}{2^n}$, so I guess by taking $x$ and $n$ large enough, we can get $g(x) \to 0$. What about the general case?",,"['real-analysis', 'limits']"
87,graph limit problem,graph limit problem,,"For this given graph, since it approaches positive infinity from the left and right sides of $x=2$, shouldn't $\lim_{x \to 2} g(x) = \infty$? Or would its value not exist (as it says in my homework answer key)?","For this given graph, since it approaches positive infinity from the left and right sides of $x=2$, shouldn't $\lim_{x \to 2} g(x) = \infty$? Or would its value not exist (as it says in my homework answer key)?",,"['algebra-precalculus', 'limits', 'functions', 'rational-functions']"
88,"Showing that the ""left-hand limit function"" is left continuous","Showing that the ""left-hand limit function"" is left continuous",,"Let $f: [0, \infty) \rightarrow \mathbb{R}$. Define the value of the left-hand limit of $f$ at $t>0$ to be $f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Define the ""left-hand limit function"" of $f$ as $f^-: (0, \infty) \rightarrow \mathbb{R}, f^-(t) = f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Prove that $f^-$ is left continuous at each $t>0$. Let $t>0$. To prove $f^-$ is left continuous at $t$, I need to show that $\lim_{t \rightarrow t^-}f^-(t) = f^-(t) = f(t^-)$. My question is, how can I use the sequential definition (not the $\epsilon-\delta$ definition) of left continuity to prove this question? In other words, let $(s_n)$ be a sequence contained in $(0, \infty)$ such that $s_n<t$ for all $n$ and $s_n \rightarrow t$. I need to show that $\lim_{n \rightarrow \infty} f^-(s_n) = f^-(t) = f(t^-)$. I have also been given a hint (but I have no idea where to incorporate it): If $f(t^-)$ exists and is finite, then it is equivalent to: $$f(t^-) = \sup \inf_{s<t} \{f(v): s \le v < t\} = \inf \sup_{s <t} \{f(v) : s \le v < t\} $$ Any help would be greatly appreciated!","Let $f: [0, \infty) \rightarrow \mathbb{R}$. Define the value of the left-hand limit of $f$ at $t>0$ to be $f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Define the ""left-hand limit function"" of $f$ as $f^-: (0, \infty) \rightarrow \mathbb{R}, f^-(t) = f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Prove that $f^-$ is left continuous at each $t>0$. Let $t>0$. To prove $f^-$ is left continuous at $t$, I need to show that $\lim_{t \rightarrow t^-}f^-(t) = f^-(t) = f(t^-)$. My question is, how can I use the sequential definition (not the $\epsilon-\delta$ definition) of left continuity to prove this question? In other words, let $(s_n)$ be a sequence contained in $(0, \infty)$ such that $s_n<t$ for all $n$ and $s_n \rightarrow t$. I need to show that $\lim_{n \rightarrow \infty} f^-(s_n) = f^-(t) = f(t^-)$. I have also been given a hint (but I have no idea where to incorporate it): If $f(t^-)$ exists and is finite, then it is equivalent to: $$f(t^-) = \sup \inf_{s<t} \{f(v): s \le v < t\} = \inf \sup_{s <t} \{f(v) : s \le v < t\} $$ Any help would be greatly appreciated!",,"['real-analysis', 'analysis', 'limits', 'proof-writing']"
89,Evaluate the limit of $\sum\limits _ { k = 0} ^ { n } \mathrm{arctg} \frac { k + 1} { n ^ { 2} }$ when $n\to\infty$,Evaluate the limit of  when,\sum\limits _ { k = 0} ^ { n } \mathrm{arctg} \frac { k + 1} { n ^ { 2} } n\to\infty,"Evaluate $$\lim _ { n \rightarrow \infty } \sum _ { k = 0} ^ { n } \mathrm{arctg} \frac { k + 1} { n ^ { 2} }$$ At first I thought this was a Riemann sum, but I couldn't insert the $1/n$ and get the right form. I also tried to write it out, and it definitely looks like it would converge, but I'm not sure how to approach it.","Evaluate $$\lim _ { n \rightarrow \infty } \sum _ { k = 0} ^ { n } \mathrm{arctg} \frac { k + 1} { n ^ { 2} }$$ At first I thought this was a Riemann sum, but I couldn't insert the $1/n$ and get the right form. I also tried to write it out, and it definitely looks like it would converge, but I'm not sure how to approach it.",,"['real-analysis', 'sequences-and-series', 'limits']"
90,Uniform convergence and bounded variation functions,Uniform convergence and bounded variation functions,,"Is this proof correct? I did it myself so I'm not sure if it's right. Let ${f_k}$ be a sequence of bounded variation on $[a,b]$ such that for each $x\in [a,b]$ $f(x)=\lim_{k\to \infty}f_k(x)$. If there exist $K>0$ such that $V(f_n,[a,b])\le K$ for all $n\in \mathbb N$, then $f$ is bounded variation on $[a,b]$. Proof: Let $\epsilon >0$. Then $V(f,[a,b])=\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\sum_{i=1}^{k}|\lim_{n\to \infty}f_n(x_i)-\lim_{n\to \infty}f_n(x_{i-1})|=\sum_{i=1}^{k}\lim_{n\to \infty}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}K=K$. Therefore $f$ is bounded variation on $[a,b]$. Please tell me if it is correct or if it's not.","Is this proof correct? I did it myself so I'm not sure if it's right. Let ${f_k}$ be a sequence of bounded variation on $[a,b]$ such that for each $x\in [a,b]$ $f(x)=\lim_{k\to \infty}f_k(x)$. If there exist $K>0$ such that $V(f_n,[a,b])\le K$ for all $n\in \mathbb N$, then $f$ is bounded variation on $[a,b]$. Proof: Let $\epsilon >0$. Then $V(f,[a,b])=\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\sum_{i=1}^{k}|\lim_{n\to \infty}f_n(x_i)-\lim_{n\to \infty}f_n(x_{i-1})|=\sum_{i=1}^{k}\lim_{n\to \infty}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}K=K$. Therefore $f$ is bounded variation on $[a,b]$. Please tell me if it is correct or if it's not.",,"['analysis', 'limits', 'proof-verification', 'uniform-convergence', 'bounded-variation']"
91,Probability of the occurrence of a random event x in a population X where n = $\infty$? [duplicate],Probability of the occurrence of a random event x in a population X where n = ? [duplicate],\infty,"This question already has answers here : Is getting a random integer even possible? (2 answers) Closed 6 years ago . Given a random event x from a discrete population X where the population $n=\infty$, and the population is uniformly distributed, what is P(x)? My intuition is that it is infinitesimal, because as $lim_{n\to\infty}$, $ lim_{p\to0}$","This question already has answers here : Is getting a random integer even possible? (2 answers) Closed 6 years ago . Given a random event x from a discrete population X where the population $n=\infty$, and the population is uniformly distributed, what is P(x)? My intuition is that it is infinitesimal, because as $lim_{n\to\infty}$, $ lim_{p\to0}$",,"['probability', 'limits']"
92,Limit of a product sequence for infinite terms [duplicate],Limit of a product sequence for infinite terms [duplicate],,This question already has answers here : Evaluating the infinite product $\prod\limits_{k=2}^\infty \left ( 1-\frac1{k^2}\right)$ (6 answers) Closed 4 years ago . I tried solving an infinite limit like this and got the answer as 0.5 I think there's a flaw in my approach. Please suggest a better approach.,This question already has answers here : Evaluating the infinite product $\prod\limits_{k=2}^\infty \left ( 1-\frac1{k^2}\right)$ (6 answers) Closed 4 years ago . I tried solving an infinite limit like this and got the answer as 0.5 I think there's a flaw in my approach. Please suggest a better approach.,,"['sequences-and-series', 'limits', 'infinite-product']"
93,Proving the sequence $\sqrt{n+1}-\sqrt{n}$ is convergent,Proving the sequence  is convergent,\sqrt{n+1}-\sqrt{n},"I know the $\epsilon - \delta$ definition of a limit in this case is $\forall \epsilon >0 \exists N\in \mathbb{N}\forall n \in \mathbb{N} (n \geq N \implies \sqrt{n+1}-\sqrt{n}<\epsilon)$. So far, I have been able to show: $$\sqrt{n+1}-\sqrt{n} = \frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{(\sqrt{n+1}+\sqrt{n})} = \frac{1}{\sqrt{n+1}+\sqrt{n}}.$$ This is the part where I find myself stuck. What do I need to do next?","I know the $\epsilon - \delta$ definition of a limit in this case is $\forall \epsilon >0 \exists N\in \mathbb{N}\forall n \in \mathbb{N} (n \geq N \implies \sqrt{n+1}-\sqrt{n}<\epsilon)$. So far, I have been able to show: $$\sqrt{n+1}-\sqrt{n} = \frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{(\sqrt{n+1}+\sqrt{n})} = \frac{1}{\sqrt{n+1}+\sqrt{n}}.$$ This is the part where I find myself stuck. What do I need to do next?",,"['sequences-and-series', 'limits']"
94,Clean Limit Proof,Clean Limit Proof,,"While attempting to solve $\int_0^\infty \frac{\sin x}{x} dx$ using Differentation Under the Integral Sign, I have stumbled across the follow limit: $$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx \tag{1}$$ Now, this should go to $0$, but I have been struggling to show this cleanly without resorting to Complex Analysis. I have managed to squeeze out a proof using Integration by Parts and letting $u=\frac{x}{x^2+1}$ to get $$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx =\lim_{a \to \infty}\frac{1}{2a}\int_{-\infty}^{\infty}\frac{\left(x^2-1\right)\cos \left(ax\right)}{\left(x^2+1\right)^2}dx$$ All that is left is to note that $-1 < \cos(ax) < 1$ and to apply the squeeze theorem. However, I am seeking alternative proofs that are clean and straightforward. Another way I could potentially go about this is by noting $$\int_0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx = \sum_{n=0}^\infty\left(\int_{2n\pi/a}^{(2n+1)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\;- \int_{(2n+1)\pi/a}^{(2n+2)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\right)$$ I could now bound the difference between the two integrals; however, this seems even more tedious than my first attempt. What other real analysis methods can be used to evaluate (1) cleanly and efficiently?","While attempting to solve $\int_0^\infty \frac{\sin x}{x} dx$ using Differentation Under the Integral Sign, I have stumbled across the follow limit: $$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx \tag{1}$$ Now, this should go to $0$, but I have been struggling to show this cleanly without resorting to Complex Analysis. I have managed to squeeze out a proof using Integration by Parts and letting $u=\frac{x}{x^2+1}$ to get $$\lim_{a \to \infty}\int _0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx =\lim_{a \to \infty}\frac{1}{2a}\int_{-\infty}^{\infty}\frac{\left(x^2-1\right)\cos \left(ax\right)}{\left(x^2+1\right)^2}dx$$ All that is left is to note that $-1 < \cos(ax) < 1$ and to apply the squeeze theorem. However, I am seeking alternative proofs that are clean and straightforward. Another way I could potentially go about this is by noting $$\int_0^{\infty}\frac{x\sin \left(ax\right)}{x^2+1}dx = \sum_{n=0}^\infty\left(\int_{2n\pi/a}^{(2n+1)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\;- \int_{(2n+1)\pi/a}^{(2n+2)\pi/a}\frac{x\sin \left(ax\right)}{x^2+1}dx\right)$$ I could now bound the difference between the two integrals; however, this seems even more tedious than my first attempt. What other real analysis methods can be used to evaluate (1) cleanly and efficiently?",,"['calculus', 'integration', 'sequences-and-series', 'limits', 'definite-integrals']"
95,find the limit of $ (1/\sin\ (x)-1/x)^x$,find the limit of, (1/\sin\ (x)-1/x)^x,"I am trying to find the limit of $\lim_{x \downarrow 0} (\frac{1}{sinx}- \frac{1}{x})^x$. My current progress: $\lim_{x \downarrow 0} (\frac{1}{\sin x}- \frac{1}{x})^x = \lim_{x \downarrow 0} (\frac{x-\sin x}{x\sin x})^x = \lim_{x \downarrow 0} e^{xln(\frac{x-\sin x}{x \sin x})} = e^{\lim_{x \downarrow 0}xln(\frac{x-\sin x}{x \sin x})} = e^{\lim_{x \downarrow 0}\frac{ln(\frac{x-\sin x}{x \sin x})}{1/x}}$ This is where I used l'Hospital, but after 2 iterations of l'Hospital it doesn't look like I'll get anywhere with it. Expression after first l'Hospital: (I'll not write the e as the base, so that the limit is easier to read) $\lim_{x \downarrow 0}\frac{-x^3\sin x\cos x - x \sin^3x}{x\sin^2x - \sin^3x} = \lim_{x \downarrow 0}\frac{-x^3\cos x - x \sin^2x}{x\sin x - \sin^2x}$ Am I missing something or is it just tedious to find the limit of this expression? After the second iteration of l'Hospital, I've got this term: $\lim_{x \downarrow 0}\frac{-3x^2\cos x + x^3\sin x - \sin^2x -2x\sin x\cos x}{x\cos x + \sin x -2\sin x\cos x} = ""\frac{0}{0}""$ is that term correct? If so, I'll try to apply l'Hospital a third time.","I am trying to find the limit of $\lim_{x \downarrow 0} (\frac{1}{sinx}- \frac{1}{x})^x$. My current progress: $\lim_{x \downarrow 0} (\frac{1}{\sin x}- \frac{1}{x})^x = \lim_{x \downarrow 0} (\frac{x-\sin x}{x\sin x})^x = \lim_{x \downarrow 0} e^{xln(\frac{x-\sin x}{x \sin x})} = e^{\lim_{x \downarrow 0}xln(\frac{x-\sin x}{x \sin x})} = e^{\lim_{x \downarrow 0}\frac{ln(\frac{x-\sin x}{x \sin x})}{1/x}}$ This is where I used l'Hospital, but after 2 iterations of l'Hospital it doesn't look like I'll get anywhere with it. Expression after first l'Hospital: (I'll not write the e as the base, so that the limit is easier to read) $\lim_{x \downarrow 0}\frac{-x^3\sin x\cos x - x \sin^3x}{x\sin^2x - \sin^3x} = \lim_{x \downarrow 0}\frac{-x^3\cos x - x \sin^2x}{x\sin x - \sin^2x}$ Am I missing something or is it just tedious to find the limit of this expression? After the second iteration of l'Hospital, I've got this term: $\lim_{x \downarrow 0}\frac{-3x^2\cos x + x^3\sin x - \sin^2x -2x\sin x\cos x}{x\cos x + \sin x -2\sin x\cos x} = ""\frac{0}{0}""$ is that term correct? If so, I'll try to apply l'Hospital a third time.",,['limits']
96,Can I manipulate a function (with a parameter) to show it converges to another function?,Can I manipulate a function (with a parameter) to show it converges to another function?,,"Please bear with me. So, for example the limit for $a$ in this hyperbola: $$\lim_{a \to 0^+} (y^2 -x^2 = a) \, ,\,\,\, \, y \ge 0 \tag{A}$$ This appears to make the equation go to:  $$y^2 = x^2 \, ,\,\,\, \, y \ge 0 \tag{B}$$ $$y = |x| \tag{C}$$ So, is $(A)$ a valid mathematical definition of the absolute value? And furthermore, is $(A)$ differentiable? Because if it was, that immediately means that it's differentiable as $a$ is aribitrarily close to $0$ which is the same thing as $(C)$, but we know $(C)$ is not differentiable at $0$. In short, I'm wondering if (and maybe the above is a bad example) we can show, via a parameter, that one function converges to another in this type of limiting process and, if so, why would it lose it's differentiability (if that even makes sense)? Here's a Desmos link link to the action I'm talking about.","Please bear with me. So, for example the limit for $a$ in this hyperbola: $$\lim_{a \to 0^+} (y^2 -x^2 = a) \, ,\,\,\, \, y \ge 0 \tag{A}$$ This appears to make the equation go to:  $$y^2 = x^2 \, ,\,\,\, \, y \ge 0 \tag{B}$$ $$y = |x| \tag{C}$$ So, is $(A)$ a valid mathematical definition of the absolute value? And furthermore, is $(A)$ differentiable? Because if it was, that immediately means that it's differentiable as $a$ is aribitrarily close to $0$ which is the same thing as $(C)$, but we know $(C)$ is not differentiable at $0$. In short, I'm wondering if (and maybe the above is a bad example) we can show, via a parameter, that one function converges to another in this type of limiting process and, if so, why would it lose it's differentiability (if that even makes sense)? Here's a Desmos link link to the action I'm talking about.",,"['limits', 'functions', 'derivatives', 'parametric']"
97,"Trying to see if $f(x,y)=xy+1-\sin\left (\frac{x^2}{2}\right)$ is differentiable at $(1,5)$",Trying to see if  is differentiable at,"f(x,y)=xy+1-\sin\left (\frac{x^2}{2}\right) (1,5)","I'm trying to find out if this function is differentiable at $(1,5)$: $$f(x,y)=xy+1-\sin \left(\frac{x^2}{2}\right)$$ First I've checked that it's continuous at the point $\lim_{(x,y) \rightarrow (1,5)}{f(x,y)}=f(1,5)=6-\sin\left(\frac{1}{2}\right)$. After that I've calculated it's partial derivatives: $f'_x=y - x \cos\left(\frac{x^2}{2}\right)$ and $f'_y=x$ Now I'm trying to see if it's differentiable by the limit definition: $$\lim_{(x,y) \rightarrow (1,5)} \frac{\left| xy+1-\sin \left(\frac{x^2}{2}\right) - \left(6-\sin\left(\frac{1}{2}\right)\right) - \left(y - x \cos\left(\frac{x^2}{2}\right)\right) (x-5) - x (y-5)\right|}{||(x,y)-(1,5)||} = 0$$ But I can't work my way around it... am I right by doing all of this? Is there another way? How can I solve it?","I'm trying to find out if this function is differentiable at $(1,5)$: $$f(x,y)=xy+1-\sin \left(\frac{x^2}{2}\right)$$ First I've checked that it's continuous at the point $\lim_{(x,y) \rightarrow (1,5)}{f(x,y)}=f(1,5)=6-\sin\left(\frac{1}{2}\right)$. After that I've calculated it's partial derivatives: $f'_x=y - x \cos\left(\frac{x^2}{2}\right)$ and $f'_y=x$ Now I'm trying to see if it's differentiable by the limit definition: $$\lim_{(x,y) \rightarrow (1,5)} \frac{\left| xy+1-\sin \left(\frac{x^2}{2}\right) - \left(6-\sin\left(\frac{1}{2}\right)\right) - \left(y - x \cos\left(\frac{x^2}{2}\right)\right) (x-5) - x (y-5)\right|}{||(x,y)-(1,5)||} = 0$$ But I can't work my way around it... am I right by doing all of this? Is there another way? How can I solve it?",,"['limits', 'functions', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
98,"Checking the continuity of the given function at the point $(1,0)$",Checking the continuity of the given function at the point,"(1,0)","$$f(x,y) = \frac{3y(x-1)}{(x-1)^2+y^2}$$ so knowing that function is continuous when it has a limit. I will applying a two path test to check that. $$f(x,0)\to (1,0) = \frac{0(x-1)}{(x-1)^2+0} = 0 $$ and $$f(x,x-1)\to (1,0) = \frac{3(x-1)(x-1)}{(x-1)^2+(x-1)^2} =  \frac{3(x-1)^2}{2(x-1)^2}= 3/2$$ so is this function discontinous?","$$f(x,y) = \frac{3y(x-1)}{(x-1)^2+y^2}$$ so knowing that function is continuous when it has a limit. I will applying a two path test to check that. $$f(x,0)\to (1,0) = \frac{0(x-1)}{(x-1)^2+0} = 0 $$ and $$f(x,x-1)\to (1,0) = \frac{3(x-1)(x-1)}{(x-1)^2+(x-1)^2} =  \frac{3(x-1)^2}{2(x-1)^2}= 3/2$$ so is this function discontinous?",,"['limits', 'multivariable-calculus', 'continuity']"
99,"Proving by definition $\lim_{(x,y)\to(-1,8)} xy = -8$",Proving by definition,"\lim_{(x,y)\to(-1,8)} xy = -8","I need to prove by definition the following limit: $$\lim_{(x,y)\to(-1,8)} xy = -8$$ I've reached a point where I don't know how to proceed. So given an $\epsilon > 0$ I need to find a $\delta>0$ such that $|f(x,y)+8|<\epsilon$ when $\left\lVert (x,y)-(-1,8)\right\rVert<\delta$. Knowing the distance from $(x,y)$ to the point $(-1,8)$ is less than delta I also know that $|x+1|<\delta$ and $|y-8|<\delta$. So I try to use that in the inequation involving $\epsilon$. $|xy + 8|<\epsilon$ $|xy + 8+1-1+x-x+y-y|<\epsilon$ $|2(x+1)+(y-8)+y(x+1)| \leq 2|x+1| + |y-8| + |y| |x+1|<\epsilon$ $|2(x+1)+(y-8)+y(x+1)| \leq 2|x+1| + |y-8| + |y| |x+1|< 3 \delta + |y| \delta < \epsilon$ So pretty much I don't know what to do with the $|y| \delta$ in $3 \delta + |y| \delta < \epsilon$ Any hints? Thanks!!","I need to prove by definition the following limit: $$\lim_{(x,y)\to(-1,8)} xy = -8$$ I've reached a point where I don't know how to proceed. So given an $\epsilon > 0$ I need to find a $\delta>0$ such that $|f(x,y)+8|<\epsilon$ when $\left\lVert (x,y)-(-1,8)\right\rVert<\delta$. Knowing the distance from $(x,y)$ to the point $(-1,8)$ is less than delta I also know that $|x+1|<\delta$ and $|y-8|<\delta$. So I try to use that in the inequation involving $\epsilon$. $|xy + 8|<\epsilon$ $|xy + 8+1-1+x-x+y-y|<\epsilon$ $|2(x+1)+(y-8)+y(x+1)| \leq 2|x+1| + |y-8| + |y| |x+1|<\epsilon$ $|2(x+1)+(y-8)+y(x+1)| \leq 2|x+1| + |y-8| + |y| |x+1|< 3 \delta + |y| \delta < \epsilon$ So pretty much I don't know what to do with the $|y| \delta$ in $3 \delta + |y| \delta < \epsilon$ Any hints? Thanks!!",,"['calculus', 'analysis', 'limits', 'epsilon-delta']"
