,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,ADM Formulation in General Relativity,ADM Formulation in General Relativity,,"In the ADM(Arnowitt – Deser – Misner) formulation, we can foliate a globally hyperbolic spacetime by spacelike hyper-surface(Cauchy surface) $\Sigma_{t}$ , which parametrised by global time function $t$ . Therefore, in each point of spacelike hyper-surface, we can let $n^{a}$ to be a future -directed timelike unit vector field normal to the hyper surface $\Sigma_{t}$ , which satisfies $n^{a}n_{a} = -1 $ and $n_{a} \propto \nabla_{a}t$ ( $\nabla_{a}$ is associated with spacetime metric. $g_{ab}$ ). Therefore, the spacetime metric $g_{ab}$ induces a spatial metric $\gamma_{ab}$ which is defined as \begin{align} \gamma_{ab} = g_{ab} + n_{a}n_{b} \end{align} My difficulties are that why the spacetime metric induce such spatial metric $\gamma_{ab}$ and how can I understand the above equation.","In the ADM(Arnowitt – Deser – Misner) formulation, we can foliate a globally hyperbolic spacetime by spacelike hyper-surface(Cauchy surface) , which parametrised by global time function . Therefore, in each point of spacelike hyper-surface, we can let to be a future -directed timelike unit vector field normal to the hyper surface , which satisfies and ( is associated with spacetime metric. ). Therefore, the spacetime metric induces a spatial metric which is defined as My difficulties are that why the spacetime metric induce such spatial metric and how can I understand the above equation.","\Sigma_{t} t n^{a} \Sigma_{t} n^{a}n_{a} = -1  n_{a} \propto \nabla_{a}t \nabla_{a} g_{ab} g_{ab} \gamma_{ab} \begin{align}
\gamma_{ab} = g_{ab} + n_{a}n_{b}
\end{align} \gamma_{ab}","['differential-geometry', 'mathematical-physics', 'general-relativity']"
1,Covariant derivative of determinant of the metric tensor,Covariant derivative of determinant of the metric tensor,,"Let $(M,g)$ be a Riemannian manifold and $g$ the Riemannian metric in coordinates $g=g_{\alpha \beta}dx^{\alpha} \otimes dx^{\beta}$ , where $x^{i}$ are local coordinates on $M$ . Denote by $g^{\alpha \beta}$ the inverse components of the inverse metric $g^{-1}$ . Let $\nabla$ be the Levi-Civita connection of the metric $g$ . Consider, locally, the function $\det((g_{\alpha \beta})_{\alpha \beta})$ . It is known that $\nabla \det((g_{\alpha \beta})_{\alpha \beta}) = 0$ by using normal coordinates etc... I would like to show this fact without using normal coordinates. Just by computation. Here is what I have so far: $$\nabla \det((g_{\alpha \beta})_{\alpha \beta}) =  \left [ g^{\gamma \delta} \partial_{\delta} \det((g_{\alpha \beta})_{\alpha \beta}) \right ] \partial_{\gamma} = \left [ \det((g_{\alpha \beta})_{\alpha \beta}) g^{\gamma \delta} g^{\beta \alpha} \partial_{\delta} g_{\alpha \beta}\right ] \partial_{\gamma}.$$ Here: the first equality sign follows from the definition of the gradient of a function and the second equality sign is the derivative of the determinant. Question: How do I continue from here without using normal coordinates? Or are there any mistakes? If yes, where and which?","Let be a Riemannian manifold and the Riemannian metric in coordinates , where are local coordinates on . Denote by the inverse components of the inverse metric . Let be the Levi-Civita connection of the metric . Consider, locally, the function . It is known that by using normal coordinates etc... I would like to show this fact without using normal coordinates. Just by computation. Here is what I have so far: Here: the first equality sign follows from the definition of the gradient of a function and the second equality sign is the derivative of the determinant. Question: How do I continue from here without using normal coordinates? Or are there any mistakes? If yes, where and which?","(M,g) g g=g_{\alpha \beta}dx^{\alpha} \otimes dx^{\beta} x^{i} M g^{\alpha \beta} g^{-1} \nabla g \det((g_{\alpha \beta})_{\alpha \beta}) \nabla \det((g_{\alpha \beta})_{\alpha \beta}) = 0 \nabla \det((g_{\alpha \beta})_{\alpha \beta}) =  \left [ g^{\gamma \delta} \partial_{\delta} \det((g_{\alpha \beta})_{\alpha \beta}) \right ] \partial_{\gamma} = \left [ \det((g_{\alpha \beta})_{\alpha \beta}) g^{\gamma \delta} g^{\beta \alpha} \partial_{\delta} g_{\alpha \beta}\right ] \partial_{\gamma}.","['differential-geometry', 'riemannian-geometry']"
2,"Smooth, approximately space-filling curves in high dimensions","Smooth, approximately space-filling curves in high dimensions",,"I'm looking for smooth (infinitely differentiable everywhere) functions (curves) $\mathbb{R}\rightarrow\mathbb{R}^d$ that are approximately space-filling, i.e. scaling allows the curve to eventually get arbitrarily close to all points in $\mathbb{R}^d$ . An intuitive example for $\mathbb{R}\rightarrow\mathbb{R}^2$ would be the Archimedean Spiral , e.g.: Example function: $$ \mathrm{f}(t)= \rho\cdot \begin{pmatrix}   \cos(t) \cdot t \\   \sin(t) \cdot t \end{pmatrix} $$ As $\rho$ approaches zero, the spiral will eventually get arbitrarily close to every point in $\mathbb{R}^2$ . It would also be great if the computational complexity of calculating such a function only increases linearly with the dimension $d$ .","I'm looking for smooth (infinitely differentiable everywhere) functions (curves) that are approximately space-filling, i.e. scaling allows the curve to eventually get arbitrarily close to all points in . An intuitive example for would be the Archimedean Spiral , e.g.: Example function: As approaches zero, the spiral will eventually get arbitrarily close to every point in . It would also be great if the computational complexity of calculating such a function only increases linearly with the dimension .","\mathbb{R}\rightarrow\mathbb{R}^d \mathbb{R}^d \mathbb{R}\rightarrow\mathbb{R}^2 
\mathrm{f}(t)=
\rho\cdot
\begin{pmatrix}
  \cos(t) \cdot t \\
  \sin(t) \cdot t
\end{pmatrix}
 \rho \mathbb{R}^2 d","['differential-geometry', 'analytic-geometry', 'curves', 'smooth-functions']"
3,"Loomis and Sternberg: Tangent Space to a manifold, using equivalence classes; help justifying one step of an argument","Loomis and Sternberg: Tangent Space to a manifold, using equivalence classes; help justifying one step of an argument",,"I am currently reading through the section in Loomis and Sternberg's Advanced Calculus (Revised Edition) on tangent spaces, but I'm having trouble justifying one step of the argument (on pp. 373-374, shown below). Here are the definitions and notations used by them. Let $M$ be a differentiable manifold (here they model their manifolds on a Banach space $V$ rather than some $\mathbb{R}^n$ ). Let $x \in M$ , and let $\varphi : I \to M$ be a differentiable map where $I$ is an interval in $\mathbb{R}$ containing $0$ , and $\varphi(0) = x.$ Then, they define an operator $D_{\varphi}: C^{\infty}(M) \to \mathbb{R}$ by $D_{\varphi}(f) = (f \circ \varphi)'(0)$ . Next, they define an equivalence relation on all the curves passing through $x$ by $\varphi \sim \psi$ if and only if $D_{\varphi} = D_{\psi}$ , and call an equivalence class of curves, $\xi$ to be a tangent vector at $x$ . So far so good. Next, they go to a chart $(W, \alpha)$ , and the underlined section below is what I don't fully understand. I get that $\varphi \sim \psi$ if and only if for every $f \in C^{\infty}(M)$ , $d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \varphi)'(0))$ $= d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \psi)'(0))$ . But I don't see how to conclude from here that the two vectors $(\alpha \circ \varphi)'(0)$ and $(\alpha \circ \psi)'(0)$ are equal. I'm guessing it has something to do with the fact that the two derivatives are equal for every $f$ ; if we somehow choose an $f$ such that the differential $d(f \circ \alpha^{-1})_{\alpha(x)} : V \to \mathbb{R}$ is injective, then its kernel is $\{0\}$ , and thus equality follows. However I doubt this is always possible, since in general $\dim(V) > 1$ , so a linear map from $V$ to $\mathbb{R}$ cannot be injective. Any help justifying this step is much appreciated.","I am currently reading through the section in Loomis and Sternberg's Advanced Calculus (Revised Edition) on tangent spaces, but I'm having trouble justifying one step of the argument (on pp. 373-374, shown below). Here are the definitions and notations used by them. Let be a differentiable manifold (here they model their manifolds on a Banach space rather than some ). Let , and let be a differentiable map where is an interval in containing , and Then, they define an operator by . Next, they define an equivalence relation on all the curves passing through by if and only if , and call an equivalence class of curves, to be a tangent vector at . So far so good. Next, they go to a chart , and the underlined section below is what I don't fully understand. I get that if and only if for every , . But I don't see how to conclude from here that the two vectors and are equal. I'm guessing it has something to do with the fact that the two derivatives are equal for every ; if we somehow choose an such that the differential is injective, then its kernel is , and thus equality follows. However I doubt this is always possible, since in general , so a linear map from to cannot be injective. Any help justifying this step is much appreciated.","M V \mathbb{R}^n x \in M \varphi : I \to M I \mathbb{R} 0 \varphi(0) = x. D_{\varphi}: C^{\infty}(M) \to \mathbb{R} D_{\varphi}(f) = (f \circ \varphi)'(0) x \varphi \sim \psi D_{\varphi} = D_{\psi} \xi x (W, \alpha) \varphi \sim \psi f \in C^{\infty}(M) d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \varphi)'(0)) = d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \psi)'(0)) (\alpha \circ \varphi)'(0) (\alpha \circ \psi)'(0) f f d(f \circ \alpha^{-1})_{\alpha(x)} : V \to \mathbb{R} \{0\} \dim(V) > 1 V \mathbb{R}","['differential-geometry', 'differential-topology', 'smooth-manifolds', 'differential', 'tangent-spaces']"
4,Deforming antiholomorphic involutions,Deforming antiholomorphic involutions,,"Let $(M,J)$ be a compact smooth almost complex manifold. We can ""deform"" $J$ as follows: if $A$ is a smooth section of the endomorphism bundle $\mathrm{End}(TM)\to M$ satisfying $ AJ=-JA, $ it follows that $ Je^A=e^{-A}J, $ where $e^A$ is the matrix exponential of $A$ . It follows immediately that $$ J':=Je^A $$ is another almost complex structure on $M$ . Suppose $\phi:M\to M$ is a diffeomorphism which is an anti- $J$ -holomorphic involution, i.e. $$ J\circ d\phi=-d\phi \circ J \qquad \text{and} \qquad \phi\circ \phi =\mathrm{id}. $$ Question. Can we deform $\phi$ into an anti- $J'$ -holomorphic involution? I think this will be true if there exist a vector field $\eta\in \Gamma(TM)$ such that the diffeomorphism $\phi':M\to M$ defined by $$ \phi'(x)=(\phi\circ \mathrm{exp}\circ \eta)(x) $$ is an anti- $J'$ -holomorphic involution (here $\exp$ is defined with respect to some Riemannian metric on $M$ ). How can I show the existence/nonexistence of $\eta$ ?","Let be a compact smooth almost complex manifold. We can ""deform"" as follows: if is a smooth section of the endomorphism bundle satisfying it follows that where is the matrix exponential of . It follows immediately that is another almost complex structure on . Suppose is a diffeomorphism which is an anti- -holomorphic involution, i.e. Question. Can we deform into an anti- -holomorphic involution? I think this will be true if there exist a vector field such that the diffeomorphism defined by is an anti- -holomorphic involution (here is defined with respect to some Riemannian metric on ). How can I show the existence/nonexistence of ?","(M,J) J A \mathrm{End}(TM)\to M 
AJ=-JA,
 
Je^A=e^{-A}J,
 e^A A 
J':=Je^A
 M \phi:M\to M J 
J\circ d\phi=-d\phi \circ J \qquad \text{and} \qquad \phi\circ \phi =\mathrm{id}.
 \phi J' \eta\in \Gamma(TM) \phi':M\to M 
\phi'(x)=(\phi\circ \mathrm{exp}\circ \eta)(x)
 J' \exp M \eta","['differential-geometry', 'riemannian-geometry', 'complex-geometry', 'symplectic-geometry']"
5,Asymptotic expansion of heat operator $e^{-\Delta{t}}$ and $e^{-\mathcal{D}t}$ of Dirac operator,Asymptotic expansion of heat operator  and  of Dirac operator,e^{-\Delta{t}} e^{-\mathcal{D}t},"For a closed Riemannian manifold $M$ of $n$ -dimension, we consider the Laplace-Beltrami operator $\Delta$ . It is known that we have an asymptotic expansion for the trace of heat operator $e^{-\Delta{t}}$ as follows $$ \mathrm{tr}(e^{-\Delta{t}})=\sum_{\lambda}e^{-\lambda{t}}\overset{t\downarrow0}{\sim}t^{-\frac{n}{2}}\sum_{n} \alpha_{n}t^{n}, $$ where $\lambda$ runs over the set of spectrum of Laplacian $\Delta$ . My question is that Denote by $\mathcal{D}$ the Dirac operator whose square coincides with the Laplacian, $i.e.$ $\mathcal{D}^{2}=\Delta$ .    Then the sum of positive eigenvalues of an operator $e^{-\mathcal{D}t}$ $$\sum_{\lambda\in\mathrm{Sp}(\Delta)}e^{-\sqrt{\lambda}{t}}$$ has an asymptotic expansion around $t=0$ ?   If it exists, then is it possible to induce a relation between coefficients? I know that the proof for the case of heat operator follows from the construction of heat kernel. But I wonder that the same construction can be applied to the Dirac operator. Thank you for your time and effort.","For a closed Riemannian manifold of -dimension, we consider the Laplace-Beltrami operator . It is known that we have an asymptotic expansion for the trace of heat operator as follows where runs over the set of spectrum of Laplacian . My question is that Denote by the Dirac operator whose square coincides with the Laplacian, .    Then the sum of positive eigenvalues of an operator has an asymptotic expansion around ?   If it exists, then is it possible to induce a relation between coefficients? I know that the proof for the case of heat operator follows from the construction of heat kernel. But I wonder that the same construction can be applied to the Dirac operator. Thank you for your time and effort.","M n \Delta e^{-\Delta{t}} 
\mathrm{tr}(e^{-\Delta{t}})=\sum_{\lambda}e^{-\lambda{t}}\overset{t\downarrow0}{\sim}t^{-\frac{n}{2}}\sum_{n} \alpha_{n}t^{n},
 \lambda \Delta \mathcal{D} i.e. \mathcal{D}^{2}=\Delta e^{-\mathcal{D}t} \sum_{\lambda\in\mathrm{Sp}(\Delta)}e^{-\sqrt{\lambda}{t}} t=0","['differential-geometry', 'partial-differential-equations', 'asymptotics', 'spectral-theory', 'differential-operators']"
6,How is the differential of a Sobolev function on a manifold regarded as an a.e. defined section of $T^*M$?,How is the differential of a Sobolev function on a manifold regarded as an a.e. defined section of ?,T^*M,"Let $(M,g)$ be a smooth compact Riemannian manifold , and let $f \in W^{1,p}(M)$ for $p\ge 1$ . (I don't assume $p>\dim M$ ). I have seen in various sources that people refer to the weak derivative of $f$ as a linear functional $T_pM \to \mathbb{R}$ , which is defined for almost every $p \in M$ . (an a.e.defined section of $T^*M$ ). How exactly is this object defined? I couldn't find any precise details about this. I define $W^{1,p}(M)$ to be the completion of the space of compactly supported smooth functions $C_c^{\infty}(M)$ w.r.t the $\|\cdot\|_{1,p}$ norm. Optional: I suggest below $2$ approaches; I would like to know if they are compatible, i.e. if they both produce the same element in $(T_pM)^*$ . (Regarding the second approach, I am not even sure if it produces a well-defined functional). Approach 1: Given $f \in W^{1,p}(M)$ , there exist $f_n \in C_c^{\infty}(M)$ , $f_n \to f$ in $W^{1,p}$ . $df_n \in \Gamma(T^*M)$ is a Cauchy sequence in $L^p(M,T^*M)$ , where $L^p(M,T^*M)$ is the completion of the space of smooth sections $\Gamma(T^*M)$ w.r.t the natural $p$ -norm. By completeness, $df_n$ converges to an element in $L^p(M,T^*M)$ , which we can realize as a measurable section $T^*M$ . We set $df=\lim_{n \to \infty} df_n$ . Approach 2 (""Local picture""): Let $\phi:U\subseteq M \to \mathbb{R}^n$ be a surjective coordinate chart around $p \in M$ , and $\phi(p)=0$ . Set $f_{\phi}=f|_U \circ \phi^{-1} :\mathbb{R}^n \to \mathbb{R}$ . Then $f_{\phi} \in W^{1,p}(\mathbb{R}^n)$ (we might need to shrink $U$ to ensure nothing will explode). We define $df_p$ by the equation $$ df_p \circ d(\phi^{-1})_0(e_i):= d(f_{\phi})_0(e_i)=(\partial_i f_{\phi})(0). \tag{1}$$ Does equation $(1)$ well-defines an element in $T_p^*M$ independently of the coordinate chart?  Does it coincide with $\lim_{n \to \infty} df_n$ from the previous approach?","Let be a smooth compact Riemannian manifold , and let for . (I don't assume ). I have seen in various sources that people refer to the weak derivative of as a linear functional , which is defined for almost every . (an a.e.defined section of ). How exactly is this object defined? I couldn't find any precise details about this. I define to be the completion of the space of compactly supported smooth functions w.r.t the norm. Optional: I suggest below approaches; I would like to know if they are compatible, i.e. if they both produce the same element in . (Regarding the second approach, I am not even sure if it produces a well-defined functional). Approach 1: Given , there exist , in . is a Cauchy sequence in , where is the completion of the space of smooth sections w.r.t the natural -norm. By completeness, converges to an element in , which we can realize as a measurable section . We set . Approach 2 (""Local picture""): Let be a surjective coordinate chart around , and . Set . Then (we might need to shrink to ensure nothing will explode). We define by the equation Does equation well-defines an element in independently of the coordinate chart?  Does it coincide with from the previous approach?","(M,g) f \in W^{1,p}(M) p\ge 1 p>\dim M f T_pM \to \mathbb{R} p \in M T^*M W^{1,p}(M) C_c^{\infty}(M) \|\cdot\|_{1,p} 2 (T_pM)^* f \in W^{1,p}(M) f_n \in C_c^{\infty}(M) f_n \to f W^{1,p} df_n \in \Gamma(T^*M) L^p(M,T^*M) L^p(M,T^*M) \Gamma(T^*M) p df_n L^p(M,T^*M) T^*M df=\lim_{n \to \infty} df_n \phi:U\subseteq M \to \mathbb{R}^n p \in M \phi(p)=0 f_{\phi}=f|_U \circ \phi^{-1} :\mathbb{R}^n \to \mathbb{R} f_{\phi} \in W^{1,p}(\mathbb{R}^n) U df_p  df_p \circ d(\phi^{-1})_0(e_i):= d(f_{\phi})_0(e_i)=(\partial_i f_{\phi})(0). \tag{1} (1) T_p^*M \lim_{n \to \infty} df_n","['differential-geometry', 'differential-topology', 'sobolev-spaces', 'smooth-manifolds', 'weak-derivatives']"
7,Diffeomorphism between $TM$ and $M\times R^n$,Diffeomorphism between  and,TM M\times R^n,"Let $(M,\mathcal{A})$ be a manifold with smooth structure $\mathcal{A}$ . For any point $x\in M$ , we define a tangent at x by the triplet $(c,x,h)$ , where $c=(U,\phi)$ is a chart at $x$ , $h\in R^n$ ( $n$ is the dimension of the manifold). For two charts $c,c'$ , define the following equivalence relation: $(c_1,x,h_1)\sim (c_2,x,h_2)$ if $D(\psi\circ\phi^{-1})(\phi(x))h_1=h_2$ , where $c_1=(U,\phi), c_2=(V,\psi)$ . Now, the tangent vectors at $x$ is defined by $T_xM=\{[c,x,h]: x\in M, h\in R^n\}$ and $[c,x,h]$ is the equivalence class. Finally, the tangent space is defined as $TM=\cup_{x\in M} T_xM$ . My question is the following: Can we show that $TM$ is diffeomorphic to $M\times R^n$ using this definition of tangent space? In particular, is it true when $M=\mathbb{S}^{n-1}$ ?","Let be a manifold with smooth structure . For any point , we define a tangent at x by the triplet , where is a chart at , ( is the dimension of the manifold). For two charts , define the following equivalence relation: if , where . Now, the tangent vectors at is defined by and is the equivalence class. Finally, the tangent space is defined as . My question is the following: Can we show that is diffeomorphic to using this definition of tangent space? In particular, is it true when ?","(M,\mathcal{A}) \mathcal{A} x\in M (c,x,h) c=(U,\phi) x h\in R^n n c,c' (c_1,x,h_1)\sim (c_2,x,h_2) D(\psi\circ\phi^{-1})(\phi(x))h_1=h_2 c_1=(U,\phi), c_2=(V,\psi) x T_xM=\{[c,x,h]: x\in M, h\in R^n\} [c,x,h] TM=\cup_{x\in M} T_xM TM M\times R^n M=\mathbb{S}^{n-1}","['differential-geometry', 'smooth-manifolds', 'tangent-bundle']"
8,"Differentials of non-smooth functions, wedge products of currents?","Differentials of non-smooth functions, wedge products of currents?",,"In a paper of McMullen he considers foliations on a manifold determined by a closed 1-form $\rho$.  He says an $L^\infty$ function $f$ is constant on the leaves of the foliation if ""$df \wedge \rho = 0$ as a current,"" and I'm having a hard time unwinding exactly what's happening with the currents (which I'm not very familiar with). My understanding is that currents are functionals on differential forms, and given a function $f$ we can define a current on $n$-forms (for an $n$-dimensional manifold) by integrating $f$ times the $n$-form.  Similarly, the 1-form $\rho$ defines a current on the $(n-1)$-forms by integrating the wedge of the form with $\rho$.  But what's going on with $df \wedge \rho$?  I get that we can define a differential of a current by $[dT](\omega) = T(d\omega)$, so I guess $df$ refers to the current on $n-1$ forms which sends $\omega \mapsto \int f d\omega$, but how is the wedge with $\rho$ defined? My only guess is that $df \wedge \rho$ is a current acting on $(n-2)$ forms sending $\omega \mapsto \int f \cdot (d\omega \wedge \rho)$.  Even if that's correct, why does $f$ being constant on leaves imply this current is zero?  It seems strange to me that functions constant on the leaves of the foliation determined by $\rho$, which have codimension $1$, can be expressed in terms of $n-2$ forms.","In a paper of McMullen he considers foliations on a manifold determined by a closed 1-form $\rho$.  He says an $L^\infty$ function $f$ is constant on the leaves of the foliation if ""$df \wedge \rho = 0$ as a current,"" and I'm having a hard time unwinding exactly what's happening with the currents (which I'm not very familiar with). My understanding is that currents are functionals on differential forms, and given a function $f$ we can define a current on $n$-forms (for an $n$-dimensional manifold) by integrating $f$ times the $n$-form.  Similarly, the 1-form $\rho$ defines a current on the $(n-1)$-forms by integrating the wedge of the form with $\rho$.  But what's going on with $df \wedge \rho$?  I get that we can define a differential of a current by $[dT](\omega) = T(d\omega)$, so I guess $df$ refers to the current on $n-1$ forms which sends $\omega \mapsto \int f d\omega$, but how is the wedge with $\rho$ defined? My only guess is that $df \wedge \rho$ is a current acting on $(n-2)$ forms sending $\omega \mapsto \int f \cdot (d\omega \wedge \rho)$.  Even if that's correct, why does $f$ being constant on leaves imply this current is zero?  It seems strange to me that functions constant on the leaves of the foliation determined by $\rho$, which have codimension $1$, can be expressed in terms of $n-2$ forms.",,"['differential-geometry', 'distribution-theory', 'differential-forms', 'foliations']"
9,Systematic application of algebraic topology to energy minimization problems?,Systematic application of algebraic topology to energy minimization problems?,,"I have come across two different occurrences of energy minimization problems which find an interpretation using notions from algebraic topology, and I was wondering whether analogous situations have been studied in a systematic way somewhere in the literature. The first is a result by Cartan in Riemannian Geometry states that, given a compact Riemannian manifold $M$, every homotopy class of loops in the path space of $M$ contains a closed geodesic. The second is a result in Hodge theory stating the existence of an isomorphism:  $$ H^k_\Delta(M) \xrightarrow{\sim} H^k_{deR}(M) $$ between the $k$th de Rham cohomology group of $M$ and the vector space of harmonic $k$ forms on $M$. In both instances, objects which are defined as infimums of sorts (closed geodesics and harmonic forms) can be identified as representatives of homotopy/cohomology classes related to the underlying manifold. Is there a body of work in which the above two results are brought to a common ground?","I have come across two different occurrences of energy minimization problems which find an interpretation using notions from algebraic topology, and I was wondering whether analogous situations have been studied in a systematic way somewhere in the literature. The first is a result by Cartan in Riemannian Geometry states that, given a compact Riemannian manifold $M$, every homotopy class of loops in the path space of $M$ contains a closed geodesic. The second is a result in Hodge theory stating the existence of an isomorphism:  $$ H^k_\Delta(M) \xrightarrow{\sim} H^k_{deR}(M) $$ between the $k$th de Rham cohomology group of $M$ and the vector space of harmonic $k$ forms on $M$. In both instances, objects which are defined as infimums of sorts (closed geodesics and harmonic forms) can be identified as representatives of homotopy/cohomology classes related to the underlying manifold. Is there a body of work in which the above two results are brought to a common ground?",,"['differential-geometry', 'algebraic-topology', 'differential-topology', 'riemannian-geometry', 'hodge-theory']"
10,Intuition for warped product manifold.,Intuition for warped product manifold.,,"I am reading about wave equations in manifold and encountered the term warped product manifold . More specifically, in my case it is defined as follows, $$N:=[0,\phi^*)  \times_g \mathbb S^{k-1}$$   where $\phi^* \in \Bbb R\cup\{+\infty\}$ and $g:\Bbb R\to\Bbb R$ is an odd smooth function such that $g(0)=0, g'(0)=1$. On $N$ we have the ''polar'' coordinates $(\phi,\chi) \in [0,\phi^*)\times \mathbb S^{k-1}$. In these coordinates the metric of $N$ takes the form    $$ d\phi^2 + g^2(\phi)d\chi^2 $$   where $d\chi^2$ is the standard metric of $\mathbb S^{k-1}\hookrightarrow \Bbb R^k$. How should I think of $N$ is this case? More importantly, what is a warped product manifold in general? Any help is very appreciated.","I am reading about wave equations in manifold and encountered the term warped product manifold . More specifically, in my case it is defined as follows, $$N:=[0,\phi^*)  \times_g \mathbb S^{k-1}$$   where $\phi^* \in \Bbb R\cup\{+\infty\}$ and $g:\Bbb R\to\Bbb R$ is an odd smooth function such that $g(0)=0, g'(0)=1$. On $N$ we have the ''polar'' coordinates $(\phi,\chi) \in [0,\phi^*)\times \mathbb S^{k-1}$. In these coordinates the metric of $N$ takes the form    $$ d\phi^2 + g^2(\phi)d\chi^2 $$   where $d\chi^2$ is the standard metric of $\mathbb S^{k-1}\hookrightarrow \Bbb R^k$. How should I think of $N$ is this case? More importantly, what is a warped product manifold in general? Any help is very appreciated.",,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry', 'general-relativity', 'semi-riemannian-geometry']"
11,Equivalent definition of proper action with sequences,Equivalent definition of proper action with sequences,,"I have to prove the following : Let G be a metric group acting continuously on X a metric space. Then the following are equivalent : (a) $G\curvearrowright X$ is proper. (b) For all compact $K\subseteq X$ the set $C= \{g;g\cdot K\cap K\neq\emptyset\}\subseteq G$ is compact. (c) For all converging sequences $(x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$, for all diverging sequences $(g_{n})_{n\in\mathbb{N}}\in G^{\mathbb{N}}$, the sequence $(g_{n}\cdot x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$ diverges. I have managed to prove (a) $\Leftrightarrow$ (b), but I am stuck when it comes to prove (b) $\Rightarrow$ (c) or (c) $\Rightarrow$ (b). I don't see how the properness and the convergence of sequences are linked (I know that in a metric space, a definition of compactness is that any sequence has a converging subsequence, but I don't see how it helps). I know that G and X are $\sigma$ locally compact because of the definition of a proper action. Since X is a metric space it is in particular hausdorff and hence any compact subspace of X is closed as well. I tried to do it by contradiction. I assumed that the sequence $g_{n}.x_{n}$ was converging and then constructed a compact set $K \subset X$ so that $g_{n}\in C \forall n$. But I don't know how to do conclude from here that C is not compact from there. Thank you very much beforehand for your help","I have to prove the following : Let G be a metric group acting continuously on X a metric space. Then the following are equivalent : (a) $G\curvearrowright X$ is proper. (b) For all compact $K\subseteq X$ the set $C= \{g;g\cdot K\cap K\neq\emptyset\}\subseteq G$ is compact. (c) For all converging sequences $(x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$, for all diverging sequences $(g_{n})_{n\in\mathbb{N}}\in G^{\mathbb{N}}$, the sequence $(g_{n}\cdot x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$ diverges. I have managed to prove (a) $\Leftrightarrow$ (b), but I am stuck when it comes to prove (b) $\Rightarrow$ (c) or (c) $\Rightarrow$ (b). I don't see how the properness and the convergence of sequences are linked (I know that in a metric space, a definition of compactness is that any sequence has a converging subsequence, but I don't see how it helps). I know that G and X are $\sigma$ locally compact because of the definition of a proper action. Since X is a metric space it is in particular hausdorff and hence any compact subspace of X is closed as well. I tried to do it by contradiction. I assumed that the sequence $g_{n}.x_{n}$ was converging and then constructed a compact set $K \subset X$ so that $g_{n}\in C \forall n$. But I don't know how to do conclude from here that C is not compact from there. Thank you very much beforehand for your help",,"['differential-geometry', 'lie-groups', 'group-actions']"
12,"de-Rham cohomology $H^1(S)$ and $ \operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))$ are isomorphic?",de-Rham cohomology  and  are isomorphic?,"H^1(S)  \operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))","Let $S$ be a connected smooth 2-dimensional manifold. Let $H^1(S)$ be its first De-rham cohomology group. \begin{align*}\operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))=\{f \vert f :\pi_1(S, s_0)\to (\mathbb{R},+)\text{ is a group homomorphism} \}\end{align*} Define\begin{align*} \Phi:H^1(S)\to \operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))  \end{align*} as $ \Phi( [\alpha])([c])=\int_c \alpha $ I know that $\Phi$ is an injective linear map. How can I show that it is surjective?","Let $S$ be a connected smooth 2-dimensional manifold. Let $H^1(S)$ be its first De-rham cohomology group. \begin{align*}\operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))=\{f \vert f :\pi_1(S, s_0)\to (\mathbb{R},+)\text{ is a group homomorphism} \}\end{align*} Define\begin{align*} \Phi:H^1(S)\to \operatorname{Hom}(\pi_1(S, s_0), (\mathbb{R},+))  \end{align*} as $ \Phi( [\alpha])([c])=\int_c \alpha $ I know that $\Phi$ is an injective linear map. How can I show that it is surjective?",,"['differential-geometry', 'algebraic-topology', 'manifolds', 'homology-cohomology', 'fundamental-groups']"
13,"Let $M$ is compact Riemann surface, if $\omega$ is a 2-form and $\int_{M} \omega =0$ then there exists a smooth function $f$ such that $\omega=d*df$","Let  is compact Riemann surface, if  is a 2-form and  then there exists a smooth function  such that",M \omega \int_{M} \omega =0 f \omega=d*df,"I want to show that: $(*)$If $\omega \in \Omega^{2}(M)$, which $M$ is compact Riemann surface and $\Omega^{2}(M)$ means 2-form, and $\int_{M} \omega =0$, then there exists a smooth function $f$(i.e. $f\in \Omega^{0}(M)$) such that $\omega=d*df$ . I try to imitate the method of proof of the $\textbf{Hodge Decomposition}$: Let $\omega \in A^1(\Sigma)$, then $$\omega=\omega_{h}+df+*dg,$$   where $\omega_{h}\in H^1$ and $f,g\in A^0(\Sigma).$ The proof of the Hodge Decomposition as following Three Steps: Step 1. To establish a complete Hilbert space Step 2. To seek $df, *dg$ (Similar to the existing of the solution of PDE) Step 3. regularity. (To use the Weyl Lemma.) Define $X=\{\phi \in \Omega^{0}(M): \int_{M} \phi d\sigma=0\}$ with $\int_{M}d\sigma=1$ and $\forall \psi, \phi \in X$, define the inner product, $$(\psi,\phi)=\langle d\psi, d\phi\rangle=\int_{\Sigma} d\psi \wedge *d\phi.$$ We could show that $\bar{X}$ is a complete Hilbert space. Moreover, if $f\in \bar{X}$, then $f\in X$(i.e. $\bar{X}\subset L^2(X,d\sigma)$). Then $\forall \phi \in C^{\infty}(X)$, we seek $g$ s.t.  $$ \int_{\Sigma}d\phi \wedge \omega =\int_{\Sigma}d\phi \wedge *dg.$$ Here we could define a bounded linear functional $l$, which  $$l: X\rightarrow \mathbb{C}$$ $$\phi \longmapsto  \int_{\Sigma}d\phi \wedge \omega$$ Using the Resiz representation theorem, $\exists g\in \bar{X}$, s.t. $$l(\phi)=\langle \phi, \bar{g}\rangle, \forall g\in \bar{X}.$$ where using the regularity implies $g\in X$. In the next,  $$\int_{M}\phi \omega= \int_{M} f d*d\phi=\int_{M} d\phi \wedge *df \ (*),$$ we get  $$ \int_{M} \phi (\omega-d*df)=0, \forall \phi \in C^{\infty}(X),$$ Hence, $\omega-*dg$ is closed form. Finished the proof of Hodge theorem. How to construct similarly of such linear functional to prove the problem $(*)?","I want to show that: $(*)$If $\omega \in \Omega^{2}(M)$, which $M$ is compact Riemann surface and $\Omega^{2}(M)$ means 2-form, and $\int_{M} \omega =0$, then there exists a smooth function $f$(i.e. $f\in \Omega^{0}(M)$) such that $\omega=d*df$ . I try to imitate the method of proof of the $\textbf{Hodge Decomposition}$: Let $\omega \in A^1(\Sigma)$, then $$\omega=\omega_{h}+df+*dg,$$   where $\omega_{h}\in H^1$ and $f,g\in A^0(\Sigma).$ The proof of the Hodge Decomposition as following Three Steps: Step 1. To establish a complete Hilbert space Step 2. To seek $df, *dg$ (Similar to the existing of the solution of PDE) Step 3. regularity. (To use the Weyl Lemma.) Define $X=\{\phi \in \Omega^{0}(M): \int_{M} \phi d\sigma=0\}$ with $\int_{M}d\sigma=1$ and $\forall \psi, \phi \in X$, define the inner product, $$(\psi,\phi)=\langle d\psi, d\phi\rangle=\int_{\Sigma} d\psi \wedge *d\phi.$$ We could show that $\bar{X}$ is a complete Hilbert space. Moreover, if $f\in \bar{X}$, then $f\in X$(i.e. $\bar{X}\subset L^2(X,d\sigma)$). Then $\forall \phi \in C^{\infty}(X)$, we seek $g$ s.t.  $$ \int_{\Sigma}d\phi \wedge \omega =\int_{\Sigma}d\phi \wedge *dg.$$ Here we could define a bounded linear functional $l$, which  $$l: X\rightarrow \mathbb{C}$$ $$\phi \longmapsto  \int_{\Sigma}d\phi \wedge \omega$$ Using the Resiz representation theorem, $\exists g\in \bar{X}$, s.t. $$l(\phi)=\langle \phi, \bar{g}\rangle, \forall g\in \bar{X}.$$ where using the regularity implies $g\in X$. In the next,  $$\int_{M}\phi \omega= \int_{M} f d*d\phi=\int_{M} d\phi \wedge *df \ (*),$$ we get  $$ \int_{M} \phi (\omega-d*df)=0, \forall \phi \in C^{\infty}(X),$$ Hence, $\omega-*dg$ is closed form. Finished the proof of Hodge theorem. How to construct similarly of such linear functional to prove the problem $(*)?",,"['differential-geometry', 'partial-differential-equations']"
14,"Show $ |k(x_o)| \geq \frac{1}{\|c(x_o)\|} $ , $c$ plane curve and $k(x)$ curvature.","Show  ,  plane curve and  curvature.", |k(x_o)| \geq \frac{1}{\|c(x_o)\|}  c k(x),"Let $ c:(a,b)→\mathbb{R^2}$ be a regular arc length parametrized plane curve. Assume that there exists $x_o$ , $a<x_0<b$, such that the function  $x \rightarrow \|c(x)\|$ has a maximum in $x_0$. Prove: $$ |k(x_o)| \geq \frac{1}{\|c(x_o)\|} $$ ($k(x)$ curvature) My Work: We define a function $f : (a,b) \rightarrow \mathbb{R^2}$ , $ x \rightarrow  \|c(x)\|^2 $. We get: $$f'(x) = 2\langle c'(x),c(x)\rangle$$ and $$ f''(x) = 2( \langle c''(x),c(x)\rangle + \langle c'(x),c'(x)\rangle) = 2(\langle c''(x),c(x)\rangle + 1) = 2(\langle k(x)n(x),c(x)\rangle +1) = 2(k(x)\langle n(x),c(x)\rangle +1) $$ Is this right? ( $k(x)$ curvature , $n(x)$ normal ) f has a maximum in $x_o \Rightarrow f'(x_0) = 0$ and $f''(x_0) \leq 0$. Unfortunately I'm stuck here. Can somebody help me to finish this proof? Maybe we could consider $f''(x_o) = 2(k(x_o)\langle n(x_o),c(x_o)\rangle +1) \leq 0?$","Let $ c:(a,b)→\mathbb{R^2}$ be a regular arc length parametrized plane curve. Assume that there exists $x_o$ , $a<x_0<b$, such that the function  $x \rightarrow \|c(x)\|$ has a maximum in $x_0$. Prove: $$ |k(x_o)| \geq \frac{1}{\|c(x_o)\|} $$ ($k(x)$ curvature) My Work: We define a function $f : (a,b) \rightarrow \mathbb{R^2}$ , $ x \rightarrow  \|c(x)\|^2 $. We get: $$f'(x) = 2\langle c'(x),c(x)\rangle$$ and $$ f''(x) = 2( \langle c''(x),c(x)\rangle + \langle c'(x),c'(x)\rangle) = 2(\langle c''(x),c(x)\rangle + 1) = 2(\langle k(x)n(x),c(x)\rangle +1) = 2(k(x)\langle n(x),c(x)\rangle +1) $$ Is this right? ( $k(x)$ curvature , $n(x)$ normal ) f has a maximum in $x_o \Rightarrow f'(x_0) = 0$ and $f''(x_0) \leq 0$. Unfortunately I'm stuck here. Can somebody help me to finish this proof? Maybe we could consider $f''(x_o) = 2(k(x_o)\langle n(x_o),c(x_o)\rangle +1) \leq 0?$",,"['differential-geometry', 'normed-spaces', 'curves', 'curvature']"
15,Confusion over notion of compact manifold with or without boundary,Confusion over notion of compact manifold with or without boundary,,"I am trying to understand how to do the following two questions concerning compact manifolds: Show that$\ M\ $is a compact manifold in $\mathbb{R}^{n},\ $then $\partial\ M\ $is also compact; if also $\ M\ $is $\ n\ $-dimensional, then $\partial\ M=\ $bdry$\ M\ $ Show that a compact manifold cannot be represented by a (single) parametric equation. My confusion is what it means for a manifold to be compact.  In some books having to do with advanced calculus or theory of manifolds, it states that the notion of compactness when describing manifolds is distinct from the topological notion of compactness.  I also come across questions where it asks to prove properties about compact manifolds without boundary.  This just makes it more confusing. It is like saying a closed interval, circle or sphere has no boundary points or an empty boundary, but yet it is closed and bounded.  On Wikipedia, compact manifold is discussed in the topic of closed manifolds.  Again, I am encountering topological notions associated with compactness.  But textbooks says otherwise. Can someone please help me with some clarifications over my confusions please.  Thank you in advance","I am trying to understand how to do the following two questions concerning compact manifolds: Show that$\ M\ $is a compact manifold in $\mathbb{R}^{n},\ $then $\partial\ M\ $is also compact; if also $\ M\ $is $\ n\ $-dimensional, then $\partial\ M=\ $bdry$\ M\ $ Show that a compact manifold cannot be represented by a (single) parametric equation. My confusion is what it means for a manifold to be compact.  In some books having to do with advanced calculus or theory of manifolds, it states that the notion of compactness when describing manifolds is distinct from the topological notion of compactness.  I also come across questions where it asks to prove properties about compact manifolds without boundary.  This just makes it more confusing. It is like saying a closed interval, circle or sphere has no boundary points or an empty boundary, but yet it is closed and bounded.  On Wikipedia, compact manifold is discussed in the topic of closed manifolds.  Again, I am encountering topological notions associated with compactness.  But textbooks says otherwise. Can someone please help me with some clarifications over my confusions please.  Thank you in advance",,"['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
16,Finding canonical coordinates for a given symplectic form in $\Bbb S^2$.,Finding canonical coordinates for a given symplectic form in .,\Bbb S^2,"Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$ (that is, the usual area form). If $f \in \mathcal{C}^\infty(\Bbb S^2)$ is positive, then $\omega^f \doteq f\omega$ is also a symplectic form. I would guess that there is a smart way to find coordinates $(\theta,\varphi)$ in $\Bbb S^2$ so that $\omega^f = {\rm d}\theta \wedge {\rm d}\varphi$, but I'm a bit at a loss of how to do that. Attempt: (?) For $f = 1$, $\omega^f = \omega$ is the usual area form, and $x = \sqrt{1-z^2} \cos \theta$, $y = \sqrt{1-z^2}\sin \theta$ does the job . So I tried calling $$\begin{cases} x(\theta,\varphi) \doteq g(\varphi) \cos \theta \quad{\rm and} \\ y(\theta, \varphi) = g(\varphi) \sin \theta, \end{cases}$$with no restriction on $z = z(\theta,\varphi)$, a priori. The condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ gives $\partial z/\partial \theta = 0$. With this  I painstakingly computed the pull-back by expressing ${\rm d}x, {\rm d}y$ and ${\rm d}z$ in terms of ${\rm d}\theta$ and ${\rm d}\varphi$, and substituting in $$\omega^f = fx\,{\rm d}y \wedge {\rm d}z + fy\,{\rm d}z\wedge{\rm d}x + fz\,{\rm d}x \wedge {\rm d}y,$$and got the relation $$f g^2 \frac{\partial z}{\partial \varphi} - fz gg' = 1.$$Since the condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ also gives $gg' + z \partial z/\partial \varphi = 0$, that simplifies to $$f\frac{\partial z}{\partial \varphi} = 1.$$ This smells bad. Is my attempt too wrong? What is the intelligent way to solve the problem? It is exercise 5.1.1 here in case you're too curious. I'm in that stage of learning that you go around scavenging for nice exercises.","Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$ (that is, the usual area form). If $f \in \mathcal{C}^\infty(\Bbb S^2)$ is positive, then $\omega^f \doteq f\omega$ is also a symplectic form. I would guess that there is a smart way to find coordinates $(\theta,\varphi)$ in $\Bbb S^2$ so that $\omega^f = {\rm d}\theta \wedge {\rm d}\varphi$, but I'm a bit at a loss of how to do that. Attempt: (?) For $f = 1$, $\omega^f = \omega$ is the usual area form, and $x = \sqrt{1-z^2} \cos \theta$, $y = \sqrt{1-z^2}\sin \theta$ does the job . So I tried calling $$\begin{cases} x(\theta,\varphi) \doteq g(\varphi) \cos \theta \quad{\rm and} \\ y(\theta, \varphi) = g(\varphi) \sin \theta, \end{cases}$$with no restriction on $z = z(\theta,\varphi)$, a priori. The condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ gives $\partial z/\partial \theta = 0$. With this  I painstakingly computed the pull-back by expressing ${\rm d}x, {\rm d}y$ and ${\rm d}z$ in terms of ${\rm d}\theta$ and ${\rm d}\varphi$, and substituting in $$\omega^f = fx\,{\rm d}y \wedge {\rm d}z + fy\,{\rm d}z\wedge{\rm d}x + fz\,{\rm d}x \wedge {\rm d}y,$$and got the relation $$f g^2 \frac{\partial z}{\partial \varphi} - fz gg' = 1.$$Since the condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ also gives $gg' + z \partial z/\partial \varphi = 0$, that simplifies to $$f\frac{\partial z}{\partial \varphi} = 1.$$ This smells bad. Is my attempt too wrong? What is the intelligent way to solve the problem? It is exercise 5.1.1 here in case you're too curious. I'm in that stage of learning that you go around scavenging for nice exercises.",,"['differential-geometry', 'partial-differential-equations', 'differential-forms', 'symplectic-geometry']"
17,Does the tangent bundle on a 2-sphere span $\mathbb R^3$ and how are the operations defined?,Does the tangent bundle on a 2-sphere span  and how are the operations defined?,\mathbb R^3,"As a follow-up to this question I would like to clarify whether the tangent bundle on a sphere in $\mathbb R^3$ spans $\mathbb R^3$ to make sure I get the concept. The tangent bundle is the set of the tangent planes at every single point on the surface of the 2-sphere $S$ and would be defined as $$TS := \{(p, v):\ p\in S, v\in T_pS\}$$ If I get the idea correctly, there would be tangent planes through each point on the surface like the following ones in the drawing representing 3 single points ($P, S, Q)$: Each plane would be translated to go through the origin to construct a vector space of tangent planes: If the above is correct, the intuition is clear: the tangent bundle on the sphere would enable us to find a plane in any possible orientation, and hence, the disjoint union of these tangent planes would span $\mathbb R^3.$ Or is the disjoint piece a game changer? QUESTIONS: Does this ""fan"" of translated tangent planes span $\mathbb R^3$? And how are the addition and scalar multiplication of a vector space defined on this tangent bundle?","As a follow-up to this question I would like to clarify whether the tangent bundle on a sphere in $\mathbb R^3$ spans $\mathbb R^3$ to make sure I get the concept. The tangent bundle is the set of the tangent planes at every single point on the surface of the 2-sphere $S$ and would be defined as $$TS := \{(p, v):\ p\in S, v\in T_pS\}$$ If I get the idea correctly, there would be tangent planes through each point on the surface like the following ones in the drawing representing 3 single points ($P, S, Q)$: Each plane would be translated to go through the origin to construct a vector space of tangent planes: If the above is correct, the intuition is clear: the tangent bundle on the sphere would enable us to find a plane in any possible orientation, and hence, the disjoint union of these tangent planes would span $\mathbb R^3.$ Or is the disjoint piece a game changer? QUESTIONS: Does this ""fan"" of translated tangent planes span $\mathbb R^3$? And how are the addition and scalar multiplication of a vector space defined on this tangent bundle?",,['differential-geometry']
18,Could anyone pass me some survey or books that display what is known about the surfaces of constant mean curvature?,Could anyone pass me some survey or books that display what is known about the surfaces of constant mean curvature?,,"Could anyone pass me some article (survey) or books that display what is known about the surfaces of constant mean curvature? Preferably to articles or books with a lighter reading, otherwise you can send what you have. ;) I have searched the internet, but I find little of what I want.","Could anyone pass me some article (survey) or books that display what is known about the surfaces of constant mean curvature? Preferably to articles or books with a lighter reading, otherwise you can send what you have. ;) I have searched the internet, but I find little of what I want.",,"['differential-geometry', 'reference-request', 'riemannian-geometry', 'book-recommendation', 'surfaces']"
19,Existence of a parallel vector field,Existence of a parallel vector field,,"I came across the following sentence in a comment on this  question: Local existence of parallel vector field ""the existence of a parallel vector field is equivalent to the condition that the metric splits locally into a Riemannian product of a one-dimensional manifold and an (n−1)-dimensional one. This implies, in particular, that the sectional curvatures of planes containing V are all zero."" and I want to know what the poster mean by the metric splitting locally into a Riemannian product and what that has to do with sectional curvatures.","I came across the following sentence in a comment on this  question: Local existence of parallel vector field ""the existence of a parallel vector field is equivalent to the condition that the metric splits locally into a Riemannian product of a one-dimensional manifold and an (n−1)-dimensional one. This implies, in particular, that the sectional curvatures of planes containing V are all zero."" and I want to know what the poster mean by the metric splitting locally into a Riemannian product and what that has to do with sectional curvatures.",,['differential-geometry']
20,Guillemin-Pollack: application of the Transversality Theorem,Guillemin-Pollack: application of the Transversality Theorem,,"I'm working on two exercises from Guillemin-Pollack which have the same flavor: (General Position Lemma) Let $X$ and $Y$ be submanifolds of $\mathbb R^N$. Show that for almost   every $a\in \mathbb R^N$, the translate $X+a$ intersects $Y$   transversally. Suppose that $X$ is a submanifold of $\mathbb R^N$. Show that almost every vector space $V$ of any fixed dimension $l$ in $\mathbb R^N$ intersects $X$ transversally. [HINT: The set $S\subset  (\mathbb R^N)^l$ consisting of all linearly independent $l$-tuples if vectors in $\mathbb R^N$ is open in $\mathbb R^{Nl}$, and the map $\mathbb R^l\times S\rightarrow \mathbb R^N$ defined by $[(t_1,\dots,t_l),v_1,\dots,v_l]\mapsto t_1v_1+\dots t_lv_l$ is a submersion.] In both cases, I believe I need to apply the following version of the Transversality Theorem (see this answer): Theorem : Suppose that $F:X\times S\to Y$ is a smooth map of manifolds and $Z$ is a submanifold of $Y$, all manifolds without boundary . If $F$ is transverse to $Z$ then for almost every $s\in S$ the map $f_s : x\mapsto F(x,s)$ is transverse to $Z$. I have the same problem in both exercises. In the first exercise, the theorem guarantees that for almost every $a\in \mathbb R^N$, the map $f_a: X\rightarrow \mathbb R^N$ given by $x\mapsto x+a$ is transversal to $Y$. Note that the image of this map is $X+a$. I need to show that the image is transversal to $Y$. How does it follow? In the second exercise, the theorem guarantees that for almost every $v=(v_1,\dots,v_l)\in S$, the map $f_v: \mathbb R^l\rightarrow \mathbb R^N$ given by $(t_1,\dots,t_l)\mapsto t_1v_1+\dots+t_lv_l$ is transversal to $X$. Note that the image of this map is an $l$-dimensional subspace of $\mathbb R^N$. I need to show that the image is transversal to $X$. How does it follow?","I'm working on two exercises from Guillemin-Pollack which have the same flavor: (General Position Lemma) Let $X$ and $Y$ be submanifolds of $\mathbb R^N$. Show that for almost   every $a\in \mathbb R^N$, the translate $X+a$ intersects $Y$   transversally. Suppose that $X$ is a submanifold of $\mathbb R^N$. Show that almost every vector space $V$ of any fixed dimension $l$ in $\mathbb R^N$ intersects $X$ transversally. [HINT: The set $S\subset  (\mathbb R^N)^l$ consisting of all linearly independent $l$-tuples if vectors in $\mathbb R^N$ is open in $\mathbb R^{Nl}$, and the map $\mathbb R^l\times S\rightarrow \mathbb R^N$ defined by $[(t_1,\dots,t_l),v_1,\dots,v_l]\mapsto t_1v_1+\dots t_lv_l$ is a submersion.] In both cases, I believe I need to apply the following version of the Transversality Theorem (see this answer): Theorem : Suppose that $F:X\times S\to Y$ is a smooth map of manifolds and $Z$ is a submanifold of $Y$, all manifolds without boundary . If $F$ is transverse to $Z$ then for almost every $s\in S$ the map $f_s : x\mapsto F(x,s)$ is transverse to $Z$. I have the same problem in both exercises. In the first exercise, the theorem guarantees that for almost every $a\in \mathbb R^N$, the map $f_a: X\rightarrow \mathbb R^N$ given by $x\mapsto x+a$ is transversal to $Y$. Note that the image of this map is $X+a$. I need to show that the image is transversal to $Y$. How does it follow? In the second exercise, the theorem guarantees that for almost every $v=(v_1,\dots,v_l)\in S$, the map $f_v: \mathbb R^l\rightarrow \mathbb R^N$ given by $(t_1,\dots,t_l)\mapsto t_1v_1+\dots+t_lv_l$ is transversal to $X$. Note that the image of this map is an $l$-dimensional subspace of $\mathbb R^N$. I need to show that the image is transversal to $X$. How does it follow?",,"['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds', 'transversality']"
21,Compute Thom and Euler class,Compute Thom and Euler class,,"If $\gamma \colon S^1 \rightarrow SO(2)$, we define $E_{\gamma}= D_{a}^2 \times \mathbb{R}^2 \sqcup D_{b}^2 \times \mathbb{R}^2 / \sim$, where $(x,v) \sim (x,\gamma(x)\cdot v)$ for $(x,v)\in S_{a}^1 \times \mathbb{R}^2$.   I need to prove that it is an oriented vector bundle, and compute the Euler class in terms of $\gamma$. I have defined $\pi \colon E_{\gamma} \rightarrow S^2$ to be $\pi([(x,v])=[x]$, if we take $S^2= D_{a}^2 \sqcup D_{b}^2 / (S_{a}^1 \sim S_{b}^1)$. I have proved that it is a vector bundle of dimension 2, but I have troubles when computing the Thom class and the Euler class. First of all, I have to define a Riemannian metric on the bundle. I have defined it in this way: $g\colon E_{\gamma} \times_{\pi} E_{\gamma} \rightarrow \mathbb{R}$ such that $g([(x_{1},v_{1})],[(x_{1},v_{2}])=\langle v_{1},v_{2} \rangle$. To compute $D(E_{\gamma})= \{[(x,v)]\in E_{\gamma} \mid g([(x,v)],[(x,v)])\leq 1 \}$, I have thought that it is $\text{int}(D_{a}^2)\times D^2 \cup \text{int}(D_{b}^2)\times D^2  \cup S^1 \times D^2$, but that is not possible since $H^2(D(E_{\gamma}))$ must be $\mathbb{Z}$. Can anyone help me with the Thom and Euler class, please? Thank you.","If $\gamma \colon S^1 \rightarrow SO(2)$, we define $E_{\gamma}= D_{a}^2 \times \mathbb{R}^2 \sqcup D_{b}^2 \times \mathbb{R}^2 / \sim$, where $(x,v) \sim (x,\gamma(x)\cdot v)$ for $(x,v)\in S_{a}^1 \times \mathbb{R}^2$.   I need to prove that it is an oriented vector bundle, and compute the Euler class in terms of $\gamma$. I have defined $\pi \colon E_{\gamma} \rightarrow S^2$ to be $\pi([(x,v])=[x]$, if we take $S^2= D_{a}^2 \sqcup D_{b}^2 / (S_{a}^1 \sim S_{b}^1)$. I have proved that it is a vector bundle of dimension 2, but I have troubles when computing the Thom class and the Euler class. First of all, I have to define a Riemannian metric on the bundle. I have defined it in this way: $g\colon E_{\gamma} \times_{\pi} E_{\gamma} \rightarrow \mathbb{R}$ such that $g([(x_{1},v_{1})],[(x_{1},v_{2}])=\langle v_{1},v_{2} \rangle$. To compute $D(E_{\gamma})= \{[(x,v)]\in E_{\gamma} \mid g([(x,v)],[(x,v)])\leq 1 \}$, I have thought that it is $\text{int}(D_{a}^2)\times D^2 \cup \text{int}(D_{b}^2)\times D^2  \cup S^1 \times D^2$, but that is not possible since $H^2(D(E_{\gamma}))$ must be $\mathbb{Z}$. Can anyone help me with the Thom and Euler class, please? Thank you.",,"['differential-geometry', 'algebraic-topology', 'vector-bundles', 'algebraic-k-theory']"
22,Orientability and trivialization of the tangent bundle over the 1-skeleton,Orientability and trivialization of the tangent bundle over the 1-skeleton,,"I was reading the following post on mathoverflow: https://mathoverflow.net/questions/80081/what-are-good-examples-of-spin-manifolds in an answer there is written that Orientability means the tangent bundle trivializes over a 1-skeleton. Dually you could think of that as saying the complement of a co-dimension 2 subcomplex has a trivial tangent bundle. As Lee Mosher suggests in the comment section, this means that the tangent bundle of an $n$ dimensional $M$ (homeomorphic to a simplicial complex) is trivial when restricted to the $1$-skeleton of $M$. Unfortunately, I do not manage to prove that this is equivalent to the definition of orientability that I am used to (for smooth manifolds). Namely that exists a  never vanishing section of the bundle of $n$-forms over an $n$ dimensional manifold. Can someone explain it to me? Question: Show that the above definition of orientability  (in the smooth setting) is equivalent to the existence of a volume form.   Also I wonder why we have that ""dual"" definition.","I was reading the following post on mathoverflow: https://mathoverflow.net/questions/80081/what-are-good-examples-of-spin-manifolds in an answer there is written that Orientability means the tangent bundle trivializes over a 1-skeleton. Dually you could think of that as saying the complement of a co-dimension 2 subcomplex has a trivial tangent bundle. As Lee Mosher suggests in the comment section, this means that the tangent bundle of an $n$ dimensional $M$ (homeomorphic to a simplicial complex) is trivial when restricted to the $1$-skeleton of $M$. Unfortunately, I do not manage to prove that this is equivalent to the definition of orientability that I am used to (for smooth manifolds). Namely that exists a  never vanishing section of the bundle of $n$-forms over an $n$ dimensional manifold. Can someone explain it to me? Question: Show that the above definition of orientability  (in the smooth setting) is equivalent to the existence of a volume form.   Also I wonder why we have that ""dual"" definition.",,"['differential-geometry', 'algebraic-topology', 'differential-topology', 'smooth-manifolds']"
23,Arclength Parameterization of the Trefoil Knot,Arclength Parameterization of the Trefoil Knot,,"I would like to find an arclength parameterization of the trefoil knot The parameterizations I can find are: $(sin(t) + 2sin(2t),$ $cos(t) - 2cos(2t),$ $sin(3t))$ and $((2+cos(3t))cos(2t),$ $(2+cos(3t))sin(2t),$ $sin(3t))$ for $t \in [0,2\pi)$ writing $t = t(\theta)$, the magnitude of the derivatives wrt. $\theta$ of these two parameterizations are: $| \frac{d}{d\theta}  (sin(t) + 2sin(2t),$ $cos(t) - 2cos(2t),$ $sin(3t)) |$ = $| (cos(t) + 4cos(2t),$ $-sin(t) + 4cos(2t),$ $3cos(3t))\frac{dt}{d\theta} |$ = $\sqrt{cos^2(t) + sin^2(t) + 16[cos^2(2t) + sin^2(2t)] + 8[cos(t)cos(2t) - sin(t)sin(2t)] + 9cos^2(3t)}\frac{dt}{d\theta}$ = $\sqrt{17 + 8cos(3t) + 9cos^2(3t)}\frac{dt}{d\theta}$ and similarly $|\frac{d}{d\theta}((2+cos(3t))cos(2t),$ $(2+cos(3t))sin(2t),$ $sin(3t))|$ = $\sqrt{25 + 16cos(3t) + 4cos^2(3t)}\frac{dt}{d\theta}$ I need a solution to, for example $\int \sqrt{17 + 8cos(3t) + 9cos^2(3t)}dt$ so that I can get $t$ in terms of $\theta$ Wolfram alpha doesn't like either of these integrals, and I can see no way to solve them. There are two types of answers to this question: one would solve one of these integrals, another would tell me how to change the parameterization, reasonably, so that the integral, and the obtained formula for t, is solvable. I guess the third is to tell me that this question isn't solvable like this. My thoughts on the latter: Below, I make the integral solvable by changing the parameterization, but I can't solve for $t(\theta)$ The freedom in the parameterizations can be expressed as: $(sin(t) + Asin(2t),$ $cos(t) - Acos(2t),$ $Bsin(3t))$ where $A > 1$ and $B > 0$ which gives integrand: $\sqrt{1 + 4A^2 + 4Acos(3t) + 9B^2cos^2(3t)}$ To eliminate the $\sqrt{}$ we want some factorization: $(1 + 4A^2 + 4Acos(3t) + 9B^2cos^2(3t)) = (3Bcos(3t) + \lambda)^2$ where $\lambda^2 = 1 + 4A^2$ and $6B\lambda = 4A$ so $\lambda = \frac{2A}{3B} $ so $\frac{4A^2}{9B^2} = 1 + 4A^2$ ie. $4A^2\frac{9B^2 - 1}{9B^2} = -1$ or $4A^2 = \frac{9B^2}{1 - 9B^2} > 4$ so $9B^2 > 4/5$ and I will choose $9B^2 = 9/10$ giving: $B = \frac{1}{\sqrt{10}}$ $A = 3/2$ $\lambda = \sqrt{10}$ ie. the trefoil $(sin(t) + \frac{3}{2}sin(2t),$ $cos(t) - \frac{3}{2}cos(2t),$ $\frac{1}{\sqrt{10}}sin(3t))$ has arclength parameterization given by $t(\theta)$ where: $\int (\frac{3}{\sqrt{10}} cos(3t) + \sqrt{10}) dt = \theta$ $\frac{sin(3t)}{\sqrt{10}} + t\sqrt{10} = \theta$ But I want $t$ in terms of $\theta$","I would like to find an arclength parameterization of the trefoil knot The parameterizations I can find are: $(sin(t) + 2sin(2t),$ $cos(t) - 2cos(2t),$ $sin(3t))$ and $((2+cos(3t))cos(2t),$ $(2+cos(3t))sin(2t),$ $sin(3t))$ for $t \in [0,2\pi)$ writing $t = t(\theta)$, the magnitude of the derivatives wrt. $\theta$ of these two parameterizations are: $| \frac{d}{d\theta}  (sin(t) + 2sin(2t),$ $cos(t) - 2cos(2t),$ $sin(3t)) |$ = $| (cos(t) + 4cos(2t),$ $-sin(t) + 4cos(2t),$ $3cos(3t))\frac{dt}{d\theta} |$ = $\sqrt{cos^2(t) + sin^2(t) + 16[cos^2(2t) + sin^2(2t)] + 8[cos(t)cos(2t) - sin(t)sin(2t)] + 9cos^2(3t)}\frac{dt}{d\theta}$ = $\sqrt{17 + 8cos(3t) + 9cos^2(3t)}\frac{dt}{d\theta}$ and similarly $|\frac{d}{d\theta}((2+cos(3t))cos(2t),$ $(2+cos(3t))sin(2t),$ $sin(3t))|$ = $\sqrt{25 + 16cos(3t) + 4cos^2(3t)}\frac{dt}{d\theta}$ I need a solution to, for example $\int \sqrt{17 + 8cos(3t) + 9cos^2(3t)}dt$ so that I can get $t$ in terms of $\theta$ Wolfram alpha doesn't like either of these integrals, and I can see no way to solve them. There are two types of answers to this question: one would solve one of these integrals, another would tell me how to change the parameterization, reasonably, so that the integral, and the obtained formula for t, is solvable. I guess the third is to tell me that this question isn't solvable like this. My thoughts on the latter: Below, I make the integral solvable by changing the parameterization, but I can't solve for $t(\theta)$ The freedom in the parameterizations can be expressed as: $(sin(t) + Asin(2t),$ $cos(t) - Acos(2t),$ $Bsin(3t))$ where $A > 1$ and $B > 0$ which gives integrand: $\sqrt{1 + 4A^2 + 4Acos(3t) + 9B^2cos^2(3t)}$ To eliminate the $\sqrt{}$ we want some factorization: $(1 + 4A^2 + 4Acos(3t) + 9B^2cos^2(3t)) = (3Bcos(3t) + \lambda)^2$ where $\lambda^2 = 1 + 4A^2$ and $6B\lambda = 4A$ so $\lambda = \frac{2A}{3B} $ so $\frac{4A^2}{9B^2} = 1 + 4A^2$ ie. $4A^2\frac{9B^2 - 1}{9B^2} = -1$ or $4A^2 = \frac{9B^2}{1 - 9B^2} > 4$ so $9B^2 > 4/5$ and I will choose $9B^2 = 9/10$ giving: $B = \frac{1}{\sqrt{10}}$ $A = 3/2$ $\lambda = \sqrt{10}$ ie. the trefoil $(sin(t) + \frac{3}{2}sin(2t),$ $cos(t) - \frac{3}{2}cos(2t),$ $\frac{1}{\sqrt{10}}sin(3t))$ has arclength parameterization given by $t(\theta)$ where: $\int (\frac{3}{\sqrt{10}} cos(3t) + \sqrt{10}) dt = \theta$ $\frac{sin(3t)}{\sqrt{10}} + t\sqrt{10} = \theta$ But I want $t$ in terms of $\theta$",,"['differential-geometry', 'knot-theory', 'parametrization']"
24,A convex closed plane curve never intersects itself?,A convex closed plane curve never intersects itself?,,"Intuitively, I think that a convex closed curve has to be simple (i.e. cannot intersect itself except at the starting and the ending points). How one can prove it rigorously? My attempt: Suppose it is a convex closed curve and it intersects itself. There exists(?) a neighborhood that the curve is not convex anymore, leading to contradiction. Also, do we need any smoothness condition?","Intuitively, I think that a convex closed curve has to be simple (i.e. cannot intersect itself except at the starting and the ending points). How one can prove it rigorously? My attempt: Suppose it is a convex closed curve and it intersects itself. There exists(?) a neighborhood that the curve is not convex anymore, leading to contradiction. Also, do we need any smoothness condition?",,"['differential-geometry', 'curves']"
25,$\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6$ injective immersion,injective immersion,\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6,"I have to prove this one Given $f:S^2\rightarrow\mathbb{R}^6$ defined by $f(x_1,x_2,x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)$, prove that: $f$ is an immersion; $f(-x_1,-x_2,-x_3)=f(x_1,x_2,x_3)$; exists an injective immersion $\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6$ such that $\tilde{f}([x_1:x_2:x_3])=f(x_1,x_2,x_3)$, $\forall [x_1:x_2:x_3]\in\mathbb{P}^2(\mathbb{R})$. I have this idea of the solution: I compute the Jacobian matrix $J_f(x_1,x_2,x_3)=\left( \begin{array}{ccc} 2x_1 & 0 & 0 \\ 0 & 2x_2 & 0 \\ 0 & 0 & 2x_3 \\ x_2 & x_1 & 0 \\ x_3 & 0 & x_1 \\ 0 & x_3 & x_2 \end{array} \right) $ and since $T_p S^2=\{v=(a,b,c)\in\mathbb{R}^3 : (x_1,x_2,x_3)\cdot(a,b,c)=0\}$, $\forall p=(x_1,x_2,x_3)\in S^2$, the expression $J_f(v)=0$ leads to the sistem $\left\{\begin{array}{l} 2x_1a=0\\ 2x_2b=0\\ 2x_3c=0\\ ax_2+bx_1=0\\ ax_3+cx_1=0\\ bx_3+cx_2=0 \end{array}\right.$ that has unique solution $(a,b,c)=(0,0,0)$. So $\ker(f_*)=\{0\}$ for every $p\in S^2$, hence $f_*$ is injective and f is then an immersion. $f(-x_1,-x_2,-x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)=f(x_1,x_2,x_3)$. I consider the diagram \begin{array}{ccc}  & f & \\ S^2 & \rightarrow & \mathbb{R}^6 \\ \pi \searrow & & \nearrow \tilde{f} \\  & \mathbb{P}^2 (\mathbb{R})&  \end{array} where $\pi$ is the projection of $S^2$ in $S^2/\sim\cong\mathbb{P}^2(\mathbb{R})$ and since it is a local diffeomorphism, $\pi^{-1}$ is an immersion. Hence because of (1),     $ \tilde{f}=f\circ\pi^{-1}$ is composition of immersion, then an immersion. Moreover it's injective because if we take $[x_1:x_2:x_3]\neq[y_1:y_2:y_3]$ we get $\tilde{f}([x_1:x_2:x_3])\neq\tilde{f}([y_1:y_2:y_3])$. Then we conclude that $\tilde{f}$ is an injective immersion. Is everything all right? Thanks for reading all this stuff","I have to prove this one Given $f:S^2\rightarrow\mathbb{R}^6$ defined by $f(x_1,x_2,x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)$, prove that: $f$ is an immersion; $f(-x_1,-x_2,-x_3)=f(x_1,x_2,x_3)$; exists an injective immersion $\tilde{f}:\mathbb{P}^2(\mathbb{R})\rightarrow\mathbb{R}^6$ such that $\tilde{f}([x_1:x_2:x_3])=f(x_1,x_2,x_3)$, $\forall [x_1:x_2:x_3]\in\mathbb{P}^2(\mathbb{R})$. I have this idea of the solution: I compute the Jacobian matrix $J_f(x_1,x_2,x_3)=\left( \begin{array}{ccc} 2x_1 & 0 & 0 \\ 0 & 2x_2 & 0 \\ 0 & 0 & 2x_3 \\ x_2 & x_1 & 0 \\ x_3 & 0 & x_1 \\ 0 & x_3 & x_2 \end{array} \right) $ and since $T_p S^2=\{v=(a,b,c)\in\mathbb{R}^3 : (x_1,x_2,x_3)\cdot(a,b,c)=0\}$, $\forall p=(x_1,x_2,x_3)\in S^2$, the expression $J_f(v)=0$ leads to the sistem $\left\{\begin{array}{l} 2x_1a=0\\ 2x_2b=0\\ 2x_3c=0\\ ax_2+bx_1=0\\ ax_3+cx_1=0\\ bx_3+cx_2=0 \end{array}\right.$ that has unique solution $(a,b,c)=(0,0,0)$. So $\ker(f_*)=\{0\}$ for every $p\in S^2$, hence $f_*$ is injective and f is then an immersion. $f(-x_1,-x_2,-x_3)=(x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3)=f(x_1,x_2,x_3)$. I consider the diagram \begin{array}{ccc}  & f & \\ S^2 & \rightarrow & \mathbb{R}^6 \\ \pi \searrow & & \nearrow \tilde{f} \\  & \mathbb{P}^2 (\mathbb{R})&  \end{array} where $\pi$ is the projection of $S^2$ in $S^2/\sim\cong\mathbb{P}^2(\mathbb{R})$ and since it is a local diffeomorphism, $\pi^{-1}$ is an immersion. Hence because of (1),     $ \tilde{f}=f\circ\pi^{-1}$ is composition of immersion, then an immersion. Moreover it's injective because if we take $[x_1:x_2:x_3]\neq[y_1:y_2:y_3]$ we get $\tilde{f}([x_1:x_2:x_3])\neq\tilde{f}([y_1:y_2:y_3])$. Then we conclude that $\tilde{f}$ is an injective immersion. Is everything all right? Thanks for reading all this stuff",,"['differential-geometry', 'projective-space', 'spheres']"
26,Are every two vector bundles weakly isomorphic?,Are every two vector bundles weakly isomorphic?,,"Let $E,F$ be two smooth vector bundles of the same rank over a smooth compact manifold $M$. Does there exists a weak isomorphism $\Phi \in W^{1,2}(M,E^* \otimes F)$ between $E$ and $F$? i.e I am asking if there exists a bundle morphism $\Phi:E \to F$ which lies in a suitable Sobolev space, and is an isomorphism (of the fibers) almost everywhere. For a concrete example, we can start with $E=T\mathbb{S}^2, F=\mathbb{S}^2 \times \mathbb{R}^2$. Is there a measurable global frame on $T\mathbb{S}^2$? Of course, in the smooth category what I am asking is nonsense- the point is that I am interested to know, if, when weakening the regularity requirements every two bundles of the same rank are indistinguishable.","Let $E,F$ be two smooth vector bundles of the same rank over a smooth compact manifold $M$. Does there exists a weak isomorphism $\Phi \in W^{1,2}(M,E^* \otimes F)$ between $E$ and $F$? i.e I am asking if there exists a bundle morphism $\Phi:E \to F$ which lies in a suitable Sobolev space, and is an isomorphism (of the fibers) almost everywhere. For a concrete example, we can start with $E=T\mathbb{S}^2, F=\mathbb{S}^2 \times \mathbb{R}^2$. Is there a measurable global frame on $T\mathbb{S}^2$? Of course, in the smooth category what I am asking is nonsense- the point is that I am interested to know, if, when weakening the regularity requirements every two bundles of the same rank are indistinguishable.",,"['differential-geometry', 'differential-topology', 'sobolev-spaces', 'equivalence-relations', 'vector-bundles']"
27,Doubt about proof,Doubt about proof,,"I was trying to solve the following problem: Show that if a surface is tangent to a plane along a curve, then the points of this curve are either parabolic or planar. I found the following solution: if $N$ is the Gauss map, and $\alpha(t)$ is the curve, then $N(\alpha(t))$ is normal to the plane and hence constant. Thus $0 \equiv dN/dt = dN(\alpha'(t))$ at all points of the curve. Assuming $\alpha$ is a regular parametrization, $\alpha'(t) \neq 0$, and so $\ker dN \neq 0$, i.e $dN$ is not injective and $det(dN) \equiv 0$ on $\alpha$. The result follows by deﬁnition. My question is: why do we need to prove that $dN$ is not injective? Once we prove that $0 \equiv dN/dt$, we are basically done since this directly implies that $\det(dN) \equiv 0$ on $\alpha$.","I was trying to solve the following problem: Show that if a surface is tangent to a plane along a curve, then the points of this curve are either parabolic or planar. I found the following solution: if $N$ is the Gauss map, and $\alpha(t)$ is the curve, then $N(\alpha(t))$ is normal to the plane and hence constant. Thus $0 \equiv dN/dt = dN(\alpha'(t))$ at all points of the curve. Assuming $\alpha$ is a regular parametrization, $\alpha'(t) \neq 0$, and so $\ker dN \neq 0$, i.e $dN$ is not injective and $det(dN) \equiv 0$ on $\alpha$. The result follows by deﬁnition. My question is: why do we need to prove that $dN$ is not injective? Once we prove that $0 \equiv dN/dt$, we are basically done since this directly implies that $\det(dN) \equiv 0$ on $\alpha$.",,"['differential-geometry', 'surfaces', 'curvature', 'parametrization']"
28,Hessian of a function at the critical points,Hessian of a function at the critical points,,"Let $f:M\to\mathbb{R}$ be a smooth function and $p\in M$ is a critical point of it. The Hessian of $f$ at a critical point $p$ is a symmetric bilinear form $\operatorname{Hess} f_p$ s.t. $\forall v,w\in T_pM$,      $$\operatorname{Hess} f_p(v,w)=V_p(W(f)),$$     where $V,W$ are the extensions of $v$ and $w$ to vector fields such that $V_p=v$ and $W_p=w$. Let the critical set of $f$ contains a submanifold $C$. Put a Riemannian metric on $M$ and $\forall p\in C$ consider the decomposition  $$T_pM=T_pC\oplus T^{\perp}_pC.$$ Let $v\in T_pC$ and $w\in T^\perp_pC$. Then show that $$\operatorname{Hess} f_p(v,w)=0.$$ I would appreciate any comment","Let $f:M\to\mathbb{R}$ be a smooth function and $p\in M$ is a critical point of it. The Hessian of $f$ at a critical point $p$ is a symmetric bilinear form $\operatorname{Hess} f_p$ s.t. $\forall v,w\in T_pM$,      $$\operatorname{Hess} f_p(v,w)=V_p(W(f)),$$     where $V,W$ are the extensions of $v$ and $w$ to vector fields such that $V_p=v$ and $W_p=w$. Let the critical set of $f$ contains a submanifold $C$. Put a Riemannian metric on $M$ and $\forall p\in C$ consider the decomposition  $$T_pM=T_pC\oplus T^{\perp}_pC.$$ Let $v\in T_pC$ and $w\in T^\perp_pC$. Then show that $$\operatorname{Hess} f_p(v,w)=0.$$ I would appreciate any comment",,"['differential-geometry', 'riemannian-geometry', 'morse-theory', 'hessian-matrix']"
29,Self intersections of a smooth closed curve being deformed,Self intersections of a smooth closed curve being deformed,,"Let $\gamma_0$ be a smooth closed curve in $\mathbb{R}^2$, such that it has no self intersection. You can smoothly deform it in order to create a (still smooth) curve $\gamma_1$ that have some self intersections. It looks like that at some point during the deformation you get at least one self intersection such that the two parts of the curve intersecting are tangent, for example as a limiting case between having $0$ and $2$ self intersections (see the image below). However, despite the problem looking quite simple I was unable to find a proof of this claim, either in a book or by myself. Therefore I am searching a reference to a proof of this claim, if it exists (which seems likely to me as the problem looks kind of standard).","Let $\gamma_0$ be a smooth closed curve in $\mathbb{R}^2$, such that it has no self intersection. You can smoothly deform it in order to create a (still smooth) curve $\gamma_1$ that have some self intersections. It looks like that at some point during the deformation you get at least one self intersection such that the two parts of the curve intersecting are tangent, for example as a limiting case between having $0$ and $2$ self intersections (see the image below). However, despite the problem looking quite simple I was unable to find a proof of this claim, either in a book or by myself. Therefore I am searching a reference to a proof of this claim, if it exists (which seems likely to me as the problem looks kind of standard).",,"['differential-geometry', 'differential-topology', 'transversality']"
30,A compact flat manifold whose first Betti number is equal to the dimension is a flat torus,A compact flat manifold whose first Betti number is equal to the dimension is a flat torus,,"I know the following to be true: If $(M,g)$ is a compact flat Riemannian manifold whose first Betti number ($= \dim H_{dR}^1(M)$) is equal to the dimension, then it is (isometric to) a flat torus. In the notes I am following it is stated that this follows from Bieberbach's Theorem (I guess it is the one about flat manifolds being finitely covered by tori), yet I have not found the way to link it to the first Betti number. Can someone show me how to prove the statement using Bieberbach's Theorem?","I know the following to be true: If $(M,g)$ is a compact flat Riemannian manifold whose first Betti number ($= \dim H_{dR}^1(M)$) is equal to the dimension, then it is (isometric to) a flat torus. In the notes I am following it is stated that this follows from Bieberbach's Theorem (I guess it is the one about flat manifolds being finitely covered by tori), yet I have not found the way to link it to the first Betti number. Can someone show me how to prove the statement using Bieberbach's Theorem?",,"['differential-geometry', 'riemannian-geometry', 'compact-manifolds', 'betti-numbers']"
31,"The space of specific $(1,1)$-tensors",The space of specific -tensors,"(1,1)","Let $(M,g)$ be a Riemannian manifold and $X$ a non-vanishing parallel vector field, i.e. $\nabla_XX=0$ . Let $${\cal A}_X:=\{T\in\ ^1_1\!\otimes(TM)\,|\ T(X)=0\}.$$ It is easy to see that ${\cal A}_X$ is a vector space. Also if $T\in {\cal A}_X$ then $\mathscr{L}_X T \in  {\cal A}_X$ and $\nabla_X T \in  {\cal A}_X$ where $\mathscr{L}_X$ is Lie derivative along $X$ . Suppose $Q$ denote the Ricci operator defined by $g(QY,Z)=Ric(Y,Z)$ . If $QX=\lambda X$ for some smooth function $\lambda$ , I think $Q$ can be written as follow: $$Q=aI+b\omega\otimes X+\sum_ic_iT_i,\quad T_i\in {\cal A}_X,\,a,b,c_i\in C^\infty(M,\Bbb R),\, \omega=g(X,.)$$ Is this conclusion  correct? Can anyone give a simple proof? What we can say about $\dim({\cal A}_X)?$ Thanks in advance","Let be a Riemannian manifold and a non-vanishing parallel vector field, i.e. . Let It is easy to see that is a vector space. Also if then and where is Lie derivative along . Suppose denote the Ricci operator defined by . If for some smooth function , I think can be written as follow: Is this conclusion  correct? Can anyone give a simple proof? What we can say about Thanks in advance","(M,g) X \nabla_XX=0 {\cal A}_X:=\{T\in\ ^1_1\!\otimes(TM)\,|\ T(X)=0\}. {\cal A}_X T\in {\cal A}_X \mathscr{L}_X T \in  {\cal A}_X \nabla_X T \in  {\cal A}_X \mathscr{L}_X X Q g(QY,Z)=Ric(Y,Z) QX=\lambda X \lambda Q Q=aI+b\omega\otimes X+\sum_ic_iT_i,\quad T_i\in {\cal A}_X,\,a,b,c_i\in C^\infty(M,\Bbb R),\, \omega=g(X,.) \dim({\cal A}_X)?","['differential-geometry', 'riemannian-geometry', 'tensors']"
32,Points distance realizing the diameter have at least two shortest connecting paths,Points distance realizing the diameter have at least two shortest connecting paths,,"Let $(M,g)$ be a complete riemanian manifold with $\text{diam}(M,g) < \infty$. If there are two points $p,q$ s.t. $d(p,q) = \text{diam}(M,g)$ then there are at least two shortest paths between them. So my idea is to take some geodesic (exists because of completeness) between p and q, lets call it $\gamma: [0,1] \to M$. Then find another geodesic from p to $\gamma(1 + \epsilon)$ which has to exists because of completeness and can not be the same as $\gamma$ since the length has to be $\le \text{diam}(M,g)$. Then what I want to do is look back at the tangent space $T_pM$ at $p$ and find something like a Cauchy-Sequence of the $v_\epsilon$, where $\exp(tv_\epsilon)$ is the geodesic from $p$ to $\gamma(1+\epsilon)$ and see that the limit of these $v_\epsilon$ gives me another geodesic. But this is where I am stuck. It is not clear that a Cauchy-Sequence like that will exist to me and that the limit does the right thing.","Let $(M,g)$ be a complete riemanian manifold with $\text{diam}(M,g) < \infty$. If there are two points $p,q$ s.t. $d(p,q) = \text{diam}(M,g)$ then there are at least two shortest paths between them. So my idea is to take some geodesic (exists because of completeness) between p and q, lets call it $\gamma: [0,1] \to M$. Then find another geodesic from p to $\gamma(1 + \epsilon)$ which has to exists because of completeness and can not be the same as $\gamma$ since the length has to be $\le \text{diam}(M,g)$. Then what I want to do is look back at the tangent space $T_pM$ at $p$ and find something like a Cauchy-Sequence of the $v_\epsilon$, where $\exp(tv_\epsilon)$ is the geodesic from $p$ to $\gamma(1+\epsilon)$ and see that the limit of these $v_\epsilon$ gives me another geodesic. But this is where I am stuck. It is not clear that a Cauchy-Sequence like that will exist to me and that the limit does the right thing.",,"['differential-geometry', 'smooth-manifolds', 'geodesic']"
33,Parallel transport around a closed loop,Parallel transport around a closed loop,,"Imagine an arbitrary closed loop in some Riemannian manifold $M$ with the Levi-Civita connection $\nabla$. If this curve $\gamma$ were infinitesimal (parametric distance ~ $\epsilon$) and given by 2 vector fields $U,V$, then starting from a point $A$ and parallel transporting some vector $W$ around the loop, we obtain the vector $$ \tau^{\gamma}_{A,A} W = W - \epsilon^2 R(U,V)W + \ldots$$ where $$ R(U,V) \equiv [\nabla_{U}, \nabla_{V}] - \nabla_{[U,V]} $$ is the curvature tensor and $$ \tau^{\gamma}_{A,A}: T_AM \rightarrow T_AM $$ is the operator of parallel transport around $\gamma$. My question is, is it possible to find such an operator (explicitly) for a general non-infinitesimal loop? I tried to decompose the loop into infinitesimal loops (as if in Kelvin-Stokes theorem) and to construct the operator from them, but I am stuck at how to traverse the infinitesimal loops. I guess the operator should somehow depend on the curvature tensor, since it is an identity for vanishing curvature. What is the dependence? N.B.: This is not a homework question. I am just curious.","Imagine an arbitrary closed loop in some Riemannian manifold $M$ with the Levi-Civita connection $\nabla$. If this curve $\gamma$ were infinitesimal (parametric distance ~ $\epsilon$) and given by 2 vector fields $U,V$, then starting from a point $A$ and parallel transporting some vector $W$ around the loop, we obtain the vector $$ \tau^{\gamma}_{A,A} W = W - \epsilon^2 R(U,V)W + \ldots$$ where $$ R(U,V) \equiv [\nabla_{U}, \nabla_{V}] - \nabla_{[U,V]} $$ is the curvature tensor and $$ \tau^{\gamma}_{A,A}: T_AM \rightarrow T_AM $$ is the operator of parallel transport around $\gamma$. My question is, is it possible to find such an operator (explicitly) for a general non-infinitesimal loop? I tried to decompose the loop into infinitesimal loops (as if in Kelvin-Stokes theorem) and to construct the operator from them, but I am stuck at how to traverse the infinitesimal loops. I guess the operator should somehow depend on the curvature tensor, since it is an identity for vanishing curvature. What is the dependence? N.B.: This is not a homework question. I am just curious.",,"['differential-geometry', 'riemannian-geometry', 'tensors', 'curvature', 'connections']"
34,Decomposing symmetric tensor field into sum of metric and tensor product,Decomposing symmetric tensor field into sum of metric and tensor product,,"Consider a symmetric tensor field $K_{ij}$ on a 2-dimensional Riemannian manifold with metric $g_{ij}$. How does one show that there exists a (smooth) function $f$ and one-form $\phi$ such that $$K_{ij}=fg_{ij}+\phi_i\phi_j?$$ I tried raising the $j$ index (so the metric becomes the identity) and solving the equation directly, but couldn't get the algebra to work out, and it would almost certainly involve division somewhere so smoothness gets dicey.","Consider a symmetric tensor field $K_{ij}$ on a 2-dimensional Riemannian manifold with metric $g_{ij}$. How does one show that there exists a (smooth) function $f$ and one-form $\phi$ such that $$K_{ij}=fg_{ij}+\phi_i\phi_j?$$ I tried raising the $j$ index (so the metric becomes the identity) and solving the equation directly, but couldn't get the algebra to work out, and it would almost certainly involve division somewhere so smoothness gets dicey.",,"['linear-algebra', 'differential-geometry', 'riemannian-geometry']"
35,$M$ is parallelizable iff $M$ is the product of a Lie group and some number of copies of $\mathbb{S}^7$,is parallelizable iff  is the product of a Lie group and some number of copies of,M M \mathbb{S}^7,"I was reading these notes and in the fifth page it is said that: Given a (smooth) manifold of dimension $n$, $M$, $M$ is parallelizable iff  $M$ is the product of a Lie group and some number of copies of $\mathbb{S}^7$. If $M$ is the product of a Lie group and some number of copies of $\mathbb{S}^7$, then $M$ is parallelizable since it is the product of parallelizable manifolds. However, I don't see the ""only if"" part of the statement. So any help would be appreciated, both a reference or an explicit argument or idea on how to prove that. Just for the sake of completeness, I say that the manifold $M$ is parallelizale if $TM \cong M \times R^n$, or, equivalently,  if it admits $n$ linearly independent vector fields. Just for future reference, the statement is false. See, for example this question .","I was reading these notes and in the fifth page it is said that: Given a (smooth) manifold of dimension $n$, $M$, $M$ is parallelizable iff  $M$ is the product of a Lie group and some number of copies of $\mathbb{S}^7$. If $M$ is the product of a Lie group and some number of copies of $\mathbb{S}^7$, then $M$ is parallelizable since it is the product of parallelizable manifolds. However, I don't see the ""only if"" part of the statement. So any help would be appreciated, both a reference or an explicit argument or idea on how to prove that. Just for the sake of completeness, I say that the manifold $M$ is parallelizale if $TM \cong M \times R^n$, or, equivalently,  if it admits $n$ linearly independent vector fields. Just for future reference, the statement is false. See, for example this question .",,"['differential-geometry', 'differential-topology', 'lie-groups', 'smooth-manifolds']"
36,Global differential form arising from an hermitian line bundle,Global differential form arising from an hermitian line bundle,,"Let $X$ be a compact, connected Riemann surface and let $(L,h)$ be an hermitian line bundle on $X$. Suppose that $s$ is a nonzero meromorphic section of $L$. I've learned that the $(1,1)$-form  $$\omega_s:=\partial\bar\partial \log (h(s,s))$$ is very important for the study of the geometry of $X$, but there are a couple of things that I don't understand: Why is $\omega_s$ defined on the whole $X$? I mean, $h(s,s)$ is well defined only out from the poles and the zeroes of $s$. Let's   denote this open set with $U$, then $\omega_s$ is a $(1,1)$-form on   $U$ and there should be a way to extend it uniquely to $X$. Assuming that $1.$ is proved, I want to understand why if $t$ is another meromorphic section of $L$, then $\omega_s=\omega_t$. Could you please give a proof of these facts? Edit: Clearly by $h$ I mean a smooth collection of hermitian products $h_x$ on the complex vector spaces $L_x$, for $x\in X$.","Let $X$ be a compact, connected Riemann surface and let $(L,h)$ be an hermitian line bundle on $X$. Suppose that $s$ is a nonzero meromorphic section of $L$. I've learned that the $(1,1)$-form  $$\omega_s:=\partial\bar\partial \log (h(s,s))$$ is very important for the study of the geometry of $X$, but there are a couple of things that I don't understand: Why is $\omega_s$ defined on the whole $X$? I mean, $h(s,s)$ is well defined only out from the poles and the zeroes of $s$. Let's   denote this open set with $U$, then $\omega_s$ is a $(1,1)$-form on   $U$ and there should be a way to extend it uniquely to $X$. Assuming that $1.$ is proved, I want to understand why if $t$ is another meromorphic section of $L$, then $\omega_s=\omega_t$. Could you please give a proof of these facts? Edit: Clearly by $h$ I mean a smooth collection of hermitian products $h_x$ on the complex vector spaces $L_x$, for $x\in X$.",,"['differential-geometry', 'differential-forms', 'riemann-surfaces', 'line-bundles']"
37,A little problem on Lie Brackets relating it with the commutator of matrices,A little problem on Lie Brackets relating it with the commutator of matrices,,"I am trying to solve this problem: There is a hint saying that I can use the fact that for  $$X=a_i\frac{\partial }{\partial x_i}\text{ with } a_i\in C^\infty(U)$$ and $$Y=b_i\frac{\partial }{\partial x_i}\text{ with } b_i\in C^\infty(U)$$ we have $$[X,Y]=\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}-b_{i}\frac{\partial a_{j}}{\partial x_{i}}\right)\frac{\partial}{\partial x_{j}}  $$ But I did not manage to use this hint in any useful way. Also, the obvious procedure seems to be the following: $$\Psi([X,Y])(P)=P[X,Y]=P(XY-YX)=PXY-PYX$$ and $$[\Psi(X),\Psi(Y)](P)=\left(\Psi(X)\Psi(Y)-\Psi(Y)\Psi(X)\right)(P)\\=\Psi(X)\left(\Psi(Y)(P)\right)-\Psi(Y)\left(\Psi(X)(P)\right)=\Psi(X)\left(PY\right)-\Psi(Y)\left(PX\right)=PYX-PXY$$ And this is clearly wrong. So: 1) What did I miss in my attempt? 2) Any hints on solving this using the given hint? EDIT: I believe I can solve this using the hint if $[\tilde{X},\tilde{Y}](P)=[\tilde{X}(P),\tilde{Y}(P)]$. However, I do not know why/if this is true, and question 1) still holds.","I am trying to solve this problem: There is a hint saying that I can use the fact that for  $$X=a_i\frac{\partial }{\partial x_i}\text{ with } a_i\in C^\infty(U)$$ and $$Y=b_i\frac{\partial }{\partial x_i}\text{ with } b_i\in C^\infty(U)$$ we have $$[X,Y]=\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}-b_{i}\frac{\partial a_{j}}{\partial x_{i}}\right)\frac{\partial}{\partial x_{j}}  $$ But I did not manage to use this hint in any useful way. Also, the obvious procedure seems to be the following: $$\Psi([X,Y])(P)=P[X,Y]=P(XY-YX)=PXY-PYX$$ and $$[\Psi(X),\Psi(Y)](P)=\left(\Psi(X)\Psi(Y)-\Psi(Y)\Psi(X)\right)(P)\\=\Psi(X)\left(\Psi(Y)(P)\right)-\Psi(Y)\left(\Psi(X)(P)\right)=\Psi(X)\left(PY\right)-\Psi(Y)\left(PX\right)=PYX-PXY$$ And this is clearly wrong. So: 1) What did I miss in my attempt? 2) Any hints on solving this using the given hint? EDIT: I believe I can solve this using the hint if $[\tilde{X},\tilde{Y}](P)=[\tilde{X}(P),\tilde{Y}(P)]$. However, I do not know why/if this is true, and question 1) still holds.",,"['differential-geometry', 'manifolds']"
38,Locus of decomposable elements,Locus of decomposable elements,,"Let $V$ be a vector space with basis $\{v_1,v_2,v_3,v_4,v_5\}$ and $W$ be the subspace of $\wedge^2(V)$ generated by $\{v_1 \wedge v_3, v_1 \wedge v_4, v_1 \wedge v_5, v_2 \wedge v_3, v_2 \wedge v_4, v_2 \wedge v_5 \}$. An element in $\wedge^2(V)$ is said to be decomposable if it is of the form $u \wedge v$ for $u,v \in V$. I need to find out the set of all decomposable elements in $W$. Of course the basis vectors of $W$ are decomposable but my guess is that the set is the image of $\mathbb P^1 \times \mathbb P^2$ under the Segre embedding but I have no clue how to show this.","Let $V$ be a vector space with basis $\{v_1,v_2,v_3,v_4,v_5\}$ and $W$ be the subspace of $\wedge^2(V)$ generated by $\{v_1 \wedge v_3, v_1 \wedge v_4, v_1 \wedge v_5, v_2 \wedge v_3, v_2 \wedge v_4, v_2 \wedge v_5 \}$. An element in $\wedge^2(V)$ is said to be decomposable if it is of the form $u \wedge v$ for $u,v \in V$. I need to find out the set of all decomposable elements in $W$. Of course the basis vectors of $W$ are decomposable but my guess is that the set is the image of $\mathbb P^1 \times \mathbb P^2$ under the Segre embedding but I have no clue how to show this.",,"['linear-algebra', 'differential-geometry', 'algebraic-geometry', 'projective-geometry', 'projective-space']"
39,$\triangledown_{X_{p}}Y$ actually depends only on the values of $Y$ along any curve tangent to $X_{p}$,actually depends only on the values of  along any curve tangent to,\triangledown_{X_{p}}Y Y X_{p},"I'm reading John Lee's Riemannian Manifolds where I encountered the following reading exercise: Improve Lemma 4.1 by showing that $\triangledown_{X_{p}}Y$ actually depends only on the values of $Y$ along any curve tangent to $X_{p}$. More precisely, suppose that $\gamma: (-\epsilon, \epsilon) \rightarrow M$ is a curve with $\gamma(0)=p$ and $\gamma'(0)=X_{p}$, and suppose that $Y$ and $\tilde{Y}$ are vector fields that agree along $\gamma$. Show that $\triangledown_{X_{p}}Y = \triangledown_{X_{p}}\tilde{Y}$ Here we are taking $M$ to be a smooth manifold and $\triangledown$ to be a linear connection. Lemma $4.1$ essentially tells us that the vector field, $\triangledown_{X}Y$ is determined locally by $Y$ and pointwise by $X$. The linearity of the connection, I hope to show that if $Y=0$ along $\gamma$ then $\triangledown_{X_{p}}Y=0$. Earlier in the text, the notion of covariant derivatives of vector fields along curves were discussed, in particular it was mentioned that when the vector field $V$ along a curve is extendible, then for any extension $\tilde{V}$, we have that $D_{t}V(t)=\triangledown_{\gamma'(t)}\tilde{V}$ in which case we have a formula for $D_{t}V(t)$. I'm a bit confused on how to involve this. Any help is appreciated.","I'm reading John Lee's Riemannian Manifolds where I encountered the following reading exercise: Improve Lemma 4.1 by showing that $\triangledown_{X_{p}}Y$ actually depends only on the values of $Y$ along any curve tangent to $X_{p}$. More precisely, suppose that $\gamma: (-\epsilon, \epsilon) \rightarrow M$ is a curve with $\gamma(0)=p$ and $\gamma'(0)=X_{p}$, and suppose that $Y$ and $\tilde{Y}$ are vector fields that agree along $\gamma$. Show that $\triangledown_{X_{p}}Y = \triangledown_{X_{p}}\tilde{Y}$ Here we are taking $M$ to be a smooth manifold and $\triangledown$ to be a linear connection. Lemma $4.1$ essentially tells us that the vector field, $\triangledown_{X}Y$ is determined locally by $Y$ and pointwise by $X$. The linearity of the connection, I hope to show that if $Y=0$ along $\gamma$ then $\triangledown_{X_{p}}Y=0$. Earlier in the text, the notion of covariant derivatives of vector fields along curves were discussed, in particular it was mentioned that when the vector field $V$ along a curve is extendible, then for any extension $\tilde{V}$, we have that $D_{t}V(t)=\triangledown_{\gamma'(t)}\tilde{V}$ in which case we have a formula for $D_{t}V(t)$. I'm a bit confused on how to involve this. Any help is appreciated.",,"['differential-geometry', 'riemannian-geometry']"
40,Exponentiating the Lie Derivative: Which direction does it rotate?,Exponentiating the Lie Derivative: Which direction does it rotate?,,"I've been working through the book Functional Differential Geometry by Sussman and Wisdom and am having trouble with an example they give. We can exponentiate a Lie derivative $$ e^{\mathsf{t\mathcal{L}_v}}\mathsf{y} = \mathsf{y + t \mathcal{L}_v y + \frac{t^2}{2!} \mathcal{L}^2_v y + \dots } $$ which evolves $\mathsf{y}$ along the integral curves of $\mathsf{v}$.  As a concrete example they evolve the coordinate basis vector $\frac{\partial}{\partial \mathsf{y}}$ along $\mathsf{J}_z = x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}$  (a counter clockwise circular field or z-angular momentum generator) and give as an answer $$\exp(a \mathcal{L}_{\mathsf{J}_z})\tfrac{\partial}{\partial \mathsf{y}} = -\sin(a)\tfrac{\partial}{\partial \mathsf{x}} + \cos(a)\tfrac{\partial}{\partial \mathsf{y}}.$$ This agrees at $a=0$ and indicates that the evolution maintains the orientation of $\mathsf{v}$ and $\mathsf{y}$ along the flow.  $\tfrac{\partial}{\partial\mathsf{y}}$ just rotates along $\mathsf{J}_z$. If I try to calculate the expansion, for the first term I get $$ a\mathcal{L}_{x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} =  a\mathcal{L}_{- y\tfrac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} = -a[y\tfrac{\partial}{\partial \mathsf{x}}, \tfrac{\partial}{\partial \mathsf{y}}] = a\tfrac{\partial}{\partial \mathsf{x}}  $$ which disagrees with the expansion of the answer by a sign, giving rotation in the opposite direction. The intuitive notions of the Lie Derivative I have also say it should be $a\tfrac{\partial}{\partial \mathsf{x}}$.  For example, beginning at $x=1, y=0$ we can travel along $\epsilon\mathsf{J}_z$ and then  $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $, or begin along $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $ and then along $\epsilon\mathsf{J}_z$ (points slightly to the left).  The difference between these paths will be $\epsilon^2 \tfrac{\partial}{\partial \mathsf{x}} $.  The same goes for beginning at $x=0,y=1$ or if I do it using a pushforward.  I'm tempted to say the book is mistaken, and that to get a rotation operator which coincides with the direction of $\mathsf{J}_z$ we should use $e^{-t\mathcal{L}_{\mathsf{J}_z}}$ but would appreciate some confirmation.","I've been working through the book Functional Differential Geometry by Sussman and Wisdom and am having trouble with an example they give. We can exponentiate a Lie derivative $$ e^{\mathsf{t\mathcal{L}_v}}\mathsf{y} = \mathsf{y + t \mathcal{L}_v y + \frac{t^2}{2!} \mathcal{L}^2_v y + \dots } $$ which evolves $\mathsf{y}$ along the integral curves of $\mathsf{v}$.  As a concrete example they evolve the coordinate basis vector $\frac{\partial}{\partial \mathsf{y}}$ along $\mathsf{J}_z = x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}$  (a counter clockwise circular field or z-angular momentum generator) and give as an answer $$\exp(a \mathcal{L}_{\mathsf{J}_z})\tfrac{\partial}{\partial \mathsf{y}} = -\sin(a)\tfrac{\partial}{\partial \mathsf{x}} + \cos(a)\tfrac{\partial}{\partial \mathsf{y}}.$$ This agrees at $a=0$ and indicates that the evolution maintains the orientation of $\mathsf{v}$ and $\mathsf{y}$ along the flow.  $\tfrac{\partial}{\partial\mathsf{y}}$ just rotates along $\mathsf{J}_z$. If I try to calculate the expansion, for the first term I get $$ a\mathcal{L}_{x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} =  a\mathcal{L}_{- y\tfrac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} = -a[y\tfrac{\partial}{\partial \mathsf{x}}, \tfrac{\partial}{\partial \mathsf{y}}] = a\tfrac{\partial}{\partial \mathsf{x}}  $$ which disagrees with the expansion of the answer by a sign, giving rotation in the opposite direction. The intuitive notions of the Lie Derivative I have also say it should be $a\tfrac{\partial}{\partial \mathsf{x}}$.  For example, beginning at $x=1, y=0$ we can travel along $\epsilon\mathsf{J}_z$ and then  $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $, or begin along $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $ and then along $\epsilon\mathsf{J}_z$ (points slightly to the left).  The difference between these paths will be $\epsilon^2 \tfrac{\partial}{\partial \mathsf{x}} $.  The same goes for beginning at $x=0,y=1$ or if I do it using a pushforward.  I'm tempted to say the book is mistaken, and that to get a rotation operator which coincides with the direction of $\mathsf{J}_z$ we should use $e^{-t\mathcal{L}_{\mathsf{J}_z}}$ but would appreciate some confirmation.",,"['differential-geometry', 'lie-derivative']"
41,Finding the flow of vector field with lie group (orthogonal group),Finding the flow of vector field with lie group (orthogonal group),,"If $X_S (A) = AS$ defines a smooth vector field, where $A \in O(n)$ (matrices with $A^{-1} = A^{T}$) and $S$ belonging to the space of skew matrices (matrices with $S^{T} = -S$). How would I calculate the flow of $X_S$ here? I'm using the definition of flow as one whose flow domain is all of $\mathbb{R} \times O(n)$, where the flow is the map $\theta : O(n) \times \mathbb{R} \to O(n)$ such that for all $s,t \in \mathbb{R}$ and $A \in O(n)$ the following holds: $$\theta(A,0) = A \space\ \space\ \text{and} \space\ \space\ \theta(\theta(A,t),s) = \theta(A, s+t)$$ I'm only familiar with finding flow via solving an ODE (or system of them). Need help understanding how this works with Lie Groups.","If $X_S (A) = AS$ defines a smooth vector field, where $A \in O(n)$ (matrices with $A^{-1} = A^{T}$) and $S$ belonging to the space of skew matrices (matrices with $S^{T} = -S$). How would I calculate the flow of $X_S$ here? I'm using the definition of flow as one whose flow domain is all of $\mathbb{R} \times O(n)$, where the flow is the map $\theta : O(n) \times \mathbb{R} \to O(n)$ such that for all $s,t \in \mathbb{R}$ and $A \in O(n)$ the following holds: $$\theta(A,0) = A \space\ \space\ \text{and} \space\ \space\ \theta(\theta(A,t),s) = \theta(A, s+t)$$ I'm only familiar with finding flow via solving an ODE (or system of them). Need help understanding how this works with Lie Groups.",,"['differential-geometry', 'lie-groups']"
42,"If $f$ an isometry on $M$ s.t. $f(p)=p$ and $(df)_p=\mathrm{id}_{T_p M}$, then $f=\mathrm{id}_M$","If  an isometry on  s.t.  and , then",f M f(p)=p (df)_p=\mathrm{id}_{T_p M} f=\mathrm{id}_M,"A sketch of the proof of this is provided here , though many of the details are incomplete, and I'm having a certain amount of difficulty filling in the gaps. Note that we require $M$ to be connected (obviously). For clarity, I repeat the question in full below. Proposition (which I wish to prove) : Let $(M,g)$ be an $n$-dimensional topologically-connected Riemannian manifold, let $f:M\to M$ be an isometry (i.e. $f^*g = g$), and suppose there exists some point $p\in M$ such that $f(p) = p$, and $(\mathrm{d}f)_p = \mathrm{id}_{\mathrm{T}_p M}$. I claim that $f = \mathrm{id}_M$. The proof goes as follows: we define $$X = \{ q\in M\ \vert\ f(q) = q,\ (\mathrm{d}f)_q = \mathrm{id}_{\mathrm{T}_q M}\}$$ so that $X$ is easily seen to be closed, and is nonempty by assumption, so that we must prove that it is open. To do so, let $q\in X$ be arbitrary, and let $$\mathrm{exp}_q:\mathrm{B}_{\mathrm{T}_q M}(0,\varepsilon)\to\mathrm{B}_M(q,\varepsilon)\subseteq M$$ be a diffeomorphism defining a normal neighborhood $\mathrm{B}_M(q,\varepsilon)$, then we will prove that $\mathrm{B}_M(q,\varepsilon)\subseteq X$ for $\varepsilon$ sufficiently small. Note that since isometries preserve geodesics, then $$f(\mathrm{exp}_q(tv)) = \mathrm{exp}_{f(q)}(t\,f_*v)$$ and therefore for any $r = \mathrm{exp}_q(v)\in\mathrm{B}_M(q,\varepsilon)$ we have that $$f(r) = f(\mathrm{exp}_q(v)) = \mathrm{exp}_{f(q)}(f_* v) = \mathrm{exp}_q v = r$$ so it suffices to prove now that there exists some $\varepsilon$ such that for all $r\in\mathrm{B}_M(q,\varepsilon)$, $(\mathrm{d}f)_r = \mathrm{id}_{\mathrm{T}_r M}$, however, I don't see how this is obvious, and Jason DeVito's explanation (drawing a triangle between $r$, $q$, and $\mathrm{exp}_q(v)$ for some $v$ such that $(\mathrm{d}f)_r v\ne v$) doesn't seem at all clear to me. So how is this last part proved?","A sketch of the proof of this is provided here , though many of the details are incomplete, and I'm having a certain amount of difficulty filling in the gaps. Note that we require $M$ to be connected (obviously). For clarity, I repeat the question in full below. Proposition (which I wish to prove) : Let $(M,g)$ be an $n$-dimensional topologically-connected Riemannian manifold, let $f:M\to M$ be an isometry (i.e. $f^*g = g$), and suppose there exists some point $p\in M$ such that $f(p) = p$, and $(\mathrm{d}f)_p = \mathrm{id}_{\mathrm{T}_p M}$. I claim that $f = \mathrm{id}_M$. The proof goes as follows: we define $$X = \{ q\in M\ \vert\ f(q) = q,\ (\mathrm{d}f)_q = \mathrm{id}_{\mathrm{T}_q M}\}$$ so that $X$ is easily seen to be closed, and is nonempty by assumption, so that we must prove that it is open. To do so, let $q\in X$ be arbitrary, and let $$\mathrm{exp}_q:\mathrm{B}_{\mathrm{T}_q M}(0,\varepsilon)\to\mathrm{B}_M(q,\varepsilon)\subseteq M$$ be a diffeomorphism defining a normal neighborhood $\mathrm{B}_M(q,\varepsilon)$, then we will prove that $\mathrm{B}_M(q,\varepsilon)\subseteq X$ for $\varepsilon$ sufficiently small. Note that since isometries preserve geodesics, then $$f(\mathrm{exp}_q(tv)) = \mathrm{exp}_{f(q)}(t\,f_*v)$$ and therefore for any $r = \mathrm{exp}_q(v)\in\mathrm{B}_M(q,\varepsilon)$ we have that $$f(r) = f(\mathrm{exp}_q(v)) = \mathrm{exp}_{f(q)}(f_* v) = \mathrm{exp}_q v = r$$ so it suffices to prove now that there exists some $\varepsilon$ such that for all $r\in\mathrm{B}_M(q,\varepsilon)$, $(\mathrm{d}f)_r = \mathrm{id}_{\mathrm{T}_r M}$, however, I don't see how this is obvious, and Jason DeVito's explanation (drawing a triangle between $r$, $q$, and $\mathrm{exp}_q(v)$ for some $v$ such that $(\mathrm{d}f)_r v\ne v$) doesn't seem at all clear to me. So how is this last part proved?",,"['differential-geometry', 'riemannian-geometry']"
43,Universal cover of flat surface obtained from the regular octagon,Universal cover of flat surface obtained from the regular octagon,,"Consider the flat surface $(X,\omega)$ obtained identifying pairs of parallel sides of the regular octagon of area 1 by translation. I’m trying to understand what its universal cover (with the induced metric) looks like. I’ve read that “it’s composed of an infinite number of copies of the octagon glued together with eight around each vertex to create cone points of angle $6\pi$”. What exactly does it mean “glued together with eight around each vertex”? Chosen a base point $x_0\in X$, the universal cover of $(X,\omega)$ should be such that the preimages of a point $x\in X$ are given by all homotopy classes of paths from $x_0$ to $x$, am I right? I'm trying to understand why these two definitions of the universal cover of $(X,\omega)$ are equivalent, but I can't see why for each homotopy class of paths between $x_0$ and $x$ one should have a point in a different copy of the octagon. Why is that?","Consider the flat surface $(X,\omega)$ obtained identifying pairs of parallel sides of the regular octagon of area 1 by translation. I’m trying to understand what its universal cover (with the induced metric) looks like. I’ve read that “it’s composed of an infinite number of copies of the octagon glued together with eight around each vertex to create cone points of angle $6\pi$”. What exactly does it mean “glued together with eight around each vertex”? Chosen a base point $x_0\in X$, the universal cover of $(X,\omega)$ should be such that the preimages of a point $x\in X$ are given by all homotopy classes of paths from $x_0$ to $x$, am I right? I'm trying to understand why these two definitions of the universal cover of $(X,\omega)$ are equivalent, but I can't see why for each homotopy class of paths between $x_0$ and $x$ one should have a point in a different copy of the octagon. Why is that?",,"['differential-geometry', 'riemannian-geometry']"
44,Compute the Differential Map on $T^*_{\rho}$ and $\Lambda^2T^*_{\rho}$,Compute the Differential Map on  and,T^*_{\rho} \Lambda^2T^*_{\rho},"The question reads: Consider the torus $T^2 = S^1 \times S^1$, parametrized in $\mathbb{R}^3$ by$$\Gamma(\theta, \varphi) = (\cos(\theta)(R+r\cos(\varphi)), \sin(\theta)(R+r\cos(\varphi)), r\sin(\varphi))$$    where $0<r<R$. Compute the differential $I_{*}$, mapping $T_{p}(T^2)$ to $T_{\Gamma(p)}(\mathbb{R}^3)$, in the coordinates $\theta, \varphi$ on the torus and the standard co-ordinate $x,y,z\in\mathbb{R}^3$. Compute the induced map $I^*$ on $T^*_{p}$ and the map on the second exterior power $\Lambda^2T^*_{p}$. Now I believe I understand the first part, just find the derivative of the map and then parametrize it into $x,y,z$, but I am totally confused about the second part. Am I supposed to compute the map on the dual space of the tangent vectors at $p$? If anyone could help it would be greatly appreciated. Thank you.","The question reads: Consider the torus $T^2 = S^1 \times S^1$, parametrized in $\mathbb{R}^3$ by$$\Gamma(\theta, \varphi) = (\cos(\theta)(R+r\cos(\varphi)), \sin(\theta)(R+r\cos(\varphi)), r\sin(\varphi))$$    where $0<r<R$. Compute the differential $I_{*}$, mapping $T_{p}(T^2)$ to $T_{\Gamma(p)}(\mathbb{R}^3)$, in the coordinates $\theta, \varphi$ on the torus and the standard co-ordinate $x,y,z\in\mathbb{R}^3$. Compute the induced map $I^*$ on $T^*_{p}$ and the map on the second exterior power $\Lambda^2T^*_{p}$. Now I believe I understand the first part, just find the derivative of the map and then parametrize it into $x,y,z$, but I am totally confused about the second part. Am I supposed to compute the map on the dual space of the tangent vectors at $p$? If anyone could help it would be greatly appreciated. Thank you.",,"['differential-geometry', 'manifolds', 'covering-spaces', 'differential', 'exterior-algebra']"
45,Calabi-Yau manifold with fiber structure,Calabi-Yau manifold with fiber structure,,"I'm reading the paper 'A bound on the Euler number for certain Calabi-Yau 3-folds' where the author made the following statement about fiberd Calabi-Yau manifold without proof. Let $X$ be a smooth projective threefold with trivial canonical bundle, $\pi:X\to Y$ be a surjective holomorphic map from $X$ to a lower dimensional manifold $Y$, if $F$ is the general fiber of $\pi$, then there are 3 possible types: a. $Y$= surface with kodaira dimension $-\infty$, $F$=elliptic curves; b. $Y= \mathbb{C}P^1$, $F$= abelian surface; c. $Y= \mathbb{C}P^1$, $F$= k3-surface. The author mentioned that $F$ should have trivial canonical bundle by ajunction formula, as far as I know, the ajunction formula says that $$K_F=(K_X\otimes \mathcal{O}_X(F))\mid_F,$$ then $K_F=\mathcal{O}_X(F)\mid_F,$ so the problem is why $\mathcal{O}_X(F)\mid_F=\mathcal{O}_F$? And I can't see why the base manifold $Y$ should be either a surface with kodaira dimension $-\infty$ or $\mathbb{C}P^1$? The author also mentioned implicitly that if $\pi$ have no singular fibers, then $X$ is essentially a product. This is amazing since this is the main result of this paper in 2013.","I'm reading the paper 'A bound on the Euler number for certain Calabi-Yau 3-folds' where the author made the following statement about fiberd Calabi-Yau manifold without proof. Let $X$ be a smooth projective threefold with trivial canonical bundle, $\pi:X\to Y$ be a surjective holomorphic map from $X$ to a lower dimensional manifold $Y$, if $F$ is the general fiber of $\pi$, then there are 3 possible types: a. $Y$= surface with kodaira dimension $-\infty$, $F$=elliptic curves; b. $Y= \mathbb{C}P^1$, $F$= abelian surface; c. $Y= \mathbb{C}P^1$, $F$= k3-surface. The author mentioned that $F$ should have trivial canonical bundle by ajunction formula, as far as I know, the ajunction formula says that $$K_F=(K_X\otimes \mathcal{O}_X(F))\mid_F,$$ then $K_F=\mathcal{O}_X(F)\mid_F,$ so the problem is why $\mathcal{O}_X(F)\mid_F=\mathcal{O}_F$? And I can't see why the base manifold $Y$ should be either a surface with kodaira dimension $-\infty$ or $\mathbb{C}P^1$? The author also mentioned implicitly that if $\pi$ have no singular fibers, then $X$ is essentially a product. This is amazing since this is the main result of this paper in 2013.",,"['differential-geometry', 'algebraic-geometry']"
46,Are all derivations of real-valued functions derivatives?,Are all derivations of real-valued functions derivatives?,,"To me it was obvious that all derivatives are derivations, since one can show that they are linear and satisfy the product rule. However, I was very surprised when I learned that every derivation of $C^{\infty}$ functions of a (smooth) manifold is essentially a directional derivative. The definition of a derivation is very simple and consists of two purely algebraic properties, while the definition of directional derivative should seemingly be inseparable from purely analytic notions like limits, Caucy/metric completeness, and the least upper bound property (although these notions may actually be purely topological, see here , although in any case they are not algebraic). However, the proof (see my community-wiki ""answer"" below) relies in a seemingly essential way on Taylor's theorem and its generalizations, and thus on the twice differentiability of the functions. This would seem to explain why calculus is taught in terms of derivatives and not derivations, in spite of the latter's greater simplicity, because to show that they are equivalent requires Taylor's theorem, which to understand requires a prior understanding of what differentiation is, and in any case is usually taught first in the second semester. Question: When we require only first differentiability and not second differentiability or continuous differentiability, is it still the case that all derivations are differential operators? Are there no derivations defined on the space of continuous real-valued functions? I.e., in every context where both derivatives and differentiation are defined, are derivations really exactly the same thing as derivatives? Or only when derivations are restricted to those functions for which Taylor's theorem holds? I have heard of there existing derivations for fields that are not even metrically complete, such as the finite fields $\mathbb{F}_p$ , and since the definition is really so much simpler than that of the derivative, I would be really surprised if they coincided exactly whenever both existed. Related questions: (1) (2) (3) Also I am not sure how to tag this question so please feel free to fix the tags.","To me it was obvious that all derivatives are derivations, since one can show that they are linear and satisfy the product rule. However, I was very surprised when I learned that every derivation of functions of a (smooth) manifold is essentially a directional derivative. The definition of a derivation is very simple and consists of two purely algebraic properties, while the definition of directional derivative should seemingly be inseparable from purely analytic notions like limits, Caucy/metric completeness, and the least upper bound property (although these notions may actually be purely topological, see here , although in any case they are not algebraic). However, the proof (see my community-wiki ""answer"" below) relies in a seemingly essential way on Taylor's theorem and its generalizations, and thus on the twice differentiability of the functions. This would seem to explain why calculus is taught in terms of derivatives and not derivations, in spite of the latter's greater simplicity, because to show that they are equivalent requires Taylor's theorem, which to understand requires a prior understanding of what differentiation is, and in any case is usually taught first in the second semester. Question: When we require only first differentiability and not second differentiability or continuous differentiability, is it still the case that all derivations are differential operators? Are there no derivations defined on the space of continuous real-valued functions? I.e., in every context where both derivatives and differentiation are defined, are derivations really exactly the same thing as derivatives? Or only when derivations are restricted to those functions for which Taylor's theorem holds? I have heard of there existing derivations for fields that are not even metrically complete, such as the finite fields , and since the definition is really so much simpler than that of the derivative, I would be really surprised if they coincided exactly whenever both existed. Related questions: (1) (2) (3) Also I am not sure how to tag this question so please feel free to fix the tags.",C^{\infty} \mathbb{F}_p,"['real-analysis', 'abstract-algebra', 'differential-geometry', 'smooth-manifolds', 'differential-algebra']"
47,Convergence of metrics,Convergence of metrics,,"Let $U\subset\mathbb{R}^2$ be an open neighborhood of the origin and let $f_n:U\rightarrow \mathbb{R}_{>0}$, $n\in \mathbb{N}$ be a sequence of differentiable functions which uniformly converges on $U$ to an integrable function $f:U\rightarrow \mathbb{R}_{>0}$. Fix two point $p_1,p_2\in U$ and call $\Gamma= \Gamma_{p_1}^{p_2}$ the set of differentiable paths $\gamma:I\rightarrow U$, $\gamma(t)=(\gamma_1(t),\gamma_2(t))$, such that $\gamma(0)=p_1$, $\gamma(1)=p_2$ and $\gamma(I)\subset U$. For every $\gamma\in \Gamma$ I'm quite sure it's true:  $$\lim_{n\rightarrow \infty}\int_0^1\sqrt{f_n(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt=\int_0^1\sqrt{f(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt,$$  since the functions $f_n$ converge uniformly. I'm not sure if it's also true  $$\lim_{n\rightarrow \infty}\inf_{\gamma\in\Gamma}\int_0^1\sqrt{f_n(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt = \inf_{\gamma\in\Gamma}\int_0^1\sqrt{f(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt $$ Is this equality true or is it only verified under additional hypothesis?","Let $U\subset\mathbb{R}^2$ be an open neighborhood of the origin and let $f_n:U\rightarrow \mathbb{R}_{>0}$, $n\in \mathbb{N}$ be a sequence of differentiable functions which uniformly converges on $U$ to an integrable function $f:U\rightarrow \mathbb{R}_{>0}$. Fix two point $p_1,p_2\in U$ and call $\Gamma= \Gamma_{p_1}^{p_2}$ the set of differentiable paths $\gamma:I\rightarrow U$, $\gamma(t)=(\gamma_1(t),\gamma_2(t))$, such that $\gamma(0)=p_1$, $\gamma(1)=p_2$ and $\gamma(I)\subset U$. For every $\gamma\in \Gamma$ I'm quite sure it's true:  $$\lim_{n\rightarrow \infty}\int_0^1\sqrt{f_n(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt=\int_0^1\sqrt{f(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt,$$  since the functions $f_n$ converge uniformly. I'm not sure if it's also true  $$\lim_{n\rightarrow \infty}\inf_{\gamma\in\Gamma}\int_0^1\sqrt{f_n(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt = \inf_{\gamma\in\Gamma}\int_0^1\sqrt{f(\gamma(t))(\dot\gamma_1^2+\dot\gamma_2^2)}dt $$ Is this equality true or is it only verified under additional hypothesis?",,"['real-analysis', 'differential-geometry', 'riemannian-geometry']"
48,Calculate of linearization of Ricci flow,Calculate of linearization of Ricci flow,,"According to this question , I want to calculate $DE(g_{ij})\widetilde g_{ij}$. If I treat $g^{ij}$ as  irrelevant variable ,then I can get the same result as picture below. But I doubt that $g^{ij}$ has some connection with $g_{ij}$, so in the calculation should consider $g^{ij}$. Consider the linearization of $\frac{\partial}{\partial x^i}\{g^{kl}\frac{\partial}{\partial x^j}g_{kl}\}$ , let  $$ H=\frac{\partial}{\partial x^i}\{g^{kl}\frac{\partial}{\partial x^j}g_{kl}\} $$  Then  $$ DH(g_{ij})\widetilde g_{ij}=\frac{d}{ds}|_{s=0}H(g_{ij}+s\widetilde g_{ij}) \\ =\frac{d}{ds}|_{s=0}\{\frac{\partial}{\partial x^i} ((g^{kl}+\frac{1}{s}\widetilde g^{kl})\frac{\partial}{\partial x^j}(g_{kl}+s\widetilde g_{kl}))\} \\ =\frac{d}{ds}|_{s=0}(\frac{1}{s}\widetilde g^{kl})\frac{\partial}{\partial x^i} \frac{\partial}{\partial x^j}(g_{kl}) + ... $$ In above calculation ,I am not sure it should be $\frac{1}{s}\widetilde g^{kl}$ or $s\widetilde g^{kl}$ in the term of $\widetilde g^{kl}$.But obviously, there are 2 order term different to the results in picture below.","According to this question , I want to calculate $DE(g_{ij})\widetilde g_{ij}$. If I treat $g^{ij}$ as  irrelevant variable ,then I can get the same result as picture below. But I doubt that $g^{ij}$ has some connection with $g_{ij}$, so in the calculation should consider $g^{ij}$. Consider the linearization of $\frac{\partial}{\partial x^i}\{g^{kl}\frac{\partial}{\partial x^j}g_{kl}\}$ , let  $$ H=\frac{\partial}{\partial x^i}\{g^{kl}\frac{\partial}{\partial x^j}g_{kl}\} $$  Then  $$ DH(g_{ij})\widetilde g_{ij}=\frac{d}{ds}|_{s=0}H(g_{ij}+s\widetilde g_{ij}) \\ =\frac{d}{ds}|_{s=0}\{\frac{\partial}{\partial x^i} ((g^{kl}+\frac{1}{s}\widetilde g^{kl})\frac{\partial}{\partial x^j}(g_{kl}+s\widetilde g_{kl}))\} \\ =\frac{d}{ds}|_{s=0}(\frac{1}{s}\widetilde g^{kl})\frac{\partial}{\partial x^i} \frac{\partial}{\partial x^j}(g_{kl}) + ... $$ In above calculation ,I am not sure it should be $\frac{1}{s}\widetilde g^{kl}$ or $s\widetilde g^{kl}$ in the term of $\widetilde g^{kl}$.But obviously, there are 2 order term different to the results in picture below.",,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry', 'ricci-flow']"
49,Current induced by a locally integrable differential form: I don't understand why it is not trivially $0$,Current induced by a locally integrable differential form: I don't understand why it is not trivially,0,"Today this question captured my attention, hence I want to generalize it. Let $X$ be a complex manifold of dimension $M$ and let $\omega $ be a $(n-p,n-q)$-differential form such that in each chart it is represented by locally integrable functions. A standard example of current on $X$ (see for example De Rham - Differential manifolds, Chap. III example 2) is the following: $$[\omega]:\alpha \mapsto\int_X \alpha\wedge\omega$$ where $\alpha$ is a $C^\infty$ $(p,q)$-form and $\alpha\wedge\omega$ is a locally integrable $(n,n)$-form. Who ensures that $\alpha\wedge\omega$ is integrable? Usually the integral is defined for smooth differential forms on oriented manifolds (see for example Lee's book). About this point I'm quite sure the answer will be: ""integration can be extended to locally integrable forms"", I just wanted  to check. Consider $n=1$, $p=0$, $q=0$ and let'es examine the example of the question linked above: $$\omega=\partial\bar\partial\log|f|$$ where $f$ is a meromorphic function on the Riemann surface $X$. Then the current $[\omega]$ is $$[\omega]:g \mapsto\int_X g\omega$$ for any $C^\infty$ function $g$. Here the problem: note that $\omega$ is $0$ almost everywhere, in particular $\omega$ is supported in the set of zeroes and poles of $f$ i.e. in a finite set! Why is the integral $\int_X g\omega$ different from $0$? It seems almost obvious to deduce that the integral of a differential form supported in a finite set is $0$ because of the properties of the Riemann integral in $\mathbb R^n$. I've been thinking to this fact all the day but without any solution.","Today this question captured my attention, hence I want to generalize it. Let $X$ be a complex manifold of dimension $M$ and let $\omega $ be a $(n-p,n-q)$-differential form such that in each chart it is represented by locally integrable functions. A standard example of current on $X$ (see for example De Rham - Differential manifolds, Chap. III example 2) is the following: $$[\omega]:\alpha \mapsto\int_X \alpha\wedge\omega$$ where $\alpha$ is a $C^\infty$ $(p,q)$-form and $\alpha\wedge\omega$ is a locally integrable $(n,n)$-form. Who ensures that $\alpha\wedge\omega$ is integrable? Usually the integral is defined for smooth differential forms on oriented manifolds (see for example Lee's book). About this point I'm quite sure the answer will be: ""integration can be extended to locally integrable forms"", I just wanted  to check. Consider $n=1$, $p=0$, $q=0$ and let'es examine the example of the question linked above: $$\omega=\partial\bar\partial\log|f|$$ where $f$ is a meromorphic function on the Riemann surface $X$. Then the current $[\omega]$ is $$[\omega]:g \mapsto\int_X g\omega$$ for any $C^\infty$ function $g$. Here the problem: note that $\omega$ is $0$ almost everywhere, in particular $\omega$ is supported in the set of zeroes and poles of $f$ i.e. in a finite set! Why is the integral $\int_X g\omega$ different from $0$? It seems almost obvious to deduce that the integral of a differential form supported in a finite set is $0$ because of the properties of the Riemann integral in $\mathbb R^n$. I've been thinking to this fact all the day but without any solution.",,"['integration', 'differential-geometry', 'differential-forms', 'riemann-surfaces', 'complex-manifolds']"
50,How to show the space of closed curve is Hilbert manifold?,How to show the space of closed curve is Hilbert manifold?,,"In the picture below ,$(M,g)$ is a Riemannian manifold. Why $\mathcal L_M$ is a Hilbert submanifold of $L^{1,2}(S^1,R^r)$ ? Besides, what is the inner and name  of $L^{1,2}(S^1,R^r)$ ? The picture below is from the 3 page of  Kwangho Choi and Thomas H. Parker's Convergence of the heat flow for closed geodesics .","In the picture below ,$(M,g)$ is a Riemannian manifold. Why $\mathcal L_M$ is a Hilbert submanifold of $L^{1,2}(S^1,R^r)$ ? Besides, what is the inner and name  of $L^{1,2}(S^1,R^r)$ ? The picture below is from the 3 page of  Kwangho Choi and Thomas H. Parker's Convergence of the heat flow for closed geodesics .",,"['differential-geometry', 'riemannian-geometry', 'global-analysis']"
51,F-relatedness and product vector field,F-relatedness and product vector field,,"Let $M_1, M_2$ be smooth manifolds. $f: M_1 \times M_2 \longrightarrow M_1$ be the projection map. Let $X: M_1 \longrightarrow TM_1$ be a vector field. Show that there exists some vector field  $$Y: M_1 \times M_2 \longrightarrow T(M_1 \times M_2)$$ Such that $Y \text{ and } X$ are $f$-related. I need to verify that $$(1) \ \ \ (f_*)_pY_p = X_{p_1} \forall p = (p_1,p_2)\in M_1 \times M_2 $$ which is equivalent to $\forall h \in C^{\infty}(M_1)$ $$(2) \ \ \ Y(h\circ f) = Xh \circ f $$ So I constructed such $Y$ where $Y_p = (X_{p_1}, 0_{p_2})$ where $0_{p_2} \in T_{p_2}M_2$ is just the zero derivation. It seems ""obvious"" that the zero derivation doesn't do anything therefore the second condition is ""automatic""? But I'm not sure how to formally state that and as of right now it seems sketchy.","Let $M_1, M_2$ be smooth manifolds. $f: M_1 \times M_2 \longrightarrow M_1$ be the projection map. Let $X: M_1 \longrightarrow TM_1$ be a vector field. Show that there exists some vector field  $$Y: M_1 \times M_2 \longrightarrow T(M_1 \times M_2)$$ Such that $Y \text{ and } X$ are $f$-related. I need to verify that $$(1) \ \ \ (f_*)_pY_p = X_{p_1} \forall p = (p_1,p_2)\in M_1 \times M_2 $$ which is equivalent to $\forall h \in C^{\infty}(M_1)$ $$(2) \ \ \ Y(h\circ f) = Xh \circ f $$ So I constructed such $Y$ where $Y_p = (X_{p_1}, 0_{p_2})$ where $0_{p_2} \in T_{p_2}M_2$ is just the zero derivation. It seems ""obvious"" that the zero derivation doesn't do anything therefore the second condition is ""automatic""? But I'm not sure how to formally state that and as of right now it seems sketchy.",,"['differential-geometry', 'smooth-manifolds']"
52,Use of Gauss-Green identity,Use of Gauss-Green identity,,"In a solution of the problem 'what is the $C^1$ curve in $\mathbb R^2$, parametrized by arc length, whose inside area is maximum' my professor said that the area could be calculated as $\int_0^{2\pi} x y' dt$ where $x(t),y(t)$ is the parametrisation of the curve. This, he said, by Gauss-Green identity. The statement I know is $\int_{\Omega} \Delta u\ v+\int_\Omega \nabla u\cdot\nabla v =\int_{\partial \Omega}v\frac{\partial u}{\partial\nu}$. How can one relate this statement to the above result? (I think one should take $u=x$, $v=y$, but I don't see how this leads to the end, although it is the intuitive way, and, also, the problem seems in fact very easy). Thank you in advance.","In a solution of the problem 'what is the $C^1$ curve in $\mathbb R^2$, parametrized by arc length, whose inside area is maximum' my professor said that the area could be calculated as $\int_0^{2\pi} x y' dt$ where $x(t),y(t)$ is the parametrisation of the curve. This, he said, by Gauss-Green identity. The statement I know is $\int_{\Omega} \Delta u\ v+\int_\Omega \nabla u\cdot\nabla v =\int_{\partial \Omega}v\frac{\partial u}{\partial\nu}$. How can one relate this statement to the above result? (I think one should take $u=x$, $v=y$, but I don't see how this leads to the end, although it is the intuitive way, and, also, the problem seems in fact very easy). Thank you in advance.",,"['integration', 'differential-geometry']"
53,"If $\phi_1$ and $\phi_2$ are two paths in a Lie group, what is the derivative $(\phi_1 \times \phi_2)^\prime(0)$?","If  and  are two paths in a Lie group, what is the derivative ?",\phi_1 \phi_2 (\phi_1 \times \phi_2)^\prime(0),"Let $\phi_1, \phi_2$ be two paths in a Lie group $G$ with $\phi_i(0) = g_i \in G$ and $\phi^\prime(0) = X_i \in \frak{g}$. Denote $\psi(t) = \phi_1(t) \phi_2(t)$. We get $\psi(0) = g_1 g_2$. What is $\psi^\prime(0)$? By definition, it is the differential defined for any smooth $f : G \to \mathbb{R}$ by \begin{eqnarray*} \psi^\prime(0)(f) &=& \frac{d}{dt}\left(f \circ \psi(t)\right)|_{t = 0} \\ &=& \frac{d}{dt}\left(f \circ (\phi_1(t) \phi_2(t))\right)|_{t = 0} \\ &=& \frac{d}{dt} \left(f ( L_{\phi_1(t)} \phi_2(t) \right)|_{t = 0} \\ \end{eqnarray*} I do not see how to proceed from there and see how the derivation looks like.","Let $\phi_1, \phi_2$ be two paths in a Lie group $G$ with $\phi_i(0) = g_i \in G$ and $\phi^\prime(0) = X_i \in \frak{g}$. Denote $\psi(t) = \phi_1(t) \phi_2(t)$. We get $\psi(0) = g_1 g_2$. What is $\psi^\prime(0)$? By definition, it is the differential defined for any smooth $f : G \to \mathbb{R}$ by \begin{eqnarray*} \psi^\prime(0)(f) &=& \frac{d}{dt}\left(f \circ \psi(t)\right)|_{t = 0} \\ &=& \frac{d}{dt}\left(f \circ (\phi_1(t) \phi_2(t))\right)|_{t = 0} \\ &=& \frac{d}{dt} \left(f ( L_{\phi_1(t)} \phi_2(t) \right)|_{t = 0} \\ \end{eqnarray*} I do not see how to proceed from there and see how the derivation looks like.",,['differential-geometry']
54,Push-forward of vector fields of Heisenberg type group $\mathbb R \times \mathbb C$,Push-forward of vector fields of Heisenberg type group,\mathbb R \times \mathbb C,"I am learning some geometry by myself, so pleas be explicit as possible. My aim is to calculate the structure constants of the Lie algebra of $\mathbb R \times \mathbb C$ equipped with the low  $$(x_0,x) \cdot (y_0,y) = (x_0+y_0+\frac{1}{2}Im(x\bar{y}), x+y)$$ I think that, with the trivial chart $(\mathbb R \times \mathbb C,Id_{\mathbb R \times \mathbb C}) $ this can be viewed as a real manifold. Moreover, it is a lie group. Let $\ell_{(z_0,z)}$ be the left multiplication operator by $(z_0,z) \in \mathbb R \times \mathbb C$ I am confused with the combination of the real and complex variables in the definition of the group, for a basis of the tangent vectors at (0,0) I guess we should take something like $$\partial_{x_0}|_{(0,0)} , \partial_{x}|_{(0,0)} , \partial_{\bar{x}}|_{(0,0)}$$ but when I try to push-forward one of them by $\ell_{(z_0,z)}$ using the Jacobian matrix technique, which turns to be a $2\times 3$ matrix, I loose information about the last vector field (with the bar). I know I am probably doing a stupid error. I don't have time now to include my explicit computation, but I will add them later if it is needed. Update : I want to add that I am interested in results without using matrix representation, for learning purpose and for my intention of doing computations in higher dimension and in other similar lie groups. I tried to imitate the computation done in the following lecture note by M.L. Fels : An Introduction to Differential Geometry through Computation Page 61 § 4.2 $$ {\ell_{(z_0,z)}}_* \partial_{x_0}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} $$  $$ {\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} $$  $$ {\ell_{(z_0,z)}}_* \partial_{\bar x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} $$  where J|_{(0,0)}, is the Jacobian matrix ${\begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}}_{|(0,0)} = \begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}$ associated to the application $\ell_{(z_0,z)}$. I am confusing the actual tangent vector at $\ell_{(z_0,z)}(0,0) = (z_0,z)$ with its coefficient in the base $\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}$. The problem is that  these computations work well in  open sets of $\mathbb R^n$ but are not consistent when $\mathbb C$ is introduced. I guess I have to proceed otherwise? Probably by  rethinking about the chart and try to devise $\mathbb C$ into $\mathbb R \times \mathbb R$ ? To illustrate the problem : if we do the computation for the following $${\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} =  \begin{pmatrix} -\frac{1}{4} \bar{z} \\ 1 \end{pmatrix}$$ the result is only two coefficients, but we need three to write the tangent vector in $\{\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}\}$","I am learning some geometry by myself, so pleas be explicit as possible. My aim is to calculate the structure constants of the Lie algebra of $\mathbb R \times \mathbb C$ equipped with the low  $$(x_0,x) \cdot (y_0,y) = (x_0+y_0+\frac{1}{2}Im(x\bar{y}), x+y)$$ I think that, with the trivial chart $(\mathbb R \times \mathbb C,Id_{\mathbb R \times \mathbb C}) $ this can be viewed as a real manifold. Moreover, it is a lie group. Let $\ell_{(z_0,z)}$ be the left multiplication operator by $(z_0,z) \in \mathbb R \times \mathbb C$ I am confused with the combination of the real and complex variables in the definition of the group, for a basis of the tangent vectors at (0,0) I guess we should take something like $$\partial_{x_0}|_{(0,0)} , \partial_{x}|_{(0,0)} , \partial_{\bar{x}}|_{(0,0)}$$ but when I try to push-forward one of them by $\ell_{(z_0,z)}$ using the Jacobian matrix technique, which turns to be a $2\times 3$ matrix, I loose information about the last vector field (with the bar). I know I am probably doing a stupid error. I don't have time now to include my explicit computation, but I will add them later if it is needed. Update : I want to add that I am interested in results without using matrix representation, for learning purpose and for my intention of doing computations in higher dimension and in other similar lie groups. I tried to imitate the computation done in the following lecture note by M.L. Fels : An Introduction to Differential Geometry through Computation Page 61 § 4.2 $$ {\ell_{(z_0,z)}}_* \partial_{x_0}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} $$  $$ {\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} $$  $$ {\ell_{(z_0,z)}}_* \partial_{\bar x}|_{(0,0)} = J|_{(0,0)} \cdot \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} $$  where J|_{(0,0)}, is the Jacobian matrix ${\begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}}_{|(0,0)} = \begin{pmatrix} 1 & -\frac{1}{4}\bar{z} & \frac{1}{4} z \\ 0 & 1 & 0 \end{pmatrix}$ associated to the application $\ell_{(z_0,z)}$. I am confusing the actual tangent vector at $\ell_{(z_0,z)}(0,0) = (z_0,z)$ with its coefficient in the base $\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}$. The problem is that  these computations work well in  open sets of $\mathbb R^n$ but are not consistent when $\mathbb C$ is introduced. I guess I have to proceed otherwise? Probably by  rethinking about the chart and try to devise $\mathbb C$ into $\mathbb R \times \mathbb R$ ? To illustrate the problem : if we do the computation for the following $${\ell_{(z_0,z)}}_* \partial_{x}|_{(0,0)} =  \begin{pmatrix} -\frac{1}{4} \bar{z} \\ 1 \end{pmatrix}$$ the result is only two coefficients, but we need three to write the tangent vector in $\{\partial_{x_0}|_{(z_0,z)} , \partial_{x}|_{(z_0,z)} , \partial_{\bar{x}}|_{(z_0,z)}\}$",,"['differential-geometry', 'lie-groups']"
55,Failure of geodesic uniqueness - what does it say about the manifold?,Failure of geodesic uniqueness - what does it say about the manifold?,,"I am more of a physicist than a mathematician, but this question is properly mathematical rather than physical, even though it is motivated by a physical application; please assume mathematical ignorance (and apologies for any abuse of notation and terminology ). Take a standard general relativist's spacetime manifold $M$ , i.e. a semi-Riemannian/Lorentzian paracompact Hausdorff manifold such that $\forall p\in M$ there is an unique (timelike) geodesic $\gamma_{p,\vec{v}}$ through $p$ with a given velocity $\vec{v}$ (i.e. for each $\vec{v}\in T_{p}M$ ). The specific question is this. If, after unspecified cut & join operations on $M$ , one finds that for some $p\in M, \vec{v}$ there are at least two (timelike) originally geodesic curves through $p$ : What properties of the manifold required for uniqueness theorems to apply (e.g Picard-Lindelöf/Cauchy-Lipschitz) are no longer satisfied (and specifically, why), and thus where do the uniqueness arguments break down? Are there some properties which necessarily have been violated, or are there one or more merely sufficient violations? I am most interested in the answers for excisions limited to the interiors of 4D regions within $M$ , but pedagogical examples of other cases are also welcome. 1 Aug 16 - Additional Illustrations & Commentary For clarification, as requested. For definiteness: initial manifold is Minkowksi space and $\gamma_{L}, \gamma_{R}$ are geodesics with $\vec{v}=0$ in the chosen frame, but the question remains for Lorentzian manifolds in general, as originally described; two spatial dimensions suppressed. Specify two tubes as sphere $\times$ the solid lines; excise the interiors (stippled regions) and identify the surfaces of the spheres as indicated at equal $t$ ; after the cut and glue*... Fig 1: The walls of the excised regions are not parallel; geodesics $\gamma_{L}, \gamma_{R}$ are initally distinct, but the identification of $p, q$ creates a Y and the geodesics can only continue on the LHS; there appear to be two geodesics through $p, q$ Fig 2: The walls of the excised regions are parallel; $\gamma_{L}, \gamma_{R}$ cross - or continue on the same side without any rationale for going one way or the other. Thoughts on what is wrong with the resulting manifold (Previously omitted to keep the question short and clean.) In excising the interiors of the 4D regions specified, a 3D boundary is created (time x 2D spatial). The indentification stated (which is traditional in physics literature) is purely spatial therefore the boundary does not seem to be completely removed. Whether, if true, this affects the differential structure and in such a way as to nullify the geodesic nature of one or more curves I don't know: in Minkowski space the metric is constant and it is hard for me to see how the cut & glue is problematic from this perspective, or in the more general case if derivatives are defined for closed intervals on $\mathbb{R}$ , which they can be, though I can see there might be a problem with the total derivative on $\mathbb{R}^{4}$ . In Fig 1 I do not think the solution is that the RHS line can be rotated to match the LHS because the implied homeomorphism does not, AFAICT, result in the neighbouhood $p,q$ becoming homemorphic to $\mathbb{R}^{4}$ because an extra gluing step is required. Is there a legitimate way to do this? That could be part of the answer; I don't know - hence the original question. Fig 2 is most illustrative because at $p,q$ the tangents are parallel, so upon identification there is no need for any ""rotation"", but is the neighbouhood of $p,q$ homeomorphic to $\mathbb{R}^{4}$ ? Conjecture I conjecture - and am trying to prove - that the excised regions have a unique geometry if local geodesic existence and uniqueness is to be preserved through the cut and glue operation, and that geometry is some (smooth, closed, etc.) surface $\times$ a geodesic congruence; my problem with proving it is that I think I can see something's wrong if the geometry is incorrect, but I can't identify the right mathematical concepts to pin down the issue. * I deliberately used ""join"" previously because I was aware of standard ""cut and glue"" methods and did not wish to imply any such that might have been incompatible with the motivation of the question.","I am more of a physicist than a mathematician, but this question is properly mathematical rather than physical, even though it is motivated by a physical application; please assume mathematical ignorance (and apologies for any abuse of notation and terminology ). Take a standard general relativist's spacetime manifold , i.e. a semi-Riemannian/Lorentzian paracompact Hausdorff manifold such that there is an unique (timelike) geodesic through with a given velocity (i.e. for each ). The specific question is this. If, after unspecified cut & join operations on , one finds that for some there are at least two (timelike) originally geodesic curves through : What properties of the manifold required for uniqueness theorems to apply (e.g Picard-Lindelöf/Cauchy-Lipschitz) are no longer satisfied (and specifically, why), and thus where do the uniqueness arguments break down? Are there some properties which necessarily have been violated, or are there one or more merely sufficient violations? I am most interested in the answers for excisions limited to the interiors of 4D regions within , but pedagogical examples of other cases are also welcome. 1 Aug 16 - Additional Illustrations & Commentary For clarification, as requested. For definiteness: initial manifold is Minkowksi space and are geodesics with in the chosen frame, but the question remains for Lorentzian manifolds in general, as originally described; two spatial dimensions suppressed. Specify two tubes as sphere the solid lines; excise the interiors (stippled regions) and identify the surfaces of the spheres as indicated at equal ; after the cut and glue*... Fig 1: The walls of the excised regions are not parallel; geodesics are initally distinct, but the identification of creates a Y and the geodesics can only continue on the LHS; there appear to be two geodesics through Fig 2: The walls of the excised regions are parallel; cross - or continue on the same side without any rationale for going one way or the other. Thoughts on what is wrong with the resulting manifold (Previously omitted to keep the question short and clean.) In excising the interiors of the 4D regions specified, a 3D boundary is created (time x 2D spatial). The indentification stated (which is traditional in physics literature) is purely spatial therefore the boundary does not seem to be completely removed. Whether, if true, this affects the differential structure and in such a way as to nullify the geodesic nature of one or more curves I don't know: in Minkowski space the metric is constant and it is hard for me to see how the cut & glue is problematic from this perspective, or in the more general case if derivatives are defined for closed intervals on , which they can be, though I can see there might be a problem with the total derivative on . In Fig 1 I do not think the solution is that the RHS line can be rotated to match the LHS because the implied homeomorphism does not, AFAICT, result in the neighbouhood becoming homemorphic to because an extra gluing step is required. Is there a legitimate way to do this? That could be part of the answer; I don't know - hence the original question. Fig 2 is most illustrative because at the tangents are parallel, so upon identification there is no need for any ""rotation"", but is the neighbouhood of homeomorphic to ? Conjecture I conjecture - and am trying to prove - that the excised regions have a unique geometry if local geodesic existence and uniqueness is to be preserved through the cut and glue operation, and that geometry is some (smooth, closed, etc.) surface a geodesic congruence; my problem with proving it is that I think I can see something's wrong if the geometry is incorrect, but I can't identify the right mathematical concepts to pin down the issue. * I deliberately used ""join"" previously because I was aware of standard ""cut and glue"" methods and did not wish to imply any such that might have been incompatible with the motivation of the question.","M \forall p\in M \gamma_{p,\vec{v}} p \vec{v} \vec{v}\in T_{p}M M p\in M, \vec{v} p M \gamma_{L}, \gamma_{R} \vec{v}=0 \times t \gamma_{L}, \gamma_{R} p, q p, q \gamma_{L}, \gamma_{R} \mathbb{R} \mathbb{R}^{4} p,q \mathbb{R}^{4} p,q p,q \mathbb{R}^{4} \times","['differential-geometry', 'geodesic', 'semi-riemannian-geometry']"
56,How to derive Lobachevsky's formula for the angle of parallelism,How to derive Lobachevsky's formula for the angle of parallelism,,I'm lightly studying some non-Euclidean Geometry and in the book I am reading there is no proof or derivation from where the Lobachevsky formula for angle of parallelism comes from: $$\Pi(x)=2\tan^{-1}\left(e^{-x}\right)$$ Any help? Thanks P.S. I couldn't find anything by google search either.,I'm lightly studying some non-Euclidean Geometry and in the book I am reading there is no proof or derivation from where the Lobachevsky formula for angle of parallelism comes from: Any help? Thanks P.S. I couldn't find anything by google search either.,\Pi(x)=2\tan^{-1}\left(e^{-x}\right),"['differential-geometry', 'hyperbolic-geometry', 'noneuclidean-geometry']"
57,Abstract algebraic definition of dual tangent spaces,Abstract algebraic definition of dual tangent spaces,,"I know that if $(M,\mathcal{A})$ is a smooth manifold, the dual tangent space at $p\in M$ can be defined as $$ T^*_pM=I_p/I_p^2, $$ where $I_p$ is the ideal of the ring $C^\infty(M)$ consisting of smooth functions that vanish at $p$ and $I_p^2$ is the second power of this ideal. I understand that this definition is useful, because, unlike other definitions, this one can be generalized to situations when you don't have the smooth structure $\mathcal{A}$, however this definition is so unintuitive I have a hard time grasping it. According to wikipedia, the product of ideals $A$ and $B$ is defined as $$ AB=\{a_1b_1+...a_nb_n|\ a_i\in A,b_i\in B,n\in\mathbb{N}\}, $$ I assume the point of this definition is that by demanding that the functions vanish, we make sure that the functions' Taylor expansion has no zeroth order terms, and since the elements of the product ideal are second order expressions, I assume the quotient is needed to get rid of second order terms. Seems logical, since by the usual definition we can infer that the contangent space is generated by differentials of functions, which are, of course, the first order part. Am I correct in this? If I am correct on the first point, what about higher than second order terms? Shouldn't we need to quotient those out too? Thinking about it as I write this post, elements of $I_p^2$ are second order algebraic expressions from $C^\infty(M)$, but they are not polynomials, a Taylor expansion, however is polynomial. I think I don't understand it now. This line of thought seems to depend on the functions having Taylor expansions. However I know this definition is generalizable to algebraic varieties as well as locally ringed spaces in general. Without having a structure that allows Taylor expansions, how do we know that this grasps the concept of ""functions having the same first order behaviour at $p$"" for those cases as well? How can we see that this quotient space is finite $n$ dimensional? If the proof is not particularily pleasant, I don't expect anyone to actually post it here, but a reference to a manifold theory textbook that utilizes this definition heavily enough to also contain related proofs would be nice. I would appreciate any response, even if it doesn't address these points directly, if it can help me see intuitively why this particular quotient space has the same meaning as the usual definition of dual tangent space. EDIT: Since there have been misunderstandings, I wish to clarify: In my last bullet point I am not asking how to prove that the tangent/cotangent space is $n$ dimensional for an $n$ dimensional manifold. I am asking how to see that the space $I_p/I_p^2$ is $n$ dimensional, or alternatively, how to see that it is isomorphic to the cotangent space (defined by any alternative means).","I know that if $(M,\mathcal{A})$ is a smooth manifold, the dual tangent space at $p\in M$ can be defined as $$ T^*_pM=I_p/I_p^2, $$ where $I_p$ is the ideal of the ring $C^\infty(M)$ consisting of smooth functions that vanish at $p$ and $I_p^2$ is the second power of this ideal. I understand that this definition is useful, because, unlike other definitions, this one can be generalized to situations when you don't have the smooth structure $\mathcal{A}$, however this definition is so unintuitive I have a hard time grasping it. According to wikipedia, the product of ideals $A$ and $B$ is defined as $$ AB=\{a_1b_1+...a_nb_n|\ a_i\in A,b_i\in B,n\in\mathbb{N}\}, $$ I assume the point of this definition is that by demanding that the functions vanish, we make sure that the functions' Taylor expansion has no zeroth order terms, and since the elements of the product ideal are second order expressions, I assume the quotient is needed to get rid of second order terms. Seems logical, since by the usual definition we can infer that the contangent space is generated by differentials of functions, which are, of course, the first order part. Am I correct in this? If I am correct on the first point, what about higher than second order terms? Shouldn't we need to quotient those out too? Thinking about it as I write this post, elements of $I_p^2$ are second order algebraic expressions from $C^\infty(M)$, but they are not polynomials, a Taylor expansion, however is polynomial. I think I don't understand it now. This line of thought seems to depend on the functions having Taylor expansions. However I know this definition is generalizable to algebraic varieties as well as locally ringed spaces in general. Without having a structure that allows Taylor expansions, how do we know that this grasps the concept of ""functions having the same first order behaviour at $p$"" for those cases as well? How can we see that this quotient space is finite $n$ dimensional? If the proof is not particularily pleasant, I don't expect anyone to actually post it here, but a reference to a manifold theory textbook that utilizes this definition heavily enough to also contain related proofs would be nice. I would appreciate any response, even if it doesn't address these points directly, if it can help me see intuitively why this particular quotient space has the same meaning as the usual definition of dual tangent space. EDIT: Since there have been misunderstandings, I wish to clarify: In my last bullet point I am not asking how to prove that the tangent/cotangent space is $n$ dimensional for an $n$ dimensional manifold. I am asking how to see that the space $I_p/I_p^2$ is $n$ dimensional, or alternatively, how to see that it is isomorphic to the cotangent space (defined by any alternative means).",,"['algebraic-geometry', 'differential-geometry', 'sheaf-theory']"
58,Why doesn't this argument show the Möbius bundle is trivial?,Why doesn't this argument show the Möbius bundle is trivial?,,"I wrote the following argument to prove that $S^1$ is parallelizable, that is, to show that the tangent bundle is trivial. It looks fairly reasonable to me. Let $\tau=2\pi$. We define a map $\varphi:S^1\times\Bbb R\to TS^1$ by   $\varphi((e^{i\tau\theta},  t))=(e^{i\tau\theta},t\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau\theta}})$. This map is clearly a bijection:   injectivity follows trivially and surjectivity follows since the   tangent space $T_{e^{i\tau\theta}}$ is one-dimensional. It remains to be shown that it is a diffeomorphism. We will, in fact,   show that it is the identity map on suitably chosen coordinates. For   any point $(e^{i\tau\theta},t)\in S^1\times\Bbb R$, we can choose the   coordinate chart such that   $(e^{i\tau(\theta+\varepsilon)},(t+\delta))$, for sufficiently small   $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$. Similarly, for any point $(e^{i\tau\theta},t\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau\theta}})\in TS^1$, we can choose the coordinate   chart such that   $(e^{i\tau(\theta+\varepsilon)},(t+\delta)\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau(\theta+\varepsilon)}})$, for sufficiently small   $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$.  Then,   we have that $\widehat\varphi$ (the function with respect to these two   coordinates) sends $(\varepsilon,\delta)\mapsto (\varepsilon,\delta)$,   as desired. However, I don't see where this is using the tangent bundle construction in any meaningful way. It seems that this would work just as well for any rank-1 vector bundle over the sphere. That concerns me, because of course the conclusion is not true: for instance the Möbius strip is (diffeomorphic to) a vector bundle over the sphere. Since it is not orientable, we know that the bundle is nontrivial. So the question is: Where does this break for a general vector bundle? Or if the argument is simply invalid, can it be fixed without much trouble?","I wrote the following argument to prove that $S^1$ is parallelizable, that is, to show that the tangent bundle is trivial. It looks fairly reasonable to me. Let $\tau=2\pi$. We define a map $\varphi:S^1\times\Bbb R\to TS^1$ by   $\varphi((e^{i\tau\theta},  t))=(e^{i\tau\theta},t\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau\theta}})$. This map is clearly a bijection:   injectivity follows trivially and surjectivity follows since the   tangent space $T_{e^{i\tau\theta}}$ is one-dimensional. It remains to be shown that it is a diffeomorphism. We will, in fact,   show that it is the identity map on suitably chosen coordinates. For   any point $(e^{i\tau\theta},t)\in S^1\times\Bbb R$, we can choose the   coordinate chart such that   $(e^{i\tau(\theta+\varepsilon)},(t+\delta))$, for sufficiently small   $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$. Similarly, for any point $(e^{i\tau\theta},t\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau\theta}})\in TS^1$, we can choose the coordinate   chart such that   $(e^{i\tau(\theta+\varepsilon)},(t+\delta)\frac{\partial}{\partial  x^1}\Big|_{e^{i\tau(\theta+\varepsilon)}})$, for sufficiently small   $\varepsilon$ and $\delta$, is given by $(\varepsilon,\delta)$.  Then,   we have that $\widehat\varphi$ (the function with respect to these two   coordinates) sends $(\varepsilon,\delta)\mapsto (\varepsilon,\delta)$,   as desired. However, I don't see where this is using the tangent bundle construction in any meaningful way. It seems that this would work just as well for any rank-1 vector bundle over the sphere. That concerns me, because of course the conclusion is not true: for instance the Möbius strip is (diffeomorphic to) a vector bundle over the sphere. Since it is not orientable, we know that the bundle is nontrivial. So the question is: Where does this break for a general vector bundle? Or if the argument is simply invalid, can it be fixed without much trouble?",,"['differential-geometry', 'differential-topology', 'vector-bundles']"
59,Is $T\mathbb{C}\mathbb{P}^n$ globally generated?,Is  globally generated?,T\mathbb{C}\mathbb{P}^n,"A vector bundle $E\to X$ is globally generated if there exists global holomorphic sections $s_1,\dots,s_n$ such that $E_x$ is spanned by $s_1(x),\dots,s_n(x)$ for all $x\in X$. Consider the projection map $p:\mathbb{C}^{n+1}\to\mathbb{P}^n$. It can be shown that $p$ is a smooth submersion. Furthermore, given $n+1$ linear functions $l_i:\mathbb{C}^{n+1}\to\mathbb{C}$, with $i=0,\dots,n$, and taking the coordinates in $\mathbb{C}^{n+1}$ to be $(z_0,\dots,z_n)$, it can be shown that the global vector field  $$ \sum_{i=0}^nl(z)\frac{\partial}{\partial z_i} $$ in $\mathbb{C}^{n+1}$ can be pushed forward to a global vector field of $\mathbb{P}^n$. Motivated by Ted Shifrin's answer in this question I believe that all vector fields in $\mathbb{P}^n$ must be of this form, implying that $T\mathbb{P}^n$ is globally generated. Explicitely, (I believe, a confirmation of the following would be nice) the pushforward of this vector field is: $$ p_*\left(\sum_{i=0}^nl(z)\frac{\partial}{\partial z_i}\right)= \sum_{i=0}^nl(z)p_*\left(\frac{\partial}{\partial z_i}\right), $$ where, for instance in the coordinate chart $U_0$ of $\mathbb{P}^n$ with coordinates $w_k=z_k/z_0$, $$ p_*\left(\frac{\partial}{\partial z_0}\right)=-\sum_{i=1}^n \frac{z_i}{z_0^2}\frac{\partial}{\partial w_i}, $$ and $$ p_*\left(\frac{\partial}{\partial z_k}\right)=\frac{1}{z_0}\frac{\partial}{\partial w_k}. $$ In particular this calculation in coordinates shows the need of multiplying by a linear function to get a well defined vector field. My problem is I don't see why every vector field in projective space must be of this form, locally and/or globally.","A vector bundle $E\to X$ is globally generated if there exists global holomorphic sections $s_1,\dots,s_n$ such that $E_x$ is spanned by $s_1(x),\dots,s_n(x)$ for all $x\in X$. Consider the projection map $p:\mathbb{C}^{n+1}\to\mathbb{P}^n$. It can be shown that $p$ is a smooth submersion. Furthermore, given $n+1$ linear functions $l_i:\mathbb{C}^{n+1}\to\mathbb{C}$, with $i=0,\dots,n$, and taking the coordinates in $\mathbb{C}^{n+1}$ to be $(z_0,\dots,z_n)$, it can be shown that the global vector field  $$ \sum_{i=0}^nl(z)\frac{\partial}{\partial z_i} $$ in $\mathbb{C}^{n+1}$ can be pushed forward to a global vector field of $\mathbb{P}^n$. Motivated by Ted Shifrin's answer in this question I believe that all vector fields in $\mathbb{P}^n$ must be of this form, implying that $T\mathbb{P}^n$ is globally generated. Explicitely, (I believe, a confirmation of the following would be nice) the pushforward of this vector field is: $$ p_*\left(\sum_{i=0}^nl(z)\frac{\partial}{\partial z_i}\right)= \sum_{i=0}^nl(z)p_*\left(\frac{\partial}{\partial z_i}\right), $$ where, for instance in the coordinate chart $U_0$ of $\mathbb{P}^n$ with coordinates $w_k=z_k/z_0$, $$ p_*\left(\frac{\partial}{\partial z_0}\right)=-\sum_{i=1}^n \frac{z_i}{z_0^2}\frac{\partial}{\partial w_i}, $$ and $$ p_*\left(\frac{\partial}{\partial z_k}\right)=\frac{1}{z_0}\frac{\partial}{\partial w_k}. $$ In particular this calculation in coordinates shows the need of multiplying by a linear function to get a well defined vector field. My problem is I don't see why every vector field in projective space must be of this form, locally and/or globally.",,"['differential-geometry', 'vector-bundles']"
60,convention of a default atlas,convention of a default atlas,,"Recently, I have been studying the basics of differential geometry and te necessary preliminaries. I arrived at the construction of differential structure on topological manifolds, where the non-uniqueness of these structures is mentioned. After this, the precence of a (pick of) structure seems to be assumed in most of what follows. From what I can tell, it often looks like there is an obvious choice, while the others are labled 'exotic'. This seems to be most obvious when looking at $\mathbb{R^4}$ . There is an entire wikipedia page dedicated to exotic versions of $\mathbb{R^4}$ , that are homeomorphic, but not diffeomorphic to $\mathbb{R^4}$ as a smooth manifold. My question then becomes: when no differential structure is given, is there a convention of a structure to pick? How do I distinguish between the exotic structures and the 'usual' ones. In the case of $\mathbb{R^4}$ , I would bet the default structure would/could consist of an atlas, build from all of the open sets, equipped with the identity from $\mathbb{R^4}$ into $\mathbb{R^4}$ . As my background is in physics, I am in particular interested in the differential structure that one would usually establish on a manifold in the context of four dimentional General Relativity. But perhaps this should be a sepparate question on the physics SE. I apologize in advance for improper use of jargon or other, possibly trivial mistakes. Feel free to correct me. edit To further clairify the source of this question as well as my confusion: From my experience with GR it seems physicists usually take a (pseudo?) Riemanian metric as a starting point. To me it seems this fixes only a piece of the manifold, described by the single coordinate map. The physicist would then declare that this describes a manifold, but i am uncertain how to pick the additional contributions to the atlas. On top of this, the physicist might sometimes perform a coordinate transformation, which seems tricky to me when the rest of the chart was not well comunicated. How would I know if the new coordinates would be in the same chart? In addition, I am uncertain weither this should be discussed here, or on the physiscs SE. edit 2 I will ay some point pose a related question about GR on the physics SE. The question then reduces to one of mathematical convention: weither or not $\mathbb{R}^n$ is usually assumed to be equipped with the identity on open sets, as an atlas, and weither or not other 'assumptions' exist when the atlas definition is left out. edit 3 new related thread in physics SE https://physics.stackexchange.com/questions/566643/how-to-select-which-differentiable-manifold-to-use-to-model-spacetime/661820","Recently, I have been studying the basics of differential geometry and te necessary preliminaries. I arrived at the construction of differential structure on topological manifolds, where the non-uniqueness of these structures is mentioned. After this, the precence of a (pick of) structure seems to be assumed in most of what follows. From what I can tell, it often looks like there is an obvious choice, while the others are labled 'exotic'. This seems to be most obvious when looking at . There is an entire wikipedia page dedicated to exotic versions of , that are homeomorphic, but not diffeomorphic to as a smooth manifold. My question then becomes: when no differential structure is given, is there a convention of a structure to pick? How do I distinguish between the exotic structures and the 'usual' ones. In the case of , I would bet the default structure would/could consist of an atlas, build from all of the open sets, equipped with the identity from into . As my background is in physics, I am in particular interested in the differential structure that one would usually establish on a manifold in the context of four dimentional General Relativity. But perhaps this should be a sepparate question on the physics SE. I apologize in advance for improper use of jargon or other, possibly trivial mistakes. Feel free to correct me. edit To further clairify the source of this question as well as my confusion: From my experience with GR it seems physicists usually take a (pseudo?) Riemanian metric as a starting point. To me it seems this fixes only a piece of the manifold, described by the single coordinate map. The physicist would then declare that this describes a manifold, but i am uncertain how to pick the additional contributions to the atlas. On top of this, the physicist might sometimes perform a coordinate transformation, which seems tricky to me when the rest of the chart was not well comunicated. How would I know if the new coordinates would be in the same chart? In addition, I am uncertain weither this should be discussed here, or on the physiscs SE. edit 2 I will ay some point pose a related question about GR on the physics SE. The question then reduces to one of mathematical convention: weither or not is usually assumed to be equipped with the identity on open sets, as an atlas, and weither or not other 'assumptions' exist when the atlas definition is left out. edit 3 new related thread in physics SE https://physics.stackexchange.com/questions/566643/how-to-select-which-differentiable-manifold-to-use-to-model-spacetime/661820",\mathbb{R^4} \mathbb{R^4} \mathbb{R^4} \mathbb{R^4} \mathbb{R^4} \mathbb{R^4} \mathbb{R}^n,"['differential-geometry', 'convention']"
61,Infinite surface area,Infinite surface area,,"I am reading an article (reference: http://www.jstor.org/stable/1971139?seq=1#page_scan_tab_contents ), and in the proof of the main theorem, the author states that ""it is a fact that complete, simply-connected surfaces in $\mathbb{R}^3$ of non-positive Gaussian curvature have infinite area"" without any reference. I was not able to prove this so far. How can one see this result? Is this fact known as a theorem?","I am reading an article (reference: http://www.jstor.org/stable/1971139?seq=1#page_scan_tab_contents ), and in the proof of the main theorem, the author states that ""it is a fact that complete, simply-connected surfaces in $\mathbb{R}^3$ of non-positive Gaussian curvature have infinite area"" without any reference. I was not able to prove this so far. How can one see this result? Is this fact known as a theorem?",,['differential-geometry']
62,Surface area of Convex bodies contained in one another,Surface area of Convex bodies contained in one another,,"Suppose we have two compact convex bodies one contained in the other in $\mathbb{R}^n$, $C\subset D\subset \mathbb{R}^n$. Does it follow that the ($n-1$ dimensional) surface area of $C$ is less than $D$? If so is there a natural sequence of $n-k$  dimensional quantities ($g_0=$Volume, $g_1=$Surface area,...) such that whenever $C\subset D\subset \mathbb{R}^n$ (where $C$ and $D$ are again compact) and $n>k$ we have $g_k(C)>g_k(D)$?","Suppose we have two compact convex bodies one contained in the other in $\mathbb{R}^n$, $C\subset D\subset \mathbb{R}^n$. Does it follow that the ($n-1$ dimensional) surface area of $C$ is less than $D$? If so is there a natural sequence of $n-k$  dimensional quantities ($g_0=$Volume, $g_1=$Surface area,...) such that whenever $C\subset D\subset \mathbb{R}^n$ (where $C$ and $D$ are again compact) and $n>k$ we have $g_k(C)>g_k(D)$?",,"['real-analysis', 'differential-geometry', 'inequality', 'convex-geometry', 'integral-geometry']"
63,Need help understanding this example of a distribution,Need help understanding this example of a distribution,,"Consider the following example of a distribution (given here ): I tried to draw this. If $p=(a,b,c)$ then  $$ X_p = (1,0,-b), Y_p = (0,1,0)$$ Then the planes in the distribution are planes spanned by $X_p,Y_p$. We see that the plane spanned by $X_p,Y_p$ is a plane that rotates around the vector $Y_p$ as $p$ moves along the $y$-axis. Assume we had a surface $S$ that was tangent to all this twirling planes. Without loss of generality, assume the surface is located in $\mathbb R^3$ such that the origin is on the surface. Then we have a plane, coincidentally parallel to the $xy$-plane, that is tangent to $S$ at $0$. In other words: the $xy$-plane is tangent to $S$. This is as far as I can follow the explanation given in the text. But everything that follows I do not understand. For example, only because the $xy$-plane is tangent to $S$ at $0$ it is not clear to me why $S$ would intersect the $x$-axis in a line segment (for example, $S^2$ can be tangent to the $xy$-plane an does not intersect the $x$-axis in a line segment). But even if this was clear to me and I assume that $S$ intersects this axis in a line segment the rest of the explanation is also not clear to me: travelling along an intersection axis does not seem to contradict that the planes are twisting. Please could someone explain this to me?","Consider the following example of a distribution (given here ): I tried to draw this. If $p=(a,b,c)$ then  $$ X_p = (1,0,-b), Y_p = (0,1,0)$$ Then the planes in the distribution are planes spanned by $X_p,Y_p$. We see that the plane spanned by $X_p,Y_p$ is a plane that rotates around the vector $Y_p$ as $p$ moves along the $y$-axis. Assume we had a surface $S$ that was tangent to all this twirling planes. Without loss of generality, assume the surface is located in $\mathbb R^3$ such that the origin is on the surface. Then we have a plane, coincidentally parallel to the $xy$-plane, that is tangent to $S$ at $0$. In other words: the $xy$-plane is tangent to $S$. This is as far as I can follow the explanation given in the text. But everything that follows I do not understand. For example, only because the $xy$-plane is tangent to $S$ at $0$ it is not clear to me why $S$ would intersect the $x$-axis in a line segment (for example, $S^2$ can be tangent to the $xy$-plane an does not intersect the $x$-axis in a line segment). But even if this was clear to me and I assume that $S$ intersects this axis in a line segment the rest of the explanation is also not clear to me: travelling along an intersection axis does not seem to contradict that the planes are twisting. Please could someone explain this to me?",,"['differential-geometry', 'contact-topology']"
64,Eigenvalues of shape operator and of curvature on second exterior power,Eigenvalues of shape operator and of curvature on second exterior power,,"Terminology note In the following, a scalar product will be a symmetric bilinear form, and a euclidean scalar product will be a positive definite scalar product. This is the terminology used by my Differential Geometry teacher. Background Let me sum up the operators coming in play in the question. To start right from the start, we have, in as general a context as possible, a Riemannian manifold $(M,g)$. The metric induces a single Levi-Civita connection , which I will denote $\nabla_XY$, where $X,Y$ are fields and $\nabla_XY$ is the covariant derivative of $Y$ along $X$, as it is also called. The curvature tensor is then defined as: $$R(X,Y)Z=\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z=[\nabla_X,\nabla_Y]Z-\nabla_{[X,Y]}Z.$$ This is the type $(1,3)$ curvature tensor, whereas the type $(0,4)$ tensor is: $$R(X,Y,Z,W)=g(R(X,Y)Z,W).$$ This can be proven to be antisimmetric in the first two and second two entries, and symmetric w.r.t. swapping the two couples (i.e. $R(X,Y,Z,W)=R(Z,W,X,Y)$). This means it induces a scalar product on the second exterior power, which we define as: $$\mathcal{R}(X\wedge Y,Z\wedge W)=R(X,Y,W,Z).$$ $g$ also induces a scalar product on the second exterior power: $$g'(v\wedge w,u\wedge t)=g(v,u)g(w,t)-g(v,t)g(w,u).$$ This can be shown to be positive definite -- in fact, if $\{v_j\}$ is orthonormal w.r.t $g$< $\{v_i\wedge v_j\}_{i<j}$ is orthonormal w.r.t. $g'$. Fixing this scalar product as a ""reference"", since $\mathcal{R}$ is also a scalar product, we get a self-adjoint operator, which we still denote $\mathcal{R}$, by setting: $$g'(\mathcal{R}(v\wedge w),u\wedge t)=\mathcal{R}(v\wedge w,u\wedge t).$$ Oh I'm writing all of this with decomposable vectors but this is extended by additivity to non-decomposable ones of course. The last operator we need is the shape operator. So we consider a hypersurface $N\subseteq M$. Locally, we can always find a normal unitary vector field $\mathcal{N}$ for $N$. Modulo a sign, this induces an operator, called shape operator , which is defined by: $$S(v)=\nabla_X\mathcal{N},$$ where $X$ is any field which evalues to $v$ at the tangency point $p$ of $v$ to $M$. It can be shown that this is a $g$-self-adjoint endomorphism of $T_pN$, and that $S(\mathcal{N}(p))=0$. Question So here comes, at last, the question. I have been given the following exercise. Let $M$ be a hypersurface of $\mathbb{R}^n$. Suppose $M$ is compact, connected, orientable (i.e. has a global unitary normal field). Assume $\{v_j\}$ is an orthonormal basis for the tangent space $T_pM$ consisting of eigenvalues of the shape operator $S$. Prove that $\{v_i\wedge v_j\}$ is an o.n.b. of $\Lambda^2T_pM$ composed of eigenvalues of $\mathcal{R}$, and more precisely $\mathcal{R}(v_i\wedge v_j)=\lambda_i\lambda_jv_i\wedge v_j$, where $S(v_i)=\lambda_iv_i$. Now the fact that those wedges provide an orthonormal basis is easy, since we know (by earlier proof in the course) that it is a basis, and orthonormality follows from a simple computation: $$g'(v_i\wedge v_j,v_k\wedge v_\ell)=g(v_i,v_k)g(v_j,v_\ell)-g(v_i,v_\ell)g(v_j,v_k)=\delta_{ik}\delta_{j\ell}-\delta_{i\ell}\delta_{jk},$$ we need $i<j$ and $k<\ell$ by definition of the basis, if the two are equal the first term is $1\cdot1$ and the second one $0\cdot0$, so it is a normal basis, and if they are distinct then the first term is 0, and if $i\neq k$ it could be that $i=\ell$ but the $j\neq k$ since $j>i=\ell>k$, so the second term is also zero, whereas if $j\neq\ell$ it could be $j=k$, but $i\neq\ell$ by a similar argument, giving again that both terms are zero. As for the fact those vectors are eigenvalues of $\mathcal{R}$, I am pretty much at a loss: what is the link between $\mathcal{R}$ and $S$? I must be missing something fundamental, but I just don't see a way to link them. PS I also know that the hypotheses given imply $S$ is everywhere positive definite or everywhere negative definite, and that $M$ is diffeomorphic to $S^d$. That is (or should be) called Hadamard's theorem. ""or should be"" because the statement was given after the proof of the theorem and along the lines of «This set of considerations are called Hadamard's theorem». At this point in the course we have not yet covered geodesics , and we never saw anything involving both geodesics and $S$. I remark this because I have seen this question . Also, principal curvature is an unknown term for me. Seeing this question makes me remember the Theorema Egregium, which we proved, and seems to be potentially useful here. However, T.E. works in $\mathbb{R}^3$, not $\mathbb{R}^{d+1}$, so it would need a generalization. Obviously, $d=2$ is solved by T.E., because if the sectional curvature (which equates to $\mathcal{R}(v_i\wedge v_j,v_i\wedge v_j)$ since the denominator is 1) is the determinant of the shape operator, then it is the product of the eigenvalues, which are precisely the $\lambda_i$.","Terminology note In the following, a scalar product will be a symmetric bilinear form, and a euclidean scalar product will be a positive definite scalar product. This is the terminology used by my Differential Geometry teacher. Background Let me sum up the operators coming in play in the question. To start right from the start, we have, in as general a context as possible, a Riemannian manifold $(M,g)$. The metric induces a single Levi-Civita connection , which I will denote $\nabla_XY$, where $X,Y$ are fields and $\nabla_XY$ is the covariant derivative of $Y$ along $X$, as it is also called. The curvature tensor is then defined as: $$R(X,Y)Z=\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z=[\nabla_X,\nabla_Y]Z-\nabla_{[X,Y]}Z.$$ This is the type $(1,3)$ curvature tensor, whereas the type $(0,4)$ tensor is: $$R(X,Y,Z,W)=g(R(X,Y)Z,W).$$ This can be proven to be antisimmetric in the first two and second two entries, and symmetric w.r.t. swapping the two couples (i.e. $R(X,Y,Z,W)=R(Z,W,X,Y)$). This means it induces a scalar product on the second exterior power, which we define as: $$\mathcal{R}(X\wedge Y,Z\wedge W)=R(X,Y,W,Z).$$ $g$ also induces a scalar product on the second exterior power: $$g'(v\wedge w,u\wedge t)=g(v,u)g(w,t)-g(v,t)g(w,u).$$ This can be shown to be positive definite -- in fact, if $\{v_j\}$ is orthonormal w.r.t $g$< $\{v_i\wedge v_j\}_{i<j}$ is orthonormal w.r.t. $g'$. Fixing this scalar product as a ""reference"", since $\mathcal{R}$ is also a scalar product, we get a self-adjoint operator, which we still denote $\mathcal{R}$, by setting: $$g'(\mathcal{R}(v\wedge w),u\wedge t)=\mathcal{R}(v\wedge w,u\wedge t).$$ Oh I'm writing all of this with decomposable vectors but this is extended by additivity to non-decomposable ones of course. The last operator we need is the shape operator. So we consider a hypersurface $N\subseteq M$. Locally, we can always find a normal unitary vector field $\mathcal{N}$ for $N$. Modulo a sign, this induces an operator, called shape operator , which is defined by: $$S(v)=\nabla_X\mathcal{N},$$ where $X$ is any field which evalues to $v$ at the tangency point $p$ of $v$ to $M$. It can be shown that this is a $g$-self-adjoint endomorphism of $T_pN$, and that $S(\mathcal{N}(p))=0$. Question So here comes, at last, the question. I have been given the following exercise. Let $M$ be a hypersurface of $\mathbb{R}^n$. Suppose $M$ is compact, connected, orientable (i.e. has a global unitary normal field). Assume $\{v_j\}$ is an orthonormal basis for the tangent space $T_pM$ consisting of eigenvalues of the shape operator $S$. Prove that $\{v_i\wedge v_j\}$ is an o.n.b. of $\Lambda^2T_pM$ composed of eigenvalues of $\mathcal{R}$, and more precisely $\mathcal{R}(v_i\wedge v_j)=\lambda_i\lambda_jv_i\wedge v_j$, where $S(v_i)=\lambda_iv_i$. Now the fact that those wedges provide an orthonormal basis is easy, since we know (by earlier proof in the course) that it is a basis, and orthonormality follows from a simple computation: $$g'(v_i\wedge v_j,v_k\wedge v_\ell)=g(v_i,v_k)g(v_j,v_\ell)-g(v_i,v_\ell)g(v_j,v_k)=\delta_{ik}\delta_{j\ell}-\delta_{i\ell}\delta_{jk},$$ we need $i<j$ and $k<\ell$ by definition of the basis, if the two are equal the first term is $1\cdot1$ and the second one $0\cdot0$, so it is a normal basis, and if they are distinct then the first term is 0, and if $i\neq k$ it could be that $i=\ell$ but the $j\neq k$ since $j>i=\ell>k$, so the second term is also zero, whereas if $j\neq\ell$ it could be $j=k$, but $i\neq\ell$ by a similar argument, giving again that both terms are zero. As for the fact those vectors are eigenvalues of $\mathcal{R}$, I am pretty much at a loss: what is the link between $\mathcal{R}$ and $S$? I must be missing something fundamental, but I just don't see a way to link them. PS I also know that the hypotheses given imply $S$ is everywhere positive definite or everywhere negative definite, and that $M$ is diffeomorphic to $S^d$. That is (or should be) called Hadamard's theorem. ""or should be"" because the statement was given after the proof of the theorem and along the lines of «This set of considerations are called Hadamard's theorem». At this point in the course we have not yet covered geodesics , and we never saw anything involving both geodesics and $S$. I remark this because I have seen this question . Also, principal curvature is an unknown term for me. Seeing this question makes me remember the Theorema Egregium, which we proved, and seems to be potentially useful here. However, T.E. works in $\mathbb{R}^3$, not $\mathbb{R}^{d+1}$, so it would need a generalization. Obviously, $d=2$ is solved by T.E., because if the sectional curvature (which equates to $\mathcal{R}(v_i\wedge v_j,v_i\wedge v_j)$ since the denominator is 1) is the determinant of the shape operator, then it is the product of the eigenvalues, which are precisely the $\lambda_i$.",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
65,"Identity in general relativity, not sure if true or not","Identity in general relativity, not sure if true or not",,"Let $(M, g_{ab})$ be a spacetime and define a new metric, $\tilde{g}_{ab}$, on $M$ by $\tilde{g}_{ab} = \Omega^2 g_{ab}$, where $\Omega$ is a smooth, positive function. Let $\nabla_a$ denote the derivative operator associated with $g_{ab}$ and let $\tilde{\nabla}_a$ denote the derivative operator associated with $\tilde{g}_{ab}$. Let $v^a$ be an arbitrary smooth vector field on $M$. Do we necessarily have the following identity:$$\tilde{\nabla}_a v^b = \nabla_a v^b = {\delta^b}_a v^c \nabla_c \text{ln}\,\Omega - v^b \nabla_a \text{ln}\,\Omega - g_{ac} v^c g^{bd} \nabla_d \text{ln}\,\Omega?$$","Let $(M, g_{ab})$ be a spacetime and define a new metric, $\tilde{g}_{ab}$, on $M$ by $\tilde{g}_{ab} = \Omega^2 g_{ab}$, where $\Omega$ is a smooth, positive function. Let $\nabla_a$ denote the derivative operator associated with $g_{ab}$ and let $\tilde{\nabla}_a$ denote the derivative operator associated with $\tilde{g}_{ab}$. Let $v^a$ be an arbitrary smooth vector field on $M$. Do we necessarily have the following identity:$$\tilde{\nabla}_a v^b = \nabla_a v^b = {\delta^b}_a v^c \nabla_c \text{ln}\,\Omega - v^b \nabla_a \text{ln}\,\Omega - g_{ac} v^c g^{bd} \nabla_d \text{ln}\,\Omega?$$",,"['differential-geometry', 'riemannian-geometry']"
66,Extending parallel 1 forms to harmonic forms on a compact set,Extending parallel 1 forms to harmonic forms on a compact set,,"Based on this question from Peterson's Riemannian Geometry: Let $(M,g)$ be an n-dimensional connected Riemannian manifold that is isometric   to Euclidean space outside some compact subset $K \subset M$, i.e., $M − K$ is   isometric to $\mathbb{R}^n − C$ for some compact set $C \subset \mathbb{R}^n$. If $\text{Ricg} \geq 0$, show that $M = \mathbb{R}^n$. There are then two hints, but they basically run into the same problem in opposite directions: the first hint leads you to conclude that there are parallel 1 forms defined on $K$ (after showing $K$ is flat, and thus the whole space is flat), but I cannot go from here to conclude that there are globally defined parallel 1 forms. The other hint tells you to extend the $n$ parallel 1 forms on $M-K$ to harmonic forms on $M$, and use Bochner's technique to conclude that these extensions are parallel. Again, I see the logic, but I am having trouble seeing how to extend the 1-forms on $M-K$ over $K$. Any ideas?","Based on this question from Peterson's Riemannian Geometry: Let $(M,g)$ be an n-dimensional connected Riemannian manifold that is isometric   to Euclidean space outside some compact subset $K \subset M$, i.e., $M − K$ is   isometric to $\mathbb{R}^n − C$ for some compact set $C \subset \mathbb{R}^n$. If $\text{Ricg} \geq 0$, show that $M = \mathbb{R}^n$. There are then two hints, but they basically run into the same problem in opposite directions: the first hint leads you to conclude that there are parallel 1 forms defined on $K$ (after showing $K$ is flat, and thus the whole space is flat), but I cannot go from here to conclude that there are globally defined parallel 1 forms. The other hint tells you to extend the $n$ parallel 1 forms on $M-K$ to harmonic forms on $M$, and use Bochner's technique to conclude that these extensions are parallel. Again, I see the logic, but I am having trouble seeing how to extend the 1-forms on $M-K$ over $K$. Any ideas?",,"['differential-geometry', 'riemannian-geometry']"
67,Seeing that the second fundamental form is the orthogonal component of the Laplacian,Seeing that the second fundamental form is the orthogonal component of the Laplacian,,"I have come across the statement a few times that, for a mapping $u:M\to N$ between a Riemannian manifold $(M,g)$ and a submanifold $N$ of Euclidean space $\mathbb{R}^n$, the part of the Laplacian of M orthogonal to the tangent plane of $N$ is given by the second fundamental form $II$ of $N$ in $\mathbb{R}^n$. $$(\Delta_gu)^\perp=g^{ij}II(u)(\partial_i u ,\partial_j u)$$ I can't find a proof of this fact or see how to demonstrate it myself. Would anyone be able to offer a proof, or a sketch of a proof?","I have come across the statement a few times that, for a mapping $u:M\to N$ between a Riemannian manifold $(M,g)$ and a submanifold $N$ of Euclidean space $\mathbb{R}^n$, the part of the Laplacian of M orthogonal to the tangent plane of $N$ is given by the second fundamental form $II$ of $N$ in $\mathbb{R}^n$. $$(\Delta_gu)^\perp=g^{ij}II(u)(\partial_i u ,\partial_j u)$$ I can't find a proof of this fact or see how to demonstrate it myself. Would anyone be able to offer a proof, or a sketch of a proof?",,['differential-geometry']
68,Derivative along a curve,Derivative along a curve,,"Suppose $M$ is a hypersurface of the sphere $S^n \subset \mathbb{R}^{n+1}$, and denote the riemannian connections of $M$, $S^n$ and $\mathbb{R}^{n+1}$ by $\nabla, \overline{\nabla}$ and $\tilde{\nabla}$, respectively. Given a differentiable curve $\alpha : (-\varepsilon, \varepsilon) \to M$ with $\alpha(0) = p$ and $\alpha'(0) = v \in T_p M$, how can I ""explicitly"" calculate $\overline{\nabla}_v \alpha'$ in terms of the well known derivatives in $\mathbb{R}^{n+1}$? Is it true that $\tilde{\nabla}_v \alpha' = \alpha''(0)$, so that $$ \overline{\nabla}_v \alpha' = \operatorname{proj}_{T_p S^n} (\alpha''(0)) = \alpha''(0) - \langle \alpha''(0), p \rangle p \,\text{ ?} $$ I am having trouble understanding covariant derivatives along curves. Any help will be appreciated.","Suppose $M$ is a hypersurface of the sphere $S^n \subset \mathbb{R}^{n+1}$, and denote the riemannian connections of $M$, $S^n$ and $\mathbb{R}^{n+1}$ by $\nabla, \overline{\nabla}$ and $\tilde{\nabla}$, respectively. Given a differentiable curve $\alpha : (-\varepsilon, \varepsilon) \to M$ with $\alpha(0) = p$ and $\alpha'(0) = v \in T_p M$, how can I ""explicitly"" calculate $\overline{\nabla}_v \alpha'$ in terms of the well known derivatives in $\mathbb{R}^{n+1}$? Is it true that $\tilde{\nabla}_v \alpha' = \alpha''(0)$, so that $$ \overline{\nabla}_v \alpha' = \operatorname{proj}_{T_p S^n} (\alpha''(0)) = \alpha''(0) - \langle \alpha''(0), p \rangle p \,\text{ ?} $$ I am having trouble understanding covariant derivatives along curves. Any help will be appreciated.",,"['differential-geometry', 'riemannian-geometry', 'connections']"
69,surface lying on one side of another surface,surface lying on one side of another surface,,"I was studying the strong maximum principle for minimal surfaces and came across the statement that surface A lies on one side of the surface B. Can you please tell me what does it mean mathematically? The Theorem Statement is: "" If $\Sigma_1 ,\Sigma_2 \subset \mathbb{R}^n $ are complete connected minimal hypersurfaces (without boundaries), $\Sigma_1 \cap \Sigma_2 \neq \emptyset,  $ and $\Sigma_2$ lies on one side of $\Sigma_1$, then $\Sigma_1 = \Sigma_2$.","I was studying the strong maximum principle for minimal surfaces and came across the statement that surface A lies on one side of the surface B. Can you please tell me what does it mean mathematically? The Theorem Statement is: "" If $\Sigma_1 ,\Sigma_2 \subset \mathbb{R}^n $ are complete connected minimal hypersurfaces (without boundaries), $\Sigma_1 \cap \Sigma_2 \neq \emptyset,  $ and $\Sigma_2$ lies on one side of $\Sigma_1$, then $\Sigma_1 = \Sigma_2$.",,['differential-geometry']
70,Smooth function on a manifold not dependent on coordinate chart,Smooth function on a manifold not dependent on coordinate chart,,I'm having trouble with the proof of the following remark from page 59 of Tu's book on Manifolds. The part I'm worried about is where he gets that $\phi\circ \psi^{-1}$ is $C^\infty$. Is he allowed to make that assumption? I thought we could only say this if the two charts were $C^\infty$-compatible. Other relevant definitions from his book:,I'm having trouble with the proof of the following remark from page 59 of Tu's book on Manifolds. The part I'm worried about is where he gets that $\phi\circ \psi^{-1}$ is $C^\infty$. Is he allowed to make that assumption? I thought we could only say this if the two charts were $C^\infty$-compatible. Other relevant definitions from his book:,,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
71,What is the structure group of Hopf map $\pi ：S^{15}\rightarrow S^8$?,What is the structure group of Hopf map ?,\pi ：S^{15}\rightarrow S^8,"What is the structure group of Hopf map $\pi ：S^{15}\rightarrow S^8$? I know that $\pi ：S^{15}\rightarrow S^8$ is a fiber bundle with fiber $S^7$. However $S^7$ cannot be a Lie group. But every fiber bundle has the structure group, then what is the structure group of  $\pi ：S^{15}\rightarrow S^8$?","What is the structure group of Hopf map $\pi ：S^{15}\rightarrow S^8$? I know that $\pi ：S^{15}\rightarrow S^8$ is a fiber bundle with fiber $S^7$. However $S^7$ cannot be a Lie group. But every fiber bundle has the structure group, then what is the structure group of  $\pi ：S^{15}\rightarrow S^8$?",,"['differential-geometry', 'fiber-bundles']"
72,Intuitive understanding into the mean curvature flow,Intuitive understanding into the mean curvature flow,,"I am trying to develop some intuition into the properties of mean curvature flow of a surface in $\mathbb{R}^3$. As an example, I am trying to understand what happens to a surface of revolution  $$S = (f(u), g(u)\text{cos }v, g(u)\text{sin }v),$$  $u \in [0, 1]$, $v \in [0, 2\pi]$. For example, what happens if $S$ is positively curved at all interior points? Or if $S$ is negatively curved at all interior points? Will it remain embedded for short time? Long time? How will it look in long time? I am trying to get some heuristic ideas. Thanks! Note: Re-asked from MathOverflow.","I am trying to develop some intuition into the properties of mean curvature flow of a surface in $\mathbb{R}^3$. As an example, I am trying to understand what happens to a surface of revolution  $$S = (f(u), g(u)\text{cos }v, g(u)\text{sin }v),$$  $u \in [0, 1]$, $v \in [0, 2\pi]$. For example, what happens if $S$ is positively curved at all interior points? Or if $S$ is negatively curved at all interior points? Will it remain embedded for short time? Long time? How will it look in long time? I am trying to get some heuristic ideas. Thanks! Note: Re-asked from MathOverflow.",,"['differential-geometry', 'reference-request', 'riemannian-geometry', 'mean-curvature-flows']"
73,Unit Normal vs Principal Normal,Unit Normal vs Principal Normal,,"Here is the problem I am working on: Deduce the equation of the main normal and binormal to the curve: $x=t, y=t^2, z=t^3,   t=1.$ I remember from Calc-3 that the binormal is unit tangent $\times$ unit normal, and that unit normal is tangent prime /magnitude of tangent prime. However, my text book has the binormal as unit tangent $\times$ principle normal, with principal normal listed as a very long formula. Is unit normal different from principal normal? I have worked my way through the unit tangent but am not sure about the normal.","Here is the problem I am working on: Deduce the equation of the main normal and binormal to the curve: $x=t, y=t^2, z=t^3,   t=1.$ I remember from Calc-3 that the binormal is unit tangent $\times$ unit normal, and that unit normal is tangent prime /magnitude of tangent prime. However, my text book has the binormal as unit tangent $\times$ principle normal, with principal normal listed as a very long formula. Is unit normal different from principal normal? I have worked my way through the unit tangent but am not sure about the normal.",,['differential-geometry']
74,Distances in geodesic triangles,Distances in geodesic triangles,,"Let $a, b \in \mathbb{R}^2$ be two points in the plane and let $\Pi$ be their perpendicular bisector (see left figure). Let $c \in \Pi$ be any point and consider the triangle $\triangle abc$. Suppose that there is another point $p \in \operatorname{Int}{\triangle abc}$. It's easy to see that $d(c, p) < d(c, a) = d(c, b)$. Is the same true of a geodesic triangle on a surface $\Sigma \subset \mathbb{R}^3$? In this case we consider the intrinsic metric $d_{\Sigma}$ defined by shortest paths on the surface $\Sigma$. We then define the 'perpendicular bisector' as the set of all points equidistant to $a, b \in \Sigma$ with respect to the metric $d_{\Sigma}$. (See right figure.) Edit: I don't think this is true since the height of the surface could increase rapidly within the interior of the triangle. A side view, looking down in the direction of $\vec{ab}$, of what I'm talking about might looking like the following figure. In which case, can I prove the result if I assume a bound on the principal curvatures in the interior of the triangle?","Let $a, b \in \mathbb{R}^2$ be two points in the plane and let $\Pi$ be their perpendicular bisector (see left figure). Let $c \in \Pi$ be any point and consider the triangle $\triangle abc$. Suppose that there is another point $p \in \operatorname{Int}{\triangle abc}$. It's easy to see that $d(c, p) < d(c, a) = d(c, b)$. Is the same true of a geodesic triangle on a surface $\Sigma \subset \mathbb{R}^3$? In this case we consider the intrinsic metric $d_{\Sigma}$ defined by shortest paths on the surface $\Sigma$. We then define the 'perpendicular bisector' as the set of all points equidistant to $a, b \in \Sigma$ with respect to the metric $d_{\Sigma}$. (See right figure.) Edit: I don't think this is true since the height of the surface could increase rapidly within the interior of the triangle. A side view, looking down in the direction of $\vec{ab}$, of what I'm talking about might looking like the following figure. In which case, can I prove the result if I assume a bound on the principal curvatures in the interior of the triangle?",,"['differential-geometry', 'differential-topology', 'riemannian-geometry', 'geodesic']"
75,Negative divergence implies convergent flow?,Negative divergence implies convergent flow?,,"Suppose we have a differentiable vector field $X:\Omega\to\mathbb{R^n}$ defined on an open, bounded and simply connected region subset $\Omega$ of $\mathbb{R^n}$, and its divergence is negative everywhere, i.e. $\nabla\cdot X(x)<0$ for any $x\in\Omega$. Can we prove that any two solution trajectories will evolve closer and closer, and thus eventually convergent? Formally, given any two points $p,q\in\Omega$ and flows $p(t), q(t)$ satisfying $p(0)=p,q(0)=q,\dot p(t)=X(p(t)), \dot q(t)=X(q(t))$, define $f(t)=||p(t)-q(t)||^2$, then is it true that $$\frac{d}{dt}f(t)<0$$ Intuitively, I think it is right because negative divergence implies any closed area will evolve smaller and smaller. So if we enclose two points with a thin tube, then the volume of this tube will get smaller and smaller, which forces points get closer. Here is my try: $$\frac{d}{dt}f(t)=2\left<p-q|X(p)-X(q)\right>=2\left<p-q|\nabla X(r)|p-q\right>$$ The first equation follows from the definition and the second from mean value theorem(though this theorem doesn't exist). But this seems the statement requires more rigid condition, say positiveness of $\nabla X$, to guarantee the corectness.","Suppose we have a differentiable vector field $X:\Omega\to\mathbb{R^n}$ defined on an open, bounded and simply connected region subset $\Omega$ of $\mathbb{R^n}$, and its divergence is negative everywhere, i.e. $\nabla\cdot X(x)<0$ for any $x\in\Omega$. Can we prove that any two solution trajectories will evolve closer and closer, and thus eventually convergent? Formally, given any two points $p,q\in\Omega$ and flows $p(t), q(t)$ satisfying $p(0)=p,q(0)=q,\dot p(t)=X(p(t)), \dot q(t)=X(q(t))$, define $f(t)=||p(t)-q(t)||^2$, then is it true that $$\frac{d}{dt}f(t)<0$$ Intuitively, I think it is right because negative divergence implies any closed area will evolve smaller and smaller. So if we enclose two points with a thin tube, then the volume of this tube will get smaller and smaller, which forces points get closer. Here is my try: $$\frac{d}{dt}f(t)=2\left<p-q|X(p)-X(q)\right>=2\left<p-q|\nabla X(r)|p-q\right>$$ The first equation follows from the definition and the second from mean value theorem(though this theorem doesn't exist). But this seems the statement requires more rigid condition, say positiveness of $\nabla X$, to guarantee the corectness.",,"['differential-geometry', 'dynamical-systems', 'vector-fields', 'fluid-dynamics']"
76,Hopf Invariant of $f$,Hopf Invariant of,f,"To set up notation: Let $f:S^3\to S^2$. For a volume form $\omega$ on $S^2$, $f^*\omega$ is a closed two form on $S^3$, which can be written as $d\alpha$ for some 1 form $\alpha$. The number $$\int_{S^3}\alpha\wedge d\alpha$$ is called the Hopf invariant of $f$. My goal is to show that the result is independent of our choice of $\alpha$. Here are my thoughts: If $\beta$ satisfies $d\beta=f^*\omega$, then we should have $\alpha=\beta+\xi$, where $\xi$ is a closed form on $S^3$. This means that $\alpha\wedge d\alpha=(\beta+\xi)\wedge d\beta=\beta\wedge d\beta +\xi\wedge d\beta$. So it seems like we must show  $$ \int_{S^3}\xi\wedge d\beta=0. $$  This is where I am stuck. I feel like I am either way off base, or need to apply Stokes Theorem somehow, but I am not sure how I should proceed.","To set up notation: Let $f:S^3\to S^2$. For a volume form $\omega$ on $S^2$, $f^*\omega$ is a closed two form on $S^3$, which can be written as $d\alpha$ for some 1 form $\alpha$. The number $$\int_{S^3}\alpha\wedge d\alpha$$ is called the Hopf invariant of $f$. My goal is to show that the result is independent of our choice of $\alpha$. Here are my thoughts: If $\beta$ satisfies $d\beta=f^*\omega$, then we should have $\alpha=\beta+\xi$, where $\xi$ is a closed form on $S^3$. This means that $\alpha\wedge d\alpha=(\beta+\xi)\wedge d\beta=\beta\wedge d\beta +\xi\wedge d\beta$. So it seems like we must show  $$ \int_{S^3}\xi\wedge d\beta=0. $$  This is where I am stuck. I feel like I am either way off base, or need to apply Stokes Theorem somehow, but I am not sure how I should proceed.",,['differential-geometry']
77,"Inclusion, pullback of differential form","Inclusion, pullback of differential form",,"Let $\omega=x\,dy\wedge dz +y\,dz\wedge dx+z\,dx\wedge dy$ or in spherical coordinates (unless I had made some mistake) $\omega=r^3\cos \theta\, d\phi\wedge d\theta$. Now I want to find $i^*\omega$ where $i:S\to\mathbb{R}^3$ is inclusion of unit sphere, using $\phi$ and $\theta$. It seems quite easy but I'm not sure how to interprete $i$ and how  to use it in $i^* \omega$. Any suggestions?","Let $\omega=x\,dy\wedge dz +y\,dz\wedge dx+z\,dx\wedge dy$ or in spherical coordinates (unless I had made some mistake) $\omega=r^3\cos \theta\, d\phi\wedge d\theta$. Now I want to find $i^*\omega$ where $i:S\to\mathbb{R}^3$ is inclusion of unit sphere, using $\phi$ and $\theta$. It seems quite easy but I'm not sure how to interprete $i$ and how  to use it in $i^* \omega$. Any suggestions?",,"['differential-geometry', 'differential-forms', 'spherical-coordinates']"
78,Finding dual forms of a frame field on a sphere,Finding dual forms of a frame field on a sphere,,"I'm attempting to calculate the Gaussian curvature of the sphere of radius $r$, but I'm not sure how to find the dual forms of the frame field. I start with the parametrization $X(\phi, \theta) = (r \sin \phi \cos \theta, r \sin \phi \sin \theta, r \cos \phi)$. Then this gives me the tangent frame field $E_1 = r \cos \phi \cos \theta \frac{\partial}{\partial x} + r \cos \phi \sin \theta \frac{\partial}{\partial y} - r \sin \phi \frac{\partial}{\partial z}$ $E_2 = -r \sin \phi \sin \theta \frac{\partial}{\partial x} + r \sin \phi \cos \theta \frac{\partial}{\partial y}$ I also compute $dx = r \cos \phi \cos \theta d\phi - r \sin \phi \sin \theta d\theta$ $dy = r \cos \phi \sin \theta d\phi + r \sin \phi \cos \theta d\theta$ $dz = -r \sin \phi\ d\phi$ Now we must find the dual forms of the frame field ${E_1, E_2}$, but how do we do that? I thought we would just substitute $\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}$ with $dx, dy, dz$ in their expressions, but this doesn't get me the right answer. I have searched for an explanation of this but couldn't find anything clear. Any help is appreciated.","I'm attempting to calculate the Gaussian curvature of the sphere of radius $r$, but I'm not sure how to find the dual forms of the frame field. I start with the parametrization $X(\phi, \theta) = (r \sin \phi \cos \theta, r \sin \phi \sin \theta, r \cos \phi)$. Then this gives me the tangent frame field $E_1 = r \cos \phi \cos \theta \frac{\partial}{\partial x} + r \cos \phi \sin \theta \frac{\partial}{\partial y} - r \sin \phi \frac{\partial}{\partial z}$ $E_2 = -r \sin \phi \sin \theta \frac{\partial}{\partial x} + r \sin \phi \cos \theta \frac{\partial}{\partial y}$ I also compute $dx = r \cos \phi \cos \theta d\phi - r \sin \phi \sin \theta d\theta$ $dy = r \cos \phi \sin \theta d\phi + r \sin \phi \cos \theta d\theta$ $dz = -r \sin \phi\ d\phi$ Now we must find the dual forms of the frame field ${E_1, E_2}$, but how do we do that? I thought we would just substitute $\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}$ with $dx, dy, dz$ in their expressions, but this doesn't get me the right answer. I have searched for an explanation of this but couldn't find anything clear. Any help is appreciated.",,"['differential-geometry', 'curvature']"
79,proving a particular subset of a Riemannian manifold is closed using continuity,proving a particular subset of a Riemannian manifold is closed using continuity,,"I have two Riemannian manifolds, $(M,g)$ and $(\widetilde{M},\widetilde{g})$ and two maps $\varphi, \psi : M \to \widetilde{M}$, which are both local isometries.  I am trying to show that the set $A = \{ p \in M \mid d \varphi_p = d \psi_p \}$ is closed in $M$ (where $d \varphi_p$ is the differential at the point $p$).  I know that I should be able to use continuity of the global differentials $d \varphi$ and $d \psi$ to prove this, but I keep running into problems when I try to write $A$ as the preimage of a closed set.  For example, I tried writing $A = \pi((d \varphi - d \psi)^{-1}(\{ 0_{\widetilde{q}} \mid \widetilde{q} \in \widetilde{M} \}))$, but this will give me that $A = M$ even if I don't have that $d \varphi_p = d \psi_p$ for all $p \in M$.  I'm sure there is something obvious that for whatever reason I'm just not seeing...  what is it?","I have two Riemannian manifolds, $(M,g)$ and $(\widetilde{M},\widetilde{g})$ and two maps $\varphi, \psi : M \to \widetilde{M}$, which are both local isometries.  I am trying to show that the set $A = \{ p \in M \mid d \varphi_p = d \psi_p \}$ is closed in $M$ (where $d \varphi_p$ is the differential at the point $p$).  I know that I should be able to use continuity of the global differentials $d \varphi$ and $d \psi$ to prove this, but I keep running into problems when I try to write $A$ as the preimage of a closed set.  For example, I tried writing $A = \pi((d \varphi - d \psi)^{-1}(\{ 0_{\widetilde{q}} \mid \widetilde{q} \in \widetilde{M} \}))$, but this will give me that $A = M$ even if I don't have that $d \varphi_p = d \psi_p$ for all $p \in M$.  I'm sure there is something obvious that for whatever reason I'm just not seeing...  what is it?",,"['differential-geometry', 'differential-topology', 'riemannian-geometry']"
80,"Uniformly convex set, lower level sets","Uniformly convex set, lower level sets",,"Let $f:\mathbb{R}^n \to \mathbb{R}$ be a smooth convex function. Then for any constant $c>0$, $f + c||x||^2$ is strongly convex. How can I show that the set $\{x \in \mathbb{R}^n : f(x) + c||x||^2 \leq 0\}$, the lower level set of $0$ is uniformly convex. By uniformly convex, I mean that the boundary has principal curvatures have a positive uniform lower bound. I am trying to understand the approximation of a convex domain by increasing uniformly convex smooth domains, as posed in another question .","Let $f:\mathbb{R}^n \to \mathbb{R}$ be a smooth convex function. Then for any constant $c>0$, $f + c||x||^2$ is strongly convex. How can I show that the set $\{x \in \mathbb{R}^n : f(x) + c||x||^2 \leq 0\}$, the lower level set of $0$ is uniformly convex. By uniformly convex, I mean that the boundary has principal curvatures have a positive uniform lower bound. I am trying to understand the approximation of a convex domain by increasing uniformly convex smooth domains, as posed in another question .",,"['real-analysis', 'differential-geometry', 'convex-analysis']"
81,basic definition of vector-fields on a manifold,basic definition of vector-fields on a manifold,,"Many textbooks introduce vector-fields on a manifold $M$ along the lines of $ X = X_i \frac{\partial}{\partial x_i} $ where -without further ado- $\frac{\partial}{\partial x_i}$ is introduced as a basis vector in Tangent space. Given that the same symbol is widely used as a differential operator in mathematics, should I think about $X$ in terms of an operator rather than a vector?  Or is that notation a clever suggestive trick to help ease calculations - similar to physicists speaking of ""ket-"" $ | \psi>$ and ""bra-"" $ < \Phi | $ vectors in a physical Hilbert space that ""magically"" turn into a scalar-product $ <\Phi|\Psi> $ when ""meeting"" in a calculation? Similar conceptual difficulties arise for me when in the the definition of 1-forms $\omega = \omega_1 dx $ the object $dx$ is introduced as a ""unit"" vector when I am used to thinking about $dx$ as an infinitesimal quantity. Cheers!","Many textbooks introduce vector-fields on a manifold $M$ along the lines of $ X = X_i \frac{\partial}{\partial x_i} $ where -without further ado- $\frac{\partial}{\partial x_i}$ is introduced as a basis vector in Tangent space. Given that the same symbol is widely used as a differential operator in mathematics, should I think about $X$ in terms of an operator rather than a vector?  Or is that notation a clever suggestive trick to help ease calculations - similar to physicists speaking of ""ket-"" $ | \psi>$ and ""bra-"" $ < \Phi | $ vectors in a physical Hilbert space that ""magically"" turn into a scalar-product $ <\Phi|\Psi> $ when ""meeting"" in a calculation? Similar conceptual difficulties arise for me when in the the definition of 1-forms $\omega = \omega_1 dx $ the object $dx$ is introduced as a ""unit"" vector when I am used to thinking about $dx$ as an infinitesimal quantity. Cheers!",,"['differential-geometry', 'vector-fields']"
82,Why is this not an inconsistency in elementary Lie theory?,Why is this not an inconsistency in elementary Lie theory?,,"I made an observation last week, and it has bothered me ever since. Recall the formulae $$\exp([X,Y])=\lim_{n\rightarrow\infty}\left(\exp\left(\frac{1}{n}X\right)\exp\left(\frac{1}{n}Y\right)\exp\left(\frac{-1}{n}X\right)\exp\left(\frac{-1}{n}Y\right)\right)^{n^2}$$ and $$[X,Y]_p(f)=\lim_{t\rightarrow 0}\frac{1}{t^2}\Bigg(f\Big(\!\exp(-tY)\exp(-tX)\exp(tY)\exp(tX)p\Big)-f(p)\Bigg).$$ The observation is this: the first formula ""moves clockwise"", while the second formula ""moves counterclockwise""! This led me back to definitions. Suppose we have a Lie group $G$ and we want to define the Lie algebra of $G$. We can define the Lie algebra using the adjoint representation, with $X_e,Y_e\in T_eG$ and $$[X_e,Y_e]=\mathrm{ad}_{X_e}(Y_e)=\lim_{t\rightarrow 0}\frac{1}{t}(L_{\exp(tX_e)*}R_{\exp(-tX_e)*}Y_e-Y_e),$$ or with left-invariant vector fields $X$ and $Y$ with $$[X,Y]_e=\mathcal{L}_X(Y)_e=\lim_{t\rightarrow 0}(\exp(-tX)_*Y_{\exp(tX)e}-Y_e).$$ I believe that I am much less proficient in the latter than the former, so perhaps this stems from lack of practice, but it looks like these are not the same Lie algebra, in the sense that the identity map on $T_eG$ is not a Lie algebra isomorphism. In fact, it looks like the identity map would be an anti -isomorphism, so that $[X_e,Y_e]=-[X,Y]_e$. Of course, anti-isomorphic Lie algebras are isomorphic, but they are not ""the same"" in the sense above. Today, I tested this on the matrix Lie group $\left\{\begin{bmatrix}a & b \\ 0 & 1\end{bmatrix}\!:a,b\in\mathbb{R}\text{ and } a>0\right\}$, but they came out to the same algebra. Why do these two definitions ""move in opposite directions"" in the sense above, and how do they determine the same Lie algebra? I am looking for a detailed explanation, preferably including whether or not these two definitions are anti-isomorphic relationships between the one-parameter subgroups $\exp(tX)$ and the flow curves $\exp(tX)e$ a little bit of geometric intuition","I made an observation last week, and it has bothered me ever since. Recall the formulae $$\exp([X,Y])=\lim_{n\rightarrow\infty}\left(\exp\left(\frac{1}{n}X\right)\exp\left(\frac{1}{n}Y\right)\exp\left(\frac{-1}{n}X\right)\exp\left(\frac{-1}{n}Y\right)\right)^{n^2}$$ and $$[X,Y]_p(f)=\lim_{t\rightarrow 0}\frac{1}{t^2}\Bigg(f\Big(\!\exp(-tY)\exp(-tX)\exp(tY)\exp(tX)p\Big)-f(p)\Bigg).$$ The observation is this: the first formula ""moves clockwise"", while the second formula ""moves counterclockwise""! This led me back to definitions. Suppose we have a Lie group $G$ and we want to define the Lie algebra of $G$. We can define the Lie algebra using the adjoint representation, with $X_e,Y_e\in T_eG$ and $$[X_e,Y_e]=\mathrm{ad}_{X_e}(Y_e)=\lim_{t\rightarrow 0}\frac{1}{t}(L_{\exp(tX_e)*}R_{\exp(-tX_e)*}Y_e-Y_e),$$ or with left-invariant vector fields $X$ and $Y$ with $$[X,Y]_e=\mathcal{L}_X(Y)_e=\lim_{t\rightarrow 0}(\exp(-tX)_*Y_{\exp(tX)e}-Y_e).$$ I believe that I am much less proficient in the latter than the former, so perhaps this stems from lack of practice, but it looks like these are not the same Lie algebra, in the sense that the identity map on $T_eG$ is not a Lie algebra isomorphism. In fact, it looks like the identity map would be an anti -isomorphism, so that $[X_e,Y_e]=-[X,Y]_e$. Of course, anti-isomorphic Lie algebras are isomorphic, but they are not ""the same"" in the sense above. Today, I tested this on the matrix Lie group $\left\{\begin{bmatrix}a & b \\ 0 & 1\end{bmatrix}\!:a,b\in\mathbb{R}\text{ and } a>0\right\}$, but they came out to the same algebra. Why do these two definitions ""move in opposite directions"" in the sense above, and how do they determine the same Lie algebra? I am looking for a detailed explanation, preferably including whether or not these two definitions are anti-isomorphic relationships between the one-parameter subgroups $\exp(tX)$ and the flow curves $\exp(tX)e$ a little bit of geometric intuition",,"['differential-geometry', 'lie-groups', 'lie-algebras']"
83,Pontryagin class of a wedge product of vector bundles.,Pontryagin class of a wedge product of vector bundles.,,"Let $E\to M$ be a real vector bundle over a differentiable manifold $M$ and let $p_{1}(E)$ denote its first Pontryagin class. I would like to know if there is any formula allowing to write $p_{1}(\Lambda^2 E)$ in terms of $p_{1}(E)$. I am mostly interested in the case where the dimension of $M$ is less or equal than seven and $E=TM$ is the tangent bundle of the manifold. In particular, $p_{1}(TM)\in H^{4}(M,\mathbb{Z})$, and for the case that I am interested in, $H^{4}(M,\mathbb{Z}) = \mathbb{Z}_{10}$ and thus $p_{1}(TM) = k\, u$, where $k=0,\dots,9,$ and $u$ is the generator of $\mathbb{Z}_{10}$. Thanks.","Let $E\to M$ be a real vector bundle over a differentiable manifold $M$ and let $p_{1}(E)$ denote its first Pontryagin class. I would like to know if there is any formula allowing to write $p_{1}(\Lambda^2 E)$ in terms of $p_{1}(E)$. I am mostly interested in the case where the dimension of $M$ is less or equal than seven and $E=TM$ is the tangent bundle of the manifold. In particular, $p_{1}(TM)\in H^{4}(M,\mathbb{Z})$, and for the case that I am interested in, $H^{4}(M,\mathbb{Z}) = \mathbb{Z}_{10}$ and thus $p_{1}(TM) = k\, u$, where $k=0,\dots,9,$ and $u$ is the generator of $\mathbb{Z}_{10}$. Thanks.",,"['differential-geometry', 'algebraic-topology', 'fiber-bundles', 'characteristic-classes']"
84,Classical differential operators with complex functions on Riemannian manifolds,Classical differential operators with complex functions on Riemannian manifolds,,"I am having some trouble understanding how to use the classical operators ($\nabla, \operatorname{div}, \Delta$) with complex functions on a Riemannian manifold $(M, g)$. Consider the formula defining the gradient: $g(\nabla f,X) = \Bbb d f (X)$. If $f$ takes only real values everything is fine. If $f$ takes general complex values, then $\nabla f$ will be a vector field with complex components, and since $g$ is a section in a real vector bundle, what meaning does the above formula have? I thought about complexifying the tangent bundle (by tensorizing with $\Bbb C$), but what should I do with $g$? If I promote it to a Hermitian form $g_{\Bbb C} (U + \Bbb i V, X + \Bbb i Y) = g(U,V) + g(V,Y) + \Bbb i \big( g(V,X) - g(U,Y) \big)$ then the right-hand side of the definition of the gradient is $\Bbb C$-linear in both $f$ and $X$, while the right-hand side is sesqui-linear, so this would be a mistake. Is $\nabla$ conjugate-linear? This would shock me. If, instead, I promote $g$ to a $\Bbb C$-bilinear form $g_{\Bbb C}$ then the definition above is non-contradictory, but if $X$ is a real vector field then $g_{\Bbb C} (\Bbb i X, \Bbb i X) \le 0$, which is a catastrophy. Therefore, when I read things like $\int f \Delta h \Bbb \; d x = - \int g (\nabla f, \nabla h) \; \Bbb d x$, is this valid only for real-valued functions? Is the defintion of $\nabla$ changed for complex-valued functions? Who is $g$ in this formula?","I am having some trouble understanding how to use the classical operators ($\nabla, \operatorname{div}, \Delta$) with complex functions on a Riemannian manifold $(M, g)$. Consider the formula defining the gradient: $g(\nabla f,X) = \Bbb d f (X)$. If $f$ takes only real values everything is fine. If $f$ takes general complex values, then $\nabla f$ will be a vector field with complex components, and since $g$ is a section in a real vector bundle, what meaning does the above formula have? I thought about complexifying the tangent bundle (by tensorizing with $\Bbb C$), but what should I do with $g$? If I promote it to a Hermitian form $g_{\Bbb C} (U + \Bbb i V, X + \Bbb i Y) = g(U,V) + g(V,Y) + \Bbb i \big( g(V,X) - g(U,Y) \big)$ then the right-hand side of the definition of the gradient is $\Bbb C$-linear in both $f$ and $X$, while the right-hand side is sesqui-linear, so this would be a mistake. Is $\nabla$ conjugate-linear? This would shock me. If, instead, I promote $g$ to a $\Bbb C$-bilinear form $g_{\Bbb C}$ then the definition above is non-contradictory, but if $X$ is a real vector field then $g_{\Bbb C} (\Bbb i X, \Bbb i X) \le 0$, which is a catastrophy. Therefore, when I read things like $\int f \Delta h \Bbb \; d x = - \int g (\nabla f, \nabla h) \; \Bbb d x$, is this valid only for real-valued functions? Is the defintion of $\nabla$ changed for complex-valued functions? Who is $g$ in this formula?",,"['differential-geometry', 'riemannian-geometry', 'fiber-bundles', 'laplacian']"
85,Can one build a homology theory using submanifolds and their boundaries?,Can one build a homology theory using submanifolds and their boundaries?,,"Consider a manifold $M$, and denote by $\Delta _p M$ the set of all submanifolds of dimension $p$ (with or without boundary) of $M$. Define $G_pM$ to be the free abelian group generated by $\Delta_p M$, and define $\partial : G _p M \rightarrow G_{p-1} M$ to be the linear extension of the boundary ""operator"" on manifolds: which takes a manifold $M$ and gives back the boundary $\partial M$. Does this give rise to an homology theory? If so, is it interesting in any way?","Consider a manifold $M$, and denote by $\Delta _p M$ the set of all submanifolds of dimension $p$ (with or without boundary) of $M$. Define $G_pM$ to be the free abelian group generated by $\Delta_p M$, and define $\partial : G _p M \rightarrow G_{p-1} M$ to be the linear extension of the boundary ""operator"" on manifolds: which takes a manifold $M$ and gives back the boundary $\partial M$. Does this give rise to an homology theory? If so, is it interesting in any way?",,"['differential-geometry', 'algebraic-topology', 'manifolds', 'homology-cohomology']"
86,Fiber bundles with category morphisms as fibers,Fiber bundles with category morphisms as fibers,,"Given a total space $E = M \times F$ of a fiber bundle where $M$ is a smooth manifold and $F$ is the fiber. The fiber $F_x$ corresponding to the point $x \in M$ is the set of morphisms between objects $\Sigma_x := \{\sigma_x \}$ at the point $x \in M$ (that vary from base manifold point to base manifold point). Any section of the fiber bundle can be characterized as $s(E) = (x,\sigma_x \mapsto \sigma'_x)$ with $\sigma_x,\sigma'_x \in \Sigma_x$. Due to the inhomogenity of the fibers (i.e. the cardinality and the structure of the set $\Sigma_x$ and therefore morphisms between this set is not independent on $x \in M$) no suitable connection can be defined; the fiber is assumed to be non-differentiable. However, I can define transfer functions $\Lambda(x,y)$ with $\Lambda(x,y)(\sigma_y \mapsto \sigma'_y) = (\sigma_x \mapsto \sigma'_x)$ and $x,y, \in M$ which is able to compare different fibers with others. Is there a way to handle with fiber bundles like this? What I can do if there cannot exist a smooth connection (e.g. Levi-Civita affine connection) on a fiber bundle?","Given a total space $E = M \times F$ of a fiber bundle where $M$ is a smooth manifold and $F$ is the fiber. The fiber $F_x$ corresponding to the point $x \in M$ is the set of morphisms between objects $\Sigma_x := \{\sigma_x \}$ at the point $x \in M$ (that vary from base manifold point to base manifold point). Any section of the fiber bundle can be characterized as $s(E) = (x,\sigma_x \mapsto \sigma'_x)$ with $\sigma_x,\sigma'_x \in \Sigma_x$. Due to the inhomogenity of the fibers (i.e. the cardinality and the structure of the set $\Sigma_x$ and therefore morphisms between this set is not independent on $x \in M$) no suitable connection can be defined; the fiber is assumed to be non-differentiable. However, I can define transfer functions $\Lambda(x,y)$ with $\Lambda(x,y)(\sigma_y \mapsto \sigma'_y) = (\sigma_x \mapsto \sigma'_x)$ and $x,y, \in M$ which is able to compare different fibers with others. Is there a way to handle with fiber bundles like this? What I can do if there cannot exist a smooth connection (e.g. Levi-Civita affine connection) on a fiber bundle?",,"['differential-geometry', 'category-theory', 'fiber-bundles']"
87,Topological information from metric tensor,Topological information from metric tensor,,"Suppose I am working with a Riemannian manifold $(M,g)$, and I have a particular coordinate expression for the metric $g$. What topological information can I infer about the manifold $M$? For example ($S^3$ with Hopf coordinates ), suppose I have coordinates $(\eta, \xi_1 , \xi_2 ) $ in which the metric takes the form: $ds^2 = d \eta^2 + \sin^2(\eta) d \xi_1^2 + \cos^2(\eta) d\xi_2^2$ for $0 < \eta < \pi/2$, and $0 < \xi_1 , \xi_2 < 2 \pi$. If I didn't already know this was a metric for $S^3$, how could I work that out? How do I know this isn't a metric for another three manifold, say $S^2 \times S^1$? Are there topological invariants I can compute from the metric to distinguish between, say, these two possibilities? EDIT: Changed the inequalities to be strict so that the metric in my example doesn't degenerate.","Suppose I am working with a Riemannian manifold $(M,g)$, and I have a particular coordinate expression for the metric $g$. What topological information can I infer about the manifold $M$? For example ($S^3$ with Hopf coordinates ), suppose I have coordinates $(\eta, \xi_1 , \xi_2 ) $ in which the metric takes the form: $ds^2 = d \eta^2 + \sin^2(\eta) d \xi_1^2 + \cos^2(\eta) d\xi_2^2$ for $0 < \eta < \pi/2$, and $0 < \xi_1 , \xi_2 < 2 \pi$. If I didn't already know this was a metric for $S^3$, how could I work that out? How do I know this isn't a metric for another three manifold, say $S^2 \times S^1$? Are there topological invariants I can compute from the metric to distinguish between, say, these two possibilities? EDIT: Changed the inequalities to be strict so that the metric in my example doesn't degenerate.",,"['differential-geometry', 'geometric-topology']"
88,What does the vertical bar mean in $ \left.\frac{\partial f}{\partial x}\right\rvert $,What does the vertical bar mean in, \left.\frac{\partial f}{\partial x}\right\rvert ,I want to know what the symbol '|' besides a function means. For example: $$ \left.\frac{\partial f}{\partial x}\right\rvert $$,I want to know what the symbol '|' besides a function means. For example: $$ \left.\frac{\partial f}{\partial x}\right\rvert $$,,"['calculus', 'differential-geometry', 'notation']"
89,"Calculate the length of $\gamma(t)=(t,t), t \in [-1,-\frac{1}{2}]$ with the metric $g=\frac{dx^2+dy^2}{y^2}$ and compare with euclidean metric",Calculate the length of  with the metric  and compare with euclidean metric,"\gamma(t)=(t,t), t \in [-1,-\frac{1}{2}] g=\frac{dx^2+dy^2}{y^2}","Consider the metric $g=\frac{dx^2+dy^2}{y^2}$ on $\mathbb{R}_+^2=\{(x,y) \in \mathbb{R}^2 : y>0\}$ . Calculate the length of the curve $\gamma(t)=(t,t), t \in [-1,-\frac{1}{2}]$ and compare with the length of the same curve with the euclidean metric on $\mathbb{R}^2$ . I know that, in theory, the length of the curve is given by $$\int_{-1}^{-\frac{1}{2}} \sqrt{g(\gamma',\gamma')}dt$$ where $\gamma'(t)=D_{\gamma_t}(\frac{d}{dt})$ . In practise I don't know how to proceed. Also, I presume that the length of the given curve with respect to the euclidean metric is $$\int_{-1}^{-\frac{1}{2}}\|\gamma'(t)\|dt = \frac{\sqrt2}{2}.$$ Am I making some confusion here? Some help would be appreciated. Thanks!","Consider the metric on . Calculate the length of the curve and compare with the length of the same curve with the euclidean metric on . I know that, in theory, the length of the curve is given by where . In practise I don't know how to proceed. Also, I presume that the length of the given curve with respect to the euclidean metric is Am I making some confusion here? Some help would be appreciated. Thanks!","g=\frac{dx^2+dy^2}{y^2} \mathbb{R}_+^2=\{(x,y) \in \mathbb{R}^2 : y>0\} \gamma(t)=(t,t), t \in [-1,-\frac{1}{2}] \mathbb{R}^2 \int_{-1}^{-\frac{1}{2}} \sqrt{g(\gamma',\gamma')}dt \gamma'(t)=D_{\gamma_t}(\frac{d}{dt}) \int_{-1}^{-\frac{1}{2}}\|\gamma'(t)\|dt = \frac{\sqrt2}{2}.","['differential-geometry', 'metric-spaces', 'riemannian-geometry']"
90,Curvature and Circumference of Circle,Curvature and Circumference of Circle,,"Theorem Let $\gamma\colon [a,b]\rightarrow \mathbb{R}^2$ be a unit speed simple closed curve, with $\gamma'(a)=\gamma'(b)$ and $N$ is the inward-pointing normal.  Then $$ \int_{a}^b \kappa_N(s)ds=2\pi. $$ Question: If a circle has curvature $\kappa$ and circumference $C$ then from above theorem, how does it follow that $\kappa C=2\pi$? Ref: Riemannian Manifolds - John M. Lee, Theorem 1.6 and comments, p.4.","Theorem Let $\gamma\colon [a,b]\rightarrow \mathbb{R}^2$ be a unit speed simple closed curve, with $\gamma'(a)=\gamma'(b)$ and $N$ is the inward-pointing normal.  Then $$ \int_{a}^b \kappa_N(s)ds=2\pi. $$ Question: If a circle has curvature $\kappa$ and circumference $C$ then from above theorem, how does it follow that $\kappa C=2\pi$? Ref: Riemannian Manifolds - John M. Lee, Theorem 1.6 and comments, p.4.",,"['differential-geometry', 'riemannian-geometry', 'plane-curves']"
91,Can a $1d$ space never be curved?,Can a  space never be curved?,1d,"I was wondering about this: Wikipedia article I refer to (here I refer to the first part: metric) This wikipedia article claims that this hyperbolic space model has constant curvature $-1.$ I believe they are talking about sectional curvature here, because they write down a metric in $n-$ coordinates. Now, I have two questions: What is about $n=1$: In that case, I think a Riemann curvature tensor will always be zero due to its symmetry properties, so there cannot be any curvature for the $1d$ case, right? Is there a way to argue that since the sectional curvature is $-1$ in the case of $n=2$ (Gauss curvature of Poincaré disk is -1) it has to be $-1$ for $n \ge 3$ and any plane, too? By a symmetry argument, the sectional curvature should be independent of the plane we are considering at a point for $n \ge 3$(because the metric is the same in all coordinates). Actually, I would assume that a metric that is conformal to the identity always has this property. Despite, I don't see why it has to be $-1$ too for $n \ge 3$ without carrying out a cumbersome calculation.","I was wondering about this: Wikipedia article I refer to (here I refer to the first part: metric) This wikipedia article claims that this hyperbolic space model has constant curvature $-1.$ I believe they are talking about sectional curvature here, because they write down a metric in $n-$ coordinates. Now, I have two questions: What is about $n=1$: In that case, I think a Riemann curvature tensor will always be zero due to its symmetry properties, so there cannot be any curvature for the $1d$ case, right? Is there a way to argue that since the sectional curvature is $-1$ in the case of $n=2$ (Gauss curvature of Poincaré disk is -1) it has to be $-1$ for $n \ge 3$ and any plane, too? By a symmetry argument, the sectional curvature should be independent of the plane we are considering at a point for $n \ge 3$(because the metric is the same in all coordinates). Actually, I would assume that a metric that is conformal to the identity always has this property. Despite, I don't see why it has to be $-1$ too for $n \ge 3$ without carrying out a cumbersome calculation.",,"['real-analysis', 'differential-geometry', 'manifolds', 'differential-topology', 'riemannian-geometry']"
92,Preimage of singular points of smooth map between manifolds,Preimage of singular points of smooth map between manifolds,,"Given a smooth ($C^{\infty}$) map $\phi: V \rightarrow SU(n)$ where $V$ is a (finite dim, real) vector space (of potentially very large dimension) and $SU(n)$ is the special unitary Lie group, what can be said about the  of singularities of this map? It is know that the preimage $\phi^{-1}(v)$ of a regular value $v$ is a submanifold of $V$. What is known about the preimage of a singular value? Is this also a manifold in this case? What about the preimage of all singular points? Do the singular values form a manifold? I known they are a null set by Sard's theorem but this does not use anything specific to $SU(n)$. Does the target space being $SU(n)$ help us to say anything more about the singular points? If we further know that the map $\phi$ is onto does this affect things?","Given a smooth ($C^{\infty}$) map $\phi: V \rightarrow SU(n)$ where $V$ is a (finite dim, real) vector space (of potentially very large dimension) and $SU(n)$ is the special unitary Lie group, what can be said about the  of singularities of this map? It is know that the preimage $\phi^{-1}(v)$ of a regular value $v$ is a submanifold of $V$. What is known about the preimage of a singular value? Is this also a manifold in this case? What about the preimage of all singular points? Do the singular values form a manifold? I known they are a null set by Sard's theorem but this does not use anything specific to $SU(n)$. Does the target space being $SU(n)$ help us to say anything more about the singular points? If we further know that the map $\phi$ is onto does this affect things?",,"['differential-geometry', 'lie-groups', 'lie-algebras', 'singularity-theory', 'morse-theory']"
93,Adjoint representation and tangent vectors,Adjoint representation and tangent vectors,,"Let $G$ be a Lie group, $\mathfrak{g}$ its Lie algebra, $\text{Ad}:G\rightarrow GL(\mathfrak{g})$ the adjoint representation of $G$. Then, for $X,Y\in \mathfrak{g}$, \begin{align*} \left(\frac{d}{dt}\bigg|_{t=0}\text{Ad}(\exp(tX))\right)Y&=\frac{d}{dt}\bigg|_{t=0}\left(\text{Ad}(\exp(tX))Y\right). \end{align*} I'm just wondering exactly why this equation is true? All I'm able to say about the situation is that $\text{Ad}(\exp(tX))$ is a curve in $GL(\mathfrak{g})$, thus \begin{align*} \frac{d}{dt}\bigg|_{t=0}\text{Ad}(\exp(tX)) \end{align*} is a tangent vector in $T_{\text{Id}}GL(\mathfrak{g})$. Any help here would be much appreciated.","Let $G$ be a Lie group, $\mathfrak{g}$ its Lie algebra, $\text{Ad}:G\rightarrow GL(\mathfrak{g})$ the adjoint representation of $G$. Then, for $X,Y\in \mathfrak{g}$, \begin{align*} \left(\frac{d}{dt}\bigg|_{t=0}\text{Ad}(\exp(tX))\right)Y&=\frac{d}{dt}\bigg|_{t=0}\left(\text{Ad}(\exp(tX))Y\right). \end{align*} I'm just wondering exactly why this equation is true? All I'm able to say about the situation is that $\text{Ad}(\exp(tX))$ is a curve in $GL(\mathfrak{g})$, thus \begin{align*} \frac{d}{dt}\bigg|_{t=0}\text{Ad}(\exp(tX)) \end{align*} is a tangent vector in $T_{\text{Id}}GL(\mathfrak{g})$. Any help here would be much appreciated.",,"['differential-geometry', 'representation-theory', 'manifolds', 'lie-groups', 'lie-algebras']"
94,a question about differential geometry(Gauss-bonnet theorem and isolated singular point in the surface),a question about differential geometry(Gauss-bonnet theorem and isolated singular point in the surface),,"Let C be a regular closed simple curve on a sphere $S^2$. Let v be a differentiable vector field on $S^2$ such that the trajectories of v are never tangent to C. prove that each of the two regions determined by C contains at least one singular point of v. My thoughts:Based on poincare's theorem,we know that $$\sum I_{i}={1\over 2\pi}\iint_{s}Kds$$,where $I_{i}$ is the index of v at the isolated singular point.Then,since the Euler-poincare characteristic of a sphere is 2,so we have $\sum I_{i}=2$. Then. I don't know how to contine to analyze the question,since I think $I_{i}$ is not necessary to be 1. So any help?","Let C be a regular closed simple curve on a sphere $S^2$. Let v be a differentiable vector field on $S^2$ such that the trajectories of v are never tangent to C. prove that each of the two regions determined by C contains at least one singular point of v. My thoughts:Based on poincare's theorem,we know that $$\sum I_{i}={1\over 2\pi}\iint_{s}Kds$$,where $I_{i}$ is the index of v at the isolated singular point.Then,since the Euler-poincare characteristic of a sphere is 2,so we have $\sum I_{i}=2$. Then. I don't know how to contine to analyze the question,since I think $I_{i}$ is not necessary to be 1. So any help?",,"['real-analysis', 'differential-geometry', 'riemannian-geometry']"
95,Orthonormal frame on hyperbolic plane,Orthonormal frame on hyperbolic plane,,"I'm having trouble comprehending a question from Do Carmo's Differential Forms and Applications. The question (in its entirety) is as follows: (Exercise 5-2 in Do Carmo) . Let $H^2$ be the upper half-plane, that is,   $$ H^2=\{(x,y)\in\mathbb{R}^2;y>0\}. $$   Consider in $H^2$ the following inner product: If $(x,y)\in H^2$ and $u,v\in T_pH^2$, then   \begin{equation} \langle u,v \rangle_p=\frac{u\cdot v}{y^2} \end{equation}   where $u\cdot v$ is the canonical inner product of $\mathbb{R}^2$. Prove that this is a Riemannian metric in $H^2$ whose Gaussian curvature is $K\equiv -1$; with this Riemannian metric $H^2$ is called the hyperbolic plane. [ Hint: Choose the orthonormal frame $e_1=\frac{a_1}{y}$, $e_2=\frac{a_2}{y}$, where $\{a_1,a_2\}$ is the canonical frame of $\mathbb{R}^2$.] I'm not all that troubled with the question, except that I don't understand the hint to the point where I think there's a typo (though there probably isn't). I get how $e_1,e_2$ are orthogonal with respect to the inner product $\langle\cdot\,,\cdot\rangle$, but $$ \langle e_1,e_1\rangle_p=\frac{e_1\cdot e_1}{y^2}=\frac{(a_1\cdot a_1)/y^2}{y^2}=\frac{1}{y^4}\neq 1. $$ So $e_1,e_2$ can't be orthonormal with the inner product. It would work perfectly with $e_1=ya_1$ and $e_2=ya_2$, but then why does the hint say something different? Am I misunderstanding what he means by ""orthonormal?"" Any help is appreciated. Thank you in advance.","I'm having trouble comprehending a question from Do Carmo's Differential Forms and Applications. The question (in its entirety) is as follows: (Exercise 5-2 in Do Carmo) . Let $H^2$ be the upper half-plane, that is,   $$ H^2=\{(x,y)\in\mathbb{R}^2;y>0\}. $$   Consider in $H^2$ the following inner product: If $(x,y)\in H^2$ and $u,v\in T_pH^2$, then   \begin{equation} \langle u,v \rangle_p=\frac{u\cdot v}{y^2} \end{equation}   where $u\cdot v$ is the canonical inner product of $\mathbb{R}^2$. Prove that this is a Riemannian metric in $H^2$ whose Gaussian curvature is $K\equiv -1$; with this Riemannian metric $H^2$ is called the hyperbolic plane. [ Hint: Choose the orthonormal frame $e_1=\frac{a_1}{y}$, $e_2=\frac{a_2}{y}$, where $\{a_1,a_2\}$ is the canonical frame of $\mathbb{R}^2$.] I'm not all that troubled with the question, except that I don't understand the hint to the point where I think there's a typo (though there probably isn't). I get how $e_1,e_2$ are orthogonal with respect to the inner product $\langle\cdot\,,\cdot\rangle$, but $$ \langle e_1,e_1\rangle_p=\frac{e_1\cdot e_1}{y^2}=\frac{(a_1\cdot a_1)/y^2}{y^2}=\frac{1}{y^4}\neq 1. $$ So $e_1,e_2$ can't be orthonormal with the inner product. It would work perfectly with $e_1=ya_1$ and $e_2=ya_2$, but then why does the hint say something different? Am I misunderstanding what he means by ""orthonormal?"" Any help is appreciated. Thank you in advance.",,['differential-geometry']
96,Clarification on basic (horizontal) differential forms,Clarification on basic (horizontal) differential forms,,"Here's a question from Lee's Smooth Manifolds (Exercise 12-9) which was more or less answered here . The question is this: Let $\pi:M\to N$ be a smooth surjective submersion between smooth manifolds $M,N$ such that each fiber is connected. A tangent vector $v\in T_pM$ is vertical if $d\pi_p(v)=0$. Suppose $\omega\in\Omega^k(M)$. Show that there exists $\eta\in\Omega^k(N)$ such that $\omega=\pi^{\ast}\eta$ if and only if $i_v\omega_p=0$ and $i_vd\omega_p=0$ for all $p\in M$ and vertical $v\in T_pM$. [Hint:first do the case in which $\pi:\mathbb{R}^{n+m}\to \mathbb{R}^n$ is a projection onto the first $n$ coordinates]. So the forward direction ($\omega=\pi^{\ast}\eta\Rightarrow$etc.) is a straightforward computation. However, the other implication is not at all clear. I could see how the hint could help in the general case, since we ought to be able to choose coordinates so that we can reduce to this case locally and glue it together to get some sort of global result with partitions of unity. However, even in this simplest case I'm not sure where to go. The linked answer does provide a sketch of the proof, although I don't understand it. Any help would be greatly appreciated.","Here's a question from Lee's Smooth Manifolds (Exercise 12-9) which was more or less answered here . The question is this: Let $\pi:M\to N$ be a smooth surjective submersion between smooth manifolds $M,N$ such that each fiber is connected. A tangent vector $v\in T_pM$ is vertical if $d\pi_p(v)=0$. Suppose $\omega\in\Omega^k(M)$. Show that there exists $\eta\in\Omega^k(N)$ such that $\omega=\pi^{\ast}\eta$ if and only if $i_v\omega_p=0$ and $i_vd\omega_p=0$ for all $p\in M$ and vertical $v\in T_pM$. [Hint:first do the case in which $\pi:\mathbb{R}^{n+m}\to \mathbb{R}^n$ is a projection onto the first $n$ coordinates]. So the forward direction ($\omega=\pi^{\ast}\eta\Rightarrow$etc.) is a straightforward computation. However, the other implication is not at all clear. I could see how the hint could help in the general case, since we ought to be able to choose coordinates so that we can reduce to this case locally and glue it together to get some sort of global result with partitions of unity. However, even in this simplest case I'm not sure where to go. The linked answer does provide a sketch of the proof, although I don't understand it. Any help would be greatly appreciated.",,"['differential-geometry', 'smooth-manifolds']"
97,Sets that are convex in two different metrics,Sets that are convex in two different metrics,,"Let $(M,g)$ be a complete Riemannian manifold, and let $C$ be a subset of $M$. We will say $C$ is convex if for any points $p,q \in C$, there exists a unique normal minimal geodesic $\gamma$ joining $p$ and $q$, such that $\gamma \subset C$. Now, suppose we have a smooth manifold $M$, and two metrics $g_1$ and $g_2$. Briefly, I am interested to know under what conditions $(M,g_1)$ and $(M,g_2)$ have the same convex sets, or at least a class of convex sets (e.g. convex geodesic balls) in common. That is, if $C \subset M$ is a convex set in $(M,g_1)$, is it convex in $(M,g_2)$? Certainly this isn't generally true, so what conditions on $M,g_1,g_2,C$ are necessary/sufficient? My guess is that very restrictive conditions are needed to ensure all convex sets are shared, so let me be more precise and describe the specific case I'm interested in: Let $M = \mathbb R^n$ and $g_1$ be the standard Euclidean metric, and let $C = B_R(x)$ be the usual ball of radius $R$ centered at $x$, and let $g_2$ be some other metric. Is $C$ convex in $(M,g_2)$? (or what conditions on $g_2,R$ are needed? In particular I imagine that $R$ cannot be too large) In practice, I will be assuming $g_2 = c^{-2}(x)dx^2$ (from a wave equation) and that the sectional curvature is bounded above, so that there is some control over the injectivity/convexity radius . EDIT (some more thoughts): I suspect this is tied to curvature; does the sectional curvature bound the curvature (as a curve in Euclidean space) of a geodesic of $g_2$?","Let $(M,g)$ be a complete Riemannian manifold, and let $C$ be a subset of $M$. We will say $C$ is convex if for any points $p,q \in C$, there exists a unique normal minimal geodesic $\gamma$ joining $p$ and $q$, such that $\gamma \subset C$. Now, suppose we have a smooth manifold $M$, and two metrics $g_1$ and $g_2$. Briefly, I am interested to know under what conditions $(M,g_1)$ and $(M,g_2)$ have the same convex sets, or at least a class of convex sets (e.g. convex geodesic balls) in common. That is, if $C \subset M$ is a convex set in $(M,g_1)$, is it convex in $(M,g_2)$? Certainly this isn't generally true, so what conditions on $M,g_1,g_2,C$ are necessary/sufficient? My guess is that very restrictive conditions are needed to ensure all convex sets are shared, so let me be more precise and describe the specific case I'm interested in: Let $M = \mathbb R^n$ and $g_1$ be the standard Euclidean metric, and let $C = B_R(x)$ be the usual ball of radius $R$ centered at $x$, and let $g_2$ be some other metric. Is $C$ convex in $(M,g_2)$? (or what conditions on $g_2,R$ are needed? In particular I imagine that $R$ cannot be too large) In practice, I will be assuming $g_2 = c^{-2}(x)dx^2$ (from a wave equation) and that the sectional curvature is bounded above, so that there is some control over the injectivity/convexity radius . EDIT (some more thoughts): I suspect this is tied to curvature; does the sectional curvature bound the curvature (as a curve in Euclidean space) of a geodesic of $g_2$?",,"['differential-geometry', 'reference-request', 'partial-differential-equations', 'riemannian-geometry']"
98,"$\Delta_L(\text{im}\,\delta^*_g)\subset\text{im}\,\delta^*_g$ and $\Delta_L\big(\text{ker}\,\text{Bian}(g)\big)\subset\text{ker}\,\text{Bian}(g)$?",and ?,"\Delta_L(\text{im}\,\delta^*_g)\subset\text{im}\,\delta^*_g \Delta_L\big(\text{ker}\,\text{Bian}(g)\big)\subset\text{ker}\,\text{Bian}(g)","Let $(M,g)$ be an Einstein manifold with Levi-Civita connection $\nabla$ and whose Ricci tensor $\text{Rc}(g)=g$, in components $R_{ij}=g_{ij}$.  The Lichnerowicz Laplacian of $g$ is the map \begin{align*} \Delta_L:\Gamma(S^2M)&\rightarrow\Gamma(S^2M)\\ h&\mapsto(\Delta_Lh)_{ij}=g^{st}\nabla_s\nabla_th_{ij}-g^{st}R_{is}h_{jt}-g^{st}T_{is}h_{it}-2g^{pq}g^{qt}R_{isjt}h_{pq}. \end{align*} The Bianchi operator is the map \begin{align*} \text{Bian}(g):\Gamma(S^2M)&\rightarrow\Gamma(T^*)\\ h&\mapsto\text{Bian}(g,h)=\delta_gg+\frac{1}{2}\text{d}(\text{tr}_gh) \end{align*} where the divergence is the map \begin{align*} \delta_g:\Gamma(S^2M)&\rightarrow\Gamma(T^*M)\\ h&\mapsto(\delta_gh)_k=-g^{st}\nabla_sh_{tk}. \end{align*} The formal $L^2$-adjoint of the divergence is the map \begin{align*} \delta^*_g:\Gamma(T^*M)&\rightarrow\Gamma(S^2M)\\ \omega&\mapsto(\delta^*_g\omega)_{ij}=\frac{1}{2}(\nabla_i\omega_j+\nabla_j\omega_i). \end{align*} Question :  Does it hold that \begin{align*} \Delta_L(\text{im}\,\delta^*_g)\subset\text{im}\,\delta^*_g\qquad\text{and}\qquad\Delta_L\big(\text{ker}\,\text{Bian}(g)\big)\subset\text{ker}\,\text{Bian}(g)? \end{align*} If these set inclusions do hold, how does one prove them?","Let $(M,g)$ be an Einstein manifold with Levi-Civita connection $\nabla$ and whose Ricci tensor $\text{Rc}(g)=g$, in components $R_{ij}=g_{ij}$.  The Lichnerowicz Laplacian of $g$ is the map \begin{align*} \Delta_L:\Gamma(S^2M)&\rightarrow\Gamma(S^2M)\\ h&\mapsto(\Delta_Lh)_{ij}=g^{st}\nabla_s\nabla_th_{ij}-g^{st}R_{is}h_{jt}-g^{st}T_{is}h_{it}-2g^{pq}g^{qt}R_{isjt}h_{pq}. \end{align*} The Bianchi operator is the map \begin{align*} \text{Bian}(g):\Gamma(S^2M)&\rightarrow\Gamma(T^*)\\ h&\mapsto\text{Bian}(g,h)=\delta_gg+\frac{1}{2}\text{d}(\text{tr}_gh) \end{align*} where the divergence is the map \begin{align*} \delta_g:\Gamma(S^2M)&\rightarrow\Gamma(T^*M)\\ h&\mapsto(\delta_gh)_k=-g^{st}\nabla_sh_{tk}. \end{align*} The formal $L^2$-adjoint of the divergence is the map \begin{align*} \delta^*_g:\Gamma(T^*M)&\rightarrow\Gamma(S^2M)\\ \omega&\mapsto(\delta^*_g\omega)_{ij}=\frac{1}{2}(\nabla_i\omega_j+\nabla_j\omega_i). \end{align*} Question :  Does it hold that \begin{align*} \Delta_L(\text{im}\,\delta^*_g)\subset\text{im}\,\delta^*_g\qquad\text{and}\qquad\Delta_L\big(\text{ker}\,\text{Bian}(g)\big)\subset\text{ker}\,\text{Bian}(g)? \end{align*} If these set inclusions do hold, how does one prove them?",,"['differential-geometry', 'manifolds', 'riemannian-geometry', 'laplacian', 'elliptic-operators']"
99,Path integral of a closed form on $\mathbb{R}^{2}\backslash\{0\}$,Path integral of a closed form on,\mathbb{R}^{2}\backslash\{0\},"Let $$ \omega=\frac{-y\; dx}{x^{2}+y^{2}}+\frac{x\; dy}{x^{2}+y^{2}} \in \Omega^{1}(\mathbb{R}^{2}\backslash\{0\}) $$ I understand that the form $\frac{-y}{x^{2}+y^{2}}dx+\frac{x}{x^{2}+y^{2}}dy$ is closed but not exact on $\mathbb{R}^{2}\backslash\{0\}$, and I am somewhat familiar with its relationship with the winding number. I encountered the assertion that (I'm pretty sure I'm recalling it correctly), if one restricts to a path, $\gamma:[0,1]\rightarrow \mathbb{R}^{2}\backslash\{0\}$ on which $\gamma(t)=(x(t),y(t))$ satisfies $y(t)\ge 0$ for all $t$, then any path, $\alpha$ satisfying these requirements whose endpoints coincide with those of $\gamma$ also satisfies $\int_{\gamma}\omega=\int_{\alpha}\omega$. Someone told me about a homotopy-based proof, but I'm not very good at proving when functions are homotopic, so I'm a little reluctant to tread down that path. If one makes a coordinate change to polar coordinates, $\omega$ becomes $d\theta$ (right?). What is the significance of this fact when $\gamma$ is forced to obey the conditions above (i.e. $y$ component is non-negative)? I sincerely apologize for the vagueness of my question, this is my first time posting on here, a bit nervous asking for help. I just find differential forms and the like are hard to parse, partly because the notes provided tend to suppress a lot of notation. If anyone else could explain the situation to me I'd be always grateful.Right now I'm talking to myself only as if I were a five year old.","Let $$ \omega=\frac{-y\; dx}{x^{2}+y^{2}}+\frac{x\; dy}{x^{2}+y^{2}} \in \Omega^{1}(\mathbb{R}^{2}\backslash\{0\}) $$ I understand that the form $\frac{-y}{x^{2}+y^{2}}dx+\frac{x}{x^{2}+y^{2}}dy$ is closed but not exact on $\mathbb{R}^{2}\backslash\{0\}$, and I am somewhat familiar with its relationship with the winding number. I encountered the assertion that (I'm pretty sure I'm recalling it correctly), if one restricts to a path, $\gamma:[0,1]\rightarrow \mathbb{R}^{2}\backslash\{0\}$ on which $\gamma(t)=(x(t),y(t))$ satisfies $y(t)\ge 0$ for all $t$, then any path, $\alpha$ satisfying these requirements whose endpoints coincide with those of $\gamma$ also satisfies $\int_{\gamma}\omega=\int_{\alpha}\omega$. Someone told me about a homotopy-based proof, but I'm not very good at proving when functions are homotopic, so I'm a little reluctant to tread down that path. If one makes a coordinate change to polar coordinates, $\omega$ becomes $d\theta$ (right?). What is the significance of this fact when $\gamma$ is forced to obey the conditions above (i.e. $y$ component is non-negative)? I sincerely apologize for the vagueness of my question, this is my first time posting on here, a bit nervous asking for help. I just find differential forms and the like are hard to parse, partly because the notes provided tend to suppress a lot of notation. If anyone else could explain the situation to me I'd be always grateful.Right now I'm talking to myself only as if I were a five year old.",,['differential-geometry']
