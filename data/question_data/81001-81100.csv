,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that the given two linear mappings commute,Show that the given two linear mappings commute,,"Given $A\in M_n(\mathbb C)$ , show that the mappings $$\alpha_A(B) = \frac{1}{2}(AB + BA^{*})$$ $$\beta_A(B) = \frac{1}{2i}(AB - BA^{*})$$ define $\mathbb R$ -linear maps $HERM_n(\mathbb C) → HERM_n(\mathbb C)$ . Also show that $\alpha_A, \beta_A$ commute with each other. What I did - First I proved that $\alpha_A$ and $\alpha_B$ are Hermitian. Then I tried to prove that $\alpha_A*\alpha_B=\alpha_B*\alpha_A$ . The final expression was $AB^2A^*=BA^*AB$ but I can't see how they are equal. .","Given , show that the mappings define -linear maps . Also show that commute with each other. What I did - First I proved that and are Hermitian. Then I tried to prove that . The final expression was but I can't see how they are equal. .","A\in M_n(\mathbb C) \alpha_A(B) = \frac{1}{2}(AB + BA^{*}) \beta_A(B) = \frac{1}{2i}(AB - BA^{*}) \mathbb R HERM_n(\mathbb C) → HERM_n(\mathbb C) \alpha_A, \beta_A \alpha_A \alpha_B \alpha_A*\alpha_B=\alpha_B*\alpha_A AB^2A^*=BA^*AB","['linear-algebra', 'matrices', 'linear-transformations', 'hermitian-matrices']"
1,quadratic equations on 2 by 2 matrices,quadratic equations on 2 by 2 matrices,,"For real non-zero $2\times 2$ matrices, can we say: For any $A,B$ , there are at most two matrices $X$ such that $XX + AX + B =0$ Is there a way to see this without going in the direction of writing the open form result of $XX + AX + B$ and solve each element is equal to zero, etc.?","For real non-zero matrices, can we say: For any , there are at most two matrices such that Is there a way to see this without going in the direction of writing the open form result of and solve each element is equal to zero, etc.?","2\times 2 A,B X XX + AX + B =0 XX + AX + B","['linear-algebra', 'matrices', 'quadratic-forms']"
2,How to find eigenvalues of the matrix,How to find eigenvalues of the matrix,,"This is a question from our end-semester exam: How to find the eigenvalues of the given matrix: M= \begin{bmatrix} 5,1,1,1,1,1\\ 1,5,1,1,1,1\\ 1,1,5,1,1,1\\ 1,1,1,5,1,1\\ 1,1,1,1,4,0\\ 1,1,1,1,0,4\\ \end{bmatrix} I know that $4$ is an eigenvalue of $M$ with multiplicity atleast $3$ since $M-4I$ has $4$ identical rows. Is there any way to find all eigenvalues of this matrix? I could find only $3$ out of $6$ .",This is a question from our end-semester exam: How to find the eigenvalues of the given matrix: M= I know that is an eigenvalue of with multiplicity atleast since has identical rows. Is there any way to find all eigenvalues of this matrix? I could find only out of .,"\begin{bmatrix}
5,1,1,1,1,1\\
1,5,1,1,1,1\\
1,1,5,1,1,1\\
1,1,1,5,1,1\\
1,1,1,1,4,0\\
1,1,1,1,0,4\\
\end{bmatrix} 4 M 3 M-4I 4 3 6","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
3,Computing the determinant of $X^*X$ given $X$.,Computing the determinant of  given .,X^*X X,"I am trying to prove that if $u_i\in \mathbb{R}$ for $i=1,...,n$ and $$X =\begin{pmatrix} 1 & 0 & 0 & \cdots & 0 \\  0 & 1 & 0 & \cdots & 0 \\  0 & 0 & 1 & \cdots & 0 \\ \ & \vdots & \ & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1 \\ u_1 & u_2 & u_3 & \cdots & u_n \end{pmatrix}$$ then $\det(X^{T}X)=1 +u_1^2 + u_2^2 + \cdots + u_n^2$ , where $X^T$ is the transpose of $X$ . Here is what I know. $X$ is an $(n+1)$ -by- $n$ matrix, and $X^T$ is an $n$ -by- $(n+1)$ matrix; thus $X^TX$ is an $n$ -by- $n$ square matrix, so we can take its determinant. Also, the formula is simple to verify for the cases $n=2,3$ . The $(i,j)$ entry of the  matrix $X^TX$ is \begin{align*} (X^TX)_{i,j} &= \sum_{k}(X^T)_{i,k}X_{k,j} = \sum_k X_{k,i}X_{k,j} \\ &= \langle X_{\cdot,i},X_{\cdot, j}  \rangle = \delta_{i,j} + u_iu_j \end{align*} where $X_{\cdot,j}$ denotes the $j$ th column of $X$ . Since I know all the entries, I could do some combination of induction and cofactor expansion, but I couldn't make it work. I also tried computation with ""block"" matrices, which is something I am not very familiar with. If $I_n$ is the $n$ -by- $n$ identity matrix and $\vec{u}=(u_1,\ldots,u_n)$ then we have $$ \vec{u} = \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix} \;\;\; \text{ and}\;\;\; \left( \vec{u}\right)^T = \begin{pmatrix} u_1 & u_2 & \cdots & u_n\end{pmatrix} $$ so that $$ X^T = \begin{pmatrix} I_n & \vec{u} \end{pmatrix} \;\;\; \text{ and}\;\;\; X = \begin{pmatrix} I_n \\ \left(\vec{u}\right)^T \end{pmatrix}. $$ Then if we are presumptuous, we can treat $X^T$ and $X$ as $1$ -by- $2$ and $2$ -by- $1$ matrices respectively. Then $$ X^TX = 1 + \vec{u}\left(\vec{u} \right)^T = 1 + \|u\|^2 $$ which clearly would imply the result I want, if this computation can be justified. I was unable to find this specific problem elsewhere, but I am sure it has been asked other times on this site. Please let me know your thoughts and tips for this problem, and help me justify or refute the block matrix computation I performed. If we cannot justify it, then why does it hint at the right answer? Thanks.","I am trying to prove that if for and then , where is the transpose of . Here is what I know. is an -by- matrix, and is an -by- matrix; thus is an -by- square matrix, so we can take its determinant. Also, the formula is simple to verify for the cases . The entry of the  matrix is where denotes the th column of . Since I know all the entries, I could do some combination of induction and cofactor expansion, but I couldn't make it work. I also tried computation with ""block"" matrices, which is something I am not very familiar with. If is the -by- identity matrix and then we have so that Then if we are presumptuous, we can treat and as -by- and -by- matrices respectively. Then which clearly would imply the result I want, if this computation can be justified. I was unable to find this specific problem elsewhere, but I am sure it has been asked other times on this site. Please let me know your thoughts and tips for this problem, and help me justify or refute the block matrix computation I performed. If we cannot justify it, then why does it hint at the right answer? Thanks.","u_i\in \mathbb{R} i=1,...,n X =\begin{pmatrix} 1 & 0 & 0 & \cdots & 0 \\
 0 & 1 & 0 & \cdots & 0 \\
 0 & 0 & 1 & \cdots & 0 \\
\ & \vdots & \ & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
u_1 & u_2 & u_3 & \cdots & u_n
\end{pmatrix} \det(X^{T}X)=1 +u_1^2 + u_2^2 + \cdots + u_n^2 X^T X X (n+1) n X^T n (n+1) X^TX n n n=2,3 (i,j) X^TX \begin{align*} (X^TX)_{i,j} &= \sum_{k}(X^T)_{i,k}X_{k,j} = \sum_k X_{k,i}X_{k,j} \\
&= \langle X_{\cdot,i},X_{\cdot, j}  \rangle = \delta_{i,j} + u_iu_j
\end{align*} X_{\cdot,j} j X I_n n n \vec{u}=(u_1,\ldots,u_n)  \vec{u} = \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix} \;\;\; \text{ and}\;\;\; \left( \vec{u}\right)^T = \begin{pmatrix} u_1 & u_2 & \cdots & u_n\end{pmatrix}   X^T = \begin{pmatrix} I_n & \vec{u} \end{pmatrix} \;\;\; \text{ and}\;\;\; X = \begin{pmatrix} I_n \\ \left(\vec{u}\right)^T \end{pmatrix}.  X^T X 1 2 2 1  X^TX = 1 + \vec{u}\left(\vec{u} \right)^T = 1 + \|u\|^2 ","['linear-algebra', 'matrices', 'determinant', 'transpose', 'block-matrices']"
4,"If $ Av=Bv=\lambda v$, can we conclude that $A=B$?","If , can we conclude that ?", Av=Bv=\lambda v A=B,"Let $A $ and $B$ be $2\times2$ matrices with integer entries. Let $v$ be an eigenvector of both $A$ and $B$ with the same eigenvalue $\lambda \neq 0$ . So we have $$ Av=Bv=\lambda v. $$ My question is the following: Under which restrictions can we conclude that $A=B$ ? Or is it always $A=B$ ? Does it depend on the singularity of $A$ and $B$ ? In other words, if a vector $v$ is an eigenvector with a non-zero eigenvalue for some matrix $A$ , is $A$ unique?","Let and be matrices with integer entries. Let be an eigenvector of both and with the same eigenvalue . So we have My question is the following: Under which restrictions can we conclude that ? Or is it always ? Does it depend on the singularity of and ? In other words, if a vector is an eigenvector with a non-zero eigenvalue for some matrix , is unique?","A  B 2\times2 v A B \lambda \neq 0 
Av=Bv=\lambda v.
 A=B A=B A B v A A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
5,"If $\vec u$ is orthogonal to both $\vec v $ and $\vec w$, and $\vec u$ not equal to $0$, prove $\vec u$ is not in the span of $ \vec v$ and $\vec w$.","If  is orthogonal to both  and , and  not equal to , prove  is not in the span of  and .",\vec u \vec v  \vec w \vec u 0 \vec u  \vec v \vec w,"QN: If u is orthogonal to both v and w , and u ≠ 0, argue that u is not in the span of v and w . Where I am at: I get stuck when it comes to solving my augmented matrix with Gauss Jordan Elimination. I also tried formulating the following steps to solve the problem. Create instances of u , v and w that pertain to the question. My visualisation in Geogebra can be viewed here: https://ggbm.at/b6xvwhpa Set u = a v + b w = u (where a and b are constants) Disprove (2) However, I could not get past step 1. Any pointers would be greatly appreciated.","QN: If u is orthogonal to both v and w , and u ≠ 0, argue that u is not in the span of v and w . Where I am at: I get stuck when it comes to solving my augmented matrix with Gauss Jordan Elimination. I also tried formulating the following steps to solve the problem. Create instances of u , v and w that pertain to the question. My visualisation in Geogebra can be viewed here: https://ggbm.at/b6xvwhpa Set u = a v + b w = u (where a and b are constants) Disprove (2) However, I could not get past step 1. Any pointers would be greatly appreciated.",,['linear-algebra']
6,Eigenvalues of two symmetric $4\times 4$ matrices: why is one negative of the other?,Eigenvalues of two symmetric  matrices: why is one negative of the other?,4\times 4,"Consider the following symmetric matrix: $$ M_0 =  \begin{pmatrix} 0 & 1 & 2 & 0 \\ 1 & 0 & 4 & 3 \\ 2 & 4 & 0 & 1 \\ 0 & 3 & 1 & 0 \end{pmatrix} $$ and a very similar matrix: $$ M_1 =  \begin{pmatrix} 0 & 1 & 2 & 0 \\ 1 & 0 & -4 & 3 \\ 2 & -4 & 0 & 1 \\ 0 & 3 & 1 & 0 \end{pmatrix} $$ To my surprise, the eigenspectrum of $M_0$ and $(-M_1)$ are the same ! Why would this be the case? I also tried playing around with the values a little; for example, if the center block is $\begin{pmatrix}1 & \pm 4 \\ \pm 4 & 1\end{pmatrix}$ instead, then they do not share the same eigenvalues. Context: I was considering the Hermitian matrix of this form ( $M_2$ below) and noted that this has the same property as the matrix $M_0$ from above. Thus, presumably, it has nothing to do with the fact that the middle block is complex. $$ M_2 =  \begin{pmatrix} 0 & 1 & 2 & 0 \\ 1 & 0 & e^{ix} & 3 \\ 2 & e^{-ix} & 0 & 1 \\ 0 & 3 & 1 & 0 \end{pmatrix} $$ ps. I will accept any answer which explains the phenomenon between the real matrices. I think that would give a hint as to why $M_2$ / Hermitian matrices have the same property. Thanks.","Consider the following symmetric matrix: and a very similar matrix: To my surprise, the eigenspectrum of and are the same ! Why would this be the case? I also tried playing around with the values a little; for example, if the center block is instead, then they do not share the same eigenvalues. Context: I was considering the Hermitian matrix of this form ( below) and noted that this has the same property as the matrix from above. Thus, presumably, it has nothing to do with the fact that the middle block is complex. ps. I will accept any answer which explains the phenomenon between the real matrices. I think that would give a hint as to why / Hermitian matrices have the same property. Thanks.","
M_0 = 
\begin{pmatrix}
0 & 1 & 2 & 0 \\
1 & 0 & 4 & 3 \\
2 & 4 & 0 & 1 \\
0 & 3 & 1 & 0
\end{pmatrix}
 
M_1 = 
\begin{pmatrix}
0 & 1 & 2 & 0 \\
1 & 0 & -4 & 3 \\
2 & -4 & 0 & 1 \\
0 & 3 & 1 & 0
\end{pmatrix}
 M_0 (-M_1) \begin{pmatrix}1 & \pm 4 \\ \pm 4 & 1\end{pmatrix} M_2 M_0 
M_2 = 
\begin{pmatrix}
0 & 1 & 2 & 0 \\
1 & 0 & e^{ix} & 3 \\
2 & e^{-ix} & 0 & 1 \\
0 & 3 & 1 & 0
\end{pmatrix}
 M_2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
7,can any symmetrix matrix be written as $A^TA$?,can any symmetrix matrix be written as ?,A^TA,"Let $A$ be a $m\times n$ real matrix. Then $B:=A^TA$ is an $n\times n$ symmetric matrix. Is the converse true? More precisely, given any $n\times n$ symmetric matrix $B$ and any positive integer $m$ , can $B$ be written as $A^TA$ for some $m\times n$ matrix $A$ ?","Let be a real matrix. Then is an symmetric matrix. Is the converse true? More precisely, given any symmetric matrix and any positive integer , can be written as for some matrix ?",A m\times n B:=A^TA n\times n n\times n B m B A^TA m\times n A,"['linear-algebra', 'matrices', 'symmetric-matrices']"
8,What vector x will maximize the norm of $\|Ax\|_2 / \|x\|_2$ (norm 2),What vector x will maximize the norm of  (norm 2),\|Ax\|_2 / \|x\|_2,"I know for norm one vector $x$ should be a basis vector. Where one is in the column of matrix $A$ . and for infinity norm $x$ should have elements $-1$ for negative values of the maximum row and $+1$ for positive values of matrix $A$ . Now I am trying to figure out how can I maximize it for norm 2. I think it is related to the eigenvalues of (AT.A), however, I don't know where to start.","I know for norm one vector should be a basis vector. Where one is in the column of matrix . and for infinity norm should have elements for negative values of the maximum row and for positive values of matrix . Now I am trying to figure out how can I maximize it for norm 2. I think it is related to the eigenvalues of (AT.A), however, I don't know where to start.",x A x -1 +1 A,"['matrices', 'numerical-methods', 'normed-spaces']"
9,Showing that Matrix $A \in M_{50}(\mathbb{R})$ is invertible,Showing that Matrix  is invertible,A \in M_{50}(\mathbb{R}),"I have matrix $A \in M_{50}(\mathbb{R})$ . This matrix has $A_{i,i} = 0$ for all $ 1 \le i \le 50$ and $A_{i,j} \in \{\pm1\}$ for all distinct $1 \le i, j \le 50$ . I must show that $A$ is invertible. In order to do so, I probably would want to show that $det(A)$ is non-zero. However, this is a 50 x 50 matrix, and surely, there must be another way to show that $det(A)$ is non-zero besides brute force computation. Someone told me to consider the image of $A$ in $M_{50}(F_{2})$ , but I am not sure how to apply this into the proof. Can anybody clarify some things? Thank you.","I have matrix . This matrix has for all and for all distinct . I must show that is invertible. In order to do so, I probably would want to show that is non-zero. However, this is a 50 x 50 matrix, and surely, there must be another way to show that is non-zero besides brute force computation. Someone told me to consider the image of in , but I am not sure how to apply this into the proof. Can anybody clarify some things? Thank you.","A \in M_{50}(\mathbb{R}) A_{i,i} = 0  1 \le i \le 50 A_{i,j} \in \{\pm1\} 1 \le i, j \le 50 A det(A) det(A) A M_{50}(F_{2})","['linear-algebra', 'abstract-algebra', 'matrices']"
10,System of linear differential equation,System of linear differential equation,,"How to solve the system of linear differential equation of the form $$x' = Ax + b$$ I can solve the homogeneous form by finding the eigenvalues and respective eigenvectors, but how to find the particular solution part. Also is there any limitation from getting eigenvalues positive, negative or complex. Any other different method involving matrix algebra is also welcome. Thank you.","How to solve the system of linear differential equation of the form $$x' = Ax + b$$ I can solve the homogeneous form by finding the eigenvalues and respective eigenvectors, but how to find the particular solution part. Also is there any limitation from getting eigenvalues positive, negative or complex. Any other different method involving matrix algebra is also welcome. Thank you.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
11,"Inverse of an ""anti-Jordan"" matrix","Inverse of an ""anti-Jordan"" matrix",,"Call a matrix anti-Jordan if it has $-1$s on the subdiagonal, values on the diagonal, and zeroes elsewhere; that is, it is written $$\begin{bmatrix} \lambda_1 & 0 & 0 & \dots \\ -1 & \lambda_2 & 0 & \dots\\ 0 & -1 & \lambda_3 & \dots\\ \vdots &&& \ddots \end{bmatrix}$$ Is there a general formula for the inverse of such a matrix? If it helps, I'm trying to compute the spectral radius of an $N \times N$ matrix $FV^{-1}$ where $$F = \begin{bmatrix}\beta_1 & \beta_2 & \beta_3 & \dots\\0 & 0 & 0 &\dots\\0 &0 & 0 & \dots\\\vdots &&& \ddots\end{bmatrix}$$ $N$ is ""very large"" and $$V = \begin{bmatrix} 1 + \mu_1 & 0 & 0 & \dots \\ -1 & 1 + \mu_2 & 0 & \dots\\ 0 & -1 & 1 + \mu_3 & \dots\\ \vdots &&& \ddots \end{bmatrix}.$$","Call a matrix anti-Jordan if it has $-1$s on the subdiagonal, values on the diagonal, and zeroes elsewhere; that is, it is written $$\begin{bmatrix} \lambda_1 & 0 & 0 & \dots \\ -1 & \lambda_2 & 0 & \dots\\ 0 & -1 & \lambda_3 & \dots\\ \vdots &&& \ddots \end{bmatrix}$$ Is there a general formula for the inverse of such a matrix? If it helps, I'm trying to compute the spectral radius of an $N \times N$ matrix $FV^{-1}$ where $$F = \begin{bmatrix}\beta_1 & \beta_2 & \beta_3 & \dots\\0 & 0 & 0 &\dots\\0 &0 & 0 & \dots\\\vdots &&& \ddots\end{bmatrix}$$ $N$ is ""very large"" and $$V = \begin{bmatrix} 1 + \mu_1 & 0 & 0 & \dots \\ -1 & 1 + \mu_2 & 0 & \dots\\ 0 & -1 & 1 + \mu_3 & \dots\\ \vdots &&& \ddots \end{bmatrix}.$$",,"['linear-algebra', 'matrices']"
12,Why ${(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1} =2\times({I + 4S})^{-1}$,Why,{(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1} =2\times({I + 4S})^{-1},"I met an interesting matrix question in a paper. My answer is ${(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1}$, but the answer in the paper is $ 2\times({I + 4S})^{-1}$. When I plugged in some value in $S$, I found they are equal, so why $${(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1} =2\times({I + 4S})^{-1}$$ where $I$ is the identity matrix, and S is a symmetric positive definite matrix.","I met an interesting matrix question in a paper. My answer is ${(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1}$, but the answer in the paper is $ 2\times({I + 4S})^{-1}$. When I plugged in some value in $S$, I found they are equal, so why $${(2S)}^{-1} - {(2S)}^{-1}({I + 4S})^{-1} =2\times({I + 4S})^{-1}$$ where $I$ is the identity matrix, and S is a symmetric positive definite matrix.",,"['linear-algebra', 'matrices', 'matrix-equations']"
13,Derivative of trace of $A^\top A$,Derivative of trace of,A^\top A,"Presume you have $$f(A) = \frac{1}{2} \mbox{trace}(A^\top A)$$  with $A$ ""nice"" enough. What is its derivative $\frac{Df(A)}{DA}$ and how do you compute it?","Presume you have $$f(A) = \frac{1}{2} \mbox{trace}(A^\top A)$$  with $A$ ""nice"" enough. What is its derivative $\frac{Df(A)}{DA}$ and how do you compute it?",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
14,Maximize $\det (A)$ subject to $\|A\|_{\text{F}} \le 1$,Maximize  subject to,\det (A) \|A\|_{\text{F}} \le 1,"$A$ is an $n \times n$ matrix such that the sum of squares of its elements is less than or equal to 1. What is $\max \det (A)$ ? (a) $n = 2$ (b) $n = 3$ My partial solution For $n = 2$ , I think the answer is 0.5. Define the constrained maximisation problem as $$\max(a_{11}a_{22}-a_{21}a_{12})$$ such that $$a_{11}^2+a_{12}^2+a_{21}^2+a_{22}^2 \le 1$$ If $a_{21}=a_{12}=0$ , the maximum $0.5$ is achieved at $a_{11}=a_{22}=1/\sqrt{2}$ . If $a_{21} \ne 0, a_{12}\ne0$ , the maximum $0.5$ is achieved at $a_{11}=a_{22}=a_{21}=0.5$ and $a_{12}=-0.5$ But how to I prove that formally? And how to I tackle the more difficult case of $n=3$ ?","is an matrix such that the sum of squares of its elements is less than or equal to 1. What is ? (a) (b) My partial solution For , I think the answer is 0.5. Define the constrained maximisation problem as such that If , the maximum is achieved at . If , the maximum is achieved at and But how to I prove that formally? And how to I tackle the more difficult case of ?","A n \times n \max \det (A) n = 2 n = 3 n = 2 \max(a_{11}a_{22}-a_{21}a_{12}) a_{11}^2+a_{12}^2+a_{21}^2+a_{22}^2 \le 1 a_{21}=a_{12}=0 0.5 a_{11}=a_{22}=1/\sqrt{2} a_{21} \ne 0, a_{12}\ne0 0.5 a_{11}=a_{22}=a_{21}=0.5 a_{12}=-0.5 n=3","['matrices', 'optimization']"
15,Can the operator $\operatorname{id}_V-(AB-BA)$ be nilpotent in the infinite-dimensional case.,Can the operator  be nilpotent in the infinite-dimensional case.,\operatorname{id}_V-(AB-BA),"Let $V$ be a infinite-dimensional vector space over the field of characteristic $0$ and $A,B$ be linear operators of $V$. Let $\operatorname{id}_V$ be an identical operator. Using trace function it is not easy to show that the operator $$ \phi = \operatorname{id}_V-(AB-BA) $$ can not be nilpotent in the case, where $V$ is finite-dimensional . My question. Can the above operator $\phi$ be nilpotent in the case, where $V$ is infinite-dimensional?","Let $V$ be a infinite-dimensional vector space over the field of characteristic $0$ and $A,B$ be linear operators of $V$. Let $\operatorname{id}_V$ be an identical operator. Using trace function it is not easy to show that the operator $$ \phi = \operatorname{id}_V-(AB-BA) $$ can not be nilpotent in the case, where $V$ is finite-dimensional . My question. Can the above operator $\phi$ be nilpotent in the case, where $V$ is infinite-dimensional?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'trace', 'nilpotence']"
16,Is it true: A real block diagonal matrix is diagonalizable if and only if each block is diagonalizable.,Is it true: A real block diagonal matrix is diagonalizable if and only if each block is diagonalizable.,,"We know this theorem and I know the proof for that. Theorem. Define $A \in M_{n}(\mathbb{C})$ such that   $$ A =  \begin{bmatrix} A_{1} & & 0  \\ &  \ddots &       \\ 0   & & A_{k} \end{bmatrix} $$    where $A_{k} \in M_{n_{i}}$ are block matrices.   Then, $A$ is diagonalizable if and only if each of $A_{k}$ is   diagonalizable. I am curious to know whether this theorem holds for real matrices or not?","We know this theorem and I know the proof for that. Theorem. Define $A \in M_{n}(\mathbb{C})$ such that   $$ A =  \begin{bmatrix} A_{1} & & 0  \\ &  \ddots &       \\ 0   & & A_{k} \end{bmatrix} $$    where $A_{k} \in M_{n_{i}}$ are block matrices.   Then, $A$ is diagonalizable if and only if each of $A_{k}$ is   diagonalizable. I am curious to know whether this theorem holds for real matrices or not?",,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-decomposition']"
17,eigenvector of compositions implies eigenvector of respective functions in composition?,eigenvector of compositions implies eigenvector of respective functions in composition?,,"Suppose the matrices $A, B \in Mat(n, \mathbb{F}).$ If a vector $v$ is an eigenvector of the matrix $AB,$ that is, of the composition $f_A \circ f_B,$ then is it also an eigenvector of $A$ and of $B,$ that is, of $f_A$ and of $f_B?$ I think not, for I came up with the example $$v \mapsto \lambda v + w$$ under $f_A$ for some non-zero vector $w$ that is linearly independent with respect to $v,$ and $$\lambda v + w \mapsto \lambda v$$ under $f_B.$ But, I'm not sure if my example makes any sense. Note that $A$ and $B$ are square matrices. Thank you!","Suppose the matrices $A, B \in Mat(n, \mathbb{F}).$ If a vector $v$ is an eigenvector of the matrix $AB,$ that is, of the composition $f_A \circ f_B,$ then is it also an eigenvector of $A$ and of $B,$ that is, of $f_A$ and of $f_B?$ I think not, for I came up with the example $$v \mapsto \lambda v + w$$ under $f_A$ for some non-zero vector $w$ that is linearly independent with respect to $v,$ and $$\lambda v + w \mapsto \lambda v$$ under $f_B.$ But, I'm not sure if my example makes any sense. Note that $A$ and $B$ are square matrices. Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'eigenfunctions']"
18,Why doesn't $A^2=I$ imply $A=\pm I$? [duplicate],Why doesn't  imply ? [duplicate],A^2=I A=\pm I,"This question already has answers here : If $A^2 = I$ (Identity Matrix) then $A = \pm I$ (5 answers) Closed 6 years ago . Im having trouble believing this T/F Question: if $\mathrm A^2=I$ then $\mathrm A = \pm \mathrm I$ The answer is False but why? If the matrix is $\mathrm A = \mathrm I,$ say \begin{bmatrix}1& 0\\      0 & 1 \end{bmatrix} then $\mathrm A^2$ is also that. And if $\mathrm A = -\mathrm I,$ then it is \begin{bmatrix}-1 & 0\\                    0 &-1  \end{bmatrix} and that squared is also the same? Where am i going wrong?","This question already has answers here : If $A^2 = I$ (Identity Matrix) then $A = \pm I$ (5 answers) Closed 6 years ago . Im having trouble believing this T/F Question: if $\mathrm A^2=I$ then $\mathrm A = \pm \mathrm I$ The answer is False but why? If the matrix is $\mathrm A = \mathrm I,$ say \begin{bmatrix}1& 0\\      0 & 1 \end{bmatrix} then $\mathrm A^2$ is also that. And if $\mathrm A = -\mathrm I,$ then it is \begin{bmatrix}-1 & 0\\                    0 &-1  \end{bmatrix} and that squared is also the same? Where am i going wrong?",,"['linear-algebra', 'matrices']"
19,Can I extract common factor from a column in matrix?,Can I extract common factor from a column in matrix?,,Can I do the following for matrices: $$ A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} == 2^{1/3}\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = 2^{1/3}B \quad ??$$ I know I can do the following for matrices: $$ If \quad A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad then \quad 2A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$ and following for determinants: $$ |A| = \begin{vmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} == 2\begin{vmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} = 2|B| \quad \checkmark\checkmark$$,Can I do the following for matrices: $$ A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} == 2^{1/3}\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = 2^{1/3}B \quad ??$$ I know I can do the following for matrices: $$ If \quad A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad then \quad 2A = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$ and following for determinants: $$ |A| = \begin{vmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} == 2\begin{vmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{vmatrix} = 2|B| \quad \checkmark\checkmark$$,,"['matrices', 'determinant']"
20,"$A, B$ are $3 \times 3$ matrices such that $(A - B)^2 = 0$. Prove that $\operatorname{Tr}(AB - BA)^3 = 0$.",are  matrices such that . Prove that .,"A, B 3 \times 3 (A - B)^2 = 0 \operatorname{Tr}(AB - BA)^3 = 0","I have been trying to solve this recent linear algebra problem : Let $A, B$ be $3 \times 3$ matrices such that $(A-B)^2 = 0$. Prove that $\det (AB - BA) = 0$. This was my approach:$\DeclareMathOperator{\Tr}{Tr}$ The following equality holds for any $3\times 3$ matrices $A, B$: $$\det (AB - BA) = \frac13 \Tr(AB - BA)^3$$ It follows from the Hamilton-Cayley theorem applied on $AB - BA$. Therefore, it suffices to prove that $\Tr(AB - BA)^3 = 0$. Expanding gives that $\Tr(AB - BA)^3$ is equal to: \begin{align} \Tr\left(\color{magenta}{ABABAB} - \color{blue}{ABABBA} - \color{purple}{ABBAAB} + \color{purple}{ABBABA} - \color{blue}{BAABAB} + \color{olive}{BAABBA} + \color{olive}{BABAAB} - \color{magenta}{BABABA}\right)\\ \end{align} where the same-colored terms are cyclic permutations of each other so have the same trace. So, $$\Tr(AB - BA)^3 = 2\Tr BAABBA - 2\Tr BAABAB = 2 \Tr BAAB(BA - AB)$$ I figured this was a good place to try to use the assumption $(A - B)^2 = 0$: $$0 = (A - B)^2 = A^2 + B^2 - AB - BA \implies A^2 = AB + BA - B^2$$ So we have: $$BAAB(BA - AB) = B(AB + BA - B^2)B(AB - BA) = $$ $$\color{OrangeRed}{BABBBA} + \color{green}{BBABBA} - BBBBBA - \color{green}{BABBAB} - \color{OrangeRed}{BBABAB} + BBBBAB$$ Again, the same-colored terms cancel out when taking the trace so: $$\Tr BAAB(BA - AB) = \Tr BBBB(AB - BA)$$ A possible development is: \begin{align}2 \Tr BAAB(BA - AB) &=  \Tr (AB-BA)(BBBB - BAAB)\\ &= \Tr (AB-BA)(BBBB - BAAB)\\ &= \Tr (AB-BA)B(B^2 - A^2)B \end{align} But using $A^2 + B^2 = AB + BA$ here again gives $\Tr (BA - AB)BAAB$, so nothing new. Is there a way to finish the proof?","I have been trying to solve this recent linear algebra problem : Let $A, B$ be $3 \times 3$ matrices such that $(A-B)^2 = 0$. Prove that $\det (AB - BA) = 0$. This was my approach:$\DeclareMathOperator{\Tr}{Tr}$ The following equality holds for any $3\times 3$ matrices $A, B$: $$\det (AB - BA) = \frac13 \Tr(AB - BA)^3$$ It follows from the Hamilton-Cayley theorem applied on $AB - BA$. Therefore, it suffices to prove that $\Tr(AB - BA)^3 = 0$. Expanding gives that $\Tr(AB - BA)^3$ is equal to: \begin{align} \Tr\left(\color{magenta}{ABABAB} - \color{blue}{ABABBA} - \color{purple}{ABBAAB} + \color{purple}{ABBABA} - \color{blue}{BAABAB} + \color{olive}{BAABBA} + \color{olive}{BABAAB} - \color{magenta}{BABABA}\right)\\ \end{align} where the same-colored terms are cyclic permutations of each other so have the same trace. So, $$\Tr(AB - BA)^3 = 2\Tr BAABBA - 2\Tr BAABAB = 2 \Tr BAAB(BA - AB)$$ I figured this was a good place to try to use the assumption $(A - B)^2 = 0$: $$0 = (A - B)^2 = A^2 + B^2 - AB - BA \implies A^2 = AB + BA - B^2$$ So we have: $$BAAB(BA - AB) = B(AB + BA - B^2)B(AB - BA) = $$ $$\color{OrangeRed}{BABBBA} + \color{green}{BBABBA} - BBBBBA - \color{green}{BABBAB} - \color{OrangeRed}{BBABAB} + BBBBAB$$ Again, the same-colored terms cancel out when taking the trace so: $$\Tr BAAB(BA - AB) = \Tr BBBB(AB - BA)$$ A possible development is: \begin{align}2 \Tr BAAB(BA - AB) &=  \Tr (AB-BA)(BBBB - BAAB)\\ &= \Tr (AB-BA)(BBBB - BAAB)\\ &= \Tr (AB-BA)B(B^2 - A^2)B \end{align} But using $A^2 + B^2 = AB + BA$ here again gives $\Tr (BA - AB)BAAB$, so nothing new. Is there a way to finish the proof?",,"['linear-algebra', 'matrices', 'trace']"
21,How to calculate a matrix $M$ by dividing 2 vectors?,How to calculate a matrix  by dividing 2 vectors?,M,"Let $u = \begin{bmatrix}a\\b\\c\end{bmatrix}$ Let $v = \begin{bmatrix}d\\e\\f\end{bmatrix}$ There exists a $3\times3$ matrix, $M$, such that: $Mu = v$ so $M = vu^{-1}$ But how do I go about calculating $vu^{-1}$? My guess is that $u^{-1}$ will be a row vector in order to make the multiplication work... But apparently you can't do inverses on non-square matrices.","Let $u = \begin{bmatrix}a\\b\\c\end{bmatrix}$ Let $v = \begin{bmatrix}d\\e\\f\end{bmatrix}$ There exists a $3\times3$ matrix, $M$, such that: $Mu = v$ so $M = vu^{-1}$ But how do I go about calculating $vu^{-1}$? My guess is that $u^{-1}$ will be a row vector in order to make the multiplication work... But apparently you can't do inverses on non-square matrices.",,"['linear-algebra', 'matrices']"
22,Eigenvalues of same-row matrices,Eigenvalues of same-row matrices,,"It has previously been discussed here that the eigenvalues of an all-ones $n \times n$ matrix $A$ such as the following are given by $0$ with multiplicity $n - 1$ and $n$ with multiplicity $1$, hence a total multiplicity of $n$ which means that the given matrix is diagonalizable. $$A = \begin{bmatrix} 1 & 1 & \cdots & 1 \\ 1 & 1 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 1 \\ \end{bmatrix} $$ I recently wrote an exam that asked us to diagonalize a matrix with multiple (3) rows that contained the same entries, so I was wondering if there was some general case to apply. Thus the question I am asking is given the following $n \times n$ matrix A, what are its eigenvalues? $$A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \\ a_1 & a_2 & \cdots & a_n \\ \vdots & \vdots & \ddots & \vdots \\ a_1 & a_2 & \cdots & a_n \\ \end{bmatrix} $$ For the sake of simplicity, lets first assume that $a_1, a_2, \ldots, a_n \in \mathbb{R} - \{0\}$; however, what happens if any (or all) are zero? It seems logical that there be the eigenvalue $0$ with $n - 1$ multiplicity since the rank of this matrix will be $1$ (assuming at least one nonzero entry), and that the other eigenvalue be the sum of entries on the diagonal by observation $a_1 + a_2 + \cdots + a_n$ with $1$ multiplicity. I could not, however, write a formal proof for that second statement.","It has previously been discussed here that the eigenvalues of an all-ones $n \times n$ matrix $A$ such as the following are given by $0$ with multiplicity $n - 1$ and $n$ with multiplicity $1$, hence a total multiplicity of $n$ which means that the given matrix is diagonalizable. $$A = \begin{bmatrix} 1 & 1 & \cdots & 1 \\ 1 & 1 & \cdots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \cdots & 1 \\ \end{bmatrix} $$ I recently wrote an exam that asked us to diagonalize a matrix with multiple (3) rows that contained the same entries, so I was wondering if there was some general case to apply. Thus the question I am asking is given the following $n \times n$ matrix A, what are its eigenvalues? $$A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \\ a_1 & a_2 & \cdots & a_n \\ \vdots & \vdots & \ddots & \vdots \\ a_1 & a_2 & \cdots & a_n \\ \end{bmatrix} $$ For the sake of simplicity, lets first assume that $a_1, a_2, \ldots, a_n \in \mathbb{R} - \{0\}$; however, what happens if any (or all) are zero? It seems logical that there be the eigenvalue $0$ with $n - 1$ multiplicity since the rank of this matrix will be $1$ (assuming at least one nonzero entry), and that the other eigenvalue be the sum of entries on the diagonal by observation $a_1 + a_2 + \cdots + a_n$ with $1$ multiplicity. I could not, however, write a formal proof for that second statement.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
23,Eigenvalues of $\ddot {y} +\dot{y}-6y = 0$,Eigenvalues of,\ddot {y} +\dot{y}-6y = 0,"I want to write $\ddot {y} +\dot{y}-6y = 0$ in the form $$\dot{x} = \cdots$$ $$\dot{y} = \cdots$$ then create a matrix from which I can calculate $\lambda_1,~\lambda_2$. But I keep getting the wrong eigenvalues if I let $x = \dot{y}$.","I want to write $\ddot {y} +\dot{y}-6y = 0$ in the form $$\dot{x} = \cdots$$ $$\dot{y} = \cdots$$ then create a matrix from which I can calculate $\lambda_1,~\lambda_2$. But I keep getting the wrong eigenvalues if I let $x = \dot{y}$.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
24,If $A$ & $B$ are $4\times 4$ matrices with $\det(A)=-5$ & $\det(B)=10$ then evaluate...,If  &  are  matrices with  &  then evaluate...,A B 4\times 4 \det(A)=-5 \det(B)=10,"If $A$ & $B$ are $4\times 4$ matrices with $\det(A)=-5 $ & $\det(B)=10$ then evaluate... a) $\det\left(A+\operatorname{adj}\left(A^{-1}\right)\right)$ b) $\det(A+B)$ Yes, those are meant to be addition signs. I wouldn't be asking if it were multiplication. ANS for a) is $-256/125.$","If $A$ & $B$ are $4\times 4$ matrices with $\det(A)=-5 $ & $\det(B)=10$ then evaluate... a) $\det\left(A+\operatorname{adj}\left(A^{-1}\right)\right)$ b) $\det(A+B)$ Yes, those are meant to be addition signs. I wouldn't be asking if it were multiplication. ANS for a) is $-256/125.$",,"['linear-algebra', 'matrices']"
25,Is it true that $A\geq B$ implies $B^{\dagger}\geq A^{\dagger}$ for singular positive semidefinite matrices?,Is it true that  implies  for singular positive semidefinite matrices?,A\geq B B^{\dagger}\geq A^{\dagger},"Here $A^{\dagger}$ is the Moore-Penrose pseudo-inverse and $\geq$ denotes the Loewner partial order for positive semidefinite matrices. I know that the statement is true for non-singular matrices, but cannot find whether this extends to singular matrices, and continuity arguments seem to not apply here. Any help will be greatly appreciated.","Here is the Moore-Penrose pseudo-inverse and denotes the Loewner partial order for positive semidefinite matrices. I know that the statement is true for non-singular matrices, but cannot find whether this extends to singular matrices, and continuity arguments seem to not apply here. Any help will be greatly appreciated.",A^{\dagger} \geq,"['matrices', 'order-theory', 'symmetric-matrices', 'positive-semidefinite', 'pseudoinverse']"
26,Why in these matrices are $AB=BA$ not equal? What is the logic behind them?,Why in these matrices are  not equal? What is the logic behind them?,AB=BA,"We know that in matrices AB=BA.Why in this Matrices $A=\begin{bmatrix} -1 & 3\\  2 & 0\end{bmatrix}$, $B=\begin{bmatrix} 1 & 2\\ -3 & -5\end{bmatrix}$ are not equal to $AB=BA$. WHY? This is matrix  of order $2\times 2$ for both $A$ and $B$.","We know that in matrices AB=BA.Why in this Matrices $A=\begin{bmatrix} -1 & 3\\  2 & 0\end{bmatrix}$, $B=\begin{bmatrix} 1 & 2\\ -3 & -5\end{bmatrix}$ are not equal to $AB=BA$. WHY? This is matrix  of order $2\times 2$ for both $A$ and $B$.",,"['linear-algebra', 'matrices', 'education', 'matrix-rank']"
27,YX - XY = X for nilpotent matrix,YX - XY = X for nilpotent matrix,,"Let $X$ be a matrix over $\Bbb C$, I have to show that exists a matrix $Y$ s.t. $YX - XY = X$ iff $X$ is nilpotent. What have I done? Given $Y$ exists, I have already shown that $tr(X^i)=0$ $\forall  i$ so $X$ is nilpotent. I miss the converse. Thank you","Let $X$ be a matrix over $\Bbb C$, I have to show that exists a matrix $Y$ s.t. $YX - XY = X$ iff $X$ is nilpotent. What have I done? Given $Y$ exists, I have already shown that $tr(X^i)=0$ $\forall  i$ so $X$ is nilpotent. I miss the converse. Thank you",,"['linear-algebra', 'matrices']"
28,Lipschitz continuity of $\sqrt{A}$,Lipschitz continuity of,\sqrt{A},"Let $U \subset\mathbb{R}^n$ be an open set, $\mathbb{S}^n$ be the set of all $n\times n$ symmetric real matrices, $A:U\to \mathbb{S}^n$ be a uniformly Lipschitz continuous function. Suppose $\exists \lambda>0$ s.t. $\forall x\in U, \xi\in\mathbb{R}^n$ $\ ^t\xi A(x)\xi\geq\lambda|\xi|^2$. Then we can define $A^{1/2}:U\to \mathbb{S}^n$, which is the nonnegative sqrt of A at each point. Then is $A^{1/2}$ also uniformly Lipschitz continuous? If it holds, how to prove it?","Let $U \subset\mathbb{R}^n$ be an open set, $\mathbb{S}^n$ be the set of all $n\times n$ symmetric real matrices, $A:U\to \mathbb{S}^n$ be a uniformly Lipschitz continuous function. Suppose $\exists \lambda>0$ s.t. $\forall x\in U, \xi\in\mathbb{R}^n$ $\ ^t\xi A(x)\xi\geq\lambda|\xi|^2$. Then we can define $A^{1/2}:U\to \mathbb{S}^n$, which is the nonnegative sqrt of A at each point. Then is $A^{1/2}$ also uniformly Lipschitz continuous? If it holds, how to prove it?",,"['calculus', 'linear-algebra', 'matrices']"
29,Matrix norm inequality proof - does this use Cauchy-Schwarz?,Matrix norm inequality proof - does this use Cauchy-Schwarz?,,"The matrix norm for $A : \mathbb{R}^n \rightarrow \mathbb{R}^m$ (so $A$ is an $m \times n$ matrix) is given by $$\|A\| = \sup_{X \in \mathbb{R}^n \setminus \{0\}} \frac{|AX|}{|X|}$$ where $| \cdot |$ is the Euclidean norm. I have been given the properties (without proof) that $|AX| \leq \|A\||X|$ and $\|AB\| \leq \|A\|\|B\|$, where $B$ is some $n \times p$ matrix. Would a proof of this use the Cauchy-Schwarz ineqaulity?","The matrix norm for $A : \mathbb{R}^n \rightarrow \mathbb{R}^m$ (so $A$ is an $m \times n$ matrix) is given by $$\|A\| = \sup_{X \in \mathbb{R}^n \setminus \{0\}} \frac{|AX|}{|X|}$$ where $| \cdot |$ is the Euclidean norm. I have been given the properties (without proof) that $|AX| \leq \|A\||X|$ and $\|AB\| \leq \|A\|\|B\|$, where $B$ is some $n \times p$ matrix. Would a proof of this use the Cauchy-Schwarz ineqaulity?",,"['matrices', 'inequality', 'normed-spaces', 'supremum-and-infimum']"
30,"If $A,B,A+I,A+B$ are idempotent matrices how to show that $AB=BA$?",If  are idempotent matrices how to show that ?,"A,B,A+I,A+B AB=BA","If $A,B,A+I,A+B$ are idempotent matrices how to show that $AB=BA$ ? MY ATTEMPT: $A\cdot A=A$ $B\cdot B=B$ $(A+I)\cdot (A+I)=A+I$ or, $A\cdot A+A\cdot I+I\cdot A+I\cdot I=A+I$ which implies $A\cdot I+I\cdot A=0$ (using above equations) $(A+B)\cdot (A+B)=A+B$ or,$A\cdot A+A\cdot B+B\cdot A+B\cdot B=A+B$ which implies $A\cdot B+B\cdot A=0$ What's next?","If $A,B,A+I,A+B$ are idempotent matrices how to show that $AB=BA$ ? MY ATTEMPT: $A\cdot A=A$ $B\cdot B=B$ $(A+I)\cdot (A+I)=A+I$ or, $A\cdot A+A\cdot I+I\cdot A+I\cdot I=A+I$ which implies $A\cdot I+I\cdot A=0$ (using above equations) $(A+B)\cdot (A+B)=A+B$ or,$A\cdot A+A\cdot B+B\cdot A+B\cdot B=A+B$ which implies $A\cdot B+B\cdot A=0$ What's next?",,['abstract-algebra']
31,Why are orthogonal projection matrices not ... orthogonal?,Why are orthogonal projection matrices not ... orthogonal?,,"I know that given an orthogonal matrix U, then orthogonal projection onto the column space of U is represented by the matrix $UU^t$, which is again orthogonal.  I've computed these types of matrices many times now. But when reading online sources such as Wolfram, they give examples of orthogonal projection matrices with a zero column or a zero row, and a couple of 1s - such as the well-known matrix that projects (x,y,z) to (x,y,0). But this matrix doesn't even  have full rank, let alone be unitary or orthogonal. Where's my conceptual mistake? Thanks,","I know that given an orthogonal matrix U, then orthogonal projection onto the column space of U is represented by the matrix $UU^t$, which is again orthogonal.  I've computed these types of matrices many times now. But when reading online sources such as Wolfram, they give examples of orthogonal projection matrices with a zero column or a zero row, and a couple of 1s - such as the well-known matrix that projects (x,y,z) to (x,y,0). But this matrix doesn't even  have full rank, let alone be unitary or orthogonal. Where's my conceptual mistake? Thanks,",,"['linear-algebra', 'matrices', 'projective-geometry', 'orthogonality']"
32,"If $A$ is similar to $B$, then $A$ is invertible $\implies$ B is invertible?","If  is similar to , then  is invertible  B is invertible?",A B A \implies,"I think that in order to achieve the invertibility, $\det(A)\ne 0$. But for similarity, we only have that $B=P^{-1}AP$ associates with. If we have $-1$ as power to both sides the equality would establish, but how do you prove the invertibility?","I think that in order to achieve the invertibility, $\det(A)\ne 0$. But for similarity, we only have that $B=P^{-1}AP$ associates with. If we have $-1$ as power to both sides the equality would establish, but how do you prove the invertibility?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
33,"Let A be a square matrix of order $n$ such that $A^{2}= I$. Prove that if $1$ is the only eigenvalue of $A$, then $A = I$.","Let A be a square matrix of order  such that . Prove that if  is the only eigenvalue of , then .",n A^{2}= I 1 A A = I,I have been trying to solve this for hours now and still have no idea. I have tried: a) substituting $\lambda = 1$ into $\lambda x - Ax = 0$. b) substituting $\lambda = 1$ into $\det(\lambda I-A)x = 0$. Both ways have proven ineffective. Any help would be appreciated.,I have been trying to solve this for hours now and still have no idea. I have tried: a) substituting $\lambda = 1$ into $\lambda x - Ax = 0$. b) substituting $\lambda = 1$ into $\det(\lambda I-A)x = 0$. Both ways have proven ineffective. Any help would be appreciated.,,"['linear-algebra', 'matrices']"
34,Matrix group isomorphic to $\mathbb Z$.,Matrix group isomorphic to .,\mathbb Z,"The set  $G=\left\{\begin{pmatrix}1 & n  \\         0 & 1  \\         \end{pmatrix}\mid  n\in \Bbb Z\right\}$ with the operation of matrix multiplication is a group. Show that $$\phi:\Bbb Z \to G,$$ $$\phi(n)=\begin{pmatrix}1 & n  \\         0 & 1  \\         \end{pmatrix}$$ is a group isomorphism (where the operation on $\Bbb Z$ is ordinary addtion). TO show it's isomorphism:  I know I must show one-to-one, onto and homomorphism. I've done these examples before but never with matrices. How can I show if $\phi(a)=\phi(b)$ then $a=b$? Same question for onto and operation preserving with matrices. Thank you!","The set  $G=\left\{\begin{pmatrix}1 & n  \\         0 & 1  \\         \end{pmatrix}\mid  n\in \Bbb Z\right\}$ with the operation of matrix multiplication is a group. Show that $$\phi:\Bbb Z \to G,$$ $$\phi(n)=\begin{pmatrix}1 & n  \\         0 & 1  \\         \end{pmatrix}$$ is a group isomorphism (where the operation on $\Bbb Z$ is ordinary addtion). TO show it's isomorphism:  I know I must show one-to-one, onto and homomorphism. I've done these examples before but never with matrices. How can I show if $\phi(a)=\phi(b)$ then $a=b$? Same question for onto and operation preserving with matrices. Thank you!",,"['abstract-algebra', 'matrices', 'group-theory']"
35,Why does Bondy's Theorem work?,Why does Bondy's Theorem work?,,"Bondy's theorem says that any $n$ by $n$ matrix with distinct rows can have one column removed from it to produce a new matrix where all the rows are still distinct.  In easy terms, how does he know this?","Bondy's theorem says that any $n$ by $n$ matrix with distinct rows can have one column removed from it to produce a new matrix where all the rows are still distinct.  In easy terms, how does he know this?",,"['linear-algebra', 'matrices', 'group-theory']"
36,Prove that if $A$ is a symmetric matric then $A^3$ and $A^2-2A+I$ are symmetric matrices.,Prove that if  is a symmetric matric then  and  are symmetric matrices.,A A^3 A^2-2A+I,"I am uncertain on how to approach this proof. For most everything I've encountered concerning symmetry, it has involved taking the transpose in order to show some property. Here, I'm not certain if and how that would be effective.","I am uncertain on how to approach this proof. For most everything I've encountered concerning symmetry, it has involved taking the transpose in order to show some property. Here, I'm not certain if and how that would be effective.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'symmetry', 'transpose']"
37,Picture of set of nilpotent $2 \times 2$ matrices over $\mathbb{R}$ [closed],Picture of set of nilpotent  matrices over  [closed],2 \times 2 \mathbb{R},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What does the set of nilpotent $2 \times 2$ matrices over $\mathbb{R}$ look like?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What does the set of nilpotent $2 \times 2$ matrices over $\mathbb{R}$ look like?",,['linear-algebra']
38,How can I divide by a matrix?,How can I divide by a matrix?,,"I can multiply a vector by a matrix like so: $$\begin{pmatrix}a\\ b \\ c\end{pmatrix}\cdot\begin{pmatrix}d & e & f \\ g & h & i \\ j & k & l \end{pmatrix} = \begin{pmatrix}  ad + be + cf \\ ag + bh + ci \\  aj + bk + al\end{pmatrix},$$ but how do I divide?  $$\begin{pmatrix} a \\ b\\c\end{pmatrix}/\begin{pmatrix}  d &e& f\\ g& h& i \\ j& k& l\end{pmatrix}=\,?$$ I've looked everywhere and simply can't find an answer.","I can multiply a vector by a matrix like so: $$\begin{pmatrix}a\\ b \\ c\end{pmatrix}\cdot\begin{pmatrix}d & e & f \\ g & h & i \\ j & k & l \end{pmatrix} = \begin{pmatrix}  ad + be + cf \\ ag + bh + ci \\  aj + bk + al\end{pmatrix},$$ but how do I divide?  $$\begin{pmatrix} a \\ b\\c\end{pmatrix}/\begin{pmatrix}  d &e& f\\ g& h& i \\ j& k& l\end{pmatrix}=\,?$$ I've looked everywhere and simply can't find an answer.",,['matrices']
39,"If for every $v$ there exists $w$ with $Aw = v$, then $A$ is invertible","If for every  there exists  with , then  is invertible",v w Aw = v A,"Let ${A}$ be a $2 \times 2$ matrix. For every two-dimensional vector ${v}$, there exists a two-dimensional vector ${w}$ such that   $Aw = v$.   Show that ${A}$ is invertible. What is the easiest way to do this? Thanks.","Let ${A}$ be a $2 \times 2$ matrix. For every two-dimensional vector ${v}$, there exists a two-dimensional vector ${w}$ such that   $Aw = v$.   Show that ${A}$ is invertible. What is the easiest way to do this? Thanks.",,"['linear-algebra', 'matrices']"
40,Property of 10x10 matrix with non negative eigenvalues,Property of 10x10 matrix with non negative eigenvalues,,"Let $A$ be a $10\times 10$ matrix with complex entries such that all its eigenvalues are non-negative real numbers, and at least one eigenvalue is positive.  Which of the following statements is always false ? A. There exists a matrix $B$ such that $AB - BA = B$. B. There exists a matrix $B$ such that $AB - BA = A$. C. There exists a matrix $B$ such that $AB + BA = A$. D. There exists a matrix $B$ such that $AB + BA = B$. I just need a hint to start thinking.","Let $A$ be a $10\times 10$ matrix with complex entries such that all its eigenvalues are non-negative real numbers, and at least one eigenvalue is positive.  Which of the following statements is always false ? A. There exists a matrix $B$ such that $AB - BA = B$. B. There exists a matrix $B$ such that $AB - BA = A$. C. There exists a matrix $B$ such that $AB + BA = A$. D. There exists a matrix $B$ such that $AB + BA = B$. I just need a hint to start thinking.",,"['linear-algebra', 'matrices']"
41,problem of a positive definite matrix,problem of a positive definite matrix,,"Let $H$ be a positive definite matrix and $I$ the identity matrix. If $k_1,k_2>0$, can we conclude that $k_1H-k_2I$ is positive definite if $k_1\gg k_2$?","Let $H$ be a positive definite matrix and $I$ the identity matrix. If $k_1,k_2>0$, can we conclude that $k_1H-k_2I$ is positive definite if $k_1\gg k_2$?",,"['linear-algebra', 'matrices', 'matrix-equations', 'positive-characteristic']"
42,Do we write a metric tensor as a matrix?,Do we write a metric tensor as a matrix?,,"The metric tensor is an (0,2) tensor that is denoted by $g_{\mu\nu}$ in general relativity. I often see people write the metric field in matrix form like  \begin{equation} g_{\mu\nu} =  \begin{bmatrix}  g_{11} & g_{12} & g_{13} & g_{14} \\ g_{21} & g_{22} & g_{23} & g_{24} \\ g_{31} & g_{32} & g_{33} & g_{34} \\ g_{41} & g_{42} & g_{43} & g_{44} \\ \end{bmatrix}  \end{equation}  But I thought all matrices are 2-tensors but not vice versa based on this thread https://physics.stackexchange.com/questions/20437/are-matrices-and-second-rank-tensors-the-same-thing Can someone explain if the above metric tensor is an appropriate way to write the metric tensor? How do we know a metric tensor can be written as a matrix? Note: I assume if $g_{\mu\nu}$ is written correctly, then I can write the Ricci tensor as \begin{equation} R_{ab} =  \begin{bmatrix}  R_{11} & R_{12} & R_{13} & R_{14} \\ R_{21} & R_{22} & R_{23} & R_{24} \\ R_{31} & R_{32} & R_{33} & R_{34} \\ R_{41} & R_{42} & R_{43} & R_{44} \\ \end{bmatrix}  \end{equation}","The metric tensor is an (0,2) tensor that is denoted by $g_{\mu\nu}$ in general relativity. I often see people write the metric field in matrix form like  \begin{equation} g_{\mu\nu} =  \begin{bmatrix}  g_{11} & g_{12} & g_{13} & g_{14} \\ g_{21} & g_{22} & g_{23} & g_{24} \\ g_{31} & g_{32} & g_{33} & g_{34} \\ g_{41} & g_{42} & g_{43} & g_{44} \\ \end{bmatrix}  \end{equation}  But I thought all matrices are 2-tensors but not vice versa based on this thread https://physics.stackexchange.com/questions/20437/are-matrices-and-second-rank-tensors-the-same-thing Can someone explain if the above metric tensor is an appropriate way to write the metric tensor? How do we know a metric tensor can be written as a matrix? Note: I assume if $g_{\mu\nu}$ is written correctly, then I can write the Ricci tensor as \begin{equation} R_{ab} =  \begin{bmatrix}  R_{11} & R_{12} & R_{13} & R_{14} \\ R_{21} & R_{22} & R_{23} & R_{24} \\ R_{31} & R_{32} & R_{33} & R_{34} \\ R_{41} & R_{42} & R_{43} & R_{44} \\ \end{bmatrix}  \end{equation}",,"['matrices', 'riemannian-geometry', 'tensors']"
43,If $A$ is a symmetric and positive definite matrix then $\text{tr}(A)^n\geq n^n\det A$,If  is a symmetric and positive definite matrix then,A \text{tr}(A)^n\geq n^n\det A,If $A$ is an $n \times n$ symmetric and positive definite matrix then is the following relation $\text{tr}(A)^n\geq n^n\det A$ true? How to show this?,If $A$ is an $n \times n$ symmetric and positive definite matrix then is the following relation $\text{tr}(A)^n\geq n^n\det A$ true? How to show this?,,"['linear-algebra', 'matrices']"
44,$A$ be $n \times n$ Hermitian matrix with $A^5+A^3+A=3I_n$ $implies$ $A=I_n$ ?,be  Hermitian matrix with    ?,A n \times n A^5+A^3+A=3I_n implies A=I_n,"Let $A$ be a Hermitian matrix of size $n$ such that $A^5+A^3+A=3I_n$ , then is it true that $A=I_n$ ? What I got is if $a$ is an eigenvalue then $a^5+a^3+a-3=0=(a-1)(a^4+a^3+2a^2+2a+3)$ this doesn't seem to get anywhere , Please help .","Let $A$ be a Hermitian matrix of size $n$ such that $A^5+A^3+A=3I_n$ , then is it true that $A=I_n$ ? What I got is if $a$ is an eigenvalue then $a^5+a^3+a-3=0=(a-1)(a^4+a^3+2a^2+2a+3)$ this doesn't seem to get anywhere , Please help .",,['linear-algebra']
45,Possible eigenvalues of a matrix $AB$,Possible eigenvalues of a matrix,AB,"Let matrices $A$, $B\in{M_2}(\mathbb{R})$, such that $A^2=B^2=I$, where $I$ is identity matrix. Why can be numbers $3+2\sqrt2$ and $3-2\sqrt2$  eigenvalues for the Matrix $AB$? Can  be numbers $2,1/2$  the eigenvalues of matrix $AB$?","Let matrices $A$, $B\in{M_2}(\mathbb{R})$, such that $A^2=B^2=I$, where $I$ is identity matrix. Why can be numbers $3+2\sqrt2$ and $3-2\sqrt2$  eigenvalues for the Matrix $AB$? Can  be numbers $2,1/2$  the eigenvalues of matrix $AB$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
46,Expressing a rank-$k$ matrix as a sum of $k$ rank-$1$ matrices,Expressing a rank- matrix as a sum of  rank- matrices,k k 1,"It's a well known fact that for any matrix $A \in \Bbb R^{m \times n}$ which is the sum of $k$ matrices $A_1,\dots,A_k\in \Bbb R^{m \times n}$ of rank $1$ , it holds that $\operatorname{rank}(A) \le k$ . Does it hold that for any matrix $A \in \Bbb R^{m \times n}$ of rank $k$ there exist a set of matrices $A_1, \dots ,A_k \in \Bbb R^{m \times n}$ of rank $1$ such that $A = A_1 + \cdots + A_k$ ? It may seem as a direct consequence of the aforementioned proposition, but it doesn't seem that obvious, at least for me. Can someone guide me to a typical proof of it?","It's a well known fact that for any matrix which is the sum of matrices of rank , it holds that . Does it hold that for any matrix of rank there exist a set of matrices of rank such that ? It may seem as a direct consequence of the aforementioned proposition, but it doesn't seem that obvious, at least for me. Can someone guide me to a typical proof of it?","A \in \Bbb R^{m \times n} k A_1,\dots,A_k\in \Bbb R^{m \times n} 1 \operatorname{rank}(A) \le k A \in \Bbb R^{m \times n} k A_1, \dots ,A_k \in \Bbb R^{m \times n} 1 A = A_1 + \cdots + A_k","['linear-algebra', 'matrices', 'matrix-rank', 'rank-1-matrices']"
47,power series for square root matrix,power series for square root matrix,,"Suppose I have a matrix of the form $$U\ =\ (I+z\thinspace X)^{\frac{1}{2}}$$ where $I$ is the $n\times n$ identity matrix,  $z\in\mathbb{C}$ and $X$ is a $n\times n$ arbitrary complex matrix with entries following $|X_{ij}|\le1$.  If $z$ has a small modulus ($|z|\ll1$), am I allowed to expand in a power series the square root matrix expression as $$U\ =\ I \ +\  \frac{z}{2}X\ +\ ...\thinspace ,$$ or is there some monkey business?","Suppose I have a matrix of the form $$U\ =\ (I+z\thinspace X)^{\frac{1}{2}}$$ where $I$ is the $n\times n$ identity matrix,  $z\in\mathbb{C}$ and $X$ is a $n\times n$ arbitrary complex matrix with entries following $|X_{ij}|\le1$.  If $z$ has a small modulus ($|z|\ll1$), am I allowed to expand in a power series the square root matrix expression as $$U\ =\ I \ +\  \frac{z}{2}X\ +\ ...\thinspace ,$$ or is there some monkey business?",,"['linear-algebra', 'matrices', 'power-series']"
48,Skew-symmetric matrix and exp function $e^A$,Skew-symmetric matrix and exp function,e^A,Let $A_{nXn}(\mathbb{R})$ Skew-symmetric matrix $A=-A^t$ prove that $e^A(e^A)^t=I$ while: $e^A=\sum_{i=0}^{\infty} \frac{A^n}{n!}$ I tried this: $A=-A^t \Rightarrow A$ is Diagonalizable with orthogonal basis over   $\mathbb{C}$ $\Rightarrow A=PDP^*$and $P^*=P^{-1}$  and D is:  \begin{pmatrix}   \lambda_1 &0 & \cdots & 0 \\   0 & \lambda_2 & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\  0 & 0 & \cdots & \lambda_n  \end{pmatrix} we know that :$\lambda_i=0$ or $\lambda_i=ib$ for $b \in R$  because A is Skew-symmetric matrix. so: $e^A=Pe^DP^t$ so if I will prove that $\lambda_i=0$ we will get $e^A=PP^t=I$. Is it possible?,Let $A_{nXn}(\mathbb{R})$ Skew-symmetric matrix $A=-A^t$ prove that $e^A(e^A)^t=I$ while: $e^A=\sum_{i=0}^{\infty} \frac{A^n}{n!}$ I tried this: $A=-A^t \Rightarrow A$ is Diagonalizable with orthogonal basis over   $\mathbb{C}$ $\Rightarrow A=PDP^*$and $P^*=P^{-1}$  and D is:  \begin{pmatrix}   \lambda_1 &0 & \cdots & 0 \\   0 & \lambda_2 & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\  0 & 0 & \cdots & \lambda_n  \end{pmatrix} we know that :$\lambda_i=0$ or $\lambda_i=ib$ for $b \in R$  because A is Skew-symmetric matrix. so: $e^A=Pe^DP^t$ so if I will prove that $\lambda_i=0$ we will get $e^A=PP^t=I$. Is it possible?,,"['linear-algebra', 'matrices', 'exponential-function']"
49,Matrices whose product is identity but do not commute.,Matrices whose product is identity but do not commute.,,"I'm supposed find two matrices $A$ and $B$ whose product $AB=I_2$, but $BA\neq I$. But I'm not sure if this is even possible since if $AB=I$, doesn't that mean that $B$ is the inverse matrix of $A$ and that leads to $BA=I$ automatically. Any help is appreciated.","I'm supposed find two matrices $A$ and $B$ whose product $AB=I_2$, but $BA\neq I$. But I'm not sure if this is even possible since if $AB=I$, doesn't that mean that $B$ is the inverse matrix of $A$ and that leads to $BA=I$ automatically. Any help is appreciated.",,"['linear-algebra', 'matrices']"
50,When is a companion matrix diagonalizable and what does this say about the associated field extension?,When is a companion matrix diagonalizable and what does this say about the associated field extension?,,"Consider the $n\times n$ matrix $$ M=\begin{pmatrix} 0 & 0 & 0 & \cdots & 0 & 0 & -c_0\\ 1 & 0 & 0 & \cdots & 0 & 0 & -c_1\\ 0 & 1 & 0 & \cdots & 0 & 0 & -c_2\\ \vdots & \vdots & \vdots & \ddots & \vdots &\vdots & \vdots\\ 0 & 0 & 0 & \cdots & 1 & 0 & -c_{n-2}\\ 0 & 0 & 0 & \cdots & 0 & 1 & -c_{n-1}\\ \end{pmatrix} $$ with rational entries. It is called the companion matrix of its charateristic polynomial $p_M$ which equals its minimal polynomial. Assume that $p_M$ is irreducible over $\mathbb{Q}$. By considering $p_M$ over the extension $\mathbb{C}\supset\mathbb{Q}$, it splits as $$ p_m(x)=(x-a_1)^{e_1}\cdots (x-a_k)^{e_k}, $$ but according to Wikipedia , $M$ does not have to be diagonalizable in general. What is an example of an $M$ (such that $p_M$ is irreducible over $\mathbb{Q}$) that is not diagonalizable over the complex numbers? As $p_M$ is irreducible over $\mathbb{Q}$, the matrix $M$ defines an algebraic field extension $L=\mathbb{Q}[x]/(p_M)$ over $\mathbb{Q}$ of degree $n$. How is the property that $M$ is diagonalizable over $\mathbb{C}$ reflected in properties of the field extension? Edit Sorry, I want to assume that $p_M$ is irreducible over $\mathbb{Q}$, of course.","Consider the $n\times n$ matrix $$ M=\begin{pmatrix} 0 & 0 & 0 & \cdots & 0 & 0 & -c_0\\ 1 & 0 & 0 & \cdots & 0 & 0 & -c_1\\ 0 & 1 & 0 & \cdots & 0 & 0 & -c_2\\ \vdots & \vdots & \vdots & \ddots & \vdots &\vdots & \vdots\\ 0 & 0 & 0 & \cdots & 1 & 0 & -c_{n-2}\\ 0 & 0 & 0 & \cdots & 0 & 1 & -c_{n-1}\\ \end{pmatrix} $$ with rational entries. It is called the companion matrix of its charateristic polynomial $p_M$ which equals its minimal polynomial. Assume that $p_M$ is irreducible over $\mathbb{Q}$. By considering $p_M$ over the extension $\mathbb{C}\supset\mathbb{Q}$, it splits as $$ p_m(x)=(x-a_1)^{e_1}\cdots (x-a_k)^{e_k}, $$ but according to Wikipedia , $M$ does not have to be diagonalizable in general. What is an example of an $M$ (such that $p_M$ is irreducible over $\mathbb{Q}$) that is not diagonalizable over the complex numbers? As $p_M$ is irreducible over $\mathbb{Q}$, the matrix $M$ defines an algebraic field extension $L=\mathbb{Q}[x]/(p_M)$ over $\mathbb{Q}$ of degree $n$. How is the property that $M$ is diagonalizable over $\mathbb{C}$ reflected in properties of the field extension? Edit Sorry, I want to assume that $p_M$ is irreducible over $\mathbb{Q}$, of course.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory', 'companion-matrices']"
51,Compute $ \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2})$,Compute, \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2}),"Let $B(x) = \begin{pmatrix} 1 & x \\x & 1 \end{pmatrix}$ , and $2=p_1<p_2<\cdots <p_n <\cdots$ primes number. Compute $$\displaystyle \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2})$$ I am sorry to post this but it's the first time I see an exercice about the convergence of infinite product of matrices. I do not know how can I deal. Can someone enlighten me ?","Let , and primes number. Compute I am sorry to post this but it's the first time I see an exercice about the convergence of infinite product of matrices. I do not know how can I deal. Can someone enlighten me ?",B(x) = \begin{pmatrix} 1 & x \\x & 1 \end{pmatrix} 2=p_1<p_2<\cdots <p_n <\cdots \displaystyle \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2}),"['sequences-and-series', 'matrices']"
52,Normal but not hermitian nor unitary,Normal but not hermitian nor unitary,,"I have to find out a normal transformation that is neither hermitian nor unitary. http://en.wikipedia.org/wiki/Normal_matrix gives me the answer. However, I would like to know how to find it out mathematically, not just guess and test. Can I go from the properties of eigenvalues of hermitian and unitary to get the answer? For example: eigenvalues of a hemitian must be real, then I choose (i,-i,0) as eigenvalues of the required matrix. Those eigenvalues satisfy the condition that the required matrix is not unitary whose eigenvalues are |1|. From those, I have characteristic equation has the form of x(x^2+1)= 0 which leads to the form of the required matrix has the trace =0 and det = 0.  Can I do it?","I have to find out a normal transformation that is neither hermitian nor unitary. http://en.wikipedia.org/wiki/Normal_matrix gives me the answer. However, I would like to know how to find it out mathematically, not just guess and test. Can I go from the properties of eigenvalues of hermitian and unitary to get the answer? For example: eigenvalues of a hemitian must be real, then I choose (i,-i,0) as eigenvalues of the required matrix. Those eigenvalues satisfy the condition that the required matrix is not unitary whose eigenvalues are |1|. From those, I have characteristic equation has the form of x(x^2+1)= 0 which leads to the form of the required matrix has the trace =0 and det = 0.  Can I do it?",,"['linear-algebra', 'matrices']"
53,Matrix derivatives,Matrix derivatives,,"Can someone please help me with this problem? ?I've already searched for similar examples in some linear algebra textbooks, but I couldn't find any... Thanks a lot! where $x\in \Bbb R^{n\times 1}$","Can someone please help me with this problem? ?I've already searched for similar examples in some linear algebra textbooks, but I couldn't find any... Thanks a lot! where $x\in \Bbb R^{n\times 1}$",,"['matrices', 'derivatives', 'matrix-calculus']"
54,"Prove that for any square matrix, an invertible matrix B exists, so that BA is triangular","Prove that for any square matrix, an invertible matrix B exists, so that BA is triangular",,"I'm given a matrix A , its dimensions are n x n . I am required to prove that an invertible matrix B exists, such that the product of the matrices BA is triangular. Any help?","I'm given a matrix A , its dimensions are n x n . I am required to prove that an invertible matrix B exists, such that the product of the matrices BA is triangular. Any help?",,"['linear-algebra', 'matrices']"
55,Intuitive understanding of determinants?,Intuitive understanding of determinants?,,"For a $n \times n$ matrix $A$ : $$\det (A) = \sum^{n}_{i=1}a_{1i}C_{1i}$$ where $C$ is the cofactor of $a_{1i}$ . If the determinant is $0$ , the matrix is not invertible. Could someone an intuitive explanation of why a zero determinant means non-invertibility? I'm not looking for a proof, the book gives me one. I'm looking for intuition. For example, consider the following properties of zero: Property 1: If a factor of a polynomial is $0$ at $x$ , then the polynomial is zero at $x$ . Intuition: Anything times zero, so if the remaining polynomial is being multiplied by zero, it has to be zero. Property: $x + 0 = x$ Intuition: If you have something and don't change anything about it, it remains the same. So why does having a zero determinant imply non-invertibility? Could someone give me a similar intuition for determinants? Thanks.","For a matrix : where is the cofactor of . If the determinant is , the matrix is not invertible. Could someone an intuitive explanation of why a zero determinant means non-invertibility? I'm not looking for a proof, the book gives me one. I'm looking for intuition. For example, consider the following properties of zero: Property 1: If a factor of a polynomial is at , then the polynomial is zero at . Intuition: Anything times zero, so if the remaining polynomial is being multiplied by zero, it has to be zero. Property: Intuition: If you have something and don't change anything about it, it remains the same. So why does having a zero determinant imply non-invertibility? Could someone give me a similar intuition for determinants? Thanks.",n \times n A \det (A) = \sum^{n}_{i=1}a_{1i}C_{1i} C a_{1i} 0 0 x x x + 0 = x,"['linear-algebra', 'matrices', 'determinant']"
56,Do all diagonalisable matrices have orthogonal eigenvectors?,Do all diagonalisable matrices have orthogonal eigenvectors?,,"So. Let us label right-eigenvectors of $M$ as $|\lambda\rangle$ and left-eigenvectors of $M$ as $\langle\lambda|$. That is, \begin{gather} M |\lambda\rangle = \lambda |\lambda\rangle \text{ and } \langle\lambda| M = \langle\lambda| \lambda . \end{gather} This makes sense (I hazard) because a left-eigenvector is adjoint, which doesn't have to mean ‘conjugate transpose’, to a right-eigenvector via a shared eigenvalue. Now, we may write \begin{gather} M|\lambda\rangle = \lambda |\lambda\rangle \text{ and } \langle\mu| M = \langle\mu| \mu , \end{gather} which implies that both \begin{gather} \langle \mu | M | \lambda \rangle = \lambda \langle \mu | \lambda \rangle \text{ and } \langle \mu | M | \lambda \rangle = \mu \langle \mu | \lambda \rangle \end{gather} which immediately implies, upon the assumption that \begin{gather} \lambda \neq \mu , \end{gather} that \begin{gather} \langle\mu|\lambda\rangle = 0 . \end{gather} What have I done wrong?","So. Let us label right-eigenvectors of $M$ as $|\lambda\rangle$ and left-eigenvectors of $M$ as $\langle\lambda|$. That is, \begin{gather} M |\lambda\rangle = \lambda |\lambda\rangle \text{ and } \langle\lambda| M = \langle\lambda| \lambda . \end{gather} This makes sense (I hazard) because a left-eigenvector is adjoint, which doesn't have to mean ‘conjugate transpose’, to a right-eigenvector via a shared eigenvalue. Now, we may write \begin{gather} M|\lambda\rangle = \lambda |\lambda\rangle \text{ and } \langle\mu| M = \langle\mu| \mu , \end{gather} which implies that both \begin{gather} \langle \mu | M | \lambda \rangle = \lambda \langle \mu | \lambda \rangle \text{ and } \langle \mu | M | \lambda \rangle = \mu \langle \mu | \lambda \rangle \end{gather} which immediately implies, upon the assumption that \begin{gather} \lambda \neq \mu , \end{gather} that \begin{gather} \langle\mu|\lambda\rangle = 0 . \end{gather} What have I done wrong?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
57,"Sum of each row is $m$, prove that $m$ divides the determinant","Sum of each row is , prove that  divides the determinant",m m,"This is a question from our reviewer for our exam for linear algebra. I just want to have some ideas how to tackle the problem. If $A$ is an $n\times n$ matrix with integer coefficients, such that the sum of each row's elements is equal to $m$, show that $m$ divides the determinant.","This is a question from our reviewer for our exam for linear algebra. I just want to have some ideas how to tackle the problem. If $A$ is an $n\times n$ matrix with integer coefficients, such that the sum of each row's elements is equal to $m$, show that $m$ divides the determinant.",,"['linear-algebra', 'matrices']"
58,"Given a vector equation with $n$ vectors, how can we determine if the span of the vectors is equal to $\mathbb{R}^n$","Given a vector equation with  vectors, how can we determine if the span of the vectors is equal to",n \mathbb{R}^n,"So I've recently started taking Linear Algebra, and I've been thinking about how to determine if the linear combinations of any n vectors can represent any vector in $\mathbb{R^n}$. More formally put, I am looking for a way to determine if, for n vectors $\vec{v_{1}}, \vec{v_{2}}, \vec{v_{3}}$... $\vec{v_n}$ have the property that $\mathrm{Span}(\vec{v_{1}}, \vec{v_{2}}, \vec{v_{3}}$... $\vec{v_{n}})$ = $\mathbb{R^n}$. So I started to approach this problem by looking at the basic unit vectors $\vec{i}, \vec{j}, \vec{k}$. I know that any vector in $\mathbb{R^3}$ can be represented by the span of these three vectors alone (it is quite intuitive, take some scalar times the first to get the first term of the vector, some scalar times the second, etc...). If we write out the augmented matrix for these three vectors and some vector $\vec{v}$ with elements $a, b, c$ we see the following: \begin{bmatrix} 1 &  0&0& a \\  0 &  1& 0& b\\  0 &  0& 1& c \end{bmatrix} The first weight for the linear combinations is strictly set to be $a$, the second weight is $b$, and the third is $c$. Since, for any vector, these three vectors $i, j, k$ will always have solutions for the weights, namely a, b, and c. So, for any vector to be in the span of n vectors, we need a specific solution for each of the weights. (no free variables) In order to accomplish this, we would need the reduced row echelon form for any n vectors to be of the above ""triangular"" form, with $1's$ going down the diagonal. Is the above statement correct, or are there other cases in which this is true? Thanks!","So I've recently started taking Linear Algebra, and I've been thinking about how to determine if the linear combinations of any n vectors can represent any vector in $\mathbb{R^n}$. More formally put, I am looking for a way to determine if, for n vectors $\vec{v_{1}}, \vec{v_{2}}, \vec{v_{3}}$... $\vec{v_n}$ have the property that $\mathrm{Span}(\vec{v_{1}}, \vec{v_{2}}, \vec{v_{3}}$... $\vec{v_{n}})$ = $\mathbb{R^n}$. So I started to approach this problem by looking at the basic unit vectors $\vec{i}, \vec{j}, \vec{k}$. I know that any vector in $\mathbb{R^3}$ can be represented by the span of these three vectors alone (it is quite intuitive, take some scalar times the first to get the first term of the vector, some scalar times the second, etc...). If we write out the augmented matrix for these three vectors and some vector $\vec{v}$ with elements $a, b, c$ we see the following: \begin{bmatrix} 1 &  0&0& a \\  0 &  1& 0& b\\  0 &  0& 1& c \end{bmatrix} The first weight for the linear combinations is strictly set to be $a$, the second weight is $b$, and the third is $c$. Since, for any vector, these three vectors $i, j, k$ will always have solutions for the weights, namely a, b, and c. So, for any vector to be in the span of n vectors, we need a specific solution for each of the weights. (no free variables) In order to accomplish this, we would need the reduced row echelon form for any n vectors to be of the above ""triangular"" form, with $1's$ going down the diagonal. Is the above statement correct, or are there other cases in which this is true? Thanks!",,"['linear-algebra', 'matrices', 'vector-spaces']"
59,another way to find inverse matrix,another way to find inverse matrix,,"The most common way to find inverse matrix is $M^{-1}=\frac1{\det(M)}\mathrm{adj}(M)$. However it is very trouble to find when the matrix is large. I found a very interesting way to get inverse matrix and I want to know why it can be done like this. For example if you want to find the inverse of $$M=\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}$$ First, write an identity matrix on the right hand side and carry out some steps: $$\begin{bmatrix}1 & 2 &1 &0 \\ 3 & 4&0&1\end{bmatrix}\to\begin{bmatrix}1 & 2 &1 &0 \\ 3/2 & 2&0&1/2\end{bmatrix}\to\begin{bmatrix}1/2 & 0 &-1 &1/2 \\ 3/2 & 2&0&1/2\end{bmatrix}\to\begin{bmatrix}3/2 & 0 &-3 &3/2 \\ 3/2 & 2&0&1/2\end{bmatrix}$$ $$\to\begin{bmatrix}3/2 & 0 &-3 &3/2 \\ 0 & 2&3&-1\end{bmatrix}\to\begin{bmatrix}1 & 0 &-2 &1 \\ 0 & 2&3&-1\end{bmatrix}\to\begin{bmatrix}1 & 0 &-2 &1 \\ 0 & 1&3/2&-1/2\end{bmatrix}$$ You can 1. swap any two row of the matrix 2. multiply a constant in any row 3. add one row to the other row. Just like you are doing Gaussian elimination. when the identical matrix shift to the left, the right hand side become $$M^{-1}=\begin{bmatrix}-2 &1 \\3/2&-1/2\end{bmatrix}$$ How to prove this method work?","The most common way to find inverse matrix is $M^{-1}=\frac1{\det(M)}\mathrm{adj}(M)$. However it is very trouble to find when the matrix is large. I found a very interesting way to get inverse matrix and I want to know why it can be done like this. For example if you want to find the inverse of $$M=\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}$$ First, write an identity matrix on the right hand side and carry out some steps: $$\begin{bmatrix}1 & 2 &1 &0 \\ 3 & 4&0&1\end{bmatrix}\to\begin{bmatrix}1 & 2 &1 &0 \\ 3/2 & 2&0&1/2\end{bmatrix}\to\begin{bmatrix}1/2 & 0 &-1 &1/2 \\ 3/2 & 2&0&1/2\end{bmatrix}\to\begin{bmatrix}3/2 & 0 &-3 &3/2 \\ 3/2 & 2&0&1/2\end{bmatrix}$$ $$\to\begin{bmatrix}3/2 & 0 &-3 &3/2 \\ 0 & 2&3&-1\end{bmatrix}\to\begin{bmatrix}1 & 0 &-2 &1 \\ 0 & 2&3&-1\end{bmatrix}\to\begin{bmatrix}1 & 0 &-2 &1 \\ 0 & 1&3/2&-1/2\end{bmatrix}$$ You can 1. swap any two row of the matrix 2. multiply a constant in any row 3. add one row to the other row. Just like you are doing Gaussian elimination. when the identical matrix shift to the left, the right hand side become $$M^{-1}=\begin{bmatrix}-2 &1 \\3/2&-1/2\end{bmatrix}$$ How to prove this method work?",,"['linear-algebra', 'matrices']"
60,How to prove that normal matrix with property $A^2=A$ is Hermitian?,How to prove that normal matrix with property  is Hermitian?,A^2=A,"I am given a matrix $A\in M(n\times n, \mathbb{C})$ normal (in matrix form $AA^*=A^*A$) and $A^2=A$. The task is to prove that the matrix is Hermitian. But when I try something like $A^*=\,\,...$ , then I can't reach $A$, because I can't ""get rid of star"" in expression. Also it is not enough to show $BA=BA^*$ for some $B$ since matrix don't form a field, and I haven't got any other thoughts. Thanks in advance!","I am given a matrix $A\in M(n\times n, \mathbb{C})$ normal (in matrix form $AA^*=A^*A$) and $A^2=A$. The task is to prove that the matrix is Hermitian. But when I try something like $A^*=\,\,...$ , then I can't reach $A$, because I can't ""get rid of star"" in expression. Also it is not enough to show $BA=BA^*$ for some $B$ since matrix don't form a field, and I haven't got any other thoughts. Thanks in advance!",,['matrices']
61,Is there a good intuitive way to understand why matrix B is inverse of A when matrix A|I is turned into I|B,Is there a good intuitive way to understand why matrix B is inverse of A when matrix A|I is turned into I|B,,"I'm looking for some help with my intuition of basic matrix operations, specifically finding a matrix's inverse (as per my subject line). I have no problems with the steps. The basic row operations are relatively simple. I'd like to understand why/ how this solves the system of linear equations. I know my question is asking more (or arguably less) than a concrete sequence of steps, a theorem, etc. But I think someone who understands linear algebra much better than I can get through to me better than my texts' treatment, which is little more than a worked example. Thanks in advance..","I'm looking for some help with my intuition of basic matrix operations, specifically finding a matrix's inverse (as per my subject line). I have no problems with the steps. The basic row operations are relatively simple. I'd like to understand why/ how this solves the system of linear equations. I know my question is asking more (or arguably less) than a concrete sequence of steps, a theorem, etc. But I think someone who understands linear algebra much better than I can get through to me better than my texts' treatment, which is little more than a worked example. Thanks in advance..",,"['linear-algebra', 'matrices', 'inverse']"
62,Is the determinant of a matrix preserved under permutations of the rows/columns of a matrix?,Is the determinant of a matrix preserved under permutations of the rows/columns of a matrix?,,"Is the determinant of a matrix preserved under permutations of the rows/columns of the matrix? If not, is its absolute value preserved?","Is the determinant of a matrix preserved under permutations of the rows/columns of the matrix? If not, is its absolute value preserved?",,"['linear-algebra', 'matrices']"
63,Finding eigenvalues of matrix of matrix.,Finding eigenvalues of matrix of matrix.,,"Let A be 4 X 4 matrix with eigenvalues -5, -2, 1, 4. Which of the following option is an eigenvalue of $\begin{bmatrix}A & I\\I & A\end{bmatrix}$, where I is 4 X 4 identity matrix? options: (A) -5  (B) -7 (C) 2 (D) 1","Let A be 4 X 4 matrix with eigenvalues -5, -2, 1, 4. Which of the following option is an eigenvalue of $\begin{bmatrix}A & I\\I & A\end{bmatrix}$, where I is 4 X 4 identity matrix? options: (A) -5  (B) -7 (C) 2 (D) 1",,"['matrices', 'eigenvalues-eigenvectors']"
64,Show matrix $A+5B$ has an inverse with integer entries given the following conditions,Show matrix  has an inverse with integer entries given the following conditions,A+5B,"Let $A$ and $B$ be 2×2 matrices with integer entries such that each of $A$, $A + B$, $A + 2B$, $A + 3B$, $A + 4B$ has an inverse with integer entries. Show that the same is true for $A + 5B$.","Let $A$ and $B$ be 2×2 matrices with integer entries such that each of $A$, $A + B$, $A + 2B$, $A + 3B$, $A + 4B$ has an inverse with integer entries. Show that the same is true for $A + 5B$.",,"['linear-algebra', 'matrices', 'inverse']"
65,Chain rule and inverse in matrix calculus,Chain rule and inverse in matrix calculus,,"I am having trouble understanding the derivation of some seemingly simple matrix derivatives and am wondering if there is an intuitive (perhaps geometric) explanation.  I am reasonably well-versed in multivariate calculus and linear algebra, but am not comfortable with tensor math. The function I am interested in is $f(t)=\mathbf{B}^T(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{A}$, where $t$ is a scalar, and $\mathbf{A},\mathbf{B},\mathbf{X},\mathbf{Y}$ are matrices with conformant dimensions. On the page 24 of the pdf of the appendix on matrix calculus in the book by Jon Dattorro (page 600 of the book), I find the formula for the first derivative of $f(t)$: $$\frac{df}{dt}=-\mathbf{B}^T(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{Y}(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{A}$$ This sort of makes sense to me from my knowledge of calculus of functions of single variable: if you have $g(t)=a(x+ty)^{-1}b=ab(x+ty)^{-1}$, then $\frac{dg}{dt}=-ab(x+ty)^{-2}y=-a(x+ty)^{-1}y(x+ty)^{-1}b$ (from the chain rule and the power rule). That is, there is a clear similarity in the form. What I don't understand is why the matrix equation for $\frac{df}{dt}$ looks the way it does.  Is it due to non-commutativity of matrix multiplication?  But how does that come in to this problem exactly?  I've found the chain rule for matrix-valued function in the same pdf on page 8 (eq 1749) but I am not sure how to apply it here. Maybe I don't understand something about the calculus of the single-variable functions. I guess I am asking if there is a way to derive the equation for $\frac{df}{dt}$ ""from first principles"" without using tensors.","I am having trouble understanding the derivation of some seemingly simple matrix derivatives and am wondering if there is an intuitive (perhaps geometric) explanation.  I am reasonably well-versed in multivariate calculus and linear algebra, but am not comfortable with tensor math. The function I am interested in is $f(t)=\mathbf{B}^T(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{A}$, where $t$ is a scalar, and $\mathbf{A},\mathbf{B},\mathbf{X},\mathbf{Y}$ are matrices with conformant dimensions. On the page 24 of the pdf of the appendix on matrix calculus in the book by Jon Dattorro (page 600 of the book), I find the formula for the first derivative of $f(t)$: $$\frac{df}{dt}=-\mathbf{B}^T(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{Y}(\mathbf{X}+t\mathbf{Y})^{-1}\mathbf{A}$$ This sort of makes sense to me from my knowledge of calculus of functions of single variable: if you have $g(t)=a(x+ty)^{-1}b=ab(x+ty)^{-1}$, then $\frac{dg}{dt}=-ab(x+ty)^{-2}y=-a(x+ty)^{-1}y(x+ty)^{-1}b$ (from the chain rule and the power rule). That is, there is a clear similarity in the form. What I don't understand is why the matrix equation for $\frac{df}{dt}$ looks the way it does.  Is it due to non-commutativity of matrix multiplication?  But how does that come in to this problem exactly?  I've found the chain rule for matrix-valued function in the same pdf on page 8 (eq 1749) but I am not sure how to apply it here. Maybe I don't understand something about the calculus of the single-variable functions. I guess I am asking if there is a way to derive the equation for $\frac{df}{dt}$ ""from first principles"" without using tensors.",,"['calculus', 'linear-algebra', 'matrices']"
66,Commutative matrices are multiples of the identity,Commutative matrices are multiples of the identity,,"Assume M is a $2 \times 2$ real matrix such that $MX = XM$ for all real $2\times2$   matrices $X$. Show that $M$ must be some real multiple $q$ of $I$. I can see that this is logical and have tried a few examples where I have multiplied $qI$ by some random $2\times2$ matrix from both sides and get the same matrix in both cases. How would I go about showing this though? Surely a few examples aren't actually showing anything for the general case, right? Also, does the property shown in the question hold for all square matrices, or just $2\times2$ ones?","Assume M is a $2 \times 2$ real matrix such that $MX = XM$ for all real $2\times2$   matrices $X$. Show that $M$ must be some real multiple $q$ of $I$. I can see that this is logical and have tried a few examples where I have multiplied $qI$ by some random $2\times2$ matrix from both sides and get the same matrix in both cases. How would I go about showing this though? Surely a few examples aren't actually showing anything for the general case, right? Also, does the property shown in the question hold for all square matrices, or just $2\times2$ ones?",,"['linear-algebra', 'matrices']"
67,How to show eigenvalues of matrix $AB$ and $A^{1/2}B A^{1/2}$ are equal?,How to show eigenvalues of matrix  and  are equal?,AB A^{1/2}B A^{1/2},How do I shown that the matrices  $AB$  and $A^{1/2}B A^{1/2}$ have same eigenvalues? Here both $A$ and $B$ are symmetric matrices and $A^{1/2}$ is the square root of matrix $A$. This book mentions the relation in Remark 4.2,How do I shown that the matrices  $AB$  and $A^{1/2}B A^{1/2}$ have same eigenvalues? Here both $A$ and $B$ are symmetric matrices and $A^{1/2}$ is the square root of matrix $A$. This book mentions the relation in Remark 4.2,,"['matrices', 'eigenvalues-eigenvectors']"
68,why does the blockwise inversion formula work?,why does the blockwise inversion formula work?,,"I used this http://en.wikipedia.org/wiki/Matrix_inverse#Blockwise_inversion formula to get the inverse of a partitioned matrix, and it works great. What I don't understand is why exactly it works. If I have a matrix $$\left(\begin{array}{cc}A& B\\C& D\end{array}\right)$$ and its inverse $$\left(\begin{array}{cc}W& X\\Y& Z\end{array}\right)$$ I can see that AW + BY = I, CX + DZ = I, and the other products are zero matrices, but when I try to use these relationships to build the blockwise formula back up, I don't get it right. Can anyone prove this formula to me?","I used this http://en.wikipedia.org/wiki/Matrix_inverse#Blockwise_inversion formula to get the inverse of a partitioned matrix, and it works great. What I don't understand is why exactly it works. If I have a matrix $$\left(\begin{array}{cc}A& B\\C& D\end{array}\right)$$ and its inverse $$\left(\begin{array}{cc}W& X\\Y& Z\end{array}\right)$$ I can see that AW + BY = I, CX + DZ = I, and the other products are zero matrices, but when I try to use these relationships to build the blockwise formula back up, I don't get it right. Can anyone prove this formula to me?",,"['linear-algebra', 'matrices']"
69,Find values which make a matrix singular,Find values which make a matrix singular,,Find all the values of c for which the following matrix is singular: $$\begin{bmatrix} 1 & c & c \\ c & c & c \\ 2 & c & 3 \end{bmatrix}$$ Anyone know how to solve this?,Find all the values of c for which the following matrix is singular: $$\begin{bmatrix} 1 & c & c \\ c & c & c \\ 2 & c & 3 \end{bmatrix}$$ Anyone know how to solve this?,,"['linear-algebra', 'matrices']"
70,Possible proof for the relation involving matrix trace,Possible proof for the relation involving matrix trace,,"Suppose a diagonal matrix $D\in\mathbb{R}^{n\times n}$ is given, with all its entries $d_{ii}\geq0$, for all $i$. Is it possible to prove $\operatorname{tr}(X^TDX)-2\operatorname{tr}(X^TDY)+\operatorname{tr}(Y^TDY)\geq 0$ for some $X, Y\in\mathbb{R}^{n\times 2}$. The above reminds to $a^2+b^2\geq2ab$, but I would need a proof in matrix terms. Also, if the above is true, for which range of $b$ is the following true $(2-b)\operatorname{tr}(X^TDX)-2(2-b)\operatorname{tr}(X^TDY)+\operatorname{tr}(2-b)(Y^TDY)\geq 0$","Suppose a diagonal matrix $D\in\mathbb{R}^{n\times n}$ is given, with all its entries $d_{ii}\geq0$, for all $i$. Is it possible to prove $\operatorname{tr}(X^TDX)-2\operatorname{tr}(X^TDY)+\operatorname{tr}(Y^TDY)\geq 0$ for some $X, Y\in\mathbb{R}^{n\times 2}$. The above reminds to $a^2+b^2\geq2ab$, but I would need a proof in matrix terms. Also, if the above is true, for which range of $b$ is the following true $(2-b)\operatorname{tr}(X^TDX)-2(2-b)\operatorname{tr}(X^TDY)+\operatorname{tr}(2-b)(Y^TDY)\geq 0$",,"['linear-algebra', 'matrices', 'vector-spaces']"
71,What are the dimensions of the product of two matrices?,What are the dimensions of the product of two matrices?,,A simple question is a (5x2)*(2x5) = a (5x5) matrix?,A simple question is a (5x2)*(2x5) = a (5x5) matrix?,,['matrices']
72,"Linear Algebra, proof about eigenvalues","Linear Algebra, proof about eigenvalues",,"I've been trying to complete a proof for a while now, but I can't. It would be great if someone could finish it for me so that I can at least learn from the solution. The problem is asked like this: Show that $\lambda$ is an eigenvalue of A iff $\lambda$ is eigenvalue of A transpose. There's a hint and it says: For any $\lambda$, $(A-\lambda I )^T = A^T-\lambda I$. By a theorem (which one?), $A^T - \lambda I$ is invertible iff $A-\lambda I$ is invertible. I know that there is a theorem, in this book called the Invertible Matrix Theorem, which says that if A is invertible then so is its transpose. But I don't see what that has to do with anything? If A has an eigenvalue, then $A-\lambda I$ is linearly dependent and is not even invertible! Is that what the hint is all about? Well, I still don't see what that says about the solution set. In order to complete this proof, I think, we need to show that the solution set of the homogenous equation $A-\lambda I = 0$ is the same as that for its transpose. Many thanks.","I've been trying to complete a proof for a while now, but I can't. It would be great if someone could finish it for me so that I can at least learn from the solution. The problem is asked like this: Show that $\lambda$ is an eigenvalue of A iff $\lambda$ is eigenvalue of A transpose. There's a hint and it says: For any $\lambda$, $(A-\lambda I )^T = A^T-\lambda I$. By a theorem (which one?), $A^T - \lambda I$ is invertible iff $A-\lambda I$ is invertible. I know that there is a theorem, in this book called the Invertible Matrix Theorem, which says that if A is invertible then so is its transpose. But I don't see what that has to do with anything? If A has an eigenvalue, then $A-\lambda I$ is linearly dependent and is not even invertible! Is that what the hint is all about? Well, I still don't see what that says about the solution set. In order to complete this proof, I think, we need to show that the solution set of the homogenous equation $A-\lambda I = 0$ is the same as that for its transpose. Many thanks.",,['linear-algebra']
73,Finding the determinant of a matrix defined by row indices,Finding the determinant of a matrix defined by row indices,,Given matrix A: $$ A\in\mathbb{R}^{n \times n} $$ $$ A_{ij}=\begin{cases} 0 ~\text{if}~ i =j\\ 2i ~\text{if}~ i\neq j  \end{cases} $$ Find the determinant of $A$ . How should I approach this? I tried rearranging the rows to get a triangular matrix but didn't do well.,Given matrix A: Find the determinant of . How should I approach this? I tried rearranging the rows to get a triangular matrix but didn't do well.,"
A\in\mathbb{R}^{n \times n}
 
A_{ij}=\begin{cases}
0 ~\text{if}~ i =j\\
2i ~\text{if}~ i\neq j 
\end{cases}
 A","['linear-algebra', 'matrices', 'determinant']"
74,$O$ orthogonal with $\det(O)=-1$ implies $||\Omega - O \Omega O^{T}|| = 2 $?,orthogonal with  implies ?,O \det(O)=-1 ||\Omega - O \Omega O^{T}|| = 2 ,Let $O\in \mathrm{O}(2n)$ be an orthogonal matrix. Let $\Omega$ be the matrix $\Omega:= \bigoplus^n_{i=1} \begin{pmatrix} 0 & 1  \\ -1 & 0  \\ \end{pmatrix}.$ Is it true that: $\det(O)=-1$ implies $||\Omega - O \Omega O^{T}|| = 2 $ ? Here $||\cdot||$ denotes the operator norm (largest singular value of the matrix).,Let be an orthogonal matrix. Let be the matrix Is it true that: implies ? Here denotes the operator norm (largest singular value of the matrix).,"O\in \mathrm{O}(2n) \Omega \Omega:=
\bigoplus^n_{i=1} \begin{pmatrix}
0 & 1  \\
-1 & 0  \\
\end{pmatrix}. \det(O)=-1 ||\Omega - O \Omega O^{T}|| = 2  ||\cdot||","['matrices', 'matrix-decomposition', 'orthogonal-matrices', 'matrix-norms', 'symplectic-linear-algebra']"
75,Other functions on eigenvalues besides trace and determinants,Other functions on eigenvalues besides trace and determinants,,"Given a linear transformation $\mathscr{A}: V \to V$ , where $V$ is a finite-dimensional vector space with the underlying field being $\mathbb{R}$ or $\mathbb{C}$ . Suppose $\mathscr{A}$ has eigenvalues $\lambda_{1},\dots,\lambda_{n}$ . We can define $\operatorname{tr}(\mathscr{A}) = \sum_{i} \lambda_{i}$ , and $\det(\mathscr{A}) = \prod_{i}\lambda_{i}$ . These are two commonly studied functions, and one can ask many interesting questions about them (which goes beyond linear algebra for instance in convex analysis ). However, it seems odd to me that I've never heard people using other different functions on eigenvalues. In particular we can define $\operatorname{oper_{1}}(\mathscr{A}) = \sum_{1\leq i,j\leq n}\lambda_{i}\lambda_{j}$ , or similarly $\operatorname{oper_{2}}(\mathscr{A}) = \sum_{1\leq i,j,k\leq n}\lambda_{i}\lambda_{j}\lambda_{k}$ , etc. In this case we do not have $\lambda_{1},\dots,\lambda_{n}$ ordered, but if we do. Say in decreasing order, we can define something like $\operatorname{oper} (\mathscr{A}) = f(\lambda_{1},\dots,\lambda_{n})$ , where $f$ is some continuous function from $\mathbb{R}^{n}$ ( $\mathbb{C}^{n}$ ) to $\mathbb{R}$ ( $\mathbb{C}$ ). So are there any results on such functions. If no, why not? Even beyond linear algebra, it might be useful in analysis.","Given a linear transformation , where is a finite-dimensional vector space with the underlying field being or . Suppose has eigenvalues . We can define , and . These are two commonly studied functions, and one can ask many interesting questions about them (which goes beyond linear algebra for instance in convex analysis ). However, it seems odd to me that I've never heard people using other different functions on eigenvalues. In particular we can define , or similarly , etc. In this case we do not have ordered, but if we do. Say in decreasing order, we can define something like , where is some continuous function from ( ) to ( ). So are there any results on such functions. If no, why not? Even beyond linear algebra, it might be useful in analysis.","\mathscr{A}: V \to V V \mathbb{R} \mathbb{C} \mathscr{A} \lambda_{1},\dots,\lambda_{n} \operatorname{tr}(\mathscr{A}) = \sum_{i} \lambda_{i} \det(\mathscr{A}) = \prod_{i}\lambda_{i} \operatorname{oper_{1}}(\mathscr{A}) = \sum_{1\leq i,j\leq n}\lambda_{i}\lambda_{j} \operatorname{oper_{2}}(\mathscr{A}) = \sum_{1\leq i,j,k\leq n}\lambda_{i}\lambda_{j}\lambda_{k} \lambda_{1},\dots,\lambda_{n} \operatorname{oper} (\mathscr{A}) = f(\lambda_{1},\dots,\lambda_{n}) f \mathbb{R}^{n} \mathbb{C}^{n} \mathbb{R} \mathbb{C}","['linear-algebra', 'matrices', 'functional-analysis', 'eigenvalues-eigenvectors', 'convex-analysis']"
76,Suppose that $\mathbf{A}-\mathbf{B}$ is positive definite. What can we say about $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$?,Suppose that  is positive definite. What can we say about ?,\mathbf{A}-\mathbf{B} \left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1},"In a physical problem related to minimum dissipation, it can be proven from physical considerations that $\mathbf{A}-\mathbf{B}$ is positive definite. According to the definition, we have $$ \forall \mathbf{x} \in \mathbb{R}^n \backslash \{0\}, \quad \mathbf{x}^\top \left( \mathbf{A}-\mathbf{B} \right) \mathbf{x} > 0 \, . $$ Both $\mathbf{A}$ and $\mathbf{B}$ are invertible and are themselves also symmetric positive definite. In addition, the eigenvalues of $\mathbf{A}$ and $\mathbf{B}$ are positive. To be able to proceed with a mathematical proof, I was wondering whether this imply that $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$ is positive definite? Already for diagonal matrices $\mathbf{A}$ and $\mathbf{B}$ , it can be shown readily that this is the case indeed. Can we say anything about the positive definiteness of $\left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}$ in the general situation of interest?","In a physical problem related to minimum dissipation, it can be proven from physical considerations that is positive definite. According to the definition, we have Both and are invertible and are themselves also symmetric positive definite. In addition, the eigenvalues of and are positive. To be able to proceed with a mathematical proof, I was wondering whether this imply that is positive definite? Already for diagonal matrices and , it can be shown readily that this is the case indeed. Can we say anything about the positive definiteness of in the general situation of interest?","\mathbf{A}-\mathbf{B} 
\forall \mathbf{x} \in \mathbb{R}^n \backslash \{0\}, \quad
\mathbf{x}^\top \left( \mathbf{A}-\mathbf{B} \right) \mathbf{x} > 0 \, .
 \mathbf{A} \mathbf{B} \mathbf{A} \mathbf{B} \left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1} \mathbf{A} \mathbf{B} \left( \mathbf{B}^{-1} - \mathbf{A}^{-1} \right)^{-1}","['linear-algebra', 'matrices', 'optimization', 'matrix-calculus', 'positive-definite']"
77,Prove that $ \det(A+2B)=3\det(B)$ under the condition that $\det(A)$ $=\det(A+B)$ $=\det(A-B)$ $=0$,Prove that  under the condition that, \det(A+2B)=3\det(B) \det(A) =\det(A+B) =\det(A-B) =0,"Let $A, B\in M_{3\times 3}(R)$ such that $$\det(A)=\det(A+B)=\det(A-B)=0$$ Show that $ \det(A+2B)=3\det(B)$ Observe that $$\det(A+0B)=\det(A+B)=\det(A-B)=0$$ and hence the $3\!-\!\mathrm{degree}$ polynomial of $$\det(A+tB)$$ has roots $t=0,1,-1$ But unfortunately there is nothing more I can tell. Could anyone give me some hint please?",Let such that Show that Observe that and hence the polynomial of has roots But unfortunately there is nothing more I can tell. Could anyone give me some hint please?,"A, B\in M_{3\times 3}(R) \det(A)=\det(A+B)=\det(A-B)=0  \det(A+2B)=3\det(B) \det(A+0B)=\det(A+B)=\det(A-B)=0 3\!-\!\mathrm{degree} \det(A+tB) t=0,1,-1","['linear-algebra', 'matrices', 'determinant', 'matrix-calculus']"
78,Order of eigenvectors within basis for Jordan Normal Form?,Order of eigenvectors within basis for Jordan Normal Form?,,"I'm currently baffled as I thought that the order of eigenvectors within the basis of a JNF decomposition doesn't matter. I may have a made a mistake in my working, but if not, is there a general rule for the order? This question specifically concerns a case where there is only one eigenvalue and therefore doesn't seem to be a clear order to put the eigenvectors in. We have: $$ \begin{equation*} A =  \begin{bmatrix} 3 & 0 & 0 \newline 0 & 4 & -1 \newline 0 & 1 & 2 \newline \end{bmatrix} \end{equation*} $$ giving us the following characteristic polynomial: $$ det(A - \lambda I) = (-\lambda + 3)(\lambda^{2} - 6\lambda + 9) $$ $$ \therefore \lambda_{1} = 3  \ \textrm{(with algebraic multiplicity 3)}$$ Calculating the eigenvectors, we get $$  \begin{equation*} E_{\lambda_{1}} = span \{ \begin{bmatrix} 1 & 0 & 0 \newline \end{bmatrix}^{T}, \begin{bmatrix} 0 & 1 & 1 \end{bmatrix}^{T} \} \end{equation*} $$ then, using $$ \begin{equation} \tag{∗} (A - \lambda I)v_{n+1} = b_{n} \end{equation} $$ we can generate our third eigenvector: $$ \left[ \begin{array}{ccc|c} 0 & 0 & 0 & 1 \newline 0 & 1 & -1 & 0 \newline 0 & 1 & -1 & 0 \end{array} \right] \xrightarrow{\text{RREF}} \left[ \begin{array}{ccc|c} 0 & 1 & -1 & 0 \newline 0 & 0 & 0 & 1 \newline 0 & 0 & 0 & 0 \end{array} \right] $$ We now have three eigenvectors with which we can form our basis, $B$ : $$ b_{1} = \begin{bmatrix} 0 \newline 1 \newline 1 \end{bmatrix}, \ b_{2} = \begin{bmatrix} 1 \newline 0 \newline 0 \end{bmatrix}, \ b_{3} = \begin{bmatrix} 0 \newline 1 \newline 0 \end{bmatrix} $$ I originally assumed that the order that the vectors are generated in by (∗) would form a JNF: $$ \begin{align} J &= B^{-1}AB \newline &=  \begin{bmatrix} 0 & 1 & 0 \newline 1 & 0 & 1 \newline 1 & 0 & 0 \newline \end{bmatrix}^{-1} \begin{bmatrix} 3 & 0 & 0 \newline 0 & 4 & -1 \newline 0 & 1 & 2 \newline \end{bmatrix} \begin{bmatrix} 0 & 1 & 0 \newline 1 & 0 & 1 \newline 1 & 0 & 0 \newline \end{bmatrix} \newline &=  \begin{bmatrix} 3 & 0 & 1 \newline 0 & 3 & 0 \newline 0 & 0 & 3 \newline \end{bmatrix} \end{align} $$ but this is clearly not in JNF. However, with $$ B = [b_{1}, b_{3}, b_{2}] = \begin{bmatrix} 0 & 0 & 1 \newline 1 & 1 & 0 \newline 1 & 0 & 0 \newline \end{bmatrix} $$ we have $$ \begin{align} J &= B^{-1}AB \newline &=  \begin{bmatrix} 0 & 0 & 1 \newline 1 & 1 & 0 \newline 1 & 0 & 0 \newline \end{bmatrix}^{-1} \begin{bmatrix} 3 & 0 & 0 \newline 0 & 4 & -1 \newline 0 & 1 & 2 \newline \end{bmatrix} \begin{bmatrix} 0 & 0 & 1 \newline 1 & 1 & 0 \newline 1 & 0 & 0 \end{bmatrix} \newline &=  \begin{bmatrix} 3 & 1 & 0 \newline 0 & 3 & 0 \newline 0 & 0 & 3 \end{bmatrix} \end{align} $$ This doesn't make much sense to me - is there a general rule for the order that the basis must be in?","I'm currently baffled as I thought that the order of eigenvectors within the basis of a JNF decomposition doesn't matter. I may have a made a mistake in my working, but if not, is there a general rule for the order? This question specifically concerns a case where there is only one eigenvalue and therefore doesn't seem to be a clear order to put the eigenvectors in. We have: giving us the following characteristic polynomial: Calculating the eigenvectors, we get then, using we can generate our third eigenvector: We now have three eigenvectors with which we can form our basis, : I originally assumed that the order that the vectors are generated in by (∗) would form a JNF: but this is clearly not in JNF. However, with we have This doesn't make much sense to me - is there a general rule for the order that the basis must be in?","
\begin{equation*}
A = 
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\end{equation*}
  det(A - \lambda I) = (-\lambda + 3)(\lambda^{2} - 6\lambda + 9)   \therefore \lambda_{1} = 3  \ \textrm{(with algebraic multiplicity 3)}  
\begin{equation*}
E_{\lambda_{1}} = span \{
\begin{bmatrix}
1 & 0 & 0 \newline
\end{bmatrix}^{T},
\begin{bmatrix}
0 & 1 & 1
\end{bmatrix}^{T}
\}
\end{equation*}
 
\begin{equation}
\tag{∗}
(A - \lambda I)v_{n+1} = b_{n}
\end{equation}
 
\left[
\begin{array}{ccc|c}
0 & 0 & 0 & 1 \newline
0 & 1 & -1 & 0 \newline
0 & 1 & -1 & 0
\end{array}
\right]
\xrightarrow{\text{RREF}}
\left[
\begin{array}{ccc|c}
0 & 1 & -1 & 0 \newline
0 & 0 & 0 & 1 \newline
0 & 0 & 0 & 0
\end{array}
\right]
 B 
b_{1} = \begin{bmatrix} 0 \newline 1 \newline 1 \end{bmatrix},
\ b_{2} = \begin{bmatrix} 1 \newline 0 \newline 0 \end{bmatrix},
\ b_{3} = \begin{bmatrix} 0 \newline 1 \newline 0 \end{bmatrix}
 
\begin{align}
J &= B^{-1}AB \newline
&= 
\begin{bmatrix}
0 & 1 & 0 \newline
1 & 0 & 1 \newline
1 & 0 & 0 \newline
\end{bmatrix}^{-1}
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\begin{bmatrix}
0 & 1 & 0 \newline
1 & 0 & 1 \newline
1 & 0 & 0 \newline
\end{bmatrix} \newline
&= 
\begin{bmatrix}
3 & 0 & 1 \newline
0 & 3 & 0 \newline
0 & 0 & 3 \newline
\end{bmatrix}
\end{align}
  B = [b_{1}, b_{3}, b_{2}] =
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0 \newline
\end{bmatrix}
 
\begin{align}
J &= B^{-1}AB \newline
&= 
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0 \newline
\end{bmatrix}^{-1}
\begin{bmatrix}
3 & 0 & 0 \newline
0 & 4 & -1 \newline
0 & 1 & 2 \newline
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 1 \newline
1 & 1 & 0 \newline
1 & 0 & 0
\end{bmatrix} \newline
&= 
\begin{bmatrix}
3 & 1 & 0 \newline
0 & 3 & 0 \newline
0 & 0 & 3
\end{bmatrix}
\end{align}
","['linear-algebra', 'matrices', 'matrix-decomposition', 'jordan-normal-form']"
79,Proof that $\frac{Ax}{\|Ax\|}$ has fixed points.,Proof that  has fixed points.,\frac{Ax}{\|Ax\|},"Let $A\in \operatorname{Mat}_{2\times 2}(\Bbb{R})$ with eigenvalues $\lambda\in (1,\infty)$ and $\mu\in (0,1)$ . Define $$T:S^1\rightarrow S^1;~~x\mapsto \frac{Ax}{\|Ax\|}$$ I need to show that $T$ has 4 fixed points. My idea was the following. Since $\mu, \lambda$ are eigenvalues there exists, $x,y\neq 0$ such that $Ax=\mu x$ and $Ay=\lambda y$ .Then I claim that $x,y$ are fixed points: $$Tx=\frac{Ax}{\|Ax\|}=\frac{\mu x}{\|\mu x\|}=\operatorname{sign}(\mu)\frac{x}{\|x\|}=\frac{x}{\|x\|}=x$$ similarly one can show that $Ty=y$ . But then I don't see where the other two fixed points should come from?","Let with eigenvalues and . Define I need to show that has 4 fixed points. My idea was the following. Since are eigenvalues there exists, such that and .Then I claim that are fixed points: similarly one can show that . But then I don't see where the other two fixed points should come from?","A\in \operatorname{Mat}_{2\times 2}(\Bbb{R}) \lambda\in (1,\infty) \mu\in (0,1) T:S^1\rightarrow S^1;~~x\mapsto \frac{Ax}{\|Ax\|} T \mu, \lambda x,y\neq 0 Ax=\mu x Ay=\lambda y x,y Tx=\frac{Ax}{\|Ax\|}=\frac{\mu x}{\|\mu x\|}=\operatorname{sign}(\mu)\frac{x}{\|x\|}=\frac{x}{\|x\|}=x Ty=y","['matrices', 'eigenvalues-eigenvectors', 'dynamical-systems', 'fixed-points']"
80,Show that $ A − A^2$ is invertible given $A$'s eigenvalues?,Show that  is invertible given 's eigenvalues?, A − A^2 A,"Suppose that the $2 \times 2$ matrix $A$ has the characteristic polynomial $p(\lambda) = (\lambda + 1)(\lambda + 2).$ Show that $ A − A^2$ is invertible and determine the eigenvalues to the inverse. So this is how I tried. $p(\lambda)=0$ gives me that $\lambda_1 = -1$ and $\lambda_2 = -2$ This means that we have at least two linearly independent vectors which means that the matrix $A$ is diagonalizable. So we have: $A = PDP^{-1}$ $A - A^2 = PDP^{-1} - PDP^{-1} PDP^{-1} = PDP^{-1} - PD^2 P^{-1} = P(D - D^2) P^{-1} $ $D = ([-1, 0]^T , [0, -2]^T)$ $D - D^2 = ([-2, 0]^T [0, -6]^T)$ But this all feels unnecessary and I feel lost. Am I even thinking right?",Suppose that the matrix has the characteristic polynomial Show that is invertible and determine the eigenvalues to the inverse. So this is how I tried. gives me that and This means that we have at least two linearly independent vectors which means that the matrix is diagonalizable. So we have: But this all feels unnecessary and I feel lost. Am I even thinking right?,"2 \times 2 A p(\lambda) = (\lambda + 1)(\lambda + 2).  A − A^2 p(\lambda)=0 \lambda_1 = -1 \lambda_2 = -2 A A = PDP^{-1} A - A^2 = PDP^{-1} - PDP^{-1} PDP^{-1} = PDP^{-1} - PD^2 P^{-1} = P(D - D^2) P^{-1}  D = ([-1, 0]^T , [0, -2]^T) D - D^2 = ([-2, 0]^T [0, -6]^T)","['linear-algebra', 'matrices', 'inverse', 'matrix-equations', 'diagonalization']"
81,An intelligent way to determine the center of the special linear group of order 2 over the field of order 3.,An intelligent way to determine the center of the special linear group of order 2 over the field of order 3.,,"The Problem : Show that the center of $SL_2(\mathbb{F}_3)$ is the group of order $2$ consisting of $\pm\mathit{I}$ , where $\mathit{I}$ is the identity matrix. My Question : Obviously one can list all the elements in $SL_2(\mathbb{F}_3)$ and compute the center using brute force; but is there an easy way? I tried to make use of The Class Equation : $$|G|=|Z(G)|+\sum_{i=1}^r|G: C_G(g_i)|$$ ( $g_i$ 's are the representatives of the distinct conjugacy classes of $G$ not contained in $Z(G)$ ), to no avail. Any help would be greatly appreciated.","The Problem : Show that the center of is the group of order consisting of , where is the identity matrix. My Question : Obviously one can list all the elements in and compute the center using brute force; but is there an easy way? I tried to make use of The Class Equation : ( 's are the representatives of the distinct conjugacy classes of not contained in ), to no avail. Any help would be greatly appreciated.",SL_2(\mathbb{F}_3) 2 \pm\mathit{I} \mathit{I} SL_2(\mathbb{F}_3) |G|=|Z(G)|+\sum_{i=1}^r|G: C_G(g_i)| g_i G Z(G),"['abstract-algebra', 'matrices', 'group-theory']"
82,Assistance with idempotent matrices,Assistance with idempotent matrices,,"I am taking linear algebra for the first time and am struggling with the concept of idempotent matrices. I know that $A = A^2$ is the concept behind it, but I can't seem to understand HOW one would find the entries, and the explanations given confused me quite a lot. I was hoping someone could give me the gist of this concept and point me in the right direction. For example, I am faced with the question of ""Find all $2\times 2$ matrices such that $A^2=A$ "" Currently, I know that with the matrix $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ that: $a = a^2 + bc\\ b = ab + bd\\ c = ca + cd\\ d = bc + d^2$ but I'm now unsure of how to determine entries off of that.","I am taking linear algebra for the first time and am struggling with the concept of idempotent matrices. I know that is the concept behind it, but I can't seem to understand HOW one would find the entries, and the explanations given confused me quite a lot. I was hoping someone could give me the gist of this concept and point me in the right direction. For example, I am faced with the question of ""Find all matrices such that "" Currently, I know that with the matrix that: but I'm now unsure of how to determine entries off of that.","A = A^2 2\times 2 A^2=A A = \begin{bmatrix}a&b\\c&d\end{bmatrix} a = a^2 + bc\\
b = ab + bd\\
c = ca + cd\\
d = bc + d^2","['linear-algebra', 'matrices', 'idempotents']"
83,Can a matrix have no eigenvectors?,Can a matrix have no eigenvectors?,,"Is it possible for a matrix to have no eigenvectors? One way this could happen: say $A$ is an invertible matrix. And the characteristic polynomial, $|A - \lambda I| = 0$ has only one solution, $\lambda = 0$ . Then, when you try to find an eigenvector via $(A-\lambda I)v=0$ , you get $Av=0$ for which $v=0$ is the only solution. If this is impossible, are there any other ways there could be no eigenvector at all?","Is it possible for a matrix to have no eigenvectors? One way this could happen: say is an invertible matrix. And the characteristic polynomial, has only one solution, . Then, when you try to find an eigenvector via , you get for which is the only solution. If this is impossible, are there any other ways there could be no eigenvector at all?",A |A - \lambda I| = 0 \lambda = 0 (A-\lambda I)v=0 Av=0 v=0,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'characteristic-polynomial']"
84,A question about the determinant of a binary matrix,A question about the determinant of a binary matrix,,"Is it true that for every positive integers $n$ and $m < n$ there exists a square matrix of order $n$ that contains only zeros and ones, whose columns contain exactly $m$ ones  (and hence $n(n-m)$ zeros) and whose determinant is not equal to zero?  It is clear that one can speak of rows instead of columns.  Here is an example of such a determinant for $n=5$ and $m=3$ : $$ \begin{vmatrix} 1 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 0 & 1 \\ 0 & 1 & 1 & 1 & 1 \\ 1 & 1 & 0 & 1 & 1 \end{vmatrix} $$ I see experimentally using MathCAD that it seems to be true.  But how to prove it?","Is it true that for every positive integers and there exists a square matrix of order that contains only zeros and ones, whose columns contain exactly ones  (and hence zeros) and whose determinant is not equal to zero?  It is clear that one can speak of rows instead of columns.  Here is an example of such a determinant for and : I see experimentally using MathCAD that it seems to be true.  But how to prove it?","n m < n n m n(n-m) n=5 m=3 
\begin{vmatrix}
1 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 \\
1 & 0 & 1 & 0 & 1 \\
0 & 1 & 1 & 1 & 1 \\
1 & 1 & 0 & 1 & 1
\end{vmatrix}
","['linear-algebra', 'matrices', 'determinant']"
85,Can a real matrix have arbitrary complex eigenvalues?,Can a real matrix have arbitrary complex eigenvalues?,,"Given a set $S$ of complex numbers such that $z\in S \implies \bar{z}\in S$ can I find a real matrix $M$ (in a space of dimension $|S|$ ) whose eigenvalues are precisely those in $S$ ? More generally is there a way to know when a real matrix M does exist? I'm struggling to come up with any counter examples. I thought about proving the stronger statement ""Every complex matrix is similar to a real matrix"" but I don't think thats easier to show. Something like jordan normal form could be useful but we don't really care about the jordan structure, just the eigenvalues. Another stronger (but less so than the last) that could work is ""Every diagonalisable complex matrix is similar to a real matrix"". There are also some nice ideas like knowing that the determinant and trace of a real matrix are real but the conjugate property probably makes this useless.","Given a set of complex numbers such that can I find a real matrix (in a space of dimension ) whose eigenvalues are precisely those in ? More generally is there a way to know when a real matrix M does exist? I'm struggling to come up with any counter examples. I thought about proving the stronger statement ""Every complex matrix is similar to a real matrix"" but I don't think thats easier to show. Something like jordan normal form could be useful but we don't really care about the jordan structure, just the eigenvalues. Another stronger (but less so than the last) that could work is ""Every diagonalisable complex matrix is similar to a real matrix"". There are also some nice ideas like knowing that the determinant and trace of a real matrix are real but the conjugate property probably makes this useless.",S z\in S \implies \bar{z}\in S M |S| S,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,Which change of basis matrix makes a companion matrix similar to its transpose?,Which change of basis matrix makes a companion matrix similar to its transpose?,,"I know that a companion matrix is similar to its transpose e.g by Smith normal form. When the characteristic polynomial splits, companion matrices become Jordan blocks. A Jordan block is similar to its transpose via the matrix comprised of 1's on the ""opposite diagonal"" and zeros elsewhere. This leads me to wonder: Is there a simple description of the matrix which exhibits a companion matrix as similar to its transpose?","I know that a companion matrix is similar to its transpose e.g by Smith normal form. When the characteristic polynomial splits, companion matrices become Jordan blocks. A Jordan block is similar to its transpose via the matrix comprised of 1's on the ""opposite diagonal"" and zeros elsewhere. This leads me to wonder: Is there a simple description of the matrix which exhibits a companion matrix as similar to its transpose?",,"['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis', 'transpose']"
87,Solving these matrix equations,Solving these matrix equations,,"Let $A, B$ be two invertible matrices of order $r$ such that $$ABA=BA^2B, \qquad A^3=I, \qquad B^{2n-1}=I \text{ for some positive integer } n$$ I am interested in checking if $A$ and $B$ are commutative. Also if $B$ is idempotent ( $B^2=B$ ) or involutory ( $B^2=I$ ). My approach As $$A^3=I, A^2=A^{-1}$$ $$ABA=BA^2B=BA^{-1}B$$ $$A^{-1}B^{-1}A^{-1}=B^{-1}AB^{-1}$$ Now I am unable to proceed from here. I thought $$B^{-1}A^{-1}=AB^{-1}AB^{-1}$$ But this also doesn't yield any fruitful result. If anyone can share some alternate ways to these type of problems for faster solving or if anyone can spot how to proceed with this, it would be a great help. Thank You.","Let be two invertible matrices of order such that I am interested in checking if and are commutative. Also if is idempotent ( ) or involutory ( ). My approach As Now I am unable to proceed from here. I thought But this also doesn't yield any fruitful result. If anyone can share some alternate ways to these type of problems for faster solving or if anyone can spot how to proceed with this, it would be a great help. Thank You.","A, B r ABA=BA^2B, \qquad A^3=I, \qquad B^{2n-1}=I \text{ for some positive integer } n A B B B^2=B B^2=I A^3=I, A^2=A^{-1} ABA=BA^2B=BA^{-1}B A^{-1}B^{-1}A^{-1}=B^{-1}AB^{-1} B^{-1}A^{-1}=AB^{-1}AB^{-1}","['matrices', 'inverse']"
88,What are the units of an inverse matrix?,What are the units of an inverse matrix?,,As the title suggests. For example if I have a matrix $A = \begin{pmatrix}  a & b\\   c& d  \end{pmatrix}$ and all elements consist of variables with units $kg$ and then I take the inverse of the matrix is the resulting units simply $kg^{-1}$ ? How can this be the case if not all matrices have inverses? Somewhat related to my other question about unit quantities in other matrix equations.,As the title suggests. For example if I have a matrix and all elements consist of variables with units and then I take the inverse of the matrix is the resulting units simply ? How can this be the case if not all matrices have inverses? Somewhat related to my other question about unit quantities in other matrix equations.,"A = \begin{pmatrix}
 a & b\\ 
 c& d 
\end{pmatrix} kg kg^{-1}","['linear-algebra', 'matrices', 'inverse', 'applications', 'unit-of-measure']"
89,Find a matrix for a unitary transform between matrices or prove that there is none,Find a matrix for a unitary transform between matrices or prove that there is none,,"I have hermitian matrices $A,\,B$ and would like to find a unitary matrix $U$ such that $$UAU^\dagger=B$$ or show that there is no such matrix. Example: For $$ A=\begin{pmatrix} 0 & 1 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & 0 & 0 & -1\\ 0 & 0 & -1 & 0 \end{pmatrix},\, B=\begin{pmatrix} 0 & 0 & -1 & 0\\ 0 & 0 & 0 & 1\\ -1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 \end{pmatrix} $$ we find $$ U=\begin{pmatrix} 1 & 0 & 0 & 0\\  0 & 0 & -1 & 0 \\ 0 & -1 & 0 & 0\\ 0 & 0 & 0 & 1 \end{pmatrix} $$ by guesswork. A constructive method would be useful. Next example, where I assume that no such $U$ exists, but do not know how to show it: $$ A=\begin{pmatrix} b & d & c & a\\ d & - b & a & - c\\ c & a & - b & - d\\ a & - c & - d & b \end{pmatrix} =a\cdot\sigma_x\otimes\tau_x + b\cdot\sigma_z\otimes\tau_z + c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x\\ B=\begin{pmatrix} - b & d & - c & - a\\ d & b & - a & c\\ - c & - a & b & - d \\- a & c & - d & - b\end{pmatrix} =-a\cdot\sigma_x\otimes\tau_x - b\cdot\sigma_z\otimes\tau_z - c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x $$ where $\sigma_x,\,\sigma_z,\,\tau_x,\,\tau_z$ denote the Pauli matrices $$ \sigma_0 = \tau_0 = \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix},\, \sigma_x = \tau_x = \begin{pmatrix} 0 & 1\\ 1 & 0 \end{pmatrix},\, \sigma_y = \tau_y = \begin{pmatrix} 0 & -i\\ i & 0 \end{pmatrix},\, \sigma_z = \tau_z = \begin{pmatrix} 1 & 0\\ 0 & -1 \end{pmatrix} $$ Note: I am mostly interested in $4\times4$ matrices, but $8\times8$ would be nice, too. EDIT : Thanks to the answer of Kurt G. . Assume $a,\,b,\,c,\,d\in\mathbb{R}\backslash\{0\}$ . I was aware that a solution for $d=0$ exists. I specifically included $d$ to avoid a simple solution of the form $\sigma_a\otimes\tau_b$ . When $d=0$ , then a solution exists independently of the other values, for example $U=\sigma_0\otimes\tau_y$","I have hermitian matrices and would like to find a unitary matrix such that or show that there is no such matrix. Example: For we find by guesswork. A constructive method would be useful. Next example, where I assume that no such exists, but do not know how to show it: where denote the Pauli matrices Note: I am mostly interested in matrices, but would be nice, too. EDIT : Thanks to the answer of Kurt G. . Assume . I was aware that a solution for exists. I specifically included to avoid a simple solution of the form . When , then a solution exists independently of the other values, for example","A,\,B U UAU^\dagger=B 
A=\begin{pmatrix}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0
\end{pmatrix},\,
B=\begin{pmatrix}
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{pmatrix}
 
U=\begin{pmatrix}
1 & 0 & 0 & 0\\
 0 & 0 & -1 & 0 \\
0 & -1 & 0 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
 U 
A=\begin{pmatrix}
b & d & c & a\\
d & - b & a & - c\\
c & a & - b & - d\\
a & - c & - d & b
\end{pmatrix}
=a\cdot\sigma_x\otimes\tau_x + b\cdot\sigma_z\otimes\tau_z
+ c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x\\
B=\begin{pmatrix}
- b & d & - c & - a\\
d & b & - a & c\\
- c & - a & b & - d
\\- a & c & - d & - b\end{pmatrix}
=-a\cdot\sigma_x\otimes\tau_x - b\cdot\sigma_z\otimes\tau_z
- c\cdot\sigma_x\otimes\tau_z + d\cdot\sigma_z\otimes\tau_x
 \sigma_x,\,\sigma_z,\,\tau_x,\,\tau_z 
\sigma_0 = \tau_0 =
\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix},\,
\sigma_x = \tau_x =
\begin{pmatrix}
0 & 1\\
1 & 0
\end{pmatrix},\,
\sigma_y = \tau_y =
\begin{pmatrix}
0 & -i\\
i & 0
\end{pmatrix},\,
\sigma_z = \tau_z =
\begin{pmatrix}
1 & 0\\
0 & -1
\end{pmatrix}
 4\times4 8\times8 a,\,b,\,c,\,d\in\mathbb{R}\backslash\{0\} d=0 d \sigma_a\otimes\tau_b d=0 U=\sigma_0\otimes\tau_y","['linear-algebra', 'matrices', 'hermitian-matrices']"
90,When is this group compact?,When is this group compact?,,"Let $K=\begin{pmatrix} I_a & 0\\ 0 & -I_b \end{pmatrix}$ , where a+b=n, and $I_a$ is the $a\times a$ identity matrix and similar definition for $I_b$ . Let G be the group consisting of all complex $n\times n$ matrices such that $A^*KA=K$ . For what values of a and b is G compact? If $a=0$ or $b=0$ , this just reduces to the unitary group, which is compact. However, if $n=2$ and $a=b=1$ , then $A=\begin{pmatrix} c & \sqrt{c^2-1}\\ \sqrt{c^2-1} & c \end{pmatrix}$ is not bounded and in G. Is there a way to generalize this?","Let , where a+b=n, and is the identity matrix and similar definition for . Let G be the group consisting of all complex matrices such that . For what values of a and b is G compact? If or , this just reduces to the unitary group, which is compact. However, if and , then is not bounded and in G. Is there a way to generalize this?","K=\begin{pmatrix}
I_a & 0\\
0 & -I_b
\end{pmatrix} I_a a\times a I_b n\times n A^*KA=K a=0 b=0 n=2 a=b=1 A=\begin{pmatrix}
c & \sqrt{c^2-1}\\
\sqrt{c^2-1} & c
\end{pmatrix}","['matrices', 'group-theory', 'lie-groups', 'compactness']"
91,Construction of QR decomposition for a singular matrix,Construction of QR decomposition for a singular matrix,,"Consider a matrix $A \in \mathbb{C}^{n \times n}$ . Suppose it's non-singular. Then it's columns $a_1, a_2, ..., a_n$ form a basis in $\mathbb{C}^n$ . Let's apply Gram-Schmidt process to them. We will obtain some basis $q_1, q_2, ..., q_n$ . The transition matrix $S$ from the basis $a$ to the basis $q$ is upper-triangular(by construction). Now, consider the matrix $Q=\big[ q_1|q_2|...|q_n \big]$ . It's columns are orthonormal $\implies$ $Q$ is unitary. And now we know that the matrix $A$ can be represented as: $$ AS=Q \\ A=QS^{-1} $$ $S$ is invertible since it is a transition matrix. $S$ is upper-triangular $\implies$ $S^{-1}$ is upper-triangular as well. E.g. we have shown that each non-degenerate matrix $A$ can be represented as a product of unitary and upper-triangular matrices. But what if $A$ is singular? I've tried the following: Let's select a basis of column space of $A$ : $a_1, ..., a_r$ (they are columns of $A$ ). And also let's reorder columns of $A$ s.t. these linearly independent vectors $a_1, ..., a_r$ are the first $r$ vectors of the matrix. So, we've done permutation of columns of $A$ . This operation can be represented by right-multiplication of A with some matrix $P$ : $A \rightarrow AP$ . Now we apply Gram-Schmidt to the basis of column space(vectors $a_1,...,a_r$ ), and after extend the resulting basis to an ONB of $\mathbb{C}^n$ : $$q_1,q_2, ..., q_n$$ We have constructed matrix $Q=\big[ q_1 | q_2 | ... | q_n \big]$ that is unitary(since it's columns form an ONB). The matrix $S$ s.t. $AP=QS$ is upper-triangular. Moreover, it looks like: $$ \begin{bmatrix} s_{11} & s_{12} & s_{13} & ... & s_{1r} & ... & s_{1n} \\ 0 & s_{22} & s_{23} & ... & s_{2r} & ... & s_{2n} \\ 0 & 0 & s_{33} & ... & s_{3r} & ... & s_{3n} \\ ... & ... & ... & ... & .... & ... & ... \\ 0 & 0 & 0 & ... & s_{rr} & ... & s_{rn} \\ 0 & 0 & 0 & ... & 0 & ...  & 0\\ 0 & 0 & 0 & ... & 0 & ...  & 0\\ ... & ... & ... & ... & .... & ... & ... \\ 0 & 0 & 0 & ... & 0 & ...  & 0\\ \end{bmatrix} $$ But how to show now that the matrix $SP^{-1}$ is upper-triangular? For me, it seems like it's not. Can you see a mistake in my thoughts? Maybe, you have other suggestions for construction of $QR$ -decomposition in case of singular matrix? I'm stuck with this issue for a week now and I have no ideas what to do. Thanks!","Consider a matrix . Suppose it's non-singular. Then it's columns form a basis in . Let's apply Gram-Schmidt process to them. We will obtain some basis . The transition matrix from the basis to the basis is upper-triangular(by construction). Now, consider the matrix . It's columns are orthonormal is unitary. And now we know that the matrix can be represented as: is invertible since it is a transition matrix. is upper-triangular is upper-triangular as well. E.g. we have shown that each non-degenerate matrix can be represented as a product of unitary and upper-triangular matrices. But what if is singular? I've tried the following: Let's select a basis of column space of : (they are columns of ). And also let's reorder columns of s.t. these linearly independent vectors are the first vectors of the matrix. So, we've done permutation of columns of . This operation can be represented by right-multiplication of A with some matrix : . Now we apply Gram-Schmidt to the basis of column space(vectors ), and after extend the resulting basis to an ONB of : We have constructed matrix that is unitary(since it's columns form an ONB). The matrix s.t. is upper-triangular. Moreover, it looks like: But how to show now that the matrix is upper-triangular? For me, it seems like it's not. Can you see a mistake in my thoughts? Maybe, you have other suggestions for construction of -decomposition in case of singular matrix? I'm stuck with this issue for a week now and I have no ideas what to do. Thanks!","A \in \mathbb{C}^{n \times n} a_1, a_2, ..., a_n \mathbb{C}^n q_1, q_2, ..., q_n S a q Q=\big[ q_1|q_2|...|q_n \big] \implies Q A 
AS=Q \\
A=QS^{-1}
 S S \implies S^{-1} A A A a_1, ..., a_r A A a_1, ..., a_r r A P A \rightarrow AP a_1,...,a_r \mathbb{C}^n q_1,q_2, ..., q_n Q=\big[ q_1 | q_2 | ... | q_n \big] S AP=QS 
\begin{bmatrix}
s_{11} & s_{12} & s_{13} & ... & s_{1r} & ... & s_{1n} \\
0 & s_{22} & s_{23} & ... & s_{2r} & ... & s_{2n} \\
0 & 0 & s_{33} & ... & s_{3r} & ... & s_{3n} \\
... & ... & ... & ... & .... & ... & ... \\
0 & 0 & 0 & ... & s_{rr} & ... & s_{rn} \\
0 & 0 & 0 & ... & 0 & ...  & 0\\
0 & 0 & 0 & ... & 0 & ...  & 0\\
... & ... & ... & ... & .... & ... & ... \\
0 & 0 & 0 & ... & 0 & ...  & 0\\
\end{bmatrix}
 SP^{-1} QR","['matrices', 'matrix-decomposition', 'unitary-matrices']"
92,Is this relation true for orthonormal matrices elements?,Is this relation true for orthonormal matrices elements?,,"Let $R$ be a $3 \times 3$ proper orthogonal and normal matrix such that $RR^T = I$ and $det(R) = 1$ , so basically a rotation matrix. $R = \begin{bmatrix} r_{11} & r_{12} & r_{13} \\ r_{21} & r_{22} & r_{23} \\ r_{31} & r_{32} & r_{33} \\  \end{bmatrix}$ Is the following relation true? $r_{13} = r_{32}r_{21} - r_{22}r_{31}$","Let be a proper orthogonal and normal matrix such that and , so basically a rotation matrix. Is the following relation true?",R 3 \times 3 RR^T = I det(R) = 1 R = \begin{bmatrix} r_{11} & r_{12} & r_{13} \\ r_{21} & r_{22} & r_{23} \\ r_{31} & r_{32} & r_{33} \\  \end{bmatrix} r_{13} = r_{32}r_{21} - r_{22}r_{31},"['linear-algebra', 'matrices', 'rotations', 'orthogonal-matrices']"
93,Is the set of all Idempotent matrix in $M_n(\mathbb{F})$ linearly independent?,Is the set of all Idempotent matrix in  linearly independent?,M_n(\mathbb{F}),Is the set $I:=\{\text{set of all Idempotent matrix in}\ M_n(\mathbb{F})\}$ linearly independent? My thought: I think the answer is no if $\mathbb{F}$ is infinite. If $\mathbb{F}$ is infinite then the class of all idempotent matrix is infinite. Any subset of $I$ with more than $n^2$ elements is not linearly independent [as dim( $M_n(\mathbb{F}))=n^2$ ]. As $\mathbb{F}$ is infinite hence $I$ is infinite. But what about if $\mathbb{F}$ is finite?,Is the set linearly independent? My thought: I think the answer is no if is infinite. If is infinite then the class of all idempotent matrix is infinite. Any subset of with more than elements is not linearly independent [as dim( ]. As is infinite hence is infinite. But what about if is finite?,I:=\{\text{set of all Idempotent matrix in}\ M_n(\mathbb{F})\} \mathbb{F} \mathbb{F} I n^2 M_n(\mathbb{F}))=n^2 \mathbb{F} I \mathbb{F},"['linear-algebra', 'matrices', 'linear-transformations', 'idempotents']"
94,Compute the matrix of norms of $A=\begin{bmatrix}3&4\\1&-3\end{bmatrix}$,Compute the matrix of norms of,A=\begin{bmatrix}3&4\\1&-3\end{bmatrix},"My work so far Using the following $\hspace{30px} L^1\ =\displaystyle \max_{\small 1\le j\le m}(\displaystyle \sum_{i=1}^n |a_{ij}|)\\ \hspace{30px} L^2\ =\sigma_{max}(A)\\ \hspace{30px} L^F\ =\sqrt{\displaystyle \sum_{i} \displaystyle \sum_{j} |a_{ij}|^2}\\ \hspace{30px} L^\infty\ =\displaystyle \max_{\small 1\le i\le n}(\displaystyle \sum_{j=1}^m |a_{ij}|)\\$ Thus, $L^1=\begin{bmatrix}3&\textbf{4}\\1&\textbf{3}\end{bmatrix}=7\\ L^2=?\\ L^F=\sqrt{3^2+4^2+1^2+(-3)^2}=\sqrt{35}=5.916079783\\ L^\infty=\begin{bmatrix}\textbf{3}&\textbf{4}\\1&-3\end{bmatrix}=7$ However, I'm unsure how to get $L^2$ . How would I start off doing this part?","My work so far Using the following Thus, However, I'm unsure how to get . How would I start off doing this part?","\hspace{30px} L^1\ =\displaystyle \max_{\small 1\le j\le m}(\displaystyle \sum_{i=1}^n |a_{ij}|)\\ \hspace{30px} L^2\ =\sigma_{max}(A)\\ \hspace{30px} L^F\ =\sqrt{\displaystyle \sum_{i} \displaystyle \sum_{j} |a_{ij}|^2}\\ \hspace{30px} L^\infty\ =\displaystyle \max_{\small 1\le i\le n}(\displaystyle \sum_{j=1}^m |a_{ij}|)\\ L^1=\begin{bmatrix}3&\textbf{4}\\1&\textbf{3}\end{bmatrix}=7\\
L^2=?\\
L^F=\sqrt{3^2+4^2+1^2+(-3)^2}=\sqrt{35}=5.916079783\\
L^\infty=\begin{bmatrix}\textbf{3}&\textbf{4}\\1&-3\end{bmatrix}=7 L^2","['linear-algebra', 'matrices']"
95,Is the Jordan normal form uniquely determined by the characteristic and minimal polynomial?,Is the Jordan normal form uniquely determined by the characteristic and minimal polynomial?,,"I was looking into this answer to a question about obtaining the Jordan normal form given the characteristic and minimal polynomials of a matrix. In this answer, it is stated that ""The multiplicity of an eigenvalue as a root of the characteristic polynomial is the size of the block with that eigenvalue in the Jordan form. The size of the largest sub-block (Elementary Jordan Block) is the multiplicity of that eigenvalue as a root of the minimal polynomial"". I was then thinking of examples of matrices to apply this to, and I came up with the example of a matrix with characteristic polynomial $f(x) = (x-1)^4(x+1)$ and minimal polynomial $m(x) = (x-1)^2(x+1)$ . Using the method described in the answer, I know that the largest elementary Jordan Block for the eigenvalue $1$ should be of size $2$ . But given this, I can make $2$ distinct Jordan blocks for the eigenvalue $1$ : $$\begin{pmatrix} 1&1&0&0\\ 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\\ \end{pmatrix} \qquad \text{and} \qquad \begin{pmatrix} 1&1&0&0\\ 0&1&0&0\\ 0&0&1&1\\ 0&0&0&1\\ \end{pmatrix} $$ where the first Jordan block has one elementary block of size $2$ and $2$ elementary blocks of size $1$ , and the second Jordan block is made up of $2$ elementary blocks, each one of size $2$ . Do the characteristic and minimal polynomial always uniquely determine the Jordan normal form? In which case my understanding is wrong, and I would ask if someone could tell me what am I missing. Or alternatively, when do the characteristic and minimal polynomial uniquely determine the Jordan normal form? Thank you!","I was looking into this answer to a question about obtaining the Jordan normal form given the characteristic and minimal polynomials of a matrix. In this answer, it is stated that ""The multiplicity of an eigenvalue as a root of the characteristic polynomial is the size of the block with that eigenvalue in the Jordan form. The size of the largest sub-block (Elementary Jordan Block) is the multiplicity of that eigenvalue as a root of the minimal polynomial"". I was then thinking of examples of matrices to apply this to, and I came up with the example of a matrix with characteristic polynomial and minimal polynomial . Using the method described in the answer, I know that the largest elementary Jordan Block for the eigenvalue should be of size . But given this, I can make distinct Jordan blocks for the eigenvalue : where the first Jordan block has one elementary block of size and elementary blocks of size , and the second Jordan block is made up of elementary blocks, each one of size . Do the characteristic and minimal polynomial always uniquely determine the Jordan normal form? In which case my understanding is wrong, and I would ask if someone could tell me what am I missing. Or alternatively, when do the characteristic and minimal polynomial uniquely determine the Jordan normal form? Thank you!","f(x) = (x-1)^4(x+1) m(x) = (x-1)^2(x+1) 1 2 2 1 \begin{pmatrix}
1&1&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} \qquad \text{and} \qquad \begin{pmatrix}
1&1&0&0\\
0&1&0&0\\
0&0&1&1\\
0&0&0&1\\
\end{pmatrix}
 2 2 1 2 2","['linear-algebra', 'matrices', 'jordan-normal-form', 'minimal-polynomials', 'characteristic-polynomial']"
96,Proving that removing any vector of the linearly dependent set gives a linearly independent set,Proving that removing any vector of the linearly dependent set gives a linearly independent set,,"Consider the matrix representing 6 linearly dependent vectors: $$\left(\begin{array}{llllll} 1 & 0 & 0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 0 & 1 & 1 \end{array}\right)$$ I know how to prove that the vectors in this matrix are linearly dependent, but how can I show (concisely) that removing any one of the vectors we get a linearly independent set?","Consider the matrix representing 6 linearly dependent vectors: I know how to prove that the vectors in this matrix are linearly dependent, but how can I show (concisely) that removing any one of the vectors we get a linearly independent set?","\left(\begin{array}{llllll}
1 & 0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 1 \\
0 & 0 & 0 & 0 & 1 & 1
\end{array}\right)","['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
97,If $AB=BA$ and $AB$ is diagonal are $A$ and $B$ both diagonal?,If  and  is diagonal are  and  both diagonal?,AB=BA AB A B,So I'm stuck with this problem. If $AB=BA$ and $AB$ is diagonal are $A$ and $B$ both diagonal?,So I'm stuck with this problem. If and is diagonal are and both diagonal?,AB=BA AB A B,"['linear-algebra', 'matrices']"
98,Jordan matrix of $A$ and $A^{-1}$,Jordan matrix of  and,A A^{-1},"Suppose I have the Jordan normal form of a matrix $A$ . I need to find the the Jordan normal form of $A^{-1}$ . I have the following suggestion: $$J_{\lambda,n}\rightarrow J_{1/\lambda,n} $$ where $J_{\lambda,n}$ is a Jordan block of $A$ , $J_{1/\lambda,n}$ is a Jordan block of $A^{-1}$ . We can see that if $\lambda $ is an eigenvalue of $A$ then $1/\lambda$ is an eigenvalue of $A^{-1}$ because $$Av=\lambda v \Leftrightarrow   A^{-1}\lambda v=v\Rightarrow \mu=1/\lambda$$ but how can we make sure that $J_{\lambda,n}$ has the same dimension as $J_{1/\lambda,n}$ ? Thank you in advance.","Suppose I have the Jordan normal form of a matrix . I need to find the the Jordan normal form of . I have the following suggestion: where is a Jordan block of , is a Jordan block of . We can see that if is an eigenvalue of then is an eigenvalue of because but how can we make sure that has the same dimension as ? Thank you in advance.","A A^{-1} J_{\lambda,n}\rightarrow J_{1/\lambda,n}  J_{\lambda,n} A J_{1/\lambda,n} A^{-1} \lambda  A 1/\lambda A^{-1} Av=\lambda v \Leftrightarrow   A^{-1}\lambda v=v\Rightarrow \mu=1/\lambda J_{\lambda,n} J_{1/\lambda,n}","['linear-algebra', 'matrices', 'inverse', 'jordan-normal-form']"
99,What is the difference between a matrix and a tuple of vectors?,What is the difference between a matrix and a tuple of vectors?,,"Why do we need the term matrix? Why can't we just use vectors to define everything we need? I understand we need the terms object, set, group, field, vector, and vector space. I don't understand why we need the term ""matrix"". Is it just shorthand? Similar to the term ""Ket"" used in Dirac Notation for Quantum Mechanics? There a ""Bra"" is a co-vector and a ""Ket"" is a vector. BTW, a $n \times 1$ matrix is a vector.","Why do we need the term matrix? Why can't we just use vectors to define everything we need? I understand we need the terms object, set, group, field, vector, and vector space. I don't understand why we need the term ""matrix"". Is it just shorthand? Similar to the term ""Ket"" used in Dirac Notation for Quantum Mechanics? There a ""Bra"" is a co-vector and a ""Ket"" is a vector. BTW, a matrix is a vector.",n \times 1,"['linear-algebra', 'matrices', 'vectors', 'terminology']"
