,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$n \times n$ positive matrix with $a_{ij} a_{ji} = 1$ has an eigenvalue not less than $n$,positive matrix with  has an eigenvalue not less than,n \times n a_{ij} a_{ji} = 1 n,"$A$ is a real $n \times n$ matrix with positive elements $\{a_{ij}\}$ . For all pairs $(i, j), a_{ij} a_{ji}=1$ . Prove that $A$ has an eigenvalue not less than $n$ .",is a real matrix with positive elements . For all pairs . Prove that has an eigenvalue not less than .,"A n \times n \{a_{ij}\} (i, j), a_{ij} a_{ji}=1 A n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
1,gramian matrix and trace relationship,gramian matrix and trace relationship,,"Suppose $M$ is an $n \times m$ incidence matrix of a simple graph $G$ with $n$ vertices and $m$ edges.  As an exercise in my graph theory course, we proved the diagonals of $M^T M$ equal $2$ for any simple graph. While determining whether I actually believed this or not using NumPy, I stumbled upon a larger idea that exposed a lack of my understanding in Linear Algebra. For any $n \times m$ matrix $A$ I speculate that,  $$tr(A^TA) = \sum_{i = 1}^{n}\sum_{j = 1}^{m}a_{ij}^2$$ Which now makes me think of trace as a function describing the ""size"" of a matrix i.e. when performing a matrix vector product how small or large can we expect differences to be in our resultant vector from different perturbations of input vectors? Is my assumption wrong? If not, is there any intuition on why this is true? I'm not satisfied with the thought that ""the math just works out"".","Suppose $M$ is an $n \times m$ incidence matrix of a simple graph $G$ with $n$ vertices and $m$ edges.  As an exercise in my graph theory course, we proved the diagonals of $M^T M$ equal $2$ for any simple graph. While determining whether I actually believed this or not using NumPy, I stumbled upon a larger idea that exposed a lack of my understanding in Linear Algebra. For any $n \times m$ matrix $A$ I speculate that,  $$tr(A^TA) = \sum_{i = 1}^{n}\sum_{j = 1}^{m}a_{ij}^2$$ Which now makes me think of trace as a function describing the ""size"" of a matrix i.e. when performing a matrix vector product how small or large can we expect differences to be in our resultant vector from different perturbations of input vectors? Is my assumption wrong? If not, is there any intuition on why this is true? I'm not satisfied with the thought that ""the math just works out"".",,"['linear-algebra', 'matrices', 'intuition', 'trace']"
2,Notation for zero matrix [closed],Notation for zero matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I'm confused by notation for matrices. Does $0_n$ mean a zero matrix of size nXn or a zero vector of size nX1 ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I'm confused by notation for matrices. Does $0_n$ mean a zero matrix of size nXn or a zero vector of size nX1 ?",,"['linear-algebra', 'matrices']"
3,How can matrix multiplication with the zero matrix be commutative?,How can matrix multiplication with the zero matrix be commutative?,,"My textbook claims: I don't understand how D can be true. Say I have a 2x4 matrix and I multiply it by the 4x2 zero matrix. I get a 2x2 zero matrix. Now assume I multiplied in the other order, I multiply the 4x2 zero matrix against the 2x4 matrix. I get the 4x4 zero matrix. They are both zero matrices, but they have different dimensions! Surely they can't be considered equal in this case?","My textbook claims: I don't understand how D can be true. Say I have a 2x4 matrix and I multiply it by the 4x2 zero matrix. I get a 2x2 zero matrix. Now assume I multiplied in the other order, I multiply the 4x2 zero matrix against the 2x4 matrix. I get the 4x4 zero matrix. They are both zero matrices, but they have different dimensions! Surely they can't be considered equal in this case?",,"['linear-algebra', 'matrices']"
4,Finding rank-nullity of a given matices,Finding rank-nullity of a given matices,,"Let $A$ be a $4 \times 7$ real matrix and $B$ be a $7 \times 4$ real matrix such that $AB=I_4$. Which of the following are true? 1) rank$(A)=4$ 2) rank$(B)=7$ 3) nullity$(B)=0$ 4) $BA=I_7$. My attempt: $4=\operatorname{rank}(AB) \leq \min\{{\operatorname{rank}(A),\operatorname{rank}(B)}\}$. So $\operatorname{rank}(A)$ must be $4$. It shows that 1) is true 2) is false by Dimension theorem How to check 3) and 4) ?","Let $A$ be a $4 \times 7$ real matrix and $B$ be a $7 \times 4$ real matrix such that $AB=I_4$. Which of the following are true? 1) rank$(A)=4$ 2) rank$(B)=7$ 3) nullity$(B)=0$ 4) $BA=I_7$. My attempt: $4=\operatorname{rank}(AB) \leq \min\{{\operatorname{rank}(A),\operatorname{rank}(B)}\}$. So $\operatorname{rank}(A)$ must be $4$. It shows that 1) is true 2) is false by Dimension theorem How to check 3) and 4) ?",,['linear-algebra']
5,Does rotating a matrix change its determinant?,Does rotating a matrix change its determinant?,,"For a $2 \times 2$, it is easy to see the determinant only changes sign. \begin{align*} \left( \begin{array}{cc}  a & b \\ c & d \end{array} \right) \mapsto \left( \begin{array}{cc}  c & a \\ d & b \end{array} \right) \end{align*} We can see that $\det(A) = -\det(A')$, where $A$ is the original matrix and $A'$ is the rotated matrix. Is this always the case for any $n \times n$ matrix? Also, this would imply that $\det(A) = \det(A'')$. Thanks for any advice!","For a $2 \times 2$, it is easy to see the determinant only changes sign. \begin{align*} \left( \begin{array}{cc}  a & b \\ c & d \end{array} \right) \mapsto \left( \begin{array}{cc}  c & a \\ d & b \end{array} \right) \end{align*} We can see that $\det(A) = -\det(A')$, where $A$ is the original matrix and $A'$ is the rotated matrix. Is this always the case for any $n \times n$ matrix? Also, this would imply that $\det(A) = \det(A'')$. Thanks for any advice!",,"['linear-algebra', 'matrices', 'determinant']"
6,Exact solution of overdetermined linear system,Exact solution of overdetermined linear system,,"Given a (possibly) overdetermined linear system $Ax=b$, where $A$ is full rank and $A \in \mathbb{R}^{m \times n}, \quad m \ge n$ Does the least squares method provide an exact solution (instead of an approximation) if and only if $m=n$ (the system is square and well-determined)? In other words can an overdetermined full rank system have an exact solution? If yes, when and how can you predict it?","Given a (possibly) overdetermined linear system $Ax=b$, where $A$ is full rank and $A \in \mathbb{R}^{m \times n}, \quad m \ge n$ Does the least squares method provide an exact solution (instead of an approximation) if and only if $m=n$ (the system is square and well-determined)? In other words can an overdetermined full rank system have an exact solution? If yes, when and how can you predict it?",,"['linear-algebra', 'matrices', 'matrix-equations', 'least-squares']"
7,Solving a linear system of ODEs in matrix form,Solving a linear system of ODEs in matrix form,,"I would like to solve the linear system of ordinary differential equations $$ \begin{align*} \dot\mu&=R^\mathrm T\mu \\ \dot\Sigma&=R^\mathrm T\Sigma+\Sigma^\mathrm T R+\mathrm{diag}(R^\mathrm T\mu) \end{align*} $$ where $\mu$ is a column vector, $\Sigma$ is a (symmetric) covariance matrix, and $R$ is a square rate matrix. Clearly $\mu(t)=e^{R^\mathrm Tt}\mu(0)$, but is there any way to solve for $\Sigma(t)$?","I would like to solve the linear system of ordinary differential equations $$ \begin{align*} \dot\mu&=R^\mathrm T\mu \\ \dot\Sigma&=R^\mathrm T\Sigma+\Sigma^\mathrm T R+\mathrm{diag}(R^\mathrm T\mu) \end{align*} $$ where $\mu$ is a column vector, $\Sigma$ is a (symmetric) covariance matrix, and $R$ is a square rate matrix. Clearly $\mu(t)=e^{R^\mathrm Tt}\mu(0)$, but is there any way to solve for $\Sigma(t)$?",,"['matrices', 'ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems']"
8,What is the linear algebra of the Schur complement?,What is the linear algebra of the Schur complement?,,"I am interested in the combinatorics of electrical networks, and this leads naturally into the notion of the Schur complement: Suppose we have a graph (possibly edge-weighted with 'resistances') with ""boundary"" and ""interior"" vertices— the idea is that we get to have control over the voltages on the boundary, but not on the interior. The graph, being a graph, has a Laplacian matrix $L=D-A$ . It turns out that the Schur complement of the Laplacian with respect to the internal vertices (thus, a square matrix of side length the number of boundary vertices) has electrical significance. Namely, this is the so-called ""response matrix"" which takes in a vector of voltages and spits out the corresponding vector of current: how much electricity will flow through the boundary vertices. Regardless of application: I don't like matrices. I like linear maps (and I'm fine with bases). Is there any way for someone like me to think about the Schur complement without going all the way down to matrix-land?","I am interested in the combinatorics of electrical networks, and this leads naturally into the notion of the Schur complement: Suppose we have a graph (possibly edge-weighted with 'resistances') with ""boundary"" and ""interior"" vertices— the idea is that we get to have control over the voltages on the boundary, but not on the interior. The graph, being a graph, has a Laplacian matrix . It turns out that the Schur complement of the Laplacian with respect to the internal vertices (thus, a square matrix of side length the number of boundary vertices) has electrical significance. Namely, this is the so-called ""response matrix"" which takes in a vector of voltages and spits out the corresponding vector of current: how much electricity will flow through the boundary vertices. Regardless of application: I don't like matrices. I like linear maps (and I'm fine with bases). Is there any way for someone like me to think about the Schur complement without going all the way down to matrix-land?",L=D-A,"['linear-algebra', 'matrices', 'schur-complement']"
9,The equivalence of $Y=XX^T$ - by Schur complement and rank constraint,The equivalence of  - by Schur complement and rank constraint,Y=XX^T,"I am confused about the following lemma which is useful to convex optimization problem: ( From http://ieeexplore.ieee.org/document/599549/ ) I know the left one is by Schur complement ( https://en.wikipedia.org/wiki/Schur_complement ): $$Z\succeq 0 \Longleftrightarrow Y\succeq 0, Y-XX^T\succeq 0 $$ However, the above only shows $Y\succeq XX^T$ .  The rank constraint (right hand side) tells me what? I have no idea how to use the right constraint to show the equality.","I am confused about the following lemma which is useful to convex optimization problem: ( From http://ieeexplore.ieee.org/document/599549/ ) I know the left one is by Schur complement ( https://en.wikipedia.org/wiki/Schur_complement ): However, the above only shows .  The rank constraint (right hand side) tells me what? I have no idea how to use the right constraint to show the equality.","Z\succeq 0 \Longleftrightarrow Y\succeq 0, Y-XX^T\succeq 0  Y\succeq XX^T","['matrices', 'convex-analysis', 'matrix-rank', 'linear-matrix-inequality']"
10,Why is a matrix invertible when its row-echelon form has no zero row?,Why is a matrix invertible when its row-echelon form has no zero row?,,"If the row echelon form of a square matrix has no zero row, it is invertible. Otherwise, it is singular. Why? If the row echelon form has a zero row, in a linear system, it has either no solution or infinitely many solutions. So, is invertibility linked to having only one solution? Is there a geometrical interpretation for my question?","If the row echelon form of a square matrix has no zero row, it is invertible. Otherwise, it is singular. Why? If the row echelon form has a zero row, in a linear system, it has either no solution or infinitely many solutions. So, is invertibility linked to having only one solution? Is there a geometrical interpretation for my question?",,"['linear-algebra', 'matrices']"
11,Quadratic form in summation form,Quadratic form in summation form,,"Why is  $$x^TAx= \sum_{j}^{n}\sum_{i}^{n} a_{ij}x_ix_j $$ $x$ is n × 1, $A$ is n × n. What I have tried? If $y=Ax$, then $$y_j =\sum_{j}^na_{ij}x_{j}$$ Now, $$x^TAx= \sum_{i}^n x_iy_i $$ which becomes $$\sum_{i}^n x_i\sum_{j}^na_{ij}x_{j}=\sum_{i}^n \sum_{j}^na_{ij}x_{i}x_j$$ Now, the orders of i and j are reversed which is the problem, and confuses me.","Why is  $$x^TAx= \sum_{j}^{n}\sum_{i}^{n} a_{ij}x_ix_j $$ $x$ is n × 1, $A$ is n × n. What I have tried? If $y=Ax$, then $$y_j =\sum_{j}^na_{ij}x_{j}$$ Now, $$x^TAx= \sum_{i}^n x_iy_i $$ which becomes $$\sum_{i}^n x_i\sum_{j}^na_{ij}x_{j}=\sum_{i}^n \sum_{j}^na_{ij}x_{i}x_j$$ Now, the orders of i and j are reversed which is the problem, and confuses me.",,"['matrices', 'quadratic-forms']"
12,"Using $E$ for ""identity matix""","Using  for ""identity matix""",E,"I've noticed that a lot of users (especially those asking questions) tend to use $E$ instead of $I$ for the identity matrix.  Is this a common convention in certain languages? I tried looking through the wiki pages for the identity matrix in different languages, but they all seem to use $I$ rather than $E$.  I'd imagine that in German, one might use $E$ for Einheitsmatrix rather than $I$ for Identitätsmatrix, but that's just a guess.","I've noticed that a lot of users (especially those asking questions) tend to use $E$ instead of $I$ for the identity matrix.  Is this a common convention in certain languages? I tried looking through the wiki pages for the identity matrix in different languages, but they all seem to use $I$ rather than $E$.  I'd imagine that in German, one might use $E$ for Einheitsmatrix rather than $I$ for Identitätsmatrix, but that's just a guess.",,"['linear-algebra', 'matrices', 'notation', 'terminology']"
13,Showing a property of the frobenius norm,Showing a property of the frobenius norm,,If we let $E=u^* v$ which an outer product. Then is the frobenius norm of E $||E||_F=||U||_F||V||_F$ How would I show this is true. The frobenius norm is $E=(\sum_{i=1}^{m} \sum_{j=1}|e_{ij}^2|)^{1/2}$ But I am not sure how to proceed,If we let $E=u^* v$ which an outer product. Then is the frobenius norm of E $||E||_F=||U||_F||V||_F$ How would I show this is true. The frobenius norm is $E=(\sum_{i=1}^{m} \sum_{j=1}|e_{ij}^2|)^{1/2}$ But I am not sure how to proceed,,"['matrices', 'normed-spaces']"
14,Proof that the 2-norm of orthogonal transformation of a matrix is invariant,Proof that the 2-norm of orthogonal transformation of a matrix is invariant,,"For any matrix A and an orthogonal matrix Q, I can prove in the standard way that $$\|QA\|_2 = \|A\|_2 $$ using $$\|QA\|_2^2 = (QAx)^T(QAx) = (Ax)^T(Ax) = \|A\|_2^2 $$ However, I am unable to cancel Q, when the transformation is AQ, i.e. $$\|AQ\|_2^2 = (AQx)^T(AQx) = x^TQ^TA^TAQx$$ After this point I am unable to prove the same that $$\|AQ\|_2 = \|A\|_2 $$","For any matrix A and an orthogonal matrix Q, I can prove in the standard way that using However, I am unable to cancel Q, when the transformation is AQ, i.e. After this point I am unable to prove the same that",\|QA\|_2 = \|A\|_2  \|QA\|_2^2 = (QAx)^T(QAx) = (Ax)^T(Ax) = \|A\|_2^2  \|AQ\|_2^2 = (AQx)^T(AQx) = x^TQ^TA^TAQx \|AQ\|_2 = \|A\|_2 ,"['linear-algebra', 'matrices', 'linear-transformations', 'normed-spaces', 'invariance']"
15,How can two matrices cancel each other out when there is a matrix in between?,How can two matrices cancel each other out when there is a matrix in between?,,"I'm watching videos of Gilbert Strang's linear algebra lectures. In lecture 17, where he goes over orthonormal bases and the Gram-Schmidt process, he proves $$ A^TB = A^T\left( b - \frac{A^Tb} {(A^TA)} A\right) = 0 $$ by having $$ A^T \frac{A^Tb} {(A^TA)} A $$ cancel out into A T b. I don't know how the A T A on the top was allowed to cancel out with the A T A on the bottom if there is an A T b sandwiched in between the A T and the A on the numerator. Shouldn't the A T and the A on the numerator not be allowed to multiply each other? I wondered if the A T on the numerator can cancel out with the A T on the denominator and the same for the two A's, but I do not know if this violates the order of operations for matrix multiplication. If I were to write (A T A) -1 rather than have the (A T A) on the denominator below the (A T b), where would it go?","I'm watching videos of Gilbert Strang's linear algebra lectures. In lecture 17, where he goes over orthonormal bases and the Gram-Schmidt process, he proves $$ A^TB = A^T\left( b - \frac{A^Tb} {(A^TA)} A\right) = 0 $$ by having $$ A^T \frac{A^Tb} {(A^TA)} A $$ cancel out into A T b. I don't know how the A T A on the top was allowed to cancel out with the A T A on the bottom if there is an A T b sandwiched in between the A T and the A on the numerator. Shouldn't the A T and the A on the numerator not be allowed to multiply each other? I wondered if the A T on the numerator can cancel out with the A T on the denominator and the same for the two A's, but I do not know if this violates the order of operations for matrix multiplication. If I were to write (A T A) -1 rather than have the (A T A) on the denominator below the (A T b), where would it go?",,"['linear-algebra', 'matrices', 'projection-matrices', 'gram-schmidt']"
16,Optimizing over matrices in the Loewner ordering,Optimizing over matrices in the Loewner ordering,,"The so-called Loewner ordering introduces a partial ordering to the set of Hermitian matrices: $X \geq Y$ if $X - Y$ is positive semidefinite $X > Y$ if $X - Y$ is positive definite. Consider then the following two problems: $$\max \left\{ X \,|\, X \text{ satisfies some conditions } \right\}\tag{1}\label{opt1}$$ $$\max \left\{ \operatorname{Tr}X \,|\, X \text{ satisfies some conditions } \right\}\tag{2}\label{opt2}$$ where $X$ is Hermitian, and the maximum in \eqref{opt1} is with respect to the Loewner ordering. Are the problems equivalent?","The so-called Loewner ordering introduces a partial ordering to the set of Hermitian matrices: if is positive semidefinite if is positive definite. Consider then the following two problems: where is Hermitian, and the maximum in \eqref{opt1} is with respect to the Loewner ordering. Are the problems equivalent?","X \geq Y X - Y X > Y X - Y \max \left\{ X \,|\, X \text{ satisfies some conditions } \right\}\tag{1}\label{opt1} \max \left\{ \operatorname{Tr}X \,|\, X \text{ satisfies some conditions } \right\}\tag{2}\label{opt2} X","['linear-algebra', 'matrices', 'optimization', 'order-theory', 'positive-semidefinite']"
17,Finding the subspace and basis of a set that is made out of matrices,Finding the subspace and basis of a set that is made out of matrices,,"I've tried to solve the next problem, but I'm not quite sure about the 'a' part and don't know how to start with 'b' Any improvements/clarifications/help is appreciated The question: Let $J$ be a $3*3$ matrix given by: \begin{bmatrix}0&1&0\\0&0&1\\0 &0&0\end{bmatrix} (a) Is the following set a subspace? $S=[A∈ℝ^{3*3}|AJ=JA]$ My attempt S is nonempty since it contains the $0$ matrix, $0*J=0=J*0$ For $A∈S$ and $\alpha$ a real  number scalar: $(\alpha A)J=\alpha (AJ)=\alpha(JA)=J(\alpha A)$ For $A∈S$, $B∈S$: $(A+B)J=AJ+BJ=JA+BA=J(A+B)$ So both closure properties are satisfied meaning S is a subspace of $ℝ^{3*3}$ (b) Find a basis for S and determine its dimension Well I don't really get the question, should I find the basis of the matrix J? or something different? I know that if the basis consists of $n$ vectors than S has dimension $n$ But how to get to this basis/these vectors is my main struggle Thanks in advance :)","I've tried to solve the next problem, but I'm not quite sure about the 'a' part and don't know how to start with 'b' Any improvements/clarifications/help is appreciated The question: Let $J$ be a $3*3$ matrix given by: \begin{bmatrix}0&1&0\\0&0&1\\0 &0&0\end{bmatrix} (a) Is the following set a subspace? $S=[A∈ℝ^{3*3}|AJ=JA]$ My attempt S is nonempty since it contains the $0$ matrix, $0*J=0=J*0$ For $A∈S$ and $\alpha$ a real  number scalar: $(\alpha A)J=\alpha (AJ)=\alpha(JA)=J(\alpha A)$ For $A∈S$, $B∈S$: $(A+B)J=AJ+BJ=JA+BA=J(A+B)$ So both closure properties are satisfied meaning S is a subspace of $ℝ^{3*3}$ (b) Find a basis for S and determine its dimension Well I don't really get the question, should I find the basis of the matrix J? or something different? I know that if the basis consists of $n$ vectors than S has dimension $n$ But how to get to this basis/these vectors is my main struggle Thanks in advance :)",,"['linear-algebra', 'matrices']"
18,Proof of $\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$ [duplicate],Proof of  [duplicate],\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl},"This question already has answers here : Proof relation between Levi-Civita symbol and Kronecker deltas in Group Theory (2 answers) Closed 3 years ago . I'm a student of physics. There is an identity in tensor calculus involving Kronecker deltas ans Levi-Civita pseudo tensors is given by $$\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$$ which is extensively used in physics in deriving various identities. I have neither found a proof of this in physics textbooks nor in Wikipedia. In particular, how does the above formula follow from the definition of $\epsilon_{ijk}$ tensor$$\epsilon_{ijk} =   \begin{cases}          +1 & \text{ for even permutations }, \\          -1 & \text{ for odd permutations } ,\\     \;\;\,0 & \text{ for repetition of indices }, \end{cases}$$ This is the only definition of I'm familiar with.","This question already has answers here : Proof relation between Levi-Civita symbol and Kronecker deltas in Group Theory (2 answers) Closed 3 years ago . I'm a student of physics. There is an identity in tensor calculus involving Kronecker deltas ans Levi-Civita pseudo tensors is given by $$\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$$ which is extensively used in physics in deriving various identities. I have neither found a proof of this in physics textbooks nor in Wikipedia. In particular, how does the above formula follow from the definition of $\epsilon_{ijk}$ tensor$$\epsilon_{ijk} =   \begin{cases}          +1 & \text{ for even permutations }, \\          -1 & \text{ for odd permutations } ,\\     \;\;\,0 & \text{ for repetition of indices }, \end{cases}$$ This is the only definition of I'm familiar with.",,"['matrices', 'tensor-products', 'tensors', 'kronecker-symbol']"
19,"Formula for adjugate of matrix: $\operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})$",Formula for adjugate of matrix:,"\operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})","The following (roughly) is written in the Adjugate Matrix Wikipedia page : If $$ p(t)~{\stackrel {\text{def}}{=}}~\det(t\mathbf {I} -\mathbf {A} )=\sum _{i=0}^{n}p_{i}t^{i}$$ is the characteristic polynomial of the real matrix $n$-by-$n$ matrix $\mathbf A$, then $$ \operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})$$ where $$ \mathrm {\Delta } p(s,t)~=\sum _{j=0}^{n-1}\sum _{i=0}^{n-j-1}p_{i+j+1}s^{i}t^{j} $$ is the first divided difference of $p$. Can anyone prove this, or provide a reference, please. I'm most interested in the case where $s=1$, since this then gives me a nice polynomial expansion of $\operatorname{adj}(\mathbf{I}-\mathbf{A})$ in terms of powers of $\mathbf{A}$, which is well-known already, I would guess. A proof or a reference for this special case would be welcome, too.","The following (roughly) is written in the Adjugate Matrix Wikipedia page : If $$ p(t)~{\stackrel {\text{def}}{=}}~\det(t\mathbf {I} -\mathbf {A} )=\sum _{i=0}^{n}p_{i}t^{i}$$ is the characteristic polynomial of the real matrix $n$-by-$n$ matrix $\mathbf A$, then $$ \operatorname{adj}(s\mathbf{I}-\mathbf{A}) = \mathrm{\Delta} p(s,\mathbf{A})$$ where $$ \mathrm {\Delta } p(s,t)~=\sum _{j=0}^{n-1}\sum _{i=0}^{n-j-1}p_{i+j+1}s^{i}t^{j} $$ is the first divided difference of $p$. Can anyone prove this, or provide a reference, please. I'm most interested in the case where $s=1$, since this then gives me a nice polynomial expansion of $\operatorname{adj}(\mathbf{I}-\mathbf{A})$ in terms of powers of $\mathbf{A}$, which is well-known already, I would guess. A proof or a reference for this special case would be welcome, too.",,"['linear-algebra', 'matrices', 'matrix-equations']"
20,Proof for if $A$ is invertible then $AB$ is invertible,Proof for if  is invertible then  is invertible,A AB,"First, $A$ and $B$ are square matrices So to prove if $AB$ is invertible, then $A$ is invertible: I let $C=(AB^{-1})B$ Then $CA=(AB^{-1})AB=I$ And $C=A^{-1}$ , so A is invertible. But how do I prove it the other way around? I'm pretty sure this would require that $B$ also be invertible for it to be true. Is there a quick way to disprove that if $A$ is invertible then $AB$ is invertible?","First, $A$ and $B$ are square matrices So to prove if $AB$ is invertible, then $A$ is invertible: I let $C=(AB^{-1})B$ Then $CA=(AB^{-1})AB=I$ And $C=A^{-1}$ , so A is invertible. But how do I prove it the other way around? I'm pretty sure this would require that $B$ also be invertible for it to be true. Is there a quick way to disprove that if $A$ is invertible then $AB$ is invertible?",,"['linear-algebra', 'matrices', 'inverse']"
21,Numerical range of a normal matrix is the convex hull of its eigenvalues,Numerical range of a normal matrix is the convex hull of its eigenvalues,,"Show that the numerical range of a normal matrix is the convex hull of its eigenvalues. That is, if $A\in M_n$ is a normal matrix with eigenvalues $x_1, x_2, \ldots, x_n$ then $$W(A) = \left\{t_1x_1+t_2x_2+...+t_nx_n:\;t_i\geq 0,\;\sum_{i=1}^nt_i=1\right\}.$$","Show that the numerical range of a normal matrix is the convex hull of its eigenvalues. That is, if $A\in M_n$ is a normal matrix with eigenvalues $x_1, x_2, \ldots, x_n$ then $$W(A) = \left\{t_1x_1+t_2x_2+...+t_nx_n:\;t_i\geq 0,\;\sum_{i=1}^nt_i=1\right\}.$$",,"['matrices', 'matrix-equations', 'matrix-decomposition']"
22,Cayley-Hamilton equation for a given matrix $A$ and other matrices,Cayley-Hamilton equation for a given matrix  and other matrices,A,"We know  that ""every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic polynomial"" i.e. $p(A)=0$ My question is: what other matrices are satisfying this particular equation $p(A)=0$ Evidently it is satisfied  by any matrix which is generated from $A$ through a change of basis and also by some more simply generated matrices, for example $A^T$ or $\lambda_i{I}$, where $\lambda_i $ is eigenvalue for matrix $A$. But maybe it is possible to find a general form of the matrix which is satisfying characteristic equation generated for a matrix A?","We know  that ""every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic polynomial"" i.e. $p(A)=0$ My question is: what other matrices are satisfying this particular equation $p(A)=0$ Evidently it is satisfied  by any matrix which is generated from $A$ through a change of basis and also by some more simply generated matrices, for example $A^T$ or $\lambda_i{I}$, where $\lambda_i $ is eigenvalue for matrix $A$. But maybe it is possible to find a general form of the matrix which is satisfying characteristic equation generated for a matrix A?",,"['linear-algebra', 'matrices', 'polynomials']"
23,"If $\exp(t(A + B)) = \exp(tA) \exp(tB)$ for all $t \geq 0$ then $A,B$ commute",If  for all  then  commute,"\exp(t(A + B)) = \exp(tA) \exp(tB) t \geq 0 A,B","Let $A,B$ be complex valued square matrices. If $\exp(t(A + B)) = \exp(tA) \exp(tB)$ for all $t \geq 0$ then $A,B$ commute. The converse of this statement can be an easy application of the Cauchy product rule and the binomial theorem. Note that this statement doesn't hold, if we restrict ourselves to $t = 1$. So far I have been trying to use the fact, that $A$ and $B$ are infinitesimal generators to the semigroups $\{\exp(tA)\}$ and $\{\exp(tB)\}$ but I have had no success. Do you have any other hints? Based on the idea of @Did, I came up with the following: Series expansions give me: $$ \sum_{n = 0}^\infty \frac{t^n(A + B)^n}{n!} = I + tA + tB + \frac{t^2(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^n(A + B)^n}{n!}  $$ and $$ \left(\sum_{n = 0}^\infty \frac{t^n(A)^n}{n!} \right) \left(\sum_{n = 0}^\infty \frac{t^n(B)^n}{n!} \right) = I + tA + tB + \frac{t^2A^2}{2} +  t^2AB +  \frac{t^2B^2}{2} + \sum_{n = 3}^\infty t^n c_n, $$ where  $$ c_n := \sum_{k = 0}^n \frac{A^k B^{n - k}}{k! n!}. $$ The comparison of both expansions gives $$ \frac{t^2(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^n(A + B)^n}{n!}  = t^2AB + \sum_{n = 3}^\infty t^n c_n. $$ Division by $t > 0$ yields: $$ \frac{(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^{n-2}(A + B)^n}{n!}  = AB + \sum_{n = 3}^\infty t^{n-2} c_n. $$ But I can't quite see, why the two sums $\sum_{n = 3}^\infty \dots$ should go to $0$ for $t \to 0$ yielding the desired equality $$ \frac{(AB + BA)}{2}   = AB . $$","Let $A,B$ be complex valued square matrices. If $\exp(t(A + B)) = \exp(tA) \exp(tB)$ for all $t \geq 0$ then $A,B$ commute. The converse of this statement can be an easy application of the Cauchy product rule and the binomial theorem. Note that this statement doesn't hold, if we restrict ourselves to $t = 1$. So far I have been trying to use the fact, that $A$ and $B$ are infinitesimal generators to the semigroups $\{\exp(tA)\}$ and $\{\exp(tB)\}$ but I have had no success. Do you have any other hints? Based on the idea of @Did, I came up with the following: Series expansions give me: $$ \sum_{n = 0}^\infty \frac{t^n(A + B)^n}{n!} = I + tA + tB + \frac{t^2(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^n(A + B)^n}{n!}  $$ and $$ \left(\sum_{n = 0}^\infty \frac{t^n(A)^n}{n!} \right) \left(\sum_{n = 0}^\infty \frac{t^n(B)^n}{n!} \right) = I + tA + tB + \frac{t^2A^2}{2} +  t^2AB +  \frac{t^2B^2}{2} + \sum_{n = 3}^\infty t^n c_n, $$ where  $$ c_n := \sum_{k = 0}^n \frac{A^k B^{n - k}}{k! n!}. $$ The comparison of both expansions gives $$ \frac{t^2(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^n(A + B)^n}{n!}  = t^2AB + \sum_{n = 3}^\infty t^n c_n. $$ Division by $t > 0$ yields: $$ \frac{(AB + BA)}{2} + \sum_{n = 3}^\infty \frac{t^{n-2}(A + B)^n}{n!}  = AB + \sum_{n = 3}^\infty t^{n-2} c_n. $$ But I can't quite see, why the two sums $\sum_{n = 3}^\infty \dots$ should go to $0$ for $t \to 0$ yielding the desired equality $$ \frac{(AB + BA)}{2}   = AB . $$",,"['matrices', 'analysis', 'exponential-function', 'semigroup-of-operators', 'matrix-exponential']"
24,"Prove that if $B$ is an antisymmetric matrix, then $\det(B+I) \neq 0$ [closed]","Prove that if  is an antisymmetric matrix, then  [closed]",B \det(B+I) \neq 0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Prove that if $B$ is an antisymmetric matrix with real entries, then $\det(B+I) \neq 0$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Prove that if $B$ is an antisymmetric matrix with real entries, then $\det(B+I) \neq 0$.",,"['linear-algebra', 'matrices', 'determinant']"
25,Derivative of $\rm a^T X^T X X^T X b$ with respect to $\rm X$,Derivative of  with respect to,\rm a^T X^T X X^T X b \rm X,"I'm trying to take the derivative of a 4th order equation with respect to a matrix. It has the following form $$\frac{\displaystyle \partial \bf a^T X^T X X^T X b}{\displaystyle \partial \bf X} = \Large ?$$ $\bf a$ and $\bf b$ are vectors and $\bf X$ is a matrix so, in effect, it's the derivative of a scalar with respect to a matrix. I found the basic derivatives in Matrix Calculus on Wikipedia and I found the second order derivative in The Matrix Cookbook . This gives me the solution for the second order case $$\frac{\displaystyle \partial \bf a^T X^T X b}{\displaystyle \partial \bf X} = \bf X (ab^T+ba^T)$$ I wonder if there is a similar solution for the 4th order case?","I'm trying to take the derivative of a 4th order equation with respect to a matrix. It has the following form $$\frac{\displaystyle \partial \bf a^T X^T X X^T X b}{\displaystyle \partial \bf X} = \Large ?$$ $\bf a$ and $\bf b$ are vectors and $\bf X$ is a matrix so, in effect, it's the derivative of a scalar with respect to a matrix. I found the basic derivatives in Matrix Calculus on Wikipedia and I found the second order derivative in The Matrix Cookbook . This gives me the solution for the second order case $$\frac{\displaystyle \partial \bf a^T X^T X b}{\displaystyle \partial \bf X} = \bf X (ab^T+ba^T)$$ I wonder if there is a similar solution for the 4th order case?",,"['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
26,Bijection from $\textrm{GL}_n/O_n$ onto the set of symmetric matrices.,Bijection from  onto the set of symmetric matrices.,\textrm{GL}_n/O_n,"Let $k$ be an algebraically closed field with characteristic $\neq 2$, and let $O_n$ be the group of orthogonal matrices, i.e. invertible matrices whose inverse is their transpose.  Let $\textrm{GL}_n/O_n$ be the set of left cosets of $O_n$ in $\textrm{GL}_n$.  Is there a natural bijection $f$ from $\textrm{GL}_n/O_n$ onto the set of symmetric $n$ by $n$ matrices? My first guess would be to associate with an $x \in \textrm{GL}_n$ the symmetric matrix $xx^t + x^tx$.  Then if $yx^{-1} \in O_n$, then $1 = (x^{-1})^ty^tyx^{-1}$ implies $x^tx = y^ty$, and similarly the fact that $xy^{-1} \in O_n$ implies that $xx^t = yy^t$, so $xx^t + x^tx = yy^t + y^ty$.  Thus the mapping $\overline{x} \mapsto xx^t + x^tx$ is at least well defined. I think I remember hearing somewhere that this mapping can be shown to be surjective.  But I am still at a loss to prove that $x,y$ invertible and $xx^t + x^tx = yy^t + y^ty$ implies that $xy^{-1} \in O_n$.","Let $k$ be an algebraically closed field with characteristic $\neq 2$, and let $O_n$ be the group of orthogonal matrices, i.e. invertible matrices whose inverse is their transpose.  Let $\textrm{GL}_n/O_n$ be the set of left cosets of $O_n$ in $\textrm{GL}_n$.  Is there a natural bijection $f$ from $\textrm{GL}_n/O_n$ onto the set of symmetric $n$ by $n$ matrices? My first guess would be to associate with an $x \in \textrm{GL}_n$ the symmetric matrix $xx^t + x^tx$.  Then if $yx^{-1} \in O_n$, then $1 = (x^{-1})^ty^tyx^{-1}$ implies $x^tx = y^ty$, and similarly the fact that $xy^{-1} \in O_n$ implies that $xx^t = yy^t$, so $xx^t + x^tx = yy^t + y^ty$.  Thus the mapping $\overline{x} \mapsto xx^t + x^tx$ is at least well defined. I think I remember hearing somewhere that this mapping can be shown to be surjective.  But I am still at a loss to prove that $x,y$ invertible and $xx^t + x^tx = yy^t + y^ty$ implies that $xy^{-1} \in O_n$.",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
27,Cayley Hamilton Theorem,Cayley Hamilton Theorem,,Question is Substitute $A=SBS^{-1}$ into the product  $$ (A-y_1I)(A-y_2I)\cdots(A-y_nI) $$ and show that the product equals $0$ where $B$ denotes the diagonal form of $A$ with eigenvalues on the diagonal and $y$ denotes the eigenvalues Question says that this is related to the Cayley Hamilton Theorem . I know that Cayley Hamilton Theorem states that the eigenvalues in the characteristic polynomial of the matrix can be exchanged with the matrix itself. But I couldn't apply it here.,Question is Substitute $A=SBS^{-1}$ into the product  $$ (A-y_1I)(A-y_2I)\cdots(A-y_nI) $$ and show that the product equals $0$ where $B$ denotes the diagonal form of $A$ with eigenvalues on the diagonal and $y$ denotes the eigenvalues Question says that this is related to the Cayley Hamilton Theorem . I know that Cayley Hamilton Theorem states that the eigenvalues in the characteristic polynomial of the matrix can be exchanged with the matrix itself. But I couldn't apply it here.,,['linear-algebra']
28,Show that $AB-BA\ne C$ for every real $3\times 3$ matrices $A$ and $B$ and some specific $3\times3$ matrix $C$,Show that  for every real  matrices  and  and some specific  matrix,AB-BA\ne C 3\times 3 A B 3\times3 C,Prove that there cannot exist real $3 \times 3$ matrices $A$ and $B$ such that $$AB-BA= \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 1 & 1\end{bmatrix}$$ I have no idea to solve it as both $A$ and $B$ are unknown.,Prove that there cannot exist real $3 \times 3$ matrices $A$ and $B$ such that $$AB-BA= \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 1 & 1\end{bmatrix}$$ I have no idea to solve it as both $A$ and $B$ are unknown.,,"['linear-algebra', 'matrices', 'matrix-equations']"
29,The relation between axes of 3D rotations,The relation between axes of 3D rotations,,"Let's suppose we have two rotations about two different axes represented by vectors $v_1$ and $v_2$: $R_1(v_1, \theta_1)$, $R_2(v_2,\theta_2)$. It's relatively easy to prove that composition of these two rotations gives rotation about axis $v_3$ distinct from axes $v_1$ and $v_2$ . Indeed if for example $v_3=v_1$ then $R_1(v_1, \theta_1) R_2(v_2,\theta_2)=R_3(v_1,\theta_3)$ leads to $R_2(v_2,\theta_2)=R_1^T(v_1, \theta_1)R_3(v_1,\theta_3)=R(v_1,\theta_3 -\theta_1)$ what gives $v_1=v_2$. ... Contradiction... We see that composition of two rotations about different axes always generates a new axis of rotation. The problem can be extended for condition of the plane generated by the axes. Question: Is it true that composition of two rotations generates the axis which   doesn't belong to the plane   which is constructed by the original axes of rotations ? How to prove it ? If the statement is not however true what are conditions for not changing a plane during the composition of rotations $ ^{[1]}$ ? $ ^{[1]}$ It can be observed that even in the case of quite regular rotations the above statement is true Let's take $Rot(z,\dfrac{\pi}{2})Rot(x,\dfrac{\pi}{2})= \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ \end{bmatrix} = Rot([1,1,1]^T, \dfrac{2}{3}\pi)$ or $Rot(x,  \pi  )Rot(z,  \pi  )= \begin{bmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & -1 \\ \end{bmatrix} \begin{bmatrix} -1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \\ \end{bmatrix} = Rot( y, \pi)$ So I suppose it is generally true but how to prove it ?","Let's suppose we have two rotations about two different axes represented by vectors $v_1$ and $v_2$: $R_1(v_1, \theta_1)$, $R_2(v_2,\theta_2)$. It's relatively easy to prove that composition of these two rotations gives rotation about axis $v_3$ distinct from axes $v_1$ and $v_2$ . Indeed if for example $v_3=v_1$ then $R_1(v_1, \theta_1) R_2(v_2,\theta_2)=R_3(v_1,\theta_3)$ leads to $R_2(v_2,\theta_2)=R_1^T(v_1, \theta_1)R_3(v_1,\theta_3)=R(v_1,\theta_3 -\theta_1)$ what gives $v_1=v_2$. ... Contradiction... We see that composition of two rotations about different axes always generates a new axis of rotation. The problem can be extended for condition of the plane generated by the axes. Question: Is it true that composition of two rotations generates the axis which   doesn't belong to the plane   which is constructed by the original axes of rotations ? How to prove it ? If the statement is not however true what are conditions for not changing a plane during the composition of rotations $ ^{[1]}$ ? $ ^{[1]}$ It can be observed that even in the case of quite regular rotations the above statement is true Let's take $Rot(z,\dfrac{\pi}{2})Rot(x,\dfrac{\pi}{2})= \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ \end{bmatrix} = Rot([1,1,1]^T, \dfrac{2}{3}\pi)$ or $Rot(x,  \pi  )Rot(z,  \pi  )= \begin{bmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & -1 \\ \end{bmatrix} \begin{bmatrix} -1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix} = \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \\ \end{bmatrix} = Rot( y, \pi)$ So I suppose it is generally true but how to prove it ?",,"['linear-algebra', 'matrices', 'rotations']"
30,"If $k$ is an eigenvalue of $A$ of algebraic multiplicity $r$, then is $p(k)$ an eigenvalue of $p(A)$ of algebraic multiplicity $r$?","If  is an eigenvalue of  of algebraic multiplicity , then is  an eigenvalue of  of algebraic multiplicity ?",k A r p(k) p(A) r,"Let $k \in \mathbb C$ be an eigenvalue of $A \in M(n,\mathbb C)$ of algebraic multiplicity $r$ (i.e. $k$ is an $r$-fold root of the characteristic polynomial of $A$). Let $p(x)$  be a polynomial with complex coefficients. Then is it true that $p(k)$ is an eigenvalue of $p(A)$ of algebraic multiplicity $r$ ? (I know that $p(k)$ is an eigenvalue of $p(A)$, but I am not sure about the algebraic multiplicity.)","Let $k \in \mathbb C$ be an eigenvalue of $A \in M(n,\mathbb C)$ of algebraic multiplicity $r$ (i.e. $k$ is an $r$-fold root of the characteristic polynomial of $A$). Let $p(x)$  be a polynomial with complex coefficients. Then is it true that $p(k)$ is an eigenvalue of $p(A)$ of algebraic multiplicity $r$ ? (I know that $p(k)$ is an eigenvalue of $p(A)$, but I am not sure about the algebraic multiplicity.)",,"['linear-algebra', 'matrices']"
31,Eigenvector corresponding to eigenvalue $ 1 $ of a stochastic matrix,Eigenvector corresponding to eigenvalue  of a stochastic matrix, 1 ,"I am trying to justify fact $ 5 $ in this link which states that if $ A $ is a column stochastic matrix, then $ A $ has eigenvalue $ 1 $ and a unique eigenvector such that all entries are either negative or positive. I have successfully proved that $ A $ has an eigenvalue $ 1 $ but still stuck on the second part of the fact. It seems that this is the Perron Frobenius theorem, but the proof for this theorem requires materials that are beyond a first course in linear algebra that I haven't learned about, so is there any way to prove this fact without using the PF theorem?","I am trying to justify fact $ 5 $ in this link which states that if $ A $ is a column stochastic matrix, then $ A $ has eigenvalue $ 1 $ and a unique eigenvector such that all entries are either negative or positive. I have successfully proved that $ A $ has an eigenvalue $ 1 $ but still stuck on the second part of the fact. It seems that this is the Perron Frobenius theorem, but the proof for this theorem requires materials that are beyond a first course in linear algebra that I haven't learned about, so is there any way to prove this fact without using the PF theorem?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'stochastic-matrices']"
32,Eigenvalues and eigenvectors of the Householder matrix $H = I - \frac{2}{u^Tu} uu^T$,Eigenvalues and eigenvectors of the Householder matrix,H = I - \frac{2}{u^Tu} uu^T,"So during my first revision for the semester exams, I went through exercises in books/internet and I found 2-3 that caught my eye. One of them was the following: Let $u \in \mathbb R^n$ be a non-zero column vector. Prove that the   matrix $$H = I - \frac{2}{u^Tu}uu^T$$ is symmetric and orthogonal. Then find the eigenvalues and   eigenvectors of $H$. Now first of all, I have already proved that $H$ is symmetric and orthogonal in 2 ways: By definition and by writing the $n$-form of the matrix $H$. After that, I feel that I am lost by trying to calculate the characteristic polynomial of the $n$-form of $H$ and then go the usual way (eigenvalues $\to$ eigenvectors). I am pretty sure I have to work by using the symmetric and orthonormal conditions that I proved first, but I can't get the hang of it. Any tip or help would be appreciated ! I cannot seem to understand why another question with another matrix equation was linked to this one, needless to say, I cannot even understand the answer. I am talking about a differently defined matrix here, with probably different properties and a differently defined question.","So during my first revision for the semester exams, I went through exercises in books/internet and I found 2-3 that caught my eye. One of them was the following: Let $u \in \mathbb R^n$ be a non-zero column vector. Prove that the   matrix $$H = I - \frac{2}{u^Tu}uu^T$$ is symmetric and orthogonal. Then find the eigenvalues and   eigenvectors of $H$. Now first of all, I have already proved that $H$ is symmetric and orthogonal in 2 ways: By definition and by writing the $n$-form of the matrix $H$. After that, I feel that I am lost by trying to calculate the characteristic polynomial of the $n$-form of $H$ and then go the usual way (eigenvalues $\to$ eigenvectors). I am pretty sure I have to work by using the symmetric and orthonormal conditions that I proved first, but I can't get the hang of it. Any tip or help would be appreciated ! I cannot seem to understand why another question with another matrix equation was linked to this one, needless to say, I cannot even understand the answer. I am talking about a differently defined matrix here, with probably different properties and a differently defined question.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'orthonormal']"
33,Gradient Chain Rule: Applying Gradient in the case of a Series of Matrix operations (Neural Net Gradient Calculation),Gradient Chain Rule: Applying Gradient in the case of a Series of Matrix operations (Neural Net Gradient Calculation),,"I have the following situation: I need to calculate the gradient of the Error of a CNN a few layers deep by hand.  Starting with the Error function, The $\operatorname{Error}[readoutX]= -\sum_i \sum_j {actualX_{ij} * \operatorname{log}(readoutX_{ij})}$ So, letting $actualX = \mathbf{a}$ and $readoutX = \mathbf{x}$, I need to take the gradient of the Error function. The error function written out more fully, is: $$ \begin{align} \operatorname[X] &= -\sum_i \sum_j a_{ij} * \operatorname{log}(x_{ij})\\ \mathbf{x} &= \operatorname{softmax}(\mathbf{B}\bullet \mathbf{A})\\ \mathbf{B} &= \mathbf{C} \bullet \mathbf{B}\\ \end{align} $$ $\text{ Where the actual dimensions of }\mathbf{A},\mathbf{B}, \text{ and }\mathbf{C} \text{ are } (1024,10), (1568,1024),(?,1568) \text{ respectively (}? = \text{number of images in batch}), \text{ and the softmax is defined as: }$ $$\sigma(\mathbf{x})_j = \frac{e^{x_j}}{\sum_{k=1}^{|\mathbf{x}|} e^{x_k}} \text{ for } j = 1,\ldots,|\mathbf{x}| $$ $\text{ with the partial derivative with respect to element }x_j:$ $$ \frac{\partial{\sigma(x)_i}}{\partial{x_j}} = \sigma(x)_i (\Delta_{ij} - \sigma(x)_j) $$ Next, I start to write the gradient: $$ \nabla\operatorname{Error}[\mathbf{x}] = \sum_i \sum_j {- \frac{\partial{(a_{ij} * \operatorname{log}(x_{ij}))}}{\partial{x_{ij}}}} = \sum_i\sum_j -a_{ij}\frac{\partial{\operatorname{log}(x_{ij})}}{\partial{x_{ij}}} = \sum_i\sum_j -a_{ij}\frac{1}{x_{ij}}\frac{\partial{x_{ij}}}{\partial{u}} $$ And I am stuck at: $\frac{\partial{x_{ij}}}{\partial{u}}$. I know that $\frac{\partial{x_{ij}}}{\partial{u}}$ should be a gradient, but I don't know how to implement it (I can just write the $\nabla$ in place of the partial and call it correct...but I have no idea if that is right...), and even after reading the Wikipedia article, I am not sure which chain rule applies here (or even if the first part of my gradient is correct).   In the past, I've done this using matrix derivatives, but it seems like the summation breaks that down... So, how do I write this first step...and transition to the next gradient (and then on to $\mathbf{C}$)...","I have the following situation: I need to calculate the gradient of the Error of a CNN a few layers deep by hand.  Starting with the Error function, The $\operatorname{Error}[readoutX]= -\sum_i \sum_j {actualX_{ij} * \operatorname{log}(readoutX_{ij})}$ So, letting $actualX = \mathbf{a}$ and $readoutX = \mathbf{x}$, I need to take the gradient of the Error function. The error function written out more fully, is: $$ \begin{align} \operatorname[X] &= -\sum_i \sum_j a_{ij} * \operatorname{log}(x_{ij})\\ \mathbf{x} &= \operatorname{softmax}(\mathbf{B}\bullet \mathbf{A})\\ \mathbf{B} &= \mathbf{C} \bullet \mathbf{B}\\ \end{align} $$ $\text{ Where the actual dimensions of }\mathbf{A},\mathbf{B}, \text{ and }\mathbf{C} \text{ are } (1024,10), (1568,1024),(?,1568) \text{ respectively (}? = \text{number of images in batch}), \text{ and the softmax is defined as: }$ $$\sigma(\mathbf{x})_j = \frac{e^{x_j}}{\sum_{k=1}^{|\mathbf{x}|} e^{x_k}} \text{ for } j = 1,\ldots,|\mathbf{x}| $$ $\text{ with the partial derivative with respect to element }x_j:$ $$ \frac{\partial{\sigma(x)_i}}{\partial{x_j}} = \sigma(x)_i (\Delta_{ij} - \sigma(x)_j) $$ Next, I start to write the gradient: $$ \nabla\operatorname{Error}[\mathbf{x}] = \sum_i \sum_j {- \frac{\partial{(a_{ij} * \operatorname{log}(x_{ij}))}}{\partial{x_{ij}}}} = \sum_i\sum_j -a_{ij}\frac{\partial{\operatorname{log}(x_{ij})}}{\partial{x_{ij}}} = \sum_i\sum_j -a_{ij}\frac{1}{x_{ij}}\frac{\partial{x_{ij}}}{\partial{u}} $$ And I am stuck at: $\frac{\partial{x_{ij}}}{\partial{u}}$. I know that $\frac{\partial{x_{ij}}}{\partial{u}}$ should be a gradient, but I don't know how to implement it (I can just write the $\nabla$ in place of the partial and call it correct...but I have no idea if that is right...), and even after reading the Wikipedia article, I am not sure which chain rule applies here (or even if the first part of my gradient is correct).   In the past, I've done this using matrix derivatives, but it seems like the summation breaks that down... So, how do I write this first step...and transition to the next gradient (and then on to $\mathbf{C}$)...",,"['matrices', 'vector-analysis', 'matrix-calculus', 'neural-networks']"
34,Eigenvalues of $3 \times 3$ block matrix,Eigenvalues of  block matrix,3 \times 3,"What are the eigenvalues of the following block matrix? $$\begin{bmatrix} A & I_n & I_n \\  I_n & I_n & O_n \\ I_n & O_n & I_n  \end{bmatrix}$$ Here, $A$ is any square matrix of order $n$ whose eigenvalues are denoted by $\lambda_1, \lambda_2, \dots, \lambda_n$ , $I_n$ is an identity matrix of order $n$ and $O_n$ is a zero matrix of order $n$ .","What are the eigenvalues of the following block matrix? Here, is any square matrix of order whose eigenvalues are denoted by , is an identity matrix of order and is a zero matrix of order .","\begin{bmatrix}
A & I_n & I_n \\ 
I_n & I_n & O_n \\
I_n & O_n & I_n 
\end{bmatrix} A n \lambda_1, \lambda_2, \dots, \lambda_n I_n n O_n n","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
35,"Is it okay to determine pivot positions in a matrix in echelon form, not in reduced echelon form?","Is it okay to determine pivot positions in a matrix in echelon form, not in reduced echelon form?",,"Definition A rectangular matrix is in echelon form (or row echelon form) if it has the   following three properties: 1. All nonzero rows are above any rows of all zeros. 2. Each leading entry of a row is in a column to the right of the leading entry of the row above it. 3. All entries in a column below a leading entry are zeros. If a matrix in echelon form satisfies the following additional conditions, then it is in reduced echelon form (or reduced row echelon form): 4. The leading entry in each nonzero row is 1. 5. Each leading 1 is the only nonzero entry in its column. Definition A pivot position in a matrix A is a location in A that corresponds to a leading 1 in the reduced echelon form of A. A pivot column is a column of A that contains a pivot position. Source: Linear Algebra and Its Applications, David C. Lay In the solution,  $\begin{bmatrix} 1 &4 &5 &-9 &7 \\ 0 &2 &4 &-6 &-6 \\ 0 &0 &0 &-5 &0 \\ 0 &0 &0 &0  &0 \\ \end{bmatrix} \space \text{are not in reduced echelon form}\\$ The author haven't yet checked if the positions in 2 and -5 satisfy the definition of pivot position when the echelon matrix is reduced to a reduced echelon form. I think if it becomes the reduced echelon form, the columns above the pivot positions, the author determined, can be nonzeros and the entries in pivot positions, author dermined, can be $0$ and or have nonzero entry to its left. So author's determining from the above matrix that leading 1, 2, -5 are pivot positions without reducing the echelon is wrong. Is there something I'm missing? Or doesn't it have a chance of getting a different pivot position when the echelon form is changed into a reduced echelon form? [EDIT  I thinked again after reading your explanation] In a $m×n$ matrix in echelon form of a linear system for some positive integers m, n, let the leading entries $(■)$ have any nonzero value, and the starred entries $(☆)$ have any value including zero. Leading entries $■$s in $R_1$ and $R_2$ in an echelon matrix can become leading 1 in a reduced echelon matrix through dividing them by $■$, and the entry ☆ in $R_1$ above $■$ in $R_2$ can be $0$ by subtracting a multiple of $■$. So $R_1$ and $R_2$ in a matrix in echelon form becomes as follows: $\begin{array}{rcl}  R_1\space & [■ ☆\cdots ☆☆☆☆]\\ R_2\space & [0 ■\cdots ☆☆☆☆]\end{array} \qquad ~ \begin{array}{rcl} R_1\space & [1 0\cdots ☆☆☆☆]\\R_2 &[0 1\cdots ☆☆☆☆]  \end{array}$ For all integers k with $2≤k<m$, $R_k$, $R_{k+1}$ in the echelon matrix can be expressed as $R_{k}\space$ $[0 \cdots 0 ■☆☆\cdots ☆]$ $R_{k+1}$     $[0 \cdots 0 0 ■☆\cdots ☆]$. Subtracting a multiple of leading entry of $R_{k+1}$ from  $R_k$ can make the entry above leading $■$ in $R_{k+1}$ be zero, and the leading $■$s in $R_k$, $R_{k+1}$ can be 1 through dividing the rows by leading entry $■$s. So the rows in echelon matrix become the following in reduced $m×n$ echelon matrix: $\begin{array}{rcl}  R_{k}\space & [0 \cdots 0 ■☆☆\cdots ☆]\\   R_{k+1} &    [0 \cdots 0 0 ■☆\cdots ☆]\\  \end{array} \qquad  \begin{array}{rcl}  R_{k} & [0 \cdots 0 1 0 ☆\cdots ☆]\\ R_{k+1} & [0 \cdots 0 0 1 ☆\cdots ☆]\\  \end{array}$ Hence, it's found that leading 1s in reduced echelon form of $m×n$ matrix of a linear system correspond to the locations of the leading non-zero values in a $m×n$ matrix in echelon form of the linear system.","Definition A rectangular matrix is in echelon form (or row echelon form) if it has the   following three properties: 1. All nonzero rows are above any rows of all zeros. 2. Each leading entry of a row is in a column to the right of the leading entry of the row above it. 3. All entries in a column below a leading entry are zeros. If a matrix in echelon form satisfies the following additional conditions, then it is in reduced echelon form (or reduced row echelon form): 4. The leading entry in each nonzero row is 1. 5. Each leading 1 is the only nonzero entry in its column. Definition A pivot position in a matrix A is a location in A that corresponds to a leading 1 in the reduced echelon form of A. A pivot column is a column of A that contains a pivot position. Source: Linear Algebra and Its Applications, David C. Lay In the solution,  $\begin{bmatrix} 1 &4 &5 &-9 &7 \\ 0 &2 &4 &-6 &-6 \\ 0 &0 &0 &-5 &0 \\ 0 &0 &0 &0  &0 \\ \end{bmatrix} \space \text{are not in reduced echelon form}\\$ The author haven't yet checked if the positions in 2 and -5 satisfy the definition of pivot position when the echelon matrix is reduced to a reduced echelon form. I think if it becomes the reduced echelon form, the columns above the pivot positions, the author determined, can be nonzeros and the entries in pivot positions, author dermined, can be $0$ and or have nonzero entry to its left. So author's determining from the above matrix that leading 1, 2, -5 are pivot positions without reducing the echelon is wrong. Is there something I'm missing? Or doesn't it have a chance of getting a different pivot position when the echelon form is changed into a reduced echelon form? [EDIT  I thinked again after reading your explanation] In a $m×n$ matrix in echelon form of a linear system for some positive integers m, n, let the leading entries $(■)$ have any nonzero value, and the starred entries $(☆)$ have any value including zero. Leading entries $■$s in $R_1$ and $R_2$ in an echelon matrix can become leading 1 in a reduced echelon matrix through dividing them by $■$, and the entry ☆ in $R_1$ above $■$ in $R_2$ can be $0$ by subtracting a multiple of $■$. So $R_1$ and $R_2$ in a matrix in echelon form becomes as follows: $\begin{array}{rcl}  R_1\space & [■ ☆\cdots ☆☆☆☆]\\ R_2\space & [0 ■\cdots ☆☆☆☆]\end{array} \qquad ~ \begin{array}{rcl} R_1\space & [1 0\cdots ☆☆☆☆]\\R_2 &[0 1\cdots ☆☆☆☆]  \end{array}$ For all integers k with $2≤k<m$, $R_k$, $R_{k+1}$ in the echelon matrix can be expressed as $R_{k}\space$ $[0 \cdots 0 ■☆☆\cdots ☆]$ $R_{k+1}$     $[0 \cdots 0 0 ■☆\cdots ☆]$. Subtracting a multiple of leading entry of $R_{k+1}$ from  $R_k$ can make the entry above leading $■$ in $R_{k+1}$ be zero, and the leading $■$s in $R_k$, $R_{k+1}$ can be 1 through dividing the rows by leading entry $■$s. So the rows in echelon matrix become the following in reduced $m×n$ echelon matrix: $\begin{array}{rcl}  R_{k}\space & [0 \cdots 0 ■☆☆\cdots ☆]\\   R_{k+1} &    [0 \cdots 0 0 ■☆\cdots ☆]\\  \end{array} \qquad  \begin{array}{rcl}  R_{k} & [0 \cdots 0 1 0 ☆\cdots ☆]\\ R_{k+1} & [0 \cdots 0 0 1 ☆\cdots ☆]\\  \end{array}$ Hence, it's found that leading 1s in reduced echelon form of $m×n$ matrix of a linear system correspond to the locations of the leading non-zero values in a $m×n$ matrix in echelon form of the linear system.",,"['linear-algebra', 'matrices']"
36,Testing the diagonalizability of matrix $B=\left(\begin{smallmatrix}\lambda_1 &a&b\\0&\lambda_1 & c\\0&0& \lambda_2\end{smallmatrix}\right)$,Testing the diagonalizability of matrix,B=\left(\begin{smallmatrix}\lambda_1 &a&b\\0&\lambda_1 & c\\0&0& \lambda_2\end{smallmatrix}\right),"How to show that the matrix $$B= \left(\begin{array}(\lambda_1 & a & b \\ 0 & \lambda_1 & c\\ 0 & 0 & \lambda_2\end{array}\right)$$ is diagonalizable when $a\neq0$, when $\lambda_1\neq \lambda_2$. How should I work this out? I tried comparing the algebraic and geometric multiplicities of the eigenvalues of $B$ but I didn't succeed in this approach yet.","How to show that the matrix $$B= \left(\begin{array}(\lambda_1 & a & b \\ 0 & \lambda_1 & c\\ 0 & 0 & \lambda_2\end{array}\right)$$ is diagonalizable when $a\neq0$, when $\lambda_1\neq \lambda_2$. How should I work this out? I tried comparing the algebraic and geometric multiplicities of the eigenvalues of $B$ but I didn't succeed in this approach yet.",,"['linear-algebra', 'matrices', 'diagonalization']"
37,Row swapping through matrix multiplication,Row swapping through matrix multiplication,,Let's say I have a matrix \begin{bmatrix}a&b\\c&d\end{bmatrix} What would I have the multiply the matrix above by to obtain the following? **\begin{bmatrix}c&d\\a&b\end{bmatrix},Let's say I have a matrix \begin{bmatrix}a&b\\c&d\end{bmatrix} What would I have the multiply the matrix above by to obtain the following? **\begin{bmatrix}c&d\\a&b\end{bmatrix},,"['linear-algebra', 'matrices']"
38,Determinant of an unknown matrix.,Determinant of an unknown matrix.,,"Let $x, y$ be two real variables. If $A$ is any $n\times n$ matrix with all entries in the set $\{x,y\}$ then prove that \begin{equation}  \det A = (x-y)^{n-1}(Px + (-1)^{n-1}Qy) \end{equation} where $P,Q$ are integers defined by \begin{equation}  P = \det A\big|_{x=1,y=0} \quad Q = \det A\big|_{x=0,y=1}. \end{equation} I tried to do this by using induction and started for $n=2.$ But I am confused about the matrix that it will form since there are more than one possibility for a $2\times 2$ matrix. For example \begin{equation}   \begin{bmatrix}     x & y  \\     y & x    \end{bmatrix}, \begin{bmatrix}     x & x  \\     y & x    \end{bmatrix}, \begin{bmatrix}     x & y  \\     x & y    \end{bmatrix}, \begin{bmatrix}     y & y  \\     y & x    \end{bmatrix},... \end{equation} There are 12 more possibilities. I have checked couple of cases and the result holds but I have no idea how to prove it in generality without checking each case. I would really appreciate any help. Also if you can give me some reference from where this type of problem is taken that will also help me a lot. Thanks in advance.","Let $x, y$ be two real variables. If $A$ is any $n\times n$ matrix with all entries in the set $\{x,y\}$ then prove that \begin{equation}  \det A = (x-y)^{n-1}(Px + (-1)^{n-1}Qy) \end{equation} where $P,Q$ are integers defined by \begin{equation}  P = \det A\big|_{x=1,y=0} \quad Q = \det A\big|_{x=0,y=1}. \end{equation} I tried to do this by using induction and started for $n=2.$ But I am confused about the matrix that it will form since there are more than one possibility for a $2\times 2$ matrix. For example \begin{equation}   \begin{bmatrix}     x & y  \\     y & x    \end{bmatrix}, \begin{bmatrix}     x & x  \\     y & x    \end{bmatrix}, \begin{bmatrix}     x & y  \\     x & y    \end{bmatrix}, \begin{bmatrix}     y & y  \\     y & x    \end{bmatrix},... \end{equation} There are 12 more possibilities. I have checked couple of cases and the result holds but I have no idea how to prove it in generality without checking each case. I would really appreciate any help. Also if you can give me some reference from where this type of problem is taken that will also help me a lot. Thanks in advance.",,"['linear-algebra', 'matrices', 'determinant']"
39,Operator norm increases under taking absolute value of all entries of a matrix,Operator norm increases under taking absolute value of all entries of a matrix,,"Let $\|A\|:=\sup_{\|v\|=1} \|Av\|$ denote the operator norm induced by the Euclidean distance. If $B$ is a matrix such that $B_{ij} = |A_{ij}|,$ show that $$\|B\|\geq\|A\|.$$","Let $\|A\|:=\sup_{\|v\|=1} \|Av\|$ denote the operator norm induced by the Euclidean distance. If $B$ is a matrix such that $B_{ij} = |A_{ij}|,$ show that $$\|B\|\geq\|A\|.$$",,['matrices']
40,Is the inverse of an invertible circulant matrix also circulant?,Is the inverse of an invertible circulant matrix also circulant?,,"The circulant matrices in $M_n(F)$ ($F$ field) form a subspace $\mathcal C_n$ spanned by $I,J,J^2,\cdots,J^{n-1}$ where $$J=\begin{bmatrix} O & I_{n-1}\\1 &  O\end{bmatrix}$$ This subspace $\mathcal C_n$ is also closed under multiplication, which makes it a subring of $M_n(F)$. Now my question is, if $A\in \mathcal C_n$ and $A$ is non-singular, can we assert that $A^{-1}\in\mathcal C_n$ too? I'm in particular encouraged to make this guess by observing  these examples: $$\begin{bmatrix}0 & 1 & 1 & \cdots & 1\\ 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 0 & \cdots & 1\\ \vdots & \vdots & \vdots & & \vdots \\ 1 & 1 & 1 & \cdots & 0 \end{bmatrix}^{-1}=\frac{1}{n-1}\begin{bmatrix} 2-n & 1 & 1 & \cdots & 1\\ 1 & 2-n & 1 & \cdots & 1 \\ 1 & 1 & 2-n & \cdots & 1\\ \vdots & \vdots & \vdots & & \vdots \\ 1 & 1 & 1 & \cdots & 2-n \end{bmatrix}$$ and $$ \begin{bmatrix}    1 & 2 & 3 & \cdots & n-1 & n \\    n & 1 & 2 & \cdots & n-2 & n-1\\    n-1 & n & 1 & \cdots & n-3 & n-2 \\    \vdots & \vdots & \vdots & & \vdots & \vdots \\    2 & 3 & 4 & \cdots & n & 1 \end{bmatrix}^{-1}= \frac1{ns} \begin{bmatrix}   1-s & 1+s & 1 & \cdots & 1 & 1\\   1 & 1-s & 1+s & \cdots & 1 & 1 \\   1 & 1 & 1-s & \cdots & 1 & 1\\   \vdots & \vdots & \vdots & & \vdots & \vdots \\   1+s & 1 & 1 & \cdots & 1 & 1-s \end{bmatrix} $$ in which $s:=1+2+\cdots+n$.","The circulant matrices in $M_n(F)$ ($F$ field) form a subspace $\mathcal C_n$ spanned by $I,J,J^2,\cdots,J^{n-1}$ where $$J=\begin{bmatrix} O & I_{n-1}\\1 &  O\end{bmatrix}$$ This subspace $\mathcal C_n$ is also closed under multiplication, which makes it a subring of $M_n(F)$. Now my question is, if $A\in \mathcal C_n$ and $A$ is non-singular, can we assert that $A^{-1}\in\mathcal C_n$ too? I'm in particular encouraged to make this guess by observing  these examples: $$\begin{bmatrix}0 & 1 & 1 & \cdots & 1\\ 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 0 & \cdots & 1\\ \vdots & \vdots & \vdots & & \vdots \\ 1 & 1 & 1 & \cdots & 0 \end{bmatrix}^{-1}=\frac{1}{n-1}\begin{bmatrix} 2-n & 1 & 1 & \cdots & 1\\ 1 & 2-n & 1 & \cdots & 1 \\ 1 & 1 & 2-n & \cdots & 1\\ \vdots & \vdots & \vdots & & \vdots \\ 1 & 1 & 1 & \cdots & 2-n \end{bmatrix}$$ and $$ \begin{bmatrix}    1 & 2 & 3 & \cdots & n-1 & n \\    n & 1 & 2 & \cdots & n-2 & n-1\\    n-1 & n & 1 & \cdots & n-3 & n-2 \\    \vdots & \vdots & \vdots & & \vdots & \vdots \\    2 & 3 & 4 & \cdots & n & 1 \end{bmatrix}^{-1}= \frac1{ns} \begin{bmatrix}   1-s & 1+s & 1 & \cdots & 1 & 1\\   1 & 1-s & 1+s & \cdots & 1 & 1 \\   1 & 1 & 1-s & \cdots & 1 & 1\\   \vdots & \vdots & \vdots & & \vdots & \vdots \\   1+s & 1 & 1 & \cdots & 1 & 1-s \end{bmatrix} $$ in which $s:=1+2+\cdots+n$.",,"['linear-algebra', 'matrices']"
41,"Find a real orthogonal matrix of order $3$ , other than +- I_3 ,having all integer elements. [closed]","Find a real orthogonal matrix of order  , other than +- I_3 ,having all integer elements. [closed]",3,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find a real orthogonal matrix of order $3$ ,other than +- I3 having all integer elements. I have no idea to solve the problem. I don't know how to start. If $A$ be such matrix then $AA^T=A^TA=I_3$ . Please help me.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find a real orthogonal matrix of order ,other than +- I3 having all integer elements. I have no idea to solve the problem. I don't know how to start. If be such matrix then . Please help me.",3 A AA^T=A^TA=I_3,"['linear-algebra', 'matrices', 'real-numbers', 'integers', 'orthogonal-matrices']"
42,Matrix functions of a non-diagonalizable matrix,Matrix functions of a non-diagonalizable matrix,,"Let $A$ be the following $3 \times 3$ matrix: $$ A = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \\ \end{pmatrix} $$ I'm supposed to calculate $A^n$, where $n \in \Bbb R$, $\exp(tA)$ and $\sin(\pi A)$. Obviously $A$ is not diagonalizable. Since we haven't had anything about Jordan decomposition in the lecture, I'm not sure how to solve this. The eigenvalues $\lambda_1 = -1 , \lambda_{2,3} = 1$ can be read off. I tried to expand the two eigenvectors into a orthonormal basis, i.e.: $$ \mathbf{x}_{\lambda_1} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ \end{pmatrix} \qquad \mathbf{x}_{\lambda_2} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \end{pmatrix} \qquad \mathbf{x}_3 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \end{pmatrix} $$ But I'm rather unsure how to continue. I suspect that $$ A^n = \begin{pmatrix} 1 & n & 0 \\ 0 & 1 & 0 \\ 0 & 0 & (-1)^n \\ \end{pmatrix} \qquad \text{for} \qquad n \in \Bbb N_0, $$ But how to expand this to $n \in \Bbb R$? In general, how can I solve such a problem of matrix functions, if I've not heard anything about Jordan decomposition? EDIT: Thanks for your help. I could show that the above mentioned matrix for $A^n$ is correct even for $n \in \Bbb Z$. The two other functions are straightforward then. If someone has an idea or hint about $A^n$ for $n \notin \Bbb Z$, i would appreciate it.","Let $A$ be the following $3 \times 3$ matrix: $$ A = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \\ \end{pmatrix} $$ I'm supposed to calculate $A^n$, where $n \in \Bbb R$, $\exp(tA)$ and $\sin(\pi A)$. Obviously $A$ is not diagonalizable. Since we haven't had anything about Jordan decomposition in the lecture, I'm not sure how to solve this. The eigenvalues $\lambda_1 = -1 , \lambda_{2,3} = 1$ can be read off. I tried to expand the two eigenvectors into a orthonormal basis, i.e.: $$ \mathbf{x}_{\lambda_1} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ \end{pmatrix} \qquad \mathbf{x}_{\lambda_2} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \end{pmatrix} \qquad \mathbf{x}_3 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \end{pmatrix} $$ But I'm rather unsure how to continue. I suspect that $$ A^n = \begin{pmatrix} 1 & n & 0 \\ 0 & 1 & 0 \\ 0 & 0 & (-1)^n \\ \end{pmatrix} \qquad \text{for} \qquad n \in \Bbb N_0, $$ But how to expand this to $n \in \Bbb R$? In general, how can I solve such a problem of matrix functions, if I've not heard anything about Jordan decomposition? EDIT: Thanks for your help. I could show that the above mentioned matrix for $A^n$ is correct even for $n \in \Bbb Z$. The two other functions are straightforward then. If someone has an idea or hint about $A^n$ for $n \notin \Bbb Z$, i would appreciate it.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
43,A complex matrix $A$ such that. $A^3 = A^2 \neq 0$,A complex matrix  such that.,A A^3 = A^2 \neq 0,"Suppose $A$ is a $4×4$ matrix over $C$ s.t. $Rank(A)=2$ and $A^3=A^2\neq0$. If $A$ is not diagonalizable then how to prove that: There exists a vector $v$ s.t. $Av\neq 0$ and $A^2v=0$. I know it is to be proved that $Imsp(A)$ is contained in $Nullsp(A)$, but really got no clue how to approach. My work: $x^2(x-1)$ is  the annihilating polynomial for $A$, but I am stuck in finding the characteristic polynomial. The only two possibilities are $x^3(x-1)$ and $x^2(x-1)^2$, but how to reject the later one? Thanks for any hint.","Suppose $A$ is a $4×4$ matrix over $C$ s.t. $Rank(A)=2$ and $A^3=A^2\neq0$. If $A$ is not diagonalizable then how to prove that: There exists a vector $v$ s.t. $Av\neq 0$ and $A^2v=0$. I know it is to be proved that $Imsp(A)$ is contained in $Nullsp(A)$, but really got no clue how to approach. My work: $x^2(x-1)$ is  the annihilating polynomial for $A$, but I am stuck in finding the characteristic polynomial. The only two possibilities are $x^3(x-1)$ and $x^2(x-1)^2$, but how to reject the later one? Thanks for any hint.",,"['linear-algebra', 'matrices']"
44,2nd degree matrix equation,2nd degree matrix equation,,Let $X$ be a matrix with 2 rows and 2 columns. Solve the following equation: $$ X^2 = \begin{pmatrix} 3 & 5\\  -5 & 8 \end{pmatrix} $$ Here is what I did: Let $ X = \begin{pmatrix} a & b\\  c & d \end{pmatrix} $. After multiplying I got the following system: $$ \left\{\begin{matrix} a^2 + bc = 3\\  ab + bd = 5\\  ac + cd = -5\\  d^2 + bc = 8 \end{matrix}\right. $$ At this point I got stucked. If you know how to solve this please help me! Thank you!,Let $X$ be a matrix with 2 rows and 2 columns. Solve the following equation: $$ X^2 = \begin{pmatrix} 3 & 5\\  -5 & 8 \end{pmatrix} $$ Here is what I did: Let $ X = \begin{pmatrix} a & b\\  c & d \end{pmatrix} $. After multiplying I got the following system: $$ \left\{\begin{matrix} a^2 + bc = 3\\  ab + bd = 5\\  ac + cd = -5\\  d^2 + bc = 8 \end{matrix}\right. $$ At this point I got stucked. If you know how to solve this please help me! Thank you!,,"['matrices', 'systems-of-equations']"
45,Transforming matrix to remove an eigenvalue,Transforming matrix to remove an eigenvalue,,"Suppose I have a square symmetric matrix $A$, whose largest eigenvalue is $\lambda_1$ and the corresponding eigenvector is $v_1$. Also, suppose the second largest eigenvalue is $\lambda_2$ and the corresponding eigenvector is $v_2$, but the second largest eigenvalue is not known (not yet calculated). Can I transform the matrix $A$ to get another matrix $A_2$ whose largest eigenvalue is now $\lambda_2$?","Suppose I have a square symmetric matrix $A$, whose largest eigenvalue is $\lambda_1$ and the corresponding eigenvector is $v_1$. Also, suppose the second largest eigenvalue is $\lambda_2$ and the corresponding eigenvector is $v_2$, but the second largest eigenvalue is not known (not yet calculated). Can I transform the matrix $A$ to get another matrix $A_2$ whose largest eigenvalue is now $\lambda_2$?",,"['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
46,Prove that Q is also upper Hessenberg in A = QR,Prove that Q is also upper Hessenberg in A = QR,,"Background: Suppose $\mathbf{A}$ is an $n \times n$ matrix and it is upper Hessenberg. Using QR-factorization, we have $\mathbf{A=QR}$, where $\mathbf{R}$ is an upper triangular matrix and $\mathbf{Q}$ is orthogonal. Problem: Prove that $\mathbf{Q}$ is upper Hessenberg. I have little idea to move on. Does it construct the $\mathbf{Q}$ directly by writing entries of both sides of $\mathbf{A=QR}$ and compare the entries? Thank you in advance.","Background: Suppose $\mathbf{A}$ is an $n \times n$ matrix and it is upper Hessenberg. Using QR-factorization, we have $\mathbf{A=QR}$, where $\mathbf{R}$ is an upper triangular matrix and $\mathbf{Q}$ is orthogonal. Problem: Prove that $\mathbf{Q}$ is upper Hessenberg. I have little idea to move on. Does it construct the $\mathbf{Q}$ directly by writing entries of both sides of $\mathbf{A=QR}$ and compare the entries? Thank you in advance.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
47,"Proof for $A,B \in M_n(\mathbb{F})$ that if $[A,B]=tA$ for $0\neq t\in\mathbb{F}$, then $A^n=0$ [duplicate]","Proof for  that if  for , then  [duplicate]","A,B \in M_n(\mathbb{F}) [A,B]=tA 0\neq t\in\mathbb{F} A^n=0","This question already has answers here : If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent [duplicate] (2 answers) Closed 8 years ago . Statement. Suppose we have a square matrices $A,B$ of order $n$ over a field $\mathbb{F}$ of characteristics $0$ or $p>n$. If $[A,B]=AB-BA=tA$ for some nonzero $t\in\mathbb{F}$, then $A^n=0$. The problem is to prove this statement. Where I've got so far. There is an idea to take trace of both sides of equation: $$ \mathrm{tr}\,(AB-BA)=\mathrm{tr}\,(tA) $$ $$ \mathrm{tr}\,(AB)-\mathrm{tr}\,(BA)=t\times\mathrm{tr}\,(A) $$ since $\mathrm{tr}\,(AB) = \mathrm{tr}\,(BA)$ (by properties of the trace) we have $$ t\times A = \mathrm{tr}\,(A) $$ and since $t\neq 0$, we have $\mathrm{tr}\,(A) = 0$. That's all I could get form it. What sould I do next?","This question already has answers here : If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent [duplicate] (2 answers) Closed 8 years ago . Statement. Suppose we have a square matrices $A,B$ of order $n$ over a field $\mathbb{F}$ of characteristics $0$ or $p>n$. If $[A,B]=AB-BA=tA$ for some nonzero $t\in\mathbb{F}$, then $A^n=0$. The problem is to prove this statement. Where I've got so far. There is an idea to take trace of both sides of equation: $$ \mathrm{tr}\,(AB-BA)=\mathrm{tr}\,(tA) $$ $$ \mathrm{tr}\,(AB)-\mathrm{tr}\,(BA)=t\times\mathrm{tr}\,(A) $$ since $\mathrm{tr}\,(AB) = \mathrm{tr}\,(BA)$ (by properties of the trace) we have $$ t\times A = \mathrm{tr}\,(A) $$ and since $t\neq 0$, we have $\mathrm{tr}\,(A) = 0$. That's all I could get form it. What sould I do next?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'symmetric-polynomials', 'nilpotence']"
48,The Proximal Operator of the $ {L}_{1} $ Norm of Matrix Multiplication,The Proximal Operator of the  Norm of Matrix Multiplication, {L}_{1} ,"I hope to solve this problem. $$\min \quad \left\| CX \right\|_{1} $$ $$ \text{s.t.}\quad AX=b, X >0 $$ where $C \in \mathbb{R}^{m \times m}$, $X \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $b \in \mathbb{R}^{k \times n}$. $C$ is known weight, $X$ is unknown matrix. My problem is how to calculate the proximal operator of $ \left\| CX \right\|_{1}$, I know, if without $C$ the proximal operator will be apply Shrinkage elementwise. This problem will be easy if $x$ is a vector, we just need to solve a LP, but my $X$ is a matrix. $$ \min \quad c^Tx $$   $$ \text{s.t.}\quad Ax=b , x>0 $$ the overall problem I hope to solve is: $$ \min \left\| CX \right\|_{1} + \lambda \left\| Y \right\|_{*} $$ $$ \text{s.t.}\quad AX+Y=b , X>0 $$ Y has the same dimension with $b \in \mathbb{R}^{k \times n}$. X is known to be sparse.","I hope to solve this problem. $$\min \quad \left\| CX \right\|_{1} $$ $$ \text{s.t.}\quad AX=b, X >0 $$ where $C \in \mathbb{R}^{m \times m}$, $X \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $b \in \mathbb{R}^{k \times n}$. $C$ is known weight, $X$ is unknown matrix. My problem is how to calculate the proximal operator of $ \left\| CX \right\|_{1}$, I know, if without $C$ the proximal operator will be apply Shrinkage elementwise. This problem will be easy if $x$ is a vector, we just need to solve a LP, but my $X$ is a matrix. $$ \min \quad c^Tx $$   $$ \text{s.t.}\quad Ax=b , x>0 $$ the overall problem I hope to solve is: $$ \min \left\| CX \right\|_{1} + \lambda \left\| Y \right\|_{*} $$ $$ \text{s.t.}\quad AX+Y=b , X>0 $$ Y has the same dimension with $b \in \mathbb{R}^{k \times n}$. X is known to be sparse.",,"['linear-algebra', 'matrices', 'convex-optimization', 'normed-spaces', 'proximal-operators']"
49,Eigenvalues of Matrix Product.,Eigenvalues of Matrix Product.,,"Is there a relationship between the eigenvalues of individual matrices and the eigenvalues of their product? What about the special case when one of these matrices is a diagonal (positive) matrix? I think that this topic is very difficult but, maybe, it could exist some particular case in which the answer to this question is known. Any pointers will be very helpful. Thanks.","Is there a relationship between the eigenvalues of individual matrices and the eigenvalues of their product? What about the special case when one of these matrices is a diagonal (positive) matrix? I think that this topic is very difficult but, maybe, it could exist some particular case in which the answer to this question is known. Any pointers will be very helpful. Thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
50,"If $\lambda_{\max} = n$, then $n\times n$ positive, reciprocal matrix $A$ is consistent","If , then  positive, reciprocal matrix  is consistent",\lambda_{\max} = n n\times n A,"At the end of chapter 3 ( PDF ), the author states the following. Suppose we have $n \times n$ matrix $A$ having only positive elements and satisfying the property $a_{ij}=1/a_{ji}$ (a matrix satisfying this property is called a reciprocal matrix).If its largest eigenvalue $\lambda_{\max}$ is equal to $n$ , then the matrix $A$ satisfies the property (consistency property) $a_{ij} a_{jk} = a_{ik}$ where $i,j,k = 1,2,\dots,n$ . I'm not convinced this theorem is true. Could anyone help, please?","At the end of chapter 3 ( PDF ), the author states the following. Suppose we have matrix having only positive elements and satisfying the property (a matrix satisfying this property is called a reciprocal matrix).If its largest eigenvalue is equal to , then the matrix satisfies the property (consistency property) where . I'm not convinced this theorem is true. Could anyone help, please?","n \times n A a_{ij}=1/a_{ji} \lambda_{\max} n A a_{ij} a_{jk} = a_{ik} i,j,k = 1,2,\dots,n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
51,Formula to obtain a submatrix?,Formula to obtain a submatrix?,,"Beginning with an $n\times n$ matrix $A$, I would like to obtain a $m\times l$ submatrix, which need not be square. I'm aware of notation for submatrices, but are there any routine operations that can be used to algebraically obtain the submatrix? Might I be able to obtain matrices $B$ and $C$ so that $BAC$ gives the desired submatrix?","Beginning with an $n\times n$ matrix $A$, I would like to obtain a $m\times l$ submatrix, which need not be square. I'm aware of notation for submatrices, but are there any routine operations that can be used to algebraically obtain the submatrix? Might I be able to obtain matrices $B$ and $C$ so that $BAC$ gives the desired submatrix?",,"['linear-algebra', 'matrices', 'linear-transformations']"
52,The inverse of a matrix in which the sum of each row is $1$,The inverse of a matrix in which the sum of each row is,1,Let $A$ be an invertible $10\times 10$ matrix with real entries such that the sum of each row is $1$. Then choose the correct option. The sum of the entries of each row of the inverse of $A$ is $1$. The sum of the entries of each column of the inverse of $ A$ is $1$. The trace of the inverse of $A$ is non-zero. None of the above. If the matrix is given we can find its inverse but how can we find its inverse if the matrix itself not given? Any idea on how to find the answer?,Let $A$ be an invertible $10\times 10$ matrix with real entries such that the sum of each row is $1$. Then choose the correct option. The sum of the entries of each row of the inverse of $A$ is $1$. The sum of the entries of each column of the inverse of $ A$ is $1$. The trace of the inverse of $A$ is non-zero. None of the above. If the matrix is given we can find its inverse but how can we find its inverse if the matrix itself not given? Any idea on how to find the answer?,,"['linear-algebra', 'matrices']"
53,An invertible sparse matrix?,An invertible sparse matrix?,,"I'm not entirely certain about how to tackle this problem.... I hope you ladies and gents can help :) If $M\in M_{n\times n}(\mathbb{R})$ be such that every row has precisely tow non-zero entries, one is precisely equal to $1$ and the other is found in the diagonal and is strictly greater than one.  Must $M$ be invertible? My thoughts to date :) I believe the answer to be yes; reasoning: Intuition : for $n\leq 2$ $M$ can be readily calculated directly. Proof sketch idea: For arbitrary large $n$, I was thinking using the mini-max theorem to obtain a lower-bound on the smallest eigenvalue; and since all the non-zero entries are sufficiently large (at least 1); I would be done since then all eigenvalues must be strictly positive.... (But is the matrix Hermitian and how can I calculate this explicitly?)","I'm not entirely certain about how to tackle this problem.... I hope you ladies and gents can help :) If $M\in M_{n\times n}(\mathbb{R})$ be such that every row has precisely tow non-zero entries, one is precisely equal to $1$ and the other is found in the diagonal and is strictly greater than one.  Must $M$ be invertible? My thoughts to date :) I believe the answer to be yes; reasoning: Intuition : for $n\leq 2$ $M$ can be readily calculated directly. Proof sketch idea: For arbitrary large $n$, I was thinking using the mini-max theorem to obtain a lower-bound on the smallest eigenvalue; and since all the non-zero entries are sufficiently large (at least 1); I would be done since then all eigenvalues must be strictly positive.... (But is the matrix Hermitian and how can I calculate this explicitly?)",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'operator-theory']"
54,Suppose $AB=BA$ and $A^{1965}=B^{2015}=I$. Prove that $A+B+I $ is invertible.,Suppose  and . Prove that  is invertible.,AB=BA A^{1965}=B^{2015}=I A+B+I ,"Supppse $A $ abd $B $ are matrices, $AB=BA $ and $A^{1965}=B^{2015}=I $. Prove that $A+B+I $ is invertible. I want to prove that $(A+B+I)C=I $ I have no idea how to start. Can any one give some hint?","Supppse $A $ abd $B $ are matrices, $AB=BA $ and $A^{1965}=B^{2015}=I $. Prove that $A+B+I $ is invertible. I want to prove that $(A+B+I)C=I $ I have no idea how to start. Can any one give some hint?",,"['linear-algebra', 'matrices', 'contest-math']"
55,Characterize magic matrices in terms of their eigenvalues. A Magic Matrix over a field $F$ is a square matrix whose row and colums sums $c\in F$.,Characterize magic matrices in terms of their eigenvalues. A Magic Matrix over a field  is a square matrix whose row and colums sums .,F c\in F,"A Magic Matrix over a field $F$ is a square matrix whose row and colums sums $c\in F$ . Characterize magic matrices in terms of their eigenvalues. (Exercise 705 from Golan, The Linear Algebra a Beginning Graduate Student Ought to Know .) I know that $c$ is an eigenvalue and $[1,...,1]^{\sf{T}}$ is an eigenvector, but that is a ""property"", so how can I define all the magic matrices by their eigenvalues? Thanks!","A Magic Matrix over a field is a square matrix whose row and colums sums . Characterize magic matrices in terms of their eigenvalues. (Exercise 705 from Golan, The Linear Algebra a Beginning Graduate Student Ought to Know .) I know that is an eigenvalue and is an eigenvector, but that is a ""property"", so how can I define all the magic matrices by their eigenvalues? Thanks!","F c\in F c [1,...,1]^{\sf{T}}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'magic-square']"
56,minimal polynomial of linear transformation,minimal polynomial of linear transformation,,"Let $V$ and $W$ are finite dimensional vector spaces over $\mathbb{R}$ and let $ T_1:V\to  V$ and $ T_2:W\to W$ be linear transformations whose minimal polynomials are given by $ f_1(x)=x^3+x^2+x+1 , f_2(x)=x^4-x^2-2$. Let $ T:V\oplus W\to V\oplus W$ be defined by   $ T(v,w)=( T_1(v),T_2(w))$ $  for  (v,w)\in V\oplus W$ and let f(x) be minimal polynomial of T then how to find nullity of T and degree of f(x)?","Let $V$ and $W$ are finite dimensional vector spaces over $\mathbb{R}$ and let $ T_1:V\to  V$ and $ T_2:W\to W$ be linear transformations whose minimal polynomials are given by $ f_1(x)=x^3+x^2+x+1 , f_2(x)=x^4-x^2-2$. Let $ T:V\oplus W\to V\oplus W$ be defined by   $ T(v,w)=( T_1(v),T_2(w))$ $  for  (v,w)\in V\oplus W$ and let f(x) be minimal polynomial of T then how to find nullity of T and degree of f(x)?",,"['linear-algebra', 'matrices']"
57,Let A be an $n\times m$ matrix and B be an $m\times n$ matrix such that AB is invertible. Then which of the following is/are always true?,Let A be an  matrix and B be an  matrix such that AB is invertible. Then which of the following is/are always true?,n\times m m\times n,"Let A be an $n\times m$ matrix and B be an $m\times n$ matrix. For a square matrix D, let Tr(D) denote trace of D, |D| denote the determinant of D. Suppose that AB is invertible. Then which of the following is/are always true? (a) $Tr(AB) = Tr(BA)$. (b) $|AB| = |BA|$. (c) $m\geq n$. (d) BA must be invertible. For a start, I took several simple matrices, e.g. [1 0], [1 2]. The answer choices I could remove are (b) and (d). From what I knew, Tr(AB) = Tr(BA) is true for matrices of same order. So, is this true for a general case $n\times m$ and $m\times n$ as well ? Moreover, I did not expect $m\geq n$ to be true, which seems true using the above example matrices. How do I solve this properly ?","Let A be an $n\times m$ matrix and B be an $m\times n$ matrix. For a square matrix D, let Tr(D) denote trace of D, |D| denote the determinant of D. Suppose that AB is invertible. Then which of the following is/are always true? (a) $Tr(AB) = Tr(BA)$. (b) $|AB| = |BA|$. (c) $m\geq n$. (d) BA must be invertible. For a start, I took several simple matrices, e.g. [1 0], [1 2]. The answer choices I could remove are (b) and (d). From what I knew, Tr(AB) = Tr(BA) is true for matrices of same order. So, is this true for a general case $n\times m$ and $m\times n$ as well ? Moreover, I did not expect $m\geq n$ to be true, which seems true using the above example matrices. How do I solve this properly ?",,"['linear-algebra', 'matrices']"
58,Central extension of the Discrete Heisenberg group $H_3(\Bbb Z)$,Central extension of the Discrete Heisenberg group,H_3(\Bbb Z),"I want to use the  Discrete Heisenberg group $(H_3(\Bbb Z),\times)$ as an example for a presentation on central extensions. $H_3(\Bbb Z) = \begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}x,y,z\in \Bbb Z$ Now I have shown that the center of $H_3(\Bbb Z)$ is $Z = \begin{bmatrix}1&0&z\\0&1&0\\0&0&1 \end{bmatrix}$ I want a short exact sequence: $$I\hookrightarrow Z \hookrightarrow H_3(\Bbb Z) \twoheadrightarrow X\twoheadrightarrow I$$ Now for this to work, I need $X\cong H_3(\Bbb Z) / Z$ I believe, but I don't fully understand quotient groups apparently. Now I know that I can take this quotient group, since $Z$ is normal of course and I believe that we will get $$X= \begin{bmatrix}1&x&0\\0&1&y\\0&0&1 \end{bmatrix}$$ I can also see that $$Z^{-1}\begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}=\begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}Z^{-1}=X$$ Where $Z^{-1} = \begin{bmatrix}1&0&-z\\0&1&0\\0&0&1 \end{bmatrix}$ Does my guessed $X$ have $X\cong  H_3(\Bbb Z) / Z$ Can I let my maps be explicitly: $$I\overset{\phi_1}{\hookrightarrow} Z \overset{\phi_2}{\hookrightarrow} H_3(\Bbb Z) \overset{\phi_3}{\twoheadrightarrow} X\overset{\phi_4}{\twoheadrightarrow} I$$ $$\phi_1:A\mapsto A$$$$\phi_2:A\mapsto A$$$$\phi_3:A\mapsto Z^{-1}A$$$$\phi_4:A\mapsto I$$","I want to use the  Discrete Heisenberg group $(H_3(\Bbb Z),\times)$ as an example for a presentation on central extensions. $H_3(\Bbb Z) = \begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}x,y,z\in \Bbb Z$ Now I have shown that the center of $H_3(\Bbb Z)$ is $Z = \begin{bmatrix}1&0&z\\0&1&0\\0&0&1 \end{bmatrix}$ I want a short exact sequence: $$I\hookrightarrow Z \hookrightarrow H_3(\Bbb Z) \twoheadrightarrow X\twoheadrightarrow I$$ Now for this to work, I need $X\cong H_3(\Bbb Z) / Z$ I believe, but I don't fully understand quotient groups apparently. Now I know that I can take this quotient group, since $Z$ is normal of course and I believe that we will get $$X= \begin{bmatrix}1&x&0\\0&1&y\\0&0&1 \end{bmatrix}$$ I can also see that $$Z^{-1}\begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}=\begin{bmatrix}1&x&z\\0&1&y\\0&0&1 \end{bmatrix}Z^{-1}=X$$ Where $Z^{-1} = \begin{bmatrix}1&0&-z\\0&1&0\\0&0&1 \end{bmatrix}$ Does my guessed $X$ have $X\cong  H_3(\Bbb Z) / Z$ Can I let my maps be explicitly: $$I\overset{\phi_1}{\hookrightarrow} Z \overset{\phi_2}{\hookrightarrow} H_3(\Bbb Z) \overset{\phi_3}{\twoheadrightarrow} X\overset{\phi_4}{\twoheadrightarrow} I$$ $$\phi_1:A\mapsto A$$$$\phi_2:A\mapsto A$$$$\phi_3:A\mapsto Z^{-1}A$$$$\phi_4:A\mapsto I$$",,['abstract-algebra']
59,"If $BA$ has $-1$ as an eigenvalue, then so does $AB$?","If  has  as an eigenvalue, then so does ?",BA -1 AB,"I was just encountered with a rather tough problem as follows: Suppose $A,B\in M_n(\mathbb R)$, prove: $$\det(I_n+AB)\ne0\Rightarrow\det(I_n+BA)\ne0$$ Although at this moment I am still at a loss how to go about proving this, I seemed to have derived something that looks very stunning to me in my previous failed attemps: First, I think this one is obvious: $$\det(I_n+AB)\ne0\Leftrightarrow-1\text{ is not an eigenvalue of }AB$$ And likewise, $$\det(I_n+BA)\ne0\Leftrightarrow-1\text{ is not an eigenvalue of }BA$$ So what I'm asked to prove is actually equivalent to showing $$-1\text{ is not an eigenvalue of }AB\Rightarrow-1\text{ is not an eigenvalue of }BA$$ Taking the converse-negative, that is to say $$-1\text{ is an eigenvalue of }BA\Rightarrow-1\text{ is an eigenvalue of }AB$$ I cannot find any flaw in my reasoning. But if what I'm about to prove is true (I'm certain to say, yes it's true, because months ago I solved it in an extremely tricky way, which didn't, of course, follow my current threads), then it means for two arbitrary matrices $A,B$ of the same size, even if they don't commute, $AB$ and $BA$ will share $-1$ as an eigenvalue!! I think it is VERY unlikely. So could you please point out where the flaw of my reasoning lies? Or, could you help me prove this tough thing? Thanks in advance!","I was just encountered with a rather tough problem as follows: Suppose $A,B\in M_n(\mathbb R)$, prove: $$\det(I_n+AB)\ne0\Rightarrow\det(I_n+BA)\ne0$$ Although at this moment I am still at a loss how to go about proving this, I seemed to have derived something that looks very stunning to me in my previous failed attemps: First, I think this one is obvious: $$\det(I_n+AB)\ne0\Leftrightarrow-1\text{ is not an eigenvalue of }AB$$ And likewise, $$\det(I_n+BA)\ne0\Leftrightarrow-1\text{ is not an eigenvalue of }BA$$ So what I'm asked to prove is actually equivalent to showing $$-1\text{ is not an eigenvalue of }AB\Rightarrow-1\text{ is not an eigenvalue of }BA$$ Taking the converse-negative, that is to say $$-1\text{ is an eigenvalue of }BA\Rightarrow-1\text{ is an eigenvalue of }AB$$ I cannot find any flaw in my reasoning. But if what I'm about to prove is true (I'm certain to say, yes it's true, because months ago I solved it in an extremely tricky way, which didn't, of course, follow my current threads), then it means for two arbitrary matrices $A,B$ of the same size, even if they don't commute, $AB$ and $BA$ will share $-1$ as an eigenvalue!! I think it is VERY unlikely. So could you please point out where the flaw of my reasoning lies? Or, could you help me prove this tough thing? Thanks in advance!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
60,Matrix multiplication memorisation,Matrix multiplication memorisation,,"So I'm writing an exam about matrices in a few weeks time, and I'd like to know if anybody has any tips about multiplying matrices.","So I'm writing an exam about matrices in a few weeks time, and I'd like to know if anybody has any tips about multiplying matrices.",,"['matrices', 'self-learning', 'learning']"
61,Comparing Eigenvalues of Positive Semidefinite Matrices,Comparing Eigenvalues of Positive Semidefinite Matrices,,"If $B\succeq A$, show that $\lambda_n(B)\ge\lambda_n(A)$, where $\lambda_n$ is the $n$th largest eigenvalue. It is Theorem 6 in this paper but the proof is only given as ""by characterization."" Do they mean the min/max characterization of eigenvalues, $$\lambda_k=\min\max\frac{x^TAx}{x^Tx}$$ and if so, how is that applied? Thanks.","If $B\succeq A$, show that $\lambda_n(B)\ge\lambda_n(A)$, where $\lambda_n$ is the $n$th largest eigenvalue. It is Theorem 6 in this paper but the proof is only given as ""by characterization."" Do they mean the min/max characterization of eigenvalues, $$\lambda_k=\min\max\frac{x^TAx}{x^Tx}$$ and if so, how is that applied? Thanks.",,"['linear-algebra', 'matrices']"
62,Matrix derivatives of determinant and inverse related to $\mathbf{X}\mathbf{X}^{T}+\mathbf{C}$,Matrix derivatives of determinant and inverse related to,\mathbf{X}\mathbf{X}^{T}+\mathbf{C},"I would like to calculate the derivatives of determinant and inverse related to the term $\mathbf{X}\mathbf{X}^{T}+\mathbf{C}$ with respect to $\mathbf{X}$, where $\mathbf{C}$ is a constant matrix. Specifically, the derivative of the determinant, $$\frac{\partial \mathrm{ln}|\mathbf{X}\mathbf{X}^{T}+\mathbf{C}|}{\partial \mathbf{X}},$$ and the derivative of the inverse, $$\frac{\partial \mathbf{a}^{T}(\mathbf{X}\mathbf{X}^{T}+\mathbf{C})^{-1}\mathbf{a}}{\partial \mathbf{X}},$$ where $\mathbf{a}$ is a constant vector. I checked the Matrix Cookbook and some other resources but did not find the formulas specific for these cases and it seems the chain rule does not directly apply to the derivative of a matrix with respect to a matrix. So I post the question here and look forward to any clues! Thanks!","I would like to calculate the derivatives of determinant and inverse related to the term $\mathbf{X}\mathbf{X}^{T}+\mathbf{C}$ with respect to $\mathbf{X}$, where $\mathbf{C}$ is a constant matrix. Specifically, the derivative of the determinant, $$\frac{\partial \mathrm{ln}|\mathbf{X}\mathbf{X}^{T}+\mathbf{C}|}{\partial \mathbf{X}},$$ and the derivative of the inverse, $$\frac{\partial \mathbf{a}^{T}(\mathbf{X}\mathbf{X}^{T}+\mathbf{C})^{-1}\mathbf{a}}{\partial \mathbf{X}},$$ where $\mathbf{a}$ is a constant vector. I checked the Matrix Cookbook and some other resources but did not find the formulas specific for these cases and it seems the chain rule does not directly apply to the derivative of a matrix with respect to a matrix. So I post the question here and look forward to any clues! Thanks!",,"['matrices', 'determinant', 'inverse', 'matrix-calculus']"
63,Why the column space of a matrix is useful?,Why the column space of a matrix is useful?,,"I know what is the column space of a matrix: it is basically the subspace formed by the linear combinations of the columns (vectors) of a matrix. From wikipedia, we have the following nice picture: Which shows up the vectors that are used to make any linear combination to form the column space. So, my problem is not strictly related to the concept of column space of a matrix, but why is it useful? Before knowing the concept of column space, I already was used to find, for example, the RREF of a matrix, to see if the columns for a basis or are linearly independent or whatever... What's in the connection between column space and all those things?","I know what is the column space of a matrix: it is basically the subspace formed by the linear combinations of the columns (vectors) of a matrix. From wikipedia, we have the following nice picture: Which shows up the vectors that are used to make any linear combination to form the column space. So, my problem is not strictly related to the concept of column space of a matrix, but why is it useful? Before knowing the concept of column space, I already was used to find, for example, the RREF of a matrix, to see if the columns for a basis or are linearly independent or whatever... What's in the connection between column space and all those things?",,"['linear-algebra', 'matrices']"
64,"Good article (or book) about coordinate-dependent linear algebra, for those already familiar with coordinate-free aspects.","Good article (or book) about coordinate-dependent linear algebra, for those already familiar with coordinate-free aspects.",,"I have a decent understanding of coordinate- free linear algebra. For example: (not-necessarily-finite-dimensional) vector spaces, linear transforms, (possibly infinite) products of vector spaces, (possibly infinite) coproducts, biproducts, free vector spaces, the concepts of ""basis"" and dimension, subspaces, quotient spaces, multilinear maps, tensor products of vector spaces, the tensor-hom adjunction, canonical self-enrichment of $\mathbf{Vect}_K$, and dual spaces. At the same time, there's a lot of holes in my knowledge: Matrix normal forms are a subject I know almost nothing about If you say ""positive definite matrix"" I will stare at you blankly If you say ""special linear group"" I will stare at you blankly Matrix similarity / congruence / equivalence, um what? I feel like I have no understanding of bilinear mappings into $\mathbb{R}$, despite that they're ""the same"" as matrices. (Given a matrix $A,$ we get a bilinear mapping into $\mathbb{R}$ given by $y,x \mapsto y^T Ax.$ This process is an isomorphism of vector spaces.) These are mainly things that can be looked up on wikipedia, of course (except for the last dot point), but its tough to get the ""big picture"" and/or the geometric meaning without the help of a good article or book. Question. Can anyone recommend an article or book that specifically deals with coordinate- dependent linear algebra, such as matrices, in a sophisticated, mathematically-mature way, and which preferably takes abstract linear algebra for granted, and even uses it to help to express and clarify the coordinate dependent stuff?","I have a decent understanding of coordinate- free linear algebra. For example: (not-necessarily-finite-dimensional) vector spaces, linear transforms, (possibly infinite) products of vector spaces, (possibly infinite) coproducts, biproducts, free vector spaces, the concepts of ""basis"" and dimension, subspaces, quotient spaces, multilinear maps, tensor products of vector spaces, the tensor-hom adjunction, canonical self-enrichment of $\mathbf{Vect}_K$, and dual spaces. At the same time, there's a lot of holes in my knowledge: Matrix normal forms are a subject I know almost nothing about If you say ""positive definite matrix"" I will stare at you blankly If you say ""special linear group"" I will stare at you blankly Matrix similarity / congruence / equivalence, um what? I feel like I have no understanding of bilinear mappings into $\mathbb{R}$, despite that they're ""the same"" as matrices. (Given a matrix $A,$ we get a bilinear mapping into $\mathbb{R}$ given by $y,x \mapsto y^T Ax.$ This process is an isomorphism of vector spaces.) These are mainly things that can be looked up on wikipedia, of course (except for the last dot point), but its tough to get the ""big picture"" and/or the geometric meaning without the help of a good article or book. Question. Can anyone recommend an article or book that specifically deals with coordinate- dependent linear algebra, such as matrices, in a sophisticated, mathematically-mature way, and which preferably takes abstract linear algebra for granted, and even uses it to help to express and clarify the coordinate dependent stuff?",,"['linear-algebra', 'matrices', 'reference-request', 'vector-spaces']"
65,Relation between minimal polynomial and divisibility,Relation between minimal polynomial and divisibility,,"Let $A$ be a $n \times n$ matrix with rational elements and $p$ a prime number such that $A^p = I$ with $p<n$. If $\det(A-I)\neq0$ it is true that $p-1$ divides $n$? Here is what I've worked so far. $$A^p = I,$$ so $$(A-I)(A^{p-1}+A^{p-2}+\cdots+A+I) = 0.$$ Since $A-I$ is invertible we can multiply by its inverse and get that $$A^{p-1}+A^{p-2}+\cdots+A+I = 0.$$ We know that the minimal polynomial of $A$ divides this polynomial of degree $p-1$. I do not understand why this implies that $p-1$ divides $n$.","Let $A$ be a $n \times n$ matrix with rational elements and $p$ a prime number such that $A^p = I$ with $p<n$. If $\det(A-I)\neq0$ it is true that $p-1$ divides $n$? Here is what I've worked so far. $$A^p = I,$$ so $$(A-I)(A^{p-1}+A^{p-2}+\cdots+A+I) = 0.$$ Since $A-I$ is invertible we can multiply by its inverse and get that $$A^{p-1}+A^{p-2}+\cdots+A+I = 0.$$ We know that the minimal polynomial of $A$ divides this polynomial of degree $p-1$. I do not understand why this implies that $p-1$ divides $n$.",,"['matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
66,Simple lower bound for a determinant,Simple lower bound for a determinant,,"Let $A$ in $\mathbb{Q}^{n \times n}$ such that $\det(A) > 0$? Is there a simple lower bound for $\det(A)$ in terms of the entries of $A$? Edit: Motivation : Let $M$ be an $m \times n$ matrix. I want to compute a lower bound for  $$ \frac{\det(A^*_i)}{\det(A_i)},   $$ where $\det(A_i)$ is a $n \times n$ matrix consisting of columns of $M$ and $A^*_i$ equals $A_i$ where the $i$-th column is replaced by some column vector $b$ (Cramer's Rule). I know that $\det(A_i)$ can be upper bounded by $2 \langle M \rangle$, where $\langle M \rangle$ denotes the encoding length of $M$. The encoding size of the lower bound should be bounded polynomially in $n$ and the encoding size of $A$.","Let $A$ in $\mathbb{Q}^{n \times n}$ such that $\det(A) > 0$? Is there a simple lower bound for $\det(A)$ in terms of the entries of $A$? Edit: Motivation : Let $M$ be an $m \times n$ matrix. I want to compute a lower bound for  $$ \frac{\det(A^*_i)}{\det(A_i)},   $$ where $\det(A_i)$ is a $n \times n$ matrix consisting of columns of $M$ and $A^*_i$ equals $A_i$ where the $i$-th column is replaced by some column vector $b$ (Cramer's Rule). I know that $\det(A_i)$ can be upper bounded by $2 \langle M \rangle$, where $\langle M \rangle$ denotes the encoding length of $M$. The encoding size of the lower bound should be bounded polynomially in $n$ and the encoding size of $A$.",,"['linear-algebra', 'matrices', 'determinant']"
67,$\operatorname{PSL}_2(\mathbb{Z})$ is the free product of two cyclic groups.,is the free product of two cyclic groups.,\operatorname{PSL}_2(\mathbb{Z}),"Let $G$ be a group generated by two matrices $ S=\left( \begin {array}{cc} 0&{-1}\\1&0\end{array}\right),\, T=\left( \begin{array}{cc}1&1\\0&1\end{array}\right) $ in $SL_2(\mathbb Z)/ \{ \pm 1 \} ,$ i.e., $G=\langle S,T\rangle$. Then how to show that $ \langle S,X ; S^2,X^3\rangle$ where $X=ST$ is a presentation of $G$? It's hard for me to show all relations can be generated by $S^2$ and $X^3$. (BTW, these groups rise from modular group.)","Let $G$ be a group generated by two matrices $ S=\left( \begin {array}{cc} 0&{-1}\\1&0\end{array}\right),\, T=\left( \begin{array}{cc}1&1\\0&1\end{array}\right) $ in $SL_2(\mathbb Z)/ \{ \pm 1 \} ,$ i.e., $G=\langle S,T\rangle$. Then how to show that $ \langle S,X ; S^2,X^3\rangle$ where $X=ST$ is a presentation of $G$? It's hard for me to show all relations can be generated by $S^2$ and $X^3$. (BTW, these groups rise from modular group.)",,"['abstract-algebra', 'group-theory', 'matrices', 'number-theory', 'modular-forms']"
68,When does the singular value decomposition fail?,When does the singular value decomposition fail?,,"Does the singular value decomposition ever not work? The statement of the associated theorem, here from Wikipedia , is surprisingly general. Are there certain conditions of the matrix $M$ that would make the decomposition fail to exist? The matrix M in question is a random matrix that is point-wise convergent. Thanks so much!","Does the singular value decomposition ever not work? The statement of the associated theorem, here from Wikipedia , is surprisingly general. Are there certain conditions of the matrix $M$ that would make the decomposition fail to exist? The matrix M in question is a random matrix that is point-wise convergent. Thanks so much!",,"['linear-algebra', 'matrices', 'svd', 'random-matrices']"
69,Gradient of $\mbox{tr} \left( (AX)^t (AX) \right)$,Gradient of,\mbox{tr} \left( (AX)^t (AX) \right),"I am trying to calculate the gradient of the following function $$f(X) = \mbox{tr} \left( (AX)^t (AX) \right)$$ Chain's rule gives $$\nabla_X(f(X)) = \nabla_X (\mbox{tr}(AX))\nabla_x(AX)$$ However, I'm having trouble with those two derivatives. What is $\nabla_X tr(AX)$ ? Is it $A^t$ ? I did the math and obtained that $\frac{\partial(tr(AX))}{\partial x_{ij}} = a_{ji}$ , but I'm not sure... And also what is $\nabla_X AX$ ? Is it simply $A$ ? I tried differentiating this but failed to see if this holds or not. Thanks in advance","I am trying to calculate the gradient of the following function Chain's rule gives However, I'm having trouble with those two derivatives. What is ? Is it ? I did the math and obtained that , but I'm not sure... And also what is ? Is it simply ? I tried differentiating this but failed to see if this holds or not. Thanks in advance",f(X) = \mbox{tr} \left( (AX)^t (AX) \right) \nabla_X(f(X)) = \nabla_X (\mbox{tr}(AX))\nabla_x(AX) \nabla_X tr(AX) A^t \frac{\partial(tr(AX))}{\partial x_{ij}} = a_{ji} \nabla_X AX A,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
70,A question about matrices such that the elements in each row add up to $1$.,A question about matrices such that the elements in each row add up to .,1,"Let $A$ be an invertible $10\times 10$ matrix with real entries such that the sum of each row is $1$. Then is the sum of the entries of each row of the inverse of $A$ also $1$? I created some examples, and found the proposition to be true. I also proved that if two matrices with the property that the sum of the elements in each row is $1$ are multiplied, then the product also has the same property. Clearly, $I$ has this property. I think I have a proof running along the following lines: $$A^{-1}A=I$$ where $A$ and $I$ satisfy the aforementioned property. Also, if $A^{-1}$ did not satisfy this property, then neither would the product of $A$ and $A^{-1}$, which is a contradiction. Is the proposition true, and if so, is my proof correct?","Let $A$ be an invertible $10\times 10$ matrix with real entries such that the sum of each row is $1$. Then is the sum of the entries of each row of the inverse of $A$ also $1$? I created some examples, and found the proposition to be true. I also proved that if two matrices with the property that the sum of the elements in each row is $1$ are multiplied, then the product also has the same property. Clearly, $I$ has this property. I think I have a proof running along the following lines: $$A^{-1}A=I$$ where $A$ and $I$ satisfy the aforementioned property. Also, if $A^{-1}$ did not satisfy this property, then neither would the product of $A$ and $A^{-1}$, which is a contradiction. Is the proposition true, and if so, is my proof correct?",,['matrices']
71,Simplifying the sum $\sum\limits_{i=1}^n\sum\limits_{j=1}^n x_i\cdot x_j$,Simplifying the sum,\sum\limits_{i=1}^n\sum\limits_{j=1}^n x_i\cdot x_j,"How can I simplify the expression $\sum\limits_{i=1}^n\sum\limits_{j=1}^n x_i\cdot x_j$? $x$ is a vector of numbers of length $n$, and I am trying to prove that the result of the expression above is positive for any $x$ vector.  Is it equal to $\sum\limits_{i=1}^n x_i\cdot \sum\limits_{j=1}^n x_j$? If it is then my problem is solved, because $\left(\sum\limits_{i=1}^n x_i\right)^2$ is non-negative (positive or zero).","How can I simplify the expression $\sum\limits_{i=1}^n\sum\limits_{j=1}^n x_i\cdot x_j$? $x$ is a vector of numbers of length $n$, and I am trying to prove that the result of the expression above is positive for any $x$ vector.  Is it equal to $\sum\limits_{i=1}^n x_i\cdot \sum\limits_{j=1}^n x_j$? If it is then my problem is solved, because $\left(\sum\limits_{i=1}^n x_i\right)^2$ is non-negative (positive or zero).",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'summation', 'vectors']"
72,integral of positive definite matrix is positive definite?,integral of positive definite matrix is positive definite?,,"Say $A_{t}$ is a positive definite matrix for all t, do we have that $\int_{0}^{1}A_{t}dt$ is a positive definite matrix. Is there an obvious counterexample? The arguement in the book was that the weighted average of positive definite matrices is also positive definite. http://books.google.ca/books?id=k4ODAwAAQBAJ&pg=PA38&lpg=PA38&dq=maximum+principle+minimal+surfaces+interior&source=bl&ots=D8AGQK9QXi&sig=vWX5-I_c2QyAKsT7CQKGXuTEECQ&hl=en&sa=X&ei=28J_VITVLIS2yASt7YCoBw&ved=0CEQQ6AEwBg#v=onepage&q=maximum%20principle%20minimal%20surfaces%20interior&f=false page 37 A condition for a matrix to be positive definite is $det(A)\geq0$ and  $Tr(A)\geq 0$. So since we have a Riemann sum, we should have $Tr(\int_{0}^{1}A_{t}dt)=lim_{n\to \infty}\sum_{i}^{n}Tr(A_{t_{i}})(t_{i}-t_{i-1})>0$. However determinants don't decompose into sums. Thank you","Say $A_{t}$ is a positive definite matrix for all t, do we have that $\int_{0}^{1}A_{t}dt$ is a positive definite matrix. Is there an obvious counterexample? The arguement in the book was that the weighted average of positive definite matrices is also positive definite. http://books.google.ca/books?id=k4ODAwAAQBAJ&pg=PA38&lpg=PA38&dq=maximum+principle+minimal+surfaces+interior&source=bl&ots=D8AGQK9QXi&sig=vWX5-I_c2QyAKsT7CQKGXuTEECQ&hl=en&sa=X&ei=28J_VITVLIS2yASt7YCoBw&ved=0CEQQ6AEwBg#v=onepage&q=maximum%20principle%20minimal%20surfaces%20interior&f=false page 37 A condition for a matrix to be positive definite is $det(A)\geq0$ and  $Tr(A)\geq 0$. So since we have a Riemann sum, we should have $Tr(\int_{0}^{1}A_{t}dt)=lim_{n\to \infty}\sum_{i}^{n}Tr(A_{t_{i}})(t_{i}-t_{i-1})>0$. However determinants don't decompose into sums. Thank you",,"['linear-algebra', 'matrices', 'matrix-calculus']"
73,Does the matrix exponential take open sets into open sets?,Does the matrix exponential take open sets into open sets?,,"This is from Hall's Lie Groups, Lie Algebras, and Representations , in theorem $2.13$: Let $B_\varepsilon$ be the open ball of radius $\varepsilon$ about zero in $M_n (\mathbb{C})$ [$= \mathbb{C}^{n\times n}$] (...). Assume that $\varepsilon < \log 2$. Then, we have shown that ""exp"" takes $B_\varepsilon$ injectively into $M_n (\mathbb{C})$, with continuous inverse that we denote ""log"". Now, let $U =\exp(B_{\varepsilon /2})$, which is an open set in $GL(n; \mathbb{C})$. The last sentence is not obvious to me. A general continuous function doesn't take open sets into open sets, so it probably has to do with the exponential being injective or with the inequality $||e^X|| \le e^{||X||}$. What is the proof of this?","This is from Hall's Lie Groups, Lie Algebras, and Representations , in theorem $2.13$: Let $B_\varepsilon$ be the open ball of radius $\varepsilon$ about zero in $M_n (\mathbb{C})$ [$= \mathbb{C}^{n\times n}$] (...). Assume that $\varepsilon < \log 2$. Then, we have shown that ""exp"" takes $B_\varepsilon$ injectively into $M_n (\mathbb{C})$, with continuous inverse that we denote ""log"". Now, let $U =\exp(B_{\varepsilon /2})$, which is an open set in $GL(n; \mathbb{C})$. The last sentence is not obvious to me. A general continuous function doesn't take open sets into open sets, so it probably has to do with the exponential being injective or with the inequality $||e^X|| \le e^{||X||}$. What is the proof of this?",,"['matrices', 'lie-groups', 'lie-algebras', 'exponential-function']"
74,"When do two $n \times n$ matrices $A, B$ have the property that $AB = BA$?",When do two  matrices  have the property that ?,"n \times n A, B AB = BA","As we all know that two $n \times n$ matrices $A, B$ need not have the relation $AB = BA.$ But when do two $n \times n$ matrices have such a property?","As we all know that two $n \times n$ matrices $A, B$ need not have the relation $AB = BA.$ But when do two $n \times n$ matrices have such a property?",,['matrices']
75,C*-algebra generated by the symmetric on 3 elements,C*-algebra generated by the symmetric on 3 elements,,"I want compute $C^*(S_3)$ where $S_3$ is the symmetric group on $\{1,2,3\}$ and $C^*(S_3)$ is the (full) C*-algebra generated by $S_3$. My attempt: Since $S_3$ is a finite group, $C^*(S_3)=C_c(S_3)$ and its dimension is equal to the group order. So, I was trying to representate this group C*-algebra in a algebra of the form $\oplus_{i=1}^nM_{d_i}$. Since $S_3$ has order $6$, there are only two possibilities: $C^*(S_3)=\oplus_{i=1}^6\mathbb{C}$ $C^*(S_3)=M_2(\mathbb{C})\oplus\mathbb{C}\oplus\mathbb{C}$ Can anyone help me identify which one is it?","I want compute $C^*(S_3)$ where $S_3$ is the symmetric group on $\{1,2,3\}$ and $C^*(S_3)$ is the (full) C*-algebra generated by $S_3$. My attempt: Since $S_3$ is a finite group, $C^*(S_3)=C_c(S_3)$ and its dimension is equal to the group order. So, I was trying to representate this group C*-algebra in a algebra of the form $\oplus_{i=1}^nM_{d_i}$. Since $S_3$ has order $6$, there are only two possibilities: $C^*(S_3)=\oplus_{i=1}^6\mathbb{C}$ $C^*(S_3)=M_2(\mathbb{C})\oplus\mathbb{C}\oplus\mathbb{C}$ Can anyone help me identify which one is it?",,"['matrices', 'operator-algebras', 'c-star-algebras', 'permutations']"
76,"If $A$ is a real skew-symmetric matrix, why is $(I-A)(I+A)^{-1}$ orthogonal?","If  is a real skew-symmetric matrix, why is  orthogonal?",A (I-A)(I+A)^{-1},"I proved the first part of this question: That $I-A$ and $I+A$ are nonsingular. The second part of question asks about orthogonality. So, I tried to prove by assuming that there exists transpose of that matrix and the product of the two matrices would be the identity. Can anybody give me some thoughts on how to complete this line of reasoning?","I proved the first part of this question: That $I-A$ and $I+A$ are nonsingular. The second part of question asks about orthogonality. So, I tried to prove by assuming that there exists transpose of that matrix and the product of the two matrices would be the identity. Can anybody give me some thoughts on how to complete this line of reasoning?",,"['linear-algebra', 'matrices']"
77,Stability of Linear Systems,Stability of Linear Systems,,"for the following matrices A, classify the stability of the linear systems x=Ax as asymptotically stable, L-stable (but not asymptotically stable) or unstable and indicate whether it is a stable node, stable degenerate node, etc: I don't really know how to get started...can someone explain what the first step would be? So someone said that I should look at the eigenvalues and: a.   \begin{array}{cc}    -6 & -3 \\    2 & -1 \\   \end{array} $λ_1=-3$, $λ_2=-4$ Asymptotically stable Node b. \begin{array}{cc}    5 & -5 \\    2 & -1 \\   \end{array} $λ_1=2+i$, $λ_2=2-i$ Unstable Spiral Point c. \begin{array}{cc}    1 & 5 \\    -2 & -1 \\   \end{array} $λ_1=3i$, $λ_2=-3i$ L-stable Center d. \begin{array}{cc}    1 & -2 \\    8 & -7 \\   \end{array} $λ_1=-3$, $λ_2=-3$ Asymptotically Stable Proper Node","for the following matrices A, classify the stability of the linear systems x=Ax as asymptotically stable, L-stable (but not asymptotically stable) or unstable and indicate whether it is a stable node, stable degenerate node, etc: I don't really know how to get started...can someone explain what the first step would be? So someone said that I should look at the eigenvalues and: a.   \begin{array}{cc}    -6 & -3 \\    2 & -1 \\   \end{array} $λ_1=-3$, $λ_2=-4$ Asymptotically stable Node b. \begin{array}{cc}    5 & -5 \\    2 & -1 \\   \end{array} $λ_1=2+i$, $λ_2=2-i$ Unstable Spiral Point c. \begin{array}{cc}    1 & 5 \\    -2 & -1 \\   \end{array} $λ_1=3i$, $λ_2=-3i$ L-stable Center d. \begin{array}{cc}    1 & -2 \\    8 & -7 \\   \end{array} $λ_1=-3$, $λ_2=-3$ Asymptotically Stable Proper Node",,"['matrices', 'eigenvalues-eigenvectors', 'dynamical-systems']"
78,Rank of euclidean distance matrix,Rank of euclidean distance matrix,,"How do I prove that the rank of a euclidean distance matrix is $p+2$, where $p$ is the dimensionality of the points from which the matrix was created?","How do I prove that the rank of a euclidean distance matrix is $p+2$, where $p$ is the dimensionality of the points from which the matrix was created?",,"['linear-algebra', 'matrices']"
79,"Is $\mathbf {B^TAB}$ non-singular for a non-singular $\mathbf A$, and $\mathbf {B}$ with full column-rank?","Is  non-singular for a non-singular , and  with full column-rank?",\mathbf {B^TAB} \mathbf A \mathbf {B},"If $\mathbf A$ is any square non-singular matrix of dimension $n \times n$. And $\mathbf B$ is a $n \times m$ matrix with $\mathrm{rank(\mathbf B)} = m$. Is the full rank condition of matrix $\mathbf B$ both sufficient as well as necessary to state that the matrix product $\mathbf {B^TAB}$ is non-singular? i.e., can we write: $ \mathbf {A}$ is non-singular $\iff$ $\mathrm{rank(\mathbf B)} = m$ and $\mathbf {B^TAB}$ is non-singular","If $\mathbf A$ is any square non-singular matrix of dimension $n \times n$. And $\mathbf B$ is a $n \times m$ matrix with $\mathrm{rank(\mathbf B)} = m$. Is the full rank condition of matrix $\mathbf B$ both sufficient as well as necessary to state that the matrix product $\mathbf {B^TAB}$ is non-singular? i.e., can we write: $ \mathbf {A}$ is non-singular $\iff$ $\mathrm{rank(\mathbf B)} = m$ and $\mathbf {B^TAB}$ is non-singular",,"['linear-algebra', 'matrices', 'matrix-rank']"
80,Finding eigenvalues of a block matrix,Finding eigenvalues of a block matrix,,"I have a block matrix of size $2N \times 2N$ of the form $$B = \begin{bmatrix} A_N & C_N \\ C_N & A_N \end{bmatrix}$$ where $A_N$ and $C_N$ are both $N \times N$ matrices. Specifically, $$A_N = \begin{bmatrix} 0 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 0 \end{bmatrix} \qquad C_N = \begin{bmatrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{bmatrix} $$ That is, $A_N$ has zeroes on the diagonal, and all other entries $1$; $C_N$ has zeroes along the minor diagonal, and all other entries are $1$. I would like to find the eigenvalues of the matrix $B$.","I have a block matrix of size $2N \times 2N$ of the form $$B = \begin{bmatrix} A_N & C_N \\ C_N & A_N \end{bmatrix}$$ where $A_N$ and $C_N$ are both $N \times N$ matrices. Specifically, $$A_N = \begin{bmatrix} 0 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 0 \end{bmatrix} \qquad C_N = \begin{bmatrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{bmatrix} $$ That is, $A_N$ has zeroes on the diagonal, and all other entries $1$; $C_N$ has zeroes along the minor diagonal, and all other entries are $1$. I would like to find the eigenvalues of the matrix $B$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
81,Is it true that $u + v$ is an eigenvector corresponding to the eigenvalue $\lambda$?,Is it true that  is an eigenvector corresponding to the eigenvalue ?,u + v \lambda,"Let $A$ be an $n \times n$ matrix, and $u, v$ be eigenvectors corresponding to an eigenvalue $\lambda$  of $ A$ (that is, $Au = \lambda u$ and $Av = \lambda v$). Is it true that $u + v$ is an eigenvector corresponding to the eigenvalue $\lambda$. I know that an eigenvector can't be the $0$ vector, but an eigenvalue can be $0$, it just means the matrix $A$ is not invertible. By definition $ Au = \lambda u$, where $u \ne 0$, and $Av = \lambda v$, where $v \ne  0$.  So,  $$Au + Av = \lambda u + \lambda v \implies A(u + v) = \lambda (u + v)$$ where $u + v \ne 0$? I feel like I'm missing something because my solution was too straightforward.","Let $A$ be an $n \times n$ matrix, and $u, v$ be eigenvectors corresponding to an eigenvalue $\lambda$  of $ A$ (that is, $Au = \lambda u$ and $Av = \lambda v$). Is it true that $u + v$ is an eigenvector corresponding to the eigenvalue $\lambda$. I know that an eigenvector can't be the $0$ vector, but an eigenvalue can be $0$, it just means the matrix $A$ is not invertible. By definition $ Au = \lambda u$, where $u \ne 0$, and $Av = \lambda v$, where $v \ne  0$.  So,  $$Au + Av = \lambda u + \lambda v \implies A(u + v) = \lambda (u + v)$$ where $u + v \ne 0$? I feel like I'm missing something because my solution was too straightforward.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
82,Proximal Operator Fixed Point Property for Matrices,Proximal Operator Fixed Point Property for Matrices,,"$\newcommand{\prox}{\operatorname{prox}}$ $\newcommand{\argmin}{\operatorname{argmin}}$ $\newcommand{\dom}{\operatorname{dom}}$ Recall again that the proximal operator for vectors $\prox_{f}: R^n \rightarrow R^n$ of $f$ is defined as: $\prox_f(v) := \argmin_{x} \left(f(x) +(1/2)\|x-v\|_2^2 \right) $, where $f: R^n \rightarrow R \cup \infty$ is a closed proper convex function and $\|\cdot \|_2$ is the Euclidean norm. $\prox$ is strongly convex and not everywhere infinite, so there it will have a unique minimizer for every $v \in R^n$ The crucial property of proximal operator is that $x^*$ minimizes $f(x)$ iff $x^* = \prox_f(x^*)$, i.e. $x^*$ is a fixed point of $\prox_f$. Let us consider the proof of this property. The first part, namely, that if $x^*$ minimizes $f$ then $\prox_f(x^*)=x^*$, is trivial: $f(x) +(1/2)\|x-x^*\|_2^2 \le f(x^*) = f(x^*) +(1/2)\|x^*-x^*\|_2^2$ The second part uses the notion of subdifferential. In the proof authors say that $\tilde{x}$ minimizes $f(x) +(1/2)\|x-v\|_2^2$ i.e. $\tilde{x} = \prox_f(v)$ iff $0 \in \partial f(\tilde{x}) + (\tilde{x}-v)$, where the sum is of a set and a point. Recall the definition of the subdifferential: $\partial f(x) = \{ y$ |  $ f(z)\le f(x) + y^T(z-x)$ $\forall z \in \dom{f}\}$ Take $\tilde{x}=v=x^*$, it follows that $0 \in \partial f(x^*)$ so $x^*$ minimizes $f$. Question 1: Consider proximal operator defined for matrices now: $\prox_f(Y) := \argmin_{X} \left(f(X) +(1/2)\|X-Y\|^2 \right)$, where $X$ is some real $m$ by $n$ matrix. What norm function should be taken in the definition of the $\prox$ in this case? Frobenius? Or spectral norm (induced 2nd norm) which is the largest eigenvalue (is spectral norm even differentiable?)? Or something else? Question 2: Can the norm for the definition of $\prox$ in case of matrices be chosen in different ways? What conditions does the proof above impose on the norm function?","$\newcommand{\prox}{\operatorname{prox}}$ $\newcommand{\argmin}{\operatorname{argmin}}$ $\newcommand{\dom}{\operatorname{dom}}$ Recall again that the proximal operator for vectors $\prox_{f}: R^n \rightarrow R^n$ of $f$ is defined as: $\prox_f(v) := \argmin_{x} \left(f(x) +(1/2)\|x-v\|_2^2 \right) $, where $f: R^n \rightarrow R \cup \infty$ is a closed proper convex function and $\|\cdot \|_2$ is the Euclidean norm. $\prox$ is strongly convex and not everywhere infinite, so there it will have a unique minimizer for every $v \in R^n$ The crucial property of proximal operator is that $x^*$ minimizes $f(x)$ iff $x^* = \prox_f(x^*)$, i.e. $x^*$ is a fixed point of $\prox_f$. Let us consider the proof of this property. The first part, namely, that if $x^*$ minimizes $f$ then $\prox_f(x^*)=x^*$, is trivial: $f(x) +(1/2)\|x-x^*\|_2^2 \le f(x^*) = f(x^*) +(1/2)\|x^*-x^*\|_2^2$ The second part uses the notion of subdifferential. In the proof authors say that $\tilde{x}$ minimizes $f(x) +(1/2)\|x-v\|_2^2$ i.e. $\tilde{x} = \prox_f(v)$ iff $0 \in \partial f(\tilde{x}) + (\tilde{x}-v)$, where the sum is of a set and a point. Recall the definition of the subdifferential: $\partial f(x) = \{ y$ |  $ f(z)\le f(x) + y^T(z-x)$ $\forall z \in \dom{f}\}$ Take $\tilde{x}=v=x^*$, it follows that $0 \in \partial f(x^*)$ so $x^*$ minimizes $f$. Question 1: Consider proximal operator defined for matrices now: $\prox_f(Y) := \argmin_{X} \left(f(X) +(1/2)\|X-Y\|^2 \right)$, where $X$ is some real $m$ by $n$ matrix. What norm function should be taken in the definition of the $\prox$ in this case? Frobenius? Or spectral norm (induced 2nd norm) which is the largest eigenvalue (is spectral norm even differentiable?)? Or something else? Question 2: Can the norm for the definition of $\prox$ in case of matrices be chosen in different ways? What conditions does the proof above impose on the norm function?",,"['matrices', 'convex-analysis', 'convex-optimization', 'proximal-operators']"
83,Prove that $Im(A)+Ker(A)=R^n \iff Ker(A^2)=Ker(A)$,Prove that,Im(A)+Ker(A)=R^n \iff Ker(A^2)=Ker(A),$\def\Im{\operatorname{Im}}\def\Ker{\operatorname{Ker}}$How to prove that for any squared matrix such that $ \Im(A)+\Ker(A)=\mathbb{R}^n$ if and only if $\Ker(A^2)=\Ker(A)$. It is evident to me that it is true if $A$ is zero matrix or $A$ is nonsingular. How to prove if $A$ singular nonzero matrix? Thank you!,$\def\Im{\operatorname{Im}}\def\Ker{\operatorname{Ker}}$How to prove that for any squared matrix such that $ \Im(A)+\Ker(A)=\mathbb{R}^n$ if and only if $\Ker(A^2)=\Ker(A)$. It is evident to me that it is true if $A$ is zero matrix or $A$ is nonsingular. How to prove if $A$ singular nonzero matrix? Thank you!,,"['linear-algebra', 'matrices']"
84,How do I compute the Singular Value Decomposition of the pseudo-inverse of a matrix?,How do I compute the Singular Value Decomposition of the pseudo-inverse of a matrix?,,"There is $A$ which is a matrix:  $$\begin{bmatrix}2 & 4 \\ 1 & -4 \\ -2 & 2\end{bmatrix}.$$ While I have easily worked out the singular value decomposition of this matrix, but I am not sure how to go about trying to present the pseudo-inverse of $A$ (i.e. $A^+$) in SVD form. What I have found out is that:  $$   A^+ = V \cdot \Sigma^{-1} \cdot U^\top $$ But trying this out has caused a problem since the matrix $\Sigma$ is not a square matrix so the inverse is not possible. So I am not quite sure if I am following the right route or not ...","There is $A$ which is a matrix:  $$\begin{bmatrix}2 & 4 \\ 1 & -4 \\ -2 & 2\end{bmatrix}.$$ While I have easily worked out the singular value decomposition of this matrix, but I am not sure how to go about trying to present the pseudo-inverse of $A$ (i.e. $A^+$) in SVD form. What I have found out is that:  $$   A^+ = V \cdot \Sigma^{-1} \cdot U^\top $$ But trying this out has caused a problem since the matrix $\Sigma$ is not a square matrix so the inverse is not possible. So I am not quite sure if I am following the right route or not ...",,"['linear-algebra', 'matrices', 'svd', 'pseudoinverse']"
85,Non-integral power of a singular matrix,Non-integral power of a singular matrix,,"I know, that if $A$ is nonsingular matrix, so $\det{A} \ne 0$, then $A^p=\exp\left(p\ln A\right)$ is true for any real exponent, but what about if $A$ is singular? Then $A$ has a zero eigenvalue, so the matrix logarithm doesn't exist. Is there any extension in this case?","I know, that if $A$ is nonsingular matrix, so $\det{A} \ne 0$, then $A^p=\exp\left(p\ln A\right)$ is true for any real exponent, but what about if $A$ is singular? Then $A$ has a zero eigenvalue, so the matrix logarithm doesn't exist. Is there any extension in this case?",,"['linear-algebra', 'matrices', 'logarithms', 'exponential-function']"
86,Permutation of matrix elements,Permutation of matrix elements,,"Let each row and each column of a $n\times n$ matrix $A$ be a permutation of $\{1,2,\ldots,n\}$ and let $A$ be symmetric. (a) If $n$ is odd, prove that each of $1,2,\ldots,n$ occurs on the principle diagonal of $A$. (b) For every even number $n$, show that there exists an $A$ in which not all of $1,2,\ldots,n$ appear on the diagonal. My knowledge of matrices is pretty basic. I have noticed similar types of questions before. What topics do I need to learn to solve these types of problems?","Let each row and each column of a $n\times n$ matrix $A$ be a permutation of $\{1,2,\ldots,n\}$ and let $A$ be symmetric. (a) If $n$ is odd, prove that each of $1,2,\ldots,n$ occurs on the principle diagonal of $A$. (b) For every even number $n$, show that there exists an $A$ in which not all of $1,2,\ldots,n$ appear on the diagonal. My knowledge of matrices is pretty basic. I have noticed similar types of questions before. What topics do I need to learn to solve these types of problems?",,"['linear-algebra', 'matrices']"
87,$3 \times 3$ matrices completely determined by their characteristic and minimal polynomials,matrices completely determined by their characteristic and minimal polynomials,3 \times 3,"How do you show that two $3 \times 3$ matrices with the same characteristic and minimal polynomials both conjugate to the same Jordan normal form, assuming no knowledge of the eigenspaces? I know that it is possible to determine completely the Jordan normal form of a matrix only with its minimal and characteristic polynomial, up to dimension $6$, but only if one can compute the dimension of the eigenspace as well. And why does this characterization fail for $4 \times 4$ matrices?","How do you show that two $3 \times 3$ matrices with the same characteristic and minimal polynomials both conjugate to the same Jordan normal form, assuming no knowledge of the eigenspaces? I know that it is possible to determine completely the Jordan normal form of a matrix only with its minimal and characteristic polynomial, up to dimension $6$, but only if one can compute the dimension of the eigenspace as well. And why does this characterization fail for $4 \times 4$ matrices?",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'minimal-polynomials']"
88,Show that any invertible matrix has a logarithm.,Show that any invertible matrix has a logarithm.,,"I was trying to remember how to show that any invertible matrix has a (possibly complex) logarithm. I thought what I came up with was kind of cool, so I thought I'd post my answer here.","I was trying to remember how to show that any invertible matrix has a (possibly complex) logarithm. I thought what I came up with was kind of cool, so I thought I'd post my answer here.",,"['matrices', 'logarithms']"
89,To Prove $x'Ax=\mathrm{tr}(xAx')=\mathrm{tr}(Axx')=\mathrm{tr}(xx'A)$,To Prove,x'Ax=\mathrm{tr}(xAx')=\mathrm{tr}(Axx')=\mathrm{tr}(xx'A),"To prove, $x'Ax=\mathrm{tr}(xAx')=\mathrm{tr}(Axx')=\mathrm{tr}(xx'A)$ where $A$ is a square matrix. $x'$ is the transpose of $x$. For each $x,x'$ are column vector, row vector.","To prove, $x'Ax=\mathrm{tr}(xAx')=\mathrm{tr}(Axx')=\mathrm{tr}(xx'A)$ where $A$ is a square matrix. $x'$ is the transpose of $x$. For each $x,x'$ are column vector, row vector.",,"['linear-algebra', 'matrices', 'trace']"
90,Real vs. complex Jordan-Chevalley decomposition,Real vs. complex Jordan-Chevalley decomposition,,"Let $A$ be a real square matrix. Then over ${\mathbb C}$, $A$ has a unique Jordan-Chevalley  decomposition $A=D+N$ with $D$ diagonalizable, $N$ nilpotent  and $DN=ND$. My question is : must $D$ and $N$ be real ? If $A$ is already trigonalizable over $\mathbb R$, then it will also have a Jordan-Chevalley decomposition over $\mathbb R$ and by uniqueness $D$ and $N$ will be real. I'm not sure about what happens when $A$ is not trigonalizable over $\mathbb R$ though.","Let $A$ be a real square matrix. Then over ${\mathbb C}$, $A$ has a unique Jordan-Chevalley  decomposition $A=D+N$ with $D$ diagonalizable, $N$ nilpotent  and $DN=ND$. My question is : must $D$ and $N$ be real ? If $A$ is already trigonalizable over $\mathbb R$, then it will also have a Jordan-Chevalley decomposition over $\mathbb R$ and by uniqueness $D$ and $N$ will be real. I'm not sure about what happens when $A$ is not trigonalizable over $\mathbb R$ though.",,"['matrices', 'jordan-normal-form']"
91,Random Matrix Theory and ESD,Random Matrix Theory and ESD,,"I need some help to understand what professor Terence Tao means in this part of ""Topics in random matrix theory"". I'm having a hard time to undertand this function ESD. How is possible for it to be a distribution function, a probability measure and a random variable at the same time? If someone knows something about, please, I need to understand the details here...I'm a bit confused. If is not ask too much, I would like to see explicitly the definitions of all objects that T. Tao mentions in this page. PS: This problem is related to this one Eigenvalues of a Random Matrix . What I need most is an explanation about how the ESD of a random matrix works with a ESD of an real matrix (a random matrix evaluated). Please take a look at the comments of that link. Thank you.","I need some help to understand what professor Terence Tao means in this part of ""Topics in random matrix theory"". I'm having a hard time to undertand this function ESD. How is possible for it to be a distribution function, a probability measure and a random variable at the same time? If someone knows something about, please, I need to understand the details here...I'm a bit confused. If is not ask too much, I would like to see explicitly the definitions of all objects that T. Tao mentions in this page. PS: This problem is related to this one Eigenvalues of a Random Matrix . What I need most is an explanation about how the ESD of a random matrix works with a ESD of an real matrix (a random matrix evaluated). Please take a look at the comments of that link. Thank you.",,"['matrices', 'random-variables', 'random-matrices']"
92,Decomposition of idempotent and symmetric matrix,Decomposition of idempotent and symmetric matrix,,"In Patterson and Thompson, 1971 I find the following claim which I cannot prove myself, nor have I been able to find a proof: ""As $S$ is idempotent and symmetric [ and of size $n\times n$ and with rank $=n-t$], it can be expressed in the form $AA'$, where $A$ is an $n \times (n - t)$ matrix such that $A'A = I$."" My questions are: How do I prove this, and can the matrix $A$ be computed in a straightforward way? I note that almost the same question was asked here and the answer accepted. However, if I understand it correctly that answer claims to prove the statement false, and that does not seem reasonable to me. I believe that answer considers square matrices $A$? Anyway, any help appreciated.","In Patterson and Thompson, 1971 I find the following claim which I cannot prove myself, nor have I been able to find a proof: ""As $S$ is idempotent and symmetric [ and of size $n\times n$ and with rank $=n-t$], it can be expressed in the form $AA'$, where $A$ is an $n \times (n - t)$ matrix such that $A'A = I$."" My questions are: How do I prove this, and can the matrix $A$ be computed in a straightforward way? I note that almost the same question was asked here and the answer accepted. However, if I understand it correctly that answer claims to prove the statement false, and that does not seem reasonable to me. I believe that answer considers square matrices $A$? Anyway, any help appreciated.",,"['linear-algebra', 'matrices']"
93,How to find 2x2 matrix with non zero elements and repeated eigenvalues?,How to find 2x2 matrix with non zero elements and repeated eigenvalues?,,I need to find a 2x2 matrix with non zero elements that has eigenvalue = 1 repeated (double). How can i do that? Thanks!,I need to find a 2x2 matrix with non zero elements that has eigenvalue = 1 repeated (double). How can i do that? Thanks!,,"['linear-algebra', 'matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
94,Prove that the set of all matrices is direct sum of the sets of skew-symmetric and symmetric matrices,Prove that the set of all matrices is direct sum of the sets of skew-symmetric and symmetric matrices,,"Let $W_1$ be the subspace of $\mathcal{M}_{n \times n}$ that consists of all $n \times n$ skew-symmetric matrices with entries from $\mathbb{F}$, and let $W_2$ be the subspace of $\mathcal{M}_{n \times n}$ consisting of all symmetric $n \times n$ matrices. Prove that $\mathcal{M}_{n \times n}(\mathbb{F}) = W_1 \oplus W_2$. I couldn't really figure out why the sum of an $n \times n$ symmetric matrix and $n \times n$ skew-symmetric matrix would form a $n \times n$ matrix (to satisfy the direct summand property $W_1 + W_2 = \mathcal{M}_{n \times n}(\mathbb{F})).$ Browsing online, I found that $$ M = \frac{1}{2}(M + M^{t}) + \frac{1}{2}(M-M^{t}),$$ where $\frac{1}{2}(M+M^{t}) \in W_2, \frac{1}{2}(M-M^t) \in W_1$, and $M \in \mathcal{M}_{n \times n}(\mathbb{F})$. This reminds me of a formula on how odd and even functions may be be added together to form a generic function. $$f(x) = \frac{f(x)+f(-x)}{2} + \frac{f(x)-f(-x)}{2}.$$ However, to me it is disturbing to use unless I know how it was derived. If anyone could show me why a skew-symmetric matrix may be represented as $\frac{1}{2}(M-M^t)$ and why a symmetric matrix may be represented as $\frac{1}{2}(M+M^t)$ than I may sleep better tonight. Thanks.","Let $W_1$ be the subspace of $\mathcal{M}_{n \times n}$ that consists of all $n \times n$ skew-symmetric matrices with entries from $\mathbb{F}$, and let $W_2$ be the subspace of $\mathcal{M}_{n \times n}$ consisting of all symmetric $n \times n$ matrices. Prove that $\mathcal{M}_{n \times n}(\mathbb{F}) = W_1 \oplus W_2$. I couldn't really figure out why the sum of an $n \times n$ symmetric matrix and $n \times n$ skew-symmetric matrix would form a $n \times n$ matrix (to satisfy the direct summand property $W_1 + W_2 = \mathcal{M}_{n \times n}(\mathbb{F})).$ Browsing online, I found that $$ M = \frac{1}{2}(M + M^{t}) + \frac{1}{2}(M-M^{t}),$$ where $\frac{1}{2}(M+M^{t}) \in W_2, \frac{1}{2}(M-M^t) \in W_1$, and $M \in \mathcal{M}_{n \times n}(\mathbb{F})$. This reminds me of a formula on how odd and even functions may be be added together to form a generic function. $$f(x) = \frac{f(x)+f(-x)}{2} + \frac{f(x)-f(-x)}{2}.$$ However, to me it is disturbing to use unless I know how it was derived. If anyone could show me why a skew-symmetric matrix may be represented as $\frac{1}{2}(M-M^t)$ and why a symmetric matrix may be represented as $\frac{1}{2}(M+M^t)$ than I may sleep better tonight. Thanks.",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
95,"What are the properties of symmetric, anti-symmetric, and diagonal matrices","What are the properties of symmetric, anti-symmetric, and diagonal matrices",,"I know the definition of each one but I don't know how to answers questions about them, or what their properties are and how I can use them to prove/disprove statements about them. If P, Q, and D are symmetric, anti-symmetric, and diagonal matrices (of the same size) respectively, how would I go about proving if $Q^{2012} + D^{2013} $ is symmetric? Or if $(P + Q)(P - Q)$ is anti-symmetric? For the first part how do I prove that all square diagonal matrices multiplied by square diagonal matrices are still diagonal? And are anti-symmetric matrices still anti-symmetric if multiplied by themselves?","I know the definition of each one but I don't know how to answers questions about them, or what their properties are and how I can use them to prove/disprove statements about them. If P, Q, and D are symmetric, anti-symmetric, and diagonal matrices (of the same size) respectively, how would I go about proving if $Q^{2012} + D^{2013} $ is symmetric? Or if $(P + Q)(P - Q)$ is anti-symmetric? For the first part how do I prove that all square diagonal matrices multiplied by square diagonal matrices are still diagonal? And are anti-symmetric matrices still anti-symmetric if multiplied by themselves?",,"['linear-algebra', 'matrices']"
96,Is $\det(I+aAVV^*A^*)$ increasing function in $a$.,Is  increasing function in .,\det(I+aAVV^*A^*) a,"Is $$\det(I+aAVV^*A^*)$$ increasing function in $a$? Here $A$, $V$ are complex matrices, $a$ is positive real value, $I$ is identity matrix and $^*$ defines conjugate transpose (Hermitian). Thank you. PS. NOT home work, related to my research work in information theory.","Is $$\det(I+aAVV^*A^*)$$ increasing function in $a$? Here $A$, $V$ are complex matrices, $a$ is positive real value, $I$ is identity matrix and $^*$ defines conjugate transpose (Hermitian). Thank you. PS. NOT home work, related to my research work in information theory.",,"['matrices', 'eigenvalues-eigenvectors']"
97,Groups under Multiplication,Groups under Multiplication,,"Let $G=GL(2,\mathbb R)$ and $ H =\left\{  \left[\begin{array}{ccc|c} a & 0 \\ 0 & b \end{array} \right]:\mbox {$a$ and $b$ are nonzero integers }\right \}$ under the operation matrix multiplication. Disprove that $H$ is a subgroup of $G=GL(2,\mathbb R)$. Well I have learned that I have to prove that: 1) Show that e∈H (where e is the identity) 2) Assume that a∈H , b∈H 3) Show that ab∈H 4) Show that $(ab)^{-1}$ (Inverse) So I know that It does not hold, But how do I prove that? When I try to prove that e∈H I only get the Identity matrix and that holds because I get that a and b are nonzero integers. When I prove that a and b is in the set I get that it is because both a and b are nonzero integers and that is the identity matrix. Now I proved that a.b∈H as follows: $$ H =  \left[ \begin{array}{ccc|c} a & 0 \\ 0 & b \end{array} \right] \left[ \begin{array}{ccc|c} c & 0 \\ 0 & d \end{array} \right] =  \left[ \begin{array}{ccc|c} ac & 0 \\ 0 & bd \end{array} \right] $$ With a = b = c = d = 1 I get the Identity matrix again. Now I know if a = b = 2 the subgroup will not hold because the inverse will be a set of rational numbers and H is only a subgroup if it contains only Integer. Is my reasoning correct and if not where did I go wrong?","Let $G=GL(2,\mathbb R)$ and $ H =\left\{  \left[\begin{array}{ccc|c} a & 0 \\ 0 & b \end{array} \right]:\mbox {$a$ and $b$ are nonzero integers }\right \}$ under the operation matrix multiplication. Disprove that $H$ is a subgroup of $G=GL(2,\mathbb R)$. Well I have learned that I have to prove that: 1) Show that e∈H (where e is the identity) 2) Assume that a∈H , b∈H 3) Show that ab∈H 4) Show that $(ab)^{-1}$ (Inverse) So I know that It does not hold, But how do I prove that? When I try to prove that e∈H I only get the Identity matrix and that holds because I get that a and b are nonzero integers. When I prove that a and b is in the set I get that it is because both a and b are nonzero integers and that is the identity matrix. Now I proved that a.b∈H as follows: $$ H =  \left[ \begin{array}{ccc|c} a & 0 \\ 0 & b \end{array} \right] \left[ \begin{array}{ccc|c} c & 0 \\ 0 & d \end{array} \right] =  \left[ \begin{array}{ccc|c} ac & 0 \\ 0 & bd \end{array} \right] $$ With a = b = c = d = 1 I get the Identity matrix again. Now I know if a = b = 2 the subgroup will not hold because the inverse will be a set of rational numbers and H is only a subgroup if it contains only Integer. Is my reasoning correct and if not where did I go wrong?",,"['abstract-algebra', 'group-theory', 'matrices']"
98,"Prove that $A\ge0, B\ge0$ and $A\ge B$ implies $B^{-1}\ge A^{-1}$",Prove that  and  implies,"A\ge0, B\ge0 A\ge B B^{-1}\ge A^{-1}",Does anyone know how to prove the following: Suppose $A$ and $B$ are both positive definite and $A - B$ is positive semi-definite. Show that $B^{-1} - A^{-1}$ is also positive semi-definite. I really appreciate any comments!,Does anyone know how to prove the following: Suppose $A$ and $B$ are both positive definite and $A - B$ is positive semi-definite. Show that $B^{-1} - A^{-1}$ is also positive semi-definite. I really appreciate any comments!,,"['linear-algebra', 'matrices', 'positive-semidefinite']"
99,Show that the matrix $A+E$ is invertible.,Show that the matrix  is invertible.,A+E,"Let $A$ be an invertible matrix, and let $E$ be an upper triangular matrix with zeros on the diagonal. Assume that $AE=EA$. Show that the matrix $A+E$ is invertible. WLOG, we can assume $E$ is Jordan form. If $A$ is Jordan form, it's trivial. If $A$ is not Jordan form, how to use $AE=EA$ to transform $A$ to a Jordan form? Any suggestion? Thanks.","Let $A$ be an invertible matrix, and let $E$ be an upper triangular matrix with zeros on the diagonal. Assume that $AE=EA$. Show that the matrix $A+E$ is invertible. WLOG, we can assume $E$ is Jordan form. If $A$ is Jordan form, it's trivial. If $A$ is not Jordan form, how to use $AE=EA$ to transform $A$ to a Jordan form? Any suggestion? Thanks.",,"['linear-algebra', 'matrices']"
