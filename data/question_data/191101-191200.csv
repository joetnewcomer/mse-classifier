,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$f(x)=|\cos x|+|\sin(2-x)|$ at which of the following point $f$ is not differentiable?,at which of the following point  is not differentiable?,f(x)=|\cos x|+|\sin(2-x)| f,"$f(x)=|\cos x|+|\sin(2-x)|$ at which of the following point $f$ is not differentiable? 1.$\{(2n+1){\pi\over2}\}$ 2.$\{n\pi\}$ 3.$\{{n\pi\over 2}\}$ 4.$\{n\pi+2\}$ in all cases $n\in\mathbb{Z}$ well, is there any easy trick to solve this type of probelem in an competitive examination? Do I need to pick $\pi \over 2$ from $1$ and cheked that $f$ is differentiable and same trick from each set? in some what I am lost.","$f(x)=|\cos x|+|\sin(2-x)|$ at which of the following point $f$ is not differentiable? 1.$\{(2n+1){\pi\over2}\}$ 2.$\{n\pi\}$ 3.$\{{n\pi\over 2}\}$ 4.$\{n\pi+2\}$ in all cases $n\in\mathbb{Z}$ well, is there any easy trick to solve this type of probelem in an competitive examination? Do I need to pick $\pi \over 2$ from $1$ and cheked that $f$ is differentiable and same trick from each set? in some what I am lost.",,"['real-analysis', 'derivatives']"
1,"If $x^2-xy+2y^2=\frac{a^2}{7}$, find $y'''$.","If , find .",x^2-xy+2y^2=\frac{a^2}{7} y''',"If $x^2-xy+2y^2=\frac{a^2}{7}$ , find $y'''$ . For our 1st derivative we got $$y'=\frac{2x-y}{x-4y}.$$ For the second derivative we got $$y''=\frac{14x^2-14xy+28y^2}{(x-4y)^3}.$$ And for the final answer we got $$y'''=\frac{4(-84x^3+119x^2y-154xy^2-84y^3)}{(x-4y)^5}.$$ Took me 2 hours and 3 white boards to get to that answer, and the teacher said another answer was right, also that there was an $a$ in the final answer, even though the derivative of a constant is zero. any insight or help is greatly appreciated, and will try to post all my work as an answer.","If , find . For our 1st derivative we got For the second derivative we got And for the final answer we got Took me 2 hours and 3 white boards to get to that answer, and the teacher said another answer was right, also that there was an in the final answer, even though the derivative of a constant is zero. any insight or help is greatly appreciated, and will try to post all my work as an answer.",x^2-xy+2y^2=\frac{a^2}{7} y''' y'=\frac{2x-y}{x-4y}. y''=\frac{14x^2-14xy+28y^2}{(x-4y)^3}. y'''=\frac{4(-84x^3+119x^2y-154xy^2-84y^3)}{(x-4y)^5}. a,"['calculus', 'derivatives', 'implicit-differentiation']"
2,Differentiation under the Integral sign for the Lebesgue integral,Differentiation under the Integral sign for the Lebesgue integral,,"I want to prove the following version of Liebniz's Rule: Let $f:[a,b]\times [c,d]\to \mathbb{R}$ be integrable with respect to the first variable, $\phi,\psi:[c,d]\to [a,b]$ be differentiable and let $F:[c,d]\to \mathbb{R}$, \begin{equation}F(y)=\int_{\phi(y)}^{\psi (y)}f(x,y)\, dx\end{equation} If $f$ is partially differentiable with respect to $y\in [c,d]$ and there exists an integrable functions $g:[a,b]\to \mathbb{R}$ so that \begin{equation}\forall x\in [a,b]\ \left|\partial_y f(x,y)\right|\le g(x)\end{equation} then $F$ is differentiable in $[c,d]$ and it holds \begin{equation}F'(y)=f(\psi(y),y)\psi'(y)-f(\phi(y),y)\phi'(y)+\int_{\phi(y)}^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} almost everywhere in $[c,d]$. Proof: By linearity it suffices to deal with constant $\phi(y)=a$. Now let $y_n\to y$ in $[c,d]$ with $y_n\neq y$. Then, \begin{equation}\frac{F(y_n)-F(y)}{y_n-y}=\frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)\, dx-\int_{a}^{\psi (y)}f(x,y)\, dx\right)=\\ \frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)-f(x,y)\, dx\right)+\frac1{y_n-y} \int_{\psi(y)}^{\psi (y_n)}f(x,y)\, dx\end{equation} The limit of the second term is essentialy the derivative of $\int_a^{\phi(t)}f(x,y) dx$ with respect to $t$ at $y$. By Lebesgue's differentiation theorem, \begin{equation}\lim_{y\to +\infty}\frac1{y_n-y}\int_{\psi(y)}^{\psi (y_n)}f(x,y)\, dx=f(\psi(y),y)\psi'(y)\end{equation} almost everywhere. For the first term, consider the integrable function \begin{equation}h_n(x)=\frac{f(x,y_n)-f(x,y)}{y_n-y}\end{equation} By the Mean Value Theorem there exists some $\xi_{n,x}$ between $y_n$ and $y$ with \begin{equation}h_n(x)=\frac{f(x,y_n)-f(x,y)}{y_n-y}=\partial_yf(x,\xi_{n,x})\end{equation} By the Dominated Convergence Theorem, as $h_n(x)\to \partial_yf(x,y)$, \begin{equation}\int_a^{\psi(y_m)}\partial_yf(x,t)\, dx=\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right) \end{equation} and so by the continuity of the integral, \begin{equation}\lim_{m\to+\infty}\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} and similarly, \begin{equation}\lim_{n\to+\infty}\lim_{m\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} As I asked here earlier today we can't deduce that \begin{equation}\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} So how do we proceed?","I want to prove the following version of Liebniz's Rule: Let $f:[a,b]\times [c,d]\to \mathbb{R}$ be integrable with respect to the first variable, $\phi,\psi:[c,d]\to [a,b]$ be differentiable and let $F:[c,d]\to \mathbb{R}$, \begin{equation}F(y)=\int_{\phi(y)}^{\psi (y)}f(x,y)\, dx\end{equation} If $f$ is partially differentiable with respect to $y\in [c,d]$ and there exists an integrable functions $g:[a,b]\to \mathbb{R}$ so that \begin{equation}\forall x\in [a,b]\ \left|\partial_y f(x,y)\right|\le g(x)\end{equation} then $F$ is differentiable in $[c,d]$ and it holds \begin{equation}F'(y)=f(\psi(y),y)\psi'(y)-f(\phi(y),y)\phi'(y)+\int_{\phi(y)}^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} almost everywhere in $[c,d]$. Proof: By linearity it suffices to deal with constant $\phi(y)=a$. Now let $y_n\to y$ in $[c,d]$ with $y_n\neq y$. Then, \begin{equation}\frac{F(y_n)-F(y)}{y_n-y}=\frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)\, dx-\int_{a}^{\psi (y)}f(x,y)\, dx\right)=\\ \frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)-f(x,y)\, dx\right)+\frac1{y_n-y} \int_{\psi(y)}^{\psi (y_n)}f(x,y)\, dx\end{equation} The limit of the second term is essentialy the derivative of $\int_a^{\phi(t)}f(x,y) dx$ with respect to $t$ at $y$. By Lebesgue's differentiation theorem, \begin{equation}\lim_{y\to +\infty}\frac1{y_n-y}\int_{\psi(y)}^{\psi (y_n)}f(x,y)\, dx=f(\psi(y),y)\psi'(y)\end{equation} almost everywhere. For the first term, consider the integrable function \begin{equation}h_n(x)=\frac{f(x,y_n)-f(x,y)}{y_n-y}\end{equation} By the Mean Value Theorem there exists some $\xi_{n,x}$ between $y_n$ and $y$ with \begin{equation}h_n(x)=\frac{f(x,y_n)-f(x,y)}{y_n-y}=\partial_yf(x,\xi_{n,x})\end{equation} By the Dominated Convergence Theorem, as $h_n(x)\to \partial_yf(x,y)$, \begin{equation}\int_a^{\psi(y_m)}\partial_yf(x,t)\, dx=\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right) \end{equation} and so by the continuity of the integral, \begin{equation}\lim_{m\to+\infty}\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} and similarly, \begin{equation}\lim_{n\to+\infty}\lim_{m\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_m)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} As I asked here earlier today we can't deduce that \begin{equation}\lim_{n\to+\infty}\frac1{y_n-y}\left(\int_{a}^{\psi (y_n)}f(x,y_n)-f(x,y)\, dx\right)=\int_a^{\psi(y)}\partial_yf(x,y)\, dx\end{equation} So how do we proceed?",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'partial-derivative']"
3,Differential of $x= (A\cos t +B\sin t)e^{-3t}$,Differential of,x= (A\cos t +B\sin t)e^{-3t},Just a quick question. I'm trying to find the solution of the following differential equations satisfying the given conditions: My general solution was: $x= (A\cos t +B\sin t)e^{-3t}$. I think I'm going wrong on the differentiation. Is the differential  $x'= -3(A\cos t +B\sin t)e^{-3t}.(-A\sin t+B\cos t)$ ? I'm supposed to get an $A$ value=1 (which I got using the above calculation) and a $B$ value=3 (but I keep getting a $B$ value of 0) Does anybody understand where I'm going wrong? Many thanks,Just a quick question. I'm trying to find the solution of the following differential equations satisfying the given conditions: My general solution was: $x= (A\cos t +B\sin t)e^{-3t}$. I think I'm going wrong on the differentiation. Is the differential  $x'= -3(A\cos t +B\sin t)e^{-3t}.(-A\sin t+B\cos t)$ ? I'm supposed to get an $A$ value=1 (which I got using the above calculation) and a $B$ value=3 (but I keep getting a $B$ value of 0) Does anybody understand where I'm going wrong? Many thanks,,['derivatives']
4,How do I set up this Newton's Method problem?,How do I set up this Newton's Method problem?,,"I'm looking at problem #40. Before going further, this is homework, so I don't want the answer. I just want guidance, and if I'm on the wrong track, I want to be pushed in the right direction. Single Variable Calculus - Problem # 40 So, I'm looking for L1 and L2 . Here is what I'm doing for L1 Write equation Get derivative of equation Simplify Use Newton's Method to solve My work for L1 $$x^5 - (2+r)x^4 + (1+2r)x^3 - (1-r)x^2  + 2(1-r)x + r - 1 = 0$$ $$ 5x^4 - 4(2+r)x^3 + 3(1+2r)x^2 - 2(1-r)x  + 2(1-r) + r = 0 $$ $$ 5x^4 - 8x^3 + 4rx^3 + 3x^2 + 6rx^2 - 2x - 2rx - 2r + r +2 = 0 $$ I assume after that I fill in the r value and then use Newton's Method to finish it off? Now for L2 Write equation Get derivative of equation Simplify Use Newton's Method to solve Here is what I'm doing for L2 $$ p(x) - 2rx^2 = 0 $$ $$ p(x)' - 4rx $$ $$ 5x^4 - 8x^3 + 4rx^3 + 3x^2 + 6rx^2 - 2x - 6rx - 2r + r +2 = 0 $$ Can someone check my work and tell me whether I'm on the correct track? If I am, do my derivatives look correct?","I'm looking at problem #40. Before going further, this is homework, so I don't want the answer. I just want guidance, and if I'm on the wrong track, I want to be pushed in the right direction. Single Variable Calculus - Problem # 40 So, I'm looking for L1 and L2 . Here is what I'm doing for L1 Write equation Get derivative of equation Simplify Use Newton's Method to solve My work for L1 $$x^5 - (2+r)x^4 + (1+2r)x^3 - (1-r)x^2  + 2(1-r)x + r - 1 = 0$$ $$ 5x^4 - 4(2+r)x^3 + 3(1+2r)x^2 - 2(1-r)x  + 2(1-r) + r = 0 $$ $$ 5x^4 - 8x^3 + 4rx^3 + 3x^2 + 6rx^2 - 2x - 2rx - 2r + r +2 = 0 $$ I assume after that I fill in the r value and then use Newton's Method to finish it off? Now for L2 Write equation Get derivative of equation Simplify Use Newton's Method to solve Here is what I'm doing for L2 $$ p(x) - 2rx^2 = 0 $$ $$ p(x)' - 4rx $$ $$ 5x^4 - 8x^3 + 4rx^3 + 3x^2 + 6rx^2 - 2x - 6rx - 2r + r +2 = 0 $$ Can someone check my work and tell me whether I'm on the correct track? If I am, do my derivatives look correct?",,"['calculus', 'derivatives']"
5,Is this function differentiable at 0?,Is this function differentiable at 0?,,"I would like to know if this function is differentiable at the origin: $$f(x) = \left\{ 	\begin{array}{cl} 		x+x^2  & \mbox{if } x \in \mathbb{Q}; \\ 		x & \mbox{if } x \not\in \mathbb{Q}.	\end{array} \right.$$ Intuitively, I know it is, but I don't know how to prove it. Any ideas? Thanks a lot.","I would like to know if this function is differentiable at the origin: $$f(x) = \left\{ 	\begin{array}{cl} 		x+x^2  & \mbox{if } x \in \mathbb{Q}; \\ 		x & \mbox{if } x \not\in \mathbb{Q}.	\end{array} \right.$$ Intuitively, I know it is, but I don't know how to prove it. Any ideas? Thanks a lot.",,"['calculus', 'real-analysis', 'derivatives']"
6,When is it correct to treat differential(e.g. $dx$) as regular number,When is it correct to treat differential(e.g. ) as regular number,dx,Many times I saw that people tread differentials as regular numbers. They for example multiply by them or divide. E.g. below there is multiplication of $\frac{df(x)}{d (x^2)}$ by $\frac{1/dx}{1/dx}$ $$\frac{df(x)}{d (x^2)} = \frac{df(x)/dx}{d(x^2)/dx} = \frac{1}{2 x} \frac{df(x)}{dx}$$ My question is when it is legal to do such things? Always in some specific cases? Are there some formal theories that formalize this nice trick?,Many times I saw that people tread differentials as regular numbers. They for example multiply by them or divide. E.g. below there is multiplication of $\frac{df(x)}{d (x^2)}$ by $\frac{1/dx}{1/dx}$ $$\frac{df(x)}{d (x^2)} = \frac{df(x)/dx}{d(x^2)/dx} = \frac{1}{2 x} \frac{df(x)}{dx}$$ My question is when it is legal to do such things? Always in some specific cases? Are there some formal theories that formalize this nice trick?,,"['calculus', 'derivatives']"
7,Why doesn't Dirichlet function have a derivative in X=0,Why doesn't Dirichlet function have a derivative in X=0,,"$\newcommand{\dirichlet}{\mathop{\rm dirichlet}\nolimits}$ I'm trying to find two examples for the following criterias: A method that is continuous in exactly one point but doesn't have a derivative  in that point A method that is continuous in exactly one point and does have a derivative  in that point After looking deeper at some examples, I found out that $f(x) = x\cdot\dirichlet(x)$ doesn't have a deriviate at $x = 0$ however $f(x) = x^2\cdot \dirichlet(x)$ does have. I can't understand the difference between the two. Any helps could help.","$\newcommand{\dirichlet}{\mathop{\rm dirichlet}\nolimits}$ I'm trying to find two examples for the following criterias: A method that is continuous in exactly one point but doesn't have a derivative  in that point A method that is continuous in exactly one point and does have a derivative  in that point After looking deeper at some examples, I found out that $f(x) = x\cdot\dirichlet(x)$ doesn't have a deriviate at $x = 0$ however $f(x) = x^2\cdot \dirichlet(x)$ does have. I can't understand the difference between the two. Any helps could help.",,"['derivatives', 'continuity']"
8,Differentiation with respect to integral boundary,Differentiation with respect to integral boundary,,"Using the chain rule show the following proposition: Let $f$ be continuously on $[a,b]$ and $g:J\to[a,b]$ continuously differentiable for an interval $J$. We write $$H(x)=\int\limits_{a}^{g(x)}f(t)\,\mathrm dt,\qquad x\in J.$$ $H$ is differentiable and one has $$H'(x)=f\left(g\left(x\right)\right)g'(x).$$ After proving the correctness of the proposition use it to compute the derivative of $$H(x)=\int\limits_{1}^{\exp(x)}\ln(2t)\,\mathrm dt.$$ Proof. Let $h\in\mathbb{R}$ with $h>0$ and $x+h\in[a,b]$. Then we know that $$\begin{align} 	\frac{H(x+h)-H(x)}{h} &= \frac{1}{h}\left(\int\limits_{a}^{g(x+h)}f(t)\,\mathrm dt - \int\limits_{a}^{g(x)}f(t)\,\mathrm dt\right)\\ 	&= \frac{1}{h}\int\limits_{g(x)}^{g(x+h)}f(t)\,\mathrm dt\\ 	&\overset{(1)}{=} \frac{1}{h}\int\limits_{x}^{x+h}f(g(t))g'(t)\,\mathrm dt = \frac{1}{h}\int\limits_{x}^{x+h}H'(t)\,\mathrm dt\\ 	&\overset{(2)}{=} \frac{1}{h}\cdot h\cdot H'(\xi)=H'(\xi),\qquad \xi\in[x,x+h]. \end{align}$$ In $(1)$ we just use backwards substitution for integration to move $g$ out of the integral boundary into the integrand. $(2)$ is more tricky because we know that for every continuously function $f:[a,b]\to\mathbb{R}$ there exists one $\xi\in[a,b]$ with $$\int\limits^b_af(x)\,\mathrm dx=(b-a)f(\xi).$$ In our case it is obvious that $(x+h)-x=h$. Now we see that $h\to 0$ yields $\xi\to x$ and therefore $$\frac{H(x+h)-H(x)}{h}\longrightarrow H'(x).$$ The same principle applies for $h<0$ and $h\to 0$.$\quad\square$ Example. Let now $$H(x)=\int\limits_{1}^{\exp(x)}\ln(2t)\,\mathrm dt$$ then $f(t)=\ln(2t),\,g(t)=\exp(t)$. This yields $$\begin{align} 	H'(x)&=\ln(2\exp(x))\cdot\exp(x)=\exp(x)(x+\ln(2)). \end{align}$$ I would like to know whether my approach is correct and whether I could simplify some steps in there, as I usually do everything more complicated than neccessary.","Using the chain rule show the following proposition: Let $f$ be continuously on $[a,b]$ and $g:J\to[a,b]$ continuously differentiable for an interval $J$. We write $$H(x)=\int\limits_{a}^{g(x)}f(t)\,\mathrm dt,\qquad x\in J.$$ $H$ is differentiable and one has $$H'(x)=f\left(g\left(x\right)\right)g'(x).$$ After proving the correctness of the proposition use it to compute the derivative of $$H(x)=\int\limits_{1}^{\exp(x)}\ln(2t)\,\mathrm dt.$$ Proof. Let $h\in\mathbb{R}$ with $h>0$ and $x+h\in[a,b]$. Then we know that $$\begin{align} 	\frac{H(x+h)-H(x)}{h} &= \frac{1}{h}\left(\int\limits_{a}^{g(x+h)}f(t)\,\mathrm dt - \int\limits_{a}^{g(x)}f(t)\,\mathrm dt\right)\\ 	&= \frac{1}{h}\int\limits_{g(x)}^{g(x+h)}f(t)\,\mathrm dt\\ 	&\overset{(1)}{=} \frac{1}{h}\int\limits_{x}^{x+h}f(g(t))g'(t)\,\mathrm dt = \frac{1}{h}\int\limits_{x}^{x+h}H'(t)\,\mathrm dt\\ 	&\overset{(2)}{=} \frac{1}{h}\cdot h\cdot H'(\xi)=H'(\xi),\qquad \xi\in[x,x+h]. \end{align}$$ In $(1)$ we just use backwards substitution for integration to move $g$ out of the integral boundary into the integrand. $(2)$ is more tricky because we know that for every continuously function $f:[a,b]\to\mathbb{R}$ there exists one $\xi\in[a,b]$ with $$\int\limits^b_af(x)\,\mathrm dx=(b-a)f(\xi).$$ In our case it is obvious that $(x+h)-x=h$. Now we see that $h\to 0$ yields $\xi\to x$ and therefore $$\frac{H(x+h)-H(x)}{h}\longrightarrow H'(x).$$ The same principle applies for $h<0$ and $h\to 0$.$\quad\square$ Example. Let now $$H(x)=\int\limits_{1}^{\exp(x)}\ln(2t)\,\mathrm dt$$ then $f(t)=\ln(2t),\,g(t)=\exp(t)$. This yields $$\begin{align} 	H'(x)&=\ln(2\exp(x))\cdot\exp(x)=\exp(x)(x+\ln(2)). \end{align}$$ I would like to know whether my approach is correct and whether I could simplify some steps in there, as I usually do everything more complicated than neccessary.",,"['real-analysis', 'integration', 'derivatives']"
9,Exponential map on manifolds and differential,Exponential map on manifolds and differential,,"I am trying to understand the proof of Theorem 3.7, page 72 of Riemannian Geometry by M. Do Carmo. For $M$ a Riemannian manifold and $(U,\varphi)$ a chart around a point    $p\in M$, he (more or less) defines a map $$F: TU\to M\times M$$ by $F(q,v)=(q,\exp_q  v)$. He then asserts that the matrix of $dF_{(p,0)}$ in coordinates    $(TU, T\varphi)$ and $(U\times U,\varphi\times\varphi)$ is    $$\left(\begin{matrix} I&I\\ 0&I \end{matrix}\right),$$    because $(d\exp_p)_0=I$. I really do not understand why! For me, the expression of $F$ in coordinates is $$(x_1,\dots,x_n,v_1,\dots,v_n)\mapsto (x_1,\dots,x_n,\exp_{x_1,\dots,x_n}(v_1,\dots,v_n))...$$","I am trying to understand the proof of Theorem 3.7, page 72 of Riemannian Geometry by M. Do Carmo. For $M$ a Riemannian manifold and $(U,\varphi)$ a chart around a point    $p\in M$, he (more or less) defines a map $$F: TU\to M\times M$$ by $F(q,v)=(q,\exp_q  v)$. He then asserts that the matrix of $dF_{(p,0)}$ in coordinates    $(TU, T\varphi)$ and $(U\times U,\varphi\times\varphi)$ is    $$\left(\begin{matrix} I&I\\ 0&I \end{matrix}\right),$$    because $(d\exp_p)_0=I$. I really do not understand why! For me, the expression of $F$ in coordinates is $$(x_1,\dots,x_n,v_1,\dots,v_n)\mapsto (x_1,\dots,x_n,\exp_{x_1,\dots,x_n}(v_1,\dots,v_n))...$$",,"['derivatives', 'riemannian-geometry']"
10,Find the equation of the tangent line to a given curve at a given point,Find the equation of the tangent line to a given curve at a given point,,"Find the equation of the tangent line to the curve $x^2 - y^2 +2x-6=0$ in the point $(x,3)$, where $x<0.$ So I tried to find the derivative of the given curve, $2x-2yy' +2=0$...here I replaced the given coordinates and I have that $y'=-3/2$ I replace in $y-3=-1.5(x+5)$ and thats it...is this correct?","Find the equation of the tangent line to the curve $x^2 - y^2 +2x-6=0$ in the point $(x,3)$, where $x<0.$ So I tried to find the derivative of the given curve, $2x-2yy' +2=0$...here I replaced the given coordinates and I have that $y'=-3/2$ I replace in $y-3=-1.5(x+5)$ and thats it...is this correct?",,['derivatives']
11,exercise of derivates and divisibility,exercise of derivates and divisibility,,How to prove that if $p$ is a prime number for all $i\geq p$ and $k\geq 0$ the coefficient of $$\frac{d^i}{dx^i}\left(\frac{x^{p+k}}{(p-1)!}\right)$$ is a integer number multiple of $p$,How to prove that if $p$ is a prime number for all $i\geq p$ and $k\geq 0$ the coefficient of $$\frac{d^i}{dx^i}\left(\frac{x^{p+k}}{(p-1)!}\right)$$ is a integer number multiple of $p$,,['derivatives']
12,Computing a finite binomial sum,Computing a finite binomial sum,,"I want to compute $$S(n,m,a)=\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}\cdot a^k.$$ With $n,m\in\mathbb N$, $a\neq0$ and $S(n,0,a)=(a+1)^n$. What I have found already: I don't see any other options then integrating until we've got Newton's formula and then differentiating it as many times as we integrated. I have found some values:$$S(n,1,a)=na\cdot(a+1)^{n-1}$$ $$S(n,2,a)=an(an+1)(a+1)^{n-2}$$ $$S(n,3,a)=an(a^2n^2+3an-a+1)(a+1)^{n-3}.$$ And, since $\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^k=a\cdot\frac{\mathrm d}{\mathrm da}\int\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^{k-1}\mathrm da=a\cdot\frac{\mathrm d}{\mathrm da}\sum_{k=0}^{n}k^{m-1}\cdot\binom{n}{k}a^k$, I have the recursion $$S(n,m,a)=a\cdot\frac{\mathrm d}{\mathrm da}S(n,m-1,a).$$ But I can't find any general formula.","I want to compute $$S(n,m,a)=\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}\cdot a^k.$$ With $n,m\in\mathbb N$, $a\neq0$ and $S(n,0,a)=(a+1)^n$. What I have found already: I don't see any other options then integrating until we've got Newton's formula and then differentiating it as many times as we integrated. I have found some values:$$S(n,1,a)=na\cdot(a+1)^{n-1}$$ $$S(n,2,a)=an(an+1)(a+1)^{n-2}$$ $$S(n,3,a)=an(a^2n^2+3an-a+1)(a+1)^{n-3}.$$ And, since $\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^k=a\cdot\frac{\mathrm d}{\mathrm da}\int\displaystyle\sum_{k=0}^{n}k^{m}\cdot\binom{n}{k}a^{k-1}\mathrm da=a\cdot\frac{\mathrm d}{\mathrm da}\sum_{k=0}^{n}k^{m-1}\cdot\binom{n}{k}a^k$, I have the recursion $$S(n,m,a)=a\cdot\frac{\mathrm d}{\mathrm da}S(n,m-1,a).$$ But I can't find any general formula.",,"['integration', 'derivatives', 'binomial-coefficients', 'summation']"
13,Prove that $f$ is differentiable,Prove that  is differentiable,f,"One exercise from a list. I have no idea how to finish it. Let $I=[c,d]\subset \mathbb{R}$ . Let $f:I\to \mathbb{R}$ be continuous at $a\in (c,d)$ . Suppose that there exists $L\in \mathbb{R}$ such that $$\lim \frac{f(y_n)-f(x_n)}{y_n-x_n}=L$$ for every pair of sequences $(x_n),(y_n)$ in $I$ , with $x_n<a<y_n$ and $\lim x_n=\lim y_n=a$ . Prove that $f'(a)$ exists and it is equal to $L$ . Any help? Thanks in advance.","One exercise from a list. I have no idea how to finish it. Let . Let be continuous at . Suppose that there exists such that for every pair of sequences in , with and . Prove that exists and it is equal to . Any help? Thanks in advance.","I=[c,d]\subset \mathbb{R} f:I\to \mathbb{R} a\in (c,d) L\in \mathbb{R} \lim \frac{f(y_n)-f(x_n)}{y_n-x_n}=L (x_n),(y_n) I x_n<a<y_n \lim x_n=\lim y_n=a f'(a) L","['real-analysis', 'derivatives']"
14,I want inductively prove that $f^{(n)}(0)=0$ for all n.,I want inductively prove that  for all n.,f^{(n)}(0)=0,"Define a function $f(x)$ such that: \begin{cases} \exp(-1/x^2), & \text{if } x>0, \\ 0, & \text{if } x\leqslant 0. \\ \end{cases} I want to inductively prove that $f^{(n)}(0)=0$ for all $n \geqslant 0$. Any suggestions, please?","Define a function $f(x)$ such that: \begin{cases} \exp(-1/x^2), & \text{if } x>0, \\ 0, & \text{if } x\leqslant 0. \\ \end{cases} I want to inductively prove that $f^{(n)}(0)=0$ for all $n \geqslant 0$. Any suggestions, please?",,"['calculus', 'derivatives', 'induction']"
15,"Evaluation of $\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx$",Evaluation of,"\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx","How to evaluate the following integral/ $$\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx $$ , $\alpha, \beta, m, n$ are real and $k$ is a positive integer. For the case where $\beta=1$ and $m=2$ , we can consider the following integral: $$ \begin{aligned} I(\alpha, k) & =\int_0^{\infty} \frac{x^\alpha}{k^2+x^2} d x \\ & =\frac{k^{\alpha-1}}{2} \int_0^{\infty} \frac{t^{(\alpha-1) / 2}}{1+t} d t \\ & =\frac{k^{\alpha-1}}{2} \mathrm{~B}\left(\frac{1+\alpha}{2}, \frac{1-\alpha}{2}\right) \\ & =\frac{k^{\alpha-1}}{2} \frac{\pi}{\cos \left(\frac{\pi}{2} a\right)} \end{aligned} $$ Differentiating $I(\alpha, k)$ with respect to $\alpha$ yields: $$ \begin{align*}   \int_{0}^{\infty} \, \frac{x^\alpha \log^n{x}}{k^2+x^2}\, dx &= \frac{\partial^{n} }{\partial a^n} \left(\frac{k^{\alpha-1}}{2}\, \frac{\pi}{\cos{\displaystyle \left(\frac{\pi}{2}\alpha\right)}}\right) \end{align*} $$ (based on this answer). Are there any other methods to evaluate the general integral? Thank you!","How to evaluate the following integral/ , are real and is a positive integer. For the case where and , we can consider the following integral: Differentiating with respect to yields: (based on this answer). Are there any other methods to evaluate the general integral? Thank you!","\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx  \alpha, \beta, m, n k \beta=1 m=2 
\begin{aligned}
I(\alpha, k) & =\int_0^{\infty} \frac{x^\alpha}{k^2+x^2} d x \\
& =\frac{k^{\alpha-1}}{2} \int_0^{\infty} \frac{t^{(\alpha-1) / 2}}{1+t} d t \\
& =\frac{k^{\alpha-1}}{2} \mathrm{~B}\left(\frac{1+\alpha}{2}, \frac{1-\alpha}{2}\right) \\
& =\frac{k^{\alpha-1}}{2} \frac{\pi}{\cos \left(\frac{\pi}{2} a\right)}
\end{aligned}
 I(\alpha, k) \alpha 
\begin{align*}
  \int_{0}^{\infty} \, \frac{x^\alpha \log^n{x}}{k^2+x^2}\, dx &= \frac{\partial^{n} }{\partial a^n} \left(\frac{k^{\alpha-1}}{2}\, \frac{\pi}{\cos{\displaystyle \left(\frac{\pi}{2}\alpha\right)}}\right)
\end{align*}
","['integration', 'derivatives', 'definite-integrals', 'logarithms', 'improper-integrals']"
16,A problem of minimum,A problem of minimum,,"Problem : Among all rectangular parallelepipeds having for base a square and of constant volume $V$ , determine the one of minimum total area also has diagonal of minimum length. Solution : Given $x$ and $y$ as the respective measurements of the base edge and height, the volume $V$ is given by $V=x^2y \iff y = \frac{V}{x^2}$ . In principle there is nothing to prevent the base edge from being as large as as desired, so it must be $x ≥ 0$ . The total area will be the sum of the two bases i.e. $2x^2$ and of the lateral faces $4xy$ so $$S(x)=2x^2+4\frac{V}{x}$$ The derivate is: $$S'(x)=\frac{4(x^3-V)}{x^2}$$ cancel if $x=\sqrt[3]{V}$ . The corresponding value of $y$ is $$y=\frac{V}{\sqrt[3]{V^2}}=x$$ so the solution is a cube. That it is a minimum is confirmed by the range of positivity of the first derivative, given by the inequality $x>\sqrt[3]{V}$ . For $x$ between $0$ and $\sqrt[3]{V}$ the derivative is negative, which confirms that it is a point of minimum. Question : How can I determine the one of minimum total area also has diagonal of minimum length ?","Problem : Among all rectangular parallelepipeds having for base a square and of constant volume , determine the one of minimum total area also has diagonal of minimum length. Solution : Given and as the respective measurements of the base edge and height, the volume is given by . In principle there is nothing to prevent the base edge from being as large as as desired, so it must be . The total area will be the sum of the two bases i.e. and of the lateral faces so The derivate is: cancel if . The corresponding value of is so the solution is a cube. That it is a minimum is confirmed by the range of positivity of the first derivative, given by the inequality . For between and the derivative is negative, which confirms that it is a point of minimum. Question : How can I determine the one of minimum total area also has diagonal of minimum length ?",V x y V V=x^2y \iff y = \frac{V}{x^2} x ≥ 0 2x^2 4xy S(x)=2x^2+4\frac{V}{x} S'(x)=\frac{4(x^3-V)}{x^2} x=\sqrt[3]{V} y y=\frac{V}{\sqrt[3]{V^2}}=x x>\sqrt[3]{V} x 0 \sqrt[3]{V},"['calculus', 'derivatives']"
17,"Prove $a^x+a^{1/x}\le 2a$ ($\frac{1}{2}\le a < 1$) for any $x\in (0,\infty)$",Prove  () for any,"a^x+a^{1/x}\le 2a \frac{1}{2}\le a < 1 x\in (0,\infty)","Prove $a^x+a^{1/x}\le 2a$ (with $\frac{1}{2}\le a < 1$ ) for any $x\in (0,\infty)$ . Is it true? I can't figure out how to prove it. It may use the conclusion $\ln x+\frac{1-x}{x}\ge 0, \forall x\in (0,+\infty) $ as this question is followed. UPDATED: I have find a complicated method to prove it. Let $f(x) = a^x+a^{1/x}-2a$ , with $\frac{1}{2}\le a<1, x\in(0,\infty)$ . $\therefore f'(x)=a^{1/x}\ln \frac{1}{a}(\frac{1}{x^2}-a^{x-1/x})$ we have $a^{1/x}\ln\frac{1}{a}>0$ , so we can only consider $g(x)=\frac{1}{x^2}-a^{x-1/x}, x>0, \frac{1}{2}\le a<1$ we can make the observation that $g(1)=0$ , but we are not sure if it has any other roots . So we have to do some careful analysis. If we keep taking derivative, we will get into endless trouble. Note that $g(x)=\frac{1}{x^2}-a^{x-1/x}=a^{-2\log_a x}-a^{x-1/x}$ because $a<1$ , so $a^x$ is decreasing, we can consider $h(x)=-2\log_a x-(x-\frac{1}{x})=-2\log_a x-x+\frac{1}{x},x>0,\frac{1}{2}\le a<1$ This is much easier. $h'(x)=-\dfrac{x^2+\dfrac{2}{\ln a}\cdot x+1}{x^2}$ because $\frac{1}{2}\le a<1$ , so $\frac{2}{\ln a}\le -\frac{2}{\ln 2}\approx -2.9$ . For the quadratic functions in the numerator, $\Delta >0$ , there are two positive zeros $x_1,x_2$ and $0<x_1<1<x_2$ , we have: In $(0,x_1)$ and $(x_2,\infty)$ ， $h'(x)<0$ , so $h(x)$ is decreasing. In $(x_1, x_2)$ , $h'(x)>0$ , so $h(x)$ is increasing. Note that $h(1)=0$ , and when $x\to 0^+$ , $h(x)\to +\infty$ , when $x\to +\infty, h(x)\to -\infty$ , therefore, the graph of $h(x)$ is illustrated as follows: $h(x)$ "" /> Therefore, there is three zeros for $h(x)$ , i.e. $x_3, x_4=1, x_5$ . In $(0,x_3)$ and $(1,x_5)$ , $h(x)>0$ , so $g(x)<0$ , so $f'(x)<0$ , so $f(x)$ is decreasing. In $(x_3,1)$ and $(x_5,+\infty)$ , $h(x)<0$ , so $g(x)>0$ , so $f'(x)>0$ , so $f(x)$ is increasing. Note that $f(1)=0$ , and when $x\to 0^+$ , $f(x)\to 1-2a\le 0$ ; when $x\to +\infty$ , $f(x)\to 1-2a\le 0$ . The graph of $f(x)$ can be illustrated as follows: $f(x)$ "" /> Now, we can surely say, the maximum of $f(x)$ is $f(1)=0$ , which means $a^x+a^{1/x}\le 2a$ , with $\frac{1}{2}\le a<1$ , $\forall x\in(0,+\infty)$ .","Prove (with ) for any . Is it true? I can't figure out how to prove it. It may use the conclusion as this question is followed. UPDATED: I have find a complicated method to prove it. Let , with . we have , so we can only consider we can make the observation that , but we are not sure if it has any other roots . So we have to do some careful analysis. If we keep taking derivative, we will get into endless trouble. Note that because , so is decreasing, we can consider This is much easier. because , so . For the quadratic functions in the numerator, , there are two positive zeros and , we have: In and ， , so is decreasing. In , , so is increasing. Note that , and when , , when , therefore, the graph of is illustrated as follows: $h(x)$ "" /> Therefore, there is three zeros for , i.e. . In and , , so , so , so is decreasing. In and , , so , so , so is increasing. Note that , and when , ; when , . The graph of can be illustrated as follows: $f(x)$ "" /> Now, we can surely say, the maximum of is , which means , with , .","a^x+a^{1/x}\le 2a \frac{1}{2}\le a < 1 x\in (0,\infty) \ln x+\frac{1-x}{x}\ge 0, \forall x\in (0,+\infty)  f(x) = a^x+a^{1/x}-2a \frac{1}{2}\le a<1, x\in(0,\infty) \therefore f'(x)=a^{1/x}\ln \frac{1}{a}(\frac{1}{x^2}-a^{x-1/x}) a^{1/x}\ln\frac{1}{a}>0 g(x)=\frac{1}{x^2}-a^{x-1/x}, x>0, \frac{1}{2}\le a<1 g(1)=0 g(x)=\frac{1}{x^2}-a^{x-1/x}=a^{-2\log_a x}-a^{x-1/x} a<1 a^x h(x)=-2\log_a x-(x-\frac{1}{x})=-2\log_a x-x+\frac{1}{x},x>0,\frac{1}{2}\le a<1 h'(x)=-\dfrac{x^2+\dfrac{2}{\ln a}\cdot x+1}{x^2} \frac{1}{2}\le a<1 \frac{2}{\ln a}\le -\frac{2}{\ln 2}\approx -2.9 \Delta >0 x_1,x_2 0<x_1<1<x_2 (0,x_1) (x_2,\infty) h'(x)<0 h(x) (x_1, x_2) h'(x)>0 h(x) h(1)=0 x\to 0^+ h(x)\to +\infty x\to +\infty, h(x)\to -\infty h(x) h(x) x_3, x_4=1, x_5 (0,x_3) (1,x_5) h(x)>0 g(x)<0 f'(x)<0 f(x) (x_3,1) (x_5,+\infty) h(x)<0 g(x)>0 f'(x)>0 f(x) f(1)=0 x\to 0^+ f(x)\to 1-2a\le 0 x\to +\infty f(x)\to 1-2a\le 0 f(x) f(x) f(1)=0 a^x+a^{1/x}\le 2a \frac{1}{2}\le a<1 \forall x\in(0,+\infty)","['derivatives', 'inequality']"
18,"If $\lim\limits_{m\to\infty}\frac{f(a+h_m)-f(a)}{h_m}$ does not exist, with $\{h_m\}\to 0$, why can we conclude $f$ not differentiable at $a$?","If  does not exist, with , why can we conclude  not differentiable at ?",\lim\limits_{m\to\infty}\frac{f(a+h_m)-f(a)}{h_m} \{h_m\}\to 0 f a,"If we have a function $f(x)$ and we prove that at a point $a$ the limit $$\lim\limits_{m\to\infty} \frac{f(a+h_m)-f(a)}{h_m}$$ does not exist, where $\{h_m\}$ is a sequence that converges to $0$ , why can we conclude that $f$ is not differentiable at $a$ ? In other words, why can we conclude something about the differentiability of a function at a point $a$ based on the limit of a sequence? I think it is related to the following theorem Spivak, Calculus , Ch. 22, Theorem 1 Let $f$ be a function defined in an open interval containing $c$ except perhaps at $c$ . Then $$\lim\limits_{x\to c} f(x)=l$$ $$\iff$$ for every sequence $\{a_n\}$ such that each $a_n$ is in the domain of $f$ each $a_n\neq c$ $\lim\limits_{n\to\infty} a_n=c$ the sequence $\{f(a_n)\}$ satisfies $$\lim\limits_{n\to\infty} f(a_n)=l$$ Let $\{a_n\}=\{a+h_m\}$ be any sequence with $\{h_m\}$ converging to $0$ . Then $a_n\in \mathbb{R}$ $a_n\neq a$ $\lim\limits_{n\to\infty} a_n=a$ Let $g(x)=\frac{f(x)-f(a)}{x-a}$ . Then, $g(a_n)=\frac{f(a+h_m)-f(a)}{h_m}$ is a sequence but we showed previously that it doesn't converge, ie $$\lim\limits_{n\to\infty} g(a_n)$$ does not exist. This seems to mean that the consequent of the theorem above is false. Hence the antecedent is false. And that means that the limit $$\lim\limits_{x\to a} g(x)=\lim\limits_{x\to a} \frac{f(x)-f(a)}{x-a}$$ does not exist, and therefore, $f$ is not differentiable at $a$ . Is this the underlying justification for our conclusion about differentiability of the function $f$ at $a$ based on a limit of a sequence?","If we have a function and we prove that at a point the limit does not exist, where is a sequence that converges to , why can we conclude that is not differentiable at ? In other words, why can we conclude something about the differentiability of a function at a point based on the limit of a sequence? I think it is related to the following theorem Spivak, Calculus , Ch. 22, Theorem 1 Let be a function defined in an open interval containing except perhaps at . Then for every sequence such that each is in the domain of each the sequence satisfies Let be any sequence with converging to . Then Let . Then, is a sequence but we showed previously that it doesn't converge, ie does not exist. This seems to mean that the consequent of the theorem above is false. Hence the antecedent is false. And that means that the limit does not exist, and therefore, is not differentiable at . Is this the underlying justification for our conclusion about differentiability of the function at based on a limit of a sequence?",f(x) a \lim\limits_{m\to\infty} \frac{f(a+h_m)-f(a)}{h_m} \{h_m\} 0 f a a f c c \lim\limits_{x\to c} f(x)=l \iff \{a_n\} a_n f a_n\neq c \lim\limits_{n\to\infty} a_n=c \{f(a_n)\} \lim\limits_{n\to\infty} f(a_n)=l \{a_n\}=\{a+h_m\} \{h_m\} 0 a_n\in \mathbb{R} a_n\neq a \lim\limits_{n\to\infty} a_n=a g(x)=\frac{f(x)-f(a)}{x-a} g(a_n)=\frac{f(a+h_m)-f(a)}{h_m} \lim\limits_{n\to\infty} g(a_n) \lim\limits_{x\to a} g(x)=\lim\limits_{x\to a} \frac{f(x)-f(a)}{x-a} f a f a,"['calculus', 'sequences-and-series', 'derivatives']"
19,Are these two integrals equal for all $n$?,Are these two integrals equal for all ?,n,"I am trying to prove that these two integrals are equal $$ \int_0^\infty\int_0^\infty\dots\int_0^\infty  \exp(-x z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n) x^{c_1-1}   z_1^{c_1+c_2-1} z_2^{c_2+c_3-1} \dots z_{n}^{c_n+c_{n+1}-1} dz_1 dz_2 \dots dz_{n} $$ $$ \int_0^\infty\int_0^\infty\dots\int_0^\infty \exp(-x ^{w_n} z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n) x^{{w_n}\,c_{n+1}-1}   z_n^{c_1+c_2-1} z_{n-1}^{c_2+c_3-1} \dots z_{1}^{c_n+c_{n+1}-1} dz_1 dz_2 \dots dz_{n} $$ where $x>0,c_n>0,\forall n$ and $$w_n = \begin{cases} 1 & \text{even}\,\,n  \\ -1 & \text{odd}\,\,n \end{cases}.$$ Using Mathematica, I was able to prove up to $n=5$ , but is this valid for all $n$ ?","I am trying to prove that these two integrals are equal where and Using Mathematica, I was able to prove up to , but is this valid for all ?","
\int_0^\infty\int_0^\infty\dots\int_0^\infty 
\exp(-x z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n)
x^{c_1-1}  
z_1^{c_1+c_2-1} z_2^{c_2+c_3-1} \dots z_{n}^{c_n+c_{n+1}-1}
dz_1 dz_2 \dots dz_{n}
 
\int_0^\infty\int_0^\infty\dots\int_0^\infty
\exp(-x ^{w_n} z_1-z_1z_2-z_2z_3-\dots-z_{n-1}z_{n}-z_n)
x^{{w_n}\,c_{n+1}-1}  
z_n^{c_1+c_2-1} z_{n-1}^{c_2+c_3-1} \dots z_{1}^{c_n+c_{n+1}-1}
dz_1 dz_2 \dots dz_{n}
 x>0,c_n>0,\forall n w_n = \begin{cases} 1 & \text{even}\,\,n  \\ -1 & \text{odd}\,\,n \end{cases}. n=5 n","['calculus', 'integration', 'derivatives']"
20,Find derivative of inverse of function $y=2x^3-6x$ and calculate it's value at $x=-2$.,Find derivative of inverse of function  and calculate it's value at .,y=2x^3-6x x=-2,Find derivative of inverse of function $y=2x^3-6x$ and calculate it's value at $x=-2$ . My Approach: We know that $(f^{-1}(f(x)))=x$ Taking derivative both side $(f^{-1}(f(x)))' \cdot f'(x)=1$ $(f^{-1}(f(x)))'=\frac{1}{f'(x)}=\dfrac{1}{6x^2-6}$ We want to find $f^{-1}(-2)$ so we must have $f(x)=-2$ i.e. $\quad$$2x^3-6x=-2$ $\implies$ $2x^3-6x+2=0$ I can't find any integer root from here. But given answer is $\frac{1}{18}$ I know other method to solve this problem but can we solve using my approach used above?,Find derivative of inverse of function and calculate it's value at . My Approach: We know that Taking derivative both side We want to find so we must have i.e. I can't find any integer root from here. But given answer is I know other method to solve this problem but can we solve using my approach used above?,y=2x^3-6x x=-2 (f^{-1}(f(x)))=x (f^{-1}(f(x)))' \cdot f'(x)=1 (f^{-1}(f(x)))'=\frac{1}{f'(x)}=\dfrac{1}{6x^2-6} f^{-1}(-2) f(x)=-2 \quad2x^3-6x=-2 \implies 2x^3-6x+2=0 \frac{1}{18},"['calculus', 'derivatives', 'inverse-function']"
21,"Power Rule Derivatives, $f(x) = x^n, f'(x) = nx^{n-1}.$ Why can $n <0?$","Power Rule Derivatives,  Why can","f(x) = x^n, f'(x) = nx^{n-1}. n <0?","I was watching and reading some online articles, and stubbled upon the power rule when dealing with derivatives. It states $f(x) = x^n$ , $n ≠ 0$ then $f'(x) = nx^{n-1}$ I saw some proofs on this but no one addressed the reason why $n ≠ 0$ . So I tested it out. $f(x) = x^0 = 1$ thus, $f'(x) = 0 \times x^{0-1} = 0$ So I now saw why $n ≠ 0$ , however knowing this, it sparked another question into my mind. What happens when $n$ is negative? Once again I tested this out, $f(x) = x^{-1}$ $f'(x) = -1 \times x^{-1 - 1}$ if you look at the last line you, a question appears, if $x = 0$ then, $f'(x) = -1 \times \frac{1}{0^2}$ Dividing by zero is undefined, thus shouldnt the power rule be not defined for $x = 0$ for values $n < 0$ . (This problem also applies for the case where $n = 0$ ) Can anyone give a explanation for all this stuff? Thx!","I was watching and reading some online articles, and stubbled upon the power rule when dealing with derivatives. It states , then I saw some proofs on this but no one addressed the reason why . So I tested it out. thus, So I now saw why , however knowing this, it sparked another question into my mind. What happens when is negative? Once again I tested this out, if you look at the last line you, a question appears, if then, Dividing by zero is undefined, thus shouldnt the power rule be not defined for for values . (This problem also applies for the case where ) Can anyone give a explanation for all this stuff? Thx!",f(x) = x^n n ≠ 0 f'(x) = nx^{n-1} n ≠ 0 f(x) = x^0 = 1 f'(x) = 0 \times x^{0-1} = 0 n ≠ 0 n f(x) = x^{-1} f'(x) = -1 \times x^{-1 - 1} x = 0 f'(x) = -1 \times \frac{1}{0^2} x = 0 n < 0 n = 0,['derivatives']
22,Time derivative of a constant vector in rotating frame,Time derivative of a constant vector in rotating frame,,"Recently I'm learning rotating frame kinematics, but I have come up with a question that confuses me a lot. Let's say a vector $P$ connects the origin and a fixed point in the rotating frame, so it is constant in the rotating frame (local frame, $LF$ ), so that $P_{x,LF}$ , $P_{y,LF}$ , and $P_{z,LF}$ never change. The coordinates of $P$ in the inertial frame ( $IF$ ) can be expressed as: $P_{IF} = R_{LF2IF}P_{LF} $ where $R_{LF2IF}$ denotes the rotation matrix from the local frame to the inertial frame. Based on the above equation, the time derivative of $P_{LF}$ is: $\dot{P_{IF}} = \dot{R_{LF2IF}}P_{LF} + R_{LF2IF}\dot{P_{LF}}$ ------------- (1) According to the rotating frame kinematics Rotating Reference Frame , the time derivative of $P_{LF}$ is: $\frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF} + \omega\times P_{LF}$ Because $P_{LF}$ is constant in the local frame, therefore: $\frac{dP_{LF}}{dt} = 0 + \omega\times P_{LF} = \omega\times P_{LF}$ However I have seen in many places where people consider $\dot{P_{LF}}$ to be zero, which makes me rather confused. It seems that the following condition is true: $\frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF}$ But, why is this, why the cross product is not calculated in this case? When should the cross product be used and when should not? Many thanks to anyone who would kindly help me to clear my confusion.","Recently I'm learning rotating frame kinematics, but I have come up with a question that confuses me a lot. Let's say a vector connects the origin and a fixed point in the rotating frame, so it is constant in the rotating frame (local frame, ), so that , , and never change. The coordinates of in the inertial frame ( ) can be expressed as: where denotes the rotation matrix from the local frame to the inertial frame. Based on the above equation, the time derivative of is: ------------- (1) According to the rotating frame kinematics Rotating Reference Frame , the time derivative of is: Because is constant in the local frame, therefore: However I have seen in many places where people consider to be zero, which makes me rather confused. It seems that the following condition is true: But, why is this, why the cross product is not calculated in this case? When should the cross product be used and when should not? Many thanks to anyone who would kindly help me to clear my confusion.","P LF P_{x,LF} P_{y,LF} P_{z,LF} P IF P_{IF} = R_{LF2IF}P_{LF}  R_{LF2IF} P_{LF} \dot{P_{IF}} = \dot{R_{LF2IF}}P_{LF} + R_{LF2IF}\dot{P_{LF}} P_{LF} \frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF} + \omega\times P_{LF} P_{LF} \frac{dP_{LF}}{dt} = 0 + \omega\times P_{LF} = \omega\times P_{LF} \dot{P_{LF}} \frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF}","['derivatives', 'vectors', 'rotations', 'cross-product', 'kinematics']"
23,how to find the limit of f(x) when x tends to infinity,how to find the limit of f(x) when x tends to infinity,,"Let $f:[1,\infty]\to \mathbb{R}$ be a differentiable function such that $f(1)=1$ and $f'(x)=\frac{1}{1+f(x)^2}$ . Then how can I find the limit of $f(x)$ when $x$ tends to $\infty$ . Clearly $f'(x)>0$ and so $f$ is strictly increasing. Again $f(1)=1$ . So $f(x)\geq 1$ for $\forall x\in \mathbb{R}$ . Now I don't get any way to calculate the limit of $f(x)$ . Please help.",Let be a differentiable function such that and . Then how can I find the limit of when tends to . Clearly and so is strictly increasing. Again . So for . Now I don't get any way to calculate the limit of . Please help.,"f:[1,\infty]\to \mathbb{R} f(1)=1 f'(x)=\frac{1}{1+f(x)^2} f(x) x \infty f'(x)>0 f f(1)=1 f(x)\geq 1 \forall x\in \mathbb{R} f(x)","['real-analysis', 'derivatives']"
24,"Partial derivative of the partial derivative of a function, with respect to the function itself","Partial derivative of the partial derivative of a function, with respect to the function itself",,"I'm trying to derive this partial derivative of a function $f$ with respect to $u_x$ and $u_y$ with the following form: $$ f(u_x, u_y) = \left(\frac{\partial u_x}{\partial x}\right)^2 + \left(\frac{\partial u_y}{\partial y} \right)^2 $$ where both $u_x$ and $u_y$ are functions of $x$ and $y$ , i.e. $u_x=u_x(x,y), u_y = u_y(x,y)$ . A more compact way to think about this is $f$ is a scalar function with a two-dimensional vector field as input, with $u_x$ and $u_y$ the components of that vector field. And I got to the point where $$ \frac{\partial f}{\partial u_x} = 2 \frac{\partial u_x}{\partial x} \frac{\partial}{\partial u_x} \frac{\partial u_x}{\partial x} $$ (similarly for partial w.r.t. $u_y$ ), Then I'm stuck on this partial derivative on the right $\frac{\partial}{\partial u_x} \frac{\partial u_x}{\partial x}$ , which seems to be the partial derivative of the partial derivative of the function, but with respect to the function itself. I don't really know how to proceed from here. Can anyone help with some pointers? Really appreciate the help.","I'm trying to derive this partial derivative of a function with respect to and with the following form: where both and are functions of and , i.e. . A more compact way to think about this is is a scalar function with a two-dimensional vector field as input, with and the components of that vector field. And I got to the point where (similarly for partial w.r.t. ), Then I'm stuck on this partial derivative on the right , which seems to be the partial derivative of the partial derivative of the function, but with respect to the function itself. I don't really know how to proceed from here. Can anyone help with some pointers? Really appreciate the help.","f u_x u_y 
f(u_x, u_y) = \left(\frac{\partial u_x}{\partial x}\right)^2 + \left(\frac{\partial u_y}{\partial y} \right)^2
 u_x u_y x y u_x=u_x(x,y), u_y = u_y(x,y) f u_x u_y 
\frac{\partial f}{\partial u_x} = 2 \frac{\partial u_x}{\partial x} \frac{\partial}{\partial u_x} \frac{\partial u_x}{\partial x}
 u_y \frac{\partial}{\partial u_x} \frac{\partial u_x}{\partial x}","['derivatives', 'partial-derivative', 'vector-fields']"
25,Change of variables during differentiation,Change of variables during differentiation,,If we have a function: $\psi(x)$ and $x=\rho\gamma$ where $\rho$ is a constant Then is the following statement correct: $1)~~$ $\psi(x) =\psi(\rho \gamma)=\rho \psi(\rho)$ What is the equation for : $\frac{d\psi(\gamma)}{d\gamma}$ and $\frac{d^2\psi(\gamma)}{d\gamma^2}$ Below is my attempt at finding it: This question is the context of findings the solution of simple harmonic oscillator I am having trouble understanding how $9.19$ is derived from $9.18$ .,If we have a function: and where is a constant Then is the following statement correct: What is the equation for : and Below is my attempt at finding it: This question is the context of findings the solution of simple harmonic oscillator I am having trouble understanding how is derived from .,\psi(x) x=\rho\gamma \rho 1)~~ \psi(x) =\psi(\rho \gamma)=\rho \psi(\rho) \frac{d\psi(\gamma)}{d\gamma} \frac{d^2\psi(\gamma)}{d\gamma^2} 9.19 9.18,"['derivatives', 'quantum-mechanics', 'change-of-variable']"
26,Checking exercise on uniform convergence and differentiation,Checking exercise on uniform convergence and differentiation,,"I have solved the following exercise and I would like to know if there are any mistakes. Thank you. Consider the sequence of functions defined by $g_n(x)=\frac{x^n}{n}$ . (a) Show $(g_n)$ converges uniformly on $[0,1]$ and find $g=\lim g_n$ . Show that $g$ is differentiable and compute $g'(x)$ for all $x\in [0,1]$ . (b) Now, show that $(g_n')$ converges on $[0,1]$ . Is the convergence uniform? Set $h=\lim g_n'(x)$ and compare $h$ and $g'$ . Are they the same? My solution: (a) Let $\varepsilon>0$ : then $|g_n(x)-0|=|\frac{x^n}{n}|\leq\frac{1}{n}$ for $x\in [0,1]$ so it suffices to choose $N>\frac{1}{\varepsilon}$ to guarantee that $|g_n(x)-0|<\varepsilon$ for all $n\geq N$ and $x\in [0,1]$ so $(g_n)$ converges uniformly to $0\equiv g(x)$ which, being a constant function, is differentiable and has $g'(x)\equiv 0$ too. (b) $g_n'(x)=x^{n-1}$ converges pointwise to $0$ for $x\in [0,1)$ and to $1$ for $x=1$ , so $h(x)=\lim g'_n(x)=\begin{cases}                                 0 & \text{if $x\in [0,1)$}\\                                 1 & \text{if $x=1$} \end{cases}\neq g'(x)$ and the convergence is not uniform: by the Continuous Limit Theorem) since $(g'_n)$ is a sequence of continuous functions, if it converged uniformly its limit $h$ would be continuous too and that is not the case.","I have solved the following exercise and I would like to know if there are any mistakes. Thank you. Consider the sequence of functions defined by . (a) Show converges uniformly on and find . Show that is differentiable and compute for all . (b) Now, show that converges on . Is the convergence uniform? Set and compare and . Are they the same? My solution: (a) Let : then for so it suffices to choose to guarantee that for all and so converges uniformly to which, being a constant function, is differentiable and has too. (b) converges pointwise to for and to for , so and the convergence is not uniform: by the Continuous Limit Theorem) since is a sequence of continuous functions, if it converged uniformly its limit would be continuous too and that is not the case.","g_n(x)=\frac{x^n}{n} (g_n) [0,1] g=\lim g_n g g'(x) x\in [0,1] (g_n') [0,1] h=\lim g_n'(x) h g' \varepsilon>0 |g_n(x)-0|=|\frac{x^n}{n}|\leq\frac{1}{n} x\in [0,1] N>\frac{1}{\varepsilon} |g_n(x)-0|<\varepsilon n\geq N x\in [0,1] (g_n) 0\equiv g(x) g'(x)\equiv 0 g_n'(x)=x^{n-1} 0 x\in [0,1) 1 x=1 h(x)=\lim g'_n(x)=\begin{cases}
                                0 & \text{if x\in [0,1)}\\
                                1 & \text{if x=1}
\end{cases}\neq g'(x) (g'_n) h","['real-analysis', 'derivatives', 'solution-verification', 'uniform-convergence']"
27,Techniques for backwards-Leibniz theorem of product of derivative?,Techniques for backwards-Leibniz theorem of product of derivative?,,"If I have a function like $$u(t)f'g + w(t) fg'$$ and I know that for general $f,g$ : $$(fg)' = f'g + fg'$$ Is there some general way for me to modify the expression so that I can end up with derivative of a product? Own work: $$\frac{\partial }{\partial t}\left\{f(a(t)) \cdot g(b(t))\right\} = a'(t)f'(a(t))\cdot g(b(t)) +f(a(t))\cdot b'(t)g'(b(t))$$ So if I can get $b',a'$ to match $u,w$ then it shall work. A special case when this works should be ""time-flipping"" $a(t) = -t$ $$\sin(t) y(t) + \cos(t) y'(t) =/y(-t) = u(t)/= (u(-t)(-\cos(t)))'$$ But this is just a toy example to see that it works for a simple function. I am curious if it always will be possible or at least for some reasonably large family of functions.","If I have a function like and I know that for general : Is there some general way for me to modify the expression so that I can end up with derivative of a product? Own work: So if I can get to match then it shall work. A special case when this works should be ""time-flipping"" But this is just a toy example to see that it works for a simple function. I am curious if it always will be possible or at least for some reasonably large family of functions.","u(t)f'g + w(t) fg' f,g (fg)' = f'g + fg' \frac{\partial }{\partial t}\left\{f(a(t)) \cdot g(b(t))\right\} = a'(t)f'(a(t))\cdot g(b(t)) +f(a(t))\cdot b'(t)g'(b(t)) b',a' u,w a(t) = -t \sin(t) y(t) + \cos(t) y'(t) =/y(-t) = u(t)/= (u(-t)(-\cos(t)))'","['calculus', 'linear-algebra', 'derivatives', 'soft-question']"
28,Interpreting derivative questions in context,Interpreting derivative questions in context,,"I have these two questions and I am able to approximate $S''(10)$ like this: $\frac{S'(12)-S'(8)}{12-8}=-0.05$ I am just unsure how to interpret the meaning of $S''(x)$ in context. I know that the second derivative tells you whether your graph is concave up or concave down, which tells you whether or not the the first derivative (slope of the tangent line) is increasing or decreasing. So would an adequate explanation be: $S''(x)$ shows the change in rate of snow deposition with respect to time? For the second problem, I have the same question. I have $D'(t) = \frac{92}{40}e^{-t/40}$ and $D'(10)\approx 1.79$ cm/hr Is the correct interpretation: $D'(10)$ is the rate that the snow is depositing in centimeters per hour at $t=10$ hours.","I have these two questions and I am able to approximate like this: I am just unsure how to interpret the meaning of in context. I know that the second derivative tells you whether your graph is concave up or concave down, which tells you whether or not the the first derivative (slope of the tangent line) is increasing or decreasing. So would an adequate explanation be: shows the change in rate of snow deposition with respect to time? For the second problem, I have the same question. I have and cm/hr Is the correct interpretation: is the rate that the snow is depositing in centimeters per hour at hours.",S''(10) \frac{S'(12)-S'(8)}{12-8}=-0.05 S''(x) S''(x) D'(t) = \frac{92}{40}e^{-t/40} D'(10)\approx 1.79 D'(10) t=10,"['calculus', 'derivatives']"
29,How to prove that the elasticity of the revenue function is $E_R(p)=\frac{E_R(q)}{E_R(q)-1}$?,How to prove that the elasticity of the revenue function is ?,E_R(p)=\frac{E_R(q)}{E_R(q)-1},"The Problem Let the demand function be $ap+bq=k$ . Prove that this equation (elasticity of revenue) is true: $$E_R(p)=\frac{E_R(q)}{E_R(q)-1}$$ Definitions Demand Function The Demand Function is defined as the relation between the price $p$ of the good and the demanded quantity $q$ of the good which in our example is: $ap+bq=k$ . Note that $D^{-1}(p) = G(q)$ Revenue Function The Revenue Function is defined as $R = p q$ , where R is the total revenue, $p$ is the selling price per unit of sales, and $q$ is the number of units sold Elasticity of a function The Elasticity of a function $f(x)$ approximates the change of $f$ given the change of $x$ and is defined as: $$ E_f(x) = \frac{df}{dx} \frac{x}{f(x)}$$ My solution attempt We need to prove $E_R(p)=\frac{E_R(q)}{E_R(q)-1}$ The demand function can be written as: $ap+bq=k \iff \boxed{D(p) = q = \frac{k-ap}{b}} \:\:(1)$ and $\boxed{G(q) = p = \frac{k-bq}{a}}\:\: (2)$ Therefore we can write the revenue function as $R(q) = pq = pD(p) \iff \boxed{ R(q) = \frac{kp-ap^2}{b} }  \:\:(3)$ and $R(p) = pq = G(q)q \iff \boxed{R(p) = \frac{kq-bq^2}{a}}\:\: (4)$ Hence, $E_R(p) = \frac{R(q)}{dq} \frac{q}{R(q)} \stackrel{(3)}{=} \left(\frac{kp-ap^2}{a}\right)'\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{k-2ap}{a}\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{\left(k-2ap\right)q}{kp-ap^2}$ $$ \boxed{ E_R(p) = \frac{\left(k-2ap\right)q}{kp-ap^2}}\:\: (5)$$ And, $E_R(q) = \frac{R(p)}{dq} \frac{p}{R(p)} \stackrel{(4)}{=}  \left( \frac{kq-bq^2}{a}   \right)' \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{k-2bq}{a}  \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{p\left(k-2bq\right)}{kq-bq^2}$ $$ \boxed{E_R(q) = \frac{p\left(k-2bq\right)}{kq-bq^2}} \:\:(6)$$ So, at last: $$E_R(p)=\frac{E_R(q)}{E_R(q)-1} \iff \\ \frac{\left(k-2ap\right)q}{kp-ap^2} = \frac{\frac{p\left(k-2bq\right)}{kq-bq^2}}{\frac{p\left(k-2bq\right)}{kq-bq^2}-1}$$ Which is overly complicated but it must hold, if there were no trivial calculation mistakes. The Question Given the fact that this was an exam subquestion, I am confident that there is an easier way to prove the elasticity equation (maybe by using elasticity function properties?), but if there is, I can't spot it. Any ideas?","The Problem Let the demand function be . Prove that this equation (elasticity of revenue) is true: Definitions Demand Function The Demand Function is defined as the relation between the price of the good and the demanded quantity of the good which in our example is: . Note that Revenue Function The Revenue Function is defined as , where R is the total revenue, is the selling price per unit of sales, and is the number of units sold Elasticity of a function The Elasticity of a function approximates the change of given the change of and is defined as: My solution attempt We need to prove The demand function can be written as: and Therefore we can write the revenue function as and Hence, And, So, at last: Which is overly complicated but it must hold, if there were no trivial calculation mistakes. The Question Given the fact that this was an exam subquestion, I am confident that there is an easier way to prove the elasticity equation (maybe by using elasticity function properties?), but if there is, I can't spot it. Any ideas?","ap+bq=k E_R(p)=\frac{E_R(q)}{E_R(q)-1} p q ap+bq=k D^{-1}(p) = G(q) R = p q p q f(x) f x  E_f(x) = \frac{df}{dx} \frac{x}{f(x)} E_R(p)=\frac{E_R(q)}{E_R(q)-1} ap+bq=k \iff \boxed{D(p) = q = \frac{k-ap}{b}} \:\:(1) \boxed{G(q) = p = \frac{k-bq}{a}}\:\: (2) R(q) = pq = pD(p) \iff \boxed{ R(q) = \frac{kp-ap^2}{b} }  \:\:(3) R(p) = pq = G(q)q \iff \boxed{R(p) = \frac{kq-bq^2}{a}}\:\: (4) E_R(p) = \frac{R(q)}{dq} \frac{q}{R(q)} \stackrel{(3)}{=} \left(\frac{kp-ap^2}{a}\right)'\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{k-2ap}{a}\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{\left(k-2ap\right)q}{kp-ap^2}  \boxed{ E_R(p) = \frac{\left(k-2ap\right)q}{kp-ap^2}}\:\: (5) E_R(q) = \frac{R(p)}{dq} \frac{p}{R(p)} \stackrel{(4)}{=}  \left( \frac{kq-bq^2}{a} 
 \right)' \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{k-2bq}{a}  \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{p\left(k-2bq\right)}{kq-bq^2}  \boxed{E_R(q) = \frac{p\left(k-2bq\right)}{kq-bq^2}} \:\:(6) E_R(p)=\frac{E_R(q)}{E_R(q)-1} \iff \\ \frac{\left(k-2ap\right)q}{kp-ap^2} = \frac{\frac{p\left(k-2bq\right)}{kq-bq^2}}{\frac{p\left(k-2bq\right)}{kq-bq^2}-1}","['calculus', 'derivatives', 'finance', 'economics']"
30,"Let $f: [a, b]\rightarrow R$ be differentiable at each point of $[a, b ]$ and $f'(a)=f'(b)$, prove that there's a line passing to $a$ tangent to $f$","Let  be differentiable at each point of  and , prove that there's a line passing to  tangent to","f: [a, b]\rightarrow R [a, b ] f'(a)=f'(b) a f","Let $f: [a, b]\rightarrow R$ be differentiable at each point of $[a, b ]$ , and suppose that $f'(a) = f'(b)$ . Prove that there is at least one point $c$ in $(a,b)$ such that $$ f'(c) = \dfrac{f(c)-f(a)}{c-a} $$ My attempt: define $h(x) = \dfrac{f(x)-f(a)}{x-a}$ on $(a,b]$ and $h(a) = f'(a)$ . Notice that $h$ is continuous on $[a,b]$ . Now $$h'(x) = \dfrac{f'(x)}{x-a}-\dfrac{f(x)-f(a)}{(x-a)^2}$$ Note that we define $h'$ on $(a,b]$ Our goal is to show that an extremum point of $h(x)$ lies in $(a,b)$ so we can claim $h'(c)=0$ for some $c\in (a,b)$ . Moving things around we see that $f'(x) = h'(x)(x-a)+h(x)$ on $(a,b]$ . We observe that if $h(x)$ is strictly increasing (or strictly decreasing), then $f'(x)$ is also strictly increasing (or strictly decreasing). Hence a contradiction to $f'(a)=f'(b)$ and so there's an extremum $c$ for $h(x)$ . Here, we obtain a contradiction because if we were to avoid a contradiction then $f'(a)>d>f'(d+\epsilon)$ (assuming $f'$ is increasing) for any $\epsilon>0$ . Applying an intermidate-value-theorem type lemma to $f'$ we contradict monotonicity. Hence, $f(a)<f(a+\epsilon)$ for any $\epsilon>0$ . Therefore, $h'(c)=0$ implies $$\dfrac{f(c)-f(a)}{c-a}=f'(c)$$ I am only looking for proof verifications. If my proof is wrong, please $\textbf{only respond with hints}$ .","Let be differentiable at each point of , and suppose that . Prove that there is at least one point in such that My attempt: define on and . Notice that is continuous on . Now Note that we define on Our goal is to show that an extremum point of lies in so we can claim for some . Moving things around we see that on . We observe that if is strictly increasing (or strictly decreasing), then is also strictly increasing (or strictly decreasing). Hence a contradiction to and so there's an extremum for . Here, we obtain a contradiction because if we were to avoid a contradiction then (assuming is increasing) for any . Applying an intermidate-value-theorem type lemma to we contradict monotonicity. Hence, for any . Therefore, implies I am only looking for proof verifications. If my proof is wrong, please .","f: [a, b]\rightarrow R [a, b ] f'(a) = f'(b) c (a,b) 
f'(c) = \dfrac{f(c)-f(a)}{c-a}
 h(x) = \dfrac{f(x)-f(a)}{x-a} (a,b] h(a) = f'(a) h [a,b] h'(x) = \dfrac{f'(x)}{x-a}-\dfrac{f(x)-f(a)}{(x-a)^2} h' (a,b] h(x) (a,b) h'(c)=0 c\in (a,b) f'(x) = h'(x)(x-a)+h(x) (a,b] h(x) f'(x) f'(a)=f'(b) c h(x) f'(a)>d>f'(d+\epsilon) f' \epsilon>0 f' f(a)<f(a+\epsilon) \epsilon>0 h'(c)=0 \dfrac{f(c)-f(a)}{c-a}=f'(c) \textbf{only respond with hints}","['derivatives', 'proof-writing']"
31,Interchange derivative and expectation operator,Interchange derivative and expectation operator,,"I have a function $$f_\Sigma(x) = c\det(\Sigma^{-1})\exp(-0.5\Vert\Sigma^{-1}x\Vert^2_2)$$ and a function $$g_\Sigma(x - u) = f_\Sigma(x - u)\exp(-0.5\Vert AW_\Sigma a\Vert^2_2),$$ where $W_\Sigma$ is a $n\times n$ diagonal matrix whose $(j,j)$ element is given by $$w_{jj} = \frac{f_\Sigma(x - X_j)}{\sum_{i=1}^nf_\Sigma(x - X_i)}.$$ $A$ is matrix and $a$ some vector. $c$ is a constant and $X_1,X_2,\dots,X_n$ is a collection of iid random variables. I want to verify whether $$\frac{\partial}{\partial\Sigma}\mathbb E[g_\Sigma(x - U)] = \mathbb E\left[\frac{\partial}{\partial\Sigma}g_\Sigma(x - U)\right],$$ where $U$ is a random variable having the same distribution as the $X_i$ . I found it quite difficult to compute the derivative. Since I only want to verify whether derivative and expectation can be interchanged, I was wondering whether there is an approach to bind the derivative so that the conditions of the DCT are satisfied. Edit : To add some background, $g_\Sigma$ is the multivariate local polynomial density estimator in case of linear fitting ( https://projecteuclid.org/euclid.aos/1032298287 ). I want to study properties of the derivative of this estimator with respect to the bandwidth parameter $\Sigma$ . Eventually, I guess the background is not too important as its more related to an application of DCT to a very very complicated function. I was hoping that someone with more calculus experience has may encountered similar functions and knows a ""trick"" to handle the difficulties arising from the $\exp$ terms.","I have a function and a function where is a diagonal matrix whose element is given by is matrix and some vector. is a constant and is a collection of iid random variables. I want to verify whether where is a random variable having the same distribution as the . I found it quite difficult to compute the derivative. Since I only want to verify whether derivative and expectation can be interchanged, I was wondering whether there is an approach to bind the derivative so that the conditions of the DCT are satisfied. Edit : To add some background, is the multivariate local polynomial density estimator in case of linear fitting ( https://projecteuclid.org/euclid.aos/1032298287 ). I want to study properties of the derivative of this estimator with respect to the bandwidth parameter . Eventually, I guess the background is not too important as its more related to an application of DCT to a very very complicated function. I was hoping that someone with more calculus experience has may encountered similar functions and knows a ""trick"" to handle the difficulties arising from the terms.","f_\Sigma(x) = c\det(\Sigma^{-1})\exp(-0.5\Vert\Sigma^{-1}x\Vert^2_2) g_\Sigma(x - u) = f_\Sigma(x - u)\exp(-0.5\Vert AW_\Sigma a\Vert^2_2), W_\Sigma n\times n (j,j) w_{jj} = \frac{f_\Sigma(x - X_j)}{\sum_{i=1}^nf_\Sigma(x - X_i)}. A a c X_1,X_2,\dots,X_n \frac{\partial}{\partial\Sigma}\mathbb E[g_\Sigma(x - U)] = \mathbb E\left[\frac{\partial}{\partial\Sigma}g_\Sigma(x - U)\right], U X_i g_\Sigma \Sigma \exp","['derivatives', 'expected-value']"
32,Why is the derivative of $f(x)^{g(x)}$ the same as the sum of the derivative of $x^n$ and $a^x$?,Why is the derivative of  the same as the sum of the derivative of  and ?,f(x)^{g(x)} x^n a^x,We did the derivation of the following formula in class today- $$(f(x)^{g(x)})'=g(x)f(x)^{g(x)-1}f'(x)+f(x)^{g(x)}ln(f(x))g'(x)$$ My question is that if there is a reason that this is just the sum of the derivative of $x^n$ and $a^x$ ?,We did the derivation of the following formula in class today- My question is that if there is a reason that this is just the sum of the derivative of and ?,(f(x)^{g(x)})'=g(x)f(x)^{g(x)-1}f'(x)+f(x)^{g(x)}ln(f(x))g'(x) x^n a^x,"['calculus', 'derivatives']"
33,Implicit differentiation: differential vs derivative,Implicit differentiation: differential vs derivative,,"When I search implicit differentiation for equation $x^2 + y^2 = r^2$ I find results of two versions: one using derivative and the other using differential. Version1: $\frac{d }{dx}(x^2 + y^2 = r^2) \Leftrightarrow 2x + 2y\frac{dy}{dx} = 0 $ Version2: $d(x^2 + y^2 = r^2) \Leftrightarrow 2xdx + 2ydy = 0 $ Using both methods, I can derive the result: $\frac{dy}{dx} = -\frac{x}{y}$ However, I am confused, could you please provide some answers to: Which one (derivative /differential) is the ""real"" implicit differentiation? What are the differences between using these two methods? When should differential be used rather than derivative?","When I search implicit differentiation for equation I find results of two versions: one using derivative and the other using differential. Version1: Version2: Using both methods, I can derive the result: However, I am confused, could you please provide some answers to: Which one (derivative /differential) is the ""real"" implicit differentiation? What are the differences between using these two methods? When should differential be used rather than derivative?",x^2 + y^2 = r^2 \frac{d }{dx}(x^2 + y^2 = r^2) \Leftrightarrow 2x + 2y\frac{dy}{dx} = 0  d(x^2 + y^2 = r^2) \Leftrightarrow 2xdx + 2ydy = 0  \frac{dy}{dx} = -\frac{x}{y},"['calculus', 'derivatives', 'differential']"
34,Left hand derivative and Right hand derivative of the inverse of a non-differentiable function,Left hand derivative and Right hand derivative of the inverse of a non-differentiable function,,"Let $f(x)$ be an injective function with domain $[a,b]$ and range $[c,d]$ . If $\alpha$ is a point in $(a,b)$ such that $f$ has left hand derivative $l$ and right hand derivative $r$ at $x=\alpha$ with both $l$ and $r$ being non-zero different and negative , then prove that the left hand derivative and the right hand derivative of $f^{-1}(x)$ at $x=f(\alpha)$ are $\frac{1}{r}$ and $\frac{1}{l}$ respectively. My Attempt: If $g(x)$ be the inverse of $f((x)$ then $f(g(x))=x$ which gives $g'(x)=\frac{1}{f'(g(x))}$ . So if $x=f(\alpha)$ then $g'(f(\alpha))=\frac{1}{f'(g(f(\alpha)))}=\frac{1}{f'(\alpha)}$ . But now how do I get the left hand derivative and the right hand derivative. I could frame an example $f(x)=\left\{ \begin{array}{ll}       \frac{1}{x} & 0<x<2 \\             \frac{2}{x^2} & x\geq 2 \\       \end{array}  \right.$ Left hand derivative at $x=2$ equals $\frac{-1}{4}=l$ Right hand derivative at $x=2$ equals $\frac{-1}{2}=r$ $f^{-1}(x)=\left\{ \begin{array}{ll}       \\\sqrt {\frac{2}{x}} & 0<x\leq \frac{1}{2} \\             \frac{1}{x} & x> \frac{1}{2} \\       \end{array}  \right.$ Left hand derivative of $f^{-1}(x)$ at $x=\frac{1}{2}$ equals $-2=\frac{1}{r}$ Right hand derivative of $f^{-1}(x)$ at $x=\frac{1}{2}$ equals $-4=\frac{1}{l}$ But I am not able to get a proper method.","Let be an injective function with domain and range . If is a point in such that has left hand derivative and right hand derivative at with both and being non-zero different and negative , then prove that the left hand derivative and the right hand derivative of at are and respectively. My Attempt: If be the inverse of then which gives . So if then . But now how do I get the left hand derivative and the right hand derivative. I could frame an example Left hand derivative at equals Right hand derivative at equals Left hand derivative of at equals Right hand derivative of at equals But I am not able to get a proper method.","f(x) [a,b] [c,d] \alpha (a,b) f l r x=\alpha l r f^{-1}(x) x=f(\alpha) \frac{1}{r} \frac{1}{l} g(x) f((x) f(g(x))=x g'(x)=\frac{1}{f'(g(x))} x=f(\alpha) g'(f(\alpha))=\frac{1}{f'(g(f(\alpha)))}=\frac{1}{f'(\alpha)} f(x)=\left\{
\begin{array}{ll}
      \frac{1}{x} & 0<x<2 \\      
      \frac{2}{x^2} & x\geq 2 \\
      \end{array} 
\right. x=2 \frac{-1}{4}=l x=2 \frac{-1}{2}=r f^{-1}(x)=\left\{
\begin{array}{ll}
      \\\sqrt {\frac{2}{x}} & 0<x\leq \frac{1}{2} \\      
      \frac{1}{x} & x> \frac{1}{2} \\
      \end{array} 
\right. f^{-1}(x) x=\frac{1}{2} -2=\frac{1}{r} f^{-1}(x) x=\frac{1}{2} -4=\frac{1}{l}","['real-analysis', 'calculus', 'derivatives', 'inverse-function']"
35,Proof of existence of $a$ in $\lim_{h\rightarrow0}\frac{a^h-1}{h}=1$,Proof of existence of  in,a \lim_{h\rightarrow0}\frac{a^h-1}{h}=1,"In deriving the derivative of function $f(x)=\exp(x)$ , it is often pointed out that in the general case of $f_a(x)=a^x$ the following expression can be deduced from the definition of the derivative: $$ \frac{d}{dx}a^x = a^x\cdot\left(\lim_{h\rightarrow0}\frac{a^h-1}{h}\right) $$ with $e$ defined as the real number for which the above limit term is equal to 1. However, this seems unsatisfactory to me as the existence of a real number $a$ satisfying the equation $\lim_{h\rightarrow0}\frac{a^h-1}{h}=1$ is not established. Can somebody please point me to a proof of the existence of such a number or provide it if possible? Thanks.","In deriving the derivative of function , it is often pointed out that in the general case of the following expression can be deduced from the definition of the derivative: with defined as the real number for which the above limit term is equal to 1. However, this seems unsatisfactory to me as the existence of a real number satisfying the equation is not established. Can somebody please point me to a proof of the existence of such a number or provide it if possible? Thanks.","f(x)=\exp(x) f_a(x)=a^x 
\frac{d}{dx}a^x = a^x\cdot\left(\lim_{h\rightarrow0}\frac{a^h-1}{h}\right)
 e a \lim_{h\rightarrow0}\frac{a^h-1}{h}=1","['calculus', 'derivatives', 'exponential-function']"
36,Sard's Theorem with whatever function,Sard's Theorem with whatever function,,"A lot of staetments of Sard's Theorem require a certain order of smoothness to the function $f$ . At least it is $C^1$ . I want to prove a more general version of this theorem which holds (clearly) only with $f : \mathbb{R}^n \to \mathbb{R}^n$ but $f$ could be literally what you want (no regularity assumption). The statement sounds like that: Let $f : \mathbb{R}^n \to \mathbb{R}^n$ and \begin{align} Z &= \{ x \in \mathbb{R^n} | f \text{ is differentiable at $x$ and }  \det(J f(x)) =0 \} \end{align} Then $\mathcal{L}^n (f(Z))=0$ Here $\mathcal{L}^n$ is the Lebesgue measure. I proved the claim for $n=1$ . I'm reporting it here: Firstly assume wlog $f :[a,b] \to \mathbb{R}$ . Let $x \in Z$ , then by definition of differentiability, $\forall \epsilon >0$ there exists $r_{\epsilon,x}>0$ such that $\forall y \in \mathbb{R}^n$ with $0 < \vert y-x\vert<r_{\epsilon,x}$ , then $\Big\rvert \dfrac{f(y)-f(x)}{y-x} -0\Big\lvert < \epsilon$ . Thus $\vert f(x) - f(y) \vert < \epsilon |y-x| < r \epsilon $ for all $r \in (0,r_{\epsilon,x}) \qquad (1)$ . Let me call $I_{x,r} := [x-r,x+r] $ and $A_M:=f(Z) \cap [-M,M]$ . I have $f(Z) = \bigcup_{M=1}^{\infty}A_M$ and $\mathcal{L}(f(I_{x,r}))< 2r\epsilon$ by $(1)$ . Claim: $\mathcal{L}(A_M)=0$ I need Vitali covering Lemma Let $A \subset \mathbb{R}^n$ bounded and $\mathcal{B}$ a fine covering of $A$ (i.e. a family of balls such that for all $x \in A$ , then $\inf \{r>0 | B(x,r) \in \mathcal{B} \} = 0$ ). Then there exists a subfamily $\mathcal{B}' \subset \mathcal{B}$ of disjoint balls (at most countable) such that $\mathcal{L}^n(A) < \mathcal{L}^n(\bigcup_{B \in \mathcal{B}'}B) + \epsilon$ Fix $\epsilon >0$ . I choose $\mathcal{B} = \{[f(x) - r \epsilon,f(x)+r\epsilon] | x \in Z, r \in (0,r_{\epsilon,x}) \text{ such that  if } y \in [x-r,x+r] \text{ then } \vert f(y)-f(x) \vert < r \epsilon \}$ Clearly this is a fine covering, hence, by Vitali, there exists a subfamily of disjoint intervals $I_i:= I_{f(x),\epsilon r_i}$ such that $\mathcal{L}(A_M) < \mathcal{L}(\bigcup I_i)  + \bar{\epsilon}$ . Now, I take $\epsilon  = \bar{\epsilon} = \dfrac{1}{100} \dfrac{\mathcal{L}(A_M)}{(1+b-a)}$ . I observe that $\mathcal{L}(\bigcup I_i) \leq 2 \sum \epsilon r_i \leq \epsilon (b-a)$ . We put all these stuffs together and we get $\mathcal{L}(A_M) < \dfrac{1}{100} \mathcal{L}(A_M)$ which implies $\mathcal{L}(A_M)=0$ . Finally $\mathcal{L}(f(Z)) \leq \sum \mathcal{L}(A_M)=0$ NOW, I want to simulate this proof for the cases $n \geq 2$ but the main isssue is that I have more than one direction to control. My idea is to prove that for every $\epsilon >0$ and every $x \in Z$ there exists $r_{\epsilon,x} >0$ such that $f(B(x,r))$ is contained in a parallelepiped with all sides controlled by $r$ and one side controlled by $\epsilon r$ for every $r \in (0, r_{\epsilon,x})$ . Then I will use Vitali and finally discover $\mathcal{L}^n(f(B(x,r)) < c \epsilon r^n$ . Wlog I can suppose that the first column of Jacobian matrix is full of zeros and this leads  me to prove that along $x_1$ -axis my image is controlled by $\epsilon r$ , but what about the other axis? Can you give me some hints or references?","A lot of staetments of Sard's Theorem require a certain order of smoothness to the function . At least it is . I want to prove a more general version of this theorem which holds (clearly) only with but could be literally what you want (no regularity assumption). The statement sounds like that: Let and Then Here is the Lebesgue measure. I proved the claim for . I'm reporting it here: Firstly assume wlog . Let , then by definition of differentiability, there exists such that with , then . Thus for all . Let me call and . I have and by . Claim: I need Vitali covering Lemma Let bounded and a fine covering of (i.e. a family of balls such that for all , then ). Then there exists a subfamily of disjoint balls (at most countable) such that Fix . I choose Clearly this is a fine covering, hence, by Vitali, there exists a subfamily of disjoint intervals such that . Now, I take . I observe that . We put all these stuffs together and we get which implies . Finally NOW, I want to simulate this proof for the cases but the main isssue is that I have more than one direction to control. My idea is to prove that for every and every there exists such that is contained in a parallelepiped with all sides controlled by and one side controlled by for every . Then I will use Vitali and finally discover . Wlog I can suppose that the first column of Jacobian matrix is full of zeros and this leads  me to prove that along -axis my image is controlled by , but what about the other axis? Can you give me some hints or references?","f C^1 f : \mathbb{R}^n \to \mathbb{R}^n f f : \mathbb{R}^n \to \mathbb{R}^n \begin{align}
Z &= \{ x \in \mathbb{R^n} | f \text{ is differentiable at x and }  \det(J f(x)) =0 \}
\end{align} \mathcal{L}^n (f(Z))=0 \mathcal{L}^n n=1 f :[a,b] \to \mathbb{R} x \in Z \forall \epsilon >0 r_{\epsilon,x}>0 \forall y \in \mathbb{R}^n 0 < \vert y-x\vert<r_{\epsilon,x} \Big\rvert \dfrac{f(y)-f(x)}{y-x} -0\Big\lvert < \epsilon \vert f(x) - f(y) \vert < \epsilon |y-x| < r \epsilon  r \in (0,r_{\epsilon,x}) \qquad (1) I_{x,r} := [x-r,x+r]  A_M:=f(Z) \cap [-M,M] f(Z) = \bigcup_{M=1}^{\infty}A_M \mathcal{L}(f(I_{x,r}))< 2r\epsilon (1) \mathcal{L}(A_M)=0 A \subset \mathbb{R}^n \mathcal{B} A x \in A \inf \{r>0 | B(x,r) \in \mathcal{B} \} = 0 \mathcal{B}' \subset \mathcal{B} \mathcal{L}^n(A) < \mathcal{L}^n(\bigcup_{B \in \mathcal{B}'}B) + \epsilon \epsilon >0 \mathcal{B} = \{[f(x) - r \epsilon,f(x)+r\epsilon] | x \in Z, r \in (0,r_{\epsilon,x}) \text{ such that  if } y \in [x-r,x+r] \text{ then } \vert f(y)-f(x) \vert < r \epsilon \} I_i:= I_{f(x),\epsilon r_i} \mathcal{L}(A_M) < \mathcal{L}(\bigcup I_i)  + \bar{\epsilon} \epsilon  = \bar{\epsilon} = \dfrac{1}{100} \dfrac{\mathcal{L}(A_M)}{(1+b-a)} \mathcal{L}(\bigcup I_i) \leq 2 \sum \epsilon r_i \leq \epsilon (b-a) \mathcal{L}(A_M) < \dfrac{1}{100} \mathcal{L}(A_M) \mathcal{L}(A_M)=0 \mathcal{L}(f(Z)) \leq \sum \mathcal{L}(A_M)=0 n \geq 2 \epsilon >0 x \in Z r_{\epsilon,x} >0 f(B(x,r)) r \epsilon r r \in (0, r_{\epsilon,x}) \mathcal{L}^n(f(B(x,r)) < c \epsilon r^n x_1 \epsilon r","['real-analysis', 'derivatives', 'lebesgue-measure']"
37,How far can I rewrite an equation before the derivative is not equivalent?,How far can I rewrite an equation before the derivative is not equivalent?,,"I've run into a problem that I can't explain to my class.  We are looking at the derivative for the equation $\frac{x}{y}+\frac{y}{x}=3y$ . We calculated it to be $\frac{y(x^2-y^2)}{x(3xy^2+x^2-y^2)}$ and we also verified it with Wolfram Alpha. A student thought about rewriting the original equation as $x^2+y^2=3xy^2$ by multiplying everything by $xy$ . I realize this adds to the domain and adds the point $(0,0)$ as a solution, but it's not differentiable at that point regardless as it's not continuous at that point. When we took the derivative of the rewritten equation and got $\frac{3y^2-2x}{2y-6xy}$ , which is not equivalent to our previous calculation. I can't figure out why the derivatives are so vastly different if all that was added to the original was a new point that's not differentiable to begin with. Any help is much appreciated!","I've run into a problem that I can't explain to my class.  We are looking at the derivative for the equation . We calculated it to be and we also verified it with Wolfram Alpha. A student thought about rewriting the original equation as by multiplying everything by . I realize this adds to the domain and adds the point as a solution, but it's not differentiable at that point regardless as it's not continuous at that point. When we took the derivative of the rewritten equation and got , which is not equivalent to our previous calculation. I can't figure out why the derivatives are so vastly different if all that was added to the original was a new point that's not differentiable to begin with. Any help is much appreciated!","\frac{x}{y}+\frac{y}{x}=3y \frac{y(x^2-y^2)}{x(3xy^2+x^2-y^2)} x^2+y^2=3xy^2 xy (0,0) \frac{3y^2-2x}{2y-6xy}","['calculus', 'derivatives', 'implicit-differentiation']"
38,Does $\frac{dx}{dy}$ at $0$ exist and $\frac{dy}{dx}\frac{dx}{dy}$ at $0$ is $1$ for the following function?,Does  at  exist and  at  is  for the following function?,\frac{dx}{dy} 0 \frac{dy}{dx}\frac{dx}{dy} 0 1,$$y =f(x)=\begin{cases}x+x^2\sin\frac1{x^2}&\text{if }x\ne 0\\ 0&\text{if }x=0\end{cases}$$ We can see $\dfrac{dy}{dx}$ at $0$ is $1$ . My question: Does $\dfrac{dx}{dy}$ at $0$ exist and $\dfrac{dy}{dx}$ . $\dfrac{dx}{dy}$ at $0$ is $1$ ? If not then  is it because $f$ can not be invertible in any nbd around the point $0$ ? Actually  I was trying to understand when $\dfrac{dx}{dy}$ at a point exists and $\dfrac{dy}{dx}$ . $\dfrac{dx}{dy}$ at that point is $1$ if $\dfrac{dy}{dx}$ at that point exists. Can anyone please help me to clear  my doubt?,We can see at is . My question: Does at exist and . at is ? If not then  is it because can not be invertible in any nbd around the point ? Actually  I was trying to understand when at a point exists and . at that point is if at that point exists. Can anyone please help me to clear  my doubt?,y =f(x)=\begin{cases}x+x^2\sin\frac1{x^2}&\text{if }x\ne 0\\ 0&\text{if }x=0\end{cases} \dfrac{dy}{dx} 0 1 \dfrac{dx}{dy} 0 \dfrac{dy}{dx} \dfrac{dx}{dy} 0 1 f 0 \dfrac{dx}{dy} \dfrac{dy}{dx} \dfrac{dx}{dy} 1 \dfrac{dy}{dx},"['real-analysis', 'calculus', 'derivatives']"
39,Basic notions about derivatives,Basic notions about derivatives,,"First of all, I'm a beginner in calculus and this problem came out to be tricky for me. Denote $\dot{x}=\frac{dx}{dt}$ for any function $x(t)$ of time $t$ . Consider the equality $$\dot{y}=c \frac{\dot{x}}{x},$$ where $y=:y(t)$ and $x=:x(t)$ . My first question is: it holds that $$\frac{d\dot{y}}{d\dot{x}}=c/x$$ or should I take into account that $\dot{y}=f(\dot{x},x)$ and $\dot{x}=g(x)$ ? Moreover, I would like to know how to treat the second derivative $$\frac{d^2\dot{y}}{d\dot{x}^2}.$$ Finally, for some function $h$ , is it correct that $\frac{dh(x)}{d\dot{x}}=\frac{dh}{dx}\frac{dx}{d\dot{x}}$ , provided that $\frac{d\dot{x}}{dx}=\frac{d\dot{x}}{dt}\frac{dt}{dx}=\frac{\ddot{x}}{\dot{x}}$ exists? Thanks in advance!","First of all, I'm a beginner in calculus and this problem came out to be tricky for me. Denote for any function of time . Consider the equality where and . My first question is: it holds that or should I take into account that and ? Moreover, I would like to know how to treat the second derivative Finally, for some function , is it correct that , provided that exists? Thanks in advance!","\dot{x}=\frac{dx}{dt} x(t) t \dot{y}=c \frac{\dot{x}}{x}, y=:y(t) x=:x(t) \frac{d\dot{y}}{d\dot{x}}=c/x \dot{y}=f(\dot{x},x) \dot{x}=g(x) \frac{d^2\dot{y}}{d\dot{x}^2}. h \frac{dh(x)}{d\dot{x}}=\frac{dh}{dx}\frac{dx}{d\dot{x}} \frac{d\dot{x}}{dx}=\frac{d\dot{x}}{dt}\frac{dt}{dx}=\frac{\ddot{x}}{\dot{x}}","['derivatives', 'self-learning']"
40,Geometric meaning of second Covariant Derivative,Geometric meaning of second Covariant Derivative,,"This other question exists, but it doesn't answer my question: Geometric interpretation of the second covariant derivative I know the Riemann Tensor can be written as the commutator of the second covariant derivative (assuming the connection is torsion-free): $$R(u,v)w = \nabla_{u,v}^2 w - \nabla_{v,u}^2 w$$ where $\nabla_{u,v}^2 w  = \nabla_u \nabla_v w - \nabla_{\nabla_u v}w$ is the ""second covariant derivative"". What I'm missing here is the ""why"". I don't understand the geometrical meaning of $\nabla_{u,v}^2 w$ , or why anyone bothered to invent this expression. How was the formula for the ""second covariant derivative"" invented, and what is its meaning? I tried drawing some diagrams showing how the vectors are positioned, but they did not bring me much insight. The other question has better diagrams in the answers. Here is a visualization of $\nabla_u \nabla_v w$ : Here is a visualization of $\nabla_{\nabla_u v}w$ :","This other question exists, but it doesn't answer my question: Geometric interpretation of the second covariant derivative I know the Riemann Tensor can be written as the commutator of the second covariant derivative (assuming the connection is torsion-free): where is the ""second covariant derivative"". What I'm missing here is the ""why"". I don't understand the geometrical meaning of , or why anyone bothered to invent this expression. How was the formula for the ""second covariant derivative"" invented, and what is its meaning? I tried drawing some diagrams showing how the vectors are positioned, but they did not bring me much insight. The other question has better diagrams in the answers. Here is a visualization of : Here is a visualization of :","R(u,v)w = \nabla_{u,v}^2 w - \nabla_{v,u}^2 w \nabla_{u,v}^2 w  = \nabla_u \nabla_v w - \nabla_{\nabla_u v}w \nabla_{u,v}^2 w \nabla_u \nabla_v w \nabla_{\nabla_u v}w","['derivatives', 'differential-geometry', 'tensors', 'covariance']"
41,Solution of $2{{I}_{xy}}={{I}_{x}}+{{I}_{y}}$,Solution of,2{{I}_{xy}}={{I}_{x}}+{{I}_{y}},"How i can find  the general solution of the following partial differential equation: $$\frac{{{\partial }^{2}}I}{\partial x\partial y}=\frac{1}{2}\left( \frac{\partial I}{\partial x}+\frac{\partial I}{\partial y} \right)$$ I made a guess $I\left( x,y \right)={{e}^{x+y}}$",How i can find  the general solution of the following partial differential equation: I made a guess,"\frac{{{\partial }^{2}}I}{\partial x\partial y}=\frac{1}{2}\left( \frac{\partial I}{\partial x}+\frac{\partial I}{\partial y} \right) I\left( x,y \right)={{e}^{x+y}}","['derivatives', 'partial-differential-equations', 'partial-derivative']"
42,"Why is that $\int_a^b \frac{\partial f}{\partial x}(x,t)dt = \frac{\partial}{\partial x}\int_a^b f(x,t)dt$",Why is that,"\int_a^b \frac{\partial f}{\partial x}(x,t)dt = \frac{\partial}{\partial x}\int_a^b f(x,t)dt","This question is concerned with the integral with parameter, so let's assume that every function below is smooth. To find the formula for the derivative of an integral with parameter, say $$g(x) = \int_{a(x)}^{b(x)}f(x,t)dt$$ for some function $f: \mathbb{R}^2 \to \mathbb{R}$ , one would define $$ F: \mathbb{R}^3 \to \mathbb{R}, F(a,b,x) = \int_a^b f(x,t)dt $$ and then use the multivariable chain-rule to determine $g'$ . Thus, we would have that $$g'(x) = \frac{\partial F}{\partial a} \cdot \frac{\partial a}{\partial x} + \frac{\partial F}{\partial b} \cdot \frac{\partial b}{\partial x} + \frac{\partial F}{\partial x}.$$ It is easy so see that $$\frac{\partial F}{\partial a} \cdot \frac{\partial a}{\partial x} = -f(x,a(x)) \cdot a'(x)$$ and that $$\frac{\partial F}{\partial b} \cdot \frac{\partial b}{\partial x} = f(x,b(x)) \cdot b'(x). $$ What I don't understand is why we have that $$\frac{\partial F}{\partial x} = \int_a^b \frac{\partial f}{\partial x}(x,t)dt. $$ Shouldn't it be $$\frac{\partial F}{\partial x} = \frac{\partial}{\partial x}\int_a^b f(x,t)dt ? $$ Could you please tell me why we are allowed to interchange the partial derivative and the integral?","This question is concerned with the integral with parameter, so let's assume that every function below is smooth. To find the formula for the derivative of an integral with parameter, say for some function , one would define and then use the multivariable chain-rule to determine . Thus, we would have that It is easy so see that and that What I don't understand is why we have that Shouldn't it be Could you please tell me why we are allowed to interchange the partial derivative and the integral?","g(x) = \int_{a(x)}^{b(x)}f(x,t)dt f: \mathbb{R}^2 \to \mathbb{R}  F: \mathbb{R}^3 \to \mathbb{R}, F(a,b,x) = \int_a^b f(x,t)dt  g' g'(x) = \frac{\partial F}{\partial a} \cdot \frac{\partial a}{\partial x} + \frac{\partial F}{\partial b} \cdot \frac{\partial b}{\partial x} + \frac{\partial F}{\partial x}. \frac{\partial F}{\partial a} \cdot \frac{\partial a}{\partial x} = -f(x,a(x)) \cdot a'(x) \frac{\partial F}{\partial b} \cdot \frac{\partial b}{\partial x} = f(x,b(x)) \cdot b'(x).  \frac{\partial F}{\partial x} = \int_a^b \frac{\partial f}{\partial x}(x,t)dt.  \frac{\partial F}{\partial x} = \frac{\partial}{\partial x}\int_a^b f(x,t)dt ? ","['real-analysis', 'integration', 'derivatives', 'chain-rule']"
43,If $f(x)=x^{-n}$ for $n\in \mathbb{N}$,If  for,f(x)=x^{-n} n\in \mathbb{N},"If $f(x)=x^{-n}$ for $n\in \mathbb{N}$ then  prove $f^{(k)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!}$ $x^{-n-k} $ Of course I'm taking it by induction, but the main issue comes when $k+1$ and my final result comes to this I derive $f^{(k)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!}$ $x^{-n-k} $ therefore $f^{(k+1)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!}$ $x^{-n-k-1}(-n-k) =$ $f^{(k+1)}(x)=(-1)^{k+1} \frac{(n+k)!}{(k-1)!}$ $x^{-n-k-1}$ but my intuition tells me that I should get to $f^{(k+1)}(x)=(-1)^{k+1} \frac{(n+k)!}{k!}$ $x^{-n-k-1}$ Any advice?","If for then  prove Of course I'm taking it by induction, but the main issue comes when and my final result comes to this I derive therefore but my intuition tells me that I should get to Any advice?",f(x)=x^{-n} n\in \mathbb{N} f^{(k)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!} x^{-n-k}  k+1 f^{(k)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!} x^{-n-k}  f^{(k+1)}(x)=(-1)^{k} \frac{(n+k-1)!}{(k-1)!} x^{-n-k-1}(-n-k) = f^{(k+1)}(x)=(-1)^{k+1} \frac{(n+k)!}{(k-1)!} x^{-n-k-1} f^{(k+1)}(x)=(-1)^{k+1} \frac{(n+k)!}{k!} x^{-n-k-1},"['calculus', 'derivatives', 'induction']"
44,Link between $f$ and $f'$,Link between  and,f f',"I am having a hard time with exercises of the form : $f'$ verify some properties then prove that $f$ is such that : ... The main problem I have is that in order to link $f$ and it's derivative I only know that : $f(x) = f'(a)(x-a) + o(x)$ in a neighboorhhod of $a$ . Yet this formula is not precise at all and only usefull to compute limit. The $o(x)$ term makes it impossible to manipulate/do $\epsilon$ proofs. So is there a way to have a precise formula that link $f$ and it's derivative so that it's more suitable for $\epsilon$ proof ? For example let's consider the following : Let $f \in C^1(\mathbb{R}, \mathbb{R})$ . Moreover we know that on $[a,b]$ $\inf \mid f' \mid = K$ then prove that $\mid f\mid \geq  Kx $ on $[a,b]$ This is intuitively obvious. Yet using the formula $f(x) = f'(a)(x-a) +o(x)$ doesn't help at all since it's only an asymptotic. Thus is there a way/formula that relates precisely the behaviour of $f$ and the behaviour of $f'$ so that $\epsilon$ rpoofs are possible ? Thank you !",I am having a hard time with exercises of the form : verify some properties then prove that is such that : ... The main problem I have is that in order to link and it's derivative I only know that : in a neighboorhhod of . Yet this formula is not precise at all and only usefull to compute limit. The term makes it impossible to manipulate/do proofs. So is there a way to have a precise formula that link and it's derivative so that it's more suitable for proof ? For example let's consider the following : Let . Moreover we know that on then prove that on This is intuitively obvious. Yet using the formula doesn't help at all since it's only an asymptotic. Thus is there a way/formula that relates precisely the behaviour of and the behaviour of so that rpoofs are possible ? Thank you !,"f' f f f(x) = f'(a)(x-a) + o(x) a o(x) \epsilon f \epsilon f \in C^1(\mathbb{R}, \mathbb{R}) [a,b] \inf \mid f' \mid = K \mid f\mid \geq  Kx  [a,b] f(x) = f'(a)(x-a) +o(x) f f' \epsilon","['real-analysis', 'integration', 'sequences-and-series', 'derivatives', 'epsilon-delta']"
45,Proving $\frac{d}{dx}a^x=a^x\ln a$ from the integral definition of the natural logarithm,Proving  from the integral definition of the natural logarithm,\frac{d}{dx}a^x=a^x\ln a,"Using the definition of the natural logarithm as $\displaystyle\ln x=\int_1^x\frac{dt}{t}$ , is there a way to prove that $\frac{d}{dx}a^x=a^x\ln a$ ? Proofs that I have generally seen have used the definition of the natural logarithm as the inverse of the exponential function $e^x$ : $$\frac{d}{dx}a^x=\frac{d}{dx}e^{x\ln a}=e^{x\ln a}\ln a=a^x\ln a$$ using the chain rule and the fact that $f(x)=e^x$ satisfies $f'(x)=f(x)$ . But is there a way to go about finding this derivative without using the definition of the natural logarithm as the inverse of the exponential function?","Using the definition of the natural logarithm as , is there a way to prove that ? Proofs that I have generally seen have used the definition of the natural logarithm as the inverse of the exponential function : using the chain rule and the fact that satisfies . But is there a way to go about finding this derivative without using the definition of the natural logarithm as the inverse of the exponential function?",\displaystyle\ln x=\int_1^x\frac{dt}{t} \frac{d}{dx}a^x=a^x\ln a e^x \frac{d}{dx}a^x=\frac{d}{dx}e^{x\ln a}=e^{x\ln a}\ln a=a^x\ln a f(x)=e^x f'(x)=f(x),"['calculus', 'derivatives', 'logarithms']"
46,Boundedness of high order derivatives,Boundedness of high order derivatives,,"Let $f:\mathbb{R}\to\mathbb{R}$ a function differentiable $p$ times in $\mathbb{R}$ , such that $f$ and $f^{(p)}$ bounded. Consequently, I would show that all intermediate derivatives $f^{(1)},...,f^{(p-1)}$ are also bounded on $\mathbb{R}$ . By myself, I succedeed in the case $p=2$ in this way: $$f(x_0+h)=f(x_0)+f^{(1)}(x_0)h+f^{(2)}(\xi _1)\frac{h^2}{2}$$ $$f(x_0-h)=f(x_0)-f^{(1)}(x_0)h+f^{(2)}(\xi _2)\frac{h^2}{2}$$ that are true $\forall x_0\in\mathbb{R}$ and $\forall h>0$ with $\xi_1\in (x_0, x_0+h)$ and $\xi_2\in (x_0-h, x_0)$ . So subtracting the first equation and the second: $$f(x_0+h)-f(x_0-h)=2hf^{(1)}(x_0)+\frac{h^2}{2}\left( f^{(2)}(\xi _1)-f^{(2)}(\xi _2) \right)$$ and so: $$f^{(1)}(x_0)=\frac{f(x_0+h)-f(x_0-h)}{2h}-\frac{h}{4}\left( f^{(2)}(\xi _1)-f^{(2)}(\xi _2) \right)$$ taking the absolute value of both members and using the triangular inequality at right member: $$|f^{(1)}(x_0)|\leq \frac{|f(x_0+h)|+|f(x_0-h)|}{2h}+\frac{h}{4}\left( |f^{(2)}(\xi _1)|+|f^{(2)}(\xi _2)| \right)$$ from which: $$|f^{(1)}(x_0)|\leq \frac{M_{0}}{h}+\frac{h}{2}M_{2}$$ where $M_{0}:=\sup_{x\in\mathbb{R}}|f(x)|$ and $M_{2}:=\sup_{x\in\mathbb{R}}|f^{(2)}(x)|$ . Consequently: $$\sup_{x\in\mathbb{R}}|f^{(1)}(x)|=:M_1\leq \frac{M_{0}}{h}+\frac{h}{2}M_{2}$$ . In a similar way I also proved the claim in the case $p=3$ , but I can't start the induction chain. Can you help me? Thanks.","Let a function differentiable times in , such that and bounded. Consequently, I would show that all intermediate derivatives are also bounded on . By myself, I succedeed in the case in this way: that are true and with and . So subtracting the first equation and the second: and so: taking the absolute value of both members and using the triangular inequality at right member: from which: where and . Consequently: . In a similar way I also proved the claim in the case , but I can't start the induction chain. Can you help me? Thanks.","f:\mathbb{R}\to\mathbb{R} p \mathbb{R} f f^{(p)} f^{(1)},...,f^{(p-1)} \mathbb{R} p=2 f(x_0+h)=f(x_0)+f^{(1)}(x_0)h+f^{(2)}(\xi _1)\frac{h^2}{2} f(x_0-h)=f(x_0)-f^{(1)}(x_0)h+f^{(2)}(\xi _2)\frac{h^2}{2} \forall x_0\in\mathbb{R} \forall h>0 \xi_1\in (x_0, x_0+h) \xi_2\in (x_0-h, x_0) f(x_0+h)-f(x_0-h)=2hf^{(1)}(x_0)+\frac{h^2}{2}\left( f^{(2)}(\xi _1)-f^{(2)}(\xi _2) \right) f^{(1)}(x_0)=\frac{f(x_0+h)-f(x_0-h)}{2h}-\frac{h}{4}\left( f^{(2)}(\xi _1)-f^{(2)}(\xi _2) \right) |f^{(1)}(x_0)|\leq \frac{|f(x_0+h)|+|f(x_0-h)|}{2h}+\frac{h}{4}\left( |f^{(2)}(\xi _1)|+|f^{(2)}(\xi _2)| \right) |f^{(1)}(x_0)|\leq \frac{M_{0}}{h}+\frac{h}{2}M_{2} M_{0}:=\sup_{x\in\mathbb{R}}|f(x)| M_{2}:=\sup_{x\in\mathbb{R}}|f^{(2)}(x)| \sup_{x\in\mathbb{R}}|f^{(1)}(x)|=:M_1\leq \frac{M_{0}}{h}+\frac{h}{2}M_{2} p=3","['real-analysis', 'derivatives', 'upper-lower-bounds']"
47,Implicit Function Theorem on Surfaces.,Implicit Function Theorem on Surfaces.,,"(a) State the Implicit Function Theorem in the most general way that you know (b) Let $\Sigma$ the set of $2 \times 2$ matrices with determinant zero. Show that if $0 \neq M \in \Sigma$ , then there is a neighborhood $V \ni M$ such that $\Sigma \cap V$ is a parameterized surfaces. (c) Write explicitly a parameterization for $\Sigma \cap V$ with $M = \left(\begin{array}{cc}1 & 1 \\ 1 & 1\end{array} \right)$ and $V \ni M$ of your choice. Attempt. (a) Let $F: \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}^{m}$ be a $C^{1}$ function. Suppose that $F(a,b) = 0$ and $\displaystyle \det\left(\frac{\partial}{\partial y}F(a,b)\right) \neq 0$ . So there is $a \in A$ and $b \in B$ open sets and a $C^{1}$ function $f: A \to B$ such that $b = f(a)$ , $F(x,f(x)) = 0$ for each $x \in A$ and $f$ is a $\frac{\partial}{\partial x_{i}}f(x,y) = -\left(\frac{\partial}{\partial y}F(a,b)\right)^{-1}\left(\frac{\partial}{\partial x_{i}}F(a,b)\right)$ . (b) Consider the function $F: (\mathbb{R}^{3}\times \mathbb{R}) \to \mathbb{R}$ given by $F(X) = \det(X) = a_{11}a_{22} - a_{12}a_{21}$ for $$X = \left(\begin{array}{cc}a_{11} & a_{12} \\ a_{21} & a_{22}\end{array}\right).$$ We have $F \in C^{1}$ , $F(M) = F(m_{11},m_{12},m_{21},m_{22}) = 0$ and $\det\left(\frac{\partial}{\partial a_{22}}F(M)\right)^{-1} \neq 0$ . By the Implicit Function Theorem, $F^{-1}(0)\cap(V \times U) = (\Sigma\cap V)\times(\Sigma\cap U)$ is the graph of a continuously differentiable function $f: M \in V \to F(M) \in U$ . But the graph of a $C^{1}$ function is a surface. Therefore, $\Sigma \cap V$ is a parameterized surface. (c) I can write $f$ as $\displaystyle a_{22} \mapsto \frac{a_{12}a_{21}}{a_{11}}$ . But I dont know how to choice $V$ and find a parameterization.","(a) State the Implicit Function Theorem in the most general way that you know (b) Let the set of matrices with determinant zero. Show that if , then there is a neighborhood such that is a parameterized surfaces. (c) Write explicitly a parameterization for with and of your choice. Attempt. (a) Let be a function. Suppose that and . So there is and open sets and a function such that , for each and is a . (b) Consider the function given by for We have , and . By the Implicit Function Theorem, is the graph of a continuously differentiable function . But the graph of a function is a surface. Therefore, is a parameterized surface. (c) I can write as . But I dont know how to choice and find a parameterization.","\Sigma 2 \times 2 0 \neq M \in \Sigma V \ni M \Sigma \cap V \Sigma \cap V M = \left(\begin{array}{cc}1 & 1 \\ 1 & 1\end{array} \right) V \ni M F: \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}^{m} C^{1} F(a,b) = 0 \displaystyle \det\left(\frac{\partial}{\partial y}F(a,b)\right) \neq 0 a \in A b \in B C^{1} f: A \to B b = f(a) F(x,f(x)) = 0 x \in A f \frac{\partial}{\partial x_{i}}f(x,y) = -\left(\frac{\partial}{\partial y}F(a,b)\right)^{-1}\left(\frac{\partial}{\partial x_{i}}F(a,b)\right) F: (\mathbb{R}^{3}\times \mathbb{R}) \to \mathbb{R} F(X) = \det(X) = a_{11}a_{22} - a_{12}a_{21} X = \left(\begin{array}{cc}a_{11} & a_{12} \\ a_{21} & a_{22}\end{array}\right). F \in C^{1} F(M) = F(m_{11},m_{12},m_{21},m_{22}) = 0 \det\left(\frac{\partial}{\partial a_{22}}F(M)\right)^{-1} \neq 0 F^{-1}(0)\cap(V \times U) = (\Sigma\cap V)\times(\Sigma\cap U) f: M \in V \to F(M) \in U C^{1} \Sigma \cap V f \displaystyle a_{22} \mapsto \frac{a_{12}a_{21}}{a_{11}} V","['real-analysis', 'derivatives', 'surfaces', 'implicit-function-theorem']"
48,On anti-derivative of functions [closed],On anti-derivative of functions [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $f,g,h: \mathbb R \to \mathbb R$ be differentiable functions. (1) Does there necessarily exist a differentiable function $F: \mathbb R \to \mathbb R $ such that $F'=\max \{f' ,g' \}$ ? (2) Does there necessarily exist a differentiable function $G: \mathbb R \to \mathbb R $ such that $G '=\max \{f',g',h'\}$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let be differentiable functions. (1) Does there necessarily exist a differentiable function such that ? (2) Does there necessarily exist a differentiable function such that ?","f,g,h: \mathbb R \to \mathbb R F: \mathbb R \to \mathbb R  F'=\max \{f' ,g' \} G: \mathbb R \to \mathbb R  G '=\max \{f',g',h'\}","['real-analysis', 'derivatives', 'riemann-integration']"
49,Bounded function with bounded second derivative imply bounded first derivative,Bounded function with bounded second derivative imply bounded first derivative,,"Let $f$ be a $C^2$ function from $(t_1,t_2)$ to $R^n$ such that $\Vert f(t)\Vert\leq A$ and $\Vert f''(t)\Vert \leq B$ for all $t\in (t_1,t_2)$ , where $A$ and $B$ are nonnegative reals. Let $t_0\in (t_1,t_2)$ and $\alpha>0$ be such that $(t_0-\alpha,t_0+\alpha)\subset (t_1,t_2)$ . I would like to prove that $\Vert f'(t_0)\Vert\leq 2A/\alpha + B\alpha/2$ . I thought of using a Taylor expansion of $f$ but cannot manage to get the inequality.","Let be a function from to such that and for all , where and are nonnegative reals. Let and be such that . I would like to prove that . I thought of using a Taylor expansion of but cannot manage to get the inequality.","f C^2 (t_1,t_2) R^n \Vert f(t)\Vert\leq A \Vert f''(t)\Vert \leq B t\in (t_1,t_2) A B t_0\in (t_1,t_2) \alpha>0 (t_0-\alpha,t_0+\alpha)\subset (t_1,t_2) \Vert f'(t_0)\Vert\leq 2A/\alpha + B\alpha/2 f","['real-analysis', 'derivatives', 'taylor-expansion']"
50,"$f:[0,1] \to \mathbb R$ be a differentiable function with bounded derivative, then is $\int_0^1 f'(x)=f(1)-f(0)$?","be a differentiable function with bounded derivative, then is ?","f:[0,1] \to \mathbb R \int_0^1 f'(x)=f(1)-f(0)","Let $f:[0,1] \to \mathbb R$ be a differentiable function such that $\sup_{x\in [0,1]} |f'(x)|$ is finite. Then since $f'(x)=\lim_{n\to \infty} \dfrac{f(x+1/n)-f(x)}{1/n}$ , so $f'(x)$ is measurable and also the Lebesgue integral $\int_0^1|f'(x)|dx$ is finite, thus $f' \in L^1([0,1])$ . My question is, is it true that $\int_0^1 f'(x)=f(1)-f(0)$ ? Note that fundamental theorem of calculus does not apply here since $f'(x)$ is not continuous (not even known to be Riemann integrable)","Let be a differentiable function such that is finite. Then since , so is measurable and also the Lebesgue integral is finite, thus . My question is, is it true that ? Note that fundamental theorem of calculus does not apply here since is not continuous (not even known to be Riemann integrable)","f:[0,1] \to \mathbb R \sup_{x\in [0,1]} |f'(x)| f'(x)=\lim_{n\to \infty} \dfrac{f(x+1/n)-f(x)}{1/n} f'(x) \int_0^1|f'(x)|dx f' \in L^1([0,1]) \int_0^1 f'(x)=f(1)-f(0) f'(x)","['real-analysis', 'derivatives', 'lebesgue-integral', 'lebesgue-measure', 'measurable-functions']"
51,The $2n$ th derivative of $\frac{1}{1+x^2y^2}$ with respect to $x$ in a way that does not require the imaginary unit.,The  th derivative of  with respect to  in a way that does not require the imaginary unit.,2n \frac{1}{1+x^2y^2} x,"I wanted to know what is the $\frac{d^{2n}}{dx^{2n}}$ of \begin{align} \frac{1}{1+x^2y^2} \end{align} But the answer I got from another post was a function relaying on the imaginary unit $i^2=-1$ , and I wonder if we can find the $2n$ th derivative in a different way.","I wanted to know what is the of But the answer I got from another post was a function relaying on the imaginary unit , and I wonder if we can find the th derivative in a different way.","\frac{d^{2n}}{dx^{2n}} \begin{align}
\frac{1}{1+x^2y^2}
\end{align} i^2=-1 2n","['calculus', 'real-analysis', 'derivatives']"
52,"Differentiate $y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2}$, $-1\leq x\leq1$","Differentiate ,",y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2} -1\leq x\leq1,"Find $\frac{dy}{dx}$ if $y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2}$, $-1\leq x\leq1$ The solution is given as $y'=0$ in my reference. But that doesn't seem to be a complete solution as the graph of the function is: My Attempt Let $x=\sin\alpha\implies \alpha=\sin^{-1}x$ $$ y=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{1-\sin^2\alpha}=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\cos^2\alpha}\\ =\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\sin^2(\tfrac{\pi}{2}-\alpha)}=\sin^{-1}(\sin\alpha)+\sin^{-1}|\sin(\tfrac{\pi}{2}-\alpha)|\\ =n\pi+(-1)^n(\alpha)+ $$ How do I proceed further and find the derivative ?","Find $\frac{dy}{dx}$ if $y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2}$, $-1\leq x\leq1$ The solution is given as $y'=0$ in my reference. But that doesn't seem to be a complete solution as the graph of the function is: My Attempt Let $x=\sin\alpha\implies \alpha=\sin^{-1}x$ $$ y=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{1-\sin^2\alpha}=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\cos^2\alpha}\\ =\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\sin^2(\tfrac{\pi}{2}-\alpha)}=\sin^{-1}(\sin\alpha)+\sin^{-1}|\sin(\tfrac{\pi}{2}-\alpha)|\\ =n\pi+(-1)^n(\alpha)+ $$ How do I proceed further and find the derivative ?",,"['calculus', 'derivatives', 'inverse-function']"
53,"Discontinuous derivative, positive on a dense set","Discontinuous derivative, positive on a dense set",,"Whether there exists a continuos monotone function $f: [0,1]\rightarrow [0,1]$ with the following properties: (1) $f$ strictly increase, $f(0)=0$ and $f(1)=1$; (2) there is no interval $A\subset [0,1]$, where the derivative $f'$  (a) exists, (b) is continuous and (c) is finite; (3) there exists a set $B\subset [0,1]$ dense in $[0,1]$, where the derivative $f'$ (a) exists, (b) is positive and (c) is finite? I can construct an example of function $f:[0,1]\rightarrow [0,1]$ with condition (1), whose derivative is either $0$, or $+\infty$ (at points, where the dewrivative exists). It follows from Lebesgues theorem that $f'$ is zero on some set $A\subset[0,1]$, which has Lebesgue measure 1 and, whence, is dence in $[0,1]$. Clearly, since the derivative is either $0$, or $+\infty$ (in the example, which I can construct), and the function is invertible, then $f'\neq 0$ at any interval. In other words, I can answer ""yes"" to my question if ommit the word ""positive"" in the condition (3). I hope that my question is ""natural"", but neither have found ""in the internet"" neither its proof, nor counter example, nor this question as it is (for example as open problem).","Whether there exists a continuos monotone function $f: [0,1]\rightarrow [0,1]$ with the following properties: (1) $f$ strictly increase, $f(0)=0$ and $f(1)=1$; (2) there is no interval $A\subset [0,1]$, where the derivative $f'$  (a) exists, (b) is continuous and (c) is finite; (3) there exists a set $B\subset [0,1]$ dense in $[0,1]$, where the derivative $f'$ (a) exists, (b) is positive and (c) is finite? I can construct an example of function $f:[0,1]\rightarrow [0,1]$ with condition (1), whose derivative is either $0$, or $+\infty$ (at points, where the dewrivative exists). It follows from Lebesgues theorem that $f'$ is zero on some set $A\subset[0,1]$, which has Lebesgue measure 1 and, whence, is dence in $[0,1]$. Clearly, since the derivative is either $0$, or $+\infty$ (in the example, which I can construct), and the function is invertible, then $f'\neq 0$ at any interval. In other words, I can answer ""yes"" to my question if ommit the word ""positive"" in the condition (3). I hope that my question is ""natural"", but neither have found ""in the internet"" neither its proof, nor counter example, nor this question as it is (for example as open problem).",,"['derivatives', 'monotone-functions']"
54,Classifying a degenerate point,Classifying a degenerate point,,"I have the function $$f(x, y)=y^4-4y^3+x^2y-x^2-2y^2+12y-7$$ I found that the function has three critical points: $(0, 3)$, $(0, 1)$ and $(0, -1)$. I'm now trying to classify the degenerate point $(0, 1)$ by considering the value of $f$ along the curve $y=cx^2+1$ for different values of c. Substituting $y=cx^2+1$ into $f$ gives: $$f(x, cx^2+1)=cx^4-8c^2x^4+c^4x^8$$ The second derivative is: $$12cx^2-96c^2x^2+56c^4x^6$$ However, substituting $x=0$ into the second derivative gives $0$, which tells us nothing about whether the degenerate critical point is a local maximum or a local minimum. Am I missing something here? Thanks in advance.","I have the function $$f(x, y)=y^4-4y^3+x^2y-x^2-2y^2+12y-7$$ I found that the function has three critical points: $(0, 3)$, $(0, 1)$ and $(0, -1)$. I'm now trying to classify the degenerate point $(0, 1)$ by considering the value of $f$ along the curve $y=cx^2+1$ for different values of c. Substituting $y=cx^2+1$ into $f$ gives: $$f(x, cx^2+1)=cx^4-8c^2x^4+c^4x^8$$ The second derivative is: $$12cx^2-96c^2x^2+56c^4x^6$$ However, substituting $x=0$ into the second derivative gives $0$, which tells us nothing about whether the degenerate critical point is a local maximum or a local minimum. Am I missing something here? Thanks in advance.",,"['calculus', 'derivatives']"
55,Inverse of partial derivative,Inverse of partial derivative,,"Good afternoon everyone, I'm posting a similar but different question with respect to this post . I have two vectors $a \in \mathbb{R}^M$ and $b \in \mathbb{R}^N$. Assume that I know the partial derivative $\frac{\partial a}{\partial b} \in \mathbb{R}^{M\times N}$ and, from that, I would like to know its inverse $\frac{\partial b}{\partial a} \in \mathbb{R}^{N \times M}$. Also assume that the functions are continuous and differentiable. I would like to know if there is a relationship between the two maps because I know that, if both vectors are of the same dimensions, it is valid that $\frac{\partial b}{\partial a} = \left(\frac{\partial a}{\partial b}\right)^{-1}$. In my case, do I have to use the Moore-Penrose pseudoinverse? Or is there any more elegant solution? Moreover, I personally think it is wrong to adjust the previous computation for vectors. $\frac{\partial b_j}{\partial a} = \left(\frac{\partial a}{\partial b_j}\right)^{-1}$ for any component of $b$. Here I obtain a vector $\in \mathbb{R}^{1 \times M}$ that I should obtain with the pseudoinverse. I think that stacking these vectors ($j \in 1,\dots,N$) is different from the first solution I proposed at the beginning. I think this is wrong. Am I correct? I also think that computing $\frac{\partial b_j}{\partial a_i} = \left(\frac{\partial a_i}{\partial b_j}\right)^{-1}, \forall i \in \{1,\dots,M\}, j \in \{1,\dots,M\}$ and composing properly the derivative is wrong. In other words: $\left(\begin{matrix} \frac{\partial b_1}{\partial a_1} & \dots & \frac{\partial b_1}{\partial a_M} \\ \dots & \dots & \dots \\ \frac{\partial b_N}{\partial a_1} & \dots & \frac{\partial b_N}{\partial a_M} \end{matrix}\right)$. Am I right that this is wrong? Thank you in advance for any reply. Best regards, Neostek P.S. just for knowledge, the reason for which the two vectors are of different dimensionality is that $a$ is a unit quaternion in $\mathbb{R}^4$ (even if it has a 1-dim constraint) while $b$ is a standard vector in $\mathbb{R}^3$.","Good afternoon everyone, I'm posting a similar but different question with respect to this post . I have two vectors $a \in \mathbb{R}^M$ and $b \in \mathbb{R}^N$. Assume that I know the partial derivative $\frac{\partial a}{\partial b} \in \mathbb{R}^{M\times N}$ and, from that, I would like to know its inverse $\frac{\partial b}{\partial a} \in \mathbb{R}^{N \times M}$. Also assume that the functions are continuous and differentiable. I would like to know if there is a relationship between the two maps because I know that, if both vectors are of the same dimensions, it is valid that $\frac{\partial b}{\partial a} = \left(\frac{\partial a}{\partial b}\right)^{-1}$. In my case, do I have to use the Moore-Penrose pseudoinverse? Or is there any more elegant solution? Moreover, I personally think it is wrong to adjust the previous computation for vectors. $\frac{\partial b_j}{\partial a} = \left(\frac{\partial a}{\partial b_j}\right)^{-1}$ for any component of $b$. Here I obtain a vector $\in \mathbb{R}^{1 \times M}$ that I should obtain with the pseudoinverse. I think that stacking these vectors ($j \in 1,\dots,N$) is different from the first solution I proposed at the beginning. I think this is wrong. Am I correct? I also think that computing $\frac{\partial b_j}{\partial a_i} = \left(\frac{\partial a_i}{\partial b_j}\right)^{-1}, \forall i \in \{1,\dots,M\}, j \in \{1,\dots,M\}$ and composing properly the derivative is wrong. In other words: $\left(\begin{matrix} \frac{\partial b_1}{\partial a_1} & \dots & \frac{\partial b_1}{\partial a_M} \\ \dots & \dots & \dots \\ \frac{\partial b_N}{\partial a_1} & \dots & \frac{\partial b_N}{\partial a_M} \end{matrix}\right)$. Am I right that this is wrong? Thank you in advance for any reply. Best regards, Neostek P.S. just for knowledge, the reason for which the two vectors are of different dimensionality is that $a$ is a unit quaternion in $\mathbb{R}^4$ (even if it has a 1-dim constraint) while $b$ is a standard vector in $\mathbb{R}^3$.",,"['calculus', 'derivatives', 'vectors', 'partial-derivative']"
56,What is the SOP for proving inequalities by Mean Value Theorem?,What is the SOP for proving inequalities by Mean Value Theorem?,,"For example, for $x>0$, $1+x<e^x$. I know the proof of this is to apply MVT to $e^x$($x>0$), and one gets $e^x-e^0=e^{\xi}x$, where $\xi$ is between $0$ and $x$, then $\frac{e^x-1}{x}=e^{\xi}>1$, hence $1+x<e^x$ for all $x>0$. However, I always feel frustrated when doing such problem. First, how do we know that we should apply MVT to the function $e^x$? By what clues or signs? I had tried to apply MVT at $F(x)=e^x-(1+x)$, but can't deduce any meaningful things. Second, how do we know that we should pick the reference point at $0$? Also, I don't know how to solve $x>\ln(1+x)$ for all $x>0$. Do there exist a SOP(Standard Operating Procedures) or thinking routine to such questions? And on the other hand, how do you know that such questions are NOT related to the usage of Taylor expansions(I mean for order higher than $1$), while some questions must use Taylor expansions? For me, when facing the question $1+x<e^x$, I may also want to try the way   rewriting $e^x=1+x+\frac{1}{2}x^2+\frac{e^{\xi}}{3!}x^3$ as Taylor expansion, but this appears not help to solve the question. However, let's see another problem, proving $1+(1/2)x-(1/8)x^2<\sqrt{1+x}$ for all $x>0$ requires the Taylor expansions(but not MVT this time!!). How to distinguish them? (Also note that proving $\sin(x) \ge x-\frac{x^3}{6}$ requires MVT but not Taylor...) These questions frustrated me a lot. Need your help.","For example, for $x>0$, $1+x<e^x$. I know the proof of this is to apply MVT to $e^x$($x>0$), and one gets $e^x-e^0=e^{\xi}x$, where $\xi$ is between $0$ and $x$, then $\frac{e^x-1}{x}=e^{\xi}>1$, hence $1+x<e^x$ for all $x>0$. However, I always feel frustrated when doing such problem. First, how do we know that we should apply MVT to the function $e^x$? By what clues or signs? I had tried to apply MVT at $F(x)=e^x-(1+x)$, but can't deduce any meaningful things. Second, how do we know that we should pick the reference point at $0$? Also, I don't know how to solve $x>\ln(1+x)$ for all $x>0$. Do there exist a SOP(Standard Operating Procedures) or thinking routine to such questions? And on the other hand, how do you know that such questions are NOT related to the usage of Taylor expansions(I mean for order higher than $1$), while some questions must use Taylor expansions? For me, when facing the question $1+x<e^x$, I may also want to try the way   rewriting $e^x=1+x+\frac{1}{2}x^2+\frac{e^{\xi}}{3!}x^3$ as Taylor expansion, but this appears not help to solve the question. However, let's see another problem, proving $1+(1/2)x-(1/8)x^2<\sqrt{1+x}$ for all $x>0$ requires the Taylor expansions(but not MVT this time!!). How to distinguish them? (Also note that proving $\sin(x) \ge x-\frac{x^3}{6}$ requires MVT but not Taylor...) These questions frustrated me a lot. Need your help.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
57,Solving the integral $\int \frac{x+1}{x (x^2+1)}dx$,Solving the integral,\int \frac{x+1}{x (x^2+1)}dx,"I'm having trouble with the following integral: $$\int \frac{x+1}{x (x^2+1)}dx$$ Through tedious partial fraction decomposition and term-by-term integration I got the following antiderivative: $$-\frac{1}2 \log(x^2+1)+\log(|x|)+\arctan(x)+C$$ But the book gives the answer: $$ \log(|x|) + \arctan(x) + C $$ I've already turned this in (it's not graded for correctness), but I want to know what the simplest way is to calculate this integral correctly.","I'm having trouble with the following integral: $$\int \frac{x+1}{x (x^2+1)}dx$$ Through tedious partial fraction decomposition and term-by-term integration I got the following antiderivative: $$-\frac{1}2 \log(x^2+1)+\log(|x|)+\arctan(x)+C$$ But the book gives the answer: $$ \log(|x|) + \arctan(x) + C $$ I've already turned this in (it's not graded for correctness), but I want to know what the simplest way is to calculate this integral correctly.",,"['calculus', 'integration', 'derivatives']"
58,Continuity & differentiability of an improper integral of discontinuous function.,Continuity & differentiability of an improper integral of discontinuous function.,,"My question regards the continuity & differentiability of an improper integral of a discontinuous function. Let the function be defined as $$ g(t) = \int_{0}^{t}f(x)\,\mathrm{d}x $$ Assume that $f(x)$ is bounded and continuous $\forall x \in \mathbb{R},x\neq 0$. Since the number of discontinuties of $f(x)$ are countable the function is Riemann integrable on $\mathbb{R}$. Since the function is Reimann integrable is that enough to state that the function $g(x)$ is continuous on $\mathbb{R}$? As for differentiability, is knowing that the function $f(x)$ is not continuous on $\mathbb{R}$ enough to conclude that the function g(x) is not differentiable on $\mathbb{R}$ as we wouldn't be able to apply the Fundamental Theorem of Calculus?","My question regards the continuity & differentiability of an improper integral of a discontinuous function. Let the function be defined as $$ g(t) = \int_{0}^{t}f(x)\,\mathrm{d}x $$ Assume that $f(x)$ is bounded and continuous $\forall x \in \mathbb{R},x\neq 0$. Since the number of discontinuties of $f(x)$ are countable the function is Riemann integrable on $\mathbb{R}$. Since the function is Reimann integrable is that enough to state that the function $g(x)$ is continuous on $\mathbb{R}$? As for differentiability, is knowing that the function $f(x)$ is not continuous on $\mathbb{R}$ enough to conclude that the function g(x) is not differentiable on $\mathbb{R}$ as we wouldn't be able to apply the Fundamental Theorem of Calculus?",,"['real-analysis', 'integration', 'derivatives']"
59,Differentiable function with conditions.,Differentiable function with conditions.,,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ a differentiable function, with: $f(x+1)-f(x)=f'(x)$; exist $\lim_{x\rightarrow \infty }f'(x)=a\in \mathbb{R}$. Prove that $f(x)=ax+b$ I used Lagrange","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ a differentiable function, with: $f(x+1)-f(x)=f'(x)$; exist $\lim_{x\rightarrow \infty }f'(x)=a\in \mathbb{R}$. Prove that $f(x)=ax+b$ I used Lagrange",,"['real-analysis', 'derivatives']"
60,"Distinguishing critical points, relative extrema, etc.","Distinguishing critical points, relative extrema, etc.",,"I'm getting slightly confused with how critical points, inflection points, and relative extrema are all related to each other in a graph. Let me tell you what I think I know, and please correct me if I'm wrong. Relative Extrema is the local maximum or minimum of a 'hill' of a graph of a polynomial of the second degree or higher. In other words, a relative extrema (extremum? extreme?) is a point where the derivative is 0. This can be found by finding where the derivative changes signs. Can this also be found just by looking where the derivative is 0? Is that the same thing? Which is easier? Critical Points , also known as stationary points(?), is any point where the derivative is equal to 0. This can be found using the same method as above. Inflection Points is the point where the rate of change of the derivative of the graph switches signs. In other words, the point where the curvature of the graph changes from concave up to down or vice versa. This can be found by finding the double derivative and finding when its sign changes/finding its zeroes (which is easier?). Now my main problem is figuring out where these overlap. So these are my questions that I can't really get to the bottom of: Are all relative extrema critical points? Are all critical points relative extrema? Can a critical point ever not be a relative extrema? Will an inflection point always be distinct from a relative extrema and a critical point?","I'm getting slightly confused with how critical points, inflection points, and relative extrema are all related to each other in a graph. Let me tell you what I think I know, and please correct me if I'm wrong. Relative Extrema is the local maximum or minimum of a 'hill' of a graph of a polynomial of the second degree or higher. In other words, a relative extrema (extremum? extreme?) is a point where the derivative is 0. This can be found by finding where the derivative changes signs. Can this also be found just by looking where the derivative is 0? Is that the same thing? Which is easier? Critical Points , also known as stationary points(?), is any point where the derivative is equal to 0. This can be found using the same method as above. Inflection Points is the point where the rate of change of the derivative of the graph switches signs. In other words, the point where the curvature of the graph changes from concave up to down or vice versa. This can be found by finding the double derivative and finding when its sign changes/finding its zeroes (which is easier?). Now my main problem is figuring out where these overlap. So these are my questions that I can't really get to the bottom of: Are all relative extrema critical points? Are all critical points relative extrema? Can a critical point ever not be a relative extrema? Will an inflection point always be distinct from a relative extrema and a critical point?",,"['calculus', 'derivatives', 'graphing-functions']"
61,Make this visual derivative of sine more rigorous,Make this visual derivative of sine more rigorous,,"Is this the correct way to make this visualization of the derivative of sine more... rigorous?  At least, for $u\in(0,\pi/2)$. Borrowed from Proofs without words .  To try to make this rigorous, I argued that when $u\pm\Delta u$ is in the first quadrant, that we have the following geometrically obtained bounds: $$\frac{\sin(u+\Delta u)-\sin(u)}{\Delta u}<\cos(u)<\frac{\sin(u-\Delta u)-\sin(u)}{-\Delta u}$$ where $\Delta u>0$.  From this, I thought that the derivative of sine comes trivially, though I am wondering if this could be more rigorous.","Is this the correct way to make this visualization of the derivative of sine more... rigorous?  At least, for $u\in(0,\pi/2)$. Borrowed from Proofs without words .  To try to make this rigorous, I argued that when $u\pm\Delta u$ is in the first quadrant, that we have the following geometrically obtained bounds: $$\frac{\sin(u+\Delta u)-\sin(u)}{\Delta u}<\cos(u)<\frac{\sin(u-\Delta u)-\sin(u)}{-\Delta u}$$ where $\Delta u>0$.  From this, I thought that the derivative of sine comes trivially, though I am wondering if this could be more rigorous.",,"['trigonometry', 'derivatives', 'alternative-proof']"
62,Existence of time derivative in the Galerkin equation of parabolic PDEs,Existence of time derivative in the Galerkin equation of parabolic PDEs,,"Good day, Let's take this initial-boundary value parabolic PDE \begin{align} \partial_t u + Lu &=f \text{ in } \Omega_T=\Omega \times (0,T] \\   u&=0 \text{ on } \partial \Omega \times (0,T) \\ u(x,0)&=u_0(x) \text{ for } x \in \Omega \end{align}   where $Lu:=-\text{div}(A^T(x,t) \nabla u)+\langle b(x,t),\nabla u \rangle+c(x,t)u $ and its weak form \begin{align} \frac{d}{dt} (u(t),v)_{L^2}+a(u(t),v;t)&=\langle f(t),v \rangle_{H^{-1},H_0^1} \text{ for all } v \in H_0^1=:V \\ u(0)&=u_0\end{align}   where $a(w,v;t):=\int \langle \nabla w(x), A(x,t) \nabla v(x) \rangle + \langle b(x,t),\nabla w(x)\rangle v(x) + c(t) w(x) v(x) \text{ d}x $ Proving the existence of a weak solution in $L^2(0,T;H_0^1)$ for such a system can be done via Galerkin approximation where we test this system with finite-dimensional test functions. We get an ODE that has a unique solution to Cauchy-Lipschitz and one can show that this solution has a subsequence that converges weakly to the solution of our parabolic system. Using Galerkin we get \begin{align}(u_n'(t),v)_{L^2} + a(u_n(t),v;t)&=\langle f(t),v \rangle \text{ for all } v \in V_n \text{, almost all } t \in (0,T] \\  u_n(0)&=u_{0n} \end{align} and since $v \in V_n:=\text{span} \{ w_1,...,w_n \}$ and $v$ is linear in this equation we can test with $w_j$ for $j=1,...,n$. Further we look for $u_n(t)\in V_n$ i.e. $u_n(t)=\sum_{k=1}^n c_{nk}(t) w_k$ where $c_{nk}(t)$ are some 'coefficients'. Therefore most authors conclude $\color{red}{u_n'(t)=\sum_{k=1}^n c'_{nk}(t) w_k}$ and get by plugging in: \begin{align}\sum_{k=1}^n \color{red}{c'_{nk}(t)}(w_k,w_i)_{L^2} + \sum_{k=1}^n c_{nk}(t) a(w_k,w_i;t)&=\langle f(t),v \rangle, ~ 1 \leq i \leq n \\  c_{nk}(0)&=\alpha_{nk} \end{align} This approach can be seen in several PDE/Finite Element books that treat weak solutions for parabolic type PDEs. Take for example 'Galerkin Finite Element Methods for Parabolic Problems' by V. Thomée. Now my question: In this last Galerkin equation we have the time derivative of these coefficients $c_{nk}(t)$ (see the red equation above). But how do we even know that these derivatives exist? It seems like we just assume that so we can solve the ODE. But this is no real explanation to me. Why do the $u_n(t)$ have such a representation with these coefficients that are differentiable? Can somebody please help me with this? Thanks a lot, Marvin","Good day, Let's take this initial-boundary value parabolic PDE \begin{align} \partial_t u + Lu &=f \text{ in } \Omega_T=\Omega \times (0,T] \\   u&=0 \text{ on } \partial \Omega \times (0,T) \\ u(x,0)&=u_0(x) \text{ for } x \in \Omega \end{align}   where $Lu:=-\text{div}(A^T(x,t) \nabla u)+\langle b(x,t),\nabla u \rangle+c(x,t)u $ and its weak form \begin{align} \frac{d}{dt} (u(t),v)_{L^2}+a(u(t),v;t)&=\langle f(t),v \rangle_{H^{-1},H_0^1} \text{ for all } v \in H_0^1=:V \\ u(0)&=u_0\end{align}   where $a(w,v;t):=\int \langle \nabla w(x), A(x,t) \nabla v(x) \rangle + \langle b(x,t),\nabla w(x)\rangle v(x) + c(t) w(x) v(x) \text{ d}x $ Proving the existence of a weak solution in $L^2(0,T;H_0^1)$ for such a system can be done via Galerkin approximation where we test this system with finite-dimensional test functions. We get an ODE that has a unique solution to Cauchy-Lipschitz and one can show that this solution has a subsequence that converges weakly to the solution of our parabolic system. Using Galerkin we get \begin{align}(u_n'(t),v)_{L^2} + a(u_n(t),v;t)&=\langle f(t),v \rangle \text{ for all } v \in V_n \text{, almost all } t \in (0,T] \\  u_n(0)&=u_{0n} \end{align} and since $v \in V_n:=\text{span} \{ w_1,...,w_n \}$ and $v$ is linear in this equation we can test with $w_j$ for $j=1,...,n$. Further we look for $u_n(t)\in V_n$ i.e. $u_n(t)=\sum_{k=1}^n c_{nk}(t) w_k$ where $c_{nk}(t)$ are some 'coefficients'. Therefore most authors conclude $\color{red}{u_n'(t)=\sum_{k=1}^n c'_{nk}(t) w_k}$ and get by plugging in: \begin{align}\sum_{k=1}^n \color{red}{c'_{nk}(t)}(w_k,w_i)_{L^2} + \sum_{k=1}^n c_{nk}(t) a(w_k,w_i;t)&=\langle f(t),v \rangle, ~ 1 \leq i \leq n \\  c_{nk}(0)&=\alpha_{nk} \end{align} This approach can be seen in several PDE/Finite Element books that treat weak solutions for parabolic type PDEs. Take for example 'Galerkin Finite Element Methods for Parabolic Problems' by V. Thomée. Now my question: In this last Galerkin equation we have the time derivative of these coefficients $c_{nk}(t)$ (see the red equation above). But how do we even know that these derivatives exist? It seems like we just assume that so we can solve the ODE. But this is no real explanation to me. Why do the $u_n(t)$ have such a representation with these coefficients that are differentiable? Can somebody please help me with this? Thanks a lot, Marvin",,"['derivatives', 'partial-differential-equations', 'numerical-methods', 'galerkin-methods', 'parabolic-pde']"
63,Deriving a recurrence relationship for derivatives of $\frac{\arctan(x)}{x}$,Deriving a recurrence relationship for derivatives of,\frac{\arctan(x)}{x},"I was trying to derive a recurrence relantionship for computing the $k$-th derivative of the function (hoping no error during copy) $$ f(x) = \frac{\arctan(x)}{x} $$ Using maple I've seen the following derivatives $$ \begin{array}{l} f^{(0)}(x) = \frac{\arctan(x)}{x} \\ f^{(1)}(x) = \frac{1}{(x^2 + 1)x} - \frac{f(x)}{x} \\ f^{(2)}(x) = \frac{-2}{(x^2+1)^2} + \frac{-2}{(x^2 + 1)x^2} + \frac{2f(x)}{x^2} \\ f^{(3)}(x) = \frac{8}{(x^2+1)^3x^{-1}} + \frac{4}{(x^2+1)^2x} + \frac{6}{(x^2+1)x^3} - \frac{6f(x)}{x^3} \\ f^{(4)}(x) = \frac{-48}{(x^2+1)^4x^{-2}} + \frac{-8}{(x^2+1)^3} + \frac{-16}{(x^2+1)^2 x^2} + \frac{-24}{(x^2+1)x^4} + \frac{24 f(x)}{x^4} \\ f^{(5)}(x) = \frac{384}{(x^2+1)^5 x^{-3}} + \frac{-48}{(x^2+1)^4x^{-1}} + \frac{64}{(x^2+1)^3x} + \frac{80}{(x^2+1)^2x^3} + \frac{120}{(x^2+1)x^5} - \frac{120f(x)}{x^5} \end{array} $$ This lead me to the following expression, as summation, for the $k$-th derivative $$ f^{(k)}(x) = \sum_{j=1}^k \frac{c_{j,k}}{(x^2+1)^jx^{k-2j+2}} + (-1)^k \frac{k! f(x)}{x^k} $$ Firstly do you agree with me that the coefficients $c_{j,k}$ fully describes the $k$-th derivative of $f$? If not can you point out the mistake I made? The next step I would writing $f^{(k+1)}$ in terms of the coefficients $c_{j,k}$ from there I should be able to derive a recurrence relationship for the coefficients (like a triangular table that would allow me to derive the coefficients $c_{j,k}$ for given $k$ and $j = 1 \ldots k$). Do you think there's a smarter way to do achieve the same result? maybe easier? the computations involved here are quite messy. Update : here is my attempt $$ f^{(k+1)}(x) = \sum_{j=1}^k c_{j,k}\left[ \frac{-2jx}{(x^2+1)^{j+1}x^{k-2j+2}} +  \frac{k-2j+2}{(x^2+1)^jx^{k-2j+3}}\right] + \frac{(-1)^k {k!}}{(x^2+1)x^{k+1}} + (-1)^{k+1} {(k+1)!} \frac{f(x)}{x^{k+1}} $$ I got stuck now...","I was trying to derive a recurrence relantionship for computing the $k$-th derivative of the function (hoping no error during copy) $$ f(x) = \frac{\arctan(x)}{x} $$ Using maple I've seen the following derivatives $$ \begin{array}{l} f^{(0)}(x) = \frac{\arctan(x)}{x} \\ f^{(1)}(x) = \frac{1}{(x^2 + 1)x} - \frac{f(x)}{x} \\ f^{(2)}(x) = \frac{-2}{(x^2+1)^2} + \frac{-2}{(x^2 + 1)x^2} + \frac{2f(x)}{x^2} \\ f^{(3)}(x) = \frac{8}{(x^2+1)^3x^{-1}} + \frac{4}{(x^2+1)^2x} + \frac{6}{(x^2+1)x^3} - \frac{6f(x)}{x^3} \\ f^{(4)}(x) = \frac{-48}{(x^2+1)^4x^{-2}} + \frac{-8}{(x^2+1)^3} + \frac{-16}{(x^2+1)^2 x^2} + \frac{-24}{(x^2+1)x^4} + \frac{24 f(x)}{x^4} \\ f^{(5)}(x) = \frac{384}{(x^2+1)^5 x^{-3}} + \frac{-48}{(x^2+1)^4x^{-1}} + \frac{64}{(x^2+1)^3x} + \frac{80}{(x^2+1)^2x^3} + \frac{120}{(x^2+1)x^5} - \frac{120f(x)}{x^5} \end{array} $$ This lead me to the following expression, as summation, for the $k$-th derivative $$ f^{(k)}(x) = \sum_{j=1}^k \frac{c_{j,k}}{(x^2+1)^jx^{k-2j+2}} + (-1)^k \frac{k! f(x)}{x^k} $$ Firstly do you agree with me that the coefficients $c_{j,k}$ fully describes the $k$-th derivative of $f$? If not can you point out the mistake I made? The next step I would writing $f^{(k+1)}$ in terms of the coefficients $c_{j,k}$ from there I should be able to derive a recurrence relationship for the coefficients (like a triangular table that would allow me to derive the coefficients $c_{j,k}$ for given $k$ and $j = 1 \ldots k$). Do you think there's a smarter way to do achieve the same result? maybe easier? the computations involved here are quite messy. Update : here is my attempt $$ f^{(k+1)}(x) = \sum_{j=1}^k c_{j,k}\left[ \frac{-2jx}{(x^2+1)^{j+1}x^{k-2j+2}} +  \frac{k-2j+2}{(x^2+1)^jx^{k-2j+3}}\right] + \frac{(-1)^k {k!}}{(x^2+1)x^{k+1}} + (-1)^{k+1} {(k+1)!} \frac{f(x)}{x^{k+1}} $$ I got stuck now...",,"['calculus', 'derivatives', 'summation', 'recurrence-relations']"
64,Derivatives of the Dirac delta function,Derivatives of the Dirac delta function,,"From what I understand the Dirac's Delta derivatives have the meaning $$\int_{-\infty}^{\infty}\delta^{(k)}(x)\phi(x)dx=(-1)^k\int_{-\infty}^{\infty}\delta(x)\phi^{(k)}(x)dx$$ Assuming, of course that the function $\phi$ is differentiable up to order $k$. If that's true, you can then say $$\phi^{(k)}(x_0)=(-1)^k\int_{-\infty}^{\infty}\delta^{(k)}(x-x_0)\phi(x)dx$$ Is this correct? And also, could it be useful in any circumstance?","From what I understand the Dirac's Delta derivatives have the meaning $$\int_{-\infty}^{\infty}\delta^{(k)}(x)\phi(x)dx=(-1)^k\int_{-\infty}^{\infty}\delta(x)\phi^{(k)}(x)dx$$ Assuming, of course that the function $\phi$ is differentiable up to order $k$. If that's true, you can then say $$\phi^{(k)}(x_0)=(-1)^k\int_{-\infty}^{\infty}\delta^{(k)}(x-x_0)\phi(x)dx$$ Is this correct? And also, could it be useful in any circumstance?",,"['real-analysis', 'derivatives', 'distribution-theory', 'dirac-delta']"
65,Is there any solution to find a condition for $f(x)=a+bx^n+cx^2-dx>0$ to always hold true?,Is there any solution to find a condition for  to always hold true?,f(x)=a+bx^n+cx^2-dx>0,"Okay, I am interested to know the criteria for a function to always hold $$f(x)=a+bx^n+cx^2-dx>0,$$ if it is given that $a, b, c>0$ and $n\in(-2,2)$ is some real number and $x>0$.  My idea was to find a minima of this function, and at minima $x_m$, the following condition has to be satisfied $$nbx_m^{n-1}+2cx_m-d=0,$$ and the function exhibits minima only when $$n(n-1)x_m^{n-2}>-2c.$$ Now to find a condition when $f(x)>0$ is always satisfied, my idea would be to find $x_m$ from second equation and replace it in the first equation. The problem is second equation is not solvable except for some special values of $n$. So my question is it this the dead end or there is something more that can be done to approach towards the solution? Any inputs would be greatly appreciated.","Okay, I am interested to know the criteria for a function to always hold $$f(x)=a+bx^n+cx^2-dx>0,$$ if it is given that $a, b, c>0$ and $n\in(-2,2)$ is some real number and $x>0$.  My idea was to find a minima of this function, and at minima $x_m$, the following condition has to be satisfied $$nbx_m^{n-1}+2cx_m-d=0,$$ and the function exhibits minima only when $$n(n-1)x_m^{n-2}>-2c.$$ Now to find a condition when $f(x)>0$ is always satisfied, my idea would be to find $x_m$ from second equation and replace it in the first equation. The problem is second equation is not solvable except for some special values of $n$. So my question is it this the dead end or there is something more that can be done to approach towards the solution? Any inputs would be greatly appreciated.",,"['calculus', 'derivatives', 'inequality']"
66,Deriving the Normalization formula for Associated Legendre functions: Stage $2$ of $4$,Deriving the Normalization formula for Associated Legendre functions: Stage  of,2 4,"The question that follows is a continuation of this previous Stage $1$ question needed as part of a derivation of the Associated Legendre Functions Normalization Formula: $$\color{blue}{\displaystyle\int_{x=-1}^{1}[{P_{L}}^m(x)]^2\,\mathrm{d}x=\left(\frac{2}{2L+1}\right)\frac{(L+m)!}{(L-m)!}}$$ where for each $m$, the functions $${P_L}^m(x)=\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\tag{1}$$ are a set of Associated Legendre functions on $[−1, 1]$. The question in my textbook asks me to Write $(1)$ with $m$ replaced with $-m$ then use the fact that $$\begin{align}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\end{align}\quad \longleftarrow\text{(Stage 1)}$$ to show that $${P_L}^{-m}(x)=(-1)^m\frac{(L-m)!}{(L+m)!}{P_L}^{m}(x)\tag{2}$$ Start of attempt: $$\begin{align}\require{enclose}{P_L}^{-m}(x)&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L\\&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\quad\quad\longleftarrow\bbox[#AFA]{\text{Using Stage 1}}\\&= \frac{(-1)^{-m/2}}{2^LL!}\left(x^2-1\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{(-1)^{-m/2}}{2^LL!}\left(x^2-1\right)^{m/2}\frac{(L-m)!}{(L+m)!}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{\color{#F8F}{\enclose{updiagonalstrike}{\color{black}{(-1)^{-m/2}}}}\cdot\color{#F8F}{\enclose{updiagonalstrike}{\color{black}{(-1)^{m/2}}}}}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\frac{(L-m)!}{(L+m)!}\\&= \frac{(L-m)!}{(L+m)!}\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{(L-m)!}{(L+m)!}{P_{L}}^m(x)\quad\quad\longleftarrow\bbox[#FFA]{\text{By substituting Equation (1)}}\\&\ne (-1)^m\frac{(L-m)!}{(L+m)!}{P_L}^{m}(x)\end{align}$$ End of attempt. So basically I am missing a factor of $(-1)^m$ as the $(-1)^{-m/2}$ and $(-1)^{m/2}$ cancelled each other out, and I'm therefore unable to prove Equation $(2)$. I have checked the proof for errors and am unable to find any. I am guessing that I have overlooked something simple thus making a trivial mistake somewhere along the line. Is anyone able to locate and explain where I have made the error? EDIT: Taking the advice of the answer given by Markus: $$\begin{align}\require{enclose}{P_L}^{-m}(x)&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L\\&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\quad\quad\longleftarrow\bbox[#FA8]{\text{Using Stage 1}}\\&= \frac{1}{2^LL!}(-1)^m\left(1-x^2\right)^m\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= (-1)^m\frac{(L-m)!}{(L+m)!}\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= (-1)^m\frac{(L-m)!}{(L+m)!}{P_{L}}^m(x)\quad\quad\longleftarrow\bbox[#AFF]{\text{By substituting Equation (1)}}\end{align}$$ as required ..... but only thanks to Markus :) ..... and not me :(","The question that follows is a continuation of this previous Stage $1$ question needed as part of a derivation of the Associated Legendre Functions Normalization Formula: $$\color{blue}{\displaystyle\int_{x=-1}^{1}[{P_{L}}^m(x)]^2\,\mathrm{d}x=\left(\frac{2}{2L+1}\right)\frac{(L+m)!}{(L-m)!}}$$ where for each $m$, the functions $${P_L}^m(x)=\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\tag{1}$$ are a set of Associated Legendre functions on $[−1, 1]$. The question in my textbook asks me to Write $(1)$ with $m$ replaced with $-m$ then use the fact that $$\begin{align}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\end{align}\quad \longleftarrow\text{(Stage 1)}$$ to show that $${P_L}^{-m}(x)=(-1)^m\frac{(L-m)!}{(L+m)!}{P_L}^{m}(x)\tag{2}$$ Start of attempt: $$\begin{align}\require{enclose}{P_L}^{-m}(x)&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L\\&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\quad\quad\longleftarrow\bbox[#AFA]{\text{Using Stage 1}}\\&= \frac{(-1)^{-m/2}}{2^LL!}\left(x^2-1\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{(-1)^{-m/2}}{2^LL!}\left(x^2-1\right)^{m/2}\frac{(L-m)!}{(L+m)!}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{\color{#F8F}{\enclose{updiagonalstrike}{\color{black}{(-1)^{-m/2}}}}\cdot\color{#F8F}{\enclose{updiagonalstrike}{\color{black}{(-1)^{m/2}}}}}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\frac{(L-m)!}{(L+m)!}\\&= \frac{(L-m)!}{(L+m)!}\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= \frac{(L-m)!}{(L+m)!}{P_{L}}^m(x)\quad\quad\longleftarrow\bbox[#FFA]{\text{By substituting Equation (1)}}\\&\ne (-1)^m\frac{(L-m)!}{(L+m)!}{P_L}^{m}(x)\end{align}$$ End of attempt. So basically I am missing a factor of $(-1)^m$ as the $(-1)^{-m/2}$ and $(-1)^{m/2}$ cancelled each other out, and I'm therefore unable to prove Equation $(2)$. I have checked the proof for errors and am unable to find any. I am guessing that I have overlooked something simple thus making a trivial mistake somewhere along the line. Is anyone able to locate and explain where I have made the error? EDIT: Taking the advice of the answer given by Markus: $$\begin{align}\require{enclose}{P_L}^{-m}(x)&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L\\&= \frac{1}{2^LL!}\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\quad\quad\longleftarrow\bbox[#FA8]{\text{Using Stage 1}}\\&= \frac{1}{2^LL!}(-1)^m\left(1-x^2\right)^m\left(1-x^2\right)^{-m/2}\frac{(L-m)!}{(L+m)!}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= (-1)^m\frac{(L-m)!}{(L+m)!}\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\\&= (-1)^m\frac{(L-m)!}{(L+m)!}{P_{L}}^m(x)\quad\quad\longleftarrow\bbox[#AFF]{\text{By substituting Equation (1)}}\end{align}$$ as required ..... but only thanks to Markus :) ..... and not me :(",,"['derivatives', 'proof-verification', 'legendre-polynomials']"
67,"Please prove the following: Given $ƒ(x) = e^x$, verify that $\lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x$.","Please prove the following: Given , verify that .",ƒ(x) = e^x \lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x,"Given $ƒ(x) = e^x$, verify that  $$\lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x$$  and explain how this illustrates that $f'(x) = \ln e \cdot f(x) = f(x)$.","Given $ƒ(x) = e^x$, verify that  $$\lim_{h\to 0}\frac{e^{x+h} – e^x}{h} = e^x$$  and explain how this illustrates that $f'(x) = \ln e \cdot f(x) = f(x)$.",,"['calculus', 'derivatives']"
68,"Let $f$ be a real-valued continuous function on $[0,1]$ which is twice continu-ously differentiable on $(0,1)$. Suppose that $f(0) = f(1) = 0$",Let  be a real-valued continuous function on  which is twice continu-ously differentiable on . Suppose that,"f [0,1] (0,1) f(0) = f(1) = 0","Let $f$ be a real-valued continuous function on $[0,1]$ which is twice continu-ously differentiable on $(0,1)$. Suppose that $f(0) = f(1) = 0$ and $f$ satisfies the following equation: $$x^2f''(x) + x^4f'(x) - f(x) = 0$$ (a) If $f$ attains its maximum $M$ at some point $x_0$ in the open interval $(0,1)$, then prove that $M= 0$. (b)Prove that $f$ is identically zero on$[0,1]$. My idea: At $x_0$, $f'(x_0) = 0$ and $f''(x_0)$ is negative as it is given that the function is twice differentiable and attains maximum at $x_0$. The given equation at $x_0$ reduces to $$f''(x_0) = \frac{f(x_0)}{x_0^2}$$ $f''(x_0) < 0$ implies $f(x_0) < 0$. That is the maximum value attained by the function $f$ is negative on $[0,1]$. Which is a contradiction as $f$ attains $0$ at the point $x=0$. (It is given $f$ attains maximum at $x_0$) Hence the only way this can be satisfied is if $f(x_0) = M = 0$. Similarly we can prove that the minimum attained by the function $m = 0 $. since this is a continous value function on $[0,1]$, $m \le f \le M$. Hence $ f $ is identically equal to $0$.","Let $f$ be a real-valued continuous function on $[0,1]$ which is twice continu-ously differentiable on $(0,1)$. Suppose that $f(0) = f(1) = 0$ and $f$ satisfies the following equation: $$x^2f''(x) + x^4f'(x) - f(x) = 0$$ (a) If $f$ attains its maximum $M$ at some point $x_0$ in the open interval $(0,1)$, then prove that $M= 0$. (b)Prove that $f$ is identically zero on$[0,1]$. My idea: At $x_0$, $f'(x_0) = 0$ and $f''(x_0)$ is negative as it is given that the function is twice differentiable and attains maximum at $x_0$. The given equation at $x_0$ reduces to $$f''(x_0) = \frac{f(x_0)}{x_0^2}$$ $f''(x_0) < 0$ implies $f(x_0) < 0$. That is the maximum value attained by the function $f$ is negative on $[0,1]$. Which is a contradiction as $f$ attains $0$ at the point $x=0$. (It is given $f$ attains maximum at $x_0$) Hence the only way this can be satisfied is if $f(x_0) = M = 0$. Similarly we can prove that the minimum attained by the function $m = 0 $. since this is a continous value function on $[0,1]$, $m \le f \le M$. Hence $ f $ is identically equal to $0$.",,"['real-analysis', 'derivatives', 'continuity']"
69,Prove that $\int_1^{\frac{1+z}{1-z}} \frac{d^n}{dz^n} (z^2-1)^n= -\frac{2(z-1)^n}{(n+1)! }\frac{d^n}{dz^n} (\frac{z}{z-1})^{n+1} $,Prove that,\int_1^{\frac{1+z}{1-z}} \frac{d^n}{dz^n} (z^2-1)^n= -\frac{2(z-1)^n}{(n+1)! }\frac{d^n}{dz^n} (\frac{z}{z-1})^{n+1} ,"Prove that    $$\int_1^{\frac{1+z}{1-z}} \frac{d^n}{dz^n} (z^2-1)^n= -\frac{2(z-1)^n}{(n+1)! }\frac{d^n}{dz^n} (\frac{z}{z-1})^{n+1} $$ Here are some attempts. $\frac{d^n}{dz^n} (z^2-1)^n=2^n n! L_n(z),$ where $L_n(z)$ is the Legendre polynomials. I tried to use following formula for Legendre polynomials  $$\frac{x^2-1}{n} \frac{d}{dx}L_n(x)=xL_n(x)-L_{n-1}(x).$$ But I get tripped up on it.","Prove that    $$\int_1^{\frac{1+z}{1-z}} \frac{d^n}{dz^n} (z^2-1)^n= -\frac{2(z-1)^n}{(n+1)! }\frac{d^n}{dz^n} (\frac{z}{z-1})^{n+1} $$ Here are some attempts. $\frac{d^n}{dz^n} (z^2-1)^n=2^n n! L_n(z),$ where $L_n(z)$ is the Legendre polynomials. I tried to use following formula for Legendre polynomials  $$\frac{x^2-1}{n} \frac{d}{dx}L_n(x)=xL_n(x)-L_{n-1}(x).$$ But I get tripped up on it.",,"['derivatives', 'orthogonal-polynomials']"
70,Numerical Differentiation Matlab,Numerical Differentiation Matlab,,"I am trying to estimate the second derivative of $\sin(x)$ at $0.4$ for $h=10^{-k}, \ \ k=1, 2, ..., 20$ using: $$\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\tag{1}$$ And then plot the error as a function of $h$ in Matlab. Attempt: I know the exact value has to be $f''(0.4)=-\sin(0.4)= -0.389418342308651$ According to my textbook equation $(1)$ has error give by $h^2f^{(4)}/12$. So I am guessing $f^{(4)}$ refers to the fifth term in the Taylor expansion which we can find using Matlab syntax taylor(f,0.4) : sin(2/5) - (sin(2/5)*(x - 2/5)^2)/2 + (sin(2/5)*(x - 2/5)^4)/24 + cos(2/5)*(x - 2/5) - **(cos(2/5)*(x - 2/5)^3)/6** + (cos(2/5)*(x - 2/5)^5)/120 So substituting this in, here is my code for $f''(0.4)$ and error: format long x=0.4;  for k = 1:1:20     h=10.^(-k);     ddf=(sin(x+h)-2*sin(x)+sin(x-h))./(h.^2)     e=((h.^2)*((cos(2/5)*(x - 2/5).^3)/6))./12      plot(h,e) end But I get all zeros for the error. And the estimation does not look right after the 7th computation: ddf = -0.389093935175844 ddf = -0.389415097166723 ddf = -0.389418309876266 ddf = -0.389418342017223 ddf = -0.389418497448446 ddf = -0.389466237038505 ddf = -0.388578058618805 ddf = -0.555111512312578 ddf = 0.00 ddf = 0.00 ddf = -5.551115123125782e+05 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = -5.551115123125782e+13 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = 0.00 So, what is wrong with my code? And why does the errors equal zero? I thought the error vs. steps size plot should look something like This . Any help is greatly appreciated.","I am trying to estimate the second derivative of $\sin(x)$ at $0.4$ for $h=10^{-k}, \ \ k=1, 2, ..., 20$ using: $$\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\tag{1}$$ And then plot the error as a function of $h$ in Matlab. Attempt: I know the exact value has to be $f''(0.4)=-\sin(0.4)= -0.389418342308651$ According to my textbook equation $(1)$ has error give by $h^2f^{(4)}/12$. So I am guessing $f^{(4)}$ refers to the fifth term in the Taylor expansion which we can find using Matlab syntax taylor(f,0.4) : sin(2/5) - (sin(2/5)*(x - 2/5)^2)/2 + (sin(2/5)*(x - 2/5)^4)/24 + cos(2/5)*(x - 2/5) - **(cos(2/5)*(x - 2/5)^3)/6** + (cos(2/5)*(x - 2/5)^5)/120 So substituting this in, here is my code for $f''(0.4)$ and error: format long x=0.4;  for k = 1:1:20     h=10.^(-k);     ddf=(sin(x+h)-2*sin(x)+sin(x-h))./(h.^2)     e=((h.^2)*((cos(2/5)*(x - 2/5).^3)/6))./12      plot(h,e) end But I get all zeros for the error. And the estimation does not look right after the 7th computation: ddf = -0.389093935175844 ddf = -0.389415097166723 ddf = -0.389418309876266 ddf = -0.389418342017223 ddf = -0.389418497448446 ddf = -0.389466237038505 ddf = -0.388578058618805 ddf = -0.555111512312578 ddf = 0.00 ddf = 0.00 ddf = -5.551115123125782e+05 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = -5.551115123125782e+13 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = 0.00 ddf = 0.00 So, what is wrong with my code? And why does the errors equal zero? I thought the error vs. steps size plot should look something like This . Any help is greatly appreciated.",,"['derivatives', 'numerical-methods', 'matlab']"
71,Working with the $\frac{d}{dx}$ operator,Working with the  operator,\frac{d}{dx},"I have a fundamental query about the way derivatives can be used in algebraic manipulations. Say $\dfrac{d(\ln x)}{dx}=\dfrac{1}{x}$ Apparently, this can be manipulated to $d(\ln x)=\dfrac{dx}{x}$. I understand that this can be integrated back to the first equation. But, the reason this was done was to depict $\dfrac{dx}{x}$ as a percentage change in $x$. The definition of $\dfrac{df(x)}{dx}=\lim_\limits{h\to 0}\dfrac{f(x+h)-f(x)}{h}$. So, isn't $\dfrac{d}{dx}$ more like a function than a ratio of two quantities ? If so, how is moving $dx$ to RHS possible as done above ? Please advise.","I have a fundamental query about the way derivatives can be used in algebraic manipulations. Say $\dfrac{d(\ln x)}{dx}=\dfrac{1}{x}$ Apparently, this can be manipulated to $d(\ln x)=\dfrac{dx}{x}$. I understand that this can be integrated back to the first equation. But, the reason this was done was to depict $\dfrac{dx}{x}$ as a percentage change in $x$. The definition of $\dfrac{df(x)}{dx}=\lim_\limits{h\to 0}\dfrac{f(x+h)-f(x)}{h}$. So, isn't $\dfrac{d}{dx}$ more like a function than a ratio of two quantities ? If so, how is moving $dx$ to RHS possible as done above ? Please advise.",,"['calculus', 'derivatives']"
72,Covariant Derivative Clarification,Covariant Derivative Clarification,,"In my notes I have the following when taking the divergence, $\partial_\mu$ of $\partial_\alpha\varphi^\alpha g^{\mu\nu}$ $$ \partial_\mu \partial_\alpha \varphi^\alpha g^{\mu\nu} = \partial_\nu \partial_\alpha \varphi^\alpha $$ where $g^{\mu\nu}$ is the metric tensor. I would have thought I should act the metric on the derivative to get obtain, $$ \partial_\mu \partial_\alpha \varphi^\alpha g^{\mu\nu} = \partial^\nu \partial^\alpha \varphi_\alpha $$ where I just reversed the contracted indices to look pretty.","In my notes I have the following when taking the divergence, $\partial_\mu$ of $\partial_\alpha\varphi^\alpha g^{\mu\nu}$ $$ \partial_\mu \partial_\alpha \varphi^\alpha g^{\mu\nu} = \partial_\nu \partial_\alpha \varphi^\alpha $$ where $g^{\mu\nu}$ is the metric tensor. I would have thought I should act the metric on the derivative to get obtain, $$ \partial_\mu \partial_\alpha \varphi^\alpha g^{\mu\nu} = \partial^\nu \partial^\alpha \varphi_\alpha $$ where I just reversed the contracted indices to look pretty.",,"['differential-geometry', 'derivatives', 'tensors']"
73,Positivity of a series of functions involving double poles constrained by certain inequalities,Positivity of a series of functions involving double poles constrained by certain inequalities,,"This is a calculation I need for my statistics project Big edit: simplify the function $f(x)$ a lot. Define for $f(x)$ , $x\geq 0$ , $$ f(x):=\sum_{k=1}^\infty \frac{a_k^2\lambda_k^2x}{(1+x\lambda_k)^2} - \sum_{k=1}^\infty \frac{b_k^2\beta_k}{(1+x\beta_k)^3} $$ where $a_k\in \mathbb R$ , $b_k\in\mathbb R$ , $\lambda_k>0$ , $\beta_k> 0$ , and $$ \sum_{k=1}^\infty b_k^2\leq \sum_{k=1}^\infty a_k^2<\infty\,\text{ and }\sum_{k=1}^\infty a_k^2 \lambda_k\leq \sum_{k=1}^\infty b_k^2 \beta_k<\infty. $$ Additional assumption: we may think each $\beta_k$ is very large. You may take it as large as you want. I am trying to prove that $f(x)$ has following graph. That is, prove that there exists $x_0>0$ such that $f(x_0)=0$ , and for all $x<x_0$ , $f(x)<0$ , and for all $x>x_0$ , $f(x)>0$ . This question has already been solved in this link We only need to take $\beta:=\min(\beta_i)$ and the function $(1+\beta x)^3f(x)$ is increasing.","This is a calculation I need for my statistics project Big edit: simplify the function a lot. Define for , , where , , , , and Additional assumption: we may think each is very large. You may take it as large as you want. I am trying to prove that has following graph. That is, prove that there exists such that , and for all , , and for all , . This question has already been solved in this link We only need to take and the function is increasing.","f(x) f(x) x\geq 0 
f(x):=\sum_{k=1}^\infty \frac{a_k^2\lambda_k^2x}{(1+x\lambda_k)^2} - \sum_{k=1}^\infty \frac{b_k^2\beta_k}{(1+x\beta_k)^3}
 a_k\in \mathbb R b_k\in\mathbb R \lambda_k>0 \beta_k> 0 
\sum_{k=1}^\infty b_k^2\leq \sum_{k=1}^\infty a_k^2<\infty\,\text{ and }\sum_{k=1}^\infty a_k^2 \lambda_k\leq \sum_{k=1}^\infty b_k^2 \beta_k<\infty.
 \beta_k f(x) x_0>0 f(x_0)=0 x<x_0 f(x)<0 x>x_0 f(x)>0 \beta:=\min(\beta_i) (1+\beta x)^3f(x)","['calculus', 'derivatives']"
74,Integrating a second derivative,Integrating a second derivative,,Admit that $f$ has a second derivative find the integer $m$. $$m\int_{0}^{1}xf''(2x)dx = \int_0^2xf''(x)dx$$ So I took $2x=u$ where $du/dx=2$ and I plugged in the integral getting $$\frac{m}{4}\int_{0}^{2}uf''(u)du =\frac{1}{4} \int_{0}^{2} uf''(\frac{u}{2})du$$ How do I proceed from here?,Admit that $f$ has a second derivative find the integer $m$. $$m\int_{0}^{1}xf''(2x)dx = \int_0^2xf''(x)dx$$ So I took $2x=u$ where $du/dx=2$ and I plugged in the integral getting $$\frac{m}{4}\int_{0}^{2}uf''(u)du =\frac{1}{4} \int_{0}^{2} uf''(\frac{u}{2})du$$ How do I proceed from here?,,"['integration', 'derivatives', 'substitution']"
75,How to calculate this integral involving an exponential?,How to calculate this integral involving an exponential?,,I would like to calculate the integral $$\int_{0}^{\infty} xe^{-x(y+1)}dy.$$ I think I get the first steps correct. First $$\int_{0}^{\infty} xe^{-x(y+1)}dy = x\int_{0}^{\infty} e^{-x(y+1)}dy.$$ I then select $u = -x(y+1)$ so $du = -xdy$ and $dy = \frac{du}{-x}$. Therefore $$x\int_{0}^{\infty} e^{-x(y+1)}dy = x\int_{0}^{\infty} \frac{e^{u}du}{-x} = -\int_{0}^{\infty} e^{u}du.$$ I don't get how to solve it from here. I tried $$-\int_{0}^{\infty} e^{u}du = -(e^{\infty} - e^{0}) = -(0 - 1) = -1$$ but my textbook says it should be $e^{-x}$. I guess it is the back-substitution step I don't understand. How to do this?,I would like to calculate the integral $$\int_{0}^{\infty} xe^{-x(y+1)}dy.$$ I think I get the first steps correct. First $$\int_{0}^{\infty} xe^{-x(y+1)}dy = x\int_{0}^{\infty} e^{-x(y+1)}dy.$$ I then select $u = -x(y+1)$ so $du = -xdy$ and $dy = \frac{du}{-x}$. Therefore $$x\int_{0}^{\infty} e^{-x(y+1)}dy = x\int_{0}^{\infty} \frac{e^{u}du}{-x} = -\int_{0}^{\infty} e^{u}du.$$ I don't get how to solve it from here. I tried $$-\int_{0}^{\infty} e^{u}du = -(e^{\infty} - e^{0}) = -(0 - 1) = -1$$ but my textbook says it should be $e^{-x}$. I guess it is the back-substitution step I don't understand. How to do this?,,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
76,Sine Derivatives of High Orders,Sine Derivatives of High Orders,,"I want to ask whether my answer, and assertions are true. I think that, for $n\ge0$, $n\in\mathbb{Z}$: If $n\equiv1\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=\cos x$ If $n\equiv2\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=-\sin x$ If $n\equiv3\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=-\cos x$ If $n\equiv0\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=\sin x$ For my problem, to find $\dfrac{d^{87}}{dx^{87}}[\sin x]$, I came up with $-\cos x$. Is this correct?","I want to ask whether my answer, and assertions are true. I think that, for $n\ge0$, $n\in\mathbb{Z}$: If $n\equiv1\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=\cos x$ If $n\equiv2\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=-\sin x$ If $n\equiv3\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=-\cos x$ If $n\equiv0\pmod{4}$, then $\dfrac{d^n}{dx^n}[\sin x]=\sin x$ For my problem, to find $\dfrac{d^{87}}{dx^{87}}[\sin x]$, I came up with $-\cos x$. Is this correct?",,"['calculus', 'trigonometry', 'derivatives', 'modular-arithmetic']"
77,Derivative of Bezier Rectangle,Derivative of Bezier Rectangle,,"From this page Derivatives of a Bézier Curve , I can see that the derivative of a degree $N$ Bezier curve is just a Bezier curve of degree $N-1$ and it explains how to calculate the control points by each control point just being $P_{i+1} - P_i$. Is it also true that the derivative of a Bezier rectangle of degree $(M,N)$ is also just a Bezier rectangle of degree $(M-1,N-1)$?  If so, how would you calculate the control points for the rectangle? By Bezier rectangle, I mean a tensor product Bezier surface, like the kind described here Wikipediate: Bezier Surface . I've been trying to work it out on paper but no luck so far.  I've been able to make progress on coming up with a derivative, but it has quite a few terms even for a biquadratic patch, and i'd like to do this with higher degrees. My end goal is that I'm trying to calculate the gradient of a univariate Bezier rectangle that has scalar control points.  Specifically, the rectangle takes $X$ and $Z$ values from 0 to 1 and outputs a $Y$ value for the given $(X,Z)$.  I want to find the gradient so that I can use it to calculate surface normals, as well as get a distance estimation for sphere tracing (ray marching). Thanks for any help you guys can provide!","From this page Derivatives of a Bézier Curve , I can see that the derivative of a degree $N$ Bezier curve is just a Bezier curve of degree $N-1$ and it explains how to calculate the control points by each control point just being $P_{i+1} - P_i$. Is it also true that the derivative of a Bezier rectangle of degree $(M,N)$ is also just a Bezier rectangle of degree $(M-1,N-1)$?  If so, how would you calculate the control points for the rectangle? By Bezier rectangle, I mean a tensor product Bezier surface, like the kind described here Wikipediate: Bezier Surface . I've been trying to work it out on paper but no luck so far.  I've been able to make progress on coming up with a derivative, but it has quite a few terms even for a biquadratic patch, and i'd like to do this with higher degrees. My end goal is that I'm trying to calculate the gradient of a univariate Bezier rectangle that has scalar control points.  Specifically, the rectangle takes $X$ and $Z$ values from 0 to 1 and outputs a $Y$ value for the given $(X,Z)$.  I want to find the gradient so that I can use it to calculate surface normals, as well as get a distance estimation for sphere tracing (ray marching). Thanks for any help you guys can provide!",,"['derivatives', 'partial-derivative', 'bezier-curve']"
78,Differential at a point and differential (Differential Geometry),Differential at a point and differential (Differential Geometry),,"Given $f\in C^\infty(U)$, $U$ open set of $\mathbb{R}^n$, we define the differential of $f$ at $p$ $$ (df)_p:T_p\mathbb{R}^n\to\mathbb{R}\\ (df)_p(v):=v(f) $$ and the differential of $f$ $$ df:U\to T^*U\\ p\mapsto (p,(df)_p) $$ where $T^*U$ is the cotagent bundle. I can understand that $(df)_p$ is the map which associates for every point $p$ any derivative $v$ to $f$. While $df$ is just obtained by gluing together all these local differentials. Now I'm asked to compute the differential of the i-th-coordinate map $x_i:\mathbb{R}^n\to\mathbb{R}$. Here is my (unsuccesful) reasonement. I have to start by calculating, for every $p$, $(dx_i)_p$. By definition $(dx_i)_p\in (T_p\mathbb{R^n})^*$ and the latter set is spanned by $\{(dx_i)_p:1\le i\le n\}$. Now, I am a bit confused. My professor defined the basis of $(T_p\mathbb{R}^n)^*$ as the dual basis of the tangent plane and the symbols $(dx_i)_p$ are just formal symbols, nothing to do with the differential. (Right?) I would proceed by applying the differential of $x_i$ at $p$ to a general element of the tangent space $v=\sum_j v_j(\frac{\partial}{\partial x_j})_p$ $$ (dx_i)_p(v):=v_i $$ Then how shall I go on?","Given $f\in C^\infty(U)$, $U$ open set of $\mathbb{R}^n$, we define the differential of $f$ at $p$ $$ (df)_p:T_p\mathbb{R}^n\to\mathbb{R}\\ (df)_p(v):=v(f) $$ and the differential of $f$ $$ df:U\to T^*U\\ p\mapsto (p,(df)_p) $$ where $T^*U$ is the cotagent bundle. I can understand that $(df)_p$ is the map which associates for every point $p$ any derivative $v$ to $f$. While $df$ is just obtained by gluing together all these local differentials. Now I'm asked to compute the differential of the i-th-coordinate map $x_i:\mathbb{R}^n\to\mathbb{R}$. Here is my (unsuccesful) reasonement. I have to start by calculating, for every $p$, $(dx_i)_p$. By definition $(dx_i)_p\in (T_p\mathbb{R^n})^*$ and the latter set is spanned by $\{(dx_i)_p:1\le i\le n\}$. Now, I am a bit confused. My professor defined the basis of $(T_p\mathbb{R}^n)^*$ as the dual basis of the tangent plane and the symbols $(dx_i)_p$ are just formal symbols, nothing to do with the differential. (Right?) I would proceed by applying the differential of $x_i$ at $p$ to a general element of the tangent space $v=\sum_j v_j(\frac{\partial}{\partial x_j})_p$ $$ (dx_i)_p(v):=v_i $$ Then how shall I go on?",,"['differential-geometry', 'derivatives', 'tensors', 'multilinear-algebra']"
79,Counter Example of Continuous Functions,Counter Example of Continuous Functions,,"I came across this question: Question : Let $f$ be a real continuous function defined on [0,1] such that $f(0) = 0$ and $f(1) = 1$.  Prove or give a counter-example to the following: a) If $f'$ exists a.e. in $[0,1]$, then $\int_0^1 f' dx  = 1$. b) If $f'$ is absolutely continuous in $[0,1]$, then $\int_0^1 f' dx = 1$. c) If $f'$ is non-decreasing in $[0,1]$, then $\int_0^1 f' dx = 1$. Thoughts : I feel like I can just take $f'(x) = x^{1/2}$ for all three parts and thus I get a counter example for all three conditions?  Am I missing something or is this correct?","I came across this question: Question : Let $f$ be a real continuous function defined on [0,1] such that $f(0) = 0$ and $f(1) = 1$.  Prove or give a counter-example to the following: a) If $f'$ exists a.e. in $[0,1]$, then $\int_0^1 f' dx  = 1$. b) If $f'$ is absolutely continuous in $[0,1]$, then $\int_0^1 f' dx = 1$. c) If $f'$ is non-decreasing in $[0,1]$, then $\int_0^1 f' dx = 1$. Thoughts : I feel like I can just take $f'(x) = x^{1/2}$ for all three parts and thus I get a counter example for all three conditions?  Am I missing something or is this correct?",,"['real-analysis', 'integration', 'derivatives', 'examples-counterexamples']"
80,Differentiation of every order and Taylor series,Differentiation of every order and Taylor series,,"Let $f(x)$ be a function defined in $(-1,1)$ with derivatives of all orders at zero equal to zero; that is: $f'(0)=0 , f''(0)=0 , f'''(0)=0 ...$ If there exists $c>0$ such that:  $Sup|f^{(n)}(x)| <= n!c^n$ with $x \in (-1,1)$ and $n \in \mathbb{N}$ where $f^{(n)}(x)$ is the $n$th-derivative of $f(x)$ and $Sup|f^{(n)}(x)|$ is the supremum of $n$th-derivative of $f(x)$ in absolute value. Prove that $f(x)=0$ for any $x \in (-1,1)$.","Let $f(x)$ be a function defined in $(-1,1)$ with derivatives of all orders at zero equal to zero; that is: $f'(0)=0 , f''(0)=0 , f'''(0)=0 ...$ If there exists $c>0$ such that:  $Sup|f^{(n)}(x)| <= n!c^n$ with $x \in (-1,1)$ and $n \in \mathbb{N}$ where $f^{(n)}(x)$ is the $n$th-derivative of $f(x)$ and $Sup|f^{(n)}(x)|$ is the supremum of $n$th-derivative of $f(x)$ in absolute value. Prove that $f(x)=0$ for any $x \in (-1,1)$.",,"['calculus', 'real-analysis', 'derivatives']"
81,Discrete-time derivative of the sign function,Discrete-time derivative of the sign function,,"How does one calculate the time derivative of $$ x_{k+1} = C_1\, \text{sign}(x_k-y_k)\sqrt{2\vert x_k-y_k\vert}, $$ with respect to $x_k$ ? I know that the right part of the equation should yield $$ \frac{\partial}{\partial\,x_k} \sqrt{2\vert x_k-y_k\vert} = \frac{x_k-y_k}{\vert x_k -y_k\vert^{\frac{3}{2}}}. $$ Yet, I don't know to get the final result. How can one differentiate the sign function?","How does one calculate the time derivative of $$ x_{k+1} = C_1\, \text{sign}(x_k-y_k)\sqrt{2\vert x_k-y_k\vert}, $$ with respect to $x_k$ ? I know that the right part of the equation should yield $$ \frac{\partial}{\partial\,x_k} \sqrt{2\vert x_k-y_k\vert} = \frac{x_k-y_k}{\vert x_k -y_k\vert^{\frac{3}{2}}}. $$ Yet, I don't know to get the final result. How can one differentiate the sign function?",,"['discrete-mathematics', 'derivatives', 'partial-derivative', 'discrete-optimization']"
82,What did i do wrong with this derivation?,What did i do wrong with this derivation?,,"$$ \cos(x) = \sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{(2n)!} $$ Therefore \begin{align} \frac{1}{\cos(x)} &= \frac{1}{1-(\frac{x^2}{2} - \frac{x^4}{4!} + \frac{x^6}{6!} - \cdots)} \\ &= \sum_{n=0}^\infty (\frac{x^2}{2} - \frac{x^4}{4!} + \frac{x^6}{6!} - \cdots)^n \\ &= \sum_{n=0}^\infty (1-\cos(x))^n \end{align} This part i am okay with, but now i will attempt to find a power series of $\tan(x)$ in terms of $\cos(x)$ with the following reasoning: I realised that by differentiating The secant function i would get $\sec(x) \tan(x)$ and by differentiating the series would also yield a $\sin(x)$, therefore i would end up with $\sec^2(x)$ (which is the derivative of $\tan(x)$) therefore by integration i would find myself a power series for $\tan(x)$ in terms of $\cos(x)$ I will do as i explained below: $$ \frac{d}{dx}[\sec(x)] = \sum_{n=0}^\infty \frac{d}{dx}(1-\cos(x))^n $$ $$ \frac{\sin(x)}{\cos^2(x)} = \sin(x) \sum_{n=0}^\infty (n+1) (1-\cos(x))^n $$ $$ \frac{1}{\cos^2(x)} = \sum_{n=0}^\infty (n+1) (1-\cos(x))^n $$ Now to redefine $(1-\cos(x))^n$ by using the following identity: $$ (1+x)^{\alpha} = \sum_{k=0}^\alpha {\alpha \choose k} x^k $$ Therefore: $$ (1-\cos(x))^n = \sum_{k=0}^n {n \choose k}(-\cos(x))^k $$ Now this is where i run into some doubt: $$ \sec^2(x) = \sum_{n=0}^\infty (n+1) \sum_{k=0}^n {n \choose k}(-\cos(x))^k $$ Since the following is true for ordinary power series: $$ fg \longleftrightarrow \left\{\sum_k a_k b_{n-k} \right\} $$ Therefore by setting $a_k = \frac{(-\cos(x))^k}{k!}$ and $b_{n-k} = \frac{1}{(n-k)!}$ $$ \sum_{n=0}^\infty (n+1) \sum_{k=0}^n {n \choose k}(-\cos(x))^k = \sum_{n=0}^\infty (n+1)! e^{1-\cos(x)} $$ This is clearly wrong obviously as the series does not converge... What was my error?","$$ \cos(x) = \sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{(2n)!} $$ Therefore \begin{align} \frac{1}{\cos(x)} &= \frac{1}{1-(\frac{x^2}{2} - \frac{x^4}{4!} + \frac{x^6}{6!} - \cdots)} \\ &= \sum_{n=0}^\infty (\frac{x^2}{2} - \frac{x^4}{4!} + \frac{x^6}{6!} - \cdots)^n \\ &= \sum_{n=0}^\infty (1-\cos(x))^n \end{align} This part i am okay with, but now i will attempt to find a power series of $\tan(x)$ in terms of $\cos(x)$ with the following reasoning: I realised that by differentiating The secant function i would get $\sec(x) \tan(x)$ and by differentiating the series would also yield a $\sin(x)$, therefore i would end up with $\sec^2(x)$ (which is the derivative of $\tan(x)$) therefore by integration i would find myself a power series for $\tan(x)$ in terms of $\cos(x)$ I will do as i explained below: $$ \frac{d}{dx}[\sec(x)] = \sum_{n=0}^\infty \frac{d}{dx}(1-\cos(x))^n $$ $$ \frac{\sin(x)}{\cos^2(x)} = \sin(x) \sum_{n=0}^\infty (n+1) (1-\cos(x))^n $$ $$ \frac{1}{\cos^2(x)} = \sum_{n=0}^\infty (n+1) (1-\cos(x))^n $$ Now to redefine $(1-\cos(x))^n$ by using the following identity: $$ (1+x)^{\alpha} = \sum_{k=0}^\alpha {\alpha \choose k} x^k $$ Therefore: $$ (1-\cos(x))^n = \sum_{k=0}^n {n \choose k}(-\cos(x))^k $$ Now this is where i run into some doubt: $$ \sec^2(x) = \sum_{n=0}^\infty (n+1) \sum_{k=0}^n {n \choose k}(-\cos(x))^k $$ Since the following is true for ordinary power series: $$ fg \longleftrightarrow \left\{\sum_k a_k b_{n-k} \right\} $$ Therefore by setting $a_k = \frac{(-\cos(x))^k}{k!}$ and $b_{n-k} = \frac{1}{(n-k)!}$ $$ \sum_{n=0}^\infty (n+1) \sum_{k=0}^n {n \choose k}(-\cos(x))^k = \sum_{n=0}^\infty (n+1)! e^{1-\cos(x)} $$ This is clearly wrong obviously as the series does not converge... What was my error?",,"['calculus', 'trigonometry', 'derivatives', 'power-series', 'generating-functions']"
83,Two times differentiable function,Two times differentiable function,,"We know that the two times differentiable function  $g : \mathbb{R} \rightarrow \mathbb{R}$ is such that $g(0) = 999$, $g'(0) = 1000$ and $|g''(x)| \le 10000$ for every $x \in \mathbb{R}$. Let $K := g(\frac{1}{1000})$. How to prove that $K$'s second digit after the decimal point equals to $9$ or $0$? What are its other digits?","We know that the two times differentiable function  $g : \mathbb{R} \rightarrow \mathbb{R}$ is such that $g(0) = 999$, $g'(0) = 1000$ and $|g''(x)| \le 10000$ for every $x \in \mathbb{R}$. Let $K := g(\frac{1}{1000})$. How to prove that $K$'s second digit after the decimal point equals to $9$ or $0$? What are its other digits?",,['derivatives']
84,Prove that a Lipschitz continuous function is differentiable at a point ${\bf x}_0$,Prove that a Lipschitz continuous function is differentiable at a point,{\bf x}_0,"Consider $f: B({\bf x}_0,r)\to \Bbb R$, that apart from being Lipschitz continuous, has directional derivatives at the point $x_0$ and $\frac{\partial f}{\partial{\bf v}}({\bf x}_0)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}({\bf x}_0)v_i$ for every direction $\bf v$.  How do I use these facts to show that it is differentiable at $x_0$? I am pretty clueless about how to go about this problem; the hint given says we need to use contradiction by assuming it is not differentiable and use the Bolzano Weierstrass theorem.","Consider $f: B({\bf x}_0,r)\to \Bbb R$, that apart from being Lipschitz continuous, has directional derivatives at the point $x_0$ and $\frac{\partial f}{\partial{\bf v}}({\bf x}_0)=\sum_{i=1}^n \frac{\partial f}{\partial x_i}({\bf x}_0)v_i$ for every direction $\bf v$.  How do I use these facts to show that it is differentiable at $x_0$? I am pretty clueless about how to go about this problem; the hint given says we need to use contradiction by assuming it is not differentiable and use the Bolzano Weierstrass theorem.",,"['real-analysis', 'derivatives', 'convergence-divergence', 'partial-derivative', 'lipschitz-functions']"
85,Partial Derivatives versus Proper Derivatives,Partial Derivatives versus Proper Derivatives,,"I'm having some difficulty understanding exactly what a partial derivative is. I had been content with the definition $$\frac{\partial F}{\partial x_i } = \lim_{\Delta x \rightarrow 0} \frac{F(x_0, x_2 ... x_{i-1}, x_i + \Delta x, x_{i+1} ... x_n ) - F(x_0 ... x_n )}{\Delta x} $$ However now I am beginning to realize that there is a bit more going on here than originally intended. Consider in the definition of the Euler Lagrange Equations where given an operator $$L (x,y(x), y'(x)) $$ We are seeking to find the optimal y for this operator. We are required to solve $$\frac{\partial L}{\partial y} -\frac{d}{dx}[\frac{\partial L}{\partial y'}] = 0 $$ To find such L. But here's my issue. Where we treat all other variables constant except what we are deriving w.r.t, How can you treat y constant and derive w.r.t. y' Because if either varies, then so does its complement. For example consider expression $L = y^2$. If y = $e^x$ then $$ \frac{\partial L}{\partial y'} = \frac{\partial e^{2x}}{\partial e^x} = 2e^x = 2y$$ But if $y = x^3$ then $$ \frac{\partial L}{\partial y'} = \frac{\partial x^6}{\partial (3x^2)} = x^4 = y^{\frac{4}{3}}$$ In other words... what the hell? What does the partial derivative REALLY mean?","I'm having some difficulty understanding exactly what a partial derivative is. I had been content with the definition $$\frac{\partial F}{\partial x_i } = \lim_{\Delta x \rightarrow 0} \frac{F(x_0, x_2 ... x_{i-1}, x_i + \Delta x, x_{i+1} ... x_n ) - F(x_0 ... x_n )}{\Delta x} $$ However now I am beginning to realize that there is a bit more going on here than originally intended. Consider in the definition of the Euler Lagrange Equations where given an operator $$L (x,y(x), y'(x)) $$ We are seeking to find the optimal y for this operator. We are required to solve $$\frac{\partial L}{\partial y} -\frac{d}{dx}[\frac{\partial L}{\partial y'}] = 0 $$ To find such L. But here's my issue. Where we treat all other variables constant except what we are deriving w.r.t, How can you treat y constant and derive w.r.t. y' Because if either varies, then so does its complement. For example consider expression $L = y^2$. If y = $e^x$ then $$ \frac{\partial L}{\partial y'} = \frac{\partial e^{2x}}{\partial e^x} = 2e^x = 2y$$ But if $y = x^3$ then $$ \frac{\partial L}{\partial y'} = \frac{\partial x^6}{\partial (3x^2)} = x^4 = y^{\frac{4}{3}}$$ In other words... what the hell? What does the partial derivative REALLY mean?",,"['calculus', 'derivatives', 'partial-differential-equations', 'partial-derivative', 'euler-lagrange-equation']"
86,Calculate the distance between intersection points of tangents to a parabola,Calculate the distance between intersection points of tangents to a parabola,,"Question Tangent lines $T_1$ and $T_2$ are drawn at two points $P_1$ and $P_2$ on the parabola $y=x^2$ and they intersect at a point $P$. Another tangent line $T$ is drawn at a point between $P_1$ and $P_2$; it intersects $T_1$ at $Q_1$ and $T_2$ at $Q_2$. Show that $$\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|} = 1$$ My attempt at the question I include a possible scenario of the graph for convenience'(I hope) sake: The outer two tangents are tangents $T_1$ and $T_2$, and the inner tangent is tangent $T$. Since points $P_1$ and $P_2$ are points on the parabola, I can give them coordinates as follows, $$\tag 1 P_1(P_{1x}, P^2_{1x})$$ and  $$\tag 2 P_2(P_{2x}, P^2_{2x})$$ Using $y\prime = 2x$, I calculate the equations for the tangents $T_1$ and $T_2$ respectively, they are, $$T_1 = y = 2P_{1x}(x - P_{1x}) + P^2_{1x}$$ and $$T_2 = y = 2P_{2x}(x - P_{2x}) + P^2_{2x}$$ By setting $T_1 = T_2$ and then solving for $x$ I show that the two tangents intersect at a point $x = \frac{P_{1x} + P_{2x}}{2}$, which in words is the two tangents to the parabola is halfway between points $P_1$ and $P_2$. Then substituting $x = \frac{P_{1x} + P_{2x}}{2}$ into any of the tangent line equations I get the $y$ coordinate of the tangent lines' intersection, which is $y = P_{1x}\cdot P_{2x}$ Now I have the coordinates for point $P$, that is $$\tag 3 P\Big(\frac{P_{1x} + P_{2x}}{2}, P_{1x}\cdot P_{2x}\Big)$$ To get coordinates for points $Q_1$ and $Q_2$ I will substitute $Q_{1x}$ in to the equation of tangent $T_1$ and substitute $Q_{2x}$ in to the equation of tangent $T_2$. That yields the following for coordinates: $$\tag 4 Q_1(Q_{1x}, \,\,2P_{1x}Q_{1x} - P_{1x}^2)$$ $$\tag 5 Q_2(Q_{2x}, \,\,2P_{2x}Q_{2x} - P_{2x}^2)$$ Since I have all the points necessary to calculate $\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|}$, I feel inclined to apply the distance formula. Doing so yielded the following: $$\tag 6 |PQ_1| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{1x})^2}}{2}$$ $$\tag 7 |PP_1| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}$$ $$\tag 8 |PQ_2| = \frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{2x})^2}}{2}$$ $$\tag 9 |PP_2| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}$$ Now I calculate $\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|}$ using the above:  $$\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|} = \frac{\frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{1x})^2}}{2}}{\frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}} + \frac{\frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{2x})^2}}{2}}{\frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}}$$ $$\tag {10} =\frac{\sqrt{(P_{1x} + P_{2x} - 2Q_{1x})^2} + \sqrt{(P_{1x} + P_{2x} - 2Q_{2x})^2}}{\sqrt{(P_{1x} - P_{2x})^2}}$$ I can't seem to find a way to show that $(10)$ is equal to $1$. I have, however tested a few instances and it held up, for what it's worth. But for now, I'm at a loss as to how to proceed. Any hints, suggestions, or alternative approaches?","Question Tangent lines $T_1$ and $T_2$ are drawn at two points $P_1$ and $P_2$ on the parabola $y=x^2$ and they intersect at a point $P$. Another tangent line $T$ is drawn at a point between $P_1$ and $P_2$; it intersects $T_1$ at $Q_1$ and $T_2$ at $Q_2$. Show that $$\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|} = 1$$ My attempt at the question I include a possible scenario of the graph for convenience'(I hope) sake: The outer two tangents are tangents $T_1$ and $T_2$, and the inner tangent is tangent $T$. Since points $P_1$ and $P_2$ are points on the parabola, I can give them coordinates as follows, $$\tag 1 P_1(P_{1x}, P^2_{1x})$$ and  $$\tag 2 P_2(P_{2x}, P^2_{2x})$$ Using $y\prime = 2x$, I calculate the equations for the tangents $T_1$ and $T_2$ respectively, they are, $$T_1 = y = 2P_{1x}(x - P_{1x}) + P^2_{1x}$$ and $$T_2 = y = 2P_{2x}(x - P_{2x}) + P^2_{2x}$$ By setting $T_1 = T_2$ and then solving for $x$ I show that the two tangents intersect at a point $x = \frac{P_{1x} + P_{2x}}{2}$, which in words is the two tangents to the parabola is halfway between points $P_1$ and $P_2$. Then substituting $x = \frac{P_{1x} + P_{2x}}{2}$ into any of the tangent line equations I get the $y$ coordinate of the tangent lines' intersection, which is $y = P_{1x}\cdot P_{2x}$ Now I have the coordinates for point $P$, that is $$\tag 3 P\Big(\frac{P_{1x} + P_{2x}}{2}, P_{1x}\cdot P_{2x}\Big)$$ To get coordinates for points $Q_1$ and $Q_2$ I will substitute $Q_{1x}$ in to the equation of tangent $T_1$ and substitute $Q_{2x}$ in to the equation of tangent $T_2$. That yields the following for coordinates: $$\tag 4 Q_1(Q_{1x}, \,\,2P_{1x}Q_{1x} - P_{1x}^2)$$ $$\tag 5 Q_2(Q_{2x}, \,\,2P_{2x}Q_{2x} - P_{2x}^2)$$ Since I have all the points necessary to calculate $\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|}$, I feel inclined to apply the distance formula. Doing so yielded the following: $$\tag 6 |PQ_1| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{1x})^2}}{2}$$ $$\tag 7 |PP_1| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}$$ $$\tag 8 |PQ_2| = \frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{2x})^2}}{2}$$ $$\tag 9 |PP_2| = \frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}$$ Now I calculate $\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|}$ using the above:  $$\frac{|PQ_1|}{|PP_1|} + \frac{|PQ_2|}{|PP_2|} = \frac{\frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{1x})^2}}{2}}{\frac{\sqrt{(4P_{1x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}} + \frac{\frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} + P_{2x} - 2Q_{2x})^2}}{2}}{\frac{\sqrt{(4P_{2x}^2 + 1)(P_{1x} - P_{2x})^2}}{2}}$$ $$\tag {10} =\frac{\sqrt{(P_{1x} + P_{2x} - 2Q_{1x})^2} + \sqrt{(P_{1x} + P_{2x} - 2Q_{2x})^2}}{\sqrt{(P_{1x} - P_{2x})^2}}$$ I can't seem to find a way to show that $(10)$ is equal to $1$. I have, however tested a few instances and it held up, for what it's worth. But for now, I'm at a loss as to how to proceed. Any hints, suggestions, or alternative approaches?",,"['calculus', 'derivatives', 'analytic-geometry']"
87,Find the derivative of the inverse of an antiderivative,Find the derivative of the inverse of an antiderivative,,"Find $(f^{-1})'(0)$ if $f(x) = \int_1^x{ \cos(\cos t)dt}$ For the problem there was no interval given so that the function $\cos(\cos t)$ was strictly increasing (which we need for inverse) Can I just pick any interval that works, say $0$ to $\pi/2$? Secondly I know by FTC that since $\cos(\cos t)$ is continuous that $f$ is differentiable. How does the integral integral over $[1,x]$ come into play here? Do I need to take an antiderivative? Any hints would be wonderful.","Find $(f^{-1})'(0)$ if $f(x) = \int_1^x{ \cos(\cos t)dt}$ For the problem there was no interval given so that the function $\cos(\cos t)$ was strictly increasing (which we need for inverse) Can I just pick any interval that works, say $0$ to $\pi/2$? Secondly I know by FTC that since $\cos(\cos t)$ is continuous that $f$ is differentiable. How does the integral integral over $[1,x]$ come into play here? Do I need to take an antiderivative? Any hints would be wonderful.",,"['calculus', 'integration', 'derivatives', 'inverse']"
88,convolution with $C^{\infty}$ produces $C^{\infty}$,convolution with  produces,C^{\infty} C^{\infty},"Problem: So I have the following function in $\mathbb{R^p}$$$f_{\sigma}\left(\boldsymbol{x}\right)=\dfrac{1}{\left(2\pi\right)^{d/2}}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$ and I would like to show that it infinitely differentiable.  where $f$ is a bounded function with compact support Attempted: So first notice that $$Df_{\sigma}\left(\boldsymbol{x}\right)=\dfrac{1}{\left(2\pi\right)^{d/2}}\lim_{x\in B,m\left(B\right)\rightarrow0}\dfrac{1}{m\left(B\right)}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$which by rearrange terms on the RHS gives $$\dfrac{1}{\left(2\pi\right)^{d/2}}\lim_{x\in B,m\left(B\right)\rightarrow0}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\dfrac{1}{m\left(B\right)}\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$. I think I am supposed to to Dominated Convergence Theorem here. But I don't see what is dominating this function, i.e, the satisfying condition for me to invoke DCT..","Problem: So I have the following function in $\mathbb{R^p}$$$f_{\sigma}\left(\boldsymbol{x}\right)=\dfrac{1}{\left(2\pi\right)^{d/2}}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$ and I would like to show that it infinitely differentiable.  where $f$ is a bounded function with compact support Attempted: So first notice that $$Df_{\sigma}\left(\boldsymbol{x}\right)=\dfrac{1}{\left(2\pi\right)^{d/2}}\lim_{x\in B,m\left(B\right)\rightarrow0}\dfrac{1}{m\left(B\right)}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$which by rearrange terms on the RHS gives $$\dfrac{1}{\left(2\pi\right)^{d/2}}\lim_{x\in B,m\left(B\right)\rightarrow0}\int_{\mathbb{R}^{p}}f\left(\boldsymbol{z}\right)\dfrac{1}{m\left(B\right)}\exp\left(-\dfrac{1}{2\sigma^{2}}\left|\boldsymbol{z}-\boldsymbol{x}\right|^{2}\right)d\boldsymbol{z}$$. I think I am supposed to to Dominated Convergence Theorem here. But I don't see what is dominating this function, i.e, the satisfying condition for me to invoke DCT..",,"['real-analysis', 'derivatives', 'convergence-divergence', 'fourier-analysis']"
89,Nth Derivative of a fucntion,Nth Derivative of a fucntion,,Find the $N^{th}$ derivative of $$f(x) = \sqrt{\frac {1-x}{1+x}}$$ I have got $1^{st}$ derivative as: $\frac{-1}{(1-x)^{1/2}(1+x)^{3/2}}$ and $2^{nd}$ derivative as: $\frac{1-2x}{(1-x)^{3/2}(1+x)^{5/2}}$ and $3^{rd}$ derivative as: $\frac{-6x^2+6x-3}{(1-x)^{5/2}(1+x)^{7/2}}$ I can see how will the denominator gets it form but can you help me with the numerator? Thanks. :),Find the $N^{th}$ derivative of $$f(x) = \sqrt{\frac {1-x}{1+x}}$$ I have got $1^{st}$ derivative as: $\frac{-1}{(1-x)^{1/2}(1+x)^{3/2}}$ and $2^{nd}$ derivative as: $\frac{1-2x}{(1-x)^{3/2}(1+x)^{5/2}}$ and $3^{rd}$ derivative as: $\frac{-6x^2+6x-3}{(1-x)^{5/2}(1+x)^{7/2}}$ I can see how will the denominator gets it form but can you help me with the numerator? Thanks. :),,['derivatives']
90,Closed form of $\sum_{k=1}^{n}\binom{n}{k} h^{(n-k)}(0)f^{(k-1)}(0)$,Closed form of,\sum_{k=1}^{n}\binom{n}{k} h^{(n-k)}(0)f^{(k-1)}(0),Is there a closed form for: $$\sum_{k=1}^{n}\binom{n}{k} h^{(n-k)}(0)f^{(k-1)}(0)$$ where: $$h(x)=(1-x)^{\alpha}(A-Bx)^{\frac{1}{\gamma}-\alpha}$$ and $$f(x)=-x(1-x)^{-1-\alpha}(A-Bx)^{-(\frac{1}{\gamma}-\alpha)-1}$$ where $h^{(n-k)}(0)$ is the $(n-k)$th derivative of $h(x)$ at $x=0$. I came across this trying to find the generating function for a distribution. I first solved a first order differential equation by integrating factor . As a result I obtained: $$y(x)=(1-x)^{\alpha}(A-Bx)^{\frac{1}{\gamma}-\alpha} \int -x(1-x)^{-1-\alpha}(A-Bx)^{-(\frac{1}{\gamma}-\alpha)-1}dx$$  Next I need to find the Taylor series of $y(x)=\sum \frac{y^{(n)}(0)}{n!}x^n$. This requires the evaluation of the sum in this question. There seems to be a relation between $h$ and $f$ that makes me think(hope) there should be a nice closed form solution for it. I would greatly appreciate any suggestion!,Is there a closed form for: $$\sum_{k=1}^{n}\binom{n}{k} h^{(n-k)}(0)f^{(k-1)}(0)$$ where: $$h(x)=(1-x)^{\alpha}(A-Bx)^{\frac{1}{\gamma}-\alpha}$$ and $$f(x)=-x(1-x)^{-1-\alpha}(A-Bx)^{-(\frac{1}{\gamma}-\alpha)-1}$$ where $h^{(n-k)}(0)$ is the $(n-k)$th derivative of $h(x)$ at $x=0$. I came across this trying to find the generating function for a distribution. I first solved a first order differential equation by integrating factor . As a result I obtained: $$y(x)=(1-x)^{\alpha}(A-Bx)^{\frac{1}{\gamma}-\alpha} \int -x(1-x)^{-1-\alpha}(A-Bx)^{-(\frac{1}{\gamma}-\alpha)-1}dx$$  Next I need to find the Taylor series of $y(x)=\sum \frac{y^{(n)}(0)}{n!}x^n$. This requires the evaluation of the sum in this question. There seems to be a relation between $h$ and $f$ that makes me think(hope) there should be a nice closed form solution for it. I would greatly appreciate any suggestion!,,"['derivatives', 'binomial-coefficients', 'generating-functions', 'closed-form']"
91,Definition of integration,Definition of integration,,The derivative of a function is defined by $$ f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{f(x+\Delta x)-f(x)}{\Delta x}} $$ provided the limit exists. For example for $f(x)=\sin(x)$ we can prove that (see here ) $$ f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{\sin(x+\Delta x)-\sin(x)}{\Delta x}}=\cos(x) $$ But for integration there are only a set of formulas that come from the above definition (i.e. with knowing the derivative of a function). Is there a general definition for integration like above definition (maybe an anti-limit!) that acts on a function directly?,The derivative of a function is defined by $$ f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{f(x+\Delta x)-f(x)}{\Delta x}} $$ provided the limit exists. For example for $f(x)=\sin(x)$ we can prove that (see here ) $$ f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{\sin(x+\Delta x)-\sin(x)}{\Delta x}}=\cos(x) $$ But for integration there are only a set of formulas that come from the above definition (i.e. with knowing the derivative of a function). Is there a general definition for integration like above definition (maybe an anti-limit!) that acts on a function directly?,,"['calculus', 'integration', 'derivatives']"
92,How can I find a curve based on its tangent lines?,How can I find a curve based on its tangent lines?,,Let's say for some curve its tangent lines at every point have a property that the length of a segment within the first quarter $[0;+\infty)^2$ is exactly $C>0$. How can such a curve be defined analytically? Maybe even in terms of $y=f(x)$. Now about the tangent lines themselves. Noticeably for all $k<0$ the tangent line $y = kx - \frac{Ck}{\sqrt{k^2 + 1}}$ seems to fit the description precisely because the length of its segment within the first quarter is equal to $C$. But what to do next?,Let's say for some curve its tangent lines at every point have a property that the length of a segment within the first quarter $[0;+\infty)^2$ is exactly $C>0$. How can such a curve be defined analytically? Maybe even in terms of $y=f(x)$. Now about the tangent lines themselves. Noticeably for all $k<0$ the tangent line $y = kx - \frac{Ck}{\sqrt{k^2 + 1}}$ seems to fit the description precisely because the length of its segment within the first quarter is equal to $C$. But what to do next?,,"['derivatives', 'plane-curves']"
93,A tough one: show that this is not differentiable at any point in R,A tough one: show that this is not differentiable at any point in R,,"Here's the question: Define  $\phi: \ \mathbb{R} \rightarrow \mathbb{R}$ by  $$ \phi(x) = \begin{cases}x & 0\leq x\leq\frac{1}{2}\\ 1-x & \frac{1}{2}\leq x\leq 1\end{cases}. $$ And then extend periodically to all of $\mathbb{R}$ by $\phi(x) = \phi(x+1)$. Now define $$S_m(x) = \sum\limits_{i=0}^m \left(\frac{3}{4}\right)^i \phi(4^ix)$$ Show that S, the limit function of $S_m(x)$ is not differentiable at any point in $\mathbb{R}$. You are allowed to define sequences somewhat implicitly, such as ""let $a_n$ be the largest multiple of $4^{-n}$ which is less than or equal to $\pi$."" My thoughts:  I'm not sure where to start. My guess is that we can choose two sequences $a_n$ and $b_n$ where $a_n$ is monotone increasing and $b_n$ is monotone decreasing and they both converge to an arbitrary point $c$ in $\mathbb{R}$ and show that $\lim_{n\rightarrow\infty} \frac{f(b_n) - f(a_n)}{b-a} \neq f'(c)$. But I'm not sure how to define those sequences. Or if that would even be sufficient.","Here's the question: Define  $\phi: \ \mathbb{R} \rightarrow \mathbb{R}$ by  $$ \phi(x) = \begin{cases}x & 0\leq x\leq\frac{1}{2}\\ 1-x & \frac{1}{2}\leq x\leq 1\end{cases}. $$ And then extend periodically to all of $\mathbb{R}$ by $\phi(x) = \phi(x+1)$. Now define $$S_m(x) = \sum\limits_{i=0}^m \left(\frac{3}{4}\right)^i \phi(4^ix)$$ Show that S, the limit function of $S_m(x)$ is not differentiable at any point in $\mathbb{R}$. You are allowed to define sequences somewhat implicitly, such as ""let $a_n$ be the largest multiple of $4^{-n}$ which is less than or equal to $\pi$."" My thoughts:  I'm not sure where to start. My guess is that we can choose two sequences $a_n$ and $b_n$ where $a_n$ is monotone increasing and $b_n$ is monotone decreasing and they both converge to an arbitrary point $c$ in $\mathbb{R}$ and show that $\lim_{n\rightarrow\infty} \frac{f(b_n) - f(a_n)}{b-a} \neq f'(c)$. But I'm not sure how to define those sequences. Or if that would even be sufficient.",,"['calculus', 'real-analysis', 'derivatives']"
94,Chain Rule: Is the notation $\frac{dy}{du} \cdot \frac{du}{dx} = \frac{dy}{dx}$ accurate?,Chain Rule: Is the notation  accurate?,\frac{dy}{du} \cdot \frac{du}{dx} = \frac{dy}{dx},"My question is if it is okay / mathematically rigorous to write the Chain Rule like that (the Leibniz way). I thought that $dx$, etc. do not follow the rules of algebra and cannot be treated as such. For example, I write $\int 1\, dx$, rather than $\int 1 \,dx$, and I write $\int \frac{dx}{a}$ instead of $\int \frac{1}{a}\, dx$. So, is it correct to say $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$ and, in essence, have the $\frac{du}{du}$ cancel to $1$? (If my notion of $dx$ is not correct, I would also like an explanation of what really that is) edit: Is it the same deal with $\frac{dy}{dx} = \frac{dy/dt}{dx/dt}$?","My question is if it is okay / mathematically rigorous to write the Chain Rule like that (the Leibniz way). I thought that $dx$, etc. do not follow the rules of algebra and cannot be treated as such. For example, I write $\int 1\, dx$, rather than $\int 1 \,dx$, and I write $\int \frac{dx}{a}$ instead of $\int \frac{1}{a}\, dx$. So, is it correct to say $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$ and, in essence, have the $\frac{du}{du}$ cancel to $1$? (If my notion of $dx$ is not correct, I would also like an explanation of what really that is) edit: Is it the same deal with $\frac{dy}{dx} = \frac{dy/dt}{dx/dt}$?",,"['derivatives', 'notation', 'education']"
95,Is this Frechet derivative correct?,Is this Frechet derivative correct?,,"Problem statement: Let $u \in L^2[0, 1]$ and $$J(u) = \int_0^1 u(t) u(1-t)dt$$ Find $J'(u)$ and $J''(u)$. Attempted solution: First derivative There is a hint that the derivative looks like this: $$J(u + h) - J(u) = \langle J'(u), h\rangle + o(\|h\|)$$ I do not really understand why, since this is not the definition of the Frechet derivative. However: $$ \begin{split} J(u + h) - J(u) &= \int_0^1\left(u\left(t\right) + h\left(t\right)\right)\left(u\left(1 - t\right) + h\left(1 - t\right)\right)dt - \int_0^1u\left(t\right)u\left(1 - t\right)dt \\ &=\int_0^1 h(t)(u(1 - t) + h(1 - t))dt + \int_0^1u(t) h(1 - t)dt \\ &= \int_0^1h(t) h(1 - t)dt + \int_0^1h(t)u(1 - t)dt + \int_0^1u(t)h(1 - t)dt  \end{split} $$  Where it can be shown that the last two integrals are actually equal. Which results in: $$J(u + h) - J(u) = \int_0^1 h(t) h(1 - t)dt + 2 \int_0^1 u(1 - t) h(t) dt$$ If we now recall the definition of scalar product in $L^2[0, 1]$: $\langle f, g \rangle = \int_0^1 f(t) g(t) dt$ then we can rewrite our result as: $$J(u + h) - J(u) = \langle 2 u(1 - t), h(t)\rangle + \langle h(t), h(1 - t)\rangle$$ Let's now show $\langle h(t), h(1 - t)\rangle = o(\|h\|)$: $$\lim_{h \to 0} \frac{|\langle h(t), h(1 - t)\rangle|}{\|h(t)\|} \le \lim_{h \to 0} \frac{\|h(t)\| \|h(1 - t)\|}{\|h(t)\|} = \lim_{h \to 0} \|h(1 - t)\| = 0$$ Where the inequality is the Cauchy-Bunyakovsky-Schwarz inequality. Second derivative There are no hints this time but here is the definition from class (B is a matrix): $$ J'(u + p) - J'(u) = Bp + o(\|p\|)$$ As far as I understand, $J''(u) = B$ at point p. Which leaves me with: $$J'(u + p) - J'(u) = 2(u(1 - t) + p(1 - t)) - 2 u(1 - t) = 2p(1 - t) = o(\|p\|)$$ Which means $J''(u) = B = 0$. My current answer to the problem: $J'(u) = 2 u(1 - t)$ $J''(u) = 0$ My questions: Is my answer correct, are there any mistakes? Why does the first hint look like this? There are no scalar products in the definition of a Frechet derivative...","Problem statement: Let $u \in L^2[0, 1]$ and $$J(u) = \int_0^1 u(t) u(1-t)dt$$ Find $J'(u)$ and $J''(u)$. Attempted solution: First derivative There is a hint that the derivative looks like this: $$J(u + h) - J(u) = \langle J'(u), h\rangle + o(\|h\|)$$ I do not really understand why, since this is not the definition of the Frechet derivative. However: $$ \begin{split} J(u + h) - J(u) &= \int_0^1\left(u\left(t\right) + h\left(t\right)\right)\left(u\left(1 - t\right) + h\left(1 - t\right)\right)dt - \int_0^1u\left(t\right)u\left(1 - t\right)dt \\ &=\int_0^1 h(t)(u(1 - t) + h(1 - t))dt + \int_0^1u(t) h(1 - t)dt \\ &= \int_0^1h(t) h(1 - t)dt + \int_0^1h(t)u(1 - t)dt + \int_0^1u(t)h(1 - t)dt  \end{split} $$  Where it can be shown that the last two integrals are actually equal. Which results in: $$J(u + h) - J(u) = \int_0^1 h(t) h(1 - t)dt + 2 \int_0^1 u(1 - t) h(t) dt$$ If we now recall the definition of scalar product in $L^2[0, 1]$: $\langle f, g \rangle = \int_0^1 f(t) g(t) dt$ then we can rewrite our result as: $$J(u + h) - J(u) = \langle 2 u(1 - t), h(t)\rangle + \langle h(t), h(1 - t)\rangle$$ Let's now show $\langle h(t), h(1 - t)\rangle = o(\|h\|)$: $$\lim_{h \to 0} \frac{|\langle h(t), h(1 - t)\rangle|}{\|h(t)\|} \le \lim_{h \to 0} \frac{\|h(t)\| \|h(1 - t)\|}{\|h(t)\|} = \lim_{h \to 0} \|h(1 - t)\| = 0$$ Where the inequality is the Cauchy-Bunyakovsky-Schwarz inequality. Second derivative There are no hints this time but here is the definition from class (B is a matrix): $$ J'(u + p) - J'(u) = Bp + o(\|p\|)$$ As far as I understand, $J''(u) = B$ at point p. Which leaves me with: $$J'(u + p) - J'(u) = 2(u(1 - t) + p(1 - t)) - 2 u(1 - t) = 2p(1 - t) = o(\|p\|)$$ Which means $J''(u) = B = 0$. My current answer to the problem: $J'(u) = 2 u(1 - t)$ $J''(u) = 0$ My questions: Is my answer correct, are there any mistakes? Why does the first hint look like this? There are no scalar products in the definition of a Frechet derivative...",,"['derivatives', 'optimization']"
96,Function derivatives,Function derivatives,,"Earlier I asked a question about derivatives and for some reason i'm just not able to answer the question. However, I've attempted a near identical question but on another past paper, and think I may have it. If someone could check over and see if it's correct. If it is correct, could someone explain to me why I can do this one but not the other -.- Given the function $f(x)=2x^3-3x^2-36x+5$ Compute the derivative $f'(x)$ Find and classify the stationary points of $f(x)$ $$\frac{d}{dx} f(x) = 6x^2 - 6x - 36$$ I then used the quadratic equation in order to find the values of $x$, being $-2$ when minus and $3$ when addition. So, I then went onto to calculate $y$ for each by substituting the values of $x$ in. From this I learned when $x = -2, y = 49$ and when $x = 3, y = -76$. Giving me $( -2, 49 )$ and $( 3, -76 )$. From here I introduced derivative 2 in order to find rate of change. From this I found  $$\frac{d^2y}{dx^2} = 12x - 6$$ Once again I substituted the values of $x$ in and found when $x = -2 dx^2 = -30$ meaning it is in fact a maximum since it's below the value 0. I then went onto to prove $x = 3$ eventually giving me that $dx^2$ is $30$ therefore $(3, -76)$ is a minimum. I'm pretty bad at maths to be honest. However, I found this pretty straight forward. However, for my other question Quadratic formula - math error I still feel literally clueless. If I understand this then should I understand my other question? The questions are basically identical, I just don't understand the logic..","Earlier I asked a question about derivatives and for some reason i'm just not able to answer the question. However, I've attempted a near identical question but on another past paper, and think I may have it. If someone could check over and see if it's correct. If it is correct, could someone explain to me why I can do this one but not the other -.- Given the function $f(x)=2x^3-3x^2-36x+5$ Compute the derivative $f'(x)$ Find and classify the stationary points of $f(x)$ $$\frac{d}{dx} f(x) = 6x^2 - 6x - 36$$ I then used the quadratic equation in order to find the values of $x$, being $-2$ when minus and $3$ when addition. So, I then went onto to calculate $y$ for each by substituting the values of $x$ in. From this I learned when $x = -2, y = 49$ and when $x = 3, y = -76$. Giving me $( -2, 49 )$ and $( 3, -76 )$. From here I introduced derivative 2 in order to find rate of change. From this I found  $$\frac{d^2y}{dx^2} = 12x - 6$$ Once again I substituted the values of $x$ in and found when $x = -2 dx^2 = -30$ meaning it is in fact a maximum since it's below the value 0. I then went onto to prove $x = 3$ eventually giving me that $dx^2$ is $30$ therefore $(3, -76)$ is a minimum. I'm pretty bad at maths to be honest. However, I found this pretty straight forward. However, for my other question Quadratic formula - math error I still feel literally clueless. If I understand this then should I understand my other question? The questions are basically identical, I just don't understand the logic..",,"['calculus', 'derivatives']"
97,Question about a solution to a problem involving Taylor's theorem and local minimum,Question about a solution to a problem involving Taylor's theorem and local minimum,,"I've been studying ""Berkeley Problems in Mathematics, Souza, Silva"" and I came across this problem: Let $f : \mathbb{R} \rightarrow \mathbb{R}$  be a $C^{\infty}$ function. Assume that $f(x)$ has a local minimum at $x = 0$. Prove there is a disc centered on the $y$ axis which lies above the graph of $f$ and touches the graph at $(0, f(0))$. We use Taylor's theorem: there is a constant $C$ such that $|f(x) - f(0) - f’(0)x| \le Cx^2$ and we assume that $|x| < 1$. Why is that? I know that if a function has a local minimum at $0$, it means that in a certain neighbourhood its values cannot be less than $f(0)$. Will anything bad happen if we instead assume that $|x|<\delta<1$ ? Please help me. I see it's a crucial step in the solution of this problem. http://thor.info.uaic.ro/~fliacob/An1/2012-2013/Concursuri/SEEMOUS-2013/Baza%20de%20documentare/Souza,%20Silva%20-%20Berkeley%20Problems%20In%20Mathematics%20(440S).pdf question: Problem 1.4.26  page 24 , solution: page 177","I've been studying ""Berkeley Problems in Mathematics, Souza, Silva"" and I came across this problem: Let $f : \mathbb{R} \rightarrow \mathbb{R}$  be a $C^{\infty}$ function. Assume that $f(x)$ has a local minimum at $x = 0$. Prove there is a disc centered on the $y$ axis which lies above the graph of $f$ and touches the graph at $(0, f(0))$. We use Taylor's theorem: there is a constant $C$ such that $|f(x) - f(0) - f’(0)x| \le Cx^2$ and we assume that $|x| < 1$. Why is that? I know that if a function has a local minimum at $0$, it means that in a certain neighbourhood its values cannot be less than $f(0)$. Will anything bad happen if we instead assume that $|x|<\delta<1$ ? Please help me. I see it's a crucial step in the solution of this problem. http://thor.info.uaic.ro/~fliacob/An1/2012-2013/Concursuri/SEEMOUS-2013/Baza%20de%20documentare/Souza,%20Silva%20-%20Berkeley%20Problems%20In%20Mathematics%20(440S).pdf question: Problem 1.4.26  page 24 , solution: page 177",,"['derivatives', 'taylor-expansion']"
98,"Complexified tangent vector, complex manifold","Complexified tangent vector, complex manifold",,"Consider a complex submanifold $M$ of a complex ambient vector space $X$. Suppose you have a base point $p \in M$ and a $C^1$ arc $\gamma(t)$ passing through $p$ and staying in $M$, with tangent vector at $p$ denoted by $u$ (the tangent vector is taken by considering $X$ as a real vector space, and $\gamma : [0,1] \rightarrow M$). Is there a complex analytic curve $\tilde{\gamma} : \Delta \rightarrow M$ ($\Delta$ being a complex disk) passing through $p$, whose image is in $M$, with tangent vector at $p$ equal to $u$ (as an element of $T_pM$) ? What if the curve $\gamma$ is only differentiable at $p$ ? Intuitively I think the answer is yes, but I can't construct such a curve $\tilde{\gamma}$. Thanks in advance for your help","Consider a complex submanifold $M$ of a complex ambient vector space $X$. Suppose you have a base point $p \in M$ and a $C^1$ arc $\gamma(t)$ passing through $p$ and staying in $M$, with tangent vector at $p$ denoted by $u$ (the tangent vector is taken by considering $X$ as a real vector space, and $\gamma : [0,1] \rightarrow M$). Is there a complex analytic curve $\tilde{\gamma} : \Delta \rightarrow M$ ($\Delta$ being a complex disk) passing through $p$, whose image is in $M$, with tangent vector at $p$ equal to $u$ (as an element of $T_pM$) ? What if the curve $\gamma$ is only differentiable at $p$ ? Intuitively I think the answer is yes, but I can't construct such a curve $\tilde{\gamma}$. Thanks in advance for your help",,"['differential-geometry', 'derivatives', 'complex-geometry']"
99,Derivation of Euler-Lagrange equation,Derivation of Euler-Lagrange equation,,"Here is  a simple (probably trivial) step in the derivation of the Euler-Lagrange equation.     If we denote $Y(x) = y(x) + \epsilon \eta(x) $, I want to know why is $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \dfrac{\partial f(y,x)}{\partial y} $ Could someone justify the  steps involved in justifying this ? I am sure am missing something elementary. EDIT: Could someone comment on correctness of the 'proof ' ?     $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \lim_{H\rightarrow 0} \dfrac{f(y + \epsilon \eta  + H,x) -f(y + \epsilon \eta,x)}{H} \Big\vert_{\epsilon = 0} = \lim_{H\rightarrow 0} \dfrac{f(y + H,x) -f(y,x)}{H} = \dfrac{\partial f(y,x)}{\partial y} $ EDIT 2: The answers provided still leave me confused. Here is another attempt at a ""proof"". $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \lim_{\epsilon\rightarrow 0} \dfrac{\partial f(Y,x)}{\partial Y} = \lim_{\epsilon \rightarrow 0} \left(\lim_{H\rightarrow 0} \dfrac{f(y + \epsilon \eta  + H,x) -f(y + \epsilon \eta,x)}{H} \right)$ Now if I could interchange the limits, then it would make sense that I get   $\lim_{H\rightarrow 0}  \dfrac{f(y + H,x) -f(y,x)}{H} = \dfrac{\partial f(y,x)}{\partial y} $. So is the interchange of limits allowed. What hypothesis must be satisfied for that to happen ?","Here is  a simple (probably trivial) step in the derivation of the Euler-Lagrange equation.     If we denote $Y(x) = y(x) + \epsilon \eta(x) $, I want to know why is $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \dfrac{\partial f(y,x)}{\partial y} $ Could someone justify the  steps involved in justifying this ? I am sure am missing something elementary. EDIT: Could someone comment on correctness of the 'proof ' ?     $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \lim_{H\rightarrow 0} \dfrac{f(y + \epsilon \eta  + H,x) -f(y + \epsilon \eta,x)}{H} \Big\vert_{\epsilon = 0} = \lim_{H\rightarrow 0} \dfrac{f(y + H,x) -f(y,x)}{H} = \dfrac{\partial f(y,x)}{\partial y} $ EDIT 2: The answers provided still leave me confused. Here is another attempt at a ""proof"". $\dfrac{\partial f(Y,x)}{\partial Y} \Big\vert_{\epsilon = 0} = \lim_{\epsilon\rightarrow 0} \dfrac{\partial f(Y,x)}{\partial Y} = \lim_{\epsilon \rightarrow 0} \left(\lim_{H\rightarrow 0} \dfrac{f(y + \epsilon \eta  + H,x) -f(y + \epsilon \eta,x)}{H} \right)$ Now if I could interchange the limits, then it would make sense that I get   $\lim_{H\rightarrow 0}  \dfrac{f(y + H,x) -f(y,x)}{H} = \dfrac{\partial f(y,x)}{\partial y} $. So is the interchange of limits allowed. What hypothesis must be satisfied for that to happen ?",,"['optimization', 'derivatives', 'calculus-of-variations', 'partial-derivative']"
