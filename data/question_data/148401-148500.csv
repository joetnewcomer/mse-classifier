,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sum of continuous functions is continuous with multiple variables,Sum of continuous functions is continuous with multiple variables,,"Background I have seen proofs showing that the sums of two continuous functions $f_1, g_1 :$ $\mathbb{R} \rightarrow \mathbb{R} $ are continuous, and I have also seen this result for functions $f_2, g_2 :$ $\mathbb{R}^n \rightarrow \mathbb{R}$ . However, I have not seen a generalisation of the claim for when we take the functions $f_3 : \mathbb{R}^k \rightarrow \mathbb{R}^n$ and $g_3 : \mathbb{R}^q \rightarrow \mathbb{R}^n $ . If both of these functions are continuous, then this should imply that the sum is also continuous, however, I am struggling to prove the claim. For clarity, we define the sum to be the function $h(x,y) := f_3(x) + g_3(y)$ , where we want to prove that this is continuous given that $f_3, g_3$ are continuous. Attempt I have shown that the projections of $ \mathbb{R}^k $ and $ \mathbb{R}^q $ $p_1(x,y) := x \space \space$ and $\space \space p_2(x,y) := y$ are both continuous. But am unsure of how to proceed from here. My initial thoughts are that an epsilon - delta argument should be able to work for two specified values of delta which are valid for showing that the individual functions are continuous (by our assumption). I’m assuming we can then use these to construct a new values of delta that will hold for the sum. However, I haven’t made much progress here as of yet. If anyone could help me construct a proof, or point me towards a reference, I would be grateful.","Background I have seen proofs showing that the sums of two continuous functions are continuous, and I have also seen this result for functions . However, I have not seen a generalisation of the claim for when we take the functions and . If both of these functions are continuous, then this should imply that the sum is also continuous, however, I am struggling to prove the claim. For clarity, we define the sum to be the function , where we want to prove that this is continuous given that are continuous. Attempt I have shown that the projections of and and are both continuous. But am unsure of how to proceed from here. My initial thoughts are that an epsilon - delta argument should be able to work for two specified values of delta which are valid for showing that the individual functions are continuous (by our assumption). I’m assuming we can then use these to construct a new values of delta that will hold for the sum. However, I haven’t made much progress here as of yet. If anyone could help me construct a proof, or point me towards a reference, I would be grateful.","f_1, g_1 : \mathbb{R} \rightarrow \mathbb{R}  f_2, g_2 : \mathbb{R}^n \rightarrow \mathbb{R} f_3 : \mathbb{R}^k \rightarrow \mathbb{R}^n g_3 : \mathbb{R}^q \rightarrow \mathbb{R}^n  h(x,y) := f_3(x) + g_3(y) f_3, g_3  \mathbb{R}^k   \mathbb{R}^q  p_1(x,y) := x \space \space \space \space p_2(x,y) := y","['real-analysis', 'analysis', 'functions', 'continuity']"
1,"Can $\sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n$ for distinct $(x_i)_{i\in \mathbb{N}}$ from interval $(0,1)$?",Can  for distinct  from interval ?,"\sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n (x_i)_{i\in \mathbb{N}} (0,1)","Can we have an example of two distinct power series $\sum_{n=0}^\infty a_nx^n$ and $\sum_{n=0}^\infty b_n x^n$ with the radius of convergence equal $1$ and there exists a sequence $\{x_i\}_{i\in\mathbb{N}}$ in $(-1,1)$ so that $$\sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n$$ for $i\in \mathbb{N}$ ? Theorem 8.5 in Rudin's Principles of Mathematical Analysis says we don't have such an example if $\{x_i\}_{i\in\mathbb{N}}$ has a limit point in $(-1,1)$ . What if $x_i\nearrow 1$ ?",Can we have an example of two distinct power series and with the radius of convergence equal and there exists a sequence in so that for ? Theorem 8.5 in Rudin's Principles of Mathematical Analysis says we don't have such an example if has a limit point in . What if ?,"\sum_{n=0}^\infty a_nx^n \sum_{n=0}^\infty b_n x^n 1 \{x_i\}_{i\in\mathbb{N}} (-1,1) \sum_{n=0}^\infty a_nx_i^n = \sum_{n=0}^\infty b_nx_i^n i\in \mathbb{N} \{x_i\}_{i\in\mathbb{N}} (-1,1) x_i\nearrow 1","['analysis', 'power-series', 'analytic-functions']"
2,Integral of $\ln(dx+1)$ [closed],Integral of  [closed],\ln(dx+1),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I want to evaluate the following integral integral of ln(dx+1) , yes the integral is inside the ln, NOT outside The inspiration came from this video integral of x^dx-1 , where the integral was divided and then multiplied by dx, so when evaluating my integral I wanted to use the same trick to get this integral . Then I took the limit as dx approaches 0 to get infinity . So finally I ended up with an integral of infinity * dx which is just equal to infinity . I know the question doesn't make any mathematical sense, so I'm just asking if the question can be solved ""symbolically""? Am I doing any mistakes in my calculation of the inegral? Is there another way to actually solve it? Many thanks,","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I want to evaluate the following integral integral of ln(dx+1) , yes the integral is inside the ln, NOT outside The inspiration came from this video integral of x^dx-1 , where the integral was divided and then multiplied by dx, so when evaluating my integral I wanted to use the same trick to get this integral . Then I took the limit as dx approaches 0 to get infinity . So finally I ended up with an integral of infinity * dx which is just equal to infinity . I know the question doesn't make any mathematical sense, so I'm just asking if the question can be solved ""symbolically""? Am I doing any mistakes in my calculation of the inegral? Is there another way to actually solve it? Many thanks,",,"['real-analysis', 'calculus', 'analysis']"
3,"For an infinite sequence of functions $\Bbb{R}\to\Bbb{R}$, each function is a composition of a certain finite set of functions $\Bbb{R}\to\Bbb{R}$.","For an infinite sequence of functions , each function is a composition of a certain finite set of functions .",\Bbb{R}\to\Bbb{R} \Bbb{R}\to\Bbb{R},"Given an infinite sequence of functions $\{g_1, g_2, \ldots, g_n, \ldots\}$ where $ g_n : \Bbb R \to \Bbb R$ prove there's a finite set of functions $ \{ f_1, f_2, \ldots, f_M \} $ such that any $ g_n $ can be represented as a composition of $ f_m $ 's. Honestly, not sure even how to approach this. The intuition is that if the infinite sequence of functions is not defined using finite set of functions and composition then the sequence definition would be infinite itself, but I don't know how to formalize that.","Given an infinite sequence of functions where prove there's a finite set of functions such that any can be represented as a composition of 's. Honestly, not sure even how to approach this. The intuition is that if the infinite sequence of functions is not defined using finite set of functions and composition then the sequence definition would be infinite itself, but I don't know how to formalize that.","\{g_1, g_2, \ldots, g_n, \ldots\}  g_n : \Bbb R \to \Bbb R  \{ f_1, f_2, \ldots, f_M \}   g_n   f_m ","['analysis', 'functions', 'function-and-relation-composition', 'sequence-of-function']"
4,Looking for a book that picks up where Understanding Analysis by Abbott left off?,Looking for a book that picks up where Understanding Analysis by Abbott left off?,,"I'm currently going through the Understanding Analysis text by Abbott and was interested in what typically comes after once I finish going through this book. Would multivariable analysis of some sort come next? If so, what book would you recommend? Or is there more ""single-variable"" analysis left to be done? Edit: I noticed the downvote, would appreciate any advice on how I can improve my answer.","I'm currently going through the Understanding Analysis text by Abbott and was interested in what typically comes after once I finish going through this book. Would multivariable analysis of some sort come next? If so, what book would you recommend? Or is there more ""single-variable"" analysis left to be done? Edit: I noticed the downvote, would appreciate any advice on how I can improve my answer.",,"['real-analysis', 'analysis', 'reference-request', 'book-recommendation']"
5,Prove that $\sup S \leq \inf T$,Prove that,\sup S \leq \inf T,Could you help me here? Let $S$ and $T$ be nonempty subsets of $\mathbb{R}$ and suppose that for all $s \in S $ and $t \in T$ we have $s \leqslant t$ . Prove that $\sup S \leqslant  \inf T $ PS: My idea was to try 2 cases: when $S \subset T$ and $ S \cap T = \emptyset $ . Am I right? I was able to prove that $t \geq \sup S$ but the final part I couldn't. Any help?,Could you help me here? Let and be nonempty subsets of and suppose that for all and we have . Prove that PS: My idea was to try 2 cases: when and . Am I right? I was able to prove that but the final part I couldn't. Any help?,S T \mathbb{R} s \in S  t \in T s \leqslant t \sup S \leqslant  \inf T  S \subset T  S \cap T = \emptyset  t \geq \sup S,"['real-analysis', 'analysis', 'inequality', 'supremum-and-infimum', 'solution-verification']"
6,The convergence of $\sum_1^{+\infty}b_n$ follows from the convergence of $\sum_1^{+\infty}a_n$ given that $\frac{a_n}{b_n}\to1$.,The convergence of  follows from the convergence of  given that .,\sum_1^{+\infty}b_n \sum_1^{+\infty}a_n \frac{a_n}{b_n}\to1,"It is true that if $$ \sum_1^{+\infty}a_n\qquad\text{and}\sum_1^{+\infty}b_n $$ satisfies $$ \lim_{n\to+\infty}\frac{a_n}{b_n}=1, $$ then the convergence of $\sum_1^{+\infty}b_n$ follows from the convergence of $\sum_1^{+\infty}a_n$ ? What I know is that if there are both positive series, this claim is true.","It is true that if satisfies then the convergence of follows from the convergence of ? What I know is that if there are both positive series, this claim is true.","
\sum_1^{+\infty}a_n\qquad\text{and}\sum_1^{+\infty}b_n
 
\lim_{n\to+\infty}\frac{a_n}{b_n}=1,
 \sum_1^{+\infty}b_n \sum_1^{+\infty}a_n",['analysis']
7,Does there exist a bijection $f: \mathbb N \rightarrow \mathbb N$ such that if $f(a) = b$ then either $b = a^2$ or $a = b^2$,Does there exist a bijection  such that if  then either  or,f: \mathbb N \rightarrow \mathbb N f(a) = b b = a^2 a = b^2,Does there exist a bijection $f: \mathbb N \rightarrow \mathbb N$ such that if $f(a) = b$ then either $b = a^2$ or $a = b^2$ ? Any help would be highly appreciated.,Does there exist a bijection such that if then either or ? Any help would be highly appreciated.,f: \mathbb N \rightarrow \mathbb N f(a) = b b = a^2 a = b^2,"['number-theory', 'analysis', 'functions']"
8,$\int f^2$ and $\int f''^2$ is convergent then so is $\int f'^2$,and  is convergent then so is,\int f^2 \int f''^2 \int f'^2,"$f$ is second order differentiable in $[0,+\infty)$ . And $\int_0^\infty f^2$ and $\int_0^\infty f''^2$ is convergent. Prove that $\int_0^\infty f'^2$ is convergent. I can prove the case that $f$ and $f''$ is monotonic. In this case $f \rightarrow 0$ and $f'' \rightarrow 0$ when $x \rightarrow +\infty$ . Therefore $$\int f'^2 \mathrm{d}x = \int f' \mathrm{d}f = f'f|_0^\infty - \int ff''\mathrm{d}x$$ and $$\int ff'' \le (\int f^2 )^{\frac{1}{2}} (\int f''^2)^{\frac{1}{2}}$$ in convergent, so is $\int f'^2$ . But I don't know how to do in the general case.","is second order differentiable in . And and is convergent. Prove that is convergent. I can prove the case that and is monotonic. In this case and when . Therefore and in convergent, so is . But I don't know how to do in the general case.","f [0,+\infty) \int_0^\infty f^2 \int_0^\infty f''^2 \int_0^\infty f'^2 f f'' f \rightarrow 0 f'' \rightarrow 0 x \rightarrow +\infty \int f'^2 \mathrm{d}x = \int f' \mathrm{d}f = f'f|_0^\infty - \int ff''\mathrm{d}x \int ff'' \le (\int f^2 )^{\frac{1}{2}} (\int f''^2)^{\frac{1}{2}} \int f'^2","['analysis', 'inequality']"
9,Proving that $b-a\ge \pi $,Proving that,b-a\ge \pi ,"Let $f : C^1(a; b)$, such that $ \lim\limits_{x\to a^+} f(x) = +\infty$,  $\lim\limits_{x\to b^-}f(x) = -\infty$ and  $f'(x)+f^2(x) \ge -1 $ for $x \in  (a; b)$. Prove that $b-a\ge \pi $ and provide an example   where $b-a= \pi $ For the second question the obvious example could be $f(x) = \cot(x)$ with $a=0$, and $b=\pi.$ Any hint for the first part? This queston is similar to this: How prove this inequality $b-a\ge \pi$","Let $f : C^1(a; b)$, such that $ \lim\limits_{x\to a^+} f(x) = +\infty$,  $\lim\limits_{x\to b^-}f(x) = -\infty$ and  $f'(x)+f^2(x) \ge -1 $ for $x \in  (a; b)$. Prove that $b-a\ge \pi $ and provide an example   where $b-a= \pi $ For the second question the obvious example could be $f(x) = \cot(x)$ with $a=0$, and $b=\pi.$ Any hint for the first part? This queston is similar to this: How prove this inequality $b-a\ge \pi$",,"['calculus', 'real-analysis', 'analysis', 'functions', 'contest-math']"
10,Proof that $|\sin(nx)| \le n|\sin(x)|$ [duplicate],Proof that  [duplicate],|\sin(nx)| \le n|\sin(x)|,"This question already has an answer here : How to prove by induction that $|\sin(nx)| \leq n|\sin x|$? (1 answer) Closed 4 years ago . I have to show that for every $x \in \mathbb{R}$ and every $n \in \mathbb{N}$ $$|\sin(nx)| \le n|\sin(x)|  $$ In the previous exercise, I have showed that $|\sin(x)|≤|x|$ with the use of the mean value theorem. I think that I cannot use this approach this time. I also tried to write $\sin(nx)$ as a series expansion but that doesn't work either. Does anyone know how I can solve this?","This question already has an answer here : How to prove by induction that $|\sin(nx)| \leq n|\sin x|$? (1 answer) Closed 4 years ago . I have to show that for every $x \in \mathbb{R}$ and every $n \in \mathbb{N}$ $$|\sin(nx)| \le n|\sin(x)|  $$ In the previous exercise, I have showed that $|\sin(x)|≤|x|$ with the use of the mean value theorem. I think that I cannot use this approach this time. I also tried to write $\sin(nx)$ as a series expansion but that doesn't work either. Does anyone know how I can solve this?",,"['real-analysis', 'analysis', 'functions', 'trigonometry']"
11,Prove $2^e$ is transcendental,Prove  is transcendental,2^e,"I remembered that a Russian mathematician proved this, but now I cannot find it again. Could you please recommend a book on these kinds of problems?","I remembered that a Russian mathematician proved this, but now I cannot find it again. Could you please recommend a book on these kinds of problems?",,"['real-analysis', 'analysis', 'proof-writing', 'irrational-numbers', 'transcendental-numbers']"
12,"Why $\lim\limits_{(x,y)\to(0,0)}\frac{\sin(x^2+y^2)}{x^2+y^2}=\lim\limits_{t\to 0}\frac{\sin t}{t}$?",Why ?,"\lim\limits_{(x,y)\to(0,0)}\frac{\sin(x^2+y^2)}{x^2+y^2}=\lim\limits_{t\to 0}\frac{\sin t}{t}","Why $\displaystyle\lim_{(x,y)\to(0,0)}\frac{\sin(x^2+y^2)}{x^2+y^2}=\lim_{t\to 0}\frac{\sin t}{t}$( and hence equals to $1$)? Any rigorous reason? (i.e. not just say by letting $t=x^2+y^2$.)","Why $\displaystyle\lim_{(x,y)\to(0,0)}\frac{\sin(x^2+y^2)}{x^2+y^2}=\lim_{t\to 0}\frac{\sin t}{t}$( and hence equals to $1$)? Any rigorous reason? (i.e. not just say by letting $t=x^2+y^2$.)",,['analysis']
13,Nonexistence of a continuous injection $f:S^2 \rightarrow \mathbb{R^2}$,Nonexistence of a continuous injection,f:S^2 \rightarrow \mathbb{R^2},"What is the ""easiest"" way to show that there is no continuous injection $f:S^2 \rightarrow \mathbb{R^2}$? Sure the Borsuk-Ulam theorem implies that result, but this may be a ""difficult"" way.","What is the ""easiest"" way to show that there is no continuous injection $f:S^2 \rightarrow \mathbb{R^2}$? Sure the Borsuk-Ulam theorem implies that result, but this may be a ""difficult"" way.",,"['analysis', 'algebraic-topology', 'differential-topology']"
14,Norms on $\mathcal{P}_N$ Vector Space of Polynomials up to Order N,Norms on  Vector Space of Polynomials up to Order N,\mathcal{P}_N,"$\|p\|_\infty :=\sup_{x\in [0,1]}|p(x)|$ and $\|p\|_{L^1}:=\int_0^1 |p(x)| dx$ . As the space of real-valued polynomials on $[0,1]$ up to order $N$ is a $N+1$ dimensional vector space and $\|\cdot\|_{\infty}$ and $\|\cdot\|_{L^1}$ are norms for it, there is a constant $C_N$ s.t. $\|\cdot\|_\infty \leq C_N \|\cdot\|_{L^1}$ . Is there an easy way how to compute this $C_N$ ?","and . As the space of real-valued polynomials on up to order is a dimensional vector space and and are norms for it, there is a constant s.t. . Is there an easy way how to compute this ?","\|p\|_\infty :=\sup_{x\in [0,1]}|p(x)| \|p\|_{L^1}:=\int_0^1 |p(x)| dx [0,1] N N+1 \|\cdot\|_{\infty} \|\cdot\|_{L^1} C_N \|\cdot\|_\infty \leq C_N \|\cdot\|_{L^1} C_N","['real-analysis', 'analysis', 'polynomials', 'vector-spaces', 'normed-spaces']"
15,"Prob. 19, Chap. 1, in Baby Rudin: For what $\mathbf{c}$ and $r > 0$ does this equivalence hold?","Prob. 19, Chap. 1, in Baby Rudin: For what  and  does this equivalence hold?",\mathbf{c} r > 0,"Here's Prob. 19 in Chap. 1 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\mathbf{a} \in \mathbb{R}^k$, $\mathbf{b} \in \mathbb{R}^k$. Find $\mathbf{c} \in \mathbb{R}^k$ and $r > 0$ such that, for all $\mathbf{x} \in \mathbb{R}^k$, we have  $$\vert \mathbf{x} - \mathbf{a} \vert = 2 \vert \mathbf{x} - \mathbf{b} \vert$$ if and only if $$\vert \mathbf{x} - \mathbf{c} \vert = r.$$ Although Rudin has given a solution, namely $3\mathbf{c} = 4 \mathbf{b} - \mathbf{a}$, $3r =  2\vert \mathbf{b} - \mathbf{a} \vert$, I'm wondering  how he's obtained it. How to attack this type of a problem? Is this problem part of the exercises by some well-thought-out design? I mean is it going to be used later on in the book? Or, is it just to give the reader some practice?","Here's Prob. 19 in Chap. 1 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\mathbf{a} \in \mathbb{R}^k$, $\mathbf{b} \in \mathbb{R}^k$. Find $\mathbf{c} \in \mathbb{R}^k$ and $r > 0$ such that, for all $\mathbf{x} \in \mathbb{R}^k$, we have  $$\vert \mathbf{x} - \mathbf{a} \vert = 2 \vert \mathbf{x} - \mathbf{b} \vert$$ if and only if $$\vert \mathbf{x} - \mathbf{c} \vert = r.$$ Although Rudin has given a solution, namely $3\mathbf{c} = 4 \mathbf{b} - \mathbf{a}$, $3r =  2\vert \mathbf{b} - \mathbf{a} \vert$, I'm wondering  how he's obtained it. How to attack this type of a problem? Is this problem part of the exercises by some well-thought-out design? I mean is it going to be used later on in the book? Or, is it just to give the reader some practice?",,"['real-analysis', 'analysis', 'analytic-geometry']"
16,"Let $A,B\subseteq \mathbb{R}$ such that $\forall b\in B ~ \exists a_n$ where $a_n \in A$ and $a_n\to b$ as $n\to\infty$. Prove $\sup(B)\leq \sup(A)$",Let  such that  where  and  as . Prove,"A,B\subseteq \mathbb{R} \forall b\in B ~ \exists a_n a_n \in A a_n\to b n\to\infty \sup(B)\leq \sup(A)","Question: Let A and B be two non-emty bounded sub-sets of $\mathbb{R}$   with the following proposition: $\forall_{b\in B}\exists_{(a_n)}$.   $(a_n)$ is made up of elements of A such that $a_n\to b$ as   $n\to\infty$ I don't know what to do with the sequence part. To be proven: $\sup(B)\leq \sup(A)$. My attempt: Suppose $a\in A$ and $b\in B$. Then $a\leq sup(A)$ and $b\leq sup(B)$ If $b<a$ then $b\leq \sup(B)\leq a\leq \sup(A)$ Suppose $\sup(B)>\sup(A)$. Then $\exists_{a\in A}$ such that $a>\sup(A)-\frac{\epsilon}{2}$. And $\exists_{b\in B}$ such that $b>\sup(B)-\frac{\epsilon}{2}$. But if $\sup(B)>\sup(A)$, then $b>a$. But $b<a$. So contradiction. So $\sup(B)\leq \sup(A)$. Hope someone will take a look, thanks in advance!","Question: Let A and B be two non-emty bounded sub-sets of $\mathbb{R}$   with the following proposition: $\forall_{b\in B}\exists_{(a_n)}$.   $(a_n)$ is made up of elements of A such that $a_n\to b$ as   $n\to\infty$ I don't know what to do with the sequence part. To be proven: $\sup(B)\leq \sup(A)$. My attempt: Suppose $a\in A$ and $b\in B$. Then $a\leq sup(A)$ and $b\leq sup(B)$ If $b<a$ then $b\leq \sup(B)\leq a\leq \sup(A)$ Suppose $\sup(B)>\sup(A)$. Then $\exists_{a\in A}$ such that $a>\sup(A)-\frac{\epsilon}{2}$. And $\exists_{b\in B}$ such that $b>\sup(B)-\frac{\epsilon}{2}$. But if $\sup(B)>\sup(A)$, then $b>a$. But $b<a$. So contradiction. So $\sup(B)\leq \sup(A)$. Hope someone will take a look, thanks in advance!",,"['real-analysis', 'analysis', 'proof-verification', 'supremum-and-infimum']"
17,Convergence of the integral $\int_0^{\pi/2}\ln(\cos(x))dx$,Convergence of the integral,\int_0^{\pi/2}\ln(\cos(x))dx,"I want to Show that whether the integral $$\int\limits_0^{\pi/2}\ln(\cos(x))dx$$ is convergent ot not. My Approach: Let $y=cos(x)$, then the above integral reduces to $$\int\limits_0^{1}\frac{\ln(y)}{\sqrt{1-y^2}}dy.$$ At this step since $\ln(y)<<y^p$, $p=1,2,...$, I compare above integral from above with the integral$$\int\limits_0^{1}\frac{y}{\sqrt{1-y^2}}dy,$$ which is convergent. Hence by comprasion I obtain that the integral $\int\limits_0^{\pi/2}\ln(\cos(x))dx$ is convergent. My Question: (1) Is my  approach true? (2) Can you suggest any different aproach? Thanks in advance...","I want to Show that whether the integral $$\int\limits_0^{\pi/2}\ln(\cos(x))dx$$ is convergent ot not. My Approach: Let $y=cos(x)$, then the above integral reduces to $$\int\limits_0^{1}\frac{\ln(y)}{\sqrt{1-y^2}}dy.$$ At this step since $\ln(y)<<y^p$, $p=1,2,...$, I compare above integral from above with the integral$$\int\limits_0^{1}\frac{y}{\sqrt{1-y^2}}dy,$$ which is convergent. Hence by comprasion I obtain that the integral $\int\limits_0^{\pi/2}\ln(\cos(x))dx$ is convergent. My Question: (1) Is my  approach true? (2) Can you suggest any different aproach? Thanks in advance...",,"['calculus', 'real-analysis', 'analysis']"
18,What is the relationship between the Archimedean Property and Calculus?,What is the relationship between the Archimedean Property and Calculus?,,"My textbook begin with a chapter dedicaded to the Real Numbers. It introduces firstly integers and rational numbers, then it introduces the irrational numbers, which appeared in the first place as a necessity of expressing the length of incommesurable line segments. To my understanding, all of this helps to introduce the notion of continuity of a variable, which is put into work when treating the continuity of a variable dependent of another variable (if we restrict ourselves to single-valued functions). The Archimedean Property is stated as follows (or as it is written in my textbook): Given any number $c > 0$, there exists  a natural number $n$, such that $n > c$. Given any positive number $\epsilon$, there always exists a natural   number $n$ such that the inequality  $1/n < \epsilon$ is fulfilled. The last statement is the first one with $c = 1/\epsilon$. Although I understand what is meant by it (I think it can readily be proved by contradiction), what I'm not getting is how the Archimedean Property fits in the preparatory material preceding that dealing with supremum, infimum and then with continuity and limits. Does it have something to do with the concept of limit, perhaps?","My textbook begin with a chapter dedicaded to the Real Numbers. It introduces firstly integers and rational numbers, then it introduces the irrational numbers, which appeared in the first place as a necessity of expressing the length of incommesurable line segments. To my understanding, all of this helps to introduce the notion of continuity of a variable, which is put into work when treating the continuity of a variable dependent of another variable (if we restrict ourselves to single-valued functions). The Archimedean Property is stated as follows (or as it is written in my textbook): Given any number $c > 0$, there exists  a natural number $n$, such that $n > c$. Given any positive number $\epsilon$, there always exists a natural   number $n$ such that the inequality  $1/n < \epsilon$ is fulfilled. The last statement is the first one with $c = 1/\epsilon$. Although I understand what is meant by it (I think it can readily be proved by contradiction), what I'm not getting is how the Archimedean Property fits in the preparatory material preceding that dealing with supremum, infimum and then with continuity and limits. Does it have something to do with the concept of limit, perhaps?",,"['analysis', 'soft-question']"
19,Are there numbers in R which cannot be expressed as a finite formula (including limits)?,Are there numbers in R which cannot be expressed as a finite formula (including limits)?,,"A fellow student just claimed that there are numbers in $\mathbb{R}$ which cannot be expressed as a limit. I don't think that is true, but first some definitions / examples: Definition : A set $S$ is countable if there exists an injective function $f:S \rightarrow \mathbb{N}$. Examples : $\{1,2,3,4\}, \mathbb{N},\mathbb{Z}, \mathbb{Q}$ are all countable. $\mathbb{R}$ is not countable. Now we can define a formal Grammar $(V, \Sigma, P, S)$ with: The terminal symbols $\Sigma = \{0, 1,2,3,4,5,6,7,8,9,.,+,-,\cdot,:,(,), \lim, n, \mathrm{pow}, !, \sum, \prod\}$ The vocabulary: $V = \Sigma \cup \{S, Z, N, \mathrm{var}\}$ The production rules $P \subseteq (V^* \setminus \Sigma^*) \times V^*$ (they follow) The start symbols $S$ The set of production rules is $S \rightarrow Z\mid\lim_{\mathrm{var}\rightarrow \infty} S \mid  S+S\mid S-S\mid S \cdot S\mid S:S\mid (S)\mid S^S\mid S!\mid \sum_{\mathrm{var}=N}^\infty S\mid \prod_{\mathrm{var}=N}^\infty S$ $Z \rightarrow ZZ\mid N.N\mid -N.N$ $N \rightarrow NN\mid 0\mid 1\mid 2\mid 3\mid 4\mid 5\mid 6\mid 7\mid 8\mid 9$ $\mathrm{var}\rightarrow n_N$ You can build some invalid expressions with this grammar (like $\frac{0}{0}$), but I think you can express every number which can be written as a finite forumula / a limit of a finite formula limit with it (can you?). Sanity check $\mathbb{Q} \in \mathcal{L}(V, \Sigma, P, S)$ $e^x = \lim_{n_1 \rightarrow \infty} (1+\frac{x}{n_1})^{n_1}$ (where $x$ is any number) $\in \mathcal{L}(V, \Sigma, P, S)$ $\pi = \sum_{n_1 = 0}^\infty \frac{(-1)^{n_1}}{2\cdot n_1 + 1}\in \mathcal{L}(V, \Sigma, P, S)$ Question Now my problem is that this grammar suggests that you can enumerate each limit with it by using breadth first search. This means the language which is created by this grammar is countable. This either means (1) that I forgot some important numbers which cannot be expressed that way or (2) that there are numbers in $\mathbb{R}$ which cannot be expressed with a finite ""formula"" and my grammar cannot be extended so that it would work. Now I want to know if (1) or (2) is the case. If (1) is the case: What are characteristics of those numbers? What is their name? Obviously they cannot be in $\mathbb{Q}$ and there have to be at least countable infinite many of this kind as you can add numbers of $\mathbb{Q}$ to them, so if (1) is the case those numbers have to have some interesting properties If (2) is the case: Which group did I forget? Which numbers cannot be expressed this way?","A fellow student just claimed that there are numbers in $\mathbb{R}$ which cannot be expressed as a limit. I don't think that is true, but first some definitions / examples: Definition : A set $S$ is countable if there exists an injective function $f:S \rightarrow \mathbb{N}$. Examples : $\{1,2,3,4\}, \mathbb{N},\mathbb{Z}, \mathbb{Q}$ are all countable. $\mathbb{R}$ is not countable. Now we can define a formal Grammar $(V, \Sigma, P, S)$ with: The terminal symbols $\Sigma = \{0, 1,2,3,4,5,6,7,8,9,.,+,-,\cdot,:,(,), \lim, n, \mathrm{pow}, !, \sum, \prod\}$ The vocabulary: $V = \Sigma \cup \{S, Z, N, \mathrm{var}\}$ The production rules $P \subseteq (V^* \setminus \Sigma^*) \times V^*$ (they follow) The start symbols $S$ The set of production rules is $S \rightarrow Z\mid\lim_{\mathrm{var}\rightarrow \infty} S \mid  S+S\mid S-S\mid S \cdot S\mid S:S\mid (S)\mid S^S\mid S!\mid \sum_{\mathrm{var}=N}^\infty S\mid \prod_{\mathrm{var}=N}^\infty S$ $Z \rightarrow ZZ\mid N.N\mid -N.N$ $N \rightarrow NN\mid 0\mid 1\mid 2\mid 3\mid 4\mid 5\mid 6\mid 7\mid 8\mid 9$ $\mathrm{var}\rightarrow n_N$ You can build some invalid expressions with this grammar (like $\frac{0}{0}$), but I think you can express every number which can be written as a finite forumula / a limit of a finite formula limit with it (can you?). Sanity check $\mathbb{Q} \in \mathcal{L}(V, \Sigma, P, S)$ $e^x = \lim_{n_1 \rightarrow \infty} (1+\frac{x}{n_1})^{n_1}$ (where $x$ is any number) $\in \mathcal{L}(V, \Sigma, P, S)$ $\pi = \sum_{n_1 = 0}^\infty \frac{(-1)^{n_1}}{2\cdot n_1 + 1}\in \mathcal{L}(V, \Sigma, P, S)$ Question Now my problem is that this grammar suggests that you can enumerate each limit with it by using breadth first search. This means the language which is created by this grammar is countable. This either means (1) that I forgot some important numbers which cannot be expressed that way or (2) that there are numbers in $\mathbb{R}$ which cannot be expressed with a finite ""formula"" and my grammar cannot be extended so that it would work. Now I want to know if (1) or (2) is the case. If (1) is the case: What are characteristics of those numbers? What is their name? Obviously they cannot be in $\mathbb{Q}$ and there have to be at least countable infinite many of this kind as you can add numbers of $\mathbb{Q}$ to them, so if (1) is the case those numbers have to have some interesting properties If (2) is the case: Which group did I forget? Which numbers cannot be expressed this way?",,"['analysis', 'elementary-set-theory']"
20,Infinite Product computation,Infinite Product computation,,How can we compute the infinite product: Do we need Gamma function? Edited: I forgot to add 1/e factor so the product converges. The product becomes,How can we compute the infinite product: Do we need Gamma function? Edited: I forgot to add 1/e factor so the product converges. The product becomes,,"['calculus', 'analysis']"
21,Poisson complete statistic,Poisson complete statistic,,"I have the same question as this thread, but I cannot understand the proof. The problem is, given $f(\lambda)=\sum_{k=0}^\infty g(k)\frac{(n\lambda)^k}{k!}=0,\forall\lambda>0$. How to show $g(k)\equiv0$? The accepted answer in that thread claims that $f(0)=g(0)$. Why? I believe $f(0)=0$. But I cannot see why $g(0)=0=f(0)$. One comment claims ""an infinite summation is zero iff each term in it is identically zero"". But there is no proof. So, can anyone prove $g(k)\equiv0$ or the claim ""an infinite summation is zero iff each term in it is identically zero""? Thanks!","I have the same question as this thread, but I cannot understand the proof. The problem is, given $f(\lambda)=\sum_{k=0}^\infty g(k)\frac{(n\lambda)^k}{k!}=0,\forall\lambda>0$. How to show $g(k)\equiv0$? The accepted answer in that thread claims that $f(0)=g(0)$. Why? I believe $f(0)=0$. But I cannot see why $g(0)=0=f(0)$. One comment claims ""an infinite summation is zero iff each term in it is identically zero"". But there is no proof. So, can anyone prove $g(k)\equiv0$ or the claim ""an infinite summation is zero iff each term in it is identically zero""? Thanks!",,"['analysis', 'statistics', 'taylor-expansion', 'poisson-distribution']"
22,"Does $\int_{0}^{1}x^nf(x)\, dx=0$ imply that $f=0$ a.e. without assuming $f \in C[0,1]$?",Does  imply that  a.e. without assuming ?,"\int_{0}^{1}x^nf(x)\, dx=0 f=0 f \in C[0,1]","Suppose that $f \in L^{1}[0,1]$ and $\int_{0}^{1}x^nf(x)\, dx=0$ for $n=0,1,2,\dots$ Does that imply that $f=0$ a.e.? I think that there will be a counterexample but it is hard to find out.","Suppose that $f \in L^{1}[0,1]$ and $\int_{0}^{1}x^nf(x)\, dx=0$ for $n=0,1,2,\dots$ Does that imply that $f=0$ a.e.? I think that there will be a counterexample but it is hard to find out.",,"['real-analysis', 'analysis']"
23,The product of uniformly continuous functions is not necessarily uniformly continuous,The product of uniformly continuous functions is not necessarily uniformly continuous,,"I was asked to show that given two functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ which are both uniformly continuous, to show that the product $fg:\mathbb{R}\rightarrow \mathbb{R}$ was not always necessarily uniformly continuous. Rather than just give a counter example, I wanted to try showing it directly assuming that it was always uniformly continuous and see where the proof gets hairy or where it seems to fail. Only problem is that I seemed to have accidentally shown myself that it is always true, so I wanted to show everyone so you could show me where I went wrong! Proof Let $\{u_n\}$ and $\{v_n\}$ be sequences in $\mathbb{R}$ such that $\lim_{n \rightarrow \infty } [u_{n} - v_{n}]=0$ If we apply the function to our sequences, then we have $\lim_{n \rightarrow \infty } [(fg)(u_{n}) - (fg)(v_{n})]$ $\lim_{n \rightarrow \infty } [f(u_{n})g(u_{n}) - f(v_{n})g(v_{n})]$ But since f and g were uniformly continuous, then limit of f(u) = f(v) and limit g(u) =g(v) So if we let the f's converge to a, and the g's converge to b, then it would seem that using the product rule for limits, we would wind up with ab - ab  which indeed equals zero, and would meet our criterion for uniform continuity. My guess is that I went wrong because I assumed that $fg(u_n) = f(u_n)g(u_n)$","I was asked to show that given two functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ which are both uniformly continuous, to show that the product $fg:\mathbb{R}\rightarrow \mathbb{R}$ was not always necessarily uniformly continuous. Rather than just give a counter example, I wanted to try showing it directly assuming that it was always uniformly continuous and see where the proof gets hairy or where it seems to fail. Only problem is that I seemed to have accidentally shown myself that it is always true, so I wanted to show everyone so you could show me where I went wrong! Proof Let $\{u_n\}$ and $\{v_n\}$ be sequences in $\mathbb{R}$ such that $\lim_{n \rightarrow \infty } [u_{n} - v_{n}]=0$ If we apply the function to our sequences, then we have $\lim_{n \rightarrow \infty } [(fg)(u_{n}) - (fg)(v_{n})]$ $\lim_{n \rightarrow \infty } [f(u_{n})g(u_{n}) - f(v_{n})g(v_{n})]$ But since f and g were uniformly continuous, then limit of f(u) = f(v) and limit g(u) =g(v) So if we let the f's converge to a, and the g's converge to b, then it would seem that using the product rule for limits, we would wind up with ab - ab  which indeed equals zero, and would meet our criterion for uniform continuity. My guess is that I went wrong because I assumed that $fg(u_n) = f(u_n)g(u_n)$",,"['real-analysis', 'analysis', 'proof-verification', 'continuity', 'uniform-continuity']"
24,Is the range of an injective function dense somewhere?,Is the range of an injective function dense somewhere?,,"Consider an injective function $\,\,f:[0,1]\rightarrow[0,1].\,$ Then is it true that there always exists some non-empty open subinterval of $[0,1]$, such that $f([0,1])$ is dense in that subinterval? That is to say, do there exist $a$ and $b$ such that $a<b$ and for any $c$, $d$ in $(a,b)$, there exists a $y$ in $(c,d)$ such that $y=f(x)$ for some $x$? I'd be much obliged if someone could give me ideas on how to go on about proving this or maybe provide a counter-example since I don't even know whether it's true or not.","Consider an injective function $\,\,f:[0,1]\rightarrow[0,1].\,$ Then is it true that there always exists some non-empty open subinterval of $[0,1]$, such that $f([0,1])$ is dense in that subinterval? That is to say, do there exist $a$ and $b$ such that $a<b$ and for any $c$, $d$ in $(a,b)$, there exists a $y$ in $(c,d)$ such that $y=f(x)$ for some $x$? I'd be much obliged if someone could give me ideas on how to go on about proving this or maybe provide a counter-example since I don't even know whether it's true or not.",,"['calculus', 'real-analysis', 'analysis', 'functions', 'real-numbers']"
25,A continuously differentiable function with vanishing determinant is non-injective?,A continuously differentiable function with vanishing determinant is non-injective?,,"(This question relates to my incomplete answer at https://math.stackexchange.com/a/892212/168832 .) Is the following true (for all n)? ""If $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is continuously differentiable and satisfies $\det(f'(x)) = 0$ for all $x$, then $f$ is not injective."" If so, what's the most elementary proof you can think of? It is clearly true for $n=1$. In lieu of a proof for the general case, I'll accept answers for other small $n$. I have a simple intuitive argument: pick a path in $\mathbb R^n$ such that at each point of the path, it points along some vector in the kernel of $Df$ at that point. (Remember, $\det(f'(x)) = 0$ for all $x$.) Now take the integral of the directional derivative (along the curve) of $f$ over the curve. It describes a difference between two values in the range of $f$, and it should come out to zero (QED). I have a problem showing that such a path exists and is suitable for the purpose described. Note: ""elementary"" means stuff that comes before chapter 3 in Spivak's ""Calculus on Manifolds"". However, note also that Spivak seems to assume that integrals and elementary facts about functions in one variable are available to the reader . Here's a list of things that were not covered in Spivak at the point where the problem came up: constant rank theorem, implicit function theorem. The inverse function theorem was introduced in the same chapter as the problem was given, so that would be ok to use. Note: This is not quite the initial problem from Spivak, and does not necessarily need to be proved to solve the original problem (see the link).","(This question relates to my incomplete answer at https://math.stackexchange.com/a/892212/168832 .) Is the following true (for all n)? ""If $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is continuously differentiable and satisfies $\det(f'(x)) = 0$ for all $x$, then $f$ is not injective."" If so, what's the most elementary proof you can think of? It is clearly true for $n=1$. In lieu of a proof for the general case, I'll accept answers for other small $n$. I have a simple intuitive argument: pick a path in $\mathbb R^n$ such that at each point of the path, it points along some vector in the kernel of $Df$ at that point. (Remember, $\det(f'(x)) = 0$ for all $x$.) Now take the integral of the directional derivative (along the curve) of $f$ over the curve. It describes a difference between two values in the range of $f$, and it should come out to zero (QED). I have a problem showing that such a path exists and is suitable for the purpose described. Note: ""elementary"" means stuff that comes before chapter 3 in Spivak's ""Calculus on Manifolds"". However, note also that Spivak seems to assume that integrals and elementary facts about functions in one variable are available to the reader . Here's a list of things that were not covered in Spivak at the point where the problem came up: constant rank theorem, implicit function theorem. The inverse function theorem was introduced in the same chapter as the problem was given, so that would be ok to use. Note: This is not quite the initial problem from Spivak, and does not necessarily need to be proved to solve the original problem (see the link).",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
26,"$A \subset \mathbb{R} $ is measurable, prove that $-A=\{x : -x \in A\}$ is measurable.","is measurable, prove that  is measurable.",A \subset \mathbb{R}  -A=\{x : -x \in A\},"$A \subset \mathbb{R} $ is measurable, prove that $-A=\{x : -x \in A\}$ is measurable. It is more than obvious that $-A$ is measurable, but I am sure that I am not supposed to say :""$-A$ is just $A$ displaced on the real line"". My Question is : if I show that for each $\epsilon > 0$ there exists an open set $O$ containing $-A$ such that $m(O \setminus -A) < \epsilon$, does this imply that $-A$ is measurable ?","$A \subset \mathbb{R} $ is measurable, prove that $-A=\{x : -x \in A\}$ is measurable. It is more than obvious that $-A$ is measurable, but I am sure that I am not supposed to say :""$-A$ is just $A$ displaced on the real line"". My Question is : if I show that for each $\epsilon > 0$ there exists an open set $O$ containing $-A$ such that $m(O \setminus -A) < \epsilon$, does this imply that $-A$ is measurable ?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
27,Give an example of two $\sigma$ algebras whose union is not an algebra,Give an example of two  algebras whose union is not an algebra,\sigma,"Give an example of two $\sigma$ algebras in a set $X$ whose union is not an algebra. I've considered the sets $\{A|\text{A is countable or $A^c$ is countable}\}\subset2^\mathbb{R}$, which is a $\sigma$ algebra. I also tried to generated a $\sigma$ algebra from a collection of $\sigma$ algebra, but I've been unfruitful. So far I know $2^X,\{\emptyset, X\},$ and the measurable sets $\mathcal{L}$ are $\sigma$ algebras, but they haven't helped me too much. Thank you!","Give an example of two $\sigma$ algebras in a set $X$ whose union is not an algebra. I've considered the sets $\{A|\text{A is countable or $A^c$ is countable}\}\subset2^\mathbb{R}$, which is a $\sigma$ algebra. I also tried to generated a $\sigma$ algebra from a collection of $\sigma$ algebra, but I've been unfruitful. So far I know $2^X,\{\emptyset, X\},$ and the measurable sets $\mathcal{L}$ are $\sigma$ algebras, but they haven't helped me too much. Thank you!",,"['real-analysis', 'analysis', 'measure-theory', 'elementary-set-theory']"
28,Understanding the proof for: $d(f^*\omega)\overset{!}{=}f^*(d\omega)$,Understanding the proof for:,d(f^*\omega)\overset{!}{=}f^*(d\omega),"Consider this Proposition: Let $U\subset\mathbb{R}^n$ and $V\subset\mathbb{R}^n$ be open sets and $\phi:U\to V$ be differentiable. For all $k\in\mathbb{N}_0$ and $\omega\in \Lambda^k(V)$ it is true that $$d(\phi^*\omega)=\phi^*(d\omega)$$ I am trying to understand its prove. But there are some steps I do not understand. Here are the first lines of the Proof: At first let $f\in \mathcal{C}^\infty (V)$ be a differentialform of degree $0$. Then $\phi^*(f)=f\circ\phi$. Hence \begin{eqnarray*} d(\phi^*(f)) &=&d(f\circ\phi)\\ &=&\sum_{j=1}^{n}\frac{\partial(f\circ\phi)}{\partial x_j}dx_j\\ &\overset{?}{=}&\sum_{i,j}\frac{\partial f}{\partial y_i}\circ\phi(\frac{\partial (\phi_i)}{\partial x_j}dx_j)\\ &=&\sum_{i=1}^{m}\frac{\partial f\circ\phi}{\partial y_i}d\phi_i\\ \end{eqnarray*} I marked the position I don't understand with a question mark. What exactly happens here?","Consider this Proposition: Let $U\subset\mathbb{R}^n$ and $V\subset\mathbb{R}^n$ be open sets and $\phi:U\to V$ be differentiable. For all $k\in\mathbb{N}_0$ and $\omega\in \Lambda^k(V)$ it is true that $$d(\phi^*\omega)=\phi^*(d\omega)$$ I am trying to understand its prove. But there are some steps I do not understand. Here are the first lines of the Proof: At first let $f\in \mathcal{C}^\infty (V)$ be a differentialform of degree $0$. Then $\phi^*(f)=f\circ\phi$. Hence \begin{eqnarray*} d(\phi^*(f)) &=&d(f\circ\phi)\\ &=&\sum_{j=1}^{n}\frac{\partial(f\circ\phi)}{\partial x_j}dx_j\\ &\overset{?}{=}&\sum_{i,j}\frac{\partial f}{\partial y_i}\circ\phi(\frac{\partial (\phi_i)}{\partial x_j}dx_j)\\ &=&\sum_{i=1}^{m}\frac{\partial f\circ\phi}{\partial y_i}d\phi_i\\ \end{eqnarray*} I marked the position I don't understand with a question mark. What exactly happens here?",,"['analysis', 'differential-geometry', 'manifolds']"
29,Showing the Cantor function is not Lipschitz.,Showing the Cantor function is not Lipschitz.,,This is one I am having a lot of difficulty with.  I'm not sure how to show that the Cantor function (or 'Devil's Staircase) is not Lipschitz.,This is one I am having a lot of difficulty with.  I'm not sure how to show that the Cantor function (or 'Devil's Staircase) is not Lipschitz.,,['analysis']
30,Derivative is a constant,Derivative is a constant,,"A function $f:U\subset\mathbb{R}^n \rightarrow \mathbb{R}^m$, $U$ open, is differentiable in $p \in U$ if there exists a linear transformation $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ such that $f(p+v)=f(p)+T(v)+R(v)$, where $R(v)$ satisfies $lim _{v\rightarrow 0}\dfrac{R(v)}{|v|}=0$, for all $v\in\mathbb{R}^n$ with $p+v\in U$. That said, if $f$ as above is differentiable and $f'(x)=T$, $\forall x\in\mathbb{R}^n$. I need to show that there is an $a\in\mathbb{R}^n$ such that $f(x)=Tx+a$. The problem I'm having is, how do I show that $R(v)=0$ for all $v$?","A function $f:U\subset\mathbb{R}^n \rightarrow \mathbb{R}^m$, $U$ open, is differentiable in $p \in U$ if there exists a linear transformation $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ such that $f(p+v)=f(p)+T(v)+R(v)$, where $R(v)$ satisfies $lim _{v\rightarrow 0}\dfrac{R(v)}{|v|}=0$, for all $v\in\mathbb{R}^n$ with $p+v\in U$. That said, if $f$ as above is differentiable and $f'(x)=T$, $\forall x\in\mathbb{R}^n$. I need to show that there is an $a\in\mathbb{R}^n$ such that $f(x)=Tx+a$. The problem I'm having is, how do I show that $R(v)=0$ for all $v$?",,['analysis']
31,"How I can calculate this partial derivative of $f(a,b)=\int_0^\infty  e^{-ax^3-bx^2}\mathrm dx$?",How I can calculate this partial derivative of ?,"f(a,b)=\int_0^\infty  e^{-ax^3-bx^2}\mathrm dx","My question is: How to prove that the function: $$f(a,b)=\int_0^\infty  e^{-ax^3-bx^2}\mathrm dx$$ is a solution of the differential equation: $$3ab\frac{{{\partial ^2}f}}{{\partial {b^2}}} - 3a\frac{{\partial f}}{{\partial b}} - 2{b^2}\frac{{\partial f}}{{\partial a}} = 1 ?$$ I have reviewed in several sites, but still nothing. :( Thanks in advance :)","My question is: How to prove that the function: $$f(a,b)=\int_0^\infty  e^{-ax^3-bx^2}\mathrm dx$$ is a solution of the differential equation: $$3ab\frac{{{\partial ^2}f}}{{\partial {b^2}}} - 3a\frac{{\partial f}}{{\partial b}} - 2{b^2}\frac{{\partial f}}{{\partial a}} = 1 ?$$ I have reviewed in several sites, but still nothing. :( Thanks in advance :)",,"['analysis', 'multivariable-calculus']"
32,Inequality problem,Inequality problem,,"Prove that if  $$|x-x_0| < \min\left(\frac{\epsilon}{2(|y_0| + 1)}, 1\right)$$ and  $$|y-y_0| < \frac{\epsilon}{2(|x_0| + 1)},$$  then $|xy - x_0y_0| < \epsilon.$ I am doing some problems in Spivak's Calculus on inequalities and came across this problem. Currently I have a sketch of a solution that breaks down the problem into many cases and it is kind of long and messy. I thought maybe someone here could provide a clean and easier solution? If there is a nice solution please tell me a bit behind the thought process (like how you came up with it), instead of giving it as it is.","Prove that if  $$|x-x_0| < \min\left(\frac{\epsilon}{2(|y_0| + 1)}, 1\right)$$ and  $$|y-y_0| < \frac{\epsilon}{2(|x_0| + 1)},$$  then $|xy - x_0y_0| < \epsilon.$ I am doing some problems in Spivak's Calculus on inequalities and came across this problem. Currently I have a sketch of a solution that breaks down the problem into many cases and it is kind of long and messy. I thought maybe someone here could provide a clean and easier solution? If there is a nice solution please tell me a bit behind the thought process (like how you came up with it), instead of giving it as it is.",,"['analysis', 'inequality']"
33,How to find a Newton-like approximation for that function?,How to find a Newton-like approximation for that function?,,"I want to find the complex fixpoint $t=b^t $ for real bases $b> \eta = \exp(\exp(-1))$. added remark: I'm aware that there is a solution using branches of the Lambert-W-function, but I've no Maple/Mathematica and only a rough implementation in Pari/GP for real values. That motivated to try a solution via Newton/Raphson. And to understand and solve such an implementation (which has to deal with derivatives and complex values) is/was then my question here. See also my comment to Fabian's answer below. I seem to have solved it myself for the ""principal branch"" (see my own answer below) but it is still open for the general case of k'th branch. [end of remark] What I have is a function depending on a parameter $ \beta $ giving the auxiliary values $$  u = \frac{\beta}{ \sin(\beta) } *\exp( i * \beta) $$ $$   t=\exp(u) $$ $$   b= f(\beta) = \exp(u/t) = \exp(u * \exp(-u))  $$ By this I can do an approximation given a base $B$ using binary search. I can find the bounds of an interval taking lower and upper-limit beta's $ \beta_l = \epsilon $ and $ \beta_u = \pi-\epsilon $ with small epsilons giving the lower and upper bases $b_l$ and $b_u$ respectively. Then comparing $b_m = f(\beta_m)$ where $ \beta_m = (\beta_l + \beta_u)/2 $ with my given base $B$ I can implement a binary search which approximates $b_m$ to $B$ arbitrarily well and having $\beta_m$ I can reconstruct u and the fixpoint t by the above formula. However, that binary search needs surprisingly many iterations and I thought, possibly a Newton-like method for that approximation would be more efficient. But since I have complex values involved I do not even see the derivative and even less the formula how to involve that derivative in such an approximation-formula and how to apply this finally to actually do the iterations... [update 4] moved my own findings into an own answer (as suggested in meta.***) [update 1] (in this old plot I used the letter s instead of b )","I want to find the complex fixpoint $t=b^t $ for real bases $b> \eta = \exp(\exp(-1))$. added remark: I'm aware that there is a solution using branches of the Lambert-W-function, but I've no Maple/Mathematica and only a rough implementation in Pari/GP for real values. That motivated to try a solution via Newton/Raphson. And to understand and solve such an implementation (which has to deal with derivatives and complex values) is/was then my question here. See also my comment to Fabian's answer below. I seem to have solved it myself for the ""principal branch"" (see my own answer below) but it is still open for the general case of k'th branch. [end of remark] What I have is a function depending on a parameter $ \beta $ giving the auxiliary values $$  u = \frac{\beta}{ \sin(\beta) } *\exp( i * \beta) $$ $$   t=\exp(u) $$ $$   b= f(\beta) = \exp(u/t) = \exp(u * \exp(-u))  $$ By this I can do an approximation given a base $B$ using binary search. I can find the bounds of an interval taking lower and upper-limit beta's $ \beta_l = \epsilon $ and $ \beta_u = \pi-\epsilon $ with small epsilons giving the lower and upper bases $b_l$ and $b_u$ respectively. Then comparing $b_m = f(\beta_m)$ where $ \beta_m = (\beta_l + \beta_u)/2 $ with my given base $B$ I can implement a binary search which approximates $b_m$ to $B$ arbitrarily well and having $\beta_m$ I can reconstruct u and the fixpoint t by the above formula. However, that binary search needs surprisingly many iterations and I thought, possibly a Newton-like method for that approximation would be more efficient. But since I have complex values involved I do not even see the derivative and even less the formula how to involve that derivative in such an approximation-formula and how to apply this finally to actually do the iterations... [update 4] moved my own findings into an own answer (as suggested in meta.***) [update 1] (in this old plot I used the letter s instead of b )",,"['analysis', 'functions', 'approximation', 'exponentiation']"
34,"Proving that if $f$ is continuous on $\mathbb{R}$ and that the image of $f$ is a subset of $\mathbb{R}$\ $\mathbb{Q}$, then $f$ is a constant function","Proving that if  is continuous on  and that the image of  is a subset of \ , then  is a constant function",f \mathbb{R} f \mathbb{R} \mathbb{Q} f,"I was asked to prove that for a function $f$ that is continuous on $\mathbb{R}$ , whose image is a subset of $\Bbb{R}$ \ $\Bbb{Q}$ , $f$ is a constant function. My proof goes as follows: Suppose for a contradiction that f is not constant, so there exists $x_1$ $\neq$ $x_2$ so that $f(x_1)$ $\neq$ $f(x_2)$ , say, $x_1$ , $x_2$ $\in$ $\Bbb{Q}$ . Now, f is continuous at c for every c $\in$ $\Bbb{R}$ , so $f_{[x_1,x_2]}$ :[ $x_1$ , $x_2$ ] $\to$ $\Bbb{R}$ is continuous at c for every c $\in$ [ $x_1$ , $x_2$ ]. Consider $v \in \Bbb{Q}$ and $v \in (f(x_1),f(x_2))$ , which exists by completeness of the set of real numbers. There exists $x_0 \in (x_1,x_2)$ such that $f(x_0)=v \in \Bbb{Q}$ , by the intermediate value theorem. A contradiction, since the image of $f$ is a subset of $\Bbb{R}$ \ $\Bbb{Q}$ . So $f$ is a constant function. $\square$ I feel there is an easier way, to prove this; nevertheless, what do you think of my proof, does it hold ? Thank you!","I was asked to prove that for a function that is continuous on , whose image is a subset of \ , is a constant function. My proof goes as follows: Suppose for a contradiction that f is not constant, so there exists so that , say, , . Now, f is continuous at c for every c , so :[ , ] is continuous at c for every c [ , ]. Consider and , which exists by completeness of the set of real numbers. There exists such that , by the intermediate value theorem. A contradiction, since the image of is a subset of \ . So is a constant function. I feel there is an easier way, to prove this; nevertheless, what do you think of my proof, does it hold ? Thank you!","f \mathbb{R} \Bbb{R} \Bbb{Q} f x_1 \neq x_2 f(x_1) \neq f(x_2) x_1 x_2 \in \Bbb{Q} \in \Bbb{R} f_{[x_1,x_2]} x_1 x_2 \to \Bbb{R} \in x_1 x_2 v \in \Bbb{Q} v \in (f(x_1),f(x_2)) x_0 \in (x_1,x_2) f(x_0)=v \in \Bbb{Q} f \Bbb{R} \Bbb{Q} f \square","['analysis', 'continuity', 'solution-verification']"
35,Strong vs Weak solution to one-dimensional elliptic PDE,Strong vs Weak solution to one-dimensional elliptic PDE,,"Consider the elliptic PDE \begin{align} &-\frac{\mathrm{d}}{\mathrm{d} x} \left(a(x) \frac{\mathrm{d}}{\mathrm{d} x}u(x)\right) = 1, \qquad 0 < x < 1,\\ &u(0) = u(1) = 0. \end{align} Here $a \in L^\infty(0,1) \cap C^0(0,1)^C$ is defined as $$ a(x) = a_1, \text{ if } x \leq 1/2, \quad a(x) = a_2, \text{ if } x > 1/2, $$ where $a_1$ and $a_2$ are positive real scalars. I am having troubles with the difference between a strong and a weak solution of this problem. Since $a$ is not continuous, I would expect $u\in H_0^1(0,1) \cap H^2(0,1)^C$ , so in particular (since we are in dimension one) I would expect $u \in C^0(0,1) \cap C^1(0,1)^C$ . Actually somewhere in between, e.g. Hölder continuous. I tried to solve the equation ""by hand"". In particular, I define $u$ piecewise as $$ u(x) = u_1(x), \text{ if } x \leq 1/2, \quad u(x) = u_2(x), \text{ if } x > 1/2, $$ Then, one can impose that $u_1$ and $u_2$ solve the equation in the strong sense on each side of $x=1/2$ and obtain $$ u_1(x) = -\frac{1}{2a_1} x^2 + C_1x + C_2, \quad u_2(x) = -\frac{1}{2a_2} x^2 + D_1x+D_2. $$ Now we can impose $u_1(0) = 0$ to obtain $C_2 = 0$ , and $u_2(1) = 0$ to obtain $$ D_1 + D_2 = \frac{1}{2a_2}. $$ We expect the equation to be continuous, so $u_1(1/2) = u_2(1/2)$ , which gives the condition $$ \frac{C_1}{2} - \frac{D_1}{2} - D_2 = \frac{1}{8a_1} - \frac{1}{8a_2}. $$ We now have three unknowns ( $C_1$ , $D_1$ , and $D_2$ ), and two conditions. We could impose moreover that $u_1'(1/2) = u_2'(1/2)$ so that the solution $u \in C^1(0,1)$ . This yields the condition $$ C_1 - D_1 = \frac{1}{2a_1} - \frac{1}{2a_2}. $$ The three linear equations above are solvable and thus define a solution $u$ on the whole domain which is $C^1$ . I tried to compare the solution (obtained by solving the linear equation and fixing $a_1 = 0.5$ , and $a_2 = 2$ ) to a FEM solution on $1000$ elements. The FEM solution converges to the weak solution $u \in H_0^1$ such that $$ \int_0^1 a(x) u'(x) v'(x) \, \mathrm{d}x = \int_0^1 v(x) \, \mathrm{d}x, $$ for all $v \in H_0^1(0, 1)$ . The result is in the picture below. The FEM solution (in blue) behaves a bit more ""similarly"" to what I expected. In particular, there is a discontinuity in the derivative at $x = 1/2$ and the solution is ""only"" Hölder continuous. In particular, the FEM solution is different than the strong solution (in red). I think there's something wrong with my reasoning that yields the strong solution, but I cannot see why or where. In particular, since the weak solution exists and is unique for this equation, it should be equal to the strong solution when it exists. I somehow trust more the FEM solver in this case.","Consider the elliptic PDE Here is defined as where and are positive real scalars. I am having troubles with the difference between a strong and a weak solution of this problem. Since is not continuous, I would expect , so in particular (since we are in dimension one) I would expect . Actually somewhere in between, e.g. Hölder continuous. I tried to solve the equation ""by hand"". In particular, I define piecewise as Then, one can impose that and solve the equation in the strong sense on each side of and obtain Now we can impose to obtain , and to obtain We expect the equation to be continuous, so , which gives the condition We now have three unknowns ( , , and ), and two conditions. We could impose moreover that so that the solution . This yields the condition The three linear equations above are solvable and thus define a solution on the whole domain which is . I tried to compare the solution (obtained by solving the linear equation and fixing , and ) to a FEM solution on elements. The FEM solution converges to the weak solution such that for all . The result is in the picture below. The FEM solution (in blue) behaves a bit more ""similarly"" to what I expected. In particular, there is a discontinuity in the derivative at and the solution is ""only"" Hölder continuous. In particular, the FEM solution is different than the strong solution (in red). I think there's something wrong with my reasoning that yields the strong solution, but I cannot see why or where. In particular, since the weak solution exists and is unique for this equation, it should be equal to the strong solution when it exists. I somehow trust more the FEM solver in this case.","\begin{align}
&-\frac{\mathrm{d}}{\mathrm{d} x} \left(a(x) \frac{\mathrm{d}}{\mathrm{d} x}u(x)\right) = 1, \qquad 0 < x < 1,\\
&u(0) = u(1) = 0.
\end{align} a \in L^\infty(0,1) \cap C^0(0,1)^C 
a(x) = a_1, \text{ if } x \leq 1/2, \quad a(x) = a_2, \text{ if } x > 1/2,
 a_1 a_2 a u\in H_0^1(0,1) \cap H^2(0,1)^C u \in C^0(0,1) \cap C^1(0,1)^C u 
u(x) = u_1(x), \text{ if } x \leq 1/2, \quad u(x) = u_2(x), \text{ if } x > 1/2,
 u_1 u_2 x=1/2 
u_1(x) = -\frac{1}{2a_1} x^2 + C_1x + C_2, \quad u_2(x) = -\frac{1}{2a_2} x^2 + D_1x+D_2.
 u_1(0) = 0 C_2 = 0 u_2(1) = 0 
D_1 + D_2 = \frac{1}{2a_2}.
 u_1(1/2) = u_2(1/2) 
\frac{C_1}{2} - \frac{D_1}{2} - D_2 = \frac{1}{8a_1} - \frac{1}{8a_2}.
 C_1 D_1 D_2 u_1'(1/2) = u_2'(1/2) u \in C^1(0,1) 
C_1 - D_1 = \frac{1}{2a_1} - \frac{1}{2a_2}.
 u C^1 a_1 = 0.5 a_2 = 2 1000 u \in H_0^1 
\int_0^1 a(x) u'(x) v'(x) \, \mathrm{d}x = \int_0^1 v(x) \, \mathrm{d}x,
 v \in H_0^1(0, 1) x = 1/2","['analysis', 'partial-differential-equations', 'finite-element-method']"
36,"On the pointwise convergence of $\sum_{n\ge 1} \frac{\sin nx}{n}$ to the sawtooth function in $(-\pi,\pi)$",On the pointwise convergence of  to the sawtooth function in,"\sum_{n\ge 1} \frac{\sin nx}{n} (-\pi,\pi)","My question stems from Exercise $8$ , in Chapter $2$ of Stein and Shakarchi's Fourier Analysis . I have verified that $$\frac{1}{2i} \sum_{n\ne 0} \frac{e^{inx}}{n} = \sum_{n\ge 1} \frac{\sin nx}{n}$$ is the Fourier series of the $2\pi$ -periodic saw-tooth function defined by $f(0) = 0$ , and $$f(x) = \begin{cases} \frac{-\pi-x}{2} & -\pi < x < 0\\ \frac{\pi - x}{2} & 0 < x < \pi\end{cases}$$ The book says: Note that this function is not continuous. Show that nevertheless, the series converges for every $x$ (by which we mean, as usual, that the symmetric partial sums of the series converge). In particular, the value of the series at the origin, namely $0$ , is the average of the values of $f(x)$ as $x$ approaches the origin from the left and the right. By Dirichlet's test for convergence, I have shown that $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges for all $x\in (-\pi,\pi) \setminus\{0\}$ . At $x = 0$ , $\sum_{n\ge 1} \frac{\sin nx}{n}$ clearly converges to $0$ . So, $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges for all $x\in (-\pi,\pi)$ . Question: The series $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges to $f$ , at $0$ . Does it also converge to $f$ at other points in $(-\pi,\pi)$ ? If yes, how can we show this with elementary methods ? Please note that this is only the second chapter of Stein and Shakarchi's Fourier Analysis , so the machinery we can use is limited . In particular, we can't use Dirichlet conditions , etc. Related questions: Post 1 . Edit(s) : Integrating the identity suggested by @mathcounterexamples.net from $0$ to $x$ , we get $$\sum_{n=1}^N \frac{\sin nx}{n} = -\frac{x}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx$$ I have trouble evaluating the last integral, and don't know how to proceed. For $0 < x < \pi$ , we want $$\left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x-\pi}{2}\right| =\left|-\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0$$ and for $-\pi < x < 0$ , we want $$\left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x+\pi}{2}\right| =\left|\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0$$","My question stems from Exercise , in Chapter of Stein and Shakarchi's Fourier Analysis . I have verified that is the Fourier series of the -periodic saw-tooth function defined by , and The book says: Note that this function is not continuous. Show that nevertheless, the series converges for every (by which we mean, as usual, that the symmetric partial sums of the series converge). In particular, the value of the series at the origin, namely , is the average of the values of as approaches the origin from the left and the right. By Dirichlet's test for convergence, I have shown that converges for all . At , clearly converges to . So, converges for all . Question: The series converges to , at . Does it also converge to at other points in ? If yes, how can we show this with elementary methods ? Please note that this is only the second chapter of Stein and Shakarchi's Fourier Analysis , so the machinery we can use is limited . In particular, we can't use Dirichlet conditions , etc. Related questions: Post 1 . Edit(s) : Integrating the identity suggested by @mathcounterexamples.net from to , we get I have trouble evaluating the last integral, and don't know how to proceed. For , we want and for , we want","8 2 \frac{1}{2i} \sum_{n\ne 0} \frac{e^{inx}}{n} = \sum_{n\ge 1} \frac{\sin nx}{n} 2\pi f(0) = 0 f(x) = \begin{cases} \frac{-\pi-x}{2} & -\pi < x < 0\\ \frac{\pi - x}{2} & 0 < x < \pi\end{cases} x 0 f(x) x \sum_{n\ge 1} \frac{\sin nx}{n} x\in (-\pi,\pi) \setminus\{0\} x = 0 \sum_{n\ge 1} \frac{\sin nx}{n} 0 \sum_{n\ge 1} \frac{\sin nx}{n} x\in (-\pi,\pi) \sum_{n\ge 1} \frac{\sin nx}{n} f 0 f (-\pi,\pi) 0 x \sum_{n=1}^N \frac{\sin nx}{n} = -\frac{x}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx 0 < x < \pi \left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x-\pi}{2}\right| =\left|-\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0 -\pi < x < 0 \left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x+\pi}{2}\right| =\left|\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0","['analysis', 'convergence-divergence', 'fourier-analysis', 'fourier-series']"
37,continuous function defined on closed bounded interval is bounded? is the continuous necessary?,continuous function defined on closed bounded interval is bounded? is the continuous necessary?,,"I've seen the proof about this theorem, but I am wondering if this condition continuous is necessary? My gut feels as long as a function is defined on a closed set, which means every element in the domain are defined, will not play around like the open set which can go to infinity. e.g $$f:(0,1)\to \mathbb{R}\\x\to \frac{1}{x}$$ appreciate any comments.","I've seen the proof about this theorem, but I am wondering if this condition continuous is necessary? My gut feels as long as a function is defined on a closed set, which means every element in the domain are defined, will not play around like the open set which can go to infinity. e.g appreciate any comments.","f:(0,1)\to \mathbb{R}\\x\to \frac{1}{x}","['real-analysis', 'analysis', 'functions', 'continuity']"
38,Is there a way to generalize the connected sets in $\mathbb{R}^2$?,Is there a way to generalize the connected sets in ?,\mathbb{R}^2,"How do I generalize the connected sets in $\mathbb{R}^2$ if there is a way. I know how connected sets in $\mathbb{R}$ look like, so is there a way to say something about the connected sets in $\mathbb{R}^2$ ?","How do I generalize the connected sets in if there is a way. I know how connected sets in look like, so is there a way to say something about the connected sets in ?",\mathbb{R}^2 \mathbb{R} \mathbb{R}^2,"['real-analysis', 'analysis', 'connectedness']"
39,About the exactness of $n$-forms,About the exactness of -forms,n,"I have the following statement in my Analysis in $\mathbb{R}^n$ book: If $\omega$ is an $n$ -form on a orientable compact $n$ -manifold $M$ without boundary, then $\omega $ is exact if and only if $\int\limits_{M}\omega=0$ . In the comment on this post , it is mentioned that it is necessary that $M$ be connected. But in my book there is no comment on the need to be connected, can this be due to the definition of manifold? I am particularly interested in the volume form, I have that the volume form cannot be exact, the problem comes from using Stokes, the volume form would be zero (because the boundary is empty), can I still state this if the manifold is disconnected? A manifold of dimension $m$ and class $C^k$ in $\mathbb{R}^n$ is a set $M\subset R^n$ that can be covered by a collection of open $U\subset\mathbb{R}^n$ such that $V=U\cup M$ admits $C^k$ parameterization $\phi: V_0 \to V$ defined in an open $V_0\subset\mathbb{R}^m$ If I don't need connectivity because of my definition, what would be the definition that requires connectivity?","I have the following statement in my Analysis in book: If is an -form on a orientable compact -manifold without boundary, then is exact if and only if . In the comment on this post , it is mentioned that it is necessary that be connected. But in my book there is no comment on the need to be connected, can this be due to the definition of manifold? I am particularly interested in the volume form, I have that the volume form cannot be exact, the problem comes from using Stokes, the volume form would be zero (because the boundary is empty), can I still state this if the manifold is disconnected? A manifold of dimension and class in is a set that can be covered by a collection of open such that admits parameterization defined in an open If I don't need connectivity because of my definition, what would be the definition that requires connectivity?",\mathbb{R}^n \omega n n M \omega  \int\limits_{M}\omega=0 M m C^k \mathbb{R}^n M\subset R^n U\subset\mathbb{R}^n V=U\cup M C^k \phi: V_0 \to V V_0\subset\mathbb{R}^m,"['analysis', 'differential-geometry', 'manifolds']"
40,Spivak's ambiguous popcorn function,Spivak's ambiguous popcorn function,,"There is a question given in Michael Spivak's Calculus chapter 6 problem 1 section iv that reads as follows. For which of the following functions $f$ is there a continuous function $\mathrm{F}$ with domain $\mathbb{R}$ such that $\mathrm{F}(x)=f(x)$ for all $x$ in the domain of $f$ ? iv) $f(x)=1/q, \;x=p/q$ rational in lowest terms. answer book says iv) No $\mathrm{F}$ , since $\mathrm{F}(a)$ would have to be $0$ for irrational $a$ , and then $\mathrm{F}$ is not continuous at $a$ is $a$ is rational. (typo intentionally reproduced from the book) (end section) The function was defined for rational numbers $p/q$ . The function was never defined for $\mathbb{R}$ . $f(x)$ for any irrational number is therefore undefined or unknown to the reader. This seems to be a sloppy example involving the popcorn function. I believe the conclusion that $f(x) = 0$ for any irrational number $x$ is not possible with the information given or maybe there is something that mathematicians know that is implied that I do not understand. Is the function undefined for $x$ where $x$ is an irrational number? This would also result in a discontinuous function but for different reasons then what was concluded by Spivak. I also lack the understanding that the denominator $q$ can be isolated and extracted after $\mathbb{Q}$ has been mapped to $\mathbb{R}$ but that might be off topic. The domain is given as $\mathbb{R}$ not $\mathbb{Q}$ .","There is a question given in Michael Spivak's Calculus chapter 6 problem 1 section iv that reads as follows. For which of the following functions is there a continuous function with domain such that for all in the domain of ? iv) rational in lowest terms. answer book says iv) No , since would have to be for irrational , and then is not continuous at is is rational. (typo intentionally reproduced from the book) (end section) The function was defined for rational numbers . The function was never defined for . for any irrational number is therefore undefined or unknown to the reader. This seems to be a sloppy example involving the popcorn function. I believe the conclusion that for any irrational number is not possible with the information given or maybe there is something that mathematicians know that is implied that I do not understand. Is the function undefined for where is an irrational number? This would also result in a discontinuous function but for different reasons then what was concluded by Spivak. I also lack the understanding that the denominator can be isolated and extracted after has been mapped to but that might be off topic. The domain is given as not .","f \mathrm{F} \mathbb{R} \mathrm{F}(x)=f(x) x f f(x)=1/q, \;x=p/q \mathrm{F} \mathrm{F}(a) 0 a \mathrm{F} a a p/q \mathbb{R} f(x) f(x) = 0 x x x q \mathbb{Q} \mathbb{R} \mathbb{R} \mathbb{Q}","['calculus', 'analysis', 'continuity', 'foundations']"
41,"Why are picewise continuous functions on $[a,b]$ bounded?",Why are picewise continuous functions on  bounded?,"[a,b]","Consider a picewise continuous function $f:[a,b] \to \mathbb{R}$ , i.e there are $a=t_0<\dots <t_n=b$ such that $f$ is continuous on each open interval $(t_i,t_{i+1})$ and the limits $\lim\limits_{x \uparrow t_i}f(x)$ and $\lim\limits_{x \downarrow t_i}f(x)$ exists for all $i=1,\dots,n$ . Why does there exist a constant $M>0$ such that $|f(x)| \leq M$ for all $x \in [a,b]$ ?","Consider a picewise continuous function , i.e there are such that is continuous on each open interval and the limits and exists for all . Why does there exist a constant such that for all ?","f:[a,b] \to \mathbb{R} a=t_0<\dots <t_n=b f (t_i,t_{i+1}) \lim\limits_{x \uparrow t_i}f(x) \lim\limits_{x \downarrow t_i}f(x) i=1,\dots,n M>0 |f(x)| \leq M x \in [a,b]",['analysis']
42,$f(x)=f^{\prime}(x)+f^{\prime\prime }(x)$ such that $f(a)=f(b)=0$ then prove that $f=0$,such that  then prove that,f(x)=f^{\prime}(x)+f^{\prime\prime }(x) f(a)=f(b)=0 f=0,"$f:[a,b] \to \Bbb R$ twice differentiable function is defined in this way that $f(x)=f^{\prime}(x)+f^{\prime\prime }(x)$ such that $f(a)=f(b)=0$ then prove that $f=0$ I have attempted this problem in this way that $f(a)=f(b)=0$ then by Rolle's theorem $\exists c\in (a,b)$ s.t $f'(c)=0$ . Now there are three cases $f""(c)>0$ i.e we have a local minimum then $f(c)=f""(c)>0$ then $\exists \delta>0$ s.t f is decreasing in $(c-\delta,c)$ so there exists a local positive maxima at $d\in (a,c)$ . Now at that point $f(d)>0$ but $f""(d)<0$ a contradiction. $f""(c)<0$ similar like case 1. $f""(c)=0$ . Here we will get $f(c)=0$ so we will repeat those three cases in $[a,c]$ . Now my questions are: Is my proof correct? If it is correct, can I make the proof much more rigorous e.g giving a proper proof of this line ""so there exist a local positive maxima at $d\in (a,c)$ . Now at that point $f(d)>0$ but $f""(d)<0$ a contradiction""? I actually made that statement seeing the picture. Can you give me any other proofs?","twice differentiable function is defined in this way that such that then prove that I have attempted this problem in this way that then by Rolle's theorem s.t . Now there are three cases i.e we have a local minimum then then s.t f is decreasing in so there exists a local positive maxima at . Now at that point but a contradiction. similar like case 1. . Here we will get so we will repeat those three cases in . Now my questions are: Is my proof correct? If it is correct, can I make the proof much more rigorous e.g giving a proper proof of this line ""so there exist a local positive maxima at . Now at that point but a contradiction""? I actually made that statement seeing the picture. Can you give me any other proofs?","f:[a,b] \to \Bbb R f(x)=f^{\prime}(x)+f^{\prime\prime }(x) f(a)=f(b)=0 f=0 f(a)=f(b)=0 \exists c\in (a,b) f'(c)=0 f""(c)>0 f(c)=f""(c)>0 \exists \delta>0 (c-\delta,c) d\in (a,c) f(d)>0 f""(d)<0 f""(c)<0 f""(c)=0 f(c)=0 [a,c] d\in (a,c) f(d)>0 f""(d)<0","['real-analysis', 'calculus', 'analysis', 'contest-math', 'maxima-minima']"
43,Can you give me an example of a real function with a behavior like this,Can you give me an example of a real function with a behavior like this,,"I'm searching for a real function $f:\bigl[0,\infty\bigr)\to\mathbb{R}$ which is: $\mathcal C^{\hspace{0.45mm}\infty}$ for simplicity; $f(0)=0$ $f''(x)<0$ in the interval $\bigl[0,x_{M}\bigr]$ where $x_{M}$ is a maximum of $f$ $\lim_{x\to\infty}f(x)=\alpha>0$ but its behavior is like $\frac{\sin x}{x}$ when $x\rightarrow\infty$ , so $f$ is oscillating around $\alpha$","I'm searching for a real function which is: for simplicity; in the interval where is a maximum of but its behavior is like when , so is oscillating around","f:\bigl[0,\infty\bigr)\to\mathbb{R} \mathcal C^{\hspace{0.45mm}\infty} f(0)=0 f''(x)<0 \bigl[0,x_{M}\bigr] x_{M} f \lim_{x\to\infty}f(x)=\alpha>0 \frac{\sin x}{x} x\rightarrow\infty f \alpha","['real-analysis', 'analysis']"
44,Fundamental Theorem of Calculus for functions with one-sided derivative.,Fundamental Theorem of Calculus for functions with one-sided derivative.,,"Let's assume we have a continuous function $F:[0,\infty)\to\Bbb R$ such that its one-sided derivative $$ f(t):=\lim_{h\searrow 0} \frac {F(t+h)-F(t)}{h} $$ exists everywhere on $[0,\infty)$ . Does the ""Fundamental Theorem of Calculus"" hold, i.e. for each $t\in[0,\infty)$ we have $$ F(t) = F(0) + \int_0^t f(s)\, ds? $$ Usually we'd use the Mean Value Theorem to prove the (normal) FCT but I don't know if something similar to the MVT would be provable with only these assumptions. If the above doesn't hold in general, what kind of condition can we impose on $F$ or $f$ to make the ""FTC"" holds? Note : I also think it is possible that the assumptions that $F$ is continuous and that its one-sided derivative exists everywhere might be strong enough to deduce better property of $F$ , like the existence of $F'$ . If anyone know a result in this direction I'd really love to hear it.","Let's assume we have a continuous function such that its one-sided derivative exists everywhere on . Does the ""Fundamental Theorem of Calculus"" hold, i.e. for each we have Usually we'd use the Mean Value Theorem to prove the (normal) FCT but I don't know if something similar to the MVT would be provable with only these assumptions. If the above doesn't hold in general, what kind of condition can we impose on or to make the ""FTC"" holds? Note : I also think it is possible that the assumptions that is continuous and that its one-sided derivative exists everywhere might be strong enough to deduce better property of , like the existence of . If anyone know a result in this direction I'd really love to hear it.","F:[0,\infty)\to\Bbb R 
f(t):=\lim_{h\searrow 0} \frac {F(t+h)-F(t)}{h}
 [0,\infty) t\in[0,\infty) 
F(t) = F(0) + \int_0^t f(s)\, ds?
 F f F F F'","['real-analysis', 'calculus', 'analysis']"
45,"Prove or disprove that there exists $K$ such that $|f(x)-f(y)|\leq K |x-y|,\;\forall\;\;x,y\in[0,1],$ edited version.",Prove or disprove that there exists  such that  edited version.,"K |f(x)-f(y)|\leq K |x-y|,\;\forall\;\;x,y\in[0,1],","Let $f$ be a function on $[0,1]$ into $\Bbb{R}$ . Suppose that if $x\in[0,1],$ there exists $K_x$ such that \begin{align}|f(x)-f(y)|\leq K_x |x-y|,\;\;\forall\;\;y\in[0,1].\end{align} Prove or disprove that there exists $K$ such that \begin{align}|f(x)-f(y)|\leq K |x-y|,\;\forall\;\;x,y\in[0,1].\end{align} DISPROOF Consider the function \begin{align} f:[0&,1]\to  \Bbb{R},  \\&x\mapsto \sqrt{x} \end{align} Let $x=0$ and $y\in (0,1]$ be fixed. Then, \begin{align} \left| f(0)-f(y) \right|&=\left|0-\sqrt{y}  \right|   \end{align} Take $y=1/(4n^2)$ for all $n.$ Then, \begin{align} \left| f(0)-f\left(\dfrac{1}{4n^2}\right) \right|&=\left|0-\dfrac{1}{\sqrt{4n^2}}  \right| \\&=2n^{3/2}\left|\dfrac{1}{4n^2} -0 \right|   \end{align} By assumption, there exists $K_0$ such that \begin{align} \left| f(0)-f\left(\dfrac{1}{4n^2}\right) \right|&=2n^{3/2}\left|\dfrac{1}{4n^2} -0 \right|\leq K_0\left|\dfrac{1}{4n^2} -0 \right|   \end{align} Sending $n\to\infty,$ we have \begin{align} \infty \leq K_0<\infty,\;\;\text{contradiction}.  \end{align} Hence, the function $f$ is not Lipschitz in $[0,1]$ . QUESTION: Is my disproof correct?","Let be a function on into . Suppose that if there exists such that Prove or disprove that there exists such that DISPROOF Consider the function Let and be fixed. Then, Take for all Then, By assumption, there exists such that Sending we have Hence, the function is not Lipschitz in . QUESTION: Is my disproof correct?","f [0,1] \Bbb{R} x\in[0,1], K_x \begin{align}|f(x)-f(y)|\leq K_x |x-y|,\;\;\forall\;\;y\in[0,1].\end{align} K \begin{align}|f(x)-f(y)|\leq K |x-y|,\;\forall\;\;x,y\in[0,1].\end{align} \begin{align} f:[0&,1]\to 
\Bbb{R},  \\&x\mapsto \sqrt{x} \end{align} x=0 y\in (0,1] \begin{align} \left| f(0)-f(y) \right|&=\left|0-\sqrt{y}  \right|   \end{align} y=1/(4n^2) n. \begin{align} \left| f(0)-f\left(\dfrac{1}{4n^2}\right) \right|&=\left|0-\dfrac{1}{\sqrt{4n^2}}  \right| \\&=2n^{3/2}\left|\dfrac{1}{4n^2} -0 \right|   \end{align} K_0 \begin{align} \left| f(0)-f\left(\dfrac{1}{4n^2}\right) \right|&=2n^{3/2}\left|\dfrac{1}{4n^2} -0 \right|\leq K_0\left|\dfrac{1}{4n^2} -0 \right|   \end{align} n\to\infty, \begin{align} \infty \leq K_0<\infty,\;\;\text{contradiction}.  \end{align} f [0,1]","['real-analysis', 'analysis', 'lipschitz-functions']"
46,Smooth Logarithm at zero/one with special conditions,Smooth Logarithm at zero/one with special conditions,,"As part of a bigger problem it turned out that the function $$V: \Bbb R \rightarrow [0,\infty ), \,  V(x) := \begin{cases} 0 &:  x \leq 1 \\ \log(x) &: x > 1 \end{cases}$$ would help me alot if it was smooth or rather $C^2$. There would be a way out of this by taking/searching a new function $\varphi: \Bbb{R} \rightarrow [0,\infty) $ instead of $V$ with the following conditions for a $\varepsilon > 0$: $\varphi|_{(-\infty,1-\varepsilon] \cup [1+\varepsilon,\infty)} = V$ $\varphi \geq V $ $\varphi \in C^2(\Bbb R)$ How can I show the existence of such an $\varphi$?","As part of a bigger problem it turned out that the function $$V: \Bbb R \rightarrow [0,\infty ), \,  V(x) := \begin{cases} 0 &:  x \leq 1 \\ \log(x) &: x > 1 \end{cases}$$ would help me alot if it was smooth or rather $C^2$. There would be a way out of this by taking/searching a new function $\varphi: \Bbb{R} \rightarrow [0,\infty) $ instead of $V$ with the following conditions for a $\varepsilon > 0$: $\varphi|_{(-\infty,1-\varepsilon] \cup [1+\varepsilon,\infty)} = V$ $\varphi \geq V $ $\varphi \in C^2(\Bbb R)$ How can I show the existence of such an $\varphi$?",,"['real-analysis', 'analysis', 'derivatives', 'logarithms']"
47,"$\int^b_af(x)\,x^k \, dx=\int^b_ag(x)\,x^k \, dx$ , for all $k \in \mathbb{N}$ [duplicate]",", for all  [duplicate]","\int^b_af(x)\,x^k \, dx=\int^b_ag(x)\,x^k \, dx k \in \mathbb{N}","This question already has answers here : Show that $\int_{a}^{b}{x^{n}f(x)dx}=0$, then $f=0$ (2 answers) Closed 6 years ago . If $f, g\in \mathcal{C}([a,b],\mathbb{R})$ and $$\int^b_af(x)\,x^k \, dx=\int^b_ag(x)\,x^k \, dx, \text{ for all } k \in \mathbb{N}$$ Prove that $f=g$. I'm trying to prove this but I don't know how to proceed. Any suggestions?","This question already has answers here : Show that $\int_{a}^{b}{x^{n}f(x)dx}=0$, then $f=0$ (2 answers) Closed 6 years ago . If $f, g\in \mathcal{C}([a,b],\mathbb{R})$ and $$\int^b_af(x)\,x^k \, dx=\int^b_ag(x)\,x^k \, dx, \text{ for all } k \in \mathbb{N}$$ Prove that $f=g$. I'm trying to prove this but I don't know how to proceed. Any suggestions?",,"['real-analysis', 'analysis', 'definite-integrals']"
48,Infinite series for $e$...,Infinite series for ...,e,How do you prove that $e=\sum_{n=0}^{\infty}\frac{1}{n!}$? Here I am assuming $e:=\lim_{n\to\infty}(1+\frac{1}{n})^n$. Do you have any good PDF file or booklet available online on this? I do not like how my analysis text handles this...,How do you prove that $e=\sum_{n=0}^{\infty}\frac{1}{n!}$? Here I am assuming $e:=\lim_{n\to\infty}(1+\frac{1}{n})^n$. Do you have any good PDF file or booklet available online on this? I do not like how my analysis text handles this...,,[]
49,Fourier series of $f(x) = \frac{1-a^2}{1+a^2-2a \cos x}$,Fourier series of,f(x) = \frac{1-a^2}{1+a^2-2a \cos x},"Consider the following function $f(x) = \frac{1-a^2}{1+a^2-2a \cos x}$. I'm looking for its represention of this kind: $\sum_{\mathbb{Z}} a_k e^{ikx}$ So we can rewrite function as $f(x) = \frac{1}{1+a^2}\frac{1-a^2}{1-\frac{2a \cos x}{1+a^2}}$ And consider it is as a geometric sequence, with $q=\frac{2a \cos x}{1+a^2} = \frac{a (e^{ix}+e^{-ix})}{1+a^2}$ So the series probally look like: $\sum_{\mathbb{Z}} \left(\frac{a}{1+a^2}\right)^{|k|} e^{ikx}$. Is it correct? Does it work for complex a?","Consider the following function $f(x) = \frac{1-a^2}{1+a^2-2a \cos x}$. I'm looking for its represention of this kind: $\sum_{\mathbb{Z}} a_k e^{ikx}$ So we can rewrite function as $f(x) = \frac{1}{1+a^2}\frac{1-a^2}{1-\frac{2a \cos x}{1+a^2}}$ And consider it is as a geometric sequence, with $q=\frac{2a \cos x}{1+a^2} = \frac{a (e^{ix}+e^{-ix})}{1+a^2}$ So the series probally look like: $\sum_{\mathbb{Z}} \left(\frac{a}{1+a^2}\right)^{|k|} e^{ikx}$. Is it correct? Does it work for complex a?",,"['analysis', 'fourier-series']"
50,Surface integral (divergence theorem ) [closed],Surface integral (divergence theorem ) [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Evaluate: $\int \int \bar{F}\cdot d\bar S $, $ \bar{F}=\left(2-x^{2}yz+y^{3},xy^{2}z+ye^{z},y^{2}+z-e^{z}\right)$ $\gamma : y^{2}+z^{2}=x^{2} $ between the planes $x=1$ and $x=2$ , The normal pointing away from the $x$-axis. Thanks in advance. Edit : I did try to solve this for hours , I have problem with the concept of orientation. I will post my attempt when am done , It is still hard for me to write using Latex but I will spend some time writing. I dont have the solution to this problem, and I dont want just the solution I want a way to master all this kind of questions , preparing for my exam. meanwhile I started a bounty to whomever can make me understand how to work these questions out . ( I have no background on physics ) thanks again","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Evaluate: $\int \int \bar{F}\cdot d\bar S $, $ \bar{F}=\left(2-x^{2}yz+y^{3},xy^{2}z+ye^{z},y^{2}+z-e^{z}\right)$ $\gamma : y^{2}+z^{2}=x^{2} $ between the planes $x=1$ and $x=2$ , The normal pointing away from the $x$-axis. Thanks in advance. Edit : I did try to solve this for hours , I have problem with the concept of orientation. I will post my attempt when am done , It is still hard for me to write using Latex but I will spend some time writing. I dont have the solution to this problem, and I dont want just the solution I want a way to master all this kind of questions , preparing for my exam. meanwhile I started a bounty to whomever can make me understand how to work these questions out . ( I have no background on physics ) thanks again",,"['calculus', 'analysis', 'multivariable-calculus']"
51,$\lim_{x\to\infty}(f(x)+f'(x))=0 \rightarrow \lim_{x\to\infty}f(x)=0$? [duplicate],? [duplicate],\lim_{x\to\infty}(f(x)+f'(x))=0 \rightarrow \lim_{x\to\infty}f(x)=0,"This question already has answers here : Limit of $y$ if limit of $y+y'$ goes to $0$? [duplicate] (2 answers) Closed 7 years ago . Suppose that $f:[0,\infty)\to\Bbb R$ is differentiable, and $\lim_{x\to\infty}(f(x)+f'(x))=0$. Prove that $\lim_{x\to\infty}f(x)=0$. I tried to show that $\lim_{x\to\infty}f(x)\neq0\rightarrow \lim_{x\to\infty}(f(x)+f'(x))\neq0$ If $\lim_{x\to\infty}f(x)\neq0, \exists e>0, \forall N\in\Bbb N, \exists x_1, x_2, ...>N,|f(x_i)|\ge e$. So that the possibility for  $\lim_{x\to\infty}(f(x)+f'(x))=0$ is only when $f(x_i)+f'(x_i)=0$ is true for any $i$. So if  $f(x_1)\gt0,$ it must be decreasing, to less than $e$ But there exists $x_2\gt x_1 s.t. |f(x_2)|\ge e$ In conclusion, $f(x)$ has to decrease when $x$ is large enough and $f(x)\ge e$ but there must exists infinitely many points whose function value is no less than $e$. But it cannot happen. Is my idea of proof valid? I don't know how to formally write my idea...please teach me..","This question already has answers here : Limit of $y$ if limit of $y+y'$ goes to $0$? [duplicate] (2 answers) Closed 7 years ago . Suppose that $f:[0,\infty)\to\Bbb R$ is differentiable, and $\lim_{x\to\infty}(f(x)+f'(x))=0$. Prove that $\lim_{x\to\infty}f(x)=0$. I tried to show that $\lim_{x\to\infty}f(x)\neq0\rightarrow \lim_{x\to\infty}(f(x)+f'(x))\neq0$ If $\lim_{x\to\infty}f(x)\neq0, \exists e>0, \forall N\in\Bbb N, \exists x_1, x_2, ...>N,|f(x_i)|\ge e$. So that the possibility for  $\lim_{x\to\infty}(f(x)+f'(x))=0$ is only when $f(x_i)+f'(x_i)=0$ is true for any $i$. So if  $f(x_1)\gt0,$ it must be decreasing, to less than $e$ But there exists $x_2\gt x_1 s.t. |f(x_2)|\ge e$ In conclusion, $f(x)$ has to decrease when $x$ is large enough and $f(x)\ge e$ but there must exists infinitely many points whose function value is no less than $e$. But it cannot happen. Is my idea of proof valid? I don't know how to formally write my idea...please teach me..",,['analysis']
52,"Double essential supremum ($\mathrm{ess\ sup}_{z\in X\times Y}f(x,y)=\mathrm{ess\ sup}_{x\in X}\mathrm{ess\ sup}_{y\in Y}f(x,y)$?)",Double essential supremum (?),"\mathrm{ess\ sup}_{z\in X\times Y}f(x,y)=\mathrm{ess\ sup}_{x\in X}\mathrm{ess\ sup}_{y\in Y}f(x,y)","Let $(X,\mathcal{F},\mu_X)$ and $(Y,\mathcal{G},\mu_Y)$ be measure spaces and $(X\times Y,\mathcal{F}\otimes\mathcal{G},\mu)$ be the product measure space. Consider $f:X\times Y\to {\mathbb{R}}$. Does $\mathrm{ess\ sup}_{z\in X\times Y}f(x,y)=\mathrm{ess\ sup}_{x\in X}\mathrm{ess\ sup}_{y\in Y}f(x,y)$ hold? (or if not, an inequality at least?) I tried to mimic the proof of the 'ordinary' supremum version answered in Simple question: the double supremum but involves two different measures, measure zero sets, the ones for product measure, and could not really figure out.","Let $(X,\mathcal{F},\mu_X)$ and $(Y,\mathcal{G},\mu_Y)$ be measure spaces and $(X\times Y,\mathcal{F}\otimes\mathcal{G},\mu)$ be the product measure space. Consider $f:X\times Y\to {\mathbb{R}}$. Does $\mathrm{ess\ sup}_{z\in X\times Y}f(x,y)=\mathrm{ess\ sup}_{x\in X}\mathrm{ess\ sup}_{y\in Y}f(x,y)$ hold? (or if not, an inequality at least?) I tried to mimic the proof of the 'ordinary' supremum version answered in Simple question: the double supremum but involves two different measures, measure zero sets, the ones for product measure, and could not really figure out.",,"['real-analysis', 'analysis', 'measure-theory', 'inequality', 'supremum-and-infimum']"
53,"$\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x)$ for a continuous function $f:[a,b]\to\mathbb{R}$",for a continuous function,"\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x) f:[a,b]\to\mathbb{R}","Let $f:[a,b]\to\mathbb{R}$ be continuous. I'm sure it's not hard, but I'm unsure what exactly we need to do to prove $$\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x)$$","Let $f:[a,b]\to\mathbb{R}$ be continuous. I'm sure it's not hard, but I'm unsure what exactly we need to do to prove $$\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x)$$",,"['real-analysis', 'analysis', 'continuity']"
54,Continuous function with finitely many discontinuities is Riemann Integral,Continuous function with finitely many discontinuities is Riemann Integral,,"After a lecture today, I just wanted to confirm that I understand the proof of the following: If $f: [a,b] \to \mathbb{R}$  is bounded and continuous and has finitely many discontinuities, $f \in \mathscr{R}$. Proof: Let $\{x_1,\ldots x_l\}$ be the set of discontinuities of $f$. Let $\epsilon > 0$. Let $M$ be such that $|f| \leq M$. Let $\delta > 0$ be such that $2(k+2)M\delta < \frac{\epsilon}{2}$. Now, consider the interval $$B= [a,b] \setminus \left( (x_1-\delta, x_1 + \delta) \cup (x_2-\delta, x_2+\delta) \cup \cdots \cup (x_k-\delta, x_k+\delta) \right),$$ clearly $B$ is compact by Heine-Borel. Thus, $f|B :B \to \mathbb{R}$ is uniformly continuous. Thus, there exists $\delta > 0$ so that, if $|x-y| < \delta$, $|f(x) - f(y)| < \frac{\epsilon}{2(b-a)}$. Now, let $n$ be a positive integer such that $\frac{b-a}{n} < \delta$. Consider the partition $$P:= \left\{a=x_0, a+\frac{x_1-\delta-a}{n}, a+2\frac{x_1-\delta}{n}, \ldots x_1- \delta, x_1+\delta, \ldots, x_k+\delta , x_k+\delta+\frac{b-(x_k+\delta)}{n},  \ldots, b \right\}$$ That is, we have two types of intervals. One type is of the form $[x_i - \delta, x_i+\delta]$ while we chop what is between $x_i+\delta$ and $x_{i+1} - \delta$ into $n$ parts. Now here's where I get a bit confused. I suppose we chop this middle piece up even further, although I'm not sure why. Consider the intervals $$D_i = \left[x_i+\delta+m\left(\frac{x_{i+1} - x_i -2\delta}{n}\right), x_i+\delta+(m+1)\left(\frac{x_{i+1} - x_i -2\delta}{n}\right)\right]$$ Call $\frac{x_{i+1} - x_i -2\delta}{n} =T$. Then, if $y_1,y_2 \in D_k$, then $$|y_1-y_2| \leq T \leq \frac{b-a}{n} < \delta$$. so that if $y_1,y_2 \in D_i$, the uniform continuity condition holds. Now, for any $y \in D_i$, $f(y) = f(y) - f(y_1) + f(y_1)$ so that $f(y) \leq \frac{\epsilon}{2(b-a)} + f(y_1)$ for all $y \in D_i$. Thus, $$\sup_{x \in D_i} f(x) \leq \frac{\epsilon}{2(b-a)} + f(y_1)$$ and $$\inf_{x \in D_k} f(x) \geq -\frac{\epsilon}{2(b-a)} + f(y_1)$$. Thus, $$\sup_{x \in D_i} f(x)- \inf_{x \in D_k}  f(x) \leq \frac{\epsilon}{(b-a)}.$$ Now, we need show that $U(f,P) - L(f,P) < \epsilon$. Let $M_i = \sup_{x \in \Delta_i} f(x)$ and $m_i$ similarly. We have that $$ \begin{align} U(f,P) - L(f,P) &= \sum_{P} (M_i-m_i)\Delta_i \\ &= \sum_{[x_i-\delta,x_i+\delta]} (M_i-m_i)\Delta_i + \sum_{D_i} (M_i-m_i)\Delta_i \\&\leq \sum_{[x_i-\delta,x_i+\delta]} 4M\delta + \sum_{D_i} \frac{\epsilon}{(b-a)}\frac{b-a}{n} \\&=  4Mk\delta + (k+1)n\frac{\epsilon}{n} \\&<  \epsilon \end{align}$$ Thus, $f \in \mathscr{R}$. Now, this proof seems a bit mathemagical to me. I understand that it works, just not why . I see that we construct little intervals around the discontinuities in order to ""ignore"" them, but I don't see, for example, why we need to create the $D_i$ intervals. Why do we need to chop the interval between the $x_i+\delta$ and $x_{i+1} -\delta$ into $n$ pieces. Why can't we just consider the middle and only the middle? Any insight to the intuition would be greatly appreciated.","After a lecture today, I just wanted to confirm that I understand the proof of the following: If $f: [a,b] \to \mathbb{R}$  is bounded and continuous and has finitely many discontinuities, $f \in \mathscr{R}$. Proof: Let $\{x_1,\ldots x_l\}$ be the set of discontinuities of $f$. Let $\epsilon > 0$. Let $M$ be such that $|f| \leq M$. Let $\delta > 0$ be such that $2(k+2)M\delta < \frac{\epsilon}{2}$. Now, consider the interval $$B= [a,b] \setminus \left( (x_1-\delta, x_1 + \delta) \cup (x_2-\delta, x_2+\delta) \cup \cdots \cup (x_k-\delta, x_k+\delta) \right),$$ clearly $B$ is compact by Heine-Borel. Thus, $f|B :B \to \mathbb{R}$ is uniformly continuous. Thus, there exists $\delta > 0$ so that, if $|x-y| < \delta$, $|f(x) - f(y)| < \frac{\epsilon}{2(b-a)}$. Now, let $n$ be a positive integer such that $\frac{b-a}{n} < \delta$. Consider the partition $$P:= \left\{a=x_0, a+\frac{x_1-\delta-a}{n}, a+2\frac{x_1-\delta}{n}, \ldots x_1- \delta, x_1+\delta, \ldots, x_k+\delta , x_k+\delta+\frac{b-(x_k+\delta)}{n},  \ldots, b \right\}$$ That is, we have two types of intervals. One type is of the form $[x_i - \delta, x_i+\delta]$ while we chop what is between $x_i+\delta$ and $x_{i+1} - \delta$ into $n$ parts. Now here's where I get a bit confused. I suppose we chop this middle piece up even further, although I'm not sure why. Consider the intervals $$D_i = \left[x_i+\delta+m\left(\frac{x_{i+1} - x_i -2\delta}{n}\right), x_i+\delta+(m+1)\left(\frac{x_{i+1} - x_i -2\delta}{n}\right)\right]$$ Call $\frac{x_{i+1} - x_i -2\delta}{n} =T$. Then, if $y_1,y_2 \in D_k$, then $$|y_1-y_2| \leq T \leq \frac{b-a}{n} < \delta$$. so that if $y_1,y_2 \in D_i$, the uniform continuity condition holds. Now, for any $y \in D_i$, $f(y) = f(y) - f(y_1) + f(y_1)$ so that $f(y) \leq \frac{\epsilon}{2(b-a)} + f(y_1)$ for all $y \in D_i$. Thus, $$\sup_{x \in D_i} f(x) \leq \frac{\epsilon}{2(b-a)} + f(y_1)$$ and $$\inf_{x \in D_k} f(x) \geq -\frac{\epsilon}{2(b-a)} + f(y_1)$$. Thus, $$\sup_{x \in D_i} f(x)- \inf_{x \in D_k}  f(x) \leq \frac{\epsilon}{(b-a)}.$$ Now, we need show that $U(f,P) - L(f,P) < \epsilon$. Let $M_i = \sup_{x \in \Delta_i} f(x)$ and $m_i$ similarly. We have that $$ \begin{align} U(f,P) - L(f,P) &= \sum_{P} (M_i-m_i)\Delta_i \\ &= \sum_{[x_i-\delta,x_i+\delta]} (M_i-m_i)\Delta_i + \sum_{D_i} (M_i-m_i)\Delta_i \\&\leq \sum_{[x_i-\delta,x_i+\delta]} 4M\delta + \sum_{D_i} \frac{\epsilon}{(b-a)}\frac{b-a}{n} \\&=  4Mk\delta + (k+1)n\frac{\epsilon}{n} \\&<  \epsilon \end{align}$$ Thus, $f \in \mathscr{R}$. Now, this proof seems a bit mathemagical to me. I understand that it works, just not why . I see that we construct little intervals around the discontinuities in order to ""ignore"" them, but I don't see, for example, why we need to create the $D_i$ intervals. Why do we need to chop the interval between the $x_i+\delta$ and $x_{i+1} -\delta$ into $n$ pieces. Why can't we just consider the middle and only the middle? Any insight to the intuition would be greatly appreciated.",,"['real-analysis', 'analysis', 'proof-writing']"
55,Do absolutely continuous functions have bounded derivative?,Do absolutely continuous functions have bounded derivative?,,"I am an outsider for this field of mathematical analysis. But to analyse a problem of Control Systems, which is my area of interest, I need to know this. I learned that absolutely continuous functions are also differentiable almost everywhere . On the other hand, Lipschitz continuity ensures bounded derivative of the function a.e. I am wondering whether there is any link between the derivative being bounded and the function being absolutely continuous. Or, is there any sufficient condition to be imposed over the absolute continuity to ascertain that the derivative of the function will be bounded?","I am an outsider for this field of mathematical analysis. But to analyse a problem of Control Systems, which is my area of interest, I need to know this. I learned that absolutely continuous functions are also differentiable almost everywhere . On the other hand, Lipschitz continuity ensures bounded derivative of the function a.e. I am wondering whether there is any link between the derivative being bounded and the function being absolutely continuous. Or, is there any sufficient condition to be imposed over the absolute continuity to ascertain that the derivative of the function will be bounded?",,"['real-analysis', 'analysis']"
56,Prove the inverse of this differentiable function is differentiable? [duplicate],Prove the inverse of this differentiable function is differentiable? [duplicate],,This question already has an answer here : Alternative proof for differentiability of inverse function? (1 answer) Closed 9 years ago . Suppose we have a differentiable function $ g $ that maps from a real interval $ I $ to the real numbers and suppose $ g'(r)>0$ for all $ r$ in $ I $.  Then I want to show that $ g^{-1}$ is differentiable on $g(I). $ Intuitively this makes sense but I can't come up with a neat proof. I was thinking to use the mean value theorem but I'm not sure if that would get me anywhere.,This question already has an answer here : Alternative proof for differentiability of inverse function? (1 answer) Closed 9 years ago . Suppose we have a differentiable function $ g $ that maps from a real interval $ I $ to the real numbers and suppose $ g'(r)>0$ for all $ r$ in $ I $.  Then I want to show that $ g^{-1}$ is differentiable on $g(I). $ Intuitively this makes sense but I can't come up with a neat proof. I was thinking to use the mean value theorem but I'm not sure if that would get me anywhere.,,"['analysis', 'functions', 'derivatives']"
57,How to prove that $\ln x\leq x-1 \forall x>0$?,How to prove that ?,\ln x\leq x-1 \forall x>0,"I need to prove that $\ln x\leq x-1 \forall  x>0$, using the Mean value theorem. For $x=1$, the equation is true. So, for starters I'll check for $x>1$. By applying the aforementioned theorem for $$f(t)=\ln t / [1,x]$$ we know that there is  a $c\in(1,x)$ with $$f'(c)=\frac{f(x)-f(1)}{x-1}\Leftrightarrow$$ $$\ln c=\frac{\ln x}{x-1}$$ And here I am stuck. I know that $c>1$ thus $\ln c >\ln 1$ thus $\ln c>0$ But I don't know how to use that to prove what I need to prove.","I need to prove that $\ln x\leq x-1 \forall  x>0$, using the Mean value theorem. For $x=1$, the equation is true. So, for starters I'll check for $x>1$. By applying the aforementioned theorem for $$f(t)=\ln t / [1,x]$$ we know that there is  a $c\in(1,x)$ with $$f'(c)=\frac{f(x)-f(1)}{x-1}\Leftrightarrow$$ $$\ln c=\frac{\ln x}{x-1}$$ And here I am stuck. I know that $c>1$ thus $\ln c >\ln 1$ thus $\ln c>0$ But I don't know how to use that to prove what I need to prove.",,['analysis']
58,Oscillation of a Function,Oscillation of a Function,,"Let $f\colon (a,b)\rightarrow \mathbb{R}$ be function. For a non-empty subset $T$ of $(a,b)$, define $\Omega(f,T)=\sup\{|f(x)-f(y)|\colon x,y\in T \}$, and the oscillation function from $(a,b)$ to $\mathbb{R}$ by $x\mapsto \omega_f(x)=\inf\{ \Omega(f,T_x)\}\colon T_x\subseteq (a,b) \mbox{ and } x\in T_x\}$. I couldn't solve few questions about $\omega_f(x)$. 1) Instead of considering all subsets $T_x$ containing $x$, can we consider the sets $B(x,r)\cap (a,b)$, $r>0$, to get the same definition of oscillation function $x\mapsto \omega_f(x)$? In other words, $\inf\{ \Omega(f,T_x)\colon T_x\subseteq (a,b), x\in T_x\} \leq \inf \{\Omega(f,B(x,r)\cap (a,b))\colon r>0 \}$ is clear, but I couldn't prove reverse inequality (if it holds). 2) What can be said about function $x\mapsto \omega_f(x)$? (i.e. is it continuous/ uniformly continuous/ Lipschitz continuous/ differentiable?) Notation: $B(x,r)=(x-r, x+r)$.","Let $f\colon (a,b)\rightarrow \mathbb{R}$ be function. For a non-empty subset $T$ of $(a,b)$, define $\Omega(f,T)=\sup\{|f(x)-f(y)|\colon x,y\in T \}$, and the oscillation function from $(a,b)$ to $\mathbb{R}$ by $x\mapsto \omega_f(x)=\inf\{ \Omega(f,T_x)\}\colon T_x\subseteq (a,b) \mbox{ and } x\in T_x\}$. I couldn't solve few questions about $\omega_f(x)$. 1) Instead of considering all subsets $T_x$ containing $x$, can we consider the sets $B(x,r)\cap (a,b)$, $r>0$, to get the same definition of oscillation function $x\mapsto \omega_f(x)$? In other words, $\inf\{ \Omega(f,T_x)\colon T_x\subseteq (a,b), x\in T_x\} \leq \inf \{\Omega(f,B(x,r)\cap (a,b))\colon r>0 \}$ is clear, but I couldn't prove reverse inequality (if it holds). 2) What can be said about function $x\mapsto \omega_f(x)$? (i.e. is it continuous/ uniformly continuous/ Lipschitz continuous/ differentiable?) Notation: $B(x,r)=(x-r, x+r)$.",,"['real-analysis', 'analysis']"
59,Show a function is monotonically decreasing.,Show a function is monotonically decreasing.,,"Show that $f(x)=\dfrac{\sin x}{x}$ is monotonically decreasing on $[0,\frac{\pi}{2}]$ I'm trying to show that $f'(x)\leq0$ to show it's monotonically decreasing.  So $f'(x)=\dfrac{x\cos x-\sin x}{x^2}$.   I can see that $f'(x)\leq0$ but having trouble proving it.","Show that $f(x)=\dfrac{\sin x}{x}$ is monotonically decreasing on $[0,\frac{\pi}{2}]$ I'm trying to show that $f'(x)\leq0$ to show it's monotonically decreasing.  So $f'(x)=\dfrac{x\cos x-\sin x}{x^2}$.   I can see that $f'(x)\leq0$ but having trouble proving it.",,"['calculus', 'analysis']"
60,Show that $\arctan(n)$ is irrational for all $n \in \mathbb{N}$,Show that  is irrational for all,\arctan(n) n \in \mathbb{N},Question : Show that $\arctan(n)$ is irrational for all $n \in \mathbb{N}$. Hint: My solution doesn't use continued fraction. I am interested in other possible proofs for this question.,Question : Show that $\arctan(n)$ is irrational for all $n \in \mathbb{N}$. Hint: My solution doesn't use continued fraction. I am interested in other possible proofs for this question.,,"['real-analysis', 'analysis', 'irrational-numbers']"
61,Improper parametric integral and differentiation under the integral sign,Improper parametric integral and differentiation under the integral sign,,"While looking at an astrophysic problem, I encountered the following integral $$ \rho_{\infty} (r) = \int_{r}^{a} \frac{\rho_{0} (r_{0})}{\sqrt{r_{0}^{2} - r^{2}}} d r_{0} \;\;\;\;\;\;\; (1)$$ The function $\rho_{0} : \, ]0,a] \mapsto \mathbb{R}^{+}$ is an $\textit{a priori}$ given and $\textit{smooth}$ function. If we consider the case $\rho_{0} = 1$, then the integral $(1)$ can be explicitly worked out and leads to $\rho_{\infty} (r) = \text{argch} \left(\frac{a}{r}\right) $. One should note that in this situation, $\rho_{\infty}$ is also a $\textit{smooth}$ function on $]0,a]$. As $\rho_{\infty}$ in my situation appears to be $\textit{smooth}$, I want to estimate its derivative. Forgetting about the conditions of applicability of the theorem about differentiation under the integral, one can rewrite $\rho_{\infty}$ under the form $$ \rho_{\infty} (r) = \int_{r}^{a} f (r_{0},r) \, d r_{0} $$ so that naively, its derivative should be given by $\rho_{\infty} \overset{?}{=} \int_{r}^{a} \frac{\partial f}{\partial r} d r_{0} - 1 \cdot f (r_{0},r)$. However, this leads to an undefined expression, since both of these terms are infinite. My question is the following one. As $\rho_{\infty}$ is a $\textit{smooth}$ function, its derivative exists. But, how should I proceed to estimate $\frac{d \rho_{\infty}}{d r}$ in terms of $\rho_{0}$ ? $\textbf{Bonus question}$ In fact, my final aim would be to $\textit{invert}$ the equation $(1)$, in order to express $\rho_{0}$ as a function of $\rho_{\infty}$. Any idea on a way to tackle this question ?","While looking at an astrophysic problem, I encountered the following integral $$ \rho_{\infty} (r) = \int_{r}^{a} \frac{\rho_{0} (r_{0})}{\sqrt{r_{0}^{2} - r^{2}}} d r_{0} \;\;\;\;\;\;\; (1)$$ The function $\rho_{0} : \, ]0,a] \mapsto \mathbb{R}^{+}$ is an $\textit{a priori}$ given and $\textit{smooth}$ function. If we consider the case $\rho_{0} = 1$, then the integral $(1)$ can be explicitly worked out and leads to $\rho_{\infty} (r) = \text{argch} \left(\frac{a}{r}\right) $. One should note that in this situation, $\rho_{\infty}$ is also a $\textit{smooth}$ function on $]0,a]$. As $\rho_{\infty}$ in my situation appears to be $\textit{smooth}$, I want to estimate its derivative. Forgetting about the conditions of applicability of the theorem about differentiation under the integral, one can rewrite $\rho_{\infty}$ under the form $$ \rho_{\infty} (r) = \int_{r}^{a} f (r_{0},r) \, d r_{0} $$ so that naively, its derivative should be given by $\rho_{\infty} \overset{?}{=} \int_{r}^{a} \frac{\partial f}{\partial r} d r_{0} - 1 \cdot f (r_{0},r)$. However, this leads to an undefined expression, since both of these terms are infinite. My question is the following one. As $\rho_{\infty}$ is a $\textit{smooth}$ function, its derivative exists. But, how should I proceed to estimate $\frac{d \rho_{\infty}}{d r}$ in terms of $\rho_{0}$ ? $\textbf{Bonus question}$ In fact, my final aim would be to $\textit{invert}$ the equation $(1)$, in order to express $\rho_{0}$ as a function of $\rho_{\infty}$. Any idea on a way to tackle this question ?",,"['real-analysis', 'integration', 'analysis', 'derivatives', 'improper-integrals']"
62,Inverse Function (and WolframAlpha gives different Result),Inverse Function (and WolframAlpha gives different Result),,"I wanted to calculate the inverse function of  $$  f(x) = \frac{1}{x} + \frac{1}{x-1} $$ Quite simple I thought, put $$  y = \frac{1}{x} + \frac{1}{x-1} = \frac{2x-1}{x(x-1)} $$ rearrange and solve $$  y(x(x-1)) - 2x + 1 = 0 $$ which give the quadratic equation $$  yx^2 - (y + 2)x + 1 = 0 $$ Using the Solution Formula we got $$  x = \frac{(y+2) \pm \sqrt{y^2+4}}{2y} $$ So the inverse function is $$  f^{-1}(x) = \frac{(x+2) \pm \sqrt{x^2+4}}{2x} $$ Just to confirm I put in WolframAlpha and it gives me $$  \frac{-x-2}{2x} \pm \frac{\sqrt{x^2+4}}{2x} $$ (just click on the link to start WolframAlpha with this parameter), which is different up to a sign in the first summand, can not see an error, do you (or is WolframAlpha wrong...)? EDIT : If the link is not working for you:","I wanted to calculate the inverse function of  $$  f(x) = \frac{1}{x} + \frac{1}{x-1} $$ Quite simple I thought, put $$  y = \frac{1}{x} + \frac{1}{x-1} = \frac{2x-1}{x(x-1)} $$ rearrange and solve $$  y(x(x-1)) - 2x + 1 = 0 $$ which give the quadratic equation $$  yx^2 - (y + 2)x + 1 = 0 $$ Using the Solution Formula we got $$  x = \frac{(y+2) \pm \sqrt{y^2+4}}{2y} $$ So the inverse function is $$  f^{-1}(x) = \frac{(x+2) \pm \sqrt{x^2+4}}{2x} $$ Just to confirm I put in WolframAlpha and it gives me $$  \frac{-x-2}{2x} \pm \frac{\sqrt{x^2+4}}{2x} $$ (just click on the link to start WolframAlpha with this parameter), which is different up to a sign in the first summand, can not see an error, do you (or is WolframAlpha wrong...)? EDIT : If the link is not working for you:",,"['real-analysis', 'analysis', 'wolfram-alpha']"
63,The multiplication formula for the Hurwitz zeta function,The multiplication formula for the Hurwitz zeta function,,"In a textbook I'm reading, the author states without proof that $$ \zeta(s,mz) = \frac{1}{m^{s}} \sum_{k=0}^{m-1} \zeta \left(s,z+\frac{k}{m} \right), \tag{1}$$ where $\zeta(s,z) $ is the Hurwitz zeta function Supposedly, this isn't hard to prove. But is it possible to prove $(1)$ using simply the series definition of the Hurwitz zeta function, that is,   $ \displaystyle\zeta(s,z) = \sum_{n=0}^{\infty} \frac{1}{(z+n)^{s}}$? It might be interesting to note that the polygamma functions (excluding the digamma function) can be expressed in terms of the Hurwitz zeta function. So from $(1)$ we can derive the multiplication formula $$\psi_{n}(mz) = \frac{1}{m^{n+1}} \sum_{k=0}^{m-1}\psi_{n} \left(z+ \frac{k}{m} \right) , \quad n \in \mathbb{Z}_{>0}.$$","In a textbook I'm reading, the author states without proof that $$ \zeta(s,mz) = \frac{1}{m^{s}} \sum_{k=0}^{m-1} \zeta \left(s,z+\frac{k}{m} \right), \tag{1}$$ where $\zeta(s,z) $ is the Hurwitz zeta function Supposedly, this isn't hard to prove. But is it possible to prove $(1)$ using simply the series definition of the Hurwitz zeta function, that is,   $ \displaystyle\zeta(s,z) = \sum_{n=0}^{\infty} \frac{1}{(z+n)^{s}}$? It might be interesting to note that the polygamma functions (excluding the digamma function) can be expressed in terms of the Hurwitz zeta function. So from $(1)$ we can derive the multiplication formula $$\psi_{n}(mz) = \frac{1}{m^{n+1}} \sum_{k=0}^{m-1}\psi_{n} \left(z+ \frac{k}{m} \right) , \quad n \in \mathbb{Z}_{>0}.$$",,"['analysis', 'special-functions', 'zeta-functions']"
64,What do physicists mean with this bra-ket notation?,What do physicists mean with this bra-ket notation?,,"In Quantum mechanics we said that $\langle x'|\psi \rangle = \psi(x)$, where  $\langle \phi|\psi \rangle $ is the dot product in $L^2(\mathbb{C})$. I found out, that this is true, if you set x' to be the delta function $\delta(x)$ Now I also found $\langle p'|\psi \rangle = \tilde{\psi}(p)$, where $\tilde{\psi}$ is the fourier transform of $\psi$. My question is: Does anybody here know what $p'$ could be, so that this expression makes sense?","In Quantum mechanics we said that $\langle x'|\psi \rangle = \psi(x)$, where  $\langle \phi|\psi \rangle $ is the dot product in $L^2(\mathbb{C})$. I found out, that this is true, if you set x' to be the delta function $\delta(x)$ Now I also found $\langle p'|\psi \rangle = \tilde{\psi}(p)$, where $\tilde{\psi}$ is the fourier transform of $\psi$. My question is: Does anybody here know what $p'$ could be, so that this expression makes sense?",,"['calculus', 'real-analysis']"
65,Prove that $\int_{0}^{1}|f(x)f'(x)|dx\leq\frac{1}{2}\int_{0}^{1}|f'(x)|^2dx$,Prove that,\int_{0}^{1}|f(x)f'(x)|dx\leq\frac{1}{2}\int_{0}^{1}|f'(x)|^2dx,"Let $f$ be a continuously differentiable function on $[0,1]$ and $f(0)=0$. Prove that $$\int_{0}^{1}|f(x)f'(x)|dx\leq\frac{1}{2}\int_{0}^{1}|f'(x)|^2dx$$ Thank you!","Let $f$ be a continuously differentiable function on $[0,1]$ and $f(0)=0$. Prove that $$\int_{0}^{1}|f(x)f'(x)|dx\leq\frac{1}{2}\int_{0}^{1}|f'(x)|^2dx$$ Thank you!",,"['analysis', 'integration', 'integral-inequality']"
66,when Fourier transform function in $\mathbb C$?,when Fourier transform function in ?,\mathbb C,"The Fourier transform of a function $f\in\mathscr L^1(\mathbb R)$ is $$\widehat f\colon\mathbb R\rightarrow\mathbb C, x\mapsto\int_{-\infty}^\infty f(t)\exp(-ixt)\,\textrm{d}t$$ When is this indeed a function in $\mathbb C$? Most of calculations you get functions in $\mathbb R$. When  in $\mathbb C$? Add: I know there are results like $\frac{e^{ait}-e^ {-ait}}{2i}=\sin(at)$ multiplied by 'anything', but I am asking for a function which you cannot write as a function in $\mathbb R$.","The Fourier transform of a function $f\in\mathscr L^1(\mathbb R)$ is $$\widehat f\colon\mathbb R\rightarrow\mathbb C, x\mapsto\int_{-\infty}^\infty f(t)\exp(-ixt)\,\textrm{d}t$$ When is this indeed a function in $\mathbb C$? Most of calculations you get functions in $\mathbb R$. When  in $\mathbb C$? Add: I know there are results like $\frac{e^{ait}-e^ {-ait}}{2i}=\sin(at)$ multiplied by 'anything', but I am asking for a function which you cannot write as a function in $\mathbb R$.",,['calculus']
67,Inequalty with complex numbers,Inequalty with complex numbers,,"i'm being fighting with this for a long, it shouldn't be that hard, can you provide some ideas? Let a,b on $\mathbb{C}$ if |a|<1 and |b|<1 prove: $$\left|\frac{a-b}{1-\bar{a}b}\right| < 1$$ Thanks!","i'm being fighting with this for a long, it shouldn't be that hard, can you provide some ideas? Let a,b on $\mathbb{C}$ if |a|<1 and |b|<1 prove: $$\left|\frac{a-b}{1-\bar{a}b}\right| < 1$$ Thanks!",,['analysis']
68,Suppose $f(x)$ is such that $\int_{-\infty}^\infty e^{tx} f(x)dx = \arcsin (t - \sqrt{1/2})$,Suppose  is such that,f(x) \int_{-\infty}^\infty e^{tx} f(x)dx = \arcsin (t - \sqrt{1/2}),Suppose $f(x)$ is such that $$\int_{-\infty}^\infty e^{tx}  f(x)dx = \arcsin(t - \sqrt{\frac{1}{2}})$$ for all $t$ where the right-side expression is defined. Compute $$\int_{-\infty}^\infty xf(x)dx$$ The subscripts on the integrals are both negative infinity - I wasn't sure how to express that with LaTeX. Also it was supposed to be $\sin^{-1}$ but I didn't know how to write the inverse function with a negative exponent. This is a question in a problem book I'm working through. Thanks.,Suppose $f(x)$ is such that $$\int_{-\infty}^\infty e^{tx}  f(x)dx = \arcsin(t - \sqrt{\frac{1}{2}})$$ for all $t$ where the right-side expression is defined. Compute $$\int_{-\infty}^\infty xf(x)dx$$ The subscripts on the integrals are both negative infinity - I wasn't sure how to express that with LaTeX. Also it was supposed to be $\sin^{-1}$ but I didn't know how to write the inverse function with a negative exponent. This is a question in a problem book I'm working through. Thanks.,,"['calculus', 'analysis', 'integration']"
69,Cauchy Problem for Heat Equation with Holder Continuous Data,Cauchy Problem for Heat Equation with Holder Continuous Data,,"This exercise comes from a past PDE qual problem.  Assume $u(x,t)$ solves $$ \left\{\begin{array}{rl} u_{t}-\Delta u=0&\text{in}\mathbb{R}^{n}\times(0,\infty)\\ u(x,0)=g(x)&\text{on}\mathbb{R}^{n}\times\{t=0\}\end{array}\right. $$ and $g$ is Holder continuous with continuity mode $0<\delta\leq1,$ that is $$|g(x)-g(y)|\leq|x-y|^{\delta}$$ for every $(x,y)\in\mathbb{R}^{n}$.  Prove the estimate $$|u_{t}|+|u_{x_{i}x_{j}}|\leq C_{n}t^{\frac{\delta}{2}-1}.$$ I have quite a few pages of scratch work in trying to prove this estimate, but I have not been able to arrive at a situation where it is even obvious how to exploit the Holder continuity of $g$.  Because of translation invariance in space, we can just prove it for the case $x=0$, so that at least simplifies some things.  But again, there is a key observation that has apparently eluded me, and a hint would be appreciated!","This exercise comes from a past PDE qual problem.  Assume $u(x,t)$ solves $$ \left\{\begin{array}{rl} u_{t}-\Delta u=0&\text{in}\mathbb{R}^{n}\times(0,\infty)\\ u(x,0)=g(x)&\text{on}\mathbb{R}^{n}\times\{t=0\}\end{array}\right. $$ and $g$ is Holder continuous with continuity mode $0<\delta\leq1,$ that is $$|g(x)-g(y)|\leq|x-y|^{\delta}$$ for every $(x,y)\in\mathbb{R}^{n}$.  Prove the estimate $$|u_{t}|+|u_{x_{i}x_{j}}|\leq C_{n}t^{\frac{\delta}{2}-1}.$$ I have quite a few pages of scratch work in trying to prove this estimate, but I have not been able to arrive at a situation where it is even obvious how to exploit the Holder continuity of $g$.  Because of translation invariance in space, we can just prove it for the case $x=0$, so that at least simplifies some things.  But again, there is a key observation that has apparently eluded me, and a hint would be appreciated!",,"['analysis', 'partial-differential-equations']"
70,Polynomial expression of $\frac{\sin x}{x} $,Polynomial expression of,\frac{\sin x}{x} ,"Could you explain to me why $$\frac{\sin x}{x} =\left(1-\frac{x^2}{\pi ^2}\right)\left(1-\frac{x^2}{(2 \pi) ^2}\right)\left(1-\frac{x^2}{(3 \pi )^2}\right)\cdots$$ I've read in this article http://twoplusonet.wordpress.com/2011/06/24/an-elegant-result/ that firstly we write $$\frac{\sin x}{x}\text{ as a polynomial }k\left(1-\frac{x}{a_1}\right)\left(1-\frac{x}{a_2}\right)\left(1-\frac{x}{a_3}\right)\cdots\tag{*}$$ Then, due to the fact that sin$x$ has roots at $_-^+ \pi, _-^+ 2\pi,\ldots$, we can write $$k\left(1-\frac{x}{\pi}\right)\left(1+\frac{x}{\pi}\right)\left(1-\frac{x}{2 \pi}\right)\cdots\tag{**}$$ Since $\lim _{x\rightarrow 0} \frac {\sin x}{x}=1$, we have $k=1$ . Using the difference of two squares we get the formula above. My question is: (why) is that enough? In the article mentioned before the author says that between writing $\frac{\sin x}{x}$ as a polynomial * and (**) we make a ""little jump of faith (and let the analysts deal with the consequences)"". Does that mean something has been omitted here?","Could you explain to me why $$\frac{\sin x}{x} =\left(1-\frac{x^2}{\pi ^2}\right)\left(1-\frac{x^2}{(2 \pi) ^2}\right)\left(1-\frac{x^2}{(3 \pi )^2}\right)\cdots$$ I've read in this article http://twoplusonet.wordpress.com/2011/06/24/an-elegant-result/ that firstly we write $$\frac{\sin x}{x}\text{ as a polynomial }k\left(1-\frac{x}{a_1}\right)\left(1-\frac{x}{a_2}\right)\left(1-\frac{x}{a_3}\right)\cdots\tag{*}$$ Then, due to the fact that sin$x$ has roots at $_-^+ \pi, _-^+ 2\pi,\ldots$, we can write $$k\left(1-\frac{x}{\pi}\right)\left(1+\frac{x}{\pi}\right)\left(1-\frac{x}{2 \pi}\right)\cdots\tag{**}$$ Since $\lim _{x\rightarrow 0} \frac {\sin x}{x}=1$, we have $k=1$ . Using the difference of two squares we get the formula above. My question is: (why) is that enough? In the article mentioned before the author says that between writing $\frac{\sin x}{x}$ as a polynomial * and (**) we make a ""little jump of faith (and let the analysts deal with the consequences)"". Does that mean something has been omitted here?",,"['analysis', 'trigonometry']"
71,Finding zeroes of given function,Finding zeroes of given function,,"If $f(x)$ is a twice differentiable function, continuous in it's domain such that $f(a)=0$ , $f(b)=2$ , $f(c)=-1$ , $f(d)=2$ and $f(e)=0$ where $a<b<c<d<e$ ; Then find the minimum number of zeroes of $g(x)=(f'(x))^{2}+f''(x).f(x)$ $\forall$ $x \in [a,e]$ . I cannot decide how I should proceed. I realize that we will have $f'(z)=0$ for three values of $z$ in $[a,e]$ (using Rolle's theorem, I believe). This will give me: $f''(z).f(z)=0$ How do I proceed from here? I don't think I can differentiate $g(x)=0$ to obtain another equation, since $f'''(x)$ is undefined. Any help is appreciated, thanks!","If is a twice differentiable function, continuous in it's domain such that , , , and where ; Then find the minimum number of zeroes of . I cannot decide how I should proceed. I realize that we will have for three values of in (using Rolle's theorem, I believe). This will give me: How do I proceed from here? I don't think I can differentiate to obtain another equation, since is undefined. Any help is appreciated, thanks!","f(x) f(a)=0 f(b)=2 f(c)=-1 f(d)=2 f(e)=0 a<b<c<d<e g(x)=(f'(x))^{2}+f''(x).f(x) \forall x \in [a,e] f'(z)=0 z [a,e] f''(z).f(z)=0 g(x)=0 f'''(x)",['analysis']
72,Proving Inequality,Proving Inequality,,I stumbled upon the following inequality and I need to prove it. $$\left(1+\frac{a}{b}\right)^x+\left(1+\frac{b}{a}\right)^x\ge 2^{x+1}$$ I am expected to use Holder's Inequality but there seem to be two different Hölder's Inequality. It looks like the one with $1/p+1/q=1$ definitely does not suit here. For different inequalities see the link below. Added : All variables above are positive. P.S. For Inequalities Follow This .,I stumbled upon the following inequality and I need to prove it. $$\left(1+\frac{a}{b}\right)^x+\left(1+\frac{b}{a}\right)^x\ge 2^{x+1}$$ I am expected to use Holder's Inequality but there seem to be two different Hölder's Inequality. It looks like the one with $1/p+1/q=1$ definitely does not suit here. For different inequalities see the link below. Added : All variables above are positive. P.S. For Inequalities Follow This .,,"['analysis', 'inequality', 'contest-math']"
73,Prove that $f'$ exists for all $x$ in $R$ if $f(x+y)=f(x)f(y)$ and $f'(0)$ exists,Prove that  exists for all  in  if  and  exists,f' x R f(x+y)=f(x)f(y) f'(0),"A function $f$ is defined in $R$, and $f'(0)$ exist. Let $f(x+y)=f(x)f(y)$ then prove that $f'$ exists for all $x$ in $R$. I think I have to use two fact: $f'(0)$ exists $f(x+y)=f(x)f(y)$ How to combine these two things to prove that statement?","A function $f$ is defined in $R$, and $f'(0)$ exist. Let $f(x+y)=f(x)f(y)$ then prove that $f'$ exists for all $x$ in $R$. I think I have to use two fact: $f'(0)$ exists $f(x+y)=f(x)f(y)$ How to combine these two things to prove that statement?",,"['analysis', 'derivatives', 'functional-equations']"
74,Why are the integrands dominated by $\alpha f$,Why are the integrands dominated by,\alpha f,"This is on page 32 of Rudin's Real and Complex Analysis , 3rd Edition: Suppose $\mu$ is a positive measure on $X$, $f: X \rightarrow [0, \infty]$ is measurable, $\int_X f d\mu = c$, where $0<c<\infty$, and $\alpha$ is a constant. Prove that$$\lim_{n \rightarrow \infty} \int_X n \log[1+(f/n)^{\alpha}]d \mu = \begin{cases} \infty & \text{ if } 0 < \alpha <1, \\  c & \text{ if } \alpha=1, \\  0 & \text{ if } 1 < \alpha < \infty. \end{cases}$$ The hint says ""if $\alpha \geq 1$, the integrands are dominated by $\alpha f$"". But why? Thanks a lot.","This is on page 32 of Rudin's Real and Complex Analysis , 3rd Edition: Suppose $\mu$ is a positive measure on $X$, $f: X \rightarrow [0, \infty]$ is measurable, $\int_X f d\mu = c$, where $0<c<\infty$, and $\alpha$ is a constant. Prove that$$\lim_{n \rightarrow \infty} \int_X n \log[1+(f/n)^{\alpha}]d \mu = \begin{cases} \infty & \text{ if } 0 < \alpha <1, \\  c & \text{ if } \alpha=1, \\  0 & \text{ if } 1 < \alpha < \infty. \end{cases}$$ The hint says ""if $\alpha \geq 1$, the integrands are dominated by $\alpha f$"". But why? Thanks a lot.",,"['real-analysis', 'analysis']"
75,Application of Inverse function theorem,Application of Inverse function theorem,,"I am stuck on the following problem, and I need any kind of help that leads to solve it: Let $L:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ be an isomorphism and let: $f(x)=L(x)+g(x)$, where: $\left \| g(x) \right \|\leq M\left \| x \right \|^{2}$ and $f\in C^{1}$. Show that $f$ is locally invertible near $0$ What I was trying to do is to show that $Jf(0)\neq 0$. Obviously: $f(0)=L(0)+g(0)=g(0)$ because $L(0)=0$. That's all what I could deduce. Any help?","I am stuck on the following problem, and I need any kind of help that leads to solve it: Let $L:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ be an isomorphism and let: $f(x)=L(x)+g(x)$, where: $\left \| g(x) \right \|\leq M\left \| x \right \|^{2}$ and $f\in C^{1}$. Show that $f$ is locally invertible near $0$ What I was trying to do is to show that $Jf(0)\neq 0$. Obviously: $f(0)=L(0)+g(0)=g(0)$ because $L(0)=0$. That's all what I could deduce. Any help?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
76,Stuck on proving the existence of $\operatorname{diam} E$,Stuck on proving the existence of,\operatorname{diam} E,"I have been attempting to solve this HW problem, from Rosenlicht's Introduction to real analysis (pg. 92, 15th problem): Given a non-empty compact metric space $E$, show that $\max\{d(x,y) \mid x,y \in E \}$ exists. There was a hint provided with the problem, but I am not sure how to utilize it (something along the lines of trying to find sequences $p_n,q_n$ such the $\lim \;d(p_n,q_n) = \sup\{d(p,q) \mid p,q \in E \}$. I guess showing that $\max\{d(x,y) \mid x,y \in E\}$ exists is equivalent to showing that $\{d(x,y) \mid x,y \in E\}$ is compact. I tried to do this by defining function $f_{p_0} = d(x,p_0)$ for some point $p_0 \in E$. Since $E$ is compact, $f_{p_0}(E)$ will be compact, therefore closed and bounded and will have a maximum. I can do this over every point in $E$. But, $E$ could be uncountable, so I will end up with uncountably many functions all of whose images would be compact but the maximum I am looking for would be in the union of all the images (closed and bounded), which need not be closed or bounded. So, I am not sure how to procced at this point. Any suggestions?","I have been attempting to solve this HW problem, from Rosenlicht's Introduction to real analysis (pg. 92, 15th problem): Given a non-empty compact metric space $E$, show that $\max\{d(x,y) \mid x,y \in E \}$ exists. There was a hint provided with the problem, but I am not sure how to utilize it (something along the lines of trying to find sequences $p_n,q_n$ such the $\lim \;d(p_n,q_n) = \sup\{d(p,q) \mid p,q \in E \}$. I guess showing that $\max\{d(x,y) \mid x,y \in E\}$ exists is equivalent to showing that $\{d(x,y) \mid x,y \in E\}$ is compact. I tried to do this by defining function $f_{p_0} = d(x,p_0)$ for some point $p_0 \in E$. Since $E$ is compact, $f_{p_0}(E)$ will be compact, therefore closed and bounded and will have a maximum. I can do this over every point in $E$. But, $E$ could be uncountable, so I will end up with uncountably many functions all of whose images would be compact but the maximum I am looking for would be in the union of all the images (closed and bounded), which need not be closed or bounded. So, I am not sure how to procced at this point. Any suggestions?",,['analysis']
77,"If $[a_n, b_n] \cap [a_m, b_m] \neq \emptyset$ then $\bigcap_{1}^{\infty} [a_n,b_n] \neq \emptyset$",If  then,"[a_n, b_n] \cap [a_m, b_m] \neq \emptyset \bigcap_{1}^{\infty} [a_n,b_n] \neq \emptyset","Let $[a_n,b_n]$, $n=1,2,3,\ldots$, be closed intervals with $[a_n,b_n] \bigcap [a_m,b_m] \neq \emptyset$ for all $n$, $m$. Prove $\bigcap_{1}^{\infty} [a_n,b_n] \neq \emptyset$. I can show by induction that $\bigcap_{1}^N [a_n,b_n] \neq \emptyset$. But I am not sure about the infinity bit, maybe, I am missing something obvius. Any hint guys? Thanks","Let $[a_n,b_n]$, $n=1,2,3,\ldots$, be closed intervals with $[a_n,b_n] \bigcap [a_m,b_m] \neq \emptyset$ for all $n$, $m$. Prove $\bigcap_{1}^{\infty} [a_n,b_n] \neq \emptyset$. I can show by induction that $\bigcap_{1}^N [a_n,b_n] \neq \emptyset$. But I am not sure about the infinity bit, maybe, I am missing something obvius. Any hint guys? Thanks",,['analysis']
78,How does $\Delta f$ behave where $f$ has jump discontinuities in first-order partials?,How does  behave where  has jump discontinuities in first-order partials?,\Delta f f,"In the wake of a prior question , I've solidified my understanding that for a one-dimensional function $f(x)$, when its first derivative has a jump discontinuity of height $h$ at $x_0$, we can regard the second derivative (for the purpose of eventually integrating it) as having a $\delta(x-x_0)$ factor plus a locally continuous function. What if, as in my former question, a continuous function is nonzero everywhere outside of the bounded region $\Omega \subset \mathbb{R}^2$, and its first-order partials have jump discontinuities at the boundary: how then should we derive the form of $\Delta f$ ? My guess is that if we define $\nabla f$ on the boundary by taking the limit from inside $\Omega$, then we'll get something akin to $\Delta f = \delta(0) \| \nabla f \|$ on the boundary, and $0$ otherwise. Or is it possible that when integrating $\Delta f$ over a path hitting a single boundary point $x_0$, the resulting ""$\delta$-factor"" depends on what direction the path hits $x_0$?","In the wake of a prior question , I've solidified my understanding that for a one-dimensional function $f(x)$, when its first derivative has a jump discontinuity of height $h$ at $x_0$, we can regard the second derivative (for the purpose of eventually integrating it) as having a $\delta(x-x_0)$ factor plus a locally continuous function. What if, as in my former question, a continuous function is nonzero everywhere outside of the bounded region $\Omega \subset \mathbb{R}^2$, and its first-order partials have jump discontinuities at the boundary: how then should we derive the form of $\Delta f$ ? My guess is that if we define $\nabla f$ on the boundary by taking the limit from inside $\Omega$, then we'll get something akin to $\Delta f = \delta(0) \| \nabla f \|$ on the boundary, and $0$ otherwise. Or is it possible that when integrating $\Delta f$ over a path hitting a single boundary point $x_0$, the resulting ""$\delta$-factor"" depends on what direction the path hits $x_0$?",,"['analysis', 'multivariable-calculus']"
79,power series radius of convergence,power series radius of convergence,,"How can I prove that if the coefficients $\{a_k\}$ of the power series $\sum_{0}^{\infty} \{a_k\}x^k$ form a bounded sequence, then the radius of convergence is at least 1?","How can I prove that if the coefficients $\{a_k\}$ of the power series $\sum_{0}^{\infty} \{a_k\}x^k$ form a bounded sequence, then the radius of convergence is at least 1?",,['analysis']
80,Why does the following proof of the Nested Interval Property require the Axiom of Completeness?,Why does the following proof of the Nested Interval Property require the Axiom of Completeness?,,"The proof in my text is as such: Let $a_1,a_2,a_3,\ldots$ and $b_1, b_2, b_3,\ldots$ be the labels of the left- and right- hand endpoints respectively. Consider the set A of the left-hand endpoints of the intervals, and let x = sup A. Since x is an upper bound for A, we have $a_n \leq x$. Since each $b_n$ is an upper bound for A, we have $x\leq b_n$. Then since $a_n\leq x \leq b_n$, we can conclude that $x\in I_n$ for every choice of $n\in \mathbb{N}$. Hence x is in the infinite intersection of nested intervals. My question is.. could the proof work with $x = a_n$ instead? It seems that all the key properties would still hold -- $a_n \leq a_n \leq b_n$ for all n. This does not seem to require the AoC to be true.","The proof in my text is as such: Let $a_1,a_2,a_3,\ldots$ and $b_1, b_2, b_3,\ldots$ be the labels of the left- and right- hand endpoints respectively. Consider the set A of the left-hand endpoints of the intervals, and let x = sup A. Since x is an upper bound for A, we have $a_n \leq x$. Since each $b_n$ is an upper bound for A, we have $x\leq b_n$. Then since $a_n\leq x \leq b_n$, we can conclude that $x\in I_n$ for every choice of $n\in \mathbb{N}$. Hence x is in the infinite intersection of nested intervals. My question is.. could the proof work with $x = a_n$ instead? It seems that all the key properties would still hold -- $a_n \leq a_n \leq b_n$ for all n. This does not seem to require the AoC to be true.",,"['real-analysis', 'analysis']"
81,Total variation of integral function,Total variation of integral function,,"Consider a function $f\in L^1([a,b])$ and define $F(x):=\int_a^x f(y)dy$ . I should prove that $V_a^bF=||f||_{L^1([a,b])}$ . My attempt was to proceed by approximation with a test function $\phi$ , but I’m not getting it. Can someone please help me, please?","Consider a function and define . I should prove that . My attempt was to proceed by approximation with a test function , but I’m not getting it. Can someone please help me, please?","f\in L^1([a,b]) F(x):=\int_a^x f(y)dy V_a^bF=||f||_{L^1([a,b])} \phi","['real-analysis', 'analysis', 'measure-theory', 'total-variation']"
82,Understanding the proof of $L^{\infty}$ is complete.,Understanding the proof of  is complete.,L^{\infty},"I got lost when reading the proof of $L^{\infty}$ is complete. The book proceed the proof as follows: We show that each absolutely convergent series in $L^{\infty}(X,\mathscr{A},\mu)$ is convergent. We do this by considering functions (instead of equivalence classes) in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ . Let $\{f_k\}$ be a sequence of functions that belong to $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ and satisfy $\sum_k\|f_k\|<+\infty$ . For each positive integer $k$ , let $N_k=\{x\in X:|f_k(x)|>\|f_k\|_{\infty}\}$ . Then the series $\sum_kf_k(x)$ converges at each $x$ outside $\bigcup_kN_k$ , and the function $f$ defined by \begin{align*} f(x) =  \begin{cases} \sum_kf_k(x)\quad&\text{if $x\notin\bigcup_kN_k$},\\ \\ 0\quad&\text{if $x\in\bigcup_kN_k$} \end{cases} \end{align*} is bounded and $\mathscr{A}$ -measurable. Since $\bigcup_kN_k$ is locally $\mu$ -null, the inequality \begin{align*} \left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \sum_{k=n+1}^{\infty}\|f_k\|_{\infty}\tag1 \end{align*} holds for each $n$ , and so \begin{align*} \lim_{n\to\infty}\left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \lim_{n\to\infty}\sum_{k=n+1}^{\infty}\|f_k\|_{\infty} = 0.\tag2 \end{align*} Thus $L^{\infty}(X,\mathscr{A},\mu)$ is complete. I have a couple of questions about this proof. The definition of $L^p(X,\mathscr{A},\mu)$ says that the elements of $L^p(X,\mathscr{A},\mu)$ are equivalence classes of functions. Why is it legit to proceed the proof by considering functions in $\mathscr{L}^p(X,\mathscr{A},\mu)$ ? In the proof, it says ""the series $\sum_kf_k(x)$ converges at each $x$ outside $\bigcup_kN_k$ "". Here is how I understand this step, and I want to know if it is correct? If $x\notin\bigcup_{k=1}^{\infty}N_k$ , then $|f_k(x)|\leq\|f_k\|_{\infty}$ for all $k\in\mathbb{N}$ , and thus the convergence of $\sum_{k=1}^{\infty}\|f_k\|_{\infty}$ implies that $\sum_{k=1}^{\infty}f_k(x)$ converges by the comparison test. The proof claims that $f$ is $\mathscr{A}$ -measurable. I couldn't see why this is true. I want to show that for each $t\in\mathbb{R}$ the set $\{x\in X:f(x)<t\}\in\mathscr{A}$ . But I honestly don't know how to do this. Can someone please help me out? I got complete lost by inequality (1) and (2). Why does $\bigcup_kN_k$ being locally $\mu$ -null imply that the inequality (1) holds for each $n$ ? Why can we just take limit for both side without proving the limits exist? Please please help! I really appreciate it! Note: $\quad$ The book I am reading defines $\|f\|_{\infty}$ to be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)>M|\}$ is locally $\mu$ -null. Reference: $\quad$ Theorem 3.4.1 from Measure Theory by Donald Cohn","I got lost when reading the proof of is complete. The book proceed the proof as follows: We show that each absolutely convergent series in is convergent. We do this by considering functions (instead of equivalence classes) in . Let be a sequence of functions that belong to and satisfy . For each positive integer , let . Then the series converges at each outside , and the function defined by is bounded and -measurable. Since is locally -null, the inequality holds for each , and so Thus is complete. I have a couple of questions about this proof. The definition of says that the elements of are equivalence classes of functions. Why is it legit to proceed the proof by considering functions in ? In the proof, it says ""the series converges at each outside "". Here is how I understand this step, and I want to know if it is correct? If , then for all , and thus the convergence of implies that converges by the comparison test. The proof claims that is -measurable. I couldn't see why this is true. I want to show that for each the set . But I honestly don't know how to do this. Can someone please help me out? I got complete lost by inequality (1) and (2). Why does being locally -null imply that the inequality (1) holds for each ? Why can we just take limit for both side without proving the limits exist? Please please help! I really appreciate it! Note: The book I am reading defines to be the infimum of those nonnegative numbers such that is locally -null. Reference: Theorem 3.4.1 from Measure Theory by Donald Cohn","L^{\infty} L^{\infty}(X,\mathscr{A},\mu) \mathscr{L}^{\infty}(X,\mathscr{A},\mu) \{f_k\} \mathscr{L}^{\infty}(X,\mathscr{A},\mu) \sum_k\|f_k\|<+\infty k N_k=\{x\in X:|f_k(x)|>\|f_k\|_{\infty}\} \sum_kf_k(x) x \bigcup_kN_k f \begin{align*}
f(x) = 
\begin{cases}
\sum_kf_k(x)\quad&\text{if x\notin\bigcup_kN_k},\\
\\
0\quad&\text{if x\in\bigcup_kN_k}
\end{cases}
\end{align*} \mathscr{A} \bigcup_kN_k \mu \begin{align*}
\left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \sum_{k=n+1}^{\infty}\|f_k\|_{\infty}\tag1
\end{align*} n \begin{align*}
\lim_{n\to\infty}\left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \lim_{n\to\infty}\sum_{k=n+1}^{\infty}\|f_k\|_{\infty} = 0.\tag2
\end{align*} L^{\infty}(X,\mathscr{A},\mu) L^p(X,\mathscr{A},\mu) L^p(X,\mathscr{A},\mu) \mathscr{L}^p(X,\mathscr{A},\mu) \sum_kf_k(x) x \bigcup_kN_k x\notin\bigcup_{k=1}^{\infty}N_k |f_k(x)|\leq\|f_k\|_{\infty} k\in\mathbb{N} \sum_{k=1}^{\infty}\|f_k\|_{\infty} \sum_{k=1}^{\infty}f_k(x) f \mathscr{A} t\in\mathbb{R} \{x\in X:f(x)<t\}\in\mathscr{A} \bigcup_kN_k \mu n \quad \|f\|_{\infty} M \{x\in X:|f(x)>M|\} \mu \quad","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation', 'lp-spaces']"
83,let $f : \mathbb{R} \rightarrow \mathbb{R}^{+} C^2$ s.t. $f' < 0$ and $\sup_x \frac{f(x)f''(x)}{f'(x)^2} < 2$ show that $f=o\left(\frac{1}{x}\right)$,let  s.t.  and  show that,f : \mathbb{R} \rightarrow \mathbb{R}^{+} C^2 f' < 0 \sup_x \frac{f(x)f''(x)}{f'(x)^2} < 2 f=o\left(\frac{1}{x}\right),Problem $$ \text{Let } f : \mathbb{R} \rightarrow \mathbb{R}^+ \text{be a }C^2 \text{ function}\text{ such that } \sup_{x \in \mathbb{R}} \frac{f(x)f''(x)}{f'(x)^2} < 2 \text{ and } f'< 0  $$ $$ \text{Show that } f(x) = o\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty. $$ My attempt : First let's notice the fact that $1-(\frac{f}{f'})' = \frac{ff''}{(f')^2}$ . We have using the first hypothesis : $-(\frac{f}{f'})' \leq 1$ so by integrating we have $-\frac{f(x)}{f'(x)} \leq x - \frac{f(0)}{f'(0)}$ and thus $f'(x) \leq f(x)\frac{1}{x - c}$ then by Gronwall lemma we get $f(x) \leq f(0)\frac{1}{c(c-x)}$ where $c = \frac{f(0)}{f'(0)}$ which proves the fact that $f = O\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty.$ I can't find a way to have a better asymptotic result.,Problem My attempt : First let's notice the fact that . We have using the first hypothesis : so by integrating we have and thus then by Gronwall lemma we get where which proves the fact that I can't find a way to have a better asymptotic result., \text{Let } f : \mathbb{R} \rightarrow \mathbb{R}^+ \text{be a }C^2 \text{ function}\text{ such that } \sup_{x \in \mathbb{R}} \frac{f(x)f''(x)}{f'(x)^2} < 2 \text{ and } f'< 0    \text{Show that } f(x) = o\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty.  1-(\frac{f}{f'})' = \frac{ff''}{(f')^2} -(\frac{f}{f'})' \leq 1 -\frac{f(x)}{f'(x)} \leq x - \frac{f(0)}{f'(0)} f'(x) \leq f(x)\frac{1}{x - c} f(x) \leq f(0)\frac{1}{c(c-x)} c = \frac{f(0)}{f'(0)} f = O\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty.,"['real-analysis', 'analysis', 'derivatives']"
84,Any ideas on how to perform this integral?,Any ideas on how to perform this integral?,,"I can numerically verify that \begin{equation} \int_{-\infty}^{\infty}\mathrm{d}p \frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p} \end{equation} is equal to \begin{equation} -2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi \end{equation} But I do not know how to analytically show it. For context; I happen to obtain this $\int_{-\infty}^{\infty}\mathrm{d}p -\frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p}-2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi$ as an energy of a particle in a model I am working with for parametric region $0<c$ and $3c<2\sin(\phi)$ . I numurically evaluated the integral and saw it is zero. It happens to be zero not only in this parametric region for for all values of $c>0$ , $0<\phi<\pi/2$ , and $c<\sin(\phi)$ . So, I would like to see how to perform the integral. Moreover, I have one more integral of similar structure \begin{equation} \int_{-\infty}^\infty \frac{i\left(e^{c|p|}+e^{2|p| \sin (\phi)}\right) e^{-|p|(c+\sin (\phi))+i p \cos (\phi)}}{p\left(e^{c|p|}+1\right)} \mathrm{d} p \end{equation} which I have not been able to solve.","I can numerically verify that is equal to But I do not know how to analytically show it. For context; I happen to obtain this as an energy of a particle in a model I am working with for parametric region and . I numurically evaluated the integral and saw it is zero. It happens to be zero not only in this parametric region for for all values of , , and . So, I would like to see how to perform the integral. Moreover, I have one more integral of similar structure which I have not been able to solve.","\begin{equation}
\int_{-\infty}^{\infty}\mathrm{d}p \frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p}
\end{equation} \begin{equation}
-2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi
\end{equation} \int_{-\infty}^{\infty}\mathrm{d}p -\frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p}-2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi 0<c 3c<2\sin(\phi) c>0 0<\phi<\pi/2 c<\sin(\phi) \begin{equation}
\int_{-\infty}^\infty \frac{i\left(e^{c|p|}+e^{2|p| \sin (\phi)}\right) e^{-|p|(c+\sin (\phi))+i p \cos (\phi)}}{p\left(e^{c|p|}+1\right)} \mathrm{d} p
\end{equation}","['real-analysis', 'integration', 'analysis', 'improper-integrals', 'indefinite-integrals']"
85,Simple Riemann Integrability Question,Simple Riemann Integrability Question,,"I would like to show that for $a\leq s < t\leq b$ , the function $$f(x) = \begin{cases} 1,\,&\mbox{ if }s<x<t;\\ 0,\,&\mbox{ otherwise} \end{cases}$$ is Riemann-integrable on $[a,b]$ and that this integral has value the $t-s$ . So far, my line of thinking has been to let $P$ be a general partition $a = x_{0}<x_{1}<\ldots<x_{n} = b$ , take $M,N$ to be such that $s\in [x_{M-1},x_{M}]$ and $t\in [x_{N-1},x_{N}]$ , and then write down the lower- and upper-Riemann sums of the function $f$ on $[a,b]$ depending on whether $s\in (x_{M-1},x_{M})$ or $s \in \left\{x_{M-1},x_{M}\right\}$ (and similarly for $t$ ). My question is this: is my line of thinking a good way to approach the problem, and if so, where do I go from here? If my line of thinking is not likely to be productive, please point me in a better direction. Thanks!","I would like to show that for , the function is Riemann-integrable on and that this integral has value the . So far, my line of thinking has been to let be a general partition , take to be such that and , and then write down the lower- and upper-Riemann sums of the function on depending on whether or (and similarly for ). My question is this: is my line of thinking a good way to approach the problem, and if so, where do I go from here? If my line of thinking is not likely to be productive, please point me in a better direction. Thanks!","a\leq s < t\leq b f(x) = \begin{cases}
1,\,&\mbox{ if }s<x<t;\\
0,\,&\mbox{ otherwise}
\end{cases} [a,b] t-s P a = x_{0}<x_{1}<\ldots<x_{n} = b M,N s\in [x_{M-1},x_{M}] t\in [x_{N-1},x_{N}] f [a,b] s\in (x_{M-1},x_{M}) s \in \left\{x_{M-1},x_{M}\right\} t","['real-analysis', 'calculus', 'integration', 'analysis', 'riemann-integration']"
86,Asymptotic estimation of an integral,Asymptotic estimation of an integral,,"I have an integral of the form $$ I = \int\limits_0^1\int\limits^{1}_{0} \exp\left(\dfrac{vt}{(u+v+1)^2 + v^2} - vt\right) dudv. $$ I intend to prove $$ I\leq c t^{-1} $$ holds for the sufficiently large $t$ , where $c$ is a positive constant independent of $t$ . By the way, the numerical integration shows that $c$ exists and is less than $9$ . Can anyone give me some hints or references to prove this estimation?","I have an integral of the form I intend to prove holds for the sufficiently large , where is a positive constant independent of . By the way, the numerical integration shows that exists and is less than . Can anyone give me some hints or references to prove this estimation?","
I = \int\limits_0^1\int\limits^{1}_{0} \exp\left(\dfrac{vt}{(u+v+1)^2 + v^2} - vt\right) dudv.
 
I\leq c t^{-1}
 t c t c 9","['calculus', 'integration', 'analysis', 'definite-integrals']"
87,"Is it rigorous to write $\frac{dy}{dx}$ with ""$y=f(x)$"" instead of $\frac{df(x)}{dx}$ ? Can ""dependent variables"" be defined mathematically?","Is it rigorous to write  with """" instead of  ? Can ""dependent variables"" be defined mathematically?",\frac{dy}{dx} y=f(x) \frac{df(x)}{dx},"In my understanding, to make a rigorous use of the Leibniz notation, one must write either $\frac{df(x)}{dx}$ $ \ \ \ $ i.e. $ \ \ $ $f'$ $ \ \ $ ( which denotes a function ) or $\frac{df(x)}{dx}(a)$ $ \ \ $ or $ \ \ $ $\frac{df(x)}{dx}|_{x=a}$ $ \ \ $ i.e. $ \ $ $f'(a)$ $ \ \ $ (which denotes a value) (where $f$ is a function that is differentiable at $a$ , and $x$ is just a placeholder/a bound variable) Thus, I suppose that writing $\frac{df}{dx}$ for $f'$ or $\frac{df}{dx}|_{x=a}$ for $f'(a)$ is merely a common abuse of notation, since the $\frac d{dx}$ must be followed by a literal expression dependant on $x$ , not by a function. ( See also this question ) What disturbs me is that I sometimes see the notation $\frac{dy}{dx}$ for $\frac{df(x)}{dx}$ where $y=f(x)$ is a ""dependent variable"". How can $y$ have any mathematical meaning ? It's seemingly neither a function (because it equals $f$ evaluated at $x$ ) nor a constant number (because it depends on $x$ ). So it is apparently a weird mathematical object linked by convention to a variable called $x$ . (I think this illustrates one of the big problems with Leibniz notation: it requires assigning fixed letters to the variables of a function, which is bogus since a function should be independent of the name given to its argument. The same problem occurs with the Leibniz notation for partial derivatives: if $f$ is a function $\mathbb{R}^2 \rightarrow \mathbb{R}$ , then unlike the unambiguous notation $\partial_1f$ (for the partial derivative w.r.t. the first argument), the Leibniz notation $\frac{\partial f}{\partial \, r}$ presupposes that the first variable of the function will always be denoted by $r$ ) So is there a rigorous way to define a « dependent variable », or is this just pseudo-mathematical quirkiness ? Edit: What is driving my question is that I have the impression that Leibniz's notation consistently treats everything as variables dependent on each other rather than as functions and arguments. As in the chain rule $\frac{dy}{dx}=\frac{dy}{dg}\frac{dg}{dx}$ that acts as if $y$ depends on $g$ even though $g$ is a function. I would like to know if there is a there is a purely mathematical aspect behind it, perhaps related to something in higher math like maybe manifolds. (I don't know what manifolds are, I don't even necessarily want to try to understand the mathematical definition of a ""dependent variable"", I would just like to know if this rigorous mathematical aspect exists or not). Edit: This thread asks questions similar to mine, but I haven't found satisfactory answers on it.","In my understanding, to make a rigorous use of the Leibniz notation, one must write either i.e. ( which denotes a function ) or or i.e. (which denotes a value) (where is a function that is differentiable at , and is just a placeholder/a bound variable) Thus, I suppose that writing for or for is merely a common abuse of notation, since the must be followed by a literal expression dependant on , not by a function. ( See also this question ) What disturbs me is that I sometimes see the notation for where is a ""dependent variable"". How can have any mathematical meaning ? It's seemingly neither a function (because it equals evaluated at ) nor a constant number (because it depends on ). So it is apparently a weird mathematical object linked by convention to a variable called . (I think this illustrates one of the big problems with Leibniz notation: it requires assigning fixed letters to the variables of a function, which is bogus since a function should be independent of the name given to its argument. The same problem occurs with the Leibniz notation for partial derivatives: if is a function , then unlike the unambiguous notation (for the partial derivative w.r.t. the first argument), the Leibniz notation presupposes that the first variable of the function will always be denoted by ) So is there a rigorous way to define a « dependent variable », or is this just pseudo-mathematical quirkiness ? Edit: What is driving my question is that I have the impression that Leibniz's notation consistently treats everything as variables dependent on each other rather than as functions and arguments. As in the chain rule that acts as if depends on even though is a function. I would like to know if there is a there is a purely mathematical aspect behind it, perhaps related to something in higher math like maybe manifolds. (I don't know what manifolds are, I don't even necessarily want to try to understand the mathematical definition of a ""dependent variable"", I would just like to know if this rigorous mathematical aspect exists or not). Edit: This thread asks questions similar to mine, but I haven't found satisfactory answers on it.","\frac{df(x)}{dx}  \ \ \   \ \  f'  \ \  \frac{df(x)}{dx}(a)  \ \   \ \  \frac{df(x)}{dx}|_{x=a}  \ \   \  f'(a)  \ \  f a x \frac{df}{dx} f' \frac{df}{dx}|_{x=a} f'(a) \frac d{dx} x \frac{dy}{dx} \frac{df(x)}{dx} y=f(x) y f x x x f \mathbb{R}^2 \rightarrow \mathbb{R} \partial_1f \frac{\partial f}{\partial \, r} r \frac{dy}{dx}=\frac{dy}{dg}\frac{dg}{dx} y g g","['calculus', 'analysis', 'derivatives', 'notation']"
88,Why is $\frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||}$?,Why is ?,\frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||},"I came across this chain of inequalities in notes I am reading. $|\sum_{1 \leq n \leq N}e^{2 \pi i \alpha n}| \leq \frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||}$ . Here $\alpha$ is a nonzero real number and $||\alpha ||$ is the distance from $\alpha$ to the nearest integer. The first inequality I was able to verify since it follows from the formula for the sum of a finite geometric series. I am not sure how to verify the 2nd and 3rd inequalities. I tried exploiting that the numerator and denominator of the 2nd term is a difference of two squares and using the relation $e^{i\theta} = \cos \theta + i\sin \theta$ but I didn't have success. No idea why the 3rd one is true either. A similar question which has been answered considers the final inequality, but I did not understand a few steps in the solution. I post it here for reference: $\left|{\sin(\pi \alpha N)}/{\sin(\pi \alpha)}\right| \leq {1}/{2 \| \alpha \|}$","I came across this chain of inequalities in notes I am reading. . Here is a nonzero real number and is the distance from to the nearest integer. The first inequality I was able to verify since it follows from the formula for the sum of a finite geometric series. I am not sure how to verify the 2nd and 3rd inequalities. I tried exploiting that the numerator and denominator of the 2nd term is a difference of two squares and using the relation but I didn't have success. No idea why the 3rd one is true either. A similar question which has been answered considers the final inequality, but I did not understand a few steps in the solution. I post it here for reference: $\left|{\sin(\pi \alpha N)}/{\sin(\pi \alpha)}\right| \leq {1}/{2 \| \alpha \|}$",|\sum_{1 \leq n \leq N}e^{2 \pi i \alpha n}| \leq \frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||} \alpha ||\alpha || \alpha e^{i\theta} = \cos \theta + i\sin \theta,"['analysis', 'exponential-sum']"
89,I can't wrap my head around Peano's 5'th axiom,I can't wrap my head around Peano's 5'th axiom,,"I'm currently reading about Peano's axiom and the construction of the natural numbers. The fifth Axiom of Peano does not make sense to me: If a subset $T$ contains $0$ as an element and for all $n \in T$ , $s(n)$ is also in $T$ , then $T$ is the set of Natural numbers $\mathbb{N}$ . I know that this axiom exists in order to exclude elements which could have the following property: $$s(a) = b$$ $$s(b) = a$$ These elements would not violate axioms $1$ to $4$ however they don't have the desired ""natural number"" properties hence axiom $5$ has been created to allegedly clean these elements up. I also know that axiom $5$ is supposed to work as follows: If $0$ is in our set $T$ then $s(0)$ is also in $T$ but if $s(0)$ is in $T$ then $s(s(0))$ is also in $T$ and so on. Thus we declare this set to be the natural numbers. Now to my actual question: suppose we throw in $a$ and $b$ and $0$ into $T$ . Since we do not yet know what the natural numbers are, we can't exclude $a,b$ either (if my thinking is correct). Then for all $n$ in $T$ the succesor $s(n)$ is also in $T$ . As previously stated $s(0), s(s(0)), ...$ are all in in $T$ (the actual natural numbers) but if such elements $a$ and $b$ are also in T then their successor $s(a)$ and $s(b)$ are also in $T$ which is  a true statement since $s(a) = b$ and $s(b) =a$ . Axiom $5$ should exclude such elements but the way I am stating it, it seems like these elements do not violate axiom $5$ .","I'm currently reading about Peano's axiom and the construction of the natural numbers. The fifth Axiom of Peano does not make sense to me: If a subset contains as an element and for all , is also in , then is the set of Natural numbers . I know that this axiom exists in order to exclude elements which could have the following property: These elements would not violate axioms to however they don't have the desired ""natural number"" properties hence axiom has been created to allegedly clean these elements up. I also know that axiom is supposed to work as follows: If is in our set then is also in but if is in then is also in and so on. Thus we declare this set to be the natural numbers. Now to my actual question: suppose we throw in and and into . Since we do not yet know what the natural numbers are, we can't exclude either (if my thinking is correct). Then for all in the succesor is also in . As previously stated are all in in (the actual natural numbers) but if such elements and are also in T then their successor and are also in which is  a true statement since and . Axiom should exclude such elements but the way I am stating it, it seems like these elements do not violate axiom .","T 0 n \in T s(n) T T \mathbb{N} s(a) = b s(b) = a 1 4 5 5 0 T s(0) T s(0) T s(s(0)) T a b 0 T a,b n T s(n) T s(0), s(s(0)), ... T a b s(a) s(b) T s(a) = b s(b) =a 5 5","['real-analysis', 'analysis', 'elementary-set-theory', 'logic']"
90,Is there any way to prove that $\sqrt {n-1} + \sqrt n + \sqrt {n+1}$ is irrational? [closed],Is there any way to prove that  is irrational? [closed],\sqrt {n-1} + \sqrt n + \sqrt {n+1},"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Before this is marked as a duplicate I just want to say that I've already read a similar thread , where the original poster asked how they would prove that $\sqrt 2 + \sqrt 5 + \sqrt 7$ is an irrational number. I've read the answers to that thread and I couldn't really ""apply""/use them in my situation. Exercise added for reference. Post was edited because it was referring to a particular case (where n was assigned a value such as 6). I'm trying to reopen the topic because I am more curious as to how you'd solve this type of exercise in the general form (like in the image).","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Before this is marked as a duplicate I just want to say that I've already read a similar thread , where the original poster asked how they would prove that is an irrational number. I've read the answers to that thread and I couldn't really ""apply""/use them in my situation. Exercise added for reference. Post was edited because it was referring to a particular case (where n was assigned a value such as 6). I'm trying to reopen the topic because I am more curious as to how you'd solve this type of exercise in the general form (like in the image).",\sqrt 2 + \sqrt 5 + \sqrt 7,"['analysis', 'real-numbers', 'radicals', 'irrational-numbers', 'rational-numbers']"
91,Lp Space: an intuitive understanding.,Lp Space: an intuitive understanding.,,"Would someone be willing to give me an intuitive understanding of the Lp space? I have several analysis books which (seemingly) approach the topic differently, which confuses me more, and a simple Google search hasn't led me to any greater enlightenment either. I would be grateful for any feedback from the community. Thanks in advance.","Would someone be willing to give me an intuitive understanding of the Lp space? I have several analysis books which (seemingly) approach the topic differently, which confuses me more, and a simple Google search hasn't led me to any greater enlightenment either. I would be grateful for any feedback from the community. Thanks in advance.",,"['analysis', 'measure-theory', 'lp-spaces']"
92,Two different roots for $P(x) = x^4+ax^2+bx+c$,Two different roots for,P(x) = x^4+ax^2+bx+c,"Let $a, b, c \in \mathbb{R}$ and $a > 0$ . Also let $P: \mathbb{R} \to \mathbb{R}$ , $P(x) = x^4+ax^2+bx+c$ . Show that the function has at most two different roots. My assumption was to use Bolzano's theorem, but I couldn't figure out how to use it here. Also I'm curious if we could use something like Vieta's formula here? Any help would be appreciated.","Let and . Also let , . Show that the function has at most two different roots. My assumption was to use Bolzano's theorem, but I couldn't figure out how to use it here. Also I'm curious if we could use something like Vieta's formula here? Any help would be appreciated.","a, b, c \in \mathbb{R} a > 0 P: \mathbb{R} \to \mathbb{R} P(x) = x^4+ax^2+bx+c",['real-analysis']
93,Why are these two definite integrals equal?,Why are these two definite integrals equal?,,"How can one prove that, for $0< z<1$ , the two integrals $$\int_0^\infty \frac{u^{z-1}}{1+u}du$$ and $$\int_0^\infty \frac{u^{-z}}{1+u} du$$ are equal? From the integral representation of the beta function $$B(z,w)=\frac12\int_{0}^\infty \frac{u^{z-1}+u^{w-1}}{(1+u)^{z+w}} du$$ If we replace $w$ with $1-z$ , the left hand side equal to $\pi/\sin(\pi z)$ while the right hand side is $$\frac12\int_{0}^\infty \frac{u^{z-1}+u^{-z}}{(1+u)^{z+w}} du$$ this is the reason of my question.","How can one prove that, for , the two integrals and are equal? From the integral representation of the beta function If we replace with , the left hand side equal to while the right hand side is this is the reason of my question.","0< z<1 \int_0^\infty \frac{u^{z-1}}{1+u}du \int_0^\infty \frac{u^{-z}}{1+u} du B(z,w)=\frac12\int_{0}^\infty \frac{u^{z-1}+u^{w-1}}{(1+u)^{z+w}} du w 1-z \pi/\sin(\pi z) \frac12\int_{0}^\infty \frac{u^{z-1}+u^{-z}}{(1+u)^{z+w}} du",['analysis']
94,A question regarding an integrable function,A question regarding an integrable function,,"This question arose while I was reading a paper. Let $f$ be a positive real valued function that is integrable on $\mathbb{R}$ . So,there exists $F > 0$ s.t $$\int_{\mathbb{R}} f(x)dx = F~.$$ Now, the author claims that it's possible to find a function $u$ : $(0,1) \to \mathbb{R}$ s.t $$\dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = t~.$$ Here comes the tricky part, The author claims that, $u$ may be discontinuous but it's strictly increasing ;hence it's differentiable almost everywhere. The latter part of the statement follows from the Lebesgue's Differentiation theorem. But, How does the author claim that $u$ must be strictly increasing? Is it possible for $u$ to be discontinuous? (It's intuitive to think that the existence of such $u$ is possible but I'm interested in a rigorous proof of this claim) My thoughts, Since $f$ is integrable, let $f_0$ be it's anti-derivative so, $$\dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = \dfrac{1}{F} [f_0 (x)]_{-\infty}^{u(t)} = t~.$$ $$\dfrac{1}{F}(f_0 (u(t)) - \lim_{x \to -\infty} f_0(x)) = t$$ From this, it can be concluded that $f_0$ increases as $t$ increases. But, can we conclude $u$ increases as well?","This question arose while I was reading a paper. Let be a positive real valued function that is integrable on . So,there exists s.t Now, the author claims that it's possible to find a function : s.t Here comes the tricky part, The author claims that, may be discontinuous but it's strictly increasing ;hence it's differentiable almost everywhere. The latter part of the statement follows from the Lebesgue's Differentiation theorem. But, How does the author claim that must be strictly increasing? Is it possible for to be discontinuous? (It's intuitive to think that the existence of such is possible but I'm interested in a rigorous proof of this claim) My thoughts, Since is integrable, let be it's anti-derivative so, From this, it can be concluded that increases as increases. But, can we conclude increases as well?","f \mathbb{R} F > 0 \int_{\mathbb{R}} f(x)dx = F~. u (0,1) \to \mathbb{R} \dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = t~. u u u u f f_0 \dfrac{1}{F} \int_{-\infty}^{u(t)} f(x)dx = \dfrac{1}{F} [f_0 (x)]_{-\infty}^{u(t)} = t~. \dfrac{1}{F}(f_0 (u(t)) - \lim_{x \to -\infty} f_0(x)) = t f_0 t u","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
95,trace 0 implies product of symmetric and skew symmetric,trace 0 implies product of symmetric and skew symmetric,,"If $A$ is symmetric and $B$ is skew-symmetric, then $tr(AB)=0$ . I would like to know if the converse holds true. If not, can you give me an example?","If is symmetric and is skew-symmetric, then . I would like to know if the converse holds true. If not, can you give me an example?",A B tr(AB)=0,"['linear-algebra', 'analysis']"
96,Element of area: mistake in the book?,Element of area: mistake in the book?,,"In the book of Miles Reid and Balazs Szendroi ""Geometry and Topology"" authors consider the hyperbolic geometry on the hyperboloid $x^2+y^2-t^2=-1$ and claim in Ex. 3.22 (b) that an element of area is $\frac{dxdy}{t}$ . They also compute it in polar coordinates (basically just multiplying by Jacobian). However a well-known formula for area element $$\sqrt{1+f_x^2+f_y^2}dxdy$$ with $f(x,y)=t=\sqrt{1+x^2+y^2}$ clearly gives completely different result. Is it a mistake? Thank you.","In the book of Miles Reid and Balazs Szendroi ""Geometry and Topology"" authors consider the hyperbolic geometry on the hyperboloid and claim in Ex. 3.22 (b) that an element of area is . They also compute it in polar coordinates (basically just multiplying by Jacobian). However a well-known formula for area element with clearly gives completely different result. Is it a mistake? Thank you.","x^2+y^2-t^2=-1 \frac{dxdy}{t} \sqrt{1+f_x^2+f_y^2}dxdy f(x,y)=t=\sqrt{1+x^2+y^2}","['integration', 'analysis', 'differential-geometry', 'riemannian-geometry', 'hyperbolic-geometry']"
97,"How to solve the PDE $u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x)$, $u(t,0)=Ae^{-t^2}\sin{\omega t}$","How to solve the PDE ,","u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x) u(t,0)=Ae^{-t^2}\sin{\omega t}","The following PDE is given: $u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x)$ , $u(t,0)=Ae^{-t^2}\sin{\omega t}$ May I ask you for hints about how to solve that PDE ? I don't think that I can use the method of characteristics, because there is a second derivative ( $u_{tt}$ ). Separation of variables doesn't seem to be an option either. And also, don't we need a second initial condition if we have a second partial derivative ? I am truly lost, and would appreciate any hint/help. Edit: separation of variables seems to work, but the initial condition makes things complicated. If we do so, $A$ would depend on $t$ , which cannot be.","The following PDE is given: , May I ask you for hints about how to solve that PDE ? I don't think that I can use the method of characteristics, because there is a second derivative ( ). Separation of variables doesn't seem to be an option either. And also, don't we need a second initial condition if we have a second partial derivative ? I am truly lost, and would appreciate any hint/help. Edit: separation of variables seems to work, but the initial condition makes things complicated. If we do so, would depend on , which cannot be.","u_{tt}(t,x)=u(t,x)+12u_t(t,x)+12u_x(t,x) u(t,0)=Ae^{-t^2}\sin{\omega t} u_{tt} A t",['real-analysis']
98,Integrals depending on a parameter: $\int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx $,Integrals depending on a parameter:,\int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx ,"I'm trying to calculate this integral: $$ \int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx $$ For $a > 0$ . This is what I did: $$ I(a) = \int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx \\\ I'(a) = \int_{0}^{\pi/2} \frac{2a\sin^2{x}}{(a^2\sin^2{x} + \cos^2{x})} dx \\\   I'(a) = \int_{0}^{\pi/2} \frac{2a}{(a^2 + \frac{\cos^2{x}}{\sin^2{x}})} dx  \\\ I'(a) = 2a \int_{0}^{\pi/2} \frac{1}{a^2 + \cot^2{x}}dx  \\\ I'(a) = \frac{2}{a} \int_{0}^{\pi/2} \frac{1}{1 + \frac{\cot^2{x}}{a^2}}dx $$ Here I tried to substitude $\frac{\cot^2{x}}{a^2} = t$ . This should lead me to $\arctan(something)$ (I write $\arctan{x} = \tan^{-1}{x}$ .) But I got stuck. After I get the derivative I should integrate it back to get $I(a)$ . Also there are steps that need some prepositions to be checked. Could one help me with this integral and also to clarify the steps that need special attention; such as the second step, where I can get the derivative $I'(a)$ only if the function under the integral can be differentiated? It must be possible to find its derivative.","I'm trying to calculate this integral: For . This is what I did: Here I tried to substitude . This should lead me to (I write .) But I got stuck. After I get the derivative I should integrate it back to get . Also there are steps that need some prepositions to be checked. Could one help me with this integral and also to clarify the steps that need special attention; such as the second step, where I can get the derivative only if the function under the integral can be differentiated? It must be possible to find its derivative."," \int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx  a > 0  I(a) = \int_{0}^{\pi/2} \ln(a^2\sin^2{x} + \cos^2{x})dx \\\
I'(a) = \int_{0}^{\pi/2} \frac{2a\sin^2{x}}{(a^2\sin^2{x} + \cos^2{x})} dx \\\  
I'(a) = \int_{0}^{\pi/2} \frac{2a}{(a^2 + \frac{\cos^2{x}}{\sin^2{x}})} dx  \\\
I'(a) = 2a \int_{0}^{\pi/2} \frac{1}{a^2 + \cot^2{x}}dx  \\\
I'(a) = \frac{2}{a} \int_{0}^{\pi/2} \frac{1}{1 + \frac{\cot^2{x}}{a^2}}dx  \frac{\cot^2{x}}{a^2} = t \arctan(something) \arctan{x} = \tan^{-1}{x} I(a) I'(a)","['real-analysis', 'calculus', 'analysis']"
99,$\underset{x\rightarrow\infty}{\lim}\frac{f(x)}{x}=0$ Implies $\underset{x\rightarrow\infty}{f'(x)}=0$ [duplicate],Implies  [duplicate],\underset{x\rightarrow\infty}{\lim}\frac{f(x)}{x}=0 \underset{x\rightarrow\infty}{f'(x)}=0,"This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) Closed 5 years ago . Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuously differentiable function such that $\underset{x\rightarrow\infty}{\lim}\frac{f(x)}{x}=0$ and suppose $\underset{x\rightarrow\infty}{f'(x)}$ exists. Then Prove that $\underset{x\rightarrow\infty}{f'(x)}=0$ I can see that if we apply L'hoptal's theorem directly to $\frac{f(x)}{x}$ then we can get the answer. But is it possible to do so without knowing the value of $\underset{x\rightarrow\infty}{f(x)}$ On the similar problem: found here , they have given the existence of $\lim_{x\rightarrow\infty} f(x)$ . But in this particular problem they haven't","This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) Closed 5 years ago . Let be a continuously differentiable function such that and suppose exists. Then Prove that I can see that if we apply L'hoptal's theorem directly to then we can get the answer. But is it possible to do so without knowing the value of On the similar problem: found here , they have given the existence of . But in this particular problem they haven't",f:\mathbb{R}\rightarrow\mathbb{R} \underset{x\rightarrow\infty}{\lim}\frac{f(x)}{x}=0 \underset{x\rightarrow\infty}{f'(x)} \underset{x\rightarrow\infty}{f'(x)}=0 \frac{f(x)}{x} \underset{x\rightarrow\infty}{f(x)} \lim_{x\rightarrow\infty} f(x),"['real-analysis', 'calculus', 'analysis']"
