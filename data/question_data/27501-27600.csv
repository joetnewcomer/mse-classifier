,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Internal direct sum of infinitely many subspaces,Internal direct sum of infinitely many subspaces,,"I understand what the (external) direct product of an infinite number of vector spaces. I understand the internal direct sum of a finite number of vector spaces. I think I understand what an external direct sum of vector spaces is. My question is, Does it make sense to talk about an internal direct sum of infinitely many subspaces of a vector space? If so, how is this defined?","I understand what the (external) direct product of an infinite number of vector spaces. I understand the internal direct sum of a finite number of vector spaces. I think I understand what an external direct sum of vector spaces is. My question is, Does it make sense to talk about an internal direct sum of infinitely many subspaces of a vector space? If so, how is this defined?",,"['linear-algebra', 'vector-spaces', 'definition', 'direct-sum']"
1,Can a matrix $A$ commute with $e^B$ without commuting with $B$?,Can a matrix  commute with  without commuting with ?,A e^B B,"As in the title. Is it possible that $[A,B]\neq0$ , but $[A,e^B]=0$ ? I tried expanding the exponential and using $[A,B^n]=\sum_k {n\choose k} B^{n-k}[A,B]B^k $ but this doesn't seem to give any insight. I'm inclined to think the answer is yes, because a sum of terms being $0$ is a weaker requirement than each term being $0$ , but I was wondering if there's a clearer way to see it. EDIT: in light of lisyarus' answer, what if the matrices in question are hermitian and have real eigenvalues?","As in the title. Is it possible that , but ? I tried expanding the exponential and using but this doesn't seem to give any insight. I'm inclined to think the answer is yes, because a sum of terms being is a weaker requirement than each term being , but I was wondering if there's a clearer way to see it. EDIT: in light of lisyarus' answer, what if the matrices in question are hermitian and have real eigenvalues?","[A,B]\neq0 [A,e^B]=0 [A,B^n]=\sum_k {n\choose k} B^{n-k}[A,B]B^k  0 0","['linear-algebra', 'matrices', 'matrix-exponential']"
2,Proof involving the spectral radius and the Jordan canonical form,Proof involving the spectral radius and the Jordan canonical form,,"Let $A$ be a square matrix. Show that if $$\lim_{n \to \infty} A^{n} = 0$$ then $\rho(A) < 1$ , where $\rho(A)$ denotes the spectral radius of $A$ . Hint: Use the Jordan canonical form. I am self-studying and have been working through a few linear algebra exercises. I'm struggling a bit in applying the hint to this problem — I don't know where to start. Any help appreciated.","Let be a square matrix. Show that if then , where denotes the spectral radius of . Hint: Use the Jordan canonical form. I am self-studying and have been working through a few linear algebra exercises. I'm struggling a bit in applying the hint to this problem — I don't know where to start. Any help appreciated.",A \lim_{n \to \infty} A^{n} = 0 \rho(A) < 1 \rho(A) A,"['linear-algebra', 'matrices', 'jordan-normal-form', 'spectral-radius']"
3,Write $\cos^2(x)$ as linear combination of $x \mapsto \sin(x)$ and $x \mapsto \cos(x)$,Write  as linear combination of  and,\cos^2(x) x \mapsto \sin(x) x \mapsto \cos(x),"Can we write $\cos^2(x)$ as linear combination of $x \mapsto \sin(x)$ and $x \mapsto \cos(x)$ ? I know $$ \cos^2(x) = \frac{\cos(2x) + 1}{2} = 1 - \sin^2(x) = \cos(2x) + \sin^2(x) $$ but none of these helped. Then, I tried to solve $$ \cos^2(x) = \alpha \sin(x) + \beta \cos(x) $$ for the coefficients $\alpha, \beta \in \mathbb{R}$ . But when plugging in $x = 0$ I get $\beta = 1$ and for $x = \frac{\pi}{2}$ I get $\alpha = 0$ . Plugging those values back in I obtain a false statement, and WolframAlpha can't do better! This is from a numerical analysis exam and the second function is $x \mapsto \sqrt{2}\cos\left(\frac{\pi}{4} - x \right)$ , which can easily be expressed in terms of $x \mapsto \sin(x)$ and $x \mapsto \cos(x)$ by the corresponding addition formula.","Can we write as linear combination of and ? I know but none of these helped. Then, I tried to solve for the coefficients . But when plugging in I get and for I get . Plugging those values back in I obtain a false statement, and WolframAlpha can't do better! This is from a numerical analysis exam and the second function is , which can easily be expressed in terms of and by the corresponding addition formula.","\cos^2(x) x \mapsto \sin(x) x \mapsto \cos(x) 
\cos^2(x)
= \frac{\cos(2x) + 1}{2}
= 1 - \sin^2(x)
= \cos(2x) + \sin^2(x)
 
\cos^2(x) = \alpha \sin(x) + \beta \cos(x)
 \alpha, \beta \in \mathbb{R} x = 0 \beta = 1 x = \frac{\pi}{2} \alpha = 0 x \mapsto \sqrt{2}\cos\left(\frac{\pi}{4} - x \right) x \mapsto \sin(x) x \mapsto \cos(x)","['linear-algebra', 'algebra-precalculus', 'trigonometry']"
4,Using $\bar{A}$ ($A$ modulo $2$) to prove that $A$ is invertible,Using  ( modulo ) to prove that  is invertible,\bar{A} A 2 A,"Following this website, https://yutsumura.com/how-to-prove-a-matrix-is-nonsingular-in-10-seconds/ : Let $\bar{A}$ be the matrix whose $(i,j)$ -entry is the $(i,j)$ -entry of $A$ modulo $2$ . That is $\bar{A}=\begin{bmatrix}     1 & 0 & 0 & 0 \\     0 & 1 & 0 & 0 \\     0 & 0 & 1 & 0 \\     0 & 0 & 0 & 1 \end{bmatrix}$ Since $\det(A)$ is a polynomial of entries of $A$ , we have $$\det(A)=\det(\bar{A}) (\text{mod} \ 2)= 1$$ I cannot see how we get the equality $\det(A)=\det(\bar{A}) (\text{mod} \ 2)$ just because $\det(A)$ is a polynomial of entries of $A$ .","Following this website, https://yutsumura.com/how-to-prove-a-matrix-is-nonsingular-in-10-seconds/ : Let be the matrix whose -entry is the -entry of modulo . That is Since is a polynomial of entries of , we have I cannot see how we get the equality just because is a polynomial of entries of .","\bar{A} (i,j) (i,j) A 2 \bar{A}=\begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
\end{bmatrix} \det(A) A \det(A)=\det(\bar{A}) (\text{mod} \ 2)= 1 \det(A)=\det(\bar{A}) (\text{mod} \ 2) \det(A) A","['linear-algebra', 'matrices', 'polynomials', 'modular-arithmetic', 'determinant']"
5,Real matrix $A_{3\times 3}$ such that $\operatorname{tr(}A)=0$ and $A^2+A^T=I$?,Real matrix  such that  and ?,A_{3\times 3} \operatorname{tr(}A)=0 A^2+A^T=I,"I'm dealing with the test of the International Mathematics Competition for University Students, 2011, and I've had a lot of difficulties, so I hope someone could help me to discuss the questions. The question 2 says: Does exist a real matrix $A_{3\times 3}$ such that $\operatorname{tr}(A)=0$ and $A^2+A^T=I$ ? The only thing I could get in that problem is that if $A$ exists, so $\operatorname{tr}(A^2)=3$ , because $\operatorname{tr}(A^2+A^T)=\operatorname{tr}(I)\Longrightarrow $ $\operatorname{tr}(A^2)+\operatorname{tr}(A^T)=3\Longrightarrow $ $\operatorname{tr}(A^2)+\operatorname{tr}(A)=3\Longrightarrow $ $\operatorname{tr}(A^2)=3$ Thanks for the help.","I'm dealing with the test of the International Mathematics Competition for University Students, 2011, and I've had a lot of difficulties, so I hope someone could help me to discuss the questions. The question 2 says: Does exist a real matrix such that and ? The only thing I could get in that problem is that if exists, so , because Thanks for the help.",A_{3\times 3} \operatorname{tr}(A)=0 A^2+A^T=I A \operatorname{tr}(A^2)=3 \operatorname{tr}(A^2+A^T)=\operatorname{tr}(I)\Longrightarrow  \operatorname{tr}(A^2)+\operatorname{tr}(A^T)=3\Longrightarrow  \operatorname{tr}(A^2)+\operatorname{tr}(A)=3\Longrightarrow  \operatorname{tr}(A^2)=3,"['linear-algebra', 'contest-math', 'trace']"
6,Eigenvalue and Eigenvector of $\small\pmatrix{0 & 0 \\ 0 & -7}$,Eigenvalue and Eigenvector of,\small\pmatrix{0 & 0 \\ 0 & -7},"I need help working out the eigenvectors for this matrix. $ \begin {pmatrix} 0 &  0 \\   0 & -7 \end{pmatrix} $ The original matrix is $ \begin {pmatrix} 5 &  0 \\   0 & -2 \end{pmatrix} $ , eigenvalues are 5,-2, but I am not sure how to about the eigenvectors, as for 5 $ \begin {pmatrix} 0 &  0 \\   0 & -7 \end{pmatrix} $ $ \begin{pmatrix} x \\ y \end{pmatrix}$ = $ \begin{pmatrix} 0 \\ 0 \end{pmatrix}$ from the first equation, $x$ and $y$ are both zero, but from the second equation $y = 0$, so what is the eigenvector?","I need help working out the eigenvectors for this matrix. $ \begin {pmatrix} 0 &  0 \\   0 & -7 \end{pmatrix} $ The original matrix is $ \begin {pmatrix} 5 &  0 \\   0 & -2 \end{pmatrix} $ , eigenvalues are 5,-2, but I am not sure how to about the eigenvectors, as for 5 $ \begin {pmatrix} 0 &  0 \\   0 & -7 \end{pmatrix} $ $ \begin{pmatrix} x \\ y \end{pmatrix}$ = $ \begin{pmatrix} 0 \\ 0 \end{pmatrix}$ from the first equation, $x$ and $y$ are both zero, but from the second equation $y = 0$, so what is the eigenvector?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
7,the set of all complex numbers constitutes a one-dimensional complex vector space.,the set of all complex numbers constitutes a one-dimensional complex vector space.,,"Show that the set of all real numbers, with the usual addition and multiplication, constitutes a one-dimensional real vector space, and the set of all complex numbers constitutes a one-dimensional complex vector space. I know that all properties to be vector space are fulfilled in real and complex but I have difficulty is in the dimension and the base of each vector space respectively. Scalars in the vector space of real numbers are real numbers and likewise with complexes? The basis for both spaces is $\{1\}$ or for the real ones it is $\{1\}$ and for the complexes it is $\{i\}$? How does one prove that these spaces have dimension $1$?  Thank you very much.","Show that the set of all real numbers, with the usual addition and multiplication, constitutes a one-dimensional real vector space, and the set of all complex numbers constitutes a one-dimensional complex vector space. I know that all properties to be vector space are fulfilled in real and complex but I have difficulty is in the dimension and the base of each vector space respectively. Scalars in the vector space of real numbers are real numbers and likewise with complexes? The basis for both spaces is $\{1\}$ or for the real ones it is $\{1\}$ and for the complexes it is $\{i\}$? How does one prove that these spaces have dimension $1$?  Thank you very much.",,"['calculus', 'real-analysis', 'linear-algebra', 'functional-analysis', 'complex-numbers']"
8,How to determine the value of a variable in a matrix to make it linearly independent of two other given matrices.,How to determine the value of a variable in a matrix to make it linearly independent of two other given matrices.,,"I am given matrices: \begin{pmatrix}1&2\\ 0&1\end{pmatrix}\begin{pmatrix}1&0\\ \:1&0\end{pmatrix}\begin{pmatrix}1&0\\ a\:&-2\end{pmatrix} and asked to determine the value of $a$ such that the above matrices are linearly independent . I know how to work with vectors: put them together into a matrix and use row reduction, if no free columns then the vectors are linearly independent. But how to do this with matrices? Thank you.","I am given matrices: \begin{pmatrix}1&2\\ 0&1\end{pmatrix}\begin{pmatrix}1&0\\ \:1&0\end{pmatrix}\begin{pmatrix}1&0\\ a\:&-2\end{pmatrix} and asked to determine the value of $a$ such that the above matrices are linearly independent . I know how to work with vectors: put them together into a matrix and use row reduction, if no free columns then the vectors are linearly independent. But how to do this with matrices? Thank you.",,"['linear-algebra', 'matrices']"
9,How to express a matrix as sum of two square zero matrices,How to express a matrix as sum of two square zero matrices,,"I have a real square matrix $M$ that I'd like to express as $M=A+B$ such that $A^2=0$,$B^2=0$. $M$ has an additional property that $M^2$ is a scalar matrix : ($M^2=s^2I$); and it's dimension is a power of 2 : $dim(M)=2^n,n>0$; Any suggestions?","I have a real square matrix $M$ that I'd like to express as $M=A+B$ such that $A^2=0$,$B^2=0$. $M$ has an additional property that $M^2$ is a scalar matrix : ($M^2=s^2I$); and it's dimension is a power of 2 : $dim(M)=2^n,n>0$; Any suggestions?",,"['linear-algebra', 'matrices']"
10,In Euclidean space interior of finite set is empty?,In Euclidean space interior of finite set is empty?,,"I seem to be somewhat lost in certain concepts. I'm asked to prove that if $W\subset\mathbb{R}^n$ is a linear subspace (a vector subspace) of $\mathbb{R}^n$, with $W\ne \mathbb{R}^n$, then the interior of $W$ is the empty set. But how is this possible? Take, for example, $\mathbb{R}^2\subset \mathbb{R}^3$. Consider an open ball $B(0;r)$ of radius $r$ around $0$ in $\mathbb{R}^2$. Don't we then have that the interior of $B(0;r)$ is $B(0;r)$ itself? Also, for every $r>0$ and every $x\in\mathbb{R}^2$, $B(x;r)\in\mathbb{R}^2$, so that the interior of $\mathbb{R}^2$ is $\mathbb{R}^2$. Also, in Wikipedia it is said that the interior of any finite subset of a Eucledian space is empty. Again, I don't see how this is possible. What is it that I'm missing here?","I seem to be somewhat lost in certain concepts. I'm asked to prove that if $W\subset\mathbb{R}^n$ is a linear subspace (a vector subspace) of $\mathbb{R}^n$, with $W\ne \mathbb{R}^n$, then the interior of $W$ is the empty set. But how is this possible? Take, for example, $\mathbb{R}^2\subset \mathbb{R}^3$. Consider an open ball $B(0;r)$ of radius $r$ around $0$ in $\mathbb{R}^2$. Don't we then have that the interior of $B(0;r)$ is $B(0;r)$ itself? Also, for every $r>0$ and every $x\in\mathbb{R}^2$, $B(x;r)\in\mathbb{R}^2$, so that the interior of $\mathbb{R}^2$ is $\mathbb{R}^2$. Also, in Wikipedia it is said that the interior of any finite subset of a Eucledian space is empty. Again, I don't see how this is possible. What is it that I'm missing here?",,"['real-analysis', 'linear-algebra', 'general-topology', 'vector-spaces']"
11,Determine the eigenvector and eigenspace and the basis of the eigenspace,Determine the eigenvector and eigenspace and the basis of the eigenspace,,"The yellow marked area is correct, so don't check for accuracy :) $A=\begin{pmatrix} 0 & -1 & 0\\  4 &  4 & 0\\  2 &  1 & 2 \end{pmatrix}$ is the matrix. Characteristic polynomial is $-\lambda^{3}+6\lambda^{2}-12\lambda+8=0$ The (tripple) eigenvalue is $\lambda=2$. Calculate the eigenvectors now: $\begin{pmatrix} -2 & -1 & 0\\  4  &  2 & 0\\  2  &  1 & 0 \end{pmatrix} \begin{pmatrix} x\\  y\\  z \end{pmatrix}= \begin{pmatrix} 0\\  0\\  0 \end{pmatrix}$ We get the equations: $I: -2x-y=0 \Leftrightarrow y = -2x$ $II: 4x+2y=0$ $III: 2x+y=0 \Leftrightarrow 2x-2x=0 \Leftrightarrow 0=0$ We see that in every eequation $z$ is unknown, so we can choose an arbitrary $z$. $x\begin{pmatrix} 1\\  -2\\  z \end{pmatrix}$ and this is the eigenspace...? And what is the basis of this eigenspace? Can I just set $x=1$ and some value for $z$? So this would be a correct basis of the eigenspace: $\begin{pmatrix} 1\\  -2\\  3 \end{pmatrix}$? Now we need three linearly independent eigenvectors but I couldn't find them as I always got linearly dependent vectors... I need a detailled, not too complicated answer that explains it well and I will give that answer a nice bounty (up to 200 rep) because I couldn't find another site explaining this correctly to me and I'm really in need of it.","The yellow marked area is correct, so don't check for accuracy :) $A=\begin{pmatrix} 0 & -1 & 0\\  4 &  4 & 0\\  2 &  1 & 2 \end{pmatrix}$ is the matrix. Characteristic polynomial is $-\lambda^{3}+6\lambda^{2}-12\lambda+8=0$ The (tripple) eigenvalue is $\lambda=2$. Calculate the eigenvectors now: $\begin{pmatrix} -2 & -1 & 0\\  4  &  2 & 0\\  2  &  1 & 0 \end{pmatrix} \begin{pmatrix} x\\  y\\  z \end{pmatrix}= \begin{pmatrix} 0\\  0\\  0 \end{pmatrix}$ We get the equations: $I: -2x-y=0 \Leftrightarrow y = -2x$ $II: 4x+2y=0$ $III: 2x+y=0 \Leftrightarrow 2x-2x=0 \Leftrightarrow 0=0$ We see that in every eequation $z$ is unknown, so we can choose an arbitrary $z$. $x\begin{pmatrix} 1\\  -2\\  z \end{pmatrix}$ and this is the eigenspace...? And what is the basis of this eigenspace? Can I just set $x=1$ and some value for $z$? So this would be a correct basis of the eigenspace: $\begin{pmatrix} 1\\  -2\\  3 \end{pmatrix}$? Now we need three linearly independent eigenvectors but I couldn't find them as I always got linearly dependent vectors... I need a detailled, not too complicated answer that explains it well and I will give that answer a nice bounty (up to 200 rep) because I couldn't find another site explaining this correctly to me and I'm really in need of it.",,"['linear-algebra', 'matrices', 'algebra-precalculus', 'eigenvalues-eigenvectors']"
12,"Order of $GL(n,\mathbb{Z}/m \mathbb{Z})$",Order of,"GL(n,\mathbb{Z}/m \mathbb{Z})","If $p$ is a prime, then the order of the general linear group $GL(n,F_p)$ is given by $$(p^n-1)(p^n-p) \cdots (p^n-p^{n-1}).$$ But what if we consider the general linear group over the rings $\mathbb{Z}/m\mathbb{Z}$ where $m$ is any integer? What will be its order?","If $p$ is a prime, then the order of the general linear group $GL(n,F_p)$ is given by $$(p^n-1)(p^n-p) \cdots (p^n-p^{n-1}).$$ But what if we consider the general linear group over the rings $\mathbb{Z}/m\mathbb{Z}$ where $m$ is any integer? What will be its order?",,['linear-algebra']
13,Is this solution correct? Eigenvector problem.,Is this solution correct? Eigenvector problem.,,"Find the eigenvectors of :$$\begin{bmatrix}0&1&0&0\\1&0&0&0\\0&0&0&1\\0&0&1&0\end{bmatrix}$$ Finding the characteristic equation, we can write \begin{align*} det(A-\lambda I) &= 0 \\ \begin{vmatrix}-\lambda&1&0&0\\1&-\lambda&0&0\\0&0&-\lambda&1\\0&0&1&-\lambda\end{vmatrix}&=0\\ -\lambda\begin{vmatrix}-\lambda&0&0\\0&-\lambda&1\\0&1&-\lambda\end{vmatrix} - \begin{vmatrix}1&0&0\\0&-\lambda&1\\0&1&-\lambda\end{vmatrix} &= 0\\ -\lambda(-\lambda(\lambda^2-1))-(\lambda^2-1) &= 0\\ -\lambda(-\lambda^3+\lambda)-\lambda^2+1 &= 0 \\ \lambda^4-\lambda^2-\lambda^2+1&=0\\ \lambda^2(\lambda^2-1)-1(\lambda^2-1)&=0\\ (\lambda^2-1)(\lambda^2-1) &= 0 \\ \lambda &= -1, 1\\ \end{align*} Now, finding the eigenvectors \begin{align*} \lambda = 1, \begin{bmatrix}-1&1&0&0\\1&-1&0&0\\0&0&-1&1\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&0&0\\0&0&-1&1\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&1&-1\\0&0&0&0\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&1&-1\\0&0&0&0\\0&0&0&0\end{bmatrix}\vec{v}_1&= 0\\ \therefore \vec{v} &= (1,1,0,0)\\ \lambda = -1, \begin{bmatrix}1&1&0&0\\1&1&0&0\\0&0&1&1\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&0&0\\0&0&1&1\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&1&1\\0&0&0&0\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&1&1\\0&0&0&0\\0&0&0&0\end{bmatrix}\vec{v}_1&= 0\\ \therefore \vec{v} &= (-1,-1,0,0)\\ \end{align*} Therefore, all of this matrix's eigenvectors are spanned by (1,1,0,0) and (-1,-1,0,0). Is this correct? I feel like there should be more, specifically (1,-1,0,0) corresponding to $\lambda = -1$, but I could be wrong.","Find the eigenvectors of :$$\begin{bmatrix}0&1&0&0\\1&0&0&0\\0&0&0&1\\0&0&1&0\end{bmatrix}$$ Finding the characteristic equation, we can write \begin{align*} det(A-\lambda I) &= 0 \\ \begin{vmatrix}-\lambda&1&0&0\\1&-\lambda&0&0\\0&0&-\lambda&1\\0&0&1&-\lambda\end{vmatrix}&=0\\ -\lambda\begin{vmatrix}-\lambda&0&0\\0&-\lambda&1\\0&1&-\lambda\end{vmatrix} - \begin{vmatrix}1&0&0\\0&-\lambda&1\\0&1&-\lambda\end{vmatrix} &= 0\\ -\lambda(-\lambda(\lambda^2-1))-(\lambda^2-1) &= 0\\ -\lambda(-\lambda^3+\lambda)-\lambda^2+1 &= 0 \\ \lambda^4-\lambda^2-\lambda^2+1&=0\\ \lambda^2(\lambda^2-1)-1(\lambda^2-1)&=0\\ (\lambda^2-1)(\lambda^2-1) &= 0 \\ \lambda &= -1, 1\\ \end{align*} Now, finding the eigenvectors \begin{align*} \lambda = 1, \begin{bmatrix}-1&1&0&0\\1&-1&0&0\\0&0&-1&1\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&0&0\\0&0&-1&1\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&1&-1\\0&0&0&0\\0&0&1&-1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}-1&1&0&0\\0&0&1&-1\\0&0&0&0\\0&0&0&0\end{bmatrix}\vec{v}_1&= 0\\ \therefore \vec{v} &= (1,1,0,0)\\ \lambda = -1, \begin{bmatrix}1&1&0&0\\1&1&0&0\\0&0&1&1\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&0&0\\0&0&1&1\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&1&1\\0&0&0&0\\0&0&1&1\end{bmatrix}\vec{v}_1&= 0\\ \begin{bmatrix}1&1&0&0\\0&0&1&1\\0&0&0&0\\0&0&0&0\end{bmatrix}\vec{v}_1&= 0\\ \therefore \vec{v} &= (-1,-1,0,0)\\ \end{align*} Therefore, all of this matrix's eigenvectors are spanned by (1,1,0,0) and (-1,-1,0,0). Is this correct? I feel like there should be more, specifically (1,-1,0,0) corresponding to $\lambda = -1$, but I could be wrong.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
14,"Prove if $A$ is hermitian and $A^m=I$, then $A^2=I$","Prove if  is hermitian and , then",A A^m=I A^2=I,"Prove If $A_{n\times n}$ is hermitian over $\mathbb C$ and $A^m=I, m\in \mathbb N$, then $A^2=I$ In other words, we need to show that $A$ is unitary, since its hermitian $A=A^*$, then $A^2=A^*A=AA^*=I$. I don't know how to tackle this though, maybe somehow use that the polynomial $x^m-1$ annihilate $A$ and both it and $x^2-1$ and have a common root?","Prove If $A_{n\times n}$ is hermitian over $\mathbb C$ and $A^m=I, m\in \mathbb N$, then $A^2=I$ In other words, we need to show that $A$ is unitary, since its hermitian $A=A^*$, then $A^2=A^*A=AA^*=I$. I don't know how to tackle this though, maybe somehow use that the polynomial $x^m-1$ annihilate $A$ and both it and $x^2-1$ and have a common root?",,"['linear-algebra', 'matrices']"
15,When does a Markov chain not have a steady state?,When does a Markov chain not have a steady state?,,"I was asked this question on an oral qual, and eventually I seemed to conclude that there have to be eigenvalues of modulus 1.  But I just realised every Markov matrix has one as an eigenvalue.  So should the condition be multiple eigenvalues with modulus 1?","I was asked this question on an oral qual, and eventually I seemed to conclude that there have to be eigenvalues of modulus 1.  But I just realised every Markov matrix has one as an eigenvalue.  So should the condition be multiple eigenvalues with modulus 1?",,"['linear-algebra', 'markov-chains']"
16,"If $\mathbf{ABC}$ non-singular prove that $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ non-singular too","If  non-singular prove that ,  and  non-singular too",\mathbf{ABC} \mathbf{A} \mathbf{B} \mathbf{C},"I am interested in the following exercise, and I have tried to solve it with the following way. Firstly , could you please check the correctness of the given answer. Secondly , can you give an alternative answer? $\textbf{Exercise}$: If the product $\mathbf{M} = \mathbf{ABC}$ of three square matrices is invertible, then $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ are invertible. $\textbf{Answer}$: Part 1 : If $\mathbf{C}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Cx} = 0 \iff \mathbf{ABCx} = 0 \iff \mathbf{Mx} = 0$. This comes in contradiction with the fact the $\mathbf{M}$ is non-singlular by the exercises definition. Thus $\mathbf{C}^{-1}$ exists. Using the last statement, we may write $\mathbf{MC}^{-1} = \mathbf{AB}$. Knowing that $\mathbf{M}$ and $\mathbf{C}^{-1}$ are invertible, we are interested to prove the invertibily of $\mathbf{MC}^{-1}$, so as to continue with similar way with the prof of matrix $\mathbf{B}$ invertibility. We may have: $$\mathbf{J} = \mathbf{MC}^{-1} \iff \mathbf{M}^{-1}\mathbf{J} = \mathbf{C}^{-1} \iff \mathbf{CM}^{-1}\mathbf{J} = \mathbf{I}$$ This means that matrix $\mathbf{MC}^{-1}$ has a left inverse given by $\mathbf{J}^{-1} = \mathbf{CM}^{-1}$. Part 2 : Based on the last statement, and similarly with the invertibility prof we followed for $\mathbf{C}$, If $\mathbf{B}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Bx} = 0 \implies \mathbf{ABx} = 0 \implies \mathbf{MC}^{-1}\mathbf{x} = 0 \implies \mathbf{J}\mathbf{x} = 0$. This comes in contradiction with the fact the $\mathbf{J}$ is non-singlular. Thus $\mathbf{B}^{-1}$ is invertible. Part 3 : Finally, we may write $\mathbf{A} = \mathbf{MC}^{-1}\mathbf{B}^{-1} = \mathbf{J}\mathbf{B}^{-1}$. Matrix $\mathbf{A}$ is non-singular, because: $$(\mathbf{JB}^{-1})^{-1}\mathbf{JB}^{-1} = \mathbf{I}~~\text{and}~~\mathbf{JB}^{-1}(\mathbf{JB}^{-1})^{-1} = \mathbf{I}$$ Thank you! PS: Changes have been made taking into account users comments. I hope the post is improved.","I am interested in the following exercise, and I have tried to solve it with the following way. Firstly , could you please check the correctness of the given answer. Secondly , can you give an alternative answer? $\textbf{Exercise}$: If the product $\mathbf{M} = \mathbf{ABC}$ of three square matrices is invertible, then $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ are invertible. $\textbf{Answer}$: Part 1 : If $\mathbf{C}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Cx} = 0 \iff \mathbf{ABCx} = 0 \iff \mathbf{Mx} = 0$. This comes in contradiction with the fact the $\mathbf{M}$ is non-singlular by the exercises definition. Thus $\mathbf{C}^{-1}$ exists. Using the last statement, we may write $\mathbf{MC}^{-1} = \mathbf{AB}$. Knowing that $\mathbf{M}$ and $\mathbf{C}^{-1}$ are invertible, we are interested to prove the invertibily of $\mathbf{MC}^{-1}$, so as to continue with similar way with the prof of matrix $\mathbf{B}$ invertibility. We may have: $$\mathbf{J} = \mathbf{MC}^{-1} \iff \mathbf{M}^{-1}\mathbf{J} = \mathbf{C}^{-1} \iff \mathbf{CM}^{-1}\mathbf{J} = \mathbf{I}$$ This means that matrix $\mathbf{MC}^{-1}$ has a left inverse given by $\mathbf{J}^{-1} = \mathbf{CM}^{-1}$. Part 2 : Based on the last statement, and similarly with the invertibility prof we followed for $\mathbf{C}$, If $\mathbf{B}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Bx} = 0 \implies \mathbf{ABx} = 0 \implies \mathbf{MC}^{-1}\mathbf{x} = 0 \implies \mathbf{J}\mathbf{x} = 0$. This comes in contradiction with the fact the $\mathbf{J}$ is non-singlular. Thus $\mathbf{B}^{-1}$ is invertible. Part 3 : Finally, we may write $\mathbf{A} = \mathbf{MC}^{-1}\mathbf{B}^{-1} = \mathbf{J}\mathbf{B}^{-1}$. Matrix $\mathbf{A}$ is non-singular, because: $$(\mathbf{JB}^{-1})^{-1}\mathbf{JB}^{-1} = \mathbf{I}~~\text{and}~~\mathbf{JB}^{-1}(\mathbf{JB}^{-1})^{-1} = \mathbf{I}$$ Thank you! PS: Changes have been made taking into account users comments. I hope the post is improved.",,['linear-algebra']
17,Is $\operatorname{rank}(AQB)=\operatorname{rank}(AB)$ if $Q$ is non-singular?,Is  if  is non-singular?,\operatorname{rank}(AQB)=\operatorname{rank}(AB) Q,"$\newcommand{\rank}{\operatorname{rank}}$ We know that $\rank(PA)=\rank(AQ)=\rank(PAQ)=\rank(A)$ where $A\in M_{m\times n}(\mathbb F), P, Q$ are $m\times m, n\times n$ invertible matrices. mean to say , from the matrix product we can remove the non-singular matrices; rank will not be effected. After studying this, it came to my mind, then what will happen in the case of $\rank(PAQB)$ where $B\in M_{n\times m}(\mathbb F)$ ? No idea. The only thing I got is $\rank(PAQB)=\rank(AQB)$ . Can we remove $Q$ as well and write $\rank(AQB)=\rank(AB)$ ? Please help. In case it has been solved earlier, provide me the link.","We know that where are invertible matrices. mean to say , from the matrix product we can remove the non-singular matrices; rank will not be effected. After studying this, it came to my mind, then what will happen in the case of where ? No idea. The only thing I got is . Can we remove as well and write ? Please help. In case it has been solved earlier, provide me the link.","\newcommand{\rank}{\operatorname{rank}} \rank(PA)=\rank(AQ)=\rank(PAQ)=\rank(A) A\in M_{m\times n}(\mathbb F), P, Q m\times m, n\times n \rank(PAQB) B\in M_{n\times m}(\mathbb F) \rank(PAQB)=\rank(AQB) Q \rank(AQB)=\rank(AB)","['linear-algebra', 'matrices', 'matrix-rank']"
18,"If a matrix has positive, real eigenvalues, is it always symmetric?","If a matrix has positive, real eigenvalues, is it always symmetric?",,"We know that symmetric matrices are orthogonally diagonalizable and have real eigenvalues.  Is the converse true?  Does a matrix with real eigenvalues have to be symmetric? A class of symmetric matrices, the positive definite matrices, have positive real eigenvalues.  Is the converse true?  Does a matrix with positive real eigenvalues have to be symmetric, positive-definite? I think the answer to all this is ""no"", but I just wanted to confirm. Thanks,","We know that symmetric matrices are orthogonally diagonalizable and have real eigenvalues.  Is the converse true?  Does a matrix with real eigenvalues have to be symmetric? A class of symmetric matrices, the positive definite matrices, have positive real eigenvalues.  Is the converse true?  Does a matrix with positive real eigenvalues have to be symmetric, positive-definite? I think the answer to all this is ""no"", but I just wanted to confirm. Thanks,",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
19,"For which values of $a,b,c$ is the matrix $A$ invertible?",For which values of  is the matrix  invertible?,"a,b,c A","$A=\begin{pmatrix}1&1&1\\a&b&c\\a^2&b^2&c^2\end{pmatrix}$ $$\Rightarrow\det(A)=\begin{vmatrix}b&c\\b^2&c^2\end{vmatrix}-\begin{vmatrix}a&c\\a^2&c^2\end{vmatrix}+\begin{vmatrix}a&b\\a^2&b^2\end{vmatrix}\\=ab^2-a^2b-ac^2+a^2c+bc^2-b^2c\\=a^2(c-b)+b^2(a-c)+c^2(b-a).$$ Clearly, $$\left\{\det(A)\neq0\left|\begin{matrix}c\neq b\\a\neq c\\b\neq a\\a,b,c\neq 0\end{matrix}\right.\right\}\\$$ Is it sufficient to say that the matrix is invertible provided that all 4 constraints are met? Would Cramer's rule yield more explicit results for $a,b,c$ such that $\det(A)\neq0$?","$A=\begin{pmatrix}1&1&1\\a&b&c\\a^2&b^2&c^2\end{pmatrix}$ $$\Rightarrow\det(A)=\begin{vmatrix}b&c\\b^2&c^2\end{vmatrix}-\begin{vmatrix}a&c\\a^2&c^2\end{vmatrix}+\begin{vmatrix}a&b\\a^2&b^2\end{vmatrix}\\=ab^2-a^2b-ac^2+a^2c+bc^2-b^2c\\=a^2(c-b)+b^2(a-c)+c^2(b-a).$$ Clearly, $$\left\{\det(A)\neq0\left|\begin{matrix}c\neq b\\a\neq c\\b\neq a\\a,b,c\neq 0\end{matrix}\right.\right\}\\$$ Is it sufficient to say that the matrix is invertible provided that all 4 constraints are met? Would Cramer's rule yield more explicit results for $a,b,c$ such that $\det(A)\neq0$?",,"['linear-algebra', 'matrices', 'determinant']"
20,Property of SO(3),Property of SO(3),,"Suppose $A\in SO(3).$   Show that there exists a vector $v\in \mathbb{R}^3$ such that $Av=v$. $ SO(3)={{A\in O(3)|detA=1}} $ and $ O(3)={A:\mathbb{R}^3\rightarrow \mathbb{R}^3}|<Au,Av>=<u,v> for\space all\space u,v\in \mathbb{R}^3 $ I'm thinking proof by contradiction: Suppose $Av\neq v \space \forall v$. But I have no clue how properties of $SO(3) $ can help from here. Would anyone give me any suggestions for the first step?","Suppose $A\in SO(3).$   Show that there exists a vector $v\in \mathbb{R}^3$ such that $Av=v$. $ SO(3)={{A\in O(3)|detA=1}} $ and $ O(3)={A:\mathbb{R}^3\rightarrow \mathbb{R}^3}|<Au,Av>=<u,v> for\space all\space u,v\in \mathbb{R}^3 $ I'm thinking proof by contradiction: Suppose $Av\neq v \space \forall v$. But I have no clue how properties of $SO(3) $ can help from here. Would anyone give me any suggestions for the first step?",,['linear-algebra']
21,"Linear dependence of $\left\{x^{n}\,\colon\, n\in\mathbb{N}\right\}$",Linear dependence of,"\left\{x^{n}\,\colon\, n\in\mathbb{N}\right\}","Consider the set $S=\left\{x^{n}\,\colon\, n\in\mathbb{N}\right\}$. (Note that $x\in\mathbb{R}$) Is this set linearly dependent? Well thinking about it we want to find some non-trivial values $\lambda_{n}$ such that $$\lambda_{1}x+\lambda_{2}x^{2} + \ldots + \lambda_{n}x^{n} + \ldots = 0.$$ In other words we want to find if $$\sum_{k=1}^{\infty}\lambda_{n}x^{n} = 0$$ for any $\lambda_{n}\not=0$. However i'm not really sure how to proceed.","Consider the set $S=\left\{x^{n}\,\colon\, n\in\mathbb{N}\right\}$. (Note that $x\in\mathbb{R}$) Is this set linearly dependent? Well thinking about it we want to find some non-trivial values $\lambda_{n}$ such that $$\lambda_{1}x+\lambda_{2}x^{2} + \ldots + \lambda_{n}x^{n} + \ldots = 0.$$ In other words we want to find if $$\sum_{k=1}^{\infty}\lambda_{n}x^{n} = 0$$ for any $\lambda_{n}\not=0$. However i'm not really sure how to proceed.",,['linear-algebra']
22,Do elementary row operations give a similar matrix transformation?,Do elementary row operations give a similar matrix transformation?,,"So we define two matrices $A,B$ to be similar if there exists an invertible square matrix $P$ such that $AP=PB$. I was wondering if $A,B$ are related via elementary row operations (say, they are connected via some permutation rows for example) then are the necessarily similar? Obviously swapping rows multiplies the determinant by $-1$ but I was thinking if we permute rows in pairs, would this allow us to construct a similarity transformation?","So we define two matrices $A,B$ to be similar if there exists an invertible square matrix $P$ such that $AP=PB$. I was wondering if $A,B$ are related via elementary row operations (say, they are connected via some permutation rows for example) then are the necessarily similar? Obviously swapping rows multiplies the determinant by $-1$ but I was thinking if we permute rows in pairs, would this allow us to construct a similarity transformation?",,"['linear-algebra', 'matrices', 'representation-theory', 'similar-matrices']"
23,complex eigenvalues and invariant spaces,complex eigenvalues and invariant spaces,,"I am currently reading Guillemin and Pollack's Differential Topology , and the following claim is made without proof: Given a linear isomorphism $E: \mathbb{R}^k \to \mathbb{R}^k$, with $k>2$ and such that $E$ can be represented by a matrix with real entries, $E$ has a one- or two-dimensional invariant space . I understand that the Fundamental Theorem of Algebra implies that $E$ has at least one real or complex eigenvalue; if it is real, then $E$ clearly has a one-dimensional fixed space. If it is complex, however, I don't see how there needs to be a two-dimensional invariant space. If $E$ has complex eigenvalue $a+bi$, then $a-bi$ must also be an eigenvalue (as $E$ contains real entries). These eigenvalues correspond to eigenvectors $v_1$ and $v_2$. I assume that the subspace spanned by $v_1$ and $v_2$ is the desired invariant space, but can't figure out how to prove it. Any help would be most appreciated.","I am currently reading Guillemin and Pollack's Differential Topology , and the following claim is made without proof: Given a linear isomorphism $E: \mathbb{R}^k \to \mathbb{R}^k$, with $k>2$ and such that $E$ can be represented by a matrix with real entries, $E$ has a one- or two-dimensional invariant space . I understand that the Fundamental Theorem of Algebra implies that $E$ has at least one real or complex eigenvalue; if it is real, then $E$ clearly has a one-dimensional fixed space. If it is complex, however, I don't see how there needs to be a two-dimensional invariant space. If $E$ has complex eigenvalue $a+bi$, then $a-bi$ must also be an eigenvalue (as $E$ contains real entries). These eigenvalues correspond to eigenvectors $v_1$ and $v_2$. I assume that the subspace spanned by $v_1$ and $v_2$ is the desired invariant space, but can't figure out how to prove it. Any help would be most appreciated.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
24,Hermitian Matrix Unitarily Diagonalizable,Hermitian Matrix Unitarily Diagonalizable,,"I am having trouble proving that Hermitian Matrices ($A = A^{*}$) are unitarily diagonalizable ($A = Q^{*}DQ$, where Q is a unitary matrix, $QQ^{*} = I$ and D is a diagonal matrix). I also know that given an hermitian operator T over a finite diminsional vector space V, V has an orthonormal basis which consists of eigenvectors of T. Any suggestions? Edit: Following Git Gud's suggestion: We shall consider the Hermitian operator $L_A$. Let E be the standard basis of V. Also, by hypothesis, we have that there exists an orthonormal basis B for V which consists of eigenvectors of $L_A$. Since the basis consists of only eigenvectors, the basis contains $dim(V)$ linearly independent vectors, so $[L_A]_B$ is diagonal. We know that $[L_A]_E = A$. Then, $$A = [L_A]_E = Q^{-1}DQ$$ and since Q must be unitary, then $Q^{-1} = Q^*$, by which we have that $$A = Q^*DQ$$","I am having trouble proving that Hermitian Matrices ($A = A^{*}$) are unitarily diagonalizable ($A = Q^{*}DQ$, where Q is a unitary matrix, $QQ^{*} = I$ and D is a diagonal matrix). I also know that given an hermitian operator T over a finite diminsional vector space V, V has an orthonormal basis which consists of eigenvectors of T. Any suggestions? Edit: Following Git Gud's suggestion: We shall consider the Hermitian operator $L_A$. Let E be the standard basis of V. Also, by hypothesis, we have that there exists an orthonormal basis B for V which consists of eigenvectors of $L_A$. Since the basis consists of only eigenvectors, the basis contains $dim(V)$ linearly independent vectors, so $[L_A]_B$ is diagonal. We know that $[L_A]_E = A$. Then, $$A = [L_A]_E = Q^{-1}DQ$$ and since Q must be unitary, then $Q^{-1} = Q^*$, by which we have that $$A = Q^*DQ$$",,"['linear-algebra', 'matrices', 'diagonalization']"
25,"If $AB = BA^2$ and $B^5 = I,$ Then how can we prove $A^{31} = I.$",If  and  Then how can we prove,"AB = BA^2 B^5 = I, A^{31} = I.","If $A$ and $B$ are two non singular matrices, $AB = BA^2$ and $B^5 = I,$ then how can we prove $A^{31} = I$ ? $\bf{My\; Trial::}$ Using $B^5 = I\Rightarrow B^5A^5 = IA^5 = A^5\Rightarrow B^4BA^2A^3 = A^5$ Now Using $BA^2 = AB$ , we get $B^4ABA^3 = A^5\Rightarrow B^4ABA^2A=A^5\Rightarrow B^4A^2BA=A^5$ I did not understand How can I prove it. plz Help me Thanks","If and are two non singular matrices, and then how can we prove ? Using Now Using , we get I did not understand How can I prove it. plz Help me Thanks","A B AB = BA^2 B^5 = I, A^{31} = I \bf{My\; Trial::} B^5 = I\Rightarrow B^5A^5 = IA^5 = A^5\Rightarrow B^4BA^2A^3 = A^5 BA^2 = AB B^4ABA^3 = A^5\Rightarrow B^4ABA^2A=A^5\Rightarrow B^4A^2BA=A^5","['linear-algebra', 'matrices']"
26,Show that $A + A^{-1} \geq 2I$ for $A > 0$.,Show that  for .,A + A^{-1} \geq 2I A > 0,"For a positive matrix $A$.  Here, we assume that all positive matrices are self-adjoint.  Show that  \begin{align} A + A^{-1} \geq 2I. \end{align} Here, $A≥0$ means that A is self-adjoint and for all $x∈ℂn,⟨Ax,x⟩≥0.$","For a positive matrix $A$.  Here, we assume that all positive matrices are self-adjoint.  Show that  \begin{align} A + A^{-1} \geq 2I. \end{align} Here, $A≥0$ means that A is self-adjoint and for all $x∈ℂn,⟨Ax,x⟩≥0.$",,['linear-algebra']
27,$3\times 3$ matrix with no real eigenvalues,matrix with no real eigenvalues,3\times 3,"I was asked this question on my hw along with any $2\times2$ matrix with no real eigenvalue and any $4\times4$ matrix with no real eigenvalue. I got the $2\times2$ which is  $$ \begin{bmatrix} 1 & 2 \\ -1 & -1 \\  \end{bmatrix} $$ can someone help me on the $3 \times 3$ and $4\times 4$, it seems really simple but I'm kind of stuck, thanks!","I was asked this question on my hw along with any $2\times2$ matrix with no real eigenvalue and any $4\times4$ matrix with no real eigenvalue. I got the $2\times2$ which is  $$ \begin{bmatrix} 1 & 2 \\ -1 & -1 \\  \end{bmatrix} $$ can someone help me on the $3 \times 3$ and $4\times 4$, it seems really simple but I'm kind of stuck, thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'examples-counterexamples']"
28,Calculating $\det(A+I)$ for matrix $A$ defined by products,Calculating  for matrix  defined by products,\det(A+I) A,"Let $b_1,\ldots,b_n\in\mathbb{R}$. I have an $n\times n$ matrix $A$ whose entry is given by $a_{ij}=b_ib_j$, and I'd like to show that $\det(A+I)=\sum_{i=1}^nb_i^2+1$. Define $b=(b_1,\ldots,b_n)$. I know that $Ab=\left(\sum_{i=1}^nb_i^2\right)b$, and $Ac=0$ for all $c$ such that $b\cdot c=0$. So all the eigenvalues of $A$ are $\sum_{i=1}^nb_i^2, 0, 0, \ldots, 0$. What can I do next?","Let $b_1,\ldots,b_n\in\mathbb{R}$. I have an $n\times n$ matrix $A$ whose entry is given by $a_{ij}=b_ib_j$, and I'd like to show that $\det(A+I)=\sum_{i=1}^nb_i^2+1$. Define $b=(b_1,\ldots,b_n)$. I know that $Ab=\left(\sum_{i=1}^nb_i^2\right)b$, and $Ac=0$ for all $c$ such that $b\cdot c=0$. So all the eigenvalues of $A$ are $\sum_{i=1}^nb_i^2, 0, 0, \ldots, 0$. What can I do next?",,"['linear-algebra', 'matrices', 'determinant']"
29,Why is the geometric multiplicity of an eigen value equal to number of jordan blocks corresponding to it?,Why is the geometric multiplicity of an eigen value equal to number of jordan blocks corresponding to it?,,"Geometric multiplicity of an eigen value is $$ \dim \mathrm{null} (A -\lambda I)\tag 1.$$ Suppose $A$ is in jordan normal form and has two Jordan forms with eigen value $\lambda$ , one of size $2 \times 2$ and other of size $3\times 3$ . Then, why is $\dim \mathrm{null} (A -\lambda I)$ necessarily equal to $2 $ i.e. why is geometric multiplicity of $\lambda =2$ ? From the concept of generalised eigen  vectors , I know the following : $(T-\lambda I)^3$ will produce a Zero Matrix in place of both these sub-blocks.","Geometric multiplicity of an eigen value is Suppose is in jordan normal form and has two Jordan forms with eigen value , one of size and other of size . Then, why is necessarily equal to i.e. why is geometric multiplicity of ? From the concept of generalised eigen  vectors , I know the following : will produce a Zero Matrix in place of both these sub-blocks.", \dim \mathrm{null} (A -\lambda I)\tag 1. A \lambda 2 \times 2 3\times 3 \dim \mathrm{null} (A -\lambda I) 2  \lambda =2 (T-\lambda I)^3,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
30,Suppose that $U$ is a subspace of $V$. What is $U+U$?,Suppose that  is a subspace of . What is ?,U V U+U,"Suppose that $U$ is a subspace of $V$. What is $U+U$? Why does $U+U = U?$ I want to think of this geometrically, say in $\mathbb{R^{3}}$  we have some random plane in space that intersects the origin. How is this subspace, when added to itself, equal to itself?","Suppose that $U$ is a subspace of $V$. What is $U+U$? Why does $U+U = U?$ I want to think of this geometrically, say in $\mathbb{R^{3}}$  we have some random plane in space that intersects the origin. How is this subspace, when added to itself, equal to itself?",,['linear-algebra']
31,"Suppose $T^2$ is diagonalizable and $\ker{T}=\{0\}$, and every eigenvalue of $T^2$ is nonnegative. Show that $T$ is diagonalizable.","Suppose  is diagonalizable and , and every eigenvalue of  is nonnegative. Show that  is diagonalizable.",T^2 \ker{T}=\{0\} T^2 T,"Suppose $T^2$ is diagonalizable and $\ker{T}=\{0\}$, and every eigenvalue of $T^2$ is nonnegative. Show that $T$ is diagonalizable. Of course $T$ is an operator on $V$. It seems to me that if I take a basis of eigenvectors of $T^2$ I can define: $$S(e_j)=\sqrt{\lambda_j}e_j$$ So by definition $S$ is diagonalizable since every eigenvector of $T^2$ is an eigenvector of $S$ and so there's a basis of eigenvectors of $S$, and obviously $S^2=T^2$. I want to say that $T$ has to be of this form (perhaps $T(e_j)=\pm\sqrt{\lambda_j}e_j$), and therefore has to be diagonalizable. I feel that this explanation is flawed somehow, and obviously strange since I didn't use the given about $\ker{T}$. Many thanks! EdiT: Okay, I found a different proof(by someone else) that uses the minimal polynomial and @julien 's algebraic note. I'll give it a little more thought but I think I got this down. Thank you.","Suppose $T^2$ is diagonalizable and $\ker{T}=\{0\}$, and every eigenvalue of $T^2$ is nonnegative. Show that $T$ is diagonalizable. Of course $T$ is an operator on $V$. It seems to me that if I take a basis of eigenvectors of $T^2$ I can define: $$S(e_j)=\sqrt{\lambda_j}e_j$$ So by definition $S$ is diagonalizable since every eigenvector of $T^2$ is an eigenvector of $S$ and so there's a basis of eigenvectors of $S$, and obviously $S^2=T^2$. I want to say that $T$ has to be of this form (perhaps $T(e_j)=\pm\sqrt{\lambda_j}e_j$), and therefore has to be diagonalizable. I feel that this explanation is flawed somehow, and obviously strange since I didn't use the given about $\ker{T}$. Many thanks! EdiT: Okay, I found a different proof(by someone else) that uses the minimal polynomial and @julien 's algebraic note. I'll give it a little more thought but I think I got this down. Thank you.",,"['linear-algebra', 'operator-theory']"
32,Finding the solution for $Ax=0$,Finding the solution for,Ax=0,"Find the solution for $Ax=0$ for the following $3 \times 3$ matrix: $$\begin{pmatrix}3 & 2& -3\\ 2& -1&1 \\ 1& 1& 1\end{pmatrix}$$ I found the row reduced form of that matrix, which was $$\begin{pmatrix}1 & 2/3& -1\\ 0& 1&-9/7 \\ 0& 0& 1\end{pmatrix}$$ I'm not sure what I'm supposed to do next to find the ""unique"" solution besides $x=0$? Do I further reduce that matrix to the identity matrix?","Find the solution for $Ax=0$ for the following $3 \times 3$ matrix: $$\begin{pmatrix}3 & 2& -3\\ 2& -1&1 \\ 1& 1& 1\end{pmatrix}$$ I found the row reduced form of that matrix, which was $$\begin{pmatrix}1 & 2/3& -1\\ 0& 1&-9/7 \\ 0& 0& 1\end{pmatrix}$$ I'm not sure what I'm supposed to do next to find the ""unique"" solution besides $x=0$? Do I further reduce that matrix to the identity matrix?",,"['calculus', 'linear-algebra', 'matrices']"
33,"For an orthogonal matrix $W$ and diagonal matrix $T$, why is $\mathrm{trace}(W^{\mathsf{T}} T W) = \mathrm{trace}(T)$?","For an orthogonal matrix  and diagonal matrix , why is ?",W T \mathrm{trace}(W^{\mathsf{T}} T W) = \mathrm{trace}(T),"Given an orthogonal matrix $W$ and a diagonal matrix $T$, both with dimensions $n \times n$, why is the following true ? Can someone write out the proof explicitly please ? $$\mathrm{trace}(W^{\mathsf{T}} T W) = \mathrm{trace}(T)$$","Given an orthogonal matrix $W$ and a diagonal matrix $T$, both with dimensions $n \times n$, why is the following true ? Can someone write out the proof explicitly please ? $$\mathrm{trace}(W^{\mathsf{T}} T W) = \mathrm{trace}(T)$$",,['linear-algebra']
34,Why do we need a diagonal matrix?,Why do we need a diagonal matrix?,,"Apart from simplifying matrix powers, why do want to diagonalize a matrix? Do they have any appealing application which can be used to motivate to study diagonal matrices.  Thanks for any answers.","Apart from simplifying matrix powers, why do want to diagonalize a matrix? Do they have any appealing application which can be used to motivate to study diagonal matrices.  Thanks for any answers.",,"['linear-algebra', 'matrices']"
35,Must a vector subspace contain the null vector?,Must a vector subspace contain the null vector?,,"If it must, why? If it mustn't, why? I was able to prove that a certain set of vectors doesn't have the zero vector as a member and normally, I've been told to conclude that it is not a vector subspace from this point. I got confused and wondered if this always shows that the set isn't an empty set since that is one of the conditions a vector subspace must satisfy.","If it must, why? If it mustn't, why? I was able to prove that a certain set of vectors doesn't have the zero vector as a member and normally, I've been told to conclude that it is not a vector subspace from this point. I got confused and wondered if this always shows that the set isn't an empty set since that is one of the conditions a vector subspace must satisfy.",,['linear-algebra']
36,Non-commuting matrices and nilpotence,Non-commuting matrices and nilpotence,,"Is it possible to find $n\times n$ matrices $M,N$  such that $(MN)^n=0$ but $(NM)^n\ne 0$? I can see that  $(MN)^n=0\implies (NM)^{n+1}= 0$ by associativity. But I don't see that it is necessarily true for $(NM)^n$. I can also see that $MN=0\not\implies NM=0$ but I cannot find an example for $(MN)^n=0$ but $(NM)^n\ne 0$.","Is it possible to find $n\times n$ matrices $M,N$  such that $(MN)^n=0$ but $(NM)^n\ne 0$? I can see that  $(MN)^n=0\implies (NM)^{n+1}= 0$ by associativity. But I don't see that it is necessarily true for $(NM)^n$. I can also see that $MN=0\not\implies NM=0$ but I cannot find an example for $(MN)^n=0$ but $(NM)^n\ne 0$.",,"['linear-algebra', 'matrices', 'nilpotence']"
37,Let $B$ be a nilpotent $n\times n$ matrix with complex entries let $A = B-I$ then find $\det(A)$,Let  be a nilpotent  matrix with complex entries let  then find,B n\times n A = B-I \det(A),Let $B$ be a given nilpotent $n\times n$ matrix with complex entries. Let $A = B-I$   find out $\det(A)$. What if B is orthogonal or skew symmetric matrix? Then can we say anything about its trace and determinant?,Let $B$ be a given nilpotent $n\times n$ matrix with complex entries. Let $A = B-I$   find out $\det(A)$. What if B is orthogonal or skew symmetric matrix? Then can we say anything about its trace and determinant?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
38,"Is the set $\{\sin (p_{i})| p_{i} \,\,\mbox{is a prime number, for all}\,\,i\in \mathbb{N}\}$ linearly independent?",Is the set  linearly independent?,"\{\sin (p_{i})| p_{i} \,\,\mbox{is a prime number, for all}\,\,i\in \mathbb{N}\}","Is the set $\{\sin (p_{i})| p_{i} \,\,\mbox{is a prime number, for all}\,\,i\in \mathbb{N}\,\,\mbox{and}\,\, p_{i}\neq p_{j}\,\,\mbox{if}\,\,i\neq j\}$ is linearly independent when $\mathbb{R}$ is a vector space over $\mathbb{Q}$? Thanks for any help.","Is the set $\{\sin (p_{i})| p_{i} \,\,\mbox{is a prime number, for all}\,\,i\in \mathbb{N}\,\,\mbox{and}\,\, p_{i}\neq p_{j}\,\,\mbox{if}\,\,i\neq j\}$ is linearly independent when $\mathbb{R}$ is a vector space over $\mathbb{Q}$? Thanks for any help.",,[]
39,Question about direct sum of function space,Question about direct sum of function space,,"I am reading Applied linear algebra: the decoupling principle by Lorenzo Adlai Sadun (btw very recommendable!) On page 30 about direct sums on vector spaces it says: Let $V$ be the space of continuous functions on a domain $U \subset \mathbb R^3$. Then $V \oplus V \oplus V$ is the space of continuous $\mathbb R^3$-valued functions on $U$. What I don't understand is how this threefold direct sum of $V$ can lead to a statement about the codomain (""-valued"" functions). But I just guess that I don't get it altogether - please enlighten me. Thank you! EDIT I think what confuses me most is that we start with $\mathbb R^3$ and end up with it. It would come more natural if we started with $\mathbb R$ and after taking the direct sum three times would end in $\mathbb R^3$.","I am reading Applied linear algebra: the decoupling principle by Lorenzo Adlai Sadun (btw very recommendable!) On page 30 about direct sums on vector spaces it says: Let $V$ be the space of continuous functions on a domain $U \subset \mathbb R^3$. Then $V \oplus V \oplus V$ is the space of continuous $\mathbb R^3$-valued functions on $U$. What I don't understand is how this threefold direct sum of $V$ can lead to a statement about the codomain (""-valued"" functions). But I just guess that I don't get it altogether - please enlighten me. Thank you! EDIT I think what confuses me most is that we start with $\mathbb R^3$ and end up with it. It would come more natural if we started with $\mathbb R$ and after taking the direct sum three times would end in $\mathbb R^3$.",,"['linear-algebra', 'functional-analysis', 'vector-spaces']"
40,A distance preserving operator that's not linear?,A distance preserving operator that's not linear?,,"Let $Q: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be an operator that preserves all distances.  Is this condition alone enough for us to say that $Q$ must be a linear operator? If not, what are some counterexamples (whether simple or pathological), where an operator preserves all distances but is not a linear operator? [Edit] Did I even give the correct interpretation of ""preserves all distances""? [Edit] I guess not!  What I meant is that $Q$ is an isometry (i.e. $||Qx - Qy|| = ||x - y||$ for any $x$, $y$).","Let $Q: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be an operator that preserves all distances.  Is this condition alone enough for us to say that $Q$ must be a linear operator? If not, what are some counterexamples (whether simple or pathological), where an operator preserves all distances but is not a linear operator? [Edit] Did I even give the correct interpretation of ""preserves all distances""? [Edit] I guess not!  What I meant is that $Q$ is an isometry (i.e. $||Qx - Qy|| = ||x - y||$ for any $x$, $y$).",,['linear-algebra']
41,Why is $(I+A)/(I-A)$ unambiguous?,Why is  unambiguous?,(I+A)/(I-A),"The following question is from Roger W. Brockett, Finite Dimensional Linear Systems ( Image of source ): Ordinarily one does not use division notation when dealing with matrices because $A/B$ might be interpreted as $AB^{-1}$ or $B^{-1}A$ and these are not necessarily the same. Why is the notation $(I+A)/(I-A)$ unambiguous? I do not know what (I+A)/(I-A) is supposed mean, let alone proving it is unambiguous. So the question is what that notation mean, and preferably why is unambigous?","The following question is from Roger W. Brockett, Finite Dimensional Linear Systems ( Image of source ): Ordinarily one does not use division notation when dealing with matrices because might be interpreted as or and these are not necessarily the same. Why is the notation unambiguous? I do not know what (I+A)/(I-A) is supposed mean, let alone proving it is unambiguous. So the question is what that notation mean, and preferably why is unambigous?",A/B AB^{-1} B^{-1}A (I+A)/(I-A),"['linear-algebra', 'matrices', 'inverse']"
42,Example of a vector space with different addition/scalar multiplication operators,Example of a vector space with different addition/scalar multiplication operators,,"I have learned that a vector space is a set of elements called vectors , on which are defined an addition operation and a scalar multiplication operation with the scalars in some field $F$ . However, all the examples of vector fields I know ( $\mathbb{R}^n$ , $\mathbb{C}^n$ , $\mathbb{F}^{(n, n)}$ , etc) all use the ""normal"" operations of addition and scalar multiplication. As an example, let's consider the set $S = \{(x, y) \in \mathbb{R}^2 \lvert x + y = 10\}$ , for instance. With the intuitive definitions of addition and scalar multiplication ( $(a, b) + (c, d) = (a + c, b + d)$ and $k(a, b) = (ka, kb)$ ), I can verify that this is a vector space, but I can't think of any other definitions of the addition/scalar multiplication operation that still makes this set a vector space. Could someone provide and explain a nonobvious example of addition/scalar multiplication operations that still keeps the set as a vector space?","I have learned that a vector space is a set of elements called vectors , on which are defined an addition operation and a scalar multiplication operation with the scalars in some field . However, all the examples of vector fields I know ( , , , etc) all use the ""normal"" operations of addition and scalar multiplication. As an example, let's consider the set , for instance. With the intuitive definitions of addition and scalar multiplication ( and ), I can verify that this is a vector space, but I can't think of any other definitions of the addition/scalar multiplication operation that still makes this set a vector space. Could someone provide and explain a nonobvious example of addition/scalar multiplication operations that still keeps the set as a vector space?","F \mathbb{R}^n \mathbb{C}^n \mathbb{F}^{(n, n)} S = \{(x, y) \in \mathbb{R}^2 \lvert x + y = 10\} (a, b) + (c, d) = (a + c, b + d) k(a, b) = (ka, kb)","['linear-algebra', 'vector-spaces', 'vectors']"
43,"Prove if $M^2 =0$, then $\operatorname{rank}(M+M^T)=2\operatorname{rank}(M)$.","Prove if , then .",M^2 =0 \operatorname{rank}(M+M^T)=2\operatorname{rank}(M),"Prove if $M^2 =0$ , then $\operatorname{rank}(M+M^T)=2\operatorname{rank}(M)$ . I have used the rank nullity theorem and the fact that rank of matrix and its transpose is same to prove it but I am not sure if it correct or not. I do not how to exactly use the fact of $M^2=0$ . I can think of it means that $M$ is nilpotent and square matrix. In my answer I get less than or equal to but it I have to prove it as equal. Help would be highly appreciated. I am using real square matrices. $\operatorname{rank}(M + M^T)\le\operatorname{rank}(M) +\operatorname{rank}(M^T)$ $\operatorname{rank}(M + M^T)\le2\operatorname{rank}(M)$","Prove if , then . I have used the rank nullity theorem and the fact that rank of matrix and its transpose is same to prove it but I am not sure if it correct or not. I do not how to exactly use the fact of . I can think of it means that is nilpotent and square matrix. In my answer I get less than or equal to but it I have to prove it as equal. Help would be highly appreciated. I am using real square matrices.",M^2 =0 \operatorname{rank}(M+M^T)=2\operatorname{rank}(M) M^2=0 M \operatorname{rank}(M + M^T)\le\operatorname{rank}(M) +\operatorname{rank}(M^T) \operatorname{rank}(M + M^T)\le2\operatorname{rank}(M),"['linear-algebra', 'matrices', 'matrix-rank', 'nilpotence', 'transpose']"
44,"If two matrices have same characteristic polynomial, then if square root for one exists, it also exists for the other one.","If two matrices have same characteristic polynomial, then if square root for one exists, it also exists for the other one.",,"This is True/False question from the recent exam. Statement: Suppose $A$ and $B$ are two elements of $M_n(\mathbb{R})$ such that their characteristic polynomials are equal. If $A=C^2$ for some $C\in M_n(\mathbb{R})$ , then $B=D^2$ for some $D\in M_n(\mathbb{R})$ . I haven't reach the conclusion yet. But here's my thought. Since $A=C^2$ , eigenvalues of $A$ are non-negative. Since $C_A(x)=C_B(x)$ , eigenvalues of $A$ and $B$ are same, which means $B$ also have non-negative eigenvalues. But will this guarantee square root for $B$ ?","This is True/False question from the recent exam. Statement: Suppose and are two elements of such that their characteristic polynomials are equal. If for some , then for some . I haven't reach the conclusion yet. But here's my thought. Since , eigenvalues of are non-negative. Since , eigenvalues of and are same, which means also have non-negative eigenvalues. But will this guarantee square root for ?",A B M_n(\mathbb{R}) A=C^2 C\in M_n(\mathbb{R}) B=D^2 D\in M_n(\mathbb{R}) A=C^2 A C_A(x)=C_B(x) A B B B,"['linear-algebra', 'characteristic-polynomial']"
45,find the number of subspaces $W$ such that $U\subseteq W$,find the number of subspaces  such that,W U\subseteq W,"Let $V$ be a vector space of dimension $n$ over a finite field $F$ with $q$ elements, and let $U\subseteq V$ be a subspace of dimension $k$ . How many subspaces $W\subseteq V$ of dimension $m$ such that $U\subseteq W $ do we have $(k\leq m\leq n)$ ? Hint : look at the set: $$ \{ (U,W) \mid U \subseteq W \subseteq V;  \, \dim(U)=k, \, \dim(W)=m \} $$ I have no idea how to use the hint. I know that the number of subspaces of dimension $k$ is: $$ \frac{\prod_{i=0}^{k-1} (q^{n-1}-1)}{\prod_{i=0}^{k-1} (q^{k-1}-1)} $$ and I don't know how to proceed from here.","Let be a vector space of dimension over a finite field with elements, and let be a subspace of dimension . How many subspaces of dimension such that do we have ? Hint : look at the set: I have no idea how to use the hint. I know that the number of subspaces of dimension is: and I don't know how to proceed from here.","V n F q U\subseteq V k W\subseteq V m U\subseteq W  (k\leq m\leq n) 
\{ (U,W) \mid U \subseteq W \subseteq V; 
\, \dim(U)=k, \, \dim(W)=m \}
 k 
\frac{\prod_{i=0}^{k-1} (q^{n-1}-1)}{\prod_{i=0}^{k-1} (q^{k-1}-1)}
","['linear-algebra', 'combinatorics', 'vector-spaces', 'finite-fields']"
46,"In a Banach Algebra, can the product of two elements be invertible if we know one of the element is non invertible?","In a Banach Algebra, can the product of two elements be invertible if we know one of the element is non invertible?",,"In a Banach Algebra, can the product of two non-invertible element be invertible? Basically in linear algebra, we know that if the product of two matrix is invertible, then both matrix must be invertible. The proof is very simple if we use determinants. Now for general Banach space with infinite dimensions, there is no such thing as a determinant, so I was wondering if we have two elements whose product is invertible, do we know if both elements are invertible? And what if the elements commute?","In a Banach Algebra, can the product of two non-invertible element be invertible? Basically in linear algebra, we know that if the product of two matrix is invertible, then both matrix must be invertible. The proof is very simple if we use determinants. Now for general Banach space with infinite dimensions, there is no such thing as a determinant, so I was wondering if we have two elements whose product is invertible, do we know if both elements are invertible? And what if the elements commute?",,"['linear-algebra', 'functional-analysis']"
47,What is an Affine Span?,What is an Affine Span?,,"According to this definition of affine spans from wikipedia , ""In mathematics, the affine hull or affine span of a set S in Euclidean space Rn is the smallest affine set containing S, or equivalently, the intersection of all affine sets containing S."" They give the definition that it is the set of all affine combinations of elements of S. I am a little confused by this for two reasons. For each affine combination, they require $$\displaystyle \sum_{i=0}^n \alpha_i = 1$$ Why is this? Just as affine spaces are translations of a vector space, can I think of affine combinations as a translation of a vector? Any clarification or further details would be appreciated!","According to this definition of affine spans from wikipedia , ""In mathematics, the affine hull or affine span of a set S in Euclidean space Rn is the smallest affine set containing S, or equivalently, the intersection of all affine sets containing S."" They give the definition that it is the set of all affine combinations of elements of S. I am a little confused by this for two reasons. For each affine combination, they require Why is this? Just as affine spaces are translations of a vector space, can I think of affine combinations as a translation of a vector? Any clarification or further details would be appreciated!",\displaystyle \sum_{i=0}^n \alpha_i = 1,"['linear-algebra', 'affine-geometry']"
48,Relation between Ax=0 and Ax=b,Relation between Ax=0 and Ax=b,,I'm not sure how to solve this. My teacher told me that if $A\vec{x}=0$ has one unique solution (the trivial) then $A\vec{x}=\vec{b}$ has only one unique solution. But I don't know how to prove this.,I'm not sure how to solve this. My teacher told me that if has one unique solution (the trivial) then has only one unique solution. But I don't know how to prove this.,A\vec{x}=0 A\vec{x}=\vec{b},['linear-algebra']
49,If two graphs are isomorphic then will their determinants be equal?,If two graphs are isomorphic then will their determinants be equal?,,"10 vertices graphs G1 and G2 I'm solving the exercises in chapter 2 from Graph Theory with Algorithms and its Applications. So far, for isomorphism I just write all the edges and try to find the patterns, which for small graphs is enough. For these 2 10-vertices 30-edges graph I think I need something more scalable. I tried 'translating' both graphs to a unified structure, but off course if one vertx changes name the graph will also look different. I wrote the adjacency matrix trying to find a pattern, and there's a column that makes me suspicious these two may not be isomorphic. However, I could always change the name of a vertex to make columns look a bit more similar. Anyway, I found that two graphs (A,B) are isomorphic if A = PBP(t) where P is a permutation matrix, and P(t) is its transpose. Since det(P) = 1 or -1 = det(P(t)) and det(AB) = det(A)*det(B) then if A and B are isomorphic: det(A) = 1|-1*det(B)*1|-1 det(A)=det(B) could two adjacency matrices have the same determinant and not be isomorphic? or is equality in the determinant a sufficient condition (and much more scalable) for isomorphism? Best, Sergio","10 vertices graphs G1 and G2 I'm solving the exercises in chapter 2 from Graph Theory with Algorithms and its Applications. So far, for isomorphism I just write all the edges and try to find the patterns, which for small graphs is enough. For these 2 10-vertices 30-edges graph I think I need something more scalable. I tried 'translating' both graphs to a unified structure, but off course if one vertx changes name the graph will also look different. I wrote the adjacency matrix trying to find a pattern, and there's a column that makes me suspicious these two may not be isomorphic. However, I could always change the name of a vertex to make columns look a bit more similar. Anyway, I found that two graphs (A,B) are isomorphic if A = PBP(t) where P is a permutation matrix, and P(t) is its transpose. Since det(P) = 1 or -1 = det(P(t)) and det(AB) = det(A)*det(B) then if A and B are isomorphic: det(A) = 1|-1*det(B)*1|-1 det(A)=det(B) could two adjacency matrices have the same determinant and not be isomorphic? or is equality in the determinant a sufficient condition (and much more scalable) for isomorphism? Best, Sergio",,"['linear-algebra', 'graph-theory', 'determinant', 'graph-isomorphism', 'adjacency-matrix']"
50,"If $\{x_1,x_2,\cdots,x_n\}$ is a basis, is $\{x_1+x_2,x_2+x_3,\cdots,x_n+x_1\}$ a basis too?","If  is a basis, is  a basis too?","\{x_1,x_2,\cdots,x_n\} \{x_1+x_2,x_2+x_3,\cdots,x_n+x_1\}","Let's say we have a vector space $V$ with a basis $\{x_1,x_2,\cdots,x_n\}$ then is $\{x_1+x_2,x_2+x_3,\cdots,x_{n-1}+x_n,x_n+x_1\}$ a basis too? My Answer: For n=2 clearly this is false because of the following counter example: \begin{pmatrix} 1 & 0 \\ 0 & 1  \end{pmatrix} If we apply the above to get the new set \begin{pmatrix} 1 & 1 \\ 1 & 1  \end{pmatrix} which is not linearly indepedent to form a basis. But what about $n\geq3 ?$ I believe it should work by intuition that $v_1 = x_1+x_2$ can only be formed using $x_1$ and $x_2$ and so on hence any of the vectors cannot be formed using the others by any linear combination.",Let's say we have a vector space with a basis then is a basis too? My Answer: For n=2 clearly this is false because of the following counter example: If we apply the above to get the new set which is not linearly indepedent to form a basis. But what about I believe it should work by intuition that can only be formed using and and so on hence any of the vectors cannot be formed using the others by any linear combination.,"V \{x_1,x_2,\cdots,x_n\} \{x_1+x_2,x_2+x_3,\cdots,x_{n-1}+x_n,x_n+x_1\} \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix} \begin{pmatrix}
1 & 1 \\
1 & 1 
\end{pmatrix} n\geq3 ? v_1 = x_1+x_2 x_1 x_2","['linear-algebra', 'change-of-basis']"
51,"If $A$ is a $2\times2$ integer matrix with $\det(A)=1$ and $|\text{tr}(A)|>2$, then $A^n\neq I$.","If  is a  integer matrix with  and , then .",A 2\times2 \det(A)=1 |\text{tr}(A)|>2 A^n\neq I,"If $A \in \Bbb Z^{2\times 2}$ with $\det(A)=1$ and $\left| \text{tr} (A)\right|>2$ , then $A^n\neq I$ for all $n\in \mathbb{N}$ . I tried to prove this by induction and by contradiction, but I'm quite a bit lost here because it seems that it all reduces to obtain a contradiction about the numbers. I don't see a way around that. Any suggestions?","If with and , then for all . I tried to prove this by induction and by contradiction, but I'm quite a bit lost here because it seems that it all reduces to obtain a contradiction about the numbers. I don't see a way around that. Any suggestions?",A \in \Bbb Z^{2\times 2} \det(A)=1 \left| \text{tr} (A)\right|>2 A^n\neq I n\in \mathbb{N},"['linear-algebra', 'matrices']"
52,Choosing the sign of determinant when taking a square root,Choosing the sign of determinant when taking a square root,,"Calculate the determinant $$\det(A)=\begin{vmatrix}a&b&c&d\\ \:\:\:-b&a&d&-c\\ \:\:\:-c&-d&a&b\\ \:\:\:-d&c&-b&a\end{vmatrix}$$ I found that $$\det(A)\det(A^T)=\det(A)^2=(a^2+b^2+c^2+d^2)^4$$ From this we get $$\det(A) = \pm (a^2+b^2+c^2+d^2)^2$$ Now, how to choose the sign? Any help is appreciated.","Calculate the determinant I found that From this we get Now, how to choose the sign? Any help is appreciated.",\det(A)=\begin{vmatrix}a&b&c&d\\ \:\:\:-b&a&d&-c\\ \:\:\:-c&-d&a&b\\ \:\:\:-d&c&-b&a\end{vmatrix} \det(A)\det(A^T)=\det(A)^2=(a^2+b^2+c^2+d^2)^4 \det(A) = \pm (a^2+b^2+c^2+d^2)^2,"['linear-algebra', 'matrices', 'determinant']"
53,"Do we have for all $M \in SL_n(\Bbb K)$, $\lVert M \rVert \geq 1$ when $\lVert \cdot \rVert$ is a matrix norm?","Do we have for all ,  when  is a matrix norm?",M \in SL_n(\Bbb K) \lVert M \rVert \geq 1 \lVert \cdot \rVert,"Let be $\lVert \cdot \rVert$ a matrix norm (submultiplicative). Do we have for all matrices of determinant 1, the following lower bound: $$\lVert M \rVert \geq 1$$ I'm very confused and could not find any counterexample and I find this statement very fishy, I tried to experiment with: \begin{bmatrix} 1&  x \\ 0&  1 \end{bmatrix} But, its Frobenius norm cannot be small enough.","Let be a matrix norm (submultiplicative). Do we have for all matrices of determinant 1, the following lower bound: I'm very confused and could not find any counterexample and I find this statement very fishy, I tried to experiment with: But, its Frobenius norm cannot be small enough.","\lVert \cdot \rVert \lVert M \rVert \geq 1 \begin{bmatrix}
1&  x \\
0&  1
\end{bmatrix}","['linear-algebra', 'matrices', 'determinant', 'normed-spaces', 'matrix-norms']"
54,Do all projections matrices take this form?,Do all projections matrices take this form?,,"Do all projection matrices take the form $P = A{(A^TA)}^{-1}A^T$ ? If so, can you help me derive it and explain it intuitively?","Do all projection matrices take the form ? If so, can you help me derive it and explain it intuitively?",P = A{(A^TA)}^{-1}A^T,"['linear-algebra', 'matrices', 'projective-geometry']"
55,Find determinant and trace of product of non square matrices,Find determinant and trace of product of non square matrices,,"Let $B \in M_{3,2}(\mathbb{R})$ , $C \in M_{2,3}(\mathbb{R})$ so that $BC=\begin{pmatrix}2 & -2 & 3\\ 0 & 0 & 3\\ 0 & 0 & 3  \end{pmatrix}$ . Find $\det(CB)$ and $Tr(CB)$ . My attempt : I tried to compute the powers of $BC$ (I actually hoped that $BC$ was idempotent) and I saw that $(BC)^n=\begin{pmatrix}2^n & -2^n & 3^n\\ 0 & 0 & 3^n\\ 0 & 0 & 3^n \end{pmatrix}$ which doesn't really help.","Let , so that . Find and . My attempt : I tried to compute the powers of (I actually hoped that was idempotent) and I saw that which doesn't really help.","B \in M_{3,2}(\mathbb{R}) C \in M_{2,3}(\mathbb{R}) BC=\begin{pmatrix}2 & -2 & 3\\
0 & 0 & 3\\
0 & 0 & 3 
\end{pmatrix} \det(CB) Tr(CB) BC BC (BC)^n=\begin{pmatrix}2^n & -2^n & 3^n\\
0 & 0 & 3^n\\
0 & 0 & 3^n
\end{pmatrix}","['linear-algebra', 'matrices']"
56,"Showing that any square matrix in $\mathbb{R}^{n \times n}$ has a ""square root""","Showing that any square matrix in  has a ""square root""",\mathbb{R}^{n \times n},"Prove that there exists $\delta > 0$ so that for all square matrices $A\in \mathbb{R}^{n\times n}$ with $\|A-I\| < \delta$ (where $I$ denotes the identity matrix) there exists $B\in \mathbb{R}^{n\times n}$ so that $B^2=A$ . My attempt so far: $$A-I= \begin{bmatrix} a_{11}-1 & a_{12} & ... & a_{1n} \\ a_{21} & a_{22}-1 & ... & a_{2n} \\ \vdots \\ a_{n1} & a_{n2} & ... & a_{nn}-1 \\ \end{bmatrix} $$ Taking $x=(1,0,...,0)\in \mathbb{R}^n$ , we have that $$\|A-I\|_{op}=\underset{\|x\|=1}{\sup}\|(A-I)x\|= \sqrt{(a_{11}-1)^2+...+a_{n1}^2}<\delta$$ Intuitively, this seems to suggest that for each basis vector of $\mathbb{R}^n$ means that $A-I$ can be made close to the zero matrix, and hence, close to being a diagonal matrix. But I am having trouble going from here. Anyone have any hints?","Prove that there exists so that for all square matrices with (where denotes the identity matrix) there exists so that . My attempt so far: Taking , we have that Intuitively, this seems to suggest that for each basis vector of means that can be made close to the zero matrix, and hence, close to being a diagonal matrix. But I am having trouble going from here. Anyone have any hints?","\delta > 0 A\in \mathbb{R}^{n\times n} \|A-I\| < \delta I B\in \mathbb{R}^{n\times n} B^2=A A-I= \begin{bmatrix}
a_{11}-1 & a_{12} & ... & a_{1n} \\
a_{21} & a_{22}-1 & ... & a_{2n} \\
\vdots \\
a_{n1} & a_{n2} & ... & a_{nn}-1 \\
\end{bmatrix}  x=(1,0,...,0)\in \mathbb{R}^n \|A-I\|_{op}=\underset{\|x\|=1}{\sup}\|(A-I)x\|= \sqrt{(a_{11}-1)^2+...+a_{n1}^2}<\delta \mathbb{R}^n A-I","['real-analysis', 'linear-algebra', 'matrices']"
57,"Given the equation $\sum_{k=11}^{99} \left[x + \frac{k}{100} \right] = 765$ find $[10 \, x]$.",Given the equation  find .,"\sum_{k=11}^{99} \left[x + \frac{k}{100} \right] = 765 [10 \, x]","Given the equation $\sum_{k=11}^{99} \left[x + \frac{k}{100} \right] = 765$ find $[10 \, x]$ . I have tried this problem in many ways and I think it uses the identity $$[x]+\left[x+\frac{1}{n}\right]+\left[x+\frac{2}{n}\right]+\left[x+\frac{3}{n}\right]+.....+\left[x+\frac{n-1}{n}\right]= [nx]$$ Could someone please help me and keep in mind x is a real number and $[\cdot]$ denotes the greatest integer function",Given the equation find . I have tried this problem in many ways and I think it uses the identity Could someone please help me and keep in mind x is a real number and denotes the greatest integer function,"\sum_{k=11}^{99} \left[x + \frac{k}{100} \right] = 765 [10 \, x] [x]+\left[x+\frac{1}{n}\right]+\left[x+\frac{2}{n}\right]+\left[x+\frac{3}{n}\right]+.....+\left[x+\frac{n-1}{n}\right]= [nx] [\cdot]","['linear-algebra', 'sequences-and-series', 'polynomials', 'ceiling-and-floor-functions']"
58,"cross product not associative, outer product associative","cross product not associative, outer product associative",,"The cross product is not associative. If $i=(1,0,0)$, $j=(0,1,0)$ and $k=(0,0,1)$, then  \begin{eqnarray}  i \times (i \times j) = i \times k = -j \\  (i \times i) \times j = 0 \end{eqnarray} However in Geometric Algebra, if $e_1=(1,0,0)$, $e_2=(0,1,0)$ and $e_3=(0,0,1)$ then  \begin{eqnarray}   e_1 \wedge (e_1 \wedge e_2) = (e_1 \wedge e_1 ) \wedge e_2 = 0 \end{eqnarray} According to D. Hestenes, New Foundations for Classical Mechanics (equation 3.13) \begin{eqnarray}    a \times b = -i a \wedge b  \quad, i=\sqrt{-1}  \end{eqnarray} So, where is the flaw here? Except for a complex scalar both definitions in $\mathbb{R}^3$ are the same. But.....one is associative and the other is not? Thanks.","The cross product is not associative. If $i=(1,0,0)$, $j=(0,1,0)$ and $k=(0,0,1)$, then  \begin{eqnarray}  i \times (i \times j) = i \times k = -j \\  (i \times i) \times j = 0 \end{eqnarray} However in Geometric Algebra, if $e_1=(1,0,0)$, $e_2=(0,1,0)$ and $e_3=(0,0,1)$ then  \begin{eqnarray}   e_1 \wedge (e_1 \wedge e_2) = (e_1 \wedge e_1 ) \wedge e_2 = 0 \end{eqnarray} According to D. Hestenes, New Foundations for Classical Mechanics (equation 3.13) \begin{eqnarray}    a \times b = -i a \wedge b  \quad, i=\sqrt{-1}  \end{eqnarray} So, where is the flaw here? Except for a complex scalar both definitions in $\mathbb{R}^3$ are the same. But.....one is associative and the other is not? Thanks.",,"['linear-algebra', 'clifford-algebras']"
59,Determine if the three matrices span the vector space of $2\times 2$ matrices,Determine if the three matrices span the vector space of  matrices,2\times 2,"Thus far I've seen vectors and polynomials but this the first and only exercise I find that introduces matrices. The question is as follows: Determine whether the three matrices $\begin{pmatrix}     1 &  1 \\     1 & 0\\ \end{pmatrix}$, $\begin{pmatrix}     -1 &  0 \\     0 & 1\\ \end{pmatrix}$, $\begin{pmatrix}      0 &  1 \\     1 & 2\\ \end{pmatrix}$ span the vector space of all 2x2 symmetric matrices. I am stuck at this stage because previously I would find the matrix of the vectors or polynomials and work on that, but this time it's 3 matrices, what are the steps that I should follow to always get it right?","Thus far I've seen vectors and polynomials but this the first and only exercise I find that introduces matrices. The question is as follows: Determine whether the three matrices $\begin{pmatrix}     1 &  1 \\     1 & 0\\ \end{pmatrix}$, $\begin{pmatrix}     -1 &  0 \\     0 & 1\\ \end{pmatrix}$, $\begin{pmatrix}      0 &  1 \\     1 & 2\\ \end{pmatrix}$ span the vector space of all 2x2 symmetric matrices. I am stuck at this stage because previously I would find the matrix of the vectors or polynomials and work on that, but this time it's 3 matrices, what are the steps that I should follow to always get it right?",,"['linear-algebra', 'matrices']"
60,What is the difference between linear function and linear map(transformation)?,What is the difference between linear function and linear map(transformation)?,,"I know that linear mapping is `T: V-> W where V,W are vector spaces` and linear function is f: V->F where V is a vector space and F is a field But I don't really know the difference intuitively. What are the examples of linear map and linear function? And is linear mapping included in linear function?","I know that linear mapping is `T: V-> W where V,W are vector spaces` and linear function is f: V->F where V is a vector space and F is a field But I don't really know the difference intuitively. What are the examples of linear map and linear function? And is linear mapping included in linear function?",,['linear-algebra']
61,Equation of the sphere that passes through 4 points,Equation of the sphere that passes through 4 points,,"Write he equation of the sphere that passes through points $$a(-5,4,1),b(3,4,-5),c(0,0,4),d(0,0,0)$$ I tried to use four points to draw a geometric shape and then calculate the center of this shape on the basis of the circle that passing  on four points. But I did not succeed Here is the book answer   $$x^2+y^2+z^2+54x−58y+4z=0$$","Write he equation of the sphere that passes through points $$a(-5,4,1),b(3,4,-5),c(0,0,4),d(0,0,0)$$ I tried to use four points to draw a geometric shape and then calculate the center of this shape on the basis of the circle that passing  on four points. But I did not succeed Here is the book answer   $$x^2+y^2+z^2+54x−58y+4z=0$$",,"['linear-algebra', 'geometry', 'analytic-geometry']"
62,How to demonstrate $\operatorname{tr}(AB)=0$ when $A$ is symmetric and $B$ is antisymmetric,How to demonstrate  when  is symmetric and  is antisymmetric,\operatorname{tr}(AB)=0 A B,"For a square matrix $A$ of order $n$, let $\operatorname{tr}(A)$ be the sum of the diagonal entries. $\mbox{tr}(A)=\sum_{i=1}^{n} \textrm{a}_\textrm{ii}$ Assume that $A$ is symmetric and $B$ is antisymmetric, how to demonstrate that $\operatorname{tr}(AB)=0$?","For a square matrix $A$ of order $n$, let $\operatorname{tr}(A)$ be the sum of the diagonal entries. $\mbox{tr}(A)=\sum_{i=1}^{n} \textrm{a}_\textrm{ii}$ Assume that $A$ is symmetric and $B$ is antisymmetric, how to demonstrate that $\operatorname{tr}(AB)=0$?",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
63,"Given this matrix $A$, find $A^{144}$","Given this matrix , find",A A^{144},"I know this is a very common question to ask when one is making their way into Linear Algebra (i.e. given a matrix, find the result of that matrix to the nth -power). I'm given this matrix: $$    A=   \left[ {\begin{array}{cc}    0 & -1 \\    1 & 0 \\   \end{array} } \right]$$ I'm asked to compute, by hand, the matrix $A^{144}$ What I tried: I calculated $A^2$, $A^3$, $A^4$ and $A^5$ and tried to find a pattern, so that I could first find the more general $A^n$ expression, and then make $n=144$. This is what I got: $$    A^2=   \left[ {\begin{array}{cc}    -1 & 0 \\    0 & -1 \\   \end{array} } \right]$$ $$    A^3=   \left[ {\begin{array}{cc}    0 & 1 \\    -1 & 0 \\   \end{array} } \right]$$ $$    A^4=   \left[ {\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array} } \right]$$ $$    A^5=   \left[ {\begin{array}{cc}    0 & -1 \\    1 & 0 \\   \end{array} } \right]$$ So there seems to be kind of a ""circular pattern"", where $A^6=A^2$ and therefore, since $144/6=24$, I'm suspecting that: $$¿\,\boxed{A^{144}=A^2} \,?$$ However, I was hoping to find the more general $A^n$ matrix first and confirm the above result. But, I can't seem to find a function such that: $$f(n) = \left\{ 	\begin{array}{ll} 		0  & \mbox{if n is odd}  \\ 		1 & \mbox{if n is even}  	\end{array} \right. $$ This function would allow me to sort out the (1,1) and (2,2) elements of $A^n$ being 0 when n is odd (and the (1,2) and (2,1) being 0 when n is even). I'm a bit confused at this point, your help is greatly appreciated.","I know this is a very common question to ask when one is making their way into Linear Algebra (i.e. given a matrix, find the result of that matrix to the nth -power). I'm given this matrix: $$    A=   \left[ {\begin{array}{cc}    0 & -1 \\    1 & 0 \\   \end{array} } \right]$$ I'm asked to compute, by hand, the matrix $A^{144}$ What I tried: I calculated $A^2$, $A^3$, $A^4$ and $A^5$ and tried to find a pattern, so that I could first find the more general $A^n$ expression, and then make $n=144$. This is what I got: $$    A^2=   \left[ {\begin{array}{cc}    -1 & 0 \\    0 & -1 \\   \end{array} } \right]$$ $$    A^3=   \left[ {\begin{array}{cc}    0 & 1 \\    -1 & 0 \\   \end{array} } \right]$$ $$    A^4=   \left[ {\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array} } \right]$$ $$    A^5=   \left[ {\begin{array}{cc}    0 & -1 \\    1 & 0 \\   \end{array} } \right]$$ So there seems to be kind of a ""circular pattern"", where $A^6=A^2$ and therefore, since $144/6=24$, I'm suspecting that: $$¿\,\boxed{A^{144}=A^2} \,?$$ However, I was hoping to find the more general $A^n$ matrix first and confirm the above result. But, I can't seem to find a function such that: $$f(n) = \left\{ 	\begin{array}{ll} 		0  & \mbox{if n is odd}  \\ 		1 & \mbox{if n is even}  	\end{array} \right. $$ This function would allow me to sort out the (1,1) and (2,2) elements of $A^n$ being 0 when n is odd (and the (1,2) and (2,1) being 0 when n is even). I'm a bit confused at this point, your help is greatly appreciated.",,"['linear-algebra', 'matrices']"
64,Meaning of Dot Products in Regards to Linear Algebra,Meaning of Dot Products in Regards to Linear Algebra,,"I'm currently taking an intro to linear algebra course. We have reached the section on dot products and I fail to understand the meaning of it. What does it tell me, besides spitting out a scalar? I initially read that the answer is similar to asking what the point of multiplication is, but I understand why multiplication is useful.","I'm currently taking an intro to linear algebra course. We have reached the section on dot products and I fail to understand the meaning of it. What does it tell me, besides spitting out a scalar? I initially read that the answer is similar to asking what the point of multiplication is, but I understand why multiplication is useful.",,"['linear-algebra', 'soft-question', 'vectors']"
65,write the given matrix as a product of elementary matrices,write the given matrix as a product of elementary matrices,,Please explain this in detail because i simply cannot understand previous explanations I have read for this type of problem. By the way this is from elementary linear algebra 10th edition section 1.5 exercise #29. There is a copy online if you want to check the problem out. Write the given matrix as a product of elementary matrices. \begin{bmatrix}-3&1\\2&2\end{bmatrix},Please explain this in detail because i simply cannot understand previous explanations I have read for this type of problem. By the way this is from elementary linear algebra 10th edition section 1.5 exercise #29. There is a copy online if you want to check the problem out. Write the given matrix as a product of elementary matrices. \begin{bmatrix}-3&1\\2&2\end{bmatrix},,['linear-algebra']
66,Tensor Products and a Basis of $\Bbb R^2 ⊗ \Bbb R^2$,Tensor Products and a Basis of,\Bbb R^2 ⊗ \Bbb R^2,"Let ${e_1, e_2}$ be the standard basis of $\Bbb R^2$.  Show that $e_1⊗e_2+e_2⊗e_1$ cannot be written in the form $u ⊗ v$ with $u,v \in \Bbb R^2$. I am just being introduced to tensor spaces and I know that $e_1⊗e_1,e_1⊗e_2,e_2⊗e_1,e_2⊗e_2$ is a basis of $\Bbb R^2 ⊗ \Bbb R^2$ but I am not sure how to show a contradiction. Any hints appreciated. Edit: I also know that   $e_1⊗e_2+e_2⊗e_1 = (e_1+e_2⊗e_1+e_2)  - e_1⊗e_1 - e_2⊗e_2$","Let ${e_1, e_2}$ be the standard basis of $\Bbb R^2$.  Show that $e_1⊗e_2+e_2⊗e_1$ cannot be written in the form $u ⊗ v$ with $u,v \in \Bbb R^2$. I am just being introduced to tensor spaces and I know that $e_1⊗e_1,e_1⊗e_2,e_2⊗e_1,e_2⊗e_2$ is a basis of $\Bbb R^2 ⊗ \Bbb R^2$ but I am not sure how to show a contradiction. Any hints appreciated. Edit: I also know that   $e_1⊗e_2+e_2⊗e_1 = (e_1+e_2⊗e_1+e_2)  - e_1⊗e_1 - e_2⊗e_2$",,"['linear-algebra', 'tensor-products']"
67,A $3\times3$ matrix with rank 2 must have a nonzero solution to $Ax=0$,A  matrix with rank 2 must have a nonzero solution to,3\times3 Ax=0,I have recently come across the following statement: A $3\times3$ matrix A with rank $2$ must have a nonzero solution to $Ax=0.$ I am having trouble understanding why there must be a nonzero solution to $Ax = 0.$ Is it due to the fact that we will have a free variable?,I have recently come across the following statement: A $3\times3$ matrix A with rank $2$ must have a nonzero solution to $Ax=0.$ I am having trouble understanding why there must be a nonzero solution to $Ax = 0.$ Is it due to the fact that we will have a free variable?,,['linear-algebra']
68,Are all matrices that fulfill $x^n-x=0$ are diagonalizable?,Are all matrices that fulfill  are diagonalizable?,x^n-x=0,"Let $f(x)=x^n-x$ prove/disprove all matrices such that $f(A)=0$ are diagonalizable? I have tried to find conditions on the minimal polynomial but it did not work, any suggestions?","Let $f(x)=x^n-x$ prove/disprove all matrices such that $f(A)=0$ are diagonalizable? I have tried to find conditions on the minimal polynomial but it did not work, any suggestions?",,['linear-algebra']
69,Prove or give a counterexample: $V=\text{null}T+\text{range}T$,Prove or give a counterexample:,V=\text{null}T+\text{range}T,"If $V$ is a finite-dimensional vector space and $T:V\rightarrow V$ is linear then $$V= \text{null}(T) + \text{range} (T).$$ I know a counterexample if the sum is replaced with a direct sum. However I can't see why the statement above would be false. Both $\text{null}(T)$ and $\text{range} (T)$  are subspaces of $V$ and $\text{null}(T) \cup\text{range} (T)=V$, so why wouldn't this statement be true? I just want to know if there is something I am missing here...","If $V$ is a finite-dimensional vector space and $T:V\rightarrow V$ is linear then $$V= \text{null}(T) + \text{range} (T).$$ I know a counterexample if the sum is replaced with a direct sum. However I can't see why the statement above would be false. Both $\text{null}(T)$ and $\text{range} (T)$  are subspaces of $V$ and $\text{null}(T) \cup\text{range} (T)=V$, so why wouldn't this statement be true? I just want to know if there is something I am missing here...",,"['linear-algebra', 'linear-transformations', 'direct-sum']"
70,Proof/Intuition for Eigenvalues to Solve Linear Differential Equations,Proof/Intuition for Eigenvalues to Solve Linear Differential Equations,,To solve an equation of the form $$\frac{dx}{dt}=ax+by \\ \frac{dy}{dt}=cx+dy$$ Does anyone know the reasoning why you solve for eigenvalues and eigenvectors to determine the functions for x and y with e raised to the power of the eigenvalue times t? I can get an answer just curious as to why the method works out. I attached a link https://www.youtube.com/watch?v=1_EPFlwS7Kc with an example. So my question is to prove this method to solve linear differential equations.,To solve an equation of the form $$\frac{dx}{dt}=ax+by \\ \frac{dy}{dt}=cx+dy$$ Does anyone know the reasoning why you solve for eigenvalues and eigenvectors to determine the functions for x and y with e raised to the power of the eigenvalue times t? I can get an answer just curious as to why the method works out. I attached a link https://www.youtube.com/watch?v=1_EPFlwS7Kc with an example. So my question is to prove this method to solve linear differential equations.,,['linear-algebra']
71,Prove that the determinant is $(a-b)(b-c)(c-a)(a+b+c)$,Prove that the determinant is,(a-b)(b-c)(c-a)(a+b+c),I have the determinant : \begin{vmatrix} 1 &1 &1 \\ a &b &c \\ a^3 &b^3 &c^3 \\ \end{vmatrix} How do I prove that this determinant is equal to $$ (a-b)(b-c)(c-a)(a+b+c) $$,I have the determinant : \begin{vmatrix} 1 &1 &1 \\ a &b &c \\ a^3 &b^3 &c^3 \\ \end{vmatrix} How do I prove that this determinant is equal to $$ (a-b)(b-c)(c-a)(a+b+c) $$,,"['linear-algebra', 'determinant']"
72,Dimension of solution space of homogeneous system of linear equations,Dimension of solution space of homogeneous system of linear equations,,"I have the homogeneous system of linear equations $$ 3x_1 + 3x_2 + 15x_3 + 11x_4 = 0, $$ $$ x_1 − 3x_2 + x_3 + x_4 = 0, $$ $$ 2x_1 + 3x_2 + 11x_3 + 8x_4 = 0. $$ I have converted to a augmented matrix and row reduced to  $$\begin{bmatrix}1 & 0 & 4 & -3 & 0\\0 & 1 & 1 & 2/3 & 0\\0 & 0 & 0 & 0 & 0\end{bmatrix}$$ And came up with the general solution: $$\begin{bmatrix}x_1 \\x_2\\x_3\\x_4\end{bmatrix} = s\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix}+ t\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}$$ I know that the basis is: $$\left\{\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix},\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}\right\}$$ But how do I determine the dimension of the solution space?","I have the homogeneous system of linear equations $$ 3x_1 + 3x_2 + 15x_3 + 11x_4 = 0, $$ $$ x_1 − 3x_2 + x_3 + x_4 = 0, $$ $$ 2x_1 + 3x_2 + 11x_3 + 8x_4 = 0. $$ I have converted to a augmented matrix and row reduced to  $$\begin{bmatrix}1 & 0 & 4 & -3 & 0\\0 & 1 & 1 & 2/3 & 0\\0 & 0 & 0 & 0 & 0\end{bmatrix}$$ And came up with the general solution: $$\begin{bmatrix}x_1 \\x_2\\x_3\\x_4\end{bmatrix} = s\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix}+ t\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}$$ I know that the basis is: $$\left\{\begin{bmatrix}4 \\-1\\1\\0\end{bmatrix},\begin{bmatrix}-3\\-2/3\\0\\1\end{bmatrix}\right\}$$ But how do I determine the dimension of the solution space?",,"['linear-algebra', 'matrices', 'homogeneous-equation']"
73,What does it mean for the determinant of $A^T A$ to be equal to zero?,What does it mean for the determinant of  to be equal to zero?,A^T A,Are there any statements that are equivalent to $\det(A^T A)=0$?,Are there any statements that are equivalent to $\det(A^T A)=0$?,,['linear-algebra']
74,Prove that if $AA^T=A$ then $A^3=A$,Prove that if  then,AA^T=A A^3=A,"The approach I'd like to use to prove this particular property necessitates that $A$ be invertible, but I don't wish to assume this (though it would certainly make the task simpler). Is there some property which shows $A$ to be invertible which I am overlooking, or is it that perhaps I need to use a different method of proof?","The approach I'd like to use to prove this particular property necessitates that $A$ be invertible, but I don't wish to assume this (though it would certainly make the task simpler). Is there some property which shows $A$ to be invertible which I am overlooking, or is it that perhaps I need to use a different method of proof?",,"['linear-algebra', 'matrices', 'inverse']"
75,Balancing chemical equations using linear algebraic methods,Balancing chemical equations using linear algebraic methods,,I know there are already plenty of questions on this site regarding this topic but I am having difficulty with a particular chemical equation. I am trying to balance the following: $$ { C }_{ 2 }{ H }_{ 2 }{ Cl }_{ 4 }\quad +\quad { C }a{ { (OH }) }_{ 2 }\quad \xrightarrow [  ]{  } \quad { C }_{ 2 }{ H }{ Cl }_{ 3 }\quad +\quad Ca{ Cl }_{ 2 }\quad +\quad { H }_{ 2 }{ O } $$ The system of linear equations produces the following augmented matrix: $$ \begin{pmatrix} 2 & 0 & -2 & 0 & 0 & 0 \\ 2 & 2 & -1 & 0 & -2 & 0 \\ 4 & 0 & -3 & -2 & 0 & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 & -1 & 0 \end{pmatrix} $$ With the rows in the following order: Carbon Hydrogen Chlorine Calcium Oxygen In row echelon form this reduces to: $$ \begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \end{pmatrix} $$ Which would indicate that: x1 = 0; x2 = 0; x3 = 0; x4 = 0; x5 = 0 which is obviously not correct. What have I done wrong?,I know there are already plenty of questions on this site regarding this topic but I am having difficulty with a particular chemical equation. I am trying to balance the following: $$ { C }_{ 2 }{ H }_{ 2 }{ Cl }_{ 4 }\quad +\quad { C }a{ { (OH }) }_{ 2 }\quad \xrightarrow [  ]{  } \quad { C }_{ 2 }{ H }{ Cl }_{ 3 }\quad +\quad Ca{ Cl }_{ 2 }\quad +\quad { H }_{ 2 }{ O } $$ The system of linear equations produces the following augmented matrix: $$ \begin{pmatrix} 2 & 0 & -2 & 0 & 0 & 0 \\ 2 & 2 & -1 & 0 & -2 & 0 \\ 4 & 0 & -3 & -2 & 0 & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 & -1 & 0 \end{pmatrix} $$ With the rows in the following order: Carbon Hydrogen Chlorine Calcium Oxygen In row echelon form this reduces to: $$ \begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \end{pmatrix} $$ Which would indicate that: x1 = 0; x2 = 0; x3 = 0; x4 = 0; x5 = 0 which is obviously not correct. What have I done wrong?,,['linear-algebra']
76,Finding a basis of a complex vector space over $\Bbb R$ given a basis over $\Bbb C$,Finding a basis of a complex vector space over  given a basis over,\Bbb R \Bbb C,"Suppose $X$ is a vector space over $\mathbb C$ and  has as basis $\{e_1,e_2,\ldots,e_n\}$. Now regard $X$ as a vector space over $\mathbb R$. What will be the basis? My thoughts: I considered $\mathbb C$ over $\mathbb C$ and $\mathbb C$ over $\mathbb R$.In the first case we have $(1,0)$ as basis and in the latter case we have $\{(1,0),(0,1)\}$ as basis i.e. $\{(1,0),(0,1)(1,0)=(0,1)\}$ as basis. So may be the answer is $\{e_1,e_2,\ldots,e_n,ie_1,\ldots,ie_n\}$. How to justify the result if its true?","Suppose $X$ is a vector space over $\mathbb C$ and  has as basis $\{e_1,e_2,\ldots,e_n\}$. Now regard $X$ as a vector space over $\mathbb R$. What will be the basis? My thoughts: I considered $\mathbb C$ over $\mathbb C$ and $\mathbb C$ over $\mathbb R$.In the first case we have $(1,0)$ as basis and in the latter case we have $\{(1,0),(0,1)\}$ as basis i.e. $\{(1,0),(0,1)(1,0)=(0,1)\}$ as basis. So may be the answer is $\{e_1,e_2,\ldots,e_n,ie_1,\ldots,ie_n\}$. How to justify the result if its true?",,"['linear-algebra', 'vector-spaces']"
77,Functional Analysis Question?,Functional Analysis Question?,,"I have a question about functional analysis. I know in finite dimensional space $\mathbb{C}^n$, all bases have the same cardinality. However let us consider $L^{2}[-\pi,\pi]$ which has TWO bases $\delta_{y}(x)$ for $y \in [-\pi,\pi]$ and fourier series $\{ e^{i n x} \}_{n=-\infty}^{\infty}$. One is countable, the other is not Voila! Same spaces different (cardinality) bases? Please give reference to how this can happen? Has there been any research on this (recently)?","I have a question about functional analysis. I know in finite dimensional space $\mathbb{C}^n$, all bases have the same cardinality. However let us consider $L^{2}[-\pi,\pi]$ which has TWO bases $\delta_{y}(x)$ for $y \in [-\pi,\pi]$ and fourier series $\{ e^{i n x} \}_{n=-\infty}^{\infty}$. One is countable, the other is not Voila! Same spaces different (cardinality) bases? Please give reference to how this can happen? Has there been any research on this (recently)?",,"['linear-algebra', 'functional-analysis', 'reference-request']"
78,How to calculate the inversion of a triangular matrix,How to calculate the inversion of a triangular matrix,,Now I want to write a piece of code to calculate the inversion of a triangular matrix which do it in parallel. I know that the equation of the triangular matrix's inversion is like this: But I want my program to calculate this parallel.In this way I can't calculate this in parallel because in each computer I can't get the data in the red circle in this picture. Can anybody tell me is there other ways to calculate the inversion of triangular matrix?,Now I want to write a piece of code to calculate the inversion of a triangular matrix which do it in parallel. I know that the equation of the triangular matrix's inversion is like this: But I want my program to calculate this parallel.In this way I can't calculate this in parallel because in each computer I can't get the data in the red circle in this picture. Can anybody tell me is there other ways to calculate the inversion of triangular matrix?,,"['linear-algebra', 'matrices']"
79,Eigenvalues of a 2x2 matrix A such that $A^2$=I,Eigenvalues of a 2x2 matrix A such that =I,A^2,"I have no idea where to begin. Let A be a $2\times 2$ matrix such that $A^2= I$, where $I$ is the identity matrix.  Find all the eigenvalues of $A$. I know there are a few matrices that support this claim, will they all have the same eigenvalues?","I have no idea where to begin. Let A be a $2\times 2$ matrix such that $A^2= I$, where $I$ is the identity matrix.  Find all the eigenvalues of $A$. I know there are a few matrices that support this claim, will they all have the same eigenvalues?",,"['linear-algebra', 'matrices']"
80,How do I intuitively understand what this linear transformation matrix is?,How do I intuitively understand what this linear transformation matrix is?,,$\begin{bmatrix}0 & 1 \\ -1 & 0 \end{bmatrix}$ I know how to get the product when given another matrix. But how do I know what this matrix is doing simply by looking at it?,$\begin{bmatrix}0 & 1 \\ -1 & 0 \end{bmatrix}$ I know how to get the product when given another matrix. But how do I know what this matrix is doing simply by looking at it?,,['linear-algebra']
81,An explicit basis for an infinite dimensional vector space.,An explicit basis for an infinite dimensional vector space.,,"Do you have any explicit example of an infinite dimensional vector space, with an explicit basis ? Not an Hilbert basis but a family of  linearly independent vectors which spans the space -any $x$ in the space is a finite linear sum of elements of the basis. In general the existence of such a basis follows by the Axiom of choice but I wonder if there is at least one non trivial (not finite dimensional) case where we have some explicit constuction.","Do you have any explicit example of an infinite dimensional vector space, with an explicit basis ? Not an Hilbert basis but a family of  linearly independent vectors which spans the space -any $x$ in the space is a finite linear sum of elements of the basis. In general the existence of such a basis follows by the Axiom of choice but I wonder if there is at least one non trivial (not finite dimensional) case where we have some explicit constuction.",,"['linear-algebra', 'analysis', 'vector-spaces']"
82,Difference between sum of vector spaces and union of subspaces?,Difference between sum of vector spaces and union of subspaces?,,"I'm having trouble understanding the difference between summing two subspaces and making ther union. My book says that the sum of two subspace is also a subspace, but I've found this example that shows that the union of a subspace is not always a subspace. So, what's the difference?","I'm having trouble understanding the difference between summing two subspaces and making ther union. My book says that the sum of two subspace is also a subspace, but I've found this example that shows that the union of a subspace is not always a subspace. So, what's the difference?",,"['linear-algebra', 'vector-spaces']"
83,Characterize stochastic matrices such that max singular value is less or equal one.,Characterize stochastic matrices such that max singular value is less or equal one.,,"By a stochastic matrix, I mean any non-negative square real matrix with rows summing to one. It is well-known that singular values of stochastic matrices can be more than one. Is there a characterization of the subset of stochastic matrices where the singular values are less or equal to one? A sufficient, but not necessary, condition would be symmetric. Is there a better characterization?","By a stochastic matrix, I mean any non-negative square real matrix with rows summing to one. It is well-known that singular values of stochastic matrices can be more than one. Is there a characterization of the subset of stochastic matrices where the singular values are less or equal to one? A sufficient, but not necessary, condition would be symmetric. Is there a better characterization?",,"['linear-algebra', 'matrices', 'markov-chains']"
84,"Prove that if $A$ has linearly dependents columns, then $A^tA$ is singular.","Prove that if  has linearly dependents columns, then  is singular.",A A^tA,"I just want to prove this statement without using determinants or rank properties; in fact, I want to see it using the fact that if $A$ has linearly dependent columns than $Ax = 0$ has infinitely many solutions.","I just want to prove this statement without using determinants or rank properties; in fact, I want to see it using the fact that if $A$ has linearly dependent columns than $Ax = 0$ has infinitely many solutions.",,"['linear-algebra', 'matrices']"
85,Can a $\mathbf{Q}$-basis of $\mathbf{R}$ be explicitly defined?,Can a -basis of  be explicitly defined?,\mathbf{Q} \mathbf{R},"Could someone give me an explicit basis of $\mathbf{R}$ as a vector space over $\mathbf{Q}$? I know some linearly independent subset, namely $1,e,e^2,\dots$ but this seems to be a deep result already (this is just the transcendence of $e$, right?), but no maximal linearly independent subset. Let me clarify: I know how to prove that every vector space (over a division ring) has a basis, it is by Zorn's lemma and equivalent to AC. I don't think that this directly implies that one cannot describe some basis explicitly; can one prove that the existence of a basis of $\mathbf{R}$ over $\mathbf{Q}$ is equivalent to AC ? This would convince me.","Could someone give me an explicit basis of $\mathbf{R}$ as a vector space over $\mathbf{Q}$? I know some linearly independent subset, namely $1,e,e^2,\dots$ but this seems to be a deep result already (this is just the transcendence of $e$, right?), but no maximal linearly independent subset. Let me clarify: I know how to prove that every vector space (over a division ring) has a basis, it is by Zorn's lemma and equivalent to AC. I don't think that this directly implies that one cannot describe some basis explicitly; can one prove that the existence of a basis of $\mathbf{R}$ over $\mathbf{Q}$ is equivalent to AC ? This would convince me.",,"['linear-algebra', 'elementary-set-theory', 'axiom-of-choice', 'rational-numbers', 'real-numbers']"
86,Why such a vectors are linearly independent? [closed],Why such a vectors are linearly independent? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Assume that $A$ is an linear operator on a real vector spacev $V$. I wish to prove that if for some $x,y \in V$ such that $x\neq 0$ or $y \neq 0$ and some  $a,b \in \mathbb R$ and $b\neq 0$, the following conditions hold  $$ Ax=ax-by, Ay=ay+bx $$ then $x,y$ are linearly independent.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Assume that $A$ is an linear operator on a real vector spacev $V$. I wish to prove that if for some $x,y \in V$ such that $x\neq 0$ or $y \neq 0$ and some  $a,b \in \mathbb R$ and $b\neq 0$, the following conditions hold  $$ Ax=ax-by, Ay=ay+bx $$ then $x,y$ are linearly independent.",,['linear-algebra']
87,Proof that $-v = (-1)*v$,Proof that,-v = (-1)*v,I need to prove that for every Vector Space this is valid: $$ -v = (-1)*v $$ -v = inverse element of addition -1 a real number $*$ the multiplication by real number of the Vector Space My teacher said that $-v$ is just a notation for the inverse element of addition. I'd like to prove that $-v = (-1)*v$. So I came up with the following solution and I'd like to know if it's correct: \begin{align} v + (-1)*v = u \\ (1)*v + (-1)*v = u \\ (1-1)*v = u \\ 0*v = u \\ o = u\\ \end{align} Since $v + -v = o$ $v + (-1)*v = o = v + -v$ adding -v to both sides v + (-1)*v -v = v + -v + -v o + (-1)*v = o + -v (-1)*v = -v Did I commit any mistakes? Did I make any assumptions that may not be valid for EVERY Vector Space? Edit: As S. Sheng said I have not proved that (0)*v = o. I'll try to prove that and come back later with a proof of that. I also haven't proved that (1)*v = v Oh my... I'm starting to think this is beyond my abilities..,I need to prove that for every Vector Space this is valid: $$ -v = (-1)*v $$ -v = inverse element of addition -1 a real number $*$ the multiplication by real number of the Vector Space My teacher said that $-v$ is just a notation for the inverse element of addition. I'd like to prove that $-v = (-1)*v$. So I came up with the following solution and I'd like to know if it's correct: \begin{align} v + (-1)*v = u \\ (1)*v + (-1)*v = u \\ (1-1)*v = u \\ 0*v = u \\ o = u\\ \end{align} Since $v + -v = o$ $v + (-1)*v = o = v + -v$ adding -v to both sides v + (-1)*v -v = v + -v + -v o + (-1)*v = o + -v (-1)*v = -v Did I commit any mistakes? Did I make any assumptions that may not be valid for EVERY Vector Space? Edit: As S. Sheng said I have not proved that (0)*v = o. I'll try to prove that and come back later with a proof of that. I also haven't proved that (1)*v = v Oh my... I'm starting to think this is beyond my abilities..,,['linear-algebra']
88,$A^2$ is diagonalizable leads to $A$ diagonalizable?,is diagonalizable leads to  diagonalizable?,A^2 A,"If $A^2$ is diagonalizable, is it necessary true that $A$ is diagonalizable? Also, the opposite: If $A$ is diagonalizable, is it necessary true that $A^2$ is diagonalizable? I'm not sure yet, tried to prove with $A^2 = P^{-1}DP$ , but no success.","If $A^2$ is diagonalizable, is it necessary true that $A$ is diagonalizable? Also, the opposite: If $A$ is diagonalizable, is it necessary true that $A^2$ is diagonalizable? I'm not sure yet, tried to prove with $A^2 = P^{-1}DP$ , but no success.",,"['linear-algebra', 'matrices', 'diagonalization']"
89,Linear algebra calculus trick.,Linear algebra calculus trick.,,"I have a matrix and a vector: $$ A=\begin{bmatrix} a &b\\ c&d \end{bmatrix}, $$ $$ \vec v=\begin{bmatrix} a+b\\ c+d \end{bmatrix} $$ Is there an algebraic operation that produce the following matrix: $$ B=\begin{bmatrix} \dfrac{a}{a+b} &\dfrac{b}{a+b}\\ \dfrac{c}{c+d} &\dfrac{d}{c+d} \end{bmatrix}? $$","I have a matrix and a vector: $$ A=\begin{bmatrix} a &b\\ c&d \end{bmatrix}, $$ $$ \vec v=\begin{bmatrix} a+b\\ c+d \end{bmatrix} $$ Is there an algebraic operation that produce the following matrix: $$ B=\begin{bmatrix} \dfrac{a}{a+b} &\dfrac{b}{a+b}\\ \dfrac{c}{c+d} &\dfrac{d}{c+d} \end{bmatrix}? $$",,"['linear-algebra', 'algebra-precalculus', 'matrices']"
90,Is it true that every eigenvalue has at least one eigenvector?,Is it true that every eigenvalue has at least one eigenvector?,,"As mentioned above: Is it true that every eigenvalue has at least one eigenvector? Or is it possible that while trying to find the basis of a specific eigenspace, i will get only the zero vector (means there are no eigenvectors corresponding to this eigenvalue)? Thank you","As mentioned above: Is it true that every eigenvalue has at least one eigenvector? Or is it possible that while trying to find the basis of a specific eigenspace, i will get only the zero vector (means there are no eigenvectors corresponding to this eigenvalue)? Thank you",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
91,Matrices and rank inequality,Matrices and rank inequality,,"Let $A \in K^{m\times n}$ and $B \in K^{n \times r}$ Prove that min$\{rk(A),rk(B)\}\geq rk(AB)\geq rk(A)+rk(B)-n$ My attempt at a solution: $(1)$ $AB=(AB_1|...|AB_j|...|AB_r)$ ($B_j$ is the j-th column of $B$), I don't know if the following statement is correct: the columns of $AB$ are a linear combination of the columns of $B$, then $rk(AB) \leq rk(B)$. $(2)$In a similar way, $AB= \begin{bmatrix} —A_1B— \\ \vdots \\ —A_jB— \\ \vdots \\—A_mB—   \end{bmatrix}$ ($A_j$ denotes the j-th row of $A$), so the rows of $AB$ are a linear combination of the rows of $A$, from here one deduces $rk(AB)\leq rk(A)$. From $(1)$ and $(2)$ it follows $rk(AB)\leq min\{rk(A),rk(B)\}$. This is what I've done so far. I am having doubts with, for example (1), this statement I've conjectured: the columns of $AB$ are a linear combination of the columns of $B$, then $rk(AB) \leq rk(B)$, but wouldn't this be the case iff $AB=(\alpha_1B_1|...|\alpha_jB_j|...|\alpha_rB_r)$ with $\alpha_1,...,\alpha_n \in K$ instead of $(AB_1|...|AB_j|...|AB_r)$ ? This is a major doubt I have, the same goes for (2). I need help to show the inequality $rk(AB)\geq rk(A)+rk(B)-n$","Let $A \in K^{m\times n}$ and $B \in K^{n \times r}$ Prove that min$\{rk(A),rk(B)\}\geq rk(AB)\geq rk(A)+rk(B)-n$ My attempt at a solution: $(1)$ $AB=(AB_1|...|AB_j|...|AB_r)$ ($B_j$ is the j-th column of $B$), I don't know if the following statement is correct: the columns of $AB$ are a linear combination of the columns of $B$, then $rk(AB) \leq rk(B)$. $(2)$In a similar way, $AB= \begin{bmatrix} —A_1B— \\ \vdots \\ —A_jB— \\ \vdots \\—A_mB—   \end{bmatrix}$ ($A_j$ denotes the j-th row of $A$), so the rows of $AB$ are a linear combination of the rows of $A$, from here one deduces $rk(AB)\leq rk(A)$. From $(1)$ and $(2)$ it follows $rk(AB)\leq min\{rk(A),rk(B)\}$. This is what I've done so far. I am having doubts with, for example (1), this statement I've conjectured: the columns of $AB$ are a linear combination of the columns of $B$, then $rk(AB) \leq rk(B)$, but wouldn't this be the case iff $AB=(\alpha_1B_1|...|\alpha_jB_j|...|\alpha_rB_r)$ with $\alpha_1,...,\alpha_n \in K$ instead of $(AB_1|...|AB_j|...|AB_r)$ ? This is a major doubt I have, the same goes for (2). I need help to show the inequality $rk(AB)\geq rk(A)+rk(B)-n$",,"['linear-algebra', 'matrices', 'matrix-rank']"
92,Rank of adjacency matrix vs rank of graph Laplacian,Rank of adjacency matrix vs rank of graph Laplacian,,What is the relation between rank of the adjacency matrix of a graph and rank of the corresponding graph Laplacian matrix?,What is the relation between rank of the adjacency matrix of a graph and rank of the corresponding graph Laplacian matrix?,,"['linear-algebra', 'graph-theory', 'spectral-graph-theory', 'graph-laplacian']"
93,Projection matrix that sum to identity are orthogonal [duplicate],Projection matrix that sum to identity are orthogonal [duplicate],,"This question already has answers here : Orthogonal projections with $\sum P_i =I$, proving that $i\ne j \Rightarrow  P_{j}P_{i}=0$ (4 answers) Closed 4 years ago . Can anyone help me to show if $Z_1$, $Z_2$, and $Z_3$ are projection matrices (i.e. idempotent and symmetric) and if $Z_1 + Z_2 + Z_3 = I_n$, then we can conclude: $$\forall  i \ne j \text{ ,} Z_i Z_j = 0$$ It seems to be an easy problem but I was not able to solve it. Thanks,","This question already has answers here : Orthogonal projections with $\sum P_i =I$, proving that $i\ne j \Rightarrow  P_{j}P_{i}=0$ (4 answers) Closed 4 years ago . Can anyone help me to show if $Z_1$, $Z_2$, and $Z_3$ are projection matrices (i.e. idempotent and symmetric) and if $Z_1 + Z_2 + Z_3 = I_n$, then we can conclude: $$\forall  i \ne j \text{ ,} Z_i Z_j = 0$$ It seems to be an easy problem but I was not able to solve it. Thanks,",,"['linear-algebra', 'matrices']"
94,What is an intuitive definition of the zero vector?,What is an intuitive definition of the zero vector?,,"I'm learning now about vector spaces and subspaces, and one of three rules that determine if something is a subspace of a larger vector space is that it must contain the zero vector... but intuitively, I can't figure out why the zero vector wouldn't exist. When I first learned about vectors over the summer, the zero vector was basically described as ""pick a point on the x-y plane. Move 0 units up and 0 units to the right. That is the zero vector... just a dot on the x-y plane"". This is assuming of course that you're talking about a vector $\vec{v}\in\mathbb{R}^2$. Now fast forward to today when I'm given the following definition and need to determine if it's a subspace or not based on the three rules: Does it contain the zero vector? Is the set closed under vector addition? Is the set closed under scalar multiplication? Take for example the question: Let $a,b,c,d$ be constants, and let $U=\left\{\left[\begin{array}{r}x\\y\\z\end{array}\right]\in\mathbb{R}^{3}\;\middle|\; ax+by+cz=d\right\}$. Show that $U$ is a subspace of $V$ if and only if $d=0$. By the first test above, $\left[\begin{array}{r}0\\0\\0\end{array}\right]$ is not in $U$ if $d\neq 0$ because $a(0)+b(0)+c(0)$ necessarily implies that $d=0$... Consider also the line $x+y=1$ in $\mathbb{R}^2$. This also does not contain the zero vector. It seems to me like ""zero vector"" is being used synonymously with ""origin"", but this doesn't fit the definition of a vector that I was given. Sure, an arrow can begin at the origin and extend outward, but not necessarily. Any arrow representing a vector is the same as any other as long as it has the same length and direction, no matter where the base of the arrow sits, the zero vector should still be the zero vector. So I ask... How can the zero vector not be in any plane, if it is indeed properly understood as ""pick a point, move 0 units on the x-axis, 0 units on the y-axis, and 0 units on the z-axis""?","I'm learning now about vector spaces and subspaces, and one of three rules that determine if something is a subspace of a larger vector space is that it must contain the zero vector... but intuitively, I can't figure out why the zero vector wouldn't exist. When I first learned about vectors over the summer, the zero vector was basically described as ""pick a point on the x-y plane. Move 0 units up and 0 units to the right. That is the zero vector... just a dot on the x-y plane"". This is assuming of course that you're talking about a vector $\vec{v}\in\mathbb{R}^2$. Now fast forward to today when I'm given the following definition and need to determine if it's a subspace or not based on the three rules: Does it contain the zero vector? Is the set closed under vector addition? Is the set closed under scalar multiplication? Take for example the question: Let $a,b,c,d$ be constants, and let $U=\left\{\left[\begin{array}{r}x\\y\\z\end{array}\right]\in\mathbb{R}^{3}\;\middle|\; ax+by+cz=d\right\}$. Show that $U$ is a subspace of $V$ if and only if $d=0$. By the first test above, $\left[\begin{array}{r}0\\0\\0\end{array}\right]$ is not in $U$ if $d\neq 0$ because $a(0)+b(0)+c(0)$ necessarily implies that $d=0$... Consider also the line $x+y=1$ in $\mathbb{R}^2$. This also does not contain the zero vector. It seems to me like ""zero vector"" is being used synonymously with ""origin"", but this doesn't fit the definition of a vector that I was given. Sure, an arrow can begin at the origin and extend outward, but not necessarily. Any arrow representing a vector is the same as any other as long as it has the same length and direction, no matter where the base of the arrow sits, the zero vector should still be the zero vector. So I ask... How can the zero vector not be in any plane, if it is indeed properly understood as ""pick a point, move 0 units on the x-axis, 0 units on the y-axis, and 0 units on the z-axis""?",,['linear-algebra']
95,Are parallel vectors always scalar multiple of each others?,Are parallel vectors always scalar multiple of each others?,,"I read this in a tutorial of a university course : We note that the vectors V, cV are parallel, and conversely, if two   vectors are parallel (that is, they have the same direction), then one   is a scalar multiple of the other. Q1. There is an implication in the statement that two vectors are parallel if they are in same direction. Isn't it half right ? I mean in 3D space, two lines could not be in same direction and still be parallel right ? Q2. If the above statement doesn't hold, then saying one is a scalar multiple of the other. is also wrong ?","I read this in a tutorial of a university course : We note that the vectors V, cV are parallel, and conversely, if two   vectors are parallel (that is, they have the same direction), then one   is a scalar multiple of the other. Q1. There is an implication in the statement that two vectors are parallel if they are in same direction. Isn't it half right ? I mean in 3D space, two lines could not be in same direction and still be parallel right ? Q2. If the above statement doesn't hold, then saying one is a scalar multiple of the other. is also wrong ?",,"['linear-algebra', 'vector-spaces']"
96,Equation of a line in homogenous coordinates given 2 points in affine coordinates,Equation of a line in homogenous coordinates given 2 points in affine coordinates,,"So if I have 2 points $A$ and $B$ such that $F(A) = (1; a, a^3)$, and $F(B) = (1; b, b^3)$. how do I find the equation of this line in homogeneous coordinates? So I know how to get a line the ""normal"" way. If I take the points $a$ and $b$ and represent them as regular Cartesian coordinates $A = (a, a^3), B = (b, b^3)$ and then say a line equation is $y = mx + b$. I could find the slope to be $(b^3-a^3)/(b-a)$ and plug that in for $m$ and then find '$b$' by plugging in the coordinates for $A$ and $B$. But then I wouldn't know how to take that $y = mx+b$ and turn it into homogeneous coordinates. I don't think this is necessarily how I'm supposed to go about it.","So if I have 2 points $A$ and $B$ such that $F(A) = (1; a, a^3)$, and $F(B) = (1; b, b^3)$. how do I find the equation of this line in homogeneous coordinates? So I know how to get a line the ""normal"" way. If I take the points $a$ and $b$ and represent them as regular Cartesian coordinates $A = (a, a^3), B = (b, b^3)$ and then say a line equation is $y = mx + b$. I could find the slope to be $(b^3-a^3)/(b-a)$ and plug that in for $m$ and then find '$b$' by plugging in the coordinates for $A$ and $B$. But then I wouldn't know how to take that $y = mx+b$ and turn it into homogeneous coordinates. I don't think this is necessarily how I'm supposed to go about it.",,"['linear-algebra', 'algebraic-geometry', 'coordinate-systems', 'affine-geometry']"
97,How to find k given determinant?,How to find k given determinant?,,"So I've got this matrix here, and need to solve for $k$ $$\text{det}\;\begin{pmatrix} 3 & 2 & -1 & 4 \\ 2 & k & 6  & 5 \\ -3& 2 & 1  & 0 \\ 6 & 4 & 2  & 3 \\ \end{pmatrix}=33$$ Doing some row operations $(R3+R1) \to R3\text{ and}\; (R4-2R1)\to R4$), I end up with $$\text{det}\;\begin{pmatrix} 3 & 2 & -1& 4 \\ 2 & k & 6 & 5 \\ 0 & 4 & 0 & 4 \\ 0 & 0 & 4 & -5\\ \end{pmatrix}=33$$ I expand along the first column and somehow my $k$ value is a decimal. Am I doing this correctly? I've tried making this into an upper and lower diagonal matrix and it just gets messy.","So I've got this matrix here, and need to solve for $k$ $$\text{det}\;\begin{pmatrix} 3 & 2 & -1 & 4 \\ 2 & k & 6  & 5 \\ -3& 2 & 1  & 0 \\ 6 & 4 & 2  & 3 \\ \end{pmatrix}=33$$ Doing some row operations $(R3+R1) \to R3\text{ and}\; (R4-2R1)\to R4$), I end up with $$\text{det}\;\begin{pmatrix} 3 & 2 & -1& 4 \\ 2 & k & 6 & 5 \\ 0 & 4 & 0 & 4 \\ 0 & 0 & 4 & -5\\ \end{pmatrix}=33$$ I expand along the first column and somehow my $k$ value is a decimal. Am I doing this correctly? I've tried making this into an upper and lower diagonal matrix and it just gets messy.",,"['linear-algebra', 'matrices', 'determinant']"
98,solve system of linear congruences mod 13,solve system of linear congruences mod 13,,"Given the system: $$ 2x+5y \equiv 1  \textrm{mod} 13 $$ $$ 5x+y \equiv 2  \textrm{mod} 13 $$ What is the value of  $$ 5x+7y \equiv   \textrm{mod} 13 $$? do I have to solve the first two equations, i.e., $x=11+5t, y=1-2t$ for the first equation, do the same for the second, obtain the common values of $x, y $, and do the calculation? or is there other method?","Given the system: $$ 2x+5y \equiv 1  \textrm{mod} 13 $$ $$ 5x+y \equiv 2  \textrm{mod} 13 $$ What is the value of  $$ 5x+7y \equiv   \textrm{mod} 13 $$? do I have to solve the first two equations, i.e., $x=11+5t, y=1-2t$ for the first equation, do the same for the second, obtain the common values of $x, y $, and do the calculation? or is there other method?",,"['linear-algebra', 'abstract-algebra']"
99,Positive Definite Matrix Determinant,Positive Definite Matrix Determinant,,"Prove that a positive definite matrix has positive determinant and   positive trace. In order to be a positive determinant the matrix must be regular and have pivots that are positive which is the definition. Its obvious that the determinant must be positive since that is what a positive definite is, so how can I prove that?","Prove that a positive definite matrix has positive determinant and   positive trace. In order to be a positive determinant the matrix must be regular and have pivots that are positive which is the definition. Its obvious that the determinant must be positive since that is what a positive definite is, so how can I prove that?",,"['linear-algebra', 'matrices']"
