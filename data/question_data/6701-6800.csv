,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,About an equivalent condition for convergence of a scalar series,About an equivalent condition for convergence of a scalar series,,So I came across this problem a while back when one of my friends suggested it. The question asks to show that the scalar series $\sum_{n=1}^\infty a_n$ converges if and only if $$\sum_{n=1}^\infty \left(1-\frac1{2^2}+\cdots+\frac{(-1)^n}{n^2}\right)a_n$$ converges. The scalar field under consideration is real or complex. We were able to show the result holds if $a_n>0$ . But the general case is eluding us. Any ideas on how to approach this?,So I came across this problem a while back when one of my friends suggested it. The question asks to show that the scalar series converges if and only if converges. The scalar field under consideration is real or complex. We were able to show the result holds if . But the general case is eluding us. Any ideas on how to approach this?,\sum_{n=1}^\infty a_n \sum_{n=1}^\infty \left(1-\frac1{2^2}+\cdots+\frac{(-1)^n}{n^2}\right)a_n a_n>0,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'convergence-divergence']"
1,Prove that this function defined over $ \mathbb{R^2} $ is continuous.,Prove that this function defined over  is continuous., \mathbb{R^2} ,"Recently, I've encountered with the following problem in an elementary analysis course: Let $ f: \mathbb{R^2} \rightarrow \mathbb{R^2} $ be a function that maps every connected set to connected set and also, every compact set to compact set. Prove that $ f $ is continuous. My attempt was to consider a convergent sequence $ (x_n, y_n) $ with its limit point, say $ (x, y) $ , which together form a compact set, and I wanted to show that $ \lim_{n \rightarrow \infty} f((x_n, y_n)) = f((x, y)) $ . But I couldn't continue. I wonder if there is anything special with $ \mathbb{R^2} $ here? Any help would be appreciated.","Recently, I've encountered with the following problem in an elementary analysis course: Let be a function that maps every connected set to connected set and also, every compact set to compact set. Prove that is continuous. My attempt was to consider a convergent sequence with its limit point, say , which together form a compact set, and I wanted to show that . But I couldn't continue. I wonder if there is anything special with here? Any help would be appreciated."," f: \mathbb{R^2} \rightarrow \mathbb{R^2}   f   (x_n, y_n)   (x, y)   \lim_{n \rightarrow \infty} f((x_n, y_n)) = f((x, y))   \mathbb{R^2} ","['real-analysis', 'continuity', 'compactness', 'connectedness']"
2,Putnam 2006 - Exercise B.6 - Alternative solution verification and questions about generalizations,Putnam 2006 - Exercise B.6 - Alternative solution verification and questions about generalizations,,"CONTEXT Here is the orginal problem statement. Let $k$ be an integer greater than 1. Suppose $a_0 > 0$ , and define $$ a_{n+1} = a_n + \frac{1}{\sqrt[k]{a_n}} $$ for $n > 0$ . Evaluate $$ \lim_{n \to \infty} \frac{a_n^{k+1}}{n^k}. $$ ** PROPOSED SOLUTION Note that for $n>0$ $$a_n \geq a_0 + \frac1{\sqrt[k]{a_0}}\sum_{j=1}^n \frac1{\sqrt[k]{j}},$$ and hence $(a_n)$ diverges. EDIT. As pointed out in comment, this step is not correct . I am working on an alternative approach. EDIT 2. A nice approach is proposed in the comment by Thomas Andrews Define $b_n = a_n^{\frac{k+1}k}$ . (Observe that $(b_n)$ , too, is divergent.) Now, we have \begin{eqnarray}\lim_{n\to \infty}(b_{n+1}-b_n)&= &\lim_{n\to \infty}\left(a_n+a_n^{-\frac1{k}}\right)^{\frac{k+1}k}-a_n^{\frac{k+1}k}=\\&=&\lim_{n\to\infty} \frac{\left(1+a_n^{-\frac{k+1}k}\right)^{\frac{k+1}k}-1}{a_n^{-\frac{k+1}k}}=\frac{k+1}{k},\end{eqnarray} where the fundamental limit $$\frac{(1+\alpha)^m-1}{\alpha}\to m, \ \ \ \mbox{for} \ \ \alpha\to 0$$ has been used. By Stolz-Cesàro Theorem, we have $$\lim_{n\to\infty}\frac{b_n}{n} = \frac{k+1}k,$$ and hence $$\lim_{n\to\infty} \frac{a_n^{k+1}}{n^k} = \left(\frac{k+1}k\right)^k.$$ $\blacksquare$ QUESTIONS Is my solution correct? Is it correct to state that the solution is valid also for any real number $k\geq 1$ ? Can we further generalize? For example: what can be stated for $0<k<1$ , maybe with some restriction on $a_0$ ?","CONTEXT Here is the orginal problem statement. Let be an integer greater than 1. Suppose , and define for . Evaluate ** PROPOSED SOLUTION Note that for and hence diverges. EDIT. As pointed out in comment, this step is not correct . I am working on an alternative approach. EDIT 2. A nice approach is proposed in the comment by Thomas Andrews Define . (Observe that , too, is divergent.) Now, we have where the fundamental limit has been used. By Stolz-Cesàro Theorem, we have and hence QUESTIONS Is my solution correct? Is it correct to state that the solution is valid also for any real number ? Can we further generalize? For example: what can be stated for , maybe with some restriction on ?","k a_0 > 0 
a_{n+1} = a_n + \frac{1}{\sqrt[k]{a_n}}
 n > 0 
\lim_{n \to \infty} \frac{a_n^{k+1}}{n^k}.
 n>0 a_n \geq a_0 + \frac1{\sqrt[k]{a_0}}\sum_{j=1}^n \frac1{\sqrt[k]{j}}, (a_n) b_n = a_n^{\frac{k+1}k} (b_n) \begin{eqnarray}\lim_{n\to \infty}(b_{n+1}-b_n)&= &\lim_{n\to \infty}\left(a_n+a_n^{-\frac1{k}}\right)^{\frac{k+1}k}-a_n^{\frac{k+1}k}=\\&=&\lim_{n\to\infty} \frac{\left(1+a_n^{-\frac{k+1}k}\right)^{\frac{k+1}k}-1}{a_n^{-\frac{k+1}k}}=\frac{k+1}{k},\end{eqnarray} \frac{(1+\alpha)^m-1}{\alpha}\to m, \ \ \ \mbox{for} \ \ \alpha\to 0 \lim_{n\to\infty}\frac{b_n}{n} = \frac{k+1}k, \lim_{n\to\infty} \frac{a_n^{k+1}}{n^k} = \left(\frac{k+1}k\right)^k. \blacksquare k\geq 1 0<k<1 a_0","['real-analysis', 'sequences-and-series', 'solution-verification', 'contest-math']"
3,On cardinality of a set of continuous functions,On cardinality of a set of continuous functions,,"What is the cardinality of the set of real valued continuous functions $f$ on $[0,1]$ such that $f(x)$ is rational whenever $x$ is rational? I know that it's at least infinitely countably many because for any rational $q$ , the contant function $f(x)=q$ on $[0,1]$ is in the set in question. Also, other functions like $f(x)=x^n$ , for any integer $n$ , are in this set. But, what throws me at this point is my inability to infer whether there are other functions that make the set uncountable. If the case there aren't, how do I prove that it's only countably many?","What is the cardinality of the set of real valued continuous functions on such that is rational whenever is rational? I know that it's at least infinitely countably many because for any rational , the contant function on is in the set in question. Also, other functions like , for any integer , are in this set. But, what throws me at this point is my inability to infer whether there are other functions that make the set uncountable. If the case there aren't, how do I prove that it's only countably many?","f [0,1] f(x) x q f(x)=q [0,1] f(x)=x^n n","['real-analysis', 'continuity']"
4,Of the reasoning behind the Mean Value Theorem,Of the reasoning behind the Mean Value Theorem,,"I have been attempting to grasp the reasoning behind the formulation of the Mean Value Theorem: If $f(x)$ is continuous in the closed interval $[x_1;x_2]$ and differentiable at every point of the open interval $(x_1;x_2)$ , then there is at least one point $\xi \in (x_1;x_2)$ such that $$ \frac{f(x_2)-f(x_1)}{x_2-x_1} = f’(\xi)$$ What I cannot understand is why we do say that $f(x)$ is not necessarily differentiable at the end-points $x_1$ and $x_2$ of the interval. I also cannot fully understand the overall value of the premise that the function is continuous at those points; for I understand that if a function possesses an infinite discontinuity or a discontinuity of that sort when the value of the function does not coincide with its limiting value at that point, but I simply cannot imagine a singular example for the MVT not holding as the function possesses no definite limit at the end-points. Thank you! Edit: I have noticed that my question is a little convoluted, so I have re-phrased it a slight bit: Consider a function which is continuous everywhere in the interval $[x_1;x_2]$ except for at least one of the end-points (say, $x_1$ ), as the function, though being defined at $x_1$ , possesses no definite limit as it approaches $x_1$ (for instance, the function $\sin\frac{1}{x}$ as $x \to 0$ ). Does there exist a function of the mentioned sort, for which MVT does not hold?","I have been attempting to grasp the reasoning behind the formulation of the Mean Value Theorem: If is continuous in the closed interval and differentiable at every point of the open interval , then there is at least one point such that What I cannot understand is why we do say that is not necessarily differentiable at the end-points and of the interval. I also cannot fully understand the overall value of the premise that the function is continuous at those points; for I understand that if a function possesses an infinite discontinuity or a discontinuity of that sort when the value of the function does not coincide with its limiting value at that point, but I simply cannot imagine a singular example for the MVT not holding as the function possesses no definite limit at the end-points. Thank you! Edit: I have noticed that my question is a little convoluted, so I have re-phrased it a slight bit: Consider a function which is continuous everywhere in the interval except for at least one of the end-points (say, ), as the function, though being defined at , possesses no definite limit as it approaches (for instance, the function as ). Does there exist a function of the mentioned sort, for which MVT does not hold?",f(x) [x_1;x_2] (x_1;x_2) \xi \in (x_1;x_2)  \frac{f(x_2)-f(x_1)}{x_2-x_1} = f’(\xi) f(x) x_1 x_2 [x_1;x_2] x_1 x_1 x_1 \sin\frac{1}{x} x \to 0,"['real-analysis', 'calculus', 'limits', 'analysis', 'derivatives']"
5,Show that there exists a strictly increasing function $g$ such that $\sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty$,Show that there exists a strictly increasing function  such that,g \sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty,"Let $f\colon \mathbb{R}^n\to [0,\infty)$ be continuous and radially unbounded, that is, for all $c>0$ , there exists $r>0$ such that if $\|x\|>r$ , then $f(x)>c.$ Assume that for all $x\in\mathbb{R}^n\backslash\{0\}$ , $f(x)>0$ . Prove or disprove that there exists a strictly increasing function $g\colon [0,\infty) \to [0,\infty)$ such that \begin{equation} \sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty. \end{equation} Here is my attempt: As $\|x\|\to\infty$ , if $f(x)$ goes to $\infty$ faster than $\|x\|$ then we can prove the claim easily. So I think the difficulty is the case where $f(x)$ goes to $\infty$ slower than $\|x\|$ . For example if $f(x)=\ln(1+\|x\|)$ , then $g(y)=e^y$ satisfy the claim. So I think the claim is true because it seems that we can always increase the growth rate of the denominator using an increasing $g$ but not sure how to prove the claim using these ideas. Any help is appreciated.","Let be continuous and radially unbounded, that is, for all , there exists such that if , then Assume that for all , . Prove or disprove that there exists a strictly increasing function such that Here is my attempt: As , if goes to faster than then we can prove the claim easily. So I think the difficulty is the case where goes to slower than . For example if , then satisfy the claim. So I think the claim is true because it seems that we can always increase the growth rate of the denominator using an increasing but not sure how to prove the claim using these ideas. Any help is appreciated.","f\colon \mathbb{R}^n\to [0,\infty) c>0 r>0 \|x\|>r f(x)>c. x\in\mathbb{R}^n\backslash\{0\} f(x)>0 g\colon [0,\infty) \to [0,\infty) \begin{equation}
\sup_{\|x\|>1}\frac{\|x\|}{g(f(x))}<\infty.
\end{equation} \|x\|\to\infty f(x) \infty \|x\| f(x) \infty \|x\| f(x)=\ln(1+\|x\|) g(y)=e^y g","['real-analysis', 'limits', 'multivariable-calculus', 'functions', 'supremum-and-infimum']"
6,"If $f \circ f =f$, then what is $f$?","If , then what is ?",f \circ f =f f,"This is an exercise from Mathematical Analysis II by Zorich:( the surface here is the same as manifold, not necessarily 2-dimensional.) Let $f:\mathbb R^n  \to \mathbb R^n$ be a smooth mapping satisfying the condition $f \circ f =f$ . a) Show that the set $f(\mathbb R^n)$ is a smooth surface in $\mathbb R^n$ . b) By what property of the mapping $f$ is the dimension of this surface determined? My solution: $$f'(f(x))f'(x)=f'(x)\Rightarrow \forall x\in f(\mathbb R^n),f(x)=x,(f'(x)-I)f'(x)=O.\tag1$$ We can conclude that eigenvalue of $f'(x)$ is $0$ or $1$ . Let $P^{-1}(x)f'(x)P(x)=J(0)+\begin{pmatrix}I_r&O\\O&O\end{pmatrix}$ where $|P(x)|\neq0$ , and according to (1) we can know that $J(0)=O$ . Since $tr(f'(x))=rank(f'(x))$ is continous on $f(\mathbb R^n)$ , and $f(\mathbb R^n)$ is connected, we can know that $\forall x\in f(\mathbb{R}^n), rank(f(x))\equiv r$ . $\forall x \in \mathbb{R}^n, (P^{-1}(x)\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P(x)-I)f'(x)=O\Rightarrow rank(f'(x))\leq r$ $\forall f(x_0)\in f(\mathbb{R}^n),\ rank (f'(f(x_0)))=r, \forall x\in O(f(x_0)),rank(f'(x))\leq r\Rightarrow \exists \bar O(f(x_0)),\forall x\in \bar O(f(x_0)), rank(f'(x))=r$ . According to The rank theroem, we can find $\tilde O(x_0), \tilde O(f(x_0))$ and diffeomorphisms $\phi ,\varphi,f=\phi\circ\pi\circ\varphi^{-1}$ where $ \pi:(u^1,\cdots,u^r,\cdots,u^m)\mapsto(u^1,\cdots,u^r)$ , so that the set $f(\mathbb R^n)$ is a smooth surface in $\mathbb R^n$ and the dimension depends on the rank of $f$ on $f(\mathbb R^n)$ . My doubts: (1) Is my solution above correct? Feel free to point out any mistakes or offer any concise solutions. (2)Does $f$ have to be a linear mapping with the form $f(x)=P^{-1}\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P$$\cdot$$x$ ? (3)If we only assume $f\in C^0(R^m,R^m)$ , what properties of $f$ can we get? Is $f(\mathbb R^n)$ still a surface?","This is an exercise from Mathematical Analysis II by Zorich:( the surface here is the same as manifold, not necessarily 2-dimensional.) Let be a smooth mapping satisfying the condition . a) Show that the set is a smooth surface in . b) By what property of the mapping is the dimension of this surface determined? My solution: We can conclude that eigenvalue of is or . Let where , and according to (1) we can know that . Since is continous on , and is connected, we can know that . . According to The rank theroem, we can find and diffeomorphisms where , so that the set is a smooth surface in and the dimension depends on the rank of on . My doubts: (1) Is my solution above correct? Feel free to point out any mistakes or offer any concise solutions. (2)Does have to be a linear mapping with the form ? (3)If we only assume , what properties of can we get? Is still a surface?","f:\mathbb R^n
 \to \mathbb R^n f \circ f =f f(\mathbb R^n) \mathbb R^n f f'(f(x))f'(x)=f'(x)\Rightarrow \forall x\in f(\mathbb R^n),f(x)=x,(f'(x)-I)f'(x)=O.\tag1 f'(x) 0 1 P^{-1}(x)f'(x)P(x)=J(0)+\begin{pmatrix}I_r&O\\O&O\end{pmatrix} |P(x)|\neq0 J(0)=O tr(f'(x))=rank(f'(x)) f(\mathbb R^n) f(\mathbb R^n) \forall x\in f(\mathbb{R}^n), rank(f(x))\equiv r \forall x \in \mathbb{R}^n, (P^{-1}(x)\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P(x)-I)f'(x)=O\Rightarrow rank(f'(x))\leq r \forall f(x_0)\in f(\mathbb{R}^n),\ rank (f'(f(x_0)))=r, \forall x\in O(f(x_0)),rank(f'(x))\leq r\Rightarrow \exists \bar O(f(x_0)),\forall x\in \bar O(f(x_0)), rank(f'(x))=r \tilde O(x_0), \tilde O(f(x_0)) \phi ,\varphi,f=\phi\circ\pi\circ\varphi^{-1}  \pi:(u^1,\cdots,u^r,\cdots,u^m)\mapsto(u^1,\cdots,u^r) f(\mathbb R^n) \mathbb R^n f f(\mathbb R^n) f f(x)=P^{-1}\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P\cdotx f\in C^0(R^m,R^m) f f(\mathbb R^n)","['real-analysis', 'multivariable-calculus', 'smooth-manifolds', 'functional-equations']"
7,Book recommendation on Dedekind's Cuts,Book recommendation on Dedekind's Cuts,,"Is there any book (except Baby Rudin) where 'Dedekind's cuts' and it's consequences are explained in detailed manner with pictures (in intuitive way)? EDIT Seems that Fikhtengol'ts's ""The Fundamental of Mathematical Analysis"" (Vol-1) has a little part dedicated to the Dedekind's cuts (which is in a more brain freidnly way).","Is there any book (except Baby Rudin) where 'Dedekind's cuts' and it's consequences are explained in detailed manner with pictures (in intuitive way)? EDIT Seems that Fikhtengol'ts's ""The Fundamental of Mathematical Analysis"" (Vol-1) has a little part dedicated to the Dedekind's cuts (which is in a more brain freidnly way).",,"['real-analysis', 'calculus', 'real-numbers']"
8,One thinking problem of uncountable set in real analysis,One thinking problem of uncountable set in real analysis,,"Assume: $E \subseteq R^2$ and $E$ is a uncountable set, try to prove that there exists $x_0 \in E$ s.t. $\forall B(x_0)$ ( $B(x_0)$ is an open circular neighborhood, $x_0 \in B(x_0)$ and the center is unnecessarily $x_0$ ), $B(x_0) \cap E$ is a uncountable set. The answer proves it reversely: assume that: $ \forall x \in E$ , there exists one circular neighborhood $B(x)$ ( $x \in B(x)$ ) s.t. $ E \cap B(x)$ is a countable set. Then $\forall x\in E$ , choose a $r_x  \in \mathbb Q$ , s.t. $x \in B(x,r_x)$ . Therefore, $E=E \cap \bigcup _{x}{B(x,r_x)}=\bigcup_{r_x}{ E \cap B(x,r_x)}$ ,thus $E$ is a countable set, which leads to contradiction. Q:For two different $x$ , there may be one same $r_x$ , which means it is not a single-shot mapping from the set of $B(x,r_x)$ to $ \mathbb Q$ . So it can’t prove that the set of all $B(x,r_x)$ is countable? The answer is wrong?","Assume: and is a uncountable set, try to prove that there exists s.t. ( is an open circular neighborhood, and the center is unnecessarily ), is a uncountable set. The answer proves it reversely: assume that: , there exists one circular neighborhood ( ) s.t. is a countable set. Then , choose a , s.t. . Therefore, ,thus is a countable set, which leads to contradiction. Q:For two different , there may be one same , which means it is not a single-shot mapping from the set of to . So it can’t prove that the set of all is countable? The answer is wrong?","E \subseteq R^2 E x_0 \in E \forall B(x_0) B(x_0) x_0 \in B(x_0) x_0 B(x_0) \cap E  \forall x \in E B(x) x \in B(x)  E \cap B(x) \forall x\in E r_x  \in \mathbb Q x \in B(x,r_x) E=E \cap \bigcup _{x}{B(x,r_x)}=\bigcup_{r_x}{ E \cap B(x,r_x)} E x r_x B(x,r_x)  \mathbb Q B(x,r_x)","['real-analysis', 'analysis']"
9,Vector norm of integral inequality,Vector norm of integral inequality,,"On the Wikipedia page for the mean value theorem there is this Lemma (Lemma 2 under Mean value theorem for vector-valued functions ) that says: Let $v:[a,b]\rightarrow\mathbb{R}^m$ be a continuous function on $[a,b]$ , then we have that $$ \left\Vert\int_a^b v(t)\,dt\right\Vert\leq\int_a^b\Vert v(t)\Vert dt. $$ I found a paper that uses/states the following similar result $$ \left\Vert\int_0^1 v(t)\,dt\right\Vert^2\leq\int_0^1\Vert v(t)\Vert^2 dt. $$ I can follow the proof from Wikipedia, but I'm not sure how to proof the 'squared' version above. I guess the fact the integral is taken from 0 to 1 is important. I did find this post which asks a similar question, but it seems rely on it being a scalar function there.","On the Wikipedia page for the mean value theorem there is this Lemma (Lemma 2 under Mean value theorem for vector-valued functions ) that says: Let be a continuous function on , then we have that I found a paper that uses/states the following similar result I can follow the proof from Wikipedia, but I'm not sure how to proof the 'squared' version above. I guess the fact the integral is taken from 0 to 1 is important. I did find this post which asks a similar question, but it seems rely on it being a scalar function there.","v:[a,b]\rightarrow\mathbb{R}^m [a,b]  \left\Vert\int_a^b v(t)\,dt\right\Vert\leq\int_a^b\Vert v(t)\Vert dt.   \left\Vert\int_0^1 v(t)\,dt\right\Vert^2\leq\int_0^1\Vert v(t)\Vert^2 dt. ","['real-analysis', 'integration', 'inequality', 'vector-analysis', 'integral-inequality']"
10,"Is there a closed form for $\int_0^1\,_3F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx$?",Is there a closed form for ?,"\int_0^1\,_3F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx","Is there a closed form evaluation for the integral $$J=\int_0^1 {_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx?$$ Context: I have been investigating integrals of the form $$e_{p,q}^{n,m}\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}}\right)=\int_0^1x^n\left[{_p}F_q\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}};x\right)\right]^mdx.$$ Obviously there is no reason to expect a general closed form, but I have found the following: $$E_1=e_{2,1}^{1,2}(\tfrac12,1;2)=12-16\ln2,\tag1$$ and $$E_2=e_{2,1}^{1,3}(\tfrac13,\tfrac23;\tfrac32)=\frac{27}{32}.\tag2$$ I found these through applying the Lagrange inversion theorem to the functions $x^2-x$ and $x^3-x$ , respectively. The proofs are below. Theorem. We have the explicit evaluation $$\int_0^1x\left[{_2}F_1(\tfrac12,1;2;x)\right]^2dx=12-16\ln2.\tag{1'}$$ Proof. Using the Lagrange inversion theorem, the function $g(x)$ , satisfying $$g(x)^2-g(x)=x,$$ is given by the hypergeometric series $g(x)=-x\,{_2}F_1(\tfrac12,1;2;-4x)$ , for $x\in[-1/4,\infty)$ . Thus, the function $F(x)={_2}F_1(\tfrac12,1;2;x)$ satisfies $$xF(x)^2=4(F(x)-1),$$ for $x\in(-\infty,1]$ . Thus $$E_1=\int_0^1xF(x)^2dx=-4+4\int_0^1F(x)dx.$$ Then using $$_2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1\frac{t^{b-1}(1-t)^{c-b-1}}{(1-zt)^a}dt,$$ we have $$F(x)=\frac2\pi\int_0^1\sqrt{\frac{1-t}{t}}\frac{dt}{1-xt}=\frac4\pi\int_0^\infty\frac{t^2dt}{(t^2+1)(t^2+1-x)}=\frac{2}{1+\sqrt{1-x}}.$$ It is then not too difficult to show that $$\int_0^1F(x)dx=\int_0^1\frac{2dx}{1+\sqrt{1-x}}=4-4\ln2,$$ which gives $(1')$ and thus $(1)$ . $\square$ Theorem. We have the explicit evaluation $$\int_0^1 x\left[{_2}F_1(\tfrac13,\tfrac23;\tfrac32;x)\right]^3dx=\frac{27}{32}.\tag{2'}$$ Proof. The Lagrange inversion theorem gives $g(x)^3-g(x)=x$ , for $$g(x)=-x{_2}F_1(\tfrac13,\tfrac23;\tfrac32;\tfrac{27}{4}x^2),\qquad |x|<\frac{2}{3\sqrt3}.$$ Setting $F(x)={_2}F_1(\tfrac13,\tfrac23;\tfrac32;x)$ , we have $$4xF(x)^3=27(F(x)-1),$$ and thus $$E_2=\int_0^1xF(x)^3dx=\frac{27}{4}\left(-1+\int_0^1F(x)dx\right).$$ Then from here and here , we have $$\int_0^1F(x)dx=\left(\frac32\right)^2\left(\frac{\Gamma(\tfrac12)\Gamma(\tfrac32)}{\Gamma(\tfrac56)\Gamma(\tfrac76)}-1\right)=\frac{9}{8},$$ which is equivalent to $(2)$ and $(2')$ . $\square$ Here is my work on the current problem. As you may have guessed, we use the Lagrange inversion theorem to see that $g(x)^4-g(x)=x$ , where $$g(x)=-x{_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;-\tfrac{4^4}{3^3}x^3).$$ Setting $F(x)={_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)$ , we have $$xF(x)^4=\frac{4^4}{3^3}(F(x)-1),$$ so that $$e_{1,4}^{3,2}(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43)=\int_0^1xF(x)^4dx=\frac{4^4}{3^3}(J-1),$$ where $J$ is the integral in the title. It may or may not help, but we can use integral representations of $_pF_q$ to get $$J=\int_0^1 F(x)dx=\frac{\Gamma(\tfrac43)\Gamma(\tfrac23)}{\pi\Gamma(\tfrac7{12})}\int_0^1\int_0^1\int_0^1\frac{dtdxdz}{x^{1/2}(1-x)^{1/2}t^{1/4}(1-t)^{5/12}(1-txz)^{1/4}}.$$ According to Desmos, the value of $J$ is roughly $J\approx 1.08494289471$ , but for some reason I can't get wolfram alpha to get me anything better. Is there any way to evaluate $J$ ? Thanks :)","Is there a closed form evaluation for the integral Context: I have been investigating integrals of the form Obviously there is no reason to expect a general closed form, but I have found the following: and I found these through applying the Lagrange inversion theorem to the functions and , respectively. The proofs are below. Theorem. We have the explicit evaluation Proof. Using the Lagrange inversion theorem, the function , satisfying is given by the hypergeometric series , for . Thus, the function satisfies for . Thus Then using we have It is then not too difficult to show that which gives and thus . Theorem. We have the explicit evaluation Proof. The Lagrange inversion theorem gives , for Setting , we have and thus Then from here and here , we have which is equivalent to and . Here is my work on the current problem. As you may have guessed, we use the Lagrange inversion theorem to see that , where Setting , we have so that where is the integral in the title. It may or may not help, but we can use integral representations of to get According to Desmos, the value of is roughly , but for some reason I can't get wolfram alpha to get me anything better. Is there any way to evaluate ? Thanks :)","J=\int_0^1 {_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x)dx? e_{p,q}^{n,m}\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}}\right)=\int_0^1x^n\left[{_p}F_q\left({\begin{array}ca_1,..., a_p\\b_1,...,b_q\end{array}};x\right)\right]^mdx. E_1=e_{2,1}^{1,2}(\tfrac12,1;2)=12-16\ln2,\tag1 E_2=e_{2,1}^{1,3}(\tfrac13,\tfrac23;\tfrac32)=\frac{27}{32}.\tag2 x^2-x x^3-x \int_0^1x\left[{_2}F_1(\tfrac12,1;2;x)\right]^2dx=12-16\ln2.\tag{1'} g(x) g(x)^2-g(x)=x, g(x)=-x\,{_2}F_1(\tfrac12,1;2;-4x) x\in[-1/4,\infty) F(x)={_2}F_1(\tfrac12,1;2;x) xF(x)^2=4(F(x)-1), x\in(-\infty,1] E_1=\int_0^1xF(x)^2dx=-4+4\int_0^1F(x)dx. _2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1\frac{t^{b-1}(1-t)^{c-b-1}}{(1-zt)^a}dt, F(x)=\frac2\pi\int_0^1\sqrt{\frac{1-t}{t}}\frac{dt}{1-xt}=\frac4\pi\int_0^\infty\frac{t^2dt}{(t^2+1)(t^2+1-x)}=\frac{2}{1+\sqrt{1-x}}. \int_0^1F(x)dx=\int_0^1\frac{2dx}{1+\sqrt{1-x}}=4-4\ln2, (1') (1) \square \int_0^1 x\left[{_2}F_1(\tfrac13,\tfrac23;\tfrac32;x)\right]^3dx=\frac{27}{32}.\tag{2'} g(x)^3-g(x)=x g(x)=-x{_2}F_1(\tfrac13,\tfrac23;\tfrac32;\tfrac{27}{4}x^2),\qquad |x|<\frac{2}{3\sqrt3}. F(x)={_2}F_1(\tfrac13,\tfrac23;\tfrac32;x) 4xF(x)^3=27(F(x)-1), E_2=\int_0^1xF(x)^3dx=\frac{27}{4}\left(-1+\int_0^1F(x)dx\right). \int_0^1F(x)dx=\left(\frac32\right)^2\left(\frac{\Gamma(\tfrac12)\Gamma(\tfrac32)}{\Gamma(\tfrac56)\Gamma(\tfrac76)}-1\right)=\frac{9}{8}, (2) (2') \square g(x)^4-g(x)=x g(x)=-x{_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;-\tfrac{4^4}{3^3}x^3). F(x)={_3}F_2(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43;x) xF(x)^4=\frac{4^4}{3^3}(F(x)-1), e_{1,4}^{3,2}(\tfrac14,\tfrac12,\tfrac34;\tfrac23,\tfrac43)=\int_0^1xF(x)^4dx=\frac{4^4}{3^3}(J-1), J _pF_q J=\int_0^1 F(x)dx=\frac{\Gamma(\tfrac43)\Gamma(\tfrac23)}{\pi\Gamma(\tfrac7{12})}\int_0^1\int_0^1\int_0^1\frac{dtdxdz}{x^{1/2}(1-x)^{1/2}t^{1/4}(1-t)^{5/12}(1-txz)^{1/4}}. J J\approx 1.08494289471 J","['real-analysis', 'integration', 'sequences-and-series', 'special-functions', 'hypergeometric-function']"
11,"Given that $g$ is only continuous at $0$, not on $[0,1]$, show $\lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0)$.","Given that  is only continuous at , not on , show .","g 0 [0,1] \lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0)","Important Notice : Observe that although this question is very similar to many other questions such as the one in proving $\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$ when f is continous on [0,1] , this problem only assumes continuity at $0$ , but not on $[0,1]$ . Here is the question: Assume $g$ is (Riemann) integrable on $[0, 1]$ and continuous at $0$ . Show $\lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0)$ . My attempt at this question: I split up the integral into two parts from $[0,1-\alpha]$ and $[1-\alpha,1]$ . For $[0,1-\alpha]$ , there is uniform continuity, so the limit $n \to \infty$ can be shifted inside the integral to get $g(0)$ . But how do I keep the integral over $[1-\alpha,1]$ to be small? Note: This is Exercise $7.4.10$ in Abbott, Understanding Analysis, 2nd edition.","Important Notice : Observe that although this question is very similar to many other questions such as the one in proving $\lim\limits_{n\to\infty} \int_{0}^{1} f(x^n)dx = f(0)$ when f is continous on [0,1] , this problem only assumes continuity at , but not on . Here is the question: Assume is (Riemann) integrable on and continuous at . Show . My attempt at this question: I split up the integral into two parts from and . For , there is uniform continuity, so the limit can be shifted inside the integral to get . But how do I keep the integral over to be small? Note: This is Exercise in Abbott, Understanding Analysis, 2nd edition.","0 [0,1] g [0, 1] 0 \lim_{n \to \infty} \int_0^1 g(x^n) dx = g(0) [0,1-\alpha] [1-\alpha,1] [0,1-\alpha] n \to \infty g(0) [1-\alpha,1] 7.4.10","['real-analysis', 'continuity', 'riemann-integration']"
12,A term for a unimodal function similar to $\exp(-x^2)$,A term for a unimodal function similar to,\exp(-x^2),"Is there a term for a smooth $\mathbb R\to\mathbb R^+$ function that is unimodal (has one local maximum), and whose $n^{\text{th}}$ derivative has $(n+1)$ local extrema? An example of such a function is the Gaussian function $f(x)=\exp(-x^2)$ . A negative example of a unimodal function that does not fit these criteria is $f(x)=1/(10+x^4)$ . Its derivatives have too many extrema. Another negative example that shows that the number of extrema can grow exponentially (with respect to the order of a derivative) is the Rvachev function $f(x)=\operatorname{up}\left(x/4\right)$ .","Is there a term for a smooth function that is unimodal (has one local maximum), and whose derivative has local extrema? An example of such a function is the Gaussian function . A negative example of a unimodal function that does not fit these criteria is . Its derivatives have too many extrema. Another negative example that shows that the number of extrema can grow exponentially (with respect to the order of a derivative) is the Rvachev function .",\mathbb R\to\mathbb R^+ n^{\text{th}} (n+1) f(x)=\exp(-x^2) f(x)=1/(10+x^4) f(x)=\operatorname{up}\left(x/4\right),"['real-analysis', 'derivatives', 'terminology', 'definition', 'maxima-minima']"
13,Composition of almost-everywhere differentiable functions $\mathbb{R} \rightarrow \mathbb{R}$,Composition of almost-everywhere differentiable functions,\mathbb{R} \rightarrow \mathbb{R},"Suppose $g, f : \mathbb{R} \to \mathbb{R}$ are almost-everywhere differentiable. Is $g \circ f$ almost-everywhere differentiable? ( This question deals with a-e. continuity, but the answer does not apply. This question deals with a-e. differentiability but in higher dimensions; the answers do not apply either.)","Suppose are almost-everywhere differentiable. Is almost-everywhere differentiable? ( This question deals with a-e. continuity, but the answer does not apply. This question deals with a-e. differentiability but in higher dimensions; the answers do not apply either.)","g, f : \mathbb{R} \to \mathbb{R} g \circ f","['real-analysis', 'measure-theory']"
14,Composition of uniformly continuous maps,Composition of uniformly continuous maps,,"Fix $p>1$ . Let $\mathscr C[0,2]$ be the space of continuous functions on $[0,2]$ with metric given by $$d_{p}(f,g)=||f-g||_p:=\left(\int_0^2 |f(x)-g(x)|^p  \ dx\right)^{\frac{1}{p}}.$$ Consider the map $\Psi:\mathscr C[0,2]\rightarrow \mathbb R$ given by $$\Psi(f)=\left(\int_0^2 f(x)\ dx \right)^4.$$ Writing $\Psi = T\circ\Phi$ where $T(x)=x^4$ and $\Phi:\mathscr C[0,2]\rightarrow \mathbb R$ is given by $$\Phi(f)=\int_0^2 f(x)\ dx,$$ one can show that $\Phi$ is uniformly continuous, and so $\Psi$ is continuous. However, one can also show that $\Psi$ is $\textbf{not}$ uniformly continuous: let $f_n(x) = n+\frac{1}{n^3}$ , $g_n(x)= n$ . Then $$||f_n-g_n||_p=\frac{2^{1/p}}{n^3}\rightarrow 0 \quad \text{as}\quad n\rightarrow \infty.$$ On the other hand, $$|\Psi(f_n)-\Psi(g_n)|=16\left(\left(n+\frac{1}{n^3}\right)^4-n^4\right)\not\rightarrow 0\quad \text{as}\quad n\rightarrow \infty.$$ This seems to contradict the fact that the composition $f\circ g$ of two uniformly functions $f,g:\mathbb R\rightarrow \mathbb R$ is uniformly continuous (see this for example). Is this phenomenon due to the different metrics used?","Fix . Let be the space of continuous functions on with metric given by Consider the map given by Writing where and is given by one can show that is uniformly continuous, and so is continuous. However, one can also show that is uniformly continuous: let , . Then On the other hand, This seems to contradict the fact that the composition of two uniformly functions is uniformly continuous (see this for example). Is this phenomenon due to the different metrics used?","p>1 \mathscr C[0,2] [0,2] d_{p}(f,g)=||f-g||_p:=\left(\int_0^2 |f(x)-g(x)|^p 
\ dx\right)^{\frac{1}{p}}. \Psi:\mathscr C[0,2]\rightarrow \mathbb R \Psi(f)=\left(\int_0^2 f(x)\ dx \right)^4. \Psi = T\circ\Phi T(x)=x^4 \Phi:\mathscr C[0,2]\rightarrow \mathbb R \Phi(f)=\int_0^2 f(x)\ dx, \Phi \Psi \Psi \textbf{not} f_n(x) = n+\frac{1}{n^3} g_n(x)= n ||f_n-g_n||_p=\frac{2^{1/p}}{n^3}\rightarrow 0 \quad \text{as}\quad n\rightarrow \infty. |\Psi(f_n)-\Psi(g_n)|=16\left(\left(n+\frac{1}{n^3}\right)^4-n^4\right)\not\rightarrow 0\quad \text{as}\quad n\rightarrow \infty. f\circ g f,g:\mathbb R\rightarrow \mathbb R",['real-analysis']
15,Going from completion to explicit description of the real numbers,Going from completion to explicit description of the real numbers,,"I have understood, I think, the construction of the real numbers as the set of equivalence classes of Cauchy sequences. That is, if $\{a_n\}$ and $\{b_n\}$ are Cauchy sequences, then we say they are equivalent if $\lim_{n\to \infty} \lvert a_n - b_n\rvert = 0$ . The set of equivalence classes is then a field under the ""obvious"" operations. This is what I believe is the completion of $\mathbb{Q}$ with respect to the absolute value. How does one go from this to the fact that any real number can be described as ""infinite decimals""? That is, given one of the equivalence classes $[\{a_n\}]$ how do we get that $$ [\{a_n\}] = \sum_{i=-n}^\infty a_i10^{-i} $$ ?","I have understood, I think, the construction of the real numbers as the set of equivalence classes of Cauchy sequences. That is, if and are Cauchy sequences, then we say they are equivalent if . The set of equivalence classes is then a field under the ""obvious"" operations. This is what I believe is the completion of with respect to the absolute value. How does one go from this to the fact that any real number can be described as ""infinite decimals""? That is, given one of the equivalence classes how do we get that ?","\{a_n\} \{b_n\} \lim_{n\to \infty} \lvert a_n - b_n\rvert = 0 \mathbb{Q} [\{a_n\}] 
[\{a_n\}] = \sum_{i=-n}^\infty a_i10^{-i}
","['real-analysis', 'number-theory', 'definition', 'real-numbers']"
16,"If $\{f_n\}$ is sequence of measurable functions on $X$, then $\{x: \lim f_n(x) \text{ exists}\}$ is a measurable set.","If  is sequence of measurable functions on , then  is a measurable set.",\{f_n\} X \{x: \lim f_n(x) \text{ exists}\},"If $\{f_n\}$ is sequence of measurable functions on $X$ , then $\{x: \lim f_n(x) \text{ exists}\}$ is a measurable set. My idea is to prove that $$\{x: \lim f_n(x) \text{exists}\}=\{x: \liminf f_n(x)=a<\infty\}\cap \{x: \limsup f_n(x)=a<\infty\}=\bigg(\cup_{j=1}^{\infty}\cap_{n\geq j}\{x: f_n(x)=a\}\bigg)\cap\bigg(\cap_{j=1}^{\infty}\cup_{n\geq j}\{x: f_n(x)=a\}\bigg)$$ which two sets are measurable and their insection is also measurable. Is it correct? Moreover, Notice that function $g=\limsup f_n-\liminf f_n$ is measurable because $f_n$ is measurable and by proposition 2.7. So $$\{x: \lim f_n\text{ exists} \}=\{x: \limsup f_n=\liminf f_n\}=\{x: g(x)=0\}.$$ is measurable which because $g^{-1}(\{0\})$ is measurable. Another question: Do I need to consider that $ \limsup f_n=\pm \infty \text{ or } \liminf f_n=\pm \infty$","If is sequence of measurable functions on , then is a measurable set. My idea is to prove that which two sets are measurable and their insection is also measurable. Is it correct? Moreover, Notice that function is measurable because is measurable and by proposition 2.7. So is measurable which because is measurable. Another question: Do I need to consider that",\{f_n\} X \{x: \lim f_n(x) \text{ exists}\} \{x: \lim f_n(x) \text{exists}\}=\{x: \liminf f_n(x)=a<\infty\}\cap \{x: \limsup f_n(x)=a<\infty\}=\bigg(\cup_{j=1}^{\infty}\cap_{n\geq j}\{x: f_n(x)=a\}\bigg)\cap\bigg(\cap_{j=1}^{\infty}\cup_{n\geq j}\{x: f_n(x)=a\}\bigg) g=\limsup f_n-\liminf f_n f_n \{x: \lim f_n\text{ exists} \}=\{x: \limsup f_n=\liminf f_n\}=\{x: g(x)=0\}. g^{-1}(\{0\})  \limsup f_n=\pm \infty \text{ or } \liminf f_n=\pm \infty,['real-analysis']
17,Finite dimensional real function spaces closed under composition,Finite dimensional real function spaces closed under composition,,"I’m interested in classifying all finite dimensional vector subspaces of $V=C^\infty(\mathbb{R})$ which are closed under composition: $f,g \in V$ implies $f\circ g\in V$ . The only such subspaces which come to mind are $0$ , the space of all constant functions, the space of all linear functions $x\mapsto mx$ , and the space of all affine functions $x\mapsto mx+b$ . I believe this list may be complete, but I can’t think of an easy proof. Is it true that these are the only four such spaces?","I’m interested in classifying all finite dimensional vector subspaces of which are closed under composition: implies . The only such subspaces which come to mind are , the space of all constant functions, the space of all linear functions , and the space of all affine functions . I believe this list may be complete, but I can’t think of an easy proof. Is it true that these are the only four such spaces?","V=C^\infty(\mathbb{R}) f,g \in V f\circ g\in V 0 x\mapsto mx x\mapsto mx+b","['real-analysis', 'functions', 'vector-spaces']"
18,Does $f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|}$ converge pointwise or uniformly?,Does  converge pointwise or uniformly?,f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|},"I want to check whether the sequence below converges pointwise or uniformly $$f_{n}(z):\{z\in\mathbb{C}:|z| = 1\}\to\mathbb{R}$$ $$\qquad\qquad f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|}$$ I have tried using the triangle inequality $||z|-|w||\leq|z-w|$ but that gives me $f_{n}(z)\leq 1$ . Intuitively, I don't even know whether the sequence is meant to converge pointwise because while $n^{2}\to\infty$ , $e^{in}$ is dense on the unit circle, so it seems to me that $|z-e^{in}|$ should be close to $0$ infinitely many times. How can I prove/disprove this sequence converges pointwise and/or uniformly?","I want to check whether the sequence below converges pointwise or uniformly I have tried using the triangle inequality but that gives me . Intuitively, I don't even know whether the sequence is meant to converge pointwise because while , is dense on the unit circle, so it seems to me that should be close to infinitely many times. How can I prove/disprove this sequence converges pointwise and/or uniformly?",f_{n}(z):\{z\in\mathbb{C}:|z| = 1\}\to\mathbb{R} \qquad\qquad f_{n}(z) = \frac{1}{1+n^{2}|z-e^{in}|} ||z|-|w||\leq|z-w| f_{n}(z)\leq 1 n^{2}\to\infty e^{in} |z-e^{in}| 0,"['real-analysis', 'convergence-divergence', 'uniform-convergence', 'pointwise-convergence']"
19,Let $f : \mathbb{R}^{2} \to \mathbb{R}$ be of class $C^{1}$. Show that $f$ not is injective. [duplicate],Let  be of class . Show that  not is injective. [duplicate],f : \mathbb{R}^{2} \to \mathbb{R} C^{1} f,"This question already has answers here : Show that no application $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, of $C^k$ class, $k \geq 1$ can be injective [duplicate] (3 answers) Closed 4 years ago . Let $f : \mathbb{R}^{2} \to \mathbb{R}$ be of class $C^{1}$ . Show that $f$ not is injective. Well, I know that $f$ is injective when $f(x) = f(y)$ implies $x = y$ . Since $f$ is of class $C^{1}$ then $f$ is differentiable in $\mathbb{R}^{2}$ such that its derivative is continuous. I can not figure out the ideas. This question seems to suggest a counter example, but none is working. Thanks for the help!","This question already has answers here : Show that no application $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, of $C^k$ class, $k \geq 1$ can be injective [duplicate] (3 answers) Closed 4 years ago . Let be of class . Show that not is injective. Well, I know that is injective when implies . Since is of class then is differentiable in such that its derivative is continuous. I can not figure out the ideas. This question seems to suggest a counter example, but none is working. Thanks for the help!",f : \mathbb{R}^{2} \to \mathbb{R} C^{1} f f f(x) = f(y) x = y f C^{1} f \mathbb{R}^{2},"['real-analysis', 'analysis', 'derivatives']"
20,Characterization of the quasi-arithmetic mean,Characterization of the quasi-arithmetic mean,,"The $f$ -mean , where $f$ is a continuous monotonically-increasing function, is defined as: $$  M_f(x_1, \dots, x_n) = f^{-1}\left( \frac{f(x_1)+ \cdots + f(x_n)}n \right). $$ For any $f$ , $M_f$ has the following nice properties: Continuous; Monotonically-increasing in each argument; Symmetric -- attains the same value for any permutation of the arguments; Fixed point: for each $x\in \mathbb{R}$ , $M_f(x,\ldots,x) = x$ . Is it true that any function satisfying these four properties is an $f$ -mean for some function $f$ ? If not - what properties should be added in order to characterize $f$ -mean?","The -mean , where is a continuous monotonically-increasing function, is defined as: For any , has the following nice properties: Continuous; Monotonically-increasing in each argument; Symmetric -- attains the same value for any permutation of the arguments; Fixed point: for each , . Is it true that any function satisfying these four properties is an -mean for some function ? If not - what properties should be added in order to characterize -mean?","f f  
M_f(x_1, \dots, x_n) = f^{-1}\left( \frac{f(x_1)+ \cdots + f(x_n)}n \right).
 f M_f x\in \mathbb{R} M_f(x,\ldots,x) = x f f f","['real-analysis', 'means']"
21,$\forall a > 0$ $\sum_{n=1}^{\infty} f(na)$ is convergent. Prove that $\int_{0}^{\infty}f(x) dx$ is convergent.,is convergent. Prove that  is convergent.,\forall a > 0 \sum_{n=1}^{\infty} f(na) \int_{0}^{\infty}f(x) dx,"Hi can you help me solve this exercise? Thanks. Let $f: [0;+\infty) \to \mathbb{R}$ be nonnegative and continuous function. Suppose $\forall a > 0$ $\sum_{n=1}^{\infty} f(na)$ is convergent. Prove that $\int_{0}^{\infty}f(x) dx$ is convergent. I tried to solve it by using the Riemann sum, but for fixed a it doesn't work. I have no other ideas.","Hi can you help me solve this exercise? Thanks. Let be nonnegative and continuous function. Suppose is convergent. Prove that is convergent. I tried to solve it by using the Riemann sum, but for fixed a it doesn't work. I have no other ideas.",f: [0;+\infty) \to \mathbb{R} \forall a > 0 \sum_{n=1}^{\infty} f(na) \int_{0}^{\infty}f(x) dx,"['real-analysis', 'analysis']"
22,Is it possible to manipulate Niven's proof of the irrationality of $\pi$ to prove the irrationality of $\sqrt{2}$?,Is it possible to manipulate Niven's proof of the irrationality of  to prove the irrationality of ?,\pi \sqrt{2},"Section 2 of Keith Conrad's note ""Irrationality of $\pi$ and $e$ "" (PDF link via uconn.edu) recounts Ivan Niven's proof of irrationality of $\pi$ . (See also Niven's original note ""A simple proof that $\pi$ is irrational"" (PDF link via ProjectEuclid.org) .) Is it possible to manipulate this proof to prove the irrationality of, say, $\sqrt{2}$ ?","Section 2 of Keith Conrad's note ""Irrationality of and "" (PDF link via uconn.edu) recounts Ivan Niven's proof of irrationality of . (See also Niven's original note ""A simple proof that is irrational"" (PDF link via ProjectEuclid.org) .) Is it possible to manipulate this proof to prove the irrationality of, say, ?",\pi e \pi \pi \sqrt{2},"['real-analysis', 'calculus', 'analysis', 'irrational-numbers']"
23,"Study convergence of $x_{n+1} = x_n^2 + 3x_n + 1$, where $x_1 = a$, and $a$ takes different values and find its limit.","Study convergence of , where , and  takes different values and find its limit.",x_{n+1} = x_n^2 + 3x_n + 1 x_1 = a a,"Given a recurrence relation: $$ x_{n+1} = x_n^2 + 3x_n + 1 \\ x_1 = a\\ n\in\Bbb N $$ Figure out whether this sequence has a limit (either finite or infinite) and find it for: $$ \begin{align*} a = -{5\over 4}\tag1 \\ a = -{3\over 4}\tag2 \end{align*} $$ Start with case $(1)$ . It took some time to notice but seems like the sequence is monotonically increasing no matter what initial conditions are given. That is because: $$ x_{n+1} = x_n^2 + 3x_n + 1 \iff x_{n+1}-x_n = x_n^2 + 2x_n + 1 = (x_n+1)^2>0 $$ Than means: $$ x_{n+1} - x_n > 0 \iff x_{n+1} > x_n $$ That observation is crucial for all the next steps. In $(1)$ we are given that: $$ x_1 = a = -{5\over 4} > -2 $$ By monotonicity of $x_n$ : $$ \forall n\in\Bbb N : x_n > -2 $$ Let's suppose the limit exists. Then by finding fixed points of the recurrence we may get an insight of what that limit might be: $$ L = L^2 + 3L + 1 \iff (L+1)^2 = 0 \iff L = -1 $$ Thus the only possible finite limit in $\Bbb R$ is $L=-1$ . Let's try to bound $x_n$ above. Using induction: $$ x_1 < x_2 = -{19\over 16}  < -1 $$ Suppose $x_n < -1$ . Then: $$ x_n \in (-2; -1) \implies \underbrace{(x_n + 1)^2 + x_n}_{x_{n+1}} \in (-2, -1) $$ Thus it follows that $x_{n+1} < -1$ . Now by monotone convergence theorem a monotonic bounded sequence has a limit. Therefore: $$ \boxed{\lim_{n\to\infty}x_n = -1} $$ This case is more of a headache. Given $a = -{3\over 4}$ makes the sequence diverge to $+\infty$ . But to show this I had to calculate the value for $6$ first terms. It follows that: $$ \forall n \ge 6: x_n > 0 $$ Moreover: $$ \forall n \ge 7: x_n > 1 $$ So: $$ \boxed{\lim_{n\to\infty}x_n = +\infty} $$ Does there exist a more elegant way to solve for case $(2)$ ? Also is this argumentation enough to show what's requested in question section? I have doubts about the second case. Because formally I should have shown that the sequence is not bounded, not sure how to do it. And the solution is ugly. Here is a sandbox I've been using to play around with the recurrence . Could you please verify the above and point to the mistakes just in case? Thank you!","Given a recurrence relation: Figure out whether this sequence has a limit (either finite or infinite) and find it for: Start with case . It took some time to notice but seems like the sequence is monotonically increasing no matter what initial conditions are given. That is because: Than means: That observation is crucial for all the next steps. In we are given that: By monotonicity of : Let's suppose the limit exists. Then by finding fixed points of the recurrence we may get an insight of what that limit might be: Thus the only possible finite limit in is . Let's try to bound above. Using induction: Suppose . Then: Thus it follows that . Now by monotone convergence theorem a monotonic bounded sequence has a limit. Therefore: This case is more of a headache. Given makes the sequence diverge to . But to show this I had to calculate the value for first terms. It follows that: Moreover: So: Does there exist a more elegant way to solve for case ? Also is this argumentation enough to show what's requested in question section? I have doubts about the second case. Because formally I should have shown that the sequence is not bounded, not sure how to do it. And the solution is ugly. Here is a sandbox I've been using to play around with the recurrence . Could you please verify the above and point to the mistakes just in case? Thank you!","
x_{n+1} = x_n^2 + 3x_n + 1 \\
x_1 = a\\
n\in\Bbb N
 
\begin{align*}
a = -{5\over 4}\tag1 \\
a = -{3\over 4}\tag2
\end{align*}
 (1) 
x_{n+1} = x_n^2 + 3x_n + 1 \iff x_{n+1}-x_n = x_n^2 + 2x_n + 1 = (x_n+1)^2>0
 
x_{n+1} - x_n > 0 \iff x_{n+1} > x_n
 (1) 
x_1 = a = -{5\over 4} > -2
 x_n 
\forall n\in\Bbb N : x_n > -2
 
L = L^2 + 3L + 1 \iff (L+1)^2 = 0 \iff L = -1
 \Bbb R L=-1 x_n 
x_1 < x_2 = -{19\over 16}  < -1
 x_n < -1 
x_n \in (-2; -1) \implies \underbrace{(x_n + 1)^2 + x_n}_{x_{n+1}} \in (-2, -1)
 x_{n+1} < -1 
\boxed{\lim_{n\to\infty}x_n = -1}
 a = -{3\over 4} +\infty 6 
\forall n \ge 6: x_n > 0
 
\forall n \ge 7: x_n > 1
 
\boxed{\lim_{n\to\infty}x_n = +\infty}
 (2)","['real-analysis', 'sequences-and-series', 'proof-verification', 'recurrence-relations']"
24,Does $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ for infinitely many $j$ imply $\{a_i\}=\{b_i\}$?,Does  for infinitely many  imply ?,\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j j \{a_i\}=\{b_i\},"Suppose $a_1,...,a_n,b_1,...,b_n$ are real numbers with $\sum_{i=1}^na_i=\sum_{i=1}^n b_i,\sum_{i=1}^n a_i^2=\sum_{i=1}^n b_i^2$ and for infinitely many $j\geq3$ , $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ . Does this imply $\{a_1,...,a_n\}=\{b_1,...,b_n\}$ ? The answer is positive if I can find infinitely many even integers $j$ such that $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ . But if I don't have this, still is the result true? Intuitively it seems to be.","Suppose are real numbers with and for infinitely many , . Does this imply ? The answer is positive if I can find infinitely many even integers such that . But if I don't have this, still is the result true? Intuitively it seems to be.","a_1,...,a_n,b_1,...,b_n \sum_{i=1}^na_i=\sum_{i=1}^n b_i,\sum_{i=1}^n a_i^2=\sum_{i=1}^n b_i^2 j\geq3 \sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j \{a_1,...,a_n\}=\{b_1,...,b_n\} j \sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j","['real-analysis', 'real-numbers']"
25,Indefinite integral of $\frac{\tan{(1+x^2)}}{1+x^2}$,Indefinite integral of,\frac{\tan{(1+x^2)}}{1+x^2},"Let $f(x) = \frac{\tan{(1+x^2)}}{1+x^2}$ , find $\int f(x)dx$ . I've tried many substitutions (including trigonometric substitutions like $x=\tan \theta$ ) and also integration by parts but didn't work . We can apply power series but it doesn't solve problem .","Let , find . I've tried many substitutions (including trigonometric substitutions like ) and also integration by parts but didn't work . We can apply power series but it doesn't solve problem .",f(x) = \frac{\tan{(1+x^2)}}{1+x^2} \int f(x)dx x=\tan \theta,"['real-analysis', 'calculus', 'integration', 'indefinite-integrals']"
26,Lower bound for an integral of a polynomial over an interval of finite length,Lower bound for an integral of a polynomial over an interval of finite length,,"Assume that $Q$ is a degree $k$ polynomial and $I$ an interval of finite length $c$ . Can you please give me some hints on how to show that $$ \left( \sum_{m=0}^k \frac{c^{m}}{m!} |Q^{(m)} (t_0) |  \right)^2  \leq \frac{C(k)^{2c}}{c} \int_{I} Q^{2}(t) dt, $$ for some positive constant $C(k)$ depending only on the degree $k$ and any $t_0 \in I$ . The statement is very puzzling and I have had a hard time establishing it. All hints are therefore greatly appreciated. EDIT: If this is wrong, as it looks like, can it be repaired under some additional assumptions? Thank you.","Assume that is a degree polynomial and an interval of finite length . Can you please give me some hints on how to show that for some positive constant depending only on the degree and any . The statement is very puzzling and I have had a hard time establishing it. All hints are therefore greatly appreciated. EDIT: If this is wrong, as it looks like, can it be repaired under some additional assumptions? Thank you.","Q k I c  \left( \sum_{m=0}^k \frac{c^{m}}{m!} |Q^{(m)} (t_0) |  \right)^2  \leq \frac{C(k)^{2c}}{c} \int_{I} Q^{2}(t) dt,  C(k) k t_0 \in I","['real-analysis', 'inequality', 'polynomials', 'integral-inequality']"
27,"If $X$ is a Borel set and $f$ has countably many discontinuities, prove that $f:X\rightarrow \mathbf{R}$ is Borel measurable.","If  is a Borel set and  has countably many discontinuities, prove that  is Borel measurable.",X f f:X\rightarrow \mathbf{R},"The Problem Good evening!  I am currently struggling with the following exercise. Suppose $X$ is a Borel subset of $\mathbf{R}$ and $f:X\rightarrow \mathbf{R}$ is a function such that $\{x\in X: f \text{ is not continuous at } x\}$ is a countable set.  Prove $f$ is a Borel measurable function. What I Know Unfortunately I am pretty lost with this one.  I was told that the following observation is an important item in my toolkit for handling this problem: To show that $f$ is a Borel measurable function, it suffices to prove that $f^{-1}((a,\infty))=\{x\in X: f(x)>a\}$ is a Borel set for all $a\in\mathbf{R}$ . I have a sneaking suspicion that $X$ is a Borel set the set of discontinuities of $f$ is countable are also equally important pieces of information here.  But I don't know why. My Question I think all I need here is a little push .  I have all the pieces in front of me, I think, but I don't know how they fit together.  In other words, I would really appreciate a tip on how you might approach this problem in an intuitive way.  This is my first time working with this material, so you can safely assume that I am unfamiliar with the more ""advanced"" theorems that could be used here.  The more basic, the better (in my case at least)! Thank you all in advance!","The Problem Good evening!  I am currently struggling with the following exercise. Suppose is a Borel subset of and is a function such that is a countable set.  Prove is a Borel measurable function. What I Know Unfortunately I am pretty lost with this one.  I was told that the following observation is an important item in my toolkit for handling this problem: To show that is a Borel measurable function, it suffices to prove that is a Borel set for all . I have a sneaking suspicion that is a Borel set the set of discontinuities of is countable are also equally important pieces of information here.  But I don't know why. My Question I think all I need here is a little push .  I have all the pieces in front of me, I think, but I don't know how they fit together.  In other words, I would really appreciate a tip on how you might approach this problem in an intuitive way.  This is my first time working with this material, so you can safely assume that I am unfamiliar with the more ""advanced"" theorems that could be used here.  The more basic, the better (in my case at least)! Thank you all in advance!","X \mathbf{R} f:X\rightarrow \mathbf{R} \{x\in X: f \text{ is not continuous at } x\} f f f^{-1}((a,\infty))=\{x\in X: f(x)>a\} a\in\mathbf{R} X f","['real-analysis', 'measure-theory', 'borel-sets']"
28,How do I find a closed form of the characteristic function of Gamma distribution?,How do I find a closed form of the characteristic function of Gamma distribution?,,"It’s written in wikipedia that $(1-i \beta t)^{-\alpha}$ is the characteristic function of the Gamma distribution $Gamma(\alpha,\beta)$ . I tried to prove this but I failed. So I googled it for a proof, but surprisingly there is no written proof on google . Even in this website, there are some questions asking for it, but there is no answer. How do I prove it? That is, Let $\alpha,\beta>0$ and $t\in \mathbb{R}$ . Then, how do I show that $\int_0^\infty x^{\alpha -1}e^{-x(1-i\beta t)/\beta} dx =\Gamma(\alpha)\beta^\alpha (1-i\beta t)^{-\alpha}$ ? Could someone prove this as an answer or tell me a refenrence which deals with this. Thank you in advance!","It’s written in wikipedia that is the characteristic function of the Gamma distribution . I tried to prove this but I failed. So I googled it for a proof, but surprisingly there is no written proof on google . Even in this website, there are some questions asking for it, but there is no answer. How do I prove it? That is, Let and . Then, how do I show that ? Could someone prove this as an answer or tell me a refenrence which deals with this. Thank you in advance!","(1-i \beta t)^{-\alpha} Gamma(\alpha,\beta) \alpha,\beta>0 t\in \mathbb{R} \int_0^\infty x^{\alpha -1}e^{-x(1-i\beta t)/\beta} dx =\Gamma(\alpha)\beta^\alpha (1-i\beta t)^{-\alpha}","['real-analysis', 'integration', 'complex-analysis', 'probability-theory', 'reference-request']"
29,How to show that $C^\infty_0$ is not dense in $L^p_{weak} (\mathbb{R}^n)$?,How to show that  is not dense in ?,C^\infty_0 L^p_{weak} (\mathbb{R}^n),"Let $C^\infty_0$ denote the  smooth, compactly supported functions on $\mathbb{R}^n$ . Let $L^p_{\mathrm{weak}}(\mathbb{R}^n)$ denote the space of all functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ which satisfy $$  \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big| \lesssim \lambda^{-p} $$ for all $\lambda > 0$ . This space when endowed with the quasi-norm $$ \Vert f\Vert_{L^p_{\mathrm{weak}}}^* : = \sup_{\lambda > 0} \lambda \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big|^{1/p} $$ is a quasi-Banach space. How do you show that $C^\infty_0$ is not dense in $L^p_{\mathrm{weak}} (\mathbb{R}^n)$ for $1< p < \infty$ ? This question was inspired by a real analysis qualifying problem, and I know that considering the function $f(x) = |x|^{-n/p}$ is important.","Let denote the  smooth, compactly supported functions on . Let denote the space of all functions which satisfy for all . This space when endowed with the quasi-norm is a quasi-Banach space. How do you show that is not dense in for ? This question was inspired by a real analysis qualifying problem, and I know that considering the function is important.","C^\infty_0 \mathbb{R}^n L^p_{\mathrm{weak}}(\mathbb{R}^n) f:\mathbb{R}^n \rightarrow \mathbb{R} 
 \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big| \lesssim \lambda^{-p}
 \lambda > 0 
\Vert f\Vert_{L^p_{\mathrm{weak}}}^* : = \sup_{\lambda > 0} \lambda \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big|^{1/p}
 C^\infty_0 L^p_{\mathrm{weak}} (\mathbb{R}^n) 1< p < \infty f(x) = |x|^{-n/p}","['real-analysis', 'general-topology', 'functional-analysis', 'weak-lp-spaces']"
30,Examples of functions where $f'(x)=f(f(x))$ for all $x$,Examples of functions where  for all,f'(x)=f(f(x)) x,I am looking for examples of functions $f:\mathbb R \to \mathbb R$ where $f'(x)=f(f(x))$ for all $x$. The only example I can find is the trivial one where f is identically 0.,I am looking for examples of functions $f:\mathbb R \to \mathbb R$ where $f'(x)=f(f(x))$ for all $x$. The only example I can find is the trivial one where f is identically 0.,,[]
31,Show that ${\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n} + \frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}}$,Show that,{\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n} + \frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}},"Let $n\in\mathbb{N}$ such that ${n}>{0}$ . Show that $${\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n}+\frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}}$$ when $x_k > 0, \forall k$ . I thought I can use AM GM inequality, but if I try to break it in two smaller pieces and apply AMGM I get a false affirmation for $\frac{\sqrt[n]{x_1x_2...x_n}}{\ x_1+\ x_2+...+x_n}$ . Any help please?","Let such that . Show that when . I thought I can use AM GM inequality, but if I try to break it in two smaller pieces and apply AMGM I get a false affirmation for . Any help please?","n\in\mathbb{N} {n}>{0} {\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n}+\frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}} x_k > 0, \forall k \frac{\sqrt[n]{x_1x_2...x_n}}{\ x_1+\ x_2+...+x_n}",['real-analysis']
32,Find $f(x)$ if $f\left(\frac{x+y}{3}\right)=\frac{2+f(x)+f(y)}{3}$,Find  if,f(x) f\left(\frac{x+y}{3}\right)=\frac{2+f(x)+f(y)}{3},"$f : \mathbb{R} \to \mathbb{R}$ is a differentiable function satisfying $$f\left(\frac{x+y}{3}\right)=\frac{2+f(x)+f(y)}{3}$$ if $f'(0)=2$, find the function My Try: we have $$f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}=\frac{2+f(x)}{3}$$ $\implies$ $$\frac{f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}}{\frac{x}{3}}=\frac{2+f(x)}{x}$$ Now taking Limit $x \to 0$ we have $$\lim_{x \to 0}\frac{f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}}{\frac{x}{3}}=\lim_{x \to 0}\frac{2+f(x)}{x}$$ $\implies$ $$f'\left(\frac{y}{3}\right)=\lim_{x \to 0}\frac{2+f(x)}{x}$$ Now since LHS to be finite , we need $0/0$ form in RHS, hence $f(0)=-2$ Now by L'Hopital's Rule we get $$f'\left(\frac{y}{3}\right)=f'(0)=2$$ Integrating we get $$3f\left(\frac{y}{3}\right)=2y+c$$ Putting $y=0$ we get $c=-6$ So $$3f\left(\frac{y}{3}\right)=2y-6$$ So $$f(y)=2y-2$$ Hence $$f(x)=2x-2$$ But this function is not satisfying given functional equation. What went wrong?","$f : \mathbb{R} \to \mathbb{R}$ is a differentiable function satisfying $$f\left(\frac{x+y}{3}\right)=\frac{2+f(x)+f(y)}{3}$$ if $f'(0)=2$, find the function My Try: we have $$f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}=\frac{2+f(x)}{3}$$ $\implies$ $$\frac{f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}}{\frac{x}{3}}=\frac{2+f(x)}{x}$$ Now taking Limit $x \to 0$ we have $$\lim_{x \to 0}\frac{f\left(\frac{x+y}{3}\right)-\frac{f(y)}{3}}{\frac{x}{3}}=\lim_{x \to 0}\frac{2+f(x)}{x}$$ $\implies$ $$f'\left(\frac{y}{3}\right)=\lim_{x \to 0}\frac{2+f(x)}{x}$$ Now since LHS to be finite , we need $0/0$ form in RHS, hence $f(0)=-2$ Now by L'Hopital's Rule we get $$f'\left(\frac{y}{3}\right)=f'(0)=2$$ Integrating we get $$3f\left(\frac{y}{3}\right)=2y+c$$ Putting $y=0$ we get $c=-6$ So $$3f\left(\frac{y}{3}\right)=2y-6$$ So $$f(y)=2y-2$$ Hence $$f(x)=2x-2$$ But this function is not satisfying given functional equation. What went wrong?",,"['real-analysis', 'algebra-precalculus', 'functions', 'functional-equations']"
33,"If a function f has no jump discontinuities, does it have the intermediate value theorem property?","If a function f has no jump discontinuities, does it have the intermediate value theorem property?",,"I realized I was confused by this concept (while preparing for my exam). If a function f has no jump discontinuities, does it have the intermediate value theorem property? Facts, I know: I know that continuity implies intermediate value theorem property.  However, intermediate value theorem property does not imply continuity.  All derivatives have the intermediate value theorem property, but we can have discontinuous derivatives. Derivatives do not have jump discontinuities, or discontinuities of the first kind. I don't know if a function has no jump discontinuities then it necessarily has the intermediate value theorem property.  I know this works for derivatives.  I was trying to construct a discontinuous function with discontinuities of the second kind, (one of left or right does not exist), with no jump discontinuities, that doesn't have intermediate value theorem property, but I couldn't think of any. Thanks","I realized I was confused by this concept (while preparing for my exam). If a function f has no jump discontinuities, does it have the intermediate value theorem property? Facts, I know: I know that continuity implies intermediate value theorem property.  However, intermediate value theorem property does not imply continuity.  All derivatives have the intermediate value theorem property, but we can have discontinuous derivatives. Derivatives do not have jump discontinuities, or discontinuities of the first kind. I don't know if a function has no jump discontinuities then it necessarily has the intermediate value theorem property.  I know this works for derivatives.  I was trying to construct a discontinuous function with discontinuities of the second kind, (one of left or right does not exist), with no jump discontinuities, that doesn't have intermediate value theorem property, but I couldn't think of any. Thanks",,"['real-analysis', 'analysis', 'limits', 'derivatives', 'continuity']"
34,A compact property of linear operators.,A compact property of linear operators.,,"Let $\Omega\subset\mathbb{R}^{n}$ be a smoothly, open, bounded set and let $1<p<q<\infty$. Is it true that any (linear) continuous operator $T:L^{p}\left(\Omega\right)\rightarrow L^{q}\left(\Omega\right)$ is a compact operator? Is it true that any (linear) continuous operator $T:W^{2,p}\left(\Omega\right)\rightarrow W^{2,q}\left(\Omega\right)$ is a compact operator? Here $W^{2,p}$ denotes the usual Sobolev space. Thanks. I think, in general, 1. is false and 2. is true. Any reference is enough for my purpose.","Let $\Omega\subset\mathbb{R}^{n}$ be a smoothly, open, bounded set and let $1<p<q<\infty$. Is it true that any (linear) continuous operator $T:L^{p}\left(\Omega\right)\rightarrow L^{q}\left(\Omega\right)$ is a compact operator? Is it true that any (linear) continuous operator $T:W^{2,p}\left(\Omega\right)\rightarrow W^{2,q}\left(\Omega\right)$ is a compact operator? Here $W^{2,p}$ denotes the usual Sobolev space. Thanks. I think, in general, 1. is false and 2. is true. Any reference is enough for my purpose.",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'compact-operators']"
35,Example 7.21 and Definition 7.22 in Baby Rudin: Why is this sequence of functions not equicontinuous?,Example 7.21 and Definition 7.22 in Baby Rudin: Why is this sequence of functions not equicontinuous?,,"Here is Definition 7.19 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\left\{ f_n \right\}$ be a sequence of functions defined on a set $E$. We say that $\left\{ f_n \right\}$ is pointwise bounded on $E$ if the sequence $\left\{ f_n (x) \right\}$ is bounded for every $x \in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that    $$ \left\lvert f_n(x) \right\rvert < \phi(x) \qquad ( x \in E, n = 1, 2, 3, \ldots). $$ We say that $\left\{ f_n \right\}$ is uniformly bounded on $E$ if there exists a number $M$ such that    $$ \left\lvert f_n(x) \right\rvert < M \qquad (x \in E, n = 1, 2, 3, \ldots). $$ Here is Example 7.21 in Baby Rudin: Let    $$ f_n (x) = \frac{x^2}{x^2+ (1-nx)^2} \qquad (0 \leq x \leq 1, n = 1, 2, 3, \ldots). $$   Then $\left\lvert f_n(x) \right\rvert \leq 1$, so that $\left\{ f_n \right\}$ is uniformly bounded on $[0, 1]$. Also    $$ \lim_{n \to \infty} f_n(x) = 0 \qquad (0 \leq x \leq 1), $$   but    $$ f_n \left( \frac{1}{n} \right) = 1 \qquad (n = 1, 2, 3, \ldots), $$   so that no subsequence can converge uniformly on $[0, 1]$. I think I understand the assertions in this Example. Here is Definition 7.22: A family $\mathscr{F}$ of complex functions $f$ defined on a set $E$ in a metric space $X$ is said to be equicontinuous on $E$ if for every $\varepsilon > 0$ there exists a $\delta > 0$ such that    $$ \lvert f(x) - f(y) \rvert < \varepsilon $$   whenever $d(x, y) < \delta$, $x \in E$, $y \in E$, and $f \in \mathscr{F}$. Here $d$ denotes the metric of $X$. Now Rudin states the following: It is clear that every member of an equicontinuous family is uniformly continuous. [This is clear to me.] The sequence of Example 7.21 is not equicontinuous. [Why? This is what is not clear to me, although I do understand that this is a sequence of uniformly continuous functions.]","Here is Definition 7.19 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\left\{ f_n \right\}$ be a sequence of functions defined on a set $E$. We say that $\left\{ f_n \right\}$ is pointwise bounded on $E$ if the sequence $\left\{ f_n (x) \right\}$ is bounded for every $x \in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that    $$ \left\lvert f_n(x) \right\rvert < \phi(x) \qquad ( x \in E, n = 1, 2, 3, \ldots). $$ We say that $\left\{ f_n \right\}$ is uniformly bounded on $E$ if there exists a number $M$ such that    $$ \left\lvert f_n(x) \right\rvert < M \qquad (x \in E, n = 1, 2, 3, \ldots). $$ Here is Example 7.21 in Baby Rudin: Let    $$ f_n (x) = \frac{x^2}{x^2+ (1-nx)^2} \qquad (0 \leq x \leq 1, n = 1, 2, 3, \ldots). $$   Then $\left\lvert f_n(x) \right\rvert \leq 1$, so that $\left\{ f_n \right\}$ is uniformly bounded on $[0, 1]$. Also    $$ \lim_{n \to \infty} f_n(x) = 0 \qquad (0 \leq x \leq 1), $$   but    $$ f_n \left( \frac{1}{n} \right) = 1 \qquad (n = 1, 2, 3, \ldots), $$   so that no subsequence can converge uniformly on $[0, 1]$. I think I understand the assertions in this Example. Here is Definition 7.22: A family $\mathscr{F}$ of complex functions $f$ defined on a set $E$ in a metric space $X$ is said to be equicontinuous on $E$ if for every $\varepsilon > 0$ there exists a $\delta > 0$ such that    $$ \lvert f(x) - f(y) \rvert < \varepsilon $$   whenever $d(x, y) < \delta$, $x \in E$, $y \in E$, and $f \in \mathscr{F}$. Here $d$ denotes the metric of $X$. Now Rudin states the following: It is clear that every member of an equicontinuous family is uniformly continuous. [This is clear to me.] The sequence of Example 7.21 is not equicontinuous. [Why? This is what is not clear to me, although I do understand that this is a sequence of uniformly continuous functions.]",,"['real-analysis', 'sequences-and-series', 'analysis', 'uniform-continuity', 'equicontinuity']"
36,When is it possible to split an infinite sum?,When is it possible to split an infinite sum?,,"I know ordering of terms matters for conditionally convergent sums, so must the sums be absolutely convergent to be able to split like so: $\sum_{n=0}^{\infty} f_n + \frac{1}{n^2} = \sum_{n=0}^{\infty} f_n + \sum_{n=0}^{\infty}\frac{1}{n^2}$","I know ordering of terms matters for conditionally convergent sums, so must the sums be absolutely convergent to be able to split like so: $\sum_{n=0}^{\infty} f_n + \frac{1}{n^2} = \sum_{n=0}^{\infty} f_n + \sum_{n=0}^{\infty}\frac{1}{n^2}$",,"['real-analysis', 'analysis', 'summation']"
37,Is this a Taylor series? $\ln(x) +1 = \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x}$,Is this a Taylor series?,\ln(x) +1 = \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x},"Can you provide a proof of this identity using only calculus? $$\ln x + 1 = \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln x)^n}{x}$$ By the way, here is how I arrived at it: There is string of length $x$ units. Select a point on the string uniformly at random and cut the string at that point. Repeat the process with the string on the left side of the cut until the string you have is shorter than $1$ unit. The problem is to figure out the expected number of cuts. Here is how I did it: Let $E(x)$ denote the expected number of cuts to be made on a string of length $x$. If $x<1$, clearly, $E(x)=0$. If $x>1$, we have: \begin{align} E(x) &= 1 + \int_0^x E(u) \cdot \frac{du}{x} \\                    &= 1 + \frac 1x \int_1^x E(u) \ du \end{align} Multiplying by $x$ and differentiating (applying the Fundamental Theorem of Calculus), \begin{align} xE'(x) +  E(x) &= 1 + E(x) \\    \Rightarrow E(x) &= \ln x + C \end{align} Clearly for $x=1, \ E(x)=1$ thus $E(x) = \ln x + 1$ However, we can also calculate $E(x)$ in a different way: Let $P(n,\ x)$ denote the probability of exactly $n$ cuts being made on a string of length $x$. If $x<1$, $P(n,\ x) = 0$; if $n=1$ and $x>1$ $P(n,\ x)=\frac 1x$; if $n>1$ and $x>1$: \begin{align} P(n,\ x) &= \int_0^x P(n-1,\ u) \cdot \frac{du}{x} \\                        &= \frac 1x \int_1^x P(n-1,\ u) \ du \end{align} I calculated that $P(1,\ x) = \frac 1x,\ P(2,\ x) = \frac{\ln x}{x},\ P(3,\ x) = \frac{(\ln x)^2}{2x},\ P(4,\ x) = \frac{(\ln x)^3}{6x}$ This led me to hypothesize that $P(n,\ x)=\frac{(\ln x)^{n-1}}{x(n-1)!}$, which can be proven by induction: \begin{align} P(n,\ x) &= \frac 1x \int_1^x P(n-1,\ u) \ du \\                        &= \frac 1x \int_1^x \frac{(\ln u)^{n-2}}{u(n-2)!} \ du \\                     &= \frac 1{x(n-2)!} \int_1^x (\ln u)^{n-2}\ d(\ln u) \\                        &= \frac 1{x(n-2)!} \left[\frac {(\ln u)^{n-1}}{n-1}  \right]_1^x \\         &= \frac{(\ln x)^{n-1}}{x(n-1)!} \end{align} But then $E(x)$ can be written as follows: \begin{align} E(x) &= \sum_{n=1}^{\infty} n \cdot P(n,\ x)  \\                     &= \sum_{n=1}^{\infty} n \cdot \frac{(\ln x)^{n-1}}{x(n-1)!} \\                 &=  \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x}  \\ \\ \therefore \ \ln x + 1 &=  \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x} \ \blacksquare \end{align} So can you prove this result by finding an appropriate Taylor series (this would be especially appreciated) or if not using Taylor series then using just the methods of calculus?","Can you provide a proof of this identity using only calculus? $$\ln x + 1 = \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln x)^n}{x}$$ By the way, here is how I arrived at it: There is string of length $x$ units. Select a point on the string uniformly at random and cut the string at that point. Repeat the process with the string on the left side of the cut until the string you have is shorter than $1$ unit. The problem is to figure out the expected number of cuts. Here is how I did it: Let $E(x)$ denote the expected number of cuts to be made on a string of length $x$. If $x<1$, clearly, $E(x)=0$. If $x>1$, we have: \begin{align} E(x) &= 1 + \int_0^x E(u) \cdot \frac{du}{x} \\                    &= 1 + \frac 1x \int_1^x E(u) \ du \end{align} Multiplying by $x$ and differentiating (applying the Fundamental Theorem of Calculus), \begin{align} xE'(x) +  E(x) &= 1 + E(x) \\    \Rightarrow E(x) &= \ln x + C \end{align} Clearly for $x=1, \ E(x)=1$ thus $E(x) = \ln x + 1$ However, we can also calculate $E(x)$ in a different way: Let $P(n,\ x)$ denote the probability of exactly $n$ cuts being made on a string of length $x$. If $x<1$, $P(n,\ x) = 0$; if $n=1$ and $x>1$ $P(n,\ x)=\frac 1x$; if $n>1$ and $x>1$: \begin{align} P(n,\ x) &= \int_0^x P(n-1,\ u) \cdot \frac{du}{x} \\                        &= \frac 1x \int_1^x P(n-1,\ u) \ du \end{align} I calculated that $P(1,\ x) = \frac 1x,\ P(2,\ x) = \frac{\ln x}{x},\ P(3,\ x) = \frac{(\ln x)^2}{2x},\ P(4,\ x) = \frac{(\ln x)^3}{6x}$ This led me to hypothesize that $P(n,\ x)=\frac{(\ln x)^{n-1}}{x(n-1)!}$, which can be proven by induction: \begin{align} P(n,\ x) &= \frac 1x \int_1^x P(n-1,\ u) \ du \\                        &= \frac 1x \int_1^x \frac{(\ln u)^{n-2}}{u(n-2)!} \ du \\                     &= \frac 1{x(n-2)!} \int_1^x (\ln u)^{n-2}\ d(\ln u) \\                        &= \frac 1{x(n-2)!} \left[\frac {(\ln u)^{n-1}}{n-1}  \right]_1^x \\         &= \frac{(\ln x)^{n-1}}{x(n-1)!} \end{align} But then $E(x)$ can be written as follows: \begin{align} E(x) &= \sum_{n=1}^{\infty} n \cdot P(n,\ x)  \\                     &= \sum_{n=1}^{\infty} n \cdot \frac{(\ln x)^{n-1}}{x(n-1)!} \\                 &=  \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x}  \\ \\ \therefore \ \ln x + 1 &=  \sum_{n=0}^{\infty} \frac{n+1}{n!} \cdot \frac{(\ln(x))^n}{x} \ \blacksquare \end{align} So can you prove this result by finding an appropriate Taylor series (this would be especially appreciated) or if not using Taylor series then using just the methods of calculus?",,"['calculus', 'real-analysis', 'probability', 'taylor-expansion']"
38,$ \bigcap\limits_{n=1}^N I_n \neq \emptyset$ for all $ N \in \mathbb{N}$ implies that $ \bigcap\limits_{n=1}^\infty I_n \neq \emptyset $?,for all  implies that ?, \bigcap\limits_{n=1}^N I_n \neq \emptyset  N \in \mathbb{N}  \bigcap\limits_{n=1}^\infty I_n \neq \emptyset ,"How can I show that a sequence of closed bounded (Not necessarily nested) intervals $ I_1, I_2, I_3 ,\ldots$ with the property that $ \bigcap\limits_{n=1}^N I_n \neq \emptyset$  for all $ N \in \mathbb{N}$ implies that $ \bigcap\limits_{n=1}^\infty I_n \neq \emptyset $ ? I'm being asked to determine if this is true.  I think that it is, because no matter how large $N$ is, we can always find an element in  $ \bigcap\limits_{n=1}^N I_n$.  So, we could simple let $N$ grow and there will always be an element in the intersection.  I was thinking about using induction, but this doesn't seem like an induction problem.  I am new to Real Analysis (self-study).  Someone tried to explain this to me using the bolzano weierstrass theorem, but I have not learned that. Any guidance will be appreciated.","How can I show that a sequence of closed bounded (Not necessarily nested) intervals $ I_1, I_2, I_3 ,\ldots$ with the property that $ \bigcap\limits_{n=1}^N I_n \neq \emptyset$  for all $ N \in \mathbb{N}$ implies that $ \bigcap\limits_{n=1}^\infty I_n \neq \emptyset $ ? I'm being asked to determine if this is true.  I think that it is, because no matter how large $N$ is, we can always find an element in  $ \bigcap\limits_{n=1}^N I_n$.  So, we could simple let $N$ grow and there will always be an element in the intersection.  I was thinking about using induction, but this doesn't seem like an induction problem.  I am new to Real Analysis (self-study).  Someone tried to explain this to me using the bolzano weierstrass theorem, but I have not learned that. Any guidance will be appreciated.",,"['real-analysis', 'sequences-and-series']"
39,Prove the unit circle is uncountable,Prove the unit circle is uncountable,,"This is a homework problem, so avoid giving the answer. I think a discussion of my attempt at a proof would be more appropriate. The problem goes as follows: Let $S$ be the circle of unit radius in the Euclidean plane: $$S = \{ (x,y) \in \mathbb{R}^2:   x^{2} + y^{2}=1 \}$$ Prove that $S$ is uncountable. This is my attempt at a proof. I don't know if it is valid, or if my logic, and for that matter my approach to the proof, is correct. Feedback/comments/thoughts of any kind are welcome. Let $G^{+} = \{(x,y)\in G: y \geq0\}$ and $G^{-} = \{(x,y)\in G: y \leq 0 \}.$These are the upper and lower segments of the unit circle. Notice that $G^{+}\subset S$ and $ \hspace{1mm}G^{-}\subset S$, so $S=G^{+} \cup G^{-}.$ Let $f: G^{+} \rightarrow [-1,1],$ where $f(x,y)=x$. This can be thought of as a projection of the semi-circle onto the $x$-axis. Since the image of $G^{+}$ under $f$ is equal to the codomain; i.e., $f(G^{+})= [-1,1]$, then $f$ is surjective. Now since for every $(x,y) \in G^{+}$ we have the cardinality $|f(x,y)|=1$, there exists an inverse function $f^{-1}:[-1,1] \rightarrow G^{+}$ defined by $f^{-1}(x) = (x,\sqrt{1-x^{2})}$. Thus, there exists a bijection between $G^{+}$ and $[-1,1].$ Similarly we have the same argument for $G^{-}.$ Let $g:G^{-} \rightarrow [-1,1]$ then $ g(G^{-})=[-1,1]$ (surjective) $|g(x,y)|=1 \hspace{4mm}  \forall (x,y)\in G^{-}$ (one-to-one) $g^{-1}: [-1,1] \rightarrow G^{-}, \hspace{4mm} g^{-1}(x)= (x,-\sqrt{1-x^{2}})$ (inverse) which shows the bijection. Since the set of real numbers $[-1,1]$ is uncountable as can be shown by Cantor diagonalization, and we have $G^{+} {\raise.17ex\hbox{$\scriptstyle\sim$}} [-1,1]$ and  $G^{-} {\raise.17ex\hbox{$\scriptstyle\sim$}} [-1,1]$, then that implies that $G^{+}$ and $G^{-}$ are uncountable. Therefore $S= G^{+} \cup G^{-}$ must also be uncountable. q.e.d. One other approach I thought about was to think of the semicircles as intervals in their own right, where the length of the upper semicircle would be $[0,\pi]$ and the interval of the lower semicircle would be $[\pi,2\pi],$ and I suppose the metric would be the arc length. So essentially you take the arc length and straighten it out, but I didn't know how to approach it or even formalize it. However, I think it is essentially the same thing as what I did in my proof.","This is a homework problem, so avoid giving the answer. I think a discussion of my attempt at a proof would be more appropriate. The problem goes as follows: Let $S$ be the circle of unit radius in the Euclidean plane: $$S = \{ (x,y) \in \mathbb{R}^2:   x^{2} + y^{2}=1 \}$$ Prove that $S$ is uncountable. This is my attempt at a proof. I don't know if it is valid, or if my logic, and for that matter my approach to the proof, is correct. Feedback/comments/thoughts of any kind are welcome. Let $G^{+} = \{(x,y)\in G: y \geq0\}$ and $G^{-} = \{(x,y)\in G: y \leq 0 \}.$These are the upper and lower segments of the unit circle. Notice that $G^{+}\subset S$ and $ \hspace{1mm}G^{-}\subset S$, so $S=G^{+} \cup G^{-}.$ Let $f: G^{+} \rightarrow [-1,1],$ where $f(x,y)=x$. This can be thought of as a projection of the semi-circle onto the $x$-axis. Since the image of $G^{+}$ under $f$ is equal to the codomain; i.e., $f(G^{+})= [-1,1]$, then $f$ is surjective. Now since for every $(x,y) \in G^{+}$ we have the cardinality $|f(x,y)|=1$, there exists an inverse function $f^{-1}:[-1,1] \rightarrow G^{+}$ defined by $f^{-1}(x) = (x,\sqrt{1-x^{2})}$. Thus, there exists a bijection between $G^{+}$ and $[-1,1].$ Similarly we have the same argument for $G^{-}.$ Let $g:G^{-} \rightarrow [-1,1]$ then $ g(G^{-})=[-1,1]$ (surjective) $|g(x,y)|=1 \hspace{4mm}  \forall (x,y)\in G^{-}$ (one-to-one) $g^{-1}: [-1,1] \rightarrow G^{-}, \hspace{4mm} g^{-1}(x)= (x,-\sqrt{1-x^{2}})$ (inverse) which shows the bijection. Since the set of real numbers $[-1,1]$ is uncountable as can be shown by Cantor diagonalization, and we have $G^{+} {\raise.17ex\hbox{$\scriptstyle\sim$}} [-1,1]$ and  $G^{-} {\raise.17ex\hbox{$\scriptstyle\sim$}} [-1,1]$, then that implies that $G^{+}$ and $G^{-}$ are uncountable. Therefore $S= G^{+} \cup G^{-}$ must also be uncountable. q.e.d. One other approach I thought about was to think of the semicircles as intervals in their own right, where the length of the upper semicircle would be $[0,\pi]$ and the interval of the lower semicircle would be $[\pi,2\pi],$ and I suppose the metric would be the arc length. So essentially you take the arc length and straighten it out, but I didn't know how to approach it or even formalize it. However, I think it is essentially the same thing as what I did in my proof.",,['real-analysis']
40,Elliptic Integrals & Gamma Functions of Rational values.,Elliptic Integrals & Gamma Functions of Rational values.,,"I recently saw this question  ... https://math.stackexchange.com/questions/2393668/double-factorial-sum-k-0-infty-left-frac-2k-1-2k-right#2393668 ... & I am unable to show it. The result   $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \frac{10}{8} \right)}$$ Now I know several results relating to those ratios of double factorials  \begin{eqnarray*} \int_0^{\frac{\pi}{2}} \sin^{2n} \theta d \theta= \frac{ \pi}{2} \frac{(2n-1)!!}{(2n)!!} \tag{1} \\ \sum_{n=0}^{\infty} \frac{(2n-1)!!}{(2n)!!} y^n = \frac{1}{\sqrt{1-y}} \tag{2} \end{eqnarray*} From these two results it is reasonably easy to derive the series for the elliptic integral of the first kind \begin{eqnarray*} K(k)=\int_0^{\frac{\pi}{2}} \frac{d \theta}{\sqrt{1-k^2 \sin^{2}(\theta)}} =\frac{\pi}{2} \left(1+ \left(\frac{1}{2}\right)^2+ \left(\frac{1.3}{2.4}\right)^2+ \cdots \right) \end{eqnarray*} & it is well known that this can be evaluated for special values of $k$ in terms of Gamma functions whose arguements are rational values. See this question Can $\Gamma(1/5)$ be written in this form? and the reference cited there. So my first thought is to use $(1)$ three times, sum the geometric plum & we have  \begin{eqnarray*} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \frac{d \alpha d \beta d \gamma}{1+ \sin^2 \alpha \sin^2 \beta \sin^2 \gamma} \end{eqnarray*} This triple integral looks difficult so ... Second thoughts ... use $(1)$ twice & then use $(2)$ to get the double integral \begin{eqnarray*} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}}  \frac{d \alpha d \beta }{\sqrt{1+ \sin^2 \alpha \sin^2 \beta }} \end{eqnarray*} now substitute $ \sqrt{s} =\sin \alpha$ and $ \sqrt{t} =\sin \beta$ (might have lost a factor of $4$) \begin{eqnarray*} \int_{0}^{1} \int_{0}^{1}  \frac{d s d t }{\sqrt{s(1-s)t(1-t)(1+st) }} \end{eqnarray*} & I am not sure what to do with this. In both of these attempts I feel I have taken a wrong turn. Can someone either give me a Big hint or a reference to the original derivation of this result or a reasonably complete solution ? Bonus question ... why was it stated with $\frac{10}{8}$ instead of $\frac{5}{4}$ ? The result   $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \color{red}{\frac{5}{4}} \right)}$$ Hint from Jack D'Auirzio : look at equation (6) here http://mathworld.wolfram.com/CompleteEllipticIntegraloftheFirstKind.html and use $2kk'=i$ where $k'$ is the complementary modulus.","I recently saw this question  ... https://math.stackexchange.com/questions/2393668/double-factorial-sum-k-0-infty-left-frac-2k-1-2k-right#2393668 ... & I am unable to show it. The result   $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \frac{10}{8} \right)}$$ Now I know several results relating to those ratios of double factorials  \begin{eqnarray*} \int_0^{\frac{\pi}{2}} \sin^{2n} \theta d \theta= \frac{ \pi}{2} \frac{(2n-1)!!}{(2n)!!} \tag{1} \\ \sum_{n=0}^{\infty} \frac{(2n-1)!!}{(2n)!!} y^n = \frac{1}{\sqrt{1-y}} \tag{2} \end{eqnarray*} From these two results it is reasonably easy to derive the series for the elliptic integral of the first kind \begin{eqnarray*} K(k)=\int_0^{\frac{\pi}{2}} \frac{d \theta}{\sqrt{1-k^2 \sin^{2}(\theta)}} =\frac{\pi}{2} \left(1+ \left(\frac{1}{2}\right)^2+ \left(\frac{1.3}{2.4}\right)^2+ \cdots \right) \end{eqnarray*} & it is well known that this can be evaluated for special values of $k$ in terms of Gamma functions whose arguements are rational values. See this question Can $\Gamma(1/5)$ be written in this form? and the reference cited there. So my first thought is to use $(1)$ three times, sum the geometric plum & we have  \begin{eqnarray*} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \frac{d \alpha d \beta d \gamma}{1+ \sin^2 \alpha \sin^2 \beta \sin^2 \gamma} \end{eqnarray*} This triple integral looks difficult so ... Second thoughts ... use $(1)$ twice & then use $(2)$ to get the double integral \begin{eqnarray*} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}}  \frac{d \alpha d \beta }{\sqrt{1+ \sin^2 \alpha \sin^2 \beta }} \end{eqnarray*} now substitute $ \sqrt{s} =\sin \alpha$ and $ \sqrt{t} =\sin \beta$ (might have lost a factor of $4$) \begin{eqnarray*} \int_{0}^{1} \int_{0}^{1}  \frac{d s d t }{\sqrt{s(1-s)t(1-t)(1+st) }} \end{eqnarray*} & I am not sure what to do with this. In both of these attempts I feel I have taken a wrong turn. Can someone either give me a Big hint or a reference to the original derivation of this result or a reasonably complete solution ? Bonus question ... why was it stated with $\frac{10}{8}$ instead of $\frac{5}{4}$ ? The result   $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \color{red}{\frac{5}{4}} \right)}$$ Hint from Jack D'Auirzio : look at equation (6) here http://mathworld.wolfram.com/CompleteEllipticIntegraloftheFirstKind.html and use $2kk'=i$ where $k'$ is the complementary modulus.",,"['real-analysis', 'integration', 'sequences-and-series', 'multivariable-calculus', 'elliptic-integrals']"
41,Prove that $\lim\limits_{n\to\infty} \frac{S_n - s}{S_n+s} = 0$ implies $\lim\limits_{n \rightarrow \infty} S_n = s$,Prove that  implies,\lim\limits_{n\to\infty} \frac{S_n - s}{S_n+s} = 0 \lim\limits_{n \rightarrow \infty} S_n = s,"Prove that if    $$ \lim\limits_{n \rightarrow \infty} \frac{S_n-s}{S_n+s} = 0$$ then $$\lim\limits_{n \rightarrow \infty} S_n = s$$ Hint: Define $t_n = \frac{S_n -s}{S_n + s}$ and solve for $S_n$ By the hint: $$t_n = \frac{S_n -s}{S_n + s}$$ $$(S_n + s)t_n = S_n -s$$ $$S_n(t_n-1)= - s -st_n$$ $$S_n= -s \cdot \frac{1+t_n}{t_n-1}$$ $$\lim\limits_{n \rightarrow \infty} S_n= -s \cdot \frac{1+\lim\limits_{n \rightarrow \infty} t_n}{\lim\limits_{n \rightarrow \infty} t_n-1}$$ As $ \lim\limits_{n \rightarrow \infty} \frac{S_n-s}{S_n+s} = \lim\limits_{n \rightarrow \infty} t_n = 0$, it follows: $$\lim\limits_{n \rightarrow \infty} S_n= s$$ Is my argumentation correct/appropriate?Anything needs to be added? Much appreciated for your input.","Prove that if    $$ \lim\limits_{n \rightarrow \infty} \frac{S_n-s}{S_n+s} = 0$$ then $$\lim\limits_{n \rightarrow \infty} S_n = s$$ Hint: Define $t_n = \frac{S_n -s}{S_n + s}$ and solve for $S_n$ By the hint: $$t_n = \frac{S_n -s}{S_n + s}$$ $$(S_n + s)t_n = S_n -s$$ $$S_n(t_n-1)= - s -st_n$$ $$S_n= -s \cdot \frac{1+t_n}{t_n-1}$$ $$\lim\limits_{n \rightarrow \infty} S_n= -s \cdot \frac{1+\lim\limits_{n \rightarrow \infty} t_n}{\lim\limits_{n \rightarrow \infty} t_n-1}$$ As $ \lim\limits_{n \rightarrow \infty} \frac{S_n-s}{S_n+s} = \lim\limits_{n \rightarrow \infty} t_n = 0$, it follows: $$\lim\limits_{n \rightarrow \infty} S_n= s$$ Is my argumentation correct/appropriate?Anything needs to be added? Much appreciated for your input.",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
42,$f\in L^{p}$ iff $\sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty$,iff,f\in L^{p} \sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty,"Let $p\in [1, \infty)$, prove that if $f \in L^{p}(\mu)$ iff $$\sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty $$ I tried the following ways but couldn't get any result: If $f\in L^{p}$  I considered the following sets: $$E_{n}= \{\ x : |f(x)|^{p} > 2^{np} \}\ =\{\ x : |f(x)| > 2^{n} \}\ $$ From that I derived that $$\sum_{n=1}^{\infty} 2^{np} \chi_{E_{n}}(x) \leq |f(x)|^{p} $$ (I hope that this inequality is correct). So then, if $f\in L^{p}$, then $\int \sum_{n=1}^{\infty}2^{np} \chi_{E_{n}} < \infty \implies \sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty $. I am stuck on the converse though! Thanks in advance for any kind of help!","Let $p\in [1, \infty)$, prove that if $f \in L^{p}(\mu)$ iff $$\sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty $$ I tried the following ways but couldn't get any result: If $f\in L^{p}$  I considered the following sets: $$E_{n}= \{\ x : |f(x)|^{p} > 2^{np} \}\ =\{\ x : |f(x)| > 2^{n} \}\ $$ From that I derived that $$\sum_{n=1}^{\infty} 2^{np} \chi_{E_{n}}(x) \leq |f(x)|^{p} $$ (I hope that this inequality is correct). So then, if $f\in L^{p}$, then $\int \sum_{n=1}^{\infty}2^{np} \chi_{E_{n}} < \infty \implies \sum_{n=1}^{\infty} 2^{np}(\mu(\{\ x : |f(x)| > 2^{n} \}\ )) < \infty $. I am stuck on the converse though! Thanks in advance for any kind of help!",,"['real-analysis', 'integration', 'measure-theory', 'lp-spaces']"
43,"Show that $(X,|||\cdot|||)$ is a Banach space.",Show that  is a Banach space.,"(X,|||\cdot|||)","In the book ' Classical Banach Spaces I and II ' by Lindenstrauss, page $1,$ Chapter $1$, he stated the following: Let $(X,\|\cdot\|)$ be a Banach space with a (Schauder) basis $(x_n)_{n=1}^{\infty}.$ For every $x = \sum_{n=1}^{\infty}a_nx_n$ in $X$ the expression $$|||x||| = \sup_{n}\left\Vert \sum_{i=1}^n a_ix_I \right\Vert$$    is finite. Evidently, $|||\cdot|||$ is a norm on $X$ and $\|x\| \leq |||x|||$ for every $x \in X.$ A simple argument shows that $X$ is complete also with respect to $|||\cdot|||.$ Question: How to use a simple argument to show that $(X, |||\cdot|||)$ is complete? I manage to prove all statements before it. Below is my partial attempt (actually is not even partial). Suppose that $(x_n)_{n=1}^{\infty}$ is a Cauchy sequence in $(X, |||\cdot|||).$ Since $\|\cdot\| \leq |||\cdot|||$ and $(X,\|\cdot\|)$ is a Banach space, there exists an $x \in X$ such that $\lim_{n}\|x_n - x\| = 0.$ I think I should I show that $x_n$ converges to $x$ in the norm $|||\cdot|||.$ But I have no idea how to show at all. Any hint would be appreciated.","In the book ' Classical Banach Spaces I and II ' by Lindenstrauss, page $1,$ Chapter $1$, he stated the following: Let $(X,\|\cdot\|)$ be a Banach space with a (Schauder) basis $(x_n)_{n=1}^{\infty}.$ For every $x = \sum_{n=1}^{\infty}a_nx_n$ in $X$ the expression $$|||x||| = \sup_{n}\left\Vert \sum_{i=1}^n a_ix_I \right\Vert$$    is finite. Evidently, $|||\cdot|||$ is a norm on $X$ and $\|x\| \leq |||x|||$ for every $x \in X.$ A simple argument shows that $X$ is complete also with respect to $|||\cdot|||.$ Question: How to use a simple argument to show that $(X, |||\cdot|||)$ is complete? I manage to prove all statements before it. Below is my partial attempt (actually is not even partial). Suppose that $(x_n)_{n=1}^{\infty}$ is a Cauchy sequence in $(X, |||\cdot|||).$ Since $\|\cdot\| \leq |||\cdot|||$ and $(X,\|\cdot\|)$ is a Banach space, there exists an $x \in X$ such that $\lim_{n}\|x_n - x\| = 0.$ I think I should I show that $x_n$ converges to $x$ in the norm $|||\cdot|||.$ But I have no idea how to show at all. Any hint would be appreciated.",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'complete-spaces', 'schauder-basis']"
44,does a convex polynomial always reach its minimum value?,does a convex polynomial always reach its minimum value?,,"Consider a convex polynomial $p$ such that $p(x_1,~x_2,\dots x_n)\geq 0~\forall x_1,~x_2,\dots x_n\in \mathbb{R}^n$. Does the polynomial reach its minimum value? This is not true for non-convex polynomials like $(1-x_1x_2)^2+x_1^2$, see the response of J.P. McCarthy on a similar question on general polynomials . This is not true for general functions either. Function $e^x$ is infinitely differentiable, convex and bounded from below, but there is no $x$ that does reach the minimum value 0 (thanks to zhw for this simpler example).","Consider a convex polynomial $p$ such that $p(x_1,~x_2,\dots x_n)\geq 0~\forall x_1,~x_2,\dots x_n\in \mathbb{R}^n$. Does the polynomial reach its minimum value? This is not true for non-convex polynomials like $(1-x_1x_2)^2+x_1^2$, see the response of J.P. McCarthy on a similar question on general polynomials . This is not true for general functions either. Function $e^x$ is infinitely differentiable, convex and bounded from below, but there is no $x$ that does reach the minimum value 0 (thanks to zhw for this simpler example).",,['real-analysis']
45,Bivariate function monotone in each variable $\Rightarrow$ continuous a.e.?,Bivariate function monotone in each variable  continuous a.e.?,\Rightarrow,"It is well known that, if a univariate function $f \colon [0,1] \rightarrow \mathbb{R}$ is monotone, then it has at most countably many discontinuity points. I was wondering to which degree this result extends to multivariate functions. In particular, let $g: [0,1]^2 \rightarrow \mathbb{R}$ be a bivariate function that is monotone in each variable, that is, $g(x,\cdot)$ and $g(\cdot,y)$ are both monotone for all $x,y \in [0,1]$. I know that $g$ may have more than countably many discontinuity points (see, for instance, Number of Discontinuities of a Monotone function of several variables ). However, is it true that the set of discontinuity points of $g$ has Lebesgue measure zero?","It is well known that, if a univariate function $f \colon [0,1] \rightarrow \mathbb{R}$ is monotone, then it has at most countably many discontinuity points. I was wondering to which degree this result extends to multivariate functions. In particular, let $g: [0,1]^2 \rightarrow \mathbb{R}$ be a bivariate function that is monotone in each variable, that is, $g(x,\cdot)$ and $g(\cdot,y)$ are both monotone for all $x,y \in [0,1]$. I know that $g$ may have more than countably many discontinuity points (see, for instance, Number of Discontinuities of a Monotone function of several variables ). However, is it true that the set of discontinuity points of $g$ has Lebesgue measure zero?",,"['real-analysis', 'continuity', 'monotone-functions']"
46,Two definitions for derivative,Two definitions for derivative,,"This may be a very stupid question and could be blatantly obvious, but I want to clear the confusion that I have about it. There are two equivalent definitions of the derivative: Let $g: A \rightarrow \mathbf{R}$ be a function defined on an interval    $A$. Given $c \in A$, the derivative of $g$ at $c$ is defined by    $$g'(c) = \lim_{x \rightarrow c} \frac{g(x) - g(c)}{x-c}$$ provided    this limit exists. and Let $g: A \rightarrow \mathbf{R}$ be a function defined on an interval    $A$. Given $c \in A$, the derivative of $g$ at $c$ is defined by    $$g'(c) = \lim_{h \rightarrow 0} \frac{g(c+h) - g(c)}{h}$$ provided    this limit exists. My question is, what's the formal reasoning why these two definitions are equivalent? For example, are we using the Algebraic Limit Theorem for functional limits? It is very clear to me ""intuitively"" why they are equivalently, i.e., simply let $x = c+h$ and one can see that in the first definition, as $x$ tends towards $c$ we ""get"" the expression $\frac{g(c)-g(c)}{c-c}$ while for the second definition, as $h$ tends towards $0$ we ""get"" the same expression $\frac{g(c)-g(c)}{c-c}$. But such reasoning is certainly not very rigorous and is very primitive, I wish to know why they are equivalent using formally justified reasons for each step in the process. For example, why can one substitute $x = c+h$ into the first definition and why after the substitution does the limiting variable change from $x$ to $h$? EDIT: To be more precise, let $\phi(x)=\frac{g(x)-g(c)}{x-c}$ and let $\gamma(h) = \frac{g(c+h)-g(c)}{h}$, how can I prove that $\lim_{x \rightarrow c} \phi(x) = \lim_{h \rightarrow 0} \gamma(h)$ with the substitution $x = c+h$?","This may be a very stupid question and could be blatantly obvious, but I want to clear the confusion that I have about it. There are two equivalent definitions of the derivative: Let $g: A \rightarrow \mathbf{R}$ be a function defined on an interval    $A$. Given $c \in A$, the derivative of $g$ at $c$ is defined by    $$g'(c) = \lim_{x \rightarrow c} \frac{g(x) - g(c)}{x-c}$$ provided    this limit exists. and Let $g: A \rightarrow \mathbf{R}$ be a function defined on an interval    $A$. Given $c \in A$, the derivative of $g$ at $c$ is defined by    $$g'(c) = \lim_{h \rightarrow 0} \frac{g(c+h) - g(c)}{h}$$ provided    this limit exists. My question is, what's the formal reasoning why these two definitions are equivalent? For example, are we using the Algebraic Limit Theorem for functional limits? It is very clear to me ""intuitively"" why they are equivalently, i.e., simply let $x = c+h$ and one can see that in the first definition, as $x$ tends towards $c$ we ""get"" the expression $\frac{g(c)-g(c)}{c-c}$ while for the second definition, as $h$ tends towards $0$ we ""get"" the same expression $\frac{g(c)-g(c)}{c-c}$. But such reasoning is certainly not very rigorous and is very primitive, I wish to know why they are equivalent using formally justified reasons for each step in the process. For example, why can one substitute $x = c+h$ into the first definition and why after the substitution does the limiting variable change from $x$ to $h$? EDIT: To be more precise, let $\phi(x)=\frac{g(x)-g(c)}{x-c}$ and let $\gamma(h) = \frac{g(c+h)-g(c)}{h}$, how can I prove that $\lim_{x \rightarrow c} \phi(x) = \lim_{h \rightarrow 0} \gamma(h)$ with the substitution $x = c+h$?",,"['real-analysis', 'limits', 'derivatives', 'definition']"
47,Are all additionlike operations on $\mathbb{R}$ of this form?,Are all additionlike operations on  of this form?,\mathbb{R},"Suppose we're given real numbers $x,x',y,y' \in \mathbb{R}$ such that $$|x-x'| \leq a, \qquad |y-y'| \leq b.$$ Then $$|(x+y)-(x'+y')| = |(x-x')+(y-y')| \leq |x-x'| + |y-y'| \leq a+b$$ This tells us that if we know a real number $x$ to a precision of $a$, and if we know $y$ to a precision of $b$, then we know $x+y$ to a precision of $a+b$. Interestingly, the above argument only needs the following two properties of addition: Axiom 0. $(x\star y) - (x' \star y') = (x-x') \star (y-y')$ Axiom 1. $|x \star y| \leq |x| \star |y|$. Call a binary operation on $\mathbb{R}$ additionlike iff it satisfies these two axioms. It's easy to see that the operation defined by $x \star y = k(x+y)$ is additionlike, for all $k \geq 0$. Question. Are all additionlike operations on $\mathbb{R}$ of this form?","Suppose we're given real numbers $x,x',y,y' \in \mathbb{R}$ such that $$|x-x'| \leq a, \qquad |y-y'| \leq b.$$ Then $$|(x+y)-(x'+y')| = |(x-x')+(y-y')| \leq |x-x'| + |y-y'| \leq a+b$$ This tells us that if we know a real number $x$ to a precision of $a$, and if we know $y$ to a precision of $b$, then we know $x+y$ to a precision of $a+b$. Interestingly, the above argument only needs the following two properties of addition: Axiom 0. $(x\star y) - (x' \star y') = (x-x') \star (y-y')$ Axiom 1. $|x \star y| \leq |x| \star |y|$. Call a binary operation on $\mathbb{R}$ additionlike iff it satisfies these two axioms. It's easy to see that the operation defined by $x \star y = k(x+y)$ is additionlike, for all $k \geq 0$. Question. Are all additionlike operations on $\mathbb{R}$ of this form?",,"['real-analysis', 'abstract-algebra', 'binary-operations', 'magma']"
48,Modification of Dini's theorem,Modification of Dini's theorem,,"Classical Dini's theorem states that if $(f_n)$ is a monotone sequence of continuous functions on a compact space converges pointwise to a continuous function $f$, then the convergence is uniform. It is shown that all conditions are needed for the conclusion to hold. If one examines carefully, the proof can be modified to obtain the following statement: If $(f_n)$ is a monotone sequence of upper semicontinuous functions converges on a compact space converges pointwise to a lower semicontinuous function $f$, then the convergence us uniform. Question: Suppose for each natural number, $f_n = g_n - h_n$ where $g_n,h_n$ are upper semicontinuous functions. If $(f_n)$ is a decreasing sequence of functions defined above on a compact space converges pointwise to $0$, then is it true that the convergence us uniform?","Classical Dini's theorem states that if $(f_n)$ is a monotone sequence of continuous functions on a compact space converges pointwise to a continuous function $f$, then the convergence is uniform. It is shown that all conditions are needed for the conclusion to hold. If one examines carefully, the proof can be modified to obtain the following statement: If $(f_n)$ is a monotone sequence of upper semicontinuous functions converges on a compact space converges pointwise to a lower semicontinuous function $f$, then the convergence us uniform. Question: Suppose for each natural number, $f_n = g_n - h_n$ where $g_n,h_n$ are upper semicontinuous functions. If $(f_n)$ is a decreasing sequence of functions defined above on a compact space converges pointwise to $0$, then is it true that the convergence us uniform?",,['real-analysis']
49,"Show that $\mu(A) = \sup \{ \mu(K) : K \subset A, K \text{ compact}\}$ is a measure",Show that  is a measure,"\mu(A) = \sup \{ \mu(K) : K \subset A, K \text{ compact}\}","Problem: Let $\mu : \mathcal{B}(\mathbb{R}^k) \rightarrow \mathbb{R}$ be a nonnegative and finitely additive set function with $\mu(\mathbb{R}^k) < \infty $. Suppose that  \begin{equation} \mu(A) = \sup \{ \mu(K) : K \subset A, K \text{ compact}\}  \end{equation} for each $A \in \mathcal{B}(\mathbb{R}^k)$. Then $\mu$ is a finite measure. I'm stuck showing that $\mu$ is countably additive. First, let $A_{i}$ disjoint, and $K_i \subset A_i$ compact such that $\mu(A_i) \leq \mu(K_i)+\epsilon/2^i$ for $\epsilon > 0$.  Then \begin{equation} \mu\left(\bigcup_{i=1}^{\infty} A_i \right) \geq \mu\left(\bigcup_{i=1}^{n} A_i \right) \geq \mu\left(\bigcup_{i=1}^{n} K_i \right) = \sum_{i=1}^n \mu(K_i) \geq \sum_{i=1}^n \mu(A_i) + \frac{\epsilon}{2^i} , \end{equation} which follows from monotonicity (obvious) and finite additivity (on the class of compact sets). Hence, letting $n \rightarrow \infty$, and since $\epsilon$ was arbitrary, \begin{equation} \mu\left(\bigcup_{i=1}^{\infty} A_i \right) \geq \sum_{i=1}^\infty \mu(A_i) . \end{equation} Next, I want to establish the reverse inequality. This is where I'm stuck. Any ideas? Thanks in advance! Chris","Problem: Let $\mu : \mathcal{B}(\mathbb{R}^k) \rightarrow \mathbb{R}$ be a nonnegative and finitely additive set function with $\mu(\mathbb{R}^k) < \infty $. Suppose that  \begin{equation} \mu(A) = \sup \{ \mu(K) : K \subset A, K \text{ compact}\}  \end{equation} for each $A \in \mathcal{B}(\mathbb{R}^k)$. Then $\mu$ is a finite measure. I'm stuck showing that $\mu$ is countably additive. First, let $A_{i}$ disjoint, and $K_i \subset A_i$ compact such that $\mu(A_i) \leq \mu(K_i)+\epsilon/2^i$ for $\epsilon > 0$.  Then \begin{equation} \mu\left(\bigcup_{i=1}^{\infty} A_i \right) \geq \mu\left(\bigcup_{i=1}^{n} A_i \right) \geq \mu\left(\bigcup_{i=1}^{n} K_i \right) = \sum_{i=1}^n \mu(K_i) \geq \sum_{i=1}^n \mu(A_i) + \frac{\epsilon}{2^i} , \end{equation} which follows from monotonicity (obvious) and finite additivity (on the class of compact sets). Hence, letting $n \rightarrow \infty$, and since $\epsilon$ was arbitrary, \begin{equation} \mu\left(\bigcup_{i=1}^{\infty} A_i \right) \geq \sum_{i=1}^\infty \mu(A_i) . \end{equation} Next, I want to establish the reverse inequality. This is where I'm stuck. Any ideas? Thanks in advance! Chris",,"['real-analysis', 'measure-theory']"
50,Prove that $f$ must have an inflection point at $0$,Prove that  must have an inflection point at,f 0,"Suppose $f$ is a function such that $f'(x)>0$ $\forall x\not=0$ and $f'(0)=0$. Also, $f''$ is a continuous $\text{one to one}$ function on some open interval containing $0$. Prove that $f$ must have an inflection point at $0$. Pf: Let a,b be arbitrary values in the open interval containing $0$ such that $a<0<b$ Since $f''$ is a continuous $1-1$ on [a,b], then either $f'''>0$ $\forall x \in [a,b]$ or $f'''<0$ $\forall x \in [a,b]$. So, -$f'' $ is continuous on $[a,b]$ and $f''$ is differentiable on $(a,b)$ By the mean value theorem, there exists $c$ with, $a<c<b$ such that $$f''(c)=\frac{f'(b)-f'(a)}{b-a}$$ Assuming I am even going the right direction with this, I do not know how to show that $f''(0)=0$ and that $f''$ changes sign at $0$. Most of what I am reading speaks in regards to a $\delta$ neighborhood of $0$ where I may be able to prove a sign change. Any helpful hints or guidance is greatly appreciated. Preferably more leaning towards hints and NOT full blown spoilers on this question. :) EDIT: $f''$ is not known to be differentiable, $f'$ is known to be differentiable and we are applying the mean value theorem to it.","Suppose $f$ is a function such that $f'(x)>0$ $\forall x\not=0$ and $f'(0)=0$. Also, $f''$ is a continuous $\text{one to one}$ function on some open interval containing $0$. Prove that $f$ must have an inflection point at $0$. Pf: Let a,b be arbitrary values in the open interval containing $0$ such that $a<0<b$ Since $f''$ is a continuous $1-1$ on [a,b], then either $f'''>0$ $\forall x \in [a,b]$ or $f'''<0$ $\forall x \in [a,b]$. So, -$f'' $ is continuous on $[a,b]$ and $f''$ is differentiable on $(a,b)$ By the mean value theorem, there exists $c$ with, $a<c<b$ such that $$f''(c)=\frac{f'(b)-f'(a)}{b-a}$$ Assuming I am even going the right direction with this, I do not know how to show that $f''(0)=0$ and that $f''$ changes sign at $0$. Most of what I am reading speaks in regards to a $\delta$ neighborhood of $0$ where I may be able to prove a sign change. Any helpful hints or guidance is greatly appreciated. Preferably more leaning towards hints and NOT full blown spoilers on this question. :) EDIT: $f''$ is not known to be differentiable, $f'$ is known to be differentiable and we are applying the mean value theorem to it.",,"['calculus', 'real-analysis', 'derivatives', 'proof-writing', 'proof-explanation']"
51,"Prove that every function f continuous on $\mathbb{R}$ can be written as $f=E+O$, where $E$ is even and continuous and $O$ is odd and continuous.","Prove that every function f continuous on  can be written as , where  is even and continuous and  is odd and continuous.",\mathbb{R} f=E+O E O,"I have been asked to prove the following in my Real Analysis class: Prove that every function $f$ continuous on $\mathbb{R}$ can be written as $f=E+O$, where $E$ is even and continuous and $O$ is odd and continuous. I am not quite sure where to begin. Thanks!","I have been asked to prove the following in my Real Analysis class: Prove that every function $f$ continuous on $\mathbb{R}$ can be written as $f=E+O$, where $E$ is even and continuous and $O$ is odd and continuous. I am not quite sure where to begin. Thanks!",,"['real-analysis', 'continuity', 'proof-writing']"
52,$\lim\limits_{n \to \infty} \frac{a_n}{b_n}=l$ ; Prove $\sum\limits_{k=1}^\infty a_k$ converges iff $\sum\limits_{k=1}^\infty b_k$ converges.,; Prove  converges iff  converges.,\lim\limits_{n \to \infty} \frac{a_n}{b_n}=l \sum\limits_{k=1}^\infty a_k \sum\limits_{k=1}^\infty b_k,"Written, in Ex.8 Ch.9.1 of the book Advanced Calculus by P. M. Fitzpatrick : Suppose that $\sum\limits_{k=1}^\infty a_k$ and $\sum\limits_{k=1}^\infty b_k$ are series of positive numbers such that $$\lim_{n \to \infty} \frac{a_n}{b_n}=l \ \ \ \text{and} \ l>0.$$ Prove that the series $\sum\limits_{k=1}^\infty a_k$ converges iff the series $\sum\limits_{k=1}^\infty b_k$ converges. Am I correct by the following (sketch of) proof? : 1- For a given $\epsilon_1$ there is $N_1$ such that $\left|\frac{a_n}{b_n} - l\right| < \epsilon_1$ for all $n \ge N_1$. 2- Since the series $\sum\limits_{k=1}^\infty a_k$ converges, for a given $\epsilon_2$ there is $N_2$ such that $\left|b_{n+1}+\dots+b_{n+k}\right|< \epsilon_1$ for all $n \ge N_2$ any for all natural numbers $k$. 3- Define $N = \max {\{N_1,N_2}\}$. 4- From Step (1), $a_{n+k} < (\epsilon_1+l) b_{n+k}$ for all $n \ge N$ any for all natural numbers $k$. [Also, $a_i$'s and $b_i$'s are all positive]. Thus $a_{n+1}+\dots+a_{n+k} < (\epsilon_1+l) (b_{n+1}+\dots+b_{n+k})< \epsilon_3$. Then the convergence of the series $\sum\limits_{k=1}^\infty b_k$ implies the convergence of the series $\sum\limits_{k=1}^\infty a_k$. 5- For the reverse implication, we use the fact that $\lim\limits_{n \to \infty}\frac{a_n}{b_n}=l \iff \lim\limits_{n \to \infty}\frac{b_n}{a_n}= \frac{1}{l}= l' >0$ and repeat the process this time for a and b exchanged. 6- The Quotient Property For Sequences hold for a nonzero limit in the denominator, but since the limit in the numerator also is zero so we may use the The Quotient Property For Sequences. Thanks.","Written, in Ex.8 Ch.9.1 of the book Advanced Calculus by P. M. Fitzpatrick : Suppose that $\sum\limits_{k=1}^\infty a_k$ and $\sum\limits_{k=1}^\infty b_k$ are series of positive numbers such that $$\lim_{n \to \infty} \frac{a_n}{b_n}=l \ \ \ \text{and} \ l>0.$$ Prove that the series $\sum\limits_{k=1}^\infty a_k$ converges iff the series $\sum\limits_{k=1}^\infty b_k$ converges. Am I correct by the following (sketch of) proof? : 1- For a given $\epsilon_1$ there is $N_1$ such that $\left|\frac{a_n}{b_n} - l\right| < \epsilon_1$ for all $n \ge N_1$. 2- Since the series $\sum\limits_{k=1}^\infty a_k$ converges, for a given $\epsilon_2$ there is $N_2$ such that $\left|b_{n+1}+\dots+b_{n+k}\right|< \epsilon_1$ for all $n \ge N_2$ any for all natural numbers $k$. 3- Define $N = \max {\{N_1,N_2}\}$. 4- From Step (1), $a_{n+k} < (\epsilon_1+l) b_{n+k}$ for all $n \ge N$ any for all natural numbers $k$. [Also, $a_i$'s and $b_i$'s are all positive]. Thus $a_{n+1}+\dots+a_{n+k} < (\epsilon_1+l) (b_{n+1}+\dots+b_{n+k})< \epsilon_3$. Then the convergence of the series $\sum\limits_{k=1}^\infty b_k$ implies the convergence of the series $\sum\limits_{k=1}^\infty a_k$. 5- For the reverse implication, we use the fact that $\lim\limits_{n \to \infty}\frac{a_n}{b_n}=l \iff \lim\limits_{n \to \infty}\frac{b_n}{a_n}= \frac{1}{l}= l' >0$ and repeat the process this time for a and b exchanged. 6- The Quotient Property For Sequences hold for a nonzero limit in the denominator, but since the limit in the numerator also is zero so we may use the The Quotient Property For Sequences. Thanks.",,"['real-analysis', 'sequences-and-series']"
53,"Let f be a function twice differentiable and with derivatives continuous on an interval $[a,b]$ containing $0$. Prove the following statement:",Let f be a function twice differentiable and with derivatives continuous on an interval  containing . Prove the following statement:,"[a,b] 0","$$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$ I have been thinking about this for a bit of time now, but I'm not getting anything. What I have done: write $f'(x)$ as it's defined: $$f'(x) = \lim_{h\to x}\frac{f(h) - f(x)}{h-x}$$ Now we can write:  $$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0}  \frac{\lim_{h\to x}\frac{f(h) - f(x)}{h-x} - \frac{f(x)-f(0)}{x}}{x}  $$ I have tried lots of algebraic manipulation after this, but nothing has come out so far. Could someone steer me in the right direction? EDIT: I just opened Rudin, which has a similar question that indicates that l'Hopital's should be used. Indeed: $$\lim_{x\to0} \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2}$$ We can apply l'Hôpital: $$ \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2} = \lim_{x\to0}\frac{f'(x) + xf''(x) - f'(x)}{2x} = \frac{f''(0)}{2}$$ EDIT #2 Using Taylor's theorem, as suggested in the comments. We choose to perform the Taylor expansion of $f(0)$ at $x_0 = x $. Then: $\exists c $ between $x$ and $0$ such that: $$f(0) = f(x) - xf'(x) + \frac{x^2f''(c)}{2} \implies \frac{f''(c)}{2} = \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x}   $$ As $x\to 0$, $c\to 0 $ and we have $$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$","$$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$ I have been thinking about this for a bit of time now, but I'm not getting anything. What I have done: write $f'(x)$ as it's defined: $$f'(x) = \lim_{h\to x}\frac{f(h) - f(x)}{h-x}$$ Now we can write:  $$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0}  \frac{\lim_{h\to x}\frac{f(h) - f(x)}{h-x} - \frac{f(x)-f(0)}{x}}{x}  $$ I have tried lots of algebraic manipulation after this, but nothing has come out so far. Could someone steer me in the right direction? EDIT: I just opened Rudin, which has a similar question that indicates that l'Hopital's should be used. Indeed: $$\lim_{x\to0} \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2}$$ We can apply l'Hôpital: $$ \lim_{x\to0} \frac{xf'(x) - f(x)-f(0)}{x^2} = \lim_{x\to0}\frac{f'(x) + xf''(x) - f'(x)}{2x} = \frac{f''(0)}{2}$$ EDIT #2 Using Taylor's theorem, as suggested in the comments. We choose to perform the Taylor expansion of $f(0)$ at $x_0 = x $. Then: $\exists c $ between $x$ and $0$ such that: $$f(0) = f(x) - xf'(x) + \frac{x^2f''(c)}{2} \implies \frac{f''(c)}{2} = \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x}   $$ As $x\to 0$, $c\to 0 $ and we have $$\lim_{x\to0}  \frac{f'(x) - \frac{f(x)-f(0)}{x}}{x} = \frac{f''(0)}{2}$$",,"['real-analysis', 'calculus']"
54,Question about proof of Implicit Function Theorem in *Analysis on Manifolds* by Munkres,Question about proof of Implicit Function Theorem in *Analysis on Manifolds* by Munkres,,"I am reading Analysis on Manifolds by Munkres, and have a question about the proof about the Implicit Function Theorem (both the statement and proof included below): Note (3rd paragraph of the proof) how Munkres chooses $U \times V$ as a neighborhood of $(a,b) \in \mathbb{R}^{k+n}$ .  I know this can be done by restricting the open set guaranteed to exist by the Inverse Function Theorem, but I don't see why we want it to be a Cartesian product. Regarding uniqueness (last paragraph of the proof), why is the argument provided necessary?  It seems unnecessarily complicated.  Here is how I reasoned it: say $(x,g(x)) \in U \times V$ s.t. $f(x,g(x))= \textbf{0}_n$ .  Then $F(x,g(x))=(x,\textbf{0}_n)$ , so $$(x,g(x))=G(x,\textbf{0}_n)=(x,h(x,\textbf{0}_n)).$$ (Here $G=F^{-1}$ and $h$ is the last $n$ coordinate functions of $G$ , following Munkres' notation).  By inspection, $g(x)=h(x,\textbf{0}_n)$ , hence uniqueness is shown because we just derived what $g(x)$ has to be. Many thanks in advance.","I am reading Analysis on Manifolds by Munkres, and have a question about the proof about the Implicit Function Theorem (both the statement and proof included below): Note (3rd paragraph of the proof) how Munkres chooses as a neighborhood of .  I know this can be done by restricting the open set guaranteed to exist by the Inverse Function Theorem, but I don't see why we want it to be a Cartesian product. Regarding uniqueness (last paragraph of the proof), why is the argument provided necessary?  It seems unnecessarily complicated.  Here is how I reasoned it: say s.t. .  Then , so (Here and is the last coordinate functions of , following Munkres' notation).  By inspection, , hence uniqueness is shown because we just derived what has to be. Many thanks in advance.","U \times V (a,b) \in \mathbb{R}^{k+n} (x,g(x)) \in U \times V f(x,g(x))= \textbf{0}_n F(x,g(x))=(x,\textbf{0}_n) (x,g(x))=G(x,\textbf{0}_n)=(x,h(x,\textbf{0}_n)). G=F^{-1} h n G g(x)=h(x,\textbf{0}_n) g(x)","['real-analysis', 'multivariable-calculus', 'derivatives', 'implicit-function-theorem']"
55,lower bound for a function of $\Gamma$,lower bound for a function of,\Gamma,"I would like to show that $$\frac{\Gamma(1-2x)\Gamma(1+x)}{\Gamma(1-x)}\geq 1$$ for real $x$ such that $|x|\leq \frac12$, where $\Gamma$ is the usual gamma function. I looked at the derivatives and went a long road to prove this. I though somehow believe there should be a simpler way. I appreciate any hints or comments. Many thanks!","I would like to show that $$\frac{\Gamma(1-2x)\Gamma(1+x)}{\Gamma(1-x)}\geq 1$$ for real $x$ such that $|x|\leq \frac12$, where $\Gamma$ is the usual gamma function. I looked at the derivatives and went a long road to prove this. I though somehow believe there should be a simpler way. I appreciate any hints or comments. Many thanks!",,"['calculus', 'real-analysis']"
56,"Lebesgue integration on $[0,1]$ of $x^\alpha$",Lebesgue integration on  of,"[0,1] x^\alpha","This question has been asked before here , but no one really dealt with the problem, and just critiqued the user's method, which seems to have been what the poster wanted. I'm looking for the full solution. In other words, is my full answer correct? If not, what should the changes be? I can find small details all over the place on Math.SE but for my own confidence (and reference), I would be very happy to have the full solution here. Perhaps others will benefit, too. My answer seems too easy to be correct, and I am suspicious that the Monotone and/or Dominated convergence theorems don't make an appearance in my solution. We are in Royden's ""Real Analysis"", page 84, question 19. The question states: ""For a number $\alpha$, define $f(x)=x^\alpha$ for $0<x\leq 1$ and $f(0)=0$. Compute $\int\limits_{0}^{1}f.$"" Now, for $\alpha\geq0$, the problem is easy, as the function is Riemann integrable, and we obtain $\frac{1}{\alpha+1}$. But what about $\alpha<0$? It seems that the three cases are then $-1<\alpha<0$, $\alpha=-1$, and $\alpha<-1$. In the second, we obtain: $$-\lim\limits_{\varepsilon\to0^+}\ln\varepsilon=\infty.$$ In the last, we obtain: $$\frac{1}{\alpha+1}+\lim\limits_{\varepsilon\to0^+}\frac{\varepsilon^{\alpha+1}}{\alpha+1}=\infty,$$ since $\alpha+1<0$. In the first, we obtain: $$\frac{1}{\alpha+1}+\lim\limits_{\varepsilon\to0^+}\frac{\varepsilon^{\alpha+1}}{\alpha+1}=\frac{1}{\alpha+1},$$ since $\alpha+1>0$. So, is my answer correct when I say $\int\limits_{0}^{1}f=\frac{1}{1+\alpha}$ for $\alpha>-1$ and $\int\limits_{0}^{1}f=\infty$ otherwise? Is the whole point of the problem that $x^\alpha$ is not Lebesgue integrable for $\alpha<-1$?","This question has been asked before here , but no one really dealt with the problem, and just critiqued the user's method, which seems to have been what the poster wanted. I'm looking for the full solution. In other words, is my full answer correct? If not, what should the changes be? I can find small details all over the place on Math.SE but for my own confidence (and reference), I would be very happy to have the full solution here. Perhaps others will benefit, too. My answer seems too easy to be correct, and I am suspicious that the Monotone and/or Dominated convergence theorems don't make an appearance in my solution. We are in Royden's ""Real Analysis"", page 84, question 19. The question states: ""For a number $\alpha$, define $f(x)=x^\alpha$ for $0<x\leq 1$ and $f(0)=0$. Compute $\int\limits_{0}^{1}f.$"" Now, for $\alpha\geq0$, the problem is easy, as the function is Riemann integrable, and we obtain $\frac{1}{\alpha+1}$. But what about $\alpha<0$? It seems that the three cases are then $-1<\alpha<0$, $\alpha=-1$, and $\alpha<-1$. In the second, we obtain: $$-\lim\limits_{\varepsilon\to0^+}\ln\varepsilon=\infty.$$ In the last, we obtain: $$\frac{1}{\alpha+1}+\lim\limits_{\varepsilon\to0^+}\frac{\varepsilon^{\alpha+1}}{\alpha+1}=\infty,$$ since $\alpha+1<0$. In the first, we obtain: $$\frac{1}{\alpha+1}+\lim\limits_{\varepsilon\to0^+}\frac{\varepsilon^{\alpha+1}}{\alpha+1}=\frac{1}{\alpha+1},$$ since $\alpha+1>0$. So, is my answer correct when I say $\int\limits_{0}^{1}f=\frac{1}{1+\alpha}$ for $\alpha>-1$ and $\int\limits_{0}^{1}f=\infty$ otherwise? Is the whole point of the problem that $x^\alpha$ is not Lebesgue integrable for $\alpha<-1$?",,"['calculus', 'real-analysis', 'integration', 'lebesgue-integral']"
57,"Show if $f(x+y)=f(x)+f(y)$ for all $x,y$ and $f$ is Lebesgue measurable, then $f$ is continuous. [duplicate]","Show if  for all  and  is Lebesgue measurable, then  is continuous. [duplicate]","f(x+y)=f(x)+f(y) x,y f f","This question already has an answer here : Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that $g(x+y) =g(x)+g(y).$ Then $g(x) = g(1)x$ . [closed] (1 answer) Closed 7 years ago . I'm trying to solve this problem $(5.8)$ from Bass' Real Analysis for Graduate Students: Show that if $f:\Bbb{R}\to\Bbb{R}$ is Lebesgue measurable and $$f(x+y)=f(x)+f(y)$$ for all $x,y\in\Bbb{R}$, then $f$ is continuous. The first thing I note is that it suffices to show that $f$ is continuous at $0$, since if this is the case then for any $\epsilon>0$ and $x_0\in\Bbb R$, there exists $\delta>0$ such that $|x|<\delta$ implies $|f(x)|<\epsilon$; in particular, replacing $x$ with $x-x_0$, we see $|x-x_0|<\delta$ implies $|f(x)-f(x_0)|=|f(x-x_0)|<\epsilon$. Now, what I need to show then is that $f^{-1}(-\epsilon,\epsilon)$ contains some interval at the origin. I know that this set is Lebesgue measurable because $(-\epsilon,\epsilon)$ is Borel measurable, and I know it contains $0$ since $f(0)=0$, but I'm not sure how to show it contains $(-\delta,\delta)$ for some $\delta$. I haven't used the fact that the set is Lebesgue measurable, so obviously theres some way I can use that but I'm not seeing it. Any suggestions? edit: because this has been tagged as a possible duplicate, I should point out that I'd prefer to figure out if the solution I am working on will work, rather than just copy a completely different solution outlined by some other user.","This question already has an answer here : Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that $g(x+y) =g(x)+g(y).$ Then $g(x) = g(1)x$ . [closed] (1 answer) Closed 7 years ago . I'm trying to solve this problem $(5.8)$ from Bass' Real Analysis for Graduate Students: Show that if $f:\Bbb{R}\to\Bbb{R}$ is Lebesgue measurable and $$f(x+y)=f(x)+f(y)$$ for all $x,y\in\Bbb{R}$, then $f$ is continuous. The first thing I note is that it suffices to show that $f$ is continuous at $0$, since if this is the case then for any $\epsilon>0$ and $x_0\in\Bbb R$, there exists $\delta>0$ such that $|x|<\delta$ implies $|f(x)|<\epsilon$; in particular, replacing $x$ with $x-x_0$, we see $|x-x_0|<\delta$ implies $|f(x)-f(x_0)|=|f(x-x_0)|<\epsilon$. Now, what I need to show then is that $f^{-1}(-\epsilon,\epsilon)$ contains some interval at the origin. I know that this set is Lebesgue measurable because $(-\epsilon,\epsilon)$ is Borel measurable, and I know it contains $0$ since $f(0)=0$, but I'm not sure how to show it contains $(-\delta,\delta)$ for some $\delta$. I haven't used the fact that the set is Lebesgue measurable, so obviously theres some way I can use that but I'm not seeing it. Any suggestions? edit: because this has been tagged as a possible duplicate, I should point out that I'd prefer to figure out if the solution I am working on will work, rather than just copy a completely different solution outlined by some other user.",,"['real-analysis', 'measure-theory']"
58,"Fake proof for ""differentiability implies continuous derivative"": review","Fake proof for ""differentiability implies continuous derivative"": review",,"We known that a function may be differentiable at a point while having discontinuous derivative at such point. The following exercise proposes a fake proof for the proposition ""let $f : \mathbb{R} \rightarrow \mathbb{R} $ be everywhere continuous and differentiable, then $f'$ is continuous over $\mathbb{R}$""; I am asked to find the mistake(s). We prove that, for every $ a \in \mathbb{R}$, $\lim_{x \rightarrow} f'(x)=f'(a)$. Fix a point $a \in \mathbb{R}$, for every $x \in \mathbb{R}$ with $x>a$, $f$ is continuous on $[a,x]$ and differentiable on $(a,x)$, hence by MVT there exists a point $\xi \in (a,x)$ such that: $$ \frac{f(x)-f(a)}{x-a} = f'(\xi) $$Now, the limit of the LHS when $x \rightarrow a$ exists (and equals $f'(a)$ since $f$ is differentiable at $x=a$), so does the limit of the RHS. Finally, notice that when $x \rightarrow a$, then  $c \rightarrow a$, leading to: $$  f'(a)=\lim_{\xi \rightarrow a} f'(\xi). $$ My thought is that the trick lies in the worlds ""when $x \rightarrow a$, then  $\xi \rightarrow a$"": $\xi$ is in fact a function of $x$ (even if not explicitly said) $\xi=\xi(x)$, so the previous assertion somehow requires the continuity of $\xi(x)$. Is that idea correct or there is something else I couldn't see?","We known that a function may be differentiable at a point while having discontinuous derivative at such point. The following exercise proposes a fake proof for the proposition ""let $f : \mathbb{R} \rightarrow \mathbb{R} $ be everywhere continuous and differentiable, then $f'$ is continuous over $\mathbb{R}$""; I am asked to find the mistake(s). We prove that, for every $ a \in \mathbb{R}$, $\lim_{x \rightarrow} f'(x)=f'(a)$. Fix a point $a \in \mathbb{R}$, for every $x \in \mathbb{R}$ with $x>a$, $f$ is continuous on $[a,x]$ and differentiable on $(a,x)$, hence by MVT there exists a point $\xi \in (a,x)$ such that: $$ \frac{f(x)-f(a)}{x-a} = f'(\xi) $$Now, the limit of the LHS when $x \rightarrow a$ exists (and equals $f'(a)$ since $f$ is differentiable at $x=a$), so does the limit of the RHS. Finally, notice that when $x \rightarrow a$, then  $c \rightarrow a$, leading to: $$  f'(a)=\lim_{\xi \rightarrow a} f'(\xi). $$ My thought is that the trick lies in the worlds ""when $x \rightarrow a$, then  $\xi \rightarrow a$"": $\xi$ is in fact a function of $x$ (even if not explicitly said) $\xi=\xi(x)$, so the previous assertion somehow requires the continuity of $\xi(x)$. Is that idea correct or there is something else I couldn't see?",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
59,Improving the constant in Hardy-Littlewood maximal inequality from $3^d$ to $2^d$,Improving the constant in Hardy-Littlewood maximal inequality from  to,3^d 2^d,"This is Excercise 42 in Terry's notes on Differentiation theorem . I find it interesting but stumbled to get what he meant in his hint. The Hardy-Littlewood maximal inequality for an absolutely integrable function $f$ reads: $$ m(\{Mf(x)\geq \lambda\}) \leq \frac{C_d}{\lambda}\int_\mathbb{R^{d}}|f(t)|dt, $$ where $m$ is the Lebesgue measure, $\lambda>0$ and $Mf(x)$ is the Hardy-Littlewood maximal function. Using a Vitali type of covering lemma one can show the constant can be $3^d$ . The exercise asks to improve this to $2^d$ , by noticing that $2$ -scaled balls cover the centers. Terry hinted that one need to do some $\epsilon$ adjustment. However, I failed to see why and how can small adjustments to $2$ -scaled balls (or perhaps something else) can still lead to a sufficient cover, given that choice of covering balls seems to be fixed. What am I missing here?","This is Excercise 42 in Terry's notes on Differentiation theorem . I find it interesting but stumbled to get what he meant in his hint. The Hardy-Littlewood maximal inequality for an absolutely integrable function reads: where is the Lebesgue measure, and is the Hardy-Littlewood maximal function. Using a Vitali type of covering lemma one can show the constant can be . The exercise asks to improve this to , by noticing that -scaled balls cover the centers. Terry hinted that one need to do some adjustment. However, I failed to see why and how can small adjustments to -scaled balls (or perhaps something else) can still lead to a sufficient cover, given that choice of covering balls seems to be fixed. What am I missing here?","f  m(\{Mf(x)\geq \lambda\}) \leq \frac{C_d}{\lambda}\int_\mathbb{R^{d}}|f(t)|dt,  m \lambda>0 Mf(x) 3^d 2^d 2 \epsilon 2","['real-analysis', 'measure-theory', 'harmonic-analysis']"
60,"Prove not differentiable at $(0,0)$ but directional derivatives every direction.",Prove not differentiable at  but directional derivatives every direction.,"(0,0)","Verify that the function $f: \mathbb R^2 \to \mathbb R$ given by $f(x,y) = \frac{x^2y}{x^4 + y^2}$ for $ (x,y) \neq 0$ and $f(0,0) = 0$ is not differentiable at $(0,0)$ and yet has directional derivatives in every direction. My idea: $\frac{\partial f(x,y)}{\partial x} = \frac{2xy(x^4+y^2)-4x^3(x^2y)}{(x^4+y^2)^2}$ $\frac{\partial f(x,y)}{\partial y} = \frac{x^2(x^4+y^2)-2y(x^2y)}{(x^4+y^2)^2}$ and neither of the partial derivatives $\frac{\partial f(x,y)}{\partial x}$ , $\frac{\partial f(x,y)}{\partial y}$ exist at $f(0,0)$, so by definition  they are not differentiable this point. Now let the unit vector $\hat{u}=(u_1,u_2)$ such that $\|\hat{u}\|=1$ Directional derivative at point $(0,0)$ is $D_uf(0,0) = \lim_{t\to 0}\frac{f(0 +tu_1,0 +tu_2)-f(0,0)}{t} $ and must exist. Is this right so far? Where do I go from here?","Verify that the function $f: \mathbb R^2 \to \mathbb R$ given by $f(x,y) = \frac{x^2y}{x^4 + y^2}$ for $ (x,y) \neq 0$ and $f(0,0) = 0$ is not differentiable at $(0,0)$ and yet has directional derivatives in every direction. My idea: $\frac{\partial f(x,y)}{\partial x} = \frac{2xy(x^4+y^2)-4x^3(x^2y)}{(x^4+y^2)^2}$ $\frac{\partial f(x,y)}{\partial y} = \frac{x^2(x^4+y^2)-2y(x^2y)}{(x^4+y^2)^2}$ and neither of the partial derivatives $\frac{\partial f(x,y)}{\partial x}$ , $\frac{\partial f(x,y)}{\partial y}$ exist at $f(0,0)$, so by definition  they are not differentiable this point. Now let the unit vector $\hat{u}=(u_1,u_2)$ such that $\|\hat{u}\|=1$ Directional derivative at point $(0,0)$ is $D_uf(0,0) = \lim_{t\to 0}\frac{f(0 +tu_1,0 +tu_2)-f(0,0)}{t} $ and must exist. Is this right so far? Where do I go from here?",,['calculus']
61,Is this sum on power free number and Euler constant true?,Is this sum on power free number and Euler constant true?,,"Let $p_{n,r}$ be $r$-th smallest $n$-power free number. For example $p_{2,7}$ is the $7$-th square free number and $p_{5,7}$ is the $7$-th smallest $5$-th power free number. Let $q_{n,r}$ be the $r$-th number which contains (or is divisible by) a $n$-th power. For example $q_{2,7}$ is the $7$-th smallest number which contains a square and $q_{5,7}$ is the $7$-th smallest number which contains a fifth power. We define $\alpha_n$ for $n \ge 2$ as the ratio of the sum of the $n$-th power free number to the sum of the $n$-th power containing numbers i.e. $$ \alpha_n = \lim_{r \to \infty}\frac{p_{n,1}+p_{n,2}+\ldots + p_{n,r}}{q_{n,1}+q_{n,2}+\ldots + q_{n,r}}. $$ Question : Is it true that $$ \sum_{n = 2}^{\infty} \frac{\alpha_n}{n} = 1 - \gamma $$ where $\gamma$ is the Euler-Mascheroni constant? Motivation : I ran a program and the sum seem to converge to 0.422785 which is close to $1-\gamma$.","Let $p_{n,r}$ be $r$-th smallest $n$-power free number. For example $p_{2,7}$ is the $7$-th square free number and $p_{5,7}$ is the $7$-th smallest $5$-th power free number. Let $q_{n,r}$ be the $r$-th number which contains (or is divisible by) a $n$-th power. For example $q_{2,7}$ is the $7$-th smallest number which contains a square and $q_{5,7}$ is the $7$-th smallest number which contains a fifth power. We define $\alpha_n$ for $n \ge 2$ as the ratio of the sum of the $n$-th power free number to the sum of the $n$-th power containing numbers i.e. $$ \alpha_n = \lim_{r \to \infty}\frac{p_{n,1}+p_{n,2}+\ldots + p_{n,r}}{q_{n,1}+q_{n,2}+\ldots + q_{n,r}}. $$ Question : Is it true that $$ \sum_{n = 2}^{\infty} \frac{\alpha_n}{n} = 1 - \gamma $$ where $\gamma$ is the Euler-Mascheroni constant? Motivation : I ran a program and the sum seem to converge to 0.422785 which is close to $1-\gamma$.",,"['real-analysis', 'number-theory', 'elementary-number-theory', 'divisibility', 'analytic-number-theory']"
62,"Showing that $\lim_{n\to\infty}\int_{[0,1]}|f-f_n|=0$ where $f_n(x)=f(x-1/n)$",Showing that  where,"\lim_{n\to\infty}\int_{[0,1]}|f-f_n|=0 f_n(x)=f(x-1/n)","I am trying to tackle the following question, but I actually don't know how to start... Let $f:\Bbb{R}\to\Bbb{R}$ be measurable function with period $1$ and $\displaystyle \int_{0}^{1}|f|<\infty$ . Define $\displaystyle f_n(x)=f\left(x-\frac{1}{n}\right)$ . Show that $$\lim_{n\to\infty}\int\limits_{[0,1]}|f-f_n|=0$$ I have tried to use periodicity of $f$ and manipulate $f_n(x)$ , but to no avail. Please help, thanks!","I am trying to tackle the following question, but I actually don't know how to start... Let be measurable function with period and . Define . Show that I have tried to use periodicity of and manipulate , but to no avail. Please help, thanks!","f:\Bbb{R}\to\Bbb{R} 1 \displaystyle \int_{0}^{1}|f|<\infty \displaystyle f_n(x)=f\left(x-\frac{1}{n}\right) \lim_{n\to\infty}\int\limits_{[0,1]}|f-f_n|=0 f f_n(x)","['real-analysis', 'convergence-divergence']"
63,Evaluation of series $\sum \limits_{n=1}^{\infty} \frac{\arctan n}{n^2}$,Evaluation of series,\sum \limits_{n=1}^{\infty} \frac{\arctan n}{n^2},Can we find a closed form for the series: $$\mathcal{S}=\sum_{n=1}^{\infty} \frac{\arctan n}{n^2}$$ Here is some basic manipulation I did: \begin{align*} \sum_{n=1}^{\infty} \frac{\arctan n}{n^2} &=\sum_{n=1}^{\infty} \frac{1}{n^2} \sum_{k=0}^{\infty} (-1)^{k-1} \frac{n^{2k+1}}{2k+1} \\   &= \sum_{k=0}^{\infty}\frac{(-1)^{k-1}}{2k+1} \sum_{n=1}^{\infty} \frac{n^{2k+1}}{n^2}\\   &= \sum_{k=0}^{\infty} \frac{(-1)^{k-1} \zeta(1-2k)}{2k+1} \end{align*} Now unfortunately I don't know how to proceed further. Maybe the sum does not admit a closed form and all that I have done so far be in vain. Another way would be : $$\sum_{n=1}^{\infty} \frac{\arctan n}{n^2} =\mathfrak{Im} \left ( \sum_{n=1}^{\infty} \frac{\ln (1+in)}{n^2} \right )$$ Now I don't know how to sum the second series! Any help?,Can we find a closed form for the series: $$\mathcal{S}=\sum_{n=1}^{\infty} \frac{\arctan n}{n^2}$$ Here is some basic manipulation I did: \begin{align*} \sum_{n=1}^{\infty} \frac{\arctan n}{n^2} &=\sum_{n=1}^{\infty} \frac{1}{n^2} \sum_{k=0}^{\infty} (-1)^{k-1} \frac{n^{2k+1}}{2k+1} \\   &= \sum_{k=0}^{\infty}\frac{(-1)^{k-1}}{2k+1} \sum_{n=1}^{\infty} \frac{n^{2k+1}}{n^2}\\   &= \sum_{k=0}^{\infty} \frac{(-1)^{k-1} \zeta(1-2k)}{2k+1} \end{align*} Now unfortunately I don't know how to proceed further. Maybe the sum does not admit a closed form and all that I have done so far be in vain. Another way would be : $$\sum_{n=1}^{\infty} \frac{\arctan n}{n^2} =\mathfrak{Im} \left ( \sum_{n=1}^{\infty} \frac{\ln (1+in)}{n^2} \right )$$ Now I don't know how to sum the second series! Any help?,,"['calculus', 'real-analysis', 'sequences-and-series', 'closed-form']"
64,"Open interval $(0,1)$ with the usual topology admits a metric space",Open interval  with the usual topology admits a metric space,"(0,1)","which of the following is/are true ? $(0,1)$ with the usual topology admits a metric which is  complete . $(0,1)$ with the usual topology admits a metric which is not  complete. $[0,1]$ with the usual topology admits a metric which is not complete. $[0,1]$ with the usual topology admits a metric which is  complete. This question has come at my competetive exam. I think this is a wrong question, because completeness a metric space property not a topological space property.In the offical answer key, answer has given (1) and (4), I want to send my representation. So please check my representation. Thank you Let $X = (0,1)$ and $d$ is a Euclidean  metric on $X$ which induces the usual topology on $X$ and a sequence $\{\frac{1}{n} \}$ is a cauchy sequence in the Euclidean metric , but  not converges in $X$. So $X$ is not complete withbthe usual topology admit a Euclidean metric. On the other hand The map $$ f:(0,1)\to\mathbb{R}:x\mapsto\tan\pi\left(x-\frac{1}{2}\right) $$ is a bijection which allows you to define the metric $$ d(x,y)=|f(x)-f(y)| $$ which makes $((0,1),d)$ complete. Since $f$ maps intervals to intervals then both topologies are equivalent. So Completeness is not a topological property. So this is irrelivent. I would be thankful, if some one check my representation","which of the following is/are true ? $(0,1)$ with the usual topology admits a metric which is  complete . $(0,1)$ with the usual topology admits a metric which is not  complete. $[0,1]$ with the usual topology admits a metric which is not complete. $[0,1]$ with the usual topology admits a metric which is  complete. This question has come at my competetive exam. I think this is a wrong question, because completeness a metric space property not a topological space property.In the offical answer key, answer has given (1) and (4), I want to send my representation. So please check my representation. Thank you Let $X = (0,1)$ and $d$ is a Euclidean  metric on $X$ which induces the usual topology on $X$ and a sequence $\{\frac{1}{n} \}$ is a cauchy sequence in the Euclidean metric , but  not converges in $X$. So $X$ is not complete withbthe usual topology admit a Euclidean metric. On the other hand The map $$ f:(0,1)\to\mathbb{R}:x\mapsto\tan\pi\left(x-\frac{1}{2}\right) $$ is a bijection which allows you to define the metric $$ d(x,y)=|f(x)-f(y)| $$ which makes $((0,1),d)$ complete. Since $f$ maps intervals to intervals then both topologies are equivalent. So Completeness is not a topological property. So this is irrelivent. I would be thankful, if some one check my representation",,"['real-analysis', 'proof-verification', 'complete-spaces']"
65,Show that if $(\sum x_n)$ converges absolutely and $(y_n)$ is bounded then $(\sum x_n y_n)$ converges,Show that if  converges absolutely and  is bounded then  converges,(\sum x_n) (y_n) (\sum x_n y_n),"This is the exercise 2.7.6 of the book Understanding analysis of Abbott, I want a check of my proof and if is needed additional information to complete it. a) Show that if the sequence $(\sum x_n)$ converges absolutely and the sequence $(y_n)$ is bounded then the sequence $(\sum x_n y_n)$ converges b) Find a counterexample that demonstrates that a) does not always hold if the convergence of $(\sum x_n)$ is conditional For the part a): if $(\sum x_n)$ converges absolutely it means, using the definition of Cauchy sequences (that demonstrates convergence on complete spaces) applied to series that $$\forall \varepsilon> 0, \exists N\in\Bbb N :\sum_{n=m}^{t} |x_n|<\varepsilon, \forall t>m> N$$ and we have the inequalities $|\sum x_n y_n|\le \sum|x_n y_n|$ and $|\sum x_n|\le\sum|x_n|$ . And cause $(y_n)$ is bounded we have that $|y_n|\le B$ and then $$\left|\sum x_n y_n\right|\le\sum|x_n y_n|\le B\sum|x_n|$$ Then we have that $$\sum_{n=m}^{t} |x_n|<\frac{\varepsilon}{B}\implies\left|\sum_{n=m}^{t} x_n y_n\right|\le B\sum_{n=m}^{t}|x_n|<\varepsilon$$ For the part b) we can take the conditional convergent sequence $(\sum\frac{(-1)^{n-1}}{n})$ and the bounded sequence $((-1)^{n-1})$ . If we multiply both as the problem define we get the divergent sequence $(\sum \frac1n)$","This is the exercise 2.7.6 of the book Understanding analysis of Abbott, I want a check of my proof and if is needed additional information to complete it. a) Show that if the sequence converges absolutely and the sequence is bounded then the sequence converges b) Find a counterexample that demonstrates that a) does not always hold if the convergence of is conditional For the part a): if converges absolutely it means, using the definition of Cauchy sequences (that demonstrates convergence on complete spaces) applied to series that and we have the inequalities and . And cause is bounded we have that and then Then we have that For the part b) we can take the conditional convergent sequence and the bounded sequence . If we multiply both as the problem define we get the divergent sequence","(\sum x_n) (y_n) (\sum x_n y_n) (\sum x_n) (\sum x_n) \forall \varepsilon> 0, \exists N\in\Bbb N :\sum_{n=m}^{t} |x_n|<\varepsilon, \forall t>m> N |\sum x_n y_n|\le \sum|x_n y_n| |\sum x_n|\le\sum|x_n| (y_n) |y_n|\le B \left|\sum x_n y_n\right|\le\sum|x_n y_n|\le B\sum|x_n| \sum_{n=m}^{t} |x_n|<\frac{\varepsilon}{B}\implies\left|\sum_{n=m}^{t} x_n y_n\right|\le B\sum_{n=m}^{t}|x_n|<\varepsilon (\sum\frac{(-1)^{n-1}}{n}) ((-1)^{n-1}) (\sum \frac1n)","['real-analysis', 'sequences-and-series', 'proof-verification', 'absolute-convergence']"
66,Deleted versus non-deleted limits,Deleted versus non-deleted limits,,"When I read Serge Lang's Undergraduate Analysis(second edition) page 41,  I found the definition of limit he define is so called non-Deleted limits , this results in some conclusion is different from the usual limit( deleted limit), for example, every isolated point $x_0$ has limit, its value is $f(x_0)$. By topology knowledge, we know every isolated point is continuous, we cannot get this result by $\lim_{x\to x_0}f(x)=f(x_0)$, if we define limit as deleted limit. However, if we define limit as non-deleted limit, we can get it. My question: is non-deleted limit better than deleted limit, if yes, why does most textbook do not use it?","When I read Serge Lang's Undergraduate Analysis(second edition) page 41,  I found the definition of limit he define is so called non-Deleted limits , this results in some conclusion is different from the usual limit( deleted limit), for example, every isolated point $x_0$ has limit, its value is $f(x_0)$. By topology knowledge, we know every isolated point is continuous, we cannot get this result by $\lim_{x\to x_0}f(x)=f(x_0)$, if we define limit as deleted limit. However, if we define limit as non-deleted limit, we can get it. My question: is non-deleted limit better than deleted limit, if yes, why does most textbook do not use it?",,['real-analysis']
67,Componentwise Convergence in $\mathbb R^n$,Componentwise Convergence in,\mathbb R^n,"I came across the following question while preparing an exercise for basic analysis: Suppose $d$ is some arbitrary metric on $\mathbb R^N$ and $(x^n) \subset \mathbb R^N$ converges to $x\in \mathbb R^N$ with respect to $d$. Question : Does $(x^n)$ converge component-wise with respect to the usual norm? (And if not: Are there simple counter-examples?) Up to now, I know the basics: The case is solved if $d$ is induced by a norm, because all norms on $\mathbb R^N$ are equivalent and $(x^n)$ converges componentwise iff it converges with respect to any $p$-norm. The reverse of my question is not true. If for example $d$ is the discrete metric or the TGV metric, convergence in norm need not to imply convergence in metric. If $\mathbb R^N$ is viewed as a topological space $(\mathbb R^N,\tau)$, then convergence with respect to $\tau$ need not to imply convergence in norm. (E.g. $\tau = \{\emptyset,\mathbb R^N\}$, which is not induced by a metric.) All examples for metrics I considered so far (the usual examples) posess the desired property (and the proofs are easy, as these metrics are defined by using componentwise distances $|x_k-y_k|$). One could say, these metrics are just too normy, but I don't know, if there are not so normy examples or if my question can only be solved in a more abstract way. My question arises naturally, so maybe there is a simple answer I overlooked up to now.  Thank you in advance for your help.","I came across the following question while preparing an exercise for basic analysis: Suppose $d$ is some arbitrary metric on $\mathbb R^N$ and $(x^n) \subset \mathbb R^N$ converges to $x\in \mathbb R^N$ with respect to $d$. Question : Does $(x^n)$ converge component-wise with respect to the usual norm? (And if not: Are there simple counter-examples?) Up to now, I know the basics: The case is solved if $d$ is induced by a norm, because all norms on $\mathbb R^N$ are equivalent and $(x^n)$ converges componentwise iff it converges with respect to any $p$-norm. The reverse of my question is not true. If for example $d$ is the discrete metric or the TGV metric, convergence in norm need not to imply convergence in metric. If $\mathbb R^N$ is viewed as a topological space $(\mathbb R^N,\tau)$, then convergence with respect to $\tau$ need not to imply convergence in norm. (E.g. $\tau = \{\emptyset,\mathbb R^N\}$, which is not induced by a metric.) All examples for metrics I considered so far (the usual examples) posess the desired property (and the proofs are easy, as these metrics are defined by using componentwise distances $|x_k-y_k|$). One could say, these metrics are just too normy, but I don't know, if there are not so normy examples or if my question can only be solved in a more abstract way. My question arises naturally, so maybe there is a simple answer I overlooked up to now.  Thank you in advance for your help.",,"['real-analysis', 'metric-spaces']"
68,Sequential continuity implies continuity in the weak topology on a normed space,Sequential continuity implies continuity in the weak topology on a normed space,,"Let $X$ be a normed vector space (over $\mathbb{R}$ or $\mathbb{C}$) and let $f$ be a linear functional on $X$ that is not necessarily continuous. If for any sequence $(x_n)$ that converges to $x$ weakly, we have $\lim f(x_n)=f(x)$, does it follow that $f$ is continuous in the weak topology? In other words, does sequential continuity imply continuity in the weak topology? (We know that the weak topology need not be first countable , so a priori we cannot characterise continuity of linear functionals in terms of sequences.)","Let $X$ be a normed vector space (over $\mathbb{R}$ or $\mathbb{C}$) and let $f$ be a linear functional on $X$ that is not necessarily continuous. If for any sequence $(x_n)$ that converges to $x$ weakly, we have $\lim f(x_n)=f(x)$, does it follow that $f$ is continuous in the weak topology? In other words, does sequential continuity imply continuity in the weak topology? (We know that the weak topology need not be first countable , so a priori we cannot characterise continuity of linear functionals in terms of sequences.)",,"['real-analysis', 'general-topology', 'functional-analysis']"
69,A differentiation under the integral sign,A differentiation under the integral sign,,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be a function Lebesgue summable on all $\mu$-measurable and bounded subsets of $\mathbb{R}^n$, where $\mu$ is the usual Lebesgue measure defined on $\mathbb{R}^n$, and let $g:\mathbb{R}^n\to\mathbb{R}$ be a function of class $C^k(\mathbb{R}^n)$ whose support is contained in the compact subset $V\subset\mathbb{R}^n$. That implies that all the derivatives of $g$, up to the $k$-th, are bounded and supported within $V$. I suspect that these conditions are enough to guarantee that the function $$h(\boldsymbol{x}):=\int_V f(\boldsymbol{y}-\boldsymbol{x})g(\boldsymbol{y})\,d\mu_{\boldsymbol{y}}$$is of class $C^k(\mathbb{R}^n)$. In fact, I notice, if I am not wrong, that $$h(\boldsymbol{x})=\int_{V-\boldsymbol{x}} f(\boldsymbol{y})\bar{g}(\boldsymbol{y}+\boldsymbol{x})\,d\mu_{\boldsymbol{y}}=\int_{\mathbb{R}^n} f(\boldsymbol{y})\bar{g}(\boldsymbol{y}+\boldsymbol{x})\,d\mu_{\boldsymbol{y}}$$where $$\bar{g}(\boldsymbol{y}) := \begin{cases} g(\boldsymbol{y}), & \boldsymbol{y}\in V \\ 0, & \boldsymbol{y}\in\mathbb{R}^n\setminus V \end{cases}$$and $V-\boldsymbol{x}=\{\boldsymbol{y}\in\mathbb{R}^n:\boldsymbol{y}+\boldsymbol{x}\in V\}$, and I suppose that the conditions on $g$ may be enough to allow us to differentiate under the integral sign. Is my intuition that $h\in C^k(\mathbb{R}^n)$ correct and, if it is, how can we prove it? A trial of mine : I know a corollary of Lebesgue's dominated convergence theorem that - if $f:V\times [a,b]\to \mathbb{R}$, $(\boldsymbol{x},t)\mapsto f(\boldsymbol{x},t)$ with $V$ measurable is such that $\forall t\in[a,b]\quad f(-,t)\in L^1(V)$, i.e. the function $\boldsymbol{x}\mapsto  f(\boldsymbol{x},t) $ is Lebesgue summable on $V$, - and if there is a neighbourhood $B(t_0,\delta)$ of $t_0$ such that, for almost all $\boldsymbol{x}\in V$ and for all $t\in B(t_0,\delta)$, $\left|\frac{\partial f(\boldsymbol{x},t)}{\partial t}\right|\le\varphi(\boldsymbol{x})$, where $\varphi\in L^1(V) $, then $$\frac{d}{dt}\int_V f(\boldsymbol{x},t) d\mu_{\boldsymbol{x}}\bigg|_{t=t_0}=\int_V\frac{\partial f(\boldsymbol{x},t_0)}{\partial t}d\mu_{\boldsymbol{x}}$$but I am not able to find a proper $\varphi$ to use in this context. Nevertheless I would not be amazed if there were an even more straightforward method to prove that the integral and derivative sign can be commutated...","Let $f:\mathbb{R}^n\to\mathbb{R}$ be a function Lebesgue summable on all $\mu$-measurable and bounded subsets of $\mathbb{R}^n$, where $\mu$ is the usual Lebesgue measure defined on $\mathbb{R}^n$, and let $g:\mathbb{R}^n\to\mathbb{R}$ be a function of class $C^k(\mathbb{R}^n)$ whose support is contained in the compact subset $V\subset\mathbb{R}^n$. That implies that all the derivatives of $g$, up to the $k$-th, are bounded and supported within $V$. I suspect that these conditions are enough to guarantee that the function $$h(\boldsymbol{x}):=\int_V f(\boldsymbol{y}-\boldsymbol{x})g(\boldsymbol{y})\,d\mu_{\boldsymbol{y}}$$is of class $C^k(\mathbb{R}^n)$. In fact, I notice, if I am not wrong, that $$h(\boldsymbol{x})=\int_{V-\boldsymbol{x}} f(\boldsymbol{y})\bar{g}(\boldsymbol{y}+\boldsymbol{x})\,d\mu_{\boldsymbol{y}}=\int_{\mathbb{R}^n} f(\boldsymbol{y})\bar{g}(\boldsymbol{y}+\boldsymbol{x})\,d\mu_{\boldsymbol{y}}$$where $$\bar{g}(\boldsymbol{y}) := \begin{cases} g(\boldsymbol{y}), & \boldsymbol{y}\in V \\ 0, & \boldsymbol{y}\in\mathbb{R}^n\setminus V \end{cases}$$and $V-\boldsymbol{x}=\{\boldsymbol{y}\in\mathbb{R}^n:\boldsymbol{y}+\boldsymbol{x}\in V\}$, and I suppose that the conditions on $g$ may be enough to allow us to differentiate under the integral sign. Is my intuition that $h\in C^k(\mathbb{R}^n)$ correct and, if it is, how can we prove it? A trial of mine : I know a corollary of Lebesgue's dominated convergence theorem that - if $f:V\times [a,b]\to \mathbb{R}$, $(\boldsymbol{x},t)\mapsto f(\boldsymbol{x},t)$ with $V$ measurable is such that $\forall t\in[a,b]\quad f(-,t)\in L^1(V)$, i.e. the function $\boldsymbol{x}\mapsto  f(\boldsymbol{x},t) $ is Lebesgue summable on $V$, - and if there is a neighbourhood $B(t_0,\delta)$ of $t_0$ such that, for almost all $\boldsymbol{x}\in V$ and for all $t\in B(t_0,\delta)$, $\left|\frac{\partial f(\boldsymbol{x},t)}{\partial t}\right|\le\varphi(\boldsymbol{x})$, where $\varphi\in L^1(V) $, then $$\frac{d}{dt}\int_V f(\boldsymbol{x},t) d\mu_{\boldsymbol{x}}\bigg|_{t=t_0}=\int_V\frac{\partial f(\boldsymbol{x},t_0)}{\partial t}d\mu_{\boldsymbol{x}}$$but I am not able to find a proper $\varphi$ to use in this context. Nevertheless I would not be amazed if there were an even more straightforward method to prove that the integral and derivative sign can be commutated...",,"['real-analysis', 'multivariable-calculus', 'lebesgue-integral', 'partial-derivative']"
70,Approximating $x=\sqrt{2}+1$,Approximating,x=\sqrt{2}+1,"Suppose $y>1$ is some approximation to $x=\sqrt{2}+1$. Give a brief reason (not a proof) why one should expect $(1/y)+2$ to be a closer approximation to $x$ than $y$ is. After testing this out for a bit, it looks like we can let $y_{n+1}=\frac{1}{y_n}+2$ and $\lim_{n\to\infty}y_n=\sqrt{2}+1$, but this does not give me any intuitive idea as to why $y_{n+1}$ should be a better approximation to $x$ than $y_n$ is. Can anyone give a brief reason for this improvement in aproximation, especially a more ""intuitive"" one than simple numerical data?","Suppose $y>1$ is some approximation to $x=\sqrt{2}+1$. Give a brief reason (not a proof) why one should expect $(1/y)+2$ to be a closer approximation to $x$ than $y$ is. After testing this out for a bit, it looks like we can let $y_{n+1}=\frac{1}{y_n}+2$ and $\lim_{n\to\infty}y_n=\sqrt{2}+1$, but this does not give me any intuitive idea as to why $y_{n+1}$ should be a better approximation to $x$ than $y_n$ is. Can anyone give a brief reason for this improvement in aproximation, especially a more ""intuitive"" one than simple numerical data?",,"['calculus', 'real-analysis', 'limits', 'intuition', 'approximation']"
71,"If $\sum a_n$and$\sum b_n$diverge, can$\sum \min\{a_n,b_n\}$converge? [duplicate]","If anddiverge, canconverge? [duplicate]","\sum a_n \sum b_n \sum \min\{a_n,b_n\}","This question already has answers here : Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges (3 answers) Closed 8 years ago . Do there exist sequences $\{a_n\}$ and $\{b_n\}$ satisfying all of the following properties? $a_n>0$ and $b_n>0$ $\{a_n\}$ and $\{b_n\}$ are both decreasing $\sum a_n$ and $\sum b_n$ both diverge $\sum\min\{a_n,b_n\}$ converges","This question already has answers here : Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges (3 answers) Closed 8 years ago . Do there exist sequences $\{a_n\}$ and $\{b_n\}$ satisfying all of the following properties? $a_n>0$ and $b_n>0$ $\{a_n\}$ and $\{b_n\}$ are both decreasing $\sum a_n$ and $\sum b_n$ both diverge $\sum\min\{a_n,b_n\}$ converges",,"['real-analysis', 'sequences-and-series', 'analysis']"
72,"counterexample to a ""theorem"" on continuity of largest deltas for continuous functions $f:[a,b]\to\mathbb{R}$","counterexample to a ""theorem"" on continuity of largest deltas for continuous functions","f:[a,b]\to\mathbb{R}","""Theorem 12"" in these notes states the following (verbatim): Let $f:[a,b]\to\mathbb{R}$ be continuous and let $\epsilon>0$. For $x\in[a,b]$, let    $$\Delta(x)=\sup\left\{\delta\,\,|\,\,\text{for all}\,y\in[a,b]\,\text{with}\,|x-y|<\delta,\,|f(x)-f(y)|<\epsilon\right\}$$   Then $\Delta$ is a continuous function of $x$. In other words (this is my paraphrase of the imprecise statement about the supremum), $\Delta(x)$ is the largest $\delta$ we can pick at $x$ that keeps the variation of $f$ within $\epsilon$. (Of course, for a specified $x$, the set of $\delta$'s is nonempty because $f$ is continuous.) Following the statement of the theorem, we're invited to use it to prove that continuous functions on closed and bounded intervals are uniformly continuous. But I think this theorem is false. Take $f(x)=\sqrt{x}$ on $[0,1]$ and $\epsilon=0.5$. I claim $$\Delta(x)=\begin{cases}\sqrt{x}-0.25&x>0.25\\\sqrt{x}+0.25&0\leq x\leq0.25\end{cases}$$ which has a jump discontinuity at $x=0.25$. This follows from a straightforward calculation I carried out for arbitrary $\epsilon$, which gives $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x>\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\leq\epsilon^2\end{cases}$$ with a jump discontinuity at $x=\epsilon^2$. This contradicts the quoted theorem. (As a side note, this computation gives a very nice brute-force way to discover that choosing $\delta=\epsilon^2$ suffices for the uniform continuity of $\sqrt{x}$ on $[0,\infty)$. Indeed it shows more: this choice of $\delta$ is the largest we can make, because it is the infimum of $\Delta$. I wish I'd done this computation in college.) I have two questions: Am I missing something, or does my counterexample show this claim is false? I've reworked the calculation and don't believe I'm wrong, but I could spell out the details if someone asks. If the claim is false, can it be repaired to do what the instructor wanted to do with it? (Please note that with question 2 I'm not asking for any old proof of the special case of the Heine-Cantor theorem that says continuous functions on closed and bounded intervals are uniformly continuous. I know the proof of the general result in arbitrary metric spaces. I'm asking: what theorem could the instructor possibly have had in mind instead of Theorem 12, as stated?) ADDED : a sketch of the calculation of $\Delta(x)$ for $\sqrt{x}$ on $[0,\infty)$ for arbitrary $\epsilon$. Break the work into cases. Case one: $\sqrt{x}>\epsilon$, i.e. $x>\epsilon^2$. The inverse image of the interval $(\sqrt{x}-\epsilon,\sqrt{x}+\epsilon)$ is $\big((\sqrt{x}-\epsilon)^2,(\sqrt{x}+\epsilon)^2\big)$. Therefore $$\Delta(x)=\min\big(x-(\sqrt{x}-\epsilon)^2,(\sqrt{x}+\epsilon)^2-x\big)=x-(\sqrt{x}-\epsilon)^2=2\epsilon\sqrt{x}-\epsilon^2$$ Case two: $\sqrt{x}\leq\epsilon$, i.e. $x<\epsilon^2$. Now we just look at the inverse image of $[0,\sqrt{x}+\epsilon)$, so it's only the right-hand boundary of the inverse image that matters for controlling the variation of $f$. The right-hand boundary is still $(\sqrt{x}+\epsilon)^2$, so $\Delta(x)=(\sqrt{x}+\epsilon)^2-x=2\epsilon\sqrt{x}+\epsilon^2$. CORRECTION (1/27/16): John Ma correctly points out in the comments that the correct calculation for $\sqrt{x}$ is the lower semicontinuous function $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x\color{red}{\geq}\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\color{red}{<}\epsilon^2\end{cases}$$ rather than my original upper semicontinuous $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x\color{blue}{>}\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\color{blue}{\leq}\epsilon^2\end{cases}$$","""Theorem 12"" in these notes states the following (verbatim): Let $f:[a,b]\to\mathbb{R}$ be continuous and let $\epsilon>0$. For $x\in[a,b]$, let    $$\Delta(x)=\sup\left\{\delta\,\,|\,\,\text{for all}\,y\in[a,b]\,\text{with}\,|x-y|<\delta,\,|f(x)-f(y)|<\epsilon\right\}$$   Then $\Delta$ is a continuous function of $x$. In other words (this is my paraphrase of the imprecise statement about the supremum), $\Delta(x)$ is the largest $\delta$ we can pick at $x$ that keeps the variation of $f$ within $\epsilon$. (Of course, for a specified $x$, the set of $\delta$'s is nonempty because $f$ is continuous.) Following the statement of the theorem, we're invited to use it to prove that continuous functions on closed and bounded intervals are uniformly continuous. But I think this theorem is false. Take $f(x)=\sqrt{x}$ on $[0,1]$ and $\epsilon=0.5$. I claim $$\Delta(x)=\begin{cases}\sqrt{x}-0.25&x>0.25\\\sqrt{x}+0.25&0\leq x\leq0.25\end{cases}$$ which has a jump discontinuity at $x=0.25$. This follows from a straightforward calculation I carried out for arbitrary $\epsilon$, which gives $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x>\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\leq\epsilon^2\end{cases}$$ with a jump discontinuity at $x=\epsilon^2$. This contradicts the quoted theorem. (As a side note, this computation gives a very nice brute-force way to discover that choosing $\delta=\epsilon^2$ suffices for the uniform continuity of $\sqrt{x}$ on $[0,\infty)$. Indeed it shows more: this choice of $\delta$ is the largest we can make, because it is the infimum of $\Delta$. I wish I'd done this computation in college.) I have two questions: Am I missing something, or does my counterexample show this claim is false? I've reworked the calculation and don't believe I'm wrong, but I could spell out the details if someone asks. If the claim is false, can it be repaired to do what the instructor wanted to do with it? (Please note that with question 2 I'm not asking for any old proof of the special case of the Heine-Cantor theorem that says continuous functions on closed and bounded intervals are uniformly continuous. I know the proof of the general result in arbitrary metric spaces. I'm asking: what theorem could the instructor possibly have had in mind instead of Theorem 12, as stated?) ADDED : a sketch of the calculation of $\Delta(x)$ for $\sqrt{x}$ on $[0,\infty)$ for arbitrary $\epsilon$. Break the work into cases. Case one: $\sqrt{x}>\epsilon$, i.e. $x>\epsilon^2$. The inverse image of the interval $(\sqrt{x}-\epsilon,\sqrt{x}+\epsilon)$ is $\big((\sqrt{x}-\epsilon)^2,(\sqrt{x}+\epsilon)^2\big)$. Therefore $$\Delta(x)=\min\big(x-(\sqrt{x}-\epsilon)^2,(\sqrt{x}+\epsilon)^2-x\big)=x-(\sqrt{x}-\epsilon)^2=2\epsilon\sqrt{x}-\epsilon^2$$ Case two: $\sqrt{x}\leq\epsilon$, i.e. $x<\epsilon^2$. Now we just look at the inverse image of $[0,\sqrt{x}+\epsilon)$, so it's only the right-hand boundary of the inverse image that matters for controlling the variation of $f$. The right-hand boundary is still $(\sqrt{x}+\epsilon)^2$, so $\Delta(x)=(\sqrt{x}+\epsilon)^2-x=2\epsilon\sqrt{x}+\epsilon^2$. CORRECTION (1/27/16): John Ma correctly points out in the comments that the correct calculation for $\sqrt{x}$ is the lower semicontinuous function $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x\color{red}{\geq}\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\color{red}{<}\epsilon^2\end{cases}$$ rather than my original upper semicontinuous $$\Delta(x)=\begin{cases}2\epsilon\sqrt{x}-\epsilon^2&x\color{blue}{>}\epsilon^2\\2\epsilon\sqrt{x}+\epsilon^2&0\leq x\color{blue}{\leq}\epsilon^2\end{cases}$$",,"['real-analysis', 'proof-verification', 'continuity', 'examples-counterexamples', 'uniform-continuity']"
73,"Two Banach spaces, if and only if criterion for range of closed unbounded operator to be closed?","Two Banach spaces, if and only if criterion for range of closed unbounded operator to be closed?",,"Let $E$ and $F$ be two Banach spaces. Let $A: D(A) \subset E \to F$ be a closed unbounded operator. How do I see that $R(A)$ is closed if and only if there exists a constant $C$ such that$$\text{dist}(u, N(A)) \le C\|Au\| \text{ for all }u \in D(A)?$$Here, $D$ denotes domain, $R$ denotes range, $N$ denotes kernel. Idea. We probably want to consider the operator $T: E_0 \to F$, where $E_0 = D(A)$ with the graph norm and $T = A$ in some regard? But I am not quite sure on what to do next.","Let $E$ and $F$ be two Banach spaces. Let $A: D(A) \subset E \to F$ be a closed unbounded operator. How do I see that $R(A)$ is closed if and only if there exists a constant $C$ such that$$\text{dist}(u, N(A)) \le C\|Au\| \text{ for all }u \in D(A)?$$Here, $D$ denotes domain, $R$ denotes range, $N$ denotes kernel. Idea. We probably want to consider the operator $T: E_0 \to F$, where $E_0 = D(A)$ with the graph norm and $T = A$ in some regard? But I am not quite sure on what to do next.",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'banach-spaces']"
74,Scalar Multiplication of Limits $\epsilon$ - $\delta $ Proof,Scalar Multiplication of Limits  -  Proof,\epsilon \delta ,"I am having troubling understanding the $\epsilon$ - $\delta $ proof of the scalar multiplication property of limits, which basically states: $$\lim_{x\to a}[f(x) \cdot c]=c\cdot L$$ The way I understand it (which doesn't feel to be a good understanding) our choice of $\delta$ is $\frac{\epsilon}{|c|}$, and then do we substitute this value of $\delta$ into the antecedent or the consequent? Second we are basically trying to satisfy the definition, in this case,  $$|x-a|<\delta \Longrightarrow |c \cdot f(x)-c \cdot L|< \epsilon$$ right? So should I start from this definition(below) and try to work create the above to the above definition(so substitute in the consequent)? $$|x-a|<\delta \Longrightarrow |  f(x)-  L|< \frac{\epsilon}{|c|}$$ Also, why does this proof use $\delta=\delta_1$?? The proof in the picture is from the following link: http://tutorial.math.lamar.edu/Classes/CalcI/LimitProofs.aspx","I am having troubling understanding the $\epsilon$ - $\delta $ proof of the scalar multiplication property of limits, which basically states: $$\lim_{x\to a}[f(x) \cdot c]=c\cdot L$$ The way I understand it (which doesn't feel to be a good understanding) our choice of $\delta$ is $\frac{\epsilon}{|c|}$, and then do we substitute this value of $\delta$ into the antecedent or the consequent? Second we are basically trying to satisfy the definition, in this case,  $$|x-a|<\delta \Longrightarrow |c \cdot f(x)-c \cdot L|< \epsilon$$ right? So should I start from this definition(below) and try to work create the above to the above definition(so substitute in the consequent)? $$|x-a|<\delta \Longrightarrow |  f(x)-  L|< \frac{\epsilon}{|c|}$$ Also, why does this proof use $\delta=\delta_1$?? The proof in the picture is from the following link: http://tutorial.math.lamar.edu/Classes/CalcI/LimitProofs.aspx",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
75,"Prove that $\int_a^b \left( \int_c^d f(x,y)dy\right) dx=\int_c^d \left( \int_a^b f(x,y)dx\right) dy$",Prove that,"\int_a^b \left( \int_c^d f(x,y)dy\right) dx=\int_c^d \left( \int_a^b f(x,y)dx\right) dy","Let $f$ be continuous function on $[a,b]\times [c,d]$. Prove that Prove that $$\int_a^b \left( \int_c^d f(x,y)dy\right) dx=\int_c^d  \left( \int_a^b f(x,y)dx\right) dy$$ First of all, note that $ \int_c^d f(x,y)dy$ is continuous in $x$ and $ \int_a^b f(x,y)dx$ is continuous in $y$ so both integrals exist. We have $$\frac{d\left[\int_a^b \left( \int_c^t f(x,y)dy\right) dx \right]}{dt}=\int_a^b f(x,t) dx$$ (We used differentiation under the integral sign and the fundamental theorem of calculus) Also $$\frac{d\left[\int_c^t  \left( \int_a^b f(x,y)dx\right) dy \right]}{dt}=\int_a^b f(x,t) dx$$ (Here we used the fundamental theorem of calculus) Thus $$\int_a^b \left( \int_c^t f(x,y)dy\right) dx-\int_c^t  \left( \int_a^b f(x,y)dx\right) dy$$ as a function of $t$ is constant on $(c,d)$. It remains to show that this constant is $0$. How can I do this?","Let $f$ be continuous function on $[a,b]\times [c,d]$. Prove that Prove that $$\int_a^b \left( \int_c^d f(x,y)dy\right) dx=\int_c^d  \left( \int_a^b f(x,y)dx\right) dy$$ First of all, note that $ \int_c^d f(x,y)dy$ is continuous in $x$ and $ \int_a^b f(x,y)dx$ is continuous in $y$ so both integrals exist. We have $$\frac{d\left[\int_a^b \left( \int_c^t f(x,y)dy\right) dx \right]}{dt}=\int_a^b f(x,t) dx$$ (We used differentiation under the integral sign and the fundamental theorem of calculus) Also $$\frac{d\left[\int_c^t  \left( \int_a^b f(x,y)dx\right) dy \right]}{dt}=\int_a^b f(x,t) dx$$ (Here we used the fundamental theorem of calculus) Thus $$\int_a^b \left( \int_c^t f(x,y)dy\right) dx-\int_c^t  \left( \int_a^b f(x,y)dx\right) dy$$ as a function of $t$ is constant on $(c,d)$. It remains to show that this constant is $0$. How can I do this?",,"['calculus', 'real-analysis', 'integration', 'proof-verification']"
76,"The Schwartz function and the sobolev space $W^{2,p}$",The Schwartz function and the sobolev space,"W^{2,p}","How do you prove the Schwartz functions in $\mathbb{R}^n$ are dense in the space $W^{2,p}(\mathbb{R}^n)?$ Terrence tao has a version of the proof of The space $C_c^{\infty}(\mathbb{R}^d)$ of test function is a dense subspace of $W^{k,p}(\mathbb{R}^d)$, then the fact $\mathcal{S}(\mathbb{R}^d)$ is dense in $L^p(\mathbb{R}^d)$ is a corollary from that. I do not understand his proof. (See lemma2) enter link description here","How do you prove the Schwartz functions in $\mathbb{R}^n$ are dense in the space $W^{2,p}(\mathbb{R}^n)?$ Terrence tao has a version of the proof of The space $C_c^{\infty}(\mathbb{R}^d)$ of test function is a dense subspace of $W^{k,p}(\mathbb{R}^d)$, then the fact $\mathcal{S}(\mathbb{R}^d)$ is dense in $L^p(\mathbb{R}^d)$ is a corollary from that. I do not understand his proof. (See lemma2) enter link description here",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'distribution-theory', 'regularity-theory-of-pdes']"
77,"Show that $\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a} \, dz=f(a)$",Show that,"\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a} \, dz=f(a)","Let $a\in\Bbb C$ and $r>0$ and denote by $B(a,r)\subseteq \Bbb C$ the open ball of center $a$ and radius $r$. Assume that $f:B(a,r)\to\Bbb C$ is a continuous function and for each $\epsilon>0$ let $\gamma_\epsilon:[0,2\pi]\to \Bbb C$ be given by $\gamma_\epsilon(t)=a+\epsilon e^{it}$. Show that    $$\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a} \, dz = f(a).$$ I tried the following $$\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a}dz=\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_0^{2\pi}\frac{f(a+\epsilon e^{it})}{a+\epsilon e^{it}-a}i\epsilon e^{it} \, dt = \lim_{\epsilon\to0^{+}} \frac{1}{2\pi} \int_0^{2\pi}f(a+\epsilon e^{it}) \, dt$$ If I can change the limit and the integral, then it is obvious. I tried to use the Lebesgue bounded convergence theorem to argue that, since $f:B(a,r)\to\Bbb C$ thus on $B(a,r)$ we have $|f(a+\epsilon e^{it})|\le M$, where M is the maximum of $|f(x)|$ on $B(a,r)$. Is that valid?","Let $a\in\Bbb C$ and $r>0$ and denote by $B(a,r)\subseteq \Bbb C$ the open ball of center $a$ and radius $r$. Assume that $f:B(a,r)\to\Bbb C$ is a continuous function and for each $\epsilon>0$ let $\gamma_\epsilon:[0,2\pi]\to \Bbb C$ be given by $\gamma_\epsilon(t)=a+\epsilon e^{it}$. Show that    $$\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a} \, dz = f(a).$$ I tried the following $$\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_{\gamma_\epsilon}\frac{f(z)}{z-a}dz=\lim_{\epsilon\to0^{+}}\frac{1}{2\pi i}\int_0^{2\pi}\frac{f(a+\epsilon e^{it})}{a+\epsilon e^{it}-a}i\epsilon e^{it} \, dt = \lim_{\epsilon\to0^{+}} \frac{1}{2\pi} \int_0^{2\pi}f(a+\epsilon e^{it}) \, dt$$ If I can change the limit and the integral, then it is obvious. I tried to use the Lebesgue bounded convergence theorem to argue that, since $f:B(a,r)\to\Bbb C$ thus on $B(a,r)$ we have $|f(a+\epsilon e^{it})|\le M$, where M is the maximum of $|f(x)|$ on $B(a,r)$. Is that valid?",,"['real-analysis', 'complex-analysis']"
78,The preimage of a Lebesgue measurable set under a measurable function need not be measurable [duplicate],The preimage of a Lebesgue measurable set under a measurable function need not be measurable [duplicate],,"This question already has an answer here : Pre-image of a measurable set A is always measurable? (1 answer) Closed 3 years ago . I am reading measure theory from Royden, and I am stuck in some of them. I have this question: suppose  $E$ is  a measurable set and let $f: E \to \mathbb{R}$. Prove that : $f$ is measurable if and only if $f^{-1}(A)$ is measurable for any $A \subseteq \mathbb{R}$. I know this is not true if measurable means ""Lebesgue measurable"", can anyone give a counterexample in details ?","This question already has an answer here : Pre-image of a measurable set A is always measurable? (1 answer) Closed 3 years ago . I am reading measure theory from Royden, and I am stuck in some of them. I have this question: suppose  $E$ is  a measurable set and let $f: E \to \mathbb{R}$. Prove that : $f$ is measurable if and only if $f^{-1}(A)$ is measurable for any $A \subseteq \mathbb{R}$. I know this is not true if measurable means ""Lebesgue measurable"", can anyone give a counterexample in details ?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
79,"If a function of two variables has a unique critical point, which is a local maximum, is it a global maximum?","If a function of two variables has a unique critical point, which is a local maximum, is it a global maximum?",,"$f(x,y)$ has partial derivatives in all $\mathbb R^2$ and a unique critical point at $(x_0,y_0)$ (local maximum). Is it a global maximum? I know that in compact sets, it isn't enough to say that if a point is the only maximum inside the set, then it's a global maximum, because in the frontier of the set it could happen that the function has a maximum greater than the inside one. But for the entire $\mathbb R^2$ there is no frontier, therefore can I admit that this unique point is a point of global maximum?","$f(x,y)$ has partial derivatives in all $\mathbb R^2$ and a unique critical point at $(x_0,y_0)$ (local maximum). Is it a global maximum? I know that in compact sets, it isn't enough to say that if a point is the only maximum inside the set, then it's a global maximum, because in the frontier of the set it could happen that the function has a maximum greater than the inside one. But for the entire $\mathbb R^2$ there is no frontier, therefore can I admit that this unique point is a point of global maximum?",,"['real-analysis', 'multivariable-calculus', 'optimization', 'partial-derivative']"
80,Determine whether the series $\sum_{n=1}^{+\infty}\left(1+\frac{1}{n}\right)a_{n}$ is convergent or divergent [duplicate],Determine whether the series  is convergent or divergent [duplicate],\sum_{n=1}^{+\infty}\left(1+\frac{1}{n}\right)a_{n},"This question already has an answer here : Too simple proof for convergence of $\sum_n a_n b_n$? (1 answer) Closed 8 years ago . If $$\sum_{n=1}^{\infty}a_{n}$$divergent, determine whether the series $$\sum_{n=1}^{+\infty}\left(1+\dfrac{1}{n}\right)a_{n}$$ is convergent or divergent. I know I have to use the ratio test.","This question already has an answer here : Too simple proof for convergence of $\sum_n a_n b_n$? (1 answer) Closed 8 years ago . If $$\sum_{n=1}^{\infty}a_{n}$$divergent, determine whether the series $$\sum_{n=1}^{+\infty}\left(1+\dfrac{1}{n}\right)a_{n}$$ is convergent or divergent. I know I have to use the ratio test.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
81,Only two parts left : Problem on Fourier Transform and convergence of Tempered Distributions,Only two parts left : Problem on Fourier Transform and convergence of Tempered Distributions,,"I recently met this problem from Folland's real analysis second edition involving a specific question on distributions (exercise 19 page 299) which reads as follows: On $ R $ let $ F_0 = PV(\frac{1}{x}) $ where PV stands for ""Principle Value"" and defined as follows: $ \langle PV(f),\phi\rangle = \lim_{\epsilon \to 0^+} \int_{|x|>\epsilon} f(x)\phi(x) \, dx $ for all $ \phi \in C_C^\infty $ . Also for $ \epsilon > 0 $ we define $ F_\epsilon(x) = x(x^2+\epsilon^2)^{-1} $ , $ G_\epsilon^\pm(x)=(x \pm i\epsilon)^{-1} $ and $ S_\epsilon(x) = \operatorname{sgn}(x) e^{-2 \pi \epsilon |x|} $ a. We are to prove $ \lim_{\epsilon \to 0} F_\epsilon=F_0 $ in the weak topology* on $ \mathcal{S}' $ (distributions on Schwartz class of functions where we define the weak topology in the usual point-wise convergence sense). As hint we are told to use the theorem below the question with a=0. b. We are to prove that $ \lim_{\epsilon \to 0} G_\epsilon^\pm = F_0 \mp i \pi \delta $ (Hint : $(x \pm i\epsilon )^{-1} = (x \mp i\epsilon)(x^2+\epsilon ^2)^{-1} $ ). c. We are to prove that $ \widehat{S}_\epsilon = (i\pi)^{-1} F_\epsilon $ and hence $ \widehat{\operatorname{sgn}} = (i\pi)^{-1}F_0 $ . d. From part c it follows that $ \widehat{F}_0 = -i\pi \operatorname{sgn} $ . We are to prove this directly by showing $ \lim_{\epsilon \to 0 , N \to \infty } H_{\epsilon,N} = F_0 $ where we define $ H_{\epsilon,N} $ to be $ \frac{1}{x} $ if $ \epsilon < |x| < N $ and 0 otherwise, and via the exercise at the bottom. e. We are to compute $ \widehat{\chi}_{(0,\infty)} $ (i) By writing $\chi = \frac{1}{2} \operatorname{sgn} + \frac{1}{2} $ and by using part c (ii) By using $ \chi(x) = \lim_{\epsilon \to 0} e^{-x\epsilon} \chi_{(0,\infty )} $ and by using b. The theorem instructed to use (Notation used here is for $ \phi \in \mathbb R^n $ we define $ \phi_t(x) = t^{-n} \phi(t^{-1}x) $ ) : The exercise instructed to use: Here are where my problems are: I cannot seem to tackle any of parts a,b,c,d and also part e, as simple as it might sound, I tried doing but always ended up getting some close result but with something wrong. So I really need the help on this in order to do it, I realize it is a long question but I tried asking two people I know and they could not help me either, and of course I appreciate the help on this. Thanks all helpers. *********** I am sorry I have just added notation for the theorem I brought here","I recently met this problem from Folland's real analysis second edition involving a specific question on distributions (exercise 19 page 299) which reads as follows: On let where PV stands for ""Principle Value"" and defined as follows: for all . Also for we define , and a. We are to prove in the weak topology* on (distributions on Schwartz class of functions where we define the weak topology in the usual point-wise convergence sense). As hint we are told to use the theorem below the question with a=0. b. We are to prove that (Hint : ). c. We are to prove that and hence . d. From part c it follows that . We are to prove this directly by showing where we define to be if and 0 otherwise, and via the exercise at the bottom. e. We are to compute (i) By writing and by using part c (ii) By using and by using b. The theorem instructed to use (Notation used here is for we define ) : The exercise instructed to use: Here are where my problems are: I cannot seem to tackle any of parts a,b,c,d and also part e, as simple as it might sound, I tried doing but always ended up getting some close result but with something wrong. So I really need the help on this in order to do it, I realize it is a long question but I tried asking two people I know and they could not help me either, and of course I appreciate the help on this. Thanks all helpers. *********** I am sorry I have just added notation for the theorem I brought here"," R   F_0 = PV(\frac{1}{x})   \langle PV(f),\phi\rangle = \lim_{\epsilon \to 0^+} \int_{|x|>\epsilon} f(x)\phi(x) \, dx   \phi \in C_C^\infty   \epsilon > 0   F_\epsilon(x) = x(x^2+\epsilon^2)^{-1}   G_\epsilon^\pm(x)=(x \pm i\epsilon)^{-1}   S_\epsilon(x) = \operatorname{sgn}(x) e^{-2 \pi \epsilon |x|}   \lim_{\epsilon \to 0} F_\epsilon=F_0   \mathcal{S}'   \lim_{\epsilon \to 0} G_\epsilon^\pm = F_0 \mp i \pi \delta  (x \pm i\epsilon )^{-1} = (x \mp i\epsilon)(x^2+\epsilon ^2)^{-1}   \widehat{S}_\epsilon = (i\pi)^{-1} F_\epsilon   \widehat{\operatorname{sgn}} = (i\pi)^{-1}F_0   \widehat{F}_0 = -i\pi \operatorname{sgn}   \lim_{\epsilon \to 0 , N \to \infty } H_{\epsilon,N} = F_0   H_{\epsilon,N}   \frac{1}{x}   \epsilon < |x| < N   \widehat{\chi}_{(0,\infty)}  \chi = \frac{1}{2} \operatorname{sgn} + \frac{1}{2}   \chi(x) = \lim_{\epsilon \to 0} e^{-x\epsilon} \chi_{(0,\infty )}   \phi \in \mathbb R^n   \phi_t(x) = t^{-n} \phi(t^{-1}x) ","['real-analysis', 'measure-theory', 'fourier-analysis', 'distribution-theory', 'weak-convergence']"
82,Which of the following condition ensure that the function $f:R^n\to R$ is continuous?,Which of the following condition ensure that the function  is continuous?,f:R^n\to R,"I encountered an interesting problem in my Economics class about continuity. Which of the following conditions on the function $f:\mathbb R^n\to \mathbb R$ ensures that the function $f$ is continuous? For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x) > y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x) \ge y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x)> y\}$ are closed; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x)\ge y\}$ are closed; I have proved using contradiction that 3, 4 implies continuity. The proof follows as below: Suppose we assume 3, and suppose to the contrary that $f$ is not continuous. Then there exists sequence $x_n\to x_0$ such that $f(x_n) \not\to f(x_0)$. So given $\epsilon>0$, there exists some $N\in \mathbb{N}$ such that for all $n > N$, $|f(x_n)-f(x_0)|>\epsilon$. The last inequality is equivalent to $f(x_n)<f(x_0)-\epsilon$ or $f(x_n)>f(x_0)+\epsilon$. Now from the assumption 3, the sets $\{x:x\in R^n, f(x) < f(x_0)\}$ and $\{x:x\in R^n, f(x)> f(x_0)\}$ are closed. Hence, it contains all its limit points. Without loss of generality, let us consider just left case. Since $f(x_n)<f(x_0)-\epsilon<f(x_0)$ for all $n>N$, the limit point of $x_n$ which is $x_0\in \{x:x\in R^n, f(x) < f(x_0)\}$. This implies $f(x_0)<f(x_0)-\epsilon$, a contradiction. But I was not quite sure how to prove/disprove 1, 2 implies continuity. Number 1 seems close to topological definition of continuity, but it is weak (I think). Any helps, hints, or counter examples will be appreciated.","I encountered an interesting problem in my Economics class about continuity. Which of the following conditions on the function $f:\mathbb R^n\to \mathbb R$ ensures that the function $f$ is continuous? For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x) > y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x) \ge y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x)> y\}$ are closed; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x)\ge y\}$ are closed; I have proved using contradiction that 3, 4 implies continuity. The proof follows as below: Suppose we assume 3, and suppose to the contrary that $f$ is not continuous. Then there exists sequence $x_n\to x_0$ such that $f(x_n) \not\to f(x_0)$. So given $\epsilon>0$, there exists some $N\in \mathbb{N}$ such that for all $n > N$, $|f(x_n)-f(x_0)|>\epsilon$. The last inequality is equivalent to $f(x_n)<f(x_0)-\epsilon$ or $f(x_n)>f(x_0)+\epsilon$. Now from the assumption 3, the sets $\{x:x\in R^n, f(x) < f(x_0)\}$ and $\{x:x\in R^n, f(x)> f(x_0)\}$ are closed. Hence, it contains all its limit points. Without loss of generality, let us consider just left case. Since $f(x_n)<f(x_0)-\epsilon<f(x_0)$ for all $n>N$, the limit point of $x_n$ which is $x_0\in \{x:x\in R^n, f(x) < f(x_0)\}$. This implies $f(x_0)<f(x_0)-\epsilon$, a contradiction. But I was not quite sure how to prove/disprove 1, 2 implies continuity. Number 1 seems close to topological definition of continuity, but it is weak (I think). Any helps, hints, or counter examples will be appreciated.",,"['real-analysis', 'general-topology', 'continuity']"
83,When are $\frac{1}{|x|^s}$ and $\log|x|$ integrable near the origin?,When are  and  integrable near the origin?,\frac{1}{|x|^s} \log|x|,"When are $\frac{1}{|x|^s}$ for $s>0$ and $\log|x|$ integrable near the origin? I'm reading Evans PDE and in the construction of the fundamental solution of Poisson's equation, he defines $$ \Phi(x) = C \log|x| $$ for $\mathbb{R}^2$ and a suitable $C$, and $$ \Phi(x) = C \frac{1}{|x|^{n-2}} $$ for $\mathbb{R}^n$ with $n\geq 3$ and $C=C(n)$ a suitable constant. The construction then goes about defining $\Phi * f$ for an $f \in C^2_c$. Clearly if $\Phi \in L^1_{\mathrm{loc}}$, then the convolution will be finite a.e., but I worry about  whether this holds because of the blow up near $0$. In general in what dimensions are $$ \log |x|,~~~~~\frac{1}{|x|^s} $$ integrable in a ball around $0$?","When are $\frac{1}{|x|^s}$ for $s>0$ and $\log|x|$ integrable near the origin? I'm reading Evans PDE and in the construction of the fundamental solution of Poisson's equation, he defines $$ \Phi(x) = C \log|x| $$ for $\mathbb{R}^2$ and a suitable $C$, and $$ \Phi(x) = C \frac{1}{|x|^{n-2}} $$ for $\mathbb{R}^n$ with $n\geq 3$ and $C=C(n)$ a suitable constant. The construction then goes about defining $\Phi * f$ for an $f \in C^2_c$. Clearly if $\Phi \in L^1_{\mathrm{loc}}$, then the convolution will be finite a.e., but I worry about  whether this holds because of the blow up near $0$. In general in what dimensions are $$ \log |x|,~~~~~\frac{1}{|x|^s} $$ integrable in a ball around $0$?",,"['real-analysis', 'partial-differential-equations', 'lebesgue-integral']"
84,Assume that $f_n\to f$ in measure and $\sup_n\|f_n\|_{L^p(E)}<\infty$ for some $p>1$. Prove that $f_n$ converges to $f$ in $L^1$ norm.,Assume that  in measure and  for some . Prove that  converges to  in  norm.,f_n\to f \sup_n\|f_n\|_{L^p(E)}<\infty p>1 f_n f L^1,"Let $\{f_n\}$ and $f$ be Lebesgue measurable functions on $E$ where $|E|<\infty$ . Assume that $f_n\to f$ in measure and $\sup_n\|f_n\|_{L^p(E)}<\infty$ for some $p>1$ . (a) Prove that $f_n$ converges to $f$ in $L^1$ norm. (b) Show by counterexample that this convergence may no longer hold if you replace the $L^p$ condition with $\sup_n \|f_n\|_{L^1(E)}$ . I can deal with the show by counterexample part. Let $f_n(x)=n\chi_{[0,\frac{1}{n}]}$ and $E=[0,1]$ , then $f_n(x)\to 0$ in measure. However, $\int_0^1 f_n(x)-f(x)dx=1$ . I am stuck in proving part (a), I tried to use Egorov theorem, which requires $f_n\to f$ a.e. Since $f_n \to f$ in measure, then there is a subsequence $f_{n_k}$ converge a.e. Then $f_{n_k}$ converges uniformly in a compact set $F$ . So $$\int_{E}|f_{n_k}(x)-f(x)|dx=\int_{F}|f_{n_k}(x)-f(x)|dx+\int_{E\backslash F}|f_{n_k}(x)-f(x)|dx$$ By the uniformly convergence of $f_{n_k}$ on $F$ , then $\int_{F}|f_{n_k}(x)-f(x)|dx<\epsilon M$ . I have trouble with the $\int_{E\backslash F}|f_{n_k}(x)-f(x)|dx$ part. $\int_{E\backslash F}|f_{n_k}(x)-f(x)|dx<\|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)}\|\Bbb{1}\|_{L^q(E\backslash F)}<\epsilon \|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)} $ by Holder's inequality. Then how to show $\|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)}$ is actually bounded by the condition given in the problem? And if $\|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)}$ is bounded, it only proves the subsequence $f_{n_k}$ converges in $L^1$ norm, how to show the $f_n$ also converges in $L^1$ norm? Could someone kindly help? Thanks!","Let and be Lebesgue measurable functions on where . Assume that in measure and for some . (a) Prove that converges to in norm. (b) Show by counterexample that this convergence may no longer hold if you replace the condition with . I can deal with the show by counterexample part. Let and , then in measure. However, . I am stuck in proving part (a), I tried to use Egorov theorem, which requires a.e. Since in measure, then there is a subsequence converge a.e. Then converges uniformly in a compact set . So By the uniformly convergence of on , then . I have trouble with the part. by Holder's inequality. Then how to show is actually bounded by the condition given in the problem? And if is bounded, it only proves the subsequence converges in norm, how to show the also converges in norm? Could someone kindly help? Thanks!","\{f_n\} f E |E|<\infty f_n\to f \sup_n\|f_n\|_{L^p(E)}<\infty p>1 f_n f L^1 L^p \sup_n \|f_n\|_{L^1(E)} f_n(x)=n\chi_{[0,\frac{1}{n}]} E=[0,1] f_n(x)\to 0 \int_0^1 f_n(x)-f(x)dx=1 f_n\to f f_n \to f f_{n_k} f_{n_k} F \int_{E}|f_{n_k}(x)-f(x)|dx=\int_{F}|f_{n_k}(x)-f(x)|dx+\int_{E\backslash F}|f_{n_k}(x)-f(x)|dx f_{n_k} F \int_{F}|f_{n_k}(x)-f(x)|dx<\epsilon M \int_{E\backslash F}|f_{n_k}(x)-f(x)|dx \int_{E\backslash F}|f_{n_k}(x)-f(x)|dx<\|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)}\|\Bbb{1}\|_{L^q(E\backslash F)}<\epsilon \|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)}  \|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)} \|f_{n_k}(x)-f(x)\|_{L^p(E\backslash F)} f_{n_k} L^1 f_n L^1","['real-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
85,Intuition behind uniformly continuous functions,Intuition behind uniformly continuous functions,,"I'm trying to have an intuition behind the uniformly continuous functions. Something to show to my students. For example, before giving the formal definition and some examples of continuous functions, we can say to beginners that roughly speaking, continuous functions are functions without ""holes"", that's why the name ""continuous"". What about uniformly continuous functions? is there some ideas to give to the students to give them an intuition behind these functions, before to show to them the formal definition? Thanks","I'm trying to have an intuition behind the uniformly continuous functions. Something to show to my students. For example, before giving the formal definition and some examples of continuous functions, we can say to beginners that roughly speaking, continuous functions are functions without ""holes"", that's why the name ""continuous"". What about uniformly continuous functions? is there some ideas to give to the students to give them an intuition behind these functions, before to show to them the formal definition? Thanks",,['real-analysis']
86,"Prove that if $\sum a_n$ converges absolutely, then $\sum a_{2n}$ converges.","Prove that if  converges absolutely, then  converges.",\sum a_n \sum a_{2n},"In posting this question, I noticed a lot of 'similar' threads pop up, but felt that they required a fundamentally different approach. If any of you feel differently, please feel free to vote this thread as a duplicate and I will delete it. Here is my approach , I would appreciate help/correction where relevant, as I am unsure of how robust my answer is: If $\sum a_n$ converges absolutely, then we have that $\sum |a_n|$ converges. This implies that $\sum a_n$ also converges, and that the sequence $(a_n)_{n \in \mathbb{N}}\to 0$.$^{(1)}$ This means that the sequence $(a_n)$ is bounded and monotone (decreasing), and as such convergent. Looking at $(a_{2n})$ we notice it is a sub-sequence of $(a_n)$ and converges to the same value (Bolzano-Weiestrass).$^{(2)}$ Then the partial sums $(s_{2n}) \to A$ and thus $\sum a_{2n} = A$ Is this sufficient? I feel it is a little wishy/washy in using bolzano-weistrass. Do I need to formally show $(a_{2n})$ is a sub-sequence of $(a_n)$ Any tips/hints/corrections are greatly appreciated. $^{(1),}$$^{(2)}$ - I am not required to show these results, as quoting the theorem from my notes is considered sufficient by my professor.","In posting this question, I noticed a lot of 'similar' threads pop up, but felt that they required a fundamentally different approach. If any of you feel differently, please feel free to vote this thread as a duplicate and I will delete it. Here is my approach , I would appreciate help/correction where relevant, as I am unsure of how robust my answer is: If $\sum a_n$ converges absolutely, then we have that $\sum |a_n|$ converges. This implies that $\sum a_n$ also converges, and that the sequence $(a_n)_{n \in \mathbb{N}}\to 0$.$^{(1)}$ This means that the sequence $(a_n)$ is bounded and monotone (decreasing), and as such convergent. Looking at $(a_{2n})$ we notice it is a sub-sequence of $(a_n)$ and converges to the same value (Bolzano-Weiestrass).$^{(2)}$ Then the partial sums $(s_{2n}) \to A$ and thus $\sum a_{2n} = A$ Is this sufficient? I feel it is a little wishy/washy in using bolzano-weistrass. Do I need to formally show $(a_{2n})$ is a sub-sequence of $(a_n)$ Any tips/hints/corrections are greatly appreciated. $^{(1),}$$^{(2)}$ - I am not required to show these results, as quoting the theorem from my notes is considered sufficient by my professor.",,"['real-analysis', 'sequences-and-series']"
87,Proving that a set isn't dense,Proving that a set isn't dense,,"Let $A$ be a set of real numbers such that $A \subseteq [0,1]$. I'm having a hard time proving that $C=\left\{\frac{a+1}{n^2} \colon a \in A, n \in \mathbb{N} \right\}$ is not dense in $[0,1]$. How should I approach this? I know that I must find an interval $(x,y) \subseteq [0,1]$ such that $C \cap (x,y) = \phi $. I tried demanding $\frac{a+1}{n^2}<x$ or $\frac{a+1}{n^2}>y$ for every $a \in A$ and $n \in \mathbb{N}$. The problem is that I know nothing about $A$. I'd appreciate any suggestions.","Let $A$ be a set of real numbers such that $A \subseteq [0,1]$. I'm having a hard time proving that $C=\left\{\frac{a+1}{n^2} \colon a \in A, n \in \mathbb{N} \right\}$ is not dense in $[0,1]$. How should I approach this? I know that I must find an interval $(x,y) \subseteq [0,1]$ such that $C \cap (x,y) = \phi $. I tried demanding $\frac{a+1}{n^2}<x$ or $\frac{a+1}{n^2}>y$ for every $a \in A$ and $n \in \mathbb{N}$. The problem is that I know nothing about $A$. I'd appreciate any suggestions.",,['real-analysis']
88,Calculate integral with cantor measure,Calculate integral with cantor measure,,"Calculate the integral $$\int_{[0,1]}x^2d\mu_F$$ where F is the cantor function. Use the following hints about the cantor function: $F(1-x)=1-F(x)$ $F(\frac x 3)=\frac{F(x)}{2}\quad\forall x\in[0,1]$ $F(0)=0$ I thought that $$\int_{[0,1]}x^2d\mu_F=\int_{[1,0]}(1-x)^2d\mu_{F}=\int_{[0,1]}x^2d\mu_{1-F(x)}$$ but here I'm stuck and I don't know how to continue calculating this integral. Furthermore, how do we use the second and third properties when given the cantor function above?","Calculate the integral where F is the cantor function. Use the following hints about the cantor function: I thought that but here I'm stuck and I don't know how to continue calculating this integral. Furthermore, how do we use the second and third properties when given the cantor function above?","\int_{[0,1]}x^2d\mu_F F(1-x)=1-F(x) F(\frac x 3)=\frac{F(x)}{2}\quad\forall x\in[0,1] F(0)=0 \int_{[0,1]}x^2d\mu_F=\int_{[1,0]}(1-x)^2d\mu_{F}=\int_{[0,1]}x^2d\mu_{1-F(x)}","['real-analysis', 'integration']"
89,Prove a Continuous Distribution Function is Uniformly Continuous,Prove a Continuous Distribution Function is Uniformly Continuous,,"Let $F$ be the distribution function for a random variable $X$ and it is given that $F$ is continuous over the entire real line. Prove that $F$ is uniformly continuous over the real line. My approach was something like this: Take any two points $x$ and $y$ in $\mathbb R$. If $|x-y|>1$ then trivially $|F(x)-F(y)|\leq1<|x-y|$ hence $F$ is Lipschitz whenever $|x-y|>1$. So for all such points $x,y$ where $|x-y|>1$, $F$ is uniformly continuous. However, I am stumped in the case where $|x-y|<1$. How do we finc a $\delta$ such that $|x-y|<\delta\implies|F(x)-F(y)|<\epsilon$, given any $\epsilon>0$? Please give me some hint(s).","Let $F$ be the distribution function for a random variable $X$ and it is given that $F$ is continuous over the entire real line. Prove that $F$ is uniformly continuous over the real line. My approach was something like this: Take any two points $x$ and $y$ in $\mathbb R$. If $|x-y|>1$ then trivially $|F(x)-F(y)|\leq1<|x-y|$ hence $F$ is Lipschitz whenever $|x-y|>1$. So for all such points $x,y$ where $|x-y|>1$, $F$ is uniformly continuous. However, I am stumped in the case where $|x-y|<1$. How do we finc a $\delta$ such that $|x-y|<\delta\implies|F(x)-F(y)|<\epsilon$, given any $\epsilon>0$? Please give me some hint(s).",,"['real-analysis', 'probability', 'analysis', 'continuity', 'uniform-continuity']"
90,Showing that a function is in $L^1$,Showing that a function is in,L^1,"I need to prove the following statement or find a counter-example: Let $u\in L^1\cap C^2$ with $u''\in L^1$. Then $u'\in L^1$. Unfortunately, I have no idea how to prove or disprove it, since the $|\bullet|$ in the definition of $L^1$ is giving me huge problems. I found counter-examples if either $u\notin L^1$ or $u''\notin L^1$, but none of them could be generalized to a example that satisfies all the conditions.","I need to prove the following statement or find a counter-example: Let $u\in L^1\cap C^2$ with $u''\in L^1$. Then $u'\in L^1$. Unfortunately, I have no idea how to prove or disprove it, since the $|\bullet|$ in the definition of $L^1$ is giving me huge problems. I found counter-examples if either $u\notin L^1$ or $u''\notin L^1$, but none of them could be generalized to a example that satisfies all the conditions.",,"['real-analysis', 'functional-analysis', 'lebesgue-integral']"
91,How I can simplify this inequality or how I can solve it?,How I can simplify this inequality or how I can solve it?,,How I can simplify this inequality or how I can solve it: $$\left\lceil\dfrac{\ln(t+2)}{\ln 2}\right\rceil-\left\lfloor\dfrac{\ln(t+1)}{\ln2}\right\rfloor>1$$ where $t$ is a positive integer. Here $\lceil\cdot\rceil$ and $\lfloor\cdot\rfloor$ are respectively the Ceiling and the Floor functions. I have no idea to start.,How I can simplify this inequality or how I can solve it: $$\left\lceil\dfrac{\ln(t+2)}{\ln 2}\right\rceil-\left\lfloor\dfrac{\ln(t+1)}{\ln2}\right\rfloor>1$$ where $t$ is a positive integer. Here $\lceil\cdot\rceil$ and $\lfloor\cdot\rfloor$ are respectively the Ceiling and the Floor functions. I have no idea to start.,,"['real-analysis', 'functions', 'special-functions']"
92,Understanding the proof of an Ergodic theorem for Markov chains,Understanding the proof of an Ergodic theorem for Markov chains,,"An ergodic theorem for Markov chains is as follows. If a Markov chain $(X_n)_{n \ge 0}$ is irreducible and has an invariant distribution $\pi$, then   $$\frac{1}{n} \sum_{k=0}^{n-1} f(X_k) \to \overline{f}:=\sum_{i \in I} \pi_i f(i)$$   almost surely as $n \to \infty$, for any bounded function $f:I \to \mathbb{R}$. I have attached the proof that appears in Norris's text below. Here, $V_i(n):= \sum_{k=0}^{n-1} \mathbf{1}_{\{X_k=i\}}$ denotes the number of visits to state $i$ before time $n$. I do not understand the step $$\sum_{i \in J} \left|\frac{V_i(n)}{n}-\pi_i\right| + \sum_{i \notin J} \left(\frac{V_i(n)}{n}+\pi_i\right)\le 2\sum_{i \in J}\left|\frac{V_i(n)}{n}-\pi_i\right| + 2 \sum_{i \notin J} \pi_i.$$ After rearranging, I see that this is equivalent to showing that $$\sum_{i \in J} \left|\frac{V_i(n)}{n}-\pi_i\right| \ge \sum_{i \notin J} \left(\frac{V_i(n)}{n}-\pi_i\right),$$ but I don't see why this is true either. Why was it necessary to examine a subset $J \subset I$? What is wrong with the following? $$\left|\frac{1}{n}\sum_{k=0}^{n-1} f(X_k)-\overline{f}\right| \le \sum_{i \in I} \left|\frac{V_i(n)}{n}-\pi_i\right|<\epsilon,$$ for $n$ large enough? Norris's proof.","An ergodic theorem for Markov chains is as follows. If a Markov chain $(X_n)_{n \ge 0}$ is irreducible and has an invariant distribution $\pi$, then   $$\frac{1}{n} \sum_{k=0}^{n-1} f(X_k) \to \overline{f}:=\sum_{i \in I} \pi_i f(i)$$   almost surely as $n \to \infty$, for any bounded function $f:I \to \mathbb{R}$. I have attached the proof that appears in Norris's text below. Here, $V_i(n):= \sum_{k=0}^{n-1} \mathbf{1}_{\{X_k=i\}}$ denotes the number of visits to state $i$ before time $n$. I do not understand the step $$\sum_{i \in J} \left|\frac{V_i(n)}{n}-\pi_i\right| + \sum_{i \notin J} \left(\frac{V_i(n)}{n}+\pi_i\right)\le 2\sum_{i \in J}\left|\frac{V_i(n)}{n}-\pi_i\right| + 2 \sum_{i \notin J} \pi_i.$$ After rearranging, I see that this is equivalent to showing that $$\sum_{i \in J} \left|\frac{V_i(n)}{n}-\pi_i\right| \ge \sum_{i \notin J} \left(\frac{V_i(n)}{n}-\pi_i\right),$$ but I don't see why this is true either. Why was it necessary to examine a subset $J \subset I$? What is wrong with the following? $$\left|\frac{1}{n}\sum_{k=0}^{n-1} f(X_k)-\overline{f}\right| \le \sum_{i \in I} \left|\frac{V_i(n)}{n}-\pi_i\right|<\epsilon,$$ for $n$ large enough? Norris's proof.",,"['real-analysis', 'probability', 'markov-chains', 'markov-process', 'ergodic-theory']"
93,Show that there is a continuous $g$ with compact support,Show that there is a continuous  with compact support,g,"If $f$ is a measurable complex function (that means that it doesn't take the values $\pm \infty$) with compact support, then for each $\epsilon >0$ there is a continuous $g$ with compact support so that $m(\{f\neq g\})<\epsilon$. Could you give me some hints how I could show that?? EDIT: Do we have to prove it as followed?? If $f$ is measurable and finite in $\mathbb{R}^d$ then for each $\epsilon>0$ there is a closed $E$ with $m(\mathbb{R}^d \setminus E)<\epsilon$ so that $f|_E$ is continuous. It is enough to show this in the case in which $f$ is defined in an open bounded cube $Q$ and it is bounded. Then it is integrable, so there is a sequence of continuous functions $g_n$ so that $||g_n-f||_1 \rightarrow 0$, so there is a subsequence $g_{k_n}$ with $g_{k_n} \rightarrow f$ almost everywhere, so from Egorov theorem, there is $A$ with $m(Q\setminus A)<\epsilon /2$ so that $g_{k_n} \rightarrow f$ uniformly in $A$. The desired $E$ is a closed subset of $A$ with $m(A\setminus E)<\epsilon /2$.","If $f$ is a measurable complex function (that means that it doesn't take the values $\pm \infty$) with compact support, then for each $\epsilon >0$ there is a continuous $g$ with compact support so that $m(\{f\neq g\})<\epsilon$. Could you give me some hints how I could show that?? EDIT: Do we have to prove it as followed?? If $f$ is measurable and finite in $\mathbb{R}^d$ then for each $\epsilon>0$ there is a closed $E$ with $m(\mathbb{R}^d \setminus E)<\epsilon$ so that $f|_E$ is continuous. It is enough to show this in the case in which $f$ is defined in an open bounded cube $Q$ and it is bounded. Then it is integrable, so there is a sequence of continuous functions $g_n$ so that $||g_n-f||_1 \rightarrow 0$, so there is a subsequence $g_{k_n}$ with $g_{k_n} \rightarrow f$ almost everywhere, so from Egorov theorem, there is $A$ with $m(Q\setminus A)<\epsilon /2$ so that $g_{k_n} \rightarrow f$ uniformly in $A$. The desired $E$ is a closed subset of $A$ with $m(A\setminus E)<\epsilon /2$.",,"['real-analysis', 'measure-theory']"
94,Integral of bounded continuous function on $R$,Integral of bounded continuous function on,R,"Let $f$ be a bounded continuous function on $R$. Prove that $$\lim_{n \to \infty} \frac{n}{\pi} \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=f(0)$$ I solved this question as follows, but I ran into a problem: Solution: Since $f$ is continuous at $0$, given $\epsilon>0$, there is $\delta >0$ such that $|f(x)-f(0)| \leq \epsilon, \forall x \in [-\delta \hspace{2 mm} \delta]$. Now consider $f_{n}(x)=\frac{nf(x)}{1+n^{2}x^{2}}$,$n \in N$, is a sequence of continuous function that converges to $0$ on $R-[-\delta \hspace{2 mm} \delta]$ when $n \to \infty$. Also, according to guestion, $f$ is bounded (i.e.$|f_{n}(x)|<g(x)$). Therefore according to Dominated Convergence Theorem   $$\lim_{n \to \infty} \frac{n}{\pi} \int_{ R-[-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} \frac{n}{\pi} \int_{ R-[-\delta \hspace{2 mm} \delta]} 0 \hspace{2 mm} dt=0$$ Now I have a problem to solve the second part. I want to prove $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(t)}{1+n^{2}t^{2}} dt =  \lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt $ which $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt =\pi f(0)$ If I can prove that, the rest of question is easy, because I can say: $\lim_{n \to \infty} n \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} n \int_{ R-[-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt+\lim_{n \to \infty} n \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} n \int_{ [-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt=\pi f(0)$ Now how can I prove: $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(t)}{1+n^{2}t^{2}} dt =  \lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt $","Let $f$ be a bounded continuous function on $R$. Prove that $$\lim_{n \to \infty} \frac{n}{\pi} \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=f(0)$$ I solved this question as follows, but I ran into a problem: Solution: Since $f$ is continuous at $0$, given $\epsilon>0$, there is $\delta >0$ such that $|f(x)-f(0)| \leq \epsilon, \forall x \in [-\delta \hspace{2 mm} \delta]$. Now consider $f_{n}(x)=\frac{nf(x)}{1+n^{2}x^{2}}$,$n \in N$, is a sequence of continuous function that converges to $0$ on $R-[-\delta \hspace{2 mm} \delta]$ when $n \to \infty$. Also, according to guestion, $f$ is bounded (i.e.$|f_{n}(x)|<g(x)$). Therefore according to Dominated Convergence Theorem   $$\lim_{n \to \infty} \frac{n}{\pi} \int_{ R-[-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} \frac{n}{\pi} \int_{ R-[-\delta \hspace{2 mm} \delta]} 0 \hspace{2 mm} dt=0$$ Now I have a problem to solve the second part. I want to prove $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(t)}{1+n^{2}t^{2}} dt =  \lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt $ which $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt =\pi f(0)$ If I can prove that, the rest of question is easy, because I can say: $\lim_{n \to \infty} n \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} n \int_{ R-[-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt+\lim_{n \to \infty} n \int_{ R} \frac {f(t)}{1+n^{2}t^{2}} dt=\lim_{n \to \infty} n \int_{ [-\delta \hspace{2 mm} \delta]} \frac {f(t)}{1+n^{2}t^{2}} dt=\pi f(0)$ Now how can I prove: $\lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(t)}{1+n^{2}t^{2}} dt =  \lim_{n \to \infty} \int_{ -\delta}^{\delta} \frac {nf(0)}{1+n^{2}t^{2}} dt $",,['real-analysis']
95,A pseudometric on the space of the measurable functions is complete,A pseudometric on the space of the measurable functions is complete,,"I'm working in the following exercise: Suppose $(X, \mathcal A, \mu)$ is a finite measure space and suppose $\mathcal F$ is the set of all $\mathcal A$-measurable functions $f: X \rightarrow \mathbb R$. For $f, g \in \mathcal F$, let $$d(f, g)=\int_X\frac{|f-g|}{1+|f-g|}d\mu.$$ Show that: $d(f, g)=0$ if and only if $f=g$ almost everywhere. $d(f, g)=d(g, f)$ $d(f, g)\leq d(f, h)+d(h, g)$ If $f_n$ is a sequence in $\mathcal F$ and if $f \in \mathcal F$ then $d(f_n, f)\rightarrow 0$ if and only if for every $\delta>0$ the following holds: $$\lim_{n\rightarrow\infty}\mu(\{x \in X: |f_n(x)-f(x)|\geq\delta\})=0.$$ If $f_n$ is a Cauchy sequence on $\mathcal F$ then there exists $f \in \mathcal F$ such that $d(f_n, f)\rightarrow 0$. i.e., I must show that this thing is a complete pseudometric space. I have already done 1. 2. 3. 4., but I'm stuck on 5. I don't know which $f$ should the $f_n$ converge to.","I'm working in the following exercise: Suppose $(X, \mathcal A, \mu)$ is a finite measure space and suppose $\mathcal F$ is the set of all $\mathcal A$-measurable functions $f: X \rightarrow \mathbb R$. For $f, g \in \mathcal F$, let $$d(f, g)=\int_X\frac{|f-g|}{1+|f-g|}d\mu.$$ Show that: $d(f, g)=0$ if and only if $f=g$ almost everywhere. $d(f, g)=d(g, f)$ $d(f, g)\leq d(f, h)+d(h, g)$ If $f_n$ is a sequence in $\mathcal F$ and if $f \in \mathcal F$ then $d(f_n, f)\rightarrow 0$ if and only if for every $\delta>0$ the following holds: $$\lim_{n\rightarrow\infty}\mu(\{x \in X: |f_n(x)-f(x)|\geq\delta\})=0.$$ If $f_n$ is a Cauchy sequence on $\mathcal F$ then there exists $f \in \mathcal F$ such that $d(f_n, f)\rightarrow 0$. i.e., I must show that this thing is a complete pseudometric space. I have already done 1. 2. 3. 4., but I'm stuck on 5. I don't know which $f$ should the $f_n$ converge to.",,['real-analysis']
96,"For $E \subseteq [0, 1]$, $m(E) > 0$, show that there are $\alpha$ and $\beta$ such that $\alpha, \alpha + \beta, \alpha + 2\beta \in E$.","For , , show that there are  and  such that .","E \subseteq [0, 1] m(E) > 0 \alpha \beta \alpha, \alpha + \beta, \alpha + 2\beta \in E","This was originally a proof verification question, but I have since moved the proof to an answer as discussed on meta . I still welcome comments on the proof as well as any alternative proofs. Let $E$ be a Lebesgue measurable subset of $[0, 1]$ with positive measure. Show that there are $\alpha$ and $\beta$ such that $\alpha, \alpha + \beta, \alpha + 2\beta \in E$. The only idea I have had is to use Lebesgue density and the Lebesgue Density Theorem, but so far no luck.","This was originally a proof verification question, but I have since moved the proof to an answer as discussed on meta . I still welcome comments on the proof as well as any alternative proofs. Let $E$ be a Lebesgue measurable subset of $[0, 1]$ with positive measure. Show that there are $\alpha$ and $\beta$ such that $\alpha, \alpha + \beta, \alpha + 2\beta \in E$. The only idea I have had is to use Lebesgue density and the Lebesgue Density Theorem, but so far no luck.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
97,Prove that $\ell ^1(\mathbb{N})$ is a Banach space.,Prove that  is a Banach space.,\ell ^1(\mathbb{N}),"I'm trying to prove that $\ell^1(\mathbb{N}) := \left\{ (x_n)_{n=1}^{\infty} : \sum\limits_{n=1}^{\infty}\left|x_n\right| < \infty \right\} $, the space of all sequences over the field $\mathbb{C}$ that converge absolutely, is a Banach space with respect to the norm $\left|\left|(x_n)\right|\right|_{1} :=  \sum\limits_{n=1}^{\infty}\left|x_n\right|$. Proof thus far: Suppose $(x_n)\in L^1(\mathbb{N})$  and let $X^{(l)} := \left\{(x_{n})_{l}\right\}^{\infty}_{l=1} \subset \ell^1(\mathbb{N}) $ denote a Cauchy sequence. Therefore, $\forall ~ \epsilon > 0, \exists ~ N \in \mathbb{N} : \left|\left| X^{(l)} - X^{(m)}\right|\right|_{1} < \epsilon   \space\space \forall ~ l,m \ge N $. By definition, $\left|\left| X^{(l)} - X^{(m)}\right|\right|_{1} = \sum\limits_{k=1}^{\infty} \left|x^{(l)}_k - x^{(m)}_k\right| \le \epsilon$ So, $\left| \sum\limits_{k=1}^{\infty} \left(x^{(l)}_k - x^{(m)}_k\right) \right| \leq  \sum\limits_{k=1}^{\infty} \left|x^{(l)}_k - x^{(m)}_k\right| \le \epsilon$ We then have,  $\forall ~ \epsilon > 0, \exists ~ N \in \mathbb{N} : \left| \sum\limits_{k=1}^{\infty} x^{(l)}_k - x^{(m)}_k\right| < \epsilon \space\space  \forall ~ l,m \ge N $ The summation is over each element of the Cauchy sequence $X^{(l)}$, which are themselves Cauchy sequences in $\mathbb{C}$ and so therefore must converge (since $\mathbb{C}$ is a Banach space). I'm not sure where to go from here though, how to relate that back to the original Cauchy sequence in $L^1(\mathbb{N})$ to show that it converges. Or even what I've done already is even correct.","I'm trying to prove that $\ell^1(\mathbb{N}) := \left\{ (x_n)_{n=1}^{\infty} : \sum\limits_{n=1}^{\infty}\left|x_n\right| < \infty \right\} $, the space of all sequences over the field $\mathbb{C}$ that converge absolutely, is a Banach space with respect to the norm $\left|\left|(x_n)\right|\right|_{1} :=  \sum\limits_{n=1}^{\infty}\left|x_n\right|$. Proof thus far: Suppose $(x_n)\in L^1(\mathbb{N})$  and let $X^{(l)} := \left\{(x_{n})_{l}\right\}^{\infty}_{l=1} \subset \ell^1(\mathbb{N}) $ denote a Cauchy sequence. Therefore, $\forall ~ \epsilon > 0, \exists ~ N \in \mathbb{N} : \left|\left| X^{(l)} - X^{(m)}\right|\right|_{1} < \epsilon   \space\space \forall ~ l,m \ge N $. By definition, $\left|\left| X^{(l)} - X^{(m)}\right|\right|_{1} = \sum\limits_{k=1}^{\infty} \left|x^{(l)}_k - x^{(m)}_k\right| \le \epsilon$ So, $\left| \sum\limits_{k=1}^{\infty} \left(x^{(l)}_k - x^{(m)}_k\right) \right| \leq  \sum\limits_{k=1}^{\infty} \left|x^{(l)}_k - x^{(m)}_k\right| \le \epsilon$ We then have,  $\forall ~ \epsilon > 0, \exists ~ N \in \mathbb{N} : \left| \sum\limits_{k=1}^{\infty} x^{(l)}_k - x^{(m)}_k\right| < \epsilon \space\space  \forall ~ l,m \ge N $ The summation is over each element of the Cauchy sequence $X^{(l)}$, which are themselves Cauchy sequences in $\mathbb{C}$ and so therefore must converge (since $\mathbb{C}$ is a Banach space). I'm not sure where to go from here though, how to relate that back to the original Cauchy sequence in $L^1(\mathbb{N})$ to show that it converges. Or even what I've done already is even correct.",,"['real-analysis', 'banach-spaces', 'lp-spaces']"
98,Rudin's 'Principle of Mathematical Analysis' Exercise 3.14,Rudin's 'Principle of Mathematical Analysis' Exercise 3.14,,"Since I'm studying real analysis using this book by myself, I'm not sure whether or not my method to prove convergence of sequence is right. I'm working on the above question's (d), and my solution was kind of different from the solution provided by the following website. The solution in the website is much more beautiful and more concise than mine, but I believe my method also works. http://minds.wisconsin.edu/handle/1793/67009 3.14 If $\{s_n\}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n = \dfrac{s_0+s_1+...+s_n}{n+1} (n=0,1,2,...).$$ (d) Put $a_n=s_n-s_{n-1}$, for $n\geq 1$. Show that $$s_n-\sigma_n=\dfrac{1}{n+1}\sum_{k=1}^n k a_n$...$(1)$$ Assume that lim$(n a_n)=0$ and that ${\sigma_n}$ converges. Prove that $\{s_n\}$ converges. My solution: Since $\sigma_n$ converges, it is a Cauchy sequence. So, $\forall\epsilon,\forall\delta>0, \exists N\in\mathbb{N}$ s.t.$\forall n, \forall m> N , |n a_n-0|<\epsilon$ and  $|\sigma_n-\sigma_m|<\delta$ Let $M=\max\{k |a_k|:1\leq k\leq m\}$ From eq. $(1)$, \begin{align} |s_n-s_m|&=|(\sigma_n-\sigma_m)+(\dfrac{1}{n+1}\sum_{k=1}^{n}k a_k-\dfrac{1}{m+1}\sum_{k=1}^{m}k a_n)|\\&\leq|\sigma_n-\sigma_m|+\dfrac{|(m+1)(a_1+2a_2+...+n a_n)-(n+1)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\&=|\sigma_n-\sigma_m|+\dfrac{|(m+1)((m+1)a_{m+1}+(m+2)a_{m+2}+...+n a_n)+(m-n)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\&<\delta+\dfrac{(m+1)(n-m)\epsilon+(m-n)mM}{(n+1)(m+1)}\\&=\delta+\dfrac{(n-m)\epsilon}{n+1}+\dfrac{(m-n)mM}{(n+1)(m+1)}\end{align} As $\epsilon, \delta \to 0, n \to \infty$ and $m \to n$, the right side converges to $0$; therefore, the Cauchy sequence $\{s_n\}$ converges.","Since I'm studying real analysis using this book by myself, I'm not sure whether or not my method to prove convergence of sequence is right. I'm working on the above question's (d), and my solution was kind of different from the solution provided by the following website. The solution in the website is much more beautiful and more concise than mine, but I believe my method also works. http://minds.wisconsin.edu/handle/1793/67009 3.14 If $\{s_n\}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n = \dfrac{s_0+s_1+...+s_n}{n+1} (n=0,1,2,...).$$ (d) Put $a_n=s_n-s_{n-1}$, for $n\geq 1$. Show that $$s_n-\sigma_n=\dfrac{1}{n+1}\sum_{k=1}^n k a_n$...$(1)$$ Assume that lim$(n a_n)=0$ and that ${\sigma_n}$ converges. Prove that $\{s_n\}$ converges. My solution: Since $\sigma_n$ converges, it is a Cauchy sequence. So, $\forall\epsilon,\forall\delta>0, \exists N\in\mathbb{N}$ s.t.$\forall n, \forall m> N , |n a_n-0|<\epsilon$ and  $|\sigma_n-\sigma_m|<\delta$ Let $M=\max\{k |a_k|:1\leq k\leq m\}$ From eq. $(1)$, \begin{align} |s_n-s_m|&=|(\sigma_n-\sigma_m)+(\dfrac{1}{n+1}\sum_{k=1}^{n}k a_k-\dfrac{1}{m+1}\sum_{k=1}^{m}k a_n)|\\&\leq|\sigma_n-\sigma_m|+\dfrac{|(m+1)(a_1+2a_2+...+n a_n)-(n+1)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\&=|\sigma_n-\sigma_m|+\dfrac{|(m+1)((m+1)a_{m+1}+(m+2)a_{m+2}+...+n a_n)+(m-n)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\&<\delta+\dfrac{(m+1)(n-m)\epsilon+(m-n)mM}{(n+1)(m+1)}\\&=\delta+\dfrac{(n-m)\epsilon}{n+1}+\dfrac{(m-n)mM}{(n+1)(m+1)}\end{align} As $\epsilon, \delta \to 0, n \to \infty$ and $m \to n$, the right side converges to $0$; therefore, the Cauchy sequence $\{s_n\}$ converges.",,"['real-analysis', 'convergence-divergence', 'proof-writing', 'proof-verification']"
99,Convergence of $S_n(a) = \sum_{an < k \leqslant (a+1)n} \frac{1}{\sqrt{kn-an^2}}$,Convergence of,S_n(a) = \sum_{an < k \leqslant (a+1)n} \frac{1}{\sqrt{kn-an^2}},"Let $a\in\mathbb{R}$ et $n \in\mathbb{N}$, Denote the following sequence, $$\displaystyle S_n(a) = \sum_{an < k \leqslant (a+1)n} \frac{1}{\sqrt{kn-an^2}}$$ For which values ​​of $a$ the sequence $(S_n(a))$ converges? With an Integral test we can prove that $$\sum_{an+1 < k \leq an+n} \frac{1}{\sqrt{kn-an^2}} \to 2$$ However, I don't see how can I continue ? Thank you in advance. EDIT: I forgot to say $k$ is an integer. My apology.","Let $a\in\mathbb{R}$ et $n \in\mathbb{N}$, Denote the following sequence, $$\displaystyle S_n(a) = \sum_{an < k \leqslant (a+1)n} \frac{1}{\sqrt{kn-an^2}}$$ For which values ​​of $a$ the sequence $(S_n(a))$ converges? With an Integral test we can prove that $$\sum_{an+1 < k \leq an+n} \frac{1}{\sqrt{kn-an^2}} \to 2$$ However, I don't see how can I continue ? Thank you in advance. EDIT: I forgot to say $k$ is an integer. My apology.",,['calculus']
