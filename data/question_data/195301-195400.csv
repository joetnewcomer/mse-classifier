,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Plese help me to express this Jacobian determinant $\frac{ \partial (x,y) }{ \partial (u,v) }$ only using u and v",Plese help me to express this Jacobian determinant  only using u and v,"\frac{ \partial (x,y) }{ \partial (u,v) }","We define u and v as $$u=\frac{2x}{x+y+1}$$ $$v=\frac{2y}{x+y+1}$$ I'm trying to get  Jacobian determinant $\frac{ \partial (x,y) }{ \partial (u,v) }$ and express it only using $u$ and $v$ . What I have tried $$x=\frac{-uy-u}{u-2}=-\frac{2y}{v^2}$$ $$y=-\frac{2x}{u^2}=\frac{-vx-v}{v-2}$$ $$\frac{ \partial x }{ \partial u }=\frac{2y+2}{(u-2)^2}$$ $$\frac{ \partial y }{ \partial u }=-\frac{2x}{u^2}$$ $$\frac{ \partial x }{ \partial v }=\frac{2y}{v^2}$$ $$\frac{ \partial y }{ \partial v }=-\frac{2x+2}{(v-2)^2}$$ But even if you calculate the determinant with these values, it end up with result below and fail to cancel x and y out. $$\frac{-16yu^2x+64yux-64yx+4yu^2v^2+16yuxv^2-16yxv^2+16yu^2xv-64yuxv+64yxv+4u^2xv^2+4u^2v^2}{u^2v^2\left(u-2\right)^2\left(v-2\right)^2}$$ Can anyone help me?","We define u and v as I'm trying to get  Jacobian determinant and express it only using and . What I have tried But even if you calculate the determinant with these values, it end up with result below and fail to cancel x and y out. Can anyone help me?","u=\frac{2x}{x+y+1} v=\frac{2y}{x+y+1} \frac{ \partial (x,y) }{ \partial (u,v) } u v x=\frac{-uy-u}{u-2}=-\frac{2y}{v^2} y=-\frac{2x}{u^2}=\frac{-vx-v}{v-2} \frac{ \partial x }{ \partial u }=\frac{2y+2}{(u-2)^2} \frac{ \partial y }{ \partial u }=-\frac{2x}{u^2} \frac{ \partial x }{ \partial v }=\frac{2y}{v^2} \frac{ \partial y }{ \partial v }=-\frac{2x+2}{(v-2)^2} \frac{-16yu^2x+64yux-64yx+4yu^2v^2+16yuxv^2-16yxv^2+16yu^2xv-64yuxv+64yxv+4u^2xv^2+4u^2v^2}{u^2v^2\left(u-2\right)^2\left(v-2\right)^2}","['derivatives', 'determinant', 'partial-derivative', 'jacobian']"
1,At which points is this complex function differentiable,At which points is this complex function differentiable,,"I'm making an exercise about the derivative. I needed to prove that $f(x)=|x|$ isn't differentiable at zero. Now I was wondering if we had a function $f:\mathbb{C} \to \mathbb{C} :z \to |z|$ so a complex function, in which points is this $f$ differentiable. If we say $z=a+bi$ , for $a=b=0$ it's not differentiable and for $a$ not equal to zero and $b=0$ it's differentiable (I think) but are there other problematic points for this function?","I'm making an exercise about the derivative. I needed to prove that isn't differentiable at zero. Now I was wondering if we had a function so a complex function, in which points is this differentiable. If we say , for it's not differentiable and for not equal to zero and it's differentiable (I think) but are there other problematic points for this function?",f(x)=|x| f:\mathbb{C} \to \mathbb{C} :z \to |z| f z=a+bi a=b=0 a b=0,['derivatives']
2,Weaker conditions for differentiating under the integral sign,Weaker conditions for differentiating under the integral sign,,"Standard theorems of real analysis give conditions under which it holds $$\int_0^1 \partial_x f(x,y)dy = \frac{d}{dx}\int_0^1 f(x,y)\,.$$ In most of the formulations that I have found, it is required that, for almost every $y$ , $f$ is everywhere differentiable. I'm wondering if this condition can be weakened, at least in some particular setting. Consider an integral operator $F$ on $L^2(0,1)$ which maps an element $\phi$ to $$ F\phi(x) = \int_0^1 k(x,y)\phi(y)dy\,.$$ $k(x,y)$ is supposed to be some bounded continuous function on $(0,1)^2$ . If $k$ is of class $C^1$ , then all functions in the image of $F$ are of class $C^1$ . But can we give some weaker condition in order to have an image at least differentiable? For example if $k(x,y)=|x-y|$ , then it can be proven explicitly (just by writing down the definition of derivative and by bounding the remainder) that it holds $$\frac{d}{dx}F\phi(x) = \int_0^1sign(x-y)\phi(y)dy\,.$$ which is $C^0$ and so $F\phi(x)$ is even $C^1$ . Is this a particular case of some general and well known result?","Standard theorems of real analysis give conditions under which it holds In most of the formulations that I have found, it is required that, for almost every , is everywhere differentiable. I'm wondering if this condition can be weakened, at least in some particular setting. Consider an integral operator on which maps an element to is supposed to be some bounded continuous function on . If is of class , then all functions in the image of are of class . But can we give some weaker condition in order to have an image at least differentiable? For example if , then it can be proven explicitly (just by writing down the definition of derivative and by bounding the remainder) that it holds which is and so is even . Is this a particular case of some general and well known result?","\int_0^1 \partial_x f(x,y)dy = \frac{d}{dx}\int_0^1 f(x,y)\,. y f F L^2(0,1) \phi  F\phi(x) = \int_0^1 k(x,y)\phi(y)dy\,. k(x,y) (0,1)^2 k C^1 F C^1 k(x,y)=|x-y| \frac{d}{dx}F\phi(x) = \int_0^1sign(x-y)\phi(y)dy\,. C^0 F\phi(x) C^1","['real-analysis', 'integration', 'functional-analysis', 'derivatives']"
3,how to solve derivative of a function including two absolute functions,how to solve derivative of a function including two absolute functions,,"So I don't understand how to solve the derivative of a function with two absolute functions. $$f(x) = |x - 1| + |x^2 - 2x|.$$ Here is the function. I need to solve the maximum and minimum values in the domain $[0,2]$ but can't figure out the derivative.",So I don't understand how to solve the derivative of a function with two absolute functions. Here is the function. I need to solve the maximum and minimum values in the domain but can't figure out the derivative.,"f(x) = |x - 1| + |x^2 - 2x|. [0,2]","['calculus', 'derivatives', 'maxima-minima', 'absolute-value']"
4,Prove that $\Gamma(\operatorname{W}(x))$ is convex $\forall x>0$,Prove that  is convex,\Gamma(\operatorname{W}(x)) \forall x>0,"Background : At the begining I was studing a function wich increases slowly and maybe have some property useful in number theory .Particulary I have found : Let $0<x\,$ define the function : $$f(x)=\Gamma(\operatorname{W}(x))$$ Where we see the Gamma function and the Lambert's function Then prove that : $$f''(x)>0\quad\forall x>0$$ Well working with WA wich is a little bit capricious I find that the minimum of the second derivative occurs on $I=[24800,24900]$ I have tried to solve the following expression see here without success . My second strategy is : if we know that mid-point convexity and conitinuity implies convexity we can says that we have : Let $ x,y>0$ then we have : $$f(x)+f(y)\geq 2f\Big(\frac{x+y}{2}\Big)$$ I can solve it for large value but not on $I$ describe above . Update : Following the good start by TheSimpliFire we have to prove : $$\psi(x)+\frac{(\psi(x))'}{\psi(x)}>1+\frac{1}{x+1}\quad \forall x>0$$ From the source we have (see (51) and (52)): $$\frac{\pi^2}{\pi^2x+6-\pi^2}\leq(\psi(x))' \quad \forall x\geq 1$$ And $$\log\Big((t-1)\frac{\pi^2}{6}+1\Big)-\gamma\leq\psi(t)<\log(2t-1)-\gamma\quad \forall t\geq 1$$ Perhaps there is an issue now . So if you have an idea or an approach like a hint it would be nice . Thanks a lot for all your contributions ! Max. Source : https://www.hindawi.com/journals/jam/2014/264652/",Background : At the begining I was studing a function wich increases slowly and maybe have some property useful in number theory .Particulary I have found : Let define the function : Where we see the Gamma function and the Lambert's function Then prove that : Well working with WA wich is a little bit capricious I find that the minimum of the second derivative occurs on I have tried to solve the following expression see here without success . My second strategy is : if we know that mid-point convexity and conitinuity implies convexity we can says that we have : Let then we have : I can solve it for large value but not on describe above . Update : Following the good start by TheSimpliFire we have to prove : From the source we have (see (51) and (52)): And Perhaps there is an issue now . So if you have an idea or an approach like a hint it would be nice . Thanks a lot for all your contributions ! Max. Source : https://www.hindawi.com/journals/jam/2014/264652/,"0<x\, f(x)=\Gamma(\operatorname{W}(x)) f''(x)>0\quad\forall x>0 I=[24800,24900]  x,y>0 f(x)+f(y)\geq 2f\Big(\frac{x+y}{2}\Big) I \psi(x)+\frac{(\psi(x))'}{\psi(x)}>1+\frac{1}{x+1}\quad \forall x>0 \frac{\pi^2}{\pi^2x+6-\pi^2}\leq(\psi(x))' \quad \forall x\geq 1 \log\Big((t-1)\frac{\pi^2}{6}+1\Big)-\gamma\leq\psi(t)<\log(2t-1)-\gamma\quad \forall t\geq 1","['derivatives', 'special-functions', 'gamma-function', 'lambert-w', 'convexity-inequality']"
5,How to differentiate $g(X)=\operatorname{tr}\left(X^{-1}\right)$? [duplicate],How to differentiate ? [duplicate],g(X)=\operatorname{tr}\left(X^{-1}\right),This question already has an answer here : Gradient of ${\mathrm X} \mapsto \mbox{tr} \left(\mathrm A \mathrm X^{-1} \mathrm B \right)$ (1 answer) Closed 3 years ago . Let $X$ be a square invertible $n \times n$ matrix. Calculate the derivative of the following function with respect to X. $$ g(X)=\operatorname{tr}\left(X^{-1}\right) $$ I'm stumped with this. As when I work through it I use  these two identities. $$\frac{\partial}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}=-\boldsymbol{f}(\boldsymbol{X})^{-1} \frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}$$ and 2. $$ \frac{\partial}{\partial \boldsymbol{X}} \operatorname{tr}(\boldsymbol{f}(\boldsymbol{X}))=\operatorname{tr}\left(\frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right) $$ I should arrive at the solution. using 1. I get $$d/dX(X^{-1}) = -X^{-1}\otimes X^{-1}$$ . So the answer should be the trace of that right? which = $$tr(-X^{-1})tr(X^{-1}).$$ but the solution seems to be $$-X^{-2T}$$ ? which I can't see,This question already has an answer here : Gradient of ${\mathrm X} \mapsto \mbox{tr} \left(\mathrm A \mathrm X^{-1} \mathrm B \right)$ (1 answer) Closed 3 years ago . Let be a square invertible matrix. Calculate the derivative of the following function with respect to X. I'm stumped with this. As when I work through it I use  these two identities. and 2. I should arrive at the solution. using 1. I get . So the answer should be the trace of that right? which = but the solution seems to be ? which I can't see,"X n \times n 
g(X)=\operatorname{tr}\left(X^{-1}\right)
 \frac{\partial}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}=-\boldsymbol{f}(\boldsymbol{X})^{-1} \frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1} 
\frac{\partial}{\partial \boldsymbol{X}} \operatorname{tr}(\boldsymbol{f}(\boldsymbol{X}))=\operatorname{tr}\left(\frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right)
 d/dX(X^{-1}) = -X^{-1}\otimes X^{-1} tr(-X^{-1})tr(X^{-1}). -X^{-2T}","['matrices', 'derivatives', 'inverse', 'matrix-calculus', 'scalar-fields']"
6,"Suppose $f(x)=xg(x)$, where $g$ is a continuous at $x_0=0$. Then $f$ is differentiable at $x_0=0$.","Suppose , where  is a continuous at . Then  is differentiable at .",f(x)=xg(x) g x_0=0 f x_0=0,"Justify: Suppose $f(x)=xg(x)$ , where $g$ is a continuous at $x_0=0$ . Then $f$ is differentiable at $x_0=0$ . I tried proving this by contradiction, but I'm not sure this is correct. My attempt: Supposed $f$ is not differentiable at $x_0=0$ . Then the following limit must not exist. $$\begin{align}\lim_{x\to0} \frac{f(x)-f(0)}{x-0} = \lim_{x\to0} \frac{xg(x)-0}{x-0} &= \lim_{x\to0} g(x)= g(0).\end{align}$$ That means $f'(0)=g(0)$ , which is a contradiction. Is this correct? If so, are there other ways to prove this?","Justify: Suppose , where is a continuous at . Then is differentiable at . I tried proving this by contradiction, but I'm not sure this is correct. My attempt: Supposed is not differentiable at . Then the following limit must not exist. That means , which is a contradiction. Is this correct? If so, are there other ways to prove this?",f(x)=xg(x) g x_0=0 f x_0=0 f x_0=0 \begin{align}\lim_{x\to0} \frac{f(x)-f(0)}{x-0} = \lim_{x\to0} \frac{xg(x)-0}{x-0} &= \lim_{x\to0} g(x)= g(0).\end{align} f'(0)=g(0),"['calculus', 'derivatives', 'proof-writing', 'solution-verification', 'alternative-proof']"
7,Cat's curve and some properties,Cat's curve and some properties,,"Well, it's my cat which inspired me today . The goal was :find a curve using elementary function which looks like my cat and I have found this : let $0<x<1$ the cat's curve is defined by the following function : $$f(x)=(1-x)^{\cos^2\Big(\frac{1}{x}\Big)}+x^{\cos^2\Big(\frac{1}{1-x}\Big)}-1$$ The graph looks like so : As far as I remenber it recall me a little bit the Cantor function in the neightborhood of zero or one . Furthermore there is a big problem with the derivatives but I don't go further than the first .Maybe it's a little bit fractal . Do you know some others interesting properties of this curve ? Exists there others curve of this kind ? Thanks in advance cheers .:-) Update: As heropup make a good remark I propose to prove that : $$\int_{0}^{1}f(x)dx<\frac{2}{3}$$ Moreover if we look at the graph of one summand I think it's hard to use taylor series (even unsuable maybe).So I don't know what tools use for.","Well, it's my cat which inspired me today . The goal was :find a curve using elementary function which looks like my cat and I have found this : let the cat's curve is defined by the following function : The graph looks like so : As far as I remenber it recall me a little bit the Cantor function in the neightborhood of zero or one . Furthermore there is a big problem with the derivatives but I don't go further than the first .Maybe it's a little bit fractal . Do you know some others interesting properties of this curve ? Exists there others curve of this kind ? Thanks in advance cheers .:-) Update: As heropup make a good remark I propose to prove that : Moreover if we look at the graph of one summand I think it's hard to use taylor series (even unsuable maybe).So I don't know what tools use for.",0<x<1 f(x)=(1-x)^{\cos^2\Big(\frac{1}{x}\Big)}+x^{\cos^2\Big(\frac{1}{1-x}\Big)}-1 \int_{0}^{1}f(x)dx<\frac{2}{3},"['derivatives', 'soft-question', 'curves']"
8,Calculating the $\dfrac{d}{dx} \arccos(x)$ with derivative definition.,Calculating the  with derivative definition.,\dfrac{d}{dx} \arccos(x),"I was asked to find the derivative of $\arccos$ $x$ with the definition of derivative . I know I have to form this limit. $f^{'}(c)= $ $\displaystyle{\lim_{h\to0}\dfrac{f(h+c)-f(c)}{h}}$ or $f^{'}(c)= $ $\displaystyle{\lim_{x\to c}\dfrac{f(x)-f(c)}{x-c}}$ which $-1<c<1$ (two limits are actually the same) I formed the first limit which is $\displaystyle{\lim_{h\to0}\dfrac{\arccos(h+c)-\arccos(c)}{h}}$ and the second limit which is $\displaystyle{\lim_{x\to c}\dfrac{\arccos(x)-\arccos(c)}{x-c}}$ I tried to use this equation: $$\arccos(x)+\arccos(y)=\arccos\left(xy-\sqrt{(1-x^2)(1-y^2)}\right) $$ but I failed and except that, I have literally NO idea how to calculate these limits.","I was asked to find the derivative of with the definition of derivative . I know I have to form this limit. or which (two limits are actually the same) I formed the first limit which is and the second limit which is I tried to use this equation: but I failed and except that, I have literally NO idea how to calculate these limits.",\arccos x f^{'}(c)=  \displaystyle{\lim_{h\to0}\dfrac{f(h+c)-f(c)}{h}} f^{'}(c)=  \displaystyle{\lim_{x\to c}\dfrac{f(x)-f(c)}{x-c}} -1<c<1 \displaystyle{\lim_{h\to0}\dfrac{\arccos(h+c)-\arccos(c)}{h}} \displaystyle{\lim_{x\to c}\dfrac{\arccos(x)-\arccos(c)}{x-c}} \arccos(x)+\arccos(y)=\arccos\left(xy-\sqrt{(1-x^2)(1-y^2)}\right) ,"['real-analysis', 'derivatives']"
9,Taking derivative with function of multiple variables?,Taking derivative with function of multiple variables?,,"Suppose we are studying the function $$  f(x,y) = xy + ax^2 + bx^2y^2, $$ We want to find the maximum $x$ satisfying the equation $$  f(x,y) = c, $$ where $a, b, c$ are constants. Somebody suggested to make use of the following auxiliary function $$  g(x,y) = xy, $$ so that $$  g + ax^2 + bg^2 = c. $$ Isolating $x$ , $$ x^2 = \frac{c-g-bg^2}{a}. $$ Now he says that the same condition as $ \frac{dx}{dy} = 0 $ is $$ \frac{dx^2}{d g(x,y)} = 0$$ Why? Edit : I saw this trick here .","Suppose we are studying the function We want to find the maximum satisfying the equation where are constants. Somebody suggested to make use of the following auxiliary function so that Isolating , Now he says that the same condition as is Why? Edit : I saw this trick here ."," 
f(x,y) = xy + ax^2 + bx^2y^2,
 x  
f(x,y) = c,
 a, b, c  
g(x,y) = xy,
  
g + ax^2 + bg^2 = c.
 x 
x^2 = \frac{c-g-bg^2}{a}.
  \frac{dx}{dy} = 0   \frac{dx^2}{d g(x,y)} = 0","['calculus', 'derivatives']"
10,Proof that $\frac{d(\sin x)}{dx} = \cos x$ for $\frac{\pi}{2} < x < \pi$,Proof that  for,\frac{d(\sin x)}{dx} = \cos x \frac{\pi}{2} < x < \pi,"Let us assume that $x$ is an angle that lies in the second quadrant i.e. $\dfrac{\pi}{2} < x < \pi$ . We have to prove that $\dfrac{d(\sin x)}{dx} = \cos x$ . I will use the unit circle to prove this. The method will be like the one used by Grant Sanderson of 3Blue1Brown in this video , which is a part of his Essence of Calculus series. The angles are measured in radians. In the diagram below, I have marked the angle $x$ and $dx$ on the unit circle. The angle $dx$ approaches $0$ , so it is very very small but for the sake of clarity, I have made it considerably large. Now, since $dx$ is actually really small, we can approximate arc $AB$ as a straight line approximately perpendicular to $OA$ . We are measuring the angles in radians and we have a unit circle, so its radius is $1 \text{ units}$ . Hence, the length of arc (now line segment) $AB$ is $\dfrac{\theta}{r}$ , where $\theta$ is $\angle AOB$ i.e. $dx$ and $r = 1 \text{ units}$ . So, $AB = \dfrac{dx}{1} = dx$ . Now, $d(\sin x) = \sin(x+dx)-\sin x$ which is the change in the ordinate of $A$ and $B$ . Now, $AP = d(\sin x)$ and $AB = dx$ . Also, $\triangle APB \sim \triangle AOQ$ . So, $\angle BAP = \angle OAQ = \pi - x$ . $\cos(\angle BAP) = \dfrac{AP}{AB} = \dfrac{d(\sin x)}{dx}$ . And $\cos(\angle BAP) = \cos (\pi - x) = -\cos x$ So, $\dfrac{d(\sin x)}{dx} = -\cos x$ which is not the case at all, since $\dfrac{d(\sin x)}{dx}$ will be negative as $\sin(x+dx) < \sin x$ but the sign of $-\cos x$ will be positive as $\cos x < 0$ . So, what mistake did I make here? According to me, the mistake was in assuming that $AP = d(\sin x)$ . I think that $AP$ should be $|d(\sin x)|$ . And as $d(\sin x) < 0 \implies |d(\sin x)| = -d(\sin x)$ . This fixes everything but I still want to verify if this indeed is the cause of the error. Thanks! PS : Let me know if I should justify why $\triangle APB \sim \triangle OQA$ to make the question clearer. PPS : It is necessary to prove that differentiating $\sin x$ with respect to $x$ gives $\cos x$ for all 4 quadrants when proving using the unit circle, right?","Let us assume that is an angle that lies in the second quadrant i.e. . We have to prove that . I will use the unit circle to prove this. The method will be like the one used by Grant Sanderson of 3Blue1Brown in this video , which is a part of his Essence of Calculus series. The angles are measured in radians. In the diagram below, I have marked the angle and on the unit circle. The angle approaches , so it is very very small but for the sake of clarity, I have made it considerably large. Now, since is actually really small, we can approximate arc as a straight line approximately perpendicular to . We are measuring the angles in radians and we have a unit circle, so its radius is . Hence, the length of arc (now line segment) is , where is i.e. and . So, . Now, which is the change in the ordinate of and . Now, and . Also, . So, . . And So, which is not the case at all, since will be negative as but the sign of will be positive as . So, what mistake did I make here? According to me, the mistake was in assuming that . I think that should be . And as . This fixes everything but I still want to verify if this indeed is the cause of the error. Thanks! PS : Let me know if I should justify why to make the question clearer. PPS : It is necessary to prove that differentiating with respect to gives for all 4 quadrants when proving using the unit circle, right?",x \dfrac{\pi}{2} < x < \pi \dfrac{d(\sin x)}{dx} = \cos x x dx dx 0 dx AB OA 1 \text{ units} AB \dfrac{\theta}{r} \theta \angle AOB dx r = 1 \text{ units} AB = \dfrac{dx}{1} = dx d(\sin x) = \sin(x+dx)-\sin x A B AP = d(\sin x) AB = dx \triangle APB \sim \triangle AOQ \angle BAP = \angle OAQ = \pi - x \cos(\angle BAP) = \dfrac{AP}{AB} = \dfrac{d(\sin x)}{dx} \cos(\angle BAP) = \cos (\pi - x) = -\cos x \dfrac{d(\sin x)}{dx} = -\cos x \dfrac{d(\sin x)}{dx} \sin(x+dx) < \sin x -\cos x \cos x < 0 AP = d(\sin x) AP |d(\sin x)| d(\sin x) < 0 \implies |d(\sin x)| = -d(\sin x) \triangle APB \sim \triangle OQA \sin x x \cos x,"['calculus', 'derivatives', 'solution-verification']"
11,How to evaluate the partial derivative of a differential?,How to evaluate the partial derivative of a differential?,,"Suppose $M$ is a finite-dimensional smooth manifold and $f\in C^\infty (M)$ . Let us now define function $F:TM\to \mathbb{R}$ by $(p,w)\mapsto \bigr(df(p)\bigr)(w)$ for $p\in M$ and $w\in T_pM$ . Now, I want to show that if $\gamma:I\to M$ is a smooth curve and $(x,U)$ is a local chart in $M$ such that $\gamma(I)\subset U$ ; then $$\frac{\partial F}{\partial x^i}(\gamma(t),\gamma'(t))=\frac{d}{dt}\Bigr(\frac{\partial F}{\partial v^i}\bigr(\gamma(t),\gamma'(t) \bigr) \Bigr),$$ for $i=1,\ldots,$ dim( $M$ ); where $\displaystyle v^i:=\frac{\partial }{\partial x^i}$ . I do know that this should follow easily from the chain rule, but I simply do not understand the partial derivative of $F$ , seems realy weird to me.","Suppose is a finite-dimensional smooth manifold and . Let us now define function by for and . Now, I want to show that if is a smooth curve and is a local chart in such that ; then for dim( ); where . I do know that this should follow easily from the chain rule, but I simply do not understand the partial derivative of , seems realy weird to me.","M f\in C^\infty (M) F:TM\to \mathbb{R} (p,w)\mapsto \bigr(df(p)\bigr)(w) p\in M w\in T_pM \gamma:I\to M (x,U) M \gamma(I)\subset U \frac{\partial F}{\partial x^i}(\gamma(t),\gamma'(t))=\frac{d}{dt}\Bigr(\frac{\partial F}{\partial v^i}\bigr(\gamma(t),\gamma'(t) \bigr) \Bigr), i=1,\ldots, M \displaystyle v^i:=\frac{\partial }{\partial x^i} F","['derivatives', 'differential-geometry', 'differential-topology']"
12,Does convexity at a point imply existence of one-sided derivatives?,Does convexity at a point imply existence of one-sided derivatives?,,"Let $\phi:\mathbb (0,\infty) \to [0,\infty)$ be a continuous function, and let $c \in (0,\infty)$ be fixed. Suppose that "" $\phi$ is convex at $c$ "". i.e. for any $x_1,x_2>0, \alpha \in [0,1]$ satisfying $\alpha x_1 + (1- \alpha)x_2 =c$ , we have $$ \phi(c)=\phi\left(\alpha x_1 + (1- \alpha)x_2 \right) \leq \alpha \phi(x_1) + (1-\alpha)\phi(x_2) . $$ Assume also that $\phi$ is strictly decreasing in a neighbourhood of $c$ . Do the one-sided derivatives $\phi'_{-}(c),\phi'_{+}(c)$ necessarily exist? Edit: As pointed by Aryaman Maithani if $c$ is a global minimum of $\phi$ , then clearly $\phi$ is convex at $c$ , but there should be no reason to expect for existence of one-sided derivatives. (e.g. $\phi(x)=\sqrt{|x|}, c=0$ ). Edit 2: In the example described here , the left derivative does not exist. Can we create an example where the right derivative does not exist?","Let be a continuous function, and let be fixed. Suppose that "" is convex at "". i.e. for any satisfying , we have Assume also that is strictly decreasing in a neighbourhood of . Do the one-sided derivatives necessarily exist? Edit: As pointed by Aryaman Maithani if is a global minimum of , then clearly is convex at , but there should be no reason to expect for existence of one-sided derivatives. (e.g. ). Edit 2: In the example described here , the left derivative does not exist. Can we create an example where the right derivative does not exist?","\phi:\mathbb (0,\infty) \to [0,\infty) c \in (0,\infty) \phi c x_1,x_2>0, \alpha \in [0,1] \alpha x_1 + (1- \alpha)x_2 =c 
\phi(c)=\phi\left(\alpha x_1 + (1- \alpha)x_2 \right) \leq \alpha \phi(x_1) + (1-\alpha)\phi(x_2) .
 \phi c \phi'_{-}(c),\phi'_{+}(c) c \phi \phi c \phi(x)=\sqrt{|x|}, c=0","['real-analysis', 'calculus', 'derivatives', 'convex-analysis', 'examples-counterexamples']"
13,How to differentiate the trace of a matrix times its diagonal,How to differentiate the trace of a matrix times its diagonal,,"Let $\mathbf{\Theta}\in\mathbb{R}^{p\times p}$ be a matrix and denote $\mbox{diag}(\mathbf{\Theta})\in\mathbb{R}^{p\times p}$ the matrix that has the same diagonal as $\mathbf{\Theta}$ and every off-diagonal element zero. I am trying to calculate $$\frac{\partial \|\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-\mbox{diag}(\mathbf{\Theta}))]\,\|_{F}^{2} }{\partial \mathbf{\Theta}}$$ where $\|\cdot\|_{F}$ denotes the Frobenius norm, $\mathbf{I}$ the identity matrix and $\mathbf{X} \in \mathbb{R}^{n \times p}$ . The frobenius norm is equal to \begin{align*} &tr(\mathbf{X}^{\intercal}\mathbf{X})+tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})\\ &-2tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))-2tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}) \end{align*} I have also worked out the derivatives to be \begin{align*} &\frac{\partial tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}, \frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2diag(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})\\ &\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=\mathbf{X}^{\intercal}\mathbf{X},\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))}{\partial \mathbf{\Theta}}=diag(\mathbf{X}^{\intercal}\mathbf{X}),\\ &\frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})+diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}). \end{align*} But when I replace I get \begin{align*} \frac{\partial ||\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-diag(\mathbf{\Theta}))]\,||_{F}^{2} }{\partial \mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}-2diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2diag(\mathbf{X}^{\intercal}\mathbf{X})-2\mathbf{X}^{\intercal}\mathbf{X}, \end{align*} which I think is wrong because the right hand side includes components from the diagonal of $\mathbf{\Theta}$ while the left hand side does not. As I am not very good with matrix calculus, I would appreciate any intuition. Thank you.","Let be a matrix and denote the matrix that has the same diagonal as and every off-diagonal element zero. I am trying to calculate where denotes the Frobenius norm, the identity matrix and . The frobenius norm is equal to I have also worked out the derivatives to be But when I replace I get which I think is wrong because the right hand side includes components from the diagonal of while the left hand side does not. As I am not very good with matrix calculus, I would appreciate any intuition. Thank you.","\mathbf{\Theta}\in\mathbb{R}^{p\times p} \mbox{diag}(\mathbf{\Theta})\in\mathbb{R}^{p\times p} \mathbf{\Theta} \frac{\partial \|\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-\mbox{diag}(\mathbf{\Theta}))]\,\|_{F}^{2} }{\partial \mathbf{\Theta}} \|\cdot\|_{F} \mathbf{I} \mathbf{X} \in \mathbb{R}^{n \times p} \begin{align*}
&tr(\mathbf{X}^{\intercal}\mathbf{X})+tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})\\
&-2tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))-2tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})
\end{align*} \begin{align*}
&\frac{\partial tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}, \frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2diag(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})\\
&\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=\mathbf{X}^{\intercal}\mathbf{X},\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))}{\partial \mathbf{\Theta}}=diag(\mathbf{X}^{\intercal}\mathbf{X}),\\
&\frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})+diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}).
\end{align*} \begin{align*}
\frac{\partial ||\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-diag(\mathbf{\Theta}))]\,||_{F}^{2} }{\partial \mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}-2diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2diag(\mathbf{X}^{\intercal}\mathbf{X})-2\mathbf{X}^{\intercal}\mathbf{X},
\end{align*} \mathbf{\Theta}","['matrices', 'derivatives', 'solution-verification', 'matrix-calculus']"
14,Problem about differentiability and continuity,Problem about differentiability and continuity,,"Suppose I have a question. Find the values of a and b, if the function f is defined as, $$ f(x)=\left\{\begin{array}{l} x^{2}+3 x+a, x \leq 1 \\ b x+2, x>1 \end{array}\right. $$ So, what my teacher asks me to do is this, Calculate the $LHD$ and $RHD$ (Left hand derivative and Right hand derivative) At $x=1$ $LHD$ is, $$ \begin{array}{l} \quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\ =\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\ =\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\ =\lim _{h \rightarrow 0}(5-h) \\ =5 \end{array} $$ $f(1-h)$ means the case when $x < 1$ , means I have to use $x^{2}+3 x+a$ And $f(1)$ means the same case, when $x \leq 1$ And $RHD$ is, Warning: this is the problem region $$ \begin{array}{l} \quad \lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\ =\lim _{h \rightarrow 0} \frac{b+b h+2-b(1)-2}{h} \\ =b \end{array} $$ As they are given differentiable at $x=1$ . Equating them, $LHD=RHD$ $b=5$ And uses continuity (equating $LHL$ and $RHL$ and $f(1)$ ) to find another equation and solve it to find $a$ Giving $a=3$ , and $b=5$ Well, in the $RHD$ didn't he use the wrong case for $f(1)$ , he should've use the case when $x \leq 1$ , why the heck did he use $x>1$ , and Everyone is telling me the logic behind this is that we are calculating $RHD$ , which means everything lies in the right neighbourhood of $1$ even $f(1)$ , I get the logic but that is still breaking rules of math, What I do is, I calculate the $LHL$ and $RHL$ and f(1) and equate them to get the same equation, yes, i have the equation for continuity as my teacher. $LHL$ $$ \begin{array}{l} \quad \lim _{x \rightarrow 1^{-}} (x^{2}+3 x+a) \\ =\lim _{h \rightarrow 0} (1+h^{2}-2 h+3-3 h+a) \\ =4+a \end{array} $$ $RHL$ $$ \begin{array}{l} \lim _{x \rightarrow 1^{+}}(b x+2) \\ \lim _{h \rightarrow 0}(b+b h+2) \\ \quad=b+2 \end{array} $$ Now, $$ \begin{array}{c} L H L=R H L \\ b+2=4+a \\ b-a=2   \hspace{10mm}. ..eq(1) \end{array} $$ Now solving for differentiability, The same $LHD$ , $$ \begin{array}{l} \quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\ =\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\ =\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\ =\lim _{h \rightarrow 0}(5-h) \\ =5 \end{array} $$ And now $RHD$ , $$ \begin{array}{c} \lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\ =\lim _{h \rightarrow 0} \frac{b+b h+2-4-a}{h} \\ =\lim _{h \rightarrow 0} \frac{b+b h-2-a}{h} \\ \text { Using } eq(1)...  \end{array} $$ $$ \begin{array}{l} =\lim _{h \rightarrow 0} \frac{2+b h-2}{h} \\ =\quad b \end{array} $$ $$ b=5\\ a=3 $$ I used the right case for $f(1)$ that is, $x^{2}+3 x+a$ , for $x \leq 1$ So, $f(1)=4+a$ , So, who did this right, my teacher who used the $x>1$ case in the $RHD$ , or me who used the actual $x \ leq 1$ case. This is what I think, differentiability and continuity are related, as we know A differentiable function is always continous but not every continous function is differentiable. So continuity is a much-needed required step to solve for differentiability, so I should solve for continuity first. As I did. Anything just say? Is he right??","Suppose I have a question. Find the values of a and b, if the function f is defined as, So, what my teacher asks me to do is this, Calculate the and (Left hand derivative and Right hand derivative) At is, means the case when , means I have to use And means the same case, when And is, Warning: this is the problem region As they are given differentiable at . Equating them, And uses continuity (equating and and ) to find another equation and solve it to find Giving , and Well, in the didn't he use the wrong case for , he should've use the case when , why the heck did he use , and Everyone is telling me the logic behind this is that we are calculating , which means everything lies in the right neighbourhood of even , I get the logic but that is still breaking rules of math, What I do is, I calculate the and and f(1) and equate them to get the same equation, yes, i have the equation for continuity as my teacher. Now, Now solving for differentiability, The same , And now , I used the right case for that is, , for So, , So, who did this right, my teacher who used the case in the , or me who used the actual case. This is what I think, differentiability and continuity are related, as we know A differentiable function is always continous but not every continous function is differentiable. So continuity is a much-needed required step to solve for differentiability, so I should solve for continuity first. As I did. Anything just say? Is he right??","
f(x)=\left\{\begin{array}{l}
x^{2}+3 x+a, x \leq 1 \\
b x+2, x>1
\end{array}\right.
 LHD RHD x=1 LHD 
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\
=\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\
=\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\
=\lim _{h \rightarrow 0}(5-h) \\
=5
\end{array}
 f(1-h) x < 1 x^{2}+3 x+a f(1) x \leq 1 RHD 
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h+2-b(1)-2}{h} \\
=b
\end{array}
 x=1 LHD=RHD b=5 LHL RHL f(1) a a=3 b=5 RHD f(1) x \leq 1 x>1 RHD 1 f(1) LHL RHL LHL 
\begin{array}{l}
\quad \lim _{x \rightarrow 1^{-}} (x^{2}+3 x+a) \\
=\lim _{h \rightarrow 0} (1+h^{2}-2 h+3-3 h+a) \\
=4+a
\end{array}
 RHL 
\begin{array}{l}
\lim _{x \rightarrow 1^{+}}(b x+2) \\
\lim _{h \rightarrow 0}(b+b h+2) \\
\quad=b+2
\end{array}
 
\begin{array}{c}
L H L=R H L \\
b+2=4+a \\
b-a=2   \hspace{10mm}. ..eq(1)
\end{array}
 LHD 
\begin{array}{l}
\quad \lim _{h \rightarrow 0} \frac{f(1-h)-f(1)}{-h} \\
=\lim _{h \rightarrow 0} \frac{1+h^{2}-2 h+3-3 h-4-2+2}{-h} \\
=\lim _{h \rightarrow 0} \frac{h^{2}-5 h}{-h} \\
=\lim _{h \rightarrow 0}(5-h) \\
=5
\end{array}
 RHD 
\begin{array}{c}
\lim _{h \rightarrow 0} \frac{f(1+h)-f(1)}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h+2-4-a}{h} \\
=\lim _{h \rightarrow 0} \frac{b+b h-2-a}{h} \\
\text { Using } eq(1)... 
\end{array}
 
\begin{array}{l}
=\lim _{h \rightarrow 0} \frac{2+b h-2}{h} \\
=\quad b
\end{array}
 
b=5\\
a=3
 f(1) x^{2}+3 x+a x \leq 1 f(1)=4+a x>1 RHD x \ leq 1","['calculus', 'limits', 'derivatives', 'continuity']"
15,Derivative of the complex norm as commonly used in physics,Derivative of the complex norm as commonly used in physics,,"On the one hand, I read that the derivative of the complex conjugate $C[z]=\overline{z}$ is not differentiable anywhere (for instance see here ). (see 1, below) On the other hand, I see in physics taking the derivative of a complex scalar field to obtain the equation of motion using the Euler-Lagrange method (for instance see enter link description here (see 2, below) So which is it, can we or can we not take the derivative? For case 1, the reference states that a complex function is differentiable if and only if it satisfies the Cauchy-Riemann equations: $$ f[z]=f[x+iy]=u[x,y]+iv[x,y] $$ Then f is differentiable if $$ \frac{\partial u}{\partial x} =\frac{\partial v}{\partial y} \\  \frac{\partial u}{\partial y} =-\frac{\partial v}{\partial x} $$ Then for the complex conjugate $C[x+iy]=x-iy$ then $\partial u/\partial x =1$ and $\partial v/\partial y=-1$ . Consequently $C[z]=\overline{z}$ is not differentiable anywhere in the complex plane. For case 2, the physics paper defines the Lagrangian of a complex scalar free field as follows: $$ \mathcal{L}=(\partial \phi^*)(\partial \phi) $$ Then they claim that $$ \frac{\partial \mathcal{L}}{\partial (\partial \phi)}=\partial \phi^*\\ \frac{\partial \mathcal{L}}{\partial (\partial \phi^*)}=\partial \phi $$ To obtain these results I assume they apply the chain rule $$ \frac{\partial }{\partial (\partial \phi)}(\partial \phi^* \partial \phi)=\partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* )+\partial \phi^* \frac{\partial }{\partial (\partial \phi)}(\partial \phi) $$ Is the following term not an 'illegal' derivative of a complex conjugate function? $$ \partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* ) $$ Why are they allowed to pose it equal to 0?","On the one hand, I read that the derivative of the complex conjugate is not differentiable anywhere (for instance see here ). (see 1, below) On the other hand, I see in physics taking the derivative of a complex scalar field to obtain the equation of motion using the Euler-Lagrange method (for instance see enter link description here (see 2, below) So which is it, can we or can we not take the derivative? For case 1, the reference states that a complex function is differentiable if and only if it satisfies the Cauchy-Riemann equations: Then f is differentiable if Then for the complex conjugate then and . Consequently is not differentiable anywhere in the complex plane. For case 2, the physics paper defines the Lagrangian of a complex scalar free field as follows: Then they claim that To obtain these results I assume they apply the chain rule Is the following term not an 'illegal' derivative of a complex conjugate function? Why are they allowed to pose it equal to 0?","C[z]=\overline{z} 
f[z]=f[x+iy]=u[x,y]+iv[x,y]
 
\frac{\partial u}{\partial x} =\frac{\partial v}{\partial y} \\ 
\frac{\partial u}{\partial y} =-\frac{\partial v}{\partial x}
 C[x+iy]=x-iy \partial u/\partial x =1 \partial v/\partial y=-1 C[z]=\overline{z} 
\mathcal{L}=(\partial \phi^*)(\partial \phi)
 
\frac{\partial \mathcal{L}}{\partial (\partial \phi)}=\partial \phi^*\\
\frac{\partial \mathcal{L}}{\partial (\partial \phi^*)}=\partial \phi
 
\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* \partial \phi)=\partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* )+\partial \phi^* \frac{\partial }{\partial (\partial \phi)}(\partial \phi)
 
\partial \phi\frac{\partial }{\partial (\partial \phi)}(\partial \phi^* )
","['complex-analysis', 'derivatives', 'complex-numbers']"
16,How does the square root disappear when differentiating $y=\frac{\sqrt{2x^2}}{\cos x}$?,How does the square root disappear when differentiating ?,y=\frac{\sqrt{2x^2}}{\cos x},"Finding the derivative of $$y=\frac{\sqrt{2x^2}}{\cos x}$$ I am going through the steps and having trouble using the quotient rule.  I have seen the final answer, and I've had no trouble using the quotient rule in the past, but this one is giving me trouble in terms of figuring out where all the fractions end up going.  Below is what I have so far: $$\frac{dy}{dx}=\frac{\dfrac{4x\cos x}{2\sqrt{2x^2}}+\sqrt{2x^2}\sin x}{(\cos x)^2}$$ I assume I can get rid of the 2 in the denominator in the first term in the numerator, leaving $\dfrac{2x\cos x}{\sqrt{2x^2}}$ and after seeing the final answer, I believe the denominator can be moved down so that the final answer's denominator is $\sqrt{2x^2}(\cos x)^2$ .  However, in the final answer the numerator is $2x\cos x +2x^2\sin x$ .  Where does the $2x^2$ come from?  How could the square root have gone away?  For reference, here is the final answer I'm supposed to get: $$\frac{2x\cos x+2x^2\sin x}{\sqrt{2x^2}(\cos x)^2}$$","Finding the derivative of I am going through the steps and having trouble using the quotient rule.  I have seen the final answer, and I've had no trouble using the quotient rule in the past, but this one is giving me trouble in terms of figuring out where all the fractions end up going.  Below is what I have so far: I assume I can get rid of the 2 in the denominator in the first term in the numerator, leaving and after seeing the final answer, I believe the denominator can be moved down so that the final answer's denominator is .  However, in the final answer the numerator is .  Where does the come from?  How could the square root have gone away?  For reference, here is the final answer I'm supposed to get:",y=\frac{\sqrt{2x^2}}{\cos x} \frac{dy}{dx}=\frac{\dfrac{4x\cos x}{2\sqrt{2x^2}}+\sqrt{2x^2}\sin x}{(\cos x)^2} \dfrac{2x\cos x}{\sqrt{2x^2}} \sqrt{2x^2}(\cos x)^2 2x\cos x +2x^2\sin x 2x^2 \frac{2x\cos x+2x^2\sin x}{\sqrt{2x^2}(\cos x)^2},"['derivatives', 'chain-rule']"
17,System of differential equations depending on parameter,System of differential equations depending on parameter,,"This is the first time to get a question like this: How to solve the system $y'=3by+(1-2b)z$ , $z'=by+z+e^{4x}$ , where $b\in\mathbb{R}$ ? Any help is welcome.","This is the first time to get a question like this: How to solve the system , , where ? Any help is welcome.",y'=3by+(1-2b)z z'=by+z+e^{4x} b\in\mathbb{R},"['ordinary-differential-equations', 'derivatives', 'systems-of-equations']"
18,Where is the error? Application of FTC,Where is the error? Application of FTC,,"Please, I need a help to see the error on this argument: $$\int_0^tf(a)g(t-a)da=\int_0^tf(t-a)g(a)da\implies$$ $$\dfrac{d}{dt}\int_0^tf(a)g(t-a)da=\dfrac{d}{dt}\int_0^tf(t-a)g(a)da\implies$$ $$f(t)g(0)=f(0)g(t)$$ If $f(0)=g(0)=k\neq 0$ , então $$f(t)=g(t)\forall t\in\Bbb R.$$ It is obviously incorrect, but I could not find the error. Many thanks!","Please, I need a help to see the error on this argument: If , então It is obviously incorrect, but I could not find the error. Many thanks!",\int_0^tf(a)g(t-a)da=\int_0^tf(t-a)g(a)da\implies \dfrac{d}{dt}\int_0^tf(a)g(t-a)da=\dfrac{d}{dt}\int_0^tf(t-a)g(a)da\implies f(t)g(0)=f(0)g(t) f(0)=g(0)=k\neq 0 f(t)=g(t)\forall t\in\Bbb R.,"['calculus', 'integration', 'derivatives', 'solution-verification', 'fake-proofs']"
19,Complex derivative of Hadamard product inside Frobenius norm,Complex derivative of Hadamard product inside Frobenius norm,,"I'm trying to find the complex derivative of $$||R - P \circ \gamma \gamma ^H||_F ^2$$ . with respect to $\gamma$ . I saw the post regarding the real counterpart of the same question here . However, when I tried applying similar principles given there $-2(P\circ M+P^T\circ M^T)\gamma$ ,  (where $M=R - P \circ \gamma \gamma^T$ ), it didn't work. Here $||.||_F$ is the Frobenius norm and $\circ$ is the Hadamard product.","I'm trying to find the complex derivative of . with respect to . I saw the post regarding the real counterpart of the same question here . However, when I tried applying similar principles given there ,  (where ), it didn't work. Here is the Frobenius norm and is the Hadamard product.",||R - P \circ \gamma \gamma ^H||_F ^2 \gamma -2(P\circ M+P^T\circ M^T)\gamma M=R - P \circ \gamma \gamma^T ||.||_F \circ,"['matrices', 'derivatives', 'complex-numbers', 'matrix-calculus']"
20,problem on Cauchy problem,problem on Cauchy problem,,"$y u_x-xu_y=0,u=g $ on $ \Omega $ has a unique solution in neighborhood of $\Omega$ for every differentiable function g: $\Omega \rightarrow R$ if 1. $\Omega =\{(x,0):x>0\}$ 2. $\Omega =\{(x,y):x^2+y^2=1\}$ 3. $\Omega =\{(x,y):x+y=1,x>1\}$ 4. $\Omega =\{(x,y):y=x^2,x>0\}$ What i have tried  I use  Lagrange's method $$\frac{dx}{y}=\frac{dy}{-x}=\frac{du}{0}$$ $\implies u=c_1,x^2+y^2=c_2$ Solution is of the form $u= \phi(x^2+y^2)$ where $\phi$ has to be match with condition  given . For  option 1 after apply condition $g= \phi (x^2)$ $\implies \phi(x)= g(√x)$ solution becomes $u = g(√x^2+y^2)$ So option 1 looks correct to me for option 2 After apply  condition $g= \phi(1)$ from this i got no solution option 2 looks wrong to me I am stuck at   3rd and 4th option please help me for these and also check my explanation for option 1and 2 . or is there other method to solve such problems",on has a unique solution in neighborhood of for every differentiable function g: if 1. 2. 3. 4. What i have tried  I use  Lagrange's method Solution is of the form where has to be match with condition  given . For  option 1 after apply condition solution becomes So option 1 looks correct to me for option 2 After apply  condition from this i got no solution option 2 looks wrong to me I am stuck at   3rd and 4th option please help me for these and also check my explanation for option 1and 2 . or is there other method to solve such problems,"y u_x-xu_y=0,u=g   \Omega  \Omega \Omega \rightarrow R \Omega =\{(x,0):x>0\} \Omega =\{(x,y):x^2+y^2=1\} \Omega =\{(x,y):x+y=1,x>1\} \Omega =\{(x,y):y=x^2,x>0\} \frac{dx}{y}=\frac{dy}{-x}=\frac{du}{0} \implies u=c_1,x^2+y^2=c_2 u= \phi(x^2+y^2) \phi g= \phi (x^2) \implies \phi(x)= g(√x) u = g(√x^2+y^2) g= \phi(1)","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'partial-derivative', 'cauchy-problem']"
21,Prove that f is not differentiable at x=0,Prove that f is not differentiable at x=0,,"Question: For $x$ $\in \mathbb{R}$ , prove that $f(x) = \lim_{n\to \infty}(x+\sqrt{1/n})$ is not differntiable for $x=0$ . My attempt (which my results contradicts the question): $f(x) = \lim_{n\to \infty}(x+\sqrt{1/n})=2x$ which is differentiable $\forall x\in\mathbb{R}$ . And so $f$ is infact differentiable at $x=0$ . I am convinced there must be a mistake in computing $f$ (line 2)?","Question: For , prove that is not differntiable for . My attempt (which my results contradicts the question): which is differentiable . And so is infact differentiable at . I am convinced there must be a mistake in computing (line 2)?",x \in \mathbb{R} f(x) = \lim_{n\to \infty}(x+\sqrt{1/n}) x=0 f(x) = \lim_{n\to \infty}(x+\sqrt{1/n})=2x \forall x\in\mathbb{R} f x=0 f,"['limits', 'derivatives']"
22,Partial Derivatives and Differentiability of a piecewise defined function,Partial Derivatives and Differentiability of a piecewise defined function,,"this is my first post on this wonderful forum. I'd like to thank all of you for the help you're going to give me.  I was doing an exercise, but I'm not sure the solution is entirely correct.  I should discuss the continuity, the existence of partial derivatives and the differentiability of this function on every point of its domain.  The function is: $f(x,y)=\left\{\begin{matrix} \frac{1}{y^3}\cdot ln(1+x^3\cdot y) \: \: \: if \: \:  y\neq 0  &  & \\ x+|x| \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: if \: \: y=0  &  &  \end{matrix}\right.$ The domain is clearly: $domf=\left \{ (x,y)\: \in \: \mathbb{R}^2 \: : \: 1+x^3\cdot y>0) \right \}$ I show you my try: The function is differentiable $\forall (x,y)\in \mathbb{R}^2 : y\neq 0 \: \: and \: \: x\neq 0$ , because it's a composition of differentiable functions. I put $x\neq 0$ cause the absolute value in the second equation will almost certainly give some problems. For these values the function is also continue and partial derivable, since its differentiable. We should check the continuity: $\lim_{y \to 0}f(x,y)=\lim_{y \to 0}\frac{1}{y^3}\cdot ln(1+x^3\cdot y)=\lim_{y \to 0}\frac{x^3}{y^2}=\left\{\begin{matrix} +\propto \: \: if \: \: x>0  &  & \\ 0 \: \: if \: \: x=0  &  & \\ -\propto \: \: if \: \: x<0  &  &  \end{matrix}\right.$ So the function is not continuous for the point of the form $(x,0)$ with $x\neq 0$ , so it is not differentiable on the y-axis (except for the origin, which we'll check later). Now I should check if the function has partial derivatives for the points of the form $(x,0)$ , which I excluded before. $\frac{\partial f}{\partial x}(x,0)=\lim_{t \to 0} \frac{f(x+t,0)-f(x,0)}{t}=\lim_{t \to 0} \frac{x+t+|x+t|-x-|x|}{t}=\left\{\begin{matrix} 2 \: \: if \: \: x>0  &  & \\ \nexists \: \: if \: \: x=0   &  & \\ 0 \: \: if \: \: x<0  &  &  \end{matrix}\right.$ $\frac{\partial f}{\partial y}(x,0)=\lim_{t \to 0}\frac{f(x,t)-f(x,0)}{t}=\frac{\frac{1}{t^3}\cdot ln(1+x^3\cdot t)-x-|x|}{t}=\left\{\begin{matrix}\nexists \: \: (=\pm\propto) \: \: if \: \: x\neq 0  &  & \\ 0 \: \: if \: \: x=0  &  &  \end{matrix}\right.$ Since $(0,0)$ doesn't admit both partial derivatives, we can conclude that the function is not differentiable in the origin. Can you please tell me if I solved the exercise correctly? I would really appreciate you're help. Thank you. P.S: Sorry if I made some english mistakes.","this is my first post on this wonderful forum. I'd like to thank all of you for the help you're going to give me.  I was doing an exercise, but I'm not sure the solution is entirely correct.  I should discuss the continuity, the existence of partial derivatives and the differentiability of this function on every point of its domain.  The function is: The domain is clearly: I show you my try: The function is differentiable , because it's a composition of differentiable functions. I put cause the absolute value in the second equation will almost certainly give some problems. For these values the function is also continue and partial derivable, since its differentiable. We should check the continuity: So the function is not continuous for the point of the form with , so it is not differentiable on the y-axis (except for the origin, which we'll check later). Now I should check if the function has partial derivatives for the points of the form , which I excluded before. Since doesn't admit both partial derivatives, we can conclude that the function is not differentiable in the origin. Can you please tell me if I solved the exercise correctly? I would really appreciate you're help. Thank you. P.S: Sorry if I made some english mistakes.","f(x,y)=\left\{\begin{matrix} \frac{1}{y^3}\cdot ln(1+x^3\cdot y) \: \: \: if \: \:  y\neq 0
 &  & \\ x+|x| \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: \: if \: \: y=0
 &  & 
\end{matrix}\right. domf=\left \{ (x,y)\: \in \: \mathbb{R}^2 \: : \: 1+x^3\cdot y>0) \right \} \forall (x,y)\in \mathbb{R}^2 : y\neq 0 \: \: and \: \: x\neq 0 x\neq 0 \lim_{y \to 0}f(x,y)=\lim_{y \to 0}\frac{1}{y^3}\cdot ln(1+x^3\cdot y)=\lim_{y \to 0}\frac{x^3}{y^2}=\left\{\begin{matrix} +\propto \: \: if \: \: x>0
 &  & \\ 0 \: \: if \: \: x=0
 &  & \\ -\propto \: \: if \: \: x<0
 &  & 
\end{matrix}\right. (x,0) x\neq 0 (x,0) \frac{\partial f}{\partial x}(x,0)=\lim_{t \to 0} \frac{f(x+t,0)-f(x,0)}{t}=\lim_{t \to 0} \frac{x+t+|x+t|-x-|x|}{t}=\left\{\begin{matrix} 2 \: \: if \: \: x>0
 &  & \\ \nexists \: \: if \: \: x=0 
 &  & \\ 0 \: \: if \: \: x<0
 &  & 
\end{matrix}\right. \frac{\partial f}{\partial y}(x,0)=\lim_{t \to 0}\frac{f(x,t)-f(x,0)}{t}=\frac{\frac{1}{t^3}\cdot ln(1+x^3\cdot t)-x-|x|}{t}=\left\{\begin{matrix}\nexists \: \: (=\pm\propto) \: \: if \: \: x\neq 0
 &  & \\ 0 \: \: if \: \: x=0
 &  & 
\end{matrix}\right. (0,0)","['real-analysis', 'limits', 'derivatives']"
23,"What is the Hessian matrix of $f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle $?",What is the Hessian matrix of ?,"f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle ","I'm trying to understand what is the Hessian matrix of $f\colon\mathbb{R}^{n}\to\mathbb{R}$ defined by $f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle $ where $A,B$ are symetric $n\times n$ matrices. What I know is that if we let $g\left(x\right)=\left\langle Ax,x\right\rangle $ and $h\left(x\right)=\left\langle Bx,x\right\rangle $ then $\nabla g\left(x\right)=2Ax,\nabla h\left(x\right)=2Bx$ and $\nabla^{2}g\left(x\right)=2A,\nabla^{2}h\left(x\right)=2B$ . Also by the product rule we have $\left(fg\right)'=f'g+fg'$ which then gives us \begin{align*} \left(fg\right)'' & =f''g+f'g'+f'g'+fg''=\\  & =f''g+2f'g'+fg'' \end{align*} Regarding $\nabla f\left(x\right)$ as a column vector, I tried to implement this on the given $f\left(x\right)$ and what I got is $$ \nabla f\left(x\right)=\nabla\left(gh\right)\left(x\right)=2Ax\cdot\left\langle Bx,x\right\rangle +\left\langle Ax,x\right\rangle \cdot2Bx $$ which seems to have worked fine with a concrete example. But then I got to the Hessian: \begin{align*} \nabla^{2}f\left(x\right) & =\nabla^{2}\left(gh\right)\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\left\langle Ax,x\right\rangle \cdot2B=\\  & =2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{8Ax\cdot Bx}}+\left\langle Ax,x\right\rangle \cdot2B \end{align*} Now as $Ax,Bx$ in $\left(\ast\right)$ are both column vectors I thought I should try this instead $$ \nabla^{2}f\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\ast\right)}}{\underbrace{8Ax\cdot\left(Bx\right)^{T}}}+\left\langle Ax,x\right\rangle \cdot2B $$ But that didn't work with my example. In general I feel the whole process of differentiating functions that are represented by matrices is quite a mystery to me when it comes to where I should transpose and so. Any help is appreciated. Thanks in advance.","I'm trying to understand what is the Hessian matrix of defined by where are symetric matrices. What I know is that if we let and then and . Also by the product rule we have which then gives us Regarding as a column vector, I tried to implement this on the given and what I got is which seems to have worked fine with a concrete example. But then I got to the Hessian: Now as in are both column vectors I thought I should try this instead But that didn't work with my example. In general I feel the whole process of differentiating functions that are represented by matrices is quite a mystery to me when it comes to where I should transpose and so. Any help is appreciated. Thanks in advance.","f\colon\mathbb{R}^{n}\to\mathbb{R} f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle  A,B n\times n g\left(x\right)=\left\langle Ax,x\right\rangle  h\left(x\right)=\left\langle Bx,x\right\rangle  \nabla g\left(x\right)=2Ax,\nabla h\left(x\right)=2Bx \nabla^{2}g\left(x\right)=2A,\nabla^{2}h\left(x\right)=2B \left(fg\right)'=f'g+fg' \begin{align*}
\left(fg\right)'' & =f''g+f'g'+f'g'+fg''=\\
 & =f''g+2f'g'+fg''
\end{align*} \nabla f\left(x\right) f\left(x\right) 
\nabla f\left(x\right)=\nabla\left(gh\right)\left(x\right)=2Ax\cdot\left\langle Bx,x\right\rangle +\left\langle Ax,x\right\rangle \cdot2Bx
 \begin{align*}
\nabla^{2}f\left(x\right) & =\nabla^{2}\left(gh\right)\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\left\langle Ax,x\right\rangle \cdot2B=\\
 & =2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{8Ax\cdot Bx}}+\left\langle Ax,x\right\rangle \cdot2B
\end{align*} Ax,Bx \left(\ast\right) 
\nabla^{2}f\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\ast\right)}}{\underbrace{8Ax\cdot\left(Bx\right)^{T}}}+\left\langle Ax,x\right\rangle \cdot2B
","['matrices', 'derivatives', 'hessian-matrix']"
24,Is the following ratio of gamma functions increasing: $\frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2}$?,Is the following ratio of gamma functions increasing: ?,\frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2},"For $n > 1$ , is the following ratio of gamma functions increasing: $\dfrac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2}$ I suspect that it is at some point where $n > 1$ . I would like figure out if the derivative is increasing or not and if increasing, from what point? I had hoped that this series ψ would be sufficient with: $$\frac{d}{dx}(\ln\Gamma(x)) = \frac{\psi(x)}{dx} = -\gamma + \sum_{k=0}^\infty(\frac{1}{k+1} - \frac{1}{k + x})$$ So, my goal would be to show that the following is increasing for $n \ge 1$ : $$\ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n)$$ This got me to: $$\frac{d}{dx}\left(\ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n)\right) = \frac{\psi(2n - \frac{1.25506n}{\ln n})}{2 - \frac{1.25506}{\ln n} + \frac{1.25506}{\ln^2 n}} - 2\psi(n)$$ When I tried to apply the last part, I was at a loss. How would I complete the argument to determine whether there exists a real $n > 0$ where the function is strictly increasing? Edit 1: I had a thought.  Does the following logic work? An easier problem is: $$\frac{d}{dx}(\ln\Gamma(2n) - 2\ln\Gamma(n)) = \frac{\psi(2n)}{2} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{2n}\right) > 0$$ If I change this to some real constant $c < 1$ : $$\frac{d}{dx}(\ln\Gamma(n(2-c)) - 2\ln\Gamma(n)) = \frac{\psi(n(2-c))}{2-c} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{n(2-c)}\right) > 0$$ Would it now be sufficient to complete the argument by showing that for $n \ge 4$ : $$\frac{1.25506}{\ln n} < 1$$ and showing that: $$\frac{d}{dx}\left(\frac{1.25506}{\ln n}\right) = -\frac{1.25506}{n\ln^2(n)}$$ which is decreasing at $n\ge 4$ . Is this enough to establish the conclusion? Edit 2: To be clear, it should be: $$\frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{[\Gamma(n)]^2}$$","For , is the following ratio of gamma functions increasing: I suspect that it is at some point where . I would like figure out if the derivative is increasing or not and if increasing, from what point? I had hoped that this series ψ would be sufficient with: So, my goal would be to show that the following is increasing for : This got me to: When I tried to apply the last part, I was at a loss. How would I complete the argument to determine whether there exists a real where the function is strictly increasing? Edit 1: I had a thought.  Does the following logic work? An easier problem is: If I change this to some real constant : Would it now be sufficient to complete the argument by showing that for : and showing that: which is decreasing at . Is this enough to establish the conclusion? Edit 2: To be clear, it should be:",n > 1 \dfrac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2} n > 1 \frac{d}{dx}(\ln\Gamma(x)) = \frac{\psi(x)}{dx} = -\gamma + \sum_{k=0}^\infty(\frac{1}{k+1} - \frac{1}{k + x}) n \ge 1 \ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n) \frac{d}{dx}\left(\ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n)\right) = \frac{\psi(2n - \frac{1.25506n}{\ln n})}{2 - \frac{1.25506}{\ln n} + \frac{1.25506}{\ln^2 n}} - 2\psi(n) n > 0 \frac{d}{dx}(\ln\Gamma(2n) - 2\ln\Gamma(n)) = \frac{\psi(2n)}{2} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{2n}\right) > 0 c < 1 \frac{d}{dx}(\ln\Gamma(n(2-c)) - 2\ln\Gamma(n)) = \frac{\psi(n(2-c))}{2-c} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{n(2-c)}\right) > 0 n \ge 4 \frac{1.25506}{\ln n} < 1 \frac{d}{dx}\left(\frac{1.25506}{\ln n}\right) = -\frac{1.25506}{n\ln^2(n)} n\ge 4 \frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{[\Gamma(n)]^2},"['derivatives', 'inequality', 'gamma-function', 'digamma-function']"
25,Lipschitz function and uniform convergence,Lipschitz function and uniform convergence,,"I am struggling with the proof of the following theorem:  Let $(f_n)$ be a sequence of differentiable functions in a closed and bounded set $[a,b]$ s.t. $(f_n(x))$ is convergent for each $x\in [a,b]$ and, for a constant $M\geq0$ , we have $|f_n^{'}(x)|\leq M$ $\qquad$ $\forall n\in\Bbb{N}$ , $\forall x\in [a,b]$ Prove that $(f_n)$ is uniformly convergent in $[a,b]$ ... I tried with this: as a consequence of the Lagrange Theorem we have that if $f_n$ is continuous and differentiable in a closed and bounded set and $f_n'$ is bounded then $f_n$ is Lipschitz. Then $|f_n(x_1)-f_n(x_2)|\leq |f_n^{'}(x) ||x_1-x_2|\leq M(x_1-x_2)<\epsilon$ $\quad$ whenever $|x_1-x_2|<\delta$ .  Thus $(f_n)$ is uniformly continuous. Is this right? and from this how can I prove that $f_n$ is uniformly convergent?","I am struggling with the proof of the following theorem:  Let be a sequence of differentiable functions in a closed and bounded set s.t. is convergent for each and, for a constant , we have , Prove that is uniformly convergent in ... I tried with this: as a consequence of the Lagrange Theorem we have that if is continuous and differentiable in a closed and bounded set and is bounded then is Lipschitz. Then whenever .  Thus is uniformly continuous. Is this right? and from this how can I prove that is uniformly convergent?","(f_n) [a,b] (f_n(x)) x\in [a,b] M\geq0 |f_n^{'}(x)|\leq M \qquad \forall n\in\Bbb{N} \forall x\in [a,b] (f_n) [a,b] f_n f_n' f_n |f_n(x_1)-f_n(x_2)|\leq |f_n^{'}(x) ||x_1-x_2|\leq M(x_1-x_2)<\epsilon \quad |x_1-x_2|<\delta (f_n) f_n","['real-analysis', 'derivatives', 'uniform-convergence', 'lipschitz-functions']"
26,Is this function involving square root smooth?,Is this function involving square root smooth?,,"Let $\psi:[0,1] \to \mathbb{R}$ be a concave, smooth, strictly increasing function satisfying $\psi(0) = 0$ , $\psi(1) = 1$ and $\psi'(0)>1$ . Assume further that $\psi$ is linear in a neighbourhood of zero , and set $c = 2\psi'(0)$ . Note that the assumptions $\psi'(0)>1,\psi(1)=1$ imply that $\psi$ cannot be linear up to $r=1$ -it must become strictly concave at some point. Set $t_0=\sup\{ \psi'(r)+\frac{\psi(r)}{r}=c\}$ . Question: Is $f(r)=\sqrt{c^2-(\psi'(r)+\frac{\psi(r)}{r})^2}$ infinitely differentiable at $t_0$ ? As I explain below, $f(r)=0$ for every $r\le t_0$ . So, this is equivalent to asking whether all the right the derivatives of $f(r)$ exist and are equal to zero at $t_0$ . Here are the details: First, we note that the function $g(r)= \psi'(r)+\frac{\psi(r)}{r}$ is non-increasing, due to the concavity of $\psi$ (see a proof at the end). Also, $\lim_{r \to 0}g(r)=2\psi'(0)=c$ . These facts implies that $g(r) \le c $ for every $r>0$ , and that $g(r)=c$ on $[0,t_0]$ . Equivalently, $\psi|_{[0,t_0]}$ is the solution to the ODE $y(r)'+y(r)/r=c$ which implies that $\psi(r)$ is linear on $[0,t_0]$ . The fact that $g(r)$ is non-increasing implies that $g(r)<c$ for every $r>t_0$ . As explained in this partial answer , the smoothness of $\psi$ implies that $$\sqrt{c^2 - \left(\psi'(t_0+h) + \frac{\psi(t_0+h)}{t_0+h}\right)^2} = o(h^n),$$ for any $n>1$ . However, unfortunately, this fact alone does not imply that this creature is smooth at $t=t_0$ . A proof that $g(r)$ is non-increasing: $$ g'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}), $$ and both summands are non-positive. $\psi'' \le 0$ by concavity. Since $\psi(r)=\int_0^r \psi'(t)dt \ge \int_0^r \psi'(r)dt=r\psi'(r)$ , the second summand is also non-positive.","Let be a concave, smooth, strictly increasing function satisfying , and . Assume further that is linear in a neighbourhood of zero , and set . Note that the assumptions imply that cannot be linear up to -it must become strictly concave at some point. Set . Question: Is infinitely differentiable at ? As I explain below, for every . So, this is equivalent to asking whether all the right the derivatives of exist and are equal to zero at . Here are the details: First, we note that the function is non-increasing, due to the concavity of (see a proof at the end). Also, . These facts implies that for every , and that on . Equivalently, is the solution to the ODE which implies that is linear on . The fact that is non-increasing implies that for every . As explained in this partial answer , the smoothness of implies that for any . However, unfortunately, this fact alone does not imply that this creature is smooth at . A proof that is non-increasing: and both summands are non-positive. by concavity. Since , the second summand is also non-positive.","\psi:[0,1] \to \mathbb{R} \psi(0) = 0 \psi(1) = 1 \psi'(0)>1 \psi c = 2\psi'(0) \psi'(0)>1,\psi(1)=1 \psi r=1 t_0=\sup\{ \psi'(r)+\frac{\psi(r)}{r}=c\} f(r)=\sqrt{c^2-(\psi'(r)+\frac{\psi(r)}{r})^2} t_0 f(r)=0 r\le t_0 f(r) t_0 g(r)= \psi'(r)+\frac{\psi(r)}{r} \psi \lim_{r \to 0}g(r)=2\psi'(0)=c g(r) \le c  r>0 g(r)=c [0,t_0] \psi|_{[0,t_0]} y(r)'+y(r)/r=c \psi(r) [0,t_0] g(r) g(r)<c r>t_0 \psi \sqrt{c^2 - \left(\psi'(t_0+h) + \frac{\psi(t_0+h)}{t_0+h}\right)^2} = o(h^n), n>1 t=t_0 g(r) 
g'(r)=\psi''(r)+\frac{1}{r}(\psi'(r)-\frac{\psi(r)}{r}),
 \psi'' \le 0 \psi(r)=\int_0^r \psi'(t)dt \ge \int_0^r \psi'(r)dt=r\psi'(r)","['real-analysis', 'calculus', 'derivatives', 'convex-analysis', 'singularity']"
27,Deriving a bound for function u(t) in the Cauchy Problem,Deriving a bound for function u(t) in the Cauchy Problem,,"Suppose that $u: [0, τ] \rightarrow \infty$ solves the Cauchy problem: $u'(t) = \frac{t^2}{1+u(t)^2}, u(0) = 1$ Find a constant $B ≥ 0$ such that $|u(t)| ≤ B$ for all $t \in [0, τ]$ . Your value of B may depend on τ. So the unit we are learning is error bounds for solutions to the Cauchy problem. This makes me think a logical first step would be to take the time derivative of $u'(t)$ and try to find an expression for $u''(t)$ where u(t) can be somehow factored out of the resulting expression. So I take $u'(t) = f(t,u)$ , then use chain rule to find $u''(t)$ : u''(t) = $d \over dt$ u'(t) = $\frac{2t(1+u^2) - u'(t) \cdot 2u \cdot t^2}{(1+u^2)^2}$ Now I plug in $u'(t)$ to simplify the numerator a bit: $u''(t) = \frac{2t(1+u^2) - \frac{t^2}{1+u^2} \cdot 2u \cdot t^2}{(1+u^2)^2}$ = $\frac{2t(1+u^2)^2 - 2u \cdot t^4}{(1+u^2)^3}$ = ... At this point I am pretty stuck. I want to extract u = u(t) from this expression, and use information about $t \in [0, τ]$ to bound u(t) but the fraction looks like a mess. Am I missing something or is my approach incorrect? For the record, for the same problem except with $f(t, u) = u'(t) = \frac{tu}{1+t^2}$ the exact same approach of differentiating u'(t) works out nicely.","Suppose that solves the Cauchy problem: Find a constant such that for all . Your value of B may depend on τ. So the unit we are learning is error bounds for solutions to the Cauchy problem. This makes me think a logical first step would be to take the time derivative of and try to find an expression for where u(t) can be somehow factored out of the resulting expression. So I take , then use chain rule to find : u''(t) = u'(t) = Now I plug in to simplify the numerator a bit: = = ... At this point I am pretty stuck. I want to extract u = u(t) from this expression, and use information about to bound u(t) but the fraction looks like a mess. Am I missing something or is my approach incorrect? For the record, for the same problem except with the exact same approach of differentiating u'(t) works out nicely.","u: [0, τ] \rightarrow \infty u'(t) = \frac{t^2}{1+u(t)^2}, u(0) = 1 B ≥ 0 |u(t)| ≤ B t \in [0, τ] u'(t) u''(t) u'(t) = f(t,u) u''(t) d \over dt \frac{2t(1+u^2) - u'(t) \cdot 2u \cdot t^2}{(1+u^2)^2} u'(t) u''(t) = \frac{2t(1+u^2) - \frac{t^2}{1+u^2} \cdot 2u \cdot t^2}{(1+u^2)^2} \frac{2t(1+u^2)^2 - 2u \cdot t^4}{(1+u^2)^3} t \in [0, τ] f(t, u) = u'(t) = \frac{tu}{1+t^2}","['ordinary-differential-equations', 'derivatives', 'upper-lower-bounds', 'cauchy-problem']"
28,"If $f$ is monotone increasing and $f$ is differentiable at $x_{0}$, then $f'(x_{0}) \geq 0$.","If  is monotone increasing and  is differentiable at , then .",f f x_{0} f'(x_{0}) \geq 0,"Let $X$ be a subset of $\textbf{R}$ , let $x_{0}\in X$ be a limit point of $X$ , and let $f:X\rightarrow\textbf{R}$ be a function. If $f$ is monotone increasing and $f$ is differentiable at $x_{0}$ , then $f'(x_{0}) \geq 0$ . If $f$ is monotone decreasing and $f$ is differentiable at $x_{0}$ , then $f'(x_{0})\leq 0$ . MY ATTEMPT Lemma Let $X\subseteq\textbf{R}$ , $f:X\rightarrow\textbf{R}$ , $g:X\rightarrow\textbf{R}$ , $x_{0}\in X$ is an adherent point, $f(x) \leq g(x)$ for every $x\in X$ and $\displaystyle\lim_{x\rightarrow x_{0}}f(x) = L$ and $\displaystyle\lim_{x\rightarrow x_{0}}g(x) = M$ . Then we have that $L \leq M$ . Proof According to the definition of limit, for every $\varepsilon > 0$ , there are $\delta_{1} > 0$ and $\delta_{2} > 0$ such that \begin{align*} \begin{cases} 0 < |x - x_{0}| < \delta_{1}\\\\ 0 < |x - x_{0}| < \delta_{2} \end{cases} \Longrightarrow \begin{cases} |f(x) - L| < \varepsilon\\\\ |g(x) - M| < \varepsilon \end{cases} \Longrightarrow L - \varepsilon < f(x) \leq g(x) < M + \varepsilon \end{align*} Let us assume that $L > M$ . In this case, we can choose $\displaystyle\varepsilon = \frac{L - M}{3}$ , whence we get that \begin{align*} M - L + 2\varepsilon > M - L + \frac{2(L - M)}{3} = \frac{M - L}{3} > 0 \Longrightarrow M > L \end{align*} which leads to a contradiction. Therefore the original claim is true and $L \leq M$ . Solution Assuming that $f$ is monotone increasing at $x_{0}$ , we have that \begin{align*} \frac{f(x) - f(x_{0})}{x - x_{0}} \geq 0 \end{align*} Taking the limit from both sides to $x_{0}$ we conclude that \begin{align*} \lim_{x\rightarrow x_{0}}\frac{f(x) - f(x_{0})}{x - x_{0}} = f'(x_{0}) \geq 0 = \lim_{x\rightarrow x_{0}}0 \end{align*} simliar reasoning applies to the monotone decreasing case, and we are done. Could someone please verify if I am arguing correctly? Any other solution is welcome.","Let be a subset of , let be a limit point of , and let be a function. If is monotone increasing and is differentiable at , then . If is monotone decreasing and is differentiable at , then . MY ATTEMPT Lemma Let , , , is an adherent point, for every and and . Then we have that . Proof According to the definition of limit, for every , there are and such that Let us assume that . In this case, we can choose , whence we get that which leads to a contradiction. Therefore the original claim is true and . Solution Assuming that is monotone increasing at , we have that Taking the limit from both sides to we conclude that simliar reasoning applies to the monotone decreasing case, and we are done. Could someone please verify if I am arguing correctly? Any other solution is welcome.","X \textbf{R} x_{0}\in X X f:X\rightarrow\textbf{R} f f x_{0} f'(x_{0}) \geq 0 f f x_{0} f'(x_{0})\leq 0 X\subseteq\textbf{R} f:X\rightarrow\textbf{R} g:X\rightarrow\textbf{R} x_{0}\in X f(x) \leq g(x) x\in X \displaystyle\lim_{x\rightarrow x_{0}}f(x) = L \displaystyle\lim_{x\rightarrow x_{0}}g(x) = M L \leq M \varepsilon > 0 \delta_{1} > 0 \delta_{2} > 0 \begin{align*}
\begin{cases}
0 < |x - x_{0}| < \delta_{1}\\\\
0 < |x - x_{0}| < \delta_{2}
\end{cases} \Longrightarrow
\begin{cases}
|f(x) - L| < \varepsilon\\\\
|g(x) - M| < \varepsilon
\end{cases} \Longrightarrow L - \varepsilon < f(x) \leq g(x) < M + \varepsilon
\end{align*} L > M \displaystyle\varepsilon = \frac{L - M}{3} \begin{align*}
M - L + 2\varepsilon > M - L + \frac{2(L - M)}{3} = \frac{M - L}{3} > 0 \Longrightarrow M > L
\end{align*} L \leq M f x_{0} \begin{align*}
\frac{f(x) - f(x_{0})}{x - x_{0}} \geq 0
\end{align*} x_{0} \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{f(x) - f(x_{0})}{x - x_{0}} = f'(x_{0}) \geq 0 = \lim_{x\rightarrow x_{0}}0
\end{align*}","['real-analysis', 'derivatives', 'solution-verification', 'alternative-proof']"
29,Derivative function at the point,Derivative function at the point,,"Can anyone help me out with the question below? Let $ f: \Bbb{R}→ \Bbb{R} $ be a function such that $ \vert f (x) −f (p)\vert\le \vert x-p \vert^{3\over 2}$ , for all $ x, p \in \Bbb{R}$ . Show that $ f$ is derivable, calculating its derivative at each point.","Can anyone help me out with the question below? Let be a function such that , for all . Show that is derivable, calculating its derivative at each point."," f: \Bbb{R}→ \Bbb{R}   \vert f (x) −f (p)\vert\le \vert x-p \vert^{3\over 2}  x, p \in \Bbb{R}  f","['calculus', 'derivatives']"
30,Continuity of Markov chain' trasnition matrix functions,Continuity of Markov chain' trasnition matrix functions,,"I'm studying Continuous-time Markov chains . Given the definition of continous-time transition functions matrix : $P(t) = (P\small{ij}(t)) = \mathbb{P}(X\small{t} = j | X\small{0} = i)$ So I have two questions related to continuity of this functions: Can there be transitional functions which are non-continous? Can there be transitional functions which are continuous, but non-differentiable?","I'm studying Continuous-time Markov chains . Given the definition of continous-time transition functions matrix : So I have two questions related to continuity of this functions: Can there be transitional functions which are non-continous? Can there be transitional functions which are continuous, but non-differentiable?",P(t) = (P\small{ij}(t)) = \mathbb{P}(X\small{t} = j | X\small{0} = i),"['calculus', 'probability-theory', 'derivatives', 'continuity', 'markov-chains']"
31,Derivative of row-wise softmax matrix w.r.t. matrix itself,Derivative of row-wise softmax matrix w.r.t. matrix itself,,"Define matrix $\mathbf{M}' \in \mathbb{R}^{n \times k}$ as the result of the row-wise softmax operation on matrix $\mathbf{M} \in \mathbb{R}^{n \times k}$ . Hence, $$\mathbf{M}'_{ij} = \frac{\exp{\mathbf{M}_{ij}}}{\sum\limits_{b=1}^k \exp{\mathbf{M}_{ib}}}.$$ Now, I look at the derivative of a scalar function, e.g., the Frobenius norm, with respect to $\textbf{M}$ , namely $$ \frac{\partial E}{\partial \textbf{M}} = \frac{\partial \left\Vert \textbf{X} - \textbf{M}'\textbf{H}\right\Vert_F}{\partial \textbf{M}}. $$ I don't have any problem calulating the derivative of the above function w.r.t. $\textbf{M}'$ . However, I am interested in finding the derivative w.r.t. $\textbf{M}$ , which means that I somehow have to deal with the row-wise softmax operation. Since softmax is a vector function, but I am interested in finding the derivative w.r.t. the whole matrix $\textbf{M}$ at once, I don't know how to deal with it best. Do I need to calculate the derivative w.r.t. each vector $\textbf{M}_{i:}$ seperately? Also, the derivative of the softmax would yield a Jacobian matrix of dimensionality $k \times k$ . Getting one Jacobian for each row vector $\textbf{M}_{i:}$ seems to mess up the dimensionality, assuming I would need to concatenate all those Jacobians... I am not sure where my mistake is. However, it feels like I am stuck. It would be great if you could help me out :) Thanks in advance and best regards.","Define matrix as the result of the row-wise softmax operation on matrix . Hence, Now, I look at the derivative of a scalar function, e.g., the Frobenius norm, with respect to , namely I don't have any problem calulating the derivative of the above function w.r.t. . However, I am interested in finding the derivative w.r.t. , which means that I somehow have to deal with the row-wise softmax operation. Since softmax is a vector function, but I am interested in finding the derivative w.r.t. the whole matrix at once, I don't know how to deal with it best. Do I need to calculate the derivative w.r.t. each vector seperately? Also, the derivative of the softmax would yield a Jacobian matrix of dimensionality . Getting one Jacobian for each row vector seems to mess up the dimensionality, assuming I would need to concatenate all those Jacobians... I am not sure where my mistake is. However, it feels like I am stuck. It would be great if you could help me out :) Thanks in advance and best regards.","\mathbf{M}' \in \mathbb{R}^{n \times k} \mathbf{M} \in \mathbb{R}^{n \times k} \mathbf{M}'_{ij} = \frac{\exp{\mathbf{M}_{ij}}}{\sum\limits_{b=1}^k \exp{\mathbf{M}_{ib}}}. \textbf{M} 
\frac{\partial E}{\partial \textbf{M}} = \frac{\partial \left\Vert \textbf{X} - \textbf{M}'\textbf{H}\right\Vert_F}{\partial \textbf{M}}.
 \textbf{M}' \textbf{M} \textbf{M} \textbf{M}_{i:} k \times k \textbf{M}_{i:}","['derivatives', 'matrix-calculus', 'scalar-fields']"
32,Derivative parameric equation,Derivative parameric equation,,"I would like to compute the derivative of the following parametric equations w.r.t $a$ and $b$ : $x=a~ \text{cos}(t)$ and $y= b~ \text{sin}(t)$ with $t \in [0, b]$ . Derivative w.r.t $a$ are easy to compute : $d_a x = \text{cos}(t)$ , $d_a y = 0$ with $t \in [0, b]$ . However, the ones w.r.t $b$ are somewhat not intuitive since $t$ depends on $b$ . For example, if $t=b$ then $d_b x =-ab \text{sin}(b)$ I would appreciate if someone has insight on how to compute these derivatives. Thank you. Reda E.","I would like to compute the derivative of the following parametric equations w.r.t and : and with . Derivative w.r.t are easy to compute : , with . However, the ones w.r.t are somewhat not intuitive since depends on . For example, if then I would appreciate if someone has insight on how to compute these derivatives. Thank you. Reda E.","a b x=a~ \text{cos}(t) y= b~ \text{sin}(t) t \in [0, b] a d_a x = \text{cos}(t) d_a y = 0 t \in [0, b] b t b t=b d_b x =-ab \text{sin}(b)","['derivatives', 'parametric']"
33,Why is the assumption $(dx)^2 = 0$ actually correct instead of just approximately correct?,Why is the assumption  actually correct instead of just approximately correct?,(dx)^2 = 0,"Imagine dividing a sphere into concentric spherical shells of thickness $dr$ and inner radius $r$ . The volume of each shell is $$dV = \frac{4\pi}{3}  [ (r + dr)^3 - r^3]$$ Expand the cubic expressions, we get: $$ (r + dr)^3 - r^3 = r^3 + dr^3 + 3r^2 dr + 3rdr^2 - r^3 = 3r^2dr + 3rdr^2 + dr^3 $$ Assuming that $dr^3 = 0$ and $rdr^2 = 0$ , we get: $$(r + dr)^3 -r^3= 3r^2dr$$ Thus, the volume of each shell is $dV = 4\pi r^2dr$ . If we integrate along the radius, then we get $$\int_0^R 4\pi r^2dr =  \frac43\pi R^3$$ This confirms that our analysis of the spherical shell volumes is correct. However, this analysis relies on the assumption that $dr^3 = 0$ and $rdr^2 = 0$ . My question is why are these assumptions correct? If we assume those values are zero, shouldn't the final value just be approximately correct by an infinitesimal amount instead of being absolutely correct?","Imagine dividing a sphere into concentric spherical shells of thickness and inner radius . The volume of each shell is Expand the cubic expressions, we get: Assuming that and , we get: Thus, the volume of each shell is . If we integrate along the radius, then we get This confirms that our analysis of the spherical shell volumes is correct. However, this analysis relies on the assumption that and . My question is why are these assumptions correct? If we assume those values are zero, shouldn't the final value just be approximately correct by an infinitesimal amount instead of being absolutely correct?","dr r dV = \frac{4\pi}{3}  [ (r + dr)^3 - r^3] 
(r + dr)^3 - r^3 = r^3 + dr^3 + 3r^2 dr + 3rdr^2 - r^3 = 3r^2dr + 3rdr^2 + dr^3
 dr^3 = 0 rdr^2 = 0 (r + dr)^3 -r^3= 3r^2dr dV = 4\pi r^2dr \int_0^R 4\pi r^2dr =  \frac43\pi R^3 dr^3 = 0 rdr^2 = 0","['calculus', 'derivatives', 'volume']"
34,Intuitively predict the direction of the acceleration vector of a single parameter vector valued function,Intuitively predict the direction of the acceleration vector of a single parameter vector valued function,,"The velocity vector of a Vector valued function (or position function), (for example $f(t)= \lt x(t),y(t) \gt$ )  seems to always be tangent to the curve of motion, its pretty intuitive why that is. However intuition about what the acceleration vector $f ^{\prime \prime}(t)$ looks like or where does it point is hard to find. Can I somehow predict or get an intuition on what the acceleration vector might be or where does it point to? how?.","The velocity vector of a Vector valued function (or position function), (for example )  seems to always be tangent to the curve of motion, its pretty intuitive why that is. However intuition about what the acceleration vector looks like or where does it point is hard to find. Can I somehow predict or get an intuition on what the acceleration vector might be or where does it point to? how?.","f(t)= \lt x(t),y(t) \gt f ^{\prime \prime}(t)","['derivatives', 'differential-geometry', 'vector-analysis', 'physics']"
35,What is the derivative of $x'(A+tB)^{-1}x$ with respect to $t$?,What is the derivative of  with respect to ?,x'(A+tB)^{-1}x t,"Can you please help me with the derivative of $x'(A+tB)^{-1}x$ with respect to $t$ ? ( $x$ is a vector, $A$ and $B$ are $n$ by $n$ matrices and $t$ is a scalar) I'm not sure, but is it $x'Fx$ where $F=-(A+tB)^{-1}(B)(A+tB)^{-1}$ ? [I applied $\frac{\delta C^{-1}}{\delta t} = -C^{-1}\frac{\delta C}{\delta t}C^{-1}$ Where here $C=(A+tB)$ , the vectors $x$ remains as it is.] Thanks in advance!","Can you please help me with the derivative of with respect to ? ( is a vector, and are by matrices and is a scalar) I'm not sure, but is it where ? [I applied Where here , the vectors remains as it is.] Thanks in advance!",x'(A+tB)^{-1}x t x A B n n t x'Fx F=-(A+tB)^{-1}(B)(A+tB)^{-1} \frac{\delta C^{-1}}{\delta t} = -C^{-1}\frac{\delta C}{\delta t}C^{-1} C=(A+tB) x,"['matrices', 'derivatives', 'matrix-calculus']"
36,Applying the Mean Value Theorem to a Natural Log Function,Applying the Mean Value Theorem to a Natural Log Function,,"After my prof's lecture I came away unsure when I can and cannot apply the Mean Value Theorem. More specifically when the domain of $f'$ extends beyond the domain of $f$ (like in the case of a function using a natural $\ln$ ). In the case of $f(x)= \ln(x+6)$ , on interval $[3,8]$ the domain would be $x>-6$ or $(-6,∞)$ . -This would satisfy the 1st hypothesis since closed interval $[3,8]$ falls within the domain of $f$ , $(-6,∞)$ and therefore $f$ would be continuous on $[3,8]$ . -But when I take the derivative, $f'(x)= 1/x+6$ , the domain of $f'$ is $(-∞,-6)U(-6,∞)$ My question then is: since the domain of $f'$ extends beyond $f$ , does this not satisfy the 2nd hypothesis for MVT on the given interval $[3,8]$ ?","After my prof's lecture I came away unsure when I can and cannot apply the Mean Value Theorem. More specifically when the domain of extends beyond the domain of (like in the case of a function using a natural ). In the case of , on interval the domain would be or . -This would satisfy the 1st hypothesis since closed interval falls within the domain of , and therefore would be continuous on . -But when I take the derivative, , the domain of is My question then is: since the domain of extends beyond , does this not satisfy the 2nd hypothesis for MVT on the given interval ?","f' f \ln f(x)= \ln(x+6) [3,8] x>-6 (-6,∞) [3,8] f (-6,∞) f [3,8] f'(x)= 1/x+6 f' (-∞,-6)U(-6,∞) f' f [3,8]","['calculus', 'derivatives']"
37,"What is $\frac{\partial\det(L)}{\partial x_i}$, where $L_{i,j} = \exp\left(-\frac{(x_i-x_j)^2} {2\sigma ^2} \right)$ using Jacobi's formula?","What is , where  using Jacobi's formula?","\frac{\partial\det(L)}{\partial x_i} L_{i,j} = \exp\left(-\frac{(x_i-x_j)^2} {2\sigma ^2} \right)","We have $n \times n$ matrix $L$ , given by the following Gaussian kernel $$L_{i,j} = \exp\left(-\frac{(x_i-x_j)^2} {2\sigma ^2} \right)$$ where the points $x_i$ and $x_j$ are real numbers that can be thought as positions of points $i$ and $j$ . ( $L$ can be seen as a covariance matrix, describing covariance depending on distance between points. The higher distance between points $i$ and $j$ the less the covariance.) I am interested in finding $$ \frac{\partial\det(L)}{\partial x_i}$$ From Matrix Book , can I use Jacobi's formula? $$\frac{\partial\det(L)}{\partial x}= \det (L) \operatorname{Tr}\left( L^{-1} \frac{\partial L}{\partial x}\right).$$ Is it correct that $\frac{\partial\det(L)}{\partial x_i} = \det (L) \operatorname{Tr}\left( L^{-1} \frac{\partial L}{\partial x_i}\right)$ where $\frac{\partial L}{\partial x_i}$ is the matrix given by the terms $\frac{\partial L_{k,l}}{\partial x_i}=0$ for $k,l \neq i$ and $\frac{\partial L_{i,j}}{\partial x_i}=\frac{\partial L_{j,i}}{\partial x_i}=-\frac{(x_i-x_j)}{ \sigma ^2} L_{i,j}$ ?","We have matrix , given by the following Gaussian kernel where the points and are real numbers that can be thought as positions of points and . ( can be seen as a covariance matrix, describing covariance depending on distance between points. The higher distance between points and the less the covariance.) I am interested in finding From Matrix Book , can I use Jacobi's formula? Is it correct that where is the matrix given by the terms for and ?","n \times n L L_{i,j} = \exp\left(-\frac{(x_i-x_j)^2} {2\sigma ^2} \right) x_i x_j i j L i j  \frac{\partial\det(L)}{\partial x_i} \frac{\partial\det(L)}{\partial x}= \det (L) \operatorname{Tr}\left( L^{-1} \frac{\partial L}{\partial x}\right). \frac{\partial\det(L)}{\partial x_i} = \det (L) \operatorname{Tr}\left( L^{-1} \frac{\partial L}{\partial x_i}\right) \frac{\partial L}{\partial x_i} \frac{\partial L_{k,l}}{\partial x_i}=0 k,l \neq i \frac{\partial L_{i,j}}{\partial x_i}=\frac{\partial L_{j,i}}{\partial x_i}=-\frac{(x_i-x_j)}{ \sigma ^2} L_{i,j}","['matrices', 'derivatives', 'partial-derivative']"
38,Value of a and b in a function if its local minimum is at a certain point?,Value of a and b in a function if its local minimum is at a certain point?,,I have a function as written below: $$ g(x) = x^3 + ax^2 + bx $$ It is known to have a local minimum at $ x = -\frac{1}{\sqrt{3}} $ whose value is $ y = -\frac{2\sqrt{3}}{9} $ I have tried using the second derivative test and I got stuck at $ -\frac{6}{\sqrt{3}} + 2a > 0 $ (second derivative at c is > 0) and $ 1 -\frac{2a}{\sqrt{3}} + b = 0 $ (first derivative equals to 0) Any ideas?,I have a function as written below: It is known to have a local minimum at whose value is I have tried using the second derivative test and I got stuck at (second derivative at c is > 0) and (first derivative equals to 0) Any ideas?, g(x) = x^3 + ax^2 + bx   x = -\frac{1}{\sqrt{3}}   y = -\frac{2\sqrt{3}}{9}   -\frac{6}{\sqrt{3}} + 2a > 0   1 -\frac{2a}{\sqrt{3}} + b = 0 ,"['analysis', 'derivatives', 'systems-of-equations', 'maxima-minima', 'applications']"
39,"Am I right in thinking that differential of $f(x)$ is $dy = {\Delta}y$ when ${\Delta}x$ is infinitesimally small, otherwise $dy$ $\ne$ ${\Delta}y$?","Am I right in thinking that differential of  is  when  is infinitesimally small, otherwise   ?",f(x) dy = {\Delta}y {\Delta}x dy \ne {\Delta}y,"Since ${\Delta}y$ is known to be the $exact$ change in $f(x)$ , and $dy$ is the $approximation$ of the change if ${\Delta}x$ is $not$ infinitesimal, then what is the difference between ${\Delta}x$ and $dx$ ? Or does $dy$ only exist when ${\Delta}x$ is infinitesimally small and is not defined otherwise ( $however,$ might be defined when using numerical methods?) I am new to Maths, correct me if anything is wrong. I need intuitive understanding to learn not to take integration/differentiation as a black box operation.","Since is known to be the change in , and is the of the change if is infinitesimal, then what is the difference between and ? Or does only exist when is infinitesimally small and is not defined otherwise ( might be defined when using numerical methods?) I am new to Maths, correct me if anything is wrong. I need intuitive understanding to learn not to take integration/differentiation as a black box operation.","{\Delta}y exact f(x) dy approximation {\Delta}x not {\Delta}x dx dy {\Delta}x however,","['calculus', 'integration', 'derivatives']"
40,"generalized derivative of $\log |x|$ (sobolev derivative), where $x\in (-1,1)$","generalized derivative of  (sobolev derivative), where","\log |x| x\in (-1,1)","Let $u\in L_{loc}(a,b)$ and $\phi \in C_0^{\infty}$ . Function $v$ is generalized derivative of $u$ , if $$1)v\in L_{loc}(a,b)$$ $$2)\int_{a}^bu(x)\phi'(x)dx=-\int_{a}^bv(x)\phi(x)dx $$ for $\forall \phi \in C_0^{\infty}$ I am trying to find the generalized derivative of $ln|x|$ when $x\in (-1,1)$ . There is one problem is point $0$ . I tried to cut special point by using limit. By definition: \begin{align*} \int_{-1}^1 \log|x|\phi'(x)dx     &=\int_{-1}^0 \log(-x)\phi'(x)dx+\int_0^1 \log(x)\phi'(x)dx \\     &=\lim_{\epsilon\to0}\int_{-1}^{-\epsilon}\log(-x)\phi'(x)dx+\lim_{\delta\to0}\int_{\delta}^1\log(x)\phi'(x)dx \\     &=\lim_{\epsilon\to0}\left[\log(-x)\phi(x)|_{-1}^{-\epsilon}-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[\log(x)\phi(x)|_{\delta}^{1}-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\     &=\lim_{\epsilon\to 0}\left[\log(\epsilon)\phi(-\epsilon)-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[-\log(\delta)\phi(\delta)-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\    &=\lim_{\epsilon\to 0, \delta\to 0}[\log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta)]-\lim_{\epsilon\to 0, \delta\to 0}\left[\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx+\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \end{align*} For existence the generalized derivative must be $\log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta) = 0$ and integrals must converge. But $\frac{1}{x}\notin L_{loc}(-1,1)$ and the equality with logarithms is not right for all $\phi$ . Then I conclude that the derivative does not exist. Is it right?","Let and . Function is generalized derivative of , if for I am trying to find the generalized derivative of when . There is one problem is point . I tried to cut special point by using limit. By definition: For existence the generalized derivative must be and integrals must converge. But and the equality with logarithms is not right for all . Then I conclude that the derivative does not exist. Is it right?","u\in L_{loc}(a,b) \phi \in C_0^{\infty} v u 1)v\in L_{loc}(a,b) 2)\int_{a}^bu(x)\phi'(x)dx=-\int_{a}^bv(x)\phi(x)dx  \forall \phi \in C_0^{\infty} ln|x| x\in (-1,1) 0 \begin{align*}
\int_{-1}^1 \log|x|\phi'(x)dx
    &=\int_{-1}^0 \log(-x)\phi'(x)dx+\int_0^1 \log(x)\phi'(x)dx \\
    &=\lim_{\epsilon\to0}\int_{-1}^{-\epsilon}\log(-x)\phi'(x)dx+\lim_{\delta\to0}\int_{\delta}^1\log(x)\phi'(x)dx \\
    &=\lim_{\epsilon\to0}\left[\log(-x)\phi(x)|_{-1}^{-\epsilon}-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[\log(x)\phi(x)|_{\delta}^{1}-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\
    &=\lim_{\epsilon\to 0}\left[\log(\epsilon)\phi(-\epsilon)-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[-\log(\delta)\phi(\delta)-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\
   &=\lim_{\epsilon\to 0, \delta\to 0}[\log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta)]-\lim_{\epsilon\to 0, \delta\to 0}\left[\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx+\int_{\delta}^1\frac{\phi(x)}{x}dx\right]
\end{align*} \log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta) = 0 \frac{1}{x}\notin L_{loc}(-1,1) \phi","['derivatives', 'sobolev-spaces']"
41,"Find the slope of the graph of $xy-2y^2=8$ at $(10,4)$",Find the slope of the graph of  at,"xy-2y^2=8 (10,4)","Find the slope of the graph of $xy-2y^2=8$ at $(10,4)$ So there are two different routes we could take: We could implicitly differentiate, then solve for $\frac{dy}{dx}$ (the slope), then plug in the point $(10,4)$ . Or, we could implicitly differentiate, then plug in the point $(10,4)$ , then solve for $\frac{dy}{dx}$ (the slope). in my solution I will solve for $\frac{dy}{dx}$ and THEN plug in the point $(10,4)$ Solution: $$\frac{d}{dx}(xy-2y^2)=\frac{d}{dx}8=0$$ $$\left(\frac{d}{dx}(xy)-2\frac{d}{dx}(y^2)\right)=0$$ We will need to use the product rule to evaluate $\frac{d}{dx}(xy)$ : $$\left(\left(\frac{d}{dx}x\right)y+\left(x\left(\frac{d}{dx}y\right)\right)-2\left(2y\frac{dy}{dx}\right)\right)=0$$ $$(1)y+\left(x(1)\frac{dy}{dx}\right)-4y\frac{dy}{dx}=0$$ $$y+x\frac{dy}{dx}-4y\frac{dy}{dx}=0$$ $$(x-4y)\frac{dy}{dx}=-y$$ $$\frac{dy}{dx}=\frac{-y}{x-4y}$$ Cool, so we have now took the implicit derivative and then solved for $\frac{dy}{dx}$ , now we plug in $(10,4)$ $$\frac{dy}{dx}=\frac{-4}{10-4(4)}$$ $$\frac{dy}{dx}=\frac{-4}{-6}$$ $$\frac{dy}{dx}=\frac{2}{3}$$ And so the slope of $xy-2y^2=8$ at $(10,4)$ is $\frac{2}{3}$","Find the slope of the graph of at So there are two different routes we could take: We could implicitly differentiate, then solve for (the slope), then plug in the point . Or, we could implicitly differentiate, then plug in the point , then solve for (the slope). in my solution I will solve for and THEN plug in the point Solution: We will need to use the product rule to evaluate : Cool, so we have now took the implicit derivative and then solved for , now we plug in And so the slope of at is","xy-2y^2=8 (10,4) \frac{dy}{dx} (10,4) (10,4) \frac{dy}{dx} \frac{dy}{dx} (10,4) \frac{d}{dx}(xy-2y^2)=\frac{d}{dx}8=0 \left(\frac{d}{dx}(xy)-2\frac{d}{dx}(y^2)\right)=0 \frac{d}{dx}(xy) \left(\left(\frac{d}{dx}x\right)y+\left(x\left(\frac{d}{dx}y\right)\right)-2\left(2y\frac{dy}{dx}\right)\right)=0 (1)y+\left(x(1)\frac{dy}{dx}\right)-4y\frac{dy}{dx}=0 y+x\frac{dy}{dx}-4y\frac{dy}{dx}=0 (x-4y)\frac{dy}{dx}=-y \frac{dy}{dx}=\frac{-y}{x-4y} \frac{dy}{dx} (10,4) \frac{dy}{dx}=\frac{-4}{10-4(4)} \frac{dy}{dx}=\frac{-4}{-6} \frac{dy}{dx}=\frac{2}{3} xy-2y^2=8 (10,4) \frac{2}{3}","['calculus', 'derivatives']"
42,Mean value theorem and the continuity of step length,Mean value theorem and the continuity of step length,,"Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}, f\in C^1$ . Then for two points $a,b \in \mathbb{R}$ , we know by mean value theorem we know that there exists $t \in (0, 1)$ such that $$ f^\prime(a+t(b-a)) = \frac{f(b)-f(a)}{b-a} $$ We can view $t$ as a parameter collectively decided by $a,b$ and $f(\cdot)$ . Now, suppose that we hold $a$ to be fixed and move $b$ , what can be said about the changes in $t$ corresponding to the changes in $b$ (e.g. can we argue if $t(a,b)$ is continuous)? Thanks in advance for your hints/suggestions!","Suppose that . Then for two points , we know by mean value theorem we know that there exists such that We can view as a parameter collectively decided by and . Now, suppose that we hold to be fixed and move , what can be said about the changes in corresponding to the changes in (e.g. can we argue if is continuous)? Thanks in advance for your hints/suggestions!","f: \mathbb{R} \rightarrow \mathbb{R}, f\in C^1 a,b \in \mathbb{R} t \in (0, 1) 
f^\prime(a+t(b-a)) = \frac{f(b)-f(a)}{b-a}
 t a,b f(\cdot) a b t b t(a,b)","['calculus', 'integration', 'derivatives', 'rolles-theorem']"
43,Derivative of Dirac delta distribution,Derivative of Dirac delta distribution,,How is the derivative of $\delta(x-y)$ with respect to $x$ related to the derivative with respect to $y$ ? I suspect they differ by a minus sign but I'm not sure. Both $x$ and $y$ are real variables.,How is the derivative of with respect to related to the derivative with respect to ? I suspect they differ by a minus sign but I'm not sure. Both and are real variables.,\delta(x-y) x y x y,"['calculus', 'derivatives', 'distribution-theory', 'dirac-delta']"
44,Fréchet Derivative of a functional appearing in variational calculus,Fréchet Derivative of a functional appearing in variational calculus,,"I would like to compute the Fréchet derivative of the functional $$ J: C^1[a,b] \rightarrow \mathbb R$$ $$y \mapsto \int_a^b L(x,y(x),y'(x)) \, dx $$ where $L$ is $C^2$ in all components. The Fréchet derivative of $J$ in a point $y$ is defined to be the bounded linear operator $A_y$ such that $$\lim_{||h||_{C^1([a,b])} \rightarrow 0} \frac{|J(y+h) -J(y) -A_yh|}{||h||_{C^1([a,b])}}=0$$ In my book, it says it is easily computable by Taylor expansions, but I really do not see how. They just go from there to Euler-Lagrange equation immediately. Online, I saw many convincing heuristics but no rigorous proof using the definition of the Fréchet derivative.","I would like to compute the Fréchet derivative of the functional where is in all components. The Fréchet derivative of in a point is defined to be the bounded linear operator such that In my book, it says it is easily computable by Taylor expansions, but I really do not see how. They just go from there to Euler-Lagrange equation immediately. Online, I saw many convincing heuristics but no rigorous proof using the definition of the Fréchet derivative."," J: C^1[a,b] \rightarrow \mathbb R y \mapsto \int_a^b L(x,y(x),y'(x)) \, dx  L C^2 J y A_y \lim_{||h||_{C^1([a,b])} \rightarrow 0} \frac{|J(y+h) -J(y) -A_yh|}{||h||_{C^1([a,b])}}=0","['real-analysis', 'functional-analysis', 'analysis', 'derivatives']"
45,"Find the equation of tangents for $x^3+y^3-3xy=0$ at $x=0,y=0$",Find the equation of tangents for  at,"x^3+y^3-3xy=0 x=0,y=0","$$x^3+y^3-3xy=0$$ Find the equation of tangents at $x=0,y=0$ My attempt is as follows:- Attempt $1$ : $$3x^2+3y^2\dfrac{dy}{dx}-3\left(x\dfrac{dy}{dx}+y\right)=0$$ $$\dfrac{dy}{dx}(y^2-x)=y-x^2$$ $$\dfrac{dy}{dx}=\dfrac{y-x^2}{y^2-x}$$ but when placing $x=0,y=0$ we are getting undefined quantity. But actual answer is $xy=0$ , how can we proceed here? Attempt $2$ : (Parametric method) $$\dfrac{y}{x}=t$$ $$y=xt$$ Putting this in the original equation $$x^3+x^3t^3-3x^2t=0$$ $$x^2(x+xt-t)=0$$ $$x=\dfrac{t}{t+1}$$ $$y=\dfrac{t^2}{t+1}$$ $$\dfrac{dy}{dx}=\dfrac{\dfrac{2t(t+1)-t^2}{(t+1)^2}}{\dfrac{t+1-t}{(t+1)^2}}$$ $$\dfrac{dy}{dx}=t^2+2t$$ $x=0$ , then $\dfrac{t}{t+1}=0 \implies t=0$ $y=0$ , then $\dfrac{t^2}{t+1}=0$ also $\implies t=0$ So we are getting slope as $0$ , hence $y=0$ can be the answer, but actual answer is $xy=0$","Find the equation of tangents at My attempt is as follows:- Attempt : but when placing we are getting undefined quantity. But actual answer is , how can we proceed here? Attempt : (Parametric method) Putting this in the original equation , then , then also So we are getting slope as , hence can be the answer, but actual answer is","x^3+y^3-3xy=0 x=0,y=0 1 3x^2+3y^2\dfrac{dy}{dx}-3\left(x\dfrac{dy}{dx}+y\right)=0 \dfrac{dy}{dx}(y^2-x)=y-x^2 \dfrac{dy}{dx}=\dfrac{y-x^2}{y^2-x} x=0,y=0 xy=0 2 \dfrac{y}{x}=t y=xt x^3+x^3t^3-3x^2t=0 x^2(x+xt-t)=0 x=\dfrac{t}{t+1} y=\dfrac{t^2}{t+1} \dfrac{dy}{dx}=\dfrac{\dfrac{2t(t+1)-t^2}{(t+1)^2}}{\dfrac{t+1-t}{(t+1)^2}} \dfrac{dy}{dx}=t^2+2t x=0 \dfrac{t}{t+1}=0 \implies t=0 y=0 \dfrac{t^2}{t+1}=0 \implies t=0 0 y=0 xy=0","['calculus', 'derivatives', 'tangent-line']"
46,Determining How Addition in New Data Point Effects Hyperparameters in Gaussian Process with Squared Exponential Kernel,Determining How Addition in New Data Point Effects Hyperparameters in Gaussian Process with Squared Exponential Kernel,,"I want to determine how the inclusion of new data effects hyperparameters of the Gaussian Process kernel. For reference assuming square exponential kernels as provided here : $$K(x,x') = \sigma^2\exp\left(\frac{-(x-x')^T(x-x')}{2l^2}\right)$$ So the derivative with respect to length scale determines what the effect to the kernel when the lengthscale changes as follows: $$\frac{\partial K}{\partial l} = \sigma^2\exp\big(\frac{-(x-x')^T(x-x')}{2l^2}\big) \frac{(x-x')^T(x-x')}{l^3}$$ I however would like to determine what is the change or effect of a single new data point to the lengthscale. What should be the symbolic expression I need to evaluate the derivative of? Is it $$\frac{\partial l}{\partial \mu}$$ of the GP? where $\mu$ is the predictive mean of the GP as follows: $$\mu(x^*)=K(x^*,X)^\top[K(X,X)+\sigma_n^2\mathbf{I}]^{-1} \mathbf{y_n}$$ If so how can the derivative expression be formulated. (Initial expression atleast, I should be able to workout derivitave from there itself)","I want to determine how the inclusion of new data effects hyperparameters of the Gaussian Process kernel. For reference assuming square exponential kernels as provided here : So the derivative with respect to length scale determines what the effect to the kernel when the lengthscale changes as follows: I however would like to determine what is the change or effect of a single new data point to the lengthscale. What should be the symbolic expression I need to evaluate the derivative of? Is it of the GP? where is the predictive mean of the GP as follows: If so how can the derivative expression be formulated. (Initial expression atleast, I should be able to workout derivitave from there itself)","K(x,x') = \sigma^2\exp\left(\frac{-(x-x')^T(x-x')}{2l^2}\right) \frac{\partial K}{\partial l} = \sigma^2\exp\big(\frac{-(x-x')^T(x-x')}{2l^2}\big) \frac{(x-x')^T(x-x')}{l^3} \frac{\partial l}{\partial \mu} \mu \mu(x^*)=K(x^*,X)^\top[K(X,X)+\sigma_n^2\mathbf{I}]^{-1} \mathbf{y_n}","['derivatives', 'machine-learning', 'covariance']"
47,derivative of inverse function problems with proof,derivative of inverse function problems with proof,,"Theorem Let $f \colon U \to V$ be homeomorphism form open set U in normed space X, to open set V in normed set Y. Let $a\in U$ , $b=f(a)$ , $f'(x_0) \colon X \to Y$ exists and it is isomorphism. Then the derivative $(f^{-1})'(b)$ exists and $(f^{-1})'(b) = (f'(a))^{-1}.$ Proof in book Let $g=f^{-1}$ It is \begin{equation*}  f(a+x)-f(x)=f'(a)x+o(x) \end{equation*} We have to prove \begin{equation*}  g(b+y)-f(b)-(f'(a))^{-1}y=o(y) \end{equation*} or \begin{equation*} \frac{g(y+b)-g(b)-(f'(a))^{-1}y}{||y||}  \to 0  \ { when } \  y\to0. \end{equation*} Set $x = g(y+b)-g(b)$ so $x+a=g(y+b)$ . We then have $f(x+a)=y+b$ . From this we obtain \begin{equation} y=f(a+x)-f(a) \end{equation} and so $g(y+b)-g(b)=(f'(a))^{-1}*o(x)$ so it is enough to show $\frac{||x||}{||y||} $ is bounded but I don't know how.","Theorem Let be homeomorphism form open set U in normed space X, to open set V in normed set Y. Let , , exists and it is isomorphism. Then the derivative exists and Proof in book Let It is We have to prove or Set so . We then have . From this we obtain and so so it is enough to show is bounded but I don't know how.","f \colon U \to V a\in U b=f(a) f'(x_0) \colon X \to Y (f^{-1})'(b) (f^{-1})'(b) = (f'(a))^{-1}. g=f^{-1} \begin{equation*} 
f(a+x)-f(x)=f'(a)x+o(x)
\end{equation*} \begin{equation*} 
g(b+y)-f(b)-(f'(a))^{-1}y=o(y)
\end{equation*} \begin{equation*}
\frac{g(y+b)-g(b)-(f'(a))^{-1}y}{||y||}  \to 0  \ { when } \  y\to0.
\end{equation*} x = g(y+b)-g(b) x+a=g(y+b) f(x+a)=y+b \begin{equation}
y=f(a+x)-f(a)
\end{equation} g(y+b)-g(b)=(f'(a))^{-1}*o(x) \frac{||x||}{||y||} ","['real-analysis', 'derivatives', 'inverse-function']"
48,Clarification on proof of $\lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\to 1$,Clarification on proof of,\lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\to 1,"Let $f:\mathbb{R}\to \mathbb{R}$ be differentiable at $a\in \mathbb{R}$ such that $f(a)>0$ . Evaluate: $$\lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}.$$ Attempt. A proof would go like: \begin{eqnarray} \left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}&=&\exp\left\{\ln\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\right\}= \exp\left\{\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{n}\right\}\nonumber\\ &=&\exp\left\{\frac{1}{n^2}\,\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{\frac{1}{n}}\right\}\nonumber\\ &\to & \exp\left\{0\cdot \frac{f'(a)}{f(a)}\right\}=1,~n\to +\infty,\nonumber \end{eqnarray} where we use the definition of derivative and the chain rule. So far, so good. My question is: one claims that since $f$ is differentible at $a,$ then $f$ is continuous at $a$ and so the limit becomes $1^0=1$ , according to the algebra of sequential limits. Is such an approach also correct? ( $1^0$ is not an indeterminate form). Thanks for the help.","Let be differentiable at such that . Evaluate: Attempt. A proof would go like: where we use the definition of derivative and the chain rule. So far, so good. My question is: one claims that since is differentible at then is continuous at and so the limit becomes , according to the algebra of sequential limits. Is such an approach also correct? ( is not an indeterminate form). Thanks for the help.","f:\mathbb{R}\to \mathbb{R} a\in \mathbb{R} f(a)>0 \lim_{n\to +\infty}\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}. \begin{eqnarray} \left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}&=&\exp\left\{\ln\left(\frac{f(a+\frac{1}{n})}{f(a)}\right)^{\frac{1}{n}}\right\}=
\exp\left\{\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{n}\right\}\nonumber\\
&=&\exp\left\{\frac{1}{n^2}\,\frac{\ln f(a+\frac{1}{n})-\ln f(a)}{\frac{1}{n}}\right\}\nonumber\\
&\to & \exp\left\{0\cdot \frac{f'(a)}{f(a)}\right\}=1,~n\to +\infty,\nonumber
\end{eqnarray} f a, f a 1^0=1 1^0","['real-analysis', 'calculus', 'limits', 'analysis', 'derivatives']"
49,Converting a matrix differential to a derivative,Converting a matrix differential to a derivative,,"I would like to write down the update rule for a set of parameters in a neural network, which minimizes a loss function that I think is general enough to be instructive for others. Let $\Phi \in \mathbb{R}^{l \times m \times n}$ be a $l \times m \times n$ tensor of learnable parameters and $\mathscr{L(\Phi)}$ be a scalar loss function of those parameters to be minimized: $$\mathscr{L} = \beta\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{n}|\Phi_{i}^{\top}\Phi_{i} - \mathbb{I}_{\text{n}}|_{jk},$$ where $|\cdot|$ is element-wise absolute value, $\beta$ is some scalar constant, $\Phi_{i}$ is a $l \times n$ matrix, and $\mathbb{I}_{\text{n}}$ is the $n \times n$ identity matrix. I would like to know the derivative of this loss with respect to an $l$ -dimensional vector: $\frac{\partial \mathscr{L}}{\partial \Phi_{ab}}$ , where $a$ and $b$ index the $m$ and $n$ dimensions of $\Phi$ , respectively. Following the chain rule described in chapter 18 from the Matrix Differential Calculus book by Magnus and Neudecker , I can use differentials to get most of the way there. Specifically, I can modify example 18.6a to let $F(X) = |X^{\top}X|$ for some $X \in \mathbb{R}^{l \times n}$ , where again $|\cdot|$ is absolute value, not determinant. Then, \begin{align} \text{d}F &= \text{d}|X^{\top}X| \\ &= \frac{X^{\top}X}{|X^{\top}X|} \text{d}(X^{\top}X) \\ &= \frac{X^{\top}X}{|X^{\top}X|} (\text{d}X)^{\top}X + \frac{X^{\top}X}{|X^{\top}X|} X^{\top} \text{d}X \\ &= 2 \frac{X^{\top}X}{|X^{\top}X|} X^{\top}\text{d}X \end{align} The book also provides an identification theorem for connecting differentials to derivatives: $$\text{d} \text{vec}F = A(X) \text{d} \text{vec}X \iff \frac{\partial\text{vec}F(X)}{\partial(\text{vec}X)^{\top}} = A(X),$$ where $\text{vec}$ is the matrix vectorization operator . I believe I can now use the chain rule to get close to my desired derivative if I set $F=|X^{\top}X-\mathbb{I}_{\text{n}}|$ and $X=\Phi_{i}$ : \begin{align} \frac{\partial\mathscr{L}}{\partial(\text{vec}\Phi_{i})^{\top}} &= \frac{\partial\mathscr{L}}{\partial\text{vec}F} \frac{\partial\text{vec}F}{\partial(\text{vec}\Phi_{i})^{\top}} \\ &= \frac{\partial\mathscr{L}}{\partial\text{vec}F} 2 \frac{\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}}{|\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}|} \Phi_{i}^{\top} \end{align} I do not know how to get from this point to a partial derivative with respect to a single vector, $\Phi_{ab}$ . I would guess that almost all of the entries from the sums in $\mathscr{L}$ will be zero for $\frac{\partial \mathscr{L}}{\partial \Phi_{ab}}$ . I think I can use this to my advantage, which I think would mean multiplying the above derivative by $\delta_{ia}\delta_{jb}\delta_{kb}$ , but this is where I am less sure. I also used this blog post as a resource. My question is very similar to this one , and also related to this one , this one , and this one , although I was not able to get to an answer from those posts.","I would like to write down the update rule for a set of parameters in a neural network, which minimizes a loss function that I think is general enough to be instructive for others. Let be a tensor of learnable parameters and be a scalar loss function of those parameters to be minimized: where is element-wise absolute value, is some scalar constant, is a matrix, and is the identity matrix. I would like to know the derivative of this loss with respect to an -dimensional vector: , where and index the and dimensions of , respectively. Following the chain rule described in chapter 18 from the Matrix Differential Calculus book by Magnus and Neudecker , I can use differentials to get most of the way there. Specifically, I can modify example 18.6a to let for some , where again is absolute value, not determinant. Then, The book also provides an identification theorem for connecting differentials to derivatives: where is the matrix vectorization operator . I believe I can now use the chain rule to get close to my desired derivative if I set and : I do not know how to get from this point to a partial derivative with respect to a single vector, . I would guess that almost all of the entries from the sums in will be zero for . I think I can use this to my advantage, which I think would mean multiplying the above derivative by , but this is where I am less sure. I also used this blog post as a resource. My question is very similar to this one , and also related to this one , this one , and this one , although I was not able to get to an answer from those posts.","\Phi \in \mathbb{R}^{l \times m \times n} l \times m \times n \mathscr{L(\Phi)} \mathscr{L} = \beta\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=1}^{n}|\Phi_{i}^{\top}\Phi_{i} - \mathbb{I}_{\text{n}}|_{jk}, |\cdot| \beta \Phi_{i} l \times n \mathbb{I}_{\text{n}} n \times n l \frac{\partial \mathscr{L}}{\partial \Phi_{ab}} a b m n \Phi F(X) = |X^{\top}X| X \in \mathbb{R}^{l \times n} |\cdot| \begin{align}
\text{d}F &= \text{d}|X^{\top}X| \\
&= \frac{X^{\top}X}{|X^{\top}X|} \text{d}(X^{\top}X) \\
&= \frac{X^{\top}X}{|X^{\top}X|} (\text{d}X)^{\top}X + \frac{X^{\top}X}{|X^{\top}X|} X^{\top} \text{d}X \\
&= 2 \frac{X^{\top}X}{|X^{\top}X|} X^{\top}\text{d}X
\end{align} \text{d} \text{vec}F = A(X) \text{d} \text{vec}X \iff \frac{\partial\text{vec}F(X)}{\partial(\text{vec}X)^{\top}} = A(X), \text{vec} F=|X^{\top}X-\mathbb{I}_{\text{n}}| X=\Phi_{i} \begin{align}
\frac{\partial\mathscr{L}}{\partial(\text{vec}\Phi_{i})^{\top}} &= \frac{\partial\mathscr{L}}{\partial\text{vec}F} \frac{\partial\text{vec}F}{\partial(\text{vec}\Phi_{i})^{\top}} \\
&= \frac{\partial\mathscr{L}}{\partial\text{vec}F} 2 \frac{\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}}{|\Phi_{i}^{\top}\Phi_{i}-\mathbb{I}_{\text{n}}|} \Phi_{i}^{\top}
\end{align} \Phi_{ab} \mathscr{L} \frac{\partial \mathscr{L}}{\partial \Phi_{ab}} \delta_{ia}\delta_{jb}\delta_{kb}","['matrices', 'derivatives', 'matrix-calculus', 'tensors', 'differential']"
50,Differentiation of a log likelihood function,Differentiation of a log likelihood function,,"I am trying to maximize a particular log likelihood function but I am stuck on the differentiation step. Given: $ \Theta_1 + ....... + \Theta_k = 1  $ The likelihood function is: $f_n(x|\Theta_1,.........,\Theta_k) = \Theta^{n_1}_1........\Theta^{n_k}_k$ Let $L(\Theta_1,......,\Theta_k) = log\,\,f_n(x|\Theta_1,.........,\Theta_k)$ and let $\Theta_k = 1 - \sum_{i=1}^{k-1} \Theta_i \qquad - (i)$ Then, $$ \frac {\partial L(\Theta_1,.......,\Theta_k)}{\partial\Theta_i} = \frac{n_i}{\Theta_i} - \frac{n_k}{\Theta_k}\qquad for \,\; i=1,.....,k-1 \qquad - (ii)$$ Case 1: We may write L as $\quad\sum_{i=1}^{k-1}n_i\,ln\,\Theta_i\,+\,n_k\;ln(1\,-\,\sum_{i=1}^{k-1} \Theta_i)\quad$ if we make the substitution in (i) Case 2: We may write L as $\quad\sum_{i=1}^{k}n_i\,ln\,\Theta_i\quad$ if we don't make the substitution in (i) For Case 1 derivative would be: $\quad\frac{n_i}{\Theta_i} - \frac{n_k}{\Theta_k}\qquad for \,\; i=1,.....,k-1$ For Case 2 derivative would be: $\quad\frac{n_i}{\Theta_i}\qquad for \,\; i=1,.....,k$ Thus for an $i\neq k$ depending upon if we make the substitution in (i) or not, we get two different results for the same partial derivative i.e. $\frac{\partial L}{\partial\Theta_i}$ Case 1 is the solution. But by this logic derivative can be anything depending on our choice of k in the set. Where am I going wrong in Case 2? Am I making an error by not making the  substitution and simply differentiating L. Please help.","I am trying to maximize a particular log likelihood function but I am stuck on the differentiation step. Given: The likelihood function is: Let and let Then, Case 1: We may write L as if we make the substitution in (i) Case 2: We may write L as if we don't make the substitution in (i) For Case 1 derivative would be: For Case 2 derivative would be: Thus for an depending upon if we make the substitution in (i) or not, we get two different results for the same partial derivative i.e. Case 1 is the solution. But by this logic derivative can be anything depending on our choice of k in the set. Where am I going wrong in Case 2? Am I making an error by not making the  substitution and simply differentiating L. Please help."," \Theta_1 + ....... + \Theta_k = 1   f_n(x|\Theta_1,.........,\Theta_k) = \Theta^{n_1}_1........\Theta^{n_k}_k L(\Theta_1,......,\Theta_k) = log\,\,f_n(x|\Theta_1,.........,\Theta_k) \Theta_k = 1 - \sum_{i=1}^{k-1} \Theta_i \qquad - (i)  \frac {\partial L(\Theta_1,.......,\Theta_k)}{\partial\Theta_i} = \frac{n_i}{\Theta_i} - \frac{n_k}{\Theta_k}\qquad for \,\; i=1,.....,k-1 \qquad - (ii) \quad\sum_{i=1}^{k-1}n_i\,ln\,\Theta_i\,+\,n_k\;ln(1\,-\,\sum_{i=1}^{k-1} \Theta_i)\quad \quad\sum_{i=1}^{k}n_i\,ln\,\Theta_i\quad \quad\frac{n_i}{\Theta_i} - \frac{n_k}{\Theta_k}\qquad for \,\; i=1,.....,k-1 \quad\frac{n_i}{\Theta_i}\qquad for \,\; i=1,.....,k i\neq k \frac{\partial L}{\partial\Theta_i}","['derivatives', 'maximum-likelihood', 'log-likelihood']"
51,"If $f$ is a polynomial in one variable with real coefficients which has all its roots real, then its derivative $f'$ has all its roots real","If  is a polynomial in one variable with real coefficients which has all its roots real, then its derivative  has all its roots real",f f',"Is  the following statement  true/false ? If $f$ is  a polynomial in one variable  with real coefficients which  has  all its roots real, then  its derivative $f'$ has all its roots real as  well My attempt  : I think  this  statement is false. Take $f(x) =  \frac{1}{3} x^3 + x$ and now $f'(x) = x^2 + 1  $ , $x^2+ 1=0 $ implies $x= i,-i$ which does not belong to $\mathbb{R}$ ,   so given the above question statement is false Edits  :another  counter example $f(x) = x+1$ , but $f'(x) =1$ has  no root in $\mathbb{R}$ Is  its  true ?","Is  the following statement  true/false ? If is  a polynomial in one variable  with real coefficients which  has  all its roots real, then  its derivative has all its roots real as  well My attempt  : I think  this  statement is false. Take and now , implies which does not belong to ,   so given the above question statement is false Edits  :another  counter example , but has  no root in Is  its  true ?","f f' f(x) =  \frac{1}{3} x^3 + x f'(x) = x^2 + 1   x^2+ 1=0  x= i,-i \mathbb{R} f(x) = x+1 f'(x) =1 \mathbb{R}","['real-analysis', 'derivatives', 'polynomials']"
52,"For What $n, m$ is $f(x)$ Differentiable? Continuously Differentiable?",For What  is  Differentiable? Continuously Differentiable?,"n, m f(x)","Let $f(x) = \begin{cases} |x|^m\sin{\left(\frac{1}{|X|^n}\right)} & x\neq 0\\ 0 & x=0\end{cases}$ . Questions: For what values of $m,n \in \mathbb{R}$ is $f$ continuous at $0$ ? Differentiable at $0$ ? Continuously Differentiable at $0$ ? My (partly successful) attempt: $f$ continuous at $0$ precisely when $0<m$ or $n\lt m\leq0$ . This is because if $m>0$ then $\lim_{x\to 0}{|x|^m}=0$ and $\sin{\left(\frac{1}{|X|^n}\right)}$ is bounded. Else, if $n<m\leq 0$ then $|x|^{m-n} \frac{\sin{\left(\frac{1}{|X|^{n}} \right)}}{\frac{1}{|X|^n}}$ converges to $0$ as well. I know I need to check when the limit $\lim_{x\to 0}{\frac{f(x)}{x}}$ exists. This is too difficult and divided into too many subcases. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case? Same as 2. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case?","Let . Questions: For what values of is continuous at ? Differentiable at ? Continuously Differentiable at ? My (partly successful) attempt: continuous at precisely when or . This is because if then and is bounded. Else, if then converges to as well. I know I need to check when the limit exists. This is too difficult and divided into too many subcases. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case? Same as 2. Is there an easier way to do it which I am missing? How do you divide the cases and show for each case?","f(x) = \begin{cases} |x|^m\sin{\left(\frac{1}{|X|^n}\right)} & x\neq 0\\ 0 & x=0\end{cases} m,n \in \mathbb{R} f 0 0 0 f 0 0<m n\lt m\leq0 m>0 \lim_{x\to 0}{|x|^m}=0 \sin{\left(\frac{1}{|X|^n}\right)} n<m\leq 0 |x|^{m-n} \frac{\sin{\left(\frac{1}{|X|^{n}} \right)}}{\frac{1}{|X|^n}} 0 \lim_{x\to 0}{\frac{f(x)}{x}}","['limits', 'derivatives', 'continuity']"
53,Expressing derivative as function plus remainder,Expressing derivative as function plus remainder,,"Could you please give a proof of the following statement? If $f$ is differentiable then $f'(a) = \lim_{x\to a} \frac{f(x)-f(a)}{x-a}$ exists.  This can alternatively be written $$f'(a) = \frac{f(x)-f(a)}{x-a} + r(x-a)$$ where the remainder function $r$ has the property $\lim_{x \to a} r(x-a)=0$ . Why is that the case? If the limit of $m(x)$ is a constant $z$ , then obviously calculating the limit of $m(x) + n(x)$ will also be $z$ assuming $\lim n(x) = 0$ . In this case: $$f'(a) = \lim_{x\to a} \left(\frac{f(x)-f(a)}{x-a} + r(x-a)\right)$$ however it doesn't explain why $f'(a)$ can be expressed as $ \frac{f(x)-f(a)}{x-a} + r(x-a)$ where $\lim_{x \to a} r(x-a)=0$ .","Could you please give a proof of the following statement? If is differentiable then exists.  This can alternatively be written where the remainder function has the property . Why is that the case? If the limit of is a constant , then obviously calculating the limit of will also be assuming . In this case: however it doesn't explain why can be expressed as where .",f f'(a) = \lim_{x\to a} \frac{f(x)-f(a)}{x-a} f'(a) = \frac{f(x)-f(a)}{x-a} + r(x-a) r \lim_{x \to a} r(x-a)=0 m(x) z m(x) + n(x) z \lim n(x) = 0 f'(a) = \lim_{x\to a} \left(\frac{f(x)-f(a)}{x-a} + r(x-a)\right) f'(a)  \frac{f(x)-f(a)}{x-a} + r(x-a) \lim_{x \to a} r(x-a)=0,"['calculus', 'derivatives']"
54,Show that $f'\left(\frac13\right)$ does not exist?,Show that  does not exist?,f'\left(\frac13\right),"Consider the function $f(x)=\begin{cases}     x^2\left|\cos\dfrac{\pi}{2x}\right|, &x\ne 0\\     0, & x=0   \end{cases}$ $$f(x)=\begin{cases}     x^2\cos\dfrac{\pi}{2x}, &\dfrac{\pi}{2x}\in\left[2n\pi-\dfrac{\pi}{2},2n\pi+\dfrac{\pi}{2}\right]\\        -x^2\cos\dfrac{\pi}{2x}, &\dfrac{\pi}{2x}\in\left(2n\pi+\dfrac{\pi}{2},2n\pi+\dfrac{3\pi}{2}\right)\\     0, & x=0   \end{cases}$$ $$f(x)=\begin{cases}     x^2\cos\dfrac{\pi}{2x}, &x\in\left[\dfrac{1}{4n+1},\dfrac{1}{4n-1}\right]\\        -x^2\cos\dfrac{\pi}{2x}, &x\in\left(\dfrac{1}{4n+3},\dfrac{1}{4n+1}\right)\\     0, & x=0   \end{cases}$$ $$f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\left(\dfrac{1}{3}+h\right)^2\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{h}$$ $$f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\left(\dfrac{1}{h}+9h+6\right)\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9}$$ $$f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9h}+\lim_{h\to0}h\cos\left(\dfrac{3\pi}{2(1+3h)}\right)+\dfrac{2}{3}\lim_{h\to0}\cos\left(\dfrac{3\pi}{2(1+3h)}\right)$$ $$f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9h}$$ Assuming $\dfrac{1}{1+3h}=y$ , as $h\rightarrow 0, \dfrac{1}{1+3h}\rightarrow 1,h=\dfrac{1-y}{3y}$ $$f'\left(\dfrac{1}{3}\right)=\lim_{y\to1}\dfrac{y\cos\left(\dfrac{3\pi y}{2}\right)}{3(1-y)}$$ Applying L'Hospital as we have $\dfrac{0}{0}$ determinant form $$f'\left(\dfrac{1}{3}\right)=\lim_{y\to1}\dfrac{-y\sin\left(\dfrac{3\pi y}{2}\right)\dfrac{3\pi}{2}+\cos\left(\dfrac{3\pi y}{2}\right)}{-3}$$ $$f'\left(\dfrac{1}{3}\right)=-\dfrac{\pi}{2}$$ It seems function is differentiable at $\dfrac{1}{3}$ , what am I missing here?","Consider the function Assuming , as Applying L'Hospital as we have determinant form It seems function is differentiable at , what am I missing here?","f(x)=\begin{cases}
    x^2\left|\cos\dfrac{\pi}{2x}\right|, &x\ne 0\\
    0, & x=0
  \end{cases} f(x)=\begin{cases}
    x^2\cos\dfrac{\pi}{2x}, &\dfrac{\pi}{2x}\in\left[2n\pi-\dfrac{\pi}{2},2n\pi+\dfrac{\pi}{2}\right]\\
       -x^2\cos\dfrac{\pi}{2x}, &\dfrac{\pi}{2x}\in\left(2n\pi+\dfrac{\pi}{2},2n\pi+\dfrac{3\pi}{2}\right)\\
    0, & x=0
  \end{cases} f(x)=\begin{cases}
    x^2\cos\dfrac{\pi}{2x}, &x\in\left[\dfrac{1}{4n+1},\dfrac{1}{4n-1}\right]\\
       -x^2\cos\dfrac{\pi}{2x}, &x\in\left(\dfrac{1}{4n+3},\dfrac{1}{4n+1}\right)\\
    0, & x=0
  \end{cases} f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\left(\dfrac{1}{3}+h\right)^2\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{h} f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\left(\dfrac{1}{h}+9h+6\right)\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9} f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9h}+\lim_{h\to0}h\cos\left(\dfrac{3\pi}{2(1+3h)}\right)+\dfrac{2}{3}\lim_{h\to0}\cos\left(\dfrac{3\pi}{2(1+3h)}\right) f'\left(\dfrac{1}{3}\right)=\lim_{h\to0}\dfrac{\cos\left(\dfrac{3\pi}{2(1+3h)}\right)}{9h} \dfrac{1}{1+3h}=y h\rightarrow 0, \dfrac{1}{1+3h}\rightarrow 1,h=\dfrac{1-y}{3y} f'\left(\dfrac{1}{3}\right)=\lim_{y\to1}\dfrac{y\cos\left(\dfrac{3\pi y}{2}\right)}{3(1-y)} \dfrac{0}{0} f'\left(\dfrac{1}{3}\right)=\lim_{y\to1}\dfrac{-y\sin\left(\dfrac{3\pi y}{2}\right)\dfrac{3\pi}{2}+\cos\left(\dfrac{3\pi y}{2}\right)}{-3} f'\left(\dfrac{1}{3}\right)=-\dfrac{\pi}{2} \dfrac{1}{3}","['calculus', 'limits', 'derivatives']"
55,$ \dfrac{d}{dz} \left \{ (1-z^2)P'(z) \right\} + \left\{ \beta - \dfrac{m^2}{1-z^2} \right\}P(z) = 0 $ Conversion of the DE into other forms.,Conversion of the DE into other forms., \dfrac{d}{dz} \left \{ (1-z^2)P'(z) \right\} + \left\{ \beta - \dfrac{m^2}{1-z^2} \right\}P(z) = 0 ,"I guess it's a bit related to physics, but the doubt is in the mathematical part.  I was just watching some stuff on Quantum Mechanics for the solution of the Hydrogen atom. The lecture ends at.. $$ \dfrac{d}{dz} \left \{ (1-z^2)P'(z) \right\} + \left\{ \beta - \dfrac{m^2}{1-z^2} \right\}p(z) = 0 \tag{*}$$ And the next one starts after missing a few minutes at: $$ Let \, P(z) = (1-z^2)^{|m|/2}G(z) $$ And $(*)$ changes to: $$ (1-z^2)G''(z) - 2 (|m|+1)z G'(z) + \{ \beta - |m|(|m|+1)\}G(z) = 0 $$ How? The closest I get is: $$ (1-z^2)P''(z) - 2z P'(z) + \{ \beta - \dfrac{m^2}{1-z^2} \}P(z) = 0 $$ Can anyone show that to me? A double derivative of $P$ is a very lengthy polynomial and seems of no use to me. My approach was to eliminate the power $|m|/2$ and to get the constants in the form $\beta - |m|(|m|+1)$ but no order of the derivative is useful here. Any help is appreciated.","I guess it's a bit related to physics, but the doubt is in the mathematical part.  I was just watching some stuff on Quantum Mechanics for the solution of the Hydrogen atom. The lecture ends at.. And the next one starts after missing a few minutes at: And changes to: How? The closest I get is: Can anyone show that to me? A double derivative of is a very lengthy polynomial and seems of no use to me. My approach was to eliminate the power and to get the constants in the form but no order of the derivative is useful here. Any help is appreciated."," \dfrac{d}{dz} \left \{ (1-z^2)P'(z) \right\} + \left\{ \beta - \dfrac{m^2}{1-z^2} \right\}p(z) = 0 \tag{*}  Let \, P(z) = (1-z^2)^{|m|/2}G(z)  (*)  (1-z^2)G''(z) - 2 (|m|+1)z G'(z) + \{ \beta - |m|(|m|+1)\}G(z) = 0   (1-z^2)P''(z) - 2z P'(z) + \{ \beta - \dfrac{m^2}{1-z^2} \}P(z) = 0  P |m|/2 \beta - |m|(|m|+1)","['calculus', 'ordinary-differential-equations']"
56,Function that requires differentiation,Function that requires differentiation,,"Differentiate: $$\ln\left(\dfrac{x^2\sqrt{2x^2+3}}{\left(x^4+x^2\right)^3}\right)$$ I have tried to figure it out here: The steps are too long so I tidy up as an image After the steps from the image, these are the final steps of simplifying: $$=\dfrac{\frac{2x^5\left(x^2+1\right)^3}{\sqrt{2x^2+3}}-\sqrt{2x^2+3}\left(4x^3\left(x^2+1\right)^3+6x^5\left(x^2+1\right)^2\right)}{x^4\left(x^2+1\right)^3\sqrt{2x^2+3}}$$ $$=\dfrac{x^4\left(x^2+1\right)^3\left(-\frac{4\sqrt{2x^2+3}}{x^5\left(x^2+1\right)^3}-\frac{6\sqrt{2x^2+3}}{x^3\left(x^2+1\right)^4}+\frac{2}{x^3\left(x^2+1\right)^3\sqrt{2x^2+3}}\right)}{\sqrt{2x^2+3}}$$ $$=\dfrac{2x}{2x^2+3}-\dfrac{6x}{x^2+1}-\dfrac{4}{x}$$ Please tell me whether this is correct or not, I would like to simplify my steps further if I could. Thanks!","Differentiate: I have tried to figure it out here: The steps are too long so I tidy up as an image After the steps from the image, these are the final steps of simplifying: Please tell me whether this is correct or not, I would like to simplify my steps further if I could. Thanks!",\ln\left(\dfrac{x^2\sqrt{2x^2+3}}{\left(x^4+x^2\right)^3}\right) =\dfrac{\frac{2x^5\left(x^2+1\right)^3}{\sqrt{2x^2+3}}-\sqrt{2x^2+3}\left(4x^3\left(x^2+1\right)^3+6x^5\left(x^2+1\right)^2\right)}{x^4\left(x^2+1\right)^3\sqrt{2x^2+3}} =\dfrac{x^4\left(x^2+1\right)^3\left(-\frac{4\sqrt{2x^2+3}}{x^5\left(x^2+1\right)^3}-\frac{6\sqrt{2x^2+3}}{x^3\left(x^2+1\right)^4}+\frac{2}{x^3\left(x^2+1\right)^3\sqrt{2x^2+3}}\right)}{\sqrt{2x^2+3}} =\dfrac{2x}{2x^2+3}-\dfrac{6x}{x^2+1}-\dfrac{4}{x},['derivatives']
57,"Prob. 2, Sec. 6.3, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: If $f\to A>0$, $g>0$, and $g\to 0$, then $f/g\to +\infty$; ...","Prob. 2, Sec. 6.3, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: If , , and , then ; ...",f\to A>0 g>0 g\to 0 f/g\to +\infty,"Here is Prob. 1, Sec. 6.3, in the book Introduction to Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Suppose that $f$ and $g$ are continuous on $[a, b]$ , differentiable on $(a, b)$ , that $c \in [a, b]$ and $g(x) \neq 0$ for $x \in [a, b]$ , $x \neq c$ . Let $A \colon= \lim_{x\to c} f$ and $B \colon= \lim_{x\to c} g$ . If $B = 0$ , and if $\lim_{x \to c} f(x)/g(x)$ exists in $\mathbb{R}$ , show that we must have $A=0$ . [ Hint : $f(x) = \big\{ f(x) / g(x) \big\} g(x)$ .] Here is my Math SE post on this problem. And, here is Prob. 2, Sec. 6,3: In addition to the suppositions of the preceding exercise, let $g(x) > 0$ for $x \in [a, b]$ , $x \neq c$ . If $A > 0$ and $B = 0$ , prove that we must have $\lim_{x \to c} f(x)/g(x) = \infty$ . If $A < 0$ and $B = 0$ , prove that we must have $\lim_{x \to c} f(x)/ g(x) = -\infty$ . My Attempt: Suppose that $f$ and $g$ are real-valued functions defined on the closed interval $[a, b]$ and that $c \in [a, b]$ is such that (i) $g(x) > 0$ for $x \in [a, b]$ such that $x \neq c$ , (ii) $\lim_{x \to c} f(x)$ exists in $\mathbb{R}$ , and (iii) $\lim_{x \to c} g(x) = 0$ . Let us put $A \colon= \lim_{x \to c} f(x)$ and $B \colon= \lim_{x \to c} g(x)$ . Then $B = 0$ of course. We study the following two cases according as $A> 0$ or $A < 0$ . Case 1. First suppose that $A > 0$ . Let $\alpha \in \mathbb{R}$ be arbitrary. Then as $\lim_{x \to c} f(x) = A$ , so there exists a real number $\delta_1 > 0$ and depending on $A > 0$ such that $$ \big\lvert  f(x) - A \big\rvert  < \frac{A}{2} $$ for all $x \in [a, b]$ such that $0 < \big\lvert x-c \big\rvert < \delta_1$ . Therefore we have $$ f(x) > \frac{A}{2} > 0 \tag{1} $$ for all $x \in [a, b]$ such that $0 < \big\lvert x-c \big\rvert < \delta_1$ . Now as $\lim_{x \to c} g(x) = 0$ , and as $$ \frac{A}{2 \big( \lvert \alpha \rvert + 1 \big) } > 0, $$ so there exists a real number $\delta_2 > 0$ and depending on $\alpha$ (and $A$ ) such that $$ 0 < g(x) =  \big\lvert g(x)  \big\rvert = \big\lvert g(x) - 0 \big\rvert < \frac{A}{2 \big( \lvert \alpha \rvert + 1 \big) } $$ for all $x \in [a, b]$ such that $0 < \lvert x-c \rvert < \delta_2$ . Therefore we must also have $$ \frac{1}{g(x)} > \frac{ 2 \big( \lvert \alpha \rvert + 1 \big) }{A} \tag{2} $$ for all $x \in [a, b]$ such that $0 < \lvert x-c \rvert < \delta_2$ . Let us put $\delta \colon= \min \left\{ \delta_1, \delta_2 \right\}$ . Then $\delta > 0$ since both $\delta_1 > 0$ and $\delta_2 > 0$ . Then from (1) and (2) above, we can conclude that for all $x \in [a, b]$ such that $0 < \lvert x-c \rvert < \delta$ , we must have $$ \frac{ f(x) }{ g(x) } = f(x) \frac{1}{g(x)} > \frac{A}{2} \frac{ 2 \big( \lvert \alpha \rvert + 1 \big) }{A} = \lvert \alpha \rvert + 1 > \lvert \alpha \rvert \geq \alpha. $$ Thus we have shown that, corresponding to any given real number $\alpha$ , there exists a real number $\delta > 0$ and depending on $\alpha$ such that $$ \frac{ f(x) }{ g(x) } > \alpha $$ for all $x \in [a, b]$ such that $0 < \lvert x-c \rvert < \delta$ . Therefore by virtue of Definition 4.3.5 (i) in Bartle & Sherbert, 4th edition, we conclude that $$ \lim_{x \to c} \frac{ f(x) }{ g(x) } = +\infty. $$ Case 2. Now suppose that $A < 0$ . Let us choose an arbitrary real number $\beta$ . Then as $A< 0$ , so $-A>0$ , and there exists a real number $\delta_1 > 0$ and depending on $A$ such that $$ \big\lvert f(x) - A \big\rvert < \frac{-A}{2} $$ and so $$ f(x) < \frac{A}{2} < 0 \tag{3} $$ for all $x \in [a, b]$ such that $0 < \lvert x-c \rvert < \delta_1$ . Also there exists a real number $\delta_2 > 0$ and depending on $\beta$ (and also on $A$ ) such that $$ 0 < g(x) = \big\lvert g(x) \big\rvert = \big\lvert g(x) - 0 \big\rvert < \frac{ A }{ -2 \big( \lvert \beta \rvert + 1 \big) } $$ and so $$ \frac{1}{g(x) } > \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } > 0 \tag{4} $$ for all $x \in [a, b]$ satisfying $0 < \lvert x-c \rvert < \delta_2$ . Then, for all $x \in [a, b]$ for which $0 < \lvert x-c \rvert < \min\left\{ \delta_1, \delta_2 \right\}$ , we also have $$ \begin{align}  \frac{ f(x) }{ g(x) } &= f(x) \frac{1}{g(x)} \\ &< f(x) \left( \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } \right) \qquad  [ \mbox{because of (4) and the fact from (3) that $f(x) < 0$} ]\\ &< \frac{A}{2} \left(  \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } \right) \qquad [ \mbox{ Refer to (3) and (4) above again. } ] \\ &= - \lvert \beta \rvert - 1  \\ &< -\lvert \beta \vert \\ &\leq \beta.  \end{align} $$ Since $\beta$ was an arbitrarily chosen real number, it follows from Definition 4.3.5 (ii) that $$ \lim_{x \to c} \frac{f(x)}{g(x)} = -\infty. $$ Is this proof correct and rigorous enough for Bartle & Sherbert? If so, then is there any necessity for the assumptions of continuity of $f$ and $g$ on $[a, b]$ and differentiability of these functions on $(a, b)$ ? Or, what is missing in my proof? Last but not least, do we really need to assume the differentiability of functions $f$ and $g$ on $(a, b)$ ? or the continuity of functions $f$ and $g$ on $[a, b]$ ?","Here is Prob. 1, Sec. 6.3, in the book Introduction to Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Suppose that and are continuous on , differentiable on , that and for , . Let and . If , and if exists in , show that we must have . [ Hint : .] Here is my Math SE post on this problem. And, here is Prob. 2, Sec. 6,3: In addition to the suppositions of the preceding exercise, let for , . If and , prove that we must have . If and , prove that we must have . My Attempt: Suppose that and are real-valued functions defined on the closed interval and that is such that (i) for such that , (ii) exists in , and (iii) . Let us put and . Then of course. We study the following two cases according as or . Case 1. First suppose that . Let be arbitrary. Then as , so there exists a real number and depending on such that for all such that . Therefore we have for all such that . Now as , and as so there exists a real number and depending on (and ) such that for all such that . Therefore we must also have for all such that . Let us put . Then since both and . Then from (1) and (2) above, we can conclude that for all such that , we must have Thus we have shown that, corresponding to any given real number , there exists a real number and depending on such that for all such that . Therefore by virtue of Definition 4.3.5 (i) in Bartle & Sherbert, 4th edition, we conclude that Case 2. Now suppose that . Let us choose an arbitrary real number . Then as , so , and there exists a real number and depending on such that and so for all such that . Also there exists a real number and depending on (and also on ) such that and so for all satisfying . Then, for all for which , we also have Since was an arbitrarily chosen real number, it follows from Definition 4.3.5 (ii) that Is this proof correct and rigorous enough for Bartle & Sherbert? If so, then is there any necessity for the assumptions of continuity of and on and differentiability of these functions on ? Or, what is missing in my proof? Last but not least, do we really need to assume the differentiability of functions and on ? or the continuity of functions and on ?","f g [a, b] (a, b) c \in [a, b] g(x) \neq 0 x \in [a, b] x \neq c A \colon= \lim_{x\to c} f B \colon= \lim_{x\to c} g B = 0 \lim_{x \to c} f(x)/g(x) \mathbb{R} A=0 f(x) = \big\{ f(x) / g(x) \big\} g(x) g(x) > 0 x \in [a, b] x \neq c A > 0 B = 0 \lim_{x \to c} f(x)/g(x) = \infty A < 0 B = 0 \lim_{x \to c} f(x)/ g(x) = -\infty f g [a, b] c \in [a, b] g(x) > 0 x \in [a, b] x \neq c \lim_{x \to c} f(x) \mathbb{R} \lim_{x \to c} g(x) = 0 A \colon= \lim_{x \to c} f(x) B \colon= \lim_{x \to c} g(x) B = 0 A> 0 A < 0 A > 0 \alpha \in \mathbb{R} \lim_{x \to c} f(x) = A \delta_1 > 0 A > 0  \big\lvert  f(x) - A \big\rvert  < \frac{A}{2}  x \in [a, b] 0 < \big\lvert x-c \big\rvert < \delta_1  f(x) > \frac{A}{2} > 0 \tag{1}  x \in [a, b] 0 < \big\lvert x-c \big\rvert < \delta_1 \lim_{x \to c} g(x) = 0  \frac{A}{2 \big( \lvert \alpha \rvert + 1 \big) } > 0,  \delta_2 > 0 \alpha A  0 < g(x) =  \big\lvert g(x)  \big\rvert = \big\lvert g(x) - 0 \big\rvert < \frac{A}{2 \big( \lvert \alpha \rvert + 1 \big) }  x \in [a, b] 0 < \lvert x-c \rvert < \delta_2  \frac{1}{g(x)} > \frac{ 2 \big( \lvert \alpha \rvert + 1 \big) }{A} \tag{2}  x \in [a, b] 0 < \lvert x-c \rvert < \delta_2 \delta \colon= \min \left\{ \delta_1, \delta_2 \right\} \delta > 0 \delta_1 > 0 \delta_2 > 0 x \in [a, b] 0 < \lvert x-c \rvert < \delta  \frac{ f(x) }{ g(x) } = f(x) \frac{1}{g(x)} > \frac{A}{2} \frac{ 2 \big( \lvert \alpha \rvert + 1 \big) }{A} = \lvert \alpha \rvert + 1 > \lvert \alpha \rvert \geq \alpha.  \alpha \delta > 0 \alpha  \frac{ f(x) }{ g(x) } > \alpha  x \in [a, b] 0 < \lvert x-c \rvert < \delta  \lim_{x \to c} \frac{ f(x) }{ g(x) } = +\infty.  A < 0 \beta A< 0 -A>0 \delta_1 > 0 A  \big\lvert f(x) - A \big\rvert < \frac{-A}{2}   f(x) < \frac{A}{2} < 0 \tag{3}  x \in [a, b] 0 < \lvert x-c \rvert < \delta_1 \delta_2 > 0 \beta A  0 < g(x) = \big\lvert g(x) \big\rvert = \big\lvert g(x) - 0 \big\rvert < \frac{ A }{ -2 \big( \lvert \beta \rvert + 1 \big) }   \frac{1}{g(x) } > \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } > 0 \tag{4}  x \in [a, b] 0 < \lvert x-c \rvert < \delta_2 x \in [a, b] 0 < \lvert x-c \rvert < \min\left\{ \delta_1, \delta_2 \right\} 
\begin{align}
 \frac{ f(x) }{ g(x) } &= f(x) \frac{1}{g(x)} \\
&< f(x) \left( \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } \right) \qquad  [ \mbox{because of (4) and the fact from (3) that f(x) < 0} ]\\
&< \frac{A}{2} \left(  \frac{-2 \big( \lvert \beta \rvert + 1 \big)  }{ A } \right) \qquad [ \mbox{ Refer to (3) and (4) above again. } ] \\
&= - \lvert \beta \rvert - 1  \\
&< -\lvert \beta \vert \\
&\leq \beta. 
\end{align}
 \beta  \lim_{x \to c} \frac{f(x)}{g(x)} = -\infty.  f g [a, b] (a, b) f g (a, b) f g [a, b]","['real-analysis', 'limits', 'analysis', 'derivatives', 'solution-verification']"
58,(Solvable?) System of differential equations,(Solvable?) System of differential equations,,"I was trying to solve a physics problem which was about a charged particle moving in a variable magnetic field. I ended up with this system of two differential equations: $$ \left\{  \begin{array}{c} \ddot x = \omega {\dot y \over y}  \\ \ddot y = -\omega {\dot x \over y} \end{array} \right.  $$ Where $x$ and $y$ are functions of time $t$ and $\omega$ is a constant. I am posting this problem here because it's the first time I come up with a system of differential equations and I don't know how to approach such a thing. I have tried by equating the $\omega \over y$ term in the equations and integrating different times, but at the end I come up with: $$ t + C = \pm \int {dy \over \sqrt {A-\mathrm{(B\pm \log y)}^2  }} $$ However, how can I solve the system? Is it possible to explicit the solutions $x$ and $y$ in terms of elementary functions? Thanks in advance, Dave","I was trying to solve a physics problem which was about a charged particle moving in a variable magnetic field. I ended up with this system of two differential equations: Where and are functions of time and is a constant. I am posting this problem here because it's the first time I come up with a system of differential equations and I don't know how to approach such a thing. I have tried by equating the term in the equations and integrating different times, but at the end I come up with: However, how can I solve the system? Is it possible to explicit the solutions and in terms of elementary functions? Thanks in advance, Dave","
\left\{ 
\begin{array}{c}
\ddot x = \omega {\dot y \over y}  \\
\ddot y = -\omega {\dot x \over y}
\end{array}
\right. 
 x y t \omega \omega \over y 
t + C = \pm \int {dy \over \sqrt {A-\mathrm{(B\pm \log y)}^2  }}
 x y","['ordinary-differential-equations', 'derivatives', 'physics', 'indefinite-integrals', 'mathematical-physics']"
59,On the notation for the Jacobian using indices,On the notation for the Jacobian using indices,,"A contravariant vector is an object that is usually written with a superscript and it is defined by the ""transformation law"": $$V^{'i} = \frac{\partial x^{' i}}{\partial x^j} V^j $$ where $i,j = 0,1,2,3$ . In the definition above the fractional terms is referred to as the Jacobian. Now my confusion is the following: I understand that in the definition above really represent 4 different equations (due to the Einstein summation convention), but why does fractional term represent a matrix (the Jacobian).","A contravariant vector is an object that is usually written with a superscript and it is defined by the ""transformation law"": where . In the definition above the fractional terms is referred to as the Jacobian. Now my confusion is the following: I understand that in the definition above really represent 4 different equations (due to the Einstein summation convention), but why does fractional term represent a matrix (the Jacobian).","V^{'i} = \frac{\partial x^{' i}}{\partial x^j} V^j  i,j = 0,1,2,3",['derivatives']
60,"minimizing distance between $A_t$ & $B_t$ , intersections of tangent","minimizing distance between  &  , intersections of tangent",A_t B_t,"So I have a function $$f(x)=\frac{a^{n+1}}{x^n} $$ It's derivative is: $$f'(x)=-n\frac{a^{n+1}}{x^{n+1}} $$ Where $a>0, n \in \mathbb{N}.$ So let $t$ be a tangent to the graph of function $f$ . So let $A_t$ be point of intersection on x-axis and $B_t$ on y-axis of the before mentioned tangent $t$ . So tangent has the equation: $y-y_0=f'(x_0)(x-x_0)$ I have to find such tangent t, that the distance between $A_t  (A,0)$ and $B_t(0,B)$ will be minimal. So the function will be: $d=A^2+B^2$ And i have to get either $A$ or $B$ as a function of the other, so that i can calculate the minimum. So my initial idea; should i just insert $A_t  (A,0)$ , $B_t(0,B)$ into the tangent equation, or is that the wrong approach. Any help would be appreciated. Thank you in advance.","So I have a function It's derivative is: Where So let be a tangent to the graph of function . So let be point of intersection on x-axis and on y-axis of the before mentioned tangent . So tangent has the equation: I have to find such tangent t, that the distance between and will be minimal. So the function will be: And i have to get either or as a function of the other, so that i can calculate the minimum. So my initial idea; should i just insert , into the tangent equation, or is that the wrong approach. Any help would be appreciated. Thank you in advance.","f(x)=\frac{a^{n+1}}{x^n}  f'(x)=-n\frac{a^{n+1}}{x^{n+1}}  a>0, n \in \mathbb{N}. t f A_t B_t t y-y_0=f'(x_0)(x-x_0) A_t  (A,0) B_t(0,B) d=A^2+B^2 A B A_t  (A,0) B_t(0,B)","['real-analysis', 'calculus', 'derivatives']"
61,Why does Lagranges equations of motion yield minimum?,Why does Lagranges equations of motion yield minimum?,,"While calculus of variation from which Lagranges equations of motion arises states integral to be an extremum, it seems almost always to yield the minimum. Is it simply way things work or is there a mathematical reason to it? Also, is it possible for a solution to Euler's equation for a given system to yield expressions for both minimum and maximum? If not why is that?","While calculus of variation from which Lagranges equations of motion arises states integral to be an extremum, it seems almost always to yield the minimum. Is it simply way things work or is there a mathematical reason to it? Also, is it possible for a solution to Euler's equation for a given system to yield expressions for both minimum and maximum? If not why is that?",,"['calculus', 'derivatives', 'calculus-of-variations', 'euler-lagrange-equation']"
62,"Show $f(x)=x-x^2/2$ is not a contracting map on $[0,1]$.",Show  is not a contracting map on .,"f(x)=x-x^2/2 [0,1]","We define a contracting map as a function such that $|f(x)-f(y)|\leq \alpha|x-y|$ for some $\alpha\in(0,1)$ . Consider $f:[0,1]\to[0,1]$ given by $f(x)=x-x^2/2$ . I wish to show that this map is not contracting on $[0,1]$ . Suppose that such a constant $\alpha$ exists. By the mean value theorem, $$f'(c)=\frac{|f(x)-f(y)|}{|x-y|}\leq \alpha$$ for some constant $c\in (0,1)$ , and thus $f'(c)=1-c\leq \alpha$ . I don't see how this helps, since it implies that $1\leq c+\alpha$ , which is not yet a contradiction.","We define a contracting map as a function such that for some . Consider given by . I wish to show that this map is not contracting on . Suppose that such a constant exists. By the mean value theorem, for some constant , and thus . I don't see how this helps, since it implies that , which is not yet a contradiction.","|f(x)-f(y)|\leq \alpha|x-y| \alpha\in(0,1) f:[0,1]\to[0,1] f(x)=x-x^2/2 [0,1] \alpha f'(c)=\frac{|f(x)-f(y)|}{|x-y|}\leq \alpha c\in (0,1) f'(c)=1-c\leq \alpha 1\leq c+\alpha",['calculus']
63,Integral of derivative of function with constant support is zero: intuitive descriptioin sought,Integral of derivative of function with constant support is zero: intuitive descriptioin sought,,"I am looking for an intuitive argument why the integral of the derivative of a function with compact support is zero. More formally, let $\phi:\mathbb{R}\to\mathbb{R}$ be a function with compact support and at least one derivative. For the integral of the derivative we easily find: \begin{align} \int_{-\infty}^\infty \frac{d\phi(t)}{dt} dt&=\lim_{a\to\infty}\phi(a)-\phi(-a)\\ &= 0 \end{align} Now I try to understand this more intuitive. In some limited interval, $\phi$ is some non zero bump or jitter, lets say $\geq 0$ for simplicity. In this interval, the derivative is some differently formed non zero curve. The mean value theorem (intuitive enough, lets say) tells me that the derivative must change sign, so there are negative parts and positive parts. The latter tells me that there is indeed the chance that the negative and positive parts cancel out over the integral. What I am looking for is an eye-opener explaining why the negative and positive parts need to cancel out exactly. Something based on the two graphs of the function and its derivative would be nice.","I am looking for an intuitive argument why the integral of the derivative of a function with compact support is zero. More formally, let be a function with compact support and at least one derivative. For the integral of the derivative we easily find: Now I try to understand this more intuitive. In some limited interval, is some non zero bump or jitter, lets say for simplicity. In this interval, the derivative is some differently formed non zero curve. The mean value theorem (intuitive enough, lets say) tells me that the derivative must change sign, so there are negative parts and positive parts. The latter tells me that there is indeed the chance that the negative and positive parts cancel out over the integral. What I am looking for is an eye-opener explaining why the negative and positive parts need to cancel out exactly. Something based on the two graphs of the function and its derivative would be nice.","\phi:\mathbb{R}\to\mathbb{R} \begin{align}
\int_{-\infty}^\infty \frac{d\phi(t)}{dt} dt&=\lim_{a\to\infty}\phi(a)-\phi(-a)\\
&= 0
\end{align} \phi \geq 0","['integration', 'derivatives']"
64,Prove that the following limit equals $f''(a)$ if $f''(a)$ exists,Prove that the following limit equals  if  exists,f''(a) f''(a),"$1.$ Suppose that $f''(a)$ exists. Show that $\lim\limits_{h\to 0} \dfrac{f(a+h)+f(a-h)-2f(a)}{h^2}=f''(a).$ $2.$ Show by example that this limit may exist even when $f''(a)$ does not. My work: $1.$ By the derivative definition, $$f''(a) = \lim\limits_{h\to 0}\dfrac{f'(a+h)-f'(a)}{h}\\$$ $$=\lim\limits_{h\to 0} \dfrac{f'(a)-f'(a-h)}{h}.$$ To see this, let $k=-h.$ Then $k\to0\Leftrightarrow h\to 0$ and $$\lim\limits_{h\to 0}\dfrac{f'(a)-f'(a-h)}{h} = \lim\limits_{k\to 0}\dfrac{f'(a)-f'(a+k)}{-k}\\ =\lim\limits_{k\to 0}\dfrac{f'(a+k)-f'(a)}{k}\\ =\lim\limits_{h\to 0}\dfrac{f'(a+h)-f'(a)}{h}.$$ So the limit is equivalent to $$\lim\limits_{h\to 0}\dfrac{\frac{f(a+h)-f(a)}{h}-\frac{f(a)-f(a-h)}{h}}{h}\\ =\lim\limits_{h\to 0}\dfrac{f(a+h)+f(a-h)-2f(a)}{h^2}.$$ $2.$ Consider $f(x)=\begin{cases} x^2\sin (1/x)& x\neq 0\\ 0& x=0\end{cases}.$ We have that $f'(x) = 2x\sin(1/x)-\cos(1/x),x\neq 0$ and $f''(x) = 2\sin(1/x)-\dfrac{2}{x}\cos (1/x)-\dfrac{\sin(1/x)}{x^2},x\neq 0.$ Note that $f'(0)=\lim\limits_{h\to 0}\dfrac{h^2\sin (1/h)}{h}\\ =\lim\limits_{h\to 0} h\sin (1/h).$ Also, note that $\forall h>0, -h\leq h\sin(1/h)\leq h$ and $\forall h\leq 0,h \leq h\sin (1/h)\leq -h.$ Hence by the Squeeze Theorem, $\lim\limits_{h\to 0}h\sin (1/h)=\lim\limits_{h\to 0}h = 0.$ In order for $f''(0)$ to exist, we must have that $f'(x)$ is differentiable at $x=0.$ However, we will show that $f'(x)$ is discontinuous at $x=0$ and hence not differentiable there. We will do so by showing that $\lim\limits_{x\to 0^-}f'(x)$ does not exist. Consider the sequence $(x_n)_{n=1}^\infty$ such that $x_n = -\dfrac{1}{\frac\pi2 + 2n\pi}$ and the sequence $(y_n)_{n=1}^\infty$ such that $y_n=-\dfrac{1}{\frac{3\pi}{2}+2n\pi}.$ $\lim\limits_{x\to 0^-}f'(x)$ does not exist because $x_n, y_n\to 0$ as $n\to \infty\Rightarrow \forall \epsilon>0, \exists N (n\geq N \Rightarrow x_n,y_n \in (-\epsilon,0)).$ Since $f'(x_n)<0<f'(y_n)\;\forall n,$ we have that $f''(0)$ does not exist. However, we have that $$\lim\limits_{h\to 0}\dfrac{f(0+h)+f(0-h)-2f(0)}{h^2}=\lim\limits_{h\to 0}\dfrac{h^2\sin (1/h)-h^2\sin(1/h)}{h^2}\\ =0.$$ Thus, the limit exists at $x=0$ but the second derivative does not. edit for the first part (i should've used the taylor series instead). We have that $f(a+h) = f(a) + f'(a)h+f''(a)\dfrac{h^2}{2}+\dots$ and $f(a-h)=f(a)-f'(a)h+f''(a)\dfrac{h^2}{2}+\dots.$ Hence $f(a+h)+f(a-h)-2f(a)=h^2f''(a)$ and the desired limit is $\lim\limits_{h\to 0} \dfrac{h^2f''(a)}{h^2}=f''(a),$ as desired.","Suppose that exists. Show that Show by example that this limit may exist even when does not. My work: By the derivative definition, To see this, let Then and So the limit is equivalent to Consider We have that and Note that Also, note that and Hence by the Squeeze Theorem, In order for to exist, we must have that is differentiable at However, we will show that is discontinuous at and hence not differentiable there. We will do so by showing that does not exist. Consider the sequence such that and the sequence such that does not exist because as Since we have that does not exist. However, we have that Thus, the limit exists at but the second derivative does not. edit for the first part (i should've used the taylor series instead). We have that and Hence and the desired limit is as desired.","1. f''(a) \lim\limits_{h\to 0} \dfrac{f(a+h)+f(a-h)-2f(a)}{h^2}=f''(a). 2. f''(a) 1. f''(a) = \lim\limits_{h\to 0}\dfrac{f'(a+h)-f'(a)}{h}\\ =\lim\limits_{h\to 0} \dfrac{f'(a)-f'(a-h)}{h}. k=-h. k\to0\Leftrightarrow h\to 0 \lim\limits_{h\to 0}\dfrac{f'(a)-f'(a-h)}{h} = \lim\limits_{k\to 0}\dfrac{f'(a)-f'(a+k)}{-k}\\
=\lim\limits_{k\to 0}\dfrac{f'(a+k)-f'(a)}{k}\\
=\lim\limits_{h\to 0}\dfrac{f'(a+h)-f'(a)}{h}. \lim\limits_{h\to 0}\dfrac{\frac{f(a+h)-f(a)}{h}-\frac{f(a)-f(a-h)}{h}}{h}\\
=\lim\limits_{h\to 0}\dfrac{f(a+h)+f(a-h)-2f(a)}{h^2}. 2. f(x)=\begin{cases} x^2\sin (1/x)& x\neq 0\\ 0& x=0\end{cases}. f'(x) = 2x\sin(1/x)-\cos(1/x),x\neq 0 f''(x) = 2\sin(1/x)-\dfrac{2}{x}\cos (1/x)-\dfrac{\sin(1/x)}{x^2},x\neq 0. f'(0)=\lim\limits_{h\to 0}\dfrac{h^2\sin (1/h)}{h}\\
=\lim\limits_{h\to 0} h\sin (1/h). \forall h>0, -h\leq h\sin(1/h)\leq h \forall h\leq 0,h \leq h\sin (1/h)\leq -h. \lim\limits_{h\to 0}h\sin (1/h)=\lim\limits_{h\to 0}h = 0. f''(0) f'(x) x=0. f'(x) x=0 \lim\limits_{x\to 0^-}f'(x) (x_n)_{n=1}^\infty x_n = -\dfrac{1}{\frac\pi2 + 2n\pi} (y_n)_{n=1}^\infty y_n=-\dfrac{1}{\frac{3\pi}{2}+2n\pi}. \lim\limits_{x\to 0^-}f'(x) x_n, y_n\to 0 n\to \infty\Rightarrow \forall \epsilon>0, \exists N (n\geq N \Rightarrow x_n,y_n \in (-\epsilon,0)). f'(x_n)<0<f'(y_n)\;\forall n, f''(0) \lim\limits_{h\to 0}\dfrac{f(0+h)+f(0-h)-2f(0)}{h^2}=\lim\limits_{h\to 0}\dfrac{h^2\sin (1/h)-h^2\sin(1/h)}{h^2}\\
=0. x=0 f(a+h) = f(a) + f'(a)h+f''(a)\dfrac{h^2}{2}+\dots f(a-h)=f(a)-f'(a)h+f''(a)\dfrac{h^2}{2}+\dots. f(a+h)+f(a-h)-2f(a)=h^2f''(a) \lim\limits_{h\to 0} \dfrac{h^2f''(a)}{h^2}=f''(a),",['calculus']
65,"Let $f:[a,b] \to \mathbb{R}$ be differentiable s.t. $f'(x) \neq 1 \ \forall x$. Then $f$ has at most one fixed point.",Let  be differentiable s.t. . Then  has at most one fixed point.,"f:[a,b] \to \mathbb{R} f'(x) \neq 1 \ \forall x f","Let $f:[a,b] \to \mathbb{R}$ be a differentiable function s.t. $f'(x) > \neq 1 \ \forall x \in [a,b]$ . Prove that $f(c)=c$ has at most one   solution $c \in [a,b]$ . My attempt: Let $f'(x) \neq 1 \ \forall x \in [a,b]$ but suppose $\exists \ c_1, c_2 \in [a,b], c_1 \neq c_2$ s.t. $f(c_1) = c_1$ and $f(c_2) = c_2$ . Then by the Mean Value Theorem $\exists  \ d \in [c_1, c_2]$ s.t. $f'(d) = \frac{f(c_1) - f(c_2)}{c_1 - c_2}$ $= \frac{c_1 - c_2}{c_1 - c_2} = 1$ which is a contradiction since $f'(x) \neq 1 \ \forall x \in [a,b]$ . Hence, there is at most one fixed point. $\Box$ I was able to show that the number of fixed points cannot be greater than $1$ . Do I need to show the possible existence of such a point if at all it exists? Or is this it?","Let be a differentiable function s.t. . Prove that has at most one   solution . My attempt: Let but suppose s.t. and . Then by the Mean Value Theorem s.t. which is a contradiction since . Hence, there is at most one fixed point. I was able to show that the number of fixed points cannot be greater than . Do I need to show the possible existence of such a point if at all it exists? Or is this it?","f:[a,b] \to \mathbb{R} f'(x)
> \neq 1 \ \forall x \in [a,b] f(c)=c c \in [a,b] f'(x) \neq 1 \ \forall x \in [a,b] \exists \ c_1, c_2 \in [a,b], c_1 \neq c_2 f(c_1) = c_1 f(c_2) = c_2 \exists  \ d \in [c_1, c_2] f'(d) = \frac{f(c_1) - f(c_2)}{c_1 - c_2} = \frac{c_1 - c_2}{c_1 - c_2} = 1 f'(x) \neq 1 \ \forall x \in [a,b] \Box 1","['proof-verification', 'derivatives']"
66,Show that the minimum of $f$ does not occur at an endpoint.,Show that the minimum of  does not occur at an endpoint.,f,"Let $f$ be a differentiable function on $[a,b]$ but the derivative may be discontinuous. Suppose that $f'(a)<0<f'(b).$ Show that the minimum of $f$ does not occur at an endpoint. What can you conclude? Here's my work so far Since $f$ is differentiable on $[a,b],$ it is continuous on $[a,b].$ We want to show that neither $f(a)$ nor $f(b)$ is a minimum. Since $f$ is continuous and $f'(a)<0<f'(b),$ there exists $\delta>0$ such that $\exists x\in [a,a+\delta)$ such that $f(x)<f(a)$ . By the definition of a derivative, since $f'(a)=\lim\limits_{h\to 0}\dfrac{f(a+h)-f(a)}{h}.$ We thus have that $\forall \epsilon >0,\exists \delta >0$ such that $0<h<\delta\Rightarrow \left|\dfrac{f(a+h)-f(a)}{h}-f'(a)\right|<\epsilon$ (I'm pretty sure this is correct). So we have that $f(a+h)<f(a)+h(f'(a)+\epsilon).$ In particular, this is true if $\epsilon +f'(a)<0\Rightarrow f(a+h)<f(a).$ Thus there exists an $x\in [a,a+\delta)$ such that $f(x)<f(a).$ Similarly, $f'(b)=\lim\limits_{h\to 0}\dfrac{f(b+h)-f(b)}{h}$ and so by the $\epsilon-\delta$ limit definition, $\forall \epsilon >0,\exists \delta >0$ such that $-\delta < h <0\Rightarrow\left|\dfrac{f(b+h)-f(b)}{h}-f'(b)\right|<\epsilon\Rightarrow f(b+h)>f(b)+h(f'(b)-\epsilon).$ In particular, this holds for $\epsilon \leq f'(b)\Rightarrow b+h < b$ and $f(b+h)<f(b).$ Thus $f(x)$ cannot have a local minimum at $b.$ Thus, I can conclude by the Extreme Value Theorem that the minimum is attained in $(a,b).$","Let be a differentiable function on but the derivative may be discontinuous. Suppose that Show that the minimum of does not occur at an endpoint. What can you conclude? Here's my work so far Since is differentiable on it is continuous on We want to show that neither nor is a minimum. Since is continuous and there exists such that such that . By the definition of a derivative, since We thus have that such that (I'm pretty sure this is correct). So we have that In particular, this is true if Thus there exists an such that Similarly, and so by the limit definition, such that In particular, this holds for and Thus cannot have a local minimum at Thus, I can conclude by the Extreme Value Theorem that the minimum is attained in","f [a,b] f'(a)<0<f'(b). f f [a,b], [a,b]. f(a) f(b) f f'(a)<0<f'(b), \delta>0 \exists x\in [a,a+\delta) f(x)<f(a) f'(a)=\lim\limits_{h\to 0}\dfrac{f(a+h)-f(a)}{h}. \forall \epsilon >0,\exists \delta >0 0<h<\delta\Rightarrow \left|\dfrac{f(a+h)-f(a)}{h}-f'(a)\right|<\epsilon f(a+h)<f(a)+h(f'(a)+\epsilon). \epsilon +f'(a)<0\Rightarrow f(a+h)<f(a). x\in [a,a+\delta) f(x)<f(a). f'(b)=\lim\limits_{h\to 0}\dfrac{f(b+h)-f(b)}{h} \epsilon-\delta \forall \epsilon >0,\exists \delta >0 -\delta < h <0\Rightarrow\left|\dfrac{f(b+h)-f(b)}{h}-f'(b)\right|<\epsilon\Rightarrow f(b+h)>f(b)+h(f'(b)-\epsilon). \epsilon \leq f'(b)\Rightarrow b+h < b f(b+h)<f(b). f(x) b. (a,b).",['calculus']
67,Prove that continuous partial derivatives imply continuous total derivative,Prove that continuous partial derivatives imply continuous total derivative,,"Good morning, I'm trying to prove that Suppose $X$ is open in $\mathbb{R}^{n}$ and $F$ is a Banach space. Then $f: X \rightarrow F$ is continuously differentiable if $f$ has continuous partial derivatives. Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! My attempt: For $a \in X$ , we define $A \in \mathcal L(\mathbb R^n,F)$ by $$h=\left(h_{1}, \ldots, h_{n}\right) \mapsto A h=\sum_{k=1}^{n} \partial_{k} f(a) h_{k}$$ Our goal is to show that $\partial f(a) = A$ or equivalently $$\lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0$$ First, we choose $\varepsilon>0$ such that $\mathbb{B}(a, \varepsilon) \subseteq X$ and let $x_k = a+ (h_1,\ldots,h_k,0,\ldots,0)$ for all $k = \overline{1,n}$ . It follows that $$f(a+h)-f(a)=\sum_{k=1}^{n}\left(f\left(x_{k}\right)-f\left(x_{k-1}\right)\right)$$ Let $\{e_1,\ldots,e_n\}$ be the standard basis of $\mathbb R^n$ . By definition, we have $$\begin{aligned}  \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+t h_{k} e_{k} + ze_k\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z} \\   &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+ (th_{k}+z)  e_{k}\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z}\\  &=  \frac{\partial f\left(x_{k-1}+t h_{k} e_{k}\right)}{\partial (th_k)}    \end{aligned}$$ By Fundamental Theorem of Calculus, we have $$\begin{aligned}  h_k\int_0^1 \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) dt &= \int_0^1 \partial_{k} f\left(x_{k-1}+t h^{k} e_{k}\right) d(th_k)\\ &=  \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) \Big|_0^1 \\ &= f\left(x_{k}\right)-f\left(x_{k-1}\right) \end{aligned}$$ As such, $$f(a+h)-f(a)=\sum_{k=1}^{n} h_{k} \int_{0}^{1} \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) d t$$ Consequently, $$\begin{aligned} \|f(a+h)-f(a) - Ah \| &=\left \|\sum_{k=1}^{n} h_{k} \int_{0}^{1} \left(\partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right) d t \right \| \\ &\le \sum_{k=1}^{n} |h_{k}|  \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n} \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1} \sup_{t \in [0,1]} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a) \right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| d t \\&= |h|_\infty \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \|\end{aligned}$$ We have $h \to 0$ implies $\|h\|_\infty \to 0$ , which in turn implies $x \to a$ . It follows from the continuity of $\partial_{k} f\left(x\right)$ that $ \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| \to 0$ as $x \to a$ . Finally, $$\lim _{h \rightarrow 0} \frac{\| f(a+h)-f(a)-A h \|}{|h|_\infty} \le \lim _{h \rightarrow 0} \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| = 0$$ Consequently, $$\lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0$$ Hence $\partial f(a) = A$ . Next we prove that $\partial f(\cdot): X \to \mathcal L(\mathbb R^n,F)$ is continuous. We have $$\begin{aligned}\|\partial f(x)h - \partial f(a)h\| &= \left\| \sum_{k=1}^{n} \partial_{k} f\left(x\right) h_{k} - \sum_{k=1}^{n} \partial_{k} f\left(a\right) h_{k} \right\| \\ &= \left\| h_k \sum_{k=1}^{n} ( \partial_{k} f\left(x\right) - \partial_{k} f\left(a\right)) \right\| \\&\le  \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \cdot |h|_\infty \end{aligned}$$ Consequently, $$\|\partial f(x) - \partial f(y)\| = \sup_{h \in X} \frac{\|\partial f(x)h - \partial f(a)h\|}{|h|_\infty} \le \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\|$$ It follows from the continuity of $\partial_{k} f\left(\cdot\right)$ that $\sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \to 0$ and thus $\|\partial f(x) - \partial f(y)\| \to 0$ as $x \to a$ . Hence $\partial f(x) \to \partial f(y)$ .","Good morning, I'm trying to prove that Suppose is open in and is a Banach space. Then is continuously differentiable if has continuous partial derivatives. Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! My attempt: For , we define by Our goal is to show that or equivalently First, we choose such that and let for all . It follows that Let be the standard basis of . By definition, we have By Fundamental Theorem of Calculus, we have As such, Consequently, We have implies , which in turn implies . It follows from the continuity of that as . Finally, Consequently, Hence . Next we prove that is continuous. We have Consequently, It follows from the continuity of that and thus as . Hence .","X \mathbb{R}^{n} F f: X \rightarrow F f a \in X A \in \mathcal L(\mathbb R^n,F) h=\left(h_{1}, \ldots, h_{n}\right) \mapsto A h=\sum_{k=1}^{n} \partial_{k} f(a) h_{k} \partial f(a) = A \lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0 \varepsilon>0 \mathbb{B}(a, \varepsilon) \subseteq X x_k = a+ (h_1,\ldots,h_k,0,\ldots,0) k = \overline{1,n} f(a+h)-f(a)=\sum_{k=1}^{n}\left(f\left(x_{k}\right)-f\left(x_{k-1}\right)\right) \{e_1,\ldots,e_n\} \mathbb R^n \begin{aligned} 
\partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+t h_{k} e_{k} + ze_k\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z} \\ 
 &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+ (th_{k}+z)  e_{k}\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z}\\
 &=  \frac{\partial f\left(x_{k-1}+t h_{k} e_{k}\right)}{\partial (th_k)}   
\end{aligned} \begin{aligned} 
h_k\int_0^1 \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) dt &= \int_0^1 \partial_{k} f\left(x_{k-1}+t h^{k} e_{k}\right) d(th_k)\\ &=  \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) \Big|_0^1 \\
&= f\left(x_{k}\right)-f\left(x_{k-1}\right)
\end{aligned} f(a+h)-f(a)=\sum_{k=1}^{n} h_{k} \int_{0}^{1} \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) d t \begin{aligned} \|f(a+h)-f(a) - Ah \| &=\left \|\sum_{k=1}^{n} h_{k} \int_{0}^{1} \left(\partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right) d t \right \| \\ &\le \sum_{k=1}^{n} |h_{k}|  \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n} \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1} \sup_{t \in [0,1]} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a) \right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| d t \\&= |h|_\infty \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \|\end{aligned} h \to 0 \|h\|_\infty \to 0 x \to a \partial_{k} f\left(x\right)  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| \to 0 x \to a \lim _{h \rightarrow 0} \frac{\| f(a+h)-f(a)-A h \|}{|h|_\infty} \le \lim _{h \rightarrow 0} \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| = 0 \lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0 \partial f(a) = A \partial f(\cdot): X \to \mathcal L(\mathbb R^n,F) \begin{aligned}\|\partial f(x)h - \partial f(a)h\| &= \left\| \sum_{k=1}^{n} \partial_{k} f\left(x\right) h_{k} - \sum_{k=1}^{n} \partial_{k} f\left(a\right) h_{k} \right\| \\ &= \left\| h_k \sum_{k=1}^{n} ( \partial_{k} f\left(x\right) - \partial_{k} f\left(a\right)) \right\| \\&\le  \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \cdot |h|_\infty \end{aligned} \|\partial f(x) - \partial f(y)\| = \sup_{h \in X} \frac{\|\partial f(x)h - \partial f(a)h\|}{|h|_\infty} \le \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \partial_{k} f\left(\cdot\right) \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \to 0 \|\partial f(x) - \partial f(y)\| \to 0 x \to a \partial f(x) \to \partial f(y)","['real-analysis', 'proof-verification', 'derivatives', 'partial-derivative']"
68,Does this statement about natural logs of functions make sense?,Does this statement about natural logs of functions make sense?,,"For two functions $a(x)$ and $b(x)$ , since $\ln(a/b) = \ln(a) - \ln(b)$ , does $$\frac{d}{dx} \ln(a/b) = \frac{d}{dx} \ln(a) - \frac{d}{dx} \ln(b) = \frac{1}{a} \cdot \frac{da}{dx} - \frac{1}{b} \cdot  \frac{db}{dx}?$$","For two functions and , since , does",a(x) b(x) \ln(a/b) = \ln(a) - \ln(b) \frac{d}{dx} \ln(a/b) = \frac{d}{dx} \ln(a) - \frac{d}{dx} \ln(b) = \frac{1}{a} \cdot \frac{da}{dx} - \frac{1}{b} \cdot  \frac{db}{dx}?,['derivatives']
69,Find time of the shortest distance between two accelerating points in a plane,Find time of the shortest distance between two accelerating points in a plane,,"Given two points $A$ and $B$ in a plane, with initial positions $P$ and velocities $V$ , and constant accelerations $a$ , find solutions for what value of $t$ (in seconds) they will be the closest? I would imagine if I started with a set of values for the 6 variables it would be easier as I could simplify a lot, but in my case I need to find $t$ for any $P_A$ , $V_A$ , $a_A$ , $P_B$ , $V_B$ , $a_B$ . I have solved this problem for when there is no acceleration, and my approach there was to create a function for their positions, and then a distance function $d(t)$ using the Pythagorean theorem, and then minimizing $d(t)$ by solving for $d'(t) = 0$ . I tried this same approach for this case with acceleration, and solving it in Wolfram Cloud gave me a few solutions for $t$ , however, each solution was more than a 1000 symbols long, with many complex parts, so not very practical to use for my case (a simulation). Is there a simpler solution to this problem?","Given two points and in a plane, with initial positions and velocities , and constant accelerations , find solutions for what value of (in seconds) they will be the closest? I would imagine if I started with a set of values for the 6 variables it would be easier as I could simplify a lot, but in my case I need to find for any , , , , , . I have solved this problem for when there is no acceleration, and my approach there was to create a function for their positions, and then a distance function using the Pythagorean theorem, and then minimizing by solving for . I tried this same approach for this case with acceleration, and solving it in Wolfram Cloud gave me a few solutions for , however, each solution was more than a 1000 symbols long, with many complex parts, so not very practical to use for my case (a simulation). Is there a simpler solution to this problem?",A B P V a t t P_A V_A a_A P_B V_B a_B d(t) d(t) d'(t) = 0 t,"['calculus', 'derivatives', 'optimization']"
70,"If a piece wise function is not differentiable at a point, is it still possible to have a derivative of the piece wise function?","If a piece wise function is not differentiable at a point, is it still possible to have a derivative of the piece wise function?",,"For problems 2 and 3 attached, I have found that for problem 2 that the piecewise function is not differentiable at x=3, but I was wondering if there could still be a derivative that can be written. I thought that there is no derivative, but now I am looking back at this problem and I am quite unsure. Also for problem 3, I found that the piecewise function is not continuous at t=1. Therefore, I concluded that the function is not differentiable at t=1 and therefore there is no derivative for the piecewise function. Am I wrong here?","For problems 2 and 3 attached, I have found that for problem 2 that the piecewise function is not differentiable at x=3, but I was wondering if there could still be a derivative that can be written. I thought that there is no derivative, but now I am looking back at this problem and I am quite unsure. Also for problem 3, I found that the piecewise function is not continuous at t=1. Therefore, I concluded that the function is not differentiable at t=1 and therefore there is no derivative for the piecewise function. Am I wrong here?",,"['calculus', 'derivatives']"
71,Spivak Calculus 4th Ed. Chapter 11 Problem 27,Spivak Calculus 4th Ed. Chapter 11 Problem 27,,"I am having trouble understanding what the following problem is getting at. I've included my attempted answers below each part. Chapter 11 Problem 27 (a) Suppose the polynomial function $f(x) = x^n + a_{n-1}x^{n-1} + .. + a_0$ has exactly $k$ critical points and that $f''(x) \neq 0$ for all critical points $x$ . Show that $n - k$ is odd. Since $f''(x) \neq 0$ all critical points are local minima or maxima. The sign of the derivative must switch at and only at each of these points since $f'$ is a polynomial. The sign of the derivative must also match the beginning and ending behavior of $f$ . For example, $f'$ starts negative and ends positive if $n$ is even. So for even $n$ , $k$ must be odd and for odd $n$ , $k$ must be even. (b) For each $n$ , show that if $n - k$ is odd, then there is a polynomial function $f$ of degree $n$ with $k$ critical points, at each of which $f''$ is nonzero. Let $f'(x) = (x - 1)(x - 2)...(x - k)(x^{n - k - 1} + 1)$ and integrate. $n - k - 1$ is even so $f$ has exactly $k$ critical points. Since each critical point $x$ is a single root of $f'$ , $f''(x) \neq 0$ (by product rule). (c) Suppose that the polynomial function $f(x) = x^n + a_{n-1}x^{n-1} + .. + a_0$ has $k_1$ local maximum points and $k_2$ local minimum points. Show that $k_2 = k_1 + 1$ if $n$ is even, and $k_2 = k_1$ if $n$ is odd. Seems like the argument in (a) also works here. Local maxima and minima must alternate and the restrictions on the end behavior enforce the given relationships. (d) Let $n, k_1, k_2$ be three integers with $k_2 = k_1 + 1$ if $n$ is even, and $k_2 = k_1$ if $n$ is odd, and $k_1 + k_2 < n$ . Show that there is a polynomial function $f$ of degree $n$ , with $k_1$ local maximum points and $k_2$ local minimum points. Hint: Pick $a_1 < a_2 < ... < a_{k_1 + k_2}$ and try $f'(x) = \prod_{i = 1}^{k_1 + k_2}(x - a_i) \cdot (1 + x^2)^{l}$ for an appropriate number $l$ . Similarly, seems like part (b) works here. All the critical points $x$ are local minima or maxima since $f''(x) \neq 0$ and the sign switching argument implies that there must be the correct number of each. However, this does not use the hint. Is this still correct? What is he getting at in the hint?","I am having trouble understanding what the following problem is getting at. I've included my attempted answers below each part. Chapter 11 Problem 27 (a) Suppose the polynomial function has exactly critical points and that for all critical points . Show that is odd. Since all critical points are local minima or maxima. The sign of the derivative must switch at and only at each of these points since is a polynomial. The sign of the derivative must also match the beginning and ending behavior of . For example, starts negative and ends positive if is even. So for even , must be odd and for odd , must be even. (b) For each , show that if is odd, then there is a polynomial function of degree with critical points, at each of which is nonzero. Let and integrate. is even so has exactly critical points. Since each critical point is a single root of , (by product rule). (c) Suppose that the polynomial function has local maximum points and local minimum points. Show that if is even, and if is odd. Seems like the argument in (a) also works here. Local maxima and minima must alternate and the restrictions on the end behavior enforce the given relationships. (d) Let be three integers with if is even, and if is odd, and . Show that there is a polynomial function of degree , with local maximum points and local minimum points. Hint: Pick and try for an appropriate number . Similarly, seems like part (b) works here. All the critical points are local minima or maxima since and the sign switching argument implies that there must be the correct number of each. However, this does not use the hint. Is this still correct? What is he getting at in the hint?","f(x) = x^n + a_{n-1}x^{n-1} + .. + a_0 k f''(x) \neq 0 x n - k f''(x) \neq 0 f' f f' n n k n k n n - k f n k f'' f'(x) = (x - 1)(x - 2)...(x - k)(x^{n - k - 1} + 1) n - k - 1 f k x f' f''(x) \neq 0 f(x) = x^n + a_{n-1}x^{n-1} + .. + a_0 k_1 k_2 k_2 = k_1 + 1 n k_2 = k_1 n n, k_1, k_2 k_2 = k_1 + 1 n k_2 = k_1 n k_1 + k_2 < n f n k_1 k_2 a_1 < a_2 < ... < a_{k_1 + k_2} f'(x) = \prod_{i = 1}^{k_1 + k_2}(x - a_i) \cdot (1 + x^2)^{l} l x f''(x) \neq 0","['real-analysis', 'calculus', 'derivatives']"
72,Matrix-by-matrix derivative,Matrix-by-matrix derivative,,"The definition of the matrix-by-matrix derivative is: $$ \frac{\partial X_{kl}}{\partial X_{ij}}=\delta_{ik}\delta_{lj} $$ If the matrices are $n\times n$ , then the resulting matrix will be $n^2 \times n^2$ . Is the following identity valid for the matrix-by-matrix derivative? $$ \frac{\partial}{\partial A} AB = \frac{\partial A}{\partial A} B + A\frac{\partial B}{\partial A} $$ If so, I do not understand how we can multiply a $n^2 \times n^2$ matrix by a $n \times n$ matrix? $$ \frac{\partial}{\partial A} AB = \underbrace{\frac{\partial A}{\partial A}}_{n^2\times n^2} \overbrace{B}^{n\times n} + \overbrace{A}^{n\times n} \underbrace{\frac{\partial B}{\partial A}}_{n^2 \times n^2} $$","The definition of the matrix-by-matrix derivative is: If the matrices are , then the resulting matrix will be . Is the following identity valid for the matrix-by-matrix derivative? If so, I do not understand how we can multiply a matrix by a matrix?","
\frac{\partial X_{kl}}{\partial X_{ij}}=\delta_{ik}\delta_{lj}
 n\times n n^2 \times n^2 
\frac{\partial}{\partial A} AB = \frac{\partial A}{\partial A} B + A\frac{\partial B}{\partial A}
 n^2 \times n^2 n \times n 
\frac{\partial}{\partial A} AB = \underbrace{\frac{\partial A}{\partial A}}_{n^2\times n^2} \overbrace{B}^{n\times n} + \overbrace{A}^{n\times n} \underbrace{\frac{\partial B}{\partial A}}_{n^2 \times n^2}
","['derivatives', 'matrix-calculus']"
73,Antiderivative of a branch function,Antiderivative of a branch function,,"Let $ f: \mathbb{R} \mapsto \mathbb{R} $ , $ f(x) = e^{x} $ for $ x \leq 0 $ and $ ax+b $ for $ x > 0 $ . I want to determine the real numbers $ a $ and $ b $ for which $ f $ admits antiderivatives. I took a plausible antiderivative $ F(x) = e^{x} + c $ for $ x < 0 $ , $ F(0) = \alpha $ and $ a \frac{x^{2}}{2} + bx + d $ for $ x > 0 $ and using the fact that $ F $ must be first continuous and then differentiable, I showed that $ b = 1 $ . This is easy to see as from continuity we must have that $ d = \alpha = c+1 $ . Then, since F is clearly differentiable on $ \mathbb{R} \setminus \{0\} $ if we impose the condition that the left and right derivatives at $ 0 $ of $ F $ to be equal so we must have: $$ \lim_{x\rightarrow 0, x <0 }\frac{F(x)-F(0)}{x-0} = \lim_{x\rightarrow 0, x > 0 }\frac{F(x)-F(0)}{x-0}$$ Thus: $$ \lim_{x\rightarrow 0, x <0 }\frac{e^{x}+c-c-1}{x} =  \lim_{x\rightarrow 0, x > 0 }\frac{a\frac{x^{2}}{2}+bx+c+1-c-1}{x} $$ Which gives us $ b = 1 $ . My question is then, does this suffice? Can $ a $ be chosen arbitrarily such that $ f $ has antiderivatives as long as $ b = 1 $ . It would seem that the function $ F(X) = e^{x}+c $ for $ x \leq 0 $ and $ F(x) = a\frac{x^{2}}{2}+x+c+1 $ for $ x > 0 $ for $ c \in \mathbb{R} $ does indeed satisfy the necessary conditions. Is it maybe that I am missing something here? I would appreciate any comments. Thank you!","Let , for and for . I want to determine the real numbers and for which admits antiderivatives. I took a plausible antiderivative for , and for and using the fact that must be first continuous and then differentiable, I showed that . This is easy to see as from continuity we must have that . Then, since F is clearly differentiable on if we impose the condition that the left and right derivatives at of to be equal so we must have: Thus: Which gives us . My question is then, does this suffice? Can be chosen arbitrarily such that has antiderivatives as long as . It would seem that the function for and for for does indeed satisfy the necessary conditions. Is it maybe that I am missing something here? I would appreciate any comments. Thank you!"," f: \mathbb{R} \mapsto \mathbb{R}   f(x) = e^{x}   x \leq 0   ax+b   x > 0   a   b   f   F(x) = e^{x} + c   x < 0   F(0) = \alpha   a \frac{x^{2}}{2} + bx + d   x > 0   F   b = 1   d = \alpha = c+1   \mathbb{R} \setminus \{0\}   0   F   \lim_{x\rightarrow 0, x <0 }\frac{F(x)-F(0)}{x-0} = \lim_{x\rightarrow 0, x > 0 }\frac{F(x)-F(0)}{x-0}  \lim_{x\rightarrow 0, x <0 }\frac{e^{x}+c-c-1}{x} =  \lim_{x\rightarrow 0, x > 0 }\frac{a\frac{x^{2}}{2}+bx+c+1-c-1}{x}   b = 1   a   f   b = 1   F(X) = e^{x}+c   x \leq 0   F(x) = a\frac{x^{2}}{2}+x+c+1   x > 0   c \in \mathbb{R} ","['real-analysis', 'integration', 'derivatives']"
74,Modelling a horizontal mass damper using differential equations,Modelling a horizontal mass damper using differential equations,,"I am trying to solve this second-order differential equation: $y'' + y'+ y + (y')^2 =0$ I was able to solve the equation $y'' + y'+ y $ , by substituting $y$ as $Ae^{kt}$ .  But now I have this new term $(y')^2$ . Note: This equation represents the simplified equation of forces on a horizontal mass damper.","I am trying to solve this second-order differential equation: I was able to solve the equation , by substituting as .  But now I have this new term . Note: This equation represents the simplified equation of forces on a horizontal mass damper.",y'' + y'+ y + (y')^2 =0 y'' + y'+ y  y Ae^{kt} (y')^2,"['calculus', 'ordinary-differential-equations', 'derivatives', 'mathematical-modeling']"
75,Gradient-based optimization: A small change in the input obtains a corresponding change in the output,Gradient-based optimization: A small change in the input obtains a corresponding change in the output,,"My textbook, Deep Learning by Goodfellow, Bengio, and Courville, says the following in a section on gradient-based optimization: The derivative of this function is denoted as $f'(x)$ or as $\dfrac{dy}{dx}$ . The derivative $f'(x)$ gives the slope of $f(x)$ as the point $x$ . In other words, it specifies how to scale a small change in the input to obtain the corresponding change in the output: $f(x + \epsilon) \approx f(x) + \epsilon f'(x)$ . The derivative is therefore useful for minimizing a function because it tells us how to change $x$ in order to make a small improvement in $y$ . For example, we know that $f(x - \epsilon \text{sign}(f'(x)))$ is less than $f(x)$ for small enough $\epsilon$ . We can thus reduce $f(x)$ by moving $x$ in small steps with the opposite sign of the derivative. Just to be clear, the reason why we can conclude that a small change in the input obtains a corresponding change in the output is because the function is continuous at every point in its domain (and must be so, since it is assumed to be differentiable)? Without this being the case, it seems to me that a small change in the input would not necessarily obtain a corresponding change in the output; and nor would the function necessarily be differentiable at every point in its domain, since there could be discontinuities or anomalies? I just want to be confirm that my thoughts about this are correct. Thank you.","My textbook, Deep Learning by Goodfellow, Bengio, and Courville, says the following in a section on gradient-based optimization: The derivative of this function is denoted as or as . The derivative gives the slope of as the point . In other words, it specifies how to scale a small change in the input to obtain the corresponding change in the output: . The derivative is therefore useful for minimizing a function because it tells us how to change in order to make a small improvement in . For example, we know that is less than for small enough . We can thus reduce by moving in small steps with the opposite sign of the derivative. Just to be clear, the reason why we can conclude that a small change in the input obtains a corresponding change in the output is because the function is continuous at every point in its domain (and must be so, since it is assumed to be differentiable)? Without this being the case, it seems to me that a small change in the input would not necessarily obtain a corresponding change in the output; and nor would the function necessarily be differentiable at every point in its domain, since there could be discontinuities or anomalies? I just want to be confirm that my thoughts about this are correct. Thank you.",f'(x) \dfrac{dy}{dx} f'(x) f(x) x f(x + \epsilon) \approx f(x) + \epsilon f'(x) x y f(x - \epsilon \text{sign}(f'(x))) f(x) \epsilon f(x) x,"['real-analysis', 'derivatives', 'numerical-methods', 'machine-learning', 'gradient-descent']"
76,How to calculate this derivate using the chain rule,How to calculate this derivate using the chain rule,,"How can we get this result, step by step $$\frac{d(\frac{f(k_t)}{k_t})}{dt}= \frac{\dot{k_t}}{k_t}(f'(k_t)-\frac{f(k_t)}{k_t})$$","How can we get this result, step by step",\frac{d(\frac{f(k_t)}{k_t})}{dt}= \frac{\dot{k_t}}{k_t}(f'(k_t)-\frac{f(k_t)}{k_t}),"['derivatives', 'chain-rule']"
77,Is the following way differentiating $x^TAx$ correct,Is the following way differentiating  correct,x^TAx,"I know that there are a lot of question where the differential is found. Before i checked out those answered i tried myself, and i can't quite figure out if the results are the same. Here is what i tried. We have the following $$ \begin{array}{c}{f(x)=x^{T} A x} \\ {x=\left(\begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{n}}\end{array}\right) \quad \text { , } A=\left[\begin{array}{ccc}{a_{11}} & {\dots} & {a_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {a_{n 1}} & {\dots} & {a_{n n}}\end{array}\right]}\end{array} $$ Find $$\frac{\partial f(x)}{\partial x_i}$$ This can be rewritten in the following way $$ f(x)=\sum_{k=1}^{n} \sum_{r=1}^{n} a_{r k} \cdot x_{r} \cdot x_{k} $$ We can divide this into the following cases $$ \left\{\begin{array}{ll}{x_{k} \cdot \sum_{r=1}^{n}\left(x_{k} \cdot a_{r k}\right)=\sum_{r=1}^{n} a_{r k} \cdot x_{k}^{2}} & {\text { for } k=i} \\ {x_{k} \cdot \sum_{r=1}^{n}\left(x_{k} \cdot a_{r k}\right)=c_{k}} & {\text { for } k \neq i}\end{array}\right. $$ Since we have to find the partial with respect to $x_i$ and constants will disapear we have to find the parital derivative of the following, $$ \begin{array}{c}{f(x)=\sum_{r=1}^{n} a_{r i} \cdot x_{i}^{2}} \\ {f_{x_{i}}^{\prime}(x)=2 \cdot \sum_{r=1}^{n} a_{r i} \cdot x_{i}}\end{array} $$ Is this method correct, or am i making an error somewhere? I understand the way other answers on here solves it, but i dont see why this is wrong - if it is.","I know that there are a lot of question where the differential is found. Before i checked out those answered i tried myself, and i can't quite figure out if the results are the same. Here is what i tried. We have the following Find This can be rewritten in the following way We can divide this into the following cases Since we have to find the partial with respect to and constants will disapear we have to find the parital derivative of the following, Is this method correct, or am i making an error somewhere? I understand the way other answers on here solves it, but i dont see why this is wrong - if it is.","
\begin{array}{c}{f(x)=x^{T} A x} \\ {x=\left(\begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{n}}\end{array}\right) \quad \text { , } A=\left[\begin{array}{ccc}{a_{11}} & {\dots} & {a_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {a_{n 1}} & {\dots} & {a_{n n}}\end{array}\right]}\end{array}
 \frac{\partial f(x)}{\partial x_i} 
f(x)=\sum_{k=1}^{n} \sum_{r=1}^{n} a_{r k} \cdot x_{r} \cdot x_{k}
 
\left\{\begin{array}{ll}{x_{k} \cdot \sum_{r=1}^{n}\left(x_{k} \cdot a_{r k}\right)=\sum_{r=1}^{n} a_{r k} \cdot x_{k}^{2}} & {\text { for } k=i} \\ {x_{k} \cdot \sum_{r=1}^{n}\left(x_{k} \cdot a_{r k}\right)=c_{k}} & {\text { for } k \neq i}\end{array}\right.
 x_i 
\begin{array}{c}{f(x)=\sum_{r=1}^{n} a_{r i} \cdot x_{i}^{2}} \\ {f_{x_{i}}^{\prime}(x)=2 \cdot \sum_{r=1}^{n} a_{r i} \cdot x_{i}}\end{array}
","['calculus', 'analysis']"
78,Limes superior and inferior inequalities with functions and its derivatives,Limes superior and inferior inequalities with functions and its derivatives,,"Lemma 2. from N.H. Duu, On The Existence of Bounded Solutions for Lotka- Volterra Equations , Acta Mathematica Vietnamica 25(2) (2000), 145-159. Let $G(t)$ and $F(t)$ be two diferentiable functions defined on $(0,\infty)$ such that $\lim_{t\to\infty} G(t) = \lim_{t\to\infty}F(t)  = +\infty$ then $$\limsup_{t\to\infty}\frac{G(t)}{F(t)}\leq \limsup_{t\to\infty}\frac{G'(t)}{F'(t)};  \quad\liminf_{t\to\infty}\frac{G(t)}{F(t)}\geq  \liminf_{t\to\infty}\frac{G'(t)}{F'(t)}.$$ Proof. By the Cauchy theorem for diferential functions, for any $t_{1}, t_{2} > 0$ there is an $\theta \in (t_{1}, t_{2})$ such that $$\frac{G'(\theta)}{F'(\theta)}=\frac{G(t_{1})-G(t_{2})}{F(t_{1})-F(t_{2})}=\dfrac{G(t_{2})}{F(t_{2})}\times\dfrac{1-\dfrac{G(t_{1})}{G(t_{2})}}{1-\dfrac{F(t_{1})}{F(t_{2})}}.$$ Letting $t_{1}$ and $t_{2} \to \infty$ such that $\lim\dfrac{G(t_{1})}{G(t_{2})}  = lim \dfrac{F(t_{1})}{F(t_{2})}= 0$ we get the result. Can anybody help me understand the last part of this proof? Exactly, how this transformation $$\frac{G'(\theta)}{F'(\theta)}=\frac{G(t_{1})-G(t_{2})}{F(t_{1})-F(t_{2})}=\dfrac{G(t_{2})}{F(t_{2})}\times\dfrac{1-\dfrac{G(t_{1})}{G(t_{2})}}{1-\dfrac{F(t_{1})}{F(t_{2})}} \text{where} \lim_{t_1,t_2\to\infty}\dfrac{G(t_{1})}{G(t_{2})}  = \lim_{t_1,t_2\to\infty} \dfrac{F(t_{1})}{F(t_{2})}= 0$$ leads to inequalities from lemma?","Lemma 2. from N.H. Duu, On The Existence of Bounded Solutions for Lotka- Volterra Equations , Acta Mathematica Vietnamica 25(2) (2000), 145-159. Let and be two diferentiable functions defined on such that then Proof. By the Cauchy theorem for diferential functions, for any there is an such that Letting and such that we get the result. Can anybody help me understand the last part of this proof? Exactly, how this transformation leads to inequalities from lemma?","G(t) F(t) (0,\infty) \lim_{t\to\infty} G(t) = \lim_{t\to\infty}F(t)
 = +\infty \limsup_{t\to\infty}\frac{G(t)}{F(t)}\leq \limsup_{t\to\infty}\frac{G'(t)}{F'(t)};
 \quad\liminf_{t\to\infty}\frac{G(t)}{F(t)}\geq
 \liminf_{t\to\infty}\frac{G'(t)}{F'(t)}. t_{1}, t_{2} > 0 \theta \in (t_{1}, t_{2}) \frac{G'(\theta)}{F'(\theta)}=\frac{G(t_{1})-G(t_{2})}{F(t_{1})-F(t_{2})}=\dfrac{G(t_{2})}{F(t_{2})}\times\dfrac{1-\dfrac{G(t_{1})}{G(t_{2})}}{1-\dfrac{F(t_{1})}{F(t_{2})}}. t_{1} t_{2} \to \infty \lim\dfrac{G(t_{1})}{G(t_{2})}
 = lim \dfrac{F(t_{1})}{F(t_{2})}= 0 \frac{G'(\theta)}{F'(\theta)}=\frac{G(t_{1})-G(t_{2})}{F(t_{1})-F(t_{2})}=\dfrac{G(t_{2})}{F(t_{2})}\times\dfrac{1-\dfrac{G(t_{1})}{G(t_{2})}}{1-\dfrac{F(t_{1})}{F(t_{2})}} \text{where} \lim_{t_1,t_2\to\infty}\dfrac{G(t_{1})}{G(t_{2})}
 = \lim_{t_1,t_2\to\infty} \dfrac{F(t_{1})}{F(t_{2})}= 0","['calculus', 'limits', 'derivatives', 'limsup-and-liminf']"
79,differentiate the parameter with function,differentiate the parameter with function,,"It seems very basic but I can't understand... Consider $f(x)=2x$ . I want to differentiate $L=(x-f(x))b + 3(f(x))$ regarding $f(x)$ . When I looked at the solution, it says $\partial L/\partial f = -b+3$ . But I am confused because I can't get why $x$ is simply considered a constant and ignored. Shouldn't it be $$ (dx/df - 1)b+3 = (1/2-1)b+3=-b/2+3? $$ Please help me out. Thank you!","It seems very basic but I can't understand... Consider . I want to differentiate regarding . When I looked at the solution, it says . But I am confused because I can't get why is simply considered a constant and ignored. Shouldn't it be Please help me out. Thank you!","f(x)=2x L=(x-f(x))b + 3(f(x)) f(x) \partial L/\partial f = -b+3 x 
(dx/df - 1)b+3 = (1/2-1)b+3=-b/2+3?
",['derivatives']
80,Newton Raphson method issues with differentiation,Newton Raphson method issues with differentiation,,"I am trying to solve for the roots of the follwing equation using the newton raphson method $$f\left(x\right)\ =-I+I_{ph}-I_s\times\left(e^\frac{q\times(V_c+I\times R_c)}{n\times k\times T}-1\right)$$ The solution that I am trying to find is: $$I_{n+1}=\ I_n-\frac{I_n-I_{ph}+I_s\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}-1\right)}{\frac{{q\times I}_s\times{R_c\times\left(e^\frac{q\times(V_c+I_n\ timesR_c)}{n\times k\times T}-1\right)}_\ }{n\times k\times T}+1}$$ However when i reproduce the method myself I arrive to: $$I_{n+1}=\ I_n-\frac{I_n-I_{ph}+I_s\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}-1\right)}{\frac{{q\times I}_s\times{R_c\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}\right)}_\ }{n\times k\times T}+1}$$ I have used matlab to find the derivative and do the calculations, I dont know where the -1 term in the denominator has gone. The Method does not work without it.","I am trying to solve for the roots of the follwing equation using the newton raphson method The solution that I am trying to find is: However when i reproduce the method myself I arrive to: I have used matlab to find the derivative and do the calculations, I dont know where the -1 term in the denominator has gone. The Method does not work without it.",f\left(x\right)\ =-I+I_{ph}-I_s\times\left(e^\frac{q\times(V_c+I\times R_c)}{n\times k\times T}-1\right) I_{n+1}=\ I_n-\frac{I_n-I_{ph}+I_s\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}-1\right)}{\frac{{q\times I}_s\times{R_c\times\left(e^\frac{q\times(V_c+I_n\ timesR_c)}{n\times k\times T}-1\right)}_\ }{n\times k\times T}+1} I_{n+1}=\ I_n-\frac{I_n-I_{ph}+I_s\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}-1\right)}{\frac{{q\times I}_s\times{R_c\times\left(e^\frac{q\times(V_c+I_n\times R_c)}{n\times k\times T}\right)}_\ }{n\times k\times T}+1},"['derivatives', 'newton-raphson']"
81,Correctly applying rules of differentiation,Correctly applying rules of differentiation,,"Let $f: \mathbb{R} \to \mathbb{R}$ be: $$      f(x)=\left\{\begin{array}{lll} \frac{x^3-\sqrt{2e^x}}{3x^3}, & x>0 \\          -x^2+2x^4-7, & x=0 \\ x^3-4e^2x, & x < 0. \end{array}\right . $$ If I want to analyze rigorously where $f$ is differentiable I would do it as follows: As $\mathbb{R_{>0}}$ is an open set we can apply the rules of differentiation (quotient rule, chain rule etc...) to $f_{\vert \mathbb{R_{>0}}}$ and hence $f$ is differentiable on the restriction $\mathbb{R_{>0}}$ . Then I apply the same reasoning to $f_{\vert \mathbb{R_{<0}}}$ . Although $f(x)=x^2+2x^4-7$ looks nice and smoothly, I mustn't apply the rules of differentiation to $f(x)=x^2+2x^4-7$ as it is only defined like this on an isolated point and not an open set. So I have to check the limit of the differential quotient to correctly prove if $f$ is differentiable or not on this restriction. I don't want to calculate the limit but can you give me a quick feedback on whether I have understood it correctly or not? I am not 100% sure if it is right to say that I mustn't apply the rules of differentiation to $f$ where it is only restricted on a sigle point. Any comments are appreciated","Let be: If I want to analyze rigorously where is differentiable I would do it as follows: As is an open set we can apply the rules of differentiation (quotient rule, chain rule etc...) to and hence is differentiable on the restriction . Then I apply the same reasoning to . Although looks nice and smoothly, I mustn't apply the rules of differentiation to as it is only defined like this on an isolated point and not an open set. So I have to check the limit of the differential quotient to correctly prove if is differentiable or not on this restriction. I don't want to calculate the limit but can you give me a quick feedback on whether I have understood it correctly or not? I am not 100% sure if it is right to say that I mustn't apply the rules of differentiation to where it is only restricted on a sigle point. Any comments are appreciated","f: \mathbb{R} \to \mathbb{R} 
     f(x)=\left\{\begin{array}{lll} \frac{x^3-\sqrt{2e^x}}{3x^3}, & x>0 \\
         -x^2+2x^4-7, & x=0 \\
x^3-4e^2x, & x < 0.
\end{array}\right .
 f \mathbb{R_{>0}} f_{\vert \mathbb{R_{>0}}} f \mathbb{R_{>0}} f_{\vert \mathbb{R_{<0}}} f(x)=x^2+2x^4-7 f(x)=x^2+2x^4-7 f f","['real-analysis', 'calculus', 'limits', 'derivatives']"
82,We have a function which takes a two-dimensional input and has two parameters. How to find (∂𝑓/∂(𝑤_2)),We have a function which takes a two-dimensional input and has two parameters. How to find (∂𝑓/∂(𝑤_2)),,"Question: We have a function which takes a two-dimensional input $x = (x_1, x_2)$ and has two parameters $w = (w_1, w_2)$ given by $f(x, w) = σ(σ(x1w1)w_2 + x_2)$ where $σ(x) = 1/(1+e^{-x}))$ . We use backpropagation to estimate the right parameter values. We start by setting both the parameters to 0. Assume that we are given a training point x1 = 1, x2 = 0, y = 5. Given this information answer the next two questions. What is the value of $∂f/∂w_2$ ? Solution: Write $σ(x_1w_1)w_2 + x_2$ as $o_2$ and $x_1w_1$ as $o_1$ $∂f/∂w_2=∂f/∂o_2*∂o_2/∂w_2$ $∂f/∂w_2= σ(o_2)(1 − σ(o_2)) × σ(o_1)$ # Need to understand here $∂f/∂w_2 = 0.5 ∗ 0.5 ∗ 0.5 =0.125$ Can some one help me to understand the solution? What is the $f$ equation, which partially derivated with $o_2$ to get $σ(o_2)(1 − σ(o_2))$ ? And not understood, from where $0.5$ came. Please help.","Question: We have a function which takes a two-dimensional input and has two parameters given by where . We use backpropagation to estimate the right parameter values. We start by setting both the parameters to 0. Assume that we are given a training point x1 = 1, x2 = 0, y = 5. Given this information answer the next two questions. What is the value of ? Solution: Write as and as # Need to understand here Can some one help me to understand the solution? What is the equation, which partially derivated with to get ? And not understood, from where came. Please help.","x = (x_1, x_2) w = (w_1, w_2) f(x, w) = σ(σ(x1w1)w_2 + x_2) σ(x) = 1/(1+e^{-x})) ∂f/∂w_2 σ(x_1w_1)w_2 + x_2 o_2 x_1w_1 o_1 ∂f/∂w_2=∂f/∂o_2*∂o_2/∂w_2 ∂f/∂w_2= σ(o_2)(1 − σ(o_2)) × σ(o_1) ∂f/∂w_2
= 0.5 ∗ 0.5 ∗ 0.5
=0.125 f o_2 σ(o_2)(1 − σ(o_2)) 0.5","['derivatives', 'partial-derivative', 'machine-learning']"
83,Tom Apostol mathematical analysis exercise 5.28 Prove the following theorem do not use L‘Hospital rule,Tom Apostol mathematical analysis exercise 5.28 Prove the following theorem do not use L‘Hospital rule,,"5.28   Let $f$ and $g$ be two functions having finite nth derivatives in (a,b).for some interior point c in (a,b),assumes that $f(c)=f’(c)=\dots=f^{n-1}(c)=0$ ,and that $g(c)=g’(c)=\dots=g^{n-1}(c)=0$ ,but $g^{n}(x)$ is never zero in (a,b),show that $$\lim_{x\to c}\frac{f(x)}{g(x)}=\frac{f^{n}(c)}{g^{n}(c)}$$ Note. $f^{(n)}$ and $g^{(n)}$ are  not assumed to be continous at c. $Hint.$ Let $$F(x)=f(x)-\frac{(x-c)^{n-1}f^{(n-1)}(c)}{(n-1)!},$$ I research for n=1. $F(x)=f(x)-f(c),G(x)=g(x)-g(c)$ ,use Cauchy mean value theorem. I got $f(x)g’(x_1)=f’(x_1)g(x)$ where $x_1$ is interior to the interval joining $x$ and $c$ $$|\frac{f(x)}{g(x)}-\frac{f’(c)}{g’(c)}|=|\frac{f’(x_1)}{g’(x_1)}-\frac{f’(c)}{g’(c)}|$$ But $f’(x),g’(x)$ are not assumed to be continuous at $c$ . I don’t know how to continue.Could you help me ?pleasa do not use L Hospital because this exercise in which chapter isn’t teach L Hospital. Thanks in advance!!! PS,The problem in the book where emphasis that $f^{n}(x),g^{n}(x)$ is not assumed to be continuous at point c","5.28   Let and be two functions having finite nth derivatives in (a,b).for some interior point c in (a,b),assumes that ,and that ,but is never zero in (a,b),show that Note. and are  not assumed to be continous at c. Let I research for n=1. ,use Cauchy mean value theorem. I got where is interior to the interval joining and But are not assumed to be continuous at . I don’t know how to continue.Could you help me ?pleasa do not use L Hospital because this exercise in which chapter isn’t teach L Hospital. Thanks in advance!!! PS,The problem in the book where emphasis that is not assumed to be continuous at point c","f g f(c)=f’(c)=\dots=f^{n-1}(c)=0 g(c)=g’(c)=\dots=g^{n-1}(c)=0 g^{n}(x) \lim_{x\to c}\frac{f(x)}{g(x)}=\frac{f^{n}(c)}{g^{n}(c)} f^{(n)} g^{(n)} Hint. F(x)=f(x)-\frac{(x-c)^{n-1}f^{(n-1)}(c)}{(n-1)!}, F(x)=f(x)-f(c),G(x)=g(x)-g(c) f(x)g’(x_1)=f’(x_1)g(x) x_1 x c |\frac{f(x)}{g(x)}-\frac{f’(c)}{g’(c)}|=|\frac{f’(x_1)}{g’(x_1)}-\frac{f’(c)}{g’(c)}| f’(x),g’(x) c f^{n}(x),g^{n}(x)",['calculus']
84,linear differential equation word problem (salt problem),linear differential equation word problem (salt problem),,The question is as follows: I am confused because of intial and after 30 mins concentration dont know how to split that. Still here is what i tried: concentration initially = $2\times 2 = 4000g/min$ after 30 min = $2\times 1 = 2000g/min$ Inflow rate : $di/dt = 2000$ after 30 mins outflow rate : $do/dt = 2 l/min$ Now not sure how to implement $y(t)$ here after 30 mins will it be ? $$4000-\frac{y}{10}=y'$$ ?,The question is as follows: I am confused because of intial and after 30 mins concentration dont know how to split that. Still here is what i tried: concentration initially = after 30 min = Inflow rate : after 30 mins outflow rate : Now not sure how to implement here after 30 mins will it be ? ?,2\times 2 = 4000g/min 2\times 1 = 2000g/min di/dt = 2000 do/dt = 2 l/min y(t) 4000-\frac{y}{10}=y',"['calculus', 'algebra-precalculus', 'ordinary-differential-equations', 'derivatives', 'word-problem']"
85,Need a Intuitive thinking for the proof of Mean value theorem for scalar field,Need a Intuitive thinking for the proof of Mean value theorem for scalar field,,"Ok so the proof is laid out on my book but I'm genuinely struggling to have a geometrical/Intuitive thinking to this particular proof. I badly need help with it. So here is the statement of the proof (PLEASE NOTE THAT $D_u$ in this case implies directional derivative in the direction of the unit vector $u$ ): Let $f: S\to \mathbb R$ be a scalar field. Let $a\in S$ and where $S\subseteq \mathbb R^n$ So let $u$ be a unit vector and $u\in\mathbb{R}^n$ and $a + ut \in S$ for all $0\leq t \leq 1.$ Suppose $D_uf(a)$ exists for all $a + ut$ where $0 \leq t \leq 1$ then there exist a $\theta$ such that $0<\theta<1$ and $$f(a+u)-f(a) = D_uf(z), \text{where }z= a + u\theta$$ Now I'm not gonna write the proof that is written in my book because I fairly understood the steps well. I don't understand where this $z= a + u\theta$ came or what does it even imply or how it looks geometrically.",Ok so the proof is laid out on my book but I'm genuinely struggling to have a geometrical/Intuitive thinking to this particular proof. I badly need help with it. So here is the statement of the proof (PLEASE NOTE THAT in this case implies directional derivative in the direction of the unit vector ): Let be a scalar field. Let and where So let be a unit vector and and for all Suppose exists for all where then there exist a such that and Now I'm not gonna write the proof that is written in my book because I fairly understood the steps well. I don't understand where this came or what does it even imply or how it looks geometrically.,"D_u u f: S\to \mathbb R a\in S S\subseteq \mathbb R^n u u\in\mathbb{R}^n a + ut \in S 0\leq t \leq 1. D_uf(a) a + ut 0 \leq t \leq 1 \theta 0<\theta<1 f(a+u)-f(a) = D_uf(z), \text{where }z= a + u\theta z= a + u\theta","['calculus', 'derivatives', 'partial-derivative']"
86,Gradient of the Sinkhorn Distance for Regularized Optimal Transport,Gradient of the Sinkhorn Distance for Regularized Optimal Transport,,"Given two probability measures $\mathbf{r}\in \Sigma_n$ and $\mathbf{c}\in \Sigma_n$ , Cuturi 2013 [1] defines the Sinkhorn distances as: $$ d_{\mathbf{M},\alpha}(\mathbf{r}, \mathbf{c}) = \min_{\mathbf{P} \in U_{\alpha}(\mathbf{r},\mathbf{c})} \langle \mathbf{P}, \mathbf{M} \rangle $$ where the cost matrix $\mathbf{M}$ defines the underlying metric space, and $U$ depicts the set of transportation plans: $$ U_{\alpha}(\mathbf{r},\mathbf{c}) = \{\mathbf{P}\in U(\mathbf{r},\mathbf{c}) \,:\, D_{\text{KL}} (\mathbf{P} || \mathbf{r}\mathbf{c}^\top) \leq \alpha \} $$ $D_{\text{KL}}$ refers to the KL-divergence and $\alpha$ controls the regularization. In practice $d(\cdot)$ can be computed efficiently by considering the dual problem. The algorithm looks like: Essentially, I would like to minimize energies that involves Sinkhorn distances, such as: $$ \arg\min_{\boldsymbol{r}\in \Sigma_n} \sum\limits_{i=1}^k d_{\mathbf{M},\alpha}(\boldsymbol{r}, \mathbf{c}_i) $$ This is only one example. Though, to this aim, I would need the gradient of the Sinkhorn distance w.r.t. the empirical measures, e.g. $\nabla_{\mathbf{r}} d_{\mathbf{M},\alpha}$ and $\nabla_{\mathbf{c}} d_{\mathbf{M},\alpha}$ . Using [2] (Proposition 1 and 2), I have hunch that the gradient w.r.t. $\mathbf{r}$ would read: $$ \nabla_\mathbf{r} d_{\mathbf{M}}^\lambda = -\frac{\log(\mathbf{u})}{\lambda}+\frac{\log(\mathbf{u})^\top\mathbf{1}_n}{\lambda n}\mathbf{1}_n $$ where $d_{\mathbf{M}}^\lambda$ refers to the dual problem that is equivalent to the primal $d_{\mathbf{M},\alpha}$ , and the pair $(\mathbf{u}\in\mathbb{R}^n_+,\mathbf{v}\in\mathbb{R}^n_+)$ is computed through the Sinkhorn matrix scaling applied to $K=\exp(-\lambda \mathbf{M})$ . The term on the right hand side is used to normalize the gradient to keep it in the tangent bundle of the simplex. Does that mean that the gradient w.r.t. $\mathbf{c}$ would be: $$ \nabla_\mathbf{c} d_{\mathbf{M}}^\lambda = -\frac{\log(\mathbf{v})}{\lambda}+\frac{\log(\mathbf{v})^\top\mathbf{1}_n}{\lambda n}\mathbf{1}_n $$ or is there another expression ? In general, any ideas on computing these quantities without resorting to auto-differentiation would be welcome.","Given two probability measures and , Cuturi 2013 [1] defines the Sinkhorn distances as: where the cost matrix defines the underlying metric space, and depicts the set of transportation plans: refers to the KL-divergence and controls the regularization. In practice can be computed efficiently by considering the dual problem. The algorithm looks like: Essentially, I would like to minimize energies that involves Sinkhorn distances, such as: This is only one example. Though, to this aim, I would need the gradient of the Sinkhorn distance w.r.t. the empirical measures, e.g. and . Using [2] (Proposition 1 and 2), I have hunch that the gradient w.r.t. would read: where refers to the dual problem that is equivalent to the primal , and the pair is computed through the Sinkhorn matrix scaling applied to . The term on the right hand side is used to normalize the gradient to keep it in the tangent bundle of the simplex. Does that mean that the gradient w.r.t. would be: or is there another expression ? In general, any ideas on computing these quantities without resorting to auto-differentiation would be welcome.","\mathbf{r}\in \Sigma_n \mathbf{c}\in \Sigma_n 
d_{\mathbf{M},\alpha}(\mathbf{r}, \mathbf{c}) = \min_{\mathbf{P} \in U_{\alpha}(\mathbf{r},\mathbf{c})} \langle \mathbf{P}, \mathbf{M} \rangle
 \mathbf{M} U 
U_{\alpha}(\mathbf{r},\mathbf{c}) = \{\mathbf{P}\in U(\mathbf{r},\mathbf{c}) \,:\, D_{\text{KL}} (\mathbf{P} || \mathbf{r}\mathbf{c}^\top) \leq \alpha \}
 D_{\text{KL}} \alpha d(\cdot) 
\arg\min_{\boldsymbol{r}\in \Sigma_n} \sum\limits_{i=1}^k d_{\mathbf{M},\alpha}(\boldsymbol{r}, \mathbf{c}_i)
 \nabla_{\mathbf{r}} d_{\mathbf{M},\alpha} \nabla_{\mathbf{c}} d_{\mathbf{M},\alpha} \mathbf{r} 
\nabla_\mathbf{r} d_{\mathbf{M}}^\lambda = -\frac{\log(\mathbf{u})}{\lambda}+\frac{\log(\mathbf{u})^\top\mathbf{1}_n}{\lambda n}\mathbf{1}_n
 d_{\mathbf{M}}^\lambda d_{\mathbf{M},\alpha} (\mathbf{u}\in\mathbb{R}^n_+,\mathbf{v}\in\mathbb{R}^n_+) K=\exp(-\lambda \mathbf{M}) \mathbf{c} 
\nabla_\mathbf{c} d_{\mathbf{M}}^\lambda = -\frac{\log(\mathbf{v})}{\lambda}+\frac{\log(\mathbf{v})^\top\mathbf{1}_n}{\lambda n}\mathbf{1}_n
","['derivatives', 'optimization', 'duality-theorems', 'subgradient', 'optimal-transport']"
87,Can the derivative of a function that involves $\sqrt{x}$ have 2 answers?,Can the derivative of a function that involves  have 2 answers?,\sqrt{x},"Textbook question- Calculate the gradient of the tangent where $x=1$ for: $f(x)=3x^3+x\sqrt{x}$ . My working- Okay here i used the power rule to determine the derivative, and i got: $f'(x)=9x^2+\frac{3}{2}\sqrt{x}$ . From my equation, $f'(1)=\frac{21}{2}$ .But my textbook answer gave: $f'(1)=\frac{15}{2}$ . Which means that the textbook's equation is $f'(x)=9x^2-\frac{3}{2}\sqrt{x}$ . It somehow turned into minus which i could not understand. So my question is can the $\frac{3}{2}\sqrt{x}$ be ± (plus or minus)? Thanks.","Textbook question- Calculate the gradient of the tangent where for: . My working- Okay here i used the power rule to determine the derivative, and i got: . From my equation, .But my textbook answer gave: . Which means that the textbook's equation is . It somehow turned into minus which i could not understand. So my question is can the be ± (plus or minus)? Thanks.",x=1 f(x)=3x^3+x\sqrt{x} f'(x)=9x^2+\frac{3}{2}\sqrt{x} f'(1)=\frac{21}{2} f'(1)=\frac{15}{2} f'(x)=9x^2-\frac{3}{2}\sqrt{x} \frac{3}{2}\sqrt{x},"['calculus', 'derivatives']"
88,Higher derivatives of Zeta function for $s>1$?,Higher derivatives of Zeta function for ?,s>1,"I'm searching for integral expressions for $\zeta^{(n)} (s)$ for $n \geq 1$ and general $s$ . I have found two useful papers 1 and 2 from which I was able to obtain the following two cases which seem to work great numerically: $$1) \qquad \Re (s)<0$$ Denote $$s_1=1-s$$ Then we have: $$(-1)^n \zeta^{(n)} (s)= \\ = \frac{1}{(2 \pi  i)^{s_1} } \int_0^{\infty } \frac{t^{s_1-1} \left((-1)^{s_1} \left(\log  t-\log (2 \pi )+\frac{\pi  i}{2}\right)^n+\left(\log t-\log (2 \pi )-\frac{\pi  i}{2}\right)^n\right)}{e^t-1} \, dt$$ It follows from a more general formula (2) in paper 1.* $$2) \qquad 0 < \Re (s)<1$$ Then we have: $$\zeta^{(n)} (s)= \\ = \frac{1}{2 \pi  i} \int_0^{\infty } \left(\frac{1}{2 (u+1)}+\log (u)-\psi (u+1)\right) \left(e^{i \pi  s} \left(\log \left(\frac{1}{u}\right)+i \pi \right)^n- \\ -e^{-i \pi  s} \left(\log \left(\frac{1}{u}\right)-i \pi \right)^n\right) \frac{du}{u^s} $$ Which directly follows from formula (1.7) in paper 2. My question is : can we find a similar explicit expression for $\zeta^{(n)} (s)$ with $\Re (s)>1$ ? $^*$ For anyone who doesn't have access to paper 1, here's the original formula, valid for all complex $s$ : $$(-1)^n \zeta^{(n)} (1-s) = \sum_{k=0}^n \binom{n}{k} \left(e^{s z} z^{n-k}+e^{s z^*} (z^*)^{n-k} \right) \left( \Gamma(s) \zeta(s) \right)^{(k)}$$ Where $z=-\log (2 \pi) - \frac{\pi i}{2}$ . A side question: what other ways exist to numerically evaluate these derivatives? How does Mathematica do it, in case you know?","I'm searching for integral expressions for for and general . I have found two useful papers 1 and 2 from which I was able to obtain the following two cases which seem to work great numerically: Denote Then we have: It follows from a more general formula (2) in paper 1.* Then we have: Which directly follows from formula (1.7) in paper 2. My question is : can we find a similar explicit expression for with ? For anyone who doesn't have access to paper 1, here's the original formula, valid for all complex : Where . A side question: what other ways exist to numerically evaluate these derivatives? How does Mathematica do it, in case you know?","\zeta^{(n)} (s) n \geq 1 s 1) \qquad \Re (s)<0 s_1=1-s (-1)^n \zeta^{(n)} (s)= \\ = \frac{1}{(2 \pi  i)^{s_1} } \int_0^{\infty } \frac{t^{s_1-1} \left((-1)^{s_1} \left(\log  t-\log (2 \pi )+\frac{\pi  i}{2}\right)^n+\left(\log t-\log (2 \pi )-\frac{\pi  i}{2}\right)^n\right)}{e^t-1} \, dt 2) \qquad 0 < \Re (s)<1 \zeta^{(n)} (s)= \\ = \frac{1}{2 \pi  i} \int_0^{\infty } \left(\frac{1}{2 (u+1)}+\log (u)-\psi (u+1)\right) \left(e^{i \pi  s} \left(\log \left(\frac{1}{u}\right)+i \pi \right)^n- \\ -e^{-i \pi  s} \left(\log \left(\frac{1}{u}\right)-i \pi \right)^n\right) \frac{du}{u^s}  \zeta^{(n)} (s) \Re (s)>1 ^* s (-1)^n \zeta^{(n)} (1-s) = \sum_{k=0}^n \binom{n}{k} \left(e^{s z} z^{n-k}+e^{s z^*} (z^*)^{n-k} \right) \left( \Gamma(s) \zeta(s) \right)^{(k)} z=-\log (2 \pi) - \frac{\pi i}{2}","['integration', 'derivatives', 'special-functions', 'riemann-zeta']"
89,Differentiating the $W$ lambert function to find optimum angle for maximum horizontal range of a projectile with air resistance,Differentiating the  lambert function to find optimum angle for maximum horizontal range of a projectile with air resistance,W,"Here is the equation I obtained, representing the horizontal range of a projectile in air resistance as a function of its initial release angle- $$x=\frac{V_0\cos(\theta)}{B}-\frac{V_0\cos(\theta)}{B}\,\exp\left(\frac{BV_0\sin(\theta)}{g}\right)\exp\left(W\left(\frac{BV_0\sin(\theta)\exp\left(\frac{BV_0\sin(\theta)}{-g}\right)}{-g}\right)\right), $$ where: $x=$ horizontal distance travelled by projectile $V_0=$ initial velocity of projectile $g=$ acceleration due to gravity $\theta=$ initial release angle I have assumed that the force of air resistance acting on the projectile is equal to a constant k multiplied by the velocity of the projectile. So, $F=kv.$ In my equation, $B= k/m,$ where $m$ is the mass of the projectile. Also, $W(\cdot)$ is the $W$ Lambert function. I have been trying to differentiate the expression with respect to $\theta$ in order find an expression for the value of $\theta$ that would result in maximum horizontal range, assuming all other terms in the equation are constant. How can this be done?","Here is the equation I obtained, representing the horizontal range of a projectile in air resistance as a function of its initial release angle- where: horizontal distance travelled by projectile initial velocity of projectile acceleration due to gravity initial release angle I have assumed that the force of air resistance acting on the projectile is equal to a constant k multiplied by the velocity of the projectile. So, In my equation, where is the mass of the projectile. Also, is the Lambert function. I have been trying to differentiate the expression with respect to in order find an expression for the value of that would result in maximum horizontal range, assuming all other terms in the equation are constant. How can this be done?","x=\frac{V_0\cos(\theta)}{B}-\frac{V_0\cos(\theta)}{B}\,\exp\left(\frac{BV_0\sin(\theta)}{g}\right)\exp\left(W\left(\frac{BV_0\sin(\theta)\exp\left(\frac{BV_0\sin(\theta)}{-g}\right)}{-g}\right)\right),  x= V_0= g= \theta= F=kv. B= k/m, m W(\cdot) W \theta \theta","['derivatives', 'classical-mechanics', 'projectile-motion']"
90,How to obtain the function knowing its higher derivatives at $0$,How to obtain the function knowing its higher derivatives at,0,"does some one knows how to obtain $f(x)$ knowing that in x=0 they have the following value $f^{n}(0)= \frac{1}{n-s}$ if $ n=1,3,5,\cdots$ and $f^{n}(0)=0$ otherwise",does some one knows how to obtain knowing that in x=0 they have the following value if and otherwise,"f(x) f^{n}(0)= \frac{1}{n-s}  n=1,3,5,\cdots f^{n}(0)=0","['derivatives', 'taylor-expansion', 'generating-functions']"
91,Proof of an exponential inequality,Proof of an exponential inequality,,"This seems to be obvious but I am having a hard time proving it. Any insight greatly appreciated. Statement: Prove for every $b,x \in \mathbb{R}$ such that $b\geq 1$ , $|x|\leq b$ , it holds that $$(1+\frac{x}{b})^b \geq e^{x}(1-\frac{x^2}{b}).$$ My attempt: Some cases are trivial: if $x = 0$ it holds with equality, or if $x^2\geq b$ the RHS is negative while the LHS is always positive. For $b = 1$ , I was able to show it by separating $x>0$ and $x<0$ cases. When $b = 1$ and $x>0$ , the function $$h_b(x) = (1+\frac{x}{b})^b-e^{x}(1-\frac{x^2}{b})$$ can be easily shown to be non-decreasing by taking the derivatives and employing $e^{-x} \geq 1-x$ establishes the result. For $b = 1$ and $x<0$ the function is neither decreasing nor increasing so we can't take the derivative. Instead,  I directly worked with $h_b(x)$ and used $e^x\leq 1+x+ \frac{x^2}{2}$ for all $x<0$ to establish the results. When $b>1$ , I observed using plots that for $x>0$ the function is increasing and for when $b\geq2$ the function is decreasing for $x>0$ . So theoretically we can prove the inequality by taking the derivatives. But  for $b >1$ the derivatives are very involved.  I also had a failed attempt using Taylor's inequality .","This seems to be obvious but I am having a hard time proving it. Any insight greatly appreciated. Statement: Prove for every such that , , it holds that My attempt: Some cases are trivial: if it holds with equality, or if the RHS is negative while the LHS is always positive. For , I was able to show it by separating and cases. When and , the function can be easily shown to be non-decreasing by taking the derivatives and employing establishes the result. For and the function is neither decreasing nor increasing so we can't take the derivative. Instead,  I directly worked with and used for all to establish the results. When , I observed using plots that for the function is increasing and for when the function is decreasing for . So theoretically we can prove the inequality by taking the derivatives. But  for the derivatives are very involved.  I also had a failed attempt using Taylor's inequality .","b,x \in \mathbb{R} b\geq 1 |x|\leq b (1+\frac{x}{b})^b \geq e^{x}(1-\frac{x^2}{b}). x = 0 x^2\geq b b = 1 x>0 x<0 b = 1 x>0 h_b(x) = (1+\frac{x}{b})^b-e^{x}(1-\frac{x^2}{b}) e^{-x} \geq 1-x b = 1 x<0 h_b(x) e^x\leq 1+x+ \frac{x^2}{2} x<0 b>1 x>0 b\geq2 x>0 b >1","['derivatives', 'inequality', 'taylor-expansion', 'exponential-function']"
92,On equivalent ways to obtain the modulus of the gradient.,On equivalent ways to obtain the modulus of the gradient.,,Recently I was following a seminar of one of the great living Italian mathematicians: Luigi Ambrosio (recently his student Alessio Figalli won the fields medal). One of his slides was dealing with the Eulerian/Lagrangian duality (a concept I was not so familiar with): I have two questions: Could one provide some intuition as to why these 3 ways of computing the modulus of the gradient are the same and could I have a reference to a proof of this fact? I am interested in all three equivalencies.,Recently I was following a seminar of one of the great living Italian mathematicians: Luigi Ambrosio (recently his student Alessio Figalli won the fields medal). One of his slides was dealing with the Eulerian/Lagrangian duality (a concept I was not so familiar with): I have two questions: Could one provide some intuition as to why these 3 ways of computing the modulus of the gradient are the same and could I have a reference to a proof of this fact? I am interested in all three equivalencies.,,"['real-analysis', 'derivatives', 'differential-geometry']"
93,Differentiating an ODE,Differentiating an ODE,,"Suppose we have the ODE $\dot{u}^2 = au^2 + bu +c $ with real constants $a,b,c$ and initial values $u(0) = \dot{u} (0) = 0$ . Since I know no better way to solve this, I want to differentiate this ODE, so I get $$2\dot{u}\ddot{u} = 2au \dot{u} + b \dot{u}.$$ After cancelling $\dot{u}$ we have a linear second-order ODE which is no problem to solve, but this gives us our required $u$ only up to a constant. Is there a fast / easy way to get this constant? If I plug the linear-ODE-solution into the equation from the beginning, the calculation gets really annoying.","Suppose we have the ODE with real constants and initial values . Since I know no better way to solve this, I want to differentiate this ODE, so I get After cancelling we have a linear second-order ODE which is no problem to solve, but this gives us our required only up to a constant. Is there a fast / easy way to get this constant? If I plug the linear-ODE-solution into the equation from the beginning, the calculation gets really annoying.","\dot{u}^2 = au^2 + bu +c  a,b,c u(0) = \dot{u} (0) = 0 2\dot{u}\ddot{u} = 2au \dot{u} + b \dot{u}. \dot{u} u","['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
94,A function $g(x)$ has one and only one real root if $g'(x)\leq k <0$.,A function  has one and only one real root if .,g(x) g'(x)\leq k <0,"$g : \mathbb{R} \to \mathbb{R}$ is differentiable on $\mathbb{R}$ .  Then $g(x)$ has one and only one real root if $g'(x)\leq k <0$ . Proof attempt: Let us assume the contrary, i.e. $g(x)$ has no real zero at all. Therefore, being continuous, $g(x)$ cannot be both positive and negative on $\mathbb{R}$ . So, firstly we assume that $g(x) <0$ for every $x \in \mathbb{R}$ . We take some $a>0$ . (WLOG, take $a=1$ ). Now, $g(1)/1<0$ and $g(1)/k >0$ . So, $\displaystyle\frac{g(-g(1)/k)-g(1)}{-g(1)/k-1} \leq k <0 \implies -k \leq \displaystyle\frac{g(-g(1)/k)-g(1)}{g(1)/k+1} \implies 0<-k \leq g(-g(1)/k) $ So, we have found at least one  point in the domain, where $g(x)$ is positive. So, $g(x)$ must have a zero. Now, $g'(x)<0, \ \  \forall x \ \in \mathbb{R}$ makes the function one-to-one. [Note that the numerator must be positive, since $1>- g(1)/k \implies g(-g(1)/k)>g(1)$ ] For the assumption that $g(x)>0$ , we consider the points $a<0$ and $-g(a)/k$ (WLOG, take $a=-1$ ). Everything else is kept the same. Are the statement and the proof both correct, or is there any mistake? Please verify.","is differentiable on .  Then has one and only one real root if . Proof attempt: Let us assume the contrary, i.e. has no real zero at all. Therefore, being continuous, cannot be both positive and negative on . So, firstly we assume that for every . We take some . (WLOG, take ). Now, and . So, So, we have found at least one  point in the domain, where is positive. So, must have a zero. Now, makes the function one-to-one. [Note that the numerator must be positive, since ] For the assumption that , we consider the points and (WLOG, take ). Everything else is kept the same. Are the statement and the proof both correct, or is there any mistake? Please verify.","g : \mathbb{R} \to \mathbb{R} \mathbb{R} g(x) g'(x)\leq k <0 g(x) g(x) \mathbb{R} g(x) <0 x \in \mathbb{R} a>0 a=1 g(1)/1<0 g(1)/k >0 \displaystyle\frac{g(-g(1)/k)-g(1)}{-g(1)/k-1} \leq k <0 \implies -k \leq \displaystyle\frac{g(-g(1)/k)-g(1)}{g(1)/k+1} \implies 0<-k \leq g(-g(1)/k)  g(x) g(x) g'(x)<0, \ \  \forall x \ \in \mathbb{R} 1>- g(1)/k \implies g(-g(1)/k)>g(1) g(x)>0 a<0 -g(a)/k a=-1","['real-analysis', 'proof-verification', 'derivatives', 'roots', 'rolles-theorem']"
95,Relationship between derivative of a function at two points and the function itself.,Relationship between derivative of a function at two points and the function itself.,,"Suppose I am given a function $u$ defined in an interval $[a,b]$ . Suppose I know $u'(a)$ , $u'(b)$ and $u((a+b)/2)$ . Is it possible to determine the values of $u(a)$ and $u(b)$ using the given information. Note: In the solution of this exercise it states that $$u(a)=u((a+b)/2)+(a-b)(u'(a)+u'(b))/4$$ and similarly $$u(b)=u((a+b)/2)-(a-b)(u'(a)+u'(b))/4$$ How exactly was this derived? Is there an underlying theorem that was used and I am missing?","Suppose I am given a function defined in an interval . Suppose I know , and . Is it possible to determine the values of and using the given information. Note: In the solution of this exercise it states that and similarly How exactly was this derived? Is there an underlying theorem that was used and I am missing?","u [a,b] u'(a) u'(b) u((a+b)/2) u(a) u(b) u(a)=u((a+b)/2)+(a-b)(u'(a)+u'(b))/4 u(b)=u((a+b)/2)-(a-b)(u'(a)+u'(b))/4","['analysis', 'derivatives']"
96,Chain rule in derivatives,Chain rule in derivatives,,"Well, I have to derive this function: $$f(x)=\sin(2x \sqrt[3]{x+1} )$$ I want to use the chain rule, and I want to use it like this; I will call: $$T=x+1$$ $$Q=2x. \sqrt[3]{t} $$ $$f=\sin (Q)$$ So then I have: $$\cfrac{df}{dx} = \cfrac{df}{dQ} . \cfrac{dQ}{dT} . \cfrac{dT}{dx} . $$ And now I just have to derive. For example, $$\cfrac{df}{dQ} = \cos (Q) $$ and then I put $$2x. \sqrt[3]{t} $$ instead of Q, and so on. But what should I do when I want to do $ \cfrac{dQ}{dT} $ ? Because I have a product, and I know that I have to use the product rule and it would be $$Q'=2x'.\sqrt[3]{t}+2x.(\sqrt[3]{t})'$$ but what shall I do when deriving that X in $(2x)'$ ? Because the derivative is $ \cfrac{dQ}{dT} $ , not $ \cfrac{dQ}{dx} $ I know that I could avoid putting names to these ""sub-functions"", but this way is easier to me, so, please, don't teach me any other method. Thanks!","Well, I have to derive this function: I want to use the chain rule, and I want to use it like this; I will call: So then I have: And now I just have to derive. For example, and then I put instead of Q, and so on. But what should I do when I want to do ? Because I have a product, and I know that I have to use the product rule and it would be but what shall I do when deriving that X in ? Because the derivative is , not I know that I could avoid putting names to these ""sub-functions"", but this way is easier to me, so, please, don't teach me any other method. Thanks!",f(x)=\sin(2x \sqrt[3]{x+1} ) T=x+1 Q=2x. \sqrt[3]{t}  f=\sin (Q) \cfrac{df}{dx} = \cfrac{df}{dQ} . \cfrac{dQ}{dT} . \cfrac{dT}{dx} .  \cfrac{df}{dQ} = \cos (Q)  2x. \sqrt[3]{t}   \cfrac{dQ}{dT}  Q'=2x'.\sqrt[3]{t}+2x.(\sqrt[3]{t})' (2x)'  \cfrac{dQ}{dT}   \cfrac{dQ}{dx} ,"['calculus', 'derivatives', 'chain-rule']"
97,Checking if functions are differentiable,Checking if functions are differentiable,,"We have two functions : $1) \;f(x,y)=\begin{cases} \frac{xy}{x^2-y^2}&\text{when }\;|x|\neq|y| \\0&\text{when }\;|x|=|y| \end{cases}$ $2) \;g(x,y)=\begin{cases} x^2\sin\frac1x+y^2&\text{when }\;(x,y)\in \{\mathbb{R} \setminus{0}\} \times  \mathbb{R} \\y^2&\text{when }\;(x,y)\in\{0\}\times\mathbb{R} \end{cases}$ I want to check if they are differentiable in $(0,0)$ My work so far : $1)$ Let's take sequences : $x_n=\frac1n,\;y_n=\frac2n$ . Then $ f(x_n,y_n)=\dfrac{\frac1n\cdot \frac2n}{\frac{1}{n^2}-\frac{4}{n^2}}=-\dfrac{2}{3}\nrightarrow0.$ So it's not continous at $(0,0)$ so it cannot be differentiable in that point. $2)$ Let's take sequences : $x_n=\frac{1}{\frac{\pi}{2}-\pi n},\;y_n=0$ . Then $$g(x_n,y_n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \sin(\frac{\pi}{2}-\pi n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \cos(n\pi)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot (-1)^n.$$ And the above sequence doesn't has limit to this function is not continuous at (0,0), so it cannot be continuous. Am I thinking correctly ?","We have two functions : I want to check if they are differentiable in My work so far : Let's take sequences : . Then So it's not continous at so it cannot be differentiable in that point. Let's take sequences : . Then And the above sequence doesn't has limit to this function is not continuous at (0,0), so it cannot be continuous. Am I thinking correctly ?","1) \;f(x,y)=\begin{cases}
\frac{xy}{x^2-y^2}&\text{when }\;|x|\neq|y|
\\0&\text{when }\;|x|=|y|
\end{cases} 2) \;g(x,y)=\begin{cases}
x^2\sin\frac1x+y^2&\text{when }\;(x,y)\in \{\mathbb{R} \setminus{0}\} \times  \mathbb{R}
\\y^2&\text{when }\;(x,y)\in\{0\}\times\mathbb{R}
\end{cases} (0,0) 1) x_n=\frac1n,\;y_n=\frac2n  f(x_n,y_n)=\dfrac{\frac1n\cdot \frac2n}{\frac{1}{n^2}-\frac{4}{n^2}}=-\dfrac{2}{3}\nrightarrow0. (0,0) 2) x_n=\frac{1}{\frac{\pi}{2}-\pi n},\;y_n=0 g(x_n,y_n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \sin(\frac{\pi}{2}-\pi n)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot \cos(n\pi)=(\frac{1}{\frac{\pi}{2}-\pi n})^2 \cdot (-1)^n.","['real-analysis', 'calculus', 'derivatives', 'continuity']"
98,Definition of Negative Half Derivative,Definition of Negative Half Derivative,,"I know the definition of positive fractional derivative, which is given by $$D^{\alpha} f(x)=\frac{1}{\Gamma(1-\alpha)} \frac{d}{d x} \int_{0}^{x} \frac{f(t)}{(x-t)^{\alpha}} dt,\quad\quad \alpha\in(0,1)$$ But I encounter the negative half derivative $D^{-1/2}$ , which I am quite confused with. My professor assigned a problem which let us solve the integral equation $$ D^{-1 / 2} h(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} h(s) \mathrm{d} s=g(t),$$ where $g$ is a nice function with $g(0)=0.$ And he gave a hint that lets us square $D^{-1/2}.$ I feel like the first equality is the definition of $D^{-1/2}$ but I am not sure. If it indeed is the definition of $D^{-1/2}$ , can someone tell me the definition of $D^{-\beta}$ for any $\beta \gt0\,?$ Furthermore, can someone give me a further hint about the problem he assigned? My attempt: Squaring $D^{-1/2}$ , we get $$h^{-1}(t)=\int_0^t h(s)\mathrm{d}s=D^{-1/2}g(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} g(s) \mathrm{d} s\,.$$ Differentiating both sides w.r.t $t$ and applying the Leibniz Integral Rule( but ${1\over \sqrt{t-t}}g(t) =\infty$ , and it is invalided to use the Leibniz Integral Rule), we get $$h(t)=\frac{-1}{2\sqrt{\pi}}\int_0^t\frac{1}{(t-s)^{\frac{3}{2}}}g(s)\mathrm{d}s$$ I think it may not be simplified further, but I am not sure. Could the integral in the right-hand-side be calculated further? Any help will be appreciated.","I know the definition of positive fractional derivative, which is given by But I encounter the negative half derivative , which I am quite confused with. My professor assigned a problem which let us solve the integral equation where is a nice function with And he gave a hint that lets us square I feel like the first equality is the definition of but I am not sure. If it indeed is the definition of , can someone tell me the definition of for any Furthermore, can someone give me a further hint about the problem he assigned? My attempt: Squaring , we get Differentiating both sides w.r.t and applying the Leibniz Integral Rule( but , and it is invalided to use the Leibniz Integral Rule), we get I think it may not be simplified further, but I am not sure. Could the integral in the right-hand-side be calculated further? Any help will be appreciated.","D^{\alpha} f(x)=\frac{1}{\Gamma(1-\alpha)} \frac{d}{d x} \int_{0}^{x} \frac{f(t)}{(x-t)^{\alpha}} dt,\quad\quad \alpha\in(0,1) D^{-1/2}  D^{-1 / 2} h(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} h(s) \mathrm{d} s=g(t), g g(0)=0. D^{-1/2}. D^{-1/2} D^{-1/2} D^{-\beta} \beta \gt0\,? D^{-1/2} h^{-1}(t)=\int_0^t h(s)\mathrm{d}s=D^{-1/2}g(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} g(s) \mathrm{d} s\,. t {1\over \sqrt{t-t}}g(t) =\infty h(t)=\frac{-1}{2\sqrt{\pi}}\int_0^t\frac{1}{(t-s)^{\frac{3}{2}}}g(s)\mathrm{d}s","['derivatives', 'integral-equations', 'fractional-calculus']"
99,Strategies for proving continuity and differentiability of trigonometric series,Strategies for proving continuity and differentiability of trigonometric series,,"Let $f$ be a function defined by a series $$f\left(x\right)=\sum c_n e^{inx}.$$ Sometimes, I can prove that the series converges pointwise (when it does), using the Dirichlet test. When the convergence is uniform by the Weierstrass test (i.e., $\sum\left|c_n\right|<\infty$ ), I can show that $f$ is continuous. When the series of derivatives converges uniformly ( $\sum\left|nc_n\right|<\infty$ ), I can show that $f$ is differentiable. However, very often I plot the series, and $f$ looks clearly continuous and/or $C^1$ (sometimes piecewise, sometimes globally), but I can't prove it. What are some other tools, besides these basic theorems/tests I mentioned above? Context: I want to show that $$f\left(x\right) = \sum_{n\geqslant 2} \frac{1-\cos nx}{n^2 \log n}$$ is $C^1$ . It's enough to define $$g\left(x\right) = \sum_{n\geqslant 2} \frac{\sin nx}{n \log n},$$ show that this series converges (easy: Dirichlet test), $g$ is continuous (hard), and then integrate it (easy again). When I plot the graph, this series looks like it's uniformly convergent, but I have no idea how to show this. I would by satisfied with a proof that it is continuous at $x=0$ , and that it conveges uniformly on any compact set away from zero. I can't show either of these things. My only clue is: I know that $\sum \left(\sin nx\right)/n$ is the sawtooth, which is discontinuous at zero, but with limits at the right and at the left. Multiplying the Fourier coefficients by $1/\log n$ looks like ""smoothing"", like a convolution, so it should be continuous indeed. How do I show this?","Let be a function defined by a series Sometimes, I can prove that the series converges pointwise (when it does), using the Dirichlet test. When the convergence is uniform by the Weierstrass test (i.e., ), I can show that is continuous. When the series of derivatives converges uniformly ( ), I can show that is differentiable. However, very often I plot the series, and looks clearly continuous and/or (sometimes piecewise, sometimes globally), but I can't prove it. What are some other tools, besides these basic theorems/tests I mentioned above? Context: I want to show that is . It's enough to define show that this series converges (easy: Dirichlet test), is continuous (hard), and then integrate it (easy again). When I plot the graph, this series looks like it's uniformly convergent, but I have no idea how to show this. I would by satisfied with a proof that it is continuous at , and that it conveges uniformly on any compact set away from zero. I can't show either of these things. My only clue is: I know that is the sawtooth, which is discontinuous at zero, but with limits at the right and at the left. Multiplying the Fourier coefficients by looks like ""smoothing"", like a convolution, so it should be continuous indeed. How do I show this?","f f\left(x\right)=\sum c_n e^{inx}. \sum\left|c_n\right|<\infty f \sum\left|nc_n\right|<\infty f f C^1 f\left(x\right) = \sum_{n\geqslant 2} \frac{1-\cos nx}{n^2 \log n} C^1 g\left(x\right) = \sum_{n\geqslant 2} \frac{\sin nx}{n \log n}, g x=0 \sum \left(\sin nx\right)/n 1/\log n","['derivatives', 'convergence-divergence', 'fourier-analysis', 'fourier-series', 'uniform-convergence']"
