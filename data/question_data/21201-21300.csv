,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the difference between a 2-dimensional plane in $R^3$ and $R^2$?,What is the difference between a 2-dimensional plane in  and ?,R^3 R^2,A $3\times 3$ matrix with $2$ independent vectors will span a $2$ dimensional plane in $\Bbb R^3$ but that plane is not $\Bbb R^2$. Is it just nomenclature or does $\Bbb R^2$ have some additional properties that other planes don't?,A $3\times 3$ matrix with $2$ independent vectors will span a $2$ dimensional plane in $\Bbb R^3$ but that plane is not $\Bbb R^2$. Is it just nomenclature or does $\Bbb R^2$ have some additional properties that other planes don't?,,['linear-algebra']
1,How many degrees of freedom would a rotation matrix in R5 have?,How many degrees of freedom would a rotation matrix in R5 have?,,I understand that a rotation matrix in R3 has three degrees of freedom because there is three linearly independent planes that the rotation can take place in. How does this translate to R5?,I understand that a rotation matrix in R3 has three degrees of freedom because there is three linearly independent planes that the rotation can take place in. How does this translate to R5?,,"['linear-algebra', 'rotations']"
2,"If $x^2 +px +1$ is a factor of $ ax^3 +bx+c$ then relate $a,b,c$",If  is a factor of  then relate,"x^2 +px +1  ax^3 +bx+c a,b,c","Suppose If $x^2 +px +1$ is a factor of $ax^3 +bx+c$ then relate $a,b,c$ such that $a,b,c \in R$ I can write $$ax^3 +bx+c=(x^2 +px +1)(\lambda x +D)$$ $$\implies ax^3 +bx+c =\lambda x^3 + x^2.p\lambda + x(\lambda+pD)+D $$ and then compare coefficient to find out relation but that will be long and tedious process , I want shorter approach to this problem . Btw I was given following options for this question A) $a^2+c^2+ab=0$ B) $a^2-c^2+ab=0$ C) $a^2-c^2-ab=0$ D) $ap^2+bp+c=0$ Maybe we can relate something by looking at options?","Suppose If is a factor of then relate such that I can write and then compare coefficient to find out relation but that will be long and tedious process , I want shorter approach to this problem . Btw I was given following options for this question A) B) C) D) Maybe we can relate something by looking at options?","x^2 +px +1 ax^3 +bx+c a,b,c a,b,c \in R ax^3 +bx+c=(x^2 +px +1)(\lambda x +D) \implies ax^3 +bx+c =\lambda x^3 + x^2.p\lambda + x(\lambda+pD)+D  a^2+c^2+ab=0 a^2-c^2+ab=0 a^2-c^2-ab=0 ap^2+bp+c=0","['linear-algebra', 'algebra-precalculus', 'number-theory']"
3,Proof of Cauchy–Schwarz inequality,Proof of Cauchy–Schwarz inequality,,"I was reading about the Cauchy–Schwarz inequality from Courant, Hilbert - Methods Of Mathematical Physics Vol 1 and I can not understand what they mean when they said the line that has been highlighted with red in the picture given below I can not understand why a and b has to be proportional and why is this so crucial for the roots to be imaginary and why we want the roots to be imaginary in the first place.","I was reading about the Cauchy–Schwarz inequality from Courant, Hilbert - Methods Of Mathematical Physics Vol 1 and I can not understand what they mean when they said the line that has been highlighted with red in the picture given below I can not understand why a and b has to be proportional and why is this so crucial for the roots to be imaginary and why we want the roots to be imaginary in the first place.",,"['linear-algebra', 'inequality', 'triangles', 'cauchy-schwarz-inequality']"
4,Linear transformation of a polygon maximizing its area with respect to its perimeter.,Linear transformation of a polygon maximizing its area with respect to its perimeter.,,"Given a polygon $P$ on the plane, is there a rigorous method or algorithm to compute or approximate a linear transformation $T$ which maximizes the following ratio? $$\frac{\mathrm{Area}[T(P)]}{\mathrm{Perimeter}[T(P)]^2}$$","Given a polygon $P$ on the plane, is there a rigorous method or algorithm to compute or approximate a linear transformation $T$ which maximizes the following ratio? $$\frac{\mathrm{Area}[T(P)]}{\mathrm{Perimeter}[T(P)]^2}$$",,['linear-algebra']
5,A proof of Cartan-Dieudonné's theorem,A proof of Cartan-Dieudonné's theorem,,"As an assignment, we were asked to prove a theorem by Cartan and Dieudonné in the following form (the Euclidean space $\mathbb{E}^n$ is meant to be the usual $\mathbb{R}^n$ endowed with the usual inner product): Every isometry of the Euclidean space $\mathbb{E}^n$ is the product of (at most) $n$ reflections about hyperplanes. I proved it in the following way and I'd like to know whether it's a valid proof or not. I'd also like to know about other proofs (I'm sure there are...) : We use induction on $n$. Let's assume the proposition for $n-1$. Let $\phi: \mathbb{E}^n\rightarrow \mathbb{E}^n$ be an isometry $\neq id$, and $\{e_1,\ldots, e_n\}$ be the canonical basis of $\mathbb{E}^n$. Consider the reflection $\sigma$ about the hyperplane orthogonal to the vector $e_1-\phi(e_1)$, which we assume non-zero, without loss of generality since $\phi\neq id$. Clearly $\sigma(e_1)=\phi(e_1)$, so $e_1$ is ok. Since $\{\phi(e_2),\ldots, \phi(e_n)\}$ and $\{\sigma(e_2),\ldots, \sigma(e_n)\}$ are orthormal bases of the subspace $\langle \sigma(e_1)\rangle ^{\perp}$, there is an isometry $\psi: \mathbb{E}^n\rightarrow \mathbb{E}^n$ such that $\psi(\sigma(e_1))=\sigma(e_1)$ and $\psi(\sigma(e_j))=\phi(e_j)$, ($j\neq 1$). Since $\psi$ induces an isometry on $\langle \sigma(e_1)\rangle ^{\perp}$, by the inductive hypothesis, it can be written as a product of reflections about subspaces of $\langle \sigma(e_1)\rangle ^{\perp}$ and dimension $n-2$. Each of these subspaces can be extended to an hyperplane of $\mathbb{E}^n$ so that $\sigma(e_1)$ belongs to that and hence remains unchanged under those reflections. $\square$","As an assignment, we were asked to prove a theorem by Cartan and Dieudonné in the following form (the Euclidean space $\mathbb{E}^n$ is meant to be the usual $\mathbb{R}^n$ endowed with the usual inner product): Every isometry of the Euclidean space $\mathbb{E}^n$ is the product of (at most) $n$ reflections about hyperplanes. I proved it in the following way and I'd like to know whether it's a valid proof or not. I'd also like to know about other proofs (I'm sure there are...) : We use induction on $n$. Let's assume the proposition for $n-1$. Let $\phi: \mathbb{E}^n\rightarrow \mathbb{E}^n$ be an isometry $\neq id$, and $\{e_1,\ldots, e_n\}$ be the canonical basis of $\mathbb{E}^n$. Consider the reflection $\sigma$ about the hyperplane orthogonal to the vector $e_1-\phi(e_1)$, which we assume non-zero, without loss of generality since $\phi\neq id$. Clearly $\sigma(e_1)=\phi(e_1)$, so $e_1$ is ok. Since $\{\phi(e_2),\ldots, \phi(e_n)\}$ and $\{\sigma(e_2),\ldots, \sigma(e_n)\}$ are orthormal bases of the subspace $\langle \sigma(e_1)\rangle ^{\perp}$, there is an isometry $\psi: \mathbb{E}^n\rightarrow \mathbb{E}^n$ such that $\psi(\sigma(e_1))=\sigma(e_1)$ and $\psi(\sigma(e_j))=\phi(e_j)$, ($j\neq 1$). Since $\psi$ induces an isometry on $\langle \sigma(e_1)\rangle ^{\perp}$, by the inductive hypothesis, it can be written as a product of reflections about subspaces of $\langle \sigma(e_1)\rangle ^{\perp}$ and dimension $n-2$. Each of these subspaces can be extended to an hyperplane of $\mathbb{E}^n$ so that $\sigma(e_1)$ belongs to that and hence remains unchanged under those reflections. $\square$",,"['linear-algebra', 'geometry']"
6,Accessible Intro to Random Matrix Theory (RMT),Accessible Intro to Random Matrix Theory (RMT),,I read this fascinating article: http://www.newscientist.com/article/mg20627550.200-enter-the-matrix-the-deep-law-that-shapes-our-reality.html Unfortunately all the other papers I find googling are just not tangible to me :-( Could anyone please point me to some material that bridges the gap from this popular science exposition to the hard core papers that seem to pile up in the Net?,I read this fascinating article: http://www.newscientist.com/article/mg20627550.200-enter-the-matrix-the-deep-law-that-shapes-our-reality.html Unfortunately all the other papers I find googling are just not tangible to me :-( Could anyone please point me to some material that bridges the gap from this popular science exposition to the hard core papers that seem to pile up in the Net?,,"['linear-algebra', 'random']"
7,Is it possible to prove that this matrix is invertible?,Is it possible to prove that this matrix is invertible?,,"I am trying to determine whether the following, $ n $ by $ n $ matrix: $$ A = \begin{pmatrix} \Delta_{11} - 1 & \Delta_{12} & ... & \Delta_{1n} \\ \Delta_{21} & \Delta_{22} - 1 & ... & \Delta_{2n} \\ ... \\ \Delta_{n1} & \Delta_{n2} & ... & \Delta_{nn} - 1  \end{pmatrix} $$ Subject to $ \Delta_{ij} \in [0,1] $ and: $$ 0 < \sum_{j=1}^{n}\Delta_{ij} < 1, \text{ } \forall i $$ Is invertible in general or not. I started out by noticing that this is equivalent to saying that the matrix $ \Delta = A + I $ does not have an eigenvalue equal to one. I was hoping that, since $ \Delta $ is ""almost a Markov matrix"" (i.e. the rows do not add up to one, but all the other properties are there), and we know that Markov matrices have an eigenvalue equal to one, then maybe I can squeeze out a proof that such an ""almost Markov matrix"" does not have an eigenvalue equal to one - but so far I've been unsuccessful.","I am trying to determine whether the following, by matrix: Subject to and: Is invertible in general or not. I started out by noticing that this is equivalent to saying that the matrix does not have an eigenvalue equal to one. I was hoping that, since is ""almost a Markov matrix"" (i.e. the rows do not add up to one, but all the other properties are there), and we know that Markov matrices have an eigenvalue equal to one, then maybe I can squeeze out a proof that such an ""almost Markov matrix"" does not have an eigenvalue equal to one - but so far I've been unsuccessful."," n   n   A = \begin{pmatrix} \Delta_{11} - 1 & \Delta_{12} & ... & \Delta_{1n} \\
\Delta_{21} & \Delta_{22} - 1 & ... & \Delta_{2n} \\
... \\ \Delta_{n1} & \Delta_{n2} & ... & \Delta_{nn} - 1
 \end{pmatrix}   \Delta_{ij} \in [0,1]   0 < \sum_{j=1}^{n}\Delta_{ij} < 1, \text{ } \forall i   \Delta = A + I   \Delta ","['linear-algebra', 'matrices']"
8,Which vectors have unique representation when it isn't a direct sum?,Which vectors have unique representation when it isn't a direct sum?,,"I know that $V = U \oplus W$ means that every $v \in V$ can be written uniquely as $v = u + w$ for some $u \in U, w \in W$ . However, what happens if $V = U + W$ is not direct? Then this means that some vector $v$ does not have a unique representation. But can we say exactly which vectors have a unique representation, and which don't? Is this even a useful question? Intuitively, I am thinking ""sometimes we don't have a direct sum, but perhaps we can still work with what we've got. In particular, let's see which vectors we can write uniquely, and maybe we can work with those.""","I know that means that every can be written uniquely as for some . However, what happens if is not direct? Then this means that some vector does not have a unique representation. But can we say exactly which vectors have a unique representation, and which don't? Is this even a useful question? Intuitively, I am thinking ""sometimes we don't have a direct sum, but perhaps we can still work with what we've got. In particular, let's see which vectors we can write uniquely, and maybe we can work with those.""","V = U \oplus W v \in V v = u + w u \in U, w \in W V = U + W v",['linear-algebra']
9,Sylvester's criteria and Negative definite Matrices.,Sylvester's criteria and Negative definite Matrices.,,A)Sylvester's criterion states that a Hermitian matrix M is positive-definite if and only if all leading principal minors are positive. AA) a Hermitian matrix M is negative-definite if and only if all leading principal minors are negative. B)a Hermitian matrix M is positive-semidefinite if and only if all principal minors of M are nonnegative. BB) a Hermitian matrix M is negative-semidefinite if and only if all principal minors of M are nonpositive. Now My question is the following:: 1) Can AA) be deduced from A) ? Or vice versa.. 2) Can BB ) be deduced from B) ?  Or vice vera... My Thoughts: I think they can be as if $A$ is positive definite or positive semi definite then $-A$ will be negative definite or negative semi definite. So AA)[BB) ] can be deduced from  A)[BB) ]. Edit I wanted to ask if AA)[BB) ] can be deduced from A) [B) ]???? Basically I wanted to know if Sylvester's law is useful to determine whether a Matrix is negative-semidefinite or negative definite?? I am sorry..PLease edit your answer accordingly.. Can anyone please correct me if I went wrong anywhere?? Thank You.,A)Sylvester's criterion states that a Hermitian matrix M is positive-definite if and only if all leading principal minors are positive. AA) a Hermitian matrix M is negative-definite if and only if all leading principal minors are negative. B)a Hermitian matrix M is positive-semidefinite if and only if all principal minors of M are nonnegative. BB) a Hermitian matrix M is negative-semidefinite if and only if all principal minors of M are nonpositive. Now My question is the following:: 1) Can AA) be deduced from A) ? Or vice versa.. 2) Can BB ) be deduced from B) ?  Or vice vera... My Thoughts: I think they can be as if $A$ is positive definite or positive semi definite then $-A$ will be negative definite or negative semi definite. So AA)[BB) ] can be deduced from  A)[BB) ]. Edit I wanted to ask if AA)[BB) ] can be deduced from A) [B) ]???? Basically I wanted to know if Sylvester's law is useful to determine whether a Matrix is negative-semidefinite or negative definite?? I am sorry..PLease edit your answer accordingly.. Can anyone please correct me if I went wrong anywhere?? Thank You.,,"['linear-algebra', 'matrices', 'quadratic-forms']"
10,Diagonalizability of elements of finite subgroups of general linear group over an algebraically closed field,Diagonalizability of elements of finite subgroups of general linear group over an algebraically closed field,,"How to show that every element of $G$, where $G$ is a finite subgroup of $GL_n(\mathbb{k})$, the general linear group of square matrices of order $n$ over some algebraic closed field $\mathbb{k}$, is diagnonalizable if $\mathbb{k}$ is an algebraic closure of $\mathbb{Q}$? I know that a matrix is diagonalizable if its minimal polynomial is seperable in the field on which the matrix is defined. Now, what if the minimal polynomial has repeated roots? How do we ensure the diginalizability? Any hints. Thanks beforehand.","How to show that every element of $G$, where $G$ is a finite subgroup of $GL_n(\mathbb{k})$, the general linear group of square matrices of order $n$ over some algebraic closed field $\mathbb{k}$, is diagnonalizable if $\mathbb{k}$ is an algebraic closure of $\mathbb{Q}$? I know that a matrix is diagonalizable if its minimal polynomial is seperable in the field on which the matrix is defined. Now, what if the minimal polynomial has repeated roots? How do we ensure the diginalizability? Any hints. Thanks beforehand.",,"['linear-algebra', 'linear-groups']"
11,The meaning behind $(X^TX)^{-1}$,The meaning behind,(X^TX)^{-1},"In linear algebra, we learn that the inverse of a matrix ""undoes"" the linear transformation.  What exactly is the meaning of the inverse of $(X^TX)^{-1}$? $X^TX$ we know as being a square matrix whose diagonal elements are the sums of squares.  So what are we doing when we take the inverse of this?  I have always used this property in my calculations but would like to understand more of the meaning behind it.","In linear algebra, we learn that the inverse of a matrix ""undoes"" the linear transformation.  What exactly is the meaning of the inverse of $(X^TX)^{-1}$? $X^TX$ we know as being a square matrix whose diagonal elements are the sums of squares.  So what are we doing when we take the inverse of this?  I have always used this property in my calculations but would like to understand more of the meaning behind it.",,"['linear-algebra', 'statistics', 'regression']"
12,"Finding $\det(I+A^{100})$ where $A\in M_3(R)$ and eigenvalues of $A$ are $\{-1,0,1\}$",Finding  where  and eigenvalues of  are,"\det(I+A^{100}) A\in M_3(R) A \{-1,0,1\}","I have a matrix $A \in M_3(R)$ and it is known that $\sigma (A)=\{-1, 0, 1\}$, where $\sigma (A)$ is a set of eigenvalues of matrix $A$. I am now supposed to calculate $\det(I + A^{100})$. I know that $A^{100}$ could be calculated using a diagonal matrix which has the eigenvalues of $A$ on it's diagonal and using matrices which are formed using the eigenvectors of $A$, but I am not sure how to get there. Or it might not even be the right approach. I know there is a similar question, but I don't really understand the answer given there. It's not fully explained. So if anyone could help, that would be great. Thanks","I have a matrix $A \in M_3(R)$ and it is known that $\sigma (A)=\{-1, 0, 1\}$, where $\sigma (A)$ is a set of eigenvalues of matrix $A$. I am now supposed to calculate $\det(I + A^{100})$. I know that $A^{100}$ could be calculated using a diagonal matrix which has the eigenvalues of $A$ on it's diagonal and using matrices which are formed using the eigenvectors of $A$, but I am not sure how to get there. Or it might not even be the right approach. I know there is a similar question, but I don't really understand the answer given there. It's not fully explained. So if anyone could help, that would be great. Thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
13,Let $A$ be an $n \times n$ matrix with real entries such that $A^2 + I = 0$ then $n$ is even.,Let  be an  matrix with real entries such that  then  is even.,A n \times n A^2 + I = 0 n,"Let $A$ be an $n \times n$ matrix with real entries such that $A^2 + I = 0$ then $n$ is even. And if $n = 2k$, then $A$ is similar over the field of real numbers to a matrix of the block form $$\begin{bmatrix}     0      & -I \\     I & 0 \\ \end{bmatrix}$$  where $I$ is the $k \times k$ identity matrix. I have done the first part. Since $\det(A^2) = \det (-I)$ we must have $\det (-I)$ non negative and hence $n$ must be even. Need Help in the second part. Thank You.","Let $A$ be an $n \times n$ matrix with real entries such that $A^2 + I = 0$ then $n$ is even. And if $n = 2k$, then $A$ is similar over the field of real numbers to a matrix of the block form $$\begin{bmatrix}     0      & -I \\     I & 0 \\ \end{bmatrix}$$  where $I$ is the $k \times k$ identity matrix. I have done the first part. Since $\det(A^2) = \det (-I)$ we must have $\det (-I)$ non negative and hence $n$ must be even. Need Help in the second part. Thank You.",,"['linear-algebra', 'cyclic-groups', 'jordan-normal-form', 'cyclic-decomposition']"
14,Can the same vector be an eigenvector of both $A$ and $A^T$?,Can the same vector be an eigenvector of both  and ?,A A^T,"It is proven that $A$ and $A^T$ have the same eigenvalues. I want to study what stands for eigenvectors. Let me make a try. Given: $$Ax=\lambda x$$ we know that $x\in C(A)$ for $\lambda \neq 0$. Suppose that for $A^T$ we have the same eigenvectors $x$: $$A^Tx=\lambda x$$ but now we have that $x\in C(A^T)$. Based on this, eigenvector's $x$ belong both in column and row space which is impossible. So, $A$ and $A^T$ have different eigenvectors. Am I right about this deduction? In any case, could you please suggest a different way if possible? Thanks. PS: After @G Tony Jacobs comments I made some changes hopping that I have less mistakes.","It is proven that $A$ and $A^T$ have the same eigenvalues. I want to study what stands for eigenvectors. Let me make a try. Given: $$Ax=\lambda x$$ we know that $x\in C(A)$ for $\lambda \neq 0$. Suppose that for $A^T$ we have the same eigenvectors $x$: $$A^Tx=\lambda x$$ but now we have that $x\in C(A^T)$. Based on this, eigenvector's $x$ belong both in column and row space which is impossible. So, $A$ and $A^T$ have different eigenvectors. Am I right about this deduction? In any case, could you please suggest a different way if possible? Thanks. PS: After @G Tony Jacobs comments I made some changes hopping that I have less mistakes.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'transpose']"
15,Is there no difference between upper triangular matrix and echelon matrix(row echelon matrix)?,Is there no difference between upper triangular matrix and echelon matrix(row echelon matrix)?,,Source: Linear Algebra with Applications Gareth Williams I see no difference between upper triangular matrix and echelon matrix(row echelon matrix). Then are they the same? Source: Linear Algebra with Applications David C. Lay,Source: Linear Algebra with Applications Gareth Williams I see no difference between upper triangular matrix and echelon matrix(row echelon matrix). Then are they the same? Source: Linear Algebra with Applications David C. Lay,,"['linear-algebra', 'matrices']"
16,What is the span of an infinite set?,What is the span of an infinite set?,,"If a subset $W$ of a vector space $V$ is a subspace of $V$, I want to show that $\operatorname{span}(W)=W$. But is it possible to define $\operatorname{span}(W)$? $W$ can either be a finite or infinite set and according to what I know $\operatorname{span}(W)$ can only be defined on a finite set $W$. Is there any problem with my argument?","If a subset $W$ of a vector space $V$ is a subspace of $V$, I want to show that $\operatorname{span}(W)=W$. But is it possible to define $\operatorname{span}(W)$? $W$ can either be a finite or infinite set and according to what I know $\operatorname{span}(W)$ can only be defined on a finite set $W$. Is there any problem with my argument?",,"['linear-algebra', 'definition']"
17,Is the scalar multiple of an eigenvector also an eigenvector for a particular eigenvalue?,Is the scalar multiple of an eigenvector also an eigenvector for a particular eigenvalue?,,"I'm working on a problem from my textbook and found that $\left(\frac{1}{2}, \frac{1}{2}, 1\right)$ is an eigenvector for a particular eigenvalue of $4$. The textbook solution says that the answer is $(1, 1, 2)$ which is just $2 \times \left(\frac{1}{2}, \frac{1}{2}, 1\right)$","I'm working on a problem from my textbook and found that $\left(\frac{1}{2}, \frac{1}{2}, 1\right)$ is an eigenvector for a particular eigenvalue of $4$. The textbook solution says that the answer is $(1, 1, 2)$ which is just $2 \times \left(\frac{1}{2}, \frac{1}{2}, 1\right)$",,"['linear-algebra', 'eigenvalues-eigenvectors']"
18,Over the reals the intersection of the orthogonal and symplectic groups in even dimension is isomorphic to the unitary group in the half dimension.,Over the reals the intersection of the orthogonal and symplectic groups in even dimension is isomorphic to the unitary group in the half dimension.,,"See the answer here . Over the reals the intersection of the orthogonal and symplectic groups in even dimension is isomorphic   to the unitary group in the half dimension:$$ U(n) = O(2n, \mathbf{R}) \cap Sp(2n, \mathbf{R}).$$ To me, this is not so obvious. How do we see this without just saying ""oh, that's trivial, cite the ""2-out-of-3 property""?","See the answer here . Over the reals the intersection of the orthogonal and symplectic groups in even dimension is isomorphic   to the unitary group in the half dimension:$$ U(n) = O(2n, \mathbf{R}) \cap Sp(2n, \mathbf{R}).$$ To me, this is not so obvious. How do we see this without just saying ""oh, that's trivial, cite the ""2-out-of-3 property""?",,"['linear-algebra', 'abstract-algebra']"
19,Largest singular value of non square matrix,Largest singular value of non square matrix,,"Let $B$ be an $m\times n$ matrix with complex number as its element. Let $\sigma$ denotes the largest singular value of $B$ Prove that  \begin{equation} \sigma = \max\limits_{\|u\|_2=1,\|v\|_2=1} |u^*Bv|. \end{equation} My partial solution: Let $x$ be the eigenvector of $B^*B$ that corespond to eigen value $\sigma^2$. Define $z=\frac{x}{\|x\|_2}$ and $w=\frac{1}{\sigma} Bz$. Note that $\|z\|_2=1$ and $\|w\|_2=\sqrt{w^*w}=\sqrt{\frac{1}{\sigma^2} z^*B^*Bz}=\frac{1}{\sigma}\sqrt{z^*\sigma^2 z}=1$. Thus \begin{align*} \sigma=\frac{1}{\sigma} z^*\sigma^2z=\frac{1}{\sigma} z^* B^*Bz=w^*Bz\leq \max\limits_{\|u\|_2=1,\|v\|_2=1} |u^*Bv|. \end{align*} My question is that how to prove the reverse inequality. If $m=n$, then I can solve the problem by using the Rayleigh-Ritz theorem for $B^*B$. But, I have no idea for $m\neq n$. Thank you.","Let $B$ be an $m\times n$ matrix with complex number as its element. Let $\sigma$ denotes the largest singular value of $B$ Prove that  \begin{equation} \sigma = \max\limits_{\|u\|_2=1,\|v\|_2=1} |u^*Bv|. \end{equation} My partial solution: Let $x$ be the eigenvector of $B^*B$ that corespond to eigen value $\sigma^2$. Define $z=\frac{x}{\|x\|_2}$ and $w=\frac{1}{\sigma} Bz$. Note that $\|z\|_2=1$ and $\|w\|_2=\sqrt{w^*w}=\sqrt{\frac{1}{\sigma^2} z^*B^*Bz}=\frac{1}{\sigma}\sqrt{z^*\sigma^2 z}=1$. Thus \begin{align*} \sigma=\frac{1}{\sigma} z^*\sigma^2z=\frac{1}{\sigma} z^* B^*Bz=w^*Bz\leq \max\limits_{\|u\|_2=1,\|v\|_2=1} |u^*Bv|. \end{align*} My question is that how to prove the reverse inequality. If $m=n$, then I can solve the problem by using the Rayleigh-Ritz theorem for $B^*B$. But, I have no idea for $m\neq n$. Thank you.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
20,Diagonalizable matrices A and B with $\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k)$ have the same characteristic polynomial? [duplicate],Diagonalizable matrices A and B with  have the same characteristic polynomial? [duplicate],\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k),"This question already has answers here : If $ \operatorname{Tr}(M^k) = \operatorname{Tr}(N^k)\;\forall 1\leq k \leq n$, then how do we show the $M$ and $N$ have the same eigenvalues? (2 answers) Closed 9 years ago . Let $A$ and $B$ be $n \times n$ matrices with entries in a field F. Suppose $A$ and $B$ are diagonalizable in some extension field E of F and that $\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k)$ for all integers $k>0$. Show that A and $B$ have the same characteristic polynomial. $A$ and $B$ are diagonalizable over the field E, so there are invertible matrices $R$ and $S$, and diagonal matrices $D_A$ and $D_B$ with entries in E such that $RAR^{-1}=D_A$ and $SBS^{-1}=D_B$ Similar Matrices have the same characteristic polynomial, so if $P_X(\lambda)$ denotes the characteristic polynomial of a matrix X in variable $\lambda$, $P_A(\lambda)=P_{D_A}(\lambda)$ and $P_B(\lambda)=P_{D_B}(\lambda)$. We also have $\mathrm{Tr}(D_A^k)=\mathrm{Tr}((RAR^{-1})^k)=\mathrm{Tr}(RA^kR^{-1})=\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k)=\mathrm{Tr}(D_B^k)$ for all integers $k>0$. I tried using the Cayley-Hamilton theorem on $P_{D_A}(\lambda)$ and taking the trace, but couldn't get anything out of it.","This question already has answers here : If $ \operatorname{Tr}(M^k) = \operatorname{Tr}(N^k)\;\forall 1\leq k \leq n$, then how do we show the $M$ and $N$ have the same eigenvalues? (2 answers) Closed 9 years ago . Let $A$ and $B$ be $n \times n$ matrices with entries in a field F. Suppose $A$ and $B$ are diagonalizable in some extension field E of F and that $\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k)$ for all integers $k>0$. Show that A and $B$ have the same characteristic polynomial. $A$ and $B$ are diagonalizable over the field E, so there are invertible matrices $R$ and $S$, and diagonal matrices $D_A$ and $D_B$ with entries in E such that $RAR^{-1}=D_A$ and $SBS^{-1}=D_B$ Similar Matrices have the same characteristic polynomial, so if $P_X(\lambda)$ denotes the characteristic polynomial of a matrix X in variable $\lambda$, $P_A(\lambda)=P_{D_A}(\lambda)$ and $P_B(\lambda)=P_{D_B}(\lambda)$. We also have $\mathrm{Tr}(D_A^k)=\mathrm{Tr}((RAR^{-1})^k)=\mathrm{Tr}(RA^kR^{-1})=\mathrm{Tr}(A^k)=\mathrm{Tr}(B^k)=\mathrm{Tr}(D_B^k)$ for all integers $k>0$. I tried using the Cayley-Hamilton theorem on $P_{D_A}(\lambda)$ and taking the trace, but couldn't get anything out of it.",,"['linear-algebra', 'matrices']"
21,Lower bounds for inner product $x^\top y$,Lower bounds for inner product,x^\top y,Cauchy-Schwarz provides an upper bound for the inner product $x^\top y$. Are there theorems that talk about lower bounds for this quantity? Assume $x\ge 0$ and $y\ge 0$ wlog.,Cauchy-Schwarz provides an upper bound for the inner product $x^\top y$. Are there theorems that talk about lower bounds for this quantity? Assume $x\ge 0$ and $y\ge 0$ wlog.,,"['linear-algebra', 'matrices', 'inequality']"
22,What does it mean when all the values of a row in a matrix are 0?,What does it mean when all the values of a row in a matrix are 0?,,"I'm fairly new to linear algebra and I'm trying to make sense of what I'm being taught in class. I'm a little confused as to what happens when all the values of a matrix's row are equal to $0$ : does this mean that the matrix has infinite solutions, no solutions, or something completely different?","I'm fairly new to linear algebra and I'm trying to make sense of what I'm being taught in class. I'm a little confused as to what happens when all the values of a matrix's row are equal to : does this mean that the matrix has infinite solutions, no solutions, or something completely different?",0,"['linear-algebra', 'matrices']"
23,Inverse of orthogonal projection,Inverse of orthogonal projection,,"I have an $N \times N$ orthogonal projection matrix $P = A^H(AA^H)^{-1}A$ that I'm trying to find the inverse for. I'm using matlab, however, I keep getting the warning ""the matrix is close to singular or badly scaled"". Now I'm wondering if it's even invertible. Are orthogonal projection matrices invertible and if so, is there any stable method for computing the inverse?","I have an $N \times N$ orthogonal projection matrix $P = A^H(AA^H)^{-1}A$ that I'm trying to find the inverse for. I'm using matlab, however, I keep getting the warning ""the matrix is close to singular or badly scaled"". Now I'm wondering if it's even invertible. Are orthogonal projection matrices invertible and if so, is there any stable method for computing the inverse?",,"['linear-algebra', 'matrices', 'inverse']"
24,"Book suggestion for linear algebra ""2""","Book suggestion for linear algebra ""2""",,"I am almost finishing Gilbert Strang's book ""An introduction to linear algebra"" (plus video lectures at MIT OCW). First and foremost, I would like to suggest this course for everyone. It has been incredibly illuminating. I would like to continue studying linear algebra, with particular focus on different properties of matrices and the transition to more general linear spaces (I am a physicist so Hilbert spaces and etc. are of particular interest). Does anyone have a a good recommendation of books/resources/etc.?","I am almost finishing Gilbert Strang's book ""An introduction to linear algebra"" (plus video lectures at MIT OCW). First and foremost, I would like to suggest this course for everyone. It has been incredibly illuminating. I would like to continue studying linear algebra, with particular focus on different properties of matrices and the transition to more general linear spaces (I am a physicist so Hilbert spaces and etc. are of particular interest). Does anyone have a a good recommendation of books/resources/etc.?",,"['linear-algebra', 'reference-request', 'soft-question']"
25,"If $\{u,v,w\}$ is a basis for $V$, then $\{u+v+w,v+w,w\}$ is also a basis: is this proof correct?","If  is a basis for , then  is also a basis: is this proof correct?","\{u,v,w\} V \{u+v+w,v+w,w\}","Let $u,v,w \in V$ a vector space over a field F such that $u \neq v \neq w$. If $\{ u , v , w \}$ is a basis for $V$, then prove that $\{ u+v+w , v+w , w \}$ is also a basis for $V$. Proof: Let $u,v,w \in V$ a vector space over a field $F$ such that $u \neq v \neq w$. Let $\{ u , v , w \}$ be a basis for $V$. Because $\{ u , v , w \}$ is a basis, then $u,v,w$ are linearly independent and $ \langle \{ u , v , w \} \rangle = V$. Let $x \in V$ be an arbitrary vector then $x$ can be uniquely expressed as a linear combination of $\{ u , v , w \}$. Let's suppose $x=au+bv+cw$ for some $a,b,c \in F$. On the other hand, let us consider $\{ u+v+w , v+w , w \} \subseteq V$. Then $$ \begin{align*} \langle \{ u+v+w , v+w , w \} \rangle &= \{d(u+v+w) + e(v+w) + f(w) \mid d,e,f \in F\} \\ &= \{du + (d+e)v +(d+e+f)w \mid d,e,f \in F \} . \end{align*}$$ If $x \in V$, then $x=du + (d+e)v +(d+e+f)w$ is another unique representation of $x \in V$ . Then for any arbitrary $x \in V$, we have $d=a$, $d+e=b $and $d+e+f=c \in F$. Because $\{ u , v , w \}$ is a basis for $V$, then $\{ u+v+w , v+w , w \}$ must also be a basis for $V$.","Let $u,v,w \in V$ a vector space over a field F such that $u \neq v \neq w$. If $\{ u , v , w \}$ is a basis for $V$, then prove that $\{ u+v+w , v+w , w \}$ is also a basis for $V$. Proof: Let $u,v,w \in V$ a vector space over a field $F$ such that $u \neq v \neq w$. Let $\{ u , v , w \}$ be a basis for $V$. Because $\{ u , v , w \}$ is a basis, then $u,v,w$ are linearly independent and $ \langle \{ u , v , w \} \rangle = V$. Let $x \in V$ be an arbitrary vector then $x$ can be uniquely expressed as a linear combination of $\{ u , v , w \}$. Let's suppose $x=au+bv+cw$ for some $a,b,c \in F$. On the other hand, let us consider $\{ u+v+w , v+w , w \} \subseteq V$. Then $$ \begin{align*} \langle \{ u+v+w , v+w , w \} \rangle &= \{d(u+v+w) + e(v+w) + f(w) \mid d,e,f \in F\} \\ &= \{du + (d+e)v +(d+e+f)w \mid d,e,f \in F \} . \end{align*}$$ If $x \in V$, then $x=du + (d+e)v +(d+e+f)w$ is another unique representation of $x \in V$ . Then for any arbitrary $x \in V$, we have $d=a$, $d+e=b $and $d+e+f=c \in F$. Because $\{ u , v , w \}$ is a basis for $V$, then $\{ u+v+w , v+w , w \}$ must also be a basis for $V$.",,['linear-algebra']
26,Prove $\| A(A^TA)^{-1}A^T\|_2 = 1$ when rank of matrix $A$ is $n$,Prove  when rank of matrix  is,\| A(A^TA)^{-1}A^T\|_2 = 1 A n,Given a matrix $A \in R^{m \times n}$ and whose rank is $n$. I need to show $\| A(A^TA)^{-1}A^T\|_2 = 1$. Can any hint me the direction in which I should solve this problem. Should I use any decomposition of matrix $A$ to show the result?,Given a matrix $A \in R^{m \times n}$ and whose rank is $n$. I need to show $\| A(A^TA)^{-1}A^T\|_2 = 1$. Can any hint me the direction in which I should solve this problem. Should I use any decomposition of matrix $A$ to show the result?,,['linear-algebra']
27,"When is a field ""compatible"" with an abelian group?","When is a field ""compatible"" with an abelian group?",,"Let $F$ be a field and $G$ an abelian group. We say that $F$ and $G$ are ""compatible"" if and only if there exists a function $M:F\times G\to G$ such that $M(\lambda,gh)=M(\lambda,g)M(\lambda,h)$ for all $\lambda\in F$ , $g,h\in G$ $M(\lambda+\mu,g)=M(\lambda,g)M(\mu,g)$ for all $\lambda,\mu\in F$ , $g\in G$ $M(\lambda\mu,g)=M(\lambda,M(\mu,g))$ for all $\lambda,\mu\in F$ , $g\in G$ $M(1,g)=g$ for all $g\in G$ $M(0,g)=\varepsilon$ for all $g\in G$ , where $\varepsilon$ is the identity of $G$ If $F$ and $G$ are compatible, then $(F,G)$ (or $\{F,G,M\}$ , or $\langle F,+_F,\cdot_F,G,+_g,M\rangle$ , or however else you care to denote it) forms a vector space, whose field is $F$ , whose vectors are $G$ with vector addition given by the group operation, and scalar multiplication given by $M$ . In fact, this is the definition of a vector space - a vector space consist precisely of a field and an abelian group related by a function like the one described. Question: What methods can be used to efficiently decide the compatibility of a given field with an arbitrary abelian group? There is a trivial case. If we take $F^X$ to be the group of functions from $X$ to $F$ with pointwise addition, and $G\cong F^X$ , then $F$ and $G$ are compatible by $M(\lambda,\varphi^{-1}(g))(x)=\lambda \varphi^{-1}(g)(x)$ , where $\varphi:F^X\to G$ is any group isomorphism. Every vector space for which I have a name is of this form, but, if I'm not mistaken, one can easily construct one which isn't by matching a group with an involution to a field whose additive group has none.","Let be a field and an abelian group. We say that and are ""compatible"" if and only if there exists a function such that for all , for all , for all , for all for all , where is the identity of If and are compatible, then (or , or , or however else you care to denote it) forms a vector space, whose field is , whose vectors are with vector addition given by the group operation, and scalar multiplication given by . In fact, this is the definition of a vector space - a vector space consist precisely of a field and an abelian group related by a function like the one described. Question: What methods can be used to efficiently decide the compatibility of a given field with an arbitrary abelian group? There is a trivial case. If we take to be the group of functions from to with pointwise addition, and , then and are compatible by , where is any group isomorphism. Every vector space for which I have a name is of this form, but, if I'm not mistaken, one can easily construct one which isn't by matching a group with an involution to a field whose additive group has none.","F G F G M:F\times G\to G M(\lambda,gh)=M(\lambda,g)M(\lambda,h) \lambda\in F g,h\in G M(\lambda+\mu,g)=M(\lambda,g)M(\mu,g) \lambda,\mu\in F g\in G M(\lambda\mu,g)=M(\lambda,M(\mu,g)) \lambda,\mu\in F g\in G M(1,g)=g g\in G M(0,g)=\varepsilon g\in G \varepsilon G F G (F,G) \{F,G,M\} \langle F,+_F,\cdot_F,G,+_g,M\rangle F G M F^X X F G\cong F^X F G M(\lambda,\varphi^{-1}(g))(x)=\lambda \varphi^{-1}(g)(x) \varphi:F^X\to G","['linear-algebra', 'abstract-algebra', 'group-theory', 'vector-spaces', 'field-theory']"
28,Norm inequality $\| x - y \| \cdot \| z \| \leq \| x - z \| \cdot \| y \| + \| z - y \| \cdot \| x \|$,Norm inequality,\| x - y \| \cdot \| z \| \leq \| x - z \| \cdot \| y \| + \| z - y \| \cdot \| x \|,"Let $x$ , $y$ , $z$ be $3$ vectors in Euclidean space $V$ . $\| x \|$ is norm of $x$ (length) How do you prove: $$\| x - y \| \cdot \|z\| \leq \| x - z\| \cdot \| y \| + \| z - y\| \cdot \| x \|?$$ I have tried Cauchy-Schwartz inequality, | < x,y > | <= ||x|| •||y||, where  is dot product between x and y, trying to move everything on one side and then proving that it is greater than zero. Also I used modules properties, |a + b| < = |a| + |b| and so on. Also triangle inequality, ||x + y || <= ||x|| + ||y|| , trying to get the other side of inequality. Considering that ||x|| is sqrt( < x,x > ) and < x , a+b > is < x ,a > + < x ,b > and that, norm is always greater than 0 and sqrt( a+b )<= sqrt(a) + sqrt(b) , if a and b are positive, I played with this properties without getting any result.. Also distance formula, d(a,b) <= d(a,c) + d(c,a), where d(a,b) = || a - b|| There is this formula: Ilz - xII^2 + Ilz - yII^2 - Ilx - yII^2 =2 Maybe it helps. I have tried to use it but without a result..","Let , , be vectors in Euclidean space . is norm of (length) How do you prove: I have tried Cauchy-Schwartz inequality, | < x,y > | <= ||x|| •||y||, where  is dot product between x and y, trying to move everything on one side and then proving that it is greater than zero. Also I used modules properties, |a + b| < = |a| + |b| and so on. Also triangle inequality, ||x + y || <= ||x|| + ||y|| , trying to get the other side of inequality. Considering that ||x|| is sqrt( < x,x > ) and < x , a+b > is < x ,a > + < x ,b > and that, norm is always greater than 0 and sqrt( a+b )<= sqrt(a) + sqrt(b) , if a and b are positive, I played with this properties without getting any result.. Also distance formula, d(a,b) <= d(a,c) + d(c,a), where d(a,b) = || a - b|| There is this formula: Ilz - xII^2 + Ilz - yII^2 - Ilx - yII^2 =2 Maybe it helps. I have tried to use it but without a result..",x y z 3 V \| x \| x \| x - y \| \cdot \|z\| \leq \| x - z\| \cdot \| y \| + \| z - y\| \cdot \| x \|?,"['linear-algebra', 'geometry', 'euclidean-geometry', 'normed-spaces']"
29,Eigenvalues of exponential matrix,Eigenvalues of exponential matrix,,"Let's consider a matrix $X \in M\left(2, \mathbb{R}\right)$. It's a well-known result that if $\lambda$ is an eigenvalue of $X$, then $\exp\left(\lambda\right)$ is an eigenvalue of $\exp\left(X \right)$. But does it hold the other way round? More precisely, my question is as follows: is we know eigenvalues of $\exp\left(X\right)$, then what can we say about eigenvalues of $X$? Does existence of eigenvalues of $\exp\left(X\right)$ even guarantee existence of (real) eigenvalues of $X$?","Let's consider a matrix $X \in M\left(2, \mathbb{R}\right)$. It's a well-known result that if $\lambda$ is an eigenvalue of $X$, then $\exp\left(\lambda\right)$ is an eigenvalue of $\exp\left(X \right)$. But does it hold the other way round? More precisely, my question is as follows: is we know eigenvalues of $\exp\left(X\right)$, then what can we say about eigenvalues of $X$? Does existence of eigenvalues of $\exp\left(X\right)$ even guarantee existence of (real) eigenvalues of $X$?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-exponential']"
30,What does the inner product of two complex general vectors have to do with complex conjugation?,What does the inner product of two complex general vectors have to do with complex conjugation?,,"$ \newcommand{\qr}[1]{|#1 \rangle}  \newcommand{\ql}[1]{\langle #1|}  \newcommand{\q}[2]{\langle #1 | #2 \rangle} \newcommand{\v}[2]{\langle #1,#2\rangle} $ I'm reading ""Quantum computer science, an introduction,"" N. David Mermin, Cambridge University Press, 2007.  In Appendix A, he writes that ""in a vector space over the complex numbers the inner product of two general vectors is a complex number satisfying $\q{\psi}{\phi} = \q{\psi}{\phi}^*$, where $*$ denotes complex conjugation."" (Page 160.) I must not have understood this.  I tried the following example.  What's the inner product of $\v{2+3i}{7+3i} \cdot \v{1+2i}{2+2i}$?  It's $4 + 27i$.  If I commute $\v{1+2i}{2+2i} \cdot \v{2+3i}{7+3i}$, I get the same $4 + 27i$ because inner products are commutative.  Taking the complex conjugate $4 + 27i$, I get $4 - 27i$ which is not the same as its complex conjugate, $4 + 27i$. So I'm pretty lost here.  What does he mean by what he said?","$ \newcommand{\qr}[1]{|#1 \rangle}  \newcommand{\ql}[1]{\langle #1|}  \newcommand{\q}[2]{\langle #1 | #2 \rangle} \newcommand{\v}[2]{\langle #1,#2\rangle} $ I'm reading ""Quantum computer science, an introduction,"" N. David Mermin, Cambridge University Press, 2007.  In Appendix A, he writes that ""in a vector space over the complex numbers the inner product of two general vectors is a complex number satisfying $\q{\psi}{\phi} = \q{\psi}{\phi}^*$, where $*$ denotes complex conjugation."" (Page 160.) I must not have understood this.  I tried the following example.  What's the inner product of $\v{2+3i}{7+3i} \cdot \v{1+2i}{2+2i}$?  It's $4 + 27i$.  If I commute $\v{1+2i}{2+2i} \cdot \v{2+3i}{7+3i}$, I get the same $4 + 27i$ because inner products are commutative.  Taking the complex conjugate $4 + 27i$, I get $4 - 27i$ which is not the same as its complex conjugate, $4 + 27i$. So I'm pretty lost here.  What does he mean by what he said?",,"['linear-algebra', 'complex-numbers', 'vector-spaces']"
31,Let$A$ be a $3\times3$ real symmetric matrix such that $A^6=I$ . Then $A^2=I$,Let be a  real symmetric matrix such that  . Then,A 3\times3 A^6=I A^2=I,Let $A$ be a $3\times3$ real symmetric matrix such $ A^6=I$ . Then $A^2=I$ . How can I prove this statement is true or false? As it is given $A$ is symmetric so $A=A^T$ . Also $ A^6=I$ . But the main problem is that I can't operate $A^{-1}$ on both sides whether it is invertible or not. Can any help me what should I do?,Let be a real symmetric matrix such . Then . How can I prove this statement is true or false? As it is given is symmetric so . Also . But the main problem is that I can't operate on both sides whether it is invertible or not. Can any help me what should I do?,A 3\times3  A^6=I A^2=I A A=A^T  A^6=I A^{-1},"['linear-algebra', 'matrices', 'symmetric-matrices']"
32,Which of the following sets of matrices are dense in the set of square $n \times n$ square matrices over $\mathbb{C}$?,Which of the following sets of matrices are dense in the set of square  square matrices over ?,n \times n \mathbb{C},Practicing for the GRE I found this question and I was wondering if anyone had any general tips to approach this type of questions or any literature I could review to approach them. Which of the following sets are dense in the set of square $n \times n$ square matrices over $\mathbb{C}$? I) Invertible matrices II) Unitary Matrices III) Symmetric Matrices IV) Diagonalizable Matrices.,Practicing for the GRE I found this question and I was wondering if anyone had any general tips to approach this type of questions or any literature I could review to approach them. Which of the following sets are dense in the set of square $n \times n$ square matrices over $\mathbb{C}$? I) Invertible matrices II) Unitary Matrices III) Symmetric Matrices IV) Diagonalizable Matrices.,,"['linear-algebra', 'matrices', 'gre-exam']"
33,dimension of intersection of hyperplanes,dimension of intersection of hyperplanes,,What are the possible dimensions of intersection of k-number of hyperplanes in $\mathbb{R}^n$ ? I look at some examples in lower dimension but I cannot come with a nice cases according to which the dimension varies. Thanks for your valuable time.,What are the possible dimensions of intersection of k-number of hyperplanes in $\mathbb{R}^n$ ? I look at some examples in lower dimension but I cannot come with a nice cases according to which the dimension varies. Thanks for your valuable time.,,['linear-algebra']
34,Solving $Ax=b$ when $x$ and $b$ are given.,Solving  when  and  are given.,Ax=b x b,"I am trying to study least square and linear regression and I understand the solution for $Ax = b$ when x is the unknown and the LS solution is given by $(A^TA)^{-1}A^TA$. Now, I was wondering if something similar exists when A is unknown. I was wondering if I do the corresponding: $$ Ax=b $$ $$ x^TA^T = b $$ Therefore, $A^T = (xx^T)^{-1}xb$ or $A = b^Tx^T ((xx^T)^{-1})^T$","I am trying to study least square and linear regression and I understand the solution for $Ax = b$ when x is the unknown and the LS solution is given by $(A^TA)^{-1}A^TA$. Now, I was wondering if something similar exists when A is unknown. I was wondering if I do the corresponding: $$ Ax=b $$ $$ x^TA^T = b $$ Therefore, $A^T = (xx^T)^{-1}xb$ or $A = b^Tx^T ((xx^T)^{-1})^T$",,"['linear-algebra', 'matrices', 'least-squares']"
35,Reviews/comparison of Python vs Matlab for teaching Linear Algebra,Reviews/comparison of Python vs Matlab for teaching Linear Algebra,,"I am wondering about advantages of using Python for teaching introductory linear algebra. I have been using Matlab and I became interested in Python mainly because of several resources, e.g., text 1 and video course 1 that use it, as well as the popularity of Sage . However, I have not used Python myself. If you have used Python for teaching linear algebra and especially if you have also used Matlab, how would you compare them. I am aware that Scilab is free and similar to Matlab. So price consideration is not the main issue here. Related: NumPy vs Matlab A comparison of language features. A Review from StackOverFlow , but not specific to teaching linear algebra. A review from a research point of view. A text on scientific computing with Python. A course and its Lab Manual on linear algebra.","I am wondering about advantages of using Python for teaching introductory linear algebra. I have been using Matlab and I became interested in Python mainly because of several resources, e.g., text 1 and video course 1 that use it, as well as the popularity of Sage . However, I have not used Python myself. If you have used Python for teaching linear algebra and especially if you have also used Matlab, how would you compare them. I am aware that Scilab is free and similar to Matlab. So price consideration is not the main issue here. Related: NumPy vs Matlab A comparison of language features. A Review from StackOverFlow , but not specific to teaching linear algebra. A review from a research point of view. A text on scientific computing with Python. A course and its Lab Manual on linear algebra.",,"['linear-algebra', 'education', 'math-software']"
36,Minimum eigenvalue and singular value of a square matrix,Minimum eigenvalue and singular value of a square matrix,,How to show that the relationship $\left | \lambda_{min} \right | \geq \sigma_{min}$ holds between the minimum eigenvalue and singular value of a square matrix $A \in \mathbb{C}^{n \times n}$?,How to show that the relationship $\left | \lambda_{min} \right | \geq \sigma_{min}$ holds between the minimum eigenvalue and singular value of a square matrix $A \in \mathbb{C}^{n \times n}$?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
37,"$A$ square matrix,nonsingular $\implies $ all submatrixes of $A$ are also nonsingular?","square matrix,nonsingular  all submatrixes of  are also nonsingular?",A \implies  A,"If a square matrix $A$ is nonsingular, then every submatrix of $A$ is also nonsingular. I am trying to come up with a counter example. But most involve really difficult examples, so I am starting to think this is actually true and probably something to do with a non zero determinant through each sub matrix. Also, does taking a zero entry in say an identity count as a non singular matrix? (1 x 1 matrix)","If a square matrix $A$ is nonsingular, then every submatrix of $A$ is also nonsingular. I am trying to come up with a counter example. But most involve really difficult examples, so I am starting to think this is actually true and probably something to do with a non zero determinant through each sub matrix. Also, does taking a zero entry in say an identity count as a non singular matrix? (1 x 1 matrix)",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
38,"Does common eigenvectors between two matrices A,B implies some property for the vectors?","Does common eigenvectors between two matrices A,B implies some property for the vectors?",,If there are two matrixes that they have common eigenvectors for some eigenvalues that implies that those two matrixes are identical? What can we say for those two matrices?,If there are two matrixes that they have common eigenvectors for some eigenvalues that implies that those two matrixes are identical? What can we say for those two matrices?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
39,"A finite graph G is $d$-regular if, and only if, its adjacency matrix has the eigenvalue $λ = d$","A finite graph G is -regular if, and only if, its adjacency matrix has the eigenvalue",d λ = d,"Show that a graph $G$ ﬁnite with $n$ vertices is $d$-regular if, and only if, the vector with all the coordinates equals to 1 is  eigenvetor from eigenvalue $λ = d$  from the  adjacency matrix  $A$ from the graph $G$. The question itself was a little confused for me...","Show that a graph $G$ ﬁnite with $n$ vertices is $d$-regular if, and only if, the vector with all the coordinates equals to 1 is  eigenvetor from eigenvalue $λ = d$  from the  adjacency matrix  $A$ from the graph $G$. The question itself was a little confused for me...",,"['linear-algebra', 'graph-theory', 'spectral-graph-theory']"
40,There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue,There exists  with  iff  and  have a common eigenvalue,C\neq0 CA=BC A B,"Question: Suppose $V$ and $W$ are finite dimensional vector spaces over $\mathbb{C}$. $A$ is a linear transformation on $V$,  $B$ is a linear transformation on $W$. Then there exists a non-zero linear map $C:V\to W$ s.t. $CA=BC$ iff $A$ and $B$ have a same eigenvalue. ===========This is incorrect========== Clearly, if $CA=BC$, suppose $Ax=\lambda x$, then $B(Cx)=CAx=C(\lambda x)=\lambda (Cx)$, so $A$ and $B$ have same eigenvalue $\lambda$. On the other hand, if $A$ and $B$ have a same eigenvalue $\lambda$, suppose $Ax=\lambda x, By=\lambda y$. Define $C:V\to W$ s.t. $Cx=y$, then $BCx=By=\lambda y=C\lambda x=CAx$. But I don't kow how to make $BC=CA$ for all $x\in V$. ======================================","Question: Suppose $V$ and $W$ are finite dimensional vector spaces over $\mathbb{C}$. $A$ is a linear transformation on $V$,  $B$ is a linear transformation on $W$. Then there exists a non-zero linear map $C:V\to W$ s.t. $CA=BC$ iff $A$ and $B$ have a same eigenvalue. ===========This is incorrect========== Clearly, if $CA=BC$, suppose $Ax=\lambda x$, then $B(Cx)=CAx=C(\lambda x)=\lambda (Cx)$, so $A$ and $B$ have same eigenvalue $\lambda$. On the other hand, if $A$ and $B$ have a same eigenvalue $\lambda$, suppose $Ax=\lambda x, By=\lambda y$. Define $C:V\to W$ s.t. $Cx=y$, then $BCx=By=\lambda y=C\lambda x=CAx$. But I don't kow how to make $BC=CA$ for all $x\in V$. ======================================",,['linear-algebra']
41,Where can I find Linear Algebra in Nature?,Where can I find Linear Algebra in Nature?,,"I'm a Computer Science major and I've been studying Analytic Geometry and Linear Algebra this semester. Today my teacher gave a hell of an explanation talking about linear systems, quadratic functions, polynomial equations, derivatives, etc., etc.. In his examples, he talked about Ballistics, about space and all matter, about the physical ""tissue"" that covers every object, cars and its aerodynamics, and so on and I was basically tripping with so many crazy ideas. I really, really loved this subject and have even been considering studying it more out of the Academia for curiosity purposes. The thing is, I couldn't find more of these examples where linear algebra can be used in nature. I know about some possible uses in Computer Science such as Computer Graphics, A.I., cryptography and etc.. Can anyone shed a light? Thanks!","I'm a Computer Science major and I've been studying Analytic Geometry and Linear Algebra this semester. Today my teacher gave a hell of an explanation talking about linear systems, quadratic functions, polynomial equations, derivatives, etc., etc.. In his examples, he talked about Ballistics, about space and all matter, about the physical ""tissue"" that covers every object, cars and its aerodynamics, and so on and I was basically tripping with so many crazy ideas. I really, really loved this subject and have even been considering studying it more out of the Academia for curiosity purposes. The thing is, I couldn't find more of these examples where linear algebra can be used in nature. I know about some possible uses in Computer Science such as Computer Graphics, A.I., cryptography and etc.. Can anyone shed a light? Thanks!",,"['linear-algebra', 'analytic-geometry']"
42,Does $V=W\oplus W^\perp$ hold when $W$ is infinitely-dimensional?,Does  hold when  is infinitely-dimensional?,V=W\oplus W^\perp W,"Let $V$ be an inner product space over $F$. Let $W$ be a subspace of $V$. I saw that Axler's Linear Algebra Done Right claimed that if $W$ is finite-dimensional , then $V=W\oplus W^\perp$. It is trivial that $W\cap W^\perp=\{0\}$ is always true. So let us focus on $V=W+W^\perp$. As you can see in the picture, the proof for $V=W+W^\perp$ used the fact that $W$ is finite-dimensional. However, does $V=W+W^\perp$ hold when $W$ is infinite-dimensional? If so, how to prove it? Update: I also wonder that if $W$ is finite-dimensional, must $W^\perp$ be finite-dimensional or infinite-dimensional? reference:","Let $V$ be an inner product space over $F$. Let $W$ be a subspace of $V$. I saw that Axler's Linear Algebra Done Right claimed that if $W$ is finite-dimensional , then $V=W\oplus W^\perp$. It is trivial that $W\cap W^\perp=\{0\}$ is always true. So let us focus on $V=W+W^\perp$. As you can see in the picture, the proof for $V=W+W^\perp$ used the fact that $W$ is finite-dimensional. However, does $V=W+W^\perp$ hold when $W$ is infinite-dimensional? If so, how to prove it? Update: I also wonder that if $W$ is finite-dimensional, must $W^\perp$ be finite-dimensional or infinite-dimensional? reference:",,"['linear-algebra', 'inner-products', 'orthogonality']"
43,Doubt over the proof of Cayley- Hamilton heorem,Doubt over the proof of Cayley- Hamilton heorem,,"I am having some doubt in the proof of Cayley Hamilton theorem.  This theorem says that every matrix is a root if its characteristic polynomial. Proof goes as follows: Let us assume that matrix $A$ is of order $n\times n$. If $P(\lambda)$ be its characteristic polynomial, then by the definition of the characteristic polynomial $P(\lambda) = det (A - \lambda I) = P_0 + P_1\lambda + P_2 \lambda^2  +\ldots P_n \lambda^n$. Next, suppose that $Q(\lambda)$ be the adjoint matrix of $(A - \lambda I)$, such that $Q(\lambda) =Q_0 + Q_1\lambda + Q_2 \lambda^2  +\ldots Q_k \lambda^k$. I am not able to understand why the polynomial expression of $Q(\lambda)$ is of degree $k$? Can't I write $Q(\lambda)$ as follows (degree $n$ polynomial in $\lambda$) $Q(\lambda) =Q_0 + Q_1\lambda + Q_2 \lambda^2  +\ldots Q_n \lambda^n$. Thank you","I am having some doubt in the proof of Cayley Hamilton theorem.  This theorem says that every matrix is a root if its characteristic polynomial. Proof goes as follows: Let us assume that matrix $A$ is of order $n\times n$. If $P(\lambda)$ be its characteristic polynomial, then by the definition of the characteristic polynomial $P(\lambda) = det (A - \lambda I) = P_0 + P_1\lambda + P_2 \lambda^2  +\ldots P_n \lambda^n$. Next, suppose that $Q(\lambda)$ be the adjoint matrix of $(A - \lambda I)$, such that $Q(\lambda) =Q_0 + Q_1\lambda + Q_2 \lambda^2  +\ldots Q_k \lambda^k$. I am not able to understand why the polynomial expression of $Q(\lambda)$ is of degree $k$? Can't I write $Q(\lambda)$ as follows (degree $n$ polynomial in $\lambda$) $Q(\lambda) =Q_0 + Q_1\lambda + Q_2 \lambda^2  +\ldots Q_n \lambda^n$. Thank you",,"['linear-algebra', 'cayley-hamilton']"
44,Vector spaces - Proving that intersection is distributive over summation of vector spaces,Vector spaces - Proving that intersection is distributive over summation of vector spaces,,"The problem Define the sum of two vector spaces (often two subspaces of a common vector space) $A$ and $B$ as $A+B=\{a+b: a \in A, b \in B \}.$ Let $U,V,W$ be arbitrary vector spaces. I want to show that $$U \cap (V+W)=(U \cap V)+(U \cap W)$$ My approach By starting with $x \in (U \cap V)+(U \cap W)$ and applying the properties of vector spaces (closure under vector addition ) to end up with $x \in U \cap (V+W)$, I've shown that $(U \cap V)+(U \cap W) \subset U \cap (V+W)$. My problem is with the reverse, i.e. showing that $U \cap (V+W) \subset (U \cap V)+(U \cap W)$. Following the usual procedure, I started with: $x \in U \cap (V+W)$, which implies that $x \in U \land x \in V+W$. Now, $x \in V+W \implies x=y+z$ for some $y \in V$ and some $z \in W$. Again, $x=y+z \in U$. If we could show that $y$ and $z$ both comes from $U$, then we will be done. But unfortunately $y+z \in U$ doesn't necessarily imply that $y \in U$ and $z \in U.$ Am I missing some very basic properties of vector spaces that would make this trivial? Any help would be greatly appreciated. P.S. Just a dumb question. Does the result break down if we replace vector spaces by arbitrary sets in the above problem?","The problem Define the sum of two vector spaces (often two subspaces of a common vector space) $A$ and $B$ as $A+B=\{a+b: a \in A, b \in B \}.$ Let $U,V,W$ be arbitrary vector spaces. I want to show that $$U \cap (V+W)=(U \cap V)+(U \cap W)$$ My approach By starting with $x \in (U \cap V)+(U \cap W)$ and applying the properties of vector spaces (closure under vector addition ) to end up with $x \in U \cap (V+W)$, I've shown that $(U \cap V)+(U \cap W) \subset U \cap (V+W)$. My problem is with the reverse, i.e. showing that $U \cap (V+W) \subset (U \cap V)+(U \cap W)$. Following the usual procedure, I started with: $x \in U \cap (V+W)$, which implies that $x \in U \land x \in V+W$. Now, $x \in V+W \implies x=y+z$ for some $y \in V$ and some $z \in W$. Again, $x=y+z \in U$. If we could show that $y$ and $z$ both comes from $U$, then we will be done. But unfortunately $y+z \in U$ doesn't necessarily imply that $y \in U$ and $z \in U.$ Am I missing some very basic properties of vector spaces that would make this trivial? Any help would be greatly appreciated. P.S. Just a dumb question. Does the result break down if we replace vector spaces by arbitrary sets in the above problem?",,['linear-algebra']
45,How to determine if vector b is in the span of matrix A? [closed],How to determine if vector b is in the span of matrix A? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given a matrix A = \begin{bmatrix} 1 &2  &3 \\  4 &5  &6 \\  7 &8  &9  \end{bmatrix} Determine if vector $b$ is in $span(A)$ where  $$ b = \begin{bmatrix} 1  \\  2  \\  4   \end{bmatrix} $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given a matrix A = \begin{bmatrix} 1 &2  &3 \\  4 &5  &6 \\  7 &8  &9  \end{bmatrix} Determine if vector $b$ is in $span(A)$ where  $$ b = \begin{bmatrix} 1  \\  2  \\  4   \end{bmatrix} $$",,['linear-algebra']
46,how to find the basis of a plane or a line?,how to find the basis of a plane or a line?,,"Find a basis for the plane $x-2y+3z=0$ in $R^3$. Then find a basis for the intersection of that plane with the $xy$ plane. Is there a proper/algebraic way of finding the basis of a plane? Just by looking at it a basis could be $(2, 1, 0)$ because any multiple of that will give you $0$ when you substitute, but how do I find this without guessing? would I use the same process when finding the basis of a line? Any hints on how to figure out the second part of the question?","Find a basis for the plane $x-2y+3z=0$ in $R^3$. Then find a basis for the intersection of that plane with the $xy$ plane. Is there a proper/algebraic way of finding the basis of a plane? Just by looking at it a basis could be $(2, 1, 0)$ because any multiple of that will give you $0$ when you substitute, but how do I find this without guessing? would I use the same process when finding the basis of a line? Any hints on how to figure out the second part of the question?",,['linear-algebra']
47,"If $A$ and $B$ are normal such that $AB=0$, does it follows that $BA=0$?","If  and  are normal such that , does it follows that ?",A B AB=0 BA=0,"If $A$ and $B$ are normal linear transformation on the finite-dimensional complex inner product space $X$ such that $AB=0$, does it follows that $BA=0$?","If $A$ and $B$ are normal linear transformation on the finite-dimensional complex inner product space $X$ such that $AB=0$, does it follows that $BA=0$?",,['linear-algebra']
48,"Computing the dimension of a vector space of matrices that commute with a given matrix B,","Computing the dimension of a vector space of matrices that commute with a given matrix B,",,"This is part $2$ of the question that I am working on. For part $1$, I showed that the space of $5\times 5$ matrices which commute with a given matrix $B$, with the ground field = $\mathbb R$ , is a vector space. But how can I compute its dimension? Thanks,","This is part $2$ of the question that I am working on. For part $1$, I showed that the space of $5\times 5$ matrices which commute with a given matrix $B$, with the ground field = $\mathbb R$ , is a vector space. But how can I compute its dimension? Thanks,",,"['linear-algebra', 'vector-spaces']"
49,Determinant always equal to zero?,Determinant always equal to zero?,,"I just finished writing a computer program that takes as input a number of matrices and computes the inverse of the product of matrices. To test this program, I wanted to input a $3 \times 2$ matrix followed by a $2\times 3$ matrix so that the product would be a $3\times 3$ matrix. No matter how hard I try, the determinant of the product turns out to be zero and so the product is non-invertible. Is there a theorem in linear algebra that implies that the product of a $3\times 2$ matrix and $2\times 3$ matrix will always have determinant zero?","I just finished writing a computer program that takes as input a number of matrices and computes the inverse of the product of matrices. To test this program, I wanted to input a $3 \times 2$ matrix followed by a $2\times 3$ matrix so that the product would be a $3\times 3$ matrix. No matter how hard I try, the determinant of the product turns out to be zero and so the product is non-invertible. Is there a theorem in linear algebra that implies that the product of a $3\times 2$ matrix and $2\times 3$ matrix will always have determinant zero?",,"['linear-algebra', 'determinant']"
50,Why in the proof of $A\cdot Adj(A)=Det(A)\cdot I_n$ entires not on the diagonal are zero?,Why in the proof of  entires not on the diagonal are zero?,A\cdot Adj(A)=Det(A)\cdot I_n,"I have read the proof . I do not get why the entires that are not on the diagonal are equal to zero, or why the following part of the proof is true: If $k\neq \ell$ $$(A\cdot\hat A)_{k\ell}=\sum_{i=1}^n (-1)^{i+\ell}a_{ki} \det A(\ell\mid i)=0$$","I have read the proof . I do not get why the entires that are not on the diagonal are equal to zero, or why the following part of the proof is true: If $k\neq \ell$ $$(A\cdot\hat A)_{k\ell}=\sum_{i=1}^n (-1)^{i+\ell}a_{ki} \det A(\ell\mid i)=0$$",,['linear-algebra']
51,"Norm bound on exponential matrix with eigenvalue negative real part, proof","Norm bound on exponential matrix with eigenvalue negative real part, proof",,"If $A$ is $n \times n$ with negative real parts of all eigenvalues, then there exists positive $K,\alpha$ such that $$\|e^{At}\| \leq Ke^{-\alpha t}$$ Furthermore, if an eigenvalue has negative part zero, but with single multiplicity, then there exists $M > 0$ such that $$\|e^{At}\| \leq M$$ Starting out: We have $e^{At} = Pe^{Jt}P^{-1}$, so $\|e^{At}\| \leq \|P\|\|e^{Jt}\|P^{-1}\| = K_1\|e^{Jt}\|$ since $P$ is invertible etc. I'm not sure how you bound the norm of the exponential Jordan matrix here. The book (Sze-Bi Hsu's ODE book) sort of just states that it's bounded somehow.","If $A$ is $n \times n$ with negative real parts of all eigenvalues, then there exists positive $K,\alpha$ such that $$\|e^{At}\| \leq Ke^{-\alpha t}$$ Furthermore, if an eigenvalue has negative part zero, but with single multiplicity, then there exists $M > 0$ such that $$\|e^{At}\| \leq M$$ Starting out: We have $e^{At} = Pe^{Jt}P^{-1}$, so $\|e^{At}\| \leq \|P\|\|e^{Jt}\|P^{-1}\| = K_1\|e^{Jt}\|$ since $P$ is invertible etc. I'm not sure how you bound the norm of the exponential Jordan matrix here. The book (Sze-Bi Hsu's ODE book) sort of just states that it's bounded somehow.",,"['linear-algebra', 'ordinary-differential-equations']"
52,Show that four points are coplanar,Show that four points are coplanar,,"I read all posts online regarding how to show four points are coplanar. However, none of them discuss the idea behind the method. Can someone explain how the triple scalar product works?","I read all posts online regarding how to show four points are coplanar. However, none of them discuss the idea behind the method. Can someone explain how the triple scalar product works?",,['linear-algebra']
53,Am I misinterpreting this matrix determinant property?,Am I misinterpreting this matrix determinant property?,,"I was reading matrix determinant properties from wikipedia . The property reads $\det(cA) = c^n \det(A)$ for $n \times n$ matrix. However I am not able to realize it. What I find is $\det(cA) = c\det(A)$ For example, multiplying matrix by 2 and then taking the determinant of the resultant matrix: $ 2\begin{bmatrix} 4 & 5 & 6 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \\ \end{bmatrix}= \begin{bmatrix} 8 & 10 & 12 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \end{bmatrix} $ and $ \begin{vmatrix} 8 & 10 & 12 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \end{vmatrix}=60 $ Now first taking the determinant and then multiplying by 2 yields the same result: $$ 2\begin{vmatrix} 4 & 5 & 6 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \\ \end{vmatrix} = 2 \cdot 30 = 60 $$ Where I am mistaking?","I was reading matrix determinant properties from wikipedia . The property reads $\det(cA) = c^n \det(A)$ for $n \times n$ matrix. However I am not able to realize it. What I find is $\det(cA) = c\det(A)$ For example, multiplying matrix by 2 and then taking the determinant of the resultant matrix: $ 2\begin{bmatrix} 4 & 5 & 6 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \\ \end{bmatrix}= \begin{bmatrix} 8 & 10 & 12 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \end{bmatrix} $ and $ \begin{vmatrix} 8 & 10 & 12 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \end{vmatrix}=60 $ Now first taking the determinant and then multiplying by 2 yields the same result: $$ 2\begin{vmatrix} 4 & 5 & 6 \\ 6 & 5 & 4 \\ 4 & 6 & 5 \\ \end{vmatrix} = 2 \cdot 30 = 60 $$ Where I am mistaking?",,"['linear-algebra', 'matrices', 'determinant']"
54,Spectrum of idempotent element,Spectrum of idempotent element,,"Let $A$ be some unitary algebra over $\mathbb{C}$. If $a^2=a$ and $0\ne a\ne 1$ then $\{0,1\}\subset \sigma_A(a)$ ($\sigma_A(a)$ is the spectrum if $a$). I believe that also $\sigma_A(a)\subset \{0,1\}$. For examlpe, if $A=\mathbb{C}\times\mathbb{C}\times...\times\mathbb{C}$ with pointwise multiplication then each idempotent is $(\epsilon_1,\epsilon_2,...,\epsilon_n)$ where $\epsilon_i\in \{0,1\}$ and $(\epsilon_1-\lambda,\epsilon_2-\lambda,...,\epsilon_n-\lambda)$ is invertible iff $\epsilon_i\ne \lambda$ i.e. $\lambda\notin \{0,1\}$. Also, if $K\subset \mathbb{R}^2$ is a compact set then the space of continuous functions $A=C(K,\mathbb{C})$ is also a $\mathbb{C}$-algebra and $f$ is idempotent iff $f(K)\subset \{0,1\}$. But $\sigma_A(f)=f(K)$. First, I am stuck on to find some easy trick to prove that the spectrum of an idempotent element (not $0$ and not $1$) is $\{0,1\}$. Secondly, this statement is easy for $\mathbb{C}^S$ (this is the algebra of all functions and $\sigma(f)=f(S)$), perhaps there is a nice way to reduce our problem to the case of $\mathbb{C}^S$?","Let $A$ be some unitary algebra over $\mathbb{C}$. If $a^2=a$ and $0\ne a\ne 1$ then $\{0,1\}\subset \sigma_A(a)$ ($\sigma_A(a)$ is the spectrum if $a$). I believe that also $\sigma_A(a)\subset \{0,1\}$. For examlpe, if $A=\mathbb{C}\times\mathbb{C}\times...\times\mathbb{C}$ with pointwise multiplication then each idempotent is $(\epsilon_1,\epsilon_2,...,\epsilon_n)$ where $\epsilon_i\in \{0,1\}$ and $(\epsilon_1-\lambda,\epsilon_2-\lambda,...,\epsilon_n-\lambda)$ is invertible iff $\epsilon_i\ne \lambda$ i.e. $\lambda\notin \{0,1\}$. Also, if $K\subset \mathbb{R}^2$ is a compact set then the space of continuous functions $A=C(K,\mathbb{C})$ is also a $\mathbb{C}$-algebra and $f$ is idempotent iff $f(K)\subset \{0,1\}$. But $\sigma_A(f)=f(K)$. First, I am stuck on to find some easy trick to prove that the spectrum of an idempotent element (not $0$ and not $1$) is $\{0,1\}$. Secondly, this statement is easy for $\mathbb{C}^S$ (this is the algebra of all functions and $\sigma(f)=f(S)$), perhaps there is a nice way to reduce our problem to the case of $\mathbb{C}^S$?",,"['linear-algebra', 'functional-analysis', 'spectral-theory', 'banach-algebras', 'idempotents']"
55,inverse of diagonal plus sum of rank one matrices,inverse of diagonal plus sum of rank one matrices,,Is there formula for the inverse of a matrix which is diagonal plus a sum of rank one matrices? $$S=\alpha I + \sum_1^N u_iu_i^T$$ $$S^{-1} =  ?$$ Is there any decomposition or special trick that I can solve this inverse faster?,Is there formula for the inverse of a matrix which is diagonal plus a sum of rank one matrices? $$S=\alpha I + \sum_1^N u_iu_i^T$$ $$S^{-1} =  ?$$ Is there any decomposition or special trick that I can solve this inverse faster?,,"['linear-algebra', 'matrices']"
56,Dimension of the corresponding eigenspace?,Dimension of the corresponding eigenspace?,,"I'm studying for my linear exam and would appreciate any help for this practise question: You are given that λ = 1 is an eigenvalue of A. What is the dimension of the corresponding eigenspace? A = $\begin{bmatrix} 1 & 0 & 0 & -2 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ -1 & 0 & 0 & 1 \end{bmatrix}$ Then with my knowing that λ = 1, I got: $\begin{bmatrix} 0 & 0 & 0 & -2 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ -1 & 0 & 0 & 0 \end{bmatrix}$ Which I assume right off the bat means my dimension is 0. Is that correct? If not how should I do it? If we had a different matrix, how would I go ahead to properly find the dimension? In layman terms I think that it would be whichever value is linearly independent? Thanks for the help.","I'm studying for my linear exam and would appreciate any help for this practise question: You are given that λ = 1 is an eigenvalue of A. What is the dimension of the corresponding eigenspace? A = Then with my knowing that λ = 1, I got: Which I assume right off the bat means my dimension is 0. Is that correct? If not how should I do it? If we had a different matrix, how would I go ahead to properly find the dimension? In layman terms I think that it would be whichever value is linearly independent? Thanks for the help.",\begin{bmatrix} 1 & 0 & 0 & -2 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ -1 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 & 0 & -2 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ -1 & 0 & 0 & 0 \end{bmatrix},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
57,Prove that $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A + B)$,Prove that,\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A + B),"Let $A,B$ be matrices $m\times n$ $(A, B \in M_{m\times n}(R))$. How can we prove that $$\operatorname{rank}(A+ B) \le \operatorname{rank}(A) + \operatorname{rank}(B)\ ?$$","Let $A,B$ be matrices $m\times n$ $(A, B \in M_{m\times n}(R))$. How can we prove that $$\operatorname{rank}(A+ B) \le \operatorname{rank}(A) + \operatorname{rank}(B)\ ?$$",,"['linear-algebra', 'inequality', 'matrix-rank']"
58,Computing $A^{50}$ for a given matrix $A$,Computing  for a given matrix,A^{50} A,$A =\left(                \begin{array}{ccc}                  1 & 0 & 0 \\                  1 & 0 & 1 \\                  0 & 1 & 0 \\                \end{array}              \right)$ then what would be $A^{50}$? For real entries,$A =\left(                \begin{array}{ccc}                  1 & 0 & 0 \\                  1 & 0 & 1 \\                  0 & 1 & 0 \\                \end{array}              \right)$ then what would be $A^{50}$? For real entries,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
59,How does extending a field affect matrix similitude? [duplicate],How does extending a field affect matrix similitude? [duplicate],,"This question already has answers here : Similar matrices and field extensions (6 answers) Closed 8 years ago . Suppose we have fields $\mathbb{K}_1, \mathbb{K}_2$ such that $$\mathbb{K}_1 \subset \mathbb{K}_2.$$ Question Let $A, B$ be square $n\times n$ matrices over $\mathbb{K}_1$. If there exists $P_2 \in \mathrm{GL}(n, \mathbb{K}_2)$ s.t. $$ P_2^{-1}AP_2=B, $$ then there exists $P_1 \in \mathrm{GL}(n, \mathbb{K}_1)$ s.t.    $$ P_1^{-1}AP_1=B. $$   Is it true?","This question already has answers here : Similar matrices and field extensions (6 answers) Closed 8 years ago . Suppose we have fields $\mathbb{K}_1, \mathbb{K}_2$ such that $$\mathbb{K}_1 \subset \mathbb{K}_2.$$ Question Let $A, B$ be square $n\times n$ matrices over $\mathbb{K}_1$. If there exists $P_2 \in \mathrm{GL}(n, \mathbb{K}_2)$ s.t. $$ P_2^{-1}AP_2=B, $$ then there exists $P_1 \in \mathrm{GL}(n, \mathbb{K}_1)$ s.t.    $$ P_1^{-1}AP_1=B. $$   Is it true?",,"['linear-algebra', 'abstract-algebra', 'field-theory']"
60,"Issue with a ""proof"" for Maschke's Theorem","Issue with a ""proof"" for Maschke's Theorem",,"I came up with a ""proof"" for the Maschke's Theorem in the representation theory of finite groups that seems to make sense. But it doesn't use the fact that the group being represented is finite. So I suspect there must be something wrong with it. Can someone please help me find where it goes wrong? In the following, the underlying field is assumed to have characteristic $0$ , and all the vector spaces involved are assumed to be finite dimensional. The following form of the theorem is taken from Fulton&Harris' Representation Theory - A First Course (Proposition 1.5). Maschke's Theorem. If $W$ is a subrepresentation of a representation $V$ of a finite group $G$ , then there is a complementary invariant subspace $W'$ of $V$ , so that $V=W\oplus W'$ . Proof. Firstly, since the action of every $g\in G$ is invertible, if a subspace $V'\subset V$ is $g$ -invariant, then $g.V'=V'$ without loss of dimensions. Hence $V'$ is also $g^{-1}$ -invariant and $g^{-1}.V'=V'$ . Suppose for now $W\subset V$ is an arbitrary subspace, not necessarily invariant under the actions of any $g\in G$ . Let $\pi: V\to W$ be the corresponding linear projection, which induces a direct sum decomposition $V=W\oplus \text{Ker}(\pi)$ . Define $p_{g,\pi}:=g^{-1}\pi g$ . Then it is easy to verify $p_{g,\pi}^2=g^{-1}\pi gg^{-1}\pi g=p_{g,\pi}$ and so it is also a linear projection. This would induce an alternative direct sum  decomposition $V= \text{ran}(p_{g,\pi})\oplus \text{Ker}(p_{g,\pi})$ . We further have $g. \text{ran}(p_{g,\pi})\subset W$ . In fact, for every $x\in \text{ran}(p_{g,\pi})$ , we have $x=p_{g,\pi}(x)=g^{-1}\pi gx$ and so $gx=\pi gx\in W$ . In case $W$ is $g$ -invariant, by first paragraph, $W$ is also $g^{-1}$ -invariant. Therefore, since $\pi|_W$ is the identity map, for every $w\in W$ , we have $\text{ran}(p_{g,\pi})\ni p_{g,\pi}(w)=g^{-1}\pi gw=g^{-1}gw=w$ . Hence $W\subset \text{ran}(p_{g,\pi})$ . Then by both previous paragraphs, $g. \text{ran}(p_{g,\pi})\subset W\subset \text{ran}(p_{g,\pi})$ and so $ W= \text{ran}(p_{g,\pi})$ , since $g$ is invertible. This gives $\text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ and $p_{g,\pi}=\pi$ . That is, the two direct sum decompositions of $V$ become identical. Then, we can check that $g$ and $g^{-1}$ both preserve the two identical kernels. In fact, for every $y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ , we have $p_{g,\pi}(g^{-1}y)=g^{-1}\pi gg^{-1}y=g^{-1}\pi y=0$ , and so $g^{-1}y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ . By the invertibility of $g^{-1}$ , the two identical kernels are both $g$ -invariant and $g^{-1}$ -invariant. Lastly, if $W$ is $G$ -invariant, then every $g\in G$ induces the same projection $p_{g,\pi}=\pi$ as reasoned above, and the subspace $W':=\text{Ker}(\pi)$ is then also $G$ -invariant and complementary to $W$ such that $V=W\oplus W'$ . ${\rm \square}$ As you can see, nowhere in the ""proof"" above is the fact used that $G$ is a finite group, and that's horrifying to me... UPDATE: According to Eric's answer, the issue is that, once we have $W= \text{ran}(p_{g,\pi})$ , we cannot conclude the two projections are identical because their kernels might still be different. However, we would still have direct sum decomposition $V=W\oplus \text{Ker}(p_{g,\pi})$ . This would be true for every $g\in G$ . But the corresponding kernel would change as $g$ runs through $G$ . This is where the ""averaging technique"" comes into play, and how the finiteness of $G$ plays a key role.","I came up with a ""proof"" for the Maschke's Theorem in the representation theory of finite groups that seems to make sense. But it doesn't use the fact that the group being represented is finite. So I suspect there must be something wrong with it. Can someone please help me find where it goes wrong? In the following, the underlying field is assumed to have characteristic , and all the vector spaces involved are assumed to be finite dimensional. The following form of the theorem is taken from Fulton&Harris' Representation Theory - A First Course (Proposition 1.5). Maschke's Theorem. If is a subrepresentation of a representation of a finite group , then there is a complementary invariant subspace of , so that . Proof. Firstly, since the action of every is invertible, if a subspace is -invariant, then without loss of dimensions. Hence is also -invariant and . Suppose for now is an arbitrary subspace, not necessarily invariant under the actions of any . Let be the corresponding linear projection, which induces a direct sum decomposition . Define . Then it is easy to verify and so it is also a linear projection. This would induce an alternative direct sum  decomposition . We further have . In fact, for every , we have and so . In case is -invariant, by first paragraph, is also -invariant. Therefore, since is the identity map, for every , we have . Hence . Then by both previous paragraphs, and so , since is invertible. This gives and . That is, the two direct sum decompositions of become identical. Then, we can check that and both preserve the two identical kernels. In fact, for every , we have , and so . By the invertibility of , the two identical kernels are both -invariant and -invariant. Lastly, if is -invariant, then every induces the same projection as reasoned above, and the subspace is then also -invariant and complementary to such that . As you can see, nowhere in the ""proof"" above is the fact used that is a finite group, and that's horrifying to me... UPDATE: According to Eric's answer, the issue is that, once we have , we cannot conclude the two projections are identical because their kernels might still be different. However, we would still have direct sum decomposition . This would be true for every . But the corresponding kernel would change as runs through . This is where the ""averaging technique"" comes into play, and how the finiteness of plays a key role.","0 W V G W' V V=W\oplus W' g\in G V'\subset V g g.V'=V' V' g^{-1} g^{-1}.V'=V' W\subset V g\in G \pi: V\to W V=W\oplus \text{Ker}(\pi) p_{g,\pi}:=g^{-1}\pi g p_{g,\pi}^2=g^{-1}\pi gg^{-1}\pi g=p_{g,\pi} V= \text{ran}(p_{g,\pi})\oplus \text{Ker}(p_{g,\pi}) g. \text{ran}(p_{g,\pi})\subset W x\in \text{ran}(p_{g,\pi}) x=p_{g,\pi}(x)=g^{-1}\pi gx gx=\pi gx\in W W g W g^{-1} \pi|_W w\in W \text{ran}(p_{g,\pi})\ni p_{g,\pi}(w)=g^{-1}\pi gw=g^{-1}gw=w W\subset \text{ran}(p_{g,\pi}) g. \text{ran}(p_{g,\pi})\subset W\subset \text{ran}(p_{g,\pi})  W= \text{ran}(p_{g,\pi}) g \text{Ker}(p_{g,\pi})=\text{Ker}(\pi) p_{g,\pi}=\pi V g g^{-1} y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi) p_{g,\pi}(g^{-1}y)=g^{-1}\pi gg^{-1}y=g^{-1}\pi y=0 g^{-1}y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi) g^{-1} g g^{-1} W G g\in G p_{g,\pi}=\pi W':=\text{Ker}(\pi) G W V=W\oplus W' {\rm \square} G W= \text{ran}(p_{g,\pi}) V=W\oplus \text{Ker}(p_{g,\pi}) g\in G g G G","['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'fake-proofs']"
61,I don't know how to exactly compute this determinant,I don't know how to exactly compute this determinant,,"I've tried to compute this determinant by row transformations and column transformations, but it gives me a formula that doesn't work. The determinant is: \begin{vmatrix} x & a & b & c & d\\ a & x & b & c & d\\ a & b & x & c & d\\ a & b & c & x & d\\ a & b & c & d & x \end{vmatrix} I thought I could start doing row 5 - row 4, row 4 - row 3, row 3 - row 2 and row 2 - row 1 and then you get this determinant: \begin{vmatrix} x & a & b & c & d\\ a-x & x-a & 0 & 0 & 0\\ 0 & b-x & x-b & 0 & 0\\ 0 & 0 & c-x & x-c & 0\\ 0 & 0 & 0 & d-x & x-d \end{vmatrix} Then I made column 1 - column 2, column 2 - column 3, column 3 - column 4, column 4 - column 5 and you get: \begin{vmatrix} x-a & a-b & b-c & c-d & d\\ 0 & x-a & 0 & 0 & 0\\ 0 & 0 & x-b & 0 & 0\\ 0 & 0 & 0 & x-c & 0\\ 0 & 0 & 0 & 0 & x-d \end{vmatrix} And, as it is triangular, you can multiply the diagonal elements, so you get that the determinant is: $(x-a)^2(x-b)(x-c)(x-d)$ But this isn't correct and I don't know what to do, could someone please help me? I'd really appreciate.","I've tried to compute this determinant by row transformations and column transformations, but it gives me a formula that doesn't work. The determinant is: I thought I could start doing row 5 - row 4, row 4 - row 3, row 3 - row 2 and row 2 - row 1 and then you get this determinant: Then I made column 1 - column 2, column 2 - column 3, column 3 - column 4, column 4 - column 5 and you get: And, as it is triangular, you can multiply the diagonal elements, so you get that the determinant is: But this isn't correct and I don't know what to do, could someone please help me? I'd really appreciate.","\begin{vmatrix}
x & a & b & c & d\\
a & x & b & c & d\\
a & b & x & c & d\\
a & b & c & x & d\\
a & b & c & d & x
\end{vmatrix} \begin{vmatrix}
x & a & b & c & d\\
a-x & x-a & 0 & 0 & 0\\
0 & b-x & x-b & 0 & 0\\
0 & 0 & c-x & x-c & 0\\
0 & 0 & 0 & d-x & x-d
\end{vmatrix} \begin{vmatrix}
x-a & a-b & b-c & c-d & d\\
0 & x-a & 0 & 0 & 0\\
0 & 0 & x-b & 0 & 0\\
0 & 0 & 0 & x-c & 0\\
0 & 0 & 0 & 0 & x-d
\end{vmatrix} (x-a)^2(x-b)(x-c)(x-d)","['linear-algebra', 'solution-verification', 'determinant']"
62,$\operatorname{Lie}(G \times H)\cong \operatorname{Lie}(G)\oplus \operatorname{Lie}(H)$,,\operatorname{Lie}(G \times H)\cong \operatorname{Lie}(G)\oplus \operatorname{Lie}(H),"I am trying to solve an exercise from Lee's Introduction to smooth manifolds book . 8-23. (a) Given Lie algebras $\mathfrak g$ and $\mathfrak h$ , show that the direct sum $\mathfrak g\oplus \mathfrak h$ is a Lie algebra with the bracket defined by $$[(X, Y),(X',Y')]=([X,X'],[Y,Y']).$$ (b) Suppose $G$ and $H$ are Lie groups. Prove that $\operatorname{Lie}(G \times H)$ is isomorphic to $\operatorname{Lie}(G)\oplus \operatorname{Lie}(H)$ The first question I could solve by showing linearity of the lie bracket and the Jacobi identity, using that the jacobi identity is true in $\mathfrak g$ and $\mathfrak h$ . But how can I solve the second point?","I am trying to solve an exercise from Lee's Introduction to smooth manifolds book . 8-23. (a) Given Lie algebras and , show that the direct sum is a Lie algebra with the bracket defined by (b) Suppose and are Lie groups. Prove that is isomorphic to The first question I could solve by showing linearity of the lie bracket and the Jacobi identity, using that the jacobi identity is true in and . But how can I solve the second point?","\mathfrak g \mathfrak h \mathfrak g\oplus \mathfrak h [(X, Y),(X',Y')]=([X,X'],[Y,Y']). G H \operatorname{Lie}(G \times H) \operatorname{Lie}(G)\oplus \operatorname{Lie}(H) \mathfrak g \mathfrak h","['linear-algebra', 'differential-geometry', 'lie-groups', 'smooth-manifolds']"
63,Free vector space over a set,Free vector space over a set,,"Given a set $S$ and a field $F$ we can construct the $F$-free vector space over $S$ in the following way. Consider the set of formal sums $$FS:=\left\{\sum_{s\in S} \alpha_s s\,:\, \alpha_s=0\, \text{except for a finite number of}\, s \in S\right\}.$$ The structure of an $F$-vector space is given to $FS$ by using addition and multiplication in $F$, ie $$\sum_{s \in S} \alpha_s s+\sum_{s \in S} \beta_s s  = \sum_{s \in S}(\alpha_s+\beta_s) s,$$$$\alpha\left(\sum_{s \in S} \alpha_s s\right) :=\sum_{s\in S}(\alpha \alpha_s)s.$$ $FS$ is called free vector space over $S$. The element of $FS$ for wich $\alpha_s=1$ and $\alpha_r=0$ if $r\neq s$ is identified with $s$. This identification embeds $S$ in $FS$ and allow us to consider $S$ as a set of generators for $FS$. In fact, by definition, every element of $FS$ can be written as a linear combination of element of $S$. My question is the following: how can I prove that $S$ is a basis? I mean, how can I prove linear independence? I think we have to add  the following condition on $FS$: given $a=\sum_{s \in S} \alpha_s s, b=\sum_{s \in S} \beta_s s$ in $FS$ then $$a=b\,\text{iff}\, \alpha_s=\beta_s \, \text{for all}\, s \in S.$$ (in this way, linear independence is trivial). Is this condition necessary or not to prove linear independence for $S$? Thanks a lot in advance.","Given a set $S$ and a field $F$ we can construct the $F$-free vector space over $S$ in the following way. Consider the set of formal sums $$FS:=\left\{\sum_{s\in S} \alpha_s s\,:\, \alpha_s=0\, \text{except for a finite number of}\, s \in S\right\}.$$ The structure of an $F$-vector space is given to $FS$ by using addition and multiplication in $F$, ie $$\sum_{s \in S} \alpha_s s+\sum_{s \in S} \beta_s s  = \sum_{s \in S}(\alpha_s+\beta_s) s,$$$$\alpha\left(\sum_{s \in S} \alpha_s s\right) :=\sum_{s\in S}(\alpha \alpha_s)s.$$ $FS$ is called free vector space over $S$. The element of $FS$ for wich $\alpha_s=1$ and $\alpha_r=0$ if $r\neq s$ is identified with $s$. This identification embeds $S$ in $FS$ and allow us to consider $S$ as a set of generators for $FS$. In fact, by definition, every element of $FS$ can be written as a linear combination of element of $S$. My question is the following: how can I prove that $S$ is a basis? I mean, how can I prove linear independence? I think we have to add  the following condition on $FS$: given $a=\sum_{s \in S} \alpha_s s, b=\sum_{s \in S} \beta_s s$ in $FS$ then $$a=b\,\text{iff}\, \alpha_s=\beta_s \, \text{for all}\, s \in S.$$ (in this way, linear independence is trivial). Is this condition necessary or not to prove linear independence for $S$? Thanks a lot in advance.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'category-theory', 'free-modules']"
64,"$AB-BA=A^k$, then $A$ is not invertible",", then  is not invertible",AB-BA=A^k A,"Let $k\in\mathbb{N}$ and $A,B\in M_n(\mathbb{R})$ such that $AB-BA=A^k$. Show that $A$ is not invertible. I can show the claim if $k=1$: Assume that $A$ is invertible. We have $AB=(I_n+B)A$, so $B=A^{-1}(I_n+B)A$. Taking the trace gives a contradiction. How can I proceed for the general case?","Let $k\in\mathbb{N}$ and $A,B\in M_n(\mathbb{R})$ such that $AB-BA=A^k$. Show that $A$ is not invertible. I can show the claim if $k=1$: Assume that $A$ is invertible. We have $AB=(I_n+B)A$, so $B=A^{-1}(I_n+B)A$. Taking the trace gives a contradiction. How can I proceed for the general case?",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
65,Is dot product a kind of linear transformation,Is dot product a kind of linear transformation,,"I wonder in the field of Linear Algebra, if the dot product also referred to as an inner product: $$ \langle u,v\rangle =u\cdot v=u^Tv = u_1v_1 + u_2v_2+...+ u_nv_n, \quad \text{for} \quad u,v \in\mathbb{R}^n$$ can be categorized in a type of linear transformation. I'm quite confused here, that the definition of $\langle \cdot,\cdot \rangle: V\times V\to F$, is a map of all vectors in a vector space, which is similar to Matrix Multiplication (a way to represent linear transformation), but the properties in the inner product is a bit different from the former. It's kind of transforming from one vector space to another. So the image or codomain of this linear transformation is the inner product space, while the domain of this linear transformation is the original vector space.","I wonder in the field of Linear Algebra, if the dot product also referred to as an inner product: $$ \langle u,v\rangle =u\cdot v=u^Tv = u_1v_1 + u_2v_2+...+ u_nv_n, \quad \text{for} \quad u,v \in\mathbb{R}^n$$ can be categorized in a type of linear transformation. I'm quite confused here, that the definition of $\langle \cdot,\cdot \rangle: V\times V\to F$, is a map of all vectors in a vector space, which is similar to Matrix Multiplication (a way to represent linear transformation), but the properties in the inner product is a bit different from the former. It's kind of transforming from one vector space to another. So the image or codomain of this linear transformation is the inner product space, while the domain of this linear transformation is the original vector space.",,"['linear-algebra', 'linear-transformations', 'inner-products']"
66,Operator norm and unitary matrix,Operator norm and unitary matrix,,"I have the following product matrix $XYZ$, with $X,Y,Z$ all $n\times n$ matrices. $X$ and $Z$ are unitary matrices, i.e., they are norm preserving: for every vector $v$, we have $\|Xv \| = \|v\|$. I am trying to prove that $\| XYZ \| = \|Y\|$, with as norm chosen the operator norm. Now I am sure that I can say $\|XYZ v \| = \|YZ v\|$, as the matrix $X$ is norm preserving for every vector (including the vector $YZ v$). But I am not sure if I can say that $\|XYZ v\| = \|XY v\|$. If the latter is also the case, I'm done.","I have the following product matrix $XYZ$, with $X,Y,Z$ all $n\times n$ matrices. $X$ and $Z$ are unitary matrices, i.e., they are norm preserving: for every vector $v$, we have $\|Xv \| = \|v\|$. I am trying to prove that $\| XYZ \| = \|Y\|$, with as norm chosen the operator norm. Now I am sure that I can say $\|XYZ v \| = \|YZ v\|$, as the matrix $X$ is norm preserving for every vector (including the vector $YZ v$). But I am not sure if I can say that $\|XYZ v\| = \|XY v\|$. If the latter is also the case, I'm done.",,"['linear-algebra', 'matrices', 'functional-analysis', 'unitary-matrices']"
67,"Orthonormal basis of $\{(x_1, \dots, x_n) \mid x_1+x_2+\cdots+x_n=0\}$ [duplicate]",Orthonormal basis of  [duplicate],"\{(x_1, \dots, x_n) \mid x_1+x_2+\cdots+x_n=0\}","This question already has answers here : Find an orthonormal basis for the subspace of $\mathbb R^4$ (2 answers) Closed 7 years ago . How can we find the orthonormal basis of $\{(x_1,\dots,x_n) \in \Bbb R^n \mid x_1+x_2+\cdots+x_n=0\}$? It is easy to find a basis, but using Gram-Schmidt procedure seems difficult to obtain an orthonormal one.","This question already has answers here : Find an orthonormal basis for the subspace of $\mathbb R^4$ (2 answers) Closed 7 years ago . How can we find the orthonormal basis of $\{(x_1,\dots,x_n) \in \Bbb R^n \mid x_1+x_2+\cdots+x_n=0\}$? It is easy to find a basis, but using Gram-Schmidt procedure seems difficult to obtain an orthonormal one.",,"['linear-algebra', 'orthonormal']"
68,Eigenvector of polynomial,Eigenvector of polynomial,,"Suppose that $T: V \rightarrow V$ is an endomorphism of the linear space V (about $\mathbb{K}$) and that $p(X)$ is a polynomial with coefficients in $\mathbb{K}$. Show that if $x$ is an eigenvector of $T$ than it is also an eigenvector of $p(T)$. My attempt: So if $x$ is an eigenvector of $T$ that means that $T(x) = \lambda x$ ($\lambda$ being the eigenvalue associated to $x$). Ok so my next step is the one I feel is not correct $p (T(x)) = p (\lambda x)$ so $\lambda$ is an eigenvalue of the polynomial. I don't feel this is a correct assumption, that you can't immediately conclude this, can  we?","Suppose that $T: V \rightarrow V$ is an endomorphism of the linear space V (about $\mathbb{K}$) and that $p(X)$ is a polynomial with coefficients in $\mathbb{K}$. Show that if $x$ is an eigenvector of $T$ than it is also an eigenvector of $p(T)$. My attempt: So if $x$ is an eigenvector of $T$ that means that $T(x) = \lambda x$ ($\lambda$ being the eigenvalue associated to $x$). Ok so my next step is the one I feel is not correct $p (T(x)) = p (\lambda x)$ so $\lambda$ is an eigenvalue of the polynomial. I don't feel this is a correct assumption, that you can't immediately conclude this, can  we?",,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
69,Proof that $\mathbb{R}^+$ is a vector space,Proof that  is a vector space,\mathbb{R}^+,"I was doing some beginner linear algebra tasks and stumbled upon this one: Proove that $\mathbb{R}^+$ is a vector space over field $\mathbb{R}$ with binary operations defined as $a+b = ab$ (where $ab$ is multiplication in $\mathbb{R}$ and $\alpha *b =b^\alpha$, where $b \in \mathbb{R}$ and $\alpha \in \mathbb{R} $. It's easy to prove that $(\mathbb{R}^+,+)$ is an Abelian group and i will leave that part of proof out. However, when proving the following property of vector spaces, there seems to be a problem: $\alpha (x+y) = \alpha x +  \alpha y$ ( where $x,y \in \mathbb{R}^+$ and $\alpha \in \mathbb{R}$) By definition: $$\alpha (x+y) =(x+y)^\alpha $$ and $$\alpha x + \alpha y = x^\alpha + y ^\alpha $$ In general case:  $(x+y)^\alpha \ne x^\alpha + y^\alpha$ so this appears not to be a vector space, but even the solution in textbook states it is ( this property proof is completely omitted). Could this be author's error or did I make a mistake? If the mistake is mine, I would like to ask and additional question ,which should probably be posted in a separate thread: How would i find one base of this vector space. By defintion, I need to find a positive real number who's linear combination would generate all  positive real numbers. This is quite simple but should i use this vector space operations to form linear combinations or general multiplication and addition i.e. would a linear combination of $a \in \mathbb{R}^+$ be $b=5a$ or would that be $b=a^5$. If it's the latter, is it safe to assume that any positive number other than 1 is a base vector ?","I was doing some beginner linear algebra tasks and stumbled upon this one: Proove that $\mathbb{R}^+$ is a vector space over field $\mathbb{R}$ with binary operations defined as $a+b = ab$ (where $ab$ is multiplication in $\mathbb{R}$ and $\alpha *b =b^\alpha$, where $b \in \mathbb{R}$ and $\alpha \in \mathbb{R} $. It's easy to prove that $(\mathbb{R}^+,+)$ is an Abelian group and i will leave that part of proof out. However, when proving the following property of vector spaces, there seems to be a problem: $\alpha (x+y) = \alpha x +  \alpha y$ ( where $x,y \in \mathbb{R}^+$ and $\alpha \in \mathbb{R}$) By definition: $$\alpha (x+y) =(x+y)^\alpha $$ and $$\alpha x + \alpha y = x^\alpha + y ^\alpha $$ In general case:  $(x+y)^\alpha \ne x^\alpha + y^\alpha$ so this appears not to be a vector space, but even the solution in textbook states it is ( this property proof is completely omitted). Could this be author's error or did I make a mistake? If the mistake is mine, I would like to ask and additional question ,which should probably be posted in a separate thread: How would i find one base of this vector space. By defintion, I need to find a positive real number who's linear combination would generate all  positive real numbers. This is quite simple but should i use this vector space operations to form linear combinations or general multiplication and addition i.e. would a linear combination of $a \in \mathbb{R}^+$ be $b=5a$ or would that be $b=a^5$. If it's the latter, is it safe to assume that any positive number other than 1 is a base vector ?",,"['linear-algebra', 'vector-spaces']"
70,"Matrix having to be orthogonal, knowing it's norm-preserving","Matrix having to be orthogonal, knowing it's norm-preserving",,"If we know that a real $m \times m$ matrix $C$ is norm-preserving, $||C\textbf{v}|| = ||\textbf{v}||$, then $C$ has to be orthogonal. Why should this be the case?","If we know that a real $m \times m$ matrix $C$ is norm-preserving, $||C\textbf{v}|| = ||\textbf{v}||$, then $C$ has to be orthogonal. Why should this be the case?",,['linear-algebra']
71,If $A$ and $B$ are $n×n$ matrices such that $AB=B$ and $BA=A$ then find the value of $A^{4} + B^{4} - A^{2} -B^ {2} + I$,If  and  are  matrices such that  and  then find the value of,A B n×n AB=B BA=A A^{4} + B^{4} - A^{2} -B^ {2} + I,"The given question is If $A$ and $B$ are $n×n$ matrices such that $AB=B$ and $BA=A$, then find the value of $A^{4} + B^{4} - A^{2} -B^ {2} + I$. Any hints?","The given question is If $A$ and $B$ are $n×n$ matrices such that $AB=B$ and $BA=A$, then find the value of $A^{4} + B^{4} - A^{2} -B^ {2} + I$. Any hints?",,"['linear-algebra', 'matrices']"
72,Are primitive row stochastic matrices diagonalizable?,Are primitive row stochastic matrices diagonalizable?,,"Let $A$ be an $n \times n$ matrix with real, non-negative entries. Assume $A$ is primitive, meaning there exists an integer $k$ such that $A^k>0$ (here the inequality means all entries in $A$ are positive). Also, assume $A$ is row stochastic, meaning that the the entries of each row sum to 1. An alternate way of stating the above is to say: let $A$ be the transition probability matrix of an irreducible, aperiodic Markov chain. Is $A$ diagonalizable? Thank you in advance for any proofs or counterexamples.","Let $A$ be an $n \times n$ matrix with real, non-negative entries. Assume $A$ is primitive, meaning there exists an integer $k$ such that $A^k>0$ (here the inequality means all entries in $A$ are positive). Also, assume $A$ is row stochastic, meaning that the the entries of each row sum to 1. An alternate way of stating the above is to say: let $A$ be the transition probability matrix of an irreducible, aperiodic Markov chain. Is $A$ diagonalizable? Thank you in advance for any proofs or counterexamples.",,"['linear-algebra', 'matrices', 'examples-counterexamples', 'markov-chains']"
73,Understanding the significance of row space and column space basis,Understanding the significance of row space and column space basis,,"I've just learned about the row and column space basis and I'm confused about what the significance of each is. My professor basically hasn't said much and has danced around any direct questions on how these things relate. So what I know is this: The column space basis is solved by taking a spanning set of vectors and forming matrix A, doing RREF(A), looking at the non-zero columns (linearly independent columns) and then relating that back to the original matrix A. The corresponding columns in the original matrix A form the column basis for A. This makes sense because you can see that any column in that matrix can be formed from some linear combination of the basis. Now when it comes to row space I can mechanically do it. Transpose A, reduce it to echelon form (not RREF), transpose it again, and read off the columns. In this case, you do not go back to the original matrix. My questions are: why don't we go back to the original matrix? Why do we only reduce it to echelon form and then read off the rows? How does this basis relate to the rows of the original matrix A? Sorry if these are elementary question. I feel like I can mechanically (and begrudgingly) do it but I really want to understand and appreciate it. Thank you!","I've just learned about the row and column space basis and I'm confused about what the significance of each is. My professor basically hasn't said much and has danced around any direct questions on how these things relate. So what I know is this: The column space basis is solved by taking a spanning set of vectors and forming matrix A, doing RREF(A), looking at the non-zero columns (linearly independent columns) and then relating that back to the original matrix A. The corresponding columns in the original matrix A form the column basis for A. This makes sense because you can see that any column in that matrix can be formed from some linear combination of the basis. Now when it comes to row space I can mechanically do it. Transpose A, reduce it to echelon form (not RREF), transpose it again, and read off the columns. In this case, you do not go back to the original matrix. My questions are: why don't we go back to the original matrix? Why do we only reduce it to echelon form and then read off the rows? How does this basis relate to the rows of the original matrix A? Sorry if these are elementary question. I feel like I can mechanically (and begrudgingly) do it but I really want to understand and appreciate it. Thank you!",,"['linear-algebra', 'matrices', 'vector-spaces']"
74,Relation between linear maps and matrices,Relation between linear maps and matrices,,"I've been reading Axler's ""Linear Algebra Done Right"", and have learned more about linear operators/ maps, but I'd like to make sure that I understand how to properly relate this information to matrices. First, any $m \times n$ matrix with entries in a field $F$ uniquely determines a linear transformation $T: F^n\to F^m$ by $(x_1,...,x_n)\mapsto (\sum_{j=1}^{n}a_{1j}x_j,...,\sum_{j=1}^{n}a_{mj}x_j)$, and if $T:V\to W$ is a linear map between finite dimensional vector space $V$ and $W$ over field $F$ and we fix a basis $B_1$ in $V$ and $B_2$ in $W$, with $dim(V)=n$ and $dim(W)=m$, then $M(T)$ (function that maps $T$ to its matrix with respect to $B_1$ and $B_2$ by the procedure outlined above) is an isomorphism between $L(V,W)$ (vector space of linear maps between $V$ and $W$) and $M$ at $(m,n,F)$ (vector space of $m \times n$ matrices with entries in $F$). QUESTIONS 1.) If I'm given a matrix with entries in $F$, how exactly would I go about determining information about it from linear maps? For example, suppose that i'm given an $n \times n$ matrix $A$ where $n$ is odd. I know that if $T$ is an operator on an odd-dimensional vector space $V$, then $T$ has an eigenvalue. Since $T$ can be any operator on any odd-dimensional vector space, can I just pick $V=F^n$? Then I could say that $A$ represents a unique linear transformation $T:F^n\to F^n$ by the assignment above, and since $T$ has an eigenvalue, $A$ must also have (the same) eigenvalue (since $T$ and $A$ are the same transformation on $F^n$)? Furthermore, if I assume the standard basis for $F^n$, then there is no other such matrix ($m \times n$ and entries in $F$) that represents the particular operator that $A$ does, and every possible operator on $F^n$ is represented by a matrix in $M$ at $(n,n,F)$. Is this all correct? Would similar arguments apply to invertibility, similarity, etc. ? 2.) Since an $m \times n$ matrix $A$ with entries in $F$ represents a unique linear map $T: F^n\to F^m$, and since finite dimensional vector spaces with the same dimension are isomorphic, can't I also interpret $A$ as representing a unique linear map $T: V\to W$, where $V$ and $W$ are any vector spaces over $F$ s.t. $dim(V)=n$ and $dim(W)=m$?","I've been reading Axler's ""Linear Algebra Done Right"", and have learned more about linear operators/ maps, but I'd like to make sure that I understand how to properly relate this information to matrices. First, any $m \times n$ matrix with entries in a field $F$ uniquely determines a linear transformation $T: F^n\to F^m$ by $(x_1,...,x_n)\mapsto (\sum_{j=1}^{n}a_{1j}x_j,...,\sum_{j=1}^{n}a_{mj}x_j)$, and if $T:V\to W$ is a linear map between finite dimensional vector space $V$ and $W$ over field $F$ and we fix a basis $B_1$ in $V$ and $B_2$ in $W$, with $dim(V)=n$ and $dim(W)=m$, then $M(T)$ (function that maps $T$ to its matrix with respect to $B_1$ and $B_2$ by the procedure outlined above) is an isomorphism between $L(V,W)$ (vector space of linear maps between $V$ and $W$) and $M$ at $(m,n,F)$ (vector space of $m \times n$ matrices with entries in $F$). QUESTIONS 1.) If I'm given a matrix with entries in $F$, how exactly would I go about determining information about it from linear maps? For example, suppose that i'm given an $n \times n$ matrix $A$ where $n$ is odd. I know that if $T$ is an operator on an odd-dimensional vector space $V$, then $T$ has an eigenvalue. Since $T$ can be any operator on any odd-dimensional vector space, can I just pick $V=F^n$? Then I could say that $A$ represents a unique linear transformation $T:F^n\to F^n$ by the assignment above, and since $T$ has an eigenvalue, $A$ must also have (the same) eigenvalue (since $T$ and $A$ are the same transformation on $F^n$)? Furthermore, if I assume the standard basis for $F^n$, then there is no other such matrix ($m \times n$ and entries in $F$) that represents the particular operator that $A$ does, and every possible operator on $F^n$ is represented by a matrix in $M$ at $(n,n,F)$. Is this all correct? Would similar arguments apply to invertibility, similarity, etc. ? 2.) Since an $m \times n$ matrix $A$ with entries in $F$ represents a unique linear map $T: F^n\to F^m$, and since finite dimensional vector spaces with the same dimension are isomorphic, can't I also interpret $A$ as representing a unique linear map $T: V\to W$, where $V$ and $W$ are any vector spaces over $F$ s.t. $dim(V)=n$ and $dim(W)=m$?",,"['linear-algebra', 'matrices']"
75,Showing $K[u]$ is a field when $u$ is algebraic over $K$.,Showing  is a field when  is algebraic over .,K[u] u K,"Let $K$ be a field, and let $u$ be algebraic over $K$.  Show that $K[u]$ is a field. Context/progress so far:  This is a generalization of a problem in the first few pages of Peterson's ""Linear Algebra.""  If possible, I'd like to avoid using the machinery from abstract algebra and show directly that elements of $K[u]$ are invertible.  I can show that $\{1, u, ..., u^{n-1}\}$ is a basis for $K[u]$ over $K$, where $n$ is the degree of the minimal polynomial of $u$.  Using the same polynomial, I can show that $u^{-1}$ exists.  But I'm having trouble inverting general elements of $K[u]$.  Any hints?","Let $K$ be a field, and let $u$ be algebraic over $K$.  Show that $K[u]$ is a field. Context/progress so far:  This is a generalization of a problem in the first few pages of Peterson's ""Linear Algebra.""  If possible, I'd like to avoid using the machinery from abstract algebra and show directly that elements of $K[u]$ are invertible.  I can show that $\{1, u, ..., u^{n-1}\}$ is a basis for $K[u]$ over $K$, where $n$ is the degree of the minimal polynomial of $u$.  Using the same polynomial, I can show that $u^{-1}$ exists.  But I'm having trouble inverting general elements of $K[u]$.  Any hints?",,"['linear-algebra', 'abstract-algebra']"
76,Solution of a Sylvester equation?,Solution of a Sylvester equation?,,"I'd like to solve $AX -BX + XC = D$, for the matrix $X$, where all matrices have real entries and $X$ is a rectangular matrix, while $B$ and $C$ are symmetric matrices and $A$ is formed by an outer product matrix (i.e, as $vv^T$ for some real vector $v$) while $D$ is 'not' symmetric. $A,B,C,D$ matrices are fixed while $X$ is the unknown. How can this equation be solved? Secondly, is there any case, where the solution of this equation has a closed form?","I'd like to solve $AX -BX + XC = D$, for the matrix $X$, where all matrices have real entries and $X$ is a rectangular matrix, while $B$ and $C$ are symmetric matrices and $A$ is formed by an outer product matrix (i.e, as $vv^T$ for some real vector $v$) while $D$ is 'not' symmetric. $A,B,C,D$ matrices are fixed while $X$ is the unknown. How can this equation be solved? Secondly, is there any case, where the solution of this equation has a closed form?",,"['linear-algebra', 'matrices', 'optimization', 'numerical-methods', 'control-theory']"
77,How to calculate the cost of Cholesky decomposition?,How to calculate the cost of Cholesky decomposition?,,The cost of Cholesky decomposition is $n^3/3$ flops (A is a $n \times n$ matrix). Could anyone show me some steps to get this number? Thank you very much.,The cost of Cholesky decomposition is $n^3/3$ flops (A is a $n \times n$ matrix). Could anyone show me some steps to get this number? Thank you very much.,,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'cholesky-decomposition']"
78,Why is an alternating $2$-form decomposable if and only if its self-wedge vanishes?,Why is an alternating -form decomposable if and only if its self-wedge vanishes?,2,"Given a vector space $V$, and a $2$-tensor $w$ in the second exterior power $\Lambda^2 V$. Assume that $w \wedge w=0$. Why is $w$ decomposable? Thanks for your help!","Given a vector space $V$, and a $2$-tensor $w$ in the second exterior power $\Lambda^2 V$. Assume that $w \wedge w=0$. Why is $w$ decomposable? Thanks for your help!",,"['linear-algebra', 'abstract-algebra']"
79,"Is the set $\{\frac{1}{a\,-\,\pi}\mid a\in\mathbb{Q}\}$ linearly independent over $\mathbb{Q}$?",Is the set  linearly independent over ?,"\{\frac{1}{a\,-\,\pi}\mid a\in\mathbb{Q}\} \mathbb{Q}",The following problem is from Golan's linear algebra book. I have posted a proposed solution in the answers. Problem: Consider $\mathbb{R}$ as a vector space over $\mathbb{Q}$. Is the subset $$\left\{\frac{1}{a-\pi}\;\middle\vert\; a\in\mathbb{Q}\right\}$$ linearly independent?,The following problem is from Golan's linear algebra book. I have posted a proposed solution in the answers. Problem: Consider $\mathbb{R}$ as a vector space over $\mathbb{Q}$. Is the subset $$\left\{\frac{1}{a-\pi}\;\middle\vert\; a\in\mathbb{Q}\right\}$$ linearly independent?,,['linear-algebra']
80,Symmetric Matrix as the Difference of Two Positive Definite Symmetric Matrices,Symmetric Matrix as the Difference of Two Positive Definite Symmetric Matrices,,"Prove that any real symmetric matrix can be expressed as the difference of two   positive definite symmetric matrices. I was trying to use the fact that real symmetric matrices are diagonalisable , but the confusion I am having is that 'if $A$ be invertible and $B$ be a positive definite diagonal matrix, then is $ABA^{-1}$ positive definite' . Thanks for any help .","Prove that any real symmetric matrix can be expressed as the difference of two   positive definite symmetric matrices. I was trying to use the fact that real symmetric matrices are diagonalisable , but the confusion I am having is that 'if $A$ be invertible and $B$ be a positive definite diagonal matrix, then is $ABA^{-1}$ positive definite' . Thanks for any help .",,"['linear-algebra', 'matrices']"
81,Finding all scalars $k$ such that $\| kv \| = 10$,Finding all scalars  such that,k \| kv \| = 10,"I have a homework question that asking to Find all scalars $k$ such that $\|kv\| = 10$ when $v=(1,-4,6)$. What I did that that I found the norm of $v$ which I found to be $\sqrt{53}$. Then I took that answer and multiplied by $k$ to get $10$ like this: $\sqrt{53}\cdot k=10$, $$ k=\frac{10}{\sqrt{53}} $$ I don't think this approach is right because it doesn't deal with the $k$ being calculated within the norm. I just don't know how to do it that was so any pointers would be greatly appreciated.","I have a homework question that asking to Find all scalars $k$ such that $\|kv\| = 10$ when $v=(1,-4,6)$. What I did that that I found the norm of $v$ which I found to be $\sqrt{53}$. Then I took that answer and multiplied by $k$ to get $10$ like this: $\sqrt{53}\cdot k=10$, $$ k=\frac{10}{\sqrt{53}} $$ I don't think this approach is right because it doesn't deal with the $k$ being calculated within the norm. I just don't know how to do it that was so any pointers would be greatly appreciated.",,"['linear-algebra', 'normed-spaces']"
82,What's bad about left $\mathbb{H}$-modules?,What's bad about left -modules?,\mathbb{H},"Can you give me non-trivial examples of propositions that can be formulated for every left $k$-module, hold whenever $k$ is a field, but do not hold when $k = \mathbb{H}$ or, more generally, need not hold when $k$ is a division ring (thanks, Bruno Stonek!) which is not a field? I'm asking because in the theory of vector bundles $\mathbb{H}$-bundles are usually considered alongside those over $\mathbb{R}$ and $\mathbb{C}$, and I'd like to know what to watch out for in this particular case.","Can you give me non-trivial examples of propositions that can be formulated for every left $k$-module, hold whenever $k$ is a field, but do not hold when $k = \mathbb{H}$ or, more generally, need not hold when $k$ is a division ring (thanks, Bruno Stonek!) which is not a field? I'm asking because in the theory of vector bundles $\mathbb{H}$-bundles are usually considered alongside those over $\mathbb{R}$ and $\mathbb{C}$, and I'd like to know what to watch out for in this particular case.",,"['linear-algebra', 'algebraic-topology']"
83,What are the counter examples books for Abstract algebra?,What are the counter examples books for Abstract algebra?,,"In mathematics field which book best for counter examples of group theory, linear algebra?","In mathematics field which book best for counter examples of group theory, linear algebra?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'examples-counterexamples', 'book-recommendation']"
84,"$D: \Bbb R[x]\to \Bbb R[x]$ satisfies $D(fg)=D(f)g+fD(g), D(x)=1$.Can we show that $D$ is linear?",satisfies .Can we show that  is linear?,"D: \Bbb R[x]\to \Bbb R[x] D(fg)=D(f)g+fD(g), D(x)=1 D","$D: \Bbb R[x]\to \Bbb R[x]$ satisfies $D(fg)=D(f)g+fD(g), D(x)=1$ .Can we show that $D$ is linear? Clearly, we have $D(x^k)=kx^{k-1}$ , $D(1)=0$ . But these two properties can ensure that $D$ is linear?","satisfies .Can we show that is linear? Clearly, we have , . But these two properties can ensure that is linear?","D: \Bbb R[x]\to \Bbb R[x] D(fg)=D(f)g+fD(g), D(x)=1 D D(x^k)=kx^{k-1} D(1)=0 D","['linear-algebra', 'derivatives', 'polynomials']"
85,Two eigenvalues and an eigenvector walk into a bar...,Two eigenvalues and an eigenvector walk into a bar...,,"Suppose I have the transformation $T(v) = Av = \lambda v$ . If two of the eigenvalues are $\lambda_1$ and $\lambda_2$ where $\lambda_1=-\lambda_2$ , is there a way to quickly find the eigenvector(s) for $\lambda_2$ if I know the eigenvector(s) for $\lambda_1$ ? I ask this because attempting to find the kernel of larger matrices with irrational numbers and writing down each step is time consuming. I noticed a subtle relationship between the eigenvectors for eigenvalues of opposite signs in that the entries are the same with the exception of a negative sign somewhere. Any assistance is much appreciated!","Suppose I have the transformation . If two of the eigenvalues are and where , is there a way to quickly find the eigenvector(s) for if I know the eigenvector(s) for ? I ask this because attempting to find the kernel of larger matrices with irrational numbers and writing down each step is time consuming. I noticed a subtle relationship between the eigenvectors for eigenvalues of opposite signs in that the entries are the same with the exception of a negative sign somewhere. Any assistance is much appreciated!",T(v) = Av = \lambda v \lambda_1 \lambda_2 \lambda_1=-\lambda_2 \lambda_2 \lambda_1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,How many negative eigenvalues can $AB + BA$ have when $A$ and $B$ are symmetric positive definite?,How many negative eigenvalues can  have when  and  are symmetric positive definite?,AB + BA A B,"Let $A, B \in \mathbb R^{n\times n}$ symmetric positive definite. Clearly, any eigenvector $v$ of either $A$ or $B$ is such that (taking for example a nonzero eigenvector of $A$ with eigenvalue $\lambda$ ) $$ v^T(AB + BA) v = 2 \lambda (v^TBv) > 0, $$ so $AB + BA$ has at least one positive eigenvalue. From this answer , we conclude that, in dimension 2, $AB + BA$ has at most one negative eigenvalue. What is the situation when $n$ is general? Can we have more than $\lfloor n/2\rfloor$ negative eigenvalues? (the value $\lfloor n/2\rfloor$ can be obtained by taking block diagonal matrices with 2x2 blocks corresponding to the 2-dimensional example.)","Let symmetric positive definite. Clearly, any eigenvector of either or is such that (taking for example a nonzero eigenvector of with eigenvalue ) so has at least one positive eigenvalue. From this answer , we conclude that, in dimension 2, has at most one negative eigenvalue. What is the situation when is general? Can we have more than negative eigenvalues? (the value can be obtained by taking block diagonal matrices with 2x2 blocks corresponding to the 2-dimensional example.)","A, B \in \mathbb R^{n\times n} v A B A \lambda 
v^T(AB + BA) v = 2 \lambda (v^TBv) > 0,
 AB + BA AB + BA n \lfloor n/2\rfloor \lfloor n/2\rfloor","['linear-algebra', 'matrices']"
87,"What is the ""determinant"" of two vectors?","What is the ""determinant"" of two vectors?",,"I came across the notation $\det(v,w)$ where $v$ and $w$ are vectors. Specifically, it was about the curvature of a plane curve: $$\kappa (t) = \frac{\det(\gamma'(t), \gamma''(t)) }{\|\gamma'(t)\|^3}$$ What is it supposed to mean?","I came across the notation where and are vectors. Specifically, it was about the curvature of a plane curve: What is it supposed to mean?","\det(v,w) v w \kappa (t) = \frac{\det(\gamma'(t), \gamma''(t)) }{\|\gamma'(t)\|^3}","['linear-algebra', 'differential-geometry', 'notation', 'determinant']"
88,How to reduce matrix into row echelon form in NumPy?,How to reduce matrix into row echelon form in NumPy?,,"I'm working on a linear algebra homework for a data science class. I'm suppose to make this matrix into row echelon form but I'm stuck. Here's the current output I would like to get rid of -0.75, 0.777, and 1.333 in A[2,0], A[3,0], and A[3,1] respectively; they should be zeroed out. Below is my current code... can anybody please nudge me in the right direction and tell me what step I'm missing? import numpy as np  def fixRowTwo(A) :      # Sets the sub-diagonal elements of row two to zero     A[2] = A[2] - A[2,0] * A[1]     A[2] = A[2] - A[2,1] * A[1]      # Test if diagonal element is not zero.     if A[2,2] == 0 :         # Add a lower row to row two.         A[2] = A[2] + A[3]          # Sets the sub-diagonal elements to zero again ???         A[2] = A[2] - A[2,0] * A[1]         A[2] = A[2] - A[2,1] * A[1]      if A[2,2] == 0 :         print(""S I N G U L A R"")         sys.Exit()      # Set the diagonal element to one     A[2] = A[2] / A[2,2]      return A  def fixRowThree(A) :      # Sets the sub-diagonal elements of row two to zero     A[3] = A[3] - A[3,0] * A[2]     A[3] = A[3] - A[3,1] * A[2]     A[3] = A[3] - A[3,2] * A[2]          # Test if diagonal element is not zero.     if A[3,3] == 0:         print(""S I N G U L A R"")         sys.Exit()      # Set the diagonal element to one     A[3] = A[3] / A[3,3]      return A  A = np.array([         [1, 7, 4, 3],         [0, 1, 2, 3],         [3, 2, 0, 3],         [1, 3, 1, 3]     ], dtype=np.float_)  fixRowTwo(A) print("""") print(""Row Two:"") print(A)  fixRowThree(A) print("""") print(""Row Three:"") print(A)","I'm working on a linear algebra homework for a data science class. I'm suppose to make this matrix into row echelon form but I'm stuck. Here's the current output I would like to get rid of -0.75, 0.777, and 1.333 in A[2,0], A[3,0], and A[3,1] respectively; they should be zeroed out. Below is my current code... can anybody please nudge me in the right direction and tell me what step I'm missing? import numpy as np  def fixRowTwo(A) :      # Sets the sub-diagonal elements of row two to zero     A[2] = A[2] - A[2,0] * A[1]     A[2] = A[2] - A[2,1] * A[1]      # Test if diagonal element is not zero.     if A[2,2] == 0 :         # Add a lower row to row two.         A[2] = A[2] + A[3]          # Sets the sub-diagonal elements to zero again ???         A[2] = A[2] - A[2,0] * A[1]         A[2] = A[2] - A[2,1] * A[1]      if A[2,2] == 0 :         print(""S I N G U L A R"")         sys.Exit()      # Set the diagonal element to one     A[2] = A[2] / A[2,2]      return A  def fixRowThree(A) :      # Sets the sub-diagonal elements of row two to zero     A[3] = A[3] - A[3,0] * A[2]     A[3] = A[3] - A[3,1] * A[2]     A[3] = A[3] - A[3,2] * A[2]          # Test if diagonal element is not zero.     if A[3,3] == 0:         print(""S I N G U L A R"")         sys.Exit()      # Set the diagonal element to one     A[3] = A[3] / A[3,3]      return A  A = np.array([         [1, 7, 4, 3],         [0, 1, 2, 3],         [3, 2, 0, 3],         [1, 3, 1, 3]     ], dtype=np.float_)  fixRowTwo(A) print("""") print(""Row Two:"") print(A)  fixRowThree(A) print("""") print(""Row Three:"") print(A)",,"['linear-algebra', 'matrices', 'python', 'gaussian-elimination']"
89,How to split up a fraction with a sum in the denominator?,How to split up a fraction with a sum in the denominator?,,"How would you split up the fraction $$\frac{x}{a+b}$$ (or $$\frac{1}{a+b}$$ )  so one fraction has $x$ and $a$ in it, only and another one has $x$ and $b$ , only?","How would you split up the fraction (or )  so one fraction has and in it, only and another one has and , only?",\frac{x}{a+b} \frac{1}{a+b} x a x b,['linear-algebra']
90,LU factorization of a nonsingular matrix exists if and only if all leading principal submatrices are nonsingular.,LU factorization of a nonsingular matrix exists if and only if all leading principal submatrices are nonsingular.,,"I'm struggling to prove this theorem. I can prove that if the $LU$ factorization exists, then the leading principal submatrices are nonsingular. To do that, I can show that the determinant of every leading principal submatrix is not zero. (The leading principal submatrix is the product of $L$ and $U$ corresponding leading principal submatrices , and determinant of every $L$ leading principal submatrix is $1$ and determinant of the $U$ leading principal submatrix is product of the diagonal elements). To prove that if the leading principal submatrices are nonsingular, then $LU$ factorization exists, I believe I should use induction, but I'm getting nowhere. Can anyone help me with the proof?","I'm struggling to prove this theorem. I can prove that if the factorization exists, then the leading principal submatrices are nonsingular. To do that, I can show that the determinant of every leading principal submatrix is not zero. (The leading principal submatrix is the product of and corresponding leading principal submatrices , and determinant of every leading principal submatrix is and determinant of the leading principal submatrix is product of the diagonal elements). To prove that if the leading principal submatrices are nonsingular, then factorization exists, I believe I should use induction, but I'm getting nowhere. Can anyone help me with the proof?",LU L U L 1 U LU,"['linear-algebra', 'matrix-decomposition']"
91,Solve the equation $X^2+X=\text{a given matrix}$,Solve the equation,X^2+X=\text{a given matrix},I want to solve the quadratic matrix equation $$X^2+X=\begin{pmatrix}1&1\\1&1\end{pmatrix}$$ If I put $X$ in the form $$X=\begin{pmatrix}a&b\\c&d\end{pmatrix}$$ then I find complicated equations. Is there a simple way to tackle the problem without using diagonalization?,I want to solve the quadratic matrix equation $$X^2+X=\begin{pmatrix}1&1\\1&1\end{pmatrix}$$ If I put $X$ in the form $$X=\begin{pmatrix}a&b\\c&d\end{pmatrix}$$ then I find complicated equations. Is there a simple way to tackle the problem without using diagonalization?,,"['linear-algebra', 'matrices', 'systems-of-equations', 'quadratics', 'matrix-equations']"
92,How to determine the equation of the hyperplane that contains several points,How to determine the equation of the hyperplane that contains several points,,"I have a question regarding the computation of a hyperplane equation (especially the orthogonal) given n points, where n>3. For lower dimensional cases, the computation is done as in : http://tutorial.math.lamar.edu/Classes/CalcIII/EqnsOfPlanes.aspx Here we simply use the cross product for determining the orthogonal. However, best of our knowledge the cross product computation via determinants is limited to dimension 7 (?). How to get the orthogonal to compute the hessian normal form in higher dimensions? Thank you in advance for any hints and with best regards Dan","I have a question regarding the computation of a hyperplane equation (especially the orthogonal) given n points, where n>3. For lower dimensional cases, the computation is done as in : http://tutorial.math.lamar.edu/Classes/CalcIII/EqnsOfPlanes.aspx Here we simply use the cross product for determining the orthogonal. However, best of our knowledge the cross product computation via determinants is limited to dimension 7 (?). How to get the orthogonal to compute the hessian normal form in higher dimensions? Thank you in advance for any hints and with best regards Dan",,"['linear-algebra', 'vectors', 'orthogonality']"
93,How to find Projection matrix onto the subspace,How to find Projection matrix onto the subspace,,I need help finding the projection of a matrix onto the subspace $W \subset \mathbb{R}^3$ given by the equation $x+y+z=0$ I am not sure what to start with to answer this question. Do I use projection formula? Many thanks in advance.,I need help finding the projection of a matrix onto the subspace $W \subset \mathbb{R}^3$ given by the equation $x+y+z=0$ I am not sure what to start with to answer this question. Do I use projection formula? Many thanks in advance.,,"['linear-algebra', 'linear-transformations', 'projection']"
94,Prove that $\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2$,Prove that,\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2,"Let $a, b, c, d \in \mathbb K$ where $\mathbb K$ is a field. Prove that $$\det \begin{bmatrix} a & -b & -c & -d\\  b & a & -d & c\\  c & d & a & -b\\  d & -c & b & a \end{bmatrix} = (a^2+b^2+c^2+d^2)^2$$ I'm looking for a smart way to solve this problem. If we denote $$A = \begin{bmatrix} a & -b \\  b & a \\  \end{bmatrix}$$ and $$B = \begin{bmatrix} -c & -d \\  -d & c \\  \end{bmatrix}$$ we have that $$ \begin{bmatrix} a & -b & -c & -d\\  b & a & -d & c\\  c & d & a & -b\\  d & -c & b & a \end{bmatrix} = \begin{bmatrix} A & B \\  -B & A \\  \end{bmatrix} $$ So it's sufficient to proof that $$ \det \begin{bmatrix} A & B \\  -B & A \\  \end{bmatrix} = (\det A - \det B)^2. $$ Help?","Let $a, b, c, d \in \mathbb K$ where $\mathbb K$ is a field. Prove that $$\det \begin{bmatrix} a & -b & -c & -d\\  b & a & -d & c\\  c & d & a & -b\\  d & -c & b & a \end{bmatrix} = (a^2+b^2+c^2+d^2)^2$$ I'm looking for a smart way to solve this problem. If we denote $$A = \begin{bmatrix} a & -b \\  b & a \\  \end{bmatrix}$$ and $$B = \begin{bmatrix} -c & -d \\  -d & c \\  \end{bmatrix}$$ we have that $$ \begin{bmatrix} a & -b & -c & -d\\  b & a & -d & c\\  c & d & a & -b\\  d & -c & b & a \end{bmatrix} = \begin{bmatrix} A & B \\  -B & A \\  \end{bmatrix} $$ So it's sufficient to proof that $$ \det \begin{bmatrix} A & B \\  -B & A \\  \end{bmatrix} = (\det A - \det B)^2. $$ Help?",,"['linear-algebra', 'matrices', 'determinant']"
95,Show orthogonal projection is minimal distance,Show orthogonal projection is minimal distance,,"In a textbook exercise a bunch of vector spaces were given along with subspaces and a vector $v$. Then the task was to compute the orthogonal projection $\pi(v)$ to that subspace $U$ with their respective scalar products. Simple enough, now the task after that that asked: Compute the distance $$d(v,U):=min_{u\in U} ||v-u||$$ for all the spaces given in the previous task. Well it's the orthogonal projection we computed above! But how can one prove this? Especially when you move to more abstract spaces like the space of continuous functions, it does not seem that obvious anymore. So how can we prove it? Maybe we can show $||v-\pi(v)||^2=\gamma(v-\pi(v),v-\pi(v))$ is minimal ( where $\gamma$ is the inner product). I don't know how you would approach showing it is the minimal distance. Finding a minimum seems more like a calculus problem! Related questions often state that this is the case, but do not show a proof, see for example : Projection of v onto orthogonal subspaces are the those with minmum distance to v?","In a textbook exercise a bunch of vector spaces were given along with subspaces and a vector $v$. Then the task was to compute the orthogonal projection $\pi(v)$ to that subspace $U$ with their respective scalar products. Simple enough, now the task after that that asked: Compute the distance $$d(v,U):=min_{u\in U} ||v-u||$$ for all the spaces given in the previous task. Well it's the orthogonal projection we computed above! But how can one prove this? Especially when you move to more abstract spaces like the space of continuous functions, it does not seem that obvious anymore. So how can we prove it? Maybe we can show $||v-\pi(v)||^2=\gamma(v-\pi(v),v-\pi(v))$ is minimal ( where $\gamma$ is the inner product). I don't know how you would approach showing it is the minimal distance. Finding a minimum seems more like a calculus problem! Related questions often state that this is the case, but do not show a proof, see for example : Projection of v onto orthogonal subspaces are the those with minmum distance to v?",,"['linear-algebra', 'vector-spaces', 'orthogonality']"
96,If $A^2=B^2=0$ and $AB=BA$ then $(A+B)^2=0$,If  and  then,A^2=B^2=0 AB=BA (A+B)^2=0,"I've been given a class assignment to try and prove or disprove the following: $A^2=B^2=0$ where $A,B \in M_n(\mathbb R)$ for $n≥2$ .    If $AB=BA$, then is $(A+B)^2=0$ ? I've been trying to prove this by saying that given the above conditions, $(A+B)^2 = 0$ is always true, $0=0$. My attempt: $$(A+B)^2 = A^2 + AB + BA + B^2 = AB + BA = 2AB$$ Now because: $(A+B)^2=0$ then: $2AB=0 \implies AB=0 $ but because $A^2=B^2=0$ then if we multiply by A on the left side we get: $AAB = A0 \implies A^2B  = 0 \implies 0 = 0$. Did I prove this correctly? or am I not allowed to multiply by $A$ when there is $0$ on one side? I tried disproving it and just could not find any counter-examples that worked.","I've been given a class assignment to try and prove or disprove the following: $A^2=B^2=0$ where $A,B \in M_n(\mathbb R)$ for $n≥2$ .    If $AB=BA$, then is $(A+B)^2=0$ ? I've been trying to prove this by saying that given the above conditions, $(A+B)^2 = 0$ is always true, $0=0$. My attempt: $$(A+B)^2 = A^2 + AB + BA + B^2 = AB + BA = 2AB$$ Now because: $(A+B)^2=0$ then: $2AB=0 \implies AB=0 $ but because $A^2=B^2=0$ then if we multiply by A on the left side we get: $AAB = A0 \implies A^2B  = 0 \implies 0 = 0$. Did I prove this correctly? or am I not allowed to multiply by $A$ when there is $0$ on one side? I tried disproving it and just could not find any counter-examples that worked.",,"['linear-algebra', 'matrices', 'proof-verification']"
97,How to prove that a function is affine?,How to prove that a function is affine?,,"I am trying to understand the concept of affinity of functions. First, I thought that every affine function has to be a linear function, too, because my teacher's notes define linear and affine functions as follows: $$ T(\sum_{i=0}^n \alpha_iu_i) = \sum_{i=0}^n\alpha_iT(u_i) $$ is a linear function. An affine function is defined as $ T(\sum_{i=0}^n \alpha_iu_i) $ with $ \sum_{i=0}^n \alpha_i = 1 $ and the above condition of a linear function. Then, I found the example of $ f(x) = 2x + 3 $ which is an affine function but not linear which is pretty confusing to me (I understand why it is not linear, but have no clue as to why it is affine according to the definitions). I also have to solve a problem such as: $$ T: \mathbb{R} \to \mathbb{R},  T(x,y,z) := (x − z + 1, y - 5, z - y, 2) $$ but I really have no idea how to proof if it's affine or not. There are no $ \alpha $ and that function is not linear, so I am kind of stuck here. I appreciate any sort of help, like links to websites or anything that helps me to understand this because I have no strategy to solve this problem.","I am trying to understand the concept of affinity of functions. First, I thought that every affine function has to be a linear function, too, because my teacher's notes define linear and affine functions as follows: $$ T(\sum_{i=0}^n \alpha_iu_i) = \sum_{i=0}^n\alpha_iT(u_i) $$ is a linear function. An affine function is defined as $ T(\sum_{i=0}^n \alpha_iu_i) $ with $ \sum_{i=0}^n \alpha_i = 1 $ and the above condition of a linear function. Then, I found the example of $ f(x) = 2x + 3 $ which is an affine function but not linear which is pretty confusing to me (I understand why it is not linear, but have no clue as to why it is affine according to the definitions). I also have to solve a problem such as: $$ T: \mathbb{R} \to \mathbb{R},  T(x,y,z) := (x − z + 1, y - 5, z - y, 2) $$ but I really have no idea how to proof if it's affine or not. There are no $ \alpha $ and that function is not linear, so I am kind of stuck here. I appreciate any sort of help, like links to websites or anything that helps me to understand this because I have no strategy to solve this problem.",,"['linear-algebra', 'vector-spaces', 'vectors']"
98,"For a compact set $K\subset \mathbb M_n(\mathbb R)$, the eigenvalues of matrices in $K$ form a bounded set","For a compact set , the eigenvalues of matrices in  form a bounded set",K\subset \mathbb M_n(\mathbb R) K,"Let $K\subset \mathbb M_n(\mathbb R)$ be a compact subset. Then I have to show that : All the eigen values of the elements of $K$ form a bounded set. My work: consider the map $K \to det K$ which is continuous. Image set is compact in $\mathbb C$, hence closed bounded. If $\lambda_i (i=1\ldots,n)$ are the eigenvalues, then $\det K=\prod \lambda_i$ is bounded which in turn  gives $\lambda_i$ bounded. Is my approach correct? Is there any better way to do it?","Let $K\subset \mathbb M_n(\mathbb R)$ be a compact subset. Then I have to show that : All the eigen values of the elements of $K$ form a bounded set. My work: consider the map $K \to det K$ which is continuous. Image set is compact in $\mathbb C$, hence closed bounded. If $\lambda_i (i=1\ldots,n)$ are the eigenvalues, then $\det K=\prod \lambda_i$ is bounded which in turn  gives $\lambda_i$ bounded. Is my approach correct? Is there any better way to do it?",,"['linear-algebra', 'general-topology', 'eigenvalues-eigenvectors']"
99,Why is it called linearly independent?,Why is it called linearly independent?,,"For a system of linear equations in $\Bbb R^n$ to be linearly independent, there must be a unique solution to the system (at least I'm pretty sure that's true). There are definitely other definitions, but this is the one I am most used to. Nonetheless, I am confused! Why should a set of vectors be called linearly independent under these circumstances? I mean, what are they independent of? Ultimately, I would like to know why we use the terms linear dependence and independence? Any help is appreciated. Thank you!","For a system of linear equations in $\Bbb R^n$ to be linearly independent, there must be a unique solution to the system (at least I'm pretty sure that's true). There are definitely other definitions, but this is the one I am most used to. Nonetheless, I am confused! Why should a set of vectors be called linearly independent under these circumstances? I mean, what are they independent of? Ultimately, I would like to know why we use the terms linear dependence and independence? Any help is appreciated. Thank you!",,"['linear-algebra', 'soft-question', 'terminology']"
