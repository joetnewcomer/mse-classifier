,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Countable class of real-valued differentiable functions with different derivatives at a point must differ in a neighborhood.,Countable class of real-valued differentiable functions with different derivatives at a point must differ in a neighborhood.,,"Consider the following statement. Consider a class of functions $\mathcal{F}=\{f_1,\ldots,f_N\}$ such that $f_{n}: \mathbb{R} \rightarrow \mathbb{R}$ are differentiable for all $n=1,\ldots, N$ . If there exists a point $x^{*} \in \mathbb{R}$ such that $f_{1}^{\prime}\left(x^{*}\right) \neq f_{n}^{\prime}\left(x^{*}\right)$ and $f_{1}\left(x^{*}\right) = f_{n}\left(x^{*}\right)$ for $n>1$ , then there exists $\varepsilon > 0$ such that $f_{1}(x) \neq f_{n}(x)$ for all $x \in (x^*,x^*+\varepsilon)$ and $n>1$ . The above statement is true when $N$ is a finite number, I wonder if we can extend this statement to the case where $\mathcal{F}$ is an infinite set of functions. To do so, we need extra assumptions because $f_1(x)=x$ and $f_n(x)=n x^2$ is a counterexample. What extra assumptions would need to be added to turn this into a true statement?","Consider the following statement. Consider a class of functions such that are differentiable for all . If there exists a point such that and for , then there exists such that for all and . The above statement is true when is a finite number, I wonder if we can extend this statement to the case where is an infinite set of functions. To do so, we need extra assumptions because and is a counterexample. What extra assumptions would need to be added to turn this into a true statement?","\mathcal{F}=\{f_1,\ldots,f_N\} f_{n}: \mathbb{R} \rightarrow \mathbb{R} n=1,\ldots, N x^{*} \in \mathbb{R} f_{1}^{\prime}\left(x^{*}\right) \neq f_{n}^{\prime}\left(x^{*}\right) f_{1}\left(x^{*}\right) = f_{n}\left(x^{*}\right) n>1 \varepsilon > 0 f_{1}(x) \neq f_{n}(x) x \in (x^*,x^*+\varepsilon) n>1 N \mathcal{F} f_1(x)=x f_n(x)=n x^2","['real-analysis', 'derivatives']"
1,"How can we calculate the derivative of Hessian, namely, the third derivative?","How can we calculate the derivative of Hessian, namely, the third derivative?",,"I have a problem when calculate the following derivative which is supposed to be $\mathbf{R}^{2\times 2}$ since $t\in \mathbf{R}$ . According to the chain rule, $$\tag{1} \frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x+tv)|_{t=0}\cdot \frac{d(x+tv)}{dt}=\nabla^3f(x)\cdot v $$ where $x,v\in \mathbf{R}^2$ . The gradient of $f(x)$ w.r.t $x$ is $$ \nabla f(x)=\left[\begin{array}{c}\frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\end{array}\right] $$ The Hessian is $$ \nabla^2 f(x)=\nabla\nabla^T f(x)=\left[\begin{array}{cc}\frac{\partial^2 f}{\partial x_1^2}&\frac{\partial^2 f}{\partial x_2\partial x_1}\\ \frac{\partial^2 f}{\partial x_1\partial x_2}&\frac{\partial^2 f}{\partial x_2^2}\end{array}\right] $$ However, how can I calculate $\nabla^3f(x)$ ? My strategy is to take derivative on each entry of $\nabla^2f(x)$ w.r.t $x$ , i.e., a column vector $\frac{\partial[\nabla^2f]_{ij}}{\partial x}=[\frac{\partial[\nabla^2f]_{ij}}{\partial x_1};[\frac{\partial[\nabla^2f]_{ij}}{\partial x_2}]$ , $$ \nabla^3f(x)=\frac{\partial \nabla^2f(x)}{\partial x}=\left[\begin{array}{cc} \frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\ \frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3} \end{array}\right]$$ The result is $\mathbf{R}^{4\times 2}$ .Continue our calculation $$ \nabla^3f(x)\cdot v=\left[\begin{array}{cc} \frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\ \frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3} \end{array}\right]\left[\begin{array}{c}v_1\\v_2\end{array}\right]= \left[\begin{array}{c} \frac{\partial^3 f}{\partial x_1^3}v_1+\frac{\partial^3 f}{\partial x_2\partial x_1^2}v_2\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\ \frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\ \frac{\partial^3 f}{\partial x_1\partial x_2^2}v_1+\frac{\partial^3 f}{\partial x_2^3}v_2 \end{array}\right] $$ The result is $\mathbf{R}^4$ not $\mathbf{R}^{2\times 2}$ . Where is wrong? Let's think about in the view of linear approximation $$\tag{2} \nabla^2f(x+tv)\approx\nabla^2 f(x) + \nabla^3f(x)v\cdot t $$ (2) does not make sense due to the inconsistent dimension on the RHS. Specifically, the first term is $2\times 2$ but the second term is $4\times 1$ . Where is wrong with my derivations? Still, $$\tag{3} \frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x)v $$ Who can help me point out the bugs, fix the bugs and get the correct form? Any instruction will be appreciated.","I have a problem when calculate the following derivative which is supposed to be since . According to the chain rule, where . The gradient of w.r.t is The Hessian is However, how can I calculate ? My strategy is to take derivative on each entry of w.r.t , i.e., a column vector , The result is .Continue our calculation The result is not . Where is wrong? Let's think about in the view of linear approximation (2) does not make sense due to the inconsistent dimension on the RHS. Specifically, the first term is but the second term is . Where is wrong with my derivations? Still, Who can help me point out the bugs, fix the bugs and get the correct form? Any instruction will be appreciated.","\mathbf{R}^{2\times 2} t\in \mathbf{R} \tag{1}
\frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x+tv)|_{t=0}\cdot \frac{d(x+tv)}{dt}=\nabla^3f(x)\cdot v
 x,v\in \mathbf{R}^2 f(x) x 
\nabla f(x)=\left[\begin{array}{c}\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2}\end{array}\right]
 
\nabla^2 f(x)=\nabla\nabla^T f(x)=\left[\begin{array}{cc}\frac{\partial^2 f}{\partial x_1^2}&\frac{\partial^2 f}{\partial x_2\partial x_1}\\
\frac{\partial^2 f}{\partial x_1\partial x_2}&\frac{\partial^2 f}{\partial x_2^2}\end{array}\right]
 \nabla^3f(x) \nabla^2f(x) x \frac{\partial[\nabla^2f]_{ij}}{\partial x}=[\frac{\partial[\nabla^2f]_{ij}}{\partial x_1};[\frac{\partial[\nabla^2f]_{ij}}{\partial x_2}] 
\nabla^3f(x)=\frac{\partial \nabla^2f(x)}{\partial x}=\left[\begin{array}{cc}
\frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3}
\end{array}\right] \mathbf{R}^{4\times 2} 
\nabla^3f(x)\cdot v=\left[\begin{array}{cc}
\frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3}
\end{array}\right]\left[\begin{array}{c}v_1\\v_2\end{array}\right]=
\left[\begin{array}{c}
\frac{\partial^3 f}{\partial x_1^3}v_1+\frac{\partial^3 f}{\partial x_2\partial x_1^2}v_2\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}v_1+\frac{\partial^3 f}{\partial x_2^3}v_2
\end{array}\right]
 \mathbf{R}^4 \mathbf{R}^{2\times 2} \tag{2}
\nabla^2f(x+tv)\approx\nabla^2 f(x) + \nabla^3f(x)v\cdot t
 2\times 2 4\times 1 \tag{3}
\frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x)v
","['derivatives', 'partial-derivative', 'hessian-matrix']"
2,Showing Differentiability/Continuity at endpoints of closed interval?,Showing Differentiability/Continuity at endpoints of closed interval?,,"I am given the function $\gamma:[-1,\frac{\pi}{2}] \rightarrow \mathbb{C}$ $\gamma(t) =  \begin{cases}  t+1 & \text{for $-1 \leq t \leq0$} \\  e^{it} & \text{for  $0 \leq t \leq\frac{\pi}{2}$} \\ \end{cases}$ And I want to show this function is continuous and piecewise continuously differentiable. To show the function is continuous at the endpoints $-1$ and $\frac{\pi}{2}$ of the interval $[-1,\frac{\pi}{2}]$ , is it sufficient to show that $\lim_{t\rightarrow-1^{+}} \gamma(t) = \gamma(-1)$ and $\lim_{t\rightarrow\frac{\pi}{2}^{-}} \gamma(t) = \gamma(\frac{\pi}{2})$ ? Then, to show the function is piecewise differentiable, would I need to show that $\gamma_{|[-1,0]}$ is differentiable at all interior points of the interval $[-1,0]$ , left differentiable at $0$ and right differentiable at $-1$ ? (And then do the equivalent for $\gamma_{|[0,\frac{\pi}{2}]}$ ) To show continuity at the endpoints of the interval for this derivative, would I need to show that $\lim_{t\rightarrow-1^{+}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(-1^{+})$ and $\lim_{t\rightarrow0^{-}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(0^{-})$ ? (And then do the equivalent for $\gamma_{|[0,\frac{\pi}{2}]}$ )","I am given the function And I want to show this function is continuous and piecewise continuously differentiable. To show the function is continuous at the endpoints and of the interval , is it sufficient to show that and ? Then, to show the function is piecewise differentiable, would I need to show that is differentiable at all interior points of the interval , left differentiable at and right differentiable at ? (And then do the equivalent for ) To show continuity at the endpoints of the interval for this derivative, would I need to show that and ? (And then do the equivalent for )","\gamma:[-1,\frac{\pi}{2}] \rightarrow \mathbb{C} \gamma(t) = 
\begin{cases}
 t+1 & \text{for -1 \leq t \leq0} \\
 e^{it} & \text{for  0 \leq t \leq\frac{\pi}{2}} \\
\end{cases} -1 \frac{\pi}{2} [-1,\frac{\pi}{2}] \lim_{t\rightarrow-1^{+}} \gamma(t) = \gamma(-1) \lim_{t\rightarrow\frac{\pi}{2}^{-}} \gamma(t) = \gamma(\frac{\pi}{2}) \gamma_{|[-1,0]} [-1,0] 0 -1 \gamma_{|[0,\frac{\pi}{2}]} \lim_{t\rightarrow-1^{+}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(-1^{+}) \lim_{t\rightarrow0^{-}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(0^{-}) \gamma_{|[0,\frac{\pi}{2}]}","['complex-analysis', 'limits', 'derivatives', 'continuity', 'piecewise-continuity']"
3,Prove that $f'(0)$ exists and $f'(0) = b/(a - 1)$,Prove that  exists and,f'(0) f'(0) = b/(a - 1),"Problem : If $f(x)$ is continous at $x=0$, and $\lim\limits_{x\to 0} \dfrac{f(ax)-f(x)}{x}=b$, $a, b$ are constants and $|a|>1$, prove that $f'(0)$ exists and $f'(0)=\dfrac{b}{a-1}$. This approach is definitely wrong: \begin{align} b&=\lim_{x\to 0} \frac{f(ax)-f(x)}{x}\\ &=\lim_{x\to 0} \frac{f(ax)-f(0)-(f(x)-f(0))}{x}\\ &=af'(0)-f'(0)\\ &=(a-1)f'(0) \end{align} I will show you a case why this approach is wrong: \[f(x)= \begin{cases} 1,&x\neq0\\ 0,&x=0 \end{cases}\]   $\lim_{x\to0}\dfrac{f(3x)-f(x)}{x}=\lim_{x\to0} \dfrac{1-1}{x}=0$ but $\lim_{x\to0}\dfrac{f(3x)}{x}=\infty$,$\lim_{x\to0}\dfrac{f(x)}{x}=\infty$ Does anyone know how to prove it? Thanks in advance!","Problem : If $f(x)$ is continous at $x=0$, and $\lim\limits_{x\to 0} \dfrac{f(ax)-f(x)}{x}=b$, $a, b$ are constants and $|a|>1$, prove that $f'(0)$ exists and $f'(0)=\dfrac{b}{a-1}$. This approach is definitely wrong: \begin{align} b&=\lim_{x\to 0} \frac{f(ax)-f(x)}{x}\\ &=\lim_{x\to 0} \frac{f(ax)-f(0)-(f(x)-f(0))}{x}\\ &=af'(0)-f'(0)\\ &=(a-1)f'(0) \end{align} I will show you a case why this approach is wrong: \[f(x)= \begin{cases} 1,&x\neq0\\ 0,&x=0 \end{cases}\]   $\lim_{x\to0}\dfrac{f(3x)-f(x)}{x}=\lim_{x\to0} \dfrac{1-1}{x}=0$ but $\lim_{x\to0}\dfrac{f(3x)}{x}=\infty$,$\lim_{x\to0}\dfrac{f(x)}{x}=\infty$ Does anyone know how to prove it? Thanks in advance!",,"['calculus', 'derivatives']"
4,Measure theoretic proof of Leibniz Rule for Differentiation under the Integral with varying integration limits,Measure theoretic proof of Leibniz Rule for Differentiation under the Integral with varying integration limits,,"I'm searching for a rigorous proof of Leibniz rule of integration for varying integration limits, i.e.: $$ \frac{d}{dx}\int^{b(x)}_{a(x)} f(x,t) dt = f(x,b(x)) \frac{db(x)}{dx} - f(x,a(x)) \frac{da(x)}{dx} + \int^{b(x)}_{a(x)} \frac{\partial f(x,t)}{\partial x} dt. $$ I've seen proofs of the statement above, but not in measure theoretic terms. Also, the proofs of Leibniz rule that use Measure Theory just prove for the case where the limits are fixed, i.e. (under proper hypothesis): $$ \frac{d}{dx} \int_\Omega f(x,t) dt = \int_\Omega \frac{\partial }{\partial x} f(x,t) dt. $$ I've tried extending the proof by using the result above and using an indicator function to write the limits as a function inside of the integral. But, I then end up having to integrate an indicator function. If someone can prove the result or point me to a reference that does the complete proof, I'd really appretiate it.","I'm searching for a rigorous proof of Leibniz rule of integration for varying integration limits, i.e.: I've seen proofs of the statement above, but not in measure theoretic terms. Also, the proofs of Leibniz rule that use Measure Theory just prove for the case where the limits are fixed, i.e. (under proper hypothesis): I've tried extending the proof by using the result above and using an indicator function to write the limits as a function inside of the integral. But, I then end up having to integrate an indicator function. If someone can prove the result or point me to a reference that does the complete proof, I'd really appretiate it.","
\frac{d}{dx}\int^{b(x)}_{a(x)} f(x,t) dt = f(x,b(x)) \frac{db(x)}{dx} -
f(x,a(x)) \frac{da(x)}{dx} + \int^{b(x)}_{a(x)} \frac{\partial f(x,t)}{\partial x} dt.
 
\frac{d}{dx} \int_\Omega f(x,t) dt = \int_\Omega \frac{\partial }{\partial x} f(x,t) dt.
","['integration', 'measure-theory', 'derivatives']"
5,Partial Derivatives of Sigmoid Function,Partial Derivatives of Sigmoid Function,,"I am confused about the partial derivatives that I'm using in my deep learning model. The formula of the sigmoid is: $i_t = \sigma(W_{ii}*x_t+b_{ii} + W_{hi}*h_{t-1}+b_{hi})$ Notation meaning: $\sigma$ = Sigmoid Function $W_{ii}$ = Weight Input $x_t$ = Input data $b_{ii}$ = Bias Input $W_{hi}$ = Weight Hidden $h_{t-1}$ = Hidden State previous Time Step $b_{ii}$ = Bias Hidden I've been searched that the regular sigmoid function partial derivatives is: $\frac{d}{d_x} =  \sigma(x) . (1-\sigma(x))$ Then, I'm trying to do a partial derivative of the $i_t$ formula w.r.t $W_{ii}$ , my current answer is: $\frac{d}{dw_{ii}} = \sigma(x_t) . (1-\sigma(x_t))$ Am I do wrong? If I do wrong, can someone help me to correct my answer? UPDATE: I've been try again and this is my step right now: Assume that $z = W_{ii}*x_t+b_{ii} + W_{hi}*h_{t-1}+b_{hi}$ Then the $i_t$ formula will be: $i_t = \sigma(z)$ Partial derivative will be: $\frac{d}{d_z}i_t =  \sigma(z) . (1-\sigma(z))$ On the other hand, the partial derivative of the $z$ is: $\frac{d}{dw_{ii}}z = x_t $ Finally, the final partial derivative formula is: $\frac{d}{dw_{ii}}i_t = \sigma(z) . (1-\sigma(z)) . x_t$ But, I don't know whether it's correct or wrong. Can someone correct my updated answer? Thank you in advance","I am confused about the partial derivatives that I'm using in my deep learning model. The formula of the sigmoid is: Notation meaning: = Sigmoid Function = Weight Input = Input data = Bias Input = Weight Hidden = Hidden State previous Time Step = Bias Hidden I've been searched that the regular sigmoid function partial derivatives is: Then, I'm trying to do a partial derivative of the formula w.r.t , my current answer is: Am I do wrong? If I do wrong, can someone help me to correct my answer? UPDATE: I've been try again and this is my step right now: Assume that Then the formula will be: Partial derivative will be: On the other hand, the partial derivative of the is: Finally, the final partial derivative formula is: But, I don't know whether it's correct or wrong. Can someone correct my updated answer? Thank you in advance",i_t = \sigma(W_{ii}*x_t+b_{ii} + W_{hi}*h_{t-1}+b_{hi}) \sigma W_{ii} x_t b_{ii} W_{hi} h_{t-1} b_{ii} \frac{d}{d_x} =  \sigma(x) . (1-\sigma(x)) i_t W_{ii} \frac{d}{dw_{ii}} = \sigma(x_t) . (1-\sigma(x_t)) z = W_{ii}*x_t+b_{ii} + W_{hi}*h_{t-1}+b_{hi} i_t i_t = \sigma(z) \frac{d}{d_z}i_t =  \sigma(z) . (1-\sigma(z)) z \frac{d}{dw_{ii}}z = x_t  \frac{d}{dw_{ii}}i_t = \sigma(z) . (1-\sigma(z)) . x_t,"['derivatives', 'partial-differential-equations', 'partial-derivative', 'machine-learning']"
6,Derivation of the Backpropagation Algorithm,Derivation of the Backpropagation Algorithm,,"I'm studying the Backpropagation algorithm and I want to derive it by myself. Therefore, I've constructed a very simple network with one input layer, one hidden layer and one output layer. You can find the details in the graphic. The network can be found here: $t$ is the true output, $i$ is the input, $z_{hi}$ and $w_{ij}$ are the weights. Moreover I have the activation function $\phi(x)$ . I think that I have understood the algorithm more or less, so I've started with the following error function: $$ E = \frac{1}{2}\sum_{j=1}^2{(t_j-o_j)^2}  $$ That function should be correct, isn't it? The next step is to calculate the partial derivative: $$ \frac{\partial E }{\partial w_{ik}} = \frac{\partial E }{\partial o_k}\cdot  \frac{\partial o_k }{\partial w_{ik}} = -(t_k-o_k) \cdot  \frac{\partial o_k }{\partial w_{ik}} =-(t_k-o_k) \cdot  \phi'(\sum_{i=1}^2 w_{i,k}\cdot a_k ) \cdot a_k $$ For me that seems to be correct, but is that right up to here (Especially the formal math aspects)? I'm asking, because i have so problems with the next steps (derivating to the z's... Thanks for helping!","I'm studying the Backpropagation algorithm and I want to derive it by myself. Therefore, I've constructed a very simple network with one input layer, one hidden layer and one output layer. You can find the details in the graphic. The network can be found here: is the true output, is the input, and are the weights. Moreover I have the activation function . I think that I have understood the algorithm more or less, so I've started with the following error function: That function should be correct, isn't it? The next step is to calculate the partial derivative: For me that seems to be correct, but is that right up to here (Especially the formal math aspects)? I'm asking, because i have so problems with the next steps (derivating to the z's... Thanks for helping!","t i z_{hi} w_{ij} \phi(x) 
E = \frac{1}{2}\sum_{j=1}^2{(t_j-o_j)^2} 
 
\frac{\partial E }{\partial w_{ik}} = \frac{\partial E }{\partial o_k}\cdot  \frac{\partial o_k }{\partial w_{ik}} = -(t_k-o_k) \cdot  \frac{\partial o_k }{\partial w_{ik}} =-(t_k-o_k) \cdot  \phi'(\sum_{i=1}^2 w_{i,k}\cdot a_k ) \cdot a_k
","['derivatives', 'partial-derivative', 'neural-networks']"
7,A problem on Mean Value Theorem from Thomas Calculus.,A problem on Mean Value Theorem from Thomas Calculus.,,"The question If $f:[0,4] \rightarrow \mathbb{R}$ is differentiable, then prove that $$ [f(4)]^2-[f(0)]^2=8f'(a)f(b) \text{ for } a,b \in (0,4) $$ My Solution Let's choose $b$ such that $$ f(b)=\frac{f(4)+f(0)}{2} \tag{1} $$ Since $f$ is differentiable in it domain, $$ \exists \text{ } c \in (0,4) \text{ | } f'(c)=\frac{f(4)-f(0)}{4-0} \tag{2} $$ If we consider $a=c$ and multiply (1) and (2), we get $$ f'(a)f(b)=\frac{[f(4)]^2-[f(0)]^2}{8} $$ and we're done. My doubt This solution has $b$ locked to one particular value and $a$ could assume a couple of values. I'm not entirely sure if the question is asking us to prove all values of $a$ and $b$ in $(0,4)$ satisfies the given assertion. I'm pretty sure that all values in $(0,4)$ would not satisfy the given assertion. Any comments is highly appreciated.","The question If is differentiable, then prove that My Solution Let's choose such that Since is differentiable in it domain, If we consider and multiply (1) and (2), we get and we're done. My doubt This solution has locked to one particular value and could assume a couple of values. I'm not entirely sure if the question is asking us to prove all values of and in satisfies the given assertion. I'm pretty sure that all values in would not satisfy the given assertion. Any comments is highly appreciated.","f:[0,4] \rightarrow \mathbb{R} 
[f(4)]^2-[f(0)]^2=8f'(a)f(b) \text{ for } a,b \in (0,4)
 b 
f(b)=\frac{f(4)+f(0)}{2} \tag{1}
 f 
\exists \text{ } c \in (0,4) \text{ | } f'(c)=\frac{f(4)-f(0)}{4-0} \tag{2}
 a=c 
f'(a)f(b)=\frac{[f(4)]^2-[f(0)]^2}{8}
 b a a b (0,4) (0,4)",['derivatives']
8,A pathological example of a distribution.,A pathological example of a distribution.,,"Everyone who studied distributions at some point have seen how locally all distribution are basically formal derivatives of continuous functions. The question is: does there exists a bounded function $f\in L^\infty(\mathbb R^d,\mathbb R^d)$ such that its distributional derivative is not a measure? The easiest formulation of the question would be to find such a function in $L^\infty (\mathbb R)$ . Since the topology on the distribuitions is the pointwise topology when looking at bounded functions we can just look at characteristic functions. Given a measurable set $A$ we know that $\chi_A$ can be approximated by above with open sets $O_n$ and from below with compact sets $K_n$ , thus creating two converging sequences $$ T_{\chi_{O_n}}\to T_{\chi_{A}}\leftarrow T_{\chi_{K_n}}. $$ We get $$ \partial T_{\chi_A}(\varphi)=- T_{\chi_A}(\varphi')=\lim -T_{\chi_{O_n}}(\varphi')=\sum_k (\varphi(a^n_k)-\varphi(b^n_k)). $$ The inequality $|\partial T_{\chi_A}(\varphi)|\leq\|\varphi'\|_\infty\mathcal{L}^1(A)$ is trivial but if the number of intervals in $O_n$ is a finite number $M_n\in\mathbb N$ for any $n\in\mathbb N$ then we get $$ |\partial T_{\chi_A}(\varphi)|\leq 2 M_n \|\varphi\|_\infty+\varepsilon, $$ thus pointing out that if $M_n\leq M$ uniformly then we would get a measurable set with a measure as a derivative. Is such a condition necessary? Of course not, because Cantor's functions is an example of a bounded function whose derivative is a measure supported on an uncountable set. Does this mean that considering the function $$ f=\sum_n(-1)^{n+1}\frac{n}{2}\chi_{[-\frac{1}{n},\frac{1}{n}]} $$ is enough? It is bounded because the alternated harmonic series converges and its derivative is $$ \int_{\mathbb R}\varphi' f\mathcal L=\sum_n(-1)^{n+1}\frac{\varphi(1/n)-\varphi(-1/n)}{2/n}. $$ Are there more trivial examples? Is there a mistake and this function in reality has a measure as derivative? Is there an example of a continuous function that doesn't have a measure as a derivative? What are some multidimensional examples?","Everyone who studied distributions at some point have seen how locally all distribution are basically formal derivatives of continuous functions. The question is: does there exists a bounded function such that its distributional derivative is not a measure? The easiest formulation of the question would be to find such a function in . Since the topology on the distribuitions is the pointwise topology when looking at bounded functions we can just look at characteristic functions. Given a measurable set we know that can be approximated by above with open sets and from below with compact sets , thus creating two converging sequences We get The inequality is trivial but if the number of intervals in is a finite number for any then we get thus pointing out that if uniformly then we would get a measurable set with a measure as a derivative. Is such a condition necessary? Of course not, because Cantor's functions is an example of a bounded function whose derivative is a measure supported on an uncountable set. Does this mean that considering the function is enough? It is bounded because the alternated harmonic series converges and its derivative is Are there more trivial examples? Is there a mistake and this function in reality has a measure as derivative? Is there an example of a continuous function that doesn't have a measure as a derivative? What are some multidimensional examples?","f\in L^\infty(\mathbb R^d,\mathbb R^d) L^\infty (\mathbb R) A \chi_A O_n K_n 
T_{\chi_{O_n}}\to T_{\chi_{A}}\leftarrow T_{\chi_{K_n}}.
 
\partial T_{\chi_A}(\varphi)=- T_{\chi_A}(\varphi')=\lim -T_{\chi_{O_n}}(\varphi')=\sum_k (\varphi(a^n_k)-\varphi(b^n_k)).
 |\partial T_{\chi_A}(\varphi)|\leq\|\varphi'\|_\infty\mathcal{L}^1(A) O_n M_n\in\mathbb N n\in\mathbb N 
|\partial T_{\chi_A}(\varphi)|\leq 2 M_n \|\varphi\|_\infty+\varepsilon,
 M_n\leq M 
f=\sum_n(-1)^{n+1}\frac{n}{2}\chi_{[-\frac{1}{n},\frac{1}{n}]}
 
\int_{\mathbb R}\varphi' f\mathcal L=\sum_n(-1)^{n+1}\frac{\varphi(1/n)-\varphi(-1/n)}{2/n}.
","['derivatives', 'distribution-theory']"
9,Derivatives of Complex Functions,Derivatives of Complex Functions,,"For single variable function, it is considered to be differentiable at a point when left derivative equal to right derivative. But in the case of complex function we need to have derivative that approach our point of interest in all direction to be equal in order to be differentiable. My question here is that a complex function have two real variables as multi variable function but why multi variable function don’t have that “approaching in all direction” problem when we define its derivative?","For single variable function, it is considered to be differentiable at a point when left derivative equal to right derivative. But in the case of complex function we need to have derivative that approach our point of interest in all direction to be equal in order to be differentiable. My question here is that a complex function have two real variables as multi variable function but why multi variable function don’t have that “approaching in all direction” problem when we define its derivative?",,"['complex-analysis', 'derivatives']"
10,Why does $f'(x_{(1)})=-r_{(1)}$ in conjugate gradient?,Why does  in conjugate gradient?,f'(x_{(1)})=-r_{(1)},I am reading through the paper Painless Conjugate Gradient by Jonathan Shewchuk and have reached the following passage: Everything in there makes sense up until he says $f'(x_{(1)})=-r_{(1)}$ . How did he derive that? I'm having a lot of trouble visualizing why that would be the case.,I am reading through the paper Painless Conjugate Gradient by Jonathan Shewchuk and have reached the following passage: Everything in there makes sense up until he says . How did he derive that? I'm having a lot of trouble visualizing why that would be the case.,f'(x_{(1)})=-r_{(1)},"['linear-algebra', 'derivatives', 'gradient-descent']"
11,Recurrence relation of L'Hôpital's rules,Recurrence relation of L'Hôpital's rules,,"Today I woke up with this thought. Imagine I have a limit that is given by: $$\lim_{x\to x_0} {[f(x)/g(x)]}$$ If $f(x_0)=g(x_0)=0$ then I can solve it using L'Hôpital's rule. If $f'(x_0) = g'(x_0)=0$ , then I should perform another derivation and keep doing that many times as I need. Imagine that after $n$ L'Hôpital derivations I get: $${[f^{(n)}(x)/g^{(n)}(x)]=\alpha[f(x)/g(x)]}.$$ Does that necessarily means that the limit does not exist in $x=x_0$ ?","Today I woke up with this thought. Imagine I have a limit that is given by: If then I can solve it using L'Hôpital's rule. If , then I should perform another derivation and keep doing that many times as I need. Imagine that after L'Hôpital derivations I get: Does that necessarily means that the limit does not exist in ?",\lim_{x\to x_0} {[f(x)/g(x)]} f(x_0)=g(x_0)=0 f'(x_0) = g'(x_0)=0 n {[f^{(n)}(x)/g^{(n)}(x)]=\alpha[f(x)/g(x)]}. x=x_0,"['calculus', 'limits', 'derivatives']"
12,Exercise 5.25 in Baby Rudin,Exercise 5.25 in Baby Rudin,,"Suppose $f$ is twice differentiable on $[a, b], f(a)<0, f(b)>0, f'(x)\ge\delta>0$ , and $0\leq f''(x)\leq M$ for all $x\in[a, b]$ . Let $\xi$ be the unique point in $(a, b)$ at which $f(\xi)=0$ . Part (b): Choose $x_1\in(\xi, b)$ , and define $\{x_n\}$ by $x_{n+1}=x_n-\frac{f(x_{n})}{f'(x_{n})}$ . Prove that $x_{n+1}<x_n$ and that $\lim_{n\to\infty}x_n=\xi$ . My attempt: We use Induction. It is easy to show that $x_2<x_1$ . Now suppose $x_{k+1}<x_k$ , then easily $f(x_{k})>0$ and $x_{k+2}=x_{k+1}-\frac{f(x_{k+1})}{f'(x_{k+1})}$ . But where do I go from here? How do I show that $f(x_{k+1})>0$ ? I tried to prove that $x_{k+1}\in(\xi, b)$ , but induction does not seem to work there as I failed to show that $x_n\in(\xi, b) \forall n\in\Bbb{N}$ . Please help.","Suppose is twice differentiable on , and for all . Let be the unique point in at which . Part (b): Choose , and define by . Prove that and that . My attempt: We use Induction. It is easy to show that . Now suppose , then easily and . But where do I go from here? How do I show that ? I tried to prove that , but induction does not seem to work there as I failed to show that . Please help.","f [a, b], f(a)<0, f(b)>0, f'(x)\ge\delta>0 0\leq f''(x)\leq M x\in[a, b] \xi (a, b) f(\xi)=0 x_1\in(\xi, b) \{x_n\} x_{n+1}=x_n-\frac{f(x_{n})}{f'(x_{n})} x_{n+1}<x_n \lim_{n\to\infty}x_n=\xi x_2<x_1 x_{k+1}<x_k f(x_{k})>0 x_{k+2}=x_{k+1}-\frac{f(x_{k+1})}{f'(x_{k+1})} f(x_{k+1})>0 x_{k+1}\in(\xi, b) x_n\in(\xi, b) \forall n\in\Bbb{N}","['real-analysis', 'derivatives', 'induction']"
13,How much of additive is in the tank?,How much of additive is in the tank?,,"A storage tank contains 7570.8 L of gasoline that initially has 45.3 kg of an additive dissolved in it. In preparation for winter weather, gasoline containing 0.24 kg of additive per ml is pumped into the tank at a rate of 151.4 L/min. The well-mixed solution is pumped out at a rate of 170.3 L/min. How much additive is in the tank 20 min after the pumping process begins (See Figure below)? Take $y(t)$ is the amount of chemical in the container at time $t$ and $V(t)$ is the total volume of liquid in the container at time $t$ . My attempt : I assume that no amount is going out of the tank then I can get the following things The amount of chemical in time $t$ min $=$ Initial amount + accumulated amount in time t $$y_1=45.3+0.24\times 151.4\times t\ kg=45.3+36336t\ kg$$ The volume of solution in the tank after time $t$ min $=$ Initial volume + accumulated volume in time t $$=7570.8+151.4\times t =7570.8+151.4t \ L$$ The concentration of chemical i.e. the amount of chemical per L of solution at time $t$ is $$=\frac{45.3+36336t}{7570.8+151.4t }\ kg\L$$ Now, the solution is taken out of the tank, The amount is taken out of the tank in time $t=$ draining rate $\times$ concentration $$y_2=170.3\times \frac{45.3+36336t}{7570.8+151.4t}$$ therefore the amount of chemical $y(t)$ left in the tank at time $t=$ amount accumulated in time $t-$ amount taken out of the tank in time $t$ $$y(t)=y_1-y_2$$ $$=45.3+36336t-170.3\times \frac{45.3+36336t}{7570.8+151.4t}$$ now, substituting $t=20$ second in above formula, the amount left in tank $$y(t)=45.3+36336(20)-170.3\times \frac{45.3+36336(20)}{7570.8+151.4(20)}$$ $$=715087.74 \ kg$$ I am not sure if I am right. Please point out my mistake if any. Is there another way to solve this problem differential equation with initial conditions? please help me solve this. thanks in advance.","A storage tank contains 7570.8 L of gasoline that initially has 45.3 kg of an additive dissolved in it. In preparation for winter weather, gasoline containing 0.24 kg of additive per ml is pumped into the tank at a rate of 151.4 L/min. The well-mixed solution is pumped out at a rate of 170.3 L/min. How much additive is in the tank 20 min after the pumping process begins (See Figure below)? Take is the amount of chemical in the container at time and is the total volume of liquid in the container at time . My attempt : I assume that no amount is going out of the tank then I can get the following things The amount of chemical in time min Initial amount + accumulated amount in time t The volume of solution in the tank after time min Initial volume + accumulated volume in time t The concentration of chemical i.e. the amount of chemical per L of solution at time is Now, the solution is taken out of the tank, The amount is taken out of the tank in time draining rate concentration therefore the amount of chemical left in the tank at time amount accumulated in time amount taken out of the tank in time now, substituting second in above formula, the amount left in tank I am not sure if I am right. Please point out my mistake if any. Is there another way to solve this problem differential equation with initial conditions? please help me solve this. thanks in advance.",y(t) t V(t) t t = y_1=45.3+0.24\times 151.4\times t\ kg=45.3+36336t\ kg t = =7570.8+151.4\times t =7570.8+151.4t \ L t =\frac{45.3+36336t}{7570.8+151.4t }\ kg\L t= \times y_2=170.3\times \frac{45.3+36336t}{7570.8+151.4t} y(t) t= t- t y(t)=y_1-y_2 =45.3+36336t-170.3\times \frac{45.3+36336t}{7570.8+151.4t} t=20 y(t)=45.3+36336(20)-170.3\times \frac{45.3+36336(20)}{7570.8+151.4(20)} =715087.74 \ kg,"['calculus', 'ordinary-differential-equations', 'derivatives']"
14,"What is the slope of $x^3+y^3 = 9$ at $(2,1)$?",What is the slope of  at ?,"x^3+y^3 = 9 (2,1)","The question is (verbatim): What is the slope of $x^3+y^3 = 9$ at $(2,1)$ Secondly, I am confused about how the differentiating carries out. If I differentiate both sides with respect to $x$ , I get $$ 3x^2 + 3y^2 \frac{dy}{dx} = 0 \implies \frac{dy}{dx} = -x^2/y^2 = -4 $$ But if I try to rearrange the given equation first to get $y$ on the LHS, I get $$ x^3+y^3 = 9 \\ y^3 = 9 - x^3 \\ y = (9 - x^3)^{1/3} \\ \frac{dy}{dx} = 1/3 * (9 - x^3)^{-2/3} * -3x^2 = -x^2 (9 - x^3)^{-2/3} $$ But now this derivative no longer depends on $y$ . What is the issue with the second approach?","The question is (verbatim): What is the slope of at Secondly, I am confused about how the differentiating carries out. If I differentiate both sides with respect to , I get But if I try to rearrange the given equation first to get on the LHS, I get But now this derivative no longer depends on . What is the issue with the second approach?","x^3+y^3 = 9 (2,1) x 
3x^2 + 3y^2 \frac{dy}{dx} = 0 \implies \frac{dy}{dx} = -x^2/y^2 = -4
 y 
x^3+y^3 = 9 \\
y^3 = 9 - x^3 \\
y = (9 - x^3)^{1/3} \\
\frac{dy}{dx} = 1/3 * (9 - x^3)^{-2/3} * -3x^2 = -x^2 (9 - x^3)^{-2/3}
 y","['calculus', 'derivatives']"
15,Find the directional derivative at a point and in the direction of a given vector.,Find the directional derivative at a point and in the direction of a given vector.,,"I have the function: $f(x,y) = x/(x+y)$ and I want to the find the directional derivative at the point $(1,2)$ and in the direction of the vector: $a=(4,3)$ . I started by finding the gradient of $f(x,y)$ which I found to be: $(y/(x+y)^2 , -x/(x+y)^2)$ , I then found the gradient at the point $(1,2)$ by substituting in $(1,2)$ and got the gradient of $f(1,2) = (2/9,-1/9)$ . Next I found the modulus of $a$ which is $\sqrt{4^2 +3^2} =5$ and I used this modulus to create $u$ where $u = a/|a|$ so $u = (4i +3j)/5$ which is equivalent to $(4/5,3/5)$ . Then from here I ran into an issue. I then tried to do $\nabla f \cdot u$ and the answer on the answer sheet is given to be $1/9$ , however when I do this multiplication I get: $((4/5 \cdot 2/9), (3/5 \cdot -1/9))$ which gives me $(8/45,-1/15)$ which is not what I want. Could someone please show me where I went wrong or what I was supposed to do once I found $\nabla f$ and $u$ Thanks in advance","I have the function: and I want to the find the directional derivative at the point and in the direction of the vector: . I started by finding the gradient of which I found to be: , I then found the gradient at the point by substituting in and got the gradient of . Next I found the modulus of which is and I used this modulus to create where so which is equivalent to . Then from here I ran into an issue. I then tried to do and the answer on the answer sheet is given to be , however when I do this multiplication I get: which gives me which is not what I want. Could someone please show me where I went wrong or what I was supposed to do once I found and Thanks in advance","f(x,y) = x/(x+y) (1,2) a=(4,3) f(x,y) (y/(x+y)^2 , -x/(x+y)^2) (1,2) (1,2) f(1,2) = (2/9,-1/9) a \sqrt{4^2 +3^2} =5 u u = a/|a| u = (4i +3j)/5 (4/5,3/5) \nabla f \cdot u 1/9 ((4/5 \cdot 2/9), (3/5 \cdot -1/9)) (8/45,-1/15) \nabla f u","['multivariable-calculus', 'derivatives']"
16,simplify radicals,simplify radicals,,"In the book $A=B$ (link) page 11 example 1.5.1 it is written By setting $\sin a = x$ and $\sin b = y$ , we see that the identity $\sin(a + b) = \sin a \cos b + \sin b \cos a$ is equivalent to $$  \arcsin x + \arcsin y = \arcsin(x \sqrt{1-y^2} + y \sqrt{1-x^2}) $$ When $x = 0$ this is tautologous, so it suffices to prove that the derivatives of both sides with respect to $x$ are the same. This is a routinely verifiable algebraic identity. By differentiating w.r.t. $x$ I obtained: \begin{gather}\frac 1 {\sqrt{1-x^2}}+ \frac {\frac{dy}{dx}} {\sqrt{1\!-\!y^2}} =\frac{\frac d {dx}\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)} {\sqrt{1-\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)^2}}=\frac{\sqrt{1\!-\!y^2}\!-\!\frac {xy}{\sqrt{1\!-\!x^2}}+\left(\sqrt{1\!-\!x^2}\!-\!\frac {xy}{\sqrt{1\!-\!y^2}}\right) \!\frac{dy}{dx}} {\sqrt{1-\left  (x\sqrt{1\!-\!y^2}+y\sqrt{1-x^2}\right)^2}} \end{gather} where $y$ is somewhat arbitrary function of $x$ . But I cannot reduce this (especially the radical in the denominator of the RHS) to show that the equality holds. Seems to me there is no way they can be equal so do I just assume it is a mistake in the text or is there someone who can show equality? He seems to go about it by implying the squares of both sides are equal using Maple code but I can't show that either (and I don't have Maple so i can't test it that way). Petkovšek, Marko; Wilf, Herbert S.; Zeilberger, Doron , $A=B$ . With foreword by Donald E. Knuth, Wellesley, MA: A. K. Peters. xii, 212 p. (1996). ZBL0848.05002 .","In the book (link) page 11 example 1.5.1 it is written By setting and , we see that the identity is equivalent to When this is tautologous, so it suffices to prove that the derivatives of both sides with respect to are the same. This is a routinely verifiable algebraic identity. By differentiating w.r.t. I obtained: where is somewhat arbitrary function of . But I cannot reduce this (especially the radical in the denominator of the RHS) to show that the equality holds. Seems to me there is no way they can be equal so do I just assume it is a mistake in the text or is there someone who can show equality? He seems to go about it by implying the squares of both sides are equal using Maple code but I can't show that either (and I don't have Maple so i can't test it that way). Petkovšek, Marko; Wilf, Herbert S.; Zeilberger, Doron , . With foreword by Donald E. Knuth, Wellesley, MA: A. K. Peters. xii, 212 p. (1996). ZBL0848.05002 .","A=B \sin a = x \sin b = y \sin(a + b) = \sin a \cos b + \sin b \cos a 
 \arcsin x + \arcsin y = \arcsin(x \sqrt{1-y^2} + y \sqrt{1-x^2})
 x = 0 x x \begin{gather}\frac 1 {\sqrt{1-x^2}}+ \frac {\frac{dy}{dx}} {\sqrt{1\!-\!y^2}}
=\frac{\frac d {dx}\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)}
{\sqrt{1-\left (x\sqrt{1\!-\!y^2}+y\sqrt{1\!-\!x^2}\right)^2}}=\frac{\sqrt{1\!-\!y^2}\!-\!\frac {xy}{\sqrt{1\!-\!x^2}}+\left(\sqrt{1\!-\!x^2}\!-\!\frac {xy}{\sqrt{1\!-\!y^2}}\right)
\!\frac{dy}{dx}} {\sqrt{1-\left  (x\sqrt{1\!-\!y^2}+y\sqrt{1-x^2}\right)^2}}
\end{gather} y x A=B","['algebra-precalculus', 'derivatives', 'radicals', 'nested-radicals']"
17,Partial derivative of $x^TA^TAx$ with respect to $A$ [closed],Partial derivative of  with respect to  [closed],x^TA^TAx A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm trying to evaluate $\nabla_{A} x^TA^TAx$ for vector $x \in \mathbb{R}^n$ , and matrix $A \in \mathbb{R}^{m \times n}$ . I evaluated it elementwise and was wondering if there is a clean, closed form formula for this?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm trying to evaluate for vector , and matrix . I evaluated it elementwise and was wondering if there is a clean, closed form formula for this?",\nabla_{A} x^TA^TAx x \in \mathbb{R}^n A \in \mathbb{R}^{m \times n},"['linear-algebra', 'matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
18,Hint on solving the second order differential equation $(x^2-3x)y''+xy'-(x+3)y=0$,Hint on solving the second order differential equation,(x^2-3x)y''+xy'-(x+3)y=0,"I want to solve this differential equation: $$(x^2-3x)y''+xy'-(x+3)y=0$$ The original question is as follows: $0<x<3$ is the largest interval in which the solution of $(x^2-3x)y''+xy'-(x+3)y=0$ , $y(1)=2$ , $y'(1)=1$ is certain to exist? So I tried by taking y=uv as complete solution. Then I tried to apply Normal form method and got $u=1/\sqrt{x-3}$ . So by putting value of P and  Q in normal form formula I got, $v''+[4x^2+x-36/4x(x-3)^2]v=0$ . How should I solve further? Is there a 'nicer' way to solve.","I want to solve this differential equation: The original question is as follows: is the largest interval in which the solution of , , is certain to exist? So I tried by taking y=uv as complete solution. Then I tried to apply Normal form method and got . So by putting value of P and  Q in normal form formula I got, . How should I solve further? Is there a 'nicer' way to solve.",(x^2-3x)y''+xy'-(x+3)y=0 0<x<3 (x^2-3x)y''+xy'-(x+3)y=0 y(1)=2 y'(1)=1 u=1/\sqrt{x-3} v''+[4x^2+x-36/4x(x-3)^2]v=0,"['ordinary-differential-equations', 'derivatives']"
19,Simplest proof of Taylor's theorem,Simplest proof of Taylor's theorem,,"I have for some time been trawling through the Internet looking for an aesthetic proof of Taylor's theorem. By which I mean this: there are plenty of proofs that introduce some arbitrary construct: no mention is given of from whence this beast came.  and you can logically hack away line by line until the thing is solved. but this kind of proof is ugly.  a beautiful proof should rise naturally from the ground. I've seen one proof claiming to do it from the fundamental theorem of calculus. It looked messy. I've seen several attempts to use integration by parts repeatedly. But surely it would be tidier to do this without bringing in  all of that extra machinery. The nicest two approaches seem to involve using the mean value theorem and Rolle's theorem.  but I can't find a lucid presentation of either approach. Maybe my brain is unusually stupid, and the approaches on Wikipedia etc are perfectly good enough for everyone else. Does anyone have a crystal clear understanding of this phenomenon? Or a web-link to such an understanding? *EDIT*: Eventually a Cambridge mathematician explained it to me in a way that I could understand, and I have written up the proof here . To my mind it is the most instructional proof I have encountered, yet putting it as an answer received mostly downvotes. It seems strange to me that no one else seems to concur.  But it should be up to the keenest mathematical minds to choose which answer should be accepted. It shouldn't be up to me. Therefore I will bow to the wisdom of the community, and accept the currently most-upvoted answer. I have learned from Machine Learning that a ""Committee of Experts"" outperforms any one expert, and I am certainly no expert.","I have for some time been trawling through the Internet looking for an aesthetic proof of Taylor's theorem. By which I mean this: there are plenty of proofs that introduce some arbitrary construct: no mention is given of from whence this beast came.  and you can logically hack away line by line until the thing is solved. but this kind of proof is ugly.  a beautiful proof should rise naturally from the ground. I've seen one proof claiming to do it from the fundamental theorem of calculus. It looked messy. I've seen several attempts to use integration by parts repeatedly. But surely it would be tidier to do this without bringing in  all of that extra machinery. The nicest two approaches seem to involve using the mean value theorem and Rolle's theorem.  but I can't find a lucid presentation of either approach. Maybe my brain is unusually stupid, and the approaches on Wikipedia etc are perfectly good enough for everyone else. Does anyone have a crystal clear understanding of this phenomenon? Or a web-link to such an understanding? *EDIT*: Eventually a Cambridge mathematician explained it to me in a way that I could understand, and I have written up the proof here . To my mind it is the most instructional proof I have encountered, yet putting it as an answer received mostly downvotes. It seems strange to me that no one else seems to concur.  But it should be up to the keenest mathematical minds to choose which answer should be accepted. It shouldn't be up to me. Therefore I will bow to the wisdom of the community, and accept the currently most-upvoted answer. I have learned from Machine Learning that a ""Committee of Experts"" outperforms any one expert, and I am certainly no expert.",,"['real-analysis', 'soft-question', 'taylor-expansion']"
20,"Reference for time-derivatives of measure-valued ""processes""","Reference for time-derivatives of measure-valued ""processes""",,"The notion of the weak derivative of a process of probability measures $(\mu_t)_{t\geq 0}$ in optimal transport theory seems to be so basic that, e.g., in ""Optimal Transport, old and new"" the author hardly bothers to even define it. Apparently it's somehow defined by ""duality"" with smooth functions, maybe somewhat like $\partial_t \int \phi \,d\mu = \int \phi'\,d(\partial_t \mu)$ for test functions $\phi$ . For some more context, it appears for example in the conservation of mass formula (which I cannot explain any further, unfortunately): $$\partial_t \mu + \nabla \cdot (\mu \xi) = 0.$$ Where can I read about the definition of $\partial_t \mu$ , what conditions are necessary and perhaps some explicit examples? (As an aside I also wonder what $\mu \xi$ could mean for some ""velocity field"" (vector field?) $\xi$ ).","The notion of the weak derivative of a process of probability measures in optimal transport theory seems to be so basic that, e.g., in ""Optimal Transport, old and new"" the author hardly bothers to even define it. Apparently it's somehow defined by ""duality"" with smooth functions, maybe somewhat like for test functions . For some more context, it appears for example in the conservation of mass formula (which I cannot explain any further, unfortunately): Where can I read about the definition of , what conditions are necessary and perhaps some explicit examples? (As an aside I also wonder what could mean for some ""velocity field"" (vector field?) ).","(\mu_t)_{t\geq 0} \partial_t \int \phi \,d\mu = \int \phi'\,d(\partial_t \mu) \phi \partial_t \mu + \nabla \cdot (\mu \xi) = 0. \partial_t \mu \mu \xi \xi","['probability-theory', 'derivatives', 'reference-request', 'optimal-transport']"
21,"Is the $n^\text{th}$ derivative of some function, where $n$ is a matrix, ever defined?","Is the  derivative of some function, where  is a matrix, ever defined?",n^\text{th} n,"For the function $f(x) = x^a$ , the $n^\text{th}$ derivative $\frac{d^n}{dx^n}x^a = \frac{a!}{(a-n)!}x^{a-n}$ . This can be extended to non integer values of $n$ thanks to the gamma function. I recently red that factorials of matrices are a thing, using the gamma function. Also, one can raise a scalar to a matrix power if it is treated as a Maclaurin series. So, if in the above equation $n,a\in\mathbb{R}^{p\times q}$ , can we treat it as mathematically correct? More generally, can fractional calculus be extended so that we have matrix-th derivatives or even tensor-th derivatives?","For the function , the derivative . This can be extended to non integer values of thanks to the gamma function. I recently red that factorials of matrices are a thing, using the gamma function. Also, one can raise a scalar to a matrix power if it is treated as a Maclaurin series. So, if in the above equation , can we treat it as mathematically correct? More generally, can fractional calculus be extended so that we have matrix-th derivatives or even tensor-th derivatives?","f(x) = x^a n^\text{th} \frac{d^n}{dx^n}x^a = \frac{a!}{(a-n)!}x^{a-n} n n,a\in\mathbb{R}^{p\times q}","['matrices', 'derivatives', 'fractional-calculus']"
22,How to interpret a second derivative with respect to a vector/matrix?,How to interpret a second derivative with respect to a vector/matrix?,,"I found not many resources are devoted to matrix derivatives, esp. when you go beyond order one. Recently I came across this (seemingly very simple) equation in some literature (it would be appreciated if someone can help me make better sense of it): It is well known that $\frac{\partial{x^T Ax}}{\partial x} = (A^T + A)x$ . Now if I do another derivative w.r.t. $x$ , I got $\frac{\partial^2{x^T Ax}}{\partial x\partial x^T} = 2A$ . My confusion is that how you get $2A$ instead of $A^T + A$ for the second derivative, and in general, when we take second derivative w.r.t. some vectors/matrices, why do we transpose them?","I found not many resources are devoted to matrix derivatives, esp. when you go beyond order one. Recently I came across this (seemingly very simple) equation in some literature (it would be appreciated if someone can help me make better sense of it): It is well known that . Now if I do another derivative w.r.t. , I got . My confusion is that how you get instead of for the second derivative, and in general, when we take second derivative w.r.t. some vectors/matrices, why do we transpose them?",\frac{\partial{x^T Ax}}{\partial x} = (A^T + A)x x \frac{\partial^2{x^T Ax}}{\partial x\partial x^T} = 2A 2A A^T + A,"['matrices', 'derivatives', 'matrix-calculus']"
23,Cauchy derivation theorem of integrals demonstration,Cauchy derivation theorem of integrals demonstration,,I am studing the cauchy derivates theorem and there is one step which i don't understand. When we are proving for $f^{(n)}$ for math induction. The hipothese are: $$f^{(n-1)}(z):=\frac{(n-1)!}{2 \pi i} \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi$$ The step is: \begin{align} f^{(n)}&=\lim_{h \to 0} \frac{f^{(n-1)}(z+h)-f^{(n-1)}(z)}{h} \\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \bigg[ \int_{\partial D} \frac{f(\xi)}{(\xi - z - h )^n}d \xi - \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi\bigg] \\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{(\xi -z)^n-(\xi -z-h)^n}{(\xi - z - h )^n(\xi-z)^n}d \xi &(1) \\ &=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{nh(\xi -z)^{n-1}-\cdots}{(\xi - z - h )^n(\xi-z)^n}d \xi  &(2) \\ &=\frac{(n-1)!}{2 \pi i} \int_{\partial D}\lim_{h \to 0} \bigg[ f(\xi)\frac{n-\cdots}{(\xi - z - h )^n(\xi-z)} \bigg]d \xi  &(3) \\ &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \vdots \end{align} I don't understand the step from $(1)$ to $(2)$ .,I am studing the cauchy derivates theorem and there is one step which i don't understand. When we are proving for for math induction. The hipothese are: The step is: I don't understand the step from to .,"f^{(n)} f^{(n-1)}(z):=\frac{(n-1)!}{2 \pi i} \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi \begin{align}
f^{(n)}&=\lim_{h \to 0} \frac{f^{(n-1)}(z+h)-f^{(n-1)}(z)}{h}
\\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \bigg[ \int_{\partial D} \frac{f(\xi)}{(\xi - z - h )^n}d \xi - \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi\bigg]
\\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{(\xi -z)^n-(\xi -z-h)^n}{(\xi - z - h )^n(\xi-z)^n}d \xi &(1)
\\ &=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{nh(\xi -z)^{n-1}-\cdots}{(\xi - z - h )^n(\xi-z)^n}d \xi  &(2)
\\ &=\frac{(n-1)!}{2 \pi i} \int_{\partial D}\lim_{h \to 0} \bigg[ f(\xi)\frac{n-\cdots}{(\xi - z - h )^n(\xi-z)} \bigg]d \xi  &(3)
\\ &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \vdots
\end{align} (1) (2)","['complex-analysis', 'derivatives', 'induction', 'cauchy-integral-formula']"
24,Derivative Product rule for non-commutative objects?,Derivative Product rule for non-commutative objects?,,"lets say I have objects $f$ and $g$ , for which one can define a derivative with the typical properties. The product rule would be expected to be $$ d(fg)=(df)g+f\,dg $$ But what if $f$ and $g$ are not commutative? $$ [f,g]\neq 0\implies d(fg)=\text{?} $$ Does the product rule get modified?","lets say I have objects and , for which one can define a derivative with the typical properties. The product rule would be expected to be But what if and are not commutative? Does the product rule get modified?","f g 
d(fg)=(df)g+f\,dg
 f g 
[f,g]\neq 0\implies d(fg)=\text{?}
",['derivatives']
25,Partial derivative of a fraction with a summation in the denominator,Partial derivative of a fraction with a summation in the denominator,,"Suppose I have the following function $$ f(\beta_1,\beta_2,s,p)=\frac{e^{\beta_1 s+\beta_2 s^2}}{\sum_{s=1}^p e^{\beta_1 s+\beta_2 s^2}}, $$ which can be rewritten as $$ e^{\beta_1 s+\beta_2 s^2}\cdot\sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}. $$ I would like to find $\frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1}$ . My approach: $\frac{\partial e^{\beta_1 s+\beta_2 s^2}}{\partial \beta_1} = s \cdot e^{\beta_1 s+\beta_2 s^2} $ $\frac{\sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}}{\partial \beta_1} = \sum_{s=1}^p -s \cdot e^{-(\beta_1 s+\beta_2 s^2)}$ Yielding according to the product rule: $\frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1}=s \cdot e^{\beta_1 s+\beta_2 s^2}\cdot \sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}+e^{\beta_1 s+\beta_2 s^2} \cdot \sum_{s=1}^p -s \cdot e^{-(\beta_1 s+\beta_2 s^2)}$ Can anyone confirm whether or not this is correct? I look forward to your suggestions! Another approach: $\frac{\sum_{s=1}^p e^{(\beta_1 s+\beta_2 s^2)}}{\partial \beta_1} = \sum_{s=1}^p s \cdot e^{(\beta_1 s+\beta_2 s^2)}$ Then using the quotient rule $\frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1}=\frac{s \cdot e^{\beta_1 s+\beta_2 s^2} \cdot \sum_{s=1}^p e^{(\beta_1 s+\beta_2 s^2)}+ e^{(\beta_1 s+\beta_2 s^2)}\cdot \sum_{s=1}^p s\cdot e^{(\beta_1 s+\beta_2 s^2)} }{(\sum_{s=1}^p\cdot e^{(\beta_1 s+\beta_2 s^2)})^2}$",Suppose I have the following function which can be rewritten as I would like to find . My approach: Yielding according to the product rule: Can anyone confirm whether or not this is correct? I look forward to your suggestions! Another approach: Then using the quotient rule,"
f(\beta_1,\beta_2,s,p)=\frac{e^{\beta_1 s+\beta_2 s^2}}{\sum_{s=1}^p e^{\beta_1 s+\beta_2 s^2}},
 
e^{\beta_1 s+\beta_2 s^2}\cdot\sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}.
 \frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1} \frac{\partial e^{\beta_1 s+\beta_2 s^2}}{\partial \beta_1} = s \cdot e^{\beta_1 s+\beta_2 s^2}  \frac{\sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}}{\partial \beta_1} = \sum_{s=1}^p -s \cdot e^{-(\beta_1 s+\beta_2 s^2)} \frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1}=s \cdot e^{\beta_1 s+\beta_2 s^2}\cdot \sum_{s=1}^p e^{-(\beta_1 s+\beta_2 s^2)}+e^{\beta_1 s+\beta_2 s^2} \cdot \sum_{s=1}^p -s \cdot e^{-(\beta_1 s+\beta_2 s^2)} \frac{\sum_{s=1}^p e^{(\beta_1 s+\beta_2 s^2)}}{\partial \beta_1} = \sum_{s=1}^p s \cdot e^{(\beta_1 s+\beta_2 s^2)} \frac{\partial f(\beta_1,\beta_2,s,p)}{\partial \beta_1}=\frac{s \cdot e^{\beta_1 s+\beta_2 s^2} \cdot \sum_{s=1}^p e^{(\beta_1 s+\beta_2 s^2)}+ e^{(\beta_1 s+\beta_2 s^2)}\cdot \sum_{s=1}^p s\cdot e^{(\beta_1 s+\beta_2 s^2)} }{(\sum_{s=1}^p\cdot e^{(\beta_1 s+\beta_2 s^2)})^2}","['derivatives', 'summation', 'partial-derivative']"
26,Recursive chain rule,Recursive chain rule,,"Given the following equations: $$ \begin{aligned} o_t&=\sigma(x_t, h_{t-1};W_o) \\ \tilde{c}_t&=\tanh(x_t, h_{t-1};W_g) \\ f_t&=\sigma(x_t, h_{t-1};W_f) \\ i_t&=\sigma(x_t, h_{t-1};W_i) \\ c_t&=f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t} \\ &=\sigma(x_t, h_{t-1};W_f)\odot{c_{t-1}}+\sigma(x_t, h_{t-1};W_i)\odot{\tanh(x_t, h_{t-1};W_g)} \\ h_t&=o_t\odot\tanh(c_t) \\ \end{aligned} $$ Lower case variables are vectors and $W$ 's are matrices. $x_t$ is a new input at each time. The subscript $t$ denotes time from $t=1,\dots N$ , where $h_0=c_0$ are zeros vectors. As can be seen these are recursive equations from which $h_t$ comes from a function of $c_t$ which comes from a function of $c_{t-1}$ . And I'm trying to derive: $$\frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial}{\partial c_{t-1}}(f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t}).$$ So my question is given that $f_t$ , $i_t$ and $g_t$ are functions of $h_{t-1}$ which is a function of $c_{t-1}$ does that matter? Is this derivative just equal to $f_t$ ? Or do the other terms need to be accounted for such that the derivation is something like: $$\frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial{f_t}}{\partial{c_{t-1}}}\odot{c_{t-1}}+f_t\odot\frac{\partial{c_{t-1}}}{\partial{c_{t-1}}}+\frac{\partial{i_t}}{\partial{c_{t-1}}}\odot{\tilde{c}_t}+i_t\odot\frac{\partial{\tilde{c}_t}}{\partial{c_{t-1}}}?$$ Any help would be greatly appreciated.","Given the following equations: Lower case variables are vectors and 's are matrices. is a new input at each time. The subscript denotes time from , where are zeros vectors. As can be seen these are recursive equations from which comes from a function of which comes from a function of . And I'm trying to derive: So my question is given that , and are functions of which is a function of does that matter? Is this derivative just equal to ? Or do the other terms need to be accounted for such that the derivation is something like: Any help would be greatly appreciated.","
\begin{aligned}
o_t&=\sigma(x_t, h_{t-1};W_o) \\
\tilde{c}_t&=\tanh(x_t, h_{t-1};W_g) \\
f_t&=\sigma(x_t, h_{t-1};W_f) \\
i_t&=\sigma(x_t, h_{t-1};W_i) \\
c_t&=f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t} \\
&=\sigma(x_t, h_{t-1};W_f)\odot{c_{t-1}}+\sigma(x_t, h_{t-1};W_i)\odot{\tanh(x_t, h_{t-1};W_g)} \\
h_t&=o_t\odot\tanh(c_t) \\
\end{aligned}
 W x_t t t=1,\dots N h_0=c_0 h_t c_t c_{t-1} \frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial}{\partial c_{t-1}}(f_t\odot{c_{t-1}}+i_t\odot{\tilde{c}_t}). f_t i_t g_t h_{t-1} c_{t-1} f_t \frac{\partial c_t}{\partial c_{t-1}}=\frac{\partial{f_t}}{\partial{c_{t-1}}}\odot{c_{t-1}}+f_t\odot\frac{\partial{c_{t-1}}}{\partial{c_{t-1}}}+\frac{\partial{i_t}}{\partial{c_{t-1}}}\odot{\tilde{c}_t}+i_t\odot\frac{\partial{\tilde{c}_t}}{\partial{c_{t-1}}}?","['multivariable-calculus', 'derivatives', 'partial-derivative', 'chain-rule', 'hadamard-product']"
27,Calculating gradient of scalar with vector jacobian products,Calculating gradient of scalar with vector jacobian products,,"I am trying to calculate the $\phi$ gradient of the scalar \begin{align*}  F = \sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\lambda_i \tag{1} \end{align*} $f, \theta \in \mathcal{R}^{m}$ , $h_i \in \mathcal{R}^k$ , $h_i$ is a shorthand for $h_i(x_i, \theta)$ where $x_i \in \mathcal{R}^k$ . $\lambda_i \in \mathcal{R}^k$ and are defined by \begin{align*}  &\lambda_{n} = 0 \\ &\lambda_{i-1} = \dfrac{\partial h_i}{\partial x_i}^T\lambda_i + \dfrac{\partial p_i}{\partial x_i}^T\beta_i(\phi) \end{align*} where $\beta_i$ and $p_i = p_i(x_i)$ are scalars. Supposedly, using vector-jacobian products (i.e. reverse mode differentiation) the gradient $\nabla_{\phi}F$ can be calculated in some constant times complexity as $F$ . However, after much trying I am unable to express the gradient using only vector-jacobian products. I can only get it down to either matrix-jacobian products, or jacobian-vector products. Either way, this means calculating the gradient would scale with the dimension of $x$ , meaning it would not be constant, but scale with $k$ . This leaves me confused as to whether I am doing something wrong, or there is some sort of caveat to reverse differentiation that I'm not aware of. I highly suspect the former but I'm not really sure what to do differently. Gradient calculation Method A Using \begin{align*}  \dfrac{\partial \lambda_{i-1}}{\partial \phi} = \dfrac{\partial h_i}{\partial x_i}^T\dfrac{\partial \lambda_i}{\partial \phi} + \dfrac{\partial p_i}{\partial x_i}^T\dfrac{\partial \beta_i}{\partial \phi} \end{align*} so that the gradient is \begin{align*}  \sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\dfrac{\partial \lambda_i }{\partial \phi} . \end{align*} You can massage this further, but you always end up having to form some sort of matrix in $\mathcal{R}^{k\times k}$ . Method B The gradient is \begin{align*}  \sum_{i=0}^{n-1}\xi_i^T\dfrac{\partial p_{i+1}}{\partial x_{i+1}}^T\dfrac{\partial \beta_{1+i}}{\partial \phi} \end{align*} where $\xi$ are given by \begin{align*}  &\xi_0^T = f^T\dfrac{\partial h_0}{\partial \theta}^T \\ & \xi_{i+1}^T = f^T\dfrac{\partial h_{i+1}}{\theta}^T + \xi_i^T\dfrac{\partial h_{i+1}}{\partial x_{i+1}}^T \end{align*} In this case you have to calculate $\partial h_{i+1}/\partial x_{i+1}$ .","I am trying to calculate the gradient of the scalar , , is a shorthand for where . and are defined by where and are scalars. Supposedly, using vector-jacobian products (i.e. reverse mode differentiation) the gradient can be calculated in some constant times complexity as . However, after much trying I am unable to express the gradient using only vector-jacobian products. I can only get it down to either matrix-jacobian products, or jacobian-vector products. Either way, this means calculating the gradient would scale with the dimension of , meaning it would not be constant, but scale with . This leaves me confused as to whether I am doing something wrong, or there is some sort of caveat to reverse differentiation that I'm not aware of. I highly suspect the former but I'm not really sure what to do differently. Gradient calculation Method A Using so that the gradient is You can massage this further, but you always end up having to form some sort of matrix in . Method B The gradient is where are given by In this case you have to calculate .","\phi \begin{align*} 
F = \sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\lambda_i \tag{1}
\end{align*} f, \theta \in \mathcal{R}^{m} h_i \in \mathcal{R}^k h_i h_i(x_i, \theta) x_i \in \mathcal{R}^k \lambda_i \in \mathcal{R}^k \begin{align*} 
&\lambda_{n} = 0 \\
&\lambda_{i-1} = \dfrac{\partial h_i}{\partial x_i}^T\lambda_i + \dfrac{\partial p_i}{\partial x_i}^T\beta_i(\phi)
\end{align*} \beta_i p_i = p_i(x_i) \nabla_{\phi}F F x k \begin{align*} 
\dfrac{\partial \lambda_{i-1}}{\partial \phi} = \dfrac{\partial h_i}{\partial x_i}^T\dfrac{\partial \lambda_i}{\partial \phi} + \dfrac{\partial p_i}{\partial x_i}^T\dfrac{\partial \beta_i}{\partial \phi}
\end{align*} \begin{align*} 
\sum_{i=0}^nf^T\dfrac{\partial h_i}{\partial \theta}^T\dfrac{\partial \lambda_i }{\partial \phi} .
\end{align*} \mathcal{R}^{k\times k} \begin{align*} 
\sum_{i=0}^{n-1}\xi_i^T\dfrac{\partial p_{i+1}}{\partial x_{i+1}}^T\dfrac{\partial \beta_{1+i}}{\partial \phi}
\end{align*} \xi \begin{align*} 
&\xi_0^T = f^T\dfrac{\partial h_0}{\partial \theta}^T \\
& \xi_{i+1}^T = f^T\dfrac{\partial h_{i+1}}{\theta}^T + \xi_i^T\dfrac{\partial h_{i+1}}{\partial x_{i+1}}^T
\end{align*} \partial h_{i+1}/\partial x_{i+1}","['derivatives', 'matrix-calculus', 'jacobian']"
28,Calculating the Derivative of a Norm,Calculating the Derivative of a Norm,,"Let $\alpha$ be a positive real number (i.e. $\alpha > 0)$ , and consider $r\left(\alpha\right) := \| Ax_{\alpha}^{\delta} - y^{\delta} \|$ , where $A: X\rightarrow Y$ is a linear operator between finite-dimensional vector spaces. Now, also define $\Psi(\alpha) := \vert\vert x_{\alpha}^{\delta} \vert\vert$ . In a lecture, we said that $$\frac{\mathrm dr}{\mathrm d\Psi}$$ would be negative for all $\alpha$ , but I don't see this yet. I think the norm which we considered was the usual Euclidean norm, so we have $$\frac{\mathrm dr}{\mathrm d\Psi} = \frac{\mathrm d \sqrt{\|Ax_{\alpha}^{\delta}\|^2 - 2A x_{\alpha}^{\delta}y^{\delta} + \| y^{\delta} \|^2}}{\mathrm d\| x_{\alpha}^{\delta}\|}.$$ I am not quite sure how to differentiate $\|Ax_{\alpha}^{\delta}\|^2$ and $- 2A x_{\alpha}^{\delta}y^{\delta}$ with respect to $\| x_{\alpha}^{\delta} \|$ . In case is helps: We know that $\Psi(\alpha)$ is monotone decreasing in $\alpha$ . (I know I didn't write what $x_{\alpha}^{\delta}$ and $y^{\delta}$ are, but I believe that this is rather irrelevant for my question.)","Let be a positive real number (i.e. , and consider , where is a linear operator between finite-dimensional vector spaces. Now, also define . In a lecture, we said that would be negative for all , but I don't see this yet. I think the norm which we considered was the usual Euclidean norm, so we have I am not quite sure how to differentiate and with respect to . In case is helps: We know that is monotone decreasing in . (I know I didn't write what and are, but I believe that this is rather irrelevant for my question.)",\alpha \alpha > 0) r\left(\alpha\right) := \| Ax_{\alpha}^{\delta} - y^{\delta} \| A: X\rightarrow Y \Psi(\alpha) := \vert\vert x_{\alpha}^{\delta} \vert\vert \frac{\mathrm dr}{\mathrm d\Psi} \alpha \frac{\mathrm dr}{\mathrm d\Psi} = \frac{\mathrm d \sqrt{\|Ax_{\alpha}^{\delta}\|^2 - 2A x_{\alpha}^{\delta}y^{\delta} + \| y^{\delta} \|^2}}{\mathrm d\| x_{\alpha}^{\delta}\|}. \|Ax_{\alpha}^{\delta}\|^2 - 2A x_{\alpha}^{\delta}y^{\delta} \| x_{\alpha}^{\delta} \| \Psi(\alpha) \alpha x_{\alpha}^{\delta} y^{\delta},"['analysis', 'derivatives']"
29,A formula for the $n^{th}$ derivative of a parametric equation,A formula for the  derivative of a parametric equation,n^{th},"Assume $x(t) \in C^{(n)}[0,\infty)$ . I am trying to find a formula for the $n^{th}$ derivative of $$Q_j(t) = e^{-x(t)}\,\frac{x(t)^j}{j!}, $$ in terms of $Q_0(t), Q_1(t), ..., Q_j(t)$ . (Or any other useful recursive formula in terms of previous derivatives, etc.) So far I have tried this: $$ Q^{'}_0(t) = -x^{'}(t)\,Q_0(t),$$ and for $j\geq1$ , $$ Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right).$$ Or we can assume $0=Q_{-1}(t)=Q_{-2}(t)=...$ and write it in a general form for $j\geq0$ , $$ Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right).$$ Therefore, for $j\geq0$ , the second derivative is \begin{equation} \begin{split}  Q_j^{""}(t) &= x^{""}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right) + (x^{'}(t))^2\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right).  \end{split} \end{equation} And for the 3rd derivative I got this, \begin{equation} \begin{split} Q^{(3)}_j(t) &= x^{(3)}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right)+ 3\,x^{'}(t)\,x^{""}(t)\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right) \\ & + (x^{'}(t))^3\,\left(Q_{j-3}(t)-3\,Q_{j-2}(t)+3\,Q_{j-1}(t)+ Q_{j}(t)\right). \end{split} \end{equation} Can we see a pattern here?","Assume . I am trying to find a formula for the derivative of in terms of . (Or any other useful recursive formula in terms of previous derivatives, etc.) So far I have tried this: and for , Or we can assume and write it in a general form for , Therefore, for , the second derivative is And for the 3rd derivative I got this, Can we see a pattern here?","x(t) \in C^{(n)}[0,\infty) n^{th} Q_j(t) = e^{-x(t)}\,\frac{x(t)^j}{j!},  Q_0(t), Q_1(t), ..., Q_j(t)  Q^{'}_0(t) = -x^{'}(t)\,Q_0(t), j\geq1  Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right). 0=Q_{-1}(t)=Q_{-2}(t)=... j\geq0  Q_j^{'}(t) = x^{'}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right). j\geq0 \begin{equation}
\begin{split}
 Q_j^{""}(t) &= x^{""}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right) + (x^{'}(t))^2\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right). 
\end{split}
\end{equation} \begin{equation}
\begin{split}
Q^{(3)}_j(t) &= x^{(3)}(t)\,\left(Q_{j-1}(t)-Q_j(t)\right)+ 3\,x^{'}(t)\,x^{""}(t)\,\left(Q_{j-2}(t)-2\,Q_{j-1}(t)+Q_{j}(t)\right) \\
& + (x^{'}(t))^3\,\left(Q_{j-3}(t)-3\,Q_{j-2}(t)+3\,Q_{j-1}(t)+ Q_{j}(t)\right).
\end{split}
\end{equation}","['derivatives', 'recursion', 'parametric']"
30,SDE representation implies differentiability? (Converse of Ito's Lemma),SDE representation implies differentiability? (Converse of Ito's Lemma),,"I'm trying to establish that a function is differentiable when the value of the function follows an SDE. In a sense, I am trying to establish a converse of Ito's Lemma. More specifically, I know two pieces of information: I know that a process $(\xi_t)_{t\ge0}$ follows an SDE $$ d \xi_t = (\delta \xi_t - X_t)dt + \omega_t dW_t,  $$ where $(W_t)_{t\ge 0}$ is a standard Brownian motion and $(X_t)_{t \ge 0}$ is some adapted process. $(\omega_t)_{t\ge 0}$ is also an adapted process. I also know that $\xi_t = G(Y_t, V_t)$ for a continuous function $G$ . Here, $(Y_t)_{t\ge 0}$ and $(V_t)_{t\ge 0}$ are two adapted processes, with volatilities $\sigma_Y(Y_t,V_t)$ and $\sigma_V(Y_t,V_t)$ , respectively. Based on this information, can I conclude that $$    \omega_t = G_Y(Y_t,V_t) \sigma_Y(Y_t,V_t) + G_V(Y_t,V_t) \sigma_V(Y_t,V_t)? $$ Or, more generally, can I conclude that the function $G$ is differentiable?","I'm trying to establish that a function is differentiable when the value of the function follows an SDE. In a sense, I am trying to establish a converse of Ito's Lemma. More specifically, I know two pieces of information: I know that a process follows an SDE where is a standard Brownian motion and is some adapted process. is also an adapted process. I also know that for a continuous function . Here, and are two adapted processes, with volatilities and , respectively. Based on this information, can I conclude that Or, more generally, can I conclude that the function is differentiable?","(\xi_t)_{t\ge0} 
d \xi_t = (\delta \xi_t - X_t)dt + \omega_t dW_t, 
 (W_t)_{t\ge 0} (X_t)_{t \ge 0} (\omega_t)_{t\ge 0} \xi_t = G(Y_t, V_t) G (Y_t)_{t\ge 0} (V_t)_{t\ge 0} \sigma_Y(Y_t,V_t) \sigma_V(Y_t,V_t) 
   \omega_t = G_Y(Y_t,V_t) \sigma_Y(Y_t,V_t) + G_V(Y_t,V_t) \sigma_V(Y_t,V_t)?
 G","['derivatives', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-differential-equations']"
31,Equivalent definitions of Fréchet differentiability,Equivalent definitions of Fréchet differentiability,,"In their book The Ricci Flow in Riemannian Geometry Andrews and Hopper have an appendix on Gâteaux and Fréchet differentiability. They define Gâteaux differentiability as follows: Let $X,Y$ be Banach spaces. A function $f: X \rightarrow Y$ is said to be Gâteaux differentiable at $x$ if there exists a bounded linear operator $T_x \in L(X,Y)$ such that for all $v \in X$ , $$\lim_{t \to 0} \frac{f(x+tv)-f(x)}{t} = T_x v.$$ The operator $T_x$ is called the Gâteaux derivative . This is the definition I was presented with in class. Fréchet differentiability, however, is defined differently than in class (In class we used the second definition mentioned in the following passage): If the limit (in the sense of the Gâteaux derivative) exists uniformly in $v$ on the unit sphere of X, we say $f$ is Fréchet differentiable at $x$ and $T_x$ is the Fréchet derivative of $f$ at $x$ . Equivalently, if we set $y=tv$ then $t \rightarrow 0$ if and only if $y \rightarrow 0$ . Thus $f$ is Fréchet differentiable at $x$ if  for all $y$ , $$f(x+y)-f(x)-T_x(y)=o(\|y\|).$$ Questions What does ""the limit exists uniformly in $v$ on the unit sphere of X"" mean exactly? I know what uniform convergence is in regards to a sequence of functions. However, I cannot figure out how this concept relates to the one presented here, i.e. what sequence of functions is considered here? How are the two presented definitions of Fréchet differentiability equivalent?","In their book The Ricci Flow in Riemannian Geometry Andrews and Hopper have an appendix on Gâteaux and Fréchet differentiability. They define Gâteaux differentiability as follows: Let be Banach spaces. A function is said to be Gâteaux differentiable at if there exists a bounded linear operator such that for all , The operator is called the Gâteaux derivative . This is the definition I was presented with in class. Fréchet differentiability, however, is defined differently than in class (In class we used the second definition mentioned in the following passage): If the limit (in the sense of the Gâteaux derivative) exists uniformly in on the unit sphere of X, we say is Fréchet differentiable at and is the Fréchet derivative of at . Equivalently, if we set then if and only if . Thus is Fréchet differentiable at if  for all , Questions What does ""the limit exists uniformly in on the unit sphere of X"" mean exactly? I know what uniform convergence is in regards to a sequence of functions. However, I cannot figure out how this concept relates to the one presented here, i.e. what sequence of functions is considered here? How are the two presented definitions of Fréchet differentiability equivalent?","X,Y f: X \rightarrow Y x T_x \in L(X,Y) v \in X \lim_{t \to 0} \frac{f(x+tv)-f(x)}{t} = T_x v. T_x v f x T_x f x y=tv t \rightarrow 0 y \rightarrow 0 f x y f(x+y)-f(x)-T_x(y)=o(\|y\|). v","['functional-analysis', 'derivatives', 'definition', 'frechet-derivative']"
32,Derivative Intuition [duplicate],Derivative Intuition [duplicate],,"This question already has answers here : The problem of instant velocity (15 answers) Closed 3 years ago . I'm doing self-study in Calculus, and I'm having trouble with intuitively understanding derivatives. The definition of a derivative is: $\lim _{h\to 0}\left(\frac{f\left(x+h\right)-f\left(x\right)}{h}\right)$ . Also, I will note that H Does not Equal Zero. Here is the issue I'm having: As $\lim _{h\to 0}\:$ does not equal zero, and therefore $h$ does not equal $0$ , there will always be some infinitesimally small distance between $x$ and $x+h$ . Therefore, the derivative is NOT a rate of change at one point, but a rate of change between $x$ and some small infinitesimal distance. I suppose one could say the slope between $x$ and $x+\text{infinitesimal}$ is close to a rate of change at a single point, but it's not the same thing. Maybe an approximation, but an approximation is not the same as the actual. What am I missing here? Thanks in advance for any assistance!","This question already has answers here : The problem of instant velocity (15 answers) Closed 3 years ago . I'm doing self-study in Calculus, and I'm having trouble with intuitively understanding derivatives. The definition of a derivative is: . Also, I will note that H Does not Equal Zero. Here is the issue I'm having: As does not equal zero, and therefore does not equal , there will always be some infinitesimally small distance between and . Therefore, the derivative is NOT a rate of change at one point, but a rate of change between and some small infinitesimal distance. I suppose one could say the slope between and is close to a rate of change at a single point, but it's not the same thing. Maybe an approximation, but an approximation is not the same as the actual. What am I missing here? Thanks in advance for any assistance!",\lim _{h\to 0}\left(\frac{f\left(x+h\right)-f\left(x\right)}{h}\right) \lim _{h\to 0}\: h 0 x x+h x x x+\text{infinitesimal},"['calculus', 'derivatives']"
33,Proving chain rule for Caratheodory derivative,Proving chain rule for Caratheodory derivative,,"I am trying to follow the argument given in the following papers: S. Kuhn, ""The Derivative a la Caratheodory"" (1991); E. Acosta G., C. Delgado G., ""Frechet vs. Caratheodory"" (1994); S. Arora, H. Browne, D. Daners, ""An alternative approach to Frechet derivatives"" (2019). Definition of differentiability. Function $f : \mathbb{R}^n \to \mathbb{R}^m $ is said to be differentiable (in a sense of Caratheodory) at point $x \in \mathbb{R}^n$ if there is a one-parameter family of linear functions $\Phi_{x, \cdot} : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$ that for all $y$ satisfies the following condition $$ \Phi_{x,y}(y-x) = f(y)-f(x) $$ and is continuous at $x$ , in other words, $$ \lim \limits_{y \to x} \Phi_{x,y} = \Phi_{x,x}. $$ Here, limit is to be understood in the usual sense where there is some metric on the space of all linear functions from $\mathbb{R}^n$ to $\mathbb{R}^m$ (usually taken to be operator norm). Derivative of $f$ at $x$ is then said to be linear function $\Phi_{x,x}$ . Proof of the chain rule given in the papers. Let us assume that we are given functions $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^p$ which are differentiable at $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ respectively. In that case, let us start by thinking of family of linear functions that might be reasonable for $g \circ f$ . Here, we use that $f$ and $g$ are differentiable and we use some one-parameter family of linear functions $\Phi^{f}_{x, \cdot}$ and $\Phi^{g}_{f(x), \cdot}$ . $$ (g \circ f)(y) - (g \circ f)(x) = \Phi^{g}_{f(x), f(y)}(f(y)-f(x)) = (\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}) (y-x)$$ This motivates to consider one-parameter family of linear functions given as follows. $$ \Phi^{g \circ f}_{x,\cdot} = \Phi^{g}_{f(x), f(\cdot)} \circ \Phi^{f}_{x,\cdot} $$ Now, the only remaining thing is to show that it is continuous at $x$ . Part of the proof I do not understand: The claim is that this one-parameter family of linear function $\Phi^{g \circ f}_{x,\cdot} $ is continuous because $\Phi^{g}_{f(x), \cdot}$ , $\Phi^{f}_{x, \cdot}$ and $f$ are all continuous functions and composition of continuous functions is again continuous. I have trouble understanding this as I think that the composition of one-parameter families of linear functions is not with respect to variable $y$ . My approach: Instead, I would propose the following proof where the key idea is not that the composition of continuous functions is continuous but rather that if we take operator norm then it has submultiplicativity property, so the proof is similar to the proof of limit of product where the product is now the composition. Use that functions are linear and triangle inequality to have the following. $$|\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}| = $$ $$ = |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x}) + (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} + \Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) | $$ $$ \leq |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x})| +  | (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) |$$ Now, use submultiplicative property of operator norm to bound norm of composition by a multiplication of individual norms. $$|\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}| $$ $$  \leq |\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| +  |\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}| | \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y} - \Phi^{f}_{x,x}| $$ Now, from continuity of one-parameter family of linear functions and from continuity of $f$ (as it is differentiable), we get that there is $\delta_1 > 0$ such that if $0 < |y-x| < \delta_1$ then $|f(y) - f(x)| < \delta_2$ and $|\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}|< \textrm{min}(1,\varepsilon)/(3+| \Phi^{f}_{x,x} |)$ . Also, there is $\delta_2 > 0$ such that if $0 < |y-x| < \delta_2$ such that $|\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| < \textrm{min}(1,\varepsilon)/(3+|\Phi^{g}_{f(x), f(x)}|)$ . Then, taking $\delta = \textrm{min}(\delta_1, \delta_2)$ gives the desired results. Problems with my approach: even if we assume that my proof is correct, this is somewhat in contradiction with what authors claim about why Caratheodory derivative might be useful. The usual claim is that all proofs become easier as one just needs to consider continuity of functions and does not rely on many computations with $\varepsilon-\delta$ . On the other hand, my proof shows that Caratheodory derivative approach requires many details to be proved, for example, it requires that out of all norms we use norm which is submultiplicative (not obvious to me why such choice of norm would be natural or obvious), and proof required to work with $\varepsilon - \delta$ . Also, it is not clear to me how (and if possible) to rewrite my approach in a more concise way using only that composition of continuous functions is continuous (or a similar result).","I am trying to follow the argument given in the following papers: S. Kuhn, ""The Derivative a la Caratheodory"" (1991); E. Acosta G., C. Delgado G., ""Frechet vs. Caratheodory"" (1994); S. Arora, H. Browne, D. Daners, ""An alternative approach to Frechet derivatives"" (2019). Definition of differentiability. Function is said to be differentiable (in a sense of Caratheodory) at point if there is a one-parameter family of linear functions that for all satisfies the following condition and is continuous at , in other words, Here, limit is to be understood in the usual sense where there is some metric on the space of all linear functions from to (usually taken to be operator norm). Derivative of at is then said to be linear function . Proof of the chain rule given in the papers. Let us assume that we are given functions and which are differentiable at and respectively. In that case, let us start by thinking of family of linear functions that might be reasonable for . Here, we use that and are differentiable and we use some one-parameter family of linear functions and . This motivates to consider one-parameter family of linear functions given as follows. Now, the only remaining thing is to show that it is continuous at . Part of the proof I do not understand: The claim is that this one-parameter family of linear function is continuous because , and are all continuous functions and composition of continuous functions is again continuous. I have trouble understanding this as I think that the composition of one-parameter families of linear functions is not with respect to variable . My approach: Instead, I would propose the following proof where the key idea is not that the composition of continuous functions is continuous but rather that if we take operator norm then it has submultiplicativity property, so the proof is similar to the proof of limit of product where the product is now the composition. Use that functions are linear and triangle inequality to have the following. Now, use submultiplicative property of operator norm to bound norm of composition by a multiplication of individual norms. Now, from continuity of one-parameter family of linear functions and from continuity of (as it is differentiable), we get that there is such that if then and . Also, there is such that if such that . Then, taking gives the desired results. Problems with my approach: even if we assume that my proof is correct, this is somewhat in contradiction with what authors claim about why Caratheodory derivative might be useful. The usual claim is that all proofs become easier as one just needs to consider continuity of functions and does not rely on many computations with . On the other hand, my proof shows that Caratheodory derivative approach requires many details to be proved, for example, it requires that out of all norms we use norm which is submultiplicative (not obvious to me why such choice of norm would be natural or obvious), and proof required to work with . Also, it is not clear to me how (and if possible) to rewrite my approach in a more concise way using only that composition of continuous functions is continuous (or a similar result).","f : \mathbb{R}^n \to \mathbb{R}^m  x \in \mathbb{R}^n \Phi_{x, \cdot} : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m) y  \Phi_{x,y}(y-x) = f(y)-f(x)  x  \lim \limits_{y \to x} \Phi_{x,y} = \Phi_{x,x}.  \mathbb{R}^n \mathbb{R}^m f x \Phi_{x,x} f : \mathbb{R}^n \to \mathbb{R}^m g : \mathbb{R}^m \to \mathbb{R}^p x \in \mathbb{R}^n f(x) \in \mathbb{R}^m g \circ f f g \Phi^{f}_{x, \cdot} \Phi^{g}_{f(x), \cdot}  (g \circ f)(y) - (g \circ f)(x) = \Phi^{g}_{f(x), f(y)}(f(y)-f(x)) = (\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}) (y-x)  \Phi^{g \circ f}_{x,\cdot} = \Phi^{g}_{f(x), f(\cdot)} \circ \Phi^{f}_{x,\cdot}  x \Phi^{g \circ f}_{x,\cdot}  \Phi^{g}_{f(x), \cdot} \Phi^{f}_{x, \cdot} f y |\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}| =   = |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x})
+ (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} + \Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) |   \leq |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x})| + 
| (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) | |\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}|    \leq |\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| + 
|\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}| | \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y} - \Phi^{f}_{x,x}|  f \delta_1 > 0 0 < |y-x| < \delta_1 |f(y) - f(x)| < \delta_2 |\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}|< \textrm{min}(1,\varepsilon)/(3+| \Phi^{f}_{x,x} |) \delta_2 > 0 0 < |y-x| < \delta_2 |\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| < \textrm{min}(1,\varepsilon)/(3+|\Phi^{g}_{f(x), f(x)}|) \delta = \textrm{min}(\delta_1, \delta_2) \varepsilon-\delta \varepsilon - \delta","['real-analysis', 'derivatives', 'chain-rule']"
34,"Let $f(x)$ be a thrice differentiable function in $[a,b]$, and $a<k_1<k_2<k_3<k_4<k_5<b$ and $f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1$","Let  be a thrice differentiable function in , and  and","f(x) [a,b] a<k_1<k_2<k_3<k_4<k_5<b f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1","Let $f(x)$ be a thrice differentiable function in $[a,b]$ , and $a<k_1<k_2<k_3<k_4<k_5<b$ and $f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1$ , then min number of root of the equation $f(x)f'''(x)+f'(x)f''(x)=0$ in $[a,b]$ Graphing the function $f(x)$ (rough sketch) gives a minimum of 4 roots. Also the given expression can be written as $$g(x)=\frac {d(f(x)f''(x))}{dx}$$ Now to satisfy the minimum root condition, let us assume that the roots of $f(x)$ and $f''(x)$ coincide. so $f(x)f''(x)$ will have a minimum of four roots. Now since the derivative will have one root less than the original function, $g(x)$ will have at least 3 roots. However the correct answer is 4. I suspect this assumption of mine was wrong let us assume that the roots of $f(x)$ and $f''(x)$ coincide But I am not able to prove that it is wrong. Is there a proper solution for this?","Let be a thrice differentiable function in , and and , then min number of root of the equation in Graphing the function (rough sketch) gives a minimum of 4 roots. Also the given expression can be written as Now to satisfy the minimum root condition, let us assume that the roots of and coincide. so will have a minimum of four roots. Now since the derivative will have one root less than the original function, will have at least 3 roots. However the correct answer is 4. I suspect this assumption of mine was wrong let us assume that the roots of and coincide But I am not able to prove that it is wrong. Is there a proper solution for this?","f(x) [a,b] a<k_1<k_2<k_3<k_4<k_5<b f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1 f(x)f'''(x)+f'(x)f''(x)=0 [a,b] f(x) g(x)=\frac {d(f(x)f''(x))}{dx} f(x) f''(x) f(x)f''(x) g(x) f(x) f''(x)","['calculus', 'functions', 'derivatives']"
35,Question about everywhere differentiable functions,Question about everywhere differentiable functions,,"Suppose function $f$ is defined on $(0,1)$ , and differentiable everywhere. If for almost every $x$ , we have $|f'(x)|=1$ , can we conclude that $f$ is a linear function, that is for all $x$ , $f'(x)\equiv 1$ or $f'(x)\equiv -1$ ? I think this is related to the intermediate value property of derivatives. If ""for almost every"" is modified as ""for every"", then it's obviously true. All I can get is that, the points where derivative is continuous forms a dense open set. Then I don't know how to progress.","Suppose function is defined on , and differentiable everywhere. If for almost every , we have , can we conclude that is a linear function, that is for all , or ? I think this is related to the intermediate value property of derivatives. If ""for almost every"" is modified as ""for every"", then it's obviously true. All I can get is that, the points where derivative is continuous forms a dense open set. Then I don't know how to progress.","f (0,1) x |f'(x)|=1 f x f'(x)\equiv 1 f'(x)\equiv -1","['real-analysis', 'derivatives']"
36,Maximise $(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)})^{\cos x}$,Maximise,(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)})^{\cos x},"Maximise $$\large \left(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)}\right)^{\cos x}$$ I wonder why Mathematica says infinity as answer, but on desmos graph it is clear that it is less than 2 $\color{blue}{\text{Even WolframAlpha gave correct answer}}\color{red}{\text{ local maximum value as 1.461}}$ My try: I used the command Maximise with and without constraints. I tried both using constaint as $\ln x >1$ But nothing correct come  up. Next I also tried FindMaximum it didn't show anything either, but WolframAlpha command gave local maximum correctly. I wonder is there any analytical way to find $\color{green}{\text{Local Maximum Value ?}}$","Maximise I wonder why Mathematica says infinity as answer, but on desmos graph it is clear that it is less than 2 My try: I used the command Maximise with and without constraints. I tried both using constaint as But nothing correct come  up. Next I also tried FindMaximum it didn't show anything either, but WolframAlpha command gave local maximum correctly. I wonder is there any analytical way to find",\large \left(\cos x\ln (x^{\frac1x})-\frac{\cos x\log_{10}x}{\ln(\ln x)}\right)^{\cos x} \color{blue}{\text{Even WolframAlpha gave correct answer}}\color{red}{\text{ local maximum value as 1.461}} \ln x >1 \color{green}{\text{Local Maximum Value ?}},"['calculus', 'derivatives', 'optimization', 'maxima-minima', 'wolfram-alpha']"
37,Proof of fundamental theorem of calculus part 1 Rudin Theorem 6.20,Proof of fundamental theorem of calculus part 1 Rudin Theorem 6.20,,"I have a question about the following proof from Rudin's Principles of Mathematical Analysis. 6.20 Theorem Let $f \in \Re$ on $[a,b]$ . For $  a \leq x \leq b$ , put $$F(x)  = \int_a^x f(t)dt$$ Then $F$ is continuous on $[a,b]$ ; furthermore, if $f$ is continuous at a point $x_0$ of $[a,b]$ , then $F$ is differentiable at $x_o$ and $$F'(x_0) = f(x_0)$$ (I have omitted the proof of continuity of $F$ on $[a,b]$ ) Suppose $f$ is continuous at $x_0$ . Given $\epsilon > 0 $ choose $\delta > 0$ such that $$\vert f(t)- f(x_o) \vert < \epsilon $$ if $\vert t- x_0 \vert < \delta$ , and $a \leq t \leq b $ . Hence, if $x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta$ $\enspace$ with: $a\ \leq s < t \leq b$ we have by theorem 6.12(d) $$\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| = \left| \frac{1}{t-s} \int_s^t [f(u) - f(x_0)]du \right| < \epsilon$$ it follows that $F'(x_0) = f(x_0)$ Why does Rudin use $s$ rather than $x_0$ in the epsilon portion of proving the derivative exists? If we are proving the derivative exists at $x_0$ , I would expect that we would prove that $$\left| \frac{F(t) - F(x_0)}{t-x_0} - f(x_0) \right| = \left| \frac{1}{t-x_0} \int_{x_0}^t [f(u) - f(x_0)]du \right| < \epsilon$$","I have a question about the following proof from Rudin's Principles of Mathematical Analysis. 6.20 Theorem Let on . For , put Then is continuous on ; furthermore, if is continuous at a point of , then is differentiable at and (I have omitted the proof of continuity of on ) Suppose is continuous at . Given choose such that if , and . Hence, if with: we have by theorem 6.12(d) it follows that Why does Rudin use rather than in the epsilon portion of proving the derivative exists? If we are proving the derivative exists at , I would expect that we would prove that","f \in \Re [a,b]   a \leq x \leq b F(x)  = \int_a^x f(t)dt F [a,b] f x_0 [a,b] F x_o F'(x_0) = f(x_0) F [a,b] f x_0 \epsilon > 0  \delta > 0 \vert f(t)- f(x_o) \vert < \epsilon  \vert t- x_0 \vert < \delta a \leq t \leq b  x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta \enspace a\ \leq s < t \leq b \left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| = \left| \frac{1}{t-s} \int_s^t [f(u) - f(x_0)]du \right| < \epsilon F'(x_0) = f(x_0) s x_0 x_0 \left| \frac{F(t) - F(x_0)}{t-x_0} - f(x_0) \right| = \left| \frac{1}{t-x_0} \int_{x_0}^t [f(u) - f(x_0)]du \right| < \epsilon","['real-analysis', 'derivatives', 'riemann-integration']"
38,Gradient of log of norm of a matrix,Gradient of log of norm of a matrix,,"I am working on an optimization problem where I need the gradient $\nabla f(X)$ of $f(X) = log(||TX-Y||^2)$ , where X is a vector. My attempt is $\nabla f(X) =  \frac {\nabla  ||TX-Y||^2}{||TX-Y||^2}  =  \frac {2(TX-Y)^TT}{||TX-Y||^2}$ . Am I correct ?","I am working on an optimization problem where I need the gradient of , where X is a vector. My attempt is . Am I correct ?","\nabla f(X) f(X) = log(||TX-Y||^2) \nabla f(X) = 
\frac {\nabla  ||TX-Y||^2}{||TX-Y||^2}  = 
\frac {2(TX-Y)^TT}{||TX-Y||^2}","['derivatives', 'gradient-descent']"
39,Prove that the following limit depending on $\alpha$ is $0$,Prove that the following limit depending on  is,\alpha 0,"Let $f$ be a smooth function with $\lim_{x\to \infty}f(x)=+\infty$ , $\lim_{x\to \infty}f'(x)=+\infty$ for $x\in [x_0,+\infty)$ , $x_0>0$ . I want to show that $$ \lim_{x\to \infty}\frac{x f'(x)}{(x^2+f(x)^2)^\alpha}=0, $$ for $\alpha>c>0$ , which I should determine. I guess that I only need $\alpha>1$ to have the result. I am trying to prove it using Taylor expansion but I am not getting a clear argument. Is there any other way to do it or a clever use of Taylor series?","Let be a smooth function with , for , . I want to show that for , which I should determine. I guess that I only need to have the result. I am trying to prove it using Taylor expansion but I am not getting a clear argument. Is there any other way to do it or a clever use of Taylor series?","f \lim_{x\to \infty}f(x)=+\infty \lim_{x\to \infty}f'(x)=+\infty x\in [x_0,+\infty) x_0>0 
\lim_{x\to \infty}\frac{x f'(x)}{(x^2+f(x)^2)^\alpha}=0,
 \alpha>c>0 \alpha>1","['limits', 'derivatives']"
40,Radon-Nikodym derivative of functions defined on graphs.,Radon-Nikodym derivative of functions defined on graphs.,,"I have been wondering for a bit what is the actual utility of a Radon-Nikodym derivative. One thing I've observed is that it provides a general notion of derivative given a measure space (under further assumptions). As application of this is a classic one is the derivative of standard functions of real variable (so they generalize the riemann integral on the real line) but another one was derivitives w.r.t. counting measures. To some extent, and correct me if I am wrong, they unify for example the notion of derivative of real function of real variables with finite differences for example. I wonder if there's another application in graph theory for example, given a graph $G = (V,E)$ we can define a function $f : V \to \mathbb{C}$ as $f(v_i) = x_i$ I am familiar with the notion of Combinatorial Laplacian however I was wondering if it is possible to associate to a graph $G$ some measure space so that the definition of derivative would follow quite naturally from the use of Radon-Nikodym derivative. I am no expert in graph theory, but this sounds one of those things that someone surely must have tried or thought about, but I don't even know what keywords should I look for.","I have been wondering for a bit what is the actual utility of a Radon-Nikodym derivative. One thing I've observed is that it provides a general notion of derivative given a measure space (under further assumptions). As application of this is a classic one is the derivative of standard functions of real variable (so they generalize the riemann integral on the real line) but another one was derivitives w.r.t. counting measures. To some extent, and correct me if I am wrong, they unify for example the notion of derivative of real function of real variables with finite differences for example. I wonder if there's another application in graph theory for example, given a graph we can define a function as I am familiar with the notion of Combinatorial Laplacian however I was wondering if it is possible to associate to a graph some measure space so that the definition of derivative would follow quite naturally from the use of Radon-Nikodym derivative. I am no expert in graph theory, but this sounds one of those things that someone surely must have tried or thought about, but I don't even know what keywords should I look for.","G = (V,E) f : V \to \mathbb{C} f(v_i) = x_i G","['measure-theory', 'derivatives', 'graph-theory', 'soft-question', 'radon-nikodym']"
41,Relating Norm and Measure for doing calculus.,Relating Norm and Measure for doing calculus.,,"In $\mathbb{R}$ , integration and differentiation are complementary to each other but as we make our definition more abstract the differ, as to define integration we need a measure space and to define derivative we need vector space with norm. These requirements are disjoint and may not always occurs together (Note once we defined each of the entities mentioned above, we can talk of anti integral or anti derivative resp as the inverse process). But I was wondering if given an measure space, can we define derivative by constructing a norm based on the measure specified (for integral) such that the anti-integral and derivative are compatible. Some comment/remark/insight will be helpful","In , integration and differentiation are complementary to each other but as we make our definition more abstract the differ, as to define integration we need a measure space and to define derivative we need vector space with norm. These requirements are disjoint and may not always occurs together (Note once we defined each of the entities mentioned above, we can talk of anti integral or anti derivative resp as the inverse process). But I was wondering if given an measure space, can we define derivative by constructing a norm based on the measure specified (for integral) such that the anti-integral and derivative are compatible. Some comment/remark/insight will be helpful",\mathbb{R},"['measure-theory', 'derivatives', 'lebesgue-integral', 'normed-spaces']"
42,Prove that $h(x)= f(x) - g(x)$ is even degree [closed],Prove that  is even degree [closed],h(x)= f(x) - g(x),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $f$ , $g$ are polynomial functions such that $f(x) = g(x)$ and $f''(x) = g''(x)$ have no real roots i) Prove that $h(x) = f(x) - g(x)$ is a polynomial of even degree ii) $f'(x)= g'(x)$ has exactly one real root I find it difficult to approach the first question. Any help would be highly appreciated. Thank you in advance","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If , are polynomial functions such that and have no real roots i) Prove that is a polynomial of even degree ii) has exactly one real root I find it difficult to approach the first question. Any help would be highly appreciated. Thank you in advance",f g f(x) = g(x) f''(x) = g''(x) h(x) = f(x) - g(x) f'(x)= g'(x),"['calculus', 'derivatives', 'polynomials']"
43,Product rule of differentiation in the multivariate case from first principles,Product rule of differentiation in the multivariate case from first principles,,"$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$ Hi, I would like to ask if this is a valid and rigorous proof for the product rule of differentiation in the multi-variate case. The book I am using for multivariate calculus doesn't provide a proof. Theorem. Let $f,g:X\subseteq \mathbf{R}^n \to \mathbf{R}$ be two scalar-valued functions that are differentiable at $\mathbf{a}\in X$ . Then, the product function $fg$ is also differentiable at $\mathbf{a}$ , and $$ D(fg)(\mathbf{a})=Df(\mathbf{a})g(\mathbf{a})+f(\mathbf{a})Dg(\mathbf{a}) $$ Sketch. Step 1. Let's first dispose off the fact, that the matrix of partial derivatives of the function $fg$ is indeed given by the expression above. I have: \begin{align*} D(fg)(\mathbf{x}) &= \left[ \frac{\partial}{\partial x_1}(fg),\frac{\partial}{\partial x_2}(fg),\ldots,\frac{\partial}{\partial x_n}(fg) \right]\\ &=\left[ \frac{\partial f}{\partial x_1}g + f\frac{\partial g}{\partial x_1},\ldots,\frac{\partial f}{\partial x_n}g + f\frac{\partial g}{\partial x_n}  \right]  [ \text{Product rule for functions of 1 variable}]\\ &=g\left[\frac{\partial f}{\partial x_1} , \ldots,\frac{\partial f}{\partial x_n} \right] + f\left[\frac{\partial g}{\partial x_1} , \ldots,\frac{\partial f}{\partial x_n} \right] \\ &=Df(\mathbf{x})g(\mathbf{x}) + f(\mathbf{x})Dg(\mathbf{x}) \end{align*} Step 2. We are interested to prove that the product function $(fg)(\mathbf{x})=f(\mathbf{x})g(\mathbf{x})$ is indeed differentiable at $\mathbf{x}=\mathbf{a}$ . Let's explore the expression $$ \frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) - D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}} $$ We can write: \begin{align*} &\frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}}\\ =& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\ =& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + \{Df(\mathbf{a})g(\mathbf{a}) + f(\mathbf{a})Dg(\mathbf{a})\}(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\ =& \small{\frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) - Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + f(\mathbf{a})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{a}) - f(\mathbf{a})Dg(\mathbf{a})(\mathbf{x}-\mathbf{a}) - Df(\mathbf{a})g(\mathbf{a})(\mathbf{x}-\mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}}}\\ \le& \small{ \frac{\norm{g(\mathbf{x})} \norm{f(\mathbf{x}) - f(\mathbf{a}) - Df(\mathbf{a})(\mathbf{x} - \mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}} + \frac{\vert f(\mathbf{a})\vert \cdot\norm{g(\mathbf{x}) - g(\mathbf{a}) - Dg(\mathbf{a})(\mathbf{x}-\mathbf{a})} }{\norm{\mathbf{x}- \mathbf{a}}} + \vert Df(\mathbf{a})\vert \norm{g(\mathbf{x}) -g(\mathbf{a}) }} \end{align*} Since, $f$ is differentiable at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_1>0$ , such that the first expression can be made smaller than $\epsilon/3$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_1$ . Since, $g$ is differentiable at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_2>0$ , such that the second term can be made smaller than $\epsilon/(3 \vert f(\mathbf{a})\vert)$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_2$ . Since, $g$ is continuous at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_3>0$ , such that the third term can be made smaller than $\epsilon/(3 \vert Df(\mathbf{a})\vert)$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_3$ . If I pick $\delta = \min\{\delta_1,\delta_2,\delta_3\}$ , I can argue, that the above sum can be made smaller than an arbitrary $\epsilon$ . I am not sure, how I can argue that the coefficient of the first term $\norm{g(\mathbf{x})}$ is in some way bounded. It is continuous in the neighbourhood $\norm{\mathbf{x} - \mathbf{a}} < \delta_1$ . So, perhaps, it's bounded, but I'm still studying analysis. :)","Hi, I would like to ask if this is a valid and rigorous proof for the product rule of differentiation in the multi-variate case. The book I am using for multivariate calculus doesn't provide a proof. Theorem. Let be two scalar-valued functions that are differentiable at . Then, the product function is also differentiable at , and Sketch. Step 1. Let's first dispose off the fact, that the matrix of partial derivatives of the function is indeed given by the expression above. I have: Step 2. We are interested to prove that the product function is indeed differentiable at . Let's explore the expression We can write: Since, is differentiable at , there exists a , such that the first expression can be made smaller than whenever . Since, is differentiable at , there exists a , such that the second term can be made smaller than whenever . Since, is continuous at , there exists a , such that the third term can be made smaller than whenever . If I pick , I can argue, that the above sum can be made smaller than an arbitrary . I am not sure, how I can argue that the coefficient of the first term is in some way bounded. It is continuous in the neighbourhood . So, perhaps, it's bounded, but I'm still studying analysis. :)","\newcommand{\norm}[1]{\left\lVert#1\right\rVert} f,g:X\subseteq \mathbf{R}^n \to \mathbf{R} \mathbf{a}\in X fg \mathbf{a} 
D(fg)(\mathbf{a})=Df(\mathbf{a})g(\mathbf{a})+f(\mathbf{a})Dg(\mathbf{a})
 fg \begin{align*}
D(fg)(\mathbf{x}) &= \left[
\frac{\partial}{\partial x_1}(fg),\frac{\partial}{\partial x_2}(fg),\ldots,\frac{\partial}{\partial x_n}(fg)
\right]\\
&=\left[
\frac{\partial f}{\partial x_1}g + f\frac{\partial g}{\partial x_1},\ldots,\frac{\partial f}{\partial x_n}g + f\frac{\partial g}{\partial x_n} 
\right]  [ \text{Product rule for functions of 1 variable}]\\
&=g\left[\frac{\partial f}{\partial x_1}
, \ldots,\frac{\partial f}{\partial x_n} \right] + f\left[\frac{\partial g}{\partial x_1}
, \ldots,\frac{\partial f}{\partial x_n} \right] \\
&=Df(\mathbf{x})g(\mathbf{x}) + f(\mathbf{x})Dg(\mathbf{x})
\end{align*} (fg)(\mathbf{x})=f(\mathbf{x})g(\mathbf{x}) \mathbf{x}=\mathbf{a} 
\frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) - D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}}
 \begin{align*}
&\frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + \{Df(\mathbf{a})g(\mathbf{a}) + f(\mathbf{a})Dg(\mathbf{a})\}(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \small{\frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) - Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + f(\mathbf{a})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{a}) - f(\mathbf{a})Dg(\mathbf{a})(\mathbf{x}-\mathbf{a}) - Df(\mathbf{a})g(\mathbf{a})(\mathbf{x}-\mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}}}\\
\le& \small{ \frac{\norm{g(\mathbf{x})} \norm{f(\mathbf{x}) - f(\mathbf{a}) - Df(\mathbf{a})(\mathbf{x} - \mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}} + \frac{\vert f(\mathbf{a})\vert \cdot\norm{g(\mathbf{x}) - g(\mathbf{a}) - Dg(\mathbf{a})(\mathbf{x}-\mathbf{a})} }{\norm{\mathbf{x}- \mathbf{a}}} + \vert Df(\mathbf{a})\vert \norm{g(\mathbf{x}) -g(\mathbf{a}) }}
\end{align*} f \mathbf{x}=\mathbf{a} \delta_1>0 \epsilon/3 0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_1 g \mathbf{x}=\mathbf{a} \delta_2>0 \epsilon/(3 \vert f(\mathbf{a})\vert) 0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_2 g \mathbf{x}=\mathbf{a} \delta_3>0 \epsilon/(3 \vert Df(\mathbf{a})\vert) 0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_3 \delta = \min\{\delta_1,\delta_2,\delta_3\} \epsilon \norm{g(\mathbf{x})} \norm{\mathbf{x} - \mathbf{a}} < \delta_1","['limits', 'multivariable-calculus', 'derivatives', 'solution-verification', 'epsilon-delta']"
44,Name for Set Of Functions Smoothly Interpolating Between Points,Name for Set Of Functions Smoothly Interpolating Between Points,,"Suppose I have a continuous differentiable surface such as the following: $$f(x,z)=\sin(e^{2\pi+x^2+z^2}), \space\space\space x,z\in\mathbb{R}$$ As $x$ and $z$ become large, the function oscillates faster and faster. You can see that in the corners of this image: Now suppose I only evaluate $f$ at some grid of points instead of the whole plane. In other words, at: $$x,z\in{\bigcup_{k=0,\space 2n}\{r(k-n)\}}\space\space n\in\mathbb{N}\space\space r\in(0,\infty)$$ for some given $n$ and $r$ . These points are evenly spaced apart on the $xz$ plane, but as $f$ oscillates more and more, the height of the points becomes less predictable (visually). Now suppose I fit a smooth function $g$ to those points. Here's an animation showing how as I increase $n$ , $g$ changes. You can see how sensitive it is near the edges: https://youtu.be/HtH3YCqR-Pw Is there a name for the set of functions that $g$ belongs to (a set of functions that smoothly interpolate through a set of points)? I'm not really sure what I would call it. Would $g$ be ""a smooth continuous interpolation of a uniform discrete sampling of $f$ ?"" I don't think that sounds right since it implies the points $f$ was evaluated at are random. But I'm not really sure what else to call it.","Suppose I have a continuous differentiable surface such as the following: As and become large, the function oscillates faster and faster. You can see that in the corners of this image: Now suppose I only evaluate at some grid of points instead of the whole plane. In other words, at: for some given and . These points are evenly spaced apart on the plane, but as oscillates more and more, the height of the points becomes less predictable (visually). Now suppose I fit a smooth function to those points. Here's an animation showing how as I increase , changes. You can see how sensitive it is near the edges: https://youtu.be/HtH3YCqR-Pw Is there a name for the set of functions that belongs to (a set of functions that smoothly interpolate through a set of points)? I'm not really sure what I would call it. Would be ""a smooth continuous interpolation of a uniform discrete sampling of ?"" I don't think that sounds right since it implies the points was evaluated at are random. But I'm not really sure what else to call it.","f(x,z)=\sin(e^{2\pi+x^2+z^2}), \space\space\space x,z\in\mathbb{R} x z f x,z\in{\bigcup_{k=0,\space 2n}\{r(k-n)\}}\space\space n\in\mathbb{N}\space\space r\in(0,\infty) n r xz f g n g g g f f","['multivariable-calculus', 'derivatives', 'surfaces', 'interpolation', 'smooth-functions']"
45,Meaning of integral identity containing cross product with the area differential,Meaning of integral identity containing cross product with the area differential,,"Edit: I just noticed that the exercise itself was asked here. However I'm not asking for a proof, but about the meaning/intuition behind the notation / exercise itself. On a Multivariable Calculus problem sheet I was asked to prove the following integral identity: Problem . Let $f$ be a smooth scalar field defined on a region $R \subseteq \mathbb{R^3}$ with a smooth boundary $\partial R$ . Show that $$\iint_{\partial R} f \mathbf{r} \wedge \text{d} \mathbf{S} = \iiint_R \mathbf{r} \wedge \nabla f \, \text{d}V$$ I did manage to solve the problem by using the fact that d $\mathbf{S}$ = $\mathbf{n}$ dS, where $\mathbf{n}$ is the outward-pointing normal to $\partial R$ at $\mathbf{r}$ and by applying the divergence theorem on a well-chosen vector field (I won't go into details in case this problem appears on a problem sheet in the future.) I have two questions about this problem: 1) Is there some sort of meaning behind taking a cross product with the area differential, or is it in this case just a shorthand notation for $\mathbf{r} \wedge \mathbf{n}$ dS? I was very confused when I first saw it, and it seems like it's not really necessary, but why would they choose this notation if it doesn't really mean anything special? 2) Is there any kind of (perhaps physical or otherwise) interpretation of this integral identity, or is it just the result of some arbitrary calculation? I can't wrap my head around what it's supposed to mean...","Edit: I just noticed that the exercise itself was asked here. However I'm not asking for a proof, but about the meaning/intuition behind the notation / exercise itself. On a Multivariable Calculus problem sheet I was asked to prove the following integral identity: Problem . Let be a smooth scalar field defined on a region with a smooth boundary . Show that I did manage to solve the problem by using the fact that d = dS, where is the outward-pointing normal to at and by applying the divergence theorem on a well-chosen vector field (I won't go into details in case this problem appears on a problem sheet in the future.) I have two questions about this problem: 1) Is there some sort of meaning behind taking a cross product with the area differential, or is it in this case just a shorthand notation for dS? I was very confused when I first saw it, and it seems like it's not really necessary, but why would they choose this notation if it doesn't really mean anything special? 2) Is there any kind of (perhaps physical or otherwise) interpretation of this integral identity, or is it just the result of some arbitrary calculation? I can't wrap my head around what it's supposed to mean...","f R \subseteq \mathbb{R^3} \partial R \iint_{\partial R} f \mathbf{r} \wedge \text{d} \mathbf{S} = \iiint_R \mathbf{r} \wedge \nabla f \, \text{d}V \mathbf{S} \mathbf{n} \mathbf{n} \partial R \mathbf{r} \mathbf{r} \wedge \mathbf{n}","['multivariable-calculus', 'derivatives', 'divergence-theorem']"
46,"If we know how one variable depends on another, can we ""plug"" this into the total derivative?","If we know how one variable depends on another, can we ""plug"" this into the total derivative?",,"Suppose we have a function $f(x,y)$ . The total derivative is $f_1(x,y)dx+f_2(x,y)dy$ . Now suppose that we know the explicit relationship between x and y. For example, suppose we know $y=h(x)$ . Then can we just write the total derivative as $f_1(x,y)dx+f_2(x,y)*h'(x)dx$ ? (which is just the ""partial"" derivative of $f(x,h(x))$ w.r.t $x$ . (I put partial in quotes because i guess in this second formulation $f$ is really just a function of one variable). If the above is true, then what do we use the total derivative for? Because for any application wouldn't we have an explicit dependency between $x$ and $y$ ? (and hence just be able to simplify to the latter case)","Suppose we have a function . The total derivative is . Now suppose that we know the explicit relationship between x and y. For example, suppose we know . Then can we just write the total derivative as ? (which is just the ""partial"" derivative of w.r.t . (I put partial in quotes because i guess in this second formulation is really just a function of one variable). If the above is true, then what do we use the total derivative for? Because for any application wouldn't we have an explicit dependency between and ? (and hence just be able to simplify to the latter case)","f(x,y) f_1(x,y)dx+f_2(x,y)dy y=h(x) f_1(x,y)dx+f_2(x,y)*h'(x)dx f(x,h(x)) x f x y","['multivariable-calculus', 'derivatives', 'partial-derivative']"
47,Proof Detail: Cauchy-Riemann Equations Imply Holomorphy,Proof Detail: Cauchy-Riemann Equations Imply Holomorphy,,"I'm working through the proof of Theorem 2.4 in Chapter 1 of Stein/Shakarchi's Complex Analysis . I'm looking for clarification on where exactly we use the hypothesis that $u$ and $v$ are continuously differentiable. My guess is that it's used right away when writing $$u(x + h_1, y + h_2) - u(x,y) = \frac{\partial u}{\partial x}h_1 + \frac{\partial u}{\partial y}h_2 + |h|\psi(h)$$ and similarly for $v$ , since that equation comes from $u$ being differentiable as a map $\mathbb{R}^2 \to \mathbb{R}$ , which we can only say provided we know that the partials of $u$ are continuously differentiable. Is this correct?","I'm working through the proof of Theorem 2.4 in Chapter 1 of Stein/Shakarchi's Complex Analysis . I'm looking for clarification on where exactly we use the hypothesis that and are continuously differentiable. My guess is that it's used right away when writing and similarly for , since that equation comes from being differentiable as a map , which we can only say provided we know that the partials of are continuously differentiable. Is this correct?","u v u(x + h_1, y + h_2) - u(x,y) = \frac{\partial u}{\partial x}h_1 + \frac{\partial u}{\partial y}h_2 + |h|\psi(h) v u \mathbb{R}^2 \to \mathbb{R} u","['complex-analysis', 'derivatives', 'cauchy-riemann-equations']"
48,Which of the following statements are true for the elements in $\mathscr S\ $?,Which of the following statements are true for the elements in ?,\mathscr S\ ,"Let $\mathscr S$ be the family of continuous real valued functions on $(0, \infty)$ defined by $$\mathscr S : = \left \{f : (0, \infty) \longrightarrow \Bbb R\ |\ f(x) = f(2x),\ \forall x \in (0, \infty) \right \}.$$ For each of the following statements, state whether it is true or false. $(a)$ Any element $f \in \mathscr S$ is bounded. $(b)$ Any element $f \in \mathscr S$ is uniformly continuous. $(c)$ Any element $f \in \mathscr S$ is differentiable. $(d)$ Any uniformly bounded sequence in $\mathscr S$ has a uniformly convergent subsequence. I have proved that $(a)$ is true because for any $x \in (0,\infty)$ there exists $n \in \Bbb Z$ such that $2^n x \in [1,2].$ Since $[1,2]$ is compact it follows that for any $f \in \mathscr S$ there exists $M_f \gt 0$ such that for all $y \in [1,2]$ we have $|f(y)| \leq M_f.$ But then by the given hypothesis $|f (x)| = |f(2^n x)| \leq M_f,$ as required. But I have no idea about $(b), (c)$ and $(d).$ I think $(b)$ and $(c)$ are false. But I am unable to construct an element $f \in \mathscr S$ which will work as a counter-example. Would anybody please help me in this regard? Thanks for your time.","Let be the family of continuous real valued functions on defined by For each of the following statements, state whether it is true or false. Any element is bounded. Any element is uniformly continuous. Any element is differentiable. Any uniformly bounded sequence in has a uniformly convergent subsequence. I have proved that is true because for any there exists such that Since is compact it follows that for any there exists such that for all we have But then by the given hypothesis as required. But I have no idea about and I think and are false. But I am unable to construct an element which will work as a counter-example. Would anybody please help me in this regard? Thanks for your time.","\mathscr S (0, \infty) \mathscr S : = \left \{f : (0, \infty) \longrightarrow \Bbb R\ |\ f(x) = f(2x),\ \forall x \in (0, \infty) \right \}. (a) f \in \mathscr S (b) f \in \mathscr S (c) f \in \mathscr S (d) \mathscr S (a) x \in (0,\infty) n \in \Bbb Z 2^n x \in [1,2]. [1,2] f \in \mathscr S M_f \gt 0 y \in [1,2] |f(y)| \leq M_f. |f (x)| = |f(2^n x)| \leq M_f, (b), (c) (d). (b) (c) f \in \mathscr S","['real-analysis', 'derivatives', 'continuity', 'examples-counterexamples', 'uniform-convergence']"
49,Finding the range of $S$,Finding the range of,S,"Let $f(x)$ and $g(x)$ be a function whose derivatives is on $R$ \ $\{b\}$ and $R$ , respectively. The graph of $f'(x)$ and g'(x) is shown below (on the image below). Let: $$h(x) = f(x) - g(x)$$ and $$S = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2$$ for $a,b,c$ are positive real numbers. Which of the following is true for $x \neq 0$ ? $A. S \in [h(c), h(a+c)] \\ B. S \leq h(c) \\ C. S \in S \in [h(c), h(a+b)] \\ D. S \in [h(a), h(c)]$ The graph: My solution : $$\begin{align} S & = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2 \\ & \\ & = -[h(b+x^2)]^2 + 2h(b+x^2)h(c) + h(b+x^2) - [h(c)]^2 \\ & \\ & = - ([h(b+x^2)]^2 - 2h(b+x^2)h(c) + [h(c)]^2) + h(b+x^2) \\ & \\ & = - ([h(b+x^2)] - [h(c)])^2 + h(b+x^2) \\ \end{align}$$ We have: $h'(x) = f'(x) - g'(x)$ When $x \in [b, c)$ : $f'(x) > g'(x)$ so $h(x)$ is increasing. As $x \to b^{+}$ , $f'(x) \to +\infty$ and $g'(x) \to g'(b)$ , therefore $h'(x) \to + \infty$ . This implies $h(x)$ start  from $-\infty$ and going up to $h(c)$ When $x = c$ : $h'(c) = f'(c) - g'(c) = 0$ . $h(x)$ stop increasing. When $x > c$ : $f'(x) < g'(x)$ so $h(x)$ is decreasing. From those indications, we know that $h(x)$ will only have a local maximum at $x = c$ for $x \in [b, +\infty] \,\, (1)$ Now, since $b + x^2 > b$ for $x \neq 0$ and based on $(1)$ , we get: $$h(b + x^2) \leq h(c)$$ Therefore: $$S \leq 0 + h(c) = h(c)$$ So $S$ has a maximum of $h(c)$ For the minimum, since $h(b + x^2) \leq h(c)$ and $h(b + x^2) \to -\infty$ as $x \to b^{+}$ or $x \to +\infty$ , $S$ can approach $-\infty$ and so there is no minimum for $S$ In conclusion, the answer is $\color{red}{B. S \leq h(c)}$ Is my solution correct? If yes, does that mean $a$ has no use in the process of solving? (This is also why I'm uncertainly about my answer) If not, what did I do wrong? If I wrong at the minimum of $S$ , then what is $S_{min}$ ?","Let and be a function whose derivatives is on \ and , respectively. The graph of and g'(x) is shown below (on the image below). Let: and for are positive real numbers. Which of the following is true for ? The graph: My solution : We have: When : so is increasing. As , and , therefore . This implies start  from and going up to When : . stop increasing. When : so is decreasing. From those indications, we know that will only have a local maximum at for Now, since for and based on , we get: Therefore: So has a maximum of For the minimum, since and as or , can approach and so there is no minimum for In conclusion, the answer is Is my solution correct? If yes, does that mean has no use in the process of solving? (This is also why I'm uncertainly about my answer) If not, what did I do wrong? If I wrong at the minimum of , then what is ?","f(x) g(x) R \{b\} R f'(x) h(x) = f(x) - g(x) S = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2 a,b,c x \neq 0 A. S \in [h(c), h(a+c)] \\ B. S \leq h(c) \\ C. S \in S \in [h(c), h(a+b)] \\ D. S \in [h(a), h(c)] \begin{align}
S & = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2 \\
& \\
& = -[h(b+x^2)]^2 + 2h(b+x^2)h(c) + h(b+x^2) - [h(c)]^2 \\
& \\
& = - ([h(b+x^2)]^2 - 2h(b+x^2)h(c) + [h(c)]^2) + h(b+x^2) \\
& \\
& = - ([h(b+x^2)] - [h(c)])^2 + h(b+x^2) \\
\end{align} h'(x) = f'(x) - g'(x) x \in [b, c) f'(x) > g'(x) h(x) x \to b^{+} f'(x) \to +\infty g'(x) \to g'(b) h'(x) \to + \infty h(x) -\infty h(c) x = c h'(c) = f'(c) - g'(c) = 0 h(x) x > c f'(x) < g'(x) h(x) h(x) x = c x \in [b, +\infty] \,\, (1) b + x^2 > b x \neq 0 (1) h(b + x^2) \leq h(c) S \leq 0 + h(c) = h(c) S h(c) h(b + x^2) \leq h(c) h(b + x^2) \to -\infty x \to b^{+} x \to +\infty S -\infty S \color{red}{B. S \leq h(c)} a S S_{min}","['calculus', 'derivatives', 'optimization', 'solution-verification']"
50,"Manipulation of an expression with derivatives: which sign is more suitable, ""d"" or ""∂""?","Manipulation of an expression with derivatives: which sign is more suitable, ""d"" or ""∂""?",,"I am writing a scientific paper about Mechanics of Beams. At a given point, I discuss the quadratic strain in function of $u(x,t)$ and $w(x,t)$ , which refer to the beam's motion in function of position $x$ and time $t$ . The quadratic strain reads: \begin{equation}     \varepsilon_q = \dfrac{\left(du+dx\right)^2+dw^2-dx^2}{dx^2} \cong \dfrac{\partial{u}}{\partial{x}}+\dfrac{1}{2}\left(\dfrac{\partial{w}}{\partial{x}}\right)^2. \end{equation} As you can see, on the right side I use "" $\partial$ "", which is suitable to the fact that $u(x,t)$ and $w(x,t)$ depend on both $x$ and $t$ . However, on the left side I use "" $d$ "", which is suitable to the fact that $du = u(x+dx)-u(x)$ and $dw = w(x+dx)-w(x)$ . I wonder if it would not be more accurate to write: \begin{equation}     \varepsilon_q = \dfrac{\left({\partial}u+{\partial}x\right)^2+{\partial}w^2-{\partial}x^2}{{\partial}x^2} \cong \dfrac{\partial{u}}{\partial{x}}+\dfrac{1}{2}\left(\dfrac{\partial{w}}{\partial{x}}\right)^2, \end{equation} even though I have neever seen $\partial{x}$ , $\partial{u}$ or $\partial{w}$ written aside - let us say out of a ""derivative fraction"" - as we commonly see in math or engineering texts for $dx$ , $du$ and $dw$ . For example, I have often seen the expression $du = u(x+dx)-u(x)$ ; but never $\partial{u} = u(x+\partial{x})-u(x)$ . That being said, which expression do you think I should choose?","I am writing a scientific paper about Mechanics of Beams. At a given point, I discuss the quadratic strain in function of and , which refer to the beam's motion in function of position and time . The quadratic strain reads: As you can see, on the right side I use "" "", which is suitable to the fact that and depend on both and . However, on the left side I use "" "", which is suitable to the fact that and . I wonder if it would not be more accurate to write: even though I have neever seen , or written aside - let us say out of a ""derivative fraction"" - as we commonly see in math or engineering texts for , and . For example, I have often seen the expression ; but never . That being said, which expression do you think I should choose?","u(x,t) w(x,t) x t \begin{equation}
    \varepsilon_q = \dfrac{\left(du+dx\right)^2+dw^2-dx^2}{dx^2} \cong \dfrac{\partial{u}}{\partial{x}}+\dfrac{1}{2}\left(\dfrac{\partial{w}}{\partial{x}}\right)^2.
\end{equation} \partial u(x,t) w(x,t) x t d du = u(x+dx)-u(x) dw = w(x+dx)-w(x) \begin{equation}
    \varepsilon_q = \dfrac{\left({\partial}u+{\partial}x\right)^2+{\partial}w^2-{\partial}x^2}{{\partial}x^2} \cong \dfrac{\partial{u}}{\partial{x}}+\dfrac{1}{2}\left(\dfrac{\partial{w}}{\partial{x}}\right)^2,
\end{equation} \partial{x} \partial{u} \partial{w} dx du dw du = u(x+dx)-u(x) \partial{u} = u(x+\partial{x})-u(x)","['derivatives', 'notation', 'partial-derivative']"
51,Is there a function whose derivative is discontinuous everywhere?,Is there a function whose derivative is discontinuous everywhere?,,"Is there a function whose derivative is discontinuous everywhere? As is well-known by Darboux, a derivative $f'$ could have discontinuity of second type. My question is: whether there exists a function $f$ , such that $f'$ is discontinuous everywhere.","Is there a function whose derivative is discontinuous everywhere? As is well-known by Darboux, a derivative could have discontinuity of second type. My question is: whether there exists a function , such that is discontinuous everywhere.",f' f f',"['calculus', 'derivatives']"
52,Specific Example for the Existence of Partial Derivatives,Specific Example for the Existence of Partial Derivatives,,"Suppose $\sigma$ has bounded first and second derivatives and refer to the following picture.  Why do we know that $u$ even has a partial derivative with respect to $y$ ?  As far as I can understand, (2.37) is justified by fixing y and applying Picard Lindelof, so I'm confused (this is on p. 296 of Karatzas and Shreve, for reference)","Suppose has bounded first and second derivatives and refer to the following picture.  Why do we know that even has a partial derivative with respect to ?  As far as I can understand, (2.37) is justified by fixing y and applying Picard Lindelof, so I'm confused (this is on p. 296 of Karatzas and Shreve, for reference)",\sigma u y,"['calculus', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
53,Gradient of a quadratic form with respect to a complex vector,Gradient of a quadratic form with respect to a complex vector,,"How would I go about calculating the derivative with respect to $x$ of $$Q(x) = x^H A x $$ with $A$ a real matrix (not necessarily symmetric) and $x$ a complex valued vector? Here $(\cdot)^H$ denotes the conjugate transpose. EDIT: This question came up in the context of the maximization problem: $$ \text{arg}\max_x \, x^H A x \qquad \text{s.t.} \quad x^Hx = 1$$ I think for solving this I need to solve $ \nabla_x \, Q (x) \stackrel{!}{=} 0$ , so what I am looking for is the complex gradient vector. Am I correct in assuming (following the Matrix Cookbook and using that $A$ is real) that: $$  \nabla_x \, Q(x) = \frac{\partial Q}{\partial Re(x)} + i \frac{\partial Q}{\partial Im(x)} = \, \dots \, =  2 A x$$ where I used that $Q(x) = (Re(x) - i\, Im(x))^T A \,(Re(x) + i\,Im(x)) $ ? EDIT2: The above seems to only hold if $x^H A \, x$ is real (additional question: for which matrices $A$ would that be the case? Maybe when I can decompose $A$ into $A = B^TB$ or respectively $B^HB$ for complex A?).","How would I go about calculating the derivative with respect to of with a real matrix (not necessarily symmetric) and a complex valued vector? Here denotes the conjugate transpose. EDIT: This question came up in the context of the maximization problem: I think for solving this I need to solve , so what I am looking for is the complex gradient vector. Am I correct in assuming (following the Matrix Cookbook and using that is real) that: where I used that ? EDIT2: The above seems to only hold if is real (additional question: for which matrices would that be the case? Maybe when I can decompose into or respectively for complex A?).","x Q(x) = x^H A x  A x (\cdot)^H  \text{arg}\max_x \, x^H A x \qquad \text{s.t.} \quad x^Hx = 1  \nabla_x \, Q (x) \stackrel{!}{=} 0 A   \nabla_x \, Q(x) = \frac{\partial Q}{\partial Re(x)} + i \frac{\partial Q}{\partial Im(x)} = \, \dots \, =  2 A x Q(x) = (Re(x) - i\, Im(x))^T A \,(Re(x) + i\,Im(x))  x^H A \, x A A A = B^TB B^HB","['complex-analysis', 'multivariable-calculus', 'derivatives', 'quadratic-forms', 'scalar-fields']"
54,N'th derivative of $f(x)$ to the power of $g(x)$,N'th derivative of  to the power of,f(x) g(x),"I am trying to find a formula for the $n$ 'th derivative of $f(x)^{g(x)}$ .  I have tried using the formula bellow along with Leibniz rule without success. $$D^n(f(g(x)))=\sum_{k=0}^{n-1} \binom {n-1}{k}D^{\left(n-k\right)}g\left(x\right)D^{\left(k\right)}\left(f'\left(g\left(x\right)\right)\right)$$ This is how I went about it: Let $y=f(x)^{g(x)}$ This implies $\frac{y^\prime}{y}=g^\prime(x)\ln(f(x))+g(x)f^\prime(x)f(x)^{-1}$ Now, using Leibniz rule on the LHS as well as on the RHS (multiple times on the RHS since there is more than two functions multiplied) followed by the equation above for the $\ln(f(x))$ I got a long formula wich can be rearranged to obtain the value of $D^ny$ , however it could be easily seen it didn't work for a lot of functions in the form $f(x)^{g(x)}$ . Is there a well known formula for what I am trying to find? Or does the method I am trying to use fail at some stage?","I am trying to find a formula for the 'th derivative of .  I have tried using the formula bellow along with Leibniz rule without success. This is how I went about it: Let This implies Now, using Leibniz rule on the LHS as well as on the RHS (multiple times on the RHS since there is more than two functions multiplied) followed by the equation above for the I got a long formula wich can be rearranged to obtain the value of , however it could be easily seen it didn't work for a lot of functions in the form . Is there a well known formula for what I am trying to find? Or does the method I am trying to use fail at some stage?",n f(x)^{g(x)} D^n(f(g(x)))=\sum_{k=0}^{n-1} \binom {n-1}{k}D^{\left(n-k\right)}g\left(x\right)D^{\left(k\right)}\left(f'\left(g\left(x\right)\right)\right) y=f(x)^{g(x)} \frac{y^\prime}{y}=g^\prime(x)\ln(f(x))+g(x)f^\prime(x)f(x)^{-1} \ln(f(x)) D^ny f(x)^{g(x)},"['calculus', 'functions', 'derivatives', 'summation']"
55,Creating a function based on specific values and derivatives. [closed],Creating a function based on specific values and derivatives. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The specific question asks me to sketch a possible graph with the following 4 conditions: F'(x)>0 on -2<x<1 F'(x)<0 on x<-2 and x>1 F(-2)=-3 F(1)=4 Now I know how to get the first two on their own, but the problem comes in trying to find an equation that will allow both the first and second pair of conditions to be realized. Is there a way to derive an equation from these conditions without creating an antiderivative, and then modifying it and guessing whether or not it will work?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The specific question asks me to sketch a possible graph with the following 4 conditions: F'(x)>0 on -2<x<1 F'(x)<0 on x<-2 and x>1 F(-2)=-3 F(1)=4 Now I know how to get the first two on their own, but the problem comes in trying to find an equation that will allow both the first and second pair of conditions to be realized. Is there a way to derive an equation from these conditions without creating an antiderivative, and then modifying it and guessing whether or not it will work?",,"['calculus', 'derivatives']"
56,Using a Partial Differential Equation to find the dynamics model of physical intearctions,Using a Partial Differential Equation to find the dynamics model of physical intearctions,,"I don't know a lot about Partial Differential Equations (PDEs) but I think they are used to find solutions to complex interaction problems. So my first question does solving a PDE enable us to obtain the forward dynamics model of physics that describes such complex interactions, at least in theory? If so, can one then use the obtained forward dynamics model and change its parameters to generate a new fluid simulation scenes? The reason that I'm asking this is because I have some videos of various liquids interacting with one another and my goal is to find a solution that governs such interactions. The way I'm currently thinking about it is that I first need to somehow obtain a PDE from this video. Then I plug this PDE into a numerical PDE solver. Assuming that the PDE solver is general and accurate enough it should give me the correct governing equation (i.e. forward dynamics model). However, I'm not sure if my intuition on how I can find that governing equation/method is correct in the first place. In other words, I'm not sure if the steps I explained ( video -> PDE -> PDE Solver -> forward dynamics model ) are correct. If obtaining a PDE is the first step, how can I obtain a PDE from fluid simulation videos? In other words, what are the general steps that one should take in order to get the corresponding PDE? Also, given that I am working with some videos, how would the corresponding PDE look like? For instance, would the PDE be a huge matrix/tensor with dimensions corresponding to changes of each element in my scene w.r.t each other over time t ? For the example above, you may also assume that I have access to the underlying causal mechanism that generated that video as well. In other words, I can tell the physics simulator how much of liquid 1, 2, ..., N should be poured into the tank at time t , what their viscosity is and so on.","I don't know a lot about Partial Differential Equations (PDEs) but I think they are used to find solutions to complex interaction problems. So my first question does solving a PDE enable us to obtain the forward dynamics model of physics that describes such complex interactions, at least in theory? If so, can one then use the obtained forward dynamics model and change its parameters to generate a new fluid simulation scenes? The reason that I'm asking this is because I have some videos of various liquids interacting with one another and my goal is to find a solution that governs such interactions. The way I'm currently thinking about it is that I first need to somehow obtain a PDE from this video. Then I plug this PDE into a numerical PDE solver. Assuming that the PDE solver is general and accurate enough it should give me the correct governing equation (i.e. forward dynamics model). However, I'm not sure if my intuition on how I can find that governing equation/method is correct in the first place. In other words, I'm not sure if the steps I explained ( video -> PDE -> PDE Solver -> forward dynamics model ) are correct. If obtaining a PDE is the first step, how can I obtain a PDE from fluid simulation videos? In other words, what are the general steps that one should take in order to get the corresponding PDE? Also, given that I am working with some videos, how would the corresponding PDE look like? For instance, would the PDE be a huge matrix/tensor with dimensions corresponding to changes of each element in my scene w.r.t each other over time t ? For the example above, you may also assume that I have access to the underlying causal mechanism that generated that video as well. In other words, I can tell the physics simulator how much of liquid 1, 2, ..., N should be poured into the tank at time t , what their viscosity is and so on.",,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'numerical-methods', 'physics']"
57,Using the chain rule correctly with matrices as the partial derivative,Using the chain rule correctly with matrices as the partial derivative,,"I hope this question fits the math.stackexchange environment. I try to understand how a Neural Network calculates the derivatives along the Backpropagation with the chain rule correctly, but I struggle a bit with some explanations with tensors. What I have calculated For my example I used the last hidden layer of a small network with only three neurons on the hidden layer and two outputs, which results in: $$ W = \begin{bmatrix}w_{1,1} & w_{1,2} & w_{1,3} \\w_{2,1} & w_{2,2} & w_{2,3} \end{bmatrix}, \vec{b} = \begin{bmatrix}b_1 \\ b_2 \end{bmatrix}, \vec{z} = \begin{bmatrix}z_1 \\ z_2 \end{bmatrix} $$ and $$ \vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3 \end{bmatrix} $$ with $$ z_1 = a_1\cdot w_{1,1} + a_2\cdot w_{1,2} + a_3\cdot w_{1,3} + b_1 $$ and $$ z_2 = a_1\cdot w_{2,1} + a_2\cdot w_{2,2} + a_3\cdot w_{2,3} + b_1 $$ or in vectorform: $$ \vec{z} = W \cdot \vec{a} + \vec{b} $$ The loss function cross-entropy is used which results in the derivative: $$ \nabla_\vec{z} Loss = \vec{q} - \vec{1}_y $$ with $\vec{q}$ being the vector presentation of the softmax-function and $\vec{1}_y$ being a zero vector with a $1$ at position $y$ . So on this Basis I started to calculate the partial derivatives, beginning with the bias-vector: $$ \frac{\partial \vec{z}}{\partial \vec{b}} = \begin{bmatrix} \frac{\partial z_1}{\partial b_1}=1 && \frac{\partial z_1}{\partial b_2}=0 \\  \frac{\partial z_2}{\partial b_1}=0 && \frac{\partial z_2}{\partial b_2}=1  \end{bmatrix} $$ With this Jacobi-Matrix I am able to use the chainrule $\biggl(\frac{\partial \vec{z}}{\partial \vec{b}}\biggr)^\top \nabla_\vec{z} Loss$ to calculate the upgrade vector for $\vec{b}$ If I do now the similar with the Matrix $W$ I result in a tensor which I represent in two seperate matrices: $$ \frac{\partial z_1}{\partial W} = \begin{bmatrix} \frac{\partial z_1}{\partial w_{1,1}}=a_1 && \frac{\partial z_1}{\partial w_{1,2}}=a_2 && \frac{\partial z_1}{\partial w_{1,3}}=a_3 \\  \frac{\partial z_1}{\partial w_{2,1}}=0 && \frac{\partial z_1}{\partial w_{2,2}}=0 && \frac{\partial z_1}{\partial w_{2,3}}=0 \end{bmatrix} $$ and $$ \frac{\partial z_2}{\partial W} = \begin{bmatrix} \frac{\partial z_2}{\partial w_{1,1}}=0 && \frac{\partial z_2}{\partial w_{1,2}}=0 && \frac{\partial z_2}{\partial w_{1,3}}=0 \\  \frac{\partial z_2}{\partial w_{2,1}}=a_1 && \frac{\partial z_2}{\partial w_{2,2}}=a_2 && \frac{\partial z_2}{\partial w_{2,3}}=a_3 \end{bmatrix} $$ Where I now struggle I now from different source (Andrew Ng, Goodfellow) the result of the following calculation, which should result in the upgrade matrix: $$ \begin{bmatrix} a_1 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) \\ a_1 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) \end{bmatrix} $$ if $y=1$ (zero-based indexing) My first problem I have is to calculate the Transpose of a tensor, since I am not able to figure how to define which axis is responsible for the transpose. My second problem is the calculation of $\biggl(\frac{\partial \vec{z}}{\partial W}\biggr)^\top \nabla_\vec{z} Loss$ . I am able to visualize at least in my mind which calculations are necessary to get the result shown above, but I think the dot-product is not the right product to get this result. Since my background is neither maths or physics, I had problems to figure out how to calculate with tensor correctly but I wanted to show a more accurate representation, than the shortcuts which I see plenty on ""TowardsDataSience"" and Co.","I hope this question fits the math.stackexchange environment. I try to understand how a Neural Network calculates the derivatives along the Backpropagation with the chain rule correctly, but I struggle a bit with some explanations with tensors. What I have calculated For my example I used the last hidden layer of a small network with only three neurons on the hidden layer and two outputs, which results in: and with and or in vectorform: The loss function cross-entropy is used which results in the derivative: with being the vector presentation of the softmax-function and being a zero vector with a at position . So on this Basis I started to calculate the partial derivatives, beginning with the bias-vector: With this Jacobi-Matrix I am able to use the chainrule to calculate the upgrade vector for If I do now the similar with the Matrix I result in a tensor which I represent in two seperate matrices: and Where I now struggle I now from different source (Andrew Ng, Goodfellow) the result of the following calculation, which should result in the upgrade matrix: if (zero-based indexing) My first problem I have is to calculate the Transpose of a tensor, since I am not able to figure how to define which axis is responsible for the transpose. My second problem is the calculation of . I am able to visualize at least in my mind which calculations are necessary to get the result shown above, but I think the dot-product is not the right product to get this result. Since my background is neither maths or physics, I had problems to figure out how to calculate with tensor correctly but I wanted to show a more accurate representation, than the shortcuts which I see plenty on ""TowardsDataSience"" and Co.","
W = \begin{bmatrix}w_{1,1} & w_{1,2} & w_{1,3} \\w_{2,1} & w_{2,2} & w_{2,3} \end{bmatrix}, \vec{b} = \begin{bmatrix}b_1 \\ b_2 \end{bmatrix}, \vec{z} = \begin{bmatrix}z_1 \\ z_2 \end{bmatrix}
 
\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3 \end{bmatrix}
 
z_1 = a_1\cdot w_{1,1} + a_2\cdot w_{1,2} + a_3\cdot w_{1,3} + b_1
 
z_2 = a_1\cdot w_{2,1} + a_2\cdot w_{2,2} + a_3\cdot w_{2,3} + b_1
 
\vec{z} = W \cdot \vec{a} + \vec{b}
 
\nabla_\vec{z} Loss = \vec{q} - \vec{1}_y
 \vec{q} \vec{1}_y 1 y 
\frac{\partial \vec{z}}{\partial \vec{b}} = \begin{bmatrix} \frac{\partial z_1}{\partial b_1}=1 && \frac{\partial z_1}{\partial b_2}=0 \\  \frac{\partial z_2}{\partial b_1}=0 && \frac{\partial z_2}{\partial b_2}=1  \end{bmatrix}
 \biggl(\frac{\partial \vec{z}}{\partial \vec{b}}\biggr)^\top \nabla_\vec{z} Loss \vec{b} W 
\frac{\partial z_1}{\partial W} = \begin{bmatrix} \frac{\partial z_1}{\partial w_{1,1}}=a_1 && \frac{\partial z_1}{\partial w_{1,2}}=a_2 && \frac{\partial z_1}{\partial w_{1,3}}=a_3 \\  \frac{\partial z_1}{\partial w_{2,1}}=0 && \frac{\partial z_1}{\partial w_{2,2}}=0 && \frac{\partial z_1}{\partial w_{2,3}}=0 \end{bmatrix}
 
\frac{\partial z_2}{\partial W} = \begin{bmatrix} \frac{\partial z_2}{\partial w_{1,1}}=0 && \frac{\partial z_2}{\partial w_{1,2}}=0 && \frac{\partial z_2}{\partial w_{1,3}}=0 \\  \frac{\partial z_2}{\partial w_{2,1}}=a_1 && \frac{\partial z_2}{\partial w_{2,2}}=a_2 && \frac{\partial z_2}{\partial w_{2,3}}=a_3 \end{bmatrix}
 
\begin{bmatrix} a_1 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) \\ a_1 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) \end{bmatrix}
 y=1 \biggl(\frac{\partial \vec{z}}{\partial W}\biggr)^\top \nabla_\vec{z} Loss","['derivatives', 'tensors']"
58,Prove that $\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right]$,Prove that,\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right],"I'm studying about Kalman filter from the book of ""optimal state estimation"" and I have one exercise where I need to prove the following equation: $$\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right].$$ I'm not quite sure how to approach this problem, because it is not clear to me are we taking the expectations with respect to what random variables? Is it the random variable $x$ , or is the $t$ here also a random variable? First, I assumed the expectations are taken w.r.t to $x$ , so I started to expand the left side of this equation first: $$\frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(x)\,dx\right)=\int \frac{d}{dt}\left(x\,f(x)\right)\,dx=\int \left[\frac{dx}{dt}f(x)+x\frac{df}{dx}\frac{dx}{dt}\right]\,dx$$ and then I looked at the right side of the equation: $$E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(x)\,dx$$ so now I'm stuck a little bit. If I take the expectations w.r.t $t$ I get: $$\frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(t)\,dt\right)=\int \frac{d}{dt}\left(x\,f(t)\right)\,dt=\int \left[\frac{dx}{dt}f(t)+x\frac{df}{dt}\right]\,dt$$ $$E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(t)\,dt.$$ or should I take the expectations w.r.t to different random variables? Another point of confusion: are the differential coefficients constant in this problem? Or should I treat them as functions of $t$ (that is if expectations are taken w.r.t. $x$ )?","I'm studying about Kalman filter from the book of ""optimal state estimation"" and I have one exercise where I need to prove the following equation: I'm not quite sure how to approach this problem, because it is not clear to me are we taking the expectations with respect to what random variables? Is it the random variable , or is the here also a random variable? First, I assumed the expectations are taken w.r.t to , so I started to expand the left side of this equation first: and then I looked at the right side of the equation: so now I'm stuck a little bit. If I take the expectations w.r.t I get: or should I take the expectations w.r.t to different random variables? Another point of confusion: are the differential coefficients constant in this problem? Or should I treat them as functions of (that is if expectations are taken w.r.t. )?","\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right]. x t x \frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(x)\,dx\right)=\int \frac{d}{dt}\left(x\,f(x)\right)\,dx=\int \left[\frac{dx}{dt}f(x)+x\frac{df}{dx}\frac{dx}{dt}\right]\,dx E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(x)\,dx t \frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(t)\,dt\right)=\int \frac{d}{dt}\left(x\,f(t)\right)\,dt=\int \left[\frac{dx}{dt}f(t)+x\frac{df}{dt}\right]\,dt E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(t)\,dt. t x","['integration', 'derivatives', 'expected-value']"
59,What is intuition behind total derivative of a function,What is intuition behind total derivative of a function,,"I know that a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is called totally differentiable at a point $a\in \mathbb{R^n}$ if $\exists$ a linear transformation $T_a$ such that \begin{equation} \|{\lim_{h\rightarrow 0} \frac{f(a+hv)-f(a) - T_a(h,v)}{h}}\| = 0 \end{equation} I also know that this expression is derived from Taylor's theorem. My question is that if this definition is supposed to be analogous to differentiablity of a function in a single variable setup. Then how does this expression justifies it. In short, my question is how does this function justify differentiablity as we know. Also, here we have used linear transformation $T_a$ as derivative of that function. How do we know that the derivative of this function will be a linear transformation?","I know that a function is called totally differentiable at a point if a linear transformation such that I also know that this expression is derived from Taylor's theorem. My question is that if this definition is supposed to be analogous to differentiablity of a function in a single variable setup. Then how does this expression justifies it. In short, my question is how does this function justify differentiablity as we know. Also, here we have used linear transformation as derivative of that function. How do we know that the derivative of this function will be a linear transformation?","f: \mathbb{R}^n \rightarrow \mathbb{R}^m a\in \mathbb{R^n} \exists T_a \begin{equation}
\|{\lim_{h\rightarrow 0} \frac{f(a+hv)-f(a) - T_a(h,v)}{h}}\| = 0
\end{equation} T_a","['multivariable-calculus', 'functions', 'derivatives']"
60,Can I simply replace the first and second derivatives with the gradient and laplacian in the curvature of the graph of a function?,Can I simply replace the first and second derivatives with the gradient and laplacian in the curvature of the graph of a function?,,"Can I simply replace the first and second derivatives with the gradient and laplacian in the curvature of the graph of a function to generalize to a multivariable curvature functional of the graph of a function? The curvature of the graph of a function is given by the following. $$\mathscr{k}_f(x) = \frac{f^{\prime\prime}(x)}{(1 + [f^{\prime}(x)]^2)^{\frac{3}{2}}}$$ Assuming a second-differentiable scalar function, is the following a valid generalization of this type of curvature to the multivariable case? $$\mathscr{k}_f(\vec{x}) = \frac{\nabla_{\vec{x}}^2 f(\vec{x})}{(1 + [\nabla_{\vec{x}} f(\vec{x})]^2)^{\frac{3}{2}}}$$ Edit Matthew Pilling convinced me that taking the square of a gradient is not directly possible. I was implicitly assuming I could collapse the vectors by summing the components, however it would be technically correct to make this explicit using either dot products or the trace of the diagonal of the resulting vectors. $$\mathscr{k}_f(\vec{x}) = \frac{\nabla_{\vec{x}}^2 f(\vec{x}) \cdot \vec{1}}{(1 + [\nabla_{\vec{x}} f(\vec{x})\cdot \vec{1}]^2)^{\frac{3}{2}}} =  \frac{\text{tr}[\text{diag}[\nabla_{\vec{x}}^2 f(\vec{x})]]}{(1 + [\text{tr}[\text{diag}[\nabla_{\vec{x}} f(\vec{x})]]]^2)^{\frac{3}{2}}}$$ This is with the diagonal of a vector $\text{Diag}(\vec{x})$ being the construction of a diagonal matrix whose diagonal entries are the corresponding components of the vector $\vec{x}$ . Beyond this correction on vector operations, I'm still interested in whether the corrected equation captures the same idea of curvature as the original equation.","Can I simply replace the first and second derivatives with the gradient and laplacian in the curvature of the graph of a function to generalize to a multivariable curvature functional of the graph of a function? The curvature of the graph of a function is given by the following. Assuming a second-differentiable scalar function, is the following a valid generalization of this type of curvature to the multivariable case? Edit Matthew Pilling convinced me that taking the square of a gradient is not directly possible. I was implicitly assuming I could collapse the vectors by summing the components, however it would be technically correct to make this explicit using either dot products or the trace of the diagonal of the resulting vectors. This is with the diagonal of a vector being the construction of a diagonal matrix whose diagonal entries are the corresponding components of the vector . Beyond this correction on vector operations, I'm still interested in whether the corrected equation captures the same idea of curvature as the original equation.",\mathscr{k}_f(x) = \frac{f^{\prime\prime}(x)}{(1 + [f^{\prime}(x)]^2)^{\frac{3}{2}}} \mathscr{k}_f(\vec{x}) = \frac{\nabla_{\vec{x}}^2 f(\vec{x})}{(1 + [\nabla_{\vec{x}} f(\vec{x})]^2)^{\frac{3}{2}}} \mathscr{k}_f(\vec{x}) = \frac{\nabla_{\vec{x}}^2 f(\vec{x}) \cdot \vec{1}}{(1 + [\nabla_{\vec{x}} f(\vec{x})\cdot \vec{1}]^2)^{\frac{3}{2}}} =  \frac{\text{tr}[\text{diag}[\nabla_{\vec{x}}^2 f(\vec{x})]]}{(1 + [\text{tr}[\text{diag}[\nabla_{\vec{x}} f(\vec{x})]]]^2)^{\frac{3}{2}}} \text{Diag}(\vec{x}) \vec{x},"['multivariable-calculus', 'derivatives', 'curvature', 'laplacian']"
61,Suppose $f$ and $g$ are strictly convex functions and $f$ is increasing. Can $(f\circ g)^{\prime \prime}$ ever equal $0$?,Suppose  and  are strictly convex functions and  is increasing. Can  ever equal ?,f g f (f\circ g)^{\prime \prime} 0,"Edit: Ah, this seems to answer my concerns. Take that Spivak, you big cheater! Original post: This concerns a problem in Calculus by Michael Spivak. Specifically 3rd edition, Chapter 11 Appendix, Problem 5. I'll give the full problem for context. My question concerns the last part (c). Please note: this concerns strictly convex functions. Spivak's convention is for ""convex"" to mean ""strictly convex"" unless otherwise specified. Problem 4 (referenced in part (a)) shows that $f$ is convex on an interval if and only if for all $x$ and $y$ in the interval we have $$f(tx + (1-t)y)<tf(x) + (1-t)f(y)\text{, for }0<t<1.$$ This is not used for part (c) but it's included here for completeness. Chapter 11 Appendix, 5. (a) Prove that if $f$ and $g$ are convex and $f$ is increasing, then $f\circ g$ is convex. (It will be easiest to use Problem 4.) (b) Give an example where $g\circ f$ is not convex (c) Suppose that $f$ and $g$ are twice differentiable. Give another proof of the result of part (a) by considering second derivatives. For part (c) I have: $$(f\circ g)^{\prime}(x) = f^{\prime}(g(x))\cdot g^{\prime}(x)$$ $$(f\circ g)^{\prime\prime}(x) = f^{\prime\prime}(g(x))\cdot [g^{\prime}(x)]^2 + f^{\prime}(g(x))\cdot g^{\prime\prime}(x)$$ If $f$ and $g$ are convex, twice differentiable, and $f$ is increasing, we know that $f^{\prime} > 0$ , except possibly at the leftmost edge of the interval, if such a point exists. This is because $f^{\prime} \geq 0$ and $f^{\prime}$ is increasing. $[g^{\prime}]^2 > 0$ except possibly at a single point. $f^{\prime\prime}$ and $g^{\prime\prime}\geq 0$ . Furthermore, they cannot be $0$ over an interval. They can maybe be zero at distinct points? Taking this all together I see that $(f\circ g)^{\prime\prime} \geq 0$ . However, I don't think I've quite shown that it's greater than $0$ . Can $(f\circ g)^{\prime\prime} = 0$ at discrete points? I think it cannot vanish over any interval, as this would require $g^{\prime\prime}$ to be zero over the interval, which in turn means $g^{\prime}$ is constant. But the discrete points thing, I'm not sure about. The Answer Book solution says: (c) We have $$ (f\circ g)^{\prime} = (f^{\prime} \circ g)g^{\prime}$$ $$ (f\circ g)^{\prime\prime} = (f^{\prime\prime}\circ g) g^{\prime 2} + (f^{\prime}\circ g)g^{\prime\prime}$$ Since $f^{\prime\prime}, g^{\prime\prime}, g^{\prime} \geq 0$ it follows that $(f\circ g)^{\prime\prime} > 0$ if $f^{\prime} > 0$ . That last sentence seems false...","Edit: Ah, this seems to answer my concerns. Take that Spivak, you big cheater! Original post: This concerns a problem in Calculus by Michael Spivak. Specifically 3rd edition, Chapter 11 Appendix, Problem 5. I'll give the full problem for context. My question concerns the last part (c). Please note: this concerns strictly convex functions. Spivak's convention is for ""convex"" to mean ""strictly convex"" unless otherwise specified. Problem 4 (referenced in part (a)) shows that is convex on an interval if and only if for all and in the interval we have This is not used for part (c) but it's included here for completeness. Chapter 11 Appendix, 5. (a) Prove that if and are convex and is increasing, then is convex. (It will be easiest to use Problem 4.) (b) Give an example where is not convex (c) Suppose that and are twice differentiable. Give another proof of the result of part (a) by considering second derivatives. For part (c) I have: If and are convex, twice differentiable, and is increasing, we know that , except possibly at the leftmost edge of the interval, if such a point exists. This is because and is increasing. except possibly at a single point. and . Furthermore, they cannot be over an interval. They can maybe be zero at distinct points? Taking this all together I see that . However, I don't think I've quite shown that it's greater than . Can at discrete points? I think it cannot vanish over any interval, as this would require to be zero over the interval, which in turn means is constant. But the discrete points thing, I'm not sure about. The Answer Book solution says: (c) We have Since it follows that if . That last sentence seems false...","f x y f(tx + (1-t)y)<tf(x) + (1-t)f(y)\text{, for }0<t<1. f g f f\circ g g\circ f f g (f\circ g)^{\prime}(x) = f^{\prime}(g(x))\cdot g^{\prime}(x) (f\circ g)^{\prime\prime}(x) = f^{\prime\prime}(g(x))\cdot [g^{\prime}(x)]^2 + f^{\prime}(g(x))\cdot g^{\prime\prime}(x) f g f f^{\prime} > 0 f^{\prime} \geq 0 f^{\prime} [g^{\prime}]^2 > 0 f^{\prime\prime} g^{\prime\prime}\geq 0 0 (f\circ g)^{\prime\prime} \geq 0 0 (f\circ g)^{\prime\prime} = 0 g^{\prime\prime} g^{\prime}  (f\circ g)^{\prime} = (f^{\prime} \circ g)g^{\prime}  (f\circ g)^{\prime\prime} = (f^{\prime\prime}\circ g) g^{\prime 2} + (f^{\prime}\circ g)g^{\prime\prime} f^{\prime\prime}, g^{\prime\prime}, g^{\prime} \geq 0 (f\circ g)^{\prime\prime} > 0 f^{\prime} > 0","['calculus', 'derivatives', 'convex-analysis']"
62,Proof of Picard's theorem,Proof of Picard's theorem,,"Let $U$ be an open set in $\mathbb{R}^2, (a,b)\in U,$ and let $F: U\to \mathbb{R}$ satisfy a Lipschitz condition on $U.$ Then there exists $\delta > 0 $ so that the differential equation $\frac{dy}{dx} = F(x,y)$ has a unique solution $y=f(x)$ with $f(a) = b,$ defined for all $x\in [a-\delta, a+\delta].$ Below is part of a proof of this theorem. Observe that $y=f(x)$ is a solution to the differential equation $\dfrac{dy}{dx} = F(x,y)$ with $f(a) = b$ iff $f(x)$ satisfies the equation $f(x) = b + \int_a^x F(t, f(t)) dt$ for all $x\in [a-\delta, a+\delta].$ Let $\ell$ be a Lipschitz constant for $F.$ Choose $r > 0 $ so that $\overline{B}((a,b),r)\subseteq U$ and let $k=\max_{(x,y)\in \overline{B}((a,b),r)} |F(x,y)|.$ Choose $\delta >0$ with $\delta < \frac{1}{\ell}$ small enough so that the rectangle $R=[a-\delta, a+\delta]\times [b-k\delta, b+k\delta]\subseteq B((a,b), r).$ I have two main questions: I do not know how to show that if $f(x)$ is a solution to the given differential equation with $f(a) = b,$ then the graph of $f$ must be contained in the rectangle $R$ . Here, $\operatorname{Graph}(f) := \{(x,f(x)) : x\in [a-\delta, a+\delta]\}.$ I was thinking of using the Mean Value Theorem and considering when $x\in [a-\delta, a), x=a, x=[a, a+\delta).$ Suppose $x\in [a-\delta, a+\delta].$ If $x=a, (x,f(x))\in R.$ Otherwise, if $x \in [a-\delta, a), \exists c \in (x,a)$ so that $f(a) - f(x) = \delta f'(c) = \delta F(c,f(c)).$ If I could show that $(c,f(c)) \in \overline{B}((a,b),r)$ (that is, $\lVert(c,f(c)) - (a,b)\rVert\leq r$ ), then the definition of $k$ gives the result. And the case where $x\in (a, a+\delta]$ should be similar. I'm also not sure how to verify that the set $X = \{f \in \overline{C}[a-\delta, a+\delta] \mid\operatorname{Graph}(f)\subseteq R\}$ is a closed subspace of the metric space $C[a-\delta, a+\delta]$ under the supremum metric. I know one way is to show that every sequence in $X$ converges in $X,$ but I'd have to show that the limit's graph is contained in $R,$ which doesn't seem that easy. I only request justification for the two claims above. I know the rest of the proof can be completed using the fact that $X$ is a closed subset of the complete metric space $C[a-\delta, a+\delta].$ Then we can define $G(f)(x) = \int_a^x F(t, f(t)) dt + b,$ has the property that $G(X)\subseteq X$ and that $c=\ell \delta < 1$ is a contraction constant for $G.$ Thus by the Banach fixed point theorem, $G$ has a unique fixed point, which is the unique solution  to the differential equation.","Let be an open set in and let satisfy a Lipschitz condition on Then there exists so that the differential equation has a unique solution with defined for all Below is part of a proof of this theorem. Observe that is a solution to the differential equation with iff satisfies the equation for all Let be a Lipschitz constant for Choose so that and let Choose with small enough so that the rectangle I have two main questions: I do not know how to show that if is a solution to the given differential equation with then the graph of must be contained in the rectangle . Here, I was thinking of using the Mean Value Theorem and considering when Suppose If Otherwise, if so that If I could show that (that is, ), then the definition of gives the result. And the case where should be similar. I'm also not sure how to verify that the set is a closed subspace of the metric space under the supremum metric. I know one way is to show that every sequence in converges in but I'd have to show that the limit's graph is contained in which doesn't seem that easy. I only request justification for the two claims above. I know the rest of the proof can be completed using the fact that is a closed subset of the complete metric space Then we can define has the property that and that is a contraction constant for Thus by the Banach fixed point theorem, has a unique fixed point, which is the unique solution  to the differential equation.","U \mathbb{R}^2, (a,b)\in U, F: U\to \mathbb{R} U. \delta > 0  \frac{dy}{dx} = F(x,y) y=f(x) f(a) = b, x\in [a-\delta, a+\delta]. y=f(x) \dfrac{dy}{dx} = F(x,y) f(a) = b f(x) f(x) = b + \int_a^x F(t, f(t)) dt x\in [a-\delta, a+\delta]. \ell F. r > 0  \overline{B}((a,b),r)\subseteq U k=\max_{(x,y)\in \overline{B}((a,b),r)} |F(x,y)|. \delta >0 \delta < \frac{1}{\ell} R=[a-\delta, a+\delta]\times [b-k\delta, b+k\delta]\subseteq B((a,b), r). f(x) f(a) = b, f R \operatorname{Graph}(f) := \{(x,f(x)) : x\in [a-\delta, a+\delta]\}. x\in [a-\delta, a), x=a, x=[a, a+\delta). x\in [a-\delta, a+\delta]. x=a, (x,f(x))\in R. x \in [a-\delta, a), \exists c \in (x,a) f(a) - f(x) = \delta f'(c) = \delta F(c,f(c)). (c,f(c)) \in \overline{B}((a,b),r) \lVert(c,f(c)) - (a,b)\rVert\leq r k x\in (a, a+\delta] X = \{f \in \overline{C}[a-\delta, a+\delta] \mid\operatorname{Graph}(f)\subseteq R\} C[a-\delta, a+\delta] X X, R, X C[a-\delta, a+\delta]. G(f)(x) = \int_a^x F(t, f(t)) dt + b, G(X)\subseteq X c=\ell \delta < 1 G. G","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
63,Questions about mass conservation in PDE,Questions about mass conservation in PDE,,"Let us consider the following PDE: $$\partial_t{f}(t,x,u)+u\cdot\nabla_xf(t,x,u)+g(x)\cdot\nabla_uf(t,x,u)=0,$$ where $(t,x,u)\in[0,T]\times\mathbb{R}^d\times\mathbb{R}^d$ and $g:\mathbb{R}^d\to \mathbb{R}^d$ is a continuous function. If $f$ is a solution of this PDE in classical sense, the usual proof of the fact that $\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)dxdu$ reads as follows: $$\frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}\partial _tf(t,x,u)dxdu=-\displaystyle\int_{\mathbb{R}^{2d}}u\cdot\nabla_xf(t,x,u)dxdu-\displaystyle\int_{\mathbb{R}^{2d}}g(x)\cdot\nabla_uf(t,x,u)dxdu.$$ By using integration by parts and asuming an appropiate decay of $f$ , we deduce that $\frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=0.$ However, there are some details of this proof that I don't understand: Why can we iterate the time derivative in the first step? Is this property still true if we consider weak solutions?, that is if $f$ verifies: $$\displaystyle\int_{0}^T\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)\left[\partial _t\psi(t,x,u)+u\cdot\nabla_x\psi(t,x,u)+g(x)\cdot\nabla_u\psi(t,x,u)\right]dxdu+\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)\psi(0,x,u)dxdu=0,\\\forall\psi \in C^1([0,T]\times \mathbb{R}^{2d}) \text{ with } \operatorname{supp}\psi\subset [0,T[\times C \text{ and } C \text{ compact in }\mathbb{R}^{2d}$$ Any help about this questions will be welcome.","Let us consider the following PDE: where and is a continuous function. If is a solution of this PDE in classical sense, the usual proof of the fact that reads as follows: By using integration by parts and asuming an appropiate decay of , we deduce that However, there are some details of this proof that I don't understand: Why can we iterate the time derivative in the first step? Is this property still true if we consider weak solutions?, that is if verifies: Any help about this questions will be welcome.","\partial_t{f}(t,x,u)+u\cdot\nabla_xf(t,x,u)+g(x)\cdot\nabla_uf(t,x,u)=0, (t,x,u)\in[0,T]\times\mathbb{R}^d\times\mathbb{R}^d g:\mathbb{R}^d\to \mathbb{R}^d f \displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)dxdu \frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}\partial _tf(t,x,u)dxdu=-\displaystyle\int_{\mathbb{R}^{2d}}u\cdot\nabla_xf(t,x,u)dxdu-\displaystyle\int_{\mathbb{R}^{2d}}g(x)\cdot\nabla_uf(t,x,u)dxdu. f \frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=0. f \displaystyle\int_{0}^T\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)\left[\partial _t\psi(t,x,u)+u\cdot\nabla_x\psi(t,x,u)+g(x)\cdot\nabla_u\psi(t,x,u)\right]dxdu+\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)\psi(0,x,u)dxdu=0,\\\forall\psi \in C^1([0,T]\times \mathbb{R}^{2d}) \text{ with } \operatorname{supp}\psi\subset [0,T[\times C \text{ and } C \text{ compact in }\mathbb{R}^{2d}","['real-analysis', 'functional-analysis', 'derivatives', 'partial-differential-equations', 'lebesgue-integral']"
64,"How do I determine if $f(x)$ has a continuous derivative on $[a,b]$?",How do I determine if  has a continuous derivative on ?,"f(x) [a,b]","When finding the length of a curve using calculus, the first step is to determine whether $f(x)$ has a continuous derivative on the curve (i.e. on $[a,b]$ ). But how do we determine this? My understanding is that you need to find the second derivative, i.e. $f''(x)$ . If $f''(x)$ exists, this implies that $f'(x)$ is a continuous derivative along the curve of $f(x)$ . Is that accurate? I apologize if this is an obvious question. I just feel that my textbook/online resources give much more convoluted answers than this, when the solution is pretty simple. My concern is that I may be overlooking something.","When finding the length of a curve using calculus, the first step is to determine whether has a continuous derivative on the curve (i.e. on ). But how do we determine this? My understanding is that you need to find the second derivative, i.e. . If exists, this implies that is a continuous derivative along the curve of . Is that accurate? I apologize if this is an obvious question. I just feel that my textbook/online resources give much more convoluted answers than this, when the solution is pretty simple. My concern is that I may be overlooking something.","f(x) [a,b] f''(x) f''(x) f'(x) f(x)","['calculus', 'derivatives', 'arc-length']"
65,Implicit form of Burgers' equation,Implicit form of Burgers' equation,,"I’m having a hard time trying to figure out how $u_t$ and $u_x$ have bern derived from $$u(x,t)=u_0(x-u(x,t)t).$$ I’ve tried using the product & chain rules multiple times but I’m just not getting the derivatives below. I know that something has been omitted in the argument but I can’t figure where it would fit in to begin with. Here is the original question:",I’m having a hard time trying to figure out how and have bern derived from I’ve tried using the product & chain rules multiple times but I’m just not getting the derivatives below. I know that something has been omitted in the argument but I can’t figure where it would fit in to begin with. Here is the original question:,"u_t u_x u(x,t)=u_0(x-u(x,t)t).","['calculus', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
66,Why are we treating du/dx as a fraction? [duplicate],Why are we treating du/dx as a fraction? [duplicate],,This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 3 years ago . When I was learning implicit differentiation in my class I was told to not think of $dy/dx$ as a fraction. Now I am doing integration by $u$ -substitution and we treat $du/dx$ as a fraction and solve for $dx$ . Why is this?,This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 3 years ago . When I was learning implicit differentiation in my class I was told to not think of as a fraction. Now I am doing integration by -substitution and we treat as a fraction and solve for . Why is this?,dy/dx u du/dx dx,"['calculus', 'integration', 'derivatives', 'fractions', 'differential-operators']"
67,How were these much simpler derivatives derived (vs. my longer/messier ones)? Efficient/correct use of chain vs. product rule?,How were these much simpler derivatives derived (vs. my longer/messier ones)? Efficient/correct use of chain vs. product rule?,,"I was trying to reverse engineer an equation from an article I was reading. The basic expressions I was working from were: $ε = \frac{1}{2}w_x^2\tag{1}$ $b_1ε_tw_{xx} + b_2ε_{tt}w_{xx} + b_1ε_{xt}w_x + b_2ε_{xtt}w_x\tag{2}$ $(2)=2b_1w_xw_{xx}w_{xt} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + b_2w_x^2w_{xxtt}\tag{3}$ I found that I was able to get their equation (3) but only by using derivations of the $ε$ terms using a specific method I'm not sure of. Solely using the product rule applying all derivatives in a stepwise fashion we get the following set of equations we will call ""Set A"": [SET A] All derivatives via stepwise product rule: $ε = \frac{1}{2}w_x^2$ $ε_x = w_xw_{xx}$ $ε_t = w_xw_{xt}$ $ε_{xt} = w_{xt}w_{xx}+w_xw_{xxt}$ $ε_{tt} = w_{xt}^2 +  w_xw_{xtt}$ $ε_{xtt} = w_{xtt}w_{xx} + 2w_{xt}w_{xxt} + w_xw_{xxtt}$ If you substitute ""Set A"" into (2) you get: $2b_1w_xw_{xx}w_{xt} + b_2w_{xt}^2w_{xx} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + 2b_2w_xw_{xt}w_{xxt} + b_2 w_x^2w_{xxtt}\tag{4}$ (4) at least superficially does not seem the same as (3). If however, we assume that you can apply a double derivative of the same nature (ie. $y_{tt}$ or $y_{xx}$ ) in one step via the product rule so that $ε_{tt} = (\frac{1}{2}w_xw_x)_{tt} = w_xw_{xtt}$ , we get ""Set B"" of definitions: [SET B] Apply dual derivatives in one step: $ε = \frac{1}{2}w_x^2$ $ε_x = w_xw_{xx}$ $ε_t = w_xw_{xt}$ $ε_{xt} = w_xw_{xxt} + w_{xt}w_{xx}$ $ε_{tt} = w_xw_{xtt}$ //DIFFERENT FROM SET A $ε_{xtt} = w_{xx}w_{xtt}+w_xw_{xxtt}$ //DIFFERENT FROM SET A If you substitute ""Set B"" into (2) you get (3) exactly, so these seem to be the derivatives they are using. However this creates the impossible equality: $ε_{tt} = w_xw_{xtt} = w_{xt}^2 +  w_xw_{xtt}\tag{5}$ Is ""Set B"" valid? Is it a manifestation of the chain rule perhaps? ie. $y_{tt} = (y_t)_t$ ? What about ""Set A""? I presume there is no way they can both be true. I wonder if the authors could have made a mistake doing this. It's perhaps possible I made some other error, but I think it would be incredibly coincidental that using ""Set B"" derives their terms identically. Everything else in our equations line up except this part. So I do presume ""Set B"" is what they did. I don't really understand the chain rule. I'm not that knowledgeable about derivatives. Which is mathematically valid: ""Set A"" or ""Set B""? Any clarification is appreciated. Thanks.","I was trying to reverse engineer an equation from an article I was reading. The basic expressions I was working from were: I found that I was able to get their equation (3) but only by using derivations of the terms using a specific method I'm not sure of. Solely using the product rule applying all derivatives in a stepwise fashion we get the following set of equations we will call ""Set A"": [SET A] All derivatives via stepwise product rule: If you substitute ""Set A"" into (2) you get: (4) at least superficially does not seem the same as (3). If however, we assume that you can apply a double derivative of the same nature (ie. or ) in one step via the product rule so that , we get ""Set B"" of definitions: [SET B] Apply dual derivatives in one step: //DIFFERENT FROM SET A //DIFFERENT FROM SET A If you substitute ""Set B"" into (2) you get (3) exactly, so these seem to be the derivatives they are using. However this creates the impossible equality: Is ""Set B"" valid? Is it a manifestation of the chain rule perhaps? ie. ? What about ""Set A""? I presume there is no way they can both be true. I wonder if the authors could have made a mistake doing this. It's perhaps possible I made some other error, but I think it would be incredibly coincidental that using ""Set B"" derives their terms identically. Everything else in our equations line up except this part. So I do presume ""Set B"" is what they did. I don't really understand the chain rule. I'm not that knowledgeable about derivatives. Which is mathematically valid: ""Set A"" or ""Set B""? Any clarification is appreciated. Thanks.",ε = \frac{1}{2}w_x^2\tag{1} b_1ε_tw_{xx} + b_2ε_{tt}w_{xx} + b_1ε_{xt}w_x + b_2ε_{xtt}w_x\tag{2} (2)=2b_1w_xw_{xx}w_{xt} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + b_2w_x^2w_{xxtt}\tag{3} ε ε = \frac{1}{2}w_x^2 ε_x = w_xw_{xx} ε_t = w_xw_{xt} ε_{xt} = w_{xt}w_{xx}+w_xw_{xxt} ε_{tt} = w_{xt}^2 +  w_xw_{xtt} ε_{xtt} = w_{xtt}w_{xx} + 2w_{xt}w_{xxt} + w_xw_{xxtt} 2b_1w_xw_{xx}w_{xt} + b_2w_{xt}^2w_{xx} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + 2b_2w_xw_{xt}w_{xxt} + b_2 w_x^2w_{xxtt}\tag{4} y_{tt} y_{xx} ε_{tt} = (\frac{1}{2}w_xw_x)_{tt} = w_xw_{xtt} ε = \frac{1}{2}w_x^2 ε_x = w_xw_{xx} ε_t = w_xw_{xt} ε_{xt} = w_xw_{xxt} + w_{xt}w_{xx} ε_{tt} = w_xw_{xtt} ε_{xtt} = w_{xx}w_{xtt}+w_xw_{xxtt} ε_{tt} = w_xw_{xtt} = w_{xt}^2 +  w_xw_{xtt}\tag{5} y_{tt} = (y_t)_t,"['derivatives', 'partial-derivative']"
68,Linearization of the Derivative Block (MATLAB),Linearization of the Derivative Block (MATLAB),,"In MATLABs documentation for the derivative block here , it states the following: The exact linearization of the Derivative block is difficult because the dynamic equation for the block is y= $\dot{u}$ , which you cannot represent as a state-space system. However, you can approximate the linearization by adding a pole to the Derivative block to create a transfer function s/(c∗s+1). Can anyone explain what it means to linearize the Derivative block? I thought differentiation was a linear operator, so I don't see why you'd want to linearize it. Thanks in advance!","In MATLABs documentation for the derivative block here , it states the following: The exact linearization of the Derivative block is difficult because the dynamic equation for the block is y= , which you cannot represent as a state-space system. However, you can approximate the linearization by adding a pole to the Derivative block to create a transfer function s/(c∗s+1). Can anyone explain what it means to linearize the Derivative block? I thought differentiation was a linear operator, so I don't see why you'd want to linearize it. Thanks in advance!",\dot{u},"['derivatives', 'matlab', 'linearization']"
69,Review a solution for second order partial derivative.,Review a solution for second order partial derivative.,,"Check please my solution for second derivative. If it's not correct, what are my mistakes? z=ctg(y/x) $\frac{dz}{dx} = \frac{y}{sin^2(\frac{y}{x})*x^2}$ $\frac{dz}{dy} = \frac{-1}{sin^2(\frac{y}{x})*x}$ A solution for second order derivatives $ z''xx = y * (\frac{1}{sin^2(\frac{y}{x})*x^2})' = y* \frac{0-sin^2(y/x)*x^2}{(sin^2(\frac{y}{x})*x^2)^2}- = \frac{y*(-2*sin(y/x)*2x)}{sin^4(\frac{y}{x})*x^4} = \frac{-y*4x*sin(\frac{y}{x})}{sin^4(\frac{y}{x})*x^4} $","Check please my solution for second derivative. If it's not correct, what are my mistakes? z=ctg(y/x) A solution for second order derivatives","\frac{dz}{dx} = \frac{y}{sin^2(\frac{y}{x})*x^2} \frac{dz}{dy} = \frac{-1}{sin^2(\frac{y}{x})*x} 
z''xx = y * (\frac{1}{sin^2(\frac{y}{x})*x^2})' = y* \frac{0-sin^2(y/x)*x^2}{(sin^2(\frac{y}{x})*x^2)^2}- = \frac{y*(-2*sin(y/x)*2x)}{sin^4(\frac{y}{x})*x^4} =
\frac{-y*4x*sin(\frac{y}{x})}{sin^4(\frac{y}{x})*x^4}
","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
70,What is the derivative of $(\mathbf{u}(x))^TA\mathbf{v}(x)$ w.r.t. $x$?,What is the derivative of  w.r.t. ?,(\mathbf{u}(x))^TA\mathbf{v}(x) x,"What is $\frac{d}{dx}(\mathbf{u}^TA\mathbf{v})$ where $\mathbf{u}$ and $\mathbf{v}$ are column-vector-valued functions of $x$ , and $A$ is a matrix? Is it $\frac{d\mathbf{u}^T}{dx}A\mathbf{v}+\mathbf{u}^TA\frac{d\mathbf{v}}{dx}$ ?","What is where and are column-vector-valued functions of , and is a matrix? Is it ?",\frac{d}{dx}(\mathbf{u}^TA\mathbf{v}) \mathbf{u} \mathbf{v} x A \frac{d\mathbf{u}^T}{dx}A\mathbf{v}+\mathbf{u}^TA\frac{d\mathbf{v}}{dx},"['linear-algebra', 'derivatives', 'matrix-calculus']"
71,"Determine if $f$ is differentiable, and computing $f '(x)$","Determine if  is differentiable, and computing",f f '(x),"I would like to know if I did this correctly: Let $$f(x)=\left\{\begin{matrix} \frac{\sin(x^2)}{x} & x \ne 0\\ 0&x=0 \end{matrix}\right. , x_{0}=0$$ So $$f'(x)=\lim_{x \to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x \to 0}\frac{\frac{\sin(x^2)}{x}-0}{x-0}=\lim_{x \to 0}\frac{\sin(x^2)}{x^2}=\lim_{t \to 0}\frac{\sin(t)}{t}=1$$ Is that correct? I didn't use the part that $f(x)=0$ if $x=0$ so I think that I'm missing something here. Thanks a lot!",I would like to know if I did this correctly: Let So Is that correct? I didn't use the part that if so I think that I'm missing something here. Thanks a lot!,"f(x)=\left\{\begin{matrix}
\frac{\sin(x^2)}{x} & x \ne 0\\
0&x=0
\end{matrix}\right. , x_{0}=0 f'(x)=\lim_{x \to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x \to 0}\frac{\frac{\sin(x^2)}{x}-0}{x-0}=\lim_{x \to 0}\frac{\sin(x^2)}{x^2}=\lim_{t \to 0}\frac{\sin(t)}{t}=1 f(x)=0 x=0","['calculus', 'limits', 'derivatives', 'solution-verification']"
72,Uniform convergence and differentiability of $\frac{nx+x^2}{2n}$ and $\frac{nx^2+1}{2n+x}$,Uniform convergence and differentiability of  and,\frac{nx+x^2}{2n} \frac{nx^2+1}{2n+x},"I have solved the following exercise and I would like to know if I have made any mistakes: Let $g_n(x)=\frac{nx+x^2}{2n}$ and set $g(x)=\lim g_n(x)$ . Show that $g$ is differentiable in two ways: (a) Compute $g(x)$ by algebraically taking the limit as $n\to\infty$ and then find $g'(x)$ . (b) Compute $g'_n(x)$ for each $n\in\mathbb{N}$ and show that the sequence of derivatives $(g'_n)$ converges uniformly on the interval $[-M,M]$ . Conclude $g'(x)=\lim g'_n(x)$ . (c) Repeat parts (a) and (b) for the sequence $f_n(x)=\frac{nx^2+1}{2n+x}$ . My solution: (a) For fixed $x\in\mathbb{R}: \frac{nx+x^2}{2n}=\frac{nx}{2n}+\frac{x^2}{2n}=\frac{x}{2}+\frac{x^2}{2n}\xrightarrow[]{n\to\infty}\frac{x}{2}=:g(x)$ so $g'(x)=\frac{1}{2}$ ; (b) $g'_n(x)=\frac{1}{2}+\frac{x}{n}$ and $|g'_n(x)-\frac{1}{2}|=\frac{|x|}{n}\leq\frac{M}{n}<\varepsilon$ for $n>\frac{M}{\varepsilon}$ so $g'_n$ is uniformly convergent to $\frac{1}{2}$ on each interval $[-M,M]$ and since $0\in [-M,M]$ for every $M\in\mathbb{R}$ and $g_n(0)=0\xrightarrow[]{n\to\infty}0$ , so we can conclude that $g_n$ converges uniformly on $[-M,M]$ and $g=\lim_{n\to\infty} g_n$ is differentiable and satisfies $g'=\lim g'_n$ . (c) For fixed $x\in\mathbb{R}: \frac{nx^2+1}{2n+x}=\frac{1}{2}\frac{x^2 +\frac{1}{n}}{1+\frac{x}{2n}}\xrightarrow[]{n\to\infty}\frac{x^2}{2}=:f(x)$ and $f'(x)=x$ ; $f'_n(x)=\frac{4n^2x+nx^2-1}{(2n+x)^2}$ and $|f'_n(x)-x|=|\frac{x^3+3nx^2+1}{(2n+x)^2}|\leq\frac{x^2 |x+3n|+1}{4n^2}\leq\frac{M^2 (M+3n)+1}{4n^2}\xrightarrow[]{n\to\infty}0$ so $f'_n$ converges uniformly to $x$ on $[-M,M]$ and since $0\in [-M,M]$ and $f_n(0)=\frac{1}{2n}\xrightarrow[]{n\to\infty}0$ so we can conclude that $f_n$ converges uniformly on $[-M,M]$ and $f=\lim f_n$ is differentiable and satisfies $f'=\lim f'_n$ .","I have solved the following exercise and I would like to know if I have made any mistakes: Let and set . Show that is differentiable in two ways: (a) Compute by algebraically taking the limit as and then find . (b) Compute for each and show that the sequence of derivatives converges uniformly on the interval . Conclude . (c) Repeat parts (a) and (b) for the sequence . My solution: (a) For fixed so ; (b) and for so is uniformly convergent to on each interval and since for every and , so we can conclude that converges uniformly on and is differentiable and satisfies . (c) For fixed and ; and so converges uniformly to on and since and so we can conclude that converges uniformly on and is differentiable and satisfies .","g_n(x)=\frac{nx+x^2}{2n} g(x)=\lim g_n(x) g g(x) n\to\infty g'(x) g'_n(x) n\in\mathbb{N} (g'_n) [-M,M] g'(x)=\lim g'_n(x) f_n(x)=\frac{nx^2+1}{2n+x} x\in\mathbb{R}: \frac{nx+x^2}{2n}=\frac{nx}{2n}+\frac{x^2}{2n}=\frac{x}{2}+\frac{x^2}{2n}\xrightarrow[]{n\to\infty}\frac{x}{2}=:g(x) g'(x)=\frac{1}{2} g'_n(x)=\frac{1}{2}+\frac{x}{n} |g'_n(x)-\frac{1}{2}|=\frac{|x|}{n}\leq\frac{M}{n}<\varepsilon n>\frac{M}{\varepsilon} g'_n \frac{1}{2} [-M,M] 0\in [-M,M] M\in\mathbb{R} g_n(0)=0\xrightarrow[]{n\to\infty}0 g_n [-M,M] g=\lim_{n\to\infty} g_n g'=\lim g'_n x\in\mathbb{R}: \frac{nx^2+1}{2n+x}=\frac{1}{2}\frac{x^2 +\frac{1}{n}}{1+\frac{x}{2n}}\xrightarrow[]{n\to\infty}\frac{x^2}{2}=:f(x) f'(x)=x f'_n(x)=\frac{4n^2x+nx^2-1}{(2n+x)^2} |f'_n(x)-x|=|\frac{x^3+3nx^2+1}{(2n+x)^2}|\leq\frac{x^2 |x+3n|+1}{4n^2}\leq\frac{M^2 (M+3n)+1}{4n^2}\xrightarrow[]{n\to\infty}0 f'_n x [-M,M] 0\in [-M,M] f_n(0)=\frac{1}{2n}\xrightarrow[]{n\to\infty}0 f_n [-M,M] f=\lim f_n f'=\lim f'_n","['real-analysis', 'derivatives', 'solution-verification', 'uniform-convergence']"
73,What is the derivative of a complex function that includes a complex conjugate variable?,What is the derivative of a complex function that includes a complex conjugate variable?,,"How can I take the derivative of $f(z) = z + z^*$ , where $z$ is complex, i.e. $z=a+ib$ ? My question is actually for $f(z, z^*)$ in general. My complex variables book doesn't include such an example - why not?","How can I take the derivative of , where is complex, i.e. ? My question is actually for in general. My complex variables book doesn't include such an example - why not?","f(z) = z + z^* z z=a+ib f(z, z^*)","['complex-analysis', 'derivatives', 'complex-numbers']"
74,"Is my proof right? (Serge Lang ""Calculus of Several Variables 3rd Edition"", Potential Theory)","Is my proof right? (Serge Lang ""Calculus of Several Variables 3rd Edition"", Potential Theory)",,"I am reading ""Calculus of Several Variables 3rd Edition"" by Serge Lang. Let $G(x,y) := (\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2})$ . Then, $\phi(x,y):=\arg(x+iy)$ is a potential function for the vector field $G$ on the plane from which the shaded region has been deleted. (Please see the picture below.) I proved the above fact. But I am not sure that I am right or not. Is my answer right? My solution is here: If $0\leq\arg(x+iy)<\frac{\pi}{2}$ , then $\phi(x,y)=\arctan(\frac{y}{x})$ . $\nabla \phi = G(x,y)$ . If $\arg(x+iy)=\frac{\pi}{2}$ , then $\phi(x,y)=\frac{\pi}{2}$ . $\lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})-\frac{\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{\pi}{2}-\frac{\pi}{2}}{h} = 0$ . So, $\nabla \phi(0,y) = G(0,y)$ . If $\frac{\pi}{2}<\arg(x+iy)<\frac{3\pi}{2}$ , then $\phi(x,y)=\arctan(\frac{y}{x})+\pi$ . $\nabla \phi = G(x,y)$ . If $\arg(x+iy)=\frac{3\pi}{2}$ , then $\phi(x,y)=\frac{3\pi}{2}$ . $\lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+2\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y}$ by L'Hospital's rule. $\lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{3\pi}{2}-\frac{3\pi}{2}}{h} = 0$ . So, $\nabla \phi(0,y) = G(0,y)$ . If $\frac{3\pi}{2}\leq\arg(x+iy)\leq 2\pi-c$ , then $\phi(x,y)=\arctan(\frac{y}{x})+2\pi$ . $\nabla \phi = G(x,y)$ .","I am reading ""Calculus of Several Variables 3rd Edition"" by Serge Lang. Let . Then, is a potential function for the vector field on the plane from which the shaded region has been deleted. (Please see the picture below.) I proved the above fact. But I am not sure that I am right or not. Is my answer right? My solution is here: If , then . . If , then . by L'Hospital's rule. by L'Hospital's rule. . So, . If , then . . If , then . by L'Hospital's rule. by L'Hospital's rule. . So, . If , then . .","G(x,y) := (\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}) \phi(x,y):=\arg(x+iy) G 0\leq\arg(x+iy)<\frac{\pi}{2} \phi(x,y)=\arctan(\frac{y}{x}) \nabla \phi = G(x,y) \arg(x+iy)=\frac{\pi}{2} \phi(x,y)=\frac{\pi}{2} \lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})-\frac{\pi}{2}}{h} = \frac{-1}{y} \lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{\pi}{2}}{h} = \frac{-1}{y} \lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{\pi}{2}-\frac{\pi}{2}}{h} = 0 \nabla \phi(0,y) = G(0,y) \frac{\pi}{2}<\arg(x+iy)<\frac{3\pi}{2} \phi(x,y)=\arctan(\frac{y}{x})+\pi \nabla \phi = G(x,y) \arg(x+iy)=\frac{3\pi}{2} \phi(x,y)=\frac{3\pi}{2} \lim_{h\to0, x>0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+2\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y} \lim_{h\to0, h<0} \frac{\arg(h+iy)-\arg(0+iy)}{h} = \lim_{h\to0, h>0} \frac{\arctan(\frac{y}{h})+\pi-\frac{3\pi}{2}}{h} = \frac{-1}{y} \lim_{h\to0} \frac{\arg(0+i(y+h))-\arg(0+iy)}{h} = \lim_{h\to0} \frac{\frac{3\pi}{2}-\frac{3\pi}{2}}{h} = 0 \nabla \phi(0,y) = G(0,y) \frac{3\pi}{2}\leq\arg(x+iy)\leq 2\pi-c \phi(x,y)=\arctan(\frac{y}{x})+2\pi \nabla \phi = G(x,y)","['derivatives', 'solution-verification', 'potential-theory']"
75,Differentiability of $fg$ when one differentiable and other not but product differentiable.,Differentiability of  when one differentiable and other not but product differentiable.,fg,"I conjectured the following theorem : Let $I$ be an open interval and let $a\in I.\,\,$ If $f,g:I\to\mathbb{R}$ such that $f$ is differentiable at $x=a$ with $f(a)=f'(a)=0$ and $g(x)$ is a bounded function over $(a-h,a+h)$ for some $h>0$ , then $f(x)g(x)$ is differentiable at $x=a.$ Proof: Let $\phi(x)=f(x)g(x)$ and let $\exists\,\, M>0\text{  such that }|g(x)|<M\,\,\forall \,\,x\in(a-h,a+h)$ $\lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=\lim_{h\to 0}\frac{f(a+h)g(a+h)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}g(a+h)$ Now $-M\left |\frac{f(a+h)-f(a)}{h}\right | \le \left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le M\left |\frac{f(a+h)-f(a)}{h}\right |$ $\implies 0\le \lim_{h\to0}\left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le 0$ $\implies \lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=0$ $\implies \phi(x)$ is differentiable. Is this working correct. Can we further generalised it ? any similar results most welcome?","I conjectured the following theorem : Let be an open interval and let If such that is differentiable at with and is a bounded function over for some , then is differentiable at Proof: Let and let Now is differentiable. Is this working correct. Can we further generalised it ? any similar results most welcome?","I a\in I.\,\, f,g:I\to\mathbb{R} f x=a f(a)=f'(a)=0 g(x) (a-h,a+h) h>0 f(x)g(x) x=a. \phi(x)=f(x)g(x) \exists\,\, M>0\text{  such that }|g(x)|<M\,\,\forall \,\,x\in(a-h,a+h) \lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=\lim_{h\to 0}\frac{f(a+h)g(a+h)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}g(a+h) -M\left |\frac{f(a+h)-f(a)}{h}\right | \le \left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le M\left |\frac{f(a+h)-f(a)}{h}\right | \implies 0\le \lim_{h\to0}\left |\frac{f(a+h)-f(a)}{h}g(a+h)\right |\le 0 \implies \lim_{h\to 0}\frac{\phi(a+h)-\phi(a)}{h}=0 \implies \phi(x)","['calculus', 'analysis', 'derivatives']"
76,How to use comparison lemma to solve the differential inequality $\dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})}$,How to use comparison lemma to solve the differential inequality,\dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})},"I am reading a paper https://ieeexplore.ieee.org/document/7782377 : (p.3864 top) Control Barrier Function Based Quadratic Programs for Safety Critical Systems The comparison Lemma says: Consider the scalar differential equation $$\dot{u} = f(t,u), \ \ \  u(t_0)=u_0,$$ where $f(t,u)$ is continuous in $t$ and locally Lipschitz in $u$ , for all $t \geq 0$ and all $u\in J \subset \mathbb{R}$ . Let $[t_0,T]$ be the interval of existence of the solution $u(t)$ . Let $v(t)$ be a differentiable function  whose derivative $Dv(t)$ satisfies the differential inequality $$Dv(t) \leq f(t, v(t)), \ \ \ \ v(t_0) \leq u_0$$ with $v(t)\in J$ for all $t\in [t_0,T)$ . Then, $v(t)\leq u(t)$ for all $t\in[t_0,T)$ . So suppose we have $$\dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})},$$ compared to the Lemma, we have $$Dv(t)=\frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})},$$ how to obtain the result $$h(x(t,x_0))\geq \frac{1}{-1+\exp\bigg(\sqrt{2\gamma t + \log^2\Big(\frac{h(x_0)+1}{h(x_0)}\Big)}\bigg)}?$$ Thanks in advanced.","I am reading a paper https://ieeexplore.ieee.org/document/7782377 : (p.3864 top) Control Barrier Function Based Quadratic Programs for Safety Critical Systems The comparison Lemma says: Consider the scalar differential equation where is continuous in and locally Lipschitz in , for all and all . Let be the interval of existence of the solution . Let be a differentiable function  whose derivative satisfies the differential inequality with for all . Then, for all . So suppose we have compared to the Lemma, we have how to obtain the result Thanks in advanced.","\dot{u} = f(t,u), \ \ \  u(t_0)=u_0, f(t,u) t u t \geq 0 u\in J \subset \mathbb{R} [t_0,T] u(t) v(t) Dv(t) Dv(t) \leq f(t, v(t)), \ \ \ \ v(t_0) \leq u_0 v(t)\in J t\in [t_0,T) v(t)\leq u(t) t\in[t_0,T) \dot{h}\geq \frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})}, Dv(t)=\frac{\gamma(h+h^2)}{\log(\frac{h}{1+h})}, h(x(t,x_0))\geq \frac{1}{-1+\exp\bigg(\sqrt{2\gamma t + \log^2\Big(\frac{h(x_0)+1}{h(x_0)}\Big)}\bigg)}?","['ordinary-differential-equations', 'derivatives', 'inequality']"
77,derivative of the laplacian operator,derivative of the laplacian operator,,"Assumes $\Delta \phi = \nabla\cdot\nabla\phi$ and $$ f(\phi) = \Delta\phi $$ then, what is $$ \frac{df(\phi)}{d\phi} $$ no clue how to solve this","Assumes and then, what is no clue how to solve this","\Delta \phi = \nabla\cdot\nabla\phi 
f(\phi) = \Delta\phi
 
\frac{df(\phi)}{d\phi}
","['derivatives', 'laplacian']"
78,Why is integration so much harder than differentiation?,Why is integration so much harder than differentiation?,,"If a function is a combination of other functions whose derivatives are known via composition, addition, etc., the derivative can be calculated using the chain rule and the like. But even the product of integrals can't be expressed in general in terms of the integral of the products, and forget about composition! Why is this?","If a function is a combination of other functions whose derivatives are known via composition, addition, etc., the derivative can be calculated using the chain rule and the like. But even the product of integrals can't be expressed in general in terms of the integral of the products, and forget about composition! Why is this?",,"['calculus', 'integration']"
79,Show the series defines an infinitely differentiable function,Show the series defines an infinitely differentiable function,,"show $f(x) = \sum\limits_{n=1}^{\infty} \frac{1}{n(x+n)}$ defined on $[0, \infty)$ is $C^\infty$ I'm working on this practice problem.  I differentiated each term to get $$g(x) = \sum \frac{d}{dx} \frac{1}{n(x+n)}=\sum \frac{-1}{nx^2+2xn^2+n^3} = \frac{d}{dx} \sum  \frac{1}{n(x+n)}=f'(x)$$ interchanging the differentiation and the sum becasue the sum of the derivatives converges uniformly on the domain. Then I continued by induction, using the fact that the order of the numerator of $g$ is 3 less than the order of the denominator, to say that if we keep taking the derivative we will keep getting uniformly convergent series and can interchange the differentiation and the sum. This seems messy to me - I'm not sure it works 100%.  Am I missing something?  Or is there a cleaner way to do this problem like transform it into a power series?","show defined on is I'm working on this practice problem.  I differentiated each term to get interchanging the differentiation and the sum becasue the sum of the derivatives converges uniformly on the domain. Then I continued by induction, using the fact that the order of the numerator of is 3 less than the order of the denominator, to say that if we keep taking the derivative we will keep getting uniformly convergent series and can interchange the differentiation and the sum. This seems messy to me - I'm not sure it works 100%.  Am I missing something?  Or is there a cleaner way to do this problem like transform it into a power series?","f(x) = \sum\limits_{n=1}^{\infty} \frac{1}{n(x+n)} [0, \infty) C^\infty g(x) = \sum \frac{d}{dx} \frac{1}{n(x+n)}=\sum \frac{-1}{nx^2+2xn^2+n^3} = \frac{d}{dx} \sum  \frac{1}{n(x+n)}=f'(x) g","['real-analysis', 'sequences-and-series', 'analysis', 'derivatives']"
80,Implicit Differentiation + Related Rates,Implicit Differentiation + Related Rates,,"I have a very simple question, but different methods lead me to different solutions which is where I am confused. (Q) Obtain a relationship for change between circumference $C$ and the area $A$ of a circle over time. $ A = \pi r^2 \\ C = 2\pi r $ Method 1: So $A = \frac{1}{4\pi}C^2$ after substituting $r$ in terms of $C$ . Then implicit differentiation gives us $\frac{dA}{dt} = \frac{1}{2\pi} \cdot \frac{dC}{dt}$ Method 2: Differentiating the first two equations, we have $\frac{dA}{dt} = 2\pi r \frac{dr}{dt}$ and $\frac{dC}{dt} = 2 \pi \frac{dr}{dt}$ respectively. Then I isolated for $\frac{dr}{dt}$ in the second equation here and substituted into the first equation resulting in the relation $\frac{dA}{dt} = 2\pi r \left(\frac{1}{2\pi} \cdot \frac{dC}{dt}\right) = r \frac{dC}{dt}$ . I believe the second method is incorrect; however, I am unsure why it is incorrect. If there is any reading that might solidify my knowledge in terms of related rates/implicit differentiation, I would appreciate it.","I have a very simple question, but different methods lead me to different solutions which is where I am confused. (Q) Obtain a relationship for change between circumference and the area of a circle over time. Method 1: So after substituting in terms of . Then implicit differentiation gives us Method 2: Differentiating the first two equations, we have and respectively. Then I isolated for in the second equation here and substituted into the first equation resulting in the relation . I believe the second method is incorrect; however, I am unsure why it is incorrect. If there is any reading that might solidify my knowledge in terms of related rates/implicit differentiation, I would appreciate it.","C A 
A = \pi r^2 \\
C = 2\pi r
 A = \frac{1}{4\pi}C^2 r C \frac{dA}{dt} = \frac{1}{2\pi} \cdot \frac{dC}{dt} \frac{dA}{dt} = 2\pi r \frac{dr}{dt} \frac{dC}{dt} = 2 \pi \frac{dr}{dt} \frac{dr}{dt} \frac{dA}{dt} = 2\pi r \left(\frac{1}{2\pi} \cdot \frac{dC}{dt}\right) = r \frac{dC}{dt}","['geometry', 'derivatives', 'implicit-differentiation', 'related-rates']"
81,Infimum of gradient's norm of a sequence of vanishing functions,Infimum of gradient's norm of a sequence of vanishing functions,,"Let $\{f_k\}_{k \in \mathbb{N}} \subset C^1(\mathbb{R}^n; \mathbb{R})$ be a sequence of continuously differentiable functions s.t. $$\lim_{k \to + \infty} f_k(x) =0 \quad \forall \, x \in \mathbb{R}^n.$$ Is it true that $$\lim_{k \to + \infty} \inf_{x \in \mathbb{R}^n} |\nabla f_k(x)| =0$$ ? Here $\nabla f_k(x)$ is the gradient of $f_k$ at the point $x$ and $|\cdot|$ is the Euclidean norm. It seems to me that the statement is true if $n=1$ , but what about $n\ge 2$ ?","Let be a sequence of continuously differentiable functions s.t. Is it true that ? Here is the gradient of at the point and is the Euclidean norm. It seems to me that the statement is true if , but what about ?","\{f_k\}_{k \in \mathbb{N}} \subset C^1(\mathbb{R}^n; \mathbb{R}) \lim_{k \to + \infty} f_k(x) =0 \quad \forall \, x \in \mathbb{R}^n. \lim_{k \to + \infty} \inf_{x \in \mathbb{R}^n} |\nabla f_k(x)| =0 \nabla f_k(x) f_k x |\cdot| n=1 n\ge 2","['derivatives', 'pointwise-convergence']"
82,Order of derivatives in automatic differentiation,Order of derivatives in automatic differentiation,,"I did this derivation and want to know if it is correct. Basically, we want to know the gradient of $f$ at the point $g(M)$ . Let $\Psi \equiv g \circ f$ be the function representing the forward pass and let $f: \mathbb{R}^{m \times n} \to \mathbb{R}^{m \times n}$ represent the part of the forward pass we are interested in, where $m$ is the batch size and $n$ the array length. Assume we can decompose $f$ into $d \in \mathbb{N}^+$ functions $f_1, f_2, \dots, f_d$ : \begin{equation*}     f \equiv f_d \circ f_{d-1} \circ \dots \circ f_1 \end{equation*} Now define $f^i$ for $i \in \{0, 1, \dots, d\}$ , where $f_0(M) := M$ : \begin{equation*}     f^i := f_i \circ f_{i - 1} \circ \dots \circ f_1 \circ f_0 \end{equation*} Defininig $f_0$ as the identity function is just a trick to simplify the below expression. Please note that we already know $\nabla g(f(M))$ as that is one of the inputs to our backward pass and since we did the forward pass, we also calculated all the $f^i$ , $i \in \{0, 1, \dots, d\}$ . Now let $M \in \mathbb{R}^{m \times n}$ . It follows using the chain rule: \begin{equation*}     \begin{aligned}         \nabla (g \circ f)(M)          & = \nabla g(f(M)) \cdot \nabla f^d(M)                                                                   \\          & = \nabla g(f(M)) \cdot \nabla f_d(f^{d-1}(M)) \cdot \nabla f^{d-1}(M)                                  \\          & = \nabla g(f(M)) \cdot \nabla f_d(f^{d-1}(M)) \cdot \nabla f_{d-1}(f^{d-2}(M)) \cdot \nabla f^{d-2}(M) \\          & \;\;\vdots                                                                                       \\          & = \nabla g(f(M)) \cdot \prod_{i=1}^{d-1} \left(\nabla f_{d-i+1}(f^{d-i}(M))\right) \cdot \nabla f^1(M) \\          &= \underbrace{\nabla g(f(M))}_{\text{known}} \cdot \prod_{i=1}^{d} \nabla f_{d-i+1}(\underbrace{f^{d-i}(M)}_{\text{known}})     \end{aligned} \end{equation*} My derivation implies that the order of calculating the $\nabla f_i(f^{i-1}(M))$ is irrelevant. However, the technique to do automatic differentiation is called backward pass, and as I understand it, you always use the previous derivative as an input to the next one, taking derivatives of the composed parts of the desired function in reverse order. Did I make a mistake or does the order not matter? I'm not specifically looking to apply this to neural networks. Is the order maybe only relevant if we have a neural network, where we want to update every layer $k$ for which we need $\prod_{i=1}^k \nabla f_{d-i+1}(f^{d-i}(M))$ ? More specifically, does this mean the backwards pass can be parallelized to some degree?","I did this derivation and want to know if it is correct. Basically, we want to know the gradient of at the point . Let be the function representing the forward pass and let represent the part of the forward pass we are interested in, where is the batch size and the array length. Assume we can decompose into functions : Now define for , where : Defininig as the identity function is just a trick to simplify the below expression. Please note that we already know as that is one of the inputs to our backward pass and since we did the forward pass, we also calculated all the , . Now let . It follows using the chain rule: My derivation implies that the order of calculating the is irrelevant. However, the technique to do automatic differentiation is called backward pass, and as I understand it, you always use the previous derivative as an input to the next one, taking derivatives of the composed parts of the desired function in reverse order. Did I make a mistake or does the order not matter? I'm not specifically looking to apply this to neural networks. Is the order maybe only relevant if we have a neural network, where we want to update every layer for which we need ? More specifically, does this mean the backwards pass can be parallelized to some degree?","f g(M) \Psi \equiv g \circ f f: \mathbb{R}^{m \times n} \to \mathbb{R}^{m \times n} m n f d \in \mathbb{N}^+ f_1, f_2, \dots, f_d \begin{equation*}
    f \equiv f_d \circ f_{d-1} \circ \dots \circ f_1
\end{equation*} f^i i \in \{0, 1, \dots, d\} f_0(M) := M \begin{equation*}
    f^i := f_i \circ f_{i - 1} \circ \dots \circ f_1 \circ f_0
\end{equation*} f_0 \nabla g(f(M)) f^i i \in \{0, 1, \dots, d\} M \in \mathbb{R}^{m \times n} \begin{equation*}
    \begin{aligned}
        \nabla (g \circ f)(M)
         & = \nabla g(f(M)) \cdot \nabla f^d(M)                                                                   \\
         & = \nabla g(f(M)) \cdot \nabla f_d(f^{d-1}(M)) \cdot \nabla f^{d-1}(M)                                  \\
         & = \nabla g(f(M)) \cdot \nabla f_d(f^{d-1}(M)) \cdot \nabla f_{d-1}(f^{d-2}(M)) \cdot \nabla f^{d-2}(M) \\
         & \;\;\vdots                                                                                       \\
         & = \nabla g(f(M)) \cdot \prod_{i=1}^{d-1} \left(\nabla f_{d-i+1}(f^{d-i}(M))\right) \cdot \nabla f^1(M) \\
         &= \underbrace{\nabla g(f(M))}_{\text{known}} \cdot \prod_{i=1}^{d} \nabla f_{d-i+1}(\underbrace{f^{d-i}(M)}_{\text{known}})
    \end{aligned}
\end{equation*} \nabla f_i(f^{i-1}(M)) k \prod_{i=1}^k \nabla f_{d-i+1}(f^{d-i}(M))","['derivatives', 'numerical-methods', 'gradient-descent', 'neural-networks']"
83,Ito formula to asian option,Ito formula to asian option,,"I was reading ""2001The Integral of Geometric Brownian Motion"" am getting confused whether $A_t^{(\mu)}=\int_0^t e^{2 \mu \tau + 2 B_\tau} d\tau$ is a function of $t$ only or a function of $t$ and $B_t$ , where $B_t$ is a standard Brownian motion. If the latter is the case, what is partial derivative of $A_t^{(\mu)}$ with respect to $B_t$ ? I suppose the author consider this as a function of $t$ only based on Eq.(2.7) and Eq.(2.8). My understanding is that stochastic differential equation only makes sense in the integral form. Since $A_0^{(\mu)}=0$ , we have $$dA_t^{(\mu)}=e^{2 \mu t + 2 B_t} dt=e^{2 \mu t + 2 B_t} dt+0 dB_t.$$ Can I interpret this as $$\frac{\partial^n A_t^{(\mu)}}{\partial B_t^n}=0$$ for $n \in \mathbb{N}$ ? However, from the second mean value theorem: if $f$ is integrable on $[a,b]$ and $g$ is monotone, then there exists $\eta \in [a,b]$ such that $$\int_a^bf(x)g(x)dx=g(a)\int_a^\eta f(x)dx+g(b)\int_\eta^bf(x)dx.$$ It reminds me that the integral could be represented as a function of integrand at $b$ for some $g$ . Do we have other results for g not monotone? Looking for your help.","I was reading ""2001The Integral of Geometric Brownian Motion"" am getting confused whether is a function of only or a function of and , where is a standard Brownian motion. If the latter is the case, what is partial derivative of with respect to ? I suppose the author consider this as a function of only based on Eq.(2.7) and Eq.(2.8). My understanding is that stochastic differential equation only makes sense in the integral form. Since , we have Can I interpret this as for ? However, from the second mean value theorem: if is integrable on and is monotone, then there exists such that It reminds me that the integral could be represented as a function of integrand at for some . Do we have other results for g not monotone? Looking for your help.","A_t^{(\mu)}=\int_0^t e^{2 \mu \tau + 2 B_\tau} d\tau t t B_t B_t A_t^{(\mu)} B_t t A_0^{(\mu)}=0 dA_t^{(\mu)}=e^{2 \mu t + 2 B_t} dt=e^{2 \mu t + 2 B_t} dt+0 dB_t. \frac{\partial^n A_t^{(\mu)}}{\partial B_t^n}=0 n \in \mathbb{N} f [a,b] g \eta \in [a,b] \int_a^bf(x)g(x)dx=g(a)\int_a^\eta f(x)dx+g(b)\int_\eta^bf(x)dx. b g","['derivatives', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-pde']"
84,Calculating a limit around a differentiable point,Calculating a limit around a differentiable point,,"Here is the question: With the knowledge that $f$ has derivative in point $a$ and $k>h>0$ , show that this limit does not necessarily exist: \begin{align*} lim_{k,h \rightarrow 0^+} \frac{f(a+k) - f(a+h)}{k-h} \end{align*} Up until this point I tried functions like $f(x) = e^x$ and polynomial functions. which only made me more suspicious about the correctness of problem. Though my closest try was $f(x) = \sqrt{x}$ at point $0$ which almost works but fails to be differentiable at point $0$ . I have also tried to approach $k,h$ in a special way for example the substitution $h = k^n$ for sufficiently large $n$ makes things a little easier but at the end I failed to find a definite answer. Now I dont know if the claim is true or not (it may not be and the limit may always exist) but if so, a nice counter would be very appreciated. Thanks.","Here is the question: With the knowledge that has derivative in point and , show that this limit does not necessarily exist: Up until this point I tried functions like and polynomial functions. which only made me more suspicious about the correctness of problem. Though my closest try was at point which almost works but fails to be differentiable at point . I have also tried to approach in a special way for example the substitution for sufficiently large makes things a little easier but at the end I failed to find a definite answer. Now I dont know if the claim is true or not (it may not be and the limit may always exist) but if so, a nice counter would be very appreciated. Thanks.","f a k>h>0 \begin{align*}
lim_{k,h \rightarrow 0^+} \frac{f(a+k) - f(a+h)}{k-h}
\end{align*} f(x) = e^x f(x) = \sqrt{x} 0 0 k,h h = k^n n","['calculus', 'limits', 'derivatives', 'examples-counterexamples']"
85,Why does the definition of two variable differentiability include the partial derivatives of x and y?,Why does the definition of two variable differentiability include the partial derivatives of x and y?,,"During calculus I was introduced to the definition of multi variable differentiability, which is defined as: $\lim_{(h,k)\to(0,0)}\frac{f(a+h, b+k)-f(a,b)-h\cdot f_x(a,b)-k\cdot f_y(a,b)}{\sqrt{h^2+k^2}}=0$ Now I don't seem to understand why the partial derivatives of x and y are inside the above definition. Let's says I would like to prove that a 2 variable function is differentiable at point (a,b), then it seems logical to state that differentiability means (this actually follows from the single variable definition): $\lim_{(h,k)\to(0,0)}\frac{f(a+h, b+k)-f(a,b)}{\sqrt{h^2+k^2}}=L_{h,k}$ Replacing $h$ and $k$ respectfully with $h=r\cos(\theta)$ and $k=r\sin(\theta)$ we acquire: $\lim_{r\to0}\frac{f(a+r\cos\theta, b+r\sin\theta)-f(a,b)}{r}=L_{r,\theta}$ So if function $f$ is differentiable then for each angle $ 0 \le\theta\le360$ in degrees a corresponding limit $L$ should exist and therefore $f$ is differentiable if all the limits holds for very small $r$ and all angles. In fact, with this definition we also test whether the partial derivatives of x and y exist. Hence I don't comprehend why the definition of two variable differentiability must include the partials x and y, it simply doesn't make sense. All it seems to be proving is that any linearization around point (a,b) can be expressed as a combination of linearizations of $f_x$ and $f_y$ . Thanks in advance. Jelle","During calculus I was introduced to the definition of multi variable differentiability, which is defined as: Now I don't seem to understand why the partial derivatives of x and y are inside the above definition. Let's says I would like to prove that a 2 variable function is differentiable at point (a,b), then it seems logical to state that differentiability means (this actually follows from the single variable definition): Replacing and respectfully with and we acquire: So if function is differentiable then for each angle in degrees a corresponding limit should exist and therefore is differentiable if all the limits holds for very small and all angles. In fact, with this definition we also test whether the partial derivatives of x and y exist. Hence I don't comprehend why the definition of two variable differentiability must include the partials x and y, it simply doesn't make sense. All it seems to be proving is that any linearization around point (a,b) can be expressed as a combination of linearizations of and . Thanks in advance. Jelle","\lim_{(h,k)\to(0,0)}\frac{f(a+h, b+k)-f(a,b)-h\cdot f_x(a,b)-k\cdot f_y(a,b)}{\sqrt{h^2+k^2}}=0 \lim_{(h,k)\to(0,0)}\frac{f(a+h, b+k)-f(a,b)}{\sqrt{h^2+k^2}}=L_{h,k} h k h=r\cos(\theta) k=r\sin(\theta) \lim_{r\to0}\frac{f(a+r\cos\theta, b+r\sin\theta)-f(a,b)}{r}=L_{r,\theta} f  0 \le\theta\le360 L f r f_x f_y","['limits', 'multivariable-calculus', 'derivatives']"
86,What is the error in my work for this related rates problem?,What is the error in my work for this related rates problem?,,"I have no clue why my math isn't working out and it is very frustrating. I feel as if I have the concepts correct; however, I just keep getting the wrong answer. Help?","I have no clue why my math isn't working out and it is very frustrating. I feel as if I have the concepts correct; however, I just keep getting the wrong answer. Help?",,"['calculus', 'derivatives', 'related-rates']"
87,Functional derivative of the sum of a product,Functional derivative of the sum of a product,,"Suppose I have a product like the following: $$P(A)=\sum_{i_1=0}^{\infty}\ldots\sum_{i_n=0}^{\infty}c_{i_1}\ldots c_{i_k} A(x^1_{1})\ldots A(x^1_{i_1})\ldots A(x^n_1)\ldots A(x_{i_n}^n)\\ =c_{0}^n+c_{1}c_0^{n-1}\sum_{k=1}^nA(x_1^k) +c_2c_0^{n-1}\sum_{k=1}^n A(x^k_1)A(x^k_2)+ c_1^2 c_{0}^{n-2}\sum_{i\neq j=1}^n A(x^i_1)A(x^j_1)+\ldots$$ where I have written explicitly the first three terms (all $i_k=0$ or one $i_k=1$ with all other zero and one $i_k=2$ with all other zero or two $i_a=i_b=1$ with all other zero). Now suppose I want to write  the $m$ -th functional derivative $\frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_m)}P(A)\rvert_{A=0}$ calculated in zero. The derivative is defined as: $$\frac{\delta}{\delta A(y_1)} A(x) = \delta(x-y_1)$$ I know that a product $$\frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_n)}\prod_{i=1}^{n}A(x_i)=\sum_{\sigma\in P_n}\prod_{i=1}^n\delta(x_{\sigma_i}-y_{i})$$ where $\sigma$ are permutations of the indices $P_n = \{1,\ldots,n\}$ and I can write $P(A)$ as: $$P(A)=\sum_{i_1=0}^{\infty}\ldots\sum_{i_n=0}^{\infty}\prod_{j=1}^{n}c_{i_j}\prod_{k=1}^{i_j}A(x_k^j)$$ However, now I don't know how to write $\frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_m)}P(A)\rvert_{A=0}$","Suppose I have a product like the following: where I have written explicitly the first three terms (all or one with all other zero and one with all other zero or two with all other zero). Now suppose I want to write  the -th functional derivative calculated in zero. The derivative is defined as: I know that a product where are permutations of the indices and I can write as: However, now I don't know how to write","P(A)=\sum_{i_1=0}^{\infty}\ldots\sum_{i_n=0}^{\infty}c_{i_1}\ldots c_{i_k} A(x^1_{1})\ldots A(x^1_{i_1})\ldots A(x^n_1)\ldots A(x_{i_n}^n)\\
=c_{0}^n+c_{1}c_0^{n-1}\sum_{k=1}^nA(x_1^k) +c_2c_0^{n-1}\sum_{k=1}^n A(x^k_1)A(x^k_2)+ c_1^2 c_{0}^{n-2}\sum_{i\neq j=1}^n A(x^i_1)A(x^j_1)+\ldots i_k=0 i_k=1 i_k=2 i_a=i_b=1 m \frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_m)}P(A)\rvert_{A=0} \frac{\delta}{\delta A(y_1)} A(x) = \delta(x-y_1) \frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_n)}\prod_{i=1}^{n}A(x_i)=\sum_{\sigma\in P_n}\prod_{i=1}^n\delta(x_{\sigma_i}-y_{i}) \sigma P_n = \{1,\ldots,n\} P(A) P(A)=\sum_{i_1=0}^{\infty}\ldots\sum_{i_n=0}^{\infty}\prod_{j=1}^{n}c_{i_j}\prod_{k=1}^{i_j}A(x_k^j) \frac{\delta}{\delta A(y_1)}\ldots \frac{\delta}{\delta A(y_m)}P(A)\rvert_{A=0}","['real-analysis', 'combinatorics', 'functional-analysis', 'derivatives', 'permutations']"
88,A proof of a criterion of vanishing of a function.,A proof of a criterion of vanishing of a function.,,"Let $f\in C^{\infty}[-1,1]$ , and there exists a constant $M>0$ s.t $$|f^{(j)}(x)|\le M \forall j \in \mathbb{Z}\forall x\in [-1,1]$$ Prove that if $f(1/k)=0$ for each $k\in \mathbb{N}$ then $f=0$ . I am thinking of using a Taylor expansion, I need somehow to show that all the coefficients vanish. So if I take $f(1/k)=0$ I take $k\to \infty$ then $f(0)=0$ but how to do the rest of the coefficients get to vanish? I don't see it.","Let , and there exists a constant s.t Prove that if for each then . I am thinking of using a Taylor expansion, I need somehow to show that all the coefficients vanish. So if I take I take then but how to do the rest of the coefficients get to vanish? I don't see it.","f\in C^{\infty}[-1,1] M>0 |f^{(j)}(x)|\le M \forall j \in \mathbb{Z}\forall x\in [-1,1] f(1/k)=0 k\in \mathbb{N} f=0 f(1/k)=0 k\to \infty f(0)=0",['calculus']
89,Showing that $\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0$,Showing that,\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0,"If $\alpha ,\beta\in\mathbb{R}^+$ , show that $$\displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0$$ My attempt: $\displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}= \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha$ . Since the function $y=z^\alpha$ is continuous, then $$ \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha=\left[\displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha$$ Let $\eta=\beta/\alpha$ . Applying L'Hôpital's rule $\displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\eta}}=\displaystyle\lim_{x\to+\infty}\frac{\frac{1}{x}}{\eta x^{\eta -1}}=\frac{1}{\eta}\displaystyle\lim_{x\to+\infty}\frac{1}{x^{\eta}}=\frac{1}{\eta}*0=0$ Is my solution correct?","If , show that My attempt: . Since the function is continuous, then Let . Applying L'Hôpital's rule Is my solution correct?","\alpha ,\beta\in\mathbb{R}^+ \displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0 \displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}= \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha y=z^\alpha  \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha=\left[\displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha \eta=\beta/\alpha \displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\eta}}=\displaystyle\lim_{x\to+\infty}\frac{\frac{1}{x}}{\eta x^{\eta -1}}=\frac{1}{\eta}\displaystyle\lim_{x\to+\infty}\frac{1}{x^{\eta}}=\frac{1}{\eta}*0=0","['real-analysis', 'calculus', 'limits', 'derivatives', 'solution-verification']"
90,Existence of an asymptote for $g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right)$,Existence of an asymptote for,g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right),"Working with the Slater's inequality (companion of Jensen's inequality) I find this statement : Let $f(x)$ be a continuous, $n$ times differentiable ,convex and non constant on $(0,\infty)$ and increasing on $(1,\infty)$ and finally with  non bounded derivatives then define : $$g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right)$$ With $g(x)$ strictly increasing on $(1,\infty)$ . Claim: $\lim_{x\to\infty}\frac{g(x)}{x}=constant$ I have not the key to approach the general case but let me try some example : For my first example I take the exponential see here .The function can be decreasing or increasing on a such interval see as example the function $f(x)=x^x$ .As particular case we have $f(x)=x+\frac{1}{x}$ this asymptote is constant.So in fact I have tried only elementary function and their compositions but it would be curious that it works only for them. My question : Have you counter-example (it would be perfect because I have some doubt on this statement) or a proof (which I think is not easy) ? Thanks in advance !","Working with the Slater's inequality (companion of Jensen's inequality) I find this statement : Let be a continuous, times differentiable ,convex and non constant on and increasing on and finally with  non bounded derivatives then define : With strictly increasing on . Claim: I have not the key to approach the general case but let me try some example : For my first example I take the exponential see here .The function can be decreasing or increasing on a such interval see as example the function .As particular case we have this asymptote is constant.So in fact I have tried only elementary function and their compositions but it would be curious that it works only for them. My question : Have you counter-example (it would be perfect because I have some doubt on this statement) or a proof (which I think is not easy) ? Thanks in advance !","f(x) n (0,\infty) (1,\infty) g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right) g(x) (1,\infty) \lim_{x\to\infty}\frac{g(x)}{x}=constant f(x)=x^x f(x)=x+\frac{1}{x}","['derivatives', 'asymptotics', 'convex-analysis', 'examples-counterexamples', 'jensen-inequality']"
91,Differentiation in fields with norm or metric,Differentiation in fields with norm or metric,,"We start learning about derivatives in the real numbers setting. And we know that the reals are the only complete ordered field possible. But what happens if we relax the ""order"" property? The definition f'(x) = lim (f(x+h) - f(x))/h, h -> 0, still holds, if we introduce a metric (or norm), so that the limit makes sense. So what is the general theory for this setting, i.e., calculus in metric, or normed, fields? I have seen various articles even for topological fields, but is there a good textbook survey of the various approaches and applications? And is this topic an active area of current research?","We start learning about derivatives in the real numbers setting. And we know that the reals are the only complete ordered field possible. But what happens if we relax the ""order"" property? The definition f'(x) = lim (f(x+h) - f(x))/h, h -> 0, still holds, if we introduce a metric (or norm), so that the limit makes sense. So what is the general theory for this setting, i.e., calculus in metric, or normed, fields? I have seen various articles even for topological fields, but is there a good textbook survey of the various approaches and applications? And is this topic an active area of current research?",,"['derivatives', 'reference-request']"
92,Properties of a function without partial derivatives,Properties of a function without partial derivatives,,"Let $f(x,y)$ be a function such that $f(sx, sy) = s f(x,y)$ for some positive real $s$ . Let $f_x(y)$ and $g_y(x) = f(x,y)$ be single variable analogues of $f$ , and suppose $f_x(y)$ and $g_y(x)$ are continuously differentiable. I'd like to show that $f(x,y) =  y f '_y(x) + x g'_y(x)$ for all positive $x,y$ . Using partial derivatives this isn't too hard. I'm wondering whether there is a solution that does not use any multivariable calculus, as it doesn't seem necessary for this problem.","Let be a function such that for some positive real . Let and be single variable analogues of , and suppose and are continuously differentiable. I'd like to show that for all positive . Using partial derivatives this isn't too hard. I'm wondering whether there is a solution that does not use any multivariable calculus, as it doesn't seem necessary for this problem.","f(x,y) f(sx, sy) = s f(x,y) s f_x(y) g_y(x) = f(x,y) f f_x(y) g_y(x) f(x,y) =  y f '_y(x) + x g'_y(x) x,y","['calculus', 'multivariable-calculus', 'derivatives']"
93,"If $Q(v)=\left(D^{2} f(a) v, v\right)$ is indefinite form, then prove that $f$ has a saddle point at $a$.","If  is indefinite form, then prove that  has a saddle point at .","Q(v)=\left(D^{2} f(a) v, v\right) f a","It is asked that $Q(v)=\left(D^{2} f(a) v, v\right)$ is indefinite form then I have to show that $f$ has a saddle point at $a$ . I am having difficulty in understanding the problem. What does $Q(v)$ represents? I think the bracket there is for inner product. And what is meant by the indefinite form here?",It is asked that is indefinite form then I have to show that has a saddle point at . I am having difficulty in understanding the problem. What does represents? I think the bracket there is for inner product. And what is meant by the indefinite form here?,"Q(v)=\left(D^{2} f(a) v, v\right) f a Q(v)","['real-analysis', 'multivariable-calculus', 'derivatives']"
94,Proving there is a constant $C$ such that $f=g+C$,Proving there is a constant  such that,C f=g+C,"Consider the following lemma: Lemma: Let $f:[a,b]\to\mathbb{R}$ continuous on $[a,b]$ and differentiable on $(a,b)$ , If $f'(x)=0$ for all $x\in [a,b]$ , then $f$ is constant on $[a,b]$ . I have to prove the following corollary: Corollary: If $f$ and $g$ are continuous functions on $[a,b]$ , differentiable on $(a,b)$ , and $f'(x)=g'(x)$ for all $x\in(a,b)$ , then there exists a constant $C$ such that $f=g+C$ . My attempt: Consider the function $h(x)=f(x)-g(x)$ . We have that $h'(x)=f'(x)-g'(x)=0$ for all $x\in(a,b)$ . By the lemma we have that $h$ is constant on $[a,b]$ . In other words, $h(x)=C$ , where $C\in\mathbb{R}$ is a constant. Then, $C=f(x)-g(x)$ in $[a,b]$ . We conclude that $C+g(x)=f(x)$ . Is this proof correct?","Consider the following lemma: Lemma: Let continuous on and differentiable on , If for all , then is constant on . I have to prove the following corollary: Corollary: If and are continuous functions on , differentiable on , and for all , then there exists a constant such that . My attempt: Consider the function . We have that for all . By the lemma we have that is constant on . In other words, , where is a constant. Then, in . We conclude that . Is this proof correct?","f:[a,b]\to\mathbb{R} [a,b] (a,b) f'(x)=0 x\in [a,b] f [a,b] f g [a,b] (a,b) f'(x)=g'(x) x\in(a,b) C f=g+C h(x)=f(x)-g(x) h'(x)=f'(x)-g'(x)=0 x\in(a,b) h [a,b] h(x)=C C\in\mathbb{R} C=f(x)-g(x) [a,b] C+g(x)=f(x)","['real-analysis', 'calculus', 'derivatives', 'solution-verification']"
95,Integrating a step function using antiderivatives,Integrating a step function using antiderivatives,,"Let $$ f(x) =    \begin{cases}\begin{align}     1\quad&\text{ if }\; 0\leq x \leq 1 \\     2 \quad&\text{ if }\; 1<x \leq 2 \\   \end{align}\end{cases}$$ Then $\int^2_0 f(x)dx=3$ and an anti derivative of f(x) is $$F(x)=\begin{cases}\begin{align}     x\quad&\text{ if }\; 0\leq x \leq 1 \\     2x \quad&\text{ if }\; 1<x \leq 2 \\   \end{align}\end{cases}$$ But, $F(2)-F(0)=4-0=4 \neq 3.$ Why has this happened? Find an anti-derivative $G(x)$ of $f(x)$ such that $G(2)-G(0)=3$, the correct answer. My attempt: $F(2)-F(0)=4 \neq 3$ because we need to break it up into two parts like this: $\int^2_1 2x dx + \int^1_0 x dx = x^2|^2_0 + \frac{1}{2} x^2|^1_0 =4-1+\frac{1}{2}-0=3.5$ but this is not right either. :( For the ""Find an anti-derivative $G(x)$...""I'm completely lost but perhaps if we solve the first part then I'll understand what it's asking for there. Any help would be appreciated.","Let $$ f(x) =    \begin{cases}\begin{align}     1\quad&\text{ if }\; 0\leq x \leq 1 \\     2 \quad&\text{ if }\; 1<x \leq 2 \\   \end{align}\end{cases}$$ Then $\int^2_0 f(x)dx=3$ and an anti derivative of f(x) is $$F(x)=\begin{cases}\begin{align}     x\quad&\text{ if }\; 0\leq x \leq 1 \\     2x \quad&\text{ if }\; 1<x \leq 2 \\   \end{align}\end{cases}$$ But, $F(2)-F(0)=4-0=4 \neq 3.$ Why has this happened? Find an anti-derivative $G(x)$ of $f(x)$ such that $G(2)-G(0)=3$, the correct answer. My attempt: $F(2)-F(0)=4 \neq 3$ because we need to break it up into two parts like this: $\int^2_1 2x dx + \int^1_0 x dx = x^2|^2_0 + \frac{1}{2} x^2|^1_0 =4-1+\frac{1}{2}-0=3.5$ but this is not right either. :( For the ""Find an anti-derivative $G(x)$...""I'm completely lost but perhaps if we solve the first part then I'll understand what it's asking for there. Any help would be appreciated.",,['integration']
96,Prove this refinement of Nesbitt's inequality based on another,Prove this refinement of Nesbitt's inequality based on another,,"Let $a,b,c\in[1,2]$ such that $a,b$ are constants then prove  : $$f(c)=\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{b+a}\geq h(c)=(c-1)\frac{g(2)-g(1)}{2-1}+g(1)\geq g(c)=\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2}{ab+bc+ca}}$$ Yes it's a probable refinement of HN_NH's inequality/ Stronger than Nesbitt inequality My refinement is based on two observations : The function $g(c)$ is convex on $[1,2]$ 2.The chord of a convex function is greater than the convex function. To know if the LHS is good I have tried derivatives . It gives a quartic and it's very ugly so I can say that I have not the solution for the LHS but it seems to be true (numerical check). Update 12/11/2020: The function : $$p(c)=f(c)-h(c)$$ Is convex on $[1,2]$ so there is a possibility to use Jensen's inequality but now I don't see any good issue . Using Jensen's inequality we have : $$p(c)+p(1)\geq 2p\left(\frac{1+c}{2}\right)$$ And : $$p\left(\frac{1+c}{2}\right)+p(1)\geq 2p\left(\frac{3+c}{4}\right)$$ And : $$p\left(\frac{3+c}{4}\right)+p(1)\geq 2p\left(\frac{3+c}{8}+\frac{1}{2}\right)$$ And so on...Playing with these inequalities we got the result I think ! Have you an idea to show the LHS(or confirm my update)? Thanks in advance Max.",Let such that are constants then prove  : Yes it's a probable refinement of HN_NH's inequality/ Stronger than Nesbitt inequality My refinement is based on two observations : The function is convex on 2.The chord of a convex function is greater than the convex function. To know if the LHS is good I have tried derivatives . It gives a quartic and it's very ugly so I can say that I have not the solution for the LHS but it seems to be true (numerical check). Update 12/11/2020: The function : Is convex on so there is a possibility to use Jensen's inequality but now I don't see any good issue . Using Jensen's inequality we have : And : And : And so on...Playing with these inequalities we got the result I think ! Have you an idea to show the LHS(or confirm my update)? Thanks in advance Max.,"a,b,c\in[1,2] a,b f(c)=\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{b+a}\geq h(c)=(c-1)\frac{g(2)-g(1)}{2-1}+g(1)\geq g(c)=\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2}{ab+bc+ca}} g(c) [1,2] p(c)=f(c)-h(c) [1,2] p(c)+p(1)\geq 2p\left(\frac{1+c}{2}\right) p\left(\frac{1+c}{2}\right)+p(1)\geq 2p\left(\frac{3+c}{4}\right) p\left(\frac{3+c}{4}\right)+p(1)\geq 2p\left(\frac{3+c}{8}+\frac{1}{2}\right)","['functions', 'derivatives', 'inequality', 'jensen-inequality', 'convexity-inequality']"
97,Function whose derivative is not continuous,Function whose derivative is not continuous,,"Let $f$ be a differtiable  function on $[a,b]$ and $f'(x)$ is not continous  at some $C$ in $(a,b)$ , then Either left limit or right limit of $f'(x)$ at $C$ does not exist???? True or false I know that $f'(x)$ don't have jump Discontinuities.So I think it is true.but not sure","Let be a differtiable  function on and is not continous  at some in , then Either left limit or right limit of at does not exist???? True or false I know that don't have jump Discontinuities.So I think it is true.but not sure","f [a,b] f'(x) C (a,b) f'(x) C f'(x)","['real-analysis', 'derivatives']"
98,"Can the Stolz-Cesàro theorem be used to prove L'Hospital's rule, or viceversa?","Can the Stolz-Cesàro theorem be used to prove L'Hospital's rule, or viceversa?",,Stolz-Cesàro theorem is usually stated to be the discrete version of L'Hospital's rule. I was merely wondering whether one of these theorems could be used to prove the other (I couldn't find any proof that does this online).,Stolz-Cesàro theorem is usually stated to be the discrete version of L'Hospital's rule. I was merely wondering whether one of these theorems could be used to prove the other (I couldn't find any proof that does this online).,,"['sequences-and-series', 'limits', 'derivatives', 'proof-writing', 'alternative-proof']"
99,Question regarding a proof of the chain rule.,Question regarding a proof of the chain rule.,,"One can derive $(5)$ by letting $$v(s)=\frac{g(s)-g(y)}{s-y}-g'(y)$$ for $s\neq y$ . The problem I have is in letting $s=f(t)$ . It is not obvious to me that the expression still holds, particularly because $s-y=f(t)-y$ might equal $0$ and there is no guarantee that the expression is well defined around $y=f(x)$ .","One can derive by letting for . The problem I have is in letting . It is not obvious to me that the expression still holds, particularly because might equal and there is no guarantee that the expression is well defined around .",(5) v(s)=\frac{g(s)-g(y)}{s-y}-g'(y) s\neq y s=f(t) s-y=f(t)-y 0 y=f(x),"['real-analysis', 'derivatives', 'proof-explanation', 'chain-rule']"
