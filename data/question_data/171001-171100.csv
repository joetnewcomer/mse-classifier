,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to proof this lemma using Hoeffding Inequality?,How to proof this lemma using Hoeffding Inequality?,,"This question follows from a previously asked question . I am trying to understand the proof of Lemma 2.1 in the paper ""A Universal Law of Robustness via isoperimetry"" by Bubeck and Sellke. We start with a lemma showing that, to optimize heyond the noise level one must necessarily correlate with the noise part of the labels. In what follows we denote $g(x)=\mathbb{E}[y \mid x]$ for the target function, and $z_{i}=y_{t}-g\left(x_{i}\right)$ for the noise part of the observed labels (namely $y_{1}$ is the sum of the target funetion $g\left(x_{i}\right)$ and the noise term $z_{1}$ ). Lemma 2.1. One has $$ \mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n}\left(y_{t}-f\left(x_{i}\right)\right)^{2} \leq \sigma^{2}-\epsilon\right) \leq 2 \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)+\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right) z_{i} \geq \frac{\epsilon}{4}\right).......(A^{1}) $$ Proof. The sequence $\left(z_{1}^{2}\right)$ is i.i.d., with mean $\sigma^{2}$ , and such that $\left|z_{i}\right|^{2} \leq 4$ . Thus Hoeffding's inequality yielde: $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)......................(1) $$ On the other hand the sequence $\left(z_{1} g\left(x_{1}\right)\right)$ is i.i.d., with mean $0\left(\right.$ since $\left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right)$ , and such that $\left|z_{1} g\left(x_{1}\right)\right| \leq 2$ . Thus Hoeffding's inequality yields: $$ \mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{i} g\left(x_{i}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)$$ I am typically got stuck in the 1st step of proof of this lemma. I have read about Hoeffding's inequality ,but unable to sync. For reference Theorem $12.3$ (Hoeffding inequality) Given independent $\left(X_{1}, \ldots, X_{n}\right)$ with $X_{i} \in\left[a_{i}, b_{i}\right]$ a.s., $$ \operatorname{Pr}\left[\frac{1}{n} \sum_{i}\left(X_{i}-\mathbb{E} X_{i}\right) \geq \epsilon\right] \leq \exp \left(-\frac{2 n^{2} \epsilon^{2}}{\sum_{i}\left(b_{i}-a_{i}\right)^{2}}\right) $$ Edit 1 Theorem 4 (Hoeffding's inequality). Let $Z_{1}, \ldots, Z_{n}$ be independent bounded random variables with $Z_{i} \in[a, b]$ for all $i$ , where $-\infty<a \leq b<\infty$ . Then $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \geq t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ and $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ for all $t \geq 0$ . Using the 2nd one I am able to reach here , $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(z_{i}^2-\mathbb{E}\left[z_{i}^2\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2-\frac{1}{n} \sum_{i=1}^{n}\mathbb{E}\left[z_{i}^2]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ as $$\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[z_{i}^2] = \sigma^2(mean) $$ Replacing the value of it $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2- \sigma^2 \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right) $$ Thus comparing with Eq $1$ , I am getting the  value of $t$ as $\epsilon/6$ . And it is given $\left|z_{i}\right|^{2} \leq 4$ . So We can rewrite it as $ -4 \leq z_{i} ^{2} \leq 4$ . So $a$ = -4 and $b$ = 4 Plugging it into RHS , $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n \epsilon^{2}}{(4+4)^{2}*36}\right) $$ $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(4+4)^{2}*18}\right) $$ $$ \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(8)^{2}*18}\right) $$ But RHS is not matching with $Eq 1$ . I am unable to figure it out , where am I wrong. Can anyone help me out? (Solved) Edit 2 $$ \mathbf{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right).....................(2.1) $$ On the other hand the sequence $\left(z_{1} g\left(x_{1}\right)\right)$ is i.i.d., with mean $0\left(\right.$ since $\left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right)$ , and such that $\left|z_{1} g\left(x_{1}\right)\right| \leq 2$ . Thus Hoeffding's inequality yields: $$ \mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{1} g\left(x_{1}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)...........(2.2) $$ Let us write $Z=\frac{1}{\sqrt{n}}\left(z_{1}, \ldots, z_{n}\right), G=\frac{1}{\sqrt{n}}\left(g\left(x_{1}\right), \ldots, g\left(x_{n}\right)\right)$ , and $F=\frac{1}{\sqrt{n}}\left(f\left(x_{1}\right), \ldots, f\left(x_{n}\right)\right)$ . We claim that if $\|Z\|^{2} \geq \sigma^{2}-\frac{\epsilon}{6}$ and $\langle Z, G\rangle \geq-\frac{\epsilon}{6}$ , then for any $f \in \mathcal{F}$ one has $$ \|G+Z-F\|^{2} \leq \sigma^{2}-\epsilon \Rightarrow\langle F, Z\rangle \geq \frac{\epsilon}{4}......(2.3) $$ This claim together with $(2.1)$ and $(2.2$ ) conclude the proof of $A^{1}$ How?? And how they have written the eq ( $2.3$ ) I have clearly understood Eqn 2.1 and 2.2 but after that how they conclude the proof I am not getting, Kindly help me to understand","This question follows from a previously asked question . I am trying to understand the proof of Lemma 2.1 in the paper ""A Universal Law of Robustness via isoperimetry"" by Bubeck and Sellke. We start with a lemma showing that, to optimize heyond the noise level one must necessarily correlate with the noise part of the labels. In what follows we denote for the target function, and for the noise part of the observed labels (namely is the sum of the target funetion and the noise term ). Lemma 2.1. One has Proof. The sequence is i.i.d., with mean , and such that . Thus Hoeffding's inequality yielde: On the other hand the sequence is i.i.d., with mean since , and such that . Thus Hoeffding's inequality yields: I am typically got stuck in the 1st step of proof of this lemma. I have read about Hoeffding's inequality ,but unable to sync. For reference Theorem (Hoeffding inequality) Given independent with a.s., Edit 1 Theorem 4 (Hoeffding's inequality). Let be independent bounded random variables with for all , where . Then and for all . Using the 2nd one I am able to reach here , as Replacing the value of it Thus comparing with Eq , I am getting the  value of as . And it is given . So We can rewrite it as . So = -4 and = 4 Plugging it into RHS , But RHS is not matching with . I am unable to figure it out , where am I wrong. Can anyone help me out? (Solved) Edit 2 On the other hand the sequence is i.i.d., with mean since , and such that . Thus Hoeffding's inequality yields: Let us write , and . We claim that if and , then for any one has This claim together with and ) conclude the proof of How?? And how they have written the eq ( ) I have clearly understood Eqn 2.1 and 2.2 but after that how they conclude the proof I am not getting, Kindly help me to understand","g(x)=\mathbb{E}[y \mid x] z_{i}=y_{t}-g\left(x_{i}\right) y_{1} g\left(x_{i}\right) z_{1} 
\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n}\left(y_{t}-f\left(x_{i}\right)\right)^{2} \leq \sigma^{2}-\epsilon\right) \leq 2 \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)+\mathbb{P}\left(\exists f \in \mathcal{F}: \frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right) z_{i} \geq \frac{\epsilon}{4}\right).......(A^{1})
 \left(z_{1}^{2}\right) \sigma^{2} \left|z_{i}\right|^{2} \leq 4 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)......................(1)
 \left(z_{1} g\left(x_{1}\right)\right) 0\left(\right. \left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right) \left|z_{1} g\left(x_{1}\right)\right| \leq 2 
\mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{i} g\left(x_{i}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right) 12.3 \left(X_{1}, \ldots, X_{n}\right) X_{i} \in\left[a_{i}, b_{i}\right] 
\operatorname{Pr}\left[\frac{1}{n} \sum_{i}\left(X_{i}-\mathbb{E} X_{i}\right) \geq \epsilon\right] \leq \exp \left(-\frac{2 n^{2} \epsilon^{2}}{\sum_{i}\left(b_{i}-a_{i}\right)^{2}}\right)
 Z_{1}, \ldots, Z_{n} Z_{i} \in[a, b] i -\infty<a \leq b<\infty 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \geq t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}\left[Z_{i}\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 t \geq 0 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}\left(z_{i}^2-\mathbb{E}\left[z_{i}^2\right]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2-\frac{1}{n} \sum_{i=1}^{n}\mathbb{E}\left[z_{i}^2]\right) \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[z_{i}^2] = \sigma^2(mean)  
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2- \sigma^2 \leq-t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n t^{2}}{(b-a)^{2}}\right)
 1 t \epsilon/6 \left|z_{i}\right|^{2} \leq 4  -4 \leq z_{i} ^{2} \leq 4 a b 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{2 n \epsilon^{2}}{(4+4)^{2}*36}\right)
 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(4+4)^{2}*18}\right)
 
\mathbb{P}\left(\frac{1}{n} \sum_{i=1}^{n}z_{i}^2 \leq \sigma^2 -t\right) \leq \exp \left(-\frac{ n \epsilon^{2}}{(8)^{2}*18}\right)
 Eq 1 
\mathbf{P}\left(\frac{1}{n} \sum_{i=1}^{n} z_{1}^{2} \leq \sigma^{2}-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right).....................(2.1)
 \left(z_{1} g\left(x_{1}\right)\right) 0\left(\right. \left.\mathbb{E}\left[z_{i} \mid x_{\mathrm{f}}\right]=0\right) \left|z_{1} g\left(x_{1}\right)\right| \leq 2 
\mathbb{P}\left(\frac{1}{n} \sum_{i-1}^{n} z_{1} g\left(x_{1}\right) \leq-\frac{\epsilon}{6}\right) \leq \exp \left(-\frac{n \epsilon^{2}}{8^{3}}\right)...........(2.2)
 Z=\frac{1}{\sqrt{n}}\left(z_{1}, \ldots, z_{n}\right), G=\frac{1}{\sqrt{n}}\left(g\left(x_{1}\right), \ldots, g\left(x_{n}\right)\right) F=\frac{1}{\sqrt{n}}\left(f\left(x_{1}\right), \ldots, f\left(x_{n}\right)\right) \|Z\|^{2} \geq \sigma^{2}-\frac{\epsilon}{6} \langle Z, G\rangle \geq-\frac{\epsilon}{6} f \in \mathcal{F} 
\|G+Z-F\|^{2} \leq \sigma^{2}-\epsilon \Rightarrow\langle F, Z\rangle \geq \frac{\epsilon}{4}......(2.3)
 (2.1) (2.2 A^{1} 2.3","['probability-theory', 'statistics']"
1,"Why are mean, median, and mode called central tendency?","Why are mean, median, and mode called central tendency?",,"Is there any difference between the two terms ""Central tendency"" and ""representative values""? Why are mean, median, and mode called central tendency or representative values?","Is there any difference between the two terms ""Central tendency"" and ""representative values""? Why are mean, median, and mode called central tendency or representative values?",,"['statistics', 'terminology', 'means', 'median', 'central-tendency']"
2,Expectation of the product of two mean independent random variables,Expectation of the product of two mean independent random variables,,"I have random variables $a$ and $b$ , which are such that $\mathbb{E}[a|b]=0$ . I am trying to compute $E[ab]$ . Is the following correct? $$\mathbb{E}[ab]=\mathbb{E}[\mathbb{E}[ab|b]]=\mathbb{E}[b\mathbb{E}[a|b]]=0$$ Where I have used the law of iterated expectations. Thank you.","I have random variables and , which are such that . I am trying to compute . Is the following correct? Where I have used the law of iterated expectations. Thank you.",a b \mathbb{E}[a|b]=0 E[ab] \mathbb{E}[ab]=\mathbb{E}[\mathbb{E}[ab|b]]=\mathbb{E}[b\mathbb{E}[a|b]]=0,"['probability', 'statistics', 'solution-verification']"
3,Why are causal inference diagrams so useful or effective?,Why are causal inference diagrams so useful or effective?,,"Is there a short explanation of why Pearl's casual inference diagrams are so highly-regarded, useful or effective? I can't help but think it's just so simple an idea that I can't tell why it could be such a big deal: It's just like a set of equations where functions are composed in an 'acyclic' way? And apparently it's like groundbreaking and there's a whole field of 'causality'?","Is there a short explanation of why Pearl's casual inference diagrams are so highly-regarded, useful or effective? I can't help but think it's just so simple an idea that I can't tell why it could be such a big deal: It's just like a set of equations where functions are composed in an 'acyclic' way? And apparently it's like groundbreaking and there's a whole field of 'causality'?",,"['statistics', 'statistical-inference', 'causality', 'causal-diagrams']"
4,$a_n$-Consistency of the Largest Order Statistic (Jun Shao Example 2.34),-Consistency of the Largest Order Statistic (Jun Shao Example 2.34),a_n,"I am learning about different modes of consistency in statistics. I am having a hard time understanding the concept of $a_n$ - consistency . Here's the definition: Let $\{a_n\}$ be a sequence of positive constants diverging to $\infty$ . $T_n(X)$ is called $a_n$ - consistent for $\vartheta$ iff $a_n[T_n(X) - \vartheta] = O_p(1)$ w.r.t. any $P \in \mathcal{P}$ , where $X = (X_1, \cdots, X_n)$ is a sample from a population $P \in \mathcal{P}$ , $\vartheta$ is the parameter vector associated with the population $P$ , and $T_n(X)$ is a point estimator of $\vartheta$ for every $n$ . In a follow-up example, he showed that the largest order statistics $X_{(n)}$ is $n^{(m+1)^{-1}}$ -consistent when $X_1, \cdots, X_n$ are i.i.d. with a continuous c.d.f. $F$ satisfying $F(\theta) = 1$ for some $\theta \in \mathcal{R}$ , $F(x) < 1$ for any $x < \theta$ , the $i$ -th order left hand derivative of $F$ at $\theta$ exists and vanishes for any $i \leq m$ , and the $m$ -th order left hand derivative of $F$ exists and is non-zero. $m$ is a nonnegative integer. I have two questions about this example: Shao quoted a ""fact"" that $P(n[1-F(X_{(n)})] \geq s) = (1 - s/n)^n$ . However, it doesn't seem very intuitive to me how I can get to this ""fact"". How do I derive this? The second question is, how do I use the ""fact"" given in 1., and the Taylor expansion of $$1 - F(X_{(n)}) = \frac{(-1)^mF^{(m+1)}(\theta-)}{(m+1)!}(\theta - X_{(n)})^{m+1} + o(|\theta - X_{(n)}|^{m+1}) \; a.s.,$$ to get to the conclusion that $(\theta - X_{(n)})^{m+1} = O_p(n^{-1})$ ? The definition $O_p$ is: $X_n = O_p(Y_n)$ iff, for any $\epsilon > 0$ , there is a constant $C_{\epsilon} > 0$ such that $\sup_n P(\|X_n\| \geq C_{\epsilon}|Y_n|) < \epsilon$ , where $X_n$ is a random vector and $Y_n$ is a random variable. Thank you very much. Here is the screenshot of the example:","I am learning about different modes of consistency in statistics. I am having a hard time understanding the concept of - consistency . Here's the definition: Let be a sequence of positive constants diverging to . is called - consistent for iff w.r.t. any , where is a sample from a population , is the parameter vector associated with the population , and is a point estimator of for every . In a follow-up example, he showed that the largest order statistics is -consistent when are i.i.d. with a continuous c.d.f. satisfying for some , for any , the -th order left hand derivative of at exists and vanishes for any , and the -th order left hand derivative of exists and is non-zero. is a nonnegative integer. I have two questions about this example: Shao quoted a ""fact"" that . However, it doesn't seem very intuitive to me how I can get to this ""fact"". How do I derive this? The second question is, how do I use the ""fact"" given in 1., and the Taylor expansion of to get to the conclusion that ? The definition is: iff, for any , there is a constant such that , where is a random vector and is a random variable. Thank you very much. Here is the screenshot of the example:","a_n \{a_n\} \infty T_n(X) a_n \vartheta a_n[T_n(X) - \vartheta] = O_p(1) P \in \mathcal{P} X = (X_1, \cdots, X_n) P \in \mathcal{P} \vartheta P T_n(X) \vartheta n X_{(n)} n^{(m+1)^{-1}} X_1, \cdots, X_n F F(\theta) = 1 \theta \in \mathcal{R} F(x) < 1 x < \theta i F \theta i \leq m m F m P(n[1-F(X_{(n)})] \geq s) = (1 - s/n)^n 1 - F(X_{(n)}) = \frac{(-1)^mF^{(m+1)}(\theta-)}{(m+1)!}(\theta - X_{(n)})^{m+1} + o(|\theta - X_{(n)}|^{m+1}) \; a.s., (\theta - X_{(n)})^{m+1} = O_p(n^{-1}) O_p X_n = O_p(Y_n) \epsilon > 0 C_{\epsilon} > 0 \sup_n P(\|X_n\| \geq C_{\epsilon}|Y_n|) < \epsilon X_n Y_n","['limits', 'probability-theory', 'statistics', 'probability-distributions', 'asymptotics']"
5,Can the average of a set be lower than all of the averages of subsets?,Can the average of a set be lower than all of the averages of subsets?,,Let's imagine there are marbles of different diameter and color. Can the average diameter of all the marbles be lower than all of the average diameter of marbles per color?,Let's imagine there are marbles of different diameter and color. Can the average diameter of all the marbles be lower than all of the average diameter of marbles per color?,,['statistics']
6,Does FOSD + log concavity of $f(x)$ and $g(x)$ imply MLRP?,Does FOSD + log concavity of  and  imply MLRP?,f(x) g(x),"I am looking for a result on the ordering of distribution functions. The probability density functions $f(x)$ and $g(x)$ bear the Monotone Likelihood Ratio Property (MLRP) if $$ \frac{f(x)}{g(x)} $$ is increasing in $x$ . By First Order Stochastic Dominance (FOSD) of the probability distribution function $F(x)$ over $G(x)$ , it follows that $$ F(x) \leq G(x) $$ for all $x$ , with strict inequality for some $x$ . It is well known that the MLRP implies First Order Stochastic Dominance (FOSD) but not vice versa. I am interested in sufficient conditions on $f(x)$ and $g(x)$ that, together with $F(x)$ FOSD $G(x)$ imply the MLRP. It seems to me that assuming that both $f(x)$ and $g(x)$ are log-concave should be sufficient. It holds in the following example: Consider $g(x) = f(x + a)$ , where $a>0$ . I.e., $g$ is a version of $f$ shifted to the left. FOSD obviously holds. In this case, MLRP also holds because $$ \frac{\partial\frac{f(x)}{g(x)}}{\partial x} > 0 \Leftrightarrow \frac{f'(x)}{f(x)} - \frac{g'(x)}{g(x)} > 0\Leftrightarrow \frac{f'(x)}{f(x)} - \frac{f'(x+a)}{f(x+a)} >0.$$ The last condition holds by log-concavity, as $\frac{f'(x)}{f(x)}$ is decreasing in $x$ . I'm not so sure how to generalize from this to other cases where $f$ and $g$ are both log-concave but where $g$ might not be a simple left shift of $f$ . Has anyone got ideas how to show this or approach this problem?","I am looking for a result on the ordering of distribution functions. The probability density functions and bear the Monotone Likelihood Ratio Property (MLRP) if is increasing in . By First Order Stochastic Dominance (FOSD) of the probability distribution function over , it follows that for all , with strict inequality for some . It is well known that the MLRP implies First Order Stochastic Dominance (FOSD) but not vice versa. I am interested in sufficient conditions on and that, together with FOSD imply the MLRP. It seems to me that assuming that both and are log-concave should be sufficient. It holds in the following example: Consider , where . I.e., is a version of shifted to the left. FOSD obviously holds. In this case, MLRP also holds because The last condition holds by log-concavity, as is decreasing in . I'm not so sure how to generalize from this to other cases where and are both log-concave but where might not be a simple left shift of . Has anyone got ideas how to show this or approach this problem?",f(x) g(x)  \frac{f(x)}{g(x)}  x F(x) G(x)  F(x) \leq G(x)  x x f(x) g(x) F(x) G(x) f(x) g(x) g(x) = f(x + a) a>0 g f  \frac{\partial\frac{f(x)}{g(x)}}{\partial x} > 0 \Leftrightarrow \frac{f'(x)}{f(x)} - \frac{g'(x)}{g(x)} > 0\Leftrightarrow \frac{f'(x)}{f(x)} - \frac{f'(x+a)}{f(x+a)} >0. \frac{f'(x)}{f(x)} x f g g f,"['real-analysis', 'probability', 'statistics', 'probability-distributions', 'order-statistics']"
7,Deriving a mixed distribution from exponential and inverse gamma,Deriving a mixed distribution from exponential and inverse gamma,,"Question You are given the following: the amount of an individual loss in the year $2022$ follows an exponential distribution with mean $15000$ Between $2022$ and $2025$ , losses will be multiplied by an inflation factor $C$ . For example, if $C = 1.2$ , then losses in $2025$ will be $20\%$ more than that in $2022$ . You are uncertain of what $C$ will be, but estimate that it will be a random draw from a inverse gamma distribution with parameters $\alpha = 2.4$ and $\delta = 3.2$ . The inverse gamma distribution has pdf $f(x; \alpha, \delta) = \frac {\delta^{\alpha}} {\Gamma(\alpha)} x^{-\alpha + 1} e^{-\frac {\delta} x}$ . Estimate the probability that a loss in $2025$ will exceed $100000$ . My thoughts I know that the mixture of an exponential and an inverse gamma will result in a Pareto distribution. However, I am not sure how to account for $C$ in this case. I mean, if the exponential had a mean of $C$ and $C$ followed an inverse gamma, I would know how to compute the integral. Does the computation of the resulting distribution require me to take into account $C$ , or does it not matter? How should I go about evaluating this? Any intuitive explanations will be greatly appreciated :)","Question You are given the following: the amount of an individual loss in the year follows an exponential distribution with mean Between and , losses will be multiplied by an inflation factor . For example, if , then losses in will be more than that in . You are uncertain of what will be, but estimate that it will be a random draw from a inverse gamma distribution with parameters and . The inverse gamma distribution has pdf . Estimate the probability that a loss in will exceed . My thoughts I know that the mixture of an exponential and an inverse gamma will result in a Pareto distribution. However, I am not sure how to account for in this case. I mean, if the exponential had a mean of and followed an inverse gamma, I would know how to compute the integral. Does the computation of the resulting distribution require me to take into account , or does it not matter? How should I go about evaluating this? Any intuitive explanations will be greatly appreciated :)","2022 15000 2022 2025 C C = 1.2 2025 20\% 2022 C \alpha = 2.4 \delta = 3.2 f(x; \alpha, \delta) = \frac {\delta^{\alpha}} {\Gamma(\alpha)} x^{-\alpha + 1} e^{-\frac {\delta} x} 2025 100000 C C C C","['probability', 'statistics', 'probability-distributions', 'exponential-distribution', 'gamma-distribution']"
8,Optimal Transport and Entropic Regularization,Optimal Transport and Entropic Regularization,,"We are working with discrete optimal transport. Let $P$ be a matrix and let $H(P) =- \sum_{i,j} P_{i,j} (\log(P_{i,j})-1)$ . Let $C$ be the cost matrix. And $\langle C,P\rangle$ the Frobenius inner product. We introduce the regularized optimal transport problem $\min_{P \in U(a,b)} \langle C,P\rangle + \epsilon H(P)$ . We want to prove that as $\epsilon \to 0$ , $P_\epsilon$ converges to an optimal solution to the original Kantrovich problem with maximal entropy. I understand the proof up to the point where it says for any subsequence of $P_\epsilon$ , we can choose a sub-subsequence of it that converges to an optimal transport plan with maximum entropy. Question 1) The part I don't get is when it says by strict convexity of $-H$ , we get $P^* = P_0^*$ . It is clear that $-H$ is strictly convex, but you still need $-H$ on a convex set. It seems we are only looking at optimal points in the Kantrovich problem, which is not a convex set. Question 2) It says that as $\epsilon \to \infty$ , $P$ gets less sparse, but I would have thought the opposite since more entropy means more uncertainties. Thank you!","We are working with discrete optimal transport. Let be a matrix and let . Let be the cost matrix. And the Frobenius inner product. We introduce the regularized optimal transport problem . We want to prove that as , converges to an optimal solution to the original Kantrovich problem with maximal entropy. I understand the proof up to the point where it says for any subsequence of , we can choose a sub-subsequence of it that converges to an optimal transport plan with maximum entropy. Question 1) The part I don't get is when it says by strict convexity of , we get . It is clear that is strictly convex, but you still need on a convex set. It seems we are only looking at optimal points in the Kantrovich problem, which is not a convex set. Question 2) It says that as , gets less sparse, but I would have thought the opposite since more entropy means more uncertainties. Thank you!","P H(P) =- \sum_{i,j} P_{i,j} (\log(P_{i,j})-1) C \langle C,P\rangle \min_{P \in U(a,b)} \langle C,P\rangle + \epsilon H(P) \epsilon \to 0 P_\epsilon P_\epsilon -H P^* = P_0^* -H -H \epsilon \to \infty P","['statistics', 'optimization', 'convex-optimization', 'regularization', 'optimal-transport']"
9,Propagating Uncertainties through The Eigenvalue Problem,Propagating Uncertainties through The Eigenvalue Problem,,"For a physics lab we are studying a hanging coupled-spring oscillator with three masses, $M_i$ , and three springs with spring constants $k_i$ . From Newton's laws, ignoring the gravitational force, and assuming the springs are ideal, we derived the following 3-by-3 matrix to describe the system: $$ \textbf{K}=\begin{bmatrix} -\frac{k_1+k_2}{M_1} & \frac{k_2}{M_1} & 0\\ \frac{k_2}{M_2} & -\frac{k_2+k_3}{M_2} & \frac{k_3}{M_2}\\ 0 & \frac{k_3}{M_3} & -\frac{k_2+k_3}{M_3} \end{bmatrix}. $$ Since the eigenvalues represent the normal mode frequencies (once appropriately multiplying them by some constants) and the eigenvectors represent the relative offset positions for the normal modes, we are interested in finding the eigenvalues and eigenvectors as well as the uncertainty of the eigenvalues and eigenvectors as each $k_i$ has some uncertainty (on a scale of $10^{-2}$ ). Is there a method that will properly evaluate the uncertainties of the eigenvalues and eigenvectors? A method we tried was to randomly select $10,000$ values from each $k_i$ $68\%$ confidence interval and average the eigenvalues and eigenvectors, however, this method failed due to both the large magnitude of the uncertainties and the sensitivity of the eigenvalue problem.","For a physics lab we are studying a hanging coupled-spring oscillator with three masses, , and three springs with spring constants . From Newton's laws, ignoring the gravitational force, and assuming the springs are ideal, we derived the following 3-by-3 matrix to describe the system: Since the eigenvalues represent the normal mode frequencies (once appropriately multiplying them by some constants) and the eigenvectors represent the relative offset positions for the normal modes, we are interested in finding the eigenvalues and eigenvectors as well as the uncertainty of the eigenvalues and eigenvectors as each has some uncertainty (on a scale of ). Is there a method that will properly evaluate the uncertainties of the eigenvalues and eigenvectors? A method we tried was to randomly select values from each confidence interval and average the eigenvalues and eigenvectors, however, this method failed due to both the large magnitude of the uncertainties and the sensitivity of the eigenvalue problem.","M_i k_i 
\textbf{K}=\begin{bmatrix}
-\frac{k_1+k_2}{M_1} & \frac{k_2}{M_1} & 0\\
\frac{k_2}{M_2} & -\frac{k_2+k_3}{M_2} & \frac{k_3}{M_2}\\
0 & \frac{k_3}{M_3} & -\frac{k_2+k_3}{M_3}
\end{bmatrix}.
 k_i 10^{-2} 10,000 k_i 68\%","['statistics', 'eigenvalues-eigenvectors']"
10,marginal probability function for X,marginal probability function for X,,"I know that $f(x,\theta) = f(x|\theta)f(\theta)$ . I then think we can combine the terms from the α and β, with $x$ and $1-x$ but not sure how to proceed. I would need to find the marginal before I can determine $E(X)$ . EDITS: Showing what I have done before: (a) Marginal of X $$f(x,\theta) = f(x|\theta)f(\theta)$$ Since $\pi(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}$ then I did $$f(x,\theta) \propto \theta^x (1-\theta)^{1-x} \theta^{\alpha-1} (1-\theta)^{\beta-1}$$ $$\propto  \theta^{\alpha+x-1} (1-\theta)^{\beta-x} $$ Therefore, $$ f(x) \propto \int_\theta f(x,\theta) $$ $$ \propto \int_0^1 \theta^{\alpha+x-1} (1-\theta)^{\beta-x} $$ $$ \propto \frac {\theta^{\alpha+x} (-1) (1-\theta)^{\beta-x+1}}{\beta-x+1}$$ $$ \propto \frac {1}{\beta-x+1} ,  \  \ \  \ x = 0, 1$$ (b) $E(X)$ by iterated expectation $$E(X) = E[E(X|\theta)]$$ $$  = \int_\theta E(X|\theta) f(\theta) d(\theta)$$ $$  = \int_0^1 \sum_{x=0}^1 x f(x|\theta) f(\theta) d(\theta)$$ $$  = \int_0^1 f(x,\theta)  d(\theta)$$ $$  \propto \int_0^1  \frac {1}{\beta-x+1}  d(\theta) \  \  \  \   \    \   \text{[from (a) above]}$$ $$  \propto   \frac {1}{\beta-x+1} \big[\theta \Big|_0^1 $$ $$  \propto   \frac {1}{\beta-x+1} $$ Does this make sense at all?","I know that . I then think we can combine the terms from the α and β, with and but not sure how to proceed. I would need to find the marginal before I can determine . EDITS: Showing what I have done before: (a) Marginal of X Since then I did Therefore, (b) by iterated expectation Does this make sense at all?","f(x,\theta) = f(x|\theta)f(\theta) x 1-x E(X) f(x,\theta) = f(x|\theta)f(\theta) \pi(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1} f(x,\theta) \propto \theta^x (1-\theta)^{1-x} \theta^{\alpha-1} (1-\theta)^{\beta-1} \propto  \theta^{\alpha+x-1} (1-\theta)^{\beta-x}   f(x) \propto \int_\theta f(x,\theta)   \propto \int_0^1 \theta^{\alpha+x-1} (1-\theta)^{\beta-x}   \propto \frac {\theta^{\alpha+x} (-1) (1-\theta)^{\beta-x+1}}{\beta-x+1}  \propto \frac {1}{\beta-x+1} ,  \  \ \  \ x = 0, 1 E(X) E(X) = E[E(X|\theta)]   = \int_\theta E(X|\theta) f(\theta) d(\theta)   = \int_0^1 \sum_{x=0}^1 x f(x|\theta) f(\theta) d(\theta)   = \int_0^1 f(x,\theta)  d(\theta)   \propto \int_0^1  \frac {1}{\beta-x+1}  d(\theta) \  \  \  \   \ 
  \   \text{[from (a) above]}   \propto   \frac {1}{\beta-x+1} \big[\theta \Big|_0^1    \propto   \frac {1}{\beta-x+1} ","['probability', 'statistics', 'probability-distributions']"
11,The probability that no runs of k consecutive heads OR tails will occur in n coin tosses,The probability that no runs of k consecutive heads OR tails will occur in n coin tosses,,"I am trying to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. After doing some reading, I found this that shows that the probability that no k consecutive tails will occur in n tosses is given by $F^k_{n+2}/2^n$ where $F^k_l$ is a  Fibonacci k-step number. Now this works if I am trying to see the probability that no k consecutive tails or no k consecutive heads occurs. But if I want to know the probability that no k consecutive tails or heads occurs, the problem becomes more complicated, because there are many coin flip combinations that include both k consecutive heads and k consecutive tails at the same time. If I could determine the number of combinations that included both k consecutive heads and k consecutive tails, I could then use it to find the number of combinations that have either k consecutive heads or k consecutive tails and sum all three of those to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. Running this with R, I found that the number of combinations where there were both k consecutive heads and k consecutive heads that occurred was as follows: k n count 3 5 0 3 6 2 3 7 8 3 8 26 3 9 74 3 10 194 3 11 482 3 12 1152 k n count 4 7 0 4 8 2 4 9 8 4 10 26 4 11 76 4 12 206 4 13 530 4 14 1314 k n count 5 9 0 5 10 2 5 11 8 5 12 26 5 13 76 5 14 208 5 15 542 5 16 1362 There seems to be a pattern that starts 2 later for each increased value of k, and has similar values that then stabilize with higher values of k. This seems to me to be related to a Fibonacci k-step number, but I can't see the pattern here. My question is this. Is there a simpler way to calculate the probability that no runs of k consecutive heads or tails will occur in n coin tosses. And if not, is there a way to calculate the number of combinations with k consecutive heads and tails, following the pattern here.","I am trying to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. After doing some reading, I found this that shows that the probability that no k consecutive tails will occur in n tosses is given by where is a  Fibonacci k-step number. Now this works if I am trying to see the probability that no k consecutive tails or no k consecutive heads occurs. But if I want to know the probability that no k consecutive tails or heads occurs, the problem becomes more complicated, because there are many coin flip combinations that include both k consecutive heads and k consecutive tails at the same time. If I could determine the number of combinations that included both k consecutive heads and k consecutive tails, I could then use it to find the number of combinations that have either k consecutive heads or k consecutive tails and sum all three of those to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. Running this with R, I found that the number of combinations where there were both k consecutive heads and k consecutive heads that occurred was as follows: k n count 3 5 0 3 6 2 3 7 8 3 8 26 3 9 74 3 10 194 3 11 482 3 12 1152 k n count 4 7 0 4 8 2 4 9 8 4 10 26 4 11 76 4 12 206 4 13 530 4 14 1314 k n count 5 9 0 5 10 2 5 11 8 5 12 26 5 13 76 5 14 208 5 15 542 5 16 1362 There seems to be a pattern that starts 2 later for each increased value of k, and has similar values that then stabilize with higher values of k. This seems to me to be related to a Fibonacci k-step number, but I can't see the pattern here. My question is this. Is there a simpler way to calculate the probability that no runs of k consecutive heads or tails will occur in n coin tosses. And if not, is there a way to calculate the number of combinations with k consecutive heads and tails, following the pattern here.",F^k_{n+2}/2^n F^k_l,"['probability', 'statistics', 'fibonacci-numbers']"
12,"Are Machine Learning Optimization Problem ever Categorized as ""P"" or ""NP""?","Are Machine Learning Optimization Problem ever Categorized as ""P"" or ""NP""?",,"In the context of Computer Science and Optimization, I have heard that different problems can be classified using the ""P vs NP"" framework . Essentially, there is a hierarchy of problems based on the inherent complexity of the problem itself. For example, a problem like ""multiplying numbers"" is considered as ""P"" and is considered fundamentally easier to solve than a problem like ""solving a sudoku"" which is ""NP"". In most Statistical and Machine Learning Models, there is usually an optimization problem ""nested"" within the model that is required to solve. For example: Regression Models : In a standard regression model, we try to find the value of the ""beta coefficients"" that either minimize the error between the (candidate) model's prediction of the response variable and true values of the response variable (Ordinary Least Squares - OLS), or we try to find the ""beta coefficients"" such that probability of reproducing the observed response values is maximized (Maximum Likelihood Estimation - MLE) . For simple regression models, there exists ""exact solutions"" to the OLS and MLE optimization problems and we can calculate these ""beta coefficients"" analytically - but in more sophisticated regression models such as Logistic Regression or Regularized Regression (e.g. LASSO, RIDGE), the corresponding optimization problem is usually solved using some approximate and iterative algorithm such as the ""Newton-Raphson"" Method. Decision Trees: A Decision Tree (e.g. CART) is formed by ""splitting"" variables into smaller subsets (i.e. ""nodes"") such that ""purity"" increases in each subsequent subset; ""purity"" is often measured through some sort of ""Information Gain"" that is based on measures such as ""Gini Index"" or ""Entropy"". Thus, Decision Trees can be interpreted as an optimization problem where ""Information Gain"" has to be optimized. I have heard that since Decision Trees often have different variable types (e.g. continuous and categorial), searching for the optimal variable splits that optimize ""Information Gain"" is a Mixed Integer Optimization Problem having an enormous Combinatorial Search Space. For the interest of creating a decent Decision Tree in a reasonable amount of time, ""Information Gain"" is optimized using a ""Greedy Search Algorithm,"" and as a result, the final Decision Tree (i.e. the answer to this Mixed Integer Optimization Problem) is almost certainly unlikely to be the optimal Decision Tree (as there is very high probability that a better Decision Tree likely exists in this large Combinatorial Search Space, but finding this Decision Tree would take too much time): Neural Networks: Successful Neural Networks are largely attributed to the effectiveness of Optimization Algorithms (e.g. Stochastic Gradient Descent) to optimize (i.e. determine the ""neuron weights"") notoriously complicated, non-deterministic (i.e. the loss function is dependent on the observed data), high dimensional and non-convex Loss Functions: My Question: Is it possible to categorize the optimization problems corresponding to Statistical and Machine Learning Models such as Regression Models, Decision Trees, and Neural Networks as ""P"" vs ""NP""? I am aware that categorizing these problems wont really have any effect on solving them, but I have the following guess: When provided with a candidate solution to any of these optimization problems (e.g. Regression beta coefficients, a particular Decision Tree, Neural Network Weights), we have no real way of checking whether this candidate solution is indeed the optimal solution (unlike a sudoku, in which even for an enormous ""n x n"" sudoku, we can instantly check if a candidate solution violates the rules or not). Thus, my guess is that many of these above optimization problems are likely either NP-Complete or NP-Hard. Is this correct? Thanks! Note: I have often heard of Pattern Recognition and Machine Learning Optimization Problems being described as ""Ill-Posed Problems"" , implying that they are inherently more difficult than ""Well-Posed Problems""  This is due to factors such as: solutions to these optimization problems ""may not exist"" solutions ""may not be unique"". slight perturbations to the function/solution can cause large changes within the system it is impossible to determine whether the final solution to the optimization problem is in fact the ""true optimal"" solution - in Machine Learning, we often settle for a ""good enough"" solution I would imagine that many ""ill-posed"" machine learning optimization problems would generally fall under the designation of ""NP"" , ""NP-Complete"" or ""NP-Hard"" compared to simply ""P"". However, I not sure about this.","In the context of Computer Science and Optimization, I have heard that different problems can be classified using the ""P vs NP"" framework . Essentially, there is a hierarchy of problems based on the inherent complexity of the problem itself. For example, a problem like ""multiplying numbers"" is considered as ""P"" and is considered fundamentally easier to solve than a problem like ""solving a sudoku"" which is ""NP"". In most Statistical and Machine Learning Models, there is usually an optimization problem ""nested"" within the model that is required to solve. For example: Regression Models : In a standard regression model, we try to find the value of the ""beta coefficients"" that either minimize the error between the (candidate) model's prediction of the response variable and true values of the response variable (Ordinary Least Squares - OLS), or we try to find the ""beta coefficients"" such that probability of reproducing the observed response values is maximized (Maximum Likelihood Estimation - MLE) . For simple regression models, there exists ""exact solutions"" to the OLS and MLE optimization problems and we can calculate these ""beta coefficients"" analytically - but in more sophisticated regression models such as Logistic Regression or Regularized Regression (e.g. LASSO, RIDGE), the corresponding optimization problem is usually solved using some approximate and iterative algorithm such as the ""Newton-Raphson"" Method. Decision Trees: A Decision Tree (e.g. CART) is formed by ""splitting"" variables into smaller subsets (i.e. ""nodes"") such that ""purity"" increases in each subsequent subset; ""purity"" is often measured through some sort of ""Information Gain"" that is based on measures such as ""Gini Index"" or ""Entropy"". Thus, Decision Trees can be interpreted as an optimization problem where ""Information Gain"" has to be optimized. I have heard that since Decision Trees often have different variable types (e.g. continuous and categorial), searching for the optimal variable splits that optimize ""Information Gain"" is a Mixed Integer Optimization Problem having an enormous Combinatorial Search Space. For the interest of creating a decent Decision Tree in a reasonable amount of time, ""Information Gain"" is optimized using a ""Greedy Search Algorithm,"" and as a result, the final Decision Tree (i.e. the answer to this Mixed Integer Optimization Problem) is almost certainly unlikely to be the optimal Decision Tree (as there is very high probability that a better Decision Tree likely exists in this large Combinatorial Search Space, but finding this Decision Tree would take too much time): Neural Networks: Successful Neural Networks are largely attributed to the effectiveness of Optimization Algorithms (e.g. Stochastic Gradient Descent) to optimize (i.e. determine the ""neuron weights"") notoriously complicated, non-deterministic (i.e. the loss function is dependent on the observed data), high dimensional and non-convex Loss Functions: My Question: Is it possible to categorize the optimization problems corresponding to Statistical and Machine Learning Models such as Regression Models, Decision Trees, and Neural Networks as ""P"" vs ""NP""? I am aware that categorizing these problems wont really have any effect on solving them, but I have the following guess: When provided with a candidate solution to any of these optimization problems (e.g. Regression beta coefficients, a particular Decision Tree, Neural Network Weights), we have no real way of checking whether this candidate solution is indeed the optimal solution (unlike a sudoku, in which even for an enormous ""n x n"" sudoku, we can instantly check if a candidate solution violates the rules or not). Thus, my guess is that many of these above optimization problems are likely either NP-Complete or NP-Hard. Is this correct? Thanks! Note: I have often heard of Pattern Recognition and Machine Learning Optimization Problems being described as ""Ill-Posed Problems"" , implying that they are inherently more difficult than ""Well-Posed Problems""  This is due to factors such as: solutions to these optimization problems ""may not exist"" solutions ""may not be unique"". slight perturbations to the function/solution can cause large changes within the system it is impossible to determine whether the final solution to the optimization problem is in fact the ""true optimal"" solution - in Machine Learning, we often settle for a ""good enough"" solution I would imagine that many ""ill-posed"" machine learning optimization problems would generally fall under the designation of ""NP"" , ""NP-Complete"" or ""NP-Hard"" compared to simply ""P"". However, I not sure about this.",,"['statistics', 'optimization', 'machine-learning', 'regression', 'neural-networks']"
13,Possible bayes theorem question regarding a shared car,Possible bayes theorem question regarding a shared car,,"The question: Suppose 2 friends share the use of a car evenly, we will call the two friends Roger and James, respectively. We know that Roger will only use the car to drive to the grocery store, on the other hand when James uses the car, 1/3rd of the time he will drive to the grocery store. If we were to see Roger and James car parked outside the grocery store with nobody inside, what are the odds that Roger is inside the grocery store ? Solution: My logic is to draw a tree diagram and focus only on the possibilities that result in Roger at the grocery store, out of the total that includes both Roger and James, which so happens to be 3/4, can anyone confirm if this is correct ?","The question: Suppose 2 friends share the use of a car evenly, we will call the two friends Roger and James, respectively. We know that Roger will only use the car to drive to the grocery store, on the other hand when James uses the car, 1/3rd of the time he will drive to the grocery store. If we were to see Roger and James car parked outside the grocery store with nobody inside, what are the odds that Roger is inside the grocery store ? Solution: My logic is to draw a tree diagram and focus only on the possibilities that result in Roger at the grocery store, out of the total that includes both Roger and James, which so happens to be 3/4, can anyone confirm if this is correct ?",,"['probability', 'statistics', 'discrete-mathematics', 'trees', 'bayes-theorem']"
14,Correlation of two random variables as functions,Correlation of two random variables as functions,,"Consider 2 random variables $X_1$ and $X_2$ : $X_1 = Y_1 \sqrt{\alpha + \beta {X_0}^2}$ $X_2 = Y_2 \sqrt{\alpha + \beta {X_1}^2}$ $ \alpha>0 $ , $\beta \ge 0$ where $Y_1$ and $Y_2$ are independent and both follow a normal standard distribution $N (0, 1)$ . Also, $Y_1$ is independent of $X_0$ and $Y_2$ is independent of $X_1$ . Find the correlation between $X_1$ and $X_2$ . Based off these informations, I know that $X_1 | X_0 = x_0$ ∼ $N(0, \alpha + \beta x_0^2)$ and $X_2 | X_1 = x_1$ ∼ $N(0, \alpha + \beta x_1^2)$ . Also, the equation for $X_2$ tells me that $X_1$ and $X_2$ are positively correlated. I thought of applying this formula: $Cov(X_1,X_2) = E[Cov(X_1,X_2|Z)] + Cov[E(X_1|Z),E(X_2|Z)]$ but I tried using $Z=X_0$ and $Z=X_1$ it seems like it doesn't work. Am I choosing the wrong formula?","Consider 2 random variables and : , where and are independent and both follow a normal standard distribution . Also, is independent of and is independent of . Find the correlation between and . Based off these informations, I know that ∼ and ∼ . Also, the equation for tells me that and are positively correlated. I thought of applying this formula: but I tried using and it seems like it doesn't work. Am I choosing the wrong formula?","X_1 X_2 X_1 = Y_1 \sqrt{\alpha + \beta {X_0}^2} X_2 = Y_2 \sqrt{\alpha + \beta {X_1}^2}  \alpha>0  \beta \ge 0 Y_1 Y_2 N (0, 1) Y_1 X_0 Y_2 X_1 X_1 X_2 X_1 | X_0 = x_0 N(0, \alpha + \beta x_0^2) X_2 | X_1 = x_1 N(0, \alpha + \beta x_1^2) X_2 X_1 X_2 Cov(X_1,X_2) = E[Cov(X_1,X_2|Z)] + Cov[E(X_1|Z),E(X_2|Z)] Z=X_0 Z=X_1","['probability', 'statistics', 'conditional-probability', 'covariance']"
15,Finding the variance of a function,Finding the variance of a function,,"In an attempt to find the variance of a function continuous in the interval $S = [a,b]$ . I have proposed a solution. But am afraid that I have something wrong over here. And before any further explanation I need to clarify that I am currently in high school year 11 so please consider my range of knowledge! without any further a do here is my attempt. For any continuous function $f(x)$ we can define $$ \tag{*} M\{f\}(a, b) = \frac{1}{b-a} \int_{a}^{b}f(x)\, dx. $$ From here we will use a theorem that states: $$ \tag{**} \sigma^2 = (\frac{1}{N}\sum_{i}^N x_i^2) - x_{avg}^2. $$ Here note that $\frac{1}{N}\sum_{i}^N x_i^2$ is just the average of the set $S$ where $S = \{x_i|i\in[1, N]\land i\in N\}$ now if we need to find the average of some function $f$ from the theorem $(*)$ we can find it. So let's plug all of that in: $$ \tag{***} (*)(**) \implies \sigma^2\{f\}(a, b) = M\{x^2\}(a, b)-[M\{f\}(a, b)]^2 = \frac{1}{b-a} \int_{a}^{b}x^2 dx - [\frac{1}{b-a} \int_{a}^{b}f(x) dx]^2. $$ Now when I input the function $\sigma^2\{\sin x\}(-3, 3)$ wolfram alpha outputs 3 but on the other hand I have written a code which divides an interval into pieces and computes the value of the function at each slicing point and then it computes the variance of the collected data which it claims to be about $0.523254$ here is the code: def mean(data):     return sum(data) / len(data)  def variance(data):     mean_flt = mean(data)     return sum([(i ** 2) / len(data) for i in data]) - mean_flt ** 2  def split_interval(interval, section):     step = (interval[1] - interval[0]) / section     array = []     i = interval[0]     while i <= interval[-1]:         array.append(i)         i += step      return array[:]  function = lambda x : math.sin(x) dataset = [function(i) for i in split_interval([-3, 3], 100)] var = variance(dataset)  print(""var-rep-100 : "", var) please help me see my mistakes whether it is in the code or it is in the math! Thank you in advance!","In an attempt to find the variance of a function continuous in the interval . I have proposed a solution. But am afraid that I have something wrong over here. And before any further explanation I need to clarify that I am currently in high school year 11 so please consider my range of knowledge! without any further a do here is my attempt. For any continuous function we can define From here we will use a theorem that states: Here note that is just the average of the set where now if we need to find the average of some function from the theorem we can find it. So let's plug all of that in: Now when I input the function wolfram alpha outputs 3 but on the other hand I have written a code which divides an interval into pieces and computes the value of the function at each slicing point and then it computes the variance of the collected data which it claims to be about here is the code: def mean(data):     return sum(data) / len(data)  def variance(data):     mean_flt = mean(data)     return sum([(i ** 2) / len(data) for i in data]) - mean_flt ** 2  def split_interval(interval, section):     step = (interval[1] - interval[0]) / section     array = []     i = interval[0]     while i <= interval[-1]:         array.append(i)         i += step      return array[:]  function = lambda x : math.sin(x) dataset = [function(i) for i in split_interval([-3, 3], 100)] var = variance(dataset)  print(""var-rep-100 : "", var) please help me see my mistakes whether it is in the code or it is in the math! Thank you in advance!","S = [a,b] f(x) 
\tag{*}
M\{f\}(a, b) = \frac{1}{b-a} \int_{a}^{b}f(x)\, dx.
 
\tag{**}
\sigma^2 = (\frac{1}{N}\sum_{i}^N x_i^2) - x_{avg}^2.
 \frac{1}{N}\sum_{i}^N x_i^2 S S = \{x_i|i\in[1, N]\land i\in N\} f (*) 
\tag{***}
(*)(**) \implies \sigma^2\{f\}(a, b) = M\{x^2\}(a, b)-[M\{f\}(a, b)]^2 = \frac{1}{b-a} \int_{a}^{b}x^2 dx - [\frac{1}{b-a} \int_{a}^{b}f(x) dx]^2.
 \sigma^2\{\sin x\}(-3, 3) 0.523254","['calculus', 'integration', 'statistics', 'definite-integrals', 'python']"
16,Convergence in probability of a random variable multiplied by n,Convergence in probability of a random variable multiplied by n,,"I'm trying to solve this 2 part example and the first part is trivial to show but I get confused about the second part. Because i know the solution it is easy to fool myself into thinking I understand it but I don't want that. I want to understand. So here goes: For $\boldsymbol{n} \geq \boldsymbol{1}$ , let $\boldsymbol{X}_{\boldsymbol{n}}$ be a Poisson random variable with parameter $1 / n$ . What can you conclude? $\square$ $\boldsymbol{X}_{\boldsymbol{n}} \rightarrow \mathbf{0}$ in probability, but $\boldsymbol{n X}_{\boldsymbol{n}}$ does not converge in probability $\square$ $\boldsymbol{X}_{n} \rightarrow 0$ in probability, $\boldsymbol{n X}_{n} \rightarrow 0$ in probability, and $\mathbb{E}\left[\left(\boldsymbol{n X}_{n}\right)^{2}\right]$ converges. $\square$ $\boldsymbol{X}_{n} \rightarrow 0$ and $n X_{n} \rightarrow 0$ in probability, but $\mathbb{E}\left[\left(n X_{n}\right)^{2}\right]$ does not converge. I started by wanting to prove $X_n$ converges in probability to 0. That was the easier bit: $$\lim_{n\to\infty} P(|X_n-0|>\epsilon)=0\Leftrightarrow\lim_{n\to\infty} P(|e^{-\frac{1}{n}}-0|>\epsilon)=0$$ Where I replaced $X_n$ by its pmf at point 0. Is this ok to write? Now I'm questioning the use of P on the RHS of above as I'm already using: $$\mathbf{P}\left(X_{n}=0\right)=\left(\frac{1}{n}\right)^{0} \frac{1}{0 !} \exp \left(-\frac{1}{n}\right)=\exp \left(-\frac{1}{n}\right)$$ What I ask of you is: Please help me be more precise about these statements Help me prove $\boldsymbol{n X}_{\boldsymbol{n}}$ does converge in probability as well. Thank you in advance!","I'm trying to solve this 2 part example and the first part is trivial to show but I get confused about the second part. Because i know the solution it is easy to fool myself into thinking I understand it but I don't want that. I want to understand. So here goes: For , let be a Poisson random variable with parameter . What can you conclude? in probability, but does not converge in probability in probability, in probability, and converges. and in probability, but does not converge. I started by wanting to prove converges in probability to 0. That was the easier bit: Where I replaced by its pmf at point 0. Is this ok to write? Now I'm questioning the use of P on the RHS of above as I'm already using: What I ask of you is: Please help me be more precise about these statements Help me prove does converge in probability as well. Thank you in advance!",\boldsymbol{n} \geq \boldsymbol{1} \boldsymbol{X}_{\boldsymbol{n}} 1 / n \square \boldsymbol{X}_{\boldsymbol{n}} \rightarrow \mathbf{0} \boldsymbol{n X}_{\boldsymbol{n}} \square \boldsymbol{X}_{n} \rightarrow 0 \boldsymbol{n X}_{n} \rightarrow 0 \mathbb{E}\left[\left(\boldsymbol{n X}_{n}\right)^{2}\right] \square \boldsymbol{X}_{n} \rightarrow 0 n X_{n} \rightarrow 0 \mathbb{E}\left[\left(n X_{n}\right)^{2}\right] X_n \lim_{n\to\infty} P(|X_n-0|>\epsilon)=0\Leftrightarrow\lim_{n\to\infty} P(|e^{-\frac{1}{n}}-0|>\epsilon)=0 X_n \mathbf{P}\left(X_{n}=0\right)=\left(\frac{1}{n}\right)^{0} \frac{1}{0 !} \exp \left(-\frac{1}{n}\right)=\exp \left(-\frac{1}{n}\right) \boldsymbol{n X}_{\boldsymbol{n}},"['probability', 'statistics']"
17,Posterior of an Interval under Beta Distribution,Posterior of an Interval under Beta Distribution,,"Suppose you have a beta prior with parameters $\alpha$ and $\beta$ .  You draw $n \geq 1$ points from a binomial distribution and observe $k > n/2$ successes.  Does your posterior probability in the interval $[1/2,1]$ increase?  Intuitively, I think ``yes'', but I am bad at evaluating integrals.  Since the beta family is conjugate to the binomial distribution, I believe that we can rephrase the question as follows.  If $k$ and $n$ are positive integers such that $1 \leq k \leq n$ and $k > (n-k)$ , is the following true? $$\frac{1}{B(\alpha, \beta)} \int_{1/2}^{1} x^{\alpha-1}(1-x)^{\beta-1}dx < \frac{1}{B(\alpha+k, \beta+(n-k))} \int_{1/2}^{1} x^{\alpha+k-1}(1-x)^{\beta+n-k-1}dx$$ where $B(\cdot, \cdot)$ is the beta-function. More generally, for $r \in [1/2,1)$ , if one observes $k$ many successes in $n$ trials and $k/n > r$ , does it follow that one's posterior in the interval $[r,1]$ increases?","Suppose you have a beta prior with parameters and .  You draw points from a binomial distribution and observe successes.  Does your posterior probability in the interval increase?  Intuitively, I think ``yes'', but I am bad at evaluating integrals.  Since the beta family is conjugate to the binomial distribution, I believe that we can rephrase the question as follows.  If and are positive integers such that and , is the following true? where is the beta-function. More generally, for , if one observes many successes in trials and , does it follow that one's posterior in the interval increases?","\alpha \beta n \geq 1 k > n/2 [1/2,1] k n 1 \leq k \leq n k > (n-k) \frac{1}{B(\alpha, \beta)} \int_{1/2}^{1} x^{\alpha-1}(1-x)^{\beta-1}dx < \frac{1}{B(\alpha+k, \beta+(n-k))} \int_{1/2}^{1} x^{\alpha+k-1}(1-x)^{\beta+n-k-1}dx B(\cdot, \cdot) r \in [1/2,1) k n k/n > r [r,1]","['statistics', 'probability-distributions', 'conditional-probability', 'bayesian']"
18,Does Hoeffding's inequality hold for uncorrelated random variables?,Does Hoeffding's inequality hold for uncorrelated random variables?,,"I know that Hoeffding's inequality holds for sums of independent random variables. However, we also know that being uncorrelated does not necessarily imply independence. But I wish to understand whether Hoeffding's inequality is valid for sums of bounded uncorrelated random variables? Or is independence a necessity?","I know that Hoeffding's inequality holds for sums of independent random variables. However, we also know that being uncorrelated does not necessarily imply independence. But I wish to understand whether Hoeffding's inequality is valid for sums of bounded uncorrelated random variables? Or is independence a necessity?",,"['probability', 'probability-theory', 'statistics', 'independence', 'concentration-of-measure']"
19,"LR-test for a Pareto$(1,\beta)$-distribution",LR-test for a Pareto-distribution,"(1,\beta)","Let $X_1,\ldots,X_n$ be a sample from a Pareto $(1,\beta)$ -distribution with density $$f(x \mid \beta) = \begin{cases} \beta x^{-(\beta+1)} & x \ge 1 \\ 0 & \, \text{else} \end{cases}$$ We test for a given $\gamma \in \mathbb{R}$ if $H_0: \beta \ge \gamma, H_A: \beta < \gamma$ . On which statitistic does the LR-test for $\beta$ depennd upon and what is its area of acceptence? So far I have computed the MLE for $\beta$ : $L_n(\beta \mid X) = \prod_{i=1}^n \beta x_i^{-(\beta+1)} = \beta^n\prod_{i=1}^n x_i^{-\beta+1}$ The log-likelihood is thus: $l(\beta \mid X) = n \log(\beta) - (\beta+1) \sum_{i=1}^n \log(x_i)$ $l^\prime (\beta \mid X) = \frac{n}{\beta} - \sum_{i=1}^n \log(x_i)$ Therefore the MLE $\hat{\beta}$ for $\beta$ is given by $$\hat{\beta} = \frac{n}{\sum_{i=1}^n \log(x_i)}$$ I know that the LR test is given by $$\frac{L(\hat{\beta_0} \mid X)}{L(\hat{\beta} \mid X)}$$ , however, I do not see what I should do now, I mean I see no way to simplify this quotient. Could you please give me a hint?","Let be a sample from a Pareto -distribution with density We test for a given if . On which statitistic does the LR-test for depennd upon and what is its area of acceptence? So far I have computed the MLE for : The log-likelihood is thus: Therefore the MLE for is given by I know that the LR test is given by , however, I do not see what I should do now, I mean I see no way to simplify this quotient. Could you please give me a hint?","X_1,\ldots,X_n (1,\beta) f(x \mid \beta) = \begin{cases}
\beta x^{-(\beta+1)} & x \ge 1 \\
0 & \, \text{else}
\end{cases} \gamma \in \mathbb{R} H_0: \beta \ge \gamma, H_A: \beta < \gamma \beta \beta L_n(\beta \mid X) = \prod_{i=1}^n \beta x_i^{-(\beta+1)} = \beta^n\prod_{i=1}^n x_i^{-\beta+1} l(\beta \mid X) = n \log(\beta) - (\beta+1) \sum_{i=1}^n \log(x_i) l^\prime (\beta \mid X) = \frac{n}{\beta} - \sum_{i=1}^n \log(x_i) \hat{\beta} \beta \hat{\beta} = \frac{n}{\sum_{i=1}^n \log(x_i)} \frac{L(\hat{\beta_0} \mid X)}{L(\hat{\beta} \mid X)}","['statistics', 'hypothesis-testing']"
20,Autocorrelation of sum of sinusoids.,Autocorrelation of sum of sinusoids.,,"Consider a signal that is a sum of sinusoids, e.g. $x(t)=Asin(at)+Bcos(bt)$ Is there an easy and general way to get an analytical solution for the autocorrelation of $x(t)$ ? Is the best way to simply plug $x(t)$ into the autocorrelation formula? $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}x(t+\tau)x(t)dt$$ $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}[Asin(a(t+\tau))+Bcos(b(t+\tau))][Asin(at)+Bcos(bt)]dt$$ $$ R_{xx}(\tau)=\int_{-\infty}^{\infty}[A^2sin(a(t+\tau))sin(at) +ABsin(a(t+\tau))cos(bt) + ABcos(b(t+\tau))sin(at) + B^2cos(b(t+\tau))cos(bt)]dt$$ Now evaluating each one independently, $$ R_{xx}(\tau)=\frac{A^2}{2}cos(a\tau)+\frac{AB}{2}sin(a\tau)-\frac{AB}{2}sin(b\tau)+\frac{B^2}{2}cos(b\tau)$$ The algebra can get pretty messy if we have a larger sum of sinusoids like $x(t)=Asin(at)+Bcos(bt)+Csin(ct)+Dcos(dt)+...$ Do I just have to bite the bullet and do the all the algebra? Or is there some general formula?","Consider a signal that is a sum of sinusoids, e.g. Is there an easy and general way to get an analytical solution for the autocorrelation of ? Is the best way to simply plug into the autocorrelation formula? Now evaluating each one independently, The algebra can get pretty messy if we have a larger sum of sinusoids like Do I just have to bite the bullet and do the all the algebra? Or is there some general formula?",x(t)=Asin(at)+Bcos(bt) x(t) x(t)  R_{xx}(\tau)=\int_{-\infty}^{\infty}x(t+\tau)x(t)dt  R_{xx}(\tau)=\int_{-\infty}^{\infty}[Asin(a(t+\tau))+Bcos(b(t+\tau))][Asin(at)+Bcos(bt)]dt  R_{xx}(\tau)=\int_{-\infty}^{\infty}[A^2sin(a(t+\tau))sin(at) +ABsin(a(t+\tau))cos(bt) + ABcos(b(t+\tau))sin(at) + B^2cos(b(t+\tau))cos(bt)]dt  R_{xx}(\tau)=\frac{A^2}{2}cos(a\tau)+\frac{AB}{2}sin(a\tau)-\frac{AB}{2}sin(b\tau)+\frac{B^2}{2}cos(b\tau) x(t)=Asin(at)+Bcos(bt)+Csin(ct)+Dcos(dt)+...,"['statistics', 'correlation', 'time-series', 'stationary-processes']"
21,"Why is my method not applicable, one-sided confidence interval","Why is my method not applicable, one-sided confidence interval",,"I have ""solved"" a question in mathematical statistics, but unfortunately received an incorrect answer and I would appreciate any help in understanding why my method did not work in this case. The question is as follows: In city A , 500 randomly selected people were tested for antibodies to covid-19, which resulted in 110 giving positive results. In city B , a random sample of 500 people was also drawn, and here 91 had positive results. Can it be said that it is statistically certain that city A has a larger population share with antibodies than city B ? (Justify your answer with an appropriate test. Use 5% significance level.) I started by producing $P^*(A)=\frac{110}{500}=0.22$ and respectively $P^*(B)=\frac{91}{500}=0.182$ . Then I chose to continued with a one-sided hypothesis testing where I assumed that the data was binomially distributed according to $Bin(n=500,p)$ , with $$H_0:P=0.182 \text{ and }H_1:>0.182.$$ And finally brought out the one-sided interval according to: $$I_p=\lambda_\alpha \sqrt{\frac{P^*(A)(1-P^*(A))}{n}}\Rightarrow [0.189,\infty].$$ And concluded that $H_0$ should be rejected since 0.182 is not included in the interval. But according to the results, $H_0$ should not be rejected and they have also used a different method than I did. Why is this method not applicable in my case? Note : At first I thought that errors were because I only used α and not $\frac{\alpha}{2}$ in the calculation, however, even if I were to replace this, 0.182 is not in the interval.","I have ""solved"" a question in mathematical statistics, but unfortunately received an incorrect answer and I would appreciate any help in understanding why my method did not work in this case. The question is as follows: In city A , 500 randomly selected people were tested for antibodies to covid-19, which resulted in 110 giving positive results. In city B , a random sample of 500 people was also drawn, and here 91 had positive results. Can it be said that it is statistically certain that city A has a larger population share with antibodies than city B ? (Justify your answer with an appropriate test. Use 5% significance level.) I started by producing and respectively . Then I chose to continued with a one-sided hypothesis testing where I assumed that the data was binomially distributed according to , with And finally brought out the one-sided interval according to: And concluded that should be rejected since 0.182 is not included in the interval. But according to the results, should not be rejected and they have also used a different method than I did. Why is this method not applicable in my case? Note : At first I thought that errors were because I only used α and not in the calculation, however, even if I were to replace this, 0.182 is not in the interval.","P^*(A)=\frac{110}{500}=0.22 P^*(B)=\frac{91}{500}=0.182 Bin(n=500,p) H_0:P=0.182 \text{ and }H_1:>0.182. I_p=\lambda_\alpha \sqrt{\frac{P^*(A)(1-P^*(A))}{n}}\Rightarrow [0.189,\infty]. H_0 H_0 \frac{\alpha}{2}","['statistics', 'confidence-interval']"
22,Area Under The Curve of CDF and Alpha Value - Chi-Sq Test Interpretation,Area Under The Curve of CDF and Alpha Value - Chi-Sq Test Interpretation,,"Can you help me understand the meaning of the area under the curve of a PDF in the interpretation of a Chi-Square test? This is what I think I know: If my test statistic (i.e. the chi-square value) increases for a given Degree of Freedom, so does my cumulative distribution function (CDF) value of the Chi-Square CDF(Chi). This value represents the area under the curve of the Chi-Square probability density function (PDF). CDF(Chi) is also the significance level or ""critical region"" called $\alpha$ (often used with 0.05). Is it correct, that if for a given test value the CDF(Chi) > 0.05 my H0 can be rejected when only interested in a one-tailed test - i.e. are two distributions different or not? This approach seems to work if I do a one-tailed test. This is however currently counterintuitive to me, because as far as I understand, p = 1 - CDF(Chi) and H0 is rejected if p < 0.05. Therefore I would believe that the above statement is not true and I would need to reject H0 if CDF > 0.95. That would also make more sense to me, as it means that H0 is only rejected if there is more than 95% ""evidence"" that it is not random or less than a 5% chance of wrongfully rejecting H0. Can someone help clear things up - I assume my last thought makes more sense and in my example, I am getting results that seem to conform to what I want to see but are actually not statically significant. Thanks!","Can you help me understand the meaning of the area under the curve of a PDF in the interpretation of a Chi-Square test? This is what I think I know: If my test statistic (i.e. the chi-square value) increases for a given Degree of Freedom, so does my cumulative distribution function (CDF) value of the Chi-Square CDF(Chi). This value represents the area under the curve of the Chi-Square probability density function (PDF). CDF(Chi) is also the significance level or ""critical region"" called (often used with 0.05). Is it correct, that if for a given test value the CDF(Chi) > 0.05 my H0 can be rejected when only interested in a one-tailed test - i.e. are two distributions different or not? This approach seems to work if I do a one-tailed test. This is however currently counterintuitive to me, because as far as I understand, p = 1 - CDF(Chi) and H0 is rejected if p < 0.05. Therefore I would believe that the above statement is not true and I would need to reject H0 if CDF > 0.95. That would also make more sense to me, as it means that H0 is only rejected if there is more than 95% ""evidence"" that it is not random or less than a 5% chance of wrongfully rejecting H0. Can someone help clear things up - I assume my last thought makes more sense and in my example, I am getting results that seem to conform to what I want to see but are actually not statically significant. Thanks!",\alpha,"['statistics', 'probability-distributions', 'hypothesis-testing', 'cumulative-distribution-functions', 'chi-squared']"
23,"Find the limiting distribution of $\frac{\sum_{i=1}^{n}(Y_i^2-\sigma Z_i^2)}{\sum_{i=1}^{n}Z_i^2}$, where $Z_i\sim N(0,1)$ and $Y_i\sim N(0,\sigma^2)$","Find the limiting distribution of , where  and","\frac{\sum_{i=1}^{n}(Y_i^2-\sigma Z_i^2)}{\sum_{i=1}^{n}Z_i^2} Z_i\sim N(0,1) Y_i\sim N(0,\sigma^2)","Let $Y_1,\ldots,Y_n \overset{\text{iid}}{\sim} N(0,\sigma^2)$ , where $\sigma > 0$ , and let $Z_1,\ldots,Z_n \overset{\text{iid}}{\sim} N(0,1)$ . Define $$U_n = \frac{\sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{\sum_{i=1}^{n} Z_i^2}. $$ What is the limiting distribution of $\sqrt{n} \,U_n$ as $n \rightarrow \infty$ ? My attempt: I started by writing $U_n$ as \begin{align*}    \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{ \frac{1}{n}\sum_{i=1}^{n} Z_i^2}, \end{align*} and then attempted to treat the numerator and denominator individally. First computing some relevant information: From the fact that the mean and variance of a chi-squared random variable with $p$ degrees of freedom are $p$ and $2p$ , respectively, we have $E(Z_i^2) = 1$ and $\text{Var}(Z_i^2) = 2$ . We also have $E(Y_i^2) = V(Y_i) + (E(Y_i))^2 = \sigma^2 + 0^2 = \sigma^2$ . Noting that $\frac{1}{\sigma}Y_i \sim N(0,1)$ , we have $(\frac{1}{\sigma} Y_i)^2 = \frac{1}{\sigma^2} Y_i^2 \sim \chi^2_1$ . Hence, $\text{Var}(\frac{1}{\sigma^2} Y_i^2) = \frac{1}{\sigma^4} \text{Var}(Y_i^2) = 2$ , and so $\text{Var}(Y_i^2) = 2\sigma^4 < \infty$ . Therefore, by the Weak Law of Large Numbers, $$\frac{1}{n} \sum_{i=1}^{n} Y_i^2 \overset{p}{\longrightarrow} \sigma^2 \text{ as } n \to \infty $$ (where $\overset{p}{\longrightarrow}$ denotes convergence in probability). Then by Slutsky's Theorem, if $\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)\right)$ converges to $W$ in distribution, then $U_n$ converges to $\sigma^2 W$ in distribution. I'm currently stuck on how to find the limiting distribution of the numerator. My idea is to apply the Central Limit Theorem somehow, maybe to the random variables $X_i := Y_i^2 - \sigma Z_i^2$ . I've computed that $E(X_i) = \sigma^2 - \sigma$ and $\text{Var}(X_i) = 2\sigma^4 + 2\sigma^2$ . Then by the CLT, $$ \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2) - (\sigma^2 - \sigma)}{\sqrt{\frac{2\sigma^4 + 2 \sigma^2}{n}} } \longrightarrow N(0,1) \text{ in distribution}.$$ But I'm not sure where to go from here...Any suggestions for how to progress?","Let , where , and let . Define What is the limiting distribution of as ? My attempt: I started by writing as and then attempted to treat the numerator and denominator individally. First computing some relevant information: From the fact that the mean and variance of a chi-squared random variable with degrees of freedom are and , respectively, we have and . We also have . Noting that , we have . Hence, , and so . Therefore, by the Weak Law of Large Numbers, (where denotes convergence in probability). Then by Slutsky's Theorem, if converges to in distribution, then converges to in distribution. I'm currently stuck on how to find the limiting distribution of the numerator. My idea is to apply the Central Limit Theorem somehow, maybe to the random variables . I've computed that and . Then by the CLT, But I'm not sure where to go from here...Any suggestions for how to progress?","Y_1,\ldots,Y_n \overset{\text{iid}}{\sim} N(0,\sigma^2) \sigma > 0 Z_1,\ldots,Z_n \overset{\text{iid}}{\sim} N(0,1) U_n = \frac{\sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{\sum_{i=1}^{n} Z_i^2}.  \sqrt{n} \,U_n n \rightarrow \infty U_n \begin{align*}
   \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{ \frac{1}{n}\sum_{i=1}^{n} Z_i^2},
\end{align*} p p 2p E(Z_i^2) = 1 \text{Var}(Z_i^2) = 2 E(Y_i^2) = V(Y_i) + (E(Y_i))^2 = \sigma^2 + 0^2 = \sigma^2 \frac{1}{\sigma}Y_i \sim N(0,1) (\frac{1}{\sigma} Y_i)^2 = \frac{1}{\sigma^2} Y_i^2 \sim \chi^2_1 \text{Var}(\frac{1}{\sigma^2} Y_i^2) = \frac{1}{\sigma^4} \text{Var}(Y_i^2) = 2 \text{Var}(Y_i^2) = 2\sigma^4 < \infty \frac{1}{n} \sum_{i=1}^{n} Y_i^2 \overset{p}{\longrightarrow} \sigma^2 \text{ as } n \to \infty  \overset{p}{\longrightarrow} \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)\right) W U_n \sigma^2 W X_i := Y_i^2 - \sigma Z_i^2 E(X_i) = \sigma^2 - \sigma \text{Var}(X_i) = 2\sigma^4 + 2\sigma^2  \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2) - (\sigma^2 - \sigma)}{\sqrt{\frac{2\sigma^4 + 2 \sigma^2}{n}} } \longrightarrow N(0,1) \text{ in distribution}.","['probability', 'statistics', 'probability-distributions', 'random-variables', 'central-limit-theorem']"
24,Linear regression with two independent variables,Linear regression with two independent variables,,"I want to consider purely theoretical situation that we have two random $\textbf{independent}$ variables $Y$ and $X$ which has $EY = EX = 0$ . Let's also consider regression $Y \sim X$ . I want to say what will be the coefficient $\beta$ in this regression associated with $X$ when we consider model with and without intercept: Model with intercept Since model has intercept we can write $\beta$ as: $$\beta = \frac{\textrm{Cov}(X, Y)}{\textrm{Var}(X)} = 0$$ So the coefficient will be $0$ , because $X, Y$ are independent (I didn't use information that $EY = EX = 0$ ) Model without intercept When we consider model without intercept, then $\beta$ is given as: $$\beta = \frac{\sum_{i = 1}^n x_iy_i}{\sum_{i = 1}^n x_i^2}$$ Which can be rewriten as: $$\beta = \frac{\sum_{i = 1}^n \frac{x_iy_i}{n}}{\sum_{i = 1}^n \frac{x_i^2}{n}} \rightarrow \frac{E[XY]}{E[X^2]} = \frac{EXEY}{E[X^2]} = 0$$ In second case, our $\beta$ will converge to $0$ , when $n$ goes to infinity. Is my justification correct?","I want to consider purely theoretical situation that we have two random variables and which has . Let's also consider regression . I want to say what will be the coefficient in this regression associated with when we consider model with and without intercept: Model with intercept Since model has intercept we can write as: So the coefficient will be , because are independent (I didn't use information that ) Model without intercept When we consider model without intercept, then is given as: Which can be rewriten as: In second case, our will converge to , when goes to infinity. Is my justification correct?","\textbf{independent} Y X EY = EX = 0 Y \sim X \beta X \beta \beta = \frac{\textrm{Cov}(X, Y)}{\textrm{Var}(X)} = 0 0 X, Y EY = EX = 0 \beta \beta = \frac{\sum_{i = 1}^n x_iy_i}{\sum_{i = 1}^n x_i^2} \beta = \frac{\sum_{i = 1}^n \frac{x_iy_i}{n}}{\sum_{i = 1}^n \frac{x_i^2}{n}} \rightarrow \frac{E[XY]}{E[X^2]} = \frac{EXEY}{E[X^2]} = 0 \beta 0 n","['probability', 'statistics', 'mathematical-modeling', 'linear-regression']"
25,Bayesian fallacy in Binomial example,Bayesian fallacy in Binomial example,,"There are $50$ students in a course. Suppose that each student independently decides to continue the course or drop out of the course randomly. Let $X$ be the total number of students who continue with the course. Each student continues with a constant probability $p$ , which is drawn randomly with $P(p=0)=0.1,  P(p=0.75) = 0.6, P(p=1) = 0.3$ prior to the decisions being taken. Find $E(X), Var(X)$ . Also, comment on the distribution of $X$ . Now I am having a logical error somewhere in my two approaches : Approach 1 We know that $X|P=p \sim Bin(50,p)$ . So, $E(X) = E(X|P=0)P(P=0) + E(X|P=0.75)P(P=0.75) + E(X|P=1)P(P=1) = 22.5 + 15 = 37.5$ . Now, $V(X) = V(E(X|P))+E(V(X|P)) = V(50P)+E(50P(1-P)) = 50^2 V(P) + 50 E(P-P^2) = K$ which is some value. Approach 2 Let us take $X_i = 1$ with probability $p$ and $0$ otherwise. We try to write $X=X_1+...+X_{50}$ Then $E(X_i |p) = E(E(X_i|P)) = 0.75$ and so, $E(X) = 50*0.75 = 37.5$ . $V(X_i) = V(E(X_i |P)) + E(V(X_i |P)) = V(P)+E(P-P^2) = E(P)-E(P)^2$ , so $V(X) = 50*V(X_i) \neq K$ . Why are the two variances from the two approaches unequal? Is it because in approach 2, $X_i |P$ are not independent and thus we cannot just simply add their variances?","There are students in a course. Suppose that each student independently decides to continue the course or drop out of the course randomly. Let be the total number of students who continue with the course. Each student continues with a constant probability , which is drawn randomly with prior to the decisions being taken. Find . Also, comment on the distribution of . Now I am having a logical error somewhere in my two approaches : Approach 1 We know that . So, . Now, which is some value. Approach 2 Let us take with probability and otherwise. We try to write Then and so, . , so . Why are the two variances from the two approaches unequal? Is it because in approach 2, are not independent and thus we cannot just simply add their variances?","50 X p P(p=0)=0.1,  P(p=0.75) = 0.6, P(p=1) = 0.3 E(X), Var(X) X X|P=p \sim Bin(50,p) E(X) = E(X|P=0)P(P=0) + E(X|P=0.75)P(P=0.75) + E(X|P=1)P(P=1) = 22.5 + 15 = 37.5 V(X) = V(E(X|P))+E(V(X|P)) = V(50P)+E(50P(1-P)) = 50^2 V(P) + 50 E(P-P^2) = K X_i = 1 p 0 X=X_1+...+X_{50} E(X_i |p) = E(E(X_i|P)) = 0.75 E(X) = 50*0.75 = 37.5 V(X_i) = V(E(X_i |P)) + E(V(X_i |P)) = V(P)+E(P-P^2) = E(P)-E(P)^2 V(X) = 50*V(X_i) \neq K X_i |P","['probability', 'statistics', 'probability-distributions', 'expected-value', 'bayesian']"
26,"Chebyshev's inequality, how is it applied in this problem?","Chebyshev's inequality, how is it applied in this problem?",,"Data set: Problem: Work attempted: There is no way that .21 % is at least 3.48, especially when 29/30 are between the bounds of (3.48, 3.96). It's not clear to me what I'm doing wrong.","Data set: Problem: Work attempted: There is no way that .21 % is at least 3.48, especially when 29/30 are between the bounds of (3.48, 3.96). It's not clear to me what I'm doing wrong.",,"['probability', 'statistics', 'chebyshev-function']"
27,"How to get $ E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*) $?",How to get ?," E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*) ","For a Rademacher variable $P(\epsilon=+1)=P(\epsilon=-1)=1/2$ and a zero-mean r.v. such $X\in [a,b]$ , let $X'$ be an independent copy of $X$ . Then we know that $X-X'=_d \epsilon(X-X')$ . But why do we have the following symmetrization trick? $$ E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*) $$ My questions are (1) Waht does notation $E_{X,X'}(\cdot)$ mean? Does it mean $E_{X}(E_{X'}(\cdot))$ ? (2) It seems that ( ) tells us $$ E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=e^{\lambda(X-X')}?  $$ But $$ E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=\frac{1}{2}e^{\lambda(X-X')}-\frac{1}{2}e^{-\lambda(X-X')} $$ They are not equal... So how to get ( )?","For a Rademacher variable and a zero-mean r.v. such , let be an independent copy of . Then we know that . But why do we have the following symmetrization trick? My questions are (1) Waht does notation mean? Does it mean ? (2) It seems that ( ) tells us But They are not equal... So how to get ( )?","P(\epsilon=+1)=P(\epsilon=-1)=1/2 X\in [a,b] X' X X-X'=_d \epsilon(X-X') 
E_{X,X'}[e^{\lambda(X-X')}]=E_{X,X',\epsilon}[e^{\lambda\epsilon(X-X')}] \,(*)
 E_{X,X'}(\cdot) E_{X}(E_{X'}(\cdot)) 
E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=e^{\lambda(X-X')}? 
 
E_{\epsilon}[e^{\lambda\epsilon(X-X')}]=\frac{1}{2}e^{\lambda(X-X')}-\frac{1}{2}e^{-\lambda(X-X')}
","['probability', 'analysis', 'statistics']"
28,How to compare asymptotic efficiency of MOM estimator with MLE?,How to compare asymptotic efficiency of MOM estimator with MLE?,,Compare the asymptotic efficiency of MOM estimator of parameter $\alpha$ of the pareto distributions with MLE (assume $X_m$ known) $$f(x;\alpha;X_m) = \alpha X_m^{\alpha} x^{-(\alpha + 1)}.$$ I computed the MLE of the pareto distribution equating to $0$ the first derivative of the log-likelihood getting $$\frac{n}{n\log(X_m)+\sum_{i=0}^n \log(X_i)}.$$ And for MOM I get $$\frac{\bar X}{\bar X- X_m}.$$ My question is: In order to compare the asymptotic efficiency should I compute the variance of the Mom and the variance of MLE? If so someone can direct me in the right direction?,Compare the asymptotic efficiency of MOM estimator of parameter of the pareto distributions with MLE (assume known) I computed the MLE of the pareto distribution equating to the first derivative of the log-likelihood getting And for MOM I get My question is: In order to compare the asymptotic efficiency should I compute the variance of the Mom and the variance of MLE? If so someone can direct me in the right direction?,\alpha X_m f(x;\alpha;X_m) = \alpha X_m^{\alpha} x^{-(\alpha + 1)}. 0 \frac{n}{n\log(X_m)+\sum_{i=0}^n \log(X_i)}. \frac{\bar X}{\bar X- X_m}.,"['statistics', 'maximum-likelihood']"
29,Generate random values using an empirical cumulative distribution function,Generate random values using an empirical cumulative distribution function,,"I have a set of data points that I have used to generate my empirical cumulative distribution function (CDF), which looks like this (to simplify things I have reduced the number of points for this question but it shouldn't matter): Given this data and plot, I need to somehow generate random values which follow this distribution. I admit that I am quite rusty when it comes to probability, but as far as I understand I need to first generate the probability density function (PDF), and then from there I can do what I want. Is that accurate? Or what is the best way to get what I want? If interested in giving an example for the answer, here are the X and Y array for the data points of the ECDF plot. I am using Python but I guess a language agnostic answer would also be really helpful x = [107.6697676209896, 430.70331251794784, 1975.0646306785532, 7793.524079329409, 27569.66699567533, 62566.73646946178, 222847.1449910263, 832591.8949493016, 2827054.7454871265, 10000733.572934577] y = [0, 0.04812202549318534, 0.09825964339269633, 0.14190143419466905, 0.27204351414405636, 0.46590411495145756, 0.6008552899988212, 0.6796719668120879, 0.8400864397710662, 1] Thanks a lot!","I have a set of data points that I have used to generate my empirical cumulative distribution function (CDF), which looks like this (to simplify things I have reduced the number of points for this question but it shouldn't matter): Given this data and plot, I need to somehow generate random values which follow this distribution. I admit that I am quite rusty when it comes to probability, but as far as I understand I need to first generate the probability density function (PDF), and then from there I can do what I want. Is that accurate? Or what is the best way to get what I want? If interested in giving an example for the answer, here are the X and Y array for the data points of the ECDF plot. I am using Python but I guess a language agnostic answer would also be really helpful x = [107.6697676209896, 430.70331251794784, 1975.0646306785532, 7793.524079329409, 27569.66699567533, 62566.73646946178, 222847.1449910263, 832591.8949493016, 2827054.7454871265, 10000733.572934577] y = [0, 0.04812202549318534, 0.09825964339269633, 0.14190143419466905, 0.27204351414405636, 0.46590411495145756, 0.6008552899988212, 0.6796719668120879, 0.8400864397710662, 1] Thanks a lot!",,"['probability', 'statistics', 'probability-distributions', 'python', 'cumulative-distribution-functions']"
30,"P(D | w) vs P(y | x, w) - likelihood notation","P(D | w) vs P(y | x, w) - likelihood notation",,"In certain ML books/lectures/slides/notes about Bayesian inference, you often see the likelihood written as $P(\mathcal{D} | w) = \Pi_{i \in N} P(y_i | x_i, w)$ , where $\mathcal{D} = {(x_1, y_1),~ ...~,(x_n, y_n)}$ . It then follows that the equation for Bayes rule is: $$ P(w | \mathcal{D}) = \frac{P(\mathcal{D} | w) \cdot p(w)}{P(\mathcal{D})} $$ I'm confused because if you separate out the covariates from the response (inputs from targets), you could get the following equation for Bayes rule: $$ P(w | x_i, y_i) = \frac{P(y_i | x_i, w) \cdot p(w | x_i)}{P(x_i, y_i)} $$ These two equations don't match up, in particular $p(w) \neq p(w | x_i)$ . So what is the justification for this? In both a frequentist sense and a Bayesian sense? I know that, for example wikipedia , defines the likehood function as the joint probability of the observed data, which would allow for this confusion to make sense. And when performing MLE, you maximize the response/targets in conjunction with the covariates/inputs, so it makes sense intuitively for this notation, but what would be the theoretical justification? Edit: I suppose I'm more confused by why would $P(y_i | x_i, w)$ have the same formulation for the likelihood function as $P(\mathcal{D} | w)$ .","In certain ML books/lectures/slides/notes about Bayesian inference, you often see the likelihood written as , where . It then follows that the equation for Bayes rule is: I'm confused because if you separate out the covariates from the response (inputs from targets), you could get the following equation for Bayes rule: These two equations don't match up, in particular . So what is the justification for this? In both a frequentist sense and a Bayesian sense? I know that, for example wikipedia , defines the likehood function as the joint probability of the observed data, which would allow for this confusion to make sense. And when performing MLE, you maximize the response/targets in conjunction with the covariates/inputs, so it makes sense intuitively for this notation, but what would be the theoretical justification? Edit: I suppose I'm more confused by why would have the same formulation for the likelihood function as .","P(\mathcal{D} | w) = \Pi_{i \in N} P(y_i | x_i, w) \mathcal{D} = {(x_1, y_1),~ ...~,(x_n, y_n)}  P(w | \mathcal{D}) = \frac{P(\mathcal{D} | w) \cdot p(w)}{P(\mathcal{D})}   P(w | x_i, y_i) = \frac{P(y_i | x_i, w) \cdot p(w | x_i)}{P(x_i, y_i)}  p(w) \neq p(w | x_i) P(y_i | x_i, w) P(\mathcal{D} | w)","['probability', 'probability-theory', 'statistics', 'bayesian', 'maximum-likelihood']"
31,Cramér-Rao lower bound - Estimator is independent of the parameter,Cramér-Rao lower bound - Estimator is independent of the parameter,,"I have followed the Cramér-Rao lower bound (CRLB) derivation, and I couldn't figure out why - If $f(x; \theta)$ be a probability density with continuous parameter $\theta$ , and $X_1, \dots, X_n$ be independent random variables with density $f(x; \theta)$ , and $\Theta(X_1, \dots ,X_n)$ be an unbiased estimator of $\theta$ . Why does the estimator, $\Theta$ , is independent of $\theta$ (the param to be estimated)? $\Theta$ is a function of $X_1, \dots, X_n$ , and in the pdf of each one of them $\theta$ appears as a param. Doesn't it imply that $\Theta$ is also dependent on $\theta$ ?","I have followed the Cramér-Rao lower bound (CRLB) derivation, and I couldn't figure out why - If be a probability density with continuous parameter , and be independent random variables with density , and be an unbiased estimator of . Why does the estimator, , is independent of (the param to be estimated)? is a function of , and in the pdf of each one of them appears as a param. Doesn't it imply that is also dependent on ?","f(x; \theta) \theta X_1, \dots, X_n f(x; \theta) \Theta(X_1, \dots ,X_n) \theta \Theta \theta \Theta X_1, \dots, X_n \theta \Theta \theta","['statistics', 'statistical-inference', 'independence', 'estimation']"
32,Calculating the asymptotic normality result of a MLE from a skew-logistic distribution,Calculating the asymptotic normality result of a MLE from a skew-logistic distribution,,"Suppose we have $X$ with cumulative distribution function $F_X(x) = (1-e^{-x})^\frac{1}{\theta}$ where $x \geq 0, \theta > 0$ . How can one find a MLE for $\theta$ from this and the asymptotic normality result? We get the density function as $f_X(x;\theta) = \frac{1}{\theta}(1-e^{-x})^{\frac{1}{\theta}-1}e^{-x}$ . $\textbf{EDIT : } $ I found the MLE to be $$\hat{\theta} = -\frac{1}{n}\sum^n_{i=1} \ln(1-e^{-x_i}).$$ Now how do we get the asymptotic normality result? I tried calculating the Fisher information number but I'm stuck at $$I(\theta) = -E\left[\frac{\partial^2}{\partial \theta^2} \ln f(x,\theta) \right] = \frac{-1}{\theta^2}+ \frac{2}{\theta^3}E[\ln(1-e^{-x})]$$",Suppose we have with cumulative distribution function where . How can one find a MLE for from this and the asymptotic normality result? We get the density function as . I found the MLE to be Now how do we get the asymptotic normality result? I tried calculating the Fisher information number but I'm stuck at,"X F_X(x) = (1-e^{-x})^\frac{1}{\theta} x \geq 0, \theta > 0 \theta f_X(x;\theta) = \frac{1}{\theta}(1-e^{-x})^{\frac{1}{\theta}-1}e^{-x} \textbf{EDIT : }  \hat{\theta} = -\frac{1}{n}\sum^n_{i=1} \ln(1-e^{-x_i}). I(\theta) = -E\left[\frac{\partial^2}{\partial \theta^2} \ln f(x,\theta) \right] = \frac{-1}{\theta^2}+ \frac{2}{\theta^3}E[\ln(1-e^{-x})]","['statistics', 'maximum-likelihood', 'parameter-estimation']"
33,Bivariate negative binomial distribution for 2d count data,Bivariate negative binomial distribution for 2d count data,,"Bivariate negative binomial distribution The probability mass function (PMF) of a bivariate negative binomial distribution ( $\mathsf{BNBin}$ ) is given by [1]: $$P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y $$ where $a, p_0, p_1, p_2 > 0$ and $p_0 + p_1 + p_2 = 1$ . It can be shown that the marginal $P(X)$ of this distribution follows a negative binomial ( $\mathsf{NBin}$ ) with parameters $r=a$ and $p=\frac{p_1}{1 - p_2}$ (and similarly $r=a$ and $p=\frac{p_2}{1 - p_1}$ for $P(Y)$ ). Small motivating example Consider the following 2d count dataset: $X$ 0 0 1 1 1 1 2 2 3 3 4 4 4 4 4 5 5 7 8 9 $Y$ 7 6 5 5 4 3 3 3 3 3 3 2 2 2 2 1 0 0 0 0 The marginal distributions are both $\mathsf{NBin}$ : However, the joint distribution $P(X, Y)$ cannot effectively be modeled as a $\mathsf{BNBin}$ . This is because the covariance of $\mathsf{BNBin}$ , $\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \frac{ap_1p_2}{p_0}$ , is always positive, but the variables $X$ and $Y$ are negatively correlated (the sample covariance is approximately -0.77). Questions Under what conditions is the $\mathsf{BNBin}$ distribution suitable for 2d count data? Is there any other 2d distribution for count data that generalises $\mathsf{BNBin}$ ? If not, how could $\mathsf{BNBin}$ be extended to deal with these failure cases? (e.g. negative correlation between both variables) Any comment or suggestion will be greatly appreciated. References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf )","Bivariate negative binomial distribution The probability mass function (PMF) of a bivariate negative binomial distribution ( ) is given by [1]: where and . It can be shown that the marginal of this distribution follows a negative binomial ( ) with parameters and (and similarly and for ). Small motivating example Consider the following 2d count dataset: 0 0 1 1 1 1 2 2 3 3 4 4 4 4 4 5 5 7 8 9 7 6 5 5 4 3 3 3 3 3 3 2 2 2 2 1 0 0 0 0 The marginal distributions are both : However, the joint distribution cannot effectively be modeled as a . This is because the covariance of , , is always positive, but the variables and are negatively correlated (the sample covariance is approximately -0.77). Questions Under what conditions is the distribution suitable for 2d count data? Is there any other 2d distribution for count data that generalises ? If not, how could be extended to deal with these failure cases? (e.g. negative correlation between both variables) Any comment or suggestion will be greatly appreciated. References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf )","\mathsf{BNBin} P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y  a, p_0, p_1, p_2 > 0 p_0 + p_1 + p_2 = 1 P(X) \mathsf{NBin} r=a p=\frac{p_1}{1 - p_2} r=a p=\frac{p_2}{1 - p_1} P(Y) X Y \mathsf{NBin} P(X, Y) \mathsf{BNBin} \mathsf{BNBin} \text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \frac{ap_1p_2}{p_0} X Y \mathsf{BNBin} \mathsf{BNBin} \mathsf{BNBin}","['probability', 'statistics', 'discrete-mathematics', 'probability-distributions']"
34,Maximum-likelihood estimator resulting in complex estimator,Maximum-likelihood estimator resulting in complex estimator,,"I'm trying to find the maximum likelihood estimator of a random sample $X=(X_1,\dots,X_n)$ from a distribution with pdf $$f(x;\lambda)=\frac{2}{\lambda x \sqrt{2 \pi}} \exp\left({\frac{[ -\log(x)]^2}{2 \lambda^2}}\right), \lambda >0, 0 \leq x \leq 1$$ Until now i have calculated the joint distribution function $$f_X(\underline{x};\lambda)=\left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n \prod^n_{i=1}\frac{1}{x_i} \exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right)$$ The likelihood function $$L(\lambda,\underline{x}) \propto \left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n\exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right)$$ The log-likelihood function $$l(\lambda,\underline{x})=(n \log(2) - n\log(\lambda)-\frac{1}{2}\log(2 \pi) + \frac{1}{2 \lambda^2} \sum_{i=1}^n[-\log(x_i)]^2$$ To find the maximum of the log-likelihood function i want to find the point $\hat{\lambda}$ where $l'(\lambda,\underline{x})=0$ and then prove that $l''(\lambda,\underline{x})|_{\lambda = \hat{\lambda}}<0$ . I have calculated the following derivatives: $$l'(\lambda;\underline{x}) = -\frac{n}{\lambda} - \frac{1}{\lambda^3}\sum_{i=1}^n[-\log(x_i)]^2$$ $$l''(\lambda,\underline{x})=\frac{1}{\lambda^2}\left( n + \frac{1}{\lambda^2} \sum_{i=1}^n[-\log(x_i)]^2\right)$$ I find $$\hat{\lambda}= \pm \sqrt{-\frac{1}{n}\sum_{i=1}^n[-\log(x_i)]^2}$$ Which would result in a complex estimator. Are my calculations off in any way?",I'm trying to find the maximum likelihood estimator of a random sample from a distribution with pdf Until now i have calculated the joint distribution function The likelihood function The log-likelihood function To find the maximum of the log-likelihood function i want to find the point where and then prove that . I have calculated the following derivatives: I find Which would result in a complex estimator. Are my calculations off in any way?,"X=(X_1,\dots,X_n) f(x;\lambda)=\frac{2}{\lambda x \sqrt{2 \pi}} \exp\left({\frac{[ -\log(x)]^2}{2 \lambda^2}}\right), \lambda >0, 0 \leq x \leq 1 f_X(\underline{x};\lambda)=\left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n \prod^n_{i=1}\frac{1}{x_i} \exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right) L(\lambda,\underline{x}) \propto \left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n\exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right) l(\lambda,\underline{x})=(n \log(2) - n\log(\lambda)-\frac{1}{2}\log(2 \pi) + \frac{1}{2 \lambda^2} \sum_{i=1}^n[-\log(x_i)]^2 \hat{\lambda} l'(\lambda,\underline{x})=0 l''(\lambda,\underline{x})|_{\lambda = \hat{\lambda}}<0 l'(\lambda;\underline{x}) = -\frac{n}{\lambda} - \frac{1}{\lambda^3}\sum_{i=1}^n[-\log(x_i)]^2 l''(\lambda,\underline{x})=\frac{1}{\lambda^2}\left( n + \frac{1}{\lambda^2} \sum_{i=1}^n[-\log(x_i)]^2\right) \hat{\lambda}= \pm \sqrt{-\frac{1}{n}\sum_{i=1}^n[-\log(x_i)]^2}","['statistics', 'probability-distributions', 'maximum-likelihood']"
35,Cubic expectation of peanuts on squares,Cubic expectation of peanuts on squares,,"There are 9 squares on the table, and we drop 9 peanuts to the table one at a time. Each peanut fall into every square with equal probability 1/9. After dropping all peanuts, we denote $x_{i}$ as the number of peanuts on the i-th square, i=1,2,3...9, and define Y = $x_{1}^{3}+x_{2}^{3}+...+x_{9}^{3}$ . Calculate expectation of Y. Updates: Based on linearity of expectation, permutation symmetry of $x_{i}$ and $x_{i}$ obeys Binomial distribution $$E[Y]=\sum_{i=1}^{9}{E[X_{i}^3]}=\sum_{i=1}^{9}{n(n-1)(n-2)p^3+3n(n-1)p^2+np}$$ Given n=9 and p=1/9 for binomial distribution, we can derive: $$E[Y]=9*(4+29/81)=39\frac{2}{9}$$ This answer should lie between $Y_{min}=9$ and $Y_{max}=9^3=729$ . For third order moment of binomial distribution, refer to The 3rd raw moment of a binomial distribution","There are 9 squares on the table, and we drop 9 peanuts to the table one at a time. Each peanut fall into every square with equal probability 1/9. After dropping all peanuts, we denote as the number of peanuts on the i-th square, i=1,2,3...9, and define Y = . Calculate expectation of Y. Updates: Based on linearity of expectation, permutation symmetry of and obeys Binomial distribution Given n=9 and p=1/9 for binomial distribution, we can derive: This answer should lie between and . For third order moment of binomial distribution, refer to The 3rd raw moment of a binomial distribution",x_{i} x_{1}^{3}+x_{2}^{3}+...+x_{9}^{3} x_{i} x_{i} E[Y]=\sum_{i=1}^{9}{E[X_{i}^3]}=\sum_{i=1}^{9}{n(n-1)(n-2)p^3+3n(n-1)p^2+np} E[Y]=9*(4+29/81)=39\frac{2}{9} Y_{min}=9 Y_{max}=9^3=729,"['probability', 'statistics']"
36,Find the probability of a face of a loaded die.,Find the probability of a face of a loaded die.,,"I have a die, for which all faces have the same probability, except for one face. I know in advance which face is different, let's say it's the 6. My goal is to find the probability of that face by tossing the die $n$ times and counting how many times it lands on the loaded face. Let's suppose I rolled the die $n = 1000$ times and it landed $k = 250$ times on the 6. The obvious answer would be that the probability of landing on the 6 is $\frac{k}{n} = \frac{1}{4}$ . But the real probability ( $\frac{k}{n}$ after infinitely many tosses) could be different. It could also be only $\frac{1}{8}$ but I got extremely lucky in this case and the die landed 250 out of 1000 times on the 6. However, this would be extremely unlikely, but how unlikely? How to calculate the probability, that x% is the real probability of the die? I'd like to have a function f(x) = probability that x is the real probability of the die , with the parameters $n$ and $k$ . I think the maximum of this function should be at $\frac{k}{n}$ , but I'm not sure exactly how the function would look. Also, I'd like to give upper and lower bounds on the probability after $n$ tosses. I know that it's technically impossible since the die could land $k$ times on the 6 no matter what the real probability is (except if it's 0% or 100%). However, I could set the bounds in a way that they ignore extremely improbable events. So, I want to say that the real probability of the die is between a% and b%, with a probability of c%. I believe this can be solved using integrals, but how?","I have a die, for which all faces have the same probability, except for one face. I know in advance which face is different, let's say it's the 6. My goal is to find the probability of that face by tossing the die times and counting how many times it lands on the loaded face. Let's suppose I rolled the die times and it landed times on the 6. The obvious answer would be that the probability of landing on the 6 is . But the real probability ( after infinitely many tosses) could be different. It could also be only but I got extremely lucky in this case and the die landed 250 out of 1000 times on the 6. However, this would be extremely unlikely, but how unlikely? How to calculate the probability, that x% is the real probability of the die? I'd like to have a function f(x) = probability that x is the real probability of the die , with the parameters and . I think the maximum of this function should be at , but I'm not sure exactly how the function would look. Also, I'd like to give upper and lower bounds on the probability after tosses. I know that it's technically impossible since the die could land times on the 6 no matter what the real probability is (except if it's 0% or 100%). However, I could set the bounds in a way that they ignore extremely improbable events. So, I want to say that the real probability of the die is between a% and b%, with a probability of c%. I believe this can be solved using integrals, but how?",n n = 1000 k = 250 \frac{k}{n} = \frac{1}{4} \frac{k}{n} \frac{1}{8} n k \frac{k}{n} n k,"['probability', 'statistics', 'probability-distributions']"
37,What indicated that order matters in this question?,What indicated that order matters in this question?,,"A journalist from the local newspaper interviews a randomly selected group of 3 medal winners. (d) Find the exact probability that there is at least one gold medal winner in the group. Background; International A-Level S1 June 2020 Q3. The question was about a contest where if you jump some distance, you get a medal. Further, 1/3 of those people who got a medal and jumped past a larger distance, got a gold medal. Therefore $$P(G)=\frac{1}{3} \text{ and } P(G')=\frac{2}{3}$$ *within the set of people who already got a medal which is what part d) considers. For the possible combinations to part d), I wrote out $$\text{GG'G'}\\\text{GGG'}\\\text{GGG}$$ And thought that order in which G or G' in the first two combinations was placed was irrelevant, because, when you pick a group of 3, as long as there is one person with gold, then it doesn't matter where that person ""stands"" in the order; the important bit is just that they are there . But the MS takes into account the order. Well, they do $$1-P(\text{G'G'G'})$$ which is a lot easier, yes... but I want to know, given that I'm writing out all the combinations, What indication is there in the question that involved taking into account order? *part d) is copied word-to-word from the past paper; that is all that is asked.","A journalist from the local newspaper interviews a randomly selected group of 3 medal winners. (d) Find the exact probability that there is at least one gold medal winner in the group. Background; International A-Level S1 June 2020 Q3. The question was about a contest where if you jump some distance, you get a medal. Further, 1/3 of those people who got a medal and jumped past a larger distance, got a gold medal. Therefore *within the set of people who already got a medal which is what part d) considers. For the possible combinations to part d), I wrote out And thought that order in which G or G' in the first two combinations was placed was irrelevant, because, when you pick a group of 3, as long as there is one person with gold, then it doesn't matter where that person ""stands"" in the order; the important bit is just that they are there . But the MS takes into account the order. Well, they do which is a lot easier, yes... but I want to know, given that I'm writing out all the combinations, What indication is there in the question that involved taking into account order? *part d) is copied word-to-word from the past paper; that is all that is asked.",P(G)=\frac{1}{3} \text{ and } P(G')=\frac{2}{3} \text{GG'G'}\\\text{GGG'}\\\text{GGG} 1-P(\text{G'G'G'}),"['probability', 'statistics']"
38,How do you measure the 'explained variance' of arbitrary linear embeddings?,How do you measure the 'explained variance' of arbitrary linear embeddings?,,"Question Elaboration: When I say 'linear embeddings' I mean a lower-dimensional representation of variables resulting from an arbitrary linear transformation. And when I say 'explained variance' I mean in the sense that 'explained variance' is measured for PCA's principal components. Context of Confusion: I realized I'm confused about exactly what this means because while on the surface the idea of 'explained variance' from lower-dimensional encodings makes sense. I've come to wonder if rigorously speaking it is at all possible for anything but a model to 'explain variance' of original variables (i.e. R^2 measure) ... Attempt 1 to draw connection to PCA: For example This website says that total variance for PCA is sum of individual variances of the PCA variables, and then explained variance for each PC is portion of that total variance present in a PC. I don't see how this could generalize to arbitrary lower dimensional representations because you could just scale some of the lower dimensional variables arbitrarily and change which represent more of the variance no? Attempt 2 to draw connection to PCA: I've also seen on numerous websites (e.g. here ) that the explained variance for a PCA variable is the corresponding eigen-value but I'm not sure what the analogue to arbitrary lower-dimensional linear representations would be...","Question Elaboration: When I say 'linear embeddings' I mean a lower-dimensional representation of variables resulting from an arbitrary linear transformation. And when I say 'explained variance' I mean in the sense that 'explained variance' is measured for PCA's principal components. Context of Confusion: I realized I'm confused about exactly what this means because while on the surface the idea of 'explained variance' from lower-dimensional encodings makes sense. I've come to wonder if rigorously speaking it is at all possible for anything but a model to 'explain variance' of original variables (i.e. R^2 measure) ... Attempt 1 to draw connection to PCA: For example This website says that total variance for PCA is sum of individual variances of the PCA variables, and then explained variance for each PC is portion of that total variance present in a PC. I don't see how this could generalize to arbitrary lower dimensional representations because you could just scale some of the lower dimensional variables arbitrarily and change which represent more of the variance no? Attempt 2 to draw connection to PCA: I've also seen on numerous websites (e.g. here ) that the explained variance for a PCA variable is the corresponding eigen-value but I'm not sure what the analogue to arbitrary lower-dimensional linear representations would be...",,"['statistics', 'variance', 'principal-component-analysis']"
39,Linear Model and Posterior distribution,Linear Model and Posterior distribution,,"Consider the following linear model: $$ \mathbf{y} = A \mathbf{x} + \sigma\mathbf{z}, $$ where $\mathbf{x} \in \{0,1\}^n$ is an unknown signal, to be recovered; $A \in \mathbb{R}^{m \times n}$ is a (known) linear measurement matrix; $\sigma > 0$ ; and $\mathbf{z} \in \mathbb{R}^{m}$ is i.i.d. Gaussian noise: $z_1,\dots,z_m \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)$ . Assume a sparse binary prior for $\mathbf{x}$ . Specifically, let $k$ be the expected sparsity and denote $\rho = \frac{k}{n}$ . The coordinates of $\mathbf{x}$ are assumed i.i.d. Bernoulli random variables: $$ x_1,\dots,x_n \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(\rho). $$ I want to determine the posterior distribution $$ \mathbb{P}(\mathbf{x} \mid \mathbf{y}) .  $$ My attempt was to use Bayes theorem, i.e., $$ \mathbb{P}(\mathbf{x} \mid \mathbf{y}) = \frac{\mathbb{P}(\mathbf{y} \mid \mathbf{x}) \cdot \mathbb{P}(\mathbf{x})}{\mathbb{P}(\mathbf{y})}. $$ Now $\mathbb{P}(\mathbf{x})$ is given by $$ \Pi_{i=1}^n \rho^{x_i}(1 - \rho)^{1-x_i} $$ I'm having trouble determining the $\mathbb{P}(\mathbf{y} \mid \mathbf{x})$ and $\mathbb{P}(\mathbf{y})$ . I would be very grateful for any help.","Consider the following linear model: where is an unknown signal, to be recovered; is a (known) linear measurement matrix; ; and is i.i.d. Gaussian noise: . Assume a sparse binary prior for . Specifically, let be the expected sparsity and denote . The coordinates of are assumed i.i.d. Bernoulli random variables: I want to determine the posterior distribution My attempt was to use Bayes theorem, i.e., Now is given by I'm having trouble determining the and . I would be very grateful for any help.","
\mathbf{y} = A \mathbf{x} + \sigma\mathbf{z},
 \mathbf{x} \in \{0,1\}^n A \in \mathbb{R}^{m \times n} \sigma > 0 \mathbf{z} \in \mathbb{R}^{m} z_1,\dots,z_m \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0,1) \mathbf{x} k \rho = \frac{k}{n} \mathbf{x} 
x_1,\dots,x_n \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(\rho).
 
\mathbb{P}(\mathbf{x} \mid \mathbf{y}) . 
 
\mathbb{P}(\mathbf{x} \mid \mathbf{y}) = \frac{\mathbb{P}(\mathbf{y} \mid \mathbf{x}) \cdot \mathbb{P}(\mathbf{x})}{\mathbb{P}(\mathbf{y})}.
 \mathbb{P}(\mathbf{x}) 
\Pi_{i=1}^n \rho^{x_i}(1 - \rho)^{1-x_i}
 \mathbb{P}(\mathbf{y} \mid \mathbf{x}) \mathbb{P}(\mathbf{y})","['statistics', 'bayesian']"
40,Application of Slutsky theroem,Application of Slutsky theroem,,"If $X_1,...,X_n$ are independent (no need to be  identically distributed), $\sigma_n=Var(\sum_jX_j)$ $$ \sum_{j=1}^n(X_j-EX_j)/\sigma_n \overset{d}{\rightarrow} N(0,1) $$ then prove: $$ n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{P}{\rightarrow}0 \Leftrightarrow \lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0 $$ “First Edit” For the "" $\Rightarrow$ "" part, It seems if "" $\color{red}{X_n\overset{P}{\rightarrow}0,Y_n\overset{d}{\rightarrow}N(0,1) \Rightarrow X_n/Y_n\overset{d}{\rightarrow}0}$ "" is succeed, then I can naturally deduce that conclusion that $\lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0$ , but I'm not sure if the red part is right. And for the "" $\Leftarrow$ "" part, $\lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0$ can deduce that $\frac{\sigma_n}{n}\overset{P}{\rightarrow}0$ , so using the slutsky theorem, I have: $$ n^{-1}\sum_{j=1}^n(X_j-EX_j)=\sum_{j=1}^n(X_j-EX_j)/\sigma_n  \cdot \frac{\sigma_n}{n}\overset{P}{\rightarrow}0 $$ Am I right about the above contents? Please help me, thanks a lot! “Second Edit” With exploring deeper in this question, I find a theroem goes like: $$ \text{If g is a continuous mapping, and if } X_n\overset{d}{\rightarrow}X, \text{then } g(X_n)\overset{d}{\rightarrow} g(X) $$ I'm not sure if I can use this theorem in this problem as follow. Let $g(x)=1/x$ , obviously $g$ is a continuous function. Let $Z_n=\sum_{j=1}^n(X_j-EX_j)/\sigma_n$ and $Z\sim N(0,1)$ , then according to to the condition, I have $Z_n\overset{d}{\rightarrow} Z$ , so applying the theorem, I get: $$ g(Z_n)=\frac{\sigma_n}{\sum_{j=1}^n(X_j-EX_j)}\overset{d}{\rightarrow}\frac{1}{Z}=g(Z) $$ and then using the Slutsky theorem, $$ \frac{\sigma_n}{n}=g(Z_n)\cdot n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{d}{\rightarrow} g(Z)\cdot 0 = 0 $$ so $\frac{\sigma_n}{n}\rightarrow 0$ . Can this work? I'm looking forward to your reply, Thanks!","If are independent (no need to be  identically distributed), then prove: “First Edit” For the "" "" part, It seems if "" "" is succeed, then I can naturally deduce that conclusion that , but I'm not sure if the red part is right. And for the "" "" part, can deduce that , so using the slutsky theorem, I have: Am I right about the above contents? Please help me, thanks a lot! “Second Edit” With exploring deeper in this question, I find a theroem goes like: I'm not sure if I can use this theorem in this problem as follow. Let , obviously is a continuous function. Let and , then according to to the condition, I have , so applying the theorem, I get: and then using the Slutsky theorem, so . Can this work? I'm looking forward to your reply, Thanks!","X_1,...,X_n \sigma_n=Var(\sum_jX_j) 
\sum_{j=1}^n(X_j-EX_j)/\sigma_n \overset{d}{\rightarrow} N(0,1)
 
n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{P}{\rightarrow}0 \Leftrightarrow \lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0
 \Rightarrow \color{red}{X_n\overset{P}{\rightarrow}0,Y_n\overset{d}{\rightarrow}N(0,1) \Rightarrow X_n/Y_n\overset{d}{\rightarrow}0} \lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0 \Leftarrow \lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0 \frac{\sigma_n}{n}\overset{P}{\rightarrow}0 
n^{-1}\sum_{j=1}^n(X_j-EX_j)=\sum_{j=1}^n(X_j-EX_j)/\sigma_n  \cdot \frac{\sigma_n}{n}\overset{P}{\rightarrow}0
 
\text{If g is a continuous mapping, and if } X_n\overset{d}{\rightarrow}X, \text{then } g(X_n)\overset{d}{\rightarrow} g(X)
 g(x)=1/x g Z_n=\sum_{j=1}^n(X_j-EX_j)/\sigma_n Z\sim N(0,1) Z_n\overset{d}{\rightarrow} Z 
g(Z_n)=\frac{\sigma_n}{\sum_{j=1}^n(X_j-EX_j)}\overset{d}{\rightarrow}\frac{1}{Z}=g(Z)
 
\frac{\sigma_n}{n}=g(Z_n)\cdot n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{d}{\rightarrow} g(Z)\cdot 0 = 0
 \frac{\sigma_n}{n}\rightarrow 0","['statistics', 'self-learning', 'statistical-inference']"
41,Expected value for a combination of density functions,Expected value for a combination of density functions,,"Let $f_1,...,f_k$ density functions with continuous random variables $X_1,...,X_k$ and define $$g(x):=\dfrac{f_1(x)+...+f_k(x)}{k}.\forall x\in\mathbb{R}$$ If $E[X_j]=j$ , for $j=1,...,k$ and Y is a random variable with density $g$ . Find $E[Y]$ Is it ok to do $$E[Y]=\int_{-\infty}^{\infty}yg(y)dy=\dfrac{1}{k} (\int_{-\infty}^{\infty}x_1f_1(x_1)dx_1+...+\int_{-\infty}^{\infty}x_kf_k(x_k)dx_k)$$ $$=\dfrac{1}{k}(E[X_1]+...+E[X_k])=\dfrac{1}{k}(1+...+k)=\dfrac{k(k+1)}{2k}$$ ?? Thank you.","Let density functions with continuous random variables and define If , for and Y is a random variable with density . Find Is it ok to do ?? Thank you.","f_1,...,f_k X_1,...,X_k g(x):=\dfrac{f_1(x)+...+f_k(x)}{k}.\forall x\in\mathbb{R} E[X_j]=j j=1,...,k g E[Y] E[Y]=\int_{-\infty}^{\infty}yg(y)dy=\dfrac{1}{k}
(\int_{-\infty}^{\infty}x_1f_1(x_1)dx_1+...+\int_{-\infty}^{\infty}x_kf_k(x_k)dx_k) =\dfrac{1}{k}(E[X_1]+...+E[X_k])=\dfrac{1}{k}(1+...+k)=\dfrac{k(k+1)}{2k}","['statistics', 'random-variables', 'expected-value', 'density-function']"
42,Constructing an LR test for a concrete example and find its critical areas and power function,Constructing an LR test for a concrete example and find its critical areas and power function,,"I am working on the following exercise: We are given an observation of a discrete RV $X$ with PMF $f(x \mid \theta)$ and $\theta \in \{0,1,2\}$ as in the table below. Find the LR test for the hypothesis $H_0: \theta = 0$ and list all possible critical areas for such an LR test. From there take a level $\alpha$ -test for $\alpha = 0.15$ and find its power function $\beta(\theta)$ . $$\begin{pmatrix} x &f(x \mid 0) &f(x \mid 1) &f(x \mid 2) \\ 1 &3/4  &1/4 &1/3 \\ 2 &1/8 &1/8 &1/3 \\ 3 &1/8 &1/2 &1/6 \\ 4 &0 &1/8 &1/6 \\ \end{pmatrix}$$ I am new to LR tests and can not quite see through its definition yet. Here is what I got so far: We defined the test statistic for the LR test as $$\lambda(X) := \frac{L(\hat{\theta_0} \mid X)}{L(\hat{\theta} \ \mid X)},$$ where $L$ is the likelihood function. If I am not mistaken we should have: $$\lambda(X) = \begin{cases}         \frac{3/4}{3/4} = 1, & \text{for } X = 1 \\         \frac{1/8}{1/3} = 3/8, & \text{for } X = 2\\         \frac{1/8}{1/2} = 1/4, & \text{for } X = 3\\         \frac{0}{1/6} = 0, & \text{for } X = 4.         \end{cases}  $$ For the LR tests we defined the critical area $K$ as $K := \{x \in \mathcal{X} \ \mid \lambda(x) < k\}$ , where $\mathcal{X}$ is the sample space. In the lecture we then said that we need to find some $k$ such that $\sup_{\theta \in \Theta_0} P_{\theta}(\lambda < k) \le \alpha$ , however, I do not see how I should do this here. Could you please help me?","I am working on the following exercise: We are given an observation of a discrete RV with PMF and as in the table below. Find the LR test for the hypothesis and list all possible critical areas for such an LR test. From there take a level -test for and find its power function . I am new to LR tests and can not quite see through its definition yet. Here is what I got so far: We defined the test statistic for the LR test as where is the likelihood function. If I am not mistaken we should have: For the LR tests we defined the critical area as , where is the sample space. In the lecture we then said that we need to find some such that , however, I do not see how I should do this here. Could you please help me?","X f(x \mid \theta) \theta \in \{0,1,2\} H_0: \theta = 0 \alpha \alpha = 0.15 \beta(\theta) \begin{pmatrix}
x &f(x \mid 0) &f(x \mid 1) &f(x \mid 2) \\
1 &3/4  &1/4 &1/3 \\
2 &1/8 &1/8 &1/3 \\
3 &1/8 &1/2 &1/6 \\
4 &0 &1/8 &1/6 \\
\end{pmatrix} \lambda(X) := \frac{L(\hat{\theta_0} \mid X)}{L(\hat{\theta} \ \mid X)}, L \lambda(X) = \begin{cases}
        \frac{3/4}{3/4} = 1, & \text{for } X = 1 \\
        \frac{1/8}{1/3} = 3/8, & \text{for } X = 2\\
        \frac{1/8}{1/2} = 1/4, & \text{for } X = 3\\
        \frac{0}{1/6} = 0, & \text{for } X = 4.
        \end{cases}
  K K := \{x \in \mathcal{X} \ \mid \lambda(x) < k\} \mathcal{X} k \sup_{\theta \in \Theta_0} P_{\theta}(\lambda < k) \le \alpha",['statistics']
43,Binomial distribution exact,Binomial distribution exact,,"From a box containing 20 balls, 14 of them black and 6 white, we are drawing four balls with replacements. What is the probability, that black ball will be drawn twice? is my solution right? $n = 4, k = 2, p = 7/10$ using the formula: i got $P(X = 2) = 0,2646$","From a box containing 20 balls, 14 of them black and 6 white, we are drawing four balls with replacements. What is the probability, that black ball will be drawn twice? is my solution right? using the formula: i got","n = 4, k = 2, p = 7/10 P(X = 2) = 0,2646","['probability', 'statistics', 'solution-verification', 'binomial-distribution']"
44,Hard problem to show almost sure boundedness,Hard problem to show almost sure boundedness,,"Let us assume that $X_i = \theta t_i + e_i, \ \ i=1,2,...,n$ where $\theta \in \Theta$ which is unknown and $\Theta$ is a closed subset of $\mathbb{R}$ . Suppose $e_i$ 's are i.i.d. on the interval $[-\tau,\tau]$ with unknown $\tau >0$ and $E(e_i)=0$ . Given $t_i$ 's are fixed constants let $$T_n= S_n(\tilde{\theta})=\min_{\gamma \in \Theta} S_n(\gamma)$$ where $$S_n(\gamma) = 2 \max_{i \le n} \frac{|X_i - \gamma t_i|}{\sqrt{1+\gamma^2}}.$$ If we assume that $\sup_{i} |t_i| < \infty$ and $\sup_i t_i - \inf_i t_i > 2\tau$ show that $\{\tilde{\theta_n}, n =1,2,...\}$ is bounded almost surely. I have really no clue how to tackle this one as there is too much detail. Judging by the expression of $S_n(\gamma)$ , it looks like the perpendicular distance of a straight line from a point and it feels that out of $t_1,...,t_n$ , I need to find that $t_i$ which is the most distance from the straight line $X_i = \gamma t_i$ . Can anyone have a crack at this? Another follow-up question : Under the same assumptions mentioned, can we show that $T_n$ is a strongly consistent estimator of $\nu = \min_{\gamma \in \Theta} S(\gamma)$ , where $S(\gamma)=\lim_{n \rightarrow \infty} S_n(\gamma)$ almost surely? By strong consistency of an estimator, you can think of almost sure convergence of the estimator to the parameter of interest. Again, I could show that for any sequence $\theta_n \rightarrow \theta$ , $S_n(\theta_n)-S_n(\theta)=O(|\theta_n-\theta|)$ almost surely. But how can I use this fact to show the almost sure convergence of $T_n$ to $\nu$ ?","Let us assume that where which is unknown and is a closed subset of . Suppose 's are i.i.d. on the interval with unknown and . Given 's are fixed constants let where If we assume that and show that is bounded almost surely. I have really no clue how to tackle this one as there is too much detail. Judging by the expression of , it looks like the perpendicular distance of a straight line from a point and it feels that out of , I need to find that which is the most distance from the straight line . Can anyone have a crack at this? Another follow-up question : Under the same assumptions mentioned, can we show that is a strongly consistent estimator of , where almost surely? By strong consistency of an estimator, you can think of almost sure convergence of the estimator to the parameter of interest. Again, I could show that for any sequence , almost surely. But how can I use this fact to show the almost sure convergence of to ?","X_i = \theta t_i + e_i, \ \ i=1,2,...,n \theta \in \Theta \Theta \mathbb{R} e_i [-\tau,\tau] \tau >0 E(e_i)=0 t_i T_n= S_n(\tilde{\theta})=\min_{\gamma \in \Theta} S_n(\gamma) S_n(\gamma) = 2 \max_{i \le n} \frac{|X_i - \gamma t_i|}{\sqrt{1+\gamma^2}}. \sup_{i} |t_i| < \infty \sup_i t_i - \inf_i t_i > 2\tau \{\tilde{\theta_n}, n =1,2,...\} S_n(\gamma) t_1,...,t_n t_i X_i = \gamma t_i T_n \nu = \min_{\gamma \in \Theta} S(\gamma) S(\gamma)=\lim_{n \rightarrow \infty} S_n(\gamma) \theta_n \rightarrow \theta S_n(\theta_n)-S_n(\theta)=O(|\theta_n-\theta|) T_n \nu","['probability-theory', 'statistics', 'random-variables', 'asymptotics', 'statistical-inference']"
45,Laplace and Orlicz characterizations of sub-Gaussianity,Laplace and Orlicz characterizations of sub-Gaussianity,,"Many different characterizations of sub-Gaussianity for a centred r.v. X exist including the ""Laplace formulation"" where $\exists \sigma > 0$ such that $$ \mathbb{E} \exp(tX) \le \exp(t^2 \sigma^2 / 2) $$ for all $t \in \Re$ , and the Orlicz norm formulation, that $\|X\|_{\psi_2} < \infty$ in $$ \|X\|_{\psi_2} := \inf \{t > 0 : \mathbb{E} \exp(X^2 / t^2) \le 2 \}. $$ I am interested in the relationship between $\sigma$ and $\|X\|_{\psi_2}$ , particularly as applied to tail bounds. Clearly this is not a bijection, because for $Z \sim N(0, 1)$ we have $\sigma_Z = 1$ and $\|Z\|_{\psi_2} = \sqrt{8/3} \approx 1.63$ by direct calculation (as setting $\mathbb{E}\exp(Z^2/t^2) = (1-2/t^2)^{-\frac12} = 2$ gives $t^2 = 8/3$ ), while for $Y$ with a Rademacher distribution $\sigma_Y = 1$ but $\|Y\|_{\psi_2} = 1/\sqrt{\log 2} \approx 1.20$ . Thus the Orlicz norm gives Chernoff bounds that are weaker and tighter respectively than those obtained through the Laplace formulation. Based on the equivalences reported in Wainwright (High Dimensional Statistics, p.47 or see p.39 in the draft here: https://www.stat.berkeley.edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf , implication I to IV) we have (since the Laplace condition implies $E \exp(s X^2 / 2\sigma^2) \le (1-s)^{-\frac12}$ ) the upper bound $$ \|X\|_{\psi_2} \le \sqrt{\frac{8}{3}} \, \sigma. $$ which is tight for $Z$ , i.e. a Gaussian r.v. maximizes the sub-Gaussian norm for fixed $\sigma$ . My question is if there is a reversed inequality, of the form $$ \sigma \le C \|X\|_{\psi_2} $$ and I am interested in the tightest possible numerical value of this constant (if it exists). Any other comments on the relationship between these formulations would also be much appreciated. I have tried manipulating the equivalences of sub-Gaussianity reported in Wainwright and Vershynin (High Dimensional Probability), but these appear to lead to upper bounds rather than the lower bounds I'm interested in.","Many different characterizations of sub-Gaussianity for a centred r.v. X exist including the ""Laplace formulation"" where such that for all , and the Orlicz norm formulation, that in I am interested in the relationship between and , particularly as applied to tail bounds. Clearly this is not a bijection, because for we have and by direct calculation (as setting gives ), while for with a Rademacher distribution but . Thus the Orlicz norm gives Chernoff bounds that are weaker and tighter respectively than those obtained through the Laplace formulation. Based on the equivalences reported in Wainwright (High Dimensional Statistics, p.47 or see p.39 in the draft here: https://www.stat.berkeley.edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf , implication I to IV) we have (since the Laplace condition implies ) the upper bound which is tight for , i.e. a Gaussian r.v. maximizes the sub-Gaussian norm for fixed . My question is if there is a reversed inequality, of the form and I am interested in the tightest possible numerical value of this constant (if it exists). Any other comments on the relationship between these formulations would also be much appreciated. I have tried manipulating the equivalences of sub-Gaussianity reported in Wainwright and Vershynin (High Dimensional Probability), but these appear to lead to upper bounds rather than the lower bounds I'm interested in.","\exists \sigma > 0  \mathbb{E} \exp(tX) \le \exp(t^2 \sigma^2 / 2)  t \in \Re \|X\|_{\psi_2} < \infty  \|X\|_{\psi_2} := \inf \{t > 0 : \mathbb{E} \exp(X^2 / t^2) \le 2 \}.  \sigma \|X\|_{\psi_2} Z \sim N(0, 1) \sigma_Z = 1 \|Z\|_{\psi_2} = \sqrt{8/3} \approx 1.63 \mathbb{E}\exp(Z^2/t^2) = (1-2/t^2)^{-\frac12} = 2 t^2 = 8/3 Y \sigma_Y = 1 \|Y\|_{\psi_2} = 1/\sqrt{\log 2} \approx 1.20 E \exp(s X^2 / 2\sigma^2) \le (1-s)^{-\frac12}  \|X\|_{\psi_2} \le \sqrt{\frac{8}{3}} \, \sigma.  Z \sigma  \sigma \le C \|X\|_{\psi_2} ","['probability', 'statistics', 'laplace-transform', 'orlicz-spaces']"
46,Car Auction with Uniformly distributed Value,Car Auction with Uniformly distributed Value,,"Suppose the true value $V$ of a car is uniformly distributed, between $0$ and $1000$ . You can bid any amount for the car, and if you bid the true value or more, then you pay your bid and get the car. You know a very good salesman, and are confident in your ability to sell the car for $50\%$ more than its true value. What should you bid to maximize your expected profit? My reasoning is this: The expected value of the car is $500$ based on the distribution we are given. Therefore, on average I will be selling it for $1.5*500 =750$ . This means that if I bid $501$ , then I should win more often than I lose. When I lose my payoff is $0$ and for the wins, my payoff should average out to be $750-501=249$ . I was however presented with the following solution, which contradicts mine. let our bid $=X.$ True value of car $=V.$ Expected value $= P(\text{win})*(\text{Payoff from winning}).$ $P(\text{win})=X/1000$ , since we win when our bid is above or at true value, and there are $x$ values less than or equal to our bid out of $1000.$ Payoff from winning $= 3/4(X)- X.$ $3/4\,x$ is the value I sell it for since if I win, that means that $X>V$ so $$V \sim \text{Unif}[0,X],$$ and therefore expected value of $V = 0+X/2=X/2$ . Selling price (as mentioned in question) $= 3/2*V=3/2\,(x/2)=3/4\,X.$ This suggests that the selling price will always be less than what we bid, so our expectation is negative. Both of the above solutions make sense to me, so I'm having a hard time understanding why one is wrong; appreciate any insight.","Suppose the true value of a car is uniformly distributed, between and . You can bid any amount for the car, and if you bid the true value or more, then you pay your bid and get the car. You know a very good salesman, and are confident in your ability to sell the car for more than its true value. What should you bid to maximize your expected profit? My reasoning is this: The expected value of the car is based on the distribution we are given. Therefore, on average I will be selling it for . This means that if I bid , then I should win more often than I lose. When I lose my payoff is and for the wins, my payoff should average out to be . I was however presented with the following solution, which contradicts mine. let our bid True value of car Expected value , since we win when our bid is above or at true value, and there are values less than or equal to our bid out of Payoff from winning is the value I sell it for since if I win, that means that so and therefore expected value of . Selling price (as mentioned in question) This suggests that the selling price will always be less than what we bid, so our expectation is negative. Both of the above solutions make sense to me, so I'm having a hard time understanding why one is wrong; appreciate any insight.","V 0 1000 50\% 500 1.5*500 =750 501 0 750-501=249 =X. =V. = P(\text{win})*(\text{Payoff from winning}). P(\text{win})=X/1000 x 1000. = 3/4(X)- X. 3/4\,x X>V V \sim \text{Unif}[0,X], V = 0+X/2=X/2 = 3/2*V=3/2\,(x/2)=3/4\,X.","['probability', 'statistics', 'expected-value']"
47,How to determine covariance while only knowing variances,How to determine covariance while only knowing variances,,"The question defines a series of random variables U, V, X, Y, and Z. X, Y, and Z are ""uncorrelated"". U = X + Z, V = Y + Z. The variances of X, Y, and Z are known (let's say for simplicity that they are 1, 2, and 3, respectively). The question is asking for the covariance of (U,V). I'm aware that I can compute cov(U,V) as cov(X+Z,Y+Z) = cov(X,Y) + cov(X,Z) + cov(Y,Z) + cov(Z,Z) and that cov(Z,Z) = var(Z) = 3. But I'm not aware of anything else I can be doing to work out these other covariances. I understand that covariance depends on expected values, and I have no information regarding any of the data, just given the variances themselves. Where do I go from here?","The question defines a series of random variables U, V, X, Y, and Z. X, Y, and Z are ""uncorrelated"". U = X + Z, V = Y + Z. The variances of X, Y, and Z are known (let's say for simplicity that they are 1, 2, and 3, respectively). The question is asking for the covariance of (U,V). I'm aware that I can compute cov(U,V) as cov(X+Z,Y+Z) = cov(X,Y) + cov(X,Z) + cov(Y,Z) + cov(Z,Z) and that cov(Z,Z) = var(Z) = 3. But I'm not aware of anything else I can be doing to work out these other covariances. I understand that covariance depends on expected values, and I have no information regarding any of the data, just given the variances themselves. Where do I go from here?",,"['statistics', 'variance', 'covariance']"
48,Show that $P(C)=P(A\cup B)P(C\mid A)-P(A\cap B)P(C'\mid A)$,Show that,P(C)=P(A\cup B)P(C\mid A)-P(A\cap B)P(C'\mid A),"Let $A,B,$ and $C$ be events defined in the samplespace $S$ , where $(A\cup B\cup C)\cap D=\emptyset$ and $ D\neq\emptyset$ $C \subset (A\cup B)$ and $C\cap A\cap B = A\cap B$ $A\cap B\neq \emptyset$ and $A\cap C\neq \emptyset$ and $B\cap C\neq \emptyset$ $P(C\mid A)=P(C\mid B)$ Draw the events in a Venn Diagram. Explain why $D$ and $E=A\cup B\cup C$ are disjoint. I have scetched a Venn Diagram like so: Explanation: Since we are given that $(A\cup B\cup C)\cap D=\emptyset$ , we know that $D$ and $E$ are disjoint, since $D\cup E=\emptyset\Rightarrow D$ and $E$ are disjoint! Now comes the part I'm having trouble with: Show that $P(C)=P(A\cup B)P(C\mid A)-P(A\cap B)P(C'\mid A)$ I started off with \begin{align*}   & P(A\cup B\cup C)=P(A)+P(B)+P(C)-P(A\cap B)-P(A\cap C)-P(B\cap C)+P(A\cap B\cap C) \\   & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B\cap C) \\   & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B) \\ \end{align*} (since we're given that $P(C\cap A\cap B) = P(A\cap B))$ ) \begin{align*} & P(C) = P(A\cup B\cup C)-P(A)-P(B)+\(\cancel{P(A\cap B)}\)+P(A\cap C)+P(B\cap C)-\(\cancel{P(A\cap B)}\) \\ & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A)+P(B)\\ & P(C) = P(A\cup B\cup C) \\ \end{align*} This is where I got stuck, then I thought I'd try something different: \begin{align*} & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B) \\ & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A)+P(B)-P(A\cap B)\\ & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cup B) \\ \end{align*} But I got stuck here as well. I tried to look at this question, but I didn't understand how to apply it to this exercise. How do I prove this? All help is appreciated!","Let and be events defined in the samplespace , where and and and and Draw the events in a Venn Diagram. Explain why and are disjoint. I have scetched a Venn Diagram like so: Explanation: Since we are given that , we know that and are disjoint, since and are disjoint! Now comes the part I'm having trouble with: Show that I started off with (since we're given that ) This is where I got stuck, then I thought I'd try something different: But I got stuck here as well. I tried to look at this question, but I didn't understand how to apply it to this exercise. How do I prove this? All help is appreciated!","A,B, C S (A\cup B\cup C)\cap D=\emptyset  D\neq\emptyset C \subset (A\cup B) C\cap A\cap B = A\cap B A\cap B\neq \emptyset A\cap C\neq \emptyset B\cap C\neq \emptyset P(C\mid A)=P(C\mid B) D E=A\cup B\cup C (A\cup B\cup C)\cap D=\emptyset D E D\cup E=\emptyset\Rightarrow D E P(C)=P(A\cup B)P(C\mid A)-P(A\cap B)P(C'\mid A) \begin{align*}
  & P(A\cup B\cup C)=P(A)+P(B)+P(C)-P(A\cap B)-P(A\cap C)-P(B\cap C)+P(A\cap B\cap C) \\
  & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B\cap C) \\
  & P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B) \\
\end{align*} P(C\cap A\cap B) = P(A\cap B)) \begin{align*}
& P(C) = P(A\cup B\cup C)-P(A)-P(B)+\(\cancel{P(A\cap B)}\)+P(A\cap C)+P(B\cap C)-\(\cancel{P(A\cap B)}\) \\
& P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A)+P(B)\\
& P(C) = P(A\cup B\cup C) \\
\end{align*} \begin{align*}
& P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cap C)+P(B\cap C)-P(A\cap B) \\
& P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A)+P(B)-P(A\cap B)\\
& P(C) = P(A\cup B\cup C)-P(A)-P(B)+P(A\cap B)+P(A\cup B) \\
\end{align*}","['probability', 'statistics', 'solution-verification']"
49,"If $X$ and $Y$ are independent standard normal random variables, and $U=X+Y$, $V=\frac{X}{Y}$ then $f_V(v) = \frac{1}{\pi (1+v^2)}$","If  and  are independent standard normal random variables, and ,  then",X Y U=X+Y V=\frac{X}{Y} f_V(v) = \frac{1}{\pi (1+v^2)},"If $X$ and $Y$ are independent standard normal random variables, and $U=X+Y$ , $V=\frac{X}{Y}$ prove that $$f_V(v) = \frac{1}{\pi (1+v^2)}$$ Solution: I know that $f_{xy}(x,y)=\frac{e^{-\frac{x^2+y^2}{2}}}{2\pi}$ Also, $$u=x+y \ \ \ \ \ \ AND \ \ \ \ \ \ v = \frac xy$$ Then $\frac{uv}{1+v}=x$ and $\frac{u}{v+1}=y$ Then the Jacobian would be $J=\frac{-u}{(v+1)^2}$ So: $$f_{u,v}=f_{xy}(h^{-1}(u,v),g^{-1}(u,v))*|J|=$$ $$\frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2}$$ My question now is, how can I prove that: $$f_V(v)=\int^{\infty}_{-\infty}\frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2}du=\frac{1}{\pi(1+v^2)}$$","If and are independent standard normal random variables, and , prove that Solution: I know that Also, Then and Then the Jacobian would be So: My question now is, how can I prove that:","X Y U=X+Y V=\frac{X}{Y} f_V(v) = \frac{1}{\pi (1+v^2)} f_{xy}(x,y)=\frac{e^{-\frac{x^2+y^2}{2}}}{2\pi} u=x+y \ \ \ \ \ \ AND \ \ \ \ \ \ v = \frac xy \frac{uv}{1+v}=x \frac{u}{v+1}=y J=\frac{-u}{(v+1)^2} f_{u,v}=f_{xy}(h^{-1}(u,v),g^{-1}(u,v))*|J|= \frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2} f_V(v)=\int^{\infty}_{-\infty}\frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2}du=\frac{1}{\pi(1+v^2)}","['statistics', 'normal-distribution', 'density-function']"
50,Using R for Introductory Statistics - Verzani Problem 8.6,Using R for Introductory Statistics - Verzani Problem 8.6,,"I am trying to solve the Verzani problem 8.6 in his book, but I have no idea how to. I have to use the confidence intervals to solve the problem. Here is the picture of the problem: Problem 8.6 A student wishes to find the proportion of left-handed people at her college. She surveys 100 fellow students and finds that only 5 are left-handed. If she computed a 95% confidence interval would it contain the value of p = 1/10? I have tried to use the confidence interval formulas, I have already solved some of these problems using the confidence interval formula, but I don't know how to solve the last part - would it contain the value of p = 1/10? Formula I am using by using this formula, lower is $-0.1410372$ and upper is $0.2410372$ . If someone would help me get an answer so I can learn from it or guide me to an answer. Thank you so much...","I am trying to solve the Verzani problem 8.6 in his book, but I have no idea how to. I have to use the confidence intervals to solve the problem. Here is the picture of the problem: Problem 8.6 A student wishes to find the proportion of left-handed people at her college. She surveys 100 fellow students and finds that only 5 are left-handed. If she computed a 95% confidence interval would it contain the value of p = 1/10? I have tried to use the confidence interval formulas, I have already solved some of these problems using the confidence interval formula, but I don't know how to solve the last part - would it contain the value of p = 1/10? Formula I am using by using this formula, lower is and upper is . If someone would help me get an answer so I can learn from it or guide me to an answer. Thank you so much...",-0.1410372 0.2410372,"['statistics', 'confidence-interval']"
51,Compute the probability that the first $k$ draws are red and the next $n-k$ are green,Compute the probability that the first  draws are red and the next  are green,k n-k,"At time $0$ there is one red ball and one green ball in the urn. At time $n$ , we draw out a ball chosen at random. We return it to the urn and add one more ball of the color chosen. Compute the probability that the first $k$ draws are red and the next $n-k$ are green. Generalize from this to conclude that if $X_n$ is a fraction of red balls at time $n$ then $P(X_n=k/(n+2))=1/n+1$ for $1\leq k\leq n+1$ . Now suppose we start with $5$ green and $1$ red. Use the stopping theorem to find an upper bound on the probability of $A=\{X_n>1/2\text{ for some } n\geq0\}$ Let $R_n=$ {the $n$ -th ball is red} and $G_n=$ {the $n$ -th ball is blue}. Then we know $P(R_n)=\frac{R}{R+G}=\frac{1}{2}$ and $P(G_n)=\frac{G}{G+R}=\frac{1}{2}$ (where $R$ and $G$ represents the initial number of red and green balls respectively). Then the probability that the first $k$ draws are red would be $\frac{1}{2}^k$ and the probability that the rest are green is $\frac{1}{2}^{n-k}$ . Putting this together, we have $\frac{1}{2}^{k}\frac{1}{2}^{n-k}=\frac{1}{2}^n$ . The Optional Stopping Theorem: Suppose $X_n$ is a submartingale and $N$ is a stopping time. Assume that $X_n$ is bounded or $N$ is bounded. That is, for some $M<\infty$ , $|X_n|\leq M$ with probability $1$ , or $N<M$ with probability $1$ . Then $E[X_1]=E[X_N]$ . If $N\leq n$ with probability $1$ then $E[X_1]\leq E[X_N]\leq E[X_n]$ . I'm not sure where to proceed with this. I am having trouble with this problem. I'd appreciate it if I could get some pointers on this. Thank you.","At time there is one red ball and one green ball in the urn. At time , we draw out a ball chosen at random. We return it to the urn and add one more ball of the color chosen. Compute the probability that the first draws are red and the next are green. Generalize from this to conclude that if is a fraction of red balls at time then for . Now suppose we start with green and red. Use the stopping theorem to find an upper bound on the probability of Let {the -th ball is red} and {the -th ball is blue}. Then we know and (where and represents the initial number of red and green balls respectively). Then the probability that the first draws are red would be and the probability that the rest are green is . Putting this together, we have . The Optional Stopping Theorem: Suppose is a submartingale and is a stopping time. Assume that is bounded or is bounded. That is, for some , with probability , or with probability . Then . If with probability then . I'm not sure where to proceed with this. I am having trouble with this problem. I'd appreciate it if I could get some pointers on this. Thank you.",0 n k n-k X_n n P(X_n=k/(n+2))=1/n+1 1\leq k\leq n+1 5 1 A=\{X_n>1/2\text{ for some } n\geq0\} R_n= n G_n= n P(R_n)=\frac{R}{R+G}=\frac{1}{2} P(G_n)=\frac{G}{G+R}=\frac{1}{2} R G k \frac{1}{2}^k \frac{1}{2}^{n-k} \frac{1}{2}^{k}\frac{1}{2}^{n-k}=\frac{1}{2}^n X_n N X_n N M<\infty |X_n|\leq M 1 N<M 1 E[X_1]=E[X_N] N\leq n 1 E[X_1]\leq E[X_N]\leq E[X_n],"['probability', 'statistics', 'stochastic-processes', 'martingales']"
52,Probability distribution function of the gestation period,Probability distribution function of the gestation period,,"The problem is : The ""gestation period"" for humans averages 40 weeks, with a standard deviation of 10 days. Using CERN's root package, calculate and plot the probability that a woman will give birth tomorrow, given that she has made it all the way to today without giving birth yet, as a function of the day. You can start about two weeks before the due date, and go until two weeks afterward. I am trying to solve this problem mathematically before solving it numerically, the approach I am taking is: Let the Guassian distribution give ideally the normalized number of women in a population N0 who give birth during each unit (day) with mean 40 weeks and standard deviation ten days ( probability distribution for large numbers of women of giving birth at 40 weeks with standard deviation 10 days). This can be represented by a normalized distribution  N0 * ( 1/ 10 sqrt(2pi))* exp{ -((x- 280)/10) (sqaure)} [ 280 days is basically 40 weeks: the mean ] The number who give birth during a period dx at time T is dN = N0 * (1/100 * Sqrt(2pi)) * exp ((T-280)/10) square dT The number of women who have given birth until a time T is then: N(T) = N0 * Integral of the exponential distribution until T which is the error function of T with some modifications for the coefficients. therefore the chance (probability)that a women will be giving birth at T having NOT done so up to that point is :    dN(T)/ [ N0- N(T)]    giving at T a function  F(T)dT where F(T) is the probability asked for.  Since the denominator is the likelihood that no birth has taken place as it is the number who have not given birth up to that point. N0 will cancel out. I don’t know if my approach works because the distribution I should get must go to 1 after the mean because logically the probability of giving birth should not decrease. Is there a flaw in the logic above ? Any help would be appreciated.","The problem is : The ""gestation period"" for humans averages 40 weeks, with a standard deviation of 10 days. Using CERN's root package, calculate and plot the probability that a woman will give birth tomorrow, given that she has made it all the way to today without giving birth yet, as a function of the day. You can start about two weeks before the due date, and go until two weeks afterward. I am trying to solve this problem mathematically before solving it numerically, the approach I am taking is: Let the Guassian distribution give ideally the normalized number of women in a population N0 who give birth during each unit (day) with mean 40 weeks and standard deviation ten days ( probability distribution for large numbers of women of giving birth at 40 weeks with standard deviation 10 days). This can be represented by a normalized distribution  N0 * ( 1/ 10 sqrt(2pi))* exp{ -((x- 280)/10) (sqaure)} [ 280 days is basically 40 weeks: the mean ] The number who give birth during a period dx at time T is dN = N0 * (1/100 * Sqrt(2pi)) * exp ((T-280)/10) square dT The number of women who have given birth until a time T is then: N(T) = N0 * Integral of the exponential distribution until T which is the error function of T with some modifications for the coefficients. therefore the chance (probability)that a women will be giving birth at T having NOT done so up to that point is :    dN(T)/ [ N0- N(T)]    giving at T a function  F(T)dT where F(T) is the probability asked for.  Since the denominator is the likelihood that no birth has taken place as it is the number who have not given birth up to that point. N0 will cancel out. I don’t know if my approach works because the distribution I should get must go to 1 after the mean because logically the probability of giving birth should not decrease. Is there a flaw in the logic above ? Any help would be appreciated.",,"['probability', 'statistics', 'probability-distributions', 'conditional-probability', 'means']"
53,Assessing bias and consistency of modified OLS estimator,Assessing bias and consistency of modified OLS estimator,,"Given the (multivariate) linear regression model $\mathbf y=\mathbf X \mathbf\beta_0 + \epsilon$ and $\mathbb E[\epsilon|\mathbf X]=0$ for $\beta_0 \in \mathbb R^k$ , determine if the following estimator is unbiased a/o consistent: $\hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf y$ where $\lambda>0$ I notice that this is basically just the OLS estimator, but with the addition of a scaled identity matrix inside the inverse. I just do not know how to get rid of this, I feel like there is an algebra trick that I'm missing, or perhaps it involves spectral decomposition. For now, this is where I have arrived: $\hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0 + (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \epsilon$ and $\mathbb E[\hat\beta_j] = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0$ This would seem to suggest that the estimator is biased, but I'm not sure this is correct. I know that it is possible to be both biased and consistent, but I also have no idea how to prove this because I can't isolate $\beta_0$ algebraically in order to take the limit necessary.","Given the (multivariate) linear regression model and for , determine if the following estimator is unbiased a/o consistent: where I notice that this is basically just the OLS estimator, but with the addition of a scaled identity matrix inside the inverse. I just do not know how to get rid of this, I feel like there is an algebra trick that I'm missing, or perhaps it involves spectral decomposition. For now, this is where I have arrived: and This would seem to suggest that the estimator is biased, but I'm not sure this is correct. I know that it is possible to be both biased and consistent, but I also have no idea how to prove this because I can't isolate algebraically in order to take the limit necessary.",\mathbf y=\mathbf X \mathbf\beta_0 + \epsilon \mathbb E[\epsilon|\mathbf X]=0 \beta_0 \in \mathbb R^k \hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf y \lambda>0 \hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0 + (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \epsilon \mathbb E[\hat\beta_j] = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0 \beta_0,"['statistics', 'regression', 'linear-regression', 'parameter-estimation']"
54,Deriving variance for running statistics,Deriving variance for running statistics,,"From the definition of the variance: $$\sigma^2 = \frac{1}{N-1}\sum_{i=0}^{N-1}(x_i-\mu)^2 \tag{1}$$ and mean: $$\mu = \frac{1}{N}\sum^{N-1}_{i=0}x_i \tag{2}$$ how is it possible to derive the variance for running statistics, which is: $$\sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\frac{1}{N}\left(\sum_{i=0}^{N-1}x_i \right)^2\right] \tag{3}$$ why the term $2x_i\mu$ is not considered? Starting from $(1)$ , I get: $$\sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\sum_{i=0}^{N-1}2x_i \mu\right]+\frac{\mu^2}{N-1} $$","From the definition of the variance: and mean: how is it possible to derive the variance for running statistics, which is: why the term is not considered? Starting from , I get:",\sigma^2 = \frac{1}{N-1}\sum_{i=0}^{N-1}(x_i-\mu)^2 \tag{1} \mu = \frac{1}{N}\sum^{N-1}_{i=0}x_i \tag{2} \sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\frac{1}{N}\left(\sum_{i=0}^{N-1}x_i \right)^2\right] \tag{3} 2x_i\mu (1) \sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\sum_{i=0}^{N-1}2x_i \mu\right]+\frac{\mu^2}{N-1} ,"['statistics', 'variance', 'signal-processing', 'standard-deviation']"
55,Verify that $I$ and $X$ are negatively correlated.,Verify that  and  are negatively correlated.,I X,"In certain situations, a random variable $X$ with known mean is simulated to obtain an estimate of $P(X \leq a)$ for some constant given $a$ . The simple estimator of a simulation for a run is $I = I (X \leq a)$ . Verify that $I$ and $X$ are negatively correlated. My approach: By definition $$Corr(X,I)=\frac{Cov(X,I)}{\sqrt{Var(X)Var(Y)}}$$ Then the only chance for the correlation to be negative is when covariance is negative. $$Cov(X,I)=E[XI]-E[X]E[I]$$ I'm not sure on how to interpret the random variable $I$ and its expected value. Any suggestion would be great!!","In certain situations, a random variable with known mean is simulated to obtain an estimate of for some constant given . The simple estimator of a simulation for a run is . Verify that and are negatively correlated. My approach: By definition Then the only chance for the correlation to be negative is when covariance is negative. I'm not sure on how to interpret the random variable and its expected value. Any suggestion would be great!!","X P(X \leq a) a I = I (X \leq a) I X Corr(X,I)=\frac{Cov(X,I)}{\sqrt{Var(X)Var(Y)}} Cov(X,I)=E[XI]-E[X]E[I] I","['probability', 'statistics', 'correlation', 'simulation', 'parameter-estimation']"
56,Uniformly most powerful test for $H_0: \lambda \ge \lambda_0$,Uniformly most powerful test for,H_0: \lambda \ge \lambda_0,"I cannot find anywhere answer to this question. Let's take exponential distribution i.e. $$f(x) = \lambda e^{-\lambda x}\mathbb{1}_{(0, +\infty)}(x)$$ I want to derive uniformly most powerful test for hypothesis: $H_0:\lambda\ge\lambda_0$ and $H_1:\lambda<\lambda_0$ . How I would start with this problem is to take two $\lambda_1 \ge\lambda_0$ and $\lambda_2 < \lambda_0$ and then start to write: $$\frac{L(x, \lambda_2)}{L(x, \lambda_1)} = \frac{\lambda_2^ne^{-\lambda_2(\sum_{i=1}^nx_i)}}{\lambda_1^ne^{-\lambda_1(\sum_{i=1}^nx_i)}}$$ Did I start to write correct likelihood proportion for my problem?",I cannot find anywhere answer to this question. Let's take exponential distribution i.e. I want to derive uniformly most powerful test for hypothesis: and . How I would start with this problem is to take two and and then start to write: Did I start to write correct likelihood proportion for my problem?,"f(x) = \lambda e^{-\lambda x}\mathbb{1}_{(0, +\infty)}(x) H_0:\lambda\ge\lambda_0 H_1:\lambda<\lambda_0 \lambda_1 \ge\lambda_0 \lambda_2 < \lambda_0 \frac{L(x, \lambda_2)}{L(x, \lambda_1)} = \frac{\lambda_2^ne^{-\lambda_2(\sum_{i=1}^nx_i)}}{\lambda_1^ne^{-\lambda_1(\sum_{i=1}^nx_i)}}","['probability', 'statistics', 'hypothesis-testing']"
57,Proving that this is a method of moments estimator for $Var(X)$ for $X\sim Geo(p)$.,Proving that this is a method of moments estimator for  for .,Var(X) X\sim Geo(p),"Let $X\sim Geo(p)$ be a random variable, and $X_1,...,X_n$ a random random sample from the distribution of $X.$ Prove that $\frac{1}{2n}\sum^n_{i=1}X_i^2-\frac{1}{2}\bar X$ is an estimator in the methods of moments for $Var(X)$ . Basically what I've tried to do is try to express $Var(X)$ as a function of $E(X)=\mu_1,E(X^2)=\mu_2$ . I did these two attempts: $Var(X)=E(X^2)-[E(X)]^2=\mu_2-\mu_1^2=g_1(\mu_1,\mu_2)$ . $Var(X)=\frac{1-p}{p^2}=\frac{1-\frac{1}{\mu_1}}{\frac{1}{\mu_1^2}}=\mu_1(\mu_1-1)=g_2(\mu_1)$ . Both these functions led to two different estimators. But $g_1$ led to: $\frac{1}{n}\sum^n_{i=1}X_i^2-(\bar X)^2$ . Which somehow looks like the estimator I'm asked to prove. But I got stuck here, I would appreciate any help in how to prove this. Thanks in advance!","Let be a random variable, and a random random sample from the distribution of Prove that is an estimator in the methods of moments for . Basically what I've tried to do is try to express as a function of . I did these two attempts: . . Both these functions led to two different estimators. But led to: . Which somehow looks like the estimator I'm asked to prove. But I got stuck here, I would appreciate any help in how to prove this. Thanks in advance!","X\sim Geo(p) X_1,...,X_n X. \frac{1}{2n}\sum^n_{i=1}X_i^2-\frac{1}{2}\bar X Var(X) Var(X) E(X)=\mu_1,E(X^2)=\mu_2 Var(X)=E(X^2)-[E(X)]^2=\mu_2-\mu_1^2=g_1(\mu_1,\mu_2) Var(X)=\frac{1-p}{p^2}=\frac{1-\frac{1}{\mu_1}}{\frac{1}{\mu_1^2}}=\mu_1(\mu_1-1)=g_2(\mu_1) g_1 \frac{1}{n}\sum^n_{i=1}X_i^2-(\bar X)^2",['statistics']
58,Asymptotic distribution of the MLE of an exponential via the CLT,Asymptotic distribution of the MLE of an exponential via the CLT,,"Let $X_1,..,X_n$ i.i.d from an exponential distribution : \begin{align*}     f(x) = \lambda e^{-\lambda x} , x > 0 \end{align*} I've computed the MLE and the Fisher information number : \begin{align*} \lambda_{MLE} &= \frac{1}{\bar{X}} \\ I(\lambda) &= \frac{1}{\lambda^2} \end{align*} I therefore obtain the following asymptotic distribution for $\lambda_{MLE} $ : \begin{align}     \sqrt{n} (\lambda_{MLE} - \lambda_0) \xrightarrow{D} \mathcal{N}(0,\lambda^2)  \end{align} I wish to check that my result is correct by using the CLT theorem and the Delta Method. CLT : \begin{align}     \sqrt{n} (\bar{X}-\mu) \xrightarrow{D} \mathcal{N}(0,1/\lambda^2) \end{align} Because $\lambda^2$ is the variance of an exponential random variable. The delta method states that : \begin{align}     \sqrt{n}(g(\hat{\theta}) -g(\theta)) \xrightarrow{D} \mathcal{N}(0,(g'(\theta))^2 \sigma^2(\theta) )  \end{align} With in this case : \begin{align}     g(x) &= \frac{1}{x} \\     \frac{dg(x)}{dx} &= \frac{-1}{x^2} \end{align} Therefore : \begin{align} \sqrt{n}(\lambda_{MLE} - \lambda_0) \xrightarrow{D} \mathcal{N}(0,\Big[\frac{-1}{\lambda^2}\Big]^2 \frac{1}{\lambda^2}) \end{align} Which would yields an asymptotic variance of $1/\lambda^6$ which cannot be true. What am I doing wrong ?",Let i.i.d from an exponential distribution : I've computed the MLE and the Fisher information number : I therefore obtain the following asymptotic distribution for : I wish to check that my result is correct by using the CLT theorem and the Delta Method. CLT : Because is the variance of an exponential random variable. The delta method states that : With in this case : Therefore : Which would yields an asymptotic variance of which cannot be true. What am I doing wrong ?,"X_1,..,X_n \begin{align*}
    f(x) = \lambda e^{-\lambda x} , x > 0
\end{align*} \begin{align*}
\lambda_{MLE} &= \frac{1}{\bar{X}} \\
I(\lambda) &= \frac{1}{\lambda^2}
\end{align*} \lambda_{MLE}  \begin{align}
    \sqrt{n} (\lambda_{MLE} - \lambda_0) \xrightarrow{D} \mathcal{N}(0,\lambda^2) 
\end{align} \begin{align}
    \sqrt{n} (\bar{X}-\mu) \xrightarrow{D} \mathcal{N}(0,1/\lambda^2)
\end{align} \lambda^2 \begin{align}
    \sqrt{n}(g(\hat{\theta}) -g(\theta)) \xrightarrow{D} \mathcal{N}(0,(g'(\theta))^2 \sigma^2(\theta) ) 
\end{align} \begin{align}
    g(x) &= \frac{1}{x} \\
    \frac{dg(x)}{dx} &= \frac{-1}{x^2}
\end{align} \begin{align}
\sqrt{n}(\lambda_{MLE} - \lambda_0) \xrightarrow{D} \mathcal{N}(0,\Big[\frac{-1}{\lambda^2}\Big]^2 \frac{1}{\lambda^2})
\end{align} 1/\lambda^6","['probability', 'statistics', 'self-learning']"
59,Simple Random walk on $\mathbb{Z}$ find $P(S_3+S_1=S_4+S_6)$,Simple Random walk on  find,\mathbb{Z} P(S_3+S_1=S_4+S_6),Given state space $\mathbb{Z}$ and a markov chain $S_n=\sum_{k=1}^n X_k$ where $P(X_k=1)=1/2$ and $P(X_K=-1)=1/2$ Find $P(S_3+S_1=S_4+S_6)$ I have a formula to compute $P(S_n=k)$ so I believe I want to simplify to that form. My attempt: $P(S_3+S_1=S_4+S_6)=P(\sum_{k=1}^3 X_k+\sum_{k=1}^1 X_k=\sum_{k=1}^4 X_k+\sum_{k=1}^6 X_k)$ Rearranging the sums I get $P(X_1-X_4-\sum_{k=1}^6X_k=0)$ I think by symmetry I can say that $-\sum_{k=1}^6 X_k=\sum_{k=1}^6 -X_k=S_6$ So I could rearrange again to $P(S_6=X_4-X_1)$ I know that they are either $1$ or $-1$ with equal probability so do I have to compute the sum $P(S_6=2)+P(S_6=-2)+P(S_6=0)$ for the possible value of $X_4-X_1$ ?,Given state space and a markov chain where and Find I have a formula to compute so I believe I want to simplify to that form. My attempt: Rearranging the sums I get I think by symmetry I can say that So I could rearrange again to I know that they are either or with equal probability so do I have to compute the sum for the possible value of ?,\mathbb{Z} S_n=\sum_{k=1}^n X_k P(X_k=1)=1/2 P(X_K=-1)=1/2 P(S_3+S_1=S_4+S_6) P(S_n=k) P(S_3+S_1=S_4+S_6)=P(\sum_{k=1}^3 X_k+\sum_{k=1}^1 X_k=\sum_{k=1}^4 X_k+\sum_{k=1}^6 X_k) P(X_1-X_4-\sum_{k=1}^6X_k=0) -\sum_{k=1}^6 X_k=\sum_{k=1}^6 -X_k=S_6 P(S_6=X_4-X_1) 1 -1 P(S_6=2)+P(S_6=-2)+P(S_6=0) X_4-X_1,"['statistics', 'markov-chains']"
60,How to estimate the maximum of a probability density function using sample points?,How to estimate the maximum of a probability density function using sample points?,,"I have an experiment that shoots particles at a wall. It hits some regions with a higher probability than others. I don't know what the underlying probability density function is. I assume it's some smooth curve and I would like to know its maximum value. I can get an estimate by firing lots of particles and recording where they land. Then make a histogram of the data and calculate the maximum value from that. The problem is I don't know what the optimum number of bins to use for the histogram is. Additionally, I don't know how accurate my estimate is. Do you know if there is a formula or method I can use to calculate the optimum number of bins for a given number of samples? Do you know if we can approximate error bars for our estimate? I have attached some Python code to help explain what I am doing. In this example, the underlying probability density function is the normal distribution, with mean, $\mu=0$ , and variance $\sigma^2=1$ . In this example, I know the exact solution is $1/\sqrt{2\pi\sigma^2}$ , however, in general, I don't know the formula for the probability density function so we need to estimate its maximum. import numpy as np import matplotlib.pyplot as plt  mu = 0 sigma = 1 len_particles_array = 5 num_particles_array = 10**np.arange(1, len_particles_array + 1)  len_bin_array = 16 num_bins_array = 2**np.arange(len_bin_array)  max_number_density_array = np.zeros((len_particles_array, len_bin_array))  analytic_solution = 1 / np.sqrt(2 * np.pi * sigma**2)  for i, num_particles in enumerate(num_particles_array):     particle_positions = np.random.normal(mu, sigma, num_particles)     for j, num_bins in enumerate(num_bins_array):         number_density, bins = np.histogram(particle_positions, num_bins, density=True)         max_number_density_array[i, j] = np.max(number_density)  fig = plt.figure() ax = fig.add_subplot(111)  for i, num_particles in enumerate(num_particles_array):     ax.loglog(num_bins_array, max_number_density_array[i, :], \               label = '# particles = ' + str(num_particles))  xlims = ax.get_xlim() ax.loglog(xlims, [analytic_solution, analytic_solution], 'k--', \           label = r'Exact solution = $1\ /\ \sqrt{2\pi\sigma^2}$ ') ax.set_xlim(xlims)  ax.set_xlabel('Number of bins') ax.set_ylabel('Estimate for the maximum of the probability density function') ax.legend()  plt.savefig('Estimate for the maximum of the probability density function.png')  plt.show() Here is the outputted figure. It shows the estimate is dependent on the number of bins I use for the histogram and the number of sample particles I use.","I have an experiment that shoots particles at a wall. It hits some regions with a higher probability than others. I don't know what the underlying probability density function is. I assume it's some smooth curve and I would like to know its maximum value. I can get an estimate by firing lots of particles and recording where they land. Then make a histogram of the data and calculate the maximum value from that. The problem is I don't know what the optimum number of bins to use for the histogram is. Additionally, I don't know how accurate my estimate is. Do you know if there is a formula or method I can use to calculate the optimum number of bins for a given number of samples? Do you know if we can approximate error bars for our estimate? I have attached some Python code to help explain what I am doing. In this example, the underlying probability density function is the normal distribution, with mean, , and variance . In this example, I know the exact solution is , however, in general, I don't know the formula for the probability density function so we need to estimate its maximum. import numpy as np import matplotlib.pyplot as plt  mu = 0 sigma = 1 len_particles_array = 5 num_particles_array = 10**np.arange(1, len_particles_array + 1)  len_bin_array = 16 num_bins_array = 2**np.arange(len_bin_array)  max_number_density_array = np.zeros((len_particles_array, len_bin_array))  analytic_solution = 1 / np.sqrt(2 * np.pi * sigma**2)  for i, num_particles in enumerate(num_particles_array):     particle_positions = np.random.normal(mu, sigma, num_particles)     for j, num_bins in enumerate(num_bins_array):         number_density, bins = np.histogram(particle_positions, num_bins, density=True)         max_number_density_array[i, j] = np.max(number_density)  fig = plt.figure() ax = fig.add_subplot(111)  for i, num_particles in enumerate(num_particles_array):     ax.loglog(num_bins_array, max_number_density_array[i, :], \               label = '# particles = ' + str(num_particles))  xlims = ax.get_xlim() ax.loglog(xlims, [analytic_solution, analytic_solution], 'k--', \           label = r'Exact solution = ') ax.set_xlim(xlims)  ax.set_xlabel('Number of bins') ax.set_ylabel('Estimate for the maximum of the probability density function') ax.legend()  plt.savefig('Estimate for the maximum of the probability density function.png')  plt.show() Here is the outputted figure. It shows the estimate is dependent on the number of bins I use for the histogram and the number of sample particles I use.",\mu=0 \sigma^2=1 1/\sqrt{2\pi\sigma^2} 1\ /\ \sqrt{2\pi\sigma^2},"['statistics', 'probability-distributions', 'physics', 'python']"
61,Mean and variance of non-stationary stochastic processes,Mean and variance of non-stationary stochastic processes,,"I'm studying AR and MA stochastic processes, I know that AR is stationary when $\theta<1$ while MA is always stationary. I need to calculate $E$ , $Var$ and $Cov$ of this two processes: $$y_t=\alpha+y_{t-1}+\varepsilon_t$$ $$ y_t=\alpha t + \varepsilon_t $$ where $\varepsilon_t$ has mean 0 and variance $\sigma^2$ . From my understanding the first process is not stationary, but I can't even calculate the mean as for $\theta=1$ the formula $E[y_t]=\frac{\alpha}{1-\theta}$ clearly cannot be computed. The second one is also not stationary as the mean depends on $t$ , but I have no idea of how to calculate it, maybe $E[y_t]=\alpha \frac{T}{2}$ ?","I'm studying AR and MA stochastic processes, I know that AR is stationary when while MA is always stationary. I need to calculate , and of this two processes: where has mean 0 and variance . From my understanding the first process is not stationary, but I can't even calculate the mean as for the formula clearly cannot be computed. The second one is also not stationary as the mean depends on , but I have no idea of how to calculate it, maybe ?",\theta<1 E Var Cov y_t=\alpha+y_{t-1}+\varepsilon_t  y_t=\alpha t + \varepsilon_t  \varepsilon_t \sigma^2 \theta=1 E[y_t]=\frac{\alpha}{1-\theta} t E[y_t]=\alpha \frac{T}{2},"['statistics', 'stochastic-processes']"
62,What is the difference between KL Divergence and merely subtracting entropy measurements?,What is the difference between KL Divergence and merely subtracting entropy measurements?,,"I'm wondering what the difference between KL divergence and just subtracting one entropy measurement from another is. I can see what the difference is mathematically...but I'm having a hard time grasping what it means. Throughout this I'm thinking of entropy measurements of English text, but I'm not sure that matters. Consider that you have two models for some random variable X, p (the good model), and q (the less-good model). If we compute entropy $$H(X) = -\sum_{x\in\mathcal{X}} p(x) \log(p(x)) = let's\,say\,2\,bits\,per\,character$$ and $$H(X) = -\sum_{x\in\mathcal{X}} q(x) \log(q(x)) = 3\,bits\,per\,character$$ we can obtain a difference measure simply by subtracting: $$H_{diff}(p(x),q(x)) = -\sum_{x\in\mathcal{X}} q(x) \log(q(x)) - -\sum_{x\in\mathcal{X}} p(x) \log(p(x)) = 1\,bit\,per\,character$$ simplified a bit to be analogous in form to a common statement of KL Divergence... $$H_{diff}(p(x),q(x)) = \sum_{x\in\mathcal{X}} p(x)\log(p(x)) - q(x)\log(q(x)) = 1\,bit\,per\,character$$ Okay, so that makes a sort of intuitive sense. If we've got 1000 characters then on average for model q we'll see 3000 bits, whereas on average for model p we'll see 2000 bits. Got it, I think? But we can also use KL divergence, which is very similar: $$D_\text{KL}(p \parallel q) = \sum_{x\in\mathcal{X}} p(x) \log\left(\frac{p(x)}{q(x)}\right)$$ Rearrange a bit... $$D_\text{KL}(p \parallel q) = \sum_{x\in\mathcal{X}} p(x)\log(p(x)) - p(x)\log(q(x)) $$ So, really the only difference between simply taking the difference of the two entropy calculations and KL Divergence is p(x) vs. q(x) in the second term on the right-hand side, which is cross-entropy in the case of KL divergence and regular old entropy in my first example. I've been thinking about it for a while here and I can't come up with an answer. What, intuitively, is the difference between these two distance measures?","I'm wondering what the difference between KL divergence and just subtracting one entropy measurement from another is. I can see what the difference is mathematically...but I'm having a hard time grasping what it means. Throughout this I'm thinking of entropy measurements of English text, but I'm not sure that matters. Consider that you have two models for some random variable X, p (the good model), and q (the less-good model). If we compute entropy and we can obtain a difference measure simply by subtracting: simplified a bit to be analogous in form to a common statement of KL Divergence... Okay, so that makes a sort of intuitive sense. If we've got 1000 characters then on average for model q we'll see 3000 bits, whereas on average for model p we'll see 2000 bits. Got it, I think? But we can also use KL divergence, which is very similar: Rearrange a bit... So, really the only difference between simply taking the difference of the two entropy calculations and KL Divergence is p(x) vs. q(x) in the second term on the right-hand side, which is cross-entropy in the case of KL divergence and regular old entropy in my first example. I've been thinking about it for a while here and I can't come up with an answer. What, intuitively, is the difference between these two distance measures?","H(X) = -\sum_{x\in\mathcal{X}} p(x) \log(p(x)) = let's\,say\,2\,bits\,per\,character H(X) = -\sum_{x\in\mathcal{X}} q(x) \log(q(x)) = 3\,bits\,per\,character H_{diff}(p(x),q(x)) = -\sum_{x\in\mathcal{X}} q(x) \log(q(x)) - -\sum_{x\in\mathcal{X}} p(x) \log(p(x)) = 1\,bit\,per\,character H_{diff}(p(x),q(x)) = \sum_{x\in\mathcal{X}} p(x)\log(p(x)) - q(x)\log(q(x)) = 1\,bit\,per\,character D_\text{KL}(p \parallel q) = \sum_{x\in\mathcal{X}} p(x) \log\left(\frac{p(x)}{q(x)}\right) D_\text{KL}(p \parallel q) = \sum_{x\in\mathcal{X}} p(x)\log(p(x)) - p(x)\log(q(x)) ","['statistics', 'information-theory', 'entropy']"
63,Normal approximation to Poisson's distribution,Normal approximation to Poisson's distribution,,"Historic data of a computer network suggest that connections with this network follow a Poisson's distribution with an average of 5 connections per minute. Find $t_0$ , which has probability equal to $0.9$ to happen at least once before $t_0$ I made an approximation using $\lambda=12$ , since $5/60 =1/12$ , but my answer doesn't match with the book (book says it's ~ $27.5$ seconds, and mine ~ $17.0$ seconds) Could someone help me?","Historic data of a computer network suggest that connections with this network follow a Poisson's distribution with an average of 5 connections per minute. Find , which has probability equal to to happen at least once before I made an approximation using , since , but my answer doesn't match with the book (book says it's ~ seconds, and mine ~ seconds) Could someone help me?",t_0 0.9 t_0 \lambda=12 5/60 =1/12 27.5 17.0,"['probability', 'statistics', 'poisson-distribution']"
64,Combining probability density funciton and probability mass function,Combining probability density funciton and probability mass function,,"I was working on this problem: Let N be a geometric random variable with parameter p. Suppose that the conditional distribution of X given that N = n is the gamma distribution with parameters n and λ. Find the conditional probability mass function of N given that X = x. And in the solution, they started with $$P(N=n|X=x)=\frac{f_{X|N}(x|n)P(N=n)}{f_{X}(x)}$$ However why is that they can simply combine the p.d.f. of X conditioned on N with the p.m.f. of N? Pmf expresses probability while pdf is a probability density.","I was working on this problem: Let N be a geometric random variable with parameter p. Suppose that the conditional distribution of X given that N = n is the gamma distribution with parameters n and λ. Find the conditional probability mass function of N given that X = x. And in the solution, they started with However why is that they can simply combine the p.d.f. of X conditioned on N with the p.m.f. of N? Pmf expresses probability while pdf is a probability density.",P(N=n|X=x)=\frac{f_{X|N}(x|n)P(N=n)}{f_{X}(x)},"['statistics', 'conditional-probability', 'geometric-probability']"
65,Probability N points in range,Probability N points in range,,"I have looked for an answer to this problem but I didn't find any. Apologies if my language is not at the level of this exchange. In my project, I deal with cells (imagine points of different colors) with coordinates x and y. These cells have different types and they are distributed on a surface. I am measuring three quantities. Let's assume we have a square surface of side $L$ .  We have $N$ points of type $A$ randomly distributes on this surface, and also $K$ points of type $B$ also randomly distributed on this surface.  We can assume these points to be circular, with radius $r_c$ << L and $r_c$ < D (definition of D follows). I define the function $S(A, D)$ that gives the number of points ""B"" at a distance lower than ""D"" from a point A (so, it's really like looking at how many points fall in the circle centered in point of type A with radius ""D""). The first measure is: $$ M_{sum} =  \sum_{i=1}^N S(A_i, D) $$ The second measure is very similar, it's the average: $$ M_{avr} =  \frac{1}{N} \sum_{i=1}^N S(A_i, D) $$ For the third measure, I need another function, called $V$ . $$   V(A, D) = \begin{cases} 1,  & \text{if $S(A,D)>0$} \\ 0, & \text{if $S(A,D)=0$} \end{cases} $$ The measure is: $$ M_{neig}=\frac{1}{N} \sum_{i=1}^N V(A_i,D)$$ The problem is the following.  I need to see if there are any significant differences in my data from a random distribution. So, is there a way to estimate the expected values for these quantities?","I have looked for an answer to this problem but I didn't find any. Apologies if my language is not at the level of this exchange. In my project, I deal with cells (imagine points of different colors) with coordinates x and y. These cells have different types and they are distributed on a surface. I am measuring three quantities. Let's assume we have a square surface of side .  We have points of type randomly distributes on this surface, and also points of type also randomly distributed on this surface.  We can assume these points to be circular, with radius << L and < D (definition of D follows). I define the function that gives the number of points ""B"" at a distance lower than ""D"" from a point A (so, it's really like looking at how many points fall in the circle centered in point of type A with radius ""D""). The first measure is: The second measure is very similar, it's the average: For the third measure, I need another function, called . The measure is: The problem is the following.  I need to see if there are any significant differences in my data from a random distribution. So, is there a way to estimate the expected values for these quantities?","L N A K B r_c r_c S(A, D)  M_{sum} =  \sum_{i=1}^N S(A_i, D)   M_{avr} =  \frac{1}{N} \sum_{i=1}^N S(A_i, D)  V 
  V(A, D) =
\begin{cases}
1,  & \text{if S(A,D)>0} \\
0, & \text{if S(A,D)=0}
\end{cases}
  M_{neig}=\frac{1}{N} \sum_{i=1}^N V(A_i,D)","['statistics', 'expected-value']"
66,What is the distribution of $T=\sum_{i=1}^{n} e^{-X_{i}}$?,What is the distribution of ?,T=\sum_{i=1}^{n} e^{-X_{i}},"Let $ \ x_{1},x_{2},...,x_{n}$ a random sample with density $$f(x;\theta) = e^{-(x-\theta)}  e^{-e^{-(x-\theta)}}$$ where $\theta \in \mathbb R$ What is the distribution of $T=\sum_{i=1}^{n} e^{-x_{i} }$ I try this way: $m_{T}(t)=E(e^{tT})=E(e^{t\sum_{i=1}^{n} e^{-x_{i} }})=E(\prod_{i=1}^{n} e^{te^{-x_{i}}})= \prod_{i=1}^{n} E(e^{te^{-x_{i}}})= \prod _{i=1}^{n} m_{e^{-x_{i}}}(t)$ And then $m_{e^{-x}}(t)=E(e^{te^{-x}})= \int e^{te^{-x}} \left( e^{-(x-\theta)}  e^{-e^{-(x-\theta)}} \right) dx$ I don´t sure what is the support.",Let a random sample with density where What is the distribution of I try this way: And then I don´t sure what is the support.," \ x_{1},x_{2},...,x_{n} f(x;\theta) = e^{-(x-\theta)}  e^{-e^{-(x-\theta)}} \theta \in \mathbb R T=\sum_{i=1}^{n} e^{-x_{i}
} m_{T}(t)=E(e^{tT})=E(e^{t\sum_{i=1}^{n} e^{-x_{i}
}})=E(\prod_{i=1}^{n} e^{te^{-x_{i}}})= \prod_{i=1}^{n} E(e^{te^{-x_{i}}})= \prod _{i=1}^{n} m_{e^{-x_{i}}}(t) m_{e^{-x}}(t)=E(e^{te^{-x}})= \int e^{te^{-x}} \left( e^{-(x-\theta)}  e^{-e^{-(x-\theta)}} \right) dx","['statistics', 'probability-distributions']"
67,"How to understand lindeberg CLT, compare with classical CLT","How to understand lindeberg CLT, compare with classical CLT",,"For Lindeberg CLT. Let $\{X_{nj},j=1,2,...k_n\}$ be independent r.v.s with $0<\sigma^2_n=Var(\sum_{j=1}^{k_n}X_{nj})<\infty$ , $n=1,2,...$ and $k_n\rightarrow\infty$ as $n\rightarrow\infty$ . If $$\sum_{j=1}^{k_n}E[(X_{nj}-EX_{nj})^2I_{\{|X_{nj}-EX_{nj}|>\epsilon\sigma_n\}}]=o(\sigma^2_n)$$ for any $\epsilon>0$ . (Lindeberg condition) Then, $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ How to understand this Lindeberg condition? If we consider iid case, with $E(X_{n1})=\mu, Var(X_{n1})=\sigma^2$ , the lindeberg condition becomes: $$\frac{\sum_{j=1}^{k_n}E[(X_{nj}-\mu)^2I_{\{|\frac{X_{nj}-\mu}{\sigma}|>\epsilon\sqrt{k_n}\}}]}{k_n\sigma^2}\rightarrow 0$$ as $k_n\rightarrow\infty$ , so $$\frac{1}{k_n}\sum_{j=1}^{k_n}E[(\frac{X_{nj}-\mu}{\sigma})^2I_{\{(\frac{X_{nj}-\mu}{\sigma})^2>\epsilon^2k_n\}}\rightarrow 0$$ as $k_n\rightarrow\infty$ , so Let $Z_{nj}=\frac{X_{nj}-\mu}{\sigma}$ $$\frac{1}{k_n}\sum_{j=1}^{k_n}E[(Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ , so $$E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ .(is this true?) Since we have $E(X_{nj})=\mu, Var(X_{nj})=\sigma^2$ , $E(Z_{nj})=0, Var(Z_{nj})=1$ . So $E(Z_{nj})^2=E(Z_{nj})^2+Var(Z_{nj})=1<\infty$ . (Is this correct?). From $E(Z_{nj}^2)<\infty$ . We can already derive $$E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ , by dominated convergence theorem see prove For non-negative r.x. $X$ , $E(X)<\infty$ iff $E(XI_{X>x})\rightarrow 0$ as $x\rightarrow\infty$ , (is this true?). Then how this Lindeberg condition be helpful? Are there any mistakes above? Or does it means that for the iid case, Lindeberg's condition always holds? From Lindeberg CLT's result $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ For iid case, this result becomes $$\frac{\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})}{\sqrt{k_nVar(X_{n1})}}\rightarrow_dN(0,1)$$ $$\frac{1}{k_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{n1})\rightarrow_dN(0,Var(X_{n1})),$$ which is the same as classical CLT's result. For $\{X_{nj}, j=1,2,...,k_n;n=1,2,3,...\}$ is triangle arrow, since $k_n\le n$ , and $k_n\rightarrow\infty$ as $n\rightarrow\infty$ . How this ""triangle"" condition is used in ""iid"" case?, since this result means that for every r.v. line $\{X_{nj},j=1,2,...k_n\}$ , CLT holds For the classical CLT, we have $$\sqrt{n}(\bar{X}-EX)\to_d N(0,Var(X_1)),$$ Here $\sqrt{n}$ plays a role of converges speed (rate) (Is my understanding correct?) whice is so called "" $\sqrt{n}_consistency$ "" in statistics If so, what's the converges speed in $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ ? Is the converging speed $\frac{1}{\sigma_n}$ or $\frac{1}{\sqrt{k_n}}$ ? Can a convergence rate depend on $Var(X_i)$ ? (I personally don't think so). If not,  what's the converge rate?","For Lindeberg CLT. Let be independent r.v.s with , and as . If for any . (Lindeberg condition) Then, How to understand this Lindeberg condition? If we consider iid case, with , the lindeberg condition becomes: as , so as , so Let as , so as .(is this true?) Since we have , . So . (Is this correct?). From . We can already derive as , by dominated convergence theorem see prove For non-negative r.x. , iff as , (is this true?). Then how this Lindeberg condition be helpful? Are there any mistakes above? Or does it means that for the iid case, Lindeberg's condition always holds? From Lindeberg CLT's result For iid case, this result becomes which is the same as classical CLT's result. For is triangle arrow, since , and as . How this ""triangle"" condition is used in ""iid"" case?, since this result means that for every r.v. line , CLT holds For the classical CLT, we have Here plays a role of converges speed (rate) (Is my understanding correct?) whice is so called "" "" in statistics If so, what's the converges speed in ? Is the converging speed or ? Can a convergence rate depend on ? (I personally don't think so). If not,  what's the converge rate?","\{X_{nj},j=1,2,...k_n\} 0<\sigma^2_n=Var(\sum_{j=1}^{k_n}X_{nj})<\infty n=1,2,... k_n\rightarrow\infty n\rightarrow\infty \sum_{j=1}^{k_n}E[(X_{nj}-EX_{nj})^2I_{\{|X_{nj}-EX_{nj}|>\epsilon\sigma_n\}}]=o(\sigma^2_n) \epsilon>0 \frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1) E(X_{n1})=\mu, Var(X_{n1})=\sigma^2 \frac{\sum_{j=1}^{k_n}E[(X_{nj}-\mu)^2I_{\{|\frac{X_{nj}-\mu}{\sigma}|>\epsilon\sqrt{k_n}\}}]}{k_n\sigma^2}\rightarrow 0 k_n\rightarrow\infty \frac{1}{k_n}\sum_{j=1}^{k_n}E[(\frac{X_{nj}-\mu}{\sigma})^2I_{\{(\frac{X_{nj}-\mu}{\sigma})^2>\epsilon^2k_n\}}\rightarrow 0 k_n\rightarrow\infty Z_{nj}=\frac{X_{nj}-\mu}{\sigma} \frac{1}{k_n}\sum_{j=1}^{k_n}E[(Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0 k_n\rightarrow\infty E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0 k_n\rightarrow\infty E(X_{nj})=\mu, Var(X_{nj})=\sigma^2 E(Z_{nj})=0, Var(Z_{nj})=1 E(Z_{nj})^2=E(Z_{nj})^2+Var(Z_{nj})=1<\infty E(Z_{nj}^2)<\infty E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0 k_n\rightarrow\infty X E(X)<\infty E(XI_{X>x})\rightarrow 0 x\rightarrow\infty \frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1) \frac{\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})}{\sqrt{k_nVar(X_{n1})}}\rightarrow_dN(0,1) \frac{1}{k_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{n1})\rightarrow_dN(0,Var(X_{n1})), \{X_{nj}, j=1,2,...,k_n;n=1,2,3,...\} k_n\le n k_n\rightarrow\infty n\rightarrow\infty \{X_{nj},j=1,2,...k_n\} \sqrt{n}(\bar{X}-EX)\to_d N(0,Var(X_1)), \sqrt{n} \sqrt{n}_consistency \frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1) \frac{1}{\sigma_n} \frac{1}{\sqrt{k_n}} Var(X_i)","['probability', 'probability-theory', 'statistics', 'central-limit-theorem']"
68,What is the probability of certain numbers to be included in a drawn sample?,What is the probability of certain numbers to be included in a drawn sample?,,"Say I have 10 ping pong balls, these balls are labeled 1 through 10. I pick 4 with replacement. What are the chances that the 4 balls I picked are the numbers 3, 5, 7, and 9? I have calculated the possibilities of choosing 4 from these 10, I believe there are 10,000 possibilities. I did this by 10^4. I also know that each number has 1/10 chance of getting chosen with replacement. But I am not sure where to go from here. Any help would be greatly appreciated!","Say I have 10 ping pong balls, these balls are labeled 1 through 10. I pick 4 with replacement. What are the chances that the 4 balls I picked are the numbers 3, 5, 7, and 9? I have calculated the possibilities of choosing 4 from these 10, I believe there are 10,000 possibilities. I did this by 10^4. I also know that each number has 1/10 chance of getting chosen with replacement. But I am not sure where to go from here. Any help would be greatly appreciated!",,"['probability', 'statistics', 'permutations', 'combinations', 'factorial']"
69,Probability that the difference of two independent Binomials with the same $n$ is negative,Probability that the difference of two independent Binomials with the same  is negative,n,"Suppose I have $X\sim \operatorname{Binom}(n,p_0)$ and $Y\sim \operatorname{Binom}(n,p_1)$ where $p_0>p_1$ , I need to bound $$P(X-Y\le 0)$$ One way to do is to write $X-Y$ as a sum of i.i.d. random variables which are differences of $\operatorname{Bern}(p_0)$ and $\operatorname{Bern}(p_1)$ and use Chernoff bound to get $$P(X-Y\le 0)\le \exp\left(-\frac{1}{2}n(p_0-p_1)^2\right)$$ I know that since Chernoff bound does not utilize the variance, one of Bernstein's inequalities could give a tighter bound. I have tried the Inequality (1) in given here . However, when I did so, I found a messy exponent in the upper bound which isn't always tighter than the one given by Chernoff bound. My main question is: Is there a way where I can prove a tighter or a normal-looking bound, with the first exponent of the form $$\exp\left(-\frac{1}{2} n\frac{(p_0-p_1)^2}{p_0(1-p_0)+p_1(1-p_1)}\right).$$ Thanks a lot!","Suppose I have and where , I need to bound One way to do is to write as a sum of i.i.d. random variables which are differences of and and use Chernoff bound to get I know that since Chernoff bound does not utilize the variance, one of Bernstein's inequalities could give a tighter bound. I have tried the Inequality (1) in given here . However, when I did so, I found a messy exponent in the upper bound which isn't always tighter than the one given by Chernoff bound. My main question is: Is there a way where I can prove a tighter or a normal-looking bound, with the first exponent of the form Thanks a lot!","X\sim \operatorname{Binom}(n,p_0) Y\sim \operatorname{Binom}(n,p_1) p_0>p_1 P(X-Y\le 0) X-Y \operatorname{Bern}(p_0) \operatorname{Bern}(p_1) P(X-Y\le 0)\le \exp\left(-\frac{1}{2}n(p_0-p_1)^2\right) \exp\left(-\frac{1}{2} n\frac{(p_0-p_1)^2}{p_0(1-p_0)+p_1(1-p_1)}\right).","['probability', 'statistics', 'binomial-distribution']"
70,Expected closest distance to a point,Expected closest distance to a point,,"Consider $X_1, X_2, \dots X_n $ all I.I.D Uniform $[0,1]$ . What is the expected distance from a uniformly selected point to its closest neighbour? I know that the expected point closest greater than it is $\frac{1}{n+1}$ . And the same result for expect point closest less than it is $\frac{1}{n+1}$ . This is clearly an upper bound on our answer. Notice the problem has a lack of symmetry. That is the expected closest point to $0$ is $\frac{1}{n+1}$ But the expected closest point to $\frac{1}{2}$ is $\frac{1}{2(n+1)}$ We do however have symmetry around one half. I wanted to condition the expectation on the selected point and integrate. I was thinking of using the law of total expectation. The sigma-algebra being generated by the number of points in $[0,2x]$ and using linearity rescale the expected closest point to $\frac{1}{2}$ . However when using the Tower Rule we have a Binomial RV on the denominator and this is tricky. Reforming the problem as this: Let $X \sim $ Uniform $[0,1]$ Then $Y_1 , Y_2 , \dots Y_{n-1}$ be IID Uniform $[0,1]$ and we are after $\mathbb{E}$ [Min $_{i \in [1,n-1]}$$\{ |X - Y_i| \}$ ] We could then find the CDF, differentiate for PDF and integrate for expectation, due to symmetry around $\frac{1}{2}$ consider only $x \in[0,\frac{1}{2}]$ Which of my methods seems most fruitful ? Is there much simpler way of doing this? If it helps here is a plot with values $n=2$ to $n=20$","Consider all I.I.D Uniform . What is the expected distance from a uniformly selected point to its closest neighbour? I know that the expected point closest greater than it is . And the same result for expect point closest less than it is . This is clearly an upper bound on our answer. Notice the problem has a lack of symmetry. That is the expected closest point to is But the expected closest point to is We do however have symmetry around one half. I wanted to condition the expectation on the selected point and integrate. I was thinking of using the law of total expectation. The sigma-algebra being generated by the number of points in and using linearity rescale the expected closest point to . However when using the Tower Rule we have a Binomial RV on the denominator and this is tricky. Reforming the problem as this: Let Uniform Then be IID Uniform and we are after [Min ] We could then find the CDF, differentiate for PDF and integrate for expectation, due to symmetry around consider only Which of my methods seems most fruitful ? Is there much simpler way of doing this? If it helps here is a plot with values to","X_1, X_2, \dots X_n  [0,1] \frac{1}{n+1} \frac{1}{n+1} 0 \frac{1}{n+1} \frac{1}{2} \frac{1}{2(n+1)} [0,2x] \frac{1}{2} X \sim  [0,1] Y_1 , Y_2 , \dots Y_{n-1} [0,1] \mathbb{E} _{i \in [1,n-1]}\{ |X - Y_i| \} \frac{1}{2} x \in[0,\frac{1}{2}] n=2 n=20","['probability', 'statistics']"
71,Probabilistic interpretation linear regression implication step,Probabilistic interpretation linear regression implication step,,"I am reading Andrew Ng's notes on linear regression, and in this section, he attempts to derive the formula for the least squares using a probability approach: http://cs229.stanford.edu/summer2020/cs229-notes1.pdf We assume: $$y^{(i)}=\theta ^{T}x^{(i)}+e^{(i)}$$ where where $\epsilon$ is distributed according to the normal distrubtion: $N(0, \sigma^2)$ Thus, we can find the probability of $e^{(i)}$ as $$p(e^{(i)})=\frac{1}{\sqrt{2π}σ}exp(-\frac{(e^{(i)})^2}{2σ^2})$$ However, in the next step, he says that this implies $$p(y^{(i)}|x^{(i)}; \theta)=\frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2σ^2})$$ and I am not sure how. We see that $e$ is a RV and is distributed according the normal, so then how can we calculate the probability on something that isn't $e$ ?","I am reading Andrew Ng's notes on linear regression, and in this section, he attempts to derive the formula for the least squares using a probability approach: http://cs229.stanford.edu/summer2020/cs229-notes1.pdf We assume: where where is distributed according to the normal distrubtion: Thus, we can find the probability of as However, in the next step, he says that this implies and I am not sure how. We see that is a RV and is distributed according the normal, so then how can we calculate the probability on something that isn't ?","y^{(i)}=\theta ^{T}x^{(i)}+e^{(i)} \epsilon N(0, \sigma^2) e^{(i)} p(e^{(i)})=\frac{1}{\sqrt{2π}σ}exp(-\frac{(e^{(i)})^2}{2σ^2}) p(y^{(i)}|x^{(i)}; \theta)=\frac{1}{\sqrt{2π}σ}exp(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2σ^2}) e e","['statistics', 'machine-learning', 'linear-regression']"
72,"Show that the joint distribution of $X_1, X_2, \dots, X_n$ belongs to a two-parameter exponential family.",Show that the joint distribution of  belongs to a two-parameter exponential family.,"X_1, X_2, \dots, X_n","Let $X_1, X_2, \dots, X_n$ be ind $ \sim\text{Ber}(\theta_i)$ where \begin{equation}         \theta_i = P(X_i=1)=\frac{\exp(\alpha+\beta t_i)}{1+\exp(\alpha + \beta t_i)} \end{equation} where $t_1, t_2, \dots, t_n$ are known constants.  Show that the joint distribution of $X_1, X_2, \dots, X_n$ belongs to a two-parameter exponential family. $\bf{My Attempt (WIP) :}$ We can start by finding the joint pmf of $X_1, X_2, \dots, X_n$ \begin{align*}         f(\mathbf{X}) &= \prod_{i=1}^n \theta_i^{x_i} (1-\theta_i)^{1-x_i}\\         &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (1-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\         &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1+e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}}-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\         &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\         &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i + 1 - x_i}}  \\         &=\prod_{i=1}^n \frac{e^{\alpha x_i+\beta t_ix_i}}{1+e^{\alpha+\beta t_i}}\\         &=\frac{e^{\alpha\sum_{i=1}^n x_i+\beta\sum_{i=1}^n t_i x_i} }{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}     \end{align*} This is exponential family with $T(x) = (\sum x_i, \sum t_i x_i)$ , $\eta= (\alpha, \beta)$ , and $A(\eta) =A(\eta)=\frac{1}{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}$","Let be ind where where are known constants.  Show that the joint distribution of belongs to a two-parameter exponential family. We can start by finding the joint pmf of This is exponential family with , , and","X_1, X_2, \dots, X_n  \sim\text{Ber}(\theta_i) \begin{equation}
        \theta_i = P(X_i=1)=\frac{\exp(\alpha+\beta t_i)}{1+\exp(\alpha + \beta t_i)}
\end{equation} t_1, t_2, \dots, t_n X_1, X_2, \dots, X_n \bf{My Attempt (WIP) :} X_1, X_2, \dots, X_n \begin{align*}
        f(\mathbf{X}) &= \prod_{i=1}^n \theta_i^{x_i} (1-\theta_i)^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (1-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1+e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}}-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i + 1 - x_i}}  \\
        &=\prod_{i=1}^n \frac{e^{\alpha x_i+\beta t_ix_i}}{1+e^{\alpha+\beta t_i}}\\
        &=\frac{e^{\alpha\sum_{i=1}^n x_i+\beta\sum_{i=1}^n t_i x_i} }{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}
    \end{align*} T(x) = (\sum x_i, \sum t_i x_i) \eta= (\alpha, \beta) A(\eta) =A(\eta)=\frac{1}{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}","['statistics', 'probability-distributions', 'statistical-inference', 'logistic-regression']"
73,"Prove that $M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k}$",Prove that,"M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k}","I want to prove that $M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k}$ Where $X_{1} , X_{2} , ... , X_{n}$ are i.i.d with expected value $E(X_{i}) = \mu$ Also let the k-th central moment be $$\mu_k = E(X - \mu)^{k}$$ A natural estimator is the k-th sample moment,given by $$M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k}$$ I know that for $k = 2$ due to the law of large numbers and the fact that $g(x) = x^{2}$ is a continuous function. That $$M_{n , 2} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{2} = \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{2} - \bar{X_{n}}^{2} \overset{p}\longrightarrow \mu_{2}$$ But how do I prove this for any k since. $$M_{n , K} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{K} \ne \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{K} - X_{n}^{k}$$","I want to prove that Where are i.i.d with expected value Also let the k-th central moment be A natural estimator is the k-th sample moment,given by I know that for due to the law of large numbers and the fact that is a continuous function. That But how do I prove this for any k since.","M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k} X_{1} , X_{2} , ... , X_{n} E(X_{i}) = \mu \mu_k = E(X - \mu)^{k} M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} k = 2 g(x) = x^{2} M_{n , 2} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{2} = \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{2} - \bar{X_{n}}^{2} \overset{p}\longrightarrow \mu_{2} M_{n , K} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{K} \ne \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{K} - X_{n}^{k}","['probability-theory', 'statistics', 'convergence-divergence']"
74,Minimal sufficient statistic for normal bivariate is complete?,Minimal sufficient statistic for normal bivariate is complete?,,"Let $\mathbf{Z}_1, \mathbf{Z}_2, \ldots, \mathbf{Z}_n$ be iid random sample of size $n$ where $\mathbf{Z}_i = (X_i,Y_i)^T$ is a normal bivariate distribution $\mathcal{N}_2(\mathbf{0},\Sigma)$ such that $$ \Sigma = \begin{pmatrix}1&\rho\\ \rho &1\end{pmatrix} \qquad \text{and} \qquad \mathbf{0}=(0,0)^T$$ I want to find a minimal sufficient statistic for $\rho$ . Here, I denote $\underline{Z}=(\mathbf{Z}_1,\mathbf{Z}_2,\ldots, \mathbf{Z}_n)$ . By the Theorem 5.3 in this e-book , I compute the ratio of the likelihood function for two samples $\underline{Z}$ and $\underline{W}$ . Since $f_\mathbf{Z}(\mathbf{z}\mid\rho)=(2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{1}{2}[\mathbf{z}^T\Sigma^{-1}\mathbf{z}]\right)$ , then we have \begin{align*} f_\underline{Z}(\underline{z}\mid\rho) &= \prod_{i=1}^n f_\mathbf{Z}(\mathbf{z}_i\mid\rho)=\prod_{i=1}^n (2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i}{2}\right)\\ &=(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right) \end{align*} Thus, \begin{align*} \frac{f_\underline{Z}(\underline{z}\mid\rho)}{f_\underline{Z}(\underline{w}\mid\rho)}&=\frac{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right)}{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)}\\ &=\exp\left(-\frac{1}{2}\left(\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)\right) \end{align*} The ratio turns to be a constant for $\rho$ if and only if \begin{equation} \sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i=0 \quad \cdots \quad(\dagger) \end{equation} Therefore, \begin{align*} \sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i&=\frac{1}{1-\rho^2}\sum_{i=1}^n \left\{x_i^2-2\rho x_iy_i + y_i^2\right\}\\ &= \frac{1}{1-\rho^2}\left(\sum_{i=1}^n x_i^2-2\rho \sum_{i=1}^n x_iy_i + \sum_{i=1}^ny_i^2\right) \end{align*} If we denote $S_X=\sum_{i=1}^n X_i^2$ , $S_Y=\sum_{i=1}^n Y_i^2$ and $S_{XY}=\sum_{i=1}^n X_iY_i$ Here, I define $T(\underline{z})=(S_X, S_Y, S_{XY})$ . Therefore, if $T(\underline{Z}) = T(\underline{W})$ , then $(\dagger)$ satifies. This proves that $T$ is a minimal sufficient statistic for $\rho$ . The second task is checking if $T$ is a complete statistic... but I can't find a way to prove if is or not complete. The complete statistic definition that I have just take any function $g$ and doesn't take in consideration the ""measurable function"" as in the definition of complete statistic from Wikipedia .","Let be iid random sample of size where is a normal bivariate distribution such that I want to find a minimal sufficient statistic for . Here, I denote . By the Theorem 5.3 in this e-book , I compute the ratio of the likelihood function for two samples and . Since , then we have Thus, The ratio turns to be a constant for if and only if Therefore, If we denote , and Here, I define . Therefore, if , then satifies. This proves that is a minimal sufficient statistic for . The second task is checking if is a complete statistic... but I can't find a way to prove if is or not complete. The complete statistic definition that I have just take any function and doesn't take in consideration the ""measurable function"" as in the definition of complete statistic from Wikipedia .","\mathbf{Z}_1, \mathbf{Z}_2, \ldots, \mathbf{Z}_n n \mathbf{Z}_i = (X_i,Y_i)^T \mathcal{N}_2(\mathbf{0},\Sigma)  \Sigma = \begin{pmatrix}1&\rho\\ \rho &1\end{pmatrix} \qquad \text{and} \qquad \mathbf{0}=(0,0)^T \rho \underline{Z}=(\mathbf{Z}_1,\mathbf{Z}_2,\ldots, \mathbf{Z}_n) \underline{Z} \underline{W} f_\mathbf{Z}(\mathbf{z}\mid\rho)=(2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{1}{2}[\mathbf{z}^T\Sigma^{-1}\mathbf{z}]\right) \begin{align*}
f_\underline{Z}(\underline{z}\mid\rho) &= \prod_{i=1}^n f_\mathbf{Z}(\mathbf{z}_i\mid\rho)=\prod_{i=1}^n (2\pi)^{-1}|\Sigma|^{-1/2}\exp\left(-\frac{\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i}{2}\right)\\
&=(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right)
\end{align*} \begin{align*}
\frac{f_\underline{Z}(\underline{z}\mid\rho)}{f_\underline{Z}(\underline{w}\mid\rho)}&=\frac{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i\right)}{(2\pi)^{-n}|\Sigma|^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)}\\
&=\exp\left(-\frac{1}{2}\left(\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i\right)\right)
\end{align*} \rho \begin{equation}
\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i- \sum_{i=1}^n\mathbf{w}_i^T\Sigma^{-1}\mathbf{w}_i=0 \quad \cdots \quad(\dagger)
\end{equation} \begin{align*}
\sum_{i=1}^n\mathbf{z}_i^T\Sigma^{-1}\mathbf{z}_i&=\frac{1}{1-\rho^2}\sum_{i=1}^n \left\{x_i^2-2\rho x_iy_i + y_i^2\right\}\\
&= \frac{1}{1-\rho^2}\left(\sum_{i=1}^n x_i^2-2\rho \sum_{i=1}^n x_iy_i + \sum_{i=1}^ny_i^2\right)
\end{align*} S_X=\sum_{i=1}^n X_i^2 S_Y=\sum_{i=1}^n Y_i^2 S_{XY}=\sum_{i=1}^n X_iY_i T(\underline{z})=(S_X, S_Y, S_{XY}) T(\underline{Z}) = T(\underline{W}) (\dagger) T \rho T g","['statistics', 'statistical-inference']"
75,Mixture distribution,Mixture distribution,,"I'm trying to come up with a reasonable way to determine the weights in a mixture distribution. Let us consider the following example: There are two districts ( $i=1,2$ ) in a city, both of which share one hospital. The distance between the districts and the hospital is denoted by $D_i, i=1,2.$ Incidents occur in both districts according to a Poisson process with intensity $\lambda$ (the same parameter for both districts). However, district 1 can transfer patients to the hospital at a unique rate of $\theta$ . In other words, transportation opportunities become available at rate $\theta$ and when it becomes available, it takes all patients waiting in that district to the hospital. Obviously, if there are not patients, there will be no dispatch and let us assume the vehicle is big enough to fit everyone! For district 2 however, there's only one vehicle that is always available but it must be full before it can be dispatched and let us denote the capacity of this vehicle by $\gamma$ If we denote the number of patients leaving every district by $X_i$ , for district 1, this is a random variable that follows a geometric distribution so we have $\mathbb{E}[X_1] = \frac{\lambda}{\theta}$ . For district 2, it is just a constant number that is $\mathbb{E}[X_2] = \gamma.$ To calculate $\mathbb{E}[Y]$ , that is the average number of patients arriving at the hospital at a given point in time, by definition, we have $$\mathbb{E}[Y] = \sum_{i=1,2} w_i \mathbb{E}[X_i].$$ Now what would be a neat way to determine $w_i, i = 1,2$ ? I think $$\text{probability}(Y = X_i) \propto \text{Frequency of dispatches from district } i,$$ which in turn depends on $\lambda$ and $\theta$ and $$\text{probability}(Y = X_i) \propto 1/D_i.$$ Any suggestions would be much appreciated.","I'm trying to come up with a reasonable way to determine the weights in a mixture distribution. Let us consider the following example: There are two districts ( ) in a city, both of which share one hospital. The distance between the districts and the hospital is denoted by Incidents occur in both districts according to a Poisson process with intensity (the same parameter for both districts). However, district 1 can transfer patients to the hospital at a unique rate of . In other words, transportation opportunities become available at rate and when it becomes available, it takes all patients waiting in that district to the hospital. Obviously, if there are not patients, there will be no dispatch and let us assume the vehicle is big enough to fit everyone! For district 2 however, there's only one vehicle that is always available but it must be full before it can be dispatched and let us denote the capacity of this vehicle by If we denote the number of patients leaving every district by , for district 1, this is a random variable that follows a geometric distribution so we have . For district 2, it is just a constant number that is To calculate , that is the average number of patients arriving at the hospital at a given point in time, by definition, we have Now what would be a neat way to determine ? I think which in turn depends on and and Any suggestions would be much appreciated.","i=1,2 D_i, i=1,2. \lambda \theta \theta \gamma X_i \mathbb{E}[X_1] = \frac{\lambda}{\theta} \mathbb{E}[X_2] = \gamma. \mathbb{E}[Y] \mathbb{E}[Y] = \sum_{i=1,2} w_i \mathbb{E}[X_i]. w_i, i = 1,2 \text{probability}(Y = X_i) \propto \text{Frequency of dispatches from district } i, \lambda \theta \text{probability}(Y = X_i) \propto 1/D_i.","['probability', 'statistics', 'probability-distributions']"
76,Probability taking a loan in a bank,Probability taking a loan in a bank,,"A bank has $1500$ guests. From them the $1000$ has taken home loan and $700$ consumer loan. a) Calculate the probability that a guest has taken home loan given that he has taken consumer loan. b) The bank want to make an offer  for consumer loan for those who took a home loan but not a consumer loan. To how many people will they send the offer? c) Each day at the bank come average $4$ guests that want to take a loan. What is the probability that one day between 8 in the morning till 4 in the afternoon, that $1$ or $2$ guests get served? d) for the guests that took consumer loan we know that $40\%$ are women and $60\%$ are men. From the men the $50\%$ are of age $45-55$ and from the women $40\%$ . A guest of age $45-55$ is in the bank to take a loan. Which is the probability that it is a woman of age $45-55$ ? e) For the guests that took consumer loan we know that the probability  one of them took a loan less than $5000$ euros is $0.14$ . If we take randomly $7$ guests, then which is probability at most $2$ of them have taken a loan less than $5000$ euros ? f) If we check these $7$ guests whici is the probability only the fourth one has taken loan less than $7000$ euros? $$$$ a) Does it hold that $P(H\cap C)=\frac{200}{1500}$ since $1000+700-1500=200$ ? Then the probability is equal to $$P(H\mid C)=\frac{P(H\cap C)}{P(C)}=\frac{\frac{200}{1500}}{\frac{700}{1500}}=\frac{200}{700}\approx 28,57\%$$ Is that correct? b) We have to calculate the number for $H\cap C^c$ . Is this equal to $1000-700 =300$ ? c) Do we have Poisson distribution since we have an average? We have that $\lambda=4$ . Then $$P(X=1)+P(X=2)=e^{-4}\cdot \frac{4^1}{1!}+e^{-4}\cdot \frac{4^2}{2!}=\frac{12}{e^4}\approx 0.21979$$ Is that correct? d) Is the probability equal to \begin{align*}P(W\mid A)&=\frac{P(A\mid W)\cdot P(W)}{P(A)}\\ & =\frac{P(A\mid W)\cdot P(W)}{P(A\mid W)\cdot P(W)+P(A\mid M)\cdot P(M)}\\ & =\frac{0.40\cdot 0.40}{0.40\cdot 0.40+0.50\cdot 0.60}\\ & =34.78\%\end{align*} e) Is the probability equal to \begin{align*}P(X\leq 2)&=P(X=0)+P(X=1)+P(X=2)\\ & =\binom{7}{0}\cdot 0.14^0\cdot (1-0.14)^{7-0}+\binom{7}{1}\cdot 0.14^1\cdot (1-0.14)^{7-1}+\binom{7}{2}\cdot 0.14^2\cdot (1-0.14)^{7-2}\\ & \approx 0.9380\end{align*} f) Is there a typo and it must be $5000$ instead of $7000$ ? Or is it possible to calculate the probability with $7000$ without knowing the probability for ""sucess"" for that?","A bank has guests. From them the has taken home loan and consumer loan. a) Calculate the probability that a guest has taken home loan given that he has taken consumer loan. b) The bank want to make an offer  for consumer loan for those who took a home loan but not a consumer loan. To how many people will they send the offer? c) Each day at the bank come average guests that want to take a loan. What is the probability that one day between 8 in the morning till 4 in the afternoon, that or guests get served? d) for the guests that took consumer loan we know that are women and are men. From the men the are of age and from the women . A guest of age is in the bank to take a loan. Which is the probability that it is a woman of age ? e) For the guests that took consumer loan we know that the probability  one of them took a loan less than euros is . If we take randomly guests, then which is probability at most of them have taken a loan less than euros ? f) If we check these guests whici is the probability only the fourth one has taken loan less than euros? a) Does it hold that since ? Then the probability is equal to Is that correct? b) We have to calculate the number for . Is this equal to ? c) Do we have Poisson distribution since we have an average? We have that . Then Is that correct? d) Is the probability equal to e) Is the probability equal to f) Is there a typo and it must be instead of ? Or is it possible to calculate the probability with without knowing the probability for ""sucess"" for that?","1500 1000 700 4 1 2 40\% 60\% 50\% 45-55 40\% 45-55 45-55 5000 0.14 7 2 5000 7 7000  P(H\cap C)=\frac{200}{1500} 1000+700-1500=200 P(H\mid C)=\frac{P(H\cap C)}{P(C)}=\frac{\frac{200}{1500}}{\frac{700}{1500}}=\frac{200}{700}\approx 28,57\% H\cap C^c 1000-700 =300 \lambda=4 P(X=1)+P(X=2)=e^{-4}\cdot \frac{4^1}{1!}+e^{-4}\cdot \frac{4^2}{2!}=\frac{12}{e^4}\approx 0.21979 \begin{align*}P(W\mid A)&=\frac{P(A\mid W)\cdot P(W)}{P(A)}\\ & =\frac{P(A\mid W)\cdot P(W)}{P(A\mid W)\cdot P(W)+P(A\mid M)\cdot P(M)}\\ & =\frac{0.40\cdot 0.40}{0.40\cdot 0.40+0.50\cdot 0.60}\\ & =34.78\%\end{align*} \begin{align*}P(X\leq 2)&=P(X=0)+P(X=1)+P(X=2)\\ & =\binom{7}{0}\cdot 0.14^0\cdot (1-0.14)^{7-0}+\binom{7}{1}\cdot 0.14^1\cdot (1-0.14)^{7-1}+\binom{7}{2}\cdot 0.14^2\cdot (1-0.14)^{7-2}\\ & \approx 0.9380\end{align*} 5000 7000 7000","['probability', 'probability-theory', 'statistics', 'conditional-probability', 'bayes-theorem']"
77,Sufficient statistic for normal distribution (not iid),Sufficient statistic for normal distribution (not iid),,"I am a bit confused with this exercise, since I never worked with samples of this type. I would appreciate if you can help me. The exercise is as follows: Let $\{Xi\} \sim N(iθ, 1)$ for $i = 1, .... , n$ be an independent, but not identically distributed sample. Check that $T = \sum_iX_i$ it is a sufficient statistic for $θ$ . What I need is to verify that the $T$ statistic is sufficient for the $\theta$ parameter. I know how to do it with the Fisher and Neymann factorization theorem, but always with a identically distributed sample of random variables. In this case, the sample is not identically distributed. Therefore, I don't know how to verify it.","I am a bit confused with this exercise, since I never worked with samples of this type. I would appreciate if you can help me. The exercise is as follows: Let for be an independent, but not identically distributed sample. Check that it is a sufficient statistic for . What I need is to verify that the statistic is sufficient for the parameter. I know how to do it with the Fisher and Neymann factorization theorem, but always with a identically distributed sample of random variables. In this case, the sample is not identically distributed. Therefore, I don't know how to verify it.","\{Xi\} \sim N(iθ, 1) i = 1, .... , n T = \sum_iX_i θ T \theta","['statistics', 'sufficient-statistics']"
78,"Given i.i.d. random variables $X, Y \sim N(0, 1)$, find the conditional distribution of $X$ given that $X + Y > 0.$","Given i.i.d. random variables , find the conditional distribution of  given that","X, Y \sim N(0, 1) X X + Y > 0.","Given i.i.d. random variables $X, Y \sim N(0, 1)$ , find the conditional distribution of $X$ given that $X + Y > 0.$ First approach: Find the conditional CDF of $X$ given $X+Y >0$ : $F_X(x | X+Y >0 ) = \mathbb{P}[X<x | X+Y > 0 ] = \mathbb{P}[X+Y >0 | X<x] \cdot \frac{\mathbb{P}[X<x]}{\mathbb{P}[X+Y >0 ]}$ . Notice that $X+Y \sim N(0,2)$ and so $\mathbb{P}[X+Y >0 ] = \frac{1}{2}$ However I cannot seem to calculate nicely $\mathbb{P}[X+Y >0 | X<x]$ Is there a better way to approach this problem? Can someone calculate $\mathbb{P}[X+Y >0 | X<x] $ , I am going to differentiate it anyways so It can be as an integral.","Given i.i.d. random variables , find the conditional distribution of given that First approach: Find the conditional CDF of given : . Notice that and so However I cannot seem to calculate nicely Is there a better way to approach this problem? Can someone calculate , I am going to differentiate it anyways so It can be as an integral.","X, Y \sim N(0, 1) X X + Y > 0. X X+Y >0 F_X(x | X+Y >0 ) = \mathbb{P}[X<x | X+Y > 0 ] = \mathbb{P}[X+Y >0 | X<x] \cdot \frac{\mathbb{P}[X<x]}{\mathbb{P}[X+Y >0 ]} X+Y \sim N(0,2) \mathbb{P}[X+Y >0 ] = \frac{1}{2} \mathbb{P}[X+Y >0 | X<x] \mathbb{P}[X+Y >0 | X<x] ","['probability', 'statistics', 'probability-distributions', 'bayesian']"
79,A Gambling Problem (statistics),A Gambling Problem (statistics),,"I was watching a twitch stream the other day and the streamer was giving away his money based on a dice game.  The game works as follows: First, he rolls a 5 sided die to determine your initial starting money (from 1 to 5 dollars).  From then on, you can either take the total money, or roll again.  If you roll again, the value of the die will be added to your total money, unless you roll a 1 where you will lose it all. He created a second game where you roll an 8 sided die with similar rules to the first game, except you lose all your money if you roll a 1 or an 8. People in chat said that it was worse than the first game (since now you have a 75% chance to lose all your money instead of 80%), but there seems to be something intuitively wrong about it I can't explain. I'm not a statistician, but if someone could point me towards some sort of way to figure out which game is better for the player it'd be much appreciated!","I was watching a twitch stream the other day and the streamer was giving away his money based on a dice game.  The game works as follows: First, he rolls a 5 sided die to determine your initial starting money (from 1 to 5 dollars).  From then on, you can either take the total money, or roll again.  If you roll again, the value of the die will be added to your total money, unless you roll a 1 where you will lose it all. He created a second game where you roll an 8 sided die with similar rules to the first game, except you lose all your money if you roll a 1 or an 8. People in chat said that it was worse than the first game (since now you have a 75% chance to lose all your money instead of 80%), but there seems to be something intuitively wrong about it I can't explain. I'm not a statistician, but if someone could point me towards some sort of way to figure out which game is better for the player it'd be much appreciated!",,"['statistics', 'gambling']"
80,"Find UMVUE of $\tau=(\lambda-\mu)e^{-(\lambda+\mu)}$ from samples $X_1, \dots, X_n\sim \rm{Pois}(\lambda)$ and $Y_1,\dots, Y_n\sim \rm{Pois}(\mu)$.",Find UMVUE of  from samples  and .,"\tau=(\lambda-\mu)e^{-(\lambda+\mu)} X_1, \dots, X_n\sim \rm{Pois}(\lambda) Y_1,\dots, Y_n\sim \rm{Pois}(\mu)","Problem Statement We have two independent Poisson samples $X_1, \ldots, X_n$ with means $\lambda$ and $Y_1, \ldots, Y_n$ with means $\mu$ . We would like to estimate $\tau=(\lambda-\mu)e^{-(\lambda+\mu)}$ . (a) Find a function of $X_1$ and $Y_1$ that is an ubiased estimator of $\tau$ . (b) Find the UMVUE of $\tau$ . (c) Calculate the asymptotic variance of this estimator when $\lambda=\mu$ . Context I am studying some old exams, and I came across this problem. I think I have the correct unbiased estimator, but my UMVUE is definitely wrong since it is a function of unknown parameters. I am not sure if I made a mistake with my complete sufficient statistic, or I did the conditional expectation wrong. Attempted Solution (a) Find some function $g(X_1,Y_1)$ such that $\mathbb{E}(g)=\tau$ . $$ \tau = \lambda e^{-(\lambda+\mu)}-\mu e^{-(\lambda+\mu)}=\mathbb{P}(X_1=1, Y_1=0)-\mathbb{P}(X_1=0, Y_1=1)=\mathbb{E}\big[I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1)\big], $$ therefore, $U=I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1)$ is unbiased for $\tau$ . (b) The Lehman-Scheffe theorem tells us that, given a complete sufficient statistic $T$ and an unbiased estimate $U$ , the UMVUE is $E(U|T)$ . Let $S_j=\sum_{i=j}^nX_i+Y_i$ . From previous results we know that $S_j\sim Pois((n+1-j)(\lambda+\mu))$ , where Poisson is in the exponential family of distributions and $S_1$ is a complete sufficient statistic. Therefore, \begin{equation*}     \begin{split}         E(U|S_1=t)=& P(X_1=1,Y_1=0|S_1=t)-P(X_1=0, Y_1=1|S_1=t)\\         =& \frac{P(X_1=1,Y_1=0, S_2=t-1)-P(X_1=0,Y_1=1, S_2=t-1)}{P(S_1=t)}\\         \overset{ind.}{=}& \frac{P(X_1=1)P(Y_1=0)P(S_2=t-1)-P(X_1=0)P(Y_1=1)P(S_2=t-1)}{P(S_1=t)}\\         =&\bigg(\frac{n-1}{n}\bigg)^t\frac{t}{(n-1)(\lambda+\mu)}(\lambda-\mu),     \end{split} \end{equation*} is the UMVUE for $\tau$ .","Problem Statement We have two independent Poisson samples with means and with means . We would like to estimate . (a) Find a function of and that is an ubiased estimator of . (b) Find the UMVUE of . (c) Calculate the asymptotic variance of this estimator when . Context I am studying some old exams, and I came across this problem. I think I have the correct unbiased estimator, but my UMVUE is definitely wrong since it is a function of unknown parameters. I am not sure if I made a mistake with my complete sufficient statistic, or I did the conditional expectation wrong. Attempted Solution (a) Find some function such that . therefore, is unbiased for . (b) The Lehman-Scheffe theorem tells us that, given a complete sufficient statistic and an unbiased estimate , the UMVUE is . Let . From previous results we know that , where Poisson is in the exponential family of distributions and is a complete sufficient statistic. Therefore, is the UMVUE for .","X_1, \ldots, X_n \lambda Y_1, \ldots, Y_n \mu \tau=(\lambda-\mu)e^{-(\lambda+\mu)} X_1 Y_1 \tau \tau \lambda=\mu g(X_1,Y_1) \mathbb{E}(g)=\tau 
\tau = \lambda e^{-(\lambda+\mu)}-\mu e^{-(\lambda+\mu)}=\mathbb{P}(X_1=1, Y_1=0)-\mathbb{P}(X_1=0, Y_1=1)=\mathbb{E}\big[I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1)\big],
 U=I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1) \tau T U E(U|T) S_j=\sum_{i=j}^nX_i+Y_i S_j\sim Pois((n+1-j)(\lambda+\mu)) S_1 \begin{equation*}
    \begin{split}
        E(U|S_1=t)=& P(X_1=1,Y_1=0|S_1=t)-P(X_1=0, Y_1=1|S_1=t)\\
        =& \frac{P(X_1=1,Y_1=0, S_2=t-1)-P(X_1=0,Y_1=1, S_2=t-1)}{P(S_1=t)}\\
        \overset{ind.}{=}& \frac{P(X_1=1)P(Y_1=0)P(S_2=t-1)-P(X_1=0)P(Y_1=1)P(S_2=t-1)}{P(S_1=t)}\\
        =&\bigg(\frac{n-1}{n}\bigg)^t\frac{t}{(n-1)(\lambda+\mu)}(\lambda-\mu),
    \end{split}
\end{equation*} \tau","['statistics', 'probability-distributions', 'conditional-expectation', 'poisson-distribution', 'parameter-estimation']"
81,"Prove that $\mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y)$, for $x<y.$","Prove that , for",\mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y) x<y.,"Let $X_1, \cdots,X_n $ be iid random variables with distribution $F. T_n(x)$ denotes the number of elements $\le x; x \in \mathbb R$ . Prove that $\mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y)$ , for $x<y$ . Attempt Let $j = T_n(y) ,i = T_n(x)$ . Then $\mathbb E [T_n(x) ~ T_n(y)] = \sum_{j = 0}^n ~\sum_{i = 0} ^j ~ ^jC_i~ [F_X(x)]^i [F_X(y) - F_X(x)]^{j-i} \cdot j \cdot i$ $= \sum_{j = 0}^n j \cdot [F_X(y) - F_X(x)]^{j}~~\sum_{i = 0} ^j ~ ^jC_i~ \dfrac {[F_X(x)]^i} {[F_X(y) - F_X(x)]^{i}} \cdot i$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j}~ \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} ~~\sum_{i = 0} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\sum_{i = 1} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~~\sum_{m = 0} ^{j-1} ~ ^{j-1}C_m~ \dfrac {[F_X(x)]^m} {[F_X(y) - F_X(x)]^m}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} +1 \big)^{j-1}$ $= \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(y)]} {[F_X(y) - F_X(x)]} \big)^{j-1}$ $= \sum_{j = 0}^n j^2 \cdot {[F_X(x)]}~{[F_X(y)]}^{j-1} $ $= \dfrac{F_X(x)}{F_X(y)}\sum_{j = 0}^n j^2 \cdot ~{[F_X(y)]}^{j} $ This doesn't lead to the result being proved. Could someone advise?","Let be iid random variables with distribution denotes the number of elements . Prove that , for . Attempt Let . Then This doesn't lead to the result being proved. Could someone advise?","X_1, \cdots,X_n  F. T_n(x) \le x; x \in \mathbb R \mathbb E [T_n(x) ~ T_n(y)] = nF_X(x) + n(n - 1)F_X(x)F_X(y) x<y j = T_n(y) ,i = T_n(x) \mathbb E [T_n(x) ~ T_n(y)] = \sum_{j = 0}^n ~\sum_{i = 0} ^j ~ ^jC_i~ [F_X(x)]^i [F_X(y) - F_X(x)]^{j-i} \cdot j \cdot i = \sum_{j = 0}^n j \cdot [F_X(y) - F_X(x)]^{j}~~\sum_{i = 0} ^j ~ ^jC_i~ \dfrac {[F_X(x)]^i} {[F_X(y) - F_X(x)]^{i}} \cdot i = \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j}~ \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} ~~\sum_{i = 0} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}} = \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\sum_{i = 1} ^j ~ ^{j-1}C_{i-1}~ \dfrac {[F_X(x)]^{i-1}} {[F_X(y) - F_X(x)]^{i-1}} = \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~~\sum_{m = 0} ^{j-1} ~ ^{j-1}C_m~ \dfrac {[F_X(x)]^m} {[F_X(y) - F_X(x)]^m} = \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(x)]} {[F_X(y) - F_X(x)]} +1 \big)^{j-1} = \sum_{j = 0}^n j^2 \cdot [F_X(y) - F_X(x)]^{j-1}~ {[F_X(x)]}~\big( \dfrac {[F_X(y)]} {[F_X(y) - F_X(x)]} \big)^{j-1} = \sum_{j = 0}^n j^2 \cdot {[F_X(x)]}~{[F_X(y)]}^{j-1}  = \dfrac{F_X(x)}{F_X(y)}\sum_{j = 0}^n j^2 \cdot ~{[F_X(y)]}^{j} ","['statistics', 'probability-distributions', 'order-statistics']"
82,"Multiple Events, Independence and Conditional Independence","Multiple Events, Independence and Conditional Independence",,"Hope you are doing well. I found an online resource from MIT that I found valuable. But, it is just notes with scenarios but no answers. In particular, I am focused on the bottom: ""If we don't know which coin it is, are the tosses independent?"". P(toss 11 = H) should be 1/2 because we don't which coin it is and we have no other information. But what is the formal proof for it? The P(toss 11 = H |first 10 tosses are H) appear dependent as we can infer from the new information that since the first 10 tosses are H, the coin in question is Coin A. Again, is there a formal proof or mathematical way to think about it?","Hope you are doing well. I found an online resource from MIT that I found valuable. But, it is just notes with scenarios but no answers. In particular, I am focused on the bottom: ""If we don't know which coin it is, are the tosses independent?"". P(toss 11 = H) should be 1/2 because we don't which coin it is and we have no other information. But what is the formal proof for it? The P(toss 11 = H |first 10 tosses are H) appear dependent as we can infer from the new information that since the first 10 tosses are H, the coin in question is Coin A. Again, is there a formal proof or mathematical way to think about it?",,"['probability', 'probability-theory']"
83,Sample size as a part of minimal sufficient statistic,Sample size as a part of minimal sufficient statistic,,"In Casella and Berger, we have the following experiment: assume we first select a positive integer randomly, let us call it $N \in \{1, 2, \ldots \}$ , s.t. $P(N = n) = p_n$ . Then, for this $N = n$ , we perform a classic Binomial experiment with parameter $\theta$ . The number of positives/ones in the sample is $X$ . It is asked to show that $(X, N)$ is a minimal sufficient statistic. I found the joint distribution of the statistic: $$f(x, n) = {n\choose x}\theta^x(1 - \theta)^{n - x}p_n$$ and the sample: $$f(x_1, \ldots, x_n \mid \theta) = \sum_{n = 1}^\infty p_n \theta^{\sum_{i = 1}^n x_i}(1-\theta)^{n - \sum_{i = 1}^n x_i}$$ By factorization theorem, we can show that this is a sufficient statistic. This series must be convergent (marginal), and we can assume that the above is a function of $X = \sum_{i = 1}^N X_i$ and $N$ , while taking $h(x) = 1$ . But how do I show it is a minimial statistic? I do not have the closed form, and I need to show that: $$\frac{f(x_1, \ldots, x_{n_1} \mid \theta)}{f(y_1,\ldots,y_{n_2} \mid \theta)} = C \iff (X, N_1) = (Y, N_2)$$ How can I show this? Additionally, the solution manual uses the joint of $f_{X, N}(x, n)$ to show the above, which is simple of course. But I think this is a mistake, because the theorem in the book requires to use the joint of the SAMPLE, and not the joint of the statistic. Theorem 6.2.13 page 281 Second Edition. Am I write considering this a mistake in the solution manual ^ PS. Solution manual link, http://www.ams.sunysb.edu/~zhu/ams570/Solutions-Casella-Berger.pdf exercise 6.12 (a). PPS. I think my confusion might come from defining the joint vs marginal distributions of $x_1, \ldots, x_n$ and $N = n$ . How can we define $P(x_1, \ldots, x_n \mid N = n)$ and $P(x_1, \ldots, x_n, n)$ ? If I observe a sample, then $N$ is a function of $x$ 's - I can count them. However, having a joint distribution, and then marginalising $N$ out also seems like a correct answer...","In Casella and Berger, we have the following experiment: assume we first select a positive integer randomly, let us call it , s.t. . Then, for this , we perform a classic Binomial experiment with parameter . The number of positives/ones in the sample is . It is asked to show that is a minimal sufficient statistic. I found the joint distribution of the statistic: and the sample: By factorization theorem, we can show that this is a sufficient statistic. This series must be convergent (marginal), and we can assume that the above is a function of and , while taking . But how do I show it is a minimial statistic? I do not have the closed form, and I need to show that: How can I show this? Additionally, the solution manual uses the joint of to show the above, which is simple of course. But I think this is a mistake, because the theorem in the book requires to use the joint of the SAMPLE, and not the joint of the statistic. Theorem 6.2.13 page 281 Second Edition. Am I write considering this a mistake in the solution manual ^ PS. Solution manual link, http://www.ams.sunysb.edu/~zhu/ams570/Solutions-Casella-Berger.pdf exercise 6.12 (a). PPS. I think my confusion might come from defining the joint vs marginal distributions of and . How can we define and ? If I observe a sample, then is a function of 's - I can count them. However, having a joint distribution, and then marginalising out also seems like a correct answer...","N \in \{1, 2, \ldots \} P(N = n) = p_n N = n \theta X (X, N) f(x, n) = {n\choose x}\theta^x(1 - \theta)^{n - x}p_n f(x_1, \ldots, x_n \mid \theta) = \sum_{n = 1}^\infty p_n \theta^{\sum_{i = 1}^n x_i}(1-\theta)^{n - \sum_{i = 1}^n x_i} X = \sum_{i = 1}^N X_i N h(x) = 1 \frac{f(x_1, \ldots, x_{n_1} \mid \theta)}{f(y_1,\ldots,y_{n_2} \mid \theta)} = C \iff (X, N_1) = (Y, N_2) f_{X, N}(x, n) x_1, \ldots, x_n N = n P(x_1, \ldots, x_n \mid N = n) P(x_1, \ldots, x_n, n) N x N","['statistics', 'statistical-inference']"
84,When is it true that $\sup g - \inf g \le 2\sup g$?,When is it true that ?,\sup g - \inf g \le 2\sup g,"I am currently reading the first version of the paper titled ""On the Margin Theory of Feedforward Neural Networks"" by Colin Wei, Jason D. Lee, Qiang Liu and Tengyu Ma. In Lemma C.4 of the appendix, the authors give upper bounds for the Rademacher complexity of some specific function class, and there is one step in the proof they give that I fail to understand. Page 21, the authors give the following upper bound : $$\begin{align} \mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert - \inf_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert\right] &\le \mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i) - \inf_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right] \\ &\le 2\mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right]\end{align} $$ To provide some context here, $\epsilon_i$ are i.i.d. Rademacher random variables, $\mathbb S^d$ is the $d$ -dimensional unit sphere, $x_i$ are elements of $\mathbb R^d$ and $\phi$ is an $M$ -Lipschitz activation function. However these details don't seem very relevant here since in the first inequality, it seems that they simply used the fact that $\sup |g| - \inf |g| \le \sup g - \inf g$ . The second inequality is quite puzzling to me because it seems to use the fact that $\sup g - \inf g \le 2\sup g$ which is equivalent to $ -\inf g \le \sup g$ and is not true in general (consider for instance $g : x \mapsto -x^2$ ). I am thinking that maybe under some assumption this inequality might be true, but I can't manage to get further than this. I would be grateful for any help in proving why the second inequality holds true.","I am currently reading the first version of the paper titled ""On the Margin Theory of Feedforward Neural Networks"" by Colin Wei, Jason D. Lee, Qiang Liu and Tengyu Ma. In Lemma C.4 of the appendix, the authors give upper bounds for the Rademacher complexity of some specific function class, and there is one step in the proof they give that I fail to understand. Page 21, the authors give the following upper bound : To provide some context here, are i.i.d. Rademacher random variables, is the -dimensional unit sphere, are elements of and is an -Lipschitz activation function. However these details don't seem very relevant here since in the first inequality, it seems that they simply used the fact that . The second inequality is quite puzzling to me because it seems to use the fact that which is equivalent to and is not true in general (consider for instance ). I am thinking that maybe under some assumption this inequality might be true, but I can't manage to get further than this. I would be grateful for any help in proving why the second inequality holds true.","\begin{align}
\mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert - \inf_{u\in\mathbb S^d}\left\lvert\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right\lvert\right] &\le \mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i) - \inf_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right] \\
&\le 2\mathbb E_{\epsilon_i}\left[\sup_{u\in\mathbb S^d}\sum_{i=1}^{n}\epsilon_i \phi(u^T x_i)\right]\end{align}  \epsilon_i \mathbb S^d d x_i \mathbb R^d \phi M \sup |g| - \inf |g| \le \sup g - \inf g \sup g - \inf g \le 2\sup g  -\inf g \le \sup g g : x \mapsto -x^2","['real-analysis', 'statistics', 'inequality', 'random-variables', 'machine-learning']"
85,a best critical region of two parameters in normal distribution,a best critical region of two parameters in normal distribution,,"This question is from Introduction to Mathematical Statistics written by Hogg et al In page 448, 1.6 Let $ X_1, X_2, \ldots, X_{10} $ is a random sample from a distribution that is $N(\theta_1, \theta_2)$ . Find a best test of the simple hypothesis $H_0 : \theta _1= \theta'_1 = 0, \theta_2=\theta'_2=1$ against the alternative simple hypothesis $H_1 : \theta _1= \theta''_1=1, \theta_2=\theta''_2=4 $ I briefly write a best critical $C=\lbrace(x_1,x_2, \ldots, x_{10})| 3\sum_{i=1}^{10}x_i^2 +2\sum_{i=1}^{10}x_i \ge k\rbrace$ But we have to consider a joint distribution of chi square and normal. Is that right? That looks so complicated so I feel unsure of a above expression. Help me please!","This question is from Introduction to Mathematical Statistics written by Hogg et al In page 448, 1.6 Let is a random sample from a distribution that is . Find a best test of the simple hypothesis against the alternative simple hypothesis I briefly write a best critical But we have to consider a joint distribution of chi square and normal. Is that right? That looks so complicated so I feel unsure of a above expression. Help me please!"," X_1, X_2, \ldots, X_{10}  N(\theta_1, \theta_2) H_0 : \theta _1= \theta'_1 = 0, \theta_2=\theta'_2=1 H_1 : \theta _1= \theta''_1=1, \theta_2=\theta''_2=4  C=\lbrace(x_1,x_2, \ldots, x_{10})| 3\sum_{i=1}^{10}x_i^2 +2\sum_{i=1}^{10}x_i \ge k\rbrace",['statistics']
86,A statistic to capture the degree of mean reversion,A statistic to capture the degree of mean reversion,,"Given a realization of a stochastic process, $x_{t_1}, x_{t_2}, \ldots, x_{t_n}$ , is there a simple statistic that captures the degree to which the stochastic process is mean reverting? For example, such a statistic would give a high value for a realization of an Ornstein-Uhlenbeck process, and a low value for Brownian motion or geometric Brownian motion.","Given a realization of a stochastic process, , is there a simple statistic that captures the degree to which the stochastic process is mean reverting? For example, such a statistic would give a high value for a realization of an Ornstein-Uhlenbeck process, and a low value for Brownian motion or geometric Brownian motion.","x_{t_1}, x_{t_2}, \ldots, x_{t_n}","['statistics', 'stochastic-processes', 'time-series', 'stationary-processes']"
87,What does X1 mean in the statistics?,What does X1 mean in the statistics?,,"I'm trying to refresh my knowledge in mathematical statistics since my university days. I'm mostly using Russian resources but assume that it's applicable for others. What isn't really clear for me is the meaning of the $X_1$ . For example, in the book https://mipt.ru/diht/students/courses/mathematical_statistics.pdf we have a definition of unbiased estimator and an example of applying it to a sample mean on 11th page. I will duplicate the example here: $$ E_\theta \bar{X} = \frac{1}{n} \sum{E_\theta X_i} = E_\theta X_1 $$ The last transition isn't obvious to me. Perhaps, $X_1$ means the first element from the sample but I don't get why we chose exactly the first element and not, for instance, the 2nd. Thank you in advance!","I'm trying to refresh my knowledge in mathematical statistics since my university days. I'm mostly using Russian resources but assume that it's applicable for others. What isn't really clear for me is the meaning of the . For example, in the book https://mipt.ru/diht/students/courses/mathematical_statistics.pdf we have a definition of unbiased estimator and an example of applying it to a sample mean on 11th page. I will duplicate the example here: The last transition isn't obvious to me. Perhaps, means the first element from the sample but I don't get why we chose exactly the first element and not, for instance, the 2nd. Thank you in advance!",X_1  E_\theta \bar{X} = \frac{1}{n} \sum{E_\theta X_i} = E_\theta X_1  X_1,"['probability-theory', 'statistics', 'expected-value']"
88,Alternative to the Binomial PMF?,Alternative to the Binomial PMF?,,"Since hypergeometric distribution is the without replacement version of the Binomial distribution why can't we replace the combinations in the Hypergeometric PMF with combinations with replacement and expect the same result with the standard Binomial PMF ? To elaborate this is the regular combination formula counting unordered without replacement : $$C(n,r)= \frac{n!}{r!(n−r)!}$$ And this is the combination with replacement formula counting unordered with replacement: $$C^R(n,r)= \frac{(n+r−1)!}{r!(n−1)!}$$ and this is the hypergeometric distribution PMF: For $X\sim\operatorname{HGeom}(w,b,n)$ $$P(X=k)= \frac{C(w,k)C(b,n-k)}{C(w+b,n)}$$ My question is, since binomial and hypergeometric are both fixed number of trials where Binomial is counting with replacement and Hypergeometric is counting without replacement, why doesn't altering the Hypergeometric formula to be $$P(X=k)= \frac{C^R(w,k)C^R(b,n-k)}{C^R(w+b,n)}$$ give us the PMF of the Binomial Distribution ? What am I missing ?","Since hypergeometric distribution is the without replacement version of the Binomial distribution why can't we replace the combinations in the Hypergeometric PMF with combinations with replacement and expect the same result with the standard Binomial PMF ? To elaborate this is the regular combination formula counting unordered without replacement : And this is the combination with replacement formula counting unordered with replacement: and this is the hypergeometric distribution PMF: For My question is, since binomial and hypergeometric are both fixed number of trials where Binomial is counting with replacement and Hypergeometric is counting without replacement, why doesn't altering the Hypergeometric formula to be give us the PMF of the Binomial Distribution ? What am I missing ?","C(n,r)= \frac{n!}{r!(n−r)!} C^R(n,r)= \frac{(n+r−1)!}{r!(n−1)!} X\sim\operatorname{HGeom}(w,b,n) P(X=k)= \frac{C(w,k)C(b,n-k)}{C(w+b,n)} P(X=k)= \frac{C^R(w,k)C^R(b,n-k)}{C^R(w+b,n)}","['probability', 'combinatorics', 'statistics', 'probability-distributions', 'binomial-distribution']"
89,Determine Maximum-Likelihood-Estimator without knowing the observed value,Determine Maximum-Likelihood-Estimator without knowing the observed value,,"Consider the following distribution with Lebesgue density $$f_{\theta}(x) = \begin{cases} \theta x^{-\theta-1}, \quad \text{if } 1 < x < \infty \\ 0, \quad \text{else} \end{cases},$$ for $\theta \in [a,b]$ with $1 < a < b < \infty$ . Let $r > 1$ be a real number and the observed value $x_1$ be greater than $r$ . However, the value of $x_1$ is unknown. Determine the MLE for $\theta$ . As we don't know the exact value of $x_1$ the classic approach cannot be done, so I determined the probability of a random variable (with the distribution above) being greater than r and that is $r^{-\theta}$ . If we consider this to be the Likelihood function, then the MLE for $\theta$ is going to be the minimum value of the interval for $\theta$ and that means $a$ , as $r^{-\theta}$ is decreasing in $\theta$ . Is this correct or is there another approach which has to be considered?","Consider the following distribution with Lebesgue density for with . Let be a real number and the observed value be greater than . However, the value of is unknown. Determine the MLE for . As we don't know the exact value of the classic approach cannot be done, so I determined the probability of a random variable (with the distribution above) being greater than r and that is . If we consider this to be the Likelihood function, then the MLE for is going to be the minimum value of the interval for and that means , as is decreasing in . Is this correct or is there another approach which has to be considered?","f_{\theta}(x) = \begin{cases} \theta x^{-\theta-1}, \quad \text{if } 1 < x < \infty \\ 0, \quad \text{else} \end{cases}, \theta \in [a,b] 1 < a < b < \infty r > 1 x_1 r x_1 \theta x_1 r^{-\theta} \theta \theta a r^{-\theta} \theta","['statistics', 'maximum-likelihood']"
90,Confidence interval for unknown $\mu$ and $\sigma^2$,Confidence interval for unknown  and,\mu \sigma^2,"I am stuck trying to solve this problem. I can only get to a certain point in my attempts. Any help is greatly appreciated. I am given the following sample of size $10$ drawn from a $\mathcal N(\mu, \sigma^2)$ -distributed population: $$(100.4, 99.2, 98.4, 99.6, 103.6, 101.6, 103.2, 99.2, 97.6, 99.2)$$ Determine the $95 \%$ confidence interval for the unknown expected value $\mu$ and known variance $\sigma^2=4$ Determine the $95 \%$ confidence interval for the unknow expected value $\mu$ and unknown variance $\sigma^2$ Determine the $95\%$ confidence interval for the unkown variance $\sigma^2$ and unknown expected value $\mu$ What I have tried so far (with some resources from the web): $$\alpha=1-0.95=0.05\\ \Phi(c)=\frac{1}{2}\left(1-\frac{\alpha}{2}\right)=0.975$$ The mean of the sample is: $$\bar{X}=\frac{1002}{10}=100.2$$ Then I can use the following formula: $$\bar{X}-c \frac{\sigma}{\sqrt{n}}\le \mu \le \bar{X}+c \frac{\sigma}{\sqrt{n}}$$ I know $\bar{X}=100.2, \sigma=2,n=10$ but how can I find $c$ ? For question two, is it just the exact same calculation as 1. but now using the sample variance? What is the difference to question 2. ? Aren't they the same?","I am stuck trying to solve this problem. I can only get to a certain point in my attempts. Any help is greatly appreciated. I am given the following sample of size drawn from a -distributed population: Determine the confidence interval for the unknown expected value and known variance Determine the confidence interval for the unknow expected value and unknown variance Determine the confidence interval for the unkown variance and unknown expected value What I have tried so far (with some resources from the web): The mean of the sample is: Then I can use the following formula: I know but how can I find ? For question two, is it just the exact same calculation as 1. but now using the sample variance? What is the difference to question 2. ? Aren't they the same?","10 \mathcal N(\mu, \sigma^2) (100.4, 99.2, 98.4, 99.6, 103.6, 101.6, 103.2, 99.2, 97.6, 99.2) 95 \% \mu \sigma^2=4 95 \% \mu \sigma^2 95\% \sigma^2 \mu \alpha=1-0.95=0.05\\ \Phi(c)=\frac{1}{2}\left(1-\frac{\alpha}{2}\right)=0.975 \bar{X}=\frac{1002}{10}=100.2 \bar{X}-c \frac{\sigma}{\sqrt{n}}\le \mu \le \bar{X}+c \frac{\sigma}{\sqrt{n}} \bar{X}=100.2, \sigma=2,n=10 c","['statistics', 'normal-distribution']"
91,Is $\mathrm{Var}(Y)\leq \max \mathrm{Var}(Y|X)$?,Is ?,\mathrm{Var}(Y)\leq \max \mathrm{Var}(Y|X),"Let $E_1,E_2,\cdots, E_n$ be mutually exclusive and disjoint events on a probability space. Let $Y$ be a random variable on the same space. I am trying to check if $\mathrm{Var}(Y)\leq \max_i \mathrm{Var}(Y|E_i)\tag{1}$ By Law of total variance $\mathrm{Var}(Y)=\mathbb{E}[\mathrm{Var}(Y|X)]+\mathrm{Var}(\mathbb{E}[Y|X])$ . So the first term in the total variance formula is less than or equal to $\max_i \mathrm{Var}(Y|E_i)$ but may I know how much can the second term contribute? Is there an example where $(1)$ is false?",Let be mutually exclusive and disjoint events on a probability space. Let be a random variable on the same space. I am trying to check if By Law of total variance . So the first term in the total variance formula is less than or equal to but may I know how much can the second term contribute? Is there an example where is false?,"E_1,E_2,\cdots, E_n Y \mathrm{Var}(Y)\leq \max_i \mathrm{Var}(Y|E_i)\tag{1} \mathrm{Var}(Y)=\mathbb{E}[\mathrm{Var}(Y|X)]+\mathrm{Var}(\mathbb{E}[Y|X]) \max_i \mathrm{Var}(Y|E_i) (1)",['probability']
92,Estimator problem. Can chebyshev's inequality be applied to a sequence?,Estimator problem. Can chebyshev's inequality be applied to a sequence?,,"Let $g: \Theta  \to \mathbb R$ be a function of an unknown parameter $\theta$ . We want to estimate $g$ . Let $\hat g_n$ be a sequence of estimators for $g(\theta)$ such that for all $\theta \in \Theta$ : $$\lim_{n\to\infty} E[\hat g_n]=g(\theta) \hspace{1cm} \text{ and} \hspace{1cm} \lim_{n\to\infty} \text{Var}(\hat g_n)=0$$ Show that the sequence $\hat g_n$ is consistent. My attempt: I was thinking of using chebyshev's inequality here but I was not sure if it can be applied to sequences like this. In order for $\hat g_n$ to be consistent I need to show that: $$\lim_{n \to \infty} \Pr(\vert \hat g_n-g \vert > \varepsilon )=0$$ I am given that $\hat g_n$ has expected value $g$ , therefore I can use chebyshevs inequality: $$ \begin{split}&\phantom{\implies} \, \, \Pr(\vert X-E[X]\vert > \varepsilon)\le\frac{\text{Var}(X)}{\varepsilon^2} \\[10pt] & \implies \Pr(\vert \hat g_n-g\vert > \varepsilon) \le \frac{\text{Var}(\hat g_n)}{\varepsilon^2} \\[10pt] & \iff \lim_{n \to \infty}\Pr(\vert \hat g_n-g\vert > \varepsilon) \le  \lim_{n \to \infty}\ \frac{\text{Var}(\hat g_n)}{\varepsilon^2}=\frac{1}{\varepsilon^2}\lim_{n\to \infty}\text{Var}(\hat g_n)=0\end{split}$$ Is this the correct way to do this or am I ""not allowed"" to use the chebyshev inequality here?","Let be a function of an unknown parameter . We want to estimate . Let be a sequence of estimators for such that for all : Show that the sequence is consistent. My attempt: I was thinking of using chebyshev's inequality here but I was not sure if it can be applied to sequences like this. In order for to be consistent I need to show that: I am given that has expected value , therefore I can use chebyshevs inequality: Is this the correct way to do this or am I ""not allowed"" to use the chebyshev inequality here?","g: \Theta  \to \mathbb R \theta g \hat g_n g(\theta) \theta \in \Theta \lim_{n\to\infty} E[\hat g_n]=g(\theta) \hspace{1cm} \text{ and} \hspace{1cm} \lim_{n\to\infty} \text{Var}(\hat g_n)=0 \hat g_n \hat g_n \lim_{n \to \infty} \Pr(\vert \hat g_n-g \vert > \varepsilon )=0 \hat g_n g  \begin{split}&\phantom{\implies} \, \, \Pr(\vert X-E[X]\vert > \varepsilon)\le\frac{\text{Var}(X)}{\varepsilon^2} \\[10pt] & \implies \Pr(\vert \hat g_n-g\vert > \varepsilon) \le \frac{\text{Var}(\hat g_n)}{\varepsilon^2} \\[10pt] & \iff \lim_{n \to \infty}\Pr(\vert \hat g_n-g\vert > \varepsilon) \le  \lim_{n \to \infty}\ \frac{\text{Var}(\hat g_n)}{\varepsilon^2}=\frac{1}{\varepsilon^2}\lim_{n\to \infty}\text{Var}(\hat g_n)=0\end{split}","['statistics', 'parameter-estimation']"
93,"Introduction to Statistical Learning, Chapter 3,Ex 3.7.7: Proofing equality of $ R^2=Cor^2$ in simple linear regression. Figuring out the algebra","Introduction to Statistical Learning, Chapter 3,Ex 3.7.7: Proofing equality of  in simple linear regression. Figuring out the algebra", R^2=Cor^2,"I am a hobby mathematican without any formal training. Currently I am chewing through 'An Introduction to Statistical Learning', 1st Ed. (abbreviated: ISLR, https://www.statlearning.com/ ). Now I am stuck on an algebraic proof in the linear regression exercise chapter 3, specifically 3.7.7. As this is a somewhat 'uphill' difficulty, I have not made much progress on the desired proof. The excercise states: ""It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$ , the $ R^2 $ statistic (3.17) is equal to the square of the correlation between $X$ and $Y $ (3.18). Prove that this is the case. For simplicity, you may assume that $ \bar{x}=\bar{y}=0 $ "". I have come this far up to now: Excercise 3.7.7: Prove that $ R^2 = Cor^2 $ DEFINITIONS: $ \bar{x} \equiv \frac{1}{n} \sum_{i=1}^n { x_i }, $ $ \bar{y} \equiv \frac{1}{n} \sum_{i=1}^n { y_i }, $ $ \hat{\beta}_1 =   \frac   { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }   { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }, $ $ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}, $ $ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i = \hat{\beta}_0 +   \frac   { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }   { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }   x_i = \bar{y} -   \frac     { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }     { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }   \bar{x} +   \frac     { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }     { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }   x_i, $ $ RSS = \sum_{i=1}^n { (y_i - \hat{y}_i)^2 }, $ $ TSS = \sum_{i=1}^n { (y_i - \bar{y}  )^2 }, $ $ R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS} = 1 - \frac   { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }   { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } }, $ $ Cor = \frac   { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }   {     \sqrt{ \sum_{i=1}^n { (x_i-\bar{x})^2 } }     \sqrt{ \sum_{i=1}^n { (y_i-\bar{y})^2 } }   } = \frac   { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }   {     \sqrt{       \sum_{i=1}^n { (x_i-\bar{x})^2 }       \sum_{i=1}^n { (y_i-\bar{y})^2 }     }   } $ $ Cor^2 = \left(   \frac     { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }     {       \sqrt{         \sum_{i=1}^n { (x_i-\bar{x})^2 }         \sum_{i=1}^n { (y_i-\bar{y})^2 }       }     } \right)^2 = \frac   { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }   {     \left( \sqrt{       \sum_{i=1}^n { (x_i-\bar{x})^2 }       \sum_{i=1}^n { (y_i-\bar{y})^2 }     } \right)^2   } = \frac   { \left( \sum_{i=1}^n { (x_i-\bar{x}) (y_i-\bar{y}) } \right)^2 }   {     \sum_{i=1}^n { (x_i-\bar{x})^2 }     \sum_{i=1}^n { (y_i-\bar{y})^2 }   } $ TO PROVE: $ R^2 = Cor^2 $ , with $ \bar{x} = \bar{y} = 0 $ . Initial expansion: $ \hat{y}_i = \bar{y} -   \frac     { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }     { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }   \bar{x} +   \frac     { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }     { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }   x_i = 0 -   \frac     { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }     { \sum_{i=1}^n{ (x_i - 0)^2     } }   0 +   \frac     { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }     { \sum_{i=1}^n{ (x_i - 0)^2     } }   x_i = \frac   { \sum_{i=1}^n{ x_i y_i   } }   { \sum_{i=1}^n{ x_i^2 } } x_i $ $ 1 - \frac   { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }   { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } } = \frac   { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }   {     \sum_{i=1}^n { (x_i-\bar{x})^2 }     \sum_{i=1}^n { (y_i-\bar{y})^2 }   }, $ $ 1 - \frac   { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }   { \sum_{i=1}^n { (y_i - 0         )^2 } } = \frac   { \left( \sum_{i=1}^n { (x_i-0)(y_i-0) } \right)^2 }   {     \sum_{i=1}^n { (x_i-0)^2 }     \sum_{i=1}^n { (y_i-0)^2 }   }, $ $ 1 - \frac   { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }   { \sum_{i=1}^n { y_i^2 } } = \frac   { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }   {     \sum_{i=1}^n { x_i^2 }     \sum_{i=1}^n { y_i^2 }   }, $ $ \boxed{ 1 - \frac   { \sum_{i=1}^n {     \left(       y_i -       \frac         { \sum_{i=1}^n{ x_i y_i } }         { \sum_{i=1}^n{ x_i^2   } }       x_i     \right)^2 }   }   { \sum_{i=1}^n { y_i^2 } } = \frac   { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }   {     \sum_{i=1}^n { x_i^2 }     \sum_{i=1}^n { y_i^2 }   } } $ Ok, so now I want to prove the equality of the boxed formula. And that is were I start to stumble. I manage to make a couple of transformations, however I can only achieve a common denominator between the two. The top part of the vulgar fraction escapes me. Here is what I have tried: RHS equals to: $ \frac   { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }   { \sum_{i=1}^n { x_i^2 } \sum_{i=1}^n { y_i^2 } } $ Transformations for LHS: T0: $ 1 - \frac   { \sum_{i=1}^n {     \left(       y_i -       \frac         { \sum_{i=1}^n{ x_i y_i } }         { \sum_{i=1}^n{ x_i^2   } }       x_i     \right)^2 }   }   { \sum_{i=1}^n { y_i^2 } } $ T1: $ \frac   {     \left( \sum_{i=1}^n { y_i^2 } \right)     -     \sum_{i=1}^n {       \left(         y_i -         \frac           { \sum_{i=1}^n{ x_i y_i } }           { \sum_{i=1}^n{ x_i^2   } }         x_i       \right)^2     }   }   { \sum_{i=1}^n { y_i^2 } } $ T2: $ \frac   {     \left( \sum_{i=1}^n { y_i^2 } \right)     -     \sum_{i=1}^n {       \left(         y_i -         \frac           { x_i \sum_{i=1}^n{ x_i y_i } }           {     \sum_{i=1}^n{ x_i^2   } }       \right)^2     }   }   { \sum_{i=1}^n { y_i^2 } } $ T3: $ \frac   {     \left( \sum_{i=1}^n { y_i^2 } \right)     -     \sum_{i=1}^n {       \left(         \frac           {             \left( y_i \sum_{i=1}^n{ x_i^2   } \right)             -             \left( x_i \sum_{i=1}^n{ x_i y_i } \right)           }           { \sum_{i=1}^n{ x_i^2 } }       \right)^2     }   }   { \sum_{i=1}^n { y_i^2 } } $ T4: $ \frac   {     \sum_{i=1}^n { x_i^2 }     \left(       \left( \sum_{i=1}^n { y_i^2 } \right)       -       \sum_{i=1}^n {         \left(           \frac             {               \left( y_i \sum_{i=1}^n{ x_i^2   } \right)               -               \left( x_i \sum_{i=1}^n{ x_i y_i } \right)             }             { \sum_{i=1}^n{ x_i^2 } }         \right)^2       }     \right)   }   {     \sum_{i=1}^n { x_i^2 }     \sum_{i=1}^n { y_i^2 }   } $ T5: $ \frac   {     \sum_{i=1}^n { x_i^2 }     \left(       \left( \sum_{i=1}^n { y_i^2 } \right)       -       \sum_{i=1}^n {         \left(           \left(             \left( y_i \sum_{i=1}^n{ x_i^2   } \right)             -             \left( x_i \sum_{i=1}^n{ x_i y_i } \right)           \right)           \frac             { 1 }             { \sum_{i=1}^n{ x_i^2 } }         \right)^2       }     \right)   }   {     \sum_{i=1}^n { x_i^2 }     \sum_{i=1}^n { y_i^2 }   } $ ... and here I am running massively out of ideas. I can see the term $ \left( \sum_{i=1}^n { x_i y_i } \right)^2 $ in the top, but no idea how to isolate it and remove the rest. Does anyone have any pointers to get me back on the right road? -terminal","I am a hobby mathematican without any formal training. Currently I am chewing through 'An Introduction to Statistical Learning', 1st Ed. (abbreviated: ISLR, https://www.statlearning.com/ ). Now I am stuck on an algebraic proof in the linear regression exercise chapter 3, specifically 3.7.7. As this is a somewhat 'uphill' difficulty, I have not made much progress on the desired proof. The excercise states: ""It is claimed in the text that in the case of simple linear regression of onto , the statistic (3.17) is equal to the square of the correlation between and (3.18). Prove that this is the case. For simplicity, you may assume that "". I have come this far up to now: Excercise 3.7.7: Prove that DEFINITIONS: TO PROVE: , with . Initial expansion: Ok, so now I want to prove the equality of the boxed formula. And that is were I start to stumble. I manage to make a couple of transformations, however I can only achieve a common denominator between the two. The top part of the vulgar fraction escapes me. Here is what I have tried: RHS equals to: Transformations for LHS: T0: T1: T2: T3: T4: T5: ... and here I am running massively out of ideas. I can see the term in the top, but no idea how to isolate it and remove the rest. Does anyone have any pointers to get me back on the right road? -terminal","Y X  R^2  X Y   \bar{x}=\bar{y}=0   R^2 = Cor^2   \bar{x} \equiv \frac{1}{n} \sum_{i=1}^n { x_i },   \bar{y} \equiv \frac{1}{n} \sum_{i=1}^n { y_i },  
\hat{\beta}_1 =
  \frac
  { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
  { \sum_{i=1}^n{ (x_i - \bar{x})^2               } },
  \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},  
\hat{y}_i
=
\hat{\beta}_0 + \hat{\beta}_1 x_i
=
\hat{\beta}_0 +
  \frac
  { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
  { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  x_i
=
\bar{y} -
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  \bar{x} +
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2               } }
  x_i,
  RSS = \sum_{i=1}^n { (y_i - \hat{y}_i)^2 },   TSS = \sum_{i=1}^n { (y_i - \bar{y}  )^2 },  
R^2
=
\frac{TSS - RSS}{TSS}
=
1 - \frac{RSS}{TSS}
=
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } },
 
Cor
=
\frac
  { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
  {
    \sqrt{ \sum_{i=1}^n { (x_i-\bar{x})^2 } }
    \sqrt{ \sum_{i=1}^n { (y_i-\bar{y})^2 } }
  }
=
\frac
  { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
  {
    \sqrt{
      \sum_{i=1}^n { (x_i-\bar{x})^2 }
      \sum_{i=1}^n { (y_i-\bar{y})^2 }
    }
  }
 
Cor^2
=
\left(
  \frac
    { \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } }
    {
      \sqrt{
        \sum_{i=1}^n { (x_i-\bar{x})^2 }
        \sum_{i=1}^n { (y_i-\bar{y})^2 }
      }
    }
\right)^2
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }
  {
    \left( \sqrt{
      \sum_{i=1}^n { (x_i-\bar{x})^2 }
      \sum_{i=1}^n { (y_i-\bar{y})^2 }
    } \right)^2
  }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x}) (y_i-\bar{y}) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-\bar{x})^2 }
    \sum_{i=1}^n { (y_i-\bar{y})^2 }
  }
  R^2 = Cor^2   \bar{x} = \bar{y} = 0  
\hat{y}_i
=
\bar{y} -
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }
  \bar{x} +
  \frac
    { \sum_{i=1}^n{ (x_i - \bar{x}) (y_i - \bar{y}) } }
    { \sum_{i=1}^n{ (x_i - \bar{x})^2            } }
  x_i
=
0 -
  \frac
    { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }
    { \sum_{i=1}^n{ (x_i - 0)^2     } }
  0 +
  \frac
    { \sum_{i=1}^n{ (x_i - 0) (y_i - 0) } }
    { \sum_{i=1}^n{ (x_i - 0)^2     } }
  x_i
=
\frac
  { \sum_{i=1}^n{ x_i y_i   } }
  { \sum_{i=1}^n{ x_i^2 } }
x_i
 
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - \bar{y}  )^2 } }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-\bar{x})(y_i-\bar{y}) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-\bar{x})^2 }
    \sum_{i=1}^n { (y_i-\bar{y})^2 }
  },
 
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { (y_i - 0         )^2 } }
=
\frac
  { \left( \sum_{i=1}^n { (x_i-0)(y_i-0) } \right)^2 }
  {
    \sum_{i=1}^n { (x_i-0)^2 }
    \sum_{i=1}^n { (y_i-0)^2 }
  },
 
1 - \frac
  { \sum_{i=1}^n { (y_i - \hat{y}_i)^2 } }
  { \sum_{i=1}^n { y_i^2 } }
=
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  },
 
\boxed{
1 - \frac
  { \sum_{i=1}^n {
    \left(
      y_i -
      \frac
        { \sum_{i=1}^n{ x_i y_i } }
        { \sum_{i=1}^n{ x_i^2   } }
      x_i
    \right)^2 }
  }
  { \sum_{i=1}^n { y_i^2 } }
=
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
}
 
\frac
  { \left( \sum_{i=1}^n { x_i y_i } \right)^2 }
  { \sum_{i=1}^n { x_i^2 } \sum_{i=1}^n { y_i^2 } }
 
1 - \frac
  { \sum_{i=1}^n {
    \left(
      y_i -
      \frac
        { \sum_{i=1}^n{ x_i y_i } }
        { \sum_{i=1}^n{ x_i^2   } }
      x_i
    \right)^2 }
  }
  { \sum_{i=1}^n { y_i^2 } }
 
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        y_i -
        \frac
          { \sum_{i=1}^n{ x_i y_i } }
          { \sum_{i=1}^n{ x_i^2   } }
        x_i
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
 
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        y_i -
        \frac
          { x_i \sum_{i=1}^n{ x_i y_i } }
          {     \sum_{i=1}^n{ x_i^2   } }
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
 
\frac
  {
    \left( \sum_{i=1}^n { y_i^2 } \right)
    -
    \sum_{i=1}^n {
      \left(
        \frac
          {
            \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
            -
            \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
          }
          { \sum_{i=1}^n{ x_i^2 } }
      \right)^2
    }
  }
  { \sum_{i=1}^n { y_i^2 } }
 
\frac
  {
    \sum_{i=1}^n { x_i^2 }
    \left(
      \left( \sum_{i=1}^n { y_i^2 } \right)
      -
      \sum_{i=1}^n {
        \left(
          \frac
            {
              \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
              -
              \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
            }
            { \sum_{i=1}^n{ x_i^2 } }
        \right)^2
      }
    \right)
  }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
 
\frac
  {
    \sum_{i=1}^n { x_i^2 }
    \left(
      \left( \sum_{i=1}^n { y_i^2 } \right)
      -
      \sum_{i=1}^n {
        \left(
          \left(
            \left( y_i \sum_{i=1}^n{ x_i^2   } \right)
            -
            \left( x_i \sum_{i=1}^n{ x_i y_i } \right)
          \right)
          \frac
            { 1 }
            { \sum_{i=1}^n{ x_i^2 } }
        \right)^2
      }
    \right)
  }
  {
    \sum_{i=1}^n { x_i^2 }
    \sum_{i=1}^n { y_i^2 }
  }
  \left( \sum_{i=1}^n { x_i y_i } \right)^2 ","['statistics', 'linear-regression']"
94,Mean of the sum of two independent geometric distributions with different probabilities of success,Mean of the sum of two independent geometric distributions with different probabilities of success,,"Let $X \sim Geom(p_1), Y \sim Geom(p_2)$ be two independent geometrically distributed random variables with probabilities of success $p_1, p_2$ respectively. I want to find the mean of the sum of these distributions; $X+Y$ . I know that the mean of a geometric distribution is $\frac{1}{p}$ , where $p$ is the probability of success. In this answer: Sum of two Geometric Random Variables with different success probability , I see that: $$\mathbf{P}(X+Y = n) = \frac{p_1p_2}{p_1 - p_2} \big( (1-p_2)^{n-1} - (1-p_1)^{n-1} \big), \qquad n = 2,3,\ldots  $$ How do I then find the mean of this distribution? How does the mean of this distribution relate to the sum of the means of the independent distributions? ( $\frac{1}{p_1} + \frac{1}{p_2}$ ) Intuitively, I'm not sure whether we'd expect the mean of the sum of the independent distributions to be roughly the same as the sum of the means of the independent distributions?","Let be two independent geometrically distributed random variables with probabilities of success respectively. I want to find the mean of the sum of these distributions; . I know that the mean of a geometric distribution is , where is the probability of success. In this answer: Sum of two Geometric Random Variables with different success probability , I see that: How do I then find the mean of this distribution? How does the mean of this distribution relate to the sum of the means of the independent distributions? ( ) Intuitively, I'm not sure whether we'd expect the mean of the sum of the independent distributions to be roughly the same as the sum of the means of the independent distributions?","X \sim Geom(p_1), Y \sim Geom(p_2) p_1, p_2 X+Y \frac{1}{p} p \mathbf{P}(X+Y = n) = \frac{p_1p_2}{p_1 - p_2}
\big( (1-p_2)^{n-1} - (1-p_1)^{n-1} \big), \qquad n = 2,3,\ldots 
 \frac{1}{p_1} + \frac{1}{p_2}","['probability', 'statistics', 'probability-distributions', 'binomial-distribution', 'negative-binomial']"
95,What is the correct format for the formula of linear regression?,What is the correct format for the formula of linear regression?,,"Lets say you have $i$ independent variables, ( $y_1$ , $y_2$ , ... $y_i$ ), and each of them have the SAME two predictors, $x_1$ and $x_2$ . I thought that the formula for the linear regression model for each $y$ would be: $$ y_i = \beta_0 + \beta_{i,1}x_1 + \beta_{i,2}x_2 + \epsilon_i $$ But based on Wikipedia, the formula looks like it would be: $$ y_i = \beta_0 + \beta_1x_{i,1} + \beta_{2}x_{i,2} + \epsilon_i$$ Here is a picture from another site: This formula suggests there are $n*p$ predictor variables with $p$ unique ones for each response variable. Why is my formula incorrect? Shouldn't the coefficients change depending on which response variable I am trying to model?","Lets say you have independent variables, ( , , ... ), and each of them have the SAME two predictors, and . I thought that the formula for the linear regression model for each would be: But based on Wikipedia, the formula looks like it would be: Here is a picture from another site: This formula suggests there are predictor variables with unique ones for each response variable. Why is my formula incorrect? Shouldn't the coefficients change depending on which response variable I am trying to model?","i y_1 y_2 y_i x_1 x_2 y 
y_i = \beta_0 + \beta_{i,1}x_1 + \beta_{i,2}x_2 + \epsilon_i
  y_i = \beta_0 + \beta_1x_{i,1} + \beta_{2}x_{i,2} + \epsilon_i n*p p","['statistics', 'regression', 'linear-regression']"
96,Intuition with multivariate Gaussian distribution conditional probability,Intuition with multivariate Gaussian distribution conditional probability,,"I have a problem with the intuition of the conditional probability. Suppose we have a multivariate normal distribution (bivariate for simplicity) with mean $\mu$ and covariance matrix $\Sigma$ with the following form. I undertand that the intuitive idea of conditional probability is to fix one of the dimensions to certain value doing what would be a ""slice"" to the multivariate Gaussian distribution, something like this . Whose result is another Gaussian distribution with one dimension less, mean $\mu'$ and covariance matrix $\Sigma'$ . Seeing that figure, intuition tells me that the conditioned probability function should vary depending on the fixed point, that is to say, higher and peaked when the fixed point is near the mean $\mu$ and flatter at the extremes. But what I find is that only the mean is affected, the variance is constant regardless of the point. See this and this . How can this be possible, is there something I am misunderstanding? Thank you in advance!","I have a problem with the intuition of the conditional probability. Suppose we have a multivariate normal distribution (bivariate for simplicity) with mean and covariance matrix with the following form. I undertand that the intuitive idea of conditional probability is to fix one of the dimensions to certain value doing what would be a ""slice"" to the multivariate Gaussian distribution, something like this . Whose result is another Gaussian distribution with one dimension less, mean and covariance matrix . Seeing that figure, intuition tells me that the conditioned probability function should vary depending on the fixed point, that is to say, higher and peaked when the fixed point is near the mean and flatter at the extremes. But what I find is that only the mean is affected, the variance is constant regardless of the point. See this and this . How can this be possible, is there something I am misunderstanding? Thank you in advance!",\mu \Sigma \mu' \Sigma' \mu,"['probability', 'statistics', 'normal-distribution', 'conditional-probability', 'gaussian']"
97,Let's find solutions to two functional differential equations,Let's find solutions to two functional differential equations,,"In order to prove the following Probability result: ""Find a family of random variables $X$ , having pdf $f$ , such that $X$ and $Y=f(X)$ have the same distribution."" Let's call $F(x)$ the CDF of the random variable $X$ and $f(x)$ its PDF. Furthermore, the random variable $Y$ has CDF $K(y)$ and PDF $k(y)$ . I know that $y$ belongs to a set of values in which it can be considered valid the relation $y=f(x)$ . I'd like to ""investigate"" when the random variables $X$ and $Y$ have the same distribution, in the case of $f$ stictly increasing for $x\leq m$ and stricly decreasing for $x \geq m$ (where $m$ is the mode of the distribution). So we have to prove when $F(y)=K(y)$ . From a previous valid result, I know that, under these assumptions, the CDF of the random variable $Y$ is: $K(y)=2 F(f^{-1}(y))=2F(x)$ or, equivalently, $K(y)=2(1-F(f^{-1}(y))=2-2F(x)$ . In order to find the explicit expression of the PDF $f$ , using the relation $F(y)=K(y)$ (our thesis), I should prove that: in the case $f$ strictly increasing for $x \leq m$ , $$K(y)=2F(x)=F(y) \Leftrightarrow 2F(x)=F(f(x)) \Leftrightarrow 2F(x)=F(F'(x)).$$ So the first functional differential equation that should be solved is: $$2F(x)=F(F'(x))$$ in order to find the expression of $F$ and then obtain $f$ , derivating $F$ . in the case of $f$ strictly decreasing for $x \geq m$ $K(y)=2-F(x)=F(y) \Leftrightarrow 2-F(x)=F(f(x)) \Leftrightarrow 2-F(x)=F(F'(x))$ . The second functional differential equation to be solved is the following one: $$F(F'(x))+F(x)=2.$$ The solutions of the aforesaid equations should be in a certain way ""symmetrical"". I should solve those functional differential equations, but I have NO IDEA on HOW to solve them because I've never met this kind of equations before in my career. Could you help me, please?","In order to prove the following Probability result: ""Find a family of random variables , having pdf , such that and have the same distribution."" Let's call the CDF of the random variable and its PDF. Furthermore, the random variable has CDF and PDF . I know that belongs to a set of values in which it can be considered valid the relation . I'd like to ""investigate"" when the random variables and have the same distribution, in the case of stictly increasing for and stricly decreasing for (where is the mode of the distribution). So we have to prove when . From a previous valid result, I know that, under these assumptions, the CDF of the random variable is: or, equivalently, . In order to find the explicit expression of the PDF , using the relation (our thesis), I should prove that: in the case strictly increasing for , So the first functional differential equation that should be solved is: in order to find the expression of and then obtain , derivating . in the case of strictly decreasing for . The second functional differential equation to be solved is the following one: The solutions of the aforesaid equations should be in a certain way ""symmetrical"". I should solve those functional differential equations, but I have NO IDEA on HOW to solve them because I've never met this kind of equations before in my career. Could you help me, please?",X f X Y=f(X) F(x) X f(x) Y K(y) k(y) y y=f(x) X Y f x\leq m x \geq m m F(y)=K(y) Y K(y)=2 F(f^{-1}(y))=2F(x) K(y)=2(1-F(f^{-1}(y))=2-2F(x) f F(y)=K(y) f x \leq m K(y)=2F(x)=F(y) \Leftrightarrow 2F(x)=F(f(x)) \Leftrightarrow 2F(x)=F(F'(x)). 2F(x)=F(F'(x)) F f F f x \geq m K(y)=2-F(x)=F(y) \Leftrightarrow 2-F(x)=F(f(x)) \Leftrightarrow 2-F(x)=F(F'(x)) F(F'(x))+F(x)=2.,"['probability', 'functional-analysis', 'statistics', 'functional-equations', 'density-function']"
98,Central limit theorem problem verification,Central limit theorem problem verification,,"Attempt: a. $\mu=(1/6)(1)+(1/6)(2)+(1/6)(3)+(1/6)(4)+(1/6)(5)+(1/6)(6)=3.5$ $\sigma=\sqrt{EX^2-(EX)^2}=\sqrt{2.9167}$ b. $\mu=(1/216)((1)(3)+(3)(4)+(6)(5)+(10)(6)+(15)(7)+(21)(8)+(25)(9)+(27)(10)+(27)(11)+(25)(12)+(21)(13)+(15)(14)+(10)(15)+(6)(16)+(3)(17)+(1)(18))=10.5$ $\sigma=\sqrt{EX^2-(EX)^2}=\sqrt{8.75}$ c.By the law of large numbers, $\mu_{\overline{X}}=10.5,\sigma_{\overline{X}}=(\sqrt{8.75})/(\sqrt{25})$ d. $P\{\overline{X}>10\}=P\{Z>-.845\}=.8$ e.In this case $P\{\overline{X}>10\}=P\{Z>-1.195\}=.88$ so the value increases. This is not a graded assignment, but an exercise I am practicing.","Attempt: a. b. c.By the law of large numbers, d. e.In this case so the value increases. This is not a graded assignment, but an exercise I am practicing.","\mu=(1/6)(1)+(1/6)(2)+(1/6)(3)+(1/6)(4)+(1/6)(5)+(1/6)(6)=3.5 \sigma=\sqrt{EX^2-(EX)^2}=\sqrt{2.9167} \mu=(1/216)((1)(3)+(3)(4)+(6)(5)+(10)(6)+(15)(7)+(21)(8)+(25)(9)+(27)(10)+(27)(11)+(25)(12)+(21)(13)+(15)(14)+(10)(15)+(6)(16)+(3)(17)+(1)(18))=10.5 \sigma=\sqrt{EX^2-(EX)^2}=\sqrt{8.75} \mu_{\overline{X}}=10.5,\sigma_{\overline{X}}=(\sqrt{8.75})/(\sqrt{25}) P\{\overline{X}>10\}=P\{Z>-.845\}=.8 P\{\overline{X}>10\}=P\{Z>-1.195\}=.88","['statistics', 'solution-verification']"
99,Solve for withdrawal rate in Monte Carlo simulation of retirement,Solve for withdrawal rate in Monte Carlo simulation of retirement,,"I've been working with compound returns and distribution of wealth over time for quite some time now and I feel like I am hitting a wall. What am I trying to achieve? Imagine that you are about to retire and have saved $100 000. You are planning to withdraw your money monthly over the next 20 years. If we assume that returns are normally distributed with: $${\mu} = 7 \%$$ $${\sigma} = 15 \%$$ I can simulate $1000$ returns from the above distribution in month $1$ . Subtract $X$ from my initial pot, realise a return and calculate the new value of the pot at the end of that month. This gives me $1000$ values at the end of period 1 and I can take the median of all these. Next month the process is repeated. I simulate 1000 returns for month 2 now, subtract the same amount $X$ from the opening balance (end of period 1 values) and multiply by the returns. This gives me yet another 1000 values at the end of period 2 and I can take the median of those. I keep doing this for all the subsequent months and this gives me the path of all median values over $20$ years with withdrawals. There has to exist a way to calculate the amount X (which is constant in each period) that allows me to exhaust my retirement pot (terminal value of 0) at the end of the withdrawal period. From Farago, Adam and Hjalmarsson, Erik, Compound Returns we know that compound returns have the following properties: $${\psi} = \log\left(\frac{\mu^2}{\sqrt{\sigma^2+\mu^2}}\right)%$$ and $${\eta} = \sqrt{\log\left(\frac{\sigma^2}{\mu^2}+1\right)} %$$ and we can find the $\alpha$ -quantile of compound returns with: $$q_\alpha\left(X_T\right) = e^{T\psi+\sqrt{T}\eta\Phi^{-1}\left(\alpha\right)}$$ i.e. I can use this formula to get the 25th percentile of compound returns from the above distribution in period T = 10 Unfortunately, this formula does not hold when you introduce either monthly savings or monthly withdrawals to the picture, as the distribution of compound wealth is then changed. I'd appreciate if anyone can give me any pointers on solving for withdrawal amount X as per the above.","I've been working with compound returns and distribution of wealth over time for quite some time now and I feel like I am hitting a wall. What am I trying to achieve? Imagine that you are about to retire and have saved $100 000. You are planning to withdraw your money monthly over the next 20 years. If we assume that returns are normally distributed with: I can simulate returns from the above distribution in month . Subtract from my initial pot, realise a return and calculate the new value of the pot at the end of that month. This gives me values at the end of period 1 and I can take the median of all these. Next month the process is repeated. I simulate 1000 returns for month 2 now, subtract the same amount from the opening balance (end of period 1 values) and multiply by the returns. This gives me yet another 1000 values at the end of period 2 and I can take the median of those. I keep doing this for all the subsequent months and this gives me the path of all median values over years with withdrawals. There has to exist a way to calculate the amount X (which is constant in each period) that allows me to exhaust my retirement pot (terminal value of 0) at the end of the withdrawal period. From Farago, Adam and Hjalmarsson, Erik, Compound Returns we know that compound returns have the following properties: and and we can find the -quantile of compound returns with: i.e. I can use this formula to get the 25th percentile of compound returns from the above distribution in period T = 10 Unfortunately, this formula does not hold when you introduce either monthly savings or monthly withdrawals to the picture, as the distribution of compound wealth is then changed. I'd appreciate if anyone can give me any pointers on solving for withdrawal amount X as per the above.",{\mu} = 7 \% {\sigma} = 15 \% 1000 1 X 1000 X 20 {\psi} = \log\left(\frac{\mu^2}{\sqrt{\sigma^2+\mu^2}}\right)% {\eta} = \sqrt{\log\left(\frac{\sigma^2}{\mu^2}+1\right)} % \alpha q_\alpha\left(X_T\right) = e^{T\psi+\sqrt{T}\eta\Phi^{-1}\left(\alpha\right)},"['statistics', 'normal-distribution', 'monte-carlo', 'simulation']"
