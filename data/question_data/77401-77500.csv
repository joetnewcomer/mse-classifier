,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is the determinant of a rotation matrix equal to 1?,Why is the determinant of a rotation matrix equal to 1?,,Why is the determinant of a rotation matrix equal to 1? I would like a geometric interpretation of this. Just curious.,Why is the determinant of a rotation matrix equal to 1? I would like a geometric interpretation of this. Just curious.,,[]
1,Represent a complex-valued matrix into real-valued matrix,Represent a complex-valued matrix into real-valued matrix,,If I have a complex matrix ${\bf W} \in {\Bbb C}^{M\times N}$ . Why can this matrix be written as follow? $$ {\bf W} = \begin{bmatrix} {\bf W}_r & -{\bf W}_i \\ {\bf W}_i & {\bf W}_r \end{bmatrix} \in {\Bbb R}^{2M\times 2N} $$ I appreciate your answers!,If I have a complex matrix . Why can this matrix be written as follow? I appreciate your answers!,{\bf W} \in {\Bbb C}^{M\times N}  {\bf W} = \begin{bmatrix} {\bf W}_r & -{\bf W}_i \\ {\bf W}_i & {\bf W}_r \end{bmatrix} \in {\Bbb R}^{2M\times 2N} ,"['matrices', 'complex-numbers', 'block-matrices']"
2,"What is a ""secular equation""?","What is a ""secular equation""?",,"Golub mentioned ""secular equation"" in his Matrix Computation and a slide . However I still don't get its definition. How is a secular equation defined? Thanks!","Golub mentioned ""secular equation"" in his Matrix Computation and a slide . However I still don't get its definition. How is a secular equation defined? Thanks!",,"['matrices', 'terminology']"
3,"To invert a Matrix, Condition number should be less than what?","To invert a Matrix, Condition number should be less than what?",,"I see that there is a matlab tag in this site, so I ask my question here and not in stackoverflow although it is also related to programming in matlab. I am going to invert a positive definite matrix in matlab and I should consider multicolinearity too. I read this in Wikipedia 's Multicollinearity : If the Condition Number is above 30, the regression is said to have significant multicollinearity. and something similar in Greene's Econometrics book (condition number must be less than 20). But there are some links that says different, like PlanetMath's Matrix Condition Number : Matrices with condition numbers much greater than one (such as around $10^5$ for a $5 \times 5$ Hilbert matrix) are said to be ill-conditioned. or Wikipedia's Condition_number : As a general rule of thumb, if the condition number $\kappa(A) = 10^k$ , then you may lose up to $k$ digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods Which one is correct? (both? I mean something is different and I do not get it?) Update I used the answer and the comment to update my question: Consider $\mathbf{X}$ to be the matrix of observations. Ordinary Least square estimates vector is $\mathbf{b=(X'X)}^{-1}\mathbf{X'y}$ . If $\mathbf{X'X}$ is non-singular, we can not calculate this vector. The matrix $\mathbf{X'X}$ is non-singular if 2 columns of $\mathbf{X}$ are linearly dependant. It is sometimes called Perfect Multicollinearity. I think this discussion is used to conclude Multicolinearity and $\mathbf{X'X}$ inversion and as a result the condition number of $\mathbf{X'X}$ are related. It means sometimes we can invert $\mathbf{X'X}$ with acceptable precision, but it does not mean that two columns of $\mathbf{X}$ are linearly dependant. Is the last paragraph correct? Thanks.","I see that there is a matlab tag in this site, so I ask my question here and not in stackoverflow although it is also related to programming in matlab. I am going to invert a positive definite matrix in matlab and I should consider multicolinearity too. I read this in Wikipedia 's Multicollinearity : If the Condition Number is above 30, the regression is said to have significant multicollinearity. and something similar in Greene's Econometrics book (condition number must be less than 20). But there are some links that says different, like PlanetMath's Matrix Condition Number : Matrices with condition numbers much greater than one (such as around for a Hilbert matrix) are said to be ill-conditioned. or Wikipedia's Condition_number : As a general rule of thumb, if the condition number , then you may lose up to digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods Which one is correct? (both? I mean something is different and I do not get it?) Update I used the answer and the comment to update my question: Consider to be the matrix of observations. Ordinary Least square estimates vector is . If is non-singular, we can not calculate this vector. The matrix is non-singular if 2 columns of are linearly dependant. It is sometimes called Perfect Multicollinearity. I think this discussion is used to conclude Multicolinearity and inversion and as a result the condition number of are related. It means sometimes we can invert with acceptable precision, but it does not mean that two columns of are linearly dependant. Is the last paragraph correct? Thanks.",10^5 5 \times 5 \kappa(A) = 10^k k \mathbf{X} \mathbf{b=(X'X)}^{-1}\mathbf{X'y} \mathbf{X'X} \mathbf{X'X} \mathbf{X} \mathbf{X'X} \mathbf{X'X} \mathbf{X'X} \mathbf{X},"['matrices', 'matlab', 'inverse']"
4,Algebraic proof of a trig matrix identity?,Algebraic proof of a trig matrix identity?,,"I'll put the question first, and then the background, because I'm not sure that the background is necessary to answer the question: I have a geometric proof, but is there an elegant algebraic proof that $$ \left[ \begin{matrix}-1 & 1 + \cos\frac{\pi}{2n} \\ -2 & 1 + 2\cos\frac{\pi}{2n}\end{matrix} \right] ^n \left[ \begin{matrix}0 \\ 1\end{matrix} \right] = \left[ \begin{matrix}\cot \frac{\pi}{4n} \\ \cot \frac{\pi}{4n}\end{matrix} \right]$$ Background: This is motivated by the latest maths puzzle from Spanish newspaper El País (problem statement there in Spanish, obviously). NB the deadline for submitting solutions to win the prize has passed, so don't worry about cheating. The relevant part of the problem is this: We have two straight lines (in Euclidean geometry) and we wish to draw a zigzag between them. We start at the intersection point and draw a straight line segment of length $r$ along one of the lines (which we shall call the ""horizontal line""). We then draw a straight line segment of length $r$ from the end-point to the other (""non-horizontal"") line. We alternate between the two with straight line segments of the same length, without overlapping or doubling back. The 20th such line segment is perpendicular to the horizontal line. What is the angle between the lines? There's a simple geometric solution (which I shan't state here, in case anyone wants to solve it himself - although I expect that an answer may include spoilers, so if you do want to solve it yourself, look away now), but before finding it I went down an algebraic approach which led me to an equation involving a matrix similar to the one above raised to the 10th power. Basically the equation above is the generalisation to the case with $2n$ line segments and the answer (derived from the geometric proof) substituted for the unknown angle. The question is motivated by little more than curiosity, because I already have a proof. I've tried to prove it myself, but the best I've got so far is a messy expression in terms of some matrices whose powers do have a relatively nice closed form. Filling in some gaps: if the lines intersect at the origin, the horizontal line is along the x-axis, the non-horizontal line is in the first quadrant, and the angle between the lines is $\alpha$, then the points are $P_{2n} = (x_{2n}, x_{2n} \tan \alpha)$ and $P_{2n+1} = (x_{2n+1}, 0)$. Given that the distance between each pair of consecutive points is $r$ we find that $x_{2n}$ and $x_{2n+2}$ are the two roots of $$(z - x_{2n+1})^2 + z^2 \tan^2 \alpha = r^2$$, and we can use the properties of the quadratic equation to get $$x_{2n+2} = \frac{2}{1 + \tan^2 \alpha}x_{2n+1} - x_{2n}$$ By using the symmetry of the triangle formed by $P_{2n+1}$, $P_{2n+2}$, $P_{2n+3}$, we get $$x_{2n+3} = 2x_{2n+2} - x_{2n+1}$$ Putting this all together we can get a recurrence which leads to an expression for $(x_{2n}, x_{2n+1})$ in terms of the nth power of a matrix applied to $(x_{0} = 0, x_{1} = 1)$. Putting any more details risks spoiling the value of $\alpha$. The RHS of the original equation comes from observing that if the $(2n)^{th}$ line segment is perpendicular to the horizontal line, $x_{2n-1} = x_{2n} = x_{2n+1}$ and $\tan \alpha = r / x_{2n}$. The furthest I've got so far with the matrix is to split it out as $$\left(  \cos\frac{\pi}{2n} \left[ \begin{matrix}0 & 1 \\ 0 & 2\end{matrix} \right] + \left[ \begin{matrix}-1 & 1 \\ -2 & 1\end{matrix} \right] \right)^n = \sum_{k=0}^{n} \left( \begin{matrix}n \\ k\end{matrix} \right) \cos^k\frac{\pi}{2n} \left[ \begin{matrix}0 & 1 \\ 0 & 2\end{matrix} \right]^k \left[ \begin{matrix}-1 & 1 \\ -2 & 1\end{matrix} \right]^{n-k} $$ where both of the matrix powers on the RHS have closed forms - the markup here doesn't seem to like the URLs and I've taken too long to write this up and need to run, but  http://www.wolframalpha.com/input/?i=[[0,1],[0,2]]^k and http://www.wolframalpha.com/input/?i=[[-1,1],[-2,1]]^(r-k)","I'll put the question first, and then the background, because I'm not sure that the background is necessary to answer the question: I have a geometric proof, but is there an elegant algebraic proof that $$ \left[ \begin{matrix}-1 & 1 + \cos\frac{\pi}{2n} \\ -2 & 1 + 2\cos\frac{\pi}{2n}\end{matrix} \right] ^n \left[ \begin{matrix}0 \\ 1\end{matrix} \right] = \left[ \begin{matrix}\cot \frac{\pi}{4n} \\ \cot \frac{\pi}{4n}\end{matrix} \right]$$ Background: This is motivated by the latest maths puzzle from Spanish newspaper El País (problem statement there in Spanish, obviously). NB the deadline for submitting solutions to win the prize has passed, so don't worry about cheating. The relevant part of the problem is this: We have two straight lines (in Euclidean geometry) and we wish to draw a zigzag between them. We start at the intersection point and draw a straight line segment of length $r$ along one of the lines (which we shall call the ""horizontal line""). We then draw a straight line segment of length $r$ from the end-point to the other (""non-horizontal"") line. We alternate between the two with straight line segments of the same length, without overlapping or doubling back. The 20th such line segment is perpendicular to the horizontal line. What is the angle between the lines? There's a simple geometric solution (which I shan't state here, in case anyone wants to solve it himself - although I expect that an answer may include spoilers, so if you do want to solve it yourself, look away now), but before finding it I went down an algebraic approach which led me to an equation involving a matrix similar to the one above raised to the 10th power. Basically the equation above is the generalisation to the case with $2n$ line segments and the answer (derived from the geometric proof) substituted for the unknown angle. The question is motivated by little more than curiosity, because I already have a proof. I've tried to prove it myself, but the best I've got so far is a messy expression in terms of some matrices whose powers do have a relatively nice closed form. Filling in some gaps: if the lines intersect at the origin, the horizontal line is along the x-axis, the non-horizontal line is in the first quadrant, and the angle between the lines is $\alpha$, then the points are $P_{2n} = (x_{2n}, x_{2n} \tan \alpha)$ and $P_{2n+1} = (x_{2n+1}, 0)$. Given that the distance between each pair of consecutive points is $r$ we find that $x_{2n}$ and $x_{2n+2}$ are the two roots of $$(z - x_{2n+1})^2 + z^2 \tan^2 \alpha = r^2$$, and we can use the properties of the quadratic equation to get $$x_{2n+2} = \frac{2}{1 + \tan^2 \alpha}x_{2n+1} - x_{2n}$$ By using the symmetry of the triangle formed by $P_{2n+1}$, $P_{2n+2}$, $P_{2n+3}$, we get $$x_{2n+3} = 2x_{2n+2} - x_{2n+1}$$ Putting this all together we can get a recurrence which leads to an expression for $(x_{2n}, x_{2n+1})$ in terms of the nth power of a matrix applied to $(x_{0} = 0, x_{1} = 1)$. Putting any more details risks spoiling the value of $\alpha$. The RHS of the original equation comes from observing that if the $(2n)^{th}$ line segment is perpendicular to the horizontal line, $x_{2n-1} = x_{2n} = x_{2n+1}$ and $\tan \alpha = r / x_{2n}$. The furthest I've got so far with the matrix is to split it out as $$\left(  \cos\frac{\pi}{2n} \left[ \begin{matrix}0 & 1 \\ 0 & 2\end{matrix} \right] + \left[ \begin{matrix}-1 & 1 \\ -2 & 1\end{matrix} \right] \right)^n = \sum_{k=0}^{n} \left( \begin{matrix}n \\ k\end{matrix} \right) \cos^k\frac{\pi}{2n} \left[ \begin{matrix}0 & 1 \\ 0 & 2\end{matrix} \right]^k \left[ \begin{matrix}-1 & 1 \\ -2 & 1\end{matrix} \right]^{n-k} $$ where both of the matrix powers on the RHS have closed forms - the markup here doesn't seem to like the URLs and I've taken too long to write this up and need to run, but  http://www.wolframalpha.com/input/?i=[[0,1],[0,2]]^k and http://www.wolframalpha.com/input/?i=[[-1,1],[-2,1]]^(r-k)",,"['matrices', 'trigonometry', 'alternative-proof']"
5,Commuting in matrix exponential [duplicate],Commuting in matrix exponential [duplicate],,"This question already has answers here : Operator Exponential $e^A e^B = e^{A+B}$ (2 answers) Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Let $A$ , and $B$ be commuting $n\times n$ matrices, i.e., $A B = B A$ . Let $$ \exp(A) := \sum_{i=0}^\infty\frac{1}{i!} A^i $$ Show that $\exp(A+B) = \exp(A) \exp(B)$ .","This question already has answers here : Operator Exponential $e^A e^B = e^{A+B}$ (2 answers) Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Let , and be commuting matrices, i.e., . Let Show that .",A B n\times n A B = B A  \exp(A) := \sum_{i=0}^\infty\frac{1}{i!} A^i  \exp(A+B) = \exp(A) \exp(B),"['matrices', 'analysis', 'exponential-function', 'matrix-exponential']"
6,How to set up Lagrangian for matrix constraints?,How to set up Lagrangian for matrix constraints?,,"Suppose we have a function $f: \mathbb{R} \to  \mathbb{R} $ which we want to optimize subject to some constraint $g(x) \le c$ , where $g:\mathbb{R} \to  \mathbb{R}$ . What we do is that we can set up a Lagrangian $$ L(x)=f(x)+\lambda(g(x)-c) $$ and optimize. Now suppose we have a function $f: \mathbb{R}^n \to  \mathbb{R} $ subject to $g(X) \le K $ but now $g:\mathbb{R}^n \to  \mathbb{R}^n $ . For example, $g$ can be $XX^T \preceq K$ , where $K$ is some matrix.  How to set up Lagrangian for this case?","Suppose we have a function which we want to optimize subject to some constraint , where . What we do is that we can set up a Lagrangian and optimize. Now suppose we have a function subject to but now . For example, can be , where is some matrix.  How to set up Lagrangian for this case?",f: \mathbb{R} \to  \mathbb{R}  g(x) \le c g:\mathbb{R} \to  \mathbb{R}  L(x)=f(x)+\lambda(g(x)-c)  f: \mathbb{R}^n \to  \mathbb{R}  g(X) \le K  g:\mathbb{R}^n \to  \mathbb{R}^n  g XX^T \preceq K K,"['matrices', 'optimization', 'convex-optimization', 'lagrange-multiplier', 'constraints']"
7,"Why is the largest element of symmetric, positive semidefinite matrix on the diagonal?","Why is the largest element of symmetric, positive semidefinite matrix on the diagonal?",,"I know the very well know equivalence of the properties of a positive, semidefinite matrix: $A$ is positive semidefinite, $A = U^T U$ for some matrix $U$, $\mathbf{x}^T A \mathbf{x}\geq 0$ for every $\mathbf{x} \in \mathbb{R}^n$, All principal minors $A$ are nonnegative. But how can you derive from this that the largest entry of the matrix $A$ appears on the diagonal and why - when a diagonal entry is equal to zero - are all the entries of the corresponding row and column also equal to zero?","I know the very well know equivalence of the properties of a positive, semidefinite matrix: $A$ is positive semidefinite, $A = U^T U$ for some matrix $U$, $\mathbf{x}^T A \mathbf{x}\geq 0$ for every $\mathbf{x} \in \mathbb{R}^n$, All principal minors $A$ are nonnegative. But how can you derive from this that the largest entry of the matrix $A$ appears on the diagonal and why - when a diagonal entry is equal to zero - are all the entries of the corresponding row and column also equal to zero?",,"['matrices', 'matrix-decomposition', 'semidefinite-programming']"
8,When eigenvectors for a matrix form a basis,When eigenvectors for a matrix form a basis,,"It is well known that if n by n matrix A has n distinct eigenvalues, the eigenvectors form a basis. Also, if A is symmetric, the same result holds. Consider $ A =\left[ {\begin{array}{ccc}    1 & 2 & 3 \\    0 & 1 & 2 \\    0 & 0 & 1 \\ \end{array}}\right] $ . This matrix has single eigenvalue $\lambda=1$, and is not symmetric. But, the eigenvectors corresponding to $\lambda=1$, ($ v_1 =\left[ {\begin{array}{c}    1  \\    0  \\    0  \\ \end{array}}\right] $ ,  $ v_2 =\left[ {\begin{array}{c}    0  \\    1/2  \\    0  \\ \end{array}}\right] $ , $ v_3 =\left[ {\begin{array}{c}    0  \\    -3/8  \\    1/4  \\ \end{array}}\right] $ ) form a basis. What sufficient conditions offer the above result?","It is well known that if n by n matrix A has n distinct eigenvalues, the eigenvectors form a basis. Also, if A is symmetric, the same result holds. Consider $ A =\left[ {\begin{array}{ccc}    1 & 2 & 3 \\    0 & 1 & 2 \\    0 & 0 & 1 \\ \end{array}}\right] $ . This matrix has single eigenvalue $\lambda=1$, and is not symmetric. But, the eigenvectors corresponding to $\lambda=1$, ($ v_1 =\left[ {\begin{array}{c}    1  \\    0  \\    0  \\ \end{array}}\right] $ ,  $ v_2 =\left[ {\begin{array}{c}    0  \\    1/2  \\    0  \\ \end{array}}\right] $ , $ v_3 =\left[ {\begin{array}{c}    0  \\    -3/8  \\    1/4  \\ \end{array}}\right] $ ) form a basis. What sufficient conditions offer the above result?",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-calculus', 'matrix-decomposition', 'eigenfunctions']"
9,Find diagonal of inverse matrix,Find diagonal of inverse matrix,,"I have computed the Cholesky of a positive semidefinite matrix $\Theta$ . However, I wish to know the diagonal elements of the inverse of $\Theta^{-1}_{ii}$ . Is it possible to do this using the Cholesky that I have computed? Or will finding the eigenvalues alone (without the orthonormal matrices of a SVD) help this cause? Are there any other suggestions or alternative decompositions that will aid finding the inverse matrix diagonal? I've seen that random projections does wonders for inverting matrices. Could something like this be applied here?","I have computed the Cholesky of a positive semidefinite matrix . However, I wish to know the diagonal elements of the inverse of . Is it possible to do this using the Cholesky that I have computed? Or will finding the eigenvalues alone (without the orthonormal matrices of a SVD) help this cause? Are there any other suggestions or alternative decompositions that will aid finding the inverse matrix diagonal? I've seen that random projections does wonders for inverting matrices. Could something like this be applied here?",\Theta \Theta^{-1}_{ii},"['matrices', 'projective-space', 'random-matrices', 'matrix-decomposition']"
10,Deriving the sub-differential of the nuclear norm,Deriving the sub-differential of the nuclear norm,,"Let $$f(K) = \| K \|_*$$ be the nuclear norm (sum of the singular values) of $K=U\Sigma V^T$ . How can one compute the subdifferential $\partial f$ ? This may be a basic question, I'm trying to work my way through a paper in which minimizing $f$ over a convex set of matrices plays a central role. For what it's worth, I have found papers that display the end result, but not the derivation. EDIT: This paper by Tao and Candes derives an expression, but refers the proof to ""Characterization of the subdifferential of some matrix norms"" which does not prove it as far as I can tell. I also found a class homework assignment posted online that said this was easy to ""grind out"" with matrix derivatives, but that there was another way via projections. Any guidance would be greatly appreciated.","Let be the nuclear norm (sum of the singular values) of . How can one compute the subdifferential ? This may be a basic question, I'm trying to work my way through a paper in which minimizing over a convex set of matrices plays a central role. For what it's worth, I have found papers that display the end result, but not the derivation. EDIT: This paper by Tao and Candes derives an expression, but refers the proof to ""Characterization of the subdifferential of some matrix norms"" which does not prove it as far as I can tell. I also found a class homework assignment posted online that said this was easy to ""grind out"" with matrix derivatives, but that there was another way via projections. Any guidance would be greatly appreciated.",f(K) = \| K \|_* K=U\Sigma V^T \partial f f,"['matrices', 'convex-optimization', 'matrix-norms', 'nuclear-norm', 'non-smooth-analysis']"
11,Gradient of $A \mapsto \operatorname{trace} (A B A' C)$,Gradient of,A \mapsto \operatorname{trace} (A B A' C),"Given three matrices $A$ , $B$ and $C$ such that $ABA^T C$ is a square matrix, the derivative of the trace with respect to $A$ is: $$ \nabla_A \operatorname{trace}( ABA^{T}C )  = CAB + C^T AB^T $$ There is a proof here, page 4 (PDF file) . However, I was not able to understand the second line of the proof. it says something like this: $$ \nabla_A \operatorname{trace}( ABA^T C )= \nabla_\bullet \operatorname{trace}(f(\bullet)A^{T}C) + \nabla_\bullet \operatorname{trace}(f(A) \bullet^T C) $$ Where $f(A) = AB$ I tried searching the proof or any hint using google, but I couldn't fine anything. Could anyone please help me with this second line. I cannot sleep. I am obsessed with this proof.","Given three matrices , and such that is a square matrix, the derivative of the trace with respect to is: There is a proof here, page 4 (PDF file) . However, I was not able to understand the second line of the proof. it says something like this: Where I tried searching the proof or any hint using google, but I couldn't fine anything. Could anyone please help me with this second line. I cannot sleep. I am obsessed with this proof.","A B C ABA^T C A 
\nabla_A \operatorname{trace}( ABA^{T}C )  = CAB + C^T AB^T
 
\nabla_A \operatorname{trace}( ABA^T C )=
\nabla_\bullet \operatorname{trace}(f(\bullet)A^{T}C) + \nabla_\bullet \operatorname{trace}(f(A) \bullet^T C)
 f(A) = AB","['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
12,Compactness of the set of $n \times n$ orthogonal matrices,Compactness of the set of  orthogonal matrices,n \times n,Show that the set of all orthogonal matrices in the set of all $n \times n$ matrices  endowed with any norm topology is compact.,Show that the set of all orthogonal matrices in the set of all $n \times n$ matrices  endowed with any norm topology is compact.,,['matrices']
13,The Matrix in ZFC: A Set-Theoretic Foundation of Matrices?,The Matrix in ZFC: A Set-Theoretic Foundation of Matrices?,,"Today, we can use ZFC to found all of mathematics. We show from ten or so axioms the existence of everything from ordered pairs and Cartesian products to relations and functions (whose existence follows from the existence of Cartesian products). I have learned, from reading books like Enderton's 1977 Elements of Set Theory , that it is not enough simply to define something (for example, we define the ordered pair of $a$ and $b$ as the set $(a,b)=\{\{a\},\{a,b\}\}$), but we must also prove that what we have defined exists within ZFC. I have done some searching, but have not found a set-theoretic construction of matrices. Wikipedia's Matrix page states the definition of an $m \times n$ matrix as simply a ""rectangular array"", but this definition does not show that a matrix is a set which actually exists in ZFC. Does anyone know how to construct matrices in ZFC? Please feel free to point me in the direction of books in set theory which tackle this. Thanks for reading my question!","Today, we can use ZFC to found all of mathematics. We show from ten or so axioms the existence of everything from ordered pairs and Cartesian products to relations and functions (whose existence follows from the existence of Cartesian products). I have learned, from reading books like Enderton's 1977 Elements of Set Theory , that it is not enough simply to define something (for example, we define the ordered pair of $a$ and $b$ as the set $(a,b)=\{\{a\},\{a,b\}\}$), but we must also prove that what we have defined exists within ZFC. I have done some searching, but have not found a set-theoretic construction of matrices. Wikipedia's Matrix page states the definition of an $m \times n$ matrix as simply a ""rectangular array"", but this definition does not show that a matrix is a set which actually exists in ZFC. Does anyone know how to construct matrices in ZFC? Please feel free to point me in the direction of books in set theory which tackle this. Thanks for reading my question!",,"['matrices', 'elementary-set-theory']"
14,Do the non-zero eigenvalues of AB and BA have the same algebraic multiplicity (for AB and BA not square)?,Do the non-zero eigenvalues of AB and BA have the same algebraic multiplicity (for AB and BA not square)?,,"I know that if A and B are square nxn matrices, then AB and BA have the same characteristic polynomial and thus the same eigenvalues (and same algebraïc multiplicity). I'm wondering though if this can be generalized: if A is a nxm matrix and B a mxn matrix, then AB is a nxn matrix and BA a mxm matrix.  So my question is: will the eigenvalues of AB and BA, that differ from zero, have the same algebraïc multiplicity?","I know that if A and B are square nxn matrices, then AB and BA have the same characteristic polynomial and thus the same eigenvalues (and same algebraïc multiplicity). I'm wondering though if this can be generalized: if A is a nxm matrix and B a mxn matrix, then AB is a nxn matrix and BA a mxm matrix.  So my question is: will the eigenvalues of AB and BA, that differ from zero, have the same algebraïc multiplicity?",,"['matrices', 'eigenvalues-eigenvectors']"
15,What are advantages of quaternion over $3\times3$ rotator matrix for representing arbitrary rotation?,What are advantages of quaternion over  rotator matrix for representing arbitrary rotation?,3\times3,"I have lots of experience computationally representing generalized 3D coordinate rotations using $3\times3$ matrix rotators, R = [ newXaxis, newYaxis, newZaxis ].  These are very well behaved and easy-intutive to think and program with.  They seem very accurate and stable.  For continuous arbitrary rotations, the ""in-between"" values (between I and R) can be computed by teasing out R's Euler axes (unit-length eigenvectors) and corresponding Euler angle.  You can then take the nth root of a rotator R to get the intermediate rotators forming a continuous rotational arc. I've heard that quaternions can also be used to represent generalized 3D rotations (computationally).  What are the advantages of doing so over the $3\times3$ rotator matrix?  The information is compressed into 4 real numbers compared to 9, so would the information overcompression make quaternion-based algorithms trickier?","I have lots of experience computationally representing generalized 3D coordinate rotations using $3\times3$ matrix rotators, R = [ newXaxis, newYaxis, newZaxis ].  These are very well behaved and easy-intutive to think and program with.  They seem very accurate and stable.  For continuous arbitrary rotations, the ""in-between"" values (between I and R) can be computed by teasing out R's Euler axes (unit-length eigenvectors) and corresponding Euler angle.  You can then take the nth root of a rotator R to get the intermediate rotators forming a continuous rotational arc. I've heard that quaternions can also be used to represent generalized 3D rotations (computationally).  What are the advantages of doing so over the $3\times3$ rotator matrix?  The information is compressed into 4 real numbers compared to 9, so would the information overcompression make quaternion-based algorithms trickier?",,['matrices']
16,What is the maximum possible value of determinant of a matrix whose entries either 0 or 1?,What is the maximum possible value of determinant of a matrix whose entries either 0 or 1?,,My question is simply the title: What is the maximum possible value of determinant of a matrix whose entries either 0 or 1 ?,My question is simply the title: What is the maximum possible value of determinant of a matrix whose entries either 0 or 1 ?,,"['matrices', 'optimization', 'determinant']"
17,How to show determinant of a specific matrix is nonnegative,How to show determinant of a specific matrix is nonnegative,,"How to show that $$\det A= \det \begin{pmatrix}\cos\frac{\pi}{n}&-\frac{\cos\theta_1}{2}&0&0&\cdots&0&-\frac{\cos\theta_n}{2} \\-\frac{\cos\theta_1}{2}&\cos\frac{\pi}{n}&-\frac{\cos\theta_2}{2}&0&0&\cdots&0\\ 0&-\frac{\cos\theta_2}{2}&\cos\frac{\pi}{n}&-\frac{\cos\theta_3}{2}&0&\cdots&0\\ \vdots&\vdots&-\frac{\cos\theta_3}{2}&\ddots&\ddots&\vdots&\vdots\\\vdots&\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\ \vdots&\vdots&\vdots&\vdots&\ddots&\ddots&-\frac{\cos\theta_{n-1}}{2}\\ -\frac{\cos\theta_n}{2}&0&\cdots&0&0&-\frac{\cos\theta_{n-1}}{2}&\cos\frac{\pi}{n} \end{pmatrix} \geq 0, $$   where $\sum\limits_{i=1}^n\theta_i=\pi$, $0<\theta_i<\frac{\pi}{2}, i=1,\dots,n$, and $n\ge 3$?","How to show that $$\det A= \det \begin{pmatrix}\cos\frac{\pi}{n}&-\frac{\cos\theta_1}{2}&0&0&\cdots&0&-\frac{\cos\theta_n}{2} \\-\frac{\cos\theta_1}{2}&\cos\frac{\pi}{n}&-\frac{\cos\theta_2}{2}&0&0&\cdots&0\\ 0&-\frac{\cos\theta_2}{2}&\cos\frac{\pi}{n}&-\frac{\cos\theta_3}{2}&0&\cdots&0\\ \vdots&\vdots&-\frac{\cos\theta_3}{2}&\ddots&\ddots&\vdots&\vdots\\\vdots&\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\ \vdots&\vdots&\vdots&\vdots&\ddots&\ddots&-\frac{\cos\theta_{n-1}}{2}\\ -\frac{\cos\theta_n}{2}&0&\cdots&0&0&-\frac{\cos\theta_{n-1}}{2}&\cos\frac{\pi}{n} \end{pmatrix} \geq 0, $$   where $\sum\limits_{i=1}^n\theta_i=\pi$, $0<\theta_i<\frac{\pi}{2}, i=1,\dots,n$, and $n\ge 3$?",,['matrices']
18,Checking if a matrix is positive semidefinite,Checking if a matrix is positive semidefinite,,"Determine whether the following $2 \times 2$ matrix is positive semidefinite (PSD) $$\begin{bmatrix}\frac{2}{x} & \frac{-2y}{x^2} \\\frac{-2y}{x^2} & \frac{2y^2}{x^3}\end{bmatrix}$$ where $x > 0$ and $y \in \mathbb R$ . A matrix is PSD if $v^T A v \geq 0$ . So, do I just multiply by a vector $v = (v_1, v_2)$ and check if it is $\geq 0$ ? Thanks for any help.","Determine whether the following matrix is positive semidefinite (PSD) where and . A matrix is PSD if . So, do I just multiply by a vector and check if it is ? Thanks for any help.","2 \times 2 \begin{bmatrix}\frac{2}{x} & \frac{-2y}{x^2} \\\frac{-2y}{x^2} & \frac{2y^2}{x^3}\end{bmatrix} x > 0 y \in \mathbb R v^T A v \geq 0 v = (v_1, v_2) \geq 0","['matrices', 'positive-semidefinite']"
19,Is every real monic polynomial the characteristic polynomial of a real matrix?,Is every real monic polynomial the characteristic polynomial of a real matrix?,,"Is every monic real polynomial the characteristic polynomial of some real square matrix? I strongly suspect so, but I couldn't find a proof.","Is every monic real polynomial the characteristic polynomial of some real square matrix? I strongly suspect so, but I couldn't find a proof.",,"['matrices', 'polynomials']"
20,Geometric intuition of graph Laplacian matrices,Geometric intuition of graph Laplacian matrices,,"I am reading about Laplacian matrices for the first time and struggling to gain intuition as to why they are so useful. Could anyone provide insight as to the geometric significance of the Laplacian of a graph? For example, why are the eigenvectors of a Laplacian matrix helpful in interpreting the corresponding graph?","I am reading about Laplacian matrices for the first time and struggling to gain intuition as to why they are so useful. Could anyone provide insight as to the geometric significance of the Laplacian of a graph? For example, why are the eigenvectors of a Laplacian matrix helpful in interpreting the corresponding graph?",,"['matrices', 'graph-theory', 'algebraic-graph-theory', 'spectral-graph-theory', 'graph-laplacian']"
21,Are there any decompositions of a symmetric matrix that allow for the inversion of any submatrix?,Are there any decompositions of a symmetric matrix that allow for the inversion of any submatrix?,,"I am given a $J \times J$ symmetric matrix $\Sigma$. For a subset of $\{1, ..., J\}$,  $v$, let $\Sigma_{v}$ denote the associated square submatrix. I am in need of an efficient scheme for inverting $\Sigma_v$ for potentially any subset $v$. Essentially, I will have to invert many different submatricies $\Sigma_v$, but I won't know which ones I need to invert in advance of running some program; I would rather invest in a good matrix decomposition at the outset, if one exists (or otherwise get whatever information necessary, if not a decomposition). I've messed around with the eigen decomposition a little bit, but wasn't able to coerce the inverse of a submatrix out of it. UPDATE: Apparently the term for the type of submatricies I want to invert is ""principal submatrix"". I'm wondering if I can't make some progress on this via the Cholesky decomposition. Suppose I'm content to calculate $\Sigma_{jj} ^ {-1}$ for any $j \in \{1, 2, ..., J\}$, where $\Sigma_{jj}$ denotes the submatrix obtained by deleting rows/columns greater than $j$. Write $\Sigma = LL^T$ and let $Q = L^{-1}$. Write  $$ L = \begin{pmatrix}L_1 & \mathbf 0 \\ B & L_2\end{pmatrix},  Q = \begin{pmatrix}Q_1 & \mathbf 0 \\ C & Q_2\end{pmatrix}  $$ where $L_1$ and $Q_1$ and $j \times j$. It follows that $\Sigma_{jj} = L_1 L_1^T$ and $Q_1 = L_1 ^{-1}$ so that $\Sigma_{jj} ^{-1} = Q_1^T Q_1$. So, once I have the Cholesky decomposition I have the inverses of the leading principal submatricies. This doesn't solve the problem as stated since I may need to deal with other principal submatricies, but it should be a useful partial solution.","I am given a $J \times J$ symmetric matrix $\Sigma$. For a subset of $\{1, ..., J\}$,  $v$, let $\Sigma_{v}$ denote the associated square submatrix. I am in need of an efficient scheme for inverting $\Sigma_v$ for potentially any subset $v$. Essentially, I will have to invert many different submatricies $\Sigma_v$, but I won't know which ones I need to invert in advance of running some program; I would rather invest in a good matrix decomposition at the outset, if one exists (or otherwise get whatever information necessary, if not a decomposition). I've messed around with the eigen decomposition a little bit, but wasn't able to coerce the inverse of a submatrix out of it. UPDATE: Apparently the term for the type of submatricies I want to invert is ""principal submatrix"". I'm wondering if I can't make some progress on this via the Cholesky decomposition. Suppose I'm content to calculate $\Sigma_{jj} ^ {-1}$ for any $j \in \{1, 2, ..., J\}$, where $\Sigma_{jj}$ denotes the submatrix obtained by deleting rows/columns greater than $j$. Write $\Sigma = LL^T$ and let $Q = L^{-1}$. Write  $$ L = \begin{pmatrix}L_1 & \mathbf 0 \\ B & L_2\end{pmatrix},  Q = \begin{pmatrix}Q_1 & \mathbf 0 \\ C & Q_2\end{pmatrix}  $$ where $L_1$ and $Q_1$ and $j \times j$. It follows that $\Sigma_{jj} = L_1 L_1^T$ and $Q_1 = L_1 ^{-1}$ so that $\Sigma_{jj} ^{-1} = Q_1^T Q_1$. So, once I have the Cholesky decomposition I have the inverses of the leading principal submatricies. This doesn't solve the problem as stated since I may need to deal with other principal submatricies, but it should be a useful partial solution.",,"['matrices', 'numerical-linear-algebra', 'block-matrices']"
22,what is the advantage of LU factorization,what is the advantage of LU factorization,,"In this question Necessity/Advantage of LU Decomposition over Gaussian Elimination it is asked why LU factorization is useful. I understand how this reduces time complexity of solving a number equations of the form Ax=b for matrix A and column matrix b but why don't you just find A -1 instead? Inversion has a lower time complexity than LU factorization (comparing the value used in the previous link and ones found here https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations ) and matrix multiplication has the same time complexity in this case as is needed to solve for different values of b. Overall, I see the value of LU factorization as opposed to resolving multiple matrix equations but I don't know why it would be better than the method I described that uses matrix inversion. Clearly LU factorization has some value, I would like to know what that it. Thanks I believe the answer to this question is that all square matrices have a P T LU factorization while not all square matrices are invertible. Therefore P T LU factorization is more versatile. Any other insights are still appreciated however so please comment or answer the question. Thanks","In this question Necessity/Advantage of LU Decomposition over Gaussian Elimination it is asked why LU factorization is useful. I understand how this reduces time complexity of solving a number equations of the form Ax=b for matrix A and column matrix b but why don't you just find A -1 instead? Inversion has a lower time complexity than LU factorization (comparing the value used in the previous link and ones found here https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations ) and matrix multiplication has the same time complexity in this case as is needed to solve for different values of b. Overall, I see the value of LU factorization as opposed to resolving multiple matrix equations but I don't know why it would be better than the method I described that uses matrix inversion. Clearly LU factorization has some value, I would like to know what that it. Thanks I believe the answer to this question is that all square matrices have a P T LU factorization while not all square matrices are invertible. Therefore P T LU factorization is more versatile. Any other insights are still appreciated however so please comment or answer the question. Thanks",,['matrices']
23,Eigenvalues of Kronecker Product,Eigenvalues of Kronecker Product,,"Maybe it's simple but I can't see the solution of this problem (Russell Merris, Multilinear Algebra , CRC Press, 1997, chapter 6, p.202, exercise 4): Let $\lambda_1,\ldots,\lambda_p$ be the eigenvalues of $A\in\mathbb C_{p,p}$ (multiplicities included), and $\omega_1,\ldots,\omega_q$ be the eigenvalues of $B\in\mathbb C_{q,q}$ respectively. Find the eigenvalues of a. $A \otimes B - B \otimes A$ . b. $A \otimes B + B \otimes A$ . From chapter 5, I know the eigenvalues of $A\otimes B$ and $A\otimes I_q + I_p \otimes B$ : The eigenvalues of $A \otimes B$ are $\lambda_i \cdot \omega_j$ , $1 \leq i \leq p$ , $1 \leq j \leq q$ The eigenvalues of $A\otimes I_q + I_p \otimes B$ are $\lambda_i + \omega_j$ , $1 \leq i \leq p$ , $1 \leq j \leq q$ These facts may give us a decomposition of $A\otimes B \pm B \otimes A$ . This may be very simple but I need a hint. I also made some Matlab calculations with integer matrices, and I get non-integer/non-real eigenvalues... maybe square roots are involved... Thanks!","Maybe it's simple but I can't see the solution of this problem (Russell Merris, Multilinear Algebra , CRC Press, 1997, chapter 6, p.202, exercise 4): Let be the eigenvalues of (multiplicities included), and be the eigenvalues of respectively. Find the eigenvalues of a. . b. . From chapter 5, I know the eigenvalues of and : The eigenvalues of are , , The eigenvalues of are , , These facts may give us a decomposition of . This may be very simple but I need a hint. I also made some Matlab calculations with integer matrices, and I get non-integer/non-real eigenvalues... maybe square roots are involved... Thanks!","\lambda_1,\ldots,\lambda_p A\in\mathbb C_{p,p} \omega_1,\ldots,\omega_q B\in\mathbb C_{q,q} A \otimes B - B \otimes A A \otimes B + B \otimes A A\otimes B A\otimes I_q + I_p \otimes B A \otimes B \lambda_i \cdot \omega_j 1 \leq i \leq p 1 \leq j \leq q A\otimes I_q + I_p \otimes B \lambda_i + \omega_j 1 \leq i \leq p 1 \leq j \leq q A\otimes B \pm B \otimes A","['matrices', 'eigenvalues-eigenvectors', 'tensor-products', 'multilinear-algebra', 'kronecker-product']"
24,Is $\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n$ injective?,Is  injective?,\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n,"More specific to my problem, this is a variation on Is $\exp:\mathbb{M_n}\to\mathbb{M_n}$ injective? which was promptly answered with a counterexample. Let $\mathbb{M}_n$ be the space of $n\times n$ matrices with real entries. Let $\overline{\mathbb{M}}_n$ be the space of square $n\times n$ matrices with entries $0$ and $1$. For any $M\in\overline{\mathbb{M}}_n$ we have $$e^M=\exp(M)=\sum_{k=0}^\infty \frac{1}{k!}M^k.$$ Is $\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n$ injective? In other words, are there two distinct $M_0,M_1\in \overline{\mathbb{M}}_n$ such that $e^{M_0}=e^{M_1}$? Since for fixed $n$ the space $\overline{\mathbb{M}}_n$ is finite we can manually check -- it is injective for $n\le4$, but I haven't been able to convince myself for arbitrary $n$. For $n=4$ there are $2^{n^2}=65536$ matrices to check, and MATLAB's expm and my hastily written matrix building code took half an hour to complete.  For $n=5$ we have $2^{25}>3.3\cdot 10^7$ matrices to test which would take ten days to run.  For larger $n$ a direct test is out of the question. The smallest nontrivial examples in my problem are $n=6$, and the interesting ones are $n=12$ and larger.","More specific to my problem, this is a variation on Is $\exp:\mathbb{M_n}\to\mathbb{M_n}$ injective? which was promptly answered with a counterexample. Let $\mathbb{M}_n$ be the space of $n\times n$ matrices with real entries. Let $\overline{\mathbb{M}}_n$ be the space of square $n\times n$ matrices with entries $0$ and $1$. For any $M\in\overline{\mathbb{M}}_n$ we have $$e^M=\exp(M)=\sum_{k=0}^\infty \frac{1}{k!}M^k.$$ Is $\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n$ injective? In other words, are there two distinct $M_0,M_1\in \overline{\mathbb{M}}_n$ such that $e^{M_0}=e^{M_1}$? Since for fixed $n$ the space $\overline{\mathbb{M}}_n$ is finite we can manually check -- it is injective for $n\le4$, but I haven't been able to convince myself for arbitrary $n$. For $n=4$ there are $2^{n^2}=65536$ matrices to check, and MATLAB's expm and my hastily written matrix building code took half an hour to complete.  For $n=5$ we have $2^{25}>3.3\cdot 10^7$ matrices to test which would take ten days to run.  For larger $n$ a direct test is out of the question. The smallest nontrivial examples in my problem are $n=6$, and the interesting ones are $n=12$ and larger.",,"['matrices', 'exponentiation', 'exponential-function', 'matrix-equations']"
25,Chain rule and matrices - I'm confused,Chain rule and matrices - I'm confused,,"I googled around and searched inside the forum but I'm still confused about a problem. I have 2 matrix functions $f,g : \mathbb{R}^{n \times n} \times \mathbb{R}^{a \times b} \rightarrow \mathbb{R}^{n \times n}$. Starting from this, I have the following expression: $$ t(Q, X, Y) = \text{tr}(f(g(Q, X),Y))$$ where $\text{tr}$ is the trace operator and $X, Y \in \mathbb{R}^{a \times b}$ and $Q \in \mathbb{R}^{n \times n}$. How do I evaluate $\frac{\partial t(Q,X, Y)}{\partial X}$ and $\frac{\partial t(Q,X, Y)}{\partial Y}$? I mean, I would like to know how to correctly apply the chain rule. * Addition * I will try to give more information about my problem. Suppose that $a = b = n$ and that $f(A,B) = AB$ and $g(A,B) = BA + AB$ (actually this is only an example of possible functions $f$ and $g$). Then I have that: $$f(g(Q,X),Y) = f(XQ + QX, Y) = XQY + QXY$$ Then, using matrix calculus (hoping there are no error!), I have that: $$ \frac{\partial t(Q,X,Y)}{\partial X} = QY + YQ\\ \frac{\partial t(Q,X,Y)}{\partial Y} = XQ + QX$$ I can easily compute the result if I know the form of $f$ and $g$. Notice that the derivatives I obtained are in a matrix form.  But actually I need to deal with generic functions. And for this reason I need to use the chain rule. The problem is that the chain rule formulas I know are helpful to derive the derivative with respect to a certain element of the matrix $X$ (or $Y$). In this case, I'm not able to have a matrix form of the derivatives. So, my question is... there is a chain rule formula I'm missing which let me describe these derivatives in a matrix form? * Addition 2 * The chain rule formulas that I know are reported here http://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-matrix_identities (see the 7th row of the table)","I googled around and searched inside the forum but I'm still confused about a problem. I have 2 matrix functions $f,g : \mathbb{R}^{n \times n} \times \mathbb{R}^{a \times b} \rightarrow \mathbb{R}^{n \times n}$. Starting from this, I have the following expression: $$ t(Q, X, Y) = \text{tr}(f(g(Q, X),Y))$$ where $\text{tr}$ is the trace operator and $X, Y \in \mathbb{R}^{a \times b}$ and $Q \in \mathbb{R}^{n \times n}$. How do I evaluate $\frac{\partial t(Q,X, Y)}{\partial X}$ and $\frac{\partial t(Q,X, Y)}{\partial Y}$? I mean, I would like to know how to correctly apply the chain rule. * Addition * I will try to give more information about my problem. Suppose that $a = b = n$ and that $f(A,B) = AB$ and $g(A,B) = BA + AB$ (actually this is only an example of possible functions $f$ and $g$). Then I have that: $$f(g(Q,X),Y) = f(XQ + QX, Y) = XQY + QXY$$ Then, using matrix calculus (hoping there are no error!), I have that: $$ \frac{\partial t(Q,X,Y)}{\partial X} = QY + YQ\\ \frac{\partial t(Q,X,Y)}{\partial Y} = XQ + QX$$ I can easily compute the result if I know the form of $f$ and $g$. Notice that the derivatives I obtained are in a matrix form.  But actually I need to deal with generic functions. And for this reason I need to use the chain rule. The problem is that the chain rule formulas I know are helpful to derive the derivative with respect to a certain element of the matrix $X$ (or $Y$). In this case, I'm not able to have a matrix form of the derivatives. So, my question is... there is a chain rule formula I'm missing which let me describe these derivatives in a matrix form? * Addition 2 * The chain rule formulas that I know are reported here http://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-matrix_identities (see the 7th row of the table)",,"['matrices', 'derivatives']"
26,"Rank, nuclear and Frobenius norms of a matrix","Rank, nuclear and Frobenius norms of a matrix",,"The nuclear norm, denoted $\| \cdot \|_*$ is a good surrogate for the rank when minimizing problems like $$\label{pb1}\tag{1} \min_X \operatorname{rank} (X) : AX = B $$ Here, we're trying to find a low-rank matrix $X$ such that $AX=B$ . If I recall correctly, when the singular values of $X$ are bounded above by $1$ , one can replace $\operatorname{rank}$ by $\| \cdot \|_*$ in the problem \eqref{pb1}. What about the Frobenius norm? Can the Frobenius norm be a good surrogate to the nuclear norm? Under which assumptions? Since we have $$\|X\|_* = \min_{X=UV^\top} \|U\|_F\|V\|_F$$ in particular we have $\|XX^\top\|_* = \|X\|_F^2$ . Then, because $\|X\|_1\le 1$ , we also have $\|XX^\top\|_1\le 1$ . So, this is pretty direct, no? Is the Frobenius norm also a good surrogate?","The nuclear norm, denoted is a good surrogate for the rank when minimizing problems like Here, we're trying to find a low-rank matrix such that . If I recall correctly, when the singular values of are bounded above by , one can replace by in the problem \eqref{pb1}. What about the Frobenius norm? Can the Frobenius norm be a good surrogate to the nuclear norm? Under which assumptions? Since we have in particular we have . Then, because , we also have . So, this is pretty direct, no? Is the Frobenius norm also a good surrogate?",\| \cdot \|_* \label{pb1}\tag{1} \min_X \operatorname{rank} (X) : AX = B  X AX=B X 1 \operatorname{rank} \| \cdot \|_* \|X\|_* = \min_{X=UV^\top} \|U\|_F\|V\|_F \|XX^\top\|_* = \|X\|_F^2 \|X\|_1\le 1 \|XX^\top\|_1\le 1,"['matrices', 'optimization', 'normed-spaces', 'matrix-rank', 'nuclear-norm']"
27,Does the same determinant make two matrices equal to each other?,Does the same determinant make two matrices equal to each other?,,Does the same determinant make two matrices equal to each other? If I have: Find all values of $x$ that make $$\begin{pmatrix}2 & -1 &4\\3 & 0 & 5\\4 & 1 & 6\end{pmatrix}=\begin{pmatrix} x & 4\\5 & x\end{pmatrix} $$ Would I calculate and equate the determinants of both matrices to solve this problem? Edit: Below is the exact question. Do the style of brackets refer to the determinants?,Does the same determinant make two matrices equal to each other? If I have: Find all values of that make Would I calculate and equate the determinants of both matrices to solve this problem? Edit: Below is the exact question. Do the style of brackets refer to the determinants?,x \begin{pmatrix}2 & -1 &4\\3 & 0 & 5\\4 & 1 & 6\end{pmatrix}=\begin{pmatrix} x & 4\\5 & x\end{pmatrix} ,"['matrices', 'determinant']"
28,Why is the group of unit upper triangular matrices solvable?,Why is the group of unit upper triangular matrices solvable?,,"Let $GL_n(k)$ be the $n$ by $n$ general linear group over $k$ , $B_n(k)$ be the subgroup of $GL_n(k)$ consisting of all upper triangular matrices, and $U_n(k)$ be the subgroup of $B_n(k)$ whose diagonal elements are all $1$ . To show $B_n(k)$ is solvable, I'm proving it now by following steps: $U_n(k)$ is a subgroup of $B_n(k)$ . (done) $U_n(k)$ is normal in $B_n(k)$ . (done) $U_n(k)$ is solvable. (question) $B_n(k) / U_n(k)$ is also solvable. (not yet) $B_n(k)$ is solvable. (by the below thm) I'll use a theorem to verify $B_n(k)$ is solvable. $G$ is solvable if and only if $H$ and $G/H$ are solvable for some normal subgroup $H$ of $G$ . So, I have to prove both step 3 and step 4. But I have no idea about them. How to prove them? Since my knowledge is not enough, I don't want to show them using Lie theory. Thanks in advance.","Let be the by general linear group over , be the subgroup of consisting of all upper triangular matrices, and be the subgroup of whose diagonal elements are all . To show is solvable, I'm proving it now by following steps: is a subgroup of . (done) is normal in . (done) is solvable. (question) is also solvable. (not yet) is solvable. (by the below thm) I'll use a theorem to verify is solvable. is solvable if and only if and are solvable for some normal subgroup of . So, I have to prove both step 3 and step 4. But I have no idea about them. How to prove them? Since my knowledge is not enough, I don't want to show them using Lie theory. Thanks in advance.",GL_n(k) n n k B_n(k) GL_n(k) U_n(k) B_n(k) 1 B_n(k) U_n(k) B_n(k) U_n(k) B_n(k) U_n(k) B_n(k) / U_n(k) B_n(k) B_n(k) G H G/H H G,"['matrices', 'group-theory', 'solvable-groups']"
29,How is that a rotation by an angle θ about the origin can be represented by this transformation matrix?,How is that a rotation by an angle θ about the origin can be represented by this transformation matrix?,,"$$ \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta &  \cos\theta \end{pmatrix} $$ How was this matrix derived? I know how to use it, but where did it come from? Can someone prove why this matrix represents a rotation about the origin by an angle theta?","$$ \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta &  \cos\theta \end{pmatrix} $$ How was this matrix derived? I know how to use it, but where did it come from? Can someone prove why this matrix represents a rotation about the origin by an angle theta?",,"['matrices', 'trigonometry', 'transformation']"
30,Determining if a symmetric matrix is positive definite,Determining if a symmetric matrix is positive definite,,"I have a symmetric matrix where all non-diagonal elements are positive and identical, and all diagonal elements are identical as well. For example, the $3 \times 3$ version of this matrix has the following form:  $$  \left( \begin{array}{ccc} 2a+b & a & a \\ a & 2a+b & a \\ a & a & 2a+b \end{array} \right)  $$ Note that $a>0\ , b>0$. For such a simple form, is there an easy way of determining that the above matrix is positive definite in the general $n \times n$   case? I'd like to show that the matrix is still positive definite when the dimension is higher. Thank you.","I have a symmetric matrix where all non-diagonal elements are positive and identical, and all diagonal elements are identical as well. For example, the $3 \times 3$ version of this matrix has the following form:  $$  \left( \begin{array}{ccc} 2a+b & a & a \\ a & 2a+b & a \\ a & a & 2a+b \end{array} \right)  $$ Note that $a>0\ , b>0$. For such a simple form, is there an easy way of determining that the above matrix is positive definite in the general $n \times n$   case? I'd like to show that the matrix is still positive definite when the dimension is higher. Thank you.",,"['matrices', 'positive-definite', 'symmetric-matrices', 'positive-semidefinite']"
31,Hessian matrix as derivative of gradient,Hessian matrix as derivative of gradient,,"From a text: For a real-valued differentiable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$, the Hessian matrix $D^2f(x)$ is the derivative matrix of the vector-valued gradient function $\nabla f(x)$; i.e., $D^2f(x)=D[\nabla f(x)]$. $\nabla f(x)$ is just an $n\times 1$ matrix consisting of $\partial f/\partial x_1,\partial f/\partial x_2,\ldots,\partial f/\partial x_n$. Then $D[\nabla f(x)]$ must be a $1\times n$ matrix. But I know that the Hessian matrix is an $n\times n$ matrix consisting of $\partial ^2f/\partial x_i\partial x_j$. How can the given definition be consistent with this?","From a text: For a real-valued differentiable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$, the Hessian matrix $D^2f(x)$ is the derivative matrix of the vector-valued gradient function $\nabla f(x)$; i.e., $D^2f(x)=D[\nabla f(x)]$. $\nabla f(x)$ is just an $n\times 1$ matrix consisting of $\partial f/\partial x_1,\partial f/\partial x_2,\ldots,\partial f/\partial x_n$. Then $D[\nabla f(x)]$ must be a $1\times n$ matrix. But I know that the Hessian matrix is an $n\times n$ matrix consisting of $\partial ^2f/\partial x_i\partial x_j$. How can the given definition be consistent with this?",,"['matrices', 'derivatives', 'definition', 'partial-derivative']"
32,Why use the Kronecker product?,Why use the Kronecker product?,,I have found many references on Kronecker product but I did not see any reference talking about why this way of multiplication exist and whats the intuitive use of this particular product. Appreciate your suggestions!,I have found many references on Kronecker product but I did not see any reference talking about why this way of multiplication exist and whats the intuitive use of this particular product. Appreciate your suggestions!,,"['matrices', 'kronecker-product', 'vectorization']"
33,Physical meaning of norm of a matrix,Physical meaning of norm of a matrix,,I know norm of a vector is a length of a vector from origin. So what is the motivation behind defining the norm of the matrix? What is the physical meaning of norm of a matrix? Any help is appreciated. Thanks.,I know norm of a vector is a length of a vector from origin. So what is the motivation behind defining the norm of the matrix? What is the physical meaning of norm of a matrix? Any help is appreciated. Thanks.,,"['matrices', 'normed-spaces']"
34,Kronecker product and the commutation matrix,Kronecker product and the commutation matrix,,"Prove that for every $m \times n$ matrix $A$ and $r \times q$ matrix $B$ it holds that $$K(r,m)(A \otimes B)K(n,q) = B \otimes A.$$ I looked up the definition of the commutation matrix which is the following: Commutation matrix . However, I still do not know exactly what does the commutation matrix look like (concretely). Therefore, I do not have a clue how I can check the equation that is claimed, since I cannot do it by taking an example and checking it. Could anyone please help me out? I would very much appreciate it.","Prove that for every $m \times n$ matrix $A$ and $r \times q$ matrix $B$ it holds that $$K(r,m)(A \otimes B)K(n,q) = B \otimes A.$$ I looked up the definition of the commutation matrix which is the following: Commutation matrix . However, I still do not know exactly what does the commutation matrix look like (concretely). Therefore, I do not have a clue how I can check the equation that is claimed, since I cannot do it by taking an example and checking it. Could anyone please help me out? I would very much appreciate it.",,['matrices']
35,Is spectral radius = operator norm for a positive valued matrix?,Is spectral radius = operator norm for a positive valued matrix?,,"For any real-valued square matrix with all positive entries, by Perron-Frobenius theory, we have that the matrix has a dominant eigenvalue that is real, positive, and of multiplicity 1. Thus, the spectral radius is equal to the largest eigenvalue. Q: Is the operator norm for such a matrix always equal to the spectral radius, or can it be strictly larger?","For any real-valued square matrix with all positive entries, by Perron-Frobenius theory, we have that the matrix has a dominant eigenvalue that is real, positive, and of multiplicity 1. Thus, the spectral radius is equal to the largest eigenvalue. Q: Is the operator norm for such a matrix always equal to the spectral radius, or can it be strictly larger?",,"['matrices', 'matrix-norms', 'spectral-radius', 'spectral-norm', 'positive-matrices']"
36,What is the practical impact of a matrix's condition number?,What is the practical impact of a matrix's condition number?,,"Let's say I am trying to solve a square linear system $Ax = b$ for whatever reason. A perturbation $\delta b$ in $b$ will lead to a perturbation $\delta x$ in $x$, whose relative norm is bounded by the condition number of A $\kappa (A)$ according to $$\frac{||\delta x||}{||x||} \leq \kappa(A) \frac{||\delta b||}{||b||}. \tag{1}$$ If the only error/uncertainty of $b$ is due to rounding errors caused by floating point operations, then I am fine as long as $\kappa(A)$ is significantly smaller than the inverse of machine epsilon $\epsilon_\text{mach}$ (right?). But it seems to me that for many (most?) applications, there will be relative errors in $b$ on an order of magnitude much greater than $\epsilon_\text{mach}^{-1}$. An average relative error of $10^{-3}$ in $b$ seems commonplace (for some applications, even $10^{-1}$ is to be expected). Condition numbers on the order of $10^3$, or even $10^6$, also seem very common, especially when the dimensions of $A$ are large. So what does this mean? If $\kappa(A) > 10^3$, I have to make sure that my relative error in $b$ is less than $10^{-3}$, preferably less than $10^{-5}$, in order to get any kind of significance in my computations when solving for $x$? Or are you going to tell me that since Equation $(1)$ is only a worst-case scenario, this will usually not be a problem? After fooling around with random matrices in MATLAB however, it seems to me that the ""average"" scenario usually is not that far from the worst-case scenario. I realize that the answer to this question probably is very problem dependent, but it seems to me that this would be a very frequent problem in practice, and yet in engineering education it is only ever discussed in math courses, and even in those courses there is not much emphasis on this seemingly severe and common problem.","Let's say I am trying to solve a square linear system $Ax = b$ for whatever reason. A perturbation $\delta b$ in $b$ will lead to a perturbation $\delta x$ in $x$, whose relative norm is bounded by the condition number of A $\kappa (A)$ according to $$\frac{||\delta x||}{||x||} \leq \kappa(A) \frac{||\delta b||}{||b||}. \tag{1}$$ If the only error/uncertainty of $b$ is due to rounding errors caused by floating point operations, then I am fine as long as $\kappa(A)$ is significantly smaller than the inverse of machine epsilon $\epsilon_\text{mach}$ (right?). But it seems to me that for many (most?) applications, there will be relative errors in $b$ on an order of magnitude much greater than $\epsilon_\text{mach}^{-1}$. An average relative error of $10^{-3}$ in $b$ seems commonplace (for some applications, even $10^{-1}$ is to be expected). Condition numbers on the order of $10^3$, or even $10^6$, also seem very common, especially when the dimensions of $A$ are large. So what does this mean? If $\kappa(A) > 10^3$, I have to make sure that my relative error in $b$ is less than $10^{-3}$, preferably less than $10^{-5}$, in order to get any kind of significance in my computations when solving for $x$? Or are you going to tell me that since Equation $(1)$ is only a worst-case scenario, this will usually not be a problem? After fooling around with random matrices in MATLAB however, it seems to me that the ""average"" scenario usually is not that far from the worst-case scenario. I realize that the answer to this question probably is very problem dependent, but it seems to me that this would be a very frequent problem in practice, and yet in engineering education it is only ever discussed in math courses, and even in those courses there is not much emphasis on this seemingly severe and common problem.",,"['matrices', 'numerical-linear-algebra', 'condition-number']"
37,Determinant of symmetric matrix with the main diagonal elements zero,Determinant of symmetric matrix with the main diagonal elements zero,,"How to prove that the determinant of a symmetric matrix with the main diagonal elements zero and all other elements positive is not zero (i.e., that the matrix is invertible)? EDIT: OP indicates in a comment that the entries above the diagonal are to be distinct.","How to prove that the determinant of a symmetric matrix with the main diagonal elements zero and all other elements positive is not zero (i.e., that the matrix is invertible)? EDIT: OP indicates in a comment that the entries above the diagonal are to be distinct.",,['matrices']
38,"Basic question about $\sup_{x\neq 0}{} \frac{\|Ax\|}{\|x\|} = \sup_{\|x\| = 1}{\|Ax\|} $, $x \in\mathbb{R}^n$","Basic question about ,",\sup_{x\neq 0}{} \frac{\|Ax\|}{\|x\|} = \sup_{\|x\| = 1}{\|Ax\|}  x \in\mathbb{R}^n,"I am having trouble with understanding the following definition while studying some basic things related with matrix norms: For every matrix $A\in M_n(\mathbb{R})$ $$\sup_{x\neq 0}{} \frac{\|Ax\|}{\|x\|} = \sup_{\|x\| = 1}{\|Ax\|},\; x \in\mathbb{R}^n$$ Why we are taking $\|x\| = 1$?  Is there any proof of above statement? Added: Why to take sup in the definition of norm? I need help to understand above mentioned fact. Thank you very much.","I am having trouble with understanding the following definition while studying some basic things related with matrix norms: For every matrix $A\in M_n(\mathbb{R})$ $$\sup_{x\neq 0}{} \frac{\|Ax\|}{\|x\|} = \sup_{\|x\| = 1}{\|Ax\|},\; x \in\mathbb{R}^n$$ Why we are taking $\|x\| = 1$?  Is there any proof of above statement? Added: Why to take sup in the definition of norm? I need help to understand above mentioned fact. Thank you very much.",,"['matrices', 'functional-analysis', 'normed-spaces']"
39,What's the algorithm of finding the convex combination of permutation matrices for a doubly stochastic matrix?,What's the algorithm of finding the convex combination of permutation matrices for a doubly stochastic matrix?,,"According to Birkhoff, $n$ -by- $n$ stochastic matrices form a convex polytope whose extreme points are precisely the permutation matrices. It implies that any doubly stochastic matrix can be written as a convex combination of finitely many permutation matrices. Given a doubly stochastic matrix, is there an algorithm to compute the coefficients of a convex combination? For example, given $$A = \begin{bmatrix} 0.5&0.2&0.3\\ 0.1&0.6&0.3\\ 0.4&0.2&0.4\end{bmatrix},$$ how do you know the following? $$A=0.4\left[\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}\right]+0.2\left[\begin{matrix}0&1&0\\0&0&1\\1&0&0\end{matrix}\right]+0.1\left[\begin{matrix}0&0&1\\1&0&0\\0&1&0\end{matrix}\right]+0.1\left[\begin{matrix}1&0&0\\0&0&1\\0&1&0\end{matrix}\right]+0.2\left[\begin{matrix}0&0&1\\0&1&0\\1&0&0\end{matrix}\right]$$ Please do briefly prove the correctness of the algorithm.","According to Birkhoff, -by- stochastic matrices form a convex polytope whose extreme points are precisely the permutation matrices. It implies that any doubly stochastic matrix can be written as a convex combination of finitely many permutation matrices. Given a doubly stochastic matrix, is there an algorithm to compute the coefficients of a convex combination? For example, given how do you know the following? Please do briefly prove the correctness of the algorithm.","n n A = \begin{bmatrix} 0.5&0.2&0.3\\ 0.1&0.6&0.3\\ 0.4&0.2&0.4\end{bmatrix}, A=0.4\left[\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}\right]+0.2\left[\begin{matrix}0&1&0\\0&0&1\\1&0&0\end{matrix}\right]+0.1\left[\begin{matrix}0&0&1\\1&0&0\\0&1&0\end{matrix}\right]+0.1\left[\begin{matrix}1&0&0\\0&0&1\\0&1&0\end{matrix}\right]+0.2\left[\begin{matrix}0&0&1\\0&1&0\\1&0&0\end{matrix}\right]","['matrices', 'polytopes', 'stochastic-matrices', 'birkhoff-polytopes', 'permutation-matrices']"
40,Matrix-by-matrix derivative formula,Matrix-by-matrix derivative formula,,I need to derive $$\frac{\delta(X^{T}MX)}{\delta X}$$ where $X$ and $M$ are $n \times n$ matrices. I know that $$\frac{\delta(AXB)}{\delta X}=B^{T} \otimes A$$ but am having a hard time deriving what I need from that or from scratch.,I need to derive where and are matrices. I know that but am having a hard time deriving what I need from that or from scratch.,\frac{\delta(X^{T}MX)}{\delta X} X M n \times n \frac{\delta(AXB)}{\delta X}=B^{T} \otimes A,"['matrices', 'derivatives', 'matrix-calculus']"
41,Solution set of an LMI is convex,Solution set of an LMI is convex,,"I was going through Boyd & Vandenberghe's Convex Optimization . On page 38, the authors mentioned that the solution set of a linear matrix inequality (LMI) is convex. $$ A(x) := x_1 A_1 + \dots + x_n A_n \preceq B $$ where $A_1, \dots, A_n, B \in \mathbb{S}^m$ , is called an LMI in $x$ . They also gave a brief explanation where they mentioned that this is because it is the inverse image of the positive semi-definite cone under the affine function. I could not figure out what would be the affine function that they mentioned.","I was going through Boyd & Vandenberghe's Convex Optimization . On page 38, the authors mentioned that the solution set of a linear matrix inequality (LMI) is convex. where , is called an LMI in . They also gave a brief explanation where they mentioned that this is because it is the inverse image of the positive semi-definite cone under the affine function. I could not figure out what would be the affine function that they mentioned."," A(x) := x_1 A_1 + \dots + x_n A_n \preceq B  A_1, \dots, A_n, B \in \mathbb{S}^m x","['matrices', 'convex-analysis', 'symmetric-matrices', 'linear-matrix-inequality', 'spectrahedra']"
42,Prove that $\displaystyle\lim_{k \to \infty} \left( I + \frac{1}{k}A \right)^{k} = e^A$,Prove that,\displaystyle\lim_{k \to \infty} \left( I + \frac{1}{k}A \right)^{k} = e^A,I'm having a little trouble here to prove the following statement: Let $A$ be an $n \times n$ matrix (real or complex). Prove that $$\lim_{k \to \infty} \left( I + \frac{1}{k} A \right)^{k} = e^{A}$$ Now I'm using matrix and possible non-commutative; I don't know where to begin. Can you give a spit? Thanks for your attention!,I'm having a little trouble here to prove the following statement: Let be an matrix (real or complex). Prove that Now I'm using matrix and possible non-commutative; I don't know where to begin. Can you give a spit? Thanks for your attention!,A n \times n \lim_{k \to \infty} \left( I + \frac{1}{k} A \right)^{k} = e^{A},"['matrices', 'limits', 'exponential-function', 'matrix-calculus', 'matrix-exponential']"
43,Calculating RGB plus Amber,Calculating RGB plus Amber,,"I'm currently working on a wide gamut light source using red, green and blue LED emitters. From an internal xyY (or CIE XYZ) representation, I can reach any color or color temperature via a 3x3 transformation matrix. The matrix is calculated from the chromaticity coordinates and the relative luminance of the actual RGB emitters. This works well both in theory and in practice. However, the RGB LEDs emitts a discontinuous spectrum with very little energy between red and green. I want to add an orange or amber LED to improve the spectrum and the color rendering index. Starting from CIE xyY, how do I calculate RGB plus Amber? The entire CIE model is based on tristimulus and I can't see how I can use it to calculate a fourth color. The ideal would probably be a model that could accept any number of colors. ADDED based on discussion: Above is an illustration of how I imagine the RGB math works. I measure the chromaticity coordinates (x,y) of each of the Red, Green and Blue emitters, and their relative brightness (Y). From xyY I calculate CIE XYZ. This is needed because xy plus Y is a projection, XYZ is the actual 3-dimensional color space. I put the XYZ values for the three emitters into a matrix and calculate the inverse of that matrix. This inverse matrix represents the difference between the ""actual"" (human vision) and the properties of these particular emitters. If I want to display a particular color, say the white shown above to the right, I can take the desired coordinates, send them through the matrix, and get the required strength for each emitter (shown as arrows). Now RGBA: Originally I thought that the math for one more emitter (RGBA) was similar to the math for RGB. That I could use a 3*4 matrix to calculate RGBA, the same way I use a 3*3 matrix to calculate RGB. However, it seems like there are problems calculating the inverse of a non-square matrix. Some sources say it can not be done, some say if can be done, but the resulting matrix be lacking in some respect. This is WAY beyond my understanding! :-) @percusse suggests that a 3*4 matrix can be used. If so, how can I calculate the inverse matrix (I'm on a shoestring budget, software like MATLAB is out of reach). Second EDIT: Based on the input from @joriki and @percusse I've tried to solve this on paper. I've spent a lot of paper, but I can't seem to find a way to do this that can be implemented as computer code, - or even produces the right answer! I'm probably making mistakes in the practical solving, but that is not actually critical. Computing will have to be done by a C implementation of a solving algorithm (gaussian elimination?) that is known to be good. This would be typ XYZ values for the four emitters calculated form measured xyY coordinates (actual values will have better precision): $$ \left[ \begin{array}{ccc} 0.47 & 0.11 & 0.19 & 0.34\\ 0.20 & 0.43 & 0.11 & 0.26\\ 0.00 & 0.06 & 1.12 & 0.01\end{array} \right] \left[ \begin{array}{ccc} R \\\ G \\\ B \\\ A \end{array} \right] \left[ \begin{array}{ccc} X \\\ Y \\\ Z \end{array} \right] $$ I've been thinking about optimization and there are a number of parameters that affects the optimal mix, mainly spectrum, efficacy, and heat. For a small system, it is probably enough to worry about the extremes of the amber emitter (avoid max amber when emitting orange-ish light, avoid min amber when emitting any other color). A solution is already suggested by @joriki [""This selects the solution that covers the spectrum most evenly""] but I don't understand the math :-) So I need to get this system of equations into a form that generates a single answer within 500us of computing time on a small embedded processor :-) Any guidance on how to get a step closer a practical implementation would be greatly appreciated! Third EDIT: I've set up a test that can drive 4 emitters, and a spectrometer to measure the output. The relative intensities of the emitters are tweaked to give a correlated color temperature of roughly 6000 Kelvin (midday daylight). RGB at ca. 6000K: RGB + Amber at ca. 6000K: RGB + White at ca. 6000K: The first image shows the spectrum from 3 emitters, Red, Green and Blue. There is very little light between 560 and 610 nm.  The next image shows the spectrum when Amber is added to RGB. Amber improves the situation significantly. (Yellow might be better, but suitable high brightness yellow LEDs can't be found). The last image shows the spectrum when White is added to RGB. White LEDs are actually phosphor converted blue. The phosphor can be made to retransmitt over a fairly broad spectrum. This seems to give the best result in terms of even spectrum. I think I have working code for Gaussian Elimination. The question now is how do I add mean square minimization to the equations in such a way that I end up with a single answer? I probably need some hints on how to solve this in practice. Sorry! :-) Fourth and fifth EDIT: So I have measured the spectra from 380 to 780nm with 1nm resolution. The output is measured at equal input value. I calculated the area under the curve by trigonometry. I calculated the average size for the 400 trapeziods between 380 and 780nm for R, G, B and A (values are scaled to me more manageable): $\langle R\rangle = 19.8719507$ $\langle G\rangle = 13.39000051$ $\langle B\rangle = 29.30636046$ $\langle A\rangle = 8.165754589$ And also the average for the product of all six (plus four) combinations of emitter pairs. I then took a stab at assembling this into a covariance matrix: $$ \left[ \begin{array}{cccc} 43.74282392 & -2.642812728 & -5.823745503 & -0.26554119\\\ -2.642812728 & 8.563382072 & -0.969894212 & -0.946563019\\\ -5.823745503 & -0.969894212 &  62.81754221 & -2.393057209\\\ -0.26554119 & -0.946563019 & -2.393057209 &  8.136438369\end{array} \right] $$ The matrix is assembled like this: $$ \left[ \begin{array}{ccc} \langle RR \rangle - \langle R \rangle\langle R \rangle & \langle RG \rangle - \langle R \rangle\langle G \rangle & \langle RB \rangle - \langle R \rangle\langle B \rangle & \langle RA \rangle - \langle R \rangle\langle A \rangle\\\ \langle RG \rangle - \langle R \rangle\langle G \rangle & ... & ... & ...\\\ ... & ... & ... & ...\\\ ... & ... & ... & ...\end{array} \right] $$ Here are the measured color coordinates of the RGBA emitters, and sample values for XYZ: $$ \left[ \begin{array}{cccc|c} 0.490449254 & 0.100440581 & 0.221653947 & 0.343906601 & 0.75\\\ 0.204678363 & 0.421052632 & 0.16374269 & 0.210526316 & 1.00\\\ -0.011955512 & 0.07388664 & 1.464803251 & -0.012677086 & 0.75\end{array} \right] $$ I've tried to get the above matrix into echelon form(?) by gaussian elimination, and then get the RGB values on the form  $u + Av$ by substitution. $R:  0.97921341 + A * -0.701207308$ $G:  1.730718699 + A * -0.1767291$ $B:  0.43 + A * 0.012215723$ The next step seems to be to calculate $Q$. This has been answered by @joriki, but am not used to the notation and I'm not at all sure how to translate the greek shorthand to a form where I can calculate the values. If this gets too basic for this forum, let me know and I'll take it offline. I have trouble understanding this calculation: $$ \begin{eqnarray} \mu &=& -\frac{\sum_{\alpha,\beta}M_{\alpha\beta}x_\alpha y_\beta}{\sum_{\alpha,\beta}M_{\alpha\beta}y_\alpha y_\beta} \;. \end{eqnarray} $$ Not entirely sure what the $x$ and $y$ values are? A pointer to an example of what this $M_{\alpha\beta}x_\alpha y_\beta$ look like in non-algebraic form would be very helpful. Sixth EDIT: So let me try to explain how I understand what needs to be done: With a set of measured RGBA emitter color coordinates and an ZYX value (the color we want the emitters to generate) as input we calculate two values for each emitter. The values are $R = u_{RED} + Av_{RED}$ $G = u_{GREEN} + Av_{GREEN}$ $B = u_{BLUE} + Av_{BLUE}$ $A = A$ The calculation involves gaussian elimination and substitution, and I have written code that performs those calculations. The value of A should preferably be the one that, together with RGB, produces the most even spectrum. This appears to be the calculation of A: $$ \begin{eqnarray} A &=& -\frac{\sum_{\alpha,\beta}M_{\alpha\beta}x_\alpha y_\beta}{\sum_{\alpha,\beta}M_{\alpha\beta}y_\alpha y_\beta} \;. \end{eqnarray} $$ One element of this equation is $M_{\alpha\beta}$ which is a 4 * 4 covariance matrix that we have precalculated from the emitter spectra. This is as far as I am right now. I don't understand from the above notation how the math works. Do I run every possible combination of emitter colors through the matrix add them all up? I have to admit I am completely lost! :-)","I'm currently working on a wide gamut light source using red, green and blue LED emitters. From an internal xyY (or CIE XYZ) representation, I can reach any color or color temperature via a 3x3 transformation matrix. The matrix is calculated from the chromaticity coordinates and the relative luminance of the actual RGB emitters. This works well both in theory and in practice. However, the RGB LEDs emitts a discontinuous spectrum with very little energy between red and green. I want to add an orange or amber LED to improve the spectrum and the color rendering index. Starting from CIE xyY, how do I calculate RGB plus Amber? The entire CIE model is based on tristimulus and I can't see how I can use it to calculate a fourth color. The ideal would probably be a model that could accept any number of colors. ADDED based on discussion: Above is an illustration of how I imagine the RGB math works. I measure the chromaticity coordinates (x,y) of each of the Red, Green and Blue emitters, and their relative brightness (Y). From xyY I calculate CIE XYZ. This is needed because xy plus Y is a projection, XYZ is the actual 3-dimensional color space. I put the XYZ values for the three emitters into a matrix and calculate the inverse of that matrix. This inverse matrix represents the difference between the ""actual"" (human vision) and the properties of these particular emitters. If I want to display a particular color, say the white shown above to the right, I can take the desired coordinates, send them through the matrix, and get the required strength for each emitter (shown as arrows). Now RGBA: Originally I thought that the math for one more emitter (RGBA) was similar to the math for RGB. That I could use a 3*4 matrix to calculate RGBA, the same way I use a 3*3 matrix to calculate RGB. However, it seems like there are problems calculating the inverse of a non-square matrix. Some sources say it can not be done, some say if can be done, but the resulting matrix be lacking in some respect. This is WAY beyond my understanding! :-) @percusse suggests that a 3*4 matrix can be used. If so, how can I calculate the inverse matrix (I'm on a shoestring budget, software like MATLAB is out of reach). Second EDIT: Based on the input from @joriki and @percusse I've tried to solve this on paper. I've spent a lot of paper, but I can't seem to find a way to do this that can be implemented as computer code, - or even produces the right answer! I'm probably making mistakes in the practical solving, but that is not actually critical. Computing will have to be done by a C implementation of a solving algorithm (gaussian elimination?) that is known to be good. This would be typ XYZ values for the four emitters calculated form measured xyY coordinates (actual values will have better precision): $$ \left[ \begin{array}{ccc} 0.47 & 0.11 & 0.19 & 0.34\\ 0.20 & 0.43 & 0.11 & 0.26\\ 0.00 & 0.06 & 1.12 & 0.01\end{array} \right] \left[ \begin{array}{ccc} R \\\ G \\\ B \\\ A \end{array} \right] \left[ \begin{array}{ccc} X \\\ Y \\\ Z \end{array} \right] $$ I've been thinking about optimization and there are a number of parameters that affects the optimal mix, mainly spectrum, efficacy, and heat. For a small system, it is probably enough to worry about the extremes of the amber emitter (avoid max amber when emitting orange-ish light, avoid min amber when emitting any other color). A solution is already suggested by @joriki [""This selects the solution that covers the spectrum most evenly""] but I don't understand the math :-) So I need to get this system of equations into a form that generates a single answer within 500us of computing time on a small embedded processor :-) Any guidance on how to get a step closer a practical implementation would be greatly appreciated! Third EDIT: I've set up a test that can drive 4 emitters, and a spectrometer to measure the output. The relative intensities of the emitters are tweaked to give a correlated color temperature of roughly 6000 Kelvin (midday daylight). RGB at ca. 6000K: RGB + Amber at ca. 6000K: RGB + White at ca. 6000K: The first image shows the spectrum from 3 emitters, Red, Green and Blue. There is very little light between 560 and 610 nm.  The next image shows the spectrum when Amber is added to RGB. Amber improves the situation significantly. (Yellow might be better, but suitable high brightness yellow LEDs can't be found). The last image shows the spectrum when White is added to RGB. White LEDs are actually phosphor converted blue. The phosphor can be made to retransmitt over a fairly broad spectrum. This seems to give the best result in terms of even spectrum. I think I have working code for Gaussian Elimination. The question now is how do I add mean square minimization to the equations in such a way that I end up with a single answer? I probably need some hints on how to solve this in practice. Sorry! :-) Fourth and fifth EDIT: So I have measured the spectra from 380 to 780nm with 1nm resolution. The output is measured at equal input value. I calculated the area under the curve by trigonometry. I calculated the average size for the 400 trapeziods between 380 and 780nm for R, G, B and A (values are scaled to me more manageable): $\langle R\rangle = 19.8719507$ $\langle G\rangle = 13.39000051$ $\langle B\rangle = 29.30636046$ $\langle A\rangle = 8.165754589$ And also the average for the product of all six (plus four) combinations of emitter pairs. I then took a stab at assembling this into a covariance matrix: $$ \left[ \begin{array}{cccc} 43.74282392 & -2.642812728 & -5.823745503 & -0.26554119\\\ -2.642812728 & 8.563382072 & -0.969894212 & -0.946563019\\\ -5.823745503 & -0.969894212 &  62.81754221 & -2.393057209\\\ -0.26554119 & -0.946563019 & -2.393057209 &  8.136438369\end{array} \right] $$ The matrix is assembled like this: $$ \left[ \begin{array}{ccc} \langle RR \rangle - \langle R \rangle\langle R \rangle & \langle RG \rangle - \langle R \rangle\langle G \rangle & \langle RB \rangle - \langle R \rangle\langle B \rangle & \langle RA \rangle - \langle R \rangle\langle A \rangle\\\ \langle RG \rangle - \langle R \rangle\langle G \rangle & ... & ... & ...\\\ ... & ... & ... & ...\\\ ... & ... & ... & ...\end{array} \right] $$ Here are the measured color coordinates of the RGBA emitters, and sample values for XYZ: $$ \left[ \begin{array}{cccc|c} 0.490449254 & 0.100440581 & 0.221653947 & 0.343906601 & 0.75\\\ 0.204678363 & 0.421052632 & 0.16374269 & 0.210526316 & 1.00\\\ -0.011955512 & 0.07388664 & 1.464803251 & -0.012677086 & 0.75\end{array} \right] $$ I've tried to get the above matrix into echelon form(?) by gaussian elimination, and then get the RGB values on the form  $u + Av$ by substitution. $R:  0.97921341 + A * -0.701207308$ $G:  1.730718699 + A * -0.1767291$ $B:  0.43 + A * 0.012215723$ The next step seems to be to calculate $Q$. This has been answered by @joriki, but am not used to the notation and I'm not at all sure how to translate the greek shorthand to a form where I can calculate the values. If this gets too basic for this forum, let me know and I'll take it offline. I have trouble understanding this calculation: $$ \begin{eqnarray} \mu &=& -\frac{\sum_{\alpha,\beta}M_{\alpha\beta}x_\alpha y_\beta}{\sum_{\alpha,\beta}M_{\alpha\beta}y_\alpha y_\beta} \;. \end{eqnarray} $$ Not entirely sure what the $x$ and $y$ values are? A pointer to an example of what this $M_{\alpha\beta}x_\alpha y_\beta$ look like in non-algebraic form would be very helpful. Sixth EDIT: So let me try to explain how I understand what needs to be done: With a set of measured RGBA emitter color coordinates and an ZYX value (the color we want the emitters to generate) as input we calculate two values for each emitter. The values are $R = u_{RED} + Av_{RED}$ $G = u_{GREEN} + Av_{GREEN}$ $B = u_{BLUE} + Av_{BLUE}$ $A = A$ The calculation involves gaussian elimination and substitution, and I have written code that performs those calculations. The value of A should preferably be the one that, together with RGB, produces the most even spectrum. This appears to be the calculation of A: $$ \begin{eqnarray} A &=& -\frac{\sum_{\alpha,\beta}M_{\alpha\beta}x_\alpha y_\beta}{\sum_{\alpha,\beta}M_{\alpha\beta}y_\alpha y_\beta} \;. \end{eqnarray} $$ One element of this equation is $M_{\alpha\beta}$ which is a 4 * 4 covariance matrix that we have precalculated from the emitter spectra. This is as far as I am right now. I don't understand from the above notation how the math works. Do I run every possible combination of emitter colors through the matrix add them all up? I have to admit I am completely lost! :-)",,['matrices']
44,Definition of reducible matrix and relation with not strongly connected digraph,Definition of reducible matrix and relation with not strongly connected digraph,,"I connot quite understand the definition of reducible matrix here . We know $A_{n\times n}$ is reducible, when there exists a permutation matrix $\textbf{P}$ such that: $$P^TAP=\begin{bmatrix}X  & Y\\0  & Z\end{bmatrix},$$ where $X$ and $Z$ are both square. I cannot understand: $a_{i_{\alpha}j_{\beta}}=0, \ \ \forall \alpha = 1, \ldots ,\mu,\ \ \text{and} \ \  \beta = 1,\ldots, \nu$.  Could anyone provide a specific example? How can we say if it is the case, then the corresponding digraph is not strongly connected. Here is one answer about this. If strongly connected digraph holds, there exists a path $i_1i_2,\ldots,i_n$. How to say this condition will violate $a_{i_{\alpha}j_{\beta}}=0$? Ex: Consider the strongly connected digraph: $1 \rightarrow 2 \rightarrow 3 \rightarrow 1$. $A$ could be chosen as $$A=\begin{bmatrix}0  & 2 & 0\\0  & 0 & 3\\ 4 & 0 & 0\end{bmatrix}$$ I cannot grasp the structure of matrix corresponding to the digraph.","I connot quite understand the definition of reducible matrix here . We know $A_{n\times n}$ is reducible, when there exists a permutation matrix $\textbf{P}$ such that: $$P^TAP=\begin{bmatrix}X  & Y\\0  & Z\end{bmatrix},$$ where $X$ and $Z$ are both square. I cannot understand: $a_{i_{\alpha}j_{\beta}}=0, \ \ \forall \alpha = 1, \ldots ,\mu,\ \ \text{and} \ \  \beta = 1,\ldots, \nu$.  Could anyone provide a specific example? How can we say if it is the case, then the corresponding digraph is not strongly connected. Here is one answer about this. If strongly connected digraph holds, there exists a path $i_1i_2,\ldots,i_n$. How to say this condition will violate $a_{i_{\alpha}j_{\beta}}=0$? Ex: Consider the strongly connected digraph: $1 \rightarrow 2 \rightarrow 3 \rightarrow 1$. $A$ could be chosen as $$A=\begin{bmatrix}0  & 2 & 0\\0  & 0 & 3\\ 4 & 0 & 0\end{bmatrix}$$ I cannot grasp the structure of matrix corresponding to the digraph.",,"['matrices', 'graph-theory']"
45,operator exponential,operator exponential,,"The matrix exponential is a well know thing but when I see online it is provided for matrices. Does it have the same expansion for a linear operator? That is, if $A$ is a linear operator, then $$e^A = I+A+\frac{1}{2}A^2+\cdots+\frac{1}{k!}A^k+\cdots$$","The matrix exponential is a well know thing but when I see online it is provided for matrices. Does it have the same expansion for a linear operator? That is, if is a linear operator, then",A e^A = I+A+\frac{1}{2}A^2+\cdots+\frac{1}{k!}A^k+\cdots,"['matrices', 'operator-theory']"
46,How to check if a matrix is positive definite,How to check if a matrix is positive definite,,"I want to know how to check if a matrix M is positive definite ,assume that M is 3x3 real numbers matrix I think one way is to put the matrix in a quadratic form $X^TMX$ , where X is a vector $X^T=[x_1   x_2   x_3]$ , my question is if I found that $X^TMX = ax_1^2 + bx_1*x_2+ ........$ can I say that the matrix M is not positive definite because the term $bx_1*x_2$ can be negative or I have to try to put the value of $X^TMX$ in the form of sum of squares e.g., $()^2+()^2+.....$ and then decide? and what is the relation between the positive definiteness of a matrix and its determinant?","I want to know how to check if a matrix M is positive definite ,assume that M is 3x3 real numbers matrix I think one way is to put the matrix in a quadratic form , where X is a vector , my question is if I found that can I say that the matrix M is not positive definite because the term can be negative or I have to try to put the value of in the form of sum of squares e.g., and then decide? and what is the relation between the positive definiteness of a matrix and its determinant?",X^TMX X^T=[x_1   x_2   x_3] X^TMX = ax_1^2 + bx_1*x_2+ ........ bx_1*x_2 X^TMX ()^2+()^2+.....,['matrices']
47,Gradient of $M \mapsto x^T M x$,Gradient of,M \mapsto x^T M x,"Suppose we have a linear scalar field $f : \mathbb{R}^{n \times m} \to \mathbb{R}$ defined by $$ f(M) := x^T M x $$ where $x \in \mathbb{R}^n$ . What is the gradient of $f(M)$ with respect to $M$ ? I think it is $xx^T$ , but why?","Suppose we have a linear scalar field defined by where . What is the gradient of with respect to ? I think it is , but why?",f : \mathbb{R}^{n \times m} \to \mathbb{R}  f(M) := x^T M x  x \in \mathbb{R}^n f(M) M xx^T,"['matrices', 'derivatives']"
48,how many unique patterns exist for a $N\times N$ grid,how many unique patterns exist for a  grid,N\times N,"I'm trying to figure out if there is a way to determine how many unique patterns exist for a given $N\times N$ grid if you choose N points on the grid. For example, for a $2\times 2$ grid we can get two unique patterns from the six possible combinations. The rest are just rotations and mirrors of the two unique patterns below [x] [x] [ ] [ ] and [x] [ ] [ ] [x] Is there a mathematical way of determining a unique number of patterns for a NxN grid where N=3,4,5,6,7,8? I figured for a 3x3, there are 14 unique patterns for picking 3 random points on the grid, but it gets tedious after that. N: N^2  : N^2 Choose N Unique pattern 2 4 6 2 3 9 84 14 4 16 1820 ???? 5 25 53130 ???? 6 36 1947792 ???? 7 49 85900584 ????","I'm trying to figure out if there is a way to determine how many unique patterns exist for a given $N\times N$ grid if you choose N points on the grid. For example, for a $2\times 2$ grid we can get two unique patterns from the six possible combinations. The rest are just rotations and mirrors of the two unique patterns below [x] [x] [ ] [ ] and [x] [ ] [ ] [x] Is there a mathematical way of determining a unique number of patterns for a NxN grid where N=3,4,5,6,7,8? I figured for a 3x3, there are 14 unique patterns for picking 3 random points on the grid, but it gets tedious after that. N: N^2  : N^2 Choose N Unique pattern 2 4 6 2 3 9 84 14 4 16 1820 ???? 5 25 53130 ???? 6 36 1947792 ???? 7 49 85900584 ????",,"['matrices', 'combinations']"
49,Significance of eigenvalue,Significance of eigenvalue,,"When I represent a graph with a matrix and calculate its eigenvalues what does it signify?  I mean, what will spectral analysis of a graph tell me?","When I represent a graph with a matrix and calculate its eigenvalues what does it signify?  I mean, what will spectral analysis of a graph tell me?",,"['matrices', 'graph-theory', 'soft-question', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
50,What are eigenvalues of higher order finite differences matrices?,What are eigenvalues of higher order finite differences matrices?,,"I know (at first empirically, then read somewhere) that for for second order finite differences matrices like $$\begin{pmatrix} -2&1&0&0&0\\ 1&-2&1&0&0\\ 0&1&-2&1&0\\ 0&0&1&-2&1\\ 0&0&0&1&-2 \end{pmatrix},$$ and other larger and smaller $n\times n$ matrices with $(1,-2,1)$ on their diagonal have eigenvalues with the following analytical expression: $$\lambda_k=-4\sin^2\left(\frac\pi{n+1}\frac k2\right).$$ I'm now interested in higher order finite differences. For example, for 4th order the matrix would have $\left(-\frac1{12},\frac43,-\frac52,\frac43,-\frac12\right)$ on diagonal, and 6th order would have $\left(\frac1{90},-\frac3{20},\frac32,-\frac{49}{18},\frac32,-\frac3{20},\frac1{90}\right)$. Here's a plot of (numerically computed) spectra of 2nd, 4th, 6th, 8th, 10th and 20th orders $100\times 100$ matrices (the higher the order, the lower the curve) , and their limit spectrum for continuous operator (black curve): Is there some analytical expression for spectra of such matrices? Is there some general result for $q$th order finite differences $n\times n$ matrix? How are such formulas found?","I know (at first empirically, then read somewhere) that for for second order finite differences matrices like $$\begin{pmatrix} -2&1&0&0&0\\ 1&-2&1&0&0\\ 0&1&-2&1&0\\ 0&0&1&-2&1\\ 0&0&0&1&-2 \end{pmatrix},$$ and other larger and smaller $n\times n$ matrices with $(1,-2,1)$ on their diagonal have eigenvalues with the following analytical expression: $$\lambda_k=-4\sin^2\left(\frac\pi{n+1}\frac k2\right).$$ I'm now interested in higher order finite differences. For example, for 4th order the matrix would have $\left(-\frac1{12},\frac43,-\frac52,\frac43,-\frac12\right)$ on diagonal, and 6th order would have $\left(\frac1{90},-\frac3{20},\frac32,-\frac{49}{18},\frac32,-\frac3{20},\frac1{90}\right)$. Here's a plot of (numerically computed) spectra of 2nd, 4th, 6th, 8th, 10th and 20th orders $100\times 100$ matrices (the higher the order, the lower the curve) , and their limit spectrum for continuous operator (black curve): Is there some analytical expression for spectra of such matrices? Is there some general result for $q$th order finite differences $n\times n$ matrix? How are such formulas found?",,"['matrices', 'eigenvalues-eigenvectors']"
51,Using quaternions instead of 4x4 matrices for transformations,Using quaternions instead of 4x4 matrices for transformations,,"I'm interested in implementing a clean solution providing an alternative to 4x4 matrices for 3D transformation. Quaternions provide the equivalent of rotation, but no translation. Therefore, in addition to a Quaternion, you need an additional vector of translations $(t_x,t_y,t_z)$. I have always seen it stated that you need 12 values for the matrix representation, and only 7 for the quaternion-based representation. What I don't understand is how to manipulate the translation values. For rotation of a quaternion, no problem. For a vector $v$, an axis vector $x$, and an angle $a$: $$q = \cos\left(\frac{a}{2}\right) + x \cdot \sin\left(\frac{a}{2}\right)$$ To rotate the vector: $$v' = qvq^{-1}$$ For multiple rotations, you can apply the transformations to the quaternion, and only when you have the final rotation do you have to apply it to the data. This is why matrix transformation is so nice in 3d graphics systems. Ok, so now if translation enters into it, what do I do? A given vector transformation is: $$T = (t_x,t_y,t_z)$$ $$v' = qvq^{-1} + T$$ If I want to apply a rotation and translation operation to this, I would have to modify $T$ and $q$. What should the result be?","I'm interested in implementing a clean solution providing an alternative to 4x4 matrices for 3D transformation. Quaternions provide the equivalent of rotation, but no translation. Therefore, in addition to a Quaternion, you need an additional vector of translations $(t_x,t_y,t_z)$. I have always seen it stated that you need 12 values for the matrix representation, and only 7 for the quaternion-based representation. What I don't understand is how to manipulate the translation values. For rotation of a quaternion, no problem. For a vector $v$, an axis vector $x$, and an angle $a$: $$q = \cos\left(\frac{a}{2}\right) + x \cdot \sin\left(\frac{a}{2}\right)$$ To rotate the vector: $$v' = qvq^{-1}$$ For multiple rotations, you can apply the transformations to the quaternion, and only when you have the final rotation do you have to apply it to the data. This is why matrix transformation is so nice in 3d graphics systems. Ok, so now if translation enters into it, what do I do? A given vector transformation is: $$T = (t_x,t_y,t_z)$$ $$v' = qvq^{-1} + T$$ If I want to apply a rotation and translation operation to this, I would have to modify $T$ and $q$. What should the result be?",,"['matrices', 'transformation', 'quaternions']"
52,Proof of Vandermonde Matrix Inverse Formula,Proof of Vandermonde Matrix Inverse Formula,,"I'm working through Exercise 40 from section 1.2.3 of Knuth's The Art of Computer Programming volume 1, but am finding myself unable to produce a rigorous proof, and the one here is suspect and not quite clear enough (for me, at least) in some of the steps; in particular, how to ""[identify] the $k$th order coefficient in [the] two polynomials."" The problem is this: given a Vandermonde matrix $[x_j^i]_n$, show that the inverse is given by $$ [b_{ij}]_n = \left[ \frac{    \sum_{\substack{1 \leq k_1 < \dotsc < k_{n-j} \leq n\\k_1,\dotsc,k_{n-j} \neq i}} (-1)^{j-1} x_{k_1} \dotsc x_{k_{n-j}} }{    x_i \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_i) } \right]_n\text{.} $$ The author gives a hint by stating that the sum in the above numerator is just the coefficient of $x^{j-1}$ in the polynomial $(x_1-x)\dotsc(x_n-x)/(x_i-x)$; and he gives an intermediate result showing the explicit multiplication of the matrix and its inverse as $$    \sum_{1 \leq t \leq n}b_{it}x_j^t    =    \frac{       x_j \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_j)    }{       x_i \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_i)    }    =    \delta_{ij}\text{.} $$ The only other hint as to the type of solution he was expecting is a reference to A. de Moivre's The Doctrine of Chances , 2nd edition, pp. 197-199, which deals with polynomial recurrence relations and difference products (available here ). At the least, I was just hoping someone could either verify the proof is correct at proofwiki and possibly fill in exactly how one identifies the $k$th order coefficient in the proof; or perhaps explain a proof strategy as to what steps to take where the intermediate result is obtained at some point before the final result. Thanks so much for any help.","I'm working through Exercise 40 from section 1.2.3 of Knuth's The Art of Computer Programming volume 1, but am finding myself unable to produce a rigorous proof, and the one here is suspect and not quite clear enough (for me, at least) in some of the steps; in particular, how to ""[identify] the $k$th order coefficient in [the] two polynomials."" The problem is this: given a Vandermonde matrix $[x_j^i]_n$, show that the inverse is given by $$ [b_{ij}]_n = \left[ \frac{    \sum_{\substack{1 \leq k_1 < \dotsc < k_{n-j} \leq n\\k_1,\dotsc,k_{n-j} \neq i}} (-1)^{j-1} x_{k_1} \dotsc x_{k_{n-j}} }{    x_i \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_i) } \right]_n\text{.} $$ The author gives a hint by stating that the sum in the above numerator is just the coefficient of $x^{j-1}$ in the polynomial $(x_1-x)\dotsc(x_n-x)/(x_i-x)$; and he gives an intermediate result showing the explicit multiplication of the matrix and its inverse as $$    \sum_{1 \leq t \leq n}b_{it}x_j^t    =    \frac{       x_j \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_j)    }{       x_i \prod_{\substack{1 \leq k \leq n\\k \neq i}} (x_k - x_i)    }    =    \delta_{ij}\text{.} $$ The only other hint as to the type of solution he was expecting is a reference to A. de Moivre's The Doctrine of Chances , 2nd edition, pp. 197-199, which deals with polynomial recurrence relations and difference products (available here ). At the least, I was just hoping someone could either verify the proof is correct at proofwiki and possibly fill in exactly how one identifies the $k$th order coefficient in the proof; or perhaps explain a proof strategy as to what steps to take where the intermediate result is obtained at some point before the final result. Thanks so much for any help.",,"['matrices', 'proof-verification', 'inverse']"
53,The infinite-dimensional limit of sequence of solutions of linear equations when the number of equations goes to infinity,The infinite-dimensional limit of sequence of solutions of linear equations when the number of equations goes to infinity,,"Suppose we have an infinite-dimensional real vector $y=(y_1,...)$. Suppose we have an infinite-dimensional real matrix $C=(c_{ij})$, $i,j\in\mathbb{N}$. Let $C^k$ be a submatrix of $C$, $C^k=(c_{ij})_{i,j=1,k}$ and $y_k$ a subvector of $y$, $y^k=(y_1,...,y_k)$. Define $$\theta^k=C_k^{-1}y^k$$ My question (which probably is too general) is what conditions should $C$ and $y$ satisfy so that pointwise limits $$\lim_{k\to\infty}\theta^k_i$$ exist? I got a feeling that this could be easily solved by applying theory of linear operators, but I cannot figure out how to reformulate the problem. To make this question less general we can assume that $y\in\ell_2$ and $C_k$ is symmetric positive-definite matrix for each $k$. This question is related to this one I've asked on mathoverflow. Update @fedja below produces a possible sketch of a proof. It requires though that $\|C_k^{-1}\|$ is bounded sequence (take the matrix norm $\|\|_2$). If we suppose that $C$ is a linear operator in $\ell_2$, does the property that each submatrix $C_k$ is positive-definite ensures that the sequence $\|C_k^{-1}\|$ is bounded? This question can be further rephrased the following way. For symmetric positive-definite matrix $A$ denote its minimal and maximal eigen values by $\lambda_{\min}$ and $\lambda_{\max}$. Then $\|A\|_2=\lambda_{\max}$ and $\|A^{-1}\|_2=\lambda_{\min}^{-1}$. With this in mind the previous question is identical whether $\lambda_{min}(C_k)$ is bounded away from zero.","Suppose we have an infinite-dimensional real vector $y=(y_1,...)$. Suppose we have an infinite-dimensional real matrix $C=(c_{ij})$, $i,j\in\mathbb{N}$. Let $C^k$ be a submatrix of $C$, $C^k=(c_{ij})_{i,j=1,k}$ and $y_k$ a subvector of $y$, $y^k=(y_1,...,y_k)$. Define $$\theta^k=C_k^{-1}y^k$$ My question (which probably is too general) is what conditions should $C$ and $y$ satisfy so that pointwise limits $$\lim_{k\to\infty}\theta^k_i$$ exist? I got a feeling that this could be easily solved by applying theory of linear operators, but I cannot figure out how to reformulate the problem. To make this question less general we can assume that $y\in\ell_2$ and $C_k$ is symmetric positive-definite matrix for each $k$. This question is related to this one I've asked on mathoverflow. Update @fedja below produces a possible sketch of a proof. It requires though that $\|C_k^{-1}\|$ is bounded sequence (take the matrix norm $\|\|_2$). If we suppose that $C$ is a linear operator in $\ell_2$, does the property that each submatrix $C_k$ is positive-definite ensures that the sequence $\|C_k^{-1}\|$ is bounded? This question can be further rephrased the following way. For symmetric positive-definite matrix $A$ denote its minimal and maximal eigen values by $\lambda_{\min}$ and $\lambda_{\max}$. Then $\|A\|_2=\lambda_{\max}$ and $\|A^{-1}\|_2=\lambda_{\min}^{-1}$. With this in mind the previous question is identical whether $\lambda_{min}(C_k)$ is bounded away from zero.",,"['matrices', 'functional-analysis', 'limits']"
54,Row-normalized and column-normalized matrix notation,Row-normalized and column-normalized matrix notation,,"I'm searching for the mathematical, algebraic notations of a row-normalized and column-normalized matrix. For example, let us consider the following matrix A: $$ A = \begin{pmatrix} 2 & 7 \\ 4 & 3 \end{pmatrix} $$ What is the mathematical notation of its corresponding row-normalized matrix? $$ \begin{pmatrix} 2/9 & 7/9 \\ 4/7 & 3/7 \end{pmatrix} $$ What is the mathematical notation of its corresponding column-normalized matrix? $$ \begin{pmatrix} 2/6 & 7/10 \\ 4/6 & 3/10 \end{pmatrix} $$ Best regards.","I'm searching for the mathematical, algebraic notations of a row-normalized and column-normalized matrix. For example, let us consider the following matrix A: $$ A = \begin{pmatrix} 2 & 7 \\ 4 & 3 \end{pmatrix} $$ What is the mathematical notation of its corresponding row-normalized matrix? $$ \begin{pmatrix} 2/9 & 7/9 \\ 4/7 & 3/7 \end{pmatrix} $$ What is the mathematical notation of its corresponding column-normalized matrix? $$ \begin{pmatrix} 2/6 & 7/10 \\ 4/6 & 3/10 \end{pmatrix} $$ Best regards.",,"['matrices', 'notation']"
55,How to solve matrix equation $AX+XB=C$ for $X$,How to solve matrix equation  for,AX+XB=C X,"How does one solve the matrix equation $AX+XB=C$ for $X$? It doesn't seem too difficult. I tried many times but failed. I'm an adult student... I am now vexed about Gilbert Strang - An Introduction to Linear Algebra. I don't even understand a single word in Wikipedia: Sylvester equation . If you have ever use some nice workable materials or lecture notes? You can generously upload and share the links of the lecture notes and assignments. Different subjects/ topics are welcome, as long as you deem they are nice and workable. The problem origins from a system of diff equation, using undetermined coefficients (matrix) to find the particular solution. Try $y_p=X\begin{pmatrix}   e^{\alpha t} \\ e^{\beta t} \end{pmatrix}$ $\dot{y}+Ay=C\begin{pmatrix} e^{\alpha t} \\ e^{\beta t}\end{pmatrix}$ $\dot{y_p}=X\begin{pmatrix} \alpha & 0 \\ 0 & \beta \end{pmatrix} \begin{pmatrix}e^{\alpha t} \\ e^{\beta t}\end{pmatrix}$ substitute $\dot{y_p}$ and $y_p$ into the original differential equation.. $X\begin{pmatrix} \alpha & 0 \\ 0 & \beta \end{pmatrix}+AX=C$","How does one solve the matrix equation $AX+XB=C$ for $X$? It doesn't seem too difficult. I tried many times but failed. I'm an adult student... I am now vexed about Gilbert Strang - An Introduction to Linear Algebra. I don't even understand a single word in Wikipedia: Sylvester equation . If you have ever use some nice workable materials or lecture notes? You can generously upload and share the links of the lecture notes and assignments. Different subjects/ topics are welcome, as long as you deem they are nice and workable. The problem origins from a system of diff equation, using undetermined coefficients (matrix) to find the particular solution. Try $y_p=X\begin{pmatrix}   e^{\alpha t} \\ e^{\beta t} \end{pmatrix}$ $\dot{y}+Ay=C\begin{pmatrix} e^{\alpha t} \\ e^{\beta t}\end{pmatrix}$ $\dot{y_p}=X\begin{pmatrix} \alpha & 0 \\ 0 & \beta \end{pmatrix} \begin{pmatrix}e^{\alpha t} \\ e^{\beta t}\end{pmatrix}$ substitute $\dot{y_p}$ and $y_p$ into the original differential equation.. $X\begin{pmatrix} \alpha & 0 \\ 0 & \beta \end{pmatrix}+AX=C$",,['matrices']
56,Proof - raising adjacency matrix to $n$-th power gives $n$-length walks between two vertices,Proof - raising adjacency matrix to -th power gives -length walks between two vertices,n n,"I came across the formula to find the number of walks of length $n$ between two vertices by raising the adjacency matrix of their graph to the $n$-th power. I took me quite some time to understand why it actually works. I thought it would be useful to write the proof by induction for this in my own words. Theorem: Raising an adjacency matrix $A$ of simple graph $G$ to the $n$-th power gives the number of $n$-length walks between two vertices $v_i$, $v_j$ of $G$ in the resulting matrix. Proof by induction: Let $P(n)$ be the predicate that the theorem is true for $n$. We let $F^{(n)}_{ij}$ be the number of $n$-length walks between vertex $v_i$ and $v_j$. $P(n)$ is then the predicate that $F^{(n)}_{ij} = A^n_{ij}$. We proceed by induction on $n$. Base case: $P(1)$ Case 1: $F^{(1)}_{ij} = A^{(1)}_{ij} = 1$ if $\{v_i, v_j\} \in E$, so there is is a walk of length $1$ between $v_i$, $v_j$. Case 2: $F^{(1)}_{ij} = A^{(1)}_{ij} = 0$ if $\{v_i, v_j\} \notin E$, so there can't be any walk of length $1$ between $v_i$ and $v_j$. In both cases $F^{(1)}_{i j} = A^{(1)}_{ij}$ holds, hence $P(1)$ is true. Inductive step: $P(n+1)$ For purpose of induction, we assume $P(n)$ is true, that is $F^{(n)}_{i j} = A^{(n)}_{ij}$ holds for $n$. We can express a walk of length $n+1$ from $v_i$ to $v_j$ as a $n$-length walk from $v_i$ to $v_k$ and a walk of length 1 from $v_k$ to $v_j$. That means, the number of $n+1$-length walks from $v_i$ to $v_j$ is the sum over all walks from $v_i$ to $v_k$ times the number of ways to walk in one step from $v_k$ to $v_j$. Thus: $$F^{(n+1)}_{ij} = \sum_{k=1}^{|V|} A_{kj}F^{(n)}_{ik} = \sum_{k=1}^{|V|} A_{kj}A^{(n)}_{ik}$$ Which is the formula for the dot-product, used in matrix multplications. Any feedback appreciated.","I came across the formula to find the number of walks of length $n$ between two vertices by raising the adjacency matrix of their graph to the $n$-th power. I took me quite some time to understand why it actually works. I thought it would be useful to write the proof by induction for this in my own words. Theorem: Raising an adjacency matrix $A$ of simple graph $G$ to the $n$-th power gives the number of $n$-length walks between two vertices $v_i$, $v_j$ of $G$ in the resulting matrix. Proof by induction: Let $P(n)$ be the predicate that the theorem is true for $n$. We let $F^{(n)}_{ij}$ be the number of $n$-length walks between vertex $v_i$ and $v_j$. $P(n)$ is then the predicate that $F^{(n)}_{ij} = A^n_{ij}$. We proceed by induction on $n$. Base case: $P(1)$ Case 1: $F^{(1)}_{ij} = A^{(1)}_{ij} = 1$ if $\{v_i, v_j\} \in E$, so there is is a walk of length $1$ between $v_i$, $v_j$. Case 2: $F^{(1)}_{ij} = A^{(1)}_{ij} = 0$ if $\{v_i, v_j\} \notin E$, so there can't be any walk of length $1$ between $v_i$ and $v_j$. In both cases $F^{(1)}_{i j} = A^{(1)}_{ij}$ holds, hence $P(1)$ is true. Inductive step: $P(n+1)$ For purpose of induction, we assume $P(n)$ is true, that is $F^{(n)}_{i j} = A^{(n)}_{ij}$ holds for $n$. We can express a walk of length $n+1$ from $v_i$ to $v_j$ as a $n$-length walk from $v_i$ to $v_k$ and a walk of length 1 from $v_k$ to $v_j$. That means, the number of $n+1$-length walks from $v_i$ to $v_j$ is the sum over all walks from $v_i$ to $v_k$ times the number of ways to walk in one step from $v_k$ to $v_j$. Thus: $$F^{(n+1)}_{ij} = \sum_{k=1}^{|V|} A_{kj}F^{(n)}_{ik} = \sum_{k=1}^{|V|} A_{kj}A^{(n)}_{ik}$$ Which is the formula for the dot-product, used in matrix multplications. Any feedback appreciated.",,"['matrices', 'discrete-mathematics', 'proof-verification', 'graph-theory', 'adjacency-matrix']"
57,Closeness the eigenvalues of a Matrix to a list of numbers,Closeness the eigenvalues of a Matrix to a list of numbers,,"Let $A,B\in M_n(\mathbb R)$, and $\ell$ a list of $n$ numbers sorted in some order (say, decreasing). Let $\lambda_i(A)$ be the $i$th eigenvalue of $A$ with respect to the chosen order. Finally, let $d(A,\ell) = ||\lambda(A)-\ell||_1 = \sum_{i = 1}^n |\lambda_i(A)-\ell_i|$ be the taxicab metric . My question is as follows: Given, $A,B,\ell$, is it possible to determine which of $d(A,\ell)$ or $d(B,\ell)$ is smaller without computing the eigenvalues of $A$ and $B$? Note : The $d(A,\ell)$ can also be any other meaningful metric that determines the matrix which its eigenvalues are closer to the list $\ell$ Update: My solution (Not working!) Let $\lambda$ and $\beta$ be the eigenvalues of $A$ and $B$ respectively. Clearly $det(A-\lambda I) = 0$ and $det(B-\beta I) = 0$ and $P_n(\lambda), P_n(\beta)$ are characteristic polynomial of $A,B$. Let sort the eigenvalues $\beta = \{\beta_1,\beta_2,...\beta_n \}$ and $\lambda = \{\lambda_1,\lambda_2,...\lambda_n \}$ in chosen order (say, decreasing). Let $\forall \ell_i \in \ell, i\in\{1,2,...,n\},$ if $\sum_{i=1}^{n} (det(A-\ell_i I)-det(B-\ell_i I)) > 0$ then eigenvalues of $B$ are closer to $\ell$ and if   $\sum_{i=1}^{n} (det(A-\ell_i I)-det(B-\ell_i I)) < 0$ then eigenvalues of $A$ are closer to $\ell$. Notice, since we don't actually need to calculate the eigenvalues of $A, B$, we don't need to solve the characteristic polynomial of $A, B$, It is enough to form the $det(A-\ell_i I)-det(B-\ell_i I)$, where $\ell_i$ is known then continue as described. Is this solution right? and is it easier to find determinant of $n$ matrices than finding the eigenvalues of a $n \times n$ matrix?","Let $A,B\in M_n(\mathbb R)$, and $\ell$ a list of $n$ numbers sorted in some order (say, decreasing). Let $\lambda_i(A)$ be the $i$th eigenvalue of $A$ with respect to the chosen order. Finally, let $d(A,\ell) = ||\lambda(A)-\ell||_1 = \sum_{i = 1}^n |\lambda_i(A)-\ell_i|$ be the taxicab metric . My question is as follows: Given, $A,B,\ell$, is it possible to determine which of $d(A,\ell)$ or $d(B,\ell)$ is smaller without computing the eigenvalues of $A$ and $B$? Note : The $d(A,\ell)$ can also be any other meaningful metric that determines the matrix which its eigenvalues are closer to the list $\ell$ Update: My solution (Not working!) Let $\lambda$ and $\beta$ be the eigenvalues of $A$ and $B$ respectively. Clearly $det(A-\lambda I) = 0$ and $det(B-\beta I) = 0$ and $P_n(\lambda), P_n(\beta)$ are characteristic polynomial of $A,B$. Let sort the eigenvalues $\beta = \{\beta_1,\beta_2,...\beta_n \}$ and $\lambda = \{\lambda_1,\lambda_2,...\lambda_n \}$ in chosen order (say, decreasing). Let $\forall \ell_i \in \ell, i\in\{1,2,...,n\},$ if $\sum_{i=1}^{n} (det(A-\ell_i I)-det(B-\ell_i I)) > 0$ then eigenvalues of $B$ are closer to $\ell$ and if   $\sum_{i=1}^{n} (det(A-\ell_i I)-det(B-\ell_i I)) < 0$ then eigenvalues of $A$ are closer to $\ell$. Notice, since we don't actually need to calculate the eigenvalues of $A, B$, we don't need to solve the characteristic polynomial of $A, B$, It is enough to form the $det(A-\ell_i I)-det(B-\ell_i I)$, where $\ell_i$ is known then continue as described. Is this solution right? and is it easier to find determinant of $n$ matrices than finding the eigenvalues of a $n \times n$ matrix?",,"['matrices', 'eigenvalues-eigenvectors']"
58,Representation for Vandermonde's permanent,Representation for Vandermonde's permanent,,"Permanent of a matrix A = $\|a_{i,j}\|_{i,j=1}^{n}$ is defined as $$    \mathrm{Perm}(A) = \sum\limits_{\sigma \in S_{n}} a_{1,\sigma_{1}},\ldots,a_{n,\sigma_{n}} $$ Is there some representation for permanent of Vandermonde's matrix similar to its determinant?  Is there some numerical methods for computing Vandermonde's permanent that uses the particularity of Vandermonde's matrix?","Permanent of a matrix A = $\|a_{i,j}\|_{i,j=1}^{n}$ is defined as $$    \mathrm{Perm}(A) = \sum\limits_{\sigma \in S_{n}} a_{1,\sigma_{1}},\ldots,a_{n,\sigma_{n}} $$ Is there some representation for permanent of Vandermonde's matrix similar to its determinant?  Is there some numerical methods for computing Vandermonde's permanent that uses the particularity of Vandermonde's matrix?",,['matrices']
59,Matrix raised to 14th power,Matrix raised to 14th power,,"Calculate $\left(\begin{matrix} 6&1&0\\0&6&1\\0&0&6\end{matrix}\right)^{14}$ Whould I do it one by one, and then find a pattern? I sense $6^{14}$ on the diagonal, and zeroes in the ""lower triangle"", but the ""upper triangle"" I'm not sure. Was thinking $14 \cdot 6^{13} $ but that's not correct.","Calculate $\left(\begin{matrix} 6&1&0\\0&6&1\\0&0&6\end{matrix}\right)^{14}$ Whould I do it one by one, and then find a pattern? I sense $6^{14}$ on the diagonal, and zeroes in the ""lower triangle"", but the ""upper triangle"" I'm not sure. Was thinking $14 \cdot 6^{13} $ but that's not correct.",,['matrices']
60,"If a matrix is triangular, is there a quicker way to tell if it is can be diagonalized?","If a matrix is triangular, is there a quicker way to tell if it is can be diagonalized?",,"I hope it is alright to ask something like this here, I am having trouble keeping up with all the special cases and my book is being kind of vague. I know how to do the standard method of finding diagonal matrices, but I know that triangular matrices are special and my book makes a connection, but it is hard to tell what it is. Thanks.","I hope it is alright to ask something like this here, I am having trouble keeping up with all the special cases and my book is being kind of vague. I know how to do the standard method of finding diagonal matrices, but I know that triangular matrices are special and my book makes a connection, but it is hard to tell what it is. Thanks.",,['matrices']
61,Raising a matrix to the infinite power,Raising a matrix to the infinite power,,"How do I raise a matrix to the infinite power? I know that the main method for doing this is by diagonalizing the matrix, but what if I can't? For example, let's say I have the matrix \begin{bmatrix}0&0&0&0&0\\2/3&0&0&0&0\\1/3&0&1&0&0\\0&3/7&0&1&0\\0&4/7&0&0&1\end{bmatrix} You can see that when I try diaganolizing the matrix in Mathematica, the eigenvector matrix is singular, so I'm unable to take its inverse. However, I know that when I raise this matrix to the power of infinity, I know I get the following matrix \begin{bmatrix}0&0&0&0&0\\0&0&0&0&0\\1/3&0&1&0&0\\2/7&3/7&0&1&0\\8/21&4/7&0&0&1\end{bmatrix} Is there any general algorithm or formula or steps I can take to get there?","How do I raise a matrix to the infinite power? I know that the main method for doing this is by diagonalizing the matrix, but what if I can't? For example, let's say I have the matrix You can see that when I try diaganolizing the matrix in Mathematica, the eigenvector matrix is singular, so I'm unable to take its inverse. However, I know that when I raise this matrix to the power of infinity, I know I get the following matrix Is there any general algorithm or formula or steps I can take to get there?",\begin{bmatrix}0&0&0&0&0\\2/3&0&0&0&0\\1/3&0&1&0&0\\0&3/7&0&1&0\\0&4/7&0&0&1\end{bmatrix} \begin{bmatrix}0&0&0&0&0\\0&0&0&0&0\\1/3&0&1&0&0\\2/7&3/7&0&1&0\\8/21&4/7&0&0&1\end{bmatrix},"['matrices', 'limits', 'matrix-calculus']"
62,Relationship between eigenvalues of a matrix and its square,Relationship between eigenvalues of a matrix and its square,,Are there any general relations between the eigenvalues of a matrix $M$ and those of $M^2$?,Are there any general relations between the eigenvalues of a matrix $M$ and those of $M^2$?,,['matrices']
63,Is there any standard notation for specifying dimension of a matrix after the matrix symbol?,Is there any standard notation for specifying dimension of a matrix after the matrix symbol?,,"I want to explicitly specify dimension of matrices in some expressions, something like $$\boldsymbol{A}_{m \times n} \boldsymbol{B}_{n \times m} = \boldsymbol{C}_{m \times m} \, .$$ Is there any more or less standard notation for this? While this notation is generally unambiguous, I think it become ambiguous, for example, in the following case. Suppose that I have some column or row vector which is conjugate transpose of correspondingly row or column vector. If I write it in the following way  $$\boldsymbol{A}_{n \times 1}^{\dagger} \, ,$$ we can have 2 different interpretations: matrix $\boldsymbol{A}_{n \times 1}$ is $n \times 1$ column vector and by $\boldsymbol{A}_{n \times 1}^{\dagger}$ I'm referring to its conjugate transpose which is $1 \times n$ row vector; matrix $\boldsymbol{A}_{n \times 1}^{\dagger}$ itself is $n \times 1$ column vector which is conjugate transpose of $1 \times n$ row vector $\boldsymbol{A}$.","I want to explicitly specify dimension of matrices in some expressions, something like $$\boldsymbol{A}_{m \times n} \boldsymbol{B}_{n \times m} = \boldsymbol{C}_{m \times m} \, .$$ Is there any more or less standard notation for this? While this notation is generally unambiguous, I think it become ambiguous, for example, in the following case. Suppose that I have some column or row vector which is conjugate transpose of correspondingly row or column vector. If I write it in the following way  $$\boldsymbol{A}_{n \times 1}^{\dagger} \, ,$$ we can have 2 different interpretations: matrix $\boldsymbol{A}_{n \times 1}$ is $n \times 1$ column vector and by $\boldsymbol{A}_{n \times 1}^{\dagger}$ I'm referring to its conjugate transpose which is $1 \times n$ row vector; matrix $\boldsymbol{A}_{n \times 1}^{\dagger}$ itself is $n \times 1$ column vector which is conjugate transpose of $1 \times n$ row vector $\boldsymbol{A}$.",,"['matrices', 'notation']"
64,Inverse of $2 \times 2$ block matrices,Inverse of  block matrices,2 \times 2,"Let matrices $A, B, C, D$ be invertible. How can I find the inverse of the following block matrix? $$\begin{bmatrix} A & B \\ C & D \\ \end{bmatrix}$$ Thank you.",Let matrices be invertible. How can I find the inverse of the following block matrix? Thank you.,"A, B, C, D \begin{bmatrix} A & B \\ C & D \\ \end{bmatrix}","['matrices', 'inverse', 'block-matrices']"
65,Why is the matrix multiplication defined as it is? [duplicate],Why is the matrix multiplication defined as it is? [duplicate],,"This question already has answers here : Intuition behind Matrix Multiplication (14 answers) Closed 8 years ago . Matrix multiplication is defined as: Let $A$ be a $n \times m$ matrix and $B$ a $m\times p$ matrix, the product $AB$ is defined as a matrix of size $n\times p$ such that $(AB)_i,_j = \sum\limits_{k=1}^mA_i,_kB_k,_j$. For what good reason did mathematicians define it like this?","This question already has answers here : Intuition behind Matrix Multiplication (14 answers) Closed 8 years ago . Matrix multiplication is defined as: Let $A$ be a $n \times m$ matrix and $B$ a $m\times p$ matrix, the product $AB$ is defined as a matrix of size $n\times p$ such that $(AB)_i,_j = \sum\limits_{k=1}^mA_i,_kB_k,_j$. For what good reason did mathematicians define it like this?",,"['matrices', 'definition']"
66,Determinant of block matrix with singular blocks on the diagonal,Determinant of block matrix with singular blocks on the diagonal,,"Let $A$ and $D$ be square matrices, and let $B$ and $C$ be matrices of valid shapes to allow the formation of $$ M = \begin{bmatrix}     A & B \\     C & D \end{bmatrix}. $$ If $\det{A}\neq0$, we may use the Schur complement to express $\det{M}$ in terms of its constituent blocks as $$ \det{M} = \det{A}\cdot\det(D-CA^{-1}B), $$ and if $\det{D}\neq0$ we have in a similar fashion that $$ \det{M} = \det(A-BD^{-1}C)\cdot\det{D}. $$ My question: Does there exist a similar formula expressing $\det{M}$ in terms of its constituent blocks, that is valid in case $\det{A}=\det{D}=0$?","Let $A$ and $D$ be square matrices, and let $B$ and $C$ be matrices of valid shapes to allow the formation of $$ M = \begin{bmatrix}     A & B \\     C & D \end{bmatrix}. $$ If $\det{A}\neq0$, we may use the Schur complement to express $\det{M}$ in terms of its constituent blocks as $$ \det{M} = \det{A}\cdot\det(D-CA^{-1}B), $$ and if $\det{D}\neq0$ we have in a similar fashion that $$ \det{M} = \det(A-BD^{-1}C)\cdot\det{D}. $$ My question: Does there exist a similar formula expressing $\det{M}$ in terms of its constituent blocks, that is valid in case $\det{A}=\det{D}=0$?",,"['matrices', 'determinant', 'block-matrices', 'schur-complement']"
67,Binomial theorem for matrices,Binomial theorem for matrices,,Suppose we have $$(A + I)^n$$ where $A$ is matrix and $I$ is an identity matrix. Does the binomial theorem apply? I know the binomial theorem but not whether it is also applicable to matrices.,Suppose we have $$(A + I)^n$$ where $A$ is matrix and $I$ is an identity matrix. Does the binomial theorem apply? I know the binomial theorem but not whether it is also applicable to matrices.,,"['matrices', 'binomial-theorem']"
68,$A$ is invertible if and only if $A^t$ is invertible,is invertible if and only if  is invertible,A A^t,"I hate these ""easy"" proofs. They always slip under my radar. How do I show that a square matrix $A$ is invertible if and only if $A^t$ is invertible?","I hate these ""easy"" proofs. They always slip under my radar. How do I show that a square matrix $A$ is invertible if and only if $A^t$ is invertible?",,[]
69,Does the exponential of a matrix commute with the matrix?,Does the exponential of a matrix commute with the matrix?,,"Can someone give me an idea for the proof that for every $t\in \mathbb{C}$ we have $e^{tA}\cdot A = A \cdot e^{tA} =$ ? I couldn't find a counterexample, so my gues is, that it would be true, but I'm not sure even how to begin the proof.","Can someone give me an idea for the proof that for every $t\in \mathbb{C}$ we have $e^{tA}\cdot A = A \cdot e^{tA} =$ ? I couldn't find a counterexample, so my gues is, that it would be true, but I'm not sure even how to begin the proof.",,"['matrices', 'matrix-calculus', 'matrix-exponential', 'matrix-analysis']"
70,Subtract matrix from scalar,Subtract matrix from scalar,,Is this even possible? Since you can subtract on the right-hand side I think there must be a way to do it from left-hand side too. I would like to calculate this: 3 - [2 1] = ??,Is this even possible? Since you can subtract on the right-hand side I think there must be a way to do it from left-hand side too. I would like to calculate this: 3 - [2 1] = ??,,['matrices']
71,Intuition for connectedness of positive definite matrices,Intuition for connectedness of positive definite matrices,,"I have read, even on this website, about connectedness of positive definite matrices. The proof is given in the form of convex equation. I wonder how to understand it intutively. e.g., as an analogy, a parabola is convex and continuous, so, its connectedness is obvious. How to understand positive definite matrices on this line. At first thought, I felt, in the vecor space, the next placed matrix might not be positive definite, as we need all leading principal minors to be positive. I felt, maybe this condition would be satisfied only after certain distance in the vector space. So, I felt these matrices would be disconnected. But of course I was wrong. I wonder how to build intuition for connectedness along these lines. Thanks.","I have read, even on this website, about connectedness of positive definite matrices. The proof is given in the form of convex equation. I wonder how to understand it intutively. e.g., as an analogy, a parabola is convex and continuous, so, its connectedness is obvious. How to understand positive definite matrices on this line. At first thought, I felt, in the vecor space, the next placed matrix might not be positive definite, as we need all leading principal minors to be positive. I felt, maybe this condition would be satisfied only after certain distance in the vector space. So, I felt these matrices would be disconnected. But of course I was wrong. I wonder how to build intuition for connectedness along these lines. Thanks.",,"['matrices', 'positive-definite']"
72,summation of determinants of $3\times3$ matrices,summation of determinants of  matrices,3\times3,"I have an algebra problem but no idea how to solve it. The problem is: ""you can create 9! matrices the elements of which lie in a set $ \{1,2,3,...,9\} \subset \mathbb N$ so that their elements do not repeat, i.e. e.g. $$ \begin{pmatrix}1&2&9\\3&5&7\\6&4&8 \end{pmatrix} $$ Find the sum of the determinants of all these matrices."" Could you give me a hint how to solve it? Thank you.","I have an algebra problem but no idea how to solve it. The problem is: ""you can create 9! matrices the elements of which lie in a set $ \{1,2,3,...,9\} \subset \mathbb N$ so that their elements do not repeat, i.e. e.g. $$ \begin{pmatrix}1&2&9\\3&5&7\\6&4&8 \end{pmatrix} $$ Find the sum of the determinants of all these matrices."" Could you give me a hint how to solve it? Thank you.",,"['matrices', 'algebra-precalculus']"
73,Convex hull of orthogonal matrices,Convex hull of orthogonal matrices,,Where can I find the proof of the fact that the convex hull of the set of orthogonal matrices is the set of matrices with norm not greater than one? It is easy to show that a convex combination of orthogonal matrices has norm (I mean the norm as operators) not larger than $1$. The reverse seems quite tricky...,Where can I find the proof of the fact that the convex hull of the set of orthogonal matrices is the set of matrices with norm not greater than one? It is easy to show that a convex combination of orthogonal matrices has norm (I mean the norm as operators) not larger than $1$. The reverse seems quite tricky...,,"['matrices', 'convex-analysis', 'orthogonal-matrices', 'convex-hulls', 'spectral-norm']"
74,Smith normal form of a polynomial matrix,Smith normal form of a polynomial matrix,,I have the following matrix $$P(s) := \begin{bmatrix} s^2 & s-1 \\ s   & s^2 \end{bmatrix}$$ How does one compute the Smith normal form of this matrix? I can't quite grasp the algorithm.,I have the following matrix How does one compute the Smith normal form of this matrix? I can't quite grasp the algorithm.,"P(s) := \begin{bmatrix}
s^2 & s-1 \\
s   & s^2
\end{bmatrix}","['matrices', 'polynomials', 'control-theory', 'smith-normal-form']"
75,Inverse of a 3x3 block matrix,Inverse of a 3x3 block matrix,,I would like to get the inverse of a 3x3 (covariance) block matrix \begin{bmatrix}A&B&C\\B'&D&E\\C'&E'&F\end{bmatrix} where the prime ' indicates the transposition operator. Is there any general formula (or a way to solve this problem)? Thank you very much.,I would like to get the inverse of a 3x3 (covariance) block matrix where the prime ' indicates the transposition operator. Is there any general formula (or a way to solve this problem)? Thank you very much.,\begin{bmatrix}A&B&C\\B'&D&E\\C'&E'&F\end{bmatrix},"['matrices', 'inverse', 'block-matrices']"
76,"Finding square roots of a matrix of the form $A^\prime A$, where $A=\begin{bmatrix}I \\ a^{T}\end{bmatrix}$ for some column vector $a$","Finding square roots of a matrix of the form , where  for some column vector",A^\prime A A=\begin{bmatrix}I \\ a^{T}\end{bmatrix} a,"I have a matrix of quite special form: $$S=A'A$$ $$A=\begin{bmatrix}  1 &0  &\dots &0 \\   0 &1  &\dots &0 \\   0 &0  &\dots &1 \\   a_{1} &a_{2} &\dots &a_{n}  \end{bmatrix}$$ Thus $A$ is a $(n+1)\times n$ and $S$ is $n\times n$ symmetric matrix.  I am wondering if there is some name for the matrices like $A$.  I am implementing some numerical algorithm and I need to find inverses and square roots of very large matrices $S$. Since the matrix I am dealing with is of quite simple form, maybe there are some properties that would allow fast square root computations. Maybe some members of this community has some insight on how to make square root calculations faster. Does the matrix have a name I can research?","I have a matrix of quite special form: $$S=A'A$$ $$A=\begin{bmatrix}  1 &0  &\dots &0 \\   0 &1  &\dots &0 \\   0 &0  &\dots &1 \\   a_{1} &a_{2} &\dots &a_{n}  \end{bmatrix}$$ Thus $A$ is a $(n+1)\times n$ and $S$ is $n\times n$ symmetric matrix.  I am wondering if there is some name for the matrices like $A$.  I am implementing some numerical algorithm and I need to find inverses and square roots of very large matrices $S$. Since the matrix I am dealing with is of quite simple form, maybe there are some properties that would allow fast square root computations. Maybe some members of this community has some insight on how to make square root calculations faster. Does the matrix have a name I can research?",,"['matrices', 'terminology']"
77,"Determinant of $a_{i,j}=(x_i+y_j)^k$",Determinant of,"a_{i,j}=(x_i+y_j)^k","How can I find the determinant of the matrix $A\in\mathcal{M}_n(\mathbb{R})$ with coefficients $a_{i,j}=(x_i+y_j)^k,k<n$ ? All the $x_u,y_u$ are real numbers. Derivating won't help, and I didn't find any good way to simplify the problem.","How can I find the determinant of the matrix $A\in\mathcal{M}_n(\mathbb{R})$ with coefficients $a_{i,j}=(x_i+y_j)^k,k<n$ ? All the $x_u,y_u$ are real numbers. Derivating won't help, and I didn't find any good way to simplify the problem.",,"['matrices', 'determinant']"
78,How to find closest positive definite matrix of non-symmetric matrix?,How to find closest positive definite matrix of non-symmetric matrix?,,I have a matrix $A$ given and I want to find the matrix $B$ which is closest to $A$ in the Frobenius norm and is positive definite. $B$ does not need to be symmetric. I found a lot of solutions if the input matrix $A$ is symmetric. Are they any for a non-symmetric matrix $A$ ? Is it possible to rewrite the problem as a minimization of a convex problem?,I have a matrix given and I want to find the matrix which is closest to in the Frobenius norm and is positive definite. does not need to be symmetric. I found a lot of solutions if the input matrix is symmetric. Are they any for a non-symmetric matrix ? Is it possible to rewrite the problem as a minimization of a convex problem?,A B A B A A,"['matrices', 'convex-optimization']"
79,Under what conditions is $AA^T$ invertible?,Under what conditions is  invertible?,AA^T,"Given a matrix $A$ with dimensions $m \times n$, is $B=AA^T$ invertible if and only if the rows of $A$ are linearly independent? So far, I've tried writing A as row vectors, $$A = \begin{bmatrix}v_1\\ v_2\\ \vdots \\ v_m\end{bmatrix}$$ where $B_{i,j} = (v_i v_j) $ The Wikipedia page on the Gram Determinant suggests that the condition I stated above is necessary and sufficient, but does not provide a proof.","Given a matrix $A$ with dimensions $m \times n$, is $B=AA^T$ invertible if and only if the rows of $A$ are linearly independent? So far, I've tried writing A as row vectors, $$A = \begin{bmatrix}v_1\\ v_2\\ \vdots \\ v_m\end{bmatrix}$$ where $B_{i,j} = (v_i v_j) $ The Wikipedia page on the Gram Determinant suggests that the condition I stated above is necessary and sufficient, but does not provide a proof.",,['matrices']
80,How does one prove that the spectral norm is less than or equal to the Frobenius norm?,How does one prove that the spectral norm is less than or equal to the Frobenius norm?,,"How does one prove that the spectral norm is less than or equal to the Frobenius norm? The given definition for the spectral norm of $A$ is the square root of the largest eigenvalue of $A*A$. I don't know how to use that. Is there another definition I could use? We also have $\displaystyle{\max\frac{\|Ax\|_p}{\|x\|_p}}$, but if $p=2$ it's the Frobenius norm, right?","How does one prove that the spectral norm is less than or equal to the Frobenius norm? The given definition for the spectral norm of $A$ is the square root of the largest eigenvalue of $A*A$. I don't know how to use that. Is there another definition I could use? We also have $\displaystyle{\max\frac{\|Ax\|_p}{\|x\|_p}}$, but if $p=2$ it's the Frobenius norm, right?",,"['matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
81,"Suppose $e^A = A$, prove that $A$ is diagonalizable","Suppose , prove that  is diagonalizable",e^A = A A,"Suppose $e^A = A$, prove  that $A$ is diagonalizable, where A is a matrix. What I have tried to do is write $A= D + N$,  where $D$ is diagonalizable, $N$ is nilpotent and $DN = ND$. Since $N$ is nilpotent, there exist a minimal $n$ such that $N^n=0$. Then $e^A=e^{D+N}=e^De^N=e^D(I+N+\frac{N^2}{2}+...+\frac{N^{n-1}}{(n-1)!})=A=D+N$. If we times $N^{n-1}$ on both side, then what remain is $e^DN^{n-1}=DN^{n-1}$. And then I don't know how to carry on. Please help! Do I need a new method to do this question? Thanks a lot!","Suppose $e^A = A$, prove  that $A$ is diagonalizable, where A is a matrix. What I have tried to do is write $A= D + N$,  where $D$ is diagonalizable, $N$ is nilpotent and $DN = ND$. Since $N$ is nilpotent, there exist a minimal $n$ such that $N^n=0$. Then $e^A=e^{D+N}=e^De^N=e^D(I+N+\frac{N^2}{2}+...+\frac{N^{n-1}}{(n-1)!})=A=D+N$. If we times $N^{n-1}$ on both side, then what remain is $e^DN^{n-1}=DN^{n-1}$. And then I don't know how to carry on. Please help! Do I need a new method to do this question? Thanks a lot!",,"['matrices', 'diagonalization']"
82,Proof of matrix norm property: submultiplicativity,Proof of matrix norm property: submultiplicativity,,"I've been searching for the definition of the submultiplicative (I think it has multiple names from what I've seen) property in proof form. Some books define it as part of the properties that define matrix norms, and some include it as an additional property. I still haven't been able to work it out for myself or find it anywhere. Let $A$ and $B$ be $n\times m$ and $m\times l$ matrices respectively, prove that: $$\begin{align} \|AB\| \le \|A\|\|B\|  \end{align}$$","I've been searching for the definition of the submultiplicative (I think it has multiple names from what I've seen) property in proof form. Some books define it as part of the properties that define matrix norms, and some include it as an additional property. I still haven't been able to work it out for myself or find it anywhere. Let $A$ and $B$ be $n\times m$ and $m\times l$ matrices respectively, prove that: $$\begin{align} \|AB\| \le \|A\|\|B\|  \end{align}$$",,"['matrices', 'proof-writing', 'normed-spaces']"
83,is a one-by-one-matrix just a number (scalar)?,is a one-by-one-matrix just a number (scalar)?,,"I was wondering. Clearly, we cannot multiply a (1x1)-matrix with a (4x3)-matrix; However, we can multiply a scalar with a matrix. This suggests a difference. On the other hand, I was, for example, in an econometrics lecture today, where we had for a (Tx1)-vector $\underline{û}=\left( \begin{array}{c} û_1\\ \vdots\\ û_T\end{array}\right)$: $S_{ûû}:= \sum_{i=1}^T û_i^2$ shall be minimized. We see see that   $S_{ûû}=\underline{û}^T\underline{û}$. Well, formally, shouldn't it be $(S_{ûû})=\underline{û}^T\underline{û}$ or $S_{ûû}=\det(\underline{û}^T\underline{û})$, to ensure that we stay in the space of matrices and not suddenly go to the space of scalars? So here, the professor (physicist) not only treats $\underline{û}^T\underline{û}$ like a scalar, but also calls it a scalar. Is this formally legit or a wrong simplification (though it does not seem to have any impact, and surely makes life easier)?","I was wondering. Clearly, we cannot multiply a (1x1)-matrix with a (4x3)-matrix; However, we can multiply a scalar with a matrix. This suggests a difference. On the other hand, I was, for example, in an econometrics lecture today, where we had for a (Tx1)-vector $\underline{û}=\left( \begin{array}{c} û_1\\ \vdots\\ û_T\end{array}\right)$: $S_{ûû}:= \sum_{i=1}^T û_i^2$ shall be minimized. We see see that   $S_{ûû}=\underline{û}^T\underline{û}$. Well, formally, shouldn't it be $(S_{ûû})=\underline{û}^T\underline{û}$ or $S_{ûû}=\det(\underline{û}^T\underline{û})$, to ensure that we stay in the space of matrices and not suddenly go to the space of scalars? So here, the professor (physicist) not only treats $\underline{û}^T\underline{û}$ like a scalar, but also calls it a scalar. Is this formally legit or a wrong simplification (though it does not seem to have any impact, and surely makes life easier)?",,"['matrices', 'complex-numbers']"
84,Determinant of a sum of matrices,Determinant of a sum of matrices,,"I would like to know if the following formula is well known and get some references for it. I don't know yet how to prove it (and I am working on it), but I am quite sure of its validity, after having performed a few symbolic computations with Maple. Given $n$ square matrices $A_1,\ldots,A_n$ of size $m<n$ : $$\sum_{p=1}^n(-1)^p\sum_{1\leqslant i_1<\cdots<i_p\leqslant n}\det(A_{i_1}+\cdots+A_{i_p})=0$$ For example, if $A,B,C$ are three $2\times2$ matrices, then : $$\det(A+B+C)-\left[\det(A+B)+\det(A+C)+\det(B+C)\right]+\det(A)+\det(B)+\det(C)=0$$","I would like to know if the following formula is well known and get some references for it. I don't know yet how to prove it (and I am working on it), but I am quite sure of its validity, after having performed a few symbolic computations with Maple. Given $n$ square matrices $A_1,\ldots,A_n$ of size $m<n$ : $$\sum_{p=1}^n(-1)^p\sum_{1\leqslant i_1<\cdots<i_p\leqslant n}\det(A_{i_1}+\cdots+A_{i_p})=0$$ For example, if $A,B,C$ are three $2\times2$ matrices, then : $$\det(A+B+C)-\left[\det(A+B)+\det(A+C)+\det(B+C)\right]+\det(A)+\det(B)+\det(C)=0$$",,"['matrices', 'determinant']"
85,Logarithm of the determinant of a positive definite matrix,Logarithm of the determinant of a positive definite matrix,,"For positive definite $C=LL^T$, where $L$ is the lower triangular Cholesky factor of $C$, why is $\log(\det(C))=2\operatorname{trace}(\log(L))$? I know that if $\{\lambda_i\}$ are the eigenvalues of $C$, $\det(C)=\prod_i\lambda_i$, so that $\log(\det(C))=\sum\log(\lambda_i)$ but I'm not sure where to go from there.","For positive definite $C=LL^T$, where $L$ is the lower triangular Cholesky factor of $C$, why is $\log(\det(C))=2\operatorname{trace}(\log(L))$? I know that if $\{\lambda_i\}$ are the eigenvalues of $C$, $\det(C)=\prod_i\lambda_i$, so that $\log(\det(C))=\sum\log(\lambda_i)$ but I'm not sure where to go from there.",,"['matrices', 'determinant', 'trace', 'positive-definite']"
86,Transformation matrix to go from one vector to another,Transformation matrix to go from one vector to another,,"I've two vectors $a = (a_1, a_2, a_3)$ and $b = (b_1, b_2, b_3)$. How to find  transformation matrix for transform from a to b?","I've two vectors $a = (a_1, a_2, a_3)$ and $b = (b_1, b_2, b_3)$. How to find  transformation matrix for transform from a to b?",,"['matrices', 'transformation', '3d']"
87,The matrix exponential is smooth,The matrix exponential is smooth,,"Let $ \exp : M(n, \mathbb{C}) \rightarrow GL(n, \mathbb{C}) $ be the matrix exponential defined by $$ \exp(X) = \sum_{k=0}^{\infty} \frac{X^k}{k!} $$ Is this map smooth as a map from $ \mathbb{C}^{n^2} \rightarrow \mathbb{C}^{n^2} $ ? My attempt: I show that the derivative at $ 0 $, $ D\exp(0) $ is the identity linear transformation on $ \mathbb{C}^{n^2} $, thus the derivative is nonsingular. This is because we have $$ \frac{|| \exp(H) - \exp(0) - H ||}{||H||} = \frac{||\sum_{k=2}^{\infty} \frac{H^k}{k!}||}{||H||} \le \sum_{k=1}^{\infty} \frac{||H||^{k}}{(k+1)!} $$ and the limit of the last expression as $ ||H|| \rightarrow 0 $ is clearly $ 0 $. My problems begin at the following: (1) I can calculate the derivative only at scalar matrices in $ M(n, \mathbb{C}) $ (I need commutativity for the identity $ \exp(A+B)=\exp(A)\exp(B) $ to hold) (2) I cannot apply the inverse function theorem yet, because I have established differentiability at only a point. How would one get around these difficulties? I know that the inverse function theorem holds for analytic functions too, but I would like to avoid it, and in any case, I would need to show that the derivative is nonsingular everywhere. I cannot see a coordinate free approach.","Let $ \exp : M(n, \mathbb{C}) \rightarrow GL(n, \mathbb{C}) $ be the matrix exponential defined by $$ \exp(X) = \sum_{k=0}^{\infty} \frac{X^k}{k!} $$ Is this map smooth as a map from $ \mathbb{C}^{n^2} \rightarrow \mathbb{C}^{n^2} $ ? My attempt: I show that the derivative at $ 0 $, $ D\exp(0) $ is the identity linear transformation on $ \mathbb{C}^{n^2} $, thus the derivative is nonsingular. This is because we have $$ \frac{|| \exp(H) - \exp(0) - H ||}{||H||} = \frac{||\sum_{k=2}^{\infty} \frac{H^k}{k!}||}{||H||} \le \sum_{k=1}^{\infty} \frac{||H||^{k}}{(k+1)!} $$ and the limit of the last expression as $ ||H|| \rightarrow 0 $ is clearly $ 0 $. My problems begin at the following: (1) I can calculate the derivative only at scalar matrices in $ M(n, \mathbb{C}) $ (I need commutativity for the identity $ \exp(A+B)=\exp(A)\exp(B) $ to hold) (2) I cannot apply the inverse function theorem yet, because I have established differentiability at only a point. How would one get around these difficulties? I know that the inverse function theorem holds for analytic functions too, but I would like to avoid it, and in any case, I would need to show that the derivative is nonsingular everywhere. I cannot see a coordinate free approach.",,"['matrices', 'lie-groups', 'lie-algebras']"
88,Connection between rational functions and matrices,Connection between rational functions and matrices,,"$$ y = \frac{ax+b}{cx+d} \Longleftrightarrow x = \frac{dy-b}{-cy+a}, ad-bc \ne 0 $$ on the other hand $$ \left( \begin{array}{cc} a & b  \\ c & d  \\  \end{array} \right)^{-1} = \frac{1}{ad-bc} \left( \begin{array}{cc} d & -b  \\ -c & a  \\  \end{array} \right), ad-bc \ne 0 $$ This looks amazing to me. Is there any meaning to this connection between rational functions and matrices? Can it be generalized for matrices of higher ranks?","$$ y = \frac{ax+b}{cx+d} \Longleftrightarrow x = \frac{dy-b}{-cy+a}, ad-bc \ne 0 $$ on the other hand $$ \left( \begin{array}{cc} a & b  \\ c & d  \\  \end{array} \right)^{-1} = \frac{1}{ad-bc} \left( \begin{array}{cc} d & -b  \\ -c & a  \\  \end{array} \right), ad-bc \ne 0 $$ This looks amazing to me. Is there any meaning to this connection between rational functions and matrices? Can it be generalized for matrices of higher ranks?",,"['matrices', 'rational-functions']"
89,Determine the winner of a tic tac toe board with a single matrix expression?,Determine the winner of a tic tac toe board with a single matrix expression?,,"Assume a tic-tac-toe board's state is stored in a matrix.  $$ S=\begin{bmatrix} -1 & 0 & 1 \\ 1 & -1 & 0 \\ 1 & 0 & -1 \\ \end{bmatrix} $$ Here, $X$ is mapped to $1$, $O$ is mapped to $-1$ and an empty state is mapped to zero, but any other numeric mapping will do if there is one more suitable for solving the problem. Is it possible to create some single expression involving the matrix $S$ which will indicate whether the board is in a winning state?  For the above matrix, the expression should show a win for $O$. I recognize that there are more direct programmatic approaches to this, so this is more of an academic question. Edit: I have been asked what to do if the board shows two winners.  You could either: Assume only valid board states. Since gameplay would stop after once side wins, it is not possible to have a board with two winners. Alternatively (or equivalently?), your expression could arbitrarily pick a winner in a board that has two.","Assume a tic-tac-toe board's state is stored in a matrix.  $$ S=\begin{bmatrix} -1 & 0 & 1 \\ 1 & -1 & 0 \\ 1 & 0 & -1 \\ \end{bmatrix} $$ Here, $X$ is mapped to $1$, $O$ is mapped to $-1$ and an empty state is mapped to zero, but any other numeric mapping will do if there is one more suitable for solving the problem. Is it possible to create some single expression involving the matrix $S$ which will indicate whether the board is in a winning state?  For the above matrix, the expression should show a win for $O$. I recognize that there are more direct programmatic approaches to this, so this is more of an academic question. Edit: I have been asked what to do if the board shows two winners.  You could either: Assume only valid board states. Since gameplay would stop after once side wins, it is not possible to have a board with two winners. Alternatively (or equivalently?), your expression could arbitrarily pick a winner in a board that has two.",,"['matrices', 'game-theory']"
90,Does $e^{AB}=e^{BA}$ imply $AB=BA$?,Does  imply ?,e^{AB}=e^{BA} AB=BA,"Let $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{n \times n}$. Is it true that $e^{AB}=e^{BA}$ implies $AB=BA$? If not, can you provide a counter example?","Let $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{n \times n}$. Is it true that $e^{AB}=e^{BA}$ implies $AB=BA$? If not, can you provide a counter example?",,"['matrices', 'matrix-exponential']"
91,"If GCD $(a_1,\ldots, a_n)=1$ then there's a matrix in $SL_n(\mathbb{Z})$ with first row $(a_1,\ldots, a_n)$",If GCD  then there's a matrix in  with first row,"(a_1,\ldots, a_n)=1 SL_n(\mathbb{Z}) (a_1,\ldots, a_n)","Let $n \geq 2$ . Let $a_1, a_2, \ldots, a_n$ be $n$ integers such that $\gcd\left(a_1, a_2, \ldots, a_n\right) = 1$ . Prove that there exists a matrix in $\operatorname{SL}_n\left(\mathbb{Z}\right)$ whose first row is $\left(a_1, a_2, \ldots, a_n\right)$ . Since the gcd of the integers $a_1,\ldots, a_n$ is $1$ , there exists weights $x_i \in \mathbb{Z}$ such that $a_1x_1+\cdots+ a_nx_n=1$ . My two ideas are (a) to brute force construct an $n\times n$ matrix with first row $a_1,\ldots ,a_n$ and to construct the remaining rows such that the determinant is $\sum a_ix_i=1$ or (b) to use induction. (a) (Constructive) This is tedious since once I find a way to construct the remaining $n-1$ rows to ensure that $a_1x_1$ appears in the determinant, I am not sure how to modify these $n-1$ rows to ensure  that only the terms $a_ix_i$ appear in the cofactor expansion. If such a matrix exists, I'd like to see it. (ii) (Non-constructive) If I proceed by induction then the base case $n=2$ is settled since I can choose the 2nd row to be $-x_2, x_1$ so that the determinant is $a_1x_1-a_2(-x_2)=1$ . However, I'm not sure how to use the inductive hypothesis to show that if I can construct such an $n\times n$ -matrix then I can construct an $\left(n+1\right) \times \left(n+1\right)$ -matrix with the desired property. In particular, if the gcd $(a_1,\ldots ,a_{n+1})$ is $1$ , it is not necesarry that the gcd of any $n$ of these terms is $1$ , so induction may not even apply here. How can I construct such a matrix or prove that one exists (without necessarily constructing it)?","Let . Let be integers such that . Prove that there exists a matrix in whose first row is . Since the gcd of the integers is , there exists weights such that . My two ideas are (a) to brute force construct an matrix with first row and to construct the remaining rows such that the determinant is or (b) to use induction. (a) (Constructive) This is tedious since once I find a way to construct the remaining rows to ensure that appears in the determinant, I am not sure how to modify these rows to ensure  that only the terms appear in the cofactor expansion. If such a matrix exists, I'd like to see it. (ii) (Non-constructive) If I proceed by induction then the base case is settled since I can choose the 2nd row to be so that the determinant is . However, I'm not sure how to use the inductive hypothesis to show that if I can construct such an -matrix then I can construct an -matrix with the desired property. In particular, if the gcd is , it is not necesarry that the gcd of any of these terms is , so induction may not even apply here. How can I construct such a matrix or prove that one exists (without necessarily constructing it)?","n \geq 2 a_1, a_2, \ldots, a_n n \gcd\left(a_1, a_2, \ldots, a_n\right) = 1 \operatorname{SL}_n\left(\mathbb{Z}\right) \left(a_1, a_2, \ldots, a_n\right) a_1,\ldots, a_n 1 x_i \in \mathbb{Z} a_1x_1+\cdots+ a_nx_n=1 n\times n a_1,\ldots ,a_n \sum a_ix_i=1 n-1 a_1x_1 n-1 a_ix_i n=2 -x_2, x_1 a_1x_1-a_2(-x_2)=1 n\times n \left(n+1\right) \times \left(n+1\right) (a_1,\ldots ,a_{n+1}) 1 n 1","['matrices', 'determinant', 'gcd-and-lcm']"
92,"Prove that $e^{t(X+Y)}=e^{tX} e^{tY}$ implies $[X,Y]=0$",Prove that  implies,"e^{t(X+Y)}=e^{tX} e^{tY} [X,Y]=0","I am currently reading about the Baker-Campbell-Hausdorff formula and in a textbook on Lie Algebras, he shows that if $$[X,[X,Y]] = 0 \quad \text{ and } [Y,[X,Y]] = 0$$ then $$e^{Xt}e^{Yt} = e^{Xt + Yt + \frac{t^{2}}{2}[X,Y]}.$$ where $[X,Y] = XY-YX$ and $X,Y$ are square matrices. I later read on wikipedia that if $$e^{X}e^{Y} = e^{(X+Y)},$$ this does not necessarily imply that $X$ and $Y$ commute, which leads me to believe that the converse of the result in the text is false. How do you prove/disprove the converse? It seems difficult and laborious to construct and verify a counterexample (and I'm not even sure the converse is false); I tried to differentiate $e^{Xt}e^{Yt} = e^{Xt+Yt + \frac{t^{2}}{2}[X,Y]}$ and evaluate at zero, but it quickly turned into a mess due to the complexity of the higher order derivatives. I then tried to prove the much easier $$e^{Xt}e^{Yt} = e^{(X+Y)t} \implies [X,Y] = 0$$ with the same technique (I hypothesized that if it can work with this statement, then it will work with the harder one after some effort), but I ended up with a collection of matrix products after the third derivative evaluated at zero that did not seem to help me at all. Edit: After some additional reading, I have found that $e^{X}e^{Y} = e^{(X+Y)} \implies [X,Y]=0$ is not true, but if $e^{Xt}e^{Yt} = e^{(X+Y)t}$ for all $t$, then this stronger statement will be true. This makes me suspect the converse I want to prove/disprove is true, but has put me no closer in actually proving it. Edit 2: After taking several higher order derivatives and evaluating at zero, I have been able to show that $[Y,[X,Y]] = 0$, but repeating this process again is becoming too difficult to compute by hand due to the large amount of nested commutators. I think there should be a way to use symmetry to use this result to conclude that $X$ must also commute with $[X,Y]$ here, but I don't know how to proceed.","I am currently reading about the Baker-Campbell-Hausdorff formula and in a textbook on Lie Algebras, he shows that if $$[X,[X,Y]] = 0 \quad \text{ and } [Y,[X,Y]] = 0$$ then $$e^{Xt}e^{Yt} = e^{Xt + Yt + \frac{t^{2}}{2}[X,Y]}.$$ where $[X,Y] = XY-YX$ and $X,Y$ are square matrices. I later read on wikipedia that if $$e^{X}e^{Y} = e^{(X+Y)},$$ this does not necessarily imply that $X$ and $Y$ commute, which leads me to believe that the converse of the result in the text is false. How do you prove/disprove the converse? It seems difficult and laborious to construct and verify a counterexample (and I'm not even sure the converse is false); I tried to differentiate $e^{Xt}e^{Yt} = e^{Xt+Yt + \frac{t^{2}}{2}[X,Y]}$ and evaluate at zero, but it quickly turned into a mess due to the complexity of the higher order derivatives. I then tried to prove the much easier $$e^{Xt}e^{Yt} = e^{(X+Y)t} \implies [X,Y] = 0$$ with the same technique (I hypothesized that if it can work with this statement, then it will work with the harder one after some effort), but I ended up with a collection of matrix products after the third derivative evaluated at zero that did not seem to help me at all. Edit: After some additional reading, I have found that $e^{X}e^{Y} = e^{(X+Y)} \implies [X,Y]=0$ is not true, but if $e^{Xt}e^{Yt} = e^{(X+Y)t}$ for all $t$, then this stronger statement will be true. This makes me suspect the converse I want to prove/disprove is true, but has put me no closer in actually proving it. Edit 2: After taking several higher order derivatives and evaluating at zero, I have been able to show that $[Y,[X,Y]] = 0$, but repeating this process again is becoming too difficult to compute by hand due to the large amount of nested commutators. I think there should be a way to use symmetry to use this result to conclude that $X$ must also commute with $[X,Y]$ here, but I don't know how to proceed.",,"['matrices', 'lie-algebras', 'matrix-calculus', 'matrix-exponential']"
93,Inequality concerning inverses of positive definite matrices,Inequality concerning inverses of positive definite matrices,,"I don't find a way to prove this: given $A$, $B$, symmetric and positive definite: $$A>B \Rightarrow A^{-1} < B^{-1},$$ where $A>B$ means that $A-B$ is positive definite.","I don't find a way to prove this: given $A$, $B$, symmetric and positive definite: $$A>B \Rightarrow A^{-1} < B^{-1},$$ where $A>B$ means that $A-B$ is positive definite.",,"['matrices', 'inequality']"
94,Correct name for multi-dimensional array/matrix/tensor,Correct name for multi-dimensional array/matrix/tensor,,What is the correct name for an n-dimensional array in mathematics? I have seen the following: nD-Matrix nD-Array nD-Tensor Which is the right way?,What is the correct name for an n-dimensional array in mathematics? I have seen the following: nD-Matrix nD-Array nD-Tensor Which is the right way?,,"['matrices', 'terminology', 'tensors']"
95,Matrix Multiplication $\to$ Function Composition?,Matrix Multiplication  Function Composition?,\to,"One property of matrices that I found very interesting is the fact that if you have two functions of the same form $$f_0(x)=\frac{ax+b}{cx+d}$$ $$f_1(x)=\frac{a'x+b'}{c'x+d'}$$ then the function $f_0\circ f_1$ is in the same form, and if you put the coefficients $a,b,c,d$ and $a',b',c',d'$ into two matrices like this: $$\begin{pmatrix} a&b \\ c&d \end{pmatrix}$$ $$\begin{pmatrix} a'&b' \\ c'&d' \end{pmatrix}$$ Then the coefficients of $f_0\circ f_1$ are given by the matrix $$\begin{pmatrix} a&b \\ c&d \end{pmatrix}\begin{pmatrix} a'&b' \\ c'&d' \end{pmatrix}$$ I use this property quite often when dealing with the composition of rational functions with linear numerators and denominators since it spares me the trouble of putting myself through some unnecessary algebra. Whilst thinking about this property, however, I was wondering if there is an analogous type of function that corresponds to a three-by-three matrix. I'm looking for some type of function so that if $g$ and $g_0$ are of this type such that the ambiguous non-$x$ variables of the type of function (suppose they are $a,b,c,d,e,f,g,h,i$ and $a',b',c',d',e',f',g',h',i'$) can be assigned to two three-by-three matrices $$\begin{pmatrix} a&b&c \\ d&e&f \\ g&h&i \end{pmatrix}$$ $$\begin{pmatrix} a'&b'&c' \\ d'&e'&f' \\ g'&h'&i' \end{pmatrix}$$ and the matrix corresponding to $g\circ g_0$ is $$\begin{pmatrix} a&b&c \\ d&e&f \\ g&h&i \end{pmatrix}\begin{pmatrix} a'&b'&c' \\ d'&e'&f' \\ g'&h'&i' \end{pmatrix}$$ Can anyone find a type of function like this? This would be very helpful to me in my studies of iterated functions.","One property of matrices that I found very interesting is the fact that if you have two functions of the same form $$f_0(x)=\frac{ax+b}{cx+d}$$ $$f_1(x)=\frac{a'x+b'}{c'x+d'}$$ then the function $f_0\circ f_1$ is in the same form, and if you put the coefficients $a,b,c,d$ and $a',b',c',d'$ into two matrices like this: $$\begin{pmatrix} a&b \\ c&d \end{pmatrix}$$ $$\begin{pmatrix} a'&b' \\ c'&d' \end{pmatrix}$$ Then the coefficients of $f_0\circ f_1$ are given by the matrix $$\begin{pmatrix} a&b \\ c&d \end{pmatrix}\begin{pmatrix} a'&b' \\ c'&d' \end{pmatrix}$$ I use this property quite often when dealing with the composition of rational functions with linear numerators and denominators since it spares me the trouble of putting myself through some unnecessary algebra. Whilst thinking about this property, however, I was wondering if there is an analogous type of function that corresponds to a three-by-three matrix. I'm looking for some type of function so that if $g$ and $g_0$ are of this type such that the ambiguous non-$x$ variables of the type of function (suppose they are $a,b,c,d,e,f,g,h,i$ and $a',b',c',d',e',f',g',h',i'$) can be assigned to two three-by-three matrices $$\begin{pmatrix} a&b&c \\ d&e&f \\ g&h&i \end{pmatrix}$$ $$\begin{pmatrix} a'&b'&c' \\ d'&e'&f' \\ g'&h'&i' \end{pmatrix}$$ and the matrix corresponding to $g\circ g_0$ is $$\begin{pmatrix} a&b&c \\ d&e&f \\ g&h&i \end{pmatrix}\begin{pmatrix} a'&b'&c' \\ d'&e'&f' \\ g'&h'&i' \end{pmatrix}$$ Can anyone find a type of function like this? This would be very helpful to me in my studies of iterated functions.",,"['matrices', 'functions', 'function-and-relation-composition']"
96,Check membership in a matrix group,Check membership in a matrix group,,"I'm looking for a (preferably somewhat efficient) algorithm for this problem: Given a normal subgroup of $SL(m, \mathbb{Z})$ generated by a finite set $\{M_1, M_2, \dotsc, M_n\}$, and some $A \in SL(m, \mathbb{Z})$, is $A$ in the normal subgroup? That is, is $A$ a product of elements of the generating set? For the application I'm looking at, an existence algorithm would be fine; I don't actually need to know what product produces $A$. Edit : The original phrasing implied commutativity, which is silly and wrong. Update 2 : @studiosus' answer looks good to me, but if there exists an efficient-enough-to-use algorithm, I'd rather award the bounty there.","I'm looking for a (preferably somewhat efficient) algorithm for this problem: Given a normal subgroup of $SL(m, \mathbb{Z})$ generated by a finite set $\{M_1, M_2, \dotsc, M_n\}$, and some $A \in SL(m, \mathbb{Z})$, is $A$ in the normal subgroup? That is, is $A$ a product of elements of the generating set? For the application I'm looking at, an existence algorithm would be fine; I don't actually need to know what product produces $A$. Edit : The original phrasing implied commutativity, which is silly and wrong. Update 2 : @studiosus' answer looks good to me, but if there exists an efficient-enough-to-use algorithm, I'd rather award the bounty there.",,"['group-theory', 'matrices']"
97,Proving a certain determinant $\left|\det A\right|$ is complete square,Proving a certain determinant  is complete square,\left|\det A\right|,"Consider the following matrix $$ A_{ij}= \begin{cases} 1\quad\text{ if }\space (i+j)\space\text{ is prime,}\\ 0\quad\text{ otherwise.} \end{cases} $$ How can one prove that $\left|\det A\right|$ is a complete square?","Consider the following matrix $$ A_{ij}= \begin{cases} 1\quad\text{ if }\space (i+j)\space\text{ is prime,}\\ 0\quad\text{ otherwise.} \end{cases} $$ How can one prove that $\left|\det A\right|$ is a complete square?",,"['number-theory', 'matrices', 'contest-math']"
98,A conjecture about traces of projections,A conjecture about traces of projections,,"Let $M_n$ denote the space of all $n\times n$ complex matrices. Define $\tau:M_n\rightarrow \mathbb{C}$ by $$\tau(X)=\frac{1}{n}\sum_{i=1}^n x_{ii},$$ where of course $X=[x_{ij}]\in M_n$. Recall that a matrix $P\in M_n$ is called an ""orthogonal projection"" if $P=P^*=P^2$. Let $A, B, C$ be orthogonal projections in $M_n$ and define two quantities - $$x=\frac{1}{3}\tau(A+B+C)$$ and $$y=\frac{1}{3}\tau(AB+BC+CA).$$ It is easy to see that for any orthogonal projections $A,B,C\in M_n$ the value of $x$ lies in $[0,1]$. I want to investigate the case when $x\in [\frac{1}{3},\frac{1}{2}]$. My goal is to compute the infimum of $y$ under this constrain on $x$. My observation is that the minimum value of $y$ is $\frac{3x-1}{4}$ when $x\in [\frac{1}{3},\frac{1}{2}]$ and for all $n\in \mathbb{N}$. However I am unable to prove this and I wondered if people here may be able to see why this might hold, or provide a counterexample to this conjecture.","Let $M_n$ denote the space of all $n\times n$ complex matrices. Define $\tau:M_n\rightarrow \mathbb{C}$ by $$\tau(X)=\frac{1}{n}\sum_{i=1}^n x_{ii},$$ where of course $X=[x_{ij}]\in M_n$. Recall that a matrix $P\in M_n$ is called an ""orthogonal projection"" if $P=P^*=P^2$. Let $A, B, C$ be orthogonal projections in $M_n$ and define two quantities - $$x=\frac{1}{3}\tau(A+B+C)$$ and $$y=\frac{1}{3}\tau(AB+BC+CA).$$ It is easy to see that for any orthogonal projections $A,B,C\in M_n$ the value of $x$ lies in $[0,1]$. I want to investigate the case when $x\in [\frac{1}{3},\frac{1}{2}]$. My goal is to compute the infimum of $y$ under this constrain on $x$. My observation is that the minimum value of $y$ is $\frac{3x-1}{4}$ when $x\in [\frac{1}{3},\frac{1}{2}]$ and for all $n\in \mathbb{N}$. However I am unable to prove this and I wondered if people here may be able to see why this might hold, or provide a counterexample to this conjecture.",,"['matrices', 'operator-theory', 'trace', 'conjectures']"
99,Is there an iterative procedure that pushes the singular values of a matrix toward unity?,Is there an iterative procedure that pushes the singular values of a matrix toward unity?,,"Consider a square matrix $A = U \Sigma V^T$.  I want to find $B = UV^T$ — however, it seems wasteful to compute the whole SVD just to re-multiply the two orthogonal matrices. Does there exist some stable procedure I can use to ""push"" the entries in the diagonal matrix $\Sigma$ toward ones?","Consider a square matrix $A = U \Sigma V^T$.  I want to find $B = UV^T$ — however, it seems wasteful to compute the whole SVD just to re-multiply the two orthogonal matrices. Does there exist some stable procedure I can use to ""push"" the entries in the diagonal matrix $\Sigma$ toward ones?",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
