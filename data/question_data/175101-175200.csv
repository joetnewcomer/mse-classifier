,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Calculate the mass of the ball $x^2+y^2+z^2=1$ with point density $e^x+e^y+e^z$,Calculate the mass of the ball  with point density,x^2+y^2+z^2=1 e^x+e^y+e^z,"I'm asked in an exercise to calculate the mass of the ball $x^2+y^2+z^2=1$ with a density of $e^x+e^y+e^z$ at a given point. We've only learned triple integration with Cartesian coordinates so far so I'm trying to set up a triple integral using those. But I get sort of stuck in figuring out how I want to set up the integral. My first thought was, I should have one coordinate, say z, go from $-1$ to $1$, y from $-\sqrt{1-z^2}$ to $\sqrt{1-z^2}$ and x from $-\sqrt{1-y^2-z^2}$ to $\sqrt{1-y^2-z^2}$. But the resulting integral turned out to be hard to calculate and the answer seems wrong. Any tips would be appreciated :). Thanks!","I'm asked in an exercise to calculate the mass of the ball $x^2+y^2+z^2=1$ with a density of $e^x+e^y+e^z$ at a given point. We've only learned triple integration with Cartesian coordinates so far so I'm trying to set up a triple integral using those. But I get sort of stuck in figuring out how I want to set up the integral. My first thought was, I should have one coordinate, say z, go from $-1$ to $1$, y from $-\sqrt{1-z^2}$ to $\sqrt{1-z^2}$ and x from $-\sqrt{1-y^2-z^2}$ to $\sqrt{1-y^2-z^2}$. But the resulting integral turned out to be hard to calculate and the answer seems wrong. Any tips would be appreciated :). Thanks!",,['multivariable-calculus']
1,What's the relationship between Gauss' law and Newton-Leibniz formula?,What's the relationship between Gauss' law and Newton-Leibniz formula?,,"Actually it's a puzzle I got in my Physics class. Someone says Gauss' law actually is a specific example of the famous Newton-Leibniz formula, but I couldn't catch it. So far I haven't learned about multivariable calculus, so maybe I need a simpler explanation~ Remark : Gauss' law: a link","Actually it's a puzzle I got in my Physics class. Someone says Gauss' law actually is a specific example of the famous Newton-Leibniz formula, but I couldn't catch it. So far I haven't learned about multivariable calculus, so maybe I need a simpler explanation~ Remark : Gauss' law: a link",,"['calculus', 'multivariable-calculus', 'physics']"
2,Minimum three variable expression $\frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|}$,Minimum three variable expression,\frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|},"If $a,b,c \ge 0$ are distinct real numbers, find the minimum value of $$\frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|}$$ What I did: I used $a^2+b^2+c^2\geq \frac{1}{3}(a+b+c)^2$ , but no success. A friend of mine said the minimum happens when one of the variables is $0$ , but I don't really understand why or how to prove this.","If are distinct real numbers, find the minimum value of What I did: I used , but no success. A friend of mine said the minimum happens when one of the variables is , but I don't really understand why or how to prove this.","a,b,c \ge 0 \frac{(a^2+b^2+c^2)^3}{(a+b+c)^3|(a-b)(b-c)(c-a)|} a^2+b^2+c^2\geq \frac{1}{3}(a+b+c)^2 0","['multivariable-calculus', 'inequality']"
3,Why $r^{2} $ instead of $r^3$ in this change of variable?,Why  instead of  in this change of variable?,r^{2}  r^3,"I'm taking a PDE's course, and several times there has been an integration over a sphere cropping up. Oftentimes, we change variables to shift the sphere we originally had to the unit sphere, and carry on calculations from there. I'll give an example: Let $ f: \mathbb{R}^3 \rightarrow   \mathbb{R}$ . We want to integrate $f$ over the surface of some ball in 3 dimensions, centered at $x$ and with radius $r$ ; that is, we are after $$ \int_{\partial B(x, r)} f(\sigma) \ d\sigma.$$ If we make the change of variables $$ \sigma = x + r\omega, $$ then the integral becomes $$ \int_{\partial B(0, 1)} f(x + r\omega)r^2 \ d\omega. $$ I don't understand why this is...I tried to justify it by using the change of variables theorem (the one involving the Jacobian determinant), but I get a different result, and I don't know what I'm misunderstanding here. Using the theorem, we can think of this change of variables as $$ T(\omega) = x + r \omega $$ for any $\omega \in \mathbb{R}^3.$ Then \begin{align}     && \int_{\partial B(x, r)} f(\sigma) \ d\sigma &=\int_{T^{-1}(\partial B(x, r)) = \partial B(0, 1)} f(T(\omega)) |J(\omega)| d \omega && \end{align} where $|J(\omega)|$ is the Jacobian determinant of $T$ . But $$T(\omega) = \langle x_1 + r \omega_1, x_2 + r \omega_2, x_3 + r \omega_3 \rangle$$ so that $$ T'(\omega) = J(\omega) = \frac{\partial }{\omega_j} ( x_i + r \omega_i) = \delta_{ij} \cdot r,$$ or, in other words, a 3 by 3 matrix with only $r$ on the diagonal, and zero everywhere else. Doesn't that mean the Jacobian determinant should be $r^3$ and not $r^2$ ? I would hope that by this point I would have grasped this change of variables theorem, but it's very possible I have misunderstood something along the way. I would really appreciate some help! Thank you!","I'm taking a PDE's course, and several times there has been an integration over a sphere cropping up. Oftentimes, we change variables to shift the sphere we originally had to the unit sphere, and carry on calculations from there. I'll give an example: Let . We want to integrate over the surface of some ball in 3 dimensions, centered at and with radius ; that is, we are after If we make the change of variables then the integral becomes I don't understand why this is...I tried to justify it by using the change of variables theorem (the one involving the Jacobian determinant), but I get a different result, and I don't know what I'm misunderstanding here. Using the theorem, we can think of this change of variables as for any Then where is the Jacobian determinant of . But so that or, in other words, a 3 by 3 matrix with only on the diagonal, and zero everywhere else. Doesn't that mean the Jacobian determinant should be and not ? I would hope that by this point I would have grasped this change of variables theorem, but it's very possible I have misunderstood something along the way. I would really appreciate some help! Thank you!"," f: \mathbb{R}^3 \rightarrow   \mathbb{R} f x r  \int_{\partial B(x, r)} f(\sigma) \ d\sigma.  \sigma = x + r\omega,   \int_{\partial B(0, 1)} f(x + r\omega)r^2 \ d\omega.   T(\omega) = x + r \omega  \omega \in \mathbb{R}^3. \begin{align} 
   && \int_{\partial B(x, r)} f(\sigma) \ d\sigma &=\int_{T^{-1}(\partial B(x, r)) = \partial B(0, 1)} f(T(\omega)) |J(\omega)| d \omega &&
\end{align} |J(\omega)| T T(\omega) = \langle x_1 + r \omega_1, x_2 + r \omega_2, x_3 + r \omega_3 \rangle  T'(\omega) = J(\omega) = \frac{\partial }{\omega_j} ( x_i + r \omega_i)
= \delta_{ij} \cdot r, r r^3 r^2","['multivariable-calculus', 'change-of-variable']"
4,Prove $ \left(\sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}\right) \left( \sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}\right) \le \frac{9}{8}n^4$,Prove, \left(\sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}\right) \left( \sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}\right) \le \frac{9}{8}n^4,"Prove that for all $n \in \mathbb{N}$ the inequality $$ \left(\sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}\right) \left( \sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}\right) \le \frac{9}{8}n^4$$ holds. My work . I proved this inequality, but my proof is ugly (it is necessary to check by brute force whether the inequality holds for $n=1,2,3,...,15$ ). I hope that there is nice proof of this inequality. Michael Rozenberg wrote a very nice solution to a similar problem ( Prove the inequality $\sum \limits_{k=1}^n \frac{k+1}{k} \cdot \sum \limits_{k=1}^n \frac{k}{k+1} \le \frac{9}{8}n^2$ ). I think this inequality has a similar proof, but I can’t prove in a similar way. I will write as I proved the inequality. Let $S_n= \sum \limits_{k=1}^n \frac{1}{k} $ . Then $$ \sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}=n^2+2n-S_n $$ and $$\sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}=n^2-2n-3+\frac{3}{n+1}+3S_n$$ We need to prove that $$3S_n^2-S_n \left( 2n^2+8n+3-\frac{3}{n+1}\right)+\frac{n^4}{8}+7n^2+3n-3+\frac{3}{n+1} \ge 0$$ To prove this inequality, I found discriminant of the quadratic polynomial and used the fact that $S_n \le n$ . It was possible to prove that the inequality holds for all $n \ge 16$ .","Prove that for all the inequality holds. My work . I proved this inequality, but my proof is ugly (it is necessary to check by brute force whether the inequality holds for ). I hope that there is nice proof of this inequality. Michael Rozenberg wrote a very nice solution to a similar problem ( Prove the inequality $\sum \limits_{k=1}^n \frac{k+1}{k} \cdot \sum \limits_{k=1}^n \frac{k}{k+1} \le \frac{9}{8}n^2$ ). I think this inequality has a similar proof, but I can’t prove in a similar way. I will write as I proved the inequality. Let . Then and We need to prove that To prove this inequality, I found discriminant of the quadratic polynomial and used the fact that . It was possible to prove that the inequality holds for all .","n \in \mathbb{N}  \left(\sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}\right) \left( \sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}\right) \le \frac{9}{8}n^4 n=1,2,3,...,15 S_n= \sum \limits_{k=1}^n \frac{1}{k}   \sum \limits_{k=1}^n (2k-1)\frac{k+1}{k}=n^2+2n-S_n  \sum \limits_{k=1}^n (2k-1)\frac{k}{k+1}=n^2-2n-3+\frac{3}{n+1}+3S_n 3S_n^2-S_n \left( 2n^2+8n+3-\frac{3}{n+1}\right)+\frac{n^4}{8}+7n^2+3n-3+\frac{3}{n+1} \ge 0 S_n \le n n \ge 16","['multivariable-calculus', 'inequality', 'summation', 'contest-math']"
5,Intuition behind how the Cauchy-Schwarz inequality's proof was obtained,Intuition behind how the Cauchy-Schwarz inequality's proof was obtained,,"I'm studying multivariable calculus. Usually, when I study, I go through a book until I find a theorem, and then try to prove it. I was unable to, so I read the proof, which is the following: Let $x, y \in \mathbb{R}^m, \alpha \in \mathbb{R}$ . Then $(x+\alpha  y)\cdot(x+\alpha y) = \vert \vert x+\alpha y\vert\vert^2 \geq0$ .  Using the properties a the inner product we get: $(x+\alpha y)\cdot(x+\alpha y) = x\cdot x+\alpha x\cdot y +  \alpha y\cdot x + \alpha^2y\cdot y  = \vert\vert x\vert\vert^2+2(x\cdot y)\alpha + \alpha^2\vert\vert y\vert\vert^2 \geq 0$ . That last inequality is true iff the discriminant of the polynomial with respect to $\alpha$ is less than or equal to 0. Therefore $\vert  x\cdot y\vert - \vert \vert x\vert\vert^2\vert\vert y\vert\vert^2  \leq 0$ , from which comes the Cauchy-Schwarz inequality. Q.E.D I can follow every step of the proof. I also get the intuition of why the inequality should be true. However, the proof seems ""empty"" to me. I don't understand what someone who wanted to prove this would do to find it. What's the intuition behind using $x+\alpha y$ ? The reason I ask this is because, after I read the proof, the way used to prove it was so beyond everything that I tried, that I am almost sure that I'd never be able to prove this on my own. How to deal with these kind of situations?","I'm studying multivariable calculus. Usually, when I study, I go through a book until I find a theorem, and then try to prove it. I was unable to, so I read the proof, which is the following: Let . Then .  Using the properties a the inner product we get: . That last inequality is true iff the discriminant of the polynomial with respect to is less than or equal to 0. Therefore , from which comes the Cauchy-Schwarz inequality. Q.E.D I can follow every step of the proof. I also get the intuition of why the inequality should be true. However, the proof seems ""empty"" to me. I don't understand what someone who wanted to prove this would do to find it. What's the intuition behind using ? The reason I ask this is because, after I read the proof, the way used to prove it was so beyond everything that I tried, that I am almost sure that I'd never be able to prove this on my own. How to deal with these kind of situations?","x, y \in \mathbb{R}^m, \alpha \in \mathbb{R} (x+\alpha
 y)\cdot(x+\alpha y) = \vert \vert x+\alpha y\vert\vert^2 \geq0 (x+\alpha y)\cdot(x+\alpha y) = x\cdot x+\alpha x\cdot y +
 \alpha y\cdot x + \alpha^2y\cdot y
 = \vert\vert x\vert\vert^2+2(x\cdot y)\alpha + \alpha^2\vert\vert y\vert\vert^2 \geq 0 \alpha \vert
 x\cdot y\vert - \vert \vert x\vert\vert^2\vert\vert y\vert\vert^2
 \leq 0 x+\alpha y","['multivariable-calculus', 'soft-question', 'intuition']"
6,Is there a smooth surjective function $f:\mathbb{R}^2 \to \mathbb{R}^3$?,Is there a smooth surjective function ?,f:\mathbb{R}^2 \to \mathbb{R}^3,"Is it possible to have three smooth functions of two variables $(x,y) \in \mathbb{R}^2$ $$\begin{array}{rcl} u &=& f_1(x,y)\\ v &=& f_2(x,y)\\ w &=& f_3(x,y)\\ \end{array} $$ such that the image $(u,v,w)$ is equal to  $\mathbb{R}^3$ I was thinking about this in relation to parametric families of member curves/functions where the curves transition smoothly when changing the parameters. Then such a family where the member curves are parameterized by three variables $u,v,w$ could be changed into a family where the member curves are parameterized by two variables $x,y$, while still keeping smooth transitions. I tried something like a tangens function e.g. $u = tan(x)$ and $v = \lfloor \frac{1}{2}+\frac{x}{\pi} \rfloor$ but this is not smooth (at every $x = \frac{1+k}{2} \pi$ ) and $v$ is only in $\mathbb{N}$.","Is it possible to have three smooth functions of two variables $(x,y) \in \mathbb{R}^2$ $$\begin{array}{rcl} u &=& f_1(x,y)\\ v &=& f_2(x,y)\\ w &=& f_3(x,y)\\ \end{array} $$ such that the image $(u,v,w)$ is equal to  $\mathbb{R}^3$ I was thinking about this in relation to parametric families of member curves/functions where the curves transition smoothly when changing the parameters. Then such a family where the member curves are parameterized by three variables $u,v,w$ could be changed into a family where the member curves are parameterized by two variables $x,y$, while still keeping smooth transitions. I tried something like a tangens function e.g. $u = tan(x)$ and $v = \lfloor \frac{1}{2}+\frac{x}{\pi} \rfloor$ but this is not smooth (at every $x = \frac{1+k}{2} \pi$ ) and $v$ is only in $\mathbb{N}$.",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'differential']"
7,Curl of gradient is not zero,Curl of gradient is not zero,,"I have heard that for some functions $T$ , if we calculate $\nabla \times (\nabla  T )$ in $2$ -dimensional polar coordinates, then we get the delta function. Why do we get that result?","I have heard that for some functions , if we calculate in -dimensional polar coordinates, then we get the delta function. Why do we get that result?",T \nabla \times (\nabla  T ) 2,"['calculus', 'multivariable-calculus', 'vectors', 'vector-analysis', 'curl']"
8,Taylor series third order approximation,Taylor series third order approximation,,"There has been this question that had been bothering me for a while and I could not find a satisfying answer on the internet or any of the books even though it is not very complex. i) Its because if I have to find a third order polynomium approximation using taylor series for a 2 variable function, then is it correct to write that the third term will look something like this, $$ ... + \frac{1}{3!}[f_{xxx}(x_0,y_0)(x-a)^3 + 6f_{xxy}(x_0,y_0)(x-a)(y-b)+f_{yyy}(x_0,y_0)(y-b)^3] + ....   $$ I was a bit unsure about the middle part. ii) About the hessian matrix, how would I write a hessian matrix if I have to make one for a third order like the one above. I know that for second order it looks like, $$H_f(x,y) = \left(\begin{array}{cccc} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{array}\right)$$ Thank You :)","There has been this question that had been bothering me for a while and I could not find a satisfying answer on the internet or any of the books even though it is not very complex. i) Its because if I have to find a third order polynomium approximation using taylor series for a 2 variable function, then is it correct to write that the third term will look something like this, $$ ... + \frac{1}{3!}[f_{xxx}(x_0,y_0)(x-a)^3 + 6f_{xxy}(x_0,y_0)(x-a)(y-b)+f_{yyy}(x_0,y_0)(y-b)^3] + ....   $$ I was a bit unsure about the middle part. ii) About the hessian matrix, how would I write a hessian matrix if I have to make one for a third order like the one above. I know that for second order it looks like, $$H_f(x,y) = \left(\begin{array}{cccc} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{array}\right)$$ Thank You :)",,"['multivariable-calculus', 'taylor-expansion', 'hessian-matrix']"
9,Differentiability of multivariate functions.,Differentiability of multivariate functions.,,"I would appreciate if someone could share some intuition as to the geometric meaning of differentiability condition of functions defined on $\mathbb{R}^n$. Such a function say $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is differentiable if there is a matrix (vector in this case) $A$ such that  $$\lim\limits_{\mathbf{h} \to 0} \frac{|f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})-A\cdot \mathbf{h}|}{|\mathbf{h}|}=0.$$  $A$ is the vector of partial derivatives of $f$, but the above condition is stronger than the mere existence. I would like to know the geometric nature of what more is required. Specifically, with respect to the formula for computing directional derivatives, $$D_{\mathbf{v}} f=A\cdot \mathbf{v}$$ Rudin gives a nice example of a non differentiable function with all directional derivatives exist but the above equation is false. So to be more precise I ask 1) Is the existence of all directional derivatives and the equation $$D_{\mathbf{v}} f=A\cdot \mathbf{v}$$ equivalent to differentiablity. 2)For which $X$ is ""all directional derivatives exist"" +X, equivalent to differentiablilty. 3) Does the existence of directional derivatives in $n$ independent directions imply all directional derivatives exist.","I would appreciate if someone could share some intuition as to the geometric meaning of differentiability condition of functions defined on $\mathbb{R}^n$. Such a function say $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is differentiable if there is a matrix (vector in this case) $A$ such that  $$\lim\limits_{\mathbf{h} \to 0} \frac{|f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})-A\cdot \mathbf{h}|}{|\mathbf{h}|}=0.$$  $A$ is the vector of partial derivatives of $f$, but the above condition is stronger than the mere existence. I would like to know the geometric nature of what more is required. Specifically, with respect to the formula for computing directional derivatives, $$D_{\mathbf{v}} f=A\cdot \mathbf{v}$$ Rudin gives a nice example of a non differentiable function with all directional derivatives exist but the above equation is false. So to be more precise I ask 1) Is the existence of all directional derivatives and the equation $$D_{\mathbf{v}} f=A\cdot \mathbf{v}$$ equivalent to differentiablity. 2)For which $X$ is ""all directional derivatives exist"" +X, equivalent to differentiablilty. 3) Does the existence of directional derivatives in $n$ independent directions imply all directional derivatives exist.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
10,Why Does the existence of $\frac{\partial f}{\partial x}$ not imply that $\frac{\partial f}{\partial x}$ is continuous?,Why Does the existence of  not imply that  is continuous?,\frac{\partial f}{\partial x} \frac{\partial f}{\partial x},"For $f(x)$, the existence of $f'(x)$ implies the continuity of $f(x)$. And I am assuming that it also implies the continuity of $f'(x)$. My question is why in a function $g(x,y)$, is the existence of $g_x$ and $g_y$ not sufficient condition for the continuity of $g_x$ and $g_y$ and hence the continuity of the function? To me, it seems existence of $f'(x)$ and $g_x$ should either both imply continuity of $f'(x)$ and $g_x$ or not. Why are they different in each case?","For $f(x)$, the existence of $f'(x)$ implies the continuity of $f(x)$. And I am assuming that it also implies the continuity of $f'(x)$. My question is why in a function $g(x,y)$, is the existence of $g_x$ and $g_y$ not sufficient condition for the continuity of $g_x$ and $g_y$ and hence the continuity of the function? To me, it seems existence of $f'(x)$ and $g_x$ should either both imply continuity of $f'(x)$ and $g_x$ or not. Why are they different in each case?",,"['real-analysis', 'multivariable-calculus', 'continuity', 'examples-counterexamples', 'partial-derivative']"
11,Helix in a helix,Helix in a helix,,"I am trying to work out a ""helix in a helix"" mathematically.  Intuitively I think of this as a steel cable, which is made up of a number of smaller steel cables all bound together in spiral.  If I wanted to find the length of one of the individual cables, it would be bound in a spiral in the smaller cable and then those cables bound in a larger spiral cable.  I know that if I wanted to do a helix whose ends meet, I would use the parametrization $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},b\sin(\omega{t})),t=0..2\pi$$ I've been trying to map out in my head how to, instead of curling the helix, making the helix travel in the path of a helix.  I've achieved it partially with $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},t),t=0..\infty$$ But this doesn't keep the smaller helix in tact, and turns it into a sine wave helix.  I've also tried  $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},tb\sin(\omega{t})),t=0..2\pi$$ But this gives me sort of a nautilus shape where the helix curls into itself and increases in size and curls around into itself.  What am I missing? EDIT:  Also, what if we wanted to do this again, like a 'helix in a helix in a ... in a helix'?","I am trying to work out a ""helix in a helix"" mathematically.  Intuitively I think of this as a steel cable, which is made up of a number of smaller steel cables all bound together in spiral.  If I wanted to find the length of one of the individual cables, it would be bound in a spiral in the smaller cable and then those cables bound in a larger spiral cable.  I know that if I wanted to do a helix whose ends meet, I would use the parametrization $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},b\sin(\omega{t})),t=0..2\pi$$ I've been trying to map out in my head how to, instead of curling the helix, making the helix travel in the path of a helix.  I've achieved it partially with $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},t),t=0..\infty$$ But this doesn't keep the smaller helix in tact, and turns it into a sine wave helix.  I've also tried  $$((a+b\cos(\omega{t}))\cos{t},(a+b\cos(\omega{t})\sin{t},tb\sin(\omega{t})),t=0..2\pi$$ But this gives me sort of a nautilus shape where the helix curls into itself and increases in size and curls around into itself.  What am I missing? EDIT:  Also, what if we wanted to do this again, like a 'helix in a helix in a ... in a helix'?",,['multivariable-calculus']
12,"Why is $\int_{\partial D}x\,dy$ invalid for calculating area of $D$?",Why is  invalid for calculating area of ?,"\int_{\partial D}x\,dy D","I am just learning about differential forms, and I had a question about employing Green's theorem to calculate area. Generalized Stokes' theorem says that $\int_{\partial D}\omega=\int_D d\omega$. Let's say $D$ is a region in $\mathbb{R}^2$. The familiar formula to calculate area is $\iint_D 1 dxdy = \frac{1}{2}\int_{\partial D}x\,dy - y\,dx$, and indeed, $d(x\,dy - y\,dx)=2\,dx\,dy$. But why aren't we allowed to simply use $\int_{\partial D}x\,dy$? Doesn't $d(x\,dy)=d(x)dy = (1\,dx + 0\,dy)dy = dx\,dy$?","I am just learning about differential forms, and I had a question about employing Green's theorem to calculate area. Generalized Stokes' theorem says that $\int_{\partial D}\omega=\int_D d\omega$. Let's say $D$ is a region in $\mathbb{R}^2$. The familiar formula to calculate area is $\iint_D 1 dxdy = \frac{1}{2}\int_{\partial D}x\,dy - y\,dx$, and indeed, $d(x\,dy - y\,dx)=2\,dx\,dy$. But why aren't we allowed to simply use $\int_{\partial D}x\,dy$? Doesn't $d(x\,dy)=d(x)dy = (1\,dx + 0\,dy)dy = dx\,dy$?",,"['differential-geometry', 'multivariable-calculus']"
13,Prove that $\frac{1}{2\pi}\frac{xdy-ydx}{x^2+y^2}$ is closed,Prove that  is closed,\frac{1}{2\pi}\frac{xdy-ydx}{x^2+y^2},"I would like to prove that $\alpha = \frac{1}{2\pi} \frac{xdy-ydx}{x^2+y^2}$ is a closed differential form on $\mathbb{R}^2-\{0\}$ . However when I apply the external derivative to this expression (and ignore the $\frac{1}{2\pi}\cdot\frac{1}{x^2+y^2}$  factor ), I get: \begin{equation} d \alpha = \frac{\partial x}{\partial x}dx\wedge dy + 	       \frac{\partial x}{\partial y}dy\wedge dy - 	       \frac{\partial y}{\partial x}dx\wedge dx - 	       \frac{\partial y}{\partial y}dy\wedge dx \end{equation} \begin{equation} d \alpha = dx\wedge dy - dy\wedge dx \end{equation} \begin{equation} d \alpha = 2 dx\wedge dy \end{equation} Which is not closed on $\mathbb{R}^2-\{0\}$. Where is my mistake ?","I would like to prove that $\alpha = \frac{1}{2\pi} \frac{xdy-ydx}{x^2+y^2}$ is a closed differential form on $\mathbb{R}^2-\{0\}$ . However when I apply the external derivative to this expression (and ignore the $\frac{1}{2\pi}\cdot\frac{1}{x^2+y^2}$  factor ), I get: \begin{equation} d \alpha = \frac{\partial x}{\partial x}dx\wedge dy + 	       \frac{\partial x}{\partial y}dy\wedge dy - 	       \frac{\partial y}{\partial x}dx\wedge dx - 	       \frac{\partial y}{\partial y}dy\wedge dx \end{equation} \begin{equation} d \alpha = dx\wedge dy - dy\wedge dx \end{equation} \begin{equation} d \alpha = 2 dx\wedge dy \end{equation} Which is not closed on $\mathbb{R}^2-\{0\}$. Where is my mistake ?",,"['multivariable-calculus', 'differential-forms']"
14,Does the operator $\sum_i x_i \frac{\partial}{\partial x_i}$ have a name?,Does the operator  have a name?,\sum_i x_i \frac{\partial}{\partial x_i},This appears in Euler's homogenous function theorem. Does it have a commonly-used name?,This appears in Euler's homogenous function theorem. Does it have a commonly-used name?,,['multivariable-calculus']
15,Finding $\iint_S \nabla \times F\ dS$,Finding,\iint_S \nabla \times F\ dS,"I have to solve the following problem which seems difficult: Find $$ \iint_S \nabla \times F\ dS $$ where $S$ is given by $$r(t,s)=\left( 9+(\cos t)(\sin s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \ 9+(\cos t)(\cos s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \  9+\frac{\sin t}{3}\left(2+\frac{\sin (5s)}{2}\right) \right) $$ where $0\leq t\leq 2\pi$ , $\ \ $ $0\leq s\leq \pi$ , $\ \ \ $ and $F:=(z,0,y)$ . I'm not sure how to proceed, any help is appreciated. Should I use Gauss divergence? When I plotted $S$ in wolfram (not sure why is different from the answer below) https://www.wolframalpha.com/input/?i=%289%2B%28cos+t%29%28sin+s%29%282%2Bsin+%285s%29%2F%282%29%29%2C++9%2B%28cos+t%29%28cos+s%29%282%2Bsin+%285s%29%2F2%29%2C+9%2B%28%28sin+t%29%2F3%29%282%2Bsin+%285s%29%2F2%29+%29%2C+0%3C%3D+s%3C%3D+pi%2C+0%3C%3Dt%3C%3D+2pi","I have to solve the following problem which seems difficult: Find where is given by where , , and . I'm not sure how to proceed, any help is appreciated. Should I use Gauss divergence? When I plotted in wolfram (not sure why is different from the answer below) https://www.wolframalpha.com/input/?i=%289%2B%28cos+t%29%28sin+s%29%282%2Bsin+%285s%29%2F%282%29%29%2C++9%2B%28cos+t%29%28cos+s%29%282%2Bsin+%285s%29%2F2%29%2C+9%2B%28%28sin+t%29%2F3%29%282%2Bsin+%285s%29%2F2%29+%29%2C+0%3C%3D+s%3C%3D+pi%2C+0%3C%3Dt%3C%3D+2pi"," \iint_S \nabla \times F\ dS  S r(t,s)=\left( 9+(\cos t)(\sin s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \ 9+(\cos t)(\cos s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \  9+\frac{\sin t}{3}\left(2+\frac{\sin (5s)}{2}\right) \right)  0\leq t\leq 2\pi \ \  0\leq s\leq \pi \ \ \  F:=(z,0,y) S",['multivariable-calculus']
16,Spivak's Proof that continuous differentiability implies differentiable,Spivak's Proof that continuous differentiability implies differentiable,,"This is from Spivak's Calculus on Manifold and I have some questions about the assumptions and implications of this proof: I noticed one of the assumptions regarding the partial derivatives is that they exist not just  at the point a, but also in an open set containing a. What if the partial derivatives only exist at the point a (and is also continuous at the point a) but nowhere else? Is there a counterexample showing that f is no longer differentiable with this change in assumption? This proof does not seem to care whether we are using the standard basis vectors or not for our partials. If we choose non orthonormal basis, would this proof still be valid? So for example in $\mathbb{R}^2$ . if we choose $\{(1,1),(1,0)\}$ as our basis and coordinate axis and proved the partial derivative with this basis is continuously differentiable, does the conclusion still hold? If what I said above is correct, then is there an alternative proof to this using directional derivatives? Say we are given the standard basis (in $\mathbb{R}^2$ , but instead, we choose two linearly independent vectors and proved that the function defined in those directions are continuously differentiable at the point a, does that still allow us to conclude the function is differentiable at the point a?","This is from Spivak's Calculus on Manifold and I have some questions about the assumptions and implications of this proof: I noticed one of the assumptions regarding the partial derivatives is that they exist not just  at the point a, but also in an open set containing a. What if the partial derivatives only exist at the point a (and is also continuous at the point a) but nowhere else? Is there a counterexample showing that f is no longer differentiable with this change in assumption? This proof does not seem to care whether we are using the standard basis vectors or not for our partials. If we choose non orthonormal basis, would this proof still be valid? So for example in . if we choose as our basis and coordinate axis and proved the partial derivative with this basis is continuously differentiable, does the conclusion still hold? If what I said above is correct, then is there an alternative proof to this using directional derivatives? Say we are given the standard basis (in , but instead, we choose two linearly independent vectors and proved that the function defined in those directions are continuously differentiable at the point a, does that still allow us to conclude the function is differentiable at the point a?","\mathbb{R}^2 \{(1,1),(1,0)\} \mathbb{R}^2","['multivariable-calculus', 'partial-derivative']"
17,Gradient in Spherical coordinates,Gradient in Spherical coordinates,,"I'm trying to derive the gradient vector in spherical polar coordinates: $$\nabla = \left( \frac{\partial}{\partial x}, \frac{\partial}{\partial y},\frac{\partial}{\partial z} \right)$$ The method I am trying to use is different from most papers/videos I found and I don't understand why it doesn't work. I am able to get expressions for the cartesian differentials: $dx, dy, dz$ . First, I am just trying to find $\frac{\partial}{\partial x}$ in spherical coordinates. I have: $$dx= \cos \theta \sin \phi dr + r \cos \theta \cos \phi d \theta + r \sin \theta \sin \phi d \phi$$ This sounds strange but my first intuition was to say that: $$\frac{\partial}{\partial x} = \frac{ \partial }{ \cos \theta \sin \phi dr + r \cos \theta \cos \phi d \theta + r \sin \theta \sin \phi d \phi} $$ Which doesn't seem to make any sense now :(. If my logic is wrong, could someone explain to me why?","I'm trying to derive the gradient vector in spherical polar coordinates: The method I am trying to use is different from most papers/videos I found and I don't understand why it doesn't work. I am able to get expressions for the cartesian differentials: . First, I am just trying to find in spherical coordinates. I have: This sounds strange but my first intuition was to say that: Which doesn't seem to make any sense now :(. If my logic is wrong, could someone explain to me why?","\nabla = \left( \frac{\partial}{\partial x}, \frac{\partial}{\partial y},\frac{\partial}{\partial z} \right) dx, dy, dz \frac{\partial}{\partial x} dx= \cos \theta \sin \phi dr + r \cos \theta \cos \phi d \theta + r \sin \theta \sin \phi d \phi \frac{\partial}{\partial x} = \frac{ \partial }{ \cos \theta \sin \phi dr + r \cos \theta \cos \phi d \theta + r \sin \theta \sin \phi d \phi} ","['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'coordinate-systems', 'spherical-coordinates']"
18,Correct way to evaluate partial derivatives for a coordinate transform?,Correct way to evaluate partial derivatives for a coordinate transform?,,"This question occurred to me when thinking about differential geometry change of coordinates. Consider the system of equations: \begin{align*} p(x, y) = x + y &\qquad x(p, q) = p - q \\ q(x, y) = y &\qquad y(p, q) = q \end{align*} Now, if I wish to evaluate $\frac{dp}{dq}$ , there are two evaluation strategies available for me: \begin{align*} \frac{dp}{dq} = \frac{d(x+y)}{dy} =  \frac{dx}{dy} + \frac{dy}{dy} = 0 + 1 = 1 \qquad (1) \end{align*} One the other hand, consider this evaluation: \begin{align*} \frac{dp}{dq} = \frac{dp}{dx}\frac{dx}{dq} + \frac{dp}{dy}\frac{dy}{dq} = 1  \cdot -1 + 1 \cdot 1 = 0 \qquad (2) \end{align*} Indeed, one can prove something much stronger: Let $p_i = f_i(x_1, x_2, \dots x_n)$ . Now, the evaluation \begin{align*} \frac{dp_i}{dp_j}  = \sum_{k=0}^n \frac{dp_i}{dx_k} \frac{dx_k}{dp_j}  = \left[\frac{dp_i}{dx_0} \dots \frac{dp_i}{dx_k} \dots  \frac{dp_i}{dx_n} \right] \cdot     \left[ \frac{dx_0}{dp_j}  \dots \frac{dx_k}{dp_j} \dots \frac{dx_n}{dp_j}  \right]^T = (J \cdot J^{-1})_{ij} = \delta_{ij} \end{align*} where $J$ is the jacobian of the function $\vec p = f(\vec x)$ , and $\delta_{ij}$ is the kronecker delta. I'm puzzled as to which interpretation I should choose. Interpretation (2) is nice if I want to think of the sets of ""co-ordinate systems"" $\{ p_i \}$ as being linearly independent, just like the original $\{ x_i \}$ are, but I have no idea if this is legal. I'd love an answer that explains to me when (1) is legal, when (2) is legal, and maybe go into more detail about the relationship with the Jacboian, and related geometric insights!","This question occurred to me when thinking about differential geometry change of coordinates. Consider the system of equations: Now, if I wish to evaluate , there are two evaluation strategies available for me: One the other hand, consider this evaluation: Indeed, one can prove something much stronger: Let . Now, the evaluation where is the jacobian of the function , and is the kronecker delta. I'm puzzled as to which interpretation I should choose. Interpretation (2) is nice if I want to think of the sets of ""co-ordinate systems"" as being linearly independent, just like the original are, but I have no idea if this is legal. I'd love an answer that explains to me when (1) is legal, when (2) is legal, and maybe go into more detail about the relationship with the Jacboian, and related geometric insights!","\begin{align*}
p(x, y) = x + y &\qquad x(p, q) = p - q \\
q(x, y) = y &\qquad y(p, q) = q
\end{align*} \frac{dp}{dq} \begin{align*}
\frac{dp}{dq} = \frac{d(x+y)}{dy} = 
\frac{dx}{dy} + \frac{dy}{dy} = 0 + 1 = 1 \qquad (1)
\end{align*} \begin{align*}
\frac{dp}{dq} = \frac{dp}{dx}\frac{dx}{dq} + \frac{dp}{dy}\frac{dy}{dq} =
1  \cdot -1 + 1 \cdot 1 = 0 \qquad (2)
\end{align*} p_i = f_i(x_1, x_2, \dots x_n) \begin{align*}
\frac{dp_i}{dp_j} 
= \sum_{k=0}^n \frac{dp_i}{dx_k} \frac{dx_k}{dp_j} 
=
\left[\frac{dp_i}{dx_0} \dots \frac{dp_i}{dx_k} \dots  \frac{dp_i}{dx_n} \right] \cdot  
  \left[ \frac{dx_0}{dp_j}  \dots \frac{dx_k}{dp_j} \dots \frac{dx_n}{dp_j}  \right]^T = (J \cdot J^{-1})_{ij} = \delta_{ij}
\end{align*} J \vec p = f(\vec x) \delta_{ij} \{ p_i \} \{ x_i \}","['multivariable-calculus', 'differential-geometry', 'coordinate-systems']"
19,Gradient of $x \mapsto \log \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right) $,Gradient of,x \mapsto \log \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right) ,"Let $f : \mathbb{R}^{n} \to \mathbb{R}$ be defined by $$f (x) := \log \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right)$$ Calculate the gradient $\nabla f$ . I have been struggling to calculate $\nabla f$ . My attempt is as follows. $$\dfrac {df}{dx} = \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right)^{-1} \cdot\dfrac {dg}{dx}$$ where $m : = \left(\dfrac {3}{10}I+xx^{T}\right)$ $$g := \det(m) =  \det \left( \dfrac {3}{10}I+xx^{T}\right)$$ $$ \dfrac {dg}{dx} = \det \left( \dfrac {3}{10}I+xx^{T}\right).Tr( \left( \dfrac {3}{10}I+xx^{T}\right)^{-1}.\dfrac {dm}{dx}) $$ $$\dfrac {dm}{dx} = \dfrac {d(xx^{T})}{dx}$$ is an $n \times n \times n$ tensor, and $Tr(\dfrac {dm}{dx})= 2x$ . So, I get $$\dfrac {df}{dx} = Tr(\left( \dfrac {3}{10}I+xx^{T}\right)^{-1}.\dfrac {d(xx^{T})}{dx})$$ A second question is about checking this or other complex multivariate differentials, I know my attempts are wrong by using autograd in Python. It would be really helpful to know  if this is possible with SymPy or something similar, or it is rarely done.","Let be defined by Calculate the gradient . I have been struggling to calculate . My attempt is as follows. where is an tensor, and . So, I get A second question is about checking this or other complex multivariate differentials, I know my attempts are wrong by using autograd in Python. It would be really helpful to know  if this is possible with SymPy or something similar, or it is rarely done.",f : \mathbb{R}^{n} \to \mathbb{R} f (x) := \log \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right) \nabla f \nabla f \dfrac {df}{dx} = \left( \det \left( \dfrac {3}{10}I+xx^{T}\right) \right)^{-1} \cdot\dfrac {dg}{dx} m : = \left(\dfrac {3}{10}I+xx^{T}\right) g := \det(m) =  \det \left( \dfrac {3}{10}I+xx^{T}\right)  \dfrac {dg}{dx} = \det \left( \dfrac {3}{10}I+xx^{T}\right).Tr( \left( \dfrac {3}{10}I+xx^{T}\right)^{-1}.\dfrac {dm}{dx})  \dfrac {dm}{dx} = \dfrac {d(xx^{T})}{dx} n \times n \times n Tr(\dfrac {dm}{dx})= 2x \dfrac {df}{dx} = Tr(\left( \dfrac {3}{10}I+xx^{T}\right)^{-1}.\dfrac {d(xx^{T})}{dx}),"['multivariable-calculus', 'derivatives', 'determinant', 'scalar-fields']"
20,The notation $\frac{\partial}{\partial x}$,The notation,\frac{\partial}{\partial x},"In Jost's Riemannian Geometry and Geometric Analysis (Sect. 1.2, Chap. 1), the tangent space at a point $x_0$ in $\mathbb{R}^d$ is defined as $$T_{x_0}\mathbb R^d=\{x_0\}\times E$$ where $E$ is the vector space spanned by $\frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d}$ . Then the books says: ""Here, $\frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d}$ are the partial derivatives at the point $x_0$ ."" This is where I get confused. They are the partial derivatives of what? The only partial derivative I know is that of a function, but no function is given here. Sure, if one wants to argue that $\frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d}$ are just formal notations here that don't mean anything other than a formal basis of $E$ , then I can accept that even though I have doubts. But then there comes something that confuses me even more. If $f:\mathbb R^d\to\mathbb R^c$ is a differentiable map, then the derivative of $f$ at $x_0$ is defined to be (Einstein convention is used below) $$df(x_0):T_{x_0}\mathbb R^d\to T_{x_0}\mathbb{R}^c\\ \quad v^i\frac{\partial}{\partial x^i}\mapsto v^i\frac{\partial f^j}{\partial x^i}\frac{\partial}{\partial f^i}$$ So apparently $\frac{\partial}{\partial f^j}$ here depend on $f$ and are not arbitraily selected, so the notation cannot simply be a formal one, which brings me back to the original question: what does $\frac{\partial}{\partial x^i}$ and $\frac{\partial}{\partial f^j}$ mean?","In Jost's Riemannian Geometry and Geometric Analysis (Sect. 1.2, Chap. 1), the tangent space at a point in is defined as where is the vector space spanned by . Then the books says: ""Here, are the partial derivatives at the point ."" This is where I get confused. They are the partial derivatives of what? The only partial derivative I know is that of a function, but no function is given here. Sure, if one wants to argue that are just formal notations here that don't mean anything other than a formal basis of , then I can accept that even though I have doubts. But then there comes something that confuses me even more. If is a differentiable map, then the derivative of at is defined to be (Einstein convention is used below) So apparently here depend on and are not arbitraily selected, so the notation cannot simply be a formal one, which brings me back to the original question: what does and mean?","x_0 \mathbb{R}^d T_{x_0}\mathbb R^d=\{x_0\}\times E E \frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d} \frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d} x_0 \frac{\partial}{\partial x^1},\cdots,\frac{\partial}{\partial x^d} E f:\mathbb R^d\to\mathbb R^c f x_0 df(x_0):T_{x_0}\mathbb R^d\to T_{x_0}\mathbb{R}^c\\
\quad v^i\frac{\partial}{\partial x^i}\mapsto v^i\frac{\partial f^j}{\partial x^i}\frac{\partial}{\partial f^i} \frac{\partial}{\partial f^j} f \frac{\partial}{\partial x^i} \frac{\partial}{\partial f^j}","['multivariable-calculus', 'derivatives', 'differential-geometry', 'notation']"
21,Clarification on the use of dot product in the formula for a plane's distance to origin,Clarification on the use of dot product in the formula for a plane's distance to origin,,"I decided to take on MIT Multivariable Calculus to get a review for next semester. Having some struggles with this question though and the solutions aren't really helping me out much, thinking I might be seeing this in a naíve way and am missing a step on how they got there. Suppose a plane $ax+by+cz = d$ We are supposed to prove the formula $D = \frac{\mid d \mid}{\sqrt{a²+b²+c²}}$ where D is the distance to the origin. I was having some troubles with the proof, checked the solution and slowly started to get a clue, but need to see if I'm getting this right. So we suppose a $P_0 = (x_0,y_0,z_0)$ and know that the normal vector is $\vec{N} = (a,b,c)$. The solution started like this $\vec{OP}.\frac{\vec{N}}{|\vec{N}|}$ and then by usual operations it got to the formula that we wanted. So my question is, am I supposed to see the dot product as the projection of the vector that connects the origin to a certain point into the direction of the Normal vector? I'm getting that value that will be the minimal distance? The intuition is to see the dot product as some kind of parser that finds the commonalities between 2 vectors and returns the vector that is composed by those commonalities? Also finally, I understand that dividing by the length of the Normal vector gives us the ""unitary"" direction vector right? But I'm not seeing the implication of not doing that, how would it deform the resulting distance? Would it make larger or smaller depending on the length of the normal vector that we choose? If these questions are basic I'm sorry, I never had much love for this class and hopefully will get better in these next few weeks as I watch this course.","I decided to take on MIT Multivariable Calculus to get a review for next semester. Having some struggles with this question though and the solutions aren't really helping me out much, thinking I might be seeing this in a naíve way and am missing a step on how they got there. Suppose a plane $ax+by+cz = d$ We are supposed to prove the formula $D = \frac{\mid d \mid}{\sqrt{a²+b²+c²}}$ where D is the distance to the origin. I was having some troubles with the proof, checked the solution and slowly started to get a clue, but need to see if I'm getting this right. So we suppose a $P_0 = (x_0,y_0,z_0)$ and know that the normal vector is $\vec{N} = (a,b,c)$. The solution started like this $\vec{OP}.\frac{\vec{N}}{|\vec{N}|}$ and then by usual operations it got to the formula that we wanted. So my question is, am I supposed to see the dot product as the projection of the vector that connects the origin to a certain point into the direction of the Normal vector? I'm getting that value that will be the minimal distance? The intuition is to see the dot product as some kind of parser that finds the commonalities between 2 vectors and returns the vector that is composed by those commonalities? Also finally, I understand that dividing by the length of the Normal vector gives us the ""unitary"" direction vector right? But I'm not seeing the implication of not doing that, how would it deform the resulting distance? Would it make larger or smaller depending on the length of the normal vector that we choose? If these questions are basic I'm sorry, I never had much love for this class and hopefully will get better in these next few weeks as I watch this course.",,"['linear-algebra', 'multivariable-calculus', 'vectors']"
22,Why is the dotproduct of direction and gradient the directional derivative? ($\nabla_\hat{v} f = \nabla f \bullet \hat{v}$),Why is the dotproduct of direction and gradient the directional derivative? (),\nabla_\hat{v} f = \nabla f \bullet \hat{v},"I have seen the claim that the directional derivative of $f$ in the direction $\hat{v}$ , where $||\hat{v}|| = 1$ , (denoted $\nabla_\hat{v} f$ ) is equal to the gradient of $\nabla f$ dotted with $\hat{v}$ . I have tried to prove this to myself, but I got stuck: $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\limit}[2]{\lim_{#1 \to #2}}$ $\newcommand{\pderiv}[2]{\dfrac{\partial#1}{\partial#2}}$ Let $f : \R^n \to \R$ I accept that $$ \nabla_{\hat{v}} f = \limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h}$$ for making intuitive sense. Furthermore I know that $$ \pderiv{f}{x_i}  = \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h} $$ where $\hat{i}$ is the unit vector of the $i$ -th dimension. I also know that $$\nabla f =  \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right)$$ Now I want to show that $\nabla_\hat{v} f = \nabla f \bullet \hat{v}$ : \begin{align*} \nabla f \bullet \hat{v} & = \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right) \bullet v\hat{v}\\                     &= \sum_{i = 1}^{n} \pderiv{f}{x_i} \cdot \hat{v}_i\\                     &= \sum_{i = 1}^{n} \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h} \cdot \hat{v}_i\\                     &= \sum_{i = 1}^{n} \limit{h}{0} \frac{\hat{v}_if(x + h\hat{i}) - \hat{v}_if(x)}{h}\\ \end{align*} And now I am stuck. I don't see a way to transform the last line into $\limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h}$ to reach $\nabla_{\hat{v}} f$ Can you help me out here? Edit I now conceptually understand why the dotproduct of direction and gradient is the directional derivative: Let's say we have a differentiable function $f$ from $\mathbb{R}^2$ to $\mathbb{R}^3$ mapping the $xy$ -plane into the $xyz$ -space and  we want to know the directional derivative of $f$ at a point $p = (x', y')$ for some vector $u$ . First what we need to know is, how much $f$ changes in $x$ direction and how much it changes in $y$ direction. Then we need to realize that for small distances whatever direction we go along the surface of $f$ , the total change in height is the sum of the change in height in the $x$ component of our direction and the change in height in $y$ component in our direction. But now we can weight the partial derivatives for the $x$ and $y$ direction with the components of $u$ to get their individual contributions for the direction in which $u$ is pointing! So if $u$ has an $x$ -component of $u_x$ we weight the partial derivative of $f$ for the $x$ direction accordingly. When we do the same for $y$ we get: $f'(x') \cdot u_x + f'(y') \cdot u_y$ which is in fact $\nabla f \bullet u$ I understood this after listeing to this lecture: https://www.youtube.com/watch?v=tDPp5uWSIiU","I have seen the claim that the directional derivative of in the direction , where , (denoted ) is equal to the gradient of dotted with . I have tried to prove this to myself, but I got stuck: Let I accept that for making intuitive sense. Furthermore I know that where is the unit vector of the -th dimension. I also know that Now I want to show that : And now I am stuck. I don't see a way to transform the last line into to reach Can you help me out here? Edit I now conceptually understand why the dotproduct of direction and gradient is the directional derivative: Let's say we have a differentiable function from to mapping the -plane into the -space and  we want to know the directional derivative of at a point for some vector . First what we need to know is, how much changes in direction and how much it changes in direction. Then we need to realize that for small distances whatever direction we go along the surface of , the total change in height is the sum of the change in height in the component of our direction and the change in height in component in our direction. But now we can weight the partial derivatives for the and direction with the components of to get their individual contributions for the direction in which is pointing! So if has an -component of we weight the partial derivative of for the direction accordingly. When we do the same for we get: which is in fact I understood this after listeing to this lecture: https://www.youtube.com/watch?v=tDPp5uWSIiU","f \hat{v} ||\hat{v}|| = 1 \nabla_\hat{v} f \nabla f \hat{v} \newcommand{\R}{\mathbb{R}} \newcommand{\limit}[2]{\lim_{#1 \to #2}} \newcommand{\pderiv}[2]{\dfrac{\partial#1}{\partial#2}} f : \R^n \to \R  \nabla_{\hat{v}} f = \limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h}  \pderiv{f}{x_i}  = \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h}  \hat{i} i \nabla f =  \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right) \nabla_\hat{v} f = \nabla f \bullet \hat{v} \begin{align*}
\nabla f \bullet \hat{v} & = \left( \pderiv{f}{x_1}, \dots, \pderiv{f}{x_n} \right) \bullet v\hat{v}\\
                    &= \sum_{i = 1}^{n} \pderiv{f}{x_i} \cdot \hat{v}_i\\
                    &= \sum_{i = 1}^{n} \limit{h}{0} \frac{f(x + h\hat{i}) - f(x)}{h} \cdot \hat{v}_i\\
                    &= \sum_{i = 1}^{n} \limit{h}{0} \frac{\hat{v}_if(x + h\hat{i}) - \hat{v}_if(x)}{h}\\
\end{align*} \limit{h}{0} \frac{f(x + h\hat{v}) - f(x)}{h} \nabla_{\hat{v}} f f \mathbb{R}^2 \mathbb{R}^3 xy xyz f p = (x', y') u f x y f x y x y u u u x u_x f x y f'(x') \cdot u_x + f'(y') \cdot u_y \nabla f \bullet u","['multivariable-calculus', 'vectors', 'vector-analysis']"
23,What is the Fréchet derivative?,What is the Fréchet derivative?,,"I'm sorry if I sound too ignorant, i don't have a high level of knowledge in math I've been lately trying to understand the Frèchet derivative. I'm just starting Calc II, but I have a tiny grasp in multivariable calculus. That is, I understand that one can treat some of the variables as constants and get a directional derivative. According to wikipedia, if the limit of the equation below as $h$ tends to $0$ is equal to $0$ then the function is said to be Fréchet differentiable at $x$. $$\frac{||f(x+h)-f(x)-T(x)||}{||h||}$$ And as much as this can work as a definition, I'm still wondering what the Fréchet derivative actually is. My confusion might also come from the fact that there are some ideas in multi-variable calculus I don't get, so I will try to summarize my questions below. Q1. If I understood correctly, $f$ could be a multi-variable function, so how come we use only one $x$? Is it because $x$ is itself a multi-variable vector? Q2. What is the meaning of $T(h)$? As in, what is it, and what is it doing in this equation? Q3. Why does this equation -when $h$ tends to $0$- somehow tells whether a function is (Fréchet) differentiable or not? How does this equation relates to what a Fréchet derivative is? Any help/thoughts would be really appreciated.","I'm sorry if I sound too ignorant, i don't have a high level of knowledge in math I've been lately trying to understand the Frèchet derivative. I'm just starting Calc II, but I have a tiny grasp in multivariable calculus. That is, I understand that one can treat some of the variables as constants and get a directional derivative. According to wikipedia, if the limit of the equation below as $h$ tends to $0$ is equal to $0$ then the function is said to be Fréchet differentiable at $x$. $$\frac{||f(x+h)-f(x)-T(x)||}{||h||}$$ And as much as this can work as a definition, I'm still wondering what the Fréchet derivative actually is. My confusion might also come from the fact that there are some ideas in multi-variable calculus I don't get, so I will try to summarize my questions below. Q1. If I understood correctly, $f$ could be a multi-variable function, so how come we use only one $x$? Is it because $x$ is itself a multi-variable vector? Q2. What is the meaning of $T(h)$? As in, what is it, and what is it doing in this equation? Q3. Why does this equation -when $h$ tends to $0$- somehow tells whether a function is (Fréchet) differentiable or not? How does this equation relates to what a Fréchet derivative is? Any help/thoughts would be really appreciated.",,"['calculus', 'multivariable-calculus', 'derivatives', 'definition', 'frechet-derivative']"
24,Can indefinite double integrals be solved by change of variables technique?,Can indefinite double integrals be solved by change of variables technique?,,"I have a simple double integral: $$\int\int(x+y)dxdy$$ Now, change the variables according to the following scheme:  $$x=u+v$$ $$y=u-v$$ The jacobian is then: $$|\frac{\partial (x,y)}{\partial (u,v)}|=1$$ Substitute u and v into the integral, solving the integral and substituting y and x back in their place equals: $$(\frac{x-y}{2})(\frac{x+y}{2})^2$$ Which is obviously not the same as the result from directly integrating: $yx(\frac{x+y}{2})$ So my question is, can this technique not be used for solving double integrals, and is there perhaps any way to make the technique work (unless I have made a mistake somewhere).","I have a simple double integral: $$\int\int(x+y)dxdy$$ Now, change the variables according to the following scheme:  $$x=u+v$$ $$y=u-v$$ The jacobian is then: $$|\frac{\partial (x,y)}{\partial (u,v)}|=1$$ Substitute u and v into the integral, solving the integral and substituting y and x back in their place equals: $$(\frac{x-y}{2})(\frac{x+y}{2})^2$$ Which is obviously not the same as the result from directly integrating: $yx(\frac{x+y}{2})$ So my question is, can this technique not be used for solving double integrals, and is there perhaps any way to make the technique work (unless I have made a mistake somewhere).",,"['multivariable-calculus', 'indefinite-integrals', 'substitution', 'jacobian', 'change-of-variable']"
25,Intuitive way to understand Polar Coordinate Gradient,Intuitive way to understand Polar Coordinate Gradient,,"I am looking for an intuitive way to explain the ""$1/r$"" factor in the gradient in polar coordinates. For instance, if $g(x,y)=f(r,\theta)$, $$\nabla g=f_r\hat{e_r}+\frac 1rf_\theta\hat{e_\theta}$$ Is there a way to explain the $\frac 1r$ factor? By dimension matching? Or any other way to see that the answer is not $$\nabla g=f_r\hat{e_r}+f_\theta\hat{e_\theta}$$ Thanks for any help.","I am looking for an intuitive way to explain the ""$1/r$"" factor in the gradient in polar coordinates. For instance, if $g(x,y)=f(r,\theta)$, $$\nabla g=f_r\hat{e_r}+\frac 1rf_\theta\hat{e_\theta}$$ Is there a way to explain the $\frac 1r$ factor? By dimension matching? Or any other way to see that the answer is not $$\nabla g=f_r\hat{e_r}+f_\theta\hat{e_\theta}$$ Thanks for any help.",,['multivariable-calculus']
26,"Critical point of a function of two variables $f(x,y)=3x^4 -4x^2y+y^2$: minimum, maximum or saddle?","Critical point of a function of two variables : minimum, maximum or saddle?","f(x,y)=3x^4 -4x^2y+y^2","If $f(x,y)=3x^4 -4x^2y+y^2$ then : a) $(0,0)$ is local max point b) $(0,0)$ is local min point c) $(0,0)$ is saddle point d) none I think we find critical points so we must find all point of equation $\nabla f =0$ then $(0,0)$ is critical point but $f_{x,x} = f_{y,y} =f_{x,y}=0$ now  we can't use  this test.",If then : a) is local max point b) is local min point c) is saddle point d) none I think we find critical points so we must find all point of equation then is critical point but now  we can't use  this test.,"f(x,y)=3x^4 -4x^2y+y^2 (0,0) (0,0) (0,0) \nabla f =0 (0,0) f_{x,x} = f_{y,y} =f_{x,y}=0","['calculus', 'multivariable-calculus', 'optimization']"
27,Proving limits with epsilon delta for Multivariable Functions,Proving limits with epsilon delta for Multivariable Functions,,"I am very stuck on this question on finding a particular delta that would finish the proof of this limit for multi variable function. Prove that $ \displaystyle \lim_{(x,y)→(0,0)} (5x^{3}-x^{2}y^{2})=0$ I don't know how I can bound this function after I factor out $x^{2}$ from the function... I know this is a polynomial function and all polynomial functions are continuous on $\mathbb{R}^{2}$ so we can just directly substitute stuff in but need to prove using epsilon - delta technique.",I am very stuck on this question on finding a particular delta that would finish the proof of this limit for multi variable function. Prove that I don't know how I can bound this function after I factor out from the function... I know this is a polynomial function and all polynomial functions are continuous on so we can just directly substitute stuff in but need to prove using epsilon - delta technique.," \displaystyle \lim_{(x,y)→(0,0)} (5x^{3}-x^{2}y^{2})=0 x^{2} \mathbb{R}^{2}",['multivariable-calculus']
28,What's the best way to think about the Hessian?,What's the best way to think about the Hessian?,,"I've always thought about the Hessian like this: Let $f:\mathbb R^n \to \mathbb R$ be smooth.  Let $g:\mathbb R^n \to \mathbb R^n$ such that $g(x) = \nabla f(x)$.  (I am using the convention that $\nabla f(x)$ is a column vector.)  Then the Hessian of $f$ at $x$ is, by my definition, the $n \times n$ matrix $H(x) = g'(x)$. However, with this way of looking at the Hessian, I'm not thinking of $H(x)$ as being a quadratic form.  I'm worried that there is a ""quadratic form"" viewpoint of the Hessian that I am missing.  There is a rule of thumb I've heard that when a matrix (such as the Hessian) is automatically symmetric, then it's often most natural to think of it as defining a quadratic form.  I realize that you can define a quadratic form at $x_0$ by $x \mapsto \langle x, H(x_0) x \rangle$, and that this quadratic form appears in Taylor's formula.  But I still think I'm missing something, because I don't see why it is fundamentally most natural to think of the Hessian as being a quadratic form. Is it true that there is a quadratic form viewpoint that I'm missing out on?  If so, what is it?  More generally, what do you think is the best way to think about the Hessian?","I've always thought about the Hessian like this: Let $f:\mathbb R^n \to \mathbb R$ be smooth.  Let $g:\mathbb R^n \to \mathbb R^n$ such that $g(x) = \nabla f(x)$.  (I am using the convention that $\nabla f(x)$ is a column vector.)  Then the Hessian of $f$ at $x$ is, by my definition, the $n \times n$ matrix $H(x) = g'(x)$. However, with this way of looking at the Hessian, I'm not thinking of $H(x)$ as being a quadratic form.  I'm worried that there is a ""quadratic form"" viewpoint of the Hessian that I am missing.  There is a rule of thumb I've heard that when a matrix (such as the Hessian) is automatically symmetric, then it's often most natural to think of it as defining a quadratic form.  I realize that you can define a quadratic form at $x_0$ by $x \mapsto \langle x, H(x_0) x \rangle$, and that this quadratic form appears in Taylor's formula.  But I still think I'm missing something, because I don't see why it is fundamentally most natural to think of the Hessian as being a quadratic form. Is it true that there is a quadratic form viewpoint that I'm missing out on?  If so, what is it?  More generally, what do you think is the best way to think about the Hessian?",,"['multivariable-calculus', 'hessian-matrix']"
29,"If every composition of a differentiable path and a function is differentiable at 0, means the function is differentiable at 0","If every composition of a differentiable path and a function is differentiable at 0, means the function is differentiable at 0",,"I'll write the question more formaly: Let $f :\mathbb{R^n} \rightarrow \mathbb{R}$ a certain function. Assume that for every differentiable path $p: [-1,1] \rightarrow \mathbb{R^n}$ so that $p(0) = 0 $ the composition $f(p(t))$ is differentiable at $t = 0$.  I need to prove that $f$ is differentiable at the zero vector. I came up with something, but i'm not sure if its correct, or too convoluted. What I came up with, was to assume by contradiction that $f$ is not differentaible. So I just need to find a path which makes the composition $f(p(t))$ not diffferentiable. Because $f$ is not differentiable at 0, by definition for every liner map $A:\mathbb{R^n} \rightarrow \mathbb{R}$ the expression: $$g(x) = \frac{|f(x) - f(0) - <A(0),x>|}{||x||}\not\rightarrow 0$$ So by the limit definition there exists a sequence $x_n \rightarrow 0$ so that $g( x_n ) \not\rightarrow 0 $. Again by definition because of the convergence of $x_n$, For every $\epsilon > 0$ there is a $n_0 \in \mathbb{N}$ so for every $n > n_0$, $x_n \in B(0,\epsilon)$. Now if I take a path inside this ball, perhaps just a straight line inside this ball, it will be a differentiable path. If I compose it with $f$ I need to show that: $$h(t) = \frac{|f(p(t)) - f(0) - a*t|}{|t|}  \not\rightarrow 0$$ So I just need one sequence $t_n \rightarrow 0$ so that $h(t_n) \not\rightarrow 0$. I'm sort of unsure of the next step, $g(x)$ and $h(t)$ look quite similiar. I can fix a linear map $A$ so that for every $x \not= 0$, $<A(0),x> = a*t_n$ for every $a \in \mathbb{R}$. And then because for every $t_n$, $p(t_n) = x$ so that $x \in B(0,\epsilon)$,  $h(t_n)$ and $g(x_n)$ will behave the same way, therefore $h(t_n)$ wont converge to $0$. This last part I'm having a hard time to explain formally, and also something seems wrong, but I can't quite figure out what. Any help will be greatly appreciated.","I'll write the question more formaly: Let $f :\mathbb{R^n} \rightarrow \mathbb{R}$ a certain function. Assume that for every differentiable path $p: [-1,1] \rightarrow \mathbb{R^n}$ so that $p(0) = 0 $ the composition $f(p(t))$ is differentiable at $t = 0$.  I need to prove that $f$ is differentiable at the zero vector. I came up with something, but i'm not sure if its correct, or too convoluted. What I came up with, was to assume by contradiction that $f$ is not differentaible. So I just need to find a path which makes the composition $f(p(t))$ not diffferentiable. Because $f$ is not differentiable at 0, by definition for every liner map $A:\mathbb{R^n} \rightarrow \mathbb{R}$ the expression: $$g(x) = \frac{|f(x) - f(0) - <A(0),x>|}{||x||}\not\rightarrow 0$$ So by the limit definition there exists a sequence $x_n \rightarrow 0$ so that $g( x_n ) \not\rightarrow 0 $. Again by definition because of the convergence of $x_n$, For every $\epsilon > 0$ there is a $n_0 \in \mathbb{N}$ so for every $n > n_0$, $x_n \in B(0,\epsilon)$. Now if I take a path inside this ball, perhaps just a straight line inside this ball, it will be a differentiable path. If I compose it with $f$ I need to show that: $$h(t) = \frac{|f(p(t)) - f(0) - a*t|}{|t|}  \not\rightarrow 0$$ So I just need one sequence $t_n \rightarrow 0$ so that $h(t_n) \not\rightarrow 0$. I'm sort of unsure of the next step, $g(x)$ and $h(t)$ look quite similiar. I can fix a linear map $A$ so that for every $x \not= 0$, $<A(0),x> = a*t_n$ for every $a \in \mathbb{R}$. And then because for every $t_n$, $p(t_n) = x$ so that $x \in B(0,\epsilon)$,  $h(t_n)$ and $g(x_n)$ will behave the same way, therefore $h(t_n)$ wont converge to $0$. This last part I'm having a hard time to explain formally, and also something seems wrong, but I can't quite figure out what. Any help will be greatly appreciated.",,"['multivariable-calculus', 'derivatives']"
30,Does the Laplacian and gradient commute?,Does the Laplacian and gradient commute?,,"If I have function $u: \mathbb{R}^n \longrightarrow \mathbb{R}$ smooth, does it always hold that: $$\nabla^2(\nabla u)= \nabla(\nabla^2 u)$$ this is true in general?","If I have function $u: \mathbb{R}^n \longrightarrow \mathbb{R}$ smooth, does it always hold that: $$\nabla^2(\nabla u)= \nabla(\nabla^2 u)$$ this is true in general?",,['multivariable-calculus']
31,Sum of Gaussian Variables may not Gaussian,Sum of Gaussian Variables may not Gaussian,,"I am currently trying to understand the following three points which we discussed in lectures recently: We say that $X=(X_1,\ldots,X_d)$ is $d$-dimensional multivariate Gaussian distributed if $X\sim N(\mu,Q)$ for some $\mu\in\mathbb R^d$, $Q=(q_{ij})\in\mathbb R^{d\times d}$, that is, $X_i\sim N(\mu_i, q_{ii})$ and $\text{cov}(X_i,X_j)=q_{ij}$. There holds: $X=(X_1,\ldots,X_d)$ is $p$-dimensional multi-variate Gaussian distributed if and only if any linear combination of $X_1,\ldots,X_d$ is Gaussian. If $X,Y$ are Gaussian variables, then $X+Y$ is not be Gaussian (in general). This only holds true if $X,Y$ are indepenent. I get the feeling that the ""multivariate Gaussian"" definition in point (1) is somewhat wrong (or incomplete), because otherwise (2) and (3) would contradict each other. But (2) seems correct (as I have found it in many other lecture notes online) and (3) seems correct as well (because of Simon Nickerson's comment in Proof that the sum of two Gaussian variables is another Gaussian ). I know there are other definitions for ""multivariate Gaussian"", which do not contradict (2) and (3), but I am basically wondering whether there is any way of fixing the definition I have got, or is it just plain wrong?","I am currently trying to understand the following three points which we discussed in lectures recently: We say that $X=(X_1,\ldots,X_d)$ is $d$-dimensional multivariate Gaussian distributed if $X\sim N(\mu,Q)$ for some $\mu\in\mathbb R^d$, $Q=(q_{ij})\in\mathbb R^{d\times d}$, that is, $X_i\sim N(\mu_i, q_{ii})$ and $\text{cov}(X_i,X_j)=q_{ij}$. There holds: $X=(X_1,\ldots,X_d)$ is $p$-dimensional multi-variate Gaussian distributed if and only if any linear combination of $X_1,\ldots,X_d$ is Gaussian. If $X,Y$ are Gaussian variables, then $X+Y$ is not be Gaussian (in general). This only holds true if $X,Y$ are indepenent. I get the feeling that the ""multivariate Gaussian"" definition in point (1) is somewhat wrong (or incomplete), because otherwise (2) and (3) would contradict each other. But (2) seems correct (as I have found it in many other lecture notes online) and (3) seems correct as well (because of Simon Nickerson's comment in Proof that the sum of two Gaussian variables is another Gaussian ). I know there are other definitions for ""multivariate Gaussian"", which do not contradict (2) and (3), but I am basically wondering whether there is any way of fixing the definition I have got, or is it just plain wrong?",,"['multivariable-calculus', 'normal-distribution']"
32,"Find the shortest distance between the point $(8,3,2)$ and the line through the points $(1,2,1)$ and $(0,4,0)$",Find the shortest distance between the point  and the line through the points  and,"(8,3,2) (1,2,1) (0,4,0)","""Find the shortest distance between the point $(8,3,2)$ and the line through the points $(1,2,1)$ and $(0,4,0)$"" $$P = (1,2,1), Q = (0,4,0), A = (8,3,2)$$ $OP$ = vector to $P$ $$PQ_ = (0,4,0) - (1,2,1)$$ I found that the equation of the line $L$ that passes through $(1,2,1)$ and $(0,4,0)$ is: $$L = OP + PQ \, t;$$ $$L = (1,2,1) + (-1,2,-1) \, t .$$ However after this I'm not sure how to proceed. I can find PA_ then draw a line from $A$ to the line $L$... advice?","""Find the shortest distance between the point $(8,3,2)$ and the line through the points $(1,2,1)$ and $(0,4,0)$"" $$P = (1,2,1), Q = (0,4,0), A = (8,3,2)$$ $OP$ = vector to $P$ $$PQ_ = (0,4,0) - (1,2,1)$$ I found that the equation of the line $L$ that passes through $(1,2,1)$ and $(0,4,0)$ is: $$L = OP + PQ \, t;$$ $$L = (1,2,1) + (-1,2,-1) \, t .$$ However after this I'm not sure how to proceed. I can find PA_ then draw a line from $A$ to the line $L$... advice?",,['multivariable-calculus']
33,"Is $f(x,y) = ( \frac{x}{x^2+y^2},\frac{2y}{x^2+y^2} )$ injective?",Is  injective?,"f(x,y) = ( \frac{x}{x^2+y^2},\frac{2y}{x^2+y^2} )","I came across this function in a context in which I need to know if it is injective. The function is $f:\mathbb{R}^2\backslash\{(0,0)\} \longrightarrow \mathbb{R}^2\backslash\{(0,0)\}$ and defined by $$f(x,y) = \Biggl( \frac{x}{x^2+y^2},\frac{2y}{x^2+y^2} \Biggl).$$ I guess it is injective. I have failed to find counterexamples, but I couldn't prove that it is injective. Maybe I should use some result, but I couldn't find any that seemed useful. Hints or answers are much appreciated.","I came across this function in a context in which I need to know if it is injective. The function is $f:\mathbb{R}^2\backslash\{(0,0)\} \longrightarrow \mathbb{R}^2\backslash\{(0,0)\}$ and defined by $$f(x,y) = \Biggl( \frac{x}{x^2+y^2},\frac{2y}{x^2+y^2} \Biggl).$$ I guess it is injective. I have failed to find counterexamples, but I couldn't prove that it is injective. Maybe I should use some result, but I couldn't find any that seemed useful. Hints or answers are much appreciated.",,['multivariable-calculus']
34,Multivariate Taylor Series Derivation (2D),Multivariate Taylor Series Derivation (2D),,"I am trying to understand the derivation here kiwi.atmos.colostate.edu/group/dave/pdf/TaylorSeries.pdf I understand how first, second total differentials are derived. I do not understand how they are plugged into a form that is compatible with the single variable Taylor Series, which is $f(x) = f(a) + f'(a)(x-a) + \frac{1}{2!}f''(a) (x-a)^2 + ...$ The coefficients of multivariate expansion are exactly the same as the single variable version. Sure given the total differential, $(x-a)$ is substituted for $dx$, $(y-b)$ for $dy$, but it's not shown why, or how the rest of $f(x,y)$ expansion should mirror the single variable form based on the total differential. Expansion for $f(x,y)$ is below $f(x,y) = f(a,b) +  \bigg[(x-a)\frac{\partial}{\partial x} + (y-b)\frac{\partial}{\partial x}\bigg]f +  \frac{1}{2!} \bigg[(x-a)\frac{\partial}{\partial x} + (y-b)\frac{\partial}{\partial x}\bigg]^2 f + ... $ If you know a better 2D Taylor Series derivation, that would be welcome as well. Thanks,","I am trying to understand the derivation here kiwi.atmos.colostate.edu/group/dave/pdf/TaylorSeries.pdf I understand how first, second total differentials are derived. I do not understand how they are plugged into a form that is compatible with the single variable Taylor Series, which is $f(x) = f(a) + f'(a)(x-a) + \frac{1}{2!}f''(a) (x-a)^2 + ...$ The coefficients of multivariate expansion are exactly the same as the single variable version. Sure given the total differential, $(x-a)$ is substituted for $dx$, $(y-b)$ for $dy$, but it's not shown why, or how the rest of $f(x,y)$ expansion should mirror the single variable form based on the total differential. Expansion for $f(x,y)$ is below $f(x,y) = f(a,b) +  \bigg[(x-a)\frac{\partial}{\partial x} + (y-b)\frac{\partial}{\partial x}\bigg]f +  \frac{1}{2!} \bigg[(x-a)\frac{\partial}{\partial x} + (y-b)\frac{\partial}{\partial x}\bigg]^2 f + ... $ If you know a better 2D Taylor Series derivation, that would be welcome as well. Thanks,",,"['calculus', 'differential-geometry', 'multivariable-calculus']"
35,Tangent Plane to a regular surface,Tangent Plane to a regular surface,,"I'd like some help with the following question: Find the equation of the tangent plane in $(x_0,y_0,z_0)$ to a regular surface given by $f^{-1}(0)$, where $0$ is a regular value. I tried to find local parametrization but It didn't work. Thanks.","I'd like some help with the following question: Find the equation of the tangent plane in $(x_0,y_0,z_0)$ to a regular surface given by $f^{-1}(0)$, where $0$ is a regular value. I tried to find local parametrization but It didn't work. Thanks.",,"['calculus', 'differential-geometry', 'multivariable-calculus']"
36,"Proving function is differentiable at $(0,0)$ using total derivative definition",Proving function is differentiable at  using total derivative definition,"(0,0)","I am looking at the function: $$\frac{x\sin(xy)}{\sqrt{x^2+y^2}}$$ at $(0,0)$ , where the function at $(0,0)$ is defined to be $0$ . I understand that I need to check that: $$\lim_{{h \to 0}} \frac{{f(\mathbf{a} + \mathbf{h}) - f(\mathbf{a}) - A\mathbf{h}}}{{\|\mathbf{h}\|}} = \lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} = 0 \ .$$ Since $D_1f$ and $D_2f$ are equal to zero, I believe that A is the matrix $(0, 0)$ . I've tried going through different paths to see if I get a different limit when doing: $$\lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} \ ,$$ and all of them give me $0$ . Therefore, I believe that function is differentiable at $(0,0)$ . But how can I prove it more soundly? Thank you!","I am looking at the function: at , where the function at is defined to be . I understand that I need to check that: Since and are equal to zero, I believe that A is the matrix . I've tried going through different paths to see if I get a different limit when doing: and all of them give me . Therefore, I believe that function is differentiable at . But how can I prove it more soundly? Thank you!","\frac{x\sin(xy)}{\sqrt{x^2+y^2}} (0,0) (0,0) 0 \lim_{{h \to 0}} \frac{{f(\mathbf{a} + \mathbf{h}) - f(\mathbf{a}) - A\mathbf{h}}}{{\|\mathbf{h}\|}} = \lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} = 0 \ . D_1f D_2f (0, 0) \lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} \ , 0 (0,0)","['multivariable-calculus', 'derivatives']"
37,prove (or disprove) $\left|x^\top\left(\frac{x}{|x|^\frac{1}{2}}-\frac{y+x}{|y+x|^\frac{1}{2}}\right)\right|\le 3|x||y|^\frac{1}{2}$,prove (or disprove),\left|x^\top\left(\frac{x}{|x|^\frac{1}{2}}-\frac{y+x}{|y+x|^\frac{1}{2}}\right)\right|\le 3|x||y|^\frac{1}{2},"As stated in the title, I need help showing (or disproving) $$\left|x^\top\left(\frac{x}{|x|^\frac{1}{2}}-\frac{y+x}{|y+x|^\frac{1}{2}}\right)\right|\le 3|x||y|^\frac{1}{2}$$ for $x, y\in\mathbb{R}^N$ . Here, we take $|\cdot|$ to be the Euclidean norm and we interpret $\frac{x}{|x|^\frac{1}{2}}$ to be $0$ at $x=0$ . I have already shown it to be true for scalar x, y but I am having trouble proving (or disproving) it for the vector valued case. I tried several approaches including inducting on the dimension, finding scalars $\tilde{x}, \tilde{y}$ corresponding to $x$ and $y$ (with $|\tilde{x}|<|x|$ and $|\tilde{y}|<|y|$ ) and upper bounding the vector valued expression by the scalar one, but none of these approaches have really panned out for me. I even wrote a MATLAB script to look for a counter example but cannot find one so far. This problem is relevant to a research problem I am working on so any help would be appreciated. Thanks! (EDIT: it should be noted that it may possible one could improve the 3 with a smaller constant, but at the moment I can't find an optimal value. However, for my purposes showing the statement with any constant in place of the 3 will suffice)","As stated in the title, I need help showing (or disproving) for . Here, we take to be the Euclidean norm and we interpret to be at . I have already shown it to be true for scalar x, y but I am having trouble proving (or disproving) it for the vector valued case. I tried several approaches including inducting on the dimension, finding scalars corresponding to and (with and ) and upper bounding the vector valued expression by the scalar one, but none of these approaches have really panned out for me. I even wrote a MATLAB script to look for a counter example but cannot find one so far. This problem is relevant to a research problem I am working on so any help would be appreciated. Thanks! (EDIT: it should be noted that it may possible one could improve the 3 with a smaller constant, but at the moment I can't find an optimal value. However, for my purposes showing the statement with any constant in place of the 3 will suffice)","\left|x^\top\left(\frac{x}{|x|^\frac{1}{2}}-\frac{y+x}{|y+x|^\frac{1}{2}}\right)\right|\le 3|x||y|^\frac{1}{2} x, y\in\mathbb{R}^N |\cdot| \frac{x}{|x|^\frac{1}{2}} 0 x=0 \tilde{x}, \tilde{y} x y |\tilde{x}|<|x| |\tilde{y}|<|y|","['linear-algebra', 'multivariable-calculus', 'inequality', 'upper-lower-bounds']"
38,What is the difference/relationship between the gradient and the Jacobian?,What is the difference/relationship between the gradient and the Jacobian?,,"What is the difference/relationship between the gradient and the Jacobian? I think it has to do with vectors/covectors, tangent spaces / contangent spaces, but I'm not sure what's going on. (A gentle and readable but thorough reference in addition to an answer would be appreciated too.) Let $R$ be the real numbers. Let $f: R^2 \longrightarrow R$ be a smooth function. Let $p \in R^2$ . (The Jacobian is typically defined for (differentiable?) functions $f: R^n \longrightarrow R^m$ , but just set $m$ to 1.) Now the gradient is a vector field, whose value at $p$ is a (column) vector (in particular, not a covector): $$\nabla f(p) := \left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]^T$$ (with the convention that vectors are column vectors and covectors are row vectors). And the Jacobian $Jf(p)$ is the $1 \times 2$ matrix (or just the row vector, ie. covector? what's the rigorous difference?): $$Jf(p) := \left[\left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]\right].$$ In particular, since the gradient and the Jacobian are just ""transposes"" (duals) of each other, when would you use one or the other? What's the point? There are related questions here , here , here , but I'm hoping for more insights.","What is the difference/relationship between the gradient and the Jacobian? I think it has to do with vectors/covectors, tangent spaces / contangent spaces, but I'm not sure what's going on. (A gentle and readable but thorough reference in addition to an answer would be appreciated too.) Let be the real numbers. Let be a smooth function. Let . (The Jacobian is typically defined for (differentiable?) functions , but just set to 1.) Now the gradient is a vector field, whose value at is a (column) vector (in particular, not a covector): (with the convention that vectors are column vectors and covectors are row vectors). And the Jacobian is the matrix (or just the row vector, ie. covector? what's the rigorous difference?): In particular, since the gradient and the Jacobian are just ""transposes"" (duals) of each other, when would you use one or the other? What's the point? There are related questions here , here , here , but I'm hoping for more insights.",R f: R^2 \longrightarrow R p \in R^2 f: R^n \longrightarrow R^m m p \nabla f(p) := \left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]^T Jf(p) 1 \times 2 Jf(p) := \left[\left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]\right].,"['calculus', 'multivariable-calculus', 'differential-geometry', 'differential-forms']"
39,Find limit using generalized binomial theorem.,Find limit using generalized binomial theorem.,,"I'm trying to prove that $$ \lim_{(x,y)\rightarrow (0,0)}\left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| = 0 $$ For every $\beta >1$ . So I'm trying to use that $$ \left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| \leq \frac{(x+y)^\beta}{\sqrt{x^2+y^2}} $$ And when I put the right limit in WolframAlpha with $\beta$ near to $1$ , always returns that the limit is $1$ so I guess it could be true for every $\beta >1$ . But I don't know how to prove it . I've tried to use some kind of generalization of the binomial theorem to generalize the argument given here But I haven't been able to do it yet. Any hints or counterexample?","I'm trying to prove that For every . So I'm trying to use that And when I put the right limit in WolframAlpha with near to , always returns that the limit is so I guess it could be true for every . But I don't know how to prove it . I've tried to use some kind of generalization of the binomial theorem to generalize the argument given here But I haven't been able to do it yet. Any hints or counterexample?","
\lim_{(x,y)\rightarrow (0,0)}\left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| = 0
 \beta >1 
\left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| \leq \frac{(x+y)^\beta}{\sqrt{x^2+y^2}}
 \beta 1 1 \beta >1","['real-analysis', 'multivariable-calculus']"
40,Intuitive Proof of the Multivariable Chain Rule,Intuitive Proof of the Multivariable Chain Rule,,"Is there any intuitive or more ""dumbed-down"" proof of the formula $$f_u = \sum_{j=1}^n f_{x_j} \frac{\partial x_j}{\partial u}$$ Because I don't seem to be able to understand where it comes from.","Is there any intuitive or more ""dumbed-down"" proof of the formula Because I don't seem to be able to understand where it comes from.",f_u = \sum_{j=1}^n f_{x_j} \frac{\partial x_j}{\partial u},"['calculus', 'multivariable-calculus', 'chain-rule']"
41,Is $xx^T$ the Hessian of anything?,Is  the Hessian of anything?,xx^T,Let $x$ be a vector in $\mathbb{R}^n$ Let $xx^T$ be the outer-product of this vector with itself. Does there exist any function $f(x): \mathbb{R}^n \to \mathbb{R}$ such that $xx^\top$ is its Hessian?,Let be a vector in Let be the outer-product of this vector with itself. Does there exist any function such that is its Hessian?,x \mathbb{R}^n xx^T f(x): \mathbb{R}^n \to \mathbb{R} xx^\top,"['linear-algebra', 'multivariable-calculus', 'optimization', 'convex-optimization']"
42,"Evaluating $\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \sqrt{{x^2+y^2+z^2}}\, dx \,dy \,dz$ by converting to spherical coordinates",Evaluating  by converting to spherical coordinates,"\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \sqrt{{x^2+y^2+z^2}}\, dx \,dy \,dz","I would like to know how to evaluate the following triple integral with the help of spherical coordinates $$\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \sqrt{{x^2+y^2+z^2}} \,dx \,dy\, dz$$ The relations between Cartesian coordinates and spherical ones are given by ${\displaystyle {\begin{aligned}x&=r\,\sin \theta \,\cos \varphi \\y&=r\,\sin \theta \,\sin \varphi \\z&=r\,\cos \theta \end{aligned}}}$ I know that a function is generally integrated over $\mathbb{R}^3$ by the following triple integral $$ \ \int \limits _{\varphi =0}^{2\pi }\ \int \limits _{\theta =0}^{\pi }\ \int \limits _{r=0}^{\infty }f(r,\theta ,\varphi )r^{2}\sin \theta \,\mathrm {d} r\,\mathrm {d} \theta \,\mathrm {d} \varphi .$$ I found a numerical solution with Wolfram Alpha (0.960592), I tried to change the bounds of integration from Cartesian to spherical but I got a different numerical value. Could someone please give a detailed solution showing how to change the limits of integration? Thanks.","I would like to know how to evaluate the following triple integral with the help of spherical coordinates The relations between Cartesian coordinates and spherical ones are given by I know that a function is generally integrated over by the following triple integral I found a numerical solution with Wolfram Alpha (0.960592), I tried to change the bounds of integration from Cartesian to spherical but I got a different numerical value. Could someone please give a detailed solution showing how to change the limits of integration? Thanks.","\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \sqrt{{x^2+y^2+z^2}} \,dx \,dy\, dz {\displaystyle {\begin{aligned}x&=r\,\sin \theta \,\cos \varphi \\y&=r\,\sin \theta \,\sin \varphi \\z&=r\,\cos \theta \end{aligned}}} \mathbb{R}^3  \ \int \limits _{\varphi =0}^{2\pi }\ \int \limits _{\theta =0}^{\pi }\ \int \limits _{r=0}^{\infty }f(r,\theta ,\varphi )r^{2}\sin \theta \,\mathrm {d} r\,\mathrm {d} \theta \,\mathrm {d} \varphi .","['multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
43,Directional derivative for a piece-wise function,Directional derivative for a piece-wise function,,"Consider the function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ given by $$f(x,y)=\begin{cases}  \frac{xy}{x^2+y^2} &\text{if} (x,y)\neq (0,0)\\ 0 &\text{if} (x,y)=(0,0) \end{cases}$$ For which vectors $v=(v_1,v_2)\neq(0,0)\in\mathbb{R}^2$ does the directional derivative $D_vf(0,0)$ exist? Evaluate the directional derivative wherever it exists. I've managed to get this down to $$D_vf(0,0)=\lim_{h\to 0}\frac{v_1v_2}{h(v_1^2+v_2^2)}$$ and as far as I'm aware this can only exist if one of $v_1$ or $v_2$ is $0$, but not both by the original condition. If this is the case does that mean the directional derivative only exits for $v=(v_1,0) \; \text{or}  \; v=(0,v_2)$? If that is the case, how would I find the value of the directional derivative as it comes down to $D_vf(0,0)=\frac{0}{0}$?","Consider the function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ given by $$f(x,y)=\begin{cases}  \frac{xy}{x^2+y^2} &\text{if} (x,y)\neq (0,0)\\ 0 &\text{if} (x,y)=(0,0) \end{cases}$$ For which vectors $v=(v_1,v_2)\neq(0,0)\in\mathbb{R}^2$ does the directional derivative $D_vf(0,0)$ exist? Evaluate the directional derivative wherever it exists. I've managed to get this down to $$D_vf(0,0)=\lim_{h\to 0}\frac{v_1v_2}{h(v_1^2+v_2^2)}$$ and as far as I'm aware this can only exist if one of $v_1$ or $v_2$ is $0$, but not both by the original condition. If this is the case does that mean the directional derivative only exits for $v=(v_1,0) \; \text{or}  \; v=(0,v_2)$? If that is the case, how would I find the value of the directional derivative as it comes down to $D_vf(0,0)=\frac{0}{0}$?",,"['multivariable-calculus', 'derivatives']"
44,Prove all tangent plane to the cone $x^2+y^2=z^2$ goes through the origin,Prove all tangent plane to the cone  goes through the origin,x^2+y^2=z^2,"Good morning, i need help with this exercise. Prove all tangent plane to the cone $x^2+y^2=z^2$ goes through the origin My work: Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ defined by $f(x,y,z)=x^2+y^2-z^2$ Then, $\nabla f(x,y,z)=(2x,2y,-2z)$ Let $(a,b,c)\in\mathbb{R}^3$ then $\nabla f(a,b,c)=(2a,2b,-2c)$ By definition, the equation of the tangent plane is \begin{eqnarray} \langle(2a,2b,-2c),(x-a,y-b,z-c)\rangle &=& 2a(x-a)+2b(y-b)+2c(z-c)\\ &=&2ax-2a^2+2by-2b^2+2cz-2c^2 \\ &=&0 \end{eqnarray} In this step i'm stuck, can someone help me?","Good morning, i need help with this exercise. Prove all tangent plane to the cone $x^2+y^2=z^2$ goes through the origin My work: Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ defined by $f(x,y,z)=x^2+y^2-z^2$ Then, $\nabla f(x,y,z)=(2x,2y,-2z)$ Let $(a,b,c)\in\mathbb{R}^3$ then $\nabla f(a,b,c)=(2a,2b,-2c)$ By definition, the equation of the tangent plane is \begin{eqnarray} \langle(2a,2b,-2c),(x-a,y-b,z-c)\rangle &=& 2a(x-a)+2b(y-b)+2c(z-c)\\ &=&2ax-2a^2+2by-2b^2+2cz-2c^2 \\ &=&0 \end{eqnarray} In this step i'm stuck, can someone help me?",,"['real-analysis', 'multivariable-calculus']"
45,"Why does $\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}$ not exist?",Why does  not exist?,"\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}","Why does this limit not exist? $$\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}$$ If you set y = 0, the limit goes to zero. If you set x = 0, the limit goes to zero. You can also manipulate it with algebra to get zero. However, if x=y you have zero/zero before you even evaluate the limit but is that proof enough? Thanks! From Larson Calculus 13.2 Exercise 27","Why does this limit not exist? $$\lim\limits_{ (x,y) \to (0,0) } \frac{x-y}{\sqrt x - \sqrt y}$$ If you set y = 0, the limit goes to zero. If you set x = 0, the limit goes to zero. You can also manipulate it with algebra to get zero. However, if x=y you have zero/zero before you even evaluate the limit but is that proof enough? Thanks! From Larson Calculus 13.2 Exercise 27",,['multivariable-calculus']
46,Is the application which associates a polynomial with its root continuous?,Is the application which associates a polynomial with its root continuous?,,"Let $f:\mathbb{R}^{2n+1}\to\mathbb{R}$ be defined through: $f\left(x_0,...,x_{2n}\right)$ is the greatest root of the polynomial $p(t)=\sum_{k=0}^{2n}x_kt^k$. Is $f$ continuous? If so, what is the greatest $r$ such that $f\in C_r\left(\mathbb{R}^{2n+1}\right)$? It seems a bit difficult to directly prove the continuity, but I wasn't able to use tools like the implicit function theorem either. How to tackle the problem?","Let $f:\mathbb{R}^{2n+1}\to\mathbb{R}$ be defined through: $f\left(x_0,...,x_{2n}\right)$ is the greatest root of the polynomial $p(t)=\sum_{k=0}^{2n}x_kt^k$. Is $f$ continuous? If so, what is the greatest $r$ such that $f\in C_r\left(\mathbb{R}^{2n+1}\right)$? It seems a bit difficult to directly prove the continuity, but I wasn't able to use tools like the implicit function theorem either. How to tackle the problem?",,"['calculus', 'multivariable-calculus', 'polynomials']"
47,"Question Regarding Proof of Taylor Remainder Theorem in Tu's ""An Introduction to Manifolds""","Question Regarding Proof of Taylor Remainder Theorem in Tu's ""An Introduction to Manifolds""",,"The statement: Let $f$ be a $C^{\infty}$ function on an open set $U\subseteq \mathbb{R}^n$ which is star shaped with respect to a point $p=(p^1,...,p^n) \in U$. Then there are functions $g_1(x),...,g_n(x)\in C^{\infty} $ such that $$f(x)=f(p)+\sum_{i=1}^n(x^i-p^i)g_i(x)$$, where $g_i(p)=\dfrac{\partial{f}}{\partial{x^i}}(p)$. The proof of the statement follows from the fact that since $U$ is star shaped, the line segment between $p$ and any point $x\in U$ is contained in $U$. Therefore for $0\leq t\leq 1$, $f(p+t(x-p))$ is defined. This upcoming portion is where I don't understand his application of the chain rule, and would appreciate some clarification. ""By the chain rule $$\dfrac{d}{dt}f(p+t(x-p))=\sum (x^i-p^i)\dfrac{\partial{f}}{\partial{x^i}}(p+t(x-p))$$ "" From my understanding, the left hand side of the equality would come from the chain rule when $f$ is a function of $x=(x^1,...,x^n)$, and each $x^i$ is a function of $t$. But in this instance each $x^i$ is not a function of $t$ as they are both used in the same equation to describe a line segment, so why can the chain rule be applied in this instance?","The statement: Let $f$ be a $C^{\infty}$ function on an open set $U\subseteq \mathbb{R}^n$ which is star shaped with respect to a point $p=(p^1,...,p^n) \in U$. Then there are functions $g_1(x),...,g_n(x)\in C^{\infty} $ such that $$f(x)=f(p)+\sum_{i=1}^n(x^i-p^i)g_i(x)$$, where $g_i(p)=\dfrac{\partial{f}}{\partial{x^i}}(p)$. The proof of the statement follows from the fact that since $U$ is star shaped, the line segment between $p$ and any point $x\in U$ is contained in $U$. Therefore for $0\leq t\leq 1$, $f(p+t(x-p))$ is defined. This upcoming portion is where I don't understand his application of the chain rule, and would appreciate some clarification. ""By the chain rule $$\dfrac{d}{dt}f(p+t(x-p))=\sum (x^i-p^i)\dfrac{\partial{f}}{\partial{x^i}}(p+t(x-p))$$ "" From my understanding, the left hand side of the equality would come from the chain rule when $f$ is a function of $x=(x^1,...,x^n)$, and each $x^i$ is a function of $t$. But in this instance each $x^i$ is not a function of $t$ as they are both used in the same equation to describe a line segment, so why can the chain rule be applied in this instance?",,"['multivariable-calculus', 'differential-geometry', 'taylor-expansion']"
48,Proving openness of a set in $\Bbb R^4$,Proving openness of a set in,\Bbb R^4,"Given a set: $U = \{(x, y, z, w) : |x| < 1, |y| < 2, |z| < 3, |w| < 4\}$ We must formally (non-graphically, not that I'd ever be able to successfully graph such a set) prove that $U$ is open. We did a similar, albeit simpler, problem in class which involved only $(x, y)$ and proving that the set of both of their absolute values was open. The route we took involved setting $\delta = \min(|1-x|,|1-y|,|-1-x|,|-1-y|)$ which made sense graphically. Although to be honest I'm not sure how the connection was made between that and a neighborhood of points in the set. Consequently, I'm not sure how to approach a formal proof of this question. Any help will be appreciated.","Given a set: $U = \{(x, y, z, w) : |x| < 1, |y| < 2, |z| < 3, |w| < 4\}$ We must formally (non-graphically, not that I'd ever be able to successfully graph such a set) prove that $U$ is open. We did a similar, albeit simpler, problem in class which involved only $(x, y)$ and proving that the set of both of their absolute values was open. The route we took involved setting $\delta = \min(|1-x|,|1-y|,|-1-x|,|-1-y|)$ which made sense graphically. Although to be honest I'm not sure how the connection was made between that and a neighborhood of points in the set. Consequently, I'm not sure how to approach a formal proof of this question. Any help will be appreciated.",,['multivariable-calculus']
49,Can you have a gradient of time?,Can you have a gradient of time?,,"Okay this maybe a very stupid question but in my calculus III class we introduced the gradient but I am curious why don't we also include the derivative of time in the gradient. Thanks, math noob","Okay this maybe a very stupid question but in my calculus III class we introduced the gradient but I am curious why don't we also include the derivative of time in the gradient. Thanks, math noob",,['multivariable-calculus']
50,Can a region always be parametrized by a single function?,Can a region always be parametrized by a single function?,,"Can some connected region in $\Bbb R^n$, possibly with some other nice conditions on the region, always be parametrized by a single function $\vec r(x_1, x_2, \dots, x_k)$ (even if it may be easier to not restrict yourself to this)?  Or do you sometimes have to use different parametrizations for different parts of the region? Edit: I was being intentionally vague with ""nice"" because I don't know what, if any, other conditions might be necessary. By parametrization I mean an injective, differentiable function that maps some parameters $x_1,\dots,x_k$, where $k \le n$, to the connected region in $R^n$ that we're concerned with.","Can some connected region in $\Bbb R^n$, possibly with some other nice conditions on the region, always be parametrized by a single function $\vec r(x_1, x_2, \dots, x_k)$ (even if it may be easier to not restrict yourself to this)?  Or do you sometimes have to use different parametrizations for different parts of the region? Edit: I was being intentionally vague with ""nice"" because I don't know what, if any, other conditions might be necessary. By parametrization I mean an injective, differentiable function that maps some parameters $x_1,\dots,x_k$, where $k \le n$, to the connected region in $R^n$ that we're concerned with.",,"['multivariable-calculus', 'differential-geometry']"
51,If the partial derivatives are $0$ is a function constant?,If the partial derivatives are  is a function constant?,0,"I am trying to prove that if we have a differentiable function: $f:\mathbb{R}^2\rightarrow \mathbb{R}$, and the partial derivatives of f is 0, then f is constant on a connected set. I am using the fact that a set is connected if every point in the set can be joined by a continuous curve. If every point could be connected by a straight line it would follow from the mean value theorem, so I guess for convex sets this is easy. But what about if we have a connected set which is not convex? Does the possibility to connect every point together with a curve imply that every point can be connected with a finite number of straight lines, where each line is contained in the set? If so it is ok, but the last thing seems difficult to prove, how do you do it? update: If the set also is open, is it correct to say that for every point there is an open ball around that point. And in that open ball every straight line is contained. So the function is constant around every point in the set. And if two points where unequal we could connect those points by a curve, and on one point on that curve the value of the function has to change, and hence we have a contradiction, because in an open ball around this point we have two different values?","I am trying to prove that if we have a differentiable function: $f:\mathbb{R}^2\rightarrow \mathbb{R}$, and the partial derivatives of f is 0, then f is constant on a connected set. I am using the fact that a set is connected if every point in the set can be joined by a continuous curve. If every point could be connected by a straight line it would follow from the mean value theorem, so I guess for convex sets this is easy. But what about if we have a connected set which is not convex? Does the possibility to connect every point together with a curve imply that every point can be connected with a finite number of straight lines, where each line is contained in the set? If so it is ok, but the last thing seems difficult to prove, how do you do it? update: If the set also is open, is it correct to say that for every point there is an open ball around that point. And in that open ball every straight line is contained. So the function is constant around every point in the set. And if two points where unequal we could connect those points by a curve, and on one point on that curve the value of the function has to change, and hence we have a contradiction, because in an open ball around this point we have two different values?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'partial-derivative', 'connectedness']"
52,What does $\iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS$ mean?,What does  mean?,\iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS,"In the Wikipedia article on vector calculus identities , we have the following $$\oint_{\partial S} \psi \; d\mathbf{\ell} = \iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS$$ The right hand side is an integral of a vector field over a surface integral against $dS$, and not $\cdot dS$ or $\cdot \mathbf{\hat{n}} \; dS$. So if it's not a flux integral, I don't know what this means. I don't think the right hand side is the flux integral of $\mathbf{\hat{n}} \times \nabla \psi$. Otherwise, by Stokes' theorem, this would equal $$-\int_{\partial S} \psi \mathbf{\hat{n}} \; \cdot d\ell = 0$$ since the normal vector is normal to the tangent vector on the boundary. Then the identity is false. So what is this called? The only way I know how to integrate a vector field along a surface it by its flux, but this clearly is not what this is. I haven't seen this notation anywhere else, does anyone know where I can find a precise, rigorous definition of what it's supposed to mean? EDIT: Here is my guess. These are vector valued integrals. The left hand side is $$\oint_{\partial S} \psi \; \mathbf{d\ell}$$ What this means is the vector $$\left(\int_{\partial S} (\psi, 0, 0) \cdot d\ell, \int_{\partial S} (0, \psi, 0) \cdot d\ell, \int_{\partial S} (0, 0, \psi) \cdot d\ell \right)$$ where the integrals are the usual line integrals of vector fields along $\partial S$. The right hand side is $$\iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS$$ Observe that $\hat{\mathbf{n}} \times \nabla \psi$ gives a vector that can be written as $$(\mathbf{G}_1 \cdot \mathbf{\hat{n}}, \mathbf{G}_2 \cdot \mathbf{\hat{n}}, \mathbf{G}_3 \cdot \mathbf{\hat{n}})$$ for some vector fields $\mathbf{G}_i$, $1 \leq i \leq 3$. The right hand side is thus the vector $$\left(\int_S \mathbf{G}_1 \cdot \mathbf{\hat{n}} \; dS, \int_S \mathbf{G}_2 \cdot \mathbf{\hat{n}} \; dS, \int_S \mathbf{G}_3 \cdot \mathbf{\hat{n}} \; dS\right)$$ where the components are the flux integrals of the $\mathbf{G}_i$. Still not sure what the $dS$ is supposed to be, since it seems a bit useless? EDIT 2: I think I understand now, the integral of a vector valued function is just the integral of the component functions with respect to the volume form of what we are integrating over ($dS$ or $d\ell$). So there are two ways to integrate a vector field over a surface. One, is to take the usual flux integral, which yields a scalar. The second, which is presented here, is the integrals of the component functions over the surface, which yields a vector. Funnily enough, I've never seen this in any of my math books...perhaps it's more common in physics?","In the Wikipedia article on vector calculus identities , we have the following $$\oint_{\partial S} \psi \; d\mathbf{\ell} = \iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS$$ The right hand side is an integral of a vector field over a surface integral against $dS$, and not $\cdot dS$ or $\cdot \mathbf{\hat{n}} \; dS$. So if it's not a flux integral, I don't know what this means. I don't think the right hand side is the flux integral of $\mathbf{\hat{n}} \times \nabla \psi$. Otherwise, by Stokes' theorem, this would equal $$-\int_{\partial S} \psi \mathbf{\hat{n}} \; \cdot d\ell = 0$$ since the normal vector is normal to the tangent vector on the boundary. Then the identity is false. So what is this called? The only way I know how to integrate a vector field along a surface it by its flux, but this clearly is not what this is. I haven't seen this notation anywhere else, does anyone know where I can find a precise, rigorous definition of what it's supposed to mean? EDIT: Here is my guess. These are vector valued integrals. The left hand side is $$\oint_{\partial S} \psi \; \mathbf{d\ell}$$ What this means is the vector $$\left(\int_{\partial S} (\psi, 0, 0) \cdot d\ell, \int_{\partial S} (0, \psi, 0) \cdot d\ell, \int_{\partial S} (0, 0, \psi) \cdot d\ell \right)$$ where the integrals are the usual line integrals of vector fields along $\partial S$. The right hand side is $$\iint_S (\hat{\mathbf{n}} \times \nabla \psi) \; dS$$ Observe that $\hat{\mathbf{n}} \times \nabla \psi$ gives a vector that can be written as $$(\mathbf{G}_1 \cdot \mathbf{\hat{n}}, \mathbf{G}_2 \cdot \mathbf{\hat{n}}, \mathbf{G}_3 \cdot \mathbf{\hat{n}})$$ for some vector fields $\mathbf{G}_i$, $1 \leq i \leq 3$. The right hand side is thus the vector $$\left(\int_S \mathbf{G}_1 \cdot \mathbf{\hat{n}} \; dS, \int_S \mathbf{G}_2 \cdot \mathbf{\hat{n}} \; dS, \int_S \mathbf{G}_3 \cdot \mathbf{\hat{n}} \; dS\right)$$ where the components are the flux integrals of the $\mathbf{G}_i$. Still not sure what the $dS$ is supposed to be, since it seems a bit useless? EDIT 2: I think I understand now, the integral of a vector valued function is just the integral of the component functions with respect to the volume form of what we are integrating over ($dS$ or $d\ell$). So there are two ways to integrate a vector field over a surface. One, is to take the usual flux integral, which yields a scalar. The second, which is presented here, is the integrals of the component functions over the surface, which yields a vector. Funnily enough, I've never seen this in any of my math books...perhaps it's more common in physics?",,['multivariable-calculus']
53,calculate Jacobian matrix without closed form or analytical form,calculate Jacobian matrix without closed form or analytical form,,"The question is probably clear in the title. In many of my applications mostly computer vision, I might not have the closed-form or analytical form of f (a multivariable function). It's calculated numerically instead and in general, non-linear, without guarantee of smoothness or differentiable. Since it includes: function f = nlfun(x)      if ..      ..     else     ..     end      switch ..     case 1:        f = nonlinear_function     end How may I investigate this f=nlfun(x) characteristics e.g. smoothness, differentiable...? and finally, calculate its Jacobian? Thank you","The question is probably clear in the title. In many of my applications mostly computer vision, I might not have the closed-form or analytical form of f (a multivariable function). It's calculated numerically instead and in general, non-linear, without guarantee of smoothness or differentiable. Since it includes: function f = nlfun(x)      if ..      ..     else     ..     end      switch ..     case 1:        f = nonlinear_function     end How may I investigate this f=nlfun(x) characteristics e.g. smoothness, differentiable...? and finally, calculate its Jacobian? Thank you",,"['multivariable-calculus', 'numerical-methods', 'numerical-optimization']"
54,Why is this proof incorrect? - $f(b) - f(a) = [Df(c)](b-a) = \nabla f(c) \cdot (b-a)$,Why is this proof incorrect? -,f(b) - f(a) = [Df(c)](b-a) = \nabla f(c) \cdot (b-a),"I'm currently a student in a Vector Calculus class, and received my exam back today. I plan to go to the professor's office hours, but I'd like to ask here (in case there's some blatantly obvious fault I'm missing). Not only do I not understand why I received no partial credit in this problem, but I don't understand why it's not completely correct. I'd really appreciate any insight. The problem statement is: Let $a,b \in \mathbb{R^n}$ with $a \neq b$ . The segment $(a,b)$ in $\mathbb{R^n}$ is defined by: $$(a,b) := \{x \in \mathbb{R^n} | ~x = (1 - t)a + tb,~ t \in (0,1) \} $$ Let $f: \mathbb{R^n} \rightarrow \mathbb{R}$ be differentiable on $\mathbb{R^n}$ . Use the Mean Value Theorem from single-variable calculus to show that there exists $c \in (a,b) \subset \mathbb{R^n}$ such that: $$f(b) - f(a) = [Df(c)](b-a) = \nabla f(c) \cdot (b-a)$$ Here's my proof: We know $f$ is differentiable on $\mathbb{R^n}$ (including on the interval $(a,b)$ ). Fix all elements in $x$ but one, call this element $x_i$ (that is, the $i^{th}$ element of $x$ . Let $a_i < c_i < b_i$ . By the mean value theorem, we know there is at least one $c_i$ such that the tangent at $c_i$ is parallel to the secant between $a_i$ and $b_i$ . That is, there exists a $c_i$ such that: $$\frac{f(b_i) - f(a_i)}{b_i - a_i} = \frac{\partial f(x_i)}{\partial c_i}$$ Multiplying by $(b_i - a_i)$ on both sides, we have: $$f(b_i) - f(a_i) = \frac{\partial f(x_i)}{\partial c_i}(b_i - a_i)$$ We now fix all other elements in $x$ and, by the same process, arrive at the same result for every component of $x$ . This gives us: $$f(b) - f(a) = \left(\frac{\partial f(x_1)}{\partial c_1} + \frac{\partial f(x_2)}{\partial c_2}  + ... + \frac{\partial f(x_n)}{\partial c_n} \right)(b-a)$$ Note that the term on the right side of the equality is just $\nabla f(c) \cdot (b-a)$ , so we know that there exists some $c$ such that: $$f(b) - f(a) = \nabla f(c) \cdot (b-a)$$ I have pretty much zero idea where I went wrong (but it's, apparently, all wrong). I'm very confused.","I'm currently a student in a Vector Calculus class, and received my exam back today. I plan to go to the professor's office hours, but I'd like to ask here (in case there's some blatantly obvious fault I'm missing). Not only do I not understand why I received no partial credit in this problem, but I don't understand why it's not completely correct. I'd really appreciate any insight. The problem statement is: Let with . The segment in is defined by: Let be differentiable on . Use the Mean Value Theorem from single-variable calculus to show that there exists such that: Here's my proof: We know is differentiable on (including on the interval ). Fix all elements in but one, call this element (that is, the element of . Let . By the mean value theorem, we know there is at least one such that the tangent at is parallel to the secant between and . That is, there exists a such that: Multiplying by on both sides, we have: We now fix all other elements in and, by the same process, arrive at the same result for every component of . This gives us: Note that the term on the right side of the equality is just , so we know that there exists some such that: I have pretty much zero idea where I went wrong (but it's, apparently, all wrong). I'm very confused.","a,b \in \mathbb{R^n} a \neq b (a,b) \mathbb{R^n} (a,b) := \{x \in \mathbb{R^n} | ~x = (1 - t)a + tb,~ t \in (0,1) \}  f: \mathbb{R^n} \rightarrow \mathbb{R} \mathbb{R^n} c \in (a,b) \subset \mathbb{R^n} f(b) - f(a) = [Df(c)](b-a) = \nabla f(c) \cdot (b-a) f \mathbb{R^n} (a,b) x x_i i^{th} x a_i < c_i < b_i c_i c_i a_i b_i c_i \frac{f(b_i) - f(a_i)}{b_i - a_i} = \frac{\partial f(x_i)}{\partial c_i} (b_i - a_i) f(b_i) - f(a_i) = \frac{\partial f(x_i)}{\partial c_i}(b_i - a_i) x x f(b) - f(a) = \left(\frac{\partial f(x_1)}{\partial c_1} + \frac{\partial f(x_2)}{\partial c_2}  + ... + \frac{\partial f(x_n)}{\partial c_n} \right)(b-a) \nabla f(c) \cdot (b-a) c f(b) - f(a) = \nabla f(c) \cdot (b-a)","['multivariable-calculus', 'proof-verification', 'partial-derivative']"
55,Is there a general way to solve a multivariate polynomial?,Is there a general way to solve a multivariate polynomial?,,"Is there a general way to solve a multivariate polynomial (for example the one given here ). Say for instance I knew some function $F(x,y) = xy + x^2 + y^3 + 2x^2y^2 = 5$ , and $G(x,y) = 7xy^2 + 4y^2x + x^2 = 7$ , could I find a solution set? Is there a general method (either analytic or numeric) for solving systems such as these?","Is there a general way to solve a multivariate polynomial (for example the one given here ). Say for instance I knew some function , and , could I find a solution set? Is there a general method (either analytic or numeric) for solving systems such as these?","F(x,y) = xy + x^2 + y^3 + 2x^2y^2 = 5 G(x,y) = 7xy^2 + 4y^2x + x^2 = 7","['multivariable-calculus', 'polynomials']"
56,"Find extrema of $f(x, y) = x - x^2 - xy^2$ with constraints $\{(x, y) : x^2 + y^2 \le 4, x \ge 0\}$?",Find extrema of  with constraints ?,"f(x, y) = x - x^2 - xy^2 \{(x, y) : x^2 + y^2 \le 4, x \ge 0\}","I know it's a bit short , But I have no idea at all how to solve such question .. I'd be very glad if you can give me some help HOW to solve it. Given fuction $f(x, y) = x - x^2 - xy^2$, and I need to find it's minimum and maximum value in the domain $D = \{(x, y) : x^2 + y^2 \le 4, x \ge 0\}$. How can I solve it? Is that somehow related with Lagrange multipliers?","I know it's a bit short , But I have no idea at all how to solve such question .. I'd be very glad if you can give me some help HOW to solve it. Given fuction $f(x, y) = x - x^2 - xy^2$, and I need to find it's minimum and maximum value in the domain $D = \{(x, y) : x^2 + y^2 \le 4, x \ge 0\}$. How can I solve it? Is that somehow related with Lagrange multipliers?",,['multivariable-calculus']
57,"Simple criteria for ""closed $\Longrightarrow$ exact""","Simple criteria for ""closed  exact""",\Longrightarrow,"In determining whether a closed form is an exact form, there is a lot of differential geometry definitions etc. that come in. I'm interested: what is the dummy, Calc III version of when closed implies exact? Is it sufficient if the domain is a simply connected smooth region in $\mathbb{R}^n$? I believe this holds in $\mathbb{R}^3$, i.e. for a simply connected smooth region in $\mathbb{R}^3$, a curl-free $C^1$ vector field is the gradient of some function, and a divergence-free vector field is the curl of some function. ( Edit : the second statement is not true.) Is ""simply connected"" the magic ingredient in all dimensions?","In determining whether a closed form is an exact form, there is a lot of differential geometry definitions etc. that come in. I'm interested: what is the dummy, Calc III version of when closed implies exact? Is it sufficient if the domain is a simply connected smooth region in $\mathbb{R}^n$? I believe this holds in $\mathbb{R}^3$, i.e. for a simply connected smooth region in $\mathbb{R}^3$, a curl-free $C^1$ vector field is the gradient of some function, and a divergence-free vector field is the curl of some function. ( Edit : the second statement is not true.) Is ""simply connected"" the magic ingredient in all dimensions?",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
58,What does elementary function mean? [duplicate],What does elementary function mean? [duplicate],,"This question already has answers here : What makes elementary functions elementary? (8 answers) Closed 11 years ago . I am looking at my double integration example problems and I see a note that says integral of $e^{-x^2}$ is not an elementary function. What does that mean? And why isn't that an elementary function? It actually says observe integral of $e^{-x^2} dx$ is not an elementary function. But why is that? Same goes for integral of (sin(x)/x) dx, why isn't that an elementary function also?","This question already has answers here : What makes elementary functions elementary? (8 answers) Closed 11 years ago . I am looking at my double integration example problems and I see a note that says integral of $e^{-x^2}$ is not an elementary function. What does that mean? And why isn't that an elementary function? It actually says observe integral of $e^{-x^2} dx$ is not an elementary function. But why is that? Same goes for integral of (sin(x)/x) dx, why isn't that an elementary function also?",,['multivariable-calculus']
59,Double Integral help: $ \int_{x=0.5}^{x=1.5}\int_{-0.5}^{0.5}\frac{dx\;dy}{\sqrt{x^2+y^2}} $,Double Integral help:, \int_{x=0.5}^{x=1.5}\int_{-0.5}^{0.5}\frac{dx\;dy}{\sqrt{x^2+y^2}} ,"So it's been a while since I've had to do any remotely difficult integration, and one of my profs kind of sprung this on us. $$ \int_{x=0.5}^{x=1.5}\int_{-0.5}^{0.5}\frac{dx\;dy}{\sqrt{x^2+y^2}} $$ If I convert to polar coordinates and integrate, I get r times theta or: $$ \sqrt{x^2+y^2}\cdot \tan^{-1}(\frac{y}{x}) $$ In rectangular coordinates. However, whenever I tired using the limits given, I arrive at an answer of 0.0933, when the expected value is close to 1. Does anybody know what could have gone awry? I think the integration method is correct, but I'm not sure.","So it's been a while since I've had to do any remotely difficult integration, and one of my profs kind of sprung this on us. $$ \int_{x=0.5}^{x=1.5}\int_{-0.5}^{0.5}\frac{dx\;dy}{\sqrt{x^2+y^2}} $$ If I convert to polar coordinates and integrate, I get r times theta or: $$ \sqrt{x^2+y^2}\cdot \tan^{-1}(\frac{y}{x}) $$ In rectangular coordinates. However, whenever I tired using the limits given, I arrive at an answer of 0.0933, when the expected value is close to 1. Does anybody know what could have gone awry? I think the integration method is correct, but I'm not sure.",,"['calculus', 'multivariable-calculus']"
60,What is the meaning of $\vec A \cdot \nabla$?,What is the meaning of ?,\vec A \cdot \nabla,Looking at the application of divergence in Cartesian coordinates in Wikipedia I was wondering about the meaning of $\vec A \cdot \nabla$? This dot product is found at the vector cross product identity: $\nabla \times (\mathbf{A} \times \mathbf{B}) = \mathbf{A} (\nabla \cdot \mathbf{B}) - \mathbf{B} (\nabla \cdot \mathbf{A}) + (\mathbf{B} \cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla) \mathbf{B}$,Looking at the application of divergence in Cartesian coordinates in Wikipedia I was wondering about the meaning of $\vec A \cdot \nabla$? This dot product is found at the vector cross product identity: $\nabla \times (\mathbf{A} \times \mathbf{B}) = \mathbf{A} (\nabla \cdot \mathbf{B}) - \mathbf{B} (\nabla \cdot \mathbf{A}) + (\mathbf{B} \cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla) \mathbf{B}$,,['multivariable-calculus']
61,How to find the double integral of an absolute value trig function?,How to find the double integral of an absolute value trig function?,,"I have this double integral: $$\int_{-\pi/2}^{\pi/2} \int_{-\pi/2}^{\pi/2} |\sin(y-x)|dydx$$ And I'm struggling to solve it. I have reached the answer $4/\pi^2$ however my answer isn't what the textbook says is correct, which is $2\pi$ . Since it's an absolute value I tried this for the new bounds: $$\int_{-\pi/2}^0 \int_0^{\pi/2} (\sin(y-x))dydx - \int_0^{\pi/2} \int_{-\pi/2}^0 (\sin(y-x))dydx$$ I'm not really sure what to do, I'm pretty sure my new bounds are wrong but I'm not sure. Any kind of help would be appreciated!!","I have this double integral: And I'm struggling to solve it. I have reached the answer however my answer isn't what the textbook says is correct, which is . Since it's an absolute value I tried this for the new bounds: I'm not really sure what to do, I'm pretty sure my new bounds are wrong but I'm not sure. Any kind of help would be appreciated!!",\int_{-\pi/2}^{\pi/2} \int_{-\pi/2}^{\pi/2} |\sin(y-x)|dydx 4/\pi^2 2\pi \int_{-\pi/2}^0 \int_0^{\pi/2} (\sin(y-x))dydx - \int_0^{\pi/2} \int_{-\pi/2}^0 (\sin(y-x))dydx,"['multivariable-calculus', 'definite-integrals', 'absolute-value']"
62,Derivative of $Ax x^\top A$ with respect to $x$,Derivative of  with respect to,Ax x^\top A x,"I do not want to use index notation. I want to compute the derivative $$ D_x (Axx^\top A) = ? $$ where $A$ is an $n\times n$ symmetric matrix and $x$ in a vector in $\mathbb{R}^n$ . I tried resources such as the matrix calculus cookbook but they don't deal with scenarios like this: Here the function $f(x) = Axx^\top A$ takes a vector as input and returns a matrix output. It is possible to express this without using index notation and I want this type of answers. I would like step by step, to figure out how I can go about performing similar calculations in the future. Attempt One attempt is using the Frechet derivative definition (I will use the Frobenius norm) $$ \begin{align} \lim_{\|v\|\to 0} \frac{\|A(x+v)(x+v)^\top A - Axx^\top A - Dv\|_F}{\|v\|} &= \lim_{\|v\|\to 0} \frac{\|A(xv^\top +vv^\top + vx^\top)A - Dv\|_F}{\|v\|} \end{align} $$","I do not want to use index notation. I want to compute the derivative where is an symmetric matrix and in a vector in . I tried resources such as the matrix calculus cookbook but they don't deal with scenarios like this: Here the function takes a vector as input and returns a matrix output. It is possible to express this without using index notation and I want this type of answers. I would like step by step, to figure out how I can go about performing similar calculations in the future. Attempt One attempt is using the Frechet derivative definition (I will use the Frobenius norm)","
D_x (Axx^\top A) = ?
 A n\times n x \mathbb{R}^n f(x) = Axx^\top A 
\begin{align}
\lim_{\|v\|\to 0} \frac{\|A(x+v)(x+v)^\top A - Axx^\top A - Dv\|_F}{\|v\|}
&= \lim_{\|v\|\to 0} \frac{\|A(xv^\top +vv^\top + vx^\top)A - Dv\|_F}{\|v\|}
\end{align}
","['multivariable-calculus', 'derivatives', 'matrix-calculus', 'tensors', 'frechet-derivative']"
63,Misapplication of the divergence theorem when calculating a surface integral?,Misapplication of the divergence theorem when calculating a surface integral?,,"Let $\mathbf{F} = (3y, -xz, yz^2)$ , and let $S=\{(x,y,z): z=\frac{1}{2}(x^2+y^2), z\leq 2\}$ .  Find $\iint_S (\nabla \times \mathbf{F}) \cdot d\mathbf{S}$ . Firstly, I know I can compute this quite easily using Stokes' theorem -- my question concerns only the divergence theorem. Using Stokes' theorem, I obtain an answer of $-20\pi$ . Now below is my working relating to the divergence theorem. If I let $\Sigma$ be the disc $x^2+y^2\leq 4$ on the plane $z=2$ , then $S \cup \Sigma$ is the boundary of $\Omega = \{(x,y,z) : \frac{1}{2}(x^2+y^2)\leq z\leq 2\}$ . So by the divergence theorem, $$ \iiint_\Omega \nabla \cdot \mathbf{F} dV = \iint_S \mathbf{F}\cdot d\mathbf{S} + \iint_\Sigma \mathbf{F}\cdot d\mathbf{S}. $$ For the region $\Sigma$ , the unit normal is $(0,0,1)$ , so we can calculate the surface integral as \begin{align*}  \iint_\Sigma \mathbf{F}\cdot d\mathbf{S} &= \iint_{\Sigma} \mathbf{F}\cdot (0,0,1) \,dA \\  &= \iint_{x^2+y^2\leq 4 \\ z=2} yz^2 dA \\  &= 4 \iint_{x^2+y^2\leq 4} y dA \\ &= 4 \int_0 ^{2\pi} \int_0^2 (r\sin\theta) drd\theta \\  &= 4 \int_0 ^{2\pi} 2\sin \theta = 0. \end{align*} Also, $\nabla \cdot \mathbf{F} = (0,0,2zy)$ , so \begin{align*} \iiint_\Omega 2zy dV &= 2\int_0^2 \iint _{x^2+y^2\leq 2z} yz\,dV \\  &=2\int_0^2\int_0^{2\pi} \int_0^\sqrt{2z} (r\sin\theta z) dr d\theta dz \end{align*} but this will still be $0$ because of the presence of $\sin \theta$ . This would imply that $\iint_S \mathbf{F}\cdot d\mathbf{S}=0$ . My question is: what went wrong with the second computation? I think it might be the way I handled the volume integral $\iiint_\Omega$ , but I'm not quite sure. Have I misunderstood the divergence theorem? Any guidance would be very much appreciated.","Let , and let .  Find . Firstly, I know I can compute this quite easily using Stokes' theorem -- my question concerns only the divergence theorem. Using Stokes' theorem, I obtain an answer of . Now below is my working relating to the divergence theorem. If I let be the disc on the plane , then is the boundary of . So by the divergence theorem, For the region , the unit normal is , so we can calculate the surface integral as Also, , so but this will still be because of the presence of . This would imply that . My question is: what went wrong with the second computation? I think it might be the way I handled the volume integral , but I'm not quite sure. Have I misunderstood the divergence theorem? Any guidance would be very much appreciated.","\mathbf{F} = (3y, -xz, yz^2) S=\{(x,y,z): z=\frac{1}{2}(x^2+y^2), z\leq 2\} \iint_S (\nabla \times \mathbf{F}) \cdot d\mathbf{S} -20\pi \Sigma x^2+y^2\leq 4 z=2 S \cup \Sigma \Omega = \{(x,y,z) : \frac{1}{2}(x^2+y^2)\leq z\leq 2\}  \iiint_\Omega \nabla \cdot \mathbf{F} dV = \iint_S \mathbf{F}\cdot d\mathbf{S} + \iint_\Sigma \mathbf{F}\cdot d\mathbf{S}.  \Sigma (0,0,1) \begin{align*}
 \iint_\Sigma \mathbf{F}\cdot d\mathbf{S} &= \iint_{\Sigma} \mathbf{F}\cdot (0,0,1) \,dA \\ 
&= \iint_{x^2+y^2\leq 4 \\ z=2} yz^2 dA \\ 
&= 4 \iint_{x^2+y^2\leq 4} y dA \\
&= 4 \int_0 ^{2\pi} \int_0^2 (r\sin\theta) drd\theta \\ 
&= 4 \int_0 ^{2\pi} 2\sin \theta = 0.
\end{align*} \nabla \cdot \mathbf{F} = (0,0,2zy) \begin{align*}
\iiint_\Omega 2zy dV &= 2\int_0^2 \iint _{x^2+y^2\leq 2z} yz\,dV \\ 
&=2\int_0^2\int_0^{2\pi} \int_0^\sqrt{2z} (r\sin\theta z) dr d\theta dz
\end{align*} 0 \sin \theta \iint_S \mathbf{F}\cdot d\mathbf{S}=0 \iiint_\Omega","['multivariable-calculus', 'solution-verification', 'vector-analysis']"
64,"Double Integral $\iint e^\frac{x+y}{x-y} \,dx \,dy$ solution",Double Integral  solution,"\iint e^\frac{x+y}{x-y} \,dx \,dy","In the Multivariable Calculus Class I'm taking we were tasked to solve $$\iint_{R} e^\frac{x+y}{x-y} \,dx \,dy\,,$$ where $R$ is the trapezoidal region with vertices $(1,0)$ , $(2,0)$ , $(0,-1)$ and $(0,-2)$ as part of our optional Problem Set. While I intuitively knew how to solve the other problems of the Set, I didn't with this one. Since integrating $e^\frac{x+y}{x-y}$ isn't that easy, I tried finding another way of solving this and found the method of Jacobians online (we haven't learned that yet). I figured that we could maybe set $u=x+y$ and $v=x-y$ based on the exemplary problem I saw online. Then I would arrive at $$x = \frac12(u+v)$$ $$y=\frac12(u-v)$$ This gives the Jacobian $$J(u,v)=\begin{vmatrix} \frac12 & \frac12 \\ \frac12 & -\frac12 \end{vmatrix}=-\frac12.$$ Where do I go from here to find the region $D$ in the plane of $(u,v)$ which corresponds to $R$ . My question therefore: How do I find $D$ ? and Is there another way to solve this without knowledge of Jacobians? I would assume that the chance of us being given this problem as a brutal introduction to Jacobian, kind of like ""You all didn't know how to solve this and are very confused now? Let's show you a method how to solve it!"" is rather low.","In the Multivariable Calculus Class I'm taking we were tasked to solve where is the trapezoidal region with vertices , , and as part of our optional Problem Set. While I intuitively knew how to solve the other problems of the Set, I didn't with this one. Since integrating isn't that easy, I tried finding another way of solving this and found the method of Jacobians online (we haven't learned that yet). I figured that we could maybe set and based on the exemplary problem I saw online. Then I would arrive at This gives the Jacobian Where do I go from here to find the region in the plane of which corresponds to . My question therefore: How do I find ? and Is there another way to solve this without knowledge of Jacobians? I would assume that the chance of us being given this problem as a brutal introduction to Jacobian, kind of like ""You all didn't know how to solve this and are very confused now? Let's show you a method how to solve it!"" is rather low.","\iint_{R} e^\frac{x+y}{x-y} \,dx \,dy\,, R (1,0) (2,0) (0,-1) (0,-2) e^\frac{x+y}{x-y} u=x+y v=x-y x = \frac12(u+v) y=\frac12(u-v) J(u,v)=\begin{vmatrix}
\frac12 & \frac12 \\
\frac12 & -\frac12
\end{vmatrix}=-\frac12. D (u,v) R D","['multivariable-calculus', 'multiple-integral', 'change-of-variable', 'jacobian']"
65,Is there a rigorous definition for matrix derivatives?,Is there a rigorous definition for matrix derivatives?,,"I know that, A function $f: \mathbb{R}^n \to \mathbb{R}$ is said to be differentiable at $x$ if there exists a vector $v$ such that, $$     \lim_{h \to 0} \frac{f(x+h) - f(x) - v^Th}     {\|h\|} = 0. $$ When $v$ exists, it is given by the ""gradient"" $\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)(x)$ Does there exist a similar definition for ""matrix derivative"" https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices","I know that, A function is said to be differentiable at if there exists a vector such that, When exists, it is given by the ""gradient"" Does there exist a similar definition for ""matrix derivative"" https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices","f: \mathbb{R}^n \to \mathbb{R} x v 
    \lim_{h \to 0} \frac{f(x+h) - f(x) - v^Th}
    {\|h\|} = 0.
 v \nabla f(x) = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)(x)","['multivariable-calculus', 'optimization', 'convex-analysis', 'definition', 'matrix-calculus']"
66,volume of tilted ellipsoid,volume of tilted ellipsoid,,"I'm supposed to calculate the volume of $$(2 x+y+z)^2 + (x+2 y+z)^2 + (x+y+2 z)^2 \leq 1$$ simplifying it gives $$6 (x^2 + y^2  + z^2) + 10 (x y + y z + x z) \leq 1$$ After drawing it using GeoGebra, I saw that it's a tilted ellipsoid inside the unit sphere, but I'm unable to think of how to solve this. I tried replacing the coordinates and I tried using spherical coordinates but I was unable to go anywhere with them. The final answer is $\frac{\pi}{3}$ meaning it's $\frac{1}{4}$ of a unit sphere's volume, and still, I wasn't able to conclude anything useful from it. Any hints would be greatly appreciated.","I'm supposed to calculate the volume of simplifying it gives After drawing it using GeoGebra, I saw that it's a tilted ellipsoid inside the unit sphere, but I'm unable to think of how to solve this. I tried replacing the coordinates and I tried using spherical coordinates but I was unable to go anywhere with them. The final answer is meaning it's of a unit sphere's volume, and still, I wasn't able to conclude anything useful from it. Any hints would be greatly appreciated.",(2 x+y+z)^2 + (x+2 y+z)^2 + (x+y+2 z)^2 \leq 1 6 (x^2 + y^2  + z^2) + 10 (x y + y z + x z) \leq 1 \frac{\pi}{3} \frac{1}{4},"['multivariable-calculus', 'vector-analysis', 'volume', 'multiple-integral']"
67,Higher dimensional volume using triple integral,Higher dimensional volume using triple integral,,"As a normal single variable integral is used to find an area under a certain region below a 2d-curve and double integral are used to find the volume under a 3d-curve, I used to think triple integrals can be used to find the volume under a 4d-graph. I'm not speaking about $$ \int \int \int  \,dx\,dy\,dz $$ but this, $$ \int \int \int f(x,y,z) \,dx\,dy\,dz $$ The former makes complete sense to me as of why it calculates volume. But doesn't latter the one should calculate the volume under a 4d-graph. Just like double integral, $ \int \int f(x,y,z) \,dx\,dy $ to find volume and $ \int \int \,dx\,dy $ to find area. But from my understanding in the $ \int \int \int f(x,y,z) \,dx\,dy\,dz $ the $f(x,y,z)$ is called density function and the integral is used to find the mass. Why density has anything to do with $f(x,y,z)$ ? can someone explain to me this?","As a normal single variable integral is used to find an area under a certain region below a 2d-curve and double integral are used to find the volume under a 3d-curve, I used to think triple integrals can be used to find the volume under a 4d-graph. I'm not speaking about but this, The former makes complete sense to me as of why it calculates volume. But doesn't latter the one should calculate the volume under a 4d-graph. Just like double integral, to find volume and to find area. But from my understanding in the the is called density function and the integral is used to find the mass. Why density has anything to do with ? can someone explain to me this?"," \int \int \int  \,dx\,dy\,dz   \int \int \int f(x,y,z) \,dx\,dy\,dz   \int \int f(x,y,z) \,dx\,dy   \int \int \,dx\,dy   \int \int \int f(x,y,z) \,dx\,dy\,dz  f(x,y,z) f(x,y,z)","['calculus', 'multivariable-calculus']"
68,How many partials need to be continuous to imply differentiability?,How many partials need to be continuous to imply differentiability?,,"Let $f:\mathbb{R}^n \to \mathbb{R}$. Let $x_0 \in \mathbb{R}^n$. Assume $n-1$ partials exist in some open ball containing $x_0$ and are continuous at $x_0$, and the remaining $1$ partial is assumed only to exist at $x_0$. A well known result states that this implies $f$ is differentiable at $x_0$. My question is whether or not this can be strengthened. Can we replace ""$n-1""$ in the above theorem with some function $g(n)$ ""smaller"" than $n-1$, and replace ""remaining $1$ partial"" with ""remaining $n-g(n)$ partials""? Feel free to play with assumptions slightly. For instance, you can replace ""continuous at $x_0$"" with ""continuous at $x_0$ and in some open ball containing $x_0$"".","Let $f:\mathbb{R}^n \to \mathbb{R}$. Let $x_0 \in \mathbb{R}^n$. Assume $n-1$ partials exist in some open ball containing $x_0$ and are continuous at $x_0$, and the remaining $1$ partial is assumed only to exist at $x_0$. A well known result states that this implies $f$ is differentiable at $x_0$. My question is whether or not this can be strengthened. Can we replace ""$n-1""$ in the above theorem with some function $g(n)$ ""smaller"" than $n-1$, and replace ""remaining $1$ partial"" with ""remaining $n-g(n)$ partials""? Feel free to play with assumptions slightly. For instance, you can replace ""continuous at $x_0$"" with ""continuous at $x_0$ and in some open ball containing $x_0$"".",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
69,"Critical points of function $f(x):=\frac{<Ax,x>}{|x|^2}$ are eigenvectors of $A$",Critical points of function  are eigenvectors of,"f(x):=\frac{<Ax,x>}{|x|^2} A","I'm stuck on the following problem: ""Show that the critical points $\bar{x}$ of the function $f:\mathbb{R}^n-\{0\}\to\mathbb{R}, f(x):=\frac{<Ax,x>}{|x|^2}$ are eigenvectors of the symmetric matrix $A$ of eigenvalue $A\bar{x}/|\bar{x}|^2$."" Being a bit rusty on linear algebra, I don't know how to proceed; so I'd like to have just an hint about how to get started.","I'm stuck on the following problem: ""Show that the critical points $\bar{x}$ of the function $f:\mathbb{R}^n-\{0\}\to\mathbb{R}, f(x):=\frac{<Ax,x>}{|x|^2}$ are eigenvectors of the symmetric matrix $A$ of eigenvalue $A\bar{x}/|\bar{x}|^2$."" Being a bit rusty on linear algebra, I don't know how to proceed; so I'd like to have just an hint about how to get started.",,"['linear-algebra', 'multivariable-calculus']"
70,"In Lagrange Multiplier, why level curves of $f$ and $g$ are tangent to each other?","In Lagrange Multiplier, why level curves of  and  are tangent to each other?",f g,"In Lagrange multiplier method, e.g. optimize a function $f(x_1, \dots, x_n)$ under a constraint $g(x_1, \dots, x_n) = 0$. There is a fact that $\nabla f$ is parallel to $\nabla g$ which is given rise from the level curves of $f$ and $g$ are tangent to each other (i.e. there tangent lines are parallel, then because gradient and tangent of level curve are orthogonal implies the fact above) at the points when $f$ optimized under constraint $g$. The only part I don't have intuitive understanding is that why level curves of $f$ and $g$ are tangent to each other at where $f$ optimized under $g$.","In Lagrange multiplier method, e.g. optimize a function $f(x_1, \dots, x_n)$ under a constraint $g(x_1, \dots, x_n) = 0$. There is a fact that $\nabla f$ is parallel to $\nabla g$ which is given rise from the level curves of $f$ and $g$ are tangent to each other (i.e. there tangent lines are parallel, then because gradient and tangent of level curve are orthogonal implies the fact above) at the points when $f$ optimized under constraint $g$. The only part I don't have intuitive understanding is that why level curves of $f$ and $g$ are tangent to each other at where $f$ optimized under $g$.",,"['multivariable-calculus', 'lagrange-multiplier']"
71,How to find which variable impacts the answer the most in this equation?,How to find which variable impacts the answer the most in this equation?,,"In this equation, if two of the variables are held constant, which variable will bring out the maximum positive change in the answer? I tried doing this in excel, but I'm having trouble figuring out if I'm on the right track. I got 27 different scenarios in the excel sheet and sorted by the highest value. Instinct tells me this is not the correct way to determine this. $$S = \frac{AB}{1+C-B}$$","In this equation, if two of the variables are held constant, which variable will bring out the maximum positive change in the answer? I tried doing this in excel, but I'm having trouble figuring out if I'm on the right track. I got 27 different scenarios in the excel sheet and sorted by the highest value. Instinct tells me this is not the correct way to determine this. $$S = \frac{AB}{1+C-B}$$",,['multivariable-calculus']
72,Differentiability and continuity of a multivariable function,Differentiability and continuity of a multivariable function,,"Let $f:\mathbb R^2\to \mathbb R$ be defined by $$f(x,y)=\begin{cases}\frac{x|y|}{\sqrt{x^2+y^2}},& (x,y)\ne(0,0)\\ 0,& (x,y)=(0,0).\end{cases}$$ For which non-zero vectors $u$ does the directional derivative exist at the point $(0,0)$? Do the partial derivatives exist? Is $f$ differentiable at $0$? Is $f$ continuous at $0$? According to my calculations, the directional derivative is zero for all non-zero vectors $u$ at $(0,0)$ which implies the partial derivatives are zero at $(0,0)$. I am not sure how to proceed with 3. and 4. I am not allowed to use the existence of partial derivatives and their continuity to check for differentiability since the author introduces this theorem in the next section. Thanks.","Let $f:\mathbb R^2\to \mathbb R$ be defined by $$f(x,y)=\begin{cases}\frac{x|y|}{\sqrt{x^2+y^2}},& (x,y)\ne(0,0)\\ 0,& (x,y)=(0,0).\end{cases}$$ For which non-zero vectors $u$ does the directional derivative exist at the point $(0,0)$? Do the partial derivatives exist? Is $f$ differentiable at $0$? Is $f$ continuous at $0$? According to my calculations, the directional derivative is zero for all non-zero vectors $u$ at $(0,0)$ which implies the partial derivatives are zero at $(0,0)$. I am not sure how to proceed with 3. and 4. I am not allowed to use the existence of partial derivatives and their continuity to check for differentiability since the author introduces this theorem in the next section. Thanks.",,"['calculus', 'multivariable-calculus', 'derivatives', 'continuity']"
73,The Leibniz rule for the curl of the product of a scalar field and a vector field,The Leibniz rule for the curl of the product of a scalar field and a vector field,,I have some scalar field $u:D \rightarrow\mathbb R; \space \space D\subset \mathbb R^3$ and a vector field $\vec{v}: D\rightarrow \mathbb R^3$ and I want to show that: curl$(u\vec{v)}=$grad$(u)\times \vec{v}+u\space$rot$(\vec{v})$ My question is: How do I multiply a vector field by a scalar field? Can I write curl$(u\vec{v})$ like this: curl$(\vec{v})=$curl$\begin{pmatrix}uv_1\\uv_2\\uv_3\end{pmatrix}=$ $\begin{vmatrix}\hat{i}&\hat{j}&\hat{k}\\\frac{d}{dx}&\frac{d}{dy}&\frac{d}{dz}\\uv_1&uv_2&uv_3\end{vmatrix}=\hat{i}(\frac{d}{dy}(uv_3)-\frac{d}{dz}(uv_2))-\hat{j}(\frac{d}{dx}(uv_3)-\frac{d}{dz}(uv_1))+\hat{k}(\frac{d}{dx}(uv_2)-\frac{d}{dy}(uv_1))$ or am I doing something wrong? Edit: grad$(u)\times\vec{v}+u\space $curl$(\vec{v})=(\frac{du}{dx}\hat{i}+\frac{du}{dy}\hat{j}+\frac{du}{dz}\hat{k}) \times (v_1 \hat{i}+v_2\hat{j}+v_3\hat{k})+u\space $curl$(\vec{v})$ $=\hat{i}(\frac{du}{dy}(v_3)-\frac{du}{dz}(v_2))-\hat{j}(\frac{du}{dx}(v_3)-\frac{du}{dz}(v_1))+\hat{k}(\frac{du}{dx}(v_2)-\frac{du}{dy}(v_1))+u[\hat{i}(\frac{d}{dy}(v_3)-\frac{d}{dz}(v_2))-\hat{j}(\frac{d}{dx}(v_3)-\frac{d}{dz}(v_1))+\hat{k}(\frac{d}{dx}(v_2)-\frac{d}{dy}(v_1)]$ $\not=\hat{i}(\frac{d}{dy}(uv_3)-\frac{d}{dz}(uv_2))-\hat{j}(\frac{d}{dx}(uv_3)-\frac{d}{dz}(uv_1))+\hat{k}(\frac{d}{dx}(uv_2)-\frac{d}{dy}(uv_1))$ I guess I am making a mistake somewhere but I can't find it. Edit 2: grad$\space u \times \vec{v}=(\frac{du}{dx}\hat{i}+\frac{du}{dy}\hat{j}+\frac{du}{dz}\hat{k}) \times (v_1 \hat{i}+v_2\hat{j}+v_3\hat{k})$ $=\begin{vmatrix}\hat{i}&\hat{j}&\hat{k}\\\frac{du}{dx}&\frac{du}{dy}&\frac{du}{dz}\\v_1&v_2&v_3\end{vmatrix}=\hat{i}(\frac{du}{dy}(v_3)-\frac{du}{dz}(v_2))-\hat{j}(\frac{du}{dx}(v_3)-\frac{du}{dz}(v_1))+\hat{k}(\frac{du}{dx}(v_2)-\frac{du}{dy}(v_1))$,I have some scalar field $u:D \rightarrow\mathbb R; \space \space D\subset \mathbb R^3$ and a vector field $\vec{v}: D\rightarrow \mathbb R^3$ and I want to show that: curl$(u\vec{v)}=$grad$(u)\times \vec{v}+u\space$rot$(\vec{v})$ My question is: How do I multiply a vector field by a scalar field? Can I write curl$(u\vec{v})$ like this: curl$(\vec{v})=$curl$\begin{pmatrix}uv_1\\uv_2\\uv_3\end{pmatrix}=$ $\begin{vmatrix}\hat{i}&\hat{j}&\hat{k}\\\frac{d}{dx}&\frac{d}{dy}&\frac{d}{dz}\\uv_1&uv_2&uv_3\end{vmatrix}=\hat{i}(\frac{d}{dy}(uv_3)-\frac{d}{dz}(uv_2))-\hat{j}(\frac{d}{dx}(uv_3)-\frac{d}{dz}(uv_1))+\hat{k}(\frac{d}{dx}(uv_2)-\frac{d}{dy}(uv_1))$ or am I doing something wrong? Edit: grad$(u)\times\vec{v}+u\space $curl$(\vec{v})=(\frac{du}{dx}\hat{i}+\frac{du}{dy}\hat{j}+\frac{du}{dz}\hat{k}) \times (v_1 \hat{i}+v_2\hat{j}+v_3\hat{k})+u\space $curl$(\vec{v})$ $=\hat{i}(\frac{du}{dy}(v_3)-\frac{du}{dz}(v_2))-\hat{j}(\frac{du}{dx}(v_3)-\frac{du}{dz}(v_1))+\hat{k}(\frac{du}{dx}(v_2)-\frac{du}{dy}(v_1))+u[\hat{i}(\frac{d}{dy}(v_3)-\frac{d}{dz}(v_2))-\hat{j}(\frac{d}{dx}(v_3)-\frac{d}{dz}(v_1))+\hat{k}(\frac{d}{dx}(v_2)-\frac{d}{dy}(v_1)]$ $\not=\hat{i}(\frac{d}{dy}(uv_3)-\frac{d}{dz}(uv_2))-\hat{j}(\frac{d}{dx}(uv_3)-\frac{d}{dz}(uv_1))+\hat{k}(\frac{d}{dx}(uv_2)-\frac{d}{dy}(uv_1))$ I guess I am making a mistake somewhere but I can't find it. Edit 2: grad$\space u \times \vec{v}=(\frac{du}{dx}\hat{i}+\frac{du}{dy}\hat{j}+\frac{du}{dz}\hat{k}) \times (v_1 \hat{i}+v_2\hat{j}+v_3\hat{k})$ $=\begin{vmatrix}\hat{i}&\hat{j}&\hat{k}\\\frac{du}{dx}&\frac{du}{dy}&\frac{du}{dz}\\v_1&v_2&v_3\end{vmatrix}=\hat{i}(\frac{du}{dy}(v_3)-\frac{du}{dz}(v_2))-\hat{j}(\frac{du}{dx}(v_3)-\frac{du}{dz}(v_1))+\hat{k}(\frac{du}{dx}(v_2)-\frac{du}{dy}(v_1))$,,"['multivariable-calculus', 'vector-fields']"
74,"To determine if set is open , closed [closed]","To determine if set is open , closed [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Define a function $f : \mathbb{R}^{2} \rightarrow  \mathbb{R} $ by $$f(x, y) =\begin{cases}1  & \text{if  $xy=0$} \\  2& \text{otherwise} \end{cases}$$ If $S  = \{(x, y): f \text{ is continous at point $( x, y)$}\}$, then set $S$ is A. Open B. Connected C. Closed D. Empty As $f$ is not continuous on the axes and is continuous at all other points. So set $S \neq \phi$. But how do I choose between other options?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Define a function $f : \mathbb{R}^{2} \rightarrow  \mathbb{R} $ by $$f(x, y) =\begin{cases}1  & \text{if  $xy=0$} \\  2& \text{otherwise} \end{cases}$$ If $S  = \{(x, y): f \text{ is continous at point $( x, y)$}\}$, then set $S$ is A. Open B. Connected C. Closed D. Empty As $f$ is not continuous on the axes and is continuous at all other points. So set $S \neq \phi$. But how do I choose between other options?",,['multivariable-calculus']
75,Confusion about the Total Derivative,Confusion about the Total Derivative,,"I just started multivariable calculus a little while ago and I'm confused about the concept of a total derivative of some function $z = z(x, y)$. I was taught that $dz = \frac{\partial z}{\partial x}\cdot dx + \frac{\partial z}{\partial y}\cdot dy$, and my understanding is that $dz$ represents an infinitesimal rate of change in $z$, and $\frac{\partial z}{\partial x}$ is the rate of change of $z$ w.r.t. $x$ while $y$ remains constant. So if $dz$ is the rate of change of $z$ and $z$ relies on two variables, shouldn't the rate of change be given by a vector $<\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}>$? Why is it simply the sum of $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$?","I just started multivariable calculus a little while ago and I'm confused about the concept of a total derivative of some function $z = z(x, y)$. I was taught that $dz = \frac{\partial z}{\partial x}\cdot dx + \frac{\partial z}{\partial y}\cdot dy$, and my understanding is that $dz$ represents an infinitesimal rate of change in $z$, and $\frac{\partial z}{\partial x}$ is the rate of change of $z$ w.r.t. $x$ while $y$ remains constant. So if $dz$ is the rate of change of $z$ and $z$ relies on two variables, shouldn't the rate of change be given by a vector $<\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}>$? Why is it simply the sum of $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$?",,"['multivariable-calculus', 'partial-derivative']"
76,Which book is appropriate for a Chemistry student that needs to learn basics about integrals?,Which book is appropriate for a Chemistry student that needs to learn basics about integrals?,,"A friend of me who is not studying mathematics now needs to deal with integrals, double integrals and triple integrals within his study of chemistry. He asked me to give him a suggestion for a basic book that explains basic facts, rules etc., about integrals. I think this means Riemann integral and that it should not be too complicated. I do not know such a book, do you know a book that deal with basic things concerning integrals for a non-mathematician? Thanks for your tips!","A friend of me who is not studying mathematics now needs to deal with integrals, double integrals and triple integrals within his study of chemistry. He asked me to give him a suggestion for a basic book that explains basic facts, rules etc., about integrals. I think this means Riemann integral and that it should not be too complicated. I do not know such a book, do you know a book that deal with basic things concerning integrals for a non-mathematician? Thanks for your tips!",,"['calculus', 'multivariable-calculus', 'reference-request', 'book-recommendation']"
77,"What is the ""proof"" for this vector calculus theorem besides intuition?","What is the ""proof"" for this vector calculus theorem besides intuition?",,"I'm reading Div, Grad, Curl, and All That , and one of the exercises reads as follows: Instead of using arrows to represent vector functions, we sometimes use families of curves called field lines . A curve $y = y(x)$ is a field line of the vector function $\mathbf{F}(x, y)$ if at each point $(x_0, y_0)$ on the curve, $\mathbf{F}(x_0, y_0)$ is tangent to the curve. Show that the field lines $y = y(x)$ of a vector function $$\mathbf{F}(x, y) = \mathbf{i}F_x(x, y) + \mathbf{j}F_y(x, y)$$ are solutions of the differential equation $$\frac{dy}{dx} = \frac{F_y(x, y)}{F_x(x, y)}.$$ (pp. 9–10) I understand the concepts here (or I think I do), and I understand that this proof makes sense. However, I don't really know how to go about proving it. It seems to me that the statement is self-evident: it's given that the vector function is tangent to the curve, so of course the slopes will be the same, by definition. Am I missing something here? How should I approach this? No solution is provided in the book (for any of the open-ended problems).","I'm reading Div, Grad, Curl, and All That , and one of the exercises reads as follows: Instead of using arrows to represent vector functions, we sometimes use families of curves called field lines . A curve $y = y(x)$ is a field line of the vector function $\mathbf{F}(x, y)$ if at each point $(x_0, y_0)$ on the curve, $\mathbf{F}(x_0, y_0)$ is tangent to the curve. Show that the field lines $y = y(x)$ of a vector function $$\mathbf{F}(x, y) = \mathbf{i}F_x(x, y) + \mathbf{j}F_y(x, y)$$ are solutions of the differential equation $$\frac{dy}{dx} = \frac{F_y(x, y)}{F_x(x, y)}.$$ (pp. 9–10) I understand the concepts here (or I think I do), and I understand that this proof makes sense. However, I don't really know how to go about proving it. It seems to me that the statement is self-evident: it's given that the vector function is tangent to the curve, so of course the slopes will be the same, by definition. Am I missing something here? How should I approach this? No solution is provided in the book (for any of the open-ended problems).",,"['calculus', 'multivariable-calculus', 'vectors']"
78,Differentiability of Linear Maps,Differentiability of Linear Maps,,"I am wondering whether all linear mappings have first-order partial derivatives (or stronger properties such as being continuously differentiable at all orders). Formally, suppose $A$ is an $m \times n$ matrix and define the mapping $F: \mathbb{R}^{n} \to \mathbb{R}^{m}$ by $$F(x) = Ax \text{ for every } x \in \mathbb{R}^{n}$$ It is a well known result that the above identity implies that $F$ is linear. Given this, do we immediately know that $F$ has first order partial derivatives? My thought process so far is that because $F$ must be linear in each of its components, and because a linear function is differentiable, then $F$ has first order partial derivatives. Am I on the right track?","I am wondering whether all linear mappings have first-order partial derivatives (or stronger properties such as being continuously differentiable at all orders). Formally, suppose $A$ is an $m \times n$ matrix and define the mapping $F: \mathbb{R}^{n} \to \mathbb{R}^{m}$ by $$F(x) = Ax \text{ for every } x \in \mathbb{R}^{n}$$ It is a well known result that the above identity implies that $F$ is linear. Given this, do we immediately know that $F$ has first order partial derivatives? My thought process so far is that because $F$ must be linear in each of its components, and because a linear function is differentiable, then $F$ has first order partial derivatives. Am I on the right track?",,"['real-analysis', 'multivariable-calculus']"
79,Show that $\int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}}<\infty$,Show that,\int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}}<\infty,"Here's my solution to an old qualifier problem. Would you tackle it differently? Is there a flaw in my work? Suppose that $\alpha_1, \dotsc, \alpha_n$ are positive numbers such that $$\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}<1.$$ Show that $$\int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}}<\infty.$$ My answer: $x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n} \overset{\text{(AM $\geq$ GM)}}{\geq} n\cdot \sqrt[n]{x_1^{\alpha_1}\dotsb x_n^{\alpha_n}}$ $$\Rightarrow \int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}} \leq \frac1n \int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1/n}\dotsb  x_n^{\alpha_n/n}},$$ at which point if we switch to hyperspherical coordinates $$\leq \frac1n \int_{S^{n-1}} \int_{1}^\infty \frac{dx_1 \dotsb dx_n}{r^{\frac{\alpha_1+\dotsb + \alpha_n}{n}}|f(\sigma)|}r^{n-1} dr\,d\sigma,\tag{1}$$ where $\sigma$ is the position on the unit $(n-1)$ sphere in $\mathbb{R}^n$ and $|f(\sigma)|\leq1$. Note I have chosen a crude bound by integrating over the entire area outside the unit ball. Now $$\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}<1 \\\Rightarrow \frac{n}{\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}}>n\\\overset{\text{AM $\geq$ HM}}{\Rightarrow}\frac{\alpha_1 + \dotsb+\alpha_n}{n}>n$$ so the integral (1) is integrating a power of $r$ which is $>1$ in the denominator, so it converges.","Here's my solution to an old qualifier problem. Would you tackle it differently? Is there a flaw in my work? Suppose that $\alpha_1, \dotsc, \alpha_n$ are positive numbers such that $$\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}<1.$$ Show that $$\int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}}<\infty.$$ My answer: $x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n} \overset{\text{(AM $\geq$ GM)}}{\geq} n\cdot \sqrt[n]{x_1^{\alpha_1}\dotsb x_n^{\alpha_n}}$ $$\Rightarrow \int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1}+\dotsb + x_n^{\alpha_n}} \leq \frac1n \int_{1}^\infty \dotsb\int_{1}^\infty \frac{dx_1 \dotsb dx_n}{x_1^{\alpha_1/n}\dotsb  x_n^{\alpha_n/n}},$$ at which point if we switch to hyperspherical coordinates $$\leq \frac1n \int_{S^{n-1}} \int_{1}^\infty \frac{dx_1 \dotsb dx_n}{r^{\frac{\alpha_1+\dotsb + \alpha_n}{n}}|f(\sigma)|}r^{n-1} dr\,d\sigma,\tag{1}$$ where $\sigma$ is the position on the unit $(n-1)$ sphere in $\mathbb{R}^n$ and $|f(\sigma)|\leq1$. Note I have chosen a crude bound by integrating over the entire area outside the unit ball. Now $$\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}<1 \\\Rightarrow \frac{n}{\frac1{\alpha_1}+ \dotsb + \frac1{\alpha_n}}>n\\\overset{\text{AM $\geq$ HM}}{\Rightarrow}\frac{\alpha_1 + \dotsb+\alpha_n}{n}>n$$ so the integral (1) is integrating a power of $r$ which is $>1$ in the denominator, so it converges.",,"['integration', 'multivariable-calculus', 'proof-verification']"
80,"Let $x = h(y, z), y = g(x, z), x = h(y, z)$ to calculate partial derivatives?",Let  to calculate partial derivatives?,"x = h(y, z), y = g(x, z), x = h(y, z)","Problem: A $\mathbb{R}^{3}$ surface is defined by  $F(x,\ y,\ z)=k$, where $k$ is a constant. Prove  $ \frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1 $. I don't see the first step to this problem, which is to let $x = h(y, z), y = g(x, z), x = h(y, z)$. Can someone clarify this please? Then when I differentiate $F(x,\ y,\ z)=c$, how can I figure out which of these functions, in terms of the other two, to use?","Problem: A $\mathbb{R}^{3}$ surface is defined by  $F(x,\ y,\ z)=k$, where $k$ is a constant. Prove  $ \frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1 $. I don't see the first step to this problem, which is to let $x = h(y, z), y = g(x, z), x = h(y, z)$. Can someone clarify this please? Then when I differentiate $F(x,\ y,\ z)=c$, how can I figure out which of these functions, in terms of the other two, to use?",,[]
81,Notes about evaluating double and triple integrals,Notes about evaluating double and triple integrals,,"I'm searching notes and exercises about multiple integrals to calculate volume of functions, but the information I find in internet is very bad. Can someone recommend me a book, pdf, videos, website... whatever to learn about this? The exercise type I have to learn to do are similar to the following: Calculate the volume inside these equations: $x^2+y^2=4, z=0, x+y+z=4$.","I'm searching notes and exercises about multiple integrals to calculate volume of functions, but the information I find in internet is very bad. Can someone recommend me a book, pdf, videos, website... whatever to learn about this? The exercise type I have to learn to do are similar to the following: Calculate the volume inside these equations: $x^2+y^2=4, z=0, x+y+z=4$.",,"['integration', 'reference-request', 'multivariable-calculus']"
82,Continuity of a vector function through continuity of its components,Continuity of a vector function through continuity of its components,,"Prove the following theorem: Let $f$ be a function from $\mathbb R$ to $\mathbb R^n$ and for $i=1,2,\ldots,n$ let $f_i\colon\mathbb R\to\mathbb R$ be the component functions of $f$, that is to say $f(t)= (f_1(t), \ldots, f_n(t))^T$. Then $f$ is continuous in a point $a \in \mathbb R$ if and only if for $i=1,\ldots,n$ the functions $f_i$ are all continuous. Here is what I tried: What is asked to prove is I think that the $\lim_{t\to a} f(t) = f(a)$. Since all component functions are continuous I know that the $\lim_{t\to a}f_i(t)=f_i(a)$. However, I don't know how to proceed from here. Could anyone please help me out?","Prove the following theorem: Let $f$ be a function from $\mathbb R$ to $\mathbb R^n$ and for $i=1,2,\ldots,n$ let $f_i\colon\mathbb R\to\mathbb R$ be the component functions of $f$, that is to say $f(t)= (f_1(t), \ldots, f_n(t))^T$. Then $f$ is continuous in a point $a \in \mathbb R$ if and only if for $i=1,\ldots,n$ the functions $f_i$ are all continuous. Here is what I tried: What is asked to prove is I think that the $\lim_{t\to a} f(t) = f(a)$. Since all component functions are continuous I know that the $\lim_{t\to a}f_i(t)=f_i(a)$. However, I don't know how to proceed from here. Could anyone please help me out?",,"['multivariable-calculus', 'continuity']"
83,Partial derivative of an implicit function,Partial derivative of an implicit function,,"I am trying to solve this question (sorry if the translation is a bit vague): $Z(x,y)$ is an implicit function of $x$ and $y$ given in the form of $$x^3z^2+\frac{2}{9}y^2\sin(z) = xyz$$ in the neighborhood of $x=2, y=\pi, z=\frac{\pi}{6}$. Find $Z_x$ and $Z_y$ at $(x,y) = (2,\pi)$ I know how to partially/totally differentiate, and I know how to find the derivative of a single-variable implicit function. How do I combine it to solve this? I have the solution in front of me but I can't understand it. Thanks!","I am trying to solve this question (sorry if the translation is a bit vague): $Z(x,y)$ is an implicit function of $x$ and $y$ given in the form of $$x^3z^2+\frac{2}{9}y^2\sin(z) = xyz$$ in the neighborhood of $x=2, y=\pi, z=\frac{\pi}{6}$. Find $Z_x$ and $Z_y$ at $(x,y) = (2,\pi)$ I know how to partially/totally differentiate, and I know how to find the derivative of a single-variable implicit function. How do I combine it to solve this? I have the solution in front of me but I can't understand it. Thanks!",,"['multivariable-calculus', 'implicit-differentiation']"
84,surface area of torus of revolution,surface area of torus of revolution,,"Here's a question from one of my exercises, Exercise 14. Let $C$ be a curve in $\mathbb{R}^2$ given by parametric equations $x=f(t)$ , $y=g(t)$ . Let $S$ be the surface of revolution of the curve $C$ about the $y$ -axis (something like the one shown in Figure 1). (a) Parametrize the surface $S$ . Parameters are $t$ and $\theta$ , where $0\leq\theta\leq2\pi$ . (b) Apply part (a) to parametrize the torus of revolution (see Figure 2) obtained by rotating the circle of radius $b$ centered at $(a,0)$ about the $y$ -axis. (Assume $a>b$ here.) I've got part (a) and (b), but how should I go about finding the surface area of this torus? Here's my solution to first 2 parts: (a) Applying cylindrical coordinates, since rotation is about $y$ -axis, let $x= r \cos \theta$ , $y = y$ and $z = r \sin \theta$ , at $\theta=0$ we get $f(t) = r$ , hence the parameterization is: $$x = f(t) \cos \theta\quad y = g(t) \quad z = f(t) \sin \theta$$ for $a \leq t \leq b$ and $0 \leq\theta\leq 2\pi$ . (b) In the plane $x,y$ the circle is $x = a + b \cos \psi$ , $y = b \sin \psi$ . From part (a), we get the following parametrization for the torus: $$x = (a + b \cos \psi)\cos \theta, \quad y = b \sin \psi, \quad z = (a + b \cos \psi)\sin \theta$$ where $0\leq\psi,\theta\leq 2\pi$ . What is the surface area of this torus?","Here's a question from one of my exercises, Exercise 14. Let be a curve in given by parametric equations , . Let be the surface of revolution of the curve about the -axis (something like the one shown in Figure 1). (a) Parametrize the surface . Parameters are and , where . (b) Apply part (a) to parametrize the torus of revolution (see Figure 2) obtained by rotating the circle of radius centered at about the -axis. (Assume here.) I've got part (a) and (b), but how should I go about finding the surface area of this torus? Here's my solution to first 2 parts: (a) Applying cylindrical coordinates, since rotation is about -axis, let , and , at we get , hence the parameterization is: for and . (b) In the plane the circle is , . From part (a), we get the following parametrization for the torus: where . What is the surface area of this torus?","C \mathbb{R}^2 x=f(t) y=g(t) S C y S t \theta 0\leq\theta\leq2\pi b (a,0) y a>b y x= r \cos \theta y = y z = r \sin \theta \theta=0 f(t) = r x = f(t) \cos \theta\quad y = g(t) \quad z = f(t) \sin \theta a \leq t \leq b 0 \leq\theta\leq 2\pi x,y x = a + b \cos \psi y = b \sin \psi x = (a + b \cos \psi)\cos \theta, \quad y = b \sin \psi, \quad z = (a + b \cos \psi)\sin \theta 0\leq\psi,\theta\leq 2\pi",['multivariable-calculus']
85,Wire mass line integral,Wire mass line integral,,"Wire is given with $y=\sqrt{25-x}$ and density is given with $ \delta(x,y)=15-y$. Mass should be calculated using line integral. This is my first assignment in this area and I need help with reasoning and modeling this problem as integral. Any help is appreciated.","Wire is given with $y=\sqrt{25-x}$ and density is given with $ \delta(x,y)=15-y$. Mass should be calculated using line integral. This is my first assignment in this area and I need help with reasoning and modeling this problem as integral. Any help is appreciated.",,['multivariable-calculus']
86,Finding the points where a tangent plane is parallel to a given plane?,Finding the points where a tangent plane is parallel to a given plane?,,"Find the points on the hyperboloid $9x^2- 45y^2 + 5z^2 - 45$ where the tangent plane is parallel to the plane $x+5y-2z = 7$? Can anyone help me figure this one out? So far, I've figured out the gradient of the hyperboloid but I'm not sure where to go from there..","Find the points on the hyperboloid $9x^2- 45y^2 + 5z^2 - 45$ where the tangent plane is parallel to the plane $x+5y-2z = 7$? Can anyone help me figure this one out? So far, I've figured out the gradient of the hyperboloid but I'm not sure where to go from there..",,['multivariable-calculus']
87,Trig substitution for a triple integral,Trig substitution for a triple integral,,"This problem involves calculating the triple integral of the following fraction, first with respect to $p$: $$ \int\limits_0^{2\pi} \int\limits_0^\pi \int\limits_0^{2}  \frac{p^2\sin(\phi)}{\sqrt{p^2 + 3}} dp d\phi d\theta $$ This involves trig-substitution (I believe), and I am just hoping for an explanation of how to do it.","This problem involves calculating the triple integral of the following fraction, first with respect to $p$: $$ \int\limits_0^{2\pi} \int\limits_0^\pi \int\limits_0^{2}  \frac{p^2\sin(\phi)}{\sqrt{p^2 + 3}} dp d\phi d\theta $$ This involves trig-substitution (I believe), and I am just hoping for an explanation of how to do it.",,"['calculus', 'integration', 'trigonometry', 'multivariable-calculus', 'definite-integrals']"
88,"Divergence Theorem, Laplacian, Energy Minimization","Divergence Theorem, Laplacian, Energy Minimization",,"I am trying to understand a proof for critical points of certain energy functions being harmonic functions. It goes as follows: For a function $u(x_1,..,x_n)$, a functional E(u) is defined as $E(u) = \frac{1}{2}\int |\nabla u|^2 $ For the proof, a curve is constructed in the space of functions by using $u+t\phi$. Restricting the energy functional to this curve gives: $E(u+t\phi) = \frac{1}{2} \int | \nabla (u+t\phi)|^2 =  \frac{1}{2} \int |\nabla u|^2 + t \int \langle u,\nabla \phi \rangle + \frac{t^2}{2} \int |\nabla \phi|^2 $ Then differentiating at $t=0$ $\frac{d}{dt_{t=0}}E(u+t\phi) = \int \langle\nabla u, \nabla \phi\rangle = - \int \phi \triangle u$ The proof then states that the last equality is due to divergence theorem which I know is $\int \int_S F \cdot n d\sigma = \int \int \int \nabla \cdot F dV$ I do not understand the connection between divergence theorem and how it is used in this proof. Any help? Link: http://arxiv.org/pdf/1102.1411 (page 2)","I am trying to understand a proof for critical points of certain energy functions being harmonic functions. It goes as follows: For a function $u(x_1,..,x_n)$, a functional E(u) is defined as $E(u) = \frac{1}{2}\int |\nabla u|^2 $ For the proof, a curve is constructed in the space of functions by using $u+t\phi$. Restricting the energy functional to this curve gives: $E(u+t\phi) = \frac{1}{2} \int | \nabla (u+t\phi)|^2 =  \frac{1}{2} \int |\nabla u|^2 + t \int \langle u,\nabla \phi \rangle + \frac{t^2}{2} \int |\nabla \phi|^2 $ Then differentiating at $t=0$ $\frac{d}{dt_{t=0}}E(u+t\phi) = \int \langle\nabla u, \nabla \phi\rangle = - \int \phi \triangle u$ The proof then states that the last equality is due to divergence theorem which I know is $\int \int_S F \cdot n d\sigma = \int \int \int \nabla \cdot F dV$ I do not understand the connection between divergence theorem and how it is used in this proof. Any help? Link: http://arxiv.org/pdf/1102.1411 (page 2)",,"['calculus', 'multivariable-calculus', 'calculus-of-variations']"
89,Derivative of cross product w.r.t. a vector,Derivative of cross product w.r.t. a vector,,"How to compute the derivative of $\vec{a}\times\vec{b}$ w.r.t. $\vec{c}$ , all of which are 3D vectors for simplicity? Here $\vec{a}(\vec{c})$ and $\vec{b}(\vec{c})$ are both dependent on $\vec{c}$ . I know that the result should be a $3\times3$ matrix. And we cannot simply apply the chain rule as $$\frac{\partial(\vec{a}\times\vec{b})}{\partial \vec{c}} = \frac{\partial\vec{a}}{\partial\vec{c}} \times \vec{b} + \vec{a} \times \frac{\partial\vec{b}}{\partial\vec{c}}$$ as $\frac{\partial\vec{a}}{\partial\vec{c}}$ would be a matrix then. Is there any analytical expression for the result? For example, $$\frac{\partial\vec{x}}{\partial\vec{x}} = \left[\array{1&0&0\\0&1&0\\0&0&1}\right]$$","How to compute the derivative of w.r.t. , all of which are 3D vectors for simplicity? Here and are both dependent on . I know that the result should be a matrix. And we cannot simply apply the chain rule as as would be a matrix then. Is there any analytical expression for the result? For example,",\vec{a}\times\vec{b} \vec{c} \vec{a}(\vec{c}) \vec{b}(\vec{c}) \vec{c} 3\times3 \frac{\partial(\vec{a}\times\vec{b})}{\partial \vec{c}} = \frac{\partial\vec{a}}{\partial\vec{c}} \times \vec{b} + \vec{a} \times \frac{\partial\vec{b}}{\partial\vec{c}} \frac{\partial\vec{a}}{\partial\vec{c}} \frac{\partial\vec{x}}{\partial\vec{x}} = \left[\array{1&0&0\\0&1&0\\0&0&1}\right],"['calculus', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'cross-product']"
90,"Are points and vectors (in $\mathbb{R}^n$) different objects? If yes, then why can we switch between them in a proof?","Are points and vectors (in ) different objects? If yes, then why can we switch between them in a proof?",\mathbb{R}^n,"Context In Hubbard and Hubbard's book on vector calculus, $\mathbb{R}^{n}$ is defined as the space of ordered lists of $n$ real numbers. The authors then say that a given element of $\mathbb{R}^{n}$ can be interpreted in the following two ways: An element of $\mathbb{R}^{n}$ is said to be a "" point "" (in $\mathbb{R}^{n}$ ) if it represents some sort of position/state. An element of $\mathbb{R}^{n}$ is said to be a "" vector "" (in $\mathbb{R}^{n}$ ) if it represents some sort of change/increment. Then the book pauses to emphasize that two points cannot be added $(\text{New York}+\text{Boston}=???),$ but two vectors can. It also goes on to define scalar multiplication for vectors, difference of two points, sum of a point and a vector and sum of two vectors. The authors give the following remark: ""An element of $\mathbb{R}^{n}$ is an ordered list of numbers whether it is interpreted as a point or as a vector. But we have very different images of points and vectors, and we hope that sharing them with you explicitly will help you build a sound intuition. In linear algebra, you should think of elements of $\mathbb{R}^{n}$ as vectors. However, differential calculus is all about increments to points. It is because the increments are vectors that linear algebra is a prerequisite for multivariate calculus: it provides the right language and tools for discussing these increments."" Another important remark is also given: ""Sometimes, often at a key point in a proof, we will suddenly start thinking of points as vectors, or vice versa."" Reflections on H+H's Explanation It seems like Hubbard and Hubbard are suggesting that points and vectors are nothing more than labels we give to the same object (a real $n$ -tuple). We choose which label we want to give a $n$ -tuple based on what we are using the $n$ -tuple for/how we are thinking about it. In this case, points and vectors are the same mathematical object ( $n$ -tuple), and the only reason for why we can't scale points, or add points to other points is because it breaks our ""mental model/interpretation"" of points being $n$ -tuples that represent locations/states ( $\text{New York+Boston=???, and 5}\cdot \text{New York=???}$ ). Also, points and vectors being the same objects means there is no harm in switching between the two (provided we don't switch one of them in a way that breaks our interpretation of points/vectors. E.g., if we have $x+y$ , for vectors $x$ and $y$ , we can switch $x$ with a point and leave $y$ alone (or vice versa), but we cannot switch both $x$ and $y$ out for points because we can't add points). This justifies the second remark from the book. This all makes sense to me. Points and vectors in $\mathbb{R}^{n}$ are the same objects ( $n$ -tuples), and we just use these two different terms to provide additional context to how we are thinking about/visualizing/using the $n$ -tuple. Points represent locations/states, vectors represent changes/increments. The ""rules"" of not being able to add two points and not being scale a point are solely there to ensure that our interpretation of what the $n$ -tuple is representing is consistent with our intuition $(\text{New York + Boston=???, 5}\cdot \text{Boston=???}).$ But I have run into a few problems. Questions In some of the popular threads of similar questions it seems like some people are claiming that points and vectors in $\mathbb{R}^{n}$ actually ARE different mathematical objects (See What is the difference between a point and a vector? ). That's not what I thought at first reading H+H, but ok, it seems reasonable that points and vectors are different mathematical objects, rather than just contextual indicators for the same object. They are used to represent different things (location vs displacement) after all, and they have different operations that can be performed on them. So I have the following question. Question 1: Are points and vectors (in $\mathbb{R}^{n}$ ) simply different interpretations of the same mathematical object ( $n$ -tuple), or are they fundamentally different objects? If they are different, how are each of them defined? And if points and vectors ARE different mathematical objects, then how can we reconcile this with the fact that we want to be able to switch between points and vectors in the middle of proofs (Remark 2 from H+H)? This is the second question I have. Question 2: If points and vectors in $\mathbb{R}^{n}$ are different mathematical objects, how are we able to switch between these two distinct objects in the middle of a proof and still have our proof be valid (i.e., How do we reconcile the fact they are different with remark $2$ from the book)? Any help at all would be extremely appreciated!","Context In Hubbard and Hubbard's book on vector calculus, is defined as the space of ordered lists of real numbers. The authors then say that a given element of can be interpreted in the following two ways: An element of is said to be a "" point "" (in ) if it represents some sort of position/state. An element of is said to be a "" vector "" (in ) if it represents some sort of change/increment. Then the book pauses to emphasize that two points cannot be added but two vectors can. It also goes on to define scalar multiplication for vectors, difference of two points, sum of a point and a vector and sum of two vectors. The authors give the following remark: ""An element of is an ordered list of numbers whether it is interpreted as a point or as a vector. But we have very different images of points and vectors, and we hope that sharing them with you explicitly will help you build a sound intuition. In linear algebra, you should think of elements of as vectors. However, differential calculus is all about increments to points. It is because the increments are vectors that linear algebra is a prerequisite for multivariate calculus: it provides the right language and tools for discussing these increments."" Another important remark is also given: ""Sometimes, often at a key point in a proof, we will suddenly start thinking of points as vectors, or vice versa."" Reflections on H+H's Explanation It seems like Hubbard and Hubbard are suggesting that points and vectors are nothing more than labels we give to the same object (a real -tuple). We choose which label we want to give a -tuple based on what we are using the -tuple for/how we are thinking about it. In this case, points and vectors are the same mathematical object ( -tuple), and the only reason for why we can't scale points, or add points to other points is because it breaks our ""mental model/interpretation"" of points being -tuples that represent locations/states ( ). Also, points and vectors being the same objects means there is no harm in switching between the two (provided we don't switch one of them in a way that breaks our interpretation of points/vectors. E.g., if we have , for vectors and , we can switch with a point and leave alone (or vice versa), but we cannot switch both and out for points because we can't add points). This justifies the second remark from the book. This all makes sense to me. Points and vectors in are the same objects ( -tuples), and we just use these two different terms to provide additional context to how we are thinking about/visualizing/using the -tuple. Points represent locations/states, vectors represent changes/increments. The ""rules"" of not being able to add two points and not being scale a point are solely there to ensure that our interpretation of what the -tuple is representing is consistent with our intuition But I have run into a few problems. Questions In some of the popular threads of similar questions it seems like some people are claiming that points and vectors in actually ARE different mathematical objects (See What is the difference between a point and a vector? ). That's not what I thought at first reading H+H, but ok, it seems reasonable that points and vectors are different mathematical objects, rather than just contextual indicators for the same object. They are used to represent different things (location vs displacement) after all, and they have different operations that can be performed on them. So I have the following question. Question 1: Are points and vectors (in ) simply different interpretations of the same mathematical object ( -tuple), or are they fundamentally different objects? If they are different, how are each of them defined? And if points and vectors ARE different mathematical objects, then how can we reconcile this with the fact that we want to be able to switch between points and vectors in the middle of proofs (Remark 2 from H+H)? This is the second question I have. Question 2: If points and vectors in are different mathematical objects, how are we able to switch between these two distinct objects in the middle of a proof and still have our proof be valid (i.e., How do we reconcile the fact they are different with remark from the book)? Any help at all would be extremely appreciated!","\mathbb{R}^{n} n \mathbb{R}^{n} \mathbb{R}^{n} \mathbb{R}^{n} \mathbb{R}^{n} \mathbb{R}^{n} (\text{New York}+\text{Boston}=???), \mathbb{R}^{n} \mathbb{R}^{n} n n n n n \text{New York+Boston=???, and 5}\cdot \text{New York=???} x+y x y x y x y \mathbb{R}^{n} n n n (\text{New York + Boston=???, 5}\cdot \text{Boston=???}). \mathbb{R}^{n} \mathbb{R}^{n} n \mathbb{R}^{n} 2","['linear-algebra', 'multivariable-calculus', 'differential-geometry', 'real-numbers']"
91,Asymptotic behavior of the inverse Fourier transform of $\frac{1}{|k|^2 + 1}$,Asymptotic behavior of the inverse Fourier transform of,\frac{1}{|k|^2 + 1},"This math.SE answer calculated the following (inverse) Fourier transform in $n$ -dimensions $$ f(x) = \int_{\mathbb{R}^n} d^nk  \frac{e^{ik\cdot x}}{|k|^2 + 1} \propto |x|^{-\frac{n}{2}+1} K_{\frac{n}{2}-1}(|x|) \tag{1} $$ where $K_\alpha(x)$ is the modified Bessel function of the second kind . But in some physics applications, one is only interested in the large- $|x|$ asymptotic behavior of $f(x)$ . Using the asymptotic expansion $$ K_\alpha(x) \propto x^{-1/2} e^{-x} [1 + O(x^{-1})] \quad \text{when } \mathrm{arg}(x) < 3\pi/2 $$ we obtain (which reproduces Eq. (14.23) in Assa Auerbach's Interacting Electrons and Quantum Magnetism ) $$ f(x) \sim |x|^{-(n-1)/2} e^{-x} \tag{2} $$ My question is: is there a simpler way than the cited math.SE answer that directly aims to find the asymptotic behavior of $f(x)$ for large $|x|$ ?","This math.SE answer calculated the following (inverse) Fourier transform in -dimensions where is the modified Bessel function of the second kind . But in some physics applications, one is only interested in the large- asymptotic behavior of . Using the asymptotic expansion we obtain (which reproduces Eq. (14.23) in Assa Auerbach's Interacting Electrons and Quantum Magnetism ) My question is: is there a simpler way than the cited math.SE answer that directly aims to find the asymptotic behavior of for large ?","n 
f(x) = \int_{\mathbb{R}^n} d^nk 
\frac{e^{ik\cdot x}}{|k|^2 + 1}
\propto |x|^{-\frac{n}{2}+1} K_{\frac{n}{2}-1}(|x|)
\tag{1}
 K_\alpha(x) |x| f(x) 
K_\alpha(x) \propto x^{-1/2} e^{-x} [1 + O(x^{-1})]
\quad \text{when } \mathrm{arg}(x) < 3\pi/2
 
f(x) \sim |x|^{-(n-1)/2} e^{-x}
\tag{2}
 f(x) |x|","['multivariable-calculus', 'asymptotics', 'fourier-transform']"
92,Calculate the integral over the half ellipse,Calculate the integral over the half ellipse,,Calculate the integral $\displaystyle \iint_E x^2 dA$ where E is the half ellipse given by the inequalities $x^2 + 2y^2 \le 2$ and $y \ge 0$ . This is what I did: $$\begin {align}\iint_E x^2 dA &=\iint r^3 \cos^2(\theta) dr d\theta\\ &= \int_1^{\sqrt2}r^3 \times \int_0^{\pi} \frac12(\cos(2\theta)+1) dr d \theta\\ &= \left[\frac{r^4}4\right]_1^{\sqrt2}\left[\frac{\sin(2\theta)}2+ \frac{\theta}2\right]_0^{\pi}\\ &= \left(0+\frac{\pi}2-0\right)\left (\frac44 - \frac14\right)\\ &= \frac{3\pi}4\end{align}$$ This is wrong and the correct answer is $\dfrac{\sqrt{2\pi}}4$,Calculate the integral where E is the half ellipse given by the inequalities and . This is what I did: This is wrong and the correct answer is,\displaystyle \iint_E x^2 dA x^2 + 2y^2 \le 2 y \ge 0 \begin {align}\iint_E x^2 dA &=\iint r^3 \cos^2(\theta) dr d\theta\\ &= \int_1^{\sqrt2}r^3 \times \int_0^{\pi} \frac12(\cos(2\theta)+1) dr d \theta\\ &= \left[\frac{r^4}4\right]_1^{\sqrt2}\left[\frac{\sin(2\theta)}2+ \frac{\theta}2\right]_0^{\pi}\\ &= \left(0+\frac{\pi}2-0\right)\left (\frac44 - \frac14\right)\\ &= \frac{3\pi}4\end{align} \dfrac{\sqrt{2\pi}}4,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'partial-derivative']"
93,Questions about Stokes's Theorem on Manifolds from Prof Shifrin's Lectures,Questions about Stokes's Theorem on Manifolds from Prof Shifrin's Lectures,,"I've been watching Professor Ted Shifrin's brilliant lectures on Stokes's Theorem, and I had a few questions that I don't think were answered: Rectangles in the plane are not manifolds with boundary as the lectures define them, so I don't think Stokes's Theorem (as discussed in the lectures) would apply to them. But Professor Shifrin said that Stokes's Theorem was a generalization of Green's Theorem, which was grounded in rectangles in the lectures. How do these two ideas hold at once? Professor Shifrin provided an example of a region on which the integral of a differential form on which wasn't parametrized ""exactly,"" but the integral over the small segment canceled out. Then, he asserted that a similar cancellation would not occur when integrating over a Möbius strip. (The relevant portion of the lecture starts here: https://youtu.be/5k13cowATAw?t=644 ) Why doesn't a similar cancellation occur when integrating over a Möbius strip? I would think that the segment would have opposite orientation when parametrized each time. Why does the integral on that segment even matter? (I know this is probably a stupid question.) On first glance, my thought was that this boundary region had 2d volume zero, so it wouldn't matter in the grand scheme of integration. Professor Shifrin said that the Möbius strip could be parametrized locally but not globally. Why don't we define the integral over the whole manifold to be the sum of the integrals along each coordinate patch, similar to what we do for piecewise smooth manifolds? Thank you so much for your help!!","I've been watching Professor Ted Shifrin's brilliant lectures on Stokes's Theorem, and I had a few questions that I don't think were answered: Rectangles in the plane are not manifolds with boundary as the lectures define them, so I don't think Stokes's Theorem (as discussed in the lectures) would apply to them. But Professor Shifrin said that Stokes's Theorem was a generalization of Green's Theorem, which was grounded in rectangles in the lectures. How do these two ideas hold at once? Professor Shifrin provided an example of a region on which the integral of a differential form on which wasn't parametrized ""exactly,"" but the integral over the small segment canceled out. Then, he asserted that a similar cancellation would not occur when integrating over a Möbius strip. (The relevant portion of the lecture starts here: https://youtu.be/5k13cowATAw?t=644 ) Why doesn't a similar cancellation occur when integrating over a Möbius strip? I would think that the segment would have opposite orientation when parametrized each time. Why does the integral on that segment even matter? (I know this is probably a stupid question.) On first glance, my thought was that this boundary region had 2d volume zero, so it wouldn't matter in the grand scheme of integration. Professor Shifrin said that the Möbius strip could be parametrized locally but not globally. Why don't we define the integral over the whole manifold to be the sum of the integrals along each coordinate patch, similar to what we do for piecewise smooth manifolds? Thank you so much for your help!!",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
94,Fundamental Theorem of Calculus with multiple variables,Fundamental Theorem of Calculus with multiple variables,,"I'm stuck on this multivariable equation: $$ \frac{d}{dx}\left(\int^x_af(g(b,t),t)dt\right) $$ where a and b are just constants. If this involved a single variable, it looks like one would just apply the fundamental theorem of calculus. Is there an equivalent for multiple variables. I know that the answer should just be $$ f(g(b,x),x) $$ but I'm hoping someone can explain / walk me through. Is there maybe some rule that lets me pass the $\frac{d}{dx}$ into the integral? Thanks","I'm stuck on this multivariable equation: where a and b are just constants. If this involved a single variable, it looks like one would just apply the fundamental theorem of calculus. Is there an equivalent for multiple variables. I know that the answer should just be but I'm hoping someone can explain / walk me through. Is there maybe some rule that lets me pass the into the integral? Thanks","
\frac{d}{dx}\left(\int^x_af(g(b,t),t)dt\right)
 
f(g(b,x),x)
 \frac{d}{dx}","['calculus', 'multivariable-calculus']"
95,Is there a formula for $\nabla^n(fg)$?,Is there a formula for ?,\nabla^n(fg),"Applying the ordinary Leibniz rule multiple times leads to the general Leibniz rule for $n$ th derivatives of $fg$ , namely $(fg)^{(n)}=\sum_{k=0}^n\binom{n}{k}f^{(n-k)}g^{(k)}$ . Is there an analogous formula for repeated gradients/divergences of a product of scalar functions? That is, $\nabla^n(fg)=$ ? Of course $\nabla^nf$ is understood to mean $\cdots\nabla(\nabla\cdot(\nabla f))$ , with $\nabla$ appearing $n$ times, either as a gradient or divergence according to what makes sense. While some identities involving $\nabla$ look very similar to the Leibniz rule, the problem is that $\nabla(X\cdot Y)\neq(\nabla X)Y+X\nabla Y$ . (If equality held here, I think a form of the generalized Leibniz rule would follow easily.) Let's agree to keep expressions like this unevaluated, so that no curls appear, nor any other operations other than gradients and divergences. I.e., the final formula can (and probably must) contain expressions like $\nabla[(\nabla f)\cdot(\nabla g)]$ . That said, if there is an alternative notation (e.g., tensor indices) that makes the formula easier to write, that would also be fine.","Applying the ordinary Leibniz rule multiple times leads to the general Leibniz rule for th derivatives of , namely . Is there an analogous formula for repeated gradients/divergences of a product of scalar functions? That is, ? Of course is understood to mean , with appearing times, either as a gradient or divergence according to what makes sense. While some identities involving look very similar to the Leibniz rule, the problem is that . (If equality held here, I think a form of the generalized Leibniz rule would follow easily.) Let's agree to keep expressions like this unevaluated, so that no curls appear, nor any other operations other than gradients and divergences. I.e., the final formula can (and probably must) contain expressions like . That said, if there is an alternative notation (e.g., tensor indices) that makes the formula easier to write, that would also be fine.",n fg (fg)^{(n)}=\sum_{k=0}^n\binom{n}{k}f^{(n-k)}g^{(k)} \nabla^n(fg)= \nabla^nf \cdots\nabla(\nabla\cdot(\nabla f)) \nabla n \nabla \nabla(X\cdot Y)\neq(\nabla X)Y+X\nabla Y \nabla[(\nabla f)\cdot(\nabla g)],"['multivariable-calculus', 'derivatives', 'vector-analysis', 'divergence-operator']"
96,Expressing a solid in spherical coordinates,Expressing a solid in spherical coordinates,,"I am trying to solve the following question: The volume of the solid $E$ can be represented as $$\int_{-3}^3 \int_0^{\sqrt{9-x^2}} 3 - \sqrt{x^2+y^2} dydx$$ Describe the solid in spherical coordinates. I graphed the solid and it looks like this: So clearly $0 \le \theta \le \pi$ and $0 \le \phi \le \frac{\pi}{2}.$ Also, for each $(\theta, \phi)$ the value of $\rho$ ranges from $0$ until the surface of the cone. The cone is $z = 3-\sqrt{x^2+y^2}$ . Using the substitutions \begin{align*} x &= \rho \cos \theta \sin \phi\\ y &= \rho \sin \theta \sin \phi\\ z &= \rho \cos \phi \end{align*} we get \begin{align*} \rho \cos \phi &= 3 - \sqrt{\rho^2 \sin^2 \phi}\\ \rho &= \frac {3}{\cos \phi + \sin \phi} \end{align*} So my answer is $$E_{\text{spherical}} = \{(\rho, \theta, \phi)| 0 \le \theta \le \pi, 0 \le \phi \le \frac{\pi}{2}, 0 \le \rho \le \frac {3}{\cos \phi + \sin \phi} \}$$ However, the correct answer is $$E_{\text{spherical}} = \{(\rho, \theta, \phi)| 0 \le \theta \le \pi, 0 \le \phi \le \frac{\pi}{4}, 0 \le \rho \le 3 \csc\phi \}$$ Did I make any mistake? Thanks!","I am trying to solve the following question: The volume of the solid can be represented as Describe the solid in spherical coordinates. I graphed the solid and it looks like this: So clearly and Also, for each the value of ranges from until the surface of the cone. The cone is . Using the substitutions we get So my answer is However, the correct answer is Did I make any mistake? Thanks!","E \int_{-3}^3 \int_0^{\sqrt{9-x^2}} 3 - \sqrt{x^2+y^2} dydx 0 \le \theta \le \pi 0 \le \phi \le \frac{\pi}{2}. (\theta, \phi) \rho 0 z = 3-\sqrt{x^2+y^2} \begin{align*}
x &= \rho \cos \theta \sin \phi\\
y &= \rho \sin \theta \sin \phi\\
z &= \rho \cos \phi
\end{align*} \begin{align*}
\rho \cos \phi &= 3 - \sqrt{\rho^2 \sin^2 \phi}\\
\rho &= \frac {3}{\cos \phi + \sin \phi}
\end{align*} E_{\text{spherical}} = \{(\rho, \theta, \phi)| 0 \le \theta \le \pi, 0 \le \phi \le \frac{\pi}{2}, 0 \le \rho \le \frac {3}{\cos \phi + \sin \phi} \} E_{\text{spherical}} = \{(\rho, \theta, \phi)| 0 \le \theta \le \pi, 0 \le \phi \le \frac{\pi}{4}, 0 \le \rho \le 3 \csc\phi \}","['multivariable-calculus', '3d']"
97,motivation of the need to use higher order infinitesimal when defining derivative,motivation of the need to use higher order infinitesimal when defining derivative,,"I am trying to derive the concept of derivative and differential from limit and linear approximation for reviewing the subject. And I cant figure the motivation of using higher order infinitesimal as a requirement to define derivative during the middle of it. Here is what I did [Step 1] : I start by supposing the only thing I know is the concept of limit. And I should proceed to develop the notion of derivative from the idea of linear approximation [Step 2] : Assume a single variable real function $f(x)$ as example, suppose $f(x)$ is defined on an interval $[(a-r), (a+r)]$ where $r$ is positive. What I want to do is to estimate the value of any $f(x)$ within this interval by using linear approximation: $$f(x)=A(x-a)+f(a)+E$$ where $E$ is the error of approximation $$E=[f(x)-f(a)]-A(x-a)$$ [Step 3]: As the constant $A$ can be chosen randomly, if I want to bring in the notion of derivative , I have to find some sort of motivation that requires me to find a specific $A$ which satisfies $$\lim\limits_{x\to a}\frac{E}{(x-a)}=0$$ that is to say the requirement for $A$ is that it has to make $E$ a higher order infinitesimal with respect to $(x-a)$ . Question : [Q] : So what is actually the motivation behind such requirement? [Q1.1] : I understand from the geometrical perspective it signifies the tangent line, but then what makes the tangent line so special that brings me the motivation to use it as my linear approximation constant $A$ ? [Q1.2] : Moreover, what is the algebraical motivation behind such requirement without considering the geometrical interpretation ?","I am trying to derive the concept of derivative and differential from limit and linear approximation for reviewing the subject. And I cant figure the motivation of using higher order infinitesimal as a requirement to define derivative during the middle of it. Here is what I did [Step 1] : I start by supposing the only thing I know is the concept of limit. And I should proceed to develop the notion of derivative from the idea of linear approximation [Step 2] : Assume a single variable real function as example, suppose is defined on an interval where is positive. What I want to do is to estimate the value of any within this interval by using linear approximation: where is the error of approximation [Step 3]: As the constant can be chosen randomly, if I want to bring in the notion of derivative , I have to find some sort of motivation that requires me to find a specific which satisfies that is to say the requirement for is that it has to make a higher order infinitesimal with respect to . Question : [Q] : So what is actually the motivation behind such requirement? [Q1.1] : I understand from the geometrical perspective it signifies the tangent line, but then what makes the tangent line so special that brings me the motivation to use it as my linear approximation constant ? [Q1.2] : Moreover, what is the algebraical motivation behind such requirement without considering the geometrical interpretation ?","f(x) f(x) [(a-r), (a+r)] r f(x) f(x)=A(x-a)+f(a)+E E E=[f(x)-f(a)]-A(x-a) A A \lim\limits_{x\to a}\frac{E}{(x-a)}=0 A E (x-a) A","['calculus', 'multivariable-calculus', 'derivatives']"
98,Existance of multivariable limit,Existance of multivariable limit,,"We are asked to find whether the limit $$ \lim_{(x,y)\to(0,0)}\frac{x^3y^3}{x^8+y^4} $$ exists. I tried to find through the epsilon-delta definition but wasn't able to apply it successfully. So I tried to come up with a convergent sequence $(x_n)$ such that it converges to $(0,0)$ but $f(x_n)$ does not converge to a single point, thus establishing non-existence of the limit. But I wasn't able to come up with any such sequence. When I checked on Wolfram-Alpha it showed the limit to be non existent. Can someone provide a proof of this or give example of a sequence $(x_n)$ converging to $(0,0)$ such that such that $f(x_n)$ does not converge ?","We are asked to find whether the limit exists. I tried to find through the epsilon-delta definition but wasn't able to apply it successfully. So I tried to come up with a convergent sequence such that it converges to but does not converge to a single point, thus establishing non-existence of the limit. But I wasn't able to come up with any such sequence. When I checked on Wolfram-Alpha it showed the limit to be non existent. Can someone provide a proof of this or give example of a sequence converging to such that such that does not converge ?","
\lim_{(x,y)\to(0,0)}\frac{x^3y^3}{x^8+y^4}
 (x_n) (0,0) f(x_n) (x_n) (0,0) f(x_n)",['multivariable-calculus']
99,"Geometrical Interpretation of the general solution of a first order, quasi linear partial differential equation","Geometrical Interpretation of the general solution of a first order, quasi linear partial differential equation",,"In a course on partial differential equations I came through this theorem about the general solution of a first order quasi-linear partial differential equation. The general solution of a first-order, quasi-linear partial differential equation $$a(x,y,u)u_x + b(x,y,u)u_y = c(x,y,u)$$ is given by $f(ϕ,ψ)=0$ , where $f$ is an arbitrary function of $ϕ(x,y,u)$ and $ψ(x,y,u).$ $ϕ=C1$ and $ψ=C2$ are solution curves of the characteristic equations $$\frac{dx}{a}=\frac{dy}{b}=\frac{du}{c}.$$ Is there any geometric interpretation of both these points so that I can have a better intuitive understanding of the graphical representation of $f$ , $\phi$ and $\psi$ ?","In a course on partial differential equations I came through this theorem about the general solution of a first order quasi-linear partial differential equation. The general solution of a first-order, quasi-linear partial differential equation is given by , where is an arbitrary function of and and are solution curves of the characteristic equations Is there any geometric interpretation of both these points so that I can have a better intuitive understanding of the graphical representation of , and ?","a(x,y,u)u_x + b(x,y,u)u_y = c(x,y,u) f(ϕ,ψ)=0 f ϕ(x,y,u) ψ(x,y,u). ϕ=C1 ψ=C2 \frac{dx}{a}=\frac{dy}{b}=\frac{du}{c}. f \phi \psi","['multivariable-calculus', 'partial-differential-equations']"
