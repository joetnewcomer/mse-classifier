,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What can we say about the graph when many eigenvalues of the Laplacian are equal to 1?,What can we say about the graph when many eigenvalues of the Laplacian are equal to 1?,,"The Laplacian of the graph has all the eigenvalues real and non-negative, the smallest being 0. I have a graph where the second smallest eigenvalue (the so called algebraic connectivity) is equal to $1$ . In fact, the multiplicity of this eigenvalue is quite high: in other words, many eigenvalues of the Laplacian are equal to $1$ . What can we say about the graph when many eigenvalues of the Laplacian are equal to 1? For example, eigenvalue $0$ implies that the row sum is $0$ . What about eigenvalue $1$ with high multiplicity? In my case, the Laplacian is defined as $L = D - A$ , where $D$ is the degree matrix (diagonal matrix with degree values on the diagonal) and $A$ is the adjacency matrix of the graph.","The Laplacian of the graph has all the eigenvalues real and non-negative, the smallest being 0. I have a graph where the second smallest eigenvalue (the so called algebraic connectivity) is equal to . In fact, the multiplicity of this eigenvalue is quite high: in other words, many eigenvalues of the Laplacian are equal to . What can we say about the graph when many eigenvalues of the Laplacian are equal to 1? For example, eigenvalue implies that the row sum is . What about eigenvalue with high multiplicity? In my case, the Laplacian is defined as , where is the degree matrix (diagonal matrix with degree values on the diagonal) and is the adjacency matrix of the graph.",1 1 0 0 1 L = D - A D A,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'graph-laplacian']"
1,Matrix exponential of a skew-symmetric matrix without series expansion,Matrix exponential of a skew-symmetric matrix without series expansion,,I have the following skew-symmetric matrix $$C = \begin{bmatrix}   0 & -a_3 &  a_2 \\                      a_3 &    0 & -a_1 \\                     -a_2 &  a_1 &    0 \end{bmatrix}$$ How do I compute $e^{C}$ without resorting to the series expansion of $e^{C}$ ? Shall I get a finite expression for it? NB:  Values of $a_i$ s can't be changed.,I have the following skew-symmetric matrix How do I compute without resorting to the series expansion of ? Shall I get a finite expression for it? NB:  Values of s can't be changed.,"C = \begin{bmatrix}   0 & -a_3 &  a_2 \\
                     a_3 &    0 & -a_1 \\
                    -a_2 &  a_1 &    0 \end{bmatrix} e^{C} e^{C} a_i","['matrices', 'matrix-calculus', 'matrix-exponential', 'skew-symmetric-matrices']"
2,Can you add a scalar to a matrix?,Can you add a scalar to a matrix?,,"If I add a scalar to every element of a matrix, e.g. for a $2\times2$ matrix $$ \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} + b \overset{?}{=} \begin{pmatrix}a_{11}+b & a_{12}+b \\ a_{21}+b & a_{22}+b\end{pmatrix},$$ with $b$ a scalar, then what is the correct notation? Matrix addition and subtraction are only defined for matrices of the same size. However, it seems tedious to first multiply $b$ with a matrix of ones to have two same-sized matrices to add: $$ J_2 = \begin{pmatrix} 1 & 1 \\ 1 & 1\end{pmatrix}.$$ Thus to write: $$ \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} + bJ_2  = \begin{pmatrix}a_{11}+b & a_{12}+b \\ a_{21}+b & a_{22}+b\end{pmatrix}.$$ Do you always write $A+bJ_d$ (with $d$ the dimensions of $A$)? Another notation would be $A+\mathbf{b}$ (bold $b$), implying a matrix of the size of $A$. However, this notation is also used for the multiplication of $b$ with the identity matrix, $bI_d$, which is different and therefore confusing. Why is the addition of a scalar to a matrix not simply defined like scalar multiplication, i.e. an operation of every matrix element? An example where this is permitted is the MATLAB language, where you can add a scalar to a matrix $A$ simply by addition: e.g. A+3 . I feel this is a logical choice. Addition of a scalar to a matrix could be defined as $A+b = A+bJ_d$, with $d$ the dimensions of $A$. This is commutative and associative, just like regular matrix addition. Then $A+\mathbf{b}$ would be the addition of $A$ and $bI_d$ and $A+B$ the matrix addition as we know it, only valid for matrices of the same dimensions. Why aren't these the definitions?","If I add a scalar to every element of a matrix, e.g. for a $2\times2$ matrix $$ \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} + b \overset{?}{=} \begin{pmatrix}a_{11}+b & a_{12}+b \\ a_{21}+b & a_{22}+b\end{pmatrix},$$ with $b$ a scalar, then what is the correct notation? Matrix addition and subtraction are only defined for matrices of the same size. However, it seems tedious to first multiply $b$ with a matrix of ones to have two same-sized matrices to add: $$ J_2 = \begin{pmatrix} 1 & 1 \\ 1 & 1\end{pmatrix}.$$ Thus to write: $$ \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix} + bJ_2  = \begin{pmatrix}a_{11}+b & a_{12}+b \\ a_{21}+b & a_{22}+b\end{pmatrix}.$$ Do you always write $A+bJ_d$ (with $d$ the dimensions of $A$)? Another notation would be $A+\mathbf{b}$ (bold $b$), implying a matrix of the size of $A$. However, this notation is also used for the multiplication of $b$ with the identity matrix, $bI_d$, which is different and therefore confusing. Why is the addition of a scalar to a matrix not simply defined like scalar multiplication, i.e. an operation of every matrix element? An example where this is permitted is the MATLAB language, where you can add a scalar to a matrix $A$ simply by addition: e.g. A+3 . I feel this is a logical choice. Addition of a scalar to a matrix could be defined as $A+b = A+bJ_d$, with $d$ the dimensions of $A$. This is commutative and associative, just like regular matrix addition. Then $A+\mathbf{b}$ would be the addition of $A$ and $bI_d$ and $A+B$ the matrix addition as we know it, only valid for matrices of the same dimensions. Why aren't these the definitions?",,"['matrices', 'notation', 'arithmetic']"
3,How to prove that the normalizer of diagonal matrices in $GL_n$ is the subgroup of generalized permutation matrices?,How to prove that the normalizer of diagonal matrices in  is the subgroup of generalized permutation matrices?,GL_n,"I'm trying to prove that de normalizer $N(T)$ of the subgroup $T\subset GL_n$ of diagonal matrices is the subgroup $P\in GL_n$ of generalized permutation matrices. I guess my biggest problem is that I don't really know how diagonal and permutation matrices (don't) commute. Because it is not true that $DM=MD$ when $D\in T$ and $M\in P$ since the permutation is either horizontal or vertical, but sometimes it seems like you can do something like it. So far, I have proved that $P\subset N(T)$, in the following way. Let $M_\sigma\in P$. Then $M_\sigma=VS_\sigma$, with $V\in T$ and $S_\sigma$ a permutation matrix. So $M_\sigma DM^{-1}=VS_\sigma D S_\sigma^T V^{-1}$. Thus if we prove that $S_\sigma D S_\sigma^T$ is diagonal we are done. This is true since $S_\sigma D S_\sigma^T=(x_1 e_{\sigma(1)} \dots x_n e_{\sigma(n)}) (e_{\sigma^{-1}(1)} \dots e_{\sigma^{-1}(n)})=(x_{\sigma^{-1}(1)}e_1 \dots x_{\sigma^{-1}(n)}e_n)$, where $e_i$ are the standard basis vectors. Even this is hopelessly written out. I'm trying to find a way to see what the product $S_\sigma D S_\sigma^T$ is without writing it in vectors. For the other way around I don't really know what to do. I'm having a hard time rewriting matrix products in a useful way. Perhaps there is a way of proving this using something completely different? Maybe you can prove it using $N(T)/T\simeq S_n$, but this actually what I want to use my question for. When I just write what I know about a matrix $M\in N(T)$ I just get a big system of equations that isn't really handy.","I'm trying to prove that de normalizer $N(T)$ of the subgroup $T\subset GL_n$ of diagonal matrices is the subgroup $P\in GL_n$ of generalized permutation matrices. I guess my biggest problem is that I don't really know how diagonal and permutation matrices (don't) commute. Because it is not true that $DM=MD$ when $D\in T$ and $M\in P$ since the permutation is either horizontal or vertical, but sometimes it seems like you can do something like it. So far, I have proved that $P\subset N(T)$, in the following way. Let $M_\sigma\in P$. Then $M_\sigma=VS_\sigma$, with $V\in T$ and $S_\sigma$ a permutation matrix. So $M_\sigma DM^{-1}=VS_\sigma D S_\sigma^T V^{-1}$. Thus if we prove that $S_\sigma D S_\sigma^T$ is diagonal we are done. This is true since $S_\sigma D S_\sigma^T=(x_1 e_{\sigma(1)} \dots x_n e_{\sigma(n)}) (e_{\sigma^{-1}(1)} \dots e_{\sigma^{-1}(n)})=(x_{\sigma^{-1}(1)}e_1 \dots x_{\sigma^{-1}(n)}e_n)$, where $e_i$ are the standard basis vectors. Even this is hopelessly written out. I'm trying to find a way to see what the product $S_\sigma D S_\sigma^T$ is without writing it in vectors. For the other way around I don't really know what to do. I'm having a hard time rewriting matrix products in a useful way. Perhaps there is a way of proving this using something completely different? Maybe you can prove it using $N(T)/T\simeq S_n$, but this actually what I want to use my question for. When I just write what I know about a matrix $M\in N(T)$ I just get a big system of equations that isn't really handy.",,"['group-theory', 'matrices']"
4,Maximize $x_1x_2+x_2x_3+\cdots+x_nx_1$,Maximize,x_1x_2+x_2x_3+\cdots+x_nx_1,"Let $x_1,x_2,\ldots,x_n$ be $n$ non-negative numbers ( $n>2$ ) with a fixed sum $S$ .  What is the maximum of $x_1 x_2 + x_2 x_3 + \dots + x_n x_1$ ?",Let be non-negative numbers ( ) with a fixed sum .  What is the maximum of ?,"x_1,x_2,\ldots,x_n n n>2 S x_1 x_2 + x_2 x_3 + \dots + x_n x_1","['matrices', 'optimization', 'problem-solving', 'quadratic-programming']"
5,Under what circumstance will a covariance matrix be positive semi-definite rather than positive definite?,Under what circumstance will a covariance matrix be positive semi-definite rather than positive definite?,,"I have a covariance matrix: $\operatorname{cov}(\mathbf{X}, \mathbf{X}) = \operatorname{E}[(\mathbf{X} - \operatorname{E}[\mathbf{X}])(\mathbf{X} - \operatorname{E}[\mathbf{X}])^T]$ According to Wikipedia , it should be a positive semi-definite matrix. Under what circumstances will it be positive semi-definite rather than positive definite? The reason I am asking is because I see that a common thing to do when implementing an Unscented Kalman Filter is to implement the square-root of the covariance matrix using the matlab command: sqrt_P = gamma * chol(P_a, 'lower') where gamma is a scaling factor and P_a is the state covariance matrix. I understand that for chol() to work, it needs to be positive definite: >> help chol  CHOL   Cholesky factorization.     CHOL(A) uses only the diagonal and upper triangle of A.     The lower triangle is assumed to be the (complex conjugate)     transpose of the upper triangle.  If A is positive definite, then     R = CHOL(A) produces an upper triangular R so that R'*R = A.     If A is not positive definite, an error message is printed. So, what are the dangers in assuming that it isn't positive semi-definite? Will it only be semi-definite when (for example) it is the zero matrix, or when there are fully correlated states? ADDENDUM: In the original post, there was a reference to ""if the states are fully correlated"".  This was rather fast and loose with the notation.  There is a discussion and an answer on it here .","I have a covariance matrix: $\operatorname{cov}(\mathbf{X}, \mathbf{X}) = \operatorname{E}[(\mathbf{X} - \operatorname{E}[\mathbf{X}])(\mathbf{X} - \operatorname{E}[\mathbf{X}])^T]$ According to Wikipedia , it should be a positive semi-definite matrix. Under what circumstances will it be positive semi-definite rather than positive definite? The reason I am asking is because I see that a common thing to do when implementing an Unscented Kalman Filter is to implement the square-root of the covariance matrix using the matlab command: sqrt_P = gamma * chol(P_a, 'lower') where gamma is a scaling factor and P_a is the state covariance matrix. I understand that for chol() to work, it needs to be positive definite: >> help chol  CHOL   Cholesky factorization.     CHOL(A) uses only the diagonal and upper triangle of A.     The lower triangle is assumed to be the (complex conjugate)     transpose of the upper triangle.  If A is positive definite, then     R = CHOL(A) produces an upper triangular R so that R'*R = A.     If A is not positive definite, an error message is printed. So, what are the dangers in assuming that it isn't positive semi-definite? Will it only be semi-definite when (for example) it is the zero matrix, or when there are fully correlated states? ADDENDUM: In the original post, there was a reference to ""if the states are fully correlated"".  This was rather fast and loose with the notation.  There is a discussion and an answer on it here .",,"['matrices', 'probability-theory', 'statistics', 'positive-semidefinite']"
6,When do two matrices have the same exponential?,When do two matrices have the same exponential?,,"Let $A$ and $B$ be $n\times n$ hermitean matrices. When do we have $e^{iA}=e^{iB}$ ? Can we somehow classify those pairs of matrices that have the same exponential? Here are some observations that I made: If $A$ and $B$ commute, then the condition is satisfied if and only if the spectrum of $A-B$ is contained in $2\pi\mathbb Z$ . (Both directions can fail if $A$ and $B$ don't commute, as the following two points show.) The condition $e^{iA}=e^{iB}$ can also be satisfied if $A$ and $B$ do not commute. Take, for example, $A=2\pi\begin{pmatrix}1&0\\0&-1\end{pmatrix}$ and $B=2\pi\begin{pmatrix}0&1\\1&0\end{pmatrix}$ . They do not commute but $e^{iA}=e^{iB}=I$ . The spectrum of their difference is not in $2\pi\mathbb Z$ . If $A=\sqrt2\pi\begin{pmatrix}1&0\\0&-1\end{pmatrix}$ and $B=\sqrt2\pi\begin{pmatrix}0&1\\1&0\end{pmatrix}$ , then the eigenvalues of $A-B$ are $\pm2\pi$ but $e^{iA}\neq e^{iB}$ . The exponential map is not a homomorphism so finding the kernel is not enough; cf. the Baker–Campbell–Hausdorff formula . Having the same exponential is an equivalence relation.","Let and be hermitean matrices. When do we have ? Can we somehow classify those pairs of matrices that have the same exponential? Here are some observations that I made: If and commute, then the condition is satisfied if and only if the spectrum of is contained in . (Both directions can fail if and don't commute, as the following two points show.) The condition can also be satisfied if and do not commute. Take, for example, and . They do not commute but . The spectrum of their difference is not in . If and , then the eigenvalues of are but . The exponential map is not a homomorphism so finding the kernel is not enough; cf. the Baker–Campbell–Hausdorff formula . Having the same exponential is an equivalence relation.",A B n\times n e^{iA}=e^{iB} A B A-B 2\pi\mathbb Z A B e^{iA}=e^{iB} A B A=2\pi\begin{pmatrix}1&0\\0&-1\end{pmatrix} B=2\pi\begin{pmatrix}0&1\\1&0\end{pmatrix} e^{iA}=e^{iB}=I 2\pi\mathbb Z A=\sqrt2\pi\begin{pmatrix}1&0\\0&-1\end{pmatrix} B=\sqrt2\pi\begin{pmatrix}0&1\\1&0\end{pmatrix} A-B \pm2\pi e^{iA}\neq e^{iB},"['matrices', 'lie-groups', 'lie-algebras']"
7,What is the minimal number of steps needed to create a chessboard?,What is the minimal number of steps needed to create a chessboard?,,"Imagine you have a $8 \times 8$ completely white field of squares on your screen. Now you can click on any square. When you do this, all the squares in that column and line (including the one you clicked on it) will change their color (if it's white it will be black and vice versa). How many steps at least would it take to create a standard chessboard? I tried it manually but it got too complicated and I lost track of the situation. But if I could tell my problem to Mathematica it could be very easy. Is there a way to write code to calculate this? Or even a formula to do this manually without any program?","Imagine you have a completely white field of squares on your screen. Now you can click on any square. When you do this, all the squares in that column and line (including the one you clicked on it) will change their color (if it's white it will be black and vice versa). How many steps at least would it take to create a standard chessboard? I tried it manually but it got too complicated and I lost track of the situation. But if I could tell my problem to Mathematica it could be very easy. Is there a way to write code to calculate this? Or even a formula to do this manually without any program?",8 \times 8,"['matrices', 'puzzle']"
8,Proof of Matrix Norm (Inverse Matrix),Proof of Matrix Norm (Inverse Matrix),,Show for any induced matrix norm and nonsingular matrix A that $$ \left\|A^{-1}\right\| ≥ (\left\|A\right\|)^{-1} $$ where $$ \left\|A^{-1}\right\| = \max_{\left\|x\right\|=1}\{\left\|A^{-1}x\right\|\}\\ \left\|A\right\| = \max_{\left\|x\right\|=1}\{\left\|Ax\right\|\}. $$ I am not sure how to show that: \begin{equation} \begin{split} \left\|A^{-1}\right\| ≥ (\left\|A\right\|)^{-1}\\ \text{or}\\ \max_{\left\|x\right\|=1}\{\left\|A^{-1}x\right\|\} ≥ (\max_{\left\|x\right\|=1}\{\left\|Ax\right\|\})^{-1} \end{split} \end{equation},Show for any induced matrix norm and nonsingular matrix A that $$ \left\|A^{-1}\right\| ≥ (\left\|A\right\|)^{-1} $$ where $$ \left\|A^{-1}\right\| = \max_{\left\|x\right\|=1}\{\left\|A^{-1}x\right\|\}\\ \left\|A\right\| = \max_{\left\|x\right\|=1}\{\left\|Ax\right\|\}. $$ I am not sure how to show that: \begin{equation} \begin{split} \left\|A^{-1}\right\| ≥ (\left\|A\right\|)^{-1}\\ \text{or}\\ \max_{\left\|x\right\|=1}\{\left\|A^{-1}x\right\|\} ≥ (\max_{\left\|x\right\|=1}\{\left\|Ax\right\|\})^{-1} \end{split} \end{equation},,"['matrices', 'normed-spaces', 'inverse']"
9,Is the rank of a matrix equal to the number of non-zero eigenvalues?,Is the rank of a matrix equal to the number of non-zero eigenvalues?,,I have studied before that the rank of a matrix = number of non zero Eigen values. But recently i came across a problem and i dont think it is valid there. I know i am going wrong somewhere. $$A= \begin{bmatrix}  0 & 4 & 0 \\ 0 & 0 & 4\\ 0 & 0 & 0 \\ \end{bmatrix} \quad $$ The Rank of this matrix is 2. So there should be 2 non zero eigen values. But I only get 0 as the eigen value(λ) using $$[A-λI]=0$$ Can anybody explain? Thanks,I have studied before that the rank of a matrix = number of non zero Eigen values. But recently i came across a problem and i dont think it is valid there. I know i am going wrong somewhere. The Rank of this matrix is 2. So there should be 2 non zero eigen values. But I only get 0 as the eigen value(λ) using Can anybody explain? Thanks,"A=
\begin{bmatrix} 
0 & 4 & 0 \\
0 & 0 & 4\\
0 & 0 & 0 \\
\end{bmatrix}
\quad
 [A-λI]=0","['matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
10,When will $AB=BA$?,When will ?,AB=BA,"Given two square matrices $A,B$ with same dimension, what conditions will lead to this result? Or what result will this condition lead to? I thought this is a quite simple question, but I can find little information about it. Thanks.","Given two square matrices $A,B$ with same dimension, what conditions will lead to this result? Or what result will this condition lead to? I thought this is a quite simple question, but I can find little information about it. Thanks.",,['matrices']
11,Decomposition of a positive semidefinite matrix,Decomposition of a positive semidefinite matrix,,"Let $Y \in \mathbb{R}^{n \times n}$ be a symmetric, positive semidefinite matrix such that $Y_{kk} = 1$ for all $k$. This matrix is supposed to be factorized as $Y = V^T V$, where $V \in \mathbb{R}^{n \times n}$. Does this factorization/decomposition have a name? How is it possible to compute $V$?","Let $Y \in \mathbb{R}^{n \times n}$ be a symmetric, positive semidefinite matrix such that $Y_{kk} = 1$ for all $k$. This matrix is supposed to be factorized as $Y = V^T V$, where $V \in \mathbb{R}^{n \times n}$. Does this factorization/decomposition have a name? How is it possible to compute $V$?",,"['matrices', 'matrix-decomposition', 'symmetric-matrices', 'positive-semidefinite']"
12,Notation for the set of symmetric matrices and symmetric positive definite matrices,Notation for the set of symmetric matrices and symmetric positive definite matrices,,"I would like to know if there exists a notation for the set of symmetric matrices and symmetric positive definite matrices. For instance, the set of $N \times N$ matrices with real entries is denoted as $\mathbb{R}^{N \times N}$.","I would like to know if there exists a notation for the set of symmetric matrices and symmetric positive definite matrices. For instance, the set of $N \times N$ matrices with real entries is denoted as $\mathbb{R}^{N \times N}$.",,"['matrices', 'notation', 'positive-definite', 'symmetric-matrices']"
13,Matrix $A^T A$ as sum of outer products,Matrix  as sum of outer products,A^T A,"I have recently read in a script about statistical methods in a chapter about linear regression that: Given an $n \times k$ -matrix $A$ , we have $$A^T A = \sum_{i=1}^{n} a_i^T a_i$$ where $a_i$ denotes the $i$ -th row of $A$ . Unfortunately, the author doesn't give a proof of that and I can't figure out one myself. Maybe someone can help me. source: script on page 10","I have recently read in a script about statistical methods in a chapter about linear regression that: Given an -matrix , we have where denotes the -th row of . Unfortunately, the author doesn't give a proof of that and I can't figure out one myself. Maybe someone can help me. source: script on page 10",n \times k A A^T A = \sum_{i=1}^{n} a_i^T a_i a_i i A,"['matrices', 'statistics', 'outer-product']"
14,Why do mathematicians use only symmetric matrices when they want positive semi-definite matrices?,Why do mathematicians use only symmetric matrices when they want positive semi-definite matrices?,,Why do mathematicians use only symmetric matrices when they want positive semi/definite matrices? I mean I haven't seen using non-symmetric positive semi/definite matrices. If non-symmetric positive semi/definite matrices exist can those be always written by a symmetric PSD matrix?,Why do mathematicians use only symmetric matrices when they want positive semi/definite matrices? I mean I haven't seen using non-symmetric positive semi/definite matrices. If non-symmetric positive semi/definite matrices exist can those be always written by a symmetric PSD matrix?,,"['matrices', 'soft-question', 'positive-semidefinite']"
15,Bound on the difference of two determinants,Bound on the difference of two determinants,,"Let $A$ and $B$ be two real, $n\times n$ matrices. Using Hadamard's inequality, it is not hard to show that $$ \left|\det A - \det B \right| \leq \|A-B\|_{2} \frac{\|A\|_{2}^n -\|B\|_{2}^n}{\|A\|_2 -\|B\|_2}. $$ Where $\|A\|_2=\sqrt{\sum_{i,j}a_{ij}^2}$.  From this, I can derive a sup bound, for example  $$ \left|\det A - \det B \right| \leq n^{n+1} \|A-B\|_{\infty} \max (\|A\|_{\infty}^{n-1},\|B\|_{\infty}^{n-1}). $$ Where $\|A\|_\infty=\sup_{i,j}|a_{ij}|$. The constant $n^{n+1}$ is not the best bound possible : any reference (or proof) for a better (or the best) one? I show below that one can obtain $n^2(n-1)^{n-1}$, but that isn't much better. I just tried  $10^5$ random matrices on Maple and obtained a maximal constant (much) smaller  than one : this is not a proof, but it looks like there is room for improvement nevertheless. Just for completeness (and in case someone sees a factor I missed), to get the first bound, writing $A=[A_1,\ldots,A_n]$ in terms of its column vectors, an expansion shows \begin{eqnarray*} \det A &=& \det (A_1 -B_1,A_2,\ldots,A_n) + \det (B_1,A_2,\ldots,A_n) \\   &=& \sum_{j=1}^n \det (B_1,\ldots, B_{j-1}, A_j -B_j,A_{j+1},\ldots,A_n) \\ && + \det B,  \end{eqnarray*} Thus by Hadamard's inequality , \begin{eqnarray*} \det A -\det B &\leq&  \sum_{j=1}^n \prod_{i=1}^{j-1} \|B_i\|_{2}\prod_{i=j+1}^{n} \|A_i\|_{2} \|A_j-B_j\|_{2} \\ &\leq&  \|A-B\|_{2} \sum_{j=1}^n \|B\|^{j-1}_{2}\|A\|^{n-j}_{2} \\  &=& \|A-B\|_{2} \frac{\|A\|_{2}^n -\|B\|_{2}^n}{\|A\|_2 -\|B\|_2}. \end{eqnarray*} The second bound is just that $x^n -y^n\leq n \max(|x|^{n-1},|y|^{n-1}) |x-y|$ and $\|A\|_2 \leq n\|A\|_\infty$. Another approach is calculus, namely, to write that $\det B - \det A = f(1)-f(0)$, with $f(t)=\det(A + t(B-A))$. By the mean value theorem $f(1)-f(0)\leq \max |f^\prime (t)|$. We can compute that $$f^\prime(t) =  {\rm trace}\left({\rm Cofm}(A +t(B-A))(B-A)\right)$$  (if I did not mess up, using the formula for the differential of a determinant , where Cofm means the matrix of Cofactors). Then, it should deliver something better, if there is a nice way to bound it. The simplest thing is to use Cauchy-Schwarz, namely $$ \left|{\rm trace}\left({\rm Cofm}(A +t(B-A))(B-A)\right)\right|\leq \|B-A\|_{2} \|{\rm Cofm}(A +t(B-A))\|_{2} $$ and then, by lack of a better idea,  $$ \|{\rm Cofm}(A +t(B-A))\|_{2}\leq n \max_{ij} |{\rm Cof}_{i,j}(A +t(B-A))|, $$ and brutally, $|{\rm Cof}_{i,j}(A +t(B-A))|\leq ((n-1) \max(\|A\|_\infty,\|B\|_\infty))^{n-1}$ gives a slightly better constant, namely $$ n^2(n-1)^{n-1} <n^{n+1} $$ but that still seems a very rough way to bound a determinant, as it is never sharp, since to attain this bound all coefficients should be equal, and therefore the cofactor would be zero. They are of the same order in $n$, and I suspect this order is wrong.","Let $A$ and $B$ be two real, $n\times n$ matrices. Using Hadamard's inequality, it is not hard to show that $$ \left|\det A - \det B \right| \leq \|A-B\|_{2} \frac{\|A\|_{2}^n -\|B\|_{2}^n}{\|A\|_2 -\|B\|_2}. $$ Where $\|A\|_2=\sqrt{\sum_{i,j}a_{ij}^2}$.  From this, I can derive a sup bound, for example  $$ \left|\det A - \det B \right| \leq n^{n+1} \|A-B\|_{\infty} \max (\|A\|_{\infty}^{n-1},\|B\|_{\infty}^{n-1}). $$ Where $\|A\|_\infty=\sup_{i,j}|a_{ij}|$. The constant $n^{n+1}$ is not the best bound possible : any reference (or proof) for a better (or the best) one? I show below that one can obtain $n^2(n-1)^{n-1}$, but that isn't much better. I just tried  $10^5$ random matrices on Maple and obtained a maximal constant (much) smaller  than one : this is not a proof, but it looks like there is room for improvement nevertheless. Just for completeness (and in case someone sees a factor I missed), to get the first bound, writing $A=[A_1,\ldots,A_n]$ in terms of its column vectors, an expansion shows \begin{eqnarray*} \det A &=& \det (A_1 -B_1,A_2,\ldots,A_n) + \det (B_1,A_2,\ldots,A_n) \\   &=& \sum_{j=1}^n \det (B_1,\ldots, B_{j-1}, A_j -B_j,A_{j+1},\ldots,A_n) \\ && + \det B,  \end{eqnarray*} Thus by Hadamard's inequality , \begin{eqnarray*} \det A -\det B &\leq&  \sum_{j=1}^n \prod_{i=1}^{j-1} \|B_i\|_{2}\prod_{i=j+1}^{n} \|A_i\|_{2} \|A_j-B_j\|_{2} \\ &\leq&  \|A-B\|_{2} \sum_{j=1}^n \|B\|^{j-1}_{2}\|A\|^{n-j}_{2} \\  &=& \|A-B\|_{2} \frac{\|A\|_{2}^n -\|B\|_{2}^n}{\|A\|_2 -\|B\|_2}. \end{eqnarray*} The second bound is just that $x^n -y^n\leq n \max(|x|^{n-1},|y|^{n-1}) |x-y|$ and $\|A\|_2 \leq n\|A\|_\infty$. Another approach is calculus, namely, to write that $\det B - \det A = f(1)-f(0)$, with $f(t)=\det(A + t(B-A))$. By the mean value theorem $f(1)-f(0)\leq \max |f^\prime (t)|$. We can compute that $$f^\prime(t) =  {\rm trace}\left({\rm Cofm}(A +t(B-A))(B-A)\right)$$  (if I did not mess up, using the formula for the differential of a determinant , where Cofm means the matrix of Cofactors). Then, it should deliver something better, if there is a nice way to bound it. The simplest thing is to use Cauchy-Schwarz, namely $$ \left|{\rm trace}\left({\rm Cofm}(A +t(B-A))(B-A)\right)\right|\leq \|B-A\|_{2} \|{\rm Cofm}(A +t(B-A))\|_{2} $$ and then, by lack of a better idea,  $$ \|{\rm Cofm}(A +t(B-A))\|_{2}\leq n \max_{ij} |{\rm Cof}_{i,j}(A +t(B-A))|, $$ and brutally, $|{\rm Cof}_{i,j}(A +t(B-A))|\leq ((n-1) \max(\|A\|_\infty,\|B\|_\infty))^{n-1}$ gives a slightly better constant, namely $$ n^2(n-1)^{n-1} <n^{n+1} $$ but that still seems a very rough way to bound a determinant, as it is never sharp, since to attain this bound all coefficients should be equal, and therefore the cofactor would be zero. They are of the same order in $n$, and I suspect this order is wrong.",,['matrices']
16,Cholesky of matrix plus identity,Cholesky of matrix plus identity,,I have a positive definite matrix $A$ ( $n \times n$ dimension) for which I have the Cholesky decomposition $A=LL^{'}$ . I want to use this to compute a) The Cholesky decomposition of $A+c^2\times I $ where $c$ is a constant and $I$ is the identity matrix b) The Cholesky decomposition of $A+BB^{'}$ where $B$ is a $n \times n$ sparse matrix with each row having at most $k$ elements for some fixed $k << n$ . Is there any analytical/ computational method/ R-package that can use the already available Cholesky decomposition of $A$ and perform (a) and (b) in a computationally scalable way i.e. ( $O(n)$ complexity). Note that (a) is a special case of (b) with $B=cI$ . Any references will be appreciated. Thanks,I have a positive definite matrix ( dimension) for which I have the Cholesky decomposition . I want to use this to compute a) The Cholesky decomposition of where is a constant and is the identity matrix b) The Cholesky decomposition of where is a sparse matrix with each row having at most elements for some fixed . Is there any analytical/ computational method/ R-package that can use the already available Cholesky decomposition of and perform (a) and (b) in a computationally scalable way i.e. ( complexity). Note that (a) is a special case of (b) with . Any references will be appreciated. Thanks,A n \times n A=LL^{'} A+c^2\times I  c I A+BB^{'} B n \times n k k << n A O(n) B=cI,"['matrices', 'computational-complexity', 'matrix-decomposition', 'cholesky-decomposition']"
17,"Fundamental domain of $\operatorname{GL}(n,\mathbb R)$ acted on by $\operatorname{GL}(n, \mathbb Z)$",Fundamental domain of  acted on by,"\operatorname{GL}(n,\mathbb R) \operatorname{GL}(n, \mathbb Z)","What is a simple description of a fundamental domain of $\operatorname{GL}(n,\mathbb R)$ acted on by $\operatorname{GL}(n,\mathbb Z)$? $\operatorname{GL}(n,\mathbb R)$ is the group of all real invertible matrices with matrix multiplication, $\operatorname{GL}(n,\mathbb Z)$ the group of all matrices with integer entries, whose inverses also have integer entries, with matrix multiplication. $h \in \operatorname{GL}(n, \mathbb Z) \subset \operatorname{GL}(n,\mathbb R)$ acts on $g \in \operatorname{GL}(n,\mathbb R)$ by letting $h\cdot g := hg$ Remarks: A fundamental domain $F$ is a subset of $\operatorname{GL}(n,\mathbb R)$ such that for any $x$ in $\operatorname{GL}(n,\mathbb R)$ there is exactly one $h$ in $\operatorname{GL}(n, \mathbb Z)$ such that $hx \in F$. I'm looking for an as clean as possible description of some $F$ in terms of the matrix entries. Clearly $\operatorname{GL}(n,\mathbb R)$ can be replaced with any set of (possibly not invertible) matrices with $n$ rows (possibly with few or more than n columns). The case with one column and $n=2$ is not too different from finding a fundamental domain of the upper half plane with respect to Möbius transformations. Also, $\operatorname{GL}$ could have been replaced with $\operatorname{SL}$. This appears as a very basic question to me, and if it turns out I'm ignorant of some useful tools or theorems I will accept pointers to such. In fact this would be even better than a direct answer to the specific question (since I have many related seemingly basic questions), as long as it helps significantly in answering the specific question.","What is a simple description of a fundamental domain of $\operatorname{GL}(n,\mathbb R)$ acted on by $\operatorname{GL}(n,\mathbb Z)$? $\operatorname{GL}(n,\mathbb R)$ is the group of all real invertible matrices with matrix multiplication, $\operatorname{GL}(n,\mathbb Z)$ the group of all matrices with integer entries, whose inverses also have integer entries, with matrix multiplication. $h \in \operatorname{GL}(n, \mathbb Z) \subset \operatorname{GL}(n,\mathbb R)$ acts on $g \in \operatorname{GL}(n,\mathbb R)$ by letting $h\cdot g := hg$ Remarks: A fundamental domain $F$ is a subset of $\operatorname{GL}(n,\mathbb R)$ such that for any $x$ in $\operatorname{GL}(n,\mathbb R)$ there is exactly one $h$ in $\operatorname{GL}(n, \mathbb Z)$ such that $hx \in F$. I'm looking for an as clean as possible description of some $F$ in terms of the matrix entries. Clearly $\operatorname{GL}(n,\mathbb R)$ can be replaced with any set of (possibly not invertible) matrices with $n$ rows (possibly with few or more than n columns). The case with one column and $n=2$ is not too different from finding a fundamental domain of the upper half plane with respect to Möbius transformations. Also, $\operatorname{GL}$ could have been replaced with $\operatorname{SL}$. This appears as a very basic question to me, and if it turns out I'm ignorant of some useful tools or theorems I will accept pointers to such. In fact this would be even better than a direct answer to the specific question (since I have many related seemingly basic questions), as long as it helps significantly in answering the specific question.",,"['matrices', 'reference-request', 'lie-groups', 'lattices-in-lie-groups']"
18,Determinant of an $n\times n$ complex matrix as an $2n\times 2n$ real determinant,Determinant of an  complex matrix as an  real determinant,n\times n 2n\times 2n,"If $A$ is an $n\times n$ complex matrix. Is it possible to write $\vert \det A\vert^2$ as a $2n\times 2n$ matrix with blocks containing the real and imaginary parts of $A$? I remember seeing such a formula, but can not remember where. Any details, (and possibly references) for such a result would be greatly appreciated.","If $A$ is an $n\times n$ complex matrix. Is it possible to write $\vert \det A\vert^2$ as a $2n\times 2n$ matrix with blocks containing the real and imaginary parts of $A$? I remember seeing such a formula, but can not remember where. Any details, (and possibly references) for such a result would be greatly appreciated.",,"['matrices', 'complex-numbers', 'determinant']"
19,$\det(I+A) = 1 + tr(A) + \det(A)$ for $n=2$ and for $n>2$?,for  and for ?,\det(I+A) = 1 + tr(A) + \det(A) n=2 n>2,Let $I$ the identity matrix and $A$ another general square matrix. In the case $n=2$ one can easily  verifies that \begin{equation} \det(I+A) = 1 + tr(A) + \det(A) \end{equation} or \begin{equation} \det(I+tA) = 1 +  t\  tr(A) + t^2\det(A) \end{equation} for some scalar $t \in \mathbb{R}$. I have tried to see if there exists a similar formula for $n>3$. This is a natural question. But the calculations are very big and difficulty to see. Then I do the answer. Is there a similar formula for $n>2$?,Let $I$ the identity matrix and $A$ another general square matrix. In the case $n=2$ one can easily  verifies that \begin{equation} \det(I+A) = 1 + tr(A) + \det(A) \end{equation} or \begin{equation} \det(I+tA) = 1 +  t\  tr(A) + t^2\det(A) \end{equation} for some scalar $t \in \mathbb{R}$. I have tried to see if there exists a similar formula for $n>3$. This is a natural question. But the calculations are very big and difficulty to see. Then I do the answer. Is there a similar formula for $n>2$?,,"['matrices', 'determinant']"
20,Is the trace of inverse matrix convex?,Is the trace of inverse matrix convex?,,"Hi I would like to know whether the trace of the inverse of a symmetric positive definite matrix $\mathrm{trace}(S^{-1})$ is convex. Actually I know that the trace of a symmetric positive definite matrix $S\in M_{m,m}$ is convex since we can find $B\in M_{n,m}$ such that $S=B^T\times B$ then we can write the trace as the sum of scalar quadratic forms, i.e. $\mathrm{trace}(S)=\mathrm{trace}(B^T\times B)=\sum_{j=1}^mb_j^T\times b_j$ where $b_j$ is the $j^{th}$ column of $B$. for instance if we have  $trace([\begin{array}{cc} 1 & 2  \\ 3 & 4  \\ \end{array}] \times [\begin{array}{cc} 1 & 3  \\ 2 & 4  \\ \end{array}])= [\begin{array}{cc} 1 & 2  \\ \end{array}]\times [\begin{array}{c} 1   \\ 2   \\ \end{array}]+ [\begin{array}{cc} 3 & 4  \\ \end{array}]\times [\begin{array}{c} 3   \\ 4   \\ \end{array}]=30$ And so I wonder if $\mathrm{trace}(S^{-1})$ is convex too..","Hi I would like to know whether the trace of the inverse of a symmetric positive definite matrix $\mathrm{trace}(S^{-1})$ is convex. Actually I know that the trace of a symmetric positive definite matrix $S\in M_{m,m}$ is convex since we can find $B\in M_{n,m}$ such that $S=B^T\times B$ then we can write the trace as the sum of scalar quadratic forms, i.e. $\mathrm{trace}(S)=\mathrm{trace}(B^T\times B)=\sum_{j=1}^mb_j^T\times b_j$ where $b_j$ is the $j^{th}$ column of $B$. for instance if we have  $trace([\begin{array}{cc} 1 & 2  \\ 3 & 4  \\ \end{array}] \times [\begin{array}{cc} 1 & 3  \\ 2 & 4  \\ \end{array}])= [\begin{array}{cc} 1 & 2  \\ \end{array}]\times [\begin{array}{c} 1   \\ 2   \\ \end{array}]+ [\begin{array}{cc} 3 & 4  \\ \end{array}]\times [\begin{array}{c} 3   \\ 4   \\ \end{array}]=30$ And so I wonder if $\mathrm{trace}(S^{-1})$ is convex too..",,"['matrices', 'convex-analysis', 'inverse', 'trace']"
21,Is $AA^T$ positive semidefinite?,Is  positive semidefinite?,AA^T,"I have a very short question. Is $AA^T$ positive semidefinite, i.e. $x^T AA^Tx\geqslant 0$ for suitable $x$?","I have a very short question. Is $AA^T$ positive semidefinite, i.e. $x^T AA^Tx\geqslant 0$ for suitable $x$?",,['matrices']
22,Matrix rows notation,Matrix rows notation,,"I'm working with a set of $M$ vectors $ \{\mathbf{w}_i \in \mathbb{R}^N, \, i = 1, \ldots, M \}$. Since single vectors are usually considered as column vectors, I'm defining a matrix $$ \mathbf{W} = [\mathbf{w}_1, \ldots, \mathbf{w}_M] \in \mathbb{R}^{N \times M} $$ by placing the vectors as matrix columns. However, for some descriptions, I need to refer to the matrix rows . Is there an elegant notation to refer to this matrix rows (preferably with less notation overhead)?","I'm working with a set of $M$ vectors $ \{\mathbf{w}_i \in \mathbb{R}^N, \, i = 1, \ldots, M \}$. Since single vectors are usually considered as column vectors, I'm defining a matrix $$ \mathbf{W} = [\mathbf{w}_1, \ldots, \mathbf{w}_M] \in \mathbb{R}^{N \times M} $$ by placing the vectors as matrix columns. However, for some descriptions, I need to refer to the matrix rows . Is there an elegant notation to refer to this matrix rows (preferably with less notation overhead)?",,"['matrices', 'notation']"
23,Matrices - Conditions for $AB+BA=0$,Matrices - Conditions for,AB+BA=0,"The Problem Let $A$ be the matrix $\bigl(\begin{smallmatrix}a&b\\c&d\end{smallmatrix} \bigr)$, where no one of $a,b,c,d$ is $0$. Let $B$ be a $2\times 2$ matrix such that $AB+BA=\bigl(\begin{smallmatrix} 0&0\\ 0&0 \end{smallmatrix} \bigr)$. Show that either $a+d=0$, in which case the general solution for $B$ depends on 2 parameters, or $ad-bc=0$, in which case the general solution for $B$ depends on one parameter. (this is question 22 of the last matrix exercise of Further Pure Mathematics by Bostock et al.) Comments Writing $B=\bigl(\begin{smallmatrix} e&f\\ g&h \end{smallmatrix} \bigr)$ and multiplying out I get that $(a+d)(f+g)+(b+c)(e+h)=0$ $ae+bg+cf+dh=0$ but I am unable to get the required restrictions on $a,b,c,d$. Is there a quick way of doing the problem that doesn't require manual computation? I thought of considering invertible and non-invertible cases but couldn't get anywhere. Help would be much appreciated.","The Problem Let $A$ be the matrix $\bigl(\begin{smallmatrix}a&b\\c&d\end{smallmatrix} \bigr)$, where no one of $a,b,c,d$ is $0$. Let $B$ be a $2\times 2$ matrix such that $AB+BA=\bigl(\begin{smallmatrix} 0&0\\ 0&0 \end{smallmatrix} \bigr)$. Show that either $a+d=0$, in which case the general solution for $B$ depends on 2 parameters, or $ad-bc=0$, in which case the general solution for $B$ depends on one parameter. (this is question 22 of the last matrix exercise of Further Pure Mathematics by Bostock et al.) Comments Writing $B=\bigl(\begin{smallmatrix} e&f\\ g&h \end{smallmatrix} \bigr)$ and multiplying out I get that $(a+d)(f+g)+(b+c)(e+h)=0$ $ae+bg+cf+dh=0$ but I am unable to get the required restrictions on $a,b,c,d$. Is there a quick way of doing the problem that doesn't require manual computation? I thought of considering invertible and non-invertible cases but couldn't get anywhere. Help would be much appreciated.",,['matrices']
24,"A weak converse of $AB=BA\implies e^Ae^B=e^Be^A$ from ""Topics in Matrix Analysis"" for matrices of algebraic numbers.","A weak converse of  from ""Topics in Matrix Analysis"" for matrices of algebraic numbers.",AB=BA\implies e^Ae^B=e^Be^A,"It is a well known fact that if $A,B\in M_{n\times n}(\mathbb C)$ and $AB=BA$, then $e^Ae^B=e^Be^A.$ The converse does not hold. Horn and Johnson give the following example in their Topics in Matrix Analysis (page 435). Let $$A=\begin{pmatrix}0&0\\0&2\pi i\end{pmatrix},\qquad B=\begin{pmatrix}0&1\\0&2\pi i\end{pmatrix}.$$ Then $$AB=\begin{pmatrix}0&0\\0&-4\pi^2\end{pmatrix}\neq\begin{pmatrix}0&2\pi i\\0&-4\pi^2\end{pmatrix}=BA.$$ We have $$e^A=\sum_{k=0}^{\infty}\frac 1{k!}\begin{pmatrix}0&0\\0&2\pi i\end{pmatrix}^k=\sum_{k=0}^{\infty}\frac 1{k!}\begin{pmatrix}0^k&0\\0&(2\pi i)^k\end{pmatrix}=\begin{pmatrix}e^0&0\\0&e^{2\pi i}\end{pmatrix}=\begin{pmatrix}1&0\\0&1\end{pmatrix}.$$ For $S=\begin{pmatrix}1&-\frac i{2\pi}\\0&1 \end{pmatrix},$ we have $$e^B=e^{SAS^{-1}}=Se^AS^{-1}=S\begin{pmatrix}1&0\\0&1\end{pmatrix}S^{-1}=\begin{pmatrix}1&0\\0&1\end{pmatrix}.$$ Therefore, $A,B$ are such non-commuting matrices that $e^Ae^B=\begin{pmatrix}1&0\\0&1\end{pmatrix}=e^Be^A.$ It is clear that $\pi$ is important in this particular example. In fact, the authors say what follows. It is known that if all entries of $A,B\in M_n$ are algebraic numbers and $n\geq 2,$ then $e^A\cdot e^B=e^B\cdot e^A$ if and only if $AB=BA.$ No proof is given. How does one go about proving that?","It is a well known fact that if $A,B\in M_{n\times n}(\mathbb C)$ and $AB=BA$, then $e^Ae^B=e^Be^A.$ The converse does not hold. Horn and Johnson give the following example in their Topics in Matrix Analysis (page 435). Let $$A=\begin{pmatrix}0&0\\0&2\pi i\end{pmatrix},\qquad B=\begin{pmatrix}0&1\\0&2\pi i\end{pmatrix}.$$ Then $$AB=\begin{pmatrix}0&0\\0&-4\pi^2\end{pmatrix}\neq\begin{pmatrix}0&2\pi i\\0&-4\pi^2\end{pmatrix}=BA.$$ We have $$e^A=\sum_{k=0}^{\infty}\frac 1{k!}\begin{pmatrix}0&0\\0&2\pi i\end{pmatrix}^k=\sum_{k=0}^{\infty}\frac 1{k!}\begin{pmatrix}0^k&0\\0&(2\pi i)^k\end{pmatrix}=\begin{pmatrix}e^0&0\\0&e^{2\pi i}\end{pmatrix}=\begin{pmatrix}1&0\\0&1\end{pmatrix}.$$ For $S=\begin{pmatrix}1&-\frac i{2\pi}\\0&1 \end{pmatrix},$ we have $$e^B=e^{SAS^{-1}}=Se^AS^{-1}=S\begin{pmatrix}1&0\\0&1\end{pmatrix}S^{-1}=\begin{pmatrix}1&0\\0&1\end{pmatrix}.$$ Therefore, $A,B$ are such non-commuting matrices that $e^Ae^B=\begin{pmatrix}1&0\\0&1\end{pmatrix}=e^Be^A.$ It is clear that $\pi$ is important in this particular example. In fact, the authors say what follows. It is known that if all entries of $A,B\in M_n$ are algebraic numbers and $n\geq 2,$ then $e^A\cdot e^B=e^B\cdot e^A$ if and only if $AB=BA.$ No proof is given. How does one go about proving that?",,"['analysis', 'matrices']"
25,How to calculate the matrix exponential explicitly for a matrix which isn't diagonalizable?,How to calculate the matrix exponential explicitly for a matrix which isn't diagonalizable?,,"How can I compute an expression for $(\exp(Qt))_{i,j}$ for some fixed $i, j$ and matrix $Q$? When $Q$ is diagonalizable, we can diagonalize, but what can be done otherwise? Thanks.","How can I compute an expression for $(\exp(Qt))_{i,j}$ for some fixed $i, j$ and matrix $Q$? When $Q$ is diagonalizable, we can diagonalize, but what can be done otherwise? Thanks.",,"['matrices', 'jordan-normal-form', 'matrix-exponential']"
26,Why is the multiset of eigenvalues called spectrum?,Why is the multiset of eigenvalues called spectrum?,,Somebody came to me and asked the following: Why is the multiset of eigenvalues called spectrum ? I cannot find the reason anywhere.,Somebody came to me and asked the following: Why is the multiset of eigenvalues called spectrum ? I cannot find the reason anywhere.,,"['matrices', 'eigenvalues-eigenvectors', 'terminology', 'math-history']"
27,Why is only a square matrix invertible?,Why is only a square matrix invertible?,,Can anyone give a very simple proof (or explanation) as to why only square matrix can possibly be invertible?,Can anyone give a very simple proof (or explanation) as to why only square matrix can possibly be invertible?,,['matrices']
28,Roots of minimal and characteristic polynomials,Roots of minimal and characteristic polynomials,,"Why is it that for matrix $A \in M_n(\mathbb{C})$ the characteristic polynomial $\chi_A(t)$ and the minimal polynomial $\mu_A(t)$ have the same roots? Since $\chi_A(t) = \mu_A(t) \cdot p(t)$ it should be easy to follow, that $\chi_A(t)$ has roots where $\mu_A(t)$ has roots. But why can't $\chi_A(t)$ have roots where $\mu_A(t)$ hasn't?","Why is it that for matrix the characteristic polynomial and the minimal polynomial have the same roots? Since it should be easy to follow, that has roots where has roots. But why can't have roots where hasn't?",A \in M_n(\mathbb{C}) \chi_A(t) \mu_A(t) \chi_A(t) = \mu_A(t) \cdot p(t) \chi_A(t) \mu_A(t) \chi_A(t) \mu_A(t),"['matrices', 'polynomials', 'roots', 'minimal-polynomials', 'characteristic-polynomial']"
29,"How is this matrix called, and does it have a purpose?","How is this matrix called, and does it have a purpose?",,"I stumbled upon the 2d rotation matrix $$R(\theta)=\begin{pmatrix} \cos(\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta) \end{pmatrix}$$ which has determinant 1 because $$ \cos^2(\theta) + \sin^2(\theta) =1$$ So I thought what would happen if I replace the trig functions with hyperbolic ones, and when you do that you end up with determinant $$ \cosh^2(t) + \sinh^2(t) $$ but that tends to infinity so instead of having $$ -\sinh(t)$$ in the top right corner I replaced it with the positive version which gives us for determinant $$\cosh^2(t) - \sinh^2(t)$$ which is nicely equal to 1, but what's the name and purpose of this matrix?","I stumbled upon the 2d rotation matrix which has determinant 1 because So I thought what would happen if I replace the trig functions with hyperbolic ones, and when you do that you end up with determinant but that tends to infinity so instead of having in the top right corner I replaced it with the positive version which gives us for determinant which is nicely equal to 1, but what's the name and purpose of this matrix?",R(\theta)=\begin{pmatrix} \cos(\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta) \end{pmatrix}  \cos^2(\theta) + \sin^2(\theta) =1  \cosh^2(t) + \sinh^2(t)   -\sinh(t) \cosh^2(t) - \sinh^2(t),['matrices']
30,Diagonalizable vs full rank vs nonsingular (square matrix),Diagonalizable vs full rank vs nonsingular (square matrix),,"There are many discussions of such type problems (comparison), for example: Diagonalizable vs Normal Today, I want to clearly understand the topic. Suppose the matrix $A\in \mathbb{R}^{n\times n}$ . Since the multiplication of all eigenvalues is equal to the determinant of the matrix, $A$ full rank is equivalent to $A$ nonsingular. The above also implies $A$ has linearly independent rows and columns.  So $A$ is invertible . $A$ is diagonalizable iff $A$ has $n$ linearly independent eigenvectors. ( $A$ is non-defective). Note: $A$ is defective if geo. multiplicity $<$ alge. multiplicity. A diagonalizable matrix does not imply full rank (or nonsingular). My problem is Does full rank matrix (nonsingular) imply it is diagonalizable? Equivalently: Does a matrix with all its columns or rows linear independent imply all its eigenvectors are linear independently?","There are many discussions of such type problems (comparison), for example: Diagonalizable vs Normal Today, I want to clearly understand the topic. Suppose the matrix . Since the multiplication of all eigenvalues is equal to the determinant of the matrix, full rank is equivalent to nonsingular. The above also implies has linearly independent rows and columns.  So is invertible . is diagonalizable iff has linearly independent eigenvectors. ( is non-defective). Note: is defective if geo. multiplicity alge. multiplicity. A diagonalizable matrix does not imply full rank (or nonsingular). My problem is Does full rank matrix (nonsingular) imply it is diagonalizable? Equivalently: Does a matrix with all its columns or rows linear independent imply all its eigenvectors are linear independently?",A\in \mathbb{R}^{n\times n} A A A A A A n A A <,"['matrices', 'matrix-rank', 'diagonalization']"
31,Interpretation of Symmetric Normalised Graph Adjacency Matrix?,Interpretation of Symmetric Normalised Graph Adjacency Matrix?,,"I'm trying to follow a blog post about Graph Convolutional Neural Networks. To set up some notation, the above blog post denotes a graph $\mathcal{G}$ , it's adjacency matrix $A$ , and the degree matrix $D$ . A section of that blog post then says: I understand how an adjacency matrix can be row-normalised with $A_{row} = D^{-1}A$ , or column normalised with $A_{col} = AD^{-1}$ . My question: is there some intuitive interpretation of a symmetrically normalized adjacency matrix $A_{sym} = D^{-1/2}AD^{-1/2}$ ?","I'm trying to follow a blog post about Graph Convolutional Neural Networks. To set up some notation, the above blog post denotes a graph , it's adjacency matrix , and the degree matrix . A section of that blog post then says: I understand how an adjacency matrix can be row-normalised with , or column normalised with . My question: is there some intuitive interpretation of a symmetrically normalized adjacency matrix ?",\mathcal{G} A D A_{row} = D^{-1}A A_{col} = AD^{-1} A_{sym} = D^{-1/2}AD^{-1/2},"['matrices', 'graph-theory', 'symmetric-matrices', 'algebraic-graph-theory', 'adjacency-matrix']"
32,The inverse of a block-upper triangular matrix,The inverse of a block-upper triangular matrix,,"Is it true that $$\begin{pmatrix} A & * \\  0 &  B  \\  \end{pmatrix}^{-1} = \begin{pmatrix}  A^{-1}  & * \\  0 &  B^{-1} \\  \end{pmatrix}$$ where $A$ and $B$ are $m \times m$ and $n \times n$ invertible, and * is for  unspecified blocks ?","Is it true that $$\begin{pmatrix} A & * \\  0 &  B  \\  \end{pmatrix}^{-1} = \begin{pmatrix}  A^{-1}  & * \\  0 &  B^{-1} \\  \end{pmatrix}$$ where $A$ and $B$ are $m \times m$ and $n \times n$ invertible, and * is for  unspecified blocks ?",,"['matrices', 'inverse', 'block-matrices']"
33,What is the equivalent of a diagonal in a non-square matrix or array?,What is the equivalent of a diagonal in a non-square matrix or array?,,"I have a non-square matrix $M$, that looks something like this: $M=\left[ \begin{array} & a & b & c \\ d & e & f \\ g & h & i \\ j & k & l \\ \end{array}\right]$ I would like to refer the cells $M_{ij}$ where $i=j$, like $\{a,e,i\}$ in this case. If this was a square matrix, that would be the diagonal. Is that still the right word, or is there something more fitting that avoids confusion?","I have a non-square matrix $M$, that looks something like this: $M=\left[ \begin{array} & a & b & c \\ d & e & f \\ g & h & i \\ j & k & l \\ \end{array}\right]$ I would like to refer the cells $M_{ij}$ where $i=j$, like $\{a,e,i\}$ in this case. If this was a square matrix, that would be the diagonal. Is that still the right word, or is there something more fitting that avoids confusion?",,"['matrices', 'notation', 'terminology', 'definition']"
34,How does multiplying by trigonometric functions in a matrix transform the matrix?,How does multiplying by trigonometric functions in a matrix transform the matrix?,,I found this comic: But I can't understand the humor because I can't understand how trig functions affect matrix multiplication. Can someone please explain?,I found this comic: But I can't understand the humor because I can't understand how trig functions affect matrix multiplication. Can someone please explain?,,"['matrices', 'trigonometry', 'transformation']"
35,Is it faster to multiply a matrix by its transpose than ordinary matrix multiplication?,Is it faster to multiply a matrix by its transpose than ordinary matrix multiplication?,,"I'm writing a program that multiples a matrix by its transpose, and was trying to find efficiency hacks I could exploit considering that the two matrices being multiplied are related. Any ideas?","I'm writing a program that multiples a matrix by its transpose, and was trying to find efficiency hacks I could exploit considering that the two matrices being multiplied are related. Any ideas?",,"['matrices', 'algorithms']"
36,Express Hadamard product as a normal matrix product,Express Hadamard product as a normal matrix product,,I have $N^2$ equations which I can write as the following Hadamard product. Is there a way I can get rid of the Hadamard product and express this using usual matrix operations? $\left[  \begin{matrix} 0 & a_{21} & \cdots & a_{n1} \\ a_{12} & 0  & \cdots & a_{n2} \\  \vdots & \vdots & \ddots & \vdots\\ a_{1n} & a_{2n} &\cdots & 0\\ \end{matrix}  \right]\bigcirc \left[\begin{matrix} b_1 & b_2 & \cdots & b_n \\ b_1 & b_2 & \cdots & b_n \\  \vdots & \vdots & \ddots & \vdots\\ b_1 & b_2 & \cdots & b_n\\ \end{matrix}  \right]=\left[  \begin{matrix} c_{11} & c_{21} & \cdots & c_{n1} \\ c_{12} & c_{22}  & \cdots & a_{n2} \\  \vdots & \vdots & \ddots & \vdots\\ c_{1n} & c_{2n} &\cdots & c_{nn}\\ \end{matrix}  \right]$,I have $N^2$ equations which I can write as the following Hadamard product. Is there a way I can get rid of the Hadamard product and express this using usual matrix operations? $\left[  \begin{matrix} 0 & a_{21} & \cdots & a_{n1} \\ a_{12} & 0  & \cdots & a_{n2} \\  \vdots & \vdots & \ddots & \vdots\\ a_{1n} & a_{2n} &\cdots & 0\\ \end{matrix}  \right]\bigcirc \left[\begin{matrix} b_1 & b_2 & \cdots & b_n \\ b_1 & b_2 & \cdots & b_n \\  \vdots & \vdots & \ddots & \vdots\\ b_1 & b_2 & \cdots & b_n\\ \end{matrix}  \right]=\left[  \begin{matrix} c_{11} & c_{21} & \cdots & c_{n1} \\ c_{12} & c_{22}  & \cdots & a_{n2} \\  \vdots & \vdots & \ddots & \vdots\\ c_{1n} & c_{2n} &\cdots & c_{nn}\\ \end{matrix}  \right]$,,"['matrices', 'hadamard-product']"
37,Matrix representation of the Quaternions?,Matrix representation of the Quaternions?,,Can anyone explain how why the matrix representation of the quaternions using real matrices is constructed as such?,Can anyone explain how why the matrix representation of the quaternions using real matrices is constructed as such?,,"['matrices', 'quaternions']"
38,Finding a Rotation Transformation from two Coordinate Frames in 3-Space,Finding a Rotation Transformation from two Coordinate Frames in 3-Space,,"The question I'm trying to figure out states that I have 3 points P1, P2 and P3 in space.  In one frame (Frame A I called it) those points are: Pa1, Pa2 and Pa3 , same story for Frame B (namely: Pb1, Pb2 and Pb3 ). Whats the rotation matrix from one to the other?  That's literally all the information I have. What was suggested was make an intermediate coordinate frame and align one of it's axis' with a point.   However I don't see how this will help with the problem. Can anyone offer some advice how to tackle this problem? I'm stumped. Thanks, Edit: Both frames have the same origin, so there is no translation component.","The question I'm trying to figure out states that I have 3 points P1, P2 and P3 in space.  In one frame (Frame A I called it) those points are: Pa1, Pa2 and Pa3 , same story for Frame B (namely: Pb1, Pb2 and Pb3 ). Whats the rotation matrix from one to the other?  That's literally all the information I have. What was suggested was make an intermediate coordinate frame and align one of it's axis' with a point.   However I don't see how this will help with the problem. Can anyone offer some advice how to tackle this problem? I'm stumped. Thanks, Edit: Both frames have the same origin, so there is no translation component.",,"['geometry', 'matrices', 'transformation', 'rigid-transformation']"
39,Why is determinant called determinant?,Why is determinant called determinant?,,"Can someone explain to me why do we call the determinant of a matrix ""determinant""? Does it have any meaning? Like it determines something for example!","Can someone explain to me why do we call the determinant of a matrix ""determinant""? Does it have any meaning? Like it determines something for example!",,"['matrices', 'terminology', 'determinant']"
40,What is the derivative of $\log \det X$ when $X$ is symmetric?,What is the derivative of  when  is symmetric?,\log \det X X,"According to Appendix A.4.1 of Boyd & Vandenberghe's Convex Optimization , the gradient of $f(X):=\log \det X$ is $$\nabla f(X) = X^{-1}$$ The domain of the $f$ here is the set of symmetric matrices $\mathbf S^n$ . However, according to the book ""Matrix Algebra from a Statistician's Perspective"" by D. Harville, $\log \det X$ for a symmetric $X$ must be (see eq. 8.12 of book) $$\log \det X = 2 X^{-1} - \text{diag} (y_{11}, y_{22}, \dots, y_{nn})$$ where $y_{ii}$ represents the $i$ th element on the diagonal of $X^{-1}$ . Now I'm not a mathematician but to me the formula of Harville seems correct, because he makes use of the fact that the entries of $X$ are not ""independent"". Indeed, in the case where the entries are ''independent'', Harville provides another formula (eq. 8.8 of his book), which matches that of Boyd & Vandenberghe. Is this an error on the book of Boyd & Vandenberghe, or am I missing something here? To me it does seem like an error, but at the same time I find this extremely unlikely as the book is very popular and if it were an error it would already be on Errata; it's much more likely that I'm misunderstanding something. This formula has already been mentioned in many questions in this website, but no question or answer that I saw mentions (the possibility of) $\log \det X$ in Boyd & Vandenberghe being wrong. Edit based on response of Profs. Boyd & Vandenberghe Prof. Boyd kindly responded to my email about this issue, provided an explanation that he and Lieven Vandenberghe think can can explain the discrepancy between the two formula. In essence, their reply suggests that the discrepancy can be due to the inner product choice. To better explain why, I need to summarize their proof in Appendix A.4.1 of the Convex Optimization book. The proof is based on the idea that the derivative of a function gives the first-order approximation of the function. That is, the derivative of $f(X)$ can be obtained by finding a matrix $f(X)$ that satisfies $$f(X+\Delta X) \approx f(X)+\langle D,\Delta X\rangle.$$ In the book Boyd&Vandenberghe use the $\text{trace}(\cdot)$ function as the inner product $\langle \cdot, \cdot \rangle$ , and show that $$f(X+\Delta X) \approx f(X)+\text{trace}(X^{-1}\Delta X).$$ The book is publicly available ; how they arrived at this expression can be seen in the Appendix A.4.1. In their reply, Prof. Boyd suggests that they suspect the discrepancy to stem from the inner product use. While they used $\text{trace}(\cdot)$ , he suggests that some other people  may use $\langle A,B\rangle = \sum_{i<=j} A_{ij}B_{ij}$ . Authors claim that this can explain the discrepancy (although I'm not sure if they looked at the proof of Harville or others about the implicit or non-implicit usage of this inner product), because the trace function puts twice as much weight on the off-diagonal entries. Some questions where Boyd & Vanderberghe's formula is mentioned: Second order approximation of log det X How to calculate the gradient of log det matrix inverse? Why the gradient of $\log{\det{X}}$ is $X^{-1}$, and where did trace tr() go??","According to Appendix A.4.1 of Boyd & Vandenberghe's Convex Optimization , the gradient of is The domain of the here is the set of symmetric matrices . However, according to the book ""Matrix Algebra from a Statistician's Perspective"" by D. Harville, for a symmetric must be (see eq. 8.12 of book) where represents the th element on the diagonal of . Now I'm not a mathematician but to me the formula of Harville seems correct, because he makes use of the fact that the entries of are not ""independent"". Indeed, in the case where the entries are ''independent'', Harville provides another formula (eq. 8.8 of his book), which matches that of Boyd & Vandenberghe. Is this an error on the book of Boyd & Vandenberghe, or am I missing something here? To me it does seem like an error, but at the same time I find this extremely unlikely as the book is very popular and if it were an error it would already be on Errata; it's much more likely that I'm misunderstanding something. This formula has already been mentioned in many questions in this website, but no question or answer that I saw mentions (the possibility of) in Boyd & Vandenberghe being wrong. Edit based on response of Profs. Boyd & Vandenberghe Prof. Boyd kindly responded to my email about this issue, provided an explanation that he and Lieven Vandenberghe think can can explain the discrepancy between the two formula. In essence, their reply suggests that the discrepancy can be due to the inner product choice. To better explain why, I need to summarize their proof in Appendix A.4.1 of the Convex Optimization book. The proof is based on the idea that the derivative of a function gives the first-order approximation of the function. That is, the derivative of can be obtained by finding a matrix that satisfies In the book Boyd&Vandenberghe use the function as the inner product , and show that The book is publicly available ; how they arrived at this expression can be seen in the Appendix A.4.1. In their reply, Prof. Boyd suggests that they suspect the discrepancy to stem from the inner product use. While they used , he suggests that some other people  may use . Authors claim that this can explain the discrepancy (although I'm not sure if they looked at the proof of Harville or others about the implicit or non-implicit usage of this inner product), because the trace function puts twice as much weight on the off-diagonal entries. Some questions where Boyd & Vanderberghe's formula is mentioned: Second order approximation of log det X How to calculate the gradient of log det matrix inverse? Why the gradient of $\log{\det{X}}$ is $X^{-1}$, and where did trace tr() go??","f(X):=\log \det X \nabla f(X) = X^{-1} f \mathbf S^n \log \det X X \log \det X = 2 X^{-1} - \text{diag} (y_{11}, y_{22}, \dots, y_{nn}) y_{ii} i X^{-1} X \log \det X f(X) f(X) f(X+\Delta X) \approx f(X)+\langle D,\Delta X\rangle. \text{trace}(\cdot) \langle \cdot, \cdot \rangle f(X+\Delta X) \approx f(X)+\text{trace}(X^{-1}\Delta X). \text{trace}(\cdot) \langle A,B\rangle = \sum_{i<=j} A_{ij}B_{ij}","['matrices', 'derivatives', 'determinant', 'matrix-calculus', 'scalar-fields']"
41,Trace Norm properties,Trace Norm properties,,"Let $\|A\|_1=\operatorname{trace}(\sqrt{A^* A})$. I already proved that for arbitrary unitary matrices $U$ and $V$, $\|UAV^*\|_1=\|A\|_1$ and $\|A\|_1=\sigma_1+\dots+\sigma_k$. Now I would like to prove that $\|A\|_1$ defines a matrix norm, $A\in M_{m\times n}\mathbb (C)$. 1) $\|A\|_1=0\Leftrightarrow A=0$. I already proved that. 2) $\|\lambda A\|_1=|\lambda|\|A\|_1$.This also. 3) $|\operatorname{trace}(A)|\leqslant \|A\|_1$. I am not sure, my idea is to use $A=U\Sigma V^*$. 4) $\|BA\|_1\leqslant \|B\|\|A\|_1$ for $B\in M_{l\times m}\mathbb (C)$ and $\|B\|=\sup\frac{\|Bx\|}{\|x\|}=\max\{\sigma_1,\dots,\sigma_k\}$. My idea is again using singular value decomposition for $A$ and a polar decomposition for $BA$. 5)$\|A\|_1=\sup_{\|B\|\leqslant 1}|\operatorname{trace}(BA)|$ with $B\in M_{n\times m}\mathbb (C)$ and $A\in M_{m\times n}\mathbb (C)$ Here I have no idea. 6) $\|A+A'\|_1\leqslant\|A\|_1+\|A'\|_1$ with $A,A'\in M_{m\times n}\mathbb (C)$ This can be followed from 5). If you could help me with 3)-5) I would really appreciate it.","Let $\|A\|_1=\operatorname{trace}(\sqrt{A^* A})$. I already proved that for arbitrary unitary matrices $U$ and $V$, $\|UAV^*\|_1=\|A\|_1$ and $\|A\|_1=\sigma_1+\dots+\sigma_k$. Now I would like to prove that $\|A\|_1$ defines a matrix norm, $A\in M_{m\times n}\mathbb (C)$. 1) $\|A\|_1=0\Leftrightarrow A=0$. I already proved that. 2) $\|\lambda A\|_1=|\lambda|\|A\|_1$.This also. 3) $|\operatorname{trace}(A)|\leqslant \|A\|_1$. I am not sure, my idea is to use $A=U\Sigma V^*$. 4) $\|BA\|_1\leqslant \|B\|\|A\|_1$ for $B\in M_{l\times m}\mathbb (C)$ and $\|B\|=\sup\frac{\|Bx\|}{\|x\|}=\max\{\sigma_1,\dots,\sigma_k\}$. My idea is again using singular value decomposition for $A$ and a polar decomposition for $BA$. 5)$\|A\|_1=\sup_{\|B\|\leqslant 1}|\operatorname{trace}(BA)|$ with $B\in M_{n\times m}\mathbb (C)$ and $A\in M_{m\times n}\mathbb (C)$ Here I have no idea. 6) $\|A+A'\|_1\leqslant\|A\|_1+\|A'\|_1$ with $A,A'\in M_{m\times n}\mathbb (C)$ This can be followed from 5). If you could help me with 3)-5) I would really appreciate it.",,"['matrices', 'normed-spaces', 'matrix-norms', 'nuclear-norm']"
42,"Given a unitary matrix $U$, how do I find $A$ such that $U=e^{iA}$?","Given a unitary matrix , how do I find  such that ?",U A U=e^{iA},"A unitary matrix $U \in \mathbb C^{n \times n}$ can always be written in exponential form $$U = e^{iA} \tag{1}$$ where $A$ is Hermitian. My goal is to find the Hermitian matrix $A$ , given the unitary matrix $U$ . I figured out a way by diagonalizing $U$ , in the following form: $$U = V^{\dagger} [e^{ia_{kk}}] V$$ Therefore, we get $$A = V^{\dagger} [a_{kk}] V$$ Is this the standard way for finding the Hermitian matrix $A$ in equation (1)? If I'd like to learn more about the exponentiation of unitary operators, and their general properties, what topics should I read?","A unitary matrix can always be written in exponential form where is Hermitian. My goal is to find the Hermitian matrix , given the unitary matrix . I figured out a way by diagonalizing , in the following form: Therefore, we get Is this the standard way for finding the Hermitian matrix in equation (1)? If I'd like to learn more about the exponentiation of unitary operators, and their general properties, what topics should I read?",U \in \mathbb C^{n \times n} U = e^{iA} \tag{1} A A U U U = V^{\dagger} [e^{ia_{kk}}] V A = V^{\dagger} [a_{kk}] V A,"['matrices', 'operator-theory', 'matrix-calculus', 'matrix-exponential', 'unitary-matrices']"
43,Why is the maximum Rayleigh quotient equal to the maximum eigenvalue?,Why is the maximum Rayleigh quotient equal to the maximum eigenvalue?,,"(Note: I'm only interested in real-valued matrices here, so I'm using ""transpose"" and ""symmetric"" instead of the more general ""transjugate"" and ""Hermitian"" in the hope that it will simplify the proof. But the theorem apparently holds for complex-valued matrices as well.) The Rayleigh quotient $R(M,v)$ of a symmetric matrix $M$ and a vector $v$ is defined as $\frac{v^T M v}{v^T v}$, where $x^T$ is the matrix transpose of $x$. I've been told that the vector $v$ which gives the largest Rayleigh quotient is, in fact, the eigenvector corresponding to the largest eigenvalue of $M$. And furthermore, the value of the quotient in this case is equal to that eigenvalue. However, I've been unable to find a full proof of this fact, or an explanation of why it should work this way. Why is there this connection between the Rayleigh quotient and the eigenvalues? Anything from an intuitive explanation to a formal proof would be appreciated.","(Note: I'm only interested in real-valued matrices here, so I'm using ""transpose"" and ""symmetric"" instead of the more general ""transjugate"" and ""Hermitian"" in the hope that it will simplify the proof. But the theorem apparently holds for complex-valued matrices as well.) The Rayleigh quotient $R(M,v)$ of a symmetric matrix $M$ and a vector $v$ is defined as $\frac{v^T M v}{v^T v}$, where $x^T$ is the matrix transpose of $x$. I've been told that the vector $v$ which gives the largest Rayleigh quotient is, in fact, the eigenvector corresponding to the largest eigenvalue of $M$. And furthermore, the value of the quotient in this case is equal to that eigenvalue. However, I've been unable to find a full proof of this fact, or an explanation of why it should work this way. Why is there this connection between the Rayleigh quotient and the eigenvalues? Anything from an intuitive explanation to a formal proof would be appreciated.",,"['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'transpose']"
44,How to check whether a relation is transitive from the matrix representation?,How to check whether a relation is transitive from the matrix representation?,,"$$\begin{bmatrix}1&0&1\\0&1&0\\1&0&1\end{bmatrix}$$ This is a matrix representation of a relation on the set $\{1, 2, 3\}$. I have to determine if this relation matrix is transitive. I know that the ordered-pairs that make this matrix transitive are $(1, 3)$, $(3,3)$, and $(3, 1)$; but what I am having trouble is applying the definition to see what the $a$, $b$, and $c$ values are that make this relation transitive. I am sorry if this problem seems trivial, but I could use some help. Thank you!","$$\begin{bmatrix}1&0&1\\0&1&0\\1&0&1\end{bmatrix}$$ This is a matrix representation of a relation on the set $\{1, 2, 3\}$. I have to determine if this relation matrix is transitive. I know that the ordered-pairs that make this matrix transitive are $(1, 3)$, $(3,3)$, and $(3, 1)$; but what I am having trouble is applying the definition to see what the $a$, $b$, and $c$ values are that make this relation transitive. I am sorry if this problem seems trivial, but I could use some help. Thank you!",,"['matrices', 'elementary-set-theory', 'relations']"
45,Proof: Tangent space of the general linear group is the set of all squared matrices,Proof: Tangent space of the general linear group is the set of all squared matrices,,"Let us assume we have the following definition of a tangent space: Definition of smooth path Let $X\subset\mathbb{R}^n$. Let $I$ be a real interval.  \begin{equation} P \text{ is a smooth path in } X \quad:\Leftrightarrow\quad P:I\rightarrow X \text{ is a differentiable function}~.  \end{equation} Definition: Tangent vector  at the identity Let $P$ be a smooth path with $P(0) = \mathtt{I}$.  \begin{equation} t \text{ is the tangent vector of } P \text{ at the idenity} \quad:\Leftrightarrow\quad t=\frac{\partial}{\partial x}P(x)|_{x=0}  \end{equation} Moreover, we call $t$ a tangent vector  of a space $X$, if a path $P:I\rightarrow X$ exists such that $t$ is  tangent vector of a $P$. Definition: Tangent space We call the space of all tangent vectors (at the identity), the tangent space (at the identity). Using these definitions, (how) can we show that the tangent space of all invertible matrices $GL(n)$ is indeed the space of all $n\times n$ matrices? (I'd like to rely mainly on these definitions and do not use the notion of the  exponential map if possible...)","Let us assume we have the following definition of a tangent space: Definition of smooth path Let $X\subset\mathbb{R}^n$. Let $I$ be a real interval.  \begin{equation} P \text{ is a smooth path in } X \quad:\Leftrightarrow\quad P:I\rightarrow X \text{ is a differentiable function}~.  \end{equation} Definition: Tangent vector  at the identity Let $P$ be a smooth path with $P(0) = \mathtt{I}$.  \begin{equation} t \text{ is the tangent vector of } P \text{ at the idenity} \quad:\Leftrightarrow\quad t=\frac{\partial}{\partial x}P(x)|_{x=0}  \end{equation} Moreover, we call $t$ a tangent vector  of a space $X$, if a path $P:I\rightarrow X$ exists such that $t$ is  tangent vector of a $P$. Definition: Tangent space We call the space of all tangent vectors (at the identity), the tangent space (at the identity). Using these definitions, (how) can we show that the tangent space of all invertible matrices $GL(n)$ is indeed the space of all $n\times n$ matrices? (I'd like to rely mainly on these definitions and do not use the notion of the  exponential map if possible...)",,"['matrices', 'differential-geometry', 'lie-groups']"
46,"Measure of ""how much diagonal"" a matrix is","Measure of ""how much diagonal"" a matrix is",,"I have a (biological) computational system that outputs square matrices. Sometimes, these matrices are diagonal-like, with higher values at and around the diagonal. I would like to have some summary measure on how ""much diagonal"" a matrix is, so that I can batch-process hundreds of outputs and score them on how much the higher entries cluster in and around the diagonal. Any ideas of some standard approach that I can generalise?","I have a (biological) computational system that outputs square matrices. Sometimes, these matrices are diagonal-like, with higher values at and around the diagonal. I would like to have some summary measure on how ""much diagonal"" a matrix is, so that I can batch-process hundreds of outputs and score them on how much the higher entries cluster in and around the diagonal. Any ideas of some standard approach that I can generalise?",,"['matrices', 'clustering']"
47,Intuitive explanation of outer product,Intuitive explanation of outer product,,The inner product between two vectors is the product of length of first vector and the length of projection of second vector on to the first vector. When I take an outer product its result is a matrix. I understand how to calculate it but I am not able to find out what it represents intuitively and why would it be useful. I have searched about it but have not found some simple explanation of it for myself. So any easy to understand explanation of it would be much appreciated. Many thanks!,The inner product between two vectors is the product of length of first vector and the length of projection of second vector on to the first vector. When I take an outer product its result is a matrix. I understand how to calculate it but I am not able to find out what it represents intuitively and why would it be useful. I have searched about it but have not found some simple explanation of it for myself. So any easy to understand explanation of it would be much appreciated. Many thanks!,,"['matrices', 'outer-product']"
48,Easier way of calculating the determinant for this matrix,Easier way of calculating the determinant for this matrix,,I have to calculate the determinant of this matrix: $$ \begin{pmatrix} a&b&c&d\\b&c&d&a\\c&d&a&b\\d&a&b&c \end{pmatrix} $$ Is there an easier way of calculating this rather than the long regular way?,I have to calculate the determinant of this matrix: $$ \begin{pmatrix} a&b&c&d\\b&c&d&a\\c&d&a&b\\d&a&b&c \end{pmatrix} $$ Is there an easier way of calculating this rather than the long regular way?,,"['matrices', 'determinant', 'circulant-matrices']"
49,The openness of the set of positive definite square matrices,The openness of the set of positive definite square matrices,,"Let $\mathbb{R}^{n\times n}$ be the vector space of square matrices with real entries. For each  $A\in \mathbb{R}^{n\times n}$ we consider the norms given by: $$ \displaystyle\|A\|_1=\max_{1\leq j\leq n}\sum_{i=1}^{n}|a_{ij}|; $$  $$ \displaystyle\|A\|_\infty=\max_{1\leq i\leq n}\sum_{j=1}^{n}|a_{ij}|; $$ $$ \displaystyle\|A\|_\text{max}=\max\{|a_{ij}|\}. $$ Matrix $A\in \mathbb{R}^{n\times n}$ is said to be positive definite iff $$ \langle Ax, x\rangle> 0 \quad \forall x\in\mathbb{R}^n\setminus\{0\}. $$  Let $S$ be the set of all positive definite matrices on $\mathbb{R}^{n\times n}$. Prove that $S$ is an open set in $(X,\|.\|_1)$, $(X,\|.\|_\infty$), $(X,\|.\|_\text{max})$. I would like to thank all for their help and comments.","Let $\mathbb{R}^{n\times n}$ be the vector space of square matrices with real entries. For each  $A\in \mathbb{R}^{n\times n}$ we consider the norms given by: $$ \displaystyle\|A\|_1=\max_{1\leq j\leq n}\sum_{i=1}^{n}|a_{ij}|; $$  $$ \displaystyle\|A\|_\infty=\max_{1\leq i\leq n}\sum_{j=1}^{n}|a_{ij}|; $$ $$ \displaystyle\|A\|_\text{max}=\max\{|a_{ij}|\}. $$ Matrix $A\in \mathbb{R}^{n\times n}$ is said to be positive definite iff $$ \langle Ax, x\rangle> 0 \quad \forall x\in\mathbb{R}^n\setminus\{0\}. $$  Let $S$ be the set of all positive definite matrices on $\mathbb{R}^{n\times n}$. Prove that $S$ is an open set in $(X,\|.\|_1)$, $(X,\|.\|_\infty$), $(X,\|.\|_\text{max})$. I would like to thank all for their help and comments.",,"['matrices', 'functional-analysis', 'normed-spaces']"
50,"Show that a 2x2 matrix A is symmetric positive definite if and only if A is symmetric, trace(A) > 0 and det(A) > 0","Show that a 2x2 matrix A is symmetric positive definite if and only if A is symmetric, trace(A) > 0 and det(A) > 0",,"I need to show two parts of the implication are true.  First: if $A$ is $2\times 2$ and is symmetric positive definite then $trace(A)>0$ and $\det(A)>0$. Second: if $trace(A)>0$ and $\det(A)>0$ then $A$ is symmetric positive definite. For the first part I was thinking: $A$ is symmetric and positive definite then $A$ has its eigenvalues positive. If $A$ is $2\times 2$  then characteristic polynomial of $A$ is  $x^2−x.trace(A)+\det(A)=0$  If we compute the discriminant we get $(tr(A))^2 ≥ 4.\det(A)$  Now $tr(A)$ is squared so it is positive. How do I know that $\det(A)$ is also positive? After copper.hat's response we argue that the eigenvalues of A are all positive because $A$ is spsd and the $\det(A)$ is the product of its eigenvalues. Thus $\det(A)$ is strictly positive. Now, how do I verify the second part? Thanks.","I need to show two parts of the implication are true.  First: if $A$ is $2\times 2$ and is symmetric positive definite then $trace(A)>0$ and $\det(A)>0$. Second: if $trace(A)>0$ and $\det(A)>0$ then $A$ is symmetric positive definite. For the first part I was thinking: $A$ is symmetric and positive definite then $A$ has its eigenvalues positive. If $A$ is $2\times 2$  then characteristic polynomial of $A$ is  $x^2−x.trace(A)+\det(A)=0$  If we compute the discriminant we get $(tr(A))^2 ≥ 4.\det(A)$  Now $tr(A)$ is squared so it is positive. How do I know that $\det(A)$ is also positive? After copper.hat's response we argue that the eigenvalues of A are all positive because $A$ is spsd and the $\det(A)$ is the product of its eigenvalues. Thus $\det(A)$ is strictly positive. Now, how do I verify the second part? Thanks.",,"['matrices', 'determinant']"
51,Postitive definiteness of the Kronecker product of two positive definite matrices,Postitive definiteness of the Kronecker product of two positive definite matrices,,Let $A$ and $B$ both be positive definite matrices. How do I  show that their Kronecker product is also positive definite? I know we can use the fact that the eigenvalues of the Kronecker product is $\lambda_A+\lambda_B$ which are all positive. But I want  to use a different approach here. Thank you.,Let $A$ and $B$ both be positive definite matrices. How do I  show that their Kronecker product is also positive definite? I know we can use the fact that the eigenvalues of the Kronecker product is $\lambda_A+\lambda_B$ which are all positive. But I want  to use a different approach here. Thank you.,,"['matrices', 'tensor-products']"
52,A $\frac{1}{3}$ Conjecture?,A  Conjecture?,\frac{1}{3},Question: Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{ij}=1$ if $i+j$ is a perfect power; otherwise equals to   $0$. Is it true that $${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij} \leq {1 \above 1.5pt 3}$$ with equality holding if and only if   $n=3$ or $n=6$ ? Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{ij}=1$ if $i+j$ is a perfect power; otherwise equals to $0$. For an example consider $A(5)$ $$A(5)= \text{ }\begin{pmatrix} 0&0&1&0&0\\ 0&1&0&0&0\\ 1&0&0&0&1\\ 0&0&0&1&1\\ 0&0&1&1&0\\ \end{pmatrix}$$ Can we show that ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij} \leq {1 \above 1.5pt 3}$ with equality holding if and only if $n=3$ or $n=6$. The graph below plots the values of ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij}$ for small $n$. The graph is what motivated me to ask the question. It appears that the maximums are achieved if $n=3$ or $n=6$. UPDATE: I have corrected several terms and added several new terms check out: A293462 . Here was my approach: Let $^t$ be the transpose map that sends the entry $a_{ij} \to a_{ji}$. The commutativity of addition shows us that if $i+j$ is a perfect power then so is $j+i$. Equivalently we see that $a_{ij}=a_{ji}$. In particular $A(n)^t=A(n)$ and so $A(n)$ is symmetric. Now observe that $(a_{ij})^2=a_{ij}$. Since $A(n)$ is symmetric $A(n)^tA(n)=A(n)^2$. The following result is easy to show $$\sum_{i=1}^n \sum_{j=1}^n a_{ij}=tr(A(n)^2)$$ Similarly it easy to show that if ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij}={1 \above 1.5 pt x}$ then $x$ is a divisor of $n$. Assume ${tr(A(n)^2) \above 1.5pt n^2}={1 \above 1.5pt 3}$ then $3$ divides $n$. We start by showing via inspection the base case of $n=3$ and $n=6$. Suppose $n=3$ then $$A(3)^2= \text{ } \begin{pmatrix} 1&0&0\\ 0&1&0\\ 0&0&1\\ \end{pmatrix}$$ And so $tr(A(3)^2)=3$. Surely ${3 \above 1.5pt 3^2}={1 \above 1.5 pt 3}$. Similarly it is easy to compute and show that if $n=6$ $$A(6)^2= \text{ }\begin{pmatrix} 1&0&0&0&1&1\\ 0&2&1&0&0&1\\ 0&1&3&1&0&0\\ 0&0&1&2&1&0\\ 1&0&0&1&2&1\\ 1&1&0&0&1&2\\ \end{pmatrix} $$ And from this we can see that $tr(A(6)^2)=12$. And again we have that ${12 \above 1.5pt 6^2}={1 \above 1.5 pt 3}$. Assume $n\neq 3$ and $n\neq 6$. Now since ${tr(A(n)^2) \above 1.5pt n^2}={1 \above 1.5pt 3}$ we know that $3\times tr(A(n)^2)=n^2$. If $a_{ii}$ is any entry on the diagonal of $A(n)^2$ then explictly $3(a_{11}+ \ldots +a_{nn})=n^2$ so $\sqrt{3}\sqrt{a_{11}+\ldots + a_{nn}}=n$ and consequently $\sqrt{3} \mid \sqrt{a_{11}+\ldots + a_{nn}}$ otherwise $n$ is not an integer which is a contradiction. ^Update 1: The argument scratched out above is wrong thanks to commentator @SEWillB.  ^Update 2: The argument previously scratched out above is correct. See edits. That is all I can come up with - and it might not be the best approach and possibly the problem is trivial and I am just missing it. It could also be wrong. The picture below provides some data for small values of $n$.,Question: Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{ij}=1$ if $i+j$ is a perfect power; otherwise equals to   $0$. Is it true that $${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij} \leq {1 \above 1.5pt 3}$$ with equality holding if and only if   $n=3$ or $n=6$ ? Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{ij}=1$ if $i+j$ is a perfect power; otherwise equals to $0$. For an example consider $A(5)$ $$A(5)= \text{ }\begin{pmatrix} 0&0&1&0&0\\ 0&1&0&0&0\\ 1&0&0&0&1\\ 0&0&0&1&1\\ 0&0&1&1&0\\ \end{pmatrix}$$ Can we show that ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij} \leq {1 \above 1.5pt 3}$ with equality holding if and only if $n=3$ or $n=6$. The graph below plots the values of ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij}$ for small $n$. The graph is what motivated me to ask the question. It appears that the maximums are achieved if $n=3$ or $n=6$. UPDATE: I have corrected several terms and added several new terms check out: A293462 . Here was my approach: Let $^t$ be the transpose map that sends the entry $a_{ij} \to a_{ji}$. The commutativity of addition shows us that if $i+j$ is a perfect power then so is $j+i$. Equivalently we see that $a_{ij}=a_{ji}$. In particular $A(n)^t=A(n)$ and so $A(n)$ is symmetric. Now observe that $(a_{ij})^2=a_{ij}$. Since $A(n)$ is symmetric $A(n)^tA(n)=A(n)^2$. The following result is easy to show $$\sum_{i=1}^n \sum_{j=1}^n a_{ij}=tr(A(n)^2)$$ Similarly it easy to show that if ${1 \above 1.5 pt n^2}\sum_{i=1}^n \sum_{j=1}^n a_{ij}={1 \above 1.5 pt x}$ then $x$ is a divisor of $n$. Assume ${tr(A(n)^2) \above 1.5pt n^2}={1 \above 1.5pt 3}$ then $3$ divides $n$. We start by showing via inspection the base case of $n=3$ and $n=6$. Suppose $n=3$ then $$A(3)^2= \text{ } \begin{pmatrix} 1&0&0\\ 0&1&0\\ 0&0&1\\ \end{pmatrix}$$ And so $tr(A(3)^2)=3$. Surely ${3 \above 1.5pt 3^2}={1 \above 1.5 pt 3}$. Similarly it is easy to compute and show that if $n=6$ $$A(6)^2= \text{ }\begin{pmatrix} 1&0&0&0&1&1\\ 0&2&1&0&0&1\\ 0&1&3&1&0&0\\ 0&0&1&2&1&0\\ 1&0&0&1&2&1\\ 1&1&0&0&1&2\\ \end{pmatrix} $$ And from this we can see that $tr(A(6)^2)=12$. And again we have that ${12 \above 1.5pt 6^2}={1 \above 1.5 pt 3}$. Assume $n\neq 3$ and $n\neq 6$. Now since ${tr(A(n)^2) \above 1.5pt n^2}={1 \above 1.5pt 3}$ we know that $3\times tr(A(n)^2)=n^2$. If $a_{ii}$ is any entry on the diagonal of $A(n)^2$ then explictly $3(a_{11}+ \ldots +a_{nn})=n^2$ so $\sqrt{3}\sqrt{a_{11}+\ldots + a_{nn}}=n$ and consequently $\sqrt{3} \mid \sqrt{a_{11}+\ldots + a_{nn}}$ otherwise $n$ is not an integer which is a contradiction. ^Update 1: The argument scratched out above is wrong thanks to commentator @SEWillB.  ^Update 2: The argument previously scratched out above is correct. See edits. That is all I can come up with - and it might not be the best approach and possibly the problem is trivial and I am just missing it. It could also be wrong. The picture below provides some data for small values of $n$.,,"['matrices', 'analytic-number-theory', 'conjectures', 'perfect-powers']"
53,Proof that the range of a map is determined by its behaviour on the boundary.,Proof that the range of a map is determined by its behaviour on the boundary.,,"Let f be a mapping from an open neighbourhood of the 3-dimensional unit ball to the 2-dimensional plane. Suppose that f is smooth (infinitely continuously differentiable on its domain) and regular (it's derivative, as a 2x3 matrix, has rank 2 everywhere). I would like a proof, or informed hint, or counterexample, for the claim that the f restricted to the closed unit ball has the same range as f restricted to the unit sphere. It is not difficult to construct a counterexample when f is allowed to be nonregular. I have stated the problem for a mapping from 3 to 2 dimensions but there may be a similar problem going from n to n-1 dimensions. For n=2 the proof is trivial: a regular differentiable mapping from the closed unit disk to the line reaches its maximum on the boundary circle.","Let f be a mapping from an open neighbourhood of the 3-dimensional unit ball to the 2-dimensional plane. Suppose that f is smooth (infinitely continuously differentiable on its domain) and regular (it's derivative, as a 2x3 matrix, has rank 2 everywhere). I would like a proof, or informed hint, or counterexample, for the claim that the f restricted to the closed unit ball has the same range as f restricted to the unit sphere. It is not difficult to construct a counterexample when f is allowed to be nonregular. I have stated the problem for a mapping from 3 to 2 dimensions but there may be a similar problem going from n to n-1 dimensions. For n=2 the proof is trivial: a regular differentiable mapping from the closed unit disk to the line reaches its maximum on the boundary circle.",,"['matrices', 'differential-geometry', 'derivatives', 'differential-topology']"
54,How to calculate the number of factorizations of a square matrix?,How to calculate the number of factorizations of a square matrix?,,"I need to  write a function, that, given a square matrix M of non-negative integers, calculates the number of representations of M as a product of two square matrices of non-negative integers. Could you please help me with it?","I need to  write a function, that, given a square matrix M of non-negative integers, calculates the number of representations of M as a product of two square matrices of non-negative integers. Could you please help me with it?",,['matrices']
55,Minimum and maximum determinant of a sudoku-matrix,Minimum and maximum determinant of a sudoku-matrix,,"Let $A$ be a sudoku-matrix. Assume that its determinant is positive. What is the lowest,  what the highest possible value for the determinant of $A$ ?  $A$ must have the dominant eigenvalue $45$, but this does not seem to help establishing  bounds. My records so far : $$\pmatrix{7&2&9&6&4&3&5&1&8 \\ 5&6&8&9&1&2&7&4&3 \\ 1&3&4&8&5&7&9&6&2 \\ 2&8&7&4&6&1&3&9&5 \\ 9&5&1&7&3&8&6&2&4 \\ 3&4&6&2&9&5&8&7&1 \\ 4&9&3&5&2&6&1&8&7 \\ 8&1&2&3&7&9&4&5&6 \\ 6&7&5&1&8&4&2&3&9}$$ leads to a sudoku-matrix with determinant $1215$. $$\pmatrix{4&3&1&9&7&5&2&6&8 \\ 6&7&2&3&8&1&9&5&4 \\ 8&9&5&6&4&2&7&1&3 \\ 5&4&9&1&6&8&3&2&7 \\ 7&1&3&4&2&9&6&8&5 \\ 2&8&6&5&3&7&4&9&1 \\ 1&5&4&7&9&6&8&3&2 \\ 9&2&7&8&5&3&1&4&6 \\ 3&6&8&2&1&4&5&7&9 }$$ leads to a sudoku-matrix with determinant $238 615 470$. Additional question : Can a sudoku-matrix have multiple eigenvalues and, even more interesting,    be not diagonalizable or have a minimal polynomial different from the    characteristic polynomial ? I also found a singular sudoku matrix : $$\pmatrix{6&5&3&9&4&7&8&1&2 \\ 9&8&7&1&6&2&4&3&5 \\ 4&2&1&3&5&8&6&7&9 \\ 5&3&8&4&2&6&1&9&7 \\ 2&7&4&5&9&1&3&8&6 \\ 1&9&6&7&8&3&2&5&4 \\ 8&6&5&2&1&9&7&4&3 \\ 3&1&9&6&7&4&5&2&8 \\ 7&4&2&8&3&5&9&6&1}$$ I found out that the determinant must be a multiple of $405$, so $405$ is a lower  bound.  I found a sudoku-matrix with determinant $405$ , so it remains to find the maximum.","Let $A$ be a sudoku-matrix. Assume that its determinant is positive. What is the lowest,  what the highest possible value for the determinant of $A$ ?  $A$ must have the dominant eigenvalue $45$, but this does not seem to help establishing  bounds. My records so far : $$\pmatrix{7&2&9&6&4&3&5&1&8 \\ 5&6&8&9&1&2&7&4&3 \\ 1&3&4&8&5&7&9&6&2 \\ 2&8&7&4&6&1&3&9&5 \\ 9&5&1&7&3&8&6&2&4 \\ 3&4&6&2&9&5&8&7&1 \\ 4&9&3&5&2&6&1&8&7 \\ 8&1&2&3&7&9&4&5&6 \\ 6&7&5&1&8&4&2&3&9}$$ leads to a sudoku-matrix with determinant $1215$. $$\pmatrix{4&3&1&9&7&5&2&6&8 \\ 6&7&2&3&8&1&9&5&4 \\ 8&9&5&6&4&2&7&1&3 \\ 5&4&9&1&6&8&3&2&7 \\ 7&1&3&4&2&9&6&8&5 \\ 2&8&6&5&3&7&4&9&1 \\ 1&5&4&7&9&6&8&3&2 \\ 9&2&7&8&5&3&1&4&6 \\ 3&6&8&2&1&4&5&7&9 }$$ leads to a sudoku-matrix with determinant $238 615 470$. Additional question : Can a sudoku-matrix have multiple eigenvalues and, even more interesting,    be not diagonalizable or have a minimal polynomial different from the    characteristic polynomial ? I also found a singular sudoku matrix : $$\pmatrix{6&5&3&9&4&7&8&1&2 \\ 9&8&7&1&6&2&4&3&5 \\ 4&2&1&3&5&8&6&7&9 \\ 5&3&8&4&2&6&1&9&7 \\ 2&7&4&5&9&1&3&8&6 \\ 1&9&6&7&8&3&2&5&4 \\ 8&6&5&2&1&9&7&4&3 \\ 3&1&9&6&7&4&5&2&8 \\ 7&4&2&8&3&5&9&6&1}$$ I found out that the determinant must be a multiple of $405$, so $405$ is a lower  bound.  I found a sudoku-matrix with determinant $405$ , so it remains to find the maximum.",,"['matrices', 'eigenvalues-eigenvectors', 'determinant', 'recreational-mathematics', 'sudoku']"
56,Does there exist a matrix $\mathbf{A}\in\mathbb{R}^{3\times3}$ such that $\mathbf{A}^{2}=-\mathbf{I}$?,Does there exist a matrix  such that ?,\mathbf{A}\in\mathbb{R}^{3\times3} \mathbf{A}^{2}=-\mathbf{I},"Is it possible for a matrix $\mathbf{A}\in\mathbb{R}^{3\times3}$, $$\mathbf{A}^2=-\mathbf{I}$$ I know that It is possible for $2\times2$ matrix, but is it possible for $3\times3$ matrix ?","Is it possible for a matrix $\mathbf{A}\in\mathbb{R}^{3\times3}$, $$\mathbf{A}^2=-\mathbf{I}$$ I know that It is possible for $2\times2$ matrix, but is it possible for $3\times3$ matrix ?",,['matrices']
57,Determinant of matrix exponential?,Determinant of matrix exponential?,,"Suppose $A$ is a $n \times n$ constant matrix. How can I prove $\det(e^A) = e^{\displaystyle \sum_{\lambda_i\in\sigma(A)} \lambda_i}$, where $\sigma(A)$ is the multiset of eigenvalues of $A$? The following matlab code shows this is true in $n = 3$ case: A = rand(3) detA = exp(sum(eig(A))) detmA = det(expm(A))","Suppose $A$ is a $n \times n$ constant matrix. How can I prove $\det(e^A) = e^{\displaystyle \sum_{\lambda_i\in\sigma(A)} \lambda_i}$, where $\sigma(A)$ is the multiset of eigenvalues of $A$? The following matlab code shows this is true in $n = 3$ case: A = rand(3) detA = exp(sum(eig(A))) detmA = det(expm(A))",,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
58,$3\times3$ matrix of distinct positive primes with determinant $0$,matrix of distinct positive primes with determinant,3\times3 0,"Is there a $3\times3$ matrix of distinct positive primes whose determinant is $0$? I came across this while attempting a partial answer to a question here (which I forgot to favorite). However, I've made no headway on it.","Is there a $3\times3$ matrix of distinct positive primes whose determinant is $0$? I came across this while attempting a partial answer to a question here (which I forgot to favorite). However, I've made no headway on it.",,"['matrices', 'number-theory', 'elementary-number-theory', 'prime-numbers']"
59,Largest determinant of a real $3\times 3$-matrix,Largest determinant of a real -matrix,3\times 3,"What is the largest determinant of a real  $3\times 3$-matrix with entries from the interval $[-1,1]$ ? A result of John Williamson says that the largest value is equal to $4$, if the entries are just either $1$ or $-1$. Is this still true for all values in $[-1,1]$ ? For complex matrices with entries $|a_{ij}|\le 1$  it is not true. We have $|\det(A)|\le 3\sqrt{3}$, and equality can be attained with a Vandermonde type of matrix containing the third roots of unity.","What is the largest determinant of a real  $3\times 3$-matrix with entries from the interval $[-1,1]$ ? A result of John Williamson says that the largest value is equal to $4$, if the entries are just either $1$ or $-1$. Is this still true for all values in $[-1,1]$ ? For complex matrices with entries $|a_{ij}|\le 1$  it is not true. We have $|\det(A)|\le 3\sqrt{3}$, and equality can be attained with a Vandermonde type of matrix containing the third roots of unity.",,"['matrices', 'determinant']"
60,Inverse of the Pascal Matrix,Inverse of the Pascal Matrix,,"Let $P_n$ be the $(n+1) \times (n+1)$ matrix that contains the numbers of Pascal's triangle in the upper triangle. For example in the case of $n=3$ $$ P_3 =  \begin{pmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 2 & 3 \\ 0 & 0 & 1 & 3 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix} $$ or in general  $$ (P_n)_{ij} = \binom{j}{i} \lfloor  i \leq j  \rceil  ~~~\text{for}~~ i,j \in \{0,...,n \}  $$ using the definition $$ \lfloor  A  \rceil := \begin{cases}     1 & \text{A is true} \\     0 & \text{A is not true}     \end{cases} $$ This matrix is invertible since $\det P_n = 1$. For smaller cases like $n=3$, I calulated the inverse of the matrix by hand and found $$ P_3^{-1} =  \begin{pmatrix} 1 & -1 & 1 & -1 \\ 0 & 1 & -2 & 3 \\ 0 & 0 & 1 & -3 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix} $$ $n=2,4$ led to similar results, so I'm guessing that the inverse should be $$ (P_n^{-1})_{ij} = (-1)^{j+i}(P_n)_{ij} = (-1)^{j+i} \binom{j}{i} \lfloor  i \leq j  \rceil $$ But I have not been able to prove or disprove this yet. So far I tried multiplying the two matirces which gives $$ \sum^j_{k=i} (-1)^{j+k} \binom{j}{k} \binom{k}{i} = \delta_{ij} $$ if one asumes that the result is the unit matrix. For $i=0$ and $j>0$ this gives $$ \delta_{0j} = 0  = \sum^j_{k=0} (-1)^{j+k} \binom{j}{k} \binom{k}{0} = \sum^j_{k=0} (-1)^{k} \binom{j}{k} $$ which is an identity I know to be true, so it reasures me a little bit that the above should also be true.","Let $P_n$ be the $(n+1) \times (n+1)$ matrix that contains the numbers of Pascal's triangle in the upper triangle. For example in the case of $n=3$ $$ P_3 =  \begin{pmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 2 & 3 \\ 0 & 0 & 1 & 3 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix} $$ or in general  $$ (P_n)_{ij} = \binom{j}{i} \lfloor  i \leq j  \rceil  ~~~\text{for}~~ i,j \in \{0,...,n \}  $$ using the definition $$ \lfloor  A  \rceil := \begin{cases}     1 & \text{A is true} \\     0 & \text{A is not true}     \end{cases} $$ This matrix is invertible since $\det P_n = 1$. For smaller cases like $n=3$, I calulated the inverse of the matrix by hand and found $$ P_3^{-1} =  \begin{pmatrix} 1 & -1 & 1 & -1 \\ 0 & 1 & -2 & 3 \\ 0 & 0 & 1 & -3 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix} $$ $n=2,4$ led to similar results, so I'm guessing that the inverse should be $$ (P_n^{-1})_{ij} = (-1)^{j+i}(P_n)_{ij} = (-1)^{j+i} \binom{j}{i} \lfloor  i \leq j  \rceil $$ But I have not been able to prove or disprove this yet. So far I tried multiplying the two matirces which gives $$ \sum^j_{k=i} (-1)^{j+k} \binom{j}{k} \binom{k}{i} = \delta_{ij} $$ if one asumes that the result is the unit matrix. For $i=0$ and $j>0$ this gives $$ \delta_{0j} = 0  = \sum^j_{k=0} (-1)^{j+k} \binom{j}{k} \binom{k}{0} = \sum^j_{k=0} (-1)^{k} \binom{j}{k} $$ which is an identity I know to be true, so it reasures me a little bit that the above should also be true.",,"['matrices', 'polynomials', 'binomial-coefficients']"
61,Can we recover the adjacency matrix of a graph from its square?,Can we recover the adjacency matrix of a graph from its square?,,"For the sake of this question, a graph here has a finite number of vertices, with undirected simple edges, no loop, and no weight or label on edges or vertices. Therefore, its adjacency matrix $A=[a_{i,j}]$ is a symmetric matrix with entries in $\{0,1\}$ , and $0$ on the main diagonal. Assuming that $A^2$ is known, can we recover $A$ ? The question comes from my self-study of graph theory and I have no idea on how to solve this question. What I know: Let's label the vertices of the graph as $v_1$ , $v_2$ , $\dots$ , $v_n$ so that $a_{i,j}=1$ if there is an edge between $v_i$ and $v_j$ , and $0$ otherwise. Then, if $a_{i,j}^{(k)}$ is the entry of $A^k$ at row $i$ and column $j$ , $a_{i,j}^{(k)}$ is the number of walks of length $k$ between $v_i$ and $v_j$ . Therefore, what is known in the problem are the numbers $a_{i,j}^{(2)}$ of common neighbors between $v_i$ and $v_j$ . If the $i$ th row of $A^2$ is composed of $0$ s, then $v_i$ is an isolated point of the graph. (If $v_i$ is not isolated, then there is a walk $v_i-v_j-v_i$ so $a_{i,i}^{(2)}\ge 1$ ). So we can simplify the problem by assuming that the graph has no isolated point. $a_{i,i}^{(2)}=1$ is equivalent to $\deg(v_i)=1$ (end vertex). There are results about square roots of positive semi-definite matrices, but $A^2$ is not positive semi-definite, and the square root would not have its entries in $\{0,1\}$ . For $n=3$ , by looking at the squares of the adjacency matrices of the few possible graphs with $3$ vertices, the answer is yes. In my question, I assume that it is known that $S=A^2$ is the square of the adjacency matrix of a graph and I wonder if there is another graph whose adjacency matrix has also $S$ for square. So a reformulation of the question is Does it exist two adjacency matrices $A$ and $B$ (as defined in the first paragraph) such that $A^2=B^2$ ?","For the sake of this question, a graph here has a finite number of vertices, with undirected simple edges, no loop, and no weight or label on edges or vertices. Therefore, its adjacency matrix is a symmetric matrix with entries in , and on the main diagonal. Assuming that is known, can we recover ? The question comes from my self-study of graph theory and I have no idea on how to solve this question. What I know: Let's label the vertices of the graph as , , , so that if there is an edge between and , and otherwise. Then, if is the entry of at row and column , is the number of walks of length between and . Therefore, what is known in the problem are the numbers of common neighbors between and . If the th row of is composed of s, then is an isolated point of the graph. (If is not isolated, then there is a walk so ). So we can simplify the problem by assuming that the graph has no isolated point. is equivalent to (end vertex). There are results about square roots of positive semi-definite matrices, but is not positive semi-definite, and the square root would not have its entries in . For , by looking at the squares of the adjacency matrices of the few possible graphs with vertices, the answer is yes. In my question, I assume that it is known that is the square of the adjacency matrix of a graph and I wonder if there is another graph whose adjacency matrix has also for square. So a reformulation of the question is Does it exist two adjacency matrices and (as defined in the first paragraph) such that ?","A=[a_{i,j}] \{0,1\} 0 A^2 A v_1 v_2 \dots v_n a_{i,j}=1 v_i v_j 0 a_{i,j}^{(k)} A^k i j a_{i,j}^{(k)} k v_i v_j a_{i,j}^{(2)} v_i v_j i A^2 0 v_i v_i v_i-v_j-v_i a_{i,i}^{(2)}\ge 1 a_{i,i}^{(2)}=1 \deg(v_i)=1 A^2 \{0,1\} n=3 3 S=A^2 S A B A^2=B^2","['matrices', 'graph-theory']"
62,Is there a way to extract the diagonal from a matrix with simple matrix operations,Is there a way to extract the diagonal from a matrix with simple matrix operations,,"I have a square matrix A. Is there a way I can apply operations like addition, subtraction, matrix multiplication, matrix inverse and transpose to get the diagonal of the matrix. For example having: $$\begin{pmatrix}1&2\\3&4\end{pmatrix}$$ I would like to get $(1,4)$. P.S. based on the conversation with mvw, here is a better description: I am on board of an alien space ship and the board computer allows only matrix operations but access to the individual matrix elements is blocked. I can only use addition, subtraction, matrix multiplication, matrix inverse and transpose. No access to individual row/column/element. I can only create matrices of any dimension $(1 x n)$, $(n x 1)$, $(n x 2n)$ that have all zeros or all ones. Is there a way for me to get a diagonal vector?","I have a square matrix A. Is there a way I can apply operations like addition, subtraction, matrix multiplication, matrix inverse and transpose to get the diagonal of the matrix. For example having: $$\begin{pmatrix}1&2\\3&4\end{pmatrix}$$ I would like to get $(1,4)$. P.S. based on the conversation with mvw, here is a better description: I am on board of an alien space ship and the board computer allows only matrix operations but access to the individual matrix elements is blocked. I can only use addition, subtraction, matrix multiplication, matrix inverse and transpose. No access to individual row/column/element. I can only create matrices of any dimension $(1 x n)$, $(n x 1)$, $(n x 2n)$ that have all zeros or all ones. Is there a way for me to get a diagonal vector?",,['matrices']
63,Intersection of conics using matrix representation [closed],Intersection of conics using matrix representation [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I came across a very interesting section of a wikipedia article on conics: http://en.wikipedia.org/wiki/Conic_section#Intersecting_two_conics I am trying to work out a couple of examples to add to the page. Example 1 - (this one turns out to be quite straight forward - see Example 2 below for a more general case): Take these two hyperbolas, Q1 and Q2: Q1:  $$x^2 - y^2 - 2 = 0$$ and Q2:  $$.5x^2 - y^2 - 1 = 0$$ Their graphs which show that they have 2 real intersections are here: http://www.wolframalpha.com/input/?i=plot%28x^2+-+y^2+-+2+%3D+0+and+.5x^2+-+y^2+-+1+%3D+0%29 First, we construct their matrices: $$Q_1 = \begin{bmatrix}1 & 0 & 0\\0 & -1 & 0\\0 & 0 & -2\end{bmatrix}$$ $$Q_2 = \begin{bmatrix}1/2 & 0 & 0\\0 & -1 & 0\\0 & 0 & -1\end{bmatrix}$$ Then, we set $$det(\lambda Q_1 + \mu Q_2) = 0.$$ Expanding this determinant, we get the equation: $$(\lambda + \frac{\mu}{2})(-\lambda-\mu)(-2\lambda-\mu) = 0$$ Here, each of the 3 factors can be independently set equal to zero to find the 3 solutions: $$\mu = -2\lambda,$$ $$\mu = -\lambda,$$ and $$\mu = -2\lambda,$$ respectively. Alternatively, we could immediately set $\lambda=1$ and solve $\mu^3/2+(5 \mu^2)/2+4 \mu+2$ algorithmically. There three degenerate conics? I.e. if we take one of the solutions ($\mu = -\lambda$), and use it to compute a particular linear combination (plugging into the linear combination equation $\lambda C_1 + \mu C_2$), we get: $$\lambda C_1 -\lambda C_2$$ which is still an expression with an unknown? Does it mean we can choose any $\lambda$ to obtain a degenerate conic? That is, set $\lambda=1$ to get: $$C_0 = \begin{bmatrix}.5 & 0 & 0\\0 & 0 & 0\\0 & 0 & -1\end{bmatrix}$$ Now we identify two coincident lines constituting this degenerate conic. We set $x^T C_0 x$ equal to zero, and we get $$.5x^2 - 1=0.$$ We see that $$x=\pm \sqrt{2}$$ (a pair of vertical lines). Intersecting these lines with the conics will give the intersection points of the original conics. Here, since the lines are vertical we can simply substitute the values $$x=\sqrt{2}$$ and $$x=-\sqrt{2}$$ to obtain the intersections (there are only 2 in this problem, but there can be up to 4): From Q_1: $$(\sqrt{2})^2 - y^2 - 2 = 0 \rightarrow y=0$$ $$(-\sqrt{2})^2 - y^2 - 2 = 0 \rightarrow y=0$$ we get intersection points $$(\sqrt{2}, 0)$$ and $$(-\sqrt{2},0).$$ From Q_2:  $$.5(\sqrt{2})^2 - y^2 - 1 = 0 \rightarrow y=0$$ $$.5(-\sqrt{2})^2 - y^2 - 1 = 0 \rightarrow y=0$$ we get intersection points $$(\sqrt{2}, 0)$$ and $$(-\sqrt{2},0).$$ Example #2 Take these two hyperbolas, Q1 and Q2: Q1:  $$.5x^2 - y^2 + .1xy + 1 = 0$$ and Q2:  $$-x^2 + y^2 + 1 = 0$$ Their graphs which show that they have 4 real intersections are here: First, we construct their matrices: $$Q_1 = \begin{bmatrix}.5 & .05 & 0\\.05 & -1 & 0\\0 & 0 & 1\end{bmatrix}$$ $$Q_2 = \begin{bmatrix}-1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}$$ Then, we set $$det(\lambda Q_1 + \mu Q_2) = 0.$$ Expanding this determinant, we get the equation: $$-.5\lambda^3 + \lambda^2\mu +.5\lambda\mu^2-\mu^3 - .0025\lambda^3 - .0025\lambda^2\mu.$$ Arbitrarily setting $\lambda=1$ and expanding, we get the equation $$-\mu^3+.5\mu^2+.9975\mu-.5025=0.$$ The 3 (approximate) solutions are: $$\mu = -1,$$ $$\mu = .505,$$ and $$\mu = .99495,$$ respectively. Going back and constructing the degenerate conic matrix (with $\lambda=1$ and $\mu=-1$), we have (Can you just pick one of the 3 solutions that comes from the determinant equation as I did?) $$C_0 = \begin{bmatrix}1.5 & .05 & 0\\.05 & -2 & 0\\0 & 0 & 0\end{bmatrix}$$ Now $$x^T C_0 x = 0 = $$ we get $$1.5x^2 + .1xy-2y^2 = 0.$$ Solving for $y$, we see that the two lines are $$y=−0.841386x$$ and $$y=0.891386x.$$ These lines intersect the conics at exactly the intersection points of the conics, as seen here: http://www.wolframalpha.com/input/?i=plot%28.5x^2-y^2%2B.1xy%2B1%3D0+and+-x^2%2By^2%2B1%3D0+and+y+%3D+%E2%88%920.841386x+and+y%3D0.891386x%29 The intersection points can be found by substituting the equation of both lines in to the equation of both conics. For example, substituting $$y=−0.841386x$$ into $$.5x^2 - y^2 + .1xy + 1 = 0$$ we get $$x=1.8505$$ Plugging this into the equation of the line $y=−0.841386x$, we see that one intersection point is $(1.8505, -1.5569)$. The other 3 intersection points can be found identically.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I came across a very interesting section of a wikipedia article on conics: http://en.wikipedia.org/wiki/Conic_section#Intersecting_two_conics I am trying to work out a couple of examples to add to the page. Example 1 - (this one turns out to be quite straight forward - see Example 2 below for a more general case): Take these two hyperbolas, Q1 and Q2: Q1:  $$x^2 - y^2 - 2 = 0$$ and Q2:  $$.5x^2 - y^2 - 1 = 0$$ Their graphs which show that they have 2 real intersections are here: http://www.wolframalpha.com/input/?i=plot%28x^2+-+y^2+-+2+%3D+0+and+.5x^2+-+y^2+-+1+%3D+0%29 First, we construct their matrices: $$Q_1 = \begin{bmatrix}1 & 0 & 0\\0 & -1 & 0\\0 & 0 & -2\end{bmatrix}$$ $$Q_2 = \begin{bmatrix}1/2 & 0 & 0\\0 & -1 & 0\\0 & 0 & -1\end{bmatrix}$$ Then, we set $$det(\lambda Q_1 + \mu Q_2) = 0.$$ Expanding this determinant, we get the equation: $$(\lambda + \frac{\mu}{2})(-\lambda-\mu)(-2\lambda-\mu) = 0$$ Here, each of the 3 factors can be independently set equal to zero to find the 3 solutions: $$\mu = -2\lambda,$$ $$\mu = -\lambda,$$ and $$\mu = -2\lambda,$$ respectively. Alternatively, we could immediately set $\lambda=1$ and solve $\mu^3/2+(5 \mu^2)/2+4 \mu+2$ algorithmically. There three degenerate conics? I.e. if we take one of the solutions ($\mu = -\lambda$), and use it to compute a particular linear combination (plugging into the linear combination equation $\lambda C_1 + \mu C_2$), we get: $$\lambda C_1 -\lambda C_2$$ which is still an expression with an unknown? Does it mean we can choose any $\lambda$ to obtain a degenerate conic? That is, set $\lambda=1$ to get: $$C_0 = \begin{bmatrix}.5 & 0 & 0\\0 & 0 & 0\\0 & 0 & -1\end{bmatrix}$$ Now we identify two coincident lines constituting this degenerate conic. We set $x^T C_0 x$ equal to zero, and we get $$.5x^2 - 1=0.$$ We see that $$x=\pm \sqrt{2}$$ (a pair of vertical lines). Intersecting these lines with the conics will give the intersection points of the original conics. Here, since the lines are vertical we can simply substitute the values $$x=\sqrt{2}$$ and $$x=-\sqrt{2}$$ to obtain the intersections (there are only 2 in this problem, but there can be up to 4): From Q_1: $$(\sqrt{2})^2 - y^2 - 2 = 0 \rightarrow y=0$$ $$(-\sqrt{2})^2 - y^2 - 2 = 0 \rightarrow y=0$$ we get intersection points $$(\sqrt{2}, 0)$$ and $$(-\sqrt{2},0).$$ From Q_2:  $$.5(\sqrt{2})^2 - y^2 - 1 = 0 \rightarrow y=0$$ $$.5(-\sqrt{2})^2 - y^2 - 1 = 0 \rightarrow y=0$$ we get intersection points $$(\sqrt{2}, 0)$$ and $$(-\sqrt{2},0).$$ Example #2 Take these two hyperbolas, Q1 and Q2: Q1:  $$.5x^2 - y^2 + .1xy + 1 = 0$$ and Q2:  $$-x^2 + y^2 + 1 = 0$$ Their graphs which show that they have 4 real intersections are here: First, we construct their matrices: $$Q_1 = \begin{bmatrix}.5 & .05 & 0\\.05 & -1 & 0\\0 & 0 & 1\end{bmatrix}$$ $$Q_2 = \begin{bmatrix}-1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}$$ Then, we set $$det(\lambda Q_1 + \mu Q_2) = 0.$$ Expanding this determinant, we get the equation: $$-.5\lambda^3 + \lambda^2\mu +.5\lambda\mu^2-\mu^3 - .0025\lambda^3 - .0025\lambda^2\mu.$$ Arbitrarily setting $\lambda=1$ and expanding, we get the equation $$-\mu^3+.5\mu^2+.9975\mu-.5025=0.$$ The 3 (approximate) solutions are: $$\mu = -1,$$ $$\mu = .505,$$ and $$\mu = .99495,$$ respectively. Going back and constructing the degenerate conic matrix (with $\lambda=1$ and $\mu=-1$), we have (Can you just pick one of the 3 solutions that comes from the determinant equation as I did?) $$C_0 = \begin{bmatrix}1.5 & .05 & 0\\.05 & -2 & 0\\0 & 0 & 0\end{bmatrix}$$ Now $$x^T C_0 x = 0 = $$ we get $$1.5x^2 + .1xy-2y^2 = 0.$$ Solving for $y$, we see that the two lines are $$y=−0.841386x$$ and $$y=0.891386x.$$ These lines intersect the conics at exactly the intersection points of the conics, as seen here: http://www.wolframalpha.com/input/?i=plot%28.5x^2-y^2%2B.1xy%2B1%3D0+and+-x^2%2By^2%2B1%3D0+and+y+%3D+%E2%88%920.841386x+and+y%3D0.891386x%29 The intersection points can be found by substituting the equation of both lines in to the equation of both conics. For example, substituting $$y=−0.841386x$$ into $$.5x^2 - y^2 + .1xy + 1 = 0$$ we get $$x=1.8505$$ Plugging this into the equation of the line $y=−0.841386x$, we see that one intersection point is $(1.8505, -1.5569)$. The other 3 intersection points can be found identically.",,"['matrices', 'conic-sections']"
64,Minimal polynomial of diagonalizable matrix,Minimal polynomial of diagonalizable matrix,,Prove that a matrix $A$ over $\mathbb{C}$ is diagonalizable if and only if its minimal polynomial's roots are all of algebraic multiplicity one.,Prove that a matrix $A$ over $\mathbb{C}$ is diagonalizable if and only if its minimal polynomial's roots are all of algebraic multiplicity one.,,"['matrices', 'diagonalization', 'minimal-polynomials']"
65,Dot product with vector and its transpose?,Dot product with vector and its transpose?,,I'm having trouble with the statement: $$||\textbf{v}||^2=\textbf{v}\cdot\textbf{v}=\textbf{v}^T\textbf{v}$$ taking $\textbf{v}$ as a column vector in an orthogonal matrix. How can you do the dot product of a vector and its transpose? Surely that would be like dotting a $n\times1$ matrix with a $1\times n$ matrix? I thought the dot product was only defined for $n\times 1$ vectors?,I'm having trouble with the statement: $$||\textbf{v}||^2=\textbf{v}\cdot\textbf{v}=\textbf{v}^T\textbf{v}$$ taking $\textbf{v}$ as a column vector in an orthogonal matrix. How can you do the dot product of a vector and its transpose? Surely that would be like dotting a $n\times1$ matrix with a $1\times n$ matrix? I thought the dot product was only defined for $n\times 1$ vectors?,,"['matrices', 'orthonormal']"
66,Every invertible matrix can be written as the exponential of one other matrix,Every invertible matrix can be written as the exponential of one other matrix,,"I'm looking for a proof of this claim: ""every invertible matrix can be written as the exponential of another matrix"".  I'm not familiar yet with logarithms of matrices, so I wonder if a proof exists, without them. I'll be happy with any proof anyways. I hope someone can help.","I'm looking for a proof of this claim: ""every invertible matrix can be written as the exponential of another matrix"".  I'm not familiar yet with logarithms of matrices, so I wonder if a proof exists, without them. I'll be happy with any proof anyways. I hope someone can help.",,['matrices']
67,Entropy of matrix,Entropy of matrix,,I am trying to understand entropy. From what I know we can get the entropy of a variable lets say X. What i dont understand is how to calculate the entropy of a matrix say m*n. I thought if columns are the attributes and rows are object we can sum the entropy of individual columns to get the final entropy(provided attributes are independent). I have couple of question IS my understanding right in case of independent attributes? What if the attributes are dependent? what happens to entropy? Is there where conditional entropy comes in? Thanks,I am trying to understand entropy. From what I know we can get the entropy of a variable lets say X. What i dont understand is how to calculate the entropy of a matrix say m*n. I thought if columns are the attributes and rows are object we can sum the entropy of individual columns to get the final entropy(provided attributes are independent). I have couple of question IS my understanding right in case of independent attributes? What if the attributes are dependent? what happens to entropy? Is there where conditional entropy comes in? Thanks,,"['matrices', 'entropy']"
68,Is there a symbol for matrix multiplication operator?,Is there a symbol for matrix multiplication operator?,,Title says it all. Is there any specific operator symbol for matrix multiplication? Not just write down side by side but symbols like cross ($\times$).,Title says it all. Is there any specific operator symbol for matrix multiplication? Not just write down side by side but symbols like cross ($\times$).,,"['matrices', 'notation']"
69,History of the matrix representation of complex numbers,History of the matrix representation of complex numbers,,"It is well-known to many that $\mathbb{C}$ can be represented by matrices of the form $$\left[ \begin{array}{cc} a & b \\ -b & a \end{array} \right]$$ For example, see this question or this question . It is also discussed in the wikipedia article history of complex numbers article . Apparently, there is even an introductory complex variable textbook by Copson from 1935 which uses such matrices to define complex numbers. This is mentioned in Numbers by Ebbinghaus et. al. on page 69. My question is simply this: What is the history of this construction? Who first explained that complex numbers could be viewed as $2 \times 2$ matrices of the special form $\left[ \begin{array}{cc} a & b \\ -b & a \end{array} \right]$ ? I realize this is just the regular representation of $\mathbb{C}$ , and I realize such matrices are the matrices of a dilation composed with a rotation and possibly a reflection, but, the question still remains, who did found these first? References are appreciated.","It is well-known to many that can be represented by matrices of the form For example, see this question or this question . It is also discussed in the wikipedia article history of complex numbers article . Apparently, there is even an introductory complex variable textbook by Copson from 1935 which uses such matrices to define complex numbers. This is mentioned in Numbers by Ebbinghaus et. al. on page 69. My question is simply this: What is the history of this construction? Who first explained that complex numbers could be viewed as matrices of the special form ? I realize this is just the regular representation of , and I realize such matrices are the matrices of a dilation composed with a rotation and possibly a reflection, but, the question still remains, who did found these first? References are appreciated.",\mathbb{C} \left[ \begin{array}{cc} a & b \\ -b & a \end{array} \right] 2 \times 2 \left[ \begin{array}{cc} a & b \\ -b & a \end{array} \right] \mathbb{C},"['matrices', 'reference-request', 'complex-numbers', 'representation-theory', 'math-history']"
70,Additivity of the matrix exponential of infinite matrices,Additivity of the matrix exponential of infinite matrices,,"It is well known that the matrix exponential of finite dimensional matrices is additive if the exponents commute: $AB=BA\implies e^Ae^B=e^{A+B}$ (cf. e.g. Bernstein , Corollary 11.1.6). Under what circumstances does it carry over to the infinite dimensional case? I am particularly interested in the case when the matrices are stochastic. Definitions Let $A=(A_{i,j})_{i,j\in\mathbb{N}_0}$, $B=(B_{i,j})_{i,j\in\mathbb{N}_0}$ be real valued infinite dimensional matrices. Scalar product, sum and product of infinite matrices are again infinite matrices that are defined component-wise as follows $$\begin{align}rA&:=(rA_{i,j})_{i,j\in\mathbb{N}_0}\\A+B&:=(A_{i,j}+B_{i,j})_{i,j\in\mathbb{N}_0}\\AB&:=(\sum_{k=0}^\infty A_{i,k}B_{k,j})_{i,j\in\mathbb{N}_0}\end{align}$$ Unlike the scalar product and the sum, the product may not be defined for some matrices. It is defined provided all the infinite series on the right hand side converge to a real number. The limit of an infinite sequence of infinite, real-valued matrices $(C^{(n)})_{n\in\mathbb{N}_0}$ as well as the sum thereof are again infinite matrices that are defined component-wise as follows $$\begin{align}\lim_{n\rightarrow\infty}C^{(n)}&:=(\lim_{n\rightarrow}C_{i,j}^{(n)})_{i,j\in\mathbb{N}_0}\\\sum_{n=0}^\infty C^{(n)}&:=(\sum_{n=0}^\infty C_{i,j}^{(n)})_{i,j\in\mathbb{N}_0}\end{align}$$ provided all the limits/sums on the right hand side converge to real numbers. We define $$\begin{align}0&:=(0)_{i,j\in\mathbb{N}_0}\\I&:=(\delta_{i,j})_{i,j\in\mathbb{N}_0}\end{align}$$ where $\delta_{i,j}$ is Kronecker's Delta. The powers of $A$ are defined recursively thus (for $n\in\mathbb{N}_0$) $$\begin{align}A^0&:=I\space\space\mathrm{(even\, if\, }A=0\mathrm{\, )}\\A^{n+1}&:=AA^n\end{align}$$ Not all powers of all infinite matrices may be defined for the same reason the product of some pairs of infinite matrices may not be defined. The exponential of $A$ is defined as $$e^A:=\sum_{n=0}^\infty \frac{1}{n!} A^n$$ provided all the powers on the right hand side are defined and that the sum converges. Question Suppose $e^A$, $e^B$ converge and their product is well-defined. Under what circumstances does $e^{A+B}$ converge and $$e^{A+B}=e^Ae^B$$ A Special Case of Interest If $A$ is a stochastic matrix (i.e. all components are non-negative and each row sums to $1$) and $s,t\in\mathbb{R}$, under what conditions does it hold that $e^{sI}$, $e^{tA}$, $e^{sI}e^{tA}$ and $e^{sI+tA}$ converge and $$e^{sI}e^{tA}=e^{sI+tA}$$","It is well known that the matrix exponential of finite dimensional matrices is additive if the exponents commute: $AB=BA\implies e^Ae^B=e^{A+B}$ (cf. e.g. Bernstein , Corollary 11.1.6). Under what circumstances does it carry over to the infinite dimensional case? I am particularly interested in the case when the matrices are stochastic. Definitions Let $A=(A_{i,j})_{i,j\in\mathbb{N}_0}$, $B=(B_{i,j})_{i,j\in\mathbb{N}_0}$ be real valued infinite dimensional matrices. Scalar product, sum and product of infinite matrices are again infinite matrices that are defined component-wise as follows $$\begin{align}rA&:=(rA_{i,j})_{i,j\in\mathbb{N}_0}\\A+B&:=(A_{i,j}+B_{i,j})_{i,j\in\mathbb{N}_0}\\AB&:=(\sum_{k=0}^\infty A_{i,k}B_{k,j})_{i,j\in\mathbb{N}_0}\end{align}$$ Unlike the scalar product and the sum, the product may not be defined for some matrices. It is defined provided all the infinite series on the right hand side converge to a real number. The limit of an infinite sequence of infinite, real-valued matrices $(C^{(n)})_{n\in\mathbb{N}_0}$ as well as the sum thereof are again infinite matrices that are defined component-wise as follows $$\begin{align}\lim_{n\rightarrow\infty}C^{(n)}&:=(\lim_{n\rightarrow}C_{i,j}^{(n)})_{i,j\in\mathbb{N}_0}\\\sum_{n=0}^\infty C^{(n)}&:=(\sum_{n=0}^\infty C_{i,j}^{(n)})_{i,j\in\mathbb{N}_0}\end{align}$$ provided all the limits/sums on the right hand side converge to real numbers. We define $$\begin{align}0&:=(0)_{i,j\in\mathbb{N}_0}\\I&:=(\delta_{i,j})_{i,j\in\mathbb{N}_0}\end{align}$$ where $\delta_{i,j}$ is Kronecker's Delta. The powers of $A$ are defined recursively thus (for $n\in\mathbb{N}_0$) $$\begin{align}A^0&:=I\space\space\mathrm{(even\, if\, }A=0\mathrm{\, )}\\A^{n+1}&:=AA^n\end{align}$$ Not all powers of all infinite matrices may be defined for the same reason the product of some pairs of infinite matrices may not be defined. The exponential of $A$ is defined as $$e^A:=\sum_{n=0}^\infty \frac{1}{n!} A^n$$ provided all the powers on the right hand side are defined and that the sum converges. Question Suppose $e^A$, $e^B$ converge and their product is well-defined. Under what circumstances does $e^{A+B}$ converge and $$e^{A+B}=e^Ae^B$$ A Special Case of Interest If $A$ is a stochastic matrix (i.e. all components are non-negative and each row sums to $1$) and $s,t\in\mathbb{R}$, under what conditions does it hold that $e^{sI}$, $e^{tA}$, $e^{sI}e^{tA}$ and $e^{sI+tA}$ converge and $$e^{sI}e^{tA}=e^{sI+tA}$$",,['matrices']
71,Rearranging a $10 \times 10$ matrix of naturals $1\le n\le 100$ s.t. the sum of every two neighbouring numbers is composite in max. 35 steps,Rearranging a  matrix of naturals  s.t. the sum of every two neighbouring numbers is composite in max. 35 steps,10 \times 10 1\le n\le 100,"We are given a $10 \times 10$ matrix which contains every natural number between $1$ and $100$ in arbitrary order. We are to prove that it is always possible to rearrange the matrix by swapping any two entries of choice in at most 35 steps, such that in the resulting matrix every two neighbouring (horizontally or vertically, not diagonally) entries' sum is composite. Naturally, one should begin with showing that such an arrangement is possible in the first place. A very simple example that sprang to mind is constructing the matrix s.t. the first 50 entries are occupied by odd numbers and the last 50 by even numbers. Then, the sum of every two odd entries is even and thus composite and the sum of every two even entries is also even and thus composite. We then need to ensure that the rows in the middle contain even-odd pairs that give composite numers, e.g. $2+7,5+10,12+9,14+11,24+3,20+13,18+17,16+23,37+8,43+6$ . It is therefore possible to construct such a matrix. However, I had some trouble proving that this can be achived by transforming any matrix in maximum 35 swappings. Can you transform any matrix to the configuration I have described in maximum 35 steps or do you need to construct a different configuration within the limits for that? Does one actually need to demonstrate the procedure or can we prove this indirectly? Thank you for your help.","We are given a matrix which contains every natural number between and in arbitrary order. We are to prove that it is always possible to rearrange the matrix by swapping any two entries of choice in at most 35 steps, such that in the resulting matrix every two neighbouring (horizontally or vertically, not diagonally) entries' sum is composite. Naturally, one should begin with showing that such an arrangement is possible in the first place. A very simple example that sprang to mind is constructing the matrix s.t. the first 50 entries are occupied by odd numbers and the last 50 by even numbers. Then, the sum of every two odd entries is even and thus composite and the sum of every two even entries is also even and thus composite. We then need to ensure that the rows in the middle contain even-odd pairs that give composite numers, e.g. . It is therefore possible to construct such a matrix. However, I had some trouble proving that this can be achived by transforming any matrix in maximum 35 swappings. Can you transform any matrix to the configuration I have described in maximum 35 steps or do you need to construct a different configuration within the limits for that? Does one actually need to demonstrate the procedure or can we prove this indirectly? Thank you for your help.","10 \times 10 1 100 2+7,5+10,12+9,14+11,24+3,20+13,18+17,16+23,37+8,43+6","['matrices', 'elementary-number-theory', 'parity']"
72,Numbers defined with matrices,Numbers defined with matrices,,"We  know that a complex number, written as $c=(a,b)$, can be expressed with the help of a matrix as  $$\begin{bmatrix}a & -b\\ b & a\end{bmatrix}$$ and operations on such matrices resemble operations on complex numbers. However with $2 \times 2$ matrices we could imagine a definition of another type ""number"" $x=(a,b)$, for example $$(a,b) \longleftrightarrow \begin{bmatrix}a & b\\b & a\end{bmatrix}.$$ Here the operations are quite well defined - multiplication and addition are commutative - the only difference to the complex numbers it seems is that not all numbers have their inverses -  for example for $(a,a)$ or $(a,-a)$ it's hard to say what is its inverse. Why  don't we use such ""numbers""? Are they numbers at all? When can we say that a given matrix represents number ? The same is true for $ 4 \times 4$ matrices ... it seems only one way of defining numbers - known as quaternions - has found  its way into the numbers world... (even though the number of possible ways for constructing matrices with $4$ values when every value is repeated in the matrix $4$ times  is much greater).","We  know that a complex number, written as $c=(a,b)$, can be expressed with the help of a matrix as  $$\begin{bmatrix}a & -b\\ b & a\end{bmatrix}$$ and operations on such matrices resemble operations on complex numbers. However with $2 \times 2$ matrices we could imagine a definition of another type ""number"" $x=(a,b)$, for example $$(a,b) \longleftrightarrow \begin{bmatrix}a & b\\b & a\end{bmatrix}.$$ Here the operations are quite well defined - multiplication and addition are commutative - the only difference to the complex numbers it seems is that not all numbers have their inverses -  for example for $(a,a)$ or $(a,-a)$ it's hard to say what is its inverse. Why  don't we use such ""numbers""? Are they numbers at all? When can we say that a given matrix represents number ? The same is true for $ 4 \times 4$ matrices ... it seems only one way of defining numbers - known as quaternions - has found  its way into the numbers world... (even though the number of possible ways for constructing matrices with $4$ values when every value is repeated in the matrix $4$ times  is much greater).",,"['matrices', 'complex-numbers']"
73,Prove $BA - A^2B^2 = I_n$.,Prove .,BA - A^2B^2 = I_n,"I have a problem with this. Actually, still don't have the right way to start :/ Problem : Let $A$ and $B$ be $n \times n$ complex matrices such that $AB - B^2A^2 = I_n$. Prove that if $A^3 + B^3 = 0$, then $BA - A^2B^2 = I_n$. Thanks for any help.","I have a problem with this. Actually, still don't have the right way to start :/ Problem : Let $A$ and $B$ be $n \times n$ complex matrices such that $AB - B^2A^2 = I_n$. Prove that if $A^3 + B^3 = 0$, then $BA - A^2B^2 = I_n$. Thanks for any help.",,['matrices']
74,Derivative of vector and vector transpose product,Derivative of vector and vector transpose product,,"I saw this answer here : Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$ . I am finding difficult to understand the part in red. What rule is that ? If I apply multiplication rule, shouldn't I get - And how do one differentiate this anyways ?","I saw this answer here : Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$ . I am finding difficult to understand the part in red. What rule is that ? If I apply multiplication rule, shouldn't I get - And how do one differentiate this anyways ?",,"['matrices', 'derivatives', 'vectors', 'partial-derivative']"
75,How to calculate matrix raised to a high power.,How to calculate matrix raised to a high power.,,"How would I go about calculating: $ \left( \begin{array}{cc} 3 & -\sqrt{3} \\ \sqrt{3} & -1 \end{array} \right)^{13}$ I have already attempted to find eigenvalues/eigenvectors, but I believe I am missing a much simpler method.","How would I go about calculating: $ \left( \begin{array}{cc} 3 & -\sqrt{3} \\ \sqrt{3} & -1 \end{array} \right)^{13}$ I have already attempted to find eigenvalues/eigenvectors, but I believe I am missing a much simpler method.",,['matrices']
76,Inversion of rotation matrix,Inversion of rotation matrix,,"For example, I have a two-dimensional rotation matrix $$   \begin{bmatrix}     0.5091 &           -0.8607 \\     0.8607 & \phantom{-}0.5091   \end{bmatrix} $$ and I have a vector I'd like to rotate, e.g. $(1, -0.5)$. My problem is to find an inverse of the rotation matrix so that I can later “undo” the rotation performed on the vector so that I get back the original vector. The rotation matrix is not parametric, created via eigendecomposition, I can't use angles to easily create an inverse matrix.","For example, I have a two-dimensional rotation matrix $$   \begin{bmatrix}     0.5091 &           -0.8607 \\     0.8607 & \phantom{-}0.5091   \end{bmatrix} $$ and I have a vector I'd like to rotate, e.g. $(1, -0.5)$. My problem is to find an inverse of the rotation matrix so that I can later “undo” the rotation performed on the vector so that I get back the original vector. The rotation matrix is not parametric, created via eigendecomposition, I can't use angles to easily create an inverse matrix.",,"['matrices', 'rotations']"
77,Prove $(A^T)^{-1}$ = $(A^{-1})^T$ whenever $A$ is invertible.,Prove  =  whenever  is invertible.,(A^T)^{-1} (A^{-1})^T A,Prove $(A^T)^{-1}$ = $(A^{-1})^T$ for any invertible matrix $A.$ I actually don't know where to start. I do not think I can just apply index laws. Any help is cool! Thanks.,Prove = for any invertible matrix I actually don't know where to start. I do not think I can just apply index laws. Any help is cool! Thanks.,(A^T)^{-1} (A^{-1})^T A.,['matrices']
78,Why are skew-symmetric matrices of interest?,Why are skew-symmetric matrices of interest?,,"I am currently following a course on nonlinear algebra (topics include varieties, elimination, linear spaces, grassmannians etc.). Especially in the exercises we work a lot with skew-symmetric matrices, however, I do not yet understand why they are of such importance. So my question is: How do skew-symmetric matrices tie in with the topics mentioned above, and also, where else in mathematics would we be interested in them and why?","I am currently following a course on nonlinear algebra (topics include varieties, elimination, linear spaces, grassmannians etc.). Especially in the exercises we work a lot with skew-symmetric matrices, however, I do not yet understand why they are of such importance. So my question is: How do skew-symmetric matrices tie in with the topics mentioned above, and also, where else in mathematics would we be interested in them and why?",,"['matrices', 'algebraic-geometry', 'reference-request', 'soft-question', 'skew-symmetric-matrices']"
79,How to calculate the degrees of freedom of an $r$-ranked matrix with the size being $n\times n$?,How to calculate the degrees of freedom of an -ranked matrix with the size being ?,r n\times n,"Treat matrices as vectors lying in $\mathbb{R}^{n^2}$. It can be imagined matrices with rank $r (r<n)$ are supposed to lie within a manifold of lower dimension. For example, the singular matrices lie within a $(n^2−1)$-dimensional manifold, because they satisfy $\det(M)=0$, the sole constraint. Typically, the number of independent constraints is equal to the difference between those two dimensions. Alternatively, we call the dimension of that lower-dimensional manifold as ""degrees of freedom"". Now how do we calculate the degrees of freedom of an $r$-ranked matrix with size $n\times n$? The answer says the degrees of freedom is $n^2-(n-r)^2=(2n-r)r$. I try to interpret it as follows: First, by elementary matrices, every matrix $M$ with the rank of $r$ can be transformed into $$M\sim\begin{pmatrix} I_r&0\\ 0&0 \end{pmatrix}_{n\times n}$$ Now the constraints come from the block at bottom right, where the entries are suppressed to be zero. So there are $(n-r)^2$ constraints, which lead to the answer (why the number of constraints do not agree with the number of zeroes is because they are not all independent, intuitively). The explanation is not formal at all. Can anyone provide a refined version? Thank you~ EDIT : Provide further explanation of ""degrees of freedom""","Treat matrices as vectors lying in $\mathbb{R}^{n^2}$. It can be imagined matrices with rank $r (r<n)$ are supposed to lie within a manifold of lower dimension. For example, the singular matrices lie within a $(n^2−1)$-dimensional manifold, because they satisfy $\det(M)=0$, the sole constraint. Typically, the number of independent constraints is equal to the difference between those two dimensions. Alternatively, we call the dimension of that lower-dimensional manifold as ""degrees of freedom"". Now how do we calculate the degrees of freedom of an $r$-ranked matrix with size $n\times n$? The answer says the degrees of freedom is $n^2-(n-r)^2=(2n-r)r$. I try to interpret it as follows: First, by elementary matrices, every matrix $M$ with the rank of $r$ can be transformed into $$M\sim\begin{pmatrix} I_r&0\\ 0&0 \end{pmatrix}_{n\times n}$$ Now the constraints come from the block at bottom right, where the entries are suppressed to be zero. So there are $(n-r)^2$ constraints, which lead to the answer (why the number of constraints do not agree with the number of zeroes is because they are not all independent, intuitively). The explanation is not formal at all. Can anyone provide a refined version? Thank you~ EDIT : Provide further explanation of ""degrees of freedom""",,['matrices']
80,When is inverting a matrix numerically unstable?,When is inverting a matrix numerically unstable?,,What does numerically unstable mean when inverting a matrix and what are the mathematical conditions that cause this problem to arise?,What does numerically unstable mean when inverting a matrix and what are the mathematical conditions that cause this problem to arise?,,['matrices']
81,Is taking the inverse of a matrix a convex operation?,Is taking the inverse of a matrix a convex operation?,,"Let $\mathbf{X,Y}$ be two positive definite matrices. Can we obtain the following Jensen-like inequality $$(1-\lambda)\mathbf{X}^{-1}+\lambda\mathbf{Y}^{-1} \succeq((1-\lambda)\mathbf{X}+\lambda\mathbf{Y})^{-1}$$ for any $\lambda \in (0,1)$ , where the notation $\mathbf{A}\succeq \mathbf{B}$ represents matrix $(\mathbf{A-B})$ is positive semi-definite?","Let be two positive definite matrices. Can we obtain the following Jensen-like inequality for any , where the notation represents matrix is positive semi-definite?","\mathbf{X,Y} (1-\lambda)\mathbf{X}^{-1}+\lambda\mathbf{Y}^{-1} \succeq((1-\lambda)\mathbf{X}+\lambda\mathbf{Y})^{-1} \lambda \in (0,1) \mathbf{A}\succeq \mathbf{B} (\mathbf{A-B})","['matrices', 'inequality', 'convex-optimization', 'inverse', 'positive-definite']"
82,"Exponential lower bound for the determiant of a (0,1)-matrix","Exponential lower bound for the determiant of a (0,1)-matrix",,"Give matrices, which only contain 0 and 1, and their determinant grows exponentially. In other words, show an $n \times n$ matrix for all n, which only contains 0 and 1, and  $$\det A(n)>d \cdot c^n,$$ where c>1 and d>0. Can't really begin, any ideas? Thanks :)","Give matrices, which only contain 0 and 1, and their determinant grows exponentially. In other words, show an $n \times n$ matrix for all n, which only contains 0 and 1, and  $$\det A(n)>d \cdot c^n,$$ where c>1 and d>0. Can't really begin, any ideas? Thanks :)",,['matrices']
83,What kind of matrix is it that when multiplied with its transpose produces the identity?,What kind of matrix is it that when multiplied with its transpose produces the identity?,,"If $A^TA = I$, where $A$ is a lower triangular matrix, does that mean $A$ has to be an identity matrix (and nothing else)? In general, which kind of matrix $A$ must be for that equality to hold?","If $A^TA = I$, where $A$ is a lower triangular matrix, does that mean $A$ has to be an identity matrix (and nothing else)? In general, which kind of matrix $A$ must be for that equality to hold?",,['matrices']
84,Norm of a Matrix-vector product,Norm of a Matrix-vector product,,Suppose I have vector $\vec x \in \mathbb R^n$ and matrix $\mathbf M$ of dimension $m\times n$. Is there an alternative expression for $\lVert \mathbf M \cdot \vec x \lVert$ that includes $\lVert \vec x \lVert$?,Suppose I have vector $\vec x \in \mathbb R^n$ and matrix $\mathbf M$ of dimension $m\times n$. Is there an alternative expression for $\lVert \mathbf M \cdot \vec x \lVert$ that includes $\lVert \vec x \lVert$?,,"['matrices', 'normed-spaces', 'products', 'vectors']"
85,Derivative of product of matrix by vector,Derivative of product of matrix by vector,,"Let $\boldsymbol{\beta} := (\beta_1, \ldots, \beta_p)^T$ and $\boldsymbol{X}$ be a matrix of dimension $n \times p$. I'd like to compute the derivative $$\nabla_{\beta} \left( X \beta \right)$$","Let $\boldsymbol{\beta} := (\beta_1, \ldots, \beta_p)^T$ and $\boldsymbol{X}$ be a matrix of dimension $n \times p$. I'd like to compute the derivative $$\nabla_{\beta} \left( X \beta \right)$$",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'jacobian']"
86,$3D$ rotation matrix uniqueness,rotation matrix uniqueness,3D,Given a $3D$ rotation matrix $R$ in a basis $B$. Can we consider $R$ as being unique in $B$? Is there any other $3D$ rotation matrix $R'$ representing the same $3D$ rotation in $B$? How could I prove that? Note: I do not consider the rotation matrix $R'$ with inverted axis and angle as being the same as $R$.,Given a $3D$ rotation matrix $R$ in a basis $B$. Can we consider $R$ as being unique in $B$? Is there any other $3D$ rotation matrix $R'$ representing the same $3D$ rotation in $B$? How could I prove that? Note: I do not consider the rotation matrix $R'$ with inverted axis and angle as being the same as $R$.,,"['matrices', '3d', 'rotations']"
87,Why is the non-negative matrix factorization problem non-convex?,Why is the non-negative matrix factorization problem non-convex?,,"Supposing $\mathbf{X}\in\mathbb{R}_+^{m\times n}$, $\mathbf{Y}\in\mathbb{R}_+^{m\times r}$, $\mathbf{W}\in\mathbb{R}_+^{r\times n}$, the non-negative matrix factorization problem is defined as: $$\min_{\mathbf{Y},\mathbf{W}}\left\|\mathbf{X}-\mathbf{Y}\mathbf{W}\right\|_F^2$$ Why is this problem non-convex?","Supposing $\mathbf{X}\in\mathbb{R}_+^{m\times n}$, $\mathbf{Y}\in\mathbb{R}_+^{m\times r}$, $\mathbf{W}\in\mathbb{R}_+^{r\times n}$, the non-negative matrix factorization problem is defined as: $$\min_{\mathbf{Y},\mathbf{W}}\left\|\mathbf{X}-\mathbf{Y}\mathbf{W}\right\|_F^2$$ Why is this problem non-convex?",,"['matrices', 'optimization', 'convex-analysis', 'matrix-decomposition', 'non-convex-optimization']"
88,Generation of unimodular matrices with bounded elements,Generation of unimodular matrices with bounded elements,,"Does anybody know what is the algorithm for generating random unimodular matrices (integer matrices with determinant $\pm 1$) whose elements do not exceed a given bound? Such an algorithm is mentioned here , and the following reference is provided: Jürgen Hausen, Generating Problems in Linear Algebra , MapleTech , Vol. 1, No. 2, 1994. However, this paper seems to be no longer accessible online. If the algorithm is based on the Hermite normal form, how do they ensure that the elements of the generated matrix are bounded by a given positive integer? Many thanks in advance for any insights :-)","Does anybody know what is the algorithm for generating random unimodular matrices (integer matrices with determinant $\pm 1$) whose elements do not exceed a given bound? Such an algorithm is mentioned here , and the following reference is provided: Jürgen Hausen, Generating Problems in Linear Algebra , MapleTech , Vol. 1, No. 2, 1994. However, this paper seems to be no longer accessible online. If the algorithm is based on the Hermite normal form, how do they ensure that the elements of the generated matrix are bounded by a given positive integer? Many thanks in advance for any insights :-)",,"['matrices', 'random']"
89,Definition of positive definite operator in infinite dimensional space,Definition of positive definite operator in infinite dimensional space,,"Let $H$ be a real Hilbert space. The linear operator $A: H\rightarrow H$ is said to be positive definite iff there exists $M>0$ such that $$ {\rm (i)} \quad\langle Ax, x\rangle \geq M \|x\|^2, \quad \forall x\in H. $$ We have known that when $\text{dim}H<\infty$, the above inequality is equivalent to $$ {\rm (ii)}\quad \langle Ax, x\rangle > 0, \quad \forall x\in H\setminus\{0\}. $$ I would like to ask why people use (i) but do not use (ii) for the definition of positive definite operator in infinite dimensional  space. Thank you for all helping and comments.","Let $H$ be a real Hilbert space. The linear operator $A: H\rightarrow H$ is said to be positive definite iff there exists $M>0$ such that $$ {\rm (i)} \quad\langle Ax, x\rangle \geq M \|x\|^2, \quad \forall x\in H. $$ We have known that when $\text{dim}H<\infty$, the above inequality is equivalent to $$ {\rm (ii)}\quad \langle Ax, x\rangle > 0, \quad \forall x\in H\setminus\{0\}. $$ I would like to ask why people use (i) but do not use (ii) for the definition of positive definite operator in infinite dimensional  space. Thank you for all helping and comments.",,"['matrices', 'functional-analysis']"
90,Connection between SVD and Discrete Fourier Transform for Denoising,Connection between SVD and Discrete Fourier Transform for Denoising,,"Denoising signals (in particular, 2D arrays, such as images) can be done by removing the high frequency components of the discrete Fourier transform (which is related to convolution with a Gaussian kernel) or by removing the smallest singular values. I was wondering if there is a known, specific mathematical connection between  these two approaches. I've seen a little discussion on the topic here and here , but I didn't really glean much specifically except the mention of circulant matrices .","Denoising signals (in particular, 2D arrays, such as images) can be done by removing the high frequency components of the discrete Fourier transform (which is related to convolution with a Gaussian kernel) or by removing the smallest singular values. I was wondering if there is a known, specific mathematical connection between  these two approaches. I've seen a little discussion on the topic here and here , but I didn't really glean much specifically except the mention of circulant matrices .",,"['matrices', 'fourier-analysis', 'signal-processing', 'matrix-decomposition', 'fourier-transform']"
91,Cholesky decomposition of the inverse of a matrix,Cholesky decomposition of the inverse of a matrix,,"I have the Cholesky decomposition of a matrix $M$. However, I need the Cholesky decomposition of the inverse of the matrix, $M^{-1}$. Is there a fast way to do this, without first computing $M^{-1}$? In other words, is there a relationship between the Cholesky decompositions of a matrix and of its inverse? My matrix is a covariance matrix and, hence, positive-definite.","I have the Cholesky decomposition of a matrix $M$. However, I need the Cholesky decomposition of the inverse of the matrix, $M^{-1}$. Is there a fast way to do this, without first computing $M^{-1}$? In other words, is there a relationship between the Cholesky decompositions of a matrix and of its inverse? My matrix is a covariance matrix and, hence, positive-definite.",,"['matrices', 'inverse', 'numerical-linear-algebra', 'matrix-decomposition', 'cholesky-decomposition']"
92,Spectral norm of random matrix,Spectral norm of random matrix,,"Suppose $A$ is a $n \times n$ random matrix with centered Gaussian (real) i.i.d. entries with variance $\frac{\sigma^2}{n}$ . What to we know about the spectral norm $s(A)$ of $A$ , that is $\sqrt{\rho(A^t A)}$ ? Here, $\rho(\cdot)$ denotes the largest eigenvalue of a matrix. In particular, if $A$ is symmetric, we know that $s(A)$ is precisely equal to $\rho(A)$ and from the circular law it implies that $s(A)$ converges to $\sigma$ as $n \to \infty$ . Is this last statement true for non-symmetric $A$ ?","Suppose is a random matrix with centered Gaussian (real) i.i.d. entries with variance . What to we know about the spectral norm of , that is ? Here, denotes the largest eigenvalue of a matrix. In particular, if is symmetric, we know that is precisely equal to and from the circular law it implies that converges to as . Is this last statement true for non-symmetric ?",A n \times n \frac{\sigma^2}{n} s(A) A \sqrt{\rho(A^t A)} \rho(\cdot) A s(A) \rho(A) s(A) \sigma n \to \infty A,"['matrices', 'probability-theory', 'random-matrices', 'spectral-radius', 'spectral-norm']"
93,Inverse of symmetric matrix plus identity matrix,Inverse of symmetric matrix plus identity matrix,,"Consider the symmetric, positive definite matrix $\mathbf{A}$. I'd like to find a general form for $$(\mathbf{I} + \mathbf{A})^{-1}$$ that only involves $\mathbf{A}^{-1}$, i.e., no other inverse appears in the solution (as, for instance, in the Woodbury matrix identity). I've tried to derive the inverse by hand but I could only obtain a result up to he $4 \times 4$ case as follows. $2 \times 2$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $3 \times 3$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} - \mathbf{A} + \big( \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $4 \times 4$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{A}^2 - \mathrm{tr}(\mathbf{A}) \mathbf{A} - \mathbf{A} + \big( \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ Can we find a general expression for higher dimensions that builds on the ones above? Even obtaining the $5 \times 5$ case is prohibitive, and so far I haven't been able to spot the pattern.","Consider the symmetric, positive definite matrix $\mathbf{A}$. I'd like to find a general form for $$(\mathbf{I} + \mathbf{A})^{-1}$$ that only involves $\mathbf{A}^{-1}$, i.e., no other inverse appears in the solution (as, for instance, in the Woodbury matrix identity). I've tried to derive the inverse by hand but I could only obtain a result up to he $4 \times 4$ case as follows. $2 \times 2$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $3 \times 3$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} - \mathbf{A} + \big( \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $4 \times 4$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{A}^2 - \mathrm{tr}(\mathbf{A}) \mathbf{A} - \mathbf{A} + \big( \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ Can we find a general expression for higher dimensions that builds on the ones above? Even obtaining the $5 \times 5$ case is prohibitive, and so far I haven't been able to spot the pattern.",,"['matrices', 'inverse']"
94,Relation between positive definite matrix and strictly convex quadratic form,Relation between positive definite matrix and strictly convex quadratic form,,"According to Wikipedia , any quadratic function can be written as $z^TMz$ , where $z$ is a column vector and $M$ is a symmetric real matrix. However, this quadratic function is strictly convex only when $M$ is symmetric positive definite. Why? I thought any quadratic function should be convex? Doesn't $z^TMz>0$ shows only that the range of this function is greater than zero? Why isn't any symmetric matrix $M$ (which represents a quadratic  function) convex? Why is it only the case that when $z^TMz > 0$ the function is strictly convex?","According to Wikipedia , any quadratic function can be written as , where is a column vector and is a symmetric real matrix. However, this quadratic function is strictly convex only when is symmetric positive definite. Why? I thought any quadratic function should be convex? Doesn't shows only that the range of this function is greater than zero? Why isn't any symmetric matrix (which represents a quadratic  function) convex? Why is it only the case that when the function is strictly convex?",z^TMz z M M z^TMz>0 M z^TMz > 0,"['matrices', 'convex-analysis', 'quadratics', 'quadratic-forms', 'positive-definite']"
95,Why I should believe that the derivative of the determinant is the trace,Why I should believe that the derivative of the determinant is the trace,,"Using a Taylor expansion, it is not hard to show that the derivative of the determinant function at the identity is the trace: $$ \lim_{t \to 0} \frac{ \det(I + tA) - \det(I) }{ t } = \operatorname{tr}(A). $$ The determinant of a matrix may be viewed as the change in area of regions in $\mathbb{R}^n$.  A square of area $1$ will be converted to a shape of area $\det(A)$.  This change in area is the product of the eigenvalues, as opposed to the sum.  What is a good intuitive way of seeing that the derivative would be the trace?","Using a Taylor expansion, it is not hard to show that the derivative of the determinant function at the identity is the trace: $$ \lim_{t \to 0} \frac{ \det(I + tA) - \det(I) }{ t } = \operatorname{tr}(A). $$ The determinant of a matrix may be viewed as the change in area of regions in $\mathbb{R}^n$.  A square of area $1$ will be converted to a shape of area $\det(A)$.  This change in area is the product of the eigenvalues, as opposed to the sum.  What is a good intuitive way of seeing that the derivative would be the trace?",,"['matrices', 'derivatives', 'determinant', 'trace']"
96,Prove that $\operatorname{trace}(ABC) = \operatorname{trace}(BCA) = \operatorname{trace}(CAB)$ [closed],Prove that  [closed],\operatorname{trace}(ABC) = \operatorname{trace}(BCA) = \operatorname{trace}(CAB),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Prove that $\operatorname{trace}(ABC) = \operatorname{trace}(BCA) = \operatorname{trace}(CAB)$ if $A,B,C$ matrices have the same size.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Prove that $\operatorname{trace}(ABC) = \operatorname{trace}(BCA) = \operatorname{trace}(CAB)$ if $A,B,C$ matrices have the same size.",,"['matrices', 'trace']"
97,How to rotate the positions of a matrix by 90 degrees,How to rotate the positions of a matrix by 90 degrees,,"I have a 5x5 matrix of values. I'm looking for a simple formula that I can use to rotate the position of the values (not the values themselves) 90 degrees within the matrix. For example, here is the original matrix: 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 and then when the position of the values are rotated 90 degrees, it would look like this: 21 16 11 06 01 22 17 12 07 02 23 18 13 08 03 24 19 14 09 04 25 20 15 10 05 I found this post and this one , and I'm sure the answer I'm after is in there somewhere, but I've been out of university for quite a few years and am having trouble following the algorithm. I need this for a C# program I'm writing and will be using Math.Net Numberics . I'm hoping there is just a simple rotation matrix/vector I can use to multiply my matrix with that will give me the result I'm after. Any suggestions are appreciated.","I have a 5x5 matrix of values. I'm looking for a simple formula that I can use to rotate the position of the values (not the values themselves) 90 degrees within the matrix. For example, here is the original matrix: 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 and then when the position of the values are rotated 90 degrees, it would look like this: 21 16 11 06 01 22 17 12 07 02 23 18 13 08 03 24 19 14 09 04 25 20 15 10 05 I found this post and this one , and I'm sure the answer I'm after is in there somewhere, but I've been out of university for quite a few years and am having trouble following the algorithm. I need this for a C# program I'm writing and will be using Math.Net Numberics . I'm hoping there is just a simple rotation matrix/vector I can use to multiply my matrix with that will give me the result I'm after. Any suggestions are appreciated.",,"['matrices', 'matrix-equations']"
98,Matrix exponential of non diagonalizable matrix?,Matrix exponential of non diagonalizable matrix?,,"I am currently self-learning about matrix exponential and found that determining the matrix of a diagonalizable matrix is pretty straight forward :). I do not, however, know how to find the exponential matrix of a non-diagonalizable matrix. For example, consider the matrix  $$\begin{bmatrix}1 & 0 \\ 1 & 1\end{bmatrix}$$ Is there any process of finding the exponential matrix of a non-diagonalizable matrix? If so, can someone please show me an example of the process? :). I am not looking for an answer of the above mentioned matrix (since I just made it up), but rather I'm interested in the actual method of finding the matrix exponential to apply to other examples :)","I am currently self-learning about matrix exponential and found that determining the matrix of a diagonalizable matrix is pretty straight forward :). I do not, however, know how to find the exponential matrix of a non-diagonalizable matrix. For example, consider the matrix  $$\begin{bmatrix}1 & 0 \\ 1 & 1\end{bmatrix}$$ Is there any process of finding the exponential matrix of a non-diagonalizable matrix? If so, can someone please show me an example of the process? :). I am not looking for an answer of the above mentioned matrix (since I just made it up), but rather I'm interested in the actual method of finding the matrix exponential to apply to other examples :)",,"['matrices', 'matrix-exponential']"
99,Is the square root of a triangular matrix necessarily triangular?,Is the square root of a triangular matrix necessarily triangular?,,"$X^2 = L$, with $L$ lower triangular, but $X$ is not lower triangular. Is it possible? I know that a lower triangular matrix $L$ ( not a diagonal matrix for this question), $$L_{nm}  \cases{=0 & for all $m > n$ \\  \ne 0 & for some $ m<n$} $$ when squared is lower triangular. But is the square root, when it exists, always lower triangular? I have found some examples that give a diagonal matrix: $$\pmatrix{1 & 0 & 1 \\ 0 & e & 0 \\ c & 0 & -1}\pmatrix{1 & 0 & 1 \\ 0 & e & 0 \\ c & 0 & -1}=\pmatrix{c+1 & 0 & 0 \\ 0 & e^2 & 0 \\ 0 & 0 & c+1}=L$$ But I am wondering about the possibility of the square being strictly triangular, not diagonal. I believe the answer is yes, that if the square root of a strictly lower triangular exists, then that is also lower triangular. I am looking for a good argument as to why, or for any counter examples. EDIT: Also, all $ 2 \times 2$ that gives the upper right zero in the square implies also the lower left is zero.  $$\pmatrix{a & c \\ d & b\\}\pmatrix{a & c \\ d & b\\}=\pmatrix{a^2 + cd & ac + cb \\ ad + bd & cd + b^2\\}$$ $$(ac+cb= 0) \Rightarrow (a = -b) \Rightarrow (ad+bd=0)$$ So a counter example will necessarily be higher dimension, but I am thinking that the same logic will still apply somehow, along the lines of $2 \times 2$ sub-matrices or something.","$X^2 = L$, with $L$ lower triangular, but $X$ is not lower triangular. Is it possible? I know that a lower triangular matrix $L$ ( not a diagonal matrix for this question), $$L_{nm}  \cases{=0 & for all $m > n$ \\  \ne 0 & for some $ m<n$} $$ when squared is lower triangular. But is the square root, when it exists, always lower triangular? I have found some examples that give a diagonal matrix: $$\pmatrix{1 & 0 & 1 \\ 0 & e & 0 \\ c & 0 & -1}\pmatrix{1 & 0 & 1 \\ 0 & e & 0 \\ c & 0 & -1}=\pmatrix{c+1 & 0 & 0 \\ 0 & e^2 & 0 \\ 0 & 0 & c+1}=L$$ But I am wondering about the possibility of the square being strictly triangular, not diagonal. I believe the answer is yes, that if the square root of a strictly lower triangular exists, then that is also lower triangular. I am looking for a good argument as to why, or for any counter examples. EDIT: Also, all $ 2 \times 2$ that gives the upper right zero in the square implies also the lower left is zero.  $$\pmatrix{a & c \\ d & b\\}\pmatrix{a & c \\ d & b\\}=\pmatrix{a^2 + cd & ac + cb \\ ad + bd & cd + b^2\\}$$ $$(ac+cb= 0) \Rightarrow (a = -b) \Rightarrow (ad+bd=0)$$ So a counter example will necessarily be higher dimension, but I am thinking that the same logic will still apply somehow, along the lines of $2 \times 2$ sub-matrices or something.",,['matrices']
