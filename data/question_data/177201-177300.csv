,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Area of spherical cap with integrals,Area of spherical cap with integrals,,"Given a sphere $S$ of fixed diameter $D$ (or radius $R=D/2$, it will be convenient to have both, I suppose), and a point $P$ on its surface, let's create a ball $B$ of variable radius $r$ with its centre in $P$. What I have been wondering was - what is the surface area $A(r)$ of the intersection of the sphere and the ball as a function of $r$? Recently I found a neat solution using the formula for the area of spherical cap - which it will be, after all, I just had to relate the missing variables for the formula to the ones I know, and I managed to get that $A=\pi r^2$ for $0 \leq r \leq D$ and I believe that to be correct (it might not necessarily be, I just was unable to find any limiting cases that would prove the formula wrong). However, my initial approach - the crux of the question - was different. I replaced B with a sphere $B'$ with the same parameters (i.e. centre in $P$ and variable radius $r$), attempted to find the perimeter of the intersection (circle) $p(r)$ and then let $A(r)=\int_{0}^{r}p(x) dx$ Finding that $$p(r)=2\pi \sqrt{r^2-\frac{r^4}{D^2}}$$ was simple, but the integration was not, and not knowing enough to reliably follow through with it, I asked Wolfram Alpha to do the heavy lifting, which gave me this: $$\int p(x) dx = \frac{2\pi (x^2-1)\sqrt{x^2-x^4}}{3x}+c$$ (I assumed D=1 for simplicity to easier see the curve, perform sanity checks, and believing that I can always multiply the resulting area by $D^2$) The curve seemed plausible, I just had to compute the exact function $A(r)$ as defined above as a definite integral. Because the function wasn't defined at 0, I had to compute $$\lim_{x \to0^+}2\pi \sqrt{x^2-\frac{x^4}{D^2}}$$ which gave me a puzzling result - $2\pi/3$ for unit sphere. My question is - why this value? What was incorrect in this approach?","Given a sphere $S$ of fixed diameter $D$ (or radius $R=D/2$, it will be convenient to have both, I suppose), and a point $P$ on its surface, let's create a ball $B$ of variable radius $r$ with its centre in $P$. What I have been wondering was - what is the surface area $A(r)$ of the intersection of the sphere and the ball as a function of $r$? Recently I found a neat solution using the formula for the area of spherical cap - which it will be, after all, I just had to relate the missing variables for the formula to the ones I know, and I managed to get that $A=\pi r^2$ for $0 \leq r \leq D$ and I believe that to be correct (it might not necessarily be, I just was unable to find any limiting cases that would prove the formula wrong). However, my initial approach - the crux of the question - was different. I replaced B with a sphere $B'$ with the same parameters (i.e. centre in $P$ and variable radius $r$), attempted to find the perimeter of the intersection (circle) $p(r)$ and then let $A(r)=\int_{0}^{r}p(x) dx$ Finding that $$p(r)=2\pi \sqrt{r^2-\frac{r^4}{D^2}}$$ was simple, but the integration was not, and not knowing enough to reliably follow through with it, I asked Wolfram Alpha to do the heavy lifting, which gave me this: $$\int p(x) dx = \frac{2\pi (x^2-1)\sqrt{x^2-x^4}}{3x}+c$$ (I assumed D=1 for simplicity to easier see the curve, perform sanity checks, and believing that I can always multiply the resulting area by $D^2$) The curve seemed plausible, I just had to compute the exact function $A(r)$ as defined above as a definite integral. Because the function wasn't defined at 0, I had to compute $$\lim_{x \to0^+}2\pi \sqrt{x^2-\frac{x^4}{D^2}}$$ which gave me a puzzling result - $2\pi/3$ for unit sphere. My question is - why this value? What was incorrect in this approach?",,"['calculus', 'integration', 'multivariable-calculus', 'differential-geometry']"
1,A set of measure zero in a product space intersects almost every fiber in a set of measure zero,A set of measure zero in a product space intersects almost every fiber in a set of measure zero,,"Let $C\subset A\times B$ be a set of content zero. Let $A'\subset A$ be the set of all $x\in A$ such that $\{y\in B: (x,y)\in C\}$ is not of content zero. Show that $A'$ is a set of measure zero. I couldn't finish this problem, after a few steps I'm not sure what could I do next. This much I know: $C$ has content zero, then $\partial C$ has content zero ($\partial C$ denotes the boundary of $C$) which means that the characteristic function $\chi_c$ is integrable in $A\times B$. Then I applied Fubini's theorem to have $\displaystyle\int_{A\times B}\chi_C=\displaystyle\int_A\left(L\displaystyle\int_B \chi_C\; dy\right)dx=\displaystyle\int_A\left(U\displaystyle\int_B\chi_C\;dy\right)dx$ where $L$ and $U$ denote that those are the lower and upper integral. Now follows  $\displaystyle\int_A\left(L\displaystyle\int_B \chi_C\; dy-U\displaystyle\int_B\chi_C\;dy\right)dx=0$ which means that $\displaystyle\int_B \chi_C\;dy$ is integrable. Since $C$ is a set of content zero, is a set of measure zero. I know that if a bounded set $C$ is of measure zero and $\displaystyle\int_B\chi_c$ exists then $\displaystyle\int_B\chi_c=0$. Here I'm not sure how to go on, I'm considering taking a partition $P$ of $B$ and knowing that $\sup\{L(\chi_C,P): \text{P partition of B}\}=0$ is possible to take $P$ such that $U(f,P)=\sum_{S\in P} M_S(\chi_C)v(S)\leq\sum_{S\in P} v(S)<\epsilon$. But it doesn't seem okay... Any thoughts about it?.","Let $C\subset A\times B$ be a set of content zero. Let $A'\subset A$ be the set of all $x\in A$ such that $\{y\in B: (x,y)\in C\}$ is not of content zero. Show that $A'$ is a set of measure zero. I couldn't finish this problem, after a few steps I'm not sure what could I do next. This much I know: $C$ has content zero, then $\partial C$ has content zero ($\partial C$ denotes the boundary of $C$) which means that the characteristic function $\chi_c$ is integrable in $A\times B$. Then I applied Fubini's theorem to have $\displaystyle\int_{A\times B}\chi_C=\displaystyle\int_A\left(L\displaystyle\int_B \chi_C\; dy\right)dx=\displaystyle\int_A\left(U\displaystyle\int_B\chi_C\;dy\right)dx$ where $L$ and $U$ denote that those are the lower and upper integral. Now follows  $\displaystyle\int_A\left(L\displaystyle\int_B \chi_C\; dy-U\displaystyle\int_B\chi_C\;dy\right)dx=0$ which means that $\displaystyle\int_B \chi_C\;dy$ is integrable. Since $C$ is a set of content zero, is a set of measure zero. I know that if a bounded set $C$ is of measure zero and $\displaystyle\int_B\chi_c$ exists then $\displaystyle\int_B\chi_c=0$. Here I'm not sure how to go on, I'm considering taking a partition $P$ of $B$ and knowing that $\sup\{L(\chi_C,P): \text{P partition of B}\}=0$ is possible to take $P$ such that $U(f,P)=\sum_{S\in P} M_S(\chi_C)v(S)\leq\sum_{S\in P} v(S)<\epsilon$. But it doesn't seem okay... Any thoughts about it?.",,"['integration', 'multivariable-calculus', 'solution-verification']"
2,How to define integration over the boundary of a curve?,How to define integration over the boundary of a curve?,,"When learning about Stokes' theorem ($\int_{\partial \Omega} \omega=\int_{\Omega} \mathrm d \omega$), we are told that it is just a generalization of the 2nd Fundamental Theorem of Calculus $(\int_a^b F'(x)dx= F(b)-F(a))$.  In particular, we are told that we can consider $F(b) - F(a)$ to be an integral over the boundary of the curve $y=F'(x), x \in [a, b]$ -- the boundary is then the disconnected set of points $\{a, b\}$ with an orientation ""from $a$ to $b$"". Edit: user8268 indicates that this orientation should $-a$ and $+b$, though I'm not entirely sure why -- other than it makes the equation work. But is this really integration?  Doesn't the ""region of integration"" of a Riemann integral have to be connected? Edit: It's not really a region if it's not connected, but I don't know what else to call it. Is there any way to define a Riemann (or possibly Lebesgue) integral  $\int_A f(x)dx$ over $A=\{a,b : a \neq b\}$ (with some orientation)?","When learning about Stokes' theorem ($\int_{\partial \Omega} \omega=\int_{\Omega} \mathrm d \omega$), we are told that it is just a generalization of the 2nd Fundamental Theorem of Calculus $(\int_a^b F'(x)dx= F(b)-F(a))$.  In particular, we are told that we can consider $F(b) - F(a)$ to be an integral over the boundary of the curve $y=F'(x), x \in [a, b]$ -- the boundary is then the disconnected set of points $\{a, b\}$ with an orientation ""from $a$ to $b$"". Edit: user8268 indicates that this orientation should $-a$ and $+b$, though I'm not entirely sure why -- other than it makes the equation work. But is this really integration?  Doesn't the ""region of integration"" of a Riemann integral have to be connected? Edit: It's not really a region if it's not connected, but I don't know what else to call it. Is there any way to define a Riemann (or possibly Lebesgue) integral  $\int_A f(x)dx$ over $A=\{a,b : a \neq b\}$ (with some orientation)?",,"['real-analysis', 'integration', 'multivariable-calculus', 'differential-forms', 'geometric-algebras']"
3,Where is my mistake $\iint_{Q} (x+y)^{2013}dxdy$,Where is my mistake,\iint_{Q} (x+y)^{2013}dxdy,"I'm preparing for a calculus exam, and I tried to solve the following question. $Q$ is square $[-1,1]^2 \subset \mathbb R^2$ We are asked to evaluate $\iint_Q (x+y)^{2013}dxdy$ Here is what I did: $$\iint_Q (x+y)^{2013}dxdy = \int_{-1}^{1} \int_{-1}^{1} (x+y)^{2013}dxdy=\int_{-1}^{1}\frac{(x+y)^{2014}}{2014} |_{x=-1}^{1}dy$$ so overall after finishing with $dx$ we have: $$\frac{1}{2014} \int_{-1}^{1}(y+1)^{2014}-(y-1)^{2014}dy=\frac{(y+1)^{2015}-(y-1)^{2015}}{2015*2014} |_{y=-1}^{1} $$ But note that $$2^{2015}-0^{2015}-0^{2015}+(-2)^{2015}=2^{2015}-2^{2015}=0$$ so the entire double integral is equal to zero. However, when I check my answer in wolfram alpha, wolfram says that this integral is equal to $9.27057*10^{599}$ I'm just wondering where is my mistake? Or perhaps wolfram alpha gave the wrong answer because of a numerical error.","I'm preparing for a calculus exam, and I tried to solve the following question. $Q$ is square $[-1,1]^2 \subset \mathbb R^2$ We are asked to evaluate $\iint_Q (x+y)^{2013}dxdy$ Here is what I did: $$\iint_Q (x+y)^{2013}dxdy = \int_{-1}^{1} \int_{-1}^{1} (x+y)^{2013}dxdy=\int_{-1}^{1}\frac{(x+y)^{2014}}{2014} |_{x=-1}^{1}dy$$ so overall after finishing with $dx$ we have: $$\frac{1}{2014} \int_{-1}^{1}(y+1)^{2014}-(y-1)^{2014}dy=\frac{(y+1)^{2015}-(y-1)^{2015}}{2015*2014} |_{y=-1}^{1} $$ But note that $$2^{2015}-0^{2015}-0^{2015}+(-2)^{2015}=2^{2015}-2^{2015}=0$$ so the entire double integral is equal to zero. However, when I check my answer in wolfram alpha, wolfram says that this integral is equal to $9.27057*10^{599}$ I'm just wondering where is my mistake? Or perhaps wolfram alpha gave the wrong answer because of a numerical error.",,"['calculus', 'integration', 'multivariable-calculus']"
4,A set is compact if and only if every continuous function is bounded on the set? [duplicate],A set is compact if and only if every continuous function is bounded on the set? [duplicate],,"This question already has answers here : $K\subseteq \mathbb{R}^n$ is a compact space iff every continuous function in $K$ is bounded. (4 answers) Closed 9 years ago . I was asked to prove the following statement: Let $K \subseteq R^n$ . show that $K$ is compact (meaning closed and bounded) if and only if every continuous function is bounded on $K$ . What I did: Suppose $K$ is not bounded, and so, it is not compact. Then the function $\sum |x_i|$ is a continuous unbounded function on $K$ . Via contrapositive, this shows that if every function is bounded, then $K$ is also bounded. What I need help with: Assume $K$ is not closed. I need to find a continuous and unbounded function on $K$ . that will prove that if every continuous function is bounded on $K$ , then $K$ is compact. after that, i still need to show that if $K$ is compact then every continuous function $f: K \to \mathbb R$ is bounded. Would someone point me in the right direction? Clarification: it's not homework. I am preparing for an exam.","This question already has answers here : $K\subseteq \mathbb{R}^n$ is a compact space iff every continuous function in $K$ is bounded. (4 answers) Closed 9 years ago . I was asked to prove the following statement: Let . show that is compact (meaning closed and bounded) if and only if every continuous function is bounded on . What I did: Suppose is not bounded, and so, it is not compact. Then the function is a continuous unbounded function on . Via contrapositive, this shows that if every function is bounded, then is also bounded. What I need help with: Assume is not closed. I need to find a continuous and unbounded function on . that will prove that if every continuous function is bounded on , then is compact. after that, i still need to show that if is compact then every continuous function is bounded. Would someone point me in the right direction? Clarification: it's not homework. I am preparing for an exam.",K \subseteq R^n K K K \sum |x_i| K K K K K K K f: K \to \mathbb R,"['calculus', 'general-topology', 'multivariable-calculus', 'compactness']"
5,"very strange phenomenon $f(x,y)=x^4-6x^2y^2+y^4$ integral goes wild",very strange phenomenon  integral goes wild,"f(x,y)=x^4-6x^2y^2+y^4","I am going over my lecture's notes in preparation for exam and I saw something a bit strange I would like someone to explain how it is possible. Look at the function $f(x,y) = x^4-6x^2y^2+y^4$ if we convert it to polar coordinates, we will get $f(r,\theta)=r^4\cos(4\theta)$ the weird thing about this function, is that it has 2 different integrals over the same area. What I mean: if we calculate the integral $$ \lim_{n \to \infty}\int_{0}^{2\pi} \int_{0}^{n} r^5\cos(4\theta)dr d\theta$$ we will see it is equal to zero. However, if we calculate $$\lim_{n \to \infty}\int_{-n}^{n} \int_{-n}^{n} x^4-6x^2y^2+y^4dxdy = \lim_{n \to \infty}\int_{-n}^{n} \frac{2n^5}{5}-4n^3y^2+2ny^4 dy=\lim_{n \to \infty} -\frac{16}{15}n^6 = -\infty$$ How come the integral is so different based just on how we choose to represent the same area? (it is $\mathbb R^2$ in both cases. first it is a giant circle, second case it is a giant square) So if asked what is $$\iint_{\mathbb R^2} x^4-6x^2y^2+y^4$$ is the correct thing to say that it does not exist?","I am going over my lecture's notes in preparation for exam and I saw something a bit strange I would like someone to explain how it is possible. Look at the function $f(x,y) = x^4-6x^2y^2+y^4$ if we convert it to polar coordinates, we will get $f(r,\theta)=r^4\cos(4\theta)$ the weird thing about this function, is that it has 2 different integrals over the same area. What I mean: if we calculate the integral $$ \lim_{n \to \infty}\int_{0}^{2\pi} \int_{0}^{n} r^5\cos(4\theta)dr d\theta$$ we will see it is equal to zero. However, if we calculate $$\lim_{n \to \infty}\int_{-n}^{n} \int_{-n}^{n} x^4-6x^2y^2+y^4dxdy = \lim_{n \to \infty}\int_{-n}^{n} \frac{2n^5}{5}-4n^3y^2+2ny^4 dy=\lim_{n \to \infty} -\frac{16}{15}n^6 = -\infty$$ How come the integral is so different based just on how we choose to represent the same area? (it is $\mathbb R^2$ in both cases. first it is a giant circle, second case it is a giant square) So if asked what is $$\iint_{\mathbb R^2} x^4-6x^2y^2+y^4$$ is the correct thing to say that it does not exist?",,"['calculus', 'integration', 'multivariable-calculus']"
6,Prove that a function is differentiable if...,Prove that a function is differentiable if...,,"I'm trying to prove that given a differentiable function $f: \mathbb{R}^2 \to \mathbb{R}^m$ in $p =(p_1, p_2) \in \mathbb{R}^2$, the function $$  g(x, y) = f(x, y) - \frac{\partial f}{\partial x}(p)(x - p_1) - \frac{\partial f}{\partial y}(p)(y - p_2)  $$ is also differentiable in $(x, y) = p$? Suppose that the partial derivatives in $p$ are $\neq 0$ and $f$ is once-differentiable. What is the easiest and fastest way to prove it (using composition or linearity maybe)? If works even if the domain of that function is $\mathbb{R}^n$ and $$ g(\vec{x}) = f(\vec{x}) - \sum^n_{i = 1} \frac{\partial f}{\partial x_i}(p)(x_i - p_i)$$?","I'm trying to prove that given a differentiable function $f: \mathbb{R}^2 \to \mathbb{R}^m$ in $p =(p_1, p_2) \in \mathbb{R}^2$, the function $$  g(x, y) = f(x, y) - \frac{\partial f}{\partial x}(p)(x - p_1) - \frac{\partial f}{\partial y}(p)(y - p_2)  $$ is also differentiable in $(x, y) = p$? Suppose that the partial derivatives in $p$ are $\neq 0$ and $f$ is once-differentiable. What is the easiest and fastest way to prove it (using composition or linearity maybe)? If works even if the domain of that function is $\mathbb{R}^n$ and $$ g(\vec{x}) = f(\vec{x}) - \sum^n_{i = 1} \frac{\partial f}{\partial x_i}(p)(x_i - p_i)$$?",,"['calculus', 'multivariable-calculus', 'derivatives']"
7,Use Lagrange Multipliers to find max and min of $x+2y$ subject to $x+y+z=1$ and $y^2+z^2=4$,Use Lagrange Multipliers to find max and min of  subject to  and,x+2y x+y+z=1 y^2+z^2=4,"Using Lagrange Multipliers, determine the maximum and minimum of the function $f(x,y,z) = x + 2y$ subject to the constraints $x + y + z = 1$ and $y^2 + z^2 = 4$: Justify that the points you have found give the maximum and minimum of $f$. So,  $$ \nabla f  = (λ_1)\nabla g_1 + (λ_2)\nabla g_2 $$ I get to this point $$ (1,2,0) = λ_1(1,1,1) + λ_2(0,2y,2z) $$ Where do I go from here to find the critical points ect.","Using Lagrange Multipliers, determine the maximum and minimum of the function $f(x,y,z) = x + 2y$ subject to the constraints $x + y + z = 1$ and $y^2 + z^2 = 4$: Justify that the points you have found give the maximum and minimum of $f$. So,  $$ \nabla f  = (λ_1)\nabla g_1 + (λ_2)\nabla g_2 $$ I get to this point $$ (1,2,0) = λ_1(1,1,1) + λ_2(0,2y,2z) $$ Where do I go from here to find the critical points ect.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
8,"minimum and maximum of $f(x,y)=\sin(x)+\sin(y)-\sin(x+y)$",minimum and maximum of,"f(x,y)=\sin(x)+\sin(y)-\sin(x+y)","we are asked to find the minimum and maximum of the function$f:A \to A$ $f(x,y)=\sin(x)+\sin(y)-\sin(x+y)$ Where $A$ is the triangle bound by $x=0$,$y=0$ and $y=-x+2\pi$ I'd like someone to review my answer. What I did: $A$ is a closed and bounded set, $f(x,y)$ is continuous, so according to Weierstrass theorem, $f$ receives maximal / minimal values, either on the boundary, or an internal point $(a,b)$ where $\triangle f(a,b)=(0,0)$,($\triangle$ represents gradient.) First let's find points where the gradient is zero: $\triangle f(x,y)=(\cos(x)-\cos(x+y),\cos(y)-\cos(x+y))=0$ this implies $\cos(x)=\cos(y)=\cos(x+y)$. On the triangle we were given, this can only happen at $(0,2\pi)$ or $(2\pi,0)$, otherwise we are outside the boundaries of the triangle. and $f(0,2\pi)=f(2\pi,0)=0$. Let's see what happens on the boundary, assume first $x=0$: $f(0,y)=\sin(0)+\sin(y)-\sin(0+y)=0$ Same thing happens when $y=0$. Now let's see $y=-x+2\pi$: $f(x,-x+2\pi)=\sin(x)+\sin(-x+2\pi)-\sin(x+x-2\pi)=\sin(x)+\sin(-x)-\sin(2x)=\sin(x)-\sin(x)-\sin(2x)=-\sin(2x)$ Let's denote $g(x)=-\sin(2x)$, then $g'(x)=-2\cos(2x)$. $g'(x)=0$ implies $\cos(2x)=0$, which implies $2x=\frac{\pi}{2}+k\pi$, then $x=\frac{\pi}{4}+k\frac{\pi}{2}$ The only such valid point on our triangle would be the point $(\frac{\pi}{4},\frac{7\pi}{4})$. and at that point: $f(\frac{\pi}{4},\frac{7\pi}{4})=\sin(\frac{\pi}{4})+\sin(\frac{7\pi}{4})-\sin(2\pi)=0$ At all the potentially extreme points, we got $f=0$. this makes me believe that $f(A)=\{0\}$. There is no point on the triangle where $f$ is not zero. Is this true? Is it possible to verify this result with trigonometric identities? to simplify $\sin(x)+\sin(y)-\sin(x+y)$ and eventually hope to reach zero?","we are asked to find the minimum and maximum of the function$f:A \to A$ $f(x,y)=\sin(x)+\sin(y)-\sin(x+y)$ Where $A$ is the triangle bound by $x=0$,$y=0$ and $y=-x+2\pi$ I'd like someone to review my answer. What I did: $A$ is a closed and bounded set, $f(x,y)$ is continuous, so according to Weierstrass theorem, $f$ receives maximal / minimal values, either on the boundary, or an internal point $(a,b)$ where $\triangle f(a,b)=(0,0)$,($\triangle$ represents gradient.) First let's find points where the gradient is zero: $\triangle f(x,y)=(\cos(x)-\cos(x+y),\cos(y)-\cos(x+y))=0$ this implies $\cos(x)=\cos(y)=\cos(x+y)$. On the triangle we were given, this can only happen at $(0,2\pi)$ or $(2\pi,0)$, otherwise we are outside the boundaries of the triangle. and $f(0,2\pi)=f(2\pi,0)=0$. Let's see what happens on the boundary, assume first $x=0$: $f(0,y)=\sin(0)+\sin(y)-\sin(0+y)=0$ Same thing happens when $y=0$. Now let's see $y=-x+2\pi$: $f(x,-x+2\pi)=\sin(x)+\sin(-x+2\pi)-\sin(x+x-2\pi)=\sin(x)+\sin(-x)-\sin(2x)=\sin(x)-\sin(x)-\sin(2x)=-\sin(2x)$ Let's denote $g(x)=-\sin(2x)$, then $g'(x)=-2\cos(2x)$. $g'(x)=0$ implies $\cos(2x)=0$, which implies $2x=\frac{\pi}{2}+k\pi$, then $x=\frac{\pi}{4}+k\frac{\pi}{2}$ The only such valid point on our triangle would be the point $(\frac{\pi}{4},\frac{7\pi}{4})$. and at that point: $f(\frac{\pi}{4},\frac{7\pi}{4})=\sin(\frac{\pi}{4})+\sin(\frac{7\pi}{4})-\sin(2\pi)=0$ At all the potentially extreme points, we got $f=0$. this makes me believe that $f(A)=\{0\}$. There is no point on the triangle where $f$ is not zero. Is this true? Is it possible to verify this result with trigonometric identities? to simplify $\sin(x)+\sin(y)-\sin(x+y)$ and eventually hope to reach zero?",,"['calculus', 'multivariable-calculus', 'derivatives', 'proof-verification']"
9,"Line integral: $u = ( \frac{-y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, z)$",Line integral:,"u = ( \frac{-y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, z)","Let $u = ( \frac{-y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, z)$ and D the domain bounded by the torus obtained by rotating the circunference $(x-2)^2 + z^2 =1, y=0$ around the z-axis. Show that $rot( u )=0 $ in D, but  $$ \int_C u.dr \neq 0$$ if $C$ is the circunference $x^2 + y^2 = 4, z=0$. Determine all the possible values for the integral $$\int_{(2,0,0)}^{(0,2,0)} u.dr$$ over a path in $D$ My attempt : I've show the first part but I am having troubles with the last one. If $y$ is not zero, then $\phi(x,y,z)=\arctan(-\frac{x}{y})+\frac{z^2}{2}$ would be a good potential for $u$, but I couldnt find the case that $y=0$. Thanks!","Let $u = ( \frac{-y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, z)$ and D the domain bounded by the torus obtained by rotating the circunference $(x-2)^2 + z^2 =1, y=0$ around the z-axis. Show that $rot( u )=0 $ in D, but  $$ \int_C u.dr \neq 0$$ if $C$ is the circunference $x^2 + y^2 = 4, z=0$. Determine all the possible values for the integral $$\int_{(2,0,0)}^{(0,2,0)} u.dr$$ over a path in $D$ My attempt : I've show the first part but I am having troubles with the last one. If $y$ is not zero, then $\phi(x,y,z)=\arctan(-\frac{x}{y})+\frac{z^2}{2}$ would be a good potential for $u$, but I couldnt find the case that $y=0$. Thanks!",,"['integration', 'multivariable-calculus', 'definite-integrals']"
10,Prove that $f$ is a $C^1$ diffeomorphism with this condition,Prove that  is a  diffeomorphism with this condition,f C^1,"this is an exercise that I cannot solve: Suppose that $f \in C^ 1 (\mathbb{R}^n , \mathbb{R}^n )$ is regular everywhere and that there exists $k>0$ such that $\|f(x_1)−f(x_2)\| \geq k\|x_1−x_2\|$, for all $x_1, x_2\in \mathbb{R}^n$. Prove that $f$ is a $C^1$ diffeomorphism. I just know that $f$ is injective and open. How can I continue? Thanks.","this is an exercise that I cannot solve: Suppose that $f \in C^ 1 (\mathbb{R}^n , \mathbb{R}^n )$ is regular everywhere and that there exists $k>0$ such that $\|f(x_1)−f(x_2)\| \geq k\|x_1−x_2\|$, for all $x_1, x_2\in \mathbb{R}^n$. Prove that $f$ is a $C^1$ diffeomorphism. I just know that $f$ is injective and open. How can I continue? Thanks.",,"['real-analysis', 'multivariable-calculus']"
11,Gradient of matrix exponential function,Gradient of matrix exponential function,,"Grateful if somebody could help me with the following. I am trying to find the gradient of the next expression: $$f(a_1, a_2, a_3, a_4)=\Vert R*y-x \Vert $$ where $y$ and $x$ are known 4x1 column vectors and R is a 4x4 orthogonal rotation matrix given as an exponential $$R = \exp(a_1*b_1+a_2*b_2+a_3*b_3+a_4*b_4), $$ where $a_1, a_2, a_3, $and $a_4$ are scalars and $b_1, b_2, b_3$, and $b_4$ are 4x4 matrices. I want to calculate the gradient of $f(\cdot)$ with respect to $a_1, a_2, a_3$, and $a_4$. Any hint is appreciated. Cristian","Grateful if somebody could help me with the following. I am trying to find the gradient of the next expression: $$f(a_1, a_2, a_3, a_4)=\Vert R*y-x \Vert $$ where $y$ and $x$ are known 4x1 column vectors and R is a 4x4 orthogonal rotation matrix given as an exponential $$R = \exp(a_1*b_1+a_2*b_2+a_3*b_3+a_4*b_4), $$ where $a_1, a_2, a_3, $and $a_4$ are scalars and $b_1, b_2, b_3$, and $b_4$ are 4x4 matrices. I want to calculate the gradient of $f(\cdot)$ with respect to $a_1, a_2, a_3$, and $a_4$. Any hint is appreciated. Cristian",,"['matrices', 'multivariable-calculus', 'optimization', 'exponential-function']"
12,Find the orthogonal projection of b onto a,Find the orthogonal projection of b onto a,,"Given  $${\mathbf a} = (-10, -8, 9) $$and $${\mathbf b} = (4, 5, 8) $$ Can someone please define what it means to find the orthogonal projection of b onto a? Also what is the formula for computing the orthogonal projection of b onto a? Thank you in advance! EDIT: Using the formula for b projection a I get the vectors: $$(80/245, 64/245, -72/245)$$  But that's incorrect for the orthogonal projection.","Given  $${\mathbf a} = (-10, -8, 9) $$and $${\mathbf b} = (4, 5, 8) $$ Can someone please define what it means to find the orthogonal projection of b onto a? Also what is the formula for computing the orthogonal projection of b onto a? Thank you in advance! EDIT: Using the formula for b projection a I get the vectors: $$(80/245, 64/245, -72/245)$$  But that's incorrect for the orthogonal projection.",,"['multivariable-calculus', 'vectors']"
13,tangent line to a level curve,tangent line to a level curve,,"Given the function $f:\mathbb{R}^2\to\mathbb{R}$ defined by $$f(x,y)=x^3 + 3x^2y-y^3$$ find the points $(a,b)$ of the plane that satisfy the tangent of the level curve $M=f(a,b)$ in the point $(a,b)$ passes through $(0,1)$. I tried solving this simply by using the equation of the tangent to a level curve: $$f_x(a,b)(x-a)+f_y(a,b)(y-b)=0$$ Hence, I get $$(3a^2+6ab)(x-a)+(3a^2-3b^2)(y-b) = 0$$ Then should I substitute $x,y$ for $(0,1)$? If so, after working it out, I get $$-a^3 -3a^2b+a^2-b^2+b^3=0$$ But how can I solve the points from here on out?","Given the function $f:\mathbb{R}^2\to\mathbb{R}$ defined by $$f(x,y)=x^3 + 3x^2y-y^3$$ find the points $(a,b)$ of the plane that satisfy the tangent of the level curve $M=f(a,b)$ in the point $(a,b)$ passes through $(0,1)$. I tried solving this simply by using the equation of the tangent to a level curve: $$f_x(a,b)(x-a)+f_y(a,b)(y-b)=0$$ Hence, I get $$(3a^2+6ab)(x-a)+(3a^2-3b^2)(y-b) = 0$$ Then should I substitute $x,y$ for $(0,1)$? If so, after working it out, I get $$-a^3 -3a^2b+a^2-b^2+b^3=0$$ But how can I solve the points from here on out?",,"['multivariable-calculus', 'partial-derivative']"
14,Meaning of Normal Vector in Surface Integration,Meaning of Normal Vector in Surface Integration,,"Is there a good interpretation of what the normal vector (and its magnitude) $$\mathbf{N}=\frac{\partial \mathbf{X}}{\partial s}\times\frac{\partial\mathbf{X}}{\partial t}$$ to the parametric surface $\mathbf{X}(s,t)$ represents? In line integrals the quantity $||\mathbf{x}'(t)||$ is the speed of the curve, and the way to ""normalize"" things is to use the arc-length parameterization. So does there always exist (for the usual ""nice"" surfaces) a similar parameterization where $||\mathbf{N}||=1$? Perhaps more stringently, does there always exist a parameterization where $$\left|\left|\frac{\partial \mathbf{X}}{\partial s}\right|\right|=\left|\left|\frac{\partial \mathbf{X}}{\partial t}\right|\right|=1\;\;\text{and}\;\;\frac{\partial \mathbf{X}}{\partial s}\cdot \frac{\partial \mathbf{X}}{\partial t}=0$$ and if so, what would this mean? Is this some kind of ""orthogonal unit speed"" parameterization or something? Cheers!","Is there a good interpretation of what the normal vector (and its magnitude) $$\mathbf{N}=\frac{\partial \mathbf{X}}{\partial s}\times\frac{\partial\mathbf{X}}{\partial t}$$ to the parametric surface $\mathbf{X}(s,t)$ represents? In line integrals the quantity $||\mathbf{x}'(t)||$ is the speed of the curve, and the way to ""normalize"" things is to use the arc-length parameterization. So does there always exist (for the usual ""nice"" surfaces) a similar parameterization where $||\mathbf{N}||=1$? Perhaps more stringently, does there always exist a parameterization where $$\left|\left|\frac{\partial \mathbf{X}}{\partial s}\right|\right|=\left|\left|\frac{\partial \mathbf{X}}{\partial t}\right|\right|=1\;\;\text{and}\;\;\frac{\partial \mathbf{X}}{\partial s}\cdot \frac{\partial \mathbf{X}}{\partial t}=0$$ and if so, what would this mean? Is this some kind of ""orthogonal unit speed"" parameterization or something? Cheers!",,['integration']
15,"Differentiation of $u(t)=\int_0^t h(s,t)ds, \ \forall t \in \mathbb{R}$ with the multivariable chain rule",Differentiation of  with the multivariable chain rule,"u(t)=\int_0^t h(s,t)ds, \ \forall t \in \mathbb{R}","Problem : Let $h: \mathbb{R}^2 \to \mathbb{R}$ be continuous and differentiable with respect to its second variable, define $u(t)= \displaystyle \int_0^t h(s,t)ds, \ t \in \mathbb{R}$ In an exercise before I already had to show that the above function is differentiable, meaning with respect to $x$ and $y$ by defining the function $f(x,y)= \int_0^x h(s,y)ds$ Now I am supposed to compute $u'(t)$ and I struggle still a lot with the multivariable chain rule. Theorem (Multivariable Chainrule): Let $f : \Omega \subset \mathbb{R}^n \to \mathbb{R}^m $ and $g: U \subset \mathbb{R}^m \to \mathbb{R}^l$ be differentiable functions. The composition $g \circ f$ is differentiable with derivative $$d(g \circ f ) (x) = dg(f(x)) \cdot df(x) $$ I know that the dot ""$\cdot$"" in the theorem actually stand for Matrix multiplication, which makes intuitive sense to me since we're working in $\mathbb{R}^n$. However I can't really grasp the context yet. My (intuitive) approach : In the exercise on the whether or not $u$ is differentiable I had to define $ f(x,y)= \int_0^x h(s,y)ds$ and I managed to show that $$ \partial_yf = \int_0^x \partial_yh(s,y)ds \\ \partial_xf = h(z,y) $$ and I am not quite sure how this is supposed to help me with finding the derivative of $u$. Following my still dominant instinct from single variable Calculus I would just have said that $$u'(t)= h(t,t) \cdot \partial_th(s,t)ds $$ Addition : I also thought about defining the below composition:  \begin{align} \begin{cases} \begin{matrix} \mathbb{R}^2 & \overset{h}{\longrightarrow} &\mathbb{R} & \overset{p}{\longrightarrow}& \mathbb{R} \\  (s,y) & \longmapsto &h(s,y)  \\ & & h(s,y) & \longmapsto & \displaystyle \int_0^t h(s,t)ds \end{matrix} \end{cases} \end{align} which might help me more to understand what's going on. I would appreciate one or two paragraphs that might help me to grasp the concept of the multivariable chain rule.","Problem : Let $h: \mathbb{R}^2 \to \mathbb{R}$ be continuous and differentiable with respect to its second variable, define $u(t)= \displaystyle \int_0^t h(s,t)ds, \ t \in \mathbb{R}$ In an exercise before I already had to show that the above function is differentiable, meaning with respect to $x$ and $y$ by defining the function $f(x,y)= \int_0^x h(s,y)ds$ Now I am supposed to compute $u'(t)$ and I struggle still a lot with the multivariable chain rule. Theorem (Multivariable Chainrule): Let $f : \Omega \subset \mathbb{R}^n \to \mathbb{R}^m $ and $g: U \subset \mathbb{R}^m \to \mathbb{R}^l$ be differentiable functions. The composition $g \circ f$ is differentiable with derivative $$d(g \circ f ) (x) = dg(f(x)) \cdot df(x) $$ I know that the dot ""$\cdot$"" in the theorem actually stand for Matrix multiplication, which makes intuitive sense to me since we're working in $\mathbb{R}^n$. However I can't really grasp the context yet. My (intuitive) approach : In the exercise on the whether or not $u$ is differentiable I had to define $ f(x,y)= \int_0^x h(s,y)ds$ and I managed to show that $$ \partial_yf = \int_0^x \partial_yh(s,y)ds \\ \partial_xf = h(z,y) $$ and I am not quite sure how this is supposed to help me with finding the derivative of $u$. Following my still dominant instinct from single variable Calculus I would just have said that $$u'(t)= h(t,t) \cdot \partial_th(s,t)ds $$ Addition : I also thought about defining the below composition:  \begin{align} \begin{cases} \begin{matrix} \mathbb{R}^2 & \overset{h}{\longrightarrow} &\mathbb{R} & \overset{p}{\longrightarrow}& \mathbb{R} \\  (s,y) & \longmapsto &h(s,y)  \\ & & h(s,y) & \longmapsto & \displaystyle \int_0^t h(s,t)ds \end{matrix} \end{cases} \end{align} which might help me more to understand what's going on. I would appreciate one or two paragraphs that might help me to grasp the concept of the multivariable chain rule.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'self-learning']"
16,Video lectures and reference book Multivariable calculus,Video lectures and reference book Multivariable calculus,,"I am in a particular situation that I am doing Master's in a Computer Science related degree, and I would like to take the course on Convex Optimisation which is taught by the Machine Learning department of our University. I did my undegrad almost 10 years ago and my memories of maths courses are quite sparse. The pre-requirements for this course are Linear Algebra and Multivariable Calculus. As for the former, I have already been going through Gilbert Strang's OCW lectures and working the problems from his book. The latter is what my question is about - I've got some disjoint knowledge of Calculus already, and also I have been going through the Spivak's Calculus book. I have considered 2 options so far: Multivarable Calculus at UC Berkeley by Edward Frenkel There is no syllabus there, but the coursebook is J. Stewart, Calculus: Early Transcendentals. Searching on the web yielded not so good reviews of this book MIT OCW Multivariable Calculus by Denis Auroux My question is what would be the most optimal way to get up to speed with Multivariable Calculus. In particular professor mentioned things we need to understand such as Vector spaces, Taylor theorem for multivariable case, level sets. I already have a fairly good understanding of Taylor expansion in single variable case, and I can imagine how it could be generalised to many variables, but I still need to do my homework. I am not necesserily looking to cherry pick certain topics and be done with it, I would like to learn everything properly, but maybe you can advice me something considering the time constraints that I have. Thank you. Update: Subsequently I found Massively Multivariable Open Online Calculus Course by Ohio State University at Coursera and I am really liking it. It is reach in examples, builds up intuition, but also provides formal proofs where necessary. It is largely a text based course, without any video lectures, but it does not bother me, as long as it guides me through. As suggested by others I will also go through the lectures at OCW.","I am in a particular situation that I am doing Master's in a Computer Science related degree, and I would like to take the course on Convex Optimisation which is taught by the Machine Learning department of our University. I did my undegrad almost 10 years ago and my memories of maths courses are quite sparse. The pre-requirements for this course are Linear Algebra and Multivariable Calculus. As for the former, I have already been going through Gilbert Strang's OCW lectures and working the problems from his book. The latter is what my question is about - I've got some disjoint knowledge of Calculus already, and also I have been going through the Spivak's Calculus book. I have considered 2 options so far: Multivarable Calculus at UC Berkeley by Edward Frenkel There is no syllabus there, but the coursebook is J. Stewart, Calculus: Early Transcendentals. Searching on the web yielded not so good reviews of this book MIT OCW Multivariable Calculus by Denis Auroux My question is what would be the most optimal way to get up to speed with Multivariable Calculus. In particular professor mentioned things we need to understand such as Vector spaces, Taylor theorem for multivariable case, level sets. I already have a fairly good understanding of Taylor expansion in single variable case, and I can imagine how it could be generalised to many variables, but I still need to do my homework. I am not necesserily looking to cherry pick certain topics and be done with it, I would like to learn everything properly, but maybe you can advice me something considering the time constraints that I have. Thank you. Update: Subsequently I found Massively Multivariable Open Online Calculus Course by Ohio State University at Coursera and I am really liking it. It is reach in examples, builds up intuition, but also provides formal proofs where necessary. It is largely a text based course, without any video lectures, but it does not bother me, as long as it guides me through. As suggested by others I will also go through the lectures at OCW.",,"['multivariable-calculus', 'reference-request', 'online-resources']"
17,Help changing the order of integration,Help changing the order of integration,,"So I need to change the order of integration. I am giving the following limits, $1 \leq x \leq 9$ and $\sqrt{x} \leq y \leq 4$. I am having no luck solving this one. Any help would be greatly appreciated!","So I need to change the order of integration. I am giving the following limits, $1 \leq x \leq 9$ and $\sqrt{x} \leq y \leq 4$. I am having no luck solving this one. Any help would be greatly appreciated!",,"['integration', 'multivariable-calculus']"
18,"Tangent plane to a function $z=f(x,y)$",Tangent plane to a function,"z=f(x,y)","I'm given with the following question: Part A: prove that for the tangent plane to a function $z=f(x,y)$ at a point $(x_0,y_0,z_0)$ is given by $z=f(x_0,y_0) + f_x(x_0,y_0)(x-x_0) +f_y(x_0,y_0)(y-y_0)$  (I did it) Part B: prove that if $f$ is differentiable at $(x_0,y_0)$ then the tangent plane mentioned above contains the tangent line to every curve lying inside the surface $z=f(x,y)$ at the point $(x_0,y_0, f(x_0,y_0))$ . I can't understand how to solve part B. Such a curve has a parameterization $\gamma(t) =(x(t), y(t) , f(x(t),y(t)) ) $  and by differentiating I get: $\gamma'(t) = (\gamma_x ,\gamma_y ,\gamma_z) \cdot (x'(t),y'(t), f'(x(t),y(t) ) ) $ and I can't understand why this should lie in the tangent plane I found in part A. I guess I should prove that $x'(t)= x-x_0,...$ but can't understand why this is true Thanks in advance","I'm given with the following question: Part A: prove that for the tangent plane to a function $z=f(x,y)$ at a point $(x_0,y_0,z_0)$ is given by $z=f(x_0,y_0) + f_x(x_0,y_0)(x-x_0) +f_y(x_0,y_0)(y-y_0)$  (I did it) Part B: prove that if $f$ is differentiable at $(x_0,y_0)$ then the tangent plane mentioned above contains the tangent line to every curve lying inside the surface $z=f(x,y)$ at the point $(x_0,y_0, f(x_0,y_0))$ . I can't understand how to solve part B. Such a curve has a parameterization $\gamma(t) =(x(t), y(t) , f(x(t),y(t)) ) $  and by differentiating I get: $\gamma'(t) = (\gamma_x ,\gamma_y ,\gamma_z) \cdot (x'(t),y'(t), f'(x(t),y(t) ) ) $ and I can't understand why this should lie in the tangent plane I found in part A. I guess I should prove that $x'(t)= x-x_0,...$ but can't understand why this is true Thanks in advance",,['multivariable-calculus']
19,Is the definite integral of the function necessarily the anti-derivative?,Is the definite integral of the function necessarily the anti-derivative?,,"Let's say you have a function defined as $$g(x)=\int_1^xf(t)dt$$ By the integral definition, g(x) is the area under the curve of f(x) from 1 to x. eg: g(5) is the area under f(x) from 1 to 5. I want to make the distinction that g(x) is NOT defined as an indefinite integral like $$g(x)=\int f(t)dt$$ Now, let's say I am given some arbitrary graph of f. With the first definition (definite integrals), is g(x) still considered the anti-derivative of f(x)?    This matters when exploring ideas about concavity and increase/decrease, and needing to examine the derivatives of g(x).   Based on the F.T.C., I know that the derivative of the integral is the function itself.  Can we say g'(x) = f(x) and that g''(x) = f'(x)?   I know I can say this if g(x) was defined the 2nd way, with indefinite integrals. But, can I say the same thing when given the limits of integration of $\int_1^x f(t)dt$ ?  Or, is g(x) now specifically defined strictly as the area under the curve of f(x) from 1 to x?   The 2 different types of integrals (definite vs indefinite) seem to mean different things.  Only the 2nd one seems to fit into the antiderivative model (vs. being a specifically defined function.)  I guess I am not seeing the connection b/w the two.  Let me know if I am unclear.","Let's say you have a function defined as $$g(x)=\int_1^xf(t)dt$$ By the integral definition, g(x) is the area under the curve of f(x) from 1 to x. eg: g(5) is the area under f(x) from 1 to 5. I want to make the distinction that g(x) is NOT defined as an indefinite integral like $$g(x)=\int f(t)dt$$ Now, let's say I am given some arbitrary graph of f. With the first definition (definite integrals), is g(x) still considered the anti-derivative of f(x)?    This matters when exploring ideas about concavity and increase/decrease, and needing to examine the derivatives of g(x).   Based on the F.T.C., I know that the derivative of the integral is the function itself.  Can we say g'(x) = f(x) and that g''(x) = f'(x)?   I know I can say this if g(x) was defined the 2nd way, with indefinite integrals. But, can I say the same thing when given the limits of integration of $\int_1^x f(t)dt$ ?  Or, is g(x) now specifically defined strictly as the area under the curve of f(x) from 1 to x?   The 2 different types of integrals (definite vs indefinite) seem to mean different things.  Only the 2nd one seems to fit into the antiderivative model (vs. being a specifically defined function.)  I guess I am not seeing the connection b/w the two.  Let me know if I am unclear.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'indefinite-integrals']"
20,GRE Mathematics Multivariable Calculus,GRE Mathematics Multivariable Calculus,,"I'm studying for the GRE subject test on 4/5, and I've been going through the princeton review book, which seems to be the canonical guide. In the multivariable section the theorems are pretty basic, they only mention green's theorem, and omit Stokes, Gausses Theorem, Surface integrals, if not more. Is this because these theorems are not tested on the subject test? In that vein does anybody know of a more comprehensive list of topics other than what is on the ets site? Thanks.","I'm studying for the GRE subject test on 4/5, and I've been going through the princeton review book, which seems to be the canonical guide. In the multivariable section the theorems are pretty basic, they only mention green's theorem, and omit Stokes, Gausses Theorem, Surface integrals, if not more. Is this because these theorems are not tested on the subject test? In that vein does anybody know of a more comprehensive list of topics other than what is on the ets site? Thanks.",,"['multivariable-calculus', 'gre-exam']"
21,An immersive map is locally left invertible,An immersive map is locally left invertible,,"Question: Suppose that $m < n$, that $U$ is an open set in $\mathbb R^m$ and that $f : U \rightarrow \mathbb R^n$ is a $C^1$ function that has rank $m$ everywhere in $U$. Show that for every $x$ in $U$ there is a neighborhood $V$ of $f(x)$ and a $C^1$ function $\tilde f : V \rightarrow \mathbb R^m$ that is a left inverse of $f$ in $V$ (that is, $\tilde f\circ  f$ is the identity on $V$). I do not understand the last sentence of this question. What does it mean by identity on $V$? Shouldn't the domain of $\tilde f\circ f$ is at least in $\mathbb R^m$ instead of $V$ which is in $\mathbb R^n$? Which of the following two is correct, please? $(\tilde f\circ f)(x^1,\cdots, x^m)=(x^1,\cdots, x^m)$; or $(\tilde f\circ f)(x^1,\cdots, x^m)=(x^1,\cdots, x^m, 0, \cdots, 0)$. In addition, I started with trying constant rank theorem, but it did not seem to work since rank theorem only change coordinates in $\mathbb R^m$ and $\mathbb R^n$ without connecting them. Any hint, please?","Question: Suppose that $m < n$, that $U$ is an open set in $\mathbb R^m$ and that $f : U \rightarrow \mathbb R^n$ is a $C^1$ function that has rank $m$ everywhere in $U$. Show that for every $x$ in $U$ there is a neighborhood $V$ of $f(x)$ and a $C^1$ function $\tilde f : V \rightarrow \mathbb R^m$ that is a left inverse of $f$ in $V$ (that is, $\tilde f\circ  f$ is the identity on $V$). I do not understand the last sentence of this question. What does it mean by identity on $V$? Shouldn't the domain of $\tilde f\circ f$ is at least in $\mathbb R^m$ instead of $V$ which is in $\mathbb R^n$? Which of the following two is correct, please? $(\tilde f\circ f)(x^1,\cdots, x^m)=(x^1,\cdots, x^m)$; or $(\tilde f\circ f)(x^1,\cdots, x^m)=(x^1,\cdots, x^m, 0, \cdots, 0)$. In addition, I started with trying constant rank theorem, but it did not seem to work since rank theorem only change coordinates in $\mathbb R^m$ and $\mathbb R^n$ without connecting them. Any hint, please?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
22,"Compute $\int_C\sin y\,dx+(x\cos y-\sin y)\,dy$",Compute,"\int_C\sin y\,dx+(x\cos y-\sin y)\,dy","Compute $\int_C\sin y\,dx+(x\cos y-\sin y)\,dy$ where $C$ is $\displaystyle \frac {x^2}{4}+\frac {y^2}{2}=1$ in the first quadrant counter clockwise. I set $x=2\cos \theta$ and $y=\sqrt{2}\sin\theta$ where $0\leq\theta\leq \frac {\pi}{2}$. But when I plug $x$ and $y$ back into the integral I am unable to solve it. Please drop some hints. Thanks. EDIT: $\displaystyle\int_{\Gamma}=\iint_D(\frac {\partial Q}{\partial x}-\frac {\partial P}{\partial y})\,dA=\int_0^2\int_0^{\sqrt{2}}2\cos y\,dy\,dx=\pi$ $\int_{c_1}+\int_{c_2}=\int_0^20+\int_0^\sqrt{2}-\sin t\,dt$, where $C_1:$ $x=t,\,dx=dt, y=0,dy=0\,dt, 0\leq t\leq 2$ $C_2: y=t, dy=\,dt, x=0, dx=0\,dt, 0\leq t\leq \sqrt{2}$ So $\displaystyle \int_c=\int_{\Gamma}-\int_{c_1}-\int_{c_2}=\pi-0-\frac {\pi}{4}+1=\frac {3\pi}{4}+1$","Compute $\int_C\sin y\,dx+(x\cos y-\sin y)\,dy$ where $C$ is $\displaystyle \frac {x^2}{4}+\frac {y^2}{2}=1$ in the first quadrant counter clockwise. I set $x=2\cos \theta$ and $y=\sqrt{2}\sin\theta$ where $0\leq\theta\leq \frac {\pi}{2}$. But when I plug $x$ and $y$ back into the integral I am unable to solve it. Please drop some hints. Thanks. EDIT: $\displaystyle\int_{\Gamma}=\iint_D(\frac {\partial Q}{\partial x}-\frac {\partial P}{\partial y})\,dA=\int_0^2\int_0^{\sqrt{2}}2\cos y\,dy\,dx=\pi$ $\int_{c_1}+\int_{c_2}=\int_0^20+\int_0^\sqrt{2}-\sin t\,dt$, where $C_1:$ $x=t,\,dx=dt, y=0,dy=0\,dt, 0\leq t\leq 2$ $C_2: y=t, dy=\,dt, x=0, dx=0\,dt, 0\leq t\leq \sqrt{2}$ So $\displaystyle \int_c=\int_{\Gamma}-\int_{c_1}-\int_{c_2}=\pi-0-\frac {\pi}{4}+1=\frac {3\pi}{4}+1$",,"['integration', 'multivariable-calculus']"
23,Multivariable Calculus Help,Multivariable Calculus Help,,"Prove that if the function z is defined implicitly by $$F( x - az, y -bz ) = 0$$  where F has continuous partial derivatives, then $$a\frac{\partial z}{\partial x}+b\frac{\partial z}{\partial y} = 1$$ I have a hunch that I must somehow use the Implicit Function Theorem, but I am not exactly sure how to apply it in this case. Any suggestions?","Prove that if the function z is defined implicitly by $$F( x - az, y -bz ) = 0$$  where F has continuous partial derivatives, then $$a\frac{\partial z}{\partial x}+b\frac{\partial z}{\partial y} = 1$$ I have a hunch that I must somehow use the Implicit Function Theorem, but I am not exactly sure how to apply it in this case. Any suggestions?",,"['multivariable-calculus', 'partial-derivative', 'implicit-function-theorem']"
24,Vector field on sphere,Vector field on sphere,,"I want to find the gradient vector field and flows of the function $f=x^2+2y^2+3z^2$ on the sphere $S^2$, however I've not done this in a while so would appreciate a bit of help. I'd like to see the vector field in the $(u,v)$ plane. I've parametrised the sphere with: $\varphi_1(u,v)=\left(\pm \sqrt{1-u^2-v^2},u,v \right)$, $\varphi_2(u,v)=\left(u,\pm \sqrt{1-u^2-v^2},v \right)$, $\varphi_3(u,v)=\left(u,v,\pm \sqrt{1-u^2-v^2} \right)$. So: $\left(f \circ \varphi_1 \right)(u,v)=1+u^2+2v^2$, $\left(f \circ \varphi_2 \right)(u,v)=2-u^2-2v^2$, $\left(f \circ \varphi_3 \right)(u,v)=3-2u^2-v^2$ Thus: $d(f \circ \varphi_1)=(2u,4v)$, $d(f \circ \varphi_2)=(-2u,-4v)$ $d(f \circ \varphi_3)=(-4u,-2v)$ Now I'm stuck. This isn't giving me anything that looks sensible. I feel that my problem is in the parametrisation. Thanks for any help.","I want to find the gradient vector field and flows of the function $f=x^2+2y^2+3z^2$ on the sphere $S^2$, however I've not done this in a while so would appreciate a bit of help. I'd like to see the vector field in the $(u,v)$ plane. I've parametrised the sphere with: $\varphi_1(u,v)=\left(\pm \sqrt{1-u^2-v^2},u,v \right)$, $\varphi_2(u,v)=\left(u,\pm \sqrt{1-u^2-v^2},v \right)$, $\varphi_3(u,v)=\left(u,v,\pm \sqrt{1-u^2-v^2} \right)$. So: $\left(f \circ \varphi_1 \right)(u,v)=1+u^2+2v^2$, $\left(f \circ \varphi_2 \right)(u,v)=2-u^2-2v^2$, $\left(f \circ \varphi_3 \right)(u,v)=3-2u^2-v^2$ Thus: $d(f \circ \varphi_1)=(2u,4v)$, $d(f \circ \varphi_2)=(-2u,-4v)$ $d(f \circ \varphi_3)=(-4u,-2v)$ Now I'm stuck. This isn't giving me anything that looks sensible. I feel that my problem is in the parametrisation. Thanks for any help.",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
25,On the hessian matrix and relative minima,On the hessian matrix and relative minima,,"I'm asked to prove the following statement: Let $\mathbb{A} \subseteq \mathbb{R}^n$ an open subset and $f: \mathbb{A} \to \mathbb{R} / f \in \mathbb{C}^3 \wedge (P \in \mathbb{A} : \nabla f(P)=0)$. Suppose the hessian matrix of f is defined positive, prove that P is a strict relative minima of f. A similar proof has already been posted here , however, there is a difference and I'd like to know whether my approach is right. Approximating $f$ by its Taylor's polynomial of grade 2 centered on P, we get the following expression: $$f(x) = f(P) + \frac{1}{2} \langle H_f(P) \cdot (x-P), x-P \rangle + R(P) \cdot (x-P)$$ where $R(P) \cdot (x-P) = \frac{1}{6}\langle D^3_f(c) \cdot (x-P), x-P\rangle$, c being some point between x and p. By replacing $x$ with $P + tV$, $t \in \mathbb{R} \wedge V \in \mathbb{A}$, we get: $$f(P + tV) = f(P) + \frac{1}{2} \langle H_f(P) \cdot (tV), tV \rangle + R(P) \cdot (tV)$$ Thus, by taking $t$ as a common factor from the vector product in the second term, we get: $$f(P + tV) = f(P) + \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle + R(P) \cdot (tV)$$ By definition, P is a strict local minima iif $f(P) < f(P + tV)$ when $tV$ is sufficiently close to 0, hence replacing the equation we get: $$0 < \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle + R(P) \cdot (tV)$$ And since $\langle H_f(P) \cdot V, V \rangle$ is the quadratic form associated to $H_f(P)$, which by hypothesis is defined positive (therefore being higher than 0), we know that the first term is higher than 0. It is the second term I'm having trouble with: how can I prove that $R(P) \cdot (tV) \ll \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle$ and thus it can't make the statement false?. I'm also worried about how rigorous this proof is, since I'm not entirely sure that approaching the problem with the $x = P + tV$ replacement is entirely correct.","I'm asked to prove the following statement: Let $\mathbb{A} \subseteq \mathbb{R}^n$ an open subset and $f: \mathbb{A} \to \mathbb{R} / f \in \mathbb{C}^3 \wedge (P \in \mathbb{A} : \nabla f(P)=0)$. Suppose the hessian matrix of f is defined positive, prove that P is a strict relative minima of f. A similar proof has already been posted here , however, there is a difference and I'd like to know whether my approach is right. Approximating $f$ by its Taylor's polynomial of grade 2 centered on P, we get the following expression: $$f(x) = f(P) + \frac{1}{2} \langle H_f(P) \cdot (x-P), x-P \rangle + R(P) \cdot (x-P)$$ where $R(P) \cdot (x-P) = \frac{1}{6}\langle D^3_f(c) \cdot (x-P), x-P\rangle$, c being some point between x and p. By replacing $x$ with $P + tV$, $t \in \mathbb{R} \wedge V \in \mathbb{A}$, we get: $$f(P + tV) = f(P) + \frac{1}{2} \langle H_f(P) \cdot (tV), tV \rangle + R(P) \cdot (tV)$$ Thus, by taking $t$ as a common factor from the vector product in the second term, we get: $$f(P + tV) = f(P) + \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle + R(P) \cdot (tV)$$ By definition, P is a strict local minima iif $f(P) < f(P + tV)$ when $tV$ is sufficiently close to 0, hence replacing the equation we get: $$0 < \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle + R(P) \cdot (tV)$$ And since $\langle H_f(P) \cdot V, V \rangle$ is the quadratic form associated to $H_f(P)$, which by hypothesis is defined positive (therefore being higher than 0), we know that the first term is higher than 0. It is the second term I'm having trouble with: how can I prove that $R(P) \cdot (tV) \ll \frac{t^2}{2} \langle H_f(P) \cdot V, V \rangle$ and thus it can't make the statement false?. I'm also worried about how rigorous this proof is, since I'm not entirely sure that approaching the problem with the $x = P + tV$ replacement is entirely correct.",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion', 'quadratic-forms']"
26,Show that the sequence $\Omega^0\Bbb{R}^2\ \longrightarrow\ \Omega^1\Bbb{R}^2\ \longrightarrow\ \Omega^2\Bbb{R}^2$ is exact.,Show that the sequence  is exact.,\Omega^0\Bbb{R}^2\ \longrightarrow\ \Omega^1\Bbb{R}^2\ \longrightarrow\ \Omega^2\Bbb{R}^2,"I have been posed the following question, which I am unable to answer: Let $a_1,a_2\in\mathcal{C}^{\infty}(\Bbb{R}^2,\Bbb{R})$ be infinitely differentiable functions such that $\frac{\partial a_1}{\partial x_2}=\frac{\partial a_2}{\partial x_1}$, and define   $$f(x,y):=x_1\cdot\int_0^1a_1(tx_1,tx_2)\ dt+x_2\cdot\int_0^1a_2(tx_1,tx_2)\ dt.$$   Show that $\tfrac{\partial f}{\partial x_i}=a_i$ for $i\in\{1,2\}$. Unfortunately I lack even the most basic knowledge of calculus. Here's my attempt at a solution: First by linearity of differentiation and second by the product rule and then cleaning up we find \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\frac{\partial}{\partial x_1}\left(x_1\cdot\int_0^1a_1(tx_1,tx_2)\ dt\right)+\frac{\partial}{\partial x_1}\left(x_2\cdot\int_0^1a_2(tx_1,tx_2)\ dt\right)\\ &=x_1\cdot\frac{\partial}{\partial x_1}\left(\int_0^1a_1(tx_1,tx_2)\ dt\right)+\int_0^1a_1(tx_1,tx_2)\ dt\cdot\frac{\partial x_1}{\partial x_1}\\ &\hspace{10pt}+x_2\cdot\frac{\partial}{\partial x_1}\left(\int_0^1a_2(tx_1,tx_2)\ dt\right)+\int_0^1a_2(tx_1,tx_2)\ dt\cdot\frac{\partial x_2}{\partial x_1}\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\frac{\partial}{\partial x_1}\int_0^1a_1(tx_1,tx_2)\ dt+x_2\cdot\frac{\partial}{\partial x_1}\int_0^1a_2(tx_1,tx_2)\ dt. \end{align*} Now changing the order of integration and differentiation, which is allowed for whatever reason, and subsequently applying the chain rule and substituting $\frac{\partial a_1}{\partial x_2}=\frac{\partial a_2}{\partial x_1}$ we find that \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1(tx_1,tx_2)}{\partial x_1}\ dt+x_2\cdot\int_0^1\frac{\partial a_2(tx_1,tx_2)}{\partial x_1}\ dt\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\cdot t\ dt+x_2\cdot\int_0^1\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\cdot t\ dt\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\cdot t\ dt+x_2\cdot\int_0^1\frac{\partial a_1}{\partial x_2}(tx_1,tx_2)\cdot t\ dt. \end{align*} These integrals look good for integration by parts with respect to $t$ and $\tfrac{\partial a_i}{\partial x_1}(tx_1,tx_2)$, which yields \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\left(\left[t\cdot\int\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\ dt\right]_0^1-\int_0^1\int\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\ dt\ dt\right)\\ &\hspace{10pt}+x_2\cdot\left(\left[t\cdot\int\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\ dt\right]_0^1-\int_0^1\int\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\ dt\ dt\right).\\ \end{align*} This is where my bright ideas end; I've juggled the terms around a bit but I see no way of evaluating any of these expressions. Any help would be much appreciated.","I have been posed the following question, which I am unable to answer: Let $a_1,a_2\in\mathcal{C}^{\infty}(\Bbb{R}^2,\Bbb{R})$ be infinitely differentiable functions such that $\frac{\partial a_1}{\partial x_2}=\frac{\partial a_2}{\partial x_1}$, and define   $$f(x,y):=x_1\cdot\int_0^1a_1(tx_1,tx_2)\ dt+x_2\cdot\int_0^1a_2(tx_1,tx_2)\ dt.$$   Show that $\tfrac{\partial f}{\partial x_i}=a_i$ for $i\in\{1,2\}$. Unfortunately I lack even the most basic knowledge of calculus. Here's my attempt at a solution: First by linearity of differentiation and second by the product rule and then cleaning up we find \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\frac{\partial}{\partial x_1}\left(x_1\cdot\int_0^1a_1(tx_1,tx_2)\ dt\right)+\frac{\partial}{\partial x_1}\left(x_2\cdot\int_0^1a_2(tx_1,tx_2)\ dt\right)\\ &=x_1\cdot\frac{\partial}{\partial x_1}\left(\int_0^1a_1(tx_1,tx_2)\ dt\right)+\int_0^1a_1(tx_1,tx_2)\ dt\cdot\frac{\partial x_1}{\partial x_1}\\ &\hspace{10pt}+x_2\cdot\frac{\partial}{\partial x_1}\left(\int_0^1a_2(tx_1,tx_2)\ dt\right)+\int_0^1a_2(tx_1,tx_2)\ dt\cdot\frac{\partial x_2}{\partial x_1}\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\frac{\partial}{\partial x_1}\int_0^1a_1(tx_1,tx_2)\ dt+x_2\cdot\frac{\partial}{\partial x_1}\int_0^1a_2(tx_1,tx_2)\ dt. \end{align*} Now changing the order of integration and differentiation, which is allowed for whatever reason, and subsequently applying the chain rule and substituting $\frac{\partial a_1}{\partial x_2}=\frac{\partial a_2}{\partial x_1}$ we find that \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1(tx_1,tx_2)}{\partial x_1}\ dt+x_2\cdot\int_0^1\frac{\partial a_2(tx_1,tx_2)}{\partial x_1}\ dt\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\cdot t\ dt+x_2\cdot\int_0^1\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\cdot t\ dt\\ &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\int_0^1\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\cdot t\ dt+x_2\cdot\int_0^1\frac{\partial a_1}{\partial x_2}(tx_1,tx_2)\cdot t\ dt. \end{align*} These integrals look good for integration by parts with respect to $t$ and $\tfrac{\partial a_i}{\partial x_1}(tx_1,tx_2)$, which yields \begin{align*} \frac{\partial f}{\partial x_1}(x_1,x_2) &=\int_0^1a_1(tx_1,tx_2)\ dt+x_1\cdot\left(\left[t\cdot\int\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\ dt\right]_0^1-\int_0^1\int\frac{\partial a_1}{\partial x_1}(tx_1,tx_2)\ dt\ dt\right)\\ &\hspace{10pt}+x_2\cdot\left(\left[t\cdot\int\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\ dt\right]_0^1-\int_0^1\int\frac{\partial a_2}{\partial x_1}(tx_1,tx_2)\ dt\ dt\right).\\ \end{align*} This is where my bright ideas end; I've juggled the terms around a bit but I see no way of evaluating any of these expressions. Any help would be much appreciated.",,"['integration', 'multivariable-calculus']"
27,A silly mistake concerning spherical coordinates and unit vectors...,A silly mistake concerning spherical coordinates and unit vectors...,,"I'm quite comfortable with vector calculus in all sorts of coordinate systems, but for the love of me, I can't seem to figure out where did I go wrong in this simple derivation of the position vector in spherical coordinates. Maybe my morning coffee didn't kick in yet, but I'd still appreciate your help! So, using spherical coordinates $(r, \theta, \phi)$, we can write the Cartesian position vector $$\vec{r} = x \hat{i} + y \hat{j} + z \hat{k}$$ as $$\vec{r} = (r \sin{\theta} \cos{\phi}) \hat{i} + (r \sin{\theta} \sin{\phi}) \hat{j} + (r \cos{\theta}) \hat{k}$$ In the next step, I substitute for Cartesian unit vectors expressed in spherical unit vectors, i.e. $$\hat{i} = (\sin{\theta} \cos{\phi} )\hat{r} + (\cos{\theta} \cos{\phi} )\hat{\theta} - (\sin{\theta})\hat{\phi}$$ $$\hat{j} = (\sin{\theta} \sin{\phi} )\hat{r} + (\cos{\theta} \sin{\phi} )\hat{\theta} + (\cos{\theta})\hat{\phi}$$ $$\hat{k} = (\cos{\theta}) \hat{r} - (\sin{\theta})\hat{\theta}$$ So, when I do that, you can see that my $\hat{\phi}$ terms don't cancel out and I don't get $$ \vec{r} = r\hat{r},$$ which is what I should get. Help me spot the mistake, my brain refuses to cooperate today.","I'm quite comfortable with vector calculus in all sorts of coordinate systems, but for the love of me, I can't seem to figure out where did I go wrong in this simple derivation of the position vector in spherical coordinates. Maybe my morning coffee didn't kick in yet, but I'd still appreciate your help! So, using spherical coordinates $(r, \theta, \phi)$, we can write the Cartesian position vector $$\vec{r} = x \hat{i} + y \hat{j} + z \hat{k}$$ as $$\vec{r} = (r \sin{\theta} \cos{\phi}) \hat{i} + (r \sin{\theta} \sin{\phi}) \hat{j} + (r \cos{\theta}) \hat{k}$$ In the next step, I substitute for Cartesian unit vectors expressed in spherical unit vectors, i.e. $$\hat{i} = (\sin{\theta} \cos{\phi} )\hat{r} + (\cos{\theta} \cos{\phi} )\hat{\theta} - (\sin{\theta})\hat{\phi}$$ $$\hat{j} = (\sin{\theta} \sin{\phi} )\hat{r} + (\cos{\theta} \sin{\phi} )\hat{\theta} + (\cos{\theta})\hat{\phi}$$ $$\hat{k} = (\cos{\theta}) \hat{r} - (\sin{\theta})\hat{\theta}$$ So, when I do that, you can see that my $\hat{\phi}$ terms don't cancel out and I don't get $$ \vec{r} = r\hat{r},$$ which is what I should get. Help me spot the mistake, my brain refuses to cooperate today.",,"['multivariable-calculus', 'vector-analysis']"
28,Solve Helmholtz equation,Solve Helmholtz equation,,"$$U_{xx}+U_{yy}+k^2U=0$$ Solve by separation of variables by assuming $u(x,y)=X(x)Y(y)$ with the following conditions:   $$ U(0,y)=0,\,\, U(2,y)=0,\,\, U(x,0)=0,\,\, U(x,1)=0, $$ This is what I have done so far: $$ X''(x)Y(y)+X(x)Y''(y)+k^2X(x)Y(y)=0 $$ To separate divide through by $X(x)Y(y)$, this gives $$ X''(x)/X(x)  +   Y''(y)/Y(y)   +k^2  =0 $$ Now I am not sure where to go from here. I know that if I take over the $y$'s then it will be separated, but where does the $k^2$ go? Does it even matter to the aux. equation? What I thought is that I can just set $X''(x)/X(x) =k^2$ and $Y''(y)/Y(y) =k^2$, but I think I am wrong. Can someone help?  Thank you","$$U_{xx}+U_{yy}+k^2U=0$$ Solve by separation of variables by assuming $u(x,y)=X(x)Y(y)$ with the following conditions:   $$ U(0,y)=0,\,\, U(2,y)=0,\,\, U(x,0)=0,\,\, U(x,1)=0, $$ This is what I have done so far: $$ X''(x)Y(y)+X(x)Y''(y)+k^2X(x)Y(y)=0 $$ To separate divide through by $X(x)Y(y)$, this gives $$ X''(x)/X(x)  +   Y''(y)/Y(y)   +k^2  =0 $$ Now I am not sure where to go from here. I know that if I take over the $y$'s then it will be separated, but where does the $k^2$ go? Does it even matter to the aux. equation? What I thought is that I can just set $X''(x)/X(x) =k^2$ and $Y''(y)/Y(y) =k^2$, but I think I am wrong. Can someone help?  Thank you",,"['multivariable-calculus', 'partial-differential-equations']"
29,Vector by Matrix derivative,Vector by Matrix derivative,,"According to wikipedia, there is no widely accepted definition of a Vector by Matrix derivative. I have a need of such a notion. For matrix w, and vector h. $$\mathbf{y=w \;h}  $$ $$ \begin{bmatrix}y_{0}\\ y_{1}\\ \vdots\\ y_{n} \end{bmatrix}=\begin{bmatrix}w_{0,0} & w_{0,1} & \cdots & w_{0,k}\\ w_{0,0} & w_{1,1} & \cdots & w_{1,k}\\ \vdots & \cdots & \cdots & \vdots\\ w_{n,0} & \cdots & \cdots & w_{n,k} \end{bmatrix}\begin{bmatrix}h_{0}\\ h_{1}\\ \vdots\\ h_{k} \end{bmatrix}=\begin{bmatrix}h_{0}w_{00}+h_{1}w_{01}...+h_{k}w_{0k}\\ h_{0}w_{10}+h_{1}w_{11}...+h_{k}w_{1k}\\ \vdots\\ h_{0}w_{n0}+h_{1}w_{n1}...+h_{k}w_{nk} \end{bmatrix} $$ It can be seen with a little thought that $$\frac{d\mathbf{y}}{d\mathbf{h}}=\mathbf{w}$$, which is to say w is a jacobian. However taking the element wise derivative of y with respect to each element of w is going to give me a number of sparse vectors, equal to the number of elements of w . I guess this could be groups into a 4d tensor, but i know nothing of them. Inparticular this looks like it is not going to work well with the chain-rule, which is essential to the larger problem i am trying to solve. $$ \begin{array}{cc} \frac{d\mathbf{y}}{dw_{00}}=\begin{bmatrix}h_{0}\\ 0\\ \vdots\\ 0 \end{bmatrix} & \frac{d\mathbf{y}}{dw_{10}}=\begin{bmatrix}0\\ h_{0}\\ \vdots\\ 0 \end{bmatrix}\\ \frac{d\mathbf{y}}{dw_{101}}=\begin{bmatrix}h_{1}\\ 0\\ \vdots\\ 0 \end{bmatrix} & \frac{d\mathbf{y}}{dw_{11}}=\begin{bmatrix}0\\ h_{1}\\ \vdots\\ 0 \end{bmatrix} \end{array} $$ Unless I can find a good notion of this derivative, I am going to have to stop thinking about the problem in terms of matrix products, and instead think of it a very large number of equations based around the members of the the matrix. Thinking about it this way loses me abstraction, and if i am not clever enough it will increase my algorithms exectution time when it comes time to run this though BLAS Software (Basic Linear Algabra System, is the special software that access the functionality of the CPU that can do matrix math quickly) I suspect thinking of it as elementwise equations is going to be my oldy way forward, most texts i've seen seem to be refering to these relationships, with definations like $$y_{i}=\sum_{0\le j\le k}h_{j}w_{i,j}  $$ So my question: Is there a good notion of a vector by matrix derivative? For reference I am attempting to rederive the backproperation learning algorithm for neural nets of arbitrary number of layers","According to wikipedia, there is no widely accepted definition of a Vector by Matrix derivative. I have a need of such a notion. For matrix w, and vector h. $$\mathbf{y=w \;h}  $$ $$ \begin{bmatrix}y_{0}\\ y_{1}\\ \vdots\\ y_{n} \end{bmatrix}=\begin{bmatrix}w_{0,0} & w_{0,1} & \cdots & w_{0,k}\\ w_{0,0} & w_{1,1} & \cdots & w_{1,k}\\ \vdots & \cdots & \cdots & \vdots\\ w_{n,0} & \cdots & \cdots & w_{n,k} \end{bmatrix}\begin{bmatrix}h_{0}\\ h_{1}\\ \vdots\\ h_{k} \end{bmatrix}=\begin{bmatrix}h_{0}w_{00}+h_{1}w_{01}...+h_{k}w_{0k}\\ h_{0}w_{10}+h_{1}w_{11}...+h_{k}w_{1k}\\ \vdots\\ h_{0}w_{n0}+h_{1}w_{n1}...+h_{k}w_{nk} \end{bmatrix} $$ It can be seen with a little thought that $$\frac{d\mathbf{y}}{d\mathbf{h}}=\mathbf{w}$$, which is to say w is a jacobian. However taking the element wise derivative of y with respect to each element of w is going to give me a number of sparse vectors, equal to the number of elements of w . I guess this could be groups into a 4d tensor, but i know nothing of them. Inparticular this looks like it is not going to work well with the chain-rule, which is essential to the larger problem i am trying to solve. $$ \begin{array}{cc} \frac{d\mathbf{y}}{dw_{00}}=\begin{bmatrix}h_{0}\\ 0\\ \vdots\\ 0 \end{bmatrix} & \frac{d\mathbf{y}}{dw_{10}}=\begin{bmatrix}0\\ h_{0}\\ \vdots\\ 0 \end{bmatrix}\\ \frac{d\mathbf{y}}{dw_{101}}=\begin{bmatrix}h_{1}\\ 0\\ \vdots\\ 0 \end{bmatrix} & \frac{d\mathbf{y}}{dw_{11}}=\begin{bmatrix}0\\ h_{1}\\ \vdots\\ 0 \end{bmatrix} \end{array} $$ Unless I can find a good notion of this derivative, I am going to have to stop thinking about the problem in terms of matrix products, and instead think of it a very large number of equations based around the members of the the matrix. Thinking about it this way loses me abstraction, and if i am not clever enough it will increase my algorithms exectution time when it comes time to run this though BLAS Software (Basic Linear Algabra System, is the special software that access the functionality of the CPU that can do matrix math quickly) I suspect thinking of it as elementwise equations is going to be my oldy way forward, most texts i've seen seem to be refering to these relationships, with definations like $$y_{i}=\sum_{0\le j\le k}h_{j}w_{i,j}  $$ So my question: Is there a good notion of a vector by matrix derivative? For reference I am attempting to rederive the backproperation learning algorithm for neural nets of arbitrary number of layers",,"['matrices', 'multivariable-calculus', 'derivatives', 'tensor-products', 'vectors']"
30,How can I calculate $\iint_S\frac{\bf x}{|{\bf x}|^3}\cdot d{\bf S}$ with a semisphere $S$ not centered at the origin?,How can I calculate  with a semisphere  not centered at the origin?,\iint_S\frac{\bf x}{|{\bf x}|^3}\cdot d{\bf S} S,"Let $$ F(x,y,z)=\frac{x{\bf i}+y{\bf j}+z{\bf k}}{(x^2+y^2+z^2)^{3/2}}.  $$   How can I calculate    $$ \iint_SF\cdot d{\bf S} $$   where $S$ a the ""upper semi-unit-sphere"" and  the boundary of $S$ given by   $$ \begin{cases} x+y+z=3\\ (x-1)^2+(y-1)^2+(z-1)^2=1 \end{cases}? $$ If I change the coordinate to make the equations of the boundary of the semisphere simpler: $$ z'=0,\quad x'^2+y'^2+z'^2=1, $$ then I messed up with $F(x',y',z')$. But if I don't change the coordinates, I messed up with the parameterization of the surface. Any idea? Does the integral $$\iint_S\frac{\bf x}{|{\bf x}|^3}\cdot d{\bf S}$$ have some meaning in physics? [Added] I didn't expect that my description of the surface in the integral is so difficult to be understood. Suppose we have a unit sphere centered at $(1,1,1)$  $$ \Omega=\{(x,y,z)\in{\Bbb R}^3:(x-1)^2+(y-1)^2+(z-1)^2=1\} $$ and the plane  $$ P=\{(x,y,z)\in{\Bbb R}^3:x+y+z=3\}. $$ Geometrically, the plane $P$ would cut the sphere $\Omega$ into two pieces and $S$ is one of them while I don't specify which one so that the result would be up to the choice of these two pieces. The confusion might due to my notation: $$ \begin{cases} x+y+z=3\\ (x-1)^2+(y-1)^2+(z-1)^2=1 \end{cases}. $$ which is equivalent to  $$ \{(x,y,z)\in{\Bbb R}^3:x+y+z=3\ {\bf and}\ (x-1)^2+(y-1)^2+(z-1)^2=1\} $$ which is the boundary of $S$.","Let $$ F(x,y,z)=\frac{x{\bf i}+y{\bf j}+z{\bf k}}{(x^2+y^2+z^2)^{3/2}}.  $$   How can I calculate    $$ \iint_SF\cdot d{\bf S} $$   where $S$ a the ""upper semi-unit-sphere"" and  the boundary of $S$ given by   $$ \begin{cases} x+y+z=3\\ (x-1)^2+(y-1)^2+(z-1)^2=1 \end{cases}? $$ If I change the coordinate to make the equations of the boundary of the semisphere simpler: $$ z'=0,\quad x'^2+y'^2+z'^2=1, $$ then I messed up with $F(x',y',z')$. But if I don't change the coordinates, I messed up with the parameterization of the surface. Any idea? Does the integral $$\iint_S\frac{\bf x}{|{\bf x}|^3}\cdot d{\bf S}$$ have some meaning in physics? [Added] I didn't expect that my description of the surface in the integral is so difficult to be understood. Suppose we have a unit sphere centered at $(1,1,1)$  $$ \Omega=\{(x,y,z)\in{\Bbb R}^3:(x-1)^2+(y-1)^2+(z-1)^2=1\} $$ and the plane  $$ P=\{(x,y,z)\in{\Bbb R}^3:x+y+z=3\}. $$ Geometrically, the plane $P$ would cut the sphere $\Omega$ into two pieces and $S$ is one of them while I don't specify which one so that the result would be up to the choice of these two pieces. The confusion might due to my notation: $$ \begin{cases} x+y+z=3\\ (x-1)^2+(y-1)^2+(z-1)^2=1 \end{cases}. $$ which is equivalent to  $$ \{(x,y,z)\in{\Bbb R}^3:x+y+z=3\ {\bf and}\ (x-1)^2+(y-1)^2+(z-1)^2=1\} $$ which is the boundary of $S$.",,[]
31,Surface integrals and parametrisation and limits.,Surface integrals and parametrisation and limits.,,"This is a question in preparation for an exam I am going to sit. It is from a previous years exam and no solution has been provided. Let S be the surface in R3 given by $$z = (x^2 + y^2)^{1/2},x^2 + y^2 \leq 1$$ Let v be the vector field given by $${\bf F} = (z^2, z, y^2)$$ I wish to compute, without use of Stokes' theorem $\int_S(\nabla \times {\bf F}) \cdot d{\bf S}$ where dS is oriented inward toward the z axis. Initially I thought about how to parametrise the cone and eventually settled with ${\bf p}(u, v) = (u, v, (u^2 + v^2)^{1/2})$ as I had seen similar in other examples. Now I compute a vector which is perpendicular to the surface. $$\frac{\partial {\bf p}}{\partial u} \times \frac{\partial {\bf p}}{\partial v} = \left( \frac{-u}{(u^2 + v^2) ^ {1/2}}, \frac{-v}{(u^2 + v^2) ^ {1/2}}, 1 \right) $$ It is apparent that this vector points toward the z axis for all u and v. I think the integral should be computed as follows, my issue is I am unsure what the limits should be. $$\int_S(\nabla \times {\bf F}) \cdot d{\bf S} = \int_u\int_v (\nabla \times {\bf F})({\bf p}(u, v)) \cdot \left( \frac{\partial {\bf p}}{\partial u} \times \frac{\partial {\bf p}}{\partial v} \right) du dv$$ If you could help me to the final step of the calculation that would be great, thanks.","This is a question in preparation for an exam I am going to sit. It is from a previous years exam and no solution has been provided. Let S be the surface in R3 given by $$z = (x^2 + y^2)^{1/2},x^2 + y^2 \leq 1$$ Let v be the vector field given by $${\bf F} = (z^2, z, y^2)$$ I wish to compute, without use of Stokes' theorem $\int_S(\nabla \times {\bf F}) \cdot d{\bf S}$ where dS is oriented inward toward the z axis. Initially I thought about how to parametrise the cone and eventually settled with ${\bf p}(u, v) = (u, v, (u^2 + v^2)^{1/2})$ as I had seen similar in other examples. Now I compute a vector which is perpendicular to the surface. $$\frac{\partial {\bf p}}{\partial u} \times \frac{\partial {\bf p}}{\partial v} = \left( \frac{-u}{(u^2 + v^2) ^ {1/2}}, \frac{-v}{(u^2 + v^2) ^ {1/2}}, 1 \right) $$ It is apparent that this vector points toward the z axis for all u and v. I think the integral should be computed as follows, my issue is I am unsure what the limits should be. $$\int_S(\nabla \times {\bf F}) \cdot d{\bf S} = \int_u\int_v (\nabla \times {\bf F})({\bf p}(u, v)) \cdot \left( \frac{\partial {\bf p}}{\partial u} \times \frac{\partial {\bf p}}{\partial v} \right) du dv$$ If you could help me to the final step of the calculation that would be great, thanks.",,['integration']
32,Slick proof of Gauss' theorem,Slick proof of Gauss' theorem,,"Below is a very concise proof of Gauss's theorem (from the book Vector and Tensor Analysis with Applications by Borisenko, Tarapov). Unfortunately, I'm having trouble understanding it, despite staring at it for a few days. I'd really appreciate it if someone would explicitly show a couple of the intermediate steps. I've marked these with $(*)$: Gauss's Theorem: Given a volume $V$ bounded by a closed surface $S$, suppose the functions $$P(x_1,x_2,x_3),\ \ Q(x_1,x_2,x_3),\ \ R(x_1,x_2,x_3)$$   and their derivatives $$\frac{\partial P}{\partial x_1},\ \frac{\partial Q}{\partial x_2},\ \frac{\partial R}{\partial x_3}$$    are continuous on $V \cup S$. Then   $$\iiint \limits_V \left( \frac{\partial P}{\partial x_1} + \frac{\partial Q}{\partial x_2} + \frac{\partial R}{\partial x_3} \right)dV = \iint \limits_S \left[ P\cos(\mathbf{n},x_1) + Q\cos(\mathbf{n},x_2)+ R\cos(\mathbf{n},x_3)\right]dS,$$   where $\mathbf{n}$ is the unit exterior normal to $S$. Proof: Suppose no line parallel to the $x_1$-axis intersects $S$ in more than two points $M^\prime$ and $M^{\prime\prime}.$ Then, if $S_{23}$ is the projection of $S$ onto the $x_2x_3$-plane, we have   $$(*)\iiint\limits_V \frac{\partial P}{\partial x_1}dV = \iint\limits_{S_{23}} \left( \int\frac{\partial P}{\partial x_1}dx_1\right)dS_{23} = \iint \limits_{S_{23}}\left[ P(M^\prime) - P(M^{\prime\prime})\right]dS_{23}.$$   But the element $dS_{23}$ of the projection $S_{23}$ can be expressed in terms of the elements of the surface $S$ at the points $M^\prime$ and $M^{\prime\prime}$:   $$dS_{23} = dS(M^\prime)\cos[\mathbf{n}(M^\prime),x_1] = -dS(M^{\prime\prime}) \cos[\mathbf{n}(M^{\prime\prime}),x_1].$$ Therefore   $$ (**) \iiint\limits_V \frac{\partial P}{\partial x_1}dV = \iint \limits_S P(M)\cos[\mathbf{n}(M),x_1]dS(M),$$ where $M$ is a variable point of the surface $S$. The formulas $$\iiint\limits_V \frac{\partial Q}{\partial x_2}dV = \iint \limits_S Q\cos[\mathbf{n},x_2]dS,$$ $$\iiint\limits_V \frac{\partial R}{\partial x_3}dV = \iint \limits_S R\cos[\mathbf{n},x_3]dS$$ are proved in the same way, provided no line parallel to the $x_2$ or $x_3$-axis intersects $S$ in more than two points. Adding these formulas, we obtain   $$\iiint \limits_V \left( \frac{\partial P}{\partial x_1} + \frac{\partial Q}{\partial x_2} + \frac{\partial R}{\partial x_3} \right)dV = \iint \limits_S \left[ P\cos(\mathbf{n},x_1) + Q\cos(\mathbf{n},x_2)+ R\cos(\mathbf{n},x_3)\right]dS.$$","Below is a very concise proof of Gauss's theorem (from the book Vector and Tensor Analysis with Applications by Borisenko, Tarapov). Unfortunately, I'm having trouble understanding it, despite staring at it for a few days. I'd really appreciate it if someone would explicitly show a couple of the intermediate steps. I've marked these with $(*)$: Gauss's Theorem: Given a volume $V$ bounded by a closed surface $S$, suppose the functions $$P(x_1,x_2,x_3),\ \ Q(x_1,x_2,x_3),\ \ R(x_1,x_2,x_3)$$   and their derivatives $$\frac{\partial P}{\partial x_1},\ \frac{\partial Q}{\partial x_2},\ \frac{\partial R}{\partial x_3}$$    are continuous on $V \cup S$. Then   $$\iiint \limits_V \left( \frac{\partial P}{\partial x_1} + \frac{\partial Q}{\partial x_2} + \frac{\partial R}{\partial x_3} \right)dV = \iint \limits_S \left[ P\cos(\mathbf{n},x_1) + Q\cos(\mathbf{n},x_2)+ R\cos(\mathbf{n},x_3)\right]dS,$$   where $\mathbf{n}$ is the unit exterior normal to $S$. Proof: Suppose no line parallel to the $x_1$-axis intersects $S$ in more than two points $M^\prime$ and $M^{\prime\prime}.$ Then, if $S_{23}$ is the projection of $S$ onto the $x_2x_3$-plane, we have   $$(*)\iiint\limits_V \frac{\partial P}{\partial x_1}dV = \iint\limits_{S_{23}} \left( \int\frac{\partial P}{\partial x_1}dx_1\right)dS_{23} = \iint \limits_{S_{23}}\left[ P(M^\prime) - P(M^{\prime\prime})\right]dS_{23}.$$   But the element $dS_{23}$ of the projection $S_{23}$ can be expressed in terms of the elements of the surface $S$ at the points $M^\prime$ and $M^{\prime\prime}$:   $$dS_{23} = dS(M^\prime)\cos[\mathbf{n}(M^\prime),x_1] = -dS(M^{\prime\prime}) \cos[\mathbf{n}(M^{\prime\prime}),x_1].$$ Therefore   $$ (**) \iiint\limits_V \frac{\partial P}{\partial x_1}dV = \iint \limits_S P(M)\cos[\mathbf{n}(M),x_1]dS(M),$$ where $M$ is a variable point of the surface $S$. The formulas $$\iiint\limits_V \frac{\partial Q}{\partial x_2}dV = \iint \limits_S Q\cos[\mathbf{n},x_2]dS,$$ $$\iiint\limits_V \frac{\partial R}{\partial x_3}dV = \iint \limits_S R\cos[\mathbf{n},x_3]dS$$ are proved in the same way, provided no line parallel to the $x_2$ or $x_3$-axis intersects $S$ in more than two points. Adding these formulas, we obtain   $$\iiint \limits_V \left( \frac{\partial P}{\partial x_1} + \frac{\partial Q}{\partial x_2} + \frac{\partial R}{\partial x_3} \right)dV = \iint \limits_S \left[ P\cos(\mathbf{n},x_1) + Q\cos(\mathbf{n},x_2)+ R\cos(\mathbf{n},x_3)\right]dS.$$",,"['multivariable-calculus', 'vector-analysis']"
33,"Volume of a sphere (r=2a) with hole(r=a) drilled through centre, using spherical polar coordinates.","Volume of a sphere (r=2a) with hole(r=a) drilled through centre, using spherical polar coordinates.",,"Need help solving 11.bi), A cylindrical hole of radius a is bored through the center of a sphere of radius 2a. Find the volume of the remaining material, using spherical polar coordinates. (You should align the bored cylinder with the z-axis). Have solved the rest of the problems, apart from 11.bi) . I can easily use spherical coordinates to find the volume of the sphere. However I can't work out how to find the volume of the bored out material using spherical coordinates. (Spent quite a few hours trying to work it out). See diagram/attached picture below for more information. Any hints/ways to start the problem, define the regions for the bored out volume would be much appreciated. Thank you for your time, Good Luck!","Need help solving 11.bi), A cylindrical hole of radius a is bored through the center of a sphere of radius 2a. Find the volume of the remaining material, using spherical polar coordinates. (You should align the bored cylinder with the z-axis). Have solved the rest of the problems, apart from 11.bi) . I can easily use spherical coordinates to find the volume of the sphere. However I can't work out how to find the volume of the bored out material using spherical coordinates. (Spent quite a few hours trying to work it out). See diagram/attached picture below for more information. Any hints/ways to start the problem, define the regions for the bored out volume would be much appreciated. Thank you for your time, Good Luck!",,"['integration', 'multivariable-calculus', 'polar-coordinates', 'spherical-coordinates', 'volume']"
34,Computing the derivative of square root of a matrix,Computing the derivative of square root of a matrix,,"maybe this is an idiot question, however I could not figure out how to solve it. Let $X =M_n(\mathbb{R})$ be the space of $n \times n$ matrix over the reals, then there exists two open neighborhoods of the identity, $U$ and $V$, such that the function $\phi: V \longrightarrow U$, $\phi(A) = \sqrt{A}$ is well defined and is differentiable at the identity $I$.Furthermore,  what's $d\phi(I)(T)$ ? I was thinking in inverting the matrix $A = I - B$ by the usual $\sum_i B^{i}$ and then, somehow, find the unique square root. Thanks in advance.","maybe this is an idiot question, however I could not figure out how to solve it. Let $X =M_n(\mathbb{R})$ be the space of $n \times n$ matrix over the reals, then there exists two open neighborhoods of the identity, $U$ and $V$, such that the function $\phi: V \longrightarrow U$, $\phi(A) = \sqrt{A}$ is well defined and is differentiable at the identity $I$.Furthermore,  what's $d\phi(I)(T)$ ? I was thinking in inverting the matrix $A = I - B$ by the usual $\sum_i B^{i}$ and then, somehow, find the unique square root. Thanks in advance.",,"['multivariable-calculus', 'matrix-calculus']"
35,"Prove that $f(x+h)-f(x) - \langle\nabla f(x), h\rangle\geq 0 \Rightarrow f $ convex",Prove that  convex,"f(x+h)-f(x) - \langle\nabla f(x), h\rangle\geq 0 \Rightarrow f ","At this link there is a demonstration that for $f$ continuously differentiable on $C \subseteq \mathbb{R}^n$ convex, $f(x+h)-f(x) - \langle\nabla f(x), h\rangle\geq 0 \Rightarrow f $ convex. This argument uses an intermediate step regarding ""$x\mapsto \nabla f(x)$ is monotone"", which I find tedious, and in general I find that this argument gets lost in alphas and betas and misses the geometry of the situation. Does anyone know a more elegant argument?","At this link there is a demonstration that for $f$ continuously differentiable on $C \subseteq \mathbb{R}^n$ convex, $f(x+h)-f(x) - \langle\nabla f(x), h\rangle\geq 0 \Rightarrow f $ convex. This argument uses an intermediate step regarding ""$x\mapsto \nabla f(x)$ is monotone"", which I find tedious, and in general I find that this argument gets lost in alphas and betas and misses the geometry of the situation. Does anyone know a more elegant argument?",,"['multivariable-calculus', 'convex-analysis']"
36,How does a fixed point of a dynamical system change when the system is changed slightly?,How does a fixed point of a dynamical system change when the system is changed slightly?,,"I've come across this problem in the course of my work, and I'm a bit stuck on it. Suppose I have an $n$-dimensional smooth dynamical system  $$ \dot{x_i} = f_i(\mathbf{x}), $$ and suppose there is a fixed point $\hat{\mathbf{x}}$, i.e. $f_i(\hat{\mathbf{x}})=0$ for every $i$. Now let us define a new dynamical system $$ \dot{x_i} = f'_i(\mathbf{x}), $$ where $f_i'(\mathbf{x}) = f_i(\mathbf{x}) + \epsilon\delta f_i(\mathbf{x})$, for some set of arbitrary smooth functions $\delta f_i$. It's possible that the fixed point of the original system was a bifurcation point, so that there will be no corresponding fixed point in the new system, no matter how small we make $\epsilon$. But supposing this isn't the case, the fixed point should just move to a slightly different position. That is, $\hat{\mathbf{x}}' = \hat{\mathbf{x}} + \delta \hat{\mathbf{x}}$, where every $f_i'(\hat{\mathbf{x}}')=0$ and $\delta\hat{\mathbf{x}}$ will be of the same order as $\epsilon$. What I want is an expression for $\delta\hat{\mathbf{x}}$ (in the small $\epsilon$ limit) in terms of $\epsilon$ and the partial derivatives of the functions $f_i$ and $\delta f_i$ at the original fixed point. It seems like it should be possible, but after going cross-eyed scribbling on the whiteboard I haven't come up with the answer. I'm probably just missing some simple trick - can anyone help me out?","I've come across this problem in the course of my work, and I'm a bit stuck on it. Suppose I have an $n$-dimensional smooth dynamical system  $$ \dot{x_i} = f_i(\mathbf{x}), $$ and suppose there is a fixed point $\hat{\mathbf{x}}$, i.e. $f_i(\hat{\mathbf{x}})=0$ for every $i$. Now let us define a new dynamical system $$ \dot{x_i} = f'_i(\mathbf{x}), $$ where $f_i'(\mathbf{x}) = f_i(\mathbf{x}) + \epsilon\delta f_i(\mathbf{x})$, for some set of arbitrary smooth functions $\delta f_i$. It's possible that the fixed point of the original system was a bifurcation point, so that there will be no corresponding fixed point in the new system, no matter how small we make $\epsilon$. But supposing this isn't the case, the fixed point should just move to a slightly different position. That is, $\hat{\mathbf{x}}' = \hat{\mathbf{x}} + \delta \hat{\mathbf{x}}$, where every $f_i'(\hat{\mathbf{x}}')=0$ and $\delta\hat{\mathbf{x}}$ will be of the same order as $\epsilon$. What I want is an expression for $\delta\hat{\mathbf{x}}$ (in the small $\epsilon$ limit) in terms of $\epsilon$ and the partial derivatives of the functions $f_i$ and $\delta f_i$ at the original fixed point. It seems like it should be possible, but after going cross-eyed scribbling on the whiteboard I haven't come up with the answer. I'm probably just missing some simple trick - can anyone help me out?",,"['multivariable-calculus', 'dynamical-systems']"
37,Vanishing of a multivariable polynomial on a lattice,Vanishing of a multivariable polynomial on a lattice,,"Let be $p(x_1,...,x_n) \in K[x_1,...,x_n]$ be a polynomial of degree $d$. Suppose there is a $n$-dimensional hyperbox $B = I \times \stackrel{n}{...} \times I = I^n$.  Divide $I$ to $d$ segements by $d+1$ points. This creates a lattice of $(d+1)^n$ point on $B$. Suppose that $p$ vanishes on the lattice (that is, for any point $(v_1,...,v_n)$ in the lattice, $p(v_1,...,v_n)=0$). Then we want to show that $p \equiv 0$ is the zero polynomial. Is this true for $K = \mathbb{R}$? Is it true when $K$ is algebraically closed field (e.g. $K = \mathbb{C}$)? If it is, is there a reference for that proposition?","Let be $p(x_1,...,x_n) \in K[x_1,...,x_n]$ be a polynomial of degree $d$. Suppose there is a $n$-dimensional hyperbox $B = I \times \stackrel{n}{...} \times I = I^n$.  Divide $I$ to $d$ segements by $d+1$ points. This creates a lattice of $(d+1)^n$ point on $B$. Suppose that $p$ vanishes on the lattice (that is, for any point $(v_1,...,v_n)$ in the lattice, $p(v_1,...,v_n)=0$). Then we want to show that $p \equiv 0$ is the zero polynomial. Is this true for $K = \mathbb{R}$? Is it true when $K$ is algebraically closed field (e.g. $K = \mathbb{C}$)? If it is, is there a reference for that proposition?",,"['algebraic-geometry', 'multivariable-calculus', 'polynomials']"
38,How to tell if multivariable function is odd?,How to tell if multivariable function is odd?,,"A question asks to use symmetry to evaluate this double integral: $$\iint_R {xy\over(1+x^4)}$$ where $R=[-1,1]\times[1,0]$ The solution states that the integrand is odd with respect to $x$, therefore $R_1=[-1,0]\times[1,0]$ and $R_2=[0,1]\times[1,0]$ cancel each other out since they are equal, but opposite, in magnitude. I understand the procedure of breaking the domain in pieces due to symmetry, but how does one notice that the function was odd in the first place?","A question asks to use symmetry to evaluate this double integral: $$\iint_R {xy\over(1+x^4)}$$ where $R=[-1,1]\times[1,0]$ The solution states that the integrand is odd with respect to $x$, therefore $R_1=[-1,0]\times[1,0]$ and $R_2=[0,1]\times[1,0]$ cancel each other out since they are equal, but opposite, in magnitude. I understand the procedure of breaking the domain in pieces due to symmetry, but how does one notice that the function was odd in the first place?",,"['calculus', 'integration', 'multivariable-calculus']"
39,"on proving bac-cab rule, $\vec{A} \times ( \vec{B} \times \vec{C})= \vec{B} (\vec{A} \cdot\vec{C})- \vec{C}(\vec{A}\cdot\vec{B})$","on proving bac-cab rule,",\vec{A} \times ( \vec{B} \times \vec{C})= \vec{B} (\vec{A} \cdot\vec{C})- \vec{C}(\vec{A}\cdot\vec{B}),"I noticed something when I was doing a proof of the BAC-CAB rule, and wanted to check if my intuition was correct. First, when I actually multiplied out $\vec{B} (\vec{A} \cdot\vec{C})-\vec{C}(\vec{A}\cdot\vec{B}) $ I noticed that I got the same thing. That is, the two quantities are equal. This makes sense since they are dot products. But that further implies that the expression goes to zero. Well, that means that $\vec{A} \times (\vec{B} \times \vec{C})$ would have to be zero too. And when I thought about it it occurred to me that $\vec{B} \times \vec{C}$ describes a vector perpendicular to both B and C; it's orthogonal. But $\vec{A} \times$ (any vector) will describe a vector perpendicular to (any vector) and in this case, that's $\vec{B} \times \vec{C}$. That is going to be parallel with the vector described by $\vec{B} \times \vec{C}$ itself and that further implies that the cross product of the two is equal to zero. Which is equal to the BAC-CAB quantity above. So, what is the big honking flaw in my reasoning here? :-)","I noticed something when I was doing a proof of the BAC-CAB rule, and wanted to check if my intuition was correct. First, when I actually multiplied out $\vec{B} (\vec{A} \cdot\vec{C})-\vec{C}(\vec{A}\cdot\vec{B}) $ I noticed that I got the same thing. That is, the two quantities are equal. This makes sense since they are dot products. But that further implies that the expression goes to zero. Well, that means that $\vec{A} \times (\vec{B} \times \vec{C})$ would have to be zero too. And when I thought about it it occurred to me that $\vec{B} \times \vec{C}$ describes a vector perpendicular to both B and C; it's orthogonal. But $\vec{A} \times$ (any vector) will describe a vector perpendicular to (any vector) and in this case, that's $\vec{B} \times \vec{C}$. That is going to be parallel with the vector described by $\vec{B} \times \vec{C}$ itself and that further implies that the cross product of the two is equal to zero. Which is equal to the BAC-CAB quantity above. So, what is the big honking flaw in my reasoning here? :-)",,"['multivariable-calculus', 'cross-product']"
40,Proving that $\boldsymbol \nabla \times (U(r) \hat{r}) = 0 $,Proving that,\boldsymbol \nabla \times (U(r) \hat{r}) = 0 ,"I was just checking to see if I wsa doing this right, as it isn't a formal proof. Just showing the identity. Let $U(r) \hat{r}$ b a vector in spherical coordinates. Given that the vector is only dependent on r, and there's no $\hat{\phi}$ or $\hat{\theta}$ component, that means that in the usual formula for the curl in spherical coordinates: $$\nabla \times U(r)\hat{r} = \hat{r} \frac{1}{\sin \theta}\left[\frac{\partial}{\partial\theta}(u_{\phi}\sin\theta) - \frac{\partial u_{\theta}}{\partial\theta} \right]+ \hat{\theta} \frac{1}{r}\left[\frac{1}{\sin \theta}\frac{\partial u_r}{\partial\phi}- \frac{\partial}{\partial r}(ru_{\phi}) \right]+\hat{\phi} \frac{1}{r}\left[\frac{\partial}{\partial\phi}(ru_{\theta})- \frac{\partial u_r}{\partial r}\right]$$ the $u_{\theta}$ and $u_{\phi}$ are just zero, since the original function has only a radial component. Since they go to zero all those partial derivatives go to zero, correct?","I was just checking to see if I wsa doing this right, as it isn't a formal proof. Just showing the identity. Let $U(r) \hat{r}$ b a vector in spherical coordinates. Given that the vector is only dependent on r, and there's no $\hat{\phi}$ or $\hat{\theta}$ component, that means that in the usual formula for the curl in spherical coordinates: $$\nabla \times U(r)\hat{r} = \hat{r} \frac{1}{\sin \theta}\left[\frac{\partial}{\partial\theta}(u_{\phi}\sin\theta) - \frac{\partial u_{\theta}}{\partial\theta} \right]+ \hat{\theta} \frac{1}{r}\left[\frac{1}{\sin \theta}\frac{\partial u_r}{\partial\phi}- \frac{\partial}{\partial r}(ru_{\phi}) \right]+\hat{\phi} \frac{1}{r}\left[\frac{\partial}{\partial\phi}(ru_{\theta})- \frac{\partial u_r}{\partial r}\right]$$ the $u_{\theta}$ and $u_{\phi}$ are just zero, since the original function has only a radial component. Since they go to zero all those partial derivatives go to zero, correct?",,"['multivariable-calculus', 'spherical-coordinates']"
41,Isometry in $\mathbb{R}^2$,Isometry in,\mathbb{R}^2,"Will there be an isometry in $\mathbb{R}^2$ taking the curve   $\alpha(t)=(\cos(t)+1, \sin(t)+2)$, where $t\in [0,\pi]$, to the curve $\beta  (t)=(t,\sin(t))$, where $t\in[0,c]$ and $c$ is a constant. I know isometries preserves distances and curvatures. So when $f:\mathbb{R}^n\to \mathbb{R}^n$ then $$f(\vec{p})=A(\vec{p})+\vec{a},$$ where $\vec{a}\in \mathbb{R}^N$ and $A$ is an orthogonal transformation. Thus an isometry is a composition of a rotation and a translation.","Will there be an isometry in $\mathbb{R}^2$ taking the curve   $\alpha(t)=(\cos(t)+1, \sin(t)+2)$, where $t\in [0,\pi]$, to the curve $\beta  (t)=(t,\sin(t))$, where $t\in[0,c]$ and $c$ is a constant. I know isometries preserves distances and curvatures. So when $f:\mathbb{R}^n\to \mathbb{R}^n$ then $$f(\vec{p})=A(\vec{p})+\vec{a},$$ where $\vec{a}\in \mathbb{R}^N$ and $A$ is an orthogonal transformation. Thus an isometry is a composition of a rotation and a translation.",,"['multivariable-calculus', 'differential-geometry']"
42,"$\int_{x=0}^{100}\int_{y=0}^{100-x} \int_{z=0}^{100-x-y}(x+y+z)^{10} \mathrm dx \, \mathrm dy \, \mathrm dz$ [closed]",[closed],"\int_{x=0}^{100}\int_{y=0}^{100-x} \int_{z=0}^{100-x-y}(x+y+z)^{10} \mathrm dx \, \mathrm dy \, \mathrm dz","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is there any easy way to calculate the following $\int_{x=0}^{100}\int_{y=0}^{100-x} \int_{z=0}^{100-x-y}(x+y+z)^{10} \mathrm dz \,  \mathrm dy \, \mathrm dx$ and $\int_{x=0}^{100}\int_{y=0}^{100-x}  \int_{z=0}^{100-x-y}5\mathrm dz \,  \mathrm dy \, \mathrm dx$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is there any easy way to calculate the following $\int_{x=0}^{100}\int_{y=0}^{100-x} \int_{z=0}^{100-x-y}(x+y+z)^{10} \mathrm dz \,  \mathrm dy \, \mathrm dx$ and $\int_{x=0}^{100}\int_{y=0}^{100-x}  \int_{z=0}^{100-x-y}5\mathrm dz \,  \mathrm dy \, \mathrm dx$",,"['integration', 'multivariable-calculus', 'definite-integrals']"
43,Curvature of a regular parametrization,Curvature of a regular parametrization,,"Prove that if $\mu: [a,b] \to \mathbb{R}^n$ is a regular   parametrization of a curve then the curvature at $\mu(t)$ is given   by: $$\kappa(t) =  \left|\left|\left(\frac{\mu'(t)}{||\mu'(t)||}\right)'\frac{1}{||\mu'(t)||}\right|\right|.$$ A regular parametrization is when $\mu'(t)\ne 0$ for all $t \in \mathbb{R}$. I know that I must use arc length to show this but I don't know how to apply it.","Prove that if $\mu: [a,b] \to \mathbb{R}^n$ is a regular   parametrization of a curve then the curvature at $\mu(t)$ is given   by: $$\kappa(t) =  \left|\left|\left(\frac{\mu'(t)}{||\mu'(t)||}\right)'\frac{1}{||\mu'(t)||}\right|\right|.$$ A regular parametrization is when $\mu'(t)\ne 0$ for all $t \in \mathbb{R}$. I know that I must use arc length to show this but I don't know how to apply it.",,"['multivariable-calculus', 'differential-geometry']"
44,"Show: $\int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx=0$",Show:,"\int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx=0","For a function $f\colon\mathbb{R}^n\to\mathbb{R}$ we write $f=o(r^{\alpha})$ if to any $\varepsilon>0$ there exists a $R>0$ so that   $$ \lvert f(x)\rvert\leq\varepsilon\lVert x\rVert^{\alpha}\mbox{ for }\lVert x\rVert\geq R. $$   Let $a\colon\mathbb{R}^n\to\mathbb{R}^n$ be a continiously differentiable vector field with   $$ a_k=o(r^{1-n}),~~~~~~~~~~\frac{\partial a_k}{\partial x_k}=o(r^{-n}),~~~~~~~~~~k=1,...,n. $$   Show that   $$ \int_{\mathbb{R}^n}\mbox{div }a(x)\ d^nx=0. $$ I thought about how to show that and came to the following: Set $$ \varepsilon:=\max\left\{\varepsilon_i:\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\varepsilon_i\lVert x\rVert^{-n}\mbox{ for }\lVert x\rVert\geq R_i(\varepsilon_i), i=1,...,n\right\} $$ and $$ R(\varepsilon):=\min\left\{R_i(\varepsilon_i):\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\varepsilon_i\lVert x\rVert^{-n}\mbox{ for }\lVert x\rVert\geq R_i(\varepsilon_i), i=1,...,n\right\}. $$ Then, because of $$ \lvert\mbox{div }a(x)\rvert=\left\lvert\sum\limits_{i=1}^n\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\sum\limits_{i=1}^n\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\sum\limits_{i=1}^n\epsilon\lVert x\rVert^{-n}=n\epsilon\lVert x\rVert^{-n}\leq\frac{n\epsilon}{R(\epsilon)^n} $$ I get $$ \left\lvert\int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx\right\rvert\leq\int_{\mathbb{R}^n}\frac{n\epsilon}{R(\epsilon)^n}\, d^nx=\frac{n\epsilon}{R(\epsilon)^n}\int_{\mathbb{R}^n}1\, d^nx\to 0\mbox{ for }\varepsilon\to 0, $$ which means $$ \int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx=0\mbox{ for }\varepsilon\to 0. $$ I do not know if this makes any sense. Maybe one additionaly needs the integral theorem of Gauss, but if yes I do not know how...","For a function $f\colon\mathbb{R}^n\to\mathbb{R}$ we write $f=o(r^{\alpha})$ if to any $\varepsilon>0$ there exists a $R>0$ so that   $$ \lvert f(x)\rvert\leq\varepsilon\lVert x\rVert^{\alpha}\mbox{ for }\lVert x\rVert\geq R. $$   Let $a\colon\mathbb{R}^n\to\mathbb{R}^n$ be a continiously differentiable vector field with   $$ a_k=o(r^{1-n}),~~~~~~~~~~\frac{\partial a_k}{\partial x_k}=o(r^{-n}),~~~~~~~~~~k=1,...,n. $$   Show that   $$ \int_{\mathbb{R}^n}\mbox{div }a(x)\ d^nx=0. $$ I thought about how to show that and came to the following: Set $$ \varepsilon:=\max\left\{\varepsilon_i:\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\varepsilon_i\lVert x\rVert^{-n}\mbox{ for }\lVert x\rVert\geq R_i(\varepsilon_i), i=1,...,n\right\} $$ and $$ R(\varepsilon):=\min\left\{R_i(\varepsilon_i):\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\varepsilon_i\lVert x\rVert^{-n}\mbox{ for }\lVert x\rVert\geq R_i(\varepsilon_i), i=1,...,n\right\}. $$ Then, because of $$ \lvert\mbox{div }a(x)\rvert=\left\lvert\sum\limits_{i=1}^n\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\sum\limits_{i=1}^n\left\lvert\frac{\partial a_i}{\partial x_i}(x)\right\rvert\leq\sum\limits_{i=1}^n\epsilon\lVert x\rVert^{-n}=n\epsilon\lVert x\rVert^{-n}\leq\frac{n\epsilon}{R(\epsilon)^n} $$ I get $$ \left\lvert\int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx\right\rvert\leq\int_{\mathbb{R}^n}\frac{n\epsilon}{R(\epsilon)^n}\, d^nx=\frac{n\epsilon}{R(\epsilon)^n}\int_{\mathbb{R}^n}1\, d^nx\to 0\mbox{ for }\varepsilon\to 0, $$ which means $$ \int_{\mathbb{R}^n}\mbox{div }a(x)\, d^nx=0\mbox{ for }\varepsilon\to 0. $$ I do not know if this makes any sense. Maybe one additionaly needs the integral theorem of Gauss, but if yes I do not know how...",,['multivariable-calculus']
45,vector representation of line,vector representation of line,,"i know that  vector representation of curve that starts at $r_0$  and ends at $r_1$ is given by $$r(t)=(1-t) r_0+t r_1\text{ where }0<t<1$$ so suppose  $r_0=(-5,3)$  and $r_1=(0,2)$, now  we should have $r(t)=(1-t) (-5,3)+t (0,2)$ if we  add separately we get $r(t)=(5 t-5,3-3 t)+(0,2 t)$ so finally it should be  $x=5 t-5$ and  $y=3-t$,but in answers it is written that $y=5 t-3$where i made mistake? thanks in advance","i know that  vector representation of curve that starts at $r_0$  and ends at $r_1$ is given by $$r(t)=(1-t) r_0+t r_1\text{ where }0<t<1$$ so suppose  $r_0=(-5,3)$  and $r_1=(0,2)$, now  we should have $r(t)=(1-t) (-5,3)+t (0,2)$ if we  add separately we get $r(t)=(5 t-5,3-3 t)+(0,2 t)$ so finally it should be  $x=5 t-5$ and  $y=3-t$,but in answers it is written that $y=5 t-3$where i made mistake? thanks in advance",,"['multivariable-calculus', 'vector-analysis']"
46,Optimization for constrained problem,Optimization for constrained problem,,"I'm reading about Lagrange multipliers from a Pattern recognition book appendix and on one point the following is stated: $\begin{align} &\color{gray}{\verb+C.4+}\,\,\,\,\,\color{#08F}{\textbf{Optimization for Constrained Problems}}\\ &\color{gray}{\verb+C.4.1+}\,\,\,\,\,\text{Equality Constraints} \end{align}$ We will first focus on linear equality constraints and then generalize to the nonlinear case. Although the philosophy for both cases is the same, it is easier to grasp the basics when linear constraints are involved . Thus the problem is cast as  $$\text{minimize}\,\,\,\, J(\theta)\\\text{subject to}\,\,\,\,A\theta=b$$ where $A$ is an $m\times l$ matrix and $b$, $\theta$ are $m\times1$ and $l\times 1$ vectors, respectively. It is assumed that the cost function $J(\theta)$ is twice continuously differentiable and it is, in general, a nonlinear function. Furthermore, we assume that the rows of $a$ are linearly independent, hence $A$ has full row rank. This assumption is known as the regularity assumption . Let $\theta_*$ be a local minimizer of $J(\theta)$ over the set $\{\theta:\,A\theta=b\}$. Then it is not difficult to show (e.g.,[Nash 96]) that, at this points, the gradient of $J(\theta)$ is given by  $$\color{red}{\boxed{\color{black}{\displaystyle \frac{\partial}{\partial\theta}(J(\theta))|_{\theta=\theta_*}=A^T\lambda}}} \tag{C.24}$$ where $\displaystyle \lambda\equiv[\lambda_1,\cdots,\lambda_m]^T$. Taking into account that $$\color{red}{\boxed{\color{black}{\displaystyle \frac{\partial}{\partial\theta}(A\theta)=A^T}}} \tag{C.25}$$ Eq. $\text{(C.24)}$ states that, at a constrained minimum , the gradient of the cost function is a linear combination of the gradients of the constraints. This is quite natural. Could perhaps someone show me easy-to-read proof of why the points in the red rectangles are true, since it's not difficult to show as the text suggests ;) Thank you for any help :)","I'm reading about Lagrange multipliers from a Pattern recognition book appendix and on one point the following is stated: $\begin{align} &\color{gray}{\verb+C.4+}\,\,\,\,\,\color{#08F}{\textbf{Optimization for Constrained Problems}}\\ &\color{gray}{\verb+C.4.1+}\,\,\,\,\,\text{Equality Constraints} \end{align}$ We will first focus on linear equality constraints and then generalize to the nonlinear case. Although the philosophy for both cases is the same, it is easier to grasp the basics when linear constraints are involved . Thus the problem is cast as  $$\text{minimize}\,\,\,\, J(\theta)\\\text{subject to}\,\,\,\,A\theta=b$$ where $A$ is an $m\times l$ matrix and $b$, $\theta$ are $m\times1$ and $l\times 1$ vectors, respectively. It is assumed that the cost function $J(\theta)$ is twice continuously differentiable and it is, in general, a nonlinear function. Furthermore, we assume that the rows of $a$ are linearly independent, hence $A$ has full row rank. This assumption is known as the regularity assumption . Let $\theta_*$ be a local minimizer of $J(\theta)$ over the set $\{\theta:\,A\theta=b\}$. Then it is not difficult to show (e.g.,[Nash 96]) that, at this points, the gradient of $J(\theta)$ is given by  $$\color{red}{\boxed{\color{black}{\displaystyle \frac{\partial}{\partial\theta}(J(\theta))|_{\theta=\theta_*}=A^T\lambda}}} \tag{C.24}$$ where $\displaystyle \lambda\equiv[\lambda_1,\cdots,\lambda_m]^T$. Taking into account that $$\color{red}{\boxed{\color{black}{\displaystyle \frac{\partial}{\partial\theta}(A\theta)=A^T}}} \tag{C.25}$$ Eq. $\text{(C.24)}$ states that, at a constrained minimum , the gradient of the cost function is a linear combination of the gradients of the constraints. This is quite natural. Could perhaps someone show me easy-to-read proof of why the points in the red rectangles are true, since it's not difficult to show as the text suggests ;) Thank you for any help :)",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization']"
47,When is Jacobian invertible?,When is Jacobian invertible?,,"Let $(f,U)$ be a chart on $M$ a smooth $n$-manifold. I know the inverse function theorem by which $f$ is invertible on $U$ iff its Jacobian is. What about the other direction? Is there any theorem about conditions under which the Jabobian of a map $f$ is going to be invertible?","Let $(f,U)$ be a chart on $M$ a smooth $n$-manifold. I know the inverse function theorem by which $f$ is invertible on $U$ iff its Jacobian is. What about the other direction? Is there any theorem about conditions under which the Jabobian of a map $f$ is going to be invertible?",,"['differential-geometry', 'multivariable-calculus']"
48,Double integral in polar coordinates,Double integral in polar coordinates,,"Use polar coordinates to find the volume of the given solid inside the sphere $$x^2+y^2+z^2=16$$ and outside the cylinder $$x^2+y^2=4$$ When I try to solve the problem, I keep getting the wrong answer, so I don't know if it's an arithmetic error or if I'm setting it up incorrectly. I've been setting the integral like so: $$\int_{0}^{2\pi}\int_{2}^4\sqrt{16-r^2}rdrd\theta$$ Is that the right set up? If so then I must have made an arithmetic error, if it's not correct, could someone help explain to me why it's not that? Thanks so much!","Use polar coordinates to find the volume of the given solid inside the sphere $$x^2+y^2+z^2=16$$ and outside the cylinder $$x^2+y^2=4$$ When I try to solve the problem, I keep getting the wrong answer, so I don't know if it's an arithmetic error or if I'm setting it up incorrectly. I've been setting the integral like so: $$\int_{0}^{2\pi}\int_{2}^4\sqrt{16-r^2}rdrd\theta$$ Is that the right set up? If so then I must have made an arithmetic error, if it's not correct, could someone help explain to me why it's not that? Thanks so much!",,"['multivariable-calculus', 'integration']"
49,Dot Product/ Cross Product Proof,Dot Product/ Cross Product Proof,,"Let $\hat{a}$, $\hat{b}$, and $\hat{c}$  $\in \mathbb{R}^3$, using the properties of vectors, prove  $$ (\hat{a} \times \hat{b}) \cdot [(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a})] = (\hat{a} \cdot (\hat{b} \times \hat{c}))^2$$ Proof: Since $\hat{u} \cdot \hat{v} = \mid \mid  \hat{u}\mid \mid \mid \mid \hat{v} \mid \mid \cos \theta$ and $\mid \mid \hat{u} \times \hat{v} \mid \mid  = \mid \mid \hat{u} \mid \mid \mid \mid \hat{v} \mid \mid \sin \theta$, the above equation can be rewritten as $$\mid \mid (\hat{a} \times \hat{b}) \mid \mid \mid \mid (\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a}) \mid \mid \cos \theta  = (\mid \mid \hat{a} \mid \mid \mid \mid (\hat{a} \times \hat{b}) \mid \mid \cos \theta)^2$$ which can be simplified further to $$\mid \mid \hat{a} \mid \mid\mid \mid \hat{b} \mid \mid \sin \theta \mid \mid \hat{b} \mid \mid \mid \mid \hat{c} \mid \mid \sin \theta \mid \mid \hat{c} \mid \mid \mid \mid \hat{a} \mid \mid \sin \theta \sin \theta \cos \theta  = (\mid \mid \hat{a} \mid \mid \mid \mid \hat{b} \mid \mid \mid \mid \hat{c} \mid \mid \sin \theta \cos \theta)^2 $$ reducing to  $$ \mid \mid \hat{a} \mid \mid^2 \mid \mid \hat{b} \mid \mid^2 \mid \mid \hat{c} \mid \mid^2 \sin^2 \theta \sin^2 \theta \cos \theta =  \mid \mid \hat{a} \mid \mid^2 \mid \mid \hat{b} \mid \mid^2 \mid \mid \hat{c} \mid \mid^2 \sin^2 \theta \cos^2 \theta$$ Please don't post an answer, I'm trying to work it on my own let $\epsilon_{i_1i_2 \cdots i_n}$ be the permutation tensor for $n$ dimensions $\hat{a} \times \hat{b} = \alpha_i e_i = \epsilon_{ijk} A_j B_k e_i$ , $\hat{b} \times \hat{c} = \beta_i e_i = \epsilon_{ijk} B_j C_k e_i$ $\hat{c} \times \hat{a} = \delta_i e_i = \epsilon_{ijk} C_j A_k e_i$ then $(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a}) = \gamma_i e_i = \epsilon_{ijk} \beta_j \delta_k e_i$ and $$ (\hat{a} \times \hat{b}) \cdot [(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a})] =\sum_{i=1}^{3}\alpha_i \gamma_i$$ so we get a new equation   $$\sum_{i=1}^{3}\alpha_i \gamma_i = (\hat{a} \cdot (\hat{b} \times \hat{c}))^2$$ $\hat{a} \cdot (\hat{b} \times \hat{c}) = \sum_{i=0}^{3} A_i \beta_i$ so we obtain  $$\sum_{i=1}^{3}\alpha_i \gamma_i  = \Big(\sum_{i=1}^{3} A_i \beta_i \Big)^2$$ How to proceed? Just an fyi guys, this is just a problem in my Stewart Calculus book, so I don't think the proof should be that rigorous...","Let $\hat{a}$, $\hat{b}$, and $\hat{c}$  $\in \mathbb{R}^3$, using the properties of vectors, prove  $$ (\hat{a} \times \hat{b}) \cdot [(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a})] = (\hat{a} \cdot (\hat{b} \times \hat{c}))^2$$ Proof: Since $\hat{u} \cdot \hat{v} = \mid \mid  \hat{u}\mid \mid \mid \mid \hat{v} \mid \mid \cos \theta$ and $\mid \mid \hat{u} \times \hat{v} \mid \mid  = \mid \mid \hat{u} \mid \mid \mid \mid \hat{v} \mid \mid \sin \theta$, the above equation can be rewritten as $$\mid \mid (\hat{a} \times \hat{b}) \mid \mid \mid \mid (\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a}) \mid \mid \cos \theta  = (\mid \mid \hat{a} \mid \mid \mid \mid (\hat{a} \times \hat{b}) \mid \mid \cos \theta)^2$$ which can be simplified further to $$\mid \mid \hat{a} \mid \mid\mid \mid \hat{b} \mid \mid \sin \theta \mid \mid \hat{b} \mid \mid \mid \mid \hat{c} \mid \mid \sin \theta \mid \mid \hat{c} \mid \mid \mid \mid \hat{a} \mid \mid \sin \theta \sin \theta \cos \theta  = (\mid \mid \hat{a} \mid \mid \mid \mid \hat{b} \mid \mid \mid \mid \hat{c} \mid \mid \sin \theta \cos \theta)^2 $$ reducing to  $$ \mid \mid \hat{a} \mid \mid^2 \mid \mid \hat{b} \mid \mid^2 \mid \mid \hat{c} \mid \mid^2 \sin^2 \theta \sin^2 \theta \cos \theta =  \mid \mid \hat{a} \mid \mid^2 \mid \mid \hat{b} \mid \mid^2 \mid \mid \hat{c} \mid \mid^2 \sin^2 \theta \cos^2 \theta$$ Please don't post an answer, I'm trying to work it on my own let $\epsilon_{i_1i_2 \cdots i_n}$ be the permutation tensor for $n$ dimensions $\hat{a} \times \hat{b} = \alpha_i e_i = \epsilon_{ijk} A_j B_k e_i$ , $\hat{b} \times \hat{c} = \beta_i e_i = \epsilon_{ijk} B_j C_k e_i$ $\hat{c} \times \hat{a} = \delta_i e_i = \epsilon_{ijk} C_j A_k e_i$ then $(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a}) = \gamma_i e_i = \epsilon_{ijk} \beta_j \delta_k e_i$ and $$ (\hat{a} \times \hat{b}) \cdot [(\hat{b} \times \hat{c}) \times (\hat{c} \times \hat{a})] =\sum_{i=1}^{3}\alpha_i \gamma_i$$ so we get a new equation   $$\sum_{i=1}^{3}\alpha_i \gamma_i = (\hat{a} \cdot (\hat{b} \times \hat{c}))^2$$ $\hat{a} \cdot (\hat{b} \times \hat{c}) = \sum_{i=0}^{3} A_i \beta_i$ so we obtain  $$\sum_{i=1}^{3}\alpha_i \gamma_i  = \Big(\sum_{i=1}^{3} A_i \beta_i \Big)^2$$ How to proceed? Just an fyi guys, this is just a problem in my Stewart Calculus book, so I don't think the proof should be that rigorous...",,"['multivariable-calculus', 'trigonometry', 'vector-analysis']"
50,Vector differentiation,Vector differentiation,,"Here is a step in a differentiation I don't understand. Let $(x^1, ...,x^k), (y^1,...,y^k)$ denote two vectors with $y^i = y^i(x^1,....,x^k)$. Given  $$ y^1(0,x^2,...,x^k) = 0$$ how can I reach the conclusion that $$ {\partial (y^1,...,y^k) \over \partial (x^1,...,x^k)}= {\partial y^1 \over \partial x^1}{\partial (y^2 , ...,y^k) \over \partial (x^2,...,x^k) }$$ ? Is it really multiplication on the RHS of the equation? I'm stumped by this.","Here is a step in a differentiation I don't understand. Let $(x^1, ...,x^k), (y^1,...,y^k)$ denote two vectors with $y^i = y^i(x^1,....,x^k)$. Given  $$ y^1(0,x^2,...,x^k) = 0$$ how can I reach the conclusion that $$ {\partial (y^1,...,y^k) \over \partial (x^1,...,x^k)}= {\partial y^1 \over \partial x^1}{\partial (y^2 , ...,y^k) \over \partial (x^2,...,x^k) }$$ ? Is it really multiplication on the RHS of the equation? I'm stumped by this.",,"['real-analysis', 'multivariable-calculus']"
51,"$\nabla \varphi \overset{?}{=} \nabla \cdot \varphi \bar{\bar{I}}$ where $\varphi$ is scalar, $\bar{\bar{I}}$ is identity tensor","where  is scalar,  is identity tensor",\nabla \varphi \overset{?}{=} \nabla \cdot \varphi \bar{\bar{I}} \varphi \bar{\bar{I}},"I am trying to determine if these two are equivalent. I have a function written with both terms, and this is the only discrepancy. The gradient increases the rank of the scalar to a vector, while the divergence reduces the rank of a tensor to a vector, but is this statement correct?","I am trying to determine if these two are equivalent. I have a function written with both terms, and this is the only discrepancy. The gradient increases the rank of the scalar to a vector, while the divergence reduces the rank of a tensor to a vector, but is this statement correct?",,"['multivariable-calculus', 'tensors', 'fluid-dynamics']"
52,$F$ is incompressible $\iff$ $G$ is incompressible,is incompressible   is incompressible,F \iff G,"If $F, G$ are vector fields, and $F(G(x,y)), G(F(x,y)):  \mathbb{R}^2 \rightarrow \mathbb{R}^2 \rightarrow \mathbb{R}^2$ are the identity $(x,y) \mapsto (x,y)$, prove that F is incompressible $\iff$ G is incompressible. I know you have to suppose F incompressible to get G incompressible and vice versa, but I'm having some trouble. I'm not sure how the composite functions link in with the functions F and G. I can see F(G(x,y)),G(F(x,y)) have divergence 2 also. Just not sure how to tackle it. Hints would be welcome.","If $F, G$ are vector fields, and $F(G(x,y)), G(F(x,y)):  \mathbb{R}^2 \rightarrow \mathbb{R}^2 \rightarrow \mathbb{R}^2$ are the identity $(x,y) \mapsto (x,y)$, prove that F is incompressible $\iff$ G is incompressible. I know you have to suppose F incompressible to get G incompressible and vice versa, but I'm having some trouble. I'm not sure how the composite functions link in with the functions F and G. I can see F(G(x,y)),G(F(x,y)) have divergence 2 also. Just not sure how to tackle it. Hints would be welcome.",,"['multivariable-calculus', 'vector-analysis']"
53,Vector-by-Vector derivative,Vector-by-Vector derivative,,Could someone please help me out with this derivative? $$ \frac{d}{dx}(xx^T) $$ with both $x$ being vector. Thanks EDIT: I should clarify that the actual state I am taking the derivative is $$ \frac{d}{dx}(xx^TPb) $$ where $Pb$ has the dimention of $x$ but is independent of $x$. So the whole state $xx^TPb$ is a vector. EDIT2: Would it become by any chance the following? $$ \frac{d}{dx}(xx^TPb) = (Pbx^T)^T + x(Pb)^T = 2x(Pb)^T $$,Could someone please help me out with this derivative? $$ \frac{d}{dx}(xx^T) $$ with both $x$ being vector. Thanks EDIT: I should clarify that the actual state I am taking the derivative is $$ \frac{d}{dx}(xx^TPb) $$ where $Pb$ has the dimention of $x$ but is independent of $x$. So the whole state $xx^TPb$ is a vector. EDIT2: Would it become by any chance the following? $$ \frac{d}{dx}(xx^TPb) = (Pbx^T)^T + x(Pb)^T = 2x(Pb)^T $$,,"['multivariable-calculus', 'derivatives', 'vector-spaces']"
54,Simplifing formulas using tensor notation,Simplifing formulas using tensor notation,,"Im trying to symplify formulas like:  $$\operatorname{div}(\operatorname{rot}\vec{F}),\qquad \operatorname{rot}(\operatorname{rot}\vec{F}) $$ or something more strange like: $$\operatorname{rot}(\vec{r}\operatorname{div}(r^4\operatorname{grad}(r^4)))$$ To do this I want to use the tensor notation and by this I mean using the Einstein convention, Levi-Civita symbol, Kronecker delta and all that. The problem is that I don't understand the rules and what its allowed and what not. As an example: $$\operatorname{div}(f( r)\cdot \textbf r)= \partial_i(f( r)\cdot \textbf r)_i = \partial_i(f( r)\cdot x_i)= \partial _if(r)x_i+f(r)\partial_ix_i= f'(r)\frac{x_i}{r}x_i+3f(r)=rf'(r)+3f(r) $$ Any help on how to do this kind of problems or where I can find useful examples? I'm working in flat space. Thanks!","Im trying to symplify formulas like:  $$\operatorname{div}(\operatorname{rot}\vec{F}),\qquad \operatorname{rot}(\operatorname{rot}\vec{F}) $$ or something more strange like: $$\operatorname{rot}(\vec{r}\operatorname{div}(r^4\operatorname{grad}(r^4)))$$ To do this I want to use the tensor notation and by this I mean using the Einstein convention, Levi-Civita symbol, Kronecker delta and all that. The problem is that I don't understand the rules and what its allowed and what not. As an example: $$\operatorname{div}(f( r)\cdot \textbf r)= \partial_i(f( r)\cdot \textbf r)_i = \partial_i(f( r)\cdot x_i)= \partial _if(r)x_i+f(r)\partial_ix_i= f'(r)\frac{x_i}{r}x_i+3f(r)=rf'(r)+3f(r) $$ Any help on how to do this kind of problems or where I can find useful examples? I'm working in flat space. Thanks!",,"['multivariable-calculus', 'notation', 'tensors']"
55,Proving the origin is a saddle point.,Proving the origin is a saddle point.,,"I have the function $g(x,y) = x^6 -y^6x^2$ and want to prove that the origin is a saddle point. I know that a critical point with an indefinite Hessian matrix is a saddle point, but this is only a sufficient condition. $(0,0)$ is indeed a critical point of $g$, but the Hessian matrix is everywhere $0$ and hence it is positive and negative semi-definite and so not indefinite. How would I go about concluding that the origin is indeed a saddle point here? Unfortunately, the only definitions of saddle points that I could find gave the usual sufficient condition of an indefinite Hessian. Many thanks.","I have the function $g(x,y) = x^6 -y^6x^2$ and want to prove that the origin is a saddle point. I know that a critical point with an indefinite Hessian matrix is a saddle point, but this is only a sufficient condition. $(0,0)$ is indeed a critical point of $g$, but the Hessian matrix is everywhere $0$ and hence it is positive and negative semi-definite and so not indefinite. How would I go about concluding that the origin is indeed a saddle point here? Unfortunately, the only definitions of saddle points that I could find gave the usual sufficient condition of an indefinite Hessian. Many thanks.",,['multivariable-calculus']
56,Strange integral in 3 space (Maybe Divergence Theorem),Strange integral in 3 space (Maybe Divergence Theorem),,"I'm trying to find something called the density of states and the model that I am using specifies $$E = \frac{h^2}{2 m} k^2$$ where $k = |\bf{k}|$. The quantity I am trying to calculate is $$D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}E}$$. I think how to simplify this is to substitute $$\mathrm{d}E = \frac{h^2}{m} \mathrm{d}k$$ and $$\nabla_k E = \nabla_k [\frac{h^2}{2 m} (k_x^2 + k_y^2 + k_z^2)].$$ Therefore, $$\nabla_k E = \frac{h^2}{m}(k_x, k_y, k_z).$$ However this leaves me with $$D(E) = \int{\frac{1}{(k_x, k_y, k_z)} \mathrm{d}k},$$ which I am sure how to solve. Any idea how to solve this with a vector in the denominator? Edit I realized that $D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}E}$ is actually $$D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}\bf{S}}.$$","I'm trying to find something called the density of states and the model that I am using specifies $$E = \frac{h^2}{2 m} k^2$$ where $k = |\bf{k}|$. The quantity I am trying to calculate is $$D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}E}$$. I think how to simplify this is to substitute $$\mathrm{d}E = \frac{h^2}{m} \mathrm{d}k$$ and $$\nabla_k E = \nabla_k [\frac{h^2}{2 m} (k_x^2 + k_y^2 + k_z^2)].$$ Therefore, $$\nabla_k E = \frac{h^2}{m}(k_x, k_y, k_z).$$ However this leaves me with $$D(E) = \int{\frac{1}{(k_x, k_y, k_z)} \mathrm{d}k},$$ which I am sure how to solve. Any idea how to solve this with a vector in the denominator? Edit I realized that $D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}E}$ is actually $$D(E) = \int{(\nabla_k E)^{-1} \mathrm{d}\bf{S}}.$$",,"['multivariable-calculus', 'physics']"
57,"Maxima and minima of multivariable function $f(x,y)=6x^3y^2-x^4y^2-x^3y^3$",Maxima and minima of multivariable function,"f(x,y)=6x^3y^2-x^4y^2-x^3y^3","$$f(x,y)=6x^3y^2-x^4y^2-x^3y^3$$ $$\frac{\delta f}{\delta x}=18x^2y^2-4x^3y^2-3x^2y^3$$ $$\frac{\delta f}{\delta y}=12x^3y-2x^4y-3x^3y^2$$ Points, in which partial derivatives ar equal to 0 are: (3,2), (x,0), (0,y), x,y are any real numbers. Now I find second derivatives $$\Delta_1=\frac{\delta f}{\delta x^2}=36xy^2-12x^2y^2-6xy^3$$ $$\frac{\delta f}{\delta y^2}=12x^3-2x^4-6x^3y$$ $$\frac{\delta f}{\delta x \delta y}=\frac{\delta f}{\delta y \delta x} = 36x^2y-8x^3y-9x^2y^2$$ $$\Delta_2=\begin{vmatrix}\frac{\partial^2 f}{\partial x^2}&\frac{\partial^2 f}{\partial y\partial x}\\\frac{\partial^2 f}{\partial x\partial y}& \frac{\partial^2 f}{\partial y^2} \end{vmatrix}$$ After plugging in the point (3,2) we get $\Delta_1<0$ and $\Delta_2>0$, so (3,2) is maxima. Now then I try to plug in (x,0) and (0,y) I obviously get $\Delta_1=0$ and $\Delta_2=0$ and I can't tell, using Sylvester's criterion, if those points are minima or maxima or neither. What should I do?","$$f(x,y)=6x^3y^2-x^4y^2-x^3y^3$$ $$\frac{\delta f}{\delta x}=18x^2y^2-4x^3y^2-3x^2y^3$$ $$\frac{\delta f}{\delta y}=12x^3y-2x^4y-3x^3y^2$$ Points, in which partial derivatives ar equal to 0 are: (3,2), (x,0), (0,y), x,y are any real numbers. Now I find second derivatives $$\Delta_1=\frac{\delta f}{\delta x^2}=36xy^2-12x^2y^2-6xy^3$$ $$\frac{\delta f}{\delta y^2}=12x^3-2x^4-6x^3y$$ $$\frac{\delta f}{\delta x \delta y}=\frac{\delta f}{\delta y \delta x} = 36x^2y-8x^3y-9x^2y^2$$ $$\Delta_2=\begin{vmatrix}\frac{\partial^2 f}{\partial x^2}&\frac{\partial^2 f}{\partial y\partial x}\\\frac{\partial^2 f}{\partial x\partial y}& \frac{\partial^2 f}{\partial y^2} \end{vmatrix}$$ After plugging in the point (3,2) we get $\Delta_1<0$ and $\Delta_2>0$, so (3,2) is maxima. Now then I try to plug in (x,0) and (0,y) I obviously get $\Delta_1=0$ and $\Delta_2=0$ and I can't tell, using Sylvester's criterion, if those points are minima or maxima or neither. What should I do?",,"['multivariable-calculus', 'optimization']"
58,Is the function identically zero?,Is the function identically zero?,,"Let $f(x, y)$ be a continuous, real-valued function on $\mathbb{R}^2$. Suppose that, for every rectangular region $R$ of area 1, the double integral of $f(x, y)$ over $R$ equals 0. Must $f(x, y)$ be identically 0?","Let $f(x, y)$ be a continuous, real-valued function on $\mathbb{R}^2$. Suppose that, for every rectangular region $R$ of area 1, the double integral of $f(x, y)$ over $R$ equals 0. Must $f(x, y)$ be identically 0?",,['multivariable-calculus']
59,Derive the solution to the Lagrangian $ \mathcal L= y(x)\sqrt{1+y'(x)^2}$,Derive the solution to the Lagrangian, \mathcal L= y(x)\sqrt{1+y'(x)^2},"I am supposed to derive the solution to the Lagrangian $$ \mathcal L= y(x)\sqrt{1+y'(x)^2}$$  Unfortunately I am unable to solve both, the Euler Lagrange equation or the Beltrami equation. It may be interesting to know, that we are supposed to bring the solution to this Lagrangian in relation to Fata Morgana mirages, maybe this helps you somehow. Are there any further ways to construct a solution to the given Lagrangian?","I am supposed to derive the solution to the Lagrangian $$ \mathcal L= y(x)\sqrt{1+y'(x)^2}$$  Unfortunately I am unable to solve both, the Euler Lagrange equation or the Beltrami equation. It may be interesting to know, that we are supposed to bring the solution to this Lagrangian in relation to Fata Morgana mirages, maybe this helps you somehow. Are there any further ways to construct a solution to the given Lagrangian?",,"['real-analysis', 'functional-analysis']"
60,Help with Gradient-related concepts,Help with Gradient-related concepts,,"I'm trying to understand the concept of a Gradient vector, and it seems I'm having trouble visualizing certain stuff. So, I was hoping if someone could resolve some of the questions I'm having on my mind. Okay, so I'm considering a real-valued function $z=f(x,y)$ which is smooth, i.e., its partial derivatives with respect to $x$ and $y$ exist and are continuous. Let $k$ be a real number in the range of $f$ and let $\vec{v}$ be a unit vector in $\mathbb{R}^2$, which is tangent to the level curve $f(x,y)=k$ Now, I am told to understand that the rate of change of $f$ in the direction of $\vec{v}$ is $0$, i.e., $D_{\vec{v}}f =0$. And the explanation for it, in almost all of the sources I've seen is, that it's because $\vec{v}$ is a tangent vector to this curve. Can anyone tell me how $\vec{v}$ being simply a tangent vector implies this? Is this concept simply extended to the case of a function in three variables, by level surfaces and tangent planes? In this case, wouldn't there be an infinite number of tangent vectors, and thus an infinite number of gradient vectors (since the gradient vector is perpendicular to the tangent vector, i.e., normal to the surface). Can anyone point out any sources that help in visualizing this? Thank you!","I'm trying to understand the concept of a Gradient vector, and it seems I'm having trouble visualizing certain stuff. So, I was hoping if someone could resolve some of the questions I'm having on my mind. Okay, so I'm considering a real-valued function $z=f(x,y)$ which is smooth, i.e., its partial derivatives with respect to $x$ and $y$ exist and are continuous. Let $k$ be a real number in the range of $f$ and let $\vec{v}$ be a unit vector in $\mathbb{R}^2$, which is tangent to the level curve $f(x,y)=k$ Now, I am told to understand that the rate of change of $f$ in the direction of $\vec{v}$ is $0$, i.e., $D_{\vec{v}}f =0$. And the explanation for it, in almost all of the sources I've seen is, that it's because $\vec{v}$ is a tangent vector to this curve. Can anyone tell me how $\vec{v}$ being simply a tangent vector implies this? Is this concept simply extended to the case of a function in three variables, by level surfaces and tangent planes? In this case, wouldn't there be an infinite number of tangent vectors, and thus an infinite number of gradient vectors (since the gradient vector is perpendicular to the tangent vector, i.e., normal to the surface). Can anyone point out any sources that help in visualizing this? Thank you!",,"['multivariable-calculus', 'vector-analysis']"
61,tricky surface integral,tricky surface integral,,"I am studying for my final and my prof gave us review questions but with no answers so I am lost with this question. If anyone can help I would really appreciate it. Question: Find the area of the portion of the surface $z=x^2+y$, that lies over the region $0\le x \le 1$, and $0\le y\le 1$. I know this is a surface integral of the form $$A(S) = \int\int_s dS$$ I computed dS, $dS = \sqrt{1+{\frac{\partial z}{\partial x}}^2 + {\frac{\partial z}{\partial y}}^2} = \sqrt{4x^2 +2}$ Then I integrate this function over the region... $\int_0^1 \int_0^1 \sqrt{4x^2+2} dydx$ but then this integration seems strange to me. If anyone can tell me if I am doing this right so far and perhaps what is the next step I would appreciate it.","I am studying for my final and my prof gave us review questions but with no answers so I am lost with this question. If anyone can help I would really appreciate it. Question: Find the area of the portion of the surface $z=x^2+y$, that lies over the region $0\le x \le 1$, and $0\le y\le 1$. I know this is a surface integral of the form $$A(S) = \int\int_s dS$$ I computed dS, $dS = \sqrt{1+{\frac{\partial z}{\partial x}}^2 + {\frac{\partial z}{\partial y}}^2} = \sqrt{4x^2 +2}$ Then I integrate this function over the region... $\int_0^1 \int_0^1 \sqrt{4x^2+2} dydx$ but then this integration seems strange to me. If anyone can tell me if I am doing this right so far and perhaps what is the next step I would appreciate it.",,"['integration', 'multivariable-calculus', 'surfaces']"
62,Double Integral of piece wise function?,Double Integral of piece wise function?,,"Let $I=[0,1]\times[0,1]$ and let $$f(x)= \begin{cases}  0, & \text{if (x,y)=(0,0)}\\ \frac{x^2-y^2}{(x^2+y^2)^2}, & \text { if (x,y)$\not=$(0,0)}\\ \end{cases}  $$ Need to show that $$\int^1_0\int^1_0 f(x,y)dxdy=\int_0^1\int_0^1 f(x,y)dydx.$$ I cant seem to integrate $$\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}dx.$$ I am thinking that I could use polar coordinates maybe by $r^2=x^2+y^2$.","Let $I=[0,1]\times[0,1]$ and let $$f(x)= \begin{cases}  0, & \text{if (x,y)=(0,0)}\\ \frac{x^2-y^2}{(x^2+y^2)^2}, & \text { if (x,y)$\not=$(0,0)}\\ \end{cases}  $$ Need to show that $$\int^1_0\int^1_0 f(x,y)dxdy=\int_0^1\int_0^1 f(x,y)dydx.$$ I cant seem to integrate $$\int_0^1\frac{x^2-y^2}{(x^2+y^2)^2}dx.$$ I am thinking that I could use polar coordinates maybe by $r^2=x^2+y^2$.",,"['calculus', 'integration', 'multivariable-calculus']"
63,"What does it mean for partial derivatives $f_x$ and $f_y$ exist near $(a,b)$ and are continuous at $(a,b)$ then $f$ is differentiable at $(a,b)$?",What does it mean for partial derivatives  and  exist near  and are continuous at  then  is differentiable at ?,"f_x f_y (a,b) (a,b) f (a,b)","I am reading my text book and I come across a theorem that says: If the partial derivatives $f_x$ and $f_y$ exist near $(a,b)$ and are continuous at $(a,b)$ then $f$ is differentiable at $(a,b)$. What does it mean for partial derivatives $f_x$ and $f_y$ exist near $(a,b)$? If I just find the partial derivatives of $f_x$ and $f_y$, does that mean they exist near $(a,b)$? How do I check? And how do I check to see if they are continuous at $(a,b)$? Can I just plug in a given point, and if get a finite answer, that means it's continuous at $(a,b)$ correct? But then there might be a gap.... so how do I know for sure with a given point? When would it not be continuous, when infinity, etc. ? Thank you","I am reading my text book and I come across a theorem that says: If the partial derivatives $f_x$ and $f_y$ exist near $(a,b)$ and are continuous at $(a,b)$ then $f$ is differentiable at $(a,b)$. What does it mean for partial derivatives $f_x$ and $f_y$ exist near $(a,b)$? If I just find the partial derivatives of $f_x$ and $f_y$, does that mean they exist near $(a,b)$? How do I check? And how do I check to see if they are continuous at $(a,b)$? Can I just plug in a given point, and if get a finite answer, that means it's continuous at $(a,b)$ correct? But then there might be a gap.... so how do I know for sure with a given point? When would it not be continuous, when infinity, etc. ? Thank you",,['multivariable-calculus']
64,Proving that a critical point is a minimum,Proving that a critical point is a minimum,,"I am solving a problem, which is ""Find the point of the paraboloid $P:=\{(x,y,z)\in\mathbb{R}^3 | x^2+y^2=z\}$ which is the nearest to the point $(1,1,\frac12)$."" I have already determined (using the Lagrange multipliers method) that $p=(2^{-\frac23},2^{-\frac23},2^{-\frac13})$ is a critical point of $f(x,y,z)=(x-1)^2+(y-1)^2+(z-\frac12)^2$ (which is the square of the distance function) with the restraint $g(x,y,z)=0$, where $g(x,y,z)=x^2+y^2-z$. I need some help in order to prove that $p$ is the minimum of $f|_P$. If possible, give me only a hint, as it is homework and I only want some idea to finish the problem by myself. EDIT: As requested, I'm showing my calculations so far. A critical point of $f|_P$ is, by the Lagrange multipliers method, one which satisfies $g(x,y,z)=0$ (that is, $x^2+y^2=z$) and $\nabla f(x,y,z)=\lambda \nabla g(x,y,z)$ for some $\lambda \in \mathbb{R}$. We have $\nabla f(x,y,z)=(2x-2,2y-2,2z-1)$ and $\nabla g(x,y,z)= (2x,2y,-1)$, which yields the system $$(2x-2=\lambda 2x)\wedge(2y-2=\lambda 2y)\wedge(2z-1=-\lambda),$$ which is equivalent to $$(x=\tfrac{1}{1-\lambda})\wedge (y=\tfrac{1}{1-\lambda}) \wedge(z=\tfrac{1-\lambda}{2}).$$ So we get $x=y$ and $2x^2=z$ (because $x^2+y^2=z$), which gives $\frac{2}{(1-\lambda)^2}=\frac{1-\lambda}{2}$, that is, $\lambda = 1-2^\frac23$. This gives us $x=y=2^{-\frac23}$ and $z=2^{-\frac13}$.","I am solving a problem, which is ""Find the point of the paraboloid $P:=\{(x,y,z)\in\mathbb{R}^3 | x^2+y^2=z\}$ which is the nearest to the point $(1,1,\frac12)$."" I have already determined (using the Lagrange multipliers method) that $p=(2^{-\frac23},2^{-\frac23},2^{-\frac13})$ is a critical point of $f(x,y,z)=(x-1)^2+(y-1)^2+(z-\frac12)^2$ (which is the square of the distance function) with the restraint $g(x,y,z)=0$, where $g(x,y,z)=x^2+y^2-z$. I need some help in order to prove that $p$ is the minimum of $f|_P$. If possible, give me only a hint, as it is homework and I only want some idea to finish the problem by myself. EDIT: As requested, I'm showing my calculations so far. A critical point of $f|_P$ is, by the Lagrange multipliers method, one which satisfies $g(x,y,z)=0$ (that is, $x^2+y^2=z$) and $\nabla f(x,y,z)=\lambda \nabla g(x,y,z)$ for some $\lambda \in \mathbb{R}$. We have $\nabla f(x,y,z)=(2x-2,2y-2,2z-1)$ and $\nabla g(x,y,z)= (2x,2y,-1)$, which yields the system $$(2x-2=\lambda 2x)\wedge(2y-2=\lambda 2y)\wedge(2z-1=-\lambda),$$ which is equivalent to $$(x=\tfrac{1}{1-\lambda})\wedge (y=\tfrac{1}{1-\lambda}) \wedge(z=\tfrac{1-\lambda}{2}).$$ So we get $x=y$ and $2x^2=z$ (because $x^2+y^2=z$), which gives $\frac{2}{(1-\lambda)^2}=\frac{1-\lambda}{2}$, that is, $\lambda = 1-2^\frac23$. This gives us $x=y=2^{-\frac23}$ and $z=2^{-\frac13}$.",,['multivariable-calculus']
65,How to find a partial derivative of an implicitly defined function at a point,How to find a partial derivative of an implicitly defined function at a point,,"Suppose that the relation $\frac{x^2}{2} + \frac{y^2}{2} + \frac{z^2}{2} + xy + xz =\frac{7}{2}$ defines $z$ as a function of $x, y$ around the point $(1, 1, 1)$. Find $\frac{dz}{dy}$ at $(1, 1, 1)$. Implicit differentiation can be used. I am not sure how to tackle this problem. All I know is that it has something to do with partial derivatives? Can someone please help me?","Suppose that the relation $\frac{x^2}{2} + \frac{y^2}{2} + \frac{z^2}{2} + xy + xz =\frac{7}{2}$ defines $z$ as a function of $x, y$ around the point $(1, 1, 1)$. Find $\frac{dz}{dy}$ at $(1, 1, 1)$. Implicit differentiation can be used. I am not sure how to tackle this problem. All I know is that it has something to do with partial derivatives? Can someone please help me?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
66,Higher orders of divergence and curl,Higher orders of divergence and curl,,"In the standard definition of div and curl, a limit is taken. If one instead expands the integral out into a series(Taylor?) of the vol/area then there are higher order terms that vanish. Do these higher order terms have any usefulness or commonly defined in mathematics or physics? For 2D, we can define the div and curl as $$div(F) = \lim_{C\rightarrow 0}\frac{1}{A}\int_C F \cdot \hat{r} dC$$ $$curl_z(F) = \lim_{C\rightarrow 0}\frac{1}{A}\int_C F \cdot \hat{v} dC$$ Where $C$ is a circle around the point under consideration, A its area, r is a normal vector to the circle, and v is a tangent vector. But what if we look at simply the integral $$\int_C F \cdot \hat{r} dC$$ and expand it out in a Taylor series about C? The the first non-constant term is the divergence of F, the term after that is what? Similarly we could do the same for the curl above.","In the standard definition of div and curl, a limit is taken. If one instead expands the integral out into a series(Taylor?) of the vol/area then there are higher order terms that vanish. Do these higher order terms have any usefulness or commonly defined in mathematics or physics? For 2D, we can define the div and curl as $$div(F) = \lim_{C\rightarrow 0}\frac{1}{A}\int_C F \cdot \hat{r} dC$$ $$curl_z(F) = \lim_{C\rightarrow 0}\frac{1}{A}\int_C F \cdot \hat{v} dC$$ Where $C$ is a circle around the point under consideration, A its area, r is a normal vector to the circle, and v is a tangent vector. But what if we look at simply the integral $$\int_C F \cdot \hat{r} dC$$ and expand it out in a Taylor series about C? The the first non-constant term is the divergence of F, the term after that is what? Similarly we could do the same for the curl above.",,"['differential-geometry', 'multivariable-calculus']"
67,Find the point or points on C closest to the origin.,Find the point or points on C closest to the origin.,,"A curve $C$ in space is defined implicitly on the cylinder $x^2 + y^2 = 1$ by the additional equation $x^2 - xy + y^2 - z^2 = 1$. Find the point or points on $C$ closest to the origin. This is an optimization problem. I tried constraining the distance $d = (x^2 + y^2 + z^2)^{1/2}$ to $x^2 - xy + y^2 - z^2 = 1$ by substituting $z^2$ into the distance equation and then finding the partial derivatives but I get $x=0, y=0$ which seems incorrect. Alternatively, I tried plugging $x^2 + y^2 = 1$ into the additional equation and then tried the same approach. I still got $x=0$ and $y=0$.","A curve $C$ in space is defined implicitly on the cylinder $x^2 + y^2 = 1$ by the additional equation $x^2 - xy + y^2 - z^2 = 1$. Find the point or points on $C$ closest to the origin. This is an optimization problem. I tried constraining the distance $d = (x^2 + y^2 + z^2)^{1/2}$ to $x^2 - xy + y^2 - z^2 = 1$ by substituting $z^2$ into the distance equation and then finding the partial derivatives but I get $x=0, y=0$ which seems incorrect. Alternatively, I tried plugging $x^2 + y^2 = 1$ into the additional equation and then tried the same approach. I still got $x=0$ and $y=0$.",,['multivariable-calculus']
68,Triple integral of a sphere being cut by a plane,Triple integral of a sphere being cut by a plane,,"Evaluate the triple integral of: $f(x,y,z)=z(x^2+y^2+z^2)^{-3/2}$ Over the part of the ball: $x^2+y^2+z^2\le 16$ with $z\ge 2$ So I converted to spherical coordinates and got: $$f(\rho,\phi,\theta)=\rho cos(\phi)\rho^{-3/2}$$ $$f(\rho,\phi,\theta)=cos(\phi)\rho^{-1/2}$$ Then for the bounds: $$\rho^2\le 16$$ $$\rho\le 4$$ and $$\rho cos(\phi)\ge 2$$ $$2/cos(\phi)\le \rho$$ So: $$2/cos(\phi)\le \rho\le 4$$ Then since the plane crosses the sphere at $z=2$ where $\rho=4$, and $z=\rho cos(\phi)$: $$4cos(\phi)=2$$ $$\phi = \pi/3$$ So: $$0\le\phi\le\pi/3$$ Since it is a sphere I have: $$0\le\theta\le 2\pi$$ So my bounds are: $$2/cos(\phi)\le \rho\le 4$$ $$2/cos(\phi)\le \rho\le 4$$ $$0\le\theta\le 2\pi$$ Over: $$f(\rho,\phi,\theta)=cos(\phi)\rho^{-1/2}$$ With $dV=\rho^2 sin(\phi)d\rho d\phi d\theta$, I write the integral as: $$\int_0^{2\pi}\int_0^{\pi/3}\int_{2/cos(\phi)}^4 \rho^{-1/2} cos(\phi) \rho^2 sin(\phi) d\rho d\phi d\theta$$ $$\int_0^{2\pi}\int_0^{\pi/3}\int_{2/cos(\phi)}^4 \rho^{3/2} cos(\phi) sin(\phi) d\rho d\phi d\theta$$ Is this all correct?","Evaluate the triple integral of: $f(x,y,z)=z(x^2+y^2+z^2)^{-3/2}$ Over the part of the ball: $x^2+y^2+z^2\le 16$ with $z\ge 2$ So I converted to spherical coordinates and got: $$f(\rho,\phi,\theta)=\rho cos(\phi)\rho^{-3/2}$$ $$f(\rho,\phi,\theta)=cos(\phi)\rho^{-1/2}$$ Then for the bounds: $$\rho^2\le 16$$ $$\rho\le 4$$ and $$\rho cos(\phi)\ge 2$$ $$2/cos(\phi)\le \rho$$ So: $$2/cos(\phi)\le \rho\le 4$$ Then since the plane crosses the sphere at $z=2$ where $\rho=4$, and $z=\rho cos(\phi)$: $$4cos(\phi)=2$$ $$\phi = \pi/3$$ So: $$0\le\phi\le\pi/3$$ Since it is a sphere I have: $$0\le\theta\le 2\pi$$ So my bounds are: $$2/cos(\phi)\le \rho\le 4$$ $$2/cos(\phi)\le \rho\le 4$$ $$0\le\theta\le 2\pi$$ Over: $$f(\rho,\phi,\theta)=cos(\phi)\rho^{-1/2}$$ With $dV=\rho^2 sin(\phi)d\rho d\phi d\theta$, I write the integral as: $$\int_0^{2\pi}\int_0^{\pi/3}\int_{2/cos(\phi)}^4 \rho^{-1/2} cos(\phi) \rho^2 sin(\phi) d\rho d\phi d\theta$$ $$\int_0^{2\pi}\int_0^{\pi/3}\int_{2/cos(\phi)}^4 \rho^{3/2} cos(\phi) sin(\phi) d\rho d\phi d\theta$$ Is this all correct?",,['multivariable-calculus']
69,Problem about finding region and function preserving areas,Problem about finding region and function preserving areas,,"This problem is driving me crazy. ""Let $S$ be a parametrized surface by $\varphi:[a,b]\times[c,d]\rightarrow\mathbb{R^3}$, $\varphi$ is of class $C^1$. Show that exists an open, connected set $D\subset\mathbb{R^2}$ and a bijection $f:D\rightarrow[a,b]\times[c,d]$ of class $C^1$ such that $\varphi\circ f$ preserves areas."" Here is what I did: I started supposing that $D$ has no holes, doing this it's possible to calculates the area of any region $D\ '\subset D$ by $$\int_{\alpha_1}^{\beta_1}g_1(x)-h_1(x) \ dx +\ldots+\int_{\alpha_n}^{\beta_n}g_n(x)-h_n(x) \ dx$$ where each $g_i(x)$ is the upper function and $h_i(x)$ is the lower function. To make it simpler, but not losing generality, I may consider only one region like this, so we have that $$\int_{\alpha}^{\beta}g(x)-h(x) \ dx$$ is the area of $D\ '$. *Note that $(\varphi\circ f)(x,y)=\Big(\varphi_1(f(x,y)),\varphi_2(f(x,y)),\varphi_3(f(x,y)\Big)$. The corresponding area in S is the area of $(\varphi\circ f)(D\ ')$, it's is given by $$\int_{D\ '} \bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f| dA=$$ $$=\int_\alpha^\beta\int_{h(x)}^{g(x)} \bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f|\ dy\ dx.$$ This takes me to think that $\bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f|=1$, that way I have the equality I'm looking for. This is probably wrong because all I have to do to solve this is equation is to find some $f$, it doesn't depend on $D$. Please, someone could point out what I'm doing wrong and show me the correct way. Thank you very much for your help.","This problem is driving me crazy. ""Let $S$ be a parametrized surface by $\varphi:[a,b]\times[c,d]\rightarrow\mathbb{R^3}$, $\varphi$ is of class $C^1$. Show that exists an open, connected set $D\subset\mathbb{R^2}$ and a bijection $f:D\rightarrow[a,b]\times[c,d]$ of class $C^1$ such that $\varphi\circ f$ preserves areas."" Here is what I did: I started supposing that $D$ has no holes, doing this it's possible to calculates the area of any region $D\ '\subset D$ by $$\int_{\alpha_1}^{\beta_1}g_1(x)-h_1(x) \ dx +\ldots+\int_{\alpha_n}^{\beta_n}g_n(x)-h_n(x) \ dx$$ where each $g_i(x)$ is the upper function and $h_i(x)$ is the lower function. To make it simpler, but not losing generality, I may consider only one region like this, so we have that $$\int_{\alpha}^{\beta}g(x)-h(x) \ dx$$ is the area of $D\ '$. *Note that $(\varphi\circ f)(x,y)=\Big(\varphi_1(f(x,y)),\varphi_2(f(x,y)),\varphi_3(f(x,y)\Big)$. The corresponding area in S is the area of $(\varphi\circ f)(D\ ')$, it's is given by $$\int_{D\ '} \bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f| dA=$$ $$=\int_\alpha^\beta\int_{h(x)}^{g(x)} \bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f|\ dy\ dx.$$ This takes me to think that $\bigg| \bigg( \frac{\partial\varphi_1(f)}{\partial x},\frac{\partial\varphi_2(f)}{\partial x},\frac{\partial\varphi_3(f)}{\partial x}, \bigg)\times\bigg( \frac{\partial\varphi_1(f)}{\partial y},\frac{\partial\varphi_2(f)}{\partial y},\frac{\partial\varphi_3(f)}{\partial y}, \bigg) \bigg|\cdot |J_f|=1$, that way I have the equality I'm looking for. This is probably wrong because all I have to do to solve this is equation is to find some $f$, it doesn't depend on $D$. Please, someone could point out what I'm doing wrong and show me the correct way. Thank you very much for your help.",,"['calculus', 'multivariable-calculus']"
70,Find volume inside the cone $ z= 2a-\sqrt{x^2+y^2} $ and inside the cylinder $x^2+y^2=2ay$,Find volume inside the cone  and inside the cylinder, z= 2a-\sqrt{x^2+y^2}  x^2+y^2=2ay,"I have this question and I have seen here that there are similar questions but I have been trying to figure it out for a while with no luck. I have to find volume inside the cone $z= 2a-\sqrt{x^2+y^2}$ and inside the cylinder $x^2+y^2=2ay$. I am trying to use cylindrical coordinates: $x=r\cos\theta\,\,\, y=r\sin\theta\,,\, z=z$. I think I am mainly having trouble finding the boundaries for the integration $\iiint rdrdzd\theta$: $0\leq \theta\leq 2\pi$, $0\leq z \leq 2a$, and for $r$. I think this is where I am wrong... I have $0 \leq r \leq z\,\sin(\theta)/(1-\sin(\theta)).$ If anyone knows or can give me some help I would appreciate it.","I have this question and I have seen here that there are similar questions but I have been trying to figure it out for a while with no luck. I have to find volume inside the cone $z= 2a-\sqrt{x^2+y^2}$ and inside the cylinder $x^2+y^2=2ay$. I am trying to use cylindrical coordinates: $x=r\cos\theta\,\,\, y=r\sin\theta\,,\, z=z$. I think I am mainly having trouble finding the boundaries for the integration $\iiint rdrdzd\theta$: $0\leq \theta\leq 2\pi$, $0\leq z \leq 2a$, and for $r$. I think this is where I am wrong... I have $0 \leq r \leq z\,\sin(\theta)/(1-\sin(\theta)).$ If anyone knows or can give me some help I would appreciate it.",,"['multivariable-calculus', 'integration', 'volume']"
71,"Contour integration, Residue Theorem","Contour integration, Residue Theorem",,"Evaluate the integral using contour integration $$ \int_{0}^{\infty }\frac{dx}{1+x^a} $$ I know how to solve the integral  $$ \int_{0}^{\infty }\frac{dx}{1+x^{2a}} $$ Using the residue theorem, however when I try to evaluate the first integral similar to the second integral the eventual residue sum $$ 2\pi i\sum res f(z) $$ Breaks down because: 1) the 2 in 2pi*i doesn't cancel 2) the numerator in that must equal 2 in order to follow the Euler Sine formula, however the numerator in my solution equals 0, which causes the entire series to collapse to 0. I'm using  $$ z^a =(-1); roots = e^{\pi i/a};  $$ yet the summation breaks down.  I have a feeling it's a simple fix, but I'm not sure where it is since I'm still pretty rusty with contour integration.  Any help is appreciated.","Evaluate the integral using contour integration $$ \int_{0}^{\infty }\frac{dx}{1+x^a} $$ I know how to solve the integral  $$ \int_{0}^{\infty }\frac{dx}{1+x^{2a}} $$ Using the residue theorem, however when I try to evaluate the first integral similar to the second integral the eventual residue sum $$ 2\pi i\sum res f(z) $$ Breaks down because: 1) the 2 in 2pi*i doesn't cancel 2) the numerator in that must equal 2 in order to follow the Euler Sine formula, however the numerator in my solution equals 0, which causes the entire series to collapse to 0. I'm using  $$ z^a =(-1); roots = e^{\pi i/a};  $$ yet the summation breaks down.  I have a feeling it's a simple fix, but I'm not sure where it is since I'm still pretty rusty with contour integration.  Any help is appreciated.",,"['multivariable-calculus', 'mathematical-physics']"
72,Implicit Function Theorem,Implicit Function Theorem,,"Q: Use Implicit Function Theorem to show that there exists a unique solution of the equation $x^{e^y} + y^{e^x} = 0$ in a neighborhood of the point $(0, 0)$. I tried to satisfy three conditions of IFT. Let $F(x,y)= x^{e^y} + y^{e^x}$ then, 1: $F(0,0)=0$ 2:  $\displaystyle{\frac{\partial F}{\partial y}= x^{e^y}e^y \log x + \frac{y^{e^x +1}}{e^x + 1}}\,$, I don't know whether it is continuous in the neighborhood of $(0,0)$ 3: $\frac{\partial F}{\partial y}(0,0)\neq 0$, but I don't know how to find the value of $\frac{\partial F}{\partial y}$ at $(0,0)$ since $\log x$ is not defined at $x=0$. Please help me.","Q: Use Implicit Function Theorem to show that there exists a unique solution of the equation $x^{e^y} + y^{e^x} = 0$ in a neighborhood of the point $(0, 0)$. I tried to satisfy three conditions of IFT. Let $F(x,y)= x^{e^y} + y^{e^x}$ then, 1: $F(0,0)=0$ 2:  $\displaystyle{\frac{\partial F}{\partial y}= x^{e^y}e^y \log x + \frac{y^{e^x +1}}{e^x + 1}}\,$, I don't know whether it is continuous in the neighborhood of $(0,0)$ 3: $\frac{\partial F}{\partial y}(0,0)\neq 0$, but I don't know how to find the value of $\frac{\partial F}{\partial y}$ at $(0,0)$ since $\log x$ is not defined at $x=0$. Please help me.",,['multivariable-calculus']
73,nonrectifiable curve,nonrectifiable curve,,"Consider the plane curve whose vector equation is $r(t) = ti +f(t)j$, where $$f(t)=t\cos\bigg(\frac{\pi}{2t}\bigg)$$ if $t$ is not equal $0$, and $0$ otherwise. Consider the following partition of the interval $[0,1]$ $$P=\bigg\{0,\frac{1}{2n},\frac{1}{2n-1},\ldots,\frac{1}{2},1\bigg\}$$ Show that corresponding inscribed polygon $$|\pi(P)|=\sum_{k=1}^n||r(t_k)-r(t_{k-1})||$$ has length  $$|\pi(P)|>1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{2n}$$ And deduce that the curve is nonrectifiable. Now this would follow if every member of the sum $|\pi(P)|$ is bigger than every member of the harmonic series. Tried to see directly - does not work. Another idea is to find a sequence so that the sum of it would be less than of the inscribed polygon but bigger than the harmonic series, but could not find one. How should I approach this problem? any hints? Thanks!","Consider the plane curve whose vector equation is $r(t) = ti +f(t)j$, where $$f(t)=t\cos\bigg(\frac{\pi}{2t}\bigg)$$ if $t$ is not equal $0$, and $0$ otherwise. Consider the following partition of the interval $[0,1]$ $$P=\bigg\{0,\frac{1}{2n},\frac{1}{2n-1},\ldots,\frac{1}{2},1\bigg\}$$ Show that corresponding inscribed polygon $$|\pi(P)|=\sum_{k=1}^n||r(t_k)-r(t_{k-1})||$$ has length  $$|\pi(P)|>1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{2n}$$ And deduce that the curve is nonrectifiable. Now this would follow if every member of the sum $|\pi(P)|$ is bigger than every member of the harmonic series. Tried to see directly - does not work. Another idea is to find a sequence so that the sum of it would be less than of the inscribed polygon but bigger than the harmonic series, but could not find one. How should I approach this problem? any hints? Thanks!",,['multivariable-calculus']
74,cross product and RHR,cross product and RHR,,"Edit : I really like the approach taken by $\mathbb{R}^n$ in the comments below. I posted this late last night and didn't get to ask him why $\bf R(a \times b) = Ra \times Rb$ for any rotation matrix $\bf R$. I'm not sure how to prove that: along with trying to evaluate the determinant directly, I looked in a linear algebra book and tried using the ""norm-preserving"" property of rotation matrices to evaluate $|\bf R(a \times b) - R(a) - R(b)|$ $^2$, but didn't succeed. How do you do this? In one of my courses, a professor briefly summarized properties of the cross product at the start of last class. I realized that the assertion $$ \bf a \times (b + c) = a\times b + a\times c$$ was actually surprising to me. Clearly this is fundamental (if you don't accept this, you can't derive a way of computing the cross product), but the proof is a bit slippery. If you start from $\bf{ a \times  b}$ $:= \bf \hat n |a||b|$ $ \sin\theta$, where $ \bf \hat n$ comes from the right hand rule, then if you can prove as a lemma $\bf a \cdot (b \times c) = (a \times b) \cdot c$, then there's a neat little proof which I found here . But the only argument I've seen for the needed lemma is about the volume of a parallelepiped, which only convinces me that $\bf | a \cdot (b \times c)| = |(a \times b) \cdot c|$. I think I prefer the approach in one of my textbooks, which starts by defining the cross product by determinants - so, distributivity holds - and proves most of the needed properties. But it wusses out at a crucial point: ""it can be proven that the orthogonal vector obtained from this matrix obeys the Right Hand Rule"". Could somebody either prove that lemma, or the textbook claim? (Preferably the latter.)","Edit : I really like the approach taken by $\mathbb{R}^n$ in the comments below. I posted this late last night and didn't get to ask him why $\bf R(a \times b) = Ra \times Rb$ for any rotation matrix $\bf R$. I'm not sure how to prove that: along with trying to evaluate the determinant directly, I looked in a linear algebra book and tried using the ""norm-preserving"" property of rotation matrices to evaluate $|\bf R(a \times b) - R(a) - R(b)|$ $^2$, but didn't succeed. How do you do this? In one of my courses, a professor briefly summarized properties of the cross product at the start of last class. I realized that the assertion $$ \bf a \times (b + c) = a\times b + a\times c$$ was actually surprising to me. Clearly this is fundamental (if you don't accept this, you can't derive a way of computing the cross product), but the proof is a bit slippery. If you start from $\bf{ a \times  b}$ $:= \bf \hat n |a||b|$ $ \sin\theta$, where $ \bf \hat n$ comes from the right hand rule, then if you can prove as a lemma $\bf a \cdot (b \times c) = (a \times b) \cdot c$, then there's a neat little proof which I found here . But the only argument I've seen for the needed lemma is about the volume of a parallelepiped, which only convinces me that $\bf | a \cdot (b \times c)| = |(a \times b) \cdot c|$. I think I prefer the approach in one of my textbooks, which starts by defining the cross product by determinants - so, distributivity holds - and proves most of the needed properties. But it wusses out at a crucial point: ""it can be proven that the orthogonal vector obtained from this matrix obeys the Right Hand Rule"". Could somebody either prove that lemma, or the textbook claim? (Preferably the latter.)",,"['linear-algebra', 'multivariable-calculus']"
75,Meaning of scalar product with normal in fluid flow,Meaning of scalar product with normal in fluid flow,,I'm having trouble with understanding the meaning of the following sentence: Let $n$ be the outward normal defined at points of the boundary of a region $W$  and let $dA$ denote the area element on this boundary.    The volume flow rate across $\partial W$ per unit area is $$u \cdot n$$ Can someone please explain to me why does this scalar product interpretation is the volume flow rate? Is there any way I can see this? [edit: $u$ is the velocity field of the fluid ] Thanks !!!,I'm having trouble with understanding the meaning of the following sentence: Let $n$ be the outward normal defined at points of the boundary of a region $W$  and let $dA$ denote the area element on this boundary.    The volume flow rate across $\partial W$ per unit area is $$u \cdot n$$ Can someone please explain to me why does this scalar product interpretation is the volume flow rate? Is there any way I can see this? [edit: $u$ is the velocity field of the fluid ] Thanks !!!,,"['calculus', 'multivariable-calculus']"
76,Finding the equation of a line entirely defined by a three variable equation.,Finding the equation of a line entirely defined by a three variable equation.,,"How can you find the equation of a line that lies completely in a set defined by a three variable equation. For instance, the equation of a line entirely in the set defined by $x^2 + y^2 - z^2 = 1$ Supposedly a potential answer is the line defined by $x=1, y=t$, and $z=t$, but I don't understand how one finds that answer.","How can you find the equation of a line that lies completely in a set defined by a three variable equation. For instance, the equation of a line entirely in the set defined by $x^2 + y^2 - z^2 = 1$ Supposedly a potential answer is the line defined by $x=1, y=t$, and $z=t$, but I don't understand how one finds that answer.",,"['multivariable-calculus', 'vector-spaces']"
77,"Given a collection of points in the closed unit ball, is there a smooth curve that fits it?","Given a collection of points in the closed unit ball, is there a smooth curve that fits it?",,"Let a set of countable points in the closed unit ball in $\mathbb R^n$ be given. Can we find a line $\lbrace tv: v \in \mathbb R^n, t \in \mathbb R \rbrace$ in it that contains an infinite number of these points. What if one relaxes the condition for a line to allow a smooth curve? Added I think the idea should be something like this: WLOG assume the points accumulates to the origin, then for each $k$, consider finite points in the annulus $B(0,\frac 1k)-B(0,\frac{1}{k+1})$ and a smooth curve $\gamma_k$ joining them whose minimum speed is a function of $\frac 1k$","Let a set of countable points in the closed unit ball in $\mathbb R^n$ be given. Can we find a line $\lbrace tv: v \in \mathbb R^n, t \in \mathbb R \rbrace$ in it that contains an infinite number of these points. What if one relaxes the condition for a line to allow a smooth curve? Added I think the idea should be something like this: WLOG assume the points accumulates to the origin, then for each $k$, consider finite points in the annulus $B(0,\frac 1k)-B(0,\frac{1}{k+1})$ and a smooth curve $\gamma_k$ joining them whose minimum speed is a function of $\frac 1k$",,['multivariable-calculus']
78,A relation involving surface integral,A relation involving surface integral,,"I am a little confused on how to apply a change of variables to a surface integral.  If I have  $ \int_\Sigma F\cdot N dS  $, and a nice map to another surface, say $f$, do I apply the change of variables as $\int_{f^{-1}(\Sigma)}F\cdot N \ |J(f^{-1})|\ dS$, with $J(f)$ the Jacobian? The particular problem is: $\Sigma = \{ (x,y,g(x,y))\in R^3 : x^2+y^2 \leq 1 \}$ where $g$ is $C^2 $ and the graph has the property that every ray from the origin in $R^3$ intersects $\Sigma$ at most once.  We are given that $\Sigma$ is contained in some ball of radius R centered at the origin, and need to relate the integral $\int_\Sigma \nabla f \cdot N dS$, where $f=\frac{1}{\|x\|}$, to a formula involving R and the area of the image of $\Sigma$ under the projection to the boundary of the R-ball. I notice that $\nabla f \cdot N$ becomes the constant $-\frac{1}{R^2}$, so can I say that the previous integral is equal to $-\frac{1}{R^2}Area(E)$, where $E$ is the area of the projection of $\Sigma$ to the surface of the ball?  I know this is not quite right, but I have to be on the right track.  Again, I am not confortable applying change of variables to a general surface integral, and any help would be appreciated.  Thanks!","I am a little confused on how to apply a change of variables to a surface integral.  If I have  $ \int_\Sigma F\cdot N dS  $, and a nice map to another surface, say $f$, do I apply the change of variables as $\int_{f^{-1}(\Sigma)}F\cdot N \ |J(f^{-1})|\ dS$, with $J(f)$ the Jacobian? The particular problem is: $\Sigma = \{ (x,y,g(x,y))\in R^3 : x^2+y^2 \leq 1 \}$ where $g$ is $C^2 $ and the graph has the property that every ray from the origin in $R^3$ intersects $\Sigma$ at most once.  We are given that $\Sigma$ is contained in some ball of radius R centered at the origin, and need to relate the integral $\int_\Sigma \nabla f \cdot N dS$, where $f=\frac{1}{\|x\|}$, to a formula involving R and the area of the image of $\Sigma$ under the projection to the boundary of the R-ball. I notice that $\nabla f \cdot N$ becomes the constant $-\frac{1}{R^2}$, so can I say that the previous integral is equal to $-\frac{1}{R^2}Area(E)$, where $E$ is the area of the projection of $\Sigma$ to the surface of the ball?  I know this is not quite right, but I have to be on the right track.  Again, I am not confortable applying change of variables to a general surface integral, and any help would be appreciated.  Thanks!",,['multivariable-calculus']
79,Modification of Divergence theorem,Modification of Divergence theorem,,"For Green's theorem in complex plane , with the help of Cauchy-Riemann equations , we can modify it to become $\iint_B{f^{'}(z)}dxdy=\frac{i}{2} \int_{\partial B}{f(z)}d\overline{z}$ where $d\overline{z}=dx-idy$ . Can we do the same for divergence theorem? In other words, can we change the divergence theorem involves only function instead of vector field ?","For Green's theorem in complex plane , with the help of Cauchy-Riemann equations , we can modify it to become where . Can we do the same for divergence theorem? In other words, can we change the divergence theorem involves only function instead of vector field ?",\iint_B{f^{'}(z)}dxdy=\frac{i}{2} \int_{\partial B}{f(z)}d\overline{z} d\overline{z}=dx-idy,"['calculus', 'multivariable-calculus']"
80,Global maximum/ minimum of a function of more than one variable.,Global maximum/ minimum of a function of more than one variable.,,"Please, can someone give me more information on how to check if points are local or global maximum/minimum. I am aware of the second derivative test of determining the local minimum/maximum. But how do I check for global min/max? An example to illustrate this would be very much appreciated. Thanks.","Please, can someone give me more information on how to check if points are local or global maximum/minimum. I am aware of the second derivative test of determining the local minimum/maximum. But how do I check for global min/max? An example to illustrate this would be very much appreciated. Thanks.",,['multivariable-calculus']
81,Saddle points - Show that surface $z = y \sin x$ has infinitely many saddle points.,Saddle points - Show that surface  has infinitely many saddle points.,z = y \sin x,Show that surface $z = y \sin x$ has infinitely many saddle points. Can someone show me the step-by-step solution for that statement? Detailed explanations will be appreciated. Thank you very much!,Show that surface $z = y \sin x$ has infinitely many saddle points. Can someone show me the step-by-step solution for that statement? Detailed explanations will be appreciated. Thank you very much!,,['multivariable-calculus']
82,"Find an equation of the plane that passes through the point $(1,2,3)$, and cuts off the smallest volume in the first octant. *help needed please*","Find an equation of the plane that passes through the point , and cuts off the smallest volume in the first octant. *help needed please*","(1,2,3)","Find an equation of the plane that passes through the point $(1,2,3)$, and cuts off the smallest volume in the first octant. This is what i've done so far.... Let $a,b,c$ be some points that the plane cuts the $x,y,z$ axes. --> $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, where $a,b,c >0$. I saw a solution for this question was to use Lagrange multiplier. The solution goes as follows... The product $abc$ will be equal to $6$ times the volume of the tetrahedron $OABC$ (could someone explain to my why is this so?) $f(a,b,c) = abc$ given the condition $(\frac1a + \frac2b + \frac3b -1)$ $f(a,b,c) = abc + \lambda (\frac1a + \frac2b + \frac3c -1)$ 2nd query to the question... $f_a = \lambda g_a \Rightarrow  bc - \frac\lambda {a^2} ; a = \sqrt \frac \lambda {bc}  \\f_b = \lambda g_b  \Rightarrow  ac - \frac\lambda {b^2} ; b = \sqrt \frac {2\lambda}{ac} \\f_c = \lambda g_c  \Rightarrow  ab - \frac\lambda {c^2} ; c = \sqrt \frac {3\lambda}{ab}$ using values of $a,b,c$ into $\frac1a+\frac1b+\frac1c = 1\Rightarrow \lambda =\frac{abc}{a+2b+3c}$. May i know how should i proceed to solve the unknowns?","Find an equation of the plane that passes through the point $(1,2,3)$, and cuts off the smallest volume in the first octant. This is what i've done so far.... Let $a,b,c$ be some points that the plane cuts the $x,y,z$ axes. --> $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, where $a,b,c >0$. I saw a solution for this question was to use Lagrange multiplier. The solution goes as follows... The product $abc$ will be equal to $6$ times the volume of the tetrahedron $OABC$ (could someone explain to my why is this so?) $f(a,b,c) = abc$ given the condition $(\frac1a + \frac2b + \frac3b -1)$ $f(a,b,c) = abc + \lambda (\frac1a + \frac2b + \frac3c -1)$ 2nd query to the question... $f_a = \lambda g_a \Rightarrow  bc - \frac\lambda {a^2} ; a = \sqrt \frac \lambda {bc}  \\f_b = \lambda g_b  \Rightarrow  ac - \frac\lambda {b^2} ; b = \sqrt \frac {2\lambda}{ac} \\f_c = \lambda g_c  \Rightarrow  ab - \frac\lambda {c^2} ; c = \sqrt \frac {3\lambda}{ab}$ using values of $a,b,c$ into $\frac1a+\frac1b+\frac1c = 1\Rightarrow \lambda =\frac{abc}{a+2b+3c}$. May i know how should i proceed to solve the unknowns?",,"['multivariable-calculus', 'lagrange-multiplier']"
83,Harmonic function with condition on part of its boundary,Harmonic function with condition on part of its boundary,,"Suppose $u$ is harmonic in the interior of the unit square $0 \leq x \leq 1$, $0\leq y\leq1$. Suppose furthermore that $u$ and its first derivatives continuously extend to the bottom side $0\leq x \leq 1$, $y=0$ and that $u = \partial_y u = 0$ there. Is it true that $u \equiv 0$? If so, can this result be extended to the unit n-cube with 'side' above replaced with the n-1-cube obtained by fixing a coordinate to 0?","Suppose $u$ is harmonic in the interior of the unit square $0 \leq x \leq 1$, $0\leq y\leq1$. Suppose furthermore that $u$ and its first derivatives continuously extend to the bottom side $0\leq x \leq 1$, $y=0$ and that $u = \partial_y u = 0$ there. Is it true that $u \equiv 0$? If so, can this result be extended to the unit n-cube with 'side' above replaced with the n-1-cube obtained by fixing a coordinate to 0?",,"['multivariable-calculus', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
84,"If $u = f(x,y)$, where $x=e^s \cos t$ and $y = e^s \sin t$, calculate $\frac{\partial^2u}{\partial x^2} + \frac{\partial^2u}{\partial y^2}$","If , where  and , calculate","u = f(x,y) x=e^s \cos t y = e^s \sin t \frac{\partial^2u}{\partial x^2} + \frac{\partial^2u}{\partial y^2}","If $ u = f(x,y)$, where $x=e^s \cos t$ and $y = e^s \sin t$, show that $$ \frac{\partial ^2u}{\partial x^2}  + \frac{\partial ^2u}{\partial y^2}= e^{-2s}\left[\frac{\partial ^2 u}{\partial s^2}+ \frac{\partial ^2 u}{\partial t^2}\right].$$ Currently, what I've done: $$\begin{align} f_x &= (f_s, f_t), \\f_s &= e^s \cos t,  \\f_t &= -e^s \sin t, \\(f_x)_x &= ((f_s)_s, (f_t)_t), \\(f_s)_s &= e^s \cos t, \\(f_s)_t &= -e^s \sin t \\(f_t)_s &= -e^s \sin t \\ (f_t)_t &= -e^s \cos t \\ \  \\f_y &=(f_s, f_t),  \\f_s &= e^s \sin t,  \\f_t &= e^s \cos t, \\(f_y)_y &= ((f_s)_s, (f_t)_t), \\(f_s)_s &= e^s \sin t, \\(f_s)_t &= e^s \cos t \\(f_t)_s &= e^s \cos t \\ (f_t)_t &= -e^s \sin t \end{align}$$ Could tutors over here advise me whether I am on the right track, as I have no idea how to proceed from here. Thanks :)","If $ u = f(x,y)$, where $x=e^s \cos t$ and $y = e^s \sin t$, show that $$ \frac{\partial ^2u}{\partial x^2}  + \frac{\partial ^2u}{\partial y^2}= e^{-2s}\left[\frac{\partial ^2 u}{\partial s^2}+ \frac{\partial ^2 u}{\partial t^2}\right].$$ Currently, what I've done: $$\begin{align} f_x &= (f_s, f_t), \\f_s &= e^s \cos t,  \\f_t &= -e^s \sin t, \\(f_x)_x &= ((f_s)_s, (f_t)_t), \\(f_s)_s &= e^s \cos t, \\(f_s)_t &= -e^s \sin t \\(f_t)_s &= -e^s \sin t \\ (f_t)_t &= -e^s \cos t \\ \  \\f_y &=(f_s, f_t),  \\f_s &= e^s \sin t,  \\f_t &= e^s \cos t, \\(f_y)_y &= ((f_s)_s, (f_t)_t), \\(f_s)_s &= e^s \sin t, \\(f_s)_t &= e^s \cos t \\(f_t)_s &= e^s \cos t \\ (f_t)_t &= -e^s \sin t \end{align}$$ Could tutors over here advise me whether I am on the right track, as I have no idea how to proceed from here. Thanks :)",,['multivariable-calculus']
85,Iterated Integral requiring u-substitution,Iterated Integral requiring u-substitution,,"This is an iterated integral. I have tried solving it several times using u-substition, but I am not getting the correct answer. My latest result is (4/15)(10^(5/2)-33). Obviously something is off, but what? Could you please show me some steps and your final answer so I can work it out on my own and make sure I get it right? Calculate the iterated integral: $$ \int_0^3\int_0^1 4xy\sqrt{x^2+y^2}~dy~dx $$ My latest attempt: u=x^2+y^2  _  du=2ydy _ @y=1, u=x^2+1 _ @y=0, u=x^2 int(0 to 3)[int(x^2 to x^2+1) 2xu^(1/2)du]dx = int(0, 3)[4/3 * xu^(3/2) for u= x^2 to u=x^2+1]dx = 4/3 * int(0 to 3) [x(x^2+1)^(3/2) - x(x^2)^(3/2)]dx = 4/3 int(0 to 3) [x(x^2+1)^(3/2)]dx - 4/3 int(0 to 3) [x^4]dx v= x^2+1 _ dv=2xdx _ @x=3, v=10 _ @x=0, v=1 4/3 int(1 to 10) [1/2 * v^(3/2)]dv - 4/3 int(0 to 3) [x^4]dx = 2/3[2/5 * v^(5/2) for v=1 to v=10] - 4/3[1/5 * x^5 for x=0 to x=3] =4/15(10^(5/2) - 1) - 4/15 (32 - 0) = 4/15(10^(5/2) - 33)","This is an iterated integral. I have tried solving it several times using u-substition, but I am not getting the correct answer. My latest result is (4/15)(10^(5/2)-33). Obviously something is off, but what? Could you please show me some steps and your final answer so I can work it out on my own and make sure I get it right? Calculate the iterated integral: $$ \int_0^3\int_0^1 4xy\sqrt{x^2+y^2}~dy~dx $$ My latest attempt: u=x^2+y^2  _  du=2ydy _ @y=1, u=x^2+1 _ @y=0, u=x^2 int(0 to 3)[int(x^2 to x^2+1) 2xu^(1/2)du]dx = int(0, 3)[4/3 * xu^(3/2) for u= x^2 to u=x^2+1]dx = 4/3 * int(0 to 3) [x(x^2+1)^(3/2) - x(x^2)^(3/2)]dx = 4/3 int(0 to 3) [x(x^2+1)^(3/2)]dx - 4/3 int(0 to 3) [x^4]dx v= x^2+1 _ dv=2xdx _ @x=3, v=10 _ @x=0, v=1 4/3 int(1 to 10) [1/2 * v^(3/2)]dv - 4/3 int(0 to 3) [x^4]dx = 2/3[2/5 * v^(5/2) for v=1 to v=10] - 4/3[1/5 * x^5 for x=0 to x=3] =4/15(10^(5/2) - 1) - 4/15 (32 - 0) = 4/15(10^(5/2) - 33)",,"['integration', 'multivariable-calculus', 'definite-integrals']"
86,Proof of the definition corollary of curl,Proof of the definition corollary of curl,,"$$(\nabla \times \mathbf{F}) \cdot \mathbf{\hat{n}} \ \overset{\underset{\mathrm{def}}{}}{=} \lim_{A \to 0}\left( \frac{1}{|A|}\oint_{C} \mathbf{F} \cdot d\mathbf{r}\right)$$ This is how the curl operator is usually defined, and I want to know the proof that the left hand and the right hand are equivalent.","$$(\nabla \times \mathbf{F}) \cdot \mathbf{\hat{n}} \ \overset{\underset{\mathrm{def}}{}}{=} \lim_{A \to 0}\left( \frac{1}{|A|}\oint_{C} \mathbf{F} \cdot d\mathbf{r}\right)$$ This is how the curl operator is usually defined, and I want to know the proof that the left hand and the right hand are equivalent.",,['multivariable-calculus']
87,Chain Rule of Partial Derivatives,Chain Rule of Partial Derivatives,,"If $f$ is a differentiable function defined $f: \mathbb{R}^2 \to \mathbb{R}$ and $f(2,1) = 3$ and $\nabla f(2,1) = (4,3)$, find $\nabla G(1,1)$ with $G(x,y) := x^2yf(x^2 +y^2, xy)$. I wrote the $G_x$ as $2xyf(x^2 +y^2, xy) + x^2yf'(x^2 + y^2, xy)f_x(x^2 + y^2, xy)$ but I don't know what the value of $f'(x^2 + y^2, xy)$ is. Thanks in advance","If $f$ is a differentiable function defined $f: \mathbb{R}^2 \to \mathbb{R}$ and $f(2,1) = 3$ and $\nabla f(2,1) = (4,3)$, find $\nabla G(1,1)$ with $G(x,y) := x^2yf(x^2 +y^2, xy)$. I wrote the $G_x$ as $2xyf(x^2 +y^2, xy) + x^2yf'(x^2 + y^2, xy)f_x(x^2 + y^2, xy)$ but I don't know what the value of $f'(x^2 + y^2, xy)$ is. Thanks in advance",,['multivariable-calculus']
88,Line integral evaluation,Line integral evaluation,,"Find line integral: $$I = \oint\limits_{C} (y-z)\mathrm{d}x + (x^2-y)\mathrm{d}y + (z-x)\mathrm{d}z$$ where curve C is given with: $$\begin{array}\\ x = a\cos{t}\\ y = a\sin{t}\\ z = a^2\cos{2t}\\ \end{array}$$ and where $t\in \mathbb{R}, 0<t<2\pi$, and direction of $C$ is the same as the direction of growth of variable $t$. What I've tried so far: 1) If we go standardly: $\mathrm{d}x=-a\sin{t}\mathrm{d}t$, $\mathrm{d}y=a\cos(t)\mathrm{d}t$, $\mathrm{d}z=-4a^2\sin{t}\cos{t}\mathrm{d}t$, and substitute all into integral, we get: $$\int\limits_{0}^{2\pi}-a^2\sin^2{t} + 3a^3\cos^2{t}\sin{t} - a^3\sin^3{t} + a^3\cos^3{t} - a^2\sin{t}\cos{t}-4a^4\cos^3{t}\sin{t} + 4a^4\sin^3{t}\cos{t})\mathrm{d}t$$ but this is very ugly and I have no idea how to proceed, apart from trying random trig manipulations, which I've tried to no success. 2) If we realize that $z = a^2\cos^2{t} - a^2\sin^2{t} = x^2 - y^2$, and that $\mathrm{d}x=-y\mathrm{d}t$, $\mathrm{d}y=x\mathrm{d}t$, and $\mathrm{d}z=-4xy\mathrm{d}t$, we can substitute that into the integral, and get: $$ \int\limits_0^{2\pi}\left(y^2+3x^2y-y^3+x^3-xy-4x^3y+4xy^3\right)\mathrm{d}t, $$ but this is also kind of hopeless :)","Find line integral: $$I = \oint\limits_{C} (y-z)\mathrm{d}x + (x^2-y)\mathrm{d}y + (z-x)\mathrm{d}z$$ where curve C is given with: $$\begin{array}\\ x = a\cos{t}\\ y = a\sin{t}\\ z = a^2\cos{2t}\\ \end{array}$$ and where $t\in \mathbb{R}, 0<t<2\pi$, and direction of $C$ is the same as the direction of growth of variable $t$. What I've tried so far: 1) If we go standardly: $\mathrm{d}x=-a\sin{t}\mathrm{d}t$, $\mathrm{d}y=a\cos(t)\mathrm{d}t$, $\mathrm{d}z=-4a^2\sin{t}\cos{t}\mathrm{d}t$, and substitute all into integral, we get: $$\int\limits_{0}^{2\pi}-a^2\sin^2{t} + 3a^3\cos^2{t}\sin{t} - a^3\sin^3{t} + a^3\cos^3{t} - a^2\sin{t}\cos{t}-4a^4\cos^3{t}\sin{t} + 4a^4\sin^3{t}\cos{t})\mathrm{d}t$$ but this is very ugly and I have no idea how to proceed, apart from trying random trig manipulations, which I've tried to no success. 2) If we realize that $z = a^2\cos^2{t} - a^2\sin^2{t} = x^2 - y^2$, and that $\mathrm{d}x=-y\mathrm{d}t$, $\mathrm{d}y=x\mathrm{d}t$, and $\mathrm{d}z=-4xy\mathrm{d}t$, we can substitute that into the integral, and get: $$ \int\limits_0^{2\pi}\left(y^2+3x^2y-y^3+x^3-xy-4x^3y+4xy^3\right)\mathrm{d}t, $$ but this is also kind of hopeless :)",,"['integration', 'multivariable-calculus']"
89,Is there a lower dimensional vector operator analogous to Curl or Div?,Is there a lower dimensional vector operator analogous to Curl or Div?,,The Curl vector operator can be defined as the limit of an infinitesimal closed line integral over a surface divided by the |area| as it approaches zero. Likewise for Div as the limit of an infinitesimal surface integral over a volume divided by the |volume| as it approaches zero. Taking the analogy to a lower dimension: Is there a lower dimensional vector operator analogous to Curl and Div? Is there a theorem associated with it analogous to Stokes' for Curl or Ostrogradsky's for Div?,The Curl vector operator can be defined as the limit of an infinitesimal closed line integral over a surface divided by the |area| as it approaches zero. Likewise for Div as the limit of an infinitesimal surface integral over a volume divided by the |volume| as it approaches zero. Taking the analogy to a lower dimension: Is there a lower dimensional vector operator analogous to Curl and Div? Is there a theorem associated with it analogous to Stokes' for Curl or Ostrogradsky's for Div?,,['multivariable-calculus']
90,Multiple Integration using polar coordinates,Multiple Integration using polar coordinates,,I am having some problem with caluculating areas using double integrals in polar coordinates. The question is :  Calculate the total area of the rose $ r = 5 \sin(2 \theta)$ using double integration in polar coordinates? I took the limits of theta from $0$ to $\pi \over 4$ and that for $r$ from $0$ to $5 \sin(2 \theta)$ and multiplied it by $2$ to get the area of one petal of the rose and then multiplied it by 4 to get the total area. My answer comes out to be $ 25 \pi \over 2$. Have I followed the right procedure and is the answer correct?,I am having some problem with caluculating areas using double integrals in polar coordinates. The question is :  Calculate the total area of the rose $ r = 5 \sin(2 \theta)$ using double integration in polar coordinates? I took the limits of theta from $0$ to $\pi \over 4$ and that for $r$ from $0$ to $5 \sin(2 \theta)$ and multiplied it by $2$ to get the area of one petal of the rose and then multiplied it by 4 to get the total area. My answer comes out to be $ 25 \pi \over 2$. Have I followed the right procedure and is the answer correct?,,['multivariable-calculus']
91,dot product identity,dot product identity,,"$$a \cdot (a \cdot b)=(a \cdot a)(a \cdot b)$$ Is this identity true when $a$ and $b$ are vectors, and when $\cdot$ is the dot product operator?  And assuming that $()()$ means multiplying the contents of the parentheses. Can anyone please post explanation and links of this identity?  I saw it used somewhere, but I cannot seem to find documentation of it in textbooks or in search engine research using the key words I could think of. Thank you. It is used in a suggested solution for the following problem: ""Show that the vector orth_a b = b - proj_a b is orthogonal to a.  (It is called an orthogonal projection of b.)"" Where a and b are vectors, and proj_a b is the vector projection of b onto a. Really what I need is to learn to answer this problem, and the part of the suggested proof I don't understand is given in my initial post above.  If you have a better answer to this problem, I would love to learn how your approach works.","$$a \cdot (a \cdot b)=(a \cdot a)(a \cdot b)$$ Is this identity true when $a$ and $b$ are vectors, and when $\cdot$ is the dot product operator?  And assuming that $()()$ means multiplying the contents of the parentheses. Can anyone please post explanation and links of this identity?  I saw it used somewhere, but I cannot seem to find documentation of it in textbooks or in search engine research using the key words I could think of. Thank you. It is used in a suggested solution for the following problem: ""Show that the vector orth_a b = b - proj_a b is orthogonal to a.  (It is called an orthogonal projection of b.)"" Where a and b are vectors, and proj_a b is the vector projection of b onto a. Really what I need is to learn to answer this problem, and the part of the suggested proof I don't understand is given in my initial post above.  If you have a better answer to this problem, I would love to learn how your approach works.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
92,Chain rule for multivariable functions confusion,Chain rule for multivariable functions confusion,,"Suppose $f=f(x,y(x))$. Then applying the chain rule we get $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}$. From this it seems that it always holds that $\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=0$. Where's the mistake?","Suppose $f=f(x,y(x))$. Then applying the chain rule we get $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}$. From this it seems that it always holds that $\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=0$. Where's the mistake?",,"['multivariable-calculus', 'derivatives']"
93,How to find extrema on a triangle,How to find extrema on a triangle,,"Let $T\subset\mathbb{R}^2$ be the (closed) triangle bounded by the lines $x+y=4$, $x\ge-1$ and $y\ge-1$. I want to find and classify all the extrema of the function $f(x,y)=-x^2y(x+y-2)$ on the triangle $T$. I've done the following: Solving $\nabla f=0$ gives the points $(0,0), (0,1)$ and $0,2$. But when I compute the Hessian , I keep getting eigenvalue 0. Is there someone who can show me how to solve this problem?","Let $T\subset\mathbb{R}^2$ be the (closed) triangle bounded by the lines $x+y=4$, $x\ge-1$ and $y\ge-1$. I want to find and classify all the extrema of the function $f(x,y)=-x^2y(x+y-2)$ on the triangle $T$. I've done the following: Solving $\nabla f=0$ gives the points $(0,0), (0,1)$ and $0,2$. But when I compute the Hessian , I keep getting eigenvalue 0. Is there someone who can show me how to solve this problem?",,"['calculus', 'multivariable-calculus']"
94,"Calculus on Manifolds (Spivak), problem 2-41(a)","Calculus on Manifolds (Spivak), problem 2-41(a)",,"Let $f:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ be differentiable. For each $x \in \mathbb{R}$ define $g_x:\mathbb{R}\to\mathbb{R}$ by $g_x(y) = f(x,y)$. Suppose that for each $x$ there is a unique $y$ with $g_x'(y) = 0$; let $c(x)$ be this $y$. (a) If $D_{2,2}(x,y) \neq 0$ for all $(x,y)$, show that $c$ is differentiable and $c'(x) = -\dfrac{D_{2,1}f(x,y)}{D_{2,2}f(x,y)}$. Hint : $g_x'(y) = 0$ can be written $D_2f(x,y) = 0$. I looked at the solution for this problem and it basically seems to go like this: Apply the implicit function theorem to obtain a differentiable local solution to $D_2f(x,y) = 0$ at each $(x, c(x))$. Since there is only one $c(x)$ for which $D_2f(x,c(x)) = 0$ at each $x$, each of these local solutions must agree with $c$. Therefore, by gluing together all these local solutions, we get an everywhere differentiable function identical to $c$. Since we know $c$ is a differentiable function, we can differentiate the relation $D_2f(x,c(x)) = 0$ using the chain rule (Theorem 2-9), which immediately gives the desired result. I understand everything in this solution except for the following: before we can apply the implicit function theorem to $D_2f(x,y) = 0$, we need $D_2f$ to be continuously differentiable, and the problem statement doesn't seem to permit that assumption. How is this resolved?","Let $f:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ be differentiable. For each $x \in \mathbb{R}$ define $g_x:\mathbb{R}\to\mathbb{R}$ by $g_x(y) = f(x,y)$. Suppose that for each $x$ there is a unique $y$ with $g_x'(y) = 0$; let $c(x)$ be this $y$. (a) If $D_{2,2}(x,y) \neq 0$ for all $(x,y)$, show that $c$ is differentiable and $c'(x) = -\dfrac{D_{2,1}f(x,y)}{D_{2,2}f(x,y)}$. Hint : $g_x'(y) = 0$ can be written $D_2f(x,y) = 0$. I looked at the solution for this problem and it basically seems to go like this: Apply the implicit function theorem to obtain a differentiable local solution to $D_2f(x,y) = 0$ at each $(x, c(x))$. Since there is only one $c(x)$ for which $D_2f(x,c(x)) = 0$ at each $x$, each of these local solutions must agree with $c$. Therefore, by gluing together all these local solutions, we get an everywhere differentiable function identical to $c$. Since we know $c$ is a differentiable function, we can differentiate the relation $D_2f(x,c(x)) = 0$ using the chain rule (Theorem 2-9), which immediately gives the desired result. I understand everything in this solution except for the following: before we can apply the implicit function theorem to $D_2f(x,y) = 0$, we need $D_2f$ to be continuously differentiable, and the problem statement doesn't seem to permit that assumption. How is this resolved?",,['multivariable-calculus']
95,Constrained optimization problems resulting in equal variable assignments,Constrained optimization problems resulting in equal variable assignments,,"Suppose you trying to find the extreme value of some function $f(x_1,x_2,\ldots,x_n)$ over the set $\{x_i\}_n$ that is constrained by an unweighted sum as follows $\sum_n x_i=S$.  Let's assume that $f(\cdot)$ is  differentiable and convex. Problems like these commonly arise in engineering and other disciplines, and one usually solves them using the method of Lagrange multipliers by constructing a Lagrangian multiplier $\mathcal{L}(\lambda,x_1,x_2,\ldots,x_n)=f(x_1,x_2,\ldots,x_n)+\lambda(\sum_n x_i-S)$ and finding the stationary point by solving a system of $n+1$ equations $\frac{\partial \mathcal{L}}{\partial x_i}=0~\mbox{for}~i=1,2,\ldots,n; \frac{\partial \mathcal{L}}{\partial \lambda}=0$. I am interested in $f(\cdot)$'s for which the result is equal $x_i$'s: $x_1=x_2=\ldots=x_n$ (under unweighted sum constraint).  Is there a way to ""test"" $f(\cdot)$ (or its derivates) to infer that the variable assignments must be equal at the stationary point? Seems to me that some kind of symmetry property is required in $f(\cdot)$ for that to occur, but I can't quite formulate it.","Suppose you trying to find the extreme value of some function $f(x_1,x_2,\ldots,x_n)$ over the set $\{x_i\}_n$ that is constrained by an unweighted sum as follows $\sum_n x_i=S$.  Let's assume that $f(\cdot)$ is  differentiable and convex. Problems like these commonly arise in engineering and other disciplines, and one usually solves them using the method of Lagrange multipliers by constructing a Lagrangian multiplier $\mathcal{L}(\lambda,x_1,x_2,\ldots,x_n)=f(x_1,x_2,\ldots,x_n)+\lambda(\sum_n x_i-S)$ and finding the stationary point by solving a system of $n+1$ equations $\frac{\partial \mathcal{L}}{\partial x_i}=0~\mbox{for}~i=1,2,\ldots,n; \frac{\partial \mathcal{L}}{\partial \lambda}=0$. I am interested in $f(\cdot)$'s for which the result is equal $x_i$'s: $x_1=x_2=\ldots=x_n$ (under unweighted sum constraint).  Is there a way to ""test"" $f(\cdot)$ (or its derivates) to infer that the variable assignments must be equal at the stationary point? Seems to me that some kind of symmetry property is required in $f(\cdot)$ for that to occur, but I can't quite formulate it.",,"['multivariable-calculus', 'optimization']"
96,Degenerate critical points and higher order derivatives (Intuition),Degenerate critical points and higher order derivatives (Intuition),,"The question I'd like to ask is about the relevance of higher order derivatives in determining the behaviour of a function around a critical point in the multivariate case. The question occurs towards the end and I am going to try to be clear about what I get and what I don't get. Now in a single variable case, we know from Taylor expansion around the critical point that $$\Delta y=\frac{1}{2}f''(x_{0})(x-x_{0})^2+R$$ where R is the remainder term which goes to zero one degree faster than the quadratic term, thus if $$f''(x_{0})=0$$, the behaviour around the critical point, i.e. the sign of $$\Delta y$$ is determined by the sign of the third derivative, assuming that its non zero and so on. In two variable case, we know from Taylor expansion around the critical point that $$\Delta y=\frac{1}{2}f_{xx}(x_{0},y_{0})(x-x_{0})^2+f_{xy}(x_{0},y_{0})(x-x_{0})(y-y_{0})+\frac{1}{2}f_{yy}(x_{0},y_{0})(y-y_{0})^2+R$$ Now if the determinant of the Hessian matrix is  strictly positive, then depending on the sign of $$f_{xx}$$, delta y is always positive or negative so we either have a local minimum or a local maximum. Similarly, if the determinant of the Hessian is strictly negative, then there are some (x,y) where the change is positive, and some (x,y) where the change is negative, so the critical point is a saddle point. Now what I don't get is, if the determinant is zero, then the polynomial becomes $$\Delta y=\frac{1}{2}f_{xx}(x_{0},y_{0})((x-x_{0})+\frac{f_{xy}(x_{0},y_{0})}{f_{xx}(x_{0},y_{0})}(y-y_{0}))^2+R$$ (completing the squares) for which the quadratic part is always positive or always negative, assuming that $$f_{xx}(x_{0},y_{0})$$ is non-zero (I don't have any problems with the case when it is zero). So how come the higher-order derivatives are relevant, since always positive or always negative implies local min or local max? The common explanation is that ""the derivative is degenerate in one direction so the behaviour in that direction is determined by the higher order terms"". I don't get this explanation from the expression above. Thanks. Note: I can get the intuition of the examples where partials drop down completely easily, i.e. for the ones where if you expand and take a Taylor expansion around (0,0), all the partials of second and third (or k-1th) order drops, so you are only left with the 4th (or kth) order term, which determines the behaviour of the function around (0,0). The examples that throw my intuition off are the ones (unfortunately, I can't provide an explicit function), where $$f_{xx}(critical)\neq 0,f_{yy}(critical)\neq 0,f_{xy}(critical)\neq 0 $$, but $$f_{xx}(critical)f_{yy}(critical)-f_{xy}(critical)^2=0$$. In those cases, the second order part of the Taylor expansion drops only partially, not completely. Is that because it loses part of it, it stops dominating the effects of higher order terms?","The question I'd like to ask is about the relevance of higher order derivatives in determining the behaviour of a function around a critical point in the multivariate case. The question occurs towards the end and I am going to try to be clear about what I get and what I don't get. Now in a single variable case, we know from Taylor expansion around the critical point that $$\Delta y=\frac{1}{2}f''(x_{0})(x-x_{0})^2+R$$ where R is the remainder term which goes to zero one degree faster than the quadratic term, thus if $$f''(x_{0})=0$$, the behaviour around the critical point, i.e. the sign of $$\Delta y$$ is determined by the sign of the third derivative, assuming that its non zero and so on. In two variable case, we know from Taylor expansion around the critical point that $$\Delta y=\frac{1}{2}f_{xx}(x_{0},y_{0})(x-x_{0})^2+f_{xy}(x_{0},y_{0})(x-x_{0})(y-y_{0})+\frac{1}{2}f_{yy}(x_{0},y_{0})(y-y_{0})^2+R$$ Now if the determinant of the Hessian matrix is  strictly positive, then depending on the sign of $$f_{xx}$$, delta y is always positive or negative so we either have a local minimum or a local maximum. Similarly, if the determinant of the Hessian is strictly negative, then there are some (x,y) where the change is positive, and some (x,y) where the change is negative, so the critical point is a saddle point. Now what I don't get is, if the determinant is zero, then the polynomial becomes $$\Delta y=\frac{1}{2}f_{xx}(x_{0},y_{0})((x-x_{0})+\frac{f_{xy}(x_{0},y_{0})}{f_{xx}(x_{0},y_{0})}(y-y_{0}))^2+R$$ (completing the squares) for which the quadratic part is always positive or always negative, assuming that $$f_{xx}(x_{0},y_{0})$$ is non-zero (I don't have any problems with the case when it is zero). So how come the higher-order derivatives are relevant, since always positive or always negative implies local min or local max? The common explanation is that ""the derivative is degenerate in one direction so the behaviour in that direction is determined by the higher order terms"". I don't get this explanation from the expression above. Thanks. Note: I can get the intuition of the examples where partials drop down completely easily, i.e. for the ones where if you expand and take a Taylor expansion around (0,0), all the partials of second and third (or k-1th) order drops, so you are only left with the 4th (or kth) order term, which determines the behaviour of the function around (0,0). The examples that throw my intuition off are the ones (unfortunately, I can't provide an explicit function), where $$f_{xx}(critical)\neq 0,f_{yy}(critical)\neq 0,f_{xy}(critical)\neq 0 $$, but $$f_{xx}(critical)f_{yy}(critical)-f_{xy}(critical)^2=0$$. In those cases, the second order part of the Taylor expansion drops only partially, not completely. Is that because it loses part of it, it stops dominating the effects of higher order terms?",,"['calculus', 'multivariable-calculus', 'approximation']"
97,Proving that inversions are isometries with respect to the hyperbolic metric.,Proving that inversions are isometries with respect to the hyperbolic metric.,,"I'd like to prove that the standard inversion $$(r,\theta)\mapsto\left(\frac{1}{r},\theta\right)$$ is an isometry with respect to the hyperbolic metric on the upper half-plane, and it would be nice to me to do the calculations using polar coordinates. First of all, I compute the length element in polar coordinates: since $x=r\cos\theta$ and $y=r\sin\theta$ we get $dx=dr\cos\theta-r\sin\theta d\theta$ and $dy=dr\sin\theta+r\cos\theta d\theta$, so that $$\frac{dx^2+dy^2}{y^2}=\frac{dr^2+r^2d\theta^2}{r^2\sin^2\theta}.$$ Now, call $(u,v)=\left(1/r,\theta\right)$ and compute $du=-dr/r^2$ and $dv=d\theta$. Therefore, the new length element expressed in polar coordinate is  $$\frac{du^2+dv^2}{v^2}=\left(\frac{dr^2}{r^4}+d\theta^2\right)\frac{1}{d\theta^2}=\frac{dr^2+r^4d\theta^2}{r^4d\theta^2},$$ which is not equal to the previous one. I guess that I'm wrong when I define $(u,v)$ directly in terms of polar coordinates, but I don't know how to fix it. Could you help me with that? Thanks.","I'd like to prove that the standard inversion $$(r,\theta)\mapsto\left(\frac{1}{r},\theta\right)$$ is an isometry with respect to the hyperbolic metric on the upper half-plane, and it would be nice to me to do the calculations using polar coordinates. First of all, I compute the length element in polar coordinates: since $x=r\cos\theta$ and $y=r\sin\theta$ we get $dx=dr\cos\theta-r\sin\theta d\theta$ and $dy=dr\sin\theta+r\cos\theta d\theta$, so that $$\frac{dx^2+dy^2}{y^2}=\frac{dr^2+r^2d\theta^2}{r^2\sin^2\theta}.$$ Now, call $(u,v)=\left(1/r,\theta\right)$ and compute $du=-dr/r^2$ and $dv=d\theta$. Therefore, the new length element expressed in polar coordinate is  $$\frac{du^2+dv^2}{v^2}=\left(\frac{dr^2}{r^4}+d\theta^2\right)\frac{1}{d\theta^2}=\frac{dr^2+r^4d\theta^2}{r^4d\theta^2},$$ which is not equal to the previous one. I guess that I'm wrong when I define $(u,v)$ directly in terms of polar coordinates, but I don't know how to fix it. Could you help me with that? Thanks.",,"['multivariable-calculus', 'riemannian-geometry', 'polar-coordinates']"
98,What does this notation mean? $\frac{\partial f}{\partial x}(x+y)=\frac{\partial }{\partial x}f(x+y)$,What does this notation mean?,\frac{\partial f}{\partial x}(x+y)=\frac{\partial }{\partial x}f(x+y),"$$\frac{\partial f}{\partial x}(x+y)=\frac{\partial }{\partial x}f(x+y)$$ I was just wondering what the left-hand side mean. (or how to do the operation based on the notation of the LHS, given a specific function $f$) In addition, when is such commutation true? Under what conditions?","$$\frac{\partial f}{\partial x}(x+y)=\frac{\partial }{\partial x}f(x+y)$$ I was just wondering what the left-hand side mean. (or how to do the operation based on the notation of the LHS, given a specific function $f$) In addition, when is such commutation true? Under what conditions?",,"['multivariable-calculus', 'notation']"
99,Scalar Product for Vector Space of Monomial Symmetric Functions,Scalar Product for Vector Space of Monomial Symmetric Functions,,"Suppose a multinomial $P(X_1, X_2,\ldots, X_n)$, that is given as a sum of monomials $m_\lambda$ with coefficients $c_k$: $$ P(\vec{X})=P(X_1, X_2,\ldots, X_n) = \sum_k c_k m_{\lambda_k} . $$ Since the monomials form a basis of the vector space of multinomials, there is also a scalar product  $$ c_k=\frac{1}{N}\langle m_{\lambda_k} \mid P\rangle, $$ where $N=\langle m_{\lambda_k}\mid m_{\lambda_k}\rangle$ would be a normalization constant. My question is: Does the Scalar Product, such that the $m_λ$ are mutually orthogonal or better orthonormal, have an elementary expression? An application could allow calculation of Kostka number, since  $$ s_{\lambda} = \sum K_{\lambda\mu} m_\mu, $$  where $s_\lambda$ is a Schur polynomial. If this is an efficient way or not, is a different question. First I thought that I had to deal with something like square integrable functions , but then I found what I posted below $\dots$","Suppose a multinomial $P(X_1, X_2,\ldots, X_n)$, that is given as a sum of monomials $m_\lambda$ with coefficients $c_k$: $$ P(\vec{X})=P(X_1, X_2,\ldots, X_n) = \sum_k c_k m_{\lambda_k} . $$ Since the monomials form a basis of the vector space of multinomials, there is also a scalar product  $$ c_k=\frac{1}{N}\langle m_{\lambda_k} \mid P\rangle, $$ where $N=\langle m_{\lambda_k}\mid m_{\lambda_k}\rangle$ would be a normalization constant. My question is: Does the Scalar Product, such that the $m_λ$ are mutually orthogonal or better orthonormal, have an elementary expression? An application could allow calculation of Kostka number, since  $$ s_{\lambda} = \sum K_{\lambda\mu} m_\mu, $$  where $s_\lambda$ is a Schur polynomial. If this is an efficient way or not, is a different question. First I thought that I had to deal with something like square integrable functions , but then I found what I posted below $\dots$",,"['polynomials', 'multivariable-calculus', 'vector-spaces', 'inner-products', 'symmetric-functions']"
