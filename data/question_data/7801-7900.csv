,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sequence Convergence of $\sum_n\frac{(-1)^{n+1}}{3n + n(-1)^n}$,Sequence Convergence of,\sum_n\frac{(-1)^{n+1}}{3n + n(-1)^n},"I have the following series $\displaystyle \sum_{n=1}^{+\infty} \frac{(-1)^{n+1}}{3n + n(-1)^n}$. Does it converge? I wanted to the alternating series test, but that's not easy because of the two $(-1)^n$, plus it's not monotone decreasing. I've split it into two series $n=2n \Rightarrow \sum{-\frac 1 {4m}}\Rightarrow$ diverges, $n=2n+1 \Rightarrow \sum{\frac 1 {2m}}\Rightarrow$ diverges. I can't say diverges + diverges = diverges, but I have this hunch, that the positive series diverges faster than the negative series and therefore it diverges. However, I don't know any test I can do to prove that? Alternating, ratio and root fails. I don't know with what I can compare it to. Thanks in advance for your help!","I have the following series $\displaystyle \sum_{n=1}^{+\infty} \frac{(-1)^{n+1}}{3n + n(-1)^n}$. Does it converge? I wanted to the alternating series test, but that's not easy because of the two $(-1)^n$, plus it's not monotone decreasing. I've split it into two series $n=2n \Rightarrow \sum{-\frac 1 {4m}}\Rightarrow$ diverges, $n=2n+1 \Rightarrow \sum{\frac 1 {2m}}\Rightarrow$ diverges. I can't say diverges + diverges = diverges, but I have this hunch, that the positive series diverges faster than the negative series and therefore it diverges. However, I don't know any test I can do to prove that? Alternating, ratio and root fails. I don't know with what I can compare it to. Thanks in advance for your help!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
1,Proving that the closure of a set contains the $\inf$ and $\sup$,Proving that the closure of a set contains the  and,\inf \sup,"I came across the following problem about closures: If $A$ is a bounded nonempty subset of $\mathbb{R}$, prove that $\sup A \in \overline{A}$ and $\inf A \in \overline{A}$. Proof. By hypothesis, $A$ satisfies the least upper bound property (and the greatest lower bound property). So $\sup A$ and $\inf A$ exist. Now we know that $$\overline{A} = \{x \in \mathbb{R}: a_n \to x \ \text{for some sequence} \ (a_n) \ \text{in} \ A \}$$ Moreover, $x \in \overline{A} \Longleftrightarrow (\forall \epsilon >0) \ \exists a \in A \ni |x-a| < \epsilon$. If $\sup A$ and $\inf A$ are in $A$, then we are done. So suppose they are not. Let $x_1 = \sup A$ and $x_2 = \inf A$. By definition, for every $\epsilon >0$, there exists $x_A \in A$ such that $x_A \leq x_2 +\epsilon$. Likewise, there exists $x_B \in A$ such that $x_1- \epsilon \leq x_B$. So we get $$x_A-x_2 \leq \epsilon$$ and $$x_1-x_B \leq \epsilon$$ It seems like we would be done if we put absolute value signs on those quantities. Because then we can approximate the supremum and infimum as closely as we like with members of $A$. How would I get the absolute value signs on the above two quantities?","I came across the following problem about closures: If $A$ is a bounded nonempty subset of $\mathbb{R}$, prove that $\sup A \in \overline{A}$ and $\inf A \in \overline{A}$. Proof. By hypothesis, $A$ satisfies the least upper bound property (and the greatest lower bound property). So $\sup A$ and $\inf A$ exist. Now we know that $$\overline{A} = \{x \in \mathbb{R}: a_n \to x \ \text{for some sequence} \ (a_n) \ \text{in} \ A \}$$ Moreover, $x \in \overline{A} \Longleftrightarrow (\forall \epsilon >0) \ \exists a \in A \ni |x-a| < \epsilon$. If $\sup A$ and $\inf A$ are in $A$, then we are done. So suppose they are not. Let $x_1 = \sup A$ and $x_2 = \inf A$. By definition, for every $\epsilon >0$, there exists $x_A \in A$ such that $x_A \leq x_2 +\epsilon$. Likewise, there exists $x_B \in A$ such that $x_1- \epsilon \leq x_B$. So we get $$x_A-x_2 \leq \epsilon$$ and $$x_1-x_B \leq \epsilon$$ It seems like we would be done if we put absolute value signs on those quantities. Because then we can approximate the supremum and infimum as closely as we like with members of $A$. How would I get the absolute value signs on the above two quantities?",,['real-analysis']
2,Function fails to be of bounded variation,Function fails to be of bounded variation,,"Let $f$ fail to be of bounded variation on [0,1]. Show that there is a point $x_0$ in [0,1] such that $f$ fails to be of bounded variation on each nondegenerate closed subinterval of [0,1] that contains $x_0$. I'm trying proving this directly, but I think maybe a proof by contradiction could work here. Couldn't we just assume that $f$ is of bounded variation on every closed, bounded subset of [0,1] to begin the proof by contradiction? Perhaps then we could split up the interval [0,1] so that we end up with a situation where $f$ is of bounded variation on a subinterval that contains $x_0$? I'd appreciate some help here.","Let $f$ fail to be of bounded variation on [0,1]. Show that there is a point $x_0$ in [0,1] such that $f$ fails to be of bounded variation on each nondegenerate closed subinterval of [0,1] that contains $x_0$. I'm trying proving this directly, but I think maybe a proof by contradiction could work here. Couldn't we just assume that $f$ is of bounded variation on every closed, bounded subset of [0,1] to begin the proof by contradiction? Perhaps then we could split up the interval [0,1] so that we end up with a situation where $f$ is of bounded variation on a subinterval that contains $x_0$? I'd appreciate some help here.",,['real-analysis']
3,Reverse Taylor Series,Reverse Taylor Series,,"Almost everyone is familiar with the famous Taylor Series: $ f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n $ which, if it converges at more than one point, will converge in some interval about $a$. Has anyone considered the ""Reverse"" Taylor Series: $ g(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x-a)}{n!} a^n $ I call it reverse, because its what you get for symbolically taking $a \rightarrow (x-a)$. You might be saying that there is absolutely no reason to believe that this series should converge to $f(x)$, but there are two big examples where it does. For $e^x$: $g(x) = \sum_{n=0}^\infty \frac{e^{(x-a)}}{n!} a^n = \frac{e^x}{e^a} \sum_{n=0}^\infty \frac{a^n}{n!} = e^x $ For $x^k$: $g(x) = \sum_{n=0}^k \frac{k(k-1)...(k-n+1)(x-a)^{k-n} a^n}{n!} = \sum_{n=0}^k {k \choose n} (x-a)^{k-n} a^n = ((x-a) + a)^k $ I have yet to find a counter-example for when the Reverse Taylor Series does not give back the original function for an analytic function, and I also have yet to think of a way to prove that the Reverse Taylor Series should converge for a given function. Does anyone have any ideas?","Almost everyone is familiar with the famous Taylor Series: $ f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n $ which, if it converges at more than one point, will converge in some interval about $a$. Has anyone considered the ""Reverse"" Taylor Series: $ g(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x-a)}{n!} a^n $ I call it reverse, because its what you get for symbolically taking $a \rightarrow (x-a)$. You might be saying that there is absolutely no reason to believe that this series should converge to $f(x)$, but there are two big examples where it does. For $e^x$: $g(x) = \sum_{n=0}^\infty \frac{e^{(x-a)}}{n!} a^n = \frac{e^x}{e^a} \sum_{n=0}^\infty \frac{a^n}{n!} = e^x $ For $x^k$: $g(x) = \sum_{n=0}^k \frac{k(k-1)...(k-n+1)(x-a)^{k-n} a^n}{n!} = \sum_{n=0}^k {k \choose n} (x-a)^{k-n} a^n = ((x-a) + a)^k $ I have yet to find a counter-example for when the Reverse Taylor Series does not give back the original function for an analytic function, and I also have yet to think of a way to prove that the Reverse Taylor Series should converge for a given function. Does anyone have any ideas?",,"['calculus', 'real-analysis', 'analysis']"
4,Prove that $\mu_*\leq\mu^*$ where the definition of an inner and outer measure is induced by a measure,Prove that  where the definition of an inner and outer measure is induced by a measure,\mu_*\leq\mu^*,"I know that similar question has been asked and answered here , here , and here . But I am looking for a different proof based on different definitions. We have the following definition of an outer and inner measure indeuced by a measure: Definition $\quad$ Let $(X,\mathcal{A})$ be a measurable space, let $\mu$ be a measure on $\mathcal{A}$ , and let $A$ be an arbitrary subset of $X$ . Then $\mu^*(A)$ , the outer measure of $A$ , is defined by \begin{align*}     \mu^*(A) = \inf\{\mu(B):A \subseteq B\ \text{and}\ B \in \mathcal{A}\}, \end{align*} and $\mu_*(A)$ , the inner measure of $A$ , is defined by \begin{align*}     \mu_*(A) = \sup\{\mu(B):B \subseteq A\ \text{and}\ B \in \mathcal{A}\}. \end{align*} I need to prove that $\mu_*(A) \leq \mu^*(A)$ holds for each subset $A$ of $X$ . This is how I would like to proceed and where I got stuck: Assume to the contrary that $\mu_*(A) > \mu^*(A)$ for some $A \in X$ . Then there is a $B \in \mathcal{A}$ such that $A \subseteq B$ and $\mu(B) < \mu_*(A)$ . From here, I wanted to say that there is a set $C \in \mathcal{A}$ such that $C \subseteq A$ and $\mu(C) > \mu(B)$ , which leads to a contradiction because $C \subseteq A \subseteq B$ implies that $\mu(C) \leq \mu(B)$ . However, I do not know how to state rigorously that such a set $C$ exists. Could anyone please help me out? Reference: Measure Theory by Donald Cohn Page 33.","I know that similar question has been asked and answered here , here , and here . But I am looking for a different proof based on different definitions. We have the following definition of an outer and inner measure indeuced by a measure: Definition Let be a measurable space, let be a measure on , and let be an arbitrary subset of . Then , the outer measure of , is defined by and , the inner measure of , is defined by I need to prove that holds for each subset of . This is how I would like to proceed and where I got stuck: Assume to the contrary that for some . Then there is a such that and . From here, I wanted to say that there is a set such that and , which leads to a contradiction because implies that . However, I do not know how to state rigorously that such a set exists. Could anyone please help me out? Reference: Measure Theory by Donald Cohn Page 33.","\quad (X,\mathcal{A}) \mu \mathcal{A} A X \mu^*(A) A \begin{align*}
    \mu^*(A) = \inf\{\mu(B):A \subseteq B\ \text{and}\ B \in \mathcal{A}\},
\end{align*} \mu_*(A) A \begin{align*}
    \mu_*(A) = \sup\{\mu(B):B \subseteq A\ \text{and}\ B \in \mathcal{A}\}.
\end{align*} \mu_*(A) \leq \mu^*(A) A X \mu_*(A) > \mu^*(A) A \in X B \in \mathcal{A} A \subseteq B \mu(B) < \mu_*(A) C \in \mathcal{A} C \subseteq A \mu(C) > \mu(B) C \subseteq A \subseteq B \mu(C) \leq \mu(B) C","['real-analysis', 'analysis', 'measure-theory', 'supremum-and-infimum', 'outer-measure']"
5,$\lim_{n\to\infty} \left(\frac{e}{n}\right)^n \int_0^n |x(x-1)(x-2)\dots(x-n)|dx$,,\lim_{n\to\infty} \left(\frac{e}{n}\right)^n \int_0^n |x(x-1)(x-2)\dots(x-n)|dx,"What is $$ \lim_{n\to\infty} \left(\frac{e}{n}\right)^n  \int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right| \, dx? $$ Context: I was trying to find an asymptotic expression for the total area of the regions enclosed by $y = x(x-1)(x-2)(x-3)\cdots(x-n)$ and the $x$ -axis. Through trial and error, it seems that $$ \int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right|\, dx  \approx 2\left(\frac{n}{e}\right)^n $$ for large $n$ , but I don't know how to prove this.","What is Context: I was trying to find an asymptotic expression for the total area of the regions enclosed by and the -axis. Through trial and error, it seems that for large , but I don't know how to prove this.","
\lim_{n\to\infty} \left(\frac{e}{n}\right)^n 
\int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right| \, dx?
 y = x(x-1)(x-2)(x-3)\cdots(x-n) x 
\int_0^n \left| x(x-1)(x-2) \cdots (x-n) \right|\, dx 
\approx 2\left(\frac{n}{e}\right)^n
 n","['real-analysis', 'calculus', 'integration', 'asymptotics', 'gamma-function']"
6,How does Green's theorem and Stokes' theorem generalize the fundamental theorem of Calculus,How does Green's theorem and Stokes' theorem generalize the fundamental theorem of Calculus,,I've read in few places that Green's theorem $$ \oint_C L dx + M dy = \iint_{D} \left(\frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}\right) dx dy $$ is a generalization of fundamental theorem of calculus. And same with Stokes' theorem. I assume all these can be described in some differential geometry language which I am not so familiar with. Could someone explain how Green and Stokes theorem are generlisation of the FTC? It would be appreciated! thank you!,I've read in few places that Green's theorem is a generalization of fundamental theorem of calculus. And same with Stokes' theorem. I assume all these can be described in some differential geometry language which I am not so familiar with. Could someone explain how Green and Stokes theorem are generlisation of the FTC? It would be appreciated! thank you!,"
\oint_C L dx + M dy = \iint_{D} \left(\frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}\right) dx dy
","['real-analysis', 'calculus', 'multivariable-calculus', 'greens-theorem']"
7,"If $d$ is a metric, for what class of functions $f$ is $f(d)$ also a metric?","If  is a metric, for what class of functions  is  also a metric?",d f f(d),"One example is that if $d$ is a metric, then so is $\frac{d}{1+d}.$ I'm wondering if there are broader generalities than this, that if we can broaden and classify the set of all functions $f$ for which $f(d)$ is also a metric.","One example is that if is a metric, then so is I'm wondering if there are broader generalities than this, that if we can broaden and classify the set of all functions for which is also a metric.",d \frac{d}{1+d}. f f(d),['real-analysis']
8,Does every diffeomorphism between $\mathbb{B}^n$ and $\mathbb{R}^n$ satisfies that $\lim_{\|x\| \to 1} \|f(x)\| = \infty$,Does every diffeomorphism between  and  satisfies that,\mathbb{B}^n \mathbb{R}^n \lim_{\|x\| \to 1} \|f(x)\| = \infty,"In this question: Diffeomorphism: Unit Ball vs. Euclidean Space a number of diffeomorphisms between $\mathbb{B}^n$ and $\mathbb{R}^n$ are presented. They all have in common that the points closer to the border are sent ""further"". This is: $$\lim_{\|x\| \to 1} \|f(x)\| = \infty$$ Intuitively these diffeomorphisms ""extend"" the ball more or less uniformly, as to fill the space. Is there any diffeomorphism that doesn't have this property, so a sucession of points that converge to the border remain at a bounded distance from the origin?","In this question: Diffeomorphism: Unit Ball vs. Euclidean Space a number of diffeomorphisms between and are presented. They all have in common that the points closer to the border are sent ""further"". This is: Intuitively these diffeomorphisms ""extend"" the ball more or less uniformly, as to fill the space. Is there any diffeomorphism that doesn't have this property, so a sucession of points that converge to the border remain at a bounded distance from the origin?",\mathbb{B}^n \mathbb{R}^n \lim_{\|x\| \to 1} \|f(x)\| = \infty,"['real-analysis', 'calculus', 'general-topology', 'geometry', 'differential-geometry']"
9,Example of incomplete metric on $\mathbb{R}$?,Example of incomplete metric on ?,\mathbb{R},"I was wondering if the following is a valid example of an incomplete metric on $\mathbb{R}$ . Let $d(x,y)$ be defined as follows: if $x,y\in\mathbb{Q}$ then $d(x,y) = |x-y|$ ; if $x,y\in\mathbb{R}\setminus\mathbb{Q}$ then $d(x,y) = |x-y|$ ; otherwise (if one is rational and the other is not), let $d(x,y) = |x-y|+1$ . We verify this is a metric. The only concern is triangle inequality. Let $x,y,z\in\mathbb{R}$ , if $x,z\in\mathbb{Q}$ or $x,z\in\mathbb{R}\setminus\mathbb{Q}$ , then we have $$d(x,z)\leq d(x,y) + d(y,z)$$ regardless of if $y\in\mathbb{Q}$ or $y\in\mathbb{R}\setminus \mathbb{Q}$ since either case can only increase the RHS. On the other hand, if $x\in\mathbb{Q}$ and $z\in\mathbb{R}\setminus \mathbb{Q}$ , then the LHS gets 1 added to it but the RHS does as well, so the triangle inequality holds. For incompleteness, consider a sequence of rationals converging to $\pi$ in the Euclidean norm. Since the metric restricted to rationals is simply the Euclidean metric, this is a Cauchy sequence. However, for each $x_n$ in the sequence is rational, it cannot converge to an irrational number as $d(x_n,y)\geq 1$ for all $y\in\mathbb{R}\setminus \mathbb{Q}$ . On the other hand, $x_n$ cannot converge to a rational as well: if it does, then it must converge to that rational (say $q$ ) in the Euclidean norm, which is a contradiction since it cannot converge to both $\pi$ and $q$ in the Euclidean norm.","I was wondering if the following is a valid example of an incomplete metric on . Let be defined as follows: if then ; if then ; otherwise (if one is rational and the other is not), let . We verify this is a metric. The only concern is triangle inequality. Let , if or , then we have regardless of if or since either case can only increase the RHS. On the other hand, if and , then the LHS gets 1 added to it but the RHS does as well, so the triangle inequality holds. For incompleteness, consider a sequence of rationals converging to in the Euclidean norm. Since the metric restricted to rationals is simply the Euclidean metric, this is a Cauchy sequence. However, for each in the sequence is rational, it cannot converge to an irrational number as for all . On the other hand, cannot converge to a rational as well: if it does, then it must converge to that rational (say ) in the Euclidean norm, which is a contradiction since it cannot converge to both and in the Euclidean norm.","\mathbb{R} d(x,y) x,y\in\mathbb{Q} d(x,y) = |x-y| x,y\in\mathbb{R}\setminus\mathbb{Q} d(x,y) = |x-y| d(x,y) = |x-y|+1 x,y,z\in\mathbb{R} x,z\in\mathbb{Q} x,z\in\mathbb{R}\setminus\mathbb{Q} d(x,z)\leq d(x,y) + d(y,z) y\in\mathbb{Q} y\in\mathbb{R}\setminus \mathbb{Q} x\in\mathbb{Q} z\in\mathbb{R}\setminus \mathbb{Q} \pi x_n d(x_n,y)\geq 1 y\in\mathbb{R}\setminus \mathbb{Q} x_n q \pi q","['real-analysis', 'general-topology', 'metric-spaces']"
10,Catalan constant's integral representation,Catalan constant's integral representation,,"Catalan constant is known to have a rich source of integral identities, here is the formula I found: $$ \int_0^\infty \frac{\sin^{-1}(\sin(x))}{x} \,dx \ =2G.$$ This can be proved by analyzing the function and using the identity: $$ \frac{2G}{\pi}-\frac{1}{2}=\ln\left(\prod_{n=1}^\infty \frac{(4n-1)^{4n-1}}{(4n-3)^{2n-1}(4n+1)^{2n}}\right).$$ Is there any ""neat"" way to prove the formula directly without using the identity? It's really hard for me to prove the identity. Update I found my own way to prove this integral indentity: Using Fourier series of triangle wave, with a little change in the coefficient, I got this equation: $$\sin^{-1}(\sin(x))=\frac{4}{\pi}\sum_{n=0}^\infty (-1)^{n}\frac{\sin((2n+1)x)}{(2n+1)^{2}}$$ Divide both sides by $x$ and integrate: $$\int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=\frac{4}{\pi}\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}\int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx$$ Using the fact that $\int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx=\int_{0}^\infty \frac{\sin(x)}{x}dx=\frac{\pi}{2}$ , we got: $$\int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=2\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}=2G$$","Catalan constant is known to have a rich source of integral identities, here is the formula I found: This can be proved by analyzing the function and using the identity: Is there any ""neat"" way to prove the formula directly without using the identity? It's really hard for me to prove the identity. Update I found my own way to prove this integral indentity: Using Fourier series of triangle wave, with a little change in the coefficient, I got this equation: Divide both sides by and integrate: Using the fact that , we got:"," \int_0^\infty \frac{\sin^{-1}(\sin(x))}{x} \,dx \ =2G.  \frac{2G}{\pi}-\frac{1}{2}=\ln\left(\prod_{n=1}^\infty \frac{(4n-1)^{4n-1}}{(4n-3)^{2n-1}(4n+1)^{2n}}\right). \sin^{-1}(\sin(x))=\frac{4}{\pi}\sum_{n=0}^\infty (-1)^{n}\frac{\sin((2n+1)x)}{(2n+1)^{2}} x \int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=\frac{4}{\pi}\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}\int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx \int_{0}^\infty \frac{\sin((2n+1)x)}{x}dx=\int_{0}^\infty \frac{\sin(x)}{x}dx=\frac{\pi}{2} \int_{0}^\infty \frac{\sin^{-1}(\sin(x))}{x}dx=2\sum_{n=0}^\infty \frac{(-1)^{n}}{(2n+1)^{2}}=2G","['real-analysis', 'integration', 'catalans-constant']"
11,Formalizing composition of sequences,Formalizing composition of sequences,,"The following first equality is intuitively evident: $$ \lim_{n\to \infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n  = \lim_{n\to \infty} \left(1+\frac{x+y}{n}\right)^n =e^{x+y} $$ how can I give a formal argument ? Informally it is something like $$ \lim_{n\to \infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n  =\lim_{n\to \infty} \left(1+\frac{\lim_{n\to \infty}\left(x+y+\frac{xy}{n}\right)}{n}\right)^n  =\lim_{n\to \infty} \left(1+\frac{x+y}{n}\right)^n  $$ Is there a "" classical "" property of sequences that backs this argument? Something like limit of composition, but here we speak of sequence only: $$ (b_n),\quad b_n \to b\quad\text{ for }n\to\infty \implies  \lim_{n\to \infty} \left(1+\frac{b_n}{n}\right)^n =\lim_{n\to \infty} \left(1+\frac{b}{n}\right)^n $$ EDIT As Qiaochu Yuan is important to define what I have already defined. This computation is at the beginning of ""Real Analysis"", lets say in the chapter of sequences. Later, in the notes of complex analysis, I develop formal series with derivation and everything is the standard way (I think). By this question I tried another approach, very early (1) define $$ e^x = \lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n $$ (2) derive the product as depicted in this question (3) derive the derivation of the $e^x$ . Thus I have (a) no logarithm, (b) no derivation of $e^x$ yet.","The following first equality is intuitively evident: how can I give a formal argument ? Informally it is something like Is there a "" classical "" property of sequences that backs this argument? Something like limit of composition, but here we speak of sequence only: EDIT As Qiaochu Yuan is important to define what I have already defined. This computation is at the beginning of ""Real Analysis"", lets say in the chapter of sequences. Later, in the notes of complex analysis, I develop formal series with derivation and everything is the standard way (I think). By this question I tried another approach, very early (1) define (2) derive the product as depicted in this question (3) derive the derivation of the . Thus I have (a) no logarithm, (b) no derivation of yet.","
\lim_{n\to \infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n 
= \lim_{n\to \infty} \left(1+\frac{x+y}{n}\right)^n
=e^{x+y}
 
\lim_{n\to \infty} \left(1+\frac{x+y+\frac{xy}{n}}{n}\right)^n 
=\lim_{n\to \infty} \left(1+\frac{\lim_{n\to \infty}\left(x+y+\frac{xy}{n}\right)}{n}\right)^n 
=\lim_{n\to \infty} \left(1+\frac{x+y}{n}\right)^n 
 
(b_n),\quad b_n \to b\quad\text{ for }n\to\infty \implies 
\lim_{n\to \infty} \left(1+\frac{b_n}{n}\right)^n
=\lim_{n\to \infty} \left(1+\frac{b}{n}\right)^n
 
e^x = \lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n
 e^x e^x","['real-analysis', 'sequences-and-series', 'exponential-function', 'limits-without-lhopital']"
12,Does $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ converge?,Does  converge?,\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n,"Let's consider the series: $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ . I would suggest that it doesn't converge. One can see it as follows: Let be $S_{2n}:=\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n}$ and $S_{2n+1}:=\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1}$ two subsequences (subseries?) of $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ . Both converge due to alternating series test (Leibniz criterion). However, if I take a look at $|S_{2n}-S_{2n+1}|$ , I notice: $$|S_{2n}-S_{2n+1}|=\Big|\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1}\Big|\\=\Big|2\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\frac{1}{2n+1}\Big|=2\sum\limits_{k=1}^{2n}\frac{(-1)^{k-1}}{k} +\frac{1}{2n+1}>1. $$ Hence, $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ violates the Cauchy-criterion, or in other words I can always find two partial sums which are not arbitrarily close to each other. Is this correct? Is there a faster or more elegant way two show this result?","Let's consider the series: . I would suggest that it doesn't converge. One can see it as follows: Let be and two subsequences (subseries?) of . Both converge due to alternating series test (Leibniz criterion). However, if I take a look at , I notice: Hence, violates the Cauchy-criterion, or in other words I can always find two partial sums which are not arbitrarily close to each other. Is this correct? Is there a faster or more elegant way two show this result?","\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n S_{2n}:=\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} S_{2n+1}:=\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1} \sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n |S_{2n}-S_{2n+1}| |S_{2n}-S_{2n+1}|=\Big|\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1}\Big|\\=\Big|2\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\frac{1}{2n+1}\Big|=2\sum\limits_{k=1}^{2n}\frac{(-1)^{k-1}}{k} +\frac{1}{2n+1}>1.
 \sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'solution-verification']"
13,Covering of a compact set,Covering of a compact set,,"Let $K$ be a compact subset of $\mathbb{R}^n$ . Fix a constant $r>0$ , I'm wondering whether there exists a finite collection of points $x_1,\dots,x_k \in K$ such that the collection of open balls $\{B(x_i,2r)\}_{i=1}^{k}$ forms an open cover of $K$ while $B(x_i,r)$ are mutually disjoint. I am looking for some variations of covering lemmas in $\mathbb{R}^n$ but failed to find any. Any insight or familiarity with would be appreciated.","Let be a compact subset of . Fix a constant , I'm wondering whether there exists a finite collection of points such that the collection of open balls forms an open cover of while are mutually disjoint. I am looking for some variations of covering lemmas in but failed to find any. Any insight or familiarity with would be appreciated.","K \mathbb{R}^n r>0 x_1,\dots,x_k \in K \{B(x_i,2r)\}_{i=1}^{k} K B(x_i,r) \mathbb{R}^n",['real-analysis']
14,How to show $\frac{d}{d x}\left(|x|^{1/2}\right)=\frac{x}{2|x|^{3/2}}$?,How to show ?,\frac{d}{d x}\left(|x|^{1/2}\right)=\frac{x}{2|x|^{3/2}},"$$\frac{d}{d x}\left(|x|^{\frac{1}{2}}\right)=\frac{x}{2|x|^{\frac{3}{2}}}$$ and also the second derivative $$\frac{d^{2}}{d x^{2}}\left(\sqrt{|x|}\right)=\frac{\delta(x)}{\sqrt{|x|}}-\frac{x^{2}}{4|x|^{\frac{7}{2}}}$$ I know this may seem basic. But I don't get how the Dirac delta (thanks for the comments for correcting me) comes in. Is it just a way someone has decided to write the result for $x >0$ , $x<0$ and $x=0$ cleverly in one line. Is it arbitrary as long as the Kronecker term beats the other term? If anyone has a clean derivation for these I'd be grateful. EDIT: thanks for a lot of your answers, I've learnt a lot. Still not entirely sure with the second derivative, the formula is here: https://www.wolframalpha.com/input/?i=second+derivative+of+%7Cx%7C%5E1%2F2 , is this wrong? I still think it is probably correct. From taking something like. d/dx( $\operatorname{sign}(x) \frac{1}{2|x|^{\frac{1}{2}}} )=1/2 ($$\frac{\delta(x)}{\sqrt{|x|}} -\frac{sign(x)*x}{2|x|^{5 / 2}})$ Don't know why first summand is missin 1/2. I know its distributions, but can't you still differentiate sign(x), you just have to be careful and when evaluating use test functions and integrals","and also the second derivative I know this may seem basic. But I don't get how the Dirac delta (thanks for the comments for correcting me) comes in. Is it just a way someone has decided to write the result for , and cleverly in one line. Is it arbitrary as long as the Kronecker term beats the other term? If anyone has a clean derivation for these I'd be grateful. EDIT: thanks for a lot of your answers, I've learnt a lot. Still not entirely sure with the second derivative, the formula is here: https://www.wolframalpha.com/input/?i=second+derivative+of+%7Cx%7C%5E1%2F2 , is this wrong? I still think it is probably correct. From taking something like. d/dx( Don't know why first summand is missin 1/2. I know its distributions, but can't you still differentiate sign(x), you just have to be careful and when evaluating use test functions and integrals",\frac{d}{d x}\left(|x|^{\frac{1}{2}}\right)=\frac{x}{2|x|^{\frac{3}{2}}} \frac{d^{2}}{d x^{2}}\left(\sqrt{|x|}\right)=\frac{\delta(x)}{\sqrt{|x|}}-\frac{x^{2}}{4|x|^{\frac{7}{2}}} x >0 x<0 x=0 \operatorname{sign}(x) \frac{1}{2|x|^{\frac{1}{2}}} )=1/2 (\frac{\delta(x)}{\sqrt{|x|}} -\frac{sign(x)*x}{2|x|^{5 / 2}}),"['real-analysis', 'calculus', 'analysis']"
15,Range of the map $(a_n)\to \prod_{n=1}^\infty(1+a_n).$,Range of the map,(a_n)\to \prod_{n=1}^\infty(1+a_n).,Let $A$ be the set of nonnegative sequences $(a_n)$ such that $\sum_{n=1}^{\infty}a_n=1.$ Find the range of the map $P:A\to \mathbb R$ defined by $$P((a_n))= \prod_{n=1}^\infty(1+a_n).$$,Let be the set of nonnegative sequences such that Find the range of the map defined by,A (a_n) \sum_{n=1}^{\infty}a_n=1. P:A\to \mathbb R P((a_n))= \prod_{n=1}^\infty(1+a_n).,"['real-analysis', 'infinite-product']"
16,Why is compactness required in Arzela Ascoli?,Why is compactness required in Arzela Ascoli?,,"My question comes from the necessity of proving that one of the hypothesis in the Arzela Ascoli theorem fails in the example: $f_n(x)=\frac{1}{1+(x-n)^2}; x\in [0,\infty)$ I have tried proving it is not uniformly equicontinuous nor uniformly bounded but keep failing, so my only assumption is that Arzela Ascoli cannot be applied because $[0,\infty)$ is closed but not bounded, so it is not compact in $\mathbb{R}$ . Yet, I cannot think of a way to show the theorem fails in this example.","My question comes from the necessity of proving that one of the hypothesis in the Arzela Ascoli theorem fails in the example: I have tried proving it is not uniformly equicontinuous nor uniformly bounded but keep failing, so my only assumption is that Arzela Ascoli cannot be applied because is closed but not bounded, so it is not compact in . Yet, I cannot think of a way to show the theorem fails in this example.","f_n(x)=\frac{1}{1+(x-n)^2}; x\in [0,\infty) [0,\infty) \mathbb{R}","['real-analysis', 'general-topology', 'functional-analysis', 'compactness', 'arzela-ascoli']"
17,Find all roots of the equation :$(1+\frac{ix}n)^n = (1-\frac{ix}n)^n$,Find all roots of the equation :,(1+\frac{ix}n)^n = (1-\frac{ix}n)^n,"This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. If $n$ is a positive integer, find all roots of the equation : $$(1+\frac{ix}n)^n = (1-\frac{ix}n)^n$$ The binomial expansion on each side will lead to: $$(n.1^n+C(n, 1).1^{n-1}.\frac{ix}n + C(n, 2).1^{n-2}.(\frac{ix}n)^2 + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (n.1^n+C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 2).1^{n-2}.(\frac{-ix}n)^2 + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots )$$ $n$ can be odd or even, but the terms on l.h.s. & r.h.s. cancel for even $n$ as power of $\frac{ix}n$ . Anyway, the first terms cancel each other. $$(C(n, 1).1^{n-1}.\frac{ix}n + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots )$$ As the term $(1)^{n-i}$ for $i \in \{1,2,\cdots\}$ don't matter in products terms, so ignore them: $$(C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = (C(n, 1).\frac{-ix}n + C(n, 3).(\frac{-ix}n)^3+\cdots )$$ $$2(C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = 0$$ $$C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots  = 0$$ Unable to pursue further.","This question is taken from book: Advanced Calculus: An Introduction to Classical Analysis, by Louis Brand. The book is concerned with introductory real analysis. I request to help find the solution. If is a positive integer, find all roots of the equation : The binomial expansion on each side will lead to: can be odd or even, but the terms on l.h.s. & r.h.s. cancel for even as power of . Anyway, the first terms cancel each other. As the term for don't matter in products terms, so ignore them: Unable to pursue further.","n (1+\frac{ix}n)^n = (1-\frac{ix}n)^n (n.1^n+C(n, 1).1^{n-1}.\frac{ix}n + C(n, 2).1^{n-2}.(\frac{ix}n)^2 + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (n.1^n+C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 2).1^{n-2}.(\frac{-ix}n)^2 + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots ) n n \frac{ix}n (C(n, 1).1^{n-1}.\frac{ix}n + C(n, 3).1^{n-3}.(\frac{ix}n)^3+\cdots ) = (C(n, 1).1^{n-1}.\frac{-ix}n + C(n, 3).1^{n-3}.(\frac{-ix}n)^3+\cdots ) (1)^{n-i} i \in \{1,2,\cdots\} (C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = (C(n, 1).\frac{-ix}n + C(n, 3).(\frac{-ix}n)^3+\cdots ) 2(C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots ) = 0 C(n, 1).\frac{ix}n + C(n, 3).(\frac{ix}n)^3+\cdots  = 0",['real-analysis']
18,Having trouble showing a function is continuous: $x/(1+|x|)$,Having trouble showing a function is continuous:,x/(1+|x|),"I am having trouble showing that a function is continuous using the $\epsilon, \delta$ definition. My function $f:\mathbb{R}\rightarrow(-1,1)$ is defined as $f(x) = \frac{x}{1+|x|}$ . Looking at the graph, the function is clearly continuous, but when I expand I don't get anywhere: $ |f(x) - f(x_0)| = |\frac{x}{1+|x|} - \frac{x_0}{1+|x_0|}|  = |\frac{x(1+|x_0|) - x_0(1+|x|) }{(1+|x|)(1+|x_0|)}|$ This does not seem to simplify much at all, I cannot find a $\delta$ that satisfies: $\forall \epsilon > 0, \exists \delta > 0, \forall x,y, \in \mathbb{R}, |x-x_0| < \delta \Rightarrow |f(x) - f(x_0)| < \epsilon $ Am I missing something here? Any advice is greatly appreciated.","I am having trouble showing that a function is continuous using the definition. My function is defined as . Looking at the graph, the function is clearly continuous, but when I expand I don't get anywhere: This does not seem to simplify much at all, I cannot find a that satisfies: Am I missing something here? Any advice is greatly appreciated.","\epsilon, \delta f:\mathbb{R}\rightarrow(-1,1) f(x) = \frac{x}{1+|x|}  |f(x) - f(x_0)| = |\frac{x}{1+|x|} - \frac{x_0}{1+|x_0|}| 
= |\frac{x(1+|x_0|) - x_0(1+|x|) }{(1+|x|)(1+|x_0|)}| \delta \forall \epsilon > 0, \exists \delta > 0, \forall x,y, \in \mathbb{R}, |x-x_0| < \delta \Rightarrow |f(x) - f(x_0)| < \epsilon ","['real-analysis', 'analysis', 'continuity', 'epsilon-delta']"
19,Radius of convergence of $\sum_{k=1}^{\infty} {{k^{\sqrt k}}x^{k}}$,Radius of convergence of,\sum_{k=1}^{\infty} {{k^{\sqrt k}}x^{k}},"I'm currently computing sums to find the interval/radius of convergence. But recently I'm stuck with this sum: $\sum_{k=1}^{\infty} {{k^{\sqrt k}}x^{k}}$. I tried applying the ratio test $\lim_{k \to \infty}|\frac{a_{k+1}}{a_k}|$, thus:   $\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}x^{k+1}}{{k^{\sqrt k}}x^{k}}|$. Now we can write this as: $\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}x}{{k^{\sqrt k}}}|$. I guess you could now write this as: $|x|\cdot\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}}{{k^{\sqrt k}}}|$. Here I'm stuck since I'm not sure how to further simplify or solve the limit. I would be very glad if someone could show me how to solve the last part.","I'm currently computing sums to find the interval/radius of convergence. But recently I'm stuck with this sum: $\sum_{k=1}^{\infty} {{k^{\sqrt k}}x^{k}}$. I tried applying the ratio test $\lim_{k \to \infty}|\frac{a_{k+1}}{a_k}|$, thus:   $\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}x^{k+1}}{{k^{\sqrt k}}x^{k}}|$. Now we can write this as: $\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}x}{{k^{\sqrt k}}}|$. I guess you could now write this as: $|x|\cdot\lim_{k \to \infty}|\frac{{(k+1)^{\sqrt {k+1}}}}{{k^{\sqrt k}}}|$. Here I'm stuck since I'm not sure how to further simplify or solve the limit. I would be very glad if someone could show me how to solve the last part.",,['real-analysis']
20,What is the value of $f(100)$?,What is the value of ?,f(100),"We have $f:\Bbb R\to \Bbb R^*$ , a function that admits primitives and admits the relations $$\cos \left(f(x)\right)=1,\ ∀x\in \Bbb R, \quad\text{and}\quad|f(\pi )−\pi |≤\pi .$$ What  is the value of $f(100)$ ? My thought. We obviously have $$\cos (f(100)) =1\overset{?}{\implies} f(100) =\arccos (1),$$ but this seems not to make any sense at all. How can I use the provided inequality $|f(\pi)−\pi|≤\pi$ ?","We have , a function that admits primitives and admits the relations What  is the value of ? My thought. We obviously have but this seems not to make any sense at all. How can I use the provided inequality ?","f:\Bbb R\to \Bbb R^* \cos \left(f(x)\right)=1,\ ∀x\in \Bbb R, \quad\text{and}\quad|f(\pi )−\pi |≤\pi . f(100) \cos (f(100)) =1\overset{?}{\implies} f(100) =\arccos (1), |f(\pi)−\pi|≤\pi","['calculus', 'real-analysis', 'functions', 'contest-math', 'functional-equations']"
21,"The IVP $\begin{cases}\dot{x}=x^3+e^{-t^2}\\x(0)=1\end{cases}$ possesses a solution in $I=(-1/9,1/9)$",The IVP  possesses a solution in,"\begin{cases}\dot{x}=x^3+e^{-t^2}\\x(0)=1\end{cases} I=(-1/9,1/9)","I'm stuck with this problem: Prove that the IVP $$\begin{cases}\dot{x}=x^3+e^{-t^2}\\x(0)=1\end{cases}$$ has an unique solution defined on $I=(-1/9,1/9)$ . Which is the largest interval of definition of the solution? We can extend that solution to $t=1$ ? Okay, so it's obvious that $f(t,x)=x^3+e^{-t^2}$ is locally Lipschitz with respect to $x$ , and it's continuous too. So by Picard we can find a unique solution defined on $\mathbb{R}$ . But then the exercise wouldn't have any sense, so I think that I'm doing something wrong. Thanks for your time.","I'm stuck with this problem: Prove that the IVP has an unique solution defined on . Which is the largest interval of definition of the solution? We can extend that solution to ? Okay, so it's obvious that is locally Lipschitz with respect to , and it's continuous too. So by Picard we can find a unique solution defined on . But then the exercise wouldn't have any sense, so I think that I'm doing something wrong. Thanks for your time.","\begin{cases}\dot{x}=x^3+e^{-t^2}\\x(0)=1\end{cases} I=(-1/9,1/9) t=1 f(t,x)=x^3+e^{-t^2} x \mathbb{R}","['calculus', 'real-analysis', 'ordinary-differential-equations', 'derivatives', 'initial-value-problems']"
22,Calculating limit $\lim\limits_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)}$ for an unknown function.,Calculating limit  for an unknown function.,\lim\limits_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)},"Given that $f(x)$ is a continuous function and satisfies $f'(x)>0$ on $(-\infty,\infty)$ and $f''(x)=2 \forall x \in(0,\infty)$. We need to find the limit $$\lim_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)}$$ Now the numerator is tending to infinity so denominator must also go to infinity else limit won't exist.So I tried the L'Hospitals rule and it became$$\lim_{x\to\infty}\frac{6x+\frac{6x}{(x^2+1)^2}-4f''(x)}{f'(x)}$$The numerator is still infinity so once again applying L'Hospitals rule (assuming denominator must still be infinity) we get $$\lim_{x\to\infty}\frac{6+\frac{6(x^2+1)^2-6x×2(x^2+1)×2x}{(x^2+1)^4}+0}{f''(x)}$$ Now putting $f''(x)=2$ we get $$3+\lim_{x\to\infty}\frac{3(x^2+1)^2-12x^2(x^2+1)^2}{(x^2+1)^4}$$ Collecting the coefficients of $x^4$ from numerator and denominator we get the limit to be$3-9=-6$ but the answer is not -6. Is applying LHospital wrong?Help. Thanks.","Given that $f(x)$ is a continuous function and satisfies $f'(x)>0$ on $(-\infty,\infty)$ and $f''(x)=2 \forall x \in(0,\infty)$. We need to find the limit $$\lim_{x\to\infty}\frac{3x^2-\frac{3}{x^2+1}-4f'(x)}{f(x)}$$ Now the numerator is tending to infinity so denominator must also go to infinity else limit won't exist.So I tried the L'Hospitals rule and it became$$\lim_{x\to\infty}\frac{6x+\frac{6x}{(x^2+1)^2}-4f''(x)}{f'(x)}$$The numerator is still infinity so once again applying L'Hospitals rule (assuming denominator must still be infinity) we get $$\lim_{x\to\infty}\frac{6+\frac{6(x^2+1)^2-6x×2(x^2+1)×2x}{(x^2+1)^4}+0}{f''(x)}$$ Now putting $f''(x)=2$ we get $$3+\lim_{x\to\infty}\frac{3(x^2+1)^2-12x^2(x^2+1)^2}{(x^2+1)^4}$$ Collecting the coefficients of $x^4$ from numerator and denominator we get the limit to be$3-9=-6$ but the answer is not -6. Is applying LHospital wrong?Help. Thanks.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'functions']"
23,Construction of the real numbers using Dedekind cuts,Construction of the real numbers using Dedekind cuts,,I recently read that a real number $r$ is any subset of the set $\mathbb{Q}$ of rational numbers such that it satisfies the following. 1) $r$ is nonempty 2) $r\neq\mathbb{Q}$ 3) $r$ is closed downwards 4) $r$ contains no greatest element My question is how can a single real number $r$ be a set of rational numbers?,I recently read that a real number $r$ is any subset of the set $\mathbb{Q}$ of rational numbers such that it satisfies the following. 1) $r$ is nonempty 2) $r\neq\mathbb{Q}$ 3) $r$ is closed downwards 4) $r$ contains no greatest element My question is how can a single real number $r$ be a set of rational numbers?,,"['real-analysis', 'real-numbers']"
24,How to prove that $\exp(x_1+x_2)=\exp(x_1)\exp(x_2)$?,How to prove that ?,\exp(x_1+x_2)=\exp(x_1)\exp(x_2),"In my analysis class, we covered the properties of the exponential function (as of now we use $\exp$ instead of $e$). One of the properties of $e$ is that $\exp(x_1+x_2)=\exp(x_1)\exp(x_2)$. In high school this was just assumed knowledge but now we have to prove that this statement is indeed true, which is proving to be quite difficult. I assume that one would need to work out the derivatives of both side and see if they are equal. Then if $f(0)=1$ we can assume that both sides of the equation are true.","In my analysis class, we covered the properties of the exponential function (as of now we use $\exp$ instead of $e$). One of the properties of $e$ is that $\exp(x_1+x_2)=\exp(x_1)\exp(x_2)$. In high school this was just assumed knowledge but now we have to prove that this statement is indeed true, which is proving to be quite difficult. I assume that one would need to work out the derivatives of both side and see if they are equal. Then if $f(0)=1$ we can assume that both sides of the equation are true.",,"['real-analysis', 'exponential-function', 'proof-explanation']"
25,How to show distance between boundary of an open set and a compact subset is positive?,How to show distance between boundary of an open set and a compact subset is positive?,,"Let $n \in \mathbb N$ and $U\subset \mathbb R^n$ be an open set.Let $K\subset U$ be a compact subset of $U$. Then How can we show that distance between $K$ and $\partial U$ is positive? Please, someone, give some hints. to solve this. Thank you.","Let $n \in \mathbb N$ and $U\subset \mathbb R^n$ be an open set.Let $K\subset U$ be a compact subset of $U$. Then How can we show that distance between $K$ and $\partial U$ is positive? Please, someone, give some hints. to solve this. Thank you.",,"['calculus', 'real-analysis']"
26,Wrong application of L'Hôpital's rule?,Wrong application of L'Hôpital's rule?,,"I was trying to find the right oblique asymptote of the following function: $$ g(x)= \frac{x^2+(x+2)\cosh(x)}{\sinh(x)}=\frac{x^2}{\sinh(x)}+(x+2)\coth (x)$$ Now since $\frac{x^2}{\sinh(x)}\to 0$ and $\coth(x)\to 1$ as $x\to \infty$, it is easy to see that this asymptote is $y=x+2$. However, when I try to find this asymptote using L'Hôpitals rule, I get a different result:  $$\begin{align} \lim_{x\to \infty}g(x) & =\lim_{x\to \infty} \frac{x^2+(x+2)\cosh(x)}{\sinh(x)} \\ & \stackrel{LH}{=}\lim_{x\to \infty}\frac{2x+\cosh(x)+(x+2)\sinh (x)}{\cosh(x)} \\ & =\lim_{x\to \infty} \frac{2x}{\cosh(x)}+1+(x+2)\tanh(x) \\ & =\lim_{x\to \infty} x+3 \end{align}$$ since $\frac{2x}{\cosh(x)}\to 0$ and $\tanh(x)\to 1$ as $x\to \infty$. This suggests that the asymptote is $y=x+3$ instead of $y=x+2$. A quick look at the function using WolframAlpha shows that $y=x+2$ is indeed the correct asymptote, so I highly suspect that I somehow applied L'Hôpitals rule in a wrong way. I have however no clue as to what I did wrong. Could anyone enlighten me?","I was trying to find the right oblique asymptote of the following function: $$ g(x)= \frac{x^2+(x+2)\cosh(x)}{\sinh(x)}=\frac{x^2}{\sinh(x)}+(x+2)\coth (x)$$ Now since $\frac{x^2}{\sinh(x)}\to 0$ and $\coth(x)\to 1$ as $x\to \infty$, it is easy to see that this asymptote is $y=x+2$. However, when I try to find this asymptote using L'Hôpitals rule, I get a different result:  $$\begin{align} \lim_{x\to \infty}g(x) & =\lim_{x\to \infty} \frac{x^2+(x+2)\cosh(x)}{\sinh(x)} \\ & \stackrel{LH}{=}\lim_{x\to \infty}\frac{2x+\cosh(x)+(x+2)\sinh (x)}{\cosh(x)} \\ & =\lim_{x\to \infty} \frac{2x}{\cosh(x)}+1+(x+2)\tanh(x) \\ & =\lim_{x\to \infty} x+3 \end{align}$$ since $\frac{2x}{\cosh(x)}\to 0$ and $\tanh(x)\to 1$ as $x\to \infty$. This suggests that the asymptote is $y=x+3$ instead of $y=x+2$. A quick look at the function using WolframAlpha shows that $y=x+2$ is indeed the correct asymptote, so I highly suspect that I somehow applied L'Hôpitals rule in a wrong way. I have however no clue as to what I did wrong. Could anyone enlighten me?",,"['calculus', 'real-analysis', 'limits', 'asymptotics']"
27,Why would you want to change the order of integration on a double integral?,Why would you want to change the order of integration on a double integral?,,I know how to do it of course I am just wondering why it is done. Are some problems unsolvable without doing that or is it just to make it faster to solve?,I know how to do it of course I am just wondering why it is done. Are some problems unsolvable without doing that or is it just to make it faster to solve?,,"['real-analysis', 'multivariable-calculus', 'multiple-integral']"
28,Convergence of the integral $\int_0^\infty \frac{\sin^2x}{x^2}~\mathrm dx.$,Convergence of the integral,\int_0^\infty \frac{\sin^2x}{x^2}~\mathrm dx.,"Determine whether the integral $$\int_0^\infty \frac{\sin^2x}{x^2}~\mathrm dx$$ converges. I know it converges, since in general we can use complex analysis, but I'd like to know if there is a simpler method that doesn't involve complex numbers. But I cannot come up with a function that I could compare the integral with.","Determine whether the integral $$\int_0^\infty \frac{\sin^2x}{x^2}~\mathrm dx$$ converges. I know it converges, since in general we can use complex analysis, but I'd like to know if there is a simpler method that doesn't involve complex numbers. But I cannot come up with a function that I could compare the integral with.",,"['calculus', 'real-analysis', 'integration', 'convergence-divergence', 'improper-integrals']"
29,Let $f:\Bbb R\rightarrow\Bbb R$ be such that $|f(x)-f(y)|\le M|x-y|^\alpha$. Prove that $f$ is constant.,Let  be such that . Prove that  is constant.,f:\Bbb R\rightarrow\Bbb R |f(x)-f(y)|\le M|x-y|^\alpha f,"Problem Let $f:\Bbb R\rightarrow\Bbb R$ be such that $|f(x)-f(y)|\le M|x-y|^\alpha$ for some $M>0, \alpha>1$ and for all $x,y\in \Bbb R$. Prove that $f$ is constant. Attempt If $f$ is not constant, then there exists $x,y$ such that $x\neq y\Rightarrow f(x)\neq f(y)$. Then, $0<|f(x)-f(y)|$. Between any two distinct real numbers, we can always find a real number. Let's pick a real number between $0$ and $|f(x)-f(y)|$, and call that number $c$.  Then, $0<c<|f(x)-f(y)|$. Now let's look at $|x-y|$, which is also greater than $0$. For our $c$, no matter what positive number we have picked, and no matter how large $|x-y|$ is, it is possible to pick $M>0$ arbitrarily close to $0$ and $\alpha>1$ arbitrarily close to $1$ so that \begin{align*}\sqrt[\alpha] {c\over M}>|x-y|&\Rightarrow M|x-y|^\alpha <c<|f(x)-f(y)|\\&\Rightarrow |f(x)-f(y)|>M|x-y|^\alpha\end{align*} But, the problem states that $|f(x)-f(y)|\le M|x-y|^\alpha$, so the only way to meet this condition is if $f(x)=f(y)$ regardless of our choice of $x,y$. If $f(x)=f(y)$ regardless of our choice of $x,y$, then this means that $f$ is constant. End of Attempt To be honest, I don't have a clue if I'm even doing anything correctly... I'd appreciate your help.","Problem Let $f:\Bbb R\rightarrow\Bbb R$ be such that $|f(x)-f(y)|\le M|x-y|^\alpha$ for some $M>0, \alpha>1$ and for all $x,y\in \Bbb R$. Prove that $f$ is constant. Attempt If $f$ is not constant, then there exists $x,y$ such that $x\neq y\Rightarrow f(x)\neq f(y)$. Then, $0<|f(x)-f(y)|$. Between any two distinct real numbers, we can always find a real number. Let's pick a real number between $0$ and $|f(x)-f(y)|$, and call that number $c$.  Then, $0<c<|f(x)-f(y)|$. Now let's look at $|x-y|$, which is also greater than $0$. For our $c$, no matter what positive number we have picked, and no matter how large $|x-y|$ is, it is possible to pick $M>0$ arbitrarily close to $0$ and $\alpha>1$ arbitrarily close to $1$ so that \begin{align*}\sqrt[\alpha] {c\over M}>|x-y|&\Rightarrow M|x-y|^\alpha <c<|f(x)-f(y)|\\&\Rightarrow |f(x)-f(y)|>M|x-y|^\alpha\end{align*} But, the problem states that $|f(x)-f(y)|\le M|x-y|^\alpha$, so the only way to meet this condition is if $f(x)=f(y)$ regardless of our choice of $x,y$. If $f(x)=f(y)$ regardless of our choice of $x,y$, then this means that $f$ is constant. End of Attempt To be honest, I don't have a clue if I'm even doing anything correctly... I'd appreciate your help.",,"['real-analysis', 'proof-verification']"
30,Convergent sequence with odd terms decreasing and even terms increasing,Convergent sequence with odd terms decreasing and even terms increasing,,"Let $\left(a_n\right)$ is convergent sequence. $a_0=0, a_1=1,a_2,a_3,...$ Odd terms decrease and even terms increase and for all $n\ge1$    $$2\le \frac{a_n-a_{n-1}}{a_n-a_{n+1}}\le3.$$   Find the boundaries in which there can be a limit of this sequence. My work so far: I proved that $$\frac{11}{24}\le\lim_{n\to\infty}a_n\le\frac{23}{24}$$ But I can not say that this is the final answer. I need an examples of sequences such that: 1) $$\lim_{n\rightarrow\infty}a_n=\frac{11}{24}$$ 2) $$\lim_{n\rightarrow\infty}a_n=\frac{23}{24}$$ or 3) (If my answer can be improved) I need numbers $m$ and $M$, where $$\frac{11}{24}<m\le\lim_{n\rightarrow\infty}a_n\le M<\frac{23}{24}$$","Let $\left(a_n\right)$ is convergent sequence. $a_0=0, a_1=1,a_2,a_3,...$ Odd terms decrease and even terms increase and for all $n\ge1$    $$2\le \frac{a_n-a_{n-1}}{a_n-a_{n+1}}\le3.$$   Find the boundaries in which there can be a limit of this sequence. My work so far: I proved that $$\frac{11}{24}\le\lim_{n\to\infty}a_n\le\frac{23}{24}$$ But I can not say that this is the final answer. I need an examples of sequences such that: 1) $$\lim_{n\rightarrow\infty}a_n=\frac{11}{24}$$ 2) $$\lim_{n\rightarrow\infty}a_n=\frac{23}{24}$$ or 3) (If my answer can be improved) I need numbers $m$ and $M$, where $$\frac{11}{24}<m\le\lim_{n\rightarrow\infty}a_n\le M<\frac{23}{24}$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
31,Find maximum value of $\int_{0}^{1}\left(f(x)\right)^3dx$,Find maximum value of,\int_{0}^{1}\left(f(x)\right)^3dx,Find maximum possible value of $\int_{0}^{1}\left(f(x)\right)^3dx$ given that $-1\leq f(x)\leq 1$ and that $\int_{0}^{1}f(x)dx=0$. My attempt: I tried to guess such functions which could satisfy all the above conditions but could not arrive at any conclusion.,Find maximum possible value of $\int_{0}^{1}\left(f(x)\right)^3dx$ given that $-1\leq f(x)\leq 1$ and that $\int_{0}^{1}f(x)dx=0$. My attempt: I tried to guess such functions which could satisfy all the above conditions but could not arrive at any conclusion.,,"['real-analysis', 'definite-integrals']"
32,Is every Riemann integrable function a uniform limit of step functions?,Is every Riemann integrable function a uniform limit of step functions?,,"Is there a way to prove that every Riemann integrable function is a uniform limit of step functions? If not, does there exist a function that contradicts this?","Is there a way to prove that every Riemann integrable function is a uniform limit of step functions? If not, does there exist a function that contradicts this?",,"['real-analysis', 'riemann-integration']"
33,Sum of limit inferior: $\liminf s_n + \liminf t_n \le \liminf (s_n+t_n)$,Sum of limit inferior:,\liminf s_n + \liminf t_n \le \liminf (s_n+t_n),"I'm seeking to prove the following statement: $$\lim \inf s_{n} + \lim \inf t_{n} \le \lim \inf (s_{n}+t_{n})$$ provided that $s_{n}$ and $t_{n}$ are bounded. My solution so far: Given that $s_{n}$ and $t_{n}$ are bounded, $\lim \inf s_{n}$ and $\lim \inf t_{n}$ both exist and are real numbers. Call them $s$ and $t$ respectively. Now since $\lim \inf s_{n} = s$, given some $\epsilon > 0$, the inequality $s_{n} > s + \epsilon$ fails for a finite amount of $n$'s. These $n$'s then have the property that $s_{n} + t_{n} \le s + t_{n} + \epsilon$... So here I was able to find an inequality relating $s_{n}$ and $t_{n}$ but I seem to have it in the reverse order. Nor do I understand how to preserve the inequality if I argue about applying $\lim \inf$ to $s_{n}$ and $t_{n}$ in the inequality. Any help would be appreciated.","I'm seeking to prove the following statement: $$\lim \inf s_{n} + \lim \inf t_{n} \le \lim \inf (s_{n}+t_{n})$$ provided that $s_{n}$ and $t_{n}$ are bounded. My solution so far: Given that $s_{n}$ and $t_{n}$ are bounded, $\lim \inf s_{n}$ and $\lim \inf t_{n}$ both exist and are real numbers. Call them $s$ and $t$ respectively. Now since $\lim \inf s_{n} = s$, given some $\epsilon > 0$, the inequality $s_{n} > s + \epsilon$ fails for a finite amount of $n$'s. These $n$'s then have the property that $s_{n} + t_{n} \le s + t_{n} + \epsilon$... So here I was able to find an inequality relating $s_{n}$ and $t_{n}$ but I seem to have it in the reverse order. Nor do I understand how to preserve the inequality if I argue about applying $\lim \inf$ to $s_{n}$ and $t_{n}$ in the inequality. Any help would be appreciated.",,"['real-analysis', 'limits']"
34,"If $f$ is continuous and $f(f(x))+f(x)+x=0$ for all $x$ in $\mathbb R^2$, then $f$ is linear?","If  is continuous and  for all  in , then  is linear?",f f(f(x))+f(x)+x=0 x \mathbb R^2 f,"Let $f\in C^0(\mathbb R^2,\mathbb R^2)$, with $f(f(x))+f(x)+x=0$ for all $x$ in $\mathbb R^2$. Is $f$ linear ( : $\forall (x,y) \in (\mathbb R^2)^2,f(x+y)=f(x)+f(y)$ ) ? This problem can be translated to a problem of Geometry of the plane.","Let $f\in C^0(\mathbb R^2,\mathbb R^2)$, with $f(f(x))+f(x)+x=0$ for all $x$ in $\mathbb R^2$. Is $f$ linear ( : $\forall (x,y) \in (\mathbb R^2)^2,f(x+y)=f(x)+f(y)$ ) ? This problem can be translated to a problem of Geometry of the plane.",,"['real-analysis', 'linear-algebra', 'geometry', 'functional-equations']"
35,Product of two convergent sequences is convergent,Product of two convergent sequences is convergent,,"I want to show that if I have $\lim_{n\to\infty}a_n = a$ and $\lim_{n\to\infty}b_n = b$ then $\lim_{n\to\infty}a_nb_n = ab$ I want to do it in a slightly different way than by book but I don't know if it is correct. $(a_n)$ converges to $a$ means $$\forall \epsilon >0, \exists n_1 \in\mathbb{N}: |a_n - a| < \frac{\epsilon}{2|b|} \,\,\,\ \forall n\in\mathbb{N}, n>n_1$$ $(b_n)$ converges to $b$ means $$\forall \epsilon >0, \exists n_2 \in\mathbb{N}: |b_n - b| < \frac{\epsilon}{2|a|} \,\,\,\ \forall n\in\mathbb{N}, n>n_2$$ Hence take $n_0 := \max\{n_1,n_2\}$ and we have $$|a_nb_n - ab| = |a_nb_n -a_nb+a_nb-ab| = |a_n(b_n-b)+b(a_n-a)| \leq |a_n|\frac{\epsilon}{2|a|}+|b|\frac{\epsilon}{2|b|}=\frac{\epsilon}{2} +\frac{\epsilon}{2} = \epsilon$$ so it's proven. Can I do that trick in the penultimate step, where I cancel out $|a_n|$ and $|a|$ ? I know that for large $n$ they will be the same, but I think this proof right now is not very rigorous!","I want to show that if I have $\lim_{n\to\infty}a_n = a$ and $\lim_{n\to\infty}b_n = b$ then $\lim_{n\to\infty}a_nb_n = ab$ I want to do it in a slightly different way than by book but I don't know if it is correct. $(a_n)$ converges to $a$ means $$\forall \epsilon >0, \exists n_1 \in\mathbb{N}: |a_n - a| < \frac{\epsilon}{2|b|} \,\,\,\ \forall n\in\mathbb{N}, n>n_1$$ $(b_n)$ converges to $b$ means $$\forall \epsilon >0, \exists n_2 \in\mathbb{N}: |b_n - b| < \frac{\epsilon}{2|a|} \,\,\,\ \forall n\in\mathbb{N}, n>n_2$$ Hence take $n_0 := \max\{n_1,n_2\}$ and we have $$|a_nb_n - ab| = |a_nb_n -a_nb+a_nb-ab| = |a_n(b_n-b)+b(a_n-a)| \leq |a_n|\frac{\epsilon}{2|a|}+|b|\frac{\epsilon}{2|b|}=\frac{\epsilon}{2} +\frac{\epsilon}{2} = \epsilon$$ so it's proven. Can I do that trick in the penultimate step, where I cancel out $|a_n|$ and $|a|$ ? I know that for large $n$ they will be the same, but I think this proof right now is not very rigorous!",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'proof-writing']"
36,"Is the sequence $\{\{\log(n!)\}\}_n$ dense in $[0,1]$?",Is the sequence  dense in ?,"\{\{\log(n!)\}\}_n [0,1]","I tried to find this problem in Mathematics Stack Exchange and in Math Overflow, but I didn't find it anywhere. Here is my problem: Is the sequence $\{\{\log(n!)\}\}_n$, the sequence of fractional parts of $\log(n!)$, dense in $[0,1]$? Note 1 : Here log stands for the logarithm with base $10$. By intuition, I think that the answer is affirmative. I tried to prove that the above sequence is dense in $[0,1]$ by using the density of the sequence $\{\{\log(n)\}\}_n$, but I stucked in the process. Motivation Problem: Let $a_0,...,a_n$ be natural numbers with $a_n \neq 0,9$. Does there exist a natural number $m$ such that the first digits in the decimal representation of $m!$ are $a_0,...,a_n$? If $\{\{\log(n!)\}\}_n$ is dense in $[0,1]$ and $l:=a_0+...+a_n10^{n}$, then there is a sufficiently large natural $m$: $\{\log(l)\}<\{\log(m!)\}<\{\log(l+1)\} \Rightarrow \\\{\log(l)\}+[\log(m!)]<\log(m!)<\{\log(l+1)\}+[\log(m!)]\Rightarrow \\\log(l)+[\log(m!)]-[\log(l)]<\log(m!)<\log(l+1)+[\log(m!)]-[log(l+1)]\ (1)$ Since $a_n \neq0,9$, we have: $10^n\leq l,l+1<10^{n+1}\Rightarrow n\leq \log(l),log(l+1)<n+1 \Rightarrow [log(l)]=[log(l+1)]=n$ Hence $[\log(m!)]-[\log(l)]$ and $[\log(m!)]-[log(l+1)]$ are the same positive integer, say $k$. Then $(1)$ yields that: $10^kl<m!<10^k(l+1)$ and therefore the answer to the question is ""yes"". Note 2 : $[x]$ is the integer part of $x$. Do you have any ideas or hints that will help me prove or disprove the claim of density?","I tried to find this problem in Mathematics Stack Exchange and in Math Overflow, but I didn't find it anywhere. Here is my problem: Is the sequence $\{\{\log(n!)\}\}_n$, the sequence of fractional parts of $\log(n!)$, dense in $[0,1]$? Note 1 : Here log stands for the logarithm with base $10$. By intuition, I think that the answer is affirmative. I tried to prove that the above sequence is dense in $[0,1]$ by using the density of the sequence $\{\{\log(n)\}\}_n$, but I stucked in the process. Motivation Problem: Let $a_0,...,a_n$ be natural numbers with $a_n \neq 0,9$. Does there exist a natural number $m$ such that the first digits in the decimal representation of $m!$ are $a_0,...,a_n$? If $\{\{\log(n!)\}\}_n$ is dense in $[0,1]$ and $l:=a_0+...+a_n10^{n}$, then there is a sufficiently large natural $m$: $\{\log(l)\}<\{\log(m!)\}<\{\log(l+1)\} \Rightarrow \\\{\log(l)\}+[\log(m!)]<\log(m!)<\{\log(l+1)\}+[\log(m!)]\Rightarrow \\\log(l)+[\log(m!)]-[\log(l)]<\log(m!)<\log(l+1)+[\log(m!)]-[log(l+1)]\ (1)$ Since $a_n \neq0,9$, we have: $10^n\leq l,l+1<10^{n+1}\Rightarrow n\leq \log(l),log(l+1)<n+1 \Rightarrow [log(l)]=[log(l+1)]=n$ Hence $[\log(m!)]-[\log(l)]$ and $[\log(m!)]-[log(l+1)]$ are the same positive integer, say $k$. Then $(1)$ yields that: $10^kl<m!<10^k(l+1)$ and therefore the answer to the question is ""yes"". Note 2 : $[x]$ is the integer part of $x$. Do you have any ideas or hints that will help me prove or disprove the claim of density?",,['real-analysis']
37,"How to prove that the set $A = \left\{ {p:{p^2} < 2,p \in {\Bbb Q^+}} \right\}$ has no greatest element? [duplicate]",How to prove that the set  has no greatest element? [duplicate],"A = \left\{ {p:{p^2} < 2,p \in {\Bbb Q^+}} \right\}","This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 3 years ago . More specifically, I have to find a $q$ in $A$ such that $q$ is larger than any $p$ in $A$. The only thing I can think of is using (${2 - {p^2}}$) somehow, or ${(p+x)^2}<2$, but other than that I don't know how to proceed. This exercise is taken from the very first chapter of Walter Rudin's Principles of Mathematical Analysis . Quoting the exercise: To do this, we associate with each rational $p > 0$ the number: $$q = p - \frac{{{p^2} - 2}}{{p + 2}} = \frac{{2p + 2}}{{p + 2}}$$ But he didn't explain how he came up with that number. The only thing I've thought of is the following procedure: $$\eqalign{   & {p^2} + (2 - {p^2}) = 2 \Rightarrow {p^2} + (\frac{{2 - {p^2}}}{2}) < 2,{\text{ and since }}2 - {p^2}{\text{ is always positive,}}  \cr    & {p^2} < \left[ {{p^2} + (\frac{{2 - {p^2}}}{2})} \right] < 2  \cr    & {\text{Therefore, let }}q = \sqrt {{p^2} + (\frac{{2 - {p^2}}}{2})} ,{\text{ then:}}  \cr    & p < q < \sqrt 2  \cr} $$ However, my result doesn't necessarily belong to ${\Bbb Q}$. I wonder how did he obtain the value of $q$ in the book. EDIT: Feel free to point out if there are more appropriate tags for this question.","This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 3 years ago . More specifically, I have to find a $q$ in $A$ such that $q$ is larger than any $p$ in $A$. The only thing I can think of is using (${2 - {p^2}}$) somehow, or ${(p+x)^2}<2$, but other than that I don't know how to proceed. This exercise is taken from the very first chapter of Walter Rudin's Principles of Mathematical Analysis . Quoting the exercise: To do this, we associate with each rational $p > 0$ the number: $$q = p - \frac{{{p^2} - 2}}{{p + 2}} = \frac{{2p + 2}}{{p + 2}}$$ But he didn't explain how he came up with that number. The only thing I've thought of is the following procedure: $$\eqalign{   & {p^2} + (2 - {p^2}) = 2 \Rightarrow {p^2} + (\frac{{2 - {p^2}}}{2}) < 2,{\text{ and since }}2 - {p^2}{\text{ is always positive,}}  \cr    & {p^2} < \left[ {{p^2} + (\frac{{2 - {p^2}}}{2})} \right] < 2  \cr    & {\text{Therefore, let }}q = \sqrt {{p^2} + (\frac{{2 - {p^2}}}{2})} ,{\text{ then:}}  \cr    & p < q < \sqrt 2  \cr} $$ However, my result doesn't necessarily belong to ${\Bbb Q}$. I wonder how did he obtain the value of $q$ in the book. EDIT: Feel free to point out if there are more appropriate tags for this question.",,"['calculus', 'real-analysis', 'algebra-precalculus', 'analysis']"
38,In what conditions a weak solution is a classical solution?,In what conditions a weak solution is a classical solution?,,"I'm studying elliptic equations in divergence form $$-D_{j}(a_{ij}(x)D_{i}u) + c(x)u = f(x) \text { in a domain } \Omega \subset \mathbb{R}^{n}$$ I call a function $u \in H^{1}(\Omega)$ a weak solution if for every $\phi \in H^1_0(\Omega)$, $$\int_\Omega (a_{ij}D_iuD_j\phi + cU\phi) = \int_\Omega f\phi$$ where The coefficients $a_{ij} \in L^{\infty}$ are uniformly elliptic; that is, there exists a positive $\lambda$ such that for all $x \in \Omega$, $\xi \in \mathbb{R}^n$, $$\sum_{i,j} a_{ij}(x) \xi_i\xi_j \ge \lambda |\xi|^2$$ The coefficient $c \in L^{\frac{n}{2}}(\Omega)$ and the inhomogeneous term $f \in L^{\frac{2n}{n+2}}(\Omega)$. I have two questions: In what conditions a weak solution is a classical solution? How to find an example of elliptic pde with weak solution but not classical solution? Thanks very much!","I'm studying elliptic equations in divergence form $$-D_{j}(a_{ij}(x)D_{i}u) + c(x)u = f(x) \text { in a domain } \Omega \subset \mathbb{R}^{n}$$ I call a function $u \in H^{1}(\Omega)$ a weak solution if for every $\phi \in H^1_0(\Omega)$, $$\int_\Omega (a_{ij}D_iuD_j\phi + cU\phi) = \int_\Omega f\phi$$ where The coefficients $a_{ij} \in L^{\infty}$ are uniformly elliptic; that is, there exists a positive $\lambda$ such that for all $x \in \Omega$, $\xi \in \mathbb{R}^n$, $$\sum_{i,j} a_{ij}(x) \xi_i\xi_j \ge \lambda |\xi|^2$$ The coefficient $c \in L^{\frac{n}{2}}(\Omega)$ and the inhomogeneous term $f \in L^{\frac{2n}{n+2}}(\Omega)$. I have two questions: In what conditions a weak solution is a classical solution? How to find an example of elliptic pde with weak solution but not classical solution? Thanks very much!",,"['real-analysis', 'partial-differential-equations', 'sobolev-spaces', 'regularity-theory-of-pdes', 'elliptic-equations']"
39,Find the value of $\sum_{n=0}^\infty\frac{1}{9n^2+9n+2}$,Find the value of,\sum_{n=0}^\infty\frac{1}{9n^2+9n+2},"I was doing some problems in algebraic number theory and this series came up $$\sum_{n=0}^\infty\frac{1}{9n^2+9n+2}.$$ So, I would like to know the value of this series. However, I don't want a full answer, only hints. I have tried to use Fourier's series, but with no success, perhaps an answer can be gotten usign this. Thanks","I was doing some problems in algebraic number theory and this series came up $$\sum_{n=0}^\infty\frac{1}{9n^2+9n+2}.$$ So, I would like to know the value of this series. However, I don't want a full answer, only hints. I have tried to use Fourier's series, but with no success, perhaps an answer can be gotten usign this. Thanks",,"['real-analysis', 'sequences-and-series']"
40,When doesn't a supremum exist?,When doesn't a supremum exist?,,"Other than ∞, is there another case where a supremum (or an infimum for that matter) doesn't exist?","Other than ∞, is there another case where a supremum (or an infimum for that matter) doesn't exist?",,['real-analysis']
41,Calculating the value of infinite limit of $2^{-n^2}/\sum_{k=n+1}^\infty 2^{-k^2}$,Calculating the value of infinite limit of,2^{-n^2}/\sum_{k=n+1}^\infty 2^{-k^2},How to solve the limit?                    $$\lim_{n \to \infty}\frac{2^{-n^2}}{\sum_{k=n+1}^\infty 2^{-k^2}}$$ My approach:-                I have used logarithmic test to test the denominator sum for convergence as follows:- $$ \lim_{ k\to\infty}k\log \frac{u_n}{u_{n+1}}=\lim_{k\to \infty}k\log\frac{2^{-k^2}}{2^{-(k+1)^2}}=\lim_{k\to\infty}(k+2k^2)\log2=\infty$$ Thus the infinite sum diverges. The numerator term also diverges because it is a term from monotonically decreasing sequence which has no lower bound. So overall solution is $$\infty$$ Is my attempt correct or wrong?,How to solve the limit?                    $$\lim_{n \to \infty}\frac{2^{-n^2}}{\sum_{k=n+1}^\infty 2^{-k^2}}$$ My approach:-                I have used logarithmic test to test the denominator sum for convergence as follows:- $$ \lim_{ k\to\infty}k\log \frac{u_n}{u_{n+1}}=\lim_{k\to \infty}k\log\frac{2^{-k^2}}{2^{-(k+1)^2}}=\lim_{k\to\infty}(k+2k^2)\log2=\infty$$ Thus the infinite sum diverges. The numerator term also diverges because it is a term from monotonically decreasing sequence which has no lower bound. So overall solution is $$\infty$$ Is my attempt correct or wrong?,,"['real-analysis', 'sequences-and-series', 'limits']"
42,Counterexample of the almost-inverse of the Fundamnetal Theorem of Calculus(Lebesgue).,Counterexample of the almost-inverse of the Fundamnetal Theorem of Calculus(Lebesgue).,,"Can anyone give me a counterexample to the following statement: Suppose $F \colon [0,1] \to \mathbb{R}$ is continuous and differentiable almost everywhere, then $F(b)-F(a)=\int_a^b F'(t)\, \text{d}t$. I guess let the discontinuity be $\mathbb{Q} \cap[0,1]$ may help.","Can anyone give me a counterexample to the following statement: Suppose $F \colon [0,1] \to \mathbb{R}$ is continuous and differentiable almost everywhere, then $F(b)-F(a)=\int_a^b F'(t)\, \text{d}t$. I guess let the discontinuity be $\mathbb{Q} \cap[0,1]$ may help.",,"['real-analysis', 'integration', 'lebesgue-integral', 'differential']"
43,"Question: Alternate proof to ""For any prime $p$, $\sqrt{p}$ not rational""?","Question: Alternate proof to ""For any prime ,  not rational""?",p \sqrt{p},"studying for a final right now, and one of the my study questions is, If $p$ is prime then $\sqrt{p}$ is not rational (i.e., irrational). I understand the standard proof by contradiction, where the contradiction is that $a, b$ are not coprimes when you represent $$\sqrt{p} = \frac{a}{b},$$ but I attempted a different way and want to see if it is acceptable or not (I'm assuming the latter case). So, Assume $\sqrt{p}$ as a rational number then, $\sqrt{p} = \frac{a}{b}$, where $a, b$ coprimes $p = {\left(\frac{a}{b}\right)}^{2}$ Now, since $p$ is a prime number, then the only way to represent $p$ as a rational number is by $p = \frac{p}{1}$. So, $a = p$ and $b = 1$. So, we have $p = \frac{p^2}{1^2}$ $p = p^2$, Contradiction. It follows that $\sqrt{p}$ cannot be rational (i.e., $\sqrt{p}$ is irrational). QED. Is this acceptable? Or are steps (3) $\rightarrow$ (4) too much of a leap? Any comments appreciated.","studying for a final right now, and one of the my study questions is, If $p$ is prime then $\sqrt{p}$ is not rational (i.e., irrational). I understand the standard proof by contradiction, where the contradiction is that $a, b$ are not coprimes when you represent $$\sqrt{p} = \frac{a}{b},$$ but I attempted a different way and want to see if it is acceptable or not (I'm assuming the latter case). So, Assume $\sqrt{p}$ as a rational number then, $\sqrt{p} = \frac{a}{b}$, where $a, b$ coprimes $p = {\left(\frac{a}{b}\right)}^{2}$ Now, since $p$ is a prime number, then the only way to represent $p$ as a rational number is by $p = \frac{p}{1}$. So, $a = p$ and $b = 1$. So, we have $p = \frac{p^2}{1^2}$ $p = p^2$, Contradiction. It follows that $\sqrt{p}$ cannot be rational (i.e., $\sqrt{p}$ is irrational). QED. Is this acceptable? Or are steps (3) $\rightarrow$ (4) too much of a leap? Any comments appreciated.",,"['real-analysis', 'proof-verification']"
44,"Prove without using graphing calculators that $f: \mathbb R\to \mathbb R,\,f(x)=x+\sin x$ is both one-to-one, onto (bijective) function.","Prove without using graphing calculators that  is both one-to-one, onto (bijective) function.","f: \mathbb R\to \mathbb R,\,f(x)=x+\sin x","Prove that the function $f:\mathbb R\to \mathbb R$ defined by $f(x)=x+\sin x$ for $x\in \mathbb R$ is a bijective function. The codomain of the $f(x)=x+\sin x$ is $\mathbb R$ and the range is also $\mathbb R$. So this function is an onto function. But I am confused in proving this function is one-to-one. I know about its graph and I know that if a function passes the  horizontal line test (i.e horizontal lines should not cut the function at more than one point), then it is a one-to-one function. The graph of this function looks like the graph of $y=x$ with sinusoids going along the $y=x$ line. If I use  a graphing calculator at hand, then I can tell that it is a one-to-one function and $f(x)=\frac{x}{2}+\sin x$ or $\frac{x}{3}+\sin x$ functions are not, but in the examination I need to prove this function is one-to-one theoritically, without graphing calculators. I tried the method which we generally use to prove a function is one-to-one but no success. Let $f(x_1)=f(x_2)$ and we have to prove that $x_1=x_2$ in order fot the function  to be one-to-one. Let $x_1+\sin x_1=x_2+\sin x_2$ But I am stuck here and could not proceed further.","Prove that the function $f:\mathbb R\to \mathbb R$ defined by $f(x)=x+\sin x$ for $x\in \mathbb R$ is a bijective function. The codomain of the $f(x)=x+\sin x$ is $\mathbb R$ and the range is also $\mathbb R$. So this function is an onto function. But I am confused in proving this function is one-to-one. I know about its graph and I know that if a function passes the  horizontal line test (i.e horizontal lines should not cut the function at more than one point), then it is a one-to-one function. The graph of this function looks like the graph of $y=x$ with sinusoids going along the $y=x$ line. If I use  a graphing calculator at hand, then I can tell that it is a one-to-one function and $f(x)=\frac{x}{2}+\sin x$ or $\frac{x}{3}+\sin x$ functions are not, but in the examination I need to prove this function is one-to-one theoritically, without graphing calculators. I tried the method which we generally use to prove a function is one-to-one but no success. Let $f(x_1)=f(x_2)$ and we have to prove that $x_1=x_2$ in order fot the function  to be one-to-one. Let $x_1+\sin x_1=x_2+\sin x_2$ But I am stuck here and could not proceed further.",,"['calculus', 'real-analysis', 'functions']"
45,How to show $1 +x + x^2/2! + \dots+ x^{2n}/(2n)!$ is positive for $x\in\Bbb{R}$?,How to show  is positive for ?,1 +x + x^2/2! + \dots+ x^{2n}/(2n)! x\in\Bbb{R},"How to show $1 + x + \frac{x^2}{2!} + \dots+ \frac{x^{2n}}{(2n)!}$ is positive for $x\in\Bbb{R}$? I realize that it's a part of the Taylor Series expansion of $e^x$ but can't proceed with this knowledge? Also, I can't figure out the significance of $2n$ being the highest power.","How to show $1 + x + \frac{x^2}{2!} + \dots+ \frac{x^{2n}}{(2n)!}$ is positive for $x\in\Bbb{R}$? I realize that it's a part of the Taylor Series expansion of $e^x$ but can't proceed with this knowledge? Also, I can't figure out the significance of $2n$ being the highest power.",,"['real-analysis', 'taylor-expansion']"
46,Limit of $\frac{(n+1)^{2n}}{(n^2+1)^n}$ as $n\to \infty$,Limit of  as,\frac{(n+1)^{2n}}{(n^2+1)^n} n\to \infty,"So it is given to find $$\lim_{n\to \infty}\dfrac{(n+1)^{2n}}{(n^2+1)^n}$$ So what I did is $$\lim_{n\to \infty}\dfrac{(n+1)^{2n}}{(n^2+1)^n}=\lim_{n\to \infty}\dfrac{(n^2+2n+1)^{n}}{(n^2+1)^n}=\lim_{n\to \infty}\left(1+\frac{2n}{n^2+1}\right)^n$$ Now the rightmost form, is it $e^2$? I mean I am unable convince myself that it is some form of exponential function. Help me out.","So it is given to find $$\lim_{n\to \infty}\dfrac{(n+1)^{2n}}{(n^2+1)^n}$$ So what I did is $$\lim_{n\to \infty}\dfrac{(n+1)^{2n}}{(n^2+1)^n}=\lim_{n\to \infty}\dfrac{(n^2+2n+1)^{n}}{(n^2+1)^n}=\lim_{n\to \infty}\left(1+\frac{2n}{n^2+1}\right)^n$$ Now the rightmost form, is it $e^2$? I mean I am unable convince myself that it is some form of exponential function. Help me out.",,"['real-analysis', 'limits', 'exponential-function']"
47,A linear transform of a closed set is closed,A linear transform of a closed set is closed,,"A linear transform of a closed set $E\subset \mathbb{R}^d \to \mathbb{R}^d$ is closed. I have seen a lot of similar questions here, but none of them exactly addresses the issue. Please if you find it duplicate make sure it is and comment about it. A set is closed if it's complement is open. A set $E\subset \mathbb{R}^d$ is open if for every $x\in E$ there exists $r>0$ with $B_r(x)\subset E$, where $B_r(x)$ is a ball centered at $x$ with radius $r$,","A linear transform of a closed set $E\subset \mathbb{R}^d \to \mathbb{R}^d$ is closed. I have seen a lot of similar questions here, but none of them exactly addresses the issue. Please if you find it duplicate make sure it is and comment about it. A set is closed if it's complement is open. A set $E\subset \mathbb{R}^d$ is open if for every $x\in E$ there exists $r>0$ with $B_r(x)\subset E$, where $B_r(x)$ is a ball centered at $x$ with radius $r$,",,"['real-analysis', 'general-topology', 'transformation', 'linear-transformations']"
48,Is there a simple way of showing that the directional derivative is the dot product of grad(f) with the directional vector u?,Is there a simple way of showing that the directional derivative is the dot product of grad(f) with the directional vector u?,,"I am reading a sort of technical proof but I would like to prove it myself in a cleaner and shorter way. So, I want to show $$D_uf = \nabla f. u$$ Any suggestions are greatly appreciated. Thanks, EDIT:  I think I will try to work backwards, using matrix multiplication, since grad(f) can be viewed as a 1x2 matrix.","I am reading a sort of technical proof but I would like to prove it myself in a cleaner and shorter way. So, I want to show $$D_uf = \nabla f. u$$ Any suggestions are greatly appreciated. Thanks, EDIT:  I think I will try to work backwards, using matrix multiplication, since grad(f) can be viewed as a 1x2 matrix.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'vector-analysis']"
49,A singular Gronwall inequality,A singular Gronwall inequality,,"Let $f : [0,T] \to \Bbb{R}^+$ be a continuous function such that $f(0)=0 $ and : $$ f(t)\le C\int_0^t s^{-1}f(s) ds,\; \forall t\in [0,T] $$ for some constant $C>0.$ Is it true that $f(t)=0,\; \forall t\in [0,T]?$",Let be a continuous function such that and : for some constant Is it true that,"f : [0,T] \to \Bbb{R}^+ f(0)=0  
f(t)\le C\int_0^t s^{-1}f(s) ds,\; \forall t\in [0,T]
 C>0. f(t)=0,\; \forall t\in [0,T]?","['real-analysis', 'integration', 'ordinary-differential-equations', 'inequality']"
50,Convergence of series of nested sequence,Convergence of series of nested sequence,,"Let $a_n =\underbrace{\sin \left ( \sin \left ( \sin \cdots (\sin x) \cdots \right ) \right )}_{n \; \rm {times}}, \; \; x \in (0, \pi/2)$. Examine if the series: $$S=\sum_{n=1}^{\infty} a_n$$ converges. I don't know how to tackle this.It seems very difficult to me!","Let $a_n =\underbrace{\sin \left ( \sin \left ( \sin \cdots (\sin x) \cdots \right ) \right )}_{n \; \rm {times}}, \; \; x \in (0, \pi/2)$. Examine if the series: $$S=\sum_{n=1}^{\infty} a_n$$ converges. I don't know how to tackle this.It seems very difficult to me!",,"['real-analysis', 'sequences-and-series']"
51,How to prove $x^ax^b = x^{a+b}$,How to prove,x^ax^b = x^{a+b},"I am looking for a proof of one of the exponent combination laws, namely the sum of powers. Here $x, a, b \in \mathbb R$ and $x > 0$. I thought about induction but since a,b are not only positive integers ($a, b \in \mathbb R$) that would not work out. Any suggestions?","I am looking for a proof of one of the exponent combination laws, namely the sum of powers. Here $x, a, b \in \mathbb R$ and $x > 0$. I thought about induction but since a,b are not only positive integers ($a, b \in \mathbb R$) that would not work out. Any suggestions?",,"['real-analysis', 'exponential-function', 'exponentiation', 'real-numbers']"
52,Finitely many discontinuities and uniform convergence,Finitely many discontinuities and uniform convergence,,"Suppose that $\left\{f_n\right\}$ converges uniformly to $f$ , and that each $f_n$ has at most $M$ discontinuities, where $M\in \mathbb{N}$ is a fixed value. The $f_n$ don't need to be discontinuous at the same points. QUESTION: Does it necessarily follow that $f$ has at most $M$ discontinuities? This occurred to me while I was taking a walk, and I was wondering if the above statement was true or if there is a counterexample(possibly pathological).","Suppose that converges uniformly to , and that each has at most discontinuities, where is a fixed value. The don't need to be discontinuous at the same points. QUESTION: Does it necessarily follow that has at most discontinuities? This occurred to me while I was taking a walk, and I was wondering if the above statement was true or if there is a counterexample(possibly pathological).",\left\{f_n\right\} f f_n M M\in \mathbb{N} f_n f M,"['real-analysis', 'analysis']"
53,"Compare $\int_0^1 f(x)\log f(x)\,dx$ and $\int_0^1f(s)\,ds\cdot\int_0^1\log f(t)\,dt$",Compare  and,"\int_0^1 f(x)\log f(x)\,dx \int_0^1f(s)\,ds\cdot\int_0^1\log f(t)\,dt","The question: Given $f$ to be a positive, measurable function on $[0,1]$, which is larger, $\displaystyle\int_0^1 f(x)\log f(x)\,dx$ or $\displaystyle\left(\int_0^1f(s)\,ds\right)\left(\int_0^1\log f(t)\,dt\right)$? I know from testing with $f(x)=x$ that the first integral is, indeed, larger. Of course, this isn't a rigorous ""proof"" (if it can even be called that). I am unsure as to go about this. At first glance, it looks similar to a Holder's Inequality problem, but appears to go the wrong direction. If anyone has a hint/suggestion as to where to start, it would be much appreciated. Thanks in advance! EDIT: I've now found examples which show equality and inequality the other direction. If I let $f(x)=\cases{c\text{ if }x\in[0,1/2)\newline1\text{ otherwise}}$, then the first integral is smaller if $0<c<1$, they are equal if $c=1$, and the second is smaller if $c>1$. The new question, is this a sufficient answer: The magnitude of the integral depends on the function $f$? RE-EDIT: I forgot about the sign change of $\log c$ when $c<1$, so I either have the integrals being equal when $c=1$ or the second integral being larger for other values of $c$. I suppose my original question still remains.","The question: Given $f$ to be a positive, measurable function on $[0,1]$, which is larger, $\displaystyle\int_0^1 f(x)\log f(x)\,dx$ or $\displaystyle\left(\int_0^1f(s)\,ds\right)\left(\int_0^1\log f(t)\,dt\right)$? I know from testing with $f(x)=x$ that the first integral is, indeed, larger. Of course, this isn't a rigorous ""proof"" (if it can even be called that). I am unsure as to go about this. At first glance, it looks similar to a Holder's Inequality problem, but appears to go the wrong direction. If anyone has a hint/suggestion as to where to start, it would be much appreciated. Thanks in advance! EDIT: I've now found examples which show equality and inequality the other direction. If I let $f(x)=\cases{c\text{ if }x\in[0,1/2)\newline1\text{ otherwise}}$, then the first integral is smaller if $0<c<1$, they are equal if $c=1$, and the second is smaller if $c>1$. The new question, is this a sufficient answer: The magnitude of the integral depends on the function $f$? RE-EDIT: I forgot about the sign change of $\log c$ when $c<1$, so I either have the integrals being equal when $c=1$ or the second integral being larger for other values of $c$. I suppose my original question still remains.",,"['real-analysis', 'integration']"
54,Lebesgue measure of any line in $\mathbb{R^2}$.,Lebesgue measure of any line in .,\mathbb{R^2},What is the Lebesgue measure of a line in $\mathbb R^2$ ? I am guessing that this zero. But I couldn't prove it rigorously. Please help. From this can I conclude that any proper subspace of $\mathbb R^n$ has measure zero?,What is the Lebesgue measure of a line in ? I am guessing that this zero. But I couldn't prove it rigorously. Please help. From this can I conclude that any proper subspace of has measure zero?,\mathbb R^2 \mathbb R^n,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'dimension-theory-analysis']"
55,Using mean value theorem to show that $\cos (x)>1-x^2/2$,Using mean value theorem to show that,\cos (x)>1-x^2/2,"I have a question, by applying the mean value theorem  to $f(x)=\frac{x^2}{2}+\cos (x)$ , on the interval $[0,x]$ , show that $\cos (x)>1-\frac{x^2}{2}$ . We know that $\frac{\text{df}(x)}{\text{dx}}=x-\sin (x)$ , for $x>0$ . By the MVT, if $x>0$ , then $f(x)-f(0)=(x+0) f'(c)$ for some $c>0$ . This is where I get confused: so, $f(x)>f(0)=1$ , but why? Is it my lack of inequality that is showing, or what am I missing? Is $f'(x)\cdot x=1$ or what is going on?","I have a question, by applying the mean value theorem  to , on the interval , show that . We know that , for . By the MVT, if , then for some . This is where I get confused: so, , but why? Is it my lack of inequality that is showing, or what am I missing? Is or what is going on?","f(x)=\frac{x^2}{2}+\cos (x) [0,x] \cos (x)>1-\frac{x^2}{2} \frac{\text{df}(x)}{\text{dx}}=x-\sin (x) x>0 x>0 f(x)-f(0)=(x+0) f'(c) c>0 f(x)>f(0)=1 f'(x)\cdot x=1","['real-analysis', 'inequality']"
56,Is there a function almost everywhere $0$ on $\mathbb{R}$ whose graph is dense in $\mathbb{R^2}$?,Is there a function almost everywhere  on  whose graph is dense in ?,0 \mathbb{R} \mathbb{R^2},Is there a  function almost everywhere $0$  on $\mathbb{R}$ whose graph is dense in $\mathbb{R^2}$? How to establish such strange funciton?,Is there a  function almost everywhere $0$  on $\mathbb{R}$ whose graph is dense in $\mathbb{R^2}$? How to establish such strange funciton?,,['real-analysis']
57,"$\int \ln (\cos x)\,dx$",,"\int \ln (\cos x)\,dx","Why is $\ln( \cos x)$ a function that cannot be integrated symbolically?  From what I have read, it's integral would produce a result that is not an elementary function, but I don't see why getting something like a polylogarithm is so bad.  Further, $\int\ln( u)\, du$ can be solved, so why not use $u$-sub and evaluate it in this manner? Thank you for your help in advance.","Why is $\ln( \cos x)$ a function that cannot be integrated symbolically?  From what I have read, it's integral would produce a result that is not an elementary function, but I don't see why getting something like a polylogarithm is so bad.  Further, $\int\ln( u)\, du$ can be solved, so why not use $u$-sub and evaluate it in this manner? Thank you for your help in advance.",,['calculus']
58,Does the series $\sum_{n=0}^{+\infty} \sin((1+\sqrt{2})^n\pi)$ converge?,Does the series  converge?,\sum_{n=0}^{+\infty} \sin((1+\sqrt{2})^n\pi),"I am trying the following exercise, Convergence of the series $\sum_{n=0}^{+\infty} \sin((1+\sqrt{2})^n\pi)$ I tried like the method for $\sum_{n=0}^{+\infty}  \sin((2+\sqrt{3})^n\pi)$, with $u_n=\sin((1-\sqrt{2})^n\pi)$ unfortunately $(1-\sqrt{2})<0$ so I cannot use theorem for positivness. There is an another trick for this one ? Thank you in advance","I am trying the following exercise, Convergence of the series $\sum_{n=0}^{+\infty} \sin((1+\sqrt{2})^n\pi)$ I tried like the method for $\sum_{n=0}^{+\infty}  \sin((2+\sqrt{3})^n\pi)$, with $u_n=\sin((1-\sqrt{2})^n\pi)$ unfortunately $(1-\sqrt{2})<0$ so I cannot use theorem for positivness. There is an another trick for this one ? Thank you in advance",,"['real-analysis', 'sequences-and-series']"
59,Riemann Integrable and Uniform Convergence,Riemann Integrable and Uniform Convergence,,"Prove that if $f_n \to f_0$ uniformly on $[a,b]$, and all of these functions are Riemann integrable, then $\lim_{n\to \infty} \int_a^b f_n(x) \, dx = \int_a^x f_0(x) \, dx.$ I would try to say since we have uniform convergence we know, $\lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}=\int_{a}^{b}f_0{(x)}\ dx$ $\Rightarrow \lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}-\int_{a}^{b}f_0{(x)}\ dx=0$ $\Rightarrow \lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}-f_0{(x)}\ dx=0$ $\Rightarrow \int_{a}^{b}[\lim_{\ n\to\infty}(f_n(x)-f_0(x))]=0$ ($\textbf{unif convg})$. Morso, $\sup|(f_n-f_0)|<\epsilon$ Here, the integrand is ""$\textbf{0}$"" since by assumption $f_n\to f_0$. I'm not sure about this.","Prove that if $f_n \to f_0$ uniformly on $[a,b]$, and all of these functions are Riemann integrable, then $\lim_{n\to \infty} \int_a^b f_n(x) \, dx = \int_a^x f_0(x) \, dx.$ I would try to say since we have uniform convergence we know, $\lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}=\int_{a}^{b}f_0{(x)}\ dx$ $\Rightarrow \lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}-\int_{a}^{b}f_0{(x)}\ dx=0$ $\Rightarrow \lim_{\ n\to\infty}\int_{a}^{b}f_n{(x)}-f_0{(x)}\ dx=0$ $\Rightarrow \int_{a}^{b}[\lim_{\ n\to\infty}(f_n(x)-f_0(x))]=0$ ($\textbf{unif convg})$. Morso, $\sup|(f_n-f_0)|<\epsilon$ Here, the integrand is ""$\textbf{0}$"" since by assumption $f_n\to f_0$. I'm not sure about this.",,['real-analysis']
60,Theorem 6.20 rudin Integration,Theorem 6.20 rudin Integration,,"How does he do the algebra? (page 134 Rudin, chapter 6 ,theorem 6.20) $\left| \frac {F(t)- F(s)}{t-s} -f(x_o) \right| = \left| \frac{1}{t-s} \int_s^t[f(u) - f(x_o)]du \right|< \epsilon $ also, how does he conclude that $F'(x_o) = f(x_0)$ : here is the theorem(and the proof by rudin) Let $f \in \Re$ on $[a,b]$. For $  a \leq x \leq b$, put: $F(x)  = \int_a^x f(t)dt$, Then $F$ is continuous on $[a,b]$; furthermore, if $f$ is continuous at a point $x_0$  of $[a,b]$, then $F$ is differentiable at $x_o$ and $F'(x_0) = f(x_0)$. (i will omit the proof of continuity of $F$ on $[a,b]$) Suppose $f$ is continuous at $x_0$. Given $\epsilon > 0 $ choose $\delta > 0$ such that: $\vert f(t)- f(x_o) \vert < \epsilon $ if $\vert t- x_0 \vert < \delta$, and  $a \leq t \leq b $.Hence, if $x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta$ $\enspace$ with:  $a\ \leq s < t \leq b$ we have by theorem 6.12(d) $\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| = \left| \frac{1}{t-s} \int_s^t [f(u) - f(x_0)]du \right| < \epsilon$ it follows that $F'(x_0) = f(x_0)$","How does he do the algebra? (page 134 Rudin, chapter 6 ,theorem 6.20) $\left| \frac {F(t)- F(s)}{t-s} -f(x_o) \right| = \left| \frac{1}{t-s} \int_s^t[f(u) - f(x_o)]du \right|< \epsilon $ also, how does he conclude that $F'(x_o) = f(x_0)$ : here is the theorem(and the proof by rudin) Let $f \in \Re$ on $[a,b]$. For $  a \leq x \leq b$, put: $F(x)  = \int_a^x f(t)dt$, Then $F$ is continuous on $[a,b]$; furthermore, if $f$ is continuous at a point $x_0$  of $[a,b]$, then $F$ is differentiable at $x_o$ and $F'(x_0) = f(x_0)$. (i will omit the proof of continuity of $F$ on $[a,b]$) Suppose $f$ is continuous at $x_0$. Given $\epsilon > 0 $ choose $\delta > 0$ such that: $\vert f(t)- f(x_o) \vert < \epsilon $ if $\vert t- x_0 \vert < \delta$, and  $a \leq t \leq b $.Hence, if $x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta$ $\enspace$ with:  $a\ \leq s < t \leq b$ we have by theorem 6.12(d) $\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| = \left| \frac{1}{t-s} \int_s^t [f(u) - f(x_0)]du \right| < \epsilon$ it follows that $F'(x_0) = f(x_0)$",,"['real-analysis', 'integration']"
61,Prove subset of $\mathbb R^2$ is open,Prove subset of  is open,\mathbb R^2,"Problem . Let $G = \{(x,y): x \ne y\}$. Prove $G$ is an open subset of $\mathbb R^2$. What I am thinking: If I could show that $\mathbb R^2 \setminus G = \{(x,y): x = y\}$ is a closed set, then its complement $G$ is open. I might be totally off. Any suggestions?","Problem . Let $G = \{(x,y): x \ne y\}$. Prove $G$ is an open subset of $\mathbb R^2$. What I am thinking: If I could show that $\mathbb R^2 \setminus G = \{(x,y): x = y\}$ is a closed set, then its complement $G$ is open. I might be totally off. Any suggestions?",,['real-analysis']
62,Why isn't the parallel between the Fourier transform and the Laplace transform complete?,Why isn't the parallel between the Fourier transform and the Laplace transform complete?,,"I mean the question in the following sense.  For Fourier, we can do it on compact intervals and then we get a sequence of coefficients.  We can do it continuum-style, and then we get a superposition of waves of continuum-varying frequencies.  We can even do it abstractly on compact groups. So why is the only Laplace transform (at least that I've ever heard about) on $[0, \infty)$?","I mean the question in the following sense.  For Fourier, we can do it on compact intervals and then we get a sequence of coefficients.  We can do it continuum-style, and then we get a superposition of waves of continuum-varying frequencies.  We can even do it abstractly on compact groups. So why is the only Laplace transform (at least that I've ever heard about) on $[0, \infty)$?",,"['real-analysis', 'analysis', 'harmonic-analysis']"
63,Algebraic Basis vs Hilbert basis,Algebraic Basis vs Hilbert basis,,I am confused between algebraic basis and hilbert basis. How do they differ exactly? Can you give me examples (possibly in infinite dimensions) on when they are the same and when they are not the same? Thanks in Advance,I am confused between algebraic basis and hilbert basis. How do they differ exactly? Can you give me examples (possibly in infinite dimensions) on when they are the same and when they are not the same? Thanks in Advance,,"['linear-algebra', 'real-analysis']"
64,Continuity implies Borel-measurability?,Continuity implies Borel-measurability?,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function. Is it necessary that $f$ is Borel-measurable? I'm considering $A=f^{-1}((a,\infty))$ where $a\in\mathbb{R}$. Is $A$ necessarily a Borel set? It looks like it should be, but I'm not sure.","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function. Is it necessary that $f$ is Borel-measurable? I'm considering $A=f^{-1}((a,\infty))$ where $a\in\mathbb{R}$. Is $A$ necessarily a Borel set? It looks like it should be, but I'm not sure.",,['real-analysis']
65,"The Cantor Set length is zero, why? Limits","The Cantor Set length is zero, why? Limits",,"My question is after the construction of the Cantor set which you might want to skip over that part . This question is not philosophical at all it is pertinent to my understanding of measure and space. Please If you think this is absurd, don't bother replying. Construction of the Cantor Set: We will denote the cantor set as $\mathfrak{C}\subset\mathbb{I}$  , $\mathbb{I}=[0,1]$   as defined in section 1, as mentioned above we construct $\mathfrak{C}$   by removing middle third of the set (not including its endpoints). We denote this first slice in $\mathbb{I}$   as $$\mathfrak{c_{1}}=\left[0,\frac{1}{3}\right]\cup\left[\frac{2}{3},1\right]=\mathbb{I}\setminus\left(\frac{1}{3},\frac{2}{3}\right)$$  Next we cut out the middle third of each interval in $\mathfrak{c_{1}}$  , that is the intervals $\left[0,\frac{1}{3}\right]\text{ and }\left[\frac{2}{3},1\right]$   so we get  $$\left[0,\frac{1}{3}\right]\to\left[\frac{0}{9},\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{3}{9}\right]=\left[0,\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{1}{3}\right]$$  $$\left[\frac{2}{3},1\right]\to\left[\frac{6}{9},\frac{7}{9}\right]\cup\left[\frac{8}{9},\frac{9}{9}\right]=\left[\frac{2}{3},\frac{7}{9}\right]\cup\left[\frac{8}{9},1\right]$$ So we write $$\mathfrak{c_{2}}=\left[0,\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{1}{3}\right]\cup\left[\frac{2}{3},\frac{7}{9}\right]\cup\left[\frac{8}{9},1\right]$$ Hence we have that $\mathfrak{c_{2}}$   is the union of $2^{2}$   intervals all of which are of the form $[k/3^{2},(k+1)/3^{2}]$  , if we continue this process for all $n\in\mathbb{N}$   we find that each $\mathfrak{c}_{n}$   is constructed of $2^{n}$   intervals all of the form $[k/3^{n},(k+1)/3^{n}]$  , and $\mathfrak{c}_{n+1}$   will be obtained by taking the middle third out of each interval in $\mathfrak{c}_{n}$  . The cantor set is what remain after we continue this processv for all $n\in\mathbb{N}$   and then take the intersection over every $\mathfrak{c}_{i}$  , or we could write $$\lim_{n\to\infty}\bigcap_{i=1}^{n}\mathfrak{c}_{i}=\mathfrak{C}$$   Where $\mathfrak{C}$   is the Cantor set. My Question: Why do we say that the Cantor set has no length? I get that the length of $\mathfrak{c_1}=\left(\frac{2}{3}\right)^1$ and that the length of $\mathfrak{c}_n=\left(\frac{2}{3}\right)^n$ so  $$\lim_{n\to \infty} \left(\frac{2}{3}\right)^n= 0$$ Since, $\mathfrak{C}\subseteq\mathfrak{c}_n$ if follows that $\mathfrak{C}$ has a length of zero. Or, even you could argue that  $$\lim_{n\to\infty}\frac{k}{3^{n}}=\lim_{n\to\infty}\frac{k+1}{3^{n}}=0$$ Since, the endpoints of each interval in $\mathfrak{c}_n$ converge to the same point each interval will become a single point in interval $\mathbb{I}$, and a single point $x\in\mathbb{R}$ has a measure of zero, i.e. $d(x,x)=0$. So, summing up distances of each intervals (of which there must be $2^{\aleph_0}$ many) in $\mathfrak{C}$, you get that the distance is zero. It is the concept of the limit and space that is tripping me up . Even if $n\to \infty$ $\frac{k}{3^n}\neq 0$ unless $k=0$ (in which case $\frac{k+1}{3^n}\neq 0$). In my mind $\frac{k}{3^n}$ can never equal $\frac{k+1}{3^n}$ because $k\in\mathbb{N}$ and $k+1$ is $k$'s successor (thus they are clearly not equal), so in a world of only the natural numbers it is clear that $k$ and $k+1$ don't occupy the same space. However, in $\mathbb{R}$ if we take the limit $n \to \infty$ of both $\frac{k}{3^n}$ and $\frac{k+1}{3^n}$ they end up equaling the same point in space, even though there numerators are different. Clearly, a difference of $1$ is negligible  in respects to $\infty$. But in my mind the every point in $\mathbb{R}^n$ is distinct, if you changed the trillionth decimal place in $\sqrt{2}$ and left every other digit intact, this new number would not share the same space (point) as $\sqrt{2}$ in $\mathbb{R}$. And in this same vein I feel like $\frac{k}{3^n}$ and $\frac{k+1}{3^n}$ will never be equal, they will differ by some decimal. And ultimately that would mean that the Cantor set has a distance/measure. Can someone explain to me why we would think differently about this? I am taking a first course in Analysis but please feel free to go beyond this If it will give a more rigorous explanation for why the cantor set has no distance. (Also, point out any abuses in language that I might have made.) Thanks!","My question is after the construction of the Cantor set which you might want to skip over that part . This question is not philosophical at all it is pertinent to my understanding of measure and space. Please If you think this is absurd, don't bother replying. Construction of the Cantor Set: We will denote the cantor set as $\mathfrak{C}\subset\mathbb{I}$  , $\mathbb{I}=[0,1]$   as defined in section 1, as mentioned above we construct $\mathfrak{C}$   by removing middle third of the set (not including its endpoints). We denote this first slice in $\mathbb{I}$   as $$\mathfrak{c_{1}}=\left[0,\frac{1}{3}\right]\cup\left[\frac{2}{3},1\right]=\mathbb{I}\setminus\left(\frac{1}{3},\frac{2}{3}\right)$$  Next we cut out the middle third of each interval in $\mathfrak{c_{1}}$  , that is the intervals $\left[0,\frac{1}{3}\right]\text{ and }\left[\frac{2}{3},1\right]$   so we get  $$\left[0,\frac{1}{3}\right]\to\left[\frac{0}{9},\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{3}{9}\right]=\left[0,\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{1}{3}\right]$$  $$\left[\frac{2}{3},1\right]\to\left[\frac{6}{9},\frac{7}{9}\right]\cup\left[\frac{8}{9},\frac{9}{9}\right]=\left[\frac{2}{3},\frac{7}{9}\right]\cup\left[\frac{8}{9},1\right]$$ So we write $$\mathfrak{c_{2}}=\left[0,\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{1}{3}\right]\cup\left[\frac{2}{3},\frac{7}{9}\right]\cup\left[\frac{8}{9},1\right]$$ Hence we have that $\mathfrak{c_{2}}$   is the union of $2^{2}$   intervals all of which are of the form $[k/3^{2},(k+1)/3^{2}]$  , if we continue this process for all $n\in\mathbb{N}$   we find that each $\mathfrak{c}_{n}$   is constructed of $2^{n}$   intervals all of the form $[k/3^{n},(k+1)/3^{n}]$  , and $\mathfrak{c}_{n+1}$   will be obtained by taking the middle third out of each interval in $\mathfrak{c}_{n}$  . The cantor set is what remain after we continue this processv for all $n\in\mathbb{N}$   and then take the intersection over every $\mathfrak{c}_{i}$  , or we could write $$\lim_{n\to\infty}\bigcap_{i=1}^{n}\mathfrak{c}_{i}=\mathfrak{C}$$   Where $\mathfrak{C}$   is the Cantor set. My Question: Why do we say that the Cantor set has no length? I get that the length of $\mathfrak{c_1}=\left(\frac{2}{3}\right)^1$ and that the length of $\mathfrak{c}_n=\left(\frac{2}{3}\right)^n$ so  $$\lim_{n\to \infty} \left(\frac{2}{3}\right)^n= 0$$ Since, $\mathfrak{C}\subseteq\mathfrak{c}_n$ if follows that $\mathfrak{C}$ has a length of zero. Or, even you could argue that  $$\lim_{n\to\infty}\frac{k}{3^{n}}=\lim_{n\to\infty}\frac{k+1}{3^{n}}=0$$ Since, the endpoints of each interval in $\mathfrak{c}_n$ converge to the same point each interval will become a single point in interval $\mathbb{I}$, and a single point $x\in\mathbb{R}$ has a measure of zero, i.e. $d(x,x)=0$. So, summing up distances of each intervals (of which there must be $2^{\aleph_0}$ many) in $\mathfrak{C}$, you get that the distance is zero. It is the concept of the limit and space that is tripping me up . Even if $n\to \infty$ $\frac{k}{3^n}\neq 0$ unless $k=0$ (in which case $\frac{k+1}{3^n}\neq 0$). In my mind $\frac{k}{3^n}$ can never equal $\frac{k+1}{3^n}$ because $k\in\mathbb{N}$ and $k+1$ is $k$'s successor (thus they are clearly not equal), so in a world of only the natural numbers it is clear that $k$ and $k+1$ don't occupy the same space. However, in $\mathbb{R}$ if we take the limit $n \to \infty$ of both $\frac{k}{3^n}$ and $\frac{k+1}{3^n}$ they end up equaling the same point in space, even though there numerators are different. Clearly, a difference of $1$ is negligible  in respects to $\infty$. But in my mind the every point in $\mathbb{R}^n$ is distinct, if you changed the trillionth decimal place in $\sqrt{2}$ and left every other digit intact, this new number would not share the same space (point) as $\sqrt{2}$ in $\mathbb{R}$. And in this same vein I feel like $\frac{k}{3^n}$ and $\frac{k+1}{3^n}$ will never be equal, they will differ by some decimal. And ultimately that would mean that the Cantor set has a distance/measure. Can someone explain to me why we would think differently about this? I am taking a first course in Analysis but please feel free to go beyond this If it will give a more rigorous explanation for why the cantor set has no distance. (Also, point out any abuses in language that I might have made.) Thanks!",,"['real-analysis', 'general-topology', 'measure-theory']"
66,Limit of $x_n/n$ for sequences of the form $x_{n+1}=x_n+1/x_n^p$,Limit of  for sequences of the form,x_n/n x_{n+1}=x_n+1/x_n^p,"Given $x_1 = 1, x_{n+1} = x_n + \frac{1}{x_n} (n\ge1)$, Prove whether the limit as follow exist or not. If so, find it $$\lim_{n\to\infty}\frac{x_n}{n}$$ Given $x_1 = 1, x_{n+1} = x_n + \frac{1}{\sqrt{x_n}} (n\ge1)$, Prove whether the limit as follow exist or not. If so, find it $$\lim_{n\to\infty}\frac{x_n}{n}$$ In both cases, $x_n$ is increasing, so I tried to get an upper bound on $x_n$ (possibly depending on $n$) to apply a squeeze theorem; but failed.","Given $x_1 = 1, x_{n+1} = x_n + \frac{1}{x_n} (n\ge1)$, Prove whether the limit as follow exist or not. If so, find it $$\lim_{n\to\infty}\frac{x_n}{n}$$ Given $x_1 = 1, x_{n+1} = x_n + \frac{1}{\sqrt{x_n}} (n\ge1)$, Prove whether the limit as follow exist or not. If so, find it $$\lim_{n\to\infty}\frac{x_n}{n}$$ In both cases, $x_n$ is increasing, so I tried to get an upper bound on $x_n$ (possibly depending on $n$) to apply a squeeze theorem; but failed.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
67,Qualifying problem for real analysis: limit involving definite integral,Qualifying problem for real analysis: limit involving definite integral,,"The following problem has appeared in 2013 January qualifying exam in Purdue University, which is publicly available here . Problem 3. Let $\{a_k\}$ be sequence of positive numbers such that $a_n\to\infty$ as $n\to\infty$ . Prove that the following limit exists $$  \lim_{k\to\infty}\int_{0}^{\infty} \frac{e^{-x}\cos(x)}{a_kx^2 + \frac{1}{a_k}} dx $$ and find it. I have hardly come across to limits of sequences that involve definite integrals (in my undergraduate education so far), so this problem just seems insurmountable at the first glance. I would appreciate any hints. One of the things that comes to mind is to use limit comparison test. For example, we can evaluate integrals such as $$\int_{0}^{\infty} e^{-x}\cos(x)=\frac{1}{2}$$ But for that we would have to bound the integrand somehow. One tempting thing is to interchange the integral and the limit, which would tell us that integrand is zero in the limit, but I highly doubt this is allowed here. Looking forward to hear your thoughts. P.S. I am not sure how to make the title informative for this post. Feel free to edit as you see fit.","The following problem has appeared in 2013 January qualifying exam in Purdue University, which is publicly available here . Problem 3. Let be sequence of positive numbers such that as . Prove that the following limit exists and find it. I have hardly come across to limits of sequences that involve definite integrals (in my undergraduate education so far), so this problem just seems insurmountable at the first glance. I would appreciate any hints. One of the things that comes to mind is to use limit comparison test. For example, we can evaluate integrals such as But for that we would have to bound the integrand somehow. One tempting thing is to interchange the integral and the limit, which would tell us that integrand is zero in the limit, but I highly doubt this is allowed here. Looking forward to hear your thoughts. P.S. I am not sure how to make the title informative for this post. Feel free to edit as you see fit.","\{a_k\} a_n\to\infty n\to\infty 
 \lim_{k\to\infty}\int_{0}^{\infty} \frac{e^{-x}\cos(x)}{a_kx^2 +
\frac{1}{a_k}} dx  \int_{0}^{\infty} e^{-x}\cos(x)=\frac{1}{2}","['real-analysis', 'sequences-and-series', 'limits', 'definite-integrals']"
68,Infinitely many zeros of a nonconstant continuous function?,Infinitely many zeros of a nonconstant continuous function?,,"Let $f:[0,1]\to\mathbb{R}$ be a nonconstant continuous function. Is $S=\{x: f(x)=0\}$ finite? I have thought of a function with countably many $0$'s like lots of triangular bumps at each point $\{1/n\}$, I mean lots of $W/M$ shapes on $[0,1]$. Is it okay?","Let $f:[0,1]\to\mathbb{R}$ be a nonconstant continuous function. Is $S=\{x: f(x)=0\}$ finite? I have thought of a function with countably many $0$'s like lots of triangular bumps at each point $\{1/n\}$, I mean lots of $W/M$ shapes on $[0,1]$. Is it okay?",,"['real-analysis', 'continuity', 'examples-counterexamples']"
69,Prove that the set of integer coefficients polynomials is countable,Prove that the set of integer coefficients polynomials is countable,,How to prove that the set of integer coefficient polynomials is countable?,How to prove that the set of integer coefficient polynomials is countable?,,"['real-analysis', 'elementary-set-theory', 'polynomials']"
70,Is uncountably summation defined?,Is uncountably summation defined?,,"We know that finite and countably summation is defined. But How about uncountably summation, say $$\sum_{i\in \mathbb{R}}0$$ Is it defined?","We know that finite and countably summation is defined. But How about uncountably summation, say $$\sum_{i\in \mathbb{R}}0$$ Is it defined?",,"['real-analysis', 'definition']"
71,"$x_{n+1} = \sqrt{3x_n}$ converges for $x_1 = 1,x_1 = 27$ Proof",converges for  Proof,"x_{n+1} = \sqrt{3x_n} x_1 = 1,x_1 = 27","Prove $x_{n+1} = \sqrt{3x_n}$ converges for $x_1 = 1,x_1 = 27$. (Separate problems for $x_1 = 1$ and $x_1 = 27$.) EDIT: Took out bad algebra.","Prove $x_{n+1} = \sqrt{3x_n}$ converges for $x_1 = 1,x_1 = 27$. (Separate problems for $x_1 = 1$ and $x_1 = 27$.) EDIT: Took out bad algebra.",,"['real-analysis', 'sequences-and-series', 'recurrence-relations']"
72,Is the set of all bounded sequences complete?,Is the set of all bounded sequences complete?,,"Let $X$ be the set of all bounded sequences $x=(x_n)$ of real numbers and let $$d(x,y)=\sup{|x_n-y_n|}.$$ I need to show that $X$ is a complete metric space. I need to show that all Cauchy sequences are convergent. I appreciate your help.","Let $X$ be the set of all bounded sequences $x=(x_n)$ of real numbers and let $$d(x,y)=\sup{|x_n-y_n|}.$$ I need to show that $X$ is a complete metric space. I need to show that all Cauchy sequences are convergent. I appreciate your help.",,['real-analysis']
73,Is odd continuous function differentiable at $x=0$?,Is odd continuous function differentiable at ?,x=0,"Suppose that $f(x)$ is continuous and odd: $f(-x) = - f(x)$. Does it have a derivative at $x=0$? Here is what I got so far: First we calculate $f(0)$ using $f(-0) = -f(0)$, from which $f(0) = 0$. Then we calculate $f'(0)$ as follows: $$ f'(0) = \lim_{x\to0}\frac{f(x)-f(0)}{x-0} = \lim_{x\to0}\frac{f(x)}{x}\,. $$ But the limit from the left is equal to the limit from the right: $$ \lim_{x\to0^-}\frac{f(x)}{x} =\lim_{x\to0^+}\frac{f(-x)}{-x} =\lim_{x\to0^+}\frac{-f(x)}{-x} =\lim_{x\to0^+}\frac{f(x)}{x}\,, $$ which means that as long as the limit from the right exists, the function is differentiable at $x=0$. The limit is of the type $\frac{0}{0}$, since both $x$ and $f(x)$ goes zero at $x=0$. However, since $f(x)$ is continuous and $f(0)=0$, then in the vicinity of $x=0$, it must have a well defined value and the limit should exist. However, I didn't figure out how to finish the proof. I've been trying to construct counter examples. A simple example is $f(x) = x^2 \sin \frac{1}{x}$, which is continuous (we define $f(0)=0$) and odd, with the derivative $f(x) = 2x\sin \frac{1}{x} - \cos \frac{1}{x}$, which oscillates between -1 and 1 around $x=0$. But at $x=0$, the derivative should be equal to zero, because $\frac{f(x)}{x} = x \sin \frac{1}{x}$ which goes to zero. So the derivative does exist at $x=0$ here.","Suppose that $f(x)$ is continuous and odd: $f(-x) = - f(x)$. Does it have a derivative at $x=0$? Here is what I got so far: First we calculate $f(0)$ using $f(-0) = -f(0)$, from which $f(0) = 0$. Then we calculate $f'(0)$ as follows: $$ f'(0) = \lim_{x\to0}\frac{f(x)-f(0)}{x-0} = \lim_{x\to0}\frac{f(x)}{x}\,. $$ But the limit from the left is equal to the limit from the right: $$ \lim_{x\to0^-}\frac{f(x)}{x} =\lim_{x\to0^+}\frac{f(-x)}{-x} =\lim_{x\to0^+}\frac{-f(x)}{-x} =\lim_{x\to0^+}\frac{f(x)}{x}\,, $$ which means that as long as the limit from the right exists, the function is differentiable at $x=0$. The limit is of the type $\frac{0}{0}$, since both $x$ and $f(x)$ goes zero at $x=0$. However, since $f(x)$ is continuous and $f(0)=0$, then in the vicinity of $x=0$, it must have a well defined value and the limit should exist. However, I didn't figure out how to finish the proof. I've been trying to construct counter examples. A simple example is $f(x) = x^2 \sin \frac{1}{x}$, which is continuous (we define $f(0)=0$) and odd, with the derivative $f(x) = 2x\sin \frac{1}{x} - \cos \frac{1}{x}$, which oscillates between -1 and 1 around $x=0$. But at $x=0$, the derivative should be equal to zero, because $\frac{f(x)}{x} = x \sin \frac{1}{x}$ which goes to zero. So the derivative does exist at $x=0$ here.",,"['real-analysis', 'derivatives', 'parity']"
74,Continuity of $\arg (z)$,Continuity of,\arg (z),"Let $\mathrm{arg}:\mathbb{C}\setminus \{0\} \to [0,2\pi) $ be the function with $\mathrm{arg}(re^{i\alpha})= \alpha$ and $\alpha \in [0,2\pi)$. How can we prove that $\mathrm{arg}: \mathbb{C}\setminus A \to \mathbb{R}$ is continuous when $A= \{z\in \mathbb{C} | x \geq 0, y=0\}$?","Let $\mathrm{arg}:\mathbb{C}\setminus \{0\} \to [0,2\pi) $ be the function with $\mathrm{arg}(re^{i\alpha})= \alpha$ and $\alpha \in [0,2\pi)$. How can we prove that $\mathrm{arg}: \mathbb{C}\setminus A \to \mathbb{R}$ is continuous when $A= \{z\in \mathbb{C} | x \geq 0, y=0\}$?",,['real-analysis']
75,Old graduate analysis qualifying exam question,Old graduate analysis qualifying exam question,,"I am working through an old real analysis qualifying exam. Most of the problems are measure theory related except perhaps one, which I am having some trouble (probably because I have not seen this type of question before). I would be very appreciated if someone can help me. Here is the question: Compute $$\underset{a,b,c}{\min}{\textstyle\int_{-1}^{1}} \left\vert x^{3}-a-bx-cx^{2}\right\vert dx$$ and find $$\max{\int_{-1}^{1}}x^{3}g(x)dx,$$ where $g$ is subject to the restrictions $$\begin{gather*} \int_{-1}^{1}g(x)dx={\int_{-1}^{1}x}g(x)dx={\int_{-1}^{1}}x^{2}g(x)dx=0;\\ {\int_{-1}^{1}}\left\vert g(x)\right\vert ^{2}dx=1 \end{gather*}$$","I am working through an old real analysis qualifying exam. Most of the problems are measure theory related except perhaps one, which I am having some trouble (probably because I have not seen this type of question before). I would be very appreciated if someone can help me. Here is the question: Compute $$\underset{a,b,c}{\min}{\textstyle\int_{-1}^{1}} \left\vert x^{3}-a-bx-cx^{2}\right\vert dx$$ and find $$\max{\int_{-1}^{1}}x^{3}g(x)dx,$$ where $g$ is subject to the restrictions $$\begin{gather*} \int_{-1}^{1}g(x)dx={\int_{-1}^{1}x}g(x)dx={\int_{-1}^{1}}x^{2}g(x)dx=0;\\ {\int_{-1}^{1}}\left\vert g(x)\right\vert ^{2}dx=1 \end{gather*}$$",,"['real-analysis', 'measure-theory', 'integration']"
76,How to find the function $f$ given $f(f(x)) = xf(x)$?,How to find the function  given ?,f f(f(x)) = xf(x),I was wondering if there is a continuous function such that $f(f(x)) = xf(x)$ for every positive number $x$.,I was wondering if there is a continuous function such that $f(f(x)) = xf(x)$ for every positive number $x$.,,"['real-analysis', 'functional-equations']"
77,Epsilon-Delta Proof for $x^n$ tends to 0,Epsilon-Delta Proof for  tends to 0,x^n,"What is the epsilon proof that $x^n \rightarrow 0$ as $n \rightarrow \infty$ provided $|x| < 1 $? I only know it's true because I know the geometric series converges, which implies its terms must tend to 0, but never seen an epsilon proof of this simple fact.","What is the epsilon proof that $x^n \rightarrow 0$ as $n \rightarrow \infty$ provided $|x| < 1 $? I only know it's true because I know the geometric series converges, which implies its terms must tend to 0, but never seen an epsilon proof of this simple fact.",,['real-analysis']
78,what is value of $\lim_{n\to\infty} n^{-2}e^{(\log(n))^a}$?,what is value of ?,\lim_{n\to\infty} n^{-2}e^{(\log(n))^a},"What is value of $\displaystyle\lim_{n\to+\infty} n^{-2}e^{(\log(n))^a}$, where $a > 1$? I tried l'Hospital's rule but it gets complicated.","What is value of $\displaystyle\lim_{n\to+\infty} n^{-2}e^{(\log(n))^a}$, where $a > 1$? I tried l'Hospital's rule but it gets complicated.",,"['real-analysis', 'limits']"
79,History of analysis?,History of analysis?,,"Any sites detailing the history of analysis post 1820 (to mid 1900s?) - vis-à-vis Cauchy, Weierstrass, Riemann, Bolzano, ..., Kuratowski, Hilbert? It's something that appears quite interesting and I would like to have a knowledge of the direction analysis took and how things like topological spaces were motivated and viewed (pedagogically they're an extension of a metric space though I think there is much, much more to the history than this though I might be incorrect). Being a poor student purchasing ""mathematical analysis by its history"" isn't an option (for two reasons; I need money for food and core material, but more importantly - mathematically the comments would be worthless as post 1900 detail is not included). I looked to the library for history and - understandably - the books look at geometry, calculus (not analysis! Leibniz versus Newton) etc.","Any sites detailing the history of analysis post 1820 (to mid 1900s?) - vis-à-vis Cauchy, Weierstrass, Riemann, Bolzano, ..., Kuratowski, Hilbert? It's something that appears quite interesting and I would like to have a knowledge of the direction analysis took and how things like topological spaces were motivated and viewed (pedagogically they're an extension of a metric space though I think there is much, much more to the history than this though I might be incorrect). Being a poor student purchasing ""mathematical analysis by its history"" isn't an option (for two reasons; I need money for food and core material, but more importantly - mathematically the comments would be worthless as post 1900 detail is not included). I looked to the library for history and - understandably - the books look at geometry, calculus (not analysis! Leibniz versus Newton) etc.",,"['real-analysis', 'analysis', 'reference-request', 'math-history']"
80,A deceiving Taylor series,A deceiving Taylor series,,"When we try to expand $$ \begin{align} f:&\mathbb R \to \mathbb R\\ &x \mapsto   \begin{cases}   \mathrm e^{-\large\frac 1{x^2}} &\Leftarrow x\neq 0\\   0 &\Leftarrow x=0   \end{cases} \end{align}$$ in the Taylor series about $x = 0$, we wrongly conclude that $f(x) \equiv 0$, because the derivative of $f(x)$ of any order at this point is $0$. Therefore, $f(x)$ is not well-behaved for this procedure. What conditions determine the good behavior of a function for the Taylor series expansion?","When we try to expand $$ \begin{align} f:&\mathbb R \to \mathbb R\\ &x \mapsto   \begin{cases}   \mathrm e^{-\large\frac 1{x^2}} &\Leftarrow x\neq 0\\   0 &\Leftarrow x=0   \end{cases} \end{align}$$ in the Taylor series about $x = 0$, we wrongly conclude that $f(x) \equiv 0$, because the derivative of $f(x)$ of any order at this point is $0$. Therefore, $f(x)$ is not well-behaved for this procedure. What conditions determine the good behavior of a function for the Taylor series expansion?",,"['calculus', 'real-analysis', 'power-series', 'taylor-expansion']"
81,"""Boundary"" of convergence of  $\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}}$","""Boundary"" of convergence of",\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}},"I ran across this confounding limit I am wondering about.  It is as follows: $$\displaystyle \lim_{n\to \infty}\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}}, \;\ 0<c<1$$ I played around with this on Maple and found that if c is less than approximately .382 (but greater than 0), it converges to 0. If c is greater than .382 (but less than 1), it diverges. What is it about .382?. .382 is an approximation. By playing around more I could have taken it out to more decimal places. The actual problem asks to prove that the above limit is < $\frac{1}{p(n)}$, where p(n) is a polynomial. I was mainly wondering how to solve the limit and why .382 is so significant. Thank you all very much. You are always a big help.","I ran across this confounding limit I am wondering about.  It is as follows: $$\displaystyle \lim_{n\to \infty}\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}}, \;\ 0<c<1$$ I played around with this on Maple and found that if c is less than approximately .382 (but greater than 0), it converges to 0. If c is greater than .382 (but less than 1), it diverges. What is it about .382?. .382 is an approximation. By playing around more I could have taken it out to more decimal places. The actual problem asks to prove that the above limit is < $\frac{1}{p(n)}$, where p(n) is a polynomial. I was mainly wondering how to solve the limit and why .382 is so significant. Thank you all very much. You are always a big help.",,"['real-analysis', 'limits']"
82,Don't Understand Proof in Rudin Principles of Mathematical Analysis Book Thm 7.24?,Don't Understand Proof in Rudin Principles of Mathematical Analysis Book Thm 7.24?,,"I was rather confused about this proof I came across in Rudin Chapter 7.  The premise is: If K is a compact metric space, if $f_n \in C(K)$ for $n =  1, 2, 3, ...,$ ($C(K)$ being a set of complex-valued, continuous and bounded functions), and if $\{f_n\}$ converges uniformly on $K$, then $\{f_n\}$ is equicontinuous on $K$. Proof: Let $\epsilon > 0$ be given.  Since $\{f_n\}$ converges uniformly, there is an integer $N$ such that $$||f_n - f_N|| < \epsilon (n > N).$$ (See Definition 7.14).  Since continuous functions are uniformly continuous on compact sets, there is a $\delta > 0$ such that $$|f_i(x) - f_i(y)| < \epsilon$$ if $1 \leq i \leq N$ and $d(x,y) < \delta$.* If $n < N$ and $d(x,y) < \delta$, it follows that $$|f_n(x) - f_n(y)| \leq |f_n(x) - f_n(y)| + |f_n(x) - f_N(y)| + |f_N(x) - f_N(y)| < 3\epsilon.$$ $QED$. ** What I don't understand is the line ending with *.  I understand that when a family of functions are uniformly continuous, for every $\epsilon > 0$, there exists an integer $N$ such that $n \geq N$ implies $|f_n(x) - f_n(y)| \leq \epsilon$, but it doesn't say anything about the case when $n \leq N$.  Perhaps it has something to do with the finiteness of the number of functions? It might be something really obvious I'm overlooking.","I was rather confused about this proof I came across in Rudin Chapter 7.  The premise is: If K is a compact metric space, if $f_n \in C(K)$ for $n =  1, 2, 3, ...,$ ($C(K)$ being a set of complex-valued, continuous and bounded functions), and if $\{f_n\}$ converges uniformly on $K$, then $\{f_n\}$ is equicontinuous on $K$. Proof: Let $\epsilon > 0$ be given.  Since $\{f_n\}$ converges uniformly, there is an integer $N$ such that $$||f_n - f_N|| < \epsilon (n > N).$$ (See Definition 7.14).  Since continuous functions are uniformly continuous on compact sets, there is a $\delta > 0$ such that $$|f_i(x) - f_i(y)| < \epsilon$$ if $1 \leq i \leq N$ and $d(x,y) < \delta$.* If $n < N$ and $d(x,y) < \delta$, it follows that $$|f_n(x) - f_n(y)| \leq |f_n(x) - f_n(y)| + |f_n(x) - f_N(y)| + |f_N(x) - f_N(y)| < 3\epsilon.$$ $QED$. ** What I don't understand is the line ending with *.  I understand that when a family of functions are uniformly continuous, for every $\epsilon > 0$, there exists an integer $N$ such that $n \geq N$ implies $|f_n(x) - f_n(y)| \leq \epsilon$, but it doesn't say anything about the case when $n \leq N$.  Perhaps it has something to do with the finiteness of the number of functions? It might be something really obvious I'm overlooking.",,"['real-analysis', 'analysis']"
83,Rolle's theorem? [duplicate],Rolle's theorem? [duplicate],,"This question already has answers here : Prove that $f$ on $[a,b]$ has only a finite number of zeros. (3 answers) How to show the set is finite (1 answer) Closed 7 months ago . The function $f$ is differentiable in $[0,1]$ and $f$ has infinite roots in $[0,1]$ . Prove that there exists $c\in [0,1]$ such that $f(c)=f'(c)=0.$ My attempt: Assume $x_1, x_2,...,x_n,...$ are roots of $f(x)=0$ . Since $(x_n)$ is bounded, there exists $({x_n}_k)$ converging to $c\in [0,1]$ . Since $f$ is continuous in $[0,1]$ , we deduce that $f(c)=0$ . Can $f'(c)=0$ ?","This question already has answers here : Prove that $f$ on $[a,b]$ has only a finite number of zeros. (3 answers) How to show the set is finite (1 answer) Closed 7 months ago . The function is differentiable in and has infinite roots in . Prove that there exists such that My attempt: Assume are roots of . Since is bounded, there exists converging to . Since is continuous in , we deduce that . Can ?","f [0,1] f [0,1] c\in [0,1] f(c)=f'(c)=0. x_1, x_2,...,x_n,... f(x)=0 (x_n) ({x_n}_k) c\in [0,1] f [0,1] f(c)=0 f'(c)=0",['real-analysis']
84,"If $(a_n)$ is a decreasing real sequence and $\sum a_n$ converges, then does $\sum (-1)^n n a_n\ $ converge?","If  is a decreasing real sequence and  converges, then does  converge?",(a_n) \sum a_n \sum (-1)^n n a_n\ ,"""Motivation""/Introduction: If $(a_n)$ is a decreasing real sequence and $\displaystyle\sum a_n $ converges, then $n a_n \to 0,\ $ for example, by the Cauchy Condensation test . If $(a_n)$ is a real sequence and $\displaystyle\sum a_n $ converges but $(a_n)$ is not necessarily decreasing, then does $n a_n\to 0?$ No: for example, take: $ a_n:= \begin{cases}  2^{-n}&\text{if}\, n\neq 2^k\\ \frac{1}{k^2}&\text{if}\, n = 2^k\\ \end{cases} $ Therefore, if $(a_n)$ is a real sequence and $\displaystyle\sum a_n $ converges but $(a_n)$ is not necessarily decreasing, then $\displaystyle\sum n a_n\ $ and $\displaystyle\sum (-1)^n n a_n\ $ do not necessarily converge, by the counter-example above. $$$$ This leads to the following question, for which I do not have an answer: If $(a_n)$ is a decreasing real sequence and $\displaystyle\sum a_n $ converges, then does $\displaystyle\sum (-1)^n n a_n\ $ converge? I was thinking we could apply Dirichlet's test or Cauchy's Condensation test somehow, but I don't quite see how.","""Motivation""/Introduction: If is a decreasing real sequence and converges, then for example, by the Cauchy Condensation test . If is a real sequence and converges but is not necessarily decreasing, then does No: for example, take: Therefore, if is a real sequence and converges but is not necessarily decreasing, then and do not necessarily converge, by the counter-example above. This leads to the following question, for which I do not have an answer: If is a decreasing real sequence and converges, then does converge? I was thinking we could apply Dirichlet's test or Cauchy's Condensation test somehow, but I don't quite see how.","(a_n) \displaystyle\sum a_n  n a_n \to 0,\  (a_n) \displaystyle\sum a_n  (a_n) n a_n\to 0? 
a_n:=
\begin{cases}
 2^{-n}&\text{if}\, n\neq 2^k\\
\frac{1}{k^2}&\text{if}\, n = 2^k\\
\end{cases}
 (a_n) \displaystyle\sum a_n  (a_n) \displaystyle\sum n a_n\  \displaystyle\sum (-1)^n n a_n\   (a_n) \displaystyle\sum a_n  \displaystyle\sum (-1)^n n a_n\ ","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'alternating-expression']"
85,What is the arc length formula in a metric space?,What is the arc length formula in a metric space?,,"Let $f(x):[a,b]\longrightarrow \mathbb{R}^n$ be injective and continuously differentiable curve. Then the arc length is given by $$\int_a^b |f'(t)|dt$$ . What will be the arc length formula if $\mathbb R^n$ is replaced by a metric space $(X,d)$ . I found the definition of metric derivative here . Is the arc length formula given by $$\int_a^b \lim_{s\to0}\frac{d(f(t+s),f(t)}{|s|}dt$$ , if the limit and integral exists? Does it make sense at all?","Let be injective and continuously differentiable curve. Then the arc length is given by . What will be the arc length formula if is replaced by a metric space . I found the definition of metric derivative here . Is the arc length formula given by , if the limit and integral exists? Does it make sense at all?","f(x):[a,b]\longrightarrow \mathbb{R}^n \int_a^b |f'(t)|dt \mathbb R^n (X,d) \int_a^b \lim_{s\to0}\frac{d(f(t+s),f(t)}{|s|}dt","['real-analysis', 'differential-geometry', 'metric-spaces', 'metric-geometry']"
86,Are metric segments convex?,Are metric segments convex?,,"For a metric space $(X,d)$ and points $x,y \in X$ we define the metric segment between them as the following set: $\left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \}$ Can we say that metric segments are convex? That is, for an arbitrary metric space $(X,d)$ and points $x,y,u,v \in X$ , does $u,v \in \left [ x,y \right ]$ imply $\left [ u,v \right ]  \subseteq \left [ x,y \right ] $ ?","For a metric space and points we define the metric segment between them as the following set: Can we say that metric segments are convex? That is, for an arbitrary metric space and points , does imply ?","(X,d) x,y \in X \left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \} (X,d) x,y,u,v \in X u,v \in \left [ x,y \right ] \left [ u,v \right ]  \subseteq \left [ x,y \right ] ","['real-analysis', 'general-topology', 'metric-spaces', 'convexity-spaces']"
87,"Issues with the Fourier Transform of $f(t)=(1-t^2)^4$ on $[-1,\,1]$, should be analytical but looks like having a singularity with noise-like rippling","Issues with the Fourier Transform of  on , should be analytical but looks like having a singularity with noise-like rippling","f(t)=(1-t^2)^4 [-1,\,1]","Issues with the Fourier Transform of $f(t)=(1-t^2)^4$ on $[-1,\,1]$ , should be analytical but looks like having a singularity with noise-like rippling Intro I was trying to made a compact-supported approximation of a Gaussian Envelope $$g(t)= e^{-4t^2} \tag{Eq. 1}\label{Eq. 1}$$ So I was trying to use instead: $$f(t)= (1-t^2)^4\cdot\theta(1-t^2) \cong \left(\frac{1-t^2+|1-t^2|}{2}\right)^4 \tag{Eq. 2}\label{Eq. 2}$$ where $\theta(t)$ is the Heaviside step function . Both which can be seen here : As is shown in the image, at ""my taste"" the match ""looks good"", but accurately speaking, it fulfill that: The function $f(t) = 0,\,|t|\geq 1$ , so it is of compact support. At the edges of the domain $\partial t =\{-1;\,1\}$ the function $s(t)=(1-t^2)^4$ is already zero: $s(-1)=s(1)=0$ , so shouldn't be much issues related with the term $\theta(1-t^2)$ , since it is going to be at most an avoidable discontinuity: $\lim\limits_{t \to \partial t^{\pm}} f(t) = \lim\limits_{t \to \partial t} f(t) = 0$ , so the function is continuous on $\mathbb{R}$ . Using Wolfram-Alpha, the following norms shows to be bounded: $\|f\|_\infty = 1 < \infty$ , $\|f\|_1 = \frac{256}{315} \approx 0.813 < \infty$ , $\|f\|_2^2 = \frac{65536}{109395} \approx 0.599 < \infty$ so is bounded, absolute integrable, and energy limited. Also its derivative is: $$\begin{array}{r c l} \frac{d}{dt}\left((1-t^2)^4\cdot\theta(1-t^2)\right) & = & \frac{d}{dt}\left((1-t^2)^4\right)\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\theta(1-t^2)\right)\\ & = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\frac{1+\text{sgn}(1-t^2)}{2}\right)\\ & = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^4\cdot\delta(1-t^2)\\ & = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^3\cdot\underbrace{(1-t^2)\cdot\delta(1-t^2)}_{=\,0,\,\text{since}\,(1-x)\delta(1-x)=0} \\ \end{array}$$ $$\Rightarrow \frac{df(t)}{dt}  = -8\,t\,(1-t^2)^3\cdot\theta(1-t^2) \tag{Eq. 3}\label{Eq. 3} $$ so its derivative is also continuous by the same reasons of point (ii), and $\|f'\|_\infty = \frac{1798}{343\sqrt{7}} \approx 1.9 < \infty$ , $\|f'\|_1 = 2 < \infty$ , and $\|f'\|_2^2 = \frac{131072}{45045} \approx 2.9 < \infty$ . So with all these point the approximation $f(t)$ look quite ""well-behaved"": note that is not a ""smooth function"", but same analysis of point $(4)$ could be done successfully for $f''$ also. From now on I will continue labeling the observations so you can accurately point where I am having a misconception. Since the function $f(t)$ is of compact-support and also is squared-integrable (since $\|f\|_2^2 < \infty$ as show in point $(3)$ ), I where expecting that the Paley–Wiener theorem to be fulfilled, so the Fourier Transform of $f(t)$ was going to be an analytical function . Using Wolfram Alpha I calculate the Fourier Transform (under the ""electricians"" definition), in 3 different ways showing each of them the same result so I believe is right calculated - way 1 , way 2 , and way 3 : $$\hat{f}(w) = \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt = \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt = \frac{768 \Big(5w\,(2 w^2-21)\cos(w)+(w^4-45w^2+105) \sin(w)\Big)}{w^9}  \tag{Eq. 4}\label{Eq. 4}$$ So far so good, since \eqref{Eq. 4} looks like the classical Fourier Transform made by a polynomial mixed with some trigonometric functions, but when trying to calculate $\|\hat{f}\|_1$ and $\|iw\hat{f}\|_1$ the integrals get stuck on Wolfram-Alpha, so I decide to plot it to see what is going on. I am going to graph it against the Fourier Transform of $g(t)$ , which under the same definition of the transform is going to be: $$\hat{g}(w) = \frac{\sqrt{\pi}}{2}e^{-\frac{w^2}{16}}\tag{Eq. 5}\label{Eq. 5}$$ They look similar, but something ""weird"" is happening near the DC component $w \approx 0$ for the solution $\hat{f}(w)$ of \eqref{Eq. 4}. To have a better insight I plot $\hat{f}(w)$ also in Desmos and there exist an ""horrible noise-like rippling"" that also ""looks"" to be diverging as a singularity! I tried to find the value of $\|\hat{f}\|_\infty$ with Wolfram-Aplha but I believe are numerical errors since the value change when changing the domain limits, but are numbers of the order of $10^{93}$ !!!. I believe that a Analytical function is smooth (as it should be the Fourier Transform $\hat{f}$ following point $(5)$ ), so it should be bounded on every closed interval of its domain: maybe here I am mistaken, but since analytic functions are smooth, they should be continuous on every closed interval of the domain (since are already differentiable), and since is continuous on a closed interval it should achieve a bounded maximum and minimum because of the Extreme value theorem . But even if I am wrong, I wasn't expecting an analytical function to be so ""ill-behaved"" as is being $\hat{f}$ on $w \in [-0.2,\,0.2]$ . Questions So the main questions are: Q1: Is $\hat{f}(w)$ of \eqref{Eq. 4} properly obtained? Also related quantities as $\|\hat{f}\|_1$ and $\|\hat{f}\|_{\infty}$ Q2: If right, Why is this ""rippling"" happening? It is right? or is a miscalculation of the graphic software? or a numerical issue? Actually its looks like ""Brownian"" or ""White Noise"". Q3: Is truly having a singularity at some angular frequency $w$ ? (doing limits in Wolfram Alpha doesn't work, at least it is saying that $\lim_{w\to 0} \hat{f}(w)=\frac{256}{315}\approx 0.813$ which is near the main tendency, but surely below the rippling peaks - maybe the singularity is near but not in $w=0$ ). Q4: Is this rippling a known result like the "" Gibbs' Phenomenon ? How is called? If it is a known phenomenon I would like to search for any references - maybe the is a way to avoid it. Q5: Where I am misleading my analysis? (since this singularity shouldn't exists under my assumptions and my actual knowledge - which is quite basic by the way) Motivation After seeing this video about the absorption of light, temporal dispersion, and Kramers-Kronig relations, required for the complex-valued modeling of the refraction index to preserve causality (is quite a good video, simple and short), I see why dispersion must happen or other way the absorbed frequency will lead to a ""time-wide"" signal which will become non-causal in the analysis (it will be entering the absorptive medium before the frequency-component subtraction had happen on the first place). But since a Gaussian Envelope is used, the ""causality-issue"" where already present since this function only vanishes at infinity, but since spectra decays faster than $1/|w|$ , the Kramers-Kronig relation still holds ""hidding"" this issue. This is why I was trying to find another envelope but with an accurate compact-support, since it is multiplied by a trigonometric function it Fourier Transform is only a displaced version of the Transform of the Envelope function, and here is where I found the ""problem"" since the approximation I take has this ""issue"" in the spectrum - don't knowing now if its a ""real-life issue"" or just a numeric problem of the calculation algorithms. Hope you find it has interesting I am, beforehand thanks you very much. added later I have noted now another interesting thing: Integrating $\hat{f}$ as is shown here , and integrating $\hat{g}$ as is shown here gives the same result: $$\int\limits_{-\infty}^{\infty}\hat{f}(w)\,dw = \int\limits_{-\infty}^{\infty}\hat{g}(w)\,dw = 2\pi$$ I have tried another slightly different approximation: $$q(t) = \left(\frac{1-t^2+|1-t^2|}{2}\right)^{\pi} \tag{Eq. 6}\label{Eq. 6}$$ And following Wolfram-Alpha its Fourier Transform is: $$\hat{q}(w) = \int\limits_{-\infty}^{\infty} q(t)\,e^{-iwt}\,dt = \sqrt{\pi}\,\Gamma(1+\pi)_0\tilde{F}_1\left(;\,\frac{3}{2}+\pi;\,-\frac{w^2}{4}\right)  \tag{Eq. 7}\label{Eq. 7}$$ which plot doesn't show the noise-like-rippling-singularity, so with this, as it was noted on the comments and answers, the situation was a numerical issue. Even so, if you directly replace all the numbers $\pi$ by the number $4$ in \eqref{Eq. 7} the rippling appears again. Later I noted that the result of \eqref{Eq. 4} is equivalent to: $$\hat{f}(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4;\,-\frac{w^2}{4}\right)  \tag{Eq. 8}\label{Eq. 8}$$ So obviously again the numerical issues arises, but for a ""good"" approximation I am using on Wolfram-Alpha : $$\hat{f}^*(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4.0001;\,-\frac{w^2}{4}\right)  \tag{Eq. 9}\label{Eq. 9}$$ and suddenly the problem is gone... it is kind of ridiculous that ""just"" for the exact numbers the problem happen. Since the regularized confluent hypergeometric function is not quite extended on every software library I find useful the following property shown by Wolfram Alpha to make approximated plots of these kind of functions, by taking an arbitrary parameter $a$ : $$\sqrt{\pi}\,\Gamma(1+a)_0\tilde{F}_1\left(;\,\frac{3}{2}+a;\,-\frac{w^2}{4}\right) = \text{sgn}(w)\sqrt{\pi}\,\left(\frac{2}{w}\right)^{a+\frac{1}{2}}\Gamma(1+a)J_{a+\frac{1}{2}}(w) \tag{Eq. 10}\label{Eq. 10}$$ with $J_n(x)$ the Bessel function of the first kind. Unfortunately, I wasn't able to find $\|\hat{f}\|_1$ under any of these approximations (both shown on the last image), ""as it where diverging"" (I don't know if this is the case), even when $\|\hat{g}\|_1=2\pi << \infty$ , which is counter-intuitive for me giving its similarity (the lobes should be adding even less area than a Gaussian in principle), but comparing their graphs shows that the compact-support function has lobes much more spread than the Gaussian, as is shown here . But nevertheless, the tails are decreasing faster than $|\frac{1}{w^2}|$ so I don´t know why the result is not finite (see here ). As example, if I used the other aporoximarion: $$\hat{f}^*(w) \approx \frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\tag{Eq. 11}\label{Eq. 11}$$ Wolfram-Alpha is unable to find $\|\hat{f}^*(w)\|_1$ , but it is possible to bound it as: $$\|\hat{f}^*(w)\|_1 < \underbrace{\int\limits_{-10}^{10}\left|\frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\right|\,dw}_{6.34416}+\underbrace{\int\limits_{10}^{\infty}\left|\frac{2}{|w^{2}|}\right|\,dw}_{0.2} <\infty$$ as can be seen here and here . And since the difference with the approximation and the main function with numerical issues is in the bounded domain $[-1,\,1]$ , it should also be bounded, so the norm $\|\hat{f}(w)\|_1<\infty$ but somehow Wolfram-Alpha can't find it.","Issues with the Fourier Transform of on , should be analytical but looks like having a singularity with noise-like rippling Intro I was trying to made a compact-supported approximation of a Gaussian Envelope So I was trying to use instead: where is the Heaviside step function . Both which can be seen here : As is shown in the image, at ""my taste"" the match ""looks good"", but accurately speaking, it fulfill that: The function , so it is of compact support. At the edges of the domain the function is already zero: , so shouldn't be much issues related with the term , since it is going to be at most an avoidable discontinuity: , so the function is continuous on . Using Wolfram-Alpha, the following norms shows to be bounded: , , so is bounded, absolute integrable, and energy limited. Also its derivative is: so its derivative is also continuous by the same reasons of point (ii), and , , and . So with all these point the approximation look quite ""well-behaved"": note that is not a ""smooth function"", but same analysis of point could be done successfully for also. From now on I will continue labeling the observations so you can accurately point where I am having a misconception. Since the function is of compact-support and also is squared-integrable (since as show in point ), I where expecting that the Paley–Wiener theorem to be fulfilled, so the Fourier Transform of was going to be an analytical function . Using Wolfram Alpha I calculate the Fourier Transform (under the ""electricians"" definition), in 3 different ways showing each of them the same result so I believe is right calculated - way 1 , way 2 , and way 3 : So far so good, since \eqref{Eq. 4} looks like the classical Fourier Transform made by a polynomial mixed with some trigonometric functions, but when trying to calculate and the integrals get stuck on Wolfram-Alpha, so I decide to plot it to see what is going on. I am going to graph it against the Fourier Transform of , which under the same definition of the transform is going to be: They look similar, but something ""weird"" is happening near the DC component for the solution of \eqref{Eq. 4}. To have a better insight I plot also in Desmos and there exist an ""horrible noise-like rippling"" that also ""looks"" to be diverging as a singularity! I tried to find the value of with Wolfram-Aplha but I believe are numerical errors since the value change when changing the domain limits, but are numbers of the order of !!!. I believe that a Analytical function is smooth (as it should be the Fourier Transform following point ), so it should be bounded on every closed interval of its domain: maybe here I am mistaken, but since analytic functions are smooth, they should be continuous on every closed interval of the domain (since are already differentiable), and since is continuous on a closed interval it should achieve a bounded maximum and minimum because of the Extreme value theorem . But even if I am wrong, I wasn't expecting an analytical function to be so ""ill-behaved"" as is being on . Questions So the main questions are: Q1: Is of \eqref{Eq. 4} properly obtained? Also related quantities as and Q2: If right, Why is this ""rippling"" happening? It is right? or is a miscalculation of the graphic software? or a numerical issue? Actually its looks like ""Brownian"" or ""White Noise"". Q3: Is truly having a singularity at some angular frequency ? (doing limits in Wolfram Alpha doesn't work, at least it is saying that which is near the main tendency, but surely below the rippling peaks - maybe the singularity is near but not in ). Q4: Is this rippling a known result like the "" Gibbs' Phenomenon ? How is called? If it is a known phenomenon I would like to search for any references - maybe the is a way to avoid it. Q5: Where I am misleading my analysis? (since this singularity shouldn't exists under my assumptions and my actual knowledge - which is quite basic by the way) Motivation After seeing this video about the absorption of light, temporal dispersion, and Kramers-Kronig relations, required for the complex-valued modeling of the refraction index to preserve causality (is quite a good video, simple and short), I see why dispersion must happen or other way the absorbed frequency will lead to a ""time-wide"" signal which will become non-causal in the analysis (it will be entering the absorptive medium before the frequency-component subtraction had happen on the first place). But since a Gaussian Envelope is used, the ""causality-issue"" where already present since this function only vanishes at infinity, but since spectra decays faster than , the Kramers-Kronig relation still holds ""hidding"" this issue. This is why I was trying to find another envelope but with an accurate compact-support, since it is multiplied by a trigonometric function it Fourier Transform is only a displaced version of the Transform of the Envelope function, and here is where I found the ""problem"" since the approximation I take has this ""issue"" in the spectrum - don't knowing now if its a ""real-life issue"" or just a numeric problem of the calculation algorithms. Hope you find it has interesting I am, beforehand thanks you very much. added later I have noted now another interesting thing: Integrating as is shown here , and integrating as is shown here gives the same result: I have tried another slightly different approximation: And following Wolfram-Alpha its Fourier Transform is: which plot doesn't show the noise-like-rippling-singularity, so with this, as it was noted on the comments and answers, the situation was a numerical issue. Even so, if you directly replace all the numbers by the number in \eqref{Eq. 7} the rippling appears again. Later I noted that the result of \eqref{Eq. 4} is equivalent to: So obviously again the numerical issues arises, but for a ""good"" approximation I am using on Wolfram-Alpha : and suddenly the problem is gone... it is kind of ridiculous that ""just"" for the exact numbers the problem happen. Since the regularized confluent hypergeometric function is not quite extended on every software library I find useful the following property shown by Wolfram Alpha to make approximated plots of these kind of functions, by taking an arbitrary parameter : with the Bessel function of the first kind. Unfortunately, I wasn't able to find under any of these approximations (both shown on the last image), ""as it where diverging"" (I don't know if this is the case), even when , which is counter-intuitive for me giving its similarity (the lobes should be adding even less area than a Gaussian in principle), but comparing their graphs shows that the compact-support function has lobes much more spread than the Gaussian, as is shown here . But nevertheless, the tails are decreasing faster than so I don´t know why the result is not finite (see here ). As example, if I used the other aporoximarion: Wolfram-Alpha is unable to find , but it is possible to bound it as: as can be seen here and here . And since the difference with the approximation and the main function with numerical issues is in the bounded domain , it should also be bounded, so the norm but somehow Wolfram-Alpha can't find it.","f(t)=(1-t^2)^4 [-1,\,1] g(t)= e^{-4t^2} \tag{Eq. 1}\label{Eq. 1} f(t)= (1-t^2)^4\cdot\theta(1-t^2) \cong \left(\frac{1-t^2+|1-t^2|}{2}\right)^4 \tag{Eq. 2}\label{Eq. 2} \theta(t) f(t) = 0,\,|t|\geq 1 \partial t =\{-1;\,1\} s(t)=(1-t^2)^4 s(-1)=s(1)=0 \theta(1-t^2) \lim\limits_{t \to \partial t^{\pm}} f(t) = \lim\limits_{t \to \partial t} f(t) = 0 \mathbb{R} \|f\|_\infty = 1 < \infty \|f\|_1 = \frac{256}{315} \approx 0.813 < \infty \|f\|_2^2 = \frac{65536}{109395} \approx 0.599 < \infty \begin{array}{r c l}
\frac{d}{dt}\left((1-t^2)^4\cdot\theta(1-t^2)\right) & = & \frac{d}{dt}\left((1-t^2)^4\right)\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\theta(1-t^2)\right)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\frac{1+\text{sgn}(1-t^2)}{2}\right)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^4\cdot\delta(1-t^2)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^3\cdot\underbrace{(1-t^2)\cdot\delta(1-t^2)}_{=\,0,\,\text{since}\,(1-x)\delta(1-x)=0} \\
\end{array} \Rightarrow \frac{df(t)}{dt}  = -8\,t\,(1-t^2)^3\cdot\theta(1-t^2) \tag{Eq. 3}\label{Eq. 3}  \|f'\|_\infty = \frac{1798}{343\sqrt{7}} \approx 1.9 < \infty \|f'\|_1 = 2 < \infty \|f'\|_2^2 = \frac{131072}{45045} \approx 2.9 < \infty f(t) (4) f'' f(t) \|f\|_2^2 < \infty (3) f(t) \hat{f}(w) = \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt = \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt = \frac{768 \Big(5w\,(2 w^2-21)\cos(w)+(w^4-45w^2+105) \sin(w)\Big)}{w^9}  \tag{Eq. 4}\label{Eq. 4} \|\hat{f}\|_1 \|iw\hat{f}\|_1 g(t) \hat{g}(w) = \frac{\sqrt{\pi}}{2}e^{-\frac{w^2}{16}}\tag{Eq. 5}\label{Eq. 5} w \approx 0 \hat{f}(w) \hat{f}(w) \|\hat{f}\|_\infty 10^{93} \hat{f} (5) \hat{f} w \in [-0.2,\,0.2] \hat{f}(w) \|\hat{f}\|_1 \|\hat{f}\|_{\infty} w \lim_{w\to 0} \hat{f}(w)=\frac{256}{315}\approx 0.813 w=0 1/|w| \hat{f} \hat{g} \int\limits_{-\infty}^{\infty}\hat{f}(w)\,dw = \int\limits_{-\infty}^{\infty}\hat{g}(w)\,dw = 2\pi q(t) = \left(\frac{1-t^2+|1-t^2|}{2}\right)^{\pi} \tag{Eq. 6}\label{Eq. 6} \hat{q}(w) = \int\limits_{-\infty}^{\infty} q(t)\,e^{-iwt}\,dt = \sqrt{\pi}\,\Gamma(1+\pi)_0\tilde{F}_1\left(;\,\frac{3}{2}+\pi;\,-\frac{w^2}{4}\right)  \tag{Eq. 7}\label{Eq. 7} \pi 4 \hat{f}(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4;\,-\frac{w^2}{4}\right)  \tag{Eq. 8}\label{Eq. 8} \hat{f}^*(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4.0001;\,-\frac{w^2}{4}\right)  \tag{Eq. 9}\label{Eq. 9} a \sqrt{\pi}\,\Gamma(1+a)_0\tilde{F}_1\left(;\,\frac{3}{2}+a;\,-\frac{w^2}{4}\right) = \text{sgn}(w)\sqrt{\pi}\,\left(\frac{2}{w}\right)^{a+\frac{1}{2}}\Gamma(1+a)J_{a+\frac{1}{2}}(w) \tag{Eq. 10}\label{Eq. 10} J_n(x) \|\hat{f}\|_1 \|\hat{g}\|_1=2\pi << \infty |\frac{1}{w^2}| \hat{f}^*(w) \approx \frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\tag{Eq. 11}\label{Eq. 11} \|\hat{f}^*(w)\|_1 \|\hat{f}^*(w)\|_1 < \underbrace{\int\limits_{-10}^{10}\left|\frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\right|\,dw}_{6.34416}+\underbrace{\int\limits_{10}^{\infty}\left|\frac{2}{|w^{2}|}\right|\,dw}_{0.2} <\infty [-1,\,1] \|\hat{f}(w)\|_1<\infty","['real-analysis', 'fourier-analysis', 'lp-spaces', 'upper-lower-bounds', 'finite-duration']"
88,Is the series $\sum_{n=1}^\infty\frac{1}{n}\left(\sum_{k=1}^n\frac{1}{k}\left(\frac{1}{2}\right)^{n-k}\right)$ convergent?,Is the series  convergent?,\sum_{n=1}^\infty\frac{1}{n}\left(\sum_{k=1}^n\frac{1}{k}\left(\frac{1}{2}\right)^{n-k}\right),"$$ \sum_{n = 1}^\infty\dfrac{1}{n}\left(\sum_{k = 1}^n\dfrac{1}{k}\left(\dfrac{1}{2}\right)^{n - k}\right) $$ Does the series converge? I calculate it using Matlab, and it seems that the sum converges to 2.4673. I also tried to use the ratio test to prove the convergent, but stopped at to calculate the sum $\sum_{k = 1}^n\dfrac{1}{k}(1/2)^{n - k}$ .","Does the series converge? I calculate it using Matlab, and it seems that the sum converges to 2.4673. I also tried to use the ratio test to prove the convergent, but stopped at to calculate the sum .","
\sum_{n = 1}^\infty\dfrac{1}{n}\left(\sum_{k = 1}^n\dfrac{1}{k}\left(\dfrac{1}{2}\right)^{n - k}\right)
 \sum_{k = 1}^n\dfrac{1}{k}(1/2)^{n - k}","['real-analysis', 'sequences-and-series']"
89,Beginner feedback on real analysis proof,Beginner feedback on real analysis proof,,"I am a bio student self-studying Abbott's Understanding Analysis and would love some feedback on one of my answers to an exercise. I have no experience writing proofs, and I'm used to plug-n-chug math taught by school, but I'm determined to get through this book as I find it fascinating. Thanks! Q: If $x ∈ (A ∩ B)^c$ , explain why $x ∈ A^c ∪ B^c$ . This shows that $(A ∩ B)^c ⊆ A^c ∪ B^c$ Pf: If $x \in A\cap B$ , then $x \in A,B$ and $x \in A\cup B$ . We can think of $A\cap B$ as the collection of elements in both $A$ and $B$ . The complement $(A\cap B)^c$ is therefore the set of elements not in $A$ and $B$ , elements can still originate from $A$ or $B$ , just not those in both. The set $(A\cap B)^c$ is equal to $A^c\cup B^c$ because $A^c$ is the set of elements not in A (but, again, can contain elements in $B$ ). But, if an element is in A and also in B, neither $A^c$ nor $B^c$ will contain that elements. Thus, $A^c\cup B^c$ can be thought of as the set of elements not in $A$ and $B$ , just as with $(A ∩ B)^c$ .","I am a bio student self-studying Abbott's Understanding Analysis and would love some feedback on one of my answers to an exercise. I have no experience writing proofs, and I'm used to plug-n-chug math taught by school, but I'm determined to get through this book as I find it fascinating. Thanks! Q: If , explain why . This shows that Pf: If , then and . We can think of as the collection of elements in both and . The complement is therefore the set of elements not in and , elements can still originate from or , just not those in both. The set is equal to because is the set of elements not in A (but, again, can contain elements in ). But, if an element is in A and also in B, neither nor will contain that elements. Thus, can be thought of as the set of elements not in and , just as with .","x ∈ (A ∩ B)^c x ∈ A^c ∪ B^c (A ∩ B)^c ⊆ A^c ∪ B^c x \in A\cap B x \in A,B x \in A\cup B A\cap B A B (A\cap B)^c A B A B (A\cap B)^c A^c\cup B^c A^c B A^c B^c A^c\cup B^c A B (A ∩ B)^c","['real-analysis', 'elementary-set-theory', 'advice']"
90,The definition of Absolute Continuous function,The definition of Absolute Continuous function,,"Absolute Continuous functions require a finite sequence of subintervals of the domain. Can we replace this finite sequence with a countably infinite one in this definition? Please note that an absolutely continuous function preserves null sets. To prove this result, I choose an open set G that contains a given null set N. Then eventually, I proved that the image of N has measure zero. In this proof, at some point, I chose a finite sequence from the countable sequence of intervals that covers G. Moreover, the converse is also true ie any continuous function with a bounded variation that preserves the null set is absolutely continuous. Based on my own shallow observation, I feel like we can replace this finite sequence criterion with a countably infinite one. Is this true? Could you give me an easy counter-example where my guess is false?  Thank you for your time.","Absolute Continuous functions require a finite sequence of subintervals of the domain. Can we replace this finite sequence with a countably infinite one in this definition? Please note that an absolutely continuous function preserves null sets. To prove this result, I choose an open set G that contains a given null set N. Then eventually, I proved that the image of N has measure zero. In this proof, at some point, I chose a finite sequence from the countable sequence of intervals that covers G. Moreover, the converse is also true ie any continuous function with a bounded variation that preserves the null set is absolutely continuous. Based on my own shallow observation, I feel like we can replace this finite sequence criterion with a countably infinite one. Is this true? Could you give me an easy counter-example where my guess is false?  Thank you for your time.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'absolute-continuity']"
91,Upper and Lower semicontinuity and right-left continuity.,Upper and Lower semicontinuity and right-left continuity.,,"Let $f: [0,1]\rightarrow\mathbb{R}$ be a function. Is there any relation between upper-lower semicontinuity of $f$ and right-left continuity of $f$ ? I mean, is there any implication? Thanks in advance.","Let be a function. Is there any relation between upper-lower semicontinuity of and right-left continuity of ? I mean, is there any implication? Thanks in advance.","f: [0,1]\rightarrow\mathbb{R} f f","['real-analysis', 'limits', 'upper-lower-bounds']"
92,Application of the chain rule to $3$-layers neural network,Application of the chain rule to -layers neural network,3,"Consider the differentiable functions $L^1(x,\theta^1),L^2(x^2,\theta^2),L^3(x^3,\theta^3)$ , where every $x_k,\theta^k$ are real vectors, for $k=1,2,3$ . Also define $\theta=(\theta^1,\theta^2,\theta^3)$ (and note that $x$ is not $ (x^1,x^2,x^3)$ ). What is the jacobian, with respect to $\theta$ , of $f(x,\theta)=L^3(L^2(L^1(x,\theta^1),\theta^2),\theta^3)?$ This question arose when computing the gradients in the backpropagation phase of a neural network, and I got a result that I don't think is correct (or at least it is not the one which backpropagation algorithms use). Here is my try. Using the chain rule: $Jf=JL^3 \cdot J(L^2(L^1(x,\theta^1),\theta^2),\theta^3)=JL^3 \begin{pmatrix} J_{x,\theta^1,\theta^2}L^2(L^1(x,\theta^1),\theta^2) & 0\\  0 & I \end{pmatrix}=\left ( J_{x^3}L^3\cdot J_{x,\theta^1,\theta^2}L^2(L^1(x,\theta^1),\theta^2)\middle |J_{\theta^3}L^3\right )$ Hence $Jf=\left ( J_{x^3}L^3\cdot JL^2(L^1(x,\theta^1),\theta^2)\middle |J_{\theta^3}L^3\right )$ , and by the above reasoning: $$Jf=\left ( J_{x^3}L^3\cdot \left (J_{x^2}L^2\cdot JL^1\middle | J_{\theta^2}L^2 \right )\middle |J_{\theta^3}L^3\right )=\left ( J_{x^3}L^3\cdot J_{x^2}L^2\cdot JL^1 \middle | J_{x^3}L^3 \cdot J_{\theta^2}L^2 \middle | J_{\theta^3}L^3\right )$$ I must conclude that $J_\theta f=\left ( J_{x^3}L^3\cdot J_{x^2}L^2\cdot J_{\theta^1}L^1 \middle | J_{x^3}L^3 \cdot J_{\theta^2}L^2 \middle | J_{\theta^3}L^3\right )$ : is this correct?","Consider the differentiable functions , where every are real vectors, for . Also define (and note that is not ). What is the jacobian, with respect to , of This question arose when computing the gradients in the backpropagation phase of a neural network, and I got a result that I don't think is correct (or at least it is not the one which backpropagation algorithms use). Here is my try. Using the chain rule: Hence , and by the above reasoning: I must conclude that : is this correct?","L^1(x,\theta^1),L^2(x^2,\theta^2),L^3(x^3,\theta^3) x_k,\theta^k k=1,2,3 \theta=(\theta^1,\theta^2,\theta^3) x  (x^1,x^2,x^3) \theta f(x,\theta)=L^3(L^2(L^1(x,\theta^1),\theta^2),\theta^3)? Jf=JL^3 \cdot J(L^2(L^1(x,\theta^1),\theta^2),\theta^3)=JL^3 \begin{pmatrix}
J_{x,\theta^1,\theta^2}L^2(L^1(x,\theta^1),\theta^2) & 0\\ 
0 & I
\end{pmatrix}=\left ( J_{x^3}L^3\cdot J_{x,\theta^1,\theta^2}L^2(L^1(x,\theta^1),\theta^2)\middle |J_{\theta^3}L^3\right ) Jf=\left ( J_{x^3}L^3\cdot JL^2(L^1(x,\theta^1),\theta^2)\middle |J_{\theta^3}L^3\right ) Jf=\left ( J_{x^3}L^3\cdot \left (J_{x^2}L^2\cdot JL^1\middle | J_{\theta^2}L^2 \right )\middle |J_{\theta^3}L^3\right )=\left ( J_{x^3}L^3\cdot J_{x^2}L^2\cdot JL^1 \middle | J_{x^3}L^3 \cdot J_{\theta^2}L^2 \middle | J_{\theta^3}L^3\right ) J_\theta f=\left ( J_{x^3}L^3\cdot J_{x^2}L^2\cdot J_{\theta^1}L^1 \middle | J_{x^3}L^3 \cdot J_{\theta^2}L^2 \middle | J_{\theta^3}L^3\right )","['real-analysis', 'statistics', 'multivariable-calculus', 'machine-learning', 'neural-networks']"
93,"Closed $[a,b]⊆\mathbb{R}$ is not a countable union of $≥2$ disjoint closed intervals?",Closed  is not a countable union of  disjoint closed intervals?,"[a,b]⊆\mathbb{R} ≥2",Show that in real line closed intervals cannot be written as a union of countable disjoint closed intervals. Honestly I cannot see how to show that. If it was given that about a closed sets i know some of the example and Cantor set is also a good example but i dont find any examples to show that a closed interval cannot be written as a union of countable disjoint closed intervals. Although if i consider the trivial representation by singletone sets then i can see that it is uncountable but satisfies given other properties. But i dont know any example. So i want help. Thank you.,Show that in real line closed intervals cannot be written as a union of countable disjoint closed intervals. Honestly I cannot see how to show that. If it was given that about a closed sets i know some of the example and Cantor set is also a good example but i dont find any examples to show that a closed interval cannot be written as a union of countable disjoint closed intervals. Although if i consider the trivial representation by singletone sets then i can see that it is uncountable but satisfies given other properties. But i dont know any example. So i want help. Thank you.,,"['real-analysis', 'metric-spaces', 'real-numbers']"
94,Prove $\sum_{n=0}^{\infty} \frac{\Gamma(n+(1/2))}{4^n(2n+1)\Gamma(n+1)}=\frac{\pi^{3/2}}{3}$,Prove,\sum_{n=0}^{\infty} \frac{\Gamma(n+(1/2))}{4^n(2n+1)\Gamma(n+1)}=\frac{\pi^{3/2}}{3},"Prove $$\sum_{n=0}^{\infty} \frac{\Gamma\left(n+\frac{1}{2}\right)}{4^n\left(2n+1\right)\Gamma\left(n+1\right)}=\frac{\pi^{\frac{3}{2}}}{3}$$ The original sum is multiplied by $\frac{\sqrt{\pi}}{2}$ and so it equals $\frac{\pi^2}{6}$ but I pulled the constant out because the actual series troubles me.  I dont know how to evaluate this.  I think maybe the Gammas and $4^n$ simplify and leave some constant divide by $2n+1$ which is the familiar arctan series. Wolfram can't help simplify it, just compute it.  Any help please?","Prove The original sum is multiplied by and so it equals but I pulled the constant out because the actual series troubles me.  I dont know how to evaluate this.  I think maybe the Gammas and simplify and leave some constant divide by which is the familiar arctan series. Wolfram can't help simplify it, just compute it.  Any help please?",\sum_{n=0}^{\infty} \frac{\Gamma\left(n+\frac{1}{2}\right)}{4^n\left(2n+1\right)\Gamma\left(n+1\right)}=\frac{\pi^{\frac{3}{2}}}{3} \frac{\sqrt{\pi}}{2} \frac{\pi^2}{6} 4^n 2n+1,"['real-analysis', 'calculus']"
95,"If function is differentiable at a point, is it continuous in a neighborhood?","If function is differentiable at a point, is it continuous in a neighborhood?",,"I was reading a proof for the multi-variable chain rule and in the proof the mean-value theorem was used. The use of the theorem requires that a function is continuous between two points. Hence the motivation for the question, if a function is differentiable at a point, is it continuous in some neighborhood? If not, then the multi-variable chain rule proof is fake?","I was reading a proof for the multi-variable chain rule and in the proof the mean-value theorem was used. The use of the theorem requires that a function is continuous between two points. Hence the motivation for the question, if a function is differentiable at a point, is it continuous in some neighborhood? If not, then the multi-variable chain rule proof is fake?",,['real-analysis']
96,Find $\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx$,Find,\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx,"Find $\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx$ . I think that the limit is infinity. $ke^{-kx^2}\leq ke^{-kx^2}\arctan(x)$ for $[\tan(1),\infty)$ , but by integrating we know that $\int_{0}^{\infty}ke^{-kx^2}\to \infty$ and so out sequence of original integrals diveres too. Is this correct?","Find . I think that the limit is infinity. for , but by integrating we know that and so out sequence of original integrals diveres too. Is this correct?","\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx ke^{-kx^2}\leq ke^{-kx^2}\arctan(x) [\tan(1),\infty) \int_{0}^{\infty}ke^{-kx^2}\to \infty","['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
97,Hilbert space has countable basis,Hilbert space has countable basis,,"Let $H$ be Hilbert space. I want to show that if $H$ has a countable orthonormal basis, then every orthonormal basis for $H$ must be countable. I spent almost a day, but could not solve this problem. I am trying to solve it with some kind of contradiction. I started by assuming that there exist an uncountable basis. Then I want to derive something crazy but cannot find it. Could you give me some hints or suggestions?","Let be Hilbert space. I want to show that if has a countable orthonormal basis, then every orthonormal basis for must be countable. I spent almost a day, but could not solve this problem. I am trying to solve it with some kind of contradiction. I started by assuming that there exist an uncountable basis. Then I want to derive something crazy but cannot find it. Could you give me some hints or suggestions?",H H H,"['real-analysis', 'linear-algebra', 'functional-analysis', 'hilbert-spaces']"
98,Unclear ideas in the proof of the Archimedean Principle,Unclear ideas in the proof of the Archimedean Principle,,"Here is how the principle is laid out in the text: Given real numbers $a$ and $b$ , with $a \gt 0$ , there is an integer $n \in \mathbb{N}$ such that $b \lt na$ First off what is important about finding an $n$ that satisfies $b \lt na$ It talks about the strategy behind the proof and goes on to say that any non-empty subset of integers that is bounded above has ""a largest integer"". If $k_0$ is the largest integer that satisfies $k_0a \leq b$ , then $n=(k_0+1)$ must satisfy $na \gt b$ . In order to justify this application of the Completeness axiom, we have two details. 1) Is the set $E:= \{k \space \in \mathbb{N} \space : \space ka \leq b\}$ bounded above and 2) Is $E$ non-empty? This answer depends on whether $b \lt a$ or not. What I don't understand is why is $k_0$ introduced? and why does it matter if $k_0a \leq b$ ? Where did the idea of $k_0a \leq b$ come from and why is it important? Why does $E$ being non-empty depend on whether $b \lt a$ or not? How does it follow that n = $k_0 + 1$ must satisfy $b \lt na$ ? Then the proof is laid out: If $b \lt a$ set $n=1$ . If. If $a \leq b$ , consider the set $E:= \{k \space \in \mathbb{N} \space : \space ka \leq b\}$ . $E$ is nonempty since $1 \in E$ . Let $k \in E$ . Since $a \gt 0$ it follows from the first multiplicative property that $k \leq \frac{b}{a}$ . This proves $E$ is bounded by $\frac{b}{a}$ . Thus by the completeness axiom $E$ has a finite supremum $s$ that belongs to $E$ . set $n= s+1$ . Then $n \in \mathbb{N}$ , n cannot belong to $E$ thus $ na \gt b$ . Whats the significance of if $b \lt a$ then set $n=1$ ? I thought we were dealing with the set $E$ . How is $E$ considered non-empty when $1 \in E$ , by that I mean where was the $1$ pulled from? Just the fact that we are dealing with the set $\mathbb{N}$ ? I'm confused. What justifies setting $n = k_0 + 1$ ?","Here is how the principle is laid out in the text: Given real numbers and , with , there is an integer such that First off what is important about finding an that satisfies It talks about the strategy behind the proof and goes on to say that any non-empty subset of integers that is bounded above has ""a largest integer"". If is the largest integer that satisfies , then must satisfy . In order to justify this application of the Completeness axiom, we have two details. 1) Is the set bounded above and 2) Is non-empty? This answer depends on whether or not. What I don't understand is why is introduced? and why does it matter if ? Where did the idea of come from and why is it important? Why does being non-empty depend on whether or not? How does it follow that n = must satisfy ? Then the proof is laid out: If set . If. If , consider the set . is nonempty since . Let . Since it follows from the first multiplicative property that . This proves is bounded by . Thus by the completeness axiom has a finite supremum that belongs to . set . Then , n cannot belong to thus . Whats the significance of if then set ? I thought we were dealing with the set . How is considered non-empty when , by that I mean where was the pulled from? Just the fact that we are dealing with the set ? I'm confused. What justifies setting ?",a b a \gt 0 n \in \mathbb{N} b \lt na n b \lt na k_0 k_0a \leq b n=(k_0+1) na \gt b E:= \{k \space \in \mathbb{N} \space : \space ka \leq b\} E b \lt a k_0 k_0a \leq b k_0a \leq b E b \lt a k_0 + 1 b \lt na b \lt a n=1 a \leq b E:= \{k \space \in \mathbb{N} \space : \space ka \leq b\} E 1 \in E k \in E a \gt 0 k \leq \frac{b}{a} E \frac{b}{a} E s E n= s+1 n \in \mathbb{N} E  na \gt b b \lt a n=1 E E 1 \in E 1 \mathbb{N} n = k_0 + 1,['real-analysis']
99,Generalizing a theorem from real analysis regarding strictly increasing functions,Generalizing a theorem from real analysis regarding strictly increasing functions,,"This is a question of my own asking, originating from a misinterpretation of OP's question in this thread where I  accidentally proved the following: Theorem : A continuous function $f:[a,b] \to \mathbb{R}$ that is strictly increasing on $(a,b)$ is also strictly increasing on $[a,b]$ . Proof : Assuming $f:[a,b] \to \mathbb{R}$ is a continuous function, we'll have $f\Big( (a,b) \Big) = (c,d)$ for some $c,d \in \mathbb{R}$ because the continuous image of a connected set is connected .  Note that this fact also implies $f(b) = w$ for some $w \in [c, d]$ .  Now let's suppose $f(b) \neq d$ ; i.e. suppose $w \in [c, d)$ .  Consider the preimage of the interval $U = (w- \varepsilon, \ w + \varepsilon)$ , where $0 < \varepsilon < |d-w|$ .  It would look like $f^{-1}(U) = V \cup \ \{b\}$ , where $V$ is such that $\{b\}$ is isolated.  Due to this isolated point, $f^{-1}(U)$ cannot be open.  Because $U$ is an open set and $f^{-1}(U)$ is not, $f$ cannot be continuous.  Thus, continuity forces $f(b) = d$ , just as it does $f(a) = c$ by an analogous argument. Having done this, I wondered how easy it would be to adapt the underlying strategy to prove a more general topological statement, and I arrived at this hypothesis ( wrong: see Eric Wofsey's post ): Let $X$ be a Hausdorff space, $U \subset X$ an open set, and $f: \overline{U} \to Y$ a continuous function into a locally compact Hausdorff space where $f:U \to f(U)$ is a homeomorphism.  Then $f: \overline{U} \to f(\overline{U})$ is also a homeomorphism. Some thoughts : Per this fact , I know that it suffices to prove that the restriction of $f$ to the boundary $\partial U$ of $U$ is bijective.  This is why I have chosen the ""locally compact Hausdorff"" condition: I think it might help me guarantee injectivity.  To wit, let's suppose $f(p) = f(q)$ for $p, q \in \partial U$ .  Per Hausdorffness, we can separate $p$ and $q$ with disjoint open neighboorhoods $S_p$ and $S_q$ .  Next, the image of $S_p$ will be a neighborhood of $f(p)$ , which necessarily contains an open neighborhood of $f(p)$ per local compactness.  Then the preimage of this set will have $\{q\}$ as an isolated point, so it cannot be open and thus $f$ would be discontinuous (this feels hand-wavy, as if I'm missing something and don't know what). Questions : Which bits of the above are wrong? What is the most general statement one can make here? How can I tackle surjectivity?  I have a few vague ideas that could make this easier, such as requiring the boundaries of each connected component of $U$ to be connected, but these are largely unsubstantiated hunches, and I feel trying to justify them here would detract from the post. To encourage as much discourse as possible, I'm willing to both accept an answer and award a bounty on a second for substantial replies, assuming I get more than one.  Meaty partial answers encouraged.","This is a question of my own asking, originating from a misinterpretation of OP's question in this thread where I  accidentally proved the following: Theorem : A continuous function that is strictly increasing on is also strictly increasing on . Proof : Assuming is a continuous function, we'll have for some because the continuous image of a connected set is connected .  Note that this fact also implies for some .  Now let's suppose ; i.e. suppose .  Consider the preimage of the interval , where .  It would look like , where is such that is isolated.  Due to this isolated point, cannot be open.  Because is an open set and is not, cannot be continuous.  Thus, continuity forces , just as it does by an analogous argument. Having done this, I wondered how easy it would be to adapt the underlying strategy to prove a more general topological statement, and I arrived at this hypothesis ( wrong: see Eric Wofsey's post ): Let be a Hausdorff space, an open set, and a continuous function into a locally compact Hausdorff space where is a homeomorphism.  Then is also a homeomorphism. Some thoughts : Per this fact , I know that it suffices to prove that the restriction of to the boundary of is bijective.  This is why I have chosen the ""locally compact Hausdorff"" condition: I think it might help me guarantee injectivity.  To wit, let's suppose for .  Per Hausdorffness, we can separate and with disjoint open neighboorhoods and .  Next, the image of will be a neighborhood of , which necessarily contains an open neighborhood of per local compactness.  Then the preimage of this set will have as an isolated point, so it cannot be open and thus would be discontinuous (this feels hand-wavy, as if I'm missing something and don't know what). Questions : Which bits of the above are wrong? What is the most general statement one can make here? How can I tackle surjectivity?  I have a few vague ideas that could make this easier, such as requiring the boundaries of each connected component of to be connected, but these are largely unsubstantiated hunches, and I feel trying to justify them here would detract from the post. To encourage as much discourse as possible, I'm willing to both accept an answer and award a bounty on a second for substantial replies, assuming I get more than one.  Meaty partial answers encouraged.","f:[a,b] \to \mathbb{R} (a,b) [a,b] f:[a,b] \to \mathbb{R} f\Big( (a,b) \Big) = (c,d) c,d \in \mathbb{R} f(b) = w w \in [c, d] f(b) \neq d w \in [c, d) U = (w- \varepsilon, \ w + \varepsilon) 0 < \varepsilon < |d-w| f^{-1}(U) = V \cup \ \{b\} V \{b\} f^{-1}(U) U f^{-1}(U) f f(b) = d f(a) = c X U \subset X f: \overline{U} \to Y f:U \to f(U) f: \overline{U} \to f(\overline{U}) f \partial U U f(p) = f(q) p, q \in \partial U p q S_p S_q S_p f(p) f(p) \{q\} f U","['real-analysis', 'general-topology', 'monotone-functions']"
