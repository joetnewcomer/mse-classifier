,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Graphs for which a calculus student can reasonably compute the arclength,Graphs for which a calculus student can reasonably compute the arclength,,"Given a differentiable real-valued function $f$ , the arclength of its graph on $[a,b]$ is given by $$\int_a^b\sqrt{1+\left(f'(x)\right)^2}\,\mathrm{d}x$$ For many choices of $f$ this can be a tricky integral to evaluate, especially for calculus students first learning integration. I've found a few choices of $f$ that make the computation pretty easy: Letting $f$ be linear is super easy, but then you don't even need the formula. Taking $f$ of the form $(\text{stuff})^{\frac{3}{2}}$ might work out nicely if $\text{stuff}$ is chosen carefully. Calculating it for $f(x) = \sqrt{1-x^2}$ is alright if you remember that $\int\frac{1}{x^2+1}\,\mathrm{d}x$ is $\arctan(x)+C$ . Letting $f(x) = \ln(\sec(x))$ results in $\int\sec(x)\,\mathrm{d}x$ , which classically sucks. But it looks like most choices of $f$ suggest at least a trig substitution $f'(x) \mapsto \tan(\theta)$ , and will be computationally intensive, and unreasonable to ask a student to do. Are there other examples of a function $f$ such that computing the arclength of the graph of $f$ won't be too arduous to ask a calculus student to do?","Given a differentiable real-valued function , the arclength of its graph on is given by For many choices of this can be a tricky integral to evaluate, especially for calculus students first learning integration. I've found a few choices of that make the computation pretty easy: Letting be linear is super easy, but then you don't even need the formula. Taking of the form might work out nicely if is chosen carefully. Calculating it for is alright if you remember that is . Letting results in , which classically sucks. But it looks like most choices of suggest at least a trig substitution , and will be computationally intensive, and unreasonable to ask a student to do. Are there other examples of a function such that computing the arclength of the graph of won't be too arduous to ask a calculus student to do?","f [a,b] \int_a^b\sqrt{1+\left(f'(x)\right)^2}\,\mathrm{d}x f f f f (\text{stuff})^{\frac{3}{2}} \text{stuff} f(x) = \sqrt{1-x^2} \int\frac{1}{x^2+1}\,\mathrm{d}x \arctan(x)+C f(x) = \ln(\sec(x)) \int\sec(x)\,\mathrm{d}x f f'(x) \mapsto \tan(\theta) f f","['calculus', 'integration', 'education', 'big-list', 'arc-length']"
1,Area covered by a constant length segment rotating around the center of a square.,Area covered by a constant length segment rotating around the center of a square.,,"This is an idea I have had in my head for years and years and I would like to know the answer, and also I would like to know if it's somehow relevant to anything or useless. I describe my thoughts with the following image: What would the area of the ""red almost half circle"" on top of the third square be, assuming you rotate the hypotenuse of a square around it's center limiting its movement so it cannot pass through the bottom of the square. My guess would be: $$\ \frac{\left(\pi*(h/2)^2 - a^2\right)}{2}$$ And also, does this have any meaning? Have I been wandering around thinking about complete nonsense for so many years?","This is an idea I have had in my head for years and years and I would like to know the answer, and also I would like to know if it's somehow relevant to anything or useless. I describe my thoughts with the following image: What would the area of the ""red almost half circle"" on top of the third square be, assuming you rotate the hypotenuse of a square around it's center limiting its movement so it cannot pass through the bottom of the square. My guess would be: $$\ \frac{\left(\pi*(h/2)^2 - a^2\right)}{2}$$ And also, does this have any meaning? Have I been wandering around thinking about complete nonsense for so many years?",,['calculus']
2,Why is the absolute value function not differentiable at $x=0$?,Why is the absolute value function not differentiable at ?,x=0,They say that the right and left limits do not approach the same value hence it does not satisfy the definition of derivative.  But what does it mean verbally in terms of rate of change?,They say that the right and left limits do not approach the same value hence it does not satisfy the definition of derivative.  But what does it mean verbally in terms of rate of change?,,"['calculus', 'absolute-value']"
3,Closed Form for $~\int_0^1\frac{\text{arctanh }x}{\tan\left(\frac\pi2~x\right)}~dx$,Closed Form for,~\int_0^1\frac{\text{arctanh }x}{\tan\left(\frac\pi2~x\right)}~dx,"Does $$~\displaystyle{\int}_0^1\frac{\text{arctanh }x}{\tan\left(\dfrac\pi2~x\right)}~dx~\simeq~0.4883854771179872995286585433480\ldots~$$ possess a closed form expression ? This recent post, in conjunction with my age-old interest in Gudermannian functions , have inspired me to ask this question. The reason I suspect that such a closed form might possibly exist is because the integration interval is “meaningful” for both functions used in the integrand. However, none of the various approaches that I can think of seem to be of any help. Perhaps I'm missing something ?","Does $$~\displaystyle{\int}_0^1\frac{\text{arctanh }x}{\tan\left(\dfrac\pi2~x\right)}~dx~\simeq~0.4883854771179872995286585433480\ldots~$$ possess a closed form expression ? This recent post, in conjunction with my age-old interest in Gudermannian functions , have inspired me to ask this question. The reason I suspect that such a closed form might possibly exist is because the integration interval is “meaningful” for both functions used in the integrand. However, none of the various approaches that I can think of seem to be of any help. Perhaps I'm missing something ?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
4,A circle rolls along a parabola,A circle rolls along a parabola,,"I'm thinking about a circle rolling along a parabola. Would this be a parametric representation? $(t + A\sin (Bt)  , Ct^2 + A\cos (Bt) )$ A gives us the radius of the circle, B changes the frequency of the rotations, C, of course, varies the parabola.  Now, if I want the circle to ""match up"" with the parabola as if they were both made of non-stretchy rope, what should I choose for B? My first guess is 1. But, the the arc length of a parabola from 0 to 1 is much less than the length from 1 to 2. And, as I examine the graphs, it seems like I might need to vary B in order to get the graph that I want. Take a look: This makes me think that the graph my equation produces will always be wrong no matter what constants I choose. It should look like a cycloid: But bent to fit on a parabola. [I started this becuase I wanted to know if such a curve could be self-intersecting. (I think yes.) When I was a child my mom asked me to draw what would happen if a circle rolled along the tray of the blackboard with a point on the rim tracing a line ... like most young people, I drew self-intersecting loops and my young mind was amazed to see that they did not intersect!] So, other than checking to see if this is even going in the right direction, I would like to know if there is a point where the curve shown (or any curve in the family I described) is most like a cycloid-- Thanks. ""It would be really really hard to tell"" is a totally acceptable answer, though it's my current answer, and I wonder if the folks here can make it a little better.","I'm thinking about a circle rolling along a parabola. Would this be a parametric representation? $(t + A\sin (Bt)  , Ct^2 + A\cos (Bt) )$ A gives us the radius of the circle, B changes the frequency of the rotations, C, of course, varies the parabola.  Now, if I want the circle to ""match up"" with the parabola as if they were both made of non-stretchy rope, what should I choose for B? My first guess is 1. But, the the arc length of a parabola from 0 to 1 is much less than the length from 1 to 2. And, as I examine the graphs, it seems like I might need to vary B in order to get the graph that I want. Take a look: This makes me think that the graph my equation produces will always be wrong no matter what constants I choose. It should look like a cycloid: But bent to fit on a parabola. [I started this becuase I wanted to know if such a curve could be self-intersecting. (I think yes.) When I was a child my mom asked me to draw what would happen if a circle rolled along the tray of the blackboard with a point on the rim tracing a line ... like most young people, I drew self-intersecting loops and my young mind was amazed to see that they did not intersect!] So, other than checking to see if this is even going in the right direction, I would like to know if there is a point where the curve shown (or any curve in the family I described) is most like a cycloid-- Thanks. ""It would be really really hard to tell"" is a totally acceptable answer, though it's my current answer, and I wonder if the folks here can make it a little better.",,"['calculus', 'geometry', 'plane-curves']"
5,Is there a step by step checklist to check if a multivariable limit exists and find its value?,Is there a step by step checklist to check if a multivariable limit exists and find its value?,,"Do we rely on certain intuition or is there an unofficial general crude checklist I should follow? I had a friend telling me that if the sum of the powers on the numerator is smaller then the denominator, there is a higher chance that it may not exists. And if the sum of powers on the numerator is higher then the denominator, most likely, it exists. Also if there is a sin or cos or e-constant, it most likely exists. How can I know what to do at the first glance given so little time exists for me in exam to ponder? If I spend all my time on figuring out a two-path test when the limit exists, that would be a huge disaster. Is this one of those cases where practice makes perfect? Example: $$\lim_{(x,y)\to(0,0)}\frac{(\sin^2x)(e^y-1)}{x^2+3y^2}$$ Please give me a hint and where do you get the hint. Example: $$\lim_{(x,y)\to(0,0)}\exp\left(-\frac{x^2+y^2}{4x^4+y^6}\right)$$ I need a hint for this too. Common methods I have learnt for reference: Two-Path test, Polar Coordinates, Spherical Coordinates, Mean Value Theorem using inequalities.","Do we rely on certain intuition or is there an unofficial general crude checklist I should follow? I had a friend telling me that if the sum of the powers on the numerator is smaller then the denominator, there is a higher chance that it may not exists. And if the sum of powers on the numerator is higher then the denominator, most likely, it exists. Also if there is a sin or cos or e-constant, it most likely exists. How can I know what to do at the first glance given so little time exists for me in exam to ponder? If I spend all my time on figuring out a two-path test when the limit exists, that would be a huge disaster. Is this one of those cases where practice makes perfect? Example: Please give me a hint and where do you get the hint. Example: I need a hint for this too. Common methods I have learnt for reference: Two-Path test, Polar Coordinates, Spherical Coordinates, Mean Value Theorem using inequalities.","\lim_{(x,y)\to(0,0)}\frac{(\sin^2x)(e^y-1)}{x^2+3y^2} \lim_{(x,y)\to(0,0)}\exp\left(-\frac{x^2+y^2}{4x^4+y^6}\right)","['calculus', 'limits', 'multivariable-calculus', 'soft-question']"
6,Intuitive understanding of the derivatives of $\sin x$ and $\cos x$,Intuitive understanding of the derivatives of  and,\sin x \cos x,"One of the first things ever taught in a differential calculus class: The derivative of $\sin x$ is $\cos x$. The derivative of $\cos x$ is $-\sin x$. This leads to a rather neat (and convenient?) chain of derivatives: sin(x) cos(x) -sin(x) -cos(x) sin(x) ... An analysis of the shape of their graphs confirms some points; for example, when $\sin x$ is at a maximum, $\cos x$ is zero and moving downwards; when $\cos x$  is at a maximum, $\sin x$ is zero and moving upwards.  But these ""matching points"" only work for multiples of $\pi/4$. Let us move back towards the original definition(s) of sine and cosine: At the most basic level, $\sin x$ is defined as -- for a right triangle with internal angle $x$ -- the length of the side opposite of the angle divided by the hypotenuse of the triangle. To generalize this to the domain of all real numbers, $\sin x$ was then defined as the Y-coordinate of a point on the unit circle that is an angle $x$ from the positive X-axis. The definition of $\cos x$ was then made the same way, but with adj/hyp and the X-coordinate, as we all know. Is there anything about this basic definition that allows someone to look at these definitions, alone, and think, ""Hey, the derivative of the sine function with respect to angle is the cosine function!"" That is, from the unit circle definition alone .  Or, even more amazingly, the right triangle definition alone .  Ignoring graphical analysis of their plot. In essence, I am asking, essentially, ""Intuitively why is the derivative of the sine the cosine?""","One of the first things ever taught in a differential calculus class: The derivative of $\sin x$ is $\cos x$. The derivative of $\cos x$ is $-\sin x$. This leads to a rather neat (and convenient?) chain of derivatives: sin(x) cos(x) -sin(x) -cos(x) sin(x) ... An analysis of the shape of their graphs confirms some points; for example, when $\sin x$ is at a maximum, $\cos x$ is zero and moving downwards; when $\cos x$  is at a maximum, $\sin x$ is zero and moving upwards.  But these ""matching points"" only work for multiples of $\pi/4$. Let us move back towards the original definition(s) of sine and cosine: At the most basic level, $\sin x$ is defined as -- for a right triangle with internal angle $x$ -- the length of the side opposite of the angle divided by the hypotenuse of the triangle. To generalize this to the domain of all real numbers, $\sin x$ was then defined as the Y-coordinate of a point on the unit circle that is an angle $x$ from the positive X-axis. The definition of $\cos x$ was then made the same way, but with adj/hyp and the X-coordinate, as we all know. Is there anything about this basic definition that allows someone to look at these definitions, alone, and think, ""Hey, the derivative of the sine function with respect to angle is the cosine function!"" That is, from the unit circle definition alone .  Or, even more amazingly, the right triangle definition alone .  Ignoring graphical analysis of their plot. In essence, I am asking, essentially, ""Intuitively why is the derivative of the sine the cosine?""",,"['calculus', 'trigonometry']"
7,Areas versus volumes of revolution: why does the area require approximation by a cone?,Areas versus volumes of revolution: why does the area require approximation by a cone?,,"Suppose we rotate the graph of $y = f(x)$ about the $x$-axis from $a$ to $b$.  Then (using the disk method) the volume is $$\int_a^b \pi f(x)^2 dx$$ since we approximate a little piece as a cylinder.  However, if we want to find the surface area, then we approximate it as part of a cone and the formula is $$\int_a^b 2\pi f(x)\sqrt{1+f'(x)^2} dx.$$  But if approximated it by a circle with thickness $dx$ we would get $$\int_a^b 2\pi f(x) dx.$$ So my question is how come for volume we can make the cruder approximation of a disk but for surface area we can't.","Suppose we rotate the graph of $y = f(x)$ about the $x$-axis from $a$ to $b$.  Then (using the disk method) the volume is $$\int_a^b \pi f(x)^2 dx$$ since we approximate a little piece as a cylinder.  However, if we want to find the surface area, then we approximate it as part of a cone and the formula is $$\int_a^b 2\pi f(x)\sqrt{1+f'(x)^2} dx.$$  But if approximated it by a circle with thickness $dx$ we would get $$\int_a^b 2\pi f(x) dx.$$ So my question is how come for volume we can make the cruder approximation of a disk but for surface area we can't.",,"['calculus', 'integration', 'volume', 'area']"
8,Is it possible to simplify $\frac{\Gamma\left(\frac{1}{10}\right)}{\Gamma\left(\frac{2}{15}\right)\ \Gamma\left(\frac{7}{15}\right)}$?,Is it possible to simplify ?,\frac{\Gamma\left(\frac{1}{10}\right)}{\Gamma\left(\frac{2}{15}\right)\ \Gamma\left(\frac{7}{15}\right)},Is it possible to simplify this expression? $$\frac{\displaystyle\Gamma\left(\frac{1}{10}\right)}{\displaystyle\Gamma\left(\frac{2}{15}\right)\ \Gamma\left(\frac{7}{15}\right)}$$ Is there a systematic way to check ratios of Gamma-functions like this for simplification possibility?,Is it possible to simplify this expression? $$\frac{\displaystyle\Gamma\left(\frac{1}{10}\right)}{\displaystyle\Gamma\left(\frac{2}{15}\right)\ \Gamma\left(\frac{7}{15}\right)}$$ Is there a systematic way to check ratios of Gamma-functions like this for simplification possibility?,,"['calculus', 'special-functions', 'closed-form', 'gamma-function']"
9,"Integral $\int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{(r-1)x^{2} + sx + 1}{(r-1)x^{2} - sx + 1} \right) \, \mathrm dx$",Integral,"\int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{(r-1)x^{2} + sx + 1}{(r-1)x^{2} - sx + 1} \right) \, \mathrm dx","Regarding this problem , I conjectured that $$ I(r, s) = \int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{(r-1)x^{2} + sx + 1}{(r-1)x^{2} - sx + 1} \right) \, \mathrm dx = 4 \pi \operatorname{arccot} \sqrt{ \frac{2r + 2\sqrt{r^{2} - s^{2}}}{s^{2}} - 1}. $$ Though we may try the same technique as in the previous problem, now I'm curious if this generality leads us to a different (and possible a more elegant) proof. Indeed, I observed that $I(r, 0) = 0$ and $$\frac{\partial I}{\partial s}(r, s) = \int_{0}^{\infty} \left\{ \frac{2\sqrt{y}}{(r-s)y^{2} + 2(2-r)y + (r+s)}+\frac{2\sqrt{y}}{(r+s)y^{2}+ 2(2-r)y + (r-s)} \right\} \,\mathrm dy, $$ which can be evaluated using standard contour integration technique. But simplifying the residue and integrating them seems still daunting. EDIT. By applying a series of change of variables, I noticed that the problem is equivalent to prove that $$ \tilde{I}(\alpha, s) := \int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{ 1 + 2sx \sin\alpha + (s^{2} - \cos^{2}\alpha) x^{2}}{ 1 - 2sx \sin\alpha + (s^{2} - \cos^{2}\alpha) x^{2}} \right) \, \mathrm dx = 4\pi \alpha $$ for $-\frac{\pi}{2} < \alpha < \frac{\pi}{2}$ and $s > 1$. (This is equivalent to the condition that the expression inside the logarithm is positive for all $x \in \Bbb{R}$.) Another simple observation. once you prove that $\tilde{I}(\alpha, s)$ does not depend on the variable $s$ for $s > 1$, then by suitable limiting process it follows that $$ \tilde{I}(\alpha, s) = \int_{-\infty}^{\infty} \log \left( \frac{ 1 + 2x \sin\alpha + x^{2}}{ 1 - 2x \sin\alpha + x^{2}} \right) \, \frac{\mathrm dx}{x}, $$ which (I guess) can be calculated by hand. The following graph may also help us understand the behavior of this integral.","Regarding this problem , I conjectured that $$ I(r, s) = \int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{(r-1)x^{2} + sx + 1}{(r-1)x^{2} - sx + 1} \right) \, \mathrm dx = 4 \pi \operatorname{arccot} \sqrt{ \frac{2r + 2\sqrt{r^{2} - s^{2}}}{s^{2}} - 1}. $$ Though we may try the same technique as in the previous problem, now I'm curious if this generality leads us to a different (and possible a more elegant) proof. Indeed, I observed that $I(r, 0) = 0$ and $$\frac{\partial I}{\partial s}(r, s) = \int_{0}^{\infty} \left\{ \frac{2\sqrt{y}}{(r-s)y^{2} + 2(2-r)y + (r+s)}+\frac{2\sqrt{y}}{(r+s)y^{2}+ 2(2-r)y + (r-s)} \right\} \,\mathrm dy, $$ which can be evaluated using standard contour integration technique. But simplifying the residue and integrating them seems still daunting. EDIT. By applying a series of change of variables, I noticed that the problem is equivalent to prove that $$ \tilde{I}(\alpha, s) := \int_{-1}^{1} \frac{1}{x}\sqrt{\frac{1+x}{1-x}} \log \left( \frac{ 1 + 2sx \sin\alpha + (s^{2} - \cos^{2}\alpha) x^{2}}{ 1 - 2sx \sin\alpha + (s^{2} - \cos^{2}\alpha) x^{2}} \right) \, \mathrm dx = 4\pi \alpha $$ for $-\frac{\pi}{2} < \alpha < \frac{\pi}{2}$ and $s > 1$. (This is equivalent to the condition that the expression inside the logarithm is positive for all $x \in \Bbb{R}$.) Another simple observation. once you prove that $\tilde{I}(\alpha, s)$ does not depend on the variable $s$ for $s > 1$, then by suitable limiting process it follows that $$ \tilde{I}(\alpha, s) = \int_{-\infty}^{\infty} \log \left( \frac{ 1 + 2x \sin\alpha + x^{2}}{ 1 - 2x \sin\alpha + x^{2}} \right) \, \frac{\mathrm dx}{x}, $$ which (I guess) can be calculated by hand. The following graph may also help us understand the behavior of this integral.",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
10,How to prove that $\log(x)<x$ when $x>1$? [duplicate],How to prove that  when ? [duplicate],\log(x)<x x>1,"This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 6 months ago . It's very basic but I'm having trouble to find a way to prove this inequality $\log(x)<x$ when $x>1$ ( $\log(x)$ is the natural logarithm) I can think about the two graphs but I can't find another way to prove it, and, besides that, I don't understand why should it not hold if $x<1$ Can anyone help me? Thanks in advance.","This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 6 months ago . It's very basic but I'm having trouble to find a way to prove this inequality when ( is the natural logarithm) I can think about the two graphs but I can't find another way to prove it, and, besides that, I don't understand why should it not hold if Can anyone help me? Thanks in advance.",\log(x)<x x>1 \log(x) x<1,"['calculus', 'inequality', 'logarithms']"
11,A very odd resolution to an integral equation,A very odd resolution to an integral equation,,"Here is something I've found on the internet $$\begin{aligned} f-\int f&=1\\ \left(1-\int\right)f&=1\\ f&=\left(\frac1{1-\int}\right)1\\ &=\left(1+\int+\int\int+\dots\right)1\\ &=1+\int1+\int\int1+\dots\\ &= 1+x+\frac{x^2}2+\dots\\ &= e^x \end{aligned}$$ At first I interpreted this as a joke, but on closer inspection I'm not so sure... The first thing I checked was the solution, and fairly enough, $e^x$ satisfies the initial equation. It is not the only solution, though: $\lambda e^x$ works jut as well for any $\lambda\in\mathbb R$ . I'm not really familiar with integral equations; I don't know if the solution space is a vector space. Maybe it is and our goal is to find a basis for it. If that is the case, then $e^x$ may actually be the expected result. That being settled, I started looking at the development. Writing $\displaystyle f-\int f$ as $\displaystyle \left(1-\int\right)f$ is completely fine, its just operational calculus. The ""division of both sides"" by $\displaystyle\left(1-\int\right)$ must actually mean applying its inverse $\displaystyle\left(\frac1{1-\int}\right)$ on both sides, right? This seems alright, but its not obvious at all to me why should this operator be invertible. In fact, the existence of multiple solutions suggests otherwise... The most ridicule step surely is writing $\displaystyle \frac1{1-\int}=1+\int+\int\int+\dots$ . If this is true, it settles my problem with the previous step. Is it, though? I can see how the infinite integral summation may be well-defined, but no further than that. I know it is referencing the formal power series equality $\displaystyle\frac1{1-X} = 1+X+X^2+\dots$ , but I don't know if the same proof applies since it requires distributiveness. Finally, there is an infinite amount of problematic constants being vanished into nonexistence at $\displaystyle 1+\int1+\int\int1+\dots$ , a single of which would be enough to destroy the solution: $e^x+1$ does not satisfy the equation. My guess is that this is really a joke, but I want to know just how much of it is true. Where does the problems arise and how big are they? Is there anything salvageable here?","Here is something I've found on the internet At first I interpreted this as a joke, but on closer inspection I'm not so sure... The first thing I checked was the solution, and fairly enough, satisfies the initial equation. It is not the only solution, though: works jut as well for any . I'm not really familiar with integral equations; I don't know if the solution space is a vector space. Maybe it is and our goal is to find a basis for it. If that is the case, then may actually be the expected result. That being settled, I started looking at the development. Writing as is completely fine, its just operational calculus. The ""division of both sides"" by must actually mean applying its inverse on both sides, right? This seems alright, but its not obvious at all to me why should this operator be invertible. In fact, the existence of multiple solutions suggests otherwise... The most ridicule step surely is writing . If this is true, it settles my problem with the previous step. Is it, though? I can see how the infinite integral summation may be well-defined, but no further than that. I know it is referencing the formal power series equality , but I don't know if the same proof applies since it requires distributiveness. Finally, there is an infinite amount of problematic constants being vanished into nonexistence at , a single of which would be enough to destroy the solution: does not satisfy the equation. My guess is that this is really a joke, but I want to know just how much of it is true. Where does the problems arise and how big are they? Is there anything salvageable here?","\begin{aligned}
f-\int f&=1\\
\left(1-\int\right)f&=1\\
f&=\left(\frac1{1-\int}\right)1\\
&=\left(1+\int+\int\int+\dots\right)1\\
&=1+\int1+\int\int1+\dots\\
&= 1+x+\frac{x^2}2+\dots\\
&= e^x
\end{aligned} e^x \lambda e^x \lambda\in\mathbb R e^x \displaystyle f-\int f \displaystyle \left(1-\int\right)f \displaystyle\left(1-\int\right) \displaystyle\left(\frac1{1-\int}\right) \displaystyle \frac1{1-\int}=1+\int+\int\int+\dots \displaystyle\frac1{1-X} = 1+X+X^2+\dots \displaystyle 1+\int1+\int\int1+\dots e^x+1","['calculus', 'functional-analysis', 'solution-verification', 'integral-equations']"
12,A really complicated calculus book,A really complicated calculus book,,"I've been studying math as a hobby, just for fun for years, and I had my goal to understand nearly every good undergraduate textbook and I think, I finally reached it. So now I need an another goal. I've just found a very nice book /S. Ramanan – Global Calculus/ from the ""Graduate Studies in Mathematics"" series and it looks nearly awesome: Sheaves and presheaves Differential manifolds Lie groups Differential operators Tensor fields Sheaf cohomology Linear connections Complex manifolds Ricci curvature tensor Elliptic operators But it's only 316 pages and it seemed not very fundamental and detailed for me (but yes, it's still great). So here's my question: what huge complicated calculus textbooks like this one do you know that I should aim to understand? The Big Creepy Books, you know :) I'm very interested in algebraic and differential geometry, general and algebraic topology, Lie groups and algebras, pseudo- and differential operators. I don't know very much about all of this yet but I'm trying so hard to do, it's so exciting! ;) I've already covered: Algebra: Chapter 0 (Graduate Studies in Mathematics) by Paolo Aluffi A Course in Algebra (Graduate Studies in Mathematics, Vol. 56) by E. B. Vinberg Linear Algebra and Geometry (Algebra, Logic and Applications) by P. K. Suetin, Alexandra I. Kostrikin and Yu I Manin Topology from the Differentiable Viewpoint by John Willard Milnor Topology and Geometry for Physicists by Charles Nash and Siddhartha Sen Mathematical Analysis I and II by V. A. Zorich and R. Cooke Complex Analysis by Serge Lang Ordinary Differential Equations by Vladimir I. Arnold and R. Cooke Differential Geometry, Lie Groups, and Symmetric Spaces (Graduate Studies in Mathematics) by Sigurdur Helgason So I'm looking for something like Ramanan's book, but maybe more detailed and fundamental.","I've been studying math as a hobby, just for fun for years, and I had my goal to understand nearly every good undergraduate textbook and I think, I finally reached it. So now I need an another goal. I've just found a very nice book /S. Ramanan – Global Calculus/ from the ""Graduate Studies in Mathematics"" series and it looks nearly awesome: Sheaves and presheaves Differential manifolds Lie groups Differential operators Tensor fields Sheaf cohomology Linear connections Complex manifolds Ricci curvature tensor Elliptic operators But it's only 316 pages and it seemed not very fundamental and detailed for me (but yes, it's still great). So here's my question: what huge complicated calculus textbooks like this one do you know that I should aim to understand? The Big Creepy Books, you know :) I'm very interested in algebraic and differential geometry, general and algebraic topology, Lie groups and algebras, pseudo- and differential operators. I don't know very much about all of this yet but I'm trying so hard to do, it's so exciting! ;) I've already covered: Algebra: Chapter 0 (Graduate Studies in Mathematics) by Paolo Aluffi A Course in Algebra (Graduate Studies in Mathematics, Vol. 56) by E. B. Vinberg Linear Algebra and Geometry (Algebra, Logic and Applications) by P. K. Suetin, Alexandra I. Kostrikin and Yu I Manin Topology from the Differentiable Viewpoint by John Willard Milnor Topology and Geometry for Physicists by Charles Nash and Siddhartha Sen Mathematical Analysis I and II by V. A. Zorich and R. Cooke Complex Analysis by Serge Lang Ordinary Differential Equations by Vladimir I. Arnold and R. Cooke Differential Geometry, Lie Groups, and Symmetric Spaces (Graduate Studies in Mathematics) by Sigurdur Helgason So I'm looking for something like Ramanan's book, but maybe more detailed and fundamental.",,"['calculus', 'reference-request', 'differential-geometry', 'algebraic-topology']"
13,What is the importance of Calculus in today's Mathematics?,What is the importance of Calculus in today's Mathematics?,,"For engineering (e. g. electrical engineering) and physics, Calculus is important. But for a future mathematician, is the classical approach to Calculus still important? What is normally taught, as a minimum, in most Universities worldwide? Added the [calculus] and the [multivariable-calculus] tags . Edit: Hoping it is useful, I transcribe three comments of mine (to this question): I had [have] in mind for instance Tom Apostol's books, although learning differentiation before integration. (in response to Qiaochu Yuan's ""What is the classical approach to calculus?"") Elementary Calculus, continuous functions, functions of several variables, partial differentiation, implicit-functions, vectors and vector fields, multiple integrals, infinite series, uniform convergence, power series, Fourier series and integrals, etc. (in response to a comment by Geoff Robinson). I had [have] in mind calculus for math students, although I am a retired electrical engineer. (in response to Andy's comment ""Are you talking about what is usually taught to engineers and physicists, or also about a calculus curriculum for math majors? "") I decided to make this question a CW (see this meta question ).","For engineering (e. g. electrical engineering) and physics, Calculus is important. But for a future mathematician, is the classical approach to Calculus still important? What is normally taught, as a minimum, in most Universities worldwide? Added the [calculus] and the [multivariable-calculus] tags . Edit: Hoping it is useful, I transcribe three comments of mine (to this question): I had [have] in mind for instance Tom Apostol's books, although learning differentiation before integration. (in response to Qiaochu Yuan's ""What is the classical approach to calculus?"") Elementary Calculus, continuous functions, functions of several variables, partial differentiation, implicit-functions, vectors and vector fields, multiple integrals, infinite series, uniform convergence, power series, Fourier series and integrals, etc. (in response to a comment by Geoff Robinson). I had [have] in mind calculus for math students, although I am a retired electrical engineer. (in response to Andy's comment ""Are you talking about what is usually taught to engineers and physicists, or also about a calculus curriculum for math majors? "") I decided to make this question a CW (see this meta question ).",,"['calculus', 'multivariable-calculus', 'soft-question', 'education', 'math-history']"
14,How the product of two integrals is iterated integral? $\int\cdot \int = \iint$,How the product of two integrals is iterated integral?,\int\cdot \int = \iint,"Let $\large{I} = \Large \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{y^2}{2}}\  dy$. Then, my textbook says, $$\tag{1} I^2 = \left( \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{y^2}{2}}\  dy\ \right) \left( \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{x^2}{2}}\  dx\ \right) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} e^{-(y^2 +x^2)/2}\ dy\ dx $$ I am not seeing how we reach the rightmost, iterated integral, and I do not remember the calculus how or why this works. Why can we restate the product of two integrals, each having funcions of  different variable, as an iterated integral including both variables? Let $\large{I} = \Large \int\limits_{-\infty}^{\infty} \normalsize f_y(y)\  dy$ for any continuous, differentiable $ f_y: \mathbb{R} \to \mathbb{R}$ Now, does (2) generally hold, $\forall x, y \in \mathbb{R}$ ? $$\tag{2} I^2 = \left( \int\limits_{-\infty}^{\infty} \normalsize f_y(y)\  dy\ \right) \left( \int\limits_{-\infty}^{\infty} \normalsize f_x(x)\  dx\ \right) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} f_y(y)f_x(x) dy\ dx $$ or is result(1) specific to the particular function $e^{-(y^2)/2}$? Please show the work or reasoning behind how and why this transformation is valid.","Let $\large{I} = \Large \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{y^2}{2}}\  dy$. Then, my textbook says, $$\tag{1} I^2 = \left( \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{y^2}{2}}\  dy\ \right) \left( \int\limits_{-\infty}^{\infty} \normalsize e^{\small -\frac{x^2}{2}}\  dx\ \right) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} e^{-(y^2 +x^2)/2}\ dy\ dx $$ I am not seeing how we reach the rightmost, iterated integral, and I do not remember the calculus how or why this works. Why can we restate the product of two integrals, each having funcions of  different variable, as an iterated integral including both variables? Let $\large{I} = \Large \int\limits_{-\infty}^{\infty} \normalsize f_y(y)\  dy$ for any continuous, differentiable $ f_y: \mathbb{R} \to \mathbb{R}$ Now, does (2) generally hold, $\forall x, y \in \mathbb{R}$ ? $$\tag{2} I^2 = \left( \int\limits_{-\infty}^{\infty} \normalsize f_y(y)\  dy\ \right) \left( \int\limits_{-\infty}^{\infty} \normalsize f_x(x)\  dx\ \right) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} f_y(y)f_x(x) dy\ dx $$ or is result(1) specific to the particular function $e^{-(y^2)/2}$? Please show the work or reasoning behind how and why this transformation is valid.",,"['calculus', 'integration']"
15,when can we interchange integration and differentiation,when can we interchange integration and differentiation,,"Let $f$ be a Riemann Integrable function over $\mathbb{R}^2$. When can we do this? $$\frac{\partial}{\partial\theta}\int_{a}^{b}f(x,\theta)dx=\int_{a}^{b}\frac{\partial}{\partial\theta}f(x,\theta)dx$$ (Here, $a$ and $b$ are not a function of $\theta$.) In the problem, which I am solving recently, are like this: $f_{\theta}(x)$, here $\theta$ is constant and $\theta\in\mathbb{R}$ (usually). For example $f_{\theta}(x)=x^2\theta$. So, I am blindly interchanging integration and differentiation because of continuity over $\theta$. But I want to know little bit more. Also, what happens if $a$ and $b$ are function of $\theta$? Thanks.","Let $f$ be a Riemann Integrable function over $\mathbb{R}^2$. When can we do this? $$\frac{\partial}{\partial\theta}\int_{a}^{b}f(x,\theta)dx=\int_{a}^{b}\frac{\partial}{\partial\theta}f(x,\theta)dx$$ (Here, $a$ and $b$ are not a function of $\theta$.) In the problem, which I am solving recently, are like this: $f_{\theta}(x)$, here $\theta$ is constant and $\theta\in\mathbb{R}$ (usually). For example $f_{\theta}(x)=x^2\theta$. So, I am blindly interchanging integration and differentiation because of continuity over $\theta$. But I want to know little bit more. Also, what happens if $a$ and $b$ are function of $\theta$? Thanks.",,"['calculus', 'integration', 'derivatives', 'partial-derivative']"
16,"Why and How do certain manipulations in indefinite integrals ""just work""?","Why and How do certain manipulations in indefinite integrals ""just work""?",,"I am going to take a very simple example to elaborate my question. When we integrate $\sec (x)\,dx$ we divide and multiply by $\sec (x) + \tan (x)$ . $$\int \sec(x)\,dx = \int \sec (x) \left[{\sec (x) + \tan (x) \over \sec (x) + \tan (x)}\right]\, dx$$ I am just solving from here. $$\int {\sec^2(x) + \sec(x)\tan(x) \over \sec(x) + \tan(x)} \, dx $$ Then we let $\sec(x) + \tan(x) = u$ $$\implies du = (\sec^2(x) + \sec(x)\tan(x))\,dx$$ $$\implies \int {du \over u}$$ $$= \ln{\left|\sec(x) + \tan(x)\right|} + c$$ Now coming to my questions. Why do we HAVE to make that manipulation of multiplying $\sec(x) + \tan(x)$ . Like I know its to get the answer...but why does it work so well? How to even think like that? Like ""if I multiply $\sec(x) + \tan(x)$ in the numerator and denominator then I'll be able to solve this very easily."" What in that integral gives one direction to think of such a manipulation ?","I am going to take a very simple example to elaborate my question. When we integrate we divide and multiply by . I am just solving from here. Then we let Now coming to my questions. Why do we HAVE to make that manipulation of multiplying . Like I know its to get the answer...but why does it work so well? How to even think like that? Like ""if I multiply in the numerator and denominator then I'll be able to solve this very easily."" What in that integral gives one direction to think of such a manipulation ?","\sec (x)\,dx \sec (x) + \tan (x) \int \sec(x)\,dx = \int \sec (x) \left[{\sec (x) + \tan (x) \over \sec (x) + \tan (x)}\right]\, dx \int {\sec^2(x) + \sec(x)\tan(x) \over \sec(x) + \tan(x)} \, dx  \sec(x) + \tan(x) = u \implies du = (\sec^2(x) + \sec(x)\tan(x))\,dx \implies \int {du \over u} = \ln{\left|\sec(x) + \tan(x)\right|} + c \sec(x) + \tan(x) \sec(x) + \tan(x)","['calculus', 'integration', 'indefinite-integrals']"
17,Is there a function whose antiderivative can be found but whose derivative cannot?,Is there a function whose antiderivative can be found but whose derivative cannot?,,"Does a function, $f(x)$, exist such that $\int f(x) dx $ can be found but $f' (x)$ cannot be found in terms of elementary functions. For example, if $f(x)=e^{x^2}$, then the derivative is easily calculated by using the chain rule. However, there does not exist an anti-derivative in terms of elementary functions. Does a function exist with the opposite property?","Does a function, $f(x)$, exist such that $\int f(x) dx $ can be found but $f' (x)$ cannot be found in terms of elementary functions. For example, if $f(x)=e^{x^2}$, then the derivative is easily calculated by using the chain rule. However, there does not exist an anti-derivative in terms of elementary functions. Does a function exist with the opposite property?",,"['calculus', 'integration', 'functions', 'derivatives', 'special-functions']"
18,What is $\int_0^1\frac{x^7-1}{\log(x)}\mathrm dx$?,What is ?,\int_0^1\frac{x^7-1}{\log(x)}\mathrm dx,"/A problem from the 2012 MIT Integration Bee is $$ \int_0^1\frac{x^7-1}{\log(x)}\mathrm dx $$ The answer is $\log(8)$. Wolfram Alpha gives an indefinite form in terms of the logarithmic integral function, but times out doing the computation. Is there a way to do it by hand?","/A problem from the 2012 MIT Integration Bee is $$ \int_0^1\frac{x^7-1}{\log(x)}\mathrm dx $$ The answer is $\log(8)$. Wolfram Alpha gives an indefinite form in terms of the logarithmic integral function, but times out doing the computation. Is there a way to do it by hand?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
19,How is the Taylor expansion for $f(x + h)$ derived?,How is the Taylor expansion for  derived?,f(x + h),"According to this Wikipedia article, the expansion for $f(x\pm h)$ is: $$f(x \pm h) = f(x) \pm hf'(x) + \frac{h^2}{2}f''(x) \pm \frac{h^3}{6}f^{(3)}(x) + O(h^4)$$ I'm not understanding how you are left with $f(x)$ terms on the right hand side. I tried working out, for example, the Taylor expansion for $f(x + h)$ (using $(x+h)$ as $x_0$) and got this: $$ f(x + h) = f(x+h) + f'(x + h)(x-(x+h)) + \frac{f''(x+h)}{2!}(x-(x+h))^2 + \frac{f'''(x+h)}{3!} (x - (x + h))^3 + \cdots $$ $$ = f(x + h) - hf'(x+h) + \frac{h^2}{2!}f''(x + h) - \frac{h^3}{3!} f'''(x+h) + \cdots$$ Am I doing this correctly?","According to this Wikipedia article, the expansion for $f(x\pm h)$ is: $$f(x \pm h) = f(x) \pm hf'(x) + \frac{h^2}{2}f''(x) \pm \frac{h^3}{6}f^{(3)}(x) + O(h^4)$$ I'm not understanding how you are left with $f(x)$ terms on the right hand side. I tried working out, for example, the Taylor expansion for $f(x + h)$ (using $(x+h)$ as $x_0$) and got this: $$ f(x + h) = f(x+h) + f'(x + h)(x-(x+h)) + \frac{f''(x+h)}{2!}(x-(x+h))^2 + \frac{f'''(x+h)}{3!} (x - (x + h))^3 + \cdots $$ $$ = f(x + h) - hf'(x+h) + \frac{h^2}{2!}f''(x + h) - \frac{h^3}{3!} f'''(x+h) + \cdots$$ Am I doing this correctly?",,"['calculus', 'numerical-methods', 'taylor-expansion', 'substitution']"
20,The $100$th derivative of $(x^2 + 1)/(x^3 - x)$,The th derivative of,100 (x^2 + 1)/(x^3 - x),"I am reading a collection of problems by the Russian mathematician Vladimir Arnol'd, titled A Mathematical Trivium . I am taking a stab at this one: Calculate the $100$th derivative of the function $$\frac{x^2 + 1}{x^3 - x}.$$ The derivative is non-trivial (in the sense that I computed it for a few rounds, and it only became more assertive). My first thought was to let $$f(x) = x^2 + 1, \text{ } g(x) = \frac{1}{x^3 - x}$$ and apply the Leibnitz rule for products, $$fg^{(n)}(x) = \sum_{k=0}^n {n\choose k} f^{(n-k)}(x)g^{(k)}(x) .$$ Since $f$ is vanishing after the third differentiation, we get $$fg^{(100)}(x) = {100 \choose 2}f^{(2)}g^{(98)} + {100 \choose 1}f^{(1)}g^{(99)} {100 \choose 0}f^{(0)}g^{(100)} \\= 9900g^{(98)} + 200xg^{(99)} + (x^2 + 1)g^{(100)}$$ This would be great if we could compute the last few derivatives of $g$. Indeed, we can boil this down: notice that $$g(x) = h(x)i(x)j(x), \hspace{4mm} h(x) = \frac{1}{x-1}, \text{  } i(x) = \frac{1}{x}, \text{  } j(x) = \frac{1}{x+1};$$ further, $h, i,$ and $j$ have friendly behavior under repeated differentation, e.g. $h^{(n)}(x) = \frac{(-1)^n n!}{(x-1)^{n + 1}}$. So overall, it is possible to use Leibnitz again to beat a lengthy derivative out of this function, (namely, $$g^{(n)}(x) = \sum_{k=0}^n {n \choose k} h^{(n-k)}(x) \Bigl(\sum_{l=0}^k {k \choose l} i^{(k-l)}(x) j^{(l)}(x)\Bigr)$$ with the details filled in). However, this is really pretty far from computing the derivative. So, my question: does anyone know how to either improve the above argument, or generate a new one, which can resolve the problem?","I am reading a collection of problems by the Russian mathematician Vladimir Arnol'd, titled A Mathematical Trivium . I am taking a stab at this one: Calculate the $100$th derivative of the function $$\frac{x^2 + 1}{x^3 - x}.$$ The derivative is non-trivial (in the sense that I computed it for a few rounds, and it only became more assertive). My first thought was to let $$f(x) = x^2 + 1, \text{ } g(x) = \frac{1}{x^3 - x}$$ and apply the Leibnitz rule for products, $$fg^{(n)}(x) = \sum_{k=0}^n {n\choose k} f^{(n-k)}(x)g^{(k)}(x) .$$ Since $f$ is vanishing after the third differentiation, we get $$fg^{(100)}(x) = {100 \choose 2}f^{(2)}g^{(98)} + {100 \choose 1}f^{(1)}g^{(99)} {100 \choose 0}f^{(0)}g^{(100)} \\= 9900g^{(98)} + 200xg^{(99)} + (x^2 + 1)g^{(100)}$$ This would be great if we could compute the last few derivatives of $g$. Indeed, we can boil this down: notice that $$g(x) = h(x)i(x)j(x), \hspace{4mm} h(x) = \frac{1}{x-1}, \text{  } i(x) = \frac{1}{x}, \text{  } j(x) = \frac{1}{x+1};$$ further, $h, i,$ and $j$ have friendly behavior under repeated differentation, e.g. $h^{(n)}(x) = \frac{(-1)^n n!}{(x-1)^{n + 1}}$. So overall, it is possible to use Leibnitz again to beat a lengthy derivative out of this function, (namely, $$g^{(n)}(x) = \sum_{k=0}^n {n \choose k} h^{(n-k)}(x) \Bigl(\sum_{l=0}^k {k \choose l} i^{(k-l)}(x) j^{(l)}(x)\Bigr)$$ with the details filled in). However, this is really pretty far from computing the derivative. So, my question: does anyone know how to either improve the above argument, or generate a new one, which can resolve the problem?",,"['calculus', 'derivatives']"
21,Is there such a thing as partial integration?,Is there such a thing as partial integration?,,"Recently in my mathematics courses I was taught partial derivatives, and I wondered if the reverse exists for integrals. This may sound like a stupid question, and it probably is, but let me explain: By the fundamental theorem of calculus:  $$ \int \frac{d}{dx}f(x)~dx = f(x) $$ So is there an operator such that: $$ \int \frac{\partial}{\partial x}f(x,y)~\partial x = f(x,y) $$ or is this complete bogus? What I am saying is that if the partial derivatives of a multi-variable equation is the slope of the line along the derivative axis, is there an operator, a ""partial integral"" that is the area under the curve along the integrated axis? Or is that just $\int f(x,y) ~dx$, since doing it over more than one axis requires $\iint f(x,y) dx dy$? Physically, this could be related to finding the x-component of the velocity given the acceleration function. Also, how stupid of a question is this?","Recently in my mathematics courses I was taught partial derivatives, and I wondered if the reverse exists for integrals. This may sound like a stupid question, and it probably is, but let me explain: By the fundamental theorem of calculus:  $$ \int \frac{d}{dx}f(x)~dx = f(x) $$ So is there an operator such that: $$ \int \frac{\partial}{\partial x}f(x,y)~\partial x = f(x,y) $$ or is this complete bogus? What I am saying is that if the partial derivatives of a multi-variable equation is the slope of the line along the derivative axis, is there an operator, a ""partial integral"" that is the area under the curve along the integrated axis? Or is that just $\int f(x,y) ~dx$, since doing it over more than one axis requires $\iint f(x,y) dx dy$? Physically, this could be related to finding the x-component of the velocity given the acceleration function. Also, how stupid of a question is this?",,"['calculus', 'integration', 'derivatives', 'partial-derivative']"
22,When can we not treat differentials as fractions? And when is it perfectly OK?,When can we not treat differentials as fractions? And when is it perfectly OK?,,"Background I am a first year calculus student so I would prefer if answers remained in Layman's terms. It is common knowledge and seems to me a mantra that I keep hearing over and over again to ""not treat differentials/derivatives as fractions"". I am of course, in particular, referring to Leibniz notation . However, aside from a quick response such as ""oh, it's because its not a fraction but rather a type of operator"", I never really got a full answer as to why we can't treat it as such. It just kind of sits at the edge of taboo in my mind where it sometimes gets used and sometimes doesn't. Confusion is further compounded when a lot of things seem to just work out if we treat them just as fractions (e.g. u-substitution/related-rates) Example Air is being pumped into a balloon at a rate of $100cm^3/s$ . We want the rate of change of radius when the radius is at $25cm$ . $$\text{we are given}\ \frac{dv}{dt}=100cm^3/s$$ $$\text{we want}\ \frac{dr}{dt}\ \text{when}\ r=25cm$$ Thus we will solve this by using the relation $v=\frac{4}{3}\pi r^3$ $$\frac{dv}{dt}=\frac{dv}{dr}\frac{dr}{dt}$$ $$\frac{dv}{dt}\frac{dr}{dv}=\frac{dr}{dt}$$ $$100\frac{1}{4\pi r^2}=\frac{1}{25\pi}$$ So the answer is $\frac{dr}{dt}=\frac{1}{25\pi}$ when $r=25cm$ *Note the manipulation of derivatives just as if they were common fractions using algebra. Question When exactly can I treat differentials/derivatives as fractions and when can I not? Please keep in mind that at the end of the day, I am a first year college student. An answer that is easy to understand is preferred over one that is more mathematically rigorous but less friendly to a beginner such as me.","Background I am a first year calculus student so I would prefer if answers remained in Layman's terms. It is common knowledge and seems to me a mantra that I keep hearing over and over again to ""not treat differentials/derivatives as fractions"". I am of course, in particular, referring to Leibniz notation . However, aside from a quick response such as ""oh, it's because its not a fraction but rather a type of operator"", I never really got a full answer as to why we can't treat it as such. It just kind of sits at the edge of taboo in my mind where it sometimes gets used and sometimes doesn't. Confusion is further compounded when a lot of things seem to just work out if we treat them just as fractions (e.g. u-substitution/related-rates) Example Air is being pumped into a balloon at a rate of . We want the rate of change of radius when the radius is at . Thus we will solve this by using the relation So the answer is when *Note the manipulation of derivatives just as if they were common fractions using algebra. Question When exactly can I treat differentials/derivatives as fractions and when can I not? Please keep in mind that at the end of the day, I am a first year college student. An answer that is easy to understand is preferred over one that is more mathematically rigorous but less friendly to a beginner such as me.",100cm^3/s 25cm \text{we are given}\ \frac{dv}{dt}=100cm^3/s \text{we want}\ \frac{dr}{dt}\ \text{when}\ r=25cm v=\frac{4}{3}\pi r^3 \frac{dv}{dt}=\frac{dv}{dr}\frac{dr}{dt} \frac{dv}{dt}\frac{dr}{dv}=\frac{dr}{dt} 100\frac{1}{4\pi r^2}=\frac{1}{25\pi} \frac{dr}{dt}=\frac{1}{25\pi} r=25cm,"['calculus', 'integration', 'derivatives', 'notation', 'terminology']"
23,Evaluating $\int_0^\infty \frac{dx}{\sqrt{x}[x^2+(1+2\sqrt{2})x+1][1-x+x^2-x^3+...+x^{50}]}$,Evaluating,\int_0^\infty \frac{dx}{\sqrt{x}[x^2+(1+2\sqrt{2})x+1][1-x+x^2-x^3+...+x^{50}]},My brother's friend gave me the following wicked integral with a beautiful result \begin{equation} {\Large\int_0^\infty} \frac{dx}{\sqrt{x} \bigg[x^2+\left(1+2\sqrt{2}\right)x+1\bigg] \bigg[1-x+x^2-x^3+\cdots+x^{50}\bigg]}={\large\left(\sqrt{2}-1\right)\pi} \end{equation} He claimed the above integral can be generalised to the following form \begin{equation} {\Large\int_0^\infty} \frac{dx}{\sqrt{x} \bigg[x^2+ax+1\bigg] \bigg[1-x+x^2-x^3+\cdots+(-x)^{n}\bigg]}=\ldots \end{equation} This is a challenging problem. How to prove it and what is the closed-form of the general integral?,My brother's friend gave me the following wicked integral with a beautiful result He claimed the above integral can be generalised to the following form This is a challenging problem. How to prove it and what is the closed-form of the general integral?,"\begin{equation}
{\Large\int_0^\infty} \frac{dx}{\sqrt{x} \bigg[x^2+\left(1+2\sqrt{2}\right)x+1\bigg] \bigg[1-x+x^2-x^3+\cdots+x^{50}\bigg]}={\large\left(\sqrt{2}-1\right)\pi}
\end{equation} \begin{equation}
{\Large\int_0^\infty} \frac{dx}{\sqrt{x} \bigg[x^2+ax+1\bigg] \bigg[1-x+x^2-x^3+\cdots+(-x)^{n}\bigg]}=\ldots
\end{equation}","['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
24,Why isn't the derivative of the volume of the cone its surface area?,Why isn't the derivative of the volume of the cone its surface area?,,"The derivative of the volume of a sphere, with respect to its radius, gives its surface area. I understand that this is because given an infinitesimal increase in radius, the change in volume will only occur at the surface. Similarly, the derivative of a circle's area with respect to its radius gives its circumference for similar reasons. A similar logic can be applied to other simple geometric shapes and the pattern holds. For a cylinder, it becomes more interesting. Its volume is $\pi r^2h$ , with the height $h$ and radius $r$ being independent. Incremental increases in  a cylinders height will ""add a circle"" to the bottom of the cylinder, so it makes sense to me that the derivative of volume with respect to height is the are of that circle $\pi r^2$ . Incremental changes to its radius, will add a ""tube"" around the cylinder, whose area is $2\pi rh$ , the derivative of volume with respect to radius. I thought I had found a nice heuristic reason for why the derivatives of volumes give surface areas, but it breaks down for the cone. The volume is $\frac{\pi r^2 h}{3}$ . Assuming the angle $\theta$ of the cone remains constant, $h$ and $r$ are dependent ( $r=h\tan\theta$ ). Now, the derivative of the cone's volume with respect to height gives the area of the base of the cone, this intuitively makes sense, but the derivative with respect to radius doesn't seem to match any geometric quantity. $V=\dfrac{\pi r^3}{3\tan\theta}~$ and $~\dfrac{\text{d}V}{\text{d}r}=\dfrac{\pi r^2}{\tan\theta}$ Similarly, I'd hoped to find some way to drive the surface area of the cone without the base $\pi rl$ where $l$ is the distance from the point of the cone to the edge of the circle at its base. However, the derivative with respect to $l$ also doesn't seem to have any meaningful geometric significance. What's going on here? Is it an issue of not defining the shape rigorously enough? Do I need to parametrise the volume rigorously in some way?","The derivative of the volume of a sphere, with respect to its radius, gives its surface area. I understand that this is because given an infinitesimal increase in radius, the change in volume will only occur at the surface. Similarly, the derivative of a circle's area with respect to its radius gives its circumference for similar reasons. A similar logic can be applied to other simple geometric shapes and the pattern holds. For a cylinder, it becomes more interesting. Its volume is , with the height and radius being independent. Incremental increases in  a cylinders height will ""add a circle"" to the bottom of the cylinder, so it makes sense to me that the derivative of volume with respect to height is the are of that circle . Incremental changes to its radius, will add a ""tube"" around the cylinder, whose area is , the derivative of volume with respect to radius. I thought I had found a nice heuristic reason for why the derivatives of volumes give surface areas, but it breaks down for the cone. The volume is . Assuming the angle of the cone remains constant, and are dependent ( ). Now, the derivative of the cone's volume with respect to height gives the area of the base of the cone, this intuitively makes sense, but the derivative with respect to radius doesn't seem to match any geometric quantity. and Similarly, I'd hoped to find some way to drive the surface area of the cone without the base where is the distance from the point of the cone to the edge of the circle at its base. However, the derivative with respect to also doesn't seem to have any meaningful geometric significance. What's going on here? Is it an issue of not defining the shape rigorously enough? Do I need to parametrise the volume rigorously in some way?",\pi r^2h h r \pi r^2 2\pi rh \frac{\pi r^2 h}{3} \theta h r r=h\tan\theta V=\dfrac{\pi r^3}{3\tan\theta}~ ~\dfrac{\text{d}V}{\text{d}r}=\dfrac{\pi r^2}{\tan\theta} \pi rl l l,"['calculus', 'geometry', 'area', 'volume']"
25,"Conjectured value of a harmonic sum $\sum_{n=1}^\infty\left(H_n-\,2H_{2n}+H_{4n}\right)^2$",Conjectured value of a harmonic sum,"\sum_{n=1}^\infty\left(H_n-\,2H_{2n}+H_{4n}\right)^2","There is a known asymptotic expansion of harmonic numbers $H_n$ for $n\to\infty$: $$\begin{align}H_n&=\gamma+\ln n+\sum_{k=1}^\infty\left(-\frac{B_k}{k\cdot n^k}\right)\\ &=\gamma+\ln n+\frac1{2n}-\frac1{12n^2}+\frac1{120n^4}-\frac1{252n^6}\,+\,\dots,\end{align}\tag1$$ where $B_k$ are Bernoulli numbers . We can take a linear combination of harmonic numbers to cancel constant and logarithmic terms, compensate for $O(n^{-1})$ term, and get the following series that is possible to evaluate in a closed form (e.g. using generating function): $$\sum_{k=1}^\infty\left(H_n-\,2H_{2n}+H_{4n}-\frac1{8n}\right)=\frac18-\frac\pi{16}.\tag2$$ Rather than compensating for $O(n^{-1})$ term, we can take a series with alternating signs, that is also possible to evaluate in a closed form: $$\sum_{n=1}^\infty\,(-1)^n\,\Big(H_n-\,2H_{2n}+H_{4n}\Big)=\frac{3\pi}{16}-\frac\pi{4\sqrt2}-\frac{\ln2}8.\tag3$$ Generalizing, we can consider two families of series: $$\mathcal A_m=\sum_{n=1}^\infty\,(-1)^n\,\Big(H_n-\,2H_{2n}+H_{4n}\Big)^m,\tag4$$ $$\mathcal B_m=\sum_{n=1}^\infty\Big(H_n-\,2H_{2n}+H_{4n}\Big)^m,\tag5$$ and try to evaluate them in a closed form. So far I have the following conjectured result: $$\large\sum_{n=1}^\infty\Big(H_n-\,2H_{2n}+H_{4n}\Big)^2\stackrel{\normalsize\color{gray}?}=\frac\pi8-\frac\pi{16}\,\ln2-\frac{\pi^2}{96}+\frac3{16}\,\ln^22-\frac{G}{4},\tag{$\diamond$}$$ where $G$ is the Catalan constant . Could you please help me to prove this result and, possibly, find other values of $\mathcal A_m,\mathcal B_m$?","There is a known asymptotic expansion of harmonic numbers $H_n$ for $n\to\infty$: $$\begin{align}H_n&=\gamma+\ln n+\sum_{k=1}^\infty\left(-\frac{B_k}{k\cdot n^k}\right)\\ &=\gamma+\ln n+\frac1{2n}-\frac1{12n^2}+\frac1{120n^4}-\frac1{252n^6}\,+\,\dots,\end{align}\tag1$$ where $B_k$ are Bernoulli numbers . We can take a linear combination of harmonic numbers to cancel constant and logarithmic terms, compensate for $O(n^{-1})$ term, and get the following series that is possible to evaluate in a closed form (e.g. using generating function): $$\sum_{k=1}^\infty\left(H_n-\,2H_{2n}+H_{4n}-\frac1{8n}\right)=\frac18-\frac\pi{16}.\tag2$$ Rather than compensating for $O(n^{-1})$ term, we can take a series with alternating signs, that is also possible to evaluate in a closed form: $$\sum_{n=1}^\infty\,(-1)^n\,\Big(H_n-\,2H_{2n}+H_{4n}\Big)=\frac{3\pi}{16}-\frac\pi{4\sqrt2}-\frac{\ln2}8.\tag3$$ Generalizing, we can consider two families of series: $$\mathcal A_m=\sum_{n=1}^\infty\,(-1)^n\,\Big(H_n-\,2H_{2n}+H_{4n}\Big)^m,\tag4$$ $$\mathcal B_m=\sum_{n=1}^\infty\Big(H_n-\,2H_{2n}+H_{4n}\Big)^m,\tag5$$ and try to evaluate them in a closed form. So far I have the following conjectured result: $$\large\sum_{n=1}^\infty\Big(H_n-\,2H_{2n}+H_{4n}\Big)^2\stackrel{\normalsize\color{gray}?}=\frac\pi8-\frac\pi{16}\,\ln2-\frac{\pi^2}{96}+\frac3{16}\,\ln^22-\frac{G}{4},\tag{$\diamond$}$$ where $G$ is the Catalan constant . Could you please help me to prove this result and, possibly, find other values of $\mathcal A_m,\mathcal B_m$?",,"['calculus', 'sequences-and-series', 'closed-form', 'conjectures', 'harmonic-numbers']"
26,Evaluate $\int_0^\infty\frac{\ln x}{1+x^2}dx$,Evaluate,\int_0^\infty\frac{\ln x}{1+x^2}dx,Evaluate $$\int_0^\infty\frac{\ln x}{1+x^2}\ dx$$ I don't know where to start with this so either the full evaluation or any hints or pushes in the right direction would be appreciated.  Thanks.,Evaluate $$\int_0^\infty\frac{\ln x}{1+x^2}\ dx$$ I don't know where to start with this so either the full evaluation or any hints or pushes in the right direction would be appreciated.  Thanks.,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
27,Derivative of a factorial,Derivative of a factorial,,"What is ${\partial\over \partial x_i}(x_i !)$ where $x_i$ is a discrete variable? Do you consider $(x_i!)=(x_i)(x_i-1)...1$ and do product rule on each term, or something else? Thanks.","What is ${\partial\over \partial x_i}(x_i !)$ where $x_i$ is a discrete variable? Do you consider $(x_i!)=(x_i)(x_i-1)...1$ and do product rule on each term, or something else? Thanks.",,"['calculus', 'factorial']"
28,Why does an infinite limit not exist?,Why does an infinite limit not exist?,,"I read in Stewart ""single variable calculus"" page 83 that the limit $$\lim_{x\to 0}{1/x^2}$$ does not exist . How precise is this statement knowing that this limit is $\infty$ ?. I thought saying the limit does not exist is not true where limits are $\infty$ . But it is said when a function does not have a limit at all like $$\lim_{x\to \infty}{\cos x.}$$","I read in Stewart ""single variable calculus"" page 83 that the limit does not exist . How precise is this statement knowing that this limit is ?. I thought saying the limit does not exist is not true where limits are . But it is said when a function does not have a limit at all like",\lim_{x\to 0}{1/x^2} \infty \infty \lim_{x\to \infty}{\cos x.},"['calculus', 'terminology']"
29,"Evaluating $\int P(\sin x, \cos x) \text{d}x$",Evaluating,"\int P(\sin x, \cos x) \text{d}x","Suppose $\displaystyle P(x,y)$ a polynomial in the variables $x,y$. For example, $\displaystyle x^4$ or $\displaystyle x^3y^2 + 3xy + 1$. Is there a general method which allows us to evaluate the indefinite integral $$ \int P(\sin x, \cos x) \text{d} x$$ What about the case when $\displaystyle P(x,y)$ is a rational function (i.e. a ratio of two polynomials)? Example of a rational function: $\displaystyle \frac{x^2y + y^3}{x+y}$. This is being asked in an effort to cut down on duplicates, see here: Coping with *abstract* duplicate questions. and here: List of Generalizations of Common Questions.","Suppose $\displaystyle P(x,y)$ a polynomial in the variables $x,y$. For example, $\displaystyle x^4$ or $\displaystyle x^3y^2 + 3xy + 1$. Is there a general method which allows us to evaluate the indefinite integral $$ \int P(\sin x, \cos x) \text{d} x$$ What about the case when $\displaystyle P(x,y)$ is a rational function (i.e. a ratio of two polynomials)? Example of a rational function: $\displaystyle \frac{x^2y + y^3}{x+y}$. This is being asked in an effort to cut down on duplicates, see here: Coping with *abstract* duplicate questions. and here: List of Generalizations of Common Questions.",,"['calculus', 'integration', 'faq']"
30,Infinite Integration by Parts,Infinite Integration by Parts,,"A while back, I was explaining to a friend a method to compute the indefinite integral of $\ln(x)$ , by taking $\int \ln(x) \, dx$ and setting $u = \ln(x), du = \frac{1}{x} \, dx, dv = 1 \, dx, v = x$ , then proceeding with integration by parts. However, I was wondering if this method could be generalized for any infinitely differentiable function $f(x)$ , so I attempted the same method. Below is what I computed: $\int f(x) \, dx$ , so let $u = f(x), du = f'(x)\,dx, dv = 1\, dx, v = x \implies$ $x f(x) - \int x f'(x) \, dx$ , so let $u = f'(x), du = f''(x) \, dx, dv = x \, dx, v = \frac{x^2}2 \implies$ $xf(x) - \frac{x^2} 2 f'(x) + \int \frac{x^2}2 f''(x) \, dx$ , so let $u = f''(x), du = f'''(x) \, dx, dv = \frac{x^2} 2 \, dx, v = \frac{x^3} 6 \ldots$ This eventually yields $$\int f(x) \, dx= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x),$$ where $f^{(n-1)}(x)$ represents the $(n-1)$ -th derivative of $f(x)$ . At first glance, while this looks like a Taylor expansion, it is actually quite different. Firstly, the general Taylor expansion does not contain the $(-1)^{n-1}$ term that this sum does. Secondly, Taylor expansions are centered around some point, while this sum is not. Finally, and perhaps most notably, in a Taylor expansion, the derivatives are evaluated at a specific point, while in this expansion, they are left as functions. My questions, then, are the following: Is this something new, or has it been documented before? (I've spent a lot of time looking to see if I could find something similar, and so far this has yielded nothing. But, if it has been done, I'd love to read more about it) If this is novel (or even if it is not), what are some of the interesting consequences of this expression? I really do appreciate all of your input and advice. I realize this is a little open ended, but it's been nagging at me for quite some while, and I'd love a second look. Thanks! EDIT: As Alex Kruckman pointed out in the comments, the claim 'this eventually yields' leads to a loss of precision, specifically as it pertains to the $+C$ term in an indefinite integral. After spending some time thinking about this, I noted several remedies to this issue: Rather than express the integral as an infinite sum, we can represent it as a finite sum added to an 'error' term of sorts. In other words, we can say: $$\int f(x)\, dx = \sum_{n=1}^k\left[\frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x)\right] + \int \frac{x^k}{k!}(-1)^{k}f^{(k)}(x) \, dx$$ Interestingly, when considering the infinite sum, we note that every term has a polynomial component of at least degree one. This implies that if $x = 0$ , the sum should evaluate to $0$ as well. Therefore, setting $G(x) = \int_{a}^{x} f(t) \, dt = F(x) - F(a)$ , we see that as $G(0) = 0$ , $F(0) - F(a) = 0,$ which implies $a = 0$ . In other words, to make my original claim more precise, we can use the definite integral: $$\int_{0}^{x} f(t) \, dt= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x)$$ I believe these two edits help to eliminate the problem with the $+C$ term. EDIT 2: I've tried a couple common functions to see how they interact with the formula. Considering $f(x) = e^x$ , we have: $$\begin{aligned}\int e^x \, dx&= \sum_{n=1}^\infty\frac{x^n}{n!} (-1)^{n - 1}e^x\\e^x + C &= e^x\cdot\sum_{n=1}^\infty\frac{x^n}{n!} (-1)^{n - 1}\\&= e^x\cdot(1 - e^{-x})=e^x-1\end{aligned}$$ This implies $C = -1$ , a result consistent with the first edit, where we integrate from $0$ to $x$ . Similarly, considering $f(x) = e^{-x}$ , we have: $$\begin{aligned}\int e^{-x} \, dx= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} e^{-x} * (-1)^{(n-1)} = \sum_{n=1}^\infty \frac{x^n}{n!} * e^{-x}\\-e^{-x} + C = e^{-x} * \sum_{n=1}^\infty \frac{x^n}{n!} = e^{-x} * (e^x - 1) = 1 - e^{-x}\end{aligned}$$ This implies $C = 1$ , which we would again get if we integrated from $0$ to $x$ . Now, I decided to consider $f(x) =\ln(x)$ , a decidedly harder function to analyze in this respect, which also is not defined at 0. Here, we have: $$\begin{aligned}\int\ln(x) \, dx&=\sum_{n=1}^\infty\frac{x^n}{n!}(-1)^{n - 1}\frac{d^{(n-1)}}{dx^{(n-1)}}\ln(x)\\x\cdot\ln(x)-x+C&=x\cdot\ln(x)+\sum_{n=2}^\infty\frac{x^n}{n!} (-1)^{n - 1} \frac{d^{(n-1)}}{dx^{(n-1)}}\ln(x)\\&=x\cdot\ln(x)-x+C&=x\cdot\ln(x)+\sum_{n=2}^\infty\frac{x^n}{n!} (-1)^{n - 1} \frac{(n-2)!}{x^{n-1}}\cdot(-1)^n\\x\cdot\ln(x)-x+C&=x\cdot\ln(x)-\sum_{n=2}^\infty\frac{x}{n(n-1)}\\&=x\cdot\ln(x)-x\end{aligned}$$ Interestingly, here, we see that $C = 0$ . While $ln(x)$ is not defined at $0$ , this intuitively still makes some sense, as we note $\lim\limits_{x\to0} (x\cdot\ln(x) - x) = 0$ .","A while back, I was explaining to a friend a method to compute the indefinite integral of , by taking and setting , then proceeding with integration by parts. However, I was wondering if this method could be generalized for any infinitely differentiable function , so I attempted the same method. Below is what I computed: , so let , so let , so let This eventually yields where represents the -th derivative of . At first glance, while this looks like a Taylor expansion, it is actually quite different. Firstly, the general Taylor expansion does not contain the term that this sum does. Secondly, Taylor expansions are centered around some point, while this sum is not. Finally, and perhaps most notably, in a Taylor expansion, the derivatives are evaluated at a specific point, while in this expansion, they are left as functions. My questions, then, are the following: Is this something new, or has it been documented before? (I've spent a lot of time looking to see if I could find something similar, and so far this has yielded nothing. But, if it has been done, I'd love to read more about it) If this is novel (or even if it is not), what are some of the interesting consequences of this expression? I really do appreciate all of your input and advice. I realize this is a little open ended, but it's been nagging at me for quite some while, and I'd love a second look. Thanks! EDIT: As Alex Kruckman pointed out in the comments, the claim 'this eventually yields' leads to a loss of precision, specifically as it pertains to the term in an indefinite integral. After spending some time thinking about this, I noted several remedies to this issue: Rather than express the integral as an infinite sum, we can represent it as a finite sum added to an 'error' term of sorts. In other words, we can say: Interestingly, when considering the infinite sum, we note that every term has a polynomial component of at least degree one. This implies that if , the sum should evaluate to as well. Therefore, setting , we see that as , which implies . In other words, to make my original claim more precise, we can use the definite integral: I believe these two edits help to eliminate the problem with the term. EDIT 2: I've tried a couple common functions to see how they interact with the formula. Considering , we have: This implies , a result consistent with the first edit, where we integrate from to . Similarly, considering , we have: This implies , which we would again get if we integrated from to . Now, I decided to consider , a decidedly harder function to analyze in this respect, which also is not defined at 0. Here, we have: Interestingly, here, we see that . While is not defined at , this intuitively still makes some sense, as we note .","\ln(x) \int \ln(x) \, dx u = \ln(x), du = \frac{1}{x} \, dx, dv = 1 \, dx, v = x f(x) \int f(x) \, dx u = f(x), du = f'(x)\,dx, dv = 1\, dx, v = x \implies x f(x) - \int x f'(x) \, dx u = f'(x), du = f''(x) \, dx, dv = x \, dx, v = \frac{x^2}2 \implies xf(x) - \frac{x^2} 2 f'(x) + \int \frac{x^2}2 f''(x) \, dx u = f''(x), du = f'''(x) \, dx, dv = \frac{x^2} 2 \, dx, v = \frac{x^3} 6 \ldots \int f(x) \, dx= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x), f^{(n-1)}(x) (n-1) f(x) (-1)^{n-1} +C \int f(x)\, dx = \sum_{n=1}^k\left[\frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x)\right] + \int \frac{x^k}{k!}(-1)^{k}f^{(k)}(x) \, dx x = 0 0 G(x) = \int_{a}^{x} f(t) \, dt = F(x) - F(a) G(0) = 0 F(0) - F(a) = 0, a = 0 \int_{0}^{x} f(t) \, dt= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} f^{(n-1)}(x) +C f(x) = e^x \begin{aligned}\int e^x \, dx&= \sum_{n=1}^\infty\frac{x^n}{n!} (-1)^{n - 1}e^x\\e^x + C &= e^x\cdot\sum_{n=1}^\infty\frac{x^n}{n!} (-1)^{n - 1}\\&= e^x\cdot(1 - e^{-x})=e^x-1\end{aligned} C = -1 0 x f(x) = e^{-x} \begin{aligned}\int e^{-x} \, dx= \sum_{n=1}^\infty \frac{x^n}{n!} (-1)^{n - 1} e^{-x} * (-1)^{(n-1)} = \sum_{n=1}^\infty \frac{x^n}{n!} * e^{-x}\\-e^{-x} + C = e^{-x} * \sum_{n=1}^\infty \frac{x^n}{n!} = e^{-x} * (e^x - 1) = 1 - e^{-x}\end{aligned} C = 1 0 x f(x) =\ln(x) \begin{aligned}\int\ln(x) \, dx&=\sum_{n=1}^\infty\frac{x^n}{n!}(-1)^{n - 1}\frac{d^{(n-1)}}{dx^{(n-1)}}\ln(x)\\x\cdot\ln(x)-x+C&=x\cdot\ln(x)+\sum_{n=2}^\infty\frac{x^n}{n!} (-1)^{n - 1} \frac{d^{(n-1)}}{dx^{(n-1)}}\ln(x)\\&=x\cdot\ln(x)-x+C&=x\cdot\ln(x)+\sum_{n=2}^\infty\frac{x^n}{n!} (-1)^{n - 1} \frac{(n-2)!}{x^{n-1}}\cdot(-1)^n\\x\cdot\ln(x)-x+C&=x\cdot\ln(x)-\sum_{n=2}^\infty\frac{x}{n(n-1)}\\&=x\cdot\ln(x)-x\end{aligned} C = 0 ln(x) 0 \lim\limits_{x\to0} (x\cdot\ln(x) - x) = 0","['calculus', 'integration', 'sequences-and-series', 'taylor-expansion']"
31,Making trigonometric substitutions rigorous,Making trigonometric substitutions rigorous,,"I've been tutoring some basic calculus, and it made me think about something pretty basic. Let me explain the problem by example: Say we are given the integral $\int \frac{x^2}{\sqrt{1-x^2}}\ \mathrm dx$. It is then customary to write $x=\cos(\alpha), \mathrm dx=-\sin(\alpha)\ \mathrm d\alpha$. So:  $$\begin{align*} \int \frac{\cos^2(\alpha)}{\sqrt{\sin^2(\alpha))}}(-\sin(\alpha))\ \mathrm d\alpha&=-\int\frac{\cos^2(\alpha)}{\sin(\alpha)}\sin(\alpha)\ \mathrm d\alpha\\ &=-\int \cos^2(\alpha)\ \mathrm d\alpha\\ &=-\int\frac{1+\cos(2\alpha)}{2}\ \mathrm d\alpha\\ &=-\frac{\alpha}{2}-\frac{\sin(2\alpha)}{4}\\ &=-\frac{\alpha}{2}-\frac{\sin(\alpha)\cos(\alpha)}{2}. \end{align*}$$ We know plug in $\alpha=\arccos(x)$. It is left to know what $\sin(\arccos(x))$ is. For this, we now we pretend that $0\leq \alpha\leq \frac{\pi}{2}$ and draw a triangle which shows via the Pythagorean theorem that $\sin(\arccos(x))=\sqrt{1-x^2}$. Similarly there are the common substitutions $x=\tan(\alpha)$ and $x=\sec(\alpha)$. This technique seems less than rigorous. Here are some of my issues with it: At first it seemed to me like this is $u$-substitution in reverse (think that $x$ plays the role of $u$ and $\alpha$ plays the role of $x$). But in $u$-substitution, if the substitution is $u=g(x)$ then we eventually plug in $g(x)$ instead of $u$. But here the roles are reversed. So it seems that this really is ordinary $u$-substitution. But then that means that the substitution is $\alpha=\arccos(x)$ -- however there are many ways to choose inverses to $\cos$ and $\arccos(x)$ is only one of them. Are we really choosing just that one? I feel uncomfortable with $\sqrt{\sin^2(x)}=\sin(x)$. What if $\sin(x)$ is negative? What's going on here? Is the issue that we are just doing it on a segment where $\sin(x)$ is positive, do the whole thing, and then after we get an answer we do some argument using analytic continuation? Or is perhaps the issue that we are looking at $-1<x<1$ (because that's where $\sqrt{1-x^2}$ is defined), and so $\arccos(x)$ would go from $0$ to $\pi$ where $\sin(x)$ is positive? Would this weird argument also work for other trigonometric substitutions? That part at the end where we figure out what the trigonometric function of an inverse trigonometric function is (in my example $\sin(\arccos(x))$ is fishy to me. To figure it out I draw a triangle, which seems to assume $\arccos(x)$ is between $0$ and $\frac{\pi}{2}$. What's going on there. So, while I'm very familiar with the method, tutoring it made me realize I'm not sure what's really behind it. Could it really be something as complicated as analytic continuation behind it? Is there a uniform way of explaining this method with respect to the substitutions $x=\cos(\alpha), x=\tan(\alpha)$ and $x=\sec(\alpha)$?","I've been tutoring some basic calculus, and it made me think about something pretty basic. Let me explain the problem by example: Say we are given the integral $\int \frac{x^2}{\sqrt{1-x^2}}\ \mathrm dx$. It is then customary to write $x=\cos(\alpha), \mathrm dx=-\sin(\alpha)\ \mathrm d\alpha$. So:  $$\begin{align*} \int \frac{\cos^2(\alpha)}{\sqrt{\sin^2(\alpha))}}(-\sin(\alpha))\ \mathrm d\alpha&=-\int\frac{\cos^2(\alpha)}{\sin(\alpha)}\sin(\alpha)\ \mathrm d\alpha\\ &=-\int \cos^2(\alpha)\ \mathrm d\alpha\\ &=-\int\frac{1+\cos(2\alpha)}{2}\ \mathrm d\alpha\\ &=-\frac{\alpha}{2}-\frac{\sin(2\alpha)}{4}\\ &=-\frac{\alpha}{2}-\frac{\sin(\alpha)\cos(\alpha)}{2}. \end{align*}$$ We know plug in $\alpha=\arccos(x)$. It is left to know what $\sin(\arccos(x))$ is. For this, we now we pretend that $0\leq \alpha\leq \frac{\pi}{2}$ and draw a triangle which shows via the Pythagorean theorem that $\sin(\arccos(x))=\sqrt{1-x^2}$. Similarly there are the common substitutions $x=\tan(\alpha)$ and $x=\sec(\alpha)$. This technique seems less than rigorous. Here are some of my issues with it: At first it seemed to me like this is $u$-substitution in reverse (think that $x$ plays the role of $u$ and $\alpha$ plays the role of $x$). But in $u$-substitution, if the substitution is $u=g(x)$ then we eventually plug in $g(x)$ instead of $u$. But here the roles are reversed. So it seems that this really is ordinary $u$-substitution. But then that means that the substitution is $\alpha=\arccos(x)$ -- however there are many ways to choose inverses to $\cos$ and $\arccos(x)$ is only one of them. Are we really choosing just that one? I feel uncomfortable with $\sqrt{\sin^2(x)}=\sin(x)$. What if $\sin(x)$ is negative? What's going on here? Is the issue that we are just doing it on a segment where $\sin(x)$ is positive, do the whole thing, and then after we get an answer we do some argument using analytic continuation? Or is perhaps the issue that we are looking at $-1<x<1$ (because that's where $\sqrt{1-x^2}$ is defined), and so $\arccos(x)$ would go from $0$ to $\pi$ where $\sin(x)$ is positive? Would this weird argument also work for other trigonometric substitutions? That part at the end where we figure out what the trigonometric function of an inverse trigonometric function is (in my example $\sin(\arccos(x))$ is fishy to me. To figure it out I draw a triangle, which seems to assume $\arccos(x)$ is between $0$ and $\frac{\pi}{2}$. What's going on there. So, while I'm very familiar with the method, tutoring it made me realize I'm not sure what's really behind it. Could it really be something as complicated as analytic continuation behind it? Is there a uniform way of explaining this method with respect to the substitutions $x=\cos(\alpha), x=\tan(\alpha)$ and $x=\sec(\alpha)$?",,"['calculus', 'integration', 'trigonometry']"
32,Chain Rule Intuition,Chain Rule Intuition,,"We know that the chain rule is used to differentiate  a composite function ,say $$f(x) = h(g(x))$$ It's defined as the derivative of the outside function times the derivative of the inner function or the other way around. $$\frac{\mathrm{dy} }{\mathrm{d} x} = \frac{\mathrm{dy} }{\mathrm{d} u} \cdot \frac{\mathrm{du} }{\mathrm{d} x}$$ Despite we know that the above expression is not a fraction (even though it's a fractional notation of the derivative used by Leibnitz) you can ""cancel"" the two du's and get back dy/dx. My question is: How can you even think of cancelling du from dy/du and du from du/dx when they are not even fractions. Just because it's been multiplied do they automatically become fractions?Are they really being ""multiplied""? I'am really looking for an intuition behind this.To me this is some kind of fantasy.It doesn't appear to be real.","We know that the chain rule is used to differentiate  a composite function ,say $$f(x) = h(g(x))$$ It's defined as the derivative of the outside function times the derivative of the inner function or the other way around. $$\frac{\mathrm{dy} }{\mathrm{d} x} = \frac{\mathrm{dy} }{\mathrm{d} u} \cdot \frac{\mathrm{du} }{\mathrm{d} x}$$ Despite we know that the above expression is not a fraction (even though it's a fractional notation of the derivative used by Leibnitz) you can ""cancel"" the two du's and get back dy/dx. My question is: How can you even think of cancelling du from dy/du and du from du/dx when they are not even fractions. Just because it's been multiplied do they automatically become fractions?Are they really being ""multiplied""? I'am really looking for an intuition behind this.To me this is some kind of fantasy.It doesn't appear to be real.",,"['calculus', 'definition', 'intuition', 'chain-rule']"
33,A conjectured closed form of $\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx$,A conjectured closed form of,\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx,"Consider the following integral: $$\mathcal{I}=\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx.$$ I tried to evaluate $\mathcal{I}$ in a closed form (both manually and using Mathematica ), but without success. However, if WolframAlpha is provided with a numerical approximation $\,\mathcal{I}\approx 3.2694067500684...$, it returns a possible closed form:  $$\mathcal{I}\stackrel?=\frac\pi{2\,\ln^2 2}.$$ Further numeric caclulations show that this value is correct up to at least $10^3$ decimal digits. So, I conjecture that this is the exact value of $\mathcal{I}$. Question: Is this conjecture correct?","Consider the following integral: $$\mathcal{I}=\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx.$$ I tried to evaluate $\mathcal{I}$ in a closed form (both manually and using Mathematica ), but without success. However, if WolframAlpha is provided with a numerical approximation $\,\mathcal{I}\approx 3.2694067500684...$, it returns a possible closed form:  $$\mathcal{I}\stackrel?=\frac\pi{2\,\ln^2 2}.$$ Further numeric caclulations show that this value is correct up to at least $10^3$ decimal digits. So, I conjecture that this is the exact value of $\mathcal{I}$. Question: Is this conjecture correct?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
34,What is so special about $\alpha=-1$ in the integral of $x^\alpha$?,What is so special about  in the integral of ?,\alpha=-1 x^\alpha,"Of course, it is easy to see, that the integral (or the antiderivative) of $f(x) = 1/x$ is $\log(|x|)$ and of course for $\alpha\neq - 1$ the antiderivative of $f(x) = x^\alpha$ is $x^{\alpha+1}/(\alpha+1)$. I was wondering if there is an intuitive (probably geometric) explanation why the case $\alpha=-1$ is so different and why the logarithm appears? Some answers which I thought of but which are not convincing: Taking the limit $\alpha=-1$ either from above or below lead to diverging functions. Some speciality of the case $\alpha=-1$ are that both asymptotes are non-integrable. However, the antidrivative is a local thing, and hence, shouldn't care about the behavior at infinity.","Of course, it is easy to see, that the integral (or the antiderivative) of $f(x) = 1/x$ is $\log(|x|)$ and of course for $\alpha\neq - 1$ the antiderivative of $f(x) = x^\alpha$ is $x^{\alpha+1}/(\alpha+1)$. I was wondering if there is an intuitive (probably geometric) explanation why the case $\alpha=-1$ is so different and why the logarithm appears? Some answers which I thought of but which are not convincing: Taking the limit $\alpha=-1$ either from above or below lead to diverging functions. Some speciality of the case $\alpha=-1$ are that both asymptotes are non-integrable. However, the antidrivative is a local thing, and hence, shouldn't care about the behavior at infinity.",,"['calculus', 'integration']"
35,What does $dx$ mean?,What does  mean?,dx,"$dx$ appears in differential equations, such us derivatives and integrals. For example, a function $f(x)$ its first derivative is $\dfrac{d}{dx}f(x)$ and its integral $\displaystyle\int f(x)dx$. But I don't really understand what $dx$ is.","$dx$ appears in differential equations, such us derivatives and integrals. For example, a function $f(x)$ its first derivative is $\dfrac{d}{dx}f(x)$ and its integral $\displaystyle\int f(x)dx$. But I don't really understand what $dx$ is.",,"['calculus', 'notation']"
36,What books are prerequisites for Spivak's Calculus?,What books are prerequisites for Spivak's Calculus?,,"For financial reasons, I dropped out my senior year of college as a piano performance major. I will be returning to college to dual major in mathematics and computer science. I've taught myself to program out of SICP and would like to develop my mathematical chops. I got a 5 on the Calc BC test my junior year of high school and have always considered myself decent at math. I prefer learning from rigorous material, so Spivak's Calculus seemed to be a logical choice. I made it through the first chapter, but was blindsided by the difficulty of the first chapter's problems. Question 1. So, after solving a few problems, I put Spivak aside and picked up How to Prove It to brush up my proof writing skills. I've long ways away from finishing that, but what prerequisite material is recommended for Spivak? I remember feeling similarly overwhelmed when starting ""Structure and Interpretation of Computer Programs"" however, there's a lot of supplementary information available for that book and I now feel I mastered that material. ""Helper"" material for Spivak seems sparse (excepting the solutions manual). Question 2. Does anyone know of any video lectures that cover Spivak? To my uninformed mind, Spivak seemed like a good way to ""brush up"" on Calculus while improving my general math skills. Obviously, I horribly underestimated its difficulty.","For financial reasons, I dropped out my senior year of college as a piano performance major. I will be returning to college to dual major in mathematics and computer science. I've taught myself to program out of SICP and would like to develop my mathematical chops. I got a 5 on the Calc BC test my junior year of high school and have always considered myself decent at math. I prefer learning from rigorous material, so Spivak's Calculus seemed to be a logical choice. I made it through the first chapter, but was blindsided by the difficulty of the first chapter's problems. Question 1. So, after solving a few problems, I put Spivak aside and picked up How to Prove It to brush up my proof writing skills. I've long ways away from finishing that, but what prerequisite material is recommended for Spivak? I remember feeling similarly overwhelmed when starting ""Structure and Interpretation of Computer Programs"" however, there's a lot of supplementary information available for that book and I now feel I mastered that material. ""Helper"" material for Spivak seems sparse (excepting the solutions manual). Question 2. Does anyone know of any video lectures that cover Spivak? To my uninformed mind, Spivak seemed like a good way to ""brush up"" on Calculus while improving my general math skills. Obviously, I horribly underestimated its difficulty.",,['calculus']
37,Is a function Lipschitz if and only if its derivative is bounded?,Is a function Lipschitz if and only if its derivative is bounded?,,"Is the following statement true? Let $f: \mathbb{R}\to\mathbb{R}$ be continuous and differentiable. $f$ Lipschitz $\leftrightarrow \exists M:\forall x\in\mathbb{R}\ |f'(x)|\leq M$ If $f'$ is bounded, it is Lipschitz, that's obvious. Does that work the other way around? Let $f$ be $M$ -Lipschitz, that is to say $\forall x_1, x_2\in\mathbb{R},\ |f(x_1) - f(x_2)| \leq M|x_1 - x_2|$ , where $M$ is independent of $x_1, x_2$ . Let $x_1 <x_2$ be arbitrary. $f$ is continuous, so by the mean value theorem, there exists $c\in\mathbb{R}, x_1 < c < x_2$ , such that $f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1} \Rightarrow |f'(c)| = |\frac{f(x_2) - f(x_1)}{x_2-x_1}| \leq M$ . Does this hold? Can you, using the mean value theorem, ""reach"" every point in the derivative? Also, another question: If $f$ is Lipschitz, is it necessarily differentiable? Thanks!","Is the following statement true? Let be continuous and differentiable. Lipschitz If is bounded, it is Lipschitz, that's obvious. Does that work the other way around? Let be -Lipschitz, that is to say , where is independent of . Let be arbitrary. is continuous, so by the mean value theorem, there exists , such that . Does this hold? Can you, using the mean value theorem, ""reach"" every point in the derivative? Also, another question: If is Lipschitz, is it necessarily differentiable? Thanks!","f: \mathbb{R}\to\mathbb{R} f \leftrightarrow \exists M:\forall x\in\mathbb{R}\ |f'(x)|\leq M f' f M \forall x_1, x_2\in\mathbb{R},\ |f(x_1) - f(x_2)| \leq M|x_1 - x_2| M x_1, x_2 x_1 <x_2 f c\in\mathbb{R}, x_1 < c < x_2 f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1} \Rightarrow |f'(c)| = |\frac{f(x_2) - f(x_1)}{x_2-x_1}| \leq M f","['calculus', 'derivatives', 'lipschitz-functions']"
38,Integral $\int_0^1\frac{\arctan^2x}{\sqrt{1-x^2}}\mathrm dx$,Integral,\int_0^1\frac{\arctan^2x}{\sqrt{1-x^2}}\mathrm dx,"Is it possible to evaluate this integral in a closed form? $$I=\int_0^1\frac{\arctan^2x}{\sqrt{1-x^2}}\mathrm dx$$ It also can be represented as $$I=\int_0^{\pi/4}\frac{\phi^2}{\cos \phi\,\sqrt{\cos 2\phi}}\mathrm d\phi$$","Is it possible to evaluate this integral in a closed form? $$I=\int_0^1\frac{\arctan^2x}{\sqrt{1-x^2}}\mathrm dx$$ It also can be represented as $$I=\int_0^{\pi/4}\frac{\phi^2}{\cos \phi\,\sqrt{\cos 2\phi}}\mathrm d\phi$$",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
39,"About the integral $\int_{0}^{+\infty}\sin(x\,\log x)\,dx$",About the integral,"\int_{0}^{+\infty}\sin(x\,\log x)\,dx","It is an interesting exercise to show that the function $f(x)=\sin(x\log x)$ is Riemann-integrable over $\mathbb{R}^+$ (as shown by robjohn in this related question , for instance). Even more interesting is to notice that: $$\int_{0}^{1}f(x)\,dx = \sum_{n\geq 0}\int_{0}^{1}\frac{(-1)^n x^{2n+1}\left(\log x\right)^{2n+1}}{(2n+1)!}\,dx=\sum_{n\geq 1}\frac{(-1)^n}{(2n)^{2n}},\tag{1}$$ with a series related with the one appearing in many sophomore's dreams . Moreover, by using Lambert's W function it is not difficult to check that: $$ I = \int_{1}^{+\infty}\sin(x\log x)\,dx = \int_{0}^{+\infty}\frac{\sin u}{1+W(u)}\,du. \tag{2} $$ Now my question: is it possible to give a nice closed form to the RHS of $(2)$ through countour integration, the residue theorem or other techniques? For instance, is there a closed form for the almost-digamma-sum: $$ g(x)=\sum_{n\geq 0}\left(\frac{1}{1+W(x+2n\pi)}-\frac{1}{1+W(2n\pi)}\right) \tag{3}$$ ? If so, we have just to compute $\int_{0}^{2\pi}g(x)\sin x\,dx$.","It is an interesting exercise to show that the function $f(x)=\sin(x\log x)$ is Riemann-integrable over $\mathbb{R}^+$ (as shown by robjohn in this related question , for instance). Even more interesting is to notice that: $$\int_{0}^{1}f(x)\,dx = \sum_{n\geq 0}\int_{0}^{1}\frac{(-1)^n x^{2n+1}\left(\log x\right)^{2n+1}}{(2n+1)!}\,dx=\sum_{n\geq 1}\frac{(-1)^n}{(2n)^{2n}},\tag{1}$$ with a series related with the one appearing in many sophomore's dreams . Moreover, by using Lambert's W function it is not difficult to check that: $$ I = \int_{1}^{+\infty}\sin(x\log x)\,dx = \int_{0}^{+\infty}\frac{\sin u}{1+W(u)}\,du. \tag{2} $$ Now my question: is it possible to give a nice closed form to the RHS of $(2)$ through countour integration, the residue theorem or other techniques? For instance, is there a closed form for the almost-digamma-sum: $$ g(x)=\sum_{n\geq 0}\left(\frac{1}{1+W(x+2n\pi)}-\frac{1}{1+W(2n\pi)}\right) \tag{3}$$ ? If so, we have just to compute $\int_{0}^{2\pi}g(x)\sin x\,dx$.",,"['calculus', 'integration', 'sequences-and-series', 'complex-analysis', 'contour-integration']"
40,What is vector division?,What is vector division?,,"My question is:  We have addition, subtraction and muliplication of vectors. Why cannot we define vector division? What is division of vectors?","My question is:  We have addition, subtraction and muliplication of vectors. Why cannot we define vector division? What is division of vectors?",,"['calculus', 'vectors', 'divisibility']"
41,How to evaluate $\int_0^\infty\operatorname{erfc}^n x\ \mathrm dx$?,How to evaluate ?,\int_0^\infty\operatorname{erfc}^n x\ \mathrm dx,Let $\operatorname{erfc}x$ be the complementary error function. I successfully evaluated these integrals: $$\int_0^\infty\operatorname{erfc}x\ \mathrm dx=\frac1{\sqrt\pi}\tag1$$ $$\int_0^\infty\operatorname{erfc}^2x\ \mathrm dx=\frac{2-\sqrt2}{\sqrt\pi}\tag2$$ (Both $\operatorname{erfc}x$ and $\operatorname{erfc}^{2}x$ have primitive functions in terms of the error function.) But I have problems with $$\int_0^\infty\operatorname{erfc}^3x\ \mathrm dx\tag3$$ and a general case $$\int_0^\infty\operatorname{erfc}^n x\ \mathrm dx.\tag4$$ Could you suggest an approach to evaluate them as well?,Let $\operatorname{erfc}x$ be the complementary error function. I successfully evaluated these integrals: $$\int_0^\infty\operatorname{erfc}x\ \mathrm dx=\frac1{\sqrt\pi}\tag1$$ $$\int_0^\infty\operatorname{erfc}^2x\ \mathrm dx=\frac{2-\sqrt2}{\sqrt\pi}\tag2$$ (Both $\operatorname{erfc}x$ and $\operatorname{erfc}^{2}x$ have primitive functions in terms of the error function.) But I have problems with $$\int_0^\infty\operatorname{erfc}^3x\ \mathrm dx\tag3$$ and a general case $$\int_0^\infty\operatorname{erfc}^n x\ \mathrm dx.\tag4$$ Could you suggest an approach to evaluate them as well?,,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'error-function']"
42,"Integral $\int_0^\infty\frac{\operatorname{arccot}\left(\sqrt{x}-2\,\sqrt{x+1}\right)}{x+1}\mathrm dx$",Integral,"\int_0^\infty\frac{\operatorname{arccot}\left(\sqrt{x}-2\,\sqrt{x+1}\right)}{x+1}\mathrm dx","Is it possible to evaluate this integral in a closed form? $$\int_0^\infty\frac{\operatorname{arccot}\left(\sqrt{x}-2\,\sqrt{x+1}\right)}{x+1}\mathrm dx$$","Is it possible to evaluate this integral in a closed form? $$\int_0^\infty\frac{\operatorname{arccot}\left(\sqrt{x}-2\,\sqrt{x+1}\right)}{x+1}\mathrm dx$$",,"['calculus', 'integration', 'trigonometry', 'improper-integrals', 'closed-form']"
43,Prove $\int\limits_{0}^{\pi/2}\frac{dx}{1+\sin^2{(\tan{x})}}=\frac{\pi}{2\sqrt{2}}\bigl(\frac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\bigr)$,Prove,\int\limits_{0}^{\pi/2}\frac{dx}{1+\sin^2{(\tan{x})}}=\frac{\pi}{2\sqrt{2}}\bigl(\frac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\bigr),Prove the following integral   $$I=\int\limits_{0}^{\frac{\pi}{2}}\dfrac{dx}{1+\sin^2{(\tan{x})}}=\dfrac{\pi}{2\sqrt{2}}\left(\dfrac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\right)$$ This integral result was calculated using Mathematica and  I like this integral. But I can't solve it. My idea: Let $$\tan{x}=t\Longrightarrow dx=\dfrac{1}{1+t^2}dt$$ so $$I=\int\limits_{0}^{\infty}\dfrac{dt}{1+\sin^2{t}}\cdot \dfrac{1}{1+t^2}$$ then I can't proceed. Can you help me? Thank you.,Prove the following integral   $$I=\int\limits_{0}^{\frac{\pi}{2}}\dfrac{dx}{1+\sin^2{(\tan{x})}}=\dfrac{\pi}{2\sqrt{2}}\left(\dfrac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\right)$$ This integral result was calculated using Mathematica and  I like this integral. But I can't solve it. My idea: Let $$\tan{x}=t\Longrightarrow dx=\dfrac{1}{1+t^2}dt$$ so $$I=\int\limits_{0}^{\infty}\dfrac{dt}{1+\sin^2{t}}\cdot \dfrac{1}{1+t^2}$$ then I can't proceed. Can you help me? Thank you.,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
44,Convergence of $\sum \limits_{n=1}^{\infty}\sin(n^k)/n$,Convergence of,\sum \limits_{n=1}^{\infty}\sin(n^k)/n,Does $S_k= \sum \limits_{n=1}^{\infty}\sin(n^k)/n$ converge for all $k>0$? Motivation : I recently learned that $S_1$ converges . I think $S_2$ converges by the integral test. Was the question known in general?,Does $S_k= \sum \limits_{n=1}^{\infty}\sin(n^k)/n$ converge for all $k>0$? Motivation : I recently learned that $S_1$ converges . I think $S_2$ converges by the integral test. Was the question known in general?,,"['calculus', 'sequences-and-series']"
45,The Meaning of the Fundamental Theorem of Calculus,The Meaning of the Fundamental Theorem of Calculus,,"I am currently taking an advanced Calculus class in college, and we are studying generalizations of the FTC. We just started on the version for Line Integrals, and one can see the explicit symmetry between the 1-dim version and this version. But as the technical meaning of this theorem sank in, I realized I never really understood the meaning of FTC, going back to 1-dim. I understand that FTC creates a bond between the two fundamental studies of calculus, the integral and the derivative. But is there some geometric or tangible meaning to this bond? What does it really mean that the integral and the derivative are ""inverse processes""? Is this in the same exact sense as an inverse function? It would seem that the derivative measures instant change, and the integral measures area; what connection can drawn between the two ideas? Is the equation in FTC just purely mechanical? Any ideas on this subject are greatly appreciated; I'm just trying to understand a nice theorem. Also, if anyone can shed light on the meaning of FTC in the broader context of its generalizations (Line Integral, Green's Theorem, Stokes) that would be nice as well.","I am currently taking an advanced Calculus class in college, and we are studying generalizations of the FTC. We just started on the version for Line Integrals, and one can see the explicit symmetry between the 1-dim version and this version. But as the technical meaning of this theorem sank in, I realized I never really understood the meaning of FTC, going back to 1-dim. I understand that FTC creates a bond between the two fundamental studies of calculus, the integral and the derivative. But is there some geometric or tangible meaning to this bond? What does it really mean that the integral and the derivative are ""inverse processes""? Is this in the same exact sense as an inverse function? It would seem that the derivative measures instant change, and the integral measures area; what connection can drawn between the two ideas? Is the equation in FTC just purely mechanical? Any ideas on this subject are greatly appreciated; I'm just trying to understand a nice theorem. Also, if anyone can shed light on the meaning of FTC in the broader context of its generalizations (Line Integral, Green's Theorem, Stokes) that would be nice as well.",,"['calculus', 'integration', 'multivariable-calculus', 'intuition', 'vector-analysis']"
46,"How can I prove this closed form for $\sum_{n=1}^\infty\frac{(4n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}$",How can I prove this closed form for,"\sum_{n=1}^\infty\frac{(4n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}","How can I prove the following conjectured identity? $$\mathcal{S}=\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}\stackrel?=\frac{\sqrt3}{2\,\pi}\left(2\sqrt{\frac8{\sqrt\alpha}-\alpha}-2\sqrt\alpha-3\right),$$ where $$\alpha=2\sqrt[3]{1+\sqrt2}-\frac2{\sqrt[3]{1+\sqrt2}}.$$ The conjecture is equivalent to saying that $\pi\,\mathcal{S}$ is the root of the polynomial $$256 x^8-6912 x^6-814752 x^4-13364784 x^2+531441,$$ belonging to the interval $-1<x<0$. The summand came as a solution to the recurrence relation $$\begin{cases}a(1)=-\frac{81\sqrt3}{512\,\pi}\\\\a(n+1)=-\frac{9\,(2n+1)(4n+1)(4 n+3)}{32\,(n+1)(3n+2)(3n+4)}a(n)\end{cases}.$$ The conjectured closed form was found using computer based on results of numerical summation. The approximate numeric result is $\mathcal{S}=-0.06339748327393640606333225108136874...$ ( click to see 1000 digits ).","How can I prove the following conjectured identity? $$\mathcal{S}=\sum_{n=1}^\infty\frac{(4\,n)!}{\Gamma\left(\frac23+n\right)\,\Gamma\left(\frac43+n\right)\,n!^2\,(-256)^n}\stackrel?=\frac{\sqrt3}{2\,\pi}\left(2\sqrt{\frac8{\sqrt\alpha}-\alpha}-2\sqrt\alpha-3\right),$$ where $$\alpha=2\sqrt[3]{1+\sqrt2}-\frac2{\sqrt[3]{1+\sqrt2}}.$$ The conjecture is equivalent to saying that $\pi\,\mathcal{S}$ is the root of the polynomial $$256 x^8-6912 x^6-814752 x^4-13364784 x^2+531441,$$ belonging to the interval $-1<x<0$. The summand came as a solution to the recurrence relation $$\begin{cases}a(1)=-\frac{81\sqrt3}{512\,\pi}\\\\a(n+1)=-\frac{9\,(2n+1)(4n+1)(4 n+3)}{32\,(n+1)(3n+2)(3n+4)}a(n)\end{cases}.$$ The conjectured closed form was found using computer based on results of numerical summation. The approximate numeric result is $\mathcal{S}=-0.06339748327393640606333225108136874...$ ( click to see 1000 digits ).",,"['calculus', 'sequences-and-series', 'closed-form', 'conjectures', 'hypergeometric-function']"
47,"Which of the numbers $1, 2^{1/2}, 3^{1/3}, 4^{1/4}, 5^{1/5}, 6^{1/6} , 7^{1/7}$ is largest, and how to find out without calculator? [closed]","Which of the numbers  is largest, and how to find out without calculator? [closed]","1, 2^{1/2}, 3^{1/3}, 4^{1/4}, 5^{1/5}, 6^{1/6} , 7^{1/7}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $1, 2^{1/2}, 3^{1/3}, 4^{1/4}, 5^{1/5}, 6^{1/6} , 7^{1/7}$ . I got this question in an Application of Derivatives test. I think log might be used here to compare the values, but even then the values are very close to each other and differ by less than 0.02, which makes it difficult to get some specific answer to this question. How to solve this by a definite method? Source: ISI entrance exam","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question . I got this question in an Application of Derivatives test. I think log might be used here to compare the values, but even then the values are very close to each other and differ by less than 0.02, which makes it difficult to get some specific answer to this question. How to solve this by a definite method? Source: ISI entrance exam","1, 2^{1/2}, 3^{1/3}, 4^{1/4}, 5^{1/5}, 6^{1/6} , 7^{1/7}","['calculus', 'inequality', 'derivatives', 'radicals']"
48,What's wrong with l'Hopital's rule?,What's wrong with l'Hopital's rule?,,"Upon looking at yet another question on this site on evaluating a limit explicitly without l'Hopital's rule, I remembered that one of my professors once said something to the effect that in Europe (where he is from) l'Hopital's rule isn't ""overused"" like it is here in the USA. My question is, is there some reason not to use l'Hopital's rule when you have an indeterminate form?  I know other techniques but l'Hopital is certainly my go-to.  Is there some reason for hostility toward l'Hopital's rule?","Upon looking at yet another question on this site on evaluating a limit explicitly without l'Hopital's rule, I remembered that one of my professors once said something to the effect that in Europe (where he is from) l'Hopital's rule isn't ""overused"" like it is here in the USA. My question is, is there some reason not to use l'Hopital's rule when you have an indeterminate form?  I know other techniques but l'Hopital is certainly my go-to.  Is there some reason for hostility toward l'Hopital's rule?",,"['calculus', 'limits', 'soft-question', 'limits-without-lhopital']"
49,Universal Chord Theorem,Universal Chord Theorem,,"Let $f \in C[0,1]$ and $f(0)=f(1)$. How do we prove $\exists a \in [0,1/2]$ such that $f(a)=f(a+1/2)$? In fact, for every positive integer $n$, there is some $a$, such that $f(a) = f(a+\frac{1}{n})$. For any other non-zero real $r$ (i.e not of the form $\frac{1}{n}$), there is a continuous function $f \in C[0,1]$, such that $f(0) = f(1)$ and $f(a) \neq f(a+r)$ for any $a$. This is called the Universal Chord Theorem and is due to Paul Levy. Note: the accepted answer answers only the first question, so please read the other answers too, and also this answer by Arturo to a different question: https://math.stackexchange.com/a/113471/1102 This is being repurposed in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions . and here: List of abstract duplicates .","Let $f \in C[0,1]$ and $f(0)=f(1)$. How do we prove $\exists a \in [0,1/2]$ such that $f(a)=f(a+1/2)$? In fact, for every positive integer $n$, there is some $a$, such that $f(a) = f(a+\frac{1}{n})$. For any other non-zero real $r$ (i.e not of the form $\frac{1}{n}$), there is a continuous function $f \in C[0,1]$, such that $f(0) = f(1)$ and $f(a) \neq f(a+r)$ for any $a$. This is called the Universal Chord Theorem and is due to Paul Levy. Note: the accepted answer answers only the first question, so please read the other answers too, and also this answer by Arturo to a different question: https://math.stackexchange.com/a/113471/1102 This is being repurposed in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions . and here: List of abstract duplicates .",,"['calculus', 'faq']"
50,Why do all elementary functions have an elementary derivative?,Why do all elementary functions have an elementary derivative?,,"Considering many elementary functions have an antiderivative which is not elementary, why does this type of thing not also happen in differential calculus?","Considering many elementary functions have an antiderivative which is not elementary, why does this type of thing not also happen in differential calculus?",,"['calculus', 'derivatives', 'elementary-functions']"
51,Why is $\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x =0$?,Why is ?,\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x =0,"We had our final exam yesterday and one of the questions was to find out the value of: $$\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x $$ Interestingly enough, using the substitution $x=\frac{1}{t}$ we get - $$-\int_{0}^{1} \frac {\ln x}{1+x^2} \mathrm{d}x = \int_{1}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x $$and therefore $\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x = 0 $ I was curious to know about the theory behind this interesting (surprising even!) example. Thank you.","We had our final exam yesterday and one of the questions was to find out the value of: $$\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x $$ Interestingly enough, using the substitution $x=\frac{1}{t}$ we get - $$-\int_{0}^{1} \frac {\ln x}{1+x^2} \mathrm{d}x = \int_{1}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x $$and therefore $\int_{0}^{\infty} \frac {\ln x}{1+x^2} \mathrm{d}x = 0 $ I was curious to know about the theory behind this interesting (surprising even!) example. Thank you.",,"['calculus', 'integration', 'improper-integrals']"
52,What is the difference between an indefinite integral and an antiderivative?,What is the difference between an indefinite integral and an antiderivative?,,"I thought these were different words for the same thing, but it seems I am wrong. Help.","I thought these were different words for the same thing, but it seems I am wrong. Help.",,"['calculus', 'terminology']"
53,What's the difference between early transcendentals and late transcendentals?,What's the difference between early transcendentals and late transcendentals?,,"Anton, Bivens, and Davis have written calculus books with late transcendentals and Stewart has a calculus book with early transcendentals. What's this all about? edit 1: (Both terms show up in the titles of the books.) Here's the Anton, Bivens, Davis book: https://www.wileyplus.com/WileyCDA/Section/Calculus-Late-Transcendentals-10e.id-813274.html Here's a link to an amazon page selling the Stewart calculus book: https://www.google.com/shopping/product/7326832019121784605?q=james+stewart+calculus+early+transcendentals&prds=hsec:specs&ved=0CAUQ4Ss&ei=ovplUsfDNceBiwLSm4DYDA","Anton, Bivens, and Davis have written calculus books with late transcendentals and Stewart has a calculus book with early transcendentals. What's this all about? edit 1: (Both terms show up in the titles of the books.) Here's the Anton, Bivens, Davis book: https://www.wileyplus.com/WileyCDA/Section/Calculus-Late-Transcendentals-10e.id-813274.html Here's a link to an amazon page selling the Stewart calculus book: https://www.google.com/shopping/product/7326832019121784605?q=james+stewart+calculus+early+transcendentals&prds=hsec:specs&ved=0CAUQ4Ss&ei=ovplUsfDNceBiwLSm4DYDA",,"['calculus', 'soft-question', 'terminology']"
54,"Closed form for $\int_0^\infty\frac{\log\left(1+\frac{\pi^2}{4\,x}\right)}{e^{\sqrt{x}}-1}\mathrm dx$",Closed form for,"\int_0^\infty\frac{\log\left(1+\frac{\pi^2}{4\,x}\right)}{e^{\sqrt{x}}-1}\mathrm dx","I encountered this integral in my calculations: $$\int_0^\infty\frac{\log\left(1+\frac{\pi^2}{4\,x}\right)}{e^{\sqrt{x}}-1}\mathrm dx=2\int_0^\infty\frac{x\log\left(1+\frac{\pi^2}{4\,x^2}\right)}{e^x-1}\mathrm dx=6.041880938342236884944983747836284...,$$ but could not find a closed-form representation for it. I tried to replace a constant factor $\frac{\pi^2}4$ with a parameter and take a derivative, that made the integral look simpler, but I still was not successful in solving it. I also tried to find possible closed forms using Inverse Symbolic Calculator and WolframAlpha but they did not find anything. Could you please help me to find a closed form (even using non-elementary special functions), if it exists?","I encountered this integral in my calculations: $$\int_0^\infty\frac{\log\left(1+\frac{\pi^2}{4\,x}\right)}{e^{\sqrt{x}}-1}\mathrm dx=2\int_0^\infty\frac{x\log\left(1+\frac{\pi^2}{4\,x^2}\right)}{e^x-1}\mathrm dx=6.041880938342236884944983747836284...,$$ but could not find a closed-form representation for it. I tried to replace a constant factor $\frac{\pi^2}4$ with a parameter and take a derivative, that made the integral look simpler, but I still was not successful in solving it. I also tried to find possible closed forms using Inverse Symbolic Calculator and WolframAlpha but they did not find anything. Could you please help me to find a closed form (even using non-elementary special functions), if it exists?",,"['calculus', 'integration', 'logarithms', 'definite-integrals', 'closed-form']"
55,Finding the largest equilateral triangle inside a given triangle,Finding the largest equilateral triangle inside a given triangle,,"My wife came up with the following problem while we were making some decorations for our baby: given a triangle, what is the largest equilateral triangle that can be inscribed in it? (In other words: given a triangular piece of cardboard, what is the largest equilateral triangle that you can cut out of it?) She also came up with the following heuristic/conjecture: take the largest angle of the given triangle. (It is guaranteed to be at least $60^\circ = \pi/3$, as not all three angles of a triangle can be less than that.) Now the answer (the largest inscribed equilateral triangle) can be found among those made by marking off a $60^\circ$ angle at that vertex, with the two ends chosen somehow. (In other words: the inscribed equilateral triangle can be chosen to have that vertex as one of its vertices.) My intuition for geometry is not so good. :-) I played with a few examples in Geogebra and couldn't find any counterexamples, nor could I think of a proof, so I'm asking here. This is similar to Find the maximum area possible of equilateral triangle that inside the given square and a special case of Largest Equilateral Triangle in a Polygon (whose paper I don't have access to—all I could find via citations to that paper is that it gives an $O(n^3)$ algorithm—and in any case the problem may be simpler for a triangle). Questions: How can one find the largest equilateral triangle that can be inscribed in a given triangle? Can such a triangle always be found with one vertex at one of the vertices of the given triangle, specifically the one with the largest angle? (If the answer to the above is no) When is the above true? For instance, is the conjecture true when the triangle is isosceles, with the two sides adjacent to the largest angle equal?","My wife came up with the following problem while we were making some decorations for our baby: given a triangle, what is the largest equilateral triangle that can be inscribed in it? (In other words: given a triangular piece of cardboard, what is the largest equilateral triangle that you can cut out of it?) She also came up with the following heuristic/conjecture: take the largest angle of the given triangle. (It is guaranteed to be at least $60^\circ = \pi/3$, as not all three angles of a triangle can be less than that.) Now the answer (the largest inscribed equilateral triangle) can be found among those made by marking off a $60^\circ$ angle at that vertex, with the two ends chosen somehow. (In other words: the inscribed equilateral triangle can be chosen to have that vertex as one of its vertices.) My intuition for geometry is not so good. :-) I played with a few examples in Geogebra and couldn't find any counterexamples, nor could I think of a proof, so I'm asking here. This is similar to Find the maximum area possible of equilateral triangle that inside the given square and a special case of Largest Equilateral Triangle in a Polygon (whose paper I don't have access to—all I could find via citations to that paper is that it gives an $O(n^3)$ algorithm—and in any case the problem may be simpler for a triangle). Questions: How can one find the largest equilateral triangle that can be inscribed in a given triangle? Can such a triangle always be found with one vertex at one of the vertices of the given triangle, specifically the one with the largest angle? (If the answer to the above is no) When is the above true? For instance, is the conjecture true when the triangle is isosceles, with the two sides adjacent to the largest angle equal?",,"['calculus', 'geometry', 'optimization', 'algorithms', 'maxima-minima']"
56,Is there a chain rule for integration?,Is there a chain rule for integration?,,"I know the chain rule for derivatives. The way as I apply it, is to get rid of specific 'bits' of a complex equation in stages, i.e I will derive the $5$th root first in the equation $(2x+3)^5$ and continue with the rest. I wonder if there is something similar with integration. I tried to integrate that way $(2x+3)^5$ but it doesn't seem to work. Well, it works in the first stage, i.e it's fine to raise in the power of $6$ and divide with $6$ to get rid of the power $5$, but afterwards, if we would apply the chain rule, we should multiply by the integral of $2x+3$!, But it doesn't work like that, we just need to multiply by $1/2$ and that's it. So my question is, is there chain rule for integrals? I want to be able to calculate integrals of complex equations as easy as I do with chain rule for derivatives.","I know the chain rule for derivatives. The way as I apply it, is to get rid of specific 'bits' of a complex equation in stages, i.e I will derive the $5$th root first in the equation $(2x+3)^5$ and continue with the rest. I wonder if there is something similar with integration. I tried to integrate that way $(2x+3)^5$ but it doesn't seem to work. Well, it works in the first stage, i.e it's fine to raise in the power of $6$ and divide with $6$ to get rid of the power $5$, but afterwards, if we would apply the chain rule, we should multiply by the integral of $2x+3$!, But it doesn't work like that, we just need to multiply by $1/2$ and that's it. So my question is, is there chain rule for integrals? I want to be able to calculate integrals of complex equations as easy as I do with chain rule for derivatives.",,"['calculus', 'integration']"
57,Is it possible for the derivative of a function to grow arbitrarily faster than the function itself?,Is it possible for the derivative of a function to grow arbitrarily faster than the function itself?,,"We know that there exist some functions $f(x)$ such that their derivative $f'(x)$ is strictly greater than the function itself. for example the function $5^x$ has a derivative $5^x\ln(5)$ which is greater than $5^x$ . Exponential functions in general are known to be proportional to their derivatives. The question I have is whether it is possible for a function to grow ""even faster"" than this. To be more precise let's take the ratio $\frac{f'(x)}{f(x)}$ for exponential functions this ratio is a constant. For most elementary functions we care about, this ratio usually tends to $0$ . But are there functions for which this ratio grows arbitrarily large? If so, is there an upper limit for how large the ratio $\frac{f'(x)}{f(x)}$ can grow? I also ask a similar question for integrals.","We know that there exist some functions such that their derivative is strictly greater than the function itself. for example the function has a derivative which is greater than . Exponential functions in general are known to be proportional to their derivatives. The question I have is whether it is possible for a function to grow ""even faster"" than this. To be more precise let's take the ratio for exponential functions this ratio is a constant. For most elementary functions we care about, this ratio usually tends to . But are there functions for which this ratio grows arbitrarily large? If so, is there an upper limit for how large the ratio can grow? I also ask a similar question for integrals.",f(x) f'(x) 5^x 5^x\ln(5) 5^x \frac{f'(x)}{f(x)} 0 \frac{f'(x)}{f(x)},"['calculus', 'integration', 'functions', 'derivatives']"
58,Intuition behind this interesting calculus result?,Intuition behind this interesting calculus result?,,"So, $$\lim_{A\rightarrow \infty}\int^A_1 \frac{\ln x}{x} dx = \infty$$ All good. But, if you take the volume of revolution of the curve, rotated about the $x$ axis $2\pi$ radians, from $1$ to $A$, the volume remains finite as $A \rightarrow \infty$ (it approaches $2\pi$): $$\lim_{A\rightarrow \infty}\pi\int^A_1 \left(\frac{\ln x}{x}\right)^2 dx = 2\pi$$ Pretty cool result, can anyone help me get my head round this intuitively? You'd assume an infinite area rotated around the x axis would produce an infinite volume... right?","So, $$\lim_{A\rightarrow \infty}\int^A_1 \frac{\ln x}{x} dx = \infty$$ All good. But, if you take the volume of revolution of the curve, rotated about the $x$ axis $2\pi$ radians, from $1$ to $A$, the volume remains finite as $A \rightarrow \infty$ (it approaches $2\pi$): $$\lim_{A\rightarrow \infty}\pi\int^A_1 \left(\frac{\ln x}{x}\right)^2 dx = 2\pi$$ Pretty cool result, can anyone help me get my head round this intuitively? You'd assume an infinite area rotated around the x axis would produce an infinite volume... right?",,"['calculus', 'improper-integrals', 'intuition']"
59,Computing $\zeta(6)=\sum\limits_{k=1}^\infty \frac1{k^6}$ with Fourier series.,Computing  with Fourier series.,\zeta(6)=\sum\limits_{k=1}^\infty \frac1{k^6},"Let $ f$ be a function such that $ f\in C_{2\pi}^{0}(\mathbb{R},\mathbb{R}) $ (f is $2\pi$-periodic) such that $ \forall x \in [0,\pi]$: $$f(x)=x(\pi-x)$$ Computing the Fourier series of $f$ and using Parseval's identity, I have computed $\zeta(2)$ and $\zeta(4)$. How can I compute $ \zeta(6) $ now? Fourier series of $ f $: $$ S(f)= \frac{\pi^2}{6}-\sum_{n=1}^{\infty} \frac{\cos(2nx)}{n^2}$$ $$ x=0,  \zeta(2)=\pi^2/6$$","Let $ f$ be a function such that $ f\in C_{2\pi}^{0}(\mathbb{R},\mathbb{R}) $ (f is $2\pi$-periodic) such that $ \forall x \in [0,\pi]$: $$f(x)=x(\pi-x)$$ Computing the Fourier series of $f$ and using Parseval's identity, I have computed $\zeta(2)$ and $\zeta(4)$. How can I compute $ \zeta(6) $ now? Fourier series of $ f $: $$ S(f)= \frac{\pi^2}{6}-\sum_{n=1}^{\infty} \frac{\cos(2nx)}{n^2}$$ $$ x=0,  \zeta(2)=\pi^2/6$$",,"['calculus', 'sequences-and-series', 'analysis', 'fourier-series', 'riemann-zeta']"
60,Why do we require radians in calculus?,Why do we require radians in calculus?,,"I think this is just something I've grown used to but can't remember any proof. When differentiating and integrating with trigonometric functions, we require angles to be taken in radians. Why does it work then and only then?","I think this is just something I've grown used to but can't remember any proof. When differentiating and integrating with trigonometric functions, we require angles to be taken in radians. Why does it work then and only then?",,"['calculus', 'integration', 'trigonometry', 'derivatives', 'unit-of-measure']"
61,Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$,Vector derivative w.r.t its transpose,\frac{d(Ax)}{d(x^T)},"Given a matrix $A$ and column vector $x$ , what is the derivative of $Ax$ with respect to $x^T$ i.e. $\frac{d(Ax)}{d(x^T)}$ , where $x^T$ is the transpose of $x$ ? Side note - my goal is to get the known derivative formula $\frac{d(x^TAx)}{dx} = x^T(A^T + A)$ from the above rule and the chain rule.","Given a matrix and column vector , what is the derivative of with respect to i.e. , where is the transpose of ? Side note - my goal is to get the known derivative formula from the above rule and the chain rule.",A x Ax x^T \frac{d(Ax)}{d(x^T)} x^T x \frac{d(x^TAx)}{dx} = x^T(A^T + A),"['calculus', 'matrices', 'multivariable-calculus', 'matrix-calculus']"
62,"A conjectural closed form for $\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!}$",A conjectural closed form for,"\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!}","Let $$S=\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!},\tag1$$ its numeric value is approximately $S \approx 0.517977853388534047...$ ${}^{[more\ digits]}$ $S$ can be represented in terms of the generalized hypergeometric function : $$S={_3F_2}\left(\frac12,1,1;\ \frac43,\frac53;\ \frac4{27}\right)\cdot\frac12.\tag2$$ Let $\sigma$ be the closed-form expression constructed from integers and elementary functions as follows: $$\sigma=3\,\alpha\,\ln(2\,\alpha+1)-\sqrt{\beta\,}\arccos\gamma,\tag3$$ where $$\alpha=\frac{\sqrt[3]{3\,}}{6\,\sqrt[3]{2\,}}\left(\sqrt[3]{9-\sqrt{69}}+\sqrt[3]{9+\sqrt{69}}\right),\tag4$$ $$\beta=\frac1{4\,\sqrt[3]{2\,}}\left(\sqrt[3]{25+3\,\sqrt{69}}+\sqrt[3]{25-3\,\sqrt{69}}\right)-\frac12,\tag5$$ $$\gamma=\frac1{6\,\sqrt[3]{2\,}}\left(\sqrt[3]{57\,\sqrt{69}-459}-\sqrt[3]{57\,\sqrt{69}+459}\right)+\frac12\tag6$$ are the unique real roots of the following cubic equations: $$8\,\alpha^3-2\,\alpha-1=0,\tag7$$ $$64\,\beta^3+96\,\beta^2+36\,\beta-23=0,\tag8$$ $$8\,\gamma^3-12\,\gamma^2+16\,\gamma+11=0.\tag9$$ Equivalently, $$\sigma = \frac{3\,p}{2}\,\ln\big(p+1\big)-\frac{1}{2}\sqrt{\frac{3-p}{p}}\arccos\Big( \frac{p-6}{6p+2}\Big)\tag{10}$$ where $p$ is the plastic constant or the real root of $$p^3-p-1=0\tag{11}$$ It can be numerically checked that the following inequality holds: $$\Big|S-\sigma\Big|<10^{-10^5},\tag{12}$$ I conjecture that the actual difference is the exact zero, and thus $S$ has an elementary closed form: $$\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!}\stackrel?=3\,\alpha\,\ln(2\,\alpha+1)-\sqrt{\beta\,}\arccos\gamma,\tag{13}$$ I am asking for you help in proving this conjecture.","Let $$S=\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!},\tag1$$ its numeric value is approximately $S \approx 0.517977853388534047...$ ${}^{[more\ digits]}$ $S$ can be represented in terms of the generalized hypergeometric function : $$S={_3F_2}\left(\frac12,1,1;\ \frac43,\frac53;\ \frac4{27}\right)\cdot\frac12.\tag2$$ Let $\sigma$ be the closed-form expression constructed from integers and elementary functions as follows: $$\sigma=3\,\alpha\,\ln(2\,\alpha+1)-\sqrt{\beta\,}\arccos\gamma,\tag3$$ where $$\alpha=\frac{\sqrt[3]{3\,}}{6\,\sqrt[3]{2\,}}\left(\sqrt[3]{9-\sqrt{69}}+\sqrt[3]{9+\sqrt{69}}\right),\tag4$$ $$\beta=\frac1{4\,\sqrt[3]{2\,}}\left(\sqrt[3]{25+3\,\sqrt{69}}+\sqrt[3]{25-3\,\sqrt{69}}\right)-\frac12,\tag5$$ $$\gamma=\frac1{6\,\sqrt[3]{2\,}}\left(\sqrt[3]{57\,\sqrt{69}-459}-\sqrt[3]{57\,\sqrt{69}+459}\right)+\frac12\tag6$$ are the unique real roots of the following cubic equations: $$8\,\alpha^3-2\,\alpha-1=0,\tag7$$ $$64\,\beta^3+96\,\beta^2+36\,\beta-23=0,\tag8$$ $$8\,\gamma^3-12\,\gamma^2+16\,\gamma+11=0.\tag9$$ Equivalently, $$\sigma = \frac{3\,p}{2}\,\ln\big(p+1\big)-\frac{1}{2}\sqrt{\frac{3-p}{p}}\arccos\Big( \frac{p-6}{6p+2}\Big)\tag{10}$$ where $p$ is the plastic constant or the real root of $$p^3-p-1=0\tag{11}$$ It can be numerically checked that the following inequality holds: $$\Big|S-\sigma\Big|<10^{-10^5},\tag{12}$$ I conjecture that the actual difference is the exact zero, and thus $S$ has an elementary closed form: $$\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!}\stackrel?=3\,\alpha\,\ln(2\,\alpha+1)-\sqrt{\beta\,}\arccos\gamma,\tag{13}$$ I am asking for you help in proving this conjecture.",,"['calculus', 'sequences-and-series', 'closed-form', 'conjectures', 'hypergeometric-function']"
63,Integral $\int_0^{\pi/2}\arctan^2\left(\frac{6\sin x}{3+\cos 2x}\right)\mathrm dx$,Integral,\int_0^{\pi/2}\arctan^2\left(\frac{6\sin x}{3+\cos 2x}\right)\mathrm dx,Is it possible to evaluate this integral in a closed form? $$I=\int_0^{\pi/2}\arctan^2\left(\frac{6\sin x}{3+\cos 2x}\right)\mathrm dx$$,Is it possible to evaluate this integral in a closed form? $$I=\int_0^{\pi/2}\arctan^2\left(\frac{6\sin x}{3+\cos 2x}\right)\mathrm dx$$,,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
64,Dirac Delta Function of a Function,Dirac Delta Function of a Function,,"I'm trying to show that $$\delta\big(f(x)\big) = \sum_{i}\frac{\delta(x-a_{i})}{\left|{\frac{df}{dx}(a_{i})}\right|}$$ Where $a_{i}$ are the roots of the function $f(x)$. I've tried to proceed by using a dummy function $g(x)$ and carrying out: $$\int_{-\infty}^{\infty}dx\,\delta\big(f(x)\big)g(x)$$ Then making the coordinate substitution $u$ = $f(x)$ and integrating over $u$. This seems to be on the right track, but I'm unsure where the absolute value comes in in the denominator, and also why it becomes a sum. $$\int_{-\infty}^{\infty}\frac{du}{\frac{df}{dx}}\delta(u)g\big(f^{-1}(u)\big) = \frac{g\big(f^{-1}(0)\big)}{\frac{df}{dx}\big(f^{-1}(0)\big)}$$ Can any one shed some light? Wikipedia just states the formula and doesn't actually show where it comes from.","I'm trying to show that $$\delta\big(f(x)\big) = \sum_{i}\frac{\delta(x-a_{i})}{\left|{\frac{df}{dx}(a_{i})}\right|}$$ Where $a_{i}$ are the roots of the function $f(x)$. I've tried to proceed by using a dummy function $g(x)$ and carrying out: $$\int_{-\infty}^{\infty}dx\,\delta\big(f(x)\big)g(x)$$ Then making the coordinate substitution $u$ = $f(x)$ and integrating over $u$. This seems to be on the right track, but I'm unsure where the absolute value comes in in the denominator, and also why it becomes a sum. $$\int_{-\infty}^{\infty}\frac{du}{\frac{df}{dx}}\delta(u)g\big(f^{-1}(u)\big) = \frac{g\big(f^{-1}(0)\big)}{\frac{df}{dx}\big(f^{-1}(0)\big)}$$ Can any one shed some light? Wikipedia just states the formula and doesn't actually show where it comes from.",,"['calculus', 'physics']"
65,How do Lagrange multipliers work to find the lowest value of a function subject to a constraint?,How do Lagrange multipliers work to find the lowest value of a function subject to a constraint?,,"I have been using Lagrange multipliers in constrained optimization problems, but I don't see how they actually work to simultaneously satisfy the constraint and find the lowest possible value of an objective function.","I have been using Lagrange multipliers in constrained optimization problems, but I don't see how they actually work to simultaneously satisfy the constraint and find the lowest possible value of an objective function.",,"['calculus', 'optimization', 'lagrange-multiplier']"
66,Why is there antagonism towards extended real numbers?,Why is there antagonism towards extended real numbers?,,"In my backstory, I was introduced to the geometric concept of infinity rather young, through reading about the inversive plane. In the course of learning calculus, I'm pretty sure I formed a concept of $\pm \infty$ lying at the endpoints of the real line (although they weren't real numbers themselves), and understood limits in terms of that. By the time I was introduced to the extended real numbers, it was simply putting a name to the prior concept, and providing a framework to work with them in a clear and precise fashion. (and similarly for the projective real numbers) Fast forward 20 years later, and through interactions with people here at MSE, I find there is a lot of antagonism towards the concept of the extended real numbers. I don't mean things like ""it would be confusing to teach them in introductory calculus"" -- I mean things like ""the extended reals $\pm \infty$ are best thought of in terms of limits rather than as actual points"" or even ""thou shalt not develop a concept of $x$ approaching something as $x \to +\infty$"" as well as some patently false claims (e.g. ""$+\infty$ cannot be a mathematical object; it can merely be a a 'concept'""). I had previously brushed off those opinions, but they seem pervasive enough that I felt I should ask the titular question: is there any good reason for this antagonism? Or is there any good reason to avoid understanding calculus in terms of the extended reals (when they are suitable objects to do so)?","In my backstory, I was introduced to the geometric concept of infinity rather young, through reading about the inversive plane. In the course of learning calculus, I'm pretty sure I formed a concept of $\pm \infty$ lying at the endpoints of the real line (although they weren't real numbers themselves), and understood limits in terms of that. By the time I was introduced to the extended real numbers, it was simply putting a name to the prior concept, and providing a framework to work with them in a clear and precise fashion. (and similarly for the projective real numbers) Fast forward 20 years later, and through interactions with people here at MSE, I find there is a lot of antagonism towards the concept of the extended real numbers. I don't mean things like ""it would be confusing to teach them in introductory calculus"" -- I mean things like ""the extended reals $\pm \infty$ are best thought of in terms of limits rather than as actual points"" or even ""thou shalt not develop a concept of $x$ approaching something as $x \to +\infty$"" as well as some patently false claims (e.g. ""$+\infty$ cannot be a mathematical object; it can merely be a a 'concept'""). I had previously brushed off those opinions, but they seem pervasive enough that I felt I should ask the titular question: is there any good reason for this antagonism? Or is there any good reason to avoid understanding calculus in terms of the extended reals (when they are suitable objects to do so)?",,['calculus']
67,When to Stop Using L'Hôpital's Rule,When to Stop Using L'Hôpital's Rule,,"I don't understand something about L'Hôpital's rule. In this case: $$ \begin{align} & {{}\phantom{=}}\lim_{x\to0}\frac{e^x-1-x^2}{x^4+x^3+x^2} \\[8pt] & =\lim_{x\to0}\frac{(e^x-1-x^2)'}{(x^4+x^3+x^2)'} \\[8pt] & =\lim_{x\to0}\frac{(e^x-2x)'}{(4x^3+3x^2+2x)'} \\[8pt] & =\lim_{x\to0}\frac{(e^x-2)'}{(12x^2+6x+2)'} \\[8pt] & = \lim_{x\to0}\frac{(e^x)'}{(24x+6)'} \\[8pt] & = \lim_{x\to0}\frac{e^x}{24} \\[8pt] & = \frac{e^0}{24} \\[8pt] & = \frac{1}{24} \end{align} $$ Why do we have to keep on solving after this step: $$\displaystyle\lim_{x\to0}\dfrac{(e^x-2)'}{(12x^2+6x+2)'}$$ Can't I just plug in $x=0$ and compute the limit at this step giving me: $$\dfrac{1-2}{0+0+2}=-\dfrac{1}{2}$$ I'm very confused, because I get different probable answers for the limit, depending on when do I stop to differentiate, as clearly $-\frac1{2}\neq \frac 1{24}$.","I don't understand something about L'Hôpital's rule. In this case: $$ \begin{align} & {{}\phantom{=}}\lim_{x\to0}\frac{e^x-1-x^2}{x^4+x^3+x^2} \\[8pt] & =\lim_{x\to0}\frac{(e^x-1-x^2)'}{(x^4+x^3+x^2)'} \\[8pt] & =\lim_{x\to0}\frac{(e^x-2x)'}{(4x^3+3x^2+2x)'} \\[8pt] & =\lim_{x\to0}\frac{(e^x-2)'}{(12x^2+6x+2)'} \\[8pt] & = \lim_{x\to0}\frac{(e^x)'}{(24x+6)'} \\[8pt] & = \lim_{x\to0}\frac{e^x}{24} \\[8pt] & = \frac{e^0}{24} \\[8pt] & = \frac{1}{24} \end{align} $$ Why do we have to keep on solving after this step: $$\displaystyle\lim_{x\to0}\dfrac{(e^x-2)'}{(12x^2+6x+2)'}$$ Can't I just plug in $x=0$ and compute the limit at this step giving me: $$\dfrac{1-2}{0+0+2}=-\dfrac{1}{2}$$ I'm very confused, because I get different probable answers for the limit, depending on when do I stop to differentiate, as clearly $-\frac1{2}\neq \frac 1{24}$.",,"['calculus', 'limits', 'functions', 'derivatives']"
68,The difference between pointwise convergence and uniform convergence of functional sequences,The difference between pointwise convergence and uniform convergence of functional sequences,,"$f_n$ converges pointwise to $f$ on $E$ if $\forall x \in E$ and $\forall \varepsilon > 0$, $\exists N \in \mathbb N$, such that $\forall n \geq N$ we have $\left|f_n(x) − f(x)\right| < \varepsilon$. $f_n$ converges uniformly to $f$ on $E$ if  $\forall \varepsilon > 0$, $\exists N \in \mathbb{N}$ so that $\forall n \geq N$ we have $\left|f_n(x) − f(x)\right| < \varepsilon$ which holds for all $x \in E$. I know the difference in definition, pointwise convergence tells us that for each point and each epsilon, we can find an $N$ (which depends from $x$ and $\varepsilon$)so that ... and the uniform convergence tells us that for each $\varepsilon$ we can find a number $N$ (which depends only from $\varepsilon$) s.t. ... . But it seems again to me a bit fuzzy. Can somebody explain the difference more 'deeper' or more 'philosophically'? Or may be illustrate it visually?","$f_n$ converges pointwise to $f$ on $E$ if $\forall x \in E$ and $\forall \varepsilon > 0$, $\exists N \in \mathbb N$, such that $\forall n \geq N$ we have $\left|f_n(x) − f(x)\right| < \varepsilon$. $f_n$ converges uniformly to $f$ on $E$ if  $\forall \varepsilon > 0$, $\exists N \in \mathbb{N}$ so that $\forall n \geq N$ we have $\left|f_n(x) − f(x)\right| < \varepsilon$ which holds for all $x \in E$. I know the difference in definition, pointwise convergence tells us that for each point and each epsilon, we can find an $N$ (which depends from $x$ and $\varepsilon$)so that ... and the uniform convergence tells us that for each $\varepsilon$ we can find a number $N$ (which depends only from $\varepsilon$) s.t. ... . But it seems again to me a bit fuzzy. Can somebody explain the difference more 'deeper' or more 'philosophically'? Or may be illustrate it visually?",,"['calculus', 'analysis']"
69,Is an integral always continuous?,Is an integral always continuous?,,"Say I have a function $f(x)$ on some interval $[a,b]$.  Say it is integrable such that $\displaystyle\int f(x)~dx $ is defined. Is $\displaystyle\int f(x)~dx $ necessarily continuous?  If I were to know that the integral is integrable itself such that $\displaystyle\int \int f(x)~dx $ is defined. Would that change anything? If so why? Thank you","Say I have a function $f(x)$ on some interval $[a,b]$.  Say it is integrable such that $\displaystyle\int f(x)~dx $ is defined. Is $\displaystyle\int f(x)~dx $ necessarily continuous?  If I were to know that the integral is integrable itself such that $\displaystyle\int \int f(x)~dx $ is defined. Would that change anything? If so why? Thank you",,"['calculus', 'integration', 'continuity']"
70,The Absolute Value in the Integral of $1/x$,The Absolute Value in the Integral of,1/x,$$\int\frac{1}{x}dx=\ln| x |+C$$ Why the absolute value? Why is the following not valid: $$\int\frac{1}{x}dx=\ln x+C$$,$$\int\frac{1}{x}dx=\ln| x |+C$$ Why the absolute value? Why is the following not valid: $$\int\frac{1}{x}dx=\ln x+C$$,,"['calculus', 'integration']"
71,What other tricks and techniques can I use in integration?,What other tricks and techniques can I use in integration?,,"So far, I know and can use a reasonable number of 'tricks' or techniques when I solve integrals. Below are the tricks/techniques that I know for indefinite and definite integrals separately. Indefinite integrals Standard integrals, such as those of polynomial, trigonometric, logarithmic and exponential functions, including usage of trig identies. Basic substitution. Weierstrass and Euler substitutions. Integration by parts. $$\int\frac{1}{x+x^n}dx=\int\frac{x^{-n}}{1+x^{1-n}}dx=\frac{1}{1-n}\ln\lvert 1+x^{1-n}\rvert+C$$ $$\int\frac{1}{x^{\frac{a+b}{a+b}}\cdot x^{\frac{a}{a+b}}+x^{\frac{b}{a+b}}}dx=\int \frac{x^{-\frac{b}{a+b}}}{\left(x^{\frac{a}{a+b}}\right)^2+1}dx=\arctan x^{\frac{a}{a+b}}+C$$ Substitution $u=\frac{1-x}{1+x}$ for integrals involving $\ln$ and/or the bounds $0$ and $1$ . Reduction formulae. $$\int e^x(f(x)+f'(x))dx=e^xf(x)+C$$ Writing $\sin$ 's and $\cos$ 's as complex exponentials. $$\int\frac{a\sin x+b\cos x}{c\sin x+d\cos x}dx=Ax+B\ln\lvert c\sin x+d\cos x\rvert+C$$ where $$A=\frac{ac+bd}{c^2+d^2}~~~B=\frac{bc-ad}{c^2+d^2}$$ which can be found using simultaneous equations. Definite integrals Differentiation under the integral sign ('Feynman's technique') $$\int_a^b f(x)dx=\int_a^bf(a+b-x)dx$$ Usage of power series to evaluate integrals such as $\int_0^1\frac{\ln(1-x)}{x}dx$ and the like. Making use of even or odd function properties. (My newest personal favourite) For even functions $f(x)$ and $g(x)$ , and an odd function $h(x)$ : $$\int_{-a}^a\frac{f(x)}{1\pm g(x)^{h(x)}}dx=\int_{0}^a f(x)~dx$$ which allows us to evaluate wonderful things like $$\int_{-\infty}^{\infty}\frac{e^{-x^2}}{1+\pi^{\sin x} }dx=\frac{\sqrt{\pi}}{2}$$ Question: Do you know any other integration techniques or tricks that I can use whose usage don't rely on anything beyond high school calculus * or perhaps the first year of a Mathematics degree course? I know that a similar question has been asked here and here but I've looked through them and nothing beyond what I have written above was mentioned, apart from some techniques I couldn't understand such as residue calculus and contour integrals. Many thanks for your help. * Roughly what I mean by high school level calculus: INCLUDED Integration of polynomials and the basic trigonometric functions, such as $\sin x$ , $\cos x$ , $\tan x$ , $\sec x$ , $\operatorname{cosec} x$ , $\cot x$ , $\sec^2 x$ , $\sec x\tan x$ , $\operatorname{cosec} x\cot x$ , $\operatorname{cosec}^2 x.$ Integration of all $x^n$ including $n=1$ . Integration of exponentials. Integration by parts. Integration using substitution, such as using trigonometric/hyperbolic substitutions, and Weierstrass and Euler substitutions (this also includes integration by 'inspection' which is really just substitution but when the individual doesn't need to substitute anything). Integration using partial fractions and logarithms, such as $\int\frac{f'(x)}{f(x)}dx$ . Reduction formulae. Ability to understand and use the concepts of even and odd functions in integration. Improper integrals. Integrating which results in elementary functions. NOT INCLUDED Fourier, Laplace and Mellin transforms. Indefinite integrals that include non-elementary functions in the solution. Contour integration. Residue calculus and similar methods.","So far, I know and can use a reasonable number of 'tricks' or techniques when I solve integrals. Below are the tricks/techniques that I know for indefinite and definite integrals separately. Indefinite integrals Standard integrals, such as those of polynomial, trigonometric, logarithmic and exponential functions, including usage of trig identies. Basic substitution. Weierstrass and Euler substitutions. Integration by parts. Substitution for integrals involving and/or the bounds and . Reduction formulae. Writing 's and 's as complex exponentials. where which can be found using simultaneous equations. Definite integrals Differentiation under the integral sign ('Feynman's technique') Usage of power series to evaluate integrals such as and the like. Making use of even or odd function properties. (My newest personal favourite) For even functions and , and an odd function : which allows us to evaluate wonderful things like Question: Do you know any other integration techniques or tricks that I can use whose usage don't rely on anything beyond high school calculus * or perhaps the first year of a Mathematics degree course? I know that a similar question has been asked here and here but I've looked through them and nothing beyond what I have written above was mentioned, apart from some techniques I couldn't understand such as residue calculus and contour integrals. Many thanks for your help. * Roughly what I mean by high school level calculus: INCLUDED Integration of polynomials and the basic trigonometric functions, such as , , , , , , , , , Integration of all including . Integration of exponentials. Integration by parts. Integration using substitution, such as using trigonometric/hyperbolic substitutions, and Weierstrass and Euler substitutions (this also includes integration by 'inspection' which is really just substitution but when the individual doesn't need to substitute anything). Integration using partial fractions and logarithms, such as . Reduction formulae. Ability to understand and use the concepts of even and odd functions in integration. Improper integrals. Integrating which results in elementary functions. NOT INCLUDED Fourier, Laplace and Mellin transforms. Indefinite integrals that include non-elementary functions in the solution. Contour integration. Residue calculus and similar methods.",\int\frac{1}{x+x^n}dx=\int\frac{x^{-n}}{1+x^{1-n}}dx=\frac{1}{1-n}\ln\lvert 1+x^{1-n}\rvert+C \int\frac{1}{x^{\frac{a+b}{a+b}}\cdot x^{\frac{a}{a+b}}+x^{\frac{b}{a+b}}}dx=\int \frac{x^{-\frac{b}{a+b}}}{\left(x^{\frac{a}{a+b}}\right)^2+1}dx=\arctan x^{\frac{a}{a+b}}+C u=\frac{1-x}{1+x} \ln 0 1 \int e^x(f(x)+f'(x))dx=e^xf(x)+C \sin \cos \int\frac{a\sin x+b\cos x}{c\sin x+d\cos x}dx=Ax+B\ln\lvert c\sin x+d\cos x\rvert+C A=\frac{ac+bd}{c^2+d^2}~~~B=\frac{bc-ad}{c^2+d^2} \int_a^b f(x)dx=\int_a^bf(a+b-x)dx \int_0^1\frac{\ln(1-x)}{x}dx f(x) g(x) h(x) \int_{-a}^a\frac{f(x)}{1\pm g(x)^{h(x)}}dx=\int_{0}^a f(x)~dx \int_{-\infty}^{\infty}\frac{e^{-x^2}}{1+\pi^{\sin x} }dx=\frac{\sqrt{\pi}}{2} \sin x \cos x \tan x \sec x \operatorname{cosec} x \cot x \sec^2 x \sec x\tan x \operatorname{cosec} x\cot x \operatorname{cosec}^2 x. x^n n=1 \int\frac{f'(x)}{f(x)}dx,"['calculus', 'integration', 'soft-question', 'indefinite-integrals', 'big-list']"
72,Closed form of $\prod_{n=1}^\infty\sqrt[2^n]{\frac{\Gamma(2^n+\frac{1}{2})}{\Gamma(2^n)}}$,Closed form of,\prod_{n=1}^\infty\sqrt[2^n]{\frac{\Gamma(2^n+\frac{1}{2})}{\Gamma(2^n)}},Is there a closed form of the following infinite product? $$\prod_{n=1}^\infty\sqrt[2^n]{\frac{\Gamma(2^n+\frac{1}{2})}{\Gamma(2^n)}}$$,Is there a closed form of the following infinite product?,\prod_{n=1}^\infty\sqrt[2^n]{\frac{\Gamma(2^n+\frac{1}{2})}{\Gamma(2^n)}},"['calculus', 'combinatorics', 'gamma-function', 'closed-form', 'infinite-product']"
73,Function that is the sum of all of its derivatives,Function that is the sum of all of its derivatives,,"I have just started learning about differential equations, as a result I started to think about this question but couldn't get anywhere. So I googled and wasn't able to find any particularly helpful results. I am more interested in the reason or method rather than the actual answer. Also I do not know if there even is a solution to this but if there isn't I am just as interested to hear why not. Is there a solution to the differential equation: $$f(x)=\sum_{n=1}^\infty f^{(n)}(x)$$","I have just started learning about differential equations, as a result I started to think about this question but couldn't get anywhere. So I googled and wasn't able to find any particularly helpful results. I am more interested in the reason or method rather than the actual answer. Also I do not know if there even is a solution to this but if there isn't I am just as interested to hear why not. Is there a solution to the differential equation: $$f(x)=\sum_{n=1}^\infty f^{(n)}(x)$$",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
74,Notation: Why write the differential first?,Notation: Why write the differential first?,,"From reading answers here, I've noticed that some people write integrals as $\int dx \; f(x)$, while other people write them as $\int f(x)\;dx$. I realize that there is no mathematical difference between the two notation forms, but was wondering why some people choose the first method over the second.  Is there some place in higher maths that it becomes beneficial to write the differential first? (I, personally, have always used the second method, just because I was taught that way...)","From reading answers here, I've noticed that some people write integrals as $\int dx \; f(x)$, while other people write them as $\int f(x)\;dx$. I realize that there is no mathematical difference between the two notation forms, but was wondering why some people choose the first method over the second.  Is there some place in higher maths that it becomes beneficial to write the differential first? (I, personally, have always used the second method, just because I was taught that way...)",,"['calculus', 'notation']"
75,How to prove a limit exists using the $\epsilon$-$\delta$ definition of a limit,How to prove a limit exists using the - definition of a limit,\epsilon \delta,"I understand how to find a limit.  I understand the concept of the $\epsilon$-$\delta$ definition of a limit.  Can you walk me through what we're doing in this worked example? It is from my student solutions manual to my textbook. I need help understanding what we're saying here, and why. I understand the math expressions, but I do not understand why we chose the ones we did, and why and how they prove anything. Can you help? Find the limit $$ \lim\limits_{x \to 1} \ (x+4) ,$$ and prove it exists using the $\epsilon$-$\delta$ definition of limit. By direct substitution, the limit is $5$. Understood. Now, here's where I start to get confused... Let $\epsilon > 0$ be given. Choose $\delta = \epsilon$. $$ 0 < | x-1 | < \delta = \epsilon .$$ $$ | (x+4) - 5 | < \epsilon $$ $$ | f(x) - L | < \epsilon $$ Proved. Uh, okay, if you say so... Now, what's going on here line by line and term by term?","I understand how to find a limit.  I understand the concept of the $\epsilon$-$\delta$ definition of a limit.  Can you walk me through what we're doing in this worked example? It is from my student solutions manual to my textbook. I need help understanding what we're saying here, and why. I understand the math expressions, but I do not understand why we chose the ones we did, and why and how they prove anything. Can you help? Find the limit $$ \lim\limits_{x \to 1} \ (x+4) ,$$ and prove it exists using the $\epsilon$-$\delta$ definition of limit. By direct substitution, the limit is $5$. Understood. Now, here's where I start to get confused... Let $\epsilon > 0$ be given. Choose $\delta = \epsilon$. $$ 0 < | x-1 | < \delta = \epsilon .$$ $$ | (x+4) - 5 | < \epsilon $$ $$ | f(x) - L | < \epsilon $$ Proved. Uh, okay, if you say so... Now, what's going on here line by line and term by term?",,"['calculus', 'limits', 'epsilon-delta']"
76,"Could you explain why $\frac{d}{dx} e^x = e^x$ ""intuitively""?","Could you explain why  ""intuitively""?",\frac{d}{dx} e^x = e^x,"As the title implies, It is seems that $e^x$ is the only function whoes derivative is the same as itself. thanks.","As the title implies, It is seems that $e^x$ is the only function whoes derivative is the same as itself. thanks.",,"['calculus', 'exponential-function', 'intuition']"
77,Why isn't the Weierstrass function $\sum_{n=0}^\infty a^n \cos(b^n\pi x)$ differentiable?,Why isn't the Weierstrass function  differentiable?,\sum_{n=0}^\infty a^n \cos(b^n\pi x),"There is a famous example of a function that has no derivative: the Weierstrass function: But just by looking at this equation - I can't seem to understand why exactly the Weierstrass Function does not have a derivative? I tried looking at a few articles online (e.g. https://www.quora.com/Why-isnt-the-Weierstrass-function-differentiable ), but I still can't seem to understand what prevents this function from having a derivative? For example, if you expand the summation term for some very large (finite) value of $n$ : $$ f(x) = a \cos(b\pi x) + a^2\cos(b^2\pi x) + a^3\cos(b^3\pi x) + ... + a^{100}\cos(b^{100}\pi x) $$ What is preventing us from taking the derivative of $f(x)$ ? Is the Weierstrass function non-differentiable only because it has ""infinite terms"" - and no function with infinite terms can be differentiated? For a finite value of $n$ , is the Weierstrass function differentiable? Thank you!","There is a famous example of a function that has no derivative: the Weierstrass function: But just by looking at this equation - I can't seem to understand why exactly the Weierstrass Function does not have a derivative? I tried looking at a few articles online (e.g. https://www.quora.com/Why-isnt-the-Weierstrass-function-differentiable ), but I still can't seem to understand what prevents this function from having a derivative? For example, if you expand the summation term for some very large (finite) value of : What is preventing us from taking the derivative of ? Is the Weierstrass function non-differentiable only because it has ""infinite terms"" - and no function with infinite terms can be differentiated? For a finite value of , is the Weierstrass function differentiable? Thank you!","n 
f(x) = a \cos(b\pi x) + a^2\cos(b^2\pi x) + a^3\cos(b^3\pi x) + ... + a^{100}\cos(b^{100}\pi x)
 f(x) n","['calculus', 'functions', 'derivatives']"
78,"What are ""instantaneous"" rates of change, really?","What are ""instantaneous"" rates of change, really?",,"Here's how I see it (please read the following if you can, because I address a lot of arguments people have already made): Let's take instantaneous speed, for example. If it's truly instantaneous, then there is no change in $x$ (time), since there's no time interval . Thus, in $\frac{f(x+h) - f(x)}{h}$,  $h$ should actually be zero (not arbitrarily close to zero, since that would still be an interval) and therefore instantaneous speed is undefined. If ""instantaneous"" is just a figure of speech for ""very very very small"", then I have two problems with it: Firstly,  well it's not instantaneous at all in the sense of ""at a single moment"". Secondly, how is ""very very very small"" conceptually different from ""small""? What's really the difference between considering $1$ second and $10^{-200}$ of a second? I've heard some people talk about ""infinitely small"" quantities. This doesn't make any sense to me. In this case, what's the process by which a number goes from ""not infinitely small"" to ""ok, now you're infinitely small""? Where's the dividing line in degree of smallness beyond which a number is infinitely small? I understand $\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$ as the limit of an infinite sequence of ratios, I have no problem with that. But I thought the point of a limit and infinity in general, is that you never get there. For example, when people say ""the sum of an infinite geometric series"", they really mean ""the limit"", since you can't possibly add infinitely many terms in the arithmetic sense of the word. So again in this case, since you never get to the limit, $h$ is always some interval, and therefore the rate is not ""instantaneous"".  Same problem with integrals actually; how do you add up infinitely many terms? Saying you can add up an infinity or terms implies that infinity is a fixed number.","Here's how I see it (please read the following if you can, because I address a lot of arguments people have already made): Let's take instantaneous speed, for example. If it's truly instantaneous, then there is no change in $x$ (time), since there's no time interval . Thus, in $\frac{f(x+h) - f(x)}{h}$,  $h$ should actually be zero (not arbitrarily close to zero, since that would still be an interval) and therefore instantaneous speed is undefined. If ""instantaneous"" is just a figure of speech for ""very very very small"", then I have two problems with it: Firstly,  well it's not instantaneous at all in the sense of ""at a single moment"". Secondly, how is ""very very very small"" conceptually different from ""small""? What's really the difference between considering $1$ second and $10^{-200}$ of a second? I've heard some people talk about ""infinitely small"" quantities. This doesn't make any sense to me. In this case, what's the process by which a number goes from ""not infinitely small"" to ""ok, now you're infinitely small""? Where's the dividing line in degree of smallness beyond which a number is infinitely small? I understand $\lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$ as the limit of an infinite sequence of ratios, I have no problem with that. But I thought the point of a limit and infinity in general, is that you never get there. For example, when people say ""the sum of an infinite geometric series"", they really mean ""the limit"", since you can't possibly add infinitely many terms in the arithmetic sense of the word. So again in this case, since you never get to the limit, $h$ is always some interval, and therefore the rate is not ""instantaneous"".  Same problem with integrals actually; how do you add up infinitely many terms? Saying you can add up an infinity or terms implies that infinity is a fixed number.",,"['calculus', 'derivatives', 'soft-question']"
79,"Closed-form of $\int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx $",Closed-form of,"\int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx ","I'm looking for a closed form of this integral. $$I = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx ,$$ where $\operatorname{Li}_2$ is the dilogarithm function . A numerical approximation of it is $$ I \approx 1.39130720750676668181096483812551383015419528634319581297153...$$ As Lucian said $I$ has the following equivalent forms: $$I = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx = \int_0^1 \frac{\operatorname{Li}_2\left( \sqrt{x} \right)}{2 \, \sqrt{x} \, \sqrt{1-x}} \,dx = \int_0^{\frac{\pi}{2}} \operatorname{Li}_2(\sin x) \, dx = \int_0^{\frac{\pi}{2}} \operatorname{Li}_2(\cos x) \, dx$$ According to Mathematica it has a closed-form in terms of generalized hypergeometric function, Claude Leibovici has given us this form . With Maple using Anastasiya-Romanova's form I could get a closed-form in term of Meijer G function. It was similar to Juan Ospina's answer , but it wasn't exactly that form. I also don't know that his form is correct, or not, because the numerical approximation has just $6$ correct digits. I'm looking for a closed form of $I$ without using generalized hypergeometric function, Meijer G function or $\operatorname{Li}_2$ or $\operatorname{Li}_3$. I hope it exists. Similar integrals are the following. $$\begin{align} J_1 & = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{1+x} \,dx = \frac{\pi^2}{6} \ln 2 - \frac58 \zeta(3) \\ J_2 & = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x}} \,dx = \pi^2 - 8 \end{align}$$ Related techniques are in this or in this paper. This one also could be useful.","I'm looking for a closed form of this integral. $$I = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx ,$$ where $\operatorname{Li}_2$ is the dilogarithm function . A numerical approximation of it is $$ I \approx 1.39130720750676668181096483812551383015419528634319581297153...$$ As Lucian said $I$ has the following equivalent forms: $$I = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x^2}} \,dx = \int_0^1 \frac{\operatorname{Li}_2\left( \sqrt{x} \right)}{2 \, \sqrt{x} \, \sqrt{1-x}} \,dx = \int_0^{\frac{\pi}{2}} \operatorname{Li}_2(\sin x) \, dx = \int_0^{\frac{\pi}{2}} \operatorname{Li}_2(\cos x) \, dx$$ According to Mathematica it has a closed-form in terms of generalized hypergeometric function, Claude Leibovici has given us this form . With Maple using Anastasiya-Romanova's form I could get a closed-form in term of Meijer G function. It was similar to Juan Ospina's answer , but it wasn't exactly that form. I also don't know that his form is correct, or not, because the numerical approximation has just $6$ correct digits. I'm looking for a closed form of $I$ without using generalized hypergeometric function, Meijer G function or $\operatorname{Li}_2$ or $\operatorname{Li}_3$. I hope it exists. Similar integrals are the following. $$\begin{align} J_1 & = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{1+x} \,dx = \frac{\pi^2}{6} \ln 2 - \frac58 \zeta(3) \\ J_2 & = \int_0^1 \frac{\operatorname{Li}_2\left( x \right)}{\sqrt{1-x}} \,dx = \pi^2 - 8 \end{align}$$ Related techniques are in this or in this paper. This one also could be useful.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
80,Is there a bounded function $f$ with $f'$ unbounded and $f''$ bounded?,Is there a bounded function  with  unbounded and  bounded?,f f' f'',"Is there a $C^{2}$-function $f:\mathbb{R}\to\mathbb{R}$ that is bounded and such that $f'(x)$ is unbounded, but $f''(x)$ is bounded again? For example, $f(x)=\sin(x^2)$ is bounded and has unbounded derivative $f'(x)$, but its second derivative is also unbounded. edit: Thanks for the great answer. The reason I came up with this question, was the following: I'd like to find a bounded continuous function $f:\mathbb{R}\to\mathbb{R}$ such that $\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi t}}e^{-(y-x)^2/2t}f(y)dy -f(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-y^2/2}(f(x+\sqrt{t}y)-f(x))dy$ does NOT uniformly converge to $0$ for $t\to 0+$. Any help would be much appreciated.","Is there a $C^{2}$-function $f:\mathbb{R}\to\mathbb{R}$ that is bounded and such that $f'(x)$ is unbounded, but $f''(x)$ is bounded again? For example, $f(x)=\sin(x^2)$ is bounded and has unbounded derivative $f'(x)$, but its second derivative is also unbounded. edit: Thanks for the great answer. The reason I came up with this question, was the following: I'd like to find a bounded continuous function $f:\mathbb{R}\to\mathbb{R}$ such that $\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi t}}e^{-(y-x)^2/2t}f(y)dy -f(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-y^2/2}(f(x+\sqrt{t}y)-f(x))dy$ does NOT uniformly converge to $0$ for $t\to 0+$. Any help would be much appreciated.",,"['calculus', 'analysis']"
81,Closed form for ${\large\int}_0^1\frac{\ln^2x}{\sqrt{1-x+x^2}}dx$,Closed form for,{\large\int}_0^1\frac{\ln^2x}{\sqrt{1-x+x^2}}dx,"I want to find a closed form for this integral: $$I=\int_0^1\frac{\ln^2x}{\sqrt{x^2-x+1}}dx\tag1$$ Mathematica and Maple cannot evaluate it directly, and I was not able to find it in tables. A numeric approximation for it is $$I\approx2.100290124838430655413586565140170651784798511276914224...\tag2$$ (click here to see more digits). Mathematica is able to find a closed form for a parameterized integral in terms of the Appell hypergeometric function : $$I(a)=\int_0^1\frac{x^a}{\sqrt{x^2-x+1}}dx\\=\frac1{a+1}F_1\left(a+1;\frac{1}{2},\frac{1}{2};a+2;(-1)^{\small1/3},-(-1)^{\small2/3}\right).\tag3$$ I suspect this expression could be rewritten in a simpler form, but I could not find it yet. It's easy to see that $I=I''(0),$ but it's unclear how to find a closed-form derivative of the Appell hypergeometric function with respect to its parameters. Could you help me to find a closed form for $I$? Update: Numerical calculations suggest that for all complex $z$ with $\Re(z)>0$ the following functional equation holds: $$z\,I(z-1)-\!\left(z+\tfrac12\right)\,I(z)+(z+1)\,I(z+1)=1.\tag4$$","I want to find a closed form for this integral: $$I=\int_0^1\frac{\ln^2x}{\sqrt{x^2-x+1}}dx\tag1$$ Mathematica and Maple cannot evaluate it directly, and I was not able to find it in tables. A numeric approximation for it is $$I\approx2.100290124838430655413586565140170651784798511276914224...\tag2$$ (click here to see more digits). Mathematica is able to find a closed form for a parameterized integral in terms of the Appell hypergeometric function : $$I(a)=\int_0^1\frac{x^a}{\sqrt{x^2-x+1}}dx\\=\frac1{a+1}F_1\left(a+1;\frac{1}{2},\frac{1}{2};a+2;(-1)^{\small1/3},-(-1)^{\small2/3}\right).\tag3$$ I suspect this expression could be rewritten in a simpler form, but I could not find it yet. It's easy to see that $I=I''(0),$ but it's unclear how to find a closed-form derivative of the Appell hypergeometric function with respect to its parameters. Could you help me to find a closed form for $I$? Update: Numerical calculations suggest that for all complex $z$ with $\Re(z)>0$ the following functional equation holds: $$z\,I(z-1)-\!\left(z+\tfrac12\right)\,I(z)+(z+1)\,I(z+1)=1.\tag4$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'hypergeometric-function']"
82,Calculus proof for the area of a circle,Calculus proof for the area of a circle,,"I was looking for proofs using Calculus for the area of a circle and come across this one $$\int 2 \pi r \, dr = 2\pi \frac {r^2}{2} = \pi r^2$$  and it struck me as being particularly easy. The only other proof I've seen was by a teacher and it involved integrating $x = \sqrt{r^2 - y^2}$ from $-1$ to $1$, using trig substitutions and then doubling the area to get $\pi r^2$ but the above proof seemed much more straight forward. Is it a valid proof, or is it based on circular logic or some other kind of fallacy?","I was looking for proofs using Calculus for the area of a circle and come across this one $$\int 2 \pi r \, dr = 2\pi \frac {r^2}{2} = \pi r^2$$  and it struck me as being particularly easy. The only other proof I've seen was by a teacher and it involved integrating $x = \sqrt{r^2 - y^2}$ from $-1$ to $1$, using trig substitutions and then doubling the area to get $\pi r^2$ but the above proof seemed much more straight forward. Is it a valid proof, or is it based on circular logic or some other kind of fallacy?",,"['calculus', 'circles']"
83,Problem with basic definition of a tangent line.,Problem with basic definition of a tangent line.,,"I have just started studying calculus for the first time, and here I see something called a tangent. They say, a tangent is a line that cuts a curve at exactly one point. But there are a lot of lines that can cut the same point just like shown in the picture- WHY aren’t we saying that lines M and C are also tangents? What is the real definition of a tangent line? “ A tangent line is a line which passes through two infinitesimally close points” Is this definition correct?  Thanks!","I have just started studying calculus for the first time, and here I see something called a tangent. They say, a tangent is a line that cuts a curve at exactly one point. But there are a lot of lines that can cut the same point just like shown in the picture- WHY aren’t we saying that lines M and C are also tangents? What is the real definition of a tangent line? “ A tangent line is a line which passes through two infinitesimally close points” Is this definition correct?  Thanks!",,"['calculus', 'definition', 'infinitesimals', 'tangent-line']"
84,Calculus book recommendations (for complete beginner) [closed],Calculus book recommendations (for complete beginner) [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question Well I have not started calculus yet but I am really keen to. I would love if you suggest some books. Points to be noted: I really don't like the way textbooks are written so please no ""textbooks"" I am COMPLETELY beginner in calculus. I know a little bit of trigonometry and what functions are but not really in depth.The book should start from the base, I mean really from the base. I may require a precalculus book too if the books don't cover that. I would really love if the book shows how calculus was developed, why it was developed and things like that. Well,thanks in advance!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question Well I have not started calculus yet but I am really keen to. I would love if you suggest some books. Points to be noted: I really don't like the way textbooks are written so please no ""textbooks"" I am COMPLETELY beginner in calculus. I know a little bit of trigonometry and what functions are but not really in depth.The book should start from the base, I mean really from the base. I may require a precalculus book too if the books don't cover that. I would really love if the book shows how calculus was developed, why it was developed and things like that. Well,thanks in advance!",,"['calculus', 'algebra-precalculus', 'reference-request', 'self-learning', 'book-recommendation']"
85,A question about differentiating equations that are impossible to solve for a variable,A question about differentiating equations that are impossible to solve for a variable,,"I thought I had a good idea on why/how implicit differentiation works until I read the following passage in my Calculus book: Furthermore, implicit differentiation works just as easily for equations such as   $$x^5+5x^4y^2+3xy^3+y^5=1$$   which are actually impossible to solve for $y$ in terms of $x$ My problem with it is the following: The way we go about differentiating, for instance, $xy=1$ is by differentiating the whole equation through with respect to $x$ and treating $y$ as a function of $x$. But (and at least that's how I see it)  we can only treat $y$ as $f(x)$ because the equation determines $y$ as a function of $x$ in a relation that can ben written expliclity (in this case, $y=\frac{1}{x}$). If we have an equation such as the quoted one, in which we just can't solve for $y$, doesn't that mean that $y$ is not a function of $x$? In such a case, wouldn't treating it as such be an invalid move? I hope I have made myself understood. Any clarification will be appreciated. Thanks","I thought I had a good idea on why/how implicit differentiation works until I read the following passage in my Calculus book: Furthermore, implicit differentiation works just as easily for equations such as   $$x^5+5x^4y^2+3xy^3+y^5=1$$   which are actually impossible to solve for $y$ in terms of $x$ My problem with it is the following: The way we go about differentiating, for instance, $xy=1$ is by differentiating the whole equation through with respect to $x$ and treating $y$ as a function of $x$. But (and at least that's how I see it)  we can only treat $y$ as $f(x)$ because the equation determines $y$ as a function of $x$ in a relation that can ben written expliclity (in this case, $y=\frac{1}{x}$). If we have an equation such as the quoted one, in which we just can't solve for $y$, doesn't that mean that $y$ is not a function of $x$? In such a case, wouldn't treating it as such be an invalid move? I hope I have made myself understood. Any clarification will be appreciated. Thanks",,"['calculus', 'derivatives']"
86,"A sine integral $\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$",A sine integral,"\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x","The following question comes from Some integral with sine post $$\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$$ but now I'd be curious to know how to deal with it by methods of complex analysis. Some suggestions, hints? Thanks!!! Sis.","The following question comes from Some integral with sine post $$\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$$ but now I'd be curious to know how to deal with it by methods of complex analysis. Some suggestions, hints? Thanks!!! Sis.",,"['calculus', 'complex-analysis', 'residue-calculus']"
87,How to find area under sines without calculus?,How to find area under sines without calculus?,,"In the section establishing that integrals and derivatives are inverse to each other, James Stewart's Calculus textbook says (pp325--pp326, Sec.4.3, 8Ed): When the French mathematician Gilles de Roberval first found the area under the sine and cosine curves in 1635, this was a very challenging problem that required a great deal of ingenuity. If we didn’t have the benefit of the Fundamental Theorem of Calculus, we would have to compute a difficult limit of sums using obscure trigonometric identities. It was even more difficult for Roberval because the apparatus of limits had not been invented in 1635. I wonder how Gilles de Roberval did it. Wikipedia and MacTutor do not contain much info on that. How to apply the method of quadrature is exactly the real challenge I suppose. This is mainly a history question, but I'm also curious as to how one would approach this in modern days. Thank you.","In the section establishing that integrals and derivatives are inverse to each other, James Stewart's Calculus textbook says (pp325--pp326, Sec.4.3, 8Ed): When the French mathematician Gilles de Roberval first found the area under the sine and cosine curves in 1635, this was a very challenging problem that required a great deal of ingenuity. If we didn’t have the benefit of the Fundamental Theorem of Calculus, we would have to compute a difficult limit of sums using obscure trigonometric identities. It was even more difficult for Roberval because the apparatus of limits had not been invented in 1635. I wonder how Gilles de Roberval did it. Wikipedia and MacTutor do not contain much info on that. How to apply the method of quadrature is exactly the real challenge I suppose. This is mainly a history question, but I'm also curious as to how one would approach this in modern days. Thank you.",,"['calculus', 'sequences-and-series', 'algebra-precalculus', 'trigonometry', 'math-history']"
88,Evaluate $ \int_{0}^{\pi/2}\frac{1+\tanh x}{1+\tan x}dx $,Evaluate, \int_{0}^{\pi/2}\frac{1+\tanh x}{1+\tan x}dx ,"I need the method to evaluate this integral (the closed-form if possible). $$ \int_{0}^{\pi/2}\frac{1+\tanh x}{1+\tan x}\,dx $$ I used the relationship between $\tan x$ and $\tanh x$ but it didn't work. Any help?","I need the method to evaluate this integral (the closed-form if possible). $$ \int_{0}^{\pi/2}\frac{1+\tanh x}{1+\tan x}\,dx $$ I used the relationship between $\tan x$ and $\tanh x$ but it didn't work. Any help?",,"['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'closed-form']"
89,"Explain $\iint \mathrm dx\,\mathrm dy = \iint r \,\mathrm \,d\alpha\,\mathrm dr$",Explain,"\iint \mathrm dx\,\mathrm dy = \iint r \,\mathrm \,d\alpha\,\mathrm dr","It is changing the coordinate from one coordinate to another. There is an angle and radius on the right side. What is it? And why? I got: $2\,\mathrm dy\,\mathrm dx = r(\cos^2\alpha-\sin^2\alpha)\,\mathrm d\alpha \,\mathrm dr$, where $x = r \cos(\alpha)$ and $y = r \sin(\alpha)$. but cannot understand and get the right side. The problem emerged when trying to integrate $\displaystyle \int_0^\infty e^{\frac{-x^2}{n^2}}\,\mathrm dx$ where I tried to change the problem knowing $r^2=x^2+y^2$ but stuck to this part. What is the change in the title called and why is it so?","It is changing the coordinate from one coordinate to another. There is an angle and radius on the right side. What is it? And why? I got: $2\,\mathrm dy\,\mathrm dx = r(\cos^2\alpha-\sin^2\alpha)\,\mathrm d\alpha \,\mathrm dr$, where $x = r \cos(\alpha)$ and $y = r \sin(\alpha)$. but cannot understand and get the right side. The problem emerged when trying to integrate $\displaystyle \int_0^\infty e^{\frac{-x^2}{n^2}}\,\mathrm dx$ where I tried to change the problem knowing $r^2=x^2+y^2$ but stuck to this part. What is the change in the title called and why is it so?",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
90,What is the integral of 0?,What is the integral of 0?,,"I am trying to convince my friend that the integral of $0$ is $C$, where $C$ is an arbitrary constant. He can't seem to grasp this concept. Can you guys help me out here? He keeps saying it is $0$.","I am trying to convince my friend that the integral of $0$ is $C$, where $C$ is an arbitrary constant. He can't seem to grasp this concept. Can you guys help me out here? He keeps saying it is $0$.",,"['calculus', 'integration', 'proof-writing']"
91,How to find $\int_0^\infty\frac{\log\left(\frac{1+x^{4+\sqrt{15}}}{1+x^{2+\sqrt{3}}}\right)}{\left(1+x^2\right)\log x}\mathrm dx$,How to find,\int_0^\infty\frac{\log\left(\frac{1+x^{4+\sqrt{15}}}{1+x^{2+\sqrt{3}}}\right)}{\left(1+x^2\right)\log x}\mathrm dx,"I was challenged to prove this identity $$\int_0^\infty\frac{\log\left(\frac{1+x^{4+\sqrt{15\vphantom{\large A}}}}{1+x^{2+\sqrt{3\vphantom{\large A}}}}\right)}{\left(1+x^2\right)\log x}\mathrm dx=\frac{\pi}{4}\left(2+\sqrt{6}\sqrt{3-\sqrt{5}}\right).$$ I was not successful, so I want to ask for your help. Can it be somehow related to integrals listed in that question ?","I was challenged to prove this identity $$\int_0^\infty\frac{\log\left(\frac{1+x^{4+\sqrt{15\vphantom{\large A}}}}{1+x^{2+\sqrt{3\vphantom{\large A}}}}\right)}{\left(1+x^2\right)\log x}\mathrm dx=\frac{\pi}{4}\left(2+\sqrt{6}\sqrt{3-\sqrt{5}}\right).$$ I was not successful, so I want to ask for your help. Can it be somehow related to integrals listed in that question ?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'improper-integrals']"
92,How to choose a proper contour for a contour integral?,How to choose a proper contour for a contour integral?,,"When analyzing real integrals with contour integrals, how does one choose a proper contour integral? Many cases can be solved by integrating around the top half of a circle with radius of infinity and then integrating along the entire real line. I understand how when integrating one would avoid the branch cuts, but how would one know to use a rectangle or a quarter of a circle as a contour?","When analyzing real integrals with contour integrals, how does one choose a proper contour integral? Many cases can be solved by integrating around the top half of a circle with radius of infinity and then integrating along the entire real line. I understand how when integrating one would avoid the branch cuts, but how would one know to use a rectangle or a quarter of a circle as a contour?",,"['calculus', 'integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
93,"How can a ""proper"" function have a vertical slope?","How can a ""proper"" function have a vertical slope?",,"Plotting the function $f(x)=x^{1/3}$ defined for any real number $x$ gives us: Since $f$ is a function, for any given $x$ value it maps to a single y value (and not more than one $y$ value, because that would mean it's not a function as it fails the vertical line test). This function also has a vertical tangent at $x=0$ . My question is: how can we have a function that also has a vertical tangent? To get a vertical tangent we need 2 vertical points, which means that we are not working with a ""proper"" function as it has multiple y values mapping to a single $x$ . How is it possible for a ""proper"" function to have a vertical tangent? As I understand, in the graph I pasted we cannot take the derivative of x=0 because the slope is vertical, hence we cannot see the instantaneous rate of change of x to y as the y value is not a value (or many values, which ever way you want to look at it). How is it possible to have a perfectly vertical slope on a function? In this case I can imagine a very steep curve at 0.... but vertical?!? I can't wrap my mind around it. How can we get a vertical slope on a non vertical function?","Plotting the function defined for any real number gives us: Since is a function, for any given value it maps to a single y value (and not more than one value, because that would mean it's not a function as it fails the vertical line test). This function also has a vertical tangent at . My question is: how can we have a function that also has a vertical tangent? To get a vertical tangent we need 2 vertical points, which means that we are not working with a ""proper"" function as it has multiple y values mapping to a single . How is it possible for a ""proper"" function to have a vertical tangent? As I understand, in the graph I pasted we cannot take the derivative of x=0 because the slope is vertical, hence we cannot see the instantaneous rate of change of x to y as the y value is not a value (or many values, which ever way you want to look at it). How is it possible to have a perfectly vertical slope on a function? In this case I can imagine a very steep curve at 0.... but vertical?!? I can't wrap my mind around it. How can we get a vertical slope on a non vertical function?",f(x)=x^{1/3} x f x y x=0 x,"['calculus', 'functions', 'derivatives', 'graphing-functions', 'tangent-line']"
94,"If a function has a finite limit at infinity, does that imply its derivative goes to zero?","If a function has a finite limit at infinity, does that imply its derivative goes to zero?",,"I've been thinking about this problem: Let $f: (a, +\infty) \to \mathbb{R}$ be a differentiable function such that $\lim\limits_{x \to +\infty} f(x) = L < \infty$. Then must it be the case that $\lim\limits_{x\to +\infty}f'(x) = 0$? It looks like it's true, but I haven't managed to work out a proof. I came up with this, but it's pretty sketchy: $$ \begin{align} \lim_{x \to +\infty} f'(x) &= \lim_{x \to +\infty} \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \\ &= \lim_{h \to 0} \lim_{x \to +\infty} \frac{f(x+h)-f(x)}{h} \\ &= \lim_{h \to 0} \frac1{h} \lim_{x \to +\infty}[f(x+h)-f(x)] \\ &= \lim_{h \to 0} \frac1{h}(L-L) \\ &= \lim_{h \to 0} \frac{0}{h} \\ &= 0 \end{align} $$ In particular, I don't think I can swap the order of the limits just like that. Is this correct, and if it isn't, how can we prove the statement? I know there is a similar question already, but I think this is different in two aspects. First, that question assumes that $\lim\limits_{x \to +\infty}f'(x)$ exists, which I don't. Second, I also wanted to know if interchanging limits is a valid operation in this case.","I've been thinking about this problem: Let $f: (a, +\infty) \to \mathbb{R}$ be a differentiable function such that $\lim\limits_{x \to +\infty} f(x) = L < \infty$. Then must it be the case that $\lim\limits_{x\to +\infty}f'(x) = 0$? It looks like it's true, but I haven't managed to work out a proof. I came up with this, but it's pretty sketchy: $$ \begin{align} \lim_{x \to +\infty} f'(x) &= \lim_{x \to +\infty} \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \\ &= \lim_{h \to 0} \lim_{x \to +\infty} \frac{f(x+h)-f(x)}{h} \\ &= \lim_{h \to 0} \frac1{h} \lim_{x \to +\infty}[f(x+h)-f(x)] \\ &= \lim_{h \to 0} \frac1{h}(L-L) \\ &= \lim_{h \to 0} \frac{0}{h} \\ &= 0 \end{align} $$ In particular, I don't think I can swap the order of the limits just like that. Is this correct, and if it isn't, how can we prove the statement? I know there is a similar question already, but I think this is different in two aspects. First, that question assumes that $\lim\limits_{x \to +\infty}f'(x)$ exists, which I don't. Second, I also wanted to know if interchanging limits is a valid operation in this case.",,"['calculus', 'limits', 'derivatives']"
95,Computing the integral of $\log(\sin x)$,Computing the integral of,\log(\sin x),"How to compute the following integral?  $$\int\log(\sin x)\,dx$$ Motivation: Since $\log(\sin x)'=\cot x$, the antiderivative $\int\log(\sin x)\,dx$ has the nice property $F''(x)=\cot x$. Can we find $F$ explicitly? Failing that, can we find the definite integral over one of intervals where $\log (\sin x)$ is defined?","How to compute the following integral?  $$\int\log(\sin x)\,dx$$ Motivation: Since $\log(\sin x)'=\cot x$, the antiderivative $\int\log(\sin x)\,dx$ has the nice property $F''(x)=\cot x$. Can we find $F$ explicitly? Failing that, can we find the definite integral over one of intervals where $\log (\sin x)$ is defined?",,"['calculus', 'integration', 'trigonometry', 'logarithms']"
96,Why does the fundamental theorem of calculus work?,Why does the fundamental theorem of calculus work?,,"I've known for some time that one of the fundamental theorems of calculus states: $$ \int_{a}^{b}\ f'(x){\mathrm{d} x} = f(b)-f(a) $$ Despite using this formula, I've yet to see a proof or even a satisfactory explanation for why this relationship holds. Any ideas?","I've known for some time that one of the fundamental theorems of calculus states: $$ \int_{a}^{b}\ f'(x){\mathrm{d} x} = f(b)-f(a) $$ Despite using this formula, I've yet to see a proof or even a satisfactory explanation for why this relationship holds. Any ideas?",,"['calculus', 'integration', 'derivatives', 'intuition']"
97,Why does the harmonic series diverge but the p-harmonic series converge,Why does the harmonic series diverge but the p-harmonic series converge,,"I am struggling understanding intuitively why the harmonic series diverges but the p-harmonic series converges. I know there are methods and applications to prove convergence, but I am only having trouble understanding intuitively why it is. I know I must never trust my intuition, but this is hard for me to grasp. In both cases, the terms of the series are getting smaller, hence are approaching zero, but they both result in different answers. $$\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots = \text{diverges}$$   $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{n^2}=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}+\cdots =\text{converges}$$","I am struggling understanding intuitively why the harmonic series diverges but the p-harmonic series converges. I know there are methods and applications to prove convergence, but I am only having trouble understanding intuitively why it is. I know I must never trust my intuition, but this is hard for me to grasp. In both cases, the terms of the series are getting smaller, hence are approaching zero, but they both result in different answers. $$\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots = \text{diverges}$$   $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{n^2}=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}+\cdots =\text{converges}$$",,"['calculus', 'sequences-and-series']"
98,"Why, although these functions have the same derivative, do they not differ by a constant?","Why, although these functions have the same derivative, do they not differ by a constant?",,I calculated the derivative of $\arctan\left(\frac{1+x}{1-x}\right)$ to be $\frac{1}{1+x^2}$. This is the same as $(\arctan)'$. Why is there no $c$ that satisfies $\arctan\left(\frac{1+x}{1-x}\right) = \arctan(x) +c$?,I calculated the derivative of $\arctan\left(\frac{1+x}{1-x}\right)$ to be $\frac{1}{1+x^2}$. This is the same as $(\arctan)'$. Why is there no $c$ that satisfies $\arctan\left(\frac{1+x}{1-x}\right) = \arctan(x) +c$?,,"['calculus', 'trigonometry', 'derivatives']"
99,Interpreting Line Integrals with respect to $x$ or $y$,Interpreting Line Integrals with respect to  or,x y,"A line integral (with respect to arc length) can be interpreted geometrically as the area under $f(x,y)$ along $C$ as in the picture. You sum up the areas of all the infinitesimally small 'rectangles' formed by $f(x,y)$ and $ds$. What I'm wondering is how do I interpret line integrals with respect to $x$ or $y$ geometrically?","A line integral (with respect to arc length) can be interpreted geometrically as the area under $f(x,y)$ along $C$ as in the picture. You sum up the areas of all the infinitesimally small 'rectangles' formed by $f(x,y)$ and $ds$. What I'm wondering is how do I interpret line integrals with respect to $x$ or $y$ geometrically?",,"['calculus', 'integration', 'multivariable-calculus', 'intuition', 'line-integrals']"
