,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding $\lim_{x \to 3} \frac{\sqrt{x-1} + 2}{x+3}$,Finding,\lim_{x \to 3} \frac{\sqrt{x-1} + 2}{x+3},I'm having problems with this example. What are all the ways I can determine if the lim tends towards positive or negative infinity without using derivatives? EX: Evaluate each of the following. If the limit does not exists state   so. If the limit is infinite then state whether the values tend   towards positive or negative infinity. $$\lim_{x \to 3} \frac{\sqrt {x-1}+2}{x+3}$$,I'm having problems with this example. What are all the ways I can determine if the lim tends towards positive or negative infinity without using derivatives? EX: Evaluate each of the following. If the limit does not exists state   so. If the limit is infinite then state whether the values tend   towards positive or negative infinity. $$\lim_{x \to 3} \frac{\sqrt {x-1}+2}{x+3}$$,,"['calculus', 'limits']"
1,"Important difference between ""claim"" and ""assume"" in proofs.","Important difference between ""claim"" and ""assume"" in proofs.",,"I was discussing with a TA friend, one of the HW problems was Find $\lim_{n\rightarrow \infty} \frac{1}{n}$. Before doing the ""$\epsilon, N$"" definition proof, the students wrote We claim that $\lim_{n\rightarrow} \frac{1}{n} = 0$, we show that $\forall \epsilon > 0, \exists N\in \mathbb{N} ...$ or Assume $\lim_{n\rightarrow} \frac{1}{n} = 0$, we show that $\forall \epsilon > 0, \exists N\in \mathbb{N} ...$ And my friend said that the 2nd proof with ""assume"" is not correct, it is proving $A$ implies $A$ which is a flaw in the logic. But I felt proof 2 is okay in a analysis course. What do you think about this?","I was discussing with a TA friend, one of the HW problems was Find $\lim_{n\rightarrow \infty} \frac{1}{n}$. Before doing the ""$\epsilon, N$"" definition proof, the students wrote We claim that $\lim_{n\rightarrow} \frac{1}{n} = 0$, we show that $\forall \epsilon > 0, \exists N\in \mathbb{N} ...$ or Assume $\lim_{n\rightarrow} \frac{1}{n} = 0$, we show that $\forall \epsilon > 0, \exists N\in \mathbb{N} ...$ And my friend said that the 2nd proof with ""assume"" is not correct, it is proving $A$ implies $A$ which is a flaw in the logic. But I felt proof 2 is okay in a analysis course. What do you think about this?",,"['real-analysis', 'limits', 'proof-writing']"
2,Limit of $\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}$,Limit of,\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}},"Here is the limit to be calculated : $$\lim_{n\to\infty}\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}$$ The question provides no other information. Doubts What is the domain of the function? If the domain is $Z^+$, does limit exists for any $n\to integer$(since function wouldn't be defined in the neighborhood) and what about ${n\to\infty}$, how to interpret it ? My Try : $$\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}=\frac{-n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}=\frac{-1}{\sqrt{ (1+1/n^2)}+ \sqrt{ (1-1/n^2)}}$$ hence $$\lim_{n\to\infty}\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}= -\frac{1}{2}$$","Here is the limit to be calculated : $$\lim_{n\to\infty}\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}$$ The question provides no other information. Doubts What is the domain of the function? If the domain is $Z^+$, does limit exists for any $n\to integer$(since function wouldn't be defined in the neighborhood) and what about ${n\to\infty}$, how to interpret it ? My Try : $$\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}=\frac{-n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}=\frac{-1}{\sqrt{ (1+1/n^2)}+ \sqrt{ (1-1/n^2)}}$$ hence $$\lim_{n\to\infty}\frac{1-2+\cdots+(2n-1)-2n}{\sqrt{ (n^2+1)}+ \sqrt{ (n^2-1)}}= -\frac{1}{2}$$",,"['sequences-and-series', 'limits']"
3,Finding the limit using definite integration,Finding the limit using definite integration,,"$$y=\lim_{n\to \infty}\left(\sin π/2n \times \sin 2π/2n \times... \sin(n-1)π/2n\right)$$ Find y.  I took log on both the sides and I got  $$\ln y= \int_0^1 {\ln \sin πx/2\, dx}$$ but I'm stuck here.","$$y=\lim_{n\to \infty}\left(\sin π/2n \times \sin 2π/2n \times... \sin(n-1)π/2n\right)$$ Find y.  I took log on both the sides and I got  $$\ln y= \int_0^1 {\ln \sin πx/2\, dx}$$ but I'm stuck here.",,"['limits', 'definite-integrals']"
4,"Find the limit as $n\to\infty$, is it $\infty$?","Find the limit as , is it ?",n\to\infty \infty,"\begin{equation}f(N)=\frac{\frac{L^2}{y-L}(1-\frac{L}{Nx})+\frac{NL}{N-M}(1-\frac{L}{Nx})\sum\limits_{k=0}^{M}\frac{\binom{N}{k}}{\binom{N}{M}}\left(\frac{L}{Nx-L}\right)^{k-M}}{\frac{L}{N(y-L)}\left[\frac{y}{y-L}(1-\frac{L}{Nx})+M\right]+\frac{1}{N-M}\sum\limits_{k=0}^{M}\frac{\binom{N}{k}}{\binom{N}{M}}k\left(\frac{L}{Nx-L}\right)^{k-M}}\end{equation} Any suggestion on how to calculate its limit as the positive integer $N\to\infty$? With given positive $M,L,x,y$. Here $Nx>L$, $y>L$, the integer $M\le N$, and $k$ is integer. I tried the transformation on the combinatorial number as follow, but is didn't seem help. \begin{equation}f(N)=\frac{\sum\limits_{k=0}^{M}\frac{\binom{M}{k}}{\binom{N-k}{N-M}}\left(\frac{L}{Nx-L}\right)^{k-M}\frac{NL}{N-M}(1-\frac{L}{Nx})+\frac{L^2}{y-L}(1-\frac{L}{Nx})}{\sum\limits_{k=0}^{M}\frac{\binom{M}{k}}{\binom{N-k}{N-M}}\left(\frac{L}{Nx-L}\right)^{k-M}\frac{k}{N-M}+\frac{L}{N(y-L)}\left[\frac{y}{y-L}(1-\frac{L}{Nx})+M\right]}\end{equation} Numerical result shows it increases sharply as N increases first, and then very slowly, and seems like approaching some constant value. It doesn't seem like the limit will be infinity. Thanks for any suggestion.","\begin{equation}f(N)=\frac{\frac{L^2}{y-L}(1-\frac{L}{Nx})+\frac{NL}{N-M}(1-\frac{L}{Nx})\sum\limits_{k=0}^{M}\frac{\binom{N}{k}}{\binom{N}{M}}\left(\frac{L}{Nx-L}\right)^{k-M}}{\frac{L}{N(y-L)}\left[\frac{y}{y-L}(1-\frac{L}{Nx})+M\right]+\frac{1}{N-M}\sum\limits_{k=0}^{M}\frac{\binom{N}{k}}{\binom{N}{M}}k\left(\frac{L}{Nx-L}\right)^{k-M}}\end{equation} Any suggestion on how to calculate its limit as the positive integer $N\to\infty$? With given positive $M,L,x,y$. Here $Nx>L$, $y>L$, the integer $M\le N$, and $k$ is integer. I tried the transformation on the combinatorial number as follow, but is didn't seem help. \begin{equation}f(N)=\frac{\sum\limits_{k=0}^{M}\frac{\binom{M}{k}}{\binom{N-k}{N-M}}\left(\frac{L}{Nx-L}\right)^{k-M}\frac{NL}{N-M}(1-\frac{L}{Nx})+\frac{L^2}{y-L}(1-\frac{L}{Nx})}{\sum\limits_{k=0}^{M}\frac{\binom{M}{k}}{\binom{N-k}{N-M}}\left(\frac{L}{Nx-L}\right)^{k-M}\frac{k}{N-M}+\frac{L}{N(y-L)}\left[\frac{y}{y-L}(1-\frac{L}{Nx})+M\right]}\end{equation} Numerical result shows it increases sharply as N increases first, and then very slowly, and seems like approaching some constant value. It doesn't seem like the limit will be infinity. Thanks for any suggestion.",,['limits']
5,The Limit of an Integral Containing Exponentials,The Limit of an Integral Containing Exponentials,,"I am unsure how to show this. Suppose $\delta(s)$ defined on $(-\infty , s_*)$ is increasing and satisfies $\lim _{s\rightarrow s_*} \delta = \lim _{s\rightarrow s_*} \frac{d \delta}{d s} = \infty$ then $e^{-\delta (s)}\int _{s_0}^se^{\delta (s')}ds'\rightarrow 0$ as $s\rightarrow s_*$ for any $s_0<s_*$. The hint is to rewrite the integration variables with respect to $\delta$. This means we need consider $\lim _{\delta _s\rightarrow\infty}e^{-\delta}\int _{\delta _0}^{\delta _s}e^{\delta}(\frac{d\delta}{ds})^{-1}d\delta$ but I am unable to proceed further. Any help is much appreciated.","I am unsure how to show this. Suppose $\delta(s)$ defined on $(-\infty , s_*)$ is increasing and satisfies $\lim _{s\rightarrow s_*} \delta = \lim _{s\rightarrow s_*} \frac{d \delta}{d s} = \infty$ then $e^{-\delta (s)}\int _{s_0}^se^{\delta (s')}ds'\rightarrow 0$ as $s\rightarrow s_*$ for any $s_0<s_*$. The hint is to rewrite the integration variables with respect to $\delta$. This means we need consider $\lim _{\delta _s\rightarrow\infty}e^{-\delta}\int _{\delta _0}^{\delta _s}e^{\delta}(\frac{d\delta}{ds})^{-1}d\delta$ but I am unable to proceed further. Any help is much appreciated.",,"['integration', 'limits', 'exponential-function', 'infinity', 'substitution']"
6,"Does $\lim_{n\to \infty}\left(\prod_{k=1}^n a_k\right)^{1/n}<1\implies \lim_{n\to \infty}\left(\prod_{k=1}^n\max\{ 1,a_k\}\right)^{1/n}=1 $",Does,"\lim_{n\to \infty}\left(\prod_{k=1}^n a_k\right)^{1/n}<1\implies \lim_{n\to \infty}\left(\prod_{k=1}^n\max\{ 1,a_k\}\right)^{1/n}=1 ","I have the positive valued sequence $\{a_n\}$ satisfying the condition $$\lim_{n\to \infty}\left(\prod_{k=1}^n a_k\right)^{1/n}<1$$ Is it true that this implies $$\lim_{n\to \infty}\left(\prod_{k=1}^n\max\{ 1,a_k\}\right)^{1/n}=1 ?$$ Though I am really dubious about this one, I am finding it difficult to prove or disprove this. A possible idea that I have is that I can write $$\max\{1,a_k\}=1+b_k$$ where $b_k\ge 0$ and then possibly find a way to show that $b_k\to 0$ which would serve the purpose, but I am really having problem pursuing this idea. So any help is appreciated. Thanks in advance.","I have the positive valued sequence $\{a_n\}$ satisfying the condition $$\lim_{n\to \infty}\left(\prod_{k=1}^n a_k\right)^{1/n}<1$$ Is it true that this implies $$\lim_{n\to \infty}\left(\prod_{k=1}^n\max\{ 1,a_k\}\right)^{1/n}=1 ?$$ Though I am really dubious about this one, I am finding it difficult to prove or disprove this. A possible idea that I have is that I can write $$\max\{1,a_k\}=1+b_k$$ where $b_k\ge 0$ and then possibly find a way to show that $b_k\to 0$ which would serve the purpose, but I am really having problem pursuing this idea. So any help is appreciated. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'limits']"
7,How do I show $\lim_{x\to\infty}f(x) = \lim_{x\to\infty} f '(x)=0$ if $\lim_{x\to\infty}f '(x)^2 + f(x)^3 = 0$? [duplicate],How do I show  if ? [duplicate],\lim_{x\to\infty}f(x) = \lim_{x\to\infty} f '(x)=0 \lim_{x\to\infty}f '(x)^2 + f(x)^3 = 0,"This question already has answers here : How prove that $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to+\infty}f'(x)=0$ if $\lim\limits_{x\to+\infty}([f'(x)]^2+f^3(x))=0$? (3 answers) Closed 8 years ago . $f(x)$ is a real valued function on the reals, and has a continuous derivative such that $$\lim_{x\to\infty} f'(x)^2 + f(x)^3 = 0.$$ How do i show that $$\lim_{x\to\infty} f(x) = \lim_{x\to\infty} f'(x)=0?$$ Note I tried to take $f(x)=\frac{1}{x}$ but it did not work Thank you for any kind of help","This question already has answers here : How prove that $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to+\infty}f'(x)=0$ if $\lim\limits_{x\to+\infty}([f'(x)]^2+f^3(x))=0$? (3 answers) Closed 8 years ago . $f(x)$ is a real valued function on the reals, and has a continuous derivative such that $$\lim_{x\to\infty} f'(x)^2 + f(x)^3 = 0.$$ How do i show that $$\lim_{x\to\infty} f(x) = \lim_{x\to\infty} f'(x)=0?$$ Note I tried to take $f(x)=\frac{1}{x}$ but it did not work Thank you for any kind of help",,"['real-analysis', 'limits', 'derivatives', 'continuity']"
8,Boundedness of the function,Boundedness of the function,,"Let $x\in(0,1)$ and $S_{n-1}=\sum\limits_{k=0}^{n-1}x^k$. Then define $f$ as the following :$$f(x)=\sum_{n=1}^{\infty}\left|\frac{nx^n}{S_{n-1}}-1\right|$$ I need to show that $\lim\limits_{x\to1^-}f(x)$ exists. Or at least, I want to show that $f$ is bounded on the given interval. Thanks for your helps.","Let $x\in(0,1)$ and $S_{n-1}=\sum\limits_{k=0}^{n-1}x^k$. Then define $f$ as the following :$$f(x)=\sum_{n=1}^{\infty}\left|\frac{nx^n}{S_{n-1}}-1\right|$$ I need to show that $\lim\limits_{x\to1^-}f(x)$ exists. Or at least, I want to show that $f$ is bounded on the given interval. Thanks for your helps.",,"['sequences-and-series', 'limits', 'convergence-divergence', 'supremum-and-infimum']"
9,How to solve this seemingly easy problem?,How to solve this seemingly easy problem?,,"In short, I need to prove that: $\sin 2nx\not\to-\sin 2x\quad x\ne\frac{k\pi}2,k\in\Bbb Z,n\in\Bbb N\quad \text{as}\quad n\to\infty$ The biggest trouble is that I know little about $x$, not even knowing if it is a rational multiple of $\pi$ or not! This doesn't seem to be a hard problem, but I somehow get stuck. Can you help me with that? I'd be very grateful! PS: as little number theory involved as possible, please. I don't want to mess up with the irrationality of $\pi$.","In short, I need to prove that: $\sin 2nx\not\to-\sin 2x\quad x\ne\frac{k\pi}2,k\in\Bbb Z,n\in\Bbb N\quad \text{as}\quad n\to\infty$ The biggest trouble is that I know little about $x$, not even knowing if it is a rational multiple of $\pi$ or not! This doesn't seem to be a hard problem, but I somehow get stuck. Can you help me with that? I'd be very grateful! PS: as little number theory involved as possible, please. I don't want to mess up with the irrationality of $\pi$.",,"['calculus', 'sequences-and-series', 'limits', 'trigonometry', 'trigonometric-series']"
10,"Does there exist a sequence $\{a_{n}\}$, but $\lim_{n\to\infty}a_{n}\neq 0$","Does there exist a sequence , but",\{a_{n}\} \lim_{n\to\infty}a_{n}\neq 0,"Does there exist a sequence $\{a_{n}\}$, such that $$\lim_{n\to\infty}(a_{n+1}-a_{n})=0\ \ \ \text{and} \ \ \lim_{n\to\infty}\dfrac{a_{1}+a_{2}+\cdots+a_{n}}{n}=0$$ but $$\lim_{n\to\infty}a_{n}\neq 0 \ ?$$","Does there exist a sequence $\{a_{n}\}$, such that $$\lim_{n\to\infty}(a_{n+1}-a_{n})=0\ \ \ \text{and} \ \ \lim_{n\to\infty}\dfrac{a_{1}+a_{2}+\cdots+a_{n}}{n}=0$$ but $$\lim_{n\to\infty}a_{n}\neq 0 \ ?$$",,['limits']
11,value of $\arctan (\cosh u)$ as $u \to -\infty $,value of  as,\arctan (\cosh u) u \to -\infty ,"I am interested in the value of $\arctan (\cosh u)$ as $u\to -\infty $  $$\arctan (\cosh u)= \dfrac i 2 \log \left| \dfrac {1-i\cosh u}{1+i\cosh u} \right|$$ and since $$\cosh u= \dfrac {e^u+e^{-u}}{2}$$ we know it tends to positive infinity. So  $$\dfrac i 2 \log \left| \dfrac {\dfrac {1}{\cosh u} - i} {\dfrac {1}{\cosh u} + i}\right| = \dfrac i 2 \log \left| \dfrac {-i}{i} \right| = - \dfrac i 2 \log \left| -1 \right|$$ Ok I am always confused with the modulus in the logs, whether it introduces modulus when we take $\log$ both sides of equation or not. If modulus does exist it becomes $0$, if not $\log(-1) = 1.36437635 i$ so $- \dfrac i 2 \log \left| -1 \right| = 0.68218817692$ But looking at the graph we can see $\arctan$ function tends to $\dfrac \pi 2$. So whats the reason for the difference when we use the log form? Many thanks in advance","I am interested in the value of $\arctan (\cosh u)$ as $u\to -\infty $  $$\arctan (\cosh u)= \dfrac i 2 \log \left| \dfrac {1-i\cosh u}{1+i\cosh u} \right|$$ and since $$\cosh u= \dfrac {e^u+e^{-u}}{2}$$ we know it tends to positive infinity. So  $$\dfrac i 2 \log \left| \dfrac {\dfrac {1}{\cosh u} - i} {\dfrac {1}{\cosh u} + i}\right| = \dfrac i 2 \log \left| \dfrac {-i}{i} \right| = - \dfrac i 2 \log \left| -1 \right|$$ Ok I am always confused with the modulus in the logs, whether it introduces modulus when we take $\log$ both sides of equation or not. If modulus does exist it becomes $0$, if not $\log(-1) = 1.36437635 i$ so $- \dfrac i 2 \log \left| -1 \right| = 0.68218817692$ But looking at the graph we can see $\arctan$ function tends to $\dfrac \pi 2$. So whats the reason for the difference when we use the log form? Many thanks in advance",,"['algebra-precalculus', 'limits', 'hyperbolic-functions']"
12,"Give an example of a continuous function $f: (-1, 1) \rightarrow \mathbb{R}$ which attains a maximum at 0, but is not differentiable at 0","Give an example of a continuous function  which attains a maximum at 0, but is not differentiable at 0","f: (-1, 1) \rightarrow \mathbb{R}","I need a little help with this exercise: Give an example of a continuous function $f: (-1, 1) \rightarrow \mathbb{R}$ which attains a maximum at 0, but is not differentiable at 0 I thought of the function $f(x) = -|x|$ for this. Proof: It clearly takes a maximum at 0, and by taking the left and right limits at 0, which are both 0, we can conclude that it is continuous in 0. By taking the difference quotients from the left and right, which are 1 and -1, it's clear that this function is not differentiable at 0. Is this function a valid example? And is the proof I gave right? Thanks in advance!","I need a little help with this exercise: Give an example of a continuous function $f: (-1, 1) \rightarrow \mathbb{R}$ which attains a maximum at 0, but is not differentiable at 0 I thought of the function $f(x) = -|x|$ for this. Proof: It clearly takes a maximum at 0, and by taking the left and right limits at 0, which are both 0, we can conclude that it is continuous in 0. By taking the difference quotients from the left and right, which are 1 and -1, it's clear that this function is not differentiable at 0. Is this function a valid example? And is the proof I gave right? Thanks in advance!",,"['calculus', 'limits', 'functions', 'absolute-value']"
13,How to prove this sequence converges,How to prove this sequence converges,,"Here is a problem in analysis: Suppose $x_n\geq0$ and for all $n$, there is   $$  x_{n+1}\leq x_n+\dfrac1{n^2} $$   Prove that $x_n$ converges. My approach: it is easy to prove $x_m-x_n\leq \epsilon$ using telescope series. But in order to prove it is a Cauchy sequence, it has to be proved that $x_n-x_m\leq \epsilon$ too. I am not sure how to prove the second step.","Here is a problem in analysis: Suppose $x_n\geq0$ and for all $n$, there is   $$  x_{n+1}\leq x_n+\dfrac1{n^2} $$   Prove that $x_n$ converges. My approach: it is easy to prove $x_m-x_n\leq \epsilon$ using telescope series. But in order to prove it is a Cauchy sequence, it has to be proved that $x_n-x_m\leq \epsilon$ too. I am not sure how to prove the second step.",,"['real-analysis', 'limits']"
14,The limit is that which is neither too big nor too small to be the limit.,The limit is that which is neither too big nor too small to be the limit.,,"Proposed definition: $$ \lim_{x\to a} f(x) = L $$ means $L$ is the only number that is neither too big nor too small to be the limit.  This can make sense only if one says precisely what ""too big"" and ""too small"" mean.  Is there some published definition that does that and that is simpler than the usual $\varepsilon$-$\delta$ definition (but logically equivalent to it)?","Proposed definition: $$ \lim_{x\to a} f(x) = L $$ means $L$ is the only number that is neither too big nor too small to be the limit.  This can make sense only if one says precisely what ""too big"" and ""too small"" mean.  Is there some published definition that does that and that is simpler than the usual $\varepsilon$-$\delta$ definition (but logically equivalent to it)?",,"['limits', 'reference-request', 'epsilon-delta']"
15,About negligible terms in a limit,About negligible terms in a limit,,"When is it valid to deal with a term as a ""negligible"" one in a limit? I am asking this question because I usually do not take limits very seriously, and I can do a lot of ""illegal"" moves just to evaluate and get back to the important thing. This is a very broad question, that is why I'll just give two examples and ask about them instead. Example $1$: $$\lim_{n \to \infty} \frac{e^n + n^{2015} + 1}{e^\sqrt{n^2 + \sin(n) + \ln(n)}}$$ I would usually say: the numerator is asymptotic to $e^n$, and the denominator is as well, because $n^2 + \sin(n) + \ln(n)$ is asymptotic to $n^2$. The limit eventually becomes $1$. Had I done an invalid move? Example $2.1$: $$\lim_{n \to \infty} \frac{n+1}{n} \times |x|^{n} = \lim_{n \to \infty} |x|^{n}$$ Example $2.2$: $$\lim_{n \to 0} \frac{n + 1}{n + 2} \times \frac{\ln(n)}{e^n} = \frac12 \lim_{n \to 0} \frac{\ln(n)}{e^n}$$ Was my writing in $2.1$ and $2.2$ valid? Please elaborate, and if possible, warn me about some common misconceptions regarding these things. Thanks a lot.","When is it valid to deal with a term as a ""negligible"" one in a limit? I am asking this question because I usually do not take limits very seriously, and I can do a lot of ""illegal"" moves just to evaluate and get back to the important thing. This is a very broad question, that is why I'll just give two examples and ask about them instead. Example $1$: $$\lim_{n \to \infty} \frac{e^n + n^{2015} + 1}{e^\sqrt{n^2 + \sin(n) + \ln(n)}}$$ I would usually say: the numerator is asymptotic to $e^n$, and the denominator is as well, because $n^2 + \sin(n) + \ln(n)$ is asymptotic to $n^2$. The limit eventually becomes $1$. Had I done an invalid move? Example $2.1$: $$\lim_{n \to \infty} \frac{n+1}{n} \times |x|^{n} = \lim_{n \to \infty} |x|^{n}$$ Example $2.2$: $$\lim_{n \to 0} \frac{n + 1}{n + 2} \times \frac{\ln(n)}{e^n} = \frac12 \lim_{n \to 0} \frac{\ln(n)}{e^n}$$ Was my writing in $2.1$ and $2.2$ valid? Please elaborate, and if possible, warn me about some common misconceptions regarding these things. Thanks a lot.",,[]
16,Convergent Sequence and its limit,Convergent Sequence and its limit,,Can anybody help me out in this problem: I am not able to figure out how the value of lambda in the 2nd problem comes out to be 2 ? In the first problem value of lambda came out to be 2 after squaring and solving the equation. (I am assuming that I am not making any serious/silly mistake here.,Can anybody help me out in this problem: I am not able to figure out how the value of lambda in the 2nd problem comes out to be 2 ? In the first problem value of lambda came out to be 2 after squaring and solving the equation. (I am assuming that I am not making any serious/silly mistake here.,,"['limits', 'convergence-divergence']"
17,A limit involving nested trigonometric functions and logarithms,A limit involving nested trigonometric functions and logarithms,,"Evaulate   $$ L = \lim_{x \to 0} \frac{1-\cos(\sin x)+\ln(\cos x)}{x^4}. $$ I can solve it using Maclaurin series, but I'm trying to figure out a way to the solution without using it. L'Hopital would probably work but needs to be applied 4 times, and the given function in numerator is too complicated for things to work out nicely on paper. I think we can somehow use $\displaystyle\lim_{x\to0}\frac{1-\cos(x)}{x^2}=\frac12$ and $\displaystyle\lim_{x\to0}\frac{\ln(1+x)}{x}=1$. With these, I can lower the degree of denominator to $2$, like this: $$ L = \lim_{x \to 0}\frac{\frac{1 - \cos(\sin x)}{\sin^2 x}\frac{\sin^2 x}{x^2}+\frac{\ln(1+(\cos x-1))}{\cos x - 1}\frac{\cos x - 1}{x^2}}{x^2}. $$ This seems closer because left and right terms in the numerator are made of known limits, but the problem is that the whole expression is still $(1/2 - 1/2)/0 = 0/0$ so I can not break down $L$ into two limits and work things out. I'm looking only for solutions involving manipulating limits in this way. No Maclaurin and no L'Hopital if it's gonna get too messy on paper.","Evaulate   $$ L = \lim_{x \to 0} \frac{1-\cos(\sin x)+\ln(\cos x)}{x^4}. $$ I can solve it using Maclaurin series, but I'm trying to figure out a way to the solution without using it. L'Hopital would probably work but needs to be applied 4 times, and the given function in numerator is too complicated for things to work out nicely on paper. I think we can somehow use $\displaystyle\lim_{x\to0}\frac{1-\cos(x)}{x^2}=\frac12$ and $\displaystyle\lim_{x\to0}\frac{\ln(1+x)}{x}=1$. With these, I can lower the degree of denominator to $2$, like this: $$ L = \lim_{x \to 0}\frac{\frac{1 - \cos(\sin x)}{\sin^2 x}\frac{\sin^2 x}{x^2}+\frac{\ln(1+(\cos x-1))}{\cos x - 1}\frac{\cos x - 1}{x^2}}{x^2}. $$ This seems closer because left and right terms in the numerator are made of known limits, but the problem is that the whole expression is still $(1/2 - 1/2)/0 = 0/0$ so I can not break down $L$ into two limits and work things out. I'm looking only for solutions involving manipulating limits in this way. No Maclaurin and no L'Hopital if it's gonna get too messy on paper.",,['limits']
18,Limit of square root where x approaches infinity,Limit of square root where x approaches infinity,,"I have to calculate the following limit, and I wondered if my solution to the question was true. Here it is: $$\lim _{x \to -\infty} (\sqrt{(1+x+x^2)}-\sqrt{1-x+x^2})$$ Now I divide by $x^2$ and get: $$\lim _{x \to -\infty} (\sqrt{\frac{1}{x^2}+\frac{x}{x^2}+\frac{x^2}{x^2}}-\sqrt{\frac{1}{x^2}-\frac{x}{x^2}+\frac{x^2}{x^2}})$$ I know that $$\lim_{x \to -\infty}\frac{1}{x}=0$$ so I get the following: $$\lim _{x \to -\infty} (\sqrt{0+0+1}-\sqrt{0-0+1})$$ So we get the following: $$\lim _{x \to -\infty} (\sqrt{1}-\sqrt{1})=0$$ Is my solution correct? Thanks.","I have to calculate the following limit, and I wondered if my solution to the question was true. Here it is: $$\lim _{x \to -\infty} (\sqrt{(1+x+x^2)}-\sqrt{1-x+x^2})$$ Now I divide by $x^2$ and get: $$\lim _{x \to -\infty} (\sqrt{\frac{1}{x^2}+\frac{x}{x^2}+\frac{x^2}{x^2}}-\sqrt{\frac{1}{x^2}-\frac{x}{x^2}+\frac{x^2}{x^2}})$$ I know that $$\lim_{x \to -\infty}\frac{1}{x}=0$$ so I get the following: $$\lim _{x \to -\infty} (\sqrt{0+0+1}-\sqrt{0-0+1})$$ So we get the following: $$\lim _{x \to -\infty} (\sqrt{1}-\sqrt{1})=0$$ Is my solution correct? Thanks.",,"['calculus', 'limits']"
19,How do I know when the limit of a function at a certain point doesn't exist?,How do I know when the limit of a function at a certain point doesn't exist?,,$$\lim_{x \to 8} \frac{\sqrt{7+\sqrt[3]{x}}-3}{x-8}$$ I have this limit. I can't use L'Hopital. After rationalizing the numerator I get: $$\lim_{x \to 8} \frac{\sqrt[3]{x}-2}{(x-8)(\sqrt{7+\sqrt[3]{x}}+3)}$$ Isn't there anything left to do after that? How to know if the limit just doesn't exist? EDIT: I typed the limit wrong.,$$\lim_{x \to 8} \frac{\sqrt{7+\sqrt[3]{x}}-3}{x-8}$$ I have this limit. I can't use L'Hopital. After rationalizing the numerator I get: $$\lim_{x \to 8} \frac{\sqrt[3]{x}-2}{(x-8)(\sqrt{7+\sqrt[3]{x}}+3)}$$ Isn't there anything left to do after that? How to know if the limit just doesn't exist? EDIT: I typed the limit wrong.,,"['calculus', 'limits', 'limits-without-lhopital']"
20,"If $\lim\limits_{x \rightarrow \infty} f'(x)^2 + f^3(x) = 0$ , show that $\lim \limits_ {x\rightarrow \infty} f(x) = 0$ [duplicate]","If  , show that  [duplicate]",\lim\limits_{x \rightarrow \infty} f'(x)^2 + f^3(x) = 0 \lim \limits_ {x\rightarrow \infty} f(x) = 0,"This question already has answers here : How prove that $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to+\infty}f'(x)=0$ if $\lim\limits_{x\to+\infty}([f'(x)]^2+f^3(x))=0$? (3 answers) Closed 9 years ago . If   $f : \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function and $\lim\limits_{x  \rightarrow \infty} f'(x)^2 + f^3(x) = 0$ , show that $\lim\limits_{x\rightarrow \infty} f(x) = 0$. I really have no clue how to start, I tried things like MVT and using definition of derivatives but I really can't figure this out.","This question already has answers here : How prove that $\lim\limits_{x\to+\infty}f(x)=\lim\limits_{x\to+\infty}f'(x)=0$ if $\lim\limits_{x\to+\infty}([f'(x)]^2+f^3(x))=0$? (3 answers) Closed 9 years ago . If   $f : \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function and $\lim\limits_{x  \rightarrow \infty} f'(x)^2 + f^3(x) = 0$ , show that $\lim\limits_{x\rightarrow \infty} f(x) = 0$. I really have no clue how to start, I tried things like MVT and using definition of derivatives but I really can't figure this out.",,"['calculus', 'limits', 'derivatives']"
21,prove increasing/decreasing sequence,prove increasing/decreasing sequence,,"Are these two statements true? If so, how does one prove them? 1) For each integer k (positive or negative), the sequence $a_n = (1 + k/n) ^ n$ (1) is increasing (at least after a certain number n). 2) For each integer k (positive or negative) the sequence $a_n = (1 + k/n) ^{n+1} $ (2) is decreasing (at least after a certain number n). Note that I have no problem proving that the two sequences are convergent and to find their limits but I have really hard time proving formally that these are monotonic (after certain n). In fact for statement (1) seems one can prove it this way. Note that: $(1 + \frac{k+1}{n} )^n = \frac{(1 + \frac{k}{n+1})^{n+1}(1 + \frac{1}{n})^{n}}{1+\frac{k}{n+1}}$  (3) Now apply induction on k. Based on (3), the (k+1)-sequence ${a_n}$ must be increasing because on the right hand side we have: the increasing k-sequence ${a_{n+1}}$ multiplied by the increasing 1-sequence ${a_n}$, and then divided by a decreasing sequence. So the left hand side must be increasing too. Is this proof correct? Actually I think it only works for k>0 otherwise the sequence in the divisor (1 + k/(n+1)) is not decreasing but is increasing. Also, what is the proof for (2)? Should be something analogous, I guess, like the above proof for (1).","Are these two statements true? If so, how does one prove them? 1) For each integer k (positive or negative), the sequence $a_n = (1 + k/n) ^ n$ (1) is increasing (at least after a certain number n). 2) For each integer k (positive or negative) the sequence $a_n = (1 + k/n) ^{n+1} $ (2) is decreasing (at least after a certain number n). Note that I have no problem proving that the two sequences are convergent and to find their limits but I have really hard time proving formally that these are monotonic (after certain n). In fact for statement (1) seems one can prove it this way. Note that: $(1 + \frac{k+1}{n} )^n = \frac{(1 + \frac{k}{n+1})^{n+1}(1 + \frac{1}{n})^{n}}{1+\frac{k}{n+1}}$  (3) Now apply induction on k. Based on (3), the (k+1)-sequence ${a_n}$ must be increasing because on the right hand side we have: the increasing k-sequence ${a_{n+1}}$ multiplied by the increasing 1-sequence ${a_n}$, and then divided by a decreasing sequence. So the left hand side must be increasing too. Is this proof correct? Actually I think it only works for k>0 otherwise the sequence in the divisor (1 + k/(n+1)) is not decreasing but is increasing. Also, what is the proof for (2)? Should be something analogous, I guess, like the above proof for (1).",,"['calculus', 'sequences-and-series', 'limits']"
22,Better way to do $\lim_{n\to\infty}\frac{(3n-1)^{1/n}}{{\sqrt{2}}^{1/n}}$,Better way to do,\lim_{n\to\infty}\frac{(3n-1)^{1/n}}{{\sqrt{2}}^{1/n}},I want to find this limit: $$\lim_{n\to\infty}\frac{(3n-1)^{1/n}}{{\sqrt{2}}}$$ because im trying to prove the convergence of the series $$\sum_{n=1}^{\infty}\frac{3n-1}{(\sqrt{2})^{n}}$$ by the n-th root test. Wolfram Alpha gives me a step by step solution that involves L'Hopital's Rule but Im not allowed to use it yet. Any ideas? Thanks :),I want to find this limit: $$\lim_{n\to\infty}\frac{(3n-1)^{1/n}}{{\sqrt{2}}}$$ because im trying to prove the convergence of the series $$\sum_{n=1}^{\infty}\frac{3n-1}{(\sqrt{2})^{n}}$$ by the n-th root test. Wolfram Alpha gives me a step by step solution that involves L'Hopital's Rule but Im not allowed to use it yet. Any ideas? Thanks :),,"['calculus', 'sequences-and-series', 'limits', 'limits-without-lhopital']"
23,Showing $ \sum _{n=2}^\infty \frac{(-1)^n}{\sqrt n +(-1)^n} $ is not convergent,Showing  is not convergent, \sum _{n=2}^\infty \frac{(-1)^n}{\sqrt n +(-1)^n} ,Consider the series $$ \sum _{n=2}^\infty \frac{(-1)^n}{\sqrt n +(-1)^n} $$ I think it is not convergent. For this purpose I want to show that the partial sums are not bounded. I have tried so much and I think the partial sums are unbounded but I can not persuade myself. I need a good proof to persuade me for unboundedness.,Consider the series $$ \sum _{n=2}^\infty \frac{(-1)^n}{\sqrt n +(-1)^n} $$ I think it is not convergent. For this purpose I want to show that the partial sums are not bounded. I have tried so much and I think the partial sums are unbounded but I can not persuade myself. I need a good proof to persuade me for unboundedness.,,"['calculus', 'sequences-and-series', 'analysis', 'limits']"
24,The limit $\lim_{x\to 0}\frac{e^{x\ln(x)}-1}{x}$,The limit,\lim_{x\to 0}\frac{e^{x\ln(x)}-1}{x},I would like to show that $$\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=-\infty$$ without using : L'Hôpital's rule expansion series. My thoughts $$ \lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=\lim_{x\to 0^{+}}\frac{\ln(x)(e^{x\ln(x)}-1)}{x\ln(x)}=\left(\lim_{x\to 0^{+}}\ln(x)\right)\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)$$ $\left(\lim_{x\to 0^{+}}\ln(x)\right)=-\infty$ $\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)$ let $x\ln(x)=t$ then we have $$\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)=\left(\lim_{t\to 0^{+}}\frac{e^{t}-1}{t}\right)=1$$ thus $$ \lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=-\infty$$ Questions: Am I right? Is there any other way that let us to calculate that limit without using L'Hôpital's rule or any expansion series?,I would like to show that $$\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=-\infty$$ without using : L'Hôpital's rule expansion series. My thoughts $$ \lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=\lim_{x\to 0^{+}}\frac{\ln(x)(e^{x\ln(x)}-1)}{x\ln(x)}=\left(\lim_{x\to 0^{+}}\ln(x)\right)\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)$$ $\left(\lim_{x\to 0^{+}}\ln(x)\right)=-\infty$ $\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)$ let $x\ln(x)=t$ then we have $$\left(\lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x\ln(x)}\right)=\left(\lim_{t\to 0^{+}}\frac{e^{t}-1}{t}\right)=1$$ thus $$ \lim_{x\to 0^{+}}\frac{e^{x\ln(x)}-1}{x}=-\infty$$ Questions: Am I right? Is there any other way that let us to calculate that limit without using L'Hôpital's rule or any expansion series?,,"['calculus', 'limits', 'limits-without-lhopital']"
25,How to determine $\lim_{x\to \frac{\pi}{2}^-}{(\tan x)^{x-\frac{\pi}{2}}}$,How to determine,\lim_{x\to \frac{\pi}{2}^-}{(\tan x)^{x-\frac{\pi}{2}}},"Could anyone help me with this trigonometric limit? I'm trying to evaluate it using L'Hôpital's rule. I tried the following: $y = x - \frac{\pi}{2}$ and as x approaches $\frac{\pi}{2}$, $y$ approaches $0$ so I got $\lim_{x\to \frac{\pi}{2}^-}{(\tan (y + \frac{\pi}{2}))^y}$ I am stuck at deriving to get a $\frac{0}{0}$","Could anyone help me with this trigonometric limit? I'm trying to evaluate it using L'Hôpital's rule. I tried the following: $y = x - \frac{\pi}{2}$ and as x approaches $\frac{\pi}{2}$, $y$ approaches $0$ so I got $\lim_{x\to \frac{\pi}{2}^-}{(\tan (y + \frac{\pi}{2}))^y}$ I am stuck at deriving to get a $\frac{0}{0}$",,"['calculus', 'limits']"
26,Explanation for divergence of $\ln(x)$,Explanation for divergence of,\ln(x),"$\ln(x)$ diverges as $x \to \infty$, yet the differential $\frac{1}{x}$ tends to $0$ as $x \to \infty$. To the naive mind, it seems that if the differential tends to $0$ then the original function should asymptote and, therefore, converge. I appreciate that this is elementary, but an intuitive explanation would really help me.","$\ln(x)$ diverges as $x \to \infty$, yet the differential $\frac{1}{x}$ tends to $0$ as $x \to \infty$. To the naive mind, it seems that if the differential tends to $0$ then the original function should asymptote and, therefore, converge. I appreciate that this is elementary, but an intuitive explanation would really help me.",,"['calculus', 'limits', 'convergence-divergence']"
27,Limit of ratio of areas of triangles defined by tangents to a circle,Limit of ratio of areas of triangles defined by tangents to a circle,,"Let $AB $ be an arc of a circle. Tangents are drawn at $A $ and $B $ to meet at $C $. Let $M $ be the midpoint of arc $AB $. Tangent drawn at $M $ meet $AC $ and $BC $ at $D $, $E $ respectively. Evaluate $$\lim_{AB \to 0}\frac {\Delta ABC}{\Delta DEC} $$ I don't think evaluating the limit will be a problem. But how do I find the areas of the triangles and in what parameters?","Let $AB $ be an arc of a circle. Tangents are drawn at $A $ and $B $ to meet at $C $. Let $M $ be the midpoint of arc $AB $. Tangent drawn at $M $ meet $AC $ and $BC $ at $D $, $E $ respectively. Evaluate $$\lim_{AB \to 0}\frac {\Delta ABC}{\Delta DEC} $$ I don't think evaluating the limit will be a problem. But how do I find the areas of the triangles and in what parameters?",,"['geometry', 'limits']"
28,Must the limit of the derivative exists under certain assumptions?,Must the limit of the derivative exists under certain assumptions?,,"Let $u:(x_0,\infty)\to\Bbb R$ be a monotonically increasing function which is differentiable everywhere and such that $\lim_{x\to\infty}u(x)=l\in\Bbb R$. Does it follow that $\lim_{x\to\infty}u'(x)$ exists? Surely, if it exists, then it would equal zero.  In fact $$0=\lim_{x\to\infty}[u(x+1)-u(x)]=\lim_{x\to\infty}u'(\xi_x)=\lim_{x\to\infty}u'(x)$$ (this is obtained by applying the mean value theorem:  for every $x$ there exists a $\xi_x$ in between $x$ and $x+1$ such that $u'(\xi_x)=u(x+1)-u(x)$). But I think that under the above assumptions $u'$ must have a limit at infinity.  Is it true?","Let $u:(x_0,\infty)\to\Bbb R$ be a monotonically increasing function which is differentiable everywhere and such that $\lim_{x\to\infty}u(x)=l\in\Bbb R$. Does it follow that $\lim_{x\to\infty}u'(x)$ exists? Surely, if it exists, then it would equal zero.  In fact $$0=\lim_{x\to\infty}[u(x+1)-u(x)]=\lim_{x\to\infty}u'(\xi_x)=\lim_{x\to\infty}u'(x)$$ (this is obtained by applying the mean value theorem:  for every $x$ there exists a $\xi_x$ in between $x$ and $x+1$ such that $u'(\xi_x)=u(x+1)-u(x)$). But I think that under the above assumptions $u'$ must have a limit at infinity.  Is it true?",,"['real-analysis', 'limits']"
29,Do absolute convergence of $a_n$ implies convergence of $K_n=\frac{1}{\ln(n^2+1)}\sum_{k=1}^{+\infty}a_k\frac{3k^3-2k}{7-k^3}\sin k$?,Do absolute convergence of  implies convergence of ?,a_n K_n=\frac{1}{\ln(n^2+1)}\sum_{k=1}^{+\infty}a_k\frac{3k^3-2k}{7-k^3}\sin k,The problem asks us to decide whether the following statement is true: Let $\{a_n\}_{n\geq1}$ be any absolutely convergent sequence. Does that imply that the sequence: $$K_n=\frac{1}{\ln(n^2+1)}\sum_{k=1}^{n}a_k\frac{3k^3-2k}{7-k^3}\sin k$$ is convergent? The term $\frac{1}{\ln(n^2+1)}$ goes to $0$ so if we showed that the series is convergent we would finish this problem. The thing is it doesn't look like a convergent series - and even if it is I have no idea how to show it - this $\sin$ function inside means I can't use any convergence test (at least I think so). So how to reason about $K_n$? UPDATE: by absolute convergence od $a_n$ I of course mean that $\sum |a_n|$ is convergent,The problem asks us to decide whether the following statement is true: Let $\{a_n\}_{n\geq1}$ be any absolutely convergent sequence. Does that imply that the sequence: $$K_n=\frac{1}{\ln(n^2+1)}\sum_{k=1}^{n}a_k\frac{3k^3-2k}{7-k^3}\sin k$$ is convergent? The term $\frac{1}{\ln(n^2+1)}$ goes to $0$ so if we showed that the series is convergent we would finish this problem. The thing is it doesn't look like a convergent series - and even if it is I have no idea how to show it - this $\sin$ function inside means I can't use any convergence test (at least I think so). So how to reason about $K_n$? UPDATE: by absolute convergence od $a_n$ I of course mean that $\sum |a_n|$ is convergent,,"['calculus', 'sequences-and-series', 'limits']"
30,How to prove that $F(x) = \lim_{n \to \infty} F^{n}( f ' (0) \cdot F^{-n}(x)) $?,How to prove that ?,F(x) = \lim_{n \to \infty} F^{n}( f ' (0) \cdot F^{-n}(x)) ,"Let $F(x)$ be a real-analytic function near $0$ ,with $0$ as one of its fixpoints and $f ' (0) > 1$. $$F(x) = F \circ F \circ F^{-1} = \lim_{n \to \infty} F^{n} \circ F \circ F^{-n} = \lim_{n \to \infty} F^{n}(f'(0)\cdot F^{-n}(x))  $$ ($*^n$ is $n$th composition) How to prove this ? Where does this expression $\lim_{n \to \infty} F^{n}(f'(0)\cdot F^{-n}(x)) $ converge ? I assume the radius of convergeance of $F^{-1}(x)$ places an upper boundary on where this can converge. But I assume this is also true for the radius of convergeance of $F(x)$. I assume this expression converges exactly in a circle centered at $0$ with radius $T$. Where $T$ is the smallest of the ROC of both $F(x)$ and $F^{-1}(x)$. ( I assume convergeance implies convergeance to the correct value ? ) I understand $f'(0) x$ is a good approximation for $F(x)$ near $0$ and that $F^{-n}(x)$ goes to $0$ but Im not convinced. I want proofs. I assume l'Hopital can not be used here ??","Let $F(x)$ be a real-analytic function near $0$ ,with $0$ as one of its fixpoints and $f ' (0) > 1$. $$F(x) = F \circ F \circ F^{-1} = \lim_{n \to \infty} F^{n} \circ F \circ F^{-n} = \lim_{n \to \infty} F^{n}(f'(0)\cdot F^{-n}(x))  $$ ($*^n$ is $n$th composition) How to prove this ? Where does this expression $\lim_{n \to \infty} F^{n}(f'(0)\cdot F^{-n}(x)) $ converge ? I assume the radius of convergeance of $F^{-1}(x)$ places an upper boundary on where this can converge. But I assume this is also true for the radius of convergeance of $F(x)$. I assume this expression converges exactly in a circle centered at $0$ with radius $T$. Where $T$ is the smallest of the ROC of both $F(x)$ and $F^{-1}(x)$. ( I assume convergeance implies convergeance to the correct value ? ) I understand $f'(0) x$ is a good approximation for $F(x)$ near $0$ and that $F^{-n}(x)$ goes to $0$ but Im not convinced. I want proofs. I assume l'Hopital can not be used here ??",,"['real-analysis', 'limits', 'dynamical-systems', 'complex-dynamics']"
31,"Equivalence of limits $\lim\limits_{x\searrow 0}\lim\limits_{\xi\searrow x}g(x,\xi)=\lim\limits_{x\searrow 0}\lim\limits_{\xi\searrow 0}g(x,\xi)$?",Equivalence of limits ?,"\lim\limits_{x\searrow 0}\lim\limits_{\xi\searrow x}g(x,\xi)=\lim\limits_{x\searrow 0}\lim\limits_{\xi\searrow 0}g(x,\xi)","In my book, there's this modified/restricted version of l'Hôpital's rule: $$\lim_{x \searrow 0} f(x)=0 ~~~\wedge~~~ \lim_{x  \searrow 0} f'(x)=:c\quad\Longrightarrow\quad\lim_{x  \searrow 0} \frac{f(x)}{x}=c.$$ Despite the assertion ""easy"", I could only come up with this: $$c:=\lim_{x  \searrow 0} f'(x)=\lim_{x  \searrow 0} \lim_{\xi  \searrow x}\frac{f(\xi)-f(x)}{\xi-x}\overset{?}{=}\lim_{x  \searrow 0} \lim_{\xi  \searrow 0}\frac{f(\xi)-f(x)}{\xi-x}=\lim_{x  \searrow 0}\frac{f(x)}{x}$$ Is this allowed? I searched the Internet, but apparently this isn't a thing. I would be thankful for any hints regarding this or an alternative approach.","In my book, there's this modified/restricted version of l'Hôpital's rule: $$\lim_{x \searrow 0} f(x)=0 ~~~\wedge~~~ \lim_{x  \searrow 0} f'(x)=:c\quad\Longrightarrow\quad\lim_{x  \searrow 0} \frac{f(x)}{x}=c.$$ Despite the assertion ""easy"", I could only come up with this: $$c:=\lim_{x  \searrow 0} f'(x)=\lim_{x  \searrow 0} \lim_{\xi  \searrow x}\frac{f(\xi)-f(x)}{\xi-x}\overset{?}{=}\lim_{x  \searrow 0} \lim_{\xi  \searrow 0}\frac{f(\xi)-f(x)}{\xi-x}=\lim_{x  \searrow 0}\frac{f(x)}{x}$$ Is this allowed? I searched the Internet, but apparently this isn't a thing. I would be thankful for any hints regarding this or an alternative approach.",,"['limits', 'derivatives', 'proof-verification']"
32,Axes-intersections of normal tangents to an ellipse,Axes-intersections of normal tangents to an ellipse,,"Question: What values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? Let $T$ and $N$ be the tangent and normal lines to the ellipse $\frac{x^2}{9} + \frac{y^2}{4} = 1$ at any point on the ellipse in the first quadrant. Let $x_T$ and $y_T$ be the $x-$ and $y$-intercepts of $T$ and $x_N$ and $y_N$ be the intercepts of $N$ As $P$ moves along the ellipse in the first quadrant(but not on the axes), what values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? What I've managed thus far Here is a graph of one of the scenarios, Slopes of lines T and N: Slope of $T$ is equal to $y\prime = -\frac{4x}{9y}$ Slope of $N$ is equal to $-\frac{1}{y\prime} = \frac{9y}{4x}$ Line equations for lines $T$ and $N$: Line $T:\quad y - y_T = -\frac{4x}{9y}\Big(x-x_T\Big) \Leftrightarrow y = -\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T$ Line $N:\quad y - y_N = \frac{9y}{4x}\Big(x - x_N\Big) \Leftrightarrow y = \frac{9y}{4} - \frac{9yx_N}{4x} + y_N$ Now I will take a few limits on the line equations of $T$ and $N$: Limits on $T$ $\lim_{x \to 0} (y) = \lim_{x \to 0}\Big(-\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T\Big)$ That gives me $$\lim_{x\to 0} y_T = 2$$ Multiplying line $T$'s equation by $9y$, I obtain the following: $$\lim_{x\to 3} x_T = 3$$ Limits on $N$ $\lim_{x \to 3} (y)  = \lim_{x \to 3} \Big(\frac{9y}{4} - \frac{9yx_N}{4x} + y_N\Big)$ That gives me, $$\lim_{x \to 3} y_N = 0$$ Multiplying line $N$'s equation by $4x$, I obtain the following: $$\lim_{x\to 0} x_N = 0$$ That's where I'm stuck, I still need to find $\lim_{x\to 0} x_T$, $\lim_{x\to 0} y_N$, $\lim_{x\to 3} y_T$, and $\lim_{x\to 3} x_N$. That is, at the moment I have partial ranges for $x_T$, $y_T$, $x_N$, and$y_N$, as follows $$x_T(3, \lim_{x\to 0} x_T)$$ $$y_T(2, \lim_{x\to 3} y_T)$$ $$x_N(0, \lim_{x\to 3} x_N)$$ $$y_N(0, \lim_{x\to 0} y_N)$$ Does anybody have any hints, suggestions, or alternative approaches?","Question: What values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? Let $T$ and $N$ be the tangent and normal lines to the ellipse $\frac{x^2}{9} + \frac{y^2}{4} = 1$ at any point on the ellipse in the first quadrant. Let $x_T$ and $y_T$ be the $x-$ and $y$-intercepts of $T$ and $x_N$ and $y_N$ be the intercepts of $N$ As $P$ moves along the ellipse in the first quadrant(but not on the axes), what values can $x_T$,$y_T$,$x_N$, and $y_N$ take on? What I've managed thus far Here is a graph of one of the scenarios, Slopes of lines T and N: Slope of $T$ is equal to $y\prime = -\frac{4x}{9y}$ Slope of $N$ is equal to $-\frac{1}{y\prime} = \frac{9y}{4x}$ Line equations for lines $T$ and $N$: Line $T:\quad y - y_T = -\frac{4x}{9y}\Big(x-x_T\Big) \Leftrightarrow y = -\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T$ Line $N:\quad y - y_N = \frac{9y}{4x}\Big(x - x_N\Big) \Leftrightarrow y = \frac{9y}{4} - \frac{9yx_N}{4x} + y_N$ Now I will take a few limits on the line equations of $T$ and $N$: Limits on $T$ $\lim_{x \to 0} (y) = \lim_{x \to 0}\Big(-\frac{4x^2}{9y} + \frac{4xx_T}{9y} + y_T\Big)$ That gives me $$\lim_{x\to 0} y_T = 2$$ Multiplying line $T$'s equation by $9y$, I obtain the following: $$\lim_{x\to 3} x_T = 3$$ Limits on $N$ $\lim_{x \to 3} (y)  = \lim_{x \to 3} \Big(\frac{9y}{4} - \frac{9yx_N}{4x} + y_N\Big)$ That gives me, $$\lim_{x \to 3} y_N = 0$$ Multiplying line $N$'s equation by $4x$, I obtain the following: $$\lim_{x\to 0} x_N = 0$$ That's where I'm stuck, I still need to find $\lim_{x\to 0} x_T$, $\lim_{x\to 0} y_N$, $\lim_{x\to 3} y_T$, and $\lim_{x\to 3} x_N$. That is, at the moment I have partial ranges for $x_T$, $y_T$, $x_N$, and$y_N$, as follows $$x_T(3, \lim_{x\to 0} x_T)$$ $$y_T(2, \lim_{x\to 3} y_T)$$ $$x_N(0, \lim_{x\to 3} x_N)$$ $$y_N(0, \lim_{x\to 0} y_N)$$ Does anybody have any hints, suggestions, or alternative approaches?",,"['calculus', 'limits', 'derivatives']"
33,Measure theory - lebesgue's density theorem and changing order of limit,Measure theory - lebesgue's density theorem and changing order of limit,,"First of all I'll say that I did see many questions in this site that try to answer a similar question like mine, but I didn't find them usefull enough. As a part of a proof of mine I would like to claim: $$\lim_{r\rightarrow 0^+} \frac{m(\bigcup_{i\in\mathbb N}E_i\bigcap(x-r,x+r))}{2r}\leq \sum_{i\in\mathbb N}\lim_{r\rightarrow 0^+}\frac{m(E_i\bigcap(x-r,x+r))}{2r}$$ ($x\in\mathbb R$, $m$ is lebesgue measure, $r>0$, $E_i\subset \mathbb R$). Assume That each RHS limit exists.","First of all I'll say that I did see many questions in this site that try to answer a similar question like mine, but I didn't find them usefull enough. As a part of a proof of mine I would like to claim: $$\lim_{r\rightarrow 0^+} \frac{m(\bigcup_{i\in\mathbb N}E_i\bigcap(x-r,x+r))}{2r}\leq \sum_{i\in\mathbb N}\lim_{r\rightarrow 0^+}\frac{m(E_i\bigcap(x-r,x+r))}{2r}$$ ($x\in\mathbb R$, $m$ is lebesgue measure, $r>0$, $E_i\subset \mathbb R$). Assume That each RHS limit exists.",,"['real-analysis', 'limits']"
34,Do these ratios of the Eulerian number triangle converge to the logarithm of x?,Do these ratios of the Eulerian number triangle converge to the logarithm of x?,,"Consider the matrix $A_3$ with the definition if $n=k$ then $A_3(n,k)=\binom{n-1}{k-1}=1$, else if $n\ge k$ then $A_3(n,k)=\frac{\binom{n-1}{k-1}}{1-x}$ else $A_3(n,k)=0$. $\binom{n-1}{k-1}$ means the binomial of $(n-1)$ over $(k-1)$, where $n$ is the row index and $k$ is the column index. This is an infinite matrix starting starting: $$A_3=\left( \begin{array}{cccccc}  1 & 0 & 0 & 0 & 0 & 0 \cdots \\  \frac{1}{1-x} & 1 & 0 & 0 & 0 & 0 \\  \frac{1}{1-x} & \frac{2}{1-x} & 1 & 0 & 0 & 0 \\  \frac{1}{1-x} & \frac{3}{1-x} & \frac{3}{1-x} & 1 & 0 & 0 \\  \frac{1}{1-x} & \frac{4}{1-x} & \frac{6}{1-x} & \frac{4}{1-x} & 1 & 0 \\  \frac{1}{1-x} & \frac{5}{1-x} & \frac{10}{1-x} & \frac{10}{1-x} & \frac{5}{1-x} & 1 \\ \vdots&&&&&&& \ddots \end{array} \right)$$ Calculate the matrix inverse of matrix $A_3$ to get another matrix $A_4$, and focus only on the expanded form of the first column, call it $b$: $$b = \left( \begin{array}{c}  1 \\  \frac{1}{x-1} \\  \frac{x}{(x-1)^2}+\frac{1}{(x-1)^2} \\  \frac{x^2}{(x-1)^3}+\frac{4 x}{(x-1)^3}+\frac{1}{(x-1)^3} \\  \frac{x^3}{(x-1)^4}+\frac{11 x^2}{(x-1)^4}+\frac{11 x}{(x-1)^4}+\frac{1}{(x-1)^4} \\  \frac{x^4}{(x-1)^5}+\frac{26 x^3}{(x-1)^5}+\frac{66 x^2}{(x-1)^5}+\frac{26 x}{(x-1)^5}+\frac{1}{(x-1)^5} \\  \frac{x^5}{(x-1)^6}+\frac{57 x^4}{(x-1)^6}+\frac{302 x^3}{(x-1)^6}+\frac{302 x^2}{(x-1)^6}+\frac{57 x}{(x-1)^6}+\frac{1}{(x-1)^6} \\  \frac{x^6}{(x-1)^7}+\frac{120 x^5}{(x-1)^7}+\frac{1191 x^4}{(x-1)^7}+\frac{2416 x^3}{(x-1)^7}+\frac{1191 x^2}{(x-1)^7}+\frac{120 x}{(x-1)^7}+\frac{1}{(x-1)^7} \\ \vdots \end{array} \right)$$ Does the limit $$ \lim_{n\to \infty} (n - 1)\frac{b(n - 1)}{b(n)}$$ converge to $\log(x)$? I noticed that the coefficients within the vector $b$ is equal to the Eulerian numbers found in the OEIS as sequence http://oeis.org/A008292 . As a Mathematica program this is: Clear[x] x = 4 nn = 12; A1 = Table[    Table[If[n == k, 1, If[n > k, 1/(1 - x), 0]], {k, 1, nn}], {n, 1,      nn}]; MatrixForm[A1] A2 = Table[    Table[If[n >= k, Binomial[n - 1, k - 1], 0], {k, 1, nn}], {n, 1,      nn}]; MatrixForm[A2] A3 = A1*A2; MatrixForm[A3] A4 = Inverse[A3]; MatrixForm[Expand[A4]] (nn - 1)*A4[[nn - 1, 1]]/A4[[nn, 1]] N[%, 20]","Consider the matrix $A_3$ with the definition if $n=k$ then $A_3(n,k)=\binom{n-1}{k-1}=1$, else if $n\ge k$ then $A_3(n,k)=\frac{\binom{n-1}{k-1}}{1-x}$ else $A_3(n,k)=0$. $\binom{n-1}{k-1}$ means the binomial of $(n-1)$ over $(k-1)$, where $n$ is the row index and $k$ is the column index. This is an infinite matrix starting starting: $$A_3=\left( \begin{array}{cccccc}  1 & 0 & 0 & 0 & 0 & 0 \cdots \\  \frac{1}{1-x} & 1 & 0 & 0 & 0 & 0 \\  \frac{1}{1-x} & \frac{2}{1-x} & 1 & 0 & 0 & 0 \\  \frac{1}{1-x} & \frac{3}{1-x} & \frac{3}{1-x} & 1 & 0 & 0 \\  \frac{1}{1-x} & \frac{4}{1-x} & \frac{6}{1-x} & \frac{4}{1-x} & 1 & 0 \\  \frac{1}{1-x} & \frac{5}{1-x} & \frac{10}{1-x} & \frac{10}{1-x} & \frac{5}{1-x} & 1 \\ \vdots&&&&&&& \ddots \end{array} \right)$$ Calculate the matrix inverse of matrix $A_3$ to get another matrix $A_4$, and focus only on the expanded form of the first column, call it $b$: $$b = \left( \begin{array}{c}  1 \\  \frac{1}{x-1} \\  \frac{x}{(x-1)^2}+\frac{1}{(x-1)^2} \\  \frac{x^2}{(x-1)^3}+\frac{4 x}{(x-1)^3}+\frac{1}{(x-1)^3} \\  \frac{x^3}{(x-1)^4}+\frac{11 x^2}{(x-1)^4}+\frac{11 x}{(x-1)^4}+\frac{1}{(x-1)^4} \\  \frac{x^4}{(x-1)^5}+\frac{26 x^3}{(x-1)^5}+\frac{66 x^2}{(x-1)^5}+\frac{26 x}{(x-1)^5}+\frac{1}{(x-1)^5} \\  \frac{x^5}{(x-1)^6}+\frac{57 x^4}{(x-1)^6}+\frac{302 x^3}{(x-1)^6}+\frac{302 x^2}{(x-1)^6}+\frac{57 x}{(x-1)^6}+\frac{1}{(x-1)^6} \\  \frac{x^6}{(x-1)^7}+\frac{120 x^5}{(x-1)^7}+\frac{1191 x^4}{(x-1)^7}+\frac{2416 x^3}{(x-1)^7}+\frac{1191 x^2}{(x-1)^7}+\frac{120 x}{(x-1)^7}+\frac{1}{(x-1)^7} \\ \vdots \end{array} \right)$$ Does the limit $$ \lim_{n\to \infty} (n - 1)\frac{b(n - 1)}{b(n)}$$ converge to $\log(x)$? I noticed that the coefficients within the vector $b$ is equal to the Eulerian numbers found in the OEIS as sequence http://oeis.org/A008292 . As a Mathematica program this is: Clear[x] x = 4 nn = 12; A1 = Table[    Table[If[n == k, 1, If[n > k, 1/(1 - x), 0]], {k, 1, nn}], {n, 1,      nn}]; MatrixForm[A1] A2 = Table[    Table[If[n >= k, Binomial[n - 1, k - 1], 0], {k, 1, nn}], {n, 1,      nn}]; MatrixForm[A2] A3 = A1*A2; MatrixForm[A3] A4 = Inverse[A3]; MatrixForm[Expand[A4]] (nn - 1)*A4[[nn - 1, 1]]/A4[[nn, 1]] N[%, 20]",,"['limits', 'logarithms', 'binomial-coefficients', 'eulerian-numbers']"
35,Epsilon delta of recursively defined function,Epsilon delta of recursively defined function,,I was wondering if the following can be proved with the definition of limits: $$a_0=1$$ $$a_{n+1}=a_n-\frac{(a_n)^2-5}{2a_n}$$ The thing to prove is $$\lim_{n\to\infty}a_n=\sqrt5$$,I was wondering if the following can be proved with the definition of limits: $$a_0=1$$ $$a_{n+1}=a_n-\frac{(a_n)^2-5}{2a_n}$$ The thing to prove is $$\lim_{n\to\infty}a_n=\sqrt5$$,,['limits']
36,The limit of upper bounds is also an upper bound,The limit of upper bounds is also an upper bound,,"Question We have a set E which is a subset of the real numbers. There is a sequence ${x_n}$ such that $\{x_n\} \subseteq E$. Suppose there is another sequence $\{y_n\}$ such that the limit as $n$ goes to infinity for both sequences is the number $y$. We also assume that every term in the sequence $\{y_n\}$ is an upper bound for $E$. Show that $y$ is an upper bound for $E$. My reasoning First since the sequence $\{x_n\}$ is inside of E, that means that every term of the sequence must be an element of the set E. Since all terms in the sequence $\{y_n\}$ are upper bounds for the set $E$, then this means that $\{x_n\} \leq \{y_n\}$ for all $n$. I'm really stuck though. I have a feeling that that both sequences are converging to the sup of the set E, but i'm not really sure how to get started with this. A few hints or things to consider would be helpful.","Question We have a set E which is a subset of the real numbers. There is a sequence ${x_n}$ such that $\{x_n\} \subseteq E$. Suppose there is another sequence $\{y_n\}$ such that the limit as $n$ goes to infinity for both sequences is the number $y$. We also assume that every term in the sequence $\{y_n\}$ is an upper bound for $E$. Show that $y$ is an upper bound for $E$. My reasoning First since the sequence $\{x_n\}$ is inside of E, that means that every term of the sequence must be an element of the set E. Since all terms in the sequence $\{y_n\}$ are upper bounds for the set $E$, then this means that $\{x_n\} \leq \{y_n\}$ for all $n$. I'm really stuck though. I have a feeling that that both sequences are converging to the sup of the set E, but i'm not really sure how to get started with this. A few hints or things to consider would be helpful.",,"['real-analysis', 'limits', 'supremum-and-infimum']"
37,"recursive sub-sequences of sequence , one is increasing and one is decreasing to same limit -> the sequence converge?","recursive sub-sequences of sequence , one is increasing and one is decreasing to same limit -> the sequence converge?",,"Let $b_1=\:0$, $b_{n+1}\:=\:\frac{1}{1+b_n}$. I need to show that $\left(b_n\right)_{n\:=1}^{\infty }$ converge. I thought about dived $b_n$ to 2 sub_sequence : $b_{2n}$, $b_{2n+1}$. (i thought about it  because intuition: it is very clear if you start to put values to n, you see that $b_{2n-1}$ increasing and $b_{2n}\:$ decreasing) So, its true to say that ? : $b_{2n}\:=\:\begin{pmatrix}1 & n=1 \\b_{2\left(n+1\right)}\:=\:\frac{1}{1+b_{2n}} & \forall n\end{pmatrix}$, $b_{2n-1}\:=\:\begin{pmatrix}0 & n=1 \\b_{2\left(n+1\right)-1}\:=\:\frac{1}{1+b_{2n-1}} & \forall n\end{pmatrix}$ If its correct, how can i prove that these two are bounded? tnx for any help!","Let $b_1=\:0$, $b_{n+1}\:=\:\frac{1}{1+b_n}$. I need to show that $\left(b_n\right)_{n\:=1}^{\infty }$ converge. I thought about dived $b_n$ to 2 sub_sequence : $b_{2n}$, $b_{2n+1}$. (i thought about it  because intuition: it is very clear if you start to put values to n, you see that $b_{2n-1}$ increasing and $b_{2n}\:$ decreasing) So, its true to say that ? : $b_{2n}\:=\:\begin{pmatrix}1 & n=1 \\b_{2\left(n+1\right)}\:=\:\frac{1}{1+b_{2n}} & \forall n\end{pmatrix}$, $b_{2n-1}\:=\:\begin{pmatrix}0 & n=1 \\b_{2\left(n+1\right)-1}\:=\:\frac{1}{1+b_{2n-1}} & \forall n\end{pmatrix}$ If its correct, how can i prove that these two are bounded? tnx for any help!",,"['calculus', 'sequences-and-series', 'limits']"
38,Evaluating $\lim_{x→\pi^{-}}$ $\ln(\sin x)$,Evaluating,\lim_{x→\pi^{-}} \ln(\sin x),"I was wondering, if I wanted to evaluate ; $$\displaystyle\lim_{x→\pi^{-}}\ln(\sin x)$$ Would there be anyway of doing it other than calculating successively values that are near $\pi $?","I was wondering, if I wanted to evaluate ; $$\displaystyle\lim_{x→\pi^{-}}\ln(\sin x)$$ Would there be anyway of doing it other than calculating successively values that are near $\pi $?",,['calculus']
39,I don't understand a proof about volume and surface of revolution,I don't understand a proof about volume and surface of revolution,,"So I am investigating volume and surface of revolution, in particular the shape called Gabriel's Horn, which is $$\int_1^\infty \frac{1}{x}dx.$$ The interesting property about this shape is that it has finite volume but infinite surface area.The wikipedia article http://en.wikipedia.org/wiki/Gabriel%27s_Horn offers a proof that the converse is impossible, i.e there is no solid of revolution with an infinite volume and finite surface area. Here's the thing: I don't understand their proof! Sorry about the weird inequalities, that's how the proof was shown on wikipedia.. Let f be a continuous differentiable function in $[1,\infty)$ $\lim_{t \to \infty} \sup_{x \geq t} f(x)^2 ~-~ f(1)^2 = \limsup_{t \to \infty} \int_{1}^{t} (f(x)^2)' \,\mathrm{d}x$ $\leqslant \int_{1}^{\infty} |(f(x)^2)'| \,\mathrm{d}x = \int_{1}^{\infty} 2 f(x) |f'(x)| \,\mathrm{d}x$ $\leqslant \int_{1}^{\infty} 2 f(x) \sqrt{1 + f'(x)^2} \,\mathrm{d}x = {A \over \pi} < \infty.$ Therefore, there exists a $t_0$ such that the supremum $\sup\{f(x) \mid x \geq t_0\}$ is finite. Hence, $M = \sup\{f(x) \mid x \geq 1\}$ must be finite since f is a continuous function, which implies that f is bounded on the interval $[1,\infty)$ I numbered them to make it easier to answer questions or to help me understand what is going on here... I understand step 1 and 2, although I am unfamiliar with the concept of a supremum (basically the maximum x value right?) I lose them from step 2-3, I don't know where the absolute value comes from. I also don't know wtf is going on in step 3, nor step 4. I understand step 5 and 6. Sorry for the long post TL;DR: I'm a plebb and need help from steps 2-4 of this proof","So I am investigating volume and surface of revolution, in particular the shape called Gabriel's Horn, which is $$\int_1^\infty \frac{1}{x}dx.$$ The interesting property about this shape is that it has finite volume but infinite surface area.The wikipedia article http://en.wikipedia.org/wiki/Gabriel%27s_Horn offers a proof that the converse is impossible, i.e there is no solid of revolution with an infinite volume and finite surface area. Here's the thing: I don't understand their proof! Sorry about the weird inequalities, that's how the proof was shown on wikipedia.. Let f be a continuous differentiable function in $[1,\infty)$ $\lim_{t \to \infty} \sup_{x \geq t} f(x)^2 ~-~ f(1)^2 = \limsup_{t \to \infty} \int_{1}^{t} (f(x)^2)' \,\mathrm{d}x$ $\leqslant \int_{1}^{\infty} |(f(x)^2)'| \,\mathrm{d}x = \int_{1}^{\infty} 2 f(x) |f'(x)| \,\mathrm{d}x$ $\leqslant \int_{1}^{\infty} 2 f(x) \sqrt{1 + f'(x)^2} \,\mathrm{d}x = {A \over \pi} < \infty.$ Therefore, there exists a $t_0$ such that the supremum $\sup\{f(x) \mid x \geq t_0\}$ is finite. Hence, $M = \sup\{f(x) \mid x \geq 1\}$ must be finite since f is a continuous function, which implies that f is bounded on the interval $[1,\infty)$ I numbered them to make it easier to answer questions or to help me understand what is going on here... I understand step 1 and 2, although I am unfamiliar with the concept of a supremum (basically the maximum x value right?) I lose them from step 2-3, I don't know where the absolute value comes from. I also don't know wtf is going on in step 3, nor step 4. I understand step 5 and 6. Sorry for the long post TL;DR: I'm a plebb and need help from steps 2-4 of this proof",,"['calculus', 'integration', 'limits', 'volume']"
40,Limes of $a_n = i^n$,Limes of,a_n = i^n,"Out of couriosity and for my understanding i want to ask: When i have the sequence $a_n = i^n$ While i is the imaginary number, i will of course have four accumulation points: $-1,1,-i,i$. So the sequence doesn't have a limit. But does it have a limes superior / inferior? My guess is no, because $\mathbb{C}$ is not a ordered field. My tutor was not able to answer me that question. Real and imaginary part might have a lim sup/lim inf but i am not sure how to prove these.. Thanks for any hints.","Out of couriosity and for my understanding i want to ask: When i have the sequence $a_n = i^n$ While i is the imaginary number, i will of course have four accumulation points: $-1,1,-i,i$. So the sequence doesn't have a limit. But does it have a limes superior / inferior? My guess is no, because $\mathbb{C}$ is not a ordered field. My tutor was not able to answer me that question. Real and imaginary part might have a lim sup/lim inf but i am not sure how to prove these.. Thanks for any hints.",,"['sequences-and-series', 'analysis', 'limits', 'complex-numbers']"
41,Taylor series of $f(x) = \arctan(x)$ converges to $\arctan(x)$,Taylor series of  converges to,f(x) = \arctan(x) \arctan(x),"I have to find out the Taylor series of $f(x) = \arctan(x)$ and prove that it converges to $f(x)$ for any $x \in (-1, 1) $. So far I determined the Taylor series to $T_f(x) = \sum \limits_{k=0}^{\infty} (-1)^k \frac{x^{2k+1}}{2k+1} $ . With the ratio test, I proved convergence for $x^2 \lt 1$ which is satisfied by the domain of definition of $x$. But how do I continue proving that $T_f(x)$ converges to $\arctan(x)$ for $-1 \lt x \lt 1$ ?  Splitting up the series in positive and negative parts didn't help me either. EDIT : I just noticed the following: $T_f = \sum \limits_{k=0}^{\infty} \frac{x^{4k+1}}{4k+1} - \sum \limits_{k=0}^{\infty} \frac{x^{4k+3}}{4k+3} $ is the series split up in a positive and negative one. If I derive both, I get $T'_f = \sum \limits_{k=1}^{\infty} x^{4k} - \sum \limits_{k=1}^{\infty} x^2 x^{4k}$ ; can I apply the formula for geometric series here? EDIT 2 : If I apply $\sum \limits_{k=0}^{\infty} a_0 q^k $ with $x^{4k} = (x^4)^k $ thus $q=x^4$ I get  $T'_f = \frac{1}{1-x^4} - \frac{x^2}{1-x^4} = \frac{1-x^2}{(1-x^2)(1+x^2)} = \frac{1}{1+x^2}$ which is the exact derivative of $\arctan(x)$. What do you think about that?","I have to find out the Taylor series of $f(x) = \arctan(x)$ and prove that it converges to $f(x)$ for any $x \in (-1, 1) $. So far I determined the Taylor series to $T_f(x) = \sum \limits_{k=0}^{\infty} (-1)^k \frac{x^{2k+1}}{2k+1} $ . With the ratio test, I proved convergence for $x^2 \lt 1$ which is satisfied by the domain of definition of $x$. But how do I continue proving that $T_f(x)$ converges to $\arctan(x)$ for $-1 \lt x \lt 1$ ?  Splitting up the series in positive and negative parts didn't help me either. EDIT : I just noticed the following: $T_f = \sum \limits_{k=0}^{\infty} \frac{x^{4k+1}}{4k+1} - \sum \limits_{k=0}^{\infty} \frac{x^{4k+3}}{4k+3} $ is the series split up in a positive and negative one. If I derive both, I get $T'_f = \sum \limits_{k=1}^{\infty} x^{4k} - \sum \limits_{k=1}^{\infty} x^2 x^{4k}$ ; can I apply the formula for geometric series here? EDIT 2 : If I apply $\sum \limits_{k=0}^{\infty} a_0 q^k $ with $x^{4k} = (x^4)^k $ thus $q=x^4$ I get  $T'_f = \frac{1}{1-x^4} - \frac{x^2}{1-x^4} = \frac{1-x^2}{(1-x^2)(1+x^2)} = \frac{1}{1+x^2}$ which is the exact derivative of $\arctan(x)$. What do you think about that?",,"['sequences-and-series', 'analysis', 'limits', 'taylor-expansion']"
42,"Two paths that show that $\frac{x-y}{x^2 + y^2}$ has no limit when $(x,y) \rightarrow (0,0)$",Two paths that show that  has no limit when,"\frac{x-y}{x^2 + y^2} (x,y) \rightarrow (0,0)","I'm having a difficult time trying to find two different paths that give me different limits  for the following: $$\lim_{(x,y) \rightarrow (0,0)}  \frac{x-y}{x^2 + y^2}$$","I'm having a difficult time trying to find two different paths that give me different limits  for the following: $$\lim_{(x,y) \rightarrow (0,0)}  \frac{x-y}{x^2 + y^2}$$",,"['limits', 'multivariable-calculus']"
43,How to compute $\lim_{x \to \infty} \sqrt{x}(e^{-1/x}-1)$?,How to compute ?,\lim_{x \to \infty} \sqrt{x}(e^{-1/x}-1),I need to compute $$\lim_{x \to \infty} \sqrt{x}(e^{-1/x}-1)$$ using L'Hôpital's rule. Please hint me how to do it. Thanks!,I need to compute $$\lim_{x \to \infty} \sqrt{x}(e^{-1/x}-1)$$ using L'Hôpital's rule. Please hint me how to do it. Thanks!,,['limits']
44,Taylor series of a definite integral,Taylor series of a definite integral,,"Consider the function of a parameter $\alpha > 0$, given by $$f(\alpha) = \frac{2}{\sqrt 2\pi} \int_0^\infty e^{\dfrac{-x^2}{2\alpha^2}}\cosh(x)\log\cosh(x) dx.$$ From Wolfram-alpha, it seems that for small values of $\alpha$,  $$f(\alpha) = \frac{\alpha^3}{2}+\frac{\alpha^5}{2} + o(\alpha^5).$$ How can one establish this rigorously? Edit: Per the comment below, I used dfrac to make it more visible.","Consider the function of a parameter $\alpha > 0$, given by $$f(\alpha) = \frac{2}{\sqrt 2\pi} \int_0^\infty e^{\dfrac{-x^2}{2\alpha^2}}\cosh(x)\log\cosh(x) dx.$$ From Wolfram-alpha, it seems that for small values of $\alpha$,  $$f(\alpha) = \frac{\alpha^3}{2}+\frac{\alpha^5}{2} + o(\alpha^5).$$ How can one establish this rigorously? Edit: Per the comment below, I used dfrac to make it more visible.",,"['calculus', 'real-analysis', 'integration', 'limits', 'definite-integrals']"
45,Compute: $\lim_{n \to \infty } \left ( 1-\frac{2}{3} \right ) ^{\frac{3}{n}}\cdots \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}}$,Compute:,\lim_{n \to \infty } \left ( 1-\frac{2}{3} \right ) ^{\frac{3}{n}}\cdots \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}},"Help me please to compute the limit of: $ \lim_{n \to \infty }  \left ( 1-\frac{2}{3} \right ) ^{\frac{3}{n}}\cdot  \left ( 1-\frac{2}{4} \right ) ^{\frac{4}{n}}\cdot  \left ( 1-\frac{2}{5} \right ) ^{\frac{5}{n}}\cdots \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}}  $ What I did: $ L=\frac{a_{n+1}}{a_n} =   \frac{ \left ( 1-\frac{2}{n+3} \right ) ^{\frac{n+3}{n+1}}}{ \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}}}=\frac{2/e}{2/e}=1 $ But it's not. Since $L=1$, I need use something else...","Help me please to compute the limit of: $ \lim_{n \to \infty }  \left ( 1-\frac{2}{3} \right ) ^{\frac{3}{n}}\cdot  \left ( 1-\frac{2}{4} \right ) ^{\frac{4}{n}}\cdot  \left ( 1-\frac{2}{5} \right ) ^{\frac{5}{n}}\cdots \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}}  $ What I did: $ L=\frac{a_{n+1}}{a_n} =   \frac{ \left ( 1-\frac{2}{n+3} \right ) ^{\frac{n+3}{n+1}}}{ \left ( 1-\frac{2}{n+2} \right ) ^{\frac{n+2}{n}}}=\frac{2/e}{2/e}=1 $ But it's not. Since $L=1$, I need use something else...",,"['calculus', 'limits']"
46,Why is an open interval needed in this definition? (definition of a limit of a function),Why is an open interval needed in this definition? (definition of a limit of a function),,"Here's a part of the definition Ross' Elementary Analysis states for limits of a function : 20.3 Definition (a) For $a\in\mathbb R$ and a function $f$ we write $\lim_{x\to a} f(x)=L$ provided $\lim_{x\to a^S} f(x)=L$ for some set $S=J\setminus\{a\}$ where $J$ is an open interval containing $a$ . $\lim_{x\to a} f(x)$ is called the [two-sided] limit of $f$ at $a$ . Note $f$ need not be defined at $a$ and, even if $f$ is defined at $a$ , the value $f(a)$ need not equal $\lim_{x\to a} f(x)$ . In fact, $f(a)=\lim_{x\to a} f(x)$ if and only if $f$ is defined at $a$ and $f$ is continuous at $a$ . (b) For $a\in\mathbb R$ and a function $f$ we write $\lim_{x\to a^+} f(x)=L$ provided $\lim_{x\to a^S} f(x)=L$ for some open interval $S=(a,b)$ . $\lim_{x\to a^+} f(x)$ is the right-hand limit of $f$ at $a$ . Again $f$ need not be defined at $a$ . In both parts of the definition, why are open intervals needed? Would it fail if it were a closed interval instead?","Here's a part of the definition Ross' Elementary Analysis states for limits of a function : 20.3 Definition (a) For and a function we write provided for some set where is an open interval containing . is called the [two-sided] limit of at . Note need not be defined at and, even if is defined at , the value need not equal . In fact, if and only if is defined at and is continuous at . (b) For and a function we write provided for some open interval . is the right-hand limit of at . Again need not be defined at . In both parts of the definition, why are open intervals needed? Would it fail if it were a closed interval instead?","a\in\mathbb R f \lim_{x\to a} f(x)=L \lim_{x\to a^S} f(x)=L S=J\setminus\{a\} J a \lim_{x\to a} f(x) f a f a f a f(a) \lim_{x\to a} f(x) f(a)=\lim_{x\to a} f(x) f a f a a\in\mathbb R f \lim_{x\to a^+} f(x)=L \lim_{x\to a^S} f(x)=L S=(a,b) \lim_{x\to a^+} f(x) f a f a","['limits', 'definition', 'intuition']"
47,Find $\lim\limits_{x \to \infty} 2x(\sqrt{x-1} - \sqrt{x+5})$,Find,\lim\limits_{x \to \infty} 2x(\sqrt{x-1} - \sqrt{x+5}),"$\lim\limits_{x \to \infty} 2x(\sqrt{x-1} - \sqrt{x+5})$ For what i've found the part in brackets is an indeterminate form. I've tried to multiply the bracket part by $\frac{\sqrt{x - 1} + \sqrt{x+5}}{\sqrt{x - 1} + \sqrt{x+5}}$ , then multiply the numerator by $2x$. I don't know what to do next.","$\lim\limits_{x \to \infty} 2x(\sqrt{x-1} - \sqrt{x+5})$ For what i've found the part in brackets is an indeterminate form. I've tried to multiply the bracket part by $\frac{\sqrt{x - 1} + \sqrt{x+5}}{\sqrt{x - 1} + \sqrt{x+5}}$ , then multiply the numerator by $2x$. I don't know what to do next.",,"['calculus', 'limits', 'functions']"
48,Question regarding the sequence definition of continuity.,Question regarding the sequence definition of continuity.,,"Here is an excerpt from Ross' Elementary Analysis (specifically the definition of continuity): ""The function $f$ is $\it \space continuous\space  at \space x_0$ if, for every sequence $(x_n)$ in $dom(f)$ converging to $x_0$, we have $lim_nf(x_n)=f(x_0)$."" My question is the subscript in the limit needed?","Here is an excerpt from Ross' Elementary Analysis (specifically the definition of continuity): ""The function $f$ is $\it \space continuous\space  at \space x_0$ if, for every sequence $(x_n)$ in $dom(f)$ converging to $x_0$, we have $lim_nf(x_n)=f(x_0)$."" My question is the subscript in the limit needed?",,"['limits', 'continuity']"
49,Asymptotic complexity for a series,Asymptotic complexity for a series,,"Given a series defined as follows $$ E[1] = 1 $$ $$ E[n] = 1 + \frac{1}{n-1} (E[1] + E[2] + \cdots + E[n-1]) ,\quad\text{for n>1}$$ What would be $E(n)$'s asymptotic growing speed (For example, E(n)~O(log(n)) or E(n)~O(loglog(n))..)? At first sight, I though this series might converge to same fixed limit, how ever after some simple calculation, there's no limit for this series. I calculte some E's for this series, which goes like the follows E[100] = 6.177378 E[200] = 6.873031 E[300] = 7.279331 E[400] = 7.567430 E[500] = 7.790823 E[600] = 7.973312 E[700] = 8.127582 E[800] = 8.261202 E[900] = 8.379055 It seems this series grows very slowly and the growing speed seems to be something like O(log(n)). But I cannot prove it. Can anybody give me some help? Thank you.","Given a series defined as follows $$ E[1] = 1 $$ $$ E[n] = 1 + \frac{1}{n-1} (E[1] + E[2] + \cdots + E[n-1]) ,\quad\text{for n>1}$$ What would be $E(n)$'s asymptotic growing speed (For example, E(n)~O(log(n)) or E(n)~O(loglog(n))..)? At first sight, I though this series might converge to same fixed limit, how ever after some simple calculation, there's no limit for this series. I calculte some E's for this series, which goes like the follows E[100] = 6.177378 E[200] = 6.873031 E[300] = 7.279331 E[400] = 7.567430 E[500] = 7.790823 E[600] = 7.973312 E[700] = 8.127582 E[800] = 8.261202 E[900] = 8.379055 It seems this series grows very slowly and the growing speed seems to be something like O(log(n)). But I cannot prove it. Can anybody give me some help? Thank you.",,"['sequences-and-series', 'complex-analysis', 'limits']"
50,A thinking problem of limit from my teacher. [duplicate],A thinking problem of limit from my teacher. [duplicate],,This question already has answers here : How find this limits with hardly form? (2 answers) Closed 9 years ago . Please find the limit:$$\mathop {\lim }\limits_{n \to \infty } n\left[ {{{\left( {\frac{1}{\pi }\left( {\sin \left( {\frac{\pi }{{\sqrt {{n^2} + 1} }}} \right) + \sin \left( {\frac{\pi }{{\sqrt {{n^2} + 2} }}} \right) +  \cdots+ \sin \left( {\frac{\pi }{{\sqrt {{n^2} + n} }}} \right)} \right)} \right)}^n} - \frac{1}{{\sqrt[4]{e}}}} \right].$$,This question already has answers here : How find this limits with hardly form? (2 answers) Closed 9 years ago . Please find the limit:$$\mathop {\lim }\limits_{n \to \infty } n\left[ {{{\left( {\frac{1}{\pi }\left( {\sin \left( {\frac{\pi }{{\sqrt {{n^2} + 1} }}} \right) + \sin \left( {\frac{\pi }{{\sqrt {{n^2} + 2} }}} \right) +  \cdots+ \sin \left( {\frac{\pi }{{\sqrt {{n^2} + n} }}} \right)} \right)} \right)}^n} - \frac{1}{{\sqrt[4]{e}}}} \right].$$,,"['calculus', 'limits']"
51,Question on limits $ \lim\limits_{x\to 0} (\frac{\cos x}{\cos2x})^{\frac{1}{x^2}}$,Question on limits, \lim\limits_{x\to 0} (\frac{\cos x}{\cos2x})^{\frac{1}{x^2}},"$$ \lim_{x\to 0} \left(\frac{\cos x}{\cos2x}\right)^{\frac{1}{x^2}}$$ so this is my first time using this, hope i typed it out correctly. Could anyone give me a hint on this? i do know that $\cos2x= 2(\cos x)^2-1$ but does that help in solving the question? Probable ans?: $ \lim_{x\to 0} \left(\frac{\cos x}{\cos2x}\right)^{\frac{1}{x^2}}$ =$ exp\lim_{x\to 0}\left(\frac{\ln\left(\frac{cosx}{cos2x}\right)}{x^2}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(\frac{2sin2x}{cos2x}-\frac{sinx}{cosx}\right)}{2x}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(2tan2x-tanx\right)}{2x}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(4(secx)^2-(secx)^2\right)}{2}\right)$ =$exp(\frac32)$ i applied L'Hôpital's rule on lines 3 & 4. Would this be the correct answer?","$$ \lim_{x\to 0} \left(\frac{\cos x}{\cos2x}\right)^{\frac{1}{x^2}}$$ so this is my first time using this, hope i typed it out correctly. Could anyone give me a hint on this? i do know that $\cos2x= 2(\cos x)^2-1$ but does that help in solving the question? Probable ans?: $ \lim_{x\to 0} \left(\frac{\cos x}{\cos2x}\right)^{\frac{1}{x^2}}$ =$ exp\lim_{x\to 0}\left(\frac{\ln\left(\frac{cosx}{cos2x}\right)}{x^2}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(\frac{2sin2x}{cos2x}-\frac{sinx}{cosx}\right)}{2x}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(2tan2x-tanx\right)}{2x}\right)$ =$ exp\lim_{x\to 0}\left(\frac{\left(4(secx)^2-(secx)^2\right)}{2}\right)$ =$exp(\frac32)$ i applied L'Hôpital's rule on lines 3 & 4. Would this be the correct answer?",,"['calculus', 'limits']"
52,Solving a limit without l'Hopital,Solving a limit without l'Hopital,,My friend passed me a limit that (in his opinion) is resovable using only derivatives or similar method.  Here is the limit:  $$\lim_{x\to0}   \frac{\ln(1+x+x^2) - (e^x-1)}{x\sin(x)}$$  I tried to solve it using basic limits (the limit form clearly shows parts of notable limits) but my solution is $1$ that is wrong. Can someone solve it avoiding derivative methods or similar methods? Or my friend is right? Sorry for my english and thank you in advance (if what I've written is not enough please advice me) I'm supposed to solve it knowing only basic limits and algebric manipulation nothing more. Just as someone that started to do limits in his first steps,My friend passed me a limit that (in his opinion) is resovable using only derivatives or similar method.  Here is the limit:  $$\lim_{x\to0}   \frac{\ln(1+x+x^2) - (e^x-1)}{x\sin(x)}$$  I tried to solve it using basic limits (the limit form clearly shows parts of notable limits) but my solution is $1$ that is wrong. Can someone solve it avoiding derivative methods or similar methods? Or my friend is right? Sorry for my english and thank you in advance (if what I've written is not enough please advice me) I'm supposed to solve it knowing only basic limits and algebric manipulation nothing more. Just as someone that started to do limits in his first steps,,"['calculus', 'limits', 'limits-without-lhopital']"
53,Computation of a limit where De l'Hospital's rule yields no results,Computation of a limit where De l'Hospital's rule yields no results,,"I found out through Wolfram Alpha that $\lim_{x\to+\infty}x\cdot e^{-ax^2} = 0$ for $a > 0$. However, I'm not sure exactly how to verify this result myself. Trying to solve this through D.H. rule does not yield any tangible results: $\lim_{x\to+\infty}x\cdot e^{-ax^2} = \lim_{x\to+\infty}\frac{e^{-ax^2}}{\frac{1}{x}} \overset{\text{D.H rule}}{=} \lim_{x\to+\infty}\frac{(-2ax)e^{-ax^2}}{\frac{-1}{x^2}} = 2a\cdot \lim_{x\to+\infty} x^3 e^{-ax^2} $ And the process goes on, increasing the exponent $p$ of the polynomial coefficient $x^p$ by a step of 2 in every iteration. I'm wondering exactly how I can retrieve the result I noted on Wolfram.","I found out through Wolfram Alpha that $\lim_{x\to+\infty}x\cdot e^{-ax^2} = 0$ for $a > 0$. However, I'm not sure exactly how to verify this result myself. Trying to solve this through D.H. rule does not yield any tangible results: $\lim_{x\to+\infty}x\cdot e^{-ax^2} = \lim_{x\to+\infty}\frac{e^{-ax^2}}{\frac{1}{x}} \overset{\text{D.H rule}}{=} \lim_{x\to+\infty}\frac{(-2ax)e^{-ax^2}}{\frac{-1}{x^2}} = 2a\cdot \lim_{x\to+\infty} x^3 e^{-ax^2} $ And the process goes on, increasing the exponent $p$ of the polynomial coefficient $x^p$ by a step of 2 in every iteration. I'm wondering exactly how I can retrieve the result I noted on Wolfram.",,"['calculus', 'limits']"
54,Evaluate the limit using only the following results,Evaluate the limit using only the following results,,"Let $u_2>u_1>0$ and also let $u_{n+1}=\sqrt{u_n u_{n-1}}$ for all $n \geq 2$. Then prove that the sequence $\{u_n\}$ converges. For this, use of only the following results is permissible, Archimedean Property . For two sequence $\{x_n\}$ and $\{y_n\}$ $$\displaystyle\lim_{n\to\infty}\left(x_n+y_n\right)=\displaystyle\lim_{n\to\infty}x_n+\displaystyle\lim_{n\to\infty}y_n$$ $$\displaystyle\lim_{n\to\infty}\left(x_n\cdot y_n\right)=\left(\displaystyle\lim_{n\to\infty}x_n\right)\cdot\left(\displaystyle\lim_{n\to\infty}y_n\right)$$ provided they exists. For a sequence $\{z_n\}$, the limit $\displaystyle\lim_{n\to\infty}z_n$ doesn't exist is equivalent to saying that, $$\exists \varepsilon>0\mid \left\lvert z_n-l\right\rvert\geq\varepsilon \ \forall l \in \mathbb{R} \land \forall n\geq n_0 (\in \mathbb {N})$$ I have been able to prove that $\displaystyle\lim_{n\to\infty}\left(u_n u_{n+1}^2\right)=u_1u_2^2$. Now one can conclude from 1 and 2 that the limit must be $\sqrt[3]{u_1u_2^2}$ but that happens only if we can prove that the limits exist. And this is exactly where I am stuck. Using only the three mentioned results I can't prove that. Any help will be appreciated.","Let $u_2>u_1>0$ and also let $u_{n+1}=\sqrt{u_n u_{n-1}}$ for all $n \geq 2$. Then prove that the sequence $\{u_n\}$ converges. For this, use of only the following results is permissible, Archimedean Property . For two sequence $\{x_n\}$ and $\{y_n\}$ $$\displaystyle\lim_{n\to\infty}\left(x_n+y_n\right)=\displaystyle\lim_{n\to\infty}x_n+\displaystyle\lim_{n\to\infty}y_n$$ $$\displaystyle\lim_{n\to\infty}\left(x_n\cdot y_n\right)=\left(\displaystyle\lim_{n\to\infty}x_n\right)\cdot\left(\displaystyle\lim_{n\to\infty}y_n\right)$$ provided they exists. For a sequence $\{z_n\}$, the limit $\displaystyle\lim_{n\to\infty}z_n$ doesn't exist is equivalent to saying that, $$\exists \varepsilon>0\mid \left\lvert z_n-l\right\rvert\geq\varepsilon \ \forall l \in \mathbb{R} \land \forall n\geq n_0 (\in \mathbb {N})$$ I have been able to prove that $\displaystyle\lim_{n\to\infty}\left(u_n u_{n+1}^2\right)=u_1u_2^2$. Now one can conclude from 1 and 2 that the limit must be $\sqrt[3]{u_1u_2^2}$ but that happens only if we can prove that the limits exist. And this is exactly where I am stuck. Using only the three mentioned results I can't prove that. Any help will be appreciated.",,['real-analysis']
55,Convergence of $x_{n+1} = x_n + \dfrac{(\vert x_n \vert)^{1/2}}{n^2}$,Convergence of,x_{n+1} = x_n + \dfrac{(\vert x_n \vert)^{1/2}}{n^2},Let $(x_n)$ be the sequence defined by $x_{n+1} = x_n + \dfrac{(\vert x_n \vert)^{1/2}}{n^2}$ for $n \geq 2$ and $x_1$ be any real number. Then I want to prove that $x_n$ is convergent. It is obvious that $x_n$ is monotonically increasing. I also wanted to show that $x_n$ is bounded above by using the inequality $x_{n+1} < x_n + \dfrac{\vert x_n \vert}{n^2}$ but I was not successful.,Let $(x_n)$ be the sequence defined by $x_{n+1} = x_n + \dfrac{(\vert x_n \vert)^{1/2}}{n^2}$ for $n \geq 2$ and $x_1$ be any real number. Then I want to prove that $x_n$ is convergent. It is obvious that $x_n$ is monotonically increasing. I also wanted to show that $x_n$ is bounded above by using the inequality $x_{n+1} < x_n + \dfrac{\vert x_n \vert}{n^2}$ but I was not successful.,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
56,Is this the wrong way to find $\delta$?,Is this the wrong way to find ?,\delta,"Find $\delta$ such that $$|x^n - a^n| < \epsilon $$ whenever $$|x-a|<\delta$$ My thought is factoring $|x^n-a^n|$ so we have $|(x-a)|\cdot|x^{n-1}+x^{n-2}a+x^{n-3}a^2+...+xa^{n-2}+a^{n-1}|$ In order to make it smaller than $\epsilon$, I let $|x-a|<1$, so I have $$(a-1)^k<x^k<(a+1)^k$$ Thus, $$|(x-a)|\cdot |(a-1)^{n-1}+(a-1)^{n-2}a+..+a^{n-1}| <|(x-a)|\cdot|x^{n-1}+x^{n-2}a+...+xa^{n-2}+a^{n-1}| \hspace{0.5cm} (1)$$ so I choose $$\delta=\min \left(1,\frac{\epsilon}{|(a-1)^{n-1}+(a-1)^{n-2}a+..+a^{n-1}|}\right)$$ However, it strikes me that it might not be true, because inequality (1) might not hold when $a$ is negative.","Find $\delta$ such that $$|x^n - a^n| < \epsilon $$ whenever $$|x-a|<\delta$$ My thought is factoring $|x^n-a^n|$ so we have $|(x-a)|\cdot|x^{n-1}+x^{n-2}a+x^{n-3}a^2+...+xa^{n-2}+a^{n-1}|$ In order to make it smaller than $\epsilon$, I let $|x-a|<1$, so I have $$(a-1)^k<x^k<(a+1)^k$$ Thus, $$|(x-a)|\cdot |(a-1)^{n-1}+(a-1)^{n-2}a+..+a^{n-1}| <|(x-a)|\cdot|x^{n-1}+x^{n-2}a+...+xa^{n-2}+a^{n-1}| \hspace{0.5cm} (1)$$ so I choose $$\delta=\min \left(1,\frac{\epsilon}{|(a-1)^{n-1}+(a-1)^{n-2}a+..+a^{n-1}|}\right)$$ However, it strikes me that it might not be true, because inequality (1) might not hold when $a$ is negative.",,"['calculus', 'limits', 'epsilon-delta']"
57,Implications of some sort of $l^2$/uniform convergence,Implications of some sort of /uniform convergence,l^2,"Sorry about the title, but I couldn't really figure out how to describe my problem in one sentence... I'm having some problems with real limits: For $f,g : \mathbb{N} \to \mathbb{R}$ let $\mathcal{F}(f,g) := \sum\limits_{k=1}^{\infty} (f(k) - f(k+1))(g(k)-g(k+1))$ and let $D := \left\lbrace f : \mathbb{N} \to \mathbb{R}~ | ~ \mathcal{F}(f,f) < \infty \right\rbrace$. For $v \in \mathbb{N}$ set $o_v(f,g) := f(v)g(v)$. Then $\mathcal{F} + o_v$ provides a complete inner product on $D$. So far so good. Now fix $v \in \mathbb N$ and suppose we have some fixed function $f$ and a sequence of functions $f_n$ converging in $D$, i.e. $\mathcal{F}(f_n - f) + o_v(f_n -f)\to 0$ as $n \to \infty$. Obviously, it follows that $f_n \to f$ point-wise and even $$\sup\limits_{x \in \mathbb{N}} |f_n(x)-f_n(x+1) - (f(x)-f(x+1))| \to 0$$ i.e. we have some sort of uniform convergence. My question is: Can I produce some statement along the lines of $f_n(n) \to \lim\limits_{x \to \infty} f(x)$? Thoughts, help and hints are greatly appreciated :)","Sorry about the title, but I couldn't really figure out how to describe my problem in one sentence... I'm having some problems with real limits: For $f,g : \mathbb{N} \to \mathbb{R}$ let $\mathcal{F}(f,g) := \sum\limits_{k=1}^{\infty} (f(k) - f(k+1))(g(k)-g(k+1))$ and let $D := \left\lbrace f : \mathbb{N} \to \mathbb{R}~ | ~ \mathcal{F}(f,f) < \infty \right\rbrace$. For $v \in \mathbb{N}$ set $o_v(f,g) := f(v)g(v)$. Then $\mathcal{F} + o_v$ provides a complete inner product on $D$. So far so good. Now fix $v \in \mathbb N$ and suppose we have some fixed function $f$ and a sequence of functions $f_n$ converging in $D$, i.e. $\mathcal{F}(f_n - f) + o_v(f_n -f)\to 0$ as $n \to \infty$. Obviously, it follows that $f_n \to f$ point-wise and even $$\sup\limits_{x \in \mathbb{N}} |f_n(x)-f_n(x+1) - (f(x)-f(x+1))| \to 0$$ i.e. we have some sort of uniform convergence. My question is: Can I produce some statement along the lines of $f_n(n) \to \lim\limits_{x \to \infty} f(x)$? Thoughts, help and hints are greatly appreciated :)",,"['calculus', 'real-analysis', 'functional-analysis', 'limits', 'convergence-divergence']"
58,"If L = lim_x→a f(x) exists, then |f(x)| → |L| as x → a .","If L = lim_x→a f(x) exists, then |f(x)| → |L| as x → a .",,"Suppose that f is a real function. a) Prove that if $L = \lim_{x\to a} f(x)$ exists, then $|f(x)|\to |L|$ as $x \to a$ . Proof: Suppose that f is a real function. And suppose $L = lim_{x\to a} f(x)$ exists. Then by definition, this is true when given a real number $a$, in an open interval, then for every $\epsilon > 0$, there exists $\delta > 0 $ s.t $0 < | x - a| < \delta  $ implies $|f(x) - L| < epsilon$. Then by the triangle inequality $|f(x) - L| <= |f(x)| - |L| < \epsilon$. Thus $ \lim_{x\to a} |f(x)| = |L|$. Therefore $|f(x)| \to |L|$ as $x \to a$ Can anyone please help me? I am not sure if this is a correct way to prove it.  Thank you.","Suppose that f is a real function. a) Prove that if $L = \lim_{x\to a} f(x)$ exists, then $|f(x)|\to |L|$ as $x \to a$ . Proof: Suppose that f is a real function. And suppose $L = lim_{x\to a} f(x)$ exists. Then by definition, this is true when given a real number $a$, in an open interval, then for every $\epsilon > 0$, there exists $\delta > 0 $ s.t $0 < | x - a| < \delta  $ implies $|f(x) - L| < epsilon$. Then by the triangle inequality $|f(x) - L| <= |f(x)| - |L| < \epsilon$. Thus $ \lim_{x\to a} |f(x)| = |L|$. Therefore $|f(x)| \to |L|$ as $x \to a$ Can anyone please help me? I am not sure if this is a correct way to prove it.  Thank you.",,"['real-analysis', 'limits', 'functions']"
59,Matrix question: implication of $\frac{1}{n}X'X\to M$,Matrix question: implication of,\frac{1}{n}X'X\to M,"Suppose $K$ is fixed and consider a matrix $X$ that is $n\times K$ and has full column rank. Assume that we know $$ \frac{1}{n}X'X\to M\text{ as } n\to\infty.\tag{i} $$ That is, as $n$ becomes larger, we add more rows to $X$ in such a way that $X$ still has full column rank and (i) holds. It is given that $M$ is a fixed positive definite matrix with all of its eigenvalues in $(b,B)$ with some $0<b<B$. Now, let $X_i$ be the $K\times 1$ vector such that $X'_i$ is the $i$-th row of $X$. Does it follow that $$ \lim_{n\to\infty}\frac{1}{n}\max_{1\leq i\leq n}|X_i|^2=0?\tag{ii} $$ I tried looking at $$ \frac{1}{n}X'X=\frac{1}{n}\begin{pmatrix}X_1&\cdots& X_n\end{pmatrix}\begin{pmatrix}X'_1\\\cdots\\X'_n\end{pmatrix}=\frac{1}{n}\sum_{i=1}^n X_iX'_i\to M.\tag{iii} $$ It seems intuitive that each summand in (iii) has to be ""small"" and perhaps that will eventually leads me to (ii) but I can't produce a decent argument. Can someone help please? Edit : I also looked at $$ \text{Tr}\left(\frac{1}{n}X'X\right)=\frac{1}{n}\sum_i\text{Tr}(X_iX_i')=\frac{1}{n}\sum_i |X_i|^2\to\text{Tr}(M)\tag{iv} $$ but I reached a deadend here too.","Suppose $K$ is fixed and consider a matrix $X$ that is $n\times K$ and has full column rank. Assume that we know $$ \frac{1}{n}X'X\to M\text{ as } n\to\infty.\tag{i} $$ That is, as $n$ becomes larger, we add more rows to $X$ in such a way that $X$ still has full column rank and (i) holds. It is given that $M$ is a fixed positive definite matrix with all of its eigenvalues in $(b,B)$ with some $0<b<B$. Now, let $X_i$ be the $K\times 1$ vector such that $X'_i$ is the $i$-th row of $X$. Does it follow that $$ \lim_{n\to\infty}\frac{1}{n}\max_{1\leq i\leq n}|X_i|^2=0?\tag{ii} $$ I tried looking at $$ \frac{1}{n}X'X=\frac{1}{n}\begin{pmatrix}X_1&\cdots& X_n\end{pmatrix}\begin{pmatrix}X'_1\\\cdots\\X'_n\end{pmatrix}=\frac{1}{n}\sum_{i=1}^n X_iX'_i\to M.\tag{iii} $$ It seems intuitive that each summand in (iii) has to be ""small"" and perhaps that will eventually leads me to (ii) but I can't produce a decent argument. Can someone help please? Edit : I also looked at $$ \text{Tr}\left(\frac{1}{n}X'X\right)=\frac{1}{n}\sum_i\text{Tr}(X_iX_i')=\frac{1}{n}\sum_i |X_i|^2\to\text{Tr}(M)\tag{iv} $$ but I reached a deadend here too.",,"['linear-algebra', 'matrices', 'limits', 'economics']"
60,Suppose that $f(x) \ge 0$ and $\lim_{x \to c} f(x) = L$. Prove $\lim_{x \to c} \sqrt{f(x)} = \sqrt{L}$,Suppose that  and . Prove,f(x) \ge 0 \lim_{x \to c} f(x) = L \lim_{x \to c} \sqrt{f(x)} = \sqrt{L},"Suppose that $f(x) \ge 0$ in some deleted neighborhood of $c$, and that $\lim_{x \to c} f(x) = L$. Prove that $\lim_{x \to c} \sqrt{f(x)} = \sqrt{L}$ under the two different assumptions on $L$: $L=0$ and $L>0$","Suppose that $f(x) \ge 0$ in some deleted neighborhood of $c$, and that $\lim_{x \to c} f(x) = L$. Prove that $\lim_{x \to c} \sqrt{f(x)} = \sqrt{L}$ under the two different assumptions on $L$: $L=0$ and $L>0$",,"['real-analysis', 'limits', 'convergence-divergence']"
61,"Decreasing sequence, bounded below.","Decreasing sequence, bounded below.",,Suppose that $0 \leq x_1 < 1$ and  $x_{n+1} = 1 - \sqrt{1 - x_n}$ for all natural $n$. Prove that $x_n$ is decreasing and bounded below as $n$ converges. Attempt: Suppose that $0 \leq x_1 < 1$ and $x_{n+1} = 1 - \sqrt{1 - x_n}$ for all natural $n$. Then we wish to show $x_{n+1} < x_n$ for all $n$ is decreasing. Then $x_{n+1} < x_n$ implies $1 -\sqrt{1 - x_n} < x_n$ implies $1 - x_n < \sqrt{1 - x_n}$. Now by our assumption we know  $0 \leq x_n < 1$ then $0 < 1- x_n \leq 1$.  Then this shows the sequence is decreasing. Can someone please help me? I would really appreciate it. I don't know how to continue. Thank you.,Suppose that $0 \leq x_1 < 1$ and  $x_{n+1} = 1 - \sqrt{1 - x_n}$ for all natural $n$. Prove that $x_n$ is decreasing and bounded below as $n$ converges. Attempt: Suppose that $0 \leq x_1 < 1$ and $x_{n+1} = 1 - \sqrt{1 - x_n}$ for all natural $n$. Then we wish to show $x_{n+1} < x_n$ for all $n$ is decreasing. Then $x_{n+1} < x_n$ implies $1 -\sqrt{1 - x_n} < x_n$ implies $1 - x_n < \sqrt{1 - x_n}$. Now by our assumption we know  $0 \leq x_n < 1$ then $0 < 1- x_n \leq 1$.  Then this shows the sequence is decreasing. Can someone please help me? I would really appreciate it. I don't know how to continue. Thank you.,,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
62,Need help understanding logic behind limit proof based on epsilon-delta limit definition,Need help understanding logic behind limit proof based on epsilon-delta limit definition,,"I just started Calculus and am making my way through limits. They make sense to me and the $\epsilon$-$\delta$ Definition of a Limit is also clear (I think). However, I'm having trouble groking this example: I get that I want to get the left side of the $\epsilon$ inequality in the format of the $\delta$ inequality. So I have: $$ 0 < |x-2| < \delta $$ and I have: $$ \\ |x^2 - 4| < \epsilon \\ |x-2||x+2| < \epsilon $$ Now we have to do something about the $|x+2|$ term on the left side. This is where I'm having trouble following along. ""For all x in the interval $(1,3)$, $x + 2 < 5$ and thus $|x + 2| < 5$."" I understand what they're saying, but where does the $(1,3)$ interval come from? I see that it's $c\pm1$, but that seems arbitrary. Based on this arbitrary interval we get the inequality $|x + 2| < 5$. If I had picked an interval for $c\pm2$ of $(0,4)$ than I'd get an inequality of $|x+2| < 6$. It looks like based on this interval, they continued the problem like this: $$ \\|x+2| < 5 \\|x-2|*5<\epsilon \\|x-2|<\frac{\epsilon}{5} = \delta \therefore \\|x-2||x+2|<(\frac{\epsilon}{5})(5) = \epsilon $$ But if I had used a different interval, say $(0,4)$, then I'd be using $6$ everywhere instead of $5$, right? ""So, letting $\delta$ be the minimum of $\epsilon / 5$ and $1$"" : I don't get what they're saying here at all. Can $\delta$ ultimately be put into terms of $\epsilon$ via a single function, or do are we choosing a function just so that it satisfies the limit definition? What am I missing? Thanks for any help.","I just started Calculus and am making my way through limits. They make sense to me and the $\epsilon$-$\delta$ Definition of a Limit is also clear (I think). However, I'm having trouble groking this example: I get that I want to get the left side of the $\epsilon$ inequality in the format of the $\delta$ inequality. So I have: $$ 0 < |x-2| < \delta $$ and I have: $$ \\ |x^2 - 4| < \epsilon \\ |x-2||x+2| < \epsilon $$ Now we have to do something about the $|x+2|$ term on the left side. This is where I'm having trouble following along. ""For all x in the interval $(1,3)$, $x + 2 < 5$ and thus $|x + 2| < 5$."" I understand what they're saying, but where does the $(1,3)$ interval come from? I see that it's $c\pm1$, but that seems arbitrary. Based on this arbitrary interval we get the inequality $|x + 2| < 5$. If I had picked an interval for $c\pm2$ of $(0,4)$ than I'd get an inequality of $|x+2| < 6$. It looks like based on this interval, they continued the problem like this: $$ \\|x+2| < 5 \\|x-2|*5<\epsilon \\|x-2|<\frac{\epsilon}{5} = \delta \therefore \\|x-2||x+2|<(\frac{\epsilon}{5})(5) = \epsilon $$ But if I had used a different interval, say $(0,4)$, then I'd be using $6$ everywhere instead of $5$, right? ""So, letting $\delta$ be the minimum of $\epsilon / 5$ and $1$"" : I don't get what they're saying here at all. Can $\delta$ ultimately be put into terms of $\epsilon$ via a single function, or do are we choosing a function just so that it satisfies the limit definition? What am I missing? Thanks for any help.",,"['calculus', 'limits']"
63,Strictly increasing and strictly convex function that does not go to negative infinity,Strictly increasing and strictly convex function that does not go to negative infinity,,"Let $f : \mathbb{R} \to \mathbb{R}$ be continuous, strictly increasing, and strictly convex for all $x$. What are necessary and sufficient conditions for $f$ to be bounded below? Strong convexity is sufficient, but seems too strong.","Let $f : \mathbb{R} \to \mathbb{R}$ be continuous, strictly increasing, and strictly convex for all $x$. What are necessary and sufficient conditions for $f$ to be bounded below? Strong convexity is sufficient, but seems too strong.",,"['limits', 'convex-analysis']"
64,Equality about limsup.,Equality about limsup.,,"Suppose $\sum_{n=1}^\infty \mathbb P(A_n)=\infty$,then: $$\limsup_{n\to\infty}\frac{(\sum_{k=1}^n \mathbb P(A_k))^2}{\sum_{i,k=1}^n\mathbb P(A_i\cap A_k)}=\limsup_{n\to\infty}\frac{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i\cap A_k)}$$ I have tried: $$(\sum_{k=1}^n \mathbb P(A_k))^2=2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)+\sum_{k=1}^n \mathbb P^2(A_k)\le 2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)+\sum_{k=1}^n \mathbb P(A_k)$$ $$\sum_{k=1}^n \mathbb P(A_k)\le  \frac{2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{k=1}^n \mathbb P(A_k)}+1$$ Then: $$\lim_{n\to\infty}\frac{\sum_{k=1}^n \mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}=0$$ How to do next? It is a part of Kochen-Stone lemma : Suppose $\sum_{n=1}^\infty \mathbb P(A_n)=\infty$,then: $$\mathbb P(A_n ,\mathrm{i.o.})\ge\limsup_{n\to\infty}\frac{(\sum_{k=1}^n \mathbb P(A_k))^2}{\sum_{i,k=1}^n\mathbb P(A_i\cap A_k)}=\limsup_{n\to\infty}\frac{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i\cap A_k)}$$ the first inequality is proved,now I want to prove the second equality.","Suppose $\sum_{n=1}^\infty \mathbb P(A_n)=\infty$,then: $$\limsup_{n\to\infty}\frac{(\sum_{k=1}^n \mathbb P(A_k))^2}{\sum_{i,k=1}^n\mathbb P(A_i\cap A_k)}=\limsup_{n\to\infty}\frac{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i\cap A_k)}$$ I have tried: $$(\sum_{k=1}^n \mathbb P(A_k))^2=2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)+\sum_{k=1}^n \mathbb P^2(A_k)\le 2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)+\sum_{k=1}^n \mathbb P(A_k)$$ $$\sum_{k=1}^n \mathbb P(A_k)\le  \frac{2\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{k=1}^n \mathbb P(A_k)}+1$$ Then: $$\lim_{n\to\infty}\frac{\sum_{k=1}^n \mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}=0$$ How to do next? It is a part of Kochen-Stone lemma : Suppose $\sum_{n=1}^\infty \mathbb P(A_n)=\infty$,then: $$\mathbb P(A_n ,\mathrm{i.o.})\ge\limsup_{n\to\infty}\frac{(\sum_{k=1}^n \mathbb P(A_k))^2}{\sum_{i,k=1}^n\mathbb P(A_i\cap A_k)}=\limsup_{n\to\infty}\frac{\sum_{1\le i<k\le n}\mathbb P(A_i)\mathbb P(A_k)}{\sum_{1\le i<k\le n}\mathbb P(A_i\cap A_k)}$$ the first inequality is proved,now I want to prove the second equality.",,"['real-analysis', 'probability', 'measure-theory', 'limits', 'inequality']"
65,Finding the derivative of a rational function with limit definition,Finding the derivative of a rational function with limit definition,,"The problem is to find the derivative of $f(x) = \frac{3x}{x^2+1}$ at $x = -4$ using the limit definition, $$ f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h} $$ Progress I plug in $-4$ for $x$ when using the limit definition and I always end up stuck with an unfactorable denominator. I tried it $5$ times already but I always end up with the same denominator. $$\frac{-45 + 12h}{17 (h^2 - 8h + 17)}$$","The problem is to find the derivative of $f(x) = \frac{3x}{x^2+1}$ at $x = -4$ using the limit definition, $$ f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h} $$ Progress I plug in $-4$ for $x$ when using the limit definition and I always end up stuck with an unfactorable denominator. I tried it $5$ times already but I always end up with the same denominator. $$\frac{-45 + 12h}{17 (h^2 - 8h + 17)}$$",,"['calculus', 'limits', 'derivatives']"
66,"A Question Pertaining to the Mean Value Theorem on the End Points of $[a, b]$",A Question Pertaining to the Mean Value Theorem on the End Points of,"[a, b]","So I'm beginning numerical analysis and an interesting thing was brought up in class. I know the rules for MVT are: $F$ is continuous on $[a,b]$ $F$ is differentiable on $(a,b)$ So a question was brought up in class I couldn't answer. Why is it that we don't consider differentiability at the end points? The professor sort of assumed we knew the answer but I didn't. My calculus is rusty. Is there any reason we can't know why the end-points aren't differentiable? A person in class suggested it had to do something with not knowing whats on the other side of the point, but that didn't make any sense to me. Can anyone break this down for me so I can really understand it? Thank you!","So I'm beginning numerical analysis and an interesting thing was brought up in class. I know the rules for MVT are: is continuous on is differentiable on So a question was brought up in class I couldn't answer. Why is it that we don't consider differentiability at the end points? The professor sort of assumed we knew the answer but I didn't. My calculus is rusty. Is there any reason we can't know why the end-points aren't differentiable? A person in class suggested it had to do something with not knowing whats on the other side of the point, but that didn't make any sense to me. Can anyone break this down for me so I can really understand it? Thank you!","F [a,b] F (a,b)","['calculus', 'limits', 'derivatives']"
67,Find the limit and derivative of integral function.,Find the limit and derivative of integral function.,,"$\psi_m(x)$ is defined as $$\int_0^{\ln|x|}e^{mt}\sin(t)^m\mathop{dt}$$ with $m$ a natural number greater then zero. Now the question is, does $\lim\limits_{x\to 0}\psi_m(x)$ exist. I've tried using the squeeze theorem using $-1\leq \sin(t)^m\leq 1$ and this resulted in $\frac{-1}{m}\leq\psi_m(x)\leq\frac{1}{m}$, which isn't useful. Another part of the question was to determine the derivative. But I have no idea how, since the variable $x$, is in the limit of integration. I tried doing integration by parts, in hope of finding recursion, but this didn't really work (integrated by parts twice). Any ideas how to solve this?","$\psi_m(x)$ is defined as $$\int_0^{\ln|x|}e^{mt}\sin(t)^m\mathop{dt}$$ with $m$ a natural number greater then zero. Now the question is, does $\lim\limits_{x\to 0}\psi_m(x)$ exist. I've tried using the squeeze theorem using $-1\leq \sin(t)^m\leq 1$ and this resulted in $\frac{-1}{m}\leq\psi_m(x)\leq\frac{1}{m}$, which isn't useful. Another part of the question was to determine the derivative. But I have no idea how, since the variable $x$, is in the limit of integration. I tried doing integration by parts, in hope of finding recursion, but this didn't really work (integrated by parts twice). Any ideas how to solve this?",,"['integration', 'limits', 'derivatives']"
68,Chapter five of Spivak's: the second lemma,Chapter five of Spivak's: the second lemma,,"In chapter five of Spivak's, the chapter on limits, Spivak lists a lemma (the second out of three total) that is the following: if $|x-x_0|< \min(1,\frac{\epsilon}{2(|y_0|+1)})$ and $|y-y_0|< \frac{\epsilon}{2|x_0|+1}$, then $|xy-x_0y_0|< \epsilon$ My question is, wouldn't $|y-y_0|< \frac{\epsilon}{2|x_0|+1}$ have to be $|y-y_0| < \min(1,\frac{\epsilon}{2|x_0|+1})$ for the lemma above to work?","In chapter five of Spivak's, the chapter on limits, Spivak lists a lemma (the second out of three total) that is the following: if $|x-x_0|< \min(1,\frac{\epsilon}{2(|y_0|+1)})$ and $|y-y_0|< \frac{\epsilon}{2|x_0|+1}$, then $|xy-x_0y_0|< \epsilon$ My question is, wouldn't $|y-y_0|< \frac{\epsilon}{2|x_0|+1}$ have to be $|y-y_0| < \min(1,\frac{\epsilon}{2|x_0|+1})$ for the lemma above to work?",,"['calculus', 'limits']"
69,Squeeze Principle,Squeeze Principle,,"Let $\{a_n\}$ be a sequence of positive integers and let $f$ be a function on the integers. Suppose that for each $\epsilon \in (0,1)$ there exists an integer $L$ such that for every $n \geq L$ we have $$ \epsilon f(n) \leq a_n \leq f(n). $$ Is is true that we can find an $L_1$ such that $a_n = f(n)$ for every $n \geq L_1$? Thanks!","Let $\{a_n\}$ be a sequence of positive integers and let $f$ be a function on the integers. Suppose that for each $\epsilon \in (0,1)$ there exists an integer $L$ such that for every $n \geq L$ we have $$ \epsilon f(n) \leq a_n \leq f(n). $$ Is is true that we can find an $L_1$ such that $a_n = f(n)$ for every $n \geq L_1$? Thanks!",,"['calculus', 'sequences-and-series', 'limits']"
70,Evaluate Derivative $\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1}$,Evaluate Derivative,\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1},"Evaluate Derivative $\displaystyle\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1}$ I've already evaluated the limit using the $\displaystyle \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}$ definition of a limit, but now I'm curious as to how you would factor the numerator in $\displaystyle\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1}$ to get rid of the $x-1$ in the denominator. A solution to the factoring problem I encountered was using Polynomial Long Division, but I'm curious for more ways to factor it.","Evaluate Derivative I've already evaluated the limit using the definition of a limit, but now I'm curious as to how you would factor the numerator in to get rid of the in the denominator. A solution to the factoring problem I encountered was using Polynomial Long Division, but I'm curious for more ways to factor it.",\displaystyle\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1} \displaystyle \lim_{h \to 0} \frac{f(a+h) - f(a)}{h} \displaystyle\lim_{x \to 1}\frac{10x-1.86x^2 - 8.14}{x - 1} x-1,"['calculus', 'limits', 'factoring']"
71,Basic limit question to understand the methods,Basic limit question to understand the methods,,"I have a very basic question about proving limits with the epsilon-delta method. So i want to prove  $\lim _{x\to 0}\left(\frac{1}{1-2x}\right)\:=\:1$ . first, i write it like that: $\left|\frac{1}{1-2x}\:-\:1\right|\:$ = $\left|\frac{2x}{1-2x}\right|\:$  = $\frac{2\left|x\right|}{\left|1-2x\right|}\:$ and from here i stuck. I know that i need to get |x|< some expression of epsilon and stuff that will be actually my delta. What i need to do from here?","I have a very basic question about proving limits with the epsilon-delta method. So i want to prove  $\lim _{x\to 0}\left(\frac{1}{1-2x}\right)\:=\:1$ . first, i write it like that: $\left|\frac{1}{1-2x}\:-\:1\right|\:$ = $\left|\frac{2x}{1-2x}\right|\:$  = $\frac{2\left|x\right|}{\left|1-2x\right|}\:$ and from here i stuck. I know that i need to get |x|< some expression of epsilon and stuff that will be actually my delta. What i need to do from here?",,"['calculus', 'limits', 'epsilon-delta']"
72,Limit of a recursiv trigonometric function,Limit of a recursiv trigonometric function,,"So while doing my math homework I recently stumbled upon this little thing: It seems that $y_n = \sin(y_{n-1}) + k$ with arbitrary $y_0$ and $k$ converges to a definite value  for $n \to \infty $. Here is a Mathematica application for little clarification if needed: f[start_, depth_, const_] :=      N /@ NestList[Sin[#] + const &, start, depth]; Grenz[start_, const_] := N[Nest[Sin[#] + const &, start, 10000], 10]; Manipulate[    Show[ListPlot[f[start, depth, const],       PlotRange -> {{0, depth + 2}, {0,          Max@f[start, depth, const] + 0.1}}],      Plot[Grenz[start, const], {x, 0, depth + 2},       PlotStyle -> Red]], {{start, 2}, 0.5, 10}, {{depth, 5}, 1, 100,      1}, {const, 0, 10}] The question now is whether there is a way to exactly calculate the limit for any k and y_0 or at least some sort of general rule etc. that could explain that behavior? My first approach was to find a x such that sin(x) + k == sin(sin(x)+k)+k but even that seems to be above my head or just not non numerically solvable. (Btw. replace sin with sin^2 and the situation gets 6 times worse!: ) I really appreciate any help you can provide.","So while doing my math homework I recently stumbled upon this little thing: It seems that $y_n = \sin(y_{n-1}) + k$ with arbitrary $y_0$ and $k$ converges to a definite value  for $n \to \infty $. Here is a Mathematica application for little clarification if needed: f[start_, depth_, const_] :=      N /@ NestList[Sin[#] + const &, start, depth]; Grenz[start_, const_] := N[Nest[Sin[#] + const &, start, 10000], 10]; Manipulate[    Show[ListPlot[f[start, depth, const],       PlotRange -> {{0, depth + 2}, {0,          Max@f[start, depth, const] + 0.1}}],      Plot[Grenz[start, const], {x, 0, depth + 2},       PlotStyle -> Red]], {{start, 2}, 0.5, 10}, {{depth, 5}, 1, 100,      1}, {const, 0, 10}] The question now is whether there is a way to exactly calculate the limit for any k and y_0 or at least some sort of general rule etc. that could explain that behavior? My first approach was to find a x such that sin(x) + k == sin(sin(x)+k)+k but even that seems to be above my head or just not non numerically solvable. (Btw. replace sin with sin^2 and the situation gets 6 times worse!: ) I really appreciate any help you can provide.",,"['sequences-and-series', 'limits']"
73,"Calculation of the limit $\lim_{n \to +\infty} n^2x(1-x)^n, x \in [0,1]$ and the supremum",Calculation of the limit  and the supremum,"\lim_{n \to +\infty} n^2x(1-x)^n, x \in [0,1]","How can I find this limit: $$\lim_{n \to +\infty} n^2x(1-x)^n, x \in [0,1]$$ Do I have to use the L'Hospital rule? If so, do I have to differentiate with respect to $n$ or to $x$ ? EDIT: I also tried to find the supremum of $n^2x(1-x)^n$..I found that $f(x)=n^2x(1-x)^n$ achieves its maximum at the point $\frac{1}{n+1}$ and that $\sup_{x \in [0,1]} n^2x(1-x)^n=\frac{n}{e} \to +\infty$ Could you tell me if it is right?","How can I find this limit: $$\lim_{n \to +\infty} n^2x(1-x)^n, x \in [0,1]$$ Do I have to use the L'Hospital rule? If so, do I have to differentiate with respect to $n$ or to $x$ ? EDIT: I also tried to find the supremum of $n^2x(1-x)^n$..I found that $f(x)=n^2x(1-x)^n$ achieves its maximum at the point $\frac{1}{n+1}$ and that $\sup_{x \in [0,1]} n^2x(1-x)^n=\frac{n}{e} \to +\infty$ Could you tell me if it is right?",,['limits']
74,Show for whih values this following function is continuous,Show for whih values this following function is continuous,,"For the function $f: [0,2 \pi] \rightarrow \mathbb{R}$ ,state at which points $c \in [0, \pi]$ is $f$ continuous or discontinuous. $$f(x)=\begin{array}{cc}   ( &      \begin{array}{cc}       0 & x \in \mathbb{Q} \\       sin(x) &  x \notin \mathbb{Q}     \end{array} \end{array}) $$ My Attempt If $c \in \mathbb{Q}$ then there is a sequence $c_n \notin \mathbb{Q}$ with $ cn \rightarrow c$ and $f$ being continuous at $c$ requires $$\lim_{n \to \infty}f(c_n)=\lim_{n \to \infty}sin(c_n)=sin(c)=0=f(c)$$ Now this condition is only necessary not sufficient. On the otherhand, if $c \notin \mathbb{Q}$ then there's a sequence $c_n \in \mathbb{Q}$  with  $c_n \rightarrow c$ and just as above the necessary condition is; $$\lim_{n \to \infty}f(c_n)=\lim_{n \to \infty}0=0=sin(c)=f(c)$$ for $f$ to be continuous at $c$. So we know that $f$ is discontinuous at all points $[0,2 \pi]$ except possibly at points where $0=sin(c)$ i.e $c=0$ $c= \pi$ and $c=2\pi$ Since the conditions above are only necessary not sufficient we cannot immediately conclude that $f$ is continuous at these points we need to verify; i.e at $c=\pi$ $$\lim_{x \to \pi}f(x)=f(\pi)=sin(\pi)=0$$ Let $|x-\pi|<\epsilon$ then if $x \in \mathbb{Q}$ $$|f(x)-f(\pi)|=|f(x)-f(\pi)|=|0|<\epsilon$$ if $x \notin \mathbb{Q}$ then $$|f(x)-f(\pi)|=|sin(x)-0|<\epsilon$$ I am not sure about the last part of the proof i.e the verification part, is this correct any feedback would be much appreciated.","For the function $f: [0,2 \pi] \rightarrow \mathbb{R}$ ,state at which points $c \in [0, \pi]$ is $f$ continuous or discontinuous. $$f(x)=\begin{array}{cc}   ( &      \begin{array}{cc}       0 & x \in \mathbb{Q} \\       sin(x) &  x \notin \mathbb{Q}     \end{array} \end{array}) $$ My Attempt If $c \in \mathbb{Q}$ then there is a sequence $c_n \notin \mathbb{Q}$ with $ cn \rightarrow c$ and $f$ being continuous at $c$ requires $$\lim_{n \to \infty}f(c_n)=\lim_{n \to \infty}sin(c_n)=sin(c)=0=f(c)$$ Now this condition is only necessary not sufficient. On the otherhand, if $c \notin \mathbb{Q}$ then there's a sequence $c_n \in \mathbb{Q}$  with  $c_n \rightarrow c$ and just as above the necessary condition is; $$\lim_{n \to \infty}f(c_n)=\lim_{n \to \infty}0=0=sin(c)=f(c)$$ for $f$ to be continuous at $c$. So we know that $f$ is discontinuous at all points $[0,2 \pi]$ except possibly at points where $0=sin(c)$ i.e $c=0$ $c= \pi$ and $c=2\pi$ Since the conditions above are only necessary not sufficient we cannot immediately conclude that $f$ is continuous at these points we need to verify; i.e at $c=\pi$ $$\lim_{x \to \pi}f(x)=f(\pi)=sin(\pi)=0$$ Let $|x-\pi|<\epsilon$ then if $x \in \mathbb{Q}$ $$|f(x)-f(\pi)|=|f(x)-f(\pi)|=|0|<\epsilon$$ if $x \notin \mathbb{Q}$ then $$|f(x)-f(\pi)|=|sin(x)-0|<\epsilon$$ I am not sure about the last part of the proof i.e the verification part, is this correct any feedback would be much appreciated.",,"['real-analysis', 'limits', 'functions', 'continuity', 'proof-verification']"
75,How do I resrict delta here?,How do I resrict delta here?,,Prove that $\lim_{x\to 1}(x^2-1)=0$ $|(x^2 -1) -0| < \epsilon$ \* $|x^2 - 1| < \epsilon$ \* $|(x+1)(x-1)| < \epsilon$ \* $|x+1||x-1| < \epsilon$ I know I need to restrict delta now but how do I do that?,Prove that $\lim_{x\to 1}(x^2-1)=0$ $|(x^2 -1) -0| < \epsilon$ \* $|x^2 - 1| < \epsilon$ \* $|(x+1)(x-1)| < \epsilon$ \* $|x+1||x-1| < \epsilon$ I know I need to restrict delta now but how do I do that?,,['limits']
76,Limits in cofinite topology/why is the limit of x_n = n equal to 1 in the cofinite topology.,Limits in cofinite topology/why is the limit of x_n = n equal to 1 in the cofinite topology.,,"Just reading about topological spaces for my exam, and I was wondering if anybody could explain exactly how limits work in the cofinite topology. So I am aware of the topological definition of a limit: $ Let~(X, \tau)$ be a topological space, and let $x_n$ be a sequence in $X$. x_n is convergent if $\exists L$ s.t.$\forall~U~\in~\tau$, with $L \in U, \exists n\geq N$ s.t. $n\geq N \implies x_n \in U. $ I just can not see how to apply this to the cofinite topology. My lectures claim that $x_n = n \rightarrow 1. $ I have looked elsewhere for answers, but I can't really grasp what they are trying to say. Hopefully, explaining this example will make it clear to me. Thanks, MSE!","Just reading about topological spaces for my exam, and I was wondering if anybody could explain exactly how limits work in the cofinite topology. So I am aware of the topological definition of a limit: $ Let~(X, \tau)$ be a topological space, and let $x_n$ be a sequence in $X$. x_n is convergent if $\exists L$ s.t.$\forall~U~\in~\tau$, with $L \in U, \exists n\geq N$ s.t. $n\geq N \implies x_n \in U. $ I just can not see how to apply this to the cofinite topology. My lectures claim that $x_n = n \rightarrow 1. $ I have looked elsewhere for answers, but I can't really grasp what they are trying to say. Hopefully, explaining this example will make it clear to me. Thanks, MSE!",,"['general-topology', 'limits', 'metric-spaces']"
77,Condition for writing an infinite summation of a sequence as a limit,Condition for writing an infinite summation of a sequence as a limit,,"Under what conditions is it true that an infinite summation of a sequence can be rewritten as a limit, i.e. $$\displaystyle\lim_{n \rightarrow \infty} \sum_{j=1}^n a_j = \sum_{k=1}^\infty a_k$$ Or alternately, when can we rewrite the limit to infinity as an infinite summation? EDIT: Taking user2388's example, consider $$\displaystyle \sum_{k=1}^\infty \frac{1}{k}$$ The sum does not exist. In such a case, would it still be valid to write: $$\displaystyle\lim_{n \rightarrow \infty} \sum_{j=1}^n \frac{1}{j} = \sum_{k=1}^\infty \frac{1}{k} $$","Under what conditions is it true that an infinite summation of a sequence can be rewritten as a limit, i.e. $$\displaystyle\lim_{n \rightarrow \infty} \sum_{j=1}^n a_j = \sum_{k=1}^\infty a_k$$ Or alternately, when can we rewrite the limit to infinity as an infinite summation? EDIT: Taking user2388's example, consider $$\displaystyle \sum_{k=1}^\infty \frac{1}{k}$$ The sum does not exist. In such a case, would it still be valid to write: $$\displaystyle\lim_{n \rightarrow \infty} \sum_{j=1}^n \frac{1}{j} = \sum_{k=1}^\infty \frac{1}{k} $$",,"['calculus', 'real-analysis', 'limits', 'convergence-divergence']"
78,limit notation syntax validity,limit notation syntax validity,,"If you know $$\lim_{x \to \infty} f(x) = -\infty$$ and $$\lim_{x \to -\infty} f(x) = -\infty$$, is it valid syntax to write $$\lim_{x \to \pm \infty} f(x) = -\infty$$?","If you know $$\lim_{x \to \infty} f(x) = -\infty$$ and $$\lim_{x \to -\infty} f(x) = -\infty$$, is it valid syntax to write $$\lim_{x \to \pm \infty} f(x) = -\infty$$?",,"['limits', 'notation']"
79,Finding the value of one-sided limits and greatest integer function.,Finding the value of one-sided limits and greatest integer function.,,"$$ \lim_{x \to 0} \frac{a}{x} \left\lfloor\frac{x}{b} \right\rfloor $$ The $\lfloor \rfloor$ stands for the greatest integer function. I have calculated and the left-hand limit is coming as (ab). But, I have doubt in the right-hand limit. I did this problem by sandwich-theorem. Can, anyone help me to find the right-hand limit correctly?","$$ \lim_{x \to 0} \frac{a}{x} \left\lfloor\frac{x}{b} \right\rfloor $$ The $\lfloor \rfloor$ stands for the greatest integer function. I have calculated and the left-hand limit is coming as (ab). But, I have doubt in the right-hand limit. I did this problem by sandwich-theorem. Can, anyone help me to find the right-hand limit correctly?",,"['calculus', 'limits', 'ceiling-and-floor-functions']"
80,Limit of $f'(x) e^{-f(x)}$,Limit of,f'(x) e^{-f(x)},"Let $f$ be a real function verifying $f''\geq C>0$, where C is a constant. Do we have : $\lim_{x\to +\infty}f'(x) e^{-f(x)}=0$ ?","Let $f$ be a real function verifying $f''\geq C>0$, where C is a constant. Do we have : $\lim_{x\to +\infty}f'(x) e^{-f(x)}=0$ ?",,"['real-analysis', 'limits']"
81,Continuity in $\mathbb R^n$.,Continuity in .,\mathbb R^n,"we just got started with this topic today, and I am confused. Let $f:\Bbb R^2 \to \Bbb R $ with $$f(x,y) =\begin{cases}    y\sin(x)/x &\text{if } x \ne 0\\   0 &\text{else} \end{cases}$$ Now, for $x \not= 0$ it is continuous, because its components are. But what do I have to do to show (dis)continuity? I know I have to approach a point (I guess $(0,y)$) with every curve possible. What I have tried so far, but believe to be false: I approximate $y\cdot\sin(x)/x$ as follows: Let $x$ be $\not= 0$. $|f(x,y)| = |y|\cdot|\sin(x)|/|x| \le   |y|\cdot|x|/|x| = |y|  \to |y| \text{ for } (x,y) \to (0,y)$ I don't quite know what I can tell from this, because of the less than or equal sign. If somebody could help me I'd appreciate it a lot! Thanks.","we just got started with this topic today, and I am confused. Let $f:\Bbb R^2 \to \Bbb R $ with $$f(x,y) =\begin{cases}    y\sin(x)/x &\text{if } x \ne 0\\   0 &\text{else} \end{cases}$$ Now, for $x \not= 0$ it is continuous, because its components are. But what do I have to do to show (dis)continuity? I know I have to approach a point (I guess $(0,y)$) with every curve possible. What I have tried so far, but believe to be false: I approximate $y\cdot\sin(x)/x$ as follows: Let $x$ be $\not= 0$. $|f(x,y)| = |y|\cdot|\sin(x)|/|x| \le   |y|\cdot|x|/|x| = |y|  \to |y| \text{ for } (x,y) \to (0,y)$ I don't quite know what I can tell from this, because of the less than or equal sign. If somebody could help me I'd appreciate it a lot! Thanks.",,"['limits', 'multivariable-calculus', 'continuity']"
82,Multivariable calculus limit proof,Multivariable calculus limit proof,,"I came across with this statement and I can't neither prove it right nor find a counterexample. The statement is: Consider two functions $F(x,y)$ and $G(x,y)$ continuous and differentiable around a point $(a,b)$ . If the limit $$\lim_{(x,y)\to(a,b)}\frac{F(x,y)}{G(x,y)}$$ is of the indeterminate form $\frac{0}{0}$ and $$\left[\frac{\partial F}{\partial x}\frac{\partial G}{\partial y}-\frac{\partial F}{\partial y}\frac{\partial G}{\partial x}\right]_{(a,b)}\neq0$$ Then the previous limit does not exist. I've tried several examples and it seems to work. Any hint will be extremely appreciated. Thanks!",I came across with this statement and I can't neither prove it right nor find a counterexample. The statement is: Consider two functions and continuous and differentiable around a point . If the limit is of the indeterminate form and Then the previous limit does not exist. I've tried several examples and it seems to work. Any hint will be extremely appreciated. Thanks!,"F(x,y) G(x,y) (a,b) \lim_{(x,y)\to(a,b)}\frac{F(x,y)}{G(x,y)} \frac{0}{0} \left[\frac{\partial F}{\partial x}\frac{\partial G}{\partial y}-\frac{\partial F}{\partial y}\frac{\partial G}{\partial x}\right]_{(a,b)}\neq0","['limits', 'multivariable-calculus']"
83,Limit of a rational function to the power of x,Limit of a rational function to the power of x,,"Ok so I have been trying for days already to find a solution to this all around the web and in math books but to no success. The problem is to evaluate a limit of a function composed by polynomial functions and an exponential function: $$\lim_{x \to +\infty} \left(\frac{3x+2}{3x-2}\right)^{2x}$$ I know from a software that the solution is $\exp\left(\dfrac{8}{3}\right)$, but I can't reach this. One thing I did to try to find the limit was: $$\lim_{x \to +\infty}\left(\frac{3x+2}{3x-2}\right)^{2x}=\lim_{x \to +\infty}\exp\left(\ln\left(\left(\frac{3x+2}{3x-2}\right)^{2x}\right)\right)\\ =\lim_{x \to +\infty}\exp\left(2x\ln\left(\frac{3x+2}{3x-2}\right)\right) =\exp{\left(\lim_{x \to +\infty}2x\times\lim_{x \to +\infty}\ln\left(\frac{3x+2}{3x-2}\right)\right)}$$ But this doesn't work because the limit to the right goes to zero while the one on the left goes to infinity. I tried other things too, but the problem only gets more complicated and a solution seems to get farther and farther away.","Ok so I have been trying for days already to find a solution to this all around the web and in math books but to no success. The problem is to evaluate a limit of a function composed by polynomial functions and an exponential function: $$\lim_{x \to +\infty} \left(\frac{3x+2}{3x-2}\right)^{2x}$$ I know from a software that the solution is $\exp\left(\dfrac{8}{3}\right)$, but I can't reach this. One thing I did to try to find the limit was: $$\lim_{x \to +\infty}\left(\frac{3x+2}{3x-2}\right)^{2x}=\lim_{x \to +\infty}\exp\left(\ln\left(\left(\frac{3x+2}{3x-2}\right)^{2x}\right)\right)\\ =\lim_{x \to +\infty}\exp\left(2x\ln\left(\frac{3x+2}{3x-2}\right)\right) =\exp{\left(\lim_{x \to +\infty}2x\times\lim_{x \to +\infty}\ln\left(\frac{3x+2}{3x-2}\right)\right)}$$ But this doesn't work because the limit to the right goes to zero while the one on the left goes to infinity. I tried other things too, but the problem only gets more complicated and a solution seems to get farther and farther away.",,"['calculus', 'limits']"
84,"Help with sequence problem, expressing it as a function of a?","Help with sequence problem, expressing it as a function of a?",,I'm working on a problem set for a math course right now and I've come across a problem that I am having some difficulty understanding/solving. The problem is below: Consider the sequence: for different values of a. Try to generalize the value of the limit as a function of a; give a written discussion as to why your limit value or values should be true mathematically. I do not understand the part where I must determine some function of a. I would really appreciate some from anyone in understanding this problem. Thanks guys,I'm working on a problem set for a math course right now and I've come across a problem that I am having some difficulty understanding/solving. The problem is below: Consider the sequence: for different values of a. Try to generalize the value of the limit as a function of a; give a written discussion as to why your limit value or values should be true mathematically. I do not understand the part where I must determine some function of a. I would really appreciate some from anyone in understanding this problem. Thanks guys,,"['sequences-and-series', 'limits']"
85,Partial limits and sets of indices,Partial limits and sets of indices,,"Let $\{a_n\}$, and $A_1...A_n \subseteq \mathbb{N}$, such that $A_1\cup A_2... \cup A_n= \mathbb{N}$. We denote $P_k$ as the set of all partial limits (= limit of a subsequence) with indices $\in$ $A_k$. Show that $P$, the set of all partial limits is:  $$P = P_1 \cup... \cup P_n$$ My thoughts: First denote $L_i$ a partial limit of $A_i$, and $L_j$, a partial limit of $A_j$. Lets assume by contradiction that there's a limit $L_k$ which involves indices from both $A_i$ and $A_j$. By definition of limit: $$\forall \varepsilon  > 0\exists K \in N.\forall k > K:\left| {{a_{{n_k}}} - {L_k}} \right| < \varepsilon $$ We notice that $\left| {{L_i} - {L_k}} \right|$ is a fixed size. The subsequences are infinite, and therefore we can choose $n_{{k_0}} \in A_i$ such that: $$\left| {{a_{{n_{{k_0}}}}} - {L_k}} \right| < \varepsilon  \wedge \left| {{a_{{n_{{k_0}}}}} - {L_i}} \right| > \varepsilon $$ Which contradicts the assumption $L_i$ is a partial limit of $\{a_n\}$. Can you criticize my work? Am I right? If not, what should I do different?","Let $\{a_n\}$, and $A_1...A_n \subseteq \mathbb{N}$, such that $A_1\cup A_2... \cup A_n= \mathbb{N}$. We denote $P_k$ as the set of all partial limits (= limit of a subsequence) with indices $\in$ $A_k$. Show that $P$, the set of all partial limits is:  $$P = P_1 \cup... \cup P_n$$ My thoughts: First denote $L_i$ a partial limit of $A_i$, and $L_j$, a partial limit of $A_j$. Lets assume by contradiction that there's a limit $L_k$ which involves indices from both $A_i$ and $A_j$. By definition of limit: $$\forall \varepsilon  > 0\exists K \in N.\forall k > K:\left| {{a_{{n_k}}} - {L_k}} \right| < \varepsilon $$ We notice that $\left| {{L_i} - {L_k}} \right|$ is a fixed size. The subsequences are infinite, and therefore we can choose $n_{{k_0}} \in A_i$ such that: $$\left| {{a_{{n_{{k_0}}}}} - {L_k}} \right| < \varepsilon  \wedge \left| {{a_{{n_{{k_0}}}}} - {L_i}} \right| > \varepsilon $$ Which contradicts the assumption $L_i$ is a partial limit of $\{a_n\}$. Can you criticize my work? Am I right? If not, what should I do different?",,"['calculus', 'limits', 'probability-limit-theorems']"
86,Show that $\int\limits_{-\infty}^\infty f(t)dt=0$ where $f\in H^\infty(\mathbb{H})$,Show that  where,\int\limits_{-\infty}^\infty f(t)dt=0 f\in H^\infty(\mathbb{H}),"The problem is stated as follows: Let $\mathbb{H}$ denote the open upper half plane. Let $f \in H^{\infty}(\mathbb{H})$ Suppose $f$ can be extended to be continuous on $\overline{\mathbb{H}}$ with  $$\int_{-\infty}^\infty |f(t)|dt<\infty$$ Show that    $$\int_{-\infty}^\infty f(t)dt=0$$ and, the following hint is provided: Remark: Let $C_R$ denote the upper semicircle with radius $R$. It is not so obvious why $\int_{C_R}f(z)dz\longrightarrow 0$ as $R\longrightarrow \infty$. A technique to overcome this is to first consider the functions $f_\epsilon(z)=f(z)e^{i\epsilon z}$ where $\epsilon>0$ It is straightforward to show that $\int_{C_R}f_\epsilon(z)dz\longrightarrow 0$ as $R\longrightarrow \infty$. The result for $f_\epsilon$ follows immediately. One then lets $\epsilon \longrightarrow 0$ and shows the convergence of the integrals."" The problem that I am having is that,Assuming $$\underset{z\in \mathbb{H}}{\max}|f(z)|=M,$$ we get $$\begin{align} \left|\int_{C_R}f_\epsilon(z)dz\right|&=\left| \int_{C_R}f(z)e^{i\epsilon z}dz\right| \\ &= \left|\int_{C_R}f(z)e^{i\epsilon(x+iy)}dz\right| \\  &\le \int_{C_R}\left| f(z)e^{-\epsilon y+i\epsilon x}\right|dz \\ &=\int_{C_R}\left| f(z)e^{-\epsilon y}\right|dz \\ &\le \int_{C_R} \frac{M}{e^{\epsilon y}}dz \end{align}$$ My Question: From here, I just don't see how to make the R.H.S. go to zero. I tried parameterizing it, but then the best I can do for an upper bound is $$ \int_0^\pi \frac{MR}{e^{\epsilon R \sin{t}}}dt$$ And, while this looks a little more promising, I still cant get this integral to go to zero either because, when $t=0$, $\sin{t}=0$. I asked my professor for clarification, and he replied that ``In any case, a straightforward application of ML to the integrand from 0 to π won't work. We had the same integral (after putting in absolute value etc) for one of the contour integral examples we did in class. We got a usable bound by making two very elementary observations on the sin function."" I tried to find this in my notes, but the only integral that looks similar to this one that I can find was quite a bit different, and, there, we were able to apply ML.","The problem is stated as follows: Let $\mathbb{H}$ denote the open upper half plane. Let $f \in H^{\infty}(\mathbb{H})$ Suppose $f$ can be extended to be continuous on $\overline{\mathbb{H}}$ with  $$\int_{-\infty}^\infty |f(t)|dt<\infty$$ Show that    $$\int_{-\infty}^\infty f(t)dt=0$$ and, the following hint is provided: Remark: Let $C_R$ denote the upper semicircle with radius $R$. It is not so obvious why $\int_{C_R}f(z)dz\longrightarrow 0$ as $R\longrightarrow \infty$. A technique to overcome this is to first consider the functions $f_\epsilon(z)=f(z)e^{i\epsilon z}$ where $\epsilon>0$ It is straightforward to show that $\int_{C_R}f_\epsilon(z)dz\longrightarrow 0$ as $R\longrightarrow \infty$. The result for $f_\epsilon$ follows immediately. One then lets $\epsilon \longrightarrow 0$ and shows the convergence of the integrals."" The problem that I am having is that,Assuming $$\underset{z\in \mathbb{H}}{\max}|f(z)|=M,$$ we get $$\begin{align} \left|\int_{C_R}f_\epsilon(z)dz\right|&=\left| \int_{C_R}f(z)e^{i\epsilon z}dz\right| \\ &= \left|\int_{C_R}f(z)e^{i\epsilon(x+iy)}dz\right| \\  &\le \int_{C_R}\left| f(z)e^{-\epsilon y+i\epsilon x}\right|dz \\ &=\int_{C_R}\left| f(z)e^{-\epsilon y}\right|dz \\ &\le \int_{C_R} \frac{M}{e^{\epsilon y}}dz \end{align}$$ My Question: From here, I just don't see how to make the R.H.S. go to zero. I tried parameterizing it, but then the best I can do for an upper bound is $$ \int_0^\pi \frac{MR}{e^{\epsilon R \sin{t}}}dt$$ And, while this looks a little more promising, I still cant get this integral to go to zero either because, when $t=0$, $\sin{t}=0$. I asked my professor for clarification, and he replied that ``In any case, a straightforward application of ML to the integrand from 0 to π won't work. We had the same integral (after putting in absolute value etc) for one of the contour integral examples we did in class. We got a usable bound by making two very elementary observations on the sin function."" I tried to find this in my notes, but the only integral that looks similar to this one that I can find was quite a bit different, and, there, we were able to apply ML.",,"['calculus', 'integration', 'complex-analysis', 'limits', 'contour-integration']"
87,Proof that $0 < \lim(a_n/b_n) < \infty$ implies convergence/divergence of $a_n$ and $b_n$,Proof that  implies convergence/divergence of  and,0 < \lim(a_n/b_n) < \infty a_n b_n,"Suppose that $a_n, b_n > 0$ for all $n$. Prove that if $0 < \lim(a_n/b_n) < \infty$, then $\sum a_n$ and $\sum b_n$ either both converge or both diverge. I'm a bit unsure on how to proceed with the proof... Any suggestions? My class uses Ross' book.","Suppose that $a_n, b_n > 0$ for all $n$. Prove that if $0 < \lim(a_n/b_n) < \infty$, then $\sum a_n$ and $\sum b_n$ either both converge or both diverge. I'm a bit unsure on how to proceed with the proof... Any suggestions? My class uses Ross' book.",,"['real-analysis', 'sequences-and-series', 'limits', 'summation']"
88,How to calculate lim inf and lim sup for given sequence of sets,How to calculate lim inf and lim sup for given sequence of sets,,"Let the indicator function be defined as $$I(x) \triangleq \begin{cases} 1, & \quad x \geq 0 \\ 0, & \quad x < 0 \end{cases}$$ and $I_{\nu}(x) \in [0,1]$ be a continuous approximation of the $I(x)$ such that $$\lim_{\nu \rightarrow \infty} I_{\nu}(x) = I(x), \quad \forall x$$ $$\lim_{x \rightarrow - \infty} I_{\nu}(x) = 0, \quad \forall \nu$$ $$\lim_{x \rightarrow \infty} I_{\nu}(x) = 1, \quad \forall \nu.$$ Let us define the sequence of connected sets $\{ C_{\nu} \}$, where each $C_{\nu}$ is the set of values assumed by $I_{\nu}(x)$, $\forall x \in \mathbb{R}$ (hence, each element of $C_{\nu}$ belongs to $[0,1]$). How do I calculate $\liminf_{\nu \rightarrow +\infty} C_{\nu}$ and $\limsup_{\nu \rightarrow +\infty} C_{\nu}$? My intuition is that $\liminf_{\nu \rightarrow +\infty} C_{\nu} = \limsup_{\nu \rightarrow +\infty} C_{\nu}=\{0,1\}$, but I can't prove it.","Let the indicator function be defined as $$I(x) \triangleq \begin{cases} 1, & \quad x \geq 0 \\ 0, & \quad x < 0 \end{cases}$$ and $I_{\nu}(x) \in [0,1]$ be a continuous approximation of the $I(x)$ such that $$\lim_{\nu \rightarrow \infty} I_{\nu}(x) = I(x), \quad \forall x$$ $$\lim_{x \rightarrow - \infty} I_{\nu}(x) = 0, \quad \forall \nu$$ $$\lim_{x \rightarrow \infty} I_{\nu}(x) = 1, \quad \forall \nu.$$ Let us define the sequence of connected sets $\{ C_{\nu} \}$, where each $C_{\nu}$ is the set of values assumed by $I_{\nu}(x)$, $\forall x \in \mathbb{R}$ (hence, each element of $C_{\nu}$ belongs to $[0,1]$). How do I calculate $\liminf_{\nu \rightarrow +\infty} C_{\nu}$ and $\limsup_{\nu \rightarrow +\infty} C_{\nu}$? My intuition is that $\liminf_{\nu \rightarrow +\infty} C_{\nu} = \limsup_{\nu \rightarrow +\infty} C_{\nu}=\{0,1\}$, but I can't prove it.",,"['functional-analysis', 'limits', 'optimization']"
89,Nonconvex set converging to a convex set despite holes,Nonconvex set converging to a convex set despite holes,,"I'm looking at the example in Figure 4-7 of ""Variational Analysis"" (Rockafellar and Wets). Basically, there's a sequence of sets $C_{\nu}$ riddled with holes, and it states that the sequence eventually converges to the set $C$ (with the same shape but without holes) as long as the holes get finer and finer and thus vanish in the limit. If the holes vanish only in the limit, those $x$ in the center of the holes do not appear in $C_{\nu}$ infinitely many times, not even all but finitely many times. Indeed, they appear just once, i.e., for $\nu=+\infty$). Can anyone explain me why then $C_{\nu}$ is said to converge to $C$?","I'm looking at the example in Figure 4-7 of ""Variational Analysis"" (Rockafellar and Wets). Basically, there's a sequence of sets $C_{\nu}$ riddled with holes, and it states that the sequence eventually converges to the set $C$ (with the same shape but without holes) as long as the holes get finer and finer and thus vanish in the limit. If the holes vanish only in the limit, those $x$ in the center of the holes do not appear in $C_{\nu}$ infinitely many times, not even all but finitely many times. Indeed, they appear just once, i.e., for $\nu=+\infty$). Can anyone explain me why then $C_{\nu}$ is said to converge to $C$?",,"['functional-analysis', 'limits', 'optimization']"
90,"Is there a limit to $(0,0)$ approaching $\infty$ for the function $x^y$?",Is there a limit to  approaching  for the function ?,"(0,0) \infty x^y","Take the function $f(x,y) = x^y$ where $x \in [0,\infty)$ and $y \in (-\infty,\infty)$. The value of $f(0,0)$ is indeterminate. I want to know: Is there some path $S$ on the $(x,y)$-plane that ends at the origin, $(0,0)$, such that $$\lim_{S\to(0,0)} x^y \to\infty$$ And if not, how do you prove that there isn't one? It's trivial to show that there are limits that can equal $0$ or $1$: For $x = 0$, $\lim\limits_{y \to 0} \ 0^y = 0 $, and for $y = 0$, $\lim\limits_{x \to 0} \ x^0 = 1 $. However, I suspect there could be a limit approaching $\infty$ because, If you take some $\delta < 0$ that may be arbitrarily close to $0$, then $$ \lim_{x \to 0} x^{\delta} \to \infty$$ Therefore, in all neighbourhoods centred at $(0,0)$ there exist points at which $x^y$ is arbitrarily large (or approaching infinity if that can be claimed of a point). It seems, to me, that if there's a path for which $x \to 0$ faster than $y \to 0$ than the value of $x^y$ could keep rising as the path goes on and the limit approaching the origin will approach infinity.","Take the function $f(x,y) = x^y$ where $x \in [0,\infty)$ and $y \in (-\infty,\infty)$. The value of $f(0,0)$ is indeterminate. I want to know: Is there some path $S$ on the $(x,y)$-plane that ends at the origin, $(0,0)$, such that $$\lim_{S\to(0,0)} x^y \to\infty$$ And if not, how do you prove that there isn't one? It's trivial to show that there are limits that can equal $0$ or $1$: For $x = 0$, $\lim\limits_{y \to 0} \ 0^y = 0 $, and for $y = 0$, $\lim\limits_{x \to 0} \ x^0 = 1 $. However, I suspect there could be a limit approaching $\infty$ because, If you take some $\delta < 0$ that may be arbitrarily close to $0$, then $$ \lim_{x \to 0} x^{\delta} \to \infty$$ Therefore, in all neighbourhoods centred at $(0,0)$ there exist points at which $x^y$ is arbitrarily large (or approaching infinity if that can be claimed of a point). It seems, to me, that if there's a path for which $x \to 0$ faster than $y \to 0$ than the value of $x^y$ could keep rising as the path goes on and the limit approaching the origin will approach infinity.",,"['calculus', 'limits', 'functions']"
91,"Logarithms, prove this limit.","Logarithms, prove this limit.",,"Mathematica knows that: $$\log (n)=\lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$$ Kind of tautological starting with logarithms, but I would like to know better why this limit works: $${\Large \log (n)=\lim_{s\to 1+\frac{2 i \pi  k}{\log (n)}} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s-i \Im(s))}$$ for $k$ an integer. Solving it symbolically for some integer $k$ while leaving $n$ as a variable Mathematica says it is equal to zero. But setting $n$ to any value I get $\log (n)$. Mathematica: Table[Limit[Zeta[s - I*Im[s]]* Total[{1 - 1/(2)^(s - 1)}],    s -> (1 + (2*I \[Pi]*k/Log[2]))], {k, 1, 12}] N[%, 12] and: Table[Table[   Limit[Zeta[s - I*Im[s]]*Total[{1 - 1/(n)^(s - 1)}],     s -> (1 + (2*I \[Pi]*k/Log[n]))], {k, 1, 6}], {n, 1, 12}]  N[%, 12]","Mathematica knows that: $$\log (n)=\lim_{s\to 1} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s)$$ Kind of tautological starting with logarithms, but I would like to know better why this limit works: $${\Large \log (n)=\lim_{s\to 1+\frac{2 i \pi  k}{\log (n)}} \, \left(1-\frac{1}{n^{s-1}}\right) \zeta (s-i \Im(s))}$$ for $k$ an integer. Solving it symbolically for some integer $k$ while leaving $n$ as a variable Mathematica says it is equal to zero. But setting $n$ to any value I get $\log (n)$. Mathematica: Table[Limit[Zeta[s - I*Im[s]]* Total[{1 - 1/(2)^(s - 1)}],    s -> (1 + (2*I \[Pi]*k/Log[2]))], {k, 1, 12}] N[%, 12] and: Table[Table[   Limit[Zeta[s - I*Im[s]]*Total[{1 - 1/(n)^(s - 1)}],     s -> (1 + (2*I \[Pi]*k/Log[n]))], {k, 1, 6}], {n, 1, 12}]  N[%, 12]",,"['limits', 'logarithms', 'riemann-zeta']"
92,Limit of the sequence $\frac {a^n} {n!}$,Limit of the sequence,\frac {a^n} {n!},"I need to prove that $$\lim_{n \rightarrow \infty} \frac {a^n} {n!}=0$$ I have no condition over $a$, just that is a real number. I thought of using L'Hôpital, but it's way too complicated for something that it should be simpler. Same goes to the epsilon proof, and I´m runnning out of options of what to do with it. Thanks! PD: I could also use a hint on how to solve $$\lim_{n \rightarrow \infty} \frac {n^n} {n!}=0$$","I need to prove that $$\lim_{n \rightarrow \infty} \frac {a^n} {n!}=0$$ I have no condition over $a$, just that is a real number. I thought of using L'Hôpital, but it's way too complicated for something that it should be simpler. Same goes to the epsilon proof, and I´m runnning out of options of what to do with it. Thanks! PD: I could also use a hint on how to solve $$\lim_{n \rightarrow \infty} \frac {n^n} {n!}=0$$",,"['calculus', 'sequences-and-series', 'limits', 'factorial']"
93,Does $\lim \sup x_{n+1}-x_n=+\infty \implies \lim \dfrac{n}{x_n}=0$,Does,\lim \sup x_{n+1}-x_n=+\infty \implies \lim \dfrac{n}{x_n}=0,"If $(x_n)$ is a real sequence such that $\lim \sup x_{n+1}-x_n=+\infty$ , then must we have $\lim \dfrac{n}{x_n}=0$ ?","If $(x_n)$ is a real sequence such that $\lim \sup x_{n+1}-x_n=+\infty$ , then must we have $\lim \dfrac{n}{x_n}=0$ ?",,"['real-analysis', 'sequences-and-series']"
94,"If $\lvert f(x)\rvert\leq \lvert x\rvert^2$, then $f$ is differentiable at $0$","If , then  is differentiable at",\lvert f(x)\rvert\leq \lvert x\rvert^2 f 0,Let $f:\Bbb{R}^2\to\Bbb{R}$ be a function such that $\lvert f(x)\rvert\leq \lvert x\rvert^2$. Show that $f$ is differentiable at $0$. My solution: We want to show that $$\lim_{h\to 0}\dfrac{f(h)-f(0)}{ h}$$ exists. By assumption $f(0)=0$ and $$\dfrac{\lvert f(h)\rvert}{\lvert h\rvert}\leq\dfrac{\lvert h\rvert^2}{\lvert h\rvert}=\lvert h\rvert$$. So by the $\epsilon-\delta$ definition the result follows. Is this correct?,Let $f:\Bbb{R}^2\to\Bbb{R}$ be a function such that $\lvert f(x)\rvert\leq \lvert x\rvert^2$. Show that $f$ is differentiable at $0$. My solution: We want to show that $$\lim_{h\to 0}\dfrac{f(h)-f(0)}{ h}$$ exists. By assumption $f(0)=0$ and $$\dfrac{\lvert f(h)\rvert}{\lvert h\rvert}\leq\dfrac{\lvert h\rvert^2}{\lvert h\rvert}=\lvert h\rvert$$. So by the $\epsilon-\delta$ definition the result follows. Is this correct?,,"['calculus', 'real-analysis', 'limits']"
95,directional derivatives implications,directional derivatives implications,,"Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$. Let $y\in\mathbb{R}^n$. Then we define the directional derivative in the direction $y$ by : \begin{equation} f'(x;y) = \lim_{t \to0}\frac{f(x+ty)-f(x)}{t}  \end{equation} I guess the directional derivative of $f$ in the direction of $\lambda y, \lambda\in \mathbb{R}$ should be the same with the one in the direction of $y$. But if $f'(x;y) $ exists then one can see that, \begin{equation} \frac{f(x+t\lambda y)-f(x)}{ t}=\lambda\frac{f(x+t\lambda y)-f(x)}{\lambda t}  \end{equation} and the limit of the right hand side when $t\rightarrow 0$ is $\lambda f'(x;y)$. Do I do something wrong here? Please help by giving a rigorous answer. Thank you.","Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$. Let $y\in\mathbb{R}^n$. Then we define the directional derivative in the direction $y$ by : \begin{equation} f'(x;y) = \lim_{t \to0}\frac{f(x+ty)-f(x)}{t}  \end{equation} I guess the directional derivative of $f$ in the direction of $\lambda y, \lambda\in \mathbb{R}$ should be the same with the one in the direction of $y$. But if $f'(x;y) $ exists then one can see that, \begin{equation} \frac{f(x+t\lambda y)-f(x)}{ t}=\lambda\frac{f(x+t\lambda y)-f(x)}{\lambda t}  \end{equation} and the limit of the right hand side when $t\rightarrow 0$ is $\lambda f'(x;y)$. Do I do something wrong here? Please help by giving a rigorous answer. Thank you.",,"['calculus', 'limits', 'multivariable-calculus']"
96,proving limit evaluates to infinity,proving limit evaluates to infinity,,"I have the following question.. \begin{align*} f(n) &= n^{\sqrt{n}}\\ g(n) &= 2^{an},\quad \text{where $a > 1$} \end{align*} Evaluate the limit of $f(n)/g(n)$ as $n \to \infty$ I can intuitively see this has to be $0$, since $2^{an}$ grows much faster than $n^{\sqrt{n}}$, but how to show this rigorously?","I have the following question.. \begin{align*} f(n) &= n^{\sqrt{n}}\\ g(n) &= 2^{an},\quad \text{where $a > 1$} \end{align*} Evaluate the limit of $f(n)/g(n)$ as $n \to \infty$ I can intuitively see this has to be $0$, since $2^{an}$ grows much faster than $n^{\sqrt{n}}$, but how to show this rigorously?",,['limits']
97,Limit involving $(\cos x)^{1/x^4}$,Limit involving,(\cos x)^{1/x^4},"I am having trouble calculating the following limit. $$\lim_{x \to 0}(\cos x)^{1/x^4}$$ In Problems in mathematical analysis by Demidovich there is a hint that in case of $1^{\infty}$ indeterminate symbol in certain limit, one can add a term $a(x)$ which for given limit approaches zero and then after manipulation with exponents, it's easy to obtain a result of $e^{p}$, $p \in \mathbb{R}$. My computations in this way lead to the result of $e^{0} = 1$, but WolframAlpha says this limit equals $0$. If anyone could give me at least hints or solution for this problem, I would be very grateful.","I am having trouble calculating the following limit. $$\lim_{x \to 0}(\cos x)^{1/x^4}$$ In Problems in mathematical analysis by Demidovich there is a hint that in case of $1^{\infty}$ indeterminate symbol in certain limit, one can add a term $a(x)$ which for given limit approaches zero and then after manipulation with exponents, it's easy to obtain a result of $e^{p}$, $p \in \mathbb{R}$. My computations in this way lead to the result of $e^{0} = 1$, but WolframAlpha says this limit equals $0$. If anyone could give me at least hints or solution for this problem, I would be very grateful.",,"['calculus', 'analysis', 'limits', 'trigonometry']"
98,Limit of $(a_{n+1}-a_{n}) e^{-a_{n}}$,Limit of,(a_{n+1}-a_{n}) e^{-a_{n}},Consider a sequence $\{a_{n}\}$ satisfying $$a_{n+2}-2a_{n+1}+a_{n} \geq C > 0$$ Do we have : $\lim_{n\to \infty}(a_{n+1}-a_{n}) e^{-a_{n}}=0$ ? PS : This question is inspired from this continuous version which is still unanswered.,Consider a sequence $\{a_{n}\}$ satisfying $$a_{n+2}-2a_{n+1}+a_{n} \geq C > 0$$ Do we have : $\lim_{n\to \infty}(a_{n+1}-a_{n}) e^{-a_{n}}=0$ ? PS : This question is inspired from this continuous version which is still unanswered.,,"['sequences-and-series', 'limits']"
99,Cesaro means and equivalent sequences,Cesaro means and equivalent sequences,,"Let $(u_n)$ be a sequence of complex numbers that converges in mean (Cesaro convergence). Let $(v_n)$ be a sequence such that $v_n\sim u_n$. Does the sequence $(v_n)$ converge in mean? Here is what I did. One has $v_k=u_k(1+\varepsilon_k)$ where $\lim_{k\to+\infty}\varepsilon_k=0$. So, $\begin{array}{rcl} \frac{1}{n}\sum_{k=0}^{n-1}v_k&=&\frac{1}{n}\sum_{k=0}^{n-1}u_k(1+\varepsilon_k)\\ &=&\frac{1}{n}\sum_{k=0}^{n-1}u_k+\frac{1}{n}\sum_{k=0}^{n-1}u_k\varepsilon_k. \end{array}$ I know that $\frac{1}{n}\sum_{k=0}^{n-1}u_k$ converges, say to $L\in\mathbb C$. So, now I would like to prove that $\lim_{n\to+\infty}\frac1n\sum_{k=0}^{n-1}u_k\varepsilon_k=0$. But I did not manage to do it.","Let $(u_n)$ be a sequence of complex numbers that converges in mean (Cesaro convergence). Let $(v_n)$ be a sequence such that $v_n\sim u_n$. Does the sequence $(v_n)$ converge in mean? Here is what I did. One has $v_k=u_k(1+\varepsilon_k)$ where $\lim_{k\to+\infty}\varepsilon_k=0$. So, $\begin{array}{rcl} \frac{1}{n}\sum_{k=0}^{n-1}v_k&=&\frac{1}{n}\sum_{k=0}^{n-1}u_k(1+\varepsilon_k)\\ &=&\frac{1}{n}\sum_{k=0}^{n-1}u_k+\frac{1}{n}\sum_{k=0}^{n-1}u_k\varepsilon_k. \end{array}$ I know that $\frac{1}{n}\sum_{k=0}^{n-1}u_k$ converges, say to $L\in\mathbb C$. So, now I would like to prove that $\lim_{n\to+\infty}\frac1n\sum_{k=0}^{n-1}u_k\varepsilon_k=0$. But I did not manage to do it.",,"['calculus', 'analysis', 'limits', 'asymptotics']"
