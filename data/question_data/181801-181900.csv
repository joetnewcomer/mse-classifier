,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Computing the limit of a 2-variable function,Computing the limit of a 2-variable function,,"Show that $\lim \limits _{(x,y)\rightarrow (0,0)} \frac{x^3y}{x^2+y^4}=0$ just using $\epsilon-\delta$ creterion. In fact, choose $\epsilon >0$ arbitary, then we have to find $\delta >0$ such that if $\sqrt{x^2+y^2}<\delta$ then $\left\vert \frac{x^3y}{x^2+y^4}\right\vert <\epsilon$.","Show that $\lim \limits _{(x,y)\rightarrow (0,0)} \frac{x^3y}{x^2+y^4}=0$ just using $\epsilon-\delta$ creterion. In fact, choose $\epsilon >0$ arbitary, then we have to find $\delta >0$ such that if $\sqrt{x^2+y^2}<\delta$ then $\left\vert \frac{x^3y}{x^2+y^4}\right\vert <\epsilon$.",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
1,Vector Cross product - Rearranging issue,Vector Cross product - Rearranging issue,,"Given Data in question I have following relations in vector space$\begin{eqnarray}n_0^{'}(s)=-\kappa(s) \times n_0(s)\\n_1^{'}(s)=-\kappa(s) \times n_1(s)\\n_2^{'}(s)=-\kappa(s) \times n_2(s)\end{eqnarray}\tag 1$ $n_0,n_1,n_2$ are orthogonal unit vectors $\kappa(s)$ is not a unit vector Derivative of a unit vector is a vector that is normal to the given vector. We can normalize it to get normalized normal unit vector. Click here for Reference if needed for this data Question Can we rewrite the equation (1) in the following form$\begin{eqnarray}n_0(s)=-\kappa(s) \times \psi_0(s)\\n_1(s)=-\kappa(s) \times \psi_1(s)\\n_2(s)=-\kappa(s) \times \psi_2(s)\end{eqnarray}\tag 2$? If so what could be the vectors $\psi_0(s),\psi_1(s),\psi_2(s)$ ? or can we rewrite equation (1) with a LHS in equation (2) irrespective of what is on R.H.S(just avoiding R.H.S condition for more relaxation in complexity of the problem )?","Given Data in question I have following relations in vector space$\begin{eqnarray}n_0^{'}(s)=-\kappa(s) \times n_0(s)\\n_1^{'}(s)=-\kappa(s) \times n_1(s)\\n_2^{'}(s)=-\kappa(s) \times n_2(s)\end{eqnarray}\tag 1$ $n_0,n_1,n_2$ are orthogonal unit vectors $\kappa(s)$ is not a unit vector Derivative of a unit vector is a vector that is normal to the given vector. We can normalize it to get normalized normal unit vector. Click here for Reference if needed for this data Question Can we rewrite the equation (1) in the following form$\begin{eqnarray}n_0(s)=-\kappa(s) \times \psi_0(s)\\n_1(s)=-\kappa(s) \times \psi_1(s)\\n_2(s)=-\kappa(s) \times \psi_2(s)\end{eqnarray}\tag 2$? If so what could be the vectors $\psi_0(s),\psi_1(s),\psi_2(s)$ ? or can we rewrite equation (1) with a LHS in equation (2) irrespective of what is on R.H.S(just avoiding R.H.S condition for more relaxation in complexity of the problem )?",,"['calculus', 'multivariable-calculus', 'derivatives', 'vector-spaces', 'vector-analysis']"
2,Prove the following function is differentiable,Prove the following function is differentiable,,"I have to prove if this function is differentiable. $$f(x,y)= \begin{cases} (x^2+y^2) \sin\frac 1{(x^2+y^2)} \iff (x,y) \neq (0,0) \\0 \iff (x,y)=(0,0) \end{cases}$$ I tried proving that all of its partial derivatives are continuous in (0,0). However, $$f_x(x,y)= \begin{cases} \frac {-2x}{(x^2+y^2)} \cos\frac 1{(x^2+y^2)}+2x\sin\frac 1{(x^2+y^2)} \iff (x,y) \neq (0,0) \\0 \iff (x,y)=(0,0) \end{cases}$$ Which is, I think,  actually not continuous in (0,0), so this approach didn't work. What would you suggest for proving $f$ is differentiable? I've tried using the definition of differentiability. Is it enough to say that $\nabla(0,0)=(0,0)$ since $$ \lim_{\mathbf{h\to 0}} \frac{\mathbf{f(0+h)-f(0)-(0,0)h}}{||\mathbf{h}||} =\lim_{\mathbf{h\to 0}} \frac{\mathbf{f(h)}}{||\mathbf{h}||} =\mathbf{0} $$ What do you think?","I have to prove if this function is differentiable. $$f(x,y)= \begin{cases} (x^2+y^2) \sin\frac 1{(x^2+y^2)} \iff (x,y) \neq (0,0) \\0 \iff (x,y)=(0,0) \end{cases}$$ I tried proving that all of its partial derivatives are continuous in (0,0). However, $$f_x(x,y)= \begin{cases} \frac {-2x}{(x^2+y^2)} \cos\frac 1{(x^2+y^2)}+2x\sin\frac 1{(x^2+y^2)} \iff (x,y) \neq (0,0) \\0 \iff (x,y)=(0,0) \end{cases}$$ Which is, I think,  actually not continuous in (0,0), so this approach didn't work. What would you suggest for proving $f$ is differentiable? I've tried using the definition of differentiability. Is it enough to say that $\nabla(0,0)=(0,0)$ since $$ \lim_{\mathbf{h\to 0}} \frac{\mathbf{f(0+h)-f(0)-(0,0)h}}{||\mathbf{h}||} =\lim_{\mathbf{h\to 0}} \frac{\mathbf{f(h)}}{||\mathbf{h}||} =\mathbf{0} $$ What do you think?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'functions', 'derivatives']"
3,Prove that g is continuous.,Prove that g is continuous.,,"Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ be a continuous function and define $g : \mathbb{R}^{n} \rightarrow \mathbb{R}$  by $g(x) = |f(x)|, x \in \mathbb{R}^{n}$ Prove that g is continuous.","Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ be a continuous function and define $g : \mathbb{R}^{n} \rightarrow \mathbb{R}$  by $g(x) = |f(x)|, x \in \mathbb{R}^{n}$ Prove that g is continuous.",,"['calculus', 'analysis', 'multivariable-calculus']"
4,Möbius band parameterizaton: Showing injective,Möbius band parameterizaton: Showing injective,,"So, I'm trying to show that the parameteization function from $\mathbb R^2$ to $\mathbb R^3$ given in the wikipedia page http://en.wikipedia.org/wiki/Mobius_band#Geometry_and_topology is injective on the interior of the domain.   (this is part of a larger exercise I'm doing in a general topology class showing it is homeomorphic to an identification space of the unit square).  Trying to do it by brute force gets me 3 equations with 4 unknowns.  Is there a multivariable analysis way to do this?  I know the implicit function theorem/inverse function theorem can give locally 1-1,  but I need 1-1 on the entire open unit square.","So, I'm trying to show that the parameteization function from $\mathbb R^2$ to $\mathbb R^3$ given in the wikipedia page http://en.wikipedia.org/wiki/Mobius_band#Geometry_and_topology is injective on the interior of the domain.   (this is part of a larger exercise I'm doing in a general topology class showing it is homeomorphic to an identification space of the unit square).  Trying to do it by brute force gets me 3 equations with 4 unknowns.  Is there a multivariable analysis way to do this?  I know the implicit function theorem/inverse function theorem can give locally 1-1,  but I need 1-1 on the entire open unit square.",,"['general-topology', 'multivariable-calculus']"
5,"Analysis of $f(x,y,z)=\frac{\sin(xyz)} {x^2+y^2+z^2}$",Analysis of,"f(x,y,z)=\frac{\sin(xyz)} {x^2+y^2+z^2}","$f(x,y,z)=\frac {\sin(xyz)} {x^2+y^2+z^2}$ or $0$ if $(x,y,z)=(0,0,0)$ The problem says the following: a) Where is this function continuous, and b) Where is it differentiable? At a quick glance, the only point which might give problems is the origin point. For (a), I did the following: To prove that the limit of the first part of the function is 0 I used an $\epsilon-\delta$ proof of said limit. If $0<|(x^2+y^2+z^2)|<\delta$ then $|\frac {\sin(xyz)} {x^2+y^2+z^2}|<\epsilon$ The inequalities I used are: $|\frac {\sin(xyz)} {x^2+y^2+z^2}|<|\frac {xyz} {x^2+y^2+z^2}|<|\frac {(x^2+y^2+z^2)^{3/2}} {x^2+y^2+z^2}|<|\sqrt{x^2+y^2+z^2}|<\epsilon$, so that if $\epsilon=\delta$, the limit is proven to exist and equal 0, so f is continuous in $R^3$ And for (b) I used spherical coordinates: $x=r\cos\theta \sin\varphi , y=r\cos\theta \sin\varphi , z=r\cos\varphi$ so the function is now $f(x,y,z)= \frac {\sin (r^3 \cos\theta \sin\theta \cos\varphi \sin^2\varphi)} {r^2}$ The differentiability condition is that, if f is to be differentiable   in a given point, f must tend to the limit faster than the distance   between the test point and the point where the function is being   evaluated. In this case, $\lim_{(x,y,z)\rightarrow (0,0,0)}\frac  {f(x,y,z)} {r} = 0$ must be true for f to be differentiable   everywhere. Substituting the function, this translates into $\lim_{(x,y,z)\rightarrow (0,0,0)}\frac {\sin (r^3 \cos\theta \sin\theta \cos\varphi \sin^2\varphi)} {r^3}$, but since r goes to 0, we can ""take   out"" the sine, and then the distances cancel out. What is left is   $\cos\theta \sin\theta \cos\varphi \sin^2\varphi$, which does not equal 0   unless you chose a path contained within one of the planes generated   by two cartesian axes (and so the limit does not exist at all), so f is differentiable everywhere but the origin. The question is, simply: What or where did I go wrong? Many, many thanks in advance.","$f(x,y,z)=\frac {\sin(xyz)} {x^2+y^2+z^2}$ or $0$ if $(x,y,z)=(0,0,0)$ The problem says the following: a) Where is this function continuous, and b) Where is it differentiable? At a quick glance, the only point which might give problems is the origin point. For (a), I did the following: To prove that the limit of the first part of the function is 0 I used an $\epsilon-\delta$ proof of said limit. If $0<|(x^2+y^2+z^2)|<\delta$ then $|\frac {\sin(xyz)} {x^2+y^2+z^2}|<\epsilon$ The inequalities I used are: $|\frac {\sin(xyz)} {x^2+y^2+z^2}|<|\frac {xyz} {x^2+y^2+z^2}|<|\frac {(x^2+y^2+z^2)^{3/2}} {x^2+y^2+z^2}|<|\sqrt{x^2+y^2+z^2}|<\epsilon$, so that if $\epsilon=\delta$, the limit is proven to exist and equal 0, so f is continuous in $R^3$ And for (b) I used spherical coordinates: $x=r\cos\theta \sin\varphi , y=r\cos\theta \sin\varphi , z=r\cos\varphi$ so the function is now $f(x,y,z)= \frac {\sin (r^3 \cos\theta \sin\theta \cos\varphi \sin^2\varphi)} {r^2}$ The differentiability condition is that, if f is to be differentiable   in a given point, f must tend to the limit faster than the distance   between the test point and the point where the function is being   evaluated. In this case, $\lim_{(x,y,z)\rightarrow (0,0,0)}\frac  {f(x,y,z)} {r} = 0$ must be true for f to be differentiable   everywhere. Substituting the function, this translates into $\lim_{(x,y,z)\rightarrow (0,0,0)}\frac {\sin (r^3 \cos\theta \sin\theta \cos\varphi \sin^2\varphi)} {r^3}$, but since r goes to 0, we can ""take   out"" the sine, and then the distances cancel out. What is left is   $\cos\theta \sin\theta \cos\varphi \sin^2\varphi$, which does not equal 0   unless you chose a path contained within one of the planes generated   by two cartesian axes (and so the limit does not exist at all), so f is differentiable everywhere but the origin. The question is, simply: What or where did I go wrong? Many, many thanks in advance.",,"['real-analysis', 'multivariable-calculus']"
6,Shape operator of the sphere.,Shape operator of the sphere.,,"I want to compute the Weingarten operator (shape) for the sphere $\{(x,y,z) \in \mathbb{R}^3  \ : \ x^2 + y^2 + z^2 = 1\}$. I am given the adapted frame: $$\left\{\begin{array}{l} E_1 = \cos \varphi \cos \theta \newcommand{\ddx}{\frac{\partial}{\partial x}} \ddx + \cos \varphi \sin \theta \newcommand{\ddy}{\frac{\partial}{\partial y}} \ddy + \sin \varphi \newcommand{\ddz}{\frac{\partial}{\partial z}} \ddz \\ E_2 = - \sin \theta \ddx + \cos \theta \ddy \\ E_3 = -\sin \varphi \cos \theta \ddx - \sin \varphi \sin \theta \ddy + \cos \varphi \ddz \end{array}\right.$$ Among other stuff, I have already computed the connection $1$-forms: $$\omega_{12} = \cos \varphi \newcommand{\d}{ \ \mathrm{d}} \d \theta, \qquad \omega_{13} = \d \varphi \quad \mbox{and}\quad \omega_{23} = \sin \varphi \d \theta$$ and the dual $1$-forms: $$\theta_1 = \d r, \quad \theta_2 = r \cos \varphi \d \theta \quad \mbox{and}\quad \theta_3 = r \d \varphi$$ I already know that $\newcommand{\s}{\mathcal{S}} \s(v) := - \nabla_v E_1$, since $E_1$ is normal to the sphere. I also know that considering the patch $\mathbf{x}(\theta,\varphi) = (\cos \varphi \cos \theta,\cos \varphi \sin \theta,\sin \varphi)$ and working with $\mathbf{x}_\theta, \mathbf{x}_\varphi$ and $\mathbf{x}_\theta \times \mathbf{x}_\varphi$, we get that $\s(v) = -\frac{1}{r}v$. But I want to understand what is going wrong in this other approach. We have that $\s = - \nabla E_1 = -(\omega_{12}E_2 + \omega_{13}E_3) = \omega_{21}E_2 + \omega_{31}E_3$, fine. If I consider $$v = v^1 E_1 + v^2E_2 + v^3E_3$$ then we get: $$ \d \theta(v) = v^1 \d\theta(E_1) + v^2 \d \theta(E_2) + v^3 \d \theta(E_3) = \frac{1}{r \cos \varphi}v^2 \\ \mbox{and} \quad \d \varphi(v) = v^1 \d\varphi(E_1) + v^2 \d \varphi(E_2) + v^3 \d \varphi(E_3) = \frac{1}{r} v^3$$ That being said, we go for $\s(v)$: $$\begin{align} \s(v) &= \omega_{21}(v)E_2 + \omega_{31}(v)E_3 \\ &= -\cos \varphi \d \theta(v) E_2 - \d \varphi(v) E_3 \\ &= -\frac{1}{r}v^2 E_2 - \frac{1}{r}v^3E_3 \\ &=-\frac{1}{r}\left(v^2E_2 + v^3E_3\right)\end{align}$$ I'm not exactly sure what is going on. As I am done typing this, it came to my mind: maybe this IS correct, since $\s$'s domain is a tangent space to the sphere, and so $v^1 = 0$? Maybe my problem is with the notation. If someone can explain me a little more about this, I'm very thankful.","I want to compute the Weingarten operator (shape) for the sphere $\{(x,y,z) \in \mathbb{R}^3  \ : \ x^2 + y^2 + z^2 = 1\}$. I am given the adapted frame: $$\left\{\begin{array}{l} E_1 = \cos \varphi \cos \theta \newcommand{\ddx}{\frac{\partial}{\partial x}} \ddx + \cos \varphi \sin \theta \newcommand{\ddy}{\frac{\partial}{\partial y}} \ddy + \sin \varphi \newcommand{\ddz}{\frac{\partial}{\partial z}} \ddz \\ E_2 = - \sin \theta \ddx + \cos \theta \ddy \\ E_3 = -\sin \varphi \cos \theta \ddx - \sin \varphi \sin \theta \ddy + \cos \varphi \ddz \end{array}\right.$$ Among other stuff, I have already computed the connection $1$-forms: $$\omega_{12} = \cos \varphi \newcommand{\d}{ \ \mathrm{d}} \d \theta, \qquad \omega_{13} = \d \varphi \quad \mbox{and}\quad \omega_{23} = \sin \varphi \d \theta$$ and the dual $1$-forms: $$\theta_1 = \d r, \quad \theta_2 = r \cos \varphi \d \theta \quad \mbox{and}\quad \theta_3 = r \d \varphi$$ I already know that $\newcommand{\s}{\mathcal{S}} \s(v) := - \nabla_v E_1$, since $E_1$ is normal to the sphere. I also know that considering the patch $\mathbf{x}(\theta,\varphi) = (\cos \varphi \cos \theta,\cos \varphi \sin \theta,\sin \varphi)$ and working with $\mathbf{x}_\theta, \mathbf{x}_\varphi$ and $\mathbf{x}_\theta \times \mathbf{x}_\varphi$, we get that $\s(v) = -\frac{1}{r}v$. But I want to understand what is going wrong in this other approach. We have that $\s = - \nabla E_1 = -(\omega_{12}E_2 + \omega_{13}E_3) = \omega_{21}E_2 + \omega_{31}E_3$, fine. If I consider $$v = v^1 E_1 + v^2E_2 + v^3E_3$$ then we get: $$ \d \theta(v) = v^1 \d\theta(E_1) + v^2 \d \theta(E_2) + v^3 \d \theta(E_3) = \frac{1}{r \cos \varphi}v^2 \\ \mbox{and} \quad \d \varphi(v) = v^1 \d\varphi(E_1) + v^2 \d \varphi(E_2) + v^3 \d \varphi(E_3) = \frac{1}{r} v^3$$ That being said, we go for $\s(v)$: $$\begin{align} \s(v) &= \omega_{21}(v)E_2 + \omega_{31}(v)E_3 \\ &= -\cos \varphi \d \theta(v) E_2 - \d \varphi(v) E_3 \\ &= -\frac{1}{r}v^2 E_2 - \frac{1}{r}v^3E_3 \\ &=-\frac{1}{r}\left(v^2E_2 + v^3E_3\right)\end{align}$$ I'm not exactly sure what is going on. As I am done typing this, it came to my mind: maybe this IS correct, since $\s$'s domain is a tangent space to the sphere, and so $v^1 = 0$? Maybe my problem is with the notation. If someone can explain me a little more about this, I'm very thankful.",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
7,"Finding a $C^1$ function $F(x,y)$ that is not of class $C^2$",Finding a  function  that is not of class,"C^1 F(x,y) C^2","I am trying to find a function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ of class $C^1$, but not of class $C^2$. Meaning that $\frac{\partial^2 F}{\partial x \partial y}$ won't be equal to  $\frac{\partial^2 F}{\partial y \partial x}$. I've thought of a lot of possible functions, but in each one of them there is only a single singular point in which they are not of class $C^2$. Can someone help me find such a function? Thanks in advance.","I am trying to find a function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ of class $C^1$, but not of class $C^2$. Meaning that $\frac{\partial^2 F}{\partial x \partial y}$ won't be equal to  $\frac{\partial^2 F}{\partial y \partial x}$. I've thought of a lot of possible functions, but in each one of them there is only a single singular point in which they are not of class $C^2$. Can someone help me find such a function? Thanks in advance.",,"['multivariable-calculus', 'examples-counterexamples', 'partial-derivative']"
8,Is this an ordinary differential equation?,Is this an ordinary differential equation?,,"If a differential equation contains only ordinary derivatives of one or more functions with respect to a single independent variable it is said to be an ordinary differential equation (ODE). If different functions are differentiated with respect to different independent variables, but each function is only differentiated with respect to only one independent variable, is the equation ordinary? That was a mouthful. For example, for this equation: $$\frac {dy}{dx} + \frac{dz}{dw} - 12y = 0$$ There are multiple independent variables  ($x$ and $w$) but each dependent variable is only differentiated with respect to one variable (as opposed to say $y$ being differentiated with respect to $x$ and $w$). So would this be considered ordinary? I know equations like this: $$\frac {dy}{dx} + \frac{dz}{dw} - \frac{dy}{dw} +12y = 0$$ aren't ordinary because the one independent variable ($y$ in this case) is being differentiated with respect to multiple dependent variables ($x$ and $w$).","If a differential equation contains only ordinary derivatives of one or more functions with respect to a single independent variable it is said to be an ordinary differential equation (ODE). If different functions are differentiated with respect to different independent variables, but each function is only differentiated with respect to only one independent variable, is the equation ordinary? That was a mouthful. For example, for this equation: $$\frac {dy}{dx} + \frac{dz}{dw} - 12y = 0$$ There are multiple independent variables  ($x$ and $w$) but each dependent variable is only differentiated with respect to one variable (as opposed to say $y$ being differentiated with respect to $x$ and $w$). So would this be considered ordinary? I know equations like this: $$\frac {dy}{dx} + \frac{dz}{dw} - \frac{dy}{dw} +12y = 0$$ aren't ordinary because the one independent variable ($y$ in this case) is being differentiated with respect to multiple dependent variables ($x$ and $w$).",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'terminology']"
9,triple integral and limits,triple integral and limits,,"$$\iiint\limits_H (x^2+y^2) \, dx \, dy \, dz \\ H=\{(x,y,z) \in R^3: 1 \le x^2+y^2+z^2 \le9, z \le 0 \}$$ I'm using Spherical coordinate system: $$x=r\cos \theta \cos\phi $$ $$y=r\cos \theta \sin\phi $$ $$z=r\sin \theta  $$ and $r \in (1,3), \theta \in (0,2 \pi)$ but I don't know how to find set for $\phi$? Could anyone explain me in an easy way how to find $\phi$ [in this case and in general]?","$$\iiint\limits_H (x^2+y^2) \, dx \, dy \, dz \\ H=\{(x,y,z) \in R^3: 1 \le x^2+y^2+z^2 \le9, z \le 0 \}$$ I'm using Spherical coordinate system: $$x=r\cos \theta \cos\phi $$ $$y=r\cos \theta \sin\phi $$ $$z=r\sin \theta  $$ and $r \in (1,3), \theta \in (0,2 \pi)$ but I don't know how to find set for $\phi$? Could anyone explain me in an easy way how to find $\phi$ [in this case and in general]?",,"['integration', 'analysis', 'multivariable-calculus', 'spherical-coordinates']"
10,Distance and absolute value differences?,Distance and absolute value differences?,,My textbook: '.. the length of a vector is in many ways analogous to the absolute value of a real number.' My question: How are the length of a vector and the absolute value of a real number  'analogous in many ways' and not simply equivalent?  In what ways are they different?,My textbook: '.. the length of a vector is in many ways analogous to the absolute value of a real number.' My question: How are the length of a vector and the absolute value of a real number  'analogous in many ways' and not simply equivalent?  In what ways are they different?,,['multivariable-calculus']
11,Suggested textbook for Multivariable Calculus [duplicate],Suggested textbook for Multivariable Calculus [duplicate],,"This question already has answers here : Which multivariable calculus books are heavily oriented at physics? (5 answers) Closed 9 years ago . I have just finished single variable differential/integral calculus (with a little bit of infinite series). I'd like to jump right into the multivariable stuff; given that I intend to major in engineering (definitely not pure mathematics). Which textbook would you recommend that covers integral/differential calculus of several variables? I am more interested in the computational stuff than the ""proofs"" if that makes sense. Some I've heard of are Stewart / Spivak - Calculus on manifolds/ Anton/ Howard/ Edwards Thank you very much!","This question already has answers here : Which multivariable calculus books are heavily oriented at physics? (5 answers) Closed 9 years ago . I have just finished single variable differential/integral calculus (with a little bit of infinite series). I'd like to jump right into the multivariable stuff; given that I intend to major in engineering (definitely not pure mathematics). Which textbook would you recommend that covers integral/differential calculus of several variables? I am more interested in the computational stuff than the ""proofs"" if that makes sense. Some I've heard of are Stewart / Spivak - Calculus on manifolds/ Anton/ Howard/ Edwards Thank you very much!",,"['multivariable-calculus', 'reference-request', 'book-recommendation']"
12,tangent line to the graph,tangent line to the graph,,"this is the problem, I reached pi/6 & -pi/6 each time yet the website is saying my answer is incorrect. steps i took. 1) derivative of 4t-3tant/4t+3tant  2)yeilds 4+3sec^2(x) on bottom  3) set to 0 4) get +/-= sec = sort(-4/3)  5) cos = sqrt(3) /2 which => pi/6 or -pi/6 ??? any ideas where I'm messing up? thank you!","this is the problem, I reached pi/6 & -pi/6 each time yet the website is saying my answer is incorrect. steps i took. 1) derivative of 4t-3tant/4t+3tant  2)yeilds 4+3sec^2(x) on bottom  3) set to 0 4) get +/-= sec = sort(-4/3)  5) cos = sqrt(3) /2 which => pi/6 or -pi/6 ??? any ideas where I'm messing up? thank you!",,"['calculus', 'multivariable-calculus']"
13,Existence of an exponential double integral (for the probabilists: Are the $L^p$-norms of Brownian local time integrable in the space variable?),Existence of an exponential double integral (for the probabilists: Are the -norms of Brownian local time integrable in the space variable?),L^p,"I have encountered the following integral and, with a lot of handwaving and some identities for Gaussian integrals (see for example https://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions ) I think that it is finite. Is there a ""simple"" way to prove this? The integral is as follows: $$\int_{0}^{\infty} x^{1+1/p} \left(\int_{1}^{\infty} \mathrm{e}^{- x^2y^2/(2t)} (y-1)^p \mathrm{d}y \right)^{1/p} \mathrm{d}x,$$ where $t>0$ and $p > 1$. For the probabilists: Up to a constant (depending on $t$), the above integral is equal to $$ \int_{-\infty}^{\infty} \operatorname{E}[(L_t^x)^p]^{1/p} \mathrm{d}x,$$ where $L_t^x$ is the local time of Browian motion in $x$ and at time $t$. The separate $L^p$-norms, i.e. the integrals $\operatorname{E}[(L_t^x)^p]$ and $\int_{-\infty}^{\infty} (L_t^x)^p \mathrm{d}x$ are both finite, but when combining the two as above things seem to become a bit more difficult, especially because of this pesky exponent $1/p$. Is the finiteness some ""well-known"" result? Can it be deduced by a simple probabilistic argument? Thanks for your help!","I have encountered the following integral and, with a lot of handwaving and some identities for Gaussian integrals (see for example https://en.wikipedia.org/wiki/List_of_integrals_of_Gaussian_functions ) I think that it is finite. Is there a ""simple"" way to prove this? The integral is as follows: $$\int_{0}^{\infty} x^{1+1/p} \left(\int_{1}^{\infty} \mathrm{e}^{- x^2y^2/(2t)} (y-1)^p \mathrm{d}y \right)^{1/p} \mathrm{d}x,$$ where $t>0$ and $p > 1$. For the probabilists: Up to a constant (depending on $t$), the above integral is equal to $$ \int_{-\infty}^{\infty} \operatorname{E}[(L_t^x)^p]^{1/p} \mathrm{d}x,$$ where $L_t^x$ is the local time of Browian motion in $x$ and at time $t$. The separate $L^p$-norms, i.e. the integrals $\operatorname{E}[(L_t^x)^p]$ and $\int_{-\infty}^{\infty} (L_t^x)^p \mathrm{d}x$ are both finite, but when combining the two as above things seem to become a bit more difficult, especially because of this pesky exponent $1/p$. Is the finiteness some ""well-known"" result? Can it be deduced by a simple probabilistic argument? Thanks for your help!",,"['calculus', 'real-analysis', 'probability', 'multivariable-calculus']"
14,"Prove that $f$ is differentiable in $(0,0)$ if and only if $\lim_{t\to0+} g(t)$ exists",Prove that  is differentiable in  if and only if  exists,"f (0,0) \lim_{t\to0+} g(t)","Let $g:[0,\infty)\to\mathbb{R}$ be a mapping and $f(x,y)=xg(\sqrt{x^2+y^2})$ for all $(x,y)\in\mathbb{R^2}$. Prove that $f$ is differentiable in $(0,0)$ $\iff$ $\lim_{t\to0+} g(t)$ exists. My attempt: $\implies:$ We suppose that $f$ is differentiable in $(0,0)$, that is, there is a linear mapping $A=(a_1 \,\, a_2)$ such that $$f(x,y)-f(0,0)-A(x,y) = R(x,y)$$ where R is a remainder term satisfying $\lim_{(x,y)\to(0,0)}\frac{R(x,y)}{|(x,y)|} = 0$. Given the definition of $f$, we see that $f(0,0)=0$, so that $$f(x,y)-A(x,y) = R(x,y) \\ \iff xg\big(\sqrt{x^2+y^2}\big)-A(x,y) = R(x,y) \\ \iff g\big(\sqrt{x^2+y^2}\big) = \frac{R(x,y)}{x}+\frac{1}{x}A(x,y) = \frac{R(x,y)}{x}+\frac{1}{x}(a_1x + a_2y).$$ Now we use polar coordinates: $x=r\cos\theta$ and $y=r\sin\theta$, which gives $$g(r) = \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+\frac{1}{r\cos\theta}(a_1r\cos\theta+a_2r\sin\theta) \\= \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+a_1+a_2\frac{\sin\theta}{\cos\theta}$$ Now taking the limit as $r\to0$ the first term vanishes and we see that the limit is $$a_1+a_2\frac{\sin\theta}{\cos\theta}.$$ $\impliedby:$ Now we suppose that $\lim_{t\to0+} g(t)$ exists. I will calculate the two partial derivatives and then show that the matrix $$A = \big(\partial_1 f(0,0) \,\,\, \partial_2 f(0,0)\big)$$ is indeed the derivative of $f$ in $(0,0)$. We have that $$\partial_1 f(0,0) = \lim_{t\to0} \frac{f(t,0)}{t} = \lim_{t\to0}\frac{tg(t)}{t}=\lim_{t\to0}g(t) = M$$ for some finite $M$; and $$\partial_2 f(0,0)=\lim_{t\to0}\frac{f(0,t)}{t} = 0.$$ We shall now investigate $$\lim_{(x,y)\to(0,0)} \frac{f(x,y) -f(0,0) - A(x,y)}{\sqrt{x^2+y^2}}.$$ Using polar coordinates this becomes $$\lim_{r\to0} \frac{r\cos\theta \,g(r) - Mr\cos\theta}{r} = \lim_{r\to0} \cos\theta \,g(r) - M\cos\theta = 0,$$ as desired. This post turned out much longer than I had anticipated -- I'm sure I made it far more complicated than necessary :)","Let $g:[0,\infty)\to\mathbb{R}$ be a mapping and $f(x,y)=xg(\sqrt{x^2+y^2})$ for all $(x,y)\in\mathbb{R^2}$. Prove that $f$ is differentiable in $(0,0)$ $\iff$ $\lim_{t\to0+} g(t)$ exists. My attempt: $\implies:$ We suppose that $f$ is differentiable in $(0,0)$, that is, there is a linear mapping $A=(a_1 \,\, a_2)$ such that $$f(x,y)-f(0,0)-A(x,y) = R(x,y)$$ where R is a remainder term satisfying $\lim_{(x,y)\to(0,0)}\frac{R(x,y)}{|(x,y)|} = 0$. Given the definition of $f$, we see that $f(0,0)=0$, so that $$f(x,y)-A(x,y) = R(x,y) \\ \iff xg\big(\sqrt{x^2+y^2}\big)-A(x,y) = R(x,y) \\ \iff g\big(\sqrt{x^2+y^2}\big) = \frac{R(x,y)}{x}+\frac{1}{x}A(x,y) = \frac{R(x,y)}{x}+\frac{1}{x}(a_1x + a_2y).$$ Now we use polar coordinates: $x=r\cos\theta$ and $y=r\sin\theta$, which gives $$g(r) = \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+\frac{1}{r\cos\theta}(a_1r\cos\theta+a_2r\sin\theta) \\= \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+a_1+a_2\frac{\sin\theta}{\cos\theta}$$ Now taking the limit as $r\to0$ the first term vanishes and we see that the limit is $$a_1+a_2\frac{\sin\theta}{\cos\theta}.$$ $\impliedby:$ Now we suppose that $\lim_{t\to0+} g(t)$ exists. I will calculate the two partial derivatives and then show that the matrix $$A = \big(\partial_1 f(0,0) \,\,\, \partial_2 f(0,0)\big)$$ is indeed the derivative of $f$ in $(0,0)$. We have that $$\partial_1 f(0,0) = \lim_{t\to0} \frac{f(t,0)}{t} = \lim_{t\to0}\frac{tg(t)}{t}=\lim_{t\to0}g(t) = M$$ for some finite $M$; and $$\partial_2 f(0,0)=\lim_{t\to0}\frac{f(0,t)}{t} = 0.$$ We shall now investigate $$\lim_{(x,y)\to(0,0)} \frac{f(x,y) -f(0,0) - A(x,y)}{\sqrt{x^2+y^2}}.$$ Using polar coordinates this becomes $$\lim_{r\to0} \frac{r\cos\theta \,g(r) - Mr\cos\theta}{r} = \lim_{r\to0} \cos\theta \,g(r) - M\cos\theta = 0,$$ as desired. This post turned out much longer than I had anticipated -- I'm sure I made it far more complicated than necessary :)",,"['multivariable-calculus', 'derivatives']"
15,Constrained Optimization of a function of two variables.,Constrained Optimization of a function of two variables.,,"I was given the following tutorial problem, and I'm having a bit of trouble seeing how it works. I've been asked to find the four critical points of this system, with two of these being degenerate points, one being a maximum, and one being a minimum; $$f(x_1, x_2) = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8$$ subject to $g(x_1, x_2) = x_1^2 + x_2^2 - 16 = 0$. First, I constructed a Lagrangian; $$L = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8 + \lambda(x_1^2 + x_2^2 - 16)$$ Then, taking the gradient of $L$, we get two equations; $$x_1 (3x_1 + 2(\lambda + 3)) = 0$$ $$x_2 (3x_2 + 2(\lambda - 3)) = 0$$ For the first equation to be satisfied, we have either $x_1 = 0$ or $3x_1 + 2(\lambda + 3) = 0$. In the case that $x_1 = 0$, we have that $x_2 = \pm 4$, due to our initial constraint. If $x_2 = 4$, we get that $\lambda = -3$, and if $x_2 = -4$, we get that $\lambda = 9$. In a similar fashion, if $x_2 = 0$, $x_1 = \pm 4$. If $x_1 = 4$, $\lambda = -9$, and if $x_1 = -4$, $\lambda = 3$. Thus, we have four critical points; $$(0,4) , \lambda = -3$$ $$(0,-4) , \lambda = 9$$ $$(4,0) , \lambda = -9$$ $$(-4,0) , \lambda = 3$$ Now, I then computed my Hessian matrix; $$H =\left( \begin{array}{ccc} 6x_1 + 6 + 2\lambda & 0 \\ 0 & 6x_2 - 6 + 2\lambda \\ \end{array} \right)$$ Then, I just plug in all my critical points (with their respective $\lambda$ values), and look at both the determinant and the principal minor of the Hessian. Clearly, both $(0,4)$ and $(-4,0)$ are degenerate points, so we've satisfied the first criteria. Looking at the Hessians for $(0,-4)$ and $(4, 0)$, I get that the principal minors for each are both positive, but each of the determinants are negative. For the point $(4,0)$, the principal minor is positive, while the determinant is negative, which implies that the point is a maximum. Similarly, $(0,-4)$ also appears to be a maximum. Now, having verified my working on Wolframalpha, I've correctly identified $(4,0)$ as a local maximum, but it's also telling me that $(0,-4)$ is a local minimum. From all the working I've done, I can't really see how $(0,-4)$ could be a minimum?? Could someone have a look at my working, and see where I've made my mistake??","I was given the following tutorial problem, and I'm having a bit of trouble seeing how it works. I've been asked to find the four critical points of this system, with two of these being degenerate points, one being a maximum, and one being a minimum; $$f(x_1, x_2) = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8$$ subject to $g(x_1, x_2) = x_1^2 + x_2^2 - 16 = 0$. First, I constructed a Lagrangian; $$L = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8 + \lambda(x_1^2 + x_2^2 - 16)$$ Then, taking the gradient of $L$, we get two equations; $$x_1 (3x_1 + 2(\lambda + 3)) = 0$$ $$x_2 (3x_2 + 2(\lambda - 3)) = 0$$ For the first equation to be satisfied, we have either $x_1 = 0$ or $3x_1 + 2(\lambda + 3) = 0$. In the case that $x_1 = 0$, we have that $x_2 = \pm 4$, due to our initial constraint. If $x_2 = 4$, we get that $\lambda = -3$, and if $x_2 = -4$, we get that $\lambda = 9$. In a similar fashion, if $x_2 = 0$, $x_1 = \pm 4$. If $x_1 = 4$, $\lambda = -9$, and if $x_1 = -4$, $\lambda = 3$. Thus, we have four critical points; $$(0,4) , \lambda = -3$$ $$(0,-4) , \lambda = 9$$ $$(4,0) , \lambda = -9$$ $$(-4,0) , \lambda = 3$$ Now, I then computed my Hessian matrix; $$H =\left( \begin{array}{ccc} 6x_1 + 6 + 2\lambda & 0 \\ 0 & 6x_2 - 6 + 2\lambda \\ \end{array} \right)$$ Then, I just plug in all my critical points (with their respective $\lambda$ values), and look at both the determinant and the principal minor of the Hessian. Clearly, both $(0,4)$ and $(-4,0)$ are degenerate points, so we've satisfied the first criteria. Looking at the Hessians for $(0,-4)$ and $(4, 0)$, I get that the principal minors for each are both positive, but each of the determinants are negative. For the point $(4,0)$, the principal minor is positive, while the determinant is negative, which implies that the point is a maximum. Similarly, $(0,-4)$ also appears to be a maximum. Now, having verified my working on Wolframalpha, I've correctly identified $(4,0)$ as a local maximum, but it's also telling me that $(0,-4)$ is a local minimum. From all the working I've done, I can't really see how $(0,-4)$ could be a minimum?? Could someone have a look at my working, and see where I've made my mistake??",,"['multivariable-calculus', 'optimization']"
16,"Compute $\iint_S \mathbf{F}\cdot d\mathbf{S}$ where $S$ is the surface that bounds the sphere $x^2+y^2+z^2=16$ and $\mathbf{F}=\langle z,y,x \rangle$",Compute  where  is the surface that bounds the sphere  and,"\iint_S \mathbf{F}\cdot d\mathbf{S} S x^2+y^2+z^2=16 \mathbf{F}=\langle z,y,x \rangle","The problem is actually to verify the divergence theorem by computing both $\iiint_E \text{div } \mathbf{F\space} dV$, which was relatively easy to compute and gives $\frac{256\pi}{3}$. To find $\iint_S \mathbf{F}\cdot d\mathbf{S}$, I parametrized the surface with spherical coordinates: $\mathbf{r}=\langle 4\sin\phi\cos\theta,4\sin\phi\sin\theta,4\cos\phi\rangle$ with $0\leq\phi\leq\pi,0\leq\theta\leq2\pi$. Now, noting that $\iint_S\mathbf{F}\cdot d\mathbf{S}=\iint_S \mathbf{F}\cdot \mathbf{n}\space dS$ where $\mathbf{n}$ is the normal vector to the $S$, and since $S$ is a sphere, we have $\mathbf{n}=\langle\sin\phi\cos\theta,\sin\phi\sin\theta,\cos\phi\rangle$, and $$\mathbf{F\cdot n}=4\sin\phi\cos\phi\cos\theta+4\sin^2\phi\sin\theta\cos\theta+4\sin\phi\cos\phi\cos\theta$$ But the integral of this is 0. Where am I going wrong?","The problem is actually to verify the divergence theorem by computing both $\iiint_E \text{div } \mathbf{F\space} dV$, which was relatively easy to compute and gives $\frac{256\pi}{3}$. To find $\iint_S \mathbf{F}\cdot d\mathbf{S}$, I parametrized the surface with spherical coordinates: $\mathbf{r}=\langle 4\sin\phi\cos\theta,4\sin\phi\sin\theta,4\cos\phi\rangle$ with $0\leq\phi\leq\pi,0\leq\theta\leq2\pi$. Now, noting that $\iint_S\mathbf{F}\cdot d\mathbf{S}=\iint_S \mathbf{F}\cdot \mathbf{n}\space dS$ where $\mathbf{n}$ is the normal vector to the $S$, and since $S$ is a sphere, we have $\mathbf{n}=\langle\sin\phi\cos\theta,\sin\phi\sin\theta,\cos\phi\rangle$, and $$\mathbf{F\cdot n}=4\sin\phi\cos\phi\cos\theta+4\sin^2\phi\sin\theta\cos\theta+4\sin\phi\cos\phi\cos\theta$$ But the integral of this is 0. Where am I going wrong?",,['multivariable-calculus']
17,A problem on norm preserving and angle preserving and their relations. [duplicate],A problem on norm preserving and angle preserving and their relations. [duplicate],,This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 4 years ago . I want to solve the following problem and finding some difficulties:- I have done the part (a) easily. My problem is in part (b) and (c). In part (b) after calculation I have achieved that $\lambda_i \lambda_j=|\lambda_i| |\lambda_j|$. From this we can only say that they are of same sign. I can not understand how can their absolute values are equal. In part (c) if I assume that (b) is true then we can say that all tha eigen values are same absolute values.Am I correct? Can someone help me please?,This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 4 years ago . I want to solve the following problem and finding some difficulties:- I have done the part (a) easily. My problem is in part (b) and (c). In part (b) after calculation I have achieved that $\lambda_i \lambda_j=|\lambda_i| |\lambda_j|$. From this we can only say that they are of same sign. I can not understand how can their absolute values are equal. In part (c) if I assume that (b) is true then we can say that all tha eigen values are same absolute values.Am I correct? Can someone help me please?,,"['functional-analysis', 'multivariable-calculus', 'normed-spaces']"
18,verifying extrema found by Lagrange multipliers,verifying extrema found by Lagrange multipliers,,"This question was inspired by reading this problem: Prove the inequality $\frac 1a + \frac 1b +\frac 1c \ge \frac{a^3+b^3+c^3}{3} +\frac 74$ Suppose I have a function $f(x,y,z)$ with continuous partial derivatives which I want to maximize and minimize on the portion of the plane $x+y+z=k$ with $x,y,z>0$. If I use Lagrange multipliers and find the points $(a_i,b_i,c_i)$ for $1\le i\le4$ where extrema could occur [by solving the system $f_x=\lambda\cdot1, f_y=\lambda\cdot1, f_z=\lambda\cdot1$], and $f(a_1,b_1,c_1)=2$ and $f(a_2,b_2,c_2)=f(a_3,b_3,c_3)=f(a_4,b_4,c_4)=\frac{7}{4}$, what additional information, if any, do I need in order to conclude that 2 is the maximum value of $f$ or that $\frac{7}{4}$ is the minimum value of $f$ on the set of points being considered?","This question was inspired by reading this problem: Prove the inequality $\frac 1a + \frac 1b +\frac 1c \ge \frac{a^3+b^3+c^3}{3} +\frac 74$ Suppose I have a function $f(x,y,z)$ with continuous partial derivatives which I want to maximize and minimize on the portion of the plane $x+y+z=k$ with $x,y,z>0$. If I use Lagrange multipliers and find the points $(a_i,b_i,c_i)$ for $1\le i\le4$ where extrema could occur [by solving the system $f_x=\lambda\cdot1, f_y=\lambda\cdot1, f_z=\lambda\cdot1$], and $f(a_1,b_1,c_1)=2$ and $f(a_2,b_2,c_2)=f(a_3,b_3,c_3)=f(a_4,b_4,c_4)=\frac{7}{4}$, what additional information, if any, do I need in order to conclude that 2 is the maximum value of $f$ or that $\frac{7}{4}$ is the minimum value of $f$ on the set of points being considered?",,"['calculus', 'multivariable-calculus', 'inequality']"
19,Lagrange multipliers/multivariable optimisation problem,Lagrange multipliers/multivariable optimisation problem,,"Problem: Maximise the volume $V$ of a cuboid shaped box with closed top, fixed surface area $S$, and side lengths $x, y,$ and $z$ What I've got so far: $V=xyz$, $S=2(xy+yz+zx)$, $\nabla V = \lambda \nabla P$ and so $$ \left\{  \begin{array}{c} \partial V / \partial x =\lambda\ \partial g / \partial x \\  \partial V / \partial y =\lambda\ \partial g / \partial y \\  \partial V / \partial z =\lambda\ \partial g / \partial z \\ S =2(xy+yz+zx) \end{array} \right.  $$ i.e.  $$ \left\{  \begin{array}{c} yz =2\lambda\ (y+z) \\  xz =2\lambda\ (x+z)\\  xy =2\lambda\ (x+y) \\ S =2(xy+yz+zx) \end{array} \right.  $$ How do I solve this set of equations? Elimination and matrix methods didn't work. And how could I extrapolate this problem to a box with no lid? A closed box of variable shape (i.e. prove the sphere has the lowest surface area to volume ratio)? A box in higher dimensions ?","Problem: Maximise the volume $V$ of a cuboid shaped box with closed top, fixed surface area $S$, and side lengths $x, y,$ and $z$ What I've got so far: $V=xyz$, $S=2(xy+yz+zx)$, $\nabla V = \lambda \nabla P$ and so $$ \left\{  \begin{array}{c} \partial V / \partial x =\lambda\ \partial g / \partial x \\  \partial V / \partial y =\lambda\ \partial g / \partial y \\  \partial V / \partial z =\lambda\ \partial g / \partial z \\ S =2(xy+yz+zx) \end{array} \right.  $$ i.e.  $$ \left\{  \begin{array}{c} yz =2\lambda\ (y+z) \\  xz =2\lambda\ (x+z)\\  xy =2\lambda\ (x+y) \\ S =2(xy+yz+zx) \end{array} \right.  $$ How do I solve this set of equations? Elimination and matrix methods didn't work. And how could I extrapolate this problem to a box with no lid? A closed box of variable shape (i.e. prove the sphere has the lowest surface area to volume ratio)? A box in higher dimensions ?",,['multivariable-calculus']
20,Definitions of differentiability,Definitions of differentiability,,"I have seen two definitions of differentiability of a real valued function and I wonder why they are equivalent. The first definition: For a function $\mathbf{F}:\mathbb R^n\to \mathbb R$ is differentiable at $\mathbf x$, if there exist a matrix $\mathbf{DF}(\mathbf x)$ such that $$\lim_{\Delta\mathbf x\to\mathbf0}\frac{\mathbf F(\mathbf x+\Delta\mathbf x)-\mathbf F(\mathbf x)-\mathbf{DF}(\mathbf x)\Delta\mathbf x}{||\Delta\mathbf x||}=0$$ It turns out that $\mathbf{DF}=\nabla \mathbf F$ So if $\mathbf F$ is differentiable, $$\lim_{\Delta\mathbf x\to\mathbf0}\frac{\Delta\mathbf F-\nabla \mathbf F\cdot\Delta\mathbf x}{\Delta \mathbf x}=0$$ The second definition: If $\mathbf F$ is differentiable,  $$\Delta\mathbf F=\nabla\mathbf F\cdot\Delta\mathbf x+\epsilon\cdot\Delta\mathbf x$$  where $\epsilon=(\epsilon_1,\epsilon_2,...,\epsilon_n)$ and $\epsilon\to\mathbf0$ as $\Delta \mathbf x\to \mathbf0$ I know how to prove 2 implies 1 by Cauchy Schwarz inequality ($\epsilon\cdot\Delta\mathbf x\le||\epsilon||||\Delta\mathbf x||$) but I don't know how to prove 1 implies 2","I have seen two definitions of differentiability of a real valued function and I wonder why they are equivalent. The first definition: For a function $\mathbf{F}:\mathbb R^n\to \mathbb R$ is differentiable at $\mathbf x$, if there exist a matrix $\mathbf{DF}(\mathbf x)$ such that $$\lim_{\Delta\mathbf x\to\mathbf0}\frac{\mathbf F(\mathbf x+\Delta\mathbf x)-\mathbf F(\mathbf x)-\mathbf{DF}(\mathbf x)\Delta\mathbf x}{||\Delta\mathbf x||}=0$$ It turns out that $\mathbf{DF}=\nabla \mathbf F$ So if $\mathbf F$ is differentiable, $$\lim_{\Delta\mathbf x\to\mathbf0}\frac{\Delta\mathbf F-\nabla \mathbf F\cdot\Delta\mathbf x}{\Delta \mathbf x}=0$$ The second definition: If $\mathbf F$ is differentiable,  $$\Delta\mathbf F=\nabla\mathbf F\cdot\Delta\mathbf x+\epsilon\cdot\Delta\mathbf x$$  where $\epsilon=(\epsilon_1,\epsilon_2,...,\epsilon_n)$ and $\epsilon\to\mathbf0$ as $\Delta \mathbf x\to \mathbf0$ I know how to prove 2 implies 1 by Cauchy Schwarz inequality ($\epsilon\cdot\Delta\mathbf x\le||\epsilon||||\Delta\mathbf x||$) but I don't know how to prove 1 implies 2",,['multivariable-calculus']
21,Generalizing the total differential to multidimensional codomains,Generalizing the total differential to multidimensional codomains,,"Consider a function $f: \mathbb{R}^n \to \mathbb{R}$ in the variables $x_1, \, x_2, \, \dots, \, x_n$. In multivariable calculus, we learn that the total differential of $f$ is defined as $$ df = \frac{\partial f}{\partial x_1} \, dx_1 + \frac{\partial f}{\partial x_2} \, dx_2 + \cdots + \frac{\partial f}{\partial x_n} \, dx_n. $$ I'm trying to generalize this to the case where the codomain $f$ is multidimensional. More specifically, consider $f: \mathbb{R}^n \to \mathbb{R}^m$ where $m$ is not necessarily one. Is there a way to define the total differential of $f$, in this case?","Consider a function $f: \mathbb{R}^n \to \mathbb{R}$ in the variables $x_1, \, x_2, \, \dots, \, x_n$. In multivariable calculus, we learn that the total differential of $f$ is defined as $$ df = \frac{\partial f}{\partial x_1} \, dx_1 + \frac{\partial f}{\partial x_2} \, dx_2 + \cdots + \frac{\partial f}{\partial x_n} \, dx_n. $$ I'm trying to generalize this to the case where the codomain $f$ is multidimensional. More specifically, consider $f: \mathbb{R}^n \to \mathbb{R}^m$ where $m$ is not necessarily one. Is there a way to define the total differential of $f$, in this case?",,['multivariable-calculus']
22,Center of Mass in $N$ dimensions,Center of Mass in  dimensions,N,"Does Center of Mass make sense in more than three dimensions? In the definition of the Center of Mass we have $dV$. Isn't the volume a three dimensional property? Or it is not and to define the center of mass for, say $4$ dimensions, we just have to calculate a four iterated integral instead of three?","Does Center of Mass make sense in more than three dimensions? In the definition of the Center of Mass we have $dV$. Isn't the volume a three dimensional property? Or it is not and to define the center of mass for, say $4$ dimensions, we just have to calculate a four iterated integral instead of three?",,"['multivariable-calculus', 'centroid']"
23,Taking Fourier transform of integral-differential equation,Taking Fourier transform of integral-differential equation,,"If $u$ is a solution of the equation $$\frac{\partial}{\partial t} u(x,t) + \int_{-\infty}^{\infty} \text{sinc}(x-y) \cdot \frac{\partial^{2}}{\partial y^{2}} u(y,t) \ dy = 0,$$ with initial condition $\ u(x,0) = f(x).$ Let $U^{t}(x) = u(t,x)$. How can I find an expression for the Fourier transform ${\widehat{U}}^{t}(k)$? I'm not sure how to take Fourier transform inside the integral.","If $u$ is a solution of the equation $$\frac{\partial}{\partial t} u(x,t) + \int_{-\infty}^{\infty} \text{sinc}(x-y) \cdot \frac{\partial^{2}}{\partial y^{2}} u(y,t) \ dy = 0,$$ with initial condition $\ u(x,0) = f(x).$ Let $U^{t}(x) = u(t,x)$. How can I find an expression for the Fourier transform ${\widehat{U}}^{t}(k)$? I'm not sure how to take Fourier transform inside the integral.",,"['multivariable-calculus', 'fourier-analysis', 'integral-equations']"
24,Finding a mistake in the computation of a double integral in polar coordinates,Finding a mistake in the computation of a double integral in polar coordinates,,"I have to find $P\left(4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2 \right) $ $f(x)$ and $f(y)$ are given, which I will use in my solution below . $$P\left(4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2 \right) = \int\int_D f(x)\cdot f(y)\hspace{1mm}dydx$$ Where D is the region inside $4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2$ Substitute $x = 45+\dfrac{r}{2}\cos\theta$ and $y = 20+\dfrac{r}{10}\sin\theta$ The Jacobian will be $r\cdot \dfrac{1}{2}\cdot \dfrac{1}{10} = \dfrac{r}{20}$ Then we can say $D \in \left( r, \theta\right)\hspace{1mm}|\hspace{2mm} 0\leq r\leq \sqrt{2},\hspace{2mm}0\leq \theta \leq 2\pi$ $$\int\int_D f(x)\cdot f(y)\hspace{1mm}dydx = \int_0^{2\pi} \int_{0}^{\sqrt{2}}\left[ \dfrac{2}{\sqrt{2\pi}}\hspace{1.4mm} e^{-0.5r^2\cos^2\theta}\right]\cdot \left[\dfrac{10}{\sqrt{2\pi}} e^{-0.5r^2\sin^2\theta} \right] \cdot \left[ \dfrac{r}{20}\right]drd\theta$$ $$= \int_0^{2\pi} \int_{0}^{\sqrt{2}}\dfrac{r}{2\pi}\hspace{1.4mm} e^{-0.5r^2\cos^2\theta-0.5r^2\sin^2\theta}\hspace{1mm}drd\theta$$ $$= \dfrac{1}{2\pi}\left[\int_0^{2\pi} d\theta \right]\left[\int_{0}^{\sqrt{2}}r\hspace{1.4mm} e^{-0.5r^2}\hspace{1mm}dr \right]$$ Substitute $-0.5r^2 = u$ and $-r dr = du$ $$\text{Limits of Integration will change from $\int_0^{\sqrt{2}}$ to $\int_{-0^2}^{-(\sqrt{2})^2} = \int_0^{-2}$}$$ $$= \dfrac{1}{2\pi}\left[{2\pi}  \right]\left[-\int_{0}^{-2}e^{u}\hspace{1mm}du \right]$$ $$=\left[-e^{u}\right]_{0}^{-2} = 1-e^{-2} = 1-\dfrac{1}{e^2}\approx 0.8647$$ Answer at the back of the book is $0.632$!","I have to find $P\left(4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2 \right) $ $f(x)$ and $f(y)$ are given, which I will use in my solution below . $$P\left(4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2 \right) = \int\int_D f(x)\cdot f(y)\hspace{1mm}dydx$$ Where D is the region inside $4\left(x-45\right)^2+100\left(y-20\right)^2\leq 2$ Substitute $x = 45+\dfrac{r}{2}\cos\theta$ and $y = 20+\dfrac{r}{10}\sin\theta$ The Jacobian will be $r\cdot \dfrac{1}{2}\cdot \dfrac{1}{10} = \dfrac{r}{20}$ Then we can say $D \in \left( r, \theta\right)\hspace{1mm}|\hspace{2mm} 0\leq r\leq \sqrt{2},\hspace{2mm}0\leq \theta \leq 2\pi$ $$\int\int_D f(x)\cdot f(y)\hspace{1mm}dydx = \int_0^{2\pi} \int_{0}^{\sqrt{2}}\left[ \dfrac{2}{\sqrt{2\pi}}\hspace{1.4mm} e^{-0.5r^2\cos^2\theta}\right]\cdot \left[\dfrac{10}{\sqrt{2\pi}} e^{-0.5r^2\sin^2\theta} \right] \cdot \left[ \dfrac{r}{20}\right]drd\theta$$ $$= \int_0^{2\pi} \int_{0}^{\sqrt{2}}\dfrac{r}{2\pi}\hspace{1.4mm} e^{-0.5r^2\cos^2\theta-0.5r^2\sin^2\theta}\hspace{1mm}drd\theta$$ $$= \dfrac{1}{2\pi}\left[\int_0^{2\pi} d\theta \right]\left[\int_{0}^{\sqrt{2}}r\hspace{1.4mm} e^{-0.5r^2}\hspace{1mm}dr \right]$$ Substitute $-0.5r^2 = u$ and $-r dr = du$ $$\text{Limits of Integration will change from $\int_0^{\sqrt{2}}$ to $\int_{-0^2}^{-(\sqrt{2})^2} = \int_0^{-2}$}$$ $$= \dfrac{1}{2\pi}\left[{2\pi}  \right]\left[-\int_{0}^{-2}e^{u}\hspace{1mm}du \right]$$ $$=\left[-e^{u}\right]_{0}^{-2} = 1-e^{-2} = 1-\dfrac{1}{e^2}\approx 0.8647$$ Answer at the back of the book is $0.632$!",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
25,If $\vec r = x \hat i + y \hat j + z \hat k$ and $r =| \vec r |$ show that $curl [f(r) \vec r] = 0$,If  and  show that,\vec r = x \hat i + y \hat j + z \hat k r =| \vec r | curl [f(r) \vec r] = 0,"I know that $\nabla \times f(r) \vec r = \nabla f(r) \times \vec r + f(r) \left ( \nabla \times \vec r \right )$. I figured that the rightmost expression is $0$. How do I prove that $\nabla f(r) \times \vec r = 0$ ? The actual question in the book is to evaluate the curl, and the answer key says that the answer is 0. I've already expanded the vector product but do not see how it becomes 0.","I know that $\nabla \times f(r) \vec r = \nabla f(r) \times \vec r + f(r) \left ( \nabla \times \vec r \right )$. I figured that the rightmost expression is $0$. How do I prove that $\nabla f(r) \times \vec r = 0$ ? The actual question in the book is to evaluate the curl, and the answer key says that the answer is 0. I've already expanded the vector product but do not see how it becomes 0.",,['multivariable-calculus']
26,Checking if the Hessian is the derivative of the gradient,Checking if the Hessian is the derivative of the gradient,,"Suppose $f: \Bbb R^n \to \Bbb R$. I have a code that computes the gradient of $f$. I have another code that computes the Hessian of $f$ times a vector. Now I want to check if they are correct. Specifically, I am checking of the Hessian times vector is the directional derivative of the gradient. I have Hessian(x).v \approx (1/h) * (grad(x+h.v) - grad(x)) where h is a scalar. Define: fderror:= (Hessian.v - (1/h) * ( grad(x+h.v) - grad(x))))^T (Hessian.v - (1/h) * ( grad(x+h.v) - grad(x)))). For some reasons, as $h$ decreases, my $\text{fderror}$ increases and vice versa. And when $h$ is around 1e-10 , my $\text{fderror}$ is around 1e+00 . Is it due to floating point or my Hessian-times-vector is not correct? I would greatly appreciate your help.","Suppose $f: \Bbb R^n \to \Bbb R$. I have a code that computes the gradient of $f$. I have another code that computes the Hessian of $f$ times a vector. Now I want to check if they are correct. Specifically, I am checking of the Hessian times vector is the directional derivative of the gradient. I have Hessian(x).v \approx (1/h) * (grad(x+h.v) - grad(x)) where h is a scalar. Define: fderror:= (Hessian.v - (1/h) * ( grad(x+h.v) - grad(x))))^T (Hessian.v - (1/h) * ( grad(x+h.v) - grad(x)))). For some reasons, as $h$ decreases, my $\text{fderror}$ increases and vice versa. And when $h$ is around 1e-10 , my $\text{fderror}$ is around 1e+00 . Is it due to floating point or my Hessian-times-vector is not correct? I would greatly appreciate your help.",,"['analysis', 'multivariable-calculus', 'numerical-methods', 'finite-differences', 'floating-point']"
27,Hadamard's Lemma in multidimensional real analysis,Hadamard's Lemma in multidimensional real analysis,,"This is Hadamard's Lemma: Let $U \subset \Bbb R^n$ be an open set, let $a \in U$ and $f: U \to \Bbb R^p$. Then the following assertions are equivalent. The mapping $f$ is differentiable at $a$. There exists an operator-valued mapping $\phi = \phi_a: U \to \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ continuous at $a$, such that $$f(x) = f(a) + \phi_a(x)(x-a).$$ If one of these conditions is satisfied we have $\phi_a(a) = Df(a)$. I'm struggling with implication $2 \Longrightarrow 1$. The book claims it is straightforward using a lemma that says that for an operator $A$ we have $\| A h \| \leq \| A \| \| h \|$, it's the Euclidean norm. I think I've solved it as follows, and I would like to know if it's correct: If $f$ has the given representation then we have $$f(a+h) = f(a) + \phi_a(a+h) h$$ for all vectors $h=x-a$. Since $\phi_a(a+h) \in \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ then $\phi_a(a+h) = \phi_a(a) + \phi_a(h)$ and we have $$f(a+h) = f(a) + \phi_a(a)h + \phi_a(h)h.$$ We need to show now that $\epsilon_a(h) = \phi_a(h)h$ satisfies $$\lim_{h \to 0} \frac{\| \epsilon_a (h)\|}{\| h\|} = 0.$$ (This is an equivalent definition of differentiability presented in the book.) We have $$0 \leq \lim_{h \to 0} \frac{\| \epsilon_a (h)\|}{\| h\|} \leq \lim_{h \to 0} \frac{\| \phi_a(h) \| \|h \|}{\| h \|} = \lim_{h \to 0} \| \phi_a(h) \|,$$ and since $\phi_a(h) \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ this evaluates to zero, proving $f$ is differentiable.","This is Hadamard's Lemma: Let $U \subset \Bbb R^n$ be an open set, let $a \in U$ and $f: U \to \Bbb R^p$. Then the following assertions are equivalent. The mapping $f$ is differentiable at $a$. There exists an operator-valued mapping $\phi = \phi_a: U \to \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ continuous at $a$, such that $$f(x) = f(a) + \phi_a(x)(x-a).$$ If one of these conditions is satisfied we have $\phi_a(a) = Df(a)$. I'm struggling with implication $2 \Longrightarrow 1$. The book claims it is straightforward using a lemma that says that for an operator $A$ we have $\| A h \| \leq \| A \| \| h \|$, it's the Euclidean norm. I think I've solved it as follows, and I would like to know if it's correct: If $f$ has the given representation then we have $$f(a+h) = f(a) + \phi_a(a+h) h$$ for all vectors $h=x-a$. Since $\phi_a(a+h) \in \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ then $\phi_a(a+h) = \phi_a(a) + \phi_a(h)$ and we have $$f(a+h) = f(a) + \phi_a(a)h + \phi_a(h)h.$$ We need to show now that $\epsilon_a(h) = \phi_a(h)h$ satisfies $$\lim_{h \to 0} \frac{\| \epsilon_a (h)\|}{\| h\|} = 0.$$ (This is an equivalent definition of differentiability presented in the book.) We have $$0 \leq \lim_{h \to 0} \frac{\| \epsilon_a (h)\|}{\| h\|} \leq \lim_{h \to 0} \frac{\| \phi_a(h) \| \|h \|}{\| h \|} = \lim_{h \to 0} \| \phi_a(h) \|,$$ and since $\phi_a(h) \operatorname{Lin}(\Bbb R^n, \Bbb R^p)$ this evaluates to zero, proving $f$ is differentiable.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry']"
28,"Triple Integral $\iiint\limits_Dz\;dx\,dy\,dz$ where $D=\{(x,y,z):x^2+y^2=1,0\le z\le\sqrt{x^2+y^2}\}$",Triple Integral  where,"\iiint\limits_Dz\;dx\,dy\,dz D=\{(x,y,z):x^2+y^2=1,0\le z\le\sqrt{x^2+y^2}\}","Calculate $\iiint_Dz\;dxdydz$ if $D$ is the region inside $z=0,z=\sqrt{x^2+y^2}$ and $x^2+y^2=1$. I would like to know if the answer I got is right. This is what I did: $(1)$ Change to cylindrical coordinates $$x=r\cos\phi,y=r\sin\phi,z=z$$ $(2)$ Determine the region in terms of $r,\phi,z$ I usually have some problems determining the region, let's see if I got this right: $(2.1)$ From $z=0$ and $z=\sqrt{x^2+y^2}$ follows $0\leq z\leq \sqrt{r^2\cos^2\phi+r^2\sin^2\phi}=r$. $(2.2)$ The regions goes the ""full circle"", so $0\leq\phi\leq 2\pi$. $(2.3)$ The region is inside the unit circle, so $0\leq r\leq 1$. $(3)$ Integrandus calculandus $$\iiint_Dz\;dxdydz=\int_0^1\int_0^{2\pi}\int_0^r zr\;dzd\phi dr\\=\frac{1}{2}\int_0^1\int_0^{2\pi}r^3d\phi dr=\pi\int_0^1r^3dr=\frac{\pi}{4}.$$ Does it look right?. I thought of an alternative way to ""solve it"" with cylindrical coordinates, I know I got the wrong answer but I cannot see why is this wrong. Instead of the limits given before, I take $$0\leq z \leq 1\\ 0\leq r \leq \sqrt{z}\\ 0\leq \phi\leq 2\pi $$ It seems that this describes the region inside the cone, and $$\int_0^1\int_0^{2\pi}\int_0^{\sqrt{z}}zr\;dr d\phi dz = \frac{1}{2}\int_0^1\int_0^{2\pi}z^2d\phi dz =\pi \int_0^1 z^2 = \frac{\pi}{3}.$$ The cone is inside the cilinder of raduis 1 from $0\leq z\leq 1$, then wouldn't be  $$\iiint_D = \text{volume cylinder - volume cone} = \pi - \pi/3 = 2\pi/3?$$ I know the answer is wrong (I already saw Danieltatis's answer) but I wanted to try a different solution, and I don't know where did I go wrong with the last one.","Calculate $\iiint_Dz\;dxdydz$ if $D$ is the region inside $z=0,z=\sqrt{x^2+y^2}$ and $x^2+y^2=1$. I would like to know if the answer I got is right. This is what I did: $(1)$ Change to cylindrical coordinates $$x=r\cos\phi,y=r\sin\phi,z=z$$ $(2)$ Determine the region in terms of $r,\phi,z$ I usually have some problems determining the region, let's see if I got this right: $(2.1)$ From $z=0$ and $z=\sqrt{x^2+y^2}$ follows $0\leq z\leq \sqrt{r^2\cos^2\phi+r^2\sin^2\phi}=r$. $(2.2)$ The regions goes the ""full circle"", so $0\leq\phi\leq 2\pi$. $(2.3)$ The region is inside the unit circle, so $0\leq r\leq 1$. $(3)$ Integrandus calculandus $$\iiint_Dz\;dxdydz=\int_0^1\int_0^{2\pi}\int_0^r zr\;dzd\phi dr\\=\frac{1}{2}\int_0^1\int_0^{2\pi}r^3d\phi dr=\pi\int_0^1r^3dr=\frac{\pi}{4}.$$ Does it look right?. I thought of an alternative way to ""solve it"" with cylindrical coordinates, I know I got the wrong answer but I cannot see why is this wrong. Instead of the limits given before, I take $$0\leq z \leq 1\\ 0\leq r \leq \sqrt{z}\\ 0\leq \phi\leq 2\pi $$ It seems that this describes the region inside the cone, and $$\int_0^1\int_0^{2\pi}\int_0^{\sqrt{z}}zr\;dr d\phi dz = \frac{1}{2}\int_0^1\int_0^{2\pi}z^2d\phi dz =\pi \int_0^1 z^2 = \frac{\pi}{3}.$$ The cone is inside the cilinder of raduis 1 from $0\leq z\leq 1$, then wouldn't be  $$\iiint_D = \text{volume cylinder - volume cone} = \pi - \pi/3 = 2\pi/3?$$ I know the answer is wrong (I already saw Danieltatis's answer) but I wanted to try a different solution, and I don't know where did I go wrong with the last one.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
29,"Notation for i, j, k component of gradient vector at point x, y, z?","Notation for i, j, k component of gradient vector at point x, y, z?",,"I have a function, $w=x^2+y^2-z^2$, and its gradient vector, $\nabla w=(2x, 2y, -2y)$. How can I write the equation for its tangent plane? Is something like the following accurate? $$ p=\nabla w_\hat i(x,y,z)x +\nabla w_\hat j(x,y,z)y + \nabla w_\hat k(x,y,z)z$$ What is the convention for something like this?","I have a function, $w=x^2+y^2-z^2$, and its gradient vector, $\nabla w=(2x, 2y, -2y)$. How can I write the equation for its tangent plane? Is something like the following accurate? $$ p=\nabla w_\hat i(x,y,z)x +\nabla w_\hat j(x,y,z)y + \nabla w_\hat k(x,y,z)z$$ What is the convention for something like this?",,['multivariable-calculus']
30,What is a closed form expression for the ∂/∂w(∂t/∂w) if w(t) is complicated function?,What is a closed form expression for the ∂/∂w(∂t/∂w) if w(t) is complicated function?,,Lets say we have a trigonometric function w(t) that can not be inverted as t(w). The derivative ∂t/∂w can be calculated as 1/(∂w(t)/∂t). What is a closed form expression for the second derivative ∂/∂w(∂t/∂w) ? Numerically do you start from the closed form or there is easier way ? This represents the group delay dispersion and third order dispersion.,Lets say we have a trigonometric function w(t) that can not be inverted as t(w). The derivative ∂t/∂w can be calculated as 1/(∂w(t)/∂t). What is a closed form expression for the second derivative ∂/∂w(∂t/∂w) ? Numerically do you start from the closed form or there is easier way ? This represents the group delay dispersion and third order dispersion.,,"['calculus', 'multivariable-calculus', 'derivatives']"
31,Calculating a triple integral with spherical coordinates?,Calculating a triple integral with spherical coordinates?,,"I need to calculate $$\iint_D \sqrt{x^2+y^2+z^2} dx dy dz$$  where $D=\{ (x,y,z):x^2+y^2+z^2\leq z\}$ . After substituting $x=r\cos\theta\sin\phi , y=r\sin\theta\sin\phi , z=r\cos\phi $ into the inequality $x^2+y^2+z^2\leq z$, I received that $0\leq r \leq\cos \phi$ so as far as I understand this $\phi$ should be in $[-\frac{\pi}{2} , \frac{\pi}{2}] $ . The problem is that when I calculate it with these boundaries I get the integral is zero, and when I calculate it for $\phi \in [0 , \frac{\pi}{2}] $ and multiply by $2$, I get $\frac{\pi}{10}$. So: Why is it not correct to take $\phi \in [-\frac{\pi}{2} , \frac{\pi}{2}] $? Thanks in advance.","I need to calculate $$\iint_D \sqrt{x^2+y^2+z^2} dx dy dz$$  where $D=\{ (x,y,z):x^2+y^2+z^2\leq z\}$ . After substituting $x=r\cos\theta\sin\phi , y=r\sin\theta\sin\phi , z=r\cos\phi $ into the inequality $x^2+y^2+z^2\leq z$, I received that $0\leq r \leq\cos \phi$ so as far as I understand this $\phi$ should be in $[-\frac{\pi}{2} , \frac{\pi}{2}] $ . The problem is that when I calculate it with these boundaries I get the integral is zero, and when I calculate it for $\phi \in [0 , \frac{\pi}{2}] $ and multiply by $2$, I get $\frac{\pi}{10}$. So: Why is it not correct to take $\phi \in [-\frac{\pi}{2} , \frac{\pi}{2}] $? Thanks in advance.",,['multivariable-calculus']
32,a question about multivariable integral!,a question about multivariable integral!,,"If $\lfloor x \rfloor$ denotes the greatest integer in $x$, evaluate the integral$$ \iint_{R} \lfloor x+y \rfloor ~ \mathrm{d}x~ \mathrm{d}y$$where $R= \{(x,y)| 1\leq x\leq 3, 2\leq y\leq 5\}$. This question is in my textbook, but I have no idea about how to solve it. Does somebody help me solve this question, or can somebody give me some hints about it?","If $\lfloor x \rfloor$ denotes the greatest integer in $x$, evaluate the integral$$ \iint_{R} \lfloor x+y \rfloor ~ \mathrm{d}x~ \mathrm{d}y$$where $R= \{(x,y)| 1\leq x\leq 3, 2\leq y\leq 5\}$. This question is in my textbook, but I have no idea about how to solve it. Does somebody help me solve this question, or can somebody give me some hints about it?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
33,solving double integrals,solving double integrals,,"I'm trying to solve a double integral: $\displaystyle \int_{0}^{\frac{1}{2}}\int_{0}^{\frac{1}{2}-y}24xy\; dx \;dy$ I first solved in respect to $y$, making the $x$ a constant and plugged in the $y$ values then took the derivative with respect to $x$, making the $y$ values a constant. 24 x2/2(1/2-y)2 - x2/2(1/2-0)2 = ?  this is where I am stuck","I'm trying to solve a double integral: $\displaystyle \int_{0}^{\frac{1}{2}}\int_{0}^{\frac{1}{2}-y}24xy\; dx \;dy$ I first solved in respect to $y$, making the $x$ a constant and plugged in the $y$ values then took the derivative with respect to $x$, making the $y$ values a constant. 24 x2/2(1/2-y)2 - x2/2(1/2-0)2 = ?  this is where I am stuck",,"['calculus', 'integration', 'multivariable-calculus']"
34,Line Integral as Circulation - But why?,Line Integral as Circulation - But why?,,"In most vector calculus texts say that if if the vector field $\vec{F}$ is viewed as the velocity vector of a fluid, then the surface integral $\iint_{S} \vec{F} \cdot d\vec{S}$, called flux, could be viewed as the amount of fluid that cross the surface $S$ per unit of time. On the other hand, the line integral $\oint_{C} \vec{F} \cdot d\vec{s}$, called circulation, could be interpreted as the amount of fluid transported tangentially to the closed curve $C$ per unit of time. Well, while I find the proposition about the flux very intuitive (and dimesionally sound), I'm very confused about the interpretation of circulation. Supposedly, the dot product $\vec{F} \cdot \Delta\vec{s}$ gives the amount of fluid transported along the path $\Delta\vec{s}$. This doesn't make sense to me (It isn't even well defined what be transported along a path means...). More over, the units of this product are $[m^2/s]$, wich looks very weird to me. Can someone help me understand this interpretation of circulation? Related question","In most vector calculus texts say that if if the vector field $\vec{F}$ is viewed as the velocity vector of a fluid, then the surface integral $\iint_{S} \vec{F} \cdot d\vec{S}$, called flux, could be viewed as the amount of fluid that cross the surface $S$ per unit of time. On the other hand, the line integral $\oint_{C} \vec{F} \cdot d\vec{s}$, called circulation, could be interpreted as the amount of fluid transported tangentially to the closed curve $C$ per unit of time. Well, while I find the proposition about the flux very intuitive (and dimesionally sound), I'm very confused about the interpretation of circulation. Supposedly, the dot product $\vec{F} \cdot \Delta\vec{s}$ gives the amount of fluid transported along the path $\Delta\vec{s}$. This doesn't make sense to me (It isn't even well defined what be transported along a path means...). More over, the units of this product are $[m^2/s]$, wich looks very weird to me. Can someone help me understand this interpretation of circulation? Related question",,['multivariable-calculus']
35,Path integral problem (mass),Path integral problem (mass),,"Find the mass of a wire whose shape is that of a curve of intersection of the sphere $x^2 + y^2 + z^2 = 1$ and the plane $x + y + z = 0$ if the density of the wire is $x^2$. I know that this problem is just a simple computation with line integrals wrt to arc length, but the issue is that I can't find a parametrization. This very question is answered here , but I'm not entirely sure how to get the parametrization given (not enough sleep last night. .  . ) Could someone help me derive that parametrization?","Find the mass of a wire whose shape is that of a curve of intersection of the sphere $x^2 + y^2 + z^2 = 1$ and the plane $x + y + z = 0$ if the density of the wire is $x^2$. I know that this problem is just a simple computation with line integrals wrt to arc length, but the issue is that I can't find a parametrization. This very question is answered here , but I'm not entirely sure how to get the parametrization given (not enough sleep last night. .  . ) Could someone help me derive that parametrization?",,['multivariable-calculus']
36,Spherical-Coordinate Reference Frame,Spherical-Coordinate Reference Frame,,"my problem looks apparently easy but I can figure out a solution. I'm going through a paper about the Boltzmann equation and I got stuck with this change of coordinates. The original formula for Q (it's the collision integral but that is not relevant at the moment) is the following: $$ Q(f,f)(v) = \int\limits_{x\in{B_{R}}} \;\int\limits_{y\in{B_{R}}} \delta(x\cdot{y}) K(|x|,|y|)[f(v+x)f(v+y)-f(v+x+y)f(v)]dxdy\,. $$ Just to let you know, f is a probability distribution and $K$ is the so-called kernel. $ \delta $ is the Dirac delta as one might guess. $ B_{R} $ is the ball of radius $R$ centered in the origin, $d$ is the dimension of $x$ and $y$. By a change of coordinates, namely $ x=\rho e $ and $ y=\rho' e' $, I ""should"" get $$ Q(f,f)(v) = \frac{1}{4} \int\limits_{e\in{\Bbb S^{d-1}}} \; \int\limits_{e'\in{\Bbb S^{d-1}}} \int_{-R}^{R} \int_{-R}^{R} \rho^{d-2} \rho'^{d-2} \delta(e\cdot{e'}) K(\rho,\rho')[f(v+\rho'e')f(v+\rho e)-f(v+\rho e+\rho'e')f(v)]d\rho d\rho'dede'\,. $$ Here $ \Bbb S^{d-1} $ denotes the sphere of unitary radius centered in the origin of dimension $d-1$. The integral makes perfectly sense and I tried to compute it directly in one case and the two expressions yield the very same result. By applying the usual change of coordinates for an $n$-sphere (same formula as the one given by Wikipedia on the $n$-sphere article) and by substituting the angles by the surface element after having computed the determinant of the Jacobian, I get the same result except for the exponent of $ \rho $. I get $ \rho^{d-1} \rho'^{d-1} $ instead of $ \rho^{d-2} \rho'^{d-2} $. Then I tried to solve the problem by considering a single vector made up by the two column vectors $x$ and $y$ (the dimension of the problem is doubled) to see whether I could get the right exponent but I couldn't. Could you please suggest a strategy to get that formula (or a reference)? Thank you anyway.","my problem looks apparently easy but I can figure out a solution. I'm going through a paper about the Boltzmann equation and I got stuck with this change of coordinates. The original formula for Q (it's the collision integral but that is not relevant at the moment) is the following: $$ Q(f,f)(v) = \int\limits_{x\in{B_{R}}} \;\int\limits_{y\in{B_{R}}} \delta(x\cdot{y}) K(|x|,|y|)[f(v+x)f(v+y)-f(v+x+y)f(v)]dxdy\,. $$ Just to let you know, f is a probability distribution and $K$ is the so-called kernel. $ \delta $ is the Dirac delta as one might guess. $ B_{R} $ is the ball of radius $R$ centered in the origin, $d$ is the dimension of $x$ and $y$. By a change of coordinates, namely $ x=\rho e $ and $ y=\rho' e' $, I ""should"" get $$ Q(f,f)(v) = \frac{1}{4} \int\limits_{e\in{\Bbb S^{d-1}}} \; \int\limits_{e'\in{\Bbb S^{d-1}}} \int_{-R}^{R} \int_{-R}^{R} \rho^{d-2} \rho'^{d-2} \delta(e\cdot{e'}) K(\rho,\rho')[f(v+\rho'e')f(v+\rho e)-f(v+\rho e+\rho'e')f(v)]d\rho d\rho'dede'\,. $$ Here $ \Bbb S^{d-1} $ denotes the sphere of unitary radius centered in the origin of dimension $d-1$. The integral makes perfectly sense and I tried to compute it directly in one case and the two expressions yield the very same result. By applying the usual change of coordinates for an $n$-sphere (same formula as the one given by Wikipedia on the $n$-sphere article) and by substituting the angles by the surface element after having computed the determinant of the Jacobian, I get the same result except for the exponent of $ \rho $. I get $ \rho^{d-1} \rho'^{d-1} $ instead of $ \rho^{d-2} \rho'^{d-2} $. Then I tried to solve the problem by considering a single vector made up by the two column vectors $x$ and $y$ (the dimension of the problem is doubled) to see whether I could get the right exponent but I couldn't. Could you please suggest a strategy to get that formula (or a reference)? Thank you anyway.",,"['multivariable-calculus', 'differential-geometry', 'spherical-coordinates']"
37,A confusing vector field differential,A confusing vector field differential,,"In my notes on theoretical mechanics, I wrote that my professor stated this vector identity: $$\mathrm{d}\mathbf{P}(\mathbf{r})=[\nabla\cdot\mathbf{P}(\mathbf{r})] \mathbf{dr} + [\nabla\times\mathbf{P}(\mathbf{r})]\times\mathbf{dr}$$ Here, $\mathbf{P}(\mathbf{r})$ stands for the vector field depending only on the position vector $\mathbf{r}$. In other words, it's a vector-valued function of a vector in $\mathbb{R}^3$. The thing is, I didn't manage to write down what the importance was, although I think it had something to do with separating parallel and perpendicular differential vectors. I tried to find something like this on the Internet and I also tried to show it, but I'm afraid I can't find an elegant way to show this and I don't think I'll gain anything by mechanically grinding it out. What is the significance of this identity? Is there a simple way to prove it?","In my notes on theoretical mechanics, I wrote that my professor stated this vector identity: $$\mathrm{d}\mathbf{P}(\mathbf{r})=[\nabla\cdot\mathbf{P}(\mathbf{r})] \mathbf{dr} + [\nabla\times\mathbf{P}(\mathbf{r})]\times\mathbf{dr}$$ Here, $\mathbf{P}(\mathbf{r})$ stands for the vector field depending only on the position vector $\mathbf{r}$. In other words, it's a vector-valued function of a vector in $\mathbb{R}^3$. The thing is, I didn't manage to write down what the importance was, although I think it had something to do with separating parallel and perpendicular differential vectors. I tried to find something like this on the Internet and I also tried to show it, but I'm afraid I can't find an elegant way to show this and I don't think I'll gain anything by mechanically grinding it out. What is the significance of this identity? Is there a simple way to prove it?",,"['multivariable-calculus', 'vector-analysis']"
38,Intuition for directional derivative,Intuition for directional derivative,,"Given some function $ z=f(x,y) $ , the directional derivative can be used to calculate the rate of change of $ z $ in the direction of some unit vector $ \vec{u} = <a,b> $ . The directional derivative for z can then be defined as $$ D_{\vec{u}}f(x,y) = \lim_{h\rightarrow0}\frac{f(x+ah, y+bh)-f(x, y)}{h} \tag{1} $$ Intuitively, this definition makes sense. By this definition, however, I wouldn't think that $ \vec{u} $ would need to be a unit vector. I feel like $ x + ah $ and $ y + bh $ will become infinitely small regardless of the length of $ \vec{u} $ . Is this indeed the case? Accepting definition $ (1) $ , the following equivalent equation makes less sense to me: $$ D_{\vec{u}}f(x,y) = \frac{\partial f}{\partial x}a + \frac{\partial f}{\partial y}b \tag{2} $$ I've seen a proof of this in which you can go from definition $ (1) $ to equation $ (2) $ via the chain rule. I can fully accept the proof, but I feel like there is something to be said about the actual form of equation $ (2) $ . Is there some geometric significance concerning $ (2) $ ? I feel like the fact that $ \vec{u} $ has to be a unit vector may come into play.","Given some function , the directional derivative can be used to calculate the rate of change of in the direction of some unit vector . The directional derivative for z can then be defined as Intuitively, this definition makes sense. By this definition, however, I wouldn't think that would need to be a unit vector. I feel like and will become infinitely small regardless of the length of . Is this indeed the case? Accepting definition , the following equivalent equation makes less sense to me: I've seen a proof of this in which you can go from definition to equation via the chain rule. I can fully accept the proof, but I feel like there is something to be said about the actual form of equation . Is there some geometric significance concerning ? I feel like the fact that has to be a unit vector may come into play."," z=f(x,y)   z   \vec{u} = <a,b>   D_{\vec{u}}f(x,y) = \lim_{h\rightarrow0}\frac{f(x+ah, y+bh)-f(x, y)}{h} \tag{1}   \vec{u}   x + ah   y + bh   \vec{u}   (1)   D_{\vec{u}}f(x,y) = \frac{\partial f}{\partial x}a + \frac{\partial f}{\partial y}b \tag{2}   (1)   (2)   (2)   (2)   \vec{u} ","['multivariable-calculus', 'derivatives']"
39,Acquiring $Df(\mathbf{x})$,Acquiring,Df(\mathbf{x}),"Sorry for the probably easy and silly question, but I try to teach myself linear algebra and I am stucked at ""the derivative as a matrix"" part.  I know how to differentiate partially and I know how $Df(\mathbf{x})$ should look like. I just don't know how to calculate it with vectors/matrices. In the excercise I need to find $Df(\mathbf{x})$ at $a=\begin{pmatrix}  1\\  2\\  \end{pmatrix}$ with $f(\mathbf{x})=\begin{pmatrix} (x+y)^3\\  x^2y^3\\  y/x\\ \end{pmatrix}$. I don't understand the process... So could you please explain to me how it works? I am using these lecture notes (this part starts on p. 53). Thank you in advance.","Sorry for the probably easy and silly question, but I try to teach myself linear algebra and I am stucked at ""the derivative as a matrix"" part.  I know how to differentiate partially and I know how $Df(\mathbf{x})$ should look like. I just don't know how to calculate it with vectors/matrices. In the excercise I need to find $Df(\mathbf{x})$ at $a=\begin{pmatrix}  1\\  2\\  \end{pmatrix}$ with $f(\mathbf{x})=\begin{pmatrix} (x+y)^3\\  x^2y^3\\  y/x\\ \end{pmatrix}$. I don't understand the process... So could you please explain to me how it works? I am using these lecture notes (this part starts on p. 53). Thank you in advance.",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives']"
40,Epsilon delta proof for continuity of $ (1-\cos|xy|)/y^2$,Epsilon delta proof for continuity of, (1-\cos|xy|)/y^2,"Let a function, $\mathbb{R}^2\to\mathbb{R}: \begin{Bmatrix} \frac{1-\cos(|xy|)}{y^2}&y\neq0\\ \frac{x^2}{2}&y=0 \end{Bmatrix} $ I have to prove this is continious. For y$\neq 0$, this is trivial, but for $y=0$ I use $\epsilon -\delta$ to prove its continious on all points $(x_0,0)$: $$\begin{align}\left|\frac{1-\cos(|xy|)}{y^2}-\frac{x_0^2}{2}\right|<\epsilon&\iff\left|\frac{2\sin^2(|xy|/2)}{y^2}-\frac{x_0^2}{2}\right|<\epsilon\\ &\Leftarrow \left|\frac{|xy|^2}{2y^2}-\frac{x_0^2}{2}\right|<\epsilon\\&\iff\left|\frac{x^2}{2}-\frac{x_0^2}{2}\right|<\epsilon\end{align}$$ And now I'm stuck. I need to get $(x-x_0)^2+y^2<\delta_\epsilon$ which seems impossible. What did I do wrong? When I made it bigger (changing the sine with it's argument), was it a bad move? I'm getting desparate.","Let a function, $\mathbb{R}^2\to\mathbb{R}: \begin{Bmatrix} \frac{1-\cos(|xy|)}{y^2}&y\neq0\\ \frac{x^2}{2}&y=0 \end{Bmatrix} $ I have to prove this is continious. For y$\neq 0$, this is trivial, but for $y=0$ I use $\epsilon -\delta$ to prove its continious on all points $(x_0,0)$: $$\begin{align}\left|\frac{1-\cos(|xy|)}{y^2}-\frac{x_0^2}{2}\right|<\epsilon&\iff\left|\frac{2\sin^2(|xy|/2)}{y^2}-\frac{x_0^2}{2}\right|<\epsilon\\ &\Leftarrow \left|\frac{|xy|^2}{2y^2}-\frac{x_0^2}{2}\right|<\epsilon\\&\iff\left|\frac{x^2}{2}-\frac{x_0^2}{2}\right|<\epsilon\end{align}$$ And now I'm stuck. I need to get $(x-x_0)^2+y^2<\delta_\epsilon$ which seems impossible. What did I do wrong? When I made it bigger (changing the sine with it's argument), was it a bad move? I'm getting desparate.",,"['multivariable-calculus', 'continuity', 'epsilon-delta']"
41,Need help understanding a certain vector integral identity (Stokes' theorem corollary),Need help understanding a certain vector integral identity (Stokes' theorem corollary),,"The Vector Integral page on the Wolfram mathworld website lists as eq.(4) the following vector integral identity: If   $$\mathbf{F}:=\mathbf{c}\,F,$$   then   $$\int_{C}F\,ds=\int_{S}d\mathbf{a}\times\nabla\mathbf{F}.$$ It seems clear from context that $\mathbf{c}$ is supposed to be a constant vector, though the page doesn't explicitly list this as a condition. Try as I might, I can't wrap my head around this integral. In fact, it looks like patent nonsense to me, since on the LHS we have a scalar line integral (that is, the line-element $ds$ is a scalar) of a scalar field resulting in a scalar value, while the integral on the RHS looks to me like it must be a 2nd rank tensor. I'd be surprised if this is an error on Wolfram's part that's somehow gone unnoticed all this time, so I can only assume that I have a fundamental misunderstanding somewhere that's preventing me from parsing this formula correctly. Can someone please set me straight? UPDATE: My thanks to @TylerHG for helping me confirm that this is indeed a typo on Wolfram's part. Instead of closing this question, I'd like to ask a follow-up question prompted by the falsehood of the identity proposed above. Given a scalar field $f(\mathbf{r})$ and a surface $\Sigma$ with boundary $\partial\Sigma$, does there exist any kind of Stokes' theorem analogue for the scalar line integral, $$\oint_{\partial\Sigma}f\,\mathrm{d}\ell =\,???$$","The Vector Integral page on the Wolfram mathworld website lists as eq.(4) the following vector integral identity: If   $$\mathbf{F}:=\mathbf{c}\,F,$$   then   $$\int_{C}F\,ds=\int_{S}d\mathbf{a}\times\nabla\mathbf{F}.$$ It seems clear from context that $\mathbf{c}$ is supposed to be a constant vector, though the page doesn't explicitly list this as a condition. Try as I might, I can't wrap my head around this integral. In fact, it looks like patent nonsense to me, since on the LHS we have a scalar line integral (that is, the line-element $ds$ is a scalar) of a scalar field resulting in a scalar value, while the integral on the RHS looks to me like it must be a 2nd rank tensor. I'd be surprised if this is an error on Wolfram's part that's somehow gone unnoticed all this time, so I can only assume that I have a fundamental misunderstanding somewhere that's preventing me from parsing this formula correctly. Can someone please set me straight? UPDATE: My thanks to @TylerHG for helping me confirm that this is indeed a typo on Wolfram's part. Instead of closing this question, I'd like to ask a follow-up question prompted by the falsehood of the identity proposed above. Given a scalar field $f(\mathbf{r})$ and a surface $\Sigma$ with boundary $\partial\Sigma$, does there exist any kind of Stokes' theorem analogue for the scalar line integral, $$\oint_{\partial\Sigma}f\,\mathrm{d}\ell =\,???$$",,"['integration', 'multivariable-calculus', 'vector-analysis']"
42,Stoke's Theorem Example - Help?,Stoke's Theorem Example - Help?,,"From Stoke's Theorem: \begin{equation*} \oint_c \textbf{F}\centerdot d\textbf{r}= \int\int_S (\nabla \times\textbf{F})\centerdot d \textbf{S}\end{equation*} Evaluate $\oint _C \textbf{F}\centerdot d\textbf{r}$ by Stoke's Theorem if $\textbf{F}=<1,-2x,3>$ and C: $x^2+y^2=1$. Choose S to be the hemisphere $x^2+y^2+z^2=1$, where  $z \geq 0$. ================================================================== The above example is the only one given to us on Stoke's Theorem by our lecturer. I am only able to solve it without using Stoke's Theorem, as I am unsure how to go about solving the double integral of the sphere in order to use Stoke's Theorem. Can anybody please give me some advice as to how to go about solving this integral using Stoke's? It is VERY important that I get this example, as it is the only one that I will have to reference when practising other examples for my multivariable examination coming up in two week's time.","From Stoke's Theorem: \begin{equation*} \oint_c \textbf{F}\centerdot d\textbf{r}= \int\int_S (\nabla \times\textbf{F})\centerdot d \textbf{S}\end{equation*} Evaluate $\oint _C \textbf{F}\centerdot d\textbf{r}$ by Stoke's Theorem if $\textbf{F}=<1,-2x,3>$ and C: $x^2+y^2=1$. Choose S to be the hemisphere $x^2+y^2+z^2=1$, where  $z \geq 0$. ================================================================== The above example is the only one given to us on Stoke's Theorem by our lecturer. I am only able to solve it without using Stoke's Theorem, as I am unsure how to go about solving the double integral of the sphere in order to use Stoke's Theorem. Can anybody please give me some advice as to how to go about solving this integral using Stoke's? It is VERY important that I get this example, as it is the only one that I will have to reference when practising other examples for my multivariable examination coming up in two week's time.",,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
43,How to find the projection of a cylinder projected onto a plane?,How to find the projection of a cylinder projected onto a plane?,,"Say you are given the equations: $x + y + z = 6$ and $x^2 + y^2 = 1$ You can easily find the plane and cylinder accordingly. But how do you find the projection of the cylinder onto that plane. The $x$ boundaries should be the radius of the circle. I've been told you parametrise both equations and work from there, is that correct? This is for evaluating Stokes' Theorem by the way.","Say you are given the equations: $x + y + z = 6$ and $x^2 + y^2 = 1$ You can easily find the plane and cylinder accordingly. But how do you find the projection of the cylinder onto that plane. The $x$ boundaries should be the radius of the circle. I've been told you parametrise both equations and work from there, is that correct? This is for evaluating Stokes' Theorem by the way.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
44,Flux through a vector field of a sphere (not centered at origin)?,Flux through a vector field of a sphere (not centered at origin)?,,"If $$F = \langle xy,yx,-(x+y)z \rangle$$ find $$\int F \cdot \hat n \, dS$$ where $S$ is a sphere of radius $2$ centered at $\langle 0,1,0 \rangle$? I don't need a numerical solution to the problem because I know how to do it...for a sphere that is centered at the origin. How do I take into account the fact that the sphere is centered at $\langle 0,1,0 \rangle$? I am going to be using spherical coordinates, so can I simply add a '$-1$' onto the $y$ term of the parametrization? Or is this completely the wrong idea?","If $$F = \langle xy,yx,-(x+y)z \rangle$$ find $$\int F \cdot \hat n \, dS$$ where $S$ is a sphere of radius $2$ centered at $\langle 0,1,0 \rangle$? I don't need a numerical solution to the problem because I know how to do it...for a sphere that is centered at the origin. How do I take into account the fact that the sphere is centered at $\langle 0,1,0 \rangle$? I am going to be using spherical coordinates, so can I simply add a '$-1$' onto the $y$ term of the parametrization? Or is this completely the wrong idea?",,"['calculus', 'multivariable-calculus']"
45,How to show that this equation involving partial derivatives is true? (Change of variables),How to show that this equation involving partial derivatives is true? (Change of variables),,"Let $z$ be a function of $u$ and $v$ where $u=x+y$ and $v=3x-3y$. I have previously shown, by the Chain Rule, that $$\frac{\partial z}{\partial x}\frac{\partial z}{\partial y}= \left(\frac{\partial z}{\partial u}\right)^2 - 9 \left(\frac{\partial z}{\partial v}\right)^2.$$ Now, ""assuming equality of mixed second-order partial derivatives"", I must show that $$\frac{\partial^2 z}{\partial x^2} + \frac{\partial^2 z}{\partial y^2} = 2\frac{\partial^2 z}{\partial u^2} + 18\frac{\partial^2 z}{\partial v^2}.$$ Firstly, I don't understand what is meant by ""equality of mixed second-order partial derivatives"". Please could somebody explain this? I think I must use the Chain Rule again to get this equation but I don't really understand how to do this. I just need a pointer to get me started. Thanks.","Let $z$ be a function of $u$ and $v$ where $u=x+y$ and $v=3x-3y$. I have previously shown, by the Chain Rule, that $$\frac{\partial z}{\partial x}\frac{\partial z}{\partial y}= \left(\frac{\partial z}{\partial u}\right)^2 - 9 \left(\frac{\partial z}{\partial v}\right)^2.$$ Now, ""assuming equality of mixed second-order partial derivatives"", I must show that $$\frac{\partial^2 z}{\partial x^2} + \frac{\partial^2 z}{\partial y^2} = 2\frac{\partial^2 z}{\partial u^2} + 18\frac{\partial^2 z}{\partial v^2}.$$ Firstly, I don't understand what is meant by ""equality of mixed second-order partial derivatives"". Please could somebody explain this? I think I must use the Chain Rule again to get this equation but I don't really understand how to do this. I just need a pointer to get me started. Thanks.",,"['multivariable-calculus', 'partial-derivative']"
46,Proving a theorem on limits,Proving a theorem on limits,,"I need to prove that: If $$\lim_{(x,y) \to (a,b)}f(x,y)= 0 \text{ and } g(x,y)\leq k,$$ then:  $$\lim_{(x,y) \to (a,b)}f(x,y) g(x,y) =0.$$ My approach is like follow: $$|f(x,y)g(x,y)-0|=|f(x,y)g(x,y)|=|f(x,y)||g(x,y)|\leq|f(x,y)|k\leq k\frac{\varepsilon }{k}=\varepsilon $$ I don't know how to formally finish the prove.","I need to prove that: If $$\lim_{(x,y) \to (a,b)}f(x,y)= 0 \text{ and } g(x,y)\leq k,$$ then:  $$\lim_{(x,y) \to (a,b)}f(x,y) g(x,y) =0.$$ My approach is like follow: $$|f(x,y)g(x,y)-0|=|f(x,y)g(x,y)|=|f(x,y)||g(x,y)|\leq|f(x,y)|k\leq k\frac{\varepsilon }{k}=\varepsilon $$ I don't know how to formally finish the prove.",,"['multivariable-calculus', 'proof-writing']"
47,Newton's binomial for matrices that don't commute?,Newton's binomial for matrices that don't commute?,,"I'll give a bit of background info as to why I'm asking. I need to find the directional derivative of $f(A)=A^m$ where $m>0$ and $A$ is an $n$ by $n$ matrix with real entries. I want to do this via the definition: $$Df(A)H=\lim_{t \to 0} \frac{f(A+tH)-f(A)}{t} =\lim_{t \to 0} \frac{(A+tH)^m-A^m}{t} $$ It is clear to see that if I expand $(A+tH)^m$ then I get $A^m+t\alpha_1+t^2\alpha_2+...+t^mH^m$ where $\alpha_i$ is some combination of $A$ and $H$ that I don't know how to calculate. So overall, in the original formula if I expand it, I'll get $$\lim_{t \to 0} \frac{t\alpha_1+t^2\alpha_2+...+t^mH^m}{t}=\alpha_1$$ If $AH=HA$ then I could use newton's binomial to find $\alpha_i$, but sadly we can't assume that holds true here. So how would I find $\alpha_1$? how would I expand $(A+tH)^m$","I'll give a bit of background info as to why I'm asking. I need to find the directional derivative of $f(A)=A^m$ where $m>0$ and $A$ is an $n$ by $n$ matrix with real entries. I want to do this via the definition: $$Df(A)H=\lim_{t \to 0} \frac{f(A+tH)-f(A)}{t} =\lim_{t \to 0} \frac{(A+tH)^m-A^m}{t} $$ It is clear to see that if I expand $(A+tH)^m$ then I get $A^m+t\alpha_1+t^2\alpha_2+...+t^mH^m$ where $\alpha_i$ is some combination of $A$ and $H$ that I don't know how to calculate. So overall, in the original formula if I expand it, I'll get $$\lim_{t \to 0} \frac{t\alpha_1+t^2\alpha_2+...+t^mH^m}{t}=\alpha_1$$ If $AH=HA$ then I could use newton's binomial to find $\alpha_i$, but sadly we can't assume that holds true here. So how would I find $\alpha_1$? how would I expand $(A+tH)^m$",,"['calculus', 'multivariable-calculus', 'derivatives', 'binomial-theorem']"
48,How to find the differential of this function,How to find the differential of this function,,"we are given the function $f: \mathbb R^n \setminus \{0\} \to \mathbb R^n$ defined by: $f(x) = \frac{x}{|x|}$ Find $Df(a)$. What I did: I tried working this out from the definition. the differential at point $a$ with direction $h$ is: $$Df(a)h= \lim_{t\to 0}\frac{f(a+th)-f(a)}{t} =\lim_{t \to 0} \frac{\frac{a+th}{|a+th|}-\frac{a}{|a|}}{t} =\lim_{t \to 0} \frac{a}{t|a+th|}-\frac{a}{t|a|}+\frac{h}{|a+th|} $$ How do I continue now? The correct answer is $$Df(a)h=\frac{h}{|a|}-\frac{a \langle a,h \rangle}{|a|^3}$$","we are given the function $f: \mathbb R^n \setminus \{0\} \to \mathbb R^n$ defined by: $f(x) = \frac{x}{|x|}$ Find $Df(a)$. What I did: I tried working this out from the definition. the differential at point $a$ with direction $h$ is: $$Df(a)h= \lim_{t\to 0}\frac{f(a+th)-f(a)}{t} =\lim_{t \to 0} \frac{\frac{a+th}{|a+th|}-\frac{a}{|a|}}{t} =\lim_{t \to 0} \frac{a}{t|a+th|}-\frac{a}{t|a|}+\frac{h}{|a+th|} $$ How do I continue now? The correct answer is $$Df(a)h=\frac{h}{|a|}-\frac{a \langle a,h \rangle}{|a|^3}$$",,"['calculus', 'multivariable-calculus', 'derivatives']"
49,"Procedure for evaluating $\int_{x=\ -1}^1\int_{y=\ -\sqrt{1-x^2}}^{\sqrt{1-x^2}}\frac{x^2+y^2}{\sqrt{{1-x^2-y^2}}}\,dy\,dx$",Procedure for evaluating,"\int_{x=\ -1}^1\int_{y=\ -\sqrt{1-x^2}}^{\sqrt{1-x^2}}\frac{x^2+y^2}{\sqrt{{1-x^2-y^2}}}\,dy\,dx","While solving another problem I have come across this integral which I am unable to evaluate. Can someone please evaluate the following integral? Thank you. $$\int_{x=\ -1}^1\int_{\large y=\ -\sqrt{1-x^2}}^{\large\sqrt{1-x^2}}\frac{x^2+y^2}{\sqrt{{1-x^2-y^2}}}\,dy\,dx.$$ I know the answer is $\dfrac{4\pi}3$, but I am more interested in the procedure followed to get to this answer.","While solving another problem I have come across this integral which I am unable to evaluate. Can someone please evaluate the following integral? Thank you. $$\int_{x=\ -1}^1\int_{\large y=\ -\sqrt{1-x^2}}^{\large\sqrt{1-x^2}}\frac{x^2+y^2}{\sqrt{{1-x^2-y^2}}}\,dy\,dx.$$ I know the answer is $\dfrac{4\pi}3$, but I am more interested in the procedure followed to get to this answer.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
50,Curl Proof Question,Curl Proof Question,,"Prove the given formula. So far I have $f\textbf{F}=(f\textbf{F}_1, f\textbf{F}_2, f\textbf{F}_3)$, but I'm not sure where to go from there. Could anyone give me some pointers? Thank you.","Prove the given formula. So far I have $f\textbf{F}=(f\textbf{F}_1, f\textbf{F}_2, f\textbf{F}_3)$, but I'm not sure where to go from there. Could anyone give me some pointers? Thank you.",,['multivariable-calculus']
51,Constructing a Cone and its Normal Vectors in Spherical Coordinates,Constructing a Cone and its Normal Vectors in Spherical Coordinates,,"I am attempting to construct a right circular cone of maximum radius $R$ and angle $\theta$ in spherical coordinates, then find the normal vector of the surface of this cone at all points. Here's what I have: $$\text{cone}(r, \theta, \phi) =  \begin{cases}  x & = & r\cos{\theta}\cos{\phi} \\  y & = & r\cos{\theta}\sin{\phi} & \\ z & = & r\sin{\theta} \end{cases} $$ $$ \text{Such That:} \begin{cases} 0 \leq \phi \leq 2\pi \\ \theta = \text{constant} \\ 0 \leq r \leq R \end{cases}$$ For the normal vector, we know that the equation of a cone in cartesian coordinates is $~~x^2 + y^2 - z^2 = 0$. To find the normal vector to this surface, we take the gradient of the equation and convert it to spherical coordinates: $$\nabla(x^2 + y^2 - z^2) = ~ <2x, 2y, -2z> ~ = ~2\cdot \text{cone}(r,-\theta,\phi)$$ Is this correct? Although it may be correct, there is some part of my brain that doesn't fully grasp what I'm doing here. I think I lack a way of thinking of the construction of the normal vector geometrically. Can anyone give me some insight?","I am attempting to construct a right circular cone of maximum radius $R$ and angle $\theta$ in spherical coordinates, then find the normal vector of the surface of this cone at all points. Here's what I have: $$\text{cone}(r, \theta, \phi) =  \begin{cases}  x & = & r\cos{\theta}\cos{\phi} \\  y & = & r\cos{\theta}\sin{\phi} & \\ z & = & r\sin{\theta} \end{cases} $$ $$ \text{Such That:} \begin{cases} 0 \leq \phi \leq 2\pi \\ \theta = \text{constant} \\ 0 \leq r \leq R \end{cases}$$ For the normal vector, we know that the equation of a cone in cartesian coordinates is $~~x^2 + y^2 - z^2 = 0$. To find the normal vector to this surface, we take the gradient of the equation and convert it to spherical coordinates: $$\nabla(x^2 + y^2 - z^2) = ~ <2x, 2y, -2z> ~ = ~2\cdot \text{cone}(r,-\theta,\phi)$$ Is this correct? Although it may be correct, there is some part of my brain that doesn't fully grasp what I'm doing here. I think I lack a way of thinking of the construction of the normal vector geometrically. Can anyone give me some insight?",,"['geometry', 'multivariable-calculus']"
52,Double integrals of exponential functions,Double integrals of exponential functions,,"I need to find the double integral of $$e^{\frac{x}{y^2}}$$ bound by the $y\mbox{-axis}$, $x=y^2$, $y=1$, and $y=2$. The limits of integration were easy to find, but I am pretty confused about how to to treat exponential functions with multiple variables in the exponent. Any help would be greatly appreciated!","I need to find the double integral of $$e^{\frac{x}{y^2}}$$ bound by the $y\mbox{-axis}$, $x=y^2$, $y=1$, and $y=2$. The limits of integration were easy to find, but I am pretty confused about how to to treat exponential functions with multiple variables in the exponent. Any help would be greatly appreciated!",,"['calculus', 'integration', 'multivariable-calculus', 'exponential-function']"
53,Find a vector orthogonal to other two given and ends at a plane,Find a vector orthogonal to other two given and ends at a plane,,"I am reviewing Calculus III using Mahavier, W. Ted's material and get stuck on one question in chapter 1. Here is the problem: Assume $\vec{u},\vec{v}\in \mathbb{R}^3$. Find a vector $\vec{x}=(x,y,z)$ so that $\vec{x}\perp\vec{u}$ and $\vec{x}\perp\vec{v}$ and $x+y+z=1$. My attempt: From the last condition, I know that $\vec{x}$ ends at the plane intersecting the $x-,y-,z-$axis at $(1,0,0),(0,1,0)$ and $(0,0,1)$. From the orthogonal conditions, $\vec{x}$ is perpendicular to the plane formed by $\vec{u},\vec{v}$ if they are distinct, otherwise, any plane that contains $\vec{u},\vec{v}$. Am I on the right track? And how do I go from here? Thanks! Edit : Thanks for all who responded! I do remember cross product. However, at this point of the book, the definition of cross product has not been introduced yet. I wonder whether there are other means to attack this problem without invoking a to-be-introduced concept? Thanks again!","I am reviewing Calculus III using Mahavier, W. Ted's material and get stuck on one question in chapter 1. Here is the problem: Assume $\vec{u},\vec{v}\in \mathbb{R}^3$. Find a vector $\vec{x}=(x,y,z)$ so that $\vec{x}\perp\vec{u}$ and $\vec{x}\perp\vec{v}$ and $x+y+z=1$. My attempt: From the last condition, I know that $\vec{x}$ ends at the plane intersecting the $x-,y-,z-$axis at $(1,0,0),(0,1,0)$ and $(0,0,1)$. From the orthogonal conditions, $\vec{x}$ is perpendicular to the plane formed by $\vec{u},\vec{v}$ if they are distinct, otherwise, any plane that contains $\vec{u},\vec{v}$. Am I on the right track? And how do I go from here? Thanks! Edit : Thanks for all who responded! I do remember cross product. However, at this point of the book, the definition of cross product has not been introduced yet. I wonder whether there are other means to attack this problem without invoking a to-be-introduced concept? Thanks again!",,"['calculus', 'multivariable-calculus', 'self-learning']"
54,Building a certain kind of a multivariable function-max/min,Building a certain kind of a multivariable function-max/min,,"I want to build a function $f(x,y)$ that has the following properties: $f(x,y)$ is a polynomial of degree 2. $\nabla f(3,-2)\neq 0 $ . The maximum of $f$ under the constraint $x+y=1$ is in the point $(3,-2)$ $f$ is not constant on the line $ x+y=1$ . Will someone help me understand how to take care of parts 3 and 4? Is there any short and elegant solution to this? Thanks in advance","I want to build a function $f(x,y)$ that has the following properties: $f(x,y)$ is a polynomial of degree 2. $\nabla f(3,-2)\neq 0 $ . The maximum of $f$ under the constraint $x+y=1$ is in the point $(3,-2)$ $f$ is not constant on the line $ x+y=1$ . Will someone help me understand how to take care of parts 3 and 4? Is there any short and elegant solution to this? Thanks in advance",,['multivariable-calculus']
55,Partial Derivative Variable/Constant Question,Partial Derivative Variable/Constant Question,,"Why do we hold a variable constant when taking a partial derivative?  What is the purpose of doing so?  Is it acceptable to just implicitly differentiate, or does that cause problems?  Also, how do you know/specify the direction of your tangent line/derivative? Thanks!","Why do we hold a variable constant when taking a partial derivative?  What is the purpose of doing so?  Is it acceptable to just implicitly differentiate, or does that cause problems?  Also, how do you know/specify the direction of your tangent line/derivative? Thanks!",,"['multivariable-calculus', 'partial-derivative']"
56,Gradient and Swiftest Ascent,Gradient and Swiftest Ascent,,"I want to understand intuitively why it is that the gradient gives the direction of steepest ascent. (I will consider the case of $f:\mathbb{R}^2\to\mathbb{R}$) The standard proof is to note that the directional derivative is $$D_vf=v\cdot \nabla f=|\nabla f|\,\cos\theta$$ which is maximized at $\theta=0$. This is a good verification, but it doesn't really help me understand the result.","I want to understand intuitively why it is that the gradient gives the direction of steepest ascent. (I will consider the case of $f:\mathbb{R}^2\to\mathbb{R}$) The standard proof is to note that the directional derivative is $$D_vf=v\cdot \nabla f=|\nabla f|\,\cos\theta$$ which is maximized at $\theta=0$. This is a good verification, but it doesn't really help me understand the result.",,['multivariable-calculus']
57,"systematic way of finding the bounds for change of variables (multivariable case), Jacobian","systematic way of finding the bounds for change of variables (multivariable case), Jacobian",,"Let's say that $X,Y$ are independent standard normal random variables. I am interested in the distribution $P(X+Y\le 2t)$. Clearly, the domain of integration in this case is $-\infty<x<\infty$ and $-\infty<y\le 2t-x.$ If I were to introduce a change of variables $u=\frac{1}{2}(x+y)$ and $v=\frac{1}{2}(x-y)$, how can I systematically determine the domain of integration for $u,v$? From the looks of it, clearly $u\le t$. And judging from the values of $x,y$, $-\infty < u \le t$ and $-\infty <v<\infty.$ But this was merely obtained by looking at the values of $x$ and $y$. How would I know that the region of integration on the $uv$-plane would have, for instance, the bounds on $v$ being a function of $u$?","Let's say that $X,Y$ are independent standard normal random variables. I am interested in the distribution $P(X+Y\le 2t)$. Clearly, the domain of integration in this case is $-\infty<x<\infty$ and $-\infty<y\le 2t-x.$ If I were to introduce a change of variables $u=\frac{1}{2}(x+y)$ and $v=\frac{1}{2}(x-y)$, how can I systematically determine the domain of integration for $u,v$? From the looks of it, clearly $u\le t$. And judging from the values of $x,y$, $-\infty < u \le t$ and $-\infty <v<\infty.$ But this was merely obtained by looking at the values of $x$ and $y$. How would I know that the region of integration on the $uv$-plane would have, for instance, the bounds on $v$ being a function of $u$?",,"['calculus', 'probability', 'multivariable-calculus', 'probability-distributions']"
58,Intersection of Ellipsoid with Ray,Intersection of Ellipsoid with Ray,,"Let $E$ be an ellipsoid centered at $p = (x,y,z) \in \mathbb{R}^3$ and   let $T:\mathbb{R}^3 \to \mathbb{R}^3 $ be a linear transformation   which transforms $E$ to a unit sphere. Let $R$ be the ray $p_0 + tv$ ($v$ is normalized). Assume we want to find the minimal $t \ge 0 $ if any such that $R$   intersects $E$. I tried to define the ray $R_T$ to be $T(p_0) + tT(v)$ (where $T(v)$ is normalized) and find the minimal $t'\ge0$ if any such that $R_T$ intersects the unit sphere centered at $T(p)$. If there was no intersection with $t\ge0$ then there is no intersection with $E$. Otherwise the new $t$ is the dot product of $v$ and $T^{-1}(t' \cdot T(v))$. Is this correct? if $T(v)$ wasn't normalized in $R_T$ and we found the intersection with a sphere then the found $t$ would be the same for $E$ ?","Let $E$ be an ellipsoid centered at $p = (x,y,z) \in \mathbb{R}^3$ and   let $T:\mathbb{R}^3 \to \mathbb{R}^3 $ be a linear transformation   which transforms $E$ to a unit sphere. Let $R$ be the ray $p_0 + tv$ ($v$ is normalized). Assume we want to find the minimal $t \ge 0 $ if any such that $R$   intersects $E$. I tried to define the ray $R_T$ to be $T(p_0) + tT(v)$ (where $T(v)$ is normalized) and find the minimal $t'\ge0$ if any such that $R_T$ intersects the unit sphere centered at $T(p)$. If there was no intersection with $t\ge0$ then there is no intersection with $E$. Otherwise the new $t$ is the dot product of $v$ and $T^{-1}(t' \cdot T(v))$. Is this correct? if $T(v)$ wasn't normalized in $R_T$ and we found the intersection with a sphere then the found $t$ would be the same for $E$ ?",,"['linear-algebra', 'geometry', 'multivariable-calculus', '3d']"
59,"continuous, two-dimensional function","continuous, two-dimensional function",,"I have a question to this two dimensional function. $f_1(x,y):=\begin{cases} \frac{2xy}{x^2+y^2},&\text{if }(x,y)\neq(0,0)\\0,&\text{else}\end{cases}$ I want to analyse if this function is continous for $(x,y)=0$ I think i can show this with an $\epsilon-\delta$ proof $$|f(x,y)-f(0,0)|=\left|\frac{2xy}{x^2+y^2}\right|\leq 1$$ Would this be correct for the start? I dont know how to finish the proof. Thanks.","I have a question to this two dimensional function. $f_1(x,y):=\begin{cases} \frac{2xy}{x^2+y^2},&\text{if }(x,y)\neq(0,0)\\0,&\text{else}\end{cases}$ I want to analyse if this function is continous for $(x,y)=0$ I think i can show this with an $\epsilon-\delta$ proof $$|f(x,y)-f(0,0)|=\left|\frac{2xy}{x^2+y^2}\right|\leq 1$$ Would this be correct for the start? I dont know how to finish the proof. Thanks.",,"['multivariable-calculus', 'continuity', 'epsilon-delta']"
60,Divergence Free Vector Fields that are undefined at the origin,Divergence Free Vector Fields that are undefined at the origin,,"I am aware of vector fields which are  undefined at the origin but whose divergence everywhere else is 0. In particular, my students have already seen the inverse square vector field, i.e. $\vec{F}=\frac{\vec{r}}{||\vec{r}||^3}$ where $\vec{r}=\langle x,y,z \rangle$ and the vector field for an ideal electric dipole $\vec{E}=\nabla \left(\frac{z}{||r||^3} \right)$. Are there still other examples of divergence free vector fields that blow up at the origin? I want to hammer the concept of computing the flux of these vector fields across solids which enclose the origin by constructing a smaller solid inside (whose flux we can easily compute) and applying the divergence theorem to the solid sandwiched in between. Thanks!","I am aware of vector fields which are  undefined at the origin but whose divergence everywhere else is 0. In particular, my students have already seen the inverse square vector field, i.e. $\vec{F}=\frac{\vec{r}}{||\vec{r}||^3}$ where $\vec{r}=\langle x,y,z \rangle$ and the vector field for an ideal electric dipole $\vec{E}=\nabla \left(\frac{z}{||r||^3} \right)$. Are there still other examples of divergence free vector fields that blow up at the origin? I want to hammer the concept of computing the flux of these vector fields across solids which enclose the origin by constructing a smaller solid inside (whose flux we can easily compute) and applying the divergence theorem to the solid sandwiched in between. Thanks!",,"['calculus', 'multivariable-calculus', 'physics']"
61,Difficult time finding critical points using Lagrange,Difficult time finding critical points using Lagrange,,"The function is $f(x,y,z) = xyz$ on $x^2 + y^2 + z^2 = 1$. So I have: $yz = 2x \lambda \\ xz = 2y \lambda \\ xy = 2z \lambda \\ x^2 + y^2 + z^2 = 1$ I guessed $x = \pm 1, y = 0, z = 0, \lambda = 0$ but apparently this isn't a critical point. There's many more that I can't seem to find using algebra either. I can't divide anything out because I can't divide by $0$, and I also can't factor anything. How can I find all the critical points?","The function is $f(x,y,z) = xyz$ on $x^2 + y^2 + z^2 = 1$. So I have: $yz = 2x \lambda \\ xz = 2y \lambda \\ xy = 2z \lambda \\ x^2 + y^2 + z^2 = 1$ I guessed $x = \pm 1, y = 0, z = 0, \lambda = 0$ but apparently this isn't a critical point. There's many more that I can't seem to find using algebra either. I can't divide anything out because I can't divide by $0$, and I also can't factor anything. How can I find all the critical points?",,"['multivariable-calculus', 'optimization']"
62,f whose second-order partial derivatives are continuous,f whose second-order partial derivatives are continuous,,"Let $f: \mathbb{R} ^2 \rightarrow  \mathbb{R}$ whose second-order  partial derivatives are continuous, if $\int_C ( \frac{∂f}{∂y}dx - \frac{∂f}{∂x} dy ) = 0 $ for any closed curve simple $C$ then $ ∇^2 f(x,y) = 0$ $ \forall (x,y) \in \mathbb{R} ^2$ My teacher explained the proof of this theorem in class but i did not understand his proof someone can explain to me in detail, please. It´s really important to me  understand this for solve exercise.","Let $f: \mathbb{R} ^2 \rightarrow  \mathbb{R}$ whose second-order  partial derivatives are continuous, if $\int_C ( \frac{∂f}{∂y}dx - \frac{∂f}{∂x} dy ) = 0 $ for any closed curve simple $C$ then $ ∇^2 f(x,y) = 0$ $ \forall (x,y) \in \mathbb{R} ^2$ My teacher explained the proof of this theorem in class but i did not understand his proof someone can explain to me in detail, please. It´s really important to me  understand this for solve exercise.",,"['integration', 'multivariable-calculus']"
63,Multivariable Chain Rule,Multivariable Chain Rule,,"Does anyone know how to use the multivariable chain rule to solve the following problem? If $G(x^2+y^2, x+y)=(7x+3y, x+5\,y)$, knowing that $G:\mathbb{R}^2\rightarrow \mathbb{R}^2$, find $(G^{-1})'(24,8)$","Does anyone know how to use the multivariable chain rule to solve the following problem? If $G(x^2+y^2, x+y)=(7x+3y, x+5\,y)$, knowing that $G:\mathbb{R}^2\rightarrow \mathbb{R}^2$, find $(G^{-1})'(24,8)$",,"['multivariable-calculus', 'derivatives', 'chain-rule']"
64,Line integral: $\int _{\gamma_1} F . d\gamma_1 =\pm \int_{\gamma_2} F . d\gamma_2$?,Line integral: ?,\int _{\gamma_1} F . d\gamma_1 =\pm \int_{\gamma_2} F . d\gamma_2,"Let $F$ be a continuous vector field in $\Omega$ and, $\gamma_1: [a,b] \to \Omega$ and $\gamma_2:[c,d] \to \Omega$ two curves of class $C^1$ such that $\text{Im}(\gamma_1) = \text{Im}(\gamma_2)$. Is the statement  $$\int _{\gamma_1} F \cdot d\gamma_1 = \pm \int_{\gamma_2} F \cdot d\gamma_2$$ true or false? Justify. I didn't think in anything to prove or disprove the statement. I would like a hint, if possible. Thanks in advance!","Let $F$ be a continuous vector field in $\Omega$ and, $\gamma_1: [a,b] \to \Omega$ and $\gamma_2:[c,d] \to \Omega$ two curves of class $C^1$ such that $\text{Im}(\gamma_1) = \text{Im}(\gamma_2)$. Is the statement  $$\int _{\gamma_1} F \cdot d\gamma_1 = \pm \int_{\gamma_2} F \cdot d\gamma_2$$ true or false? Justify. I didn't think in anything to prove or disprove the statement. I would like a hint, if possible. Thanks in advance!",,"['integration', 'multivariable-calculus']"
65,"Using the method of Lagrange multipliers, find the extreme values of a function","Using the method of Lagrange multipliers, find the extreme values of a function",,"Using the method of Lagrange multipliers, find the extreme values of the function $f(x,y)= \frac{2y^3}{3} + 2x^2 +1$ on the ellipse $5x^2 + y^2 = 1/9$  . Identify the (absolute) maximal and minimal values of f taken on the ellipse. currently I have that $\nabla f(x,y)=L*\nabla g(x,y)$ where $g(x,y)=5x^2 + y^2-1/9$ this leads to: $4x-10Lx=0$, so $x=0$ or $L=2/5$ and:$2y^2-2Ly=0$, so $y=0$ or $y=L$. where do I go from here? andy help would be greatly appreciated, thank you in advance.","Using the method of Lagrange multipliers, find the extreme values of the function $f(x,y)= \frac{2y^3}{3} + 2x^2 +1$ on the ellipse $5x^2 + y^2 = 1/9$  . Identify the (absolute) maximal and minimal values of f taken on the ellipse. currently I have that $\nabla f(x,y)=L*\nabla g(x,y)$ where $g(x,y)=5x^2 + y^2-1/9$ this leads to: $4x-10Lx=0$, so $x=0$ or $L=2/5$ and:$2y^2-2Ly=0$, so $y=0$ or $y=L$. where do I go from here? andy help would be greatly appreciated, thank you in advance.",,"['multivariable-calculus', 'lagrange-multiplier']"
66,Is finding the tangent plane to a surface made any more complicated if the surface $\neq 0$?,Is finding the tangent plane to a surface made any more complicated if the surface ?,\neq 0,"So I have $x^2 + y^2 - z^2 = 4$ as my surface and the point I'm looking at is $(2,1,1)$. So if it was $0$, I'd do my partial derivatives and get the equation: $4(x - 2 ) + 2(y-1) - 2(z-1) = 0 $ right? And that's my equation. Apparently that's worth 7 marks but it certainly doesn't seem like it. If it's not 0, but 4, is my answer not just $4(x - 2 ) + 2(y-1) - 2(z-1) = 4 $ ? I'm essentially trying to teach myself multivariable calculus in 2 weeks so I apologize for seeming slow.","So I have $x^2 + y^2 - z^2 = 4$ as my surface and the point I'm looking at is $(2,1,1)$. So if it was $0$, I'd do my partial derivatives and get the equation: $4(x - 2 ) + 2(y-1) - 2(z-1) = 0 $ right? And that's my equation. Apparently that's worth 7 marks but it certainly doesn't seem like it. If it's not 0, but 4, is my answer not just $4(x - 2 ) + 2(y-1) - 2(z-1) = 4 $ ? I'm essentially trying to teach myself multivariable calculus in 2 weeks so I apologize for seeming slow.",,"['multivariable-calculus', 'derivatives']"
67,Setting up a double integral domain,Setting up a double integral domain,,How would I set up the folowing double integral $4\displaystyle \iint x^{1/2}dA$$ where the domain is the closed region bounded by $y=x$ and $y=x^2-3x$ I know the domain is  curve shaped so I would do dx cut vertically So would my integral be $$\int_0^4\int_{x^2-3x}^x dydx$$,How would I set up the folowing double integral $4\displaystyle \iint x^{1/2}dA$$ where the domain is the closed region bounded by $y=x$ and $y=x^2-3x$ I know the domain is  curve shaped so I would do dx cut vertically So would my integral be $$\int_0^4\int_{x^2-3x}^x dydx$$,,"['integration', 'multivariable-calculus', 'area']"
68,Surface integral domain to surface function...,Surface integral domain to surface function...,,"In a surface integral where the domain of the surface is its projection on a plane, how can there be a function from the projection to the surface if the area of the surface is greater than its projection? I understand that we compute |ru x rv| to scale the area, but the surface is still the image of its domain. How is this possible if the number of ""points"" increases because the area of the surface increases?","In a surface integral where the domain of the surface is its projection on a plane, how can there be a function from the projection to the surface if the area of the surface is greater than its projection? I understand that we compute |ru x rv| to scale the area, but the surface is still the image of its domain. How is this possible if the number of ""points"" increases because the area of the surface increases?",,"['calculus', 'multivariable-calculus']"
69,Extreme value Lagrange multiplier (max or min?),Extreme value Lagrange multiplier (max or min?),,"I am to determine the the range of the volume of a tetrahedron enclosed by the coordinate axes and a tangentplane on the ellipsoid $x^2 + 2y^2 + 3z^2 = 1$. The volume of the tetrahedron can be derived to be given by $$V(x,y,z) = \frac 1{36xyz}.$$ The constraint is $g(x,y,z) = x^2 + 2y^2 + 3z^2 = 1$ and using Lagrange multiplies, we can arrive at the critical points $$ \left( \pm \frac 1{\sqrt 3}, \pm \frac 1{\sqrt 6}, \pm \frac 13 \right) $$ We can calculate $$ V\left(\frac 1{\sqrt 3}, \frac 1{\sqrt 6}, \frac 13\right) = \frac {\sqrt 2}4 $$ But it turns out that this is the smallest volume the tetrahedron can assume but Lagrange's multiplier-method does not give us this information. How can we ascertain that this indeed is a minimum and not a maximum point? Also one thing I wonder is whether there is any boundary to study, to me it appears there is no boundary but perhaps I am mistaken?","I am to determine the the range of the volume of a tetrahedron enclosed by the coordinate axes and a tangentplane on the ellipsoid $x^2 + 2y^2 + 3z^2 = 1$. The volume of the tetrahedron can be derived to be given by $$V(x,y,z) = \frac 1{36xyz}.$$ The constraint is $g(x,y,z) = x^2 + 2y^2 + 3z^2 = 1$ and using Lagrange multiplies, we can arrive at the critical points $$ \left( \pm \frac 1{\sqrt 3}, \pm \frac 1{\sqrt 6}, \pm \frac 13 \right) $$ We can calculate $$ V\left(\frac 1{\sqrt 3}, \frac 1{\sqrt 6}, \frac 13\right) = \frac {\sqrt 2}4 $$ But it turns out that this is the smallest volume the tetrahedron can assume but Lagrange's multiplier-method does not give us this information. How can we ascertain that this indeed is a minimum and not a maximum point? Also one thing I wonder is whether there is any boundary to study, to me it appears there is no boundary but perhaps I am mistaken?",,"['multivariable-calculus', 'lagrange-multiplier']"
70,Finding the flux integral?,Finding the flux integral?,,"Evaluate the flux integral $FdS$  where $F=\langle5y,2z,3x\rangle$  and $S$ is the part of the plane $6x+2y+z=12$ in the first octant oriented upward. This is how I solved it but the answer is incorrect so I just was hoping someone could look at my work and tell me where I'm going wrong. $\langle5,2,3\rangle \cdot \langle6,2,1\rangle = 37$ $6x+2y=12$ I solved for $y$. So $y=6-3x$ So I took the integral of $37dydx$ where $y$ goes from $0$ to $6-3x$ and $x$ goes from $0$ to $2$. My final answer was $222$.","Evaluate the flux integral $FdS$  where $F=\langle5y,2z,3x\rangle$  and $S$ is the part of the plane $6x+2y+z=12$ in the first octant oriented upward. This is how I solved it but the answer is incorrect so I just was hoping someone could look at my work and tell me where I'm going wrong. $\langle5,2,3\rangle \cdot \langle6,2,1\rangle = 37$ $6x+2y=12$ I solved for $y$. So $y=6-3x$ So I took the integral of $37dydx$ where $y$ goes from $0$ to $6-3x$ and $x$ goes from $0$ to $2$. My final answer was $222$.",,"['integration', 'multivariable-calculus']"
71,Doubt on proof of Implicit function theorem,Doubt on proof of Implicit function theorem,,"On The second part of the proof, where it's stated that V is open as it is the inverse image of the open set $V_0$ under the continuous mapping $y \rightarrow (0, y)$. Let $\pi$ be this continuous mapping. Then, $\forall _{U_{open\text{ in }\mathbb{R}^n\times\mathbb{R}^p}} \pi^{-1}(U)$ is open in / relative to the domain of $\pi$, since $\forall_\epsilon \exists_\delta Dom(\pi)\cap B(v_1;\delta)\subset \pi^{-1}(B(v_0;\epsilon))$ with $\lim_{y\rightarrow v_1} \pi (y) = \pi(v_0)$. In this situation, is $Dom(\pi)=V$ or $Dom(\pi)=V_1$, where $V$ and $V_1$ are as defined in the image? If it's the first, then I do not understand how $V= \pi^{-1} (V_0)$ is open in / relative to $\mathbb{R}^p$, even if $V_0$ is open in $\mathbb{R}^n\times\mathbb{R}^p$... If it's the second possibility, then I understand that $\pi^{-1} (V_0)$ is open in $V_1$, and since $V_1$ is open in $\mathbb{R}^p$, $\pi^{-1} (V_0)$ is also open in $\mathbb{R}^p$. In this last possibility is that then $\pi^{-1} (V_0)\neq V$... Then how do we prove that $V$ is open in $\mathbb{R}^p$ ?","On The second part of the proof, where it's stated that V is open as it is the inverse image of the open set $V_0$ under the continuous mapping $y \rightarrow (0, y)$. Let $\pi$ be this continuous mapping. Then, $\forall _{U_{open\text{ in }\mathbb{R}^n\times\mathbb{R}^p}} \pi^{-1}(U)$ is open in / relative to the domain of $\pi$, since $\forall_\epsilon \exists_\delta Dom(\pi)\cap B(v_1;\delta)\subset \pi^{-1}(B(v_0;\epsilon))$ with $\lim_{y\rightarrow v_1} \pi (y) = \pi(v_0)$. In this situation, is $Dom(\pi)=V$ or $Dom(\pi)=V_1$, where $V$ and $V_1$ are as defined in the image? If it's the first, then I do not understand how $V= \pi^{-1} (V_0)$ is open in / relative to $\mathbb{R}^p$, even if $V_0$ is open in $\mathbb{R}^n\times\mathbb{R}^p$... If it's the second possibility, then I understand that $\pi^{-1} (V_0)$ is open in $V_1$, and since $V_1$ is open in $\mathbb{R}^p$, $\pi^{-1} (V_0)$ is also open in $\mathbb{R}^p$. In this last possibility is that then $\pi^{-1} (V_0)\neq V$... Then how do we prove that $V$ is open in $\mathbb{R}^p$ ?",,"['multivariable-calculus', 'proof-writing', 'proof-verification']"
72,What mistake am I making trying to calculate the line integral $\oint_C3xy^2dx+8x^3dy$.,What mistake am I making trying to calculate the line integral .,\oint_C3xy^2dx+8x^3dy,"Evaluate the line integral   $$\oint_C3xy^2dx+8x^3dy$$   where $C$ is the boundary of the region between the circles $x^2+y^2=1$ and $x^2+y^2=64$ having positive orientation. I actually used Green's theorem to find this. I know $r$ ranges from $1$ to $8$ and $\theta$ ranges from $0$ to $\pi$. Okay, so I found $\dfrac{d}{dx}8x^3=24x^2$, and $\dfrac{d}{dy}3xy^2=6xy$. Now I'm integrating $24x^2-6xy$. I converted it to polar coordinates and took the integral and I got $0$. However this is incorrect. What am I doing wrong?","Evaluate the line integral   $$\oint_C3xy^2dx+8x^3dy$$   where $C$ is the boundary of the region between the circles $x^2+y^2=1$ and $x^2+y^2=64$ having positive orientation. I actually used Green's theorem to find this. I know $r$ ranges from $1$ to $8$ and $\theta$ ranges from $0$ to $\pi$. Okay, so I found $\dfrac{d}{dx}8x^3=24x^2$, and $\dfrac{d}{dy}3xy^2=6xy$. Now I'm integrating $24x^2-6xy$. I converted it to polar coordinates and took the integral and I got $0$. However this is incorrect. What am I doing wrong?",,"['integration', 'multivariable-calculus', 'greens-function']"
73,"Find out if $f$ differentiable, continuous and have partial derivatives - or if it does not. Using definitions.","Find out if  differentiable, continuous and have partial derivatives - or if it does not. Using definitions.",f,"Problem Find out if the function $$f(x)=\left\{\begin{array}{l} \frac{x^3}{x^2+y^2},\:\text{if $(x,y) \not= (0,0)$;}\\  0,\:\text{if  $(x,y)=(0,0)$;} \end{array}\right.$$ is continuous, have partial derivatives, and is differentiable. Attempt my calculations (link). Question Specific questions: See the three yellow places in my calculations. General question: I tried doing this via: $C^1 \Rightarrow$ differentiable  $\Rightarrow$ partial derivatives exists  $\Rightarrow$ continuous. Starting with $C^1$ (since the test is easy) and then moving on the the other cases, using the definitions.","Problem Find out if the function $$f(x)=\left\{\begin{array}{l} \frac{x^3}{x^2+y^2},\:\text{if $(x,y) \not= (0,0)$;}\\  0,\:\text{if  $(x,y)=(0,0)$;} \end{array}\right.$$ is continuous, have partial derivatives, and is differentiable. Attempt my calculations (link). Question Specific questions: See the three yellow places in my calculations. General question: I tried doing this via: $C^1 \Rightarrow$ differentiable  $\Rightarrow$ partial derivatives exists  $\Rightarrow$ continuous. Starting with $C^1$ (since the test is easy) and then moving on the the other cases, using the definitions.",,"['limits', 'multivariable-calculus', 'partial-derivative', 'epsilon-delta']"
74,"Partial differentiability with respect to $x$ and $y$ of $\int_0^x z(s,y)ds$ where $\partial_y z \in C^0$",Partial differentiability with respect to  and  of  where,"x y \int_0^x z(s,y)ds \partial_y z \in C^0","I have the following (not accredited and not mandatory) Exercise: Problem : Let $z : \mathbb{R}^2 \to \mathbb{R}$ be continuous and in respect to its second variable partially differentiable. Define  $$f: \begin{cases} \mathbb{R}^2 & \longrightarrow \mathbb{R} \\ (x,y) & \longmapsto \displaystyle \int_0^x z(s,y)ds \end{cases} $$ and show that $f$ is differentiable in respect to $x$ and $y$ I believe I have done 'most of the work' already and confuse myself now in an unnecessary way which is most likely due that this is only my third week in multivariable Calculus and we did not have any helpful theorems yet that I can apply. Sketch (succeeded, wrt $y$) : I will only provide a rough sketch of what I did considering partially differentiating $f$ in respect to $y$, because it might be helpful to do the same with respect to $x$ which is where I am stuck. 1) Defined rectangle $R= \lbrace (x,y) \in \mathbb{R}^2 \mid a \leq x \leq b, \ c \leq y \leq d \rbrace \subset \mathbb{R}^2$ 2) Defined mapping $F: [c,d] \to \mathbb{R}, \ y \mapsto \int_0^x z(s,y)ds$ and guessed $F'(y)= \int_0^x \partial_y z(s,y)ds$ 3) Computed $ \left|F(y_0+h)-F(y_0)-\left(\int_0^x \partial_yz(s,y)ds\right)h\right|$, which I managed to show is $\leq |h|\epsilon x$ by applying the triangle equality and the mean value theorem, namely $\partial_y z(s, \gamma)h = z(s,y_0+h)-z(s,y_0)$ for $\gamma \in (y_0,y_0+h)$ Since this worked out rather nicely I thought I might as well choose the same approach to show that $f$ can be differentiated in respect to $x$, but here I struggle My approach (wrt $x$) : First I defined $$G: \begin{cases}[a,b] & \longrightarrow \mathbb{R} \\ x & \longmapsto \displaystyle \int_0^x z(s,y)ds\end{cases} $$ And my problem now clearly is that although I can guess that $G'(x)=z(x,y)-z(0,y)$ it won't help me applying the definition as above to verify this because I obtain $$ G(x_0+h)-G(x_0)-(z(x,y)-z(0,y))h=\int_0^{x_0+h}z(s,y)ds-\int_0^{x_0}z(s,y)ds-z(x,y)h+z(0,y)h \\ =\int_{x_0}^{x_0+h}z(s,y)ds-h(z(x,y)-z(0,y)) $$ which is a rather useless expression. Update : I don't understand how I could implement the fundamental theorem of Calculus into this problem because even if I hold $y$ is constant, I am dealing with  a function that I could differentiate the way as described above, for constant $y$ it would be $G'(x)=z(x,y)$. But if that is the correct derivative, shouldn't I be able to verify it using the definition of a derivative in $\mathbb{R}^n$ ?","I have the following (not accredited and not mandatory) Exercise: Problem : Let $z : \mathbb{R}^2 \to \mathbb{R}$ be continuous and in respect to its second variable partially differentiable. Define  $$f: \begin{cases} \mathbb{R}^2 & \longrightarrow \mathbb{R} \\ (x,y) & \longmapsto \displaystyle \int_0^x z(s,y)ds \end{cases} $$ and show that $f$ is differentiable in respect to $x$ and $y$ I believe I have done 'most of the work' already and confuse myself now in an unnecessary way which is most likely due that this is only my third week in multivariable Calculus and we did not have any helpful theorems yet that I can apply. Sketch (succeeded, wrt $y$) : I will only provide a rough sketch of what I did considering partially differentiating $f$ in respect to $y$, because it might be helpful to do the same with respect to $x$ which is where I am stuck. 1) Defined rectangle $R= \lbrace (x,y) \in \mathbb{R}^2 \mid a \leq x \leq b, \ c \leq y \leq d \rbrace \subset \mathbb{R}^2$ 2) Defined mapping $F: [c,d] \to \mathbb{R}, \ y \mapsto \int_0^x z(s,y)ds$ and guessed $F'(y)= \int_0^x \partial_y z(s,y)ds$ 3) Computed $ \left|F(y_0+h)-F(y_0)-\left(\int_0^x \partial_yz(s,y)ds\right)h\right|$, which I managed to show is $\leq |h|\epsilon x$ by applying the triangle equality and the mean value theorem, namely $\partial_y z(s, \gamma)h = z(s,y_0+h)-z(s,y_0)$ for $\gamma \in (y_0,y_0+h)$ Since this worked out rather nicely I thought I might as well choose the same approach to show that $f$ can be differentiated in respect to $x$, but here I struggle My approach (wrt $x$) : First I defined $$G: \begin{cases}[a,b] & \longrightarrow \mathbb{R} \\ x & \longmapsto \displaystyle \int_0^x z(s,y)ds\end{cases} $$ And my problem now clearly is that although I can guess that $G'(x)=z(x,y)-z(0,y)$ it won't help me applying the definition as above to verify this because I obtain $$ G(x_0+h)-G(x_0)-(z(x,y)-z(0,y))h=\int_0^{x_0+h}z(s,y)ds-\int_0^{x_0}z(s,y)ds-z(x,y)h+z(0,y)h \\ =\int_{x_0}^{x_0+h}z(s,y)ds-h(z(x,y)-z(0,y)) $$ which is a rather useless expression. Update : I don't understand how I could implement the fundamental theorem of Calculus into this problem because even if I hold $y$ is constant, I am dealing with  a function that I could differentiate the way as described above, for constant $y$ it would be $G'(x)=z(x,y)$. But if that is the correct derivative, shouldn't I be able to verify it using the definition of a derivative in $\mathbb{R}^n$ ?",,"['real-analysis', 'multivariable-calculus', 'proof-verification']"
75,What are $a$ and $b$ when the zeropoints of $f(z)=(a+bi)z+2-i=0$ is at $1-i$?,What are  and  when the zeropoints of  is at ?,a b f(z)=(a+bi)z+2-i=0 1-i,$f(z)=(a+bi)z+2-i$. What are the values of a and b when $1-i$ is the zeropoint of f? $f(z)=(a+bi)z+2-i=0$ $(a+bi)(1-i)+2-i=0$ $a+bi-ai-bi^2+2-i = 0$ $(a+b+2)+(-a+b-1)i=0$ I don't know what the next steps are.,$f(z)=(a+bi)z+2-i$. What are the values of a and b when $1-i$ is the zeropoint of f? $f(z)=(a+bi)z+2-i=0$ $(a+bi)(1-i)+2-i=0$ $a+bi-ai-bi^2+2-i = 0$ $(a+b+2)+(-a+b-1)i=0$ I don't know what the next steps are.,,"['algebra-precalculus', 'multivariable-calculus', 'complex-numbers']"
76,Tangent planes perpendicular at each point of intersection,Tangent planes perpendicular at each point of intersection,,"Find the set of all points $(a,b,c)$ in 3-space for which the two spheres $(x-a)^2+(y-b)^2+(z-c)^2=1$ and $x^2+y^2+z^2=1$ intersect orthogonally.( Their tangent planes should be perpendicular at each point of intersection.) If we consider the equations of the two spheres as level surfaces and take the gradients the we get - $$\nabla f_1 = 2(x-a)i+2(y-b)j+2(z-c)k;$$ $$\nabla f_2 = 2xi+2yj+2zk.$$ Since the two spheres intersect orthogonally at $(x,y,z)$, we must have $\nabla f_1 \cdot \nabla f_2=0$. This gives $$x(x-a)+y(y-a)+z(z-a)=0.$$ Can anyone suggest how to proceed from here  to obtain the set of points (a,b,c)?","Find the set of all points $(a,b,c)$ in 3-space for which the two spheres $(x-a)^2+(y-b)^2+(z-c)^2=1$ and $x^2+y^2+z^2=1$ intersect orthogonally.( Their tangent planes should be perpendicular at each point of intersection.) If we consider the equations of the two spheres as level surfaces and take the gradients the we get - $$\nabla f_1 = 2(x-a)i+2(y-b)j+2(z-c)k;$$ $$\nabla f_2 = 2xi+2yj+2zk.$$ Since the two spheres intersect orthogonally at $(x,y,z)$, we must have $\nabla f_1 \cdot \nabla f_2=0$. This gives $$x(x-a)+y(y-a)+z(z-a)=0.$$ Can anyone suggest how to proceed from here  to obtain the set of points (a,b,c)?",,"['multivariable-calculus', 'derivatives']"
77,Calculating the center of mass in spherical coordinates,Calculating the center of mass in spherical coordinates,,"So normally, to calculate the center of mass you would use a triple integral. In my particular problem, I need to calculate the center of mass of an eight of a sphere where it's density is proportional to the distance from origin. Say we want to get the x coordinate of the center of mass. The formula is something like $\frac{1}{M}\int\int\int (r^2\sin\phi)(\lambda r)(r\sin\phi\cos\theta)$ where the groups in that product are the jacobian of the spherical transformation, the density function of r and the x coordinate expressed with spherical variables. Now, I'm thinking if this is really necessary. Could I just multiply with $r$ instead of $(r\sin\phi\cos\theta)$ to get the radius of the center of mass, then do the same for $\phi$ and $\theta$, and then transform them back to $x, y, z$? This makes sense to me since what I'm basically doing it summing up all the coordinates for each point in the body with each point having different influence on the result. The way I see it, the jacobian is there to account for the fact that the transformation is much more dense around the origin, the $\lambda r$ is there to give denser areas more influence on the final center of mass. I don't really see any reason why I couldn't sum up their coordinates in spherical form. The potential problem I could see here is that if I tried to calculate the center of mass of a whole sphere, the $x,y,z$ would all be 0, but in spherical coordinates I might get unexpected results due to angles not being defined.","So normally, to calculate the center of mass you would use a triple integral. In my particular problem, I need to calculate the center of mass of an eight of a sphere where it's density is proportional to the distance from origin. Say we want to get the x coordinate of the center of mass. The formula is something like $\frac{1}{M}\int\int\int (r^2\sin\phi)(\lambda r)(r\sin\phi\cos\theta)$ where the groups in that product are the jacobian of the spherical transformation, the density function of r and the x coordinate expressed with spherical variables. Now, I'm thinking if this is really necessary. Could I just multiply with $r$ instead of $(r\sin\phi\cos\theta)$ to get the radius of the center of mass, then do the same for $\phi$ and $\theta$, and then transform them back to $x, y, z$? This makes sense to me since what I'm basically doing it summing up all the coordinates for each point in the body with each point having different influence on the result. The way I see it, the jacobian is there to account for the fact that the transformation is much more dense around the origin, the $\lambda r$ is there to give denser areas more influence on the final center of mass. I don't really see any reason why I couldn't sum up their coordinates in spherical form. The potential problem I could see here is that if I tried to calculate the center of mass of a whole sphere, the $x,y,z$ would all be 0, but in spherical coordinates I might get unexpected results due to angles not being defined.",,"['integration', 'multivariable-calculus']"
78,Gradient Vector Question?,Gradient Vector Question?,,"The temperature in some three-dimensional body is modeled by the equation $$f(x,y,z)=49-x^2-y^2-z^2$$ Find the largest rate at which the temperature is increasing when T=0. I believe this is a gradient vector question, but when I do the problem I feel like the answer I got is very incorrect. $$f(x,y,z)=<-2x,-2y,-2z>$$ since you derive for $f_x, f_y, f_z$ Since T=0 $$f(0,0,0)=<0,0,0>$$ $$||f(0,0,0)||=0$$ Am I missing something in the equation, did something wrong, or is this not even a gradient vector question and a completely different type of question? Thanks for the help in advance.","The temperature in some three-dimensional body is modeled by the equation $$f(x,y,z)=49-x^2-y^2-z^2$$ Find the largest rate at which the temperature is increasing when T=0. I believe this is a gradient vector question, but when I do the problem I feel like the answer I got is very incorrect. $$f(x,y,z)=<-2x,-2y,-2z>$$ since you derive for $f_x, f_y, f_z$ Since T=0 $$f(0,0,0)=<0,0,0>$$ $$||f(0,0,0)||=0$$ Am I missing something in the equation, did something wrong, or is this not even a gradient vector question and a completely different type of question? Thanks for the help in advance.",,"['calculus', 'multivariable-calculus']"
79,Surface Area of Two Cylinders Calculus 3,Surface Area of Two Cylinders Calculus 3,,Find the surface area of two cylinders $$y^2 + z^2 = 1$$ and $$x^2 + y^2 = 1$$ I have so far set the two equations to equal $$x= \pm z$$ and $$y= \sqrt{(1-z^2)}$$ I am a little confused on how to set up the integration problem. So far I have $$1/\sqrt{(1-z^2)}dy$$ from 0 to 1 and am not certain if that is the correct approach.,Find the surface area of two cylinders $$y^2 + z^2 = 1$$ and $$x^2 + y^2 = 1$$ I have so far set the two equations to equal $$x= \pm z$$ and $$y= \sqrt{(1-z^2)}$$ I am a little confused on how to set up the integration problem. So far I have $$1/\sqrt{(1-z^2)}dy$$ from 0 to 1 and am not certain if that is the correct approach.,,"['calculus', 'multivariable-calculus', 'surfaces']"
80,A question on gradients: $f$ must assume equal value at two points,A question on gradients:  must assume equal value at two points,f,"If $\nabla f(x,y,z)$ is always parallel to $x i+y j+z k$, show that $f$ must assume equal values at the points $(0,0,a)$ and $(0,0,-a)$.","If $\nabla f(x,y,z)$ is always parallel to $x i+y j+z k$, show that $f$ must assume equal values at the points $(0,0,a)$ and $(0,0,-a)$.",,['multivariable-calculus']
81,Application of chain rule,Application of chain rule,,"The equations $u=f(x,y),x=X(t),y=Y(t)$ define $u$ as a function of $t$, say $u=F(t)$. Compute $F'(t)$ in terms of $t$ if, $$f(x,y)=\log [(1+e^{x^2})/(1+e^{y^2})] , X(t)=e , Y(t)^t=e^{-t}.$$ From the chain rule we have - $$F'(t) = \frac{\partial f}{\partial x}X'(t)+\frac{\partial f}{\partial y}Y'(t).$$ Since $X(t)$ and $Y(t)$ are constant here and respectively equal to $e$ and $e^{-1}$, does this not mean that $F'(t)=0$?","The equations $u=f(x,y),x=X(t),y=Y(t)$ define $u$ as a function of $t$, say $u=F(t)$. Compute $F'(t)$ in terms of $t$ if, $$f(x,y)=\log [(1+e^{x^2})/(1+e^{y^2})] , X(t)=e , Y(t)^t=e^{-t}.$$ From the chain rule we have - $$F'(t) = \frac{\partial f}{\partial x}X'(t)+\frac{\partial f}{\partial y}Y'(t).$$ Since $X(t)$ and $Y(t)$ are constant here and respectively equal to $e$ and $e^{-1}$, does this not mean that $F'(t)=0$?",,"['multivariable-calculus', 'derivatives']"
82,Derivatives of component inverse functions,Derivatives of component inverse functions,,"I might have missed the point of the following questions. Anyone kindly give a suggestion? Let $f:\mathbb{R}_\mathbf{x}^3\to\mathbb{R}_\mathbf{y}^3$ and   $g:\mathbb{R}_\mathbf{y}^3\to\mathbb{R}_\mathbf{x}^3$ be   $\mathscr{C}^1$ inverse functions. Show that $$ \frac{\partial g_1}{\partial y_1}=\frac{1}{J} \frac{\partial (f_2 ,  f_3)}{\partial (x_2, x_3)},J=\frac{\partial (f_1 , f_2 ,  f_3)}{\partial (x_1 , x_2 , x_3)} $$ And obtain similar formulas for the other derivatives of the component   functions of $g$. A similar question in terms of implicit functions: If the equations $f(x,y,z)=0$, $g(x,y,z)=0$ can be solved for $y$ and   $z$ as differentiable functions of $x$, show that  \begin{eqnarray*}  \frac{dy}{dx}=\frac{1}{J}\frac{\partial(f,g)}{\partial(z,x)}, &  &  \frac{dz}{dx}=\frac{1}{J}\frac{\partial(f,g)}{\partial(x,y)},  \end{eqnarray*} where $J=\frac{\partial(f,g)}{\partial(y,z)}$. which I have just solved as below.","I might have missed the point of the following questions. Anyone kindly give a suggestion? Let $f:\mathbb{R}_\mathbf{x}^3\to\mathbb{R}_\mathbf{y}^3$ and   $g:\mathbb{R}_\mathbf{y}^3\to\mathbb{R}_\mathbf{x}^3$ be   $\mathscr{C}^1$ inverse functions. Show that $$ \frac{\partial g_1}{\partial y_1}=\frac{1}{J} \frac{\partial (f_2 ,  f_3)}{\partial (x_2, x_3)},J=\frac{\partial (f_1 , f_2 ,  f_3)}{\partial (x_1 , x_2 , x_3)} $$ And obtain similar formulas for the other derivatives of the component   functions of $g$. A similar question in terms of implicit functions: If the equations $f(x,y,z)=0$, $g(x,y,z)=0$ can be solved for $y$ and   $z$ as differentiable functions of $x$, show that  \begin{eqnarray*}  \frac{dy}{dx}=\frac{1}{J}\frac{\partial(f,g)}{\partial(z,x)}, &  &  \frac{dz}{dx}=\frac{1}{J}\frac{\partial(f,g)}{\partial(x,y)},  \end{eqnarray*} where $J=\frac{\partial(f,g)}{\partial(y,z)}$. which I have just solved as below.",,"['calculus', 'matrices', 'analysis', 'multivariable-calculus', 'inverse']"
83,Changing Variables in Multiple Integration,Changing Variables in Multiple Integration,,"On a test I had a question where the unit square $[0,1]^2$ in $u,v$-space was being transformed by something like $T(u,v)=(uv,v)$ into a new region in $x,y$-space (for a double integral). I had to find the image of the transformation, and intuitively I thought: well, clearly $$0\le uv\le 1$$ $$0\le v\le 1$$ so the image must be the unit square. Luckily I caught my error, and ended up realizing that the image was actually a triangle in the $x,y$-plane. My question is: how can one systematically go about finding the image of a transformation from $R^n\to R^n$? It's not that I don't like actually ""thinking,"" but I feel like there must be some more methodical way of going about these problems. Alternatively, is there a way to find the bounds of integration without bothering with the region itself?","On a test I had a question where the unit square $[0,1]^2$ in $u,v$-space was being transformed by something like $T(u,v)=(uv,v)$ into a new region in $x,y$-space (for a double integral). I had to find the image of the transformation, and intuitively I thought: well, clearly $$0\le uv\le 1$$ $$0\le v\le 1$$ so the image must be the unit square. Luckily I caught my error, and ended up realizing that the image was actually a triangle in the $x,y$-plane. My question is: how can one systematically go about finding the image of a transformation from $R^n\to R^n$? It's not that I don't like actually ""thinking,"" but I feel like there must be some more methodical way of going about these problems. Alternatively, is there a way to find the bounds of integration without bothering with the region itself?",,[]
84,Evaluating the surface integral?,Evaluating the surface integral?,,I have to evaluate the $\iint_S z ds$  where $S$ is the part of the of the plane $5x+3y+z=15$ that lies in the first octant. I have been working on this problem for lon capa but I keep getting the wrong answer. I just want to make sure that my integral is correct so I can figure out if my error is in my computation or if it is in the beginning of the problem. I'm taking the integral $$ \int_0^3 \int_0^{5-(5/3)x} (15-5x-3y)\sqrt{35} \ \ dy \ dx $$ Okay I first integrated with respect to y and got $$ \int_0^{5-(5/3)x}  \ 15y-5xy - \frac{3}{2}y^2 \ \ dy $$ Solving this I got $ \ \frac{75}{2}-25x-\frac{25}{6}x^2 \ . $ Then I integrated this from 0 to 3 and got $ \ \frac{-75}{2} \ (\sqrt{35}) \ . $,I have to evaluate the $\iint_S z ds$  where $S$ is the part of the of the plane $5x+3y+z=15$ that lies in the first octant. I have been working on this problem for lon capa but I keep getting the wrong answer. I just want to make sure that my integral is correct so I can figure out if my error is in my computation or if it is in the beginning of the problem. I'm taking the integral $$ \int_0^3 \int_0^{5-(5/3)x} (15-5x-3y)\sqrt{35} \ \ dy \ dx $$ Okay I first integrated with respect to y and got $$ \int_0^{5-(5/3)x}  \ 15y-5xy - \frac{3}{2}y^2 \ \ dy $$ Solving this I got $ \ \frac{75}{2}-25x-\frac{25}{6}x^2 \ . $ Then I integrated this from 0 to 3 and got $ \ \frac{-75}{2} \ (\sqrt{35}) \ . $,,['multivariable-calculus']
85,Hessian equals zero.,Hessian equals zero.,,"I'm currently just working through some maxima/minima problems, but came across one that was a bit different from the 'standard' ones. So they used the usual procedures and ended up finding that the Hessian is zero at the critical point (0,0). They set $x=y$, which resulted in $f(x,x)=-x^3$, which has an inflection point at the origin, which is the 2D version of the saddle point. I have a few questions about this. How did they 'know' to set x=y, or is this a standard technique for these problems? ie: Set $x=f(y)$ and choose some convenient $f(y)$? In a geometric sense, what does setting $x=y$ mean? I'm having trouble visualising this.","I'm currently just working through some maxima/minima problems, but came across one that was a bit different from the 'standard' ones. So they used the usual procedures and ended up finding that the Hessian is zero at the critical point (0,0). They set $x=y$, which resulted in $f(x,x)=-x^3$, which has an inflection point at the origin, which is the 2D version of the saddle point. I have a few questions about this. How did they 'know' to set x=y, or is this a standard technique for these problems? ie: Set $x=f(y)$ and choose some convenient $f(y)$? In a geometric sense, what does setting $x=y$ mean? I'm having trouble visualising this.",,['multivariable-calculus']
86,Solving problems using the *definition* of differentiability,Solving problems using the *definition* of differentiability,,"There is a problem in my textbook, that I could not solve and was not able to understand the solution to. The problem had part a, b, c, d. Only a were solved. I am out of luck. I hope, if somebody writes the solution so one of these, I might understand it. Question: Use the definition of differentiability to prove that the following functions are in fact differentiable @ each point. b. $(1+x+2y)^2 \quad @ (1,1)$ c. $e^{x+2y} \quad \quad \quad @ (2,2)$ d. $\sin(x+y) \quad @ (1,1)$","There is a problem in my textbook, that I could not solve and was not able to understand the solution to. The problem had part a, b, c, d. Only a were solved. I am out of luck. I hope, if somebody writes the solution so one of these, I might understand it. Question: Use the definition of differentiability to prove that the following functions are in fact differentiable @ each point. b. $(1+x+2y)^2 \quad @ (1,1)$ c. $e^{x+2y} \quad \quad \quad @ (2,2)$ d. $\sin(x+y) \quad @ (1,1)$",,"['calculus', 'multivariable-calculus']"
87,Find maximum on ellipsoid using implicit function theorem...again,Find maximum on ellipsoid using implicit function theorem...again,,"I feel like im drowning this site with question about implicit function theorem but I really do not understand how I can find the differential. we are given elipsoid $x^2+y^2+z^2+xy+yz-54=0$ We are asked to show that the maximal value of $z$ on the ellipsoid is at $(x,y,z)=(3,-6,9)$ using the implicit function theorem if we define $F(x,y,z)=x^2+y^2+z^2+xy+yz-54$ we will see that the gradient (differential) is: $(2x+y,2y+x+z,2z+y)$ meaning if $z \neq -\frac{y}{2}$ then we can represent $z=z(x,y)$ as a function of $x$ and $y$ according to implicit function theorem. the maximal value of $z$ will be attained when $z'(x,y)=0$ or at a point where $z=-\frac{y}{2}$ (at which case, we cant represent $z$ as a function of $x$ and $y$ .) how do we find the differential of $z$ ? I mean we want, $\frac{\partial z}{\partial x} = -\frac{F_x}{F_z} = \frac{-2x-y}{2z+y}=0$ and $\frac{\partial z}{\partial y} = -\frac{F_y}{F_z} =  \frac{-2y-x}{2z+y}=0$ That would imply $x=0,y=0$ and $z=\sqrt{54}$ . But perhaps there are higher values of $z$ where we cant express $z$ as a function...how do we find those?","I feel like im drowning this site with question about implicit function theorem but I really do not understand how I can find the differential. we are given elipsoid We are asked to show that the maximal value of on the ellipsoid is at using the implicit function theorem if we define we will see that the gradient (differential) is: meaning if then we can represent as a function of and according to implicit function theorem. the maximal value of will be attained when or at a point where (at which case, we cant represent as a function of and .) how do we find the differential of ? I mean we want, and That would imply and . But perhaps there are higher values of where we cant express as a function...how do we find those?","x^2+y^2+z^2+xy+yz-54=0 z (x,y,z)=(3,-6,9) F(x,y,z)=x^2+y^2+z^2+xy+yz-54 (2x+y,2y+x+z,2z+y) z \neq -\frac{y}{2} z=z(x,y) x y z z'(x,y)=0 z=-\frac{y}{2} z x y z \frac{\partial z}{\partial x} = -\frac{F_x}{F_z} = \frac{-2x-y}{2z+y}=0 \frac{\partial z}{\partial y} = -\frac{F_y}{F_z} =  \frac{-2y-x}{2z+y}=0 x=0,y=0 z=\sqrt{54} z z","['calculus', 'multivariable-calculus', 'implicit-function-theorem', 'ellipsoids']"
88,Polar coordinates for $xz$-plane: $z = r\sin\theta$ ? [Stewart P1091 16.7.25],Polar coordinates for -plane:  ? [Stewart P1091 16.7.25],xz z = r\sin\theta,"$1.$ The unit disk is projected onto the xz-plane, so shouldn’t $x = 1\cos \theta$ and $\color{red}{z = 1 \sin \theta} $? User Semsem below kindly identified the problem: The normal to the disk is on the direction $-j$ so we have to reverse the orientation as follows. $2.$ Would someone please explain why the orientation must be reversed? By ""reverse"", does Semsem mean the following, that thee $xz$-plane should be viewed in the direction of the green arrow (instead in that of the red arrow, which was my problem)?","$1.$ The unit disk is projected onto the xz-plane, so shouldn’t $x = 1\cos \theta$ and $\color{red}{z = 1 \sin \theta} $? User Semsem below kindly identified the problem: The normal to the disk is on the direction $-j$ so we have to reverse the orientation as follows. $2.$ Would someone please explain why the orientation must be reversed? By ""reverse"", does Semsem mean the following, that thee $xz$-plane should be viewed in the direction of the green arrow (instead in that of the red arrow, which was my problem)?",,"['multivariable-calculus', 'proof-verification']"
89,When does [the distance from origin to some point in space] vary and when is it fixed?,When does [the distance from origin to some point in space] vary and when is it fixed?,,"As defined in spherical coordinates, $p =$ the distance from the origin to a point P in space. In Stewart P1092 16.7.47 (below), $p$ is fixed by the solution to be $a$. Yet in Stewart P1103 16.9.13, $p$ is a variable. How does one determine whether $p$ varies or is fixed? I realise that in 13, the Divergence Theorem effects a triple integral so 3 variables must be integrated. In the interest of spherical coordinates, $\phi \, \& \, \theta$ are used, so $p$ may fit. This is desultory guesswork, so I'd like to learn a more definitive explanation.","As defined in spherical coordinates, $p =$ the distance from the origin to a point P in space. In Stewart P1092 16.7.47 (below), $p$ is fixed by the solution to be $a$. Yet in Stewart P1103 16.9.13, $p$ is a variable. How does one determine whether $p$ varies or is fixed? I realise that in 13, the Divergence Theorem effects a triple integral so 3 variables must be integrated. In the interest of spherical coordinates, $\phi \, \& \, \theta$ are used, so $p$ may fit. This is desultory guesswork, so I'd like to learn a more definitive explanation.",,[]
90,Does the inverse of a function from $\mathbb{R}^n$ to $\mathbb{R}^m$ exist?,Does the inverse of a function from  to  exist?,\mathbb{R}^n \mathbb{R}^m,"Suppose $T:\mathbb{R}^2 \to \mathbb{R}^3$ is defined by $T(x,y) = (x,y,xy).$  What then is the inverse of $T$?  Is it $$T^{-1}: \mathbb{R}^3 \to \mathbb{R}^2; (a,b,c) \mapsto (a,b)?$$  It is clear that, in this case, $T^{-1}(T(x,y)) = (x,y).$ But then $$T(T^{-1}(x,y,z)) = (x, y, xy) \neq (x,y,z).$$  It doesn't seem that this function has an inverse.  This problem seems to extend to any map from $\mathbb{R}^n \to \mathbb{R}^m$ when $n \neq m.$  When are such functions invertible?","Suppose $T:\mathbb{R}^2 \to \mathbb{R}^3$ is defined by $T(x,y) = (x,y,xy).$  What then is the inverse of $T$?  Is it $$T^{-1}: \mathbb{R}^3 \to \mathbb{R}^2; (a,b,c) \mapsto (a,b)?$$  It is clear that, in this case, $T^{-1}(T(x,y)) = (x,y).$ But then $$T(T^{-1}(x,y,z)) = (x, y, xy) \neq (x,y,z).$$  It doesn't seem that this function has an inverse.  This problem seems to extend to any map from $\mathbb{R}^n \to \mathbb{R}^m$ when $n \neq m.$  When are such functions invertible?",,['multivariable-calculus']
91,Show that $\textbf F$ is a gradient field by giving a scalar function $f$ on $\Omega^+$ such that $\nabla f=\textbf F$.,Show that  is a gradient field by giving a scalar function  on  such that .,\textbf F f \Omega^+ \nabla f=\textbf F,"Show that $\textbf F$ is a gradient field by giving a scalar function $f$ on $\Omega^+$ such that $\nabla f=\textbf F$. $\textbf F = (\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2},0)$,  $\Omega^+ = \{(x,y)|x,y>0\}$. My attempt: We must have that $\frac{\partial f}{\partial x} = \frac{-y}{x^2+y^2}$, so I obtained that $f = -arctan(x/y)+g(y,z)$. Also $\frac{\partial f}{\partial y} = \frac{x}{x^2+y^2}$, so I got that $f = arctan(y/x)+h(x,z)$. And $\frac{\partial f}{\partial z} = 0$ tells us that $f$ is in terms of $x$ and $y$. But now I am stuck. How is it possible that $-arctan(x/y)+g(y,z)=arctan(y/x)+h(x,z)$?","Show that $\textbf F$ is a gradient field by giving a scalar function $f$ on $\Omega^+$ such that $\nabla f=\textbf F$. $\textbf F = (\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2},0)$,  $\Omega^+ = \{(x,y)|x,y>0\}$. My attempt: We must have that $\frac{\partial f}{\partial x} = \frac{-y}{x^2+y^2}$, so I obtained that $f = -arctan(x/y)+g(y,z)$. Also $\frac{\partial f}{\partial y} = \frac{x}{x^2+y^2}$, so I got that $f = arctan(y/x)+h(x,z)$. And $\frac{\partial f}{\partial z} = 0$ tells us that $f$ is in terms of $x$ and $y$. But now I am stuck. How is it possible that $-arctan(x/y)+g(y,z)=arctan(y/x)+h(x,z)$?",,['multivariable-calculus']
92,vector fields and closed curves.,vector fields and closed curves.,,"The vector field $\vec F(\vec R)$ is defined by $$\vec F(\vec R) = \oint_C \|\vec r − \vec R\|^2 d\vec r$$ where $\vec r$ lies on the simple closed curve $C$. Show that there are constant vectors $\vec A$ and $\vec B$ such that $\vec F(\vec R) = \vec R \times \vec A + \vec B$. Deduce that $\nabla\times \vec F = -4\iint_S d\vec S $, Where $S$ is any smooth surface spanning $C$.","The vector field $\vec F(\vec R)$ is defined by $$\vec F(\vec R) = \oint_C \|\vec r − \vec R\|^2 d\vec r$$ where $\vec r$ lies on the simple closed curve $C$. Show that there are constant vectors $\vec A$ and $\vec B$ such that $\vec F(\vec R) = \vec R \times \vec A + \vec B$. Deduce that $\nabla\times \vec F = -4\iint_S d\vec S $, Where $S$ is any smooth surface spanning $C$.",,['multivariable-calculus']
93,characteristic curves tangent and gradient,characteristic curves tangent and gradient,,"In the PDE $aU_x+bU_y = 0$.This is equivalent to  $\frac a{\sqrt {a^2+b^2}}U_x+ \frac b{\sqrt {a^2+b^2}}U_y = 0$. $\langle\frac a{\sqrt {a^2+b^2}}, \frac b{\sqrt {a^2+b^2}}\rangle$.$\nabla U=0$. Then I can say that this is the directional derivative of U in the direction of $\widehat {\langle a,b \rangle}$. This implies that the rate of change of $U$ in the direction of  $\widehat{\langle a,b \rangle }$=0.That is we are on a level surface . Since the gradient is perpendicular to the level surface and the dot product is zero, tangent to the level curve at that point is orthogonal to the gradient. Is there a relationship between the tangent to the level surface and the direction of the directional derivative? Because in characteristic curves method I have seen that it says that   ""The directonal derivative in the direction of the vector $\langle a,b \rangle$ is zero. The curves on the xy plane with $\langle a,b \rangle$ as tangent vectors have slopes$\frac ba.$ Thus $\frac {dy}{dx}=\frac ba$ "".Please explain the relationship of directional derivative and its tangent vector . In the PDE $aU_x+bU_y+cU = 0$ .This also has the directional derivative in the direction $\widehat {\langle a,b \rangle}$. But here  the rate of change of U in the direction of  $\widehat{\langle a,b\rangle}$ is not zero. That is we are not in  a level surface.  Hence can I say that the $\nabla U $ is perpendicular to the surface and the tangent at each point is in the direction $\langle a,b \rangle$. I don't understand the relationship of characteristic curves have with the gradient and the tangent plane.","In the PDE $aU_x+bU_y = 0$.This is equivalent to  $\frac a{\sqrt {a^2+b^2}}U_x+ \frac b{\sqrt {a^2+b^2}}U_y = 0$. $\langle\frac a{\sqrt {a^2+b^2}}, \frac b{\sqrt {a^2+b^2}}\rangle$.$\nabla U=0$. Then I can say that this is the directional derivative of U in the direction of $\widehat {\langle a,b \rangle}$. This implies that the rate of change of $U$ in the direction of  $\widehat{\langle a,b \rangle }$=0.That is we are on a level surface . Since the gradient is perpendicular to the level surface and the dot product is zero, tangent to the level curve at that point is orthogonal to the gradient. Is there a relationship between the tangent to the level surface and the direction of the directional derivative? Because in characteristic curves method I have seen that it says that   ""The directonal derivative in the direction of the vector $\langle a,b \rangle$ is zero. The curves on the xy plane with $\langle a,b \rangle$ as tangent vectors have slopes$\frac ba.$ Thus $\frac {dy}{dx}=\frac ba$ "".Please explain the relationship of directional derivative and its tangent vector . In the PDE $aU_x+bU_y+cU = 0$ .This also has the directional derivative in the direction $\widehat {\langle a,b \rangle}$. But here  the rate of change of U in the direction of  $\widehat{\langle a,b\rangle}$ is not zero. That is we are not in  a level surface.  Hence can I say that the $\nabla U $ is perpendicular to the surface and the tangent at each point is in the direction $\langle a,b \rangle$. I don't understand the relationship of characteristic curves have with the gradient and the tangent plane.",,"['multivariable-calculus', 'partial-differential-equations']"
94,Very basic questions on chain rules and product rules,Very basic questions on chain rules and product rules,,"I have serious gaps in maths and would like to ask some basic questions. I know there is the following chain rule for the first derivative:  $$ Dh(x) = Dg(f(x))Df(x)\quad\quad (1) $$ where $h(x) = g(f(x))$ with $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$. There also is the following rule for the second derivative: $$ \nabla^2h(x) = g'(f(x)) \nabla^2 f(x) + g''(f(x)) \nabla f(x)\nabla f(x)^\top, \quad\quad (2) $$ but restricted to $f:\mathbb{R}^n \to \mathbb{R}$ and $g:\mathbb{R} \to \mathbb{R}$. My question: is there any chain rule for the second derivative in the general case: $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$? I tried to prove $(2)$ using $\nabla^2h(x) = D\nabla h(x)$: From $(1)$ we have $\nabla h(x)= g'(f(x)) \nabla f(x)$. Now we need to calculate the derivative of a product, which implies my second question: is there a product rule for $D(fg)(x)$ where $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^n \to \mathbb{R}$? Is there a (dot) product rule for $D(f\cdot g)(x)$ where $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^n \to \mathbb{R}^m$? Could you please suggest a textbook with complete theory (theorems and proofs) and many examples and/or problems with solutions (on calculating the derivative)? Thank you so much!","I have serious gaps in maths and would like to ask some basic questions. I know there is the following chain rule for the first derivative:  $$ Dh(x) = Dg(f(x))Df(x)\quad\quad (1) $$ where $h(x) = g(f(x))$ with $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$. There also is the following rule for the second derivative: $$ \nabla^2h(x) = g'(f(x)) \nabla^2 f(x) + g''(f(x)) \nabla f(x)\nabla f(x)^\top, \quad\quad (2) $$ but restricted to $f:\mathbb{R}^n \to \mathbb{R}$ and $g:\mathbb{R} \to \mathbb{R}$. My question: is there any chain rule for the second derivative in the general case: $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$? I tried to prove $(2)$ using $\nabla^2h(x) = D\nabla h(x)$: From $(1)$ we have $\nabla h(x)= g'(f(x)) \nabla f(x)$. Now we need to calculate the derivative of a product, which implies my second question: is there a product rule for $D(fg)(x)$ where $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^n \to \mathbb{R}$? Is there a (dot) product rule for $D(f\cdot g)(x)$ where $f:\mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^n \to \mathbb{R}^m$? Could you please suggest a textbook with complete theory (theorems and proofs) and many examples and/or problems with solutions (on calculating the derivative)? Thank you so much!",,"['calculus', 'real-analysis', 'multivariable-calculus', 'differential-geometry', 'derivatives']"
95,Application of Taylor's Formula,Application of Taylor's Formula,,"If we are given that $f''(x) = f(x)$, how do we show that there exist constants $a$ and $b$ such that $f(x) = ae^x + be^{-x}$ for all $x$? A hint is given: We can define another function $g$ by $g(x) = f(x) - ae^x - be^{-x}$, and choose constants $a$ and $b$ such that $g(0) = g'(0) = 0$. I don't really understand this hint, though... could anyone explain?","If we are given that $f''(x) = f(x)$, how do we show that there exist constants $a$ and $b$ such that $f(x) = ae^x + be^{-x}$ for all $x$? A hint is given: We can define another function $g$ by $g(x) = f(x) - ae^x - be^{-x}$, and choose constants $a$ and $b$ such that $g(0) = g'(0) = 0$. I don't really understand this hint, though... could anyone explain?",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
96,Two Multivariable Limits help!,Two Multivariable Limits help!,,"Will someone please help me understand the following? $\lim \limits_{(x,y) \to (0,0)} \dfrac{\tan y \cdot \sin^2(x-7y)}{x^2+y^2} $ which is the expression I get after doing the substitution $x-7=x , y-1=y $ to the expression: $\lim \limits_{(x,y) \to (7,1)} \dfrac{\tan(y-1) \cdot \sin^2(x-7y)}{(x-7)^2+(y-1)^2} $. I tried dividing and and multiplying by the same expression to get the limit of $ \frac{y(x-7y)^2}{x^2+y^2} $ but I am not sure how to calculate this limit. $\lim \limits_{(x,y) \to (0,0)} \dfrac{\sqrt{1+2xy}+7xy-1}{\sin(xy)} $ . After multiplying the entire expression by $\dfrac{ \sqrt{1+2xy} -7xy+1}{ \sqrt{1+2xy} -7xy+1 }  $ I get $\lim \limits_{(x,y) \to (0,0)} \dfrac{1+2xy-(7xy-1)^2}{\sin(xy) (\sqrt{1+2xy} -7xy+1) }$  and I don't think it gives me something. Will you please help me? Thanks a lot","Will someone please help me understand the following? $\lim \limits_{(x,y) \to (0,0)} \dfrac{\tan y \cdot \sin^2(x-7y)}{x^2+y^2} $ which is the expression I get after doing the substitution $x-7=x , y-1=y $ to the expression: $\lim \limits_{(x,y) \to (7,1)} \dfrac{\tan(y-1) \cdot \sin^2(x-7y)}{(x-7)^2+(y-1)^2} $. I tried dividing and and multiplying by the same expression to get the limit of $ \frac{y(x-7y)^2}{x^2+y^2} $ but I am not sure how to calculate this limit. $\lim \limits_{(x,y) \to (0,0)} \dfrac{\sqrt{1+2xy}+7xy-1}{\sin(xy)} $ . After multiplying the entire expression by $\dfrac{ \sqrt{1+2xy} -7xy+1}{ \sqrt{1+2xy} -7xy+1 }  $ I get $\lim \limits_{(x,y) \to (0,0)} \dfrac{1+2xy-(7xy-1)^2}{\sin(xy) (\sqrt{1+2xy} -7xy+1) }$  and I don't think it gives me something. Will you please help me? Thanks a lot",,"['limits', 'multivariable-calculus']"
97,Tricky Surface Parametrization,Tricky Surface Parametrization,,"I am to parametrize the surface given by the ellipse $$9(z-1)^2 + x^2 = 1$$ in the $xz$-plane and rotated about the $x$-axis. I then have to find the volume of the region enclosed. The concept of ""rotated about the $x$-axis"" is causing me some difficulty. I have come up with $$x = cos\theta$$$$z = (\frac1 3 sin\theta + 1)sin\phi$$ Which I am not even sure is right, and then the best I can get for $y$ is $$y = zcos\phi$$ There is a 3d render of it https://i.sstatic.net/rsWxJ.png Any help would be appreciated.","I am to parametrize the surface given by the ellipse $$9(z-1)^2 + x^2 = 1$$ in the $xz$-plane and rotated about the $x$-axis. I then have to find the volume of the region enclosed. The concept of ""rotated about the $x$-axis"" is causing me some difficulty. I have come up with $$x = cos\theta$$$$z = (\frac1 3 sin\theta + 1)sin\phi$$ Which I am not even sure is right, and then the best I can get for $y$ is $$y = zcos\phi$$ There is a 3d render of it https://i.sstatic.net/rsWxJ.png Any help would be appreciated.",,['multivariable-calculus']
98,"Is $f$ differentiable at $(x,y)$?",Is  differentiable at ?,"f (x,y)","I am working on a practice question, and I am not sure if what I have done would be considered, 'complete justification'. I would greatly appreciate some feedback or helpful advice on how it could be better etc. The question is here: Let $f: \mathbb {R}^2 \to \mathbb{R} $ be a function defined by: $$ \  f(x,y) =   \begin{cases}    \frac{\sin(x^2 + y^2)}{x^2 + y^2} & \text{if } (x,y) \ne (0,0) \\    1       & \text{if } (x,y) = (0,0)   \end{cases} $$ Is $f$ diff'ble at $(x,y) = (0,0)$? Here is what I have: By definition, $f(x,y)$ is diff'ble at $(0,0)$ if $$ \lim_{(x,y) \to (0,0)} f(x,y)= \frac{f(x,y) - [f(0,0) + f_{x}(0,0)(x-0) + f_{y}(0,0)(y-0)]}{\sqrt{x^2 + y^2}} = 0\tag{*}$$ Since $f(x,y)$ is piecewise, $f_{x}(0,0)$ and $f_{y}(0,0)$ is derived from 1st principles: So, $$\begin{align}f_{x}(0,0) &= \lim_{h \to 0}  \frac{\frac{\sin((0+h)^2 + (0)^2)}{(0+h)^2 + (0)^2} - f_{x}(0,0)}{h} \\ &=  \lim_{h \to 0} \frac{\frac{\sin(h^2)}{h^2} - 1}{h}\\ &=  \lim_{h \to 0}  \frac{\sin(h^2) - h^2}{h^3} \\ &=  \lim_{h \to 0}  \frac{2h \cos(h^2) - 2h}{3h^2}\end{align} $$ by L'Hopital's rule, apply this twice more I can see that the limit is $0$. Similarly, $f_y(0,0)$ is derived the same way. Then from $(*)$, I have: $$f_{x}(0,0) = \lim_{h \to 0}  \frac{\frac{\sin(x^2 + y^2)}{x^2 + y^2} - 1}{\sqrt{x^2 + y^2}} $$ From here I haven't had luck trying to get this to $0$. So instead, I try to make it easier. Since $f_x(0,0)$ and $f_y(0,0)$ exist we must evaluate whether $f_x(x,y)$ is continuous at $(x,y)=(0,0)$ i.e. if $$\lim_{(x,y) \to (0,0)}  \frac{\sin(x^2 + y^2)}{x^2 + y^2} = 1 $$ then  $f(x,y)$ is differentiable at $(0,0)$. So letting $u = x^2 + y^2$, $$\lim_{(x,y) \to (0,0)}  \frac{\sin(x^2 + y^2)}{x^2 + y^2} = \lim_{ (x,y) \to (0,0)}  \frac{\sin(u)}{u} = 1 $$ by L'Hopital once more. Therefore, $f$ is differentiable at (0,0). I feel like I am perhaps not completely justifying this, because I take a short cut - but is it valid? Should I perhaps use a epsilon-delta proof? Or is there a way to work with that tricky limit I have in $(*)$? Many thanks for the help in advance!","I am working on a practice question, and I am not sure if what I have done would be considered, 'complete justification'. I would greatly appreciate some feedback or helpful advice on how it could be better etc. The question is here: Let $f: \mathbb {R}^2 \to \mathbb{R} $ be a function defined by: $$ \  f(x,y) =   \begin{cases}    \frac{\sin(x^2 + y^2)}{x^2 + y^2} & \text{if } (x,y) \ne (0,0) \\    1       & \text{if } (x,y) = (0,0)   \end{cases} $$ Is $f$ diff'ble at $(x,y) = (0,0)$? Here is what I have: By definition, $f(x,y)$ is diff'ble at $(0,0)$ if $$ \lim_{(x,y) \to (0,0)} f(x,y)= \frac{f(x,y) - [f(0,0) + f_{x}(0,0)(x-0) + f_{y}(0,0)(y-0)]}{\sqrt{x^2 + y^2}} = 0\tag{*}$$ Since $f(x,y)$ is piecewise, $f_{x}(0,0)$ and $f_{y}(0,0)$ is derived from 1st principles: So, $$\begin{align}f_{x}(0,0) &= \lim_{h \to 0}  \frac{\frac{\sin((0+h)^2 + (0)^2)}{(0+h)^2 + (0)^2} - f_{x}(0,0)}{h} \\ &=  \lim_{h \to 0} \frac{\frac{\sin(h^2)}{h^2} - 1}{h}\\ &=  \lim_{h \to 0}  \frac{\sin(h^2) - h^2}{h^3} \\ &=  \lim_{h \to 0}  \frac{2h \cos(h^2) - 2h}{3h^2}\end{align} $$ by L'Hopital's rule, apply this twice more I can see that the limit is $0$. Similarly, $f_y(0,0)$ is derived the same way. Then from $(*)$, I have: $$f_{x}(0,0) = \lim_{h \to 0}  \frac{\frac{\sin(x^2 + y^2)}{x^2 + y^2} - 1}{\sqrt{x^2 + y^2}} $$ From here I haven't had luck trying to get this to $0$. So instead, I try to make it easier. Since $f_x(0,0)$ and $f_y(0,0)$ exist we must evaluate whether $f_x(x,y)$ is continuous at $(x,y)=(0,0)$ i.e. if $$\lim_{(x,y) \to (0,0)}  \frac{\sin(x^2 + y^2)}{x^2 + y^2} = 1 $$ then  $f(x,y)$ is differentiable at $(0,0)$. So letting $u = x^2 + y^2$, $$\lim_{(x,y) \to (0,0)}  \frac{\sin(x^2 + y^2)}{x^2 + y^2} = \lim_{ (x,y) \to (0,0)}  \frac{\sin(u)}{u} = 1 $$ by L'Hopital once more. Therefore, $f$ is differentiable at (0,0). I feel like I am perhaps not completely justifying this, because I take a short cut - but is it valid? Should I perhaps use a epsilon-delta proof? Or is there a way to work with that tricky limit I have in $(*)$? Many thanks for the help in advance!",,['multivariable-calculus']
99,Show that the intersection of a plane...,Show that the intersection of a plane...,,"Show that the intersection of the plane $z = 2y$ with the elliptic cylinder $\frac{x^2}{5} + y^2 = 1$ is a circle. Find the radius and center of this circle. Hint: How can one describe a circle in 3D? At this point, all I have put together is the equation of a cylinder is $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ but I don't see a $z$ in the equation, unless I'm supposed to incorporate it by using $z = 2y$ somehow. If I did that, would I just go $z = 2y$, so $\frac{z}{2} = y$, so $\frac{x^2}{5} + y + \frac{z}{2} = 1$? Now, I realize that it's the equation of an ellipsoid, but I'm trying to describe it in 3D like the hint suggests.","Show that the intersection of the plane $z = 2y$ with the elliptic cylinder $\frac{x^2}{5} + y^2 = 1$ is a circle. Find the radius and center of this circle. Hint: How can one describe a circle in 3D? At this point, all I have put together is the equation of a cylinder is $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ but I don't see a $z$ in the equation, unless I'm supposed to incorporate it by using $z = 2y$ somehow. If I did that, would I just go $z = 2y$, so $\frac{z}{2} = y$, so $\frac{x^2}{5} + y + \frac{z}{2} = 1$? Now, I realize that it's the equation of an ellipsoid, but I'm trying to describe it in 3D like the hint suggests.",,"['multivariable-calculus', 'conic-sections']"
