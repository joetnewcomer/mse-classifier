,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sublinear function $f$ with $f(x)+f(-x)=0$ for some $x \neq 0$ is linear.,Sublinear function  with  for some  is linear.,f f(x)+f(-x)=0 x \neq 0,"A sublinear function $f$ has the following properties: $f(x+y) \leq f(x) + f(y)\;\; \forall x,y \in \mathbb R^d$ for any $t > 0, x \in\mathbb R^d, f(tx) = tf(x)$ Now let $f(x_0) + f(-x_0) = 0$ at some point $x_0 \neq 0$ , I want to prove that $f$ is actually linear. I have proved $f(0)=0$ by simply let $x = y = 0$ and $x = x_0, y= -x_0$ , but I don't know how to proceed. Could someone help me?","A sublinear function has the following properties: for any Now let at some point , I want to prove that is actually linear. I have proved by simply let and , but I don't know how to proceed. Could someone help me?","f f(x+y) \leq f(x) + f(y)\;\; \forall x,y \in \mathbb R^d t > 0, x \in\mathbb R^d, f(tx) = tf(x) f(x_0) + f(-x_0) = 0 x_0 \neq 0 f f(0)=0 x = y = 0 x = x_0, y= -x_0","['real-analysis', 'linear-algebra', 'convex-analysis', 'convex-optimization']"
1,Any relation between $|\mathbf{a}\times\mathbf{b}|^{2}+|\mathbf{a}\cdot\mathbf{b}|^{2}=|\mathbf{a}|^{2}|\mathbf{b}|^{2}$ and Pythagoras' Theorem?,Any relation between  and Pythagoras' Theorem?,|\mathbf{a}\times\mathbf{b}|^{2}+|\mathbf{a}\cdot\mathbf{b}|^{2}=|\mathbf{a}|^{2}|\mathbf{b}|^{2},"With vectors, we have this result : $$\left|\mathbf{a}\times\mathbf{b}\right|^{2}+\left|\mathbf{a}\cdot\mathbf{b}\right|^{2}=\left|\mathbf{a}\right|^{2}\left|\mathbf{b}\right|^{2}$$ (This result also works in the 2D case.) It looks similar to Pythagoras' Theorem so I was wondering if there might indeed be any relation (or if it's just a coincidence). Definitions used: In 3D case, let $\mathbf{a}=(a_1,a_2,a_3)$ and $\mathbf{b}=(b_1,b_2,b_3)$ . Then $\mathbf{a}\times\mathbf{b}=(a_2b_3-a_3b_2,a_3b_1-a_1b_3,a_1b_2-a_2b_1)$ , $\mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2+a_3b_3$ , $|\mathbf{a}|=\sqrt{a_1^2 +a_2^2 +a_3^2}$ . In 2D case, let $\mathbf{a}=(a_1,a_2)$ and $\mathbf{b}=(b_1,b_2)$ . Then $\mathbf{a}\times\mathbf{b}=a_1b_2-a_2b_1$ , $\mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2$ , $|\mathbf{a}|=\sqrt{a_1^2 +a_2^2}$ . Pythagoras' Theorem: If $\mathbf{a}\cdot\mathbf{b}=0$ , then $|\mathbf{a}|^2+|\mathbf{b}|^2=|\mathbf{a}+\mathbf{b}|^2$ .","With vectors, we have this result : (This result also works in the 2D case.) It looks similar to Pythagoras' Theorem so I was wondering if there might indeed be any relation (or if it's just a coincidence). Definitions used: In 3D case, let and . Then , , . In 2D case, let and . Then , , . Pythagoras' Theorem: If , then .","\left|\mathbf{a}\times\mathbf{b}\right|^{2}+\left|\mathbf{a}\cdot\mathbf{b}\right|^{2}=\left|\mathbf{a}\right|^{2}\left|\mathbf{b}\right|^{2} \mathbf{a}=(a_1,a_2,a_3) \mathbf{b}=(b_1,b_2,b_3) \mathbf{a}\times\mathbf{b}=(a_2b_3-a_3b_2,a_3b_1-a_1b_3,a_1b_2-a_2b_1) \mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2+a_3b_3 |\mathbf{a}|=\sqrt{a_1^2 +a_2^2 +a_3^2} \mathbf{a}=(a_1,a_2) \mathbf{b}=(b_1,b_2) \mathbf{a}\times\mathbf{b}=a_1b_2-a_2b_1 \mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2 |\mathbf{a}|=\sqrt{a_1^2 +a_2^2} \mathbf{a}\cdot\mathbf{b}=0 |\mathbf{a}|^2+|\mathbf{b}|^2=|\mathbf{a}+\mathbf{b}|^2",['linear-algebra']
2,Why is span defined as the linear combination with *finite* terms?,Why is span defined as the linear combination with *finite* terms?,,"I understand that, for a vector space $V$ with scalar field $K$ , the linear span of a family of vectors $S\subseteq V$ is usually defined as $$ \mathrm{sp}(S):=\left\{ \sum_{i=1}^k a_iv_i \mid k\in \mathbb{N},  a_i \in K, v_i\in S\right\} $$ In words: the span is the set of finite linear combinations of the vectors in $S$ . My question is about the motivation for defining the span to be a finite linear combination. Is there some uncomfortable conclusions  that would result from using a definition like $$ \mathrm{sp}(S):= \left\{ \sum_{v\in S} a_v v \mid a_v \in K \right\}$$ I suspect that this has something to do with preserving important structures of $S$ in the span of $S$ (perhaps topological structures?), but I have no concrete examples.","I understand that, for a vector space with scalar field , the linear span of a family of vectors is usually defined as In words: the span is the set of finite linear combinations of the vectors in . My question is about the motivation for defining the span to be a finite linear combination. Is there some uncomfortable conclusions  that would result from using a definition like I suspect that this has something to do with preserving important structures of in the span of (perhaps topological structures?), but I have no concrete examples.","V K S\subseteq V  \mathrm{sp}(S):=\left\{ \sum_{i=1}^k a_iv_i \mid k\in \mathbb{N},  a_i \in K, v_i\in S\right\}  S  \mathrm{sp}(S):= \left\{ \sum_{v\in S} a_v v \mid a_v \in K \right\} S S","['linear-algebra', 'definition', 'topological-vector-spaces', 'motivation']"
3,Prove $ A^2 = 2A $,Prove, A^2 = 2A ,"I have the following question in my linear algebra textbook: Let $ A, B \in M_{n \times n}(\mathbb{F}) $ such that $ A(B - I) = A $ and $ B(A - I) = B $ . Prove that $ A^2 = 2A $ . Here is what I have so far: $$ A(B - I) = A \Rightarrow AB - A = A \Rightarrow AB = 2A $$ I understand now that I have to prove $ A^2 = AB $ , but I have no idea how to continue, could you please give me a clue?","I have the following question in my linear algebra textbook: Let such that and . Prove that . Here is what I have so far: I understand now that I have to prove , but I have no idea how to continue, could you please give me a clue?"," A, B \in M_{n \times n}(\mathbb{F})   A(B - I) = A   B(A - I) = B   A^2 = 2A   A(B - I) = A \Rightarrow AB - A = A \Rightarrow AB = 2A   A^2 = AB ","['linear-algebra', 'matrices']"
4,Prove that Kernel(T)=Kernel(T²) if and only if the intersection between Kernel and Im(T) =0,Prove that Kernel(T)=Kernel(T²) if and only if the intersection between Kernel and Im(T) =0,,"I was working on this problem: If you have a linear transformation $T: V \to V$ ( $V$ is a vectorial space) then: a) $\ker(T)$ is included in $\ker\left(T^2 \right)$ And: b) $\ker(T) = \ker\left(T^2 \right) $ if and only if $\ker(T) \cap \text{im}(T) = \{ 0 \}$ . I proved a) saying that if $v \in \ker(T)$ then, $T(v) = 0$ . But then $T^2(v)= T(T(v))= T(0)$ which is equal to $0$ (because is a linear transformation), so we can say that $v \in \ker\left(T^2 \right)$ , and $\ker(T)$ is included in $\ker\left(T^2 \right)$ . I'm not sure if that's right. But in part b) I'm a little confused, I was trying to prove it assuming that it's not true and then getting a contradiction, but I'm not sure about how to use: "" $\ker(T) \cap \text{im}(T) = \{0\}$ "".","I was working on this problem: If you have a linear transformation ( is a vectorial space) then: a) is included in And: b) if and only if . I proved a) saying that if then, . But then which is equal to (because is a linear transformation), so we can say that , and is included in . I'm not sure if that's right. But in part b) I'm a little confused, I was trying to prove it assuming that it's not true and then getting a contradiction, but I'm not sure about how to use: "" "".",T: V \to V V \ker(T) \ker\left(T^2 \right) \ker(T) = \ker\left(T^2 \right)  \ker(T) \cap \text{im}(T) = \{ 0 \} v \in \ker(T) T(v) = 0 T^2(v)= T(T(v))= T(0) 0 v \in \ker\left(T^2 \right) \ker(T) \ker\left(T^2 \right) \ker(T) \cap \text{im}(T) = \{0\},"['linear-algebra', 'linear-transformations']"
5,Check if the applications defined below are linear transformations:,Check if the applications defined below are linear transformations:,,"Check if the applications defined below are linear transformations: a) $T: \mathbb R^2 \to \mathbb R^2$ , $T(x_1, x_2) = (x_1 – 1, x_2).$ b) $T: \mathbb R^2 \to \mathbb R^2$ , $T(x_1, x_2) = (x_2, x_1).$ c) $T: M_{2,2} \to \mathbb R, T (\left [ \begin{matrix}     a & b \\     c & d \\    \end{matrix} \right ]) = a – 2b + c – 2d$ . My attempt: a) Let $u = (u_1, u_2)$ e $v = (v_1, v_2)$ $T(u+v) = T(u_1+v_1, u_2+v_2) = (u_1 – 1 + v_1 – 1, u_2 + v_2) = (u_1 + v_1 – 2, u_2 + v_2) \ne T(u) + T(v)$ $T(cu) = T(cu_1, cu_2) = (c(u_1 - 1), cu_2) = (cu_1 - c, u_2) \ne cT(u)$ . No. b)Let $u = (u_1, u_2)$ e $v = (v_1, v_2)$ $T(u+v) = T(u_1+v_1, u_2+v_2) =  (u_2 + v_2 , u_1 + v_1) = T(v) + T(u).$ $T(cu) = T(cu_1, cu_2) = (cu_2, cu_1) = c(u_2, u_1) \ne cT(u).$ No. c)Let $u = (a, b, c, d)$ e $v = (e, f, g, h)$ $T(u+v) = T([a, b, c, d] + [e, f, g, h]) =  (a+b) – 2(b+f) + (c+g) – 2(d+h) = (a -2b +c -2d, e -2f + c -2h) = T(u) + T(v)$ $T(ku) = T(k[a, b, c, d]) = ka – k2b + kc – k2d = k(a – 2b + c – 2d) = kT(u).$ Yes. Are my checks correct? Thanks.","Check if the applications defined below are linear transformations: a) , b) , c) . My attempt: a) Let e . No. b)Let e No. c)Let e Yes. Are my checks correct? Thanks.","T: \mathbb R^2 \to \mathbb R^2 T(x_1, x_2) = (x_1 – 1, x_2). T: \mathbb R^2 \to \mathbb R^2 T(x_1, x_2) = (x_2, x_1). T: M_{2,2} \to \mathbb R, T (\left [ \begin{matrix}
    a & b \\
    c & d \\
   \end{matrix} \right ]) = a – 2b + c – 2d u = (u_1, u_2) v = (v_1, v_2) T(u+v) = T(u_1+v_1, u_2+v_2) = (u_1 – 1 + v_1 – 1, u_2 + v_2) = (u_1 + v_1 – 2, u_2 + v_2) \ne T(u) + T(v) T(cu) = T(cu_1, cu_2) = (c(u_1 - 1), cu_2) = (cu_1 - c, u_2) \ne cT(u) u = (u_1, u_2) v = (v_1, v_2) T(u+v) = T(u_1+v_1, u_2+v_2) =  (u_2 + v_2 , u_1 + v_1) = T(v) + T(u). T(cu) = T(cu_1, cu_2) = (cu_2, cu_1) = c(u_2, u_1) \ne cT(u). u = (a, b, c, d) v = (e, f, g, h) T(u+v) = T([a, b, c, d] + [e, f, g, h]) =  (a+b) – 2(b+f) + (c+g) – 2(d+h) = (a -2b +c -2d, e -2f + c -2h) = T(u) + T(v) T(ku) = T(k[a, b, c, d]) = ka – k2b + kc – k2d = k(a – 2b + c – 2d) = kT(u).","['linear-algebra', 'solution-verification', 'linear-transformations']"
6,"On the induction argument of the ""many paths to a basis"" theorem","On the induction argument of the ""many paths to a basis"" theorem",,"I'm trying to make practical sense of the induction argument of the following famous theorem on the dimension of a finite-dimensional vector space. Theorem Let $n$ be a positive integer. If $V$ is a vector space containing two lists of vectors $x_1,\ldots,x_n$ and $y_1,\ldots,y_n$ of the same length $n$ , such that (1) $x_1,\ldots,x_n$ generate $V$ , and (2) $y_1,\ldots,y_n$ are independent, then both lists are bases of $V$ . Update: For your convenience I've updated the post adding the full proof. In the proof below, $\theta$ denotes the zero vector of $V$ . Proof. We have to show that list (1) is independent and list (2) is generating. The proof is by induction on $n$ . Let $F$ be the field of scalars. Suppose $n=1$ . By assumption, $V = Fx_1$ and $y_1\neq \theta$ . Say $y_1 = c x_1$ . Since $y_1$ is nonzero, both $c$ and $x_1$ are non zero. From $x_1\neq\theta$ we see that the list $x_1$ is independent. Since every vector is a multiple of $x_1 = c^{-1}y_1$ , and therefore of $y_1$ the list $y_1$ is generating. Let $n\geq 2$ and assume that the statement in the theorem is true for lists of length $n-1$ . We assert first that $x_1,\ldots,x_n$ are independent. Assume to the contrary that this list is linearly dependent, i.e. one of the $x_i$ is a linear combination of the others. Suppose, for illustration, that $x_n$ is a linear combination  of $x_1,\ldots,x_{n-1}$ . It follows that the list $x_1,\ldots,x_{n-1}$ generates $V$ ; for, its linear span includes $x_n$ as well as $x_1,\ldots,x_{n-1}$ , so it must be all of $V$ by (1). Since the list $y_1,\ldots,y_{n-1}$ is independent by (2), it follows from the induction hypothesis that $y_1,\ldots,y_{n-1}$ generate $V$ . In particular, $y_n$ is a linear combination of $y_1,\ldots, y_{n-1}$ , contradicting (2). The contradiction shows that $x_1,\ldots,x_{n}$ is independent, as asserted. The proof that $y_1,\ldots, y_n$ are generating will be accomplished by invoking the induction hypothesis in a suitable quotient space $V/M$ ; the first step is to construct an appropriate linear subspace $M$ . Express $y_n$ as a linear combination of $x_1,\ldots,x_n$ , say $$ y_n = c_1x_1+\cdots+c_nx_n. $$ Since $y_n\neq \theta$ , one of the coefficients $c_i$ must be nonzero; rearranging the $x_i$ , we can suppose that $c_n\neq 0$ . It follows that $$ x_n = (-c_1/c_n)x_1+\cdots+(-c_{n-1}/c_n)x_{n-1}+(1/c_n)y_n, $$ thus the linear span of the list $x_1,\ldots,x_{n-1},y_n$ includes all of the vectors $x_1,\ldots,x_n$ ; in view of (1) we conclude that $$ (*)\,\,\,\, x_1,\ldots,x_{n-1},y_n\,\,\text{generate } V. $$ Let $M = Fy_n$ and let $Q:V\to V/M$ be the quotient mapping. Then $$ (3)\,\,\,\ Qx_1,\ldots,Qx_{n-1}\text{ generate } V/M $$ (by (*)) and $$ (4)\,\,\,\ Qy_1,\ldots,Qy_{n-1}\text{ are independent} $$ (by (2)), so by the induction hypothesis, both the lists (3) and (4) are bases of $V/M$ . In particular, the list (4) is generating for $V/M$ ; since $M$ is generated by $y_n$ , it follows (from another result not shown here) that $y_1,\ldots,y_n$ generate $V$ . Q.E.D. I'm stuck at the induction step (in bold). I do understand that the assumption is a necessary logical step which is given for truth without questioning, in order to show that what follows is true. My question is: if $V$ has basis $x_1,\ldots,x_{n-1}$ , how can it also have (a larger) basis $x_{1},\ldots,x_{n}$ ?","I'm trying to make practical sense of the induction argument of the following famous theorem on the dimension of a finite-dimensional vector space. Theorem Let be a positive integer. If is a vector space containing two lists of vectors and of the same length , such that (1) generate , and (2) are independent, then both lists are bases of . Update: For your convenience I've updated the post adding the full proof. In the proof below, denotes the zero vector of . Proof. We have to show that list (1) is independent and list (2) is generating. The proof is by induction on . Let be the field of scalars. Suppose . By assumption, and . Say . Since is nonzero, both and are non zero. From we see that the list is independent. Since every vector is a multiple of , and therefore of the list is generating. Let and assume that the statement in the theorem is true for lists of length . We assert first that are independent. Assume to the contrary that this list is linearly dependent, i.e. one of the is a linear combination of the others. Suppose, for illustration, that is a linear combination  of . It follows that the list generates ; for, its linear span includes as well as , so it must be all of by (1). Since the list is independent by (2), it follows from the induction hypothesis that generate . In particular, is a linear combination of , contradicting (2). The contradiction shows that is independent, as asserted. The proof that are generating will be accomplished by invoking the induction hypothesis in a suitable quotient space ; the first step is to construct an appropriate linear subspace . Express as a linear combination of , say Since , one of the coefficients must be nonzero; rearranging the , we can suppose that . It follows that thus the linear span of the list includes all of the vectors ; in view of (1) we conclude that Let and let be the quotient mapping. Then (by (*)) and (by (2)), so by the induction hypothesis, both the lists (3) and (4) are bases of . In particular, the list (4) is generating for ; since is generated by , it follows (from another result not shown here) that generate . Q.E.D. I'm stuck at the induction step (in bold). I do understand that the assumption is a necessary logical step which is given for truth without questioning, in order to show that what follows is true. My question is: if has basis , how can it also have (a larger) basis ?","n V x_1,\ldots,x_n y_1,\ldots,y_n n x_1,\ldots,x_n V y_1,\ldots,y_n V \theta V n F n=1 V = Fx_1 y_1\neq \theta y_1 = c x_1 y_1 c x_1 x_1\neq\theta x_1 x_1 = c^{-1}y_1 y_1 y_1 n\geq 2 n-1 x_1,\ldots,x_n x_i x_n x_1,\ldots,x_{n-1} x_1,\ldots,x_{n-1} V x_n x_1,\ldots,x_{n-1} V y_1,\ldots,y_{n-1} y_1,\ldots,y_{n-1} V y_n y_1,\ldots, y_{n-1} x_1,\ldots,x_{n} y_1,\ldots, y_n V/M M y_n x_1,\ldots,x_n 
y_n = c_1x_1+\cdots+c_nx_n.
 y_n\neq \theta c_i x_i c_n\neq 0 
x_n = (-c_1/c_n)x_1+\cdots+(-c_{n-1}/c_n)x_{n-1}+(1/c_n)y_n,
 x_1,\ldots,x_{n-1},y_n x_1,\ldots,x_n 
(*)\,\,\,\, x_1,\ldots,x_{n-1},y_n\,\,\text{generate } V.
 M = Fy_n Q:V\to V/M 
(3)\,\,\,\ Qx_1,\ldots,Qx_{n-1}\text{ generate } V/M
 
(4)\,\,\,\ Qy_1,\ldots,Qy_{n-1}\text{ are independent}
 V/M V/M M y_n y_1,\ldots,y_n V V x_1,\ldots,x_{n-1} x_{1},\ldots,x_{n}","['linear-algebra', 'vector-spaces', 'induction']"
7,"Exercise 13, Section 2.C - Linear Algebra Done Right.","Exercise 13, Section 2.C - Linear Algebra Done Right.",,"Exercise: Suppose $U$ and $W$ are both $4$ -dimensional subspaces of $C^6$ . Prove that there exist two vectors in $U \cap W$ such that neither of these vectors is a scalar multiple of the other. My attempt at a proof is as follows. Proof: Let $u_1,. . .,u_4$ be a basis of $U$ and let $w_1,. . .,w_4$ be a basis of $W$ . Then, $u_1,. . .,u_4,w_1,. . .,w_4$ spans $U+W$ . Because $U+W$ is a subspace of $C^6$ , the $\dim(U+W)\le 6$ . Thus, $u_1,. . .,u_4,w_1,. . .,w_4$ can be reduced to a basis of $U+W$ . In the process, none of the $u's$ get removed as $u_1,. . .,u_4$ is linearly independent. Thus, some of the $w's$ get removed in the process. Because $\dim(U+W)\le 6$ , at least two of the $w's$ get removed. These are the $w's\in U\cap W$ . Because $w_1, . .,w_4$ is linearly independent, none of these two vectors are a scalar multiple of each other. Is the proof correct? Edit: I implicitly use that theorem that every spanning list in a vector space can be reduced to a basis of that vector space. In the process, we remove those vectors that are in the span of the previous ones. Thus, if we have the list $v_1,. . .,v_k$ . We remove $v_j$ only if $v_j$ is in the span of $v_1,. . .,v_{j-1}$ . Edit 2: I have come to know that the proof is wrong. For future readers, I am writing another proof that is also suggested as a hint in the answers. Proof 2: Using the formula $\dim (U+W)=\dim(U)+\dim(W)-\dim(U\cap W)$ , we see that $\dim(U\cap W) \ge 2$ . This is because $\dim(C^6)=6$ and $U+W$ is a subspace of $C^6$ . Thus, $\dim(U+W)\le 6$ . Let $j\in Z^+$ with $2\le j\le 6$ . Let $\dim(U\cap W)=j$ . Let $v_1,. . .,v_j$ be a basis of $U\cap W$ . Then we have that $v_1,v_2\in U\cap W$ are not scalar multiples of each other as they are linearly independent. Completing the proof.","Exercise: Suppose and are both -dimensional subspaces of . Prove that there exist two vectors in such that neither of these vectors is a scalar multiple of the other. My attempt at a proof is as follows. Proof: Let be a basis of and let be a basis of . Then, spans . Because is a subspace of , the . Thus, can be reduced to a basis of . In the process, none of the get removed as is linearly independent. Thus, some of the get removed in the process. Because , at least two of the get removed. These are the . Because is linearly independent, none of these two vectors are a scalar multiple of each other. Is the proof correct? Edit: I implicitly use that theorem that every spanning list in a vector space can be reduced to a basis of that vector space. In the process, we remove those vectors that are in the span of the previous ones. Thus, if we have the list . We remove only if is in the span of . Edit 2: I have come to know that the proof is wrong. For future readers, I am writing another proof that is also suggested as a hint in the answers. Proof 2: Using the formula , we see that . This is because and is a subspace of . Thus, . Let with . Let . Let be a basis of . Then we have that are not scalar multiples of each other as they are linearly independent. Completing the proof.","U W 4 C^6 U \cap W u_1,. . .,u_4 U w_1,. . .,w_4 W u_1,. . .,u_4,w_1,. . .,w_4 U+W U+W C^6 \dim(U+W)\le 6 u_1,. . .,u_4,w_1,. . .,w_4 U+W u's u_1,. . .,u_4 w's \dim(U+W)\le 6 w's w's\in U\cap W w_1, . .,w_4 v_1,. . .,v_k v_j v_j v_1,. . .,v_{j-1} \dim (U+W)=\dim(U)+\dim(W)-\dim(U\cap W) \dim(U\cap W) \ge 2 \dim(C^6)=6 U+W C^6 \dim(U+W)\le 6 j\in Z^+ 2\le j\le 6 \dim(U\cap W)=j v_1,. . .,v_j U\cap W v_1,v_2\in U\cap W","['linear-algebra', 'vector-spaces', 'solution-verification']"
8,A question about $GL_n(\mathbb{Z})$ and $GL_n(\mathbb{F}_p)$,A question about  and,GL_n(\mathbb{Z}) GL_n(\mathbb{F}_p),"Let $p\ge 3$ be a prime number. $G$ is a subgroup of $GL_n(\mathbb{Z})$ and $|G|<\infty$ . Let $\sigma: GL_n(\mathbb{Z})\to GL_n(\mathbb{F}_p)$ be the natural map. Prove that $\sigma|_G$ is injective. Suppose $\exists A,B \in G,A\ne B$ , s.t. $\sigma(A)=\sigma(B)$ i.e. $A=B \pmod{ p}$ . Since $|G|<\infty$ , $\exists m,k\in\mathbb{Z}^+$ , $A^m=B^k=I$ . In linear algebra we know $A=C\ {\rm diag}(\zeta_1,\dots,\zeta_n)\ C^{-1}$ where $\zeta_i^m=1,C\in GL_n(\mathbb{C})$ . But I don't know if $m=k$ . Taking trace and norm can't solve this problem. I think $A=B \pmod{p} $ is not easy to use. Any ideas?","Let be a prime number. is a subgroup of and . Let be the natural map. Prove that is injective. Suppose , s.t. i.e. . Since , , . In linear algebra we know where . But I don't know if . Taking trace and norm can't solve this problem. I think is not easy to use. Any ideas?","p\ge 3 G GL_n(\mathbb{Z}) |G|<\infty \sigma: GL_n(\mathbb{Z})\to GL_n(\mathbb{F}_p) \sigma|_G \exists A,B \in G,A\ne B \sigma(A)=\sigma(B) A=B \pmod{ p} |G|<\infty \exists m,k\in\mathbb{Z}^+ A^m=B^k=I A=C\ {\rm diag}(\zeta_1,\dots,\zeta_n)\ C^{-1} \zeta_i^m=1,C\in GL_n(\mathbb{C}) m=k A=B \pmod{p} ","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory']"
9,"Regarding the derivative of Euclidean L2 norm, Definition of differentiation in Rudin.","Regarding the derivative of Euclidean L2 norm, Definition of differentiation in Rudin.",,"I am trying to understand the answer posted by hemanth in this post . I understand how he derived the derivative $f:\Bbb R^n \rightarrow \Bbb R$ defined as $f(x)=\rVert x \rVert$ , $$ Df(x) = \nabla_x\rVert x\rVert_2= \frac{x}{\lVert x \rVert}. $$ So we have $Df:\Bbb R^n \rightarrow L(\Bbb R^n,\Bbb R)$ , but according to Rudin, $Df(x)$ must be a linear transformation from $\Bbb R^n$ to $\Bbb R$ , while I do not see $Df(x)h \in \Bbb R$ for any h $ \in \Bbb R^n$ . I am trying to derive $Df(x)$ such that: $$ \lim_{h\rightarrow0}=\frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert},$$ so $Df(x)h$ must be a real number. Could it be that $Df(x)h$ is a product of a column vector and a row vector? If so, how does $$ \begin{split} \frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert} &= \frac{\Vert\rVert x+h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}\\  &\le\frac{\rVert\rVert x \rVert+\rVert h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}=\frac{\rVert\rVert h \rVert -\frac{x}{\rVert x \rVert}h \rVert}{\rVert h \rVert}\rightarrow 0\;, \end{split} $$ as $h \rightarrow 0$ ? Anyone can help me with this confusion? It is my first time asking the question here, so my formatting might be bad. Thank you for understanding.","I am trying to understand the answer posted by hemanth in this post . I understand how he derived the derivative defined as , So we have , but according to Rudin, must be a linear transformation from to , while I do not see for any h . I am trying to derive such that: so must be a real number. Could it be that is a product of a column vector and a row vector? If so, how does as ? Anyone can help me with this confusion? It is my first time asking the question here, so my formatting might be bad. Thank you for understanding.","f:\Bbb R^n \rightarrow \Bbb R f(x)=\rVert x \rVert 
Df(x) = \nabla_x\rVert x\rVert_2= \frac{x}{\lVert x \rVert}.
 Df:\Bbb R^n \rightarrow L(\Bbb R^n,\Bbb R) Df(x) \Bbb R^n \Bbb R Df(x)h \in \Bbb R  \in \Bbb R^n Df(x) 
\lim_{h\rightarrow0}=\frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert}, Df(x)h Df(x)h 
\begin{split}
\frac{\rVert f(x+h)-f(x)-Df(x)h \rVert}{\rVert h \rVert} &= \frac{\Vert\rVert x+h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}\\ 
&\le\frac{\rVert\rVert x \rVert+\rVert h \rVert - \rVert x \rVert -Df(x)h \Vert}{\rVert h\rVert}=\frac{\rVert\rVert h \rVert -\frac{x}{\rVert x \rVert}h \rVert}{\rVert h \rVert}\rightarrow 0\;,
\end{split}
 h \rightarrow 0","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'derivatives', 'linear-transformations']"
10,Right and left multiplication by scalar on a vector space,Right and left multiplication by scalar on a vector space,,"This is a really basic question but I always wanted to know the answer to it. If we have a vector space $V$ over a field $K$ , then scalar multiplication is usually defined by taking $\alpha v$ , $\alpha \in K$ and $v \in V$ and the scalar always come on the left side. I know that in this case one can freely change the order of multiplication $\alpha v = v\alpha$ . However, what axiom of vector space justifies this exchange? Since scalar multiplication is always introduced by placing the scalar on the left, how can we even start to consider a scalar on the right? Shouldn't we have some property about, say, $1v = v1 = v$ ? I have never found a reference in which some property is explicitly stated.","This is a really basic question but I always wanted to know the answer to it. If we have a vector space over a field , then scalar multiplication is usually defined by taking , and and the scalar always come on the left side. I know that in this case one can freely change the order of multiplication . However, what axiom of vector space justifies this exchange? Since scalar multiplication is always introduced by placing the scalar on the left, how can we even start to consider a scalar on the right? Shouldn't we have some property about, say, ? I have never found a reference in which some property is explicitly stated.",V K \alpha v \alpha \in K v \in V \alpha v = v\alpha 1v = v1 = v,"['linear-algebra', 'vector-spaces']"
11,Help me prove or disprove $v^T(A+vv^T)^{-1}v=1$ for singular A,Help me prove or disprove  for singular A,v^T(A+vv^T)^{-1}v=1,"Given arbitrary singular square matrix $A$ deficient in rank by $1$ , and vector $v$ , in numerical experiments with numerous values of the variables, I always get $v^T(A+vv^T)^{-1}v=1$ .  Why does $A$ cancel out like this?  (And when not, aside from obvious zero situations?) EDIT: My $A$ matrices have been symmetric positive semidefinite (so far).  And the relation keeps holding when many or all elements of $v$ are tiny.  (It takes around 6 orders of magnitude smaller than the A elements to break down).","Given arbitrary singular square matrix deficient in rank by , and vector , in numerical experiments with numerous values of the variables, I always get .  Why does cancel out like this?  (And when not, aside from obvious zero situations?) EDIT: My matrices have been symmetric positive semidefinite (so far).  And the relation keeps holding when many or all elements of are tiny.  (It takes around 6 orders of magnitude smaller than the A elements to break down).",A 1 v v^T(A+vv^T)^{-1}v=1 A A v,"['linear-algebra', 'matrices']"
12,"Show that $\langle x,x\rangle > 0$ if $x \neq0$",Show that  if,"\langle x,x\rangle > 0 x \neq0","Let $x=(x_1,x_2)$ and $ y=(y_1,y_2)$ be vectors in the vector space $C^2$ over $C$ and define $\langle \cdot,\cdot\rangle : C^2 \times C^2 \rightarrow C$ by $ \langle x,y\rangle =3x_1 \overline{y_1}+(1+i)x_1 \overline{y_2}+(1-i)x_2 \overline{y_1}+x_2 \overline{y_2} $ Show that $\langle x,x\rangle > 0$ if $x \neq0$ . So what I have so far is: $\begin{aligned} \langle x,x\rangle &= 3x_1 \overline{x_1}+(1+i)x_1 \overline{x_2}+(1-i)x_2 \overline{x_1}+x_2 \overline{x_2} \\ &= 3|x_1|^2 + |x_2|^2 +x_1 \overline x_2 + ix_1\overline x_2 - ix_2 \overline x_1 +x_2 \overline x_1 \\ &=3|x_1|^2 + |x_2|^2 + (1+i)(x_1 \overline x_2 - i\overline x_1 x_2) \end{aligned}$ Now $3|x_1|^2 + |x_2|^2 >0$ if $x \neq0$ , but I don't know how to show that $(1+i)(x_1 \overline x_2 - i\overline x_1 x_2) >0$ as well. Any guidance would be appreciated. Thank you.","Let and be vectors in the vector space over and define by Show that if . So what I have so far is: Now if , but I don't know how to show that as well. Any guidance would be appreciated. Thank you.","x=(x_1,x_2)  y=(y_1,y_2) C^2 C \langle \cdot,\cdot\rangle : C^2 \times C^2 \rightarrow C  \langle x,y\rangle =3x_1 \overline{y_1}+(1+i)x_1 \overline{y_2}+(1-i)x_2 \overline{y_1}+x_2 \overline{y_2}  \langle x,x\rangle > 0 x \neq0 \begin{aligned} \langle x,x\rangle &= 3x_1 \overline{x_1}+(1+i)x_1 \overline{x_2}+(1-i)x_2 \overline{x_1}+x_2 \overline{x_2} \\
&= 3|x_1|^2 + |x_2|^2 +x_1 \overline x_2 + ix_1\overline x_2 - ix_2 \overline x_1 +x_2 \overline x_1 \\
&=3|x_1|^2 + |x_2|^2 + (1+i)(x_1 \overline x_2 - i\overline x_1 x_2)
\end{aligned} 3|x_1|^2 + |x_2|^2 >0 x \neq0 (1+i)(x_1 \overline x_2 - i\overline x_1 x_2) >0","['linear-algebra', 'inner-products']"
13,Real coefficients such that any matrix is invertible,Real coefficients such that any matrix is invertible,,"Let $n>1$ . Does there exist $n^2$ real numbers $a_1, a_2, ..., a_{n^2}$ such that any $n \times n$ matrix using these coefficients is invertible? I wanted to use the fact that a matrix is invertible iff its rank is $n$ . My idea is to find numbers such that any column of $n$ of these numbers is not a linear combination of the other columns. So, I started looking at square roots of prime numbers. I was at first conjecturing that a sum of $n$ square roots of prime numbers cannot be itself a square root of prime numbers; It’s true for $n=2$ : If $p$ and $q$ are distinct prime numbers, let’s suppose by contradiction $\sqrt{p}+\sqrt{q}=\sqrt{k}$ with $k \in \mathbb{N}$ . Squaring this identity gives $p+q+2\sqrt{pq}=k$ which is absurd since $\sqrt{pq}$ must be irrationnal. For more primes, it gets more complicated since a similar proof for $n$ primes $p_1,...,p_n$ would lead to show that $\sum_{i<j} \sqrt{p_ip_j}$ cannot be a rationnal number, which I assume to be true but cannot prove it. Moreover, proving the result only for sums cannot be sufficient since we’re looking for linear combinations, and there obviously exist a real $\lambda$ such that $\sqrt{p_1}=\lambda \sqrt{p_2}$ , which makes impossible to do the study number by number as I was doing for now. Could my idea lead anywhere, or is there an easier approach to this problem?","Let . Does there exist real numbers such that any matrix using these coefficients is invertible? I wanted to use the fact that a matrix is invertible iff its rank is . My idea is to find numbers such that any column of of these numbers is not a linear combination of the other columns. So, I started looking at square roots of prime numbers. I was at first conjecturing that a sum of square roots of prime numbers cannot be itself a square root of prime numbers; It’s true for : If and are distinct prime numbers, let’s suppose by contradiction with . Squaring this identity gives which is absurd since must be irrationnal. For more primes, it gets more complicated since a similar proof for primes would lead to show that cannot be a rationnal number, which I assume to be true but cannot prove it. Moreover, proving the result only for sums cannot be sufficient since we’re looking for linear combinations, and there obviously exist a real such that , which makes impossible to do the study number by number as I was doing for now. Could my idea lead anywhere, or is there an easier approach to this problem?","n>1 n^2 a_1, a_2, ..., a_{n^2} n \times n n n n n=2 p q \sqrt{p}+\sqrt{q}=\sqrt{k} k \in \mathbb{N} p+q+2\sqrt{pq}=k \sqrt{pq} n p_1,...,p_n \sum_{i<j} \sqrt{p_ip_j} \lambda \sqrt{p_1}=\lambda \sqrt{p_2}","['linear-algebra', 'matrices', 'inverse']"
14,Can eigenvectors be scaled and still be eigenvectors?,Can eigenvectors be scaled and still be eigenvectors?,,"Given the matrix: $$ A =  \begin{bmatrix} 2 & 0 \\ 1 & 4 \end{bmatrix}. $$ The eigenvalues are: $$λ_1 = 2,$$ $$λ_2 = 4.$$ To find the eigenvectors : $$v_1 = \operatorname{nullity}(A - λ_1I),$$ $$v_2 = \operatorname{nullity}(A - λ_2I).$$ Which yielded the following eigenvectors for $v_1$ : $$ v_1 =  \begin{bmatrix} -\frac{2}{3}\\ 1 \end{bmatrix}. $$ However, I have seen this alternative eigenvector for $v_1$ : $$ v_1 =  \begin{bmatrix} -2\\ 3 \end{bmatrix}. $$ Question: Are both eigenvectors for $v_1$ correct? In other words, can you scale an eigenvector by any real number and it still be an eigenvector of that matrix? This would make sense, as I believe we are only really interested in the direction (eigenvectors) and the factor by which the matrix scaled (eigenvalues). Thus, I would argue that the magnitude of the values within an eigenvector are meaningless. For example, the following eigenvector for $v_1$ is just as correct: $$ v_1 =  \begin{bmatrix} -44444\\ 66666 \end{bmatrix} $$ Which perhaps begs the question: Why aren't these called eigenvalues and eigendirections?","Given the matrix: The eigenvalues are: To find the eigenvectors : Which yielded the following eigenvectors for : However, I have seen this alternative eigenvector for : Question: Are both eigenvectors for correct? In other words, can you scale an eigenvector by any real number and it still be an eigenvector of that matrix? This would make sense, as I believe we are only really interested in the direction (eigenvectors) and the factor by which the matrix scaled (eigenvalues). Thus, I would argue that the magnitude of the values within an eigenvector are meaningless. For example, the following eigenvector for is just as correct: Which perhaps begs the question: Why aren't these called eigenvalues and eigendirections?","
A = 
\begin{bmatrix}
2 & 0 \\
1 & 4
\end{bmatrix}.
 λ_1 = 2, λ_2 = 4. v_1 = \operatorname{nullity}(A - λ_1I), v_2 = \operatorname{nullity}(A - λ_2I). v_1 
v_1 = 
\begin{bmatrix}
-\frac{2}{3}\\
1
\end{bmatrix}.
 v_1 
v_1 = 
\begin{bmatrix}
-2\\
3
\end{bmatrix}.
 v_1 v_1 
v_1 = 
\begin{bmatrix}
-44444\\
66666
\end{bmatrix}
","['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'vectors']"
15,A counterexample of Riesz Representation Theorem?,A counterexample of Riesz Representation Theorem?,,"I am working on the exercise 6.B.15 from Axler's book: Linear Algebra Done Right. This problem states that the Riesz Representation Theorem may fail on an infinite-dimensional vector space: Suppose $C_R([-1,1])$ is the vector space of continuous real-valued functions on the interval $[-1,1]$ with inner product given by: $$\left<f,g \right> = \int_{-1}^{1}f(x)g(x)dx$$ for $f, g \in C_R[-1,1]$ . Let $\varphi$ be the linear functional on $C_R[-1,1]$ defined by $\varphi(f)=f(0)$ . Show that there does not exist $g \in C_R[-1,1]$ such that $$ \varphi(f)= \left< f, g \right> $$ for every $f \in C_R[-1,1]$ . My argument is that I came up with two special sequences of functions in $C_R[-1,1]$ , and proved a contradiction: Consider $f_{n}, h_{n} \subset C_{\mathbb{R}}[-1,1]$ , defined by $$     f_{n}(x)= \begin{cases}     1, ~ x \in [0,1] \\     n x + 1, ~  x \in [-\frac{1}{n},0] \\     0, ~  x \in [-1,-\frac{1}{n}] \end{cases} $$ and $$     h_{n}(x)= \begin{cases}     1, ~ x \in [-1,0] \\     -n x + 1, ~  x \in [0,\frac{1}{n}] \\     1, ~  x \in [\frac{1}{n},1] \end{cases} $$ Then $$ f_n \to f $$ and $$h_n \to h$$ where $f = 0$ on $[-1,0]$ , $f = 1$ on $[0,1]$ , while $h = 0$ on $[0,1]$ and $h=1$ on $[-1,0]$ . Then for any $n \in \mathbb{N}$ ,  we have $\varphi(f_{n}) = f_{n}(0) = 1$ and $\varphi(h_{n}) = h_{n}(x) = 1$ . For any constant function $T_{m}(x) = m \neq  0$ on $[-1,1]$ , $\varphi(T_{m}) = \left< T_{m}, g \right> = T_{m}(0)= m = \int_{-1}^{1}m g(x)\,\mathrm{d}x$ . As a result, then $$     \int_{-1}^{1}g(x)\,\mathrm{d}x = 1. $$ It implies that $g \in L^{1}[-1,1]$ . So $\left\vert f_{n} g \right\vert \leqslant \left\vert g \right\vert $ and $\left\vert  h_{n} g \right\vert  \leqslant \left\vert g \right\vert $ . By the LDCT, \begin{align*}     \lim_{n    \to \infty} \int_{-1}^{1}f_{n}(x)g(x)\,\mathrm{d}x = 1 = \int_{-1}^{1}f(x)g(x)\,\mathrm{d}x = \int_{0}^{1}g(x)\,\mathrm{d}x. \end{align*} The same argument applies to $h_{n}$ , then we have $$     \int_{-1}^{0}g(x)\,\mathrm{d}x = 1 $$ and then $$     \int_{-1}^{1}g(x)\,\mathrm{d}x = \int_{-1}^{0}g(x)\,\mathrm{d}x + \int_{0}^{1}g(x)\,\mathrm{d}x = 2 $$ which contradicts with $\int_{-1}^1 g(x) dx = 1.$ My question is, if this argument is legitimate, why such contradiction could arise? Is it because the inner product space is not Hilbert?","I am working on the exercise 6.B.15 from Axler's book: Linear Algebra Done Right. This problem states that the Riesz Representation Theorem may fail on an infinite-dimensional vector space: Suppose is the vector space of continuous real-valued functions on the interval with inner product given by: for . Let be the linear functional on defined by . Show that there does not exist such that for every . My argument is that I came up with two special sequences of functions in , and proved a contradiction: Consider , defined by and Then and where on , on , while on and on . Then for any ,  we have and . For any constant function on , . As a result, then It implies that . So and . By the LDCT, The same argument applies to , then we have and then which contradicts with My question is, if this argument is legitimate, why such contradiction could arise? Is it because the inner product space is not Hilbert?","C_R([-1,1]) [-1,1] \left<f,g \right> = \int_{-1}^{1}f(x)g(x)dx f, g \in C_R[-1,1] \varphi C_R[-1,1] \varphi(f)=f(0) g \in C_R[-1,1] 
\varphi(f)= \left< f, g \right>
 f \in C_R[-1,1] C_R[-1,1] f_{n}, h_{n} \subset C_{\mathbb{R}}[-1,1] 
    f_{n}(x)=
\begin{cases}
    1, ~ x \in [0,1] \\
    n x + 1, ~  x \in [-\frac{1}{n},0] \\
    0, ~  x \in [-1,-\frac{1}{n}]
\end{cases}
 
    h_{n}(x)=
\begin{cases}
    1, ~ x \in [-1,0] \\
    -n x + 1, ~  x \in [0,\frac{1}{n}] \\
    1, ~  x \in [\frac{1}{n},1]
\end{cases}
  f_n \to f  h_n \to h f = 0 [-1,0] f = 1 [0,1] h = 0 [0,1] h=1 [-1,0] n \in \mathbb{N} \varphi(f_{n}) = f_{n}(0) = 1 \varphi(h_{n}) = h_{n}(x) = 1 T_{m}(x) = m \neq  0 [-1,1] \varphi(T_{m}) = \left< T_{m}, g \right> = T_{m}(0)= m = \int_{-1}^{1}m g(x)\,\mathrm{d}x 
    \int_{-1}^{1}g(x)\,\mathrm{d}x = 1.
 g \in L^{1}[-1,1] \left\vert f_{n} g \right\vert \leqslant \left\vert g \right\vert  \left\vert  h_{n} g \right\vert  \leqslant \left\vert g \right\vert  \begin{align*}
    \lim_{n    \to \infty} \int_{-1}^{1}f_{n}(x)g(x)\,\mathrm{d}x = 1 = \int_{-1}^{1}f(x)g(x)\,\mathrm{d}x = \int_{0}^{1}g(x)\,\mathrm{d}x.
\end{align*} h_{n} 
    \int_{-1}^{0}g(x)\,\mathrm{d}x = 1
 
    \int_{-1}^{1}g(x)\,\mathrm{d}x = \int_{-1}^{0}g(x)\,\mathrm{d}x + \int_{0}^{1}g(x)\,\mathrm{d}x = 2
 \int_{-1}^1 g(x) dx = 1.","['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis', 'riesz-representation-theorem']"
16,"Relating ""change of coordinates"" to change of basis - how to find change in representations of vectors","Relating ""change of coordinates"" to change of basis - how to find change in representations of vectors",,"I've been studying about change of basis in $\mathbb{R}^2$ (could be $\mathbb{R}^n$ but sticking to $\mathbb{R}^2$ for simplicity) - how it affects representations of vectors, metrics and endomorphisms. Let's say I start with a basis $\mathcal{B}=\{\vec u_1, \vec u_2\}$ , and want to switch to a different basis $\mathcal{A}=\{\vec v_1,\vec v_2\}$ . That is, if earlier we were expressing the components of some vector $\vec w$ in the $\mathcal{B}$ basis, we now want to express its components in the $\mathcal{A}$ basis). For this I can use a change of basis matrix $M_{\mathcal{A}\leftarrow\mathcal{B}}$ whose columns are the representations of $\vec u_1,\vec u_2$ in the $\mathcal{A}$ basis . And then I can relate the representations of $\vec w$ in the two bases by: $$[\vec w]_{\mathcal{A}} = M_{\mathcal{A}\leftarrow\mathcal{B}}[\vec w]_{\mathcal{B}}$$ If I have a linear transformation $T$ of the vector space to itself (endomorphism), and if I know its representation in the old basis, then I can get its representation in the new basis like this (let's say $T$ maps $\vec w_1$ to $\vec w_2$ and the change of basis matrix is invertible ): $$[\vec w_2]_{\mathcal{A}}=[T]_{\mathcal{A}}[\vec w_1]_{\mathcal{A}} \\\implies M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_2]_{\mathcal{A}}=M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_1]_{\mathcal{A}} \\\implies [\vec w_2]_{\mathcal{B}}=(M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}})[\vec w_1]_{\mathcal{B}} \\\implies [T]_{\mathcal{B}} = M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}}$$ Finally, if $\eta$ is the metric, then invariance of inner product gives us: $$[\vec w_1]^T_{\mathcal{A}}[\eta]_{\mathcal{A}}[\vec w_2]_{\mathcal{A}}= [\vec w_1]^T_{\mathcal{B}}[\eta]_{\mathcal{B}}[\vec w_2]_{\mathcal{B}} \\=[\vec w_1]^T_{\mathcal{A}}M^T_{\mathcal{B}\leftarrow\mathcal{A}}[\eta]_{\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_2]_{\mathcal{A}} \\\implies [\eta]_{\mathcal{A}}=M^T_{\mathcal{B}\leftarrow\mathcal{A}}[\eta]_{\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}}$$ So far so good. I can use the above to find representations in the new coordinate system in the case of some simple coordinate system changes - e.g. if I shift the coordinate system in some direction or if I rotate it by some angle $\phi$ . But I'm at a loss on how to extend this same formalism (of finding representations in the new coordinate system), if we change from Cartesian to polar coordinates. If I try to form a change of basis matrix (assuming that the new system is $(r,\phi)$ ), I get $[1,0]^T$ and $[1,\pi/2]^T$ as the columns of my CoB matrix, which gives wrong results when I try to use it to get components of a vector in the polar coordinate system. Next thing I thought was, am I fundamentally confusing a change of coordinates with a change of basis? For that, I tested the above procedure of finding vector component transformation in case of rescaling/rotating only one of the axes - even in that scenario, the above procedure works. This leads me to suspect that the above formulas for transformation of components between bases hold in general for any rectilinear coordinate systems - whether orthogonal or not. What do I do in case of a Cartesian to polar coordinate system change to find representations of vectors, metric and linear transformations?","I've been studying about change of basis in (could be but sticking to for simplicity) - how it affects representations of vectors, metrics and endomorphisms. Let's say I start with a basis , and want to switch to a different basis . That is, if earlier we were expressing the components of some vector in the basis, we now want to express its components in the basis). For this I can use a change of basis matrix whose columns are the representations of in the basis . And then I can relate the representations of in the two bases by: If I have a linear transformation of the vector space to itself (endomorphism), and if I know its representation in the old basis, then I can get its representation in the new basis like this (let's say maps to and the change of basis matrix is invertible ): Finally, if is the metric, then invariance of inner product gives us: So far so good. I can use the above to find representations in the new coordinate system in the case of some simple coordinate system changes - e.g. if I shift the coordinate system in some direction or if I rotate it by some angle . But I'm at a loss on how to extend this same formalism (of finding representations in the new coordinate system), if we change from Cartesian to polar coordinates. If I try to form a change of basis matrix (assuming that the new system is ), I get and as the columns of my CoB matrix, which gives wrong results when I try to use it to get components of a vector in the polar coordinate system. Next thing I thought was, am I fundamentally confusing a change of coordinates with a change of basis? For that, I tested the above procedure of finding vector component transformation in case of rescaling/rotating only one of the axes - even in that scenario, the above procedure works. This leads me to suspect that the above formulas for transformation of components between bases hold in general for any rectilinear coordinate systems - whether orthogonal or not. What do I do in case of a Cartesian to polar coordinate system change to find representations of vectors, metric and linear transformations?","\mathbb{R}^2 \mathbb{R}^n \mathbb{R}^2 \mathcal{B}=\{\vec u_1, \vec u_2\} \mathcal{A}=\{\vec v_1,\vec v_2\} \vec w \mathcal{B} \mathcal{A} M_{\mathcal{A}\leftarrow\mathcal{B}} \vec u_1,\vec u_2 \mathcal{A} \vec w [\vec w]_{\mathcal{A}} = M_{\mathcal{A}\leftarrow\mathcal{B}}[\vec w]_{\mathcal{B}} T T \vec w_1 \vec w_2 [\vec w_2]_{\mathcal{A}}=[T]_{\mathcal{A}}[\vec w_1]_{\mathcal{A}}
\\\implies M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_2]_{\mathcal{A}}=M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_1]_{\mathcal{A}}
\\\implies [\vec w_2]_{\mathcal{B}}=(M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}})[\vec w_1]_{\mathcal{B}}
\\\implies [T]_{\mathcal{B}} = M_{\mathcal{B}\leftarrow\mathcal{A}}[T]_{\mathcal{A}}M_{\mathcal{A}\leftarrow\mathcal{B}} \eta [\vec w_1]^T_{\mathcal{A}}[\eta]_{\mathcal{A}}[\vec w_2]_{\mathcal{A}}=
[\vec w_1]^T_{\mathcal{B}}[\eta]_{\mathcal{B}}[\vec w_2]_{\mathcal{B}}
\\=[\vec w_1]^T_{\mathcal{A}}M^T_{\mathcal{B}\leftarrow\mathcal{A}}[\eta]_{\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}}[\vec w_2]_{\mathcal{A}}
\\\implies [\eta]_{\mathcal{A}}=M^T_{\mathcal{B}\leftarrow\mathcal{A}}[\eta]_{\mathcal{B}}M_{\mathcal{B}\leftarrow\mathcal{A}} \phi (r,\phi) [1,0]^T [1,\pi/2]^T",['linear-algebra']
17,Can an Perron eigenvector of a non-symmetric irreducible nonnegative matrix be also a Perron eigenvector of its transpose?,Can an Perron eigenvector of a non-symmetric irreducible nonnegative matrix be also a Perron eigenvector of its transpose?,,"Let $\mathbf{X}$ be nonnegative irreducible matrix such that $\mathbf{X} \ne \mathbf{X}^T$ . Let $\mathbf{p}(\mathbf{A})$ denote a right eigenvector corresponding to the Perron root of $\mathbf{A}$ , $\rho(\mathbf{A})$ . Similarly, $\mathbf{q}(\mathbf{A})$ denotes a left eigenvector corresponding to $\rho(\mathbf{A})$ . (In the textbook I'm reading, left eigenvectors are column vectors corresponding to the right eigenvector of $\mathbf{A}^T$ .) A remark in the textbook seems to imply that as long as $\mathbf{X} \ne \mathbf{X} ^ T$ , $\mathbf{p}(\mathbf{X}) \ne \alpha \mathbf{p}(\mathbf{X}^T)$ for any constant $\alpha$ . EDIT: As requested by @user1551, here is the exact remark from Fundamentals of Resource Allocation in Wireless Networks : Finally, as $\mathbf{X}^T\in W_K(\mathbf{X})$ for any $\mathbf{X} \in X_K$ , it follows from Theorem 1.14 that $\rho((1-\mu) \mathbf{X} + \mu\mathbf{X}^T)$ is a concave function on $\mu \in [0,1]$ . If, in addition, $\mathbf{X}\ne \mathbf{X}^T$ , we have $\mathbf{p}(\mathbf{X}) \ne \alpha \mathbf{p}(\mathbf{X^T})$ for any constant $\alpha$ . $X_K$ is the set of nonnegative irreducible $K$ -by- $K$ matrices.  For $\mathbf{X}\in X_K$ , $W_K(\mathbf{X}) = \{\mathbf{Y}\in X_K : \mathbf{q}(Y)\circ \mathbf{p}(\mathbf{Y}) = \mathbf{q}(X)\circ \mathbf{p}(\mathbf{X}) \in \Pi_K^{+}\}$ , where $\Pi_K^+$ is the standard simplex restricted to positive values. In an effort to prove the remark, I started a proof by contradiction. Suppose that $\mathbf{p}(\mathbf{X}^T)\equiv \mathbf{q}(\mathbf{X})$ is also a right Perron eigenvector of $\mathbf{X}$ . By definition of left and right Perron eigenvectors, $\mathbf{X}^T \mathbf{q}(\mathbf{X}) = \rho(\mathbf{X})\mathbf{q}(\mathbf{X})$ and $\mathbf{X}\mathbf{q}(\mathbf{X}) = \rho(\mathbf{X})\mathbf{q}(\mathbf{X})$ . Subtracting the two equations, I get $(\mathbf{X}-\mathbf{X}^T)\mathbf{q}(\mathbf{X}) = \mathbf{0}$ , and I'm not sure how to proceed. The only conclusion I get from this equation is that $\mathbf{q}(\mathbf{X})$ is in the null space of $\mathbf{X} - \mathbf{X}^T$ .","Let be nonnegative irreducible matrix such that . Let denote a right eigenvector corresponding to the Perron root of , . Similarly, denotes a left eigenvector corresponding to . (In the textbook I'm reading, left eigenvectors are column vectors corresponding to the right eigenvector of .) A remark in the textbook seems to imply that as long as , for any constant . EDIT: As requested by @user1551, here is the exact remark from Fundamentals of Resource Allocation in Wireless Networks : Finally, as for any , it follows from Theorem 1.14 that is a concave function on . If, in addition, , we have for any constant . is the set of nonnegative irreducible -by- matrices.  For , , where is the standard simplex restricted to positive values. In an effort to prove the remark, I started a proof by contradiction. Suppose that is also a right Perron eigenvector of . By definition of left and right Perron eigenvectors, and . Subtracting the two equations, I get , and I'm not sure how to proceed. The only conclusion I get from this equation is that is in the null space of .","\mathbf{X} \mathbf{X} \ne \mathbf{X}^T \mathbf{p}(\mathbf{A}) \mathbf{A} \rho(\mathbf{A}) \mathbf{q}(\mathbf{A}) \rho(\mathbf{A}) \mathbf{A}^T \mathbf{X} \ne \mathbf{X} ^ T \mathbf{p}(\mathbf{X}) \ne \alpha \mathbf{p}(\mathbf{X}^T) \alpha \mathbf{X}^T\in W_K(\mathbf{X}) \mathbf{X} \in X_K \rho((1-\mu) \mathbf{X} + \mu\mathbf{X}^T) \mu \in [0,1] \mathbf{X}\ne \mathbf{X}^T \mathbf{p}(\mathbf{X}) \ne \alpha \mathbf{p}(\mathbf{X^T}) \alpha X_K K K \mathbf{X}\in X_K W_K(\mathbf{X}) = \{\mathbf{Y}\in X_K : \mathbf{q}(Y)\circ \mathbf{p}(\mathbf{Y}) = \mathbf{q}(X)\circ \mathbf{p}(\mathbf{X}) \in \Pi_K^{+}\} \Pi_K^+ \mathbf{p}(\mathbf{X}^T)\equiv \mathbf{q}(\mathbf{X}) \mathbf{X} \mathbf{X}^T \mathbf{q}(\mathbf{X}) = \rho(\mathbf{X})\mathbf{q}(\mathbf{X}) \mathbf{X}\mathbf{q}(\mathbf{X}) = \rho(\mathbf{X})\mathbf{q}(\mathbf{X}) (\mathbf{X}-\mathbf{X}^T)\mathbf{q}(\mathbf{X}) = \mathbf{0} \mathbf{q}(\mathbf{X}) \mathbf{X} - \mathbf{X}^T","['linear-algebra', 'eigenvalues-eigenvectors', 'transpose']"
18,General formula for $f(n)$,General formula for,f(n),"Let for $n\geq 3, C_n$ denote the $(2n) \times (2n)$ matrix such that all entries along the diagonal are $2$ , all entries along the sub- and super-diagonal are $1$ , all entries along the antidiagonal are $1$ , all entries along the diagonals directly above and below the antidiagonal are $2$ , and all other entries are zero. Let $f(n) : \mathbb{N}\to\mathbb{N}, f(n) = \det(C_n)$ for $n\geq 3.$ Prove that $$f(n) = \begin{cases}0,&\text{if }n = 3k+2,\ k\in\mathbb{N}\\ 3^n,& \text{otherwise}\end{cases}.$$ I'm not sure how to go about doing this. I tried cofactor expansion along the first column, but I couldn't make much progress. I can't seem to find a recursive relationship. So I just tried converting $C_n$ to an upper triangular matrix using row operations. This results in a matrix satisfying certain patterns, but I can't seem to find a way to prove why reducing the matrix always produces these patterns (I can prove that the $k$ th diagonal entry of the resulting upper triangular matrix is $\frac{k+1}k,$ where $1\leq k\leq n$ but I can't deal with the other $n$ diagonal entries well).","Let for denote the matrix such that all entries along the diagonal are , all entries along the sub- and super-diagonal are , all entries along the antidiagonal are , all entries along the diagonals directly above and below the antidiagonal are , and all other entries are zero. Let for Prove that I'm not sure how to go about doing this. I tried cofactor expansion along the first column, but I couldn't make much progress. I can't seem to find a recursive relationship. So I just tried converting to an upper triangular matrix using row operations. This results in a matrix satisfying certain patterns, but I can't seem to find a way to prove why reducing the matrix always produces these patterns (I can prove that the th diagonal entry of the resulting upper triangular matrix is where but I can't deal with the other diagonal entries well).","n\geq 3, C_n (2n) \times (2n) 2 1 1 2 f(n) : \mathbb{N}\to\mathbb{N}, f(n) = \det(C_n) n\geq 3. f(n) = \begin{cases}0,&\text{if }n = 3k+2,\ k\in\mathbb{N}\\
3^n,& \text{otherwise}\end{cases}. C_n k \frac{k+1}k, 1\leq k\leq n n","['linear-algebra', 'matrices', 'determinant', 'sparse-matrices']"
19,Eigenvectors with many rational entries,Eigenvectors with many rational entries,,"Suppose that $A$ is an invertible $3 \times 3$ matrix with integral entries and that $v$ is an eigenvector corresponding to an irrational eigenvalue.  Of course by scaling $v$ we may arrange that it has at least one rational entry.  But is it possible for $v$ to have two rational entries, and only one irrational one? (Feel free to share any higher-dimensional results, of course!)","Suppose that is an invertible matrix with integral entries and that is an eigenvector corresponding to an irrational eigenvalue.  Of course by scaling we may arrange that it has at least one rational entry.  But is it possible for to have two rational entries, and only one irrational one? (Feel free to share any higher-dimensional results, of course!)",A 3 \times 3 v v v,['linear-algebra']
20,Any smart approach to finding a matrix given the characteristic polynomial?,Any smart approach to finding a matrix given the characteristic polynomial?,,"Find a matrix whose characteristic polynomial is $\newcommand{\l}{\lambda}\l^6-5\l^5+3\l^3+4\l^2-\l+5.$ Given this characteristic polynomial, I tried factorizing it to obtain its roots, then work from there to find an original matrix $A$ . However, here the roots seem either irrational or complex by rational root theorem, which makes this approach impossible. Are there other characteristics of the eigenvalues that I can use here to find an original matrix $A$ ?","Find a matrix whose characteristic polynomial is Given this characteristic polynomial, I tried factorizing it to obtain its roots, then work from there to find an original matrix . However, here the roots seem either irrational or complex by rational root theorem, which makes this approach impossible. Are there other characteristics of the eigenvalues that I can use here to find an original matrix ?",\newcommand{\l}{\lambda}\l^6-5\l^5+3\l^3+4\l^2-\l+5. A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'characteristic-polynomial']"
21,Do eigenvectors of a squared matrix A tell us anything about the eigenvectors of A?,Do eigenvectors of a squared matrix A tell us anything about the eigenvectors of A?,,I have a following task: Let $X$ be an eigenvector of $A$ . a) Prove that $X$ is also an eigenvector of $A^2$ b) Is the convere theorem true? I had no problem with the first part. Ive tried disproving the second part this way: Let $e$ be the eigenvalue of $A^2$ corresponding to the eigenvector $X$ . By definition we know that: $A^2X=eX \to (A^2-eI)X=0 \to (A-eI)(A+eI)X=0$ If im not mistaken that proves that $X$ is an eigenvector either for $A$ or $-A$ (since $X$ is a nonzero vector). How can I evaluate which of those two matrices share the common eigenvector with $A^2$ ? Thanks in advance!,I have a following task: Let be an eigenvector of . a) Prove that is also an eigenvector of b) Is the convere theorem true? I had no problem with the first part. Ive tried disproving the second part this way: Let be the eigenvalue of corresponding to the eigenvector . By definition we know that: If im not mistaken that proves that is an eigenvector either for or (since is a nonzero vector). How can I evaluate which of those two matrices share the common eigenvector with ? Thanks in advance!,X A X A^2 e A^2 X A^2X=eX \to (A^2-eI)X=0 \to (A-eI)(A+eI)X=0 X A -A X A^2,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
22,Derivative of Dot Product as matrix multiplication,Derivative of Dot Product as matrix multiplication,,I've come across this definition when looking into how to differentiate parameter vectors in statistics. Given $ \pmb{x}^{T} \pmb{x}$ $$\frac{\partial (\pmb{x}^{T} \pmb{x}) }{\partial \pmb{x}}=2\ \pmb{x}^{T}$$ And the proofs I've seen utilize the product rule while holding $\pmb{x}^{T}$ and $\pmb{x}$ constant. (Source: http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf ) $$\frac{\partial (\pmb{x}^{T} \pmb{x}) }{\partial \pmb{x}}=  \frac{\partial ({x}^{T} \pmb{x}) }{\partial \pmb{x}} + \frac{\partial (\pmb{x}^{T} x) }{\partial \pmb{x}}= \pmb{x}^{T} + \pmb{x}^{T} = 2\pmb{x}^{T} $$ I understand how we get $\pmb{x}^{T}$ if we differentiate $\frac{\partial ({x}^{T} \pmb{x}) }{\partial \pmb{x}}$ . What I'm not seeing is how we get $\frac{\partial (\pmb{x}^{T} x) }{\partial \pmb{x}} = \frac{\partial (\pmb{x}^{T}) }{\partial \pmb{x}}x=\pmb{x}^{T}$ Why is it that $\pmb{x}$ becomes $\pmb{x}^{T}$ when we differentiate $\pmb{x}^{T}$ with respect to $\pmb{x}$ ?,I've come across this definition when looking into how to differentiate parameter vectors in statistics. Given And the proofs I've seen utilize the product rule while holding and constant. (Source: http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf ) I understand how we get if we differentiate . What I'm not seeing is how we get Why is it that becomes when we differentiate with respect to ?, \pmb{x}^{T} \pmb{x} \frac{\partial (\pmb{x}^{T} \pmb{x}) }{\partial \pmb{x}}=2\ \pmb{x}^{T} \pmb{x}^{T} \pmb{x} \frac{\partial (\pmb{x}^{T} \pmb{x}) }{\partial \pmb{x}}=  \frac{\partial ({x}^{T} \pmb{x}) }{\partial \pmb{x}} + \frac{\partial (\pmb{x}^{T} x) }{\partial \pmb{x}}= \pmb{x}^{T} + \pmb{x}^{T} = 2\pmb{x}^{T}  \pmb{x}^{T} \frac{\partial ({x}^{T} \pmb{x}) }{\partial \pmb{x}} \frac{\partial (\pmb{x}^{T} x) }{\partial \pmb{x}} = \frac{\partial (\pmb{x}^{T}) }{\partial \pmb{x}}x=\pmb{x}^{T} \pmb{x} \pmb{x}^{T} \pmb{x}^{T} \pmb{x},"['linear-algebra', 'derivatives', 'vector-analysis', 'inner-products', 'matrix-calculus']"
23,Optimise allocation to minimise variance,Optimise allocation to minimise variance,,"Background I am trying to allocate customers $C_i$ to financial advisers $P_j$ . Each customer has a policy value $x_i$ . I'm assuming that the number of customers ( $n$ ) allocated to each adviser is the same, and that the same customer cannot be assigned to multiple advisers. Therefore each partner will have an allocation of policy values like so: $P_1 = [x_1,x_2,x_3]$ , $P_2 = [x_4,x_5,x_6]$ , $P_3 = [x_7,x_8,x_9]$ etc. The Problem After allocating customers, the average policy value is calculated for each adviser. I want to allocate customers to advisers in a way that minimises the variance in average policy values between each of the advisers. What I have tried My current algorithm randomly samples $n$ customers without replacement from the dataset and assigns each sample to an adviser. Once the allocations are made, the average policy value for each adviser is calculated and I compute the variance between each of the advisers. I repeat this over a defined number of iterations and return the allocation with the lowest variance. As expected, when dealing with larger volumes of advisers and customers this is not the most efficient process. My Question Is there an algorithm that converges towards the optimal allocation rather than randomly allocating on every iteration? How can I frame this problem as an objective function to minimise the variance? Any links to implementations (preferably Python) would be appreciated if possible","Background I am trying to allocate customers to financial advisers . Each customer has a policy value . I'm assuming that the number of customers ( ) allocated to each adviser is the same, and that the same customer cannot be assigned to multiple advisers. Therefore each partner will have an allocation of policy values like so: , , etc. The Problem After allocating customers, the average policy value is calculated for each adviser. I want to allocate customers to advisers in a way that minimises the variance in average policy values between each of the advisers. What I have tried My current algorithm randomly samples customers without replacement from the dataset and assigns each sample to an adviser. Once the allocations are made, the average policy value for each adviser is calculated and I compute the variance between each of the advisers. I repeat this over a defined number of iterations and return the allocation with the lowest variance. As expected, when dealing with larger volumes of advisers and customers this is not the most efficient process. My Question Is there an algorithm that converges towards the optimal allocation rather than randomly allocating on every iteration? How can I frame this problem as an objective function to minimise the variance? Any links to implementations (preferably Python) would be appreciated if possible","C_i P_j x_i n P_1 = [x_1,x_2,x_3] P_2 = [x_4,x_5,x_6] P_3 = [x_7,x_8,x_9] n","['linear-algebra', 'optimization', 'linear-programming', 'problem-solving', 'numerical-optimization']"
24,"Show that $B(X,Y^*)$ and $B(Y,X^*)$ are isometrically isomorphic.",Show that  and  are isometrically isomorphic.,"B(X,Y^*) B(Y,X^*)","If $X$ and $Y$ are normed spaces then we define. $$B(X,Y)= \{ f:X\rightarrow Y | f :\text{ f is a linear operator and bounded }\}$$ $X^*= \{f:X \rightarrow \mathbb{R}  | \text{ f is a linear operator and bounded }\}$ I only know Hanh-Banach Theorem. And I don't know how even start. Something that confuse me is how to send functions that send a vector in a linear functional in a function that send a vector in a functional. This seems super hard. How should I think about this?",If and are normed spaces then we define. I only know Hanh-Banach Theorem. And I don't know how even start. Something that confuse me is how to send functions that send a vector in a linear functional in a function that send a vector in a functional. This seems super hard. How should I think about this?,"X Y B(X,Y)= \{ f:X\rightarrow Y | f :\text{ f is a linear operator and bounded }\} X^*= \{f:X \rightarrow \mathbb{R}  | \text{ f is a linear operator and bounded }\}","['linear-algebra', 'functional-analysis', 'dual-spaces']"
25,What is the significance of the largest eigenvalue of a matrix?,What is the significance of the largest eigenvalue of a matrix?,,"The Tracy-Widom distribution gives the limiting distribution of the largest eigenvalue of a random matrix (in the $\beta$ -Hermite ensemble, where $\beta$ is 1,2 or 4). The second smallest eigenvalue of the Laplacian helps you divide the graph into communities, known as the algebraic connectivity... But what is so important about the largest eigenvalue of a matrix? Is it related to geometry? Or dynamics, where such a matrix may have a direct meaning?","The Tracy-Widom distribution gives the limiting distribution of the largest eigenvalue of a random matrix (in the -Hermite ensemble, where is 1,2 or 4). The second smallest eigenvalue of the Laplacian helps you divide the graph into communities, known as the algebraic connectivity... But what is so important about the largest eigenvalue of a matrix? Is it related to geometry? Or dynamics, where such a matrix may have a direct meaning?",\beta \beta,"['linear-algebra', 'probability', 'matrices', 'eigenvalues-eigenvectors']"
26,Is it true that the eigenvalues of $A + B$ are the sum of some eigenvalue of $A$ and some eigenvalue of $B$?,Is it true that the eigenvalues of  are the sum of some eigenvalue of  and some eigenvalue of ?,A + B A B,"Is it true that the eigenvalues of $A + B$ are the sum of some eigenvalue of $A$ and some eigenvalue of $B$ ? I'm taking a linear algebra class, and I recently learned about eigenvalues. I think that this claim is true, but it is not a theorem that I can find anywhere. I've tried it for many examples, and I just wanted someone to confirm. Thanks","Is it true that the eigenvalues of are the sum of some eigenvalue of and some eigenvalue of ? I'm taking a linear algebra class, and I recently learned about eigenvalues. I think that this claim is true, but it is not a theorem that I can find anywhere. I've tried it for many examples, and I just wanted someone to confirm. Thanks",A + B A B,['linear-algebra']
27,How to prove eigenvectors of a matrix exponential are the same as those for the matrix?,How to prove eigenvectors of a matrix exponential are the same as those for the matrix?,,"It's relatively easy to show that any eigenvector of an arbitrary $n \times n$ matrix $A$ is also an eigenvector of it's matrix exponential, $B = e^A$ . But how does one show the reverse is true: that any eigenvector of $B = e^A$ is also an eigenvector of $A$ ?","It's relatively easy to show that any eigenvector of an arbitrary matrix is also an eigenvector of it's matrix exponential, . But how does one show the reverse is true: that any eigenvector of is also an eigenvector of ?",n \times n A B = e^A B = e^A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,Exponential of a skew-skymmetric matrix,Exponential of a skew-skymmetric matrix,,"If $A$ is a skew-symmetrical matrix with it's diagonal elements as $0$ , Prove that it's exponent $e^A$ is an orthogonal matrix.","If is a skew-symmetrical matrix with it's diagonal elements as , Prove that it's exponent is an orthogonal matrix.",A 0 e^A,['linear-algebra']
29,"If 2x2 real matrices $A$, $B$ and $A-B$ are all idempotent, does this imply $AB=BA$?","If 2x2 real matrices ,  and  are all idempotent, does this imply ?",A B A-B AB=BA,"I am not sure if this is true. If $2 \times 2$ real matrices $A$ , $B$ and $A-B$ are all idempotent, does this imply $AB=BA$ ?   I can't yet complete the proof nor find a counter example.","I am not sure if this is true. If real matrices , and are all idempotent, does this imply ?   I can't yet complete the proof nor find a counter example.",2 \times 2 A B A-B AB=BA,"['linear-algebra', 'abstract-algebra']"
30,"Is there a variant of the dot-product operation that returns $\frac{a_1}{b_1} + \frac{a_2}{b_2}$ from vectors $[a_1,a_2]$ and $[b_1, b_2]$?",Is there a variant of the dot-product operation that returns  from vectors  and ?,"\frac{a_1}{b_1} + \frac{a_2}{b_2} [a_1,a_2] [b_1, b_2]","The dot product of $[a_1,a_2]$ and $[b_1, b_2]$ is $a_1b_1 + a_2b_2$ (and so on for bigger vectors). What I'm wondering is if there's any definition of a function s.t. the ""invdot"" product of $[a_1,a_2]$ and $[b_1, b_2]$ is $$\frac{a_1}{b_1} + \frac{a_2}{b_2}$$ If so, are there any other properties/derivations of this function from the dot product? I can see it's the same as taking the reciprocal of all the components of $b$ and then doing the dot product on that, but I can't understand how I'd do that in my specific use case. For context, I'm trying to solve a problem where I want to algebraically manipulate $a_1b_1 + a_2b_2$ to $\frac{a_1}{b_1} + \frac{a_2}{b_2}$ , and think that this might help if it's a well known operation.","The dot product of and is (and so on for bigger vectors). What I'm wondering is if there's any definition of a function s.t. the ""invdot"" product of and is If so, are there any other properties/derivations of this function from the dot product? I can see it's the same as taking the reciprocal of all the components of and then doing the dot product on that, but I can't understand how I'd do that in my specific use case. For context, I'm trying to solve a problem where I want to algebraically manipulate to , and think that this might help if it's a well known operation.","[a_1,a_2] [b_1, b_2] a_1b_1 + a_2b_2 [a_1,a_2] [b_1, b_2] \frac{a_1}{b_1} + \frac{a_2}{b_2} b a_1b_1 + a_2b_2 \frac{a_1}{b_1} + \frac{a_2}{b_2}","['linear-algebra', 'algebra-precalculus', 'vectors']"
31,Different definitions of an algebra over a commutative ring,Different definitions of an algebra over a commutative ring,,"Let $R$ be a commutative ring. Here are two definitions of an $R$ -algebra: An $R$ -algebra is a ring $A$ together with a ring homomorphism $f: R\to A$ (Atiyah) An $R$ -algebra is an $R$ -module $A$ together with an $R$ -bilinear mapping $\cdot: A\times A \to A$ ( https://proofwiki.org/wiki/Definition:Algebra_over_Ring ) Are these two equivalent? I can see that there is an $R$ -module structure on $A$ in the first definition; addition is given by addition in $A$ and multiplication by elts of $R$ is given by $(r,a)\mapsto f(r)a$ (the product in $A$ ). But definition 2 says that there is also an $R$ -bilinear mapping. What is it in the first definition?",Let be a commutative ring. Here are two definitions of an -algebra: An -algebra is a ring together with a ring homomorphism (Atiyah) An -algebra is an -module together with an -bilinear mapping ( https://proofwiki.org/wiki/Definition:Algebra_over_Ring ) Are these two equivalent? I can see that there is an -module structure on in the first definition; addition is given by addition in and multiplication by elts of is given by (the product in ). But definition 2 says that there is also an -bilinear mapping. What is it in the first definition?,"R R R A f: R\to A R R A R \cdot: A\times A \to A R A A R (r,a)\mapsto f(r)a A R","['linear-algebra', 'abstract-algebra', 'modules']"
32,Orthogonal matrices only defined for standard inner product?,Orthogonal matrices only defined for standard inner product?,,"$\newcommand{\tp}[1]{#1^\mathrm{T}} \newcommand{\Id}{\mathrm{Id}} \newcommand{\n}{\{1,\ldots,n\}} \newcommand{\siff}{\quad\Leftrightarrow\quad} \newcommand{\ijth}[2][\tp{Q}Q]{[#1]_{#2}} \newcommand{\K}{\mathbb{K}}$ Let orthogonal matrices be defined as follows. A matrix $Q\in\mathcal{M}_{m\times n}(\mathbb{K})$ , where $\mathbb{K}$ is a field, is said to be orthogonal if $$ Q^\mathrm{T}Q = \mathrm{Id}_n$$ I'm not fully sure if I'm understanding the following fact correctly: A matrix $Q\in\mathcal{M}_{m\times n}(\K)$ is orthogonal iff   the columns of $Q$ form an orthonormal set in $\K^m$ . Proof Let $q_i$ denote the $i$ -th column of $Q$ for all $i\in\{1,\ldots,n\}$ , and let $\ijth[A]{ij}$ denote the $(i,j)$ -th   element of $A$ for any matrix $A$ . Then, $Q$ being an orthogonal   matrix is equivalent to $$\tp{Q}Q = \Id_n \siff \ijth{ij} = \delta_{ij}\,,$$ where $\delta_{ij}$ is the Kronecker delta. On the   other hand, by the definition of matrix multiplication, $$\ijth{ij} = \sum_{k=1}^{m} \ijth[\tp{Q}]{ik}\ijth[{Q}]{kj} = \sum_{k=1}^{m} \ijth[Q]{ki}\ijth[{Q}]{kj} \stackrel{\color{red}*}{=} \langle q_i, q_j\rangle\,.$$ Thus $Q$ is orthogonal iff $$ \langle q_i, q_j\rangle = \delta_{ij} \qquad\forall (i,j)\in\n\times\n\,, $$ which is true iff $(q_i)_{i\in\n}$ form an orthonormal set. Particularly, I'm suspicious of the equality marked with the red asterisk. Isn't that true only for the standard inner product (i.e. the dot product), defined as $ \langle u, v \rangle = \sum_i u_iv_i\  $ ? So, are orthogonal matrices only treated in the context of the standard inner product? If so, is there a ""generalization"" of orthogonal matrices for general inner product spaces?","Let orthogonal matrices be defined as follows. A matrix , where is a field, is said to be orthogonal if I'm not fully sure if I'm understanding the following fact correctly: A matrix is orthogonal iff   the columns of form an orthonormal set in . Proof Let denote the -th column of for all , and let denote the -th   element of for any matrix . Then, being an orthogonal   matrix is equivalent to where is the Kronecker delta. On the   other hand, by the definition of matrix multiplication, Thus is orthogonal iff which is true iff form an orthonormal set. Particularly, I'm suspicious of the equality marked with the red asterisk. Isn't that true only for the standard inner product (i.e. the dot product), defined as ? So, are orthogonal matrices only treated in the context of the standard inner product? If so, is there a ""generalization"" of orthogonal matrices for general inner product spaces?","\newcommand{\tp}[1]{#1^\mathrm{T}} \newcommand{\Id}{\mathrm{Id}} \newcommand{\n}{\{1,\ldots,n\}} \newcommand{\siff}{\quad\Leftrightarrow\quad} \newcommand{\ijth}[2][\tp{Q}Q]{[#1]_{#2}} \newcommand{\K}{\mathbb{K}} Q\in\mathcal{M}_{m\times n}(\mathbb{K}) \mathbb{K}  Q^\mathrm{T}Q = \mathrm{Id}_n Q\in\mathcal{M}_{m\times n}(\K) Q \K^m q_i i Q i\in\{1,\ldots,n\} \ijth[A]{ij} (i,j) A A Q \tp{Q}Q = \Id_n \siff \ijth{ij} = \delta_{ij}\,, \delta_{ij} \ijth{ij} = \sum_{k=1}^{m} \ijth[\tp{Q}]{ik}\ijth[{Q}]{kj} = \sum_{k=1}^{m} \ijth[Q]{ki}\ijth[{Q}]{kj} \stackrel{\color{red}*}{=} \langle q_i, q_j\rangle\,. Q  \langle q_i, q_j\rangle = \delta_{ij} \qquad\forall (i,j)\in\n\times\n\,,  (q_i)_{i\in\n} 
\langle u, v \rangle = \sum_i u_iv_i\ 
","['linear-algebra', 'inner-products', 'orthogonal-matrices']"
33,How to give counterexample for given claim,How to give counterexample for given claim,,"Suppose $A_1,\dots,A_m$ be distinct $n\times n $ real matrices such that $A_iA_j=0$ for all $i\neq j$ . Show that $m\leq n$ . I think this true because i tried for $3\times 3$ and $2\times 2$ case I got only $3$ and $2$ matrices with that property. But given that this not true . Can any one help me to find counterexample. And what is best approach to tackle such problem.",Suppose be distinct real matrices such that for all . Show that . I think this true because i tried for and case I got only and matrices with that property. But given that this not true . Can any one help me to find counterexample. And what is best approach to tackle such problem.,"A_1,\dots,A_m n\times n  A_iA_j=0 i\neq j m\leq n 3\times 3 2\times 2 3 2","['linear-algebra', 'matrices', 'examples-counterexamples']"
34,centered finite difference approximation,centered finite difference approximation,,"Show that the centered finite difference approximation for the first derivative of a function on a uniform mesh yields the exact derivative for any quadratic polynomial $P_2(x) = a+bx+cx^2$ Using the taylor series: \begin{align} f(x+h) &= f(x) +f'(x)h + f''(x) \frac{h^2}{2}+f'''(x)\frac{h^3}{6}+ \dots \\ f(x-h) &= f(x) -f'(x)h + f''(x) \frac{h^2}{2}-f'''(x)\frac{h^3}{6}+ \dots \end{align} From the first equation, we get the forward difference approximation: $$f'(x) = \frac{f(x+h)-f(x)}{h}+O(h).$$ From the second equation, we get the backward difference approximation $$f'(x) = \frac{f(x)-f(x-h)}{h}+O(h),$$ and if we subtract the second equation from the first equation we get $$f'(x) = \frac{f(x+h)-f(x-h)}{2h}+O(h^2),$$ which is the central difference formula. But how do I use this to solve my problem?  Is this formula the same as the centered finite difference approximation mentioned in the question?","Show that the centered finite difference approximation for the first derivative of a function on a uniform mesh yields the exact derivative for any quadratic polynomial Using the taylor series: From the first equation, we get the forward difference approximation: From the second equation, we get the backward difference approximation and if we subtract the second equation from the first equation we get which is the central difference formula. But how do I use this to solve my problem?  Is this formula the same as the centered finite difference approximation mentioned in the question?","P_2(x) = a+bx+cx^2 \begin{align}
f(x+h) &= f(x) +f'(x)h + f''(x) \frac{h^2}{2}+f'''(x)\frac{h^3}{6}+ \dots \\
f(x-h) &= f(x) -f'(x)h + f''(x) \frac{h^2}{2}-f'''(x)\frac{h^3}{6}+ \dots
\end{align} f'(x) = \frac{f(x+h)-f(x)}{h}+O(h). f'(x) = \frac{f(x)-f(x-h)}{h}+O(h), f'(x) = \frac{f(x+h)-f(x-h)}{2h}+O(h^2),","['calculus', 'linear-algebra', 'numerical-methods']"
35,Dimension of $W_{2}$?,Dimension of ?,W_{2},"Let $A = \begin{bmatrix} 1 & -1 & -5 & 1 & 4\\ -1 & 2 & 8 & -3 & -4\\ 3 & -1 & -9 & 0 & 4 \\ 2 & 2 & 2 & -5 & -10\\ 0&-3&-9&5&13\end{bmatrix}$ Now we define the subspace $W_{1},W_{2}$ of $A$ as follows - $W_{1} = \{X \in M_{5 \times 5}| AX = 0\}$ $W_{2} = \{Y \in M_{5 \times 5} | YA =0\}$ I can see that $W_{1}$ is the nullspace of $A$ using rank nullity theorem I got Nullity of $A$ as $2$ since we have the rank of matrix $A$ to be 3. Now I am thinking about the dimension of $W_{2}$ ? As from the comments and we know that row rank = column rank, hence dim $(W_{2}) = 2$ But Now I am thinking about the dimension of $W_{1} \cap W_{2}$ and $W_{1} + W_{2}$ ? Any ideas?","Let Now we define the subspace of as follows - I can see that is the nullspace of using rank nullity theorem I got Nullity of as since we have the rank of matrix to be 3. Now I am thinking about the dimension of ? As from the comments and we know that row rank = column rank, hence dim But Now I am thinking about the dimension of and ? Any ideas?","A = \begin{bmatrix} 1 & -1 & -5 & 1 & 4\\ -1 & 2 & 8 & -3 & -4\\ 3 & -1 & -9 & 0 & 4 \\ 2 & 2 & 2 & -5 & -10\\ 0&-3&-9&5&13\end{bmatrix} W_{1},W_{2} A W_{1} = \{X \in M_{5 \times 5}| AX = 0\} W_{2} = \{Y \in M_{5 \times 5} | YA =0\} W_{1} A A 2 A W_{2} (W_{2}) = 2 W_{1} \cap W_{2} W_{1} + W_{2}","['linear-algebra', 'matrices', 'matrix-rank']"
36,"Solving a complex equation $Ax=b$ with $A,b\in\mathbb C$ using linear algebra",Solving a complex equation  with  using linear algebra,"Ax=b A,b\in\mathbb C","Consider the following very simple linear system with one unknown: \begin{equation}\label{a}\tag{1} Ax=b \\ \left ( 3+4i \right )x=(6+8i). \end{equation} This paper ("" On the numerical solving of complex linear systems "") says that I can solve the linear system by transforming A to matrix form and then solving it as follows: \begin{equation}\label{b}\tag{2} \begin{pmatrix} 3 & -4\\  4 & 3 \end{pmatrix} \binom{x_r}{x_c} = \binom{b_r}{b_c}, \end{equation} where $b_r = 6,b_c=8$ . Question: The translation of the A is fairly easy to understand. What I don't get is why b is not converted to matrix form yet solving the above system yields the correct answer. In other words, the following equations \begin{equation}\label{c}\tag{3} \left\{\begin{matrix} 3x_r-4x_c = 6\\  4x_r+3x_c = 8 \end{matrix}\right. \end{equation} makes no sense to me. Note that , I know how to solve the linear system. I'm looking for a detailed explanation of what's happening between (1) and (2) Thank you","Consider the following very simple linear system with one unknown: This paper ("" On the numerical solving of complex linear systems "") says that I can solve the linear system by transforming A to matrix form and then solving it as follows: where . Question: The translation of the A is fairly easy to understand. What I don't get is why b is not converted to matrix form yet solving the above system yields the correct answer. In other words, the following equations makes no sense to me. Note that , I know how to solve the linear system. I'm looking for a detailed explanation of what's happening between (1) and (2) Thank you","\begin{equation}\label{a}\tag{1}
Ax=b \\
\left ( 3+4i \right )x=(6+8i).
\end{equation} \begin{equation}\label{b}\tag{2}
\begin{pmatrix}
3 & -4\\ 
4 & 3
\end{pmatrix}
\binom{x_r}{x_c}
=
\binom{b_r}{b_c},
\end{equation} b_r = 6,b_c=8 \begin{equation}\label{c}\tag{3}
\left\{\begin{matrix}
3x_r-4x_c = 6\\ 
4x_r+3x_c = 8
\end{matrix}\right.
\end{equation}","['linear-algebra', 'complex-numbers', 'matrix-equations']"
37,Finding the eigenvalues of a linear transformation which takes inputs from the set of all $n\times n$ matrices.,Finding the eigenvalues of a linear transformation which takes inputs from the set of all  matrices.,n\times n,"We define $T(X) = AX - XB$ for fixed $A,B$ . We allow $X$ to be any matrix in $M_n(F)$ . Write down all the eigenvalues of $T$ in terms of the eigenvalues of $A$ and $B$ . I think I saw another question here which said that for $T(X) = AX - XA$ , if $u$ is an eigenvector for $A$ and $v$ an eigenvector for $A^T$ , then $uv^T$ is an eigenvector for $T$ , but I don't know how to prove this nor do I know if it generalizes if we replace one instance of $A$ with $B$ .","We define for fixed . We allow to be any matrix in . Write down all the eigenvalues of in terms of the eigenvalues of and . I think I saw another question here which said that for , if is an eigenvector for and an eigenvector for , then is an eigenvector for , but I don't know how to prove this nor do I know if it generalizes if we replace one instance of with .","T(X) = AX - XB A,B X M_n(F) T A B T(X) = AX - XA u A v A^T uv^T T A B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
38,Inverse of a specific matrix,Inverse of a specific matrix,,"Let $A$ be an $(n+1)\times(n+1)$ matrix defined by $a_{ij} = (i-1)^{j-1}$ , with the convention that $0^0 = 1$ . $$A =  \left[\begin{matrix} 0^0 & 0^1 & 0^2 & \ldots & 0^n\\ 1^0 & 1^1 & 1^2 & \ldots & 1^n\\ 2^0 & 2^1 & 2^2 & \ldots & 2^n\\ \vdots & \vdots & \vdots &\ddots & \vdots\\ n^0 & n^1 & n^2 & \ldots & n^n\\ \end{matrix}\right] $$ What is a closed form of $A^{-1}$ ? I don't know how to approach this problem. Any help will be appreciated.","Let be an matrix defined by , with the convention that . What is a closed form of ? I don't know how to approach this problem. Any help will be appreciated.","A (n+1)\times(n+1) a_{ij} = (i-1)^{j-1} 0^0 = 1 A = 
\left[\begin{matrix}
0^0 & 0^1 & 0^2 & \ldots & 0^n\\
1^0 & 1^1 & 1^2 & \ldots & 1^n\\
2^0 & 2^1 & 2^2 & \ldots & 2^n\\
\vdots & \vdots & \vdots &\ddots & \vdots\\
n^0 & n^1 & n^2 & \ldots & n^n\\
\end{matrix}\right]
 A^{-1}","['linear-algebra', 'combinatorics', 'matrices', 'algebra-precalculus']"
39,Can the adjugate matrix of $A$ be expressed as a polynomial of $A$?,Can the adjugate matrix of  be expressed as a polynomial of ?,A A,"We know that if $A$ is invertible, then $A^{-1}$ can be expressed as a polynomial of $A$ , that is to say, there exists a polynomial $f(x)$ such that $$ A^{-1} = f(A) $$ Of course in this case, $\operatorname{adj}(A)$ (the adjugate matrix of $A$ ) can also be expressed as a polynomial of $A$ . I wonder if $A$ is not invertible, can $\operatorname{adj}(A)$ be expressed as a polynomial of $A$ ？ Hope for your comments.","We know that if is invertible, then can be expressed as a polynomial of , that is to say, there exists a polynomial such that Of course in this case, (the adjugate matrix of ) can also be expressed as a polynomial of . I wonder if is not invertible, can be expressed as a polynomial of ？ Hope for your comments.","A A^{-1} A f(x) 
A^{-1} = f(A)
 \operatorname{adj}(A) A A A \operatorname{adj}(A) A","['linear-algebra', 'matrices', 'matrix-equations']"
40,Geometric intuition for the complex shoelace formula,Geometric intuition for the complex shoelace formula,,"The complex shoelace formula for the signed area of a triangle with vertices given by the complex numbers $a, b, c$ is $$\frac{i}{4}     \begin{vmatrix}     1 & 1 & 1 \\     a & b & c \\     \overline{a} & \overline{b} & \overline{c} \\     \end{vmatrix} $$ I have seen the algebraic proof for this formula using elementary row operations and the multilinear nature of the determinant, however that style of proof appears to imply that the simplicity of the final result (i.e. a determinant involving only conjugates) is a coincidence; furthermore it provides little intuition. I am looking for a way to understand this formula through a geometric/intuitive argument (not necessarily rigorous) in order to gain a deeper understanding rather than just accepting that the algebra works out.","The complex shoelace formula for the signed area of a triangle with vertices given by the complex numbers $a, b, c$ is $$\frac{i}{4}     \begin{vmatrix}     1 & 1 & 1 \\     a & b & c \\     \overline{a} & \overline{b} & \overline{c} \\     \end{vmatrix} $$ I have seen the algebraic proof for this formula using elementary row operations and the multilinear nature of the determinant, however that style of proof appears to imply that the simplicity of the final result (i.e. a determinant involving only conjugates) is a coincidence; furthermore it provides little intuition. I am looking for a way to understand this formula through a geometric/intuitive argument (not necessarily rigorous) in order to gain a deeper understanding rather than just accepting that the algebra works out.",,"['linear-algebra', 'geometry', 'complex-numbers', 'determinant', 'intuition']"
41,Find the geometry of the curves of the contour lines of $f(x) = \frac{1}{2}x^tAx + b^tx + c$,Find the geometry of the curves of the contour lines of,f(x) = \frac{1}{2}x^tAx + b^tx + c,"Find the geometry of the curves of the contour lines of a quadratic   function $$f(x) = \frac{1}{2}x^tAx + b^tx + c$$ where $A \in \mathbb{R}^{2 \times 2}$, $b\in \mathbb{R}^2$ and $c\in \mathbb{R}$ in the following cases: $A>0$ $A\ge 0$ and there exists $x$ such that $Ax+b = 0$ $A\ge 0$ and there is no $x$ such that $Ax + b = 0$ $A$ is undefined and non-singular. I suppose $^t$ is the transpose. What is $A>0$? I'm trying to develop a technique to see this. If we write $A = \begin{bmatrix}       a & b \\      b & c\\        \end{bmatrix}$ then the function becomes $$f((x_1,x_2)) = \frac{1}{2}\begin{bmatrix}       x_1  & x_2        \end{bmatrix}\begin{bmatrix}       a & b \\      b & c\\        \end{bmatrix}\begin{bmatrix}       x_1 \\      x_2\\        \end{bmatrix} + \begin{bmatrix}       b_1  & b_2        \end{bmatrix}\begin{bmatrix}       x_1 \\      x_2\\        \end{bmatrix} + d = \\ \frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2) + b_1x_1 + b_2x_2 + d $$ I don't know if I can simplify $(ax_1^2 + 2bx_1x_2 + cx_2^2)$. I think not. Maybe it's a shape of its own and I should recognize. I thin I can see it as approximate $x^2 + y^2$ everywhere since they grow much faster than $xy$. So the contour here would be circles? Is there a more accurate way of drawing the contour? I cannot suppose they're circles, I need to find what they truly are. Anyways, $A>0$ implies that $\frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2)>0$ right? And $A\ge 0 \implies \frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2)\ge 0$, and the condition there exists $x$ such that $Ax+b=0$ means there exists $x$ such that $\begin{bmatrix}       ax_1 + bx_2 + b_1 \\      ax_2 + bx_1 + b_2\\        \end{bmatrix} = \begin{bmatrix}       0 \\      0\\        \end{bmatrix}\implies ax_1 + bx_2 + b_1 = 0, ax_2 + bx_1 + b_2 = 0$ What this should tell me? $A$ being undefined and non singular means an arbitrary nonsingular matrix, I suppose. So the invertibility or the determinant of $A$ plays a role here.","Find the geometry of the curves of the contour lines of a quadratic   function $$f(x) = \frac{1}{2}x^tAx + b^tx + c$$ where $A \in \mathbb{R}^{2 \times 2}$, $b\in \mathbb{R}^2$ and $c\in \mathbb{R}$ in the following cases: $A>0$ $A\ge 0$ and there exists $x$ such that $Ax+b = 0$ $A\ge 0$ and there is no $x$ such that $Ax + b = 0$ $A$ is undefined and non-singular. I suppose $^t$ is the transpose. What is $A>0$? I'm trying to develop a technique to see this. If we write $A = \begin{bmatrix}       a & b \\      b & c\\        \end{bmatrix}$ then the function becomes $$f((x_1,x_2)) = \frac{1}{2}\begin{bmatrix}       x_1  & x_2        \end{bmatrix}\begin{bmatrix}       a & b \\      b & c\\        \end{bmatrix}\begin{bmatrix}       x_1 \\      x_2\\        \end{bmatrix} + \begin{bmatrix}       b_1  & b_2        \end{bmatrix}\begin{bmatrix}       x_1 \\      x_2\\        \end{bmatrix} + d = \\ \frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2) + b_1x_1 + b_2x_2 + d $$ I don't know if I can simplify $(ax_1^2 + 2bx_1x_2 + cx_2^2)$. I think not. Maybe it's a shape of its own and I should recognize. I thin I can see it as approximate $x^2 + y^2$ everywhere since they grow much faster than $xy$. So the contour here would be circles? Is there a more accurate way of drawing the contour? I cannot suppose they're circles, I need to find what they truly are. Anyways, $A>0$ implies that $\frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2)>0$ right? And $A\ge 0 \implies \frac{1}{2}(ax_1^2 + 2bx_1x_2 + cx_2^2)\ge 0$, and the condition there exists $x$ such that $Ax+b=0$ means there exists $x$ such that $\begin{bmatrix}       ax_1 + bx_2 + b_1 \\      ax_2 + bx_1 + b_2\\        \end{bmatrix} = \begin{bmatrix}       0 \\      0\\        \end{bmatrix}\implies ax_1 + bx_2 + b_1 = 0, ax_2 + bx_1 + b_2 = 0$ What this should tell me? $A$ being undefined and non singular means an arbitrary nonsingular matrix, I suppose. So the invertibility or the determinant of $A$ plays a role here.",,"['linear-algebra', 'matrices', 'geometry', 'systems-of-equations']"
42,Contraction of Tensors is Independent of the Choice of Basis,Contraction of Tensors is Independent of the Choice of Basis,,"Definition: Let $ T:(V^*)^k \times V^l \rightarrow \mathbb{R}$ be a tensor of type $ (k,l)$. Let $ \{v_1,...,v_n\}$ be a basis of $V$ and $ \{v^{1^*},...,v^{n^*}\}$ be the corresponding dual basis. The contraction of $ T$ with respect to the $ i$th (dual vector) and $j$th slot (which is a tensor of type $(k-1,l-1)$) is given by $\displaystyle CT=\sum_{k=1}^n T(\bullet,...,v^{k^*},...,\bullet; \bullet,...,v_k,...,\bullet)$ where the substitutions are done at the $i$th dual vector slot and the $j$th vector slot. Theorem: The contraction of a tensor is well-defined, i.e. it is independent of the choice of the basis of $V$. Proof: Recall that if $v \in V$, $w \in V^*$, then we have $$\displaystyle v=\sum_{k=1}^n v^{k^*}(v)v_k$$ $$\displaystyle w=\sum_{k=1}^n w(v_k)v^{k^*}$$ Let $\{v_1',...,v_n'\}$ be another basis of $V$ and $\{v^{{1^*}'},...,v^{{n^*}'}\}$ be the corresponding dual basis. Then we have $$\displaystyle v_i'=\sum_{k=1}^n v^{k^*}(v_i')v_k$$ $$\displaystyle v^{{i^*}'}=\sum_{k=1}^nv^{{i^*}'}(v_k)v^{k^*}$$ Hence, we get $$\begin{aligned} \displaystyle \sum_{k=1}^n T(\bullet,...,v^{{k^*}'},...,\bullet; \bullet,...,v_k',...,\bullet) &=\sum_{k=1}^n T(\bullet,...,\sum_{j=1}^nv^{{k^*}'}(v_j)v^{j^*},...,\bullet; \bullet,...,\sum_{p=1}^n v^{p^*}(v_k')v_p,...,\bullet)\\&= \sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{{k^*}'}(v_j)v^{p^*}(v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\\&=\sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{p^*}(v^{{k^*}'}(v_j)v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\end{aligned}$$ I have no idea how to proceed. Can anyone give me some suggestions? Thanks.","Definition: Let $ T:(V^*)^k \times V^l \rightarrow \mathbb{R}$ be a tensor of type $ (k,l)$. Let $ \{v_1,...,v_n\}$ be a basis of $V$ and $ \{v^{1^*},...,v^{n^*}\}$ be the corresponding dual basis. The contraction of $ T$ with respect to the $ i$th (dual vector) and $j$th slot (which is a tensor of type $(k-1,l-1)$) is given by $\displaystyle CT=\sum_{k=1}^n T(\bullet,...,v^{k^*},...,\bullet; \bullet,...,v_k,...,\bullet)$ where the substitutions are done at the $i$th dual vector slot and the $j$th vector slot. Theorem: The contraction of a tensor is well-defined, i.e. it is independent of the choice of the basis of $V$. Proof: Recall that if $v \in V$, $w \in V^*$, then we have $$\displaystyle v=\sum_{k=1}^n v^{k^*}(v)v_k$$ $$\displaystyle w=\sum_{k=1}^n w(v_k)v^{k^*}$$ Let $\{v_1',...,v_n'\}$ be another basis of $V$ and $\{v^{{1^*}'},...,v^{{n^*}'}\}$ be the corresponding dual basis. Then we have $$\displaystyle v_i'=\sum_{k=1}^n v^{k^*}(v_i')v_k$$ $$\displaystyle v^{{i^*}'}=\sum_{k=1}^nv^{{i^*}'}(v_k)v^{k^*}$$ Hence, we get $$\begin{aligned} \displaystyle \sum_{k=1}^n T(\bullet,...,v^{{k^*}'},...,\bullet; \bullet,...,v_k',...,\bullet) &=\sum_{k=1}^n T(\bullet,...,\sum_{j=1}^nv^{{k^*}'}(v_j)v^{j^*},...,\bullet; \bullet,...,\sum_{p=1}^n v^{p^*}(v_k')v_p,...,\bullet)\\&= \sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{{k^*}'}(v_j)v^{p^*}(v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\\&=\sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{p^*}(v^{{k^*}'}(v_j)v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\end{aligned}$$ I have no idea how to proceed. Can anyone give me some suggestions? Thanks.",,"['linear-algebra', 'differential-geometry', 'tensors', 'multilinear-algebra']"
43,"For $2 \times 2$ matrices $AB=-BA$ with $BA$ not $0$, prove that $\mathrm{tr}(A)=\mathrm{tr}(B)=\mathrm{tr}(AB)=0$","For  matrices  with  not , prove that",2 \times 2 AB=-BA BA 0 \mathrm{tr}(A)=\mathrm{tr}(B)=\mathrm{tr}(AB)=0,"It is easy to derive from $AB=-BA$ that $\mathrm{tr}(AB)=0$ since $\mathrm{tr}(AB)=\mathrm{tr}(-BA)=-\mathrm{tr}(BA)=-\mathrm{tr}(AB)$. However, I cannot get that $\mathrm{tr}(A)=\mathrm{tr}(B)=0$ without the fact that $A$ and $B$ are invertible. My Professor suggested I use the Cayley-Hamilton Theorem. However, that just gives me a few extra conditions on the elements of $A$ and $B$, and I still can't get that their traces equal $0$. Any ideas are greatly appreciated!","It is easy to derive from $AB=-BA$ that $\mathrm{tr}(AB)=0$ since $\mathrm{tr}(AB)=\mathrm{tr}(-BA)=-\mathrm{tr}(BA)=-\mathrm{tr}(AB)$. However, I cannot get that $\mathrm{tr}(A)=\mathrm{tr}(B)=0$ without the fact that $A$ and $B$ are invertible. My Professor suggested I use the Cayley-Hamilton Theorem. However, that just gives me a few extra conditions on the elements of $A$ and $B$, and I still can't get that their traces equal $0$. Any ideas are greatly appreciated!",,"['linear-algebra', 'matrices', 'trace', 'cayley-hamilton']"
44,An implication concerning $\operatorname{range}T\cap\operatorname{null}T = \{0\}$,An implication concerning,\operatorname{range}T\cap\operatorname{null}T = \{0\},Is the following argument correct? Proposition . Given a finite-dimensional vector space $V$ and a linear map $T:V\to V$ such that $\operatorname{rank}(T^2) = \operatorname{rank}(T)$ prove that $\operatorname{range}T\cap\operatorname{null}T = \{0\}$. Proof. Assume that there exists at least one non-zero vector $v\in\operatorname{range}T\cap\operatorname{null}T$ and let $U$ be the linear map formed by restricting the domain of $T$ to $\operatorname{range}T$ then the rank-nullity theorem implies  $$\dim\operatorname{range}T = \dim\operatorname{null}U+\dim\operatorname{range}U = \dim \operatorname{null}U+\dim\operatorname{range}T^2$$ Now since $\operatorname{range}T\cap\operatorname{null}T\subseteq\operatorname{null}U$ it follows that $v\in\operatorname{null}U$ and thus $\dim\operatorname{null}U\ge1$ consequently $\dim\operatorname{range}T^2<\dim\operatorname{range}T$. $\blacksquare$,Is the following argument correct? Proposition . Given a finite-dimensional vector space $V$ and a linear map $T:V\to V$ such that $\operatorname{rank}(T^2) = \operatorname{rank}(T)$ prove that $\operatorname{range}T\cap\operatorname{null}T = \{0\}$. Proof. Assume that there exists at least one non-zero vector $v\in\operatorname{range}T\cap\operatorname{null}T$ and let $U$ be the linear map formed by restricting the domain of $T$ to $\operatorname{range}T$ then the rank-nullity theorem implies  $$\dim\operatorname{range}T = \dim\operatorname{null}U+\dim\operatorname{range}U = \dim \operatorname{null}U+\dim\operatorname{range}T^2$$ Now since $\operatorname{range}T\cap\operatorname{null}T\subseteq\operatorname{null}U$ it follows that $v\in\operatorname{null}U$ and thus $\dim\operatorname{null}U\ge1$ consequently $\dim\operatorname{range}T^2<\dim\operatorname{range}T$. $\blacksquare$,,"['linear-algebra', 'proof-verification', 'linear-transformations']"
45,"symmetric $(2k)\times (2k)$ matrix, $k \geq 4$, consisting of $0$s and $1$s with exactly $k$ ones in each row and column","symmetric  matrix, , consisting of s and s with exactly  ones in each row and column",(2k)\times (2k) k \geq 4 0 1 k,"Let $k \geq 4$. Let $M=(m_{ij})_{1 \leq i,j\leq 2k}$ be a symmetric $(2k)\times (2k)$ matrix whose entries are either $0$s or $1$s and (1) All diagonal entries are zero, i.e., $m_{ii}=0$; (2) Every row and every column have precisely $k$ ones. (3) The total number of $1$s is $2k^2$. (4) Assume that  $$M \neq  \begin{bmatrix}0^{k\times k} &1^{k\times k}\\1^{k\times k} &0^{k\times k}\end{bmatrix}, $$ or any matrix that consists of an elementary permutation of this matrix (I could not think of a better way to say this). Suppose that $M_1, \ldots, M_{2k}$ are the row vectors (also the column vectors by symmetry) of $M$. I want to prove the existence of two integers $1 \leq i<j \leq 2k$ such that $m_{ij}=m_{ji}=1$ and $M_i \cdot M_j^T\geq 2$. I cannot find a counterexample for smaller cases like $k=4$ and $k=5$. I wanted to try to prove this by contradiction: Suppose that for each $1\leq i<j\leq 2k$ such that $m_{ij}=m_{ji}=1$ and $M_i \cdot M_j^T\in \{0,1\}$. Here is where I hit a wall. Any suggestions? I will include the graph theory equivalent problem: Let $k \geq 4$. Given a $k$-regular graph on $2k$ vertices and $k^2$ edges such that $G \not \cong K_{k,k}$, show that there exists an edge $xy \in E(G)$ such that $|N(x)\cap N(y)|\geq 2$, i.e., $x$ and $y$ have at least two neighbors. This is equivalent to showing that $G$ must contain a subgraph isomorphic to $K_4-e$. If it is of any help, there is a theorem by Paul Erdős that says that such a graph, as described above, should have at least $k-1$ triangles. I want to show that there exist two triangles that share an edge.","Let $k \geq 4$. Let $M=(m_{ij})_{1 \leq i,j\leq 2k}$ be a symmetric $(2k)\times (2k)$ matrix whose entries are either $0$s or $1$s and (1) All diagonal entries are zero, i.e., $m_{ii}=0$; (2) Every row and every column have precisely $k$ ones. (3) The total number of $1$s is $2k^2$. (4) Assume that  $$M \neq  \begin{bmatrix}0^{k\times k} &1^{k\times k}\\1^{k\times k} &0^{k\times k}\end{bmatrix}, $$ or any matrix that consists of an elementary permutation of this matrix (I could not think of a better way to say this). Suppose that $M_1, \ldots, M_{2k}$ are the row vectors (also the column vectors by symmetry) of $M$. I want to prove the existence of two integers $1 \leq i<j \leq 2k$ such that $m_{ij}=m_{ji}=1$ and $M_i \cdot M_j^T\geq 2$. I cannot find a counterexample for smaller cases like $k=4$ and $k=5$. I wanted to try to prove this by contradiction: Suppose that for each $1\leq i<j\leq 2k$ such that $m_{ij}=m_{ji}=1$ and $M_i \cdot M_j^T\in \{0,1\}$. Here is where I hit a wall. Any suggestions? I will include the graph theory equivalent problem: Let $k \geq 4$. Given a $k$-regular graph on $2k$ vertices and $k^2$ edges such that $G \not \cong K_{k,k}$, show that there exists an edge $xy \in E(G)$ such that $|N(x)\cap N(y)|\geq 2$, i.e., $x$ and $y$ have at least two neighbors. This is equivalent to showing that $G$ must contain a subgraph isomorphic to $K_4-e$. If it is of any help, there is a theorem by Paul Erdős that says that such a graph, as described above, should have at least $k-1$ triangles. I want to show that there exist two triangles that share an edge.",,"['linear-algebra', 'combinatorics', 'matrices', 'discrete-mathematics', 'graph-theory']"
46,"Area of parallelograms in $\mathbb{R}^n$ (or more generally, the volume of parallelotopes in $\mathbb{R}^n$ )","Area of parallelograms in  (or more generally, the volume of parallelotopes in  )",\mathbb{R}^n \mathbb{R}^n,"It is known that in $\mathbb{R}^2$, the area of the parallelogram spanned by two vectors $(a,b)$ and $(d,e)$ is given by $$A=\begin{vmatrix} a&b\\d&e \end{vmatrix}$$ while in $\mathbb{R}^3$, the area of the parallelogram spanned by two vectors $(a,b,c)$ and $(d,e,f)$ is given by the norm of the cross product of $(a,b,c)$ and $(d,e,f)$. The two dimension formula can be easily proved from the three dimensional case by setting $c=f=0$. How do we generalise this to the $n-$dimensional space? Namely, what   is the area of the parallelogram spanned by two vectors   $(x_1,\ldots,x_n)$ and $(y_1,\ldots,y_n)$? A more generalised question: Given two positive integers $k$ and $n$, what is the $k-$dimensional   volume of the parallelotope spanned by $k$ vectors   $(a_{11},\ldots,a_{1n}),\ldots,(a_{k1},\ldots,a_{kn})$? I understand that in the special case that $k=n$, it is given by an $n\times n$ determinant. But how about the case that $k\neq n$? Edit : In the two dimensional case, the area should be the absolute value of $\begin{vmatrix} a&b\\d&e \end{vmatrix}$.","It is known that in $\mathbb{R}^2$, the area of the parallelogram spanned by two vectors $(a,b)$ and $(d,e)$ is given by $$A=\begin{vmatrix} a&b\\d&e \end{vmatrix}$$ while in $\mathbb{R}^3$, the area of the parallelogram spanned by two vectors $(a,b,c)$ and $(d,e,f)$ is given by the norm of the cross product of $(a,b,c)$ and $(d,e,f)$. The two dimension formula can be easily proved from the three dimensional case by setting $c=f=0$. How do we generalise this to the $n-$dimensional space? Namely, what   is the area of the parallelogram spanned by two vectors   $(x_1,\ldots,x_n)$ and $(y_1,\ldots,y_n)$? A more generalised question: Given two positive integers $k$ and $n$, what is the $k-$dimensional   volume of the parallelotope spanned by $k$ vectors   $(a_{11},\ldots,a_{1n}),\ldots,(a_{k1},\ldots,a_{kn})$? I understand that in the special case that $k=n$, it is given by an $n\times n$ determinant. But how about the case that $k\neq n$? Edit : In the two dimensional case, the area should be the absolute value of $\begin{vmatrix} a&b\\d&e \end{vmatrix}$.",,"['linear-algebra', 'area', 'volume']"
47,Eigenvalues without any calculations,Eigenvalues without any calculations,,"Question is from Intro to Linear Algebra (5th Ed) by Gilbert Strang, Chapter 6-39. Without writing down any calculations, can you find the eigenvalues of this matrix?  Also find $A^{2017}$. $$	A = \begin{Bmatrix} 		110 & 55 & -164\\ 		42 & 21 & -62\\ 		88 & 44 & -131 	\end{Bmatrix} $$ Obviously one of the eigenvalues is $0$.  Not sure how to find the rest without calculation.","Question is from Intro to Linear Algebra (5th Ed) by Gilbert Strang, Chapter 6-39. Without writing down any calculations, can you find the eigenvalues of this matrix?  Also find $A^{2017}$. $$	A = \begin{Bmatrix} 		110 & 55 & -164\\ 		42 & 21 & -62\\ 		88 & 44 & -131 	\end{Bmatrix} $$ Obviously one of the eigenvalues is $0$.  Not sure how to find the rest without calculation.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
48,"Why does computation of the determinant have anything to do with the ""volume"" of the fundamental ""parallelepiped""?","Why does computation of the determinant have anything to do with the ""volume"" of the fundamental ""parallelepiped""?",,"By the ""volume"" of ""parallelepiped"", I mean the Lebesgue measure of n-Parallelotope. If I have $$\vec{v_i}=\begin{bmatrix}a_{1i} \\ a_{2i} \\ \vdots \\ a_{ni}\end{bmatrix} \qquad \text{ for } i\in\{1,2,3\ldots,n\}$$ and $$\mathbf A=\begin{bmatrix}\vec{v_1} & \vec{v_2} & \cdots & \vec{v_n}\end{bmatrix}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\ a_{21} & a_{22} & \cdots & a_{2n}\\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}\end{bmatrix}$$ Now there are two ways to define the determinant. Definition 1: If $\mathbf C_{ij}$ is the cofactor, then $$\det(\mathbf A)=\sum_{k=1}^{n}a_{ik}C_{ik}=\sum_{k=1}^{n}a_{kj}C_{kj} \qquad \text{for any } i,j\in\{1,2,3,\ldots,n\}$$ Definition 2: $\det(\mathbf A)$ is the Lebesgue measure of the fundamental n-Parallelotope spanned by the the column vectors $\vec{v_i}\in\mathbb R^n$. How do I prove that the two definitions are equivalent? I personally like definition 2 because, I can visualize it but in definition 1, first we need to show that the summations are giving same values for all i and j. I can use the second definition definition for n=2, first thing I noted was that column operations does not change the area of the parallelogram because of simple geometry properties. Thus, $$\begin{vmatrix}a & c\\ b & d\end{vmatrix}=\begin{vmatrix}a & c-a\frac{c}{a}\\ b & d-b\frac{c}{a}\end{vmatrix}=\begin{vmatrix}a & 0\\ b & \frac{ad-bc}{a}\end{vmatrix}=\begin{vmatrix}a-0\frac{ab}{ad-bc} & 0\\ b-\frac{ad-bc}{a}\frac{ab}{ad-bc} & \frac{ad-bc}{a}\end{vmatrix}=\begin{vmatrix}a & 0\\ 0 & \frac{ad-bc}{a}\end{vmatrix}$$ This turns it into a rectangle whose area can be calculated easily. $$\begin{vmatrix}a & c\\ b & d\end{vmatrix}=ad-bc$$ But this is the same as we get from definition 1. Thus both definitions are equivalent for n=2. The argument in finding determinant for n=2 by the second definition generalizes easily, but the method of computation feels completely different as compared to definition 1. Like for n=3, I got $$\begin{vmatrix}a&d&g\\b&e&h\\c&f&i\end{vmatrix}=\begin{vmatrix}\frac{a(ei-hf)-d(bi-ch)-g(ec-bf)}{ei-hf}&0&0\\0&\frac{ei-hf}{i}&0\\0&0&i\end{vmatrix}=a(ei-hf)-d(bi-ch)+g(bf-ec)$$ I can see a little bit connection for $i=1$ in definition 1. I got to know that definition 1 is called Laplace Expansion , but the proof written on Wikipedia went above my tiny brain. I am in 11th grade and I know very little about Linear algebra(I know only the stuff Grant Sanderson told in his essence of LA playlist). After reading the answer Determinant of transpose I can make sense why row operations do not change the determinant as well. I would be really happy if someone proves definition 1 using definition 2.","By the ""volume"" of ""parallelepiped"", I mean the Lebesgue measure of n-Parallelotope. If I have $$\vec{v_i}=\begin{bmatrix}a_{1i} \\ a_{2i} \\ \vdots \\ a_{ni}\end{bmatrix} \qquad \text{ for } i\in\{1,2,3\ldots,n\}$$ and $$\mathbf A=\begin{bmatrix}\vec{v_1} & \vec{v_2} & \cdots & \vec{v_n}\end{bmatrix}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\ a_{21} & a_{22} & \cdots & a_{2n}\\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}\end{bmatrix}$$ Now there are two ways to define the determinant. Definition 1: If $\mathbf C_{ij}$ is the cofactor, then $$\det(\mathbf A)=\sum_{k=1}^{n}a_{ik}C_{ik}=\sum_{k=1}^{n}a_{kj}C_{kj} \qquad \text{for any } i,j\in\{1,2,3,\ldots,n\}$$ Definition 2: $\det(\mathbf A)$ is the Lebesgue measure of the fundamental n-Parallelotope spanned by the the column vectors $\vec{v_i}\in\mathbb R^n$. How do I prove that the two definitions are equivalent? I personally like definition 2 because, I can visualize it but in definition 1, first we need to show that the summations are giving same values for all i and j. I can use the second definition definition for n=2, first thing I noted was that column operations does not change the area of the parallelogram because of simple geometry properties. Thus, $$\begin{vmatrix}a & c\\ b & d\end{vmatrix}=\begin{vmatrix}a & c-a\frac{c}{a}\\ b & d-b\frac{c}{a}\end{vmatrix}=\begin{vmatrix}a & 0\\ b & \frac{ad-bc}{a}\end{vmatrix}=\begin{vmatrix}a-0\frac{ab}{ad-bc} & 0\\ b-\frac{ad-bc}{a}\frac{ab}{ad-bc} & \frac{ad-bc}{a}\end{vmatrix}=\begin{vmatrix}a & 0\\ 0 & \frac{ad-bc}{a}\end{vmatrix}$$ This turns it into a rectangle whose area can be calculated easily. $$\begin{vmatrix}a & c\\ b & d\end{vmatrix}=ad-bc$$ But this is the same as we get from definition 1. Thus both definitions are equivalent for n=2. The argument in finding determinant for n=2 by the second definition generalizes easily, but the method of computation feels completely different as compared to definition 1. Like for n=3, I got $$\begin{vmatrix}a&d&g\\b&e&h\\c&f&i\end{vmatrix}=\begin{vmatrix}\frac{a(ei-hf)-d(bi-ch)-g(ec-bf)}{ei-hf}&0&0\\0&\frac{ei-hf}{i}&0\\0&0&i\end{vmatrix}=a(ei-hf)-d(bi-ch)+g(bf-ec)$$ I can see a little bit connection for $i=1$ in definition 1. I got to know that definition 1 is called Laplace Expansion , but the proof written on Wikipedia went above my tiny brain. I am in 11th grade and I know very little about Linear algebra(I know only the stuff Grant Sanderson told in his essence of LA playlist). After reading the answer Determinant of transpose I can make sense why row operations do not change the determinant as well. I would be really happy if someone proves definition 1 using definition 2.",,"['linear-algebra', 'matrices']"
49,Effect of adding a zero row and column on the eigenvalues of a matrix,Effect of adding a zero row and column on the eigenvalues of a matrix,,"Suppose I have a matrix, and for some $n\in \mathbb{N}$, I insert a row of zeros between the $n$th and $n+1$th rows, and a column of zeroes between the $n$th and $n+1$the columns. For example, for $n=1$ and the matrix: $$A=\begin{bmatrix} 5&7&9 \\ 7&1&1 \\ 9&1&3 \\ \end{bmatrix}$$ I would obtain: $$B=\begin{bmatrix} 5&0&7&9 \\ 0&0&0&0 \\ 7&0&1&1 \\ 9&0&1&3 \\ \end{bmatrix}$$ In general, what effect does this have on the eigenvalues? If we subtract $\lambda I$ and cofactor expand along the zero row or zero column of a matrix transformed in this way, clearly zero must be an eigenvalue. From this, and playing around with some matrices, I believe the following: If $A$ is a singular matrix, then the eigenvalues of $B$ are the same as the eigenvalues of $A$. If $A$ is an invertible matrix, then the eigenvalues of $B$ are the eigenvalues of $A$ as well as 0. Is this claim true, and how would I prove it if so?","Suppose I have a matrix, and for some $n\in \mathbb{N}$, I insert a row of zeros between the $n$th and $n+1$th rows, and a column of zeroes between the $n$th and $n+1$the columns. For example, for $n=1$ and the matrix: $$A=\begin{bmatrix} 5&7&9 \\ 7&1&1 \\ 9&1&3 \\ \end{bmatrix}$$ I would obtain: $$B=\begin{bmatrix} 5&0&7&9 \\ 0&0&0&0 \\ 7&0&1&1 \\ 9&0&1&3 \\ \end{bmatrix}$$ In general, what effect does this have on the eigenvalues? If we subtract $\lambda I$ and cofactor expand along the zero row or zero column of a matrix transformed in this way, clearly zero must be an eigenvalue. From this, and playing around with some matrices, I believe the following: If $A$ is a singular matrix, then the eigenvalues of $B$ are the same as the eigenvalues of $A$. If $A$ is an invertible matrix, then the eigenvalues of $B$ are the eigenvalues of $A$ as well as 0. Is this claim true, and how would I prove it if so?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
50,'Diagonalization' of Jordan block,'Diagonalization' of Jordan block,,"We know the Jordan block  $$J =  \begin{bmatrix} 	\lambda & 1 & & \\ 	& \lambda & \ddots & \\ 	& & \ddots & 1\\ 	& & & \lambda        \end{bmatrix} $$ Prove that there exists an invertible $\bf{S}$ such that $$\bf{SJS^{-1}} =  \begin{bmatrix} 	\lambda & \varepsilon & & \\ 	& \lambda & \ddots & \\ 	& & \ddots & \varepsilon\\ 	& & & \lambda        \end{bmatrix} $$  with any nonzero $\varepsilon$. I have no idea how to construct the $\bf{S}$, or how to 'diagonalize' the Jordan block. Could anyone give me some hints? Thanks in advance!","We know the Jordan block  $$J =  \begin{bmatrix} 	\lambda & 1 & & \\ 	& \lambda & \ddots & \\ 	& & \ddots & 1\\ 	& & & \lambda        \end{bmatrix} $$ Prove that there exists an invertible $\bf{S}$ such that $$\bf{SJS^{-1}} =  \begin{bmatrix} 	\lambda & \varepsilon & & \\ 	& \lambda & \ddots & \\ 	& & \ddots & \varepsilon\\ 	& & & \lambda        \end{bmatrix} $$  with any nonzero $\varepsilon$. I have no idea how to construct the $\bf{S}$, or how to 'diagonalize' the Jordan block. Could anyone give me some hints? Thanks in advance!",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'jordan-normal-form']"
51,Are only normal matrices diagonalizable?,Are only normal matrices diagonalizable?,,"For a square $n\times n$ matrix to be diagonalizable, it needs to have $n$ linearly independent eigenvectors. If and only if a matrix is normal can the $n$ eigen-vectors be made to form an orthonormal basis. $n$ linearly independent vectors can always be made to form a orthonormal basis Does this mean that only normal matrices are diagonalizable?","For a square matrix to be diagonalizable, it needs to have linearly independent eigenvectors. If and only if a matrix is normal can the eigen-vectors be made to form an orthonormal basis. linearly independent vectors can always be made to form a orthonormal basis Does this mean that only normal matrices are diagonalizable?",n\times n n n n,"['linear-algebra', 'matrices', 'linear-transformations', 'diagonalization']"
52,Let $A$ be an $m\times n$ matrix. Prove that $\operatorname{rank}(AA^T) = \operatorname{rank}(A)$.,Let  be an  matrix. Prove that .,A m\times n \operatorname{rank}(AA^T) = \operatorname{rank}(A),"Let $A$ be an $m\times n$ matrix. Prove that $\operatorname{rank}(AA^T) = \operatorname{rank}(A)$ . The problem tells me to prove it with the theorem that $\operatorname{rank}(A^TA) = \operatorname{rank}(A)$ . I'm a bit lost here... $AA^T$ and $A$ don't even have the same number of columns. I'm thinking maybe to prove it by showing that $m - \operatorname{nullity}(AA^T) = n - \operatorname{nullity}(A),$ but then I'm stuck here. Let $A$ be an $m\times n$ matrix. Prove that the column space and row space of $A^TA$ are the same. The problem tells me to prove it also with the theorem $\operatorname{rank}(A^TA) = \operatorname{rank}(A)$ . But I'm really running out of ideas. Help?",Let be an matrix. Prove that . The problem tells me to prove it with the theorem that . I'm a bit lost here... and don't even have the same number of columns. I'm thinking maybe to prove it by showing that but then I'm stuck here. Let be an matrix. Prove that the column space and row space of are the same. The problem tells me to prove it also with the theorem . But I'm really running out of ideas. Help?,"A m\times n \operatorname{rank}(AA^T) = \operatorname{rank}(A) \operatorname{rank}(A^TA) = \operatorname{rank}(A) AA^T A m - \operatorname{nullity}(AA^T) = n - \operatorname{nullity}(A), A m\times n A^TA \operatorname{rank}(A^TA) = \operatorname{rank}(A)","['linear-algebra', 'matrix-rank', 'transpose']"
53,Positive eigenvectors for nonnegative matrices,Positive eigenvectors for nonnegative matrices,,"Let $ A $ a nonnegative matrix (i.e. all the entries of $ A $ are real, nonnegative) of dimensions $ n \times n $. Is it true that the conditions: $ A x_1 = \lambda_1 x_1 $ $ A x_2 = \lambda_2 x_2 $ $ x_1 \gg 0, x_2 \gg 0 $ (i.e. all the components of $ x_1 $ and $ x_2 $ are strictly positive) imply $ \lambda_1 = \lambda_2 $? (obviously $ \lambda_1, \lambda_2 $ are real scalars, while $ x_1, x_2 $ are real vectors).","Let $ A $ a nonnegative matrix (i.e. all the entries of $ A $ are real, nonnegative) of dimensions $ n \times n $. Is it true that the conditions: $ A x_1 = \lambda_1 x_1 $ $ A x_2 = \lambda_2 x_2 $ $ x_1 \gg 0, x_2 \gg 0 $ (i.e. all the components of $ x_1 $ and $ x_2 $ are strictly positive) imply $ \lambda_1 = \lambda_2 $? (obviously $ \lambda_1, \lambda_2 $ are real scalars, while $ x_1, x_2 $ are real vectors).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
54,For an orthonormal basis and a linear transformation $T$ which is not possible,For an orthonormal basis and a linear transformation  which is not possible,T,"Let, $H$ be a Hilbert space and let, $\{e_n:n \ge 1\}$ be an orthonormal basis of $H$. Suppose that $T:H\to H$ be a bounded linear operator. Then which of the following CANNOT be true: (A) $T(e_n)=e_1$. (B) $T(e_n)=e_{n+1}$ (C) $T(e_n)=\sqrt\frac{n+1}{n}e_n$ (D) $T(e_n)=e_{n-1}$ for all $n \ge 2$ and $T(e_1)=0$ How to start? Any hint?","Let, $H$ be a Hilbert space and let, $\{e_n:n \ge 1\}$ be an orthonormal basis of $H$. Suppose that $T:H\to H$ be a bounded linear operator. Then which of the following CANNOT be true: (A) $T(e_n)=e_1$. (B) $T(e_n)=e_{n+1}$ (C) $T(e_n)=\sqrt\frac{n+1}{n}e_n$ (D) $T(e_n)=e_{n-1}$ for all $n \ge 2$ and $T(e_1)=0$ How to start? Any hint?",,"['linear-algebra', 'functional-analysis', 'analysis', 'hilbert-spaces']"
55,Calculate percentage of point in range that includes negative numbers,Calculate percentage of point in range that includes negative numbers,,"I'm writing a computer program with a user interface that contains sliders with differing ranges. The range can be only positive, e.g. [1, 2] , or contain negative values, e.g. [-1,1] . So I need to be able to calculate the percentage of a number inside of a range, where the numbers can be negative or positive. For example I can have a range [-127, 127] , and if the value is 0 , it would be 50% . Another example using only positive numbers would be [0, 127] , where 0 would be 0% , but 63.5 would be 50% . I would also like to be be able to calculate a number on a range from a percentage, so I think this would be the inverse. I've been able to write functions that work for example 1 or 2, but not both. Introducing the negative numbers seems to add a lot of complexity (at least or me!) Many thanks.","I'm writing a computer program with a user interface that contains sliders with differing ranges. The range can be only positive, e.g. [1, 2] , or contain negative values, e.g. [-1,1] . So I need to be able to calculate the percentage of a number inside of a range, where the numbers can be negative or positive. For example I can have a range [-127, 127] , and if the value is 0 , it would be 50% . Another example using only positive numbers would be [0, 127] , where 0 would be 0% , but 63.5 would be 50% . I would also like to be be able to calculate a number on a range from a percentage, so I think this would be the inverse. I've been able to write functions that work for example 1 or 2, but not both. Introducing the negative numbers seems to add a lot of complexity (at least or me!) Many thanks.",,"['linear-algebra', 'numerical-methods']"
56,Basis of $\mathbb{R}$ over $\mathbb{Q}$ - reversing Zorn's Lemma,Basis of  over  - reversing Zorn's Lemma,\mathbb{R} \mathbb{Q},"I was trying to prove the following statement: For $S \subseteq \mathbb{R}$, define $\mathbb{Q}(S)$ by: $$ \mathbb{Q}(S) = \{q_0s_0 + \cdots + q_ks_k : k \in \omega, q_i \in \mathbb{Q}, \text{ and } s_i \in S \text{ for } i \leq k\} $$ $B \subseteq \mathbb{R}$ is a basis for $\mathbb{R}$ over $\mathbb{Q}$ if $B$ is $\subseteq$-minimal such that $\mathbb{R} = \mathbb{Q}(B)$, i.e. $\mathbb{R} = \mathbb{Q}(B)$ and for all $B' \subseteq B$ if $\mathbb{R} = \mathbb{Q}(B')$ then $B = B'$. Now I can imagine you can prove this by just noting that $\mathbb{R}$ is a vector space over $\mathbb{Q}$ and thus has a basis in the linear algebra sense and showing this aligns with the basis being $\subseteq$-minimal. However, I came up with this alternate proof and was wondering about its validity. Let $S_0 =\mathbb{R}$. It is evident that $\mathbb{Q}(S_0) = \mathbb{R}$, so if this is not a basis then there must be some $S_1$ s.t. $S_0 \supset S_1$ and $\mathbb{Q}(S_1) = \mathbb{R}$. Iterating this, either we will get a basis or we will have a chain $S_0 \supset S_1 \supset \ldots $ s.t. for any $i \in \mathbb{N}$, we have $\mathbb{Q}(S_i) = \mathbb{R}$. Then we can take the intersection $S_\omega = \bigcap\{S_i : i \in \mathbb{N} \}$ and  $\mathbb{Q}(S_\omega) = \mathbb{R}$. Eventually we have to hit a basis otherwise we will exhaust all the elements in the set since we are indexing by ordinals. This argument is exactly Zorn's lemma deconstructed for the $\supset$ relation and I was wondering if the reasoning is valid. More formally, you would consider the set $A = \{X : \mathbb{Q}(X) = \mathbb{R}\}$ and show that for any chain $B \subseteq A$, we have $\bigcap B \in A$ (i.e. $B$ has an upper bound) and thus the set $A$ must have a maximal element $S$, which is a basis for $\mathbb{R}$ over $\mathbb{Q}$.","I was trying to prove the following statement: For $S \subseteq \mathbb{R}$, define $\mathbb{Q}(S)$ by: $$ \mathbb{Q}(S) = \{q_0s_0 + \cdots + q_ks_k : k \in \omega, q_i \in \mathbb{Q}, \text{ and } s_i \in S \text{ for } i \leq k\} $$ $B \subseteq \mathbb{R}$ is a basis for $\mathbb{R}$ over $\mathbb{Q}$ if $B$ is $\subseteq$-minimal such that $\mathbb{R} = \mathbb{Q}(B)$, i.e. $\mathbb{R} = \mathbb{Q}(B)$ and for all $B' \subseteq B$ if $\mathbb{R} = \mathbb{Q}(B')$ then $B = B'$. Now I can imagine you can prove this by just noting that $\mathbb{R}$ is a vector space over $\mathbb{Q}$ and thus has a basis in the linear algebra sense and showing this aligns with the basis being $\subseteq$-minimal. However, I came up with this alternate proof and was wondering about its validity. Let $S_0 =\mathbb{R}$. It is evident that $\mathbb{Q}(S_0) = \mathbb{R}$, so if this is not a basis then there must be some $S_1$ s.t. $S_0 \supset S_1$ and $\mathbb{Q}(S_1) = \mathbb{R}$. Iterating this, either we will get a basis or we will have a chain $S_0 \supset S_1 \supset \ldots $ s.t. for any $i \in \mathbb{N}$, we have $\mathbb{Q}(S_i) = \mathbb{R}$. Then we can take the intersection $S_\omega = \bigcap\{S_i : i \in \mathbb{N} \}$ and  $\mathbb{Q}(S_\omega) = \mathbb{R}$. Eventually we have to hit a basis otherwise we will exhaust all the elements in the set since we are indexing by ordinals. This argument is exactly Zorn's lemma deconstructed for the $\supset$ relation and I was wondering if the reasoning is valid. More formally, you would consider the set $A = \{X : \mathbb{Q}(X) = \mathbb{R}\}$ and show that for any chain $B \subseteq A$, we have $\bigcap B \in A$ (i.e. $B$ has an upper bound) and thus the set $A$ must have a maximal element $S$, which is a basis for $\mathbb{R}$ over $\mathbb{Q}$.",,"['linear-algebra', 'set-theory', 'axiom-of-choice']"
57,Looking for a rigorous linear algebra book,Looking for a rigorous linear algebra book,,"I'm a mathematics undergrad student who finished his first university year succesfully. For this post, it should be interesting to note that I already took a course in group theory. I also had a course in linear algebra where we treated the following topics: 1) Vector spaces; basis; span; linear dependency; direct sum 2) Linear transformations, matrices, rank 3) Linear varieties, system of linear equations 4) Determinants 5) Eigenvalues and eigenvectors, diagonalisation, triangalisation, cayley-hamilton, 6) Euclidean spaces: inner products, norm, hermitian transformations, orthogonal basis, Gramm-Schmidt, orthogonal transformations 7) Prehilbert spaces 8) Isometries 9) Bilinear forms and quadrics in $2$ and $3$ dimensions Now, I want to revise these topics, but it would be too boring to reread these books again, so now I look for a book that has all these topics, but also some additional requirements: Focus on intuition (e.g. connections between linear algebra and geometry). This is a hard requirement for me, since there was no attention given to this in the course I took) Additional topics such as Jordan form of matrices, quotient vector spaces, ... Good exercises (preferred with solutions somewhere) The book must be rigorous In the course I had, we mainly discussed finite-dimensional vector spaces, but I am also interested in a theory of infinite dimensional vector spaces. Can someone hint me towards a good book that would suit me? If I have to add any information, please leave a comment and I will edit my post. Thank you for your time.","I'm a mathematics undergrad student who finished his first university year succesfully. For this post, it should be interesting to note that I already took a course in group theory. I also had a course in linear algebra where we treated the following topics: 1) Vector spaces; basis; span; linear dependency; direct sum 2) Linear transformations, matrices, rank 3) Linear varieties, system of linear equations 4) Determinants 5) Eigenvalues and eigenvectors, diagonalisation, triangalisation, cayley-hamilton, 6) Euclidean spaces: inner products, norm, hermitian transformations, orthogonal basis, Gramm-Schmidt, orthogonal transformations 7) Prehilbert spaces 8) Isometries 9) Bilinear forms and quadrics in $2$ and $3$ dimensions Now, I want to revise these topics, but it would be too boring to reread these books again, so now I look for a book that has all these topics, but also some additional requirements: Focus on intuition (e.g. connections between linear algebra and geometry). This is a hard requirement for me, since there was no attention given to this in the course I took) Additional topics such as Jordan form of matrices, quotient vector spaces, ... Good exercises (preferred with solutions somewhere) The book must be rigorous In the course I had, we mainly discussed finite-dimensional vector spaces, but I am also interested in a theory of infinite dimensional vector spaces. Can someone hint me towards a good book that would suit me? If I have to add any information, please leave a comment and I will edit my post. Thank you for your time.",,['linear-algebra']
58,Why does rotation preserve angles and distance in the Euclidean plane?,Why does rotation preserve angles and distance in the Euclidean plane?,,"An angle in radians is a distance moved around a unit circle. A rotation of a point around a center of rotation, moves the point a distance around a circle around the center that goes through the point. The distance is given by the angle of the rotation multiplied by the distance from center. So why do rotations preserve distance and angles when the distance measure is Pythagorean.","An angle in radians is a distance moved around a unit circle. A rotation of a point around a center of rotation, moves the point a distance around a circle around the center that goes through the point. The distance is given by the angle of the rotation multiplied by the distance from center. So why do rotations preserve distance and angles when the distance measure is Pythagorean.",,"['linear-algebra', 'euclidean-geometry', 'rotations']"
59,One distinct eigenvalue $\iff A-\lambda E_n$ is nilpotent,One distinct eigenvalue  is nilpotent,\iff A-\lambda E_n,"Let $A\in \mathbb{C}^{n\times n}$ and $\lambda \in \mathbb{C}.$ $\lambda$ only eigenvalue of $A \iff A-\lambda E_n$ is nilpotent. So the first direction: let $\lambda$ be the only eigenvalue of $A$ then the characteristic polynomial of $A$ has the form $\chi_A(\lambda)=\left(x-\lambda\right)^n$ and so does the minimal polynomial $\mu_A(\lambda)=\left(x-\lambda\right)^m, m\le n$ then by Cayley Hamilton theorem, the matrix satisfies $(A-\lambda E_n)^n=0$ and also $(A-\lambda E_n)^m=0$ so $A$ is nilpotent of degree $m.$ Any help with the other implication would be appreciated.","Let $A\in \mathbb{C}^{n\times n}$ and $\lambda \in \mathbb{C}.$ $\lambda$ only eigenvalue of $A \iff A-\lambda E_n$ is nilpotent. So the first direction: let $\lambda$ be the only eigenvalue of $A$ then the characteristic polynomial of $A$ has the form $\chi_A(\lambda)=\left(x-\lambda\right)^n$ and so does the minimal polynomial $\mu_A(\lambda)=\left(x-\lambda\right)^m, m\le n$ then by Cayley Hamilton theorem, the matrix satisfies $(A-\lambda E_n)^n=0$ and also $(A-\lambda E_n)^m=0$ so $A$ is nilpotent of degree $m.$ Any help with the other implication would be appreciated.",,[]
60,Can a subspace of a vector space $V$ have a different form of zero vector?,Can a subspace of a vector space  have a different form of zero vector?,V,"In Linear algebra, a necessary condition for a subset of a vector space to be a subspace, is to contain the zero vector. For example, The following subspaces of Vector space $\mathbb {R^3}$ has $0$-D subspace( the origin ), 1-D subspace ( lines through the origin ),2-D subspace ( planes through the origin ) and finally $\mathbb {R^3}$ have the same zero vector But the subspaces of Vector space of all polynomials with degree at most n, $\mathbb{P_n}$ have the same form of zero vector defined as the function value to be $0$ $\forall$ t , with different coordinate mappings which are one to one and onto $\mathbb{R^k}$ with $k$ = dim $\mathbb{P_m}$ $\forall$ $m < n$ Can a subset of a Vector space $V$ having a zero vector in its own right different from that of $V$ ( may be non trivial, here is an example Zero vector of a vector space ) closed under vector addition and scalar multiplication be called a subspace of $V$ ?","In Linear algebra, a necessary condition for a subset of a vector space to be a subspace, is to contain the zero vector. For example, The following subspaces of Vector space $\mathbb {R^3}$ has $0$-D subspace( the origin ), 1-D subspace ( lines through the origin ),2-D subspace ( planes through the origin ) and finally $\mathbb {R^3}$ have the same zero vector But the subspaces of Vector space of all polynomials with degree at most n, $\mathbb{P_n}$ have the same form of zero vector defined as the function value to be $0$ $\forall$ t , with different coordinate mappings which are one to one and onto $\mathbb{R^k}$ with $k$ = dim $\mathbb{P_m}$ $\forall$ $m < n$ Can a subset of a Vector space $V$ having a zero vector in its own right different from that of $V$ ( may be non trivial, here is an example Zero vector of a vector space ) closed under vector addition and scalar multiplication be called a subspace of $V$ ?",,"['linear-algebra', 'vector-spaces']"
61,Prove the matrix is not diagonalizable,Prove the matrix is not diagonalizable,,"For any reals, $a, b$ show that $$D = \begin{bmatrix} a & 1\\  0 &  b \end{bmatrix}$$   Is not diagonalizable. I got that the eigenvalues are $\lambda_1 = a, \lambda_2 = b$ However, for $a \ne b$, there are $2$ distinct eigenvalues so $D$ must be diagonalizable right? So what about $a= b$?","For any reals, $a, b$ show that $$D = \begin{bmatrix} a & 1\\  0 &  b \end{bmatrix}$$   Is not diagonalizable. I got that the eigenvalues are $\lambda_1 = a, \lambda_2 = b$ However, for $a \ne b$, there are $2$ distinct eigenvalues so $D$ must be diagonalizable right? So what about $a= b$?",,"['linear-algebra', 'matrices']"
62,Sparse basis orthogonal to the ones vector,Sparse basis orthogonal to the ones vector,,"Let $v\in \mathbb{R}^{n}$ be the vector of ones, $v=(1,1,1,\cdots,1).$ I need an orthogonal basis for the orthogonal complement $v^{\perp}$, the space of all vectors orthogonal to $v$. Of course, one can solve for such a basis using Gram-Schmidt, but I have an extra requirement: that the basis vectors are sparse . Is there a standard such basis? I can start writing down basis vectors in an ad-hoc way, e.g. \begin{align*} b_1 &= (1, -1, 0, 0, \cdots)\\ b_2 &= (0, 0, 1, -1, \cdots) \end{align*} but obviously this pattern runs out after $n/2$ vectors.","Let $v\in \mathbb{R}^{n}$ be the vector of ones, $v=(1,1,1,\cdots,1).$ I need an orthogonal basis for the orthogonal complement $v^{\perp}$, the space of all vectors orthogonal to $v$. Of course, one can solve for such a basis using Gram-Schmidt, but I have an extra requirement: that the basis vectors are sparse . Is there a standard such basis? I can start writing down basis vectors in an ad-hoc way, e.g. \begin{align*} b_1 &= (1, -1, 0, 0, \cdots)\\ b_2 &= (0, 0, 1, -1, \cdots) \end{align*} but obviously this pattern runs out after $n/2$ vectors.",,['linear-algebra']
63,Is this subset a basis of the vector space?,Is this subset a basis of the vector space?,,"Decide if the subset $ \left\{v_{1},v_{2}\right\}=\left\{ \begin{pmatrix} 1\\  2 \end{pmatrix}, \begin{pmatrix} \alpha\\  1 \end{pmatrix}\right \}$ is a basis of the vector space $V = \mathbb{R}^{2}$ for an arbitrary fixed $\alpha \in \mathbb{R}, \alpha \neq \frac{1}{2}$ I first check if both vectors are linearly independent. I use the determinant trick for it. $$\begin{vmatrix} 1 & \alpha\\  2 & 1 \end{vmatrix}$$ $$\text{Determinant}= 1 \cdot 1 - 2\alpha$$ This can never equal zero as $\alpha \neq \frac{1}{2}$ and thus both vectors are linearly independent. Moreover the dimension of the vector space is $2$ since $V=\mathbb{R}^{2}$ and we have the same number of linearly independent vectors, $2$. Thus the subset $\left\{v_{1},v_{2}\right\}$ is indeed a basis of the vector space $V$. Can you please tell me if I did it correctly? It's very important for me to know as I would do it like that in the exam. And also tell me if the notation is fine :)","Decide if the subset $ \left\{v_{1},v_{2}\right\}=\left\{ \begin{pmatrix} 1\\  2 \end{pmatrix}, \begin{pmatrix} \alpha\\  1 \end{pmatrix}\right \}$ is a basis of the vector space $V = \mathbb{R}^{2}$ for an arbitrary fixed $\alpha \in \mathbb{R}, \alpha \neq \frac{1}{2}$ I first check if both vectors are linearly independent. I use the determinant trick for it. $$\begin{vmatrix} 1 & \alpha\\  2 & 1 \end{vmatrix}$$ $$\text{Determinant}= 1 \cdot 1 - 2\alpha$$ This can never equal zero as $\alpha \neq \frac{1}{2}$ and thus both vectors are linearly independent. Moreover the dimension of the vector space is $2$ since $V=\mathbb{R}^{2}$ and we have the same number of linearly independent vectors, $2$. Thus the subset $\left\{v_{1},v_{2}\right\}$ is indeed a basis of the vector space $V$. Can you please tell me if I did it correctly? It's very important for me to know as I would do it like that in the exam. And also tell me if the notation is fine :)",,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
64,Calculate the image and a basis of the image (matrix),Calculate the image and a basis of the image (matrix),,"What's the image of the matrix? What's the basis of the image?   $M=\begin{pmatrix} -1 &  1 &  1\\  -2 & -3 &  6\\   0 & -1 &  1 \end{pmatrix}$ First transposed the matrix: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   1 & -3 & -1\\   1 &  6 &  1 \end{pmatrix}$ Now we use Gauss and get zero lines. Take the first line and add it to the third: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   0 & -5 & -1\\   1 &  6 &  1 \end{pmatrix}$ Take the first line and add it to the third: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   0 & -5 & -1\\   0 &  4 &  1 \end{pmatrix}$ Multiply the second line with $4$, multiply the third line with $5$, then add second line to third: $M^{T}=\begin{pmatrix} -1 & -2  &  0\\   0 & -20 & -4\\   0 &  0  &  1 \end{pmatrix}$ Transpose back: $M=\begin{pmatrix} -1 &  0  &  0\\  -2 & -20 &  0\\   0 & -4  &  1 \end{pmatrix}$ The image of the matrix is $\text{Im(M)}= \text{span} \left ( \left\{ \begin{pmatrix} -1\\  -2\\  0 \end{pmatrix}, \begin{pmatrix} 0\\  -20\\  4 \end{pmatrix},\begin{pmatrix} 0\\  0\\  1 \end{pmatrix} \right\} \right)$ The basis of the image is $\left\{ \begin{pmatrix} -1\\  -2\\  0 \end{pmatrix}, \begin{pmatrix} 0\\  -20\\  4 \end{pmatrix},\begin{pmatrix} 0\\  0\\  1 \end{pmatrix} \right\}$ Please tell me if I did everything correctly? It's very important for me to know as I would do it like that in the exam :) I hope it's correct and please also tell me if the notation is.","What's the image of the matrix? What's the basis of the image?   $M=\begin{pmatrix} -1 &  1 &  1\\  -2 & -3 &  6\\   0 & -1 &  1 \end{pmatrix}$ First transposed the matrix: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   1 & -3 & -1\\   1 &  6 &  1 \end{pmatrix}$ Now we use Gauss and get zero lines. Take the first line and add it to the third: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   0 & -5 & -1\\   1 &  6 &  1 \end{pmatrix}$ Take the first line and add it to the third: $M^{T}=\begin{pmatrix} -1 & -2 &  0\\   0 & -5 & -1\\   0 &  4 &  1 \end{pmatrix}$ Multiply the second line with $4$, multiply the third line with $5$, then add second line to third: $M^{T}=\begin{pmatrix} -1 & -2  &  0\\   0 & -20 & -4\\   0 &  0  &  1 \end{pmatrix}$ Transpose back: $M=\begin{pmatrix} -1 &  0  &  0\\  -2 & -20 &  0\\   0 & -4  &  1 \end{pmatrix}$ The image of the matrix is $\text{Im(M)}= \text{span} \left ( \left\{ \begin{pmatrix} -1\\  -2\\  0 \end{pmatrix}, \begin{pmatrix} 0\\  -20\\  4 \end{pmatrix},\begin{pmatrix} 0\\  0\\  1 \end{pmatrix} \right\} \right)$ The basis of the image is $\left\{ \begin{pmatrix} -1\\  -2\\  0 \end{pmatrix}, \begin{pmatrix} 0\\  -20\\  4 \end{pmatrix},\begin{pmatrix} 0\\  0\\  1 \end{pmatrix} \right\}$ Please tell me if I did everything correctly? It's very important for me to know as I would do it like that in the exam :) I hope it's correct and please also tell me if the notation is.",,"['linear-algebra', 'matrices', 'vector-spaces']"
65,Four Fundamental subspaces,Four Fundamental subspaces,,"Given an $m \times n$ matrix, where $m$ is the number of rows and $n$ is the number of columns. Four Fundamental Subspaces The row space is $C(A^t)$, a subspace of $\mathbb{R}^n$. The column space is $C(A)$, a subspace of $\mathbb{R}^m$. The nullspace is $N(A)$, a subspace of $\mathbb{R}^n$. The left nullspace is $N(A^t)$, a subspace of $\mathbb{R}^m$. This is our new space. If the column space is the space that is spanned by the column vectors, why is it that it is a subspace of $\mathbb{R}^m$ and not $\mathbb{R}^n$, since the dimension of the columns should be $n$ instead? Or am I missing something fundamental here?","Given an $m \times n$ matrix, where $m$ is the number of rows and $n$ is the number of columns. Four Fundamental Subspaces The row space is $C(A^t)$, a subspace of $\mathbb{R}^n$. The column space is $C(A)$, a subspace of $\mathbb{R}^m$. The nullspace is $N(A)$, a subspace of $\mathbb{R}^n$. The left nullspace is $N(A^t)$, a subspace of $\mathbb{R}^m$. This is our new space. If the column space is the space that is spanned by the column vectors, why is it that it is a subspace of $\mathbb{R}^m$ and not $\mathbb{R}^n$, since the dimension of the columns should be $n$ instead? Or am I missing something fundamental here?",,['linear-algebra']
66,What is the dimension of a topology?,What is the dimension of a topology?,,"We know about the manifold dimensions of topological objects. I have a query, as every vector space has Hamel basis and the dimension of a vector space is defined as the number of elements in the basis set. My question is why this definition of dimension is not used to defined the basis of a topological space. Is that the reason, a topological space may not have same number of elements in its every basis?","We know about the manifold dimensions of topological objects. I have a query, as every vector space has Hamel basis and the dimension of a vector space is defined as the number of elements in the basis set. My question is why this definition of dimension is not used to defined the basis of a topological space. Is that the reason, a topological space may not have same number of elements in its every basis?",,"['linear-algebra', 'general-topology', 'vector-spaces']"
67,Area of paralellogram,Area of paralellogram,,"Find the area of parallelogram of the two vectors (6,0,1,3) and (2,1,3,1). The area is magnitude of the cross product of the two vectors. Right now the only way i was taught to do the cross product is getting the determinant of putting the vectors in and i,j,k matrix. I dont know how to do it with 4 points of the vector.","Find the area of parallelogram of the two vectors (6,0,1,3) and (2,1,3,1). The area is magnitude of the cross product of the two vectors. Right now the only way i was taught to do the cross product is getting the determinant of putting the vectors in and i,j,k matrix. I dont know how to do it with 4 points of the vector.",,"['linear-algebra', 'vectors', 'determinant', 'area', 'cross-product']"
68,Prove that all nxn nilpotent matrices of order n are similar.,Prove that all nxn nilpotent matrices of order n are similar.,,I have to show that all $n \times n$ nilpotent matrices of order n are similar. My initial approach was to show that for all nilpotent matrices their minimun characteristic polynomial is of the form: $$\lambda^n$$ Is this sufficient? Can someone show me a formal approach to this problem? Thanks!,I have to show that all $n \times n$ nilpotent matrices of order n are similar. My initial approach was to show that for all nilpotent matrices their minimun characteristic polynomial is of the form: $$\lambda^n$$ Is this sufficient? Can someone show me a formal approach to this problem? Thanks!,,"['linear-algebra', 'matrices', 'nilpotence']"
69,"Is the largest diagonal diagonal entry of a symmetric, positive semidefinite matrix a lower bound on the largest eigenvalue?","Is the largest diagonal diagonal entry of a symmetric, positive semidefinite matrix a lower bound on the largest eigenvalue?",,"Well, the question is as simple as that: Given a symmetric and positive semidefinite matrix. Is it true that the largest diagonal entry is always smaller or equal to the largest eigenvalue? I was just getting a little frustrated while proving this for a specific kind of matrix (that happens to be symmetric and positve definite). So I'm wondering whether I can just skip the algebra and show that this is true in general. But a quick web search didn't reveal any promising references. Can someone clear up for me whether this is true or not?","Well, the question is as simple as that: Given a symmetric and positive semidefinite matrix. Is it true that the largest diagonal entry is always smaller or equal to the largest eigenvalue? I was just getting a little frustrated while proving this for a specific kind of matrix (that happens to be symmetric and positve definite). So I'm wondering whether I can just skip the algebra and show that this is true in general. But a quick web search didn't reveal any promising references. Can someone clear up for me whether this is true or not?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
70,There is no real $3\times3$ matrix $A$ for which $A^2=-I$.,There is no real  matrix  for which .,3\times3 A A^2=-I,I'm asked to prove or disprove this but I'm stuck. I feel like this is actually true but i'm not sure as to how I can formulate a proof for this statement. Any leads?,I'm asked to prove or disprove this but I'm stuck. I feel like this is actually true but i'm not sure as to how I can formulate a proof for this statement. Any leads?,,"['linear-algebra', 'matrices']"
71,Eigenvalues and eigenvectors of a unitary operator,Eigenvalues and eigenvectors of a unitary operator,,"I have $φ: V → V$ as a unitary operator on a complex inner product space $V$. How can I show, without using any diagonalization results, that every eigenvalue $λ$ of $φ$ satisfies $|λ|=1$ and that eigenvectors corresponding to distinct eigenvalues are orthogonal?","I have $φ: V → V$ as a unitary operator on a complex inner product space $V$. How can I show, without using any diagonalization results, that every eigenvalue $λ$ of $φ$ satisfies $|λ|=1$ and that eigenvectors corresponding to distinct eigenvalues are orthogonal?",,"['linear-algebra', 'abstract-algebra']"
72,What is a trivial and a non-trivial solution in terms of linear algebra?,What is a trivial and a non-trivial solution in terms of linear algebra?,,"The homogeneous unique solution always gives a trivial solution but trivial solution consists of zeros e.g $\{0,0,0\}$ but what if the solution is still homogeneous unique solution but not in the form of zeros e.g $\{1,2,3\}$. Is this solution also trivial? How?","The homogeneous unique solution always gives a trivial solution but trivial solution consists of zeros e.g $\{0,0,0\}$ but what if the solution is still homogeneous unique solution but not in the form of zeros e.g $\{1,2,3\}$. Is this solution also trivial? How?",,['linear-algebra']
73,Please recommend a textbook for first-time learning Linear Algebra like Stewart's Calculus?,Please recommend a textbook for first-time learning Linear Algebra like Stewart's Calculus?,,"I'm trying to learn linear algebra on my own. When I taught myself calculus I-III a year ago, I used Stewart's Concepts and Contexts 4th edition and I absolutely loved it. Theorems and definitions were displayed clearly, the graphs were detailed, and the problem selection was quite large. Is there a linear algebra textbook similar to how Stewart's textbook was designed? Additionally, I'm eventually planning to learn differential equations and I'm aware that quite a few texts overlap when teaching these topics. I'd prefer to learn them individually, so I'd like to avoid texts with a good amount of differential equation, unless there is a text that covers both fields completely  in the same way that Stewart's text covered single and multivariable calculus together. P.S I don't mean to be praising Stewart's. It's the only mathematics textbook I've used to self-study so I don't have much else to relate to.","I'm trying to learn linear algebra on my own. When I taught myself calculus I-III a year ago, I used Stewart's Concepts and Contexts 4th edition and I absolutely loved it. Theorems and definitions were displayed clearly, the graphs were detailed, and the problem selection was quite large. Is there a linear algebra textbook similar to how Stewart's textbook was designed? Additionally, I'm eventually planning to learn differential equations and I'm aware that quite a few texts overlap when teaching these topics. I'd prefer to learn them individually, so I'd like to avoid texts with a good amount of differential equation, unless there is a text that covers both fields completely  in the same way that Stewart's text covered single and multivariable calculus together. P.S I don't mean to be praising Stewart's. It's the only mathematics textbook I've used to self-study so I don't have much else to relate to.",,"['linear-algebra', 'reference-request']"
74,"Notation: is it proper to multiply matrix with a vector represented using an n-tuple $\begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)$?",Notation: is it proper to multiply matrix with a vector represented using an n-tuple ?,"\begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)","I've recently noticed talking to a few classmates from another school in undergrad engineering, they are denoting a vector using n-tuple i.e. $x = (x_1, x_2, x_3, \ldots, x_n) \in \mathbb{R}^n$ It is fine with me but then I noticed that they would proceed to multiply a matrix (for instance a $2 \times 2$ matrix $A$) as: $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)$ I was raised to denote $x$ as a column vector, so $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix}$ feels more comfortable to me. Truthfully, it also feels more correct. Would $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)$ be considered to be correct as well? Is it good practice?","I've recently noticed talking to a few classmates from another school in undergrad engineering, they are denoting a vector using n-tuple i.e. $x = (x_1, x_2, x_3, \ldots, x_n) \in \mathbb{R}^n$ It is fine with me but then I noticed that they would proceed to multiply a matrix (for instance a $2 \times 2$ matrix $A$) as: $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)$ I was raised to denote $x$ as a column vector, so $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix}$ feels more comfortable to me. Truthfully, it also feels more correct. Would $Ax = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} x = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}(x_1, x_2)$ be considered to be correct as well? Is it good practice?",,"['linear-algebra', 'matrices', 'soft-question', 'notation']"
75,The direct sum of a rank and null space,The direct sum of a rank and null space,,"Firstly, I have no idea how to use most of the fancy mathjax formatting stuffs yet. I'm currently too frustrated trying to figure out how to understand this problem (due in 10 hours), so please feel free to refer anything for that while answering. :) I've got a hw problem stated as such: Let $V$ be a finite-dimensional vector space and T:V->V be linear. A) Suppose that V = R(T) + N(T). Prove that V = R(T) $\oplus$ N(T). B) Suppose that R(T) $\cap$ N(T) = {$0$}. Prove that V = R(T) $\oplus$ N(T). I've found two separate answer explanations for part A (both along the lines of $P$ = $P^2$ with $P$(1 - $P$) = 0), but I don't understand why that is the answer. The largest problem is probably my confusion with how direct sums work. I understand that the subspace $V$ $\oplus$ subspace $W$ = a vector space, when - 1) $V$ $\nsubseteq$ $W$ (I think that's the correct notation? Basically all members of V and all of W will not be equal/overlap), and when - 2) $V$ $\cap$ $W$ = {$0$} (i.e. they only have the zero vector in common) But I'm having issues correlating those two points to anything outside of the definition, so understanding and proving this is completely foreign to me. I'd love answers even after this is due, since I suspect this class will be heavy on understanding these concepts. Thanks in advance!","Firstly, I have no idea how to use most of the fancy mathjax formatting stuffs yet. I'm currently too frustrated trying to figure out how to understand this problem (due in 10 hours), so please feel free to refer anything for that while answering. :) I've got a hw problem stated as such: Let $V$ be a finite-dimensional vector space and T:V->V be linear. A) Suppose that V = R(T) + N(T). Prove that V = R(T) $\oplus$ N(T). B) Suppose that R(T) $\cap$ N(T) = {$0$}. Prove that V = R(T) $\oplus$ N(T). I've found two separate answer explanations for part A (both along the lines of $P$ = $P^2$ with $P$(1 - $P$) = 0), but I don't understand why that is the answer. The largest problem is probably my confusion with how direct sums work. I understand that the subspace $V$ $\oplus$ subspace $W$ = a vector space, when - 1) $V$ $\nsubseteq$ $W$ (I think that's the correct notation? Basically all members of V and all of W will not be equal/overlap), and when - 2) $V$ $\cap$ $W$ = {$0$} (i.e. they only have the zero vector in common) But I'm having issues correlating those two points to anything outside of the definition, so understanding and proving this is completely foreign to me. I'd love answers even after this is due, since I suspect this class will be heavy on understanding these concepts. Thanks in advance!",,"['linear-algebra', 'vector-spaces', 'direct-sum']"
76,Geometric Intuition of Eigenvalues of Hessian Matrix,Geometric Intuition of Eigenvalues of Hessian Matrix,,"I have a very simple question, which I suspect speaks more to my lack of intuitive understanding of parts of linear algebra than anything calculus related. I have come across this statement (or variants thereof) in the context of the Morse Index of a critical point on various occasions: Statement : The number of negative eigenvalues of the Hessian matrix of a function $F:M\to N$ at a point $p$ is equal to the dimension of the maximal subspace of the tangent space $TM_p$ of $M$ at $p$ on which $F$ is negative definite. My question is: Question : Why do the eigenvalues of the Hessian encode this information? As I stated above, this almost certainly is the result of a lack of comfort in dealing with eigenvalues (rather than anything differential) that I have put off confronting far too long into my mathematical education, but it is not clear to me how to make the jump from in the statement.","I have a very simple question, which I suspect speaks more to my lack of intuitive understanding of parts of linear algebra than anything calculus related. I have come across this statement (or variants thereof) in the context of the Morse Index of a critical point on various occasions: Statement : The number of negative eigenvalues of the Hessian matrix of a function $F:M\to N$ at a point $p$ is equal to the dimension of the maximal subspace of the tangent space $TM_p$ of $M$ at $p$ on which $F$ is negative definite. My question is: Question : Why do the eigenvalues of the Hessian encode this information? As I stated above, this almost certainly is the result of a lack of comfort in dealing with eigenvalues (rather than anything differential) that I have put off confronting far too long into my mathematical education, but it is not clear to me how to make the jump from in the statement.",,"['linear-algebra', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'differential-topology', 'morse-theory']"
77,Sequence of rotation matrices,Sequence of rotation matrices,,"I want a rotation matrix $R$ that transforms +x axis to +y, +y to +z, and +z to +x. One way of doing it is by a rotation about +x by 90 deg anti-clockwise, followed by a rotation about +y by 90 deg anti-clockwise. The matrices respectively are: $$R_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{bmatrix}$$ $$R_2 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ -1 & 0 & 0 \\ \end{bmatrix}$$ Since R1 first operates first followed by R2, the resultant, I think, is R=(R2)(R1). But when I cross-check by applying R to an arbitrary point, say (1,2,3), I don't get the right transform of (3,1,2). Instead, I get it right when I do R=(R1)(R2)  which doesn't make sense to me. Please help","I want a rotation matrix $R$ that transforms +x axis to +y, +y to +z, and +z to +x. One way of doing it is by a rotation about +x by 90 deg anti-clockwise, followed by a rotation about +y by 90 deg anti-clockwise. The matrices respectively are: $$R_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{bmatrix}$$ $$R_2 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ -1 & 0 & 0 \\ \end{bmatrix}$$ Since R1 first operates first followed by R2, the resultant, I think, is R=(R2)(R1). But when I cross-check by applying R to an arbitrary point, say (1,2,3), I don't get the right transform of (3,1,2). Instead, I get it right when I do R=(R1)(R2)  which doesn't make sense to me. Please help",,"['linear-algebra', 'matrices', 'transformation', 'coordinate-systems', 'rotations']"
78,"If Brauer characters are $\bar{\mathbb{Q}}$-linearly independent, why are they $\mathbb{C}$-linearly independent?","If Brauer characters are -linearly independent, why are they -linearly independent?",\bar{\mathbb{Q}} \mathbb{C},"If Brauer characters are $\bar{\mathbb{Q}}$-linearly independent, why are they $\mathbb{C}$-linearly independent? I think this is a linear algebra fact showing up when proving the irreducible Brauer characters on a finite group are linearly independent over $\mathbb{C}$. The proof I've seen observes that the characters take values in the ring of algebraic integers, and then proves linear independence over $\bar{\mathbb{Q}}$. Why is it sufficient to only check linear independence over $\bar{\mathbb{Q}}$? It seems like something could go wrong when extending the field all the way up to $\mathbb{C}$. The proof I'm reading is Theorem 15.5 in Isaacs' Character Theory of Finite Groups .","If Brauer characters are $\bar{\mathbb{Q}}$-linearly independent, why are they $\mathbb{C}$-linearly independent? I think this is a linear algebra fact showing up when proving the irreducible Brauer characters on a finite group are linearly independent over $\mathbb{C}$. The proof I've seen observes that the characters take values in the ring of algebraic integers, and then proves linear independence over $\bar{\mathbb{Q}}$. Why is it sufficient to only check linear independence over $\bar{\mathbb{Q}}$? It seems like something could go wrong when extending the field all the way up to $\mathbb{C}$. The proof I'm reading is Theorem 15.5 in Isaacs' Character Theory of Finite Groups .",,"['linear-algebra', 'abstract-algebra', 'representation-theory', 'characters']"
79,Why can't a set of four vectors in $\mathbb{R}^3$ be linearly independent?,Why can't a set of four vectors in  be linearly independent?,\mathbb{R}^3,"Why can't a set of four vectors in $\mathbb{R}^3$  be linearly independent? I know that if the determinant of the vectors together is not $0$ then the vectors are linearly independent. But this is not relevant to this case of a non square matrix. Is the answer ""because there can only ever be 3 pivots when reduced"" a good answer?","Why can't a set of four vectors in $\mathbb{R}^3$  be linearly independent? I know that if the determinant of the vectors together is not $0$ then the vectors are linearly independent. But this is not relevant to this case of a non square matrix. Is the answer ""because there can only ever be 3 pivots when reduced"" a good answer?",,"['linear-algebra', 'vector-spaces']"
80,Non-negative Linear Span of Vectors,Non-negative Linear Span of Vectors,,"I would like to understand if there is a common concept of a `linear span' of a set of vectors which are combined with non-negative multipliers. I know that usual definition of the span of a set of vectors as follows [wiki] : $\operatorname{span}(S) =  \left \{ {\sum_{i=1}^k \lambda_i v_i \Big| k \in \mathbb{N}, v_i  \in S, \lambda _i  \in \mathbf{K}} \right \}.$ Is there a commonly used name for a span when $\lambda _i  \in R^+$ (positive real numbers) so that I can study this further? I cannot find much material in this area. Ultimately, I am interested if there is a way to distinguish between matrices (or collections of vectors) that have full rank even if only positive multipliers are permitted. Thanks for any guidance.","I would like to understand if there is a common concept of a `linear span' of a set of vectors which are combined with non-negative multipliers. I know that usual definition of the span of a set of vectors as follows [wiki] : $\operatorname{span}(S) =  \left \{ {\sum_{i=1}^k \lambda_i v_i \Big| k \in \mathbb{N}, v_i  \in S, \lambda _i  \in \mathbf{K}} \right \}.$ Is there a commonly used name for a span when $\lambda _i  \in R^+$ (positive real numbers) so that I can study this further? I cannot find much material in this area. Ultimately, I am interested if there is a way to distinguish between matrices (or collections of vectors) that have full rank even if only positive multipliers are permitted. Thanks for any guidance.",,"['linear-algebra', 'vector-spaces', 'matrix-rank']"
81,"Proving that $X^TX+\lambda A$ is invertible when $\lambda > 0$ ($A$ is a large matrix, see inside)","Proving that  is invertible when  ( is a large matrix, see inside)",X^TX+\lambda A \lambda > 0 A,"I want to show that $X^TX + \lambda \begin{pmatrix} 0 & 0 & 0 & 0 & \dots & 0 \\ 0 & 1 & 0 & 0 & \dots & 0\\ 0 & 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & \dots & 1\end{pmatrix} $ is invertible if $\lambda >0$ I saw it in an online course but the professor gave no explanation. $X$ is an $m$ by $n+1$ matrix, and so the matrix $\lambda$ multiplies is $n+1$ by $n+1$, and I don't know if its relevant to the question, but we can safely assume that the first column of $X$ is all ones, but that may not be needed. how can i prove this? there is no attribute of the determinant of the sum of matrices, this matrix is indeed symmetric so it might be positive definite, but I don't think we can say much about the eigenvalues...","I want to show that $X^TX + \lambda \begin{pmatrix} 0 & 0 & 0 & 0 & \dots & 0 \\ 0 & 1 & 0 & 0 & \dots & 0\\ 0 & 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 0 & 1 & \dots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & \dots & 1\end{pmatrix} $ is invertible if $\lambda >0$ I saw it in an online course but the professor gave no explanation. $X$ is an $m$ by $n+1$ matrix, and so the matrix $\lambda$ multiplies is $n+1$ by $n+1$, and I don't know if its relevant to the question, but we can safely assume that the first column of $X$ is all ones, but that may not be needed. how can i prove this? there is no attribute of the determinant of the sum of matrices, this matrix is indeed symmetric so it might be positive definite, but I don't think we can say much about the eigenvalues...",,"['linear-algebra', 'matrices']"
82,Irreducible polynomials in $\mathbb F_3[x]$,Irreducible polynomials in,\mathbb F_3[x],"Finding irreducible polynomials in $\mathbb F_3[x]$ of degree less or equal to $4$ for $d=2,3$ the polynomial should not have a root case $d=2$ there are $2\cdot 3\cdot 3=18$ polynomials with degree $2$. Testing all I get; $x^2+1, x^2+x+2, x^2+2x+2, 2x^2+2, 2x^2+x+1, 2x^2+2x+1$ are irreducible when $d=3$ it gets complicated, at least by a theorem I know that there are $9$ irreducible monic polynomials ($x^{p^d}-x$ is the product of all irreducible monic polynomials in $\mathbb F_p[x]$) Is there a technique to find these ? here for example there is a complicated formula, which only determines the monic ones","Finding irreducible polynomials in $\mathbb F_3[x]$ of degree less or equal to $4$ for $d=2,3$ the polynomial should not have a root case $d=2$ there are $2\cdot 3\cdot 3=18$ polynomials with degree $2$. Testing all I get; $x^2+1, x^2+x+2, x^2+2x+2, 2x^2+2, 2x^2+x+1, 2x^2+2x+1$ are irreducible when $d=3$ it gets complicated, at least by a theorem I know that there are $9$ irreducible monic polynomials ($x^{p^d}-x$ is the product of all irreducible monic polynomials in $\mathbb F_p[x]$) Is there a technique to find these ? here for example there is a complicated formula, which only determines the monic ones",,"['linear-algebra', 'abstract-algebra']"
83,Minimize $\mbox{trace}(AX)$ over $X$ with a positive semidefinite $X$,Minimize  over  with a positive semidefinite,\mbox{trace}(AX) X X,"I want to minimize $\mbox{trace}(AX)$ over $X$, under the constraint that $X$ is positive semidefinite. I guess the solution should be bounded only for a positive semidefinite $A$, and it's zero, or the solution should be minus infinity. If this is correct, can anyone tell me why? or if it is wrong, please tell me the correct solution. Thank you very much!!","I want to minimize $\mbox{trace}(AX)$ over $X$, under the constraint that $X$ is positive semidefinite. I guess the solution should be bounded only for a positive semidefinite $A$, and it's zero, or the solution should be minus infinity. If this is correct, can anyone tell me why? or if it is wrong, please tell me the correct solution. Thank you very much!!",,"['linear-algebra', 'matrices', 'optimization', 'semidefinite-programming', 'positive-semidefinite']"
84,Scalar multiplication as a special form of matrix multiplication,Scalar multiplication as a special form of matrix multiplication,,"Question What do we gain or lose, conceptually, if we consider scalar multiplication as a special form of matrix multiplication ? Background The question bothers me since I have been reading about dilations and scaling of geometrical objects in Paul Lockhart's book ""Measurement"". Geometrically, dilation is a transformation that stretches an object in one dimension by a certain factor. Analogously, the linear transformation $$ \begin{pmatrix}   1 & 0 & 0 \\   0 & 1 & 0 \\   0 & 0 & \lambda \end{pmatrix} \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix} $$ ""stretches"" the third component by the factor $\lambda$. Scaling is a geometric transformation that stretches an object in all dimensions by a certain factor. Analogously, the linear transformation $$ \begin{pmatrix}   \lambda & 0 & 0 \\   0 & \lambda & 0 \\   0 & 0 & \lambda \end{pmatrix} \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix} $$ ""stretches"" all three components by the factor $\lambda$. This, however, can be written more succinctly using scalar multiplication: $$ \lambda \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix}. $$ In fact, every scalar multiplication can be expressed as a multiplication with a special matrix, and it turns out to be a mere shortcut. On the face of it, this observation is not very spectacular; however, it raises interesting philosophical and conceptual questions as to the foundations of linear algebra. For example, if scalar multiplication is only a nice-to-have shortcut, then isn't it in fact superfluous conceptually? Currently, scalar multiplication is taught as if it was a distinct concept, independent of matrix multiplication. What would change if we got rid of this shortcut? What could alternative axioms of vector spaces and moduls look like? What about linear transformations? What is easier, what is harder$-$not to write down, but conceptually? I know that this topic is very broad, but I would like to collect opinions, ideas, examples.","Question What do we gain or lose, conceptually, if we consider scalar multiplication as a special form of matrix multiplication ? Background The question bothers me since I have been reading about dilations and scaling of geometrical objects in Paul Lockhart's book ""Measurement"". Geometrically, dilation is a transformation that stretches an object in one dimension by a certain factor. Analogously, the linear transformation $$ \begin{pmatrix}   1 & 0 & 0 \\   0 & 1 & 0 \\   0 & 0 & \lambda \end{pmatrix} \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix} $$ ""stretches"" the third component by the factor $\lambda$. Scaling is a geometric transformation that stretches an object in all dimensions by a certain factor. Analogously, the linear transformation $$ \begin{pmatrix}   \lambda & 0 & 0 \\   0 & \lambda & 0 \\   0 & 0 & \lambda \end{pmatrix} \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix} $$ ""stretches"" all three components by the factor $\lambda$. This, however, can be written more succinctly using scalar multiplication: $$ \lambda \cdot \begin{pmatrix}   x_1 \\   x_2 \\   x_3 \end{pmatrix}. $$ In fact, every scalar multiplication can be expressed as a multiplication with a special matrix, and it turns out to be a mere shortcut. On the face of it, this observation is not very spectacular; however, it raises interesting philosophical and conceptual questions as to the foundations of linear algebra. For example, if scalar multiplication is only a nice-to-have shortcut, then isn't it in fact superfluous conceptually? Currently, scalar multiplication is taught as if it was a distinct concept, independent of matrix multiplication. What would change if we got rid of this shortcut? What could alternative axioms of vector spaces and moduls look like? What about linear transformations? What is easier, what is harder$-$not to write down, but conceptually? I know that this topic is very broad, but I would like to collect opinions, ideas, examples.",,"['linear-algebra', 'soft-question', 'philosophy']"
85,Isomorphic and Homomorphic,Isomorphic and Homomorphic,,"I was wondering if someone could explain the ideas behind isomorphisms and homomorphisms and the difference between them. I understand that a linear map is an isomorphism if it is bijective, and that an isomorphism is a bijective homomorphism but I don't fully understand what a homomorphism actually is. For example, what does it mean to say that $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)\simeq MAT_{\mathbb{C} }\left(3,2 \right)$ where $MAT_{\mathbb{C}}$ is the $3\times2$ matrix with complex entries. After some research, I think that this example is saying that $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)$ is essentially a perfect approximation of $MAT_{\mathbb{C} }\left(3,2 \right)$ however I am struggling to understand what the notation $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)$ actually means. Note: I haven't studied a course in group theory and therefore have little knowledge on it, this is for a Linear Algebra Course","I was wondering if someone could explain the ideas behind isomorphisms and homomorphisms and the difference between them. I understand that a linear map is an isomorphism if it is bijective, and that an isomorphism is a bijective homomorphism but I don't fully understand what a homomorphism actually is. For example, what does it mean to say that $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)\simeq MAT_{\mathbb{C} }\left(3,2 \right)$ where $MAT_{\mathbb{C}}$ is the $3\times2$ matrix with complex entries. After some research, I think that this example is saying that $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)$ is essentially a perfect approximation of $MAT_{\mathbb{C} }\left(3,2 \right)$ however I am struggling to understand what the notation $Hom_{\mathbb{C}}\left(\mathbb{C}^2,\mathbb{C}^3\right)$ actually means. Note: I haven't studied a course in group theory and therefore have little knowledge on it, this is for a Linear Algebra Course",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'group-isomorphism', 'group-homomorphism']"
86,"A matrix is unitary if, and only if, diagonalizable and all eigenvalues are on the unit circle","A matrix is unitary if, and only if, diagonalizable and all eigenvalues are on the unit circle",,I would like to know if the class of unitary matrices is the same as the class of diagonalizable matrices who have all their eigenvalues on the unit circle.,I would like to know if the class of unitary matrices is the same as the class of diagonalizable matrices who have all their eigenvalues on the unit circle.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
87,How does computing the determinant of a matrix with unit vectors give you the Cross Product?,How does computing the determinant of a matrix with unit vectors give you the Cross Product?,,"Say you had $(a_x,a_y,a_z)\times(b_x,b_y,b_z)$, you would set up a matrix like the following: And the resulting would be your Cross Product or the coordinates of an orthogonal vector. My question is why? Why does forming it that way give you the magnitude of an orthogonal vector and how is it related to the $\sin(\theta)$ definition of Cross Product.","Say you had $(a_x,a_y,a_z)\times(b_x,b_y,b_z)$, you would set up a matrix like the following: And the resulting would be your Cross Product or the coordinates of an orthogonal vector. My question is why? Why does forming it that way give you the magnitude of an orthogonal vector and how is it related to the $\sin(\theta)$ definition of Cross Product.",,"['linear-algebra', 'physics', 'cross-product']"
88,Underdetermined Linear Systems,Underdetermined Linear Systems,,"I'm working through an introductory linear algebra textbook and one exercise gives the system $2x+3y+5z+2w=0$ $-5x+6y-17z-3w=0$ $7x-4y+3z+13w=0$ And asks why, without doing any calculations, it has infinitely many solutions. Now, a previous exercise gives the same system without the fourth column and asks why, without any calculation, you can tell it's consistent, and I realized that it's because it has the trivial solution (0,0,0). But I'm struggling to see how that implies that this new system has infinitely many solutions. I did some research and found that if an underdetermined linear system has a solution then it has infinitely many, but the explanations of this seem to talk about rank and other things that I'm not familiar with. So if someone could please explain why you can just tell without doing any calculation why this system has infinitely many solutions (I'm guessing it has something to do with the previous problem that's the same just without that fourth column of variables) from the authors perspective (i.e. they're only assuming we have algebra 2 at this early point in the book)  it would be much appreciated.","I'm working through an introductory linear algebra textbook and one exercise gives the system $2x+3y+5z+2w=0$ $-5x+6y-17z-3w=0$ $7x-4y+3z+13w=0$ And asks why, without doing any calculations, it has infinitely many solutions. Now, a previous exercise gives the same system without the fourth column and asks why, without any calculation, you can tell it's consistent, and I realized that it's because it has the trivial solution (0,0,0). But I'm struggling to see how that implies that this new system has infinitely many solutions. I did some research and found that if an underdetermined linear system has a solution then it has infinitely many, but the explanations of this seem to talk about rank and other things that I'm not familiar with. So if someone could please explain why you can just tell without doing any calculation why this system has infinitely many solutions (I'm guessing it has something to do with the previous problem that's the same just without that fourth column of variables) from the authors perspective (i.e. they're only assuming we have algebra 2 at this early point in the book)  it would be much appreciated.",,['linear-algebra']
89,Does $\operatorname{rank}(T) = \dim(W)$ imply linear transformation is onto.,Does  imply linear transformation is onto.,\operatorname{rank}(T) = \dim(W),"If you have a linear transformation going from $V,W$ , where $V,W$ are finite-dimensional vector spaces. Does the $\dim(\operatorname{range}(T))$ being equal to the dimension of the target space imply that the linear transformation is onto. If it does do you mind explaining why.","If you have a linear transformation going from , where are finite-dimensional vector spaces. Does the being equal to the dimension of the target space imply that the linear transformation is onto. If it does do you mind explaining why.","V,W V,W \dim(\operatorname{range}(T))","['linear-algebra', 'vector-spaces', 'linear-transformations']"
90,What is the relationship between the eigenvectors of $A A^T$ and $A^TA$?,What is the relationship between the eigenvectors of  and ?,A A^T A^TA,"I tried using the fact that the left eigenvectors and right eigenvectors will form a diagonal matrix, but cannot proceed ahead.","I tried using the fact that the left eigenvectors and right eigenvectors will form a diagonal matrix, but cannot proceed ahead.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
91,Why adjugate matrix 2x2 is different from 3x3 and others?,Why adjugate matrix 2x2 is different from 3x3 and others?,,"I've understand the simple way of calculating the adjugate matrix. In short: 1). We need to calculate all cofactors: Using the next formula: $A_{ij} = (-1)^{i+j} M_{ij}$ 2). Transpose it. It's also can be read from Wikipedia: https://en.wikipedia.org/wiki/Adjugate_matrix In linear algebra, the adjugate, classical adjoint, or adjunct of a   square matrix is the transpose of the cofactor matrix. If to view examples, such short algorithm is correct for squared matrices 3x3 and larger... But, for 2x2 is just a rule: M = [ a b ]     [ c d ]  adj( M ) = [  d -b ]            [ -c  a ] What do I dislike? I dislike that, because it's some kind of prayer. And many forums in web are saying: Just remember it? right? Damn! People, it's a MATH, it's not some humanity science like history. It's A MATH, math consists of logic and explanation as any other technical stuff. I don't want just to remember stupidly some formula, I want understand why is exactly using such adjugate calculation especially for matrix 2x2? I don't understand the next... For the matrix 3x3 we calculate all cofactors and then transpose it, for e.g.: Original: 1   2   3 2   5   4 5   2   3  Cofactor matrix:   7  14 -21   0 -12  8  -7   2  1  Transposed cofactor matrix:   7   0 -7  14 -12  2 -21   8  1 As you can see the transpose process for 3x3 didn't exchange the value a11 and a33, but why there is such an exchange for 2x3 matrix? I don't understand... I want to get logical explanation, not just a strict prayer. I want to hear explanation why? and such explanation must be logical.","I've understand the simple way of calculating the adjugate matrix. In short: 1). We need to calculate all cofactors: Using the next formula: $A_{ij} = (-1)^{i+j} M_{ij}$ 2). Transpose it. It's also can be read from Wikipedia: https://en.wikipedia.org/wiki/Adjugate_matrix In linear algebra, the adjugate, classical adjoint, or adjunct of a   square matrix is the transpose of the cofactor matrix. If to view examples, such short algorithm is correct for squared matrices 3x3 and larger... But, for 2x2 is just a rule: M = [ a b ]     [ c d ]  adj( M ) = [  d -b ]            [ -c  a ] What do I dislike? I dislike that, because it's some kind of prayer. And many forums in web are saying: Just remember it? right? Damn! People, it's a MATH, it's not some humanity science like history. It's A MATH, math consists of logic and explanation as any other technical stuff. I don't want just to remember stupidly some formula, I want understand why is exactly using such adjugate calculation especially for matrix 2x2? I don't understand the next... For the matrix 3x3 we calculate all cofactors and then transpose it, for e.g.: Original: 1   2   3 2   5   4 5   2   3  Cofactor matrix:   7  14 -21   0 -12  8  -7   2  1  Transposed cofactor matrix:   7   0 -7  14 -12  2 -21   8  1 As you can see the transpose process for 3x3 didn't exchange the value a11 and a33, but why there is such an exchange for 2x3 matrix? I don't understand... I want to get logical explanation, not just a strict prayer. I want to hear explanation why? and such explanation must be logical.",,"['linear-algebra', 'matrices']"
92,What is the relation between eigenvectors and singular vectors of real skew-symmetric matrices?,What is the relation between eigenvectors and singular vectors of real skew-symmetric matrices?,,"I've read that any real skew-symmetric matrix $A$, where $A^T = -A$, can be brought into block diagonal form $ A = Q \, \Sigma \, Q^T = \left( \begin{array}{ccccl} \vec{q}_1 & \vec{q}_2 & \vec{q}_3 & \vec{q}_4 & \dots \end{array} \right) \, \left( \begin{array}{ccccl} 0 & \lambda_1 & 0 & 0 & \dots \\ -\lambda_1 & 0 & 0 & 0 & \dots \\ 0 & 0 & 0 & \lambda_2 & \dots \\ 0 & 0 & -\lambda_2 & 0 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{array} \right) \, \left( \begin{array}{c} \vec{q}^T_1 \\ \vec{q}^T_2 \\ \vec{q}^T_3 \\ \vec{q}^T_4 \\ \vdots \end{array} \right)$ where $\vec{q}_i$ are real, orthogonal column vectors. But also, the non-zero eigenvalues of $A$ are purely imaginary and occur in pairs $\pm i \lambda_i$, which are the same $\lambda_i$ as occur in the blocks of $\Sigma$. We could then also write $A$ in complex diagonal form $ A = U \, D \, U^\dagger = \left( \begin{array}{ccccl} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 & \vec{u}_4 & \dots \end{array} \right) \, \left( \begin{array}{ccccl} i\lambda_1 & 0 & 0 & 0 & \dots \\ 0 & -i\lambda_1 & 0 & 0 & \dots \\ 0 & 0 & i\lambda_2 & 0 & \dots \\ 0 & 0 & 0 & -i\lambda_2 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{array} \right) \, \left( \begin{array}{c} \vec{u}^\dagger_1 \\ \vec{u}^\dagger_2 \\ \vec{u}^\dagger_3 \\ \vec{u}^\dagger_4 \\ \vdots \end{array} \right)$ where $\vec{u}_i$ are complex, orthogonal column vectors. My question is what is the relationship between the complex eigenvectors $\vec{u}_i$ and the real vectors $\vec{q}_i$ that bring $A$ into block diagonal form? And how can I show the link?","I've read that any real skew-symmetric matrix $A$, where $A^T = -A$, can be brought into block diagonal form $ A = Q \, \Sigma \, Q^T = \left( \begin{array}{ccccl} \vec{q}_1 & \vec{q}_2 & \vec{q}_3 & \vec{q}_4 & \dots \end{array} \right) \, \left( \begin{array}{ccccl} 0 & \lambda_1 & 0 & 0 & \dots \\ -\lambda_1 & 0 & 0 & 0 & \dots \\ 0 & 0 & 0 & \lambda_2 & \dots \\ 0 & 0 & -\lambda_2 & 0 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{array} \right) \, \left( \begin{array}{c} \vec{q}^T_1 \\ \vec{q}^T_2 \\ \vec{q}^T_3 \\ \vec{q}^T_4 \\ \vdots \end{array} \right)$ where $\vec{q}_i$ are real, orthogonal column vectors. But also, the non-zero eigenvalues of $A$ are purely imaginary and occur in pairs $\pm i \lambda_i$, which are the same $\lambda_i$ as occur in the blocks of $\Sigma$. We could then also write $A$ in complex diagonal form $ A = U \, D \, U^\dagger = \left( \begin{array}{ccccl} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 & \vec{u}_4 & \dots \end{array} \right) \, \left( \begin{array}{ccccl} i\lambda_1 & 0 & 0 & 0 & \dots \\ 0 & -i\lambda_1 & 0 & 0 & \dots \\ 0 & 0 & i\lambda_2 & 0 & \dots \\ 0 & 0 & 0 & -i\lambda_2 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{array} \right) \, \left( \begin{array}{c} \vec{u}^\dagger_1 \\ \vec{u}^\dagger_2 \\ \vec{u}^\dagger_3 \\ \vec{u}^\dagger_4 \\ \vdots \end{array} \right)$ where $\vec{u}_i$ are complex, orthogonal column vectors. My question is what is the relationship between the complex eigenvectors $\vec{u}_i$ and the real vectors $\vec{q}_i$ that bring $A$ into block diagonal form? And how can I show the link?",,"['linear-algebra', 'matrices']"
93,If $T\in\mathcal{L}(V)$ is diagonalizable then $V = \mathrm{null}\; T \oplus \mathrm{range}\; T$,If  is diagonalizable then,T\in\mathcal{L}(V) V = \mathrm{null}\; T \oplus \mathrm{range}\; T,"I am self-studying Linear Algebra using Axler Linear Algebra Done Right 3rd Edition. Usually I use the problems from the 2nd edition because there is a solution manual available whereas the 3rd edition has no solutions at all. But I made a mistake with the above because it looked simple. It is 5.C question 1. I think I can prove the statement with $+$ instead of $\oplus$: If $T\in\mathcal{L}(V)$ is diagonalizable then $V = \mathrm{null}\; T \oplus \mathrm{range}\; T$ By 4.41 $V = E(\lambda_1, T) \oplus \dots \oplus E(\lambda_m, T)$. Suppose that $v\in E(\lambda_i, T)$ If $\lambda_i \neq 0$ then $T \frac{1}{\lambda_i} v = \frac{1}{\lambda_i} Tv =  \frac{1}{\lambda_i}  \lambda_i v = v$.  So $v\in \mathrm{range}\;T$. Therefore $E(\lambda_i, T) \subseteq \mathrm{range}\;T$ whenever $\lambda_i\neq 0$. If $\lambda_i = 0$ then $Tv = 0v = 0$ and $v \in \mathrm{null}\;T$. Therefore $E(\lambda_i, T) \subseteq \mathrm{null}\;T$ when $\lambda_i =0$. Let $v\in V$. Then $v = e_1 \dots e_m$ with $e_i \in  E(\lambda_i, T)$. But each $e_i$ is also in  $\mathrm{range}\; T$ or $\mathrm{null}\; T$. So $v \in \mathrm{null}\; T + \mathrm{range}\; T $. This means $V \subseteq \mathrm{null}\; T + \mathrm{range}\; T$ and because $\mathrm{range}\; T$ and $\mathrm{null}\; T$ are subspaces of $V$ it follows that $V = \mathrm{null}\; T +\mathrm{range}\; T$ But I do not see how to prove the sum is a direct sum. I think one needs to prove that the only $v$ in $\mathrm{null}\;T$ that also makes it into $\mathrm{range}\;T$ is $0$ but I can't do that.","I am self-studying Linear Algebra using Axler Linear Algebra Done Right 3rd Edition. Usually I use the problems from the 2nd edition because there is a solution manual available whereas the 3rd edition has no solutions at all. But I made a mistake with the above because it looked simple. It is 5.C question 1. I think I can prove the statement with $+$ instead of $\oplus$: If $T\in\mathcal{L}(V)$ is diagonalizable then $V = \mathrm{null}\; T \oplus \mathrm{range}\; T$ By 4.41 $V = E(\lambda_1, T) \oplus \dots \oplus E(\lambda_m, T)$. Suppose that $v\in E(\lambda_i, T)$ If $\lambda_i \neq 0$ then $T \frac{1}{\lambda_i} v = \frac{1}{\lambda_i} Tv =  \frac{1}{\lambda_i}  \lambda_i v = v$.  So $v\in \mathrm{range}\;T$. Therefore $E(\lambda_i, T) \subseteq \mathrm{range}\;T$ whenever $\lambda_i\neq 0$. If $\lambda_i = 0$ then $Tv = 0v = 0$ and $v \in \mathrm{null}\;T$. Therefore $E(\lambda_i, T) \subseteq \mathrm{null}\;T$ when $\lambda_i =0$. Let $v\in V$. Then $v = e_1 \dots e_m$ with $e_i \in  E(\lambda_i, T)$. But each $e_i$ is also in  $\mathrm{range}\; T$ or $\mathrm{null}\; T$. So $v \in \mathrm{null}\; T + \mathrm{range}\; T $. This means $V \subseteq \mathrm{null}\; T + \mathrm{range}\; T$ and because $\mathrm{range}\; T$ and $\mathrm{null}\; T$ are subspaces of $V$ it follows that $V = \mathrm{null}\; T +\mathrm{range}\; T$ But I do not see how to prove the sum is a direct sum. I think one needs to prove that the only $v$ in $\mathrm{null}\;T$ that also makes it into $\mathrm{range}\;T$ is $0$ but I can't do that.",,['linear-algebra']
94,"Proof that $\dim\text{span}\{x,x^2\cos x,\cos x\}=3$",Proof that,"\dim\text{span}\{x,x^2\cos x,\cos x\}=3","I have the following question : Let $V$ a space of functions from $\mathbb{R}$ to $\mathbb{R}$. Proof that $\dim  \text{span}\{x,x^2\cos x,\cos x\}=3$. For some reason I managed to show the opposite direction, this what I did : Therefore I should show that for any $x \in \mathbb{R}$ $$\lambda_1 x+\lambda_2x^2\cos x+\lambda_3 \cos x=0 \implies \lambda_1=\lambda_2=\lambda_3=0$$ But for $x=0 \implies \lambda_3=0$ and $\lambda_1 \neq 0$ and also $\lambda_2 \neq 0$ Therefore we found $x$ that $\lambda_1 \neq 0$ and $\lambda_2 \neq 0$ so this group is linearly independent. I don't quite understand the thing with the $x$ usually when I need to show linearly independent I get vectors not functions how should I handle it? Any help will be appreciated. Please, do not use any calculus (differentiation, integrals) since I'm not allowed to use it in order to solve the problem. Thank you!","I have the following question : Let $V$ a space of functions from $\mathbb{R}$ to $\mathbb{R}$. Proof that $\dim  \text{span}\{x,x^2\cos x,\cos x\}=3$. For some reason I managed to show the opposite direction, this what I did : Therefore I should show that for any $x \in \mathbb{R}$ $$\lambda_1 x+\lambda_2x^2\cos x+\lambda_3 \cos x=0 \implies \lambda_1=\lambda_2=\lambda_3=0$$ But for $x=0 \implies \lambda_3=0$ and $\lambda_1 \neq 0$ and also $\lambda_2 \neq 0$ Therefore we found $x$ that $\lambda_1 \neq 0$ and $\lambda_2 \neq 0$ so this group is linearly independent. I don't quite understand the thing with the $x$ usually when I need to show linearly independent I get vectors not functions how should I handle it? Any help will be appreciated. Please, do not use any calculus (differentiation, integrals) since I'm not allowed to use it in order to solve the problem. Thank you!",,"['linear-algebra', 'independence']"
95,Prove that $\det\left[A^{T}B-B^{T}A\right]=\det[A+B]\cdot\det\left[A-B\right]$,Prove that,\det\left[A^{T}B-B^{T}A\right]=\det[A+B]\cdot\det\left[A-B\right],"So I need to prove that: $$\det\left[A^{T}B-B^{T}A\right]=\det[A+B]\cdot\det\left[A-B\right]$$ where $A$, $B$ are two orthogonal matrices, but it seems I'm missing something.","So I need to prove that: $$\det\left[A^{T}B-B^{T}A\right]=\det[A+B]\cdot\det\left[A-B\right]$$ where $A$, $B$ are two orthogonal matrices, but it seems I'm missing something.",,['linear-algebra']
96,"$a=b^x+c^x$, How to solve for $x$?",", How to solve for ?",a=b^x+c^x x,"If $a=b^x$, then $x$ could be written in terms of $a$ and $b$; $x=\dfrac{\log(a)}{\log(b)}$. What about $a=b^x+c^x$? Could $x$ be written in terms of $a, b$ and $c$? $x={}$?","If $a=b^x$, then $x$ could be written in terms of $a$ and $b$; $x=\dfrac{\log(a)}{\log(b)}$. What about $a=b^x+c^x$? Could $x$ be written in terms of $a, b$ and $c$? $x={}$?",,"['linear-algebra', 'logarithms']"
97,Dimension of vector space of real numbers over rational number field,Dimension of vector space of real numbers over rational number field,,I know that dimension  of $\mathbb{R} $ over $ \mathbb{Q} $ is infinite. What can i say about the cardinality of its basis mean whether  it is countable or uncountable. Can we find exact basis for that.,I know that dimension  of $\mathbb{R} $ over $ \mathbb{Q} $ is infinite. What can i say about the cardinality of its basis mean whether  it is countable or uncountable. Can we find exact basis for that.,,[]
98,How to determine whether a set is a vector space or not?,How to determine whether a set is a vector space or not?,,"I'm currently learning Vector Spaces and although I understand the definition of what a vector space is, I can't seem to be able to find the correct answers when doing some questions. I would even say that I'm getting some answers right by pure luck and that's defeats the purpose of mathematics. My problem is that I don't know the correct approach to solve these questions. Here are a few questions which I'm trying to do but I'm not sure how to arrive to my answers: In the questions below, I should determine whether each of the sets given is a vector space or not: $V = \{(x,y,z)\in \mathbb{R}^3 : x-y+2z = 3\}$ $V = \{p\in P_{4}[x] : p(0) + p(1) = 0\}$ $V = \{A\in M_{3*3} : A = A^{t}\}$ $V = \{A\in M_{3*3} : AA^{t} = -I\}$ Now, all of the answers are yes except for number $4$, is a vector space. I got $1$ wrong and $4$ wrong. I got numbers $2$ and $3$ right but I'm not sure if my way of finding the answer is right. I normally just use the definition of a Vector Space but it doesn't work all the time. Edit: I'm not simply looking for the final answer( I already have them) but I'm more interested in understanding how to approach such questions to reach the final answer. Edit 2: The answers given in the memo are as follows: 1. Vector Space 2. Vector Space 3. Vector Space 4. Not a  Vector Space Could anyone please explain how to get the answer in detail and if there is a trick to quickly find the answers? Thanks.","I'm currently learning Vector Spaces and although I understand the definition of what a vector space is, I can't seem to be able to find the correct answers when doing some questions. I would even say that I'm getting some answers right by pure luck and that's defeats the purpose of mathematics. My problem is that I don't know the correct approach to solve these questions. Here are a few questions which I'm trying to do but I'm not sure how to arrive to my answers: In the questions below, I should determine whether each of the sets given is a vector space or not: $V = \{(x,y,z)\in \mathbb{R}^3 : x-y+2z = 3\}$ $V = \{p\in P_{4}[x] : p(0) + p(1) = 0\}$ $V = \{A\in M_{3*3} : A = A^{t}\}$ $V = \{A\in M_{3*3} : AA^{t} = -I\}$ Now, all of the answers are yes except for number $4$, is a vector space. I got $1$ wrong and $4$ wrong. I got numbers $2$ and $3$ right but I'm not sure if my way of finding the answer is right. I normally just use the definition of a Vector Space but it doesn't work all the time. Edit: I'm not simply looking for the final answer( I already have them) but I'm more interested in understanding how to approach such questions to reach the final answer. Edit 2: The answers given in the memo are as follows: 1. Vector Space 2. Vector Space 3. Vector Space 4. Not a  Vector Space Could anyone please explain how to get the answer in detail and if there is a trick to quickly find the answers? Thanks.",,"['linear-algebra', 'vector-spaces']"
99,Orthogonal projection matrix,Orthogonal projection matrix,,"Let $A\in M_{m\times n}(\mathbb{R})$. Denoting by $R(A)$ the column space of $A$ and $N(A)$ the null space of $A$. I know that $z^*=Ax^*$ is a projection of $b\in R^m$ on $R(A)=N(A^T)$ where $x^*=Ab$ $r^*=b-z^*$ is a projection of $b\in R^m$ on $N(A^T)=R(A)^\perp$ Similarly $z^*=A^Tx^*$ is a projection of $b\in R^n$ on $R(A^T)=N(A)^\perp$ where $x^*=A^TAx=Ab$ $r^*=b-z^*$ is a projection of $b\in R^n$ on $N(A)=R(A^T)^\perp$ Finally I know the orthogonal projection matrix on the subspace $R(A)$ is defined by $P=A(A^TA)^{-1}A^T$. my question is how do I determine for example the projection matrix on $R(A^T),N(A^T),N(A)$? EDIT:* means nothing is just notation that I used","Let $A\in M_{m\times n}(\mathbb{R})$. Denoting by $R(A)$ the column space of $A$ and $N(A)$ the null space of $A$. I know that $z^*=Ax^*$ is a projection of $b\in R^m$ on $R(A)=N(A^T)$ where $x^*=Ab$ $r^*=b-z^*$ is a projection of $b\in R^m$ on $N(A^T)=R(A)^\perp$ Similarly $z^*=A^Tx^*$ is a projection of $b\in R^n$ on $R(A^T)=N(A)^\perp$ where $x^*=A^TAx=Ab$ $r^*=b-z^*$ is a projection of $b\in R^n$ on $N(A)=R(A^T)^\perp$ Finally I know the orthogonal projection matrix on the subspace $R(A)$ is defined by $P=A(A^TA)^{-1}A^T$. my question is how do I determine for example the projection matrix on $R(A^T),N(A^T),N(A)$? EDIT:* means nothing is just notation that I used",,"['linear-algebra', 'matrices']"
