,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Does the open mapping theorem imply the Baire category theorem?,Does the open mapping theorem imply the Baire category theorem?,,"A nice observation by C.E. Blair 1, 2, 3 shows that the Baire category theorem for complete metric spaces is equivalent to the axiom of (countable) dependent choice . On the other hand, the three classical consequences of the Baire category theorem in basic functional analysis — the open mapping theorem , the closed graph theorem and the uniform boundedness principle (as well as Zabreiko's lemma ) — are equivalent to each other in Zermelo–Fraenkel set theory without choice: that is to say, if one is added as an axiom to ZF then the others follow 4 . Each of these results has a more or less direct proof from the Baire category theorem and all the proofs “avoiding Baire” I'm aware of 5 involve dependent choice in a way that doesn't seem to be replaceable by weaker forms of choice. Hence I'm asking about the converse: Does the open mapping theorem imply the Baire category theorem? If not, is it at least true that the open mapping theorem implies the axiom of dependent choice for subsets of the reals? I imagine that applying any of the above results to a judiciously chosen space and/or operator(s) might yield the desired conclusion, similarly to what happens in Bell's and Fremlin's geometric version of the axiom of choice 6 . Unfortunately, I couldn't find a promising place to start. Needless to say that I checked numerous things on the web form of Howard and Rubin's book Consequences of the Axiom of Choice , but without much success: The only articles that I found this way are J.D. Maitland Wright's articles 7 . Footnotes and References: 1 Charles E. Blair, The Baire category theorem implies the principle of dependent choices , Bull. Acad. Polon. Sci. Sér. Sci. Math. Astronom. Phys. 25 (1977), no. 10, 933–934. 2 Since Blair's article is hard to find, the proof can be found in the notes to chapter 9 , page 95 of John C. Oxtoby, Measure and Category , Springer GTM 2, Second Edition, 1980. 3 Here's the idea of Blair's argument for the implication Baire Category Theorem $\Rightarrow$ Dependent Choice: let $S$ be a set and let $R \subset S \times S$ be a relation such that for all $s \in S$ there exists $t \in S$ such that $(s,t) \in R$. Equip $S^{\mathbb{N}}$ with the complete metric $d(f,g) = 2^{-\min\{n\,:\,f(n)\neq g(n)\}}$, put $$ U_n = \bigcup_{m = n+1}^{\infty} \bigcup_{(s,t) \in R} \{f \in S^{\mathbb{N}}\,:\,f(n) = s, \,f(m)=t\}, $$ observe that $U_n$ is open and dense and use $f \in \bigcap_{n=1}^\infty U_n$ and the well-order on $\mathbb{N}$ to find a strictly increasing sequence $k_1 \lt k_2 \lt \cdots$ such that the sequence $(x_n)_{n=1}^\infty$ given by $x_n = f(k_n)$ satisfies $(x_n, x_{n+1}) \in R$ for all $n \in \mathbb{N}$. 4 See e.g. E. Schechter, Handbook of Analysis and its foundations , 27.27, pp. 734 ff. 5 A good example for this is Sokal's A Really Simple Elementary Proof of the Uniform Boundedness Theorem , The American Mathematical Monthly Vol. 118 , No. 5 (May 2011), pp. 450–452, ArXiV Version . While admittedly it is beautifully simple and elementary, it involves a plain application of dependent choice in the main argument. 6 Bell and Fremlin, A Geometric Form of the Axiom of Choice , Fund. Math. vol. 77 (1972), 167–170. 7 The full list of relevant articles can be obtained with this ZBlatt query two of which appeared in rather obscure proceedings, so I couldn't get my hands on them, yet. The third article is J. D. Maitland Wright, All operators on a Hilbert space are bounded , Bull. Amer. Math. Soc. 79 (1973), 1247–1250.","A nice observation by C.E. Blair 1, 2, 3 shows that the Baire category theorem for complete metric spaces is equivalent to the axiom of (countable) dependent choice . On the other hand, the three classical consequences of the Baire category theorem in basic functional analysis — the open mapping theorem , the closed graph theorem and the uniform boundedness principle (as well as Zabreiko's lemma ) — are equivalent to each other in Zermelo–Fraenkel set theory without choice: that is to say, if one is added as an axiom to ZF then the others follow 4 . Each of these results has a more or less direct proof from the Baire category theorem and all the proofs “avoiding Baire” I'm aware of 5 involve dependent choice in a way that doesn't seem to be replaceable by weaker forms of choice. Hence I'm asking about the converse: Does the open mapping theorem imply the Baire category theorem? If not, is it at least true that the open mapping theorem implies the axiom of dependent choice for subsets of the reals? I imagine that applying any of the above results to a judiciously chosen space and/or operator(s) might yield the desired conclusion, similarly to what happens in Bell's and Fremlin's geometric version of the axiom of choice 6 . Unfortunately, I couldn't find a promising place to start. Needless to say that I checked numerous things on the web form of Howard and Rubin's book Consequences of the Axiom of Choice , but without much success: The only articles that I found this way are J.D. Maitland Wright's articles 7 . Footnotes and References: 1 Charles E. Blair, The Baire category theorem implies the principle of dependent choices , Bull. Acad. Polon. Sci. Sér. Sci. Math. Astronom. Phys. 25 (1977), no. 10, 933–934. 2 Since Blair's article is hard to find, the proof can be found in the notes to chapter 9 , page 95 of John C. Oxtoby, Measure and Category , Springer GTM 2, Second Edition, 1980. 3 Here's the idea of Blair's argument for the implication Baire Category Theorem $\Rightarrow$ Dependent Choice: let $S$ be a set and let $R \subset S \times S$ be a relation such that for all $s \in S$ there exists $t \in S$ such that $(s,t) \in R$. Equip $S^{\mathbb{N}}$ with the complete metric $d(f,g) = 2^{-\min\{n\,:\,f(n)\neq g(n)\}}$, put $$ U_n = \bigcup_{m = n+1}^{\infty} \bigcup_{(s,t) \in R} \{f \in S^{\mathbb{N}}\,:\,f(n) = s, \,f(m)=t\}, $$ observe that $U_n$ is open and dense and use $f \in \bigcap_{n=1}^\infty U_n$ and the well-order on $\mathbb{N}$ to find a strictly increasing sequence $k_1 \lt k_2 \lt \cdots$ such that the sequence $(x_n)_{n=1}^\infty$ given by $x_n = f(k_n)$ satisfies $(x_n, x_{n+1}) \in R$ for all $n \in \mathbb{N}$. 4 See e.g. E. Schechter, Handbook of Analysis and its foundations , 27.27, pp. 734 ff. 5 A good example for this is Sokal's A Really Simple Elementary Proof of the Uniform Boundedness Theorem , The American Mathematical Monthly Vol. 118 , No. 5 (May 2011), pp. 450–452, ArXiV Version . While admittedly it is beautifully simple and elementary, it involves a plain application of dependent choice in the main argument. 6 Bell and Fremlin, A Geometric Form of the Axiom of Choice , Fund. Math. vol. 77 (1972), 167–170. 7 The full list of relevant articles can be obtained with this ZBlatt query two of which appeared in rather obscure proceedings, so I couldn't get my hands on them, yet. The third article is J. D. Maitland Wright, All operators on a Hilbert space are bounded , Bull. Amer. Math. Soc. 79 (1973), 1247–1250.",,"['functional-analysis', 'set-theory', 'banach-spaces', 'axiom-of-choice', 'baire-category']"
1,$L^p$ and $L^q$ space inclusion,and  space inclusion,L^p L^q,"Let $(X, \mathcal B, m)$ be a measure space.  For $1 \leq p < q \leq \infty$, under what condition is it true that $L^q(X, \mathcal B, m) \subset L^p(X, \mathcal B, m)$ and what is a counterexample in the case the condition is not satisfied?","Let $(X, \mathcal B, m)$ be a measure space.  For $1 \leq p < q \leq \infty$, under what condition is it true that $L^q(X, \mathcal B, m) \subset L^p(X, \mathcal B, m)$ and what is a counterexample in the case the condition is not satisfied?",,"['functional-analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
2,Continuous projections on $\ell_1$ with norm $>1$,Continuous projections on  with norm,\ell_1 >1,"I was trying to find papers and articles about non-contractive continuous projections on $\ell_1(S)$ where $S$ is an arbitrary set. If it is not studied yet, I would like to know results for the case $S=\mathbb{N}$. I've found one quite general condition for the closed linear subspace to be image of a continuous projection. Such a subspace must be the closure of the linear span of so-called relatively disjoint vectors. This subspaces gives us explicit examples of projections with norm greater than 1. For details see the paper of H. P. Rosenthal On relatively disjoint families of measures, with some applications to Banach space theory . As a special case we can get subspaces that are the closure of the linear span of disjointly supported vectors. These subspaces give us examples of norm one projections. Moreover, only such subspaces are give rise to norm one projections. For details see the survey by Beata Randrianantoanina Norm one projections in Banach spaces . Thus, for projections of norm 1 we have a complete description. For the rest quite a big source of examples. In the first mentioned paper the author states that he doesn't know any other examples of continuous projections on $\ell_1(S)$ that are not generated by some relatively disjoint family of vectors. So, could someone give me a reference where I can read about other examples of projections on $\ell_1(S)$, or may be their complete characterization? Also I will be grateful if you give me some explicit examples of discontinuous projections on $\ell_1(S)$. The same question on mathoverflow.net.","I was trying to find papers and articles about non-contractive continuous projections on $\ell_1(S)$ where $S$ is an arbitrary set. If it is not studied yet, I would like to know results for the case $S=\mathbb{N}$. I've found one quite general condition for the closed linear subspace to be image of a continuous projection. Such a subspace must be the closure of the linear span of so-called relatively disjoint vectors. This subspaces gives us explicit examples of projections with norm greater than 1. For details see the paper of H. P. Rosenthal On relatively disjoint families of measures, with some applications to Banach space theory . As a special case we can get subspaces that are the closure of the linear span of disjointly supported vectors. These subspaces give us examples of norm one projections. Moreover, only such subspaces are give rise to norm one projections. For details see the survey by Beata Randrianantoanina Norm one projections in Banach spaces . Thus, for projections of norm 1 we have a complete description. For the rest quite a big source of examples. In the first mentioned paper the author states that he doesn't know any other examples of continuous projections on $\ell_1(S)$ that are not generated by some relatively disjoint family of vectors. So, could someone give me a reference where I can read about other examples of projections on $\ell_1(S)$, or may be their complete characterization? Also I will be grateful if you give me some explicit examples of discontinuous projections on $\ell_1(S)$. The same question on mathoverflow.net.",,"['reference-request', 'functional-analysis', 'banach-spaces']"
3,Why don't analysts do category theory?,Why don't analysts do category theory?,,"I'm a mathematics student in abstract algebra and algebraic geometry. Most of my books cover a great deal of category theory and it is an essential tool in understanding these two subjects. Recently, I started taking some functional analysis courses and I discovered that there is almost no category theory done in these courses. But since most of the spaces studied in functional analysis are objects in categories (e.g. the normed spaces form a category), I find it rather strange that the books leave the category theory out. Is there a reason for this?","I'm a mathematics student in abstract algebra and algebraic geometry. Most of my books cover a great deal of category theory and it is an essential tool in understanding these two subjects. Recently, I started taking some functional analysis courses and I discovered that there is almost no category theory done in these courses. But since most of the spaces studied in functional analysis are objects in categories (e.g. the normed spaces form a category), I find it rather strange that the books leave the category theory out. Is there a reason for this?",,"['analysis', 'functional-analysis', 'soft-question', 'category-theory']"
4,Good book for self study of functional analysis,Good book for self study of functional analysis,,I am a EE grad. student who has had one undergraduate course in real analysis (which was pretty much the only pure math course that I have ever done). I would like to do a self study of some basic functional analysis so that I can be better prepared to take a graduate course in that material in my university. I plan to do that next fall so I do have some time to work through a book fully. Could some one recommend some good books to start working on this? Thanks in advance,I am a EE grad. student who has had one undergraduate course in real analysis (which was pretty much the only pure math course that I have ever done). I would like to do a self study of some basic functional analysis so that I can be better prepared to take a graduate course in that material in my university. I plan to do that next fall so I do have some time to work through a book fully. Could some one recommend some good books to start working on this? Thanks in advance,,"['functional-analysis', 'reference-request', 'book-recommendation']"
5,Lebesgue measure theory vs differential forms?,Lebesgue measure theory vs differential forms?,,"I am currently reading various differential geometry books. From what I understand differential forms allow us to generalize calculus to manifolds and thus perform integration on manifolds. I gather that it is, in general, completely distinct from Lebesgue measure theory and is more like a generalization of Riemann integration. Ok so here's the problem. I have always viewed Lebesgue measure theory as 'solving the issues with Riemann integration'. For example, a big problem with Riemann integration  is that the space of Riemann integral functions is not complete. The fact that $L^p$ spaces in the Lebesgue theory are complete seems like a huge improvement on the Riemann situation, and is vital for so many concepts in functional analysis, PDEs, operator theory, and numerical analysis. So if we then consider differential geometry and integration via differential forms, unless I am misunderstanding something, we lose all the benefits of Lebesgue theory? It seems like if do lose all those benefits we are in a very bad situation. For example, how are we supposed to rigorously define solution spaces for PDEs if we can't use $L^p$ spaces and thus can't use Sobolev spaces? How can we obtain acceptable convergence of some sequence that may arise during our work if we are operating in this generalized Riemann setting where we lack completeness? In summary, if differential forms are a generalization of Riemann integration how are we supposed to perform analysis when we no longer have the power and utility of Lebesgue measure theory?","I am currently reading various differential geometry books. From what I understand differential forms allow us to generalize calculus to manifolds and thus perform integration on manifolds. I gather that it is, in general, completely distinct from Lebesgue measure theory and is more like a generalization of Riemann integration. Ok so here's the problem. I have always viewed Lebesgue measure theory as 'solving the issues with Riemann integration'. For example, a big problem with Riemann integration  is that the space of Riemann integral functions is not complete. The fact that $L^p$ spaces in the Lebesgue theory are complete seems like a huge improvement on the Riemann situation, and is vital for so many concepts in functional analysis, PDEs, operator theory, and numerical analysis. So if we then consider differential geometry and integration via differential forms, unless I am misunderstanding something, we lose all the benefits of Lebesgue theory? It seems like if do lose all those benefits we are in a very bad situation. For example, how are we supposed to rigorously define solution spaces for PDEs if we can't use $L^p$ spaces and thus can't use Sobolev spaces? How can we obtain acceptable convergence of some sequence that may arise during our work if we are operating in this generalized Riemann setting where we lack completeness? In summary, if differential forms are a generalization of Riemann integration how are we supposed to perform analysis when we no longer have the power and utility of Lebesgue measure theory?",,"['functional-analysis', 'measure-theory', 'differential-geometry', 'partial-differential-equations', 'differential-forms']"
6,Space of bounded continuous functions is complete,Space of bounded continuous functions is complete,,"I have lecture notes with the claim $(C_b(X), \|\cdot\|_\infty)$, the space of bounded continuous functions with the sup norm is complete. The lecturer then proved two things, (i) that $f(x) = \lim f_n (x)$ is bounded and (ii) that $\lim f_n \in \mathbb{R}$. I don't understand why it's not enough that $f$ is bounded. I think the limit of a sequence of continuous functions is continuous and then if $f$ is bounded, it's in $C_b(X)$. So what is this $\lim f_n \in \mathbb{R}$ about? Many thanks for your help.","I have lecture notes with the claim $(C_b(X), \|\cdot\|_\infty)$, the space of bounded continuous functions with the sup norm is complete. The lecturer then proved two things, (i) that $f(x) = \lim f_n (x)$ is bounded and (ii) that $\lim f_n \in \mathbb{R}$. I don't understand why it's not enough that $f$ is bounded. I think the limit of a sequence of continuous functions is continuous and then if $f$ is bounded, it's in $C_b(X)$. So what is this $\lim f_n \in \mathbb{R}$ about? Many thanks for your help.",,"['functional-analysis', 'banach-spaces', 'complete-spaces']"
7,Not every metric is induced from a norm,Not every metric is induced from a norm,,"I have studied that every normed space $(V, \lVert\cdot \lVert)$ is a metric space with respect to distance function $d(u,v) = \lVert u - v \rVert$, $u,v \in V$. My question is whether every metric on a linear space can be induced by norm? I know answer is  no but I need proper justification. Edit: Is there any method to check whether a given metric space is induced by norm ? Thanks for help","I have studied that every normed space $(V, \lVert\cdot \lVert)$ is a metric space with respect to distance function $d(u,v) = \lVert u - v \rVert$, $u,v \in V$. My question is whether every metric on a linear space can be induced by norm? I know answer is  no but I need proper justification. Edit: Is there any method to check whether a given metric space is induced by norm ? Thanks for help",,"['functional-analysis', 'metric-spaces', 'normed-spaces', 'examples-counterexamples']"
8,The Duals of $l^\infty$ and $L^{\infty}$,The Duals of  and,l^\infty L^{\infty},"Can we identify the dual space of $l^\infty$ with another ""natural space""?  If the answer is yes, what can we say about $L^\infty$ ? By the dual space I mean the space of all continuous linear functionals.","Can we identify the dual space of with another ""natural space""?  If the answer is yes, what can we say about ? By the dual space I mean the space of all continuous linear functionals.",l^\infty L^\infty,"['functional-analysis', 'banach-spaces', 'lp-spaces', 'dual-spaces']"
9,Paul Erdos's Two-Line Functional Analysis Proof,Paul Erdos's Two-Line Functional Analysis Proof,,"Legends hold that once upon a time, some mathematicians were rather pleased about a 30-ish page result in functional analysis. Paul Erdos, upon learning of the problem, spent ten or so minutes thinking about the original problem, and came up with a two-line proof. I believe I read about this first in the biography The Man Who Loved Only Numbers , and it seems as though the internet maintains this legend (q.v. here , here , here ). This seems extraordinary, which leads me to some skepticism. I cannot seem to find any other reference to the actual proof or the problem. Is this simply an urban legend? Did it actually happen? What was the problem, and what was the original 30-page result?","Legends hold that once upon a time, some mathematicians were rather pleased about a 30-ish page result in functional analysis. Paul Erdos, upon learning of the problem, spent ten or so minutes thinking about the original problem, and came up with a two-line proof. I believe I read about this first in the biography The Man Who Loved Only Numbers , and it seems as though the internet maintains this legend (q.v. here , here , here ). This seems extraordinary, which leads me to some skepticism. I cannot seem to find any other reference to the actual proof or the problem. Is this simply an urban legend? Did it actually happen? What was the problem, and what was the original 30-page result?",,"['functional-analysis', 'reference-request', 'math-history']"
10,What are the applications of functional analysis?,What are the applications of functional analysis?,,"I recently had a course on functional analysis. I was thinking of studying the mathematical applications of functional analysis. I came to know it had some applications on calculus of variations. I am not specifically interested in applications of functional analysis on pure branches of mathematics but rather interested in applied mathematics. Can anyone give a brief on what are the mathematical applications of functional analysis? Also, please suggest some good books for it.","I recently had a course on functional analysis. I was thinking of studying the mathematical applications of functional analysis. I came to know it had some applications on calculus of variations. I am not specifically interested in applications of functional analysis on pure branches of mathematics but rather interested in applied mathematics. Can anyone give a brief on what are the mathematical applications of functional analysis? Also, please suggest some good books for it.",,"['functional-analysis', 'reference-request', 'book-recommendation', 'applications']"
11,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable.,Let  be an infinite dimensional Banach space. Prove that every Hamel basis of  is uncountable.,X X,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable. Can anyone help how can I solve the above problem?,Let be an infinite dimensional Banach space. Prove that every Hamel basis of is uncountable. Can anyone help how can I solve the above problem?,X X,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'baire-category', 'hamel-basis']"
12,Equivalent Definitions of the Operator Norm,Equivalent Definitions of the Operator Norm,,How do you prove that these four definitions of the operator norm are equivalent? $$\begin{align*} \lVert A\rVert_{\mathrm{op}} &= \inf\{ c\;\colon\; \lVert Av\rVert\leq c\lVert v\rVert \text{ for all }v\in V\}\\ &=\sup\{ \lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert\leq 1\}\\ &=\sup\{\lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert = 1 \}\\ &=\sup\left\{ \frac{\lVert Av\rVert}{\lVert v\rVert}\;\colon\; v\in V\text{ with }v\neq 0\right\}. \end{align*}$$,How do you prove that these four definitions of the operator norm are equivalent?,"\begin{align*}
\lVert A\rVert_{\mathrm{op}} &= \inf\{ c\;\colon\; \lVert Av\rVert\leq c\lVert v\rVert \text{ for all }v\in V\}\\
&=\sup\{ \lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert\leq 1\}\\
&=\sup\{\lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert = 1 \}\\
&=\sup\left\{ \frac{\lVert Av\rVert}{\lVert v\rVert}\;\colon\; v\in V\text{ with }v\neq 0\right\}.
\end{align*}","['functional-analysis', 'linear-transformations', 'operator-theory', 'normed-spaces']"
13,"Connections between metrics, norms and scalar products (for understanding e.g. Banach and Hilbert spaces)","Connections between metrics, norms and scalar products (for understanding e.g. Banach and Hilbert spaces)",,"I am trying to understand the differences between $$ \begin{array}{|l|l|l|} \textbf{vector space} & \textbf{general} & \textbf{+ completeness}\\\hline \text{metric}& \text{metric space} & \text{complete space}\\ \text{norm} & \text{normed} & \text{Banach space}\\ \text{scalar product} & \text{pre-Hilbert space}  & \text{Hilbert space}\\\hline \end{array} $$ What I don't understand are the differences and connections between metric, norm and scalar product. Obviously, there is some kind of hierarchy but I don't get the full picture. Can anybody help with some good explanations/examples and/or readable references?","I am trying to understand the differences between $$ \begin{array}{|l|l|l|} \textbf{vector space} & \textbf{general} & \textbf{+ completeness}\\\hline \text{metric}& \text{metric space} & \text{complete space}\\ \text{norm} & \text{normed} & \text{Banach space}\\ \text{scalar product} & \text{pre-Hilbert space}  & \text{Hilbert space}\\\hline \end{array} $$ What I don't understand are the differences and connections between metric, norm and scalar product. Obviously, there is some kind of hierarchy but I don't get the full picture. Can anybody help with some good explanations/examples and/or readable references?",,"['functional-analysis', 'vector-spaces', 'banach-spaces', 'hilbert-spaces', 'inner-products']"
14,"Laplace, Legendre, Fourier, Hankel, Mellin, Hilbert, Borel, Z...: unified treatment of transforms?","Laplace, Legendre, Fourier, Hankel, Mellin, Hilbert, Borel, Z...: unified treatment of transforms?",,"I understand ""transform methods"" as recipes, but beyond this they are a big mystery to me. There are two aspects of them I find bewildering. One is the sheer number of them.  Is there a unified framework that includes all these transforms as special cases? The second one is heuristic: what would lead anyone to discover such a transform in the course of solving a problem? (My hope is to find a unified treatment of the subject that simultaneously addresses both of these questions.)","I understand ""transform methods"" as recipes, but beyond this they are a big mystery to me. There are two aspects of them I find bewildering. One is the sheer number of them.  Is there a unified framework that includes all these transforms as special cases? The second one is heuristic: what would lead anyone to discover such a transform in the course of solving a problem? (My hope is to find a unified treatment of the subject that simultaneously addresses both of these questions.)",,"['functional-analysis', 'problem-solving', 'transformation', 'integral-transforms']"
15,How do you show monotonicity of the $\ell^p$ norms?,How do you show monotonicity of the  norms?,\ell^p,I can't seem to work out the inequality $(\sum |x_n|^q)^{1/q} \leq (\sum |x_n|^p)^{1/p}$ for $p \leq q$ (which I'm assuming is the way to go about it).,I can't seem to work out the inequality $(\sum |x_n|^q)^{1/q} \leq (\sum |x_n|^p)^{1/p}$ for $p \leq q$ (which I'm assuming is the way to go about it).,,"['analysis', 'functional-analysis', 'inequality', 'lp-spaces']"
16,Dual norm intuition,Dual norm intuition,,"The dual of a norm $\|\cdot \|$ is defined as: $$\|z\|_* = \sup \{ z^Tx \text{ } | \text{ } \|x\| \le 1\}$$ Could anybody give me an intuition of this concept? I know the definition, I am using it to solve problems, but in reality I still lack intuitive understanding of it.","The dual of a norm $\|\cdot \|$ is defined as: $$\|z\|_* = \sup \{ z^Tx \text{ } | \text{ } \|x\| \le 1\}$$ Could anybody give me an intuition of this concept? I know the definition, I am using it to solve problems, but in reality I still lack intuitive understanding of it.",,"['functional-analysis', 'convex-analysis', 'normed-spaces']"
17,"""Every linear mapping on a finite dimensional space is continuous""","""Every linear mapping on a finite dimensional space is continuous""",,"From Wiki Every linear function on a finite-dimensional space is continuous. I was wondering what the domain and codomain of such linear function are? Are they any two topological vector spaces (not necessarily the same), as along as the domain is finite-dimensional? Can the codomain be a different normed space (and may not be finite-dimensional)? I asked this because I saw elsewhere the same statement except the domain is a finite-dimensional normed space, and am also not sure if the codomain can be a different normed space (and may not be finite-dimensional). Thanks and regards!","From Wiki Every linear function on a finite-dimensional space is continuous. I was wondering what the domain and codomain of such linear function are? Are they any two topological vector spaces (not necessarily the same), as along as the domain is finite-dimensional? Can the codomain be a different normed space (and may not be finite-dimensional)? I asked this because I saw elsewhere the same statement except the domain is a finite-dimensional normed space, and am also not sure if the codomain can be a different normed space (and may not be finite-dimensional). Thanks and regards!",,"['functional-analysis', 'continuity', 'linear-transformations', 'normed-spaces', 'topological-vector-spaces']"
18,Does a Fourier transformation on a (pseudo-)Riemannian manifold make sense?,Does a Fourier transformation on a (pseudo-)Riemannian manifold make sense?,,"the Fourier transformation of a scalar function with respect to one variable might be defined as $\mathcal{F}\left[w\right](\omega )\equiv \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}w(t)e^{-\mathrm{i}\omega t}dt$ In physics, this transformation along with its generalization, the Laplace transformation, has a tremendous importance because of its feature to turn linear partial differential equations into algebraic ones. Now, suppose that we have a pseudo-Riemannian manifold $\mathcal{M}$ where $\det{g_{\mu\nu}} = -1$ holds like in special and general relativity. I am wondering, what would be the generalization of the Fourier transformation of scalar functions or forms? The difficulty I have with this question is that a three-dimensional slicing of $\mathcal{M}$ is not unique, so how to take the integral in an invariant form? What will happen to the differntials $dx^{\mu}$, e.g. $dt,dx^i\rightarrow d\omega dx^i$ in some sense? Any insight would be well appreciated. Thank you in advance Robert","the Fourier transformation of a scalar function with respect to one variable might be defined as $\mathcal{F}\left[w\right](\omega )\equiv \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}w(t)e^{-\mathrm{i}\omega t}dt$ In physics, this transformation along with its generalization, the Laplace transformation, has a tremendous importance because of its feature to turn linear partial differential equations into algebraic ones. Now, suppose that we have a pseudo-Riemannian manifold $\mathcal{M}$ where $\det{g_{\mu\nu}} = -1$ holds like in special and general relativity. I am wondering, what would be the generalization of the Fourier transformation of scalar functions or forms? The difficulty I have with this question is that a three-dimensional slicing of $\mathcal{M}$ is not unique, so how to take the integral in an invariant form? What will happen to the differntials $dx^{\mu}$, e.g. $dt,dx^i\rightarrow d\omega dx^i$ in some sense? Any insight would be well appreciated. Thank you in advance Robert",,"['differential-geometry', 'functional-analysis', 'integral-transforms']"
19,"Explicit norm on $\mathcal{C}^0(\mathbb{R},\mathbb{R})$",Explicit norm on,"\mathcal{C}^0(\mathbb{R},\mathbb{R})","Do you know an explicit norm on $\mathcal{C}^0(\mathbb{R},\mathbb{R})$? Using the axiom of choice, every vector space admits a norm but have you an explicit formula on  $\mathcal{C}^0(\mathbb{R},\mathbb{R})$? A related question is: Can we proved that $\mathcal{C}^0(\mathbb{R},\mathbb{R})$ has a norm without the axiom of choice?","Do you know an explicit norm on $\mathcal{C}^0(\mathbb{R},\mathbb{R})$? Using the axiom of choice, every vector space admits a norm but have you an explicit formula on  $\mathcal{C}^0(\mathbb{R},\mathbb{R})$? A related question is: Can we proved that $\mathcal{C}^0(\mathbb{R},\mathbb{R})$ has a norm without the axiom of choice?",,"['functional-analysis', 'axiom-of-choice', 'normed-spaces']"
20,The identity cannot be a commutator in a Banach algebra?,The identity cannot be a commutator in a Banach algebra?,,"The Wikipedia article on Banach algebras claims, without a proof or reference, that there does not exist a (unital) Banach algebra $B$ and elements $x, y \in B$ such that $xy - yx = 1$. This is surprising to me, but maybe the proof is straightforward; anyone have a proof and/or a reference? More generally, I would have naively thought that I could embed any ring into a Banach algebra. I guess there are actually serious restrictions to doing this; are these issues discussed anywhere?","The Wikipedia article on Banach algebras claims, without a proof or reference, that there does not exist a (unital) Banach algebra $B$ and elements $x, y \in B$ such that $xy - yx = 1$. This is surprising to me, but maybe the proof is straightforward; anyone have a proof and/or a reference? More generally, I would have naively thought that I could embed any ring into a Banach algebra. I guess there are actually serious restrictions to doing this; are these issues discussed anywhere?",,"['functional-analysis', 'banach-algebras']"
21,When is the image of a linear operator closed?,When is the image of a linear operator closed?,,"Let $X$ , $Y$ be Banach spaces. Let $T \colon X \to Y$ be a bounded linear operator. Under what circumstances is the image of $T$ closed in $Y$ (except finite-dimensional image). In particular, I wonder under which assumptions $T \colon X \to T(X)$ is a bounded linear bijection between Banach spaces, so it is at least an isomorphism onto its image by bounded inverse theorem.","Let , be Banach spaces. Let be a bounded linear operator. Under what circumstances is the image of closed in (except finite-dimensional image). In particular, I wonder under which assumptions is a bounded linear bijection between Banach spaces, so it is at least an isomorphism onto its image by bounded inverse theorem.",X Y T \colon X \to Y T Y T \colon X \to T(X),"['functional-analysis', 'banach-spaces']"
22,Are vague convergence and weak convergence of measures both weak* convergence?,Are vague convergence and weak convergence of measures both weak* convergence?,,"For quite a long time, I have been confused about the definitions of weak convergence and vague convergence of measures among other modes of convergence that root from functional analysis, mainly due to many different definitions and theorems from probability books. I would appreiciate it if someone can clarify the terms and give a clear picture of the concepts. (Note that Did has answered some of my related questions before. Thank you, Did!) In Kallenberg's probability book, he defines weak convergence of a sequence measures to be Consider any probability measures $\mu$ and $\mu_1, \mu_2, \dots$ on some metric space $(S, \rho)$ with Borel a-field $S$, and say   that $\mu_n$ converges weakly to $\mu$, if $\int f d\mu_n \to \int f d\mu$ for every $f \in C_b(S)$, the class of bounded, continuous   functions $f: S \to \mathbb R$. Kallenberg defines vague convergence of a sequence of measures to be Consider the space $\mathcal M = \mathcal M(\mathbb R^d) $of locally finite mea- sures on $\mathbb R^d$. On $\mathcal M$ we may   introduce the vague topology, generated by the mappings $\mu \mapsto \int f d\mu$ for all $f \in C_K^+$, the class of continuous   functions $f: \mathbb R^d \to  \mathbb R_+$ with compact support.   In particular, $\mu_n$ is said to converge vaguely to $\mu$  if   $\mu_n f \to \mu f$ for all $f \in C_K^+$. If the $\mu_n$ are   probability measures, then clearly $\mu(\mathbb R^d) < 1$. Folland in his real ananlysis book defines vague topology and therefore vague convergence for complex Radon measures on a locally compact Hausdorff (LCH) space $X$ as weak* topology and weak* convergence wrt $C_0(X)$. He says the term ""vague"" is common in probability theory, and has the advantage of forming an adverb more gracefully than ""weak*"". The vague topology is sometimes called the weak topology, but this terminology conflicts with his, since $C_0(X)$ is rarely reflexive. In Kai Lai Chung's probability book, a sequence of subprobability measures $\mu_n$ on $\mathbb R$ are defined to vaguely converge to another subprobability measure $\mu$, if there exists a dense subset $D$ of $\mathbb R$ s.t. $\forall a, b \in D, a < b, \mu_n((a,b]) \to \mu((a,b])$. Next in Chung's, Theorem 4.4.1 says in case of subprobability measures, vague convergence is equivalent to  weak* convergence wrt $C_0(\mathbb R)$ and $C_K(\mathbb R)$. Theorem 4.4.2 says in case of probability measures, vague convergence is equivalent to  weak* convergence wrt $C_b(\mathbb R)$. I was wondering if the above definitions of weak convergence and vague convergence are all weak* convergence, in the sense that the measures form (a subset of) the continuous dual of $C_b$, $C_K$, $C_K^+$, and $C_0$? When defining vague convergence and vague topology, why does kallenberg use $C_K^+$ instead of $C_K$, Folland use $C_0$, and Kai Lai Chung uses $C_K$, $C_0$ and $C_b$? Are their definitions of vague convergence consistent with each other? Among the convergences of measures wrt $C_b$, $C_K$, $C_K^+$, and $C_0$, when does which imply which? When is which equivalent to which? The last question is to see if there are some unifications of the above concepts. Can the above definitions be generalized to more general measures (probability measures, subprobability measures, locally finite measures are used in the definitions above), and to more general underlying spaces (metric space, $\mathbb R^d$ and $\mathbb R$ are used in the definitions above)? Thanks and regards!!","For quite a long time, I have been confused about the definitions of weak convergence and vague convergence of measures among other modes of convergence that root from functional analysis, mainly due to many different definitions and theorems from probability books. I would appreiciate it if someone can clarify the terms and give a clear picture of the concepts. (Note that Did has answered some of my related questions before. Thank you, Did!) In Kallenberg's probability book, he defines weak convergence of a sequence measures to be Consider any probability measures $\mu$ and $\mu_1, \mu_2, \dots$ on some metric space $(S, \rho)$ with Borel a-field $S$, and say   that $\mu_n$ converges weakly to $\mu$, if $\int f d\mu_n \to \int f d\mu$ for every $f \in C_b(S)$, the class of bounded, continuous   functions $f: S \to \mathbb R$. Kallenberg defines vague convergence of a sequence of measures to be Consider the space $\mathcal M = \mathcal M(\mathbb R^d) $of locally finite mea- sures on $\mathbb R^d$. On $\mathcal M$ we may   introduce the vague topology, generated by the mappings $\mu \mapsto \int f d\mu$ for all $f \in C_K^+$, the class of continuous   functions $f: \mathbb R^d \to  \mathbb R_+$ with compact support.   In particular, $\mu_n$ is said to converge vaguely to $\mu$  if   $\mu_n f \to \mu f$ for all $f \in C_K^+$. If the $\mu_n$ are   probability measures, then clearly $\mu(\mathbb R^d) < 1$. Folland in his real ananlysis book defines vague topology and therefore vague convergence for complex Radon measures on a locally compact Hausdorff (LCH) space $X$ as weak* topology and weak* convergence wrt $C_0(X)$. He says the term ""vague"" is common in probability theory, and has the advantage of forming an adverb more gracefully than ""weak*"". The vague topology is sometimes called the weak topology, but this terminology conflicts with his, since $C_0(X)$ is rarely reflexive. In Kai Lai Chung's probability book, a sequence of subprobability measures $\mu_n$ on $\mathbb R$ are defined to vaguely converge to another subprobability measure $\mu$, if there exists a dense subset $D$ of $\mathbb R$ s.t. $\forall a, b \in D, a < b, \mu_n((a,b]) \to \mu((a,b])$. Next in Chung's, Theorem 4.4.1 says in case of subprobability measures, vague convergence is equivalent to  weak* convergence wrt $C_0(\mathbb R)$ and $C_K(\mathbb R)$. Theorem 4.4.2 says in case of probability measures, vague convergence is equivalent to  weak* convergence wrt $C_b(\mathbb R)$. I was wondering if the above definitions of weak convergence and vague convergence are all weak* convergence, in the sense that the measures form (a subset of) the continuous dual of $C_b$, $C_K$, $C_K^+$, and $C_0$? When defining vague convergence and vague topology, why does kallenberg use $C_K^+$ instead of $C_K$, Folland use $C_0$, and Kai Lai Chung uses $C_K$, $C_0$ and $C_b$? Are their definitions of vague convergence consistent with each other? Among the convergences of measures wrt $C_b$, $C_K$, $C_K^+$, and $C_0$, when does which imply which? When is which equivalent to which? The last question is to see if there are some unifications of the above concepts. Can the above definitions be generalized to more general measures (probability measures, subprobability measures, locally finite measures are used in the definitions above), and to more general underlying spaces (metric space, $\mathbb R^d$ and $\mathbb R$ are used in the definitions above)? Thanks and regards!!",,"['functional-analysis', 'measure-theory', 'probability-theory']"
23,Are these two Banach spaces isometrically isomorphic?,Are these two Banach spaces isometrically isomorphic?,,"Let $c$ denote the space of convergent sequences in $\mathbb C$, $c_0\subset c$ be the space of all sequences that converge to $0$. Given the uniform metric, both of them can be made into Banach spaces. It can be shown that the dual spaces of them are isometrically isomorphic, i.e. $c^*\cong c_0^*$. Are $c$ and $c_0$ isometrically isomorphic? If not, how can one show the absence of such a isometric isomorphism? Thanks!","Let $c$ denote the space of convergent sequences in $\mathbb C$, $c_0\subset c$ be the space of all sequences that converge to $0$. Given the uniform metric, both of them can be made into Banach spaces. It can be shown that the dual spaces of them are isometrically isomorphic, i.e. $c^*\cong c_0^*$. Are $c$ and $c_0$ isometrically isomorphic? If not, how can one show the absence of such a isometric isomorphism? Thanks!",,"['functional-analysis', 'banach-spaces']"
24,"How to show that quotient space $X/Y$ is complete when $X$ is Banach space, and $Y$ is a closed subspace of $X$?","How to show that quotient space  is complete when  is Banach space, and  is a closed subspace of ?",X/Y X Y X,"How to show that quotient space $X/Y$ is complete when $X$ is Banach space, and $Y$ is a closed subspace of $X$? Here's my attempt: Given a Cauchy sequence $\{q_n\}_{n \in \mathbb{N}}$ in $X/Y$, each $q_n$ is an equivalence class induced by $Y$, I want to find a representative $x_n$ in $q_n$ so that the induced sequence $\{x_n\}_{n \in \mathbb{N}}$ is also a Cauchy sequence in $X$. But I don't know how to construct such sequence.","How to show that quotient space $X/Y$ is complete when $X$ is Banach space, and $Y$ is a closed subspace of $X$? Here's my attempt: Given a Cauchy sequence $\{q_n\}_{n \in \mathbb{N}}$ in $X/Y$, each $q_n$ is an equivalence class induced by $Y$, I want to find a representative $x_n$ in $q_n$ so that the induced sequence $\{x_n\}_{n \in \mathbb{N}}$ is also a Cauchy sequence in $X$. But I don't know how to construct such sequence.",,"['functional-analysis', 'vector-spaces', 'banach-spaces', 'quotient-spaces']"
25,Example of a compact set that isn't the spectrum of an operator,Example of a compact set that isn't the spectrum of an operator,,"This question is a follow-up to this recent question and related to that one . Is there an easy example of an (infinite-dimensional) Banach space $X$ and a non-empty compact set $K \subset \mathbb{C}$ that can't be the spectrum of a bounded operator $A: X \to X$? Of course, if $X$ contains an infinite-dimensional Hilbert space as a norm one complemented subspace and $\emptyset \neq K \subset \mathbb{C}$ is an arbitrary compact set then we can produce an operator $A: X \to X$ such that $\sigma(A) = K$. As Jonas Meyer pointed out in his answer , the recent breakthrough by Argyros and Haydon settling the long-standing scalar-plus-compact problem (see Gowers's blog entry for some background) shows that there is a space with the property that the only possbile spectra of bounded operators are the countable and compact subsets of $\mathbb{C}$ with at most one accumulation point. ( Update: Jonas Meyer has added further information and pointers to the literature to his answer, I'll refrain from repeating this information here since I couldn't add anything of interest.) But these examples are definitely far more involved than I would like them to be. If it turns out that the example has to be so difficult for some reason that eludes me, I'd like to hear about that, too. More optimistically, one might ask: Are there known classes of infinite-dimensional Banach spaces for which there is a characterization of the compact subsets of $\mathbb{C}$ that may arise as spectra of bounded operators? A nice answer to this optimistic question would be: The class of such-and-such Banach spaces has the property that only/precisely the, say, totally disconnected compact subsets of $\mathbb{C}$ arise as spectra of bounded operators. Update: In view of the question in the title, I'm of course most interested in answers that exclude certain compact subsets of $\mathbb{C}$. Update 2: I asked a slightly updated version of this question on MathOverflow .","This question is a follow-up to this recent question and related to that one . Is there an easy example of an (infinite-dimensional) Banach space $X$ and a non-empty compact set $K \subset \mathbb{C}$ that can't be the spectrum of a bounded operator $A: X \to X$? Of course, if $X$ contains an infinite-dimensional Hilbert space as a norm one complemented subspace and $\emptyset \neq K \subset \mathbb{C}$ is an arbitrary compact set then we can produce an operator $A: X \to X$ such that $\sigma(A) = K$. As Jonas Meyer pointed out in his answer , the recent breakthrough by Argyros and Haydon settling the long-standing scalar-plus-compact problem (see Gowers's blog entry for some background) shows that there is a space with the property that the only possbile spectra of bounded operators are the countable and compact subsets of $\mathbb{C}$ with at most one accumulation point. ( Update: Jonas Meyer has added further information and pointers to the literature to his answer, I'll refrain from repeating this information here since I couldn't add anything of interest.) But these examples are definitely far more involved than I would like them to be. If it turns out that the example has to be so difficult for some reason that eludes me, I'd like to hear about that, too. More optimistically, one might ask: Are there known classes of infinite-dimensional Banach spaces for which there is a characterization of the compact subsets of $\mathbb{C}$ that may arise as spectra of bounded operators? A nice answer to this optimistic question would be: The class of such-and-such Banach spaces has the property that only/precisely the, say, totally disconnected compact subsets of $\mathbb{C}$ arise as spectra of bounded operators. Update: In view of the question in the title, I'm of course most interested in answers that exclude certain compact subsets of $\mathbb{C}$. Update 2: I asked a slightly updated version of this question on MathOverflow .",,"['functional-analysis', 'banach-spaces', 'spectral-theory']"
26,Separable Hilbert space have a countable orthonormal basis,Separable Hilbert space have a countable orthonormal basis,,"I want to show that every an infinite-dimensional separable (contains countable dense set) Hilbert space has a countable orthonormal basis. I know that every orthogonal set in a separable Hilbert space is countable, it is help me with the proof?","I want to show that every an infinite-dimensional separable (contains countable dense set) Hilbert space has a countable orthonormal basis. I know that every orthogonal set in a separable Hilbert space is countable, it is help me with the proof?",,"['functional-analysis', 'hilbert-spaces']"
27,Hahn-Banach From Systems of Linear Equations,Hahn-Banach From Systems of Linear Equations,,"In this paper 1 on the history of functional analysis, the author mentions the following example of an infinite system of linear equations in an infinite number of variables $c_i = A_{ij} x_j$: \begin{align*} \begin{array}{ccccccccc} 1 & = & x_1 & + & x_2 & + & x_3 & + & \dots  \\ 1 & = &     &   & x_2 & + & x_3 & + & \dots \\ 1 & = &     &   &     &   & x_3 & + & \dots \\   & \vdots  &   &     &   &     &   &   &   \ddots  \end{array} \to \begin{bmatrix} 1 \\ 1 \\ 1 \\ \vdots \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 & \dots \\   & 1 & 1 & \dots \\   &   & 1 & \dots \\   &   &   & \ddots \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \end{bmatrix}  \end{align*} as an example of a system such that any finite truncation of the system down to an $n \times n$ system has a unique solution $x_1 = \dots = x_{n=1} = 0, x_n = 1$ but for which the full system has no solution. This book 2 has the following passage on systems such as this one: The Hahn-Banach theorem arose from attempts to solve infinite systems of linear equations... The key to the solvability is determining ""compatibility"" of the system of equations. For example, the system $x + y = 2$ and $x + y = 4$ cannot be solved because it requires contradictory things and so are ""incompatible"". The first attempts to determine compatibility for infinite systems of linear equations extended known determinant and row-reduction techniques. It was a classical analysis - almost solve the problem in a finite situation, then take a limit. A fatal defect of these approaches was the need for the (very rare) convergence of infinite products."" and then mentions a theorem about these systems that motivates Hahn-Banach: Theorem 7.10.1 shows that to solve a certain system of linear equations,    it is necessary and sufficient that a continuity-type condition be satisfied. Theorem 7.10.1 (The Functional Problem): Let $X$ be a normed space over $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$, let $\{x_s \ : \ s  \in S \}$ and $\{ c_s \ : \ s \in S \}$ be sets of vectors and scalars, respectively. Then there is a continuous linear functional $f$ on $X$ such    that $f(x_s) = c_s$ for each $s \in S$ iff there exists $K > 0$ such that    \begin{equation} \left|\sum_{s \in S} a_s c_s \right| \leq K  \left\| \sum_{s \in S} a_s x_S \right\| \tag{1},  \end{equation}   for any choice of scalars $\{a_s \ : \ s \in S \}$ for which $a_s = 0$ for all but finitely many $s \in S$ (""almost all"" the $a_s = 0$). Banach used the Hahn-Banach theorem to prove Theorem 7.10.1 but Theorem 7.10.1 implies the Hahn-Banach theorem: Assuming that Theorem 7.10.1 holds, let $\{ x_s \}$ be the vectors of a subspace $M$, let $f$ be a continuous linear functional on $M$; for each $s \in S$, let $c_s = f(x_s)$. Since $f$ is continuous, $(1)$ is satisfied and $f$ possesses a continuous extension to $X$. My question is: If you knew none of the theorems just mentioned, how would one begin from the system $c_i = A_{ij} x_j$ at the beginning of this post and think of setting up the conditions of theorem 7.10.1 as a way to test whether this system has a solution? How does this test show the system has no solution? How do we re-formulate this process as though we were applying the Hahn-Banach theorem? Does anybody know of a reference for the classical analysis of systems in terms of infinite products? 1 Neal L. Carothers: A Brief History of Functional Analysis . 2 Lawrence Narici, Edward Beckenstein: Topological Vector Spaces , 2nd Edition.","In this paper 1 on the history of functional analysis, the author mentions the following example of an infinite system of linear equations in an infinite number of variables $c_i = A_{ij} x_j$: \begin{align*} \begin{array}{ccccccccc} 1 & = & x_1 & + & x_2 & + & x_3 & + & \dots  \\ 1 & = &     &   & x_2 & + & x_3 & + & \dots \\ 1 & = &     &   &     &   & x_3 & + & \dots \\   & \vdots  &   &     &   &     &   &   &   \ddots  \end{array} \to \begin{bmatrix} 1 \\ 1 \\ 1 \\ \vdots \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 & \dots \\   & 1 & 1 & \dots \\   &   & 1 & \dots \\   &   &   & \ddots \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \end{bmatrix}  \end{align*} as an example of a system such that any finite truncation of the system down to an $n \times n$ system has a unique solution $x_1 = \dots = x_{n=1} = 0, x_n = 1$ but for which the full system has no solution. This book 2 has the following passage on systems such as this one: The Hahn-Banach theorem arose from attempts to solve infinite systems of linear equations... The key to the solvability is determining ""compatibility"" of the system of equations. For example, the system $x + y = 2$ and $x + y = 4$ cannot be solved because it requires contradictory things and so are ""incompatible"". The first attempts to determine compatibility for infinite systems of linear equations extended known determinant and row-reduction techniques. It was a classical analysis - almost solve the problem in a finite situation, then take a limit. A fatal defect of these approaches was the need for the (very rare) convergence of infinite products."" and then mentions a theorem about these systems that motivates Hahn-Banach: Theorem 7.10.1 shows that to solve a certain system of linear equations,    it is necessary and sufficient that a continuity-type condition be satisfied. Theorem 7.10.1 (The Functional Problem): Let $X$ be a normed space over $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$, let $\{x_s \ : \ s  \in S \}$ and $\{ c_s \ : \ s \in S \}$ be sets of vectors and scalars, respectively. Then there is a continuous linear functional $f$ on $X$ such    that $f(x_s) = c_s$ for each $s \in S$ iff there exists $K > 0$ such that    \begin{equation} \left|\sum_{s \in S} a_s c_s \right| \leq K  \left\| \sum_{s \in S} a_s x_S \right\| \tag{1},  \end{equation}   for any choice of scalars $\{a_s \ : \ s \in S \}$ for which $a_s = 0$ for all but finitely many $s \in S$ (""almost all"" the $a_s = 0$). Banach used the Hahn-Banach theorem to prove Theorem 7.10.1 but Theorem 7.10.1 implies the Hahn-Banach theorem: Assuming that Theorem 7.10.1 holds, let $\{ x_s \}$ be the vectors of a subspace $M$, let $f$ be a continuous linear functional on $M$; for each $s \in S$, let $c_s = f(x_s)$. Since $f$ is continuous, $(1)$ is satisfied and $f$ possesses a continuous extension to $X$. My question is: If you knew none of the theorems just mentioned, how would one begin from the system $c_i = A_{ij} x_j$ at the beginning of this post and think of setting up the conditions of theorem 7.10.1 as a way to test whether this system has a solution? How does this test show the system has no solution? How do we re-formulate this process as though we were applying the Hahn-Banach theorem? Does anybody know of a reference for the classical analysis of systems in terms of infinite products? 1 Neal L. Carothers: A Brief History of Functional Analysis . 2 Lawrence Narici, Edward Beckenstein: Topological Vector Spaces , 2nd Edition.",,"['functional-analysis', 'systems-of-equations', 'topological-vector-spaces', 'infinite-product', 'hahn-banach-theorem']"
28,When exactly is the dual of $L^1$ isomorphic to $L^\infty$ via the natural map?,When exactly is the dual of  isomorphic to  via the natural map?,L^1 L^\infty,"The dual space to the Banach space $L^1(\mu)$ for a sigma-finite measure $\mu$ is $L^\infty(\mu)$, given by the correspondence $\phi \in L^\infty(\mu) \mapsto I_\phi$, where $I_\phi(f) = \int f \cdot \phi \,d\mu$ for $f \in L^1(\mu)$. Now, we consider this for more general measures $\mu$. If $\mu$ is not semifinite, then there is a measurable subset $A$ with $\mu(A) = \infty$ but $\mu(B) = 0$ for every subset $B$ of $A$ with $\mu(B) < \infty$. Any $\phi$ supported on $A$ will then map to the zero functional, so the correspondence is not one-to-one. If $\mu$ is semifinite, the correspondence is one-to-one, but is it onto? Using the Radon-Nikodym Thorem, any member of $L^1(B)^*$ for $\mu(B) < \infty$ can be represented by an $L^\infty(B)$ function class $[\phi_B]$. Unlike in the $\sigma$-finite measure case, however, I do not see an easy way to form a ""patchwork"" $\phi$ with $\phi|_B = \phi_B$ a.e.$[\mu]$. I suspect this may have an easy answer, but it eludes me. Or perhaps there is a well-known counterexample?","The dual space to the Banach space $L^1(\mu)$ for a sigma-finite measure $\mu$ is $L^\infty(\mu)$, given by the correspondence $\phi \in L^\infty(\mu) \mapsto I_\phi$, where $I_\phi(f) = \int f \cdot \phi \,d\mu$ for $f \in L^1(\mu)$. Now, we consider this for more general measures $\mu$. If $\mu$ is not semifinite, then there is a measurable subset $A$ with $\mu(A) = \infty$ but $\mu(B) = 0$ for every subset $B$ of $A$ with $\mu(B) < \infty$. Any $\phi$ supported on $A$ will then map to the zero functional, so the correspondence is not one-to-one. If $\mu$ is semifinite, the correspondence is one-to-one, but is it onto? Using the Radon-Nikodym Thorem, any member of $L^1(B)^*$ for $\mu(B) < \infty$ can be represented by an $L^\infty(B)$ function class $[\phi_B]$. Unlike in the $\sigma$-finite measure case, however, I do not see an easy way to form a ""patchwork"" $\phi$ with $\phi|_B = \phi_B$ a.e.$[\mu]$. I suspect this may have an easy answer, but it eludes me. Or perhaps there is a well-known counterexample?",,"['functional-analysis', 'measure-theory', 'lp-spaces', 'dual-spaces']"
29,Looking for a function such that...,Looking for a function such that...,,"There was this question on one of the whiteboards at my company, and I found it intriguing. Maybe it's a dumb thing to ask. Maybe there is a simple answer that I couldn't see. Anyway, here it is: Does there exist a non-trivial, monotonically increasing function such that $f'(x) = f(f(x))$ (in $\mathbb{R}$)? I checked a bunch of functions, from elementary to special (gamma, digamma, zeta, Riemann, Lambert ...) and none seems to work (not surprisingly). I managed to convince myself that a function expressible as a power series would not work, regardless of convergence issues, because the derivative lowers the degree of polynomials, when the composition raises it. The Dirac delta or some sort of generalized function looked promising for a while, but the Dirac delta is not monotonically increasing anyway, and I'm not very familiar with generalized functions. I tried to use the Fourier transform on both sides, but it seems the Fourier transform is difficult for f(f(x)) (at least for me). I thought about somehow seeing ""taking the derivative"" as a differential operator, finding its (infinite) matrix in some basis (which one?), do the same thing to the RHS and show that the 2 matrices could not be identified (reducing the problem to a linear algebra problem) - that didn't work. Nothing on the geometric front either. I thought about trying to prove that there is no such function by deducing a contradiction, but didn't manage that. My hunch is that no such function exists, based on the completely invalid and semi-meaningless idea that differentiation pulls f in one direction, and composition in the other. Any idea?","There was this question on one of the whiteboards at my company, and I found it intriguing. Maybe it's a dumb thing to ask. Maybe there is a simple answer that I couldn't see. Anyway, here it is: Does there exist a non-trivial, monotonically increasing function such that $f'(x) = f(f(x))$ (in $\mathbb{R}$)? I checked a bunch of functions, from elementary to special (gamma, digamma, zeta, Riemann, Lambert ...) and none seems to work (not surprisingly). I managed to convince myself that a function expressible as a power series would not work, regardless of convergence issues, because the derivative lowers the degree of polynomials, when the composition raises it. The Dirac delta or some sort of generalized function looked promising for a while, but the Dirac delta is not monotonically increasing anyway, and I'm not very familiar with generalized functions. I tried to use the Fourier transform on both sides, but it seems the Fourier transform is difficult for f(f(x)) (at least for me). I thought about somehow seeing ""taking the derivative"" as a differential operator, finding its (infinite) matrix in some basis (which one?), do the same thing to the RHS and show that the 2 matrices could not be identified (reducing the problem to a linear algebra problem) - that didn't work. Nothing on the geometric front either. I thought about trying to prove that there is no such function by deducing a contradiction, but didn't manage that. My hunch is that no such function exists, based on the completely invalid and semi-meaningless idea that differentiation pulls f in one direction, and composition in the other. Any idea?",,"['functional-analysis', 'functions', 'functional-equations']"
30,A Banach space is reflexive if and only if its dual is reflexive,A Banach space is reflexive if and only if its dual is reflexive,,How to show that a Banach space $X$ is reflexive if and only if its dual $X'$ is reflexive?,How to show that a Banach space $X$ is reflexive if and only if its dual $X'$ is reflexive?,,"['functional-analysis', 'banach-spaces', 'dual-spaces', 'reflexive-space']"
31,Derivative of convolution,Derivative of convolution,,"Assume that $f(x),g(x)$ are positive and are in $L^1$ . Moreover, they are differentiable and their derivative is integrable. Let $h(x)=f(x)*g(x)$ , the convolution of $f$ and $g$ . Does the derivative of $h(x)$ exist? If yes, how can we prove that $$ \frac{d}{dx}(f(x)*g(x)) = \left(\frac{d}{dx}f(x)\right)*g(x)$$ Thanks","Assume that are positive and are in . Moreover, they are differentiable and their derivative is integrable. Let , the convolution of and . Does the derivative of exist? If yes, how can we prove that Thanks","f(x),g(x) L^1 h(x)=f(x)*g(x) f g h(x)  \frac{d}{dx}(f(x)*g(x)) = \left(\frac{d}{dx}f(x)\right)*g(x)","['functional-analysis', 'fourier-analysis']"
32,"Is there an explicit isomorphism between $L^\infty[0,1]$ and $\ell^\infty$?",Is there an explicit isomorphism between  and ?,"L^\infty[0,1] \ell^\infty","Is there an explicit isomorphism between $L^\infty[0,1]$ and   $\ell^\infty$? In some sense, this is a follow-up to my answer to this question where the non-isomorphism between the spaces $L^r$ and $\ell^s$ for $1 \leq r,s \lt \infty$, unless $r$ and $s$ are both two was discussed (among other things). There is the somewhat surprising fact that the Banach spaces $X = L^\infty[0,1]$ and $Y = \ell^\infty$ are isomorphic. More precisely, there are mutually inverse bounded linear maps $T:  X \to Y$ and $S: Y \to X$ (see below for a proof of existence). Is there a direct and explicit way to prove this? In other words: I'm wondering     whether there is an explicit and natural expression for either $S$ or $T$. Here's the argument I know: Using Pełczyński's decomposition technique one can prove that $X = L^\infty$ and $Y = \ell^\infty$ are isomorphic as Banach spaces: Choose a countable partition $[0,1] = \bigcup_{n=0}^\infty E_n$ into disjoint sets of positive measure and send $(x_n)_{n \in \mathbb{N}} \in \ell^\infty$ to $\sum_{n=0}^\infty x_n [E_n]$ to get an isometric embedding $i: Y \hookrightarrow X$. Since $\ell^\infty$ is injective, its image is complemented, in particular, this yields a decomposition $X \cong Y \oplus \widetilde{Y}$. Choose a dense sequence $(f_n)_{n \in \mathbb{N}}$ of the unit sphere of $L^1[0,1]$. For $h \in L^\infty[0,1]$ let $j(h) = \left( \int f_n \, h \right)_{n \in \mathbb{N}} \in \ell^\infty$ to get an isometric map $j: L^\infty[0,1] \to \ell^\infty$. Since $L^\infty[0,1]$ is injective, its image is complemented in $\ell^\infty$, so this yields a decomposition $Y \cong X \oplus \widetilde{X}$. Observe that $X \cong X \oplus X$ since $L^\infty[0,1] = L^\infty[0,1/2] \oplus L^\infty[1/2,1] \cong L^\infty [0,1] \oplus L^\infty [0,1]$ and $Y \cong Y \oplus Y$ by decomposing $\mathbb{N}$ into the sets of even and odd numbers. Thus, Pełczyński's argument yields: $$X \cong Y \oplus \widetilde{Y} \cong (Y \oplus Y) \oplus \widetilde{Y} \cong Y \oplus (Y \oplus \widetilde{Y}) \cong Y \oplus X$$ and  $$Y \cong X \oplus \widetilde{X} \cong (X \oplus X) \oplus \widetilde{X} \cong X \oplus (X \oplus \widetilde{X}) \cong X \oplus Y$$ so that $X \cong Y \oplus X \cong X \oplus Y \cong Y$. Of course, one can trace through this argument and “construct” an  isomorphism, but the resulting maps are rather messier than what I'm looking for. A further deficit of this argument is that the appeal to injectivity properties makes this inherently non-constructive. Any simplifications of this argument or pointers to the literature would be welcome.","Is there an explicit isomorphism between $L^\infty[0,1]$ and   $\ell^\infty$? In some sense, this is a follow-up to my answer to this question where the non-isomorphism between the spaces $L^r$ and $\ell^s$ for $1 \leq r,s \lt \infty$, unless $r$ and $s$ are both two was discussed (among other things). There is the somewhat surprising fact that the Banach spaces $X = L^\infty[0,1]$ and $Y = \ell^\infty$ are isomorphic. More precisely, there are mutually inverse bounded linear maps $T:  X \to Y$ and $S: Y \to X$ (see below for a proof of existence). Is there a direct and explicit way to prove this? In other words: I'm wondering     whether there is an explicit and natural expression for either $S$ or $T$. Here's the argument I know: Using Pełczyński's decomposition technique one can prove that $X = L^\infty$ and $Y = \ell^\infty$ are isomorphic as Banach spaces: Choose a countable partition $[0,1] = \bigcup_{n=0}^\infty E_n$ into disjoint sets of positive measure and send $(x_n)_{n \in \mathbb{N}} \in \ell^\infty$ to $\sum_{n=0}^\infty x_n [E_n]$ to get an isometric embedding $i: Y \hookrightarrow X$. Since $\ell^\infty$ is injective, its image is complemented, in particular, this yields a decomposition $X \cong Y \oplus \widetilde{Y}$. Choose a dense sequence $(f_n)_{n \in \mathbb{N}}$ of the unit sphere of $L^1[0,1]$. For $h \in L^\infty[0,1]$ let $j(h) = \left( \int f_n \, h \right)_{n \in \mathbb{N}} \in \ell^\infty$ to get an isometric map $j: L^\infty[0,1] \to \ell^\infty$. Since $L^\infty[0,1]$ is injective, its image is complemented in $\ell^\infty$, so this yields a decomposition $Y \cong X \oplus \widetilde{X}$. Observe that $X \cong X \oplus X$ since $L^\infty[0,1] = L^\infty[0,1/2] \oplus L^\infty[1/2,1] \cong L^\infty [0,1] \oplus L^\infty [0,1]$ and $Y \cong Y \oplus Y$ by decomposing $\mathbb{N}$ into the sets of even and odd numbers. Thus, Pełczyński's argument yields: $$X \cong Y \oplus \widetilde{Y} \cong (Y \oplus Y) \oplus \widetilde{Y} \cong Y \oplus (Y \oplus \widetilde{Y}) \cong Y \oplus X$$ and  $$Y \cong X \oplus \widetilde{X} \cong (X \oplus X) \oplus \widetilde{X} \cong X \oplus (X \oplus \widetilde{X}) \cong X \oplus Y$$ so that $X \cong Y \oplus X \cong X \oplus Y \cong Y$. Of course, one can trace through this argument and “construct” an  isomorphism, but the resulting maps are rather messier than what I'm looking for. A further deficit of this argument is that the appeal to injectivity properties makes this inherently non-constructive. Any simplifications of this argument or pointers to the literature would be welcome.",,"['functional-analysis', 'reference-request', 'banach-spaces', 'lp-spaces', 'axiom-of-choice']"
33,Does convergence of polynomials imply that of its coefficients?,Does convergence of polynomials imply that of its coefficients?,,"Let $\{p_{n}\}$ be a sequence of polynomials and $f$ a continuous function on $[0,1]$ such that $\int\limits_{0}^{1}|p_{n}(x)-f(x)|dx\to 0$ . Let $c_{n,k}$ be the coefficient of $x^{k}$ in $p_{n}(x)$ . Can we conclude that $\underset{n\rightarrow \infty }{\lim }c_{n,k}$ exists for each $k$ ?. What I know so far: if the degrees of $p_{n}^{\prime }s$ are bounded then this is true. In fact, we can replace $L^{1}$ convergence by convergence in any norm on $C[0,1]$ ; to see this we just have to note that for fixed $N$ , $% \sum_{k=0}^{N}c_{i}x^{i}\rightarrow (c_{0},c_{1},...,c_{N})$ is a linear map on a finite-dimensional subspace and hence it is continuous. My guess is that the result fails when there is no restriction on the degrees. But if $p_{n}(z)$ converges uniformly in some disk around $0$ in the complex plane then the conclusion holds. To construct a counterexample we have to avoid this situation. Maybe there is a very simple example but I haven't been to find one. Thank you for investing your time on this.","Let be a sequence of polynomials and a continuous function on such that . Let be the coefficient of in . Can we conclude that exists for each ?. What I know so far: if the degrees of are bounded then this is true. In fact, we can replace convergence by convergence in any norm on ; to see this we just have to note that for fixed , is a linear map on a finite-dimensional subspace and hence it is continuous. My guess is that the result fails when there is no restriction on the degrees. But if converges uniformly in some disk around in the complex plane then the conclusion holds. To construct a counterexample we have to avoid this situation. Maybe there is a very simple example but I haven't been to find one. Thank you for investing your time on this.","\{p_{n}\} f [0,1] \int\limits_{0}^{1}|p_{n}(x)-f(x)|dx\to 0 c_{n,k} x^{k} p_{n}(x) \underset{n\rightarrow \infty }{\lim }c_{n,k} k p_{n}^{\prime }s L^{1} C[0,1] N %
\sum_{k=0}^{N}c_{i}x^{i}\rightarrow (c_{0},c_{1},...,c_{N}) p_{n}(z) 0","['functional-analysis', 'polynomials', 'convergence-divergence']"
34,Was Grothendieck familiar with Stone's work on Boolean algebras?,Was Grothendieck familiar with Stone's work on Boolean algebras?,,"In short, my question is: Was Grothendieck familiar with Stone's work on Boolean algebras? Background: In an answer to Pierre-Yves Gaillard's question Did Zariski really define the Zariski topology on the prime spectrum of a ring? I let myself get carried away and explained a result of Grothendieck that (for me) implies that Grothendieck certainly was familiar with Stone's work on spectra and even proved theorems with it. Qiaochu suggested that I ask a question and answer myself (apparently officially encouraged , see his remark), so I'm doing that in order to avoid an off-topic answer to Pierre-Yves's question. Qiaochu's accepted answer quotes excerpts from Johnstone's Stone spaces that seem to imply that Grothendieck never quoted Stone. Precisely I'm having the following passage in mind: But again, one will not find any reference to Stone in the work of Grothendieck, even though his use of the word 'spectrum' is an obvious echo of [Stone 1940] , and Grothendieck, with his background in functional analysis, must have been familiar with Stone's work in that field. I did not seriously try to verify or falsify the first part of the sentence (and please do provide references if you happen to know of them). My own long answer addresses the second part of the sentence and tries to make a point that must have been should be replaced by was . Now fire away and complain about this being a nitpick, but I'm trying to explain a nice and interesting piece of mathematics and both Jonas Meyer and Qiaochu Yuan said I should post this answer, so: that's what I'm doing here.","In short, my question is: Was Grothendieck familiar with Stone's work on Boolean algebras? Background: In an answer to Pierre-Yves Gaillard's question Did Zariski really define the Zariski topology on the prime spectrum of a ring? I let myself get carried away and explained a result of Grothendieck that (for me) implies that Grothendieck certainly was familiar with Stone's work on spectra and even proved theorems with it. Qiaochu suggested that I ask a question and answer myself (apparently officially encouraged , see his remark), so I'm doing that in order to avoid an off-topic answer to Pierre-Yves's question. Qiaochu's accepted answer quotes excerpts from Johnstone's Stone spaces that seem to imply that Grothendieck never quoted Stone. Precisely I'm having the following passage in mind: But again, one will not find any reference to Stone in the work of Grothendieck, even though his use of the word 'spectrum' is an obvious echo of [Stone 1940] , and Grothendieck, with his background in functional analysis, must have been familiar with Stone's work in that field. I did not seriously try to verify or falsify the first part of the sentence (and please do provide references if you happen to know of them). My own long answer addresses the second part of the sentence and tries to make a point that must have been should be replaced by was . Now fire away and complain about this being a nitpick, but I'm trying to explain a nice and interesting piece of mathematics and both Jonas Meyer and Qiaochu Yuan said I should post this answer, so: that's what I'm doing here.",,"['functional-analysis', 'category-theory', 'banach-spaces', 'math-history', 'tensor-products']"
35,What is the relation between weak convergence of measures and weak convergence from functional analysis,What is the relation between weak convergence of measures and weak convergence from functional analysis,,"To keep things simple, we assume $X$ to be a polish space (think of $X$ as $\mathbb{R}^n$ for example). Let's denote with $P(X)$ the space of all Borel probability measure on $X$. We say $\{\mu_n\}\subset P(X)$ converges weakly to $\mu\in P(X)$, denoted by $\mu_n\Rightarrow \mu$ if $$\int fd\mu_n\to\int fd\mu,\forall f\in C_b(X)$$ where $C_b(X)$ is the space of all bounded continuous real valued functions. Let this definition be seen as the purely probabilistic one. From functional analysis we also have a concept of weak convergence and weak topology. Let $E$ be a Banach space and denote by $E'$ the dual. Then by considering the family $\{\phi_f:f\in E'\}$, where $\phi_f:E\to\mathbb{R}$ is the linear functional $\phi_f(x):=\langle f,x\rangle$, the weak topology on $E$ is the coarsest topology which makes all $\phi_f$ continuous. One can prove $x_n\to x$ weakly (in weak topology) if and only if $\phi_f(x_n)\to\phi(x)$ for all $f\in E'$. Since $X$ is Polish, we have that $P(X)$ is Polish too. Now I can define a continuous linear functional $\phi_f: P(X)\to\mathbb{R}$ for $f\in C_b(X)$ by $$\phi_f(\mu):=\int fd\mu$$ Infact $\phi_f\in P(X)'$. Therefore we have $C_b(X)\subset P(X)'$. My questions are $\textbf{1. Question}$ Is the dual $P(X)'$ known? Is it isomorphic to a well known space? Is $C_b(X)$ a proper subspace? $\textbf{2. Question}$ The notation of weak convergence in the probabilistic sense ($\mu_n\Rightarrow \mu$) is weaker than the weak convergence in the functional analytical sense. What I mean is: If $\mu_n\to\mu$ in the weak toplogy, i.e. $\langle f,\mu_n\rangle \to \langle f,\mu\rangle $ for all $f\in P(X)'$ this implies $\mu_n\Rightarrow \mu$ since $C_b(X)\subset P(X)'$. Is this the reason why one calls $\mu_n\Rightarrow \mu$ weak convergence, or is there any other reason?","To keep things simple, we assume $X$ to be a polish space (think of $X$ as $\mathbb{R}^n$ for example). Let's denote with $P(X)$ the space of all Borel probability measure on $X$. We say $\{\mu_n\}\subset P(X)$ converges weakly to $\mu\in P(X)$, denoted by $\mu_n\Rightarrow \mu$ if $$\int fd\mu_n\to\int fd\mu,\forall f\in C_b(X)$$ where $C_b(X)$ is the space of all bounded continuous real valued functions. Let this definition be seen as the purely probabilistic one. From functional analysis we also have a concept of weak convergence and weak topology. Let $E$ be a Banach space and denote by $E'$ the dual. Then by considering the family $\{\phi_f:f\in E'\}$, where $\phi_f:E\to\mathbb{R}$ is the linear functional $\phi_f(x):=\langle f,x\rangle$, the weak topology on $E$ is the coarsest topology which makes all $\phi_f$ continuous. One can prove $x_n\to x$ weakly (in weak topology) if and only if $\phi_f(x_n)\to\phi(x)$ for all $f\in E'$. Since $X$ is Polish, we have that $P(X)$ is Polish too. Now I can define a continuous linear functional $\phi_f: P(X)\to\mathbb{R}$ for $f\in C_b(X)$ by $$\phi_f(\mu):=\int fd\mu$$ Infact $\phi_f\in P(X)'$. Therefore we have $C_b(X)\subset P(X)'$. My questions are $\textbf{1. Question}$ Is the dual $P(X)'$ known? Is it isomorphic to a well known space? Is $C_b(X)$ a proper subspace? $\textbf{2. Question}$ The notation of weak convergence in the probabilistic sense ($\mu_n\Rightarrow \mu$) is weaker than the weak convergence in the functional analytical sense. What I mean is: If $\mu_n\to\mu$ in the weak toplogy, i.e. $\langle f,\mu_n\rangle \to \langle f,\mu\rangle $ for all $f\in P(X)'$ this implies $\mu_n\Rightarrow \mu$ since $C_b(X)\subset P(X)'$. Is this the reason why one calls $\mu_n\Rightarrow \mu$ weak convergence, or is there any other reason?",,"['functional-analysis', 'measure-theory', 'probability-theory']"
36,On the norm of a quotient of a Banach space.,On the norm of a quotient of a Banach space.,,"Let $E$ be a Banach space and $F$ a closed subspace. It is well known that the quotient space $E/F$ is also a Banach space with respect to the norm  $$ \left\Vert x+F\right\Vert_{E/F}=\inf\{\left\Vert y\right\Vert_E\mid y\in x+F\}. $$ Unfortunately in a set of lecture notes on (Lie) group representations (material for our study group) the author accidentally used here $\min$ instead of $\inf$. Probably a mostly harmless booboo, because at that point it was only needed to get a Banach space structure on the quotient, and we will probably be concentrating on Hilbert spaces anyway, where the problem does not arise. Namely from Rudin's Functional Analysis I could not find a proof that the minimum should always be attained. Except in the case of a Hilbert space, where an application of parallelogram law (the sum of the squared norms of the two diagonals of a parallelogram equals that of the four sides) allows us to find a Cauchy sequence among a sequence of vectors $(y_n)\subset x+F$ such that  $$\lim_{n\to\infty}\left\Vert y_n\right\Vert_E=\left\Vert x+F\right\Vert_{E/F}.$$ But anyway, the suspicion was left that the infimum is there for a reason (other than conveniently allowing us to sweep this detail under the rug at that point of the development of theory), so in the interest of serving our study group I had to come up with a specific example, where the minimum is not achieved. It's been 25 years since I really had to exercise the Banach space gland in my brain, so it has shrunk to size of a raisin. Searching this site did help, because I found this question . There we have $E=C([0,1])$, the space of continuous real functions on $[0,1]$ equipped with the sup-norm. If we denote by $\Lambda$ the continuous functional $$ \Lambda: E\to\mathbb{R},f\mapsto\int_0^{1/2}f-\int_{1/2}^1f $$ and let $F=\ker\Lambda$, then the answer to the linked question proves that there is no minimum sup-norm function in the coset $\Lambda^{-1}(1)$. So I have a (counter)example, and the main question has evolved to: When can we use minimum in place of infimum in the definition of the quotient space norm? My thinking: It seems to me that the answer is affirmative, if $F$ has a complement, i.e. we can write $E=F\oplus F'$ as a direct sum of two closed subspaces such that the norm on $E$ is equivalent to the sum of the norms on $F$ and $F'$-components. But the first point also raises the suspicion that the question may be a bit ill-defined (and uninteresting) in the sense that the answer might depend on the choice of the norm $\left\Vert\cdot\right\Vert_E$ among the set of equivalent norms. However, if we, for example, perturb the sup-norm of $C([0,1])$ in the above example by multiplying the functions with a fixed positive definite function before taking the sup-norm, the argument seems to survive, so may be replacing the norm with an equivalent one is irrelevant? So to satisfy my curiosity I also welcome ""your favorite example"" (one with a finite-dimensional $F$ would be nice to see), where we absolutely need the infimum here. Bits about any sufficient or necessary conditions for the minimum to be sufficient or (as a last resort :-) pointers to relevant literature are, of course, also appreciated.","Let $E$ be a Banach space and $F$ a closed subspace. It is well known that the quotient space $E/F$ is also a Banach space with respect to the norm  $$ \left\Vert x+F\right\Vert_{E/F}=\inf\{\left\Vert y\right\Vert_E\mid y\in x+F\}. $$ Unfortunately in a set of lecture notes on (Lie) group representations (material for our study group) the author accidentally used here $\min$ instead of $\inf$. Probably a mostly harmless booboo, because at that point it was only needed to get a Banach space structure on the quotient, and we will probably be concentrating on Hilbert spaces anyway, where the problem does not arise. Namely from Rudin's Functional Analysis I could not find a proof that the minimum should always be attained. Except in the case of a Hilbert space, where an application of parallelogram law (the sum of the squared norms of the two diagonals of a parallelogram equals that of the four sides) allows us to find a Cauchy sequence among a sequence of vectors $(y_n)\subset x+F$ such that  $$\lim_{n\to\infty}\left\Vert y_n\right\Vert_E=\left\Vert x+F\right\Vert_{E/F}.$$ But anyway, the suspicion was left that the infimum is there for a reason (other than conveniently allowing us to sweep this detail under the rug at that point of the development of theory), so in the interest of serving our study group I had to come up with a specific example, where the minimum is not achieved. It's been 25 years since I really had to exercise the Banach space gland in my brain, so it has shrunk to size of a raisin. Searching this site did help, because I found this question . There we have $E=C([0,1])$, the space of continuous real functions on $[0,1]$ equipped with the sup-norm. If we denote by $\Lambda$ the continuous functional $$ \Lambda: E\to\mathbb{R},f\mapsto\int_0^{1/2}f-\int_{1/2}^1f $$ and let $F=\ker\Lambda$, then the answer to the linked question proves that there is no minimum sup-norm function in the coset $\Lambda^{-1}(1)$. So I have a (counter)example, and the main question has evolved to: When can we use minimum in place of infimum in the definition of the quotient space norm? My thinking: It seems to me that the answer is affirmative, if $F$ has a complement, i.e. we can write $E=F\oplus F'$ as a direct sum of two closed subspaces such that the norm on $E$ is equivalent to the sum of the norms on $F$ and $F'$-components. But the first point also raises the suspicion that the question may be a bit ill-defined (and uninteresting) in the sense that the answer might depend on the choice of the norm $\left\Vert\cdot\right\Vert_E$ among the set of equivalent norms. However, if we, for example, perturb the sup-norm of $C([0,1])$ in the above example by multiplying the functions with a fixed positive definite function before taking the sup-norm, the argument seems to survive, so may be replacing the norm with an equivalent one is irrelevant? So to satisfy my curiosity I also welcome ""your favorite example"" (one with a finite-dimensional $F$ would be nice to see), where we absolutely need the infimum here. Bits about any sufficient or necessary conditions for the minimum to be sufficient or (as a last resort :-) pointers to relevant literature are, of course, also appreciated.",,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'quotient-spaces']"
37,Example of a closed subspace of a Banach space which is not complemented?,Example of a closed subspace of a Banach space which is not complemented?,,"In this post, all vector spaces are assumed to be real or complex. Let $(X, ||\cdot||)$ be a Banach space, $Y \subset X$ a closed subspace. $Y$ is called $\underline{\mathrm{complemented}}$, if there is a closed subspace $Z \subset X$ such that $X =Y \oplus Z$ as topological vector spaces. If $H$ is a Hilbert space every closed subspace $Y$ is complemented; the orthogonal complement $Y^{\bot}$ is a closed subspace of $H$ and we have $H=Y \oplus Y^{\bot}$. A famous theorem of Lindenstrauß and Tzafriri (which can be found in their article ""On the complemented subspaces problem"", Isreal Journal of Mathematics, Vol. 9, No.2, pp. 263-269) asserts that the converse is true as well. More precisely, if $(X, ||\cdot||)$ is a Banach space such that every closed subspace is complemented then $||\cdot||$ is induced by a scalarproduct, i.e. $(X,||\cdot||)$ is a Hilbert space. Now to my question. Can you give me an example of a Banach space $(X,||\cdot||)$, which is not a Hilbert space, and of a closed subspace $Y \subset X$ which is not complemented? It is easily seen that $Y$ must be both infinite-dimensional and infinite-codimensional, for every finite-dimensional and every (closed) finite-codimensional subspace is complemented. I thought about something like $c_{0} \subset (\ell^{\infty}, ||\cdot||_{\infty})$ the closed subspace of null sequences in the Banach space of bounded sequences but couldn't produce a proof that no closed complement exists in that case. Can you help me either proving that $c_{0}$ is not complemented (if that's true at all) or by giving me a different example?","In this post, all vector spaces are assumed to be real or complex. Let $(X, ||\cdot||)$ be a Banach space, $Y \subset X$ a closed subspace. $Y$ is called $\underline{\mathrm{complemented}}$, if there is a closed subspace $Z \subset X$ such that $X =Y \oplus Z$ as topological vector spaces. If $H$ is a Hilbert space every closed subspace $Y$ is complemented; the orthogonal complement $Y^{\bot}$ is a closed subspace of $H$ and we have $H=Y \oplus Y^{\bot}$. A famous theorem of Lindenstrauß and Tzafriri (which can be found in their article ""On the complemented subspaces problem"", Isreal Journal of Mathematics, Vol. 9, No.2, pp. 263-269) asserts that the converse is true as well. More precisely, if $(X, ||\cdot||)$ is a Banach space such that every closed subspace is complemented then $||\cdot||$ is induced by a scalarproduct, i.e. $(X,||\cdot||)$ is a Hilbert space. Now to my question. Can you give me an example of a Banach space $(X,||\cdot||)$, which is not a Hilbert space, and of a closed subspace $Y \subset X$ which is not complemented? It is easily seen that $Y$ must be both infinite-dimensional and infinite-codimensional, for every finite-dimensional and every (closed) finite-codimensional subspace is complemented. I thought about something like $c_{0} \subset (\ell^{\infty}, ||\cdot||_{\infty})$ the closed subspace of null sequences in the Banach space of bounded sequences but couldn't produce a proof that no closed complement exists in that case. Can you help me either proving that $c_{0}$ is not complemented (if that's true at all) or by giving me a different example?",,"['functional-analysis', 'banach-spaces']"
38,Why are Sobolev spaces useful?,Why are Sobolev spaces useful?,,"Why are Sobolev spaces useful, and what problems were they developed to overcome? I'm particularly interested in their relation to PDEs, as they are often described as the 'natural space in which to look for PDE solutions' - why is this? How do weak solutions and distributions come into this? There are plenty of books on the subject, but these seem to jump straight into the details and I'm struggling to see the big picture. I know that the Sobolev norm makes the function spaces complete, which would guarantee that infinite linear combinations of solutions do not leave the space, as can be a problem when working with $\mathscr{C}^2$ , for example, but are there any other reasons why this norm is important? I'm also interested in the Sobolev embedding theorems, since I believe that they're important in the problems I'm trying to solve. These are (1) proving the compactness of the integral operator whose kernel is the Green's function for the Laplacian on a bounded domain $\Omega \subset \mathbb{R}^{n}$ with smooth boundary, and (2) understanding why minimising functions of the Rayleigh quotient, $${\arg\min}_{f \in T} \int_{\Omega} \frac{\nabla f \cdot \nabla f}{\left< f , f \right>}$$ always exist, and that they are necessarily smooth ( $\mathscr{C}^\infty(\Omega)$ ) among the set of trial functions $T$ of continuous functions with piecewise continuous first derivatives which vanish at the boundary and are not identically zero. To me, this sounds like the Sobolev space $H_0^1 (\Omega)$ at work, where the smoothness is the result of a Sobolev embedding theorem; however, I'm very new to Sobolev spaces and so don't know much about this. Could anyone provide me with some insight into how results (1) and (2) might be proven?","Why are Sobolev spaces useful, and what problems were they developed to overcome? I'm particularly interested in their relation to PDEs, as they are often described as the 'natural space in which to look for PDE solutions' - why is this? How do weak solutions and distributions come into this? There are plenty of books on the subject, but these seem to jump straight into the details and I'm struggling to see the big picture. I know that the Sobolev norm makes the function spaces complete, which would guarantee that infinite linear combinations of solutions do not leave the space, as can be a problem when working with , for example, but are there any other reasons why this norm is important? I'm also interested in the Sobolev embedding theorems, since I believe that they're important in the problems I'm trying to solve. These are (1) proving the compactness of the integral operator whose kernel is the Green's function for the Laplacian on a bounded domain with smooth boundary, and (2) understanding why minimising functions of the Rayleigh quotient, always exist, and that they are necessarily smooth ( ) among the set of trial functions of continuous functions with piecewise continuous first derivatives which vanish at the boundary and are not identically zero. To me, this sounds like the Sobolev space at work, where the smoothness is the result of a Sobolev embedding theorem; however, I'm very new to Sobolev spaces and so don't know much about this. Could anyone provide me with some insight into how results (1) and (2) might be proven?","\mathscr{C}^2 \Omega \subset \mathbb{R}^{n} {\arg\min}_{f \in T} \int_{\Omega} \frac{\nabla f \cdot \nabla f}{\left< f , f \right>} \mathscr{C}^\infty(\Omega) T H_0^1 (\Omega)","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
39,"Distinguishing between symmetric, Hermitian and self-adjoint operators","Distinguishing between symmetric, Hermitian and self-adjoint operators",,"I am permanently confused about the distinction between Hermitian and self-adjoint operators in an infinite-dimensional space. The preceding statement may even be ill-defined. My confusion is due to consulting Wikipedia , upon which action I have the following notion. Let $H$ be a pre-Hilbert space equipped with an inner product ${\langle}.,.{\rangle}$ and $T:D(T){\subset}H{\longmapsto}H$ a linear operator. Then If ${\langle}Tx,y{\rangle}$=${\langle}x,Ty{\rangle}$ for all $x,y{\in}D(T)$ then $T$ is symmetric . If $T$ is symmetric and also bounded then it is Hermitian . If $T$ is symmetric and $D(T)=H$ then $T$ is self-adjoint . As a corollary, if the above is true then a symmetric and self-adjoint operator must be Hermitian since a symmetric operator defined on all of $H$ must be bounded. On the other hand, a Hermitian operator need not be self-adjoint: it would not be if its domain were a strict subset of $H$. Would people agree with this?  I always see the second and third of these treated as equivalent, hence my confusion.","I am permanently confused about the distinction between Hermitian and self-adjoint operators in an infinite-dimensional space. The preceding statement may even be ill-defined. My confusion is due to consulting Wikipedia , upon which action I have the following notion. Let $H$ be a pre-Hilbert space equipped with an inner product ${\langle}.,.{\rangle}$ and $T:D(T){\subset}H{\longmapsto}H$ a linear operator. Then If ${\langle}Tx,y{\rangle}$=${\langle}x,Ty{\rangle}$ for all $x,y{\in}D(T)$ then $T$ is symmetric . If $T$ is symmetric and also bounded then it is Hermitian . If $T$ is symmetric and $D(T)=H$ then $T$ is self-adjoint . As a corollary, if the above is true then a symmetric and self-adjoint operator must be Hermitian since a symmetric operator defined on all of $H$ must be bounded. On the other hand, a Hermitian operator need not be self-adjoint: it would not be if its domain were a strict subset of $H$. Would people agree with this?  I always see the second and third of these treated as equivalent, hence my confusion.",,['functional-analysis']
40,"What is ""white noise"" and how is it related to the Brownian motion?","What is ""white noise"" and how is it related to the Brownian motion?",,"In the Chapter 1.2 of Stochastic Partial Differential Equations: An Introduction by Wei Liu and Michael Röckner, the authors introduce stochastic partial differential equations by considering equations of the form $$\frac{{\rm d}X_t}{{\rm d}t}=F\left(t,X_t,\dot B_t\right)$$ where $\left(\dot B_t\right)_{t\ge 0}$ is a ""white noise in time"" (whatever that means) with values in a separable Hilbert space $U$. $\left(\dot B_t\right)_{t\ge 0}$ is said to be the ""generalized time-derivative of a $U$-valued Brownian motion $(B_t)_{t\ge 0}$. Question: What exactly do the authors mean? What is a ""white noise in time"" and why (and in which sense) is it the ""generalized time-derivative"" of a Brownian motion? You can skip the following, if you know the answer to these questions. I will present what I've found out so far: I've searched the terms ""white noise"" and ""distributional derivative of Brownian motion"" on the internet and found few and inconsistent definitions. Definition 1 : In the book An Introduction to Computational Stochastic PDEs the authors do the following: Let $(\phi_n)_{n\in\mathbb N}$ be an orthonormal basis of $L^2([0,1])$, e.g. $\phi_n(t):=\sqrt 2\sin(n\pi t)$. Then $$W_t:=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;\;\;\text{for }t\in [0,1]\;,$$ where the $\xi_i$ are independent and standard normally distributed random variables on a probability space $(\Omega,\mathcal A,\operatorname P)$, is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $\operatorname E[W_t]=0$ and $$\operatorname E[W_sW_t]=\delta(s-t)\;\;\;\text{for all }s,t\in [0,1]$$ where $\delta$ denotes the Dirac delta function . They call $(W_t)_{t\in [0,1]}$ white noise . This definition seems to depend on the explicit choice of the orthnormal basis $(\phi_n)_{n\in\mathbb N}$ and I don't see the connection to a ""derivative"" of a Brownian motion (needless to say that I don't see how this would generalize to a cylindrical Brownian motion). However, maybe it has something to do with the following: Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$. Then the Karhunen–Loève theorem yields $$B_t=\lim_{n\to\infty}\sum_{i=1}^n\sqrt{\zeta_i}\phi_i(t)\xi_i\;\;\;\text{for all }t\in [0,T]$$ in $L^2(\operatorname P)$ and uniformly in $t$, where $(\phi_n)_{n\in\mathbb N}$ is an orthonormal basis of $L^2([0,1])$ and $(\xi_n)_{n\in\mathbb N}$ is a sequence of indepedent standard normally distributed random variables on $(\Omega,\mathcal A,\operatorname P)$. In particular, $$\zeta_i=\frac 4{(2i-1)^2\pi^2}$$ and $$\phi_i(t)=\sqrt 2\sin\frac t{\sqrt{\zeta_i}}\;.$$ The authors state, that we can formally consider the derivative of $B$ as being the process $$\dot B_t=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;.$$ I have no idea why. Nevertheless, we may notice the following: Let $${\rm D}^{(\Delta t)}_t:=\frac{B_{t+\Delta t}-B_t}{\Delta t}\;\;\;\text{for }t\ge 0$$ for some $\Delta t>0$. Then $\left({\rm D}^{(\Delta t)}_t\right)$ is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $$\operatorname E\left[{\rm D}^{(\Delta t)}_t\right]=0\;\;\;\text{for all }t\ge 0$$ and $$\operatorname{Cov}\left[{\rm D}^{(\Delta t)}_s,{\rm D}^{(\Delta t)}_t\right]=\left.\begin{cases}\displaystyle\frac{\Delta t-|s-t|}{\Delta t^2}&\text{, if }|s-t|\le \Delta t\\0&\text{, if }|s-t|\ge \Delta t\end{cases}\right\}=:\eta^{(\Delta t)}(s-t)\;\;\;\text{for all }s,t\ge 0\;.$$ Since $$\int\eta^{(\Delta t)}(x)\;{\rm d}x=\int_{-\Delta t}^{\Delta t}\eta^{(\Delta t)}(x)\;{\rm d}x=1$$ we obtain $$\eta^{(\Delta t)}(x)\stackrel{\Delta t\to 0}\to\delta(x)\;,$$ but I have no idea how this is related to white noise. Definition 2 : In Stochastic Differential Equations with Applications to Physics and Engineering , Modeling, Simulation, and Optimization of Integrated Circuits and Generalized Functions - Vol 4: Applications of Harmonic Analysis they take a real-valued Brownian motion $(B_t)_{t\ge 0}$ on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\phi\rangle:=\int\phi(t)B_t\;{\rm d}\lambda\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty([0,\infty))\;.$$ Let $\mathcal D'$ be the dual space of $\mathcal D$. We can show that $W$ is a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$, i.e. $$\left(\langle W,\phi_1\rangle,\ldots,\langle W,\phi_n\rangle\right)\text{ is }n\text{-dimensionally normally distributed}$$ for all linearly independent $\phi_1,\ldots,\phi_n\in\mathcal D$, with expectation $$\operatorname E[W](\phi):=\operatorname E\left[\langle W,\phi\rangle\right]=0\;\;\;\text{for all }\phi\in\mathcal D$$ and covariance $$\rho[W](\phi,\psi):=\operatorname E\left[\langle W,\phi\rangle\langle W,\psi\rangle\right]=\int\int\min(s,t)\phi(s)\psi(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\;\;\;\text{for all }\phi,\psi\in\mathcal D\;.$$ Moreover, the derivative $$\langle W',\phi\rangle:=-\langle W,\phi\rangle\;\;\;\text{for }\phi\in\mathcal D\tag 1$$ is again a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$ with expectation $$\operatorname E[W'](\phi)=0\;\;\;\text{for all }\phi\in\mathcal D\tag 2$$ and covariance \begin{equation} \begin{split} \varrho[W'](\phi,\psi)&=\int\int\min(s,t)\phi'(s)\psi'(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\\ &=\int\int\delta(t-s)\phi(s)\psi(t)\;{\rm d}\lambda(t)\;{\rm d}\lambda(s) \end{split} \end{equation} for all $\phi,\psi\in\mathcal D$. Now they call a generalized Gaussian stochastic process with expectation and covariance given by $(1)$ and $(2)$ a Gaussian white noise . Thus, the generalized derivative $W'$ of the generalized Brownian motion $W$ is a Gaussian white noise. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian motion. Moreover, this definition seems to be less naturally to me and I don't think that this is the notion Liu and Röckner had in mind. Definition 3 : In some lecture notes, I've seen the following the definition: Let $W$ be a centered Gaussian process, indexed by test functions $\phi\in C^\infty([0,\infty]\times\mathbb R^d)$ whose covariance is given by $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\int_{\mathbb R^d}{\rm d}y\phi(t,x)\psi(t,x)\delta(x-y)\tag 3$$ or $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\phi(t,x)\psi(t,x)\tag 4\;.$$ Then $W$ is called ""white noise in time and colored noise in space"" in the case $(3)$ and ""white noise, both in time and space"" in the case $(4)$. They simply state that $\delta$ is some ""reasonable"" kernel which might blow up to inifinity at $0$. I suppose this is related to Definition 2. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian moton. Definition 4 : This definition is very sloppy in its notation: Let Let $(W_t)_t$ be a centered Gaussian process with covariance $\operatorname E[W_sW_t]=\delta(s-t)$ where $\delta$ denotes the Dirac delta function. Then, in a [lecture note] I've found (Example 3.56), they state that $$B_t:=\int_0^tW_s\;{\rm d}B_s\tag 5\;\;\;\text{for }t\ge 0$$ is a real-valued Brownian motion. I haven't verified that result. Is it correct? Whatever the case is, if this is the reason, why white noise is considered to be the derivative of a Brownian motion, we should be able that every Brownian motion as a representation of the form $(5)$. Can this be shown? The same questions as above remain. Definition 5 : Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\varphi\rangle:=\int_0^\infty\varphi(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty((0,\infty))\;.$$ Then $$\langle W',\varphi\rangle:=\int_0^\infty\varphi'(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D$$ is considered to be the generalized derivative of the generalized Brownian motion $W$. The same questions as above remain. Conclusion : I've found different notions of ""white noise"" and ""generalized derivative"" of a Brownian motion, but I don't know in which sense they are consistent and which of them Liu and Röckner meant. So, I would be very happy if someone could give a rigorous definition of these terms in the case of a cylindrical Brownian motion or at least in the case of a Hilbert space valued Brownian motion.","In the Chapter 1.2 of Stochastic Partial Differential Equations: An Introduction by Wei Liu and Michael Röckner, the authors introduce stochastic partial differential equations by considering equations of the form $$\frac{{\rm d}X_t}{{\rm d}t}=F\left(t,X_t,\dot B_t\right)$$ where $\left(\dot B_t\right)_{t\ge 0}$ is a ""white noise in time"" (whatever that means) with values in a separable Hilbert space $U$. $\left(\dot B_t\right)_{t\ge 0}$ is said to be the ""generalized time-derivative of a $U$-valued Brownian motion $(B_t)_{t\ge 0}$. Question: What exactly do the authors mean? What is a ""white noise in time"" and why (and in which sense) is it the ""generalized time-derivative"" of a Brownian motion? You can skip the following, if you know the answer to these questions. I will present what I've found out so far: I've searched the terms ""white noise"" and ""distributional derivative of Brownian motion"" on the internet and found few and inconsistent definitions. Definition 1 : In the book An Introduction to Computational Stochastic PDEs the authors do the following: Let $(\phi_n)_{n\in\mathbb N}$ be an orthonormal basis of $L^2([0,1])$, e.g. $\phi_n(t):=\sqrt 2\sin(n\pi t)$. Then $$W_t:=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;\;\;\text{for }t\in [0,1]\;,$$ where the $\xi_i$ are independent and standard normally distributed random variables on a probability space $(\Omega,\mathcal A,\operatorname P)$, is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $\operatorname E[W_t]=0$ and $$\operatorname E[W_sW_t]=\delta(s-t)\;\;\;\text{for all }s,t\in [0,1]$$ where $\delta$ denotes the Dirac delta function . They call $(W_t)_{t\in [0,1]}$ white noise . This definition seems to depend on the explicit choice of the orthnormal basis $(\phi_n)_{n\in\mathbb N}$ and I don't see the connection to a ""derivative"" of a Brownian motion (needless to say that I don't see how this would generalize to a cylindrical Brownian motion). However, maybe it has something to do with the following: Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$. Then the Karhunen–Loève theorem yields $$B_t=\lim_{n\to\infty}\sum_{i=1}^n\sqrt{\zeta_i}\phi_i(t)\xi_i\;\;\;\text{for all }t\in [0,T]$$ in $L^2(\operatorname P)$ and uniformly in $t$, where $(\phi_n)_{n\in\mathbb N}$ is an orthonormal basis of $L^2([0,1])$ and $(\xi_n)_{n\in\mathbb N}$ is a sequence of indepedent standard normally distributed random variables on $(\Omega,\mathcal A,\operatorname P)$. In particular, $$\zeta_i=\frac 4{(2i-1)^2\pi^2}$$ and $$\phi_i(t)=\sqrt 2\sin\frac t{\sqrt{\zeta_i}}\;.$$ The authors state, that we can formally consider the derivative of $B$ as being the process $$\dot B_t=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;.$$ I have no idea why. Nevertheless, we may notice the following: Let $${\rm D}^{(\Delta t)}_t:=\frac{B_{t+\Delta t}-B_t}{\Delta t}\;\;\;\text{for }t\ge 0$$ for some $\Delta t>0$. Then $\left({\rm D}^{(\Delta t)}_t\right)$ is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $$\operatorname E\left[{\rm D}^{(\Delta t)}_t\right]=0\;\;\;\text{for all }t\ge 0$$ and $$\operatorname{Cov}\left[{\rm D}^{(\Delta t)}_s,{\rm D}^{(\Delta t)}_t\right]=\left.\begin{cases}\displaystyle\frac{\Delta t-|s-t|}{\Delta t^2}&\text{, if }|s-t|\le \Delta t\\0&\text{, if }|s-t|\ge \Delta t\end{cases}\right\}=:\eta^{(\Delta t)}(s-t)\;\;\;\text{for all }s,t\ge 0\;.$$ Since $$\int\eta^{(\Delta t)}(x)\;{\rm d}x=\int_{-\Delta t}^{\Delta t}\eta^{(\Delta t)}(x)\;{\rm d}x=1$$ we obtain $$\eta^{(\Delta t)}(x)\stackrel{\Delta t\to 0}\to\delta(x)\;,$$ but I have no idea how this is related to white noise. Definition 2 : In Stochastic Differential Equations with Applications to Physics and Engineering , Modeling, Simulation, and Optimization of Integrated Circuits and Generalized Functions - Vol 4: Applications of Harmonic Analysis they take a real-valued Brownian motion $(B_t)_{t\ge 0}$ on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\phi\rangle:=\int\phi(t)B_t\;{\rm d}\lambda\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty([0,\infty))\;.$$ Let $\mathcal D'$ be the dual space of $\mathcal D$. We can show that $W$ is a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$, i.e. $$\left(\langle W,\phi_1\rangle,\ldots,\langle W,\phi_n\rangle\right)\text{ is }n\text{-dimensionally normally distributed}$$ for all linearly independent $\phi_1,\ldots,\phi_n\in\mathcal D$, with expectation $$\operatorname E[W](\phi):=\operatorname E\left[\langle W,\phi\rangle\right]=0\;\;\;\text{for all }\phi\in\mathcal D$$ and covariance $$\rho[W](\phi,\psi):=\operatorname E\left[\langle W,\phi\rangle\langle W,\psi\rangle\right]=\int\int\min(s,t)\phi(s)\psi(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\;\;\;\text{for all }\phi,\psi\in\mathcal D\;.$$ Moreover, the derivative $$\langle W',\phi\rangle:=-\langle W,\phi\rangle\;\;\;\text{for }\phi\in\mathcal D\tag 1$$ is again a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$ with expectation $$\operatorname E[W'](\phi)=0\;\;\;\text{for all }\phi\in\mathcal D\tag 2$$ and covariance \begin{equation} \begin{split} \varrho[W'](\phi,\psi)&=\int\int\min(s,t)\phi'(s)\psi'(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\\ &=\int\int\delta(t-s)\phi(s)\psi(t)\;{\rm d}\lambda(t)\;{\rm d}\lambda(s) \end{split} \end{equation} for all $\phi,\psi\in\mathcal D$. Now they call a generalized Gaussian stochastic process with expectation and covariance given by $(1)$ and $(2)$ a Gaussian white noise . Thus, the generalized derivative $W'$ of the generalized Brownian motion $W$ is a Gaussian white noise. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian motion. Moreover, this definition seems to be less naturally to me and I don't think that this is the notion Liu and Röckner had in mind. Definition 3 : In some lecture notes, I've seen the following the definition: Let $W$ be a centered Gaussian process, indexed by test functions $\phi\in C^\infty([0,\infty]\times\mathbb R^d)$ whose covariance is given by $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\int_{\mathbb R^d}{\rm d}y\phi(t,x)\psi(t,x)\delta(x-y)\tag 3$$ or $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\phi(t,x)\psi(t,x)\tag 4\;.$$ Then $W$ is called ""white noise in time and colored noise in space"" in the case $(3)$ and ""white noise, both in time and space"" in the case $(4)$. They simply state that $\delta$ is some ""reasonable"" kernel which might blow up to inifinity at $0$. I suppose this is related to Definition 2. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian moton. Definition 4 : This definition is very sloppy in its notation: Let Let $(W_t)_t$ be a centered Gaussian process with covariance $\operatorname E[W_sW_t]=\delta(s-t)$ where $\delta$ denotes the Dirac delta function. Then, in a [lecture note] I've found (Example 3.56), they state that $$B_t:=\int_0^tW_s\;{\rm d}B_s\tag 5\;\;\;\text{for }t\ge 0$$ is a real-valued Brownian motion. I haven't verified that result. Is it correct? Whatever the case is, if this is the reason, why white noise is considered to be the derivative of a Brownian motion, we should be able that every Brownian motion as a representation of the form $(5)$. Can this be shown? The same questions as above remain. Definition 5 : Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\varphi\rangle:=\int_0^\infty\varphi(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty((0,\infty))\;.$$ Then $$\langle W',\varphi\rangle:=\int_0^\infty\varphi'(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D$$ is considered to be the generalized derivative of the generalized Brownian motion $W$. The same questions as above remain. Conclusion : I've found different notions of ""white noise"" and ""generalized derivative"" of a Brownian motion, but I don't know in which sense they are consistent and which of them Liu and Röckner meant. So, I would be very happy if someone could give a rigorous definition of these terms in the case of a cylindrical Brownian motion or at least in the case of a Hilbert space valued Brownian motion.",,"['functional-analysis', 'probability-theory', 'stochastic-processes', 'brownian-motion', 'stochastic-analysis']"
41,What is the spectral theorem for compact self-adjoint operators on a Hilbert space actually for?,What is the spectral theorem for compact self-adjoint operators on a Hilbert space actually for?,,"Please excuse the naive question.  I have had two classes now in which this theorem was taught and proven, but I have only ever seen a single (indirect?) application involving the quantum harmonic oscillator.  Even if this is not the strongest spectral theorem, it still seems useful enough that there should be many nice examples illustrating its utility.  So... what are some of those examples? (I couldn't readily find any nice examples looking through a few functional analysis textbooks, either.  Maybe I have the wrong books.)","Please excuse the naive question.  I have had two classes now in which this theorem was taught and proven, but I have only ever seen a single (indirect?) application involving the quantum harmonic oscillator.  Even if this is not the strongest spectral theorem, it still seems useful enough that there should be many nice examples illustrating its utility.  So... what are some of those examples? (I couldn't readily find any nice examples looking through a few functional analysis textbooks, either.  Maybe I have the wrong books.)",,"['big-list', 'functional-analysis', 'examples-counterexamples']"
42,Finding the adjoint of an operator,Finding the adjoint of an operator,,"This is from my homework, I'm totally lost as to how to proceed. Consider the operator $T: L^2([0,1]) \rightarrow L^2([0,1])$ defined by  $(Tf)(x) = \int^x_0 f(s) \ ds$ What is the adjoint of $T$? This operator doesn't seem to be an orthogonal projection, nor is it self-adjoint. How does one find the adjoint of an operator in general? Thanks in advance!","This is from my homework, I'm totally lost as to how to proceed. Consider the operator $T: L^2([0,1]) \rightarrow L^2([0,1])$ defined by  $(Tf)(x) = \int^x_0 f(s) \ ds$ What is the adjoint of $T$? This operator doesn't seem to be an orthogonal projection, nor is it self-adjoint. How does one find the adjoint of an operator in general? Thanks in advance!",,"['functional-analysis', 'hilbert-spaces']"
43,Is Banach-Alaoglu equivalent to AC?,Is Banach-Alaoglu equivalent to AC?,,The Banach-Alaoglu theorem is well-known. It states that the closed unit ball in the dual space of a normed space is $\text{wk}^*$-compact. The proof relies heavily on  Tychonoff's theorem. As I have recently figured out thanks to the nice guys on the chat belonging to this website is that Tychonoff's theorem is equivalent to the Axiom of Choice. Can we prove Tychonoff (or something else equivalent) from Alaoglu?,The Banach-Alaoglu theorem is well-known. It states that the closed unit ball in the dual space of a normed space is $\text{wk}^*$-compact. The proof relies heavily on  Tychonoff's theorem. As I have recently figured out thanks to the nice guys on the chat belonging to this website is that Tychonoff's theorem is equivalent to the Axiom of Choice. Can we prove Tychonoff (or something else equivalent) from Alaoglu?,,"['functional-analysis', 'set-theory', 'axiom-of-choice']"
44,Compact operator maps weakly convergent sequences into strongly convergent sequences,Compact operator maps weakly convergent sequences into strongly convergent sequences,,"I found the following property of compact operators in a proof, and I can't prove it. Prove that if $T \in \mathcal{L}(E,F)$ is compact, and if $u_n \rightharpoonup u$ (the sequence converges weakly to $u$ in $\sigma(E,E^*)$) then $Tu_n \to Tu$ strongly in $F$. I was able to prove that $Tu_n$ has a convergent subsequence ($u_n \rightharpoonup u$ implies that $(u_n)$ is bounded in $E$. Then, because $T$ is compact it must follow that $(Tu_n)$ must contain some strong convergent subsequence), but didn't manage to finalize the proof. Any reference or hints are welcome.","I found the following property of compact operators in a proof, and I can't prove it. Prove that if $T \in \mathcal{L}(E,F)$ is compact, and if $u_n \rightharpoonup u$ (the sequence converges weakly to $u$ in $\sigma(E,E^*)$) then $Tu_n \to Tu$ strongly in $F$. I was able to prove that $Tu_n$ has a convergent subsequence ($u_n \rightharpoonup u$ implies that $(u_n)$ is bounded in $E$. Then, because $T$ is compact it must follow that $(Tu_n)$ must contain some strong convergent subsequence), but didn't manage to finalize the proof. Any reference or hints are welcome.",,"['functional-analysis', 'banach-spaces']"
45,$T$ is continuous if and only if $\ker T$ is closed,is continuous if and only if  is closed,T \ker T,"Let $X,Y$ be normed linear spaces. Let $T: X\to Y$ be linear. If $X$ is finite dimensional, show that $T$ is continuous. If $Y$ is finite dimensional, show that $T$ is continuous if and only if $\ker T$ is closed. I am able to show that $X$, finite dimensional $\implies$ $T$ is bounded, hence continuous. For the second part: This is what I have: Suppose $T$ is continuous. By definition $\ker T = \{ x\in X : Tx = 0 \}$ , and so $\ker T$ is the continuous inverse of a closed set. Hence $\ker T $ is closed. First, is what I have attempted okay. How about the other direction?","Let $X,Y$ be normed linear spaces. Let $T: X\to Y$ be linear. If $X$ is finite dimensional, show that $T$ is continuous. If $Y$ is finite dimensional, show that $T$ is continuous if and only if $\ker T$ is closed. I am able to show that $X$, finite dimensional $\implies$ $T$ is bounded, hence continuous. For the second part: This is what I have: Suppose $T$ is continuous. By definition $\ker T = \{ x\in X : Tx = 0 \}$ , and so $\ker T$ is the continuous inverse of a closed set. Hence $\ker T $ is closed. First, is what I have attempted okay. How about the other direction?",,"['functional-analysis', 'normed-spaces']"
46,What is functional analysis in simple words?,What is functional analysis in simple words?,,"To begin with , I am only a secondary school student (17yo) but I am very interested in higher mathematics. However we only learn so little in my school (only single variable calculus and basic linear algebra). In the past I have self-learnt some abstract algebra and very basic topology by finding online resources, but I can never get deep into those subjects. When I read about functional analysis, I encounter objects like function spaces and infinite-dimensional spaces which I can never understand. What does it exactly mean to be a function space, how do you measure metric? I know it is hard and requires much real analysis. Can anyone give me some easy ideas and introductions?","To begin with , I am only a secondary school student (17yo) but I am very interested in higher mathematics. However we only learn so little in my school (only single variable calculus and basic linear algebra). In the past I have self-learnt some abstract algebra and very basic topology by finding online resources, but I can never get deep into those subjects. When I read about functional analysis, I encounter objects like function spaces and infinite-dimensional spaces which I can never understand. What does it exactly mean to be a function space, how do you measure metric? I know it is hard and requires much real analysis. Can anyone give me some easy ideas and introductions?",,"['functional-analysis', 'intuition']"
47,Why are functions with vanishing normal derivative dense in smooth functions?,Why are functions with vanishing normal derivative dense in smooth functions?,,"Question Let $M$ be a compact Riemannian manifold with piecewise smooth boundary. Why are smooth functions with vanishing normal derivative dense in $C^\infty(M)$ in the $H^1$ norm? Here I define $C^\infty(M)$ to be those functions which have all orders of derivative continuous on $M$ and smooth in its interior. For example, $(x\mapsto \sin(\pi x))\in C^\infty([0,1])$ but $(x\mapsto \sqrt{x})\notin C^\infty([0,1])$. I have crossposted this to MathOverflow . Background This is inspired by my belief that the form domain of the Friedrichs extension of the Neumann Laplacian on $M$ is equal to $H^1(M)$. If my belief is wrong, I would certainly accept as answer a counterexample, preferably with some discussion/references. Here are the approaches I'm exploring. Given $u\in C^\infty(M)$ construct a function that agrees away from an $\epsilon$ neighborhood of $\partial M$, but has been modified to have zero normal derivative. This is described below, but runs into some trouble at corners and with smoothing. Given $u\in C^\infty(M)$, find some $\eta$ supported on an $\epsilon$ neighborhood of $\partial M$ such that $\nabla\eta|_{\partial M}$ is equal to the projection of $\nabla u|_{\partial M}$ onto the normal direction and $\eta$ and $|\nabla\eta|$ uniformly bounded in $\epsilon$. Then $u - \eta$ will be the desired approximation and uniform boundedness will imply $\|u - (u-\eta)\|_{H^1}\to 0$. I'm not sure if this is a search for an integrable harmonic vector field or if it's a constrained optimization problem. Simply show that any $u\in C^\infty(M)$ that is perpendicular to all smooth functions with vanishing normal derivative must be zero. In order to do this, I think it still runs into the same fundamental difficulty as the others, which is constructing functions with vanishing normal derivative supported on an $\epsilon$-neighborhood of the boundary. (I'm also happy to simply have a reference to follow up. A reference for domains in $\mathbb{R}^n$ should be fine, too, and just a partition of unity away from a Riemannian manifold.) Current Approach The approach I'm considering right now is an elaboration on possibility (2) from my list. The sketch is: set the boundary condition that $\nabla\eta$ on the boundary be equal to the normal component of $\nabla u$ and find an integrable harmonic vector field $E$ satisfying that boundary condition. Then cut off $E$ so it is only supported in a neighborhood of $\partial M$, and let $\eta$ be so that $E = \nabla\eta$. Intuition is that the maximum principle will control $|E|$ and $|\eta|$, so that $\eta$ will be bounded above in the $H^1$ norm by a constant times the volume of the $\epsilon$-neighborhood of $\partial M$. Another approach is inspired by zhw's comment below: approximate $\nabla u$ among $L^2$ vector fields, multiply it by a cutoff function so it is supported in the interior of $M$, and then integrate it to approximate $u$. This should work with some tweaking in $[0,1]$ but I'm not sure how well it will work in general. Older work My approach has been as follows. The intuition is to take an arbitrary smooth function, restrict it to the complement of a collar neighborhood, then extend the restriction to the collar neighborhood so that the value is constant on inward-normal geodesics. However I'm running into issues at corners. Let $\epsilon > 0$ be such that $\{p\in M\ |\ d(p,\partial M) < \epsilon\}$ is a collar neighborhood of $\partial M$. Let $e_\epsilon(p)$ for $p\in\partial M$ be the smaller of $\epsilon$ or the greatest time parameter such that the inward normal geodesic collides with no other inward normal geodesic. Let $N$ be the set of inward normal vectors on $\partial M$ whose lengths are no greater than $e_\epsilon$. The interior of the set $\operatorname{exp}(N)$ is foliated by geodesics. Edit- Note this is not necessarily true if the manifold has inward corners. Given a smooth, continuous function $u$ on $M$, define $\bar{u}$ to be the restriction of $u$ in the complement of $\operatorname{exp}(N)$ and on each geodesic leaf of the interior of $\operatorname{exp}(N)$ define $\bar{u}$ to take the value that $u$ takes on the inward limit of that leaf. Here's trouble. As $\bar{u}$ need not be smooth, I'd like to mollify it. Edit- I had a detail incorrect. A standard mollifier produces a function defined on compact subsets of the interior of $M$. So the mollifier has to be modified. One idea I'm following up on is varying the support of the mollifying function based on distance to $\partial M$. I'm skeptical, as varying the mollifying function will add another component to the gradient, but if it works I'll post as an answer.","Question Let $M$ be a compact Riemannian manifold with piecewise smooth boundary. Why are smooth functions with vanishing normal derivative dense in $C^\infty(M)$ in the $H^1$ norm? Here I define $C^\infty(M)$ to be those functions which have all orders of derivative continuous on $M$ and smooth in its interior. For example, $(x\mapsto \sin(\pi x))\in C^\infty([0,1])$ but $(x\mapsto \sqrt{x})\notin C^\infty([0,1])$. I have crossposted this to MathOverflow . Background This is inspired by my belief that the form domain of the Friedrichs extension of the Neumann Laplacian on $M$ is equal to $H^1(M)$. If my belief is wrong, I would certainly accept as answer a counterexample, preferably with some discussion/references. Here are the approaches I'm exploring. Given $u\in C^\infty(M)$ construct a function that agrees away from an $\epsilon$ neighborhood of $\partial M$, but has been modified to have zero normal derivative. This is described below, but runs into some trouble at corners and with smoothing. Given $u\in C^\infty(M)$, find some $\eta$ supported on an $\epsilon$ neighborhood of $\partial M$ such that $\nabla\eta|_{\partial M}$ is equal to the projection of $\nabla u|_{\partial M}$ onto the normal direction and $\eta$ and $|\nabla\eta|$ uniformly bounded in $\epsilon$. Then $u - \eta$ will be the desired approximation and uniform boundedness will imply $\|u - (u-\eta)\|_{H^1}\to 0$. I'm not sure if this is a search for an integrable harmonic vector field or if it's a constrained optimization problem. Simply show that any $u\in C^\infty(M)$ that is perpendicular to all smooth functions with vanishing normal derivative must be zero. In order to do this, I think it still runs into the same fundamental difficulty as the others, which is constructing functions with vanishing normal derivative supported on an $\epsilon$-neighborhood of the boundary. (I'm also happy to simply have a reference to follow up. A reference for domains in $\mathbb{R}^n$ should be fine, too, and just a partition of unity away from a Riemannian manifold.) Current Approach The approach I'm considering right now is an elaboration on possibility (2) from my list. The sketch is: set the boundary condition that $\nabla\eta$ on the boundary be equal to the normal component of $\nabla u$ and find an integrable harmonic vector field $E$ satisfying that boundary condition. Then cut off $E$ so it is only supported in a neighborhood of $\partial M$, and let $\eta$ be so that $E = \nabla\eta$. Intuition is that the maximum principle will control $|E|$ and $|\eta|$, so that $\eta$ will be bounded above in the $H^1$ norm by a constant times the volume of the $\epsilon$-neighborhood of $\partial M$. Another approach is inspired by zhw's comment below: approximate $\nabla u$ among $L^2$ vector fields, multiply it by a cutoff function so it is supported in the interior of $M$, and then integrate it to approximate $u$. This should work with some tweaking in $[0,1]$ but I'm not sure how well it will work in general. Older work My approach has been as follows. The intuition is to take an arbitrary smooth function, restrict it to the complement of a collar neighborhood, then extend the restriction to the collar neighborhood so that the value is constant on inward-normal geodesics. However I'm running into issues at corners. Let $\epsilon > 0$ be such that $\{p\in M\ |\ d(p,\partial M) < \epsilon\}$ is a collar neighborhood of $\partial M$. Let $e_\epsilon(p)$ for $p\in\partial M$ be the smaller of $\epsilon$ or the greatest time parameter such that the inward normal geodesic collides with no other inward normal geodesic. Let $N$ be the set of inward normal vectors on $\partial M$ whose lengths are no greater than $e_\epsilon$. The interior of the set $\operatorname{exp}(N)$ is foliated by geodesics. Edit- Note this is not necessarily true if the manifold has inward corners. Given a smooth, continuous function $u$ on $M$, define $\bar{u}$ to be the restriction of $u$ in the complement of $\operatorname{exp}(N)$ and on each geodesic leaf of the interior of $\operatorname{exp}(N)$ define $\bar{u}$ to take the value that $u$ takes on the inward limit of that leaf. Here's trouble. As $\bar{u}$ need not be smooth, I'd like to mollify it. Edit- I had a detail incorrect. A standard mollifier produces a function defined on compact subsets of the interior of $M$. So the mollifier has to be modified. One idea I'm following up on is varying the support of the mollifying function based on distance to $\partial M$. I'm skeptical, as varying the mollifying function will add another component to the gradient, but if it works I'll post as an answer.",,"['functional-analysis', 'reference-request', 'riemannian-geometry', 'sobolev-spaces', 'manifolds-with-boundary']"
48,$A$ and $B$ commute on a dense set but $e^{iA}$ and $e^{iB}$ do not,and  commute on a dense set but  and  do not,A B e^{iA} e^{iB},"Let $A$ and $B$ be unbounded, symmetric operators on a Hilbert space $H$ with a common domain $D$. If $AB = BA$ on $D$, is it necessarily that case that $e^{iA}$ and $e^{iB}$ also commute? If $A$ and $B$ are bounded, then I know that this must be the case. However, I am not sure if the same must be true for unbounded operators. Does anyone have a proof or a counterexample?","Let $A$ and $B$ be unbounded, symmetric operators on a Hilbert space $H$ with a common domain $D$. If $AB = BA$ on $D$, is it necessarily that case that $e^{iA}$ and $e^{iB}$ also commute? If $A$ and $B$ are bounded, then I know that this must be the case. However, I am not sure if the same must be true for unbounded operators. Does anyone have a proof or a counterexample?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
49,What is spectrum for Laplacian in $\mathbb{R}^n$?,What is spectrum for Laplacian in ?,\mathbb{R}^n,"I know very well that Laplacian in bounded domain has a discrete spectrum. How about Laplacian in $\mathbb{R}^n$?(not in some fancy-shaped unbounded domain, but the whole domain) Where can I find such results? Moreover, is there a counterpart of Hilbert-Schmidt theorem for Laplacian in $\mathbb{R}^n$? Hilbert-Schmidt asserts there is a countable set of eigenfunctions $\phi_n$ so that $x=\sum \langle x,\phi_n\rangle \phi_n,\forall x\in H$. Is there a similar theorem saying $x=\int_0^\infty \langle x,\phi_\lambda\rangle \phi_\lambda\,\mathrm{d}\lambda,\forall x\in H$ where $\phi_\lambda$ is the eigenfunction of Laplacian to spectral value $\lambda$?","I know very well that Laplacian in bounded domain has a discrete spectrum. How about Laplacian in $\mathbb{R}^n$?(not in some fancy-shaped unbounded domain, but the whole domain) Where can I find such results? Moreover, is there a counterpart of Hilbert-Schmidt theorem for Laplacian in $\mathbb{R}^n$? Hilbert-Schmidt asserts there is a countable set of eigenfunctions $\phi_n$ so that $x=\sum \langle x,\phi_n\rangle \phi_n,\forall x\in H$. Is there a similar theorem saying $x=\int_0^\infty \langle x,\phi_\lambda\rangle \phi_\lambda\,\mathrm{d}\lambda,\forall x\in H$ where $\phi_\lambda$ is the eigenfunction of Laplacian to spectral value $\lambda$?",,"['functional-analysis', 'reference-request', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'spectral-theory']"
50,"Banach Spaces - How can $B,B',B'', B''', B'''',B''''',\ldots$ behave?",Banach Spaces - How can  behave?,"B,B',B'', B''', B'''',B''''',\ldots","(ZFC) Let $ \big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $ be a Banach space. Define $ \mathbf{B} \; = \;\big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $. Define $\: \mathbf{B}_0 = \mathbf{B} \:$. For all non-negative integers $n$, define $\mathbf{B}_{n+1}$ to be the Banach space that is the continuous dual of $\mathbf{B}_n$. Define the relation $\:\sim\:$ on $\:\{0,1,2,3,4,5,\ldots\}\:$ by $m\sim n \:$ if and only if $\: \mathbf{B}_m$ is isometrically isomorphic to $\mathbf{B}_n$. $\sim\:$ is obviously an equivalence relation. What can the quotient of $\:\{0,1,2,3,4,5,\ldots\}\:$ by $\:\sim\:$ be? The only thing I know about this is that $\:\{\{0,1,2,3,4,5,\ldots\}\}\:$ and $\:\{\{0,2,4,6,8,\ldots\},\{1,3,5,7,9,\ldots\}\}\:$ are both possible.","(ZFC) Let $ \big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $ be a Banach space. Define $ \mathbf{B} \; = \;\big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $. Define $\: \mathbf{B}_0 = \mathbf{B} \:$. For all non-negative integers $n$, define $\mathbf{B}_{n+1}$ to be the Banach space that is the continuous dual of $\mathbf{B}_n$. Define the relation $\:\sim\:$ on $\:\{0,1,2,3,4,5,\ldots\}\:$ by $m\sim n \:$ if and only if $\: \mathbf{B}_m$ is isometrically isomorphic to $\mathbf{B}_n$. $\sim\:$ is obviously an equivalence relation. What can the quotient of $\:\{0,1,2,3,4,5,\ldots\}\:$ by $\:\sim\:$ be? The only thing I know about this is that $\:\{\{0,1,2,3,4,5,\ldots\}\}\:$ and $\:\{\{0,2,4,6,8,\ldots\},\{1,3,5,7,9,\ldots\}\}\:$ are both possible.",,"['functional-analysis', 'normed-spaces']"
51,"Real numbers equipped with the metric $ d (x,y) = | \arctan(x) - \arctan(y)| $ is an incomplete metric space",Real numbers equipped with the metric  is an incomplete metric space," d (x,y) = | \arctan(x) - \arctan(y)| ","I have to show that the real numbers equipped with the metric $ d (x,y) = | \arctan(x) - \arctan(y)| $ is an incomplete metric space. Certainly, I have to search for a Cauchy sequence of real numbers with respect to given metric that must not be convergent. But I am unable to figure out that.  Can anybody help me with this. Thanks for helping me.","I have to show that the real numbers equipped with the metric $ d (x,y) = | \arctan(x) - \arctan(y)| $ is an incomplete metric space. Certainly, I have to search for a Cauchy sequence of real numbers with respect to given metric that must not be convergent. But I am unable to figure out that.  Can anybody help me with this. Thanks for helping me.",,"['functional-analysis', 'metric-spaces']"
52,Norm of an inverse operator: $\|T^{-1}\|=\|T\|^{-1}$?,Norm of an inverse operator: ?,\|T^{-1}\|=\|T\|^{-1},"I am a beginner of functional analysis. I have a simple question when I study this subject. Let $L(X)$ denote the Banach algebra of all bounded linear operators on Banach space $X$ , $T\in X$ is invertible, then $||T^{-1}||=||T||^{-1}$ ? Is this result correct?","I am a beginner of functional analysis. I have a simple question when I study this subject. Let denote the Banach algebra of all bounded linear operators on Banach space , is invertible, then ? Is this result correct?",L(X) X T\in X ||T^{-1}||=||T||^{-1},"['functional-analysis', 'operator-theory', 'operator-algebras']"
53,Why are $L^p$-spaces so ubiquitous?,Why are -spaces so ubiquitous?,L^p,"It always baffled me why $L^p$-spaces are the spaces of choice in almost any area (sometimes with some added regularity (Sobolev/Besov/...)). I understand that the exponent allows for convenient algebraic manipulations, but is there more behind it than mathematical convenience? What bugs me about $L^p$-spaces is that they don't build a scale (of inclusions) but still only allow for one parameter, so by making a choice of exponent you make a choice about two (to my current knowledge) unrelated properties of your function, a) its behavior at singularities (which get milder with high exponent) and b) its tail behavior (which gets less nice with high exponent). How can it still be a good idea to ask ""does this operator map $L^p$ to $L^p$"" rather than ""what does this operator do with singularities and what does it do with tails""? Of course answers to the latter will be harder to formulate and prove, but is that all?","It always baffled me why $L^p$-spaces are the spaces of choice in almost any area (sometimes with some added regularity (Sobolev/Besov/...)). I understand that the exponent allows for convenient algebraic manipulations, but is there more behind it than mathematical convenience? What bugs me about $L^p$-spaces is that they don't build a scale (of inclusions) but still only allow for one parameter, so by making a choice of exponent you make a choice about two (to my current knowledge) unrelated properties of your function, a) its behavior at singularities (which get milder with high exponent) and b) its tail behavior (which gets less nice with high exponent). How can it still be a good idea to ask ""does this operator map $L^p$ to $L^p$"" rather than ""what does this operator do with singularities and what does it do with tails""? Of course answers to the latter will be harder to formulate and prove, but is that all?",,"['functional-analysis', 'partial-differential-equations', 'soft-question', 'sobolev-spaces', 'lp-spaces']"
54,Strong and weak convergence in $\ell^1$,Strong and weak convergence in,\ell^1,"Let $\ell^1$ be the space of absolutely summable real or complex sequences. Let us say that a sequence $(x_1, x_2, \ldots)$ of vectors in $\ell^1$ converges weakly to $x \in \ell^1$ if for every bounded linear functional $\varphi \in (\ell^1)^*$, $\varphi(x_n) \rightarrow \varphi(x)$ as $n \to \infty$. How may I show that weak convergence, in this sense, is the same as the usual convergence-in-norm? It's clear the weak convergence implies pointwise convergence, but that's not good enough to conclude strong convergence... By linearity, it suffices to prove that if $\varphi(x_n) \longrightarrow 0$ for every $\varphi \in (\ell^1)^*$, then $\| x_n \| \longrightarrow 0$. Let $x_n(k)$ be the $k$-th component of the vector $x_n$. Then, $x_n(k) \longrightarrow 0$ for every $k$, so $\sup_n |x_n(k)| < \infty$ for each $k$, and this implies $$\lim_{N \to \infty} \lim_{n \to \infty} \sum_{k=1}^{N} |x_n(k)| = 0$$ This is almost what I want, but the limits are the wrong way around. The obvious next thing to try is to construct some clever functional, or even a family of clever functionals, but I can't think of anything useful here. I can see that pointwise convergence alone is not good enough — if $x_n$ is the standard basis vector, then $x_n \longrightarrow 0$ pointwise, but $\| x_n \| = 1$ for all $n$. The fact that it doesn't converge strongly can be detected by the linear functional $\varphi(x_n) = \sum_k x_n(k)$, but I'm at a loss as to how to generalise this.","Let $\ell^1$ be the space of absolutely summable real or complex sequences. Let us say that a sequence $(x_1, x_2, \ldots)$ of vectors in $\ell^1$ converges weakly to $x \in \ell^1$ if for every bounded linear functional $\varphi \in (\ell^1)^*$, $\varphi(x_n) \rightarrow \varphi(x)$ as $n \to \infty$. How may I show that weak convergence, in this sense, is the same as the usual convergence-in-norm? It's clear the weak convergence implies pointwise convergence, but that's not good enough to conclude strong convergence... By linearity, it suffices to prove that if $\varphi(x_n) \longrightarrow 0$ for every $\varphi \in (\ell^1)^*$, then $\| x_n \| \longrightarrow 0$. Let $x_n(k)$ be the $k$-th component of the vector $x_n$. Then, $x_n(k) \longrightarrow 0$ for every $k$, so $\sup_n |x_n(k)| < \infty$ for each $k$, and this implies $$\lim_{N \to \infty} \lim_{n \to \infty} \sum_{k=1}^{N} |x_n(k)| = 0$$ This is almost what I want, but the limits are the wrong way around. The obvious next thing to try is to construct some clever functional, or even a family of clever functionals, but I can't think of anything useful here. I can see that pointwise convergence alone is not good enough — if $x_n$ is the standard basis vector, then $x_n \longrightarrow 0$ pointwise, but $\| x_n \| = 1$ for all $n$. The fact that it doesn't converge strongly can be detected by the linear functional $\varphi(x_n) = \sum_k x_n(k)$, but I'm at a loss as to how to generalise this.",,"['functional-analysis', 'convergence-divergence', 'banach-spaces', 'lp-spaces', 'weak-convergence']"
55,If $\sum a_n b_n <\infty$ for all $(b_n)\in \ell^2$ then $(a_n) \in \ell^2$,If  for all  then,\sum a_n b_n <\infty (b_n)\in \ell^2 (a_n) \in \ell^2,"I'm trying to prove the following: If $(a_n)$ is a sequence of positive numbers such that $\sum_{n=1}^\infty a_n b_n<\infty$ for all sequences of positive numbers $(b_n)$ such that $\sum_{n=1}^\infty b_n^2<\infty$, then $\sum_{n=1}^\infty a_n^2 <\infty$. The context here is functional analysis homework, in the subject of Hilbert spaces. Here's what I've thought: Let $f=(a_n)>0$. Then the problem reads: if $\int f\overline{g}<\infty$ for all $g>0,g\in \ell^2$, then $f\in \ell^2$. This brings the problem into the realm of $\ell^p$ spaces. I know the inner product is defined only in $\ell^2$, but it's sort of like saying: if $\langle f,g\rangle <\infty$ for all $g>0,g\in \ell^2$ then $f\in \ell^2$. I read this as: ""to check a positive sequence is in $\ell^2$, just check its inner product with any positive sequence in $\ell^2$ is finite, then you're done"", which I find nice, but I can't prove it :P From there, I don't know what else to do. I thought of Hölder's inequality which in this context states: $$\sum_{n=1}^\infty a_nb_n \leq \left( \sum_{n=1}^\infty a_n^2 \right)^{1/2} \left( \sum_{n=1}^\infty b_n^2 \right)^{1/2}$$ but it's not useful here.","I'm trying to prove the following: If $(a_n)$ is a sequence of positive numbers such that $\sum_{n=1}^\infty a_n b_n<\infty$ for all sequences of positive numbers $(b_n)$ such that $\sum_{n=1}^\infty b_n^2<\infty$, then $\sum_{n=1}^\infty a_n^2 <\infty$. The context here is functional analysis homework, in the subject of Hilbert spaces. Here's what I've thought: Let $f=(a_n)>0$. Then the problem reads: if $\int f\overline{g}<\infty$ for all $g>0,g\in \ell^2$, then $f\in \ell^2$. This brings the problem into the realm of $\ell^p$ spaces. I know the inner product is defined only in $\ell^2$, but it's sort of like saying: if $\langle f,g\rangle <\infty$ for all $g>0,g\in \ell^2$ then $f\in \ell^2$. I read this as: ""to check a positive sequence is in $\ell^2$, just check its inner product with any positive sequence in $\ell^2$ is finite, then you're done"", which I find nice, but I can't prove it :P From there, I don't know what else to do. I thought of Hölder's inequality which in this context states: $$\sum_{n=1}^\infty a_nb_n \leq \left( \sum_{n=1}^\infty a_n^2 \right)^{1/2} \left( \sum_{n=1}^\infty b_n^2 \right)^{1/2}$$ but it's not useful here.",,"['functional-analysis', 'hilbert-spaces']"
56,"Major error in classic ""Banach Spaces of Analytic Functions""","Major error in classic ""Banach Spaces of Analytic Functions""",,"""Banach Spaces of Analytic Functions"" by Kenneth Hoffman is an excellent introduction to $H^{p}$ spaces and is considered a classic mathematical analysis textbook. I was therefore surprised to find major mistakes in the proof of a central theorem (""Fatou's Theorem""). Below you will find the proofs in question (pages 79-81) followed by short explanations of their shortcomings. I will give an overview of the proofs so this can be skipped on first reading. Here $A$ is the space of continuous functions on the closed unit disk that are analytic in the interior and $Q_{r}(t)=\text{Im}(\frac{1+re^{i\theta}}{1-re^{i\theta}})$ . $\textbf{Proofs:}$ Theorem. Let $f$ be an integrable function on the circle and $$v(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi f(\theta-t)Q_r(t)~\mathrm{d}t.$$ If $\theta$ is any number such that the integral $$v(\theta)=-\frac{1}{2\pi}\int_{-\pi}^\pi\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t}~\mathrm{d}t$$ exists, then $\lim_{r\to1}v(r,\theta)=v(\theta)$ . Proof. Let $$\phi_\theta(t)=\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t}$$ so that we are assuming $\phi_\theta$ is integrable. Now \begin{align*} v(r,\theta)-v(\theta) &=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\left[1-\frac{2r\sin t\tan\frac{1}{2}t}{1-2r\cos t+r^2}\right]~\mathrm{d}t \\ &=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\frac{(1-r)^2}{1-2r\cos t+r^2}~\mathrm{d}t. \end{align*} Now if $$g_r(t)=\frac{(1-r)^2}{1-2r\cos t+r^2}$$ then $0<g_r(t)<1$ and $\lim_{r\to1}g_r(t)=0$ , except at $t=0$ . Since $\phi_\theta$ is integrable, we must have $\int\phi_\theta g_r\to0$ , i.e., $$\lim_{r\to1}v(r,\theta)=v(\theta).$$ Corollary. If $f$ is differentiable at $\theta$ then $$\lim_{r\to1}v(r,\theta)=v(\theta)$$ exists. If, say, $f$ is continuously differentiable on a closed interval $\lvert\theta-\theta_0\rvert\leq\delta$ , then on that interval the functions $v_r$ converge uniformly as $r\to1$ . Proof. The function $\phi_\theta$ is clearly integrable on any interval $\lvert t\rvert\geq\varepsilon>0$ . If $f$ is differentiable at $\theta$ , then $\phi_\theta$ is bounded as $t\to0$ , so $\phi_\theta$ is integrable. If $f$ is continuously differentiable on $\lvert\theta-\theta_0\rvert\leq\delta$ , then we obtain a uniform bound on $\phi_\theta$ for $\theta$ in the interval and $t$ small; it is easy to see that $v(r,\theta)$ is uniformly close to $v(\theta)$ . Theorem (Fatou). Let $K$ be a closed set of Lebesgue measure zero on the unit circle. Then there exists a function in $A$ which vanishes precisely on $K$ . Proof. Let $w$ be an extended real-valued function on the circle such that $w=-\infty$ on $K$ , and tends continuously to $-\infty$ as $e^{i\theta}$ approaches $K$ ; $w\leq-1$ on the circle; $w$ is finite-valued and continuously differentiable on $C-K$ ; $w$ is integrable. Such a $w$ can be found since $K$ has measure zero. One naive way to construct such a function is the following. Since $K$ is closed, the complement $C-K$ is the union of a countable number of disjoint open intervals (arcs) $I_n$ . Let $\varepsilon_n$ be the length of $I_n$ . Choose a strictly positive and continuously differentiable function $y_n$ on $I_n$ such that $y_n\leq e^{-1}$ , $y_n$ tends to zero at the endpoints of $I_n$ , and $$\int_{I_n}\log y_n\geq-2\varepsilon_n.$$ If we define $y$ to be zero on $K$ and $y=y_n$ on $I_n$ , then $0\leq y\leq e^{-1}$ ; the zeros of $y$ are precisely the points of $K$ ; $y$ is continuous on $C$ and continuously differentiable on $C-K$ ; and $\log y$ is integrable. Let $w=\log y$ . Now define $$h(z)=\frac{1}{2\pi}\int_{-\pi}^\pi\frac{e^{i\theta}+z}{e^{i\theta}-z}w(\theta)~\mathrm{d}\theta.$$ Then $h$ is analytic in the open disc and $\operatorname{Re}h\leq-1$ . By property 3 of $w$ and the Corollary above, $h$ is actually continuous on the complement of $K$ in the closed disc. Since $w$ tends continuously to $-\infty$ at each point of $K$ , the function $$\operatorname{Re}h(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi w(t)P_r(\theta-t)~\mathrm{d}t$$ tends radially to $-\infty$ for each $\theta$ in $K$ . Now let $$g=\frac{1}{h}.$$ It is apparent that $g$ is in $A$ , $\operatorname{Re}g\leq0$ , and the zeros of $g$ on the closed disc are exactly the points of $K$ . We remark that $\operatorname{Re}g=0$ exactly on $K$ . $\textbf{Errors:}$ The goal of Fatou's theorem is to construct a function $g$ in $A$ who's restriction to the boundary vanishes exactly on a given subset $K$ of the circle with measure zero. In the proof of Fatou's theorem this is done by first constructing an analytic function in the open disk, $h$ , via a continuous function on the boundary, $w$ . Since the complement of $K$ on the circle consists of open arcs, to show that the imaginary part, $v(r,\theta)$ , of $h$ is continuous on the complement of $K$ in the closed disk, it suffices to show that $v_{r}(\theta)=v(r,\theta)$ converges uniformly as $r\rightarrow 1$ on the corresponding sector. This is the content of the corollary where $w$ will be playing the role of $f$ . $\textbf{1.}$ The biggest problem with the proof of Fatou's theorem is its reliance on the previous Corollary. Hoffman claims that continuous differentiability of $f$ on an interval $[\theta_{0}-\delta, \theta_{0}+\delta]$ implies that $v(r,\theta)$ converges uniformly to $v$ on this interval as $r\rightarrow 1$ , but the proof he gives falls short. He claims in the last sentence of the proof of the corollary that we can find $\epsilon>0$ such that $\phi_{\theta}(t)$ is uniformly bounded on $[\theta_{0}-\delta, \theta_{0}+\delta]\times [-\epsilon,\epsilon]$ . The idea seems clear: since $f'$ is bounded on this interval we can appeal to the mean value theorem to show that $\phi_{\theta}(t)$ is bounded. But a glance at the definition of $\phi_{\theta}(t)$ shows that this argument wont work since when $\theta=\theta_{0}\pm\delta$ , $f(\theta\pm t)$ takes values outside of $[\theta_{0}-\delta, \theta_{0}+\delta]$ whenever $t\neq 0$ . Now, the mean value theorem does imply that $\phi_{\theta}(t)$ is uniformly bounded for $\theta$ in a slightly smaller interval, but then we don't get the uniform convergence of $v_{r}$ on the entire interval $[\theta_{0}-\delta, \theta_{0}+\delta]$ which is needed for the proof of Fatou's theorem (where $[\theta_{0}-\delta, \theta_{0}+\delta]$ plays the role of the arcs $I_{n}$ ). $\textbf{2.}$ The second issue that I see with the proof of Fatou's theorem is with the construction of $w$ , which relies in turn on the construction of a continuous function $y$ on the circle. However the function $y$ he constructs need not be continuous at points of $K$ as he claims, which is necessary for $w$ to converge to $-\infty$ at points of $K$ . Indeed, since the average of $\text{log}(y_{n})$ is greater than $-2$ on $I_{n}$ , $y_{n}$ must take value greater than $e^{-2}$ on each $I_{n}$ . But if $x\in K$ , then it is possible that every neighborhood of $x$ contains an arc $I_{n}$ , and hence $y$ cannot be continuous at $x$ . $\textbf{3.}$ Finally, even if a function $w$ can be constructed with the four desired properties, I do not see why radial convergence of $\text{Re}(h)$ to $-\infty$ at points $\theta\in K$ is sufficient to imply continuity of $1/h$ on the closed disk since we don't know that this convergence happens at the same rate for each point in $K$ . $\textbf{Question.}$ Considering how popular the book was (Walter Rudin calls it a ""classic"" in his Real and Complex Analyis) and how large the errors in this proof are, it is surpising that they were never corrected in a later edition.  I would really like to know if there is any way to save these arguments to provide a correct proof of Fatou's theorem along these same lines. This would require a new argument for showing that $|v(r,\theta)-v(\theta)|$ converges uniformly on $[\theta_{0}-\delta, \theta_{0}+\delta]$ and a new way to construct a function $w$ with the desired properties.","""Banach Spaces of Analytic Functions"" by Kenneth Hoffman is an excellent introduction to spaces and is considered a classic mathematical analysis textbook. I was therefore surprised to find major mistakes in the proof of a central theorem (""Fatou's Theorem""). Below you will find the proofs in question (pages 79-81) followed by short explanations of their shortcomings. I will give an overview of the proofs so this can be skipped on first reading. Here is the space of continuous functions on the closed unit disk that are analytic in the interior and . Theorem. Let be an integrable function on the circle and If is any number such that the integral exists, then . Proof. Let so that we are assuming is integrable. Now Now if then and , except at . Since is integrable, we must have , i.e., Corollary. If is differentiable at then exists. If, say, is continuously differentiable on a closed interval , then on that interval the functions converge uniformly as . Proof. The function is clearly integrable on any interval . If is differentiable at , then is bounded as , so is integrable. If is continuously differentiable on , then we obtain a uniform bound on for in the interval and small; it is easy to see that is uniformly close to . Theorem (Fatou). Let be a closed set of Lebesgue measure zero on the unit circle. Then there exists a function in which vanishes precisely on . Proof. Let be an extended real-valued function on the circle such that on , and tends continuously to as approaches ; on the circle; is finite-valued and continuously differentiable on ; is integrable. Such a can be found since has measure zero. One naive way to construct such a function is the following. Since is closed, the complement is the union of a countable number of disjoint open intervals (arcs) . Let be the length of . Choose a strictly positive and continuously differentiable function on such that , tends to zero at the endpoints of , and If we define to be zero on and on , then ; the zeros of are precisely the points of ; is continuous on and continuously differentiable on ; and is integrable. Let . Now define Then is analytic in the open disc and . By property 3 of and the Corollary above, is actually continuous on the complement of in the closed disc. Since tends continuously to at each point of , the function tends radially to for each in . Now let It is apparent that is in , , and the zeros of on the closed disc are exactly the points of . We remark that exactly on . The goal of Fatou's theorem is to construct a function in who's restriction to the boundary vanishes exactly on a given subset of the circle with measure zero. In the proof of Fatou's theorem this is done by first constructing an analytic function in the open disk, , via a continuous function on the boundary, . Since the complement of on the circle consists of open arcs, to show that the imaginary part, , of is continuous on the complement of in the closed disk, it suffices to show that converges uniformly as on the corresponding sector. This is the content of the corollary where will be playing the role of . The biggest problem with the proof of Fatou's theorem is its reliance on the previous Corollary. Hoffman claims that continuous differentiability of on an interval implies that converges uniformly to on this interval as , but the proof he gives falls short. He claims in the last sentence of the proof of the corollary that we can find such that is uniformly bounded on . The idea seems clear: since is bounded on this interval we can appeal to the mean value theorem to show that is bounded. But a glance at the definition of shows that this argument wont work since when , takes values outside of whenever . Now, the mean value theorem does imply that is uniformly bounded for in a slightly smaller interval, but then we don't get the uniform convergence of on the entire interval which is needed for the proof of Fatou's theorem (where plays the role of the arcs ). The second issue that I see with the proof of Fatou's theorem is with the construction of , which relies in turn on the construction of a continuous function on the circle. However the function he constructs need not be continuous at points of as he claims, which is necessary for to converge to at points of . Indeed, since the average of is greater than on , must take value greater than on each . But if , then it is possible that every neighborhood of contains an arc , and hence cannot be continuous at . Finally, even if a function can be constructed with the four desired properties, I do not see why radial convergence of to at points is sufficient to imply continuity of on the closed disk since we don't know that this convergence happens at the same rate for each point in . Considering how popular the book was (Walter Rudin calls it a ""classic"" in his Real and Complex Analyis) and how large the errors in this proof are, it is surpising that they were never corrected in a later edition.  I would really like to know if there is any way to save these arguments to provide a correct proof of Fatou's theorem along these same lines. This would require a new argument for showing that converges uniformly on and a new way to construct a function with the desired properties.","H^{p} A Q_{r}(t)=\text{Im}(\frac{1+re^{i\theta}}{1-re^{i\theta}}) \textbf{Proofs:} f v(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi f(\theta-t)Q_r(t)~\mathrm{d}t. \theta v(\theta)=-\frac{1}{2\pi}\int_{-\pi}^\pi\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t}~\mathrm{d}t \lim_{r\to1}v(r,\theta)=v(\theta) \phi_\theta(t)=\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t} \phi_\theta \begin{align*}
v(r,\theta)-v(\theta)
&=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\left[1-\frac{2r\sin t\tan\frac{1}{2}t}{1-2r\cos t+r^2}\right]~\mathrm{d}t \\
&=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\frac{(1-r)^2}{1-2r\cos t+r^2}~\mathrm{d}t.
\end{align*} g_r(t)=\frac{(1-r)^2}{1-2r\cos t+r^2} 0<g_r(t)<1 \lim_{r\to1}g_r(t)=0 t=0 \phi_\theta \int\phi_\theta g_r\to0 \lim_{r\to1}v(r,\theta)=v(\theta). f \theta \lim_{r\to1}v(r,\theta)=v(\theta) f \lvert\theta-\theta_0\rvert\leq\delta v_r r\to1 \phi_\theta \lvert t\rvert\geq\varepsilon>0 f \theta \phi_\theta t\to0 \phi_\theta f \lvert\theta-\theta_0\rvert\leq\delta \phi_\theta \theta t v(r,\theta) v(\theta) K A K w w=-\infty K -\infty e^{i\theta} K w\leq-1 w C-K w w K K C-K I_n \varepsilon_n I_n y_n I_n y_n\leq e^{-1} y_n I_n \int_{I_n}\log y_n\geq-2\varepsilon_n. y K y=y_n I_n 0\leq y\leq e^{-1} y K y C C-K \log y w=\log y h(z)=\frac{1}{2\pi}\int_{-\pi}^\pi\frac{e^{i\theta}+z}{e^{i\theta}-z}w(\theta)~\mathrm{d}\theta. h \operatorname{Re}h\leq-1 w h K w -\infty K \operatorname{Re}h(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi w(t)P_r(\theta-t)~\mathrm{d}t -\infty \theta K g=\frac{1}{h}. g A \operatorname{Re}g\leq0 g K \operatorname{Re}g=0 K \textbf{Errors:} g A K h w K v(r,\theta) h K v_{r}(\theta)=v(r,\theta) r\rightarrow 1 w f \textbf{1.} f [\theta_{0}-\delta, \theta_{0}+\delta] v(r,\theta) v r\rightarrow 1 \epsilon>0 \phi_{\theta}(t) [\theta_{0}-\delta, \theta_{0}+\delta]\times [-\epsilon,\epsilon] f' \phi_{\theta}(t) \phi_{\theta}(t) \theta=\theta_{0}\pm\delta f(\theta\pm t) [\theta_{0}-\delta, \theta_{0}+\delta] t\neq 0 \phi_{\theta}(t) \theta v_{r} [\theta_{0}-\delta, \theta_{0}+\delta] [\theta_{0}-\delta, \theta_{0}+\delta] I_{n} \textbf{2.} w y y K w -\infty K \text{log}(y_{n}) -2 I_{n} y_{n} e^{-2} I_{n} x\in K x I_{n} y x \textbf{3.} w \text{Re}(h) -\infty \theta\in K 1/h K \textbf{Question.} |v(r,\theta)-v(\theta)| [\theta_{0}-\delta, \theta_{0}+\delta] w","['functional-analysis', 'complex-analysis', 'measure-theory', 'solution-verification', 'hardy-spaces']"
57,Conceptual difference between strong and weak formulations,Conceptual difference between strong and weak formulations,,"What are the conceptual differences in presenting a problem in strong or weak form?  For example for a 2D Poisson problem the strong form is: \begin{split}- \nabla^2 u(\pmb{x}) &= f(\pmb{x}),\quad \pmb{x}\mbox{ in } \Omega, \\ u(\pmb{x}) &= u_0(\pmb{x}),\quad \pmb{x}\mbox{ on } \partial \Omega\thinspace .\end{split} where $\Omega$ is the spatial domain and $\partial\Omega$ is the boundary of $\Omega$ . The variational or weak formulation: \begin{equation}    \int_{\Omega} \nabla u \cdot \nabla v \, \mathrm{d}x =        \int_{\Omega} fv \, \mathrm{d}x        \quad \forall v \in \hat{V}.\ \end{equation} where $\hat{V}$ is the test space and $V$ is the trial space: \begin{split}\hat{V} &= \{v \in H^1(\Omega) : v = 0 \mbox{ on } \partial\Omega\}, \\  V      &= \{v \in H^1(\Omega) : v = u_0 \mbox{ on } \partial\Omega\}\thinspace .\end{split} I know that the weak form is very useful in the Finite Element Method, but I don't understand why. Wikipedia's Weak formulation says that the problem requieres a solution in the sense of a distribution. What does this mean? Why are they called Strong and Weak? What is the intuition behind this formulations? Thanks!","What are the conceptual differences in presenting a problem in strong or weak form?  For example for a 2D Poisson problem the strong form is: where is the spatial domain and is the boundary of . The variational or weak formulation: where is the test space and is the trial space: I know that the weak form is very useful in the Finite Element Method, but I don't understand why. Wikipedia's Weak formulation says that the problem requieres a solution in the sense of a distribution. What does this mean? Why are they called Strong and Weak? What is the intuition behind this formulations? Thanks!","\begin{split}- \nabla^2 u(\pmb{x}) &= f(\pmb{x}),\quad \pmb{x}\mbox{ in } \Omega,
\\
u(\pmb{x}) &= u_0(\pmb{x}),\quad \pmb{x}\mbox{ on } \partial \Omega\thinspace .\end{split} \Omega \partial\Omega \Omega \begin{equation}    \int_{\Omega} \nabla u \cdot \nabla v \, \mathrm{d}x =
       \int_{\Omega} fv \, \mathrm{d}x
       \quad \forall v \in \hat{V}.\
\end{equation} \hat{V} V \begin{split}\hat{V} &= \{v \in H^1(\Omega) : v = 0 \mbox{ on } \partial\Omega\}, \\
 V      &= \{v \in H^1(\Omega) : v = u_0 \mbox{ on } \partial\Omega\}\thinspace .\end{split}","['functional-analysis', 'partial-differential-equations', 'calculus-of-variations']"
58,Instructive proofs in functional analysis,Instructive proofs in functional analysis,,"I am beginning to learn functional analysis (from Folland and Royden), but I am from a non-mathematical background, so I often encounter techniques in proofs that I am not familiar with (for example the proof of the open mapping theorem). An idle curiosity masquerading as a pedagogical question, what are proofs of some theorems in functional analysis that make use of techniques/constructions/results that one should be familiar with? I am happy to look up redirections to specific theorems in texts. This question might as well be called - what is your favorite instructive proof in functional analysis?","I am beginning to learn functional analysis (from Folland and Royden), but I am from a non-mathematical background, so I often encounter techniques in proofs that I am not familiar with (for example the proof of the open mapping theorem). An idle curiosity masquerading as a pedagogical question, what are proofs of some theorems in functional analysis that make use of techniques/constructions/results that one should be familiar with? I am happy to look up redirections to specific theorems in texts. This question might as well be called - what is your favorite instructive proof in functional analysis?",,"['analysis', 'functional-analysis', 'fourier-analysis', 'big-list']"
59,Hölder's inequality with three functions,Hölder's inequality with three functions,,"Let $p,q,r \in (1,\infty)$ with $1/p+1/q+1/r=1$. Prove that for every functions   $f \in L^p(\mathbb{R})$, $g \in L^q(\mathbb{R})$,and $h \in L^r(\mathbb{R})$    $$\int_{\mathbb{R}} |fgh|\leq \|f\|_p\centerdot \|g\|_q \centerdot\|h\|_r.$$ I was going to use Hölder's inequality by letting  $1/p+1/q= 1/(pq/p+q)$ and WLOG  let $p<q$ so that $L_q(\mathbb{R})\subseteq L_p(\mathbb{R})$, but I cannot use this inclusion because $\mathbb{R}$ does not have finite measure. Would you please help me if you have any other method to approach this problem?","Let $p,q,r \in (1,\infty)$ with $1/p+1/q+1/r=1$. Prove that for every functions   $f \in L^p(\mathbb{R})$, $g \in L^q(\mathbb{R})$,and $h \in L^r(\mathbb{R})$    $$\int_{\mathbb{R}} |fgh|\leq \|f\|_p\centerdot \|g\|_q \centerdot\|h\|_r.$$ I was going to use Hölder's inequality by letting  $1/p+1/q= 1/(pq/p+q)$ and WLOG  let $p<q$ so that $L_q(\mathbb{R})\subseteq L_p(\mathbb{R})$, but I cannot use this inclusion because $\mathbb{R}$ does not have finite measure. Would you please help me if you have any other method to approach this problem?",,"['functional-analysis', 'measure-theory', 'inequality', 'lp-spaces']"
60,What is the relation between a Banach space and a Hilbert space? [closed],What is the relation between a Banach space and a Hilbert space? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question What is the relation between a Banach space and a Hilbert space? I know that a Hilbert space has inner product (and so a norm), but a Banach space has  a just norm.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question What is the relation between a Banach space and a Hilbert space? I know that a Hilbert space has inner product (and so a norm), but a Banach space has  a just norm.",,"['functional-analysis', 'soft-question']"
61,Spectrum of Indefinite Integral Operators,Spectrum of Indefinite Integral Operators,,"I've considered the following spectral problems for a  long time, I did not know how to tackle them. Maybe they needs some skills with inequalities. For the first, suppose $T:L^{2}[0,1]\rightarrow L^{2}[0,1]$ is defined by $$Tf(x)=\int_{0}^{x} \! f(t) \, dt$$ How can I calculate: the radius of the spectrum of $T$ ? $T^{*}T$ ? the norm of $T$ ? I guess $r(T)$ should be $0$ . but I did know how to prove it. My idea is use Fourier to tackle it, however it does not seem to work. The other problem may be very similar to this one. Let $T:C[0,1]\rightarrow C[0,1]$ be  defined by $$Tf(x)=\int_{0}^{1-x}f(t)dt$$ It is obvious that $T$ is compact and I guess its radius of spectrum is zero, but I do not know how to prove it. Any references and advice will be much appreciated.","I've considered the following spectral problems for a  long time, I did not know how to tackle them. Maybe they needs some skills with inequalities. For the first, suppose is defined by How can I calculate: the radius of the spectrum of ? ? the norm of ? I guess should be . but I did know how to prove it. My idea is use Fourier to tackle it, however it does not seem to work. The other problem may be very similar to this one. Let be  defined by It is obvious that is compact and I guess its radius of spectrum is zero, but I do not know how to prove it. Any references and advice will be much appreciated.","T:L^{2}[0,1]\rightarrow L^{2}[0,1] Tf(x)=\int_{0}^{x} \! f(t) \, dt T T^{*}T T r(T) 0 T:C[0,1]\rightarrow C[0,1] Tf(x)=\int_{0}^{1-x}f(t)dt T","['functional-analysis', 'spectral-theory', 'integral-equations']"
62,"How to prove that if a normed space has Schauder basis, then it is separable? What about the converse?","How to prove that if a normed space has Schauder basis, then it is separable? What about the converse?",,Can we take as a dense subset the collection of all the linear combinations of the vectors of the Schauder basis using the rationals as scalars (or the complex numbers with rational real and imaginary parts for that matter)? What can we say about the converse?,Can we take as a dense subset the collection of all the linear combinations of the vectors of the Schauder basis using the rationals as scalars (or the complex numbers with rational real and imaginary parts for that matter)? What can we say about the converse?,,"['functional-analysis', 'normed-spaces', 'separable-spaces', 'schauder-basis']"
63,The dual space of $c$ is $\ell^1$,The dual space of  is,c \ell^1,"Here is what I know/proved so far: Let $c_0\subset\ell^\infty$ be the collection of all sequences that converge to zero. Prove that the dual space $c_0^*=\ell^1$ . $Proof$ : Let $x\in c_0$ and let $y\in\ell^1$ . We claim that $f_y(x)=\sum_{k=1}^\infty x_ky_k$ is a bounded linear functional. Clearly $f_y$ is bounded since $$ |f_y(x)|=\left|\sum_{k=1}^\infty x_ky_k\right|\le\sum_{k=1}^\infty |x_k||y_k|\le||x_k||_\infty\sum_{k=1}^\infty |y_k|=||x_k||_\infty||y||_1. $$ We can also easily see that $f_y$ is linear. Let $x,z\in c_0$ and $y\in\ell^1$ then $$ f_y(x+z)=\sum_{k=1}^\infty (x_k+z_k)y_k=\sum_{k=1}^\infty (x_ky_k+z_ky_k)=\sum_{k=1}^\infty x_ky_k+\sum_{k=1}^\infty z_ky_k=f_y(x)+f_y(z) $$ and for $\alpha\in\mathbb{R}$ $$ f_y(\alpha x)=\sum_{k=1}^\infty \alpha x_k=\alpha\sum_{k=1}^\infty x_k=\alpha f_y(x). $$ Let $\varepsilon>0$ and since $y\in\ell^1$ , we know that $\sum_{k=1}^\infty |y_k|$ converges. So there exists $N\in\mathbb{N}$ such that whenever $n>N$ we have $$ \sum_{k=n}^\infty |y_k|<\varepsilon. $$ Now define the following sequence $x=\{x_k\}_{k=1}^\infty$ as $$ x_k=\begin{cases} \operatorname{sgn}(y_k),&\,k\le N\\ 0, & \,k>N \end{cases}. $$ Thus $x\in c_0$ and $$ \begin{align} \left|f_y(x)-||y||_1\right|&=\left|\sum_{k=1}^\infty x_ky_k-\sum_{k=1}^\infty |y_k|\right|\\ &=\left|\sum_{k=1}^N\operatorname{sgn}(y_k)y_k-\sum_{k=1}^\infty |y_k|\right|=\left|\sum_{k=1}^N |y_k|-\sum_{k=1}^\infty |y_k|\right|=\left|\sum_{k=N+1}^\infty |y_k|\right|<\varepsilon. \end{align} $$ So we conclude that \begin{equation} \ell^1\subseteq c_0^*. \end{equation} Observe that the above argument also establishes that $||f_y||_*=||y||_1$ . Now let $f$ be any linear functional on $c_0$ and let $\{e_k\}$ be the sequence with a 1 in the $k$ -th position and zero elsewhere. Then for any $x\in c_0$ we have $$ |f(x)|=\left|f\left(\sum_{k=1}^\infty e_kx_k\right)\right|=\left|\sum_{k=1}^\infty f(e_k)x_k\right|\le\sum_{k=1}^\infty |f(e_k)|\,|x_k|\le||x||_\infty\sum_{k=1}^\infty |f(e_k)|. $$ Since $f$ is a bounded functional, we must have $\sum_{k=1}^\infty |f(e_k)|$ converging, otherwise $f(x)$ would be unbounded. Thus $\{f(e_k)\}_{k=1}^\infty\in c_0$ and we conclude that \begin{equation} c_0^*\subseteq\ell^1.  \end{equation} Thus (1) and (2) tell us that $$c_0^*=\ell^1.$$ QUESTIONS: If we take $c$ to be the collection of sequences that converge to some real number and $c^*$ to be it dual space, I know that $c^*=\ell^1$ as well, but I am not sure how to prove it. Is it enough to observe that if $x\in c$ and that $x\to x'$ then $x-x'\in c_0$ , therefore they have the same dual space? I am a little fuzzy here, obviously. Also, can someone clean up my align environment. I can't figure out how to make it compile properly. The code looks fine on my LaTeX implementation, but it doesn't work here.","Here is what I know/proved so far: Let be the collection of all sequences that converge to zero. Prove that the dual space . : Let and let . We claim that is a bounded linear functional. Clearly is bounded since We can also easily see that is linear. Let and then and for Let and since , we know that converges. So there exists such that whenever we have Now define the following sequence as Thus and So we conclude that Observe that the above argument also establishes that . Now let be any linear functional on and let be the sequence with a 1 in the -th position and zero elsewhere. Then for any we have Since is a bounded functional, we must have converging, otherwise would be unbounded. Thus and we conclude that Thus (1) and (2) tell us that QUESTIONS: If we take to be the collection of sequences that converge to some real number and to be it dual space, I know that as well, but I am not sure how to prove it. Is it enough to observe that if and that then , therefore they have the same dual space? I am a little fuzzy here, obviously. Also, can someone clean up my align environment. I can't figure out how to make it compile properly. The code looks fine on my LaTeX implementation, but it doesn't work here.","c_0\subset\ell^\infty c_0^*=\ell^1 Proof x\in c_0 y\in\ell^1 f_y(x)=\sum_{k=1}^\infty x_ky_k f_y 
|f_y(x)|=\left|\sum_{k=1}^\infty x_ky_k\right|\le\sum_{k=1}^\infty |x_k||y_k|\le||x_k||_\infty\sum_{k=1}^\infty |y_k|=||x_k||_\infty||y||_1.
 f_y x,z\in c_0 y\in\ell^1 
f_y(x+z)=\sum_{k=1}^\infty (x_k+z_k)y_k=\sum_{k=1}^\infty (x_ky_k+z_ky_k)=\sum_{k=1}^\infty x_ky_k+\sum_{k=1}^\infty z_ky_k=f_y(x)+f_y(z)
 \alpha\in\mathbb{R} 
f_y(\alpha x)=\sum_{k=1}^\infty \alpha x_k=\alpha\sum_{k=1}^\infty x_k=\alpha f_y(x).
 \varepsilon>0 y\in\ell^1 \sum_{k=1}^\infty |y_k| N\in\mathbb{N} n>N 
\sum_{k=n}^\infty |y_k|<\varepsilon.
 x=\{x_k\}_{k=1}^\infty 
x_k=\begin{cases}
\operatorname{sgn}(y_k),&\,k\le N\\
0, & \,k>N
\end{cases}.
 x\in c_0 
\begin{align}
\left|f_y(x)-||y||_1\right|&=\left|\sum_{k=1}^\infty x_ky_k-\sum_{k=1}^\infty |y_k|\right|\\
&=\left|\sum_{k=1}^N\operatorname{sgn}(y_k)y_k-\sum_{k=1}^\infty |y_k|\right|=\left|\sum_{k=1}^N |y_k|-\sum_{k=1}^\infty |y_k|\right|=\left|\sum_{k=N+1}^\infty |y_k|\right|<\varepsilon.
\end{align}
 \begin{equation}
\ell^1\subseteq c_0^*.
\end{equation} ||f_y||_*=||y||_1 f c_0 \{e_k\} k x\in c_0 
|f(x)|=\left|f\left(\sum_{k=1}^\infty e_kx_k\right)\right|=\left|\sum_{k=1}^\infty f(e_k)x_k\right|\le\sum_{k=1}^\infty |f(e_k)|\,|x_k|\le||x||_\infty\sum_{k=1}^\infty |f(e_k)|.
 f \sum_{k=1}^\infty |f(e_k)| f(x) \{f(e_k)\}_{k=1}^\infty\in c_0 \begin{equation}
c_0^*\subseteq\ell^1. 
\end{equation} c_0^*=\ell^1. c c^* c^*=\ell^1 x\in c x\to x' x-x'\in c_0","['functional-analysis', 'dual-spaces']"
64,Distinction between 'adjoint' and 'formal adjoint',Distinction between 'adjoint' and 'formal adjoint',,"in functional analysis, you encounter the terms 'adjoint' and 'formal adjoint'. What does 'formal' in that case mean? It Sounds like a hint that 'formal adjoints' lack a certain property to make them a 'true' adjoint. I have nowhere found a definition, and would be eager to know.","in functional analysis, you encounter the terms 'adjoint' and 'formal adjoint'. What does 'formal' in that case mean? It Sounds like a hint that 'formal adjoints' lack a certain property to make them a 'true' adjoint. I have nowhere found a definition, and would be eager to know.",,"['terminology', 'functional-analysis']"
65,Does $\sigma(T) = \{1\}$ and $\|T\| = 1$ imply that $T$ is the identity?,Does  and  imply that  is the identity?,\sigma(T) = \{1\} \|T\| = 1 T,"Suppose that $T$ is a bounded linear operator on a complex Banach space X and that we know that $\sigma(T) = \{1\}$ and $\|T\| = 1$ (i.e. the spectrum of the contraction $T$ consists only of a single point, 1). Does it follow that $T$ is the identity operator? This is true in finite dimensions. In finite dimensions, the operator $N := T - \mathbb{1}$ is nilpotent. If $N\neq 0$ , then there exists a strictly positive interger $D$ , such that $N^D \neq 0$ and $N^{D+1} = 0$ . For $K \geq D$ , we have $$1 = \|T\| = \|\mathbb{1} +N\| = \|(\mathbb{1} + N)^K\| = \|\mathbb{1} + \sum_{i = 1}^K {K\choose i} N^i\| = \|\mathbb{1} + \sum_{i = 1}^D {K\choose i} N^i\|.$$ Choose a vector $x \in X$ such that $N^Dx \neq 0$ , then the vectors $x, Nx, N^2x, \dots N^Dx$ are linearly independent. The coordinate function of $Nx$ is ${K\choose 1} = K$ , which is unbounded as $K \rightarrow \infty$ . This contradicts that $\|T\| = 1$ . In infinite dimensions, the difficulty is that $N$ is not nilpotent but merely quasinilpotent and that the coordinate functions may not be continuous. At the moment I can neither prove this nor construct a counter example.","Suppose that is a bounded linear operator on a complex Banach space X and that we know that and (i.e. the spectrum of the contraction consists only of a single point, 1). Does it follow that is the identity operator? This is true in finite dimensions. In finite dimensions, the operator is nilpotent. If , then there exists a strictly positive interger , such that and . For , we have Choose a vector such that , then the vectors are linearly independent. The coordinate function of is , which is unbounded as . This contradicts that . In infinite dimensions, the difficulty is that is not nilpotent but merely quasinilpotent and that the coordinate functions may not be continuous. At the moment I can neither prove this nor construct a counter example.","T \sigma(T) = \{1\} \|T\| = 1 T T N := T - \mathbb{1} N\neq 0 D N^D \neq 0 N^{D+1} = 0 K \geq D 1 = \|T\| = \|\mathbb{1} +N\| = \|(\mathbb{1} + N)^K\| = \|\mathbb{1} + \sum_{i = 1}^K {K\choose i} N^i\| = \|\mathbb{1} + \sum_{i = 1}^D {K\choose i} N^i\|. x \in X N^Dx \neq 0 x, Nx, N^2x, \dots N^Dx Nx {K\choose 1} = K K \rightarrow \infty \|T\| = 1 N","['functional-analysis', 'operator-theory', 'spectral-theory', 'semigroup-of-operators']"
66,Examples of compact sets that are infinite dimensional and not bounded,Examples of compact sets that are infinite dimensional and not bounded,,"In an infinite dimensional Banach space, does a compact subset have to be finite dimensional? I know it cannot contain any infinite dimensional balls, if this mean it has to be finite dimensional, then why do I sometimes see phrases like ""for any compact subspaces of some vector space""? Also, do compact sets in infinite dimensional Banach spaces have to be bounded? Is there a general result that tells us what compact subsets of infinite dimensional vector spaces look like? Thanks so much!","In an infinite dimensional Banach space, does a compact subset have to be finite dimensional? I know it cannot contain any infinite dimensional balls, if this mean it has to be finite dimensional, then why do I sometimes see phrases like ""for any compact subspaces of some vector space""? Also, do compact sets in infinite dimensional Banach spaces have to be bounded? Is there a general result that tells us what compact subsets of infinite dimensional vector spaces look like? Thanks so much!",,"['functional-analysis', 'banach-spaces', 'compactness', 'examples-counterexamples', 'normed-spaces']"
67,Why is uniqueness important for PDEs?,Why is uniqueness important for PDEs?,,"Every text on PDEs I come across will spend alot of time on showing the existence and uniqueness of solutions to a particular PDE. The importance of the existence of a solution to a PDE is obvious, but I can't see why so much time is spent on uniqueness. Why do we care whether a solution is unique or not as long as we know that there is a solution? Is uniqueness just shown for the sake of it, ie. the sake of completeness, or is there some deeper reason why it's considered important to show uniqueness?","Every text on PDEs I come across will spend alot of time on showing the existence and uniqueness of solutions to a particular PDE. The importance of the existence of a solution to a PDE is obvious, but I can't see why so much time is spent on uniqueness. Why do we care whether a solution is unique or not as long as we know that there is a solution? Is uniqueness just shown for the sake of it, ie. the sake of completeness, or is there some deeper reason why it's considered important to show uniqueness?",,"['functional-analysis', 'partial-differential-equations']"
68,Cardinality of a Hamel basis of $\ell_1(\mathbb{R})$,Cardinality of a Hamel basis of,\ell_1(\mathbb{R}),"What is the cardinality of a Hamel basis of $\ell_1(\mathbb R)$? Is it deducible in ZFC that it is seemingly continuum? Does it follow from this that each Banach space of density $\leqslant 2^{\aleph_0}$ has a Hamel basis of cardinality continuum (OK, I do know it cannot be smaller for an inf.-dim. Banach space)?","What is the cardinality of a Hamel basis of $\ell_1(\mathbb R)$? Is it deducible in ZFC that it is seemingly continuum? Does it follow from this that each Banach space of density $\leqslant 2^{\aleph_0}$ has a Hamel basis of cardinality continuum (OK, I do know it cannot be smaller for an inf.-dim. Banach space)?",,"['functional-analysis', 'set-theory', 'vector-spaces', 'banach-spaces', 'cardinals']"
69,Showing that $\ker T$ is closed if and only if $T$ is continuous. [duplicate],Showing that  is closed if and only if  is continuous. [duplicate],\ker T T,"This question already has answers here : $T$ is continuous if and only if $\ker T$ is closed (2 answers) Closed 3 years ago . Possible Duplicate: $T$ is continuous if and only if $\ker T$ is closed Let $T: X\to \mathbf{R}$ be linear. Suppose that $X$ is a Banach space. I want to show that $T$ is continuous if and only if $\ker T $ is closed. My Attempt. $(\Rightarrow)$ Suppose $T$ is continuous. Then if $x_n\to x$, then $T(x_n)\to T(x)$. Let $x_n \in \ker T$. Then $T(x_n) = 0$. Using continuity, $$ T(x) = \lim_{n\to \infty} T(x_n) = 0.$$ Hence $x\in \ker T$ and thus $\ker T$ is closed. $(\Leftarrow)$  Suppose $T$ is not continuous. So $T$ is not bounded. i.e. $\exists$ a sequence $x_n$ such that $T(x_n) \to \infty$ as $n\to \infty$. Let $a\notin \ker T$. Then defining $$x_n' = a - \frac{T(a)}{T(x_n)}x_n ,$$ it is clear that $T(x_n') = 0$ and so $x_n'\in \ker T$. Also $x_n' \to a \notin \ker T.$ So $\ker T$ is not closed.  Hence $\ker T$ closed implies that $T$ is continuous. Have I approached this question correctly? Are there other ways of approaching it?","This question already has answers here : $T$ is continuous if and only if $\ker T$ is closed (2 answers) Closed 3 years ago . Possible Duplicate: $T$ is continuous if and only if $\ker T$ is closed Let $T: X\to \mathbf{R}$ be linear. Suppose that $X$ is a Banach space. I want to show that $T$ is continuous if and only if $\ker T $ is closed. My Attempt. $(\Rightarrow)$ Suppose $T$ is continuous. Then if $x_n\to x$, then $T(x_n)\to T(x)$. Let $x_n \in \ker T$. Then $T(x_n) = 0$. Using continuity, $$ T(x) = \lim_{n\to \infty} T(x_n) = 0.$$ Hence $x\in \ker T$ and thus $\ker T$ is closed. $(\Leftarrow)$  Suppose $T$ is not continuous. So $T$ is not bounded. i.e. $\exists$ a sequence $x_n$ such that $T(x_n) \to \infty$ as $n\to \infty$. Let $a\notin \ker T$. Then defining $$x_n' = a - \frac{T(a)}{T(x_n)}x_n ,$$ it is clear that $T(x_n') = 0$ and so $x_n'\in \ker T$. Also $x_n' \to a \notin \ker T.$ So $\ker T$ is not closed.  Hence $\ker T$ closed implies that $T$ is continuous. Have I approached this question correctly? Are there other ways of approaching it?",,['functional-analysis']
70,$C(X)$ is separable when $X$ is compact?,is separable when  is compact?,C(X) X,"$X$ is a compact metric space, then $C(X)$ is separable, where $C(X)$ denotes the space of continuous functions on $X$ . How to prove it? And if $X$ is just a compact Hausdorff space, then is $C(X)$ still separable? Or if $X$ is just a compact (not necessarily Hausdorff) space, then is $C(X)$ still separable? Please help me. Thanks in advance.","is a compact metric space, then is separable, where denotes the space of continuous functions on . How to prove it? And if is just a compact Hausdorff space, then is still separable? Or if is just a compact (not necessarily Hausdorff) space, then is still separable? Please help me. Thanks in advance.",X C(X) C(X) X X C(X) X C(X),['functional-analysis']
71,"The difference between hermitian, symmetric and self adjoint operators.","The difference between hermitian, symmetric and self adjoint operators.",,"I am struggling with the concept of hermitian operators, symmetric operators and self adjoint operators. All of the relevant material seems quite self contradictory, and the only notes I have do not quite seem to do the job. Overall I would like to know if there is a specific chain of implications, I.e self adjoint $\Rightarrow$ symmetric $\Rightarrow$ Hermitian. Is there such a chain? Also when I consider an inner product $\langle u,v\rangle$ , I would consider the antilinearity in the second argument, i.e $\langle u,av+bw\rangle= \overline{a}\langle u,v\rangle +\overline{b}\langle u,w\rangle$ Thank you all!","I am struggling with the concept of hermitian operators, symmetric operators and self adjoint operators. All of the relevant material seems quite self contradictory, and the only notes I have do not quite seem to do the job. Overall I would like to know if there is a specific chain of implications, I.e self adjoint symmetric Hermitian. Is there such a chain? Also when I consider an inner product , I would consider the antilinearity in the second argument, i.e Thank you all!","\Rightarrow \Rightarrow \langle u,v\rangle \langle u,av+bw\rangle= \overline{a}\langle u,v\rangle +\overline{b}\langle u,w\rangle","['functional-analysis', 'operator-theory', 'definition', 'inner-products']"
72,The space of Riemannian metrics on a given manifold.,The space of Riemannian metrics on a given manifold.,,"For a finite-dimensional smooth (Hausdorff, second-countable) manifold $M$, consider the set  $$\mathcal{Met}(M) = \{ g : g \text{ is a Riemannian metric on }M \}.$$ I'd like to know about the typical differentiable structures one can place on $\mathcal{Met}(M)$, and how are they constructed. More specifically, is this in general an infinite-dimensional Banach or Hilbert manifold? Or perhaps a Fréchet manifold? What if $M$ is compact? Also, any references containing a good deal of details, proofs, etc. would be deeply appreciated. Thanks!","For a finite-dimensional smooth (Hausdorff, second-countable) manifold $M$, consider the set  $$\mathcal{Met}(M) = \{ g : g \text{ is a Riemannian metric on }M \}.$$ I'd like to know about the typical differentiable structures one can place on $\mathcal{Met}(M)$, and how are they constructed. More specifically, is this in general an infinite-dimensional Banach or Hilbert manifold? Or perhaps a Fréchet manifold? What if $M$ is compact? Also, any references containing a good deal of details, proofs, etc. would be deeply appreciated. Thanks!",,"['functional-analysis', 'differential-geometry', 'banach-spaces', 'riemannian-geometry']"
73,Is it possible to characterize completeness of a normed vector space by convergence of Neumann series?,Is it possible to characterize completeness of a normed vector space by convergence of Neumann series?,,"If $X$ is a normed vector space and if for each bounded operator $T \in B(X)$ with $\| T\| < 1$, the operator ${\rm id} - T$ is boundedly invertible, does it follow that $X$ is complete? Context: It is well known that if $X$ is a Banach space and if $T \in B(X) = B(X,X)$ is a bounded linear operator on $X$ with $\| T \|  <1$, then the Neumann series $\sum_{n=0}^\infty T^n$ converges (in the operator norm) to $({\rm id} - T)^{-1}$. In particular, ${\rm id} - T$ is invertible. There are counterexample to this fact if we do not assume $X$ to be complete. For example, we can take $X = \ell_0 (\Bbb{N})$ (the finitely supported sequences) and $T = \frac{1}{2} S$, where $S$ is the right shift operator. In this case, it is easy to see that $\sum_{n=0}^\infty T^n$ does not converge to a well-defined operator from $X$ to $X$. After I came up with the above counterexample, I wondered if we can characterize completeness of the normed vector space $X$ by the above property, as in the question stated above. Thoughts on the problem: Equivalently, we could require that $\sum_{n=0}^\infty T^n$ converges to a well-defined operator from $X\to X$ as soon as $\|T\|<1$, since in the completion $\overline{X}$, we still know that $T$ extends to a contiuous linear operator $\overline{T} : \overline{X} \to \overline{X}$ with $\| \overline{T} \| = \| T\|<1$, so that $S := {\rm id_{\overline{X}}} - \overline{T}$ is invertible with $S^{-1} = \sum_{n=0}^\infty \overline{T}^n$ and the restriction of $S^{-1}$ to $X$ is the inverse of ${\rm id} - T$, so that $({\rm id}_X - T)^{-1} = \sum_{n=0}^\infty T^n$. I know that $X$ is complete iff $B(X)$ is, so that it would suffice to show that $B(X)$ is complete. To show that a normed vector space $Y$ is complete, it suffices to show that ""absolute convergence"" of a series implies convergence, or even more restrictive that if $\|x_n\|\leq 2^{-n}$ for all $n$, then the series $\sum_{n=1}^\infty x_n$ converges in $Y$. My problem with applying observation 3 to $Y = B(X)$ is that we only know that the statement for 3 is true for $x_n = T^n$ with suitable $T$, which seems to be too restrictive. In fact, I don't know how to construct any kind of nontrivial bounded operators on a general normed vector space $X$, apart from operators of the form $x \mapsto \varphi(x) \cdot x_0$ (and linear combinations of those), where $\varphi $ is a bounded functional on $Y$ and $x_0 \in Y$. But for operators as above (i.e. with finite dimensional range), convergence of the series $\sum_{n=0}^\infty T^n$ is always true, since in fact we only need to consider a finite dimensional subspace, which certainly is complete.","If $X$ is a normed vector space and if for each bounded operator $T \in B(X)$ with $\| T\| < 1$, the operator ${\rm id} - T$ is boundedly invertible, does it follow that $X$ is complete? Context: It is well known that if $X$ is a Banach space and if $T \in B(X) = B(X,X)$ is a bounded linear operator on $X$ with $\| T \|  <1$, then the Neumann series $\sum_{n=0}^\infty T^n$ converges (in the operator norm) to $({\rm id} - T)^{-1}$. In particular, ${\rm id} - T$ is invertible. There are counterexample to this fact if we do not assume $X$ to be complete. For example, we can take $X = \ell_0 (\Bbb{N})$ (the finitely supported sequences) and $T = \frac{1}{2} S$, where $S$ is the right shift operator. In this case, it is easy to see that $\sum_{n=0}^\infty T^n$ does not converge to a well-defined operator from $X$ to $X$. After I came up with the above counterexample, I wondered if we can characterize completeness of the normed vector space $X$ by the above property, as in the question stated above. Thoughts on the problem: Equivalently, we could require that $\sum_{n=0}^\infty T^n$ converges to a well-defined operator from $X\to X$ as soon as $\|T\|<1$, since in the completion $\overline{X}$, we still know that $T$ extends to a contiuous linear operator $\overline{T} : \overline{X} \to \overline{X}$ with $\| \overline{T} \| = \| T\|<1$, so that $S := {\rm id_{\overline{X}}} - \overline{T}$ is invertible with $S^{-1} = \sum_{n=0}^\infty \overline{T}^n$ and the restriction of $S^{-1}$ to $X$ is the inverse of ${\rm id} - T$, so that $({\rm id}_X - T)^{-1} = \sum_{n=0}^\infty T^n$. I know that $X$ is complete iff $B(X)$ is, so that it would suffice to show that $B(X)$ is complete. To show that a normed vector space $Y$ is complete, it suffices to show that ""absolute convergence"" of a series implies convergence, or even more restrictive that if $\|x_n\|\leq 2^{-n}$ for all $n$, then the series $\sum_{n=1}^\infty x_n$ converges in $Y$. My problem with applying observation 3 to $Y = B(X)$ is that we only know that the statement for 3 is true for $x_n = T^n$ with suitable $T$, which seems to be too restrictive. In fact, I don't know how to construct any kind of nontrivial bounded operators on a general normed vector space $X$, apart from operators of the form $x \mapsto \varphi(x) \cdot x_0$ (and linear combinations of those), where $\varphi $ is a bounded functional on $Y$ and $x_0 \in Y$. But for operators as above (i.e. with finite dimensional range), convergence of the series $\sum_{n=0}^\infty T^n$ is always true, since in fact we only need to consider a finite dimensional subspace, which certainly is complete.",,"['functional-analysis', 'operator-theory', 'banach-spaces', 'normed-spaces']"
74,Show that $l^2$ is a Hilbert space,Show that  is a Hilbert space,l^2,"Let $l^2$ be the space of square summable sequences with the inner product $\langle x,y\rangle=\sum_\limits{i=1}^\infty x_iy_i$. (a) show that $l^2$ is H Hilbert space. To show that it's a Hilbert space I need to show that the space is complete. For that I need to construct a Cauchy sequence and show it converges with respect to the norm. However, I find it confusing to construct a Cauchy sequence of sequences?","Let $l^2$ be the space of square summable sequences with the inner product $\langle x,y\rangle=\sum_\limits{i=1}^\infty x_iy_i$. (a) show that $l^2$ is H Hilbert space. To show that it's a Hilbert space I need to show that the space is complete. For that I need to construct a Cauchy sequence and show it converges with respect to the norm. However, I find it confusing to construct a Cauchy sequence of sequences?",,"['functional-analysis', 'hilbert-spaces', 'banach-spaces', 'lp-spaces', 'complete-spaces']"
75,$\ell_p$ is Hilbert if and only if $p=2$,is Hilbert if and only if,\ell_p p=2,"Can anybody please help me to prove this: Let $p$ be greater than or equal to $1$ . Show that for the space $\ell_p=\{(u_n):\sum_{n=1}^\infty |u_n|^p<\infty\}$ of all $p$ -summable sequences (with norm $||u||_p=\sqrt[p]{\sum_{n=1}^\infty |u_n|^p}\ )$ , there is an inner product $<\_\,|\,\_> $ s.t. $||u||^2=<u\,|\,u>$ if and only if $p=2$ .","Can anybody please help me to prove this: Let be greater than or equal to . Show that for the space of all -summable sequences (with norm , there is an inner product s.t. if and only if .","p 1 \ell_p=\{(u_n):\sum_{n=1}^\infty |u_n|^p<\infty\} p ||u||_p=\sqrt[p]{\sum_{n=1}^\infty |u_n|^p}\ ) <\_\,|\,\_>  ||u||^2=<u\,|\,u> p=2","['functional-analysis', 'hilbert-spaces', 'inner-products', 'lp-spaces']"
76,weak sequential continuity of linear operators,weak sequential continuity of linear operators,,Suppose I have a weakly sequentially continuous linear operator T between two normed linear spaces X and Y (i.e. $x_n \stackrel {w}{\rightharpoonup} x$ in $X$ $\Rightarrow$ $T(x_n) \stackrel {w}{\rightharpoonup} T(x)$ in $Y$). Does this imply that my operator T must be bounded?,Suppose I have a weakly sequentially continuous linear operator T between two normed linear spaces X and Y (i.e. $x_n \stackrel {w}{\rightharpoonup} x$ in $X$ $\Rightarrow$ $T(x_n) \stackrel {w}{\rightharpoonup} T(x)$ in $Y$). Does this imply that my operator T must be bounded?,,"['functional-analysis', 'weak-convergence']"
77,Prerequisites for functional analysis,Prerequisites for functional analysis,,"I'm having a few months of free time, and I decided to do a self-study on functional analysis (in-depth) in the meantime. I'm aware that functional analysis requires a good deal of foundation from real analysis and linear algebra. How much of them is exactly needed? I've taken courses on analysis and linear algebra which cover Axler's Linear Algebra Done Right and the first 7 chapters of Rudin. Would that be enough? Also, can you recommend me some books to study functional analysis thoroughly? Thanks in advance.","I'm having a few months of free time, and I decided to do a self-study on functional analysis (in-depth) in the meantime. I'm aware that functional analysis requires a good deal of foundation from real analysis and linear algebra. How much of them is exactly needed? I've taken courses on analysis and linear algebra which cover Axler's Linear Algebra Done Right and the first 7 chapters of Rudin. Would that be enough? Also, can you recommend me some books to study functional analysis thoroughly? Thanks in advance.",,"['reference-request', 'functional-analysis']"
78,Dual of $l^\infty$ is not $l^1$,Dual of  is not,l^\infty l^1,"I know that the dual space of $l^\infty$ is not $l^1$, but I didn't understand the reason. Could you give me a example of an $x \in l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is not a linear bounded functional on $l^\infty$, or maybe an example of a $x \notin l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is a linear bounded functional on $l^\infty$?","I know that the dual space of $l^\infty$ is not $l^1$, but I didn't understand the reason. Could you give me a example of an $x \in l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is not a linear bounded functional on $l^\infty$, or maybe an example of a $x \notin l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is a linear bounded functional on $l^\infty$?",,"['functional-analysis', 'banach-spaces', 'lp-spaces', 'dual-spaces']"
79,Does there exist a linearly independent and dense subset?,Does there exist a linearly independent and dense subset?,,Do there exist in infinitely dimensional normed spaces linearly independent and dense subsets? (Existence of linearly independent dense subset is equivalent of existence of dense Hamel Basis.) Thanks.,Do there exist in infinitely dimensional normed spaces linearly independent and dense subsets? (Existence of linearly independent dense subset is equivalent of existence of dense Hamel Basis.) Thanks.,,['functional-analysis']
80,Proving that closed (and open) balls are convex,Proving that closed (and open) balls are convex,,"Let $X$ be a normed linear space, $x\in X$ and $r>0$. Define the open and closed ball centered at $x$ as  $$ B(x, r) = \{y \in X : \Vert x − y\Vert < r\} $$  $$ \overline{B}(x, r) = \{y \in X : \Vert x − y\Vert \leq r\}. $$ Then $B(x, r)$ and $\overline{B}(x, r)$ are convex. I tried to prove this, but either my calculation is incorrect, or I am on the wrong path: I aim to show for the closed ball $\overline{B}(x,r)$ (for open ball I assume the proof is similar). Suppose $y,z \in \overline{B}(x, r)$. Then $\Vert x − y\Vert \leq r$ and $\Vert x − z\Vert \leq r$. We must show that $t \in [0,1]$ implies $ty + (1-t)z \in \overline{B}(x,r)$. But $t \in [0,1]$ implies $$ \Vert ty + (1-t)z - x\Vert = \Vert t(y-z) + z - x\Vert \leq |t| \Vert y-z\Vert + \Vert z-x\Vert \leq |t|(\Vert y-x\Vert + \Vert x-z\Vert) + \Vert z-x\Vert < |t|(2r) + r = r(2|t| + 1), $$ which is not necessarily $\leq r$. We probably wanted to end up with $< |t|r$ or $\leq |t|r$ as our final inequality. Thanks in advance.","Let $X$ be a normed linear space, $x\in X$ and $r>0$. Define the open and closed ball centered at $x$ as  $$ B(x, r) = \{y \in X : \Vert x − y\Vert < r\} $$  $$ \overline{B}(x, r) = \{y \in X : \Vert x − y\Vert \leq r\}. $$ Then $B(x, r)$ and $\overline{B}(x, r)$ are convex. I tried to prove this, but either my calculation is incorrect, or I am on the wrong path: I aim to show for the closed ball $\overline{B}(x,r)$ (for open ball I assume the proof is similar). Suppose $y,z \in \overline{B}(x, r)$. Then $\Vert x − y\Vert \leq r$ and $\Vert x − z\Vert \leq r$. We must show that $t \in [0,1]$ implies $ty + (1-t)z \in \overline{B}(x,r)$. But $t \in [0,1]$ implies $$ \Vert ty + (1-t)z - x\Vert = \Vert t(y-z) + z - x\Vert \leq |t| \Vert y-z\Vert + \Vert z-x\Vert \leq |t|(\Vert y-x\Vert + \Vert x-z\Vert) + \Vert z-x\Vert < |t|(2r) + r = r(2|t| + 1), $$ which is not necessarily $\leq r$. We probably wanted to end up with $< |t|r$ or $\leq |t|r$ as our final inequality. Thanks in advance.",,['functional-analysis']
81,Prove or disprove a claim related to $L^p$ space,Prove or disprove a claim related to  space,L^p,"The following question is just a toy model: Let $f:[0,1] \rightarrow \mathbb{R}$ be Lebesgue integrable, and suppose that for any $0\le a<b \le1$, $$\int_a^b |f(x)|dx \le \sqrt{b-a}$$ then prove or disprove that $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{1/2}}: E \subset [0,1]\right\}<+\infty$$ If the claim above is false, then is it possible to prove that for any fixed $0<t<1/2$,  $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{t}}: E \subset [0,1]\right\}<+\infty$$ Motivation : This is a long story. Throughout the following, we assume that $f$ is a measurable function from a bounded regular open set $\Omega \subset \mathbb{R}^n$ to $\mathbb{R}$. We know that if $f$ is $L^p(\Omega) \space(p>1)$, then by Holder's inequality, $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}<+\infty \quad \quad\quad\quad(*)$$Then naturally I wanted to ask the inverse question:  $\space$ Does $(*)$ imply $f \in L^p(\Omega)$? One of my smart friends figured out that $(*)$ is equivalent to $f$ is weak $L^p$. See the answer here: a characterization of $L^p$ space . Then naturally I want to know in $(*)$, instead of taking supremum over all measurable sets, what would happen if taking supremum over all cubes or balls? This leads to the toy model I asked at the beginning: the toy model is for $p=2, n=1$, $f$ is integrable, and cube is thus just an interval. Now let me formulate my question neatly as follows: Set $M_p:=\left\{f:\sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}<+\infty\right\}$ $\tilde{M_p}:=\left\{f:\sup \left\{\frac{\int_B |f|dx}{|B|^{1-1/p}}: \text{$B$ is a ball $\subset \Omega$}\right\}<+\infty\right\}$ $L_p^w$ := the weak-$L^p$ space. $\tilde{L_p}:=\{f \in L^q(\Omega): \forall 1\le q<p\}$ Then by my friend's result and interpolation theorem, $$L_p^w=M_p \subset \tilde{L_p}$$Also trivially, $M_p \subset \tilde{M_p}$. So the ultimate goal is that I want to know the relationship between $M_p, \tilde{M_p}$, and $\tilde{L_p}$. In particular, the toy model I asked at the beginning focuses on whether $M_p = \tilde{M_p}$. An equivalent statement of whether $M_p=\tilde{M_p}$ is the following: Let $0<s<1$. If $\mu$ is a finite measure on $\Omega$ and absolutely continuous with respect to Lebesgue measure in $\mathbb{R}^n$, and  $\lim\sup _{r \rightarrow 0} \frac{\mu({B_r(x)})}{r^{ns}} \le 1, \forall x\in \Omega$ , is it true that $sup \{\frac{\mu(E)}{|E|^s}: E \subset \Omega\}<+\infty$ ? I also want to understand the following question: $\space$ If $M_p = \tilde{M_p}$ and $f \in M_p$, is it true that $\sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}=\sup \left\{\frac{\int_B |f|dx}{|B|^{1-1/p}}: \text{$B$ is a ball $\subset \Omega$}\right\}?$ Or what can we say about the ratio? By the way, the definition $\tilde{M_p}$ here is the same as $M^p$ defined in Gilbarg and Trudinger on Page 164, which is the so called Morrey Space. I looked up some references but didn't find any claims whether or not $M^p \subset L^q \quad \forall 1 \le q < p$. Maybe I'm thinking too much. I should focus on solving one problem and then go step by step. My effort: In terms of the possible approaches, I think the approximate continuity of any measurable function and a nice covering argument would be helpful. Also, if $f$ is integrable, then one can observe that if $$T_pf(x):=\lim\sup_{r \rightarrow 0} \frac{1}{|B_r(x)|^{1−1/p}}\int_{B_r(x)}|f(y)|dy$$ is bounded in $\Omega$, then $$\mathcal{M}_pf(x):=\sup_{r > 0} \frac{1}{|B_r(x)|^{1−1/p}}\int_{B_r(x)}|f(y)|dy$$ is also bounded, and vice versa. Also, $$T_pf(x)=0,\mathcal{H}^{s}-a.e, \forall s\ge 1-1/p$$ So the size of the blow-up points should be very small, and thus a nice covering argument may be applied, at least we don't need to worry about cover the singular sets by balls or other arbituary sets. Maybe at least $\tilde{M_p} \subset M_q, \forall 1\le q<p$ can be provable. I have a lot of other observations, but it is cumbersome to type them down. Overall, I think these problems should be related to geometric measure theory and are not trivial. Also, my smart friend suggests me try to apply the Littlewood-Paley Theory. He thinks of them as standard problems in harmonic analysis. Any ideas, comments and partial result would be fully appreciated. I've no idea even about the toy model proposed.","The following question is just a toy model: Let $f:[0,1] \rightarrow \mathbb{R}$ be Lebesgue integrable, and suppose that for any $0\le a<b \le1$, $$\int_a^b |f(x)|dx \le \sqrt{b-a}$$ then prove or disprove that $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{1/2}}: E \subset [0,1]\right\}<+\infty$$ If the claim above is false, then is it possible to prove that for any fixed $0<t<1/2$,  $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{t}}: E \subset [0,1]\right\}<+\infty$$ Motivation : This is a long story. Throughout the following, we assume that $f$ is a measurable function from a bounded regular open set $\Omega \subset \mathbb{R}^n$ to $\mathbb{R}$. We know that if $f$ is $L^p(\Omega) \space(p>1)$, then by Holder's inequality, $$ \sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}<+\infty \quad \quad\quad\quad(*)$$Then naturally I wanted to ask the inverse question:  $\space$ Does $(*)$ imply $f \in L^p(\Omega)$? One of my smart friends figured out that $(*)$ is equivalent to $f$ is weak $L^p$. See the answer here: a characterization of $L^p$ space . Then naturally I want to know in $(*)$, instead of taking supremum over all measurable sets, what would happen if taking supremum over all cubes or balls? This leads to the toy model I asked at the beginning: the toy model is for $p=2, n=1$, $f$ is integrable, and cube is thus just an interval. Now let me formulate my question neatly as follows: Set $M_p:=\left\{f:\sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}<+\infty\right\}$ $\tilde{M_p}:=\left\{f:\sup \left\{\frac{\int_B |f|dx}{|B|^{1-1/p}}: \text{$B$ is a ball $\subset \Omega$}\right\}<+\infty\right\}$ $L_p^w$ := the weak-$L^p$ space. $\tilde{L_p}:=\{f \in L^q(\Omega): \forall 1\le q<p\}$ Then by my friend's result and interpolation theorem, $$L_p^w=M_p \subset \tilde{L_p}$$Also trivially, $M_p \subset \tilde{M_p}$. So the ultimate goal is that I want to know the relationship between $M_p, \tilde{M_p}$, and $\tilde{L_p}$. In particular, the toy model I asked at the beginning focuses on whether $M_p = \tilde{M_p}$. An equivalent statement of whether $M_p=\tilde{M_p}$ is the following: Let $0<s<1$. If $\mu$ is a finite measure on $\Omega$ and absolutely continuous with respect to Lebesgue measure in $\mathbb{R}^n$, and  $\lim\sup _{r \rightarrow 0} \frac{\mu({B_r(x)})}{r^{ns}} \le 1, \forall x\in \Omega$ , is it true that $sup \{\frac{\mu(E)}{|E|^s}: E \subset \Omega\}<+\infty$ ? I also want to understand the following question: $\space$ If $M_p = \tilde{M_p}$ and $f \in M_p$, is it true that $\sup \left\{\frac{\int_E |f|dx}{|E|^{1-1/p}}: E \subset \Omega\right\}=\sup \left\{\frac{\int_B |f|dx}{|B|^{1-1/p}}: \text{$B$ is a ball $\subset \Omega$}\right\}?$ Or what can we say about the ratio? By the way, the definition $\tilde{M_p}$ here is the same as $M^p$ defined in Gilbarg and Trudinger on Page 164, which is the so called Morrey Space. I looked up some references but didn't find any claims whether or not $M^p \subset L^q \quad \forall 1 \le q < p$. Maybe I'm thinking too much. I should focus on solving one problem and then go step by step. My effort: In terms of the possible approaches, I think the approximate continuity of any measurable function and a nice covering argument would be helpful. Also, if $f$ is integrable, then one can observe that if $$T_pf(x):=\lim\sup_{r \rightarrow 0} \frac{1}{|B_r(x)|^{1−1/p}}\int_{B_r(x)}|f(y)|dy$$ is bounded in $\Omega$, then $$\mathcal{M}_pf(x):=\sup_{r > 0} \frac{1}{|B_r(x)|^{1−1/p}}\int_{B_r(x)}|f(y)|dy$$ is also bounded, and vice versa. Also, $$T_pf(x)=0,\mathcal{H}^{s}-a.e, \forall s\ge 1-1/p$$ So the size of the blow-up points should be very small, and thus a nice covering argument may be applied, at least we don't need to worry about cover the singular sets by balls or other arbituary sets. Maybe at least $\tilde{M_p} \subset M_q, \forall 1\le q<p$ can be provable. I have a lot of other observations, but it is cumbersome to type them down. Overall, I think these problems should be related to geometric measure theory and are not trivial. Also, my smart friend suggests me try to apply the Littlewood-Paley Theory. He thinks of them as standard problems in harmonic analysis. Any ideas, comments and partial result would be fully appreciated. I've no idea even about the toy model proposed.",,"['functional-analysis', 'measure-theory', 'harmonic-analysis', 'geometric-measure-theory', 'littlewood-paley-theory']"
82,Every weakly convergent sequence is bounded,Every weakly convergent sequence is bounded,,"Theorem: Every weakly convergent sequence in X is bounded. Let $\{x_n\}$ be a weakly convergent sequence in X. Let $T_n \in X^{**}$ be defined by $T_n(\ell) = \ell(x_n)$ for all $\ell \in X^*$. Fix an $\ell \in X^*$. For any $n \in \mathbb{N}$, since the sequence $\{\ell(x_n)\}$ is convergent, $\{T_n(\ell)\}$ is a bounded set. By Uniform Boundedness Principle $ \sup_{n \in \mathbb{N}} \|x_n\| = \sup_{n \in \mathbb{N}} \|T_n\| < \infty,$ i.e. $\{x_n\}$ is bounded. My question is: why $ \sup_{n \in \mathbb{N}} \|x_n\| = \sup_{n \in \mathbb{N}} \|T_n\|$ ?","Theorem: Every weakly convergent sequence in X is bounded. Let $\{x_n\}$ be a weakly convergent sequence in X. Let $T_n \in X^{**}$ be defined by $T_n(\ell) = \ell(x_n)$ for all $\ell \in X^*$. Fix an $\ell \in X^*$. For any $n \in \mathbb{N}$, since the sequence $\{\ell(x_n)\}$ is convergent, $\{T_n(\ell)\}$ is a bounded set. By Uniform Boundedness Principle $ \sup_{n \in \mathbb{N}} \|x_n\| = \sup_{n \in \mathbb{N}} \|T_n\| < \infty,$ i.e. $\{x_n\}$ is bounded. My question is: why $ \sup_{n \in \mathbb{N}} \|x_n\| = \sup_{n \in \mathbb{N}} \|T_n\|$ ?",,['functional-analysis']
83,A natural proof of the Cauchy-Schwarz inequality,A natural proof of the Cauchy-Schwarz inequality,,"Most of the proofs of the Cauchy-Schwarz inequality on a pre-Hilbert space use a fact that if a quadratic polynomial with real coefficients takes positive values everywhere on the real line, then its discriminant is negative(e.g. Conway: A course in functional analysis). I think this is somewhat tricky. Moreover I often forget its proof when the pre-Hilbert space is defined over the field of complex numbers. Is there a more natural proof (hence it's easy to remember) which is based on a completely different idea?","Most of the proofs of the Cauchy-Schwarz inequality on a pre-Hilbert space use a fact that if a quadratic polynomial with real coefficients takes positive values everywhere on the real line, then its discriminant is negative(e.g. Conway: A course in functional analysis). I think this is somewhat tricky. Moreover I often forget its proof when the pre-Hilbert space is defined over the field of complex numbers. Is there a more natural proof (hence it's easy to remember) which is based on a completely different idea?",,"['functional-analysis', 'inequality']"
84,If every absolutely convergent series is convergent then $X$ is Banach,If every absolutely convergent series is convergent then  is Banach,X,"Show that A Normed Linear Space $X$ is a Banach Space iff every absolutely convergent series is convergent. My try : Let $X$ is a Banach Space .Let $\sum x_n$ be an absolutely convergent series .Consider $s_n=\sum_{i=1}^nx_i$. Now $\sum \|x_n\|<\infty \implies \exists N$ such that $\sum_{i=N}^ \infty \|x_i\|<\epsilon$ for any $\epsilon>0$ Then $\|s_n-s_m\|\le \sum _{i=m+1}^n \|x_i\|<\epsilon \forall n,m>N$ So $s_n$ is Cauchy in $X$ and hence converges $s_n\to s$ (say). Thus $\sum x_i$ converges. Conversely, let $x_n$ be a Cauchy Sequence in $X$. Here I can't proceed how to use the given fact. Any help will be great.","Show that A Normed Linear Space $X$ is a Banach Space iff every absolutely convergent series is convergent. My try : Let $X$ is a Banach Space .Let $\sum x_n$ be an absolutely convergent series .Consider $s_n=\sum_{i=1}^nx_i$. Now $\sum \|x_n\|<\infty \implies \exists N$ such that $\sum_{i=N}^ \infty \|x_i\|<\epsilon$ for any $\epsilon>0$ Then $\|s_n-s_m\|\le \sum _{i=m+1}^n \|x_i\|<\epsilon \forall n,m>N$ So $s_n$ is Cauchy in $X$ and hence converges $s_n\to s$ (say). Thus $\sum x_i$ converges. Conversely, let $x_n$ be a Cauchy Sequence in $X$. Here I can't proceed how to use the given fact. Any help will be great.",,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'absolute-convergence']"
85,"Is there no norm in $C^\infty ([a,b])$?",Is there no norm in ?,"C^\infty ([a,b])","Does anyone knows a reference, which proves the following: Let $a,b\in \mathbb{R}$ with $a<b$. There is no norm in the space $C^\infty([a,b])$, which makes it a Banach space.","Does anyone knows a reference, which proves the following: Let $a,b\in \mathbb{R}$ with $a<b$. There is no norm in the space $C^\infty([a,b])$, which makes it a Banach space.",,"['functional-analysis', 'reference-request']"
86,Weak-to-weak continuous operator which is not norm-continuous,Weak-to-weak continuous operator which is not norm-continuous,,"Can one give a ""relatively easy"" example of a linear mapping $T\colon X\to X$ ($X$ a Banach space) which is a) weak-to-weak continuous b) weak*-to-weak* continuous ($X=Y^*$) but not norm-to-norm continuous (not bounded). This needs some choice I guess.","Can one give a ""relatively easy"" example of a linear mapping $T\colon X\to X$ ($X$ a Banach space) which is a) weak-to-weak continuous b) weak*-to-weak* continuous ($X=Y^*$) but not norm-to-norm continuous (not bounded). This needs some choice I guess.",,"['functional-analysis', 'banach-spaces']"
87,How to prove that an operator is compact?,How to prove that an operator is compact?,,Consider $T\colon\ell^2\to\ell^2$ an operator such that  $Te_k=\lambda_k e_k$ with $\lambda_k\to 0$ as $k \to \infty$ how to prove that it is compact?,Consider $T\colon\ell^2\to\ell^2$ an operator such that  $Te_k=\lambda_k e_k$ with $\lambda_k\to 0$ as $k \to \infty$ how to prove that it is compact?,,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'compact-operators']"
88,Tensor products of functions generate dense subspace?,Tensor products of functions generate dense subspace?,,"Let $X$ and $Y$ be two spaces in certain category, $F(\cdot)$ a functor associating each space with a function space (with certain topology). Assume that for any $f\in F(X)$ and $g\in F(Y)$, $f\otimes g := f(x)g(y)\in F(X\times Y)$. Denote by $F(X)\otimes F(Y)$ the closed subspace of $F(X\times Y)$ generated by functions of the form $f\otimes g$. General Question :  When do we have $F(X\times Y)=F(X)\otimes F(Y)$? Applying Stone-Weierstrass theorem, we obtain an example: Theorem : Let $X$ and $Y$ be compact Hausdorff spaces, $C(X)$ and $C(Y)$ the space of continuous functions on X and Y respectively, then we have $C(X\times Y)=C(X)\otimes C(Y)$. As one can imagine, this type of results could be very useful in reducing multi-dimensional problems into one-dimensional problems. The following are two concrete questions asking for validity of the identity (and they are the true purpose of this post). Question I : $C^{\infty}_c(\mathbb{R}\times \mathbb{R})=C^{\infty}_c(\mathbb{R})\otimes C^{\infty}_c(\mathbb{R})$? and Question II : $\mathscr{S}(\mathbb{R}\times \mathbb{R})=\mathscr{S}(\mathbb{R})\otimes \mathscr{S}(\mathbb{R})$? Here $C^{\infty}_c(\mathbb{R})$ denotes the space of smooth functions with compact support, $\mathscr{S}(\mathbb{R})$ denotes the Schwartz space.","Let $X$ and $Y$ be two spaces in certain category, $F(\cdot)$ a functor associating each space with a function space (with certain topology). Assume that for any $f\in F(X)$ and $g\in F(Y)$, $f\otimes g := f(x)g(y)\in F(X\times Y)$. Denote by $F(X)\otimes F(Y)$ the closed subspace of $F(X\times Y)$ generated by functions of the form $f\otimes g$. General Question :  When do we have $F(X\times Y)=F(X)\otimes F(Y)$? Applying Stone-Weierstrass theorem, we obtain an example: Theorem : Let $X$ and $Y$ be compact Hausdorff spaces, $C(X)$ and $C(Y)$ the space of continuous functions on X and Y respectively, then we have $C(X\times Y)=C(X)\otimes C(Y)$. As one can imagine, this type of results could be very useful in reducing multi-dimensional problems into one-dimensional problems. The following are two concrete questions asking for validity of the identity (and they are the true purpose of this post). Question I : $C^{\infty}_c(\mathbb{R}\times \mathbb{R})=C^{\infty}_c(\mathbb{R})\otimes C^{\infty}_c(\mathbb{R})$? and Question II : $\mathscr{S}(\mathbb{R}\times \mathbb{R})=\mathscr{S}(\mathbb{R})\otimes \mathscr{S}(\mathbb{R})$? Here $C^{\infty}_c(\mathbb{R})$ denotes the space of smooth functions with compact support, $\mathscr{S}(\mathbb{R})$ denotes the Schwartz space.",,"['analysis', 'functional-analysis', 'fourier-analysis', 'approximation', 'distribution-theory']"
89,Dual space of the space of finite measures,Dual space of the space of finite measures,,"Since I am reading some stuff about weak convergence of probability measures, I started to wonder what is the dual space of the space consisting of all the finite (signed) measures (which is well known to be a Banach space with the norm being total variation). Is there any characterization of it? We may impose extra assumptions on the underlying space if necessary.","Since I am reading some stuff about weak convergence of probability measures, I started to wonder what is the dual space of the space consisting of all the finite (signed) measures (which is well known to be a Banach space with the norm being total variation). Is there any characterization of it? We may impose extra assumptions on the underlying space if necessary.",,"['measure-theory', 'functional-analysis', 'probability-theory', 'banach-spaces']"
90,On the vector spaces of Taylor Series and Fourier Series,On the vector spaces of Taylor Series and Fourier Series,,"Taylor series expansion of function, $f$, is a vector in the vector space with basis: $\{(x-a)^0, (x-a)^1, (x-a)^3, \ldots, (x-a)^n, \ldots\}$. This vector space has a countably infinite dimension. When $f$ is expressed as linear combination of the basis vector the scalar multiple for the $n$-th basis vector is $\operatorname{Diff}_n{f}(a)/n!$ Fourier series expansion of function, $f$, is a vector in the vector space with basis: $\{\sin(1x), \cos(1x), \sin(2x), \cos(2x), \ldots, \sin(nx), \cos(nx), \ldots\}$. This vector space has a countably infinite dimension. When $f$ is expressed as linear combination of the basis vector the scalar multiple for the $n$-th basis vectors are $\operatorname{Int}\{f\cdot\sin(nx)\}$ and $\operatorname{Int}\{f\cdot\cos(nx)\}$. Questions: The vector space for the Fourier series has an inner product, $\operatorname{Int}\{f\cdot g\}$, and it's this inner product that provides the above expressions like $\operatorname{Int}\{f\cdot\sin(nx)\}$ and $\operatorname{Int}\{f\cdot\cos(nx)\}$. Is there a similar inner product based derivation of the scalar multiples for the vector space of spanned by the polynomial basis in Taylor series? What is the relationship, if any, between the vector space produced by Taylor Series and that of Fourier Series? E.g. is one a subspace of the other? When Fourier series is taught, why isn't Taylor Series re-explained in the vector space framework used for Fourier series? And would this approach not lead the discussion of the implication of the choice of basis (and perhaps the choice of inner product) for function spaces? Just as Fourier series get generalized to Fourier Transform (the summation of the series becomes an integral), is there something equivalent to Taylor series? Are there any recommended resources (books, courses, etc.) available which can help clarify my thinking regarding these issues?","Taylor series expansion of function, $f$, is a vector in the vector space with basis: $\{(x-a)^0, (x-a)^1, (x-a)^3, \ldots, (x-a)^n, \ldots\}$. This vector space has a countably infinite dimension. When $f$ is expressed as linear combination of the basis vector the scalar multiple for the $n$-th basis vector is $\operatorname{Diff}_n{f}(a)/n!$ Fourier series expansion of function, $f$, is a vector in the vector space with basis: $\{\sin(1x), \cos(1x), \sin(2x), \cos(2x), \ldots, \sin(nx), \cos(nx), \ldots\}$. This vector space has a countably infinite dimension. When $f$ is expressed as linear combination of the basis vector the scalar multiple for the $n$-th basis vectors are $\operatorname{Int}\{f\cdot\sin(nx)\}$ and $\operatorname{Int}\{f\cdot\cos(nx)\}$. Questions: The vector space for the Fourier series has an inner product, $\operatorname{Int}\{f\cdot g\}$, and it's this inner product that provides the above expressions like $\operatorname{Int}\{f\cdot\sin(nx)\}$ and $\operatorname{Int}\{f\cdot\cos(nx)\}$. Is there a similar inner product based derivation of the scalar multiples for the vector space of spanned by the polynomial basis in Taylor series? What is the relationship, if any, between the vector space produced by Taylor Series and that of Fourier Series? E.g. is one a subspace of the other? When Fourier series is taught, why isn't Taylor Series re-explained in the vector space framework used for Fourier series? And would this approach not lead the discussion of the implication of the choice of basis (and perhaps the choice of inner product) for function spaces? Just as Fourier series get generalized to Fourier Transform (the summation of the series becomes an integral), is there something equivalent to Taylor series? Are there any recommended resources (books, courses, etc.) available which can help clarify my thinking regarding these issues?",,"['functional-analysis', 'vector-spaces', 'fourier-analysis', 'taylor-expansion', 'fourier-series']"
91,"Two possible definitions of ""vector-valued distribution""","Two possible definitions of ""vector-valued distribution""",,"Let $X$ be a reflexive real Banach space, the complex case should be totally analogous. Define $$\tag{1} \mathcal{D}^\star(0, T; X)=\left\{ u\colon \mathcal{D}(0, T)\to X\ \text{linear and continuous}\right\} $$ where the topology on $\mathcal{D}(0, T)$ , the space of real-valued test functions, is the usual one from distribution theory. Now define $$ \tag{2} \left[\mathcal{D}(0, T; X^\star)\right]^\star = \left\{u \colon \mathcal{D}(0, T; X^\star)\to \mathbb{R}\ \text{linear and continuous}\right\},  $$ where $\mathcal{D}(0, T; X^\star)$ denotes the space of the smooth $f\colon (0, T)\to X^\star$ such that the support $\operatorname*{Supp}(f)$ is compact. We equip this vector space with the obvious analogue of the topology of $\mathcal{D}(0, T)$ . Precisely, we consider the unique topology $^{[1]}$ such that, if $\phi_n, \phi\in \mathcal{D}(0, T; X^\star)$ then $\phi_n\to \phi$ is equivalent to $$ \begin{cases} \operatorname*{Supp}\phi_n \subset [a, b]\subset (0, T),\ \text{for fixed }a,b;\\  \left\lVert \frac{d^k \phi_n}{dx^k}-\frac{d^k\phi}{dx^k} \right\rVert_{\infty} \to 0,\quad\forall k\in \mathbb{N}. \end{cases} $$ Both definitions give rise to something which might be reasonably called ""space of $X$ -valued distributions"". Question . Are these two spaces isomorphic? Example . Let $X=\mathbb{R}^n$ and consider a continuous function $\boldsymbol{u}\colon (0, T)\to \mathbb{R}^n$ . (The boldface font refers to vector valued functions). The two definitions above give rise to the following two representations of $\boldsymbol u$ as a vector valued distribution. Using definition (1) $$ \boldsymbol{u}\text{ acts on }\mathcal{D}(0, T)\text{ through the pairing }\langle \boldsymbol{u}, \phi\rangle = \int_0^T \boldsymbol{u}(t)\phi(t)\, dt,\text{ where }\phi\in \mathcal{D}(0, T).$$ Note that the test function $\phi$ is scalar-valued. On the other hand, using definition (2) $$ \boldsymbol{u}\text{ acts on }\mathcal{D}(0, T; \mathbb{R}^n)\text{ through the pairing }\langle \boldsymbol{u}, \boldsymbol{\psi}\rangle = \int_0^T \boldsymbol{u}(t)\cdot \boldsymbol \psi(t)\, dt,\text{ where }\boldsymbol\psi\in \mathcal{D}(0, T; \mathbb{R}^n).$$ Here the test function is vector-valued and the pairing uses the dot product of $\mathbb{R}^n$ . Note. From some lecture notes which I found online it seems that Laurent Schwartz himself chose definition (1). $^{[1]}$ Actually, I am cheating here. I know neither if such a topology exists nor if it is unique. I am just guessing that the usual construction which works for real valued test functions works here as well.","Let be a reflexive real Banach space, the complex case should be totally analogous. Define where the topology on , the space of real-valued test functions, is the usual one from distribution theory. Now define where denotes the space of the smooth such that the support is compact. We equip this vector space with the obvious analogue of the topology of . Precisely, we consider the unique topology such that, if then is equivalent to Both definitions give rise to something which might be reasonably called ""space of -valued distributions"". Question . Are these two spaces isomorphic? Example . Let and consider a continuous function . (The boldface font refers to vector valued functions). The two definitions above give rise to the following two representations of as a vector valued distribution. Using definition (1) Note that the test function is scalar-valued. On the other hand, using definition (2) Here the test function is vector-valued and the pairing uses the dot product of . Note. From some lecture notes which I found online it seems that Laurent Schwartz himself chose definition (1). Actually, I am cheating here. I know neither if such a topology exists nor if it is unique. I am just guessing that the usual construction which works for real valued test functions works here as well.","X \tag{1}
\mathcal{D}^\star(0, T; X)=\left\{ u\colon \mathcal{D}(0, T)\to X\ \text{linear and continuous}\right\}
 \mathcal{D}(0, T) 
\tag{2}
\left[\mathcal{D}(0, T; X^\star)\right]^\star = \left\{u \colon \mathcal{D}(0, T; X^\star)\to \mathbb{R}\ \text{linear and continuous}\right\}, 
 \mathcal{D}(0, T; X^\star) f\colon (0, T)\to X^\star \operatorname*{Supp}(f) \mathcal{D}(0, T) ^{[1]} \phi_n, \phi\in \mathcal{D}(0, T; X^\star) \phi_n\to \phi 
\begin{cases}
\operatorname*{Supp}\phi_n \subset [a, b]\subset (0, T),\ \text{for fixed }a,b;\\ 
\left\lVert \frac{d^k \phi_n}{dx^k}-\frac{d^k\phi}{dx^k} \right\rVert_{\infty} \to 0,\quad\forall k\in \mathbb{N}.
\end{cases}
 X X=\mathbb{R}^n \boldsymbol{u}\colon (0, T)\to \mathbb{R}^n \boldsymbol u 
\boldsymbol{u}\text{ acts on }\mathcal{D}(0, T)\text{ through the pairing }\langle \boldsymbol{u}, \phi\rangle = \int_0^T \boldsymbol{u}(t)\phi(t)\, dt,\text{ where }\phi\in \mathcal{D}(0, T). \phi 
\boldsymbol{u}\text{ acts on }\mathcal{D}(0, T; \mathbb{R}^n)\text{ through the pairing }\langle \boldsymbol{u}, \boldsymbol{\psi}\rangle = \int_0^T \boldsymbol{u}(t)\cdot \boldsymbol \psi(t)\, dt,\text{ where }\boldsymbol\psi\in \mathcal{D}(0, T; \mathbb{R}^n). \mathbb{R}^n ^{[1]}","['functional-analysis', 'distribution-theory']"
92,Why is this a first integral? - particle near Schwarzschild black hole,Why is this a first integral? - particle near Schwarzschild black hole,,"Background I know that the Schwarzschild metric is: $$d s^{2}=c^{2}\left(1-\frac{2 \mu}{r}\right) d t^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} d r^{2}-r^{2} d \Omega^{2}$$ I know that if I divide by $d \lambda^2$ , I obtain the Lagrangian: $$ L=c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\theta}^{2}-r^{2} \sin ^{2} \theta \dot{\phi}^{2} $$ (where we have also expanded $\Omega^{2}$ into $\theta$ and $\phi$ dependent parts but that's not tha main point). Overdots denote differentiation with respect to affine parameter $\lambda$ . The Euler-Lagrange equations are: $$\frac{\partial L}{\partial x^{\mu}}=\frac{d}{d \lambda}\left(\frac{\partial L}{\partial \dot{x}^{\mu}}\right)$$ Which is, for $x^{\mu}=r$ , $\theta=\pi/2$ , results in: $$\left(1-\frac{2 \mu}{r}\right)^{-1} \ddot{r}+\frac{\mu c^{2}}{r^{2}} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-2} \frac{\mu}{r^{2}} \dot{r}^{2}-r \dot{\phi}^{2}=0$$ Lets set $\theta=\pi/2$ for the remainder of this post. The problem I am happy with everything up to this point. Now my notes say: However, it is often more convenient to use a further first integral of the motion, which follows directly from $L = c^2$ for a massive particle, and $L = 0$ for a massless one: $$ \left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc} c^{2} & \text { massive } \\ 0 & \text { massless } \end{array}\right. $$ Why is this called a first integral? Isn't this just the Lagrangian? My notes from another course has this to say on first integrals: When $L\left(y(\lambda), y^{\prime}(\lambda) ; \lambda\right)$ has no explicit dependence on $\lambda$ , i.e. when $\frac{\partial L}{\partial \lambda}=0,$ then we have the first integral $$ \dot{y} \frac{\partial L}{\partial \dot{y}}-L=\mathrm{const.} $$ So why does the above quote claim that the Lagrangian itself is the first integral? and why not $\dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.}$ is my first integral? Attempted resolution Let's calculate $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ , in the hope that it might reveal that $\dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.}$ and $ L=\left\{\begin{array}{lc} c^{2} & \text { massive } \\ 0 & \text { massless } \end{array}\right. $ is the same thing put in a different way. $\frac{\partial L}{\partial \dot{r}}=-2\left(1-\frac{2 V}{r}\right)^{-1} \dot{r}$ Then $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ becomes: $$-\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}+r^{2} \dot{\phi}^{2}=\operatorname{const}$$ Flip signs, then, compare the two expressions: $$\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const}$$ $$ \left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc} c^{2} & \text { massive } \\ 0 & \text { massless } \end{array}\right. $$ We can see that some signs differ if I believe that the first integral is $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ and not $L$ itself. I am pretty sure though that the result I get using $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ is wrong, since we use the other result throughout the lecture notes and it seem to be working. I am mostly happy with the relation: $$ \left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc} c^{2} & \text { massive } \\ 0 & \text { massless } \end{array}\right. $$ This is true if the affine parameter is proper time and the particle is massive. (Then $ds^2=c^2d\tau^2$ , so $ds^2/d\tau^2 = c^2$ .) If the affine parameter cannot be proper time, then the particle travels with the $c$ and therefore it is a photon, which has null-like path, making $ds^2$ zero. I can make the leap of faith that if this is true for proper time as affine parameter it is true for non-proper time affine parameters. I am also happy with the relation: $$\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const}$$ because the derivation seems correct. Question reapproached What I am not happy with is calling the first relation a first integral. It is probably rightly called that, an exam question (PDF page 24, third paragraph from bottom) asking for (I think) that equation saying ""[...] use a simpler expression given by the first integral of the geodesic equations."" So I think there is something here which I don't get. Checking algebra of Othin's answer As suggested, lets calculate $\dot{t}\frac{\partial L}{\partial \dot{t}} - L=\operatorname{const}$ . $$\frac{\partial L}{\partial t}=2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t}$$ Then $$\dot{t}\frac{\partial L}{\partial \dot{t}} - L = \dot{t} 2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t} - \left(c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}\right)=\operatorname{const}$$ ie $$c^{2}\left(1-\frac{2 H}{r}\right) \dot{t}^2 \bbox[5px,border:3px solid green]{+} \left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2} \bbox[5px,border:3px solid green]{+} r^{2} \dot{\phi}^{2}=\operatorname{const}$$ Which is not $L$ , but close. (Signs are wrong.)","Background I know that the Schwarzschild metric is: I know that if I divide by , I obtain the Lagrangian: (where we have also expanded into and dependent parts but that's not tha main point). Overdots denote differentiation with respect to affine parameter . The Euler-Lagrange equations are: Which is, for , , results in: Lets set for the remainder of this post. The problem I am happy with everything up to this point. Now my notes say: However, it is often more convenient to use a further first integral of the motion, which follows directly from for a massive particle, and for a massless one: Why is this called a first integral? Isn't this just the Lagrangian? My notes from another course has this to say on first integrals: When has no explicit dependence on , i.e. when then we have the first integral So why does the above quote claim that the Lagrangian itself is the first integral? and why not is my first integral? Attempted resolution Let's calculate , in the hope that it might reveal that and is the same thing put in a different way. Then becomes: Flip signs, then, compare the two expressions: We can see that some signs differ if I believe that the first integral is and not itself. I am pretty sure though that the result I get using is wrong, since we use the other result throughout the lecture notes and it seem to be working. I am mostly happy with the relation: This is true if the affine parameter is proper time and the particle is massive. (Then , so .) If the affine parameter cannot be proper time, then the particle travels with the and therefore it is a photon, which has null-like path, making zero. I can make the leap of faith that if this is true for proper time as affine parameter it is true for non-proper time affine parameters. I am also happy with the relation: because the derivation seems correct. Question reapproached What I am not happy with is calling the first relation a first integral. It is probably rightly called that, an exam question (PDF page 24, third paragraph from bottom) asking for (I think) that equation saying ""[...] use a simpler expression given by the first integral of the geodesic equations."" So I think there is something here which I don't get. Checking algebra of Othin's answer As suggested, lets calculate . Then ie Which is not , but close. (Signs are wrong.)","d s^{2}=c^{2}\left(1-\frac{2 \mu}{r}\right) d t^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} d r^{2}-r^{2} d \Omega^{2} d \lambda^2 
L=c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\theta}^{2}-r^{2} \sin ^{2} \theta \dot{\phi}^{2}
 \Omega^{2} \theta \phi \lambda \frac{\partial L}{\partial x^{\mu}}=\frac{d}{d \lambda}\left(\frac{\partial L}{\partial \dot{x}^{\mu}}\right) x^{\mu}=r \theta=\pi/2 \left(1-\frac{2 \mu}{r}\right)^{-1} \ddot{r}+\frac{\mu c^{2}}{r^{2}} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-2} \frac{\mu}{r^{2}} \dot{r}^{2}-r \dot{\phi}^{2}=0 \theta=\pi/2 L = c^2 L = 0 
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
 L\left(y(\lambda), y^{\prime}(\lambda) ; \lambda\right) \lambda \frac{\partial L}{\partial \lambda}=0, 
\dot{y} \frac{\partial L}{\partial \dot{y}}-L=\mathrm{const.}
 \dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.} \dot{r} \frac{\partial L}{\partial \dot{r}}-L \dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.} 
L=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
 \frac{\partial L}{\partial \dot{r}}=-2\left(1-\frac{2 V}{r}\right)^{-1} \dot{r} \dot{r} \frac{\partial L}{\partial \dot{r}}-L -\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}+r^{2} \dot{\phi}^{2}=\operatorname{const} \left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const} 
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
 \dot{r} \frac{\partial L}{\partial \dot{r}}-L L \dot{r} \frac{\partial L}{\partial \dot{r}}-L 
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
 ds^2=c^2d\tau^2 ds^2/d\tau^2 = c^2 c ds^2 \left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const} \dot{t}\frac{\partial L}{\partial \dot{t}} - L=\operatorname{const} \frac{\partial L}{\partial t}=2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t} \dot{t}\frac{\partial L}{\partial \dot{t}} - L = \dot{t} 2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t} - \left(c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}\right)=\operatorname{const} c^{2}\left(1-\frac{2 H}{r}\right) \dot{t}^2 \bbox[5px,border:3px solid green]{+} \left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2} \bbox[5px,border:3px solid green]{+} r^{2} \dot{\phi}^{2}=\operatorname{const} L","['functional-analysis', 'mathematical-physics', 'euler-lagrange-equation', 'general-relativity', 'mathematical-astronomy']"
93,"If $T$ is bounded and $F$ has finite rank, what is the spectrum of $T+F$?","If  is bounded and  has finite rank, what is the spectrum of ?",T F T+F,"Suppose that $T$ is a bounded operator with finite spectrum. What happens with the spectrum of $T+F$, where $F$ has finite rank? Is it possible that $\sigma(T+F)$ has non-empty interior? Is it always at most countable? Update : If $\sigma(T)=\{0\}$ then $0$ is in the essential spectrum of $T$ ($T$ is not invertible in the Calkin algebra),  hence for any compact $K$, $\sigma_{ess}(T+K)=\sigma_{ess}(T)=\{0\}$. For operators such that the essential spectrum is $\{0\}$, it is known that their spectrum is either finite or consists of a sequence converging to $0$. I think it should be the same for operators with finite spectrum, but I cannot find a proof or reference.","Suppose that $T$ is a bounded operator with finite spectrum. What happens with the spectrum of $T+F$, where $F$ has finite rank? Is it possible that $\sigma(T+F)$ has non-empty interior? Is it always at most countable? Update : If $\sigma(T)=\{0\}$ then $0$ is in the essential spectrum of $T$ ($T$ is not invertible in the Calkin algebra),  hence for any compact $K$, $\sigma_{ess}(T+K)=\sigma_{ess}(T)=\{0\}$. For operators such that the essential spectrum is $\{0\}$, it is known that their spectrum is either finite or consists of a sequence converging to $0$. I think it should be the same for operators with finite spectrum, but I cannot find a proof or reference.",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
94,Why is the numerical range of an operator convex?,Why is the numerical range of an operator convex?,,"Let $T$ be a Hilbert space operator. Its numerical range is \begin{equation} W(T)=\{\langle Tx,x\rangle:\|x\|=1\}.\end{equation} It is a well-known fact that $W(T)$ is a convex subset of the complex plane. However, every proof I know is by brute force computation. First for $2\times 2$ matrices, then the general case. Even though the computation can be carried out in clever ways, it still fails to provide some explanation why this is true. What is the link between this result and other concepts of the theory? I wonder whether there is any conceptual explanation for this result. I do not ask the explanation to be rigorous, just some ideas.","Let $T$ be a Hilbert space operator. Its numerical range is \begin{equation} W(T)=\{\langle Tx,x\rangle:\|x\|=1\}.\end{equation} It is a well-known fact that $W(T)$ is a convex subset of the complex plane. However, every proof I know is by brute force computation. First for $2\times 2$ matrices, then the general case. Even though the computation can be carried out in clever ways, it still fails to provide some explanation why this is true. What is the link between this result and other concepts of the theory? I wonder whether there is any conceptual explanation for this result. I do not ask the explanation to be rigorous, just some ideas.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'spectral-theory']"
95,Is there a chain rule for functional derivatives?,Is there a chain rule for functional derivatives?,,"Given a functional $S=S\{Y[X(r)]\}$, is the following ""chain rule"" valid? $$\frac{\delta S\{Y[X]\}}{\delta X(r)}=\frac{\partial Y(r)}{\partial X(r)}\frac{\delta S[Y]}{\delta Y(r)}$$","Given a functional $S=S\{Y[X(r)]\}$, is the following ""chain rule"" valid? $$\frac{\delta S\{Y[X]\}}{\delta X(r)}=\frac{\partial Y(r)}{\partial X(r)}\frac{\delta S[Y]}{\delta Y(r)}$$",,['functional-analysis']
96,Easy Proof Adjoint(Compact)=Compact,Easy Proof Adjoint(Compact)=Compact,,"I am looking for an easy proof that the adjoint of a compact operator on a Hilbert space is again compact. This makes the big characterization theorem for compact operators (i.e. compact iff image of unit ball is relatively compact iff image of unit ball is compact iff norm limit of finite rank operators) much easier to prove, provided that you have already developed spectral theory for C*-algebras. By the way, I'm using the definition that an operator $T\colon H \to H$ is compact if and only if given any [bounded] sequence of vectors $(x_n)$, the image sequence $(Tx_n)$ has a convergent subsequence. edited for bounded","I am looking for an easy proof that the adjoint of a compact operator on a Hilbert space is again compact. This makes the big characterization theorem for compact operators (i.e. compact iff image of unit ball is relatively compact iff image of unit ball is compact iff norm limit of finite rank operators) much easier to prove, provided that you have already developed spectral theory for C*-algebras. By the way, I'm using the definition that an operator $T\colon H \to H$ is compact if and only if given any [bounded] sequence of vectors $(x_n)$, the image sequence $(Tx_n)$ has a convergent subsequence. edited for bounded",,"['functional-analysis', 'operator-theory']"
97,Difference between Measurable and Borel Measurable function,Difference between Measurable and Borel Measurable function,,"Definition of measurable function :  If $X$ is measurable space, $Y$ is topological space, then $f:X\to Y$ is measurable provided that $f^{-1}(V)$ is measurable set in $X$ for every open set $V$ in $Y$. Definition of Borel measurable function : If $f:X\to Y$ is continuous mapping of $X$, where $Y$ is any topological space, $ (X,\mathfrak B)$ is measurable space and $f^{-1}(V)\in\mathfrak B$ for every open set $V$ in $Y$, then $f$ is Borel measurable function. Both functions are mapping from measurable space to topological space what's the difference between the two definition?","Definition of measurable function :  If $X$ is measurable space, $Y$ is topological space, then $f:X\to Y$ is measurable provided that $f^{-1}(V)$ is measurable set in $X$ for every open set $V$ in $Y$. Definition of Borel measurable function : If $f:X\to Y$ is continuous mapping of $X$, where $Y$ is any topological space, $ (X,\mathfrak B)$ is measurable space and $f^{-1}(V)\in\mathfrak B$ for every open set $V$ in $Y$, then $f$ is Borel measurable function. Both functions are mapping from measurable space to topological space what's the difference between the two definition?",,"['functional-analysis', 'measure-theory']"
98,What is a resolvent of an operator?,What is a resolvent of an operator?,,"What is a resolvent $R$ of an operator $L$ , and why do we care about it? $$R=(\lambda I-L)^{-1}.$$","What is a resolvent of an operator , and why do we care about it?",R L R=(\lambda I-L)^{-1}.,"['functional-analysis', 'terminology', 'spectral-theory']"
99,Isometric to Dual implies Hilbertable?,Isometric to Dual implies Hilbertable?,,"Let $X$ be a Banach space and suppose that $X$ is isometric to its continuous dual space $X^*$. Must $X$ be hilbertable in the sense that there exists an inner product which induces the norm on $X$? The converse of this statement is the Riesz representation theorem for hilbert spaces; I am wondering if the theorem can be stengthened to ""if and only if"".","Let $X$ be a Banach space and suppose that $X$ is isometric to its continuous dual space $X^*$. Must $X$ be hilbertable in the sense that there exists an inner product which induces the norm on $X$? The converse of this statement is the Riesz representation theorem for hilbert spaces; I am wondering if the theorem can be stengthened to ""if and only if"".",,"['functional-analysis', 'banach-spaces', 'examples-counterexamples', 'hilbert-spaces']"
