,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Understanding an aplication of Faà di Bruno's formula,Understanding an aplication of Faà di Bruno's formula,,"Let $g(x)=\sum_{n=0}^\infty a_n x^n$ . The main aim here is to get a closed form for the coefficients $b_n$ of $\frac{d \log{g}}{dx}$ . Faà di Bruno's formula tells us that $$\frac{d^n}{dx^n}f(g(x)) = \sum_{k=1}^n f^{(k)}(g(x))B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})$$ Where $B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})$ is a partial Bell polynomial. Moreover, we know that $$B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})=\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right )$$ Where $C_k$ is the set of compositions of $n$ and $\pi_k$ is the composition of $n$ in exactly $k$ parts. Taking $f=\log$ and $g$ our original power series and applying both Faà di Bruno's formula and Taylor's Theorem, and as we know that $f^{(r)}(g(x))=(-1)^{r-1}(r-1)!g(x)^{-r}$ we can get to the conclusion that: $$\frac{d^n \log{f}}{dx^n} = \sum_{k=1}^n (-1)^{k-1}(k-1)!g(0)^{-k}\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right ) $$ Then: $$b_n=\frac{d^n \log{f}}{dx^n}\frac{1}{n!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k \ {a_0}^{k}} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} a_{\lambda_j}$$ This can also be seen here and here (section a3, very similar). The problem comes when I try to work with this formula. As an example, let $g(x)=\sum_{n=0}x^n$ . Then, $$\frac{d \log{g(x)}}{dx} = \frac{\sum_{n=0}nx^{n-1}}{\sum_{n=0}x^n} = \sum_{n=0}x^n$$ So that the coefficients we want are $b_n=1 \ \forall n \in \mathbb{N}$ . On the other hand, applying our formula for, for example, $b_3$ , we would get: $$b_3=\frac{d^3 \log{f}}{dx^3}\frac{1}{3!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k} \sum_{\pi_k \in C_3} 1= 1+\left ( \frac{-1}{2} \right ) 2 + \frac{1}{3}=\frac{1}{3}$$ Since $C_3=\{\{3\},\{1,2\},\{2,1\},\{1,1,1\}\}$ . Then, what am I doing wrong here? Is there any problem when applying Faà di Bruno's Formula? Am I understanding well the concept of composition? Where is the flaw?","Let . The main aim here is to get a closed form for the coefficients of . Faà di Bruno's formula tells us that Where is a partial Bell polynomial. Moreover, we know that Where is the set of compositions of and is the composition of in exactly parts. Taking and our original power series and applying both Faà di Bruno's formula and Taylor's Theorem, and as we know that we can get to the conclusion that: Then: This can also be seen here and here (section a3, very similar). The problem comes when I try to work with this formula. As an example, let . Then, So that the coefficients we want are . On the other hand, applying our formula for, for example, , we would get: Since . Then, what am I doing wrong here? Is there any problem when applying Faà di Bruno's Formula? Am I understanding well the concept of composition? Where is the flaw?","g(x)=\sum_{n=0}^\infty a_n x^n b_n \frac{d \log{g}}{dx} \frac{d^n}{dx^n}f(g(x)) = \sum_{k=1}^n f^{(k)}(g(x))B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)}) B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)}) B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})=\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right ) C_k n \pi_k n k f=\log g f^{(r)}(g(x))=(-1)^{r-1}(r-1)!g(x)^{-r} \frac{d^n \log{f}}{dx^n} = \sum_{k=1}^n (-1)^{k-1}(k-1)!g(0)^{-k}\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right )  b_n=\frac{d^n \log{f}}{dx^n}\frac{1}{n!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k \ {a_0}^{k}} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} a_{\lambda_j} g(x)=\sum_{n=0}x^n \frac{d \log{g(x)}}{dx} = \frac{\sum_{n=0}nx^{n-1}}{\sum_{n=0}x^n} = \sum_{n=0}x^n b_n=1 \ \forall n \in \mathbb{N} b_3 b_3=\frac{d^3 \log{f}}{dx^3}\frac{1}{3!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k} \sum_{\pi_k \in C_3} 1= 1+\left ( \frac{-1}{2} \right ) 2 + \frac{1}{3}=\frac{1}{3} C_3=\{\{3\},\{1,2\},\{2,1\},\{1,1,1\}\}","['sequences-and-series', 'derivatives', 'power-series', 'taylor-expansion']"
1,"The meaning and definition of $\psi^{(-2)}(x)$, and the convergence of some related series involving the Möbius function","The meaning and definition of , and the convergence of some related series involving the Möbius function",\psi^{(-2)}(x),"While I was playing with a CAS I find that makes sense the function $$\psi^{(-k)}(x),$$ for example $\psi^{(-2)}(x)$, where $\psi^{(n)}(x)$ denotes the $n$th derivative of the digamma function, see this MathWorld . Question 1 (Answered see the comments). Can you explain what is the function $\psi^{(-2)}(x)$? I am asking about what is its definition. Many thanks. I think that maybe is a notation for the second antiderivative, but I would like to know a definition with rigor about what is previous function, and what is previous notation. As a puzzle I wondered if it is possible to deduce the convergence of some series involving previous functions, and the Möbius function $\mu(n)$, see the definition of this arithmetic function from this MathWorld . Question 2. Can you deduce the convergence of $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-1)}(n)}{n^3}\tag{1}$$ or $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-2)}(n)}{n^3}\,?\tag{2}$$ Many thanks. Only is required to prove the convergence of some example in previous Question 2, well the first or the second series.","While I was playing with a CAS I find that makes sense the function $$\psi^{(-k)}(x),$$ for example $\psi^{(-2)}(x)$, where $\psi^{(n)}(x)$ denotes the $n$th derivative of the digamma function, see this MathWorld . Question 1 (Answered see the comments). Can you explain what is the function $\psi^{(-2)}(x)$? I am asking about what is its definition. Many thanks. I think that maybe is a notation for the second antiderivative, but I would like to know a definition with rigor about what is previous function, and what is previous notation. As a puzzle I wondered if it is possible to deduce the convergence of some series involving previous functions, and the Möbius function $\mu(n)$, see the definition of this arithmetic function from this MathWorld . Question 2. Can you deduce the convergence of $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-1)}(n)}{n^3}\tag{1}$$ or $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-2)}(n)}{n^3}\,?\tag{2}$$ Many thanks. Only is required to prove the convergence of some example in previous Question 2, well the first or the second series.",,"['derivatives', 'convergence-divergence']"
2,"If $f''(x)$ exist in $(a, b)$ then there exist $\xi \in (a, b)$ so that $f''(\xi)(b-a)=f'(b)-f'(a)$",If  exist in  then there exist  so that,"f''(x) (a, b) \xi \in (a, b) f''(\xi)(b-a)=f'(b)-f'(a)","Let $f$ be differentiable on $[a, b]$ and $f'$ is differentiable on $(a, b)$. How can we prove that there exists $\xi \in (a, b)$, such that $f'(b)-f'(a)=f''(\xi)(b-a)$ ? We cannot use the mean value theorem because $f'(x)$ may have discontinuities at $a$ or $b$. I came up with this: If $f''(\xi_1)>\frac{f'(b)-f'(a)}{b-a}$ and $f''(\xi_2)<\frac{f'(b)-f'(a)}{b-a}$ for some $\xi_1, \xi_2$, then by Darboux's theorem there will be $\xi$, such that $f''(\xi)=\frac{f'(b)-f'(a)}{b-a}$. Therefore we can assume $f''(x)>\frac{f'(b)-f'(a)}{b-a}$ or $f''(x)<\frac{f'(b)-f'(a)}{b-a}$ for all $x$. Am I going the right direction?","Let $f$ be differentiable on $[a, b]$ and $f'$ is differentiable on $(a, b)$. How can we prove that there exists $\xi \in (a, b)$, such that $f'(b)-f'(a)=f''(\xi)(b-a)$ ? We cannot use the mean value theorem because $f'(x)$ may have discontinuities at $a$ or $b$. I came up with this: If $f''(\xi_1)>\frac{f'(b)-f'(a)}{b-a}$ and $f''(\xi_2)<\frac{f'(b)-f'(a)}{b-a}$ for some $\xi_1, \xi_2$, then by Darboux's theorem there will be $\xi$, such that $f''(\xi)=\frac{f'(b)-f'(a)}{b-a}$. Therefore we can assume $f''(x)>\frac{f'(b)-f'(a)}{b-a}$ or $f''(x)<\frac{f'(b)-f'(a)}{b-a}$ for all $x$. Am I going the right direction?",,"['real-analysis', 'derivatives']"
3,Is the derivative directional?,Is the derivative directional?,,"I was met with a surprising face when I assumed that a derivative is a directional change, i.e. that $$\frac{df(x)}{dx}$$ describes the change in $f(x)$ following an positive change in $x$. Moreover, the negative derivative describes the change in $f(x)$ following a negative change in $x$: $$-\frac{df(x)}{dx}$$ Am I mistaken?","I was met with a surprising face when I assumed that a derivative is a directional change, i.e. that $$\frac{df(x)}{dx}$$ describes the change in $f(x)$ following an positive change in $x$. Moreover, the negative derivative describes the change in $f(x)$ following a negative change in $x$: $$-\frac{df(x)}{dx}$$ Am I mistaken?",,"['calculus', 'functional-analysis', 'derivatives']"
4,Why the generalized derivatives defined? Why was it needed?,Why the generalized derivatives defined? Why was it needed?,,"While I was searching ""why the generalized derivatives defined?"", I saw the expression: ""An extension of the idea of a derivative to some classes of non-differentiable functions."" on https://www.encyclopediaofmath.org/index.php/Generalized_derivative . But I need more knowledge about the emergence of the generalized derivative (or in the sense of Sobolev). Why this type derivatives defined? Why was it needed? Can somebody recommend a source to me?","While I was searching ""why the generalized derivatives defined?"", I saw the expression: ""An extension of the idea of a derivative to some classes of non-differentiable functions."" on https://www.encyclopediaofmath.org/index.php/Generalized_derivative . But I need more knowledge about the emergence of the generalized derivative (or in the sense of Sobolev). Why this type derivatives defined? Why was it needed? Can somebody recommend a source to me?",,"['derivatives', 'sobolev-spaces', 'weak-derivatives']"
5,Subdifferential of $a^\text{T}x+\alpha\sqrt{x^\text{T}Bx}$ at $x=0$,Subdifferential of  at,a^\text{T}x+\alpha\sqrt{x^\text{T}Bx} x=0,"I would like to compute the subdifferential of the function $$ f(x)=a^\text{T}x+\alpha\sqrt{x^\text{T}Bx} $$ where $\alpha>0$ and $B$ is symmetric positive definite. Attempted Solution (I am brand new to subdifferentiability) Since subderivatives, like normal derivatives, are linear, we can compute term-by-term. Since the first term is differentiable, we're really interested in computing the subdifferential of $$ \sqrt{x^\text{T}Bx} $$ at $x=0$. This has been asked in the question here , and I gather that the subdifferential should be $$ \{z:\sqrt{z^\text{T}{B}z}\leqslant\lambda_\text{min}\} $$ where $\lambda_\text{min}$ is the smallest eigenvalue of $B$. All together, the subdifferential is $$ \{a+\alpha z:\sqrt{z^\text{T}Bz}\leqslant\lambda_\text{min}\} $$ Is this correct? If so, can anyone elucidate why the subdifferential of $\sqrt{z^\text{T}Bz}$ is what it is? The linked question confused me.","I would like to compute the subdifferential of the function $$ f(x)=a^\text{T}x+\alpha\sqrt{x^\text{T}Bx} $$ where $\alpha>0$ and $B$ is symmetric positive definite. Attempted Solution (I am brand new to subdifferentiability) Since subderivatives, like normal derivatives, are linear, we can compute term-by-term. Since the first term is differentiable, we're really interested in computing the subdifferential of $$ \sqrt{x^\text{T}Bx} $$ at $x=0$. This has been asked in the question here , and I gather that the subdifferential should be $$ \{z:\sqrt{z^\text{T}{B}z}\leqslant\lambda_\text{min}\} $$ where $\lambda_\text{min}$ is the smallest eigenvalue of $B$. All together, the subdifferential is $$ \{a+\alpha z:\sqrt{z^\text{T}Bz}\leqslant\lambda_\text{min}\} $$ Is this correct? If so, can anyone elucidate why the subdifferential of $\sqrt{z^\text{T}Bz}$ is what it is? The linked question confused me.",,"['derivatives', 'convex-analysis']"
6,Prove a function is differentiable with chain rule,Prove a function is differentiable with chain rule,,"Let $A$ be an open set of $\mathbb{R}^n$ and let $f : A \rightarrow \mathbb{R}^m$. Fix $u \in \mathbb{R}^m$ and define $g : A \rightarrow \mathbb{R}$ by $g(x) = f(x) \bullet u$ for all $x \in A$, where $\bullet$ is the inner product. Show that if $f$ is differentiable on $A$ then $g$ is differentiable on $A$ with derivative $g'(a) = f'(a) \bullet u$. I am trying to prove this with the chain rule. If $h : \mathbb{R}^m \rightarrow \mathbb{R}$ is defined by $h(x) = x \bullet u$ then $h$ is differentiable, so $h \circ f = g$ is differentiable. I am having a hard time showing that $g'(a) = f'(a) \bullet u$.","Let $A$ be an open set of $\mathbb{R}^n$ and let $f : A \rightarrow \mathbb{R}^m$. Fix $u \in \mathbb{R}^m$ and define $g : A \rightarrow \mathbb{R}$ by $g(x) = f(x) \bullet u$ for all $x \in A$, where $\bullet$ is the inner product. Show that if $f$ is differentiable on $A$ then $g$ is differentiable on $A$ with derivative $g'(a) = f'(a) \bullet u$. I am trying to prove this with the chain rule. If $h : \mathbb{R}^m \rightarrow \mathbb{R}$ is defined by $h(x) = x \bullet u$ then $h$ is differentiable, so $h \circ f = g$ is differentiable. I am having a hard time showing that $g'(a) = f'(a) \bullet u$.",,"['real-analysis', 'derivatives', 'chain-rule']"
7,"Prove that $f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]$ is monotonic increasing",Prove that  is monotonic increasing,"f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]","I would like to show that $f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]$ is monotonic increasing. If we can show that $f'(x)>0$, $\forall x\in [0,\frac{\pi}{2}]$, then  $f(x)$ is increasing there. We have $f'(x)=\sin 2x-2x\cos x+x^2\sin x$. How can I show that $f'(x)>0, \forall x\in [0,\frac{\pi}{2}]$","I would like to show that $f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]$ is monotonic increasing. If we can show that $f'(x)>0$, $\forall x\in [0,\frac{\pi}{2}]$, then  $f(x)$ is increasing there. We have $f'(x)=\sin 2x-2x\cos x+x^2\sin x$. How can I show that $f'(x)>0, \forall x\in [0,\frac{\pi}{2}]$",,"['calculus', 'derivatives', 'monotone-functions']"
8,Inverse trigonometic functions [duplicate],Inverse trigonometic functions [duplicate],,This question already has answers here : Simplify $\arctan (\frac{1}{2}\tan (2A)) + \arctan (\cot (A)) + \arctan (\cot ^{3}(A)) $ (2 answers) Closed 6 years ago . Question: Prove that $\tan^{-1}(\frac{1}{2} \tan2A)+\tan^{-1}(\cot A)+\tan^{-1}(\cot^{3} A)=0$ MyProblem We can just use the formula of $\tan^{-1} A +\tan^{-1} B$ but I think it would be a waste of time. Is there any other shorter and simpler method to solve it.,This question already has answers here : Simplify $\arctan (\frac{1}{2}\tan (2A)) + \arctan (\cot (A)) + \arctan (\cot ^{3}(A)) $ (2 answers) Closed 6 years ago . Question: Prove that $\tan^{-1}(\frac{1}{2} \tan2A)+\tan^{-1}(\cot A)+\tan^{-1}(\cot^{3} A)=0$ MyProblem We can just use the formula of $\tan^{-1} A +\tan^{-1} B$ but I think it would be a waste of time. Is there any other shorter and simpler method to solve it.,,"['real-analysis', 'trigonometry']"
9,How do you write log to the base e?,How do you write log to the base e?,,"I was given a question, find $f'(1)$ of $f(x) = \ln \sqrt{2-x}$. So I wrote $$1/2 \ln (2-x)^{(-1/2)(-1)} = -1/2 \ln (2-x)^{-1/2}$$ $$= -(1/2\ln)/\sqrt{2-x}$$ But when I sub in $x = 1$ I get a SYNTAX error, I realised log base e cannot be put in my calculator. I don't know how to put this into my calculator, can anyone help? Thanks!!","I was given a question, find $f'(1)$ of $f(x) = \ln \sqrt{2-x}$. So I wrote $$1/2 \ln (2-x)^{(-1/2)(-1)} = -1/2 \ln (2-x)^{-1/2}$$ $$= -(1/2\ln)/\sqrt{2-x}$$ But when I sub in $x = 1$ I get a SYNTAX error, I realised log base e cannot be put in my calculator. I don't know how to put this into my calculator, can anyone help? Thanks!!",,"['derivatives', 'logarithms']"
10,Question about a technique used to find extremum of a polynomial function,Question about a technique used to find extremum of a polynomial function,,"Someone told me to find extremum of a function in form of $f(x) = (x-a)^n (x-b)^m$ we can use this method: (this a method so that we can find the extremum without using the derivative) ($n$ and $m$ are not necessarily natural numbers and can be any real number) $$x_{ext} = \frac{ma+nb}{m+n}$$ i.e. we calculated the weighted mean of roots where their weights are the degree of the other term. I want to know that does this method have any name for example name of a mathematician or something else? And I also want to know can this method  be somehow generalized into polynomial functions with more than two roots i.e. $(x-a)^n(x-b)^m(x-c)^p ...$ ? If yes, how?","Someone told me to find extremum of a function in form of $f(x) = (x-a)^n (x-b)^m$ we can use this method: (this a method so that we can find the extremum without using the derivative) ($n$ and $m$ are not necessarily natural numbers and can be any real number) $$x_{ext} = \frac{ma+nb}{m+n}$$ i.e. we calculated the weighted mean of roots where their weights are the degree of the other term. I want to know that does this method have any name for example name of a mathematician or something else? And I also want to know can this method  be somehow generalized into polynomial functions with more than two roots i.e. $(x-a)^n(x-b)^m(x-c)^p ...$ ? If yes, how?",,"['calculus', 'derivatives', 'polynomials', 'maxima-minima']"
11,"$|f(x)-f(y)|≤(x-y)^2$ for all real $x,y$. What can we say about $f$?",for all real . What can we say about ?,"|f(x)-f(y)|≤(x-y)^2 x,y f","I can see that the absolute value of the difference quotient (for $y≠x$) is always less than $|x-y|$. From this I conclude that the limit of the difference quotient at $y$ i.e., the derivative at any point $y$ is $0$. So the function $f$ is a constant function. Is my math correct?","I can see that the absolute value of the difference quotient (for $y≠x$) is always less than $|x-y|$. From this I conclude that the limit of the difference quotient at $y$ i.e., the derivative at any point $y$ is $0$. So the function $f$ is a constant function. Is my math correct?",,['derivatives']
12,"Taking the derivative of a product, which is made up of two quotients","Taking the derivative of a product, which is made up of two quotients",,"I'm working on a calculus question, where we're asked to find $g'(z)$ for $$g(z)=\frac{z^2-2z-8}{z-3}\cdot\frac{z^2-9}{z-4}$$ So I was thinking that first you have to use the quotient rule on each side individually and then multiply them, but I seem to keep getting stuck and it's becoming pretty frustrating.","I'm working on a calculus question, where we're asked to find $g'(z)$ for $$g(z)=\frac{z^2-2z-8}{z-3}\cdot\frac{z^2-9}{z-4}$$ So I was thinking that first you have to use the quotient rule on each side individually and then multiply them, but I seem to keep getting stuck and it's becoming pretty frustrating.",,"['calculus', 'derivatives']"
13,Finding the derivative of a function using its series expansion,Finding the derivative of a function using its series expansion,,"I recently came across a proof which stated that the derivative of $\sin{x}$ is $\cos{x}$ because if we differentiate the Maclaurin series expansion of $\sin x$, we get the series expansion of $\cos x$. Is this a valid proof? My doubt is that the Maclaurin series expansion itself is defined based on the derivative of the function.  The following expansion is the Maclaurin series where $a=0$ and $f^{(n)}$ denotes the $n^{th}$ derivative of the function. $$\sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} \, (x-a)^{n} $$ We can clearly see that we need the derivative in the first place before we can get the expansion. Is the proof valid?","I recently came across a proof which stated that the derivative of $\sin{x}$ is $\cos{x}$ because if we differentiate the Maclaurin series expansion of $\sin x$, we get the series expansion of $\cos x$. Is this a valid proof? My doubt is that the Maclaurin series expansion itself is defined based on the derivative of the function.  The following expansion is the Maclaurin series where $a=0$ and $f^{(n)}$ denotes the $n^{th}$ derivative of the function. $$\sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} \, (x-a)^{n} $$ We can clearly see that we need the derivative in the first place before we can get the expansion. Is the proof valid?",,"['calculus', 'derivatives', 'taylor-expansion']"
14,"If $\sin (y)=x\sin (a+y)$, prove that:","If , prove that:",\sin (y)=x\sin (a+y),"If $\sin (y)=x\sin (a+y)$, prove that: $$\dfrac {dy}{dx}=\dfrac {\sin^2 (a+y)}{\sin (a)}$$ My Attempt: $$\sin (y)=x\sin (a+y)$$ $$\dfrac {d}{dy} \sin (y)=\dfrac {d}{dy} (x\sin (a+y))$$ $$\cos (y)=x\cdot \cos (a+y)+\sin (a+y)\cdot\dfrac {dx}{dy}$$ $$\cos (y)-x\cdot\cos (a+y)=\sin (a+y)\cdot\dfrac {dx}{dy}$$","If $\sin (y)=x\sin (a+y)$, prove that: $$\dfrac {dy}{dx}=\dfrac {\sin^2 (a+y)}{\sin (a)}$$ My Attempt: $$\sin (y)=x\sin (a+y)$$ $$\dfrac {d}{dy} \sin (y)=\dfrac {d}{dy} (x\sin (a+y))$$ $$\cos (y)=x\cdot \cos (a+y)+\sin (a+y)\cdot\dfrac {dx}{dy}$$ $$\cos (y)-x\cdot\cos (a+y)=\sin (a+y)\cdot\dfrac {dx}{dy}$$",,"['calculus', 'derivatives']"
15,limit and derivative question,limit and derivative question,,"for $f(x)=x \log(e+\frac{1}{x}) (x>0)$ , answer this questions: 1.) $\alpha=\lim\limits_{x \to \infty} ⁡{\frac{f(x)}{x}}$, find $\alpha$ 2.) from result $\alpha$ from number 1, $\beta=\lim\limits_{x \to 0^+} ⁡{(f(x)-\alpha(x))}$ and $t=\frac{1}{x}$, $x\to \infty$ and $t\to 0^+$ find $\beta$ 3.) from result $\alpha$ and $\beta$ in number 1 and 2, find $\lim\limits_{x \to \infty} ⁡\frac{f(x)-(\alpha x+\beta)}{\frac{1}{x}}$ attempt: 1.)$\alpha=\lim\limits_{x \to \infty} ⁡{\frac{f(x)}{x}}$= $\frac{x \log(e+\frac{1}{x}) }{x}=\log(e+\frac{1}{x})$=$\log e$ 2.) $\beta=\lim\limits_{x \to \infty} ⁡{(f(x)-\alpha(x))}$, if $t=\frac{1}{x}$ , then $x\to\infty$ $t\to0$, change limit to $t$ $\beta=\lim\limits_{x \to \infty} ⁡{(f(x)-\alpha(x))}$=$\lim\limits_{x \to \infty} ⁡{x\log(e+\frac{1}{x})-x \log e}$ $=\lim\limits_{x \to 0} ⁡{\frac{1}{t}\ln(e+t)-\frac{1}{t}\ln e}$ =$$\beta=\lim\limits_{t \to 0}{\frac{\ln(\frac{e+t}{e})}{t}}=0$$ 3.) from $\alpha $ and $\beta$ in number 1 and 2 $\lim\limits_{x \to \infty} ⁡\frac{f(x)-(\alpha x+\beta)}{\frac{1}{x}}$ = $\lim\limits_{x \to \infty} ⁡\frac{x (\ln(e+1/x)-\ln e}{\frac{1}{x}}$ and change to $t=\frac{1}{x}$, $\lim\limits_{x \to 0} ⁡\frac{\ln(\frac{e+t}{e})}{t^2}$= $\infty$ however im not sure with this result, because i got $\infty $ in third one. is my attempt correct? thanks so much!!!","for $f(x)=x \log(e+\frac{1}{x}) (x>0)$ , answer this questions: 1.) $\alpha=\lim\limits_{x \to \infty} ⁡{\frac{f(x)}{x}}$, find $\alpha$ 2.) from result $\alpha$ from number 1, $\beta=\lim\limits_{x \to 0^+} ⁡{(f(x)-\alpha(x))}$ and $t=\frac{1}{x}$, $x\to \infty$ and $t\to 0^+$ find $\beta$ 3.) from result $\alpha$ and $\beta$ in number 1 and 2, find $\lim\limits_{x \to \infty} ⁡\frac{f(x)-(\alpha x+\beta)}{\frac{1}{x}}$ attempt: 1.)$\alpha=\lim\limits_{x \to \infty} ⁡{\frac{f(x)}{x}}$= $\frac{x \log(e+\frac{1}{x}) }{x}=\log(e+\frac{1}{x})$=$\log e$ 2.) $\beta=\lim\limits_{x \to \infty} ⁡{(f(x)-\alpha(x))}$, if $t=\frac{1}{x}$ , then $x\to\infty$ $t\to0$, change limit to $t$ $\beta=\lim\limits_{x \to \infty} ⁡{(f(x)-\alpha(x))}$=$\lim\limits_{x \to \infty} ⁡{x\log(e+\frac{1}{x})-x \log e}$ $=\lim\limits_{x \to 0} ⁡{\frac{1}{t}\ln(e+t)-\frac{1}{t}\ln e}$ =$$\beta=\lim\limits_{t \to 0}{\frac{\ln(\frac{e+t}{e})}{t}}=0$$ 3.) from $\alpha $ and $\beta$ in number 1 and 2 $\lim\limits_{x \to \infty} ⁡\frac{f(x)-(\alpha x+\beta)}{\frac{1}{x}}$ = $\lim\limits_{x \to \infty} ⁡\frac{x (\ln(e+1/x)-\ln e}{\frac{1}{x}}$ and change to $t=\frac{1}{x}$, $\lim\limits_{x \to 0} ⁡\frac{\ln(\frac{e+t}{e})}{t^2}$= $\infty$ however im not sure with this result, because i got $\infty $ in third one. is my attempt correct? thanks so much!!!",,"['integration', 'limits', 'derivatives']"
16,Derivative of a function $y(x)= \sum\limits_{n=1}^\infty nx^{-n}$,Derivative of a function,y(x)= \sum\limits_{n=1}^\infty nx^{-n},"Can anyone show and explain how to obtain the derivative of the following function,   $$y(x)= \sum_{n=1}^\infty nx^{-n}$$ which is, for $n=3$, equivalent with, $$y(x)=x^{-1}+2x^{-2}+3x^{-3}$$ now, what is the derivative of $x$ with respect to $y$, i.e. $\dfrac{dx}{dy}$ for $n=\infty$? Can this kind of problem be solved analytically?","Can anyone show and explain how to obtain the derivative of the following function,   $$y(x)= \sum_{n=1}^\infty nx^{-n}$$ which is, for $n=3$, equivalent with, $$y(x)=x^{-1}+2x^{-2}+3x^{-3}$$ now, what is the derivative of $x$ with respect to $y$, i.e. $\dfrac{dx}{dy}$ for $n=\infty$? Can this kind of problem be solved analytically?",,"['calculus', 'real-analysis', 'derivatives', 'geometric-series']"
17,Where is this function continuous/differentiable?,Where is this function continuous/differentiable?,,"Let $f: \mathbb R \to \mathbb R$ defined by $$f(x):= \begin{cases} x\sin x & x \in \mathbb Q \\ 0 & x\in \mathbb R \setminus \mathbb Q \end{cases}$$ In which $x\in \mathbb R$ is $f$ continuous (differentiable)? Looking at the graph the obvious assumption is that $f$ is continuous at $k\pi, k\in \mathbb Z$ and discontinuous elsewhere. Let $x\in \mathbb R \setminus \{k\pi, k\in \mathbb Z \}$. If $x$ is irrational: Let $a_n$ be a sequence of rational numbers converging to $x$. Then $$\lim_{n\to \infty} f(a_n) = a_n\sin(a_n) \neq 0,$$ but $x$ is irrational so $f(x) = 0$. Analogous proof for $x$ rational. Now if $x = k\pi$ for some $k\in \mathbb Z$, let $a_n$ be a sequence converging to $x$. We have  $$\left \lvert f(a_n) \right \rvert \leq \rvert a_n \sin a_n \lvert \to 0 = x\sin(x),$$ hence $f$ is continuous in $x$. But how to proof where $f$ is differentiable? I assume it is only in $0$, any solution?","Let $f: \mathbb R \to \mathbb R$ defined by $$f(x):= \begin{cases} x\sin x & x \in \mathbb Q \\ 0 & x\in \mathbb R \setminus \mathbb Q \end{cases}$$ In which $x\in \mathbb R$ is $f$ continuous (differentiable)? Looking at the graph the obvious assumption is that $f$ is continuous at $k\pi, k\in \mathbb Z$ and discontinuous elsewhere. Let $x\in \mathbb R \setminus \{k\pi, k\in \mathbb Z \}$. If $x$ is irrational: Let $a_n$ be a sequence of rational numbers converging to $x$. Then $$\lim_{n\to \infty} f(a_n) = a_n\sin(a_n) \neq 0,$$ but $x$ is irrational so $f(x) = 0$. Analogous proof for $x$ rational. Now if $x = k\pi$ for some $k\in \mathbb Z$, let $a_n$ be a sequence converging to $x$. We have  $$\left \lvert f(a_n) \right \rvert \leq \rvert a_n \sin a_n \lvert \to 0 = x\sin(x),$$ hence $f$ is continuous in $x$. But how to proof where $f$ is differentiable? I assume it is only in $0$, any solution?",,"['real-analysis', 'sequences-and-series', 'analysis', 'derivatives', 'real-numbers']"
18,Is it true for the symmetric matrices?,Is it true for the symmetric matrices?,,Suppose there are two symmetric matrices $A$ and $B$. Then can we write $$\frac{tr(AB)}{B}=A$$ Any help in this regard will be much appreciated. The reason for question: I am trying to understand an example from appendix A of convex optimization book (by Stephen Boyd). On page 642 it is written that $f(Z)\simeq f(X)+tr(X^{-1}(Z-X))$. Now the gradient of $f(X)$ is then shown to be $X^{-1}$. If I put $f(Z)$ in Equation A.4 then I have $$\frac{||f(Z)-f(X)-Df(X)(Z-X)||_{2}}{||Z-X||_2}=0$$ which results in $$tr(X^{-1}(Z-X))=Df(X)(Z-X)$$ Now if $tr(X^{-1}(Z-X))=X^{-1}(Z-X)$ then I can understand but I am not sure whether it is true or not. So any help in this regard will be much appreciated. Thanks in advance.,Suppose there are two symmetric matrices $A$ and $B$. Then can we write $$\frac{tr(AB)}{B}=A$$ Any help in this regard will be much appreciated. The reason for question: I am trying to understand an example from appendix A of convex optimization book (by Stephen Boyd). On page 642 it is written that $f(Z)\simeq f(X)+tr(X^{-1}(Z-X))$. Now the gradient of $f(X)$ is then shown to be $X^{-1}$. If I put $f(Z)$ in Equation A.4 then I have $$\frac{||f(Z)-f(X)-Df(X)(Z-X)||_{2}}{||Z-X||_2}=0$$ which results in $$tr(X^{-1}(Z-X))=Df(X)(Z-X)$$ Now if $tr(X^{-1}(Z-X))=X^{-1}(Z-X)$ then I can understand but I am not sure whether it is true or not. So any help in this regard will be much appreciated. Thanks in advance.,,"['matrices', 'derivatives', 'optimization']"
19,$n_{th}$ derivative of function?,derivative of function?,n_{th},"I have to take the nth derivative of a function (below).  The research paper, which I have taken it from calls it the composite function. Any idea how to find the solution? $$L_d(s) = exp\{-C_d s^{2/\alpha_N}\}$$ I can take as many derivates as I want manually, but cannot find the $nth$ derivate or any formula for it, For example see below: $$F' = -\dfrac{2C_dx^{\frac{2}{\alpha_N}-1}\mathrm{e}^{-C_dx^\frac{2}{\alpha_N}}}{\alpha_N}$$ $$F'' = \dfrac{2C_dx^{\frac{2}{\alpha_N}-2}\left(2C_dx^\frac{2}{\alpha_N}+\alpha_N-2\right)\mathrm{e}^{-ax^\frac{2}{\alpha_N}}}{\alpha_N^2}$$ Any guidance would be appreciated for $F^n$","I have to take the nth derivative of a function (below).  The research paper, which I have taken it from calls it the composite function. Any idea how to find the solution? $$L_d(s) = exp\{-C_d s^{2/\alpha_N}\}$$ I can take as many derivates as I want manually, but cannot find the $nth$ derivate or any formula for it, For example see below: $$F' = -\dfrac{2C_dx^{\frac{2}{\alpha_N}-1}\mathrm{e}^{-C_dx^\frac{2}{\alpha_N}}}{\alpha_N}$$ $$F'' = \dfrac{2C_dx^{\frac{2}{\alpha_N}-2}\left(2C_dx^\frac{2}{\alpha_N}+\alpha_N-2\right)\mathrm{e}^{-ax^\frac{2}{\alpha_N}}}{\alpha_N^2}$$ Any guidance would be appreciated for $F^n$",,['derivatives']
20,Can one apply LHopitals' rule to differentiable functions defined over the naturals?,Can one apply LHopitals' rule to differentiable functions defined over the naturals?,,"For e.g, if we have $\lim_{n \rightarrow \infty} \frac {f(n)}{g(n)}$= $\frac 00$, $f:\mathbb {R} \rightarrow \mathbb {R}$ and $g:\mathbb {R} \rightarrow \mathbb {R}$ (note that $f$,$g$ are defined on $R$ so the derivative makes sense)  so in essence, we're considering sequences.","For e.g, if we have $\lim_{n \rightarrow \infty} \frac {f(n)}{g(n)}$= $\frac 00$, $f:\mathbb {R} \rightarrow \mathbb {R}$ and $g:\mathbb {R} \rightarrow \mathbb {R}$ (note that $f$,$g$ are defined on $R$ so the derivative makes sense)  so in essence, we're considering sequences.",,"['calculus', 'real-analysis', 'derivatives', 'indeterminate-forms']"
21,Replace $X$ with $\mbox{diag}(x)$ in trace matrix derivative identity,Replace  with  in trace matrix derivative identity,X \mbox{diag}(x),"There is a scaler-by-matrix derivative identity: $$\frac{\partial}{\partial X}trace\left(AXBX'C\right)=B'X'A'C'+BX'CA$$ How does this change if instead I am trying to find $$\frac{\partial}{\partial x}trace\left(Adiag(x)Bdiag(x)'C\right)$$ where $x$ is a vector rather than a matrix. My thinking is that all I have to do is multiply the original identity by a vector of ones as that would be the derivative of $diag(x)$. However, I'm not sure how the chain rule interacts with traces. I ask as I am trying to calculate. $$\frac{\partial}{\partial w}trace\left(Ddiag(w)\Omega diag(w)D'\right)$$ where $w \mathbb{\in R^{N}}$, $D\mathbb{\in R^{M\times N}}$, and $\Omega\mathbb{\in R^{N\times N}}$. Also $\Omega$ can be assumed to be positive definite. This implies the result would be $$\left(2\Omega diag(w)D'D\right)e$$ where $e \mathbb{\in R^{N}}$ is a vector of ones.","There is a scaler-by-matrix derivative identity: $$\frac{\partial}{\partial X}trace\left(AXBX'C\right)=B'X'A'C'+BX'CA$$ How does this change if instead I am trying to find $$\frac{\partial}{\partial x}trace\left(Adiag(x)Bdiag(x)'C\right)$$ where $x$ is a vector rather than a matrix. My thinking is that all I have to do is multiply the original identity by a vector of ones as that would be the derivative of $diag(x)$. However, I'm not sure how the chain rule interacts with traces. I ask as I am trying to calculate. $$\frac{\partial}{\partial w}trace\left(Ddiag(w)\Omega diag(w)D'\right)$$ where $w \mathbb{\in R^{N}}$, $D\mathbb{\in R^{M\times N}}$, and $\Omega\mathbb{\in R^{N\times N}}$. Also $\Omega$ can be assumed to be positive definite. This implies the result would be $$\left(2\Omega diag(w)D'D\right)e$$ where $e \mathbb{\in R^{N}}$ is a vector of ones.",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
22,Show that $g \circ f$ is n times differentiable,Show that  is n times differentiable,g \circ f,"Let $D,E \subseteq \mathbb{R}$ and $f: D \to E, g: E \to \mathbb{R}$ be two $n$ times differentiable functions. Then $g \circ f: D \to \mathbb{R}$ is $n$ times differentiable. I was thinking about using induction over $n$. I know the base case with $n=1$ is true and assume it is true for $n$. But I struggle to show the induction step $(g \circ f)^{(n+1)} := \Big( (g \circ f)^{(n)} \Big)' = \ldots$ If possible I want to avoid using the concrete formula for the n-th chain rule because it is quite complicated.","Let $D,E \subseteq \mathbb{R}$ and $f: D \to E, g: E \to \mathbb{R}$ be two $n$ times differentiable functions. Then $g \circ f: D \to \mathbb{R}$ is $n$ times differentiable. I was thinking about using induction over $n$. I know the base case with $n=1$ is true and assume it is true for $n$. But I struggle to show the induction step $(g \circ f)^{(n+1)} := \Big( (g \circ f)^{(n)} \Big)' = \ldots$ If possible I want to avoid using the concrete formula for the n-th chain rule because it is quite complicated.",,"['real-analysis', 'derivatives', 'chain-rule']"
23,polar coordinate subtitution,polar coordinate subtitution,,"$$\int\int_{(x^2+y^2) \le1} \left(\frac{x^4}{a^4}+ \frac{y^4}{b^4} \right)dxdy$$ I try to solve this multivariable integration, by substitute $x=ar \sin \theta$ and $y=br \sin \theta$ to  $(\frac{x^4}{a^4}+ \frac{y^4}{b^4} )$ so it will produce $r^4 $ with Jacobin $abr$ but when I substitute to $x^2+y^2 \le1$ it won't produce $r^2$  $a^2r^2\sin^2 \theta+b^2r^2 \cos^2 \theta \le1$, so I cannot change it to polar coordinate. Is my approach wrong? Can someone give me hint too!","$$\int\int_{(x^2+y^2) \le1} \left(\frac{x^4}{a^4}+ \frac{y^4}{b^4} \right)dxdy$$ I try to solve this multivariable integration, by substitute $x=ar \sin \theta$ and $y=br \sin \theta$ to  $(\frac{x^4}{a^4}+ \frac{y^4}{b^4} )$ so it will produce $r^4 $ with Jacobin $abr$ but when I substitute to $x^2+y^2 \le1$ it won't produce $r^2$  $a^2r^2\sin^2 \theta+b^2r^2 \cos^2 \theta \le1$, so I cannot change it to polar coordinate. Is my approach wrong? Can someone give me hint too!",,"['integration', 'derivatives', 'polar-coordinates', 'substitution']"
24,What is the scalar derivative?,What is the scalar derivative?,,"I quote paragraph 2.5 of The Matrix Cookbook document: Assume $F(X)$ to be a differentiable function of each of the elements of $X$... $f(\cdot)$ is the scalar derivative of $F(\cdot)$ . $X$ is here a matrix. What is the scalar derivative ? It is not defined in this document and I have issues to find a definition using Mister Google. But the way, I'm puzzled by formula (100) of that document: $$\frac{\partial}{\partial X} \mathsf{Tr}(XA) = A^T$$ $X \mapsto {Tr}(XA)$ is a linear form defined on the matrices vector space and therefore it's derivative is itself everywhere $$\frac{\partial}{\partial X} \mathsf{Tr}(XA).H = \mathsf{Tr}(HA)$$ What is the link with $A^T$?","I quote paragraph 2.5 of The Matrix Cookbook document: Assume $F(X)$ to be a differentiable function of each of the elements of $X$... $f(\cdot)$ is the scalar derivative of $F(\cdot)$ . $X$ is here a matrix. What is the scalar derivative ? It is not defined in this document and I have issues to find a definition using Mister Google. But the way, I'm puzzled by formula (100) of that document: $$\frac{\partial}{\partial X} \mathsf{Tr}(XA) = A^T$$ $X \mapsto {Tr}(XA)$ is a linear form defined on the matrices vector space and therefore it's derivative is itself everywhere $$\frac{\partial}{\partial X} \mathsf{Tr}(XA).H = \mathsf{Tr}(HA)$$ What is the link with $A^T$?",,"['matrices', 'derivatives']"
25,Is gateaux derivative same as the directional derivative,Is gateaux derivative same as the directional derivative,,"In the question here , Directional derivative is given as $$D_{\mathbf v}\mathbf g(\mathbf p) := \lim_{\theta\to 0}\frac{\mathbf g(\mathbf p+\theta\mathbf v)-\mathbf g(\mathbf p)}{\theta} \\ \\ \text{Here v is a unit vector}$$ - which is the same one I studied in high school. However, I came across this Gateaux Derivative,  $$ \begin{align*} \lim_{ \theta \to 0} \frac{F[f+\theta h]- F[f]}{\theta} \end{align*} \\ \\ \text{Here h is just a vector}$$ These definitions are so similar. I really cannot appreciate the difference between them. Can someone help me understand this with some simple examples?  Thank you. P.S: I am just a beginner and I am not familiar with Banach Spaces and stuff.","In the question here , Directional derivative is given as $$D_{\mathbf v}\mathbf g(\mathbf p) := \lim_{\theta\to 0}\frac{\mathbf g(\mathbf p+\theta\mathbf v)-\mathbf g(\mathbf p)}{\theta} \\ \\ \text{Here v is a unit vector}$$ - which is the same one I studied in high school. However, I came across this Gateaux Derivative,  $$ \begin{align*} \lim_{ \theta \to 0} \frac{F[f+\theta h]- F[f]}{\theta} \end{align*} \\ \\ \text{Here h is just a vector}$$ These definitions are so similar. I really cannot appreciate the difference between them. Can someone help me understand this with some simple examples?  Thank you. P.S: I am just a beginner and I am not familiar with Banach Spaces and stuff.",,['derivatives']
26,"Find from first principle, the derivative of","Find from first principle, the derivative of",,"Find from first principle, the derivative of $$f(x)=\dfrac {ax+b}{\sqrt {x}}.$$ My Attempt: $$f(x)=\dfrac {ax+b}{\sqrt {x}}$$ $$f(x+\Delta x)=\dfrac {a(x+\Delta x)+b}{\sqrt {x+\Delta x}}$$ where $\Delta x$ is a small increment in $x$. From first principle, $$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$ $$=\frac {\left(\frac {ax+a\Delta x+b}{\sqrt {x+\Delta x} - \frac {ax+b}{\sqrt {x}}}\right)}{\Delta x}$$","Find from first principle, the derivative of $$f(x)=\dfrac {ax+b}{\sqrt {x}}.$$ My Attempt: $$f(x)=\dfrac {ax+b}{\sqrt {x}}$$ $$f(x+\Delta x)=\dfrac {a(x+\Delta x)+b}{\sqrt {x+\Delta x}}$$ where $\Delta x$ is a small increment in $x$. From first principle, $$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$ $$=\frac {\left(\frac {ax+a\Delta x+b}{\sqrt {x+\Delta x} - \frac {ax+b}{\sqrt {x}}}\right)}{\Delta x}$$",,"['calculus', 'derivatives']"
27,Hessian of non-linear quadratic form,Hessian of non-linear quadratic form,,"I have a quadratic form like  $$ ( \boldsymbol x - f( \boldsymbol x ) )^T \boldsymbol A ( \boldsymbol x - f( \boldsymbol x ) )$$  with $f(\cdot): \mathbb{R}^n \rightarrow \mathbb{R}^m$. And I would like to compute the Hessian of it based on Jacobeans and higher order derivatives of $f(\cdot)$. I already found some hints (Section Basic properties) for the product rule, but couldn't figure out how to apply this further to the Hessian. Edit: My current approach: $z(x) = g(x)h(x)$, where $g(x) = (x-f)^T$ and $h(x)=A(x-f)$ The first derivative is then given by $$ D[z] = h^TD[g] + gD[h] $$ When using $u(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times p}$, $v(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times q}$ from here, page 4, second last equation $$ D[uv] = (v^T\otimes I_m) D[f] + (I_q\otimes u)D[v] $$ Where in my case we have $m=q=1, p=n$. Applying this to the first derivative above I get $$ D^2[z] = (D[g])^T D[h^T] + \{ I_n \otimes h^T \} D^2[g] + (D[h])^T D[g] + \{ I_n\otimes g \} D^2[h]\\\\ = \{ I_n- D[f^T] \}^T\{ I_n - D[f] \}^T A^T + \{ I_n \otimes (x-f)^TA^T \} D^2[g] + \{ I_n -D[f] \}^TA^T\{ I_n - D[f^T] \} + \{ I_n \otimes (x-f)^T \} D^2[h] $$ My questions are now basically? Is that correct? And how are $D[g]=-D^2[f^T]$ and $D^2[h]=-A\cdot{}D[D[f]^T]$ actually defined? Is the former like  $$-\begin{bmatrix} D^2[f_1] \\ \vdots  \\ D^2[f_n] \end{bmatrix}$$ i.e. stacked Hessians of the elements of $f(x)$ ? But what about the latter?","I have a quadratic form like  $$ ( \boldsymbol x - f( \boldsymbol x ) )^T \boldsymbol A ( \boldsymbol x - f( \boldsymbol x ) )$$  with $f(\cdot): \mathbb{R}^n \rightarrow \mathbb{R}^m$. And I would like to compute the Hessian of it based on Jacobeans and higher order derivatives of $f(\cdot)$. I already found some hints (Section Basic properties) for the product rule, but couldn't figure out how to apply this further to the Hessian. Edit: My current approach: $z(x) = g(x)h(x)$, where $g(x) = (x-f)^T$ and $h(x)=A(x-f)$ The first derivative is then given by $$ D[z] = h^TD[g] + gD[h] $$ When using $u(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times p}$, $v(x):\mathbb{R}^n\rightarrow\mathbb{R}^{m\times q}$ from here, page 4, second last equation $$ D[uv] = (v^T\otimes I_m) D[f] + (I_q\otimes u)D[v] $$ Where in my case we have $m=q=1, p=n$. Applying this to the first derivative above I get $$ D^2[z] = (D[g])^T D[h^T] + \{ I_n \otimes h^T \} D^2[g] + (D[h])^T D[g] + \{ I_n\otimes g \} D^2[h]\\\\ = \{ I_n- D[f^T] \}^T\{ I_n - D[f] \}^T A^T + \{ I_n \otimes (x-f)^TA^T \} D^2[g] + \{ I_n -D[f] \}^TA^T\{ I_n - D[f^T] \} + \{ I_n \otimes (x-f)^T \} D^2[h] $$ My questions are now basically? Is that correct? And how are $D[g]=-D^2[f^T]$ and $D^2[h]=-A\cdot{}D[D[f]^T]$ actually defined? Is the former like  $$-\begin{bmatrix} D^2[f_1] \\ \vdots  \\ D^2[f_n] \end{bmatrix}$$ i.e. stacked Hessians of the elements of $f(x)$ ? But what about the latter?",,"['matrices', 'derivatives', 'hessian-matrix']"
28,Quick doubt on an application of the chain rule,Quick doubt on an application of the chain rule,,"In Perloff's Microeconomics With Calculus 3rd edition, on page 63, one proceeds to the differentiation of the following equation with respect to $\tau$: $$D(p(\tau))=S(p(\tau)-\tau)$$ And the result is: $$\frac{\text{d}D}{\text{d}p}\frac{\text{dp}}{\text{d}\tau}=\frac{\text{d}S}{\text{d}p}\frac{\text{d}(p(\tau)-\tau)}{\text{d}\tau}=\frac{\text{d}S}{\text{d}p}(\frac{\text{d}p}{\text{d}\tau}-1)$$ My doubt is: why is the third member of the equation equal to the one in the middle? I really can't see how to go from one to another. Besides, according to the chain rule, shouldn't the derivative of $S(p(\tau)-\tau)$ with respect to $\tau$ be $\frac{\text{d}S}{\text{d}(p(\tau)-\tau)}\frac{\text{d}(p(\tau)-\tau)}{\text{d}\tau}$? Any help will be much appreciated. Thanks very much in advance.","In Perloff's Microeconomics With Calculus 3rd edition, on page 63, one proceeds to the differentiation of the following equation with respect to $\tau$: $$D(p(\tau))=S(p(\tau)-\tau)$$ And the result is: $$\frac{\text{d}D}{\text{d}p}\frac{\text{dp}}{\text{d}\tau}=\frac{\text{d}S}{\text{d}p}\frac{\text{d}(p(\tau)-\tau)}{\text{d}\tau}=\frac{\text{d}S}{\text{d}p}(\frac{\text{d}p}{\text{d}\tau}-1)$$ My doubt is: why is the third member of the equation equal to the one in the middle? I really can't see how to go from one to another. Besides, according to the chain rule, shouldn't the derivative of $S(p(\tau)-\tau)$ with respect to $\tau$ be $\frac{\text{d}S}{\text{d}(p(\tau)-\tau)}\frac{\text{d}(p(\tau)-\tau)}{\text{d}\tau}$? Any help will be much appreciated. Thanks very much in advance.",,"['calculus', 'derivatives', 'implicit-differentiation']"
29,Calculate the derivative using limit definition.,Calculate the derivative using limit definition.,,This is the function $f(x)$$=\frac{1}{\sqrt{3x-2}}$ . I wrote that $$\lim_{h\to 0}\frac{\frac{\sqrt{3x+3h-2}}{3x+3h-2}-\frac{\sqrt{3x-2}}{3x-2}}{h}.$$ I am not able to continue further.,This is the function $f(x)$$=\frac{1}{\sqrt{3x-2}}$ . I wrote that $$\lim_{h\to 0}\frac{\frac{\sqrt{3x+3h-2}}{3x+3h-2}-\frac{\sqrt{3x-2}}{3x-2}}{h}.$$ I am not able to continue further.,,"['calculus', 'derivatives']"
30,References for this theorem,References for this theorem,,"Here is the statement (it is a sort of a division theorem) : Let $f\in \mathcal{C}^{\infty}(\mathbb{R})$ such that $f(0)=0$  and   $g:\mathbb{R^*}\to \mathbb{R},\ x\mapsto \frac{f(x)}{x}$ and   $g(0)=f'(0)$. Then $g\in\mathcal{C}^{\infty}(\mathbb{R})$ and $\forall  n \in \mathbb{N}$ : $g^{(n)}(0)=\frac{f^{(n+1)}(0)}{n+1}$. I was wondering if there are references for this statement ? I just found the statement as an exercise on the internet. Thanks in advance !","Here is the statement (it is a sort of a division theorem) : Let $f\in \mathcal{C}^{\infty}(\mathbb{R})$ such that $f(0)=0$  and   $g:\mathbb{R^*}\to \mathbb{R},\ x\mapsto \frac{f(x)}{x}$ and   $g(0)=f'(0)$. Then $g\in\mathcal{C}^{\infty}(\mathbb{R})$ and $\forall  n \in \mathbb{N}$ : $g^{(n)}(0)=\frac{f^{(n+1)}(0)}{n+1}$. I was wondering if there are references for this statement ? I just found the statement as an exercise on the internet. Thanks in advance !",,"['derivatives', 'reference-request', 'taylor-expansion']"
31,How to show $|x|$ is convex on R?,How to show  is convex on R?,|x|,How to show $f(x):=|x|$ is convex on R? $f(x) = \left\{ \begin{array}{cc}x & x\geq 0 \\ -x & x<0 \end{array} \right.$ $f'(x) = \left\{\begin{array}{cc} 1 &x> 0 \\ -1 & x<0 \end{array} \right.$ $\; \; f'(x)$ does not exist for $x=0$ $f''(x)=\left\{\begin{array}{cc} 0 &x> 0 \\ 0 & x<0 \end{array}\right.$ But still I don't know $f''(0)$ I wanted to show $f''(x)\geq 0 \; \forall \; x \in R$ to show that it is convex on R,How to show $f(x):=|x|$ is convex on R? $f(x) = \left\{ \begin{array}{cc}x & x\geq 0 \\ -x & x<0 \end{array} \right.$ $f'(x) = \left\{\begin{array}{cc} 1 &x> 0 \\ -1 & x<0 \end{array} \right.$ $\; \; f'(x)$ does not exist for $x=0$ $f''(x)=\left\{\begin{array}{cc} 0 &x> 0 \\ 0 & x<0 \end{array}\right.$ But still I don't know $f''(0)$ I wanted to show $f''(x)\geq 0 \; \forall \; x \in R$ to show that it is convex on R,,"['real-analysis', 'derivatives']"
32,Help explaining formula to calc students,Help explaining formula to calc students,,This is a stupid question but what is the proper name of this formula? Been trying to look for a proper definition for this but can't find it online. It's been a long time since I had to use these that my brain is drawing a blank. $$||a||=\sqrt{\left(\frac{{\rm d}^{2}y}{{\rm d}t^{2}}\right)^{2}+\left(\frac{{\rm d}^{2}x}{{\rm d}t^{2}}\right)^{2}}$$ Edit: just found it used in this question: Acceleration of a particle described by parametric equations So it's the magnitude of the vector's speed?,This is a stupid question but what is the proper name of this formula? Been trying to look for a proper definition for this but can't find it online. It's been a long time since I had to use these that my brain is drawing a blank. $$||a||=\sqrt{\left(\frac{{\rm d}^{2}y}{{\rm d}t^{2}}\right)^{2}+\left(\frac{{\rm d}^{2}x}{{\rm d}t^{2}}\right)^{2}}$$ Edit: just found it used in this question: Acceleration of a particle described by parametric equations So it's the magnitude of the vector's speed?,,['calculus']
33,"Tangent plane of the surface: $z-g(x,y)=0$ in $(x_0, y_0, z_0)$",Tangent plane of the surface:  in,"z-g(x,y)=0 (x_0, y_0, z_0)","How can I determine the equation of tangent plane of the surface: $$z-g(x,y)=0$$ in the point: $$(x_0, y_0, z_0)$$ in terms of implicit derivatives?","How can I determine the equation of tangent plane of the surface: $$z-g(x,y)=0$$ in the point: $$(x_0, y_0, z_0)$$ in terms of implicit derivatives?",,"['integration', 'derivatives', 'implicit-function-theorem']"
34,Find an equation of a tangent line,Find an equation of a tangent line,,"Find an equation of a the tangent line to the graph of $x^2 - xy - y^2= 1$ when $(x,y) = (2,1)$. Here's what I have so far: $$ x^2 - xy - y^2 = 1 $$ $$\implies x^2 - xy - y^2  - 1 = 0 $$ $$\implies y' = 2x - xy' - 2yy' $$ $$\implies -2x = y'(-x -2y) $$ By trying to simplify the equation so that $y'$ is on one side, is that the correct way to begin solving the problem in the first place? If so, is my simplification correct as shown above?","Find an equation of a the tangent line to the graph of $x^2 - xy - y^2= 1$ when $(x,y) = (2,1)$. Here's what I have so far: $$ x^2 - xy - y^2 = 1 $$ $$\implies x^2 - xy - y^2  - 1 = 0 $$ $$\implies y' = 2x - xy' - 2yy' $$ $$\implies -2x = y'(-x -2y) $$ By trying to simplify the equation so that $y'$ is on one side, is that the correct way to begin solving the problem in the first place? If so, is my simplification correct as shown above?",,"['calculus', 'derivatives', 'tangent-line']"
35,Derivate of the determinant,Derivate of the determinant,,"Is there a simple way to see that $$\frac{d}{dt}\det(\exp(tX))\bigg|_{t=0}=\text{tr}(X)\quad\forall X\in\text{Mat}(n,\mathbb{C})?$$ Of course there are formulas for the derivate of the determinant function but let us assume I don't know any of them.","Is there a simple way to see that $$\frac{d}{dt}\det(\exp(tX))\bigg|_{t=0}=\text{tr}(X)\quad\forall X\in\text{Mat}(n,\mathbb{C})?$$ Of course there are formulas for the derivate of the determinant function but let us assume I don't know any of them.",,['linear-algebra']
36,How to solve this limit using Taylor formula?,How to solve this limit using Taylor formula?,,I have difficulties in solving this limit using Taylor formula. $$ \lim_{x \rightarrow+ \infty} \big((x+a)^{1+\frac {1}{x}} - x^{1+\frac{1}{x+a}}\big) $$ Can you give me some advice? I will be very thankful for your help.,I have difficulties in solving this limit using Taylor formula. $$ \lim_{x \rightarrow+ \infty} \big((x+a)^{1+\frac {1}{x}} - x^{1+\frac{1}{x+a}}\big) $$ Can you give me some advice? I will be very thankful for your help.,,"['derivatives', 'taylor-expansion']"
37,$\frac{d}{dx}\int_a^{x}f(x-t)dt$ and $\frac{d}{dx}\int_a^{x}f(x-t)g(t)dt$,and,\frac{d}{dx}\int_a^{x}f(x-t)dt \frac{d}{dx}\int_a^{x}f(x-t)g(t)dt,"$\frac{d}{dx}\int_a^{x}f(x-t)dt=f(x-a)$ I reached this after doing a variable substitution, and applying the fundamental theorem of calculus. However, what would $\frac{d}{dx}\int_a^{x}f(x-t)g(t)dt$ equal to? I can't use the same reasoning, since I would just push the x to the function $g$. Any help would be appreciated.","$\frac{d}{dx}\int_a^{x}f(x-t)dt=f(x-a)$ I reached this after doing a variable substitution, and applying the fundamental theorem of calculus. However, what would $\frac{d}{dx}\int_a^{x}f(x-t)g(t)dt$ equal to? I can't use the same reasoning, since I would just push the x to the function $g$. Any help would be appreciated.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'derivatives']"
38,Calculate the derivative of $f$. Let $f(x)=\sqrt{1+|x|^2}$ for $x\in H$ a real Hilbert space.,Calculate the derivative of . Let  for  a real Hilbert space.,f f(x)=\sqrt{1+|x|^2} x\in H,"Let $f:H\to H,\, x\mapsto x/\sqrt{1+|x|^2}$ for $H$ a real Hilbert space. Calculate the derivative of $f$. I want to confirm if my calculation is correct. Setting $g(x):=(1+(x|x))^{-1/2}$ we have that $f(x)=g(x)\cdot x$ where the dot means the scalar multiplication in $H$ (not the inner product, just the product of a real number by a vector). Then applying the product rule we find $$\partial f(x)h=\partial g(x)h\cdot x+g(x)\cdot h,\quad x,h\in H$$ where $\partial g(x)h=-(1+|x|^2)^{-3/2}(x|h)$. Is that correct or there is something wrong? Thank you in advance.","Let $f:H\to H,\, x\mapsto x/\sqrt{1+|x|^2}$ for $H$ a real Hilbert space. Calculate the derivative of $f$. I want to confirm if my calculation is correct. Setting $g(x):=(1+(x|x))^{-1/2}$ we have that $f(x)=g(x)\cdot x$ where the dot means the scalar multiplication in $H$ (not the inner product, just the product of a real number by a vector). Then applying the product rule we find $$\partial f(x)h=\partial g(x)h\cdot x+g(x)\cdot h,\quad x,h\in H$$ where $\partial g(x)h=-(1+|x|^2)^{-3/2}(x|h)$. Is that correct or there is something wrong? Thank you in advance.",,"['real-analysis', 'analysis', 'derivatives', 'proof-verification', 'hilbert-spaces']"
39,"let $f(x) = \left\lbrace\begin{array}{ll} x^2+3x, &x \geq1\\ x^2-3x+6, & x<1 \end{array}\right.$ then find the limit…",let  then find the limit…,"f(x) = \left\lbrace\begin{array}{ll} x^2+3x, &x \geq1\\ x^2-3x+6, & x<1 \end{array}\right.","Let  $f(x) = \begin{cases} x^2+3x,  &x \geq1\\[2ex] x^2-3x+6, & x<1 \end{cases}$ Then find the $$\lim_{h \to0} \frac{f(1)-f(1-h^2)}{h^2}=?$$ My Try : $$f(1)=4$$ $$f(1-h^2)=(1-h^2)^2-3(1-h^2)+6=h^4-h^2+4=h^2(h^2-1)+4$$ So we have : $$\lim_{h \to0} \frac{h^2(h^2-1)}{h^2}=-1$$ it is right ?","Let  $f(x) = \begin{cases} x^2+3x,  &x \geq1\\[2ex] x^2-3x+6, & x<1 \end{cases}$ Then find the $$\lim_{h \to0} \frac{f(1)-f(1-h^2)}{h^2}=?$$ My Try : $$f(1)=4$$ $$f(1-h^2)=(1-h^2)^2-3(1-h^2)+6=h^4-h^2+4=h^2(h^2-1)+4$$ So we have : $$\lim_{h \to0} \frac{h^2(h^2-1)}{h^2}=-1$$ it is right ?",,"['calculus', 'derivatives']"
40,On the definition of piecewise differentiable curves,On the definition of piecewise differentiable curves,,"The definition of a piecewise differentiable curve as given by Do Carmo in his Riemannian Geometry text is as follows: A piecewise differentiable curve is a mapping $c: [a, b] \to M$ of a closed interval $[a, b] \subset R$ into $M$ satisfying the following condition: there exists a partition $a<t_o<t_1< ... <t_{k-1}<t_k=b$ of $[a,b]$ such that the restrictions of c in each closed subset $[t_i,t_{i+1}]$, $i=0,..,k-1$, are differentiable. The question: Why is each subset, in which we want $c$ to be differentiable, closed? Since the derivative of a function has to do with a neighborhood of the function near the point that we are taking the derivative, how can we take the derivative at the points $a, b$ that are mentioned in the definition?","The definition of a piecewise differentiable curve as given by Do Carmo in his Riemannian Geometry text is as follows: A piecewise differentiable curve is a mapping $c: [a, b] \to M$ of a closed interval $[a, b] \subset R$ into $M$ satisfying the following condition: there exists a partition $a<t_o<t_1< ... <t_{k-1}<t_k=b$ of $[a,b]$ such that the restrictions of c in each closed subset $[t_i,t_{i+1}]$, $i=0,..,k-1$, are differentiable. The question: Why is each subset, in which we want $c$ to be differentiable, closed? Since the derivative of a function has to do with a neighborhood of the function near the point that we are taking the derivative, how can we take the derivative at the points $a, b$ that are mentioned in the definition?",,"['derivatives', 'differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds']"
41,$\forall :=\textbf{for every} \ x\in D_f\cap N^+_{\epsilon}(a) \Rightarrow := \textbf{implies that} \ f(a)=f(x)$,,\forall :=\textbf{for every} \ x\in D_f\cap N^+_{\epsilon}(a) \Rightarrow := \textbf{implies that} \ f(a)=f(x),Clime : let $f'(a +)=0$ then $$ \exists  \  N^+_{\epsilon}(a) $$ such that : $$\forall :=\textbf{for every} \ x\in D_f\cap  N^+_{\epsilon}(a) \Rightarrow := \textbf{implies that} \ f(a)=f(x)$$ it is right ? how prove ? please helpe me,Clime : let then such that : it is right ? how prove ? please helpe me,f'(a +)=0  \exists  \  N^+_{\epsilon}(a)  \forall :=\textbf{for every} \ x\in D_f\cap  N^+_{\epsilon}(a) \Rightarrow := \textbf{implies that} \ f(a)=f(x),"['calculus', 'derivatives']"
42,Prove that there are $0<c_1 < c_2<1$ s.t. $\frac{1}{g'(c_1)}+\frac{1}{g'(c_2)}=2$ . The same for $g'(c_1)g'(c_2)=1$.,Prove that there are  s.t.  . The same for .,0<c_1 < c_2<1 \frac{1}{g'(c_1)}+\frac{1}{g'(c_2)}=2 g'(c_1)g'(c_2)=1,"Given $g$ is continuous on $[0,1]$ and differentiable on  $(0,1)$,   $g(0)=0$, and $g(1)=1$. 1) Prove that there exist $0<c_1<c_2<1$ such that   $$\frac{1}{g'(c_1)}+\frac{1}{g'(c_2)}=2.$$ 2) Prove that there exist $0<c_1<c_2<1$ such that    $$g'(c_1)g'(c_2)=1.$$ I am thinking if Mean Value Theorem would help, but Mean Value theorem could only prove that in interval (0,1) there exists $g'(c)=1$.","Given $g$ is continuous on $[0,1]$ and differentiable on  $(0,1)$,   $g(0)=0$, and $g(1)=1$. 1) Prove that there exist $0<c_1<c_2<1$ such that   $$\frac{1}{g'(c_1)}+\frac{1}{g'(c_2)}=2.$$ 2) Prove that there exist $0<c_1<c_2<1$ such that    $$g'(c_1)g'(c_2)=1.$$ I am thinking if Mean Value Theorem would help, but Mean Value theorem could only prove that in interval (0,1) there exists $g'(c)=1$.",,"['calculus', 'real-analysis', 'derivatives']"
43,Equivalent condition for complex differentiability,Equivalent condition for complex differentiability,,"Let the standard definition for complex differentiability at $a \in D \subset \Bbb{C}$ of a function $f : D \to \Bbb{C}$ be that the limit $$ \lim\limits_{z \to a} \dfrac{f(z) - f(a)}{z - a} $$ exists and equals $l \in \Bbb{C}$. This is equivalent to the condition: There exists a function $\varphi : D \to \Bbb{C}$ which is continuous at $a$ such that $f(z) = f(a) + \varphi(z)(z - a)$ and $\varphi(a) = l$. Since $a$ is a point of accumulation of $D$ define $\varphi(z) = \dfrac{f(z) - f(a)}{z - a}, \ \varphi : D \to \Bbb{C}$ except at $a$ where we define $\varphi(a) = l$.  How do I prove that $\varphi(z)$ is continuous?  Since I don't know whether $f$ is yet (that's part of a corollary to this list of equivalent conditions).","Let the standard definition for complex differentiability at $a \in D \subset \Bbb{C}$ of a function $f : D \to \Bbb{C}$ be that the limit $$ \lim\limits_{z \to a} \dfrac{f(z) - f(a)}{z - a} $$ exists and equals $l \in \Bbb{C}$. This is equivalent to the condition: There exists a function $\varphi : D \to \Bbb{C}$ which is continuous at $a$ such that $f(z) = f(a) + \varphi(z)(z - a)$ and $\varphi(a) = l$. Since $a$ is a point of accumulation of $D$ define $\varphi(z) = \dfrac{f(z) - f(a)}{z - a}, \ \varphi : D \to \Bbb{C}$ except at $a$ where we define $\varphi(a) = l$.  How do I prove that $\varphi(z)$ is continuous?  Since I don't know whether $f$ is yet (that's part of a corollary to this list of equivalent conditions).",,"['complex-analysis', 'limits', 'derivatives', 'continuity']"
44,Find the values of a and b that makes the following function differentiable,Find the values of a and b that makes the following function differentiable,,"I am told for the following piecewise function: $$f(x)=\left\{ \begin{array}{ll}       ax+b, & x>-1; \\       bx^2-3ax+4, & x\leq -1 \\ \end{array}  \right. $$ I am asked to find the values of $a$ and $b$ that make $f(x)$ differentiable. So I simply differentiated $f(x)$ to get: $$f'(x)=\left\{ \begin{array}{ll}       a, & x>-1; \\       2bx-3a, & x\leq -1 \\ \end{array}  \right. $$ So since a differentiable function must be continuous, I get that: $\displaystyle{\lim_{x \to -1^-}}(a)=f'(-1)$ So this implies: $a=-2b-3a$ $4a=-2b$ $-2a=b$ So that means as long as for any $a$, if I get a $b$ value such that $-2a=b$, $f(x)$ is differentiable. Is that correct?","I am told for the following piecewise function: $$f(x)=\left\{ \begin{array}{ll}       ax+b, & x>-1; \\       bx^2-3ax+4, & x\leq -1 \\ \end{array}  \right. $$ I am asked to find the values of $a$ and $b$ that make $f(x)$ differentiable. So I simply differentiated $f(x)$ to get: $$f'(x)=\left\{ \begin{array}{ll}       a, & x>-1; \\       2bx-3a, & x\leq -1 \\ \end{array}  \right. $$ So since a differentiable function must be continuous, I get that: $\displaystyle{\lim_{x \to -1^-}}(a)=f'(-1)$ So this implies: $a=-2b-3a$ $4a=-2b$ $-2a=b$ So that means as long as for any $a$, if I get a $b$ value such that $-2a=b$, $f(x)$ is differentiable. Is that correct?",,"['limits', 'derivatives', 'continuity']"
45,Find the derivative using limit definition: $\frac{x}{1 + x\sin(1/x)}$ [closed],Find the derivative using limit definition:  [closed],\frac{x}{1 + x\sin(1/x)},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can I find the derivative of this function  (defined below) using the definition of the limit? $$f(x):= \frac{x}{1 + x\sin(1/x)}$$ So I know the definition of the limit but I am unable to apply it here. Any help or hint would be appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can I find the derivative of this function  (defined below) using the definition of the limit? So I know the definition of the limit but I am unable to apply it here. Any help or hint would be appreciated.",f(x):= \frac{x}{1 + x\sin(1/x)},"['calculus', 'derivatives']"
46,extending continuity in proof of l'Hopital rule,extending continuity in proof of l'Hopital rule,,"Let the functions $f(x)$ and $g(x)$ are defined and differentiable in the interval $(a,b)$ and everywhere in it $g'(x)\ne 0.$ Let $$\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0.$$ Let also the limit of the fraction of the derivatives $$ \lim_{x\to a}\frac{f'(x)}{g'(x)} $$ exists in a generalised sense. Then $$ \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)} $$ The proof of that theorem starts by saying that we can extend the continuity of the functions $f(x)$ and $g(x)$ in the point $a$ by denoting  $f(a)=g(a)=0$ and preserve the same denotations for the extended functions. I really don't get that. The function is continuous in the interval $(a,b)$ from which I conclude that it can be undefined in the point a. How are we allowed to say that $f(a)=g(a)=0$?","Let the functions $f(x)$ and $g(x)$ are defined and differentiable in the interval $(a,b)$ and everywhere in it $g'(x)\ne 0.$ Let $$\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0.$$ Let also the limit of the fraction of the derivatives $$ \lim_{x\to a}\frac{f'(x)}{g'(x)} $$ exists in a generalised sense. Then $$ \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)} $$ The proof of that theorem starts by saying that we can extend the continuity of the functions $f(x)$ and $g(x)$ in the point $a$ by denoting  $f(a)=g(a)=0$ and preserve the same denotations for the extended functions. I really don't get that. The function is continuous in the interval $(a,b)$ from which I conclude that it can be undefined in the point a. How are we allowed to say that $f(a)=g(a)=0$?",,"['real-analysis', 'limits', 'derivatives', 'continuity']"
47,Matrix Derivative of $ {L}_{1} $ Norm,Matrix Derivative of  Norm, {L}_{1} ,"I'm with problems to find step-by-step resolution for a matrix differentiation. My main problem is to solve the L1 norm into this derivative: $\nabla K = \frac{\partial }{\partial x}\left | BCx - Nk + Fk \right |_{1}^{1} $ where B, C, N and F are matrix and x are a vector. Can anyone help with this? Thanks for the help!","I'm with problems to find step-by-step resolution for a matrix differentiation. My main problem is to solve the L1 norm into this derivative: $\nabla K = \frac{\partial }{\partial x}\left | BCx - Nk + Fk \right |_{1}^{1} $ where B, C, N and F are matrix and x are a vector. Can anyone help with this? Thanks for the help!",,"['linear-algebra', 'matrices', 'derivatives']"
48,"Proving the function $f:t\mapsto (\cos t,\sin t)$ is differentiable",Proving the function  is differentiable,"f:t\mapsto (\cos t,\sin t)","I want to prove that $f$ is differentiable. I use the principle of differentiability that says that $f$ is differentiable if there exists an A such that: $$\lim \cfrac{\|f(t)-f(\xi)-A(t-\xi)\|}{\|t-\xi\|}=0, t\rightarrow \xi$$ I know that $A$ is the derivative of $f$ thus $A=(-\sin t, \cos t)$. But when I substitute this in the equation I get that: $$\lim \cfrac{\|(\cos t,\sin t)-(\cos\xi,\sin\xi)-(-\sin(t-\xi),\cos(t-\xi))\|}{\|t-\xi\|} = $$ $$\lim \cfrac{\|(\cos t - \cos\xi + \sin(t-\xi), \sin t-\sin\xi-\cos(t-\xi))\|}{\|t-\xi\|} = $$ $$\lim\cfrac{\|(\cos t - \cos\xi + \sin t\cos\xi-\cos t\sin\xi, \sin t- \sin\xi - \cos t\cos\xi-\sin t\sin\xi)\|}{\|t-\xi\|}$$ I don't really know where to go from here can anyone help me with this?","I want to prove that $f$ is differentiable. I use the principle of differentiability that says that $f$ is differentiable if there exists an A such that: $$\lim \cfrac{\|f(t)-f(\xi)-A(t-\xi)\|}{\|t-\xi\|}=0, t\rightarrow \xi$$ I know that $A$ is the derivative of $f$ thus $A=(-\sin t, \cos t)$. But when I substitute this in the equation I get that: $$\lim \cfrac{\|(\cos t,\sin t)-(\cos\xi,\sin\xi)-(-\sin(t-\xi),\cos(t-\xi))\|}{\|t-\xi\|} = $$ $$\lim \cfrac{\|(\cos t - \cos\xi + \sin(t-\xi), \sin t-\sin\xi-\cos(t-\xi))\|}{\|t-\xi\|} = $$ $$\lim\cfrac{\|(\cos t - \cos\xi + \sin t\cos\xi-\cos t\sin\xi, \sin t- \sin\xi - \cos t\cos\xi-\sin t\sin\xi)\|}{\|t-\xi\|}$$ I don't really know where to go from here can anyone help me with this?",,"['limits', 'derivatives']"
49,Application of l'Hopital's Rule in a past paper,Application of l'Hopital's Rule in a past paper,,In an old exam paper the following solution is given: $$\lim_{r \to 0}\cfrac{\sin\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{r}{R}}\overset{l'Hopital}{=}\cfrac{\cfrac{1}{R}\cos\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{1}{R}}=\frac{1}{\mu_1}$$ My understanding of l'Hopital is that the result should be $1$: $$\lim_{r \to 0}\cfrac{\sin\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{r}{R}}=\cfrac{\cfrac{\mu_1}{R}\cos\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{1}{R}}=1$$ Is this a mistake or have I misunderstood something?,In an old exam paper the following solution is given: $$\lim_{r \to 0}\cfrac{\sin\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{r}{R}}\overset{l'Hopital}{=}\cfrac{\cfrac{1}{R}\cos\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{1}{R}}=\frac{1}{\mu_1}$$ My understanding of l'Hopital is that the result should be $1$: $$\lim_{r \to 0}\cfrac{\sin\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{r}{R}}=\cfrac{\cfrac{\mu_1}{R}\cos\left(\cfrac{\mu_1\cdot r}{R}\right)}{\mu_1\cfrac{1}{R}}=1$$ Is this a mistake or have I misunderstood something?,,"['calculus', 'derivatives']"
50,Proof verification: $\frac{d}{dx}\sin^{-1}(x)=\frac{1}{\sqrt{1-x^2}}$,Proof verification:,\frac{d}{dx}\sin^{-1}(x)=\frac{1}{\sqrt{1-x^2}},"In this question, I will use $\sin^{-1}(x)$ instead of $\arcsin(x)$ since to me it's simpler this way. Here's the proof that needs verifying: $$y=\sin^{-1}(x)$$ hence, $$x=\sin(y)$$ $$\frac{dx}{dy}=cos(y)$$ $$\frac{dy}{dx}=\frac{1}{cos(y)}$$ then, using the $\sin^2(y)+\cos^2(y)=1$ identity, $\cos(y)=\pm\sqrt{1-\sin^2(y)}$, or $\cos(y)=\pm\sqrt{1-x^2}$, given that $x=\sin(y)$. However, since $-\frac{\pi}{2}<\sin^{-1}(x)<\frac{\pi}{2}$, and $y=\sin^{-1}(x)$, and $\cos(y) > 0$ in the given interval, it can be inferred that the answer is: $$\frac{dy}{dx}=\frac{1}{\sqrt{1-x^2}}$$ the question is, is this proof correct, since at some point throughout this proof: it was implied that $$\frac{dy}{dx}=\sec(y)$$ and therefore that $$\sec(y)=\frac{1}{\cos(\sin^{-1}(x))}=\frac{1}{\sqrt{1-x^2}}$$ and I've proven numerically that this identity is true for the value $x=\frac{1}{2}$, but I don't know if this is true for every value of $-1 < x < 1$and if therefore, the means of proving I've used are false. TLDR: Is this proof correct and rigorous?","In this question, I will use $\sin^{-1}(x)$ instead of $\arcsin(x)$ since to me it's simpler this way. Here's the proof that needs verifying: $$y=\sin^{-1}(x)$$ hence, $$x=\sin(y)$$ $$\frac{dx}{dy}=cos(y)$$ $$\frac{dy}{dx}=\frac{1}{cos(y)}$$ then, using the $\sin^2(y)+\cos^2(y)=1$ identity, $\cos(y)=\pm\sqrt{1-\sin^2(y)}$, or $\cos(y)=\pm\sqrt{1-x^2}$, given that $x=\sin(y)$. However, since $-\frac{\pi}{2}<\sin^{-1}(x)<\frac{\pi}{2}$, and $y=\sin^{-1}(x)$, and $\cos(y) > 0$ in the given interval, it can be inferred that the answer is: $$\frac{dy}{dx}=\frac{1}{\sqrt{1-x^2}}$$ the question is, is this proof correct, since at some point throughout this proof: it was implied that $$\frac{dy}{dx}=\sec(y)$$ and therefore that $$\sec(y)=\frac{1}{\cos(\sin^{-1}(x))}=\frac{1}{\sqrt{1-x^2}}$$ and I've proven numerically that this identity is true for the value $x=\frac{1}{2}$, but I don't know if this is true for every value of $-1 < x < 1$and if therefore, the means of proving I've used are false. TLDR: Is this proof correct and rigorous?",,"['trigonometry', 'derivatives', 'proof-verification']"
51,Surjectivity of the derivative of $A \mapsto A^t A$.,Surjectivity of the derivative of .,A \mapsto A^t A,"I'm trying to solve this problem, but when I calculate the derivative of $\Phi$ I get $$(D\Phi)_A (H) = A^t H + H^t A.$$ In particular, the derivative at identity is $H + H^t$. But this can't be surjective as $(2)$ states as it would imply every matrix in $GL(n;\mathbb{R})$ is equal to its transpose.","I'm trying to solve this problem, but when I calculate the derivative of $\Phi$ I get $$(D\Phi)_A (H) = A^t H + H^t A.$$ In particular, the derivative at identity is $H + H^t$. But this can't be surjective as $(2)$ states as it would imply every matrix in $GL(n;\mathbb{R})$ is equal to its transpose.",,"['derivatives', 'differential-geometry', 'matrix-calculus']"
52,find maxima on a cardinal (catmull rom) spline,find maxima on a cardinal (catmull rom) spline,,"I am looking to find the maximum deviation from the straight line joining control points on a cardinal spline. This link gives a polynomial equation for the spline: $$q(t) = 0.5 \cdot(  	(2 \cdot P_1) + (-P_0 + P_2) \cdot t + (2\cdot P_0 - 5\cdot P_1 + 4\cdot P_2 - P_3)  \cdot t^2 + (-P_0 + 3\cdot P_1- 3 \cdot P_2 + P_3) \cdot t^3)$$ The only input I have is the 4 points in 3 dimensions. My idea was to compute the tangent by taking the first derivative of the above equation so that I get the slope at any point on the spline. Then I could compare it with the slope of the line joining the two points. But I am kind of confused how to do it in 3D. How do I compare the slopes in 3D? I can get a unit vector in the direction of the straight line joining the points, but how can I get a unit vector at the tangent. Is there a simpler way to do this?","I am looking to find the maximum deviation from the straight line joining control points on a cardinal spline. This link gives a polynomial equation for the spline: $$q(t) = 0.5 \cdot(  	(2 \cdot P_1) + (-P_0 + P_2) \cdot t + (2\cdot P_0 - 5\cdot P_1 + 4\cdot P_2 - P_3)  \cdot t^2 + (-P_0 + 3\cdot P_1- 3 \cdot P_2 + P_3) \cdot t^3)$$ The only input I have is the 4 points in 3 dimensions. My idea was to compute the tangent by taking the first derivative of the above equation so that I get the slope at any point on the spline. Then I could compare it with the slope of the line joining the two points. But I am kind of confused how to do it in 3D. How do I compare the slopes in 3D? I can get a unit vector in the direction of the straight line joining the points, but how can I get a unit vector at the tangent. Is there a simpler way to do this?",,"['geometry', 'derivatives']"
53,Why is this symbolic derivative zero?,Why is this symbolic derivative zero?,,"I'm trying to compute the derivative of this function with respect to $\theta$ (i.e., $\frac{dv}{d\theta}$): $$ v(t) = m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \cos(\theta(t)-\phi(t)) $$ I thought the answer was: $$ \frac{dv}{d\theta} = -m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \sin(\theta(t)-\phi(t)) $$ But Matlab yields zero using this script: clear variables clc syms t a1 a2 m2 th(t) p(t)  f = m2*a1*a2*diff(p,t)*diff(th,t)*cos(th-p); v = sym('th'); dvdth = subs(diff(subs(f,th,v),v),v,th) Is there any problem with the preceding script?","I'm trying to compute the derivative of this function with respect to $\theta$ (i.e., $\frac{dv}{d\theta}$): $$ v(t) = m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \cos(\theta(t)-\phi(t)) $$ I thought the answer was: $$ \frac{dv}{d\theta} = -m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \sin(\theta(t)-\phi(t)) $$ But Matlab yields zero using this script: clear variables clc syms t a1 a2 m2 th(t) p(t)  f = m2*a1*a2*diff(p,t)*diff(th,t)*cos(th-p); v = sym('th'); dvdth = subs(diff(subs(f,th,v),v),v,th) Is there any problem with the preceding script?",,"['derivatives', 'matlab', 'symbolic-computation']"
54,Derivatives from the value to the power of $1/\rho$,Derivatives from the value to the power of,1/\rho,"I know from check information in the task that $$\frac 1 \rho\cdot  {\frac{\partial}{\partial \phi}}(\cos^2\phi\cdot  \tanh z) - \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}] \frac{\cosh\rho}{\cosh z} \right)=0$$ After calculation $$\frac{1}{\rho} \cdot \frac\partial {\partial \phi}(\cos^2\phi\cdot \tanh z)= \frac{1}{\rho} \cdot 2 \cos\phi \sin\phi \cdot \tanh z= \frac{1}{\rho} \cdot \sin2\phi \cdot \tanh z $$ I have problem with the second derivative \begin{align} & \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}]{\frac{\cosh\rho}{\cosh z}} \right) \\[10pt] = {} & \sin2\phi \cdot \frac 1 {\sqrt[{\Large\,\rho\,}]{\frac{\cosh\rho}{\cosh z}}} \cdot \frac 1 \rho \cdot \left(\frac{\cosh\rho}{\cosh z} \right)^{(1/\rho)-1} \cdot \sinh z \\[10pt] = {} & \frac 1 \rho \cdot \sin2\phi\cdot \frac{ \left(\frac{\cosh\rho}{\cosh z} \right)^{(1/\rho)-1}} {\left(\frac{\cosh\rho}{\cosh z}\right)^{1/\rho}} \cdot \sinh z \end{align} As we can compare the line 2nd and 4th one I'm not getting the same result I don't know what I'm doing wrong $$ \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}]\frac{\cosh\rho}{\cosh z} \right)=  \frac 1 \rho \cdot \sin2\phi\cdot \left(\frac{\cosh z}{\cosh\rho}\right)\cdot \sinh z$$","I know from check information in the task that $$\frac 1 \rho\cdot  {\frac{\partial}{\partial \phi}}(\cos^2\phi\cdot  \tanh z) - \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}] \frac{\cosh\rho}{\cosh z} \right)=0$$ After calculation $$\frac{1}{\rho} \cdot \frac\partial {\partial \phi}(\cos^2\phi\cdot \tanh z)= \frac{1}{\rho} \cdot 2 \cos\phi \sin\phi \cdot \tanh z= \frac{1}{\rho} \cdot \sin2\phi \cdot \tanh z $$ I have problem with the second derivative \begin{align} & \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}]{\frac{\cosh\rho}{\cosh z}} \right) \\[10pt] = {} & \sin2\phi \cdot \frac 1 {\sqrt[{\Large\,\rho\,}]{\frac{\cosh\rho}{\cosh z}}} \cdot \frac 1 \rho \cdot \left(\frac{\cosh\rho}{\cosh z} \right)^{(1/\rho)-1} \cdot \sinh z \\[10pt] = {} & \frac 1 \rho \cdot \sin2\phi\cdot \frac{ \left(\frac{\cosh\rho}{\cosh z} \right)^{(1/\rho)-1}} {\left(\frac{\cosh\rho}{\cosh z}\right)^{1/\rho}} \cdot \sinh z \end{align} As we can compare the line 2nd and 4th one I'm not getting the same result I don't know what I'm doing wrong $$ \frac \partial {\partial z} \left( \sin2\phi \cdot \ln \sqrt[{\Large\,\rho\,}]\frac{\cosh\rho}{\cosh z} \right)=  \frac 1 \rho \cdot \sin2\phi\cdot \left(\frac{\cosh z}{\cosh\rho}\right)\cdot \sinh z$$",,"['derivatives', 'vectors', 'vector-analysis']"
55,Is the entropy function over the simplex differentiable?,Is the entropy function over the simplex differentiable?,,"Consider the entropy function over the simplex, $f(x) = \sum\limits_{i = 1}^J x_i \log(x_i), x_i \geq 0, \sum\limits_{i = 1}^J x_i =1$ and $f(x) = +\infty$ everywhere else This function is given in Boyd and Vanderberg's book Pg. 93. My question is whether this function is differentiable, i.e. does $\nabla f(x)$ exist? I was guessing it does exist, because taking a derivative with respect to each $x_i$ gives me $(\nabla f(x))_i = (1 + \log(x_i))$. But I saw some discussion online (can't find it now, maybe here: https://johncarlosbaez.wordpress.com/ ) that this function is not differentiable. Which also makes sense, because the sharp corner of this function. So if it is not differentiable, then what is the $(1 + \log(x_i))$ term after I take its gradient as usual?","Consider the entropy function over the simplex, $f(x) = \sum\limits_{i = 1}^J x_i \log(x_i), x_i \geq 0, \sum\limits_{i = 1}^J x_i =1$ and $f(x) = +\infty$ everywhere else This function is given in Boyd and Vanderberg's book Pg. 93. My question is whether this function is differentiable, i.e. does $\nabla f(x)$ exist? I was guessing it does exist, because taking a derivative with respect to each $x_i$ gives me $(\nabla f(x))_i = (1 + \log(x_i))$. But I saw some discussion online (can't find it now, maybe here: https://johncarlosbaez.wordpress.com/ ) that this function is not differentiable. Which also makes sense, because the sharp corner of this function. So if it is not differentiable, then what is the $(1 + \log(x_i))$ term after I take its gradient as usual?",,"['real-analysis', 'functional-analysis', 'derivatives', 'convex-analysis', 'convex-optimization']"
56,Gradient of the distance of point and set,Gradient of the distance of point and set,,"Let $A⊂\mathbb{R}^{n}$ be a closed subset and $d:\mathbb{R}^n \to \mathbb{R}$   defined by $d(x)=\inf\{||x-y|| | y\in A\}$ ($x \in \mathbb{R}^n$) where $||v||$ is the Euclid norm of $v \in \mathbb{R}^n$.If $d$ is totally differentiable at $x_{0}\notin A$,prove that $||(\mathrm{grad} \ d)(x_{0})|| = 1$. If $A$ is the one point,It is obvious.However I have no idea in the case $A$ is merely closed subset. It comes down to the one point case ?","Let $A⊂\mathbb{R}^{n}$ be a closed subset and $d:\mathbb{R}^n \to \mathbb{R}$   defined by $d(x)=\inf\{||x-y|| | y\in A\}$ ($x \in \mathbb{R}^n$) where $||v||$ is the Euclid norm of $v \in \mathbb{R}^n$.If $d$ is totally differentiable at $x_{0}\notin A$,prove that $||(\mathrm{grad} \ d)(x_{0})|| = 1$. If $A$ is the one point,It is obvious.However I have no idea in the case $A$ is merely closed subset. It comes down to the one point case ?",,"['calculus', 'derivatives', 'metric-spaces']"
57,Pointwise limits of the sequence of derivates of a fixed function =?,Pointwise limits of the sequence of derivates of a fixed function =?,,"Assume you are given a function $f:\mathbb R\to \mathbb R$ with the following properties: All derivatives $f^{(k)}$ exist everywhere The sequence $(f^{(k)})_k$ converges pointwise to a function $h:\mathbb R \to \mathbb R$ What can be said about the function $h$ ? I looked at a few examples and came up with functions like $f(x) = 3e^x+7e^{-\frac x 2} +42\sin(x/7)+x^{32}$. In this special example the limit funtion is $h(x) = 3e^x$. What I would like to know: Is it always true that if you start with a function $f$ satisfying the assumptions above, that you end up with a function $h(x)$ which is just a scalar multiple of $e^x$ ?","Assume you are given a function $f:\mathbb R\to \mathbb R$ with the following properties: All derivatives $f^{(k)}$ exist everywhere The sequence $(f^{(k)})_k$ converges pointwise to a function $h:\mathbb R \to \mathbb R$ What can be said about the function $h$ ? I looked at a few examples and came up with functions like $f(x) = 3e^x+7e^{-\frac x 2} +42\sin(x/7)+x^{32}$. In this special example the limit funtion is $h(x) = 3e^x$. What I would like to know: Is it always true that if you start with a function $f$ satisfying the assumptions above, that you end up with a function $h(x)$ which is just a scalar multiple of $e^x$ ?",,"['calculus', 'derivatives', 'pointwise-convergence']"
58,"If $\int_{\Omega} u(x) ~\text{div} \phi(x) ~\text{d}x =0$ for all $\phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$, then $u=\text{constant}$?","If  for all , then ?","\int_{\Omega} u(x) ~\text{div} \phi(x) ~\text{d}x =0 \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d) u=\text{constant}","I want to take the weak gradient operator $$ \begin{aligned} \nabla: L^2(\Omega) &\to W^{-1,2}(\Omega,\mathbb{R}^d) \\ \langle \nabla u, \phi \rangle_{W^{-1,2},W_0^{1,2}}&:=(u,\text{div}\phi)_{L^2}=\int_\Omega u(x) \,\text{div}\phi(x) ~\text{d}x \end{aligned} $$ for all $\phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$. Assume $\nabla u=0$. Show that $u$ is constant. I know that if I interpret $u$ as a tempered distribution and take the gradient as a distributional derivative it should be possible to conclude $u$ is constant. But I'd like to take the definition of the weak gradient operator above. We have $$ \int_\Omega u(x) \,  \text{div}\phi(x) ~dx=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d).$$ If $d=1$ I could conclude $u=\text{const}$ by the fundamental lemma of calculus of variations . But in general $$ \int_\Omega u(x) \,  (\partial_{1}\phi_1(x)+...+\partial_d \phi_d(x)) ~d(x_1,...,x_d)=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$$ and I don't know if there is a similar theorem.","I want to take the weak gradient operator $$ \begin{aligned} \nabla: L^2(\Omega) &\to W^{-1,2}(\Omega,\mathbb{R}^d) \\ \langle \nabla u, \phi \rangle_{W^{-1,2},W_0^{1,2}}&:=(u,\text{div}\phi)_{L^2}=\int_\Omega u(x) \,\text{div}\phi(x) ~\text{d}x \end{aligned} $$ for all $\phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$. Assume $\nabla u=0$. Show that $u$ is constant. I know that if I interpret $u$ as a tempered distribution and take the gradient as a distributional derivative it should be possible to conclude $u$ is constant. But I'd like to take the definition of the weak gradient operator above. We have $$ \int_\Omega u(x) \,  \text{div}\phi(x) ~dx=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d).$$ If $d=1$ I could conclude $u=\text{const}$ by the fundamental lemma of calculus of variations . But in general $$ \int_\Omega u(x) \,  (\partial_{1}\phi_1(x)+...+\partial_d \phi_d(x)) ~d(x_1,...,x_d)=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$$ and I don't know if there is a similar theorem.",,"['functional-analysis', 'derivatives', 'operator-theory', 'sobolev-spaces']"
59,"$f:\mathbb{R^m} \rightarrow \mathbb{R}, f(x) = |x|^a, a\in \mathbb{R}$. Calculate the derivative of f.",. Calculate the derivative of f.,"f:\mathbb{R^m} \rightarrow \mathbb{R}, f(x) = |x|^a, a\in \mathbb{R}","Let $f:\mathbb{R^m} \rightarrow \mathbb{R}, f(x) = |x|^a, a\in \mathbb{R}$ and $|\cdot |$ is the euclidean norm ($|x| = \sqrt{<x,x>}$). Show that $df(x)\cdot v = a |x|^{a-2}<x,v>$ $df(x)\cdot v = \frac{\partial f(x)}{\partial v} = \lim_{t \to 0}\frac{f(x+tv)-f(x)}{t}$ by definition. $\lim_{t \to 0}\frac{f(x+tv)-f(x)}{t} = \lim_{t \to 0}\frac{|x+tv|^a-|x|^a}{t} =  \lim_{t \to 0}\frac{(<x+tv,x+tv>)^{a/2}-(<x,x>)^{a/2}}{t}$ $$=\lim_{t\to 0}\frac{(<x,x>+2t<x,v>+t^2<v,v>)^{a/2}-<x,x>^{a/2}}{t}$$ From this part I can't make anything else, any hints?","Let $f:\mathbb{R^m} \rightarrow \mathbb{R}, f(x) = |x|^a, a\in \mathbb{R}$ and $|\cdot |$ is the euclidean norm ($|x| = \sqrt{<x,x>}$). Show that $df(x)\cdot v = a |x|^{a-2}<x,v>$ $df(x)\cdot v = \frac{\partial f(x)}{\partial v} = \lim_{t \to 0}\frac{f(x+tv)-f(x)}{t}$ by definition. $\lim_{t \to 0}\frac{f(x+tv)-f(x)}{t} = \lim_{t \to 0}\frac{|x+tv|^a-|x|^a}{t} =  \lim_{t \to 0}\frac{(<x+tv,x+tv>)^{a/2}-(<x,x>)^{a/2}}{t}$ $$=\lim_{t\to 0}\frac{(<x,x>+2t<x,v>+t^2<v,v>)^{a/2}-<x,x>^{a/2}}{t}$$ From this part I can't make anything else, any hints?",,"['real-analysis', 'derivatives']"
60,Proving that alternate definitions of the derivative in Differential Topology are equivalent,Proving that alternate definitions of the derivative in Differential Topology are equivalent,,"Preface This post may be quite long, however I wanted my question to be as complete as possible so please bear with me. Suppose $M \subseteq \mathbb{R}^k$ and $N \subseteq \mathbb{R}^l$ are two smooth manifolds of dimensions $m$ and $n$ respectively, and let $f : M \to N$ be a smooth mapping between these two manifolds, and let $x \in M$. Definition $(1)$ - As a map between tangent spaces The derivative of $f$ at $x$ is the a linear transformation of the tangent spaces $$df_x : TM_x \to TN_{f(x)}$$ Now we know the following is true: Suppose $\phi : U \subseteq \mathbb{R}^m \to M$ is a parameterization of a neighbourhood $\phi[U]$ of $x$, with $\phi(u) =x$ for some $u \in U$, and similarly suppose $\psi : V \subseteq \mathbb{R}^n \to N$ is a parameterization of a neighbourhood $\psi[V]$ of $f(x)$, with $\psi(v) =f(x)$ for some $v \in V$. Then taking derivatives we obtain $d\phi_u : \mathbb{R}^m \to \mathbb{R}^k$ and $d\psi_v : \mathbb{R}^n \to \mathbb{R}^l$, and we have $$TM_x = d\phi_u[\mathbb{R}^m] \ \ \text{ and } \ \ d\psi_v[\mathbb{R}^n] = TN_{f(x)}$$ and it follows that $TM_x \cong \mathbb{R}^m$ and $TN_{f(x)} \cong \mathbb{R}^n$. Definition $(2)$ - Assigning directional derivatives However the other definition of the derivative can be defined as follows. Now we don't know if $M$ is open or not in $\mathbb{R}^k$. So we have two cases to look at Case 1: $M$ is open in $\mathbb{R}^k$ The derivative of $f$ in the direction $h \in \mathbb{R}^k$, taken at the point $x \in M$ is defined by the conventional limit $$df_x(h) = \lim_{t \to 0} \frac{f(x + th) - f(x)}{t}$$ Now with $x$ fixed, we define $$df_x : \mathbb{R^k} \to \mathbb{R^l}$$ by assigning to each vector $h \in \mathbb{R}^k$ the directional derivative $df_x(h) \in \mathbb{R}^l$, this map is called the derivative of $f$ at $x$. Case 2: $M$ is not open in $\mathbb{R}^k$ We do know that $f$ is smooth, hence for each $x \in M$, there is an open set $W \subseteq \mathbb{R}^k$ and a smooth map $F : W \to \mathbb{R}^l$, such that $F = f$ on $W \cap M$ In this case I'm not even sure how one defines the derivative of $f$ at $x$ Now in Case 1 above, clearly $\mathbb{R}^m \not\cong \mathbb{R}^k$ and $\mathbb{R}^n \not\cong \mathbb{R}^l$, unless $m = k$ and $n = l$, so how can the two definitons of the derivative of $f$ at $x$ agree for this case. Furthermore I'm not even sure how Case 2 one can define the derivative of $f$ at $x$. So how can one prove that the two definitions of the derivative are equivalent?","Preface This post may be quite long, however I wanted my question to be as complete as possible so please bear with me. Suppose $M \subseteq \mathbb{R}^k$ and $N \subseteq \mathbb{R}^l$ are two smooth manifolds of dimensions $m$ and $n$ respectively, and let $f : M \to N$ be a smooth mapping between these two manifolds, and let $x \in M$. Definition $(1)$ - As a map between tangent spaces The derivative of $f$ at $x$ is the a linear transformation of the tangent spaces $$df_x : TM_x \to TN_{f(x)}$$ Now we know the following is true: Suppose $\phi : U \subseteq \mathbb{R}^m \to M$ is a parameterization of a neighbourhood $\phi[U]$ of $x$, with $\phi(u) =x$ for some $u \in U$, and similarly suppose $\psi : V \subseteq \mathbb{R}^n \to N$ is a parameterization of a neighbourhood $\psi[V]$ of $f(x)$, with $\psi(v) =f(x)$ for some $v \in V$. Then taking derivatives we obtain $d\phi_u : \mathbb{R}^m \to \mathbb{R}^k$ and $d\psi_v : \mathbb{R}^n \to \mathbb{R}^l$, and we have $$TM_x = d\phi_u[\mathbb{R}^m] \ \ \text{ and } \ \ d\psi_v[\mathbb{R}^n] = TN_{f(x)}$$ and it follows that $TM_x \cong \mathbb{R}^m$ and $TN_{f(x)} \cong \mathbb{R}^n$. Definition $(2)$ - Assigning directional derivatives However the other definition of the derivative can be defined as follows. Now we don't know if $M$ is open or not in $\mathbb{R}^k$. So we have two cases to look at Case 1: $M$ is open in $\mathbb{R}^k$ The derivative of $f$ in the direction $h \in \mathbb{R}^k$, taken at the point $x \in M$ is defined by the conventional limit $$df_x(h) = \lim_{t \to 0} \frac{f(x + th) - f(x)}{t}$$ Now with $x$ fixed, we define $$df_x : \mathbb{R^k} \to \mathbb{R^l}$$ by assigning to each vector $h \in \mathbb{R}^k$ the directional derivative $df_x(h) \in \mathbb{R}^l$, this map is called the derivative of $f$ at $x$. Case 2: $M$ is not open in $\mathbb{R}^k$ We do know that $f$ is smooth, hence for each $x \in M$, there is an open set $W \subseteq \mathbb{R}^k$ and a smooth map $F : W \to \mathbb{R}^l$, such that $F = f$ on $W \cap M$ In this case I'm not even sure how one defines the derivative of $f$ at $x$ Now in Case 1 above, clearly $\mathbb{R}^m \not\cong \mathbb{R}^k$ and $\mathbb{R}^n \not\cong \mathbb{R}^l$, unless $m = k$ and $n = l$, so how can the two definitons of the derivative of $f$ at $x$ agree for this case. Furthermore I'm not even sure how Case 2 one can define the derivative of $f$ at $x$. So how can one prove that the two definitions of the derivative are equivalent?",,"['general-topology', 'derivatives', 'differential-geometry', 'differential-topology']"
61,Inverse image of submanifold is not a submanifold,Inverse image of submanifold is not a submanifold,,"Someone asked me the following question yesterday: Let $X$ and $Y$ be to manifolds, $S$ be a submanifold of $Y$ and $f \colon X \to Y$ be differentiable. Prove or disprove that $f^{-1}(S)$ a submanifold. I came up with the following counterexample: Let $X=Y=\mathbb{R}^2$ and $S$ be the unit circle. Define $f \colon X \to Y$ as $$f(x,y)=\begin{cases}(1,0)&\text{ if }x>1,y=0\\(x,y), &\text{ otherwise }\end{cases}$$ Then $f^{-1}(S)=S \cup \{(x,0)\colon x>1\}$ which is not a submanifold. The point where I'm unsure is whether $f$ is differentiable. We only need to check it at $(1,0)$ and I think it should be differentiable but I'm not $100\%$ sure.  Am I correct? If yes, how do I show differentiability at $(1,0)$? Thanks in advance for any hints.","Someone asked me the following question yesterday: Let $X$ and $Y$ be to manifolds, $S$ be a submanifold of $Y$ and $f \colon X \to Y$ be differentiable. Prove or disprove that $f^{-1}(S)$ a submanifold. I came up with the following counterexample: Let $X=Y=\mathbb{R}^2$ and $S$ be the unit circle. Define $f \colon X \to Y$ as $$f(x,y)=\begin{cases}(1,0)&\text{ if }x>1,y=0\\(x,y), &\text{ otherwise }\end{cases}$$ Then $f^{-1}(S)=S \cup \{(x,0)\colon x>1\}$ which is not a submanifold. The point where I'm unsure is whether $f$ is differentiable. We only need to check it at $(1,0)$ and I think it should be differentiable but I'm not $100\%$ sure.  Am I correct? If yes, how do I show differentiability at $(1,0)$? Thanks in advance for any hints.",,"['derivatives', 'manifolds']"
62,Derivative of power sum?,Derivative of power sum?,,"In one part of a proof I'm reading I see the following: $$ p\frac{d}{dp}\sum_{k=0}^\infty p^k = p\frac{1}{1-p} $$ However, I'm confused how the derivative of $\frac{d}{dp}\sum_{k=0}^\infty p^k = \frac{1}{1-p}$, wouldn't it equal $\frac{1}{(1-p)^2}$? Here's a picture of the actual two lines as I've stripped them to the part I'm confused about: https://i.sstatic.net/MkqSQ.jpg","In one part of a proof I'm reading I see the following: $$ p\frac{d}{dp}\sum_{k=0}^\infty p^k = p\frac{1}{1-p} $$ However, I'm confused how the derivative of $\frac{d}{dp}\sum_{k=0}^\infty p^k = \frac{1}{1-p}$, wouldn't it equal $\frac{1}{(1-p)^2}$? Here's a picture of the actual two lines as I've stripped them to the part I'm confused about: https://i.sstatic.net/MkqSQ.jpg",,"['derivatives', 'summation']"
63,Global Injectivity Of A Multi Variable Function,Global Injectivity Of A Multi Variable Function,,"Let $f:A\subseteq \Bbb R^n\to\Bbb R^n$ be differentiable and $Df(x)$ denote the Jacobian of $f$ at $x$. If, $$Df(x)\neq 0,\;\;\;\forall x\in \Bbb R^n-------(*)$$ Then by Inverse Function Theorem we can conclude that, $\forall x \in \Bbb R^n \;\;\exists $ a neighborhood of $x$ in which $f$ is invertible, also $f$ is injective therein. My question is- Does $(*)\implies$ $f$ is injective on $A$? In general how do we check whether $f$ is injective or surjective on $A$ ? Are we supposed to simply use the definitions of injectivity and surjectivity or can we make use of the values of Jacobian to conclude something?","Let $f:A\subseteq \Bbb R^n\to\Bbb R^n$ be differentiable and $Df(x)$ denote the Jacobian of $f$ at $x$. If, $$Df(x)\neq 0,\;\;\;\forall x\in \Bbb R^n-------(*)$$ Then by Inverse Function Theorem we can conclude that, $\forall x \in \Bbb R^n \;\;\exists $ a neighborhood of $x$ in which $f$ is invertible, also $f$ is injective therein. My question is- Does $(*)\implies$ $f$ is injective on $A$? In general how do we check whether $f$ is injective or surjective on $A$ ? Are we supposed to simply use the definitions of injectivity and surjectivity or can we make use of the values of Jacobian to conclude something?",,"['calculus', 'derivatives', 'jacobian']"
64,Proving an infinite series is differentiable,Proving an infinite series is differentiable,,"I'm trying to prove that the following series is differentiable with respect to $x$ on any interval $[a,b]$ with $0<a<b$: $$f(x)=\sum_{n=1}^\infty \frac{e^{-nx}}{n}$$ What I've tried is applying the Term-wise Differentiability Theorem, where I need to show that the following three conditions hold: Each $f_n$ is differentiable on $[a,b]$ $\sum_{i=1}^n f_i'$ converges to $g$ uniformly on $[a,b]$ (This is where I really get stuck) $\sum_{i=1}^n f_i(x_0)$ converges for some $x_0$ on [a,b] I already get stuck on choosing $f_n$ and going on with step 1 and 2, and I don't know of any other way to show that the infinite series is differentiable with my current knowledge, so I hope someone could help me out.","I'm trying to prove that the following series is differentiable with respect to $x$ on any interval $[a,b]$ with $0<a<b$: $$f(x)=\sum_{n=1}^\infty \frac{e^{-nx}}{n}$$ What I've tried is applying the Term-wise Differentiability Theorem, where I need to show that the following three conditions hold: Each $f_n$ is differentiable on $[a,b]$ $\sum_{i=1}^n f_i'$ converges to $g$ uniformly on $[a,b]$ (This is where I really get stuck) $\sum_{i=1}^n f_i(x_0)$ converges for some $x_0$ on [a,b] I already get stuck on choosing $f_n$ and going on with step 1 and 2, and I don't know of any other way to show that the infinite series is differentiable with my current knowledge, so I hope someone could help me out.",,"['real-analysis', 'sequences-and-series', 'derivatives']"
65,Application of limits on individual terms,Application of limits on individual terms,,"While evaluating limits, when (and when not) can I apply limits to the individual terms of an expression?","While evaluating limits, when (and when not) can I apply limits to the individual terms of an expression?",,"['calculus', 'limits', 'derivatives', 'soft-question']"
66,Diameter cutting x Center mass,Diameter cutting x Center mass,,"I'm trying to set the ratio between the variation of the diameter cutting in the region lower a sphere with respect to distance from the center of mass with the center of the radius of the sphere. For example, if the diameter is zero the center of mass is zero. If the diameter lower is equal to the diameter of the sphere, the Center of mass is $1.875$. I have this equation, but this is relative to $H$. Using trigonometry could find this reason. But was thinking to have a answer in terms of integration or derivation. I wanted to understand how was obtained the above equation. I believe that has been by integration. Something like this: Equation","I'm trying to set the ratio between the variation of the diameter cutting in the region lower a sphere with respect to distance from the center of mass with the center of the radius of the sphere. For example, if the diameter is zero the center of mass is zero. If the diameter lower is equal to the diameter of the sphere, the Center of mass is $1.875$. I have this equation, but this is relative to $H$. Using trigonometry could find this reason. But was thinking to have a answer in terms of integration or derivation. I wanted to understand how was obtained the above equation. I believe that has been by integration. Something like this: Equation",,"['integration', 'derivatives']"
67,Approximation of the first derivative by writing Taylor expansions,Approximation of the first derivative by writing Taylor expansions,,"What is the most accurate approximation you can write for $f'(x)$ using the values $f(x-2h)$, $f(x)$ and $f(x+4h)$? What is the order of this approximation? Note: I know how to approximate $f'(x)$ with these given values by using Taylor expansion but I couldn't understand how to find the most accurate one.","What is the most accurate approximation you can write for $f'(x)$ using the values $f(x-2h)$, $f(x)$ and $f(x+4h)$? What is the order of this approximation? Note: I know how to approximate $f'(x)$ with these given values by using Taylor expansion but I couldn't understand how to find the most accurate one.",,"['derivatives', 'numerical-methods', 'taylor-expansion', 'approximation']"
68,"Show that the map $B(f) = f^{\prime}(0)$ defines a linear functional on the normed vector space $(C^{(1)}[0,1], \| \cdot \|)$ that is not bounded.",Show that the map  defines a linear functional on the normed vector space  that is not bounded.,"B(f) = f^{\prime}(0) (C^{(1)}[0,1], \| \cdot \|)","Let $C^{(1)}[0,1]$ be the space of all continuous functions on $[0,1]$   that have continuous derivative. (This space is called the space of   continuously differentiable functions.)   Define a norm $\|\cdot \|$   on $C^{(1)}[0,1]$ by $\| f \|=\max _{t \in T}|f(t)|.$   Show that the   map   $$B(f) = f^{\prime}(0)$$  defines a linear functional on the   normed vector space $(C^{(1)}[0,1], \| \cdot \|)$ that is not bounded. My attempt: For any $f,g \in C^{(1)}[0,1],$ we have $B(f+g) = (f + g)^{\prime}(0) = f^{\prime}(0) + g^{\prime}(0) = B(f) + B(g).$     Therefore, $B$ is a linear functional. To show that $B$ is not bounded, we wish to show that for any $M > 0$, there exists $f \in C^{(1)}[0,1]$ such that $\| f \| \leq 1$ but $|B(f)| > M.$ Let $M > 0 $ be given. I have trouble constructing a function with maximum $\leq1$ and has derivative at $x = 0$ greater than $M.$ I plan to construct $f$ which looks similar to $\sin(x)$. For any given $M > 0$, I can draw $\sin(x)$ such that it is above the line $y = Mx$. However, I do not know how to formulate this phenomena using mathematical symbols.","Let $C^{(1)}[0,1]$ be the space of all continuous functions on $[0,1]$   that have continuous derivative. (This space is called the space of   continuously differentiable functions.)   Define a norm $\|\cdot \|$   on $C^{(1)}[0,1]$ by $\| f \|=\max _{t \in T}|f(t)|.$   Show that the   map   $$B(f) = f^{\prime}(0)$$  defines a linear functional on the   normed vector space $(C^{(1)}[0,1], \| \cdot \|)$ that is not bounded. My attempt: For any $f,g \in C^{(1)}[0,1],$ we have $B(f+g) = (f + g)^{\prime}(0) = f^{\prime}(0) + g^{\prime}(0) = B(f) + B(g).$     Therefore, $B$ is a linear functional. To show that $B$ is not bounded, we wish to show that for any $M > 0$, there exists $f \in C^{(1)}[0,1]$ such that $\| f \| \leq 1$ but $|B(f)| > M.$ Let $M > 0 $ be given. I have trouble constructing a function with maximum $\leq1$ and has derivative at $x = 0$ greater than $M.$ I plan to construct $f$ which looks similar to $\sin(x)$. For any given $M > 0$, I can draw $\sin(x)$ such that it is above the line $y = Mx$. However, I do not know how to formulate this phenomena using mathematical symbols.",,"['functional-analysis', 'derivatives', 'continuity']"
69,What does it mean for a function to be differentiable?,What does it mean for a function to be differentiable?,,What does it mean for the derivative of a function to exist at every point on the function's domain? It seems a very abstract thing to visualize. Can someone elaborate?  Answers are much appreciated.,What does it mean for the derivative of a function to exist at every point on the function's domain? It seems a very abstract thing to visualize. Can someone elaborate?  Answers are much appreciated.,,"['calculus', 'derivatives']"
70,$n^{th}$ derivative of $y=(x \sqrt{1+x^2})^m$,derivative of,n^{th} y=(x \sqrt{1+x^2})^m,"I was trying to find the $n^{th}$ derivative and $y_n(0)$(nth derivative at zero) of the function  $$y=(x \sqrt{1+x^2})^m$$  Here is my approach: $$y^2=(x^2 (1+x^2)^m$$ $$2yy_1=m(x^2 (1+x^2))^{m-1} \cdot(2x+4x^3)$$ $$2yy_1=m\frac{(x^2 (1+x^2))^{m}}{(x^2 (1+x^2)} \cdot(2x+4x^3)$$ $$2yy_1=m\frac{y^2}{(x^2 (1+x^2)} \cdot(2x+4x^3)$$ $$2yy_1((x^2 (1+x^2))=my^2(2x+4x^3)$$ $$y_1(x^2(1+x^2))=my(x+2x^3)$$ Derivating again I can write $$y_2(x(1+x^2))+y_1(1+3x^2))=my_1(x+2x^3) + my(1+6x^2)$$ $$y_2(x(1+x^2))+y_1((1+3x^2)-m(x+2x^3))-my(1+6x^2)=0$$ Now my book says that the answer is  $$y_n(0)=m^2(m^2-2^2)(m^2-4^2).......[m^2-(2n-2)^2]; n=odd$$ $$y_n(0)=m^2(m^2-1^2)(m^2-3^2).......[m^2-(2n-1)^2]; n=even$$ But after applying Leibnitz theorem for the derivative, I don't end up with anything like this. Can anybody please tell me how to approach this question.","I was trying to find the $n^{th}$ derivative and $y_n(0)$(nth derivative at zero) of the function  $$y=(x \sqrt{1+x^2})^m$$  Here is my approach: $$y^2=(x^2 (1+x^2)^m$$ $$2yy_1=m(x^2 (1+x^2))^{m-1} \cdot(2x+4x^3)$$ $$2yy_1=m\frac{(x^2 (1+x^2))^{m}}{(x^2 (1+x^2)} \cdot(2x+4x^3)$$ $$2yy_1=m\frac{y^2}{(x^2 (1+x^2)} \cdot(2x+4x^3)$$ $$2yy_1((x^2 (1+x^2))=my^2(2x+4x^3)$$ $$y_1(x^2(1+x^2))=my(x+2x^3)$$ Derivating again I can write $$y_2(x(1+x^2))+y_1(1+3x^2))=my_1(x+2x^3) + my(1+6x^2)$$ $$y_2(x(1+x^2))+y_1((1+3x^2)-m(x+2x^3))-my(1+6x^2)=0$$ Now my book says that the answer is  $$y_n(0)=m^2(m^2-2^2)(m^2-4^2).......[m^2-(2n-2)^2]; n=odd$$ $$y_n(0)=m^2(m^2-1^2)(m^2-3^2).......[m^2-(2n-1)^2]; n=even$$ But after applying Leibnitz theorem for the derivative, I don't end up with anything like this. Can anybody please tell me how to approach this question.",,"['calculus', 'derivatives']"
71,Expressing Derivative of Cable Tension Component,Expressing Derivative of Cable Tension Component,,"I am looking at Hibbeler's section on a `Cable Subjected to a Distributed Load' in Engineering Mechanics - Statics, Ed13, as shown in the attached images. I am getting stuck deriving equation (7-7). I derive the horizontal equilibrium equation just fine; $$ |\vec{T} + \vec{\Delta T}| \cdot \cos(\theta + \Delta \theta) - |\vec{T}|\cdot \cos(\theta) = 0 $$ I rearrange to describe horizontal tension component as a function of $\Delta x$ (given $\Delta T$ and $\Delta \theta$ are ultimately functions of $\Delta x$); $$ |\vec{T}|\cdot \cos(\theta) = |\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta) $$ Then I express the differential; $$ \frac{\Delta(|\vec{T}|\cdot \cos(\theta))}{\Delta x} = \frac{|\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta)}{\Delta x} $$ Then when I go to express the derivative, I come up with a limit which does not exist. $$ \frac{d(|\vec{T}|\cdot \cos(\theta))}{dx} = \lim_{\Delta x \to 0}\frac{|\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta)}{\Delta x} $$ Any pointers as to where my error is? Appreciate any advice offered. Thanks.","I am looking at Hibbeler's section on a `Cable Subjected to a Distributed Load' in Engineering Mechanics - Statics, Ed13, as shown in the attached images. I am getting stuck deriving equation (7-7). I derive the horizontal equilibrium equation just fine; $$ |\vec{T} + \vec{\Delta T}| \cdot \cos(\theta + \Delta \theta) - |\vec{T}|\cdot \cos(\theta) = 0 $$ I rearrange to describe horizontal tension component as a function of $\Delta x$ (given $\Delta T$ and $\Delta \theta$ are ultimately functions of $\Delta x$); $$ |\vec{T}|\cdot \cos(\theta) = |\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta) $$ Then I express the differential; $$ \frac{\Delta(|\vec{T}|\cdot \cos(\theta))}{\Delta x} = \frac{|\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta)}{\Delta x} $$ Then when I go to express the derivative, I come up with a limit which does not exist. $$ \frac{d(|\vec{T}|\cdot \cos(\theta))}{dx} = \lim_{\Delta x \to 0}\frac{|\vec{T} + \vec{\Delta T}|\cdot \cos(\theta + \Delta \theta)}{\Delta x} $$ Any pointers as to where my error is? Appreciate any advice offered. Thanks.",,"['calculus', 'derivatives']"
72,Volume by traversed triangle,Volume by traversed triangle,,"Given three smooth functions $f,g,h:\mathbb{R}\to\mathbb{R}^3$ how would one find the volume traversed by the triangle $\triangle(f(t),g(t),h(t))$ from $t=a$ to $t=b$? Assume that all triangles $\triangle(f,g,h)$ are distinct (i.e., there are no intersections between the areas of any two triangles $\triangle(f(s),g(s),h(s))$ and $\triangle(f(t),g(t),h(t))$ where $s\ne t$).","Given three smooth functions $f,g,h:\mathbb{R}\to\mathbb{R}^3$ how would one find the volume traversed by the triangle $\triangle(f(t),g(t),h(t))$ from $t=a$ to $t=b$? Assume that all triangles $\triangle(f,g,h)$ are distinct (i.e., there are no intersections between the areas of any two triangles $\triangle(f(s),g(s),h(s))$ and $\triangle(f(t),g(t),h(t))$ where $s\ne t$).",,"['calculus', 'integration', 'geometry', 'derivatives', 'volume']"
73,"Show that $f(x) = x\ln(x)$ is continuous on $(0,1)$",Show that  is continuous on,"f(x) = x\ln(x) (0,1)","The easiest thing I can think of is to compute the derivative which comes out to be $ f'(x) = \ln(x) + 1 $. Since this is defined for all values on the interval $(0,1)$, the original function must be continuous. However, I find $\epsilon - \delta$ proofs of continuity to be more... beautiful and elegant (and hard for me to get right). My attempt at that approach was this: $$ |f(x) - f(k)| = |x\ln(x) - k\ln(k)| = |x\ln(x) - k\ln(x) + k\ln(x) - k\ln(k)| \\ \leq |x\ln(x) - k\ln(x)||k\ln(x) - k\ln(k)| \leq |x - k||\ln(x)||k||\ln(x) - \ln(k)| \\ \leq  |x - k||\ln(x)||\ln(x) - \ln(k)| $$ But I get stuck here. Is the derivative method okay? And what could I do to make the other method work? Thank you for your input!","The easiest thing I can think of is to compute the derivative which comes out to be $ f'(x) = \ln(x) + 1 $. Since this is defined for all values on the interval $(0,1)$, the original function must be continuous. However, I find $\epsilon - \delta$ proofs of continuity to be more... beautiful and elegant (and hard for me to get right). My attempt at that approach was this: $$ |f(x) - f(k)| = |x\ln(x) - k\ln(k)| = |x\ln(x) - k\ln(x) + k\ln(x) - k\ln(k)| \\ \leq |x\ln(x) - k\ln(x)||k\ln(x) - k\ln(k)| \leq |x - k||\ln(x)||k||\ln(x) - \ln(k)| \\ \leq  |x - k||\ln(x)||\ln(x) - \ln(k)| $$ But I get stuck here. Is the derivative method okay? And what could I do to make the other method work? Thank you for your input!",,"['derivatives', 'continuity', 'epsilon-delta']"
74,Why I can't understand the derivative (with example). Can someone help me understand?,Why I can't understand the derivative (with example). Can someone help me understand?,,"The derivative of a function is said to be $\lim_{\Delta{x}\to 0} \frac{f(\Delta{x}+x)-f(x)}{\Delta{x}}$. This I believe I understand. But, I always have trouble believing something that is so theoretical without walking through a concrete example. So, I took the derivative of the function $f(x)=x^2+2x+4$ and I, of course, got $f'(x)=2x+2$. So, this says that for a marginal change in $x$, there will be a change in $y$ of $2(1)+2=4$. I evaluate the original function at $x=1$ and I find that $y=7$. Then I evaluate the original function at $x=2$ and I find that $y=12$. In this case $\frac{\Delta{y}}{\Delta{x}}=5$, not the $4$ predicted by our derivative function. Where is the mistake in my thinking? Can someone provide me with a concrete example of the derivative that will show me that it does work?","The derivative of a function is said to be $\lim_{\Delta{x}\to 0} \frac{f(\Delta{x}+x)-f(x)}{\Delta{x}}$. This I believe I understand. But, I always have trouble believing something that is so theoretical without walking through a concrete example. So, I took the derivative of the function $f(x)=x^2+2x+4$ and I, of course, got $f'(x)=2x+2$. So, this says that for a marginal change in $x$, there will be a change in $y$ of $2(1)+2=4$. I evaluate the original function at $x=1$ and I find that $y=7$. Then I evaluate the original function at $x=2$ and I find that $y=12$. In this case $\frac{\Delta{y}}{\Delta{x}}=5$, not the $4$ predicted by our derivative function. Where is the mistake in my thinking? Can someone provide me with a concrete example of the derivative that will show me that it does work?",,"['calculus', 'derivatives']"
75,Jacobian of a dot product,Jacobian of a dot product,,"An online video showed me that: $$ J(a \cdot b) = a^{T} J(b) + b^{T} J(a) $$ for a,b being vectors. But my question is: isn't the dot product a scalar? So isn't the jacobian of a scalar just 0? Video for reference: https://www.youtube.com/watch?v=r8oIjsBBljA at 3:00","An online video showed me that: $$ J(a \cdot b) = a^{T} J(b) + b^{T} J(a) $$ for a,b being vectors. But my question is: isn't the dot product a scalar? So isn't the jacobian of a scalar just 0? Video for reference: https://www.youtube.com/watch?v=r8oIjsBBljA at 3:00",,"['derivatives', 'jacobian']"
76,Derivative of Newton iteration in Banach spaces,Derivative of Newton iteration in Banach spaces,,"I'm studying the Newton's method on Banach spaces and having some trouble to understand its derivative. Given two Banach spaces $\mathbb{E}, \mathbb{F}$ and a smooth map $f:\mathbb{E} \to \mathbb{F}$, a Newton's iteration of $f$ is the map $N_f:\mathbb{E}\backslash\mathcal{S} \to \mathbb{E}$ such that $$N_f(x) = x - (Df(x))^{-1}f(x),$$ where $\mathcal{S}$ is the set of points $x \in \mathbb{E}$ such that $Df(x)$ is not invertible. After that, I know that if $x \in \mathbb{E}\backslash\mathcal{S}$ is a zero of $f$, then $DN_f(x) = 0$. I don't have a proof of this fact, they (the people of my book) just show this for complex functions and say it is the same thing for Banach spaces. I tried to prove and doesn't looks the same thing. Here are my thoughts: I can define the map $D_f^{-1}:\mathbb{E}\backslash\mathcal{S} \to \mathcal{L}(\mathbb{F},\mathbb{E})$ such that $D_f^{-1}(x) = (Df(x))^{-1}$. Doing this I can informally write $$N_f = I - D_f^{-1}\cdot f,$$ where $I$ stands for the identity map. If I just follow what they say and treat $N_f$ as an ordinary complex function, then $$DN_f = I - (-1)D_f^{-2}\cdot D(D_f^{-1})\cdot f - D_f^{-1}\cdot D_f = $$ $$ = I + D_f^{-2}\cdot D(D_f^{-1})\cdot f - I = D_f^{-2}\cdot D(D_f^{-1})\cdot f.$$ Note that I used the chain rule and the product rule in the usual way, which supposedly I'm allowed to do. Now we substitute to get $DN_f(x) = D_f^{-2}(x)\cdot D(D_f^{-1})(x)f(x) = D_f^{-2}(x)\cdot D(D_f^{-1})(x)\cdot 0 = 0$, as claimed. I have two questions: 1) Are my calculations correct (do they make sense)? 2) What is $D_f^{-2}$ and $D(D_f^{-1})$ ? Even if my calculations are ok, I just don't know what are these maps. I don't know how to interpret it. Thanks.","I'm studying the Newton's method on Banach spaces and having some trouble to understand its derivative. Given two Banach spaces $\mathbb{E}, \mathbb{F}$ and a smooth map $f:\mathbb{E} \to \mathbb{F}$, a Newton's iteration of $f$ is the map $N_f:\mathbb{E}\backslash\mathcal{S} \to \mathbb{E}$ such that $$N_f(x) = x - (Df(x))^{-1}f(x),$$ where $\mathcal{S}$ is the set of points $x \in \mathbb{E}$ such that $Df(x)$ is not invertible. After that, I know that if $x \in \mathbb{E}\backslash\mathcal{S}$ is a zero of $f$, then $DN_f(x) = 0$. I don't have a proof of this fact, they (the people of my book) just show this for complex functions and say it is the same thing for Banach spaces. I tried to prove and doesn't looks the same thing. Here are my thoughts: I can define the map $D_f^{-1}:\mathbb{E}\backslash\mathcal{S} \to \mathcal{L}(\mathbb{F},\mathbb{E})$ such that $D_f^{-1}(x) = (Df(x))^{-1}$. Doing this I can informally write $$N_f = I - D_f^{-1}\cdot f,$$ where $I$ stands for the identity map. If I just follow what they say and treat $N_f$ as an ordinary complex function, then $$DN_f = I - (-1)D_f^{-2}\cdot D(D_f^{-1})\cdot f - D_f^{-1}\cdot D_f = $$ $$ = I + D_f^{-2}\cdot D(D_f^{-1})\cdot f - I = D_f^{-2}\cdot D(D_f^{-1})\cdot f.$$ Note that I used the chain rule and the product rule in the usual way, which supposedly I'm allowed to do. Now we substitute to get $DN_f(x) = D_f^{-2}(x)\cdot D(D_f^{-1})(x)f(x) = D_f^{-2}(x)\cdot D(D_f^{-1})(x)\cdot 0 = 0$, as claimed. I have two questions: 1) Are my calculations correct (do they make sense)? 2) What is $D_f^{-2}$ and $D(D_f^{-1})$ ? Even if my calculations are ok, I just don't know what are these maps. I don't know how to interpret it. Thanks.",,"['derivatives', 'banach-spaces', 'newton-raphson']"
77,Does the distributional derivative of the Dirac Comb have the same properties as a single Dirac Delta?,Does the distributional derivative of the Dirac Comb have the same properties as a single Dirac Delta?,,"A single Dirac delta has a distributional derivative $\delta'$ defined in the sense that $$ \int_{-\infty}^{\infty} \textrm{d}x \, \delta'(x) \, \phi(x) = -\int_{-\infty}^{\infty} \textrm{d}x \, \delta(x) \, \phi'(x) \, . $$ I understand this comes from the integral of the total derivative of $\delta(x) \, \phi(x)$ vanishing, $$ \int_{-\infty}^{\infty} \textrm{d}x \, \frac{\textrm{d}}{\textrm{d} x} \big( \delta(x) \, \phi(x) \big) = \delta(x) \, \phi(x) \, \big|_{-\infty}^{\infty} = 0 \, , $$ which means the product rule for differentation gives the above expression defining $\delta'$. If we instead consider the Dirac comb $$ \mathrm{III}(t) = \sum_{k=-\infty}^{\infty} \, \delta(x - kT) \, , $$ which is periodic with period $T$, is the same total derivative trick valid - $$ \int_{-\infty}^{\infty} \textrm{d}x \, \frac{\textrm{d}}{\textrm{d} x} \big( \textrm{III}(x) \, \phi(x) \big) = 0 \, ? $$ Allowing us to define the distributional derivative of the Dirac comb analogously to the Dirac delta $$ \int_{-\infty}^{\infty} \textrm{d}x \, \textrm{III}'(x) \, \phi(x) = -\int_{-\infty}^{\infty} \textrm{d}x \, \textrm{III}(x) \, \phi'(x) \, . $$","A single Dirac delta has a distributional derivative $\delta'$ defined in the sense that $$ \int_{-\infty}^{\infty} \textrm{d}x \, \delta'(x) \, \phi(x) = -\int_{-\infty}^{\infty} \textrm{d}x \, \delta(x) \, \phi'(x) \, . $$ I understand this comes from the integral of the total derivative of $\delta(x) \, \phi(x)$ vanishing, $$ \int_{-\infty}^{\infty} \textrm{d}x \, \frac{\textrm{d}}{\textrm{d} x} \big( \delta(x) \, \phi(x) \big) = \delta(x) \, \phi(x) \, \big|_{-\infty}^{\infty} = 0 \, , $$ which means the product rule for differentation gives the above expression defining $\delta'$. If we instead consider the Dirac comb $$ \mathrm{III}(t) = \sum_{k=-\infty}^{\infty} \, \delta(x - kT) \, , $$ which is periodic with period $T$, is the same total derivative trick valid - $$ \int_{-\infty}^{\infty} \textrm{d}x \, \frac{\textrm{d}}{\textrm{d} x} \big( \textrm{III}(x) \, \phi(x) \big) = 0 \, ? $$ Allowing us to define the distributional derivative of the Dirac comb analogously to the Dirac delta $$ \int_{-\infty}^{\infty} \textrm{d}x \, \textrm{III}'(x) \, \phi(x) = -\int_{-\infty}^{\infty} \textrm{d}x \, \textrm{III}(x) \, \phi'(x) \, . $$",,"['derivatives', 'fourier-series', 'distribution-theory', 'dirac-delta']"
78,Determine the acceleration given the position function of a particle,Determine the acceleration given the position function of a particle,,The position function of a particle in a test laboratory is $s(t) = \frac{10t} {t^2+3}$.   Determine the acceleration of the particle of particle after 4 seconds. The speed $v(t)$ of the particle is the derivative of the position $\frac{ds(t)}{dt}$. I got $v(t) = \frac{10(-t^2+3)}{(t^2+3)^2}$ For the acceleration $a(t)$ I tried taking the derivative of $v(t)$ but got the wrong answer at $a(4)$. $$a(t)=\frac{-2t\cdot (t^3+3)^2-10(-t^2+3)\cdot 2(t^2+3)\cdot(2t)}{(t^2+3)^4}$$,The position function of a particle in a test laboratory is $s(t) = \frac{10t} {t^2+3}$.   Determine the acceleration of the particle of particle after 4 seconds. The speed $v(t)$ of the particle is the derivative of the position $\frac{ds(t)}{dt}$. I got $v(t) = \frac{10(-t^2+3)}{(t^2+3)^2}$ For the acceleration $a(t)$ I tried taking the derivative of $v(t)$ but got the wrong answer at $a(4)$. $$a(t)=\frac{-2t\cdot (t^3+3)^2-10(-t^2+3)\cdot 2(t^2+3)\cdot(2t)}{(t^2+3)^4}$$,,"['calculus', 'derivatives']"
79,Three Distinct Points and Their Normal Lines,Three Distinct Points and Their Normal Lines,,"Suppose That three points on the graph of $y=x^2$ have the property that their normal lines intersect at a common point. Show that the sum of their $x$-coordinates is $0$. I have a lot going but can not finish it. Proof: Let $(a,a^2)$, $(b,b^2)$, and $(c,c^2)$ be three distinct points on $y=x^2$ such that $a\not=b\not=c$. Find the tangent slope and the slope of their normal lines. $$(a,a^2) \hspace{4mm}m_{tan}=2a, \hspace{4mm} m_{norm}=\frac{-1}{2a}$$ $$(b,b^2) \hspace{4mm}m_{tan}=2b, \hspace{4mm} m_{norm}=\frac{-1}{2b}$$ $$(c,c^2) \hspace{4mm}m_{tan}=2c, \hspace{4mm} m_{norm}=\frac{-1}{2c}$$ Normal Line $(a,a^2)$ $y-a^2=-\frac{1}{2a}(x-a) \implies y=-\frac{1}{2a}x+\frac{1}{2}+a^2$ Normal Line $(b,b^2)$ $y-b^2=-\frac{1}{2b}(x-b) \implies y=-\frac{1}{2b}x+\frac{1}{2}+b^2$ Normal Line $(c,c^2)$ $y-c^2=-\frac{1}{2c}(x-c) \implies y=-\frac{1}{2c}x+\frac{1}{2}+c^2$ Find the $x$-interception points between the normal lines of $(a,a^2)$ and $(b,b^2)$. $-\frac{1}{2a}x+\frac{1}{2}+a^2=-\frac{1}{2b}x+\frac{1}{2}+b^2 \implies x=-(b+a)2ab$ Find the $x$-interception points between the normal lines of $(a,a^2)$ and $(c,c^2)$. $-\frac{1}{2a}x+\frac{1}{2}+a^2=-\frac{1}{2c}x+\frac{1}{2}+c^2 \implies x=-(c+a)2ac$ Find the $x$-interception points between the normal lines of $(b,b^2)$ and $(c,c^2)$. $-\frac{1}{2b}x+\frac{1}{2}+b^2=-\frac{1}{2c}x+\frac{1}{2}+c^2 \implies x=-(c+b)2bc$ Show that $a+b+c=0$ $$\begin{align} ....\\ ....\\ ....\\ \end{align}$$ I do not know how to show that $a+b+c=0$. Any advice on how to continue? Thanks in advance!","Suppose That three points on the graph of $y=x^2$ have the property that their normal lines intersect at a common point. Show that the sum of their $x$-coordinates is $0$. I have a lot going but can not finish it. Proof: Let $(a,a^2)$, $(b,b^2)$, and $(c,c^2)$ be three distinct points on $y=x^2$ such that $a\not=b\not=c$. Find the tangent slope and the slope of their normal lines. $$(a,a^2) \hspace{4mm}m_{tan}=2a, \hspace{4mm} m_{norm}=\frac{-1}{2a}$$ $$(b,b^2) \hspace{4mm}m_{tan}=2b, \hspace{4mm} m_{norm}=\frac{-1}{2b}$$ $$(c,c^2) \hspace{4mm}m_{tan}=2c, \hspace{4mm} m_{norm}=\frac{-1}{2c}$$ Normal Line $(a,a^2)$ $y-a^2=-\frac{1}{2a}(x-a) \implies y=-\frac{1}{2a}x+\frac{1}{2}+a^2$ Normal Line $(b,b^2)$ $y-b^2=-\frac{1}{2b}(x-b) \implies y=-\frac{1}{2b}x+\frac{1}{2}+b^2$ Normal Line $(c,c^2)$ $y-c^2=-\frac{1}{2c}(x-c) \implies y=-\frac{1}{2c}x+\frac{1}{2}+c^2$ Find the $x$-interception points between the normal lines of $(a,a^2)$ and $(b,b^2)$. $-\frac{1}{2a}x+\frac{1}{2}+a^2=-\frac{1}{2b}x+\frac{1}{2}+b^2 \implies x=-(b+a)2ab$ Find the $x$-interception points between the normal lines of $(a,a^2)$ and $(c,c^2)$. $-\frac{1}{2a}x+\frac{1}{2}+a^2=-\frac{1}{2c}x+\frac{1}{2}+c^2 \implies x=-(c+a)2ac$ Find the $x$-interception points between the normal lines of $(b,b^2)$ and $(c,c^2)$. $-\frac{1}{2b}x+\frac{1}{2}+b^2=-\frac{1}{2c}x+\frac{1}{2}+c^2 \implies x=-(c+b)2bc$ Show that $a+b+c=0$ $$\begin{align} ....\\ ....\\ ....\\ \end{align}$$ I do not know how to show that $a+b+c=0$. Any advice on how to continue? Thanks in advance!",,"['calculus', 'geometry', 'derivatives', 'proof-writing', 'conic-sections']"
80,Find the derivative of $\frac{d}{dt} (\int_0^t xf(x)dx)$,Find the derivative of,\frac{d}{dt} (\int_0^t xf(x)dx),"I was going through the solved example 2.d in Sheldon Ross ""A First Course in Probability 8th Edition"" Chapter 5, and i got stuck on a reduction involving calculus differentiation. with regards to specifics of the problem as per the equation below, x and t are non-negative variables, and c, k are constants. f(x) represents probability density function, F(x) represents the cumulative probability density function. $$ ExpectedCost = ct\int_0^t f(x)dx - c\int_0^t xf(x)dx + k\int_t^\infty xf(x)dx -kt\int_t^\infty f(x)dx $$ The step i am unable to comprehend is the derivative of $ExpectedCost$ with respect to t, which, as per the book, yields: $$ \frac{d}{dt} (ExpectedCost) = ctf(t) + cF(t) - ctf(t) - ktf(t) + ktf(t) - k[1-F(t)] $$ My own calculations differ from those of the book: Integral#1: $$ \frac{d}{dt} (\int_0^t f(x)dx) = f(t) $$ Integral#2: $$ \frac{d}{dt} (\int_t^\infty f(x)dx) = -f(t) $$ Integral#3: $$ \frac{d}{dt} (\int_0^t xf(x)dx) \\ = \frac{d}{dt} (xF(x)|_0^t - \int_0^t F(x)dx) \\ = \frac{d}{dt} (xF(x)|_0^t) - \frac{d}{dt} (\int_0^t F(x)dx) \\ = \frac{d}{dt} (tF(t)) - \frac{d}{dt} (\int_0^t F(x)dx) \\ = F(t) + tf(t) - F(t) \\ = tf(t) $$ Integral#4: $$ \frac{d}{dt} (\int_t^\infty xf(x)dx) \\ = \frac{d}{dt} (xF(x)|_t^\infty - \int_t^\infty F(x)dx) \\ = \frac{d}{dt} (xF(x)|_t^\infty) - \frac{d}{dt} (\int_t^\infty F(x)dx) \\ = \frac{d}{dt} (-tF(t)) - \frac{d}{dt} (\int_t^\infty F(x)dx) \\ = -F(t) -tf(t) - (-F(t)) \\ = -tf(t) $$ As per my calculations, the derivative of the $ExpectedCost$ with respect to t, comes to zero. My guess is that i am going wrong with regards to the calculation of derivatives of Integrals #3 and #4. Please help correct my mistake. Thanks in Advance.","I was going through the solved example 2.d in Sheldon Ross ""A First Course in Probability 8th Edition"" Chapter 5, and i got stuck on a reduction involving calculus differentiation. with regards to specifics of the problem as per the equation below, x and t are non-negative variables, and c, k are constants. f(x) represents probability density function, F(x) represents the cumulative probability density function. $$ ExpectedCost = ct\int_0^t f(x)dx - c\int_0^t xf(x)dx + k\int_t^\infty xf(x)dx -kt\int_t^\infty f(x)dx $$ The step i am unable to comprehend is the derivative of $ExpectedCost$ with respect to t, which, as per the book, yields: $$ \frac{d}{dt} (ExpectedCost) = ctf(t) + cF(t) - ctf(t) - ktf(t) + ktf(t) - k[1-F(t)] $$ My own calculations differ from those of the book: Integral#1: $$ \frac{d}{dt} (\int_0^t f(x)dx) = f(t) $$ Integral#2: $$ \frac{d}{dt} (\int_t^\infty f(x)dx) = -f(t) $$ Integral#3: $$ \frac{d}{dt} (\int_0^t xf(x)dx) \\ = \frac{d}{dt} (xF(x)|_0^t - \int_0^t F(x)dx) \\ = \frac{d}{dt} (xF(x)|_0^t) - \frac{d}{dt} (\int_0^t F(x)dx) \\ = \frac{d}{dt} (tF(t)) - \frac{d}{dt} (\int_0^t F(x)dx) \\ = F(t) + tf(t) - F(t) \\ = tf(t) $$ Integral#4: $$ \frac{d}{dt} (\int_t^\infty xf(x)dx) \\ = \frac{d}{dt} (xF(x)|_t^\infty - \int_t^\infty F(x)dx) \\ = \frac{d}{dt} (xF(x)|_t^\infty) - \frac{d}{dt} (\int_t^\infty F(x)dx) \\ = \frac{d}{dt} (-tF(t)) - \frac{d}{dt} (\int_t^\infty F(x)dx) \\ = -F(t) -tf(t) - (-F(t)) \\ = -tf(t) $$ As per my calculations, the derivative of the $ExpectedCost$ with respect to t, comes to zero. My guess is that i am going wrong with regards to the calculation of derivatives of Integrals #3 and #4. Please help correct my mistake. Thanks in Advance.",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
81,Understanding Intermediate value theorem.,Understanding Intermediate value theorem.,,"I will first state the theorem in my words and then my problem. If function $f$ is continuous on $[a,b]$ and $f(a) > 0 > f(b)$ or $f(a) < 0 < f(b)$ then the function has at least a zero in $[a,b]$. I have two problems with this theorem. My book does not say that "" at least "" part, it just says ""a zero"", is not the book wrong ? example : $f(x) = x^3 - 5x^2 +  6x$ on $[-100,100]$. Let $f$ be such that $f(x) = x^2$. Now $f$ is certainly continuous on $[-1,1]$ but $f(-1) = f(1) = 1$. Thus it does not satisfy any of the last two conditions still it has a zero at $x = 0$. What have I missed ?","I will first state the theorem in my words and then my problem. If function $f$ is continuous on $[a,b]$ and $f(a) > 0 > f(b)$ or $f(a) < 0 < f(b)$ then the function has at least a zero in $[a,b]$. I have two problems with this theorem. My book does not say that "" at least "" part, it just says ""a zero"", is not the book wrong ? example : $f(x) = x^3 - 5x^2 +  6x$ on $[-100,100]$. Let $f$ be such that $f(x) = x^2$. Now $f$ is certainly continuous on $[-1,1]$ but $f(-1) = f(1) = 1$. Thus it does not satisfy any of the last two conditions still it has a zero at $x = 0$. What have I missed ?",,"['calculus', 'real-analysis']"
82,Rate at which $x$ changes with respect to $\theta$,Rate at which  changes with respect to,x \theta,"A ladder $10$ft long rests against a vertical wall. Let $\theta$ be the angle with the horizontal. The top of the ladder is $x$ feet above the ground. If the bottom of the ladder is pushed toward the wall, find the rate at which $x$ changes with respect to $\theta$ when $\theta=60$ degrees. Express the answer in units of feet per degree. So I start by illustrating an imagine of a ladder against a building to make a triangle where hypotenuse $=10$, $x$ is the vertical, and $\theta$ is the angle opposite the vertical. So, $$\sin \theta=\frac{x}{10} \implies x=10\sin \theta$$ And we need to find the rate at which $x$ changes with respect to $\theta=60$ degrees, $$x'=10\cos \theta \implies x'(\theta)=5$$ Question: Does this show that the rate at which $x$ changes with respect to $\theta=60$ degrees is equal to $5$? i.e For $\theta=60$ degrees, $\frac{dx}{d\theta}10\sin \theta=10\cos \theta=5$. If so, how can I be sure that my units are in $\frac{feet}{deg}$?","A ladder $10$ft long rests against a vertical wall. Let $\theta$ be the angle with the horizontal. The top of the ladder is $x$ feet above the ground. If the bottom of the ladder is pushed toward the wall, find the rate at which $x$ changes with respect to $\theta$ when $\theta=60$ degrees. Express the answer in units of feet per degree. So I start by illustrating an imagine of a ladder against a building to make a triangle where hypotenuse $=10$, $x$ is the vertical, and $\theta$ is the angle opposite the vertical. So, $$\sin \theta=\frac{x}{10} \implies x=10\sin \theta$$ And we need to find the rate at which $x$ changes with respect to $\theta=60$ degrees, $$x'=10\cos \theta \implies x'(\theta)=5$$ Question: Does this show that the rate at which $x$ changes with respect to $\theta=60$ degrees is equal to $5$? i.e For $\theta=60$ degrees, $\frac{dx}{d\theta}10\sin \theta=10\cos \theta=5$. If so, how can I be sure that my units are in $\frac{feet}{deg}$?",,"['calculus', 'trigonometry', 'derivatives']"
83,Implicit differentiation from Larson 13.5,Implicit differentiation from Larson 13.5,,"Implicit differentiation: $$\frac{x}{x^{2}+y^{2}}-y^{2}=5$$ I've tried several ways including Wolfram, and the answer is not getting accepted. This is how I did it so far:","Implicit differentiation: I've tried several ways including Wolfram, and the answer is not getting accepted. This is how I did it so far:",\frac{x}{x^{2}+y^{2}}-y^{2}=5,['derivatives']
84,Prove whether the function is differentiable about a given interval.,Prove whether the function is differentiable about a given interval.,,"So It's obvious that the given function is continuous between -1 to 0 and 0 to 1, hence the only point left to test is whether the function is continuous at 0 and so I took the limit of x->0+ to be equal to the limit of x->0- and computed the value of a to be 0. As for the second part of the question, I substituted the value of a as 0 and then used the formula of differentiation ie lim(h-->0)  (f(x+h)-f(0))/h and I got the value of -1 for the left hand limit and -4 for the right hand limit. Does that conclude the function is not differentiable, for the value of a when it is continuous?","So It's obvious that the given function is continuous between -1 to 0 and 0 to 1, hence the only point left to test is whether the function is continuous at 0 and so I took the limit of x->0+ to be equal to the limit of x->0- and computed the value of a to be 0. As for the second part of the question, I substituted the value of a as 0 and then used the formula of differentiation ie lim(h-->0)  (f(x+h)-f(0))/h and I got the value of -1 for the left hand limit and -4 for the right hand limit. Does that conclude the function is not differentiable, for the value of a when it is continuous?",,"['calculus', 'derivatives', 'continuity']"
85,Derivative of trace and determinant of matrix with respect to a vector,Derivative of trace and determinant of matrix with respect to a vector,,"I encounter a difficulty in one of my research dealing with derivative of a matrix. I will appreciate much for your expertise on this. Let $\mathbf{x }=vec\left( X \right) $ where $X $ is a $T\times r $ matrix of unknown parameters ($T$ and $r$ are constant, $T>>r$) and $vec$ is the vectorization, let \begin{equation} \mathbf{H}\left( \mathbf{X }\right) =\left( X ,\mathbf{I}_{T}\right) ^{\prime },  \label{def_H_theta} \end{equation} where $\mathbf{I}_T$ is $T \times T $ identity matrix. Let \begin{equation} \Omega \left( \mathbf{X }\right) _{T\times T}=\left( \mathbf{I}% _{T}+XX ^{\prime }\right) .  \label{var-cov_TS_multi} \end{equation} What is the derivative of \begin{equation} \frac{\partial \log \left\vert \Omega \left( \mathbf{X }\right) \right\vert }{\partial \mathbf{x }}=?? \end{equation} $|A|$ is the determinant of $A$, and \begin{equation} \frac{\partial tr\left[ \mathbf{H}\left( \mathbf{X }\right) ^{\prime }\Omega \left( \mathbf{X }\right) ^{-1}\mathbf{H}\left( \mathbf{% X }\right) \right] }{\partial \mathbf{x }}=?? \end{equation} where $tr$ is the trace operator. Many thanks!","I encounter a difficulty in one of my research dealing with derivative of a matrix. I will appreciate much for your expertise on this. Let $\mathbf{x }=vec\left( X \right) $ where $X $ is a $T\times r $ matrix of unknown parameters ($T$ and $r$ are constant, $T>>r$) and $vec$ is the vectorization, let \begin{equation} \mathbf{H}\left( \mathbf{X }\right) =\left( X ,\mathbf{I}_{T}\right) ^{\prime },  \label{def_H_theta} \end{equation} where $\mathbf{I}_T$ is $T \times T $ identity matrix. Let \begin{equation} \Omega \left( \mathbf{X }\right) _{T\times T}=\left( \mathbf{I}% _{T}+XX ^{\prime }\right) .  \label{var-cov_TS_multi} \end{equation} What is the derivative of \begin{equation} \frac{\partial \log \left\vert \Omega \left( \mathbf{X }\right) \right\vert }{\partial \mathbf{x }}=?? \end{equation} $|A|$ is the determinant of $A$, and \begin{equation} \frac{\partial tr\left[ \mathbf{H}\left( \mathbf{X }\right) ^{\prime }\Omega \left( \mathbf{X }\right) ^{-1}\mathbf{H}\left( \mathbf{% X }\right) \right] }{\partial \mathbf{x }}=?? \end{equation} where $tr$ is the trace operator. Many thanks!",,"['derivatives', 'matrix-calculus']"
86,"By considering $g(z)=f(z)f(-z)$, show that $f(-z)=\frac{1}{f(z)}$ for all $z$.","By considering , show that  for all .",g(z)=f(z)f(-z) f(-z)=\frac{1}{f(z)} z,"According to the question, we must let f be a function, which is holomorphic on the plane and satisfies $f'(z) = f(z)$ and $f(0)=1$. We are also told to assume that a function which is holomorphic on the plane with zero derivative is constant. I know that a holomorphic function is a ""complex-valued function of one or more complex variables that is complex differntiable"", but I am still unsure how to answer the question above. Could someone please offer some assistance? Thank you :)","According to the question, we must let f be a function, which is holomorphic on the plane and satisfies $f'(z) = f(z)$ and $f(0)=1$. We are also told to assume that a function which is holomorphic on the plane with zero derivative is constant. I know that a holomorphic function is a ""complex-valued function of one or more complex variables that is complex differntiable"", but I am still unsure how to answer the question above. Could someone please offer some assistance? Thank you :)",,"['derivatives', 'complex-numbers', 'holomorphic-functions']"
87,Composite Functions and Derivatives,Composite Functions and Derivatives,,"I have two functions, f is monotonically increasing for every x. g has a local minimum at x=0. Now I define a new function h(x)=f(g(x)). I need to determine if h has minimum at x=0, maximum, either minimum or maximum, or if h is monotonically increasing like f is. I am not sure how to approach this. Any hints will be mostly appreciated ! Thank you.","I have two functions, f is monotonically increasing for every x. g has a local minimum at x=0. Now I define a new function h(x)=f(g(x)). I need to determine if h has minimum at x=0, maximum, either minimum or maximum, or if h is monotonically increasing like f is. I am not sure how to approach this. Any hints will be mostly appreciated ! Thank you.",,"['calculus', 'derivatives']"
88,"$f:\mathbb{R}\rightarrow \mathbb{R}$ differentiable, $f'(x)>f(x)$ for all $x$, and $f(0)=0$, then $f(x)>0$ for all $x>0$","differentiable,  for all , and , then  for all",f:\mathbb{R}\rightarrow \mathbb{R} f'(x)>f(x) x f(0)=0 f(x)>0 x>0,"Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable everywhere, $f'(x)>f(x)$ for all $x\in \mathbb{R}$, and $f(0)=0$, then $f(x)>0$ for all $x>0$. I tried using the proof given in this post: Show that if $f( 0 ) = 0$ and $f'( x ) > f( x )$ for all $x \in \mathbb{ R }$ then $f( x ) > 0$ for all $x > 0$. but I realized that I am not given that $f'$ is continuous. I have been playing with the following: Since $f'(x)>f(x)$ for all $x$ then $f'(0)=\lim_{h\rightarrow 0} \frac{f(h)}{h}>f(0)=0$. I want to say that this means that $f$ must be positive over $N_{\epsilon}(0)$ for some $\epsilon>0$. Otherwise, this would somehow contradict that $f$ is differentiable. If I know that $f$ is positive over some epsilon neighborhood around zero, I know that the function has ""lift off"" after zero. From there I can argue that the function can't come back down to zero because it would contradict that $f'(x)>f(x)$. Can someone help me formalize this hand wavy argument? I would greatly appreciate it!","Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable everywhere, $f'(x)>f(x)$ for all $x\in \mathbb{R}$, and $f(0)=0$, then $f(x)>0$ for all $x>0$. I tried using the proof given in this post: Show that if $f( 0 ) = 0$ and $f'( x ) > f( x )$ for all $x \in \mathbb{ R }$ then $f( x ) > 0$ for all $x > 0$. but I realized that I am not given that $f'$ is continuous. I have been playing with the following: Since $f'(x)>f(x)$ for all $x$ then $f'(0)=\lim_{h\rightarrow 0} \frac{f(h)}{h}>f(0)=0$. I want to say that this means that $f$ must be positive over $N_{\epsilon}(0)$ for some $\epsilon>0$. Otherwise, this would somehow contradict that $f$ is differentiable. If I know that $f$ is positive over some epsilon neighborhood around zero, I know that the function has ""lift off"" after zero. From there I can argue that the function can't come back down to zero because it would contradict that $f'(x)>f(x)$. Can someone help me formalize this hand wavy argument? I would greatly appreciate it!",,"['real-analysis', 'derivatives', 'continuity']"
89,What is the second derivative of a B-spline?,What is the second derivative of a B-spline?,,"A B-spline of degree $j$ is defined at knots $\vec k$ by the Cox-de Boor recursion formula \begin{align}      B_{i,1}(x) &= \left\{         \begin{matrix}              1 & \mathrm{if} \quad k_i \leq x < k_{i+1} \\             0 & \mathrm{otherwise}          \end{matrix}     \right. \\     B_{i,j}(x) &= \frac{x - k_i}{k_{i+j-1} - k_i} B_{i,j-1}(x) + \frac{k_{i+j} - x}{k_{i+j} - k_{i+1}} B_{i+1,j-1}(x) \end{align} and has derivative \begin{equation}     \frac{\text{d}B_{i,j}(x)}{\text{d}x}     =     (j-1)     \left(         \frac{-B_{i+1,j-1}(x)}{k_{i+j}-k_{i+1}}         +         \frac{B_{i,j-1}(x)}{k_{i+j-1}-k_i}     \right). \end{equation} I am trying to implement the O'Sullivan penalty which requires second derivatives. What is the second derivative of a B-spline?","A B-spline of degree $j$ is defined at knots $\vec k$ by the Cox-de Boor recursion formula \begin{align}      B_{i,1}(x) &= \left\{         \begin{matrix}              1 & \mathrm{if} \quad k_i \leq x < k_{i+1} \\             0 & \mathrm{otherwise}          \end{matrix}     \right. \\     B_{i,j}(x) &= \frac{x - k_i}{k_{i+j-1} - k_i} B_{i,j-1}(x) + \frac{k_{i+j} - x}{k_{i+j} - k_{i+1}} B_{i+1,j-1}(x) \end{align} and has derivative \begin{equation}     \frac{\text{d}B_{i,j}(x)}{\text{d}x}     =     (j-1)     \left(         \frac{-B_{i+1,j-1}(x)}{k_{i+j}-k_{i+1}}         +         \frac{B_{i,j-1}(x)}{k_{i+j-1}-k_i}     \right). \end{equation} I am trying to implement the O'Sullivan penalty which requires second derivatives. What is the second derivative of a B-spline?",,"['calculus', 'derivatives', 'polynomials', 'spline']"
90,Show that a function is differentiable,Show that a function is differentiable,,"I have a function  $$f(x)= \begin{cases} x & \text{if } x \ge0 \\ x^2 & \text{if } x<0 \end{cases} $$ and want to show that it is continuous but not differentiable at $x=0$ Now to show that a function is differentable we show that  $$f'(x_0)= \lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$$ but I am always confused with such funtions. Do I have to choose $x$ or $x^2$ Taking the comments into consideration a functions is differentiable if the difference quotient $\frac{f(x)-f(x_0)}{x-x_0}, x_0\ne0$ approaches a limit. And limit exist only if left- and right-hand side limit is equal. So $$\lim_{x \to 0^-}= \frac{x^2-0}{x-0}=\frac{x^2}{x}=x=0$$ and $$\lim_{x \to 0^+}= \frac{x-0}{x-0}=\frac{x}{x}=1$$ thus they are not equal which means f is not differentiable. For the continuity part I am considering the definition: $\forall \varepsilon >0 \ \exists \delta>0$ s.t $\mid f(x)-f(x_0)\mid < \varepsilon$ if $\mid x-x_0 \mid < \delta.$","I have a function  $$f(x)= \begin{cases} x & \text{if } x \ge0 \\ x^2 & \text{if } x<0 \end{cases} $$ and want to show that it is continuous but not differentiable at $x=0$ Now to show that a function is differentable we show that  $$f'(x_0)= \lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$$ but I am always confused with such funtions. Do I have to choose $x$ or $x^2$ Taking the comments into consideration a functions is differentiable if the difference quotient $\frac{f(x)-f(x_0)}{x-x_0}, x_0\ne0$ approaches a limit. And limit exist only if left- and right-hand side limit is equal. So $$\lim_{x \to 0^-}= \frac{x^2-0}{x-0}=\frac{x^2}{x}=x=0$$ and $$\lim_{x \to 0^+}= \frac{x-0}{x-0}=\frac{x}{x}=1$$ thus they are not equal which means f is not differentiable. For the continuity part I am considering the definition: $\forall \varepsilon >0 \ \exists \delta>0$ s.t $\mid f(x)-f(x_0)\mid < \varepsilon$ if $\mid x-x_0 \mid < \delta.$",,"['real-analysis', 'derivatives']"
91,How the Derivative of Distance is Different from Speed,How the Derivative of Distance is Different from Speed,,"If the velocity vector is given by $\frac{d\overrightarrow{{r}}}{dt}$ and physically, the speed observed is just the magnitude of the velocity vector at any time $t$, then what is the physical interpretation of the derivative of the scalar distance with respect to time (i.e. the $\frac{d|\overrightarrow{{r}}|}{dt}$)? Expressing the displacement $\overrightarrow{r}$ in rectangular coordinates, I verified that $\frac{d|\overrightarrow{{r}}|}{dt}$ which is the derivative of the scalar distance is not identical to $|\frac{d\overrightarrow{{r}}}{dt}|$ which is the ""speed"" of the moving body. So, again, what is the physical or geometric interpretation of $\frac{d|\overrightarrow{{r}}|}{dt}$?","If the velocity vector is given by $\frac{d\overrightarrow{{r}}}{dt}$ and physically, the speed observed is just the magnitude of the velocity vector at any time $t$, then what is the physical interpretation of the derivative of the scalar distance with respect to time (i.e. the $\frac{d|\overrightarrow{{r}}|}{dt}$)? Expressing the displacement $\overrightarrow{r}$ in rectangular coordinates, I verified that $\frac{d|\overrightarrow{{r}}|}{dt}$ which is the derivative of the scalar distance is not identical to $|\frac{d\overrightarrow{{r}}}{dt}|$ which is the ""speed"" of the moving body. So, again, what is the physical or geometric interpretation of $\frac{d|\overrightarrow{{r}}|}{dt}$?",,"['derivatives', 'vectors', 'physics', 'vector-analysis']"
92,An interesting twist on Cauchy's functional equation,An interesting twist on Cauchy's functional equation,,"I was looking at Cauchy's functional equation when I happened upon an interesting result. Consider a differentiable function $f:\mathbb R\to\mathbb R$ with $f(1)=1$ and $f(x+y)=f(x)+f(y)$. With $x=y=0$, it follows that $f(0)=0$. ...and then I do something not so orthodox: $$f(x+h)=f(x)+f(h)$$ $$f(x+h)-f(x)=f(h)=f(0+h)-f(0)$$ $$\frac{f(x+h)-f(x)}h=\frac{f(0+h)-f(0)}h$$ as $h\to0$, we end up with, nicely, $$f'(x)=f'(0)$$ Thus, $f(x)$ is linear, and interpolating with $f(0)=0$ and $f(1)=1$, we end up with $$f(x)=x$$ just as suspected. Now, I don't usually see this done with functional equations, using derivatives and all, so I was wondering 2 questions: Is what I did ok, given the conditions on $f$? Could someone produce an interesting scenario of when a hard to solve functional equation reduces nicely with derivatives?  Thanks :-)","I was looking at Cauchy's functional equation when I happened upon an interesting result. Consider a differentiable function $f:\mathbb R\to\mathbb R$ with $f(1)=1$ and $f(x+y)=f(x)+f(y)$. With $x=y=0$, it follows that $f(0)=0$. ...and then I do something not so orthodox: $$f(x+h)=f(x)+f(h)$$ $$f(x+h)-f(x)=f(h)=f(0+h)-f(0)$$ $$\frac{f(x+h)-f(x)}h=\frac{f(0+h)-f(0)}h$$ as $h\to0$, we end up with, nicely, $$f'(x)=f'(0)$$ Thus, $f(x)$ is linear, and interpolating with $f(0)=0$ and $f(1)=1$, we end up with $$f(x)=x$$ just as suspected. Now, I don't usually see this done with functional equations, using derivatives and all, so I was wondering 2 questions: Is what I did ok, given the conditions on $f$? Could someone produce an interesting scenario of when a hard to solve functional equation reduces nicely with derivatives?  Thanks :-)",,"['derivatives', 'functional-equations', 'alternative-proof']"
93,Understanding the derivatives in backpropagation algorithm,Understanding the derivatives in backpropagation algorithm,,"I'm having trouble understanding the derivatives in the backpropagation algorithm. I'll use the example presented here . If you're unfamiliar with the algorithms I'm talking about - it's Okay, my question is only about derivatives. So I have the following functions: $$ x_1 = W_1x_0$$ $$ x_2 = f_1(x_1)$$ $$E = \frac{1}{2} || x_2 - y||^2$$ where $x_0$ is a vector of size 4x1, $W_2$ is a matrix of size 5x4, and $f_1$ is some nonlinear function (for example, the logistic function). $y$ is a vector with the same dimension as $x_2$. Now, I need to take the derivative of E w.r.t. $W_1$. I'll use the chain rule: $$ \frac{\partial E}{\partial W_1} = \frac{\partial E}{\partial x_2} \frac{\partial x_2}{\partial x_1} \frac{\partial x_1}{\partial W_2}$$ I can understand the first derivative: the derivative of a scalar (that comes from the function E) w.r.t. a vector is a vector. I'm not sure about the next part. The derivative of $x_2$ w.r.t. $x_1$ is the derivative of a vector w.r.t. a vector. Isn't that supposed to be a matrix, somehow? And the part I least understand is the last: The derivative of $x_1$ w.r.t. $W_1$. Isn't it impossible to take the derivative of a vector w.r.t. a matrix?","I'm having trouble understanding the derivatives in the backpropagation algorithm. I'll use the example presented here . If you're unfamiliar with the algorithms I'm talking about - it's Okay, my question is only about derivatives. So I have the following functions: $$ x_1 = W_1x_0$$ $$ x_2 = f_1(x_1)$$ $$E = \frac{1}{2} || x_2 - y||^2$$ where $x_0$ is a vector of size 4x1, $W_2$ is a matrix of size 5x4, and $f_1$ is some nonlinear function (for example, the logistic function). $y$ is a vector with the same dimension as $x_2$. Now, I need to take the derivative of E w.r.t. $W_1$. I'll use the chain rule: $$ \frac{\partial E}{\partial W_1} = \frac{\partial E}{\partial x_2} \frac{\partial x_2}{\partial x_1} \frac{\partial x_1}{\partial W_2}$$ I can understand the first derivative: the derivative of a scalar (that comes from the function E) w.r.t. a vector is a vector. I'm not sure about the next part. The derivative of $x_2$ w.r.t. $x_1$ is the derivative of a vector w.r.t. a vector. Isn't that supposed to be a matrix, somehow? And the part I least understand is the last: The derivative of $x_1$ w.r.t. $W_1$. Isn't it impossible to take the derivative of a vector w.r.t. a matrix?",,"['calculus', 'derivatives', 'partial-derivative', 'neural-networks']"
94,Coincidence of standard derivative and weak derivative,Coincidence of standard derivative and weak derivative,,"Let $f:\mathbb{R}^n \to \mathbb{R}$ be in $W^{1,p}(\mathbb{R}^n)$ and differentiable (in the classical sense) almost everywhere. Is it true that the standard derivative and the weak derivative conicide? When $p>n$ this is a corollary from theorem 4.9 (""LECTURES ON LIPSCHITZ ANALYSIS""- by Heinonen; In fact, in that case $f \in W^{1,p}$ implies that $f$ is differentiable almost everywhere). What happens for other values of $p$?","Let $f:\mathbb{R}^n \to \mathbb{R}$ be in $W^{1,p}(\mathbb{R}^n)$ and differentiable (in the classical sense) almost everywhere. Is it true that the standard derivative and the weak derivative conicide? When $p>n$ this is a corollary from theorem 4.9 (""LECTURES ON LIPSCHITZ ANALYSIS""- by Heinonen; In fact, in that case $f \in W^{1,p}$ implies that $f$ is differentiable almost everywhere). What happens for other values of $p$?",,"['real-analysis', 'derivatives', 'sobolev-spaces', 'weak-derivatives']"
95,"Directional derivative of $z(x,y)=yg\left(\frac{x}y\right)+\frac{8y^3}x$ on the unit circle",Directional derivative of  on the unit circle,"z(x,y)=yg\left(\frac{x}y\right)+\frac{8y^3}x","Hi can someone help me with this question? Let $g(t)$ be differentiable for all $t$ with $g\left(\frac12\right)=1$ and $$z(x,y)=yg\left(\frac{x}y\right)+\frac{8y^3}x$$ $(x,y)$ is a point on the circle $x^2+y^2=1$ and $x\ne0$. What is the directional derivative of the function $z(x,y)$ in the point $(x,y)$, in the direction of the vector that points to the center of the circle? (the answer should be a function of $x,y,z$ and not $g$).","Hi can someone help me with this question? Let $g(t)$ be differentiable for all $t$ with $g\left(\frac12\right)=1$ and $$z(x,y)=yg\left(\frac{x}y\right)+\frac{8y^3}x$$ $(x,y)$ is a point on the circle $x^2+y^2=1$ and $x\ne0$. What is the directional derivative of the function $z(x,y)$ in the point $(x,y)$, in the direction of the vector that points to the center of the circle? (the answer should be a function of $x,y,z$ and not $g$).",,"['derivatives', 'partial-derivative']"
96,Exercise on a twice differentiable function,Exercise on a twice differentiable function,,"I have the following exercise: Let $f$ be a twice differentiable function on $\mathbb R$ such that: $f(-1)<0$ $f(1)>0$ $\lim_{x\to \pm \infty} f(x) =0$ Prove that $f''(x)= 0$ for at least three values of $x.$ I managed to find one such point reasoning about local maximum/minimum, and I know that the function has at least one zero. But I don't know how to proceed further.","I have the following exercise: Let $f$ be a twice differentiable function on $\mathbb R$ such that: $f(-1)<0$ $f(1)>0$ $\lim_{x\to \pm \infty} f(x) =0$ Prove that $f''(x)= 0$ for at least three values of $x.$ I managed to find one such point reasoning about local maximum/minimum, and I know that the function has at least one zero. But I don't know how to proceed further.",,"['calculus', 'real-analysis', 'derivatives']"
97,Relating the derivatives of an injective function to the derivatives of it's inverse function,Relating the derivatives of an injective function to the derivatives of it's inverse function,,"Suppose $f : \mathbb{R} \to \mathbb{R}$ an injective function that is differentiable. Being injective; it has an inverse function $f^{-1}$. How are the derivatives of the inverse function related to the derivatives of the original function? For instance, I know that for nice enough $f$ where $f(x) = y$ we have: $$(f^{-1}){'}(y) = \lim_{y \to y_0} \frac{(f^{-1})(y) - (f^{-1})(y_0)}{y - y_0} = \lim_{x \to x_0} \frac{x - x_0}{f(x) - f(x_0)}  = \frac{1}{f'(x)}$$ For nice functions, are higher order derivatives of the inverse function (such as $(f^{-1}){'}{'}(y)$) related in nice ways to the derivatives of the ordinary function?","Suppose $f : \mathbb{R} \to \mathbb{R}$ an injective function that is differentiable. Being injective; it has an inverse function $f^{-1}$. How are the derivatives of the inverse function related to the derivatives of the original function? For instance, I know that for nice enough $f$ where $f(x) = y$ we have: $$(f^{-1}){'}(y) = \lim_{y \to y_0} \frac{(f^{-1})(y) - (f^{-1})(y_0)}{y - y_0} = \lim_{x \to x_0} \frac{x - x_0}{f(x) - f(x_0)}  = \frac{1}{f'(x)}$$ For nice functions, are higher order derivatives of the inverse function (such as $(f^{-1}){'}{'}(y)$) related in nice ways to the derivatives of the ordinary function?",,"['calculus', 'derivatives']"
98,Find $f'(x)$ where $f(x) = x^\frac{3}{2}$ using $h \to 0$ method,Find  where  using  method,f'(x) f(x) = x^\frac{3}{2} h \to 0,$$f(x) = x^{\frac{3}{2}}$$ $$f'(x) = ?$$ How would you solve this using the $h\to 0$ method? Here's the initial setup. $$f'(x) = \lim_{h \to 0}\frac{(x+h)^\frac{3}{2}-x^{3/2}}{h}$$ I tried difference of cubes but it didn't get me anywhere.,$$f(x) = x^{\frac{3}{2}}$$ $$f'(x) = ?$$ How would you solve this using the $h\to 0$ method? Here's the initial setup. $$f'(x) = \lim_{h \to 0}\frac{(x+h)^\frac{3}{2}-x^{3/2}}{h}$$ I tried difference of cubes but it didn't get me anywhere.,,"['limits', 'derivatives']"
99,For what value of $a\ f(x) = e^x + ax^3$ has inflection points,For what value of  has inflection points,a\ f(x) = e^x + ax^3,I took the second derivative: $f^{''}(x)= e^x + 6ax$. So $f$ has inflection points $\iff$ there exists $x_0$ such that $e^{x_0} + 6ax_0 = 0$. I don't know how to go further and find such $a$ for which exists at least one positive root of that equation.,I took the second derivative: $f^{''}(x)= e^x + 6ax$. So $f$ has inflection points $\iff$ there exists $x_0$ such that $e^{x_0} + 6ax_0 = 0$. I don't know how to go further and find such $a$ for which exists at least one positive root of that equation.,,"['calculus', 'real-analysis', 'derivatives']"
