,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"$E[\frac{1}{X}]$ for $X\sim\Gamma(n,\theta)$",for,"E[\frac{1}{X}] X\sim\Gamma(n,\theta)","Consider iid random varibales $(X_i)_{1\le i\le n}$ with $X_i\sim Exp(\theta)$ for $1\le i\le n$ and $\theta\in(0,\infty)$ . Then we have $$\sum_{i=1}^nX_i\sim \Gamma(n,\theta)$$ with density function $$f(x)=\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x}\mathbb{1}_{[0,\infty)}(x).$$ Now I want to calculate \begin{align}E\Bigg[\frac{1}{\frac{1}{n}\sum_{i=1}^nX_i}\Bigg]=n\cdot E\Bigg[\frac{1}{\sum_{i=1}^nX_i}\Bigg]&=n\cdot\int_{0}^\infty \frac{1}{x}\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x} dx\\ &=\frac{n\theta^n}{(n-1)!}\cdot\int_{0}^\infty x^{n-2}e^{-\theta x} dx,\end{align} but I do not know where I have to go from here. I know the antiderivate of $x^{n-2}e^{-x}$ , but I do not know how to deal with $x^{n-2}e^{-\theta x}$ . Can someone give me a hint? Thanks in advance!","Consider iid random varibales with for and . Then we have with density function Now I want to calculate but I do not know where I have to go from here. I know the antiderivate of , but I do not know how to deal with . Can someone give me a hint? Thanks in advance!","(X_i)_{1\le i\le n} X_i\sim Exp(\theta) 1\le i\le n \theta\in(0,\infty) \sum_{i=1}^nX_i\sim \Gamma(n,\theta) f(x)=\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x}\mathbb{1}_{[0,\infty)}(x). \begin{align}E\Bigg[\frac{1}{\frac{1}{n}\sum_{i=1}^nX_i}\Bigg]=n\cdot E\Bigg[\frac{1}{\sum_{i=1}^nX_i}\Bigg]&=n\cdot\int_{0}^\infty \frac{1}{x}\frac{\theta^n}{(n-1)!}x^{n-1}e^{-\theta x} dx\\
&=\frac{n\theta^n}{(n-1)!}\cdot\int_{0}^\infty x^{n-2}e^{-\theta x} dx,\end{align} x^{n-2}e^{-x} x^{n-2}e^{-\theta x}","['probability', 'probability-theory', 'statistics', 'stochastic-calculus', 'descriptive-statistics']"
1,Expected value and standard deviation of a pmf function,Expected value and standard deviation of a pmf function,,"In a family, the probability mass function of the number of people $x$ who have contracted the flu is given by $$P(x) = Kx \qquad x\in\{0,1,\ldots,N\}$$ where $N$ is the number of people in the family. (a) If nine people are expected to have flu in the family, calculate $K$ and $N$ . (b) Calculate the probability that the number of people in the family with flu is within one standard deviation of the mean. My attempt: This is a binomial distribution with probability parameter $p=0.5$ and mean $\mu=9$ ; hence $N = \mu/p=9/0.5 = 18$ . Now summing $Kx$ from $x = 0$ to $x=18$ gives $$\begin{align} K + 2K + 3K+ \cdots + 18K &= 1 \\ K(1+ 2+ 3+ \cdots +18) &= 1 \\ 171K &= 1 \end{align}$$ Hence $K = 1/171$ . The standard deviation for this distribution is $$Np(1-p) = 18\times 0.5\times0.5 = 4.5$$ Am I on the right path?","In a family, the probability mass function of the number of people who have contracted the flu is given by where is the number of people in the family. (a) If nine people are expected to have flu in the family, calculate and . (b) Calculate the probability that the number of people in the family with flu is within one standard deviation of the mean. My attempt: This is a binomial distribution with probability parameter and mean ; hence . Now summing from to gives Hence . The standard deviation for this distribution is Am I on the right path?","x P(x) = Kx \qquad x\in\{0,1,\ldots,N\} N K N p=0.5 \mu=9 N = \mu/p=9/0.5 = 18 Kx x = 0 x=18 \begin{align}
K + 2K + 3K+ \cdots + 18K &= 1 \\
K(1+ 2+ 3+ \cdots +18) &= 1 \\
171K &= 1 \end{align} K = 1/171 Np(1-p) = 18\times 0.5\times0.5 = 4.5","['probability', 'statistics', 'probability-distributions', 'standard-deviation']"
2,Showing unbiasedness of the variance estimator: $E(\hat \sigma^2)=\sigma^2$,Showing unbiasedness of the variance estimator:,E(\hat \sigma^2)=\sigma^2,"There is an infinite population characterized by a probability model. A simple random sample of size $n$ is $X_1,X_2,...,X_n$ . Moreoever, $X_i$ are i.i.d. according to the model. The population mean is $\mu = E(X_i)$ . The sample mean is $\hat \mu=\frac {1}{n}\sum^n _{i=1}X_i$ . $E(\hat \mu)=\mu$ . Variance of the estimator is $V(\hat \mu)=\frac {\sigma^2}{n}$ , where ${\sigma^2}=V(X_i)$ . Also, $\hat \sigma^2=\frac {1}{n-1}\sum^n_{i=1}(X_i-\hat \mu)^2$ . I want to show that $E(\hat \sigma^2)=\sigma^2$ . My confusion here is that if $X_i$ is random variable, does this mean the sample mean $\hat \mu$ is also a random variable? I also tried the fact that $\sum^n_{i=1}(X_i-\hat \mu)^2 = \sum^n_{i=1}[(X_i- \mu)+(\mu-\hat \mu)]^2$ , but I'm stuck on how to proceed forward.","There is an infinite population characterized by a probability model. A simple random sample of size is . Moreoever, are i.i.d. according to the model. The population mean is . The sample mean is . . Variance of the estimator is , where . Also, . I want to show that . My confusion here is that if is random variable, does this mean the sample mean is also a random variable? I also tried the fact that , but I'm stuck on how to proceed forward.","n X_1,X_2,...,X_n X_i \mu = E(X_i) \hat \mu=\frac {1}{n}\sum^n _{i=1}X_i E(\hat \mu)=\mu V(\hat \mu)=\frac {\sigma^2}{n} {\sigma^2}=V(X_i) \hat \sigma^2=\frac {1}{n-1}\sum^n_{i=1}(X_i-\hat \mu)^2 E(\hat \sigma^2)=\sigma^2 X_i \hat \mu \sum^n_{i=1}(X_i-\hat \mu)^2 = \sum^n_{i=1}[(X_i- \mu)+(\mu-\hat \mu)]^2","['real-analysis', 'probability', 'statistics', 'random-variables']"
3,Probability and Statistics: Over-booking flights,Probability and Statistics: Over-booking flights,,"I am finding difficulty trying to answer the second part of this question. The answer is one this website but I do now get how they worked it out. https://nrich.maths.org/4932 I also know that binomial and normal distribution plays a big part in this. An airline flies a plane with 400 seats. Each passenger who buys a ticket arrives for the flight (that is, does not miss the flight) with probability 0.95. If the airline sells 400 tickets what is the expected number of empty seats? (I found the mean to be 380 and the Sd to be 20) The airline regularly books more than 400 passengers for its flights. How many tickets can the airline sell if it wants to have to refuse passengers who arrive for the flight with tickets in no more than about two per cent of the flights? (I do not know how to reach the answer of 411. Can someone explain with step by step working?)","I am finding difficulty trying to answer the second part of this question. The answer is one this website but I do now get how they worked it out. https://nrich.maths.org/4932 I also know that binomial and normal distribution plays a big part in this. An airline flies a plane with 400 seats. Each passenger who buys a ticket arrives for the flight (that is, does not miss the flight) with probability 0.95. If the airline sells 400 tickets what is the expected number of empty seats? (I found the mean to be 380 and the Sd to be 20) The airline regularly books more than 400 passengers for its flights. How many tickets can the airline sell if it wants to have to refuse passengers who arrive for the flight with tickets in no more than about two per cent of the flights? (I do not know how to reach the answer of 411. Can someone explain with step by step working?)",,"['probability', 'statistics', 'normal-distribution', 'binomial-theorem']"
4,Possion distribution exercise,Possion distribution exercise,,"Suppose box A contains countless black balls and box B contains one white ball. We took the black ball out of box A, put it into box B, and experimented with selecting one ball randomly from box B. At this time, the number of black balls taken out of box A and carried to box B follows Poisson distribution. How could I calculate the probability that the ball from B is white. and how could I calculate if the ball from box B was white the probability distribution for the number of black balls moved to box B.","Suppose box A contains countless black balls and box B contains one white ball. We took the black ball out of box A, put it into box B, and experimented with selecting one ball randomly from box B. At this time, the number of black balls taken out of box A and carried to box B follows Poisson distribution. How could I calculate the probability that the ball from B is white. and how could I calculate if the ball from box B was white the probability distribution for the number of black balls moved to box B.",,"['statistics', 'poisson-distribution']"
5,Is it best to approach solving this question using Binomial Distribution and Conditional Probability in the following way?,Is it best to approach solving this question using Binomial Distribution and Conditional Probability in the following way?,,"Problem summary: 99 fair coins 1 unfair coin with a probability of getting heads $P(H)=0.9$ A coin is selected and flipped 10 times. If the coin lands on head 10 times out of 10 flips, what is the probability that the unfair coin was selected? My approach: Event $A$ : unfair coin was picked Event $B$ : Getting 10 heads from 10 flips Event $A^{c}$ : did not pick unfair coin Problem: find $P(A|B)$ $$P(A|B)=\displaystyle \frac{P(A \cap B)}{P(B)}$$ $=\displaystyle \frac{P(A \cap B) }{P(A \cap B) \cup P(A^{c} \cap B)} \label{a}\tag{1}$ $$P(A \cap B)={10 \choose 10} 0.9^{10}0.1^{0}=0.3487 \label{b}\tag{2}$$ $$P(A^{c} \cap B)={10 \choose 10}0.5^{10}0.5^{0}=0.00097656$$ $$P(B)=P(A \cap B)+P(A^{c} \cap B)$$ $$=0.3487+0.00097656=0.34967656 \label{c}\tag{3}$$ Subbing (2) and (3) into (1) gives $$P(A|B)=\frac{0.3487}{0.34967656}=0.997207248$$ Does this seem correct?","Problem summary: 99 fair coins 1 unfair coin with a probability of getting heads A coin is selected and flipped 10 times. If the coin lands on head 10 times out of 10 flips, what is the probability that the unfair coin was selected? My approach: Event : unfair coin was picked Event : Getting 10 heads from 10 flips Event : did not pick unfair coin Problem: find Subbing (2) and (3) into (1) gives Does this seem correct?",P(H)=0.9 A B A^{c} P(A|B) P(A|B)=\displaystyle \frac{P(A \cap B)}{P(B)} =\displaystyle \frac{P(A \cap B) }{P(A \cap B) \cup P(A^{c} \cap B)} \label{a}\tag{1} P(A \cap B)={10 \choose 10} 0.9^{10}0.1^{0}=0.3487 \label{b}\tag{2} P(A^{c} \cap B)={10 \choose 10}0.5^{10}0.5^{0}=0.00097656 P(B)=P(A \cap B)+P(A^{c} \cap B) =0.3487+0.00097656=0.34967656 \label{c}\tag{3} P(A|B)=\frac{0.3487}{0.34967656}=0.997207248,"['statistics', 'binomial-distribution', 'conditional-probability']"
6,Is this a good way to show that $I$ and $J$ are not independent?,Is this a good way to show that  and  are not independent?,I J,"Given $$P(I\mid J)=0.7$$$$P(I^c\mid J^c)=0.3$$ does the following bit of working out show that $I$ and $J$ are not independent events? $$P(I\mid J)P(J)=P(I \cap J)=0.7P(J)$$ $$P(I^c\mid J^c)P(J^c)=P(I^c \cap J^c)=0.3P(J^c)$$ Since $P(I^c \cap J^c)=1-P(I \cap J)$, and $P(J^c)=1-P(J)$ $$1-0.7P(J)=0.3(1-P(J))$$ $$1-0.7P(J)=0.3-0.3P(J)$$ $$0.4P(J)=0.7 \rightarrow P(J)=7/4,$$ which is silly. I feel like there's a better way to show that the two are not independent though...","Given $$P(I\mid J)=0.7$$$$P(I^c\mid J^c)=0.3$$ does the following bit of working out show that $I$ and $J$ are not independent events? $$P(I\mid J)P(J)=P(I \cap J)=0.7P(J)$$ $$P(I^c\mid J^c)P(J^c)=P(I^c \cap J^c)=0.3P(J^c)$$ Since $P(I^c \cap J^c)=1-P(I \cap J)$, and $P(J^c)=1-P(J)$ $$1-0.7P(J)=0.3(1-P(J))$$ $$1-0.7P(J)=0.3-0.3P(J)$$ $$0.4P(J)=0.7 \rightarrow P(J)=7/4,$$ which is silly. I feel like there's a better way to show that the two are not independent though...",,"['statistics', 'independence', 'conditional-probability']"
7,Finding the mean ($\bar{y}$ vector) given $y$ vector [closed],Finding the mean ( vector) given  vector [closed],\bar{y} y,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $y$ be an $n \times 1$ vector of observations. Consider the $n \times 1$ vector $\bar{y}$, which contains the mean of $y$ vector. How does one show that the $\bar{y}$ vector is given by $$\bar{y}=\vec{1} \left( \vec{1}^T \vec{1} \right)^{-1} \vec{1}^Ty$$ where $\vec{1}$ is the vector of ones (also $n \times 1$).","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $y$ be an $n \times 1$ vector of observations. Consider the $n \times 1$ vector $\bar{y}$, which contains the mean of $y$ vector. How does one show that the $\bar{y}$ vector is given by $$\bar{y}=\vec{1} \left( \vec{1}^T \vec{1} \right)^{-1} \vec{1}^Ty$$ where $\vec{1}$ is the vector of ones (also $n \times 1$).",,"['linear-algebra', 'probability', 'probability-theory', 'statistics']"
8,Conditional probability involving sum of two independent geometric random variables,Conditional probability involving sum of two independent geometric random variables,,"Let $X$ and $Y$ be two independent geometric random variables with common   parameter $p$. Find $P(Y = y|X + Y = z)$ where $z ≥ 2$ and $y = 1, 2, · · · , z − 1$. I am quite lost with this question.  The correct solution is $1\over z-1$, however I have no idea how they got there.  This is what I started with. $$P(Y = y|X + Y = z)$$ $$ P(Y = y)\cap P(X + Y = z)\over P(X + Y = z)$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp^2(1-p)^{z-2}\over \sum_{n=0}^zp^2(1-p)^{z-2}$$ $$ p(1-p)^{y-1}\cap(z+1)p^2(1-p)^{z-2}\over (z+1)p^2(1-p)^{z-2}$$ As you can see, I have made quite a mess and am nowhere near the correct solution. Can somebody please help me out with this?","Let $X$ and $Y$ be two independent geometric random variables with common   parameter $p$. Find $P(Y = y|X + Y = z)$ where $z ≥ 2$ and $y = 1, 2, · · · , z − 1$. I am quite lost with this question.  The correct solution is $1\over z-1$, however I have no idea how they got there.  This is what I started with. $$P(Y = y|X + Y = z)$$ $$ P(Y = y)\cap P(X + Y = z)\over P(X + Y = z)$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp^2(1-p)^{z-2}\over \sum_{n=0}^zp^2(1-p)^{z-2}$$ $$ p(1-p)^{y-1}\cap(z+1)p^2(1-p)^{z-2}\over (z+1)p^2(1-p)^{z-2}$$ As you can see, I have made quite a mess and am nowhere near the correct solution. Can somebody please help me out with this?",,"['probability', 'statistics', 'probability-distributions']"
9,Fitting probability distribution connection to regression,Fitting probability distribution connection to regression,,"My question is whether there is some connection between fitting probability distribution on some data set and linear regression? Or this two tools are for different problems? By fitting probability distribution I mean that I have some data $x_{1},...,x_{n}$ and I believe they came from for example normally distributed population with parameters $\mu$ and $\sigma^2$. And I will estimate these parameters with likelihood estimation technique or method of moments technique. By regression, I mean that I have random variable $X$ that is modelled for example as $X \sim a+bY+cZ$ where $Y$ and $Z$ are some random variables and I am estimating parameters $a,b$ and $c$. So my question is whether there is some link between these two ""mathematical topics""? Whether for example, I can look on fitting probability distribution as some specific problem of linear regression? Thank you for your answer.","My question is whether there is some connection between fitting probability distribution on some data set and linear regression? Or this two tools are for different problems? By fitting probability distribution I mean that I have some data $x_{1},...,x_{n}$ and I believe they came from for example normally distributed population with parameters $\mu$ and $\sigma^2$. And I will estimate these parameters with likelihood estimation technique or method of moments technique. By regression, I mean that I have random variable $X$ that is modelled for example as $X \sim a+bY+cZ$ where $Y$ and $Z$ are some random variables and I am estimating parameters $a,b$ and $c$. So my question is whether there is some link between these two ""mathematical topics""? Whether for example, I can look on fitting probability distribution as some specific problem of linear regression? Thank you for your answer.",,"['statistics', 'regression']"
10,Probability of events with retries?,Probability of events with retries?,,"Let's say I want to roll $n$ 20-sided dice, and I want none of those dice to be a 1. I figure that the probability at least one die will be a 1 is $\frac{19}{20}^n$. But now let's say that we will re-roll each individual die that is a 1 up to $r$ times. I want to know 2 things: Given the above, what is the probability one or more of the dice will be a 1? Suppose I play this game a million times. How many dice rolls will a given game make on average? In other words, for each game, I will make $n+t$ dice rolls, where $t$ is the number of retries I've made. What would $t$ be on average?","Let's say I want to roll $n$ 20-sided dice, and I want none of those dice to be a 1. I figure that the probability at least one die will be a 1 is $\frac{19}{20}^n$. But now let's say that we will re-roll each individual die that is a 1 up to $r$ times. I want to know 2 things: Given the above, what is the probability one or more of the dice will be a 1? Suppose I play this game a million times. How many dice rolls will a given game make on average? In other words, for each game, I will make $n+t$ dice rolls, where $t$ is the number of retries I've made. What would $t$ be on average?",,"['probability', 'statistics']"
11,"Find smallest $c \leq 1$ such that $\operatorname{var} \left(\max(\mathbb{E}[X], X) \right) \leq c \,\operatorname{var}(X)$",Find smallest  such that,"c \leq 1 \operatorname{var} \left(\max(\mathbb{E}[X], X) \right) \leq c \,\operatorname{var}(X)","Consider random variable $X \geq 0$. Find smallest $c \leq 1$ such that $\operatorname{var} \left(\max(\mathbb{E}[X], X) \right) \leq c \operatorname{var}(X)$. Intuitively, it seems to be true for some $c < 1$, but I could only prove it for $c = 1$. Any idea?","Consider random variable $X \geq 0$. Find smallest $c \leq 1$ such that $\operatorname{var} \left(\max(\mathbb{E}[X], X) \right) \leq c \operatorname{var}(X)$. Intuitively, it seems to be true for some $c < 1$, but I could only prove it for $c = 1$. Any idea?",,"['probability', 'probability-theory', 'statistics']"
12,Find the 99% confidence interval (Interval and test for proportion),Find the 99% confidence interval (Interval and test for proportion),,"I have the following question and I am getting different results from a friend (I think he forgot to halve $ \alpha $). Problem is: A random sample of 100 students from a large school was taken. It was found 38 went on a trip last month, 62 had not. Obtain a 99% confidence interval for the proportion of students who went on a trip last month. Solution: $$ N=100, X=38$$ $$\hat{p} = {X\over{N}}$$ $$ \hat{p} \pm z_{\alpha/2} \sqrt{{\hat{p}(1-\hat{p})}\over{N}}$$ I then take $z_{\alpha/2}$ from the t-table corresponding to $\alpha = 0.005$ and $n=\infty$ which is 2.576 . Is this correct? Therefore, I get the interval of (0.255, 0.505).","I have the following question and I am getting different results from a friend (I think he forgot to halve $ \alpha $). Problem is: A random sample of 100 students from a large school was taken. It was found 38 went on a trip last month, 62 had not. Obtain a 99% confidence interval for the proportion of students who went on a trip last month. Solution: $$ N=100, X=38$$ $$\hat{p} = {X\over{N}}$$ $$ \hat{p} \pm z_{\alpha/2} \sqrt{{\hat{p}(1-\hat{p})}\over{N}}$$ I then take $z_{\alpha/2}$ from the t-table corresponding to $\alpha = 0.005$ and $n=\infty$ which is 2.576 . Is this correct? Therefore, I get the interval of (0.255, 0.505).",,"['statistics', 'statistical-inference', 'confidence-interval']"
13,How to understand if $a-c \lt b \lt a+c$ then $b-c \lt a \lt b+c$,How to understand if  then,a-c \lt b \lt a+c b-c \lt a \lt b+c,"I'm learning the concept of confidence interval. One of the important inference is that ""the probability that sample mean is within two standard deviation of population mean equals to 95%."" is equivalent to ""there is a 95% probability that population mean is within two standard  deviation of sample mean."" It seems the logic is if $a-c \lt b \lt a+c$ then to $b-c \lt a \lt b+c$ where a = population mean, b = sample mean and c = 2SD. I don't understand.","I'm learning the concept of confidence interval. One of the important inference is that ""the probability that sample mean is within two standard deviation of population mean equals to 95%."" is equivalent to ""there is a 95% probability that population mean is within two standard  deviation of sample mean."" It seems the logic is if $a-c \lt b \lt a+c$ then to $b-c \lt a \lt b+c$ where a = population mean, b = sample mean and c = 2SD. I don't understand.",,"['algebra-precalculus', 'statistics', 'inequality', 'confidence-interval']"
14,Sample variance recursive relation. $n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_n - \overline{X}_{n-1})^2$,Sample variance recursive relation.,n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_n - \overline{X}_{n-1})^2,"Let $\overline{X}_k$ and $S_k^2$ denote the sample mean and sample variance based on the first $k$ observations. Then establish the following  $$n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_{n+1} - \overline{X}_n)^2.$$ I am trying to use the relation  $$\overline{X}_{n+1} = \frac{X_{n+1} + n\overline{X}_n}{n+1}.$$ I tried to decompose the sum into the first $n$ and the last $n+1$th term, however I get $$n S_{n+1}^2 = \sum_{i=1}^{n+1} (X_i - \overline{X}_{n+1})^2 = \sum_{i=1}^n (X_i - \frac{X_{n+1} + n \overline{X}_n}{n+1})^2 + (\frac{n}{n+1} X_{n+1} - \overline{X}_n)^2,$$ which has the square of $\frac{n}{n+1}$. How can I get this relation? I would greatly appreciate some help.","Let $\overline{X}_k$ and $S_k^2$ denote the sample mean and sample variance based on the first $k$ observations. Then establish the following  $$n S_{n+1}^2 = (n-1)S_n^2 + \big( \frac{n}{n+1}\big) (X_{n+1} - \overline{X}_n)^2.$$ I am trying to use the relation  $$\overline{X}_{n+1} = \frac{X_{n+1} + n\overline{X}_n}{n+1}.$$ I tried to decompose the sum into the first $n$ and the last $n+1$th term, however I get $$n S_{n+1}^2 = \sum_{i=1}^{n+1} (X_i - \overline{X}_{n+1})^2 = \sum_{i=1}^n (X_i - \frac{X_{n+1} + n \overline{X}_n}{n+1})^2 + (\frac{n}{n+1} X_{n+1} - \overline{X}_n)^2,$$ which has the square of $\frac{n}{n+1}$. How can I get this relation? I would greatly appreciate some help.",,"['algebra-precalculus', 'statistics']"
15,"In ""between-class"" scatter matrix, what does ""overall mean"" $m$ refer to? [closed]","In ""between-class"" scatter matrix, what does ""overall mean""  refer to? [closed]",m,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question In ""between-class"" scatter matrix, what does ""overall mean"" $m$ refer to? Because the formulation is $$\sum_{i=1}^c N_i (m_i - m) (m_i - m)^T$$ where $N_i$s is the size of $i$th sample (each column is a sample), $m_i$ is the sample mean of $i$th sample. But what does $m$, the overall mean, refer to?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question In ""between-class"" scatter matrix, what does ""overall mean"" $m$ refer to? Because the formulation is $$\sum_{i=1}^c N_i (m_i - m) (m_i - m)^T$$ where $N_i$s is the size of $i$th sample (each column is a sample), $m_i$ is the sample mean of $i$th sample. But what does $m$, the overall mean, refer to?",,"['statistics', 'machine-learning', 'means']"
16,The difference between a Z-Score and a Z-statistic? Why do we divide by $\sqrt{n}$ for the latter?,The difference between a Z-Score and a Z-statistic? Why do we divide by  for the latter?,\sqrt{n},"Trying to understand statistics/hypothesis testing. The example in the book discusses using a Z-Test. I am familiar with and on an intuitive level, I understand a Z-score, a Z-score basically measures how many standard deviations from the mean a point in a sample space is. This makes sense to me. What I do not understand is why for a Z-test, we seemingly take the Z-score and divide it by $\sqrt{n}$ ? Can anyone explain the difference, it seems like we are moving away from the intuitive explanation of ""number of standard deviations from the mean"" which is how we measure the probability via the area under the curve","Trying to understand statistics/hypothesis testing. The example in the book discusses using a Z-Test. I am familiar with and on an intuitive level, I understand a Z-score, a Z-score basically measures how many standard deviations from the mean a point in a sample space is. This makes sense to me. What I do not understand is why for a Z-test, we seemingly take the Z-score and divide it by $\sqrt{n}$ ? Can anyone explain the difference, it seems like we are moving away from the intuitive explanation of ""number of standard deviations from the mean"" which is how we measure the probability via the area under the curve",,"['statistics', 'hypothesis-testing']"
17,CDF for exponential family,CDF for exponential family,,"Might be a stupid question though... we know for Exponential Family, we have the density is $$f(x|\theta)=h(x)\exp[\eta(\theta)\cdot T(x)-A(\theta)]$$ Is there a general form for the CDF function? I know if we specify a member from the exponential family, like normal, exponential, etc, the question would be simple. But is there a general form of the CDF? Thanks~","Might be a stupid question though... we know for Exponential Family, we have the density is $$f(x|\theta)=h(x)\exp[\eta(\theta)\cdot T(x)-A(\theta)]$$ Is there a general form for the CDF function? I know if we specify a member from the exponential family, like normal, exponential, etc, the question would be simple. But is there a general form of the CDF? Thanks~",,"['statistics', 'probability-distributions', 'statistical-inference', 'distribution-theory']"
18,Estimation $\mu^2$ under certain conditions.,Estimation  under certain conditions.,\mu^2,"Let $X_1,X_2,....,X_n$ be a random sample of size $n$ from a population with cdf $F()$. Let $E(X)=\mu$ exist. Then estimate $\mu^2$ unbiasedly for the following three cases:- (i) $Var(X)=\sigma^2$ exists and is known. (ii) $Var(X)=\sigma^2$ exists and is unknown. (iii) $Var(X)$ does not exist. Now for (i) & (ii) I have obtained the following solutions:- (i) We define $$\bar X=\frac1n\sum_{i=1}^n X_i$$ The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{\sigma^2}{n}$$ (ii) We define $$S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$ The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{S^2}{n}$$ For (iii) I need help. And please let me know if my answers thus far are correct. Thanks.","Let $X_1,X_2,....,X_n$ be a random sample of size $n$ from a population with cdf $F()$. Let $E(X)=\mu$ exist. Then estimate $\mu^2$ unbiasedly for the following three cases:- (i) $Var(X)=\sigma^2$ exists and is known. (ii) $Var(X)=\sigma^2$ exists and is unknown. (iii) $Var(X)$ does not exist. Now for (i) & (ii) I have obtained the following solutions:- (i) We define $$\bar X=\frac1n\sum_{i=1}^n X_i$$ The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{\sigma^2}{n}$$ (ii) We define $$S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$ The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{S^2}{n}$$ For (iii) I need help. And please let me know if my answers thus far are correct. Thanks.",,"['statistics', 'statistical-inference', 'estimation', 'parameter-estimation']"
19,"What is the full name for the ""ARD"" of ARD kernel","What is the full name for the ""ARD"" of ARD kernel",,"See below, my question is not technical, but just asking what is the full name for the ""ARD"" in ARD kernel.","See below, my question is not technical, but just asking what is the full name for the ""ARD"" in ARD kernel.",,"['linear-algebra', 'statistics']"
20,Finding a pivotal function from uniform distribution,Finding a pivotal function from uniform distribution,,"Let $X_1,...,X_n$ be a random sample from a uniform distribution $(0,\theta)$. I have found the maximum likelihood estimator to be $Y_n = \text{max}\{X_1,...,X_n\}$ The solution then claims that $\frac{Y_n}{\theta}$ is a pivot. I don't understand how to show that it is a pivot.","Let $X_1,...,X_n$ be a random sample from a uniform distribution $(0,\theta)$. I have found the maximum likelihood estimator to be $Y_n = \text{max}\{X_1,...,X_n\}$ The solution then claims that $\frac{Y_n}{\theta}$ is a pivot. I don't understand how to show that it is a pivot.",,['statistics']
21,computing probabilities of an negative binomial random variable,computing probabilities of an negative binomial random variable,,"Would appreciate if you could check if I answered the questions correctly: $X$ is a Negative Binomial random variable with the parameters $\frac{1}{2}$ and $r =1,2,3,\ldots$. $Y$ is a random variable that is defined as: $Y =  \begin{cases} X,  & x \le r+1 \\ X-1, & x \ge r+2 \end{cases}$ (1) Find $\Pr\{ Y = r+1 \}$ The result I got: $r\,(\frac{1}{2})^{r+2}$. Since we need, according to the definition of Negative Binomial, to calculate the probability of trials to make $r$ success, then we can calculate it by multiplying the parameters ($1/2$ and $r$) times $r+1+1$ to cover it all. (2) Find $\mathbb{E}[Y]$ The result I obtained is $2r-1-\frac{r+2}{2^r}$. I calculated it using $r/p$ while trying to cover all of $Y$. Hope I did this one correctly. Would appreciate your comments or corrections.","Would appreciate if you could check if I answered the questions correctly: $X$ is a Negative Binomial random variable with the parameters $\frac{1}{2}$ and $r =1,2,3,\ldots$. $Y$ is a random variable that is defined as: $Y =  \begin{cases} X,  & x \le r+1 \\ X-1, & x \ge r+2 \end{cases}$ (1) Find $\Pr\{ Y = r+1 \}$ The result I got: $r\,(\frac{1}{2})^{r+2}$. Since we need, according to the definition of Negative Binomial, to calculate the probability of trials to make $r$ success, then we can calculate it by multiplying the parameters ($1/2$ and $r$) times $r+1+1$ to cover it all. (2) Find $\mathbb{E}[Y]$ The result I obtained is $2r-1-\frac{r+2}{2^r}$. I calculated it using $r/p$ while trying to cover all of $Y$. Hope I did this one correctly. Would appreciate your comments or corrections.",,"['statistics', 'probability-distributions', 'negative-binomial']"
22,Understanding solution of tossing a coin 400 times..,Understanding solution of tossing a coin 400 times..,,"Why  in this If, tossing a coin 400 times, we count the heads, what is the probability that the number of heads is [160,190]? question heropup's asnwer is like that? I don't understand the blue text, I think it should be 190 instead of $\color{blue}{200}.$ And why to do this step $\color{blue}{\Pr[159.5 \le X \le 200.5] }?$ when you can pass directly to standarization. This is his/her answer: With $n = 400$ trials, the exact probability distribution for the number of heads $X$ observed is given by $X \sim {\rm Binomial}(n = 400, p = 1/2)$, assuming the coin is fair.  Since calculating $\Pr[160 \le X \le \color{blue}{200}]$ requires a computer, and $n$ is large, we can approximate the distribution of $X$ as ${\rm Normal}(\mu = np = 200, \sigma^2 = np(1-p) = 100)$.  Thus $$\begin{align*} \Pr[160 \le X \le 200] &\approx \color{blue}{\Pr[159.5 \le X \le 200.5] }\\ &= \Pr\left[\frac{159.5 - 200}{\sqrt{100}} \le \frac{X - \mu}{\sigma} \le \frac{200.5 - 200}{\sqrt{100}} \right] \\ &= \Pr[-4.05 \le Z \le 0.05] \\ &= \Phi(0.05) - \Phi(-4.05) \\ &\approx 0.519913. \end{align*}$$  Note that we employed continuity correction for this calculation.  The exact probability is $0.5199104479\ldots$. A similar calculation applies for $\Pr[160 \le X \le 190]$.  Using the normal approximation to the binomial, you would get an approximate value of $0.171031$.  Using the exact distribution, the probability is $0.17103699497659\ldots$.","Why  in this If, tossing a coin 400 times, we count the heads, what is the probability that the number of heads is [160,190]? question heropup's asnwer is like that? I don't understand the blue text, I think it should be 190 instead of $\color{blue}{200}.$ And why to do this step $\color{blue}{\Pr[159.5 \le X \le 200.5] }?$ when you can pass directly to standarization. This is his/her answer: With $n = 400$ trials, the exact probability distribution for the number of heads $X$ observed is given by $X \sim {\rm Binomial}(n = 400, p = 1/2)$, assuming the coin is fair.  Since calculating $\Pr[160 \le X \le \color{blue}{200}]$ requires a computer, and $n$ is large, we can approximate the distribution of $X$ as ${\rm Normal}(\mu = np = 200, \sigma^2 = np(1-p) = 100)$.  Thus $$\begin{align*} \Pr[160 \le X \le 200] &\approx \color{blue}{\Pr[159.5 \le X \le 200.5] }\\ &= \Pr\left[\frac{159.5 - 200}{\sqrt{100}} \le \frac{X - \mu}{\sigma} \le \frac{200.5 - 200}{\sqrt{100}} \right] \\ &= \Pr[-4.05 \le Z \le 0.05] \\ &= \Phi(0.05) - \Phi(-4.05) \\ &\approx 0.519913. \end{align*}$$  Note that we employed continuity correction for this calculation.  The exact probability is $0.5199104479\ldots$. A similar calculation applies for $\Pr[160 \le X \le 190]$.  Using the normal approximation to the binomial, you would get an approximate value of $0.171031$.  Using the exact distribution, the probability is $0.17103699497659\ldots$.",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'proof-explanation']"
23,"Score Based On Number of Games Played, and Average Score for All Games Played","Score Based On Number of Games Played, and Average Score for All Games Played",,"I have two data points X: Number of Games Played ( lowest: 1 highest: 1000) Y: Average of all game scores[ sum of all game scores / number of games played ] ( lowest: 4 highest: 6.5) I don't have record of the individual game scores, I only have the average. How can i generate a score based on these two variables, to make a good comparison mechanisms between multiple entities that have different x,y values? in a sense that when [x=1,y=6.5] is worse[has a lower score] than [x=5, y=6.1] or something similar","I have two data points X: Number of Games Played ( lowest: 1 highest: 1000) Y: Average of all game scores[ sum of all game scores / number of games played ] ( lowest: 4 highest: 6.5) I don't have record of the individual game scores, I only have the average. How can i generate a score based on these two variables, to make a good comparison mechanisms between multiple entities that have different x,y values? in a sense that when [x=1,y=6.5] is worse[has a lower score] than [x=5, y=6.1] or something similar",,"['statistics', 'scoring-algorithm']"
24,Is there an ideal $k$ such that the sequence $\mathrm{frac}(kn)$ is most evenly distributed?,Is there an ideal  such that the sequence  is most evenly distributed?,k \mathrm{frac}(kn),"I am generating a sequence of numbers in $[0..1)$ where the $n$th number in the sequence is $f(n)=\mathrm{frac}(kn)$ for some constant $k$ and $\mathrm{frac}(x)=x-\mathrm{floor}(x)$. I want numbers in this sequence to be distributed as evenly as possible, i.e. they should statistically resemble a uniform random distribution. (The application is generating colours, a colour is $\mathrm{HSV}(f(n),1,1)$ and I want the set of colours to have optimal coverage across the spectrum.) Is there an ""ideal"" value of $k$ for this? By intuition I am taking $k=\phi$, which gives the following sequence: 0 0.618 0.236 0.854 0.472 0.09 0.708 0.326 0.944 0.562 0.18 0.798 0.416 This performs adequately for current purposes, but I am wondering if there is a stronger way to formalise this.","I am generating a sequence of numbers in $[0..1)$ where the $n$th number in the sequence is $f(n)=\mathrm{frac}(kn)$ for some constant $k$ and $\mathrm{frac}(x)=x-\mathrm{floor}(x)$. I want numbers in this sequence to be distributed as evenly as possible, i.e. they should statistically resemble a uniform random distribution. (The application is generating colours, a colour is $\mathrm{HSV}(f(n),1,1)$ and I want the set of colours to have optimal coverage across the spectrum.) Is there an ""ideal"" value of $k$ for this? By intuition I am taking $k=\phi$, which gives the following sequence: 0 0.618 0.236 0.854 0.472 0.09 0.708 0.326 0.944 0.562 0.18 0.798 0.416 This performs adequately for current purposes, but I am wondering if there is a stronger way to formalise this.",,['statistics']
25,Find an estimator of $\theta$ using the method of moments; call it $\hat \theta$. Prove $\hat \theta$ is a consistent estimator of $\theta$,Find an estimator of  using the method of moments; call it . Prove  is a consistent estimator of,\theta \hat \theta \hat \theta \theta,"A random sample of size $n$, $Y_1, Y_2, \ldots, Y_n$ is drawn from a population. The population distribution has the following probability density function $f(y;\theta)= \begin{cases} \sqrt{\theta}y^{\sqrt{\theta}-1},  & \text{when 0 $\le$ y $\le$ 1} \\ 0, & \text{elsewhere} \end{cases}$ Find an estimator of $\theta$ using the method of moments; call it $\hat \theta$. Prove $\hat \theta$ is a consistent estimator of $\theta$ What I have done so far: First I opted to find $\mu$. So I set up my integral and solved: $$\mu = \int_0^1 \sqrt{\theta}y^{\sqrt{\theta}}dy = \frac{\sqrt{\theta}}{\sqrt{\theta}+1}$$ Then I solved the equation for $\theta$ to get $\theta = \frac{\mu^2}{(\mu-1)^2}$. So I let the estimator be $\hat{\theta} = \frac{\bar{Y^2}}{(\bar{Y}-1)^2}$ My trouble is coming from how I will prove $\hat{\theta}$ is consistent. I know two ways to show something is consistent, but I am not entirely sure how to apply that in this case. I know if: 1) $\lim \limits_{n \to \infty}E(\hat{\theta})= \theta$ and $\lim \limits_{n \to \infty}V({\hat{\theta}}) = 0$ or 2) $\lim \limits_{n \to \infty}P(|\hat{\theta_n} - \theta| \le \epsilon)=1$ or $\lim \limits_{n \to \infty}P(|\hat{\theta_n} - \theta| \gt \epsilon)= 0$ Then $\hat{\theta}$ is a consistent estimator for $\theta$. I'm just struggling to make use of those definitions. I'm not sure if my estimator is incorrect, or if I'm forgetting something, but any help would be greatly appreciated.","A random sample of size $n$, $Y_1, Y_2, \ldots, Y_n$ is drawn from a population. The population distribution has the following probability density function $f(y;\theta)= \begin{cases} \sqrt{\theta}y^{\sqrt{\theta}-1},  & \text{when 0 $\le$ y $\le$ 1} \\ 0, & \text{elsewhere} \end{cases}$ Find an estimator of $\theta$ using the method of moments; call it $\hat \theta$. Prove $\hat \theta$ is a consistent estimator of $\theta$ What I have done so far: First I opted to find $\mu$. So I set up my integral and solved: $$\mu = \int_0^1 \sqrt{\theta}y^{\sqrt{\theta}}dy = \frac{\sqrt{\theta}}{\sqrt{\theta}+1}$$ Then I solved the equation for $\theta$ to get $\theta = \frac{\mu^2}{(\mu-1)^2}$. So I let the estimator be $\hat{\theta} = \frac{\bar{Y^2}}{(\bar{Y}-1)^2}$ My trouble is coming from how I will prove $\hat{\theta}$ is consistent. I know two ways to show something is consistent, but I am not entirely sure how to apply that in this case. I know if: 1) $\lim \limits_{n \to \infty}E(\hat{\theta})= \theta$ and $\lim \limits_{n \to \infty}V({\hat{\theta}}) = 0$ or 2) $\lim \limits_{n \to \infty}P(|\hat{\theta_n} - \theta| \le \epsilon)=1$ or $\lim \limits_{n \to \infty}P(|\hat{\theta_n} - \theta| \gt \epsilon)= 0$ Then $\hat{\theta}$ is a consistent estimator for $\theta$. I'm just struggling to make use of those definitions. I'm not sure if my estimator is incorrect, or if I'm forgetting something, but any help would be greatly appreciated.",,"['probability', 'statistics', 'probability-distributions']"
26,How can we write a non-central chi-squared distribution as gamma distribution?,How can we write a non-central chi-squared distribution as gamma distribution?,,"Consider a random variable that has a non-central chi-squared distribution \begin{eqnarray*} L & = & \chi_{1}^{2}(b^{2}), \end{eqnarray*} where$\chi_{1}^{2}(b^{2})$ represents a non-central chi-squared with one degree of freedom. In fact $\chi_{1}^{2}(b^{2})$ is the square of $\mathcal{N}(b,1)$. How can we write $L$ as a Gamma distribution please? I know that if $b=0,$ we can write  \begin{eqnarray*} L & \sim & \Gamma(\frac{1}{2},2). \end{eqnarray*} What happens when $b\neq0$ please? Thanks.","Consider a random variable that has a non-central chi-squared distribution \begin{eqnarray*} L & = & \chi_{1}^{2}(b^{2}), \end{eqnarray*} where$\chi_{1}^{2}(b^{2})$ represents a non-central chi-squared with one degree of freedom. In fact $\chi_{1}^{2}(b^{2})$ is the square of $\mathcal{N}(b,1)$. How can we write $L$ as a Gamma distribution please? I know that if $b=0,$ we can write  \begin{eqnarray*} L & \sim & \Gamma(\frac{1}{2},2). \end{eqnarray*} What happens when $b\neq0$ please? Thanks.",,"['statistics', 'probability-distributions', 'gamma-distribution', 'chi-squared']"
27,Why does the expected value of $\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right]$ equal $(\mu_4-\sigma^4)/4$?,Why does the expected value of  equal ?,\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right] (\mu_4-\sigma^4)/4,"Question Let $X_i,X_j,X_k$ be IID random variables with finite moments. In particular we denote $E[X_i] := \mu$, $E[(X_i - \mu)^4] := \mu_4$ and $Var[X_i] := \sigma^2$. Why does the expected value of $$\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right]$$ equal $(\mu_4-\sigma^4)/4$? This question comes from this answer (point 2) to a question about the variance of the sample variance. My attempt The only route I see is to expand the product, I obtain $$ \frac{1}{4} E[X_i^4] + E[X_i^2]^2 - E[X_i^3]E[X_i] -\frac{5}{2} \sigma^2 E[X_i^2] +3\sigma^2E[X_i^2] + \sigma^4$$ which is not correct.","Question Let $X_i,X_j,X_k$ be IID random variables with finite moments. In particular we denote $E[X_i] := \mu$, $E[(X_i - \mu)^4] := \mu_4$ and $Var[X_i] := \sigma^2$. Why does the expected value of $$\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right]$$ equal $(\mu_4-\sigma^4)/4$? This question comes from this answer (point 2) to a question about the variance of the sample variance. My attempt The only route I see is to expand the product, I obtain $$ \frac{1}{4} E[X_i^4] + E[X_i^2]^2 - E[X_i^3]E[X_i] -\frac{5}{2} \sigma^2 E[X_i^2] +3\sigma^2E[X_i^2] + \sigma^4$$ which is not correct.",,"['probability', 'statistics', 'expected-value']"
28,Independence and Conditional Expectations,Independence and Conditional Expectations,,"I have a simple question. It is known that, if X and Y are independent, then $\mathbb{E}[X | Y] = E[X]$ where X and Y are just random variables. However, is the converse true? i.e. Given that $\mathbb{E}[X | Y] = E[X]$, can I assume they are independent? Thanks","I have a simple question. It is known that, if X and Y are independent, then $\mathbb{E}[X | Y] = E[X]$ where X and Y are just random variables. However, is the converse true? i.e. Given that $\mathbb{E}[X | Y] = E[X]$, can I assume they are independent? Thanks",,"['probability', 'statistics', 'conditional-expectation']"
29,Is sampling without replacement well-defined on an infinite set?,Is sampling without replacement well-defined on an infinite set?,,"In a finite set, the concept of sampling with or without replacement is both well-defined. But is it the case for an infinite set? In other words, if X is finite, then Choosing 5 things at the same time from `X` and Choosing 5 things sequentially from `X` are different. For one, the former is combination and the latter is permutation. But are they any different if X is infinite?","In a finite set, the concept of sampling with or without replacement is both well-defined. But is it the case for an infinite set? In other words, if X is finite, then Choosing 5 things at the same time from `X` and Choosing 5 things sequentially from `X` are different. For one, the former is combination and the latter is permutation. But are they any different if X is infinite?",,"['probability', 'statistics', 'sampling']"
30,Sharpe Ratio with two assets,Sharpe Ratio with two assets,,"You have $1 million to invest. You can only invest in two stocks, A and B, with the following annualized expected returns and volatilities (i.e. standard deviation of return): Stock Expected Return Volatility A     10 %     10 % B     15 %     20 % Assume that interest rates are zero, and that the stocks’ returns are independent. Find the fully invested portfolios that maximize the Sharpe ratio, which is defined as the ratio between expected return and volatility. What is the maximum Sharpe ratio you can achieve by combining investments in A and B in this way? Attempt: I believe the expected returns are $E[R] = (1-w)*.10 + w*.15 = .10 + .05w$ However, the I am not sure how to set up the volatility. I would appreciate a thorough solution and explanation of this problem.","You have $1 million to invest. You can only invest in two stocks, A and B, with the following annualized expected returns and volatilities (i.e. standard deviation of return): Stock Expected Return Volatility A     10 %     10 % B     15 %     20 % Assume that interest rates are zero, and that the stocks’ returns are independent. Find the fully invested portfolios that maximize the Sharpe ratio, which is defined as the ratio between expected return and volatility. What is the maximum Sharpe ratio you can achieve by combining investments in A and B in this way? Attempt: I believe the expected returns are $E[R] = (1-w)*.10 + w*.15 = .10 + .05w$ However, the I am not sure how to set up the volatility. I would appreciate a thorough solution and explanation of this problem.",,"['statistics', 'finance']"
31,Determine the probability density function for $Y=X^2$.,Determine the probability density function for .,Y=X^2,"Question: Let $X\sim U(0,1)$.Determine the probability density function of the following variable $Y=X^2$. Defining the p.d.f for $X$ we have $f_X(x)=\begin{cases}0\:\:if\:x\notin[0,1]\\1\:\:if\:x\in[0,1]\end{cases}$ I know that $E(Y)=\int_\limits{R}{}g(x)f_x(x)dx=\int_\limits{0}^{1} x^2\times 1=\frac{1}{2}$, for $Y=g(X)=X^2$ $Var(Y)=E(Y^2)-(E(Y))^2=\frac{1}{5}-\frac{1}{9}=\frac{4}{45}$ According to my resolution the p.d.f should remain the same $f_X(x)=\begin{cases}0\:\:if\:x\notin[0,1]\\1\:\:if\:x\in[0,1]\end{cases}$. Question : What is the point of askin the p.d.f for $Y$? Or is my answer wrong? If so, why? Thanks in advance!","Question: Let $X\sim U(0,1)$.Determine the probability density function of the following variable $Y=X^2$. Defining the p.d.f for $X$ we have $f_X(x)=\begin{cases}0\:\:if\:x\notin[0,1]\\1\:\:if\:x\in[0,1]\end{cases}$ I know that $E(Y)=\int_\limits{R}{}g(x)f_x(x)dx=\int_\limits{0}^{1} x^2\times 1=\frac{1}{2}$, for $Y=g(X)=X^2$ $Var(Y)=E(Y^2)-(E(Y))^2=\frac{1}{5}-\frac{1}{9}=\frac{4}{45}$ According to my resolution the p.d.f should remain the same $f_X(x)=\begin{cases}0\:\:if\:x\notin[0,1]\\1\:\:if\:x\in[0,1]\end{cases}$. Question : What is the point of askin the p.d.f for $Y$? Or is my answer wrong? If so, why? Thanks in advance!",,"['probability', 'statistics']"
32,"If I toss $n$ coins and you toss $n + m$, what is the probability you'll have more heads?","If I toss  coins and you toss , what is the probability you'll have more heads?",n n + m,"My intuition is that it's the probability that you get at least 1 head in your extra $m$ tosses. Basically on the first $n$ tosses we have equal chance of winning so let's not even count those and only bet on the $m$ extra ones you have. I haven't been able to formalize this. My closest attempt is that this probability is $$P(M + N_2 > N_1) = P(M >N_2-N_1)=E[I(M>N_2-N1)]$$ where $M$ is the random variable for the number of heads in your extra coins, $N_2$ is for the heads in your first $n$ tosses and $N_1$ for mine. I don't know what to do with that indicator variable but I believe the solution lies that way.","My intuition is that it's the probability that you get at least 1 head in your extra $m$ tosses. Basically on the first $n$ tosses we have equal chance of winning so let's not even count those and only bet on the $m$ extra ones you have. I haven't been able to formalize this. My closest attempt is that this probability is $$P(M + N_2 > N_1) = P(M >N_2-N_1)=E[I(M>N_2-N1)]$$ where $M$ is the random variable for the number of heads in your extra coins, $N_2$ is for the heads in your first $n$ tosses and $N_1$ for mine. I don't know what to do with that indicator variable but I believe the solution lies that way.",,"['probability', 'statistics']"
33,"Standard Deviation for sums of fair dice given the number of dice, and the number of sides on each die","Standard Deviation for sums of fair dice given the number of dice, and the number of sides on each die",,"$n$ fair die are rolled, and each dice has $x$ sides, with the numbers on the sides going from $1$ to $x$, and with each side having a different number from the other sides. how do I figure out the standard deviation for the probability of the sums of the numbers that the die land on?","$n$ fair die are rolled, and each dice has $x$ sides, with the numbers on the sides going from $1$ to $x$, and with each side having a different number from the other sides. how do I figure out the standard deviation for the probability of the sums of the numbers that the die land on?",,"['probability', 'statistics']"
34,"Do people care about ""Six Sigma"" for probability distributions that are not Normal?","Do people care about ""Six Sigma"" for probability distributions that are not Normal?",,"Background At work there is a ""Six Sigma"" team that focuses on reducing quality defects. They're named that way because six sigma covers 99.7% around the mean (for a normal distribution). So if a manufacturing process covers six sigma of deviations when the randomness is normally distributed then the process is perhaps good. Question 1 Is it a thing to talk about how many sigmas are needed to ""cover"" other distributions? For example, the Gamma or the Poisson? Question 2 Is the following correct? $X$ is a continuous Uniform RV from $(0,1)$ which means the variance is $1/12$ and the SD is $\frac{1}{\sqrt{12}}$ Therefore ""six sigma"" covers $6\frac{1}{\sqrt{12}} \approx 1.73$ or all observations Question 3 I'm trying to calculate how many sigmas to cover the Exponential for any $\lambda$. Is the following correct? This is tough for me to figure out because unlike the binomial and normal, the exponential is not symmetrical around the mean I'm going to cop out and calculate how many sigmas cover the pdf starting from zero instead of around the mean... The variance of an exponential is $\frac{1}{\lambda^2}$ so sigma is $\frac{1}{\lambda}$ and I want to know how many multiples $m$ of sigma cover 99.7% $$P(X \le \frac{m}{\lambda}) = 0.997$$ $$1 - e^{-\lambda \frac{m}{\lambda}} = 0.997$$ $$1- e^{-m} = 0.997$$ $$ e^{-m} = 0.003$$ $$ m = -ln(0.003)$$ Therefore, regardless of lambda, to cover 99.7% of the pdf you need 5.8 sigmas (which is close to six sigmas). Thanks for your help!!","Background At work there is a ""Six Sigma"" team that focuses on reducing quality defects. They're named that way because six sigma covers 99.7% around the mean (for a normal distribution). So if a manufacturing process covers six sigma of deviations when the randomness is normally distributed then the process is perhaps good. Question 1 Is it a thing to talk about how many sigmas are needed to ""cover"" other distributions? For example, the Gamma or the Poisson? Question 2 Is the following correct? $X$ is a continuous Uniform RV from $(0,1)$ which means the variance is $1/12$ and the SD is $\frac{1}{\sqrt{12}}$ Therefore ""six sigma"" covers $6\frac{1}{\sqrt{12}} \approx 1.73$ or all observations Question 3 I'm trying to calculate how many sigmas to cover the Exponential for any $\lambda$. Is the following correct? This is tough for me to figure out because unlike the binomial and normal, the exponential is not symmetrical around the mean I'm going to cop out and calculate how many sigmas cover the pdf starting from zero instead of around the mean... The variance of an exponential is $\frac{1}{\lambda^2}$ so sigma is $\frac{1}{\lambda}$ and I want to know how many multiples $m$ of sigma cover 99.7% $$P(X \le \frac{m}{\lambda}) = 0.997$$ $$1 - e^{-\lambda \frac{m}{\lambda}} = 0.997$$ $$1- e^{-m} = 0.997$$ $$ e^{-m} = 0.003$$ $$ m = -ln(0.003)$$ Therefore, regardless of lambda, to cover 99.7% of the pdf you need 5.8 sigmas (which is close to six sigmas). Thanks for your help!!",,"['probability', 'statistics']"
35,Non-normal Bivariate distribution with normal margins,Non-normal Bivariate distribution with normal margins,,"I was working on a question and it stated the following: Now I am supposed to show that the bivariate distribution is non normal even though the marginal distributions are. I generated 1000 iid variates for X1 and X2 graphed them in the following: I know that the bivariate distribution is non-normal graphically but someone told me that we can show this analytically which I do not follow. Let $Z=X_1+X_2$, If $(X_1,X_2)$ was bivariate normal then $Z$ would also be normal, but $P(Z=0) \ge  P(|X_1| \le 1) \approx 0.68$ i.e. It has positive mass at zero, which cannot be the case with a continuous distribution. I believe the probabilities come from the way we defined $X_1$ and $X_2$ but I do not really see it. Why is $P(|X_1| \le 1) \approx 0.68$ And then why is $P(Z=0) \ge  P(|X_1| \le 1)$","I was working on a question and it stated the following: Now I am supposed to show that the bivariate distribution is non normal even though the marginal distributions are. I generated 1000 iid variates for X1 and X2 graphed them in the following: I know that the bivariate distribution is non-normal graphically but someone told me that we can show this analytically which I do not follow. Let $Z=X_1+X_2$, If $(X_1,X_2)$ was bivariate normal then $Z$ would also be normal, but $P(Z=0) \ge  P(|X_1| \le 1) \approx 0.68$ i.e. It has positive mass at zero, which cannot be the case with a continuous distribution. I believe the probabilities come from the way we defined $X_1$ and $X_2$ but I do not really see it. Why is $P(|X_1| \le 1) \approx 0.68$ And then why is $P(Z=0) \ge  P(|X_1| \le 1)$",,"['probability', 'statistics', 'probability-distributions', 'bivariate-distributions']"
36,Probability of throwing exactly 100 sixes on an unfair dice.,Probability of throwing exactly 100 sixes on an unfair dice.,,"Suppose we have a dice with probabilities $\frac{1}{6}$ for rolling one, $\frac{1}{12}$ for rolling two, $\frac{1}{12}$ for three, $\frac{1}{6}$ for four, $\frac{1}{6}$ for five, and $\frac{1}{3}$ for six. What is the probability of rolling exactly 100  sixes from 250 rolls? I think I should use multinomial distribition in this case. My try: C(150,250)*(2/3)^150/6^250","Suppose we have a dice with probabilities $\frac{1}{6}$ for rolling one, $\frac{1}{12}$ for rolling two, $\frac{1}{12}$ for three, $\frac{1}{6}$ for four, $\frac{1}{6}$ for five, and $\frac{1}{3}$ for six. What is the probability of rolling exactly 100  sixes from 250 rolls? I think I should use multinomial distribition in this case. My try: C(150,250)*(2/3)^150/6^250",,"['probability', 'statistics']"
37,Use Stein's Identity to Calculate Variance of X Bar Squared,Use Stein's Identity to Calculate Variance of X Bar Squared,,"Suppose that I have a random sample where each random variable is iid normally distributed with mean $\mu$ and variance $\sigma^2$.  Suppose I want to calculate the variance of $\bar{X}^2$ ""using Stein's idendity.""  I'm not quite sure that I understand how Stein's identity applies to this situation.  I know that the normal distribution is part of the linear exponential family, but I don't understand how that can help me find properties about $\bar{X}^2$.  Can somebody please show how this is supposed to work?  Thank you.","Suppose that I have a random sample where each random variable is iid normally distributed with mean $\mu$ and variance $\sigma^2$.  Suppose I want to calculate the variance of $\bar{X}^2$ ""using Stein's idendity.""  I'm not quite sure that I understand how Stein's identity applies to this situation.  I know that the normal distribution is part of the linear exponential family, but I don't understand how that can help me find properties about $\bar{X}^2$.  Can somebody please show how this is supposed to work?  Thank you.",,"['probability', 'probability-theory', 'statistics', 'normal-distribution', 'parameter-estimation']"
38,Variance of the Sum of $3$ Independent Trials,Variance of the Sum of  Independent Trials,3,"Note: I was able to solve this question while I was typing it up, but I'll just leave this here in case someone else needs it. Feel free to improve on it in the answers. Suppose $3$ marksmen shoot at a target. The $i$th marksman fire $n_i$ times, hitting the target each time with probability $p_i$, independently of his other shots and the shots of the other marksmen. Let $X$ be the total number of shots the target is hit. Find $Var(X)$. Let $X_i$ be the number of times the $i$th marksman hits the target. $Var(X)$ $= Var(X_1 + X_2 + X_3)$ $= Var(X_1) + Var(X_2) + Var(X_3)$ since the shots of one marksman is independent from the others. $= \sum_{i=1}^{3} Var(X_i)$ Let $I_i$ be the indicator that the $i$th shot hit. $Var(X_i)$ $= Var(I_1 + I_2 + \ldots +I_{n_i})$ $= Var(I_1) + Var(I_2) + \ldots + Var(I_{n_i})$ since each shots of one marksman is independent of his other shots. $= \sum_{j=1}^{n_i}Var(I_j)$ Now we have: $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}Var(I_j)$ $Var(I_j)$ $=E(I_j^2) - (E(I_j))^2$ $= (1^2 \cdot p_i+0^2 \cdot (1- p_i)) - p_i^2$ considering each shot has the same probability ($p_i$). = $p_i - p^2_i$ Thus, $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}(p_i - p^2_i) = \sum_{i=1}^{3}n_i(p_i - p^2_i) = \sum_{i=1}^{3}n_ip_i(1 - p_i)=\sum_{i=1}^{3}n_ip_iq_i$ where $q_i = 1 - p_i$","Note: I was able to solve this question while I was typing it up, but I'll just leave this here in case someone else needs it. Feel free to improve on it in the answers. Suppose $3$ marksmen shoot at a target. The $i$th marksman fire $n_i$ times, hitting the target each time with probability $p_i$, independently of his other shots and the shots of the other marksmen. Let $X$ be the total number of shots the target is hit. Find $Var(X)$. Let $X_i$ be the number of times the $i$th marksman hits the target. $Var(X)$ $= Var(X_1 + X_2 + X_3)$ $= Var(X_1) + Var(X_2) + Var(X_3)$ since the shots of one marksman is independent from the others. $= \sum_{i=1}^{3} Var(X_i)$ Let $I_i$ be the indicator that the $i$th shot hit. $Var(X_i)$ $= Var(I_1 + I_2 + \ldots +I_{n_i})$ $= Var(I_1) + Var(I_2) + \ldots + Var(I_{n_i})$ since each shots of one marksman is independent of his other shots. $= \sum_{j=1}^{n_i}Var(I_j)$ Now we have: $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}Var(I_j)$ $Var(I_j)$ $=E(I_j^2) - (E(I_j))^2$ $= (1^2 \cdot p_i+0^2 \cdot (1- p_i)) - p_i^2$ considering each shot has the same probability ($p_i$). = $p_i - p^2_i$ Thus, $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}(p_i - p^2_i) = \sum_{i=1}^{3}n_i(p_i - p^2_i) = \sum_{i=1}^{3}n_ip_i(1 - p_i)=\sum_{i=1}^{3}n_ip_iq_i$ where $q_i = 1 - p_i$",,"['statistics', 'variance']"
39,Expected number of turns in dice throwing,Expected number of turns in dice throwing,,"I generated a transition probability matrix for a scenario where I throw five dice and set aside those dice that are sixes. Then, I throw the remaining dice and again set aside the sixes - then I repeat this procedure until I get all the sixes. $X_n$ here represents the number of dices that are sixes after n rolls. $$\begin{pmatrix}\frac{5^5}{6^5} & \frac{3125}{6^5} & \frac{1250}{6^5} & \frac{250}{6^5} & \frac{25}{6^5} & \frac{1}{6^5}\\\ 0 & \frac{625}{6^4} & \frac{500}{6^4} & \frac{150}{6^4} & \frac{20}{6^4} & \frac{1}{6^4} \\\ 0& 0 & \frac{125}{6^3}& \frac{75}{6^3}& \frac{15}{6^3} & \frac{1}{6^3} \\\ 0 & 0& 0& \frac{25}{6^2}& \frac{10}{6^2}& \frac{1}{6^2}& \\ 0 & 0 & 0 & 0 & \frac{5}{6} & \frac{1}{6} \end{pmatrix}$$ I want to figure out how many turns it takes for me on average to get all sixes. I'm not even sure where to start with this problem. Is it a right approach to write a program where I calculate $P^n$ and see when the 6th column all equals to 1? Any pointers would be greatly appreciated.","I generated a transition probability matrix for a scenario where I throw five dice and set aside those dice that are sixes. Then, I throw the remaining dice and again set aside the sixes - then I repeat this procedure until I get all the sixes. $X_n$ here represents the number of dices that are sixes after n rolls. $$\begin{pmatrix}\frac{5^5}{6^5} & \frac{3125}{6^5} & \frac{1250}{6^5} & \frac{250}{6^5} & \frac{25}{6^5} & \frac{1}{6^5}\\\ 0 & \frac{625}{6^4} & \frac{500}{6^4} & \frac{150}{6^4} & \frac{20}{6^4} & \frac{1}{6^4} \\\ 0& 0 & \frac{125}{6^3}& \frac{75}{6^3}& \frac{15}{6^3} & \frac{1}{6^3} \\\ 0 & 0& 0& \frac{25}{6^2}& \frac{10}{6^2}& \frac{1}{6^2}& \\ 0 & 0 & 0 & 0 & \frac{5}{6} & \frac{1}{6} \end{pmatrix}$$ I want to figure out how many turns it takes for me on average to get all sixes. I'm not even sure where to start with this problem. Is it a right approach to write a program where I calculate $P^n$ and see when the 6th column all equals to 1? Any pointers would be greatly appreciated.",,"['probability', 'statistics', 'markov-chains']"
40,Asymptotic Distribution of Sample Mean,Asymptotic Distribution of Sample Mean,,"Suppose I have a discrete random variable $X$ which follows a geometric distribution on $x=0,1,2,...$ and I take a random sample from this distribution of size $n$.  What is the asymptotic distribution of $\bar X$? I already know that $E(X)=\frac{1-p}{p}$ and $V(X)=\frac{1-p}{p^2}$. This seems like an application of central limit theorem, so I'm sure that $\bar X$ converges to a normal distribution.  However, the part that's tripping me up is calculating the mean and variance of the normal distribution that it's converging to.  How do you do this?","Suppose I have a discrete random variable $X$ which follows a geometric distribution on $x=0,1,2,...$ and I take a random sample from this distribution of size $n$.  What is the asymptotic distribution of $\bar X$? I already know that $E(X)=\frac{1-p}{p}$ and $V(X)=\frac{1-p}{p^2}$. This seems like an application of central limit theorem, so I'm sure that $\bar X$ converges to a normal distribution.  However, the part that's tripping me up is calculating the mean and variance of the normal distribution that it's converging to.  How do you do this?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'central-limit-theorem']"
41,Strong consistency of sample variance,Strong consistency of sample variance,,"An unbiased estimator for a population's variance is: $$s^2=\frac{1}{n-1}\sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2$$ where $$\bar{X} = \frac{1}{n}\sum_{j=1}^{n} X_j$$ Now, it is widely known that this sample variance estimator is simply consistent (convergence in probability). I wonder, is it also true that it is strongly consistent, i.e. it converges to population variance almost surely? And if yes, are there any additional requirements for $\{X_n\}_{n\geq 1}$?","An unbiased estimator for a population's variance is: $$s^2=\frac{1}{n-1}\sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2$$ where $$\bar{X} = \frac{1}{n}\sum_{j=1}^{n} X_j$$ Now, it is widely known that this sample variance estimator is simply consistent (convergence in probability). I wonder, is it also true that it is strongly consistent, i.e. it converges to population variance almost surely? And if yes, are there any additional requirements for $\{X_n\}_{n\geq 1}$?",,['statistics']
42,Probability of mean and sample variance.,Probability of mean and sample variance.,,"Let $X_1,X_2 $ and $X_3$ be a random sample from $N(3,12)$ distribution. If $\bar X=\dfrac{1}{3} \sum_{i=1}^{3}X_i$ and $S^2=\dfrac{1}{2}\sum_{i=1}^{3}(X_i-\bar X)^2$ denote the sample mean and the sample variance respectively, then  $P(1.65<\bar X \leq4.35 ,.12<S^2 \leq55.26) $ is $(A)=.49$ $(B)=.50$ $(C)=.98$ $(D)={}$None of the above My first question is what does ** $P(1.65<\bar X \leq4.35 ,.12< S^2\leq55.26)$ **  mean? Is it $P(1.65<\bar X \leq4.35 ,.12<S^2 \leq55.26)=P(1.65<\bar X \leq4.35)\cap P(.12<S^2 \leq55.26)$? I have doubt in this very first step also i have done some work after that assuming(the step above mentioned) and i want to ask about that too but that i will ask if this step is fine. I will edit my question but before that please tell me about it. (My English is weak so please ignore grammar mistakes).","Let $X_1,X_2 $ and $X_3$ be a random sample from $N(3,12)$ distribution. If $\bar X=\dfrac{1}{3} \sum_{i=1}^{3}X_i$ and $S^2=\dfrac{1}{2}\sum_{i=1}^{3}(X_i-\bar X)^2$ denote the sample mean and the sample variance respectively, then  $P(1.65<\bar X \leq4.35 ,.12<S^2 \leq55.26) $ is $(A)=.49$ $(B)=.50$ $(C)=.98$ $(D)={}$None of the above My first question is what does ** $P(1.65<\bar X \leq4.35 ,.12< S^2\leq55.26)$ **  mean? Is it $P(1.65<\bar X \leq4.35 ,.12<S^2 \leq55.26)=P(1.65<\bar X \leq4.35)\cap P(.12<S^2 \leq55.26)$? I have doubt in this very first step also i have done some work after that assuming(the step above mentioned) and i want to ask about that too but that i will ask if this step is fine. I will edit my question but before that please tell me about it. (My English is weak so please ignore grammar mistakes).",,"['statistics', 'normal-distribution', 'statistical-inference']"
43,"Determine the marginal distributions of $(T_1, T_2)$",Determine the marginal distributions of,"(T_1, T_2)","A random variable $(T_1, T_2)$ is called two-dimensional exponentially   distributed with parameters $(\lambda_1, \lambda_2, \lambda_3)$, if   the distribution function has the form $$F(t_1,t_2) = \left\{\begin{matrix} 1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\lambda_1t_1-\lambda_2t_2-\lambda_3 \text{max}(t_1,t_2)}, \text{ if } t_1>0, t_2>0\\  0  \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\text{ else } \end{matrix}\right.$$ Determine the marginal distributions of $(T_1,T_2)$ I never do such task that's why I find this example to see how it work correct so I understand better and do it correct in test hopefully. When I understand correct, I need to calculate limit to $\infty$ of the distribution function for $t_1$ and seperately for $t_2$ then I have the marginal distributions of $(T_1,T_2)$. $$\lim_{t_1 \rightarrow \infty}f(t_1,t_2) = 1-e^{-\infty}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\infty} = 1+0-e^{-(\lambda_2+\lambda_3)t_2}+0=1-e^{-(\lambda_2+\lambda_3)t_2}$$ $$\lim_{t_2 \rightarrow \infty}f(t_1,t_2) = 1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-\infty}+e^{-\infty} = 1-e^{-(\lambda_1+\lambda_3)t_1}-0+0 = 1-e^{-(\lambda_1+\lambda_3)t_1}$$ Is it good like that or is all wrong? :(","A random variable $(T_1, T_2)$ is called two-dimensional exponentially   distributed with parameters $(\lambda_1, \lambda_2, \lambda_3)$, if   the distribution function has the form $$F(t_1,t_2) = \left\{\begin{matrix} 1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\lambda_1t_1-\lambda_2t_2-\lambda_3 \text{max}(t_1,t_2)}, \text{ if } t_1>0, t_2>0\\  0  \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\text{ else } \end{matrix}\right.$$ Determine the marginal distributions of $(T_1,T_2)$ I never do such task that's why I find this example to see how it work correct so I understand better and do it correct in test hopefully. When I understand correct, I need to calculate limit to $\infty$ of the distribution function for $t_1$ and seperately for $t_2$ then I have the marginal distributions of $(T_1,T_2)$. $$\lim_{t_1 \rightarrow \infty}f(t_1,t_2) = 1-e^{-\infty}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\infty} = 1+0-e^{-(\lambda_2+\lambda_3)t_2}+0=1-e^{-(\lambda_2+\lambda_3)t_2}$$ $$\lim_{t_2 \rightarrow \infty}f(t_1,t_2) = 1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-\infty}+e^{-\infty} = 1-e^{-(\lambda_1+\lambda_3)t_1}-0+0 = 1-e^{-(\lambda_1+\lambda_3)t_1}$$ Is it good like that or is all wrong? :(",,"['probability', 'probability-theory', 'statistics', 'discrete-mathematics']"
44,Expectation of minimum of $n$ samples from Gaussian distribution,Expectation of minimum of  samples from Gaussian distribution,n,Suppose we have a normal distribution with mean $\mu$ and variance $\sigma^2$. Suppose we draw $n$ samples from this distribution. What is the expectation of the minimum of these samples?,Suppose we have a normal distribution with mean $\mu$ and variance $\sigma^2$. Suppose we draw $n$ samples from this distribution. What is the expectation of the minimum of these samples?,,['statistics']
45,Error propagation of Gaussian measures,Error propagation of Gaussian measures,,"I am a physicist and I’ve never taken any statistics or probability classes. Nevertheless, I'm trying to build a set of lecture notes for my students of experimental physics regarding error propagation. Sadly, I haven’t found much literature on the subject from a rigorous point of view (it probably has a name and I just don’t know it). In any case, in physics we usually denote our measurements by $x\pm\Delta x$ where $x$ is our “best guess” and $\Delta x$ is an interval where we are reasonably certain that our measurement lies. If we have multiple measurements $x_1\pm\Delta x_1,\dots,x_n\pm\Delta x_n$ and we want to calculate a function of these, we would report it as $$f(x_1,\dots,x_n)\pm\sqrt{\sum_{i=1}^n\big(\partial_i f(x_1,\dots,x_n)\Delta x_i\big)^2}.$$ In my set of notes, I wanted to take a more rigorous approach. Instead of using the $x\pm \Delta x$ notation, I insisted on measurements being probability measures on an outcome space $X$. Then error propagation in a random variable $f:X\rightarrow Y$ is just the probability measure induced on $Y$ by $f$ $$P_f(F)=P(f^{-1}(F)).$$ Nevertheless, for my students to make calculations, I wanted to relate back to the old notation. I have two options: I can define $x\pm\Delta x$ as the probability measure on $\mathbb{R}$ whose probability density function is a gaussian centered at $x$ with standard deviation $\Delta x$. If I take this option, I would like to prove that for all random variables $f:\mathbb{R}^n\rightarrow \mathbb{R}$, the induced measure is reasonably approximated by a Gaussian measure described by the physicists’ formula. Moreover, I find the approach very appealing because of the central limit theorem. Am I right in that that theorem can be interpreted as saying that the average of a bunch of probability measures (the process of averaging understood as a random variable) tends to induce a Gaussian probability measure? I can define the expected value of a measure and its variance. I introduce the notation $x\pm \Delta x$ as stating that it is just some measure with expected value $x$ and variance $\Delta x$. Can I justify the physicists’ formula? I would really appreciate if the reader can pick its favorite choice and help me build arguments for the questions asked.","I am a physicist and I’ve never taken any statistics or probability classes. Nevertheless, I'm trying to build a set of lecture notes for my students of experimental physics regarding error propagation. Sadly, I haven’t found much literature on the subject from a rigorous point of view (it probably has a name and I just don’t know it). In any case, in physics we usually denote our measurements by $x\pm\Delta x$ where $x$ is our “best guess” and $\Delta x$ is an interval where we are reasonably certain that our measurement lies. If we have multiple measurements $x_1\pm\Delta x_1,\dots,x_n\pm\Delta x_n$ and we want to calculate a function of these, we would report it as $$f(x_1,\dots,x_n)\pm\sqrt{\sum_{i=1}^n\big(\partial_i f(x_1,\dots,x_n)\Delta x_i\big)^2}.$$ In my set of notes, I wanted to take a more rigorous approach. Instead of using the $x\pm \Delta x$ notation, I insisted on measurements being probability measures on an outcome space $X$. Then error propagation in a random variable $f:X\rightarrow Y$ is just the probability measure induced on $Y$ by $f$ $$P_f(F)=P(f^{-1}(F)).$$ Nevertheless, for my students to make calculations, I wanted to relate back to the old notation. I have two options: I can define $x\pm\Delta x$ as the probability measure on $\mathbb{R}$ whose probability density function is a gaussian centered at $x$ with standard deviation $\Delta x$. If I take this option, I would like to prove that for all random variables $f:\mathbb{R}^n\rightarrow \mathbb{R}$, the induced measure is reasonably approximated by a Gaussian measure described by the physicists’ formula. Moreover, I find the approach very appealing because of the central limit theorem. Am I right in that that theorem can be interpreted as saying that the average of a bunch of probability measures (the process of averaging understood as a random variable) tends to induce a Gaussian probability measure? I can define the expected value of a measure and its variance. I introduce the notation $x\pm \Delta x$ as stating that it is just some measure with expected value $x$ and variance $\Delta x$. Can I justify the physicists’ formula? I would really appreciate if the reader can pick its favorite choice and help me build arguments for the questions asked.",,"['probability', 'statistics', 'measure-theory', 'physics', 'error-propagation']"
46,"Suppose that $X\sim\operatorname{Exp}(\theta)$, show that no unbiased estimator for $\theta$ exists.","Suppose that , show that no unbiased estimator for  exists.",X\sim\operatorname{Exp}(\theta) \theta,"Exercise: Suppose that $X\sim\operatorname{Exp}(\theta)$, show that no unbiased estimator for $\theta$ exists. Hint: use the fact that $X$ is a complete and sufficient statistic for $\theta$. I have the following definitions to work with: Definition 1: An estimator $d(X)$ is unbiased for $\theta$ if $\operatorname{E}_\theta(d(X)) = \theta.$ Definition 2: Statistic $T$ is sufficient for $\theta\in\Omega$ if the conditional distribution of $X$ given $T$ is known (does not depend on $\theta$). Definition 3: Statistic $T$ is complete for $\theta\in\Omega$ if for any Borel function $f$ we have that $\operatorname{E}_\theta f(T) = 0$ for all $P_\theta\in\Omega$ implies that $f(T) = 0$. What I've tried: I'm not sure how to show that no unbiased estimator exists by using completeness or sufficiency, but I think it's possible in fact use a proof by contradiction, as is done here. If $d(X)$ is an unbiased estimator we have that $$\operatorname{E}_\theta d(X) = \int d(x)\theta e^{-\theta x}dx = \theta.$$ Unfortunately I don't know how to proceed from here. I've thought of using sufficiency and completeness to show that the above equation is not possible, but I haven't succeeded. Question: How do I show that there exists no unbiased estimator for $\theta$? Thanks in advance!","Exercise: Suppose that $X\sim\operatorname{Exp}(\theta)$, show that no unbiased estimator for $\theta$ exists. Hint: use the fact that $X$ is a complete and sufficient statistic for $\theta$. I have the following definitions to work with: Definition 1: An estimator $d(X)$ is unbiased for $\theta$ if $\operatorname{E}_\theta(d(X)) = \theta.$ Definition 2: Statistic $T$ is sufficient for $\theta\in\Omega$ if the conditional distribution of $X$ given $T$ is known (does not depend on $\theta$). Definition 3: Statistic $T$ is complete for $\theta\in\Omega$ if for any Borel function $f$ we have that $\operatorname{E}_\theta f(T) = 0$ for all $P_\theta\in\Omega$ implies that $f(T) = 0$. What I've tried: I'm not sure how to show that no unbiased estimator exists by using completeness or sufficiency, but I think it's possible in fact use a proof by contradiction, as is done here. If $d(X)$ is an unbiased estimator we have that $$\operatorname{E}_\theta d(X) = \int d(x)\theta e^{-\theta x}dx = \theta.$$ Unfortunately I don't know how to proceed from here. I've thought of using sufficiency and completeness to show that the above equation is not possible, but I haven't succeeded. Question: How do I show that there exists no unbiased estimator for $\theta$? Thanks in advance!",,"['statistics', 'probability-distributions', 'statistical-inference', 'parameter-estimation']"
47,"Show that if $X\sim\text{Pois}(\theta)$, then $I(\theta;X) = 1/\theta$.","Show that if , then .",X\sim\text{Pois}(\theta) I(\theta;X) = 1/\theta,"The Fisher information matrix $I(\theta;X)$ about $\theta$ based on $X$ is defined as the matrix with elements $$I_{i,j}(\theta;X) = \operatorname{Cov}_\theta\bigg(\frac{\partial}{\partial \theta_i}\log f_X(X\mid\theta), \frac{\partial}{\partial\theta_i}\log f_X(X\mid \theta) \bigg).$$ Exercise: Let $X\sim \text{Pois}(\theta)$. Show that $I(\theta;X) = 1/\theta$. What I've tried: If I'm not mistaken then $\dfrac{\partial}{\partial \theta_i} \log f_X(X \mid \theta) = \dfrac{x_i}{\theta} + \log e$. The Fisher information matrix (a $1\times 1$ matrix in this example) would be given by $\operatorname{Cov}_\theta\left(\dfrac{x_i} \theta + \log e, \dfrac{x_i} \theta + \log e\right)$. We know that $\operatorname{Cov}_\theta\left(\dfrac{x_i} \theta + \log e, \dfrac{x_i}{\theta} + \log e\right) = \operatorname{Var}_\theta\left(\dfrac{x_i}\theta + \log e\right) = \operatorname{Var}_\theta \left(\dfrac{x_i}{\theta}\right) = x_i^2 \operatorname{Var}_\theta \left(\dfrac 1 \theta \right).$ Obviously I'm doing something wrong as the $x_i^2$ before the variance is a problem. Besides that, I'm not sure if I can calculate $\operatorname{Var}_\theta\left(\dfrac 1   \theta \right)$. Question: How do I show that $I(\theta;X) = 1/\theta$? Thanks in advance!","The Fisher information matrix $I(\theta;X)$ about $\theta$ based on $X$ is defined as the matrix with elements $$I_{i,j}(\theta;X) = \operatorname{Cov}_\theta\bigg(\frac{\partial}{\partial \theta_i}\log f_X(X\mid\theta), \frac{\partial}{\partial\theta_i}\log f_X(X\mid \theta) \bigg).$$ Exercise: Let $X\sim \text{Pois}(\theta)$. Show that $I(\theta;X) = 1/\theta$. What I've tried: If I'm not mistaken then $\dfrac{\partial}{\partial \theta_i} \log f_X(X \mid \theta) = \dfrac{x_i}{\theta} + \log e$. The Fisher information matrix (a $1\times 1$ matrix in this example) would be given by $\operatorname{Cov}_\theta\left(\dfrac{x_i} \theta + \log e, \dfrac{x_i} \theta + \log e\right)$. We know that $\operatorname{Cov}_\theta\left(\dfrac{x_i} \theta + \log e, \dfrac{x_i}{\theta} + \log e\right) = \operatorname{Var}_\theta\left(\dfrac{x_i}\theta + \log e\right) = \operatorname{Var}_\theta \left(\dfrac{x_i}{\theta}\right) = x_i^2 \operatorname{Var}_\theta \left(\dfrac 1 \theta \right).$ Obviously I'm doing something wrong as the $x_i^2$ before the variance is a problem. Besides that, I'm not sure if I can calculate $\operatorname{Var}_\theta\left(\dfrac 1   \theta \right)$. Question: How do I show that $I(\theta;X) = 1/\theta$? Thanks in advance!",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
48,Find examples of $f$ such that $f'$ is a probability density function,Find examples of  such that  is a probability density function,f f',"Let $f$ be a probability density function with a continuous derivative on $[1,+\infty)$. I know that the conditions for a function to be probability density is as follows (a) $f(x)\geq0$ for all $x\in\mathbb{R}$ (b) $\int_{-\infty}^{+\infty}f(x)dx=1$ Now I want to find examples of $f$ such that $f'$ is a probability density function. Is there exist any way to do this?","Let $f$ be a probability density function with a continuous derivative on $[1,+\infty)$. I know that the conditions for a function to be probability density is as follows (a) $f(x)\geq0$ for all $x\in\mathbb{R}$ (b) $\int_{-\infty}^{+\infty}f(x)dx=1$ Now I want to find examples of $f$ such that $f'$ is a probability density function. Is there exist any way to do this?",,"['probability', 'probability-theory', 'statistics']"
49,conditional expectation deck of cards,conditional expectation deck of cards,,"Let $X1, \ X2, \ X3, \ X4$ denote the number of hearts, diamonds, clubs, and spades drawn from 10 draws with replacement from a standard 52 card deck of playing cards. What is $E\ [X2\ |\ X1 + X4 = 5]$  ? Is the answer $1/4 * 5 = 5/4$ ? My reasoning is that we know that 5 of the 10 cards are not $X2$, so we are only concerned with the other 5. So we multiply the other 5 by the probability of getting a heart, i.e. $1/4$. Thanks in advance","Let $X1, \ X2, \ X3, \ X4$ denote the number of hearts, diamonds, clubs, and spades drawn from 10 draws with replacement from a standard 52 card deck of playing cards. What is $E\ [X2\ |\ X1 + X4 = 5]$  ? Is the answer $1/4 * 5 = 5/4$ ? My reasoning is that we know that 5 of the 10 cards are not $X2$, so we are only concerned with the other 5. So we multiply the other 5 by the probability of getting a heart, i.e. $1/4$. Thanks in advance",,"['probability', 'probability-theory', 'statistics', 'expectation', 'conditional-expectation']"
50,Show that Chebyshev's inequality becomes an equality for this particular k,Show that Chebyshev's inequality becomes an equality for this particular k,,"For the discrete random variable X satisfying \begin{align} p(X= 0) & = 1-\frac{1}{k^2}\\ p(X=-k) & = \frac{1}{2k^2} \\ p(X=+k) & = \frac{1}{2k^2} \\ \end{align} with $k\geq1$, how can I show that Chebyshev's inequality becomes an equality for this particular value of k?","For the discrete random variable X satisfying \begin{align} p(X= 0) & = 1-\frac{1}{k^2}\\ p(X=-k) & = \frac{1}{2k^2} \\ p(X=+k) & = \frac{1}{2k^2} \\ \end{align} with $k\geq1$, how can I show that Chebyshev's inequality becomes an equality for this particular value of k?",,"['statistics', 'random-variables']"
51,Probability density for velocity in mechanical energy,Probability density for velocity in mechanical energy,,"To be sure my basic physics isn't rusty... Consider a 2-D bowled shaped classical potential well within which a classical particle of mass m is rolling. In this system the conservation of energy holds so the particle of mass m would roll from one end to another. Because conservation of energy holds, we expect the mechanical energy to be $E=T + U = \frac{1}{2}mv^{2}-mgh$ where v is the velocity of the particle g is the gravitational acceleration and h is the heigh relative to the ground. In classical physics, the maximum velocity of the particle occurs when the particle is at $r=\left ( x,h=0 \right )$  and the minimum velocity occurs when the particle is at some position $r=\left ( x,h \right )$ \exists h on both end of the well such that its kinetic energy T is 0 and potential energy U is at maximum. Again, this follows from the conservation of energy: $\Delta T= - \Delta U$ Now, I would like to construct a mathematical equation describing the probability of finding this particle of mass m as a function of its velocity. Intuitively, the greater the velocity of the particle at some point the lower the probability to find the particle and the smaller its velocity is the higher the probability to find the particle. Solving $ E=T + U = \frac{1}{2}mv^{2}-mgh$ for v: $v=\sqrt{\frac{2 \left ( E+mgh \right ) }{m}}$ If we want to explictly determine the probability of finding the particle as a function of its velocity, we should expect the probability density as a function of velocity to be of the form $P=P\left ( v \right ) \propto \frac{1}{v}$ which comports to our common sense intuition. How can I go about constructing a more explicit and informative equation that would enable to me determine the probability of finding the particle as a function of its velocity? Any help is appreciated.","To be sure my basic physics isn't rusty... Consider a 2-D bowled shaped classical potential well within which a classical particle of mass m is rolling. In this system the conservation of energy holds so the particle of mass m would roll from one end to another. Because conservation of energy holds, we expect the mechanical energy to be $E=T + U = \frac{1}{2}mv^{2}-mgh$ where v is the velocity of the particle g is the gravitational acceleration and h is the heigh relative to the ground. In classical physics, the maximum velocity of the particle occurs when the particle is at $r=\left ( x,h=0 \right )$  and the minimum velocity occurs when the particle is at some position $r=\left ( x,h \right )$ \exists h on both end of the well such that its kinetic energy T is 0 and potential energy U is at maximum. Again, this follows from the conservation of energy: $\Delta T= - \Delta U$ Now, I would like to construct a mathematical equation describing the probability of finding this particle of mass m as a function of its velocity. Intuitively, the greater the velocity of the particle at some point the lower the probability to find the particle and the smaller its velocity is the higher the probability to find the particle. Solving $ E=T + U = \frac{1}{2}mv^{2}-mgh$ for v: $v=\sqrt{\frac{2 \left ( E+mgh \right ) }{m}}$ If we want to explictly determine the probability of finding the particle as a function of its velocity, we should expect the probability density as a function of velocity to be of the form $P=P\left ( v \right ) \propto \frac{1}{v}$ which comports to our common sense intuition. How can I go about constructing a more explicit and informative equation that would enable to me determine the probability of finding the particle as a function of its velocity? Any help is appreciated.",,"['statistics', 'physics', 'mathematical-physics', 'mathematical-modeling', 'applications']"
52,"How to find a power function, representing a model for data?","How to find a power function, representing a model for data?",,"In a calculus textbook by late James Stewart I encountered an exercise to find a power function, as a mathematical model approximately representing data: The table shows the number N of species of reptiles and amphibians inhabiting Caribbean islands and the area A of the island in square miles.   a) Use a power function to model N as a function of A.  Island      A        N  Saba        4        5 Monserrat   40       9 Puerto Rico 3,459    40 Jamaica     4,411    39 Hispaniola  29,418   84 Cuba        44,218   76 The confusing thing is that in the previous pages of the textbook there were no explanation of how to do it, except using special software like Mathematica© and only for a linear model. The correct answer from the textbook is: $N = 3.1046 A^{0.308}$ Could anyone explain to me, please, how this answer was obtained? I myself found another answer $N = A^{-2.47295*10^{-6} * A + 0.504732}$ by finding $log_A N$ for every item in the table (except the first one, which looked far out of order) and then finding the linear regression for these values with Mathematica©'s tool Fit (which I took as the exponent value for my function): Island      A        N(real)   N(the textbook's func.) N(my func.) Saba        4        5         4.75817                 2.01314 Monserrat   40       9         9.6703                  6.43358 Puerto Rico 3,459    40        38.1946                 57.0098 Jamaica     4,411    39        41.1645                 63.0608 Hispaniola  29,418   84        73.848                  85.1851 Cuba        44,218   76        83.724                  68.6738 The right answer definitely looks more close to the reality, so could anyone, please, explain to me, how it was obtained?","In a calculus textbook by late James Stewart I encountered an exercise to find a power function, as a mathematical model approximately representing data: The table shows the number N of species of reptiles and amphibians inhabiting Caribbean islands and the area A of the island in square miles.   a) Use a power function to model N as a function of A.  Island      A        N  Saba        4        5 Monserrat   40       9 Puerto Rico 3,459    40 Jamaica     4,411    39 Hispaniola  29,418   84 Cuba        44,218   76 The confusing thing is that in the previous pages of the textbook there were no explanation of how to do it, except using special software like Mathematica© and only for a linear model. The correct answer from the textbook is: $N = 3.1046 A^{0.308}$ Could anyone explain to me, please, how this answer was obtained? I myself found another answer $N = A^{-2.47295*10^{-6} * A + 0.504732}$ by finding $log_A N$ for every item in the table (except the first one, which looked far out of order) and then finding the linear regression for these values with Mathematica©'s tool Fit (which I took as the exponent value for my function): Island      A        N(real)   N(the textbook's func.) N(my func.) Saba        4        5         4.75817                 2.01314 Monserrat   40       9         9.6703                  6.43358 Puerto Rico 3,459    40        38.1946                 57.0098 Jamaica     4,411    39        41.1645                 63.0608 Hispaniola  29,418   84        73.848                  85.1851 Cuba        44,218   76        83.724                  68.6738 The right answer definitely looks more close to the reality, so could anyone, please, explain to me, how it was obtained?",,"['statistics', 'mathematical-modeling']"
53,On the functional form of Kullback-Leibler information,On the functional form of Kullback-Leibler information,,"I'm studying about information criteria and came across with the following proof in my reference material (page 34): On the functional form of Kullback-Leibler information If the differentiable function $F$ defined on $(0, \infty)$ satisfies   the relationship $$\sum_{i=1}^k g_iF(f_i) \leq \sum_{i=1}^k g_iF(g_i)$$ for any two probability functions $\{g_1, ..., g_k\}$ and $\{f_1, ..., f_k\}$, then $F(g)=\alpha + \beta\log g$ for some $\alpha, \beta$ with    $\beta>0$. Proof: In order to demonstrate that $F (g) = \alpha + \beta \log g$,   it suffices to show that $gF'(g)=\beta>0$ and hence that $\partial F /  \partial g =\beta/g$ Let $h = (h_1, ..., h_k)^T$ be an arbitrary vector that satisfies   $\sum_{i=1}^k h_i=0$ and $|h_i| \leq \max\{g_i , 1 − g_i\}$. Since   $g+\lambda h$ is a probability distribution, it follows from the   assumption that $$\phi(\lambda)\equiv \sum_{i=1}^k g_iF(g_i+\lambda h_i) \leq  \sum_{i=1}^k g_iF(g_i)=\phi(0).$$ Therefore, since $$\phi'(\lambda)= \sum_{i=1}^k g_iF'(g_i+\lambda  h_i)h_i,\;\;\;\;{\color{red}{\phi'(0)= \sum_{i=1}^k g_iF'(g_i)h_i=0}}$$ are always true, by writing $h_1=C, h_2=-C, h_i=0\;(i=3, ..., k)$, we   have $$g_1F'(g_1)=g_2F'(g_2)=\text{const}=\beta.$$ The equality for other values of $i$ can be shown in a similar manner   Q.E.D. QUESTION 1: What happened starting from the equation I marked as red? Why is this equation equal to zero? QUESTION 2: It was not given in my material but I assume the values $\lambda$ and $C$ are constants?","I'm studying about information criteria and came across with the following proof in my reference material (page 34): On the functional form of Kullback-Leibler information If the differentiable function $F$ defined on $(0, \infty)$ satisfies   the relationship $$\sum_{i=1}^k g_iF(f_i) \leq \sum_{i=1}^k g_iF(g_i)$$ for any two probability functions $\{g_1, ..., g_k\}$ and $\{f_1, ..., f_k\}$, then $F(g)=\alpha + \beta\log g$ for some $\alpha, \beta$ with    $\beta>0$. Proof: In order to demonstrate that $F (g) = \alpha + \beta \log g$,   it suffices to show that $gF'(g)=\beta>0$ and hence that $\partial F /  \partial g =\beta/g$ Let $h = (h_1, ..., h_k)^T$ be an arbitrary vector that satisfies   $\sum_{i=1}^k h_i=0$ and $|h_i| \leq \max\{g_i , 1 − g_i\}$. Since   $g+\lambda h$ is a probability distribution, it follows from the   assumption that $$\phi(\lambda)\equiv \sum_{i=1}^k g_iF(g_i+\lambda h_i) \leq  \sum_{i=1}^k g_iF(g_i)=\phi(0).$$ Therefore, since $$\phi'(\lambda)= \sum_{i=1}^k g_iF'(g_i+\lambda  h_i)h_i,\;\;\;\;{\color{red}{\phi'(0)= \sum_{i=1}^k g_iF'(g_i)h_i=0}}$$ are always true, by writing $h_1=C, h_2=-C, h_i=0\;(i=3, ..., k)$, we   have $$g_1F'(g_1)=g_2F'(g_2)=\text{const}=\beta.$$ The equality for other values of $i$ can be shown in a similar manner   Q.E.D. QUESTION 1: What happened starting from the equation I marked as red? Why is this equation equal to zero? QUESTION 2: It was not given in my material but I assume the values $\lambda$ and $C$ are constants?",,"['statistics', 'information-theory']"
54,In a sequence of Bernoulli trials let X be the length of the run,In a sequence of Bernoulli trials let X be the length of the run,,"In a sequence of Bernoulli trials let X be the length of the run (of either successes or failures) started by the first trial. Find the distribution of X, E(X) and Var(X) The answers to this question (from the back of the book) are: $p_k=p^kq+q^kp$ $E(X)=pq^{-1}+qp^{-1}$ $Var(X)=pq^{-2}+qp^{-2}-2$ I don't understand how to get these answers though. Can anyone explain in detail?","In a sequence of Bernoulli trials let X be the length of the run (of either successes or failures) started by the first trial. Find the distribution of X, E(X) and Var(X) The answers to this question (from the back of the book) are: $p_k=p^kq+q^kp$ $E(X)=pq^{-1}+qp^{-1}$ $Var(X)=pq^{-2}+qp^{-2}-2$ I don't understand how to get these answers though. Can anyone explain in detail?",,"['statistics', 'bernoulli-numbers']"
55,Show a statistic is complete but not suffcient,Show a statistic is complete but not suffcient,,"Let $X_1, ...,X_n$$(n \geq 2)$ be i.i.d. random variables having the normal distribution $N(\theta, 2)$ when $\theta = 0$ and the normal distribution $N(\theta, 1)$ when $θ ∈ R$ and $θ \neq 0$. Show that the sample mean $\bar X$ is a complete statistic for $θ$ but it is not a sufficient statistic for $θ$. My idea is to show $E[f(\bar X)]=0$ implies $f(\bar X)=0$ (where $f$ is any measurable function), which is the definition of completeness.And I get the following formula: $$\int_{{\mathbb{R}}^n}f(\bar X)\exp\left(-\frac{x_1^2+\cdots+x_n^2}{4}\right)\text{d}x_1\cdots\text{d}x_n=0 $$ when $θ = 0$ ,and $$\int_{{\mathbb{R}}^n}f(\bar X)\exp\left(-\frac{(x_1-\theta)^2+\cdots+(x_n-\theta)^2}{2}\right)\text{d}x_1\cdots\text{d}x_n=0 $$  when $θ \neq 0$. But I don't know how to prove $f(\bar X)=0$ by the two formula above. Can someone help me?","Let $X_1, ...,X_n$$(n \geq 2)$ be i.i.d. random variables having the normal distribution $N(\theta, 2)$ when $\theta = 0$ and the normal distribution $N(\theta, 1)$ when $θ ∈ R$ and $θ \neq 0$. Show that the sample mean $\bar X$ is a complete statistic for $θ$ but it is not a sufficient statistic for $θ$. My idea is to show $E[f(\bar X)]=0$ implies $f(\bar X)=0$ (where $f$ is any measurable function), which is the definition of completeness.And I get the following formula: $$\int_{{\mathbb{R}}^n}f(\bar X)\exp\left(-\frac{x_1^2+\cdots+x_n^2}{4}\right)\text{d}x_1\cdots\text{d}x_n=0 $$ when $θ = 0$ ,and $$\int_{{\mathbb{R}}^n}f(\bar X)\exp\left(-\frac{(x_1-\theta)^2+\cdots+(x_n-\theta)^2}{2}\right)\text{d}x_1\cdots\text{d}x_n=0 $$  when $θ \neq 0$. But I don't know how to prove $f(\bar X)=0$ by the two formula above. Can someone help me?",,['statistics']
56,What is the difference between dispersion and mean root square deviation?,What is the difference between dispersion and mean root square deviation?,,"So the question is why do we have mean root square deviation (standard deviation)?  Isn't it enough just to use only such characteristic as dispersion? Why is root square deviation (standard deviation) used if we already have dispersion? I'd be very happy to discuss this topic with you, guys! Thank you in advance!","So the question is why do we have mean root square deviation (standard deviation)?  Isn't it enough just to use only such characteristic as dispersion? Why is root square deviation (standard deviation) used if we already have dispersion? I'd be very happy to discuss this topic with you, guys! Thank you in advance!",,"['calculus', 'probability', 'probability-theory', 'statistics', 'standard-deviation']"
57,Let $T_1$ be an unbiased estimator of $\theta$. Show that $T_1^2$ is a biased estimator of $\theta^2$.,Let  be an unbiased estimator of . Show that  is a biased estimator of .,T_1 \theta T_1^2 \theta^2,Let $T$ be an unbiased estimator of $\theta$. Show that $T^2$ is a biased estimator of $\theta^2$. We have $\operatorname{E}(T)=\theta$ by assumption. Then $\operatorname{E}(T^2)=\operatorname{Var}(T)+(\operatorname{E}(T))^2=\operatorname{Var}(T)+\theta^2$. But we can't guarantee $\operatorname{Var}(T)$ is not zero. How can the question be answered?,Let $T$ be an unbiased estimator of $\theta$. Show that $T^2$ is a biased estimator of $\theta^2$. We have $\operatorname{E}(T)=\theta$ by assumption. Then $\operatorname{E}(T^2)=\operatorname{Var}(T)+(\operatorname{E}(T))^2=\operatorname{Var}(T)+\theta^2$. But we can't guarantee $\operatorname{Var}(T)$ is not zero. How can the question be answered?,,"['statistics', 'expectation', 'estimation', 'variance']"
58,Probability Density Function Interpretation,Probability Density Function Interpretation,,"I am BEGINNING to study Statistics and Probability and am trying to understand what a probability density function is/is used for. My current interpretation is : The name function indicates to me something that provides an output dependent on the input I give it. Taking for example the PDF for the standard normal distribution (shown below); $$ p(x) = \mathcal{N}(x;0,1) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2} $$ In my mind the above equation describes the probability/likelihood that a continuous random variable $x$ takes on a value in it's sample space (i.e. set of all possible values). So lets say this PDF (normal distribution) describes the time taken for men to run a marathon (real average is about 4 hours). If plotting this PDF the $y$-axis would contain non-negligible values for corresponding marathon times from around 2 hours (on the extreme left) to 6 hours (on the extreme right) with the average/mean centered at 4 hours. If I programmed the PDF equation (above) into computer and then ran a script that requested a input $x$; I could provide any real valued input in the domain from $-\infty$ to $+\infty$ and the output of the PDF equation would give me the probability that a man would finish the race in that time? Why is this useful; If i'm standing at the start line before the race begins and a competitor walks over to me and bets me $20 that he can finish the race in exactly 3 hours, if I know nothing else about him, his training regime etc... I can quickly take out my phone, run the script and enter the value 3 hours and the output can be interpreted as the probability the man will finish the race in exactly 3 hours? If I fancy the odds I might decide it is a good idea to accept his bet. Questions related to my current understanding are as follows : (1) Is the the above interpretation correct whole/partially? (1.1) If partially then where exactly am I getting my wires crossed? (2) Bonus Question : How would you link an understanding of standard deviation and/or variance into this example?","I am BEGINNING to study Statistics and Probability and am trying to understand what a probability density function is/is used for. My current interpretation is : The name function indicates to me something that provides an output dependent on the input I give it. Taking for example the PDF for the standard normal distribution (shown below); $$ p(x) = \mathcal{N}(x;0,1) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2} $$ In my mind the above equation describes the probability/likelihood that a continuous random variable $x$ takes on a value in it's sample space (i.e. set of all possible values). So lets say this PDF (normal distribution) describes the time taken for men to run a marathon (real average is about 4 hours). If plotting this PDF the $y$-axis would contain non-negligible values for corresponding marathon times from around 2 hours (on the extreme left) to 6 hours (on the extreme right) with the average/mean centered at 4 hours. If I programmed the PDF equation (above) into computer and then ran a script that requested a input $x$; I could provide any real valued input in the domain from $-\infty$ to $+\infty$ and the output of the PDF equation would give me the probability that a man would finish the race in that time? Why is this useful; If i'm standing at the start line before the race begins and a competitor walks over to me and bets me $20 that he can finish the race in exactly 3 hours, if I know nothing else about him, his training regime etc... I can quickly take out my phone, run the script and enter the value 3 hours and the output can be interpreted as the probability the man will finish the race in exactly 3 hours? If I fancy the odds I might decide it is a good idea to accept his bet. Questions related to my current understanding are as follows : (1) Is the the above interpretation correct whole/partially? (1.1) If partially then where exactly am I getting my wires crossed? (2) Bonus Question : How would you link an understanding of standard deviation and/or variance into this example?",,"['probability', 'statistics', 'normal-distribution']"
59,Probability of flipping heads where the coin's bias is drawn from a uniform distribution,Probability of flipping heads where the coin's bias is drawn from a uniform distribution,,"What is the probability of flipping a head on a coin where the coin's probability of flipping a head is drawn uniformly from $[0,1]$. I think this question is easy, however I am getting a result that makes no sense. Please don't directly give me the answer. I can look that up in a book. Let $H$ be the binary event that the a head is flipped. Let $Q$ be Uniform($0,1$). Informal attempt: Firstly, it's easy to see that given that we know $Q=q$, the probability of flipping heads is simply $1/q$. Since the distribution of $Q$ is uniform, we just need the average value of the function $1/q$. In particular, the probability must be $\frac{1}{1-0} \int_0^1 \frac{1}{q} \, dq$, which does not converge! Formal attempt: We wish to compute $P(H= \text{True})$. By the total law of probability, we have: $$P(H=\text{True}) = \int_0^1 P(H=\text{True}\mid Q=q)f_Q(q) \, dq = \int_0^1 \frac{1}{q} (1) \,dq$$ Again, this does not converge!? What am I doing wrong? I have a strong feeling my mistake lies in the fact that I am mixing continuous and discrete distributions. That is, I am conditioning on an event that actually cannot exist, i.e. $P(Q=q)=0$.","What is the probability of flipping a head on a coin where the coin's probability of flipping a head is drawn uniformly from $[0,1]$. I think this question is easy, however I am getting a result that makes no sense. Please don't directly give me the answer. I can look that up in a book. Let $H$ be the binary event that the a head is flipped. Let $Q$ be Uniform($0,1$). Informal attempt: Firstly, it's easy to see that given that we know $Q=q$, the probability of flipping heads is simply $1/q$. Since the distribution of $Q$ is uniform, we just need the average value of the function $1/q$. In particular, the probability must be $\frac{1}{1-0} \int_0^1 \frac{1}{q} \, dq$, which does not converge! Formal attempt: We wish to compute $P(H= \text{True})$. By the total law of probability, we have: $$P(H=\text{True}) = \int_0^1 P(H=\text{True}\mid Q=q)f_Q(q) \, dq = \int_0^1 \frac{1}{q} (1) \,dq$$ Again, this does not converge!? What am I doing wrong? I have a strong feeling my mistake lies in the fact that I am mixing continuous and discrete distributions. That is, I am conditioning on an event that actually cannot exist, i.e. $P(Q=q)=0$.",,"['probability', 'statistics']"
60,Is there an easy way to (mentally) sum i.i.d. discrete uniform random variables?,Is there an easy way to (mentally) sum i.i.d. discrete uniform random variables?,,"I was recently trying to do some mental mathematics and found myself stumped when trying to handle what ought to be a simple problem. My issues essentially boiled down to the question that I've asked, although for the sake of making the numbers easy I suppose that we can stick with the discrete uniform distribution with P(X=x)=1/6, i.e. the distribution that most often be used in problems related to six-sided fair dice. So, with all of this is mind, is there any ""easy"" way to even mentally do something as simple as find the distribution of the sum of two i.i.d discrete uniform random variables, each with P(X=x)=1/6? What if I was trying do something more advanced like finding the probability that the sum of 6 fair six-sided dice is greater than or equal to 18?","I was recently trying to do some mental mathematics and found myself stumped when trying to handle what ought to be a simple problem. My issues essentially boiled down to the question that I've asked, although for the sake of making the numbers easy I suppose that we can stick with the discrete uniform distribution with P(X=x)=1/6, i.e. the distribution that most often be used in problems related to six-sided fair dice. So, with all of this is mind, is there any ""easy"" way to even mentally do something as simple as find the distribution of the sum of two i.i.d discrete uniform random variables, each with P(X=x)=1/6? What if I was trying do something more advanced like finding the probability that the sum of 6 fair six-sided dice is greater than or equal to 18?",,['statistics']
61,Applications of Beta Distribution [closed],Applications of Beta Distribution [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question What properties of random variable leads to modeling it with Beta Distribution? Context: If you ask the same question about Bernouli distribution, the answer would be the distribution of a random variable that has only two outcomes and the probability of one outcome is fixed at $p$. Now how can we answer a similar question about Beta distribution?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question What properties of random variable leads to modeling it with Beta Distribution? Context: If you ask the same question about Bernouli distribution, the answer would be the distribution of a random variable that has only two outcomes and the probability of one outcome is fixed at $p$. Now how can we answer a similar question about Beta distribution?",,"['probability', 'statistics']"
62,Understanding the dependency of trials.,Understanding the dependency of trials.,,"Problem : A certain population is made up of 80% Mexican Americans.Select a jury of 12.Find the mean and standard deviation. Problem Link : https://youtu.be/4Ew60JEPGUk?list=PL5102DFDC6790F3D0&t=865 The instructor assumes success as selecting a Mexican American.So the problem boils down to calculating the expected no of successes i.e if X is a random variable representing the number of successes then we have to find  $E(X)$. According to my understanding, we can use binomial distribution only if the trials are independent but in this problem selecting a person from the population changes one of the amounts and hence the probability of success so how is it feasible to apply binomial distribution here? Let me know if I'm not clear.","Problem : A certain population is made up of 80% Mexican Americans.Select a jury of 12.Find the mean and standard deviation. Problem Link : https://youtu.be/4Ew60JEPGUk?list=PL5102DFDC6790F3D0&t=865 The instructor assumes success as selecting a Mexican American.So the problem boils down to calculating the expected no of successes i.e if X is a random variable representing the number of successes then we have to find  $E(X)$. According to my understanding, we can use binomial distribution only if the trials are independent but in this problem selecting a person from the population changes one of the amounts and hence the probability of success so how is it feasible to apply binomial distribution here? Let me know if I'm not clear.",,"['statistics', 'probability-distributions', 'binomial-distribution']"
63,How many times on average would one need to retransmit a message before it is received correctly?,How many times on average would one need to retransmit a message before it is received correctly?,,In a communication system each message is sent 4 times over an unreliable channel. Assume that individual transmissions are independent from each other. It is known that the probability to receive the message correctly at least once is 0.9984. How many times on average would one need to retransmit a message before it is received correctly?,In a communication system each message is sent 4 times over an unreliable channel. Assume that individual transmissions are independent from each other. It is known that the probability to receive the message correctly at least once is 0.9984. How many times on average would one need to retransmit a message before it is received correctly?,,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'binomial-distribution']"
64,How to create a positive definite covariance matrix from an adjacency matrix?,How to create a positive definite covariance matrix from an adjacency matrix?,,"I am currently trying to create a covariance matrix from an adjacency matrix. I would like to create the covariance matrix by the following two steps: 1) Wherever there is a connection, I would place a correlation parameter $\rho$. 2) Replace the diagonals of all $0$'s with a common variance, like $1$. As a reproducible example, consider the following adjacency matrix for $5$ nodes: $$ A = \begin{pmatrix} 0 & 0 & 0 & 1 & 1 \\  0 & 0 & 1 & 0 & 1 \\  0 & 1 & 0 & 1 & 0 \\  1 & 0 & 1 & 0 & 0 \\  1 & 1 & 0 & 0 & 0 \\  \end{pmatrix} $$ We can see from the first row that node $1$ is connected with node $4$ and node $5$. From this, we replace all the $1$'s with a common correlation parameter $\rho$ and put a $1$ in the diagonal to get: $$ \Sigma = \begin{pmatrix} 1 & 0 & 0 & \rho & \rho \\  0 & 1 & \rho & 0 & \rho \\  0 & \rho & 1 & \rho & 0 \\  \rho & 0 & \rho & 1 & 0 \\  \rho & \rho & 0 & 0 & 1 \\  \end{pmatrix} $$ HOWEVER, this covariance matrix is not positive definite. if I introduce say, $\rho = 0.9$. Hence, if I fit a multivariate normal model on this covariance matrix, I would get errors in programs. Is there a way around it? I also noticed if I increase the marginal variances ($1$ above), then the problem goes away. Is there a reason why? Thanks!","I am currently trying to create a covariance matrix from an adjacency matrix. I would like to create the covariance matrix by the following two steps: 1) Wherever there is a connection, I would place a correlation parameter $\rho$. 2) Replace the diagonals of all $0$'s with a common variance, like $1$. As a reproducible example, consider the following adjacency matrix for $5$ nodes: $$ A = \begin{pmatrix} 0 & 0 & 0 & 1 & 1 \\  0 & 0 & 1 & 0 & 1 \\  0 & 1 & 0 & 1 & 0 \\  1 & 0 & 1 & 0 & 0 \\  1 & 1 & 0 & 0 & 0 \\  \end{pmatrix} $$ We can see from the first row that node $1$ is connected with node $4$ and node $5$. From this, we replace all the $1$'s with a common correlation parameter $\rho$ and put a $1$ in the diagonal to get: $$ \Sigma = \begin{pmatrix} 1 & 0 & 0 & \rho & \rho \\  0 & 1 & \rho & 0 & \rho \\  0 & \rho & 1 & \rho & 0 \\  \rho & 0 & \rho & 1 & 0 \\  \rho & \rho & 0 & 0 & 1 \\  \end{pmatrix} $$ HOWEVER, this covariance matrix is not positive definite. if I introduce say, $\rho = 0.9$. Hence, if I fit a multivariate normal model on this covariance matrix, I would get errors in programs. Is there a way around it? I also noticed if I increase the marginal variances ($1$ above), then the problem goes away. Is there a reason why? Thanks!",,"['probability', 'statistics', 'graph-theory', 'random-graphs']"
65,Inference about standard deviation of normal sample,Inference about standard deviation of normal sample,,"An experiment was conducted to investigate the filling capability of packing equipment at a winery. 20 bottles were randomly selected, and the fill volume $(in$ $ml)$ was measured. Assume that the fill volume has a normal distribution. The data is shown below: \begin{array}{|c|c|c|c|c|} \hline 753& 751 & 752 & 753 & 753 \\ \hline  753& 752& 753&  754& 754\\ \hline  752&  751& 752&  750& 753\\ \hline  755&  753& 756&  751& 750\\ \hline \end{array} $a)$  Does the data support the claim that the standard deviation of fill volume is less than 1 ml? Use $α=0.05$. $b)$  Find a 95% two-sided confidence interval on the standard deviation of fill volume. $$\\$$ This is what I have so far: $a)$ $H_o:σ = 1$ $H_a:σ < 1$ where: $\bar{X}$ = 752.55  and  $S_x$ = 1.538112309 $$\\$$ $\chi^2 =  \frac{(n-1)s^2}{\sigma^2}$  = $\frac{(19)1.538112309^2}{1^2}$ = 44.950 Is what I did correct? & I don't know how to find the confidence interval on the standard deviation, I would really appreciate your help!","An experiment was conducted to investigate the filling capability of packing equipment at a winery. 20 bottles were randomly selected, and the fill volume $(in$ $ml)$ was measured. Assume that the fill volume has a normal distribution. The data is shown below: \begin{array}{|c|c|c|c|c|} \hline 753& 751 & 752 & 753 & 753 \\ \hline  753& 752& 753&  754& 754\\ \hline  752&  751& 752&  750& 753\\ \hline  755&  753& 756&  751& 750\\ \hline \end{array} $a)$  Does the data support the claim that the standard deviation of fill volume is less than 1 ml? Use $α=0.05$. $b)$  Find a 95% two-sided confidence interval on the standard deviation of fill volume. $$\\$$ This is what I have so far: $a)$ $H_o:σ = 1$ $H_a:σ < 1$ where: $\bar{X}$ = 752.55  and  $S_x$ = 1.538112309 $$\\$$ $\chi^2 =  \frac{(n-1)s^2}{\sigma^2}$  = $\frac{(19)1.538112309^2}{1^2}$ = 44.950 Is what I did correct? & I don't know how to find the confidence interval on the standard deviation, I would really appreciate your help!",,"['statistics', 'hypothesis-testing', 'confidence-interval']"
66,Initial state of a Markov Process,Initial state of a Markov Process,,"Intuitively (without using matrices) , why in a Markov process the probability vector , after a huge number of iterations, is indipendent from the initial probability vector ? (Im considering the particular case in which this happens) Thanks","Intuitively (without using matrices) , why in a Markov process the probability vector , after a huge number of iterations, is indipendent from the initial probability vector ? (Im considering the particular case in which this happens) Thanks",,"['linear-algebra', 'probability', 'abstract-algebra', 'statistics']"
67,Probability of x success with n d10s where a 10 is worth 2 successes,Probability of x success with n d10s where a 10 is worth 2 successes,,"I want to work out the probability of at least x successes on n D10's where 7 an above is a success but 10 counts as two successes. Easy enough without the 10's counting as two successes with binomial distribution. Probability of success is 0.4 per dice and can work out the probability for any value of x and n using an online calculator, excel etc. But with a 10 on a d10 counts as 2 successes instead of 1, I'm lost. How do I work this out mathematically? Edit This is for a game. A friend is making an excel worksheet to work out the probabilities of success based on varying dice pools ( n) and number of successes needed to pass ( x ). But he doesn't math. I kinda do... well once upon a time... I'm not after answers and explanations on how the math behind them work but the names of functions so I can work out how things work with the power of google would be appreciated. And it will all be plugged into excel eventually if possible so answers with that in mind would be appreciated.","I want to work out the probability of at least x successes on n D10's where 7 an above is a success but 10 counts as two successes. Easy enough without the 10's counting as two successes with binomial distribution. Probability of success is 0.4 per dice and can work out the probability for any value of x and n using an online calculator, excel etc. But with a 10 on a d10 counts as 2 successes instead of 1, I'm lost. How do I work this out mathematically? Edit This is for a game. A friend is making an excel worksheet to work out the probabilities of success based on varying dice pools ( n) and number of successes needed to pass ( x ). But he doesn't math. I kinda do... well once upon a time... I'm not after answers and explanations on how the math behind them work but the names of functions so I can work out how things work with the power of google would be appreciated. And it will all be plugged into excel eventually if possible so answers with that in mind would be appreciated.",,"['statistics', 'dice']"
68,How to show that there is a unique unbiased estimator of $p$?,How to show that there is a unique unbiased estimator of ?,p,"The taxis in a town are marked with reference numbers $1,2,...,p$ where $p$ is unknown. I am observing $n \leq p$ of them with pairwise different reference numbers $x_1,x_2,...,x_n$. It is to be assumed that every combination $(x_1,x_2,...,x_3)$ has the same probability to occure. Let $g:=\max x_i$. How can I show that there is a unique function $f(g)$ being an unbiased estimator of $p$? Any hints?","The taxis in a town are marked with reference numbers $1,2,...,p$ where $p$ is unknown. I am observing $n \leq p$ of them with pairwise different reference numbers $x_1,x_2,...,x_n$. It is to be assumed that every combination $(x_1,x_2,...,x_3)$ has the same probability to occure. Let $g:=\max x_i$. How can I show that there is a unique function $f(g)$ being an unbiased estimator of $p$? Any hints?",,"['probability-theory', 'statistics']"
69,Expected value of strictly convex function is strictly convex?,Expected value of strictly convex function is strictly convex?,,"I have one question that I am confused about.  X is random variable with finite mean, $\alpha$ $\in$ (0,1) and $\phi_{1}$,$\phi_{2}$ are strictly convex functions. Then I know that for each t the function $g(t,x) = \alpha \phi_{1}((t-x)_{+}) + (1-\alpha) \phi_{2}((t-x)_{-})$ is strictly convex in x. It should follow that $E[g(X,x)]$ is strictly convex in x. (Reason for this is that it implies that its minimizer is unique) I don't understand why it follows that $E[g(X,x)]$ is strictly convex in x ? Thanks for your time.","I have one question that I am confused about.  X is random variable with finite mean, $\alpha$ $\in$ (0,1) and $\phi_{1}$,$\phi_{2}$ are strictly convex functions. Then I know that for each t the function $g(t,x) = \alpha \phi_{1}((t-x)_{+}) + (1-\alpha) \phi_{2}((t-x)_{-})$ is strictly convex in x. It should follow that $E[g(X,x)]$ is strictly convex in x. (Reason for this is that it implies that its minimizer is unique) I don't understand why it follows that $E[g(X,x)]$ is strictly convex in x ? Thanks for your time.",,"['probability', 'statistics']"
70,Calculate mean value from given data,Calculate mean value from given data,,"This question is about requesting some applicable algorithm rather than mathematical idea and is not part of any homework or study project. The solution may be obvious, but I can't see it by myself. Suppose you have a data of time and some possible events (stock prices, number of trading assets, volume of liquid through a pipe, number of people somehow -- whatever you wish) represented by a table such that this possible events somehow depend on each other. For example, suppose you a purchaser with some limited amount of money and data on a table represents possible value of offers from supplies. Then it is clear then the more you buy in the beginning the less you will be able to buy in a future because of possibility to run out of money. Or suppose you a trader in a stock market, who buy stock if he has no one and sell otherwise -- the situation is the same. My question is how can I algorithmically determine an expected mean value from such kind of data when I have no strategy and going to do things randomly? The problem at my point of view is that even for a $50$ rows table there would be $2^{50} = 1125899906842624$ possibilities of my behavior. On the other hand, I believe that this problem may naturally appear in many areas of applied mathematics in industry, so people somehow already solved it for a much bigger amount of data. I am expecting that anyone will provide and explain for me an algorithm which will avoid considerating $2^n$ possible pathes for $n$ rows table. Any ideas will be very appreciative. UPDATE : Let me clarify my problem in a simple example. Suppose that you have a list of ten elements, for example, let it be $\{14,15,8,6,7,3,19,25,9,15\}$. You pick the first element and then toss a fair coin: tail corresponds to the case when you pick second, and in case of head you do nothing. The expected value after such two steps is $Ex_2=0.5\cdot(14+0)+0.5\cdot(14+15)=\frac{43}{2}$. You repeat such procedure on each step, namely, after third we obtain $$Ex_3=0.25\cdot(14+0+0)+0.25\cdot(14+0+8)+0.25\cdot(14+15+0)+0.25\cdot(14+15+8)=\frac{51}{2},$$ and so on. $Ex_n$ (expected mean value after $n$ steps) is given by a sum of $2^{n-1}$ summands that increases very quickly with increasing $n$. I'm interested is there a simple way to solve that kind of problem for any reasonable $n$ (say $50$ or $100$) without brute force calculation of all possible cases? This is a very simple explanation in one toy problem what I'm seeking for.","This question is about requesting some applicable algorithm rather than mathematical idea and is not part of any homework or study project. The solution may be obvious, but I can't see it by myself. Suppose you have a data of time and some possible events (stock prices, number of trading assets, volume of liquid through a pipe, number of people somehow -- whatever you wish) represented by a table such that this possible events somehow depend on each other. For example, suppose you a purchaser with some limited amount of money and data on a table represents possible value of offers from supplies. Then it is clear then the more you buy in the beginning the less you will be able to buy in a future because of possibility to run out of money. Or suppose you a trader in a stock market, who buy stock if he has no one and sell otherwise -- the situation is the same. My question is how can I algorithmically determine an expected mean value from such kind of data when I have no strategy and going to do things randomly? The problem at my point of view is that even for a $50$ rows table there would be $2^{50} = 1125899906842624$ possibilities of my behavior. On the other hand, I believe that this problem may naturally appear in many areas of applied mathematics in industry, so people somehow already solved it for a much bigger amount of data. I am expecting that anyone will provide and explain for me an algorithm which will avoid considerating $2^n$ possible pathes for $n$ rows table. Any ideas will be very appreciative. UPDATE : Let me clarify my problem in a simple example. Suppose that you have a list of ten elements, for example, let it be $\{14,15,8,6,7,3,19,25,9,15\}$. You pick the first element and then toss a fair coin: tail corresponds to the case when you pick second, and in case of head you do nothing. The expected value after such two steps is $Ex_2=0.5\cdot(14+0)+0.5\cdot(14+15)=\frac{43}{2}$. You repeat such procedure on each step, namely, after third we obtain $$Ex_3=0.25\cdot(14+0+0)+0.25\cdot(14+0+8)+0.25\cdot(14+15+0)+0.25\cdot(14+15+8)=\frac{51}{2},$$ and so on. $Ex_n$ (expected mean value after $n$ steps) is given by a sum of $2^{n-1}$ summands that increases very quickly with increasing $n$. I'm interested is there a simple way to solve that kind of problem for any reasonable $n$ (say $50$ or $100$) without brute force calculation of all possible cases? This is a very simple explanation in one toy problem what I'm seeking for.",,"['probability', 'statistics', 'algorithms', 'means', 'algorithmic-randomness']"
71,sufficient statistic for a strange parameter on normal distribution,sufficient statistic for a strange parameter on normal distribution,,"Let $X_1,\ldots,X_n$  identically indepedent and distributed like   $N(b,1)$ . I'm supposed to find a sufficient statistic for $a=P[X_1<1]$.","Let $X_1,\ldots,X_n$  identically indepedent and distributed like   $N(b,1)$ . I'm supposed to find a sufficient statistic for $a=P[X_1<1]$.",,['statistics']
72,Calculate the Poisson parameter given a probability,Calculate the Poisson parameter given a probability,,"I was looking at this question , but it never received an answer. Is it possible to calculate $\lambda$ (the expected number) from a probability? I know that $\lambda$ is easily calculated if, say, we know that the P(X=0 crashes per month) = .05. Then $\lambda$ is $-\ln(.05)$ crashes per month. But what if we are given that P(X=5 crashes per month)=.05. How would I find the expected number of crashes a month in that case? (if it is even possible)","I was looking at this question , but it never received an answer. Is it possible to calculate $\lambda$ (the expected number) from a probability? I know that $\lambda$ is easily calculated if, say, we know that the P(X=0 crashes per month) = .05. Then $\lambda$ is $-\ln(.05)$ crashes per month. But what if we are given that P(X=5 crashes per month)=.05. How would I find the expected number of crashes a month in that case? (if it is even possible)",,"['probability', 'statistics', 'poisson-distribution']"
73,Can a Binomial Distribution be standardized to a Standard Normal distribution? [closed],Can a Binomial Distribution be standardized to a Standard Normal distribution? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Can a Binomial Distribution be standardized to a Standard Normal distribution? Using Central Limit Theorem..,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Can a Binomial Distribution be standardized to a Standard Normal distribution? Using Central Limit Theorem..,,"['statistics', 'probability-distributions', 'normal-distribution', 'binomial-distribution', 'central-limit-theorem']"
74,Marginal distribution for correlated uniform variables,Marginal distribution for correlated uniform variables,,"Question Let $X$ and $Y$ be two uniformly distributed random variables with bounds $x_\text{low}$, $x_\text{up}$, $y_\text{low}$ and $y_\text{up}.$ $X$ and $Y$ are correlated with a correlation coefficient of $R$. Given an observed outcome $x$ from the variable $X$ and given the correlation coefficient $R$, how can one calculate the probability of a particular outcome $y$ from variable $Y$. In other words, how can one calculate $$P(Y=y \mid X=x, R) = \text{?}$$ Extreme cases The extreme cases are easy. If $R=0$ ($X$ and $Y$ are independent), then $$P(Y=y \mid X=x, R) = \frac 1 {Y_\text{up} - Y_\text{low}}$$ If $R = 1$, then $$P(Y=y \mid X=x, R) = \begin{cases} 1,  & \text{if} \space y = \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}} \\ 0, & \text{if} \space y ≠ \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}} \end{cases}$$ Goal In case it is of interest, my goal when asking this question is to write a short algorithm that sample points from this bivariate uniform distribution with specified correlation coefficient.","Question Let $X$ and $Y$ be two uniformly distributed random variables with bounds $x_\text{low}$, $x_\text{up}$, $y_\text{low}$ and $y_\text{up}.$ $X$ and $Y$ are correlated with a correlation coefficient of $R$. Given an observed outcome $x$ from the variable $X$ and given the correlation coefficient $R$, how can one calculate the probability of a particular outcome $y$ from variable $Y$. In other words, how can one calculate $$P(Y=y \mid X=x, R) = \text{?}$$ Extreme cases The extreme cases are easy. If $R=0$ ($X$ and $Y$ are independent), then $$P(Y=y \mid X=x, R) = \frac 1 {Y_\text{up} - Y_\text{low}}$$ If $R = 1$, then $$P(Y=y \mid X=x, R) = \begin{cases} 1,  & \text{if} \space y = \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}} \\ 0, & \text{if} \space y ≠ \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}} \end{cases}$$ Goal In case it is of interest, my goal when asking this question is to write a short algorithm that sample points from this bivariate uniform distribution with specified correlation coefficient.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'correlation']"
75,Find the asymptotic relative efficiency for two estimators from a Poisson family,Find the asymptotic relative efficiency for two estimators from a Poisson family,,"If two estimators $W_n$ and $V_n$ satisfy $$\sqrt n[W_n - \tau(\theta)] \rightarrow n[0,\sigma^2_W] \\ \sqrt n [V_n - \tau(\theta)] \rightarrow n[0, \sigma^2_V] $$ in distribution then the asymptotic relative efficiency (ARE) of $V_n$ with respect to $W_n$ is $$ ARE(V_n, W_n) = \frac{\sigma^2_W}{\sigma^2_V} .$$ Consider a random sample of size $n$ from a Poisson population with mean $\lambda$. I am asked to find the $ARE(\left(\frac{n-1}{n}\right)^{n \bar x}, e^{-\bar x})$ where $\left(\frac{n-1}{n}\right)^{n \bar x}$ is the uniformly minimum variance unbiased estimator of $e^{-\lambda}$ and $e^{-\bar x}$ is the maximum likelihood estimator of $e^{- \lambda}$ Its easy to see from the delta method that $$ \sqrt n[e^{-\bar x} -e^{- \lambda} ] \rightarrow n[0,\lambda e^{-2 \lambda}] $$ which takes care of finding the asymptotic variance of the MLE, $\sigma^2_{MLE} = \lambda e^{-2\lambda}$.  But I run into a problem when trying to do the same for the UMVUE. First we note that $$ \sqrt n [\bar X - \lambda ] \rightarrow n[0, \lambda] $$ so by the delta method  $$ \sqrt n [g(\bar X) - g(\lambda) ] \rightarrow n[0,(g'(\lambda))^2 \lambda] .$$ If we let $ g(\lambda) = \left(\frac{n-1}{n}\right)^{n\lambda}$ then this implys that $$ \sqrt n \left[\left( \frac{n-1}{n} \right)^{n\bar x} - \left(\frac {n-1}{n} \right)^{n \lambda}\right] \rightarrow n\left[0, \lambda \left( \left(\frac{n-1}{n}\right)^{n \lambda} \ln{\left(\frac{n-1}{n} \right)^n} \right)^2 \right]. $$ I get the right answer is I use $\lambda \left( \left(\frac{n-1}{n}\right)^{n \lambda} \ln{\left(\frac{n-1}{n} \right)^n} \right)^2$ as the $\sigma^2_{UMVUE}$ but the condition was not met for the definiton of the ARE since the $\tau(\theta)$ my problem does not match like it requires in the definition of the ARE. However I noticed that $\left(\frac{n-1}{n} \right)^{n\lambda} \rightarrow e^{- \lambda}$ so I was wondering if that is good enough or is there another step that I need to take.","If two estimators $W_n$ and $V_n$ satisfy $$\sqrt n[W_n - \tau(\theta)] \rightarrow n[0,\sigma^2_W] \\ \sqrt n [V_n - \tau(\theta)] \rightarrow n[0, \sigma^2_V] $$ in distribution then the asymptotic relative efficiency (ARE) of $V_n$ with respect to $W_n$ is $$ ARE(V_n, W_n) = \frac{\sigma^2_W}{\sigma^2_V} .$$ Consider a random sample of size $n$ from a Poisson population with mean $\lambda$. I am asked to find the $ARE(\left(\frac{n-1}{n}\right)^{n \bar x}, e^{-\bar x})$ where $\left(\frac{n-1}{n}\right)^{n \bar x}$ is the uniformly minimum variance unbiased estimator of $e^{-\lambda}$ and $e^{-\bar x}$ is the maximum likelihood estimator of $e^{- \lambda}$ Its easy to see from the delta method that $$ \sqrt n[e^{-\bar x} -e^{- \lambda} ] \rightarrow n[0,\lambda e^{-2 \lambda}] $$ which takes care of finding the asymptotic variance of the MLE, $\sigma^2_{MLE} = \lambda e^{-2\lambda}$.  But I run into a problem when trying to do the same for the UMVUE. First we note that $$ \sqrt n [\bar X - \lambda ] \rightarrow n[0, \lambda] $$ so by the delta method  $$ \sqrt n [g(\bar X) - g(\lambda) ] \rightarrow n[0,(g'(\lambda))^2 \lambda] .$$ If we let $ g(\lambda) = \left(\frac{n-1}{n}\right)^{n\lambda}$ then this implys that $$ \sqrt n \left[\left( \frac{n-1}{n} \right)^{n\bar x} - \left(\frac {n-1}{n} \right)^{n \lambda}\right] \rightarrow n\left[0, \lambda \left( \left(\frac{n-1}{n}\right)^{n \lambda} \ln{\left(\frac{n-1}{n} \right)^n} \right)^2 \right]. $$ I get the right answer is I use $\lambda \left( \left(\frac{n-1}{n}\right)^{n \lambda} \ln{\left(\frac{n-1}{n} \right)^n} \right)^2$ as the $\sigma^2_{UMVUE}$ but the condition was not met for the definiton of the ARE since the $\tau(\theta)$ my problem does not match like it requires in the definition of the ARE. However I noticed that $\left(\frac{n-1}{n} \right)^{n\lambda} \rightarrow e^{- \lambda}$ so I was wondering if that is good enough or is there another step that I need to take.",,"['statistics', 'convergence-divergence']"
76,Why is an M-estimator from statistics not necessarily measurable?,Why is an M-estimator from statistics not necessarily measurable?,,"I read somewhere that an M-estimator , defined as estimators that maximize a criterion of the form: $$ \theta \mapsto M_n(\theta) = \sum_{i=1}^{n}m_\theta(Y_i) $$ for some functions $\{m_\theta: \theta \in \mathcal{H}\}$. $Y_i$ here are random variables. I read a remark in a textbook that an M-estimator is not necessarily measurable, since it is defined as an argmin or an argmax over an uncountable set $\mathcal{H}$. I do not understand what minimizing and maximizing over an uncountable set necessarily implies measurability. Could someone help me here? thanks.","I read somewhere that an M-estimator , defined as estimators that maximize a criterion of the form: $$ \theta \mapsto M_n(\theta) = \sum_{i=1}^{n}m_\theta(Y_i) $$ for some functions $\{m_\theta: \theta \in \mathcal{H}\}$. $Y_i$ here are random variables. I read a remark in a textbook that an M-estimator is not necessarily measurable, since it is defined as an argmin or an argmax over an uncountable set $\mathcal{H}$. I do not understand what minimizing and maximizing over an uncountable set necessarily implies measurability. Could someone help me here? thanks.",,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'robust-statistics']"
77,Finding the probability density from cumulative distribution function,Finding the probability density from cumulative distribution function,,"I have been confused about this question for a while now. If anyone could help, it would be really very helpful. The cumulative distribution function of the random variable $X$ is: $$ F(x)= \begin{cases} 0& \text{for }x<-1,\\   (x+1)/2& \text{for } -1\leq x<1,\\    1& \text{for }x\geq1.  \end{cases}$$ Find the probability density function of $X$.  I am not sure but my guess is the derivative of cumulative distribution function gives you the probability density function???","I have been confused about this question for a while now. If anyone could help, it would be really very helpful. The cumulative distribution function of the random variable $X$ is: $$ F(x)= \begin{cases} 0& \text{for }x<-1,\\   (x+1)/2& \text{for } -1\leq x<1,\\    1& \text{for }x\geq1.  \end{cases}$$ Find the probability density function of $X$.  I am not sure but my guess is the derivative of cumulative distribution function gives you the probability density function???",,"['probability', 'statistics']"
78,Find $k$ such that $f(x) = kx^3 e^{-x/2}$ is a density function - shortcut for integration?,Find  such that  is a density function - shortcut for integration?,k f(x) = kx^3 e^{-x/2},"Given the PDF $$f(x) = kx^3 e^{-x/2},\ \ \forall x > 0$$ And $0$ otherwise, find the value of $k$ that makes $f(x)$ a density function.  I know this is solvable by integration by parts, but I recall there being a shortcut that you can use to skip integration since this is an exponential distribution, does anyone remember it?","Given the PDF $$f(x) = kx^3 e^{-x/2},\ \ \forall x > 0$$ And $0$ otherwise, find the value of $k$ that makes $f(x)$ a density function.  I know this is solvable by integration by parts, but I recall there being a shortcut that you can use to skip integration since this is an exponential distribution, does anyone remember it?",,"['probability', 'statistics', 'probability-distributions', 'density-function']"
79,"$X$ given $\Theta$ is $\operatorname{Poisson}(\Theta)$, $\Theta$ is distributed as $\operatorname{Gamma}(a,b)$. What are the Mean and Variance of $X$?","given  is ,  is distributed as . What are the Mean and Variance of ?","X \Theta \operatorname{Poisson}(\Theta) \Theta \operatorname{Gamma}(a,b) X","This is what I have so far: Joint Distribution (product of pmf of $X$ and pdf of $\Theta$)  $$f(x,\theta)=\frac{\theta ^xe^{-\theta}}{x!}\frac{b^a}{\Gamma (a)}\theta ^{a-1}e^{-b\theta}$$ and with a little algebra we can get $$f(x,\theta)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}\cdot\frac{(b+1)^{x+a}}{\Gamma (x+a)} \theta ^{(x+a)-1}e^{-(b+1)\theta}$$ for easy derivation of the pdf of $X$: $$f(x)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}\int_0^\infty\frac{(b+1)^{x+a}}{\Gamma (x+a)} \theta ^{(x+a)-1}e^{-(b+1)\theta}d\theta$$ leaving us with$$f(x)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}$$ Which looks kind of similar to some kind of beta distribution? What kind of distribution can this be identified as? If it's not a standard distribution, how can I calculate the raw moments? Is integration or summation appropriate (Changing $x!$ to $\Gamma (x+1)$)? Thanks for your help!","This is what I have so far: Joint Distribution (product of pmf of $X$ and pdf of $\Theta$)  $$f(x,\theta)=\frac{\theta ^xe^{-\theta}}{x!}\frac{b^a}{\Gamma (a)}\theta ^{a-1}e^{-b\theta}$$ and with a little algebra we can get $$f(x,\theta)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}\cdot\frac{(b+1)^{x+a}}{\Gamma (x+a)} \theta ^{(x+a)-1}e^{-(b+1)\theta}$$ for easy derivation of the pdf of $X$: $$f(x)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}\int_0^\infty\frac{(b+1)^{x+a}}{\Gamma (x+a)} \theta ^{(x+a)-1}e^{-(b+1)\theta}d\theta$$ leaving us with$$f(x)=\frac{1}{x!}\frac{b^a}{(b+1)^{x+a}}\frac{\Gamma (x+a)}{\Gamma (a)}$$ Which looks kind of similar to some kind of beta distribution? What kind of distribution can this be identified as? If it's not a standard distribution, how can I calculate the raw moments? Is integration or summation appropriate (Changing $x!$ to $\Gamma (x+1)$)? Thanks for your help!",,"['probability', 'statistics', 'probability-distributions', 'expectation', 'poisson-distribution']"
80,Is following function valid to be a copula function? [closed],Is following function valid to be a copula function? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I know $C(u,v)=uv$ is a copula corresponding to the independent case. How about $C(u,v)=u^2v^2$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I know $C(u,v)=uv$ is a copula corresponding to the independent case. How about $C(u,v)=u^2v^2$?",,['statistics']
81,Boxplot Skewness,Boxplot Skewness,,"I do know there are some rules about boxes and whiskers to determine the skewness in a boxplot, but I am confused with some rules in this particular case: Keeping in mind the rules, in this boxplot the median falls to the right of the center of the box, thus its distribution is negatively skewed. But I also can see that the right line is larger than the left line, thus ""according to the rules"" the distribution is positively skewed. How do I know the real skewness. Thanks in advance.","I do know there are some rules about boxes and whiskers to determine the skewness in a boxplot, but I am confused with some rules in this particular case: Keeping in mind the rules, in this boxplot the median falls to the right of the center of the box, thus its distribution is negatively skewed. But I also can see that the right line is larger than the left line, thus ""according to the rules"" the distribution is positively skewed. How do I know the real skewness. Thanks in advance.",,['statistics']
82,Expected value $E(X^2 + Y^2)$,Expected value,E(X^2 + Y^2),"I know that the expected value of a joint distribution is: $$E(XY) = \sum_{all\, x} \sum_{all\, y} xyP(x,y)$$ However, for $E(X^2 + Y^2)$, does the same hold true? ie. $$E(X^2+Y^2) = \sum_x \sum_y (x^2 + y^2)P(x,y)$$ I feel like the P(x,y) should be something else, am I seeing it right?","I know that the expected value of a joint distribution is: $$E(XY) = \sum_{all\, x} \sum_{all\, y} xyP(x,y)$$ However, for $E(X^2 + Y^2)$, does the same hold true? ie. $$E(X^2+Y^2) = \sum_x \sum_y (x^2 + y^2)P(x,y)$$ I feel like the P(x,y) should be something else, am I seeing it right?",,"['statistics', 'expectation']"
83,Find unbiased estimator,Find unbiased estimator,,"Let $X_i$ be observations from $U[0, \theta]$ (continuous uniform distribution). Find unbiased estimator for $\frac{1}{\theta}$ What i did is: let $\theta^* = g(X)$ be an estimator. Than, to be unbiased, the following must hold: $$\int\limits_{0}^{\theta} \theta^*(x) \frac{1}{\theta} dx =\frac{1}{\theta}\int\limits_{0}^{\theta} g(x) dx = \frac{1}{\theta}\big[G(\theta) - G(0) \big]$$ where $G' = g$, so the clue is to find some function $g$ with such properties. This is the part where i need help.","Let $X_i$ be observations from $U[0, \theta]$ (continuous uniform distribution). Find unbiased estimator for $\frac{1}{\theta}$ What i did is: let $\theta^* = g(X)$ be an estimator. Than, to be unbiased, the following must hold: $$\int\limits_{0}^{\theta} \theta^*(x) \frac{1}{\theta} dx =\frac{1}{\theta}\int\limits_{0}^{\theta} g(x) dx = \frac{1}{\theta}\big[G(\theta) - G(0) \big]$$ where $G' = g$, so the clue is to find some function $g$ with such properties. This is the part where i need help.",,"['probability', 'statistics', 'estimation']"
84,Gamma distribution and pdf,Gamma distribution and pdf,,"Let $X \sim \mathsf{Gamma}(2,3)$ and $Y = 2X.$ Find the pdf of $Y$ at $y=13.5.$ Attempt: $f_X(x)= 2*[1/9*\Gamma(2)]*x*e^{-x/3}.$ Do I have to integrate now?","Let $X \sim \mathsf{Gamma}(2,3)$ and $Y = 2X.$ Find the pdf of $Y$ at $y=13.5.$ Attempt: $f_X(x)= 2*[1/9*\Gamma(2)]*x*e^{-x/3}.$ Do I have to integrate now?",,['statistics']
85,Continuous random variable distribution probability,Continuous random variable distribution probability,,"In a final exam, there are 300 students and one-third of them bring a water bottle. Suppose that for those who bring a water bottle, the chance that each of them leaves behind his/her water bottle in the examination room after the exam is 0.06. a) What are the mean and standard deviation of the total number of water bottles left behind after the exam? Give your answers to 2 decimal places. b) Using an appropriate approximation method, find the probability that 5 or less water bottles are left behind. Carry your answers from the previous part to at least 6 decimal places in your calculation here. Give you final answer to 4 decimal places. c) Using an appropriate approximation method, at most how many water bottles will be left behind 84.13% of the time? Give your answer to the nearest integer. So far, I have found the mean. I think it should be 300$\frac{1}{3}$(0.06) = 6. I got the standard deviation as 23.74 by doing $\sqrt{Var(X)}$ where Var(X)= E($X^{2}$) - $[E(X)]^{2}$. I'm pretty lost because I don't know how to determine which distribution to use. Any hints or help is appreciated.","In a final exam, there are 300 students and one-third of them bring a water bottle. Suppose that for those who bring a water bottle, the chance that each of them leaves behind his/her water bottle in the examination room after the exam is 0.06. a) What are the mean and standard deviation of the total number of water bottles left behind after the exam? Give your answers to 2 decimal places. b) Using an appropriate approximation method, find the probability that 5 or less water bottles are left behind. Carry your answers from the previous part to at least 6 decimal places in your calculation here. Give you final answer to 4 decimal places. c) Using an appropriate approximation method, at most how many water bottles will be left behind 84.13% of the time? Give your answer to the nearest integer. So far, I have found the mean. I think it should be 300$\frac{1}{3}$(0.06) = 6. I got the standard deviation as 23.74 by doing $\sqrt{Var(X)}$ where Var(X)= E($X^{2}$) - $[E(X)]^{2}$. I'm pretty lost because I don't know how to determine which distribution to use. Any hints or help is appreciated.",,"['probability', 'statistics', 'random-variables']"
86,How to find the value of c that makes this PDF valid?,How to find the value of c that makes this PDF valid?,,"Trying to figure out this stats problem, but I'm feeling stuck on it: The PDF for a continuous random variable X is the following: $f(x)$ = $\dfrac{c}{x^4}$ if $x>2$ and $0$ otherwise. What is the value of $c$ that makes this PDF valid? It hints that $\lim_{n\to \infty} n^{-a} = 0$ for any constant $a>0$ . I'm not quite sure how to interpret this hint or how to solve the problem. Thanks for the help!","Trying to figure out this stats problem, but I'm feeling stuck on it: The PDF for a continuous random variable X is the following: = if and otherwise. What is the value of that makes this PDF valid? It hints that for any constant . I'm not quite sure how to interpret this hint or how to solve the problem. Thanks for the help!",f(x) \dfrac{c}{x^4} x>2 0 c \lim_{n\to \infty} n^{-a} = 0 a>0,"['statistics', 'probability-distributions']"
87,Maximum likelihood estimate for 1/p in Binomial distribution,Maximum likelihood estimate for 1/p in Binomial distribution,,"Consider a discrete random variable $X$ with the binomial distribution $b(n,p)$ where $n$ is the number of Bernoulli trials and $p\in(0,1)$ is the success probability. The probability mass function for $X$, i.e., the number of successes in $n$ trials, is given by $$ P(X=x)={n \choose x} p^x(1-p)^{n-x},\quad x=0,1,\ldots,n$$ The maximum likelihood estimate (MLE) for $p$ is given by $$\widehat{p} = \frac{x}{n}$$ if one observes the event $X=x$. My questions are the following: Can we compute the MLE for $1/p$ as follows: $$\widehat{\frac{1}{p}} = \frac{n}{x}$$ using the invariance property of the MLE? It seems that the above estimator has infinite mean and variance for any finite $n$ since we have $X=0$ with probability $(1-p)^n$. Does this disturb asymptotic consistency, unbiasedness, and efficiency properties of the MLE?","Consider a discrete random variable $X$ with the binomial distribution $b(n,p)$ where $n$ is the number of Bernoulli trials and $p\in(0,1)$ is the success probability. The probability mass function for $X$, i.e., the number of successes in $n$ trials, is given by $$ P(X=x)={n \choose x} p^x(1-p)^{n-x},\quad x=0,1,\ldots,n$$ The maximum likelihood estimate (MLE) for $p$ is given by $$\widehat{p} = \frac{x}{n}$$ if one observes the event $X=x$. My questions are the following: Can we compute the MLE for $1/p$ as follows: $$\widehat{\frac{1}{p}} = \frac{n}{x}$$ using the invariance property of the MLE? It seems that the above estimator has infinite mean and variance for any finite $n$ since we have $X=0$ with probability $(1-p)^n$. Does this disturb asymptotic consistency, unbiasedness, and efficiency properties of the MLE?",,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'maximum-likelihood']"
88,Statistics probability die question,Statistics probability die question,,"Suppose a die has been loaded so that a six is scored five times more often than any other score, while all the other scores are equally likely. Express your answers to three decimals. I have gotten the following answers. What is the probability of scoring a three? 0.090909091 I have deciphered since it is a 11 sided die so I simply came up with 1/11 since there is only 1/11 chance of getting a 3 What is the probability of scoring a six?  0.454545455 I have reasoned since there are 5 chances in the 11 sided die so I have gotten 5/11. I have gotten both of them wrong. What are the answers?","Suppose a die has been loaded so that a six is scored five times more often than any other score, while all the other scores are equally likely. Express your answers to three decimals. I have gotten the following answers. What is the probability of scoring a three? 0.090909091 I have deciphered since it is a 11 sided die so I simply came up with 1/11 since there is only 1/11 chance of getting a 3 What is the probability of scoring a six?  0.454545455 I have reasoned since there are 5 chances in the 11 sided die so I have gotten 5/11. I have gotten both of them wrong. What are the answers?",,"['probability', 'statistics', 'dice']"
89,What is the name of this 'property' in statistics?,What is the name of this 'property' in statistics?,,"Today we have learned about something which roughly translates to 'Markov-property' at school. I have looked it up and found this page: https://en.wikipedia.org/wiki/Markov_property , however, this is way too complicated for me and I am afraid this is not what I am looking for. We learned that for statistical populations of units  $x_1, x_2, x_3, \dots, x_n$ where all units are positive numbers (I am sorry if I use the terminology wrong, these words are completely new to me and English is not my native language), the following stands: Let $A > \bar{x}$ (Where $\bar{x}$ is the arithmetic average of the population). Then there are $\frac{n\bar{x}}{A}$ numbers which are greater or equal to $A$ (Where $n$ is the number of units in the population). I hope I have been clear enough...  Could anybody tell me what this is and where can I find more information about it?","Today we have learned about something which roughly translates to 'Markov-property' at school. I have looked it up and found this page: https://en.wikipedia.org/wiki/Markov_property , however, this is way too complicated for me and I am afraid this is not what I am looking for. We learned that for statistical populations of units  $x_1, x_2, x_3, \dots, x_n$ where all units are positive numbers (I am sorry if I use the terminology wrong, these words are completely new to me and English is not my native language), the following stands: Let $A > \bar{x}$ (Where $\bar{x}$ is the arithmetic average of the population). Then there are $\frac{n\bar{x}}{A}$ numbers which are greater or equal to $A$ (Where $n$ is the number of units in the population). I hope I have been clear enough...  Could anybody tell me what this is and where can I find more information about it?",,"['statistics', 'terminology']"
90,Determine the distribution of the sum of n independent identically distrubted poisson random variable $X_i$?,Determine the distribution of the sum of n independent identically distrubted poisson random variable ?,X_i,"Determine the distribution of the sum of n independent identically distributed Poisson random variable $X_i$ ~ $\mathsf{Poisson}(\lambda)$, $ i = 1, ...., n$? My approach is that since it is independent identically and the sum is basically  $X_1 + X_2 + ... + X_n$ we can just use moment generate function to find it. So $M_x (t) = E e^{xt} = e^{-\lambda} * e^{e^{\lambda*t} },$ which is just basically that to the $n$ power. Can anyone confirm that I'm on the right path?","Determine the distribution of the sum of n independent identically distributed Poisson random variable $X_i$ ~ $\mathsf{Poisson}(\lambda)$, $ i = 1, ...., n$? My approach is that since it is independent identically and the sum is basically  $X_1 + X_2 + ... + X_n$ we can just use moment generate function to find it. So $M_x (t) = E e^{xt} = e^{-\lambda} * e^{e^{\lambda*t} },$ which is just basically that to the $n$ power. Can anyone confirm that I'm on the right path?",,"['statistics', 'poisson-distribution', 'order-statistics']"
91,Constrained combinations of marbles in jars,Constrained combinations of marbles in jars,,"I'm fairly new to combinations so I apologize upfront that I don't even know where to start with this one. My problem is of the form:  If I have $N$ marbles, and $M$ jars that can only fit a max of $Z$ marbles each, for how many combinations may I distribute all marbles amongst the jars? (Assume $N$ is between $1$ and $M \cdot Z$, such that you are guaranteed to have at least one jar to place a marble.) Can someone please provide some insight, or point me in the right direction?","I'm fairly new to combinations so I apologize upfront that I don't even know where to start with this one. My problem is of the form:  If I have $N$ marbles, and $M$ jars that can only fit a max of $Z$ marbles each, for how many combinations may I distribute all marbles amongst the jars? (Assume $N$ is between $1$ and $M \cdot Z$, such that you are guaranteed to have at least one jar to place a marble.) Can someone please provide some insight, or point me in the right direction?",,"['probability', 'combinatorics', 'statistics', 'combinations']"
92,Differences between two formulations of the Lehmann-Scheffe Theorem,Differences between two formulations of the Lehmann-Scheffe Theorem,,"I have seen different formulations of the Lehmann Scheffe Theorem but am not sure if they are the same. For example, on wikipedia it states: If $T$ is a complete sufficient statistic for $\theta$ and $E(g(T)) = \tau(\theta)$, then $g(T)$ is the uniformly minimum-variance unbiased estimator (UMVUE) of $\tau(\theta)$. But here it states that: If $T(X)$ is a complete sufficient statistic and $W(X)$ is an unbiased estimator of $τ(θ)$, then $φ(T) = E(W|T)$ is an UMVUE of $τ(θ)$. Furthermore, $φ(T)$ is the unique UMVUE in the sense that if $T^*$ is any other UMVUE, then $P\theta(φ(T) = T^*) = 1$ for all $θ$. At a glance, it seems that the first wikipedia statement is needed for the second, and that the second uses the Rao-Blackwell Theorem. Are these two statements one and the same?","I have seen different formulations of the Lehmann Scheffe Theorem but am not sure if they are the same. For example, on wikipedia it states: If $T$ is a complete sufficient statistic for $\theta$ and $E(g(T)) = \tau(\theta)$, then $g(T)$ is the uniformly minimum-variance unbiased estimator (UMVUE) of $\tau(\theta)$. But here it states that: If $T(X)$ is a complete sufficient statistic and $W(X)$ is an unbiased estimator of $τ(θ)$, then $φ(T) = E(W|T)$ is an UMVUE of $τ(θ)$. Furthermore, $φ(T)$ is the unique UMVUE in the sense that if $T^*$ is any other UMVUE, then $P\theta(φ(T) = T^*) = 1$ for all $θ$. At a glance, it seems that the first wikipedia statement is needed for the second, and that the second uses the Rao-Blackwell Theorem. Are these two statements one and the same?",,['statistics']
93,Derivative of intertwined matrix expression,Derivative of intertwined matrix expression,,"What are the derivatives of $\mathbf{\hat{y}}(\mathbf{w})$ with respect to the elements $w_{i}$ of the vector $\mathbf{w}$, where \begin{equation}  \mathbf{\hat{y}}(\mathbf{w}) =  \mathbf{\Omega}^{T}\mathbf{a} + \mathbf{1}b \end{equation} and $\mathbf{a}$ and $b$ can be found as follows \begin{equation} \begin{bmatrix}  b \\  \mathbf{a}  \end{bmatrix} = \begin{bmatrix}  0 & \mathbf{1}^T \\  \mathbf{1} & \mathbf{\Omega}+diag(\frac{1}{\gamma\mathbf{w}}) \end{bmatrix}^{-1} \begin{bmatrix}  0 \\  \mathbf{y}  \end{bmatrix} \end{equation} Bold letters indicate vectors, $\mathbf{1}$ is a vector of ones, $\Omega$ a square matrix and $diag(\cdot)$ is a diagonal matrix with entries $(\gamma w_i)^{-1}$. I am a bit at loss due to the intertwined structure. Is this even possible? Both equations are related to the LSSVM classifier as described in ftp://ftp.esat.kuleuven.be/sista/ida/reports./98-72.pdf","What are the derivatives of $\mathbf{\hat{y}}(\mathbf{w})$ with respect to the elements $w_{i}$ of the vector $\mathbf{w}$, where \begin{equation}  \mathbf{\hat{y}}(\mathbf{w}) =  \mathbf{\Omega}^{T}\mathbf{a} + \mathbf{1}b \end{equation} and $\mathbf{a}$ and $b$ can be found as follows \begin{equation} \begin{bmatrix}  b \\  \mathbf{a}  \end{bmatrix} = \begin{bmatrix}  0 & \mathbf{1}^T \\  \mathbf{1} & \mathbf{\Omega}+diag(\frac{1}{\gamma\mathbf{w}}) \end{bmatrix}^{-1} \begin{bmatrix}  0 \\  \mathbf{y}  \end{bmatrix} \end{equation} Bold letters indicate vectors, $\mathbf{1}$ is a vector of ones, $\Omega$ a square matrix and $diag(\cdot)$ is a diagonal matrix with entries $(\gamma w_i)^{-1}$. I am a bit at loss due to the intertwined structure. Is this even possible? Both equations are related to the LSSVM classifier as described in ftp://ftp.esat.kuleuven.be/sista/ida/reports./98-72.pdf",,"['statistics', 'derivatives', 'matrix-calculus']"
94,Statistic is not complete: uniform distribution,Statistic is not complete: uniform distribution,,"How can one proove, that $X$, which is uniformly distributed on the interval $(\theta, \theta+1), \theta \in \mathbb{R}$ is not complete for $\theta$? I have to find a function, such that $E_{\theta}[h(T)] = 0$ where $h(t) \neq 0$ right? Does someone have an idea?","How can one proove, that $X$, which is uniformly distributed on the interval $(\theta, \theta+1), \theta \in \mathbb{R}$ is not complete for $\theta$? I have to find a function, such that $E_{\theta}[h(T)] = 0$ where $h(t) \neq 0$ right? Does someone have an idea?",,"['probability', 'probability-theory', 'statistics', 'uniform-distribution']"
95,Convergence Almost Surely Exercise,Convergence Almost Surely Exercise,,"We have to check convergence almost surely of $X_n$ to $0$ $$P(X_n=e^n)=\frac{1}{n+1}, \quad P(X_n=0)=1-\frac{1}{n+1}$$ I'm not sure I understand in practice the convergence Almost Surely and its difference from convergence in Probability.","We have to check convergence almost surely of $X_n$ to $0$ $$P(X_n=e^n)=\frac{1}{n+1}, \quad P(X_n=0)=1-\frac{1}{n+1}$$ I'm not sure I understand in practice the convergence Almost Surely and its difference from convergence in Probability.",,"['probability-theory', 'statistics', 'convergence-divergence']"
96,How to find the mode and median of a Gamma distribution? [closed],How to find the mode and median of a Gamma distribution? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question A random variable has Gamma distribution with mean of $10$ and standard deviation of $5$. The mode and median are to be found. I realize that this means that $\alpha$ and $\beta$ are both $\sqrt{5}$. My professor told me that R is needed for one of them, and the exact answer can be found another way. Please help. I am confused at what to do.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question A random variable has Gamma distribution with mean of $10$ and standard deviation of $5$. The mode and median are to be found. I realize that this means that $\alpha$ and $\beta$ are both $\sqrt{5}$. My professor told me that R is needed for one of them, and the exact answer can be found another way. Please help. I am confused at what to do.",,"['statistics', 'gamma-distribution']"
97,MLE of $\delta$ for the distribution $f(x)=e^{\delta-x}$ for $x\geq\delta$.,MLE of  for the distribution  for .,\delta f(x)=e^{\delta-x} x\geq\delta,"Let $X_1,X_2,\dots, X_n$ be a random sample form a distribution $f(x)=e^{\delta-x}$ for $x\geq\delta$. Find MLE of $\delta$. My solution : $$L(\hat\gamma)=\prod_{i=1}^nf(x_i\mid \delta)=\prod_{i=1}^ne^{\delta-x} =e^{n\delta}e^{\sum_{i=1}^nx_i}$$ The log likelihood is $l(\hat\delta)=n\delta-\sum_{i=1}^n x_i$. To find the $\max$ we take the 1st derivative with respect to $\delta$: $$l'(\hat\delta)=n$$ the first derivative cannot be zero. I am not sure how to proceed.","Let $X_1,X_2,\dots, X_n$ be a random sample form a distribution $f(x)=e^{\delta-x}$ for $x\geq\delta$. Find MLE of $\delta$. My solution : $$L(\hat\gamma)=\prod_{i=1}^nf(x_i\mid \delta)=\prod_{i=1}^ne^{\delta-x} =e^{n\delta}e^{\sum_{i=1}^nx_i}$$ The log likelihood is $l(\hat\delta)=n\delta-\sum_{i=1}^n x_i$. To find the $\max$ we take the 1st derivative with respect to $\delta$: $$l'(\hat\delta)=n$$ the first derivative cannot be zero. I am not sure how to proceed.",,"['statistics', 'statistical-inference', 'exponential-distribution', 'maximum-likelihood']"
98,Number of ways to deal $5$ cards to $5$ players from a $52$ card deck.,Number of ways to deal  cards to  players from a  card deck.,5 5 52,"The question is: In a game of poker, five players are each dealt $5$ cards from a $52$-card deck. How many ways are there to deal the cards? The answer is $\frac{52P5}{(5!)^5}$ or $\frac{52!}{47!5!}$ I understand the reason after seeing the answer: $25$ ordered cards are picked from the deck of $52$, then divided by the number of ways each hand can be ordered, because the order of a hand does not matter. However, before seeing the answer I came up with this attempt: You choose $5$ cards from $52$, the order doesn't matter so we have $52\choose{5}$. Then you're left with $47$ cards and for the next hand we have $47\choose{5}$. The final sum is ${{52}\choose{5}} + {{47}\choose{5}} + {{42}\choose{5}} + {{37}\choose{5}} + {{32}\choose{5}}$. This sum is considerably larger, but while I understand the reason $\frac{52!}{47!5!}$ is the correct, I can't find the mistake in my first attempt.","The question is: In a game of poker, five players are each dealt $5$ cards from a $52$-card deck. How many ways are there to deal the cards? The answer is $\frac{52P5}{(5!)^5}$ or $\frac{52!}{47!5!}$ I understand the reason after seeing the answer: $25$ ordered cards are picked from the deck of $52$, then divided by the number of ways each hand can be ordered, because the order of a hand does not matter. However, before seeing the answer I came up with this attempt: You choose $5$ cards from $52$, the order doesn't matter so we have $52\choose{5}$. Then you're left with $47$ cards and for the next hand we have $47\choose{5}$. The final sum is ${{52}\choose{5}} + {{47}\choose{5}} + {{42}\choose{5}} + {{37}\choose{5}} + {{32}\choose{5}}$. This sum is considerably larger, but while I understand the reason $\frac{52!}{47!5!}$ is the correct, I can't find the mistake in my first attempt.",,"['combinatorics', 'statistics']"
99,Reason why $P(X = k) = 0$ but $P( k_1 \leq X \leq k_2) \neq 0$ for continous random variable $X$,Reason why  but  for continous random variable,P(X = k) = 0 P( k_1 \leq X \leq k_2) \neq 0 X,"This might be too easy (or almost trivial) for many out here, but i am having troubles with this.  I know for any continous random variable $X$, $P(X = k) = 0$, however the probability that $X$ falls in $[k_1,k_2]$ is not necessarily zero, where $k_1 < k_2$. I've been using this notion since high school and collage, but I never seem to grasp the idea. Am i being too superficial or is it okay to pause and wonder a bit here? Thanks for your time.","This might be too easy (or almost trivial) for many out here, but i am having troubles with this.  I know for any continous random variable $X$, $P(X = k) = 0$, however the probability that $X$ falls in $[k_1,k_2]$ is not necessarily zero, where $k_1 < k_2$. I've been using this notion since high school and collage, but I never seem to grasp the idea. Am i being too superficial or is it okay to pause and wonder a bit here? Thanks for your time.",,"['probability', 'statistics']"
