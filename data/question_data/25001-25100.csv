,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Hessian on linear least squares problem,Hessian on linear least squares problem,,"I tried to calculate the Hessian matrix of linear least squares problem (L-2 norm), in particular: $$f(x) = \|AX - B \|_2$$ where $f:{\rm I\!R}^{11\times 2}\rightarrow {\rm I\!R}$ Can someone help me? Thanks a lot.","I tried to calculate the Hessian matrix of linear least squares problem (L-2 norm), in particular: $$f(x) = \|AX - B \|_2$$ where $f:{\rm I\!R}^{11\times 2}\rightarrow {\rm I\!R}$ Can someone help me? Thanks a lot.",,"['linear-algebra', 'least-squares', 'hessian-matrix']"
1,A rational matrix such that $A^{2017}=I_n$,A rational matrix such that,A^{2017}=I_n,"Let $A \in \mathcal{M}_{n}(\mathbb{Q})$ such that $A^{2017}=I_n$. Is it always true that $$A=I_n?$$ When $n \leq 2015$, I proved that this result must hold using the minimal polynomial and the fact that $2017$ is prime. I suspect that when $n \geq 2016$, this equality is not always true, but I can't find any matrix other than $I_n$ with that property, which also has only rational entries. I also thought of an approach with eigenvalues, but since they are all complex roots of $2017$, I am not sure how to make use of them here.","Let $A \in \mathcal{M}_{n}(\mathbb{Q})$ such that $A^{2017}=I_n$. Is it always true that $$A=I_n?$$ When $n \leq 2015$, I proved that this result must hold using the minimal polynomial and the fact that $2017$ is prime. I suspect that when $n \geq 2016$, this equality is not always true, but I can't find any matrix other than $I_n$ with that property, which also has only rational entries. I also thought of an approach with eigenvalues, but since they are all complex roots of $2017$, I am not sure how to make use of them here.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
2,Null Space and Orthogonal Complement,Null Space and Orthogonal Complement,,"I'm having trouble understanding in a mathematical sense the reason why: (1) $$Null(A) = [R(A^{T})]^\perp $$ (2) $$Null(A^T) = [R(A)]^\perp$$ What I've tried so far is picking some arbitrary vector $\vec{v}$ in $[R(A^{T})]^\perp$ . Picking some arbitrary vector $\vec{y}$ in $R(A^T)$, we then have the relation that $<\vec{v}, \vec{y}> = 0$ if (1) is true. We can rewrite $\vec{y}$ as $A\vec{x}$ and subsitute that into the equation. This gives us: $$<\vec{v}, A\vec{x}> = 0$$ Which can be rewritten as: $$\vec{v}^T A\vec{x} = 0$$ $$[\vec{v}^T (A\vec{x})]^T = 0$$ since the LHS of the equation is just a 1x1 matrix. $$ (A\vec{x})^T \vec{v} = 0$$ $$ \vec{x}^T A^T \vec{v} = 0$$ Finally, we simplify to: $$<\vec{x}, A^T \vec{v}> = 0$$ which doesn't seem to help. How do I show that (1) and (2) are true?","I'm having trouble understanding in a mathematical sense the reason why: (1) $$Null(A) = [R(A^{T})]^\perp $$ (2) $$Null(A^T) = [R(A)]^\perp$$ What I've tried so far is picking some arbitrary vector $\vec{v}$ in $[R(A^{T})]^\perp$ . Picking some arbitrary vector $\vec{y}$ in $R(A^T)$, we then have the relation that $<\vec{v}, \vec{y}> = 0$ if (1) is true. We can rewrite $\vec{y}$ as $A\vec{x}$ and subsitute that into the equation. This gives us: $$<\vec{v}, A\vec{x}> = 0$$ Which can be rewritten as: $$\vec{v}^T A\vec{x} = 0$$ $$[\vec{v}^T (A\vec{x})]^T = 0$$ since the LHS of the equation is just a 1x1 matrix. $$ (A\vec{x})^T \vec{v} = 0$$ $$ \vec{x}^T A^T \vec{v} = 0$$ Finally, we simplify to: $$<\vec{x}, A^T \vec{v}> = 0$$ which doesn't seem to help. How do I show that (1) and (2) are true?",,"['linear-algebra', 'linear-transformations', 'orthogonality']"
3,Implicit equation of a line in 3 Dimensions (disambiguation)?,Implicit equation of a line in 3 Dimensions (disambiguation)?,,"The implicit equation of a line in 2D is: $Ax + By +C =0$ It's analogous in 3D is an implicit plane, described by: $Ax+By+Cz+D=0$ What is then the implicit equation of a line described parametrically by: $P=P_0+t\vec v$ In 3 dimensions?","The implicit equation of a line in 2D is: $Ax + By +C =0$ It's analogous in 3D is an implicit plane, described by: $Ax+By+Cz+D=0$ What is then the implicit equation of a line described parametrically by: $P=P_0+t\vec v$ In 3 dimensions?",,"['linear-algebra', 'geometry', 'implicit-function']"
4,Book to learn Advanced Linear Algebra and Matrix Theory,Book to learn Advanced Linear Algebra and Matrix Theory,,"I am looking for a book to learn Advanced Linear Algebra and Matrix Theory in detail. Sheldon Axler :Doesn't cover matrix theory,Hoffman,Kunze:Doesn't have many exercises and examples on each of the topics Please suggest some alternatives Requisites: Theorems with proofs,easy ones left to reader,Enough examples,Good Exercises(with Hints if possible) Topics to cover : Systems of Linear equations Diagonalization of a square matrix Vector Spaces Solutions of Linear Systems: Gaussian elimination   , Null Space and Range   , Rank and nullity, Consistency conditions in terms of rank   , General Solution of a linear system   , Elementary Row and Column operations   , Row  Reduced Form   ,Triangular Matrix Factorization 5.Important Subspaces associsted with a matrix: Range and Null space, Rank and Nullity,Rank Nullity theorem . 6.Orthogonality: Inner product, Inner product Spaces   , Cauchy – Schwarz inequality   , Norm   , Orthogonality   , Gram – Schmidt orthonormalization   , Orthonormal basis   , Expansion in terms of orthonormal basis – Fourier   series   , Orthogonal complement. 7.Eigenvalues and Eigenvectors Hermitian Matrices:Real symmetric and Hermitian Matrices   Properties of eigenvalues and eigenvectors. 9.General Matrices: The matrices $AA^T,A^TA$    Rank, Nullity, Range and Null Space of $AA^T,A^TA$   ,Singular Value Decomposition. 10.Jordan Cnonical form:    Primary Decomposition Theorem    Nilpotent matrices    Canonical form for a nilpotent matrix Mostly results on MSE said to follow Matrix Analysis-Horn,Johnson but the book does not cover all the topics in great detail.It focuses on more advanced topics. Please suggest  a book accordingly as I need to prepare for my exam.","I am looking for a book to learn Advanced Linear Algebra and Matrix Theory in detail. Sheldon Axler :Doesn't cover matrix theory,Hoffman,Kunze:Doesn't have many exercises and examples on each of the topics Please suggest some alternatives Requisites: Theorems with proofs,easy ones left to reader,Enough examples,Good Exercises(with Hints if possible) Topics to cover : Systems of Linear equations Diagonalization of a square matrix Vector Spaces Solutions of Linear Systems: Gaussian elimination   , Null Space and Range   , Rank and nullity, Consistency conditions in terms of rank   , General Solution of a linear system   , Elementary Row and Column operations   , Row  Reduced Form   ,Triangular Matrix Factorization 5.Important Subspaces associsted with a matrix: Range and Null space, Rank and Nullity,Rank Nullity theorem . 6.Orthogonality: Inner product, Inner product Spaces   , Cauchy – Schwarz inequality   , Norm   , Orthogonality   , Gram – Schmidt orthonormalization   , Orthonormal basis   , Expansion in terms of orthonormal basis – Fourier   series   , Orthogonal complement. 7.Eigenvalues and Eigenvectors Hermitian Matrices:Real symmetric and Hermitian Matrices   Properties of eigenvalues and eigenvectors. 9.General Matrices: The matrices $AA^T,A^TA$    Rank, Nullity, Range and Null Space of $AA^T,A^TA$   ,Singular Value Decomposition. 10.Jordan Cnonical form:    Primary Decomposition Theorem    Nilpotent matrices    Canonical form for a nilpotent matrix Mostly results on MSE said to follow Matrix Analysis-Horn,Johnson but the book does not cover all the topics in great detail.It focuses on more advanced topics. Please suggest  a book accordingly as I need to prepare for my exam.",,"['linear-algebra', 'matrices', 'reference-request', 'soft-question', 'book-recommendation']"
5,How to compute symmetrical determinant,How to compute symmetrical determinant,,I'm learning of determinants and am trying to find a trick to compute this one \begin{pmatrix}  2 & 1 & 1  & 1 & 1\\   1 & 3 & 1 & 1 & 1\\   1 &  1 &  4 &  1 & 1\\   1 &  1 &  1 &  5 & 1\\   1 &  1 &  1 &  1 & 6 \end{pmatrix} I expanded it out and got $349$ but I feel there must be some trick to easily compute it.,I'm learning of determinants and am trying to find a trick to compute this one \begin{pmatrix}  2 & 1 & 1  & 1 & 1\\   1 & 3 & 1 & 1 & 1\\   1 &  1 &  4 &  1 & 1\\   1 &  1 &  1 &  5 & 1\\   1 &  1 &  1 &  1 & 6 \end{pmatrix} I expanded it out and got $349$ but I feel there must be some trick to easily compute it.,,"['linear-algebra', 'determinant']"
6,"If $\lVert x\rVert=\lVert y \rVert$, prove that exists a unitary transformation such as $Tx=y$","If , prove that exists a unitary transformation such as",\lVert x\rVert=\lVert y \rVert Tx=y,"Given $V$ a unitary vector space with a finite dimension, and let $x, y$ vectors in $V$ such as their norm is the same $\left(\lVert x\rVert=\lVert y \rVert\right)$ . Prove that exists a unitary transformation $T:V\to V$ that assigns $x$ to $y$ $(T(x)=y)$ . What I've been trying to do is to complete $x$ and $y$ to a basis of $V$ , $B=\{x,y,v_3, v_4, ..., v_n\}$ , and then define $T$ on B's vectors, like so- $\ \ T(x)=y,\ T(y)=x,\ T(v_i)=v_i \;\forall i\in\{3,\ldots,n\}$ . However I cannot seem to be able to prove that this transformation is indeed unitary, and at this point I'm not even sure it is. Would love a hint, and thanks in advance.","Given a unitary vector space with a finite dimension, and let vectors in such as their norm is the same . Prove that exists a unitary transformation that assigns to . What I've been trying to do is to complete and to a basis of , , and then define on B's vectors, like so- . However I cannot seem to be able to prove that this transformation is indeed unitary, and at this point I'm not even sure it is. Would love a hint, and thanks in advance.","V x, y V \left(\lVert x\rVert=\lVert y \rVert\right) T:V\to V x y (T(x)=y) x y V B=\{x,y,v_3, v_4, ..., v_n\} T \ \ T(x)=y,\ T(y)=x,\ T(v_i)=v_i \;\forall i\in\{3,\ldots,n\}","['linear-algebra', 'linear-transformations']"
7,If $B$ is invertible then there exists a scalar $c$ such that $A+cB$ is not invertible,If  is invertible then there exists a scalar  such that  is not invertible,B c A+cB,"Let $A,B$ be $n\times n$ complex matrices. If $B$ is invertible then there exists a scalar $c \in \mathbb C$ such that $A+cB$ is not invertible. Since $\det(A+cB)$ is a polynomial in $\mathbb C$, it must have a root in $\mathbb C$, i.e. there must exist a $c$ such that $\det(A+cB)=0$. Then why is the condition ""$B$ is invertible"" necessary?","Let $A,B$ be $n\times n$ complex matrices. If $B$ is invertible then there exists a scalar $c \in \mathbb C$ such that $A+cB$ is not invertible. Since $\det(A+cB)$ is a polynomial in $\mathbb C$, it must have a root in $\mathbb C$, i.e. there must exist a $c$ such that $\det(A+cB)=0$. Then why is the condition ""$B$ is invertible"" necessary?",,"['linear-algebra', 'matrices']"
8,Diagonalization of triangular matrices,Diagonalization of triangular matrices,,"Show that if $A$ is a strictly upper triangular nonzero matrix, then $A$ cannot be diagonalizable. I have only shown that $A$ is nilpotent, but I don't know if that implies that $A$ isn't diagonalizable.","Show that if is a strictly upper triangular nonzero matrix, then cannot be diagonalizable. I have only shown that is nilpotent, but I don't know if that implies that isn't diagonalizable.",A A A A,"['linear-algebra', 'matrices', 'diagonalization']"
9,Uncountable basis of vector space of infinite sequences in R [duplicate],Uncountable basis of vector space of infinite sequences in R [duplicate],,"This question already has answers here : Vector space of infinite sequences in $\Bbb R$ [closed] (2 answers) Closed 7 years ago . How does one go about showing that the dimension of a vector space of infinite sequences is uncountable? My methods was to try and show the existence of an uncountable, linearly independent of sequences which implies that the basis must be uncountable. I have tried the set of following sequences i.e. 1) An = n^r where r is any real number 2) set of convergent sequences. However, I cant seem to show that they are uncountably infinite dimensional. Any tips?  P.S Other solutions are welcomed but i have read other similar posts and do not understand the solutions","This question already has answers here : Vector space of infinite sequences in $\Bbb R$ [closed] (2 answers) Closed 7 years ago . How does one go about showing that the dimension of a vector space of infinite sequences is uncountable? My methods was to try and show the existence of an uncountable, linearly independent of sequences which implies that the basis must be uncountable. I have tried the set of following sequences i.e. 1) An = n^r where r is any real number 2) set of convergent sequences. However, I cant seem to show that they are uncountably infinite dimensional. Any tips?  P.S Other solutions are welcomed but i have read other similar posts and do not understand the solutions",,"['linear-algebra', 'elementary-set-theory']"
10,$(XY)^*=X^* Y^*$?,?,(XY)^*=X^* Y^*,"Is it true for complex matrices $X,Y$ that $$ (XY)^*=X^* Y^*? $$ where $^*$ refers to complex conjugation.  How can we prove this if so?  Thanks! Note: I am referring to complex conjugation, not the hermitan transpose. The answer below refers to hermitan tranpose.","Is it true for complex matrices $X,Y$ that $$ (XY)^*=X^* Y^*? $$ where $^*$ refers to complex conjugation.  How can we prove this if so?  Thanks! Note: I am referring to complex conjugation, not the hermitan transpose. The answer below refers to hermitan tranpose.",,"['linear-algebra', 'matrices', 'complex-numbers']"
11,How can I prove $AX=BX$ for every $n\times1$ column matrix $X \implies A=B$,How can I prove  for every  column matrix,AX=BX n\times1 X \implies A=B,Let $A$ and $B$ be matrices $n\times n$. Suppose $AX=BX$ for every $n\times 1$ column matrix $X$. How can I prove this implies $A=B$?,Let $A$ and $B$ be matrices $n\times n$. Suppose $AX=BX$ for every $n\times 1$ column matrix $X$. How can I prove this implies $A=B$?,,"['linear-algebra', 'matrices', 'systems-of-equations']"
12,Properties of symmetric projection matrices [closed],Properties of symmetric projection matrices [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question A square matrix $P$ is called a symmetric projection matrix if $P = P^T$ and $P ^2 = P$ . Show that a symmetric projection matrix $P$ satisfies the following properties. $\|x\|^2=\|Px\|^2+\|(1-P)x\|^2$ for all $x$ $P$ is positive semidefinite","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question A square matrix is called a symmetric projection matrix if and . Show that a symmetric projection matrix satisfies the following properties. for all is positive semidefinite",P P = P^T P ^2 = P P \|x\|^2=\|Px\|^2+\|(1-P)x\|^2 x P,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite', 'projection-matrices']"
13,Sum of elements of the inverse matrix (without deriving the inverse matrix) using elementary methods.,Sum of elements of the inverse matrix (without deriving the inverse matrix) using elementary methods.,,"I have the matrix    $$\begin{pmatrix} 3&2&2&\\ 2&3&2\\ 2&2&3 \end{pmatrix}.$$ Find the sum of elements of the inverse matrix without computing the inverse . I have seen this post , but I need much more elementary method. I have checked that inverse exists, and $\Delta=7$. The answer is $\frac37$. What I did, is by ( not so )simple computation, I worked out that every matrix of the form  $\begin{pmatrix} a&b&b&\\ b&a&b\\ b&b&a \end{pmatrix}$ always achieves an inverse of the form  $$\frac{a-b}{a+2b} \begin{pmatrix} a+b&-b&-b&\\ -b&a+b&-b\\ -b&-b&a+b \end{pmatrix}.$$ And by this technique, the expected result follows for my case $a=3, b=2$. But I don't think this is an elegant method. Because I need to prove this lemma, then have to claim the result. And also, I was told in question not to compute inverse, where I, here, am computing inverse of a general case.","I have the matrix    $$\begin{pmatrix} 3&2&2&\\ 2&3&2\\ 2&2&3 \end{pmatrix}.$$ Find the sum of elements of the inverse matrix without computing the inverse . I have seen this post , but I need much more elementary method. I have checked that inverse exists, and $\Delta=7$. The answer is $\frac37$. What I did, is by ( not so )simple computation, I worked out that every matrix of the form  $\begin{pmatrix} a&b&b&\\ b&a&b\\ b&b&a \end{pmatrix}$ always achieves an inverse of the form  $$\frac{a-b}{a+2b} \begin{pmatrix} a+b&-b&-b&\\ -b&a+b&-b\\ -b&-b&a+b \end{pmatrix}.$$ And by this technique, the expected result follows for my case $a=3, b=2$. But I don't think this is an elegant method. Because I need to prove this lemma, then have to claim the result. And also, I was told in question not to compute inverse, where I, here, am computing inverse of a general case.",,['linear-algebra']
14,"If $\lambda$ is the eigen-value of a $n\times n$ non-singular orthogonal matrix $A$, then prove that $\frac{1}{\lambda}$ is also an eigen-value.","If  is the eigen-value of a  non-singular orthogonal matrix , then prove that  is also an eigen-value.",\lambda n\times n A \frac{1}{\lambda},"QUESTION : If $\lambda$ is the eigen-value of a $n\times n$ non-singular matrix $A$ and $A$ is a real   orthogonal matrix, then prove that $\frac{1}{\lambda}$ is an   eigen-value of the matrix $A$. MY ATTEMPT: Since $\lambda$ is the eigen-value of a $n\times n$ matrix $A$, we have  $$|A-\lambda I_n|=0$$ Also since $A$ is a real orthogonal matrix,we have  $$AA^T=A^TA=I_n$$ So we can conclude that $$|A-\lambda( AA^T)|=0$$ Or, $$|\lambda A\left(\frac{1}{\lambda}I_n- A^T\right)|=0$$ Or, $$\left|\lambda A\right|\cdot \left|\frac{1}{\lambda}I_n- A^T\right|=0$$ Or, since $A$ is non-singular, $$\left|A^T-\frac{1}{\lambda} I_n\right|=0$$ So, we can conclude that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A^T$. But how do I prove that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A$ ? Is my working faulty? Or is there a mistake in the question? Please help.","QUESTION : If $\lambda$ is the eigen-value of a $n\times n$ non-singular matrix $A$ and $A$ is a real   orthogonal matrix, then prove that $\frac{1}{\lambda}$ is an   eigen-value of the matrix $A$. MY ATTEMPT: Since $\lambda$ is the eigen-value of a $n\times n$ matrix $A$, we have  $$|A-\lambda I_n|=0$$ Also since $A$ is a real orthogonal matrix,we have  $$AA^T=A^TA=I_n$$ So we can conclude that $$|A-\lambda( AA^T)|=0$$ Or, $$|\lambda A\left(\frac{1}{\lambda}I_n- A^T\right)|=0$$ Or, $$\left|\lambda A\right|\cdot \left|\frac{1}{\lambda}I_n- A^T\right|=0$$ Or, since $A$ is non-singular, $$\left|A^T-\frac{1}{\lambda} I_n\right|=0$$ So, we can conclude that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A^T$. But how do I prove that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A$ ? Is my working faulty? Or is there a mistake in the question? Please help.",,"['linear-algebra', 'matrices', 'algebra-precalculus', 'eigenvalues-eigenvectors']"
15,Why is $S$ specified to be nonempty in Axler’s definition of function spaces $\mathbf F^S$?,Why is  specified to be nonempty in Axler’s definition of function spaces ?,S \mathbf F^S,"In Example 1.24 of S. Axler’s Linear Algebra Done Right , the following statement is made: If $S$ is a nonempty set, then $\mathbf F^S$ […] is a vector space over $\mathbf F$. (Here, $\mathbf F^S$ is the set of functions from $S$ to $\mathbf F$, and $\mathbf F$ is either $\mathbb R$ or $\mathbb C$.) However, this statement is true even if $S=\varnothing$. (Then there is only one function $S \to \mathbf F$, and $\mathbf F^S$ is trivial.) Why would Axler have written nonempty ? Is he simply distracting from “weird” corner cases for didactic purposes, or is there some deeper motivation?","In Example 1.24 of S. Axler’s Linear Algebra Done Right , the following statement is made: If $S$ is a nonempty set, then $\mathbf F^S$ […] is a vector space over $\mathbf F$. (Here, $\mathbf F^S$ is the set of functions from $S$ to $\mathbf F$, and $\mathbf F$ is either $\mathbb R$ or $\mathbb C$.) However, this statement is true even if $S=\varnothing$. (Then there is only one function $S \to \mathbf F$, and $\mathbf F^S$ is trivial.) Why would Axler have written nonempty ? Is he simply distracting from “weird” corner cases for didactic purposes, or is there some deeper motivation?",,"['linear-algebra', 'vector-spaces', 'definition']"
16,If an operator have only Real eigenvalues + symmetric then it's self-adjoint?,If an operator have only Real eigenvalues + symmetric then it's self-adjoint?,,"I know that if an operator is self-adjoint then has Real eigenvalues but I'm not sure about the converse i.e. if it has only Real eigenvalues and is symmetric then the operator is selfadjoint. Is that true? ----Edit--- The difference between selfadjoint and symmetric being the definition set. Symmetric has an extension which coincide in the original domain, while a Selfadjoint operator has the same domain of definition I define symmetric as follows. Let be $\mathcal{A}=\left(A,\mathfrak{D}_{A}\right)$ an operator densely defined and $\mathcal{A}^{*}=\left(A^{*},\mathfrak{D}_{A^{*}}\right)$ the adjoint operator, then $\mathcal{A}$ it is called symmetric if \begin{eqnarray}  &  & \mathfrak{D}_{A^{*}}\supseteq\mathfrak{D}_{A}\\  &  & A^{*}\psi=A\psi\qquad\forall\psi\in\mathfrak{D}_{A}. \end{eqnarray} While I define Self-adjoint like this Let be $\mathcal{A}=\left(A,\mathfrak{D}_{A}\right)$ an operator densely defined and $\mathcal{A}^{*}=\left(A^{*},\mathfrak{D}_{A^{*}}\right)$ the adjoint operator, then $\mathcal{A}$ it is called selfadjoint if \begin{eqnarray}  &  & \mathfrak{D}_{A^{*}}=\mathfrak{D}_{A}\\  &  & A^{*}\psi=A\psi\qquad\forall\psi\in\mathfrak{D}_{A}. \end{eqnarray}","I know that if an operator is self-adjoint then has Real eigenvalues but I'm not sure about the converse i.e. if it has only Real eigenvalues and is symmetric then the operator is selfadjoint. Is that true? ----Edit--- The difference between selfadjoint and symmetric being the definition set. Symmetric has an extension which coincide in the original domain, while a Selfadjoint operator has the same domain of definition I define symmetric as follows. Let be an operator densely defined and the adjoint operator, then it is called symmetric if While I define Self-adjoint like this Let be an operator densely defined and the adjoint operator, then it is called selfadjoint if","\mathcal{A}=\left(A,\mathfrak{D}_{A}\right) \mathcal{A}^{*}=\left(A^{*},\mathfrak{D}_{A^{*}}\right) \mathcal{A} \begin{eqnarray}
 &  & \mathfrak{D}_{A^{*}}\supseteq\mathfrak{D}_{A}\\
 &  & A^{*}\psi=A\psi\qquad\forall\psi\in\mathfrak{D}_{A}.
\end{eqnarray} \mathcal{A}=\left(A,\mathfrak{D}_{A}\right) \mathcal{A}^{*}=\left(A^{*},\mathfrak{D}_{A^{*}}\right) \mathcal{A} \begin{eqnarray}
 &  & \mathfrak{D}_{A^{*}}=\mathfrak{D}_{A}\\
 &  & A^{*}\psi=A\psi\qquad\forall\psi\in\mathfrak{D}_{A}.
\end{eqnarray}","['linear-algebra', 'functional-analysis', 'operator-theory', 'operator-algebras', 'banach-algebras']"
17,How Do I Compute the Eigenvalues of a Small Matrix?,How Do I Compute the Eigenvalues of a Small Matrix?,,"If I have a $2\times 2$ or $3\times 3$ matrix, how should I go about computing the eigenvalues and eigenvectors of the matrix? NB: I am making this question to provide a unified answer to questions about eigenvalues of small matrices so that all of the specific examples that come up can be marked as duplicates of this post. See here .","If I have a $2\times 2$ or $3\times 3$ matrix, how should I go about computing the eigenvalues and eigenvectors of the matrix? NB: I am making this question to provide a unified answer to questions about eigenvalues of small matrices so that all of the specific examples that come up can be marked as duplicates of this post. See here .",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'faq']"
18,"Existence of Hamel basis, choice and regularity","Existence of Hamel basis, choice and regularity",,"Blass (1984) shows that the existence of Hamel basis for arbitrary vector space over any field implies the axiom of choice. However such implication needs the axiom of regularity. As in Blass' article, existence of basis just implies axiom of multiple choice, strictly weaker than AC when without assuming regularity. In the article he says whether the existence of basis implies the axiom of choice in $\mathsf{ZF - regularity}$ remains open. However it has been 31 years since the paper published and I wonder there is a progress about it. I would appreciate your answer.","Blass (1984) shows that the existence of Hamel basis for arbitrary vector space over any field implies the axiom of choice. However such implication needs the axiom of regularity. As in Blass' article, existence of basis just implies axiom of multiple choice, strictly weaker than AC when without assuming regularity. In the article he says whether the existence of basis implies the axiom of choice in $\mathsf{ZF - regularity}$ remains open. However it has been 31 years since the paper published and I wonder there is a progress about it. I would appreciate your answer.",,"['linear-algebra', 'set-theory', 'axiom-of-choice']"
19,Help in finding the Jordan canonical form of a matrix,Help in finding the Jordan canonical form of a matrix,,"Determine the Jordan Canonical Form of the following matrix:  $$A=\begin{bmatrix}  1 & 2 & 3\\  0 & 4 & 5\\  0 & 0 & 4\\ \end{bmatrix}$$ I am trying to determine the Jordan Basis first. For that purpose I am trying to find out the generalized Eigenvectors of this matrix. Corresponding to $1$, Let $U_1$ be the generalized eigenspace. My calculations show that $$U_1=span\{(1,0,0)^t\}$$ and $U_2$ be the corresponding generalized eigenspace for $4$. I found out $$U_2=span\{(1,0,-9)^t,(0,1,6)^t\}$$All I need to do now is find the Jordan basis. Since $(A-\lambda_i I)|_{U_i  }$ is nilpotent, all I need to do is find the basis for each such $i$. I am confused from here on what to take as the jordan basis. I am sure that $(1,0,0)^t$ will feature as the first column. I am not sure about the other two. Thanks for the help!!","Determine the Jordan Canonical Form of the following matrix:  $$A=\begin{bmatrix}  1 & 2 & 3\\  0 & 4 & 5\\  0 & 0 & 4\\ \end{bmatrix}$$ I am trying to determine the Jordan Basis first. For that purpose I am trying to find out the generalized Eigenvectors of this matrix. Corresponding to $1$, Let $U_1$ be the generalized eigenspace. My calculations show that $$U_1=span\{(1,0,0)^t\}$$ and $U_2$ be the corresponding generalized eigenspace for $4$. I found out $$U_2=span\{(1,0,-9)^t,(0,1,6)^t\}$$All I need to do now is find the Jordan basis. Since $(A-\lambda_i I)|_{U_i  }$ is nilpotent, all I need to do is find the basis for each such $i$. I am confused from here on what to take as the jordan basis. I am sure that $(1,0,0)^t$ will feature as the first column. I am not sure about the other two. Thanks for the help!!",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
20,Are the coefficients of a vector according to a basis unique?,Are the coefficients of a vector according to a basis unique?,,"If I have a vector space $V$ ( of dimension $n$ ) over real numbers such that $\{v_1,v_2...v_n\}$ is the basis for the space ( not orthogonal ). Then I can write any vector $l$ in this space as $l=\sum_i\alpha_iv_i$. Here $\alpha_1,\alpha_2...\alpha_n$ are the coefficients that define the vector  $l$ according to this basis. Can another set of coefficients $\beta_1,\beta_2...\beta_n$ give the same vector $l$ ? If the basis was orthogonal the answer would be no, but I can't prove for a non orthogonal basis.","If I have a vector space $V$ ( of dimension $n$ ) over real numbers such that $\{v_1,v_2...v_n\}$ is the basis for the space ( not orthogonal ). Then I can write any vector $l$ in this space as $l=\sum_i\alpha_iv_i$. Here $\alpha_1,\alpha_2...\alpha_n$ are the coefficients that define the vector  $l$ according to this basis. Can another set of coefficients $\beta_1,\beta_2...\beta_n$ give the same vector $l$ ? If the basis was orthogonal the answer would be no, but I can't prove for a non orthogonal basis.",,"['linear-algebra', 'vector-spaces']"
21,Proof that $(A+B)^T=A^T+B^T$ (homework question),Proof that  (homework question),(A+B)^T=A^T+B^T,"Homework question: Proof that $(A+B)^T=A^T+B^T$ Let A and B be $m \times n$ matrices. Prove that $(A+B)^T=A^T+B^T$ by comparing the ij-th entries of the matrices on each side of this equation. (Let $A=(a_{ij})$ and $B=(b_{ij}$).) I am not sure how to do this proof, I know how to prove it by substituting ij-th entries with arbitrary numbers but I do not know how to do it by 'comparing the ij-th entries'","Homework question: Proof that $(A+B)^T=A^T+B^T$ Let A and B be $m \times n$ matrices. Prove that $(A+B)^T=A^T+B^T$ by comparing the ij-th entries of the matrices on each side of this equation. (Let $A=(a_{ij})$ and $B=(b_{ij}$).) I am not sure how to do this proof, I know how to prove it by substituting ij-th entries with arbitrary numbers but I do not know how to do it by 'comparing the ij-th entries'",,"['linear-algebra', 'matrices', 'matrix-equations']"
22,İf x is diagonalizable then ad(x) is also diagonalizable,İf x is diagonalizable then ad(x) is also diagonalizable,,"I start to study lie algebras from K. Erdmann, Mark J. Wildon-Introduction to Lie Algebras and I try to solve question below but actually I can't see. How can I start ? Give me a hint please Let $V$ be an $n$ -dimensional complex vector space and let $ L = gl(V)$ . Suppose that $x \in L$ is diagonalisable, with eigenvalues $λ_1, . . . , λ_n$ . Show that $\text{ad}(x) \in gl(L)$ is also diagonalisable and that its eigenvalues are $λ_i − λ_j$ for $1 ≤ i, j ≤ n$ . Definition: Let $L$ be a Lie algebra $\text{ad}(x)$ is a linear map from $L$ to itself defined by $\text{ad}(x)(y)=[x,y]$ :","I start to study lie algebras from K. Erdmann, Mark J. Wildon-Introduction to Lie Algebras and I try to solve question below but actually I can't see. How can I start ? Give me a hint please Let be an -dimensional complex vector space and let . Suppose that is diagonalisable, with eigenvalues . Show that is also diagonalisable and that its eigenvalues are for . Definition: Let be a Lie algebra is a linear map from to itself defined by :","V n  L = gl(V) x \in L λ_1, . . . , λ_n \text{ad}(x) \in gl(L) λ_i − λ_j 1 ≤ i, j ≤ n L \text{ad}(x) L \text{ad}(x)(y)=[x,y]","['linear-algebra', 'lie-groups', 'lie-algebras']"
23,Whether a $2 \times 2$ matrix of rank $1$ has a zero eigenvalue,Whether a  matrix of rank  has a zero eigenvalue,2 \times 2 1,"""Does $A = \begin{bmatrix}1&2\\2&4\end{bmatrix}$ have a zero eigenvalue?"" Well, it would be a funny question to ask if the asker didn't state that he wants us to explain without computing the characteristic polynomial. I have no idea how to do this.","""Does $A = \begin{bmatrix}1&2\\2&4\end{bmatrix}$ have a zero eigenvalue?"" Well, it would be a funny question to ask if the asker didn't state that he wants us to explain without computing the characteristic polynomial. I have no idea how to do this.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
24,Complex projections order in inner product,Complex projections order in inner product,,"So the complex projection is defined as $$\operatorname{proj}_\vec{u} \vec{v} = \frac{\langle \vec{v},\vec{u}\rangle}{\langle\vec{u},\vec{u}\rangle} \vec{u}$$ with complex inner product. I was wondering if there is a reason why we have to compute the inner product in the order $\langle\vec{v},\vec{u}\rangle$ instead of $\langle\vec{u},\vec{v}\rangle$. I understand that the result will change if the order is changed, however I'm trying to look for perhaps a geometric reasoning behind the order the inner product must be computed.","So the complex projection is defined as $$\operatorname{proj}_\vec{u} \vec{v} = \frac{\langle \vec{v},\vec{u}\rangle}{\langle\vec{u},\vec{u}\rangle} \vec{u}$$ with complex inner product. I was wondering if there is a reason why we have to compute the inner product in the order $\langle\vec{v},\vec{u}\rangle$ instead of $\langle\vec{u},\vec{v}\rangle$. I understand that the result will change if the order is changed, however I'm trying to look for perhaps a geometric reasoning behind the order the inner product must be computed.",,['linear-algebra']
25,Prove $W \cap W^\perp =\{\vec{0}\}$,Prove,W \cap W^\perp =\{\vec{0}\},"If $W$ is a subspace of $\mathbb{R}^n$, then $W^\perp = \overline{W} =  \{v \cdot w = 0, \forall w \in W\}$ Prove $W \cap W^\perp = \{\vec{0}\}$. How do I fully prove this intersection is $\vec{0}$? I showed $\vec{0}$ is in $W$, $\overline{W}$, since they are both subspaces of $\mathbb{R}^n$, thus $\vec{0}$ is a subset of $W \cap \overline{W}$  which is also a subset of $\vec{0}$. Then I must show $u = \vec{0}$ to complete. I have let $u$ be an element of  $W \cap \overline{W}$  which implies $u$ is an element of the zero vector (because they are subsets of each other). But does this show $u$ must be equal to $\vec{0}$? If not, how do I show that part?","If $W$ is a subspace of $\mathbb{R}^n$, then $W^\perp = \overline{W} =  \{v \cdot w = 0, \forall w \in W\}$ Prove $W \cap W^\perp = \{\vec{0}\}$. How do I fully prove this intersection is $\vec{0}$? I showed $\vec{0}$ is in $W$, $\overline{W}$, since they are both subspaces of $\mathbb{R}^n$, thus $\vec{0}$ is a subset of $W \cap \overline{W}$  which is also a subset of $\vec{0}$. Then I must show $u = \vec{0}$ to complete. I have let $u$ be an element of  $W \cap \overline{W}$  which implies $u$ is an element of the zero vector (because they are subsets of each other). But does this show $u$ must be equal to $\vec{0}$? If not, how do I show that part?",,"['linear-algebra', 'vector-spaces']"
26,Trace of nilpotent matrix over a ring,Trace of nilpotent matrix over a ring,,"Let $R$ be a commutative ring with unity, and $n$ a positive integer. Let $A\in \mathfrak{M}_n(R)$ such that there exists $m\in \mathbb N$, for which $A^m=0$. Is it true that there exists $\ell\in \mathbb N$, such that $\bigl(\text{tr}(A)\bigr)^\ell=0$ ? Remark : It is true for $n=1$ and $n=2$.","Let $R$ be a commutative ring with unity, and $n$ a positive integer. Let $A\in \mathfrak{M}_n(R)$ such that there exists $m\in \mathbb N$, for which $A^m=0$. Is it true that there exists $\ell\in \mathbb N$, such that $\bigl(\text{tr}(A)\bigr)^\ell=0$ ? Remark : It is true for $n=1$ and $n=2$.",,"['linear-algebra', 'ring-theory']"
27,Row space and column space for SVD,Row space and column space for SVD,,I have two questions. I know if I multiply a matrix like $A$ by vector $x$ then $Ax$ is like linear combination of columns of $A$. Now I would like to know what is the intuition when I multiply a matrix $A$ by another matrix $V$ so how can I imagine $AV$ like $Ax$? In terms of $\operatorname{SVD}$: $AV=U\Sigma$ why here $U$ is considered as a column space of $A$ and more importantly $V$ is in row space of $A$?,I have two questions. I know if I multiply a matrix like $A$ by vector $x$ then $Ax$ is like linear combination of columns of $A$. Now I would like to know what is the intuition when I multiply a matrix $A$ by another matrix $V$ so how can I imagine $AV$ like $Ax$? In terms of $\operatorname{SVD}$: $AV=U\Sigma$ why here $U$ is considered as a column space of $A$ and more importantly $V$ is in row space of $A$?,,"['linear-algebra', 'matrices', 'svd']"
28,Combining two convolution kernels,Combining two convolution kernels,,"Is it possible to combine two convolution kernels (convolution in terms of image processing, so it's actually a correlation) into one, so that covnolving the image with the new kernel gives the same output as convolving it with the first, and then the second kernel? For example, if I want to convolve an image with 1st 3x3 kernel and then 2nd 3x3 kernel, apparently I should be able to combine those two kernels and convolve my image with this new kernel. What is the size of my new kernel? Is it 3x3? If so, there's something worrying me about that. Let me explain. Let's say I'm convolving Input image with Kernel 1, then convolving the result with Kernel 2. Pixel marked X depends on value of pixel P2 (convolving the image with kernel 2). Pixel P2 lies on the new image that was produced by convolving input image with Kernel 1. So the value of P2 was calculated with taking P1 into account. Now, X relies on values of P1 and P2. That's what stops me from believing I could achieve the same result with convolving my original image with a 3x3 kernel combining kernel 1 and kernel 2. Pixel X cannot ""know"" anything about pixel at location P1.","Is it possible to combine two convolution kernels (convolution in terms of image processing, so it's actually a correlation) into one, so that covnolving the image with the new kernel gives the same output as convolving it with the first, and then the second kernel? For example, if I want to convolve an image with 1st 3x3 kernel and then 2nd 3x3 kernel, apparently I should be able to combine those two kernels and convolve my image with this new kernel. What is the size of my new kernel? Is it 3x3? If so, there's something worrying me about that. Let me explain. Let's say I'm convolving Input image with Kernel 1, then convolving the result with Kernel 2. Pixel marked X depends on value of pixel P2 (convolving the image with kernel 2). Pixel P2 lies on the new image that was produced by convolving input image with Kernel 1. So the value of P2 was calculated with taking P1 into account. Now, X relies on values of P1 and P2. That's what stops me from believing I could achieve the same result with convolving my original image with a 3x3 kernel combining kernel 1 and kernel 2. Pixel X cannot ""know"" anything about pixel at location P1.",,"['linear-algebra', 'convolution', 'image-processing']"
29,Is the U factor in LU decomposition for rectangular matrices always in row echelon form?,Is the U factor in LU decomposition for rectangular matrices always in row echelon form?,,"I have come across the following rectangular 5 x 10 matrix and carried out a LU decomposition of it, in the form PA = LU. The following matrices were obtained by function scipy.linalg.lu from module scipy to Python language. At first I thought was a computational issue, but after a contributor obtained the same result in both matlab and R, I decided to post it here (See: https://stackoverflow.com/questions/25186560/is-the-upper-triangular-matrix-in-function-scipy-linalg-lu-always-in-row-echelon ) \begin{equation}      A = \begin{bmatrix}         1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0 \\         1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 \\         1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\         0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 \\         1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \end{bmatrix} \end{equation} \begin{equation}     U = \begin{bmatrix}         1 &  1 &  1 &  1 &  0 &  1 &  1 &  1 &  1 & 0 \\         0 &  1 &  0 &  1 &  1 &  0 &  1 &  0 &  0 &  1 \\         0 &  0 & -1 & -1 &  0 &  0 &  0 & -1 & -1 &  0 \\         0 &  0 &  0 &  0 &  1 & -1 &  0 &  0 &  1 &  1 \\         0 &  0 &  0 &  0 &  1 &  0 &  0 &  1 &  1 &  1       \end{bmatrix} \end{equation} Notice that U is upper triangular, though it is not in row echelon form. Element U[5,5] = 1, but it should be zero, since the pivot for row 4 is immediately above it. According to Strang (1988, p.72), the U factor of a LU decomposition of a rectangular matrix should be in row echelon form, with the following characteristics: ""(i) The nonzero rows come first - otherwise there would have been row exchanges - and pivots are the first nonzero entries in those rows. (ii) Below each pivot is a column of zeros, obtained by elimination. (iii) Each pivot lies to the right of the pivot in the row above. This produces the staircase pattern."" STRANG, G. Linear Algebra and its applications. 3rd Ed., Thomson, 1988. The L factor and permutation matrix P are below. Notice in P that Gaussian elimination swapped rows 2 and 4. \begin{equation}     L = \begin{bmatrix}          1 &  0 &  0 &  0 &  0 \\           0 &  1 &  0 &  0 &  0 \\          1 &  0 &  1 &  0 &  0 \\          1 &  0 &  1 &  1 &  0 \\          1 &  0 &  1 &  0 &  1     \end{bmatrix} \end{equation} \begin{equation}     P = \begin{bmatrix}          1 &  0 &  0 &  0 &  0 \\          0 &  0 &  0 &  1 &  0 \\          0 &  0 &  1 &  0 &  0 \\          0 &  1 &  0 &  0 &  0 \\          0 &  0 &  0 &  0 &  1 \\     \end{bmatrix} \end{equation} Say one tries to cure the problem by cancelling out element U[5,5] = 1, by subtracting row 5 from row 4, and finally producing the row echelon form. L will not be modified, since L[5,5] will remain equals 1, and $PA \neq LU$. I suspect that Gaussian elimination does not always produce the row echelon form.","I have come across the following rectangular 5 x 10 matrix and carried out a LU decomposition of it, in the form PA = LU. The following matrices were obtained by function scipy.linalg.lu from module scipy to Python language. At first I thought was a computational issue, but after a contributor obtained the same result in both matlab and R, I decided to post it here (See: https://stackoverflow.com/questions/25186560/is-the-upper-triangular-matrix-in-function-scipy-linalg-lu-always-in-row-echelon ) \begin{equation}      A = \begin{bmatrix}         1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0 \\         1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 \\         1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\         0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 \\         1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \end{bmatrix} \end{equation} \begin{equation}     U = \begin{bmatrix}         1 &  1 &  1 &  1 &  0 &  1 &  1 &  1 &  1 & 0 \\         0 &  1 &  0 &  1 &  1 &  0 &  1 &  0 &  0 &  1 \\         0 &  0 & -1 & -1 &  0 &  0 &  0 & -1 & -1 &  0 \\         0 &  0 &  0 &  0 &  1 & -1 &  0 &  0 &  1 &  1 \\         0 &  0 &  0 &  0 &  1 &  0 &  0 &  1 &  1 &  1       \end{bmatrix} \end{equation} Notice that U is upper triangular, though it is not in row echelon form. Element U[5,5] = 1, but it should be zero, since the pivot for row 4 is immediately above it. According to Strang (1988, p.72), the U factor of a LU decomposition of a rectangular matrix should be in row echelon form, with the following characteristics: ""(i) The nonzero rows come first - otherwise there would have been row exchanges - and pivots are the first nonzero entries in those rows. (ii) Below each pivot is a column of zeros, obtained by elimination. (iii) Each pivot lies to the right of the pivot in the row above. This produces the staircase pattern."" STRANG, G. Linear Algebra and its applications. 3rd Ed., Thomson, 1988. The L factor and permutation matrix P are below. Notice in P that Gaussian elimination swapped rows 2 and 4. \begin{equation}     L = \begin{bmatrix}          1 &  0 &  0 &  0 &  0 \\           0 &  1 &  0 &  0 &  0 \\          1 &  0 &  1 &  0 &  0 \\          1 &  0 &  1 &  1 &  0 \\          1 &  0 &  1 &  0 &  1     \end{bmatrix} \end{equation} \begin{equation}     P = \begin{bmatrix}          1 &  0 &  0 &  0 &  0 \\          0 &  0 &  0 &  1 &  0 \\          0 &  0 &  1 &  0 &  0 \\          0 &  1 &  0 &  0 &  0 \\          0 &  0 &  0 &  0 &  1 \\     \end{bmatrix} \end{equation} Say one tries to cure the problem by cancelling out element U[5,5] = 1, by subtracting row 5 from row 4, and finally producing the row echelon form. L will not be modified, since L[5,5] will remain equals 1, and $PA \neq LU$. I suspect that Gaussian elimination does not always produce the row echelon form.",,"['linear-algebra', 'gaussian-elimination', 'matrix-decomposition']"
30,Show that the rank of $A+B$ is no more than the sum of the ranks of $A$ and $B$ [duplicate],Show that the rank of  is no more than the sum of the ranks of  and  [duplicate],A+B A B,"This question already has answers here : Rank of the difference of matrices [duplicate] (2 answers) Closed 9 years ago . Let $k$ and $n$ be positive integers and let $F$ be a field. For matrices $A,B \in M_{k\times n} (F)$ , show that the rank of $A+B$ is no more than the sum of the ranks of $A$ and $B$ I believe this question is addressed here , but to be honest I don't quite understand the explanations given. Can someone possibly help with a more detailed explanation?","This question already has answers here : Rank of the difference of matrices [duplicate] (2 answers) Closed 9 years ago . Let and be positive integers and let be a field. For matrices , show that the rank of is no more than the sum of the ranks of and I believe this question is addressed here , but to be honest I don't quite understand the explanations given. Can someone possibly help with a more detailed explanation?","k n F A,B \in M_{k\times n} (F) A+B A B","['linear-algebra', 'matrices', 'matrix-rank']"
31,Is the empty set is a subspace of any vector space,Is the empty set is a subspace of any vector space,,"Is the empty set is a subspace of any vector space? im not too sure about this one, is the zero vector in the empty set?","Is the empty set is a subspace of any vector space? im not too sure about this one, is the zero vector in the empty set?",,['linear-algebra']
32,A Householder matrix is symmetric,A Householder matrix is symmetric,,"I want to show that a Householder matrix is symmetric, so I must show that $H^T = H$, but from the formula $$H= I - (uu^T/\beta),$$ they are not equal. What's wrong with my reasoning? EDIT: I forgot that $(uu^T)^T$ would be $(u^T)^T(u)^T$ from the following properties:  $(AB)^T=B^TA^T$","I want to show that a Householder matrix is symmetric, so I must show that $H^T = H$, but from the formula $$H= I - (uu^T/\beta),$$ they are not equal. What's wrong with my reasoning? EDIT: I forgot that $(uu^T)^T$ would be $(u^T)^T(u)^T$ from the following properties:  $(AB)^T=B^TA^T$",,['linear-algebra']
33,Find the eigenvalues and eigenvectors of A geometrically,Find the eigenvalues and eigenvectors of A geometrically,,I am really confused with this question:   Find the eigenvalues and eigenvectors of A geometrically: $$ A = \begin {pmatrix} 0 & 1 \\ 1 & 0 \end {pmatrix} $$^ reflection in the line $y=x$. Thanks.,I am really confused with this question:   Find the eigenvalues and eigenvectors of A geometrically: $$ A = \begin {pmatrix} 0 & 1 \\ 1 & 0 \end {pmatrix} $$^ reflection in the line $y=x$. Thanks.,,"['linear-algebra', 'geometry', 'matrices', 'eigenvalues-eigenvectors']"
34,"Relations between matrices and linear transformations, diagonalization. [duplicate]","Relations between matrices and linear transformations, diagonalization. [duplicate]",,"This question already has answers here : A basic question on diagonalizability of a matrix (4 answers) Closed 10 years ago . This question bothered me for a while and hopefully someone can shed some light on the issue. A matrix $A$ is said to be diagonalizable if there is an invertible matrix $P$ and a diagonal matrix $D$ such that $A=P^{-1}DP$. That is the definition of a matrix being diagonalizable. Now, a linear transformation is said to be diagonalizable if there exists a basis $C$ of eigenvectors. Why is that the same thing? Let's say that $A$ is diagonalizable, I define a linear mapping: \begin{gather*} T\colon V \to V \\ T(v)=Av \end{gather*} Why does this say that $T$ is diagonalizable? Why does this mean that there is a basis of eigenvectors? And also the other way around: let's say that the transformation $T$ is diagonalizable. Why does this mean that $A$ is diagonalizable?","This question already has answers here : A basic question on diagonalizability of a matrix (4 answers) Closed 10 years ago . This question bothered me for a while and hopefully someone can shed some light on the issue. A matrix $A$ is said to be diagonalizable if there is an invertible matrix $P$ and a diagonal matrix $D$ such that $A=P^{-1}DP$. That is the definition of a matrix being diagonalizable. Now, a linear transformation is said to be diagonalizable if there exists a basis $C$ of eigenvectors. Why is that the same thing? Let's say that $A$ is diagonalizable, I define a linear mapping: \begin{gather*} T\colon V \to V \\ T(v)=Av \end{gather*} Why does this say that $T$ is diagonalizable? Why does this mean that there is a basis of eigenvectors? And also the other way around: let's say that the transformation $T$ is diagonalizable. Why does this mean that $A$ is diagonalizable?",,"['linear-algebra', 'matrices', 'transformation', 'diagonalization']"
35,"If $S\circ T=T\circ S$, then $S$ and $T$ have a common eigenvector.","If , then  and  have a common eigenvector.",S\circ T=T\circ S S T,"Assume $S$ and $T$ are diagonalizable maps on $\mathbb{R}^n$ such that $S\circ T$=$T \circ S$. Then $S$ and $T$ have a common eigenvector. I already have proof, but I just need validation in one part. My proof: Let $F$ be an eigenvector of $T$. This means $\exists \; \lambda \in R$ such that $T(v)=\lambda v$. Then, using the fact that $S\circ T$=$T \circ S$, we have $$ S(T(v)) = (S\circ T)(v)=(T \circ S)(v)=T(S(v)) \Longrightarrow T(S(v))=\lambda S(v)$$ Thus, $S(v)$ is also an eigenvector of $T$. So, $S$ maps eigenvectors of $T$ to eigenvevtors of $T$. Thus, $S$ must have an eigenvector of $T$. How would one rigorously prove that if $S$ maps eigenvectors of $T$ to eigenvectors of $T$, then $S$ also has an eigenvector of $T$? Thanks.","Assume $S$ and $T$ are diagonalizable maps on $\mathbb{R}^n$ such that $S\circ T$=$T \circ S$. Then $S$ and $T$ have a common eigenvector. I already have proof, but I just need validation in one part. My proof: Let $F$ be an eigenvector of $T$. This means $\exists \; \lambda \in R$ such that $T(v)=\lambda v$. Then, using the fact that $S\circ T$=$T \circ S$, we have $$ S(T(v)) = (S\circ T)(v)=(T \circ S)(v)=T(S(v)) \Longrightarrow T(S(v))=\lambda S(v)$$ Thus, $S(v)$ is also an eigenvector of $T$. So, $S$ maps eigenvectors of $T$ to eigenvevtors of $T$. Thus, $S$ must have an eigenvector of $T$. How would one rigorously prove that if $S$ maps eigenvectors of $T$ to eigenvectors of $T$, then $S$ also has an eigenvector of $T$? Thanks.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
36,"Nilpotent commutative matrices $A, B$ $\Longrightarrow$ $A+B$ nilpotent.",Nilpotent commutative matrices    nilpotent.,"A, B \Longrightarrow A+B","How to prove that if $A, B$ are matrix of $n\times n$ nilpotents so that $AB=BA$ then $A+B$ is nilpotent.","How to prove that if $A, B$ are matrix of $n\times n$ nilpotents so that $AB=BA$ then $A+B$ is nilpotent.",,"['linear-algebra', 'nilpotence']"
37,"If the determinant and the trace of a matrix are the same, then can we show they have the same eigenvalues?","If the determinant and the trace of a matrix are the same, then can we show they have the same eigenvalues?",,"If $\det A = \det B$ and $\operatorname{tr}A=\operatorname{tr} B$, then can we show $A$ and $B$ have the same eigenvalues?","If $\det A = \det B$ and $\operatorname{tr}A=\operatorname{tr} B$, then can we show $A$ and $B$ have the same eigenvalues?",,"['linear-algebra', 'matrices']"
38,"If $\langle Tv,v\rangle\in\mathbb{R}$, prove that $T$ is self-adjoint [duplicate]","If , prove that  is self-adjoint [duplicate]","\langle Tv,v\rangle\in\mathbb{R} T","This question already has answers here : If $\langle Ta, a\rangle \in \mathbb{R}$ for all $a$ then $T$ is self-adjoint (3 answers) Closed 5 years ago . Let $V$ be a finite-dimensional vector space over $\mathbb{C}$, together with a Hermitian inner product $\langle\cdot\,,\cdot\rangle$. Let $T:V\to V$ be a linear function. Prove that $T$ is self-adjoint if $\langle Tv,v\rangle\in\mathbb{R}$. I know that since $\langle Tv, v\rangle \in\mathbb{R}$, $\langle Tv,v\rangle=\langle v, Tv\rangle $ ... I'm not sure where to go from here.","This question already has answers here : If $\langle Ta, a\rangle \in \mathbb{R}$ for all $a$ then $T$ is self-adjoint (3 answers) Closed 5 years ago . Let $V$ be a finite-dimensional vector space over $\mathbb{C}$, together with a Hermitian inner product $\langle\cdot\,,\cdot\rangle$. Let $T:V\to V$ be a linear function. Prove that $T$ is self-adjoint if $\langle Tv,v\rangle\in\mathbb{R}$. I know that since $\langle Tv, v\rangle \in\mathbb{R}$, $\langle Tv,v\rangle=\langle v, Tv\rangle $ ... I'm not sure where to go from here.",,"['linear-algebra', 'inner-products']"
39,Find closest vector to A which is perpendicular to B,Find closest vector to A which is perpendicular to B,,"To start, I would like to apologize if the answer to my question was easily googled, I am quite new to this and googling ""Find closest vector to A which is perpendicular to B"" gave me no results. My problem: I am a procedural generation programmer looking for a way to do something slightly similar to a cross product: The cross product returns a vector which is perpendicular to two other vectors. I need a vector which is only perpendicular to one vector. However, I need this vector to be the closest vector to another vector. In other words, I would like to find a Vector C, with the smallest amount of difference between its self and Vector A -- But this vector MUST be perpendicular to Vector B. Is there any way to do this using a series of Dot/Cross products (or some other sort of vector arithmetic)? I am, sadly, not familiar with linear algebra, so I won't be able to solve for C in an answer in the form of an equation, so if you do decide to post an equation, please also post how I might make a computer solve for C using that equation. Extra details: I am working with unit vectors (Magnitude = 1) I am working in 3D (not 2D... or 4D for that matter) By ""Closest Vector to A"" I mean ""Dot product between C and A closest to 1""","To start, I would like to apologize if the answer to my question was easily googled, I am quite new to this and googling ""Find closest vector to A which is perpendicular to B"" gave me no results. My problem: I am a procedural generation programmer looking for a way to do something slightly similar to a cross product: The cross product returns a vector which is perpendicular to two other vectors. I need a vector which is only perpendicular to one vector. However, I need this vector to be the closest vector to another vector. In other words, I would like to find a Vector C, with the smallest amount of difference between its self and Vector A -- But this vector MUST be perpendicular to Vector B. Is there any way to do this using a series of Dot/Cross products (or some other sort of vector arithmetic)? I am, sadly, not familiar with linear algebra, so I won't be able to solve for C in an answer in the form of an equation, so if you do decide to post an equation, please also post how I might make a computer solve for C using that equation. Extra details: I am working with unit vectors (Magnitude = 1) I am working in 3D (not 2D... or 4D for that matter) By ""Closest Vector to A"" I mean ""Dot product between C and A closest to 1""",,"['linear-algebra', 'cross-product']"
40,A problem on linear algebra,A problem on linear algebra,,If $M$ is a $3 \times 3$ matrix such that $$[ 0 ~~1 ~~2 ]M = [ 1 ~~0~~ 0 ] \text{  and  } [ 3~~ 4 ~~5 ]M = [ 0 ~~1 ~~0 ]$$ then what is the value of $[ 6 ~~7 ~~8 ]M$ ? I guess some matrix property needs to be used.,If $M$ is a $3 \times 3$ matrix such that $$[ 0 ~~1 ~~2 ]M = [ 1 ~~0~~ 0 ] \text{  and  } [ 3~~ 4 ~~5 ]M = [ 0 ~~1 ~~0 ]$$ then what is the value of $[ 6 ~~7 ~~8 ]M$ ? I guess some matrix property needs to be used.,,[]
41,The matrix has rank $n$ if and only if $A$ is nonsingular and $B = A^{-1}$.,The matrix has rank  if and only if  is nonsingular and .,n A B = A^{-1},Let $A$ and $B$ be $n \times n$ matrices with real entries. Show that the matrix $$M = \left( \begin{matrix} A&I\\ I&B \end{matrix} \right)$$  has rank $n$ if and only if $A$ is nonsingular and $B = A^{-1}$. Totally stuck on it. Can I get some help.Thanks for your time.,Let $A$ and $B$ be $n \times n$ matrices with real entries. Show that the matrix $$M = \left( \begin{matrix} A&I\\ I&B \end{matrix} \right)$$  has rank $n$ if and only if $A$ is nonsingular and $B = A^{-1}$. Totally stuck on it. Can I get some help.Thanks for your time.,,"['linear-algebra', 'matrices']"
42,Understanding directional derivative and the gradient,Understanding directional derivative and the gradient,,I'm having trouble understanding the proof of directional derivative and the gradient. Could someone give me a easy-to-read proof of the directional derivative and explain why does the gradient point to the direction of maximum increase? Thank you very much for any help! =),I'm having trouble understanding the proof of directional derivative and the gradient. Could someone give me a easy-to-read proof of the directional derivative and explain why does the gradient point to the direction of maximum increase? Thank you very much for any help! =),,"['calculus', 'linear-algebra']"
43,"$A$ is skew hermitian, prove $(I-A)^{-1} (I+A)$ is unitary","is skew hermitian, prove  is unitary",A (I-A)^{-1} (I+A),"Given $A$ is a skew-hermitian, (i.e $A^H=−A$), the Cayley transform of $A$ is defined as: $W=(I-A)^{-1} (I+A)$. How can be proved that $W$ is unitary (i.e. $W^H W = W W^H = I$)?","Given $A$ is a skew-hermitian, (i.e $A^H=−A$), the Cayley transform of $A$ is defined as: $W=(I-A)^{-1} (I+A)$. How can be proved that $W$ is unitary (i.e. $W^H W = W W^H = I$)?",,"['linear-algebra', 'matrices']"
44,Correlation between polynomial equations and matrix determinants,Correlation between polynomial equations and matrix determinants,,"Expanding $p(x)=(ax-b)(cx+d)$ we get $acx^2+(ad-bc)x-bd$. Notice the determinant of the matrix $\begin{pmatrix} a & b \\ c & d \end{pmatrix} $ is $ad-bc$ exactly like the constant of $x$ in the polynomial expansion. So I searched links between the equation and the matrix determinant. I only found the solution of the equation $p(x)=0$ to be $x \in \{b/a, -d/c\}$. Then I found numerous other things, but none of them were simple enough to be considered worthy remembering. My question: What are the correlations between the polynomials and determinants of the matrices with entries being the coefficients of the polynomial?","Expanding $p(x)=(ax-b)(cx+d)$ we get $acx^2+(ad-bc)x-bd$. Notice the determinant of the matrix $\begin{pmatrix} a & b \\ c & d \end{pmatrix} $ is $ad-bc$ exactly like the constant of $x$ in the polynomial expansion. So I searched links between the equation and the matrix determinant. I only found the solution of the equation $p(x)=0$ to be $x \in \{b/a, -d/c\}$. Then I found numerous other things, but none of them were simple enough to be considered worthy remembering. My question: What are the correlations between the polynomials and determinants of the matrices with entries being the coefficients of the polynomial?",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
45,How to show $T$ is not one-one and $T$ is not ont0?,How to show  is not one-one and  is not ont0?,T T,"Suppose $V$ is the space of all $n \times n$ matrices with real elements. Define $T : V \to  V$ by $$T (A) = AB − BA,\; A \in V,$$  where $B \in V$ is a fixed   matrix. Show that for any $B \in V$, (a) $T$ is linear; (b) $T$ is not one-one; (c) $T$ is not onto. Trial: Part (a): $$\begin{align}T(aA_1+bA_2)&=(aA_1+bA_2)B-B(aA_1+bA_2)\\&=a(A_1B-BA_1)+B(A_2B-BA_2)\\&=aT(A_1)+bT(A_2)\end{align}$$ So $T$ is linear. Please help with the others.","Suppose $V$ is the space of all $n \times n$ matrices with real elements. Define $T : V \to  V$ by $$T (A) = AB − BA,\; A \in V,$$  where $B \in V$ is a fixed   matrix. Show that for any $B \in V$, (a) $T$ is linear; (b) $T$ is not one-one; (c) $T$ is not onto. Trial: Part (a): $$\begin{align}T(aA_1+bA_2)&=(aA_1+bA_2)B-B(aA_1+bA_2)\\&=a(A_1B-BA_1)+B(A_2B-BA_2)\\&=aT(A_1)+bT(A_2)\end{align}$$ So $T$ is linear. Please help with the others.",,"['linear-algebra', 'matrices']"
46,$S\subseteq V \Rightarrow \text{span}(S)\cong S^{00}$,,S\subseteq V \Rightarrow \text{span}(S)\cong S^{00},"Let $S$ be a subset of a finite dimensional vector space $V$. How to show that $$\text{span}(S)\cong S^{00}?$$ Here, $^0$ denotes the annihilator. That is, $$S^0:=\{f\in V^*:f(s)=0,\forall s\in S\}$$ where $V^*$ is the dual space. So $$S^{00}=\{F\in V^{**}:F(f)=0,\forall f\in S^0\}$$","Let $S$ be a subset of a finite dimensional vector space $V$. How to show that $$\text{span}(S)\cong S^{00}?$$ Here, $^0$ denotes the annihilator. That is, $$S^0:=\{f\in V^*:f(s)=0,\forall s\in S\}$$ where $V^*$ is the dual space. So $$S^{00}=\{F\in V^{**}:F(f)=0,\forall f\in S^0\}$$",,"['linear-algebra', 'dual-spaces']"
47,Does $A^n$ is unitary imply that $A$ is unitary?,Does  is unitary imply that  is unitary?,A^n A,"On a complex-valued vector space, does $A^n$ being unitary imply that $A$ is unitary?","On a complex-valued vector space, does being unitary imply that is unitary?",A^n A,"['linear-algebra', 'matrices', 'unitary-matrices']"
48,Finding two more vectors to make up an axis,Finding two more vectors to make up an axis,,"Given a vector $\vec{v} = (x, y, z)$, how do I find two vectors that make up an axis with $\vec{v}$? In other words, one of them is perpendicular and lies in the same plane and the other is normal to those two vectors.","Given a vector $\vec{v} = (x, y, z)$, how do I find two vectors that make up an axis with $\vec{v}$? In other words, one of them is perpendicular and lies in the same plane and the other is normal to those two vectors.",,[]
49,Proof that a given matrix has rank $n-1$,Proof that a given matrix has rank,n-1,Let $x_1 \cdots x_n$ be real numbers $x_i>1$ such that $$\frac{1}{x_1} + \cdots + \frac{1}{x_n} = 1$$ Is it true that the matrix $$ \left[\begin{matrix} x_1-1 & -1 & \cdots & -1 \\ -1 & x_2-1 & \cdots & -1 \\ \vdots & \vdots & \ddots & -1 \\ -1 & -1 & \cdots & x_n-1 \end{matrix}\right] $$ has rank $n-1$? Is there a simple proof?,Let $x_1 \cdots x_n$ be real numbers $x_i>1$ such that $$\frac{1}{x_1} + \cdots + \frac{1}{x_n} = 1$$ Is it true that the matrix $$ \left[\begin{matrix} x_1-1 & -1 & \cdots & -1 \\ -1 & x_2-1 & \cdots & -1 \\ \vdots & \vdots & \ddots & -1 \\ -1 & -1 & \cdots & x_n-1 \end{matrix}\right] $$ has rank $n-1$? Is there a simple proof?,,['linear-algebra']
50,"""Show"" that the direction cosines of a vector satisfies...","""Show"" that the direction cosines of a vector satisfies...",,"""Show"" that the direction cosines of a vector satisfies $$\cos^2 \alpha + \cos^2 \beta + \cos^2 \gamma = 1$$ I am stumped on these things: ""SHOW"" that the direction cosines corresponds to a given vector to satisfy the relation above. ----> How do you ""show"" this? What does this mean? Does this mean to use the direction cosines of a vector? I'm sure this is a proof but I don't know what the end result would look like or better, what I am expected to learn from this proof. I am not looking for a mere answer but really an in-depth explanation of the problem. Greatly appreciated. :)","""Show"" that the direction cosines of a vector satisfies $$\cos^2 \alpha + \cos^2 \beta + \cos^2 \gamma = 1$$ I am stumped on these things: ""SHOW"" that the direction cosines corresponds to a given vector to satisfy the relation above. ----> How do you ""show"" this? What does this mean? Does this mean to use the direction cosines of a vector? I'm sure this is a proof but I don't know what the end result would look like or better, what I am expected to learn from this proof. I am not looking for a mere answer but really an in-depth explanation of the problem. Greatly appreciated. :)",,"['linear-algebra', 'trigonometry']"
51,vector space without verification of the axioms,vector space without verification of the axioms,,"I'm trying to show that the functions $c_1 + c_2 \sin^2 x + c_3 \cos^2 x$ forms a vector space. And I will need to find a basis of it, and its dimension. Is there a way how to do this without verifying the 8 axioms for a vector space, and if we let the set $X = \{c_1 + c_2 \sin^2 x + c_3 \cos^2 x\}$ then we note that $1 = \sin^2 x + \cos^2 x$, and this is enough. So the dimension is $2$. Thanks. Can you please provide clarification on how the argument of the subspace of the vector space follows? I think you did it already by inspection, but its not very complete to me, can you please write it down? Thanks","I'm trying to show that the functions $c_1 + c_2 \sin^2 x + c_3 \cos^2 x$ forms a vector space. And I will need to find a basis of it, and its dimension. Is there a way how to do this without verifying the 8 axioms for a vector space, and if we let the set $X = \{c_1 + c_2 \sin^2 x + c_3 \cos^2 x\}$ then we note that $1 = \sin^2 x + \cos^2 x$, and this is enough. So the dimension is $2$. Thanks. Can you please provide clarification on how the argument of the subspace of the vector space follows? I think you did it already by inspection, but its not very complete to me, can you please write it down? Thanks",,['linear-algebra']
52,Prove: Every Subspace is the Kernel of a Linear Map between Vector Spaces,Prove: Every Subspace is the Kernel of a Linear Map between Vector Spaces,,"I'm trying to show that, given two finite-dimensional vector spaces $V,W$, and any subspace $V'$ of $V$, that there is a linear map $T:V\to W$, whose kernel is precisely $V'$, given the condition that  $\dim V-\dim(\ker T)<\dim W$. I would like to know if the same is true for infinite-dimensional spaces. Because of Rank-Nullity, we have restrictions on the respective dimensions; we need $$\dim W =\dim V-\dim(\ker T), \qquad\mbox{ (I think) }.$$ This is my work: let $\dim V=m $, $\dim W=r$; $r=m-n  $, for $\dim(\ker T)=n$. We start by taking a basis $$B_V':=\{v'_1,\ldots,v'_n\},$$  and extend $B_V'$ into a basis $B_V:=\{v'_1,\ldots,v'_n,v'_{n+1},\ldots,v'_m\}$ for $V$. Let $B_W:=\{w_1,w_2,\ldots,w_r\}$. Now, we define $T$: $$T(B_V'):=0,$$  i.e., $T$ is zero for every vector in $B_V'$, and $T$ is linear. By linearity, $T$ is zero on $V'$. Now: This is the part that seems harder: how to define $T$ outside of $V'$, so that $T(w) \neq0$ for $w \in V\setminus V'$. My idea is: i)We set up a bijection between the basis vectors in $B_V\setminus B_V'$, and the basis vectors in $B_W$, say: $$T(v'_{n+1})=w_1,$$ $$T(v'_{n+2})=w_2,$$ $$\vdots$$ $$T(v'_m)=w_r,$$ and extend $T$ linearly. ii) Since a bijection between basis vectors extended linearly gives rise to a Vector Space isomorphism, the kernel of $T|_{V\setminus V'}\rightarrow W$ is an isomorphism, so that its kernel is $0$. Does this work? Can we extend it to the infinite-dimensional case? Thanks.","I'm trying to show that, given two finite-dimensional vector spaces $V,W$, and any subspace $V'$ of $V$, that there is a linear map $T:V\to W$, whose kernel is precisely $V'$, given the condition that  $\dim V-\dim(\ker T)<\dim W$. I would like to know if the same is true for infinite-dimensional spaces. Because of Rank-Nullity, we have restrictions on the respective dimensions; we need $$\dim W =\dim V-\dim(\ker T), \qquad\mbox{ (I think) }.$$ This is my work: let $\dim V=m $, $\dim W=r$; $r=m-n  $, for $\dim(\ker T)=n$. We start by taking a basis $$B_V':=\{v'_1,\ldots,v'_n\},$$  and extend $B_V'$ into a basis $B_V:=\{v'_1,\ldots,v'_n,v'_{n+1},\ldots,v'_m\}$ for $V$. Let $B_W:=\{w_1,w_2,\ldots,w_r\}$. Now, we define $T$: $$T(B_V'):=0,$$  i.e., $T$ is zero for every vector in $B_V'$, and $T$ is linear. By linearity, $T$ is zero on $V'$. Now: This is the part that seems harder: how to define $T$ outside of $V'$, so that $T(w) \neq0$ for $w \in V\setminus V'$. My idea is: i)We set up a bijection between the basis vectors in $B_V\setminus B_V'$, and the basis vectors in $B_W$, say: $$T(v'_{n+1})=w_1,$$ $$T(v'_{n+2})=w_2,$$ $$\vdots$$ $$T(v'_m)=w_r,$$ and extend $T$ linearly. ii) Since a bijection between basis vectors extended linearly gives rise to a Vector Space isomorphism, the kernel of $T|_{V\setminus V'}\rightarrow W$ is an isomorphism, so that its kernel is $0$. Does this work? Can we extend it to the infinite-dimensional case? Thanks.",,['linear-algebra']
53,Why is the group action on the vector space of polynomials naturally a left action?,Why is the group action on the vector space of polynomials naturally a left action?,,"When seeking irreducible representations of a group (for example $\text{SL}(2,\mathbb{C})$ or $\text{SU}(2)$), one meets the following construction. Let $V$ be the space of polynomials in two variables. Define a group action on $V$ by $g\cdot P(z) = P(zg)$ (in some texts the convention is $P(g^{-1}z)$). Here $z=(z_1 \; z_2)$ is a row vector and matrix multiplication is implied. So my question is, why is this a left action? In both conventions (multiplication on the right, multiplication on the left by inverse) it naively appears to be a right action. I check that $g_1\cdot(g_2\cdot P(z))=g_1\cdot P(zg_2)=P(zg_2g_1)=(g_2g_1)\cdot P(z)$, which certainly looks like a right action. What am I doing wrong here? And how can I understand this issue on a more fundamental level? Is there some contravariant functor lurking around which converts the action? I found this discussion by Michael Joyce, which seemed relevant. The space of (homogeneous) polynomials can actually be realized as $\operatorname{Sym}^k(V^*).$ Maybe that dual space functor explains something?","When seeking irreducible representations of a group (for example $\text{SL}(2,\mathbb{C})$ or $\text{SU}(2)$), one meets the following construction. Let $V$ be the space of polynomials in two variables. Define a group action on $V$ by $g\cdot P(z) = P(zg)$ (in some texts the convention is $P(g^{-1}z)$). Here $z=(z_1 \; z_2)$ is a row vector and matrix multiplication is implied. So my question is, why is this a left action? In both conventions (multiplication on the right, multiplication on the left by inverse) it naively appears to be a right action. I check that $g_1\cdot(g_2\cdot P(z))=g_1\cdot P(zg_2)=P(zg_2g_1)=(g_2g_1)\cdot P(z)$, which certainly looks like a right action. What am I doing wrong here? And how can I understand this issue on a more fundamental level? Is there some contravariant functor lurking around which converts the action? I found this discussion by Michael Joyce, which seemed relevant. The space of (homogeneous) polynomials can actually be realized as $\operatorname{Sym}^k(V^*).$ Maybe that dual space functor explains something?",,"['linear-algebra', 'group-theory', 'representation-theory']"
54,Proving that $ A^n=0 $ if $UAU^{-1}=cA$?,Proving that  if ?, A^n=0  UAU^{-1}=cA,"Let $A$ be an $n\times n$ matrix and $U$ an invertible $n\times n$ matrix, both with coefficients in $\mathbb R$, and suppose that $ UAU^{-1}=cA $ for some $ c \in \mathbb R,c \neq 0, \pm 1$. How can we prove that $ A^n=0 $?","Let $A$ be an $n\times n$ matrix and $U$ an invertible $n\times n$ matrix, both with coefficients in $\mathbb R$, and suppose that $ UAU^{-1}=cA $ for some $ c \in \mathbb R,c \neq 0, \pm 1$. How can we prove that $ A^n=0 $?",,"['linear-algebra', 'abstract-algebra']"
55,Does a matrix have block diagonal structure if and only if its matrix exponential has it as well?,Does a matrix have block diagonal structure if and only if its matrix exponential has it as well?,,"Obviously, if $$\mathbf{A} = \begin{bmatrix}\mathbf{C} & \mathbf{0}\\ \mathbf{0} & \mathbf{D}\end{bmatrix}$$ then $$e^{A}=\begin{bmatrix}\mathbf{e^C} & \mathbf{0}\\ \mathbf{0} & \mathbf{e^D}\end{bmatrix}$$ Given any invertible matrix $X$ we can express it as $X = e^A$ (but complex matrix $A$ is not unique until $X$ is not from one-parameter subgroup of $\mbox{GL}(n, \mathbb{C})$ ). Anyway, given that $X$ has a block diagonal structure then does $A$ need to have block diagonal structure as well?","Obviously, if then Given any invertible matrix we can express it as (but complex matrix is not unique until is not from one-parameter subgroup of ). Anyway, given that has a block diagonal structure then does need to have block diagonal structure as well?","\mathbf{A} = \begin{bmatrix}\mathbf{C} & \mathbf{0}\\ \mathbf{0} & \mathbf{D}\end{bmatrix} e^{A}=\begin{bmatrix}\mathbf{e^C} & \mathbf{0}\\ \mathbf{0} & \mathbf{e^D}\end{bmatrix} X X = e^A A X \mbox{GL}(n, \mathbb{C}) X A","['linear-algebra', 'matrices', 'block-matrices', 'matrix-exponential']"
56,The Dual Pairing,The Dual Pairing,,"My understanding from the reading the Wikipedia article on Dual Pairs is that a dual pair is comprised of two vector spaces $X$ and $Y$  over a field $\mathbb{K}$ together with a nondegenerate bilinear mapping $\langle \cdot, \cdot \rangle: X \times Y \rightarrow \mathbb{K}$. For $V$ a vector space and it (algebraic) dual $V^{*}$, I have  seen this notation used to effectively represent the evaluation of an element $f \in V^{*}$ at a point $v \in V$ to yield an element of $\mathbb{K}$, i.e., $\langle f, v \rangle = f(v)$ But, from reading the Wikipedia definition, there seems to be no requirement that $X$ and $Y$ are themselves ""dual"" spaces. So, if we simply supply an inner product $(\cdot | \cdot)$ for $V$ have we effectively induced a ""dual paring"" between $V$ and itself, i.e., $\langle u, v \rangle = (u | v)$ ? Finally, I have also seen the Lie derivative of a smooth function $f$ on a manifold $M$ with respect to a smooth vector field on $M$ defined by $L_vf = \langle df, v \rangle$. How should the ""dual paring"" in this instance be interpreted? Here, $v$ is in the tangent bundle and $df$ is an element of the cotangent bundle so in this case these spaces truly dual in the algebraic sense. Does this notation then just mean to evaluate the differential of $f$ on $v$? A concrete example would be helpful.","My understanding from the reading the Wikipedia article on Dual Pairs is that a dual pair is comprised of two vector spaces $X$ and $Y$  over a field $\mathbb{K}$ together with a nondegenerate bilinear mapping $\langle \cdot, \cdot \rangle: X \times Y \rightarrow \mathbb{K}$. For $V$ a vector space and it (algebraic) dual $V^{*}$, I have  seen this notation used to effectively represent the evaluation of an element $f \in V^{*}$ at a point $v \in V$ to yield an element of $\mathbb{K}$, i.e., $\langle f, v \rangle = f(v)$ But, from reading the Wikipedia definition, there seems to be no requirement that $X$ and $Y$ are themselves ""dual"" spaces. So, if we simply supply an inner product $(\cdot | \cdot)$ for $V$ have we effectively induced a ""dual paring"" between $V$ and itself, i.e., $\langle u, v \rangle = (u | v)$ ? Finally, I have also seen the Lie derivative of a smooth function $f$ on a manifold $M$ with respect to a smooth vector field on $M$ defined by $L_vf = \langle df, v \rangle$. How should the ""dual paring"" in this instance be interpreted? Here, $v$ is in the tangent bundle and $df$ is an element of the cotangent bundle so in this case these spaces truly dual in the algebraic sense. Does this notation then just mean to evaluate the differential of $f$ on $v$? A concrete example would be helpful.",,"['linear-algebra', 'differential-geometry', 'notation']"
57,Deriving the inverse of $\mathbf{I}$+Idempotent matrix,Deriving the inverse of +Idempotent matrix,\mathbf{I},"Suppose I have an idempotent matrix such that $A^2=A$ . From its properties, if $A$ is not an identity matrix, then it is singular. Through trial and error, I can see that for all $I+A$ are invertible. But how can I show that $I+A$ is indeed invertible and then to find out its inverse? I searched on this and found out that the $(I+A)^{-1}=\frac{1}{2}(2I-A)$ so it must be through that $I+A$ has an inverse but how was this derived?","Suppose I have an idempotent matrix such that . From its properties, if is not an identity matrix, then it is singular. Through trial and error, I can see that for all are invertible. But how can I show that is indeed invertible and then to find out its inverse? I searched on this and found out that the so it must be through that has an inverse but how was this derived?",A^2=A A I+A I+A (I+A)^{-1}=\frac{1}{2}(2I-A) I+A,"['linear-algebra', 'matrices']"
58,How to find a basis for this sub-space?,How to find a basis for this sub-space?,,"I am given a subspace of all polynomials $f(t)$ in $\mathbf{P}_2$ such that $f(1)=0$. I know that a basis for this space is $1-t$, $1-t^{2}$, and when I look at it, it makes perfect sense as to why. I was just wondering what is a systematic way of finding it, without eyeballing. Thanks!","I am given a subspace of all polynomials $f(t)$ in $\mathbf{P}_2$ such that $f(1)=0$. I know that a basis for this space is $1-t$, $1-t^{2}$, and when I look at it, it makes perfect sense as to why. I was just wondering what is a systematic way of finding it, without eyeballing. Thanks!",,[]
59,Calculate the rank of the following matrices,Calculate the rank of the following matrices,,"Question: Calculate the rank of the following matrices: $A = \left( \begin{array}{cc} 1 & n \\ n & 1 \end{array} \right), n \in \mathbb{Z}$ and $B = \left( \begin{array}{ccc} 1 & x & x^{2} \\ 1 & y & y^{2} \\ 1 & z & z^{2} \end{array} \right)$,  $x,y,z \in \mathbb{R}$. So the way I understand rank($A$), is the number of pivots in an echelon form of $A$. To put $A$ into echelon form I would subtract $n$ times the first row from the second row: $A \sim \left( \begin{array}{cc} 1 & n \\ n & 1 \end{array} \right) \sim \left( \begin{array}{cc} 1 & n \\ 0 & 1 - n^{2} \end{array} \right) \Rightarrow $rank$(A) = 2$. With $B$ I would have done pretty much the same thing, subtracting row 1 from both row 2 and row 3: $B \sim \left( \begin{array}{ccc} 1 & x & x^{2} \\ 1 & y & y^{2} \\ 1 & z & z^{2} \end{array} \right) \sim \left( \begin{array}{ccc} 1 & x & x^{2} \\ 0 & y - x & y^{2} - x^{2} \\ 0 & z - x & z^{2} - x^{2} \end{array} \right)$ (at this point I could multiply row 2 by $-(\frac{z-x}{y-x})$ and add it to row 3 which ends up being a long polynomial....) However, with both parts, I am pretty confident that it is not so simple and that I am missing the point of this exercise. Could somebody please help point me in the right direction?","Question: Calculate the rank of the following matrices: $A = \left( \begin{array}{cc} 1 & n \\ n & 1 \end{array} \right), n \in \mathbb{Z}$ and $B = \left( \begin{array}{ccc} 1 & x & x^{2} \\ 1 & y & y^{2} \\ 1 & z & z^{2} \end{array} \right)$,  $x,y,z \in \mathbb{R}$. So the way I understand rank($A$), is the number of pivots in an echelon form of $A$. To put $A$ into echelon form I would subtract $n$ times the first row from the second row: $A \sim \left( \begin{array}{cc} 1 & n \\ n & 1 \end{array} \right) \sim \left( \begin{array}{cc} 1 & n \\ 0 & 1 - n^{2} \end{array} \right) \Rightarrow $rank$(A) = 2$. With $B$ I would have done pretty much the same thing, subtracting row 1 from both row 2 and row 3: $B \sim \left( \begin{array}{ccc} 1 & x & x^{2} \\ 1 & y & y^{2} \\ 1 & z & z^{2} \end{array} \right) \sim \left( \begin{array}{ccc} 1 & x & x^{2} \\ 0 & y - x & y^{2} - x^{2} \\ 0 & z - x & z^{2} - x^{2} \end{array} \right)$ (at this point I could multiply row 2 by $-(\frac{z-x}{y-x})$ and add it to row 3 which ends up being a long polynomial....) However, with both parts, I am pretty confident that it is not so simple and that I am missing the point of this exercise. Could somebody please help point me in the right direction?",,"['linear-algebra', 'matrices']"
60,Alternative of exterior power as a tensor algebra,Alternative of exterior power as a tensor algebra,,"This question is related to my previous question . The $n$ -th exterior power of a vector space (over some field of characteristic zero) $U$ is a pair $(\wedge^{n}, \bigwedge^{n}U)$ where $\bigwedge^{n}U$ is a vector space over the same field and $\wedge^{n}: \overbrace{U\times \cdots \times U}^{\text{$n$ times}} \to \bigwedge^{n}U$ is an alternating $n$ -linear map satisfying the universal property. It can be proved that the $n$ -th exterior power of $U$ , if it exists, is unique up to isomorphisms. To prove that it in fact exists, one can explicitly construct a pair with the desired properties. In the literature, this is most commonly done by taking $\bigwedge^{n}U = \bigotimes^{n}U/K_{n}(U)$ , that is, the quotient space of the tensor algebra with the subspace $K_{n}(U)$ generated by products of the form $u_{1}\otimes \cdots \otimes u_{n}$ with $u_{i} = u_{j}$ for at least one pair of indices $i \neq j$ . One important result then is that such $\bigwedge^{n}U$ is isomorphic to $\text{Alt}^{n}(U)$ , the space of alternating $n$ -tensors. In other words, one can identify: $$\wedge^{n}(u_{1},...,u_{n}) = u_{1}\wedge\cdots \wedge u_{n} = \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)}.$$ There is one thing that bothers me and, after a quick search on MSE, it seems that it bothers a lot of other students as well. I still did not find a clear explanation of it, this is why I am asking this question. For simplicity, assume that $U$ is a finite-dimensional vector space over $\mathbb{C}$ . Let $f: U\times \cdots \times U \to \text{Alt}^{n}(U)$ be given by: $$f(u_{1},...,u_{n}) := \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)}$$ be the alternating map. As the name suggests, this is an alternating $n$ -linear map and, from my previous question, it satisfies the universal property. In other words, $f$ can be used to define $\bigwedge^{n}U$ ; simply take $\bigwedge^{n}U = \text{Alt}^{n}(U)$ and $\wedge^{n} = f$ . However, this is not the standard construction and many posts on MSE claim that this is not a good way of defining $\bigwedge^{n}U$ . But I don't understand why. To be fair, some of the questions I saw about this topic have a more abstract setting; one takes $U$ to be a $\mathbb{K}$ -module. I don't have much background in algebra yet, but maybe in these more abstract settings these constructions are not equivalent for some reason. Concerning this topic and in the case of vector spaces , I have the following questions: Are these constructions equivalent or not? Can I use the alternating map $f$ to define my exterior power $\bigwedge^{n}U$ ? If so, does it work only in finite dimension or not? (It does not seem to me that it depends on the dimension but I might be missing something). Is there any real advantage of defining $\bigwedge^{n}U$ as $\bigotimes^{n}U/K_{n}(U)$ instead of $\bigwedge^{n}U = \text{Alt}^{n}(U)$ ?","This question is related to my previous question . The -th exterior power of a vector space (over some field of characteristic zero) is a pair where is a vector space over the same field and is an alternating -linear map satisfying the universal property. It can be proved that the -th exterior power of , if it exists, is unique up to isomorphisms. To prove that it in fact exists, one can explicitly construct a pair with the desired properties. In the literature, this is most commonly done by taking , that is, the quotient space of the tensor algebra with the subspace generated by products of the form with for at least one pair of indices . One important result then is that such is isomorphic to , the space of alternating -tensors. In other words, one can identify: There is one thing that bothers me and, after a quick search on MSE, it seems that it bothers a lot of other students as well. I still did not find a clear explanation of it, this is why I am asking this question. For simplicity, assume that is a finite-dimensional vector space over . Let be given by: be the alternating map. As the name suggests, this is an alternating -linear map and, from my previous question, it satisfies the universal property. In other words, can be used to define ; simply take and . However, this is not the standard construction and many posts on MSE claim that this is not a good way of defining . But I don't understand why. To be fair, some of the questions I saw about this topic have a more abstract setting; one takes to be a -module. I don't have much background in algebra yet, but maybe in these more abstract settings these constructions are not equivalent for some reason. Concerning this topic and in the case of vector spaces , I have the following questions: Are these constructions equivalent or not? Can I use the alternating map to define my exterior power ? If so, does it work only in finite dimension or not? (It does not seem to me that it depends on the dimension but I might be missing something). Is there any real advantage of defining as instead of ?","n U (\wedge^{n}, \bigwedge^{n}U) \bigwedge^{n}U \wedge^{n}: \overbrace{U\times \cdots \times U}^{\text{n times}} \to \bigwedge^{n}U n n U \bigwedge^{n}U = \bigotimes^{n}U/K_{n}(U) K_{n}(U) u_{1}\otimes \cdots \otimes u_{n} u_{i} = u_{j} i \neq j \bigwedge^{n}U \text{Alt}^{n}(U) n \wedge^{n}(u_{1},...,u_{n}) = u_{1}\wedge\cdots \wedge u_{n} = \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)}. U \mathbb{C} f: U\times \cdots \times U \to \text{Alt}^{n}(U) f(u_{1},...,u_{n}) := \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)} n f \bigwedge^{n}U \bigwedge^{n}U = \text{Alt}^{n}(U) \wedge^{n} = f \bigwedge^{n}U U \mathbb{K} f \bigwedge^{n}U \bigwedge^{n}U \bigotimes^{n}U/K_{n}(U) \bigwedge^{n}U = \text{Alt}^{n}(U)","['linear-algebra', 'abstract-algebra', 'differential-geometry', 'differential-topology', 'exterior-algebra']"
61,"For three vectors in any inner product space, prove the following inequality.","For three vectors in any inner product space, prove the following inequality.",,"Let $x, y, z$ be three unit vectors in a real inner product space. Assume that we are given the inner products $\langle x, y \rangle = a \in \mathbb{R}$ and $\langle y, z \rangle = b \in \mathbb{R}$ . How do you obtain a bound on the value of $\langle z, x \rangle$ , in particular, prove the following? $$ \langle z, x \rangle \geq ab - \sqrt{1 - a^2} \sqrt{1 - b^2} $$ My Approach I was able to prove this inequality for the vector space $\mathbb{R}^n$ geometrically. I assumed that the angle between $x$ and $y$ is $\theta$ and the angle between $y$ and $z$ is $\phi$ . Thus $a = \cos{\theta}, b = \cos{\phi}$ . The inner product of $x$ and $z$ is minimized when the angle between them is the largest and this happens when all three vectors are coplanar and $x$ and $z$ are on opposite sides of $y$ $$\therefore \min \langle z, x \rangle = \cos{(\theta + \phi)} = \cos{\theta} \cos{\phi} - \sin{\theta} \sin{\phi} = ab - \sqrt{1 - a^2} \sqrt{1 - b^2}$$ I however want to approach this question from a purely algebraic perspective without involving trigonometric or visualization, using purely the results from the algebra of inner product spaces. Thanks in advance!","Let be three unit vectors in a real inner product space. Assume that we are given the inner products and . How do you obtain a bound on the value of , in particular, prove the following? My Approach I was able to prove this inequality for the vector space geometrically. I assumed that the angle between and is and the angle between and is . Thus . The inner product of and is minimized when the angle between them is the largest and this happens when all three vectors are coplanar and and are on opposite sides of I however want to approach this question from a purely algebraic perspective without involving trigonometric or visualization, using purely the results from the algebra of inner product spaces. Thanks in advance!","x, y, z \langle x, y \rangle = a \in \mathbb{R} \langle y, z \rangle = b \in \mathbb{R} \langle z, x \rangle  \langle z, x \rangle \geq ab - \sqrt{1 - a^2} \sqrt{1 - b^2}  \mathbb{R}^n x y \theta y z \phi a = \cos{\theta}, b = \cos{\phi} x z x z y \therefore \min \langle z, x \rangle = \cos{(\theta + \phi)} = \cos{\theta} \cos{\phi} - \sin{\theta} \sin{\phi} = ab - \sqrt{1 - a^2} \sqrt{1 - b^2}","['linear-algebra', 'inequality', 'inner-products']"
62,Minimizing $\frac{\|p\|_2^2}{\|p\|_\infty}$ for probability vector $p$,Minimizing  for probability vector,\frac{\|p\|_2^2}{\|p\|_\infty} p,"Suppose $p$ is a discrete probability distribution over $d$ outcomes. I need to minimize the following $$J=\frac{\|p\|_2^2}{\|p\|_\infty}$$ For $d=4$ , Mathematica suggests the minimum is $\frac{2}{3}$ achieved at $\left(\frac{1}{2},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right)$ . Is there a formula for general $d$ ? Notebook Motivation This provides the shape of quadratic $H$ for which a single step of gradient descent with $1/\|H\|$ step size makes the least progress. Hence it gives a bound on progress of gradient descent in general.","Suppose is a discrete probability distribution over outcomes. I need to minimize the following For , Mathematica suggests the minimum is achieved at . Is there a formula for general ? Notebook Motivation This provides the shape of quadratic for which a single step of gradient descent with step size makes the least progress. Hence it gives a bound on progress of gradient descent in general.","p d J=\frac{\|p\|_2^2}{\|p\|_\infty} d=4 \frac{2}{3} \left(\frac{1}{2},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right) d H 1/\|H\|","['linear-algebra', 'probability', 'optimization', 'upper-lower-bounds']"
63,How do you find the area of a triangle given two $2D$ vectors?,How do you find the area of a triangle given two  vectors?,2D,"Question: The absolute value of the determinant of a matrix computes the volume of the shape defined by its row vectors. In $2D$ , this is the area of the parallelogram. What is the area of a triangle with vectors $u = (3, 6)$ , $v = (8, 10)$ as sides? I found a lot of resources of finding the area with $3D$ vectors, but none for $2D$ . In this case, I'm not able to follow the steps of finding the cross product first, then magnitude and finally, the area of the triangle. How do I solve this?","Question: The absolute value of the determinant of a matrix computes the volume of the shape defined by its row vectors. In , this is the area of the parallelogram. What is the area of a triangle with vectors , as sides? I found a lot of resources of finding the area with vectors, but none for . In this case, I'm not able to follow the steps of finding the cross product first, then magnitude and finally, the area of the triangle. How do I solve this?","2D u = (3, 6) v = (8, 10) 3D 2D","['linear-algebra', 'matrices', 'geometry', 'area']"
64,3blue1brown and the visual argument that a vector is fundamentally different from a matrix,3blue1brown and the visual argument that a vector is fundamentally different from a matrix,,"After watching the wonderful playlist of Linear Algebra on the YouTube channel 3blue1brown , I realized that vectors and matrices are, fundamentally, different concepts: Vectors are numerical entities in a $n$ -dimensional space. Matrices are how theses numbers are (linearly) transformed. I am restricting it to a Linear Algebra. Of course, for multilinear algebra, matrices (and tensors) are, in fact, numerical entities in the same way that a vector is. Regardless, for Linear Algebra, this argument seems to have a much deeper interpretation that what is commonly said about vectors and matrices : A vector is just a one-column matrix I am not saying that such answer is wrong, once denoting a vector as a column is merely a convention. But doesn't it seem that this argument leaves out the main purpose of using matrices in linear algebra, which is apply linear transformations on vectors though matrix product,i.e., $\mathbf{Ax}$ ?","After watching the wonderful playlist of Linear Algebra on the YouTube channel 3blue1brown , I realized that vectors and matrices are, fundamentally, different concepts: Vectors are numerical entities in a -dimensional space. Matrices are how theses numbers are (linearly) transformed. I am restricting it to a Linear Algebra. Of course, for multilinear algebra, matrices (and tensors) are, in fact, numerical entities in the same way that a vector is. Regardless, for Linear Algebra, this argument seems to have a much deeper interpretation that what is commonly said about vectors and matrices : A vector is just a one-column matrix I am not saying that such answer is wrong, once denoting a vector as a column is merely a convention. But doesn't it seem that this argument leaves out the main purpose of using matrices in linear algebra, which is apply linear transformations on vectors though matrix product,i.e., ?",n \mathbf{Ax},"['linear-algebra', 'matrices', 'vectors', 'linear-transformations']"
65,Property of positive definite matrix: Why is this True?,Property of positive definite matrix: Why is this True?,,"I have left maths world a few years ago and doing Machine learning nowadays. But In one of the paper I am reading, the author just writes (well slightly different wording/ notation) $H$ is positive definite so $H^{-1}v$ = arg min $_t \{ t^\top H t - v^\top t \}$ and quite honestly I have no idea why this is the case. Help would be nice thanks!","I have left maths world a few years ago and doing Machine learning nowadays. But In one of the paper I am reading, the author just writes (well slightly different wording/ notation) is positive definite so = arg min and quite honestly I have no idea why this is the case. Help would be nice thanks!",H H^{-1}v _t \{ t^\top H t - v^\top t \},"['linear-algebra', 'matrices', 'positive-definite']"
66,What is the largest (finite) order of an element of $GL_{10}(\mathbb{Q})$?,What is the largest (finite) order of an element of ?,GL_{10}(\mathbb{Q}),"What is the largest (finite) order of an element of $GL_{10}(\mathbb{Q})$ ? The following is my guess: Let $A \in GL_{10}(\mathbb Q)$ is of finite order $n$ (say). Then $A^n=I$ , or every eigenvalue of $A$ is an $n$ -th root of $1$ . Thus an eigenvalue $\lambda$ of $A$ has degree $\leq\phi(n)$ over $\mathbb Q$ . Equality holds only when $\lambda$ is primitive $n$ -th root of $1$ . OTOH $\lambda$ also satisfies the characteristic polynomial of $A$ , which is of degree $10$ . Thus if the characteristic polynomial is irreducible (e.g., $A$ is companion matrix of $X^{10}+X^9+\cdots+X+1$ ) then degree of $\lambda=10$ , then we must have $\phi(n)\leq10$ . The maximum $n$ such that $\phi(n)=10$ is $15$ . If I am doing some mistake please correct me. Thanks.","What is the largest (finite) order of an element of ? The following is my guess: Let is of finite order (say). Then , or every eigenvalue of is an -th root of . Thus an eigenvalue of has degree over . Equality holds only when is primitive -th root of . OTOH also satisfies the characteristic polynomial of , which is of degree . Thus if the characteristic polynomial is irreducible (e.g., is companion matrix of ) then degree of , then we must have . The maximum such that is . If I am doing some mistake please correct me. Thanks.",GL_{10}(\mathbb{Q}) A \in GL_{10}(\mathbb Q) n A^n=I A n 1 \lambda A \leq\phi(n) \mathbb Q \lambda n 1 \lambda A 10 A X^{10}+X^9+\cdots+X+1 \lambda=10 \phi(n)\leq10 n \phi(n)=10 15,"['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory']"
67,"A matrix equation, are there solutions?","A matrix equation, are there solutions?",,"I try to prove that: there are no matrices $A, B \in M_{3 \times 3}(\mathbb{R})$ such that: $ \begin{pmatrix} 0&1&0\\ 0&0&1\\ 0&0&0 \end{pmatrix} = A^2 + B^2 $ How can I do that? I have no idea.",I try to prove that: there are no matrices such that: How can I do that? I have no idea.,"A, B \in M_{3 \times 3}(\mathbb{R})  \begin{pmatrix} 0&1&0\\ 0&0&1\\ 0&0&0 \end{pmatrix} = A^2 + B^2 ",['linear-algebra']
68,"For real Lie algebras, is any invariant bilinear form a scalar multiple of the Killing form?","For real Lie algebras, is any invariant bilinear form a scalar multiple of the Killing form?",,"I know that for a complex Lie algebra, Schur's lemma can be used to show that any invariant bilinear form on a simple Lie algebra is a scalar multiple of the killing form, but Schur's lemma does not hold for $\mathbb{R}$ , so I guess this isn't true for real simple Lie algebras. Does someone know a example? I can't seem to find one.","I know that for a complex Lie algebra, Schur's lemma can be used to show that any invariant bilinear form on a simple Lie algebra is a scalar multiple of the killing form, but Schur's lemma does not hold for , so I guess this isn't true for real simple Lie algebras. Does someone know a example? I can't seem to find one.",\mathbb{R},"['linear-algebra', 'abstract-algebra', 'lie-groups', 'lie-algebras']"
69,"Eigenvalues of an $n\times n$ matrix where all rows are equal to $[1,2,...,n]$",Eigenvalues of an  matrix where all rows are equal to,"n\times n [1,2,...,n]","I'm stuck on a problem to calculate the eigenvalues for a matrix: \begin{equation*} A_{n,n} =  \begin{bmatrix} 1 & 2 & 3 &\ldots& n \\ 1 & 2 & 3 & \ldots& n \\ 1 & 2 & 3 &\ldots& n \\ 1 & 2 & 3 &\ldots& n \\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 1 & 2 & 3 &\ldots& n  \end{bmatrix} \end{equation*} Been trying to go via: \begin{equation} |A_{n,n} - \lambda| = 0 \\ \end{equation} Not sure if QR decomposition be a better route to go? Any help would be appreciated!",I'm stuck on a problem to calculate the eigenvalues for a matrix: Been trying to go via: Not sure if QR decomposition be a better route to go? Any help would be appreciated!,"\begin{equation*}
A_{n,n} = 
\begin{bmatrix}
1 & 2 & 3 &\ldots& n \\
1 & 2 & 3 & \ldots& n \\
1 & 2 & 3 &\ldots& n \\
1 & 2 & 3 &\ldots& n \\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1 & 2 & 3 &\ldots& n 
\end{bmatrix}
\end{equation*} \begin{equation}
|A_{n,n} - \lambda| = 0 \\
\end{equation}","['linear-algebra', 'eigenvalues-eigenvectors']"
70,Why is $LU$ preferred over $A^{-1}$ to solve matrix equations?,Why is  preferred over  to solve matrix equations?,LU A^{-1},"I understand the whole $LU$ -decomposition vs Gaussian elimination argument. The fact that you can isolate the computationally expensive elimination step and re-use the $L$ and $U$ matrices for $Ax=b$ style equations with different $b$ :s makes sense to me. But I can't seem to find a reason for why the $L$ and $U$ matrices are preferred over an $A^{-1}$ matrix. It can also be used for for multiple $b$ :s. So that's my question, why is $LU$ preferred?","I understand the whole -decomposition vs Gaussian elimination argument. The fact that you can isolate the computationally expensive elimination step and re-use the and matrices for style equations with different :s makes sense to me. But I can't seem to find a reason for why the and matrices are preferred over an matrix. It can also be used for for multiple :s. So that's my question, why is preferred?",LU L U Ax=b b L U A^{-1} b LU,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'lu-decomposition']"
71,Proof that SN (or Jordan-Chevalley) Decomposition is unique?,Proof that SN (or Jordan-Chevalley) Decomposition is unique?,,"Let $M$ be a matrix with entries in $\mathbb C$ . The SN (or Jordan-Chevalley) decomposition theorem states that we can find unique matrices $S$ and $N$ such that: $M=S+N$ $S$ is diagonalizable $N$ is nilpotent $SN=NS$ . I would like to prove the uniqueness part of this theorem, since everything else is immediate from the fact that all complex matrices can be put into Jordan normal form for some choice of basis. (If $M=AJA^{-1}$ , where $J$ is in Jordan normal form, write $J=J_S+J_N$ , where $J_S$ consists of the diagonal part of $J$ with zeroes elsewhere, and $J_N$ consists of the line above the diagonal with zeroes elsewhere. Then let $S=AJ_SA^{-1}$ and $N=AJ_NA^{-1}$ and direct calculation verifies that these meet the criteria above.) This post attempted to answer the same question, but unfortunately the proof is invalid because it assumes that the difference of two nilpotent matrices is another nilpotent matrix, which is not true (a counterexample is given in this post ).","Let be a matrix with entries in . The SN (or Jordan-Chevalley) decomposition theorem states that we can find unique matrices and such that: is diagonalizable is nilpotent . I would like to prove the uniqueness part of this theorem, since everything else is immediate from the fact that all complex matrices can be put into Jordan normal form for some choice of basis. (If , where is in Jordan normal form, write , where consists of the diagonal part of with zeroes elsewhere, and consists of the line above the diagonal with zeroes elsewhere. Then let and and direct calculation verifies that these meet the criteria above.) This post attempted to answer the same question, but unfortunately the proof is invalid because it assumes that the difference of two nilpotent matrices is another nilpotent matrix, which is not true (a counterexample is given in this post ).",M \mathbb C S N M=S+N S N SN=NS M=AJA^{-1} J J=J_S+J_N J_S J J_N S=AJ_SA^{-1} N=AJ_NA^{-1},"['linear-algebra', 'matrices', 'matrix-decomposition', 'jordan-normal-form']"
72,Is the set of all invertible 2x2 matrices a subspace of all 2x2 matrices?,Is the set of all invertible 2x2 matrices a subspace of all 2x2 matrices?,,"Is the set of all invertible 2 x 2 matrices a subspace of all 2 x 2 matrices? If not, can someone give me a counterexample to disprove this statement.","Is the set of all invertible 2 x 2 matrices a subspace of all 2 x 2 matrices? If not, can someone give me a counterexample to disprove this statement.",,"['linear-algebra', 'matrices']"
73,"The Sub Gradient and the Proximal Operator of the of $ {L}_{2, 1} $ Norm (Mixed Norm)",The Sub Gradient and the Proximal Operator of the of  Norm (Mixed Norm)," {L}_{2, 1} ","What would be the the Sub Gradient of $$ f \left( X \right) = {\left\| A X \right\|}_{2, 1} $$ Where $ X \in \mathbb{R}^{m \times n} $ , $ {A} \in \mathbb{R}^{k \times m} $ and $ {\left\| Y \right\|}_{2, 1} = \sum_{j} \sqrt{ \sum_{i} {Y}_{i,j}^{2} } $ . What would be the Prox of: $$ \operatorname{Prox}_{\lambda {\left\| \cdot \right\|}_{2,1}} \left( Y \right) = \arg \min_{X} \frac{1}{2} {\left\| X - Y \right\|}_{F}^{2}  + \lambda {\left\| X \right\|}_{2, 1}, \; X, Y \in {\mathbb{R}}^{m \times n} $$ Can either be generalized for $ {\left\| \cdot \right\|}_{q, p} $ ?","What would be the the Sub Gradient of Where , and . What would be the Prox of: Can either be generalized for ?"," f \left( X \right) = {\left\| A X \right\|}_{2, 1}   X \in \mathbb{R}^{m \times n}   {A} \in \mathbb{R}^{k \times m}   {\left\| Y \right\|}_{2, 1} = \sum_{j} \sqrt{ \sum_{i} {Y}_{i,j}^{2} }   \operatorname{Prox}_{\lambda {\left\| \cdot \right\|}_{2,1}} \left( Y \right) = \arg \min_{X} \frac{1}{2} {\left\| X - Y \right\|}_{F}^{2}  + \lambda {\left\| X \right\|}_{2, 1}, \; X, Y \in {\mathbb{R}}^{m \times n}   {\left\| \cdot \right\|}_{q, p} ","['linear-algebra', 'multivariable-calculus', 'convex-analysis', 'convex-optimization', 'proximal-operators']"
74,Proving magic squares determinant is a multiple of 3 when any numbers can be used,Proving magic squares determinant is a multiple of 3 when any numbers can be used,,"I am trying to prove that the determinant of a magic square, where all rows, columns and diagonal add to the same amount, is divisible by 3. I proved it for magic squares which have entries $1,\ldots, 9$ , but it turns out I need to show it for magic squares which can have any entries, e.g. \begin{pmatrix}  1 & 1 & 1 \\ 1 & 1 & 1 \\  1 & 1 & 1  \end{pmatrix} or \begin{pmatrix}  3 & 1 & 2 \\ 1 & 2 & 3 \\  2 & 3 & 1  \end{pmatrix} How can I do this? I tried working out the determinant using $a, b,\ldots, i$ as entries but could not find it. Thank you!","I am trying to prove that the determinant of a magic square, where all rows, columns and diagonal add to the same amount, is divisible by 3. I proved it for magic squares which have entries , but it turns out I need to show it for magic squares which can have any entries, e.g. or How can I do this? I tried working out the determinant using as entries but could not find it. Thank you!","1,\ldots, 9 \begin{pmatrix} 
1 & 1 & 1 \\
1 & 1 & 1 \\
 1 & 1 & 1 
\end{pmatrix} \begin{pmatrix} 
3 & 1 & 2 \\
1 & 2 & 3 \\
 2 & 3 & 1 
\end{pmatrix} a, b,\ldots, i","['linear-algebra', 'matrices', 'determinant', 'magic-square']"
75,"For matrices, under what conditions can we write $AB = BC$?","For matrices, under what conditions can we write ?",AB = BC,"If $A$ and $B$ are real matrices, when does there exist a matrix $C$ such that $AB = BC$ ? I understand that there is the case where $A$ and $B$ commute so $AB =BA$ , but is there a more general rule (ie. necessary and sufficient conditions)? Thanks!","If and are real matrices, when does there exist a matrix such that ? I understand that there is the case where and commute so , but is there a more general rule (ie. necessary and sufficient conditions)? Thanks!",A B C AB = BC A B AB =BA,"['linear-algebra', 'matrices', 'matrix-equations']"
76,Does there exist a $1-1$ ring homomorphism from $M_d(\mathbb{F})$ to $M_n(\mathbb{F})$ for $d <n$?,Does there exist a  ring homomorphism from  to  for ?,1-1 M_d(\mathbb{F}) M_n(\mathbb{F}) d <n,"Let $\mathbb{F}$ be a field and $d,n$ be positive integers with $d <n.$ Then does there exist a injective ring homomorphism from $M_d(\mathbb{F})$ to $M_n(\mathbb{F})$ ? (BTW A ring map sends $1$ to $1$ ) I failed to produce a $1-1$ ring map. This appears in the process of solving a field theory exercise from Dummit and Foote. For your ref. it is Problem number $19.(b)$ in sec. $13$ (Second Edition). Any help will be appreciated. Thanks. Edited Later:Prob. $19.(b).,$ Section $13.2,$ from Abstract Algebra by Dummit and Foote(Second Edition) is stated below. Let $K$ be an extension of $F$ of degree $n.$ Prove that $K$ is isomorphic to a subfield of the ring $M_n(F),$ so $M_n(F)$ contains an isomorphic copy of every extension of $F$ of degree $\leq n.$",Let be a field and be positive integers with Then does there exist a injective ring homomorphism from to ? (BTW A ring map sends to ) I failed to produce a ring map. This appears in the process of solving a field theory exercise from Dummit and Foote. For your ref. it is Problem number in sec. (Second Edition). Any help will be appreciated. Thanks. Edited Later:Prob. Section from Abstract Algebra by Dummit and Foote(Second Edition) is stated below. Let be an extension of of degree Prove that is isomorphic to a subfield of the ring so contains an isomorphic copy of every extension of of degree,"\mathbb{F} d,n d <n. M_d(\mathbb{F}) M_n(\mathbb{F}) 1 1 1-1 19.(b) 13 19.(b)., 13.2, K F n. K M_n(F), M_n(F) F \leq n.","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'field-theory']"
77,Computing an almost Vandermonde matrix,Computing an almost Vandermonde matrix,,I have this determinant which looks like a Vandermonde matrix $$D=\begin{vmatrix}1& a_1 & \cdots & a_1^{n-2}& a_1^n\\ 1& a_2 & \cdots & a_2^{n-2}& a_2^n\\ \vdots &\vdots & \ddots & \vdots & \vdots\\ 1& a_n & \cdots & a_n^{n-2}& a_n^n \end{vmatrix}$$ Using the software maxima I found that probably $D$ has this form $$D= \prod_{i<j}(a_j-a_i)(a_1+a_2+\cdots+ a_n)$$ but I couldn't prove it. Is my conjecture true and how can I prove it?,I have this determinant which looks like a Vandermonde matrix Using the software maxima I found that probably has this form but I couldn't prove it. Is my conjecture true and how can I prove it?,"D=\begin{vmatrix}1& a_1 & \cdots & a_1^{n-2}& a_1^n\\
1& a_2 & \cdots & a_2^{n-2}& a_2^n\\
\vdots &\vdots & \ddots & \vdots & \vdots\\
1& a_n & \cdots & a_n^{n-2}& a_n^n
\end{vmatrix} D D= \prod_{i<j}(a_j-a_i)(a_1+a_2+\cdots+ a_n)","['linear-algebra', 'determinant']"
78,Why is vertical distance the standard in least squares?,Why is vertical distance the standard in least squares?,,"When fitting a curve in $\mathbb R^2$ to data points in $\mathbb R^2$ ( example ), why is each point's vertical distance from the curve squared instead of its shortest (possibly diagonal) distance from the curve? Ignoring my poorly drawn curves, it seems obvious that is a worse curve-to-point fit than even though the red line is shorter in the first image, because you can draw the much shorter blue line instead (which I labeled b in the second image).  Minimizing $b^2$ seems much more important than minimizing $a^2$.","When fitting a curve in $\mathbb R^2$ to data points in $\mathbb R^2$ ( example ), why is each point's vertical distance from the curve squared instead of its shortest (possibly diagonal) distance from the curve? Ignoring my poorly drawn curves, it seems obvious that is a worse curve-to-point fit than even though the red line is shorter in the first image, because you can draw the much shorter blue line instead (which I labeled b in the second image).  Minimizing $b^2$ seems much more important than minimizing $a^2$.",,"['linear-algebra', 'geometry', 'statistics', 'intuition', 'least-squares']"
79,Orthogonality in different inner products,Orthogonality in different inner products,,Given a vector space V where two vectors v and u are orthogonal with respect to a given inner product. Will the same vectors be orthogonal with respect to all the other inner products?,Given a vector space V where two vectors v and u are orthogonal with respect to a given inner product. Will the same vectors be orthogonal with respect to all the other inner products?,,"['linear-algebra', 'vector-spaces', 'vectors', 'inner-products', 'orthogonality']"
80,Change of basis with rotation matrices,Change of basis with rotation matrices,,"Let's work on $\mathbb{R}^3$ for the time being and rotate our standard basis as  $$e_1 \to e_2 \\ e_2 \to -e_1 \\ e_3 \to e_3,$$ hence our transition matrix is $$P = \begin{bmatrix}     0       & -1 & 0 \\     1       & 0 & 0 \\     0       & 0 & 1 \end{bmatrix}$$ , and consider the point $a = (1, 0 ,0)$ in standard basis. Now since we have rotated our, so called, x-y plane $90^{\circ}$ degree counterclockwise wrt z-axis, our point wrt to the new coordinate system should $a'=(0,-1,0)$. However, when we multiply $a$ with the matrix $P$ (from right, i.e P$\cdot $a), we get $(0, 1, 0)$, which is a wrong result, so where is my mistake ? I mean I have checked my logic and calculation over and over again, but couldn't find the mistake that I'm doing.","Let's work on $\mathbb{R}^3$ for the time being and rotate our standard basis as  $$e_1 \to e_2 \\ e_2 \to -e_1 \\ e_3 \to e_3,$$ hence our transition matrix is $$P = \begin{bmatrix}     0       & -1 & 0 \\     1       & 0 & 0 \\     0       & 0 & 1 \end{bmatrix}$$ , and consider the point $a = (1, 0 ,0)$ in standard basis. Now since we have rotated our, so called, x-y plane $90^{\circ}$ degree counterclockwise wrt z-axis, our point wrt to the new coordinate system should $a'=(0,-1,0)$. However, when we multiply $a$ with the matrix $P$ (from right, i.e P$\cdot $a), we get $(0, 1, 0)$, which is a wrong result, so where is my mistake ? I mean I have checked my logic and calculation over and over again, but couldn't find the mistake that I'm doing.",,"['linear-algebra', 'rotations', 'change-of-basis']"
81,Any Examples Of Unbounded Linear Maps Between Normed Spaces Apart From The Differentiation Operator?,Any Examples Of Unbounded Linear Maps Between Normed Spaces Apart From The Differentiation Operator?,,"Let $X$ and $Y$ be normed spaces, either both real or both complex; let $T \colon X \to Y$ be a linear mapping. If there is a real number $r > 0$ such that  $$ \lVert T(x) \rVert_Y \leq r \lVert x \rVert_X \qquad \mbox{ for all } x \in X, $$ then $T$ is said to be a bounded linear operator . One example of this operator is the following: Let $X$ denote the normed space of all the real polynomials defined everywhere on the closed interval $[0, 1]$, with the maximum norm, and let $T \colon X \to X$ be the map $x \mapsto x^\prime$. Then this linear operator is unbounded. Can we find any other examples of unbounded linear operators? I know that every linear operator whose domain is a finite-dimensional normed space is bounded.","Let $X$ and $Y$ be normed spaces, either both real or both complex; let $T \colon X \to Y$ be a linear mapping. If there is a real number $r > 0$ such that  $$ \lVert T(x) \rVert_Y \leq r \lVert x \rVert_X \qquad \mbox{ for all } x \in X, $$ then $T$ is said to be a bounded linear operator . One example of this operator is the following: Let $X$ denote the normed space of all the real polynomials defined everywhere on the closed interval $[0, 1]$, with the maximum norm, and let $T \colon X \to X$ be the map $x \mapsto x^\prime$. Then this linear operator is unbounded. Can we find any other examples of unbounded linear operators? I know that every linear operator whose domain is a finite-dimensional normed space is bounded.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis', 'unbounded-operators']"
82,Let $V$ be a finite dimensional vector space over a field $F$ and $T$ a linear operator on $V$ such that $T^2 = I_V$.,Let  be a finite dimensional vector space over a field  and  a linear operator on  such that .,V F T V T^2 = I_V,"Let $V$ be a finite dimensional vector space over a field $\mathbb{F}$   and $T$ a linear operator on $V$ such that $T^2 = I_V$. If $\mathbb{F}  = \mathbb{R}$ or $\mathbb{C}$, show that $T$ is diagonalizable! I have no idea how to do this question, the best i did was to have take any basis $B$, we have $$[T^2]_B = [T]_B[T]_B = [I_V]_B$$ which is pretty meaningless. My answer sheet used a way in which i do not understand, it claimed that any $V$ in this situation will be a direct sum of the eigenspace of $1$ and $-1$. Anyone has a better proof or direct me to the right direction! THanks","Let $V$ be a finite dimensional vector space over a field $\mathbb{F}$   and $T$ a linear operator on $V$ such that $T^2 = I_V$. If $\mathbb{F}  = \mathbb{R}$ or $\mathbb{C}$, show that $T$ is diagonalizable! I have no idea how to do this question, the best i did was to have take any basis $B$, we have $$[T^2]_B = [T]_B[T]_B = [I_V]_B$$ which is pretty meaningless. My answer sheet used a way in which i do not understand, it claimed that any $V$ in this situation will be a direct sum of the eigenspace of $1$ and $-1$. Anyone has a better proof or direct me to the right direction! THanks",,"['linear-algebra', 'vector-spaces', 'diagonalization']"
83,Definition of a Subspace. When to prove that zero vector is in the set?,Definition of a Subspace. When to prove that zero vector is in the set?,,"I was going through this PDF and was reminded of an issue I always run into as outlined below. A subspace for $R^n$ is any collection S of vectors in $R^n$ such that The zero vector 0 is in S. If u and v are in S, then u+v is in S [closed under addition]. If u is in S and c is scalar, then cu is in S [closed under multiplication]. Property 1 is only needed to ensure that S is non-empty; for non-empty S, property 1 follows from property 3, 0a = 0. So, when do we know we have to check if the set is nonempty? I just don't understand when we have to check for zero vector(Property 1). I just check for all three each time.","I was going through this PDF and was reminded of an issue I always run into as outlined below. A subspace for $R^n$ is any collection S of vectors in $R^n$ such that The zero vector 0 is in S. If u and v are in S, then u+v is in S [closed under addition]. If u is in S and c is scalar, then cu is in S [closed under multiplication]. Property 1 is only needed to ensure that S is non-empty; for non-empty S, property 1 follows from property 3, 0a = 0. So, when do we know we have to check if the set is nonempty? I just don't understand when we have to check for zero vector(Property 1). I just check for all three each time.",,['linear-algebra']
84,Do there exist $n^2$ real coefficients such that all matrices in $\Bbb R^{n∗n}$ containing those coefficients are invertible?,Do there exist  real coefficients such that all matrices in  containing those coefficients are invertible?,n^2 \Bbb R^{n∗n},"Do there exist $n^2$ real coefficients such that all matrices in $\Bbb R^{n∗n}$ containing those coefficients (for any permutations) are invertible? Can we take these coefficients to be in $[1,2]$ ?","Do there exist $n^2$ real coefficients such that all matrices in $\Bbb R^{n∗n}$ containing those coefficients (for any permutations) are invertible? Can we take these coefficients to be in $[1,2]$ ?",,['linear-algebra']
85,Is a finite order endomorphism a rotation?,Is a finite order endomorphism a rotation?,,"Let $V$ be a real $2$-dimensional vector space, and $T\colon V\to V$ be an endomorphism such that $$ T^q = Id \qquad \textrm{and} \qquad T^j\not= Id\quad\textrm{if}\ 0<j<q, $$ where $T^0 = Id$ and $T^{j-1}\circ T = T^{j}$. Moreover, let $T$ be orientation-preserving. Can we say that there exist a basis $B$ for $V$ such that $T$ is the rotation of $\pm\frac{2\pi}{q}$, that is $$ [T]_B = \begin{bmatrix}\cos(\frac{2\pi}{q}) & \pm\sin(\frac{2\pi}{q})\\ \mp\sin(\frac{2\pi}{q}) & \cos(\frac{2\pi}{q})\end{bmatrix}? $$","Let $V$ be a real $2$-dimensional vector space, and $T\colon V\to V$ be an endomorphism such that $$ T^q = Id \qquad \textrm{and} \qquad T^j\not= Id\quad\textrm{if}\ 0<j<q, $$ where $T^0 = Id$ and $T^{j-1}\circ T = T^{j}$. Moreover, let $T$ be orientation-preserving. Can we say that there exist a basis $B$ for $V$ such that $T$ is the rotation of $\pm\frac{2\pi}{q}$, that is $$ [T]_B = \begin{bmatrix}\cos(\frac{2\pi}{q}) & \pm\sin(\frac{2\pi}{q})\\ \mp\sin(\frac{2\pi}{q}) & \cos(\frac{2\pi}{q})\end{bmatrix}? $$",,['linear-algebra']
86,Why $\bigwedge^{d-1}A=\bigwedge^{d-1}B \Rightarrow A= \pm B$,Why,\bigwedge^{d-1}A=\bigwedge^{d-1}B \Rightarrow A= \pm B,"Let $V,W$ be $d$-dimensional vector spaces, and let $A,B \in \text{Hom}(V,W)$. Consider the induced maps on the exterior algebras: $\bigwedge^{d-1}A,\bigwedge^{d-1}B :\Lambda_{d-1}(V) \to \Lambda_{d-1}(W)$. Suppose that  $\bigwedge^{d-1}A=\bigwedge^{d-1}B$, and that $A,B$ are invertible. I want to prove that $A=\pm B$. (Note that this implies $A=B$ in the case $d$ is even). The assumption implies $$\bigwedge^{d-1}(AB^{-1})=\text{Id}_{\Lambda_{d-1}(V)}.$$ Hence, the problem reduces to showing that for $S \in \text{GL}(V)$  ,$$\bigwedge^{d-1}S=\text{Id}_{\Lambda_{d-1}(V)} \Rightarrow S=\pm \text{Id}_V.$$ I can show this by introducing an inner product and orientation on $V$, but this seems ""unnatural"" to me. Is there a proof which avoids this?","Let $V,W$ be $d$-dimensional vector spaces, and let $A,B \in \text{Hom}(V,W)$. Consider the induced maps on the exterior algebras: $\bigwedge^{d-1}A,\bigwedge^{d-1}B :\Lambda_{d-1}(V) \to \Lambda_{d-1}(W)$. Suppose that  $\bigwedge^{d-1}A=\bigwedge^{d-1}B$, and that $A,B$ are invertible. I want to prove that $A=\pm B$. (Note that this implies $A=B$ in the case $d$ is even). The assumption implies $$\bigwedge^{d-1}(AB^{-1})=\text{Id}_{\Lambda_{d-1}(V)}.$$ Hence, the problem reduces to showing that for $S \in \text{GL}(V)$  ,$$\bigwedge^{d-1}S=\text{Id}_{\Lambda_{d-1}(V)} \Rightarrow S=\pm \text{Id}_V.$$ I can show this by introducing an inner product and orientation on $V$, but this seems ""unnatural"" to me. Is there a proof which avoids this?",,"['linear-algebra', 'differential-geometry', 'exterior-algebra']"
87,"Find 2 basis $M=\left\{v_1,v_2,v_3\right\}$ and $N=\left\{w_1,w_2,w_3\right\}$ such that $T_{MN(f)}$",Find 2 basis  and  such that,"M=\left\{v_1,v_2,v_3\right\} N=\left\{w_1,w_2,w_3\right\} T_{MN(f)}","$A=\begin{pmatrix} 1 & 3 & -1\\  -1 & 0 & -2\\  1 & 1 & 1 \end{pmatrix}$ is a real matrix and $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3, \text{ } f(x)= A \cdot x$ is a   linear mapping. Find two basis $M= \left\{v_1,v_2,v_3\right\}$ and $N= \left\{w_1,w_2,w_3\right\}$ of $\mathbb{R}^3$ such that $T_{MN}(f) = \begin{pmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  0 & 0 & 0 \end{pmatrix}$ I have absolutely no idea how to do this task, nor understand the notation $T_{MN}$. Maybe I could do it if I  would understand the notation. I hope some will explain me what it means / how this could be solved? If it matters, $Ker(f)=\left\{\begin{pmatrix} -2z\\  z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ and $Im(f)= span\left(\left\{\begin{pmatrix} 1\\  -1\\  1 \end{pmatrix},\begin{pmatrix} 0\\  -3\\  2 \end{pmatrix}\right\}\right)$ (This is no homework!) I will set a bounty on this question of 200 rep because I really want understand it.","$A=\begin{pmatrix} 1 & 3 & -1\\  -1 & 0 & -2\\  1 & 1 & 1 \end{pmatrix}$ is a real matrix and $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3, \text{ } f(x)= A \cdot x$ is a   linear mapping. Find two basis $M= \left\{v_1,v_2,v_3\right\}$ and $N= \left\{w_1,w_2,w_3\right\}$ of $\mathbb{R}^3$ such that $T_{MN}(f) = \begin{pmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  0 & 0 & 0 \end{pmatrix}$ I have absolutely no idea how to do this task, nor understand the notation $T_{MN}$. Maybe I could do it if I  would understand the notation. I hope some will explain me what it means / how this could be solved? If it matters, $Ker(f)=\left\{\begin{pmatrix} -2z\\  z\\  z \end{pmatrix} \mid z \in \mathbb{R}\right\}$ and $Im(f)= span\left(\left\{\begin{pmatrix} 1\\  -1\\  1 \end{pmatrix},\begin{pmatrix} 0\\  -3\\  2 \end{pmatrix}\right\}\right)$ (This is no homework!) I will set a bounty on this question of 200 rep because I really want understand it.",,"['linear-algebra', 'matrices', 'linear-transformations']"
88,The set of $n \times n$ matrices having trace equal to zero is a subspace of $M_{n \times n} \left(F\right)$,The set of  matrices having trace equal to zero is a subspace of,n \times n M_{n \times n} \left(F\right),"The set of $n \times n$ matrices having trace equal to zero is a subspace of $M_{n \times n} \left(F\right)$ I understand that the trace of a matrix is the sum of the diagonal entries. I also understand that an $n \times n$ matrix is defined as a square matrix. However, I do not understand why this means that the aforementioned matrix must therefore be subspace of a square matrix, $\mathrm M_{n \times n} \left(F\right)$. I would greatly appreciate it if someone could please take the time to make this concept clear to me.","The set of $n \times n$ matrices having trace equal to zero is a subspace of $M_{n \times n} \left(F\right)$ I understand that the trace of a matrix is the sum of the diagonal entries. I also understand that an $n \times n$ matrix is defined as a square matrix. However, I do not understand why this means that the aforementioned matrix must therefore be subspace of a square matrix, $\mathrm M_{n \times n} \left(F\right)$. I would greatly appreciate it if someone could please take the time to make this concept clear to me.",,"['linear-algebra', 'matrices', 'vector-spaces']"
89,Finding the determinant of a block diagonal matrix,Finding the determinant of a block diagonal matrix,,"We have the following square block matrix $X= \begin{bmatrix}     A       & 0  \\     0       & B  \end{bmatrix}$, where $A$ and $B$ are square. I want to get the determinant of this matrix (I know it equals $\det A \times \det B$). I've looked through the question catalog and found a lot of questions similar to this one, but the answers they received use techniques I am not familiar with - Leibniz' formula, techniques beyond the scope of an introductory linear algebra course, etc. So I want to know how to do this with Laplace expansion . I know how Laplace expansion works for regular matrices but I don't know how to use it here. Let's say we want to use Laplace expansion along the first row of $A$, then we get that $ \det A = \displaystyle \sum_{j=1}^n (-1)^{j+1}a_{1j}\det A_{1j} $, but how do we use this to obtain the result for the entire block matrix? Can we say that because the elements of $A$ are always in different rows and columns as compared to the elements of $B$, the entire matrix $B$ is always present in the minors of $A$? If so, the result seems a bit more intuitive to me, but I still don't know how to computationally obtain it.","We have the following square block matrix $X= \begin{bmatrix}     A       & 0  \\     0       & B  \end{bmatrix}$, where $A$ and $B$ are square. I want to get the determinant of this matrix (I know it equals $\det A \times \det B$). I've looked through the question catalog and found a lot of questions similar to this one, but the answers they received use techniques I am not familiar with - Leibniz' formula, techniques beyond the scope of an introductory linear algebra course, etc. So I want to know how to do this with Laplace expansion . I know how Laplace expansion works for regular matrices but I don't know how to use it here. Let's say we want to use Laplace expansion along the first row of $A$, then we get that $ \det A = \displaystyle \sum_{j=1}^n (-1)^{j+1}a_{1j}\det A_{1j} $, but how do we use this to obtain the result for the entire block matrix? Can we say that because the elements of $A$ are always in different rows and columns as compared to the elements of $B$, the entire matrix $B$ is always present in the minors of $A$? If so, the result seems a bit more intuitive to me, but I still don't know how to computationally obtain it.",,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
90,"I am confused by the statement ""the null space of A is a nontrivial""","I am confused by the statement ""the null space of A is a nontrivial""",,Correct me if I'm wrong but if a null space of a matrix A is nontrivial would it be correct to say that it is the opposite of the list of points in the Invertible Matrix Theorem? A is an invertible matrix A is row equivalent to the identity matrix A has n pivot columns The equation has only a trivial solution to ax=0 The columns of A are linearly independent The equation Ax=b has at least one solution for each b in Rn The column of A span Rn maps Rn onto Rn There is a nxn matrix C such that CA is equal to the identity matrix There is an nxn matrix D such that AD is equal to the identity matrix ....,Correct me if I'm wrong but if a null space of a matrix A is nontrivial would it be correct to say that it is the opposite of the list of points in the Invertible Matrix Theorem? A is an invertible matrix A is row equivalent to the identity matrix A has n pivot columns The equation has only a trivial solution to ax=0 The columns of A are linearly independent The equation Ax=b has at least one solution for each b in Rn The column of A span Rn maps Rn onto Rn There is a nxn matrix C such that CA is equal to the identity matrix There is an nxn matrix D such that AD is equal to the identity matrix ....,,['linear-algebra']
91,Find $x$ in $\mathbb{R}^2$ whose cordinate vector relative to the basis $B$,Find  in  whose cordinate vector relative to the basis,x \mathbb{R}^2 B,Consider the basis $B$ of $\mathbb{R}^2$ consisting of vectors $\begin{bmatrix}5\\-1\end{bmatrix}$ and $\begin{bmatrix}-7\\-4\end{bmatrix}$ . Find $x$ in $\mathbb{R}^2$ whose coordinate vector relative to the basis $B$ is $[x]_B = \begin{bmatrix}2\\5\end{bmatrix}$ $x = ?$ I put the matrices together obtaining a $3 \times 3$ matrix that I row reduced to get $\begin{bmatrix}1&0&-1\\0&1&-7/11\end{bmatrix}$ but then when I tried $x = \begin{bmatrix}-1\\-7/11\end{bmatrix}$ it said it was incorrect. I'm confused what i'm doing wrong,Consider the basis of consisting of vectors and . Find in whose coordinate vector relative to the basis is I put the matrices together obtaining a matrix that I row reduced to get but then when I tried it said it was incorrect. I'm confused what i'm doing wrong,B \mathbb{R}^2 \begin{bmatrix}5\\-1\end{bmatrix} \begin{bmatrix}-7\\-4\end{bmatrix} x \mathbb{R}^2 B [x]_B = \begin{bmatrix}2\\5\end{bmatrix} x = ? 3 \times 3 \begin{bmatrix}1&0&-1\\0&1&-7/11\end{bmatrix} x = \begin{bmatrix}-1\\-7/11\end{bmatrix},['linear-algebra']
92,Why Gaussian elimination on the columns changes the column space?,Why Gaussian elimination on the columns changes the column space?,,"This page on theorem 8.2 states that, Neither of the operations of the gaussian elimination changes the row space of an $m \times n$ matrix after applying the operation. It says later that this is only true about the row space and not the column space. I can clearly see how multiplying and adding two vectors does not change the row space. Let's assume any pair of two dimensional non parallel non zero vectors. These vectors span $R^2$. Thus it does not matter how we combine them linearly, they will still span $R^2$. The column space for any two non zero non parallel vectors can be thought of as being two dimensional vectors, spanning another two dimensional space again. Lets call this one $R^2_c$. Now here is my question, doing gaussian elimination on the columns of a matrix, will firstly, do nothing to the span of the column space, $R^2_c$, because that space still can be spanned with the two new column vectors, and secondly, it will result in two new row vectors in $R^2$ that can still span $R^2$. So it seems to me by doing linear combinations on the column space, neither the row nor the column space change. What am I doing wrong?","This page on theorem 8.2 states that, Neither of the operations of the gaussian elimination changes the row space of an $m \times n$ matrix after applying the operation. It says later that this is only true about the row space and not the column space. I can clearly see how multiplying and adding two vectors does not change the row space. Let's assume any pair of two dimensional non parallel non zero vectors. These vectors span $R^2$. Thus it does not matter how we combine them linearly, they will still span $R^2$. The column space for any two non zero non parallel vectors can be thought of as being two dimensional vectors, spanning another two dimensional space again. Lets call this one $R^2_c$. Now here is my question, doing gaussian elimination on the columns of a matrix, will firstly, do nothing to the span of the column space, $R^2_c$, because that space still can be spanned with the two new column vectors, and secondly, it will result in two new row vectors in $R^2$ that can still span $R^2$. So it seems to me by doing linear combinations on the column space, neither the row nor the column space change. What am I doing wrong?",,"['linear-algebra', 'vector-spaces', 'self-learning']"
93,Any $n \times n$ matrix $A$ can be written as $A = B + C$ with $B$ is symmetric and $C$ skew-symmetric.,Any  matrix  can be written as  with  is symmetric and  skew-symmetric.,n \times n A A = B + C B C,"How can I proof the following statement? Any $n \times n$ matrix $A$ can be written as a sum   $$   A = B + C $$   where $B$ is symmetric and $C$ is skew-symmetric. I tried to work out the properties of a matrix to be symmetric or skew-symmetric, but I could not prove this. Does someone know a way to prove it? Thank you. PS: The question Prove: Square Matrix Can Be Written As A Sum Of A Symmetric And Skew-Symmetric Matrices may be similiar, in fact gives a hint to a solution, but if someone does not mind in expose another way, our a track to reach to what is mentioned in the question of the aforementioned link.","How can I proof the following statement? Any $n \times n$ matrix $A$ can be written as a sum   $$   A = B + C $$   where $B$ is symmetric and $C$ is skew-symmetric. I tried to work out the properties of a matrix to be symmetric or skew-symmetric, but I could not prove this. Does someone know a way to prove it? Thank you. PS: The question Prove: Square Matrix Can Be Written As A Sum Of A Symmetric And Skew-Symmetric Matrices may be similiar, in fact gives a hint to a solution, but if someone does not mind in expose another way, our a track to reach to what is mentioned in the question of the aforementioned link.",,"['linear-algebra', 'proof-writing', 'alternative-proof', 'proof-explanation']"
94,What is an intuition behind permanent?,What is an intuition behind permanent?,,"I would like to know what is your intuition behind permanent of a matrix. For me, it looks like someone came and saw determinant, deleted permutation sign and voila, we have permanent and it counts the number of perfect matchings. But I am sure that is not how it was. :)","I would like to know what is your intuition behind permanent of a matrix. For me, it looks like someone came and saw determinant, deleted permutation sign and voila, we have permanent and it counts the number of perfect matchings. But I am sure that is not how it was. :)",,"['linear-algebra', 'abstract-algebra']"
95,shortest distance from point to hyperplane lagrange method,shortest distance from point to hyperplane lagrange method,,"I need to find the shortest distance, in D-dimensional Euclidean space ($\mathbb{R}^D$) from a point $\textbf{x}_0$ to a hyperplane $H: \textbf{w}^T \textbf{x} + b = 0$, using the method of Lagrange multipliers. The answer should be an expression in terms of $\textbf{w}, b$ and $\textbf{x}_0$. Note: I am aware that a few similar questions exist, such this one. I am creating a new question because I need to know how the derivation steps work in order to get a solution in a specific form. I know how to solve this problem in three dimensions, but not with linear algebra. Any help would be appreciated.","I need to find the shortest distance, in D-dimensional Euclidean space ($\mathbb{R}^D$) from a point $\textbf{x}_0$ to a hyperplane $H: \textbf{w}^T \textbf{x} + b = 0$, using the method of Lagrange multipliers. The answer should be an expression in terms of $\textbf{w}, b$ and $\textbf{x}_0$. Note: I am aware that a few similar questions exist, such this one. I am creating a new question because I need to know how the derivation steps work in order to get a solution in a specific form. I know how to solve this problem in three dimensions, but not with linear algebra. Any help would be appreciated.",,"['linear-algebra', 'lagrange-multiplier']"
96,Is the space R^N convex?,Is the space R^N convex?,,"My question is if $\mathbb{R}^{N}$ is convex. The definition I have for a set S to be convex is that if any convex combination of any two elements of S is in S, where a convex combination is defined as follows: Given vectors $x,y\in \mathbb{R}^{N}$, a vector $z\in\mathbb{R}^{N}$ is a convex combination of x an y if there exists a scalar $\alpha \in [0,1]$ such that $z=\alpha x + (1-\alpha)y$. Intuitively, I'd say that the answer is yes, my reasoning being that scalar multiplication and addition do not change the dimension of a vector, so if $x,y\in \mathbb{R}^{N}$ then $\alpha x + (1-\alpha)y$ must also be in $\mathbb{R}^{N}$, so all convex combinations of any two elements in $\mathbb{R}^{N}$ must also be in $\mathbb{R}^{N}$ and thus $\mathbb{R}^{N}$ is convex, though I am not sure that this would be sufficient to prove the concept.  Any tips?","My question is if $\mathbb{R}^{N}$ is convex. The definition I have for a set S to be convex is that if any convex combination of any two elements of S is in S, where a convex combination is defined as follows: Given vectors $x,y\in \mathbb{R}^{N}$, a vector $z\in\mathbb{R}^{N}$ is a convex combination of x an y if there exists a scalar $\alpha \in [0,1]$ such that $z=\alpha x + (1-\alpha)y$. Intuitively, I'd say that the answer is yes, my reasoning being that scalar multiplication and addition do not change the dimension of a vector, so if $x,y\in \mathbb{R}^{N}$ then $\alpha x + (1-\alpha)y$ must also be in $\mathbb{R}^{N}$, so all convex combinations of any two elements in $\mathbb{R}^{N}$ must also be in $\mathbb{R}^{N}$ and thus $\mathbb{R}^{N}$ is convex, though I am not sure that this would be sufficient to prove the concept.  Any tips?",,"['linear-algebra', 'convex-analysis']"
97,Evaluation of $5\times 5$ determinant,Evaluation of  determinant,5\times 5,"The following $5\times 5$ det. comes from a Russian book. I don't want to expand the det. rather than do some operations on it and extract the result. Prove: $$\begin{vmatrix} -1 &1  &1  &1  &x \\   1& -1 &1  &1  &y \\   1& 1 & -1 & 1 &z \\   1& 1 & 1 & -1 & u\\   x& y & z & u &0  \end{vmatrix}= -4 [x^2+y^2+z^2 +u^2 - 2(xy+zx+zu+yz+yu+\color{red}{x}u)]$$ I have tried many things e.g $R_1 \leftrightarrow R_5+ x R_1$ and of course cyclic ($R_2 \leftrightarrow R_5+ y R_2$ etc) and then adding the last row to the second etc. resulting in: $$\begin{vmatrix} 0 &y+x  &z+x  &u+x  &x^2 \\   y+x&0  &z+y  &u+y  &y^2 \\   x+z& z+y & 0 &z+u  & z^2\\   u+x& u+y &u+z  & 0 &u^2 \\  x &y  &z  &u  &0  \end{vmatrix}$$ but does not look that promising. On the other hand if I add the the first column to the others I do get a lot of zeros, since the first row for example takes the form $-1, 0, 0, 0, x+1$. This looks more promising. But in either case I get stuck. May I have some hints or answers? Edit: The red letters as suggested by Peter.","The following $5\times 5$ det. comes from a Russian book. I don't want to expand the det. rather than do some operations on it and extract the result. Prove: $$\begin{vmatrix} -1 &1  &1  &1  &x \\   1& -1 &1  &1  &y \\   1& 1 & -1 & 1 &z \\   1& 1 & 1 & -1 & u\\   x& y & z & u &0  \end{vmatrix}= -4 [x^2+y^2+z^2 +u^2 - 2(xy+zx+zu+yz+yu+\color{red}{x}u)]$$ I have tried many things e.g $R_1 \leftrightarrow R_5+ x R_1$ and of course cyclic ($R_2 \leftrightarrow R_5+ y R_2$ etc) and then adding the last row to the second etc. resulting in: $$\begin{vmatrix} 0 &y+x  &z+x  &u+x  &x^2 \\   y+x&0  &z+y  &u+y  &y^2 \\   x+z& z+y & 0 &z+u  & z^2\\   u+x& u+y &u+z  & 0 &u^2 \\  x &y  &z  &u  &0  \end{vmatrix}$$ but does not look that promising. On the other hand if I add the the first column to the others I do get a lot of zeros, since the first row for example takes the form $-1, 0, 0, 0, x+1$. This looks more promising. But in either case I get stuck. May I have some hints or answers? Edit: The red letters as suggested by Peter.",,"['linear-algebra', 'determinant']"
98,Prove that $\det(AA^T+I)\ge 1$,Prove that,\det(AA^T+I)\ge 1,"If $A$ is a matrix with real entries, prove that $$\det(AA^T+I)\ge 1.$$ I tried using the eigenvalues. One thing came into my mind: maybe $AA^T$ is positive definite (I don't know whether this is true or not). However, I prefer a solution that does not use properties of positive definite matrix. So I have 2 questions here: Is the statement ""$AA^T$ is positive definite"" true? Could you help me with a solution that does not use properties of positive definite matrix? Thanks a lot.","If $A$ is a matrix with real entries, prove that $$\det(AA^T+I)\ge 1.$$ I tried using the eigenvalues. One thing came into my mind: maybe $AA^T$ is positive definite (I don't know whether this is true or not). However, I prefer a solution that does not use properties of positive definite matrix. So I have 2 questions here: Is the statement ""$AA^T$ is positive definite"" true? Could you help me with a solution that does not use properties of positive definite matrix? Thanks a lot.",,"['linear-algebra', 'inequality', 'eigenvalues-eigenvectors', 'determinant']"
99,linear approximation with respect to L1 norm,linear approximation with respect to L1 norm,,"I am trying to solve this problem: Find the best $L^1$ linear approximation of $e^x$ on [0,1] i.e. minimize  $\int_0^1|e^x-\alpha-\beta x| dx$ any hints how to proceed","I am trying to solve this problem: Find the best $L^1$ linear approximation of $e^x$ on [0,1] i.e. minimize  $\int_0^1|e^x-\alpha-\beta x| dx$ any hints how to proceed",,"['linear-algebra', 'optimization', 'convex-optimization']"
