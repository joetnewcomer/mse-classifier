,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,total differential (thermodynamic relations),total differential (thermodynamic relations),,"Assume that: there are four quantities $S,T,P,V$; any two of them can be varied independently, but the other two are then determined ---(1) $dU=TdS-PdV$ ---(2) $F=U-ST$ ---(3) then, according to my book: $$dF=d(U-ST)=dU-TdS-SdT$$ but I can't get the above result. Here is my reasoning: Choose $S$ and $T$ as independent variables According to the definition of total differential: $$dF=\frac{\partial (U-ST)}{\partial U}dU+\frac{\partial (U-ST)}{\partial S}dS+\frac{\partial (U-ST)}{\partial T}dT$$ By (2), we know that the value of $U$ depends on $S$ and $T$, so: $$dF=\left(\frac{\partial U}{\partial U}-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\frac{\partial S}{\partial S}\right)dS+\left(\frac{\partial U}{\partial T}-S\frac{\partial T}{\partial T}\right)dT \\ =\left(1-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\right)dS+\left(\frac{\partial U}{\partial T}-S\right)dT $$ It seems that the author of my book assumed $U$ to be indepedent of $S$ and $T$ to get $\frac{\partial (ST)}{\partial U}=0$, $\frac{\partial U}{\partial S}=0$ and $\frac{\partial U}{\partial T}=0$, but doesn't it contradict assumption (2)? Would you explain to me how can the author obtain his result? Oh! I finally got it! There is no contradiction with assumption (2). I thought there was, just because I didn't pay attention to an important fact: when doing partial differentiation with respect to one variable, the other variables are regarded as constants! Also, as Hurkyl said below, the number of variables doesn't matter at all! Let me try to explain it: Case 1: we treat $F$ as a function of three variables, i.e. $F(U,S,T)$, then we got the following result as I did in my original post: $$dF=\left(1-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\right)dS+\left(\frac{\partial U}{\partial T}-S\right)dT$$ but when we are doing partial differentiation of $F(U,S,T)$ with respect to $U$, the other variables, $S$ and $T$, are regarded as constants, so $\frac{\partial (ST)}{\partial U}=0$, similarly, $\frac{\partial U}{\partial T}=0$ ($U$ and $S$ kept constant) and $\frac{\partial U}{\partial S}=0$ ($U$ and $T$ kept constant), so $$dF=(1-0)dU+(0-T)dS+(0-S)dT=dU-TdS-SdT$$ notice that it does not contradict with assumption (2), because the variables are regarded as unrelated just because we are doing partial differentiation, whereas in assumption (2), we are defining $dU$, not doing partial differentiation. Case 2: we treat $F$ as a function of any two variables, not necessarily $S$ and $T$. For example, we treat $F$ as $F(U,S)$, then $$dF=\left(\frac{\partial (U-ST)}{\partial U}\right)_S dU+\left(\frac{\partial (U-ST)}{\partial S}\right)_U dS$$ notice that in this case, $T$ can not be regarded as constant, because it is not a variable of $F$, but a function of $U$ and $S$, so: $$dF=\left(1-S\frac{\partial T}{\partial U}\right)dU+\left(-S\frac{\partial T}{\partial S}-T\right)dS \\  =dU-S\left(\frac{\partial T}{\partial U}dU+\frac{\partial T}{\partial S}dS\right)-TdS \\ =dU-SdT-TdS$$ we can get the same result if we choose any two variables. Case 3: we treat $F$ as a function of one variable, $F(t)$, where $U=U(t)$, $S=S(t)$, $T=T(t)$, in this case we can just use single variable differentiation: $$\frac{dF}{dt}=\frac{d(U-ST)}{dt}=\frac{dU}{dt}-\left(T\frac{dS}{dt}+S\frac{dT}{dt}\right)$$ multiply the above equation by $dt$, we can get $dF=dU-TdS-SdT$ So, we can get the same result no matter how we choose variables. And it doesn't matter whether the variables are independent or not! We can still have good approximation from the total differential $dF$ (please refer to Mary Boas's Mathematical Methods in the Physical Sciences , 3rd Edition, p.200 and p.201 problem #8 for an explanation of this). Anyway, thank all of you for sharing your ideas here.","Assume that: there are four quantities $S,T,P,V$; any two of them can be varied independently, but the other two are then determined ---(1) $dU=TdS-PdV$ ---(2) $F=U-ST$ ---(3) then, according to my book: $$dF=d(U-ST)=dU-TdS-SdT$$ but I can't get the above result. Here is my reasoning: Choose $S$ and $T$ as independent variables According to the definition of total differential: $$dF=\frac{\partial (U-ST)}{\partial U}dU+\frac{\partial (U-ST)}{\partial S}dS+\frac{\partial (U-ST)}{\partial T}dT$$ By (2), we know that the value of $U$ depends on $S$ and $T$, so: $$dF=\left(\frac{\partial U}{\partial U}-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\frac{\partial S}{\partial S}\right)dS+\left(\frac{\partial U}{\partial T}-S\frac{\partial T}{\partial T}\right)dT \\ =\left(1-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\right)dS+\left(\frac{\partial U}{\partial T}-S\right)dT $$ It seems that the author of my book assumed $U$ to be indepedent of $S$ and $T$ to get $\frac{\partial (ST)}{\partial U}=0$, $\frac{\partial U}{\partial S}=0$ and $\frac{\partial U}{\partial T}=0$, but doesn't it contradict assumption (2)? Would you explain to me how can the author obtain his result? Oh! I finally got it! There is no contradiction with assumption (2). I thought there was, just because I didn't pay attention to an important fact: when doing partial differentiation with respect to one variable, the other variables are regarded as constants! Also, as Hurkyl said below, the number of variables doesn't matter at all! Let me try to explain it: Case 1: we treat $F$ as a function of three variables, i.e. $F(U,S,T)$, then we got the following result as I did in my original post: $$dF=\left(1-\frac{\partial (ST)}{\partial U}\right)dU+\left(\frac{\partial U}{\partial S}-T\right)dS+\left(\frac{\partial U}{\partial T}-S\right)dT$$ but when we are doing partial differentiation of $F(U,S,T)$ with respect to $U$, the other variables, $S$ and $T$, are regarded as constants, so $\frac{\partial (ST)}{\partial U}=0$, similarly, $\frac{\partial U}{\partial T}=0$ ($U$ and $S$ kept constant) and $\frac{\partial U}{\partial S}=0$ ($U$ and $T$ kept constant), so $$dF=(1-0)dU+(0-T)dS+(0-S)dT=dU-TdS-SdT$$ notice that it does not contradict with assumption (2), because the variables are regarded as unrelated just because we are doing partial differentiation, whereas in assumption (2), we are defining $dU$, not doing partial differentiation. Case 2: we treat $F$ as a function of any two variables, not necessarily $S$ and $T$. For example, we treat $F$ as $F(U,S)$, then $$dF=\left(\frac{\partial (U-ST)}{\partial U}\right)_S dU+\left(\frac{\partial (U-ST)}{\partial S}\right)_U dS$$ notice that in this case, $T$ can not be regarded as constant, because it is not a variable of $F$, but a function of $U$ and $S$, so: $$dF=\left(1-S\frac{\partial T}{\partial U}\right)dU+\left(-S\frac{\partial T}{\partial S}-T\right)dS \\  =dU-S\left(\frac{\partial T}{\partial U}dU+\frac{\partial T}{\partial S}dS\right)-TdS \\ =dU-SdT-TdS$$ we can get the same result if we choose any two variables. Case 3: we treat $F$ as a function of one variable, $F(t)$, where $U=U(t)$, $S=S(t)$, $T=T(t)$, in this case we can just use single variable differentiation: $$\frac{dF}{dt}=\frac{d(U-ST)}{dt}=\frac{dU}{dt}-\left(T\frac{dS}{dt}+S\frac{dT}{dt}\right)$$ multiply the above equation by $dt$, we can get $dF=dU-TdS-SdT$ So, we can get the same result no matter how we choose variables. And it doesn't matter whether the variables are independent or not! We can still have good approximation from the total differential $dF$ (please refer to Mary Boas's Mathematical Methods in the Physical Sciences , 3rd Edition, p.200 and p.201 problem #8 for an explanation of this). Anyway, thank all of you for sharing your ideas here.",,"['multivariable-calculus', 'partial-derivative']"
1,Outward Flux of a Divergenceless Vector Field on an Ellipsoid,Outward Flux of a Divergenceless Vector Field on an Ellipsoid,,"tl;dr: How do you evaluate $\iint_S \mathbf{F} \cdot d\mathbf{S}$ where $\mathbf{F}(x,y,z) = \frac{1}{(x^2+y^2+z^2)^{3/2}}\langle x,y,z\rangle$ and $\mathbf{S}$ is the outward oriented surface given by $9x^2+4y^2+16z^2=144$? Long story: My multivariate calculus teacher recently gave our class the following problem: Compute the outward flux $\iint_S \mathbf{F} \cdot d\mathbf{S}$ where $$\mathbf{F}(x,y,z)=(y + \frac{x}{(x^2+y^2+z^2)^{3/2}})\mathbf{i} + (x + \frac{y}{(x^2+y^2+z^2)^{3/2}})\mathbf{j} + (z + \frac{z}{(x^2+y^2+z^2)^{3/2}})\mathbf{k} $$ and $S$ is the surface of the ellipsoid given by $9x^2+4y^2+16z^2=144$. The solution he gave us ran along the following lines: Let $\mathbf{F} = \mathbf{F_1} + \mathbf{F_2}$ where $$\mathbf{F_1} = \langle y,x,z\rangle;\; \mathbf{F_2} = \frac{1}{(x^2+y^2+z^2)^{3/2}}\langle x,y,z\rangle$$ which gives us $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_S \mathbf{F_1} \cdot d\mathbf{S} + \iint_S \mathbf{F_2} \cdot d\mathbf{S} $. By applying the divergence theorem and spherical parametrization, we can find that  $\iint_S \mathbf{F_1} \cdot d\mathbf{S} = 96\pi$. This made sense to me; I had no problem with understanding the transformation and evaluation of the integrals. After that, though, I got lost. According to my notes, he discussed how you can transpose $\iint_S \mathbf{F_2} \cdot d\mathbf{S}$ onto a unit sphere because $\mathbf{F_2}$ is divergenceless, and as a consequence it is equal to the surface integral of the unit sphere, yielding $\iint_S \mathbf{F_2} \cdot d\mathbf{S} = 4\pi$. This raised two questions: first, of course, was what the heck did my teacher just do? Second was: what's wrong with $\iint_S \mathbf{F_2} \cdot d\mathbf{S} = \iiint_E\nabla\cdot\mathbf{F_2}\,dV = \iiint_E{0}\,dV = 0$ (where $E$ is the region enclosed by surface $S$)? A friend said that it had to do with the fact that $\mathbf{F_2}$ is undefined at $(0,0,0)$, but I'm still confused. It would be great if someone could explain to me how to evaluate the outward flux of $\mathbf{F_2}$ on $S$. Thanks.","tl;dr: How do you evaluate $\iint_S \mathbf{F} \cdot d\mathbf{S}$ where $\mathbf{F}(x,y,z) = \frac{1}{(x^2+y^2+z^2)^{3/2}}\langle x,y,z\rangle$ and $\mathbf{S}$ is the outward oriented surface given by $9x^2+4y^2+16z^2=144$? Long story: My multivariate calculus teacher recently gave our class the following problem: Compute the outward flux $\iint_S \mathbf{F} \cdot d\mathbf{S}$ where $$\mathbf{F}(x,y,z)=(y + \frac{x}{(x^2+y^2+z^2)^{3/2}})\mathbf{i} + (x + \frac{y}{(x^2+y^2+z^2)^{3/2}})\mathbf{j} + (z + \frac{z}{(x^2+y^2+z^2)^{3/2}})\mathbf{k} $$ and $S$ is the surface of the ellipsoid given by $9x^2+4y^2+16z^2=144$. The solution he gave us ran along the following lines: Let $\mathbf{F} = \mathbf{F_1} + \mathbf{F_2}$ where $$\mathbf{F_1} = \langle y,x,z\rangle;\; \mathbf{F_2} = \frac{1}{(x^2+y^2+z^2)^{3/2}}\langle x,y,z\rangle$$ which gives us $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_S \mathbf{F_1} \cdot d\mathbf{S} + \iint_S \mathbf{F_2} \cdot d\mathbf{S} $. By applying the divergence theorem and spherical parametrization, we can find that  $\iint_S \mathbf{F_1} \cdot d\mathbf{S} = 96\pi$. This made sense to me; I had no problem with understanding the transformation and evaluation of the integrals. After that, though, I got lost. According to my notes, he discussed how you can transpose $\iint_S \mathbf{F_2} \cdot d\mathbf{S}$ onto a unit sphere because $\mathbf{F_2}$ is divergenceless, and as a consequence it is equal to the surface integral of the unit sphere, yielding $\iint_S \mathbf{F_2} \cdot d\mathbf{S} = 4\pi$. This raised two questions: first, of course, was what the heck did my teacher just do? Second was: what's wrong with $\iint_S \mathbf{F_2} \cdot d\mathbf{S} = \iiint_E\nabla\cdot\mathbf{F_2}\,dV = \iiint_E{0}\,dV = 0$ (where $E$ is the region enclosed by surface $S$)? A friend said that it had to do with the fact that $\mathbf{F_2}$ is undefined at $(0,0,0)$, but I'm still confused. It would be great if someone could explain to me how to evaluate the outward flux of $\mathbf{F_2}$ on $S$. Thanks.",,['multivariable-calculus']
2,What is the definition of $\textbf{d}s$ in path integrals?,What is the definition of  in path integrals?,\textbf{d}s,"I am aware that the length of a path $\gamma :[a,b] \rightarrow M$ is often defined as \begin{equation}\tag{1} L = \int_{\gamma}\textbf{d}s \end{equation} which can be rewritten in terms of the  metric tensor $g_{ij}$ as \begin{equation} \tag{2} L = \int_{\gamma}\sqrt{g_{ij}\textbf{d}x^i\textbf{d}x^j} \end{equation} For some coordinate system $(U, x^1,\ldots , x^m)$ for which $Im(\gamma)\subset U$ and $m = \dim(M)$ . However it is not clear to me how one transitions from (1) to (2). Is there an object $s$ whose differential gives the expression in $(2)$ . Also, I would assume if there is such an object, we are really talking about $|\textbf{d}s|$ . In both $(1)$ and $(2)$ , the interpretation is clear; in $(1)$ $\textbf{d}s$ is a differential length and in $(2)$ the metric tensor gives the metric in the tangent space at a given point on the manifold, and the whole expression is the length of a differential segment. So to clarify my question, is there an algebraic process which allows one to go from $(1)$ to $(2)$ or is it just the recognition that the quantity in $(2)$ is the quantity we want to integrate over a path. As a follow up question, why doesn't $(2)$ account for orientation?","I am aware that the length of a path is often defined as which can be rewritten in terms of the  metric tensor as For some coordinate system for which and . However it is not clear to me how one transitions from (1) to (2). Is there an object whose differential gives the expression in . Also, I would assume if there is such an object, we are really talking about . In both and , the interpretation is clear; in is a differential length and in the metric tensor gives the metric in the tangent space at a given point on the manifold, and the whole expression is the length of a differential segment. So to clarify my question, is there an algebraic process which allows one to go from to or is it just the recognition that the quantity in is the quantity we want to integrate over a path. As a follow up question, why doesn't account for orientation?","\gamma :[a,b] \rightarrow M \begin{equation}\tag{1} L = \int_{\gamma}\textbf{d}s \end{equation} g_{ij} \begin{equation} \tag{2} L = \int_{\gamma}\sqrt{g_{ij}\textbf{d}x^i\textbf{d}x^j} \end{equation} (U, x^1,\ldots , x^m) Im(\gamma)\subset U m = \dim(M) s (2) |\textbf{d}s| (1) (2) (1) \textbf{d}s (2) (1) (2) (2) (2)","['multivariable-calculus', 'manifolds']"
3,"Calculating the volume limited by $y=z^2$, $z=x^2$, $x=y^2$, $2y=z^2$, $2z=x^2$, $2x=y^2$","Calculating the volume limited by , , , , ,",y=z^2 z=x^2 x=y^2 2y=z^2 2z=x^2 2x=y^2,"I've tried calculating the volume limited by the surfaces $y=z^2$ , $z=x^2$ , $x=y^2$ , $2y=z^2$ , $2z=x^2$ , $2x=y^2$ . I didn't know how to begin so instead I tried solving for $y=z^2$ , $z=x^2$ , $x=y^2$ . So I checked where they intersect and they can only have values from $0$ to $1$ . I projected over the $xy$ -plane and got: $$\iint_D\left(\int_0^{x^2}\mathrm dz\right)\,\mathrm dx\mathrm dy=\int_0^1\left(\int_0^{\sqrt x}x^2\,\mathrm dy\right)\,\mathrm dx=\frac25.$$ I don't think that I got this right, not really confident in what I did. If what I did is correct, how should I proceed to resolve the original problem now?","I've tried calculating the volume limited by the surfaces , , , , , . I didn't know how to begin so instead I tried solving for , , . So I checked where they intersect and they can only have values from to . I projected over the -plane and got: I don't think that I got this right, not really confident in what I did. If what I did is correct, how should I proceed to resolve the original problem now?","y=z^2 z=x^2 x=y^2 2y=z^2 2z=x^2 2x=y^2 y=z^2 z=x^2 x=y^2 0 1 xy \iint_D\left(\int_0^{x^2}\mathrm dz\right)\,\mathrm dx\mathrm dy=\int_0^1\left(\int_0^{\sqrt x}x^2\,\mathrm dy\right)\,\mathrm dx=\frac25.",['multivariable-calculus']
4,Why does the Lagrange remainder work for multivariate functions?,Why does the Lagrange remainder work for multivariate functions?,,"I am familiar with the proof of the Lagrange remainder for single-variable functions (see Theorem $4$ ), but why does this concept carry over to multivariate functions? If $\ f: \mathbb R^k\to \mathbb R$ is $n+1$ times differentiable, then there exists a point $\mathbf c$ , where $c_i$ is between $a_i$ and $x_i$ , such that $$R_n(\mathbf x,\mathbf a)=\sum_{|\alpha|=n+1}\frac {D^\alpha f(\mathbf c)}{\alpha!}(\mathbf x-\mathbf a)^\alpha$$ My attempt: From Wikipedia , $$R_k(\mathbf x,\mathbf a)=\sum_{|\alpha|=k+1}\left(\begin{matrix} k+1 \\ \alpha\end{matrix} \right)\frac{(\mathbf x-\mathbf a)^\alpha }{k!} \int_0^1 (1-t)^k (D^\alpha f)(\mathbf a+t(\mathbf x-\mathbf a))\,dt\tag1$$ and using Theorem $2$ we get (?) $$\begin{align} R_k(\mathbf x,\mathbf a)&=\sum_{|\alpha|=k+1}\left(\begin{matrix} k+1 \\ \alpha\end{matrix} \right)\frac{(\mathbf x-\mathbf a)^\alpha }{(k+1)!} (D^\alpha f)(\mathbf c) \tag2\\ &=\sum_{|\alpha|=k+1}\frac {D^\alpha f(\mathbf c)}{\alpha!}(\mathbf x-\mathbf a)^\alpha \end{align}$$ However, I don't think that it is possible to go from $(1)$ to $(2)$ because, when taking $(D^\alpha f)(\mathbf a+t(\mathbf x-\mathbf a))$ out of the integral, the value that $t\in(0,1)$ takes may change for each summand.","I am familiar with the proof of the Lagrange remainder for single-variable functions (see Theorem ), but why does this concept carry over to multivariate functions? If is times differentiable, then there exists a point , where is between and , such that My attempt: From Wikipedia , and using Theorem we get (?) However, I don't think that it is possible to go from to because, when taking out of the integral, the value that takes may change for each summand.","4 \ f: \mathbb R^k\to \mathbb R n+1 \mathbf c c_i a_i x_i R_n(\mathbf x,\mathbf a)=\sum_{|\alpha|=n+1}\frac {D^\alpha f(\mathbf c)}{\alpha!}(\mathbf x-\mathbf a)^\alpha R_k(\mathbf x,\mathbf a)=\sum_{|\alpha|=k+1}\left(\begin{matrix} k+1 \\ \alpha\end{matrix} \right)\frac{(\mathbf x-\mathbf a)^\alpha }{k!}
\int_0^1 (1-t)^k (D^\alpha f)(\mathbf a+t(\mathbf x-\mathbf a))\,dt\tag1 2 \begin{align}
R_k(\mathbf x,\mathbf a)&=\sum_{|\alpha|=k+1}\left(\begin{matrix} k+1 \\ \alpha\end{matrix} \right)\frac{(\mathbf x-\mathbf a)^\alpha }{(k+1)!}
(D^\alpha f)(\mathbf c) \tag2\\
&=\sum_{|\alpha|=k+1}\frac {D^\alpha f(\mathbf c)}{\alpha!}(\mathbf x-\mathbf a)^\alpha
\end{align} (1) (2) (D^\alpha f)(\mathbf a+t(\mathbf x-\mathbf a)) t\in(0,1)","['multivariable-calculus', 'taylor-expansion']"
5,"Do directional derivatives require direction vectors to have unit length? If so, why?","Do directional derivatives require direction vectors to have unit length? If so, why?",,"I have just started studying directional derivative from the book Mathematical Analysis by T.M. Apostol. The directional derivative is the generalization of partial derivative. The partial derivative represents the rate of change of a function due to small change of one of the independent variables involved, whereas the directional derivative represents the rate of change of the function due to the small change of a point in it's domain along any arbitrary direction. The concept of directional derivative is as follows : Let $\mathbf{f} : S \longrightarrow \mathbb R^{m}$ be a vector valued function defined over $S \subset \mathbb R^{n}$ . Suppose we are to find out the rate of change of $\mathbf{f}$ when we move from a point $\mathbf{c}$ of $S$ to a nearby point $\mathbf{c} + \mathbf{u}$ along a line segment. Since each point of the line can be taken as $\mathbf{c} + h\mathbf{u}$ for some $h \in \mathbb R$ , we can take $h$ sufficiently small so that $\mathbf{c} + h\mathbf{u}$ is in $S$ . Then the quantity $$\lim_{h \rightarrow 0} \frac {\mathbf{f}(\mathbf{c} + h\mathbf{u}) - \mathbf{f}(\mathbf{c})} {h}$$ if it exists is called the directional derivative of $\mathbf{f}$ at $\mathbf{c} \in S$ in the direction of $\mathbf{u}$ . I am having some difficulty here. According to my teacher's lecture notes, $||\mathbf{u}|| = 1$ . For this reason the directional derivative of a given function $\mathbf{f}$ at a point along some certain direction may differ. Is there any significance of considering $||\mathbf{u}|| = 1$ ? If the answer to my question is affirmative, then why?","I have just started studying directional derivative from the book Mathematical Analysis by T.M. Apostol. The directional derivative is the generalization of partial derivative. The partial derivative represents the rate of change of a function due to small change of one of the independent variables involved, whereas the directional derivative represents the rate of change of the function due to the small change of a point in it's domain along any arbitrary direction. The concept of directional derivative is as follows : Let be a vector valued function defined over . Suppose we are to find out the rate of change of when we move from a point of to a nearby point along a line segment. Since each point of the line can be taken as for some , we can take sufficiently small so that is in . Then the quantity if it exists is called the directional derivative of at in the direction of . I am having some difficulty here. According to my teacher's lecture notes, . For this reason the directional derivative of a given function at a point along some certain direction may differ. Is there any significance of considering ? If the answer to my question is affirmative, then why?",\mathbf{f} : S \longrightarrow \mathbb R^{m} S \subset \mathbb R^{n} \mathbf{f} \mathbf{c} S \mathbf{c} + \mathbf{u} \mathbf{c} + h\mathbf{u} h \in \mathbb R h \mathbf{c} + h\mathbf{u} S \lim_{h \rightarrow 0} \frac {\mathbf{f}(\mathbf{c} + h\mathbf{u}) - \mathbf{f}(\mathbf{c})} {h} \mathbf{f} \mathbf{c} \in S \mathbf{u} ||\mathbf{u}|| = 1 \mathbf{f} ||\mathbf{u}|| = 1,"['multivariable-calculus', 'derivatives']"
6,Intuitively what is the second directional derivative?,Intuitively what is the second directional derivative?,,"I'm thinking that the second directional derivative, if both dd's are evaluated in the same direction, will just give you the concavity (the second scalar derivative) in that direction.  Is that right? But what if the second directional derivative is evaluated in a different direction?  As in $D_{\vec v}D_{\vec u} f(\vec x)$ where $\vec v\ne \vec u$.  Then what would this thing mean?  Does it still have to do with concavity?","I'm thinking that the second directional derivative, if both dd's are evaluated in the same direction, will just give you the concavity (the second scalar derivative) in that direction.  Is that right? But what if the second directional derivative is evaluated in a different direction?  As in $D_{\vec v}D_{\vec u} f(\vec x)$ where $\vec v\ne \vec u$.  Then what would this thing mean?  Does it still have to do with concavity?",,['multivariable-calculus']
7,Proof that a log-of-sum-of-exponentials is a convex function,Proof that a log-of-sum-of-exponentials is a convex function,,"It's well known in statistical mechanics that the following is a convex function of the vector $\theta$: $$ A(\theta) = \log \left( \sum_{i=1}^\infty e^{\theta \cdot f(i)}   \right) $$ where $f(i)$ is a vector function of $i$. In the context of statistical mechanics, $A$ is known as the log partition function. However, all the proofs of the convexity property that I know of rely on the interpretation of this function in terms of probability theory. One defines a probability distribution given by $p_i = e^{\theta\cdot f(i)-A(\theta)}$ and shows (for example) that the partial derivatives of $A$ form a covariance matrix, which implies that its Hessian must be positive definite. For the sake of enhancing my understanding, I would like a more direct proof, one that proceeds directly from the mathematical form of $\psi$ as defined above, without considering a probability distribution. Is there a straightforward way to see that $A$ as defined above is a convex function of $\theta$, independently of its interpretation as a partition function in statistical mechanics?","It's well known in statistical mechanics that the following is a convex function of the vector $\theta$: $$ A(\theta) = \log \left( \sum_{i=1}^\infty e^{\theta \cdot f(i)}   \right) $$ where $f(i)$ is a vector function of $i$. In the context of statistical mechanics, $A$ is known as the log partition function. However, all the proofs of the convexity property that I know of rely on the interpretation of this function in terms of probability theory. One defines a probability distribution given by $p_i = e^{\theta\cdot f(i)-A(\theta)}$ and shows (for example) that the partial derivatives of $A$ form a covariance matrix, which implies that its Hessian must be positive definite. For the sake of enhancing my understanding, I would like a more direct proof, one that proceeds directly from the mathematical form of $\psi$ as defined above, without considering a probability distribution. Is there a straightforward way to see that $A$ as defined above is a convex function of $\theta$, independently of its interpretation as a partition function in statistical mechanics?",,"['multivariable-calculus', 'convex-analysis', 'statistical-mechanics']"
8,Is it ever easier to show differentiability than continuity?,Is it ever easier to show differentiability than continuity?,,"I'm TAing a course right now in multivariable calculus and in the lecture notes the professor gave the students the theorem stating that differentiability implies continuity, as well as another theorem that says a function $F(x,y)$ is differentiable at a point $(a,b)$ if $\partial_x F$ and $\partial_y F$ exist and are continuous at $(a,b)$. I was trying to think of an example where the continuity of $F(x,y)$ is not obvious, but the continuity of the partial derivatives is more apparent and came up with nothing.","I'm TAing a course right now in multivariable calculus and in the lecture notes the professor gave the students the theorem stating that differentiability implies continuity, as well as another theorem that says a function $F(x,y)$ is differentiable at a point $(a,b)$ if $\partial_x F$ and $\partial_y F$ exist and are continuous at $(a,b)$. I was trying to think of an example where the continuity of $F(x,y)$ is not obvious, but the continuity of the partial derivatives is more apparent and came up with nothing.",,['multivariable-calculus']
9,Differential forms and determinants,Differential forms and determinants,,"2-forms are defined as $du^{j} \wedge du^{k}(v,w) = v^{j}w^{k}-v^{k}w^{j} = \begin{vmatrix} du^{j}(v) & du^{j}(w)  \\ du^{k}(v) & du^{k}(w) \end{vmatrix}$ But what if I have two concret 1-forms in $R^{3}$? For example $(2dx-3dy+dz)\wedge (dx+2dy-dz)$ which gives $(2dx-3dy+dz)\wedge (dx+2dy-dz)=-7dy \wedge dx +3dz \wedge dx - dy \wedge dz= 7 dx \wedge dy + 3 dz \wedge dx + dy \wedge dz$ I know this is the same as the vector product between $(2,-3,1)^{T}$ and $(1,2,-1)^{T}$. What is the relationship with the determinant? Because when I calculate the wedge product between two 1-forms in $R^{2}$ then I get the value of the determinant $(2dx+4dx)\wedge (3dx+9dy) = -18 dx\wedge dy +12 dx \wedge dy = 6 dx \wedge dy$, but for 1-forms in $R^{3}$ I get the vector product. And is  the interpretation right that parts of the area spanned by the 2-form above (the vector product) is that 7 get's projected onto the $xy$ plane, 3 onto $zx$ and  1 onto $yz$? Or is it in this case another coordinate system with planes $dxdy$, $dzdx$ and $dydz$? And since all the differential forms are functions of vectors, what happens when they come into the picture? Because doesn't $7 dx \wedge dy + 3 dz \wedge dx + dy \wedge dz$ looks like this? $7 \begin{vmatrix} dx(v) & dx(w)  \\ dy(v) & dy(w) \end{vmatrix} + 3 \begin{vmatrix} dz(v) & dz(w)  \\ dx(v) & dx(w) \end{vmatrix} + \begin{vmatrix} dy(v) & dy(w)  \\ dz(v) & dz(w) \end{vmatrix}$ For some vectors $v,w \in R^{3}$?","2-forms are defined as $du^{j} \wedge du^{k}(v,w) = v^{j}w^{k}-v^{k}w^{j} = \begin{vmatrix} du^{j}(v) & du^{j}(w)  \\ du^{k}(v) & du^{k}(w) \end{vmatrix}$ But what if I have two concret 1-forms in $R^{3}$? For example $(2dx-3dy+dz)\wedge (dx+2dy-dz)$ which gives $(2dx-3dy+dz)\wedge (dx+2dy-dz)=-7dy \wedge dx +3dz \wedge dx - dy \wedge dz= 7 dx \wedge dy + 3 dz \wedge dx + dy \wedge dz$ I know this is the same as the vector product between $(2,-3,1)^{T}$ and $(1,2,-1)^{T}$. What is the relationship with the determinant? Because when I calculate the wedge product between two 1-forms in $R^{2}$ then I get the value of the determinant $(2dx+4dx)\wedge (3dx+9dy) = -18 dx\wedge dy +12 dx \wedge dy = 6 dx \wedge dy$, but for 1-forms in $R^{3}$ I get the vector product. And is  the interpretation right that parts of the area spanned by the 2-form above (the vector product) is that 7 get's projected onto the $xy$ plane, 3 onto $zx$ and  1 onto $yz$? Or is it in this case another coordinate system with planes $dxdy$, $dzdx$ and $dydz$? And since all the differential forms are functions of vectors, what happens when they come into the picture? Because doesn't $7 dx \wedge dy + 3 dz \wedge dx + dy \wedge dz$ looks like this? $7 \begin{vmatrix} dx(v) & dx(w)  \\ dy(v) & dy(w) \end{vmatrix} + 3 \begin{vmatrix} dz(v) & dz(w)  \\ dx(v) & dx(w) \end{vmatrix} + \begin{vmatrix} dy(v) & dy(w)  \\ dz(v) & dz(w) \end{vmatrix}$ For some vectors $v,w \in R^{3}$?",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
10,Very simple partial differential equation,Very simple partial differential equation,,"I am solving $$ \frac {\partial f}{\partial x} = \frac y{x^2 + y^2} \\ \frac {\partial f}{\partial y} = \frac {-x}{x^2 +y^2} $$ As $y$ was held constant when the partial derivative with respect to $x$ was obtained, we hold  $y$ constant and integrate the first equation. Then we obtain $$f(x,y) = \arctan \frac xy + g(y)$$ We can now differentiate with respect to $y$ and obtain $$\frac {\partial f}{\partial y} = \frac {-x}{x^2 + y^2} + g'(y)$$ If we equate this to the second initial equation, we can conclude $g(y) = C$. A similar approach beginning with the second initial equation we get $f(x,y) = - \arctan \frac yx + K$. So we basically have $$ f(x,y) = \arctan \frac xy + C \\ f(x,y) = - \arctan \frac yx + K $$ But now what? We got an ambiguous result, so which one is it going to be?","I am solving $$ \frac {\partial f}{\partial x} = \frac y{x^2 + y^2} \\ \frac {\partial f}{\partial y} = \frac {-x}{x^2 +y^2} $$ As $y$ was held constant when the partial derivative with respect to $x$ was obtained, we hold  $y$ constant and integrate the first equation. Then we obtain $$f(x,y) = \arctan \frac xy + g(y)$$ We can now differentiate with respect to $y$ and obtain $$\frac {\partial f}{\partial y} = \frac {-x}{x^2 + y^2} + g'(y)$$ If we equate this to the second initial equation, we can conclude $g(y) = C$. A similar approach beginning with the second initial equation we get $f(x,y) = - \arctan \frac yx + K$. So we basically have $$ f(x,y) = \arctan \frac xy + C \\ f(x,y) = - \arctan \frac yx + K $$ But now what? We got an ambiguous result, so which one is it going to be?",,"['multivariable-calculus', 'partial-differential-equations', 'systems-of-equations']"
11,real meaning of divergence and its mathematical intuition,real meaning of divergence and its mathematical intuition,,"how does divergence which means sink or source equal to ∂Fx/∂x+∂Fy/∂y +∂Fz/∂z.I have been thinking it for a long time and i think ""divergence tells us how fast the vector increases when we move apart from the vector source""","how does divergence which means sink or source equal to ∂Fx/∂x+∂Fy/∂y +∂Fz/∂z.I have been thinking it for a long time and i think ""divergence tells us how fast the vector increases when we move apart from the vector source""",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
12,"Lagrange multiplier method, find maximum of $e^{-x}\cdot (x^2-3)\cdot (y^2-3)$ on a circle","Lagrange multiplier method, find maximum of  on a circle",e^{-x}\cdot (x^2-3)\cdot (y^2-3),"I attempted to design an exercise for my engineer students and couldn't solve it myself. Maybe here are some experts in calculus who have some better tricks than I do: The exercise would be to find the maxima of $e^{-x}(x^2-3)(y^2-3)$ on the circle $x^2+(y-1)^2=4$. Now using the Lagrange multiplier method this amounts to solving the following system of equations: $$\begin{align*} e^{-x}(y^2-3)(-x^2+2x+3)+\lambda\cdot 2x&=0\\ e^{-x}(x^2-3)(2y)+\lambda\cdot 2(y-1)&=0\\ x^2+(y-1)^2&=4 \end{align*}$$ I did not succeed to find the solutions and also my standard online calculor didn't. Now I thought, this is partly because of the $e^{-x}$-term, so it would be good if one could eliminate it. Noting that $x^2+(y-1)^2-4=0$ iff $e^{-x}\cdot (x^2+(y-1)^2-4)=0$ we can instead use Lagrange multipliers on this condition. This amounts to solving the easier system: $$\begin{align*} (y^2-3)(-x^2+2x+3)+\lambda(-x^2+2x-(y-1)^2+4)&=0\\ (x^2-3)y+\lambda(y-1)&=0\\ x^2+(y-1)^2-4&=0 \end{align*}$$ In fact I could still not solve it, but the computer could (but the form is not very nice). So the question is now two-fold: Do you have any ideas how to solve either of the systems? If not, do you have any ideas how to tweak it a little bit (preferrable on the circle condition side) so that it becomes easier to solve?","I attempted to design an exercise for my engineer students and couldn't solve it myself. Maybe here are some experts in calculus who have some better tricks than I do: The exercise would be to find the maxima of $e^{-x}(x^2-3)(y^2-3)$ on the circle $x^2+(y-1)^2=4$. Now using the Lagrange multiplier method this amounts to solving the following system of equations: $$\begin{align*} e^{-x}(y^2-3)(-x^2+2x+3)+\lambda\cdot 2x&=0\\ e^{-x}(x^2-3)(2y)+\lambda\cdot 2(y-1)&=0\\ x^2+(y-1)^2&=4 \end{align*}$$ I did not succeed to find the solutions and also my standard online calculor didn't. Now I thought, this is partly because of the $e^{-x}$-term, so it would be good if one could eliminate it. Noting that $x^2+(y-1)^2-4=0$ iff $e^{-x}\cdot (x^2+(y-1)^2-4)=0$ we can instead use Lagrange multipliers on this condition. This amounts to solving the easier system: $$\begin{align*} (y^2-3)(-x^2+2x+3)+\lambda(-x^2+2x-(y-1)^2+4)&=0\\ (x^2-3)y+\lambda(y-1)&=0\\ x^2+(y-1)^2-4&=0 \end{align*}$$ In fact I could still not solve it, but the computer could (but the form is not very nice). So the question is now two-fold: Do you have any ideas how to solve either of the systems? If not, do you have any ideas how to tweak it a little bit (preferrable on the circle condition side) so that it becomes easier to solve?",,"['multivariable-calculus', 'lagrange-multiplier']"
13,Can a spiral have its centroid at the origin?,Can a spiral have its centroid at the origin?,,"A spiral is a curve $\gamma$ with the polar equation $r=f(\theta)$ where $f$ is a continuous positive strictly monotone function on some interval $[a, b]$, $-\infty<a<b<\infty$. Best known examples are the logarithmic spiral and the Archimedean spiral . Problem : Find a spiral whose centroid is the origin of the coordinate system. Progress so far : We want $$\int_\gamma x\,ds = \int_\gamma y \,ds = 0 \tag1$$ Note that $x = f(\theta)\cos\theta$, $y = f(\theta)\sin\theta$, and $ds = \sqrt{(f'(\theta))^2 + f(\theta)^2}\,d\theta$. Thus, we need the function  $$g(\theta) = f(\theta) \sqrt{(f'(\theta))^2 + f(\theta)^2} $$  to be orthogonal to both $\cos \theta$ and $\sin\theta$ on the interval $[a, b]$, meaning $$\int_a^b g(\theta)\cos\theta\,d\theta = \int_a^b g(\theta)\sin\theta\,d\theta  = 0\tag2$$ A natural way to satisfy (2) is to take $[a, b] = [0, 2\pi]$ and $g$ to be constant (say $g\equiv 1$ as scaling does not matter). However this fails, because solving the equation $g\equiv 1$ for $f$ (as an autonomous ODE) yields $f(\theta) = \sqrt{\sin 2\theta}$ (up to a shift), which is not even defined, let alone monotone, on any interval of length $2\pi$. Note : It is not required for $[a, b]$ to have length $2\pi$ or a multiple of $2\pi$; it can be any nontrivial finite interval.","A spiral is a curve $\gamma$ with the polar equation $r=f(\theta)$ where $f$ is a continuous positive strictly monotone function on some interval $[a, b]$, $-\infty<a<b<\infty$. Best known examples are the logarithmic spiral and the Archimedean spiral . Problem : Find a spiral whose centroid is the origin of the coordinate system. Progress so far : We want $$\int_\gamma x\,ds = \int_\gamma y \,ds = 0 \tag1$$ Note that $x = f(\theta)\cos\theta$, $y = f(\theta)\sin\theta$, and $ds = \sqrt{(f'(\theta))^2 + f(\theta)^2}\,d\theta$. Thus, we need the function  $$g(\theta) = f(\theta) \sqrt{(f'(\theta))^2 + f(\theta)^2} $$  to be orthogonal to both $\cos \theta$ and $\sin\theta$ on the interval $[a, b]$, meaning $$\int_a^b g(\theta)\cos\theta\,d\theta = \int_a^b g(\theta)\sin\theta\,d\theta  = 0\tag2$$ A natural way to satisfy (2) is to take $[a, b] = [0, 2\pi]$ and $g$ to be constant (say $g\equiv 1$ as scaling does not matter). However this fails, because solving the equation $g\equiv 1$ for $f$ (as an autonomous ODE) yields $f(\theta) = \sqrt{\sin 2\theta}$ (up to a shift), which is not even defined, let alone monotone, on any interval of length $2\pi$. Note : It is not required for $[a, b]$ to have length $2\pi$ or a multiple of $2\pi$; it can be any nontrivial finite interval.",,"['multivariable-calculus', 'polar-coordinates']"
14,Applying Green's Theorem,Applying Green's Theorem,,"I'm studying for a grad-school preliminary exam, and came across this problem which I am unable to solve. Let $C$ be a closed curve in the plane $ax + by + cz = 0$ (where $a,b,c \in \mathbb{R}$ are not all zero $0$), enclosing a region with area $S$.  Evaluate  $$ I:= \oint\limits_C \left| \begin{array}{ccc}  dx & dy & dz \\ a  & b  & c  \\ x  & y  & z \end{array} \right| $$ where the integral along $C$ is counterclockwise relative to the normal direction $(a,b,c)$ to the plane. Expanding the determinant yields $$ I = \oint\limits_C (bz - cy)dx + (cx - az)dy + (ay - bx)dz. $$ If we call $\Omega$ the region bounded by $C$, an application of Green's Theorem (or Stokes Theorem, if you prefer) yields $$ I = 2\iint\limits_\Omega a dydz + bdzdx + c dxdy. $$ From here, I'm not sure what to do; I suspect that it's a simple solution and that I've just forgotten my vector calculus.  Anyway, any help is appreciated. EDIT: Substitute in $z = \frac{1}{c}\left(-ax - by \right)$ and then applying Green's Theorem doesn't bring me any closer.  Making such a substitution yields $$ I = \frac{a^2 + b^2 + c^2}{c} \iint_\Omega dxdy $$ If $c \neq 0$ as required to make such a substitution, it is not the case that $\iint dxdy = S$. SECOND EDIT:  I've solved the problem, making use of the following version of Stokes' theorem: $$ \iint_\Omega (\nabla \times \mathbf{F} ) \cdot \mathbf{n} dS = \oint_{\partial \Omega} \mathbf{F} \cdot d\mathbf{r}. $$ The above can be written as \begin{align*} I &= \oint\limits_C (bz - cy,cx - az,ay - bx)\cdot d\mathbf{r} \\ &= \iint_\Omega (\nabla \times (bz - cy,cx - az,ay - bx)) \cdot \left(\frac{1}{\sqrt{a^2 + b^2 + c^2}}(a,b,c)\right) dS  \\ &=\iint_\Omega (2a,2b,2c) \cdot \left(\frac{1}{\sqrt{a^2 + b^2 + c^2}}(a,b,c)\right) dS  \\ &=  2 \sqrt{a^2 + b^2 + c^2} \iint_\Omega dS  \\ &= 2\sqrt{a^2 + b^2 + c^2} (\text{Area of } \Omega) \end{align*}","I'm studying for a grad-school preliminary exam, and came across this problem which I am unable to solve. Let $C$ be a closed curve in the plane $ax + by + cz = 0$ (where $a,b,c \in \mathbb{R}$ are not all zero $0$), enclosing a region with area $S$.  Evaluate  $$ I:= \oint\limits_C \left| \begin{array}{ccc}  dx & dy & dz \\ a  & b  & c  \\ x  & y  & z \end{array} \right| $$ where the integral along $C$ is counterclockwise relative to the normal direction $(a,b,c)$ to the plane. Expanding the determinant yields $$ I = \oint\limits_C (bz - cy)dx + (cx - az)dy + (ay - bx)dz. $$ If we call $\Omega$ the region bounded by $C$, an application of Green's Theorem (or Stokes Theorem, if you prefer) yields $$ I = 2\iint\limits_\Omega a dydz + bdzdx + c dxdy. $$ From here, I'm not sure what to do; I suspect that it's a simple solution and that I've just forgotten my vector calculus.  Anyway, any help is appreciated. EDIT: Substitute in $z = \frac{1}{c}\left(-ax - by \right)$ and then applying Green's Theorem doesn't bring me any closer.  Making such a substitution yields $$ I = \frac{a^2 + b^2 + c^2}{c} \iint_\Omega dxdy $$ If $c \neq 0$ as required to make such a substitution, it is not the case that $\iint dxdy = S$. SECOND EDIT:  I've solved the problem, making use of the following version of Stokes' theorem: $$ \iint_\Omega (\nabla \times \mathbf{F} ) \cdot \mathbf{n} dS = \oint_{\partial \Omega} \mathbf{F} \cdot d\mathbf{r}. $$ The above can be written as \begin{align*} I &= \oint\limits_C (bz - cy,cx - az,ay - bx)\cdot d\mathbf{r} \\ &= \iint_\Omega (\nabla \times (bz - cy,cx - az,ay - bx)) \cdot \left(\frac{1}{\sqrt{a^2 + b^2 + c^2}}(a,b,c)\right) dS  \\ &=\iint_\Omega (2a,2b,2c) \cdot \left(\frac{1}{\sqrt{a^2 + b^2 + c^2}}(a,b,c)\right) dS  \\ &=  2 \sqrt{a^2 + b^2 + c^2} \iint_\Omega dS  \\ &= 2\sqrt{a^2 + b^2 + c^2} (\text{Area of } \Omega) \end{align*}",,['multivariable-calculus']
15,Smooth submanifold of $\mathbb R^6$. Not smooth submanifold of $\mathbb R^3$,Smooth submanifold of . Not smooth submanifold of,\mathbb R^6 \mathbb R^3,"So I'm pretty new to studying manifolds and have little to no background on differential geometry, but this is a question from lecture notes on a multivariable analysis unit: Show that $S:=\{(x^2,y^2,z^2,yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\}$ is a smooth 2-submanifold of $\mathbb R^6$ and show that the projection of $S$ onto the last three coordinates (The Roman Surface $R:=\{(yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\}$ ) is not a smooth 2-submanifold of $\mathbb R^3$ For the first part I tried to construct an atlas for $S$ , given by: $\phi_1: D \to S$ , where D is the open unit disc around the origin in $\mathbb R^2$ , such that $$(x,y) \mapsto (x^2,y^2,1-x^2-y^2,y\sqrt{1-x^2-y^2},x\sqrt{1-x^2-y^2},xy)$$ and $\phi_2$ and $\phi_3$ are defined similarly by isolating $x$ and $y$ , respectively, on $x^2+y^2+z^2=1$ Now what I would like to know is: 1) Is this enough to show that $S$ is a smooth 2-submanifold of $\mathbb R^6$ ? 2) If the answer to 1) is yes, then, is this generally the best way to prove that a subset is a submanifold? 3) For the second part I have no idea how to show something is not a submanifold and would appreciate any help on the matter","So I'm pretty new to studying manifolds and have little to no background on differential geometry, but this is a question from lecture notes on a multivariable analysis unit: Show that is a smooth 2-submanifold of and show that the projection of onto the last three coordinates (The Roman Surface ) is not a smooth 2-submanifold of For the first part I tried to construct an atlas for , given by: , where D is the open unit disc around the origin in , such that and and are defined similarly by isolating and , respectively, on Now what I would like to know is: 1) Is this enough to show that is a smooth 2-submanifold of ? 2) If the answer to 1) is yes, then, is this generally the best way to prove that a subset is a submanifold? 3) For the second part I have no idea how to show something is not a submanifold and would appreciate any help on the matter","S:=\{(x^2,y^2,z^2,yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\} \mathbb R^6 S R:=\{(yz,xz,xy)|x,y,z \in \mathbb R, x^2+y^2+z^2=1\} \mathbb R^3 S \phi_1: D \to S \mathbb R^2 (x,y) \mapsto (x^2,y^2,1-x^2-y^2,y\sqrt{1-x^2-y^2},x\sqrt{1-x^2-y^2},xy) \phi_2 \phi_3 x y x^2+y^2+z^2=1 S \mathbb R^6","['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
16,Smooth function on a closed set.,Smooth function on a closed set.,,"Evans book on PDE's defines for a given open subset $U$ of $\mathbb{R}^{n}$, $C^{k}(\overline{U})=\lbrace u:U\rightarrow \mathbb{R}^{n}$, such that $D^{\alpha}u$ exists and is uniformly continuous on bounded subsets of $U$, for all mulitindexes $\alpha$, with $|\alpha|\leq k\rbrace$ Alternatively one could define, $C^{k}(\overline{U})=\lbrace u:\overline{U}\rightarrow \mathbb{R}^{n},$ such that there exists an open subset $V$ of $\mathbb{R}^{n}$ containing $\overline{U}$, and an extension of $u$ to $V$ that has continous partial derivatives up to order $k$ in $V\rbrace$ Are this definitions equivalent? Is this trivial?","Evans book on PDE's defines for a given open subset $U$ of $\mathbb{R}^{n}$, $C^{k}(\overline{U})=\lbrace u:U\rightarrow \mathbb{R}^{n}$, such that $D^{\alpha}u$ exists and is uniformly continuous on bounded subsets of $U$, for all mulitindexes $\alpha$, with $|\alpha|\leq k\rbrace$ Alternatively one could define, $C^{k}(\overline{U})=\lbrace u:\overline{U}\rightarrow \mathbb{R}^{n},$ such that there exists an open subset $V$ of $\mathbb{R}^{n}$ containing $\overline{U}$, and an extension of $u$ to $V$ that has continous partial derivatives up to order $k$ in $V\rbrace$ Are this definitions equivalent? Is this trivial?",,['multivariable-calculus']
17,What is the difference between parametrization and change of variables?,What is the difference between parametrization and change of variables?,,"I am embarrassed to ask this, but really need to, in order to clarify my confusion. I am taking multi-variable calculus and I am confused as to the difference between when I should be parametrizing and when I am making a change of variables. My question is really motivated by a question such as this: Compute $	\int \int_{E} e^{-4x^2-9y^2}\,dxdy $ where $E$ is the Ellipse   $4x^2+9y^2 \le 25$ When I see a question like this, the first thing I think is, ok parametrize the curve, so I simply did: $x = \frac{5}{2}cos\theta \space$and$ \space y = \frac{5}{3}sin\theta$ and then proceeded to to substitute and do: $	\int \int_{E} e^{-25}\,rdrd\theta $ getting the wrong result. Actually as I type this, I think I may have answered my own question. Have I got the wrong idea because, I have essentially turned a double integral with two variables into a 1 variable thing? And use an incorrect  $r$? What is a good rule of thumb to keep in mind the difference of when I am making a change of variables and when I am just parametrizing?","I am embarrassed to ask this, but really need to, in order to clarify my confusion. I am taking multi-variable calculus and I am confused as to the difference between when I should be parametrizing and when I am making a change of variables. My question is really motivated by a question such as this: Compute $	\int \int_{E} e^{-4x^2-9y^2}\,dxdy $ where $E$ is the Ellipse   $4x^2+9y^2 \le 25$ When I see a question like this, the first thing I think is, ok parametrize the curve, so I simply did: $x = \frac{5}{2}cos\theta \space$and$ \space y = \frac{5}{3}sin\theta$ and then proceeded to to substitute and do: $	\int \int_{E} e^{-25}\,rdrd\theta $ getting the wrong result. Actually as I type this, I think I may have answered my own question. Have I got the wrong idea because, I have essentially turned a double integral with two variables into a 1 variable thing? And use an incorrect  $r$? What is a good rule of thumb to keep in mind the difference of when I am making a change of variables and when I am just parametrizing?",,['multivariable-calculus']
18,Question on using Leibniz formula to derive thin-film equation from Navier-Stokes,Question on using Leibniz formula to derive thin-film equation from Navier-Stokes,,"I am trying to work through the derivation in this paper by Petr Vita, which derives a thin-film simplification of the Navier-Stokes equation, similar to the Reynolds or Lubrication Equation , but including inertial terms as well.  To walk through the major steps to the point where I have questions: Start with basic N-S equation (eq 1 in the paper): $$\rho \left( \frac{\partial \mathbf{u}}{\partial t} + \nabla \cdot(\mathbf{u u}) \right) = -\nabla p + \rho \mathbf{g} + \nabla \cdot \mathbf{\underline{T}} $$ Here $ \mathbf{\underline{T}}$ is the deviatoric stress tensor, and I have left off the final body force term since it isn't used in the rest of the paper. Use the thin-film assumption that $u_z=0$ and define (eq 4 in the paper) $$\bar{\mathbf{u}}=\frac{1}{h}\int_0^h \mathbf{u}\, dz$$ Integrate the N-S equation with respect to $z$ from $0$ to $h$ (equation 8 in the paper). $$ \rho \frac{\partial}{\partial t} (h \bar{\mathbf{u}}) + \rho \int_0^h \nabla \cdot (\mathbf{u u})dz = -h \nabla p -\left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=0} $$ Obviously there are a few steps being skipped over here.  For the time-derivative term, I use the Leibniz formula to derive the following: $$ \int_0^h \rho \frac{\partial \mathbf u}{\partial t} dz = \rho \frac{\partial}{\partial t} \int_0^h \mathbf u\,dz - \rho \frac{\partial h}{\partial t} \mathbf u (x,y,h,t) + \rho \frac{\partial 0}{\partial t} \mathbf u (x,y,0,t)$$ Obviously the last term is $0$ and can be dropped. Also $ \rho \frac{\partial}{\partial t} \int_0^h \mathbf u\,dz = \rho \frac{\partial}{\partial t} (h \bar{\mathbf{u}}) $, giving the form seen in the equation.  However, I don't see how $ \color{blue}{\rho \frac{\partial h}{\partial t} \mathbf u (x,y,h,t)} $ can be taken to be $0$.  The height of the film is certainly changing with time, and the top surface has a von Neumann boundary condition, not a Dirichlet no-slip boundary. Any insight here? Also, the deviatoric stress has to be integrated as well.  I think the divergence theorem can be used here: $$ \int_V \nabla \cdot \mathbf{\underline{T}}\, dV = \int_S \mathbf{n} \cdot \mathbf{\underline{T}} \, dS$$ In this case that should come out to be $$ \int_0^h \nabla \cdot \mathbf{\underline{T}}\, dz =\left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=h} - \left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=0} $$ The top surface stress is $0$, leaving the bottom stress term as is found in the derived equation, right? Now we get to my main question, the integration of the $\nabla \cdot(\mathbf{u u})$ term.  The author is able to evaluate this term by using a combination of Pohlhausen's method of assuming a cubic profile for the liquid flow, and the Reynold's Averaged Navier-Stokes method of splitting the velocity into an average velocity and deviation from that average. For the cubic profile he defines: $$ \mathbf{u}(x,y,z) = u(x,y,\xi) \text{, where} $$ $$ u(x,y,\xi) = a_0 + a_1\xi + a_2\xi^2 + a_3\xi^3,\quad \xi \in \langle 0,1 \rangle,\; z=h\xi $$ Then he applies the boundary conditions and integral relation to obtain  $$ u(x,y,\xi) = \mathbf{u}_{disk} + (\bar{\mathbf{u}}-\mathbf{u}_{disk})\left( \frac{12}{5}\xi - \frac{4}{5}\xi^3 \right) $$ This step is fine, I had no problems figuring it out.  Then the author defines the velocity fluctuation (with respect to the vertical direction) $\mathbf{\tilde u}$ as  $$ \mathbf{u} = \mathbf{\bar u} + \mathbf{\tilde u} \text{.  This makes:} $$ $$ \int_0^h \mathbf{\bar u}\, dz = h\,\mathbf{\bar u}\text{, and } \int_0^h \mathbf{\tilde u}\, dz = 0 \text{.}$$ Anyway, so the author does this to integrate the advection term: $$ \int_0^h \nabla \cdot ( \mathbf{uu} )\,dz = \nabla \cdot \left( \int_0^h \left[ \mathbf{\bar u} \mathbf{\bar u}+\mathbf{\bar u} \mathbf{\tilde u} + \mathbf{\tilde u} \mathbf{\bar u} + \mathbf{\tilde u} \mathbf{\tilde u} \right]\,dz \right)$$ So on the RHS the 1st term is just a constant, the 2nd and 3rd terms become $0$, and then he uses the derived polynomial form of $u$ to evaluate the last term.  However, how was he able to pull the divergence operator out of the integral?  He didn't use the divergence theorem, and I don't know if you can use the Leibniz formula on a divergence operator.  If you could do that though, wouldn't you have a term that's something like $\nabla \cdot h \left.(\mathbf{u}\mathbf{u})\right|_{z=h}$ leftover as well?","I am trying to work through the derivation in this paper by Petr Vita, which derives a thin-film simplification of the Navier-Stokes equation, similar to the Reynolds or Lubrication Equation , but including inertial terms as well.  To walk through the major steps to the point where I have questions: Start with basic N-S equation (eq 1 in the paper): $$\rho \left( \frac{\partial \mathbf{u}}{\partial t} + \nabla \cdot(\mathbf{u u}) \right) = -\nabla p + \rho \mathbf{g} + \nabla \cdot \mathbf{\underline{T}} $$ Here $ \mathbf{\underline{T}}$ is the deviatoric stress tensor, and I have left off the final body force term since it isn't used in the rest of the paper. Use the thin-film assumption that $u_z=0$ and define (eq 4 in the paper) $$\bar{\mathbf{u}}=\frac{1}{h}\int_0^h \mathbf{u}\, dz$$ Integrate the N-S equation with respect to $z$ from $0$ to $h$ (equation 8 in the paper). $$ \rho \frac{\partial}{\partial t} (h \bar{\mathbf{u}}) + \rho \int_0^h \nabla \cdot (\mathbf{u u})dz = -h \nabla p -\left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=0} $$ Obviously there are a few steps being skipped over here.  For the time-derivative term, I use the Leibniz formula to derive the following: $$ \int_0^h \rho \frac{\partial \mathbf u}{\partial t} dz = \rho \frac{\partial}{\partial t} \int_0^h \mathbf u\,dz - \rho \frac{\partial h}{\partial t} \mathbf u (x,y,h,t) + \rho \frac{\partial 0}{\partial t} \mathbf u (x,y,0,t)$$ Obviously the last term is $0$ and can be dropped. Also $ \rho \frac{\partial}{\partial t} \int_0^h \mathbf u\,dz = \rho \frac{\partial}{\partial t} (h \bar{\mathbf{u}}) $, giving the form seen in the equation.  However, I don't see how $ \color{blue}{\rho \frac{\partial h}{\partial t} \mathbf u (x,y,h,t)} $ can be taken to be $0$.  The height of the film is certainly changing with time, and the top surface has a von Neumann boundary condition, not a Dirichlet no-slip boundary. Any insight here? Also, the deviatoric stress has to be integrated as well.  I think the divergence theorem can be used here: $$ \int_V \nabla \cdot \mathbf{\underline{T}}\, dV = \int_S \mathbf{n} \cdot \mathbf{\underline{T}} \, dS$$ In this case that should come out to be $$ \int_0^h \nabla \cdot \mathbf{\underline{T}}\, dz =\left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=h} - \left.\mu \frac{\partial \mathbf{u}}{\partial z}\right|_{z=0} $$ The top surface stress is $0$, leaving the bottom stress term as is found in the derived equation, right? Now we get to my main question, the integration of the $\nabla \cdot(\mathbf{u u})$ term.  The author is able to evaluate this term by using a combination of Pohlhausen's method of assuming a cubic profile for the liquid flow, and the Reynold's Averaged Navier-Stokes method of splitting the velocity into an average velocity and deviation from that average. For the cubic profile he defines: $$ \mathbf{u}(x,y,z) = u(x,y,\xi) \text{, where} $$ $$ u(x,y,\xi) = a_0 + a_1\xi + a_2\xi^2 + a_3\xi^3,\quad \xi \in \langle 0,1 \rangle,\; z=h\xi $$ Then he applies the boundary conditions and integral relation to obtain  $$ u(x,y,\xi) = \mathbf{u}_{disk} + (\bar{\mathbf{u}}-\mathbf{u}_{disk})\left( \frac{12}{5}\xi - \frac{4}{5}\xi^3 \right) $$ This step is fine, I had no problems figuring it out.  Then the author defines the velocity fluctuation (with respect to the vertical direction) $\mathbf{\tilde u}$ as  $$ \mathbf{u} = \mathbf{\bar u} + \mathbf{\tilde u} \text{.  This makes:} $$ $$ \int_0^h \mathbf{\bar u}\, dz = h\,\mathbf{\bar u}\text{, and } \int_0^h \mathbf{\tilde u}\, dz = 0 \text{.}$$ Anyway, so the author does this to integrate the advection term: $$ \int_0^h \nabla \cdot ( \mathbf{uu} )\,dz = \nabla \cdot \left( \int_0^h \left[ \mathbf{\bar u} \mathbf{\bar u}+\mathbf{\bar u} \mathbf{\tilde u} + \mathbf{\tilde u} \mathbf{\bar u} + \mathbf{\tilde u} \mathbf{\tilde u} \right]\,dz \right)$$ So on the RHS the 1st term is just a constant, the 2nd and 3rd terms become $0$, and then he uses the derived polynomial form of $u$ to evaluate the last term.  However, how was he able to pull the divergence operator out of the integral?  He didn't use the divergence theorem, and I don't know if you can use the Leibniz formula on a divergence operator.  If you could do that though, wouldn't you have a term that's something like $\nabla \cdot h \left.(\mathbf{u}\mathbf{u})\right|_{z=h}$ leftover as well?",,"['multivariable-calculus', 'fluid-dynamics']"
19,Book Recommendation - Hard problems for Multivariable Calculus w/ Solutions,Book Recommendation - Hard problems for Multivariable Calculus w/ Solutions,,"I'm looking for a text that covers roughly what's sometimes called ""Calculus III"" or multivariable calculus.*  But this text must satisfy certain additional criteria: (1) It must be more in-depth (and consequently have harder exercises) than usual; (2) It must contain many solutions to the exercises; (3) It must not engage in the sort of unconvincing hand-waving common to ""mathematical methods"" texts. Basically I'm looking for a difficult, thorough version of a calc 3 text.  Bonus points if the book discusses differentials. PS: I'm aware that a somewhat similar question has been asked here Multivariable calculus: hard problems with solutions but the problems in the books recommended were not hard enough. *The sort of material in Stewart's Calculus.","I'm looking for a text that covers roughly what's sometimes called ""Calculus III"" or multivariable calculus.*  But this text must satisfy certain additional criteria: (1) It must be more in-depth (and consequently have harder exercises) than usual; (2) It must contain many solutions to the exercises; (3) It must not engage in the sort of unconvincing hand-waving common to ""mathematical methods"" texts. Basically I'm looking for a difficult, thorough version of a calc 3 text.  Bonus points if the book discusses differentials. PS: I'm aware that a somewhat similar question has been asked here Multivariable calculus: hard problems with solutions but the problems in the books recommended were not hard enough. *The sort of material in Stewart's Calculus.",,"['reference-request', 'multivariable-calculus']"
20,Proof of inverse function theorem by approximation property,Proof of inverse function theorem by approximation property,,"In proving the inverse function theorem using the approximation characterization of the derivative, we are given $F:\mathbb{R}^n \to \mathbb{R}^n$ such that $$F(p_0 + h) - F(p_0) = DF_{p_0}(h) + o(\|h\|).$$ Applying $(DF_{p_0})^{-1}$ to both sides, we get $$(DF_{p_0})^{-1}(\eta) = h + (DF_{p_0})^{-1}(o(\|h\|))$$ $$\Rightarrow F^{-1}(q_0 + \eta)-F^{-1}(q_0) = (DF_{p_0})^{-1}(\eta) - (DF_{p_0})^{-1}(o(\|h\|)), \tag{1}$$ where $F(p_0) = q_0$ and $F(p_0 + h) - F(p_0) = \eta$. The proof is over if we can show that $(DF_{p_0})^{-1}(o(\|h\|)) = o(\|\eta\|)$. My Question: Is the following proof that $(DF_{p_0})^{-1}(o(\|h\|)) = o(\|\eta\|)$ correct? Because $(DF_{p_0})^{-1}$ is invertible, we have for all $x\in \mathbb{R}^n$ $\exists m, M>0$ s.t. $$m \|x\| \leq \| (DF_{p_0})^{-1} (x) \| \leq M \|x\|.$$ Now $(1)$ implies that $$(DF_{p_0})^{-1}(\eta) = h + (DF_{p_0})^{-1}(o(\|h\|)) \\ \Rightarrow M\|\eta\| \geq \|h\| - M \epsilon \|h\| \\ \Rightarrow \frac {M \|\eta\|} {1 - M\epsilon} \geq \|h\|$$ So $\|h\| = O(\|\eta\|)$. (Here we've chosen $h$ small enough so that $|o(h)| \leq \epsilon \|h\|.$) Now $$(DF_{p_0})^{-1}(o(\|h\|)) \leq M(o(\|h\|))= M(o(O(\|\eta\|)))=o(\|\eta\|).$$ Does that work?","In proving the inverse function theorem using the approximation characterization of the derivative, we are given $F:\mathbb{R}^n \to \mathbb{R}^n$ such that $$F(p_0 + h) - F(p_0) = DF_{p_0}(h) + o(\|h\|).$$ Applying $(DF_{p_0})^{-1}$ to both sides, we get $$(DF_{p_0})^{-1}(\eta) = h + (DF_{p_0})^{-1}(o(\|h\|))$$ $$\Rightarrow F^{-1}(q_0 + \eta)-F^{-1}(q_0) = (DF_{p_0})^{-1}(\eta) - (DF_{p_0})^{-1}(o(\|h\|)), \tag{1}$$ where $F(p_0) = q_0$ and $F(p_0 + h) - F(p_0) = \eta$. The proof is over if we can show that $(DF_{p_0})^{-1}(o(\|h\|)) = o(\|\eta\|)$. My Question: Is the following proof that $(DF_{p_0})^{-1}(o(\|h\|)) = o(\|\eta\|)$ correct? Because $(DF_{p_0})^{-1}$ is invertible, we have for all $x\in \mathbb{R}^n$ $\exists m, M>0$ s.t. $$m \|x\| \leq \| (DF_{p_0})^{-1} (x) \| \leq M \|x\|.$$ Now $(1)$ implies that $$(DF_{p_0})^{-1}(\eta) = h + (DF_{p_0})^{-1}(o(\|h\|)) \\ \Rightarrow M\|\eta\| \geq \|h\| - M \epsilon \|h\| \\ \Rightarrow \frac {M \|\eta\|} {1 - M\epsilon} \geq \|h\|$$ So $\|h\| = O(\|\eta\|)$. (Here we've chosen $h$ small enough so that $|o(h)| \leq \epsilon \|h\|.$) Now $$(DF_{p_0})^{-1}(o(\|h\|)) \leq M(o(\|h\|))= M(o(O(\|\eta\|)))=o(\|\eta\|).$$ Does that work?",,"['multivariable-calculus', 'proof-verification']"
21,Partial derivative with respect to $y$ of $(y/x)$?,Partial derivative with respect to  of ?,y (y/x),"I'm just starting partials and don't understand this at all. I'm told to hold $y$ ""constant"", so I treat $y$ like just some number and take the derivative of $\frac{1}{x}$, which I hope I'm correct in saying is $-\frac{1}{x^2}$, then multiply by $y$, getting $-\frac{y}{x^2}$. But apparently the correct answer is $\frac{1}{x}$. What am I missing?","I'm just starting partials and don't understand this at all. I'm told to hold $y$ ""constant"", so I treat $y$ like just some number and take the derivative of $\frac{1}{x}$, which I hope I'm correct in saying is $-\frac{1}{x^2}$, then multiply by $y$, getting $-\frac{y}{x^2}$. But apparently the correct answer is $\frac{1}{x}$. What am I missing?",,"['multivariable-calculus', 'partial-derivative']"
22,How can a gradient be thought of as a function?,How can a gradient be thought of as a function?,,"If a function $f(x,y)$ would output a value in a third dimension, $z=f(x,y)$ for example. How can we treat the gradient of $f$ as a function in $x$ and $y$ , when the output of the gradient is a vector in the two dimensions $x$ and $y$ which are the dimensions of the inputs. I guess my question is, is it normal for a function to map to the dimensions of its inputs?","If a function would output a value in a third dimension, for example. How can we treat the gradient of as a function in and , when the output of the gradient is a vector in the two dimensions and which are the dimensions of the inputs. I guess my question is, is it normal for a function to map to the dimensions of its inputs?","f(x,y) z=f(x,y) f x y x y","['multivariable-calculus', 'functions']"
23,Partial derivative confusion.,Partial derivative confusion.,,"I don't understand partial derivatives. Here's an example that nails down my confusion: Suppose we have some variables $x$, $p$, and $q$ with $p=x^2$ and $q=e^x$. Then $$\frac{\partial q}{\partial p} = \frac{\partial}{\partial p}e^{\left(p^{1/2}\right)}=\frac{e^{\left(p^{1/2}\right)}}{2p^{1/2}}$$ So far so good. Now suppose we have variables $p$, $q$ and $a$ with $a=pq$. Then $$\frac{\partial q}{\partial p}=\frac{\partial}{\partial p}\frac{a}{p}=-\frac{a}{p^2}$$(where $a$ is being held constant) What happens when we have both of these at once? i.e. $p=x^2$, $q=e^x$ and $a=pq$. What's $\frac{\partial q}{\partial p}$? Does it depend on what we hold constant? Does holding $x$ constant even make sense?","I don't understand partial derivatives. Here's an example that nails down my confusion: Suppose we have some variables $x$, $p$, and $q$ with $p=x^2$ and $q=e^x$. Then $$\frac{\partial q}{\partial p} = \frac{\partial}{\partial p}e^{\left(p^{1/2}\right)}=\frac{e^{\left(p^{1/2}\right)}}{2p^{1/2}}$$ So far so good. Now suppose we have variables $p$, $q$ and $a$ with $a=pq$. Then $$\frac{\partial q}{\partial p}=\frac{\partial}{\partial p}\frac{a}{p}=-\frac{a}{p^2}$$(where $a$ is being held constant) What happens when we have both of these at once? i.e. $p=x^2$, $q=e^x$ and $a=pq$. What's $\frac{\partial q}{\partial p}$? Does it depend on what we hold constant? Does holding $x$ constant even make sense?",,"['multivariable-calculus', 'derivatives']"
24,Derivative as a linear transformation,Derivative as a linear transformation,,"i can't understand this : consider $f:A \rightarrow Y$ for normed space $Y$, $~~A$ is an open subset of $\mathbb R$ ,and $a \in A$. In this case ,the existence of vector derivative $f'(a)$ is equivalent to fact that in the neighbourhood of $a$ ,$f(a+r)$ can be very well estimated by $f(a)+[f'(a)](r)$. The meaning of 'very well estimated' is given precisely as: for each $\epsilon \gt 0$ ,there is a $\delta \gt 0$ such that if $|r|<\delta$ ,then $\|f(a+r)-f(a)-[f'(a)](r) \| \leq \epsilon |r|.$ The notation $[f'(a)]$ is meant to be suggestive.Any vector y $\in Y$ corresponds to linear transformation $T_{y}:\mathbb R \rightarrow Y$ given by $T_{y}(r)=r$ y .Conversely ,any linear transformation $T$ in $L(\mathbb R,Y)$ corresponds to vector y =$T(1)$. It turns out that thinking of $[f'(a)]$ as a linear transformation in $L(\mathbb R,Y)~$, rather than a vector , is the key that lets us generalize the definition to cases where the domain space $X$ is not $\mathbb R$. What I can't understand is why we think of derivative as a linear transformation here ?Please help..... EDIT : In particular I can't understand why does the book states: Any vector y $\in Y$ corresponds to a linear transformation  := $T_{y}:\mathbb R \rightarrow Y$  by$T_{y}(r)=r$ y .Conversely ,any linear transformation $T$ in $L(\mathbb R,Y)$ corresponds to vector y =$T(1)$.It turns out that thinking of $[f'(a)]$ as a linear transformation in $L(\mathbb R,Y)~$, rather than a vector , is the key that lets us generalize the definition to cases where the domain space $X$ is not $\mathbb R$. Please If anyone could explain me this and while constructing the answer knowing that I just know fundamentals of calculus and analysis and don't know much vector spaces...","i can't understand this : consider $f:A \rightarrow Y$ for normed space $Y$, $~~A$ is an open subset of $\mathbb R$ ,and $a \in A$. In this case ,the existence of vector derivative $f'(a)$ is equivalent to fact that in the neighbourhood of $a$ ,$f(a+r)$ can be very well estimated by $f(a)+[f'(a)](r)$. The meaning of 'very well estimated' is given precisely as: for each $\epsilon \gt 0$ ,there is a $\delta \gt 0$ such that if $|r|<\delta$ ,then $\|f(a+r)-f(a)-[f'(a)](r) \| \leq \epsilon |r|.$ The notation $[f'(a)]$ is meant to be suggestive.Any vector y $\in Y$ corresponds to linear transformation $T_{y}:\mathbb R \rightarrow Y$ given by $T_{y}(r)=r$ y .Conversely ,any linear transformation $T$ in $L(\mathbb R,Y)$ corresponds to vector y =$T(1)$. It turns out that thinking of $[f'(a)]$ as a linear transformation in $L(\mathbb R,Y)~$, rather than a vector , is the key that lets us generalize the definition to cases where the domain space $X$ is not $\mathbb R$. What I can't understand is why we think of derivative as a linear transformation here ?Please help..... EDIT : In particular I can't understand why does the book states: Any vector y $\in Y$ corresponds to a linear transformation  := $T_{y}:\mathbb R \rightarrow Y$  by$T_{y}(r)=r$ y .Conversely ,any linear transformation $T$ in $L(\mathbb R,Y)$ corresponds to vector y =$T(1)$.It turns out that thinking of $[f'(a)]$ as a linear transformation in $L(\mathbb R,Y)~$, rather than a vector , is the key that lets us generalize the definition to cases where the domain space $X$ is not $\mathbb R$. Please If anyone could explain me this and while constructing the answer knowing that I just know fundamentals of calculus and analysis and don't know much vector spaces...",,"['multivariable-calculus', 'derivatives', 'linear-transformations']"
25,"Derivative of a function divided by its norm, i.e., $\phi(x) = \frac{f(x)}{\|f(x)\|}$","Derivative of a function divided by its norm, i.e.,",\phi(x) = \frac{f(x)}{\|f(x)\|},"Setting $f:\mathbb{R}^n\to\mathbb{R}^n$ and $\|\cdot \|$ be the usual Euclidean norm. I would like to compute the derivative with respect to $x$ of $$ \phi(x)  = \frac{f(x)}{\|f(x)\|} $$ My Attempt at a Solution $$ \nabla_x \frac{f(x)}{||f(x)||} = \frac{\nabla_x f(x)}{||f(x)||} + f(x)\nabla_x\left(f(x)^\top f(x)\right)^{-\frac{1}{2}} = \frac{\nabla_x f(x)}{||f(x)||}-\frac{1}{2}\frac{f(x)}{||f(x)||^3} 2f(x)^\top \nabla_x f(x) = \left(I - \frac{f(x)f(x)^\top}{||f(x)||^2}\right)\frac{\nabla_x f(x)}{||f(x)||} $$ However I am very unsure about this. In particular, I have a feeling that the second term would be $f(x)^\top \nabla_x f(x)$ and hence lead to $$ \nabla_x \frac{f(x)}{||f(x)||}  = \frac{\nabla_x f(x)}{||f(x)||} - \frac{\nabla_x f(x)}{||f(x)||} = 0 $$ However this wouldn't make much sense cause surely the gradient is not $0$ for every function.","Setting and be the usual Euclidean norm. I would like to compute the derivative with respect to of My Attempt at a Solution However I am very unsure about this. In particular, I have a feeling that the second term would be and hence lead to However this wouldn't make much sense cause surely the gradient is not for every function.","f:\mathbb{R}^n\to\mathbb{R}^n \|\cdot \| x 
\phi(x)  = \frac{f(x)}{\|f(x)\|}
 
\nabla_x \frac{f(x)}{||f(x)||} = \frac{\nabla_x f(x)}{||f(x)||} + f(x)\nabla_x\left(f(x)^\top f(x)\right)^{-\frac{1}{2}} = \frac{\nabla_x f(x)}{||f(x)||}-\frac{1}{2}\frac{f(x)}{||f(x)||^3} 2f(x)^\top \nabla_x f(x) = \left(I - \frac{f(x)f(x)^\top}{||f(x)||^2}\right)\frac{\nabla_x f(x)}{||f(x)||}
 f(x)^\top \nabla_x f(x) 
\nabla_x \frac{f(x)}{||f(x)||}  = \frac{\nabla_x f(x)}{||f(x)||} - \frac{\nabla_x f(x)}{||f(x)||} = 0
 0","['multivariable-calculus', 'derivatives']"
26,Intuition behind why is unit speed parametrization and arc length parametrization the same?,Intuition behind why is unit speed parametrization and arc length parametrization the same?,,"I have found a bunch of simple and not so simple proofs about why a vector function ( $f(t)$ ) parametrized in such a way that it's derivative is always 1 ( $|f'(t)|=1$ ) is the same as parametrizing it by arc length ( $f(s) \iff |f'(s)|=1$ ). Just to provide one example of a proof found, I write down the definition for arclength with $t$ substituted with $s$ $$s=\int_0^s{|f(s)|}ds$$ apply derivative by both sides with respect to $s$ $$1=|f(s)|$$ BOOM!, proven. (as long as the reparametrization is a biyective, smooth and has an inverse) The question is, How can i understand this as an intuitive thing? I think im missing the ""aha"" moment where is makes sense that an arc length function would have unit speed.","I have found a bunch of simple and not so simple proofs about why a vector function ( ) parametrized in such a way that it's derivative is always 1 ( ) is the same as parametrizing it by arc length ( ). Just to provide one example of a proof found, I write down the definition for arclength with substituted with apply derivative by both sides with respect to BOOM!, proven. (as long as the reparametrization is a biyective, smooth and has an inverse) The question is, How can i understand this as an intuitive thing? I think im missing the ""aha"" moment where is makes sense that an arc length function would have unit speed.",f(t) |f'(t)|=1 f(s) \iff |f'(s)|=1 t s s=\int_0^s{|f(s)|}ds s 1=|f(s)|,"['multivariable-calculus', 'differential-geometry', 'intuition']"
27,Volume of the largest rectangular parallelepiped inscribed in an ellipsoid,Volume of the largest rectangular parallelepiped inscribed in an ellipsoid,,Show that the volume of the largest rectangular parallelepiped that can be inscribed in the ellipsoid $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ is $\dfrac{8abc}{3\sqrt3}$. I proceeded by assuming that the volume is $xyz$ and used a Lagrange multiplier to start with $$xyz+\lambda \left(\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}-1\right)$$ I proceeded further to arrive at $\frac{abc}{3\sqrt3}$. Somehow I seemed to be have missed $8$. Can someone please tell me where I did go wrong?,Show that the volume of the largest rectangular parallelepiped that can be inscribed in the ellipsoid $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ is $\dfrac{8abc}{3\sqrt3}$. I proceeded by assuming that the volume is $xyz$ and used a Lagrange multiplier to start with $$xyz+\lambda \left(\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}-1\right)$$ I proceeded further to arrive at $\frac{abc}{3\sqrt3}$. Somehow I seemed to be have missed $8$. Can someone please tell me where I did go wrong?,,"['multivariable-calculus', 'optimization', 'volume', 'lagrange-multiplier']"
28,Evaluate $\int_0^1\int_p^1 \frac {x^3}{\sqrt{1-y^6}} dydx$,Evaluate,\int_0^1\int_p^1 \frac {x^3}{\sqrt{1-y^6}} dydx,"I have been working on this sum for a while. The question asks to evaluate the double integral. $$\int_0^1\int_p^1 \frac {x^3}{\sqrt{1-y^6}} dydx$$ where $p$ is equal to $x^2$. I know that I have to solve the $y$ integral first and then the $x$. But I don't know how to solve the root integral. Applying the formula $$\int \frac{1}{\sqrt{1-t^2}}dt$$ where $t=y^3$ isn't working. Any ideas as to how I must proceed with the integral? Once I get the integral, I must substitute the limits and then the integral would be in terms of $x$ and I must integrate it. Am I correct?","I have been working on this sum for a while. The question asks to evaluate the double integral. $$\int_0^1\int_p^1 \frac {x^3}{\sqrt{1-y^6}} dydx$$ where $p$ is equal to $x^2$. I know that I have to solve the $y$ integral first and then the $x$. But I don't know how to solve the root integral. Applying the formula $$\int \frac{1}{\sqrt{1-t^2}}dt$$ where $t=y^3$ isn't working. Any ideas as to how I must proceed with the integral? Once I get the integral, I must substitute the limits and then the integral would be in terms of $x$ and I must integrate it. Am I correct?",,['multivariable-calculus']
29,Error in the tree diagram of the chain rule?,Error in the tree diagram of the chain rule?,,"I was given a problem, we define: $$z(x,y) = x\sin(y^2), y(x) = 2x+1.$$ We wish to use the chain rule to compute $\frac{\partial z}{\partial x}.$ I know, I can get the correct answer using the Jacobian; however, the tree diagram fails me I this regard because, $$ \frac{\partial z}{\partial x} = \frac{\partial z}{\partial x} + \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}.$$ Obviously, we have a problem here. Am I misunderstanding something?","I was given a problem, we define: We wish to use the chain rule to compute I know, I can get the correct answer using the Jacobian; however, the tree diagram fails me I this regard because, Obviously, we have a problem here. Am I misunderstanding something?","z(x,y) = x\sin(y^2), y(x) = 2x+1. \frac{\partial z}{\partial x}.  \frac{\partial z}{\partial x} = \frac{\partial z}{\partial x} + \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}.",['multivariable-calculus']
30,Differential forms: $dx \wedge dy$ vs $dx dy$,Differential forms:  vs,dx \wedge dy dx dy,"When evaluating the integrals of differential forms over a parameterized manifold, you often end up with something like: $$\int _{|\gamma(s)|} dx\wedge dy + y dx \wedge dz = \iint_{[a,b] \times [c,d]} f(s,t)\,ds\,dt$$ However, this led me to a conceptual contradiction: the left hand side involves the wedge product of differential one forms, and is thus an expression involving differential two-forms. The right hand side, however, has differentials (presumably differential one-forms) just multiplied together. Traditionally, I would interpret $ds dt$ as just representing an ""infinitesimally"" small square in the s-t plane. However, how do I reconcile this notation/concept with that of the differential form?","When evaluating the integrals of differential forms over a parameterized manifold, you often end up with something like: $$\int _{|\gamma(s)|} dx\wedge dy + y dx \wedge dz = \iint_{[a,b] \times [c,d]} f(s,t)\,ds\,dt$$ However, this led me to a conceptual contradiction: the left hand side involves the wedge product of differential one forms, and is thus an expression involving differential two-forms. The right hand side, however, has differentials (presumably differential one-forms) just multiplied together. Traditionally, I would interpret $ds dt$ as just representing an ""infinitesimally"" small square in the s-t plane. However, how do I reconcile this notation/concept with that of the differential form?",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
31,How to prove Cauchy-Schwarz integral inequality?,How to prove Cauchy-Schwarz integral inequality?,,"The Cauchy-Schwarz integral inequality is as follows: $$ \displaystyle \left({\int_a^b f \left({t}\right) g \left({t}\right) \ \mathrm d t}\right)^2 \le \int_a^b \left({f \left({t}\right)}\right)^2 \mathrm d t \int_a^b \left({g \left({t}\right)}\right)^2 \mathrm d t $$ How do I prove this using multivariable calculus methods, preferably with double integrals?","The Cauchy-Schwarz integral inequality is as follows: $$ \displaystyle \left({\int_a^b f \left({t}\right) g \left({t}\right) \ \mathrm d t}\right)^2 \le \int_a^b \left({f \left({t}\right)}\right)^2 \mathrm d t \int_a^b \left({g \left({t}\right)}\right)^2 \mathrm d t $$ How do I prove this using multivariable calculus methods, preferably with double integrals?",,"['multivariable-calculus', 'integral-inequality']"
32,Computing a Lie Bracket: General Questions,Computing a Lie Bracket: General Questions,,"I'm asked to compute the following Lie Bracket: $\left [ -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} , \dfrac{\partial}{\partial x} \right] $ on $\mathbb{R}^2$. Just writing it out, I get $\left( -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right) \dfrac{\partial}{\partial x} - \dfrac{\partial}{\partial x} \left(-y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right)$. How can I simplify this? I know this is a very trivial question, but I'm getting stuck for some stupid reason. Any help would be greatful :)","I'm asked to compute the following Lie Bracket: $\left [ -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} , \dfrac{\partial}{\partial x} \right] $ on $\mathbb{R}^2$. Just writing it out, I get $\left( -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right) \dfrac{\partial}{\partial x} - \dfrac{\partial}{\partial x} \left(-y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right)$. How can I simplify this? I know this is a very trivial question, but I'm getting stuck for some stupid reason. Any help would be greatful :)",,"['multivariable-calculus', 'differential-geometry', 'lie-algebras']"
33,How to find the minimum value of this function?,How to find the minimum value of this function?,,"How to find the minimum value of  $$\frac{x}{3y^2+3z^2+3yz+1}+\frac{y}{3x^2+3z^2+3xz+1}+\frac{z}{3x^2+3y^2+3xy+1}$$,where $x,y,z\geq 0$ and $x+y+z=1$. It seems to be hard if we use calculus methods. Are there another method? I have no idea. Thank you.","How to find the minimum value of  $$\frac{x}{3y^2+3z^2+3yz+1}+\frac{y}{3x^2+3z^2+3xz+1}+\frac{z}{3x^2+3y^2+3xy+1}$$,where $x,y,z\geq 0$ and $x+y+z=1$. It seems to be hard if we use calculus methods. Are there another method? I have no idea. Thank you.",,"['multivariable-calculus', 'inequality', 'optimization', 'cauchy-schwarz-inequality', 'a.m.-g.m.-inequality']"
34,Vector physics boat problem,Vector physics boat problem,,"A friend of mine posed this question to me: A boat with a maximum speed of $5$ meters per second crosses a river that is flowing at $3$ meters per second, and is fifteen meters wide. The boat always faces its target $(0,15)$. Does the boat reach its target? If so, how long does it take? What is the path of the boat? If the boat is at a point $(x,y)$, then the angle $\theta$ to the target is $\theta(x,y) = \arctan\left(\frac{15 - y}{x}\right)$ and the velocity is $v(\theta) = (-3 + 5 \cos \theta) \hat{\imath} + (5 \sin \theta) \hat{\jmath}$. I couldn't find parametric equations, so I wrote a small program to solve it for me. The program steps through small intervals of time, moves the boat by $v\,\mathrm dt$, and recalculates the velocity. The program halts when the boat has arrived at the target. The graph of the path of the boat generated by this program should approximate the actual graph. The program found that the time taken is about $4.7$ seconds. How can I find parametric equations for the path of the boat?","A friend of mine posed this question to me: A boat with a maximum speed of $5$ meters per second crosses a river that is flowing at $3$ meters per second, and is fifteen meters wide. The boat always faces its target $(0,15)$. Does the boat reach its target? If so, how long does it take? What is the path of the boat? If the boat is at a point $(x,y)$, then the angle $\theta$ to the target is $\theta(x,y) = \arctan\left(\frac{15 - y}{x}\right)$ and the velocity is $v(\theta) = (-3 + 5 \cos \theta) \hat{\imath} + (5 \sin \theta) \hat{\jmath}$. I couldn't find parametric equations, so I wrote a small program to solve it for me. The program steps through small intervals of time, moves the boat by $v\,\mathrm dt$, and recalculates the velocity. The program halts when the boat has arrived at the target. The graph of the path of the boat generated by this program should approximate the actual graph. The program found that the time taken is about $4.7$ seconds. How can I find parametric equations for the path of the boat?",,"['multivariable-calculus', 'physics']"
35,"compute $\int_Q\frac{1}{|x|} \, dx$ on $Q=[0,1]^2$",compute  on,"\int_Q\frac{1}{|x|} \, dx Q=[0,1]^2","Let $Q=[0,1]^2$, compute the integral: $$\int_Q\frac{1}{|x|} \, dx$$ I tried to take $x=(x_1,x_2)$, then the integral is: $$\int_Q\frac{1}{\sqrt{x_1^2+x_2^2}} \, dx_1 \, dx_2$$ Then I moved to polar coordinates $(x_1,x_2)=(r\cos\theta,r\sin\theta)$, so the integrand is $\frac{1}{\sqrt{r^2}}r=1$. I could't find the integral's domain. I took $0\leq r\cos\theta\leq1$ and $0\leq r\sin\theta\leq1$ from $(x_1,x_2)\in Q$. Then I got $0\leq r\leq\sqrt2$, but I didn't mange to find $\theta$'s interval, is it $\displaystyle \Big[0,\frac{\pi}{2}\Big]$? Final result by WA is: $2\log(1 + \sqrt2)$","Let $Q=[0,1]^2$, compute the integral: $$\int_Q\frac{1}{|x|} \, dx$$ I tried to take $x=(x_1,x_2)$, then the integral is: $$\int_Q\frac{1}{\sqrt{x_1^2+x_2^2}} \, dx_1 \, dx_2$$ Then I moved to polar coordinates $(x_1,x_2)=(r\cos\theta,r\sin\theta)$, so the integrand is $\frac{1}{\sqrt{r^2}}r=1$. I could't find the integral's domain. I took $0\leq r\cos\theta\leq1$ and $0\leq r\sin\theta\leq1$ from $(x_1,x_2)\in Q$. Then I got $0\leq r\leq\sqrt2$, but I didn't mange to find $\theta$'s interval, is it $\displaystyle \Big[0,\frac{\pi}{2}\Big]$? Final result by WA is: $2\log(1 + \sqrt2)$",,"['multivariable-calculus', 'improper-integrals']"
36,Why is this map called a fold?,Why is this map called a fold?,,"Consider the map $\varphi : \mathbb R^2 \to \mathbb R^2$ defined by $(x,y) \mapsto (x,y^2)$. Apparently this map is called a fold as the $(x,y)$-plane is folded over and creased along the axis $y=0$. But I really don't see how this is the case: $y^2$ does not ""fold"" or ""crease"" anything it just bends the plane very very slightly. No? Please could someone explain to me how this map folds the plane? I   obviously misunderstand it completely.","Consider the map $\varphi : \mathbb R^2 \to \mathbb R^2$ defined by $(x,y) \mapsto (x,y^2)$. Apparently this map is called a fold as the $(x,y)$-plane is folded over and creased along the axis $y=0$. But I really don't see how this is the case: $y^2$ does not ""fold"" or ""crease"" anything it just bends the plane very very slightly. No? Please could someone explain to me how this map folds the plane? I   obviously misunderstand it completely.",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
37,What to do when the second derivative test fails?,What to do when the second derivative test fails?,,"What do we do when the second derivative test fails? For example, I'm asked to find all the critical points of the function $$f(x,y)=x^{2013}−y^{2013}$$ And determine the nature of the critical points. The critical point that I have found is at $(0,0)$, but I'm unable to determine its nature as the second derivative test fails here.","What do we do when the second derivative test fails? For example, I'm asked to find all the critical points of the function $$f(x,y)=x^{2013}−y^{2013}$$ And determine the nature of the critical points. The critical point that I have found is at $(0,0)$, but I'm unable to determine its nature as the second derivative test fails here.",,['multivariable-calculus']
38,Curvature of a curve lying on a sphere?,Curvature of a curve lying on a sphere?,,"This is a sample question from a multivariate calculus class. Any insight would be appreciated. Suppose the curve $\mathbf{r} = \mathbf{r}(s)$ is parametrized by a natural parameter and lies on the unit sphere centered at the origin. Show that its curvature satisfies $$\kappa = \sqrt{ 1 + \left(\mathbf{r}'' \cdot \left(\mathbf{r} \times \mathbf{r}' \right) \right)^2}.$$ Below is what I'm familiar with and what I've tried to use, but I can't seem to connect the ideas together. The unit sphere at the origin can be represented as $x^2 + y^2 + z^2 = 1$. If $\mathbf{r} = \langle\ x(s),\ y(s),\ z(s) \ \rangle$ lies on the sphere, then $\mathbf{r}$ will intersect the sphere at any point such that $[x(s)]^2 + [y(s)]^2 + [z(s)]^2 = 1.$ From this I gather that $\| {\mathbf{r}} \| = 1.$ Since the norm of $\mathbf{r}$ is constant, then $\mathbf{r} \cdot \mathbf{r}' = 0$. Therefore $\mathbf{r}$ and $\mathbf{r}'$ are orthogonal to one another. But we know that $\mathbf{r}'$ is tangent to our curve, and $\mathbf{r}''$ would be normal to our curve. I suppose we could use unit vectors and then the Frenet-Serret equations may come into play, but I don't see it. I'm familiar with the various curvature formulas, and I'd like to believe $\kappa = \| \mathbf{r}''(s) \|$ will be the one that works for us. Thank you very much!","This is a sample question from a multivariate calculus class. Any insight would be appreciated. Suppose the curve $\mathbf{r} = \mathbf{r}(s)$ is parametrized by a natural parameter and lies on the unit sphere centered at the origin. Show that its curvature satisfies $$\kappa = \sqrt{ 1 + \left(\mathbf{r}'' \cdot \left(\mathbf{r} \times \mathbf{r}' \right) \right)^2}.$$ Below is what I'm familiar with and what I've tried to use, but I can't seem to connect the ideas together. The unit sphere at the origin can be represented as $x^2 + y^2 + z^2 = 1$. If $\mathbf{r} = \langle\ x(s),\ y(s),\ z(s) \ \rangle$ lies on the sphere, then $\mathbf{r}$ will intersect the sphere at any point such that $[x(s)]^2 + [y(s)]^2 + [z(s)]^2 = 1.$ From this I gather that $\| {\mathbf{r}} \| = 1.$ Since the norm of $\mathbf{r}$ is constant, then $\mathbf{r} \cdot \mathbf{r}' = 0$. Therefore $\mathbf{r}$ and $\mathbf{r}'$ are orthogonal to one another. But we know that $\mathbf{r}'$ is tangent to our curve, and $\mathbf{r}''$ would be normal to our curve. I suppose we could use unit vectors and then the Frenet-Serret equations may come into play, but I don't see it. I'm familiar with the various curvature formulas, and I'd like to believe $\kappa = \| \mathbf{r}''(s) \|$ will be the one that works for us. Thank you very much!",,"['multivariable-calculus', 'differential-geometry']"
39,Which multivariable calculus books are heavily oriented at physics?,Which multivariable calculus books are heavily oriented at physics?,,"Please recommend multivariable calculus books that are really physics oriented. My wife is looking to brush up on multivariable calculus, at the same time she needs to brush up on the related physics.","Please recommend multivariable calculus books that are really physics oriented. My wife is looking to brush up on multivariable calculus, at the same time she needs to brush up on the related physics.",,"['multivariable-calculus', 'reference-request', 'book-recommendation']"
40,Gauss's Theorem vs. Stokes's Theorem?,Gauss's Theorem vs. Stokes's Theorem?,,"What's the difference between Gauss' Theorem and Stokes' Theorem? Does Gauss's Theorem take an integral over an ""inner product"" derivative while Stokes's Theorem takes an integral over an exterior derivative? And is ""divergence"" associated with Gauss's Theorem and ""curl"" associated with Stokes's Theorem? And does ""divergence"" refer to movements of (e.g. fluids) TO (and from) a surface, while curl refers to movements AROUND a surface)?","What's the difference between Gauss' Theorem and Stokes' Theorem? Does Gauss's Theorem take an integral over an ""inner product"" derivative while Stokes's Theorem takes an integral over an exterior derivative? And is ""divergence"" associated with Gauss's Theorem and ""curl"" associated with Stokes's Theorem? And does ""divergence"" refer to movements of (e.g. fluids) TO (and from) a surface, while curl refers to movements AROUND a surface)?",,['multivariable-calculus']
41,Do scalar integrals correspond to integration of differential forms?,Do scalar integrals correspond to integration of differential forms?,,"I have read through part of Spivak's Calculus on Manifolds up to the chapter Integration on Chains. Now I know that given a smooth vector field on $\Bbb R^3$, doing line integral over a parametrised smooth curve, and doing surface integral over a parametrised smooth surface, correspond to integrating a $1$-form over a $1$-chain, and integrating a $2$-form over a $2$-chain, respectively. But what about if we are given a smooth scalar field and then we do scalar line integral or scalar surface integral? Do they correspond to integrating some differential forms? If not, can we generalise scalar integral in some other ways?","I have read through part of Spivak's Calculus on Manifolds up to the chapter Integration on Chains. Now I know that given a smooth vector field on $\Bbb R^3$, doing line integral over a parametrised smooth curve, and doing surface integral over a parametrised smooth surface, correspond to integrating a $1$-form over a $1$-chain, and integrating a $2$-form over a $2$-chain, respectively. But what about if we are given a smooth scalar field and then we do scalar line integral or scalar surface integral? Do they correspond to integrating some differential forms? If not, can we generalise scalar integral in some other ways?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'smooth-manifolds', 'differential-forms']"
42,How to find unit normal vector of a 2d line,How to find unit normal vector of a 2d line,,"I have been given the line equation of $y-8=0$, and have to find the unit normal vector? How would I go about doing this? is there a specific equation I need to use? Following this I have been given the question: Find the equation of a line passing through point $A(9, −6)$ and orthogonal to vector $v=[−4,0]^T$. Which i am also unsure how to complete. Any help would be much appreciated, it's been quite a while since i have done this sort of maths. Thanks","I have been given the line equation of $y-8=0$, and have to find the unit normal vector? How would I go about doing this? is there a specific equation I need to use? Following this I have been given the question: Find the equation of a line passing through point $A(9, −6)$ and orthogonal to vector $v=[−4,0]^T$. Which i am also unsure how to complete. Any help would be much appreciated, it's been quite a while since i have done this sort of maths. Thanks",,['multivariable-calculus']
43,"What does a ""heat ball"" look like?","What does a ""heat ball"" look like?",,"I am learning Mean value property (MVP) of the heat equation. MVP of Laplace equation was relatively easy to understand I think it is because of the spherical symmetry. But I am not able to appreciate the MVP of heat equation. It's not very easy to imagine the ""heat ball"" in the following theorem from a note : Here are questions : How do I define a heat ball? How does it actually look like?","I am learning Mean value property (MVP) of the heat equation. MVP of Laplace equation was relatively easy to understand I think it is because of the spherical symmetry. But I am not able to appreciate the MVP of heat equation. It's not very easy to imagine the ""heat ball"" in the following theorem from a note : Here are questions : How do I define a heat ball? How does it actually look like?",,"['multivariable-calculus', 'partial-differential-equations']"
44,"How would the double derivative of $f:\mathbb{R}^N \to \mathbb{R}^M$ i.e., $f''$ look like?","How would the double derivative of  i.e.,  look like?",f:\mathbb{R}^N \to \mathbb{R}^M f'',"The derivative of $f:\mathbb{R}^N \to \mathbb{R}^M$ is of the form $f':\mathbb{R}^N \to \mathbb{R}^{M \times N}$. I'd like to know how the double derivative look like, i.e, how would $f''$ be ? It maps from $\mathbb{R}^N$ to which space? PS : Please suggest some good references on this topic.","The derivative of $f:\mathbb{R}^N \to \mathbb{R}^M$ is of the form $f':\mathbb{R}^N \to \mathbb{R}^{M \times N}$. I'd like to know how the double derivative look like, i.e, how would $f''$ be ? It maps from $\mathbb{R}^N$ to which space? PS : Please suggest some good references on this topic.",,"['differential-geometry', 'multivariable-calculus']"
45,Inversion of matrices is a diffeomorphism.,Inversion of matrices is a diffeomorphism.,,"I am having problems showing that the function $$ \operatorname{inv}:G\rightarrow G$$ $$A\rightarrow A^{-1}$$ where $G$ is the set of all invertible $n\times n$ matrices, is a diffeomorphism. I have already shown that such function is a homeomorphism, and its inverse is itself, but I don't know how I can show that this function is differentiable. The exercise also tells us that the derivative of $\operatorname{inv}$ in $A$ is the linear mapping $M\rightarrow M$ such that $X\rightarrow -A^{-1}\cdot X\cdot A^{-1}$. Can anybody give me a hint?","I am having problems showing that the function $$ \operatorname{inv}:G\rightarrow G$$ $$A\rightarrow A^{-1}$$ where $G$ is the set of all invertible $n\times n$ matrices, is a diffeomorphism. I have already shown that such function is a homeomorphism, and its inverse is itself, but I don't know how I can show that this function is differentiable. The exercise also tells us that the derivative of $\operatorname{inv}$ in $A$ is the linear mapping $M\rightarrow M$ such that $X\rightarrow -A^{-1}\cdot X\cdot A^{-1}$. Can anybody give me a hint?",,['multivariable-calculus']
46,Why is the the $k$-th derivative a symmetric multilinear map?,Why is the the -th derivative a symmetric multilinear map?,k,"I am having trouble understanding, why the $k$-th derivative of a map $F\colon\mathbb R^n \to\mathbb R^m$ is a symmetric multilinear map for each $x$ in $\mathbb R^n$. Can you please explain which vectors this map accepts as input where multilinearity comes from ? Also, why is symmetry mentioned ? Thank you  readingframe","I am having trouble understanding, why the $k$-th derivative of a map $F\colon\mathbb R^n \to\mathbb R^m$ is a symmetric multilinear map for each $x$ in $\mathbb R^n$. Can you please explain which vectors this map accepts as input where multilinearity comes from ? Also, why is symmetry mentioned ? Thank you  readingframe",,"['multivariable-calculus', 'derivatives', 'multilinear-algebra']"
47,"Is there a ""geometric reason"" for why the gradient of $f(x,y)=\frac{x}{\sqrt{x^2+y^2}}$ is tangent to the unit circle?","Is there a ""geometric reason"" for why the gradient of  is tangent to the unit circle?","f(x,y)=\frac{x}{\sqrt{x^2+y^2}}","Consider $f(x,y)=\frac{x}{\sqrt{x^2+y^2}}$ defined on the unit $2$ -dimensional disk without the origin. A direct computation shows that $\nabla f(x,y)\perp (x,y)$ , so $\nabla f(x,y)$ is tangent to the circle of radius $\sqrt{{x^2+y^2}}$ at the point $(x,y)$ . Is this fact ""obvious"", when looking with the right glasses? I wonder it there is a way to ""see this"" with as little computation as possible. One intuition I do have is that taking an ""infinitesimal step"" on the circle keeps the denominator $\sqrt{{x^2+y^2}}$ constant while increasing the numerator. However, this does not seem to me a rigorous explanation, since it is not clear on advance why the maximal increase is obtained at this situation (there might be a better trade-off increasing the numerator and the denominator simultaneously). Is there a simple (rigorous) explanation? Here are two different explanations I have (I still wish for a justification which will make it seem more ""obvious""): The first is to use the formula for the gradient in polar coordinates. Since $f$ depends only on $\theta$ , the result follows. Edit: I now think that the fact $f$ depends only on $\theta$ provides sufficient intuition for the result, even without knowing the formula for the gradient. The point is that, since $f$ depends only on $\theta$ , it would be ""inefficient"" to change the radius as well; the best we can do is to choose the right angular direction-i.e. increasing or decreasing $\theta$ . The second is to recall that the gradient of $f$ at $(x_0,y_0)$ is normal to the level set of $f$ through $(x_0,y_0)$ , i.e. normal to $\{ (x,y) \, | \, f(x,y) =f(x_0,y_0) \}$ at $(x_0,y_0)$ . Geometrically, this level set corresponds to $\cos   \theta=\text{const}$ in polar coordinates, which is equivalent  (locally around $(x_0,y_0)$ ) to $\theta=\text{const}$ , i.e. the level set is just a ray passing through the origin, and it is visually clear that its tangent line at a point $(x_0,y_0)$ is itself- $\text{span}((x_0,y_0))$ .","Consider defined on the unit -dimensional disk without the origin. A direct computation shows that , so is tangent to the circle of radius at the point . Is this fact ""obvious"", when looking with the right glasses? I wonder it there is a way to ""see this"" with as little computation as possible. One intuition I do have is that taking an ""infinitesimal step"" on the circle keeps the denominator constant while increasing the numerator. However, this does not seem to me a rigorous explanation, since it is not clear on advance why the maximal increase is obtained at this situation (there might be a better trade-off increasing the numerator and the denominator simultaneously). Is there a simple (rigorous) explanation? Here are two different explanations I have (I still wish for a justification which will make it seem more ""obvious""): The first is to use the formula for the gradient in polar coordinates. Since depends only on , the result follows. Edit: I now think that the fact depends only on provides sufficient intuition for the result, even without knowing the formula for the gradient. The point is that, since depends only on , it would be ""inefficient"" to change the radius as well; the best we can do is to choose the right angular direction-i.e. increasing or decreasing . The second is to recall that the gradient of at is normal to the level set of through , i.e. normal to at . Geometrically, this level set corresponds to in polar coordinates, which is equivalent  (locally around ) to , i.e. the level set is just a ray passing through the origin, and it is visually clear that its tangent line at a point is itself- .","f(x,y)=\frac{x}{\sqrt{x^2+y^2}} 2 \nabla f(x,y)\perp (x,y) \nabla f(x,y) \sqrt{{x^2+y^2}} (x,y) \sqrt{{x^2+y^2}} f \theta f \theta f \theta \theta f (x_0,y_0) f (x_0,y_0) \{ (x,y) \, | \, f(x,y) =f(x_0,y_0) \} (x_0,y_0) \cos 
 \theta=\text{const} (x_0,y_0) \theta=\text{const} (x_0,y_0) \text{span}((x_0,y_0))","['multivariable-calculus', 'differential-geometry', 'soft-question', 'riemannian-geometry', 'alternative-proof']"
48,A tricky integration over the unit sphere,A tricky integration over the unit sphere,,"Please help me solve the following integral, $$ I=\int\limits_{\{x,y,z\}\in \mathbb{S}^2}\!\!\!\max\{0,x,x\cos{\theta}+y\sin{\theta}\}\,\mathrm dx\,\mathrm dy\,\mathrm dz $$ where $\mathbb{S}^2$ is a unit sphere and $\theta$ is some constant such that $0\le\theta\le2\pi$ . Numerically (up to arbitrary precision) this is equal to $$ I=\pi(1+\frac{\sqrt{(1-\cos{\theta})^2+\sin^2{\theta}}}{2}). $$ I am unable to solve/prove this analytically. This is for a research project, any help would be appreciated and acknowledged in the article.","Please help me solve the following integral, where is a unit sphere and is some constant such that . Numerically (up to arbitrary precision) this is equal to I am unable to solve/prove this analytically. This is for a research project, any help would be appreciated and acknowledged in the article.","
I=\int\limits_{\{x,y,z\}\in \mathbb{S}^2}\!\!\!\max\{0,x,x\cos{\theta}+y\sin{\theta}\}\,\mathrm dx\,\mathrm dy\,\mathrm dz
 \mathbb{S}^2 \theta 0\le\theta\le2\pi 
I=\pi(1+\frac{\sqrt{(1-\cos{\theta})^2+\sin^2{\theta}}}{2}).
","['multivariable-calculus', 'multiple-integral']"
49,Arguing the limit of a function using epsilon delta,Arguing the limit of a function using epsilon delta,,"So I have some function f(x,y) where $$f(x,y) = \frac{3xy^2}{x^2+y^4}$$ and I have used other ways to determine that $\lim \limits_{x,y \to 0,0} {f(x,y)}$ might be $0$. So now I want to use the epsilon-delta definition to prove that the limit is $0$. What I have so far is that $\delta > \sqrt{x^2 + y^2} > 0$ and $\epsilon > \frac{3|x|y^2}{x^2+y^4}$. My argument is that because $\sqrt{x^2+y^2}$ is always positive for $(x,y)$ in the domain of $f(x,y)$ then there exists a $\delta > 0$ for all $\epsilon > 0$ and therefore the limit is $0$. I was wondering if there is a flaw in this logic and if there is how it should be corrected. Thank you.","So I have some function f(x,y) where $$f(x,y) = \frac{3xy^2}{x^2+y^4}$$ and I have used other ways to determine that $\lim \limits_{x,y \to 0,0} {f(x,y)}$ might be $0$. So now I want to use the epsilon-delta definition to prove that the limit is $0$. What I have so far is that $\delta > \sqrt{x^2 + y^2} > 0$ and $\epsilon > \frac{3|x|y^2}{x^2+y^4}$. My argument is that because $\sqrt{x^2+y^2}$ is always positive for $(x,y)$ in the domain of $f(x,y)$ then there exists a $\delta > 0$ for all $\epsilon > 0$ and therefore the limit is $0$. I was wondering if there is a flaw in this logic and if there is how it should be corrected. Thank you.",,"['multivariable-calculus', 'proof-verification']"
50,How to add the derivative of a matrix to the chain rule?,How to add the derivative of a matrix to the chain rule?,,"In machine learning, I'm optimizing a parameter matrix $W$ . The loss function is $$L=f(y),$$ where $L$ is a scalar, $y=Wx$ , $x\in \mathbb{R}^n$ , $y\in \mathbb{R}^m$ and the order of $W$ is $m\times n$ . In all math textbooks, it is usually $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial L}{\partial y}W.$$ Where $\dfrac{\partial L}{\partial y}$ is a $1\times m$ vector. This is quite easy to understand. However, in machine learning, $x$ is the input and $W$ is the parameter matrix to optimize, it should be $$\frac{\partial L}{\partial W}=\frac{\partial L}{\partial y}\frac{\partial y}{\partial W}.$$ But what is $\dfrac{\partial y}{\partial W}$ ? Is it $x$ ? Is it correct? According to wikipedia, the derivative of a scalar to a matrix is a matrix \begin{equation*} \frac{\partial L}{\partial W} =  \begin{pmatrix} \frac{\partial L}{\partial W_{11}} & \frac{\partial L}{\partial W_{21}} & \cdots & \frac{\partial L}{\partial W_{m1}} \\ \frac{\partial L}{\partial W_{12}} & \frac{\partial L}{\partial W_{22}} & \cdots & \frac{\partial L}{\partial W_{m2}} \\ \vdots  & \vdots  & \ddots & \vdots  \\ \frac{\partial L}{\partial W_{1n}} & \frac{\partial L}{\partial W_{2n}} & \cdots & \frac{\partial L}{\partial W_{mn}}  \end{pmatrix} \end{equation*} where $$\frac{\partial L}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}x_i$$ therefore \begin{equation*} \frac{\partial L}{\partial W} =  \begin{pmatrix} \frac{\partial L}{\partial y_1}x_1 & \frac{\partial L}{\partial y_2}x_1 & \cdots & \frac{\partial L}{\partial y_m}x_1 \\ \frac{\partial L}{\partial y_1}x_2 & \frac{\partial L}{\partial y_2}x_2 & \cdots & \frac{\partial L}{\partial y_m}x_2 \\ \vdots  & \vdots  & \ddots & \vdots  \\ \frac{\partial L}{\partial y_1}x_n & \frac{\partial L}{\partial y_2}x_n & \cdots & \frac{\partial L}{\partial y_m}x_n \\  \end{pmatrix} \end{equation*} Does this even fit the chain rule? To fit the chain rule $$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W}$$ $\dfrac{\partial L}{\partial W}$ is a $n*m$ matrix, $\dfrac{\partial L}{\partial y}$ is a $1\times m$ vector, how to fit it? PS: I just found there is an operation called kronecker product, and $\dfrac{\partial L}{\partial W}$ can be written as $\dfrac{\partial L}{\partial y}\bigotimes x$ , but this is still beyond me. First, why does the chain rule lead to kronecker product? Isn't the chain rule about matrix multiplication? Second, does this mean $\dfrac{\partial y}{\partial W} = x$ ? I didn't see the definition of the derivative of a vector to a matrix in wikipedia. The third and most important question is, even I know the derivative $\dfrac{\partial L}{\partial W}$ , how should I update my parameter matrix? We all know the gradient descent works because of directional derivative $$\nabla_v f = \frac{\partial f}{\partial v}v$$ so we should take the negative gradient direction to lower $f$ . Does this even exist for the derivative of a matrix? I mean $\dfrac{\partial L}{\partial W}$ multiplies $\Delta W$ won't reproduce $\Delta L$ anyway.","In machine learning, I'm optimizing a parameter matrix . The loss function is where is a scalar, , , and the order of is . In all math textbooks, it is usually Where is a vector. This is quite easy to understand. However, in machine learning, is the input and is the parameter matrix to optimize, it should be But what is ? Is it ? Is it correct? According to wikipedia, the derivative of a scalar to a matrix is a matrix where therefore Does this even fit the chain rule? To fit the chain rule is a matrix, is a vector, how to fit it? PS: I just found there is an operation called kronecker product, and can be written as , but this is still beyond me. First, why does the chain rule lead to kronecker product? Isn't the chain rule about matrix multiplication? Second, does this mean ? I didn't see the definition of the derivative of a vector to a matrix in wikipedia. The third and most important question is, even I know the derivative , how should I update my parameter matrix? We all know the gradient descent works because of directional derivative so we should take the negative gradient direction to lower . Does this even exist for the derivative of a matrix? I mean multiplies won't reproduce anyway.","W L=f(y), L y=Wx x\in \mathbb{R}^n y\in \mathbb{R}^m W m\times n \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial L}{\partial y}W. \dfrac{\partial L}{\partial y} 1\times m x W \frac{\partial L}{\partial W}=\frac{\partial L}{\partial y}\frac{\partial y}{\partial W}. \dfrac{\partial y}{\partial W} x \begin{equation*}
\frac{\partial L}{\partial W} = 
\begin{pmatrix}
\frac{\partial L}{\partial W_{11}} & \frac{\partial L}{\partial W_{21}} & \cdots & \frac{\partial L}{\partial W_{m1}} \\
\frac{\partial L}{\partial W_{12}} & \frac{\partial L}{\partial W_{22}} & \cdots & \frac{\partial L}{\partial W_{m2}} \\
\vdots  & \vdots  & \ddots & \vdots  \\
\frac{\partial L}{\partial W_{1n}} & \frac{\partial L}{\partial W_{2n}} & \cdots & \frac{\partial L}{\partial W_{mn}} 
\end{pmatrix}
\end{equation*} \frac{\partial L}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}x_i \begin{equation*}
\frac{\partial L}{\partial W} = 
\begin{pmatrix}
\frac{\partial L}{\partial y_1}x_1 & \frac{\partial L}{\partial y_2}x_1 & \cdots & \frac{\partial L}{\partial y_m}x_1 \\
\frac{\partial L}{\partial y_1}x_2 & \frac{\partial L}{\partial y_2}x_2 & \cdots & \frac{\partial L}{\partial y_m}x_2 \\
\vdots  & \vdots  & \ddots & \vdots  \\
\frac{\partial L}{\partial y_1}x_n & \frac{\partial L}{\partial y_2}x_n & \cdots & \frac{\partial L}{\partial y_m}x_n \\ 
\end{pmatrix}
\end{equation*} \frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W} \dfrac{\partial L}{\partial W} n*m \dfrac{\partial L}{\partial y} 1\times m \dfrac{\partial L}{\partial W} \dfrac{\partial L}{\partial y}\bigotimes x \dfrac{\partial y}{\partial W} = x \dfrac{\partial L}{\partial W} \nabla_v f = \frac{\partial f}{\partial v}v f \dfrac{\partial L}{\partial W} \Delta W \Delta L","['multivariable-calculus', 'derivatives', 'partial-derivative', 'chain-rule', 'backpropagation']"
51,Dot product between two vectors in cylindrical coordinates?,Dot product between two vectors in cylindrical coordinates?,,"Let's say I have two Vectors in cylindrical coordinates: $\vec{A} = A_r \hat{r} + A_\theta \hat{\theta} + A_z \hat{z}$ $\vec{B} = B_r \hat{r} + B_\theta \hat{\theta} + B_z \hat{z}$ How do I calculate the dot product of these two vectors specified in cylindrical coordinates? Is it the same as in Cartesian coordinates where you just multiply the components of the same basis vector together and then sum all the products together. Example: $\vec{A} \bullet \vec{B} = (A_r \hat{r} + A_\theta \hat{\theta} + A_z \hat{z}) \bullet (B_r \hat{r} + B_\theta \hat{\theta} + B_z \hat{z})$ $\vec{A} \bullet \vec{B} = (A_rB_r + A_\theta B_\theta + A_z B_z)$ I was a little bit confused because Gradiant, Div, and Curl all have formulas that are specific to the coordinate system being used... but I was not sure if anything special needed to be done when finding the dot product between two ordinary vectors in cylindrical coordinates. If I convert to cartesian coordinates: $\vec{A} = A_r \cos A_\theta \hat{\text{i}} + A_r \sin A_\theta \hat{\text{j}} + A_z \hat{\text{k}}$ $\vec{B} = B_r \cos B_\theta \hat{\text{i}} + B_r \sin B_\theta \hat{\text{j}} + B_z \hat{\text{k}}$ $\vec{A} \bullet \vec{B} = A_r B_r \cos A_\theta \cos B_\theta +  A_r B_r \sin A_\theta \sin B_\theta + A_z B_z$ At this point, i'm wondering if there's a trig identity that will convert this expression into this expression: $\vec{A} \bullet \vec{B} = (A_rB_r + A_\theta B_\theta + A_z B_z)$ One thought is using this trig identity: $\cos(X-Y)=\cos(X)\cos(Y)+\sin(X)\sin(Y)$ $\vec{A} \bullet \vec{B} = A_r B_r (\cos A_\theta \cos B_\theta +  \sin A_\theta \sin B_\theta) + A_z B_z$ $\vec{A} \bullet \vec{B} = A_r B_r \cos (A_\theta - B_\theta) + A_z B_z$ Did I make any mistakes?","Let's say I have two Vectors in cylindrical coordinates: How do I calculate the dot product of these two vectors specified in cylindrical coordinates? Is it the same as in Cartesian coordinates where you just multiply the components of the same basis vector together and then sum all the products together. Example: I was a little bit confused because Gradiant, Div, and Curl all have formulas that are specific to the coordinate system being used... but I was not sure if anything special needed to be done when finding the dot product between two ordinary vectors in cylindrical coordinates. If I convert to cartesian coordinates: At this point, i'm wondering if there's a trig identity that will convert this expression into this expression: One thought is using this trig identity: Did I make any mistakes?",\vec{A} = A_r \hat{r} + A_\theta \hat{\theta} + A_z \hat{z} \vec{B} = B_r \hat{r} + B_\theta \hat{\theta} + B_z \hat{z} \vec{A} \bullet \vec{B} = (A_r \hat{r} + A_\theta \hat{\theta} + A_z \hat{z}) \bullet (B_r \hat{r} + B_\theta \hat{\theta} + B_z \hat{z}) \vec{A} \bullet \vec{B} = (A_rB_r + A_\theta B_\theta + A_z B_z) \vec{A} = A_r \cos A_\theta \hat{\text{i}} + A_r \sin A_\theta \hat{\text{j}} + A_z \hat{\text{k}} \vec{B} = B_r \cos B_\theta \hat{\text{i}} + B_r \sin B_\theta \hat{\text{j}} + B_z \hat{\text{k}} \vec{A} \bullet \vec{B} = A_r B_r \cos A_\theta \cos B_\theta +  A_r B_r \sin A_\theta \sin B_\theta + A_z B_z \vec{A} \bullet \vec{B} = (A_rB_r + A_\theta B_\theta + A_z B_z) \cos(X-Y)=\cos(X)\cos(Y)+\sin(X)\sin(Y) \vec{A} \bullet \vec{B} = A_r B_r (\cos A_\theta \cos B_\theta +  \sin A_\theta \sin B_\theta) + A_z B_z \vec{A} \bullet \vec{B} = A_r B_r \cos (A_\theta - B_\theta) + A_z B_z,"['multivariable-calculus', 'trigonometry', 'vector-analysis']"
52,Differentiation of one function with respect to another in multivariable calculus?,Differentiation of one function with respect to another in multivariable calculus?,,I want to differentiate the function say $g(x)=x^2+y^2$ with respect to the function $f(x)=xy$ .  But I haven't been able to figure it out. Though I have derived the formula $$\frac{dg}{df}=\frac{\left(\frac{\partial g}{\partial x}\right)\left(\frac{\partial g}{\partial y}\right)}{x\left(\frac{\partial g}{\partial x}\right)+y\left(\frac{\partial g}{\partial y}\right)}$$ But I don't even know whether it is correct or NOT(highlighted because that's most probable outcome),I want to differentiate the function say with respect to the function .  But I haven't been able to figure it out. Though I have derived the formula But I don't even know whether it is correct or NOT(highlighted because that's most probable outcome),g(x)=x^2+y^2 f(x)=xy \frac{dg}{df}=\frac{\left(\frac{\partial g}{\partial x}\right)\left(\frac{\partial g}{\partial y}\right)}{x\left(\frac{\partial g}{\partial x}\right)+y\left(\frac{\partial g}{\partial y}\right)},['multivariable-calculus']
53,Divergence of a vector tensor product/outer product: $ u \bullet \nabla u = \nabla \bullet (u \otimes u) $,Divergence of a vector tensor product/outer product:, u \bullet \nabla u = \nabla \bullet (u \otimes u) ,"I'm currently studying the derivation of the RANS (Reynolds Averaged Navier Stokes) equations, used in the study of turbulence, and I've stumbled upon a step wich I don't understand very well. The problem is on the following identity (one of the steps of the formal derivation of the RANS equations, present in many online texts): $$ u \bullet \nabla u = \nabla \bullet (u \otimes u) $$ , where $ u = \{ u \; v \; w \} $ is a 3D vector and $ u \otimes u $ is the tensor dot product / outer product of vector $u$ by itself: $$ u \otimes u =          \begin{pmatrix}         u^2 & uv & uw \\         vu & v^2 & vw \\         wu & wv & w^2 \\         \end{pmatrix} $$ My question is: where can I find a proof of the previous equality? When I try to derive it by hand I don't end up with the result above (I'm probably commiting an error somewhere). Can someone derive it (in a simple way) here by hand? I've searched online, but I haven't found anything this specific.","I'm currently studying the derivation of the RANS (Reynolds Averaged Navier Stokes) equations, used in the study of turbulence, and I've stumbled upon a step wich I don't understand very well. The problem is on the following identity (one of the steps of the formal derivation of the RANS equations, present in many online texts): $$ u \bullet \nabla u = \nabla \bullet (u \otimes u) $$ , where $ u = \{ u \; v \; w \} $ is a 3D vector and $ u \otimes u $ is the tensor dot product / outer product of vector $u$ by itself: $$ u \otimes u =          \begin{pmatrix}         u^2 & uv & uw \\         vu & v^2 & vw \\         wu & wv & w^2 \\         \end{pmatrix} $$ My question is: where can I find a proof of the previous equality? When I try to derive it by hand I don't end up with the result above (I'm probably commiting an error somewhere). Can someone derive it (in a simple way) here by hand? I've searched online, but I haven't found anything this specific.",,"['multivariable-calculus', 'tensor-products']"
54,Proving maximum of dot product using derivatives,Proving maximum of dot product using derivatives,,"I am curious to know whether there is a way to prove that the maximum of the dot product occurs when two vectors are parallel to each other using derivatives. In particular, given: $c = \textbf{a}\cdot\textbf{b}$ with $\textbf{a},\textbf{b}\in\mathbb{R}^3$ Assuming that $\textbf{b}$ is fixed, and I can only change $\textbf{a} = (x,y,z)^T$, how would one go about it? I would not even know how to properly take the derivatives in this case. This is not homework and because of that the question as I put it might be ill-posed, I apologise in advance.","I am curious to know whether there is a way to prove that the maximum of the dot product occurs when two vectors are parallel to each other using derivatives. In particular, given: $c = \textbf{a}\cdot\textbf{b}$ with $\textbf{a},\textbf{b}\in\mathbb{R}^3$ Assuming that $\textbf{b}$ is fixed, and I can only change $\textbf{a} = (x,y,z)^T$, how would one go about it? I would not even know how to properly take the derivatives in this case. This is not homework and because of that the question as I put it might be ill-posed, I apologise in advance.",,['multivariable-calculus']
55,Books on differential geometry in the cases $n=2$ and $n=3$,Books on differential geometry in the cases  and,n=2 n=3,"I'm interested in learning the differential geometry of standard, ""physical"" space, that is $\mathbb R^2$ and $\mathbb R^3$. The sort of problems that were studied in the 18th and 19th century... curvature and so on. NJ Wildberger's lectures give an idea of the sort of content I'm looking for. I'm interested in books with a very down-to-earth, physical way of approaching things. Books that encourage thinking of a curve as a line through physical space, rather than a particular set of ordered triples, if you see what I mean. To give a better idea of my needs, I've been struggling with multivariable calculus because all the definitions are in the context of $\mathbb R^n$ for arbitrary $n$, and I find it all very abstract and unmotivated. I feel I would get a better grip on the situation if I spent some time really deepening my appreciation for the concepts as they apply to familiar $2$ and $3$ dimensional geometry.","I'm interested in learning the differential geometry of standard, ""physical"" space, that is $\mathbb R^2$ and $\mathbb R^3$. The sort of problems that were studied in the 18th and 19th century... curvature and so on. NJ Wildberger's lectures give an idea of the sort of content I'm looking for. I'm interested in books with a very down-to-earth, physical way of approaching things. Books that encourage thinking of a curve as a line through physical space, rather than a particular set of ordered triples, if you see what I mean. To give a better idea of my needs, I've been struggling with multivariable calculus because all the definitions are in the context of $\mathbb R^n$ for arbitrary $n$, and I find it all very abstract and unmotivated. I feel I would get a better grip on the situation if I spent some time really deepening my appreciation for the concepts as they apply to familiar $2$ and $3$ dimensional geometry.",,"['multivariable-calculus', 'differential-geometry', 'reference-request', 'book-recommendation']"
56,Taylor expansion of a vector field (notation question),Taylor expansion of a vector field (notation question),,"Is there an index-less notation (using gradiends, Jacobians, curls, hessians, anything) to describe a second-order term in the Taylor expansion of a vector field $\mathbf{f}(\mathbf{x}): \mathbb{R}^n \to \mathbb{R}^n$? Up to the linear terms, it's nicely written using the Jacobian matrix: $$ \mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{x_0}) + D_{\mathbf{x}}\mathbf{f}(\mathbf{x_0})(\mathbf{x} - \mathbf{x_0}) + O(\mathbf{x}^2) $$ I don't know how is it better to express the quadratic part without ugly indices: $$ O(\mathbf{x}^2)_j = \frac{1}{2}(\mathbf{x} - \mathbf{x_0})^TD^2f_j(\mathbf{x_0})(\mathbf{x} - \mathbf{x_0}) + O(\mathbf{x}^3)_j $$","Is there an index-less notation (using gradiends, Jacobians, curls, hessians, anything) to describe a second-order term in the Taylor expansion of a vector field $\mathbf{f}(\mathbf{x}): \mathbb{R}^n \to \mathbb{R}^n$? Up to the linear terms, it's nicely written using the Jacobian matrix: $$ \mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{x_0}) + D_{\mathbf{x}}\mathbf{f}(\mathbf{x_0})(\mathbf{x} - \mathbf{x_0}) + O(\mathbf{x}^2) $$ I don't know how is it better to express the quadratic part without ugly indices: $$ O(\mathbf{x}^2)_j = \frac{1}{2}(\mathbf{x} - \mathbf{x_0})^TD^2f_j(\mathbf{x_0})(\mathbf{x} - \mathbf{x_0}) + O(\mathbf{x}^3)_j $$",,"['multivariable-calculus', 'taylor-expansion']"
57,A question on generalization of the concept of derivative,A question on generalization of the concept of derivative,,"I am looking for some material to understand the process of generalization of the concept of derivative. I would not like to just read and apply the definition of the concept of differentiation in order to comprehend this generalization. I would like to work with Differential Calculus fluently so please forgive me if this is not a high-level question. What I've read is that for functions $f:\mathbb R\to\mathbb R$ it would be equivalent the fact that the limit $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h} $$ exists and the existence of a (unique) linear map $\lambda:\mathbb R\to\mathbb R$ (depending on $x_0$) such that $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)-\lambda(h)}{h} = 0. $$ Correct me if I'm wrong but only in the case $f:\mathbb R^1 \to \mathbb R^1$ and because of the definition of the function $f'$, it would be $\lambda(h):=h f'(x_0)$. But we use the equivalence with the next equation in order to generalize the concept for $\mathbb R^n\to \mathbb R$ (and also $\mathbb R^n\to\mathbb R^m$) functions: $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)-\lambda(h)}{\lVert h\rVert} = 0\;\: (\in\mathbb R), $$ which does not depend on the undefined operation of division $\frac{1}{h}$ in $\mathbb R^n$ (why then not requiring the existence of the limit $\lim_{h\to 0}(f(x_0+h)-f(x_0))/{\lVert h\rVert}$?). Is this correct? Have I committed a lot of inaccuracies? Concluding, what I would like to mean by this question is that I want to understand every step taken in the method of generalization of the concept of derivative. Thanks in advance.","I am looking for some material to understand the process of generalization of the concept of derivative. I would not like to just read and apply the definition of the concept of differentiation in order to comprehend this generalization. I would like to work with Differential Calculus fluently so please forgive me if this is not a high-level question. What I've read is that for functions $f:\mathbb R\to\mathbb R$ it would be equivalent the fact that the limit $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h} $$ exists and the existence of a (unique) linear map $\lambda:\mathbb R\to\mathbb R$ (depending on $x_0$) such that $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)-\lambda(h)}{h} = 0. $$ Correct me if I'm wrong but only in the case $f:\mathbb R^1 \to \mathbb R^1$ and because of the definition of the function $f'$, it would be $\lambda(h):=h f'(x_0)$. But we use the equivalence with the next equation in order to generalize the concept for $\mathbb R^n\to \mathbb R$ (and also $\mathbb R^n\to\mathbb R^m$) functions: $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)-\lambda(h)}{\lVert h\rVert} = 0\;\: (\in\mathbb R), $$ which does not depend on the undefined operation of division $\frac{1}{h}$ in $\mathbb R^n$ (why then not requiring the existence of the limit $\lim_{h\to 0}(f(x_0+h)-f(x_0))/{\lVert h\rVert}$?). Is this correct? Have I committed a lot of inaccuracies? Concluding, what I would like to mean by this question is that I want to understand every step taken in the method of generalization of the concept of derivative. Thanks in advance.",,"['reference-request', 'multivariable-calculus', 'derivatives']"
58,$\nabla^2u=0$ implies every critical point is a saddle point,implies every critical point is a saddle point,\nabla^2u=0,Hi everybody I need help with this problem: let $u:R^n \rightarrow R$ be a function so that $\nabla^2u=0$ prove that every critical point of the function is a saddle point.,Hi everybody I need help with this problem: let $u:R^n \rightarrow R$ be a function so that $\nabla^2u=0$ prove that every critical point of the function is a saddle point.,,['multivariable-calculus']
59,Taylor-like expansion for multivariable functions,Taylor-like expansion for multivariable functions,,"Is there any analogue for taylor series for multivariable functions? In other words, can we rewrite any function as a sum of algebraic terms? For example, $x^y$. Can it be written of the form $\sum C_{m,n}x^my^n$, where $C_{m,n}$ is some constant pertaining to the particular m,n (most probably in terms of $\frac{\partial^m}{\partial x^m}x^y$ etc). Is there a generalization for more then two variables? Another example would be $\frac{x+y}{x^2+y^2}$. I suspect that it can be derived by using partial differentials and mashing together the taylor expansions of $f(x,constant)$ and $f(constant,y)$, but I can't manage to do it.","Is there any analogue for taylor series for multivariable functions? In other words, can we rewrite any function as a sum of algebraic terms? For example, $x^y$. Can it be written of the form $\sum C_{m,n}x^my^n$, where $C_{m,n}$ is some constant pertaining to the particular m,n (most probably in terms of $\frac{\partial^m}{\partial x^m}x^y$ etc). Is there a generalization for more then two variables? Another example would be $\frac{x+y}{x^2+y^2}$. I suspect that it can be derived by using partial differentials and mashing together the taylor expansions of $f(x,constant)$ and $f(constant,y)$, but I can't manage to do it.",,['multivariable-calculus']
60,Proving that $\iint\limits_{x^2+y^2\leq 1} e^x \cos y dxdy=\pi$,Proving that,\iint\limits_{x^2+y^2\leq 1} e^x \cos y dxdy=\pi,"I want to prove that  $$\iint\limits_{x^2+y^2<1} u(x,y) dxdy=\pi$$ where $u(x,y)=e^x \cos y$. There is a theorem which says that if $u\in C^2(\Omega)$ and $\nabla^2 u=0$ in a domain $\Omega\subseteq \mathbb{R}^n$, then for any ball $B=B_R(v)$ with $\overline{B}\subset\Omega$,  $$u(v)=\frac{1}{\omega_n R^n}\int_B u dx$$ where $\omega_n$ is the volume of the ball of radius 1 in $\mathbb{R}^n$. The double integral above can be seen as a particular case of the theorem, since $\omega_2=\pi$, $R=1$ and $u(0,0)=1$. It's also clear that $\nabla^2 u=0$ (It's the real part of $e^z,z\in\mathbb{C}$). I want to prove it without using this mean value theorem. In a standard way, I get to the integral $$2\int_{-1}^1 e^x\sin \sqrt{1-x^2}dx$$ which seems crazy. Numerically it seems to be $\pi$ efectively. How could I calculate the integral?","I want to prove that  $$\iint\limits_{x^2+y^2<1} u(x,y) dxdy=\pi$$ where $u(x,y)=e^x \cos y$. There is a theorem which says that if $u\in C^2(\Omega)$ and $\nabla^2 u=0$ in a domain $\Omega\subseteq \mathbb{R}^n$, then for any ball $B=B_R(v)$ with $\overline{B}\subset\Omega$,  $$u(v)=\frac{1}{\omega_n R^n}\int_B u dx$$ where $\omega_n$ is the volume of the ball of radius 1 in $\mathbb{R}^n$. The double integral above can be seen as a particular case of the theorem, since $\omega_2=\pi$, $R=1$ and $u(0,0)=1$. It's also clear that $\nabla^2 u=0$ (It's the real part of $e^z,z\in\mathbb{C}$). I want to prove it without using this mean value theorem. In a standard way, I get to the integral $$2\int_{-1}^1 e^x\sin \sqrt{1-x^2}dx$$ which seems crazy. Numerically it seems to be $\pi$ efectively. How could I calculate the integral?",,['multivariable-calculus']
61,Finding the shortest distance between an arbitrary point and a parabola,Finding the shortest distance between an arbitrary point and a parabola,,"I'm attempting to find the shortest distance between a point and a parabola. The point in question is (0,b), for any b, and the parabola that we are given is$\ y = x^2 $. How would you approach the problem and find the shortest distance for any given b? What about if the point was (0,0,b), and the equation was $\ z = x^2 + y^2$?","I'm attempting to find the shortest distance between a point and a parabola. The point in question is (0,b), for any b, and the parabola that we are given is$\ y = x^2 $. How would you approach the problem and find the shortest distance for any given b? What about if the point was (0,0,b), and the equation was $\ z = x^2 + y^2$?",,['multivariable-calculus']
62,Time derivative of flux,Time derivative of flux,,"We have a time and even ""position"" invariant vector field and a surface. If the surface is moving with constant velocity, is the flux through the surface should constant in time? Also, is there an easy to follow proof for the formula $$\frac{d}{dt} \iint_{S(t)}\overline{V}(\overline{r},t) \cdot d\overline{A}  $$","We have a time and even ""position"" invariant vector field and a surface. If the surface is moving with constant velocity, is the flux through the surface should constant in time? Also, is there an easy to follow proof for the formula $$\frac{d}{dt} \iint_{S(t)}\overline{V}(\overline{r},t) \cdot d\overline{A}  $$",,['multivariable-calculus']
63,Show that $\nabla^2(fg)= f\nabla^2g+g\nabla^2f+2\nabla f\cdot\nabla g$,Show that,\nabla^2(fg)= f\nabla^2g+g\nabla^2f+2\nabla f\cdot\nabla g,"How do I show that $$\nabla^2(fg) =  f \nabla^2g + g\nabla^2f + 2 \nabla f \cdot \nabla g$$ Any help would be greatly appreciated! Regards, Andrew","How do I show that $$\nabla^2(fg) =  f \nabla^2g + g\nabla^2f + 2 \nabla f \cdot \nabla g$$ Any help would be greatly appreciated! Regards, Andrew",,[]
64,Change of variables Double integral,Change of variables Double integral,,"I have  $$\iint_A \frac{1}{(x^2+y^2)^2}\,dx\,dy.$$ $A$ is bounded by the conditions $x^2 + y^2 \leq 1$ and $x+y \geq 1$. I initially thought to make the switch the polar coordinates, but the line $x+y=1$ is making it hard to find the limits of integration.","I have  $$\iint_A \frac{1}{(x^2+y^2)^2}\,dx\,dy.$$ $A$ is bounded by the conditions $x^2 + y^2 \leq 1$ and $x+y \geq 1$. I initially thought to make the switch the polar coordinates, but the line $x+y=1$ is making it hard to find the limits of integration.",,"['multivariable-calculus', 'definite-integrals', 'integral-transforms', 'bounds-of-integration']"
65,Formulae for PDEs : Commuting derivatives and/or integrals,Formulae for PDEs : Commuting derivatives and/or integrals,,"Many times I come across some new formula being used to work with and/or reduce partial differentials. As kleingordon said , these things are mysteriously not taught anywhere(atleast in physics courses). I can't find any list on the internet, either. I'm talking about formulae like these: $$\frac{\mathrm{d}}{\mathrm{d}\alpha}\int f(x,\alpha) \mathrm{d}x=\int\frac{\partial f(x,\alpha)}{\partial \alpha}\mathrm{d}x$$ $$\frac{\partial}{\partial x}\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}\frac{\partial f}{\partial x}$$ (for continuous functions) I've also seen that you can stuff a derivative inside a PD $$ \frac{\rm d}{\rm dt}\left(\frac{\partial f}{\partial x}\right)=\frac{\partial \dot f}{\partial x}$$ (Note-$\dot f=\frac{\rm df}{\rm dt}$) There's also a formula that allows one to split a function into a sum of partial derivatives. I think this is the multivariable chain rule. I'd like a list of such formulae, or links to these lists. Books are also fine, though I'd prfer internet sources.","Many times I come across some new formula being used to work with and/or reduce partial differentials. As kleingordon said , these things are mysteriously not taught anywhere(atleast in physics courses). I can't find any list on the internet, either. I'm talking about formulae like these: $$\frac{\mathrm{d}}{\mathrm{d}\alpha}\int f(x,\alpha) \mathrm{d}x=\int\frac{\partial f(x,\alpha)}{\partial \alpha}\mathrm{d}x$$ $$\frac{\partial}{\partial x}\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}\frac{\partial f}{\partial x}$$ (for continuous functions) I've also seen that you can stuff a derivative inside a PD $$ \frac{\rm d}{\rm dt}\left(\frac{\partial f}{\partial x}\right)=\frac{\partial \dot f}{\partial x}$$ (Note-$\dot f=\frac{\rm df}{\rm dt}$) There's also a formula that allows one to split a function into a sum of partial derivatives. I think this is the multivariable chain rule. I'd like a list of such formulae, or links to these lists. Books are also fine, though I'd prfer internet sources.",,"['reference-request', 'multivariable-calculus', 'partial-differential-equations', 'big-list']"
66,Understanding of the Mean Value Theorem in PDE,Understanding of the Mean Value Theorem in PDE,,"I learned the following theorem in Folland's Introduction to Partial Differential Equations (p.69 Chapter 2): Suppose $u$ is harmonic on an open set $\Omega\subset{\mathbb R}^n$. If $x\in\Omega$ and $r>0$  is small enough so that $\overline{B_r(x)}\subset\Omega$, then   $$u(x)=\frac{1}{r^{n-1}\omega_n}\int_{S_r(x)}u(y)d\sigma(y)=\frac{1}{\omega_n}\int_{S_1(0)}u(x+ry)d\sigma(y)，$$   where $$\omega_n=\frac{2\pi^{n/2}}{\Gamma(n/2)}.$$ I found that I could not immediately reconstruct a proof for the theorem. A key point is that one needs to use the Green's identity, which is a basic property of harmonic functions. But I don't see any ""clue"" that how people actually come up with this theorem and such proof. (Maybe this is the common problem, at least for me, for most of the textbooks.) A curious search in Google returns nothing satisfactory to me. Since this is a basic property of harmonic functions , I am wondering that if one needs to know this history of harmonic functions in order to know this theorem well. Here is my question: Can any one here come up with a motivation of this theorem in PDE? My second question may be more vague: How can I approach the proof of this theorem more ""naturally"" instead of just remembering bunch of facts ? (In the language of Polya, any heuristics here?)","I learned the following theorem in Folland's Introduction to Partial Differential Equations (p.69 Chapter 2): Suppose $u$ is harmonic on an open set $\Omega\subset{\mathbb R}^n$. If $x\in\Omega$ and $r>0$  is small enough so that $\overline{B_r(x)}\subset\Omega$, then   $$u(x)=\frac{1}{r^{n-1}\omega_n}\int_{S_r(x)}u(y)d\sigma(y)=\frac{1}{\omega_n}\int_{S_1(0)}u(x+ry)d\sigma(y)，$$   where $$\omega_n=\frac{2\pi^{n/2}}{\Gamma(n/2)}.$$ I found that I could not immediately reconstruct a proof for the theorem. A key point is that one needs to use the Green's identity, which is a basic property of harmonic functions. But I don't see any ""clue"" that how people actually come up with this theorem and such proof. (Maybe this is the common problem, at least for me, for most of the textbooks.) A curious search in Google returns nothing satisfactory to me. Since this is a basic property of harmonic functions , I am wondering that if one needs to know this history of harmonic functions in order to know this theorem well. Here is my question: Can any one here come up with a motivation of this theorem in PDE? My second question may be more vague: How can I approach the proof of this theorem more ""naturally"" instead of just remembering bunch of facts ? (In the language of Polya, any heuristics here?)",,"['soft-question', 'multivariable-calculus']"
67,Laplacian of a Function depending on r in Polar Coordinates,Laplacian of a Function depending on r in Polar Coordinates,,"From a bank of exams: Let $u(x,y) = f(r)$ be a smooth   function in the plane that depends   only on $r = \sqrt{x^2 + y^2}$.   Compute $\Delta u = u_{xx} + u_{yy}$   in terms of $f$ and its derivatives. Wikipedia states that the Laplace operator in polar coordinates is $$\Delta f = \frac{1}{r}\frac{\partial f}{\partial r} \left( r \frac{\partial f}{\partial r} \right) + \frac{1}{r^2}\frac{\partial^2 f}{\partial \theta^2},$$ which I suppose I could memorize directly, but I thought there might be an easier way. I tried to prove this directly, by thinking that $$ u_{xx} = \frac{d^2f}{dr^2} \frac{\partial r}{\partial x} + \frac{df}{dr} \frac{\partial ^2r}{\partial x^2}$$ and  $$ u_{yy} = \frac{d^2f}{dr^2} \frac{\partial r}{\partial y} + \frac{df}{dr} \frac{\partial ^2r}{\partial y^2}.$$  But then I get stuck at $$ u_{xx} + u_{yy} = \frac{d^2f}{dr^2} \frac{x+y}{\sqrt{x^2+y^2}} + \frac{df}{dr}\frac{1}{\sqrt{x^2+y^2}} =  \frac{d^2f}{dr^2} \frac{r(\cos \theta + \sin \theta)}{r} + \frac{df}{dr}\frac{1}{r}.$$ Any idea on where I'm going wrong? It looks like I need $\displaystyle{\frac{r(\cos \theta + \sin \theta)}{r} = 1}$.","From a bank of exams: Let $u(x,y) = f(r)$ be a smooth   function in the plane that depends   only on $r = \sqrt{x^2 + y^2}$.   Compute $\Delta u = u_{xx} + u_{yy}$   in terms of $f$ and its derivatives. Wikipedia states that the Laplace operator in polar coordinates is $$\Delta f = \frac{1}{r}\frac{\partial f}{\partial r} \left( r \frac{\partial f}{\partial r} \right) + \frac{1}{r^2}\frac{\partial^2 f}{\partial \theta^2},$$ which I suppose I could memorize directly, but I thought there might be an easier way. I tried to prove this directly, by thinking that $$ u_{xx} = \frac{d^2f}{dr^2} \frac{\partial r}{\partial x} + \frac{df}{dr} \frac{\partial ^2r}{\partial x^2}$$ and  $$ u_{yy} = \frac{d^2f}{dr^2} \frac{\partial r}{\partial y} + \frac{df}{dr} \frac{\partial ^2r}{\partial y^2}.$$  But then I get stuck at $$ u_{xx} + u_{yy} = \frac{d^2f}{dr^2} \frac{x+y}{\sqrt{x^2+y^2}} + \frac{df}{dr}\frac{1}{\sqrt{x^2+y^2}} =  \frac{d^2f}{dr^2} \frac{r(\cos \theta + \sin \theta)}{r} + \frac{df}{dr}\frac{1}{r}.$$ Any idea on where I'm going wrong? It looks like I need $\displaystyle{\frac{r(\cos \theta + \sin \theta)}{r} = 1}$.",,"['multivariable-calculus', 'polar-coordinates']"
68,Proving $abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0$,Proving,abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0,"The question is this: If $a\ge b\ge c\ge 0$ and $a^2+b^2+c^2=3$ , then prove that $$abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0$$ For my work on this inequality, I have proved already under constraints that it is true. Proof for: $\sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0.$ $$ \sqrt{3}abc +  \sqrt{2}a  - \sqrt{3} - \sqrt{2}c  \geqslant 0 $$ $$ a\left( \sqrt{3}bc +  \sqrt{2} \right)  + (-1)\left( \sqrt{3} +  \sqrt{2}c  \right) \geqslant 0 $$ $$ (1 + 1)(a\left( \sqrt{3}bc +  \sqrt{2} \right)  + (-1)\left( \sqrt{3} +  \sqrt{2}c  \right)) \geqslant 0 $$ By Chebyshev, $$ (a - 1) (\sqrt{3}bc +  \sqrt{2} +  \sqrt{3} +   \sqrt{2}c  )\geqslant0 $$ $$ a \geqslant 1 $$ Chebyshev Inequality requires the sequences to be monotonous. As $a+1>0$ , we need to have the other sequence also in the same order, hence the condition: $\sqrt{3}bc + \sqrt{2} \geqslant\sqrt{3} + \sqrt{2}c$ . The sequences are $(a,-1)$ and $(\sqrt{3}bc + \sqrt{2} ,\sqrt{3} + \sqrt{2}c)$ . I have tried another way but that was untrue. I have reached this far. The constraint $\sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0$ isn't true always. Try $(a,b,c) = (\sqrt{3},0,0)$ . Thanks for extensions or other solutions too are welcome!","The question is this: If and , then prove that For my work on this inequality, I have proved already under constraints that it is true. Proof for: By Chebyshev, Chebyshev Inequality requires the sequences to be monotonous. As , we need to have the other sequence also in the same order, hence the condition: . The sequences are and . I have tried another way but that was untrue. I have reached this far. The constraint isn't true always. Try . Thanks for extensions or other solutions too are welcome!","a\ge b\ge c\ge 0 a^2+b^2+c^2=3 abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0 \sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0. 
\sqrt{3}abc + 
\sqrt{2}a  -
\sqrt{3} -
\sqrt{2}c 
\geqslant 0
 
a\left(
\sqrt{3}bc + 
\sqrt{2}
\right) 
+ (-1)\left(
\sqrt{3} + 
\sqrt{2}c 
\right) \geqslant 0
 
(1 + 1)(a\left(
\sqrt{3}bc + 
\sqrt{2}
\right) 
+ (-1)\left(
\sqrt{3} + 
\sqrt{2}c 
\right)) \geqslant 0
 
(a - 1)
(\sqrt{3}bc + 
\sqrt{2} + 
\sqrt{3} +  
\sqrt{2}c 
)\geqslant0
 
a \geqslant 1
 a+1>0 \sqrt{3}bc + \sqrt{2} \geqslant\sqrt{3} + \sqrt{2}c (a,-1) (\sqrt{3}bc + \sqrt{2} ,\sqrt{3} + \sqrt{2}c) \sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0 (a,b,c) = (\sqrt{3},0,0)","['multivariable-calculus', 'inequality', 'lagrange-multiplier']"
69,A conceptual reason for why the Jacobian of a rotation by a changing angle is $1$?,A conceptual reason for why the Jacobian of a rotation by a changing angle is ?,1,"Consider $$f(x,y)=(x\cos r^2+y\sin r^2, y\cos r^2-x\sin r^2)\qquad\text{with }r=\sqrt{x^2+y^2},$$ as a map $\mathbb{R}^2 \to \mathbb{R}^2$ . Geometrically, $f(x,y)$ is obtained from the vector $(x,y)$ by rotating it by angle $r^2$ . (we rotate the more we move away from the origin). Wolframe Alpha claim that $\det (df)=1$ holds identically. Is there an elegant rigorous way of seeing this, without too much computations? Should this be ""obvious"" in retrospect? I was a bit surprised by this result... and direct computation is not so nice to do by hand (although tractable). I thought using the chain rule (treating the angle as a function of $x,y$ ) but got nowhere. Edit: I agree that roughly speaking, near a point $p$ , this function is like a standard rotation by a fixed angle $|p|^2$ . However, I do not consider this a rigorous explanation. Indeed, this vague intuition is still with us when we replace $r^2=x^2+y^2$ by $x^4+y^4$ , but then the Jacobian is non-constant. So, this property doesn't even hold for smooth radially symmetric angle function $\theta(x,y)$ . Is this just a coincidence then? Can we charavterize the angle functions (or at least radially symmetric ones) which satisfy this? (Wolframe says the Jacobian remains $1$ when we replace $r^2$ by $r$ or $r^4$ . Perhaps this remains true for any power of $r$ ?)","Consider as a map . Geometrically, is obtained from the vector by rotating it by angle . (we rotate the more we move away from the origin). Wolframe Alpha claim that holds identically. Is there an elegant rigorous way of seeing this, without too much computations? Should this be ""obvious"" in retrospect? I was a bit surprised by this result... and direct computation is not so nice to do by hand (although tractable). I thought using the chain rule (treating the angle as a function of ) but got nowhere. Edit: I agree that roughly speaking, near a point , this function is like a standard rotation by a fixed angle . However, I do not consider this a rigorous explanation. Indeed, this vague intuition is still with us when we replace by , but then the Jacobian is non-constant. So, this property doesn't even hold for smooth radially symmetric angle function . Is this just a coincidence then? Can we charavterize the angle functions (or at least radially symmetric ones) which satisfy this? (Wolframe says the Jacobian remains when we replace by or . Perhaps this remains true for any power of ?)","f(x,y)=(x\cos r^2+y\sin r^2, y\cos r^2-x\sin r^2)\qquad\text{with }r=\sqrt{x^2+y^2}, \mathbb{R}^2 \to \mathbb{R}^2 f(x,y) (x,y) r^2 \det (df)=1 x,y p |p|^2 r^2=x^2+y^2 x^4+y^4 \theta(x,y) 1 r^2 r r^4 r","['multivariable-calculus', 'differential-geometry', 'rotations', 'jacobian', 'orthogonal-matrices']"
70,Integral over a surface in 4-dimensions,Integral over a surface in 4-dimensions,,"Consider the integral of a function $f(x,y,z)$ over a surface embedded in 3 dimensions.  The surface has a parameterization: $$g(u,v) = (x(u,v), y(u,v), z(u,v)) $$ The integral is given by: $$ \iint_{u,v}f(g(u,v))\|\vec{g}_u \times \vec{g}_v\|\,dudv$$ In words, you are placing a curvilinear coordinate system over the surface (a ""form-fitting"" coordinate system to the surface). You use these curvilinear grid lines to tile the surface with parallelogram area elements. These parallelograms are actually linear approximations to the curvilinear area elements. Question Consider a surface integral of a function $f(x,y,z,w)$ over a surface embedded in 4 dimensions. The surface has a parameterization: $$ g(u,v) = (x(u,v), y(u,v), z(u,v), w(u,v)) $$ The integral is given by $$ \iint_{u,v} f(g(u,v))(\;\;\;\;)dudv$$ I can embedded parallelograms in 4 dimensions. However my question is, what is the notation used for the linear approximation? The cross product doesn't work in 4-dimensions. So I was wondering if I should go back to something more geometric: $A = bh$. The two side lengths of an infinitesimal parallelogram are given by $\|\vec{g}_u\|du$ and $\|\vec{g}_v\|dv$. The angle between them is given by $$ \theta = \arccos\Big(\frac{\vec{g}_udu \cdot \vec{g}_vdv}{\|\vec{g}_u\|du\|\vec{g}_v\|dv}\Big) \\ \theta = \arccos\Big(\frac{\vec{g}_u \cdot \vec{g}_v}{\|\vec{g}_u\|\|\vec{g}_v\|}\Big) $$ And therefore my integral becomes $$ \iint_{u,v} f(g(u,v)) \|\vec{g}_u\|\|\vec{g}_v\|\sin(\arccos\Big(\frac{\vec{g}_u \cdot \vec{g}_v}{\|\vec{g}_u\|\|\vec{g}_v\|}\Big)) \,dudv$$ Is this even correct? If so, is there simplifying notation for the linear approximation?","Consider the integral of a function $f(x,y,z)$ over a surface embedded in 3 dimensions.  The surface has a parameterization: $$g(u,v) = (x(u,v), y(u,v), z(u,v)) $$ The integral is given by: $$ \iint_{u,v}f(g(u,v))\|\vec{g}_u \times \vec{g}_v\|\,dudv$$ In words, you are placing a curvilinear coordinate system over the surface (a ""form-fitting"" coordinate system to the surface). You use these curvilinear grid lines to tile the surface with parallelogram area elements. These parallelograms are actually linear approximations to the curvilinear area elements. Question Consider a surface integral of a function $f(x,y,z,w)$ over a surface embedded in 4 dimensions. The surface has a parameterization: $$ g(u,v) = (x(u,v), y(u,v), z(u,v), w(u,v)) $$ The integral is given by $$ \iint_{u,v} f(g(u,v))(\;\;\;\;)dudv$$ I can embedded parallelograms in 4 dimensions. However my question is, what is the notation used for the linear approximation? The cross product doesn't work in 4-dimensions. So I was wondering if I should go back to something more geometric: $A = bh$. The two side lengths of an infinitesimal parallelogram are given by $\|\vec{g}_u\|du$ and $\|\vec{g}_v\|dv$. The angle between them is given by $$ \theta = \arccos\Big(\frac{\vec{g}_udu \cdot \vec{g}_vdv}{\|\vec{g}_u\|du\|\vec{g}_v\|dv}\Big) \\ \theta = \arccos\Big(\frac{\vec{g}_u \cdot \vec{g}_v}{\|\vec{g}_u\|\|\vec{g}_v\|}\Big) $$ And therefore my integral becomes $$ \iint_{u,v} f(g(u,v)) \|\vec{g}_u\|\|\vec{g}_v\|\sin(\arccos\Big(\frac{\vec{g}_u \cdot \vec{g}_v}{\|\vec{g}_u\|\|\vec{g}_v\|}\Big)) \,dudv$$ Is this even correct? If so, is there simplifying notation for the linear approximation?",,"['multivariable-calculus', 'notation', 'surface-integrals']"
71,A quicker way to do a Lagrange multiplier problem,A quicker way to do a Lagrange multiplier problem,,"I was working on the problem: minimize $x + 4z$ subject to $x^2 + y^2 + z^2 \le 2 $.  I have it solved, I want a faster method for use in standardized exams. My work: I tackled this using Lagrange Multipliers, considering the interior by looking for points where all individual partial derivatives of $x+ 4z$ are zero, (of which there are none).  Then considering the boundary $$ (x + 4z) - \lambda ( x^2 + y^2 +z^2 - 2) $$ From here I differentiated w.r.t x,y,z, $\lambda$ and set equal to 0 to yield $$ 1 - 2\lambda x = 0 \rightarrow 1 = 2\lambda x $$  $$ - 2 \lambda y = 0  \rightarrow 0 = 2 \lambda y \rightarrow y=0$$ $$ 4 - 2 \lambda z = 0 \rightarrow 4 = 2\lambda z$$ $$  - (x^2 + y^2 +z^2  -2 ) = 0  \rightarrow x^2 +z^2 = 2$$ Looking at equations 1, 3 we have $$ \frac{1}{2} = \lambda x, 2 = \lambda z $$ And therefore $$ \frac{1}{4} + 4 = \lambda^2 (x^2 +z^2 ) = 2 \lambda ^2 $$ $$ \frac{17}{8} = \lambda ^2 $$ And thus $$ \lambda = \pm \sqrt{ \frac{17}{8} } $$ $x = \frac{1}{2\lambda}, z = \frac{2}{\lambda} $ Yields $$ x + 4z = \frac{1}{2\lambda} + 4 \frac{2}{\lambda} = \frac{17}{2 \lambda}   = \pm \sqrt{17} \sqrt{2} = \pm \sqrt{34}$$ Clearly $-\sqrt{34}$ is smaller, so we opt for that as our solution. Now while this works, and makes sense, its not satisfactory as it TAKES SO LONG. And on a Math GRE where the expectation is to do this under 30 seconds a problem, I was hoping there was a faster method. Any suggestions? [Also open to ways to speed up the process, since even the same method with a different angle might be superior]","I was working on the problem: minimize $x + 4z$ subject to $x^2 + y^2 + z^2 \le 2 $.  I have it solved, I want a faster method for use in standardized exams. My work: I tackled this using Lagrange Multipliers, considering the interior by looking for points where all individual partial derivatives of $x+ 4z$ are zero, (of which there are none).  Then considering the boundary $$ (x + 4z) - \lambda ( x^2 + y^2 +z^2 - 2) $$ From here I differentiated w.r.t x,y,z, $\lambda$ and set equal to 0 to yield $$ 1 - 2\lambda x = 0 \rightarrow 1 = 2\lambda x $$  $$ - 2 \lambda y = 0  \rightarrow 0 = 2 \lambda y \rightarrow y=0$$ $$ 4 - 2 \lambda z = 0 \rightarrow 4 = 2\lambda z$$ $$  - (x^2 + y^2 +z^2  -2 ) = 0  \rightarrow x^2 +z^2 = 2$$ Looking at equations 1, 3 we have $$ \frac{1}{2} = \lambda x, 2 = \lambda z $$ And therefore $$ \frac{1}{4} + 4 = \lambda^2 (x^2 +z^2 ) = 2 \lambda ^2 $$ $$ \frac{17}{8} = \lambda ^2 $$ And thus $$ \lambda = \pm \sqrt{ \frac{17}{8} } $$ $x = \frac{1}{2\lambda}, z = \frac{2}{\lambda} $ Yields $$ x + 4z = \frac{1}{2\lambda} + 4 \frac{2}{\lambda} = \frac{17}{2 \lambda}   = \pm \sqrt{17} \sqrt{2} = \pm \sqrt{34}$$ Clearly $-\sqrt{34}$ is smaller, so we opt for that as our solution. Now while this works, and makes sense, its not satisfactory as it TAKES SO LONG. And on a Math GRE where the expectation is to do this under 30 seconds a problem, I was hoping there was a faster method. Any suggestions? [Also open to ways to speed up the process, since even the same method with a different angle might be superior]",,"['multivariable-calculus', 'lagrange-multiplier']"
72,Understanding higher derivatives as multilinear mappings,Understanding higher derivatives as multilinear mappings,,"I'm trying to understand how to relate the higher derivatives to multilinear mappings. Let $f$ be a differentiable function. Then, since we have $Df:V\subset \mathbb{R}^n\rightarrow \text{Lin}(\mathbb{R}^n,\mathbb{R}^p) $, can I say that $Df\in \text{Lin}(\mathbb{R}^n,\text{Lin}(\mathbb{R}^n,\mathbb{R}^p))$? Also I'm trying to relate this new way - for me at least - of thinking of higher order derivatives with what I already know, for example calculating the hessian matrix by taking the usual partial derivatives. The book I'm using has the following theorem to allow me to compute the derivatives of multilinear mappings. So, if I can think of $Df$ as in $\text{Lin}(\mathbb{R}^n\times\mathbb{R}^n,\mathbb{R}^p)$, then by the above theorem, we have $D(Df)(a_1,a_2)(h_1,h_2)=Df(h_1)(a_2)+Df(a_2)(h_2)$. However, I'm not seeing how this relates to the usual simpler calculation of the partial derivatives Any help would be appreciated.","I'm trying to understand how to relate the higher derivatives to multilinear mappings. Let $f$ be a differentiable function. Then, since we have $Df:V\subset \mathbb{R}^n\rightarrow \text{Lin}(\mathbb{R}^n,\mathbb{R}^p) $, can I say that $Df\in \text{Lin}(\mathbb{R}^n,\text{Lin}(\mathbb{R}^n,\mathbb{R}^p))$? Also I'm trying to relate this new way - for me at least - of thinking of higher order derivatives with what I already know, for example calculating the hessian matrix by taking the usual partial derivatives. The book I'm using has the following theorem to allow me to compute the derivatives of multilinear mappings. So, if I can think of $Df$ as in $\text{Lin}(\mathbb{R}^n\times\mathbb{R}^n,\mathbb{R}^p)$, then by the above theorem, we have $D(Df)(a_1,a_2)(h_1,h_2)=Df(h_1)(a_2)+Df(a_2)(h_2)$. However, I'm not seeing how this relates to the usual simpler calculation of the partial derivatives Any help would be appreciated.",,"['multivariable-calculus', 'differential-geometry', 'multilinear-algebra']"
73,How to show that a continous function $f:\mathbb{R}^m \to \mathbb{R}$ has a maximum?,How to show that a continous function  has a maximum?,f:\mathbb{R}^m \to \mathbb{R},"My task is this: Suppose $f:\mathbb{R}^m \to \mathbb{R}$ is a positive, continous function such that $\lim_{\mid \textbf{x}\mid \to \infty} f(\textbf{x}) = \textbf{0}$. Show that $f$ has a maximum. I am not sure exactly how to show this since the domain of $f$ is all of $\mathbb{R}^m$. According to the extreme value theorem, we need $A\subset \mathbb{R}^m$ to be closed, bounded and that $f:A\to\mathbb{R}$ to be continous (which it is over the entire $\mathbb{R}^m$). I am thinking that if we could somehow split up the domain into closed sets and take the union, one could probably show that $f$ had a maximum, but I am not sure if that's the right approach. Any help would be more than welcome!","My task is this: Suppose $f:\mathbb{R}^m \to \mathbb{R}$ is a positive, continous function such that $\lim_{\mid \textbf{x}\mid \to \infty} f(\textbf{x}) = \textbf{0}$. Show that $f$ has a maximum. I am not sure exactly how to show this since the domain of $f$ is all of $\mathbb{R}^m$. According to the extreme value theorem, we need $A\subset \mathbb{R}^m$ to be closed, bounded and that $f:A\to\mathbb{R}$ to be continous (which it is over the entire $\mathbb{R}^m$). I am thinking that if we could somehow split up the domain into closed sets and take the union, one could probably show that $f$ had a maximum, but I am not sure if that's the right approach. Any help would be more than welcome!",,['multivariable-calculus']
74,How to convert (or transform) from one range to another? [duplicate],How to convert (or transform) from one range to another? [duplicate],,"This question already has answers here : Shift numbers into a different range (3 answers) Closed 3 years ago . I have score ranges min score = 40 and max score = 60 . I have same gpa ranges too 1.00 - 1.99 . Which formula I can use to calculate the gpa.  Like If I entered 45 then it should print 1.25 . Range of score and grade can be different. P.S. I am a web developer, and I am a little poor in Math. I need to apply this formula in my coding.","This question already has answers here : Shift numbers into a different range (3 answers) Closed 3 years ago . I have score ranges min score = 40 and max score = 60 . I have same gpa ranges too 1.00 - 1.99 . Which formula I can use to calculate the gpa.  Like If I entered 45 then it should print 1.25 . Range of score and grade can be different. P.S. I am a web developer, and I am a little poor in Math. I need to apply this formula in my coding.",,['multivariable-calculus']
75,Triple Integral of $1/\sqrt{2 + x^2 + y^2 + z^2}$ over unit sphere,Triple Integral of  over unit sphere,1/\sqrt{2 + x^2 + y^2 + z^2},"I'm studying triple integrals (physics major), and I'm having trouble solving this little beast: $$ \iiint_V \frac{1}{\sqrt{2 + x^2 +y^2 +z^2}} \,dx\,dy\,dz$$ where V is $$x^2+y^2+z^2=1$$ Of course we use spherical coordinates: $$I = \iiint_V \frac{r^2 \sin φ}{\sqrt{2+r^2}} \,dr\,dφ\,dθ$$ In order to solve the first integral over r I simplified the denominator using $$\sqrt{2+r^2} = \sqrt{2\left(1+\frac{r^2}{\sqrt{2}^2}\right)}$$ in order to substitute $\tan ω = \frac{r}{\sqrt{2}}$. However again even this integral leads to 2 pages of computations and I still haven't reached a correct result. Is there any shortcut I'm not seeing? PS: The limits of $r,θ,φ$ are the standard ones since we are on the unit sphere.","I'm studying triple integrals (physics major), and I'm having trouble solving this little beast: $$ \iiint_V \frac{1}{\sqrt{2 + x^2 +y^2 +z^2}} \,dx\,dy\,dz$$ where V is $$x^2+y^2+z^2=1$$ Of course we use spherical coordinates: $$I = \iiint_V \frac{r^2 \sin φ}{\sqrt{2+r^2}} \,dr\,dφ\,dθ$$ In order to solve the first integral over r I simplified the denominator using $$\sqrt{2+r^2} = \sqrt{2\left(1+\frac{r^2}{\sqrt{2}^2}\right)}$$ in order to substitute $\tan ω = \frac{r}{\sqrt{2}}$. However again even this integral leads to 2 pages of computations and I still haven't reached a correct result. Is there any shortcut I'm not seeing? PS: The limits of $r,θ,φ$ are the standard ones since we are on the unit sphere.",,"['multivariable-calculus', 'definite-integrals']"
76,Why does the order not matter? Partial D,Why does the order not matter? Partial D,,"When taking partial derivatives, why does the order not matter as long as the function is continuous? Any proof, intuitive or rigorous?","When taking partial derivatives, why does the order not matter as long as the function is continuous? Any proof, intuitive or rigorous?",,"['multivariable-calculus', 'derivatives', 'intuition']"
77,Is there a vector field that is the complete opposite of a conservative one,Is there a vector field that is the complete opposite of a conservative one,,"Is there a three-dimensional vector field such that for every non-selfintersecting closed curve (that is not just one point, to avoid degenerate cases) the respective line-integral on the curve becomes non-zero? If not, what if I know that every point of the curve has all its coordinates positive?","Is there a three-dimensional vector field such that for every non-selfintersecting closed curve (that is not just one point, to avoid degenerate cases) the respective line-integral on the curve becomes non-zero? If not, what if I know that every point of the curve has all its coordinates positive?",,"['multivariable-calculus', 'vector-analysis']"
78,Derivation or Intuition of Formula for Levi-Civita Symbol,Derivation or Intuition of Formula for Levi-Civita Symbol,,"http://www.ees.nmt.edu/outside/courses/GEOP523/Docs/index-notation.pdf spouted off and threw out with no motivation $$\epsilon_{ijk} = \frac{1}{2}(i - j)(j - k)(k - i) \, \forall \, \, k \in \{1, 2, 3\}$$ where  $ \epsilon_{i_1i_2i_3...i_n} = \left\{ \begin{array}{rcl} +1 & \mbox{if } (i_1, i_2, i_3, ..., i_n) \text{ is an even permutation of } (1, 2, 3, ..., n) & \\   -1 & \mbox{if } (i_1, i_2, i_3, ..., i_n)\text{ is an odd permutation of } (1, 2, 3, ..., n) \\ 0 & \mbox{if } (i_1, i_2, i_3, ..., i_n) \text{ is NOT a permutation of } (1, 2, 3, ..., n) \end{array}\right. $ What is the intuition or derivation? I tried looking online but found nothing.","http://www.ees.nmt.edu/outside/courses/GEOP523/Docs/index-notation.pdf spouted off and threw out with no motivation $$\epsilon_{ijk} = \frac{1}{2}(i - j)(j - k)(k - i) \, \forall \, \, k \in \{1, 2, 3\}$$ where  $ \epsilon_{i_1i_2i_3...i_n} = \left\{ \begin{array}{rcl} +1 & \mbox{if } (i_1, i_2, i_3, ..., i_n) \text{ is an even permutation of } (1, 2, 3, ..., n) & \\   -1 & \mbox{if } (i_1, i_2, i_3, ..., i_n)\text{ is an odd permutation of } (1, 2, 3, ..., n) \\ 0 & \mbox{if } (i_1, i_2, i_3, ..., i_n) \text{ is NOT a permutation of } (1, 2, 3, ..., n) \end{array}\right. $ What is the intuition or derivation? I tried looking online but found nothing.",,['multivariable-calculus']
79,Different Definitions of the Directional Derivative,Different Definitions of the Directional Derivative,,"I have seen several different starting points for definition the directional derivative of a function $f$ at a point $p$. Ultimately though, they can all be reduced to the equivalent definition via the gradient: $$ D_v f(p) = \langle \nabla f(p), v \rangle $$ What is not clear though is why some texts only allow $v$ to be a unit vector and why other texts have no such restriction. If $v$ is not a unit vector one can always be produced by dividing $v$ by its norm; however, strictly speaking, the two definitions (one which requires a unit vector and one which doesn't) will differ by a scaling factor. So, my question is, is there any reason to restrict the definition to a unit vector? What is the motivation for some texts to allow only unit vectors?","I have seen several different starting points for definition the directional derivative of a function $f$ at a point $p$. Ultimately though, they can all be reduced to the equivalent definition via the gradient: $$ D_v f(p) = \langle \nabla f(p), v \rangle $$ What is not clear though is why some texts only allow $v$ to be a unit vector and why other texts have no such restriction. If $v$ is not a unit vector one can always be produced by dividing $v$ by its norm; however, strictly speaking, the two definitions (one which requires a unit vector and one which doesn't) will differ by a scaling factor. So, my question is, is there any reason to restrict the definition to a unit vector? What is the motivation for some texts to allow only unit vectors?",,['multivariable-calculus']
80,Generalizing Lagrange multipliers to use the subdifferential,Generalizing Lagrange multipliers to use the subdifferential,,"Background: This is a followup to the question Lagrange multipliers with non-smooth constraints . Lagrange multipliers can be used for constrained optimization problems of the form $$\min_{\vec x} f(\vec x) \text{ such that } g(\vec x) = 0$$ Briefly, the method works by constructing the Lagrangian, $L(\vec x, \lambda) = f(\vec x) + \lambda g(\vec x)$ , then finding points where $\forall i, \frac{\partial L}{\partial x_i} L = 0$ and $\frac{\partial L}{\partial \lambda} L = 0$ . As was kindly pointed out in this answer , the method fails when $g$ is non-differentiable (but continuous), because the partial derivatives may not exist at points of optimality.  For example, in the problem, minimize $x_1$ subject to $g(x_1,x_2) = x_1 - |x_2| = 0$ . The minimum is at $(0,0)$ , where $\frac{\partial g}{\partial x_2}$ does not exist. Question: It seems that there should be a natural generalization of the method that uses subgradients and the subdifferential.  Does the following work?  Is there a reference that describes this in more detail? Proposal: construct the Lagrangian as usual, but instead of seeking a point where all partial derivatives are 0, seek a point where 0 is in each partial subdifferential.  So in the example above, the subdifferential with respect to $x_2$ when $x_2=0$ is the interval $[-\lambda, \lambda]$ .  Thus, if we were given the solution $x_1=0,x_2=0,\lambda=-1$ , we could verify it is a critical point by noting that $\frac{\partial L}{\partial \lambda} = x_1 - |x_2| = 0$ , $\frac{\partial L}{\partial x_1} = 1 + \lambda = 0$ , and 0 is in the subdifferential of $L$ w.r.t. $x_2$ . Is this argument correct?  My intuitive justification is that for any value $f'(x)$ in some variable's subdifferential at $x$ , we should be able to construct a smooth function that has $f'(x)$ as its partial derivative at $x$ , then solve the smoothed problem with a standard application of Lagrange multipliers. (Aside: my goal is actually not to find a method to optimize the function.  I have a method for optimizing such functions, and I'm trying to develop some theoretical understanding of the solutions that it produces.  In particular, I'm trying to understand if proving that a solution satisfies the condition described in the Proposal section is meaningful.)","Background: This is a followup to the question Lagrange multipliers with non-smooth constraints . Lagrange multipliers can be used for constrained optimization problems of the form Briefly, the method works by constructing the Lagrangian, , then finding points where and . As was kindly pointed out in this answer , the method fails when is non-differentiable (but continuous), because the partial derivatives may not exist at points of optimality.  For example, in the problem, minimize subject to . The minimum is at , where does not exist. Question: It seems that there should be a natural generalization of the method that uses subgradients and the subdifferential.  Does the following work?  Is there a reference that describes this in more detail? Proposal: construct the Lagrangian as usual, but instead of seeking a point where all partial derivatives are 0, seek a point where 0 is in each partial subdifferential.  So in the example above, the subdifferential with respect to when is the interval .  Thus, if we were given the solution , we could verify it is a critical point by noting that , , and 0 is in the subdifferential of w.r.t. . Is this argument correct?  My intuitive justification is that for any value in some variable's subdifferential at , we should be able to construct a smooth function that has as its partial derivative at , then solve the smoothed problem with a standard application of Lagrange multipliers. (Aside: my goal is actually not to find a method to optimize the function.  I have a method for optimizing such functions, and I'm trying to develop some theoretical understanding of the solutions that it produces.  In particular, I'm trying to understand if proving that a solution satisfies the condition described in the Proposal section is meaningful.)","\min_{\vec x} f(\vec x) \text{ such that } g(\vec x) = 0 L(\vec x, \lambda) = f(\vec x) + \lambda g(\vec x) \forall i, \frac{\partial L}{\partial x_i} L = 0 \frac{\partial L}{\partial \lambda} L = 0 g x_1 g(x_1,x_2) = x_1 - |x_2| = 0 (0,0) \frac{\partial g}{\partial x_2} x_2 x_2=0 [-\lambda, \lambda] x_1=0,x_2=0,\lambda=-1 \frac{\partial L}{\partial \lambda} = x_1 - |x_2| = 0 \frac{\partial L}{\partial x_1} = 1 + \lambda = 0 L x_2 f'(x) x f'(x) x","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'non-smooth-analysis', 'non-smooth-optimization']"
81,"Homework Help - Finding a Vector when given two points, and then finding a unit vector in the same direction","Homework Help - Finding a Vector when given two points, and then finding a unit vector in the same direction",,"I've attempted to solve the problem, but I got $\langle \frac{1}{\sqrt{\frac{29}{4}}}, \frac{5}{\sqrt{\frac{29}{4}}}\rangle$, which is incorrect. There is not a similar problem in my textbook that I can reference. I know that to find a unit vector, we first find the length/magnitude of the given vector, and multiply $$1/\sqrt{magnitude}$$ by the original vector. $$L = \sqrt{x^2 + y^2}.$$ Can anyone give me any ideas on how to solve this problem? Find the unit vector that has the same direction as the vector from the point A = (-1,2) to the point B = (3,3). Thank you in advance.","I've attempted to solve the problem, but I got $\langle \frac{1}{\sqrt{\frac{29}{4}}}, \frac{5}{\sqrt{\frac{29}{4}}}\rangle$, which is incorrect. There is not a similar problem in my textbook that I can reference. I know that to find a unit vector, we first find the length/magnitude of the given vector, and multiply $$1/\sqrt{magnitude}$$ by the original vector. $$L = \sqrt{x^2 + y^2}.$$ Can anyone give me any ideas on how to solve this problem? Find the unit vector that has the same direction as the vector from the point A = (-1,2) to the point B = (3,3). Thank you in advance.",,['multivariable-calculus']
82,Why the answer for double integral is coming as zero?,Why the answer for double integral is coming as zero?,,"I am trying to evaluate $$\iint_{R} x+y \:d A$$ , where $R$ is the region formed by the vertices $$(0,0),(5,0),\left(\frac{5}{2}, \frac{5}{2}\right) \text { and }\left(\frac{5}{2},-\frac{5}{2}\right)$$ . My try: Here is the picture of the region which has two triangular regions. Let the top traingle is $R1$ and bottom triangle is $R2$ We have $$\iint _{R}(x+y)dA=\iint_{R1}(x+y)dA+\iint_{R2}(x+y)dA$$ Now we have: $$\iint_{R1}(x+y)dA=\int_{x=0}^{5}\int_{y=x}^{5-x}(x+y)dydx=\frac{-125}{6}$$ Also $$\iint_{R2}(x+y)dA=\int_{x=0}^{5}\int_{y=-x}^{x-5}(x+y)dydx=\frac{125}{6}$$ Adding both i am getting zero. But that is not the answer. What's wrong in this approach?","I am trying to evaluate , where is the region formed by the vertices . My try: Here is the picture of the region which has two triangular regions. Let the top traingle is and bottom triangle is We have Now we have: Also Adding both i am getting zero. But that is not the answer. What's wrong in this approach?","\iint_{R} x+y \:d A R (0,0),(5,0),\left(\frac{5}{2}, \frac{5}{2}\right) \text { and }\left(\frac{5}{2},-\frac{5}{2}\right) R1 R2 \iint _{R}(x+y)dA=\iint_{R1}(x+y)dA+\iint_{R2}(x+y)dA \iint_{R1}(x+y)dA=\int_{x=0}^{5}\int_{y=x}^{5-x}(x+y)dydx=\frac{-125}{6} \iint_{R2}(x+y)dA=\int_{x=0}^{5}\int_{y=-x}^{x-5}(x+y)dydx=\frac{125}{6}","['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
83,Differentiability implies Lipschitz continuity (multivariable),Differentiability implies Lipschitz continuity (multivariable),,"I am studying from Marsden: Elementary Classical Analysis ( $2^{\rm{nd}}$ ed.). I am not able to write down the complete proof of the following theorem (Theorem 6.3.1, page 334). The theorem essentially says that: Let $A\subset \mathbb{R}^n$ be an open set and let $f:A \to \mathbb{R}^m$ be a differentiable function. Then $f$ is locally Lipschitz, i.e. for each $x_0\in A$ , there is $M>0$ and $\delta_0>0$ such that $$\|x-x_0\|<\delta_0 \quad \Rightarrow \quad \|f(x)-f(x_0)\|<M\|x-x_0\|.$$ I could not locate the proof anywhere, nor was able to generalize the one-variable proof to multivariable (For example: here , here . This one is slightly different, with an extra condition of $f'$ being continuous.) The reason I cannot apply these arguments to multivariable case is all these proofs use MVT. Can someone help with the proof? OR direct me to a reference / book which has a proof?","I am studying from Marsden: Elementary Classical Analysis ( ed.). I am not able to write down the complete proof of the following theorem (Theorem 6.3.1, page 334). The theorem essentially says that: Let be an open set and let be a differentiable function. Then is locally Lipschitz, i.e. for each , there is and such that I could not locate the proof anywhere, nor was able to generalize the one-variable proof to multivariable (For example: here , here . This one is slightly different, with an extra condition of being continuous.) The reason I cannot apply these arguments to multivariable case is all these proofs use MVT. Can someone help with the proof? OR direct me to a reference / book which has a proof?",2^{\rm{nd}} A\subset \mathbb{R}^n f:A \to \mathbb{R}^m f x_0\in A M>0 \delta_0>0 \|x-x_0\|<\delta_0 \quad \Rightarrow \quad \|f(x)-f(x_0)\|<M\|x-x_0\|. f',"['multivariable-calculus', 'derivatives', 'lipschitz-functions']"
84,"Partial derivative of a two variables function, one of which dependent on the other","Partial derivative of a two variables function, one of which dependent on the other",,"I found this exercise on the book of multivariable calculus from which I'm studying: ""Find the partial derivative $\frac{\partial{z}}{\partial{x}}$ and the total derivative $\frac{\text{d}z}{\text{d}x}$ of $z(x,y)=e^{xy}$ where $y=\phi(x)$ ."" Now, this to me looks like a function of a single variable $f:\mathbb{R}\to\mathbb{R}$ and so in this case the partial derivative of $f$ with respect to $x$ and total derivative would be equivalent; in particular, I end up with something like: $$\frac{\text{d}z}{\text{d}x}=e^{xy}(\phi(x)+x\phi'(x))$$ In the solution, while the result for the total derivative is the same as mine, the partial derivative of $f$ with respect to $x$ is written as follows: $$\frac{\partial{z}}{\partial{x}}=ye^{xy}$$ Why is this the case? Since the partial derivative of $f$ with respect to $x$ shows the incremental behaviour of the function as $x$ changes, shouldn't I account for the presence of $x$ in the functional representation of $y$ while computing the derivative with respect to $x$ ? Sorry in advance for the super basic question :)","I found this exercise on the book of multivariable calculus from which I'm studying: ""Find the partial derivative and the total derivative of where ."" Now, this to me looks like a function of a single variable and so in this case the partial derivative of with respect to and total derivative would be equivalent; in particular, I end up with something like: In the solution, while the result for the total derivative is the same as mine, the partial derivative of with respect to is written as follows: Why is this the case? Since the partial derivative of with respect to shows the incremental behaviour of the function as changes, shouldn't I account for the presence of in the functional representation of while computing the derivative with respect to ? Sorry in advance for the super basic question :)","\frac{\partial{z}}{\partial{x}} \frac{\text{d}z}{\text{d}x} z(x,y)=e^{xy} y=\phi(x) f:\mathbb{R}\to\mathbb{R} f x \frac{\text{d}z}{\text{d}x}=e^{xy}(\phi(x)+x\phi'(x)) f x \frac{\partial{z}}{\partial{x}}=ye^{xy} f x x x y x","['multivariable-calculus', 'derivatives', 'partial-derivative']"
85,(Why) can we treat a function of a variable as another independent variable?,(Why) can we treat a function of a variable as another independent variable?,,"I'm currently reading my numerical analysis textbook and something's bugging me. To get into it, let's take a look at the following differential equation; $$u'(x) = f(x, u(x))$$ In order to determine the stability of the equation, one may calculate the Jacobian, $$J(x, u(x)) = \frac{\partial f}{\partial u}|_{(x, u(x))}$$ Here is a specific differential equation: $$u'(x) = -\alpha(u(x) - sin(x)) + cos(x)$$ For which the Jacobian is $$J(x, u(x)) = -\alpha$$ Basically, we treated both $sin(x)$ and $cos(x)$ as constants with respect to $u$ , but I don't really understand why. Most of the time, when we take a derivative the variables are independant, which is not the case here as they both depend on the same variable $x$ . This means that the ""rate of change of $sin(x)$ with respect to $u(x)$ "" is zero, but the value of $u(x)$ only changes if the value of x itself changes, so shouldn't the value of $sin(x)$ change aswell? Thank you!","I'm currently reading my numerical analysis textbook and something's bugging me. To get into it, let's take a look at the following differential equation; In order to determine the stability of the equation, one may calculate the Jacobian, Here is a specific differential equation: For which the Jacobian is Basically, we treated both and as constants with respect to , but I don't really understand why. Most of the time, when we take a derivative the variables are independant, which is not the case here as they both depend on the same variable . This means that the ""rate of change of with respect to "" is zero, but the value of only changes if the value of x itself changes, so shouldn't the value of change aswell? Thank you!","u'(x) = f(x, u(x)) J(x, u(x)) = \frac{\partial f}{\partial u}|_{(x, u(x))} u'(x) = -\alpha(u(x) - sin(x)) + cos(x) J(x, u(x)) = -\alpha sin(x) cos(x) u x sin(x) u(x) u(x) sin(x)","['multivariable-calculus', 'derivatives', 'numerical-methods', 'jacobian', 'numerical-calculus']"
86,What are the necessary and sufficient condions for a laplacian to be zero?,What are the necessary and sufficient condions for a laplacian to be zero?,,"Let $F$ be a function of $x,y,z$ , namely $F(x,y,z)$ . My question: What are the necessary and sufficient conditions for $\triangledown$$^2$$F(x,y,z)$ = $0$ , what does it signify? I am aware that if $d$ is a differential operator, then $d$ $(\triangledown$$F(x,y,z))$$=0$ , where $i, j, k$ are $dx,dy,dz$ respectively. That is, $d(\frac{\partial F}{\partial x}dx,\frac{\partial F}{\partial y}dy,\frac{\partial F}{\partial z}dz)=0$ . I know this is true if $F(x,y,z)$ is the force associated with a conservative potential function or in other words conservative vector field. However, I can not seem to relate it with the Laplacian. Are the two related, or they just seem related to me? Thank you.","Let be a function of , namely . My question: What are the necessary and sufficient conditions for = , what does it signify? I am aware that if is a differential operator, then , where are respectively. That is, . I know this is true if is the force associated with a conservative potential function or in other words conservative vector field. However, I can not seem to relate it with the Laplacian. Are the two related, or they just seem related to me? Thank you.","F x,y,z F(x,y,z) \triangledown^2F(x,y,z) 0 d d (\triangledownF(x,y,z))=0 i, j, k dx,dy,dz d(\frac{\partial F}{\partial x}dx,\frac{\partial F}{\partial y}dy,\frac{\partial F}{\partial z}dz)=0 F(x,y,z)","['multivariable-calculus', 'vector-analysis', 'physics', 'laplacian']"
87,Why is the Jacobian matrix so useful? [closed],Why is the Jacobian matrix so useful? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 4 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I am a first year undergraduate and I always see the Jacobian crop up in some many places, e.g., integration, solving systems of equations, analysis and so many more places. I was wondering, what makes it so useful?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 4 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I am a first year undergraduate and I always see the Jacobian crop up in some many places, e.g., integration, solving systems of equations, analysis and so many more places. I was wondering, what makes it so useful?",,"['multivariable-calculus', 'vector-fields', 'jacobian']"
88,Using higher order derivatives,Using higher order derivatives,,"I am currently learning about the general Notion of Differentiability. I came across some difficulties when working with higher order derivatives and I am hoping for confirmation or comments on some questions I have. In the following, let $E$, $F$ be Banach Spaces, and let $X\subseteq E$ be open. I do understand that for $x_0\in X$ it is $Df(x_0)\in\mathcal{L}(E,F)$. My directional derivative for $v\in E\setminus\{0\}$ is defined as the derivative in $0$ of the function $(-\varepsilon,\varepsilon)\to F, t\mapsto f(x_0+tv)$ with $\varepsilon>0$ suitable to keep $x_0\pm\varepsilon v$ in $X$. So it is $D_vf(x_0)\in\mathcal{L}(\mathbb{R},F)$. When then stating $D_vf(x_0)=Df(x_0)v$ while $Df(x_0)v\in F$, are we already using the identification $\mathcal{L}(\mathbb{R},F)\cong F$? When extending the notion to higher order derivatives $D^kf(x_0)\in\mathcal{L}^k(E,F)$ I came across the statement $$D^kf(x_0)(h_1,\dots,h_k)=D(\dots D(Df(x_0)h_1)h_2\dots)h_k$$ that should somehow be linked to the above identification and that I really cannot wrap my head around. It would be nice to see some step-by-step computation of that formula. Should I read myself more into multi-linear maps? Thanks in advance for any comment.","I am currently learning about the general Notion of Differentiability. I came across some difficulties when working with higher order derivatives and I am hoping for confirmation or comments on some questions I have. In the following, let $E$, $F$ be Banach Spaces, and let $X\subseteq E$ be open. I do understand that for $x_0\in X$ it is $Df(x_0)\in\mathcal{L}(E,F)$. My directional derivative for $v\in E\setminus\{0\}$ is defined as the derivative in $0$ of the function $(-\varepsilon,\varepsilon)\to F, t\mapsto f(x_0+tv)$ with $\varepsilon>0$ suitable to keep $x_0\pm\varepsilon v$ in $X$. So it is $D_vf(x_0)\in\mathcal{L}(\mathbb{R},F)$. When then stating $D_vf(x_0)=Df(x_0)v$ while $Df(x_0)v\in F$, are we already using the identification $\mathcal{L}(\mathbb{R},F)\cong F$? When extending the notion to higher order derivatives $D^kf(x_0)\in\mathcal{L}^k(E,F)$ I came across the statement $$D^kf(x_0)(h_1,\dots,h_k)=D(\dots D(Df(x_0)h_1)h_2\dots)h_k$$ that should somehow be linked to the above identification and that I really cannot wrap my head around. It would be nice to see some step-by-step computation of that formula. Should I read myself more into multi-linear maps? Thanks in advance for any comment.",,"['multivariable-calculus', 'derivatives']"
89,Book recommend for topics of Integrals in multivariable calculus.,Book recommend for topics of Integrals in multivariable calculus.,,"I am an average student and have to study following topics on my own for the exam : The measure of a bounded interval in $\mathbb R^n$ , the Riemann integral of a bounded function defined    on a compact interval in $\mathbb R^n$ , Sets of measure zero and Lebesgue’s criterion for existence of a    multiple Riemann Integral, Evaluation of a multiple integral by iterated integration. Please can anyone suggest some good self-study book providing good insight into the above topics ..","I am an average student and have to study following topics on my own for the exam : The measure of a bounded interval in $\mathbb R^n$ , the Riemann integral of a bounded function defined    on a compact interval in $\mathbb R^n$ , Sets of measure zero and Lebesgue’s criterion for existence of a    multiple Riemann Integral, Evaluation of a multiple integral by iterated integration. Please can anyone suggest some good self-study book providing good insight into the above topics ..",,"['multivariable-calculus', 'reference-request', 'soft-question', 'advice', 'book-recommendation']"
90,Which Cross Product for the Desired Orientation of a Sphere ? [Stewart P1091 16.7.23],Which Cross Product for the Desired Orientation of a Sphere ? [Stewart P1091 16.7.23],,"P1086: For a closed surface, the positive orientation is the one for which the normal vectors point outward from the surface, and inward-pointing normals give the negative orientation. P1087: If $S$ is a smooth orientable surface given in parametric form by a vector function $\mathbf{r}(u,v)$ , then it is automatically supplied with the orientation of the unit normal $\mathbf{n} = \cfrac{\partial_u\mathbf{r} \times \partial_v\mathbf{r}}{\vert \partial_u\mathbf{r} \times \partial_v \mathbf{r} \vert} $ ... P1093: The orientation of a surface S induces the positive orientation of the boundary curve C shown in the figure. This means that if one walks in the positive direction around the curve with one's head pointing in the direction of $\mathbf{n}$ , then the surface is always on one's left. How does one determine whether $\partial_{\huge{u}}\mathbf{r} \times \partial_{\huge{v}}\mathbf{r} \quad \text{ or } \quad \partial_{\huge{v}}\mathbf{r} \times \partial_{\huge{u}}\mathbf{r} \quad  $ (negatives of each other) matches the desired orientation? Since a surface may be hard to sketch (especially under exam conditions), I was hoping for an argument that isn't geometric or visual.  But if geometry and visualisation are the easiest,  would you please provide pictures for your explanations? P1091 16.7. $23 \text{ generalised.}$ $\mathbf{F} = (x,-z,y)$ and $S$ is the part of $x^2 + y^2 + z^2 = p$ in the first octant and oriented towards $(0,0,0)$ . Evaluate the surface integral $\iint_S \mathbf{F} \cdot d\mathbf{S}$ . For closed surfaces, use the positive (outward) orientation. Solution: Since $S$ is a sphere, parameterize with $r(\theta, \phi) = (p\sin \theta \cos \theta, p \sin \theta \sin \theta, p \cos \phi)$ . Then $\mathbf{F[r(\theta, \phi)]} \cdot \color{red}{(\partial_{\theta} r \times \ \partial_{\phi} r )} = p^3 \sin^3 \theta \cos^2 \theta \qquad (♦)$ Then $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_D \mathbf{F} \cdot (\partial_{\theta} r \times \ \partial_{\phi} r ) \, dA = p^3\int^{2\pi}_0 \cos^2 \theta \, d\theta \int^{\pi/2}_9 \sin^3 \phi \, d\phi = ... = p^3 \quad \pi \quad 1/3.$ The answer is given as $ -p^\color{red}{2} \quad \pi \quad 1/3 $ . How would've one determined that the cross product in (♦) coloured in red is wrong, and that it should've been $\color{green}{ \partial_{\large{\phi}} r \times \partial_{\large{\theta}} r  }$ ? Predicated on user Dan's Answer:","P1086: For a closed surface, the positive orientation is the one for which the normal vectors point outward from the surface, and inward-pointing normals give the negative orientation. P1087: If is a smooth orientable surface given in parametric form by a vector function , then it is automatically supplied with the orientation of the unit normal ... P1093: The orientation of a surface S induces the positive orientation of the boundary curve C shown in the figure. This means that if one walks in the positive direction around the curve with one's head pointing in the direction of , then the surface is always on one's left. How does one determine whether (negatives of each other) matches the desired orientation? Since a surface may be hard to sketch (especially under exam conditions), I was hoping for an argument that isn't geometric or visual.  But if geometry and visualisation are the easiest,  would you please provide pictures for your explanations? P1091 16.7. and is the part of in the first octant and oriented towards . Evaluate the surface integral . For closed surfaces, use the positive (outward) orientation. Solution: Since is a sphere, parameterize with . Then Then The answer is given as . How would've one determined that the cross product in (♦) coloured in red is wrong, and that it should've been ? Predicated on user Dan's Answer:","S \mathbf{r}(u,v) \mathbf{n} = \cfrac{\partial_u\mathbf{r} \times \partial_v\mathbf{r}}{\vert \partial_u\mathbf{r} \times \partial_v \mathbf{r} \vert}  \mathbf{n} \partial_{\huge{u}}\mathbf{r} \times \partial_{\huge{v}}\mathbf{r} \quad \text{ or } \quad \partial_{\huge{v}}\mathbf{r} \times \partial_{\huge{u}}\mathbf{r} \quad   23 \text{ generalised.} \mathbf{F} = (x,-z,y) S x^2 + y^2 + z^2 = p (0,0,0) \iint_S \mathbf{F} \cdot d\mathbf{S} S r(\theta, \phi) = (p\sin \theta \cos \theta, p \sin \theta \sin \theta, p \cos \phi) \mathbf{F[r(\theta, \phi)]} \cdot \color{red}{(\partial_{\theta} r \times \ \partial_{\phi} r )} = p^3 \sin^3 \theta \cos^2 \theta \qquad (♦) \iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_D \mathbf{F} \cdot (\partial_{\theta} r \times \ \partial_{\phi} r ) \, dA = p^3\int^{2\pi}_0 \cos^2 \theta \, d\theta \int^{\pi/2}_9 \sin^3 \phi \, d\phi = ... = p^3 \quad \pi \quad 1/3.  -p^\color{red}{2} \quad \pi \quad 1/3  \color{green}{ \partial_{\large{\phi}} r \times \partial_{\large{\theta}} r  }",['multivariable-calculus']
91,Stokes' Theorem,Stokes' Theorem,,"Let $C$ be the following, let $C$ be the curve of intersection of the cylinder $x^2 + y^2 = 1$ and the given surface $z = f(x,y)$, oriented counterclockwise around the cylinder. Use Stokes' theorem to compute the line integral by first converting it to a surface integral. (a) $\int_C (y \, \mathrm{d}x + z \, \mathrm{d}y + x \, \mathrm{d}z),\quad z=x \cdot y$. I'm having a problem setting up the problem. I appreciate any assistance.","Let $C$ be the following, let $C$ be the curve of intersection of the cylinder $x^2 + y^2 = 1$ and the given surface $z = f(x,y)$, oriented counterclockwise around the cylinder. Use Stokes' theorem to compute the line integral by first converting it to a surface integral. (a) $\int_C (y \, \mathrm{d}x + z \, \mathrm{d}y + x \, \mathrm{d}z),\quad z=x \cdot y$. I'm having a problem setting up the problem. I appreciate any assistance.",,['multivariable-calculus']
92,Intuition behind $\nabla \times \mathbf{F}$,Intuition behind,\nabla \times \mathbf{F},"Is there a simple explanation why this form for the curl of a vector field $\mathbf{F}$, $$\nabla \times \mathbf{F}=\begin{vmatrix} \hat{x} & \hat{y}  &\hat{z}  \\   \frac{\partial}{\partial x}& \frac{\partial}{\partial y} &\frac{\partial}{\partial z} \\   F_x& F_y &F_z  \end{vmatrix}$$ Corresponds to the amount of 'twiting' of $\mathbf{F}$ (and any other qulities of $\nabla \times \mathbf{F}$)? When I first saw the equation, it seemed, very roughly, to be a measure of how much a component of $\mathbf{F}$ is affected by the other two components. However, this only really differentiates between $0$ and ' not $0$' curl, and anyway  there are thousands of possible equations that would give the same first impression. What's so unique about this one?","Is there a simple explanation why this form for the curl of a vector field $\mathbf{F}$, $$\nabla \times \mathbf{F}=\begin{vmatrix} \hat{x} & \hat{y}  &\hat{z}  \\   \frac{\partial}{\partial x}& \frac{\partial}{\partial y} &\frac{\partial}{\partial z} \\   F_x& F_y &F_z  \end{vmatrix}$$ Corresponds to the amount of 'twiting' of $\mathbf{F}$ (and any other qulities of $\nabla \times \mathbf{F}$)? When I first saw the equation, it seemed, very roughly, to be a measure of how much a component of $\mathbf{F}$ is affected by the other two components. However, this only really differentiates between $0$ and ' not $0$' curl, and anyway  there are thousands of possible equations that would give the same first impression. What's so unique about this one?",,"['multivariable-calculus', 'intuition']"
93,"A function whose partial derivatives exist everywhere, but is nowhere continuous?","A function whose partial derivatives exist everywhere, but is nowhere continuous?",,"Consider $f: \mathbb{R}^2 \rightarrow  \mathbb{R}$. Unlike functions of one variable, the partial derivatives may exist at a point even though $f$ is not continuous there. I have seen examples where $f_x$ and $f_y$ exist in the neighbourhood of a point, but $f$ is not continuous at that point. Can anyone provide an example of a function such that $f_x$ and $f_y$ exist everywhere but $f$ is nowhere continuous? It seems likely that such a function should exist, but I haven't been able to find any candidate.","Consider $f: \mathbb{R}^2 \rightarrow  \mathbb{R}$. Unlike functions of one variable, the partial derivatives may exist at a point even though $f$ is not continuous there. I have seen examples where $f_x$ and $f_y$ exist in the neighbourhood of a point, but $f$ is not continuous at that point. Can anyone provide an example of a function such that $f_x$ and $f_y$ exist everywhere but $f$ is nowhere continuous? It seems likely that such a function should exist, but I haven't been able to find any candidate.",,[]
94,Calculating triple integral over an ellipsoid,Calculating triple integral over an ellipsoid,,"I'm trying to calculate $$\iiint \frac{dx\,dy\,dz}{\sqrt{1 - \left(\frac{x^2}{9}+\frac{y^2}{16}+\frac{z^2}{25}\right)}}$$  over the ellipsoid $\frac{x^2}{9}+\frac{y^2}{16}+\frac{z^2}{25}=1$. I couldn't find a solution. Can anyone help me?","I'm trying to calculate $$\iiint \frac{dx\,dy\,dz}{\sqrt{1 - \left(\frac{x^2}{9}+\frac{y^2}{16}+\frac{z^2}{25}\right)}}$$  over the ellipsoid $\frac{x^2}{9}+\frac{y^2}{16}+\frac{z^2}{25}=1$. I couldn't find a solution. Can anyone help me?",,['multivariable-calculus']
95,Why is the magnitude of the cross product equal to the parallelogram spanned by the two vectors?,Why is the magnitude of the cross product equal to the parallelogram spanned by the two vectors?,,"Given vectors $a, b \in \mathbb{R}^3$ , the cross (vector) product of $a \times b = c$ is defined as a vector orthogonal to both $a$ and $b$ such that the right hand rule is satisfied, with the additional stipulation that $\|c\|=\|a\|\|b\|\sin(\theta)$ where $\theta$ is the angle between $a,b$ . But any vector $c’ \in \mathbb{R}^3$ such that \begin{align*} a \cdot c’ &= 0, \\ b \cdot c’ &=0 \end{align*} satisfies the orthogonality condition. For non-parallel $a,b$ , there are infinitely many vectors $c’$ that do this (by the rank theorem). Albeit, these $c’$ lie on the same line. My questions are: What is the purpose of wanting the cross product to have magnitude equal to the parallelogram spanned by $a$ and $b$ ? How does the determinant formulation of the cross product $$ c = \mathrm{det} \begin{bmatrix} \hat{\imath} & \hat{\jmath} & \hat{k} \\ a_x & a_y & a_z \\ b_x & b_y & b_z \end{bmatrix} $$ ensure that $\|c\|=\|a\|\|b\|\sin(\theta)$ ? I understand how to work backwards, i.e. to show that given this formulation, $\|c\|=\|a\|\|b\|\sin(\theta)$ . I am more interested in working forwards: why does this particular formulation give the desired magnitude?","Given vectors , the cross (vector) product of is defined as a vector orthogonal to both and such that the right hand rule is satisfied, with the additional stipulation that where is the angle between . But any vector such that satisfies the orthogonality condition. For non-parallel , there are infinitely many vectors that do this (by the rank theorem). Albeit, these lie on the same line. My questions are: What is the purpose of wanting the cross product to have magnitude equal to the parallelogram spanned by and ? How does the determinant formulation of the cross product ensure that ? I understand how to work backwards, i.e. to show that given this formulation, . I am more interested in working forwards: why does this particular formulation give the desired magnitude?","a, b \in \mathbb{R}^3 a \times b = c a b \|c\|=\|a\|\|b\|\sin(\theta) \theta a,b c’ \in \mathbb{R}^3 \begin{align*}
a \cdot c’ &= 0, \\
b \cdot c’ &=0
\end{align*} a,b c’ c’ a b 
c = \mathrm{det} \begin{bmatrix}
\hat{\imath} & \hat{\jmath} & \hat{k} \\
a_x & a_y & a_z \\
b_x & b_y & b_z
\end{bmatrix}
 \|c\|=\|a\|\|b\|\sin(\theta) \|c\|=\|a\|\|b\|\sin(\theta)","['multivariable-calculus', 'vectors', 'vector-analysis', 'cross-product']"
96,"Describing $\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(x, n) \; \mathrm{d}n$ as a contour integral.",Describing  as a contour integral.,"\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(x, n) \; \mathrm{d}n","My question essentially has to do with the derivative of a Contour Integral's parameterized curve. $$\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n$$ to be exact. Where $\partial \Omega(x)$ is a Jordan curve which is differentiable for any $x \in \mathbb{C}$ , and $f(n, x): \mathbb{C}^2 \to  \mathbb{C}$ integrable around the curve $\partial \Omega(x)$ in respect to $n$ . Define $\gamma$ as the parameterized curve of $\partial \Omega$ , and the terminology $f_x(n, x) = \frac{\partial f(n, x)}{\partial x}$ is used. My work has essentially gotten down to these steps. STEP 1: Turning the contour integral into the usual integral. $$\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n = \frac{\partial}{\partial x} \int_{0}^{2\pi} \gamma_\theta(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta.$$ STEP 2: Using the Liebniz rule. $$\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n = \int_{0}^{2\pi} \frac{\partial}{\partial x} \gamma_\theta(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta.$$ STEP 3: Taking the derivative. $$= \int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) + \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) + \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta.$$ STEP 4: Separating the integrals. $$= \int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) \; \mathrm{d}\theta$$ $$+ \int_{0}^{2\pi} \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) \mathrm{d}\theta$$ $$+ \int_{0}^{2\pi} \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta.$$ STEP 5: Simplifying the first integral into a contour integral. $$\int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) \; \mathrm{d}\theta = \oint_{\partial \Omega(x)} f_x(n, x) \; \mathrm{d}n.$$ STEP 6: Plugging in the first integral to get the final answer. $$\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n$$ $$= \int_{\partial \Omega(x)} f_x(n, x) \; \mathrm{d}n+\int_{0}^{2\pi} \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) \; \mathrm{d}\theta$$ $$+\int_{0}^{2\pi} \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta.$$ I am unsure of how to simplify this further or if this is even a decent approach. Does anybody have a good resource for this? My goal is to write this derivative as multiple contour integrals, bar any $\gamma$ -parameterized functions. (Disclaimer: This same question has been posted by myself to MathOverflow)","My question essentially has to do with the derivative of a Contour Integral's parameterized curve. to be exact. Where is a Jordan curve which is differentiable for any , and integrable around the curve in respect to . Define as the parameterized curve of , and the terminology is used. My work has essentially gotten down to these steps. STEP 1: Turning the contour integral into the usual integral. STEP 2: Using the Liebniz rule. STEP 3: Taking the derivative. STEP 4: Separating the integrals. STEP 5: Simplifying the first integral into a contour integral. STEP 6: Plugging in the first integral to get the final answer. I am unsure of how to simplify this further or if this is even a decent approach. Does anybody have a good resource for this? My goal is to write this derivative as multiple contour integrals, bar any -parameterized functions. (Disclaimer: This same question has been posted by myself to MathOverflow)","\frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n \partial \Omega(x) x \in \mathbb{C} f(n, x): \mathbb{C}^2 \to 
\mathbb{C} \partial \Omega(x) n \gamma \partial \Omega f_x(n, x) = \frac{\partial f(n, x)}{\partial x} \frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n = \frac{\partial}{\partial x} \int_{0}^{2\pi} \gamma_\theta(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta. \frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n = \int_{0}^{2\pi} \frac{\partial}{\partial x} \gamma_\theta(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta. = \int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) + \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) + \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta. = \int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) \; \mathrm{d}\theta + \int_{0}^{2\pi} \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) \mathrm{d}\theta + \int_{0}^{2\pi} \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta. \int_{0}^{2\pi} \gamma_\theta(\theta, x) f_x(\gamma(\theta, x), x) \; \mathrm{d}\theta = \oint_{\partial \Omega(x)} f_x(n, x) \; \mathrm{d}n. \frac{\partial}{\partial x} \oint_{\partial \Omega(x)} f(n, x) \; \mathrm{d}n = \int_{\partial \Omega(x)} f_x(n, x) \; \mathrm{d}n+\int_{0}^{2\pi} \gamma_x(\theta, x) \gamma_\theta(\theta, x) f_n(\gamma(\theta, x), x) \; \mathrm{d}\theta +\int_{0}^{2\pi} \gamma_{\theta x}(\theta, x) f(\gamma(\theta, x), x) \; \mathrm{d}\theta. \gamma","['multivariable-calculus', 'partial-derivative', 'contour-integration', 'parametric']"
97,Finding a parametre that satisfies an inequality,Finding a parametre that satisfies an inequality,,"For what values of $k>0$ does $$a^2+b^2+c^2+d^2+4(\sqrt3 -1)(abcd)^k\geq\sqrt{12(abc+abd+acd+bcd)}$$ hold for all $a,b,c,d\geq0$ satisfying $a+b+c+d=4$ ? On the one hand, I found a lower bound for the LHS by using an equivalent form of Turkevich's inequality : $3(a^2+b^2+c^2+d^2)\geq4(4-abcd)$ . On the other hand, for the RHS one can find an upper bound by using the famous ISL 1997 $64+44abcd\geq27(abc+abd+acd+bcd)$ . Yet, I am stuck, I cannot make any progress towards finding the range of $k$ .","For what values of does hold for all satisfying ? On the one hand, I found a lower bound for the LHS by using an equivalent form of Turkevich's inequality : . On the other hand, for the RHS one can find an upper bound by using the famous ISL 1997 . Yet, I am stuck, I cannot make any progress towards finding the range of .","k>0 a^2+b^2+c^2+d^2+4(\sqrt3 -1)(abcd)^k\geq\sqrt{12(abc+abd+acd+bcd)} a,b,c,d\geq0 a+b+c+d=4 3(a^2+b^2+c^2+d^2)\geq4(4-abcd) 64+44abcd\geq27(abc+abd+acd+bcd) k","['multivariable-calculus', 'inequality', 'optimization', 'contest-math', 'mixing-variables']"
98,Inequality : $\frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big)$,Inequality :,\frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big),"It's a charming problem : Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big)$$ I know the identity : Let $a,b,c>0$ such that $a+b+c=1$ then we have : $$\frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a}=1.5$$ But I think it's not relevant here . I try also majorization with the inequality : Let $a\geq b\geq c>0$ such that $a+b+c=1$ then we have : $$\exp\Big(\frac{-2}{3}\Big)a\geq \frac{a}{\exp(a+b)}$$ Second line of the majorization : $$\exp\Big(\frac{-2}{3}\Big)^2ab\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)}$$ Third line of the majorization : $$\exp\Big(\frac{-2}{3}\Big)^3abc\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)}\frac{c}{\exp(c+a)}$$ The lines are easy to check with the condition remains to apply Karamata's inequality and we are done . Unfortunately the second line fails . My question : Have you a proof ? Thanks a lot for sharing your time and knowledge .",It's a charming problem : Let such that then we have : I know the identity : Let such that then we have : But I think it's not relevant here . I try also majorization with the inequality : Let such that then we have : Second line of the majorization : Third line of the majorization : The lines are easy to check with the condition remains to apply Karamata's inequality and we are done . Unfortunately the second line fails . My question : Have you a proof ? Thanks a lot for sharing your time and knowledge .,"a,b,c>0 a+b+c=1 \frac{a}{\exp(a+b)}+\frac{b}{\exp(b+c)}+\frac{c}{\exp(c+a)}\leq \exp\Big(\frac{-2}{3}\Big) a,b,c>0 a+b+c=1 \frac{a}{a+b}+\frac{b}{b+c}+\frac{c}{c+a}=1.5 a\geq b\geq c>0 a+b+c=1 \exp\Big(\frac{-2}{3}\Big)a\geq \frac{a}{\exp(a+b)} \exp\Big(\frac{-2}{3}\Big)^2ab\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)} \exp\Big(\frac{-2}{3}\Big)^3abc\geq \frac{a}{\exp(a+b)}\frac{b}{\exp(b+c)}\frac{c}{\exp(c+a)}","['multivariable-calculus', 'inequality', 'exponential-function', 'sum-of-squares-method', 'rearrangement-inequality']"
99,Is this true for a continuously differentiable function,Is this true for a continuously differentiable function,,"Prove or find a counterexample: if $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is continuously differentiable with $f(0)=0$  then there exist continuous functions $g_1,\dots,g_n:\mathbb{R}^n\rightarrow \mathbb{R}$ with $$f(x)=x_1g_1(x_1,\dots, x_n)+\dots+x_ng_n(x_1,\dots, x_n)$$ I don't know where to start. Should I use the implicit function theorem? Please give me some hints. Happy new year.","Prove or find a counterexample: if $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is continuously differentiable with $f(0)=0$  then there exist continuous functions $g_1,\dots,g_n:\mathbb{R}^n\rightarrow \mathbb{R}$ with $$f(x)=x_1g_1(x_1,\dots, x_n)+\dots+x_ng_n(x_1,\dots, x_n)$$ I don't know where to start. Should I use the implicit function theorem? Please give me some hints. Happy new year.",,['multivariable-calculus']
