,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Set of continuity points of a real function,Set of continuity points of a real function,,"I have a question about subsets $$ A \subseteq \mathbb R $$ for which there exists a function $$f : \mathbb R \to \mathbb R$$ such that the set of continuity points of $f$ is $A$. Can I characterize this kind of sets? In a topological,measurable or in some way? For example, does there exist a function continuous on $\mathbb Q$ and discontinuous on the irrationals?","I have a question about subsets $$ A \subseteq \mathbb R $$ for which there exists a function $$f : \mathbb R \to \mathbb R$$ such that the set of continuity points of $f$ is $A$. Can I characterize this kind of sets? In a topological,measurable or in some way? For example, does there exist a function continuous on $\mathbb Q$ and discontinuous on the irrationals?",,"['real-analysis', 'general-topology', 'measure-theory']"
1,How do people apply the Lebesgue integration theory?,How do people apply the Lebesgue integration theory?,,"This question has puzzled me for a long time. It may be too vague to ask here. I hope I can narrow down the question well so that one can offer some ideas. In a lot of calculus textbooks, there is usually a chapter about ""applications"" after the one for Riemann integral. Students can do a lot of calculations and appreciate the power of Riemann integration --- they solve many problems in physics and geometry. While learning the Lebesgue integral, or more generally, integration on measure space, I cannot appreciate the power of this kind of integration util I learn some modern PDE. One the other hand, I found that there are much much more inequalities when doing Lebesgue integration than equations when applying Riemann integral. Instead of calculating something, people do estimation with the convergence theorems. Here are my questions: How do people apply Lebesgue integration theory? If putting the methods into categories, can I say that it primarily deals with the problems related to convergence ? What's the fundamental difference between applying these two different integration techniques? Is there an example such that people solve some problem which may be very hard (but still can be solved) when using Riemann integral but relatively easy with Lebesgue integral?","This question has puzzled me for a long time. It may be too vague to ask here. I hope I can narrow down the question well so that one can offer some ideas. In a lot of calculus textbooks, there is usually a chapter about ""applications"" after the one for Riemann integral. Students can do a lot of calculations and appreciate the power of Riemann integration --- they solve many problems in physics and geometry. While learning the Lebesgue integral, or more generally, integration on measure space, I cannot appreciate the power of this kind of integration util I learn some modern PDE. One the other hand, I found that there are much much more inequalities when doing Lebesgue integration than equations when applying Riemann integral. Instead of calculating something, people do estimation with the convergence theorems. Here are my questions: How do people apply Lebesgue integration theory? If putting the methods into categories, can I say that it primarily deals with the problems related to convergence ? What's the fundamental difference between applying these two different integration techniques? Is there an example such that people solve some problem which may be very hard (but still can be solved) when using Riemann integral but relatively easy with Lebesgue integral?",,"['real-analysis', 'measure-theory']"
2,Differentiating an Inner Product,Differentiating an Inner Product,,"If $(V, \langle \cdot, \cdot \rangle)$ is a finite-dimensional inner product space and $f,g : \mathbb{R} \longrightarrow V$ are differentiable functions, a straightforward calculation with components shows that $$ \frac{d}{dt} \langle f, g \rangle = \langle f(t), g^{\prime}(t) \rangle + \langle f^{\prime}(t), g(t) \rangle $$ This approach is not very satisfying. However, attempting to apply the definition of the derivative directly doesn't seem to work for me. Is there a slick, perhaps intrinsic way, to prove this that doesn't involve working in coordinates?","If $(V, \langle \cdot, \cdot \rangle)$ is a finite-dimensional inner product space and $f,g : \mathbb{R} \longrightarrow V$ are differentiable functions, a straightforward calculation with components shows that $$ \frac{d}{dt} \langle f, g \rangle = \langle f(t), g^{\prime}(t) \rangle + \langle f^{\prime}(t), g(t) \rangle $$ This approach is not very satisfying. However, attempting to apply the definition of the derivative directly doesn't seem to work for me. Is there a slick, perhaps intrinsic way, to prove this that doesn't involve working in coordinates?",,"['real-analysis', 'functional-analysis', 'inner-products', 'derivatives']"
3,Is $n \sin n$ dense on the real line?,Is  dense on the real line?,n \sin n,"Is $\{n \sin n | n \in \mathbb{N}\}$ dense on the real line? If so, is $\{n^p \sin n | n \in \mathbb{N}\}$ dense for all $p>0$? This seems much harder than showing that $\sin n$ is dense on [-1,1], which is easy to show. EDIT: This seems a bit harder than the following related problem, which might give some insight: When is $\{n^p [ \sqrt{2} n ] | n \in \mathbb{N}\}$ dense on the real line, where $[\cdot]$ is the fractional part of the expression? I am thinking that there should be some probabilistic argument for these things. EDIT 2: Ok, so plotting a histogram over $n \sin n$ is similar to plotting $n \sin(2\pi X)$ where $X$ is a uniform distribution on $[-1,1].$ This is not surprising, since $n$ mod $2\pi$ is distributed uniformly on  $[0,2\pi].$ Now, the pdf of $\sin(2\pi X)$ is given by $f(x)=\frac{2}{\pi \sqrt{1-x^2}}$ in $(-1,1)$ and 0 outside this set. The pdf for $n \sin(2\pi X)$ is $g_n(x)=\sum_{k=1}^n \frac{1}{nk} f(x/k)$ so the limit density is what we get when $n \rightarrow \infty.$ (This integrates to 1 over the real line). Now, it should be straightforward to show that for any interval $[a,b],$ $\int_a^b g_n(x) dx \rightarrow 0$ as $n \rightarrow \infty.$ Thus, the series $g_n$ is ""too flat"" to be able to accumulate positive probability anywhere. (The gaussian distribution on the other hand, has positive integral on every interval).","Is $\{n \sin n | n \in \mathbb{N}\}$ dense on the real line? If so, is $\{n^p \sin n | n \in \mathbb{N}\}$ dense for all $p>0$? This seems much harder than showing that $\sin n$ is dense on [-1,1], which is easy to show. EDIT: This seems a bit harder than the following related problem, which might give some insight: When is $\{n^p [ \sqrt{2} n ] | n \in \mathbb{N}\}$ dense on the real line, where $[\cdot]$ is the fractional part of the expression? I am thinking that there should be some probabilistic argument for these things. EDIT 2: Ok, so plotting a histogram over $n \sin n$ is similar to plotting $n \sin(2\pi X)$ where $X$ is a uniform distribution on $[-1,1].$ This is not surprising, since $n$ mod $2\pi$ is distributed uniformly on  $[0,2\pi].$ Now, the pdf of $\sin(2\pi X)$ is given by $f(x)=\frac{2}{\pi \sqrt{1-x^2}}$ in $(-1,1)$ and 0 outside this set. The pdf for $n \sin(2\pi X)$ is $g_n(x)=\sum_{k=1}^n \frac{1}{nk} f(x/k)$ so the limit density is what we get when $n \rightarrow \infty.$ (This integrates to 1 over the real line). Now, it should be straightforward to show that for any interval $[a,b],$ $\int_a^b g_n(x) dx \rightarrow 0$ as $n \rightarrow \infty.$ Thus, the series $g_n$ is ""too flat"" to be able to accumulate positive probability anywhere. (The gaussian distribution on the other hand, has positive integral on every interval).",,['real-analysis']
4,Proving that $m+n\sqrt{2}$ is dense in $\mathbb R$,Proving that  is dense in,m+n\sqrt{2} \mathbb R,"I am having trouble proving the statement: Let $$S = \{m + n\sqrt 2 : m, n \in\mathbb Z\}$$ Prove that for every $\epsilon > 0$ , the intersection of $S$ and $(0, \epsilon)$ is nonempty.","I am having trouble proving the statement: Let Prove that for every , the intersection of and is nonempty.","S = \{m + n\sqrt 2 : m, n \in\mathbb Z\} \epsilon > 0 S (0, \epsilon)","['real-analysis', 'irrational-numbers', 'diophantine-approximation']"
5,"Showing that $\int\limits_{-a}^a \frac{f(x)}{1+e^{x}} \mathrm dx = \int\limits_0^a f(x) \mathrm dx$, when $f$ is even","Showing that , when  is even",\int\limits_{-a}^a \frac{f(x)}{1+e^{x}} \mathrm dx = \int\limits_0^a f(x) \mathrm dx f,"I have a question: Suppose $f$ is continuous and even on $[-a,a]$, $a>0$ then prove that   $$\int\limits_{-a}^a \frac{f(x)}{1+e^{x}} \mathrm dx = \int\limits_0^a f(x) \mathrm dx$$ How can I do this? Don't know how to start.","I have a question: Suppose $f$ is continuous and even on $[-a,a]$, $a>0$ then prove that   $$\int\limits_{-a}^a \frac{f(x)}{1+e^{x}} \mathrm dx = \int\limits_0^a f(x) \mathrm dx$$ How can I do this? Don't know how to start.",,"['calculus', 'real-analysis', 'integration']"
6,Does the series $ \sum\limits_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}} $ converge or diverge?,Does the series  converge or diverge?, \sum\limits_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}} ,Does the following series converge or diverge? I would like to see a demonstration. $$ \sum_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}}. $$ I can see that: $$ \sum_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}} \leqslant \sum_{n=1}^{\infty} \frac{1}{n^{1 + {\sin^{2}}(n)}} \leqslant \sum_{n=1}^{\infty} \frac{1}{n^{1 + {\sin^{2n}}(n)}}. $$,Does the following series converge or diverge? I would like to see a demonstration. $$ \sum_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}}. $$ I can see that: $$ \sum_{n=1}^{\infty} \frac{1}{n^{1 + |\sin(n)|}} \leqslant \sum_{n=1}^{\infty} \frac{1}{n^{1 + {\sin^{2}}(n)}} \leqslant \sum_{n=1}^{\infty} \frac{1}{n^{1 + {\sin^{2n}}(n)}}. $$,,"['real-analysis', 'sequences-and-series']"
7,What is the difference between minimum and infimum?,What is the difference between minimum and infimum?,,What is the difference between minimum and infimum? I have a great confusion about this.,What is the difference between minimum and infimum? I have a great confusion about this.,,"['real-analysis', 'convex-optimization']"
8,Is $ \sum\limits_{n=1}^\infty \frac{|\sin n|^n}n$ convergent？,Is  convergent？, \sum\limits_{n=1}^\infty \frac{|\sin n|^n}n,"Is the series $$ \sum_{n=1}^\infty \frac{|\sin n|^n}n\tag{1}$$ convergent？ If one want to use Abel's test, is $$ \sum_{n=1}^\infty |\sin n|^n\tag{2}$$ convergent？ Thank you very much","Is the series $$ \sum_{n=1}^\infty \frac{|\sin n|^n}n\tag{1}$$ convergent？ If one want to use Abel's test, is $$ \sum_{n=1}^\infty |\sin n|^n\tag{2}$$ convergent？ Thank you very much",,"['real-analysis', 'calculus', 'sequences-and-series', 'trigonometry', 'convergence-divergence']"
9,What's wrong with this reasoning that $\frac{\infty}{\infty}=0$?,What's wrong with this reasoning that ?,\frac{\infty}{\infty}=0,"$$\frac{n}{\infty} + \frac{n}{\infty} +\dots = \frac{\infty}{\infty}$$ You can always break up $\infty/\infty$ into the left hand side, where n is an arbitrary number. However, on the left hand side $\frac{n}{\infty}$ is always equal to $0$. Thus $\frac{\infty}{\infty}$ should always equal $0$.","$$\frac{n}{\infty} + \frac{n}{\infty} +\dots = \frac{\infty}{\infty}$$ You can always break up $\infty/\infty$ into the left hand side, where n is an arbitrary number. However, on the left hand side $\frac{n}{\infty}$ is always equal to $0$. Thus $\frac{\infty}{\infty}$ should always equal $0$.",,"['real-analysis', 'infinity', 'fake-proofs', 'indeterminate-forms']"
10,Choice of $q$ in Baby Rudin's Example 1.1,Choice of  in Baby Rudin's Example 1.1,q,"First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number $q$ that satisfies equations (3) and (4), plus other conditions needed to show that $q$ is the right number for the proof.  As an exercise, I tried to derive his choice of $q$ so that I may learn more about the problem. If we write equations (3) as $q = p - (p^2 - 2)x$ , we can write (4) as $$ q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2]. $$ Here, we need a rational $x > 0$ , chosen such that the expression in $[...]$ is positive.  Using the quadratic formula and the sign of $(p^2 - 2)$ , it can be shown that we need $$ x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A, $$ or, for $p \in B$ , $x < 1/\left(p + \sqrt{2}\right)$ or $x > 1/\left(p - \sqrt{2}\right)$ . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting $x = 1/(p + n)$ for $n \geq 2$ .  Notice that Rudin chooses $n = 2$ for his answer, but it checks out easily for other $n$ . The Question: Why does Rudin choose $x = 1/(p + 2)$ specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number $q$ that I am missing?","First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number that satisfies equations (3) and (4), plus other conditions needed to show that is the right number for the proof.  As an exercise, I tried to derive his choice of so that I may learn more about the problem. If we write equations (3) as , we can write (4) as Here, we need a rational , chosen such that the expression in is positive.  Using the quadratic formula and the sign of , it can be shown that we need or, for , or . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting for .  Notice that Rudin chooses for his answer, but it checks out easily for other . The Question: Why does Rudin choose specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number that I am missing?","q q q q = p - (p^2 - 2)x 
q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2].
 x > 0 [...] (p^2 - 2) 
x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A,
 p \in B x < 1/\left(p + \sqrt{2}\right) x > 1/\left(p - \sqrt{2}\right) x = 1/(p + n) n \geq 2 n = 2 n x = 1/(p + 2) q","['real-analysis', 'analysis', 'self-learning']"
11,Is there any geometric intuition for the factorials in Taylor expansions?,Is there any geometric intuition for the factorials in Taylor expansions?,,"Given a smooth real function $f$ , we can approximate it as a sum of polynomials as $$f(x+h)=f(x)+h f'(x) + \frac{h^2}{2!} f''(x)+ \dotsb = \sum_{k=0}^n \frac{h^k}{k!} f^{(k)}(x) + h^n R_n(h),$$ where $\lim_{h\to0} R_n(h)=0$ . There are multiple ways to derive this result. Some are already discussed in the answers to the question "" Where do the factorials come from in the taylor series? "". An easy way to see why the $1/k!$ factors must be there is to observe that computing $\partial_h^k f(x+h)\rvert_{h=0}$ , we need the $1/k!$ factors to balance the $k!$ factors arising from $\partial_h^k h^k=k!$ in order to get a consistent result on the left- and right-hand sides. However, even though algebraically it is very clear why we need these factorials, I don't have any intuition as to why they should be there. Is there any geometrical (or similarly intuitive) argument to see where they come from?","Given a smooth real function , we can approximate it as a sum of polynomials as where . There are multiple ways to derive this result. Some are already discussed in the answers to the question "" Where do the factorials come from in the taylor series? "". An easy way to see why the factors must be there is to observe that computing , we need the factors to balance the factors arising from in order to get a consistent result on the left- and right-hand sides. However, even though algebraically it is very clear why we need these factorials, I don't have any intuition as to why they should be there. Is there any geometrical (or similarly intuitive) argument to see where they come from?","f f(x+h)=f(x)+h f'(x) + \frac{h^2}{2!} f''(x)+ \dotsb = \sum_{k=0}^n \frac{h^k}{k!} f^{(k)}(x) + h^n R_n(h), \lim_{h\to0} R_n(h)=0 1/k! \partial_h^k f(x+h)\rvert_{h=0} 1/k! k! \partial_h^k h^k=k!","['real-analysis', 'calculus', 'derivatives', 'polynomials', 'taylor-expansion']"
12,What is the average rational number?,What is the average rational number?,,"Let $Q=\mathbb Q \cap(0,1)= \{r_1,r_2,\ldots\}$ be the rational numbers in $(0,1)$ listed out so we can count them. Define $x_n=\frac{1}{n}\sum_{k=1}^nr_n$ to be the average of the first $n$ rational numbers from the list. Questions: What is required for $x_n$ to converge? Certainly $0< x_n < 1$ for all $n$. Does $x_n$ converge to a rational or irrational? How does the behavior of the sequence depend on the choice of list? I.e. what if we rearrange the list $\mathbb Q \cap(0,1)=\{r_{p(1)},r_{p(2)},\ldots\}$ with some one-to-one permutation $p: \mathbb N \to \mathbb N$? How does the behavior of $x_n$ depend on $p$? My thoughts: Intuitively, I feel that we might be able to choose a $p$ so that $x_n\rightarrow y$ for any $y\in[0,1]$. However, it also makes intuitive sense that, if each rational appears only once in the list, that the limit is required to be $\frac{1}{2}.$ Of course, intuition can be very misleading with infinities! If we are allowed to repeat rational numbers with arbitrary frequency (but still capturing every rational eventually), then we might be able to choose a listing so that $x_n\rightarrow y$ for any $y\in(0,\infty)$. This last point might be proved by the fact that every positive real number has a sequence of positive rationals converging to it, and every rational in that list can be expressed as a sum of positive rationals less than one. However, the averaging may complicate that idea, and I'll have to think about it more. Example I: No repetition: $$Q=\bigcup_{n=1}^\infty \bigcup_{k=1}^n \left\{\frac{k}{n+1}\right\} =\left\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{3}{4},\frac{1}{5},\ldots\right\}$$ in which case $x_n\rightarrow\frac{1}{2},$ a very nice and simple example. Even if we keep the non-reduced fractions and allow repetition, i.e. with $Q=\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\boxed{\frac{2}{4},}\frac{3}{4},\frac{1}{5},\ldots\},$ then $x_n\rightarrow\frac{1}{2}.$ The latter case is easy to prove since we have the subsequence $x_{n_k}=\frac{1}{2}$ for $n_k=\frac{k(k+1)}{2},$ and the deviations from $1/2$ decrease. The non-repetition case, I haven't proved, but simulated numerically, so there may be an error, but I figure there is an easy calculation to show whether it's correct. Example II: Consider the list generated from the Stern-Brocot tree: $$Q=\left\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{2}{5},\frac{3}{5},\frac{3}{4},\ldots\right\}.$$ I'm sure this list could be studied analytically, but for now, I've just done a numerical simulation. The sequence of averages $x_n$ hits $\frac{1}{2}$ infinitely often, but may be oscillatory and hence not converge. If it converges, it does so much slower than the previous examples. It appears that $x_{2^k-1}=0.5$ for all $k$ and that between those values it comes very close to $0.44,$ e.g. $x_{95743}\approx 0.4399.$ However, my computer code is probably not very efficient, and becomes very slow past this.","Let $Q=\mathbb Q \cap(0,1)= \{r_1,r_2,\ldots\}$ be the rational numbers in $(0,1)$ listed out so we can count them. Define $x_n=\frac{1}{n}\sum_{k=1}^nr_n$ to be the average of the first $n$ rational numbers from the list. Questions: What is required for $x_n$ to converge? Certainly $0< x_n < 1$ for all $n$. Does $x_n$ converge to a rational or irrational? How does the behavior of the sequence depend on the choice of list? I.e. what if we rearrange the list $\mathbb Q \cap(0,1)=\{r_{p(1)},r_{p(2)},\ldots\}$ with some one-to-one permutation $p: \mathbb N \to \mathbb N$? How does the behavior of $x_n$ depend on $p$? My thoughts: Intuitively, I feel that we might be able to choose a $p$ so that $x_n\rightarrow y$ for any $y\in[0,1]$. However, it also makes intuitive sense that, if each rational appears only once in the list, that the limit is required to be $\frac{1}{2}.$ Of course, intuition can be very misleading with infinities! If we are allowed to repeat rational numbers with arbitrary frequency (but still capturing every rational eventually), then we might be able to choose a listing so that $x_n\rightarrow y$ for any $y\in(0,\infty)$. This last point might be proved by the fact that every positive real number has a sequence of positive rationals converging to it, and every rational in that list can be expressed as a sum of positive rationals less than one. However, the averaging may complicate that idea, and I'll have to think about it more. Example I: No repetition: $$Q=\bigcup_{n=1}^\infty \bigcup_{k=1}^n \left\{\frac{k}{n+1}\right\} =\left\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{3}{4},\frac{1}{5},\ldots\right\}$$ in which case $x_n\rightarrow\frac{1}{2},$ a very nice and simple example. Even if we keep the non-reduced fractions and allow repetition, i.e. with $Q=\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\boxed{\frac{2}{4},}\frac{3}{4},\frac{1}{5},\ldots\},$ then $x_n\rightarrow\frac{1}{2}.$ The latter case is easy to prove since we have the subsequence $x_{n_k}=\frac{1}{2}$ for $n_k=\frac{k(k+1)}{2},$ and the deviations from $1/2$ decrease. The non-repetition case, I haven't proved, but simulated numerically, so there may be an error, but I figure there is an easy calculation to show whether it's correct. Example II: Consider the list generated from the Stern-Brocot tree: $$Q=\left\{\frac{1}{2},\frac{1}{3},\frac{2}{3},\frac{1}{4},\frac{2}{5},\frac{3}{5},\frac{3}{4},\ldots\right\}.$$ I'm sure this list could be studied analytically, but for now, I've just done a numerical simulation. The sequence of averages $x_n$ hits $\frac{1}{2}$ infinitely often, but may be oscillatory and hence not converge. If it converges, it does so much slower than the previous examples. It appears that $x_{2^k-1}=0.5$ for all $k$ and that between those values it comes very close to $0.44,$ e.g. $x_{95743}\approx 0.4399.$ However, my computer code is probably not very efficient, and becomes very slow past this.",,"['real-analysis', 'sequences-and-series', 'rational-numbers']"
13,Sum of two closed sets in $\mathbb R$ is closed?,Sum of two closed sets in  is closed?,\mathbb R,"Is there a counterexample for the claim in the question subject, that a sum of two closed sets in $\mathbb R$ is closed? If not, how can we prove it? (By sum of sets $X+Y$ I mean the set of all sums $x+y$ where $x$ is in $X$ and $y$ is in $Y$) Thanks!","Is there a counterexample for the claim in the question subject, that a sum of two closed sets in $\mathbb R$ is closed? If not, how can we prove it? (By sum of sets $X+Y$ I mean the set of all sums $x+y$ where $x$ is in $X$ and $y$ is in $Y$) Thanks!",,['real-analysis']
14,Can squares of infinite area always cover a unit square?,Can squares of infinite area always cover a unit square?,,"This is a claim one of my students made without justification on his exam. It definitely wasn't the right way to approach the problem, but now I've been nerdsniped into trying to figure out if it is true. Let $a_i$ be a sequence of positive reals such that $\sum a_i^2 = \infty$. Then $[0,1]^2$ can be covered by translates of the squares $[0,a_i]^2$. It is definitely not enough that $\sum a_i^2>1$: You can't cover a unit square with $5$ axis-aligned squares of sidelength $1/\sqrt{5}$.","This is a claim one of my students made without justification on his exam. It definitely wasn't the right way to approach the problem, but now I've been nerdsniped into trying to figure out if it is true. Let $a_i$ be a sequence of positive reals such that $\sum a_i^2 = \infty$. Then $[0,1]^2$ can be covered by translates of the squares $[0,a_i]^2$. It is definitely not enough that $\sum a_i^2>1$: You can't cover a unit square with $5$ axis-aligned squares of sidelength $1/\sqrt{5}$.",,"['real-analysis', 'geometry', 'tiling']"
15,$\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite,is infinite,\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}},How do I prove that $ \displaystyle\lim_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite?,How do I prove that $ \displaystyle\lim_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite?,,"['real-analysis', 'sequences-and-series', 'limits', 'factorial', 'radicals']"
16,Is Complex Analysis equivalent Real Analysis with $f:\mathbb R^2 \to \mathbb R^2$?,Is Complex Analysis equivalent Real Analysis with ?,f:\mathbb R^2 \to \mathbb R^2,"Am I correct in noticing that Complex Analysis seems to be a synonym for analysis of functions $\mathbb R^2 \to \mathbb R^2$? If this is the case, surely all the results from complex analysis carry over to the study of these $\mathbb R^2 \to \mathbb R^2$ functions. Does anything from complex analysis carry over into the study of functions in even higher dimensions? Furthermore, is there an area of Mathematics similar to complex analysis that investigates functions, say, $\mathbb R^3 \to \mathbb R^3$ to the same level of detail?","Am I correct in noticing that Complex Analysis seems to be a synonym for analysis of functions $\mathbb R^2 \to \mathbb R^2$? If this is the case, surely all the results from complex analysis carry over to the study of these $\mathbb R^2 \to \mathbb R^2$ functions. Does anything from complex analysis carry over into the study of functions in even higher dimensions? Furthermore, is there an area of Mathematics similar to complex analysis that investigates functions, say, $\mathbb R^3 \to \mathbb R^3$ to the same level of detail?",,"['real-analysis', 'complex-analysis', 'soft-question']"
17,Is there a rational number between any two irrationals?,Is there a rational number between any two irrationals?,,"Suppose $i_1$ and $i_2$ are distinct irrational numbers with $i_1 < i_2$.  Is it necessarily the case that there is a rational number $r$ in the interval $[i_1, i_2]$?  How would you construct such a rational number? [I posted this only so that the useful answers at https://math.stackexchange.com/questions/414036/rationals-and-irrationals-on-the-real-number-line/414048#414048 could be merged here before that question was deleted.]","Suppose $i_1$ and $i_2$ are distinct irrational numbers with $i_1 < i_2$.  Is it necessarily the case that there is a rational number $r$ in the interval $[i_1, i_2]$?  How would you construct such a rational number? [I posted this only so that the useful answers at https://math.stackexchange.com/questions/414036/rationals-and-irrationals-on-the-real-number-line/414048#414048 could be merged here before that question was deleted.]",,"['real-analysis', 'irrational-numbers', 'rational-numbers']"
18,What is the 'implicit function theorem'?,What is the 'implicit function theorem'?,,"Please give me an intuitive explanation of 'implicit function theorem'. I read some bits and pieces of information from some textbook, but they look too confusing, especially I do not understand why they use Jacobian matrix to illustrate this theorem.","Please give me an intuitive explanation of 'implicit function theorem'. I read some bits and pieces of information from some textbook, but they look too confusing, especially I do not understand why they use Jacobian matrix to illustrate this theorem.",,"['real-analysis', 'intuition', 'multivariable-calculus']"
19,Proving $\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right)$,Proving,\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right),"The equality$$\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right)\tag{1}$$follows from the fact that the sum of the first series is $\dfrac\pi4$, whereas the sum of the second one is $\dfrac{\pi^2}6$. My question is: can someone provide a proof that $(1)$ holds without using this?","The equality$$\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right)\tag{1}$$follows from the fact that the sum of the first series is $\dfrac\pi4$, whereas the sum of the second one is $\dfrac{\pi^2}6$. My question is: can someone provide a proof that $(1)$ holds without using this?",,"['real-analysis', 'sequences-and-series', 'summation']"
20,"Is there a ""good"" reason why $\left\lfloor \frac{n!}{11e}\right\rfloor$ is always even?","Is there a ""good"" reason why  is always even?",\left\lfloor \frac{n!}{11e}\right\rfloor,"(A follow-up of sorts to this question .) The quantity $\left\lfloor \frac{n!}{11e}\right\rfloor$ is always even, which can be proved as follows. Using the sum for $\frac{1}{e}$, we split the fraction up into three parts: $A_n=\sum_{k=0}^{n-11} (-1)^k\frac{n!}{11k!}$ is a multiple of the even integer $10! \binom{n}{11}$, and so can be ignored. $B_n=\sum_{k=n-10}^n (-1)^k\frac{n!}{11k!}=\frac{1}{11}\sum_{k=0}^{10} (-1)^{n-k}(n)_k$. This is a finite sum of falling factorials, all of which are polynomial in $n$ with integer coefficients. So $B_n$ is of the form $\frac{P(n)}{11}$ where $P(n)$ is a polynomial in $n$ with integer coefficients. $C_n = \sum_{k=n+1}^{\infty} (-1)^k\frac{n!}{11k!}$ is an alternating series whose terms decrease monotonically in absolute value, and so $|C_n|<\frac{n!}{11(n+1)!}<\frac{1}{11}$. Putting all this together, we can see that: Since $B_n$ is always an integer multiple of $\frac{1}{11}$ and $|C_n|<\frac{1}{11}$, $C_n$ can only affect the value of $\left\lfloor \frac{n!}{11e}\right\rfloor$ when $B_n$ is an integer. In this case it will change the parity when $C_n$ is negative (i.e., $n$ is even) and leave it alone when $C_n$ is positive. Since $P(n)=11B_n$ is a polynomial with integer coefficients, $B_n$'s integer status is $11$-periodic, which means that whether $C_n$ affects the parity of $\left\lfloor \frac{n!}{11e}\right\rfloor$ is $22$-periodic. Similarly, the parity of $\lfloor B_n \rfloor$ is also $22$-periodic. So the parity of $\left\lfloor \frac{n!}{11e}\right\rfloor$ is $22$-periodic. Moreover, we can compute its first $22$ values to be: $$ 0, 0, 0, 0, 0, 4, 24, 168, 1348, 12136, 121360, 1334960, 16019530, 208253902, 2915554640, 43733319612, 699733113794, 11895462934514, 214118332821268, 4068248323604100, 81364966472082010, 1708664295913722230 $$ (a sequence which does not appear to be in OEIS). All of these are even, and so $\left\lfloor \frac{n!}{11e}\right\rfloor$ must be even for all $n$. This is not a very satisfying proof, though; in the end, it looks like we need a random $2^{22}$-fold coincidence to go our way in order to get the result we want. (In fact, that's the only place we used the specific value of $11$ in our proof at all; the rest of the proof shows that the parity of $\left\lfloor \frac{n!}{ke}\right\rfloor$ is $2k$-periodic for all positive integers $k$.) Even though $11$ was chosen arbitrarily, it looks like the heuristic probability of everything lining up falls off rapidly enough that it's surprising it all works out for any $k$ which is even that large. Can someone convince me that this fact is less surprising than it looks? I would take either a completely different proof that established the result with less case analysis, or a reason why the parity of these numbers can be expected to be non-independent...","(A follow-up of sorts to this question .) The quantity $\left\lfloor \frac{n!}{11e}\right\rfloor$ is always even, which can be proved as follows. Using the sum for $\frac{1}{e}$, we split the fraction up into three parts: $A_n=\sum_{k=0}^{n-11} (-1)^k\frac{n!}{11k!}$ is a multiple of the even integer $10! \binom{n}{11}$, and so can be ignored. $B_n=\sum_{k=n-10}^n (-1)^k\frac{n!}{11k!}=\frac{1}{11}\sum_{k=0}^{10} (-1)^{n-k}(n)_k$. This is a finite sum of falling factorials, all of which are polynomial in $n$ with integer coefficients. So $B_n$ is of the form $\frac{P(n)}{11}$ where $P(n)$ is a polynomial in $n$ with integer coefficients. $C_n = \sum_{k=n+1}^{\infty} (-1)^k\frac{n!}{11k!}$ is an alternating series whose terms decrease monotonically in absolute value, and so $|C_n|<\frac{n!}{11(n+1)!}<\frac{1}{11}$. Putting all this together, we can see that: Since $B_n$ is always an integer multiple of $\frac{1}{11}$ and $|C_n|<\frac{1}{11}$, $C_n$ can only affect the value of $\left\lfloor \frac{n!}{11e}\right\rfloor$ when $B_n$ is an integer. In this case it will change the parity when $C_n$ is negative (i.e., $n$ is even) and leave it alone when $C_n$ is positive. Since $P(n)=11B_n$ is a polynomial with integer coefficients, $B_n$'s integer status is $11$-periodic, which means that whether $C_n$ affects the parity of $\left\lfloor \frac{n!}{11e}\right\rfloor$ is $22$-periodic. Similarly, the parity of $\lfloor B_n \rfloor$ is also $22$-periodic. So the parity of $\left\lfloor \frac{n!}{11e}\right\rfloor$ is $22$-periodic. Moreover, we can compute its first $22$ values to be: $$ 0, 0, 0, 0, 0, 4, 24, 168, 1348, 12136, 121360, 1334960, 16019530, 208253902, 2915554640, 43733319612, 699733113794, 11895462934514, 214118332821268, 4068248323604100, 81364966472082010, 1708664295913722230 $$ (a sequence which does not appear to be in OEIS). All of these are even, and so $\left\lfloor \frac{n!}{11e}\right\rfloor$ must be even for all $n$. This is not a very satisfying proof, though; in the end, it looks like we need a random $2^{22}$-fold coincidence to go our way in order to get the result we want. (In fact, that's the only place we used the specific value of $11$ in our proof at all; the rest of the proof shows that the parity of $\left\lfloor \frac{n!}{ke}\right\rfloor$ is $2k$-periodic for all positive integers $k$.) Even though $11$ was chosen arbitrarily, it looks like the heuristic probability of everything lining up falls off rapidly enough that it's surprising it all works out for any $k$ which is even that large. Can someone convince me that this fact is less surprising than it looks? I would take either a completely different proof that established the result with less case analysis, or a reason why the parity of these numbers can be expected to be non-independent...",,"['real-analysis', 'sequences-and-series', 'elementary-number-theory', 'ceiling-and-floor-functions']"
21,What is the theme of analysis?,What is the theme of analysis?,,"It is safe to say that every mathematician, at some point in their career, has had some form of exposure to analysis. Quite often, it appears first in the form of an undergraduate course in real analysis. It is there that one is often exposed to a rigorous viewpoint to the techniques of calculus that one is already familiar with. At this stage, one might argue that real analysis is the study of real numbers, but is it? A big chunk of it involves algebraic properties, and as such lies in the realm of algebra. It is the order properties, though, that do have a sort of analysis point of view. Sure, some of these aspects generalise to the level of topologies, but not all. Completeness, for one, is clearly something that is central to analysis. Similar arguments can be made for complex analysis and functional analysis. Now, the question is: As for all the topics that are bunched together as  analysis, is there any central theme to them? What topics would you say that belongs to this theme? And what are the underlying themes in these individual subtopics? Add . It may be a subjective question, but having a rough idea of what the central themes of a certain field are helps one to construct appropriate questions. As such, I think it is important. I am not expecting a single answer, but more of a diverse set of opinions on the matter.","It is safe to say that every mathematician, at some point in their career, has had some form of exposure to analysis. Quite often, it appears first in the form of an undergraduate course in real analysis. It is there that one is often exposed to a rigorous viewpoint to the techniques of calculus that one is already familiar with. At this stage, one might argue that real analysis is the study of real numbers, but is it? A big chunk of it involves algebraic properties, and as such lies in the realm of algebra. It is the order properties, though, that do have a sort of analysis point of view. Sure, some of these aspects generalise to the level of topologies, but not all. Completeness, for one, is clearly something that is central to analysis. Similar arguments can be made for complex analysis and functional analysis. Now, the question is: As for all the topics that are bunched together as  analysis, is there any central theme to them? What topics would you say that belongs to this theme? And what are the underlying themes in these individual subtopics? Add . It may be a subjective question, but having a rough idea of what the central themes of a certain field are helps one to construct appropriate questions. As such, I think it is important. I am not expecting a single answer, but more of a diverse set of opinions on the matter.",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis', 'soft-question']"
22,Does $ \int_0^{\infty}\frac{\sin x}{x}dx $ have an improper Riemann integral or a Lebesgue integral?,Does  have an improper Riemann integral or a Lebesgue integral?, \int_0^{\infty}\frac{\sin x}{x}dx ,"In this wikipedia article for improper integrals,  $$ \int_0^{\infty}\frac{\sin x}{x}dx $$ is given as an example for the integrals that have an improper Riemann integral but do not have a (proper) Lebesgue integral. Here are my questions : Why does this one have an improper Riemann integral? (I don't see why $\int_0^a\frac{\sin x}{x}dx$ and $\int_a^{\infty}\frac{\sin x}{x}dx$ converge.) Why doesn't this integral have a Lebesgue integral? Is it because that $\frac{\sin x}{x}$ is unbounded on $(0,\infty)$ and Lebesgue integral doesn't deal with unbounded functions?","In this wikipedia article for improper integrals,  $$ \int_0^{\infty}\frac{\sin x}{x}dx $$ is given as an example for the integrals that have an improper Riemann integral but do not have a (proper) Lebesgue integral. Here are my questions : Why does this one have an improper Riemann integral? (I don't see why $\int_0^a\frac{\sin x}{x}dx$ and $\int_a^{\infty}\frac{\sin x}{x}dx$ converge.) Why doesn't this integral have a Lebesgue integral? Is it because that $\frac{\sin x}{x}$ is unbounded on $(0,\infty)$ and Lebesgue integral doesn't deal with unbounded functions?",,['real-analysis']
23,Why is $L^{\infty}$ not separable?,Why is  not separable?,L^{\infty},"$l^p (1≤p<{\infty})$ and $L^p (1≤p<∞)$ are separable spaces. What on earth has changed when the value of $p$ turns from a finite number to ${\infty}$? Our teacher gave us some hints that there exists an uncountable subset such that the distance of any two elements in it is no less than some $\delta>0$. Actually I don't understand the question very well, but I hope I have made the question clear enough. Thank you in advance.","$l^p (1≤p<{\infty})$ and $L^p (1≤p<∞)$ are separable spaces. What on earth has changed when the value of $p$ turns from a finite number to ${\infty}$? Our teacher gave us some hints that there exists an uncountable subset such that the distance of any two elements in it is no less than some $\delta>0$. Actually I don't understand the question very well, but I hope I have made the question clear enough. Thank you in advance.",,"['real-analysis', 'functional-analysis', 'normed-spaces', 'lp-spaces', 'separable-spaces']"
24,Different Approaches for Introducing Elementary Functions,Different Approaches for Introducing Elementary Functions,,"Motivation We all get familiar with elementary functions in high-school or college. However, as the system of learning is not that much integrated we have learned them in different ways and the connections between these ways are not clarified mostly by teachers. Once I read the calculus book by Apostol, I just found out that one can define these functions in a treatise systematic way only analytically . The approach used in the book with some minor changes is like this $1.$ Firstly, introduce the natural logarithm function by $\ln(x)=\int_{1}^{x}\frac{1}{t}dt$ for $x>0$. Accordingly, one defines the logarithm function by $\log_{b}x=\frac{\ln(x)}{\ln(b)}$ for $b>0$, $b \ne 1$ and $x>0$. $2.$ Then introduce the natural exponential function as the inverse of natural logarithm $\exp(x)=\ln^{-1}(x)$. Afterwards, introduce the exponential function $a^x=\exp(x\ln(a))$ for $a>0$ and real $x$. Interchanging $x$ and $a$, one can introduce the power function $x^a=\exp(a\ln(x))$ for $x \gt 0$ and real $a$. $3.$ Next, define hyperbolic functions $\cosh(x)$ and $\sinh(x)$ by using exponential function $$\matrix{    {\cosh (x) = {{\exp (x) + \exp ( - x)} \over 2}} \hfill & {\sinh (x) = {{\exp (x) - \exp ( - x)} \over 2}} \hfill  \cr   } $$ and then defining the other hyperbolic functions. Consequently, one can define the inverse-hyperbolic functions. $4.$ Finally, the author gives three ways for introducing the trigonometric functions. $\qquad 4.1-$ Introduces the $\sin x$ and $\cos x$ functions by the following properties \begin{align*}{} \text{(a)}\,\,& \text{The domain of $\sin x$ and $\cos x$ is $\mathbb R$} \\ \text{(b)}\,\,& \cos 0 = \sin \frac{\pi}{2}=0,\, \cos \pi=-1 \\ \text{(c)}\,\,& \cos (y-x)= \cos y \cos x + \sin y \sin x \\ \text{(d)}\,\,& \text{For $0 \le x \le \frac{\pi}{2}$ we have $0 \le \cos x \le \frac{\sin x}{x} \le \frac{1}{\cos x}$} \end{align*} $\qquad 4.2-$ Using formal geometric definitions employing the unit circle. $\qquad 4.3-$ Introducing $\sin x$ and $\cos x$ functions by their Taylor series. and then defining the other trigonometric ones and the inverse-trigonometric functions. In my point of view, the approach is good but it seems a little disconnected as the relation between the trigonometric and exponential functions is not illustrated as the author insisted to stay in the real domain when introducing these functions. Also, exponential and power functions are just defined for positive real numbers $a$ and $x$ while they can be extended to negative ones. Questions $1.$ How many other approaches are used for this purpose? Are there many or just a few? Is there some list for this? $2.$ Would you please explain just one of the other heuristic ways to introduce the elementary functions analytically with appropriate details? Notes Historical remarks are welcome as they provide a good motivation. Answers which connect more advanced (not too elementary) mathematical concepts to the development of elementary functions are really welcome. As nice example of this is the answer by Aloizio Macedo given below. It is hard to choose the best answer between these nice answers so I decided to choose none. I just gave the bounties to the ones that are more compatible with the studies from high-school. However, please feel free to add new answers including your own ideas or what you may think that is interesting so we can have a valuable list of different approaches recorded here. This can serve as a nice guide for future readers. Useful Links Here is a link to a paper by W. F. Eberlein suggested in the comments. The paper deals with introducing the trigonometric functions in a systematic way. There are six pdfs created by Paramanand Singh who has an answer below. It discusses some approaches for introducing logarithmic, exponential and circular functions. I have combined them all into one pdf which can be downloaded from here . I am sure that it will be useful.","Motivation We all get familiar with elementary functions in high-school or college. However, as the system of learning is not that much integrated we have learned them in different ways and the connections between these ways are not clarified mostly by teachers. Once I read the calculus book by Apostol, I just found out that one can define these functions in a treatise systematic way only analytically . The approach used in the book with some minor changes is like this $1.$ Firstly, introduce the natural logarithm function by $\ln(x)=\int_{1}^{x}\frac{1}{t}dt$ for $x>0$. Accordingly, one defines the logarithm function by $\log_{b}x=\frac{\ln(x)}{\ln(b)}$ for $b>0$, $b \ne 1$ and $x>0$. $2.$ Then introduce the natural exponential function as the inverse of natural logarithm $\exp(x)=\ln^{-1}(x)$. Afterwards, introduce the exponential function $a^x=\exp(x\ln(a))$ for $a>0$ and real $x$. Interchanging $x$ and $a$, one can introduce the power function $x^a=\exp(a\ln(x))$ for $x \gt 0$ and real $a$. $3.$ Next, define hyperbolic functions $\cosh(x)$ and $\sinh(x)$ by using exponential function $$\matrix{    {\cosh (x) = {{\exp (x) + \exp ( - x)} \over 2}} \hfill & {\sinh (x) = {{\exp (x) - \exp ( - x)} \over 2}} \hfill  \cr   } $$ and then defining the other hyperbolic functions. Consequently, one can define the inverse-hyperbolic functions. $4.$ Finally, the author gives three ways for introducing the trigonometric functions. $\qquad 4.1-$ Introduces the $\sin x$ and $\cos x$ functions by the following properties \begin{align*}{} \text{(a)}\,\,& \text{The domain of $\sin x$ and $\cos x$ is $\mathbb R$} \\ \text{(b)}\,\,& \cos 0 = \sin \frac{\pi}{2}=0,\, \cos \pi=-1 \\ \text{(c)}\,\,& \cos (y-x)= \cos y \cos x + \sin y \sin x \\ \text{(d)}\,\,& \text{For $0 \le x \le \frac{\pi}{2}$ we have $0 \le \cos x \le \frac{\sin x}{x} \le \frac{1}{\cos x}$} \end{align*} $\qquad 4.2-$ Using formal geometric definitions employing the unit circle. $\qquad 4.3-$ Introducing $\sin x$ and $\cos x$ functions by their Taylor series. and then defining the other trigonometric ones and the inverse-trigonometric functions. In my point of view, the approach is good but it seems a little disconnected as the relation between the trigonometric and exponential functions is not illustrated as the author insisted to stay in the real domain when introducing these functions. Also, exponential and power functions are just defined for positive real numbers $a$ and $x$ while they can be extended to negative ones. Questions $1.$ How many other approaches are used for this purpose? Are there many or just a few? Is there some list for this? $2.$ Would you please explain just one of the other heuristic ways to introduce the elementary functions analytically with appropriate details? Notes Historical remarks are welcome as they provide a good motivation. Answers which connect more advanced (not too elementary) mathematical concepts to the development of elementary functions are really welcome. As nice example of this is the answer by Aloizio Macedo given below. It is hard to choose the best answer between these nice answers so I decided to choose none. I just gave the bounties to the ones that are more compatible with the studies from high-school. However, please feel free to add new answers including your own ideas or what you may think that is interesting so we can have a valuable list of different approaches recorded here. This can serve as a nice guide for future readers. Useful Links Here is a link to a paper by W. F. Eberlein suggested in the comments. The paper deals with introducing the trigonometric functions in a systematic way. There are six pdfs created by Paramanand Singh who has an answer below. It discusses some approaches for introducing logarithmic, exponential and circular functions. I have combined them all into one pdf which can be downloaded from here . I am sure that it will be useful.",,"['calculus', 'real-analysis', 'algebra-precalculus', 'big-list', 'elementary-functions']"
25,Why doesn't induction extend to infinity? (re: Fourier series),Why doesn't induction extend to infinity? (re: Fourier series),,"While reading some things about analytic functions earlier tonight it came to my attention that Fourier series are not necessarily analytic. I used to think one could prove that they are analytic using induction Let $P(n)$ be some statement parametrized by the natural number $n$ (in this case: the $n$th partial sum of the Fourier series is analytic) Show that $P(0)$ is true Show that $P(n-1)\Rightarrow P(n)$ (Invalid) conclusion: $P(n)$ continues to be true as we take the limit $n\to\infty$ * Why exactly is the conclusion not valid here? It seems very strange that even though $P(n)$ is true for any finite $n$, it ceases to be valid when I remove the explicit upper bound on $n$. Are there circumstances under which I can make an argument of this form? Example of invalid proof: Define the truncated Fourier series $F_n(x)$ as the partial sum $$F_n(x) = \sum_{k=0}^{n} A_k\sin\biggl(\frac{kx}{T}\biggr) + B_k\cos\biggl(\frac{kx}{T}\biggr)$$ where $A_k$ and $B_k$ are the Fourier coefficients for some arbitrary function $f$. Using the facts that $\sin(t)$ and $\cos(t)$ are analytic, and that any linear combination of analytic functions is analytic: $P(n)$ is the statement ""$F_n(x)$ is analytic"" $F_0(x)$ is clearly analytic because it is a linear combination of sine and cosine functions $F_n(x)$ can be written as the linear combination $$F_{n}(x) = F_{n-1}(x) + A_n\sin\biggl(\frac{nx}{T}\biggr) + B_n\cos\biggl(\frac{nx}{T}\biggr)$$ So if $F_{n-1}(x)$ is analytic, $F_n(x)$ is analytic. $F(x) \equiv \lim_{n\to\infty} F_n(x)$ is analytic. But $F(x)$ is the Fourier series for $f$; therefore, the Fourier series for $f$ is analytic. * I'm assuming that $P(n)$ is a statement about some sequence which is parametrized by $n$ and for which taking the limit as $n\to\infty$ is meaningful","While reading some things about analytic functions earlier tonight it came to my attention that Fourier series are not necessarily analytic. I used to think one could prove that they are analytic using induction Let $P(n)$ be some statement parametrized by the natural number $n$ (in this case: the $n$th partial sum of the Fourier series is analytic) Show that $P(0)$ is true Show that $P(n-1)\Rightarrow P(n)$ (Invalid) conclusion: $P(n)$ continues to be true as we take the limit $n\to\infty$ * Why exactly is the conclusion not valid here? It seems very strange that even though $P(n)$ is true for any finite $n$, it ceases to be valid when I remove the explicit upper bound on $n$. Are there circumstances under which I can make an argument of this form? Example of invalid proof: Define the truncated Fourier series $F_n(x)$ as the partial sum $$F_n(x) = \sum_{k=0}^{n} A_k\sin\biggl(\frac{kx}{T}\biggr) + B_k\cos\biggl(\frac{kx}{T}\biggr)$$ where $A_k$ and $B_k$ are the Fourier coefficients for some arbitrary function $f$. Using the facts that $\sin(t)$ and $\cos(t)$ are analytic, and that any linear combination of analytic functions is analytic: $P(n)$ is the statement ""$F_n(x)$ is analytic"" $F_0(x)$ is clearly analytic because it is a linear combination of sine and cosine functions $F_n(x)$ can be written as the linear combination $$F_{n}(x) = F_{n-1}(x) + A_n\sin\biggl(\frac{nx}{T}\biggr) + B_n\cos\biggl(\frac{nx}{T}\biggr)$$ So if $F_{n-1}(x)$ is analytic, $F_n(x)$ is analytic. $F(x) \equiv \lim_{n\to\infty} F_n(x)$ is analytic. But $F(x)$ is the Fourier series for $f$; therefore, the Fourier series for $f$ is analytic. * I'm assuming that $P(n)$ is a statement about some sequence which is parametrized by $n$ and for which taking the limit as $n\to\infty$ is meaningful",,"['real-analysis', 'induction']"
26,Can there be an injective function whose derivative is equivalent to its inverse function?,Can there be an injective function whose derivative is equivalent to its inverse function?,,"Let's say $f:D\to R$ is an injective function on some domain where it is also differentiable. For a real function, i.e. $D\subset\mathbb R, R\subset\mathbb R$, is it possible that $f'(x)\equiv f^{-1}(x)$? Intuitively speaking, I suspect that this is not possible , but I can't provide a reasonable proof since I know very little nothing about functional analysis. Can anyone provide a (counter)example or prove that such function does not exist?","Let's say $f:D\to R$ is an injective function on some domain where it is also differentiable. For a real function, i.e. $D\subset\mathbb R, R\subset\mathbb R$, is it possible that $f'(x)\equiv f^{-1}(x)$? Intuitively speaking, I suspect that this is not possible , but I can't provide a reasonable proof since I know very little nothing about functional analysis. Can anyone provide a (counter)example or prove that such function does not exist?",,"['real-analysis', 'functions', 'functional-equations', 'inverse-function']"
27,Is there any geometric way to characterize $e$?,Is there any geometric way to characterize ?,e,"Let me explain it better: after this question, I've been looking for a way to put famous constants in the real line in a geometrical way -- just for fun. Putting $\sqrt2$ is really easy: constructing a $45^\circ$ - $90^\circ$ - $45^\circ$ triangle with unitary sides will make me have an idea of what $\sqrt2$ is. Extending this to $\sqrt5$ , $\sqrt{13}$ , and other algebraic numbers is easy using Trigonometry; however, it turned difficult working with some transcendental constants. Constructing $\pi$ is easy using circumferences; but I couldn't figure out how I should work with $e$ . Looking at made me realize that $e$ is the point $\omega$ such that $\displaystyle\int_1^{\omega}\frac{1}{x}dx = 1$ . However, I don't have any other ideas. And I keep asking myself: Is there any way to ""see"" $e$ geometrically? And more: is it true that one can build any real number geometrically? Any help will be appreciated. Thanks.","Let me explain it better: after this question, I've been looking for a way to put famous constants in the real line in a geometrical way -- just for fun. Putting is really easy: constructing a - - triangle with unitary sides will make me have an idea of what is. Extending this to , , and other algebraic numbers is easy using Trigonometry; however, it turned difficult working with some transcendental constants. Constructing is easy using circumferences; but I couldn't figure out how I should work with . Looking at made me realize that is the point such that . However, I don't have any other ideas. And I keep asking myself: Is there any way to ""see"" geometrically? And more: is it true that one can build any real number geometrically? Any help will be appreciated. Thanks.",\sqrt2 45^\circ 90^\circ 45^\circ \sqrt2 \sqrt5 \sqrt{13} \pi e e \omega \displaystyle\int_1^{\omega}\frac{1}{x}dx = 1 e,"['real-analysis', 'algebra-precalculus']"
28,Every power series is the Taylor series of some $C^{\infty}$ function,Every power series is the Taylor series of some  function,C^{\infty},"Do you have some reference to a proof of the so-called Borel theorem, i.e. every power series is the Taylor series of some $C^{\infty}$ function?","Do you have some reference to a proof of the so-called Borel theorem, i.e. every power series is the Taylor series of some $C^{\infty}$ function?",,['real-analysis']
29,Monotone Convergence Theorem for non-negative decreasing sequence of measurable functions,Monotone Convergence Theorem for non-negative decreasing sequence of measurable functions,,"Let $(X,\mathcal{M},\mu)$ be a measure space and suppose $\{f_n\}$ are non-negative measurable functions decreasing pointwise to $f$. Suppose also that $\int f_1 \lt \infty$. Then $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$ Atempt: Since $\{f_n\}$ are decreasing, and converges pointwise to $f$, then $\{-f_n\}$ is increasing pointwise to $f$. So by the monotone convergence theorem  $$ \int_X -f~d\mu = \lim_{n\to\infty}\int_X -f_n ~d\mu$$ and so $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$","Let $(X,\mathcal{M},\mu)$ be a measure space and suppose $\{f_n\}$ are non-negative measurable functions decreasing pointwise to $f$. Suppose also that $\int f_1 \lt \infty$. Then $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$ Atempt: Since $\{f_n\}$ are decreasing, and converges pointwise to $f$, then $\{-f_n\}$ is increasing pointwise to $f$. So by the monotone convergence theorem  $$ \int_X -f~d\mu = \lim_{n\to\infty}\int_X -f_n ~d\mu$$ and so $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
30,Finding Value of the Infinite Product $\prod \Bigl(1-\frac{1}{n^{2}}\Bigr)$,Finding Value of the Infinite Product,\prod \Bigl(1-\frac{1}{n^{2}}\Bigr),"While trying some problems along with my friends we had difficulty in this question. True or False: The value of the infinite product $$\prod\limits_{n=2}^{\infty} \biggl(1-\frac{1}{n^{2}}\biggr)$$ is $1$. I couldn't do it and my friend justified it by saying that since the terms in the product have values less than $1$, so the value of the product can never be $1$. I don't know whether this justification is correct or not. But i referred to Tom Apostol's Mathematical Analysis book and found a theorem which states, that The infinite product $\prod(1-a_{n})$ converges if the series $\sum a_{n}$ converges. This assures that the above product converges. Could anyone help me in finding out where it converges to? And, Does there exist a function $f$ in $\mathbb{N}$ ( like $n^{2}$, $n^{3}$) such that $\displaystyle \prod\limits_{n=1}^{\infty} \Bigl(1-\frac{1}{f(n)}\Bigr)$ has the value $1$?","While trying some problems along with my friends we had difficulty in this question. True or False: The value of the infinite product $$\prod\limits_{n=2}^{\infty} \biggl(1-\frac{1}{n^{2}}\biggr)$$ is $1$. I couldn't do it and my friend justified it by saying that since the terms in the product have values less than $1$, so the value of the product can never be $1$. I don't know whether this justification is correct or not. But i referred to Tom Apostol's Mathematical Analysis book and found a theorem which states, that The infinite product $\prod(1-a_{n})$ converges if the series $\sum a_{n}$ converges. This assures that the above product converges. Could anyone help me in finding out where it converges to? And, Does there exist a function $f$ in $\mathbb{N}$ ( like $n^{2}$, $n^{3}$) such that $\displaystyle \prod\limits_{n=1}^{\infty} \Bigl(1-\frac{1}{f(n)}\Bigr)$ has the value $1$?",,"['real-analysis', 'sequences-and-series']"
31,$\epsilon$-$\delta$ proof that $\lim\limits_{x \to 1} \frac{1}{x} = 1$.,- proof that .,\epsilon \delta \lim\limits_{x \to 1} \frac{1}{x} = 1,"I'm starting Spivak's Calculus and finally decided to learn how to write epsilon-delta proofs. I have been working on chapter 5, number 3(ii). The problem, in essence, asks to prove that $$\lim\limits_{x \to 1} \frac{1}{x} = 1.$$ Here's how I started my proof, $$\left| f(x)-l \right|=\left| \frac{1}{x} - 1 \right| =\left| \frac{1}{x} \right| \left| x - 1\right| < \epsilon \implies \left| x-1 \right| < \epsilon |x|$$ I haven't made any further progress past this point. Is it possible to salvage this proof? Should I try an alternate approach?","I'm starting Spivak's Calculus and finally decided to learn how to write epsilon-delta proofs. I have been working on chapter 5, number 3(ii). The problem, in essence, asks to prove that $$\lim\limits_{x \to 1} \frac{1}{x} = 1.$$ Here's how I started my proof, $$\left| f(x)-l \right|=\left| \frac{1}{x} - 1 \right| =\left| \frac{1}{x} \right| \left| x - 1\right| < \epsilon \implies \left| x-1 \right| < \epsilon |x|$$ I haven't made any further progress past this point. Is it possible to salvage this proof? Should I try an alternate approach?",,"['real-analysis', 'limits', 'epsilon-delta']"
32,Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist,Proving that  when  and  exist,\lim\limits_{x\to\infty}f'(x) = 0 \lim\limits_{x\to\infty}f(x) \lim\limits_{x\to\infty}f'(x),"I've been trying to solve the following problem: Suppose that $f$ and $f'$ are continuous functions on $\mathbb{R}$, and that $\displaystyle\lim_{x\to\infty}f(x)$ and $\displaystyle\lim_{x\to\infty}f'(x)$ exist. Show that $\displaystyle\lim_{x\to\infty}f'(x) = 0$. I'm not entirely sure what to do. Since there's not a lot of information given, I guess there isn't very much one can do. I tried using the definition of the derivative and showing that it went to $0$ as $x$ went to $\infty$ but that didn't really work out. Now I'm thinking I should assume $\displaystyle\lim_{x\to\infty}f'(x) = L \neq 0$ and try to get a contradiction, but I'm not sure where the contradiction would come from. Could somebody point me in the right direction (e.g. a certain theorem or property I have to use?) Thanks","I've been trying to solve the following problem: Suppose that $f$ and $f'$ are continuous functions on $\mathbb{R}$, and that $\displaystyle\lim_{x\to\infty}f(x)$ and $\displaystyle\lim_{x\to\infty}f'(x)$ exist. Show that $\displaystyle\lim_{x\to\infty}f'(x) = 0$. I'm not entirely sure what to do. Since there's not a lot of information given, I guess there isn't very much one can do. I tried using the definition of the derivative and showing that it went to $0$ as $x$ went to $\infty$ but that didn't really work out. Now I'm thinking I should assume $\displaystyle\lim_{x\to\infty}f'(x) = L \neq 0$ and try to get a contradiction, but I'm not sure where the contradiction would come from. Could somebody point me in the right direction (e.g. a certain theorem or property I have to use?) Thanks",,"['calculus', 'real-analysis', 'limits', 'functions', 'derivatives']"
33,"If a function $f(x)$ is Riemann integrable on $[a,b]$, is $f(x)$ bounded on $[a,b]$?","If a function  is Riemann integrable on , is  bounded on ?","f(x) [a,b] f(x) [a,b]","Most statements regarding Riemann integrals (at least the ones that I have encountered) begin with the statement ""for $f(x)$ bounded on $[a,b]$."" I am wondering if Riemann integrability implies boundedness. I think that this has to be the case, but I am not sure. If Riemann integrability does imply boundedness, are improper integrals considered Riemann integrals? I would think that improper integrals wouldn't be Riemann integrals since improper integrals are allowed to be equal to $+\infty$ or $-\infty$. Or are improper integrals that are not equal to $\infty$ considered Riemann integrals? I am confused.","Most statements regarding Riemann integrals (at least the ones that I have encountered) begin with the statement ""for $f(x)$ bounded on $[a,b]$."" I am wondering if Riemann integrability implies boundedness. I think that this has to be the case, but I am not sure. If Riemann integrability does imply boundedness, are improper integrals considered Riemann integrals? I would think that improper integrals wouldn't be Riemann integrals since improper integrals are allowed to be equal to $+\infty$ or $-\infty$. Or are improper integrals that are not equal to $\infty$ considered Riemann integrals? I am confused.",,"['real-analysis', 'integration', 'bounded-variation']"
34,Open maps which are not continuous,Open maps which are not continuous,,"What is an example of an open map $(0,1) \to \mathbb{R}$ which is not continuous? Is it even possible for one to exist? What about in higher dimensions? The simplest example I've been able to think of is the map $e^{1/z}$ from $\mathbb{C}$ to $\mathbb{C}$ (filled in to be $0$ at $0$). There must be a simpler example, using the usual Euclidean topology, right?","What is an example of an open map $(0,1) \to \mathbb{R}$ which is not continuous? Is it even possible for one to exist? What about in higher dimensions? The simplest example I've been able to think of is the map $e^{1/z}$ from $\mathbb{C}$ to $\mathbb{C}$ (filled in to be $0$ at $0$). There must be a simpler example, using the usual Euclidean topology, right?",,"['real-analysis', 'general-topology', 'examples-counterexamples', 'open-map']"
35,Funny double infinite sum,Funny double infinite sum,,I was playing with a modified version of Pascal's triangle (with ${n \choose k}^{-1}$ instead of $n \choose k$ everywhere) and this infinite sum popped out: $$\sum_{k=2}^{\infty}\sum_ {n=1}^{\infty} \frac{1}{n(n+1)(n+2)...(n+k-1)} $$ The partial sums seem to approach $\alpha \approx 1.317...$ Does a closed form for $\alpha$ exist?,I was playing with a modified version of Pascal's triangle (with ${n \choose k}^{-1}$ instead of $n \choose k$ everywhere) and this infinite sum popped out: $$\sum_{k=2}^{\infty}\sum_ {n=1}^{\infty} \frac{1}{n(n+1)(n+2)...(n+k-1)} $$ The partial sums seem to approach $\alpha \approx 1.317...$ Does a closed form for $\alpha$ exist?,,"['real-analysis', 'sequences-and-series']"
36,Midpoint-Convex and Continuous Implies Convex,Midpoint-Convex and Continuous Implies Convex,,"Given that $$f\left(\frac{x+y}{2}\right)\leqslant \frac{f(x)+f(y)}{2}~,$$ how can I show that $f$ is convex. Thanks. Edit: I'm sorry for all the confusion. $f$ is assumed to be continuous on an interval $(a,b)$.","Given that $$f\left(\frac{x+y}{2}\right)\leqslant \frac{f(x)+f(y)}{2}~,$$ how can I show that $f$ is convex. Thanks. Edit: I'm sorry for all the confusion. $f$ is assumed to be continuous on an interval $(a,b)$.",,"['real-analysis', 'convex-analysis']"
37,In which ordered fields does absolute convergence imply convergence?,In which ordered fields does absolute convergence imply convergence?,,"In the process of touching up some notes on infinite series, I came across the following ""result"": Theorem: For an ordered field $(F,<)$ , the following are equivalent: (i) Every Cauchy sequence in $F$ is convergent. (ii) Absolutely convergent series converge: $\sum_n |a_n|$ converges in $F$ $\implies$ $\sum_n a_n$ converges in $F$ . But at present only the proof of (i) $\implies$ (ii) is included, and unfortunately I can no longer remember what I had in mind for the converse direction.  After thinking it over for a bit, I wonder if I was confusing it with this result: Proposition: In a normed abelian group $(A,+,|\cdot|)$ , the following are equivalent: (i) Every Cauchy sequence is convergent. (ii) Absolutely convergent series converge: $\sum_n |a_n|$ converges in $\mathbb{R}$ $\implies$ $\sum_n a_n$ converges in $A$ . For instance, one can use a telescoping sum argument, as is done in the case of normed linear spaces over $\mathbb{R}$ in (VIII) of this note . But the desired result is not a special case of this, because by definition the norm on a normed abelian group takes values in $\mathbb{R}^{\geq 0}$ , whereas the absolute value on an ordered field $F$ takes values in $F^{\geq 0}$ . I can show (ii) $\implies$ (i) of the Theorem for ordered subfields of $\mathbb{R}$ .  Namely, every real number $\alpha$ admits a signed binary expansion $\alpha = \sum_{n = N_0}^{\infty} \frac{\epsilon_n}{2^n}$ , with $N_0 \in \mathbb{Z}$ and $\epsilon_n \in \{ \pm 1\}$ , and the associated ""absolute series"" is $\sum_{n=N_0}^{\infty} \frac{1}{2^n} = 2^{1-N_0}$ . Because an ordered field is isomorphic to an ordered subfield of $\mathbb{R}$ iff it is Archimedean, this actually proves (ii) $\implies$ (i) for Archimedean ordered fields.  But on the one hand I would prefer a proof of this that does not use the (nontrivial) result of the previous sentence, and on the other hand...what about non-Archimedean ordered fields? Added : The article based on this question and answer has at last appeared: Clark, Pete L.; Diepeveen, Niels J.; Absolute Convergence in Ordered Fields. Amer. Math. Monthly 121 (2014), no. 10, 909–916. If you are a member of the MAA, you will be frustrated if you try to access it directly: the issue is currently advertised on their website but the articles are not actually available to members.  The article is available on JSTOR and through MathSciNet.  Anyway, here is an isomorphic copy .  Thanks again to Niels Diepeveen!","In the process of touching up some notes on infinite series, I came across the following ""result"": Theorem: For an ordered field , the following are equivalent: (i) Every Cauchy sequence in is convergent. (ii) Absolutely convergent series converge: converges in converges in . But at present only the proof of (i) (ii) is included, and unfortunately I can no longer remember what I had in mind for the converse direction.  After thinking it over for a bit, I wonder if I was confusing it with this result: Proposition: In a normed abelian group , the following are equivalent: (i) Every Cauchy sequence is convergent. (ii) Absolutely convergent series converge: converges in converges in . For instance, one can use a telescoping sum argument, as is done in the case of normed linear spaces over in (VIII) of this note . But the desired result is not a special case of this, because by definition the norm on a normed abelian group takes values in , whereas the absolute value on an ordered field takes values in . I can show (ii) (i) of the Theorem for ordered subfields of .  Namely, every real number admits a signed binary expansion , with and , and the associated ""absolute series"" is . Because an ordered field is isomorphic to an ordered subfield of iff it is Archimedean, this actually proves (ii) (i) for Archimedean ordered fields.  But on the one hand I would prefer a proof of this that does not use the (nontrivial) result of the previous sentence, and on the other hand...what about non-Archimedean ordered fields? Added : The article based on this question and answer has at last appeared: Clark, Pete L.; Diepeveen, Niels J.; Absolute Convergence in Ordered Fields. Amer. Math. Monthly 121 (2014), no. 10, 909–916. If you are a member of the MAA, you will be frustrated if you try to access it directly: the issue is currently advertised on their website but the articles are not actually available to members.  The article is available on JSTOR and through MathSciNet.  Anyway, here is an isomorphic copy .  Thanks again to Niels Diepeveen!","(F,<) F \sum_n |a_n| F \implies \sum_n a_n F \implies (A,+,|\cdot|) \sum_n |a_n| \mathbb{R} \implies \sum_n a_n A \mathbb{R} \mathbb{R}^{\geq 0} F F^{\geq 0} \implies \mathbb{R} \alpha \alpha = \sum_{n = N_0}^{\infty} \frac{\epsilon_n}{2^n} N_0 \in \mathbb{Z} \epsilon_n \in \{ \pm 1\} \sum_{n=N_0}^{\infty} \frac{1}{2^n} = 2^{1-N_0} \mathbb{R} \implies","['real-analysis', 'sequences-and-series', 'ordered-fields']"
38,On the functional square root of $x^2+1$,On the functional square root of,x^2+1,"There are some math quizzes like: find a function $\phi:\mathbb{R}\rightarrow\mathbb{R}$  such that $\phi(\phi(x)) = f(x) \equiv x^2 + 1.$ If such $\phi$ exists (it does in this example), $\phi$ can be viewed as a ""square root"" of $f$ in the sense of function composition because $\phi\circ\phi = f$.  Is there a general theory on the mathematical properties of this kind of square roots? (For instance, for what $f$ will a real analytic $\phi$ exist?)","There are some math quizzes like: find a function $\phi:\mathbb{R}\rightarrow\mathbb{R}$  such that $\phi(\phi(x)) = f(x) \equiv x^2 + 1.$ If such $\phi$ exists (it does in this example), $\phi$ can be viewed as a ""square root"" of $f$ in the sense of function composition because $\phi\circ\phi = f$.  Is there a general theory on the mathematical properties of this kind of square roots? (For instance, for what $f$ will a real analytic $\phi$ exist?)",,"['real-analysis', 'functions', 'functional-equations', 'function-and-relation-composition']"
39,"How I can prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}$ converges to 2?",How I can prove that the sequence  converges to 2?,"\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}","Prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}},  \sqrt{2\sqrt{2\sqrt{2}}} \ $ converges to $2$ . My attempt I proved that the sequence is increasing and bounded by $2$ , can anyone help me show that the sequence converges to $2$ ? Thanks for your help.","Prove that the sequence converges to . My attempt I proved that the sequence is increasing and bounded by , can anyone help me show that the sequence converges to ? Thanks for your help.","\sqrt{2} , \sqrt{2\sqrt{2}},  \sqrt{2\sqrt{2\sqrt{2}}} \  2 2 2","['real-analysis', 'sequences-and-series']"
40,Why are real numbers useful?,Why are real numbers useful?,,"A question (by a fellow CS student taking a first course in calculus, presumably after the lecture in which continuity was introduced: was as follows. In the real, physical world, we deal with numbers that are sort of “finite” or “discrete” by their nature; there's no such thing as a perfect circle in the physical world. In CS, we model computers with discrete mathematics and it’s enough. So why real analysis? Why concepts like continuity and “completeness” of real numbers are useful? Why do we need them? I found Math.se has lots of questions for similar “concrete” justifications for complex numbers and great answers for them, but I didn’t really manage to find similar for this. The question Are all numbers real numbers? is related, but I’m not sure it’s exactly what I’m looking for. My attempt at an answer was along the line: Are we content with the length of the side of a square that has area of 2 units remaining a undefined number, even if we never manage find such a square? and Calculus and real analysis provide results that are useful even if in numerical calculations we use finite approximations. To really understand what's going on, we want it to be rigorous. but I’m not sure if I was persuasive enough. Any better ideas?","A question (by a fellow CS student taking a first course in calculus, presumably after the lecture in which continuity was introduced: was as follows. In the real, physical world, we deal with numbers that are sort of “finite” or “discrete” by their nature; there's no such thing as a perfect circle in the physical world. In CS, we model computers with discrete mathematics and it’s enough. So why real analysis? Why concepts like continuity and “completeness” of real numbers are useful? Why do we need them? I found Math.se has lots of questions for similar “concrete” justifications for complex numbers and great answers for them, but I didn’t really manage to find similar for this. The question Are all numbers real numbers? is related, but I’m not sure it’s exactly what I’m looking for. My attempt at an answer was along the line: Are we content with the length of the side of a square that has area of 2 units remaining a undefined number, even if we never manage find such a square? and Calculus and real analysis provide results that are useful even if in numerical calculations we use finite approximations. To really understand what's going on, we want it to be rigorous. but I’m not sure if I was persuasive enough. Any better ideas?",,"['calculus', 'real-analysis', 'soft-question', 'continuity', 'real-numbers']"
41,Why is the determinant continuous?,Why is the determinant continuous?,,"Why is the determinant as a function from $M_n(\mathbb{R})$ to $\mathbb{R}$ continuous? Can anyone explain precisely and rigorously? So far, I know the explanation which comes from the facts that polynomials are continuous, sum and product of continuous functions are continuous. Also I have the confusion regarding the metric on $M_n(\mathbb{R})$ .","Why is the determinant as a function from to continuous? Can anyone explain precisely and rigorously? So far, I know the explanation which comes from the facts that polynomials are continuous, sum and product of continuous functions are continuous. Also I have the confusion regarding the metric on .",M_n(\mathbb{R}) \mathbb{R} M_n(\mathbb{R}),"['real-analysis', 'linear-algebra']"
42,Is it possible to write a sum as an integral to solve it?,Is it possible to write a sum as an integral to solve it?,,"I was wondering, for example, Can: $$ \sum_{n=1}^{\infty} \frac{1}{(3n-1)(3n+2)}$$ Be written as an Integral? To solve it.  I am NOT talking about a method for using tricks with integrals. But actually writing an integral form. Like $$\displaystyle \sum_{n=1}^{\infty} \frac{1}{(3n-1)(3n+2)} = \int_{a}^{b} g(x) \space dx$$ What are some general tricks in finding infinite sum series.","I was wondering, for example, Can: $$ \sum_{n=1}^{\infty} \frac{1}{(3n-1)(3n+2)}$$ Be written as an Integral? To solve it.  I am NOT talking about a method for using tricks with integrals. But actually writing an integral form. Like $$\displaystyle \sum_{n=1}^{\infty} \frac{1}{(3n-1)(3n+2)} = \int_{a}^{b} g(x) \space dx$$ What are some general tricks in finding infinite sum series.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'summation']"
43,Evaluate $\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx$ [closed],Evaluate  [closed],\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 22 days ago . Improve this question Evaluate $$\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 22 days ago . Improve this question Evaluate $$\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
44,Is every Lebesgue measurable function on $\mathbb{R}$ the pointwise limit of continuous functions?,Is every Lebesgue measurable function on  the pointwise limit of continuous functions?,\mathbb{R},"I know that if $f$ is a Lebesgue measurable function on $[a,b]$ then there exists a continuous function $g$ such that $|f(x)-g(x)|< \epsilon$ for all $x\in [a,b]\setminus P$ where the measure of $P$ is less than $\epsilon$. This seems to imply that every Lebesgue measurable function on $\mathbb{R}$ is the pointwise limit of continuous functions.  Is this correct?","I know that if $f$ is a Lebesgue measurable function on $[a,b]$ then there exists a continuous function $g$ such that $|f(x)-g(x)|< \epsilon$ for all $x\in [a,b]\setminus P$ where the measure of $P$ is less than $\epsilon$. This seems to imply that every Lebesgue measurable function on $\mathbb{R}$ is the pointwise limit of continuous functions.  Is this correct?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
45,Infinite Series $\sum\limits_{n=1}^\infty\frac{(H_n)^2}{n^3}$ [closed],Infinite Series  [closed],\sum\limits_{n=1}^\infty\frac{(H_n)^2}{n^3},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question How to prove that $$\sum_{n=1}^{\infty}\frac{(H_n)^2}{n^3}=\frac{7}{2}\zeta(5)-\zeta(2)\zeta(3)$$ $H_n$ denotes the harmonic numbers.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question How to prove that $$\sum_{n=1}^{\infty}\frac{(H_n)^2}{n^3}=\frac{7}{2}\zeta(5)-\zeta(2)\zeta(3)$$ $H_n$ denotes the harmonic numbers.",,"['real-analysis', 'sequences-and-series']"
46,Formal basis for variable substitution in limits,Formal basis for variable substitution in limits,,"So we do a lot of variable substitution in limits at school. Stuff like $\lim\limits_{x\to5}\ (x+1)^2\ =\ \lim\limits_{y\to6}\ y^2$, where we define the substitution $y = x + 1$. But I've never been clear on what exactly the theoretical basis for this is. What is the formula that you're actually applying when you do variable substitution? What are the formal conditions under which it is possible? My conjecture would be the following: For all continuous $f$, and all real $a$: $\lim\limits_{x\to a}\ (f\circ g)(x)\ =\lim\limits_{x\to g(a)}\ f(x)$, where $g$ is a continuous function So to take my first example, $f$ would be $x^2$, $g$ would be $x + 1$, and $a$ would be $5$. Am I in the right area? If this is correct, can it be proven using $\epsilon$-$\delta$? I had a half-hearted shot at it the other night and didn't get anywhere.","So we do a lot of variable substitution in limits at school. Stuff like $\lim\limits_{x\to5}\ (x+1)^2\ =\ \lim\limits_{y\to6}\ y^2$, where we define the substitution $y = x + 1$. But I've never been clear on what exactly the theoretical basis for this is. What is the formula that you're actually applying when you do variable substitution? What are the formal conditions under which it is possible? My conjecture would be the following: For all continuous $f$, and all real $a$: $\lim\limits_{x\to a}\ (f\circ g)(x)\ =\lim\limits_{x\to g(a)}\ f(x)$, where $g$ is a continuous function So to take my first example, $f$ would be $x^2$, $g$ would be $x + 1$, and $a$ would be $5$. Am I in the right area? If this is correct, can it be proven using $\epsilon$-$\delta$? I had a half-hearted shot at it the other night and didn't get anywhere.",,"['real-analysis', 'calculus', 'limits']"
47,Simplest way to get the lower bound $\pi > 3.14$,Simplest way to get the lower bound,\pi > 3.14,"Inspired from this answer and my comment to it, I seek alternative ways to establish $\pi>3.14$. The goal is to achieve simpler/easy to understand approaches as well as to minimize the calculations involved. The method in my comment is based on Ramanujan's series $$\frac{4}{\pi}=\frac{1123}{882}-\frac{22583}{882^{3}}\cdot\frac{1}{2}\cdot\frac{1\cdot 3}{4^{2}}+\frac{44043}{882^{5}}\cdot\frac{1\cdot 3}{2\cdot 4}\cdot\frac{1\cdot 3\cdot 5\cdot 7}{4^{2}\cdot 8^{2}}-\dots\tag{1}$$ This is quite hard to understand (at least in my opinion, see the blog posts to convince yourself) but achieves the goal of minimal calculations with evaluation of just the first term being necessary. On the other end of spectrum is the reasonably easy to understand series $$\frac\pi4=1-\frac13+\frac15-\cdots\tag2$$ But this requires a large number of terms to get any reasonable accuracy. I would like a happy compromise between the two and approaches based on other ideas apart from series are also welcome. A previous question of mine gives an approach to estimate the error in truncating the Leibniz series $(2)$ and it gives bounds for $\pi$ with very little amount of calculation. However it requires the use of continued fractions and proving the desired continued fraction does require some effort. Another set of approximations to $\pi$ from below are obtained using Ramanujan's class invariant $g_n$ namely $$\pi\approx\frac{24}{\sqrt{n}}\log(2^{1/4}g_n)\tag{3}$$ and $n=10$ gives the approximation $\pi\approx 3.14122$ but this approach has a story similar to that of equation $(1)$.","Inspired from this answer and my comment to it, I seek alternative ways to establish $\pi>3.14$. The goal is to achieve simpler/easy to understand approaches as well as to minimize the calculations involved. The method in my comment is based on Ramanujan's series $$\frac{4}{\pi}=\frac{1123}{882}-\frac{22583}{882^{3}}\cdot\frac{1}{2}\cdot\frac{1\cdot 3}{4^{2}}+\frac{44043}{882^{5}}\cdot\frac{1\cdot 3}{2\cdot 4}\cdot\frac{1\cdot 3\cdot 5\cdot 7}{4^{2}\cdot 8^{2}}-\dots\tag{1}$$ This is quite hard to understand (at least in my opinion, see the blog posts to convince yourself) but achieves the goal of minimal calculations with evaluation of just the first term being necessary. On the other end of spectrum is the reasonably easy to understand series $$\frac\pi4=1-\frac13+\frac15-\cdots\tag2$$ But this requires a large number of terms to get any reasonable accuracy. I would like a happy compromise between the two and approaches based on other ideas apart from series are also welcome. A previous question of mine gives an approach to estimate the error in truncating the Leibniz series $(2)$ and it gives bounds for $\pi$ with very little amount of calculation. However it requires the use of continued fractions and proving the desired continued fraction does require some effort. Another set of approximations to $\pi$ from below are obtained using Ramanujan's class invariant $g_n$ namely $$\pi\approx\frac{24}{\sqrt{n}}\log(2^{1/4}g_n)\tag{3}$$ and $n=10$ gives the approximation $\pi\approx 3.14122$ but this approach has a story similar to that of equation $(1)$.",,"['real-analysis', 'sequences-and-series', 'inequality', 'pi', 'upper-lower-bounds']"
48,Evaluating the nested radical $ \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} $. [closed],Evaluating the nested radical . [closed], \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} ,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question How does one prove the following limit? $$   \lim_{n \to \infty}   \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots \sqrt{1 + (n - 1) \sqrt{1 + n}}}}} = 3. $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question How does one prove the following limit? $$   \lim_{n \to \infty}   \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots \sqrt{1 + (n - 1) \sqrt{1 + n}}}}} = 3. $$",,"['real-analysis', 'calculus']"
49,Better Proofs Than Rudin's For The Inverse And Implicit Function Theorems,Better Proofs Than Rudin's For The Inverse And Implicit Function Theorems,,"I am finding Rudin's proofs of these theorems very non-intuitive and difficult to recall. I can understand and follow both as I work through them, but if you were to ask me a week later to prove one or the other, I couldn't do it. For instance, the use of a contraction mapping in the inverse function theorem seems to require one to memorize, at the very least, a non-obvious (at least to me) function (viz. $\phi(\mathbf{x}) = \mathbf{x} + \mathbf{A}^{-1}(\mathbf{y}-\operatorname{f}(\mathbf{x}))$) and constant (viz. $\lambda^{-1} = 2 \Vert \mathbf{A}^{-1}\Vert$), where $\mathbf{A}$ is the differential of $\operatorname{f}$ at $\mathbf{a}$. The implicit function theorem proof, while not as bad, also requires one to construct a new function without ever hinting as to what the motivation is. I searched the previous questions on this site and haven't found this addressed, so I figured I'd ask. I did finnd this proof to have a much more intuitive approach to the inverse function theorem, but would like to see what proofs are preferred by others.","I am finding Rudin's proofs of these theorems very non-intuitive and difficult to recall. I can understand and follow both as I work through them, but if you were to ask me a week later to prove one or the other, I couldn't do it. For instance, the use of a contraction mapping in the inverse function theorem seems to require one to memorize, at the very least, a non-obvious (at least to me) function (viz. $\phi(\mathbf{x}) = \mathbf{x} + \mathbf{A}^{-1}(\mathbf{y}-\operatorname{f}(\mathbf{x}))$) and constant (viz. $\lambda^{-1} = 2 \Vert \mathbf{A}^{-1}\Vert$), where $\mathbf{A}$ is the differential of $\operatorname{f}$ at $\mathbf{a}$. The implicit function theorem proof, while not as bad, also requires one to construct a new function without ever hinting as to what the motivation is. I searched the previous questions on this site and haven't found this addressed, so I figured I'd ask. I did finnd this proof to have a much more intuitive approach to the inverse function theorem, but would like to see what proofs are preferred by others.",,"['real-analysis', 'analysis', 'reference-request']"
50,Is composition of measurable functions measurable?,Is composition of measurable functions measurable?,,"We know that if $ f: E \to \mathbb{R} $ is a Lebesgue-measurable function and $ g: \mathbb{R} \to \mathbb{R} $ is a continuous function, then $ g \circ f $ is Lebesgue-measurable. Can one replace the continuous function $ g $ by a Lebesgue-measurable function without affecting the validity of the previous result?","We know that if $ f: E \to \mathbb{R} $ is a Lebesgue-measurable function and $ g: \mathbb{R} \to \mathbb{R} $ is a continuous function, then $ g \circ f $ is Lebesgue-measurable. Can one replace the continuous function $ g $ by a Lebesgue-measurable function without affecting the validity of the previous result?",,"['real-analysis', 'measure-theory', 'functions']"
51,Why does an integral change signs when flipping the boundaries?,Why does an integral change signs when flipping the boundaries?,,"Let us define a very simple integral: $f(x) = \int_{a}^{b}{x}$ for $a,b\ge 0$. Why do we have the identity $\int_{a}^{b}{x} = -\int_{b}^{a}{x}$? I drew the graphs and thought about it but to me integration, at least in two-dimensions, is just taking the area underneath a curve so why does it matter which direction you take the sum?","Let us define a very simple integral: $f(x) = \int_{a}^{b}{x}$ for $a,b\ge 0$. Why do we have the identity $\int_{a}^{b}{x} = -\int_{b}^{a}{x}$? I drew the graphs and thought about it but to me integration, at least in two-dimensions, is just taking the area underneath a curve so why does it matter which direction you take the sum?",,"['real-analysis', 'integration']"
52,Infinite Series $\sum\limits_{n=1}^\infty\left(\frac{H_n}n\right)^2$,Infinite Series,\sum\limits_{n=1}^\infty\left(\frac{H_n}n\right)^2,How can I find a closed form for the following sum? $$\sum_{n=1}^{\infty}\left(\frac{H_n}{n}\right)^2$$ ($H_n=\sum_{k=1}^n\frac{1}{k}$).,How can I find a closed form for the following sum? $$\sum_{n=1}^{\infty}\left(\frac{H_n}{n}\right)^2$$ ($H_n=\sum_{k=1}^n\frac{1}{k}$).,,"['real-analysis', 'sequences-and-series']"
53,Looking for a rigorous analysis book,Looking for a rigorous analysis book,,"I'm a mathematics undergrad student who finished his first university year succesfully. I got courses of calculus, but these weren't very rigorous. I did learn about stuff like epsilon and delta proofs but we never made exercises on those things. The theory I saw contained proofs but the main goal of the course was to succesfully learn to solve integrals (line integrals, surface integrals, double integrals, volume integrals, ...), solve differential equations, etc. I already took proof based courses like linear algebra and group theory, so I think I am ready to start to learn rigorous real analysis, so I'm looking for a book that suits me. I want the book to contain the following topics: The usual analysis stuff: a construction of $\mathbb{R}$ or a system that takes $\mathbb{R}$ axiomatically for granted rigorous treatment of limits, sequences, derivatives, series, integrals the book can be about single variable analysis, but this is no requirement exercises to practice (I want certainly be able to prove things using epsilon and delta definitions after reading and working through the book) Other requirements: The book must be suited for self study (I have 3 months until the next school year starts, and I want to be able to prepare for the analysis courses). I have heard about the books 'Real numbers and real analysis' by Ethan D. Block and 'Principles of mathematical analysis' by Walter Rudin, and those seem to be good books. Can someone hint me towards a good book? If you want me to add information, feel free to leave a comment.","I'm a mathematics undergrad student who finished his first university year succesfully. I got courses of calculus, but these weren't very rigorous. I did learn about stuff like epsilon and delta proofs but we never made exercises on those things. The theory I saw contained proofs but the main goal of the course was to succesfully learn to solve integrals (line integrals, surface integrals, double integrals, volume integrals, ...), solve differential equations, etc. I already took proof based courses like linear algebra and group theory, so I think I am ready to start to learn rigorous real analysis, so I'm looking for a book that suits me. I want the book to contain the following topics: The usual analysis stuff: a construction of $\mathbb{R}$ or a system that takes $\mathbb{R}$ axiomatically for granted rigorous treatment of limits, sequences, derivatives, series, integrals the book can be about single variable analysis, but this is no requirement exercises to practice (I want certainly be able to prove things using epsilon and delta definitions after reading and working through the book) Other requirements: The book must be suited for self study (I have 3 months until the next school year starts, and I want to be able to prepare for the analysis courses). I have heard about the books 'Real numbers and real analysis' by Ethan D. Block and 'Principles of mathematical analysis' by Walter Rudin, and those seem to be good books. Can someone hint me towards a good book? If you want me to add information, feel free to leave a comment.",,['real-analysis']
54,How did early mathematicians make it without Set theory?,How did early mathematicians make it without Set theory?,,"It is said that Cauchy was a pioneer of rigour in calculus and a founder of complex analysis. Yet if baffles me as set theory was an invention of the 1870s, 20 years after the death of Cauchy. Currently the beginning of most concepts in mathematics begins with the concept of set. Furthermore the concept of groups whose foundations were laid by Galois and Abel were done so long before set theory. I hope there is a genral way to answer these questions 1) We define functions with a domain and range both being sets. But when Cauchy used the symbol 'f(x)', what did it really mean to him? As Cauchy was notorious for his rigorous approach, it is hard to believe that he may have just used the word function ambiguously with intuitive satisfaction. (If the following question makes the topic too broad I'd be more than happy to list it as a separate question. 2)To a certain extent I can even brush away the idea of functions before sets. But I simply cannot grasp how the concept of group was formulated without a set and I'm puzzled as to how Galois and Abel were independently able to frame methods to prove the unsolvability of the quintic (these days the proof makes generous use of set theory)without sets. In these days where N, Z, Q and R all sets, how did the early masters do what they did? How on earth was calculus made rigorous without the sets of different numbers?","It is said that Cauchy was a pioneer of rigour in calculus and a founder of complex analysis. Yet if baffles me as set theory was an invention of the 1870s, 20 years after the death of Cauchy. Currently the beginning of most concepts in mathematics begins with the concept of set. Furthermore the concept of groups whose foundations were laid by Galois and Abel were done so long before set theory. I hope there is a genral way to answer these questions 1) We define functions with a domain and range both being sets. But when Cauchy used the symbol 'f(x)', what did it really mean to him? As Cauchy was notorious for his rigorous approach, it is hard to believe that he may have just used the word function ambiguously with intuitive satisfaction. (If the following question makes the topic too broad I'd be more than happy to list it as a separate question. 2)To a certain extent I can even brush away the idea of functions before sets. But I simply cannot grasp how the concept of group was formulated without a set and I'm puzzled as to how Galois and Abel were independently able to frame methods to prove the unsolvability of the quintic (these days the proof makes generous use of set theory)without sets. In these days where N, Z, Q and R all sets, how did the early masters do what they did? How on earth was calculus made rigorous without the sets of different numbers?",,"['real-analysis', 'group-theory', 'elementary-set-theory', 'soft-question', 'math-history']"
55,Functions that take rationals to rationals,Functions that take rationals to rationals,,"What is known about $\mathcal C^\infty$ functions $\mathbb R\to\mathbb R$ that always take rationals to rationals? Are they all quotients of polynomials? If not, are there any that are bounded yet don't tend to a limit for $x\to +\infty$? If there are, then can we also require them to be analytic? (This is basically just a random musing after I found myself using trigonometric functions twice in unrelated throwaway counterexamples. It struck me that it was kind of conceptual overkill to whip out a transcendental function for the purpose I needed: basically that it had to keep wiggling forever. I couldn't think of a nice non -trancendental function to do this job, however, and now wonder whether that is because they can't exist).","What is known about $\mathcal C^\infty$ functions $\mathbb R\to\mathbb R$ that always take rationals to rationals? Are they all quotients of polynomials? If not, are there any that are bounded yet don't tend to a limit for $x\to +\infty$? If there are, then can we also require them to be analytic? (This is basically just a random musing after I found myself using trigonometric functions twice in unrelated throwaway counterexamples. It struck me that it was kind of conceptual overkill to whip out a transcendental function for the purpose I needed: basically that it had to keep wiggling forever. I couldn't think of a nice non -trancendental function to do this job, however, and now wonder whether that is because they can't exist).",,['real-analysis']
56,Elementary proof that the derivative of a real function is continuous somewhere,Elementary proof that the derivative of a real function is continuous somewhere,,"One can use the Baire category theorem to show that if $f:\mathbb{R} \to \mathbb{R}$ is differentiable, then $f'$ is continuous at some $c \in \mathbb{R}$. Is there an elementary proof of this fact? By ""elementary"" I mean at the level of intro real analysis. Edit: In spite of the decent response this question has gotten, after more than two and a half months there are still no answers. It's perhaps possible that there's some ""deep"" reason we should not expect an elementary proof of this. I will therefore also accept a well reasoned discussion as to why such a proof is unlikely.","One can use the Baire category theorem to show that if $f:\mathbb{R} \to \mathbb{R}$ is differentiable, then $f'$ is continuous at some $c \in \mathbb{R}$. Is there an elementary proof of this fact? By ""elementary"" I mean at the level of intro real analysis. Edit: In spite of the decent response this question has gotten, after more than two and a half months there are still no answers. It's perhaps possible that there's some ""deep"" reason we should not expect an elementary proof of this. I will therefore also accept a well reasoned discussion as to why such a proof is unlikely.",,"['real-analysis', 'derivatives']"
57,"Prove/disprove $(\int_0^{2 \pi} \!\!\cos f(x) \, d x)^2+(\int_0^{2 \pi}\!\!\! \sqrt{(f'(x))^2+\sin ^2 f(x)} \, dx)^2\ge 4\pi^2$",Prove/disprove,"(\int_0^{2 \pi} \!\!\cos f(x) \, d x)^2+(\int_0^{2 \pi}\!\!\! \sqrt{(f'(x))^2+\sin ^2 f(x)} \, dx)^2\ge 4\pi^2","Let $f(x)$ be a differentiable function on $[0,2\pi]$ s.t. $0\leq f(x)\leq 2\pi$ and $f(0)=f(2\pi)$ . Prove or disprove that $$ \left(\int_0^{2 \pi} \cos f(x) \,d x\right)^2+\left(\int_0^{2 \pi} \sqrt{(f'(x))^2+\sin^2 f(x)} \, d x\right)^2 \geq(2 \pi)^2 $$ It seems that when $f$ is an arbitrary constant, the left side equals $(2\pi)^2$ and seems to be the minimum. But how can I show that there's no other $f$ that makes the left side equal (or be less than) $(2\pi)^2$ ? A geometric interpretation of the inequality has been found: Consider a closed curve on a sphere: $C=\{(\cos x\cdot\sin f(x),\,\sin x\cdot\sin f(x),\,\cos f(x))\mid x\in[0,2\pi)\}$ , we have its perimeter $\displaystyle L=\int_0^{2\pi}\sqrt{(f'(x))^2+\sin^2 f(x)}\,dx$ and its area $\displaystyle S=2\pi-\int_0^{2\pi}\cos f(x)\,dx$ . From spherical isoperimetric inequality $L^2\ge S(4\pi-S)$ , we have $(2\pi-S)^2+L^2\ge(2\pi)^2$ , and the equality holds iff $C$ is any circle on the sphere. In this way we get the original inequality in the sense of geometry. Now the question is, how to prove the inequality with only pure analysis methods ?","Let be a differentiable function on s.t. and . Prove or disprove that It seems that when is an arbitrary constant, the left side equals and seems to be the minimum. But how can I show that there's no other that makes the left side equal (or be less than) ? A geometric interpretation of the inequality has been found: Consider a closed curve on a sphere: , we have its perimeter and its area . From spherical isoperimetric inequality , we have , and the equality holds iff is any circle on the sphere. In this way we get the original inequality in the sense of geometry. Now the question is, how to prove the inequality with only pure analysis methods ?","f(x) [0,2\pi] 0\leq f(x)\leq 2\pi f(0)=f(2\pi) 
\left(\int_0^{2 \pi} \cos f(x) \,d x\right)^2+\left(\int_0^{2 \pi} \sqrt{(f'(x))^2+\sin^2 f(x)} \, d x\right)^2 \geq(2 \pi)^2
 f (2\pi)^2 f (2\pi)^2 C=\{(\cos x\cdot\sin f(x),\,\sin x\cdot\sin f(x),\,\cos f(x))\mid x\in[0,2\pi)\} \displaystyle L=\int_0^{2\pi}\sqrt{(f'(x))^2+\sin^2 f(x)}\,dx \displaystyle S=2\pi-\int_0^{2\pi}\cos f(x)\,dx L^2\ge S(4\pi-S) (2\pi-S)^2+L^2\ge(2\pi)^2 C","['real-analysis', 'integration', 'analysis', 'inequality', 'definite-integrals']"
58,General Lebesgue Dominated Convergence Theorem,General Lebesgue Dominated Convergence Theorem,,"In Royden (4th edition), it says one can prove the General Lebesgue Dominated Convergence Theorem by simply replacing $g-f_n$ and $g+f_n$ with $g_n-f_n$ and $g_n+f_n$ .  I proceeded to do this, but I feel like the proof is incorrect. So here is the statement: Let $\{f_n\}_{n=1}^\infty$ be a sequence of measurable functions on $E$ that converge pointwise a.e. on $E$ to $f$ .  Suppose there is a sequence $\{g_n\}$ of integrable functions on $E$ that converge pointwise a.e. on $E$ to $g$ such that $|f_n| \leq g_n$ for all $n \in \mathbb{N}$ .  If $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $g_n$ = $\int_E$ $g$ , then $\lim\limits_{n \rightarrow \infty}$ $\int_E$ $f_n$ = $\int_E$ $f$ . Proof: $$\int_E (g-f) =  \int_E\liminf( g_n-f_n).$$ By the linearity of the integral: $$\int_E g - \int_E f = \int_E g-f \leq \liminf \int_E g_n -f_n = \int_E g - \limsup \int_E f_n.$$ So, $$\limsup \int_E f_n \leq \int_E f.$$ Similarly for the other one. Am I missing a step or is it really a simple case of replacing.","In Royden (4th edition), it says one can prove the General Lebesgue Dominated Convergence Theorem by simply replacing and with and .  I proceeded to do this, but I feel like the proof is incorrect. So here is the statement: Let be a sequence of measurable functions on that converge pointwise a.e. on to .  Suppose there is a sequence of integrable functions on that converge pointwise a.e. on to such that for all .  If = , then = . Proof: By the linearity of the integral: So, Similarly for the other one. Am I missing a step or is it really a simple case of replacing.",g-f_n g+f_n g_n-f_n g_n+f_n \{f_n\}_{n=1}^\infty E E f \{g_n\} E E g |f_n| \leq g_n n \in \mathbb{N} \lim\limits_{n \rightarrow \infty} \int_E g_n \int_E g \lim\limits_{n \rightarrow \infty} \int_E f_n \int_E f \int_E (g-f) =  \int_E\liminf( g_n-f_n). \int_E g - \int_E f = \int_E g-f \leq \liminf \int_E g_n -f_n = \int_E g - \limsup \int_E f_n. \limsup \int_E f_n \leq \int_E f.,"['real-analysis', 'measure-theory']"
59,Show that the countable product of metric spaces is metrizable,Show that the countable product of metric spaces is metrizable,,"Given a countable collection of metric spaces $\{(X_n,\rho_n)\}_{n=1}^{\infty}$.  Form the Cartesian Product of these sets $X=\displaystyle\prod_{n=1}^{\infty}X_n$, and define $\rho:X\times X\rightarrow\mathbb{R}$ by $$\rho(x,y)=\displaystyle\sum_{n=1}^{\infty}\frac{\rho_n(x_n,y_n)}{2^n[1+\rho_n(x_n,y_n)]}.$$ Show that $\rho$ is a metric on $X$ whose induced topology is equivalent to the product topology on $X$. So basically what this problem is saying is that there's a canonical way to define a metric on the countable product of metric spaces.  I showed in a previous problem that the topology induced by $\rho_n$ is equivalent to that induced by $\frac{\rho_n(x_n,y_n)}{1+\rho_n(x_n,y_n)}$.  And thus we can go ahead and just assume that $\rho_n< 1$ for all $n$ and replace our infinite series by $$\rho(x,y)=\displaystyle\sum_{n=1}^{\infty}\frac{\rho_n(x_n,y_n)}{2^n}.$$ Now comes the interesting part: how should I go about showing that the product topology on $X$ and the topology induced by $\rho$ are equivalent? The basis for the product topology given to me in my book's definition is that of cartesian products made up the $X_n$ except for finitely many which are $O_n$ for some open subset of $X_n$.  However I believe I was able to improve upon this and show that I could decompose these into a basis where the $O_n$ were all open balls induced by their respect $\rho_n$ metric. For $\rho$ I'm using the basis of open balls that it induces, as I can see no other reasonable choice. However I can't seem to match these two bases up.  There are many different points $\{x_n\}\in X$ which make my infinite series less than a certain value and there is so much freedom in which terms in the series I choose to reduce in size that it seems hopeless to try and fit an open ball induced by $\rho$ into any one of my basis elements for the product topology. Is there a more appropriate strategy for proving these two topologies are equivalent?","Given a countable collection of metric spaces $\{(X_n,\rho_n)\}_{n=1}^{\infty}$.  Form the Cartesian Product of these sets $X=\displaystyle\prod_{n=1}^{\infty}X_n$, and define $\rho:X\times X\rightarrow\mathbb{R}$ by $$\rho(x,y)=\displaystyle\sum_{n=1}^{\infty}\frac{\rho_n(x_n,y_n)}{2^n[1+\rho_n(x_n,y_n)]}.$$ Show that $\rho$ is a metric on $X$ whose induced topology is equivalent to the product topology on $X$. So basically what this problem is saying is that there's a canonical way to define a metric on the countable product of metric spaces.  I showed in a previous problem that the topology induced by $\rho_n$ is equivalent to that induced by $\frac{\rho_n(x_n,y_n)}{1+\rho_n(x_n,y_n)}$.  And thus we can go ahead and just assume that $\rho_n< 1$ for all $n$ and replace our infinite series by $$\rho(x,y)=\displaystyle\sum_{n=1}^{\infty}\frac{\rho_n(x_n,y_n)}{2^n}.$$ Now comes the interesting part: how should I go about showing that the product topology on $X$ and the topology induced by $\rho$ are equivalent? The basis for the product topology given to me in my book's definition is that of cartesian products made up the $X_n$ except for finitely many which are $O_n$ for some open subset of $X_n$.  However I believe I was able to improve upon this and show that I could decompose these into a basis where the $O_n$ were all open balls induced by their respect $\rho_n$ metric. For $\rho$ I'm using the basis of open balls that it induces, as I can see no other reasonable choice. However I can't seem to match these two bases up.  There are many different points $\{x_n\}\in X$ which make my infinite series less than a certain value and there is so much freedom in which terms in the series I choose to reduce in size that it seems hopeless to try and fit an open ball induced by $\rho$ into any one of my basis elements for the product topology. Is there a more appropriate strategy for proving these two topologies are equivalent?",,"['real-analysis', 'general-topology', 'metric-spaces', 'product-space']"
60,Video lectures on Real Analysis?,Video lectures on Real Analysis?,,"One of the most annoying gaps in my math education is Real analysis. I tried hard, but all I could find are either Harvey Mudd College lectures or MathDoctorBob . The latter are too short and the former are in horrendous format, I can barely make out what is written on the blackboard. Ideally I'd like the lectures to cover topics such as proofs of continuity, differentiation, some main inequalities, $\limsup$ and $\liminf$, uniform/pointwise convergence and continuity, dominated and monotone convergence and maybe a bit more. So if anyone knows if this material is available online, I'd be quite grateful.","One of the most annoying gaps in my math education is Real analysis. I tried hard, but all I could find are either Harvey Mudd College lectures or MathDoctorBob . The latter are too short and the former are in horrendous format, I can barely make out what is written on the blackboard. Ideally I'd like the lectures to cover topics such as proofs of continuity, differentiation, some main inequalities, $\limsup$ and $\liminf$, uniform/pointwise convergence and continuity, dominated and monotone convergence and maybe a bit more. So if anyone knows if this material is available online, I'd be quite grateful.",,"['real-analysis', 'reference-request', 'online-resources']"
61,$\sqrt x$ is uniformly continuous,is uniformly continuous,\sqrt x,"Prove that the function $\sqrt x$ is uniformly continuous on $\{x\in \mathbb{R} | x \ge 0\}$. To show uniformly continuity I must show for a given $\epsilon > 0$ there exists a $\delta>0$ such that for all $x_1, x_2 \in \mathbb{R}$ we have $|x_1 - x_2| < \delta$ implies that $|f(x_1) - f(x_2)|< \epsilon.$ What I did was $\left|\sqrt x - \sqrt x_0\right| = \left|\frac{(\sqrt x - \sqrt x_0)(\sqrt x + \sqrt x_0)}{(\sqrt x + \sqrt x_0)}\right| = \left|\frac{x - x_0}{\sqrt x + \sqrt x_0}\right| < \frac{\delta}{\sqrt x + \sqrt x_0}$ but I found some proof online that made $\delta = \epsilon^2$ where I don't understand how they got? So, in order for $\delta =\epsilon^2$ then $\sqrt x + \sqrt x_0$ must $\le$ $\epsilon$ then $\frac{\delta}{\sqrt x + \sqrt x_0} \le \frac{\delta}{\epsilon} = \epsilon$. But then why would $\epsilon \le \sqrt x + \sqrt x_0? $ Ah, I think I understand it now just by typing this out and from an earlier hint by Michael Hardy here .","Prove that the function $\sqrt x$ is uniformly continuous on $\{x\in \mathbb{R} | x \ge 0\}$. To show uniformly continuity I must show for a given $\epsilon > 0$ there exists a $\delta>0$ such that for all $x_1, x_2 \in \mathbb{R}$ we have $|x_1 - x_2| < \delta$ implies that $|f(x_1) - f(x_2)|< \epsilon.$ What I did was $\left|\sqrt x - \sqrt x_0\right| = \left|\frac{(\sqrt x - \sqrt x_0)(\sqrt x + \sqrt x_0)}{(\sqrt x + \sqrt x_0)}\right| = \left|\frac{x - x_0}{\sqrt x + \sqrt x_0}\right| < \frac{\delta}{\sqrt x + \sqrt x_0}$ but I found some proof online that made $\delta = \epsilon^2$ where I don't understand how they got? So, in order for $\delta =\epsilon^2$ then $\sqrt x + \sqrt x_0$ must $\le$ $\epsilon$ then $\frac{\delta}{\sqrt x + \sqrt x_0} \le \frac{\delta}{\epsilon} = \epsilon$. But then why would $\epsilon \le \sqrt x + \sqrt x_0? $ Ah, I think I understand it now just by typing this out and from an earlier hint by Michael Hardy here .",,"['real-analysis', 'uniform-continuity']"
62,"Improper integral $\sin(x)/x $ converges absolutely, conditionally or diverges?","Improper integral  converges absolutely, conditionally or diverges?",\sin(x)/x ,"Improper integral of $\sin(x)/x $ converges absolutely, conditionally or diverges? We have $$\int_1^{\infty}\frac{\sin x}{x}\text{d}x$$ Integrating by parts $$u=\frac{1}{x}$$ $$\text{d}u=-\frac{1}{x^2}\text{d}x$$ $$\text{d}v=\sin x\;\text{d}x$$ $$v=-\cos x$$ $$ \begin{aligned} \int_1^{\infty} \frac{\sin x}{x} \text{d}x & = \frac{-\cos x}{x} \Big|_1^{\infty}      - \int_1^{\infty} \frac{\cos x}{x^2} \text{d}x \\ & = \cos 1 - \int_1^{\infty} \frac{\cos x}{x^2} \text{d}x \end{aligned} $$ $\int_1^{\infty} \frac{\cos x}{x^2} \text{d}x$ converges absolutely (using the Comparison Test For Improper Integrals): $$  \int_1^{\infty} \frac{|\cos x|}{x^2} \text{d}x <  \int_1^{\infty} \frac{1}{x^2} \text{d}x $$ So $\int_1^{\infty} \frac{\sin x}{x} \text{d}x$ converges. Now I need to find out if $\int_1^{\infty} |\frac{\sin x}{x}| \text{d}x$ converges or diverges.","Improper integral of converges absolutely, conditionally or diverges? We have Integrating by parts converges absolutely (using the Comparison Test For Improper Integrals): So converges. Now I need to find out if converges or diverges.","\sin(x)/x  \int_1^{\infty}\frac{\sin x}{x}\text{d}x u=\frac{1}{x} \text{d}u=-\frac{1}{x^2}\text{d}x \text{d}v=\sin x\;\text{d}x v=-\cos x 
\begin{aligned}
\int_1^{\infty} \frac{\sin x}{x} \text{d}x
& = \frac{-\cos x}{x} \Big|_1^{\infty} 
    - \int_1^{\infty} \frac{\cos x}{x^2} \text{d}x \\
& = \cos 1 - \int_1^{\infty} \frac{\cos x}{x^2} \text{d}x
\end{aligned}
 \int_1^{\infty} \frac{\cos x}{x^2} \text{d}x  
\int_1^{\infty} \frac{|\cos x|}{x^2} \text{d}x < 
\int_1^{\infty} \frac{1}{x^2} \text{d}x
 \int_1^{\infty} \frac{\sin x}{x} \text{d}x \int_1^{\infty} |\frac{\sin x}{x}| \text{d}x","['real-analysis', 'integration', 'improper-integrals']"
63,Why does the Hilbert curve fill the whole square?,Why does the Hilbert curve fill the whole square?,,"I have never seen a formal definition of the Hilbert curve, much less a careful analysis of why it fills the whole square. The Wikipedia and Mathworld articles are typically handwavy. I suppose the idea is something like this: one defines a sequence of functions $f_i(t) : [0,1]\to {\Bbb R}^2$, and then considers the pointwise limit $f(t) = \lim_{i\to\infty} f_i(t)$. But looking at the diagram, it is not clear that the sequence converges. I can imagine that we might think of following a single point in the range of $f_i$ as $i$ increases, and that point might move around, but only by $2^{-i}$ at each stage as we pass from $f_i$ to $f_{i+1}$, and so eventually approaches a limiting position. But I don't know how we'd get from there to showing that $f_i(t)$ converges for a particular value of the argument, especially for an argument other than a dyadic rational. And even if it does converge, every point in the range of each $f_i$ has at least one rational coordinate, so it is not at all clear why a point like $({1\over\sqrt2}, {1\over\sqrt2})$ should be in the range of $f$. If there is an easy explanation, I would be glad to hear it, but I would also be  glad to get a reference in English.","I have never seen a formal definition of the Hilbert curve, much less a careful analysis of why it fills the whole square. The Wikipedia and Mathworld articles are typically handwavy. I suppose the idea is something like this: one defines a sequence of functions $f_i(t) : [0,1]\to {\Bbb R}^2$, and then considers the pointwise limit $f(t) = \lim_{i\to\infty} f_i(t)$. But looking at the diagram, it is not clear that the sequence converges. I can imagine that we might think of following a single point in the range of $f_i$ as $i$ increases, and that point might move around, but only by $2^{-i}$ at each stage as we pass from $f_i$ to $f_{i+1}$, and so eventually approaches a limiting position. But I don't know how we'd get from there to showing that $f_i(t)$ converges for a particular value of the argument, especially for an argument other than a dyadic rational. And even if it does converge, every point in the range of each $f_i$ has at least one rational coordinate, so it is not at all clear why a point like $({1\over\sqrt2}, {1\over\sqrt2})$ should be in the range of $f$. If there is an easy explanation, I would be glad to hear it, but I would also be  glad to get a reference in English.",,"['real-analysis', 'reference-request', 'examples-counterexamples', 'fractals']"
64,Is there a differentiable function such that $f(\mathbb Q) \subseteq \mathbb Q$ but $f'(\mathbb Q) \not \subseteq \mathbb Q$?,Is there a differentiable function such that  but ?,f(\mathbb Q) \subseteq \mathbb Q f'(\mathbb Q) \not \subseteq \mathbb Q,"Is there a differentiable function $f:\mathbb R \rightarrow \mathbb R$ such that $f(\mathbb Q) \subseteq \mathbb Q$ , but $f'(\mathbb Q) \not \subseteq \mathbb Q$ ? A friend of mine asserted this without giving any examples. I seriously doubt it, but I had hard time trying to disprove it since analysis isn't really my thing. I can't even think of any class of differentiable functions with $f(\mathbb Q) \subseteq \mathbb Q$ other than the rational functions.","Is there a differentiable function such that , but ? A friend of mine asserted this without giving any examples. I seriously doubt it, but I had hard time trying to disprove it since analysis isn't really my thing. I can't even think of any class of differentiable functions with other than the rational functions.",f:\mathbb R \rightarrow \mathbb R f(\mathbb Q) \subseteq \mathbb Q f'(\mathbb Q) \not \subseteq \mathbb Q f(\mathbb Q) \subseteq \mathbb Q,['real-analysis']
65,Example of a Borel set that is neither $F_\sigma$ nor $G_\delta$,Example of a Borel set that is neither  nor,F_\sigma G_\delta,I'm looking for subset $A$ of $\mathbb R$ such that $A$ is a Borel set but $A$ is neither $F_\sigma$ nor $G_\delta$.,I'm looking for subset $A$ of $\mathbb R$ such that $A$ is a Borel set but $A$ is neither $F_\sigma$ nor $G_\delta$.,,"['real-analysis', 'general-topology', 'examples-counterexamples']"
66,Are there any Hessian matrices that are asymmetric on a large set?,Are there any Hessian matrices that are asymmetric on a large set?,,"Are there any functions, $f:U\subset \mathbb{R}^n \to \mathbb{R}$, with Hessian matrix which is asymmetric on a large set (say with positive measure)? I'm familiar with examples of functions with mixed partials not equal at a point, and I also know that if $f$ is lucky enough to have a weak second derivative $D^2f$, then $D^2 f$ is symmetric almost everywhere.","Are there any functions, $f:U\subset \mathbb{R}^n \to \mathbb{R}$, with Hessian matrix which is asymmetric on a large set (say with positive measure)? I'm familiar with examples of functions with mixed partials not equal at a point, and I also know that if $f$ is lucky enough to have a weak second derivative $D^2f$, then $D^2 f$ is symmetric almost everywhere.",,"['real-analysis', 'multivariable-calculus', 'hessian-matrix']"
67,Two questions about weakly convergent series related to $\sin(n^2)$ and Weyl's inequality,Two questions about weakly convergent series related to  and Weyl's inequality,\sin(n^2),"By using partial summation and Weyl's inequality, it is not hard to show that the series $\sum_{n\geq 1}\frac{\sin(n^2)}{n}$ is convergent. Is is true that $$\frac{1}{2}=\inf\left\{\alpha\in\mathbb{R}^+:\sum_{n\geq 1}\frac{\sin(n^2)}{n^\alpha}\mbox{ is convergent}\right\}?$$ In the case of a positive answer to the previous question, what is $$\inf\left\{\beta\in\mathbb{R}^+:\sum_{n\geq 1}\frac{\sin(n^2)}{\sqrt{n}(\log n)^\beta}\mbox{ is convergent}\right\}?$$","By using partial summation and Weyl's inequality, it is not hard to show that the series $\sum_{n\geq 1}\frac{\sin(n^2)}{n}$ is convergent. Is is true that $$\frac{1}{2}=\inf\left\{\alpha\in\mathbb{R}^+:\sum_{n\geq 1}\frac{\sin(n^2)}{n^\alpha}\mbox{ is convergent}\right\}?$$ In the case of a positive answer to the previous question, what is $$\inf\left\{\beta\in\mathbb{R}^+:\sum_{n\geq 1}\frac{\sin(n^2)}{\sqrt{n}(\log n)^\beta}\mbox{ is convergent}\right\}?$$",,"['real-analysis', 'sequences-and-series', 'number-theory', 'uniform-distribution', 'diophantine-approximation']"
68,What functions or classes of functions are $L^1$ but not $L^2$?,What functions or classes of functions are  but not ?,L^1 L^2,"We know that if a real function is in $L^2$ then it is in $L^1$, but the reverse is not necessarily true. So what are the examples of functions that are $L^1$ but not $L^2$, especially those intrinsically unbounded ones if any? Welcome more examples, or classes of examples. Thanks.","We know that if a real function is in $L^2$ then it is in $L^1$, but the reverse is not necessarily true. So what are the examples of functions that are $L^1$ but not $L^2$, especially those intrinsically unbounded ones if any? Welcome more examples, or classes of examples. Thanks.",,"['real-analysis', 'functional-analysis']"
69,"Is there a simple, constructive, 1-1 mapping between the reals and the irrationals?","Is there a simple, constructive, 1-1 mapping between the reals and the irrationals?",,"Is there a simple, constructive, 1-1 mapping between the reals and the irrationals? I know that the Cantor–Bernstein–Schroeder theorem implies the existence of a 1-1 mapping between the reals and the irrationals, but the proofs of this theorem are nonconstructive. I wondered if a simple (not involving an infinite set of mappings) constructive (so the mapping is straightforwardly specified) mapping existed. I have considered things like mapping the rationals to the rationals plus a fixed irrational, but then I could not figure out how to prevent an infinite (possible uncountably infinite) regression.","Is there a simple, constructive, 1-1 mapping between the reals and the irrationals? I know that the Cantor–Bernstein–Schroeder theorem implies the existence of a 1-1 mapping between the reals and the irrationals, but the proofs of this theorem are nonconstructive. I wondered if a simple (not involving an infinite set of mappings) constructive (so the mapping is straightforwardly specified) mapping existed. I have considered things like mapping the rationals to the rationals plus a fixed irrational, but then I could not figure out how to prevent an infinite (possible uncountably infinite) regression.",,"['real-analysis', 'elementary-set-theory']"
70,"Purely ""algebraic"" proof of Young's Inequality","Purely ""algebraic"" proof of Young's Inequality",,"Young's inequality states that if $a, b \geq 0$, $p, q > 0$, and $\frac{1}{p} + \frac{1}{q} = 1$, then $$ab\leq \frac{a^p}{p} + \frac{b^q}{q}$$ (with equality only when $a^p = b^q$).  Back when I was in my first course in real analysis, I was assigned this as homework, but I couldn't figure it out.  I kept trying to manipulate the expressions algebraically, and I couldn't get anywhere.  But every proof that I've seen since uses calculus in some way to prove this.  For example, a common proof is based on this proof without words and integration.  The proof on Wikipedia uses the fact that $\log$ is concave, which I believe requires the analytic definition of the logarithm to prove (correct me if I'm wrong). Can this be proven using just algebraic manipulations?  I know that that is a somewhat vague question, because ""algebraic"" is not well-defined, but I'm not sure how to make it more rigorous. But for example, the proof when $p = q = 2$ is something I would consider to be ""purely algebraic"": $$0 \leq (a - b)^2 = a^2 + b^2 - 2ab,$$ so $$ab \leq \frac{a^2}{2} + \frac{b^2}{2}.$$","Young's inequality states that if $a, b \geq 0$, $p, q > 0$, and $\frac{1}{p} + \frac{1}{q} = 1$, then $$ab\leq \frac{a^p}{p} + \frac{b^q}{q}$$ (with equality only when $a^p = b^q$).  Back when I was in my first course in real analysis, I was assigned this as homework, but I couldn't figure it out.  I kept trying to manipulate the expressions algebraically, and I couldn't get anywhere.  But every proof that I've seen since uses calculus in some way to prove this.  For example, a common proof is based on this proof without words and integration.  The proof on Wikipedia uses the fact that $\log$ is concave, which I believe requires the analytic definition of the logarithm to prove (correct me if I'm wrong). Can this be proven using just algebraic manipulations?  I know that that is a somewhat vague question, because ""algebraic"" is not well-defined, but I'm not sure how to make it more rigorous. But for example, the proof when $p = q = 2$ is something I would consider to be ""purely algebraic"": $$0 \leq (a - b)^2 = a^2 + b^2 - 2ab,$$ so $$ab \leq \frac{a^2}{2} + \frac{b^2}{2}.$$",,"['real-analysis', 'algebra-precalculus', 'inequality', 'young-inequality']"
71,An exotic sequence,An exotic sequence,,Let $a=\frac{1+i\sqrt 7}{2}$ and $u_n=\Re(a^n)$   show that  $(|u_n|)\to +\infty$ I think basics method does not works here. Any ideas ?,Let $a=\frac{1+i\sqrt 7}{2}$ and $u_n=\Re(a^n)$   show that  $(|u_n|)\to +\infty$ I think basics method does not works here. Any ideas ?,,"['real-analysis', 'sequences-and-series']"
72,An example of a function uniformly continuous on $\mathbb{R}$ but not Lipschitz continuous,An example of a function uniformly continuous on  but not Lipschitz continuous,\mathbb{R},Could you give me An example of a function uniformly continuous on $\mathbb{R}$ but not Lipschitz continuous?,Could you give me An example of a function uniformly continuous on $\mathbb{R}$ but not Lipschitz continuous?,,"['real-analysis', 'analysis']"
73,max and min versus sup and inf [closed],max and min versus sup and inf [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question What is the difference between max, min and sup, inf?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question What is the difference between max, min and sup, inf?",,"['real-analysis', 'definition', 'supremum-and-infimum']"
74,"Is $dx\,dy$ really a multiplication of $dx$ and $dy$?",Is  really a multiplication of  and ?,"dx\,dy dx dy","On the answers of the question Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? it was told that $\frac{dy}{dx}$ cannot be seen as a quotient, even though it looks like a fraction. My question is: does $dxdy$ in the double integral represent a multiplication of differentials? The problem then can be generalized for a multiple integral.","On the answers of the question Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? it was told that cannot be seen as a quotient, even though it looks like a fraction. My question is: does in the double integral represent a multiplication of differentials? The problem then can be generalized for a multiple integral.",\frac{dy}{dx} dxdy,"['calculus', 'real-analysis', 'analysis', 'nonstandard-analysis']"
75,Is every function with the intermediate value property a derivative?,Is every function with the intermediate value property a derivative?,,"As it is well known every continuous function has the intermediate value property, but even some discontinuous functions like  $$f(x)=\left\{ \begin{array}{cl} \sin\left(\frac{1}{x}\right) & x\neq 0 \\ 0 & x=0 \end{array} \right.$$ are having this property. In fact we know that a derivative always have this property. My question is, if for every function $f$ with the intermediate value property exists a function $F$ such that $F'=f$.  And if so, is $F$ unique (up to a constant you may add)? My attempts till now: (Just skip them if you feel so) The most natural way of finding those functions would be integrating, so I guess the question can be reduced to, if functions with the intermediate value property can be integrated. This one depends heavy on how you define when a functions is integrable, we (my analysis class) said that we call a function integrable when it is a regulated function (the limits $x\to x_0^+ f(x)$ and $x\to x_0^- f(x)$ exists ) . As my example above shows, not every function with the intermediate value property is a regulated function. But if we say a function is integrabel when every riemann sum converges the above function is integrable, so it seems like this would be a better definition for my problem. Edit: As Kevin Carlson points out in a commentar that being a derivative is different from being riemann integrable, he even gave an example for a function which is differentiable but it's derivative is not riemann integrable. So we can't show that those functions are riemann integrable as they  are not riemann integrable in general. Now I have no clue how to find an answer.","As it is well known every continuous function has the intermediate value property, but even some discontinuous functions like  $$f(x)=\left\{ \begin{array}{cl} \sin\left(\frac{1}{x}\right) & x\neq 0 \\ 0 & x=0 \end{array} \right.$$ are having this property. In fact we know that a derivative always have this property. My question is, if for every function $f$ with the intermediate value property exists a function $F$ such that $F'=f$.  And if so, is $F$ unique (up to a constant you may add)? My attempts till now: (Just skip them if you feel so) The most natural way of finding those functions would be integrating, so I guess the question can be reduced to, if functions with the intermediate value property can be integrated. This one depends heavy on how you define when a functions is integrable, we (my analysis class) said that we call a function integrable when it is a regulated function (the limits $x\to x_0^+ f(x)$ and $x\to x_0^- f(x)$ exists ) . As my example above shows, not every function with the intermediate value property is a regulated function. But if we say a function is integrabel when every riemann sum converges the above function is integrable, so it seems like this would be a better definition for my problem. Edit: As Kevin Carlson points out in a commentar that being a derivative is different from being riemann integrable, he even gave an example for a function which is differentiable but it's derivative is not riemann integrable. So we can't show that those functions are riemann integrable as they  are not riemann integrable in general. Now I have no clue how to find an answer.",,"['real-analysis', 'examples-counterexamples']"
76,How can someone reject a math result if everything has to be proved?,How can someone reject a math result if everything has to be proved?,,"I'm reading a book on axiomatic set theory, classic Set Theory: For Guided Independent Study, and at the beginning of chapter 4 it says: So far in this book we have given the impression that sets are needed to help explain the important number systems on which so much of mathematics (and the science that exploits mathematics) is based. Dedekind's construction of the real numbers, along with the associated axioms for the reals, completes the process of putting the calculus (and much more) on a rigorous footing. and then it says: It is important to realize that there are schools of mathematics that would reject 'standard' real analysis and, along with it, Dedekind's work. How is it possible that ""schools of mathematics"" reject standard real analysis and Dedekind's work? I don't know if I'm misinterpreting things but, how can people reject a whole branch of mathematics if everything has to be proved to be called a theorem and cannot be disproved unless a logical mistake is found? I've even watched this video in the past: https://www.youtube.com/watch?reload=9&v=jlnBo3APRlU and this guy, who's supposed to be a teacher, says that real numbers don't exist and that they are only rational numbers. I don't know if this is a related problem but how is this possible?","I'm reading a book on axiomatic set theory, classic Set Theory: For Guided Independent Study, and at the beginning of chapter 4 it says: So far in this book we have given the impression that sets are needed to help explain the important number systems on which so much of mathematics (and the science that exploits mathematics) is based. Dedekind's construction of the real numbers, along with the associated axioms for the reals, completes the process of putting the calculus (and much more) on a rigorous footing. and then it says: It is important to realize that there are schools of mathematics that would reject 'standard' real analysis and, along with it, Dedekind's work. How is it possible that ""schools of mathematics"" reject standard real analysis and Dedekind's work? I don't know if I'm misinterpreting things but, how can people reject a whole branch of mathematics if everything has to be proved to be called a theorem and cannot be disproved unless a logical mistake is found? I've even watched this video in the past: https://www.youtube.com/watch?reload=9&v=jlnBo3APRlU and this guy, who's supposed to be a teacher, says that real numbers don't exist and that they are only rational numbers. I don't know if this is a related problem but how is this possible?",,"['real-analysis', 'logic', 'set-theory', 'foundations', 'philosophy']"
77,Infinite Series $\sum_{n=1}^\infty\frac{H_n}{n^32^n}$,Infinite Series,\sum_{n=1}^\infty\frac{H_n}{n^32^n},"I'm trying to find a closed form for the following sum $$\sum_{n=1}^\infty\frac{H_n}{n^3\,2^n},$$ where $H_n=\displaystyle\sum_{k=1}^n\frac{1}{k}$ is a harmonic number. Could you help me with it?","I'm trying to find a closed form for the following sum $$\sum_{n=1}^\infty\frac{H_n}{n^3\,2^n},$$ where $H_n=\displaystyle\sum_{k=1}^n\frac{1}{k}$ is a harmonic number. Could you help me with it?",,"['real-analysis', 'sequences-and-series', 'closed-form', 'harmonic-numbers', 'zeta-functions']"
78,A series involving the harmonic numbers : $\sum_{n=1}^{\infty}\frac{H_n}{n^3}$,A series involving the harmonic numbers :,\sum_{n=1}^{\infty}\frac{H_n}{n^3},Let $H_{n}$ be the n th harmonic number defined by $ H_{n} := \sum_{k=1}^{n} \frac{1}{k}$. How would you prove that $$\sum_{n=1}^{\infty}\frac{H_n}{n^3}=\frac{\pi^4}{72}?$$ Simply replacing $H_{n}$ with $\sum_{k=1}^{n} \frac{1}{k}$ does not seem like a good starting point. Perhaps another representation of the n th harmonic number would be more useful.,Let $H_{n}$ be the n th harmonic number defined by $ H_{n} := \sum_{k=1}^{n} \frac{1}{k}$. How would you prove that $$\sum_{n=1}^{\infty}\frac{H_n}{n^3}=\frac{\pi^4}{72}?$$ Simply replacing $H_{n}$ with $\sum_{k=1}^{n} \frac{1}{k}$ does not seem like a good starting point. Perhaps another representation of the n th harmonic number would be more useful.,,"['calculus', 'real-analysis', 'sequences-and-series', 'harmonic-numbers']"
79,No continuous function switches $\mathbb{Q}$ and the irrationals,No continuous function switches  and the irrationals,\mathbb{Q},Is there a way to prove the following result using connectedness? Result : Let $J=\mathbb{R} \setminus \mathbb{Q}$ denote the set of irrational numbers. There is no continuous map $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f(\mathbb{Q}) \subseteq J$ and $f(J) \subseteq \mathbb{Q}$ . Link,Is there a way to prove the following result using connectedness? Result : Let denote the set of irrational numbers. There is no continuous map such that and . Link,J=\mathbb{R} \setminus \mathbb{Q} f: \mathbb{R} \rightarrow \mathbb{R} f(\mathbb{Q}) \subseteq J f(J) \subseteq \mathbb{Q},"['real-analysis', 'functions', 'continuity']"
80,Prove $\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n$,Prove,\limsup\limits_{n \to \infty} (a_n+b_n) \le \limsup\limits_{n \to \infty} a_n + \limsup\limits_{n \to \infty} b_n,"I am stuck with the following problem. Prove that $$\limsup_{n \to \infty} (a_n+b_n) \le \limsup_{n \to \infty} a_n + \limsup_{n \to \infty} b_n$$ I was thinking of using the triangle inequality saying $$|a_n + b_n| \le |a_n| + |b_n|$$ but the problem is not about absolute values of the sequence. Intuitively it's clear that this is true because $a_n$ and $b_n$ can ""reduce each others magnitude"" if they have opposite signs, but I cannot express that algebraically... Can someone help me out ?","I am stuck with the following problem. Prove that $$\limsup_{n \to \infty} (a_n+b_n) \le \limsup_{n \to \infty} a_n + \limsup_{n \to \infty} b_n$$ I was thinking of using the triangle inequality saying $$|a_n + b_n| \le |a_n| + |b_n|$$ but the problem is not about absolute values of the sequence. Intuitively it's clear that this is true because $a_n$ and $b_n$ can ""reduce each others magnitude"" if they have opposite signs, but I cannot express that algebraically... Can someone help me out ?",,"['real-analysis', 'limits', 'inequality', 'limsup-and-liminf']"
81,"Is complex analysis more ""real"" than real analysis?","Is complex analysis more ""real"" than real analysis?",,"In physics, in the past, complex numbers were used only to remember or simplify formulas and computations. But after the birth of quantum physics, they found that a thing as real as ""matter"" itself had to be described by complex wave functions and there's no way to describe it using only real numbers. In mathematics, in real analysis, there's examples like the function $f(x)=\frac{1}{1+x^2}$ , and why this function does not have the ""smoothness"" of the exponential function, polynomials, or the sine and cosine functions, why it has a radius of convergence equals 1 despite the fact that this function is infinitely differentiable. You can't see the reality of this function until you see it through the field of complex analysis, where you can observe that $f$ is not that smooth because it has 2 singularities in the complex plane. I am just asking for examples like this such that when you see it in the narrow ""window"" of real analysis, you can't see the ""reality"" until you view it from the window of complex analysis. I am just starting to self learn complex analysis and I find it more natural than real analysis and it tells you the ""truth"" behind a lot of things.","In physics, in the past, complex numbers were used only to remember or simplify formulas and computations. But after the birth of quantum physics, they found that a thing as real as ""matter"" itself had to be described by complex wave functions and there's no way to describe it using only real numbers. In mathematics, in real analysis, there's examples like the function , and why this function does not have the ""smoothness"" of the exponential function, polynomials, or the sine and cosine functions, why it has a radius of convergence equals 1 despite the fact that this function is infinitely differentiable. You can't see the reality of this function until you see it through the field of complex analysis, where you can observe that is not that smooth because it has 2 singularities in the complex plane. I am just asking for examples like this such that when you see it in the narrow ""window"" of real analysis, you can't see the ""reality"" until you view it from the window of complex analysis. I am just starting to self learn complex analysis and I find it more natural than real analysis and it tells you the ""truth"" behind a lot of things.",f(x)=\frac{1}{1+x^2} f,"['real-analysis', 'complex-analysis', 'soft-question', 'self-learning', 'philosophy']"
82,On the equality case of the Hölder and Minkowski inequalities,On the equality case of the Hölder and Minkowski inequalities,,"I'm following the book Measure and Integral of Richard L. Wheeden and Antoni Zygmund. This is problem 4 of chapter 8. Consider $E\subseteq \mathbb{R}^n$ a measurable set. In the following all the integrals are taken over $E$ , $1/p + 1/q=1$ , with $1\lt p\lt \infty$ . I'm trying to prove that $$\int \vert fg\vert =\Vert f \Vert_p\Vert g \Vert_q$$ if and only if $\vert f \vert^p$ is multiple of $\vert g \vert^q$ almost everywhere. To do this, I want to consider the following cases: if $\Vert f \Vert_p=0$ or $\Vert g \Vert_q=0$ , we are done. Then suppose that $\Vert f \Vert_p\ne 0$ and $\Vert g \Vert_q\ne 0$ . If $\Vert f \Vert_p=\infty$ or $\Vert g \Vert_q=\infty$ , we are done (I hope). If $0\lt\Vert f \Vert_p\lt\infty$ and $0\lt\Vert g \Vert_q\lt\infty$ , proceed as follows. When we are proving the Hölder's inequality, we use that for $a,b\geq 0$ $$ab\leq \frac{a^p}{p}+\frac{b^q}{q},$$ where the equality holds if and only if $b=a^{p/q}$ . Explicitly $$\int\vert fg \vert\leq \Vert f \Vert_p \Vert g \Vert_q \int\left( \frac{\vert f \vert^p}{p\Vert f \Vert_p^p} + \frac{\vert g \vert^q}{q\Vert g \Vert_q^q}\right)=\Vert f \Vert_p \Vert g \Vert_q.$$ From here, we see that the equality in Hölder's inequalty holds iff $$\frac{\vert fg \vert}{\Vert f \Vert_p \Vert g \Vert_q}=\frac{\vert f \vert^p}{p\Vert f \Vert_p^p} + \frac{\vert g \vert^q}{q\Vert g \Vert_q^q}, \text{ a.e.}$$ iff $$\frac{\vert g \vert}{\Vert g \Vert_q}=\left( \frac{\vert f \vert}{\Vert f \Vert_p} \right)^{p/q},\text{ a.e.}$$ iff $$\vert g \vert^q\cdot \Vert f \Vert_p^p=\vert f \vert^p \cdot \Vert g \Vert_q^q,\text{ a.e.}$$ Q.E.D. But, assuming that $\Vert f \Vert_p\ne 0$ and $\Vert g \Vert_q\ne 0$ , what about when $\Vert f \Vert_p=\infty$ or $\Vert g \Vert_q=\infty$ ? How can I deal with it? In the case of Minkowski inequality, suppose that the equality holds and that $g\not \equiv 0$ (and then $\left( \int \vert f+g \vert^p\right)\ne 0$ ). I need to prove that $\Vert f \Vert_p$ is multiple of $\Vert g \Vert_q$ almost everywhere. I can reduce to the ""Hölder's equality case"". I can get $$\vert f \vert^p=\left( \int \vert f+g \vert^p\right)^{-1}\Vert f \Vert_p^p\vert f+g \vert^p$$ $$\vert g \vert^p=\left( \int \vert f+g \vert^p\right)^{-1}\Vert g \Vert_p^p\vert f+g \vert^p$$ almost everywhere, but again, using the finiteness and nonzeroness of $\Vert f \Vert_p$ and $\Vert g \Vert_p$ .","I'm following the book Measure and Integral of Richard L. Wheeden and Antoni Zygmund. This is problem 4 of chapter 8. Consider a measurable set. In the following all the integrals are taken over , , with . I'm trying to prove that if and only if is multiple of almost everywhere. To do this, I want to consider the following cases: if or , we are done. Then suppose that and . If or , we are done (I hope). If and , proceed as follows. When we are proving the Hölder's inequality, we use that for where the equality holds if and only if . Explicitly From here, we see that the equality in Hölder's inequalty holds iff iff iff Q.E.D. But, assuming that and , what about when or ? How can I deal with it? In the case of Minkowski inequality, suppose that the equality holds and that (and then ). I need to prove that is multiple of almost everywhere. I can reduce to the ""Hölder's equality case"". I can get almost everywhere, but again, using the finiteness and nonzeroness of and .","E\subseteq \mathbb{R}^n E 1/p + 1/q=1 1\lt p\lt \infty \int \vert fg\vert =\Vert f \Vert_p\Vert g \Vert_q \vert f \vert^p \vert g \vert^q \Vert f \Vert_p=0 \Vert g \Vert_q=0 \Vert f \Vert_p\ne 0 \Vert g \Vert_q\ne 0 \Vert f \Vert_p=\infty \Vert g \Vert_q=\infty 0\lt\Vert f \Vert_p\lt\infty 0\lt\Vert g \Vert_q\lt\infty a,b\geq 0 ab\leq \frac{a^p}{p}+\frac{b^q}{q}, b=a^{p/q} \int\vert fg \vert\leq \Vert f \Vert_p \Vert g \Vert_q \int\left( \frac{\vert f \vert^p}{p\Vert f \Vert_p^p} + \frac{\vert g \vert^q}{q\Vert g \Vert_q^q}\right)=\Vert f \Vert_p \Vert g \Vert_q. \frac{\vert fg \vert}{\Vert f \Vert_p \Vert g \Vert_q}=\frac{\vert f \vert^p}{p\Vert f \Vert_p^p} + \frac{\vert g \vert^q}{q\Vert g \Vert_q^q}, \text{ a.e.} \frac{\vert g \vert}{\Vert g \Vert_q}=\left( \frac{\vert f \vert}{\Vert f \Vert_p} \right)^{p/q},\text{ a.e.} \vert g \vert^q\cdot \Vert f \Vert_p^p=\vert f \vert^p \cdot \Vert g \Vert_q^q,\text{ a.e.} \Vert f \Vert_p\ne 0 \Vert g \Vert_q\ne 0 \Vert f \Vert_p=\infty \Vert g \Vert_q=\infty g\not \equiv 0 \left( \int \vert f+g \vert^p\right)\ne 0 \Vert f \Vert_p \Vert g \Vert_q \vert f \vert^p=\left( \int \vert f+g \vert^p\right)^{-1}\Vert f \Vert_p^p\vert f+g \vert^p \vert g \vert^p=\left( \int \vert f+g \vert^p\right)^{-1}\Vert g \Vert_p^p\vert f+g \vert^p \Vert f \Vert_p \Vert g \Vert_p","['real-analysis', 'measure-theory', 'inequality', 'lp-spaces', 'holder-inequality']"
83,"Prove that $C^1([a,b])$ with the $C^1$- norm is a Banach Space.",Prove that  with the - norm is a Banach Space.,"C^1([a,b]) C^1","Consider the space of continuously differentiable functions, $$C^1([a,b]) = \{f:[a,b]\rightarrow \mathbb{R}\mid f \text{ differentiable with }f' \text{ continuous}\}$$ with the $C^1$ -norm $$\lVert f\rVert := \sup_{a\leq x\leq b}|f(x)|+\sup_{a\leq x\leq b}|f'(x)|.$$ Prove that $C^1([a,b])$ is a Banach Space. This was the proof we were given: Assuming $C^1([a,b])$ is a normed linear space all we need to show is completeness. Let $(f_n)$ be a Cauchy Sequence in $C^1([a,b])$ with respect to the $C^1$ -norm. Then each $f_n,f'_n\in (C([a,b]),\|\cdot\|_{\sup})$ . We know that $C([a,b])$ is complete and thus there exists $f,g\in C([a,b])$ such that $f_n\rightarrow f$ , and $f'_n\rightarrow g$ (uniformly) with respect to $\|\cdot\|_{\sup}$ . If we let $$ F_n(x) = \int_a^x f_n(t)dt, \hspace{2mm}  F(x) = \int_a^x f(t)dt $$ then $F_n\rightarrow F$ uniformly because $$\lVert F_n-F\rVert_{\sup}\leq \sup_{a\leq x\leq b}\int_a^x|f_n(t)-f(t)|dt\leq \lVert f_n-f\rVert_{\sup}<\epsilon.$$ From the fundamental theorem of calculus: $$f_n(x)-f_n(a) = \int_a^x f'_n(t)dt $$ Since $f'_n\rightarrow g$ uniformly then $$ \int_a^xf'_n(t)dt\rightarrow \int_a^x g(t)dt $$ Since we know that $f_n\rightarrow f$ uniformly, $$f(x)-f(a) = \int_a^x g(t) dt $$ which by the fundamental theorem of calculues implies $f'=g$ . So we know have $f_n\rightarrow f$ and $f'_n\rightarrow g=f'$ which mean $f_n\rightarrow f\in C^1([a,b])$ with respect to $C^1$ -norm. So every cauchy sequence converges. Hence $C^1([a,b])$ is a Banach Space. So I understand most of the proof. Where I get confused is that how did we actually show this satisfies the $C^1$ -norm? Maybe I don't understand what this norm actually does. Thank you for any help, comments and advice!","Consider the space of continuously differentiable functions, with the -norm Prove that is a Banach Space. This was the proof we were given: Assuming is a normed linear space all we need to show is completeness. Let be a Cauchy Sequence in with respect to the -norm. Then each . We know that is complete and thus there exists such that , and (uniformly) with respect to . If we let then uniformly because From the fundamental theorem of calculus: Since uniformly then Since we know that uniformly, which by the fundamental theorem of calculues implies . So we know have and which mean with respect to -norm. So every cauchy sequence converges. Hence is a Banach Space. So I understand most of the proof. Where I get confused is that how did we actually show this satisfies the -norm? Maybe I don't understand what this norm actually does. Thank you for any help, comments and advice!","C^1([a,b]) = \{f:[a,b]\rightarrow \mathbb{R}\mid f \text{ differentiable with }f' \text{ continuous}\} C^1 \lVert f\rVert := \sup_{a\leq x\leq b}|f(x)|+\sup_{a\leq x\leq b}|f'(x)|. C^1([a,b]) C^1([a,b]) (f_n) C^1([a,b]) C^1 f_n,f'_n\in (C([a,b]),\|\cdot\|_{\sup}) C([a,b]) f,g\in C([a,b]) f_n\rightarrow f f'_n\rightarrow g \|\cdot\|_{\sup}  F_n(x) = \int_a^x f_n(t)dt, \hspace{2mm}  F(x) = \int_a^x f(t)dt  F_n\rightarrow F \lVert F_n-F\rVert_{\sup}\leq \sup_{a\leq x\leq b}\int_a^x|f_n(t)-f(t)|dt\leq \lVert f_n-f\rVert_{\sup}<\epsilon. f_n(x)-f_n(a) = \int_a^x f'_n(t)dt  f'_n\rightarrow g  \int_a^xf'_n(t)dt\rightarrow \int_a^x g(t)dt  f_n\rightarrow f f(x)-f(a) = \int_a^x g(t) dt  f'=g f_n\rightarrow f f'_n\rightarrow g=f' f_n\rightarrow f\in C^1([a,b]) C^1 C^1([a,b]) C^1","['real-analysis', 'functional-analysis', 'banach-spaces']"
84,Possible definitions of exponential function,Possible definitions of exponential function,,"I was wondering how many definitions of exponential functions can we think of. The basic ones could be: $$e^x:=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ also $$e^x:=\lim_{n\to\infty}\bigg(1+\frac{x}{n}\bigg)^n$$ or this one: Define $e^x:\mathbb{R}\rightarrow\mathbb{R}\\$ as unique function satisfying: \begin{align} e^x\geq x+1\\ \forall x,y\in\mathbb{R}:e^{x+y}=e^xe^y \end{align} Can anyone come up with something unusual? (Possibly with some explanation or references).",I was wondering how many definitions of exponential functions can we think of. The basic ones could be: also or this one: Define as unique function satisfying: Can anyone come up with something unusual? (Possibly with some explanation or references).,"e^x:=\sum_{k=0}^{\infty}\frac{x^k}{k!} e^x:=\lim_{n\to\infty}\bigg(1+\frac{x}{n}\bigg)^n e^x:\mathbb{R}\rightarrow\mathbb{R}\\ \begin{align}
e^x\geq x+1\\
\forall x,y\in\mathbb{R}:e^{x+y}=e^xe^y
\end{align}","['real-analysis', 'definition', 'big-list']"
85,How much do we really care about Riemann integration compared to Lebesgue integration?,How much do we really care about Riemann integration compared to Lebesgue integration?,,"Let me ask right at the start: what is Riemann integration really used for ? As far as I'm aware, we use Lebesgue integration in: probability theory theory of PDE's Fourier transforms and really, anywhere I can think of where integration is used (perhaps in the form of Haar measure, as a generalization, although I'm sure this is vastly incomplete picture). Let me state a well known theorem: Let $f:[a,b]\to\mathbb R$ be bounded function. Then: (i) $f$ is Riemann-integrable if and only if $f$ is continuous almost everywhere on $[a,b]$ (with respect to Lebesgue measure). (ii) If $f$ is Riemann-integrable on $[a,b]$, then $f$ is Lebesgue-integrable and Riemann and Lebesgue integrals coincide. (I will try to be fair, we use this result and Riemann integration to calculate many Lebesgue integrals) From this we can conclude that Riemann-integrability is a stronger condition and we might naively conclude that it might behave better. However it does not; Riemann integral does not well behave under limits, while Lebesgue integral does: we have Lebesgue monotone and dominated convergence theorems. Furthermore, I'm not aware of any universal property of Riemann integration, while in contrast we have this result presented by Tom Leinster; it establishes Lebesgue integration as initial in appropriate category (category of Banach spaces with mean). Also, I'm familiar with Lebesgue-Stiltjes integral, greatly used for example in probability theory to define appropriate measures. I'm not so familiar with the concept or usage of Riemann-Stiltjes integral, and I'd greatly appreciate if someone could provide any comparison. As far as I can tell, the only accomplishment of Riemann integration is the Fundamental theorem of calculus (not to neglect it's importance). I'm very interested to know if there are more important results. To summarize the question: Where is Riemann integral used compared to Lebesgue integral (which seems much better behaved) and why do we care? Update: It seems that it is agreed upon that Riemann integration primarily serves didactical purpose in teaching introductionary courses in analysis as a stepping stone for Lebesgue integration in later courses when measure theory is introduced. Also, improper integrals were brought as an example of something Lebesgue integration doesn't handle well. However, in several answers and comments we have another notion - that of a gauge integral (Henstock–Kurzweil integral, (narrow) Denjoy integral, Luzin integral or Perron integral). This integral not only generalizes both Riemann and Lebesgue integration, but also has much more satisfactory Fundamental theorem of calculus: if a function is a.e. differentiable than it's differential is gauge-integrable and conversely function defined by gauge integral is a.e. differentiable (here almost everywhere means everywhere up to a countable set). Thank you for all the answers. This question should probably be altered to the following form (in more open-ended manner): What are pros and cons of different kinds of integrals, and when should we use one over the other?","Let me ask right at the start: what is Riemann integration really used for ? As far as I'm aware, we use Lebesgue integration in: probability theory theory of PDE's Fourier transforms and really, anywhere I can think of where integration is used (perhaps in the form of Haar measure, as a generalization, although I'm sure this is vastly incomplete picture). Let me state a well known theorem: Let $f:[a,b]\to\mathbb R$ be bounded function. Then: (i) $f$ is Riemann-integrable if and only if $f$ is continuous almost everywhere on $[a,b]$ (with respect to Lebesgue measure). (ii) If $f$ is Riemann-integrable on $[a,b]$, then $f$ is Lebesgue-integrable and Riemann and Lebesgue integrals coincide. (I will try to be fair, we use this result and Riemann integration to calculate many Lebesgue integrals) From this we can conclude that Riemann-integrability is a stronger condition and we might naively conclude that it might behave better. However it does not; Riemann integral does not well behave under limits, while Lebesgue integral does: we have Lebesgue monotone and dominated convergence theorems. Furthermore, I'm not aware of any universal property of Riemann integration, while in contrast we have this result presented by Tom Leinster; it establishes Lebesgue integration as initial in appropriate category (category of Banach spaces with mean). Also, I'm familiar with Lebesgue-Stiltjes integral, greatly used for example in probability theory to define appropriate measures. I'm not so familiar with the concept or usage of Riemann-Stiltjes integral, and I'd greatly appreciate if someone could provide any comparison. As far as I can tell, the only accomplishment of Riemann integration is the Fundamental theorem of calculus (not to neglect it's importance). I'm very interested to know if there are more important results. To summarize the question: Where is Riemann integral used compared to Lebesgue integral (which seems much better behaved) and why do we care? Update: It seems that it is agreed upon that Riemann integration primarily serves didactical purpose in teaching introductionary courses in analysis as a stepping stone for Lebesgue integration in later courses when measure theory is introduced. Also, improper integrals were brought as an example of something Lebesgue integration doesn't handle well. However, in several answers and comments we have another notion - that of a gauge integral (Henstock–Kurzweil integral, (narrow) Denjoy integral, Luzin integral or Perron integral). This integral not only generalizes both Riemann and Lebesgue integration, but also has much more satisfactory Fundamental theorem of calculus: if a function is a.e. differentiable than it's differential is gauge-integrable and conversely function defined by gauge integral is a.e. differentiable (here almost everywhere means everywhere up to a countable set). Thank you for all the answers. This question should probably be altered to the following form (in more open-ended manner): What are pros and cons of different kinds of integrals, and when should we use one over the other?",,"['real-analysis', 'integration', 'lebesgue-integral', 'riemann-integration']"
86,Prove that the function$\ f(x)=\sin(x^2)$ is not uniformly continuous on the domain $\mathbb{R}$.,Prove that the function is not uniformly continuous on the domain .,\ f(x)=\sin(x^2) \mathbb{R},"If I want to prove that the function $\ f(x)=\sin(x^2)$ is not uniformly continuous on the domain $\mathbb{R}$ , I need to show that: $\exists\varepsilon>0$ $\forall\delta>0$ $\exists{x,y}\in\mathbb{R}\ : |x-y|<\delta$ and $|\sin(x^2) - \sin(y^2)|\geq\varepsilon$ . So let's take $\varepsilon = 1$ . Then I want $|\sin(x^2)-\sin(y^2)|\ge1$ . That's the case if $\sin(x^2)=0$ and $\sin(y^2)=\pm1$ . Thus $x^2=n\pi$ and $y^2=n\pi + \frac{1}{2}\pi$ . Now I'm stuck on expressing x and y, which I want to express in $\delta$ , to ensure that $|x-y|<\delta$ . Thanks in advance for any help.","If I want to prove that the function is not uniformly continuous on the domain , I need to show that: and . So let's take . Then I want . That's the case if and . Thus and . Now I'm stuck on expressing x and y, which I want to express in , to ensure that . Thanks in advance for any help.","\ f(x)=\sin(x^2) \mathbb{R} \exists\varepsilon>0 \forall\delta>0 \exists{x,y}\in\mathbb{R}\ : |x-y|<\delta |\sin(x^2) - \sin(y^2)|\geq\varepsilon \varepsilon = 1 |\sin(x^2)-\sin(y^2)|\ge1 \sin(x^2)=0 \sin(y^2)=\pm1 x^2=n\pi y^2=n\pi + \frac{1}{2}\pi \delta |x-y|<\delta","['real-analysis', 'continuity', 'uniform-continuity']"
87,Convergence/Divergence of infinite series $\sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$,Convergence/Divergence of infinite series,\sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n},"$$ \sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$$ Does it converge or diverge? Can we have a rigorous proof that is not probabilistic? For reference, this question is supposedly a mix of real analysis and calculus.","Does it converge or diverge? Can we have a rigorous proof that is not probabilistic? For reference, this question is supposedly a mix of real analysis and calculus.", \sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n},"['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
88,Prove that $\int_0^{\infty} \frac{\sin(2013 x)}{x(\cos x+\cosh x)}dx=\frac{\pi}{4}$,Prove that,\int_0^{\infty} \frac{\sin(2013 x)}{x(\cos x+\cosh x)}dx=\frac{\pi}{4},Prove that $$\int_0^{\infty} \frac{\sin(2013 x)}{x(\cos x+\cosh x)}dx=\frac{\pi}{4}$$,Prove that $$\int_0^{\infty} \frac{\sin(2013 x)}{x(\cos x+\cosh x)}dx=\frac{\pi}{4}$$,,"['calculus', 'real-analysis', 'trigonometry', 'definite-integrals', 'improper-integrals']"
89,Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed],Let  a differentiable function such that  for all  [closed],f:\mathbb{R}\longrightarrow \mathbb{R} f'(x)=0 x\in\mathbb{Q},Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}.$ $f$ is a constant function?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}.$ $f$ is a constant function?,,['real-analysis']
90,How to show $\lim_{x \to 1} \frac{x + x^2 + \dots + x^n - n}{x - 1} = \frac{n(n + 1)}{2}$?,How to show ?,\lim_{x \to 1} \frac{x + x^2 + \dots + x^n - n}{x - 1} = \frac{n(n + 1)}{2},"I am able to evaluate the limit $$\lim_{x \to 1} \frac{x + x^2 + \dots + x^n - n}{x - 1} = \frac{n(n + 1)}{2}$$ for a given $n$ using l'Hôspital's (Bernoulli's) rule. The problem is I don't quite like the solution, as it depends on such a heavy weaponry. A limit this simple, should easily be evaluable using some clever idea. Here is a list of what I tried: Substitute $y = x - 1$. This leads nowhere, I think. Find the Taylor polynomial. Makes no sense, it is a polynomial. Divide by major term. Dividing by $x$ got me nowhere. Find the value $f(x)$ at $x = 1$ directly. I cannot as the function is not defined at $x = 1$. Simplify the expression. I do not see how I could. Using l'Hôspital's (Bernoulli's) rule. Works, but I do not quite like it. If somebody sees a simple way, please do let me know. Added later: The approach proposed by Sami Ben Romdhane is universal as asmeurer pointed out. Examples of another limits that can be easily solved this way: $\lim_{x \to 0} \frac{\sqrt[m]{1 + ax} - \sqrt[n]{1 + bx}}{x}$ where $m, n \in \mathbb{N}$ and $a, b \in \mathbb{R}$ are given, or $\lim_{x \to 0} \frac{\arctan(1 + x) - \arctan(1 - x)}{x}$. It sems that all limits in the form $\lim_{x \to a} \frac{f(x)}{x - a}$ where $a \in \mathbb{R}$, $f(a) = 0$ and for which $\exists f'(a)$, can be evaluated this way, which is as fast as finding $f'$ and calculating $f'(a)$. This adds a very useful tool into my calculus toolbox: Some limits can be evaluated easily using derivatives if one looks for $f(a) = 0$, without the l'Hôspital's rule. I have not seen this in widespread use; I propose we call this Sami's rule :).","I am able to evaluate the limit $$\lim_{x \to 1} \frac{x + x^2 + \dots + x^n - n}{x - 1} = \frac{n(n + 1)}{2}$$ for a given $n$ using l'Hôspital's (Bernoulli's) rule. The problem is I don't quite like the solution, as it depends on such a heavy weaponry. A limit this simple, should easily be evaluable using some clever idea. Here is a list of what I tried: Substitute $y = x - 1$. This leads nowhere, I think. Find the Taylor polynomial. Makes no sense, it is a polynomial. Divide by major term. Dividing by $x$ got me nowhere. Find the value $f(x)$ at $x = 1$ directly. I cannot as the function is not defined at $x = 1$. Simplify the expression. I do not see how I could. Using l'Hôspital's (Bernoulli's) rule. Works, but I do not quite like it. If somebody sees a simple way, please do let me know. Added later: The approach proposed by Sami Ben Romdhane is universal as asmeurer pointed out. Examples of another limits that can be easily solved this way: $\lim_{x \to 0} \frac{\sqrt[m]{1 + ax} - \sqrt[n]{1 + bx}}{x}$ where $m, n \in \mathbb{N}$ and $a, b \in \mathbb{R}$ are given, or $\lim_{x \to 0} \frac{\arctan(1 + x) - \arctan(1 - x)}{x}$. It sems that all limits in the form $\lim_{x \to a} \frac{f(x)}{x - a}$ where $a \in \mathbb{R}$, $f(a) = 0$ and for which $\exists f'(a)$, can be evaluated this way, which is as fast as finding $f'$ and calculating $f'(a)$. This adds a very useful tool into my calculus toolbox: Some limits can be evaluated easily using derivatives if one looks for $f(a) = 0$, without the l'Hôspital's rule. I have not seen this in widespread use; I propose we call this Sami's rule :).",,"['calculus', 'real-analysis', 'limits']"
91,Proof of $\lim_{n\to \infty} \sqrt[n]{n}=1$,Proof of,\lim_{n\to \infty} \sqrt[n]{n}=1,"Thomson et al. provide a proof that $\lim_{n\rightarrow \infty} \sqrt[n]{n}=1$ in this book (page 73) . It has to do with using an inequality that relies on the binomial theorem: I have an alternative proof that I know (from elsewhere) as follows. Proof . \begin{align} \lim_{n\rightarrow \infty} \frac{ \log n}{n} = 0 \end{align} Then using this, I can instead prove: \begin{align} \lim_{n\rightarrow \infty} \sqrt[n]{n} &= \lim_{n\rightarrow \infty} \exp{\frac{ \log n}{n}} \newline & = \exp{0} \newline & = 1 \end{align} On the one hand, it seems like a valid proof to me. On the other hand, I know I should be careful with infinite sequences. The step I'm most unsure of is: \begin{align} \lim_{n\rightarrow \infty} \sqrt[n]{n} = \lim_{n\rightarrow \infty} \exp{\frac{ \log n}{n}} \end{align} I know such an identity would hold for bounded $n$ but I'm not sure I can use this identity when $n\rightarrow \infty$ . Question: If I am correct, then would there be any cases where I would be wrong? Specifically, given any sequence $x_n$ , can I always assume: \begin{align} \lim_{n\rightarrow \infty} x_n = \lim_{n\rightarrow \infty} \exp(\log x_n) \end{align} Or are there sequences that invalidate that identity? (Edited to expand the last question) given any sequence $x_n$ , can I always assume: \begin{align} \lim_{n\rightarrow \infty} x_n &=  \exp(\log \lim_{n\rightarrow \infty} x_n) \newline &=  \exp(\lim_{n\rightarrow \infty} \log x_n) \newline &=  \lim_{n\rightarrow \infty} \exp( \log x_n) \end{align} Or are there sequences that invalidate any of the above identities? (Edited to repurpose this question). Please also feel free to add different proofs of $\lim_{n\rightarrow \infty} \sqrt[n]{n}=1$ .","Thomson et al. provide a proof that in this book (page 73) . It has to do with using an inequality that relies on the binomial theorem: I have an alternative proof that I know (from elsewhere) as follows. Proof . Then using this, I can instead prove: On the one hand, it seems like a valid proof to me. On the other hand, I know I should be careful with infinite sequences. The step I'm most unsure of is: I know such an identity would hold for bounded but I'm not sure I can use this identity when . Question: If I am correct, then would there be any cases where I would be wrong? Specifically, given any sequence , can I always assume: Or are there sequences that invalidate that identity? (Edited to expand the last question) given any sequence , can I always assume: Or are there sequences that invalidate any of the above identities? (Edited to repurpose this question). Please also feel free to add different proofs of .","\lim_{n\rightarrow \infty} \sqrt[n]{n}=1 \begin{align}
\lim_{n\rightarrow \infty} \frac{ \log n}{n} = 0
\end{align} \begin{align}
\lim_{n\rightarrow \infty} \sqrt[n]{n} &= \lim_{n\rightarrow \infty} \exp{\frac{ \log n}{n}} \newline
& = \exp{0} \newline
& = 1
\end{align} \begin{align}
\lim_{n\rightarrow \infty} \sqrt[n]{n} = \lim_{n\rightarrow \infty} \exp{\frac{ \log n}{n}}
\end{align} n n\rightarrow \infty x_n \begin{align}
\lim_{n\rightarrow \infty} x_n = \lim_{n\rightarrow \infty} \exp(\log x_n)
\end{align} x_n \begin{align}
\lim_{n\rightarrow \infty} x_n &=  \exp(\log \lim_{n\rightarrow \infty} x_n) \newline
&=  \exp(\lim_{n\rightarrow \infty} \log x_n) \newline
&=  \lim_{n\rightarrow \infty} \exp( \log x_n)
\end{align} \lim_{n\rightarrow \infty} \sqrt[n]{n}=1","['real-analysis', 'sequences-and-series', 'limits', 'radicals']"
92,Continuity $\Rightarrow$ Intermediate Value Property. Why is the opposite not true?,Continuity  Intermediate Value Property. Why is the opposite not true?,\Rightarrow,Continuity $\Rightarrow$ Intermediate Value Property. Why is the opposite not true? It seems to me like they are equal definitions in a way. Can you give me a counter-example? Thanks,Continuity $\Rightarrow$ Intermediate Value Property. Why is the opposite not true? It seems to me like they are equal definitions in a way. Can you give me a counter-example? Thanks,,"['real-analysis', 'continuity', 'examples-counterexamples']"
93,"How can I prove $\sup(A+B)=\sup A+\sup B$ if $A+B=\{a+b\mid a\in A, b\in B\}$",How can I prove  if,"\sup(A+B)=\sup A+\sup B A+B=\{a+b\mid a\in A, b\in B\}","If $A,B$ non empty, upper bounded sets and $A+B=\{a+b\mid a\in A, b\in B\}$, how can I prove that $\sup(A+B)=\sup A+\sup B$?","If $A,B$ non empty, upper bounded sets and $A+B=\{a+b\mid a\in A, b\in B\}$, how can I prove that $\sup(A+B)=\sup A+\sup B$?",,"['real-analysis', 'supremum-and-infimum', 'sumset']"
94,What is the difference between outer measure and Lebesgue measure?,What is the difference between outer measure and Lebesgue measure?,,"What is the difference between outer measure and Lebesgue measure? We know that there are sets which are not Lebesgue measurable, whereas we know that outer measure is defined for any subset of $\mathbb{R}$.","What is the difference between outer measure and Lebesgue measure? We know that there are sets which are not Lebesgue measurable, whereas we know that outer measure is defined for any subset of $\mathbb{R}$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
95,How does the existence of a limit imply that a function is uniformly continuous,How does the existence of a limit imply that a function is uniformly continuous,,"I am working on a homework problem from Avner Friedman's Advanced Calculus (#1 page 68) which asks Suppose that $f(x)$ is a continuous function on the interval $[0,\infty)$. Prove that if $\lim_{x\to\infty} f(x)$ exists (as a real number), then $f(x)$ is uniformly continuous on this interval. Intuitively, this argument makes sense to me. Since the limit of $f(x)$ exists on $[0,\infty)$, we will be able to find a $\delta > |x_0 - x_1|$ and this implies that, for any $\epsilon>0$, we have $\epsilon > |f(x_0) - f(x_1)|$ (independent of the points chosen). I am aware that the condition of uniform continuity requires that $\delta$ can only be a function of $\epsilon$, not $x$. What information does the existence of a real-valued limit provide that implies $f(x)$ is uniformly continuous on this interval?","I am working on a homework problem from Avner Friedman's Advanced Calculus (#1 page 68) which asks Suppose that $f(x)$ is a continuous function on the interval $[0,\infty)$. Prove that if $\lim_{x\to\infty} f(x)$ exists (as a real number), then $f(x)$ is uniformly continuous on this interval. Intuitively, this argument makes sense to me. Since the limit of $f(x)$ exists on $[0,\infty)$, we will be able to find a $\delta > |x_0 - x_1|$ and this implies that, for any $\epsilon>0$, we have $\epsilon > |f(x_0) - f(x_1)|$ (independent of the points chosen). I am aware that the condition of uniform continuity requires that $\delta$ can only be a function of $\epsilon$, not $x$. What information does the existence of a real-valued limit provide that implies $f(x)$ is uniformly continuous on this interval?",,"['calculus', 'real-analysis', 'limits', 'uniform-continuity']"
96,Proving that a function is odd,Proving that a function is odd,,"Assume that there exists a function $f:\mathbb{R}\to\mathbb{R}$ that is bijective and satisfies $$ f(x) + f^{-1}(x)=x $$ for all $x$. Here $f^{-1}$ is the inverse function. Show that $f$ is odd. This was a brain-teaser given to me by a friend. Two other related questions are: Show that $f$ is discontinuous Give an example of such a function (if indeed one exists). Edit: As an initial idea, maybe approaching the problem graphically would help? A function and its inverse are reflections of each other about $y=x$ on the $x$-$y$ plane. Does this lead to anywhere?","Assume that there exists a function $f:\mathbb{R}\to\mathbb{R}$ that is bijective and satisfies $$ f(x) + f^{-1}(x)=x $$ for all $x$. Here $f^{-1}$ is the inverse function. Show that $f$ is odd. This was a brain-teaser given to me by a friend. Two other related questions are: Show that $f$ is discontinuous Give an example of such a function (if indeed one exists). Edit: As an initial idea, maybe approaching the problem graphically would help? A function and its inverse are reflections of each other about $y=x$ on the $x$-$y$ plane. Does this lead to anywhere?",,"['real-analysis', 'functions']"
97,Is the real number structure unique?,Is the real number structure unique?,,"For a frame of reference, I'm an undergraduate in mathematics who has taken the introductory analysis series and the graduate level algebra sequence at my university. In my analysis class, our book lists axioms describing the structure of the reals. This seemed unnatural to me, as we can often write definitions or lists of rules that no set actually satisfies. So the axiomatic approach doesn't reassure me that the thing we are discussing can actually exist. Our teacher talked to us about dedekind cuts as a way of explicitly constructing reals, which seemed more useful to me. In my modern algebra class we discussed completion of the rationals as a way to construct the real numbers. But this leads me to wonder - How do we know that all of these approaches and constructions result in the same structure, namely $\mathbb{R}$? Also, are the real numbers  the only complete, totally ordered field? If so, why?","For a frame of reference, I'm an undergraduate in mathematics who has taken the introductory analysis series and the graduate level algebra sequence at my university. In my analysis class, our book lists axioms describing the structure of the reals. This seemed unnatural to me, as we can often write definitions or lists of rules that no set actually satisfies. So the axiomatic approach doesn't reassure me that the thing we are discussing can actually exist. Our teacher talked to us about dedekind cuts as a way of explicitly constructing reals, which seemed more useful to me. In my modern algebra class we discussed completion of the rationals as a way to construct the real numbers. But this leads me to wonder - How do we know that all of these approaches and constructions result in the same structure, namely $\mathbb{R}$? Also, are the real numbers  the only complete, totally ordered field? If so, why?",,"['real-analysis', 'analysis', 'real-numbers']"
98,Bijection from $\mathbb R$ to $\mathbb {R^N}$,Bijection from  to,\mathbb R \mathbb {R^N},How does one create an explicit bijection from the reals to the set of all sequences of reals? I know how to make a bijection from $\mathbb R$ to $\mathbb {R \times R}$ . I have an idea but I am not sure if it will work. I will post it as my own answer because I don't want to anchor your answers and I want to see what other possible ways of doing this are.,How does one create an explicit bijection from the reals to the set of all sequences of reals? I know how to make a bijection from $\mathbb R$ to $\mathbb {R \times R}$ . I have an idea but I am not sure if it will work. I will post it as my own answer because I don't want to anchor your answers and I want to see what other possible ways of doing this are.,,"['real-analysis', 'elementary-set-theory']"
99,Are weak derivatives and distributional derivatives different?,Are weak derivatives and distributional derivatives different?,,"Given a real function $f\in L^1_{\text{loc}}(\Omega)$ , we define both weak or distributional derivatives by $\int f'\phi = - \int f \phi'$ for all test functions $\phi$ . Now, take $\Omega = (-1,1)$ , and $f(x) = I_{x>0}$ , an indicator function.  Then, according to Example 2 of Section 5.2 of Evans' PDE book , there is no weak derivatives. But, it is well known that $f' = \delta_0$ as a distribution. In fact, every distribution has its derivative according to Rudin's book on Functional Analysis, see Section 6.1. At this far, can anybody clarify the following questions? weak derivatives is stronger than distributional derivatives? If yes, how strong? Is $\delta_0$ a $L^1_{loc}(-1,1)$ , a locally integrable function? see also this question . Most PDE books use weak derivatives, not distributional one?","Given a real function , we define both weak or distributional derivatives by for all test functions . Now, take , and , an indicator function.  Then, according to Example 2 of Section 5.2 of Evans' PDE book , there is no weak derivatives. But, it is well known that as a distribution. In fact, every distribution has its derivative according to Rudin's book on Functional Analysis, see Section 6.1. At this far, can anybody clarify the following questions? weak derivatives is stronger than distributional derivatives? If yes, how strong? Is a , a locally integrable function? see also this question . Most PDE books use weak derivatives, not distributional one?","f\in L^1_{\text{loc}}(\Omega) \int f'\phi = - \int f \phi' \phi \Omega = (-1,1) f(x) = I_{x>0} f' = \delta_0 \delta_0 L^1_{loc}(-1,1)","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'distribution-theory', 'weak-derivatives']"
