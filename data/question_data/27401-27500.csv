,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove $A_{ij} = \frac{1}{\sqrt{i+j}}$ is positive definite,Prove  is positive definite,A_{ij} = \frac{1}{\sqrt{i+j}},"I'm currently working through an exercise in an undergraduate functional analysis textbook and encountered the following problem: Prove that the $n\times n$ matrix $A$ with entries defined as $A_{ij} = \frac{1}{\sqrt{i+j}}$ is positive definite. In our lectures, we've just been introduced to the concept of inner products, and this problem was presented soon after. I suspect that the solution may not require advanced theorems. I have checked the result with numeric methods. And it turns out that at least for $n\le13$ it's true. For $n=14$ somehow the determinant becomes $−2.26×10^{−111}$ . Is it really negative or a numeric error?","I'm currently working through an exercise in an undergraduate functional analysis textbook and encountered the following problem: Prove that the matrix with entries defined as is positive definite. In our lectures, we've just been introduced to the concept of inner products, and this problem was presented soon after. I suspect that the solution may not require advanced theorems. I have checked the result with numeric methods. And it turns out that at least for it's true. For somehow the determinant becomes . Is it really negative or a numeric error?",n\times n A A_{ij} = \frac{1}{\sqrt{i+j}} n\le13 n=14 −2.26×10^{−111},"['linear-algebra', 'functional-analysis']"
1,Is there an upper bound on the trace of positive definite matrix as a function of the trace of the inverse of the matrix?,Is there an upper bound on the trace of positive definite matrix as a function of the trace of the inverse of the matrix?,,"Let $A$ be an $n \times n$ positive definite matrix. In this answer (based on a question I previously asked), a lower bound is given on $\text{tr}[A]$ as a function of $\text{tr}[A^{-1}]$ : $$\text{tr}[A] \geq \dfrac{n^2}{\text{tr}[A^{-1}]}$$ Is there also an upper bound? I can think of an upper bound on the trace as a function of the maximum eigenvalue of $A$ . For example, $$ \text{tr}[A] \leq n \lambda_{\max}[A] $$ However, I can't think of an upper bound as a function of the trace of the inverse of $A$ .","Let be an positive definite matrix. In this answer (based on a question I previously asked), a lower bound is given on as a function of : Is there also an upper bound? I can think of an upper bound on the trace as a function of the maximum eigenvalue of . For example, However, I can't think of an upper bound as a function of the trace of the inverse of .",A n \times n \text{tr}[A] \text{tr}[A^{-1}] \text{tr}[A] \geq \dfrac{n^2}{\text{tr}[A^{-1}]} A  \text{tr}[A] \leq n \lambda_{\max}[A]  A,"['linear-algebra', 'inequality', 'upper-lower-bounds', 'trace']"
2,AB diagonalizable then BA also diagonalizable,AB diagonalizable then BA also diagonalizable,,If A and B don't commute are there counterexamples that AB is diagonalizable but BA not? I read that if AB=BA then both AB and BA are diagonalizable.,If A and B don't commute are there counterexamples that AB is diagonalizable but BA not? I read that if AB=BA then both AB and BA are diagonalizable.,,['linear-algebra']
3,Show that $A^2+2A+5I=0$ has a solution if and only if $n$ is even.,Show that  has a solution if and only if  is even.,A^2+2A+5I=0 n,"I have a real square matrix $A$ . I am told to prove that there is $A$ such that $A^2+2A+5I=0$ if and only if $n$ is even. (If $A$ is 6x6, $n=6$ ) I honestly have no clue how to start. Maybe I could turn this into a question with minimal polynomial and use $x^2+2x+5$ . This polynomial doesn't have a root, and it is making me even more confused. Could someone help? Thank you.","I have a real square matrix . I am told to prove that there is such that if and only if is even. (If is 6x6, ) I honestly have no clue how to start. Maybe I could turn this into a question with minimal polynomial and use . This polynomial doesn't have a root, and it is making me even more confused. Could someone help? Thank you.",A A A^2+2A+5I=0 n A n=6 x^2+2x+5,"['linear-algebra', 'abstract-algebra', 'matrices']"
4,Finding the exponential of a matrix,Finding the exponential of a matrix,,"I am trying to find $\exp\left(\frac{i{\mu}t\mathbf{H}}{\sqrt{2}}\right)$ where $$\mathbf{H}=\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ I've got as far as figuring out that that, for $n=1,2,3,\dots$ $$ \mathbf{H}^{2n}=2^{n-1}\mathbf{H}^2=2^{n-1} \begin{pmatrix}1 & 0 & 1 \\ 0 & 2 & 0 \\ 1 & 0 & 1\end{pmatrix} $$ $$\mathbf{H}^{2n-1}=2^{n-1}\mathbf{H}=2^{n-1}\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ but I'm not getting anywhere substituting this into the power series and I've no idea how to proceed. I've seen mention of Jordan form in other questions about this but I've never heard of it. If I need to learn it so-be-it, but I think there must be some other way as we have never been shown it in class.","I am trying to find $\exp\left(\frac{i{\mu}t\mathbf{H}}{\sqrt{2}}\right)$ where $$\mathbf{H}=\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ I've got as far as figuring out that that, for $n=1,2,3,\dots$ $$ \mathbf{H}^{2n}=2^{n-1}\mathbf{H}^2=2^{n-1} \begin{pmatrix}1 & 0 & 1 \\ 0 & 2 & 0 \\ 1 & 0 & 1\end{pmatrix} $$ $$\mathbf{H}^{2n-1}=2^{n-1}\mathbf{H}=2^{n-1}\begin{pmatrix}0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0\end{pmatrix}$$ but I'm not getting anywhere substituting this into the power series and I've no idea how to proceed. I've seen mention of Jordan form in other questions about this but I've never heard of it. If I need to learn it so-be-it, but I think there must be some other way as we have never been shown it in class.",,"['linear-algebra', 'matrices', 'matrix-exponential']"
5,Why two vectors cannot span ${\bf R}^3$?,Why two vectors cannot span ?,{\bf R}^3,"Consider two vectors  $$ u_{1} = \begin{bmatrix} -1 \\  3 \\  2 \\ \end{bmatrix},\quad u_{2} = \begin{bmatrix}  6 \\   1 \\  1 \\ \end{bmatrix} $$ I can tell they don't span $R^3$ because $R^3$ requires three vectors to span it. But is there another way I can see that it does not span $R^3$, for example, in terms of overdetermined system, or not a square matrix? Just trying to develop a mature understanding of this topic, that's all.","Consider two vectors  $$ u_{1} = \begin{bmatrix} -1 \\  3 \\  2 \\ \end{bmatrix},\quad u_{2} = \begin{bmatrix}  6 \\   1 \\  1 \\ \end{bmatrix} $$ I can tell they don't span $R^3$ because $R^3$ requires three vectors to span it. But is there another way I can see that it does not span $R^3$, for example, in terms of overdetermined system, or not a square matrix? Just trying to develop a mature understanding of this topic, that's all.",,"['linear-algebra', 'vector-spaces']"
6,Find $\mathrm{Ker}(T)$ and $\mathrm{Im}(T)$ of the following linear transformation with bases,Find  and  of the following linear transformation with bases,\mathrm{Ker}(T) \mathrm{Im}(T),"Question : Let T: $\mathbb R^{3} → M_{2\times 2}(\mathbb R)$ be the linear transformation defined by $$T((a,b,c)) = \begin{bmatrix}          a & 5a\\          c & 3c\\         \end{bmatrix} $$ Consider the bases $\alpha = \{(0, 1, 0),(0, 1, 1),(1, 1, 0)\}$ of $\mathbb R^{3}$ and ,  $\beta = \{{\begin{bmatrix}\          1 &−1\\          0 & 0\\         \end{bmatrix}} ,{\begin{bmatrix}          0 & 1\\          -1 & 0\\         \end{bmatrix}} ,{\begin{bmatrix}          0 & 0\\          1 & 1\\         \end{bmatrix}}, {\begin{bmatrix}          0 &1\\          0 & -1\\         \end{bmatrix}}\}$ of $M_{2\times 2}(\mathbb R)$. Find $\mathrm{Ker}(T)$ and $\mathrm{Im}(T)$. Context : This question is a practice problem given to us by our introductory linear algebra professor for solving and understanding concepts (Note: this is not an assignment or graded homework question) Attempt : Finding $\mathrm{Ker}(T)$ : Row reducing the transformation matrix to RREF would give the matrix \begin{bmatrix}          1 &0\\          0 & 1\\         \end{bmatrix} leaving no free variables thus $\mathrm{Ker}(T)=\mathrm{span}\{0\} $ Finding $\mathrm{Im}(T)$ : Since there are no free variables, we use the original columns from $T((a,b,c))$ to get our Image i.e. $\mathrm{Col}(A) =\mathrm{span}\{\begin{bmatrix}          a \\          c \\         \end{bmatrix}, \begin{bmatrix}          5a \\          3c \\         \end{bmatrix}\}$. I know we use this with the basis $\beta$ to obtain our image. How do we proceed from here? Do we perform $a\begin{bmatrix}          1&-1 \\          0&0 \\         \end{bmatrix} + c\begin{bmatrix}          0&0 \\          1&1 \\         \end{bmatrix}$ and $5a\begin{bmatrix}          1&-1 \\          0&0 \\         \end{bmatrix} + 3c\begin{bmatrix}          0&0 \\          1&1 \\         \end{bmatrix}$? This gives us $\mathrm{Im}(T) = \mathrm{span} \{\begin{bmatrix}          a&-a \\          c&c \\         \end{bmatrix}, \begin{bmatrix}          a&-5a \\          3c&3c \\         \end{bmatrix}\}$. Doubt : Is this correct or am I going about this the wrong way? I have a feeling I made a mistake because according to the R-N theorem, my rank and nullity must add up to $3$ but at this point, it adds up to $2$.","Question : Let T: $\mathbb R^{3} → M_{2\times 2}(\mathbb R)$ be the linear transformation defined by $$T((a,b,c)) = \begin{bmatrix}          a & 5a\\          c & 3c\\         \end{bmatrix} $$ Consider the bases $\alpha = \{(0, 1, 0),(0, 1, 1),(1, 1, 0)\}$ of $\mathbb R^{3}$ and ,  $\beta = \{{\begin{bmatrix}\          1 &−1\\          0 & 0\\         \end{bmatrix}} ,{\begin{bmatrix}          0 & 1\\          -1 & 0\\         \end{bmatrix}} ,{\begin{bmatrix}          0 & 0\\          1 & 1\\         \end{bmatrix}}, {\begin{bmatrix}          0 &1\\          0 & -1\\         \end{bmatrix}}\}$ of $M_{2\times 2}(\mathbb R)$. Find $\mathrm{Ker}(T)$ and $\mathrm{Im}(T)$. Context : This question is a practice problem given to us by our introductory linear algebra professor for solving and understanding concepts (Note: this is not an assignment or graded homework question) Attempt : Finding $\mathrm{Ker}(T)$ : Row reducing the transformation matrix to RREF would give the matrix \begin{bmatrix}          1 &0\\          0 & 1\\         \end{bmatrix} leaving no free variables thus $\mathrm{Ker}(T)=\mathrm{span}\{0\} $ Finding $\mathrm{Im}(T)$ : Since there are no free variables, we use the original columns from $T((a,b,c))$ to get our Image i.e. $\mathrm{Col}(A) =\mathrm{span}\{\begin{bmatrix}          a \\          c \\         \end{bmatrix}, \begin{bmatrix}          5a \\          3c \\         \end{bmatrix}\}$. I know we use this with the basis $\beta$ to obtain our image. How do we proceed from here? Do we perform $a\begin{bmatrix}          1&-1 \\          0&0 \\         \end{bmatrix} + c\begin{bmatrix}          0&0 \\          1&1 \\         \end{bmatrix}$ and $5a\begin{bmatrix}          1&-1 \\          0&0 \\         \end{bmatrix} + 3c\begin{bmatrix}          0&0 \\          1&1 \\         \end{bmatrix}$? This gives us $\mathrm{Im}(T) = \mathrm{span} \{\begin{bmatrix}          a&-a \\          c&c \\         \end{bmatrix}, \begin{bmatrix}          a&-5a \\          3c&3c \\         \end{bmatrix}\}$. Doubt : Is this correct or am I going about this the wrong way? I have a feeling I made a mistake because according to the R-N theorem, my rank and nullity must add up to $3$ but at this point, it adds up to $2$.",,"['linear-algebra', 'linear-transformations']"
7,Prove that the rank of a real skew symmetric matrix is not $1$,Prove that the rank of a real skew symmetric matrix is not,1,"I was thinking of using the RREF of $A$, where $A$ is an $n \times n$ skew symmetric matrix. So, suppose that it's rank is 1. then the first row has at least one non zero element(which is 1), while all other rows are 0. Now we can get back to original the skew symmetric matrix by applying the required row operations. Now the rows of the original matrix must be a multiple of the 1st row of the RREF of A. But the entry in the $k$th column of the first row of the RREF matrix is 1. Hence this implies that the $k$th row is $\theta \in \mathbb{R^n}$. So, $a_{k1}=0$. Hence $a_{1k}=0$. This is a contradiction. Hence the rank cannot be 1. Is this correct? Please do not use eigenvalues if you post an answer.  Thank you for reading.","I was thinking of using the RREF of $A$, where $A$ is an $n \times n$ skew symmetric matrix. So, suppose that it's rank is 1. then the first row has at least one non zero element(which is 1), while all other rows are 0. Now we can get back to original the skew symmetric matrix by applying the required row operations. Now the rows of the original matrix must be a multiple of the 1st row of the RREF of A. But the entry in the $k$th column of the first row of the RREF matrix is 1. Hence this implies that the $k$th row is $\theta \in \mathbb{R^n}$. So, $a_{k1}=0$. Hence $a_{1k}=0$. This is a contradiction. Hence the rank cannot be 1. Is this correct? Please do not use eigenvalues if you post an answer.  Thank you for reading.",,"['linear-algebra', 'matrices']"
8,To find factor of a polynomial equation,To find factor of a polynomial equation,,One of the factors of $4x^2+y^2+14x-7y-4xy+12$ is equal to $2x-y+4$ $2x-y-3$ $2x+y-4$ $2x-y+3$ Step $1$ : $4x^2+y^2-4xy$ can be simplified as $(2x-y)^2$ Step $2$ : $14x-7y$ can be simplified as $7(2x-y)$ and finally $(2x-y) (2x-y+7) + 12$ I can able to factor to this extent only. however can't able to arrive at the answer. The answer is given in the book. it states that $4x^2+y^2+14x-7y-4xy+12$ is product of $(2x-y+3)$ and $(2x-y+4)$ I am in need of steps,One of the factors of is equal to Step : can be simplified as Step : can be simplified as and finally I can able to factor to this extent only. however can't able to arrive at the answer. The answer is given in the book. it states that is product of and I am in need of steps,4x^2+y^2+14x-7y-4xy+12 2x-y+4 2x-y-3 2x+y-4 2x-y+3 1 4x^2+y^2-4xy (2x-y)^2 2 14x-7y 7(2x-y) (2x-y) (2x-y+7) + 12 4x^2+y^2+14x-7y-4xy+12 (2x-y+3) (2x-y+4),['linear-algebra']
9,The set of all matrices of full rank is an open set,The set of all matrices of full rank is an open set,,"Let $m$ and $n$ be positive integers such that $m<n$ and let $M(m\times n,\mathbb{R})$ be he set of all $m\times n$ matrices with entries from $\mathbb{R}$. Let $M_{m}$ be the subset of $M(m\times n,\mathbb{R})$ consisting of the matrices of rank $m$. Show that $M_{m}$ is an open subset of $M(m\times n,\mathbb{R})$. I know from Linear Algebra that an element of $M_{m}$ must have an invertible $m\times m$  submatrix. I am pretty sure that this fact, along with the fact that the set of invertible $m\times m$ matrices with entries from $\mathbb{R}$, $GL(m,\mathbb{R})$, is an open set, is used in the proof, but I cant connect them together.","Let $m$ and $n$ be positive integers such that $m<n$ and let $M(m\times n,\mathbb{R})$ be he set of all $m\times n$ matrices with entries from $\mathbb{R}$. Let $M_{m}$ be the subset of $M(m\times n,\mathbb{R})$ consisting of the matrices of rank $m$. Show that $M_{m}$ is an open subset of $M(m\times n,\mathbb{R})$. I know from Linear Algebra that an element of $M_{m}$ must have an invertible $m\times m$  submatrix. I am pretty sure that this fact, along with the fact that the set of invertible $m\times m$ matrices with entries from $\mathbb{R}$, $GL(m,\mathbb{R})$, is an open set, is used in the proof, but I cant connect them together.",,"['linear-algebra', 'general-topology']"
10,Can You Add a Multiple of a Matrix Row to itself?,Can You Add a Multiple of a Matrix Row to itself?,,"I apologize in advance, as it seems this might be one of those questions that is so mind-boggling obvious to those who know the answer that most don't even think to treat upon it. I've searched up and down for information on matrice elementary row operations, but none of them have thought to explicitly treat upon the addition of a multiple of a matrix row to itself. All of them say that it is possible to add a multiple of a matrix row to another row, but does adding a multiple of a matrix row to itself present a special case? Is it legal? It's a relatively straightforward question, I suppose, but I haven't been able to find confirmation one way or another on the topic. :( Thank you very much for your time.","I apologize in advance, as it seems this might be one of those questions that is so mind-boggling obvious to those who know the answer that most don't even think to treat upon it. I've searched up and down for information on matrice elementary row operations, but none of them have thought to explicitly treat upon the addition of a multiple of a matrix row to itself. All of them say that it is possible to add a multiple of a matrix row to another row, but does adding a multiple of a matrix row to itself present a special case? Is it legal? It's a relatively straightforward question, I suppose, but I haven't been able to find confirmation one way or another on the topic. :( Thank you very much for your time.",,"['linear-algebra', 'matrices']"
11,Rank + nullity theorem,Rank + nullity theorem,,"Looking to give an example of a $4×5$ matrix A with $dim(Null(A)) = 3$. My thinking here is that by the $rank+null$ theorem, $rank+null= number of columns$. So the numbers of columns is $5$ so $ 5 = rank + 3 $. I got 3 from $dim(Null(A)) = 3$. So the rank of $A$ would be $2$. So this would just be a $4x5$ matrix with 2 leading 1's. Just wondering if my logic was correct. Can I get the null of the matrix from the statement $dim(Null(A)) = 3$ like I did?","Looking to give an example of a $4×5$ matrix A with $dim(Null(A)) = 3$. My thinking here is that by the $rank+null$ theorem, $rank+null= number of columns$. So the numbers of columns is $5$ so $ 5 = rank + 3 $. I got 3 from $dim(Null(A)) = 3$. So the rank of $A$ would be $2$. So this would just be a $4x5$ matrix with 2 leading 1's. Just wondering if my logic was correct. Can I get the null of the matrix from the statement $dim(Null(A)) = 3$ like I did?",,"['linear-algebra', 'matrix-rank']"
12,Matrix $A$ has two distinct real eigenvalues iff $k$ is greater than what?,Matrix  has two distinct real eigenvalues iff  is greater than what?,A k,"The matrix $A = \begin{bmatrix}3&k\\8&8\end{bmatrix}$ has two distinct real eigenvalues iff $k > ?$ So I found the determinant by doing: $(3 - \lambda)(8 - \lambda) - 8k = \lambda^2 - 11\lambda + 24 - 8k \implies \lambda = 8, \lambda = 3$ The thing is, I'm not really sure what they are asking me because I have found what the eigenvalues are: $\lambda_1 = 8, \lambda_2 = 3$. I'm assuming I need to solve for $k$ somehow but it doesn't seem very straightforward to me, what am I missing here?","The matrix $A = \begin{bmatrix}3&k\\8&8\end{bmatrix}$ has two distinct real eigenvalues iff $k > ?$ So I found the determinant by doing: $(3 - \lambda)(8 - \lambda) - 8k = \lambda^2 - 11\lambda + 24 - 8k \implies \lambda = 8, \lambda = 3$ The thing is, I'm not really sure what they are asking me because I have found what the eigenvalues are: $\lambda_1 = 8, \lambda_2 = 3$. I'm assuming I need to solve for $k$ somehow but it doesn't seem very straightforward to me, what am I missing here?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
13,Prove that the determinant of skew-symmetric matrices of odd order is zero,Prove that the determinant of skew-symmetric matrices of odd order is zero,,I have to prove that determinant of skew-symmetric matrix of odd order is zero and also that its adjoint doesn't exist. I am sorry if the question is duplicate or already exists.I am not getting any start. I study in Class 11 so please give the proof accordingly. Thanks!,I have to prove that determinant of skew-symmetric matrix of odd order is zero and also that its adjoint doesn't exist. I am sorry if the question is duplicate or already exists.I am not getting any start. I study in Class 11 so please give the proof accordingly. Thanks!,,"['linear-algebra', 'matrices', 'determinant', 'skew-symmetric-matrices']"
14,Vector space or vector field?,Vector space or vector field?,,I seem to be having a problem distinguishing between a vector space (which I know to be a set of vectors over some scalar set) and a vector field. I know that in Multivariable Calculus a vector field is a vector-valued function (being a derivative for conserved fields.) I feel they are different but how does one explain in layman terms the difference between the two?,I seem to be having a problem distinguishing between a vector space (which I know to be a set of vectors over some scalar set) and a vector field. I know that in Multivariable Calculus a vector field is a vector-valued function (being a derivative for conserved fields.) I feel they are different but how does one explain in layman terms the difference between the two?,,"['linear-algebra', 'multivariable-calculus', 'vector-spaces', 'vector-fields']"
15,Linear Algebra Done Right Example 1.7,Linear Algebra Done Right Example 1.7,,"In the book Linear Algebra Done Right I came across this example for the sum of vector spaces. Did he say the second $W + U$ is still equal to 1.7 because it doesn't matter whether you say $(x+y, y, 0)$ or $(x, y, 0)$ or am I missing something? It seems like this notation can get confusing if that is the case. Thanks, Jackson","In the book Linear Algebra Done Right I came across this example for the sum of vector spaces. Did he say the second $W + U$ is still equal to 1.7 because it doesn't matter whether you say $(x+y, y, 0)$ or $(x, y, 0)$ or am I missing something? It seems like this notation can get confusing if that is the case. Thanks, Jackson",,"['linear-algebra', 'notation']"
16,Roots of a cubic,Roots of a cubic,,"How would I find the real root, I know I can say another root is (5+i) but would I use the product of the roots at all?","How would I find the real root, I know I can say another root is (5+i) but would I use the product of the roots at all?",,['linear-algebra']
17,All nilpotent $2\times 2$ matrices,All nilpotent  matrices,2\times 2,I want to find all nilpotent $2\times 2$ matrices. All nilpotent $2 \times 2$ matrices are similar($A=P^{-1}JP$) to $J = \begin{bmatrix} 0&1\\0&0\end{bmatrix}$ But how do I find all of these matrices? I do think that the only such cases are $J$ and $J^ T$,I want to find all nilpotent $2\times 2$ matrices. All nilpotent $2 \times 2$ matrices are similar($A=P^{-1}JP$) to $J = \begin{bmatrix} 0&1\\0&0\end{bmatrix}$ But how do I find all of these matrices? I do think that the only such cases are $J$ and $J^ T$,,"['linear-algebra', 'abstract-algebra']"
18,Intuitive understanding of the $BAB^{-1}$ formula for changing basis in linear transformations.,Intuitive understanding of the  formula for changing basis in linear transformations.,BAB^{-1},"I would please like to have a better understanding on why we use the formula $BAB^{-1}$ when chaning basis on linear transformations. (I have not a complete understanding on this so there are probably errors in my question, please point them out so I can learn more...) For example: In $\mathbb{R^3}$ Lets say we have a basis $e = {(e_1, e_2, e_3)}$ and another basis $f = {(f_1, f_2, f_3)}$. We have a change of basis matrix from $f$ to $e$ called $B$ that expresses what $f_1, f_2, f_3$ is in the basis $e$. We have defined a linear transformation $R_e$ in the basis $e$ with the matrix $A$. Let's say we have three vectors expressed in the base $f$: $P = \left( \begin{array}{c} s\\ t\\ u\\ \end{array} \right)$ Now we want to apply the linear transformation $R_e = A$ to these vectors: First we have to change the basis of $P$ which gives us: $BP$. $BP$ is now $P$ expressed in the basis $e$. To apply the linear transformation we now only has to multiply $BP$ with $A$: $ABP$. But according to the formula it should be $BAB^{-1}P$. Where is my thinking incorrect?","I would please like to have a better understanding on why we use the formula $BAB^{-1}$ when chaning basis on linear transformations. (I have not a complete understanding on this so there are probably errors in my question, please point them out so I can learn more...) For example: In $\mathbb{R^3}$ Lets say we have a basis $e = {(e_1, e_2, e_3)}$ and another basis $f = {(f_1, f_2, f_3)}$. We have a change of basis matrix from $f$ to $e$ called $B$ that expresses what $f_1, f_2, f_3$ is in the basis $e$. We have defined a linear transformation $R_e$ in the basis $e$ with the matrix $A$. Let's say we have three vectors expressed in the base $f$: $P = \left( \begin{array}{c} s\\ t\\ u\\ \end{array} \right)$ Now we want to apply the linear transformation $R_e = A$ to these vectors: First we have to change the basis of $P$ which gives us: $BP$. $BP$ is now $P$ expressed in the basis $e$. To apply the linear transformation we now only has to multiply $BP$ with $A$: $ABP$. But according to the formula it should be $BAB^{-1}P$. Where is my thinking incorrect?",,['linear-algebra']
19,What is the SVD of $A^{-1}$?,What is the SVD of ?,A^{-1},"Let $A\in R^{n\times n}$ with full SVD $U\Sigma V^T$ where $U$ and $V$ are orthogonal $n\times n$ matrices and $\Sigma$ is an $n\times n$ diagonal matrix with entries $\sigma_1 \geq\cdots\geq \sigma_n \geq 0$. 1- What is the SVD of $A^{-1}$ ? 2- Given that ||A|| = $\sigma_1$, how would we express ||$A^{-1}$|| in terms of the singular values of A? 3- What is the condition number of A?","Let $A\in R^{n\times n}$ with full SVD $U\Sigma V^T$ where $U$ and $V$ are orthogonal $n\times n$ matrices and $\Sigma$ is an $n\times n$ diagonal matrix with entries $\sigma_1 \geq\cdots\geq \sigma_n \geq 0$. 1- What is the SVD of $A^{-1}$ ? 2- Given that ||A|| = $\sigma_1$, how would we express ||$A^{-1}$|| in terms of the singular values of A? 3- What is the condition number of A?",,"['linear-algebra', 'matrices']"
20,Find a second degree polynomial that goes through 3 points,Find a second degree polynomial that goes through 3 points,,"I am having trouble calculating the quadratic curve $f(x)$ that goes through 3 points; for example a curve that goes through $A(1,3), B(-1,-5), and C(-2,12)$. I can only guess that the curve is upwards and that I may create the system: $$ y_1 = ax^2_1 + bx_1 + c\\ y_2 = ax^2_2 + bx_2 + c\\ y_3 = ax^2_3 + bx_3 + c $$ assuming that the points are in the format $A(x,y)$ and from this point what do I do? Do I build a matrix and use the Gaussian eliminations? EDIT: I also know that $f(4) = 120$","I am having trouble calculating the quadratic curve $f(x)$ that goes through 3 points; for example a curve that goes through $A(1,3), B(-1,-5), and C(-2,12)$. I can only guess that the curve is upwards and that I may create the system: $$ y_1 = ax^2_1 + bx_1 + c\\ y_2 = ax^2_2 + bx_2 + c\\ y_3 = ax^2_3 + bx_3 + c $$ assuming that the points are in the format $A(x,y)$ and from this point what do I do? Do I build a matrix and use the Gaussian eliminations? EDIT: I also know that $f(4) = 120$",,"['linear-algebra', 'gaussian-elimination']"
21,"Does $ST=TS$ with $S,T$ diagonalizable matrices imply that they share eigenspaces?",Does  with  diagonalizable matrices imply that they share eigenspaces?,"ST=TS S,T","I know it's been answered before (at least to the case with $n$ different eigenvalues) but I didn't find a proof for the general case, and I would like some help with this question. We are given linear transforms $S,T: V\to V$ where $V$ is some vector space. We are given that $S$ and $T$ commute, $ST=TS$, and that they are diagonalizable: $T=PD_1P^{-1}$ and $S=KD_2K^{-1}$, where $D_1, D_2$ are diagonal and $K,P$ are invertible. We are asked to show that $S$ and $T$ have a common eigenspace. My solution Maybe I understood the question wrong, but what I tried to do is show that if $v$ is an eigenvector of $S$ then it is also an eigenvector of $T$. let $Sv=\lambda v$. $STv=TSv=T\lambda v=\lambda Tv$ which implies that $Tv$ is an eigenvector of $S$ with eigenvalue $\lambda$. Why does that mean that $v$ is an eigenvalue of $T$? Another possible way to solve this question is write: $PD_1P^{-1}KD_2K^{-1} = KD_2K^{-1}PD_1P^{-1}$ and get that $P=K$ but I don't know how to do that either.","I know it's been answered before (at least to the case with $n$ different eigenvalues) but I didn't find a proof for the general case, and I would like some help with this question. We are given linear transforms $S,T: V\to V$ where $V$ is some vector space. We are given that $S$ and $T$ commute, $ST=TS$, and that they are diagonalizable: $T=PD_1P^{-1}$ and $S=KD_2K^{-1}$, where $D_1, D_2$ are diagonal and $K,P$ are invertible. We are asked to show that $S$ and $T$ have a common eigenspace. My solution Maybe I understood the question wrong, but what I tried to do is show that if $v$ is an eigenvector of $S$ then it is also an eigenvector of $T$. let $Sv=\lambda v$. $STv=TSv=T\lambda v=\lambda Tv$ which implies that $Tv$ is an eigenvector of $S$ with eigenvalue $\lambda$. Why does that mean that $v$ is an eigenvalue of $T$? Another possible way to solve this question is write: $PD_1P^{-1}KD_2K^{-1} = KD_2K^{-1}PD_1P^{-1}$ and get that $P=K$ but I don't know how to do that either.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
22,Two inner products being equal up to a scalar,Two inner products being equal up to a scalar,,"I would appreciate a hint on the following problem: Let $V$ be a finite dimensional vector space over $F$. There are two scalar products such that: $$ \forall \ w,v \in V \ \Big(\langle v,w\rangle_1=0 \implies \langle v,w\rangle_2=0\Big) $$ Show that $$ \exists \ c \in F \ \ \forall \ w,v \in V \ \Big(\langle v,w\rangle_1=c \langle v,w\rangle_2\Big) $$ I have tried to define an orthonormal basis with respect to $\langle \cdot,\cdot\rangle_1$ hoping that the transformation matrices would be different by a constant, yet it seems to lead nowhere.","I would appreciate a hint on the following problem: Let $V$ be a finite dimensional vector space over $F$. There are two scalar products such that: $$ \forall \ w,v \in V \ \Big(\langle v,w\rangle_1=0 \implies \langle v,w\rangle_2=0\Big) $$ Show that $$ \exists \ c \in F \ \ \forall \ w,v \in V \ \Big(\langle v,w\rangle_1=c \langle v,w\rangle_2\Big) $$ I have tried to define an orthonormal basis with respect to $\langle \cdot,\cdot\rangle_1$ hoping that the transformation matrices would be different by a constant, yet it seems to lead nowhere.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
23,Is the sum of two singular matrices also singular?,Is the sum of two singular matrices also singular?,,"If $A$ and $B$ are $n \times n$ singular matrices, is $A+B$ also singular?","If and are singular matrices, is also singular?",A B n \times n A+B,"['linear-algebra', 'matrices', 'examples-counterexamples']"
24,Is it possible for a matrix to not be onto or 1-1?,Is it possible for a matrix to not be onto or 1-1?,,Is it possible for a matrix to be neither onto nor 1-1?,Is it possible for a matrix to be neither onto nor 1-1?,,"['linear-algebra', 'matrices']"
25,Subspace of a Vector Space must be non-empty.,Subspace of a Vector Space must be non-empty.,,"I am in the process of teaching some first year students about subspaces of $\mathbb{R}^{n}$ and need to stress that to check that $V\subseteq\mathbb{R}^{n}$ is a subspace, then you need to show that it is non empty. Specifically, they know that a subset $V\subseteq \mathbb{R}^{n}$ is a subspace if and only if the following three conditions hold: $V\neq\emptyset$; If $x,y\in V$, then $(x+y)\in V$; and If $x\in V$ and $\lambda\in\mathbb{R}$, then $\lambda\cdot x\in V$. Is there actually an example of a subset $\emptyset=V\subseteq\mathbb{R}^{n}$ that is defined by a set of conditions such that if $x$ and $y$ were to satisfy those conditions, then $(x+y)$ and $\lambda\cdot x$ would also satisfy those conditions? Edit: So specifically, has anyone got an example of a set of conditions on the coefficients of vectors in $\mathbb{R}^{n}$ such that $V=\left\{\left(\begin{array}{c} a_{1} \\ a_{2} \\ \vdots \\ a_{n} \end{array}\right):a_{1},\ldots,a_{n}\text{ satisfy certain conditions}\right\}=\emptyset$ but that if $(a_{1},\ldots,a_{n})^{T}$ and  $(b_{1},\ldots,b_{n})^{T}$ were to satisfy the conditions, then so would $a_{1}+b_{1}$ etc. So basically I want a set of conditions that are closed under addition and scalare multiplication, but together are inconsistent. For example, $3\vert a_{1}$ is a condition that is closed under addition but not under scalar multiplication, $\vert a_{1}\vert \geq \vert a_{2}\vert$ is closed under scalar multiplication but not addition etc. I hope that clarifies things a bit.","I am in the process of teaching some first year students about subspaces of $\mathbb{R}^{n}$ and need to stress that to check that $V\subseteq\mathbb{R}^{n}$ is a subspace, then you need to show that it is non empty. Specifically, they know that a subset $V\subseteq \mathbb{R}^{n}$ is a subspace if and only if the following three conditions hold: $V\neq\emptyset$; If $x,y\in V$, then $(x+y)\in V$; and If $x\in V$ and $\lambda\in\mathbb{R}$, then $\lambda\cdot x\in V$. Is there actually an example of a subset $\emptyset=V\subseteq\mathbb{R}^{n}$ that is defined by a set of conditions such that if $x$ and $y$ were to satisfy those conditions, then $(x+y)$ and $\lambda\cdot x$ would also satisfy those conditions? Edit: So specifically, has anyone got an example of a set of conditions on the coefficients of vectors in $\mathbb{R}^{n}$ such that $V=\left\{\left(\begin{array}{c} a_{1} \\ a_{2} \\ \vdots \\ a_{n} \end{array}\right):a_{1},\ldots,a_{n}\text{ satisfy certain conditions}\right\}=\emptyset$ but that if $(a_{1},\ldots,a_{n})^{T}$ and  $(b_{1},\ldots,b_{n})^{T}$ were to satisfy the conditions, then so would $a_{1}+b_{1}$ etc. So basically I want a set of conditions that are closed under addition and scalare multiplication, but together are inconsistent. For example, $3\vert a_{1}$ is a condition that is closed under addition but not under scalar multiplication, $\vert a_{1}\vert \geq \vert a_{2}\vert$ is closed under scalar multiplication but not addition etc. I hope that clarifies things a bit.",,"['linear-algebra', 'vector-spaces']"
26,Spectrum of the sum of two commuting matrices,Spectrum of the sum of two commuting matrices,,"Suppose $A$ and $B$ are two commuting square matrices over an algebraically closed field. It is true that the spectrum of $A+B$ is contained in the set $\{\lambda_1+\lambda_2:\lambda_1 \in \sigma(A), \lambda_2 \in \sigma(B)\}$ where $\sigma(A)$ denotes the spectrum of $A$? If so, can you provide a reference for this? can more be said? Thanks","Suppose $A$ and $B$ are two commuting square matrices over an algebraically closed field. It is true that the spectrum of $A+B$ is contained in the set $\{\lambda_1+\lambda_2:\lambda_1 \in \sigma(A), \lambda_2 \in \sigma(B)\}$ where $\sigma(A)$ denotes the spectrum of $A$? If so, can you provide a reference for this? can more be said? Thanks",,['linear-algebra']
27,"If a symmetric matrix commutes with all symmetric matrices, is it then a multiple of the identity?","If a symmetric matrix commutes with all symmetric matrices, is it then a multiple of the identity?",,"I know that, if a matrix commutes with all matrices, then it is a multiple of the identity; see here . The same conclusion holds if a (special) orthogonal matrix commutes with all (special) orthogonal matrices, as shown here and here . In this context, I wonder if the following claim is true. Claim : If a symmetric matrix $A$ commutes with every other symmetric matrix, does it then follow that $A = \lambda  I$ for some $\lambda \in \mathbb{R}$ ?","I know that, if a matrix commutes with all matrices, then it is a multiple of the identity; see here . The same conclusion holds if a (special) orthogonal matrix commutes with all (special) orthogonal matrices, as shown here and here . In this context, I wonder if the following claim is true. Claim : If a symmetric matrix commutes with every other symmetric matrix, does it then follow that for some ?",A A = \lambda  I \lambda \in \mathbb{R},"['linear-algebra', 'abstract-algebra']"
28,Proof that all $n\times n$ matrices that are nilpotent of order $n$ are similar.,Proof that all  matrices that are nilpotent of order  are similar.,n\times n n,"Can someone give a proof that all $n\times n$ matrices that are nilpotent of order $n$ are similar? A matrix $A$ is called nilpotent if there exists some positive integer $k$ such that $A^k$ is the $0$ -matrix. The order of nilpotency of a matrix is the smallest $k$ with this property. That is, the question is about matrices such that $A^n=0$ yet $A^{n-1} \neq 0$ . I am new in linear algebra and know up to linear transformation and isomorphisms and matrix representation of transformations. The same question is answered in this site using Jordan Matrix and other things which I do not know. So can anyone give a proof using elementary things which I know.","Can someone give a proof that all matrices that are nilpotent of order are similar? A matrix is called nilpotent if there exists some positive integer such that is the -matrix. The order of nilpotency of a matrix is the smallest with this property. That is, the question is about matrices such that yet . I am new in linear algebra and know up to linear transformation and isomorphisms and matrix representation of transformations. The same question is answered in this site using Jordan Matrix and other things which I do not know. So can anyone give a proof using elementary things which I know.",n\times n n A k A^k 0 k A^n=0 A^{n-1} \neq 0,['linear-algebra']
29,Is $\mathbb{Q}^n$ a vector space over $\mathbb{Z}$ or $\mathbb{Q}$?,Is  a vector space over  or ?,\mathbb{Q}^n \mathbb{Z} \mathbb{Q},"Is $\mathbb{Q}^n$ a vector space over $\mathbb{Z}$ or over $\mathbb{Q}$ ? $\mathbb{Q}^n$ is clearly not a vector space over $\mathbb{R}$ , because scalar multiplication of some $q \in \mathbb{Q}$ by $\pi$ renders $\pi q$ , which is irrational.","Is a vector space over or over ? is clearly not a vector space over , because scalar multiplication of some by renders , which is irrational.",\mathbb{Q}^n \mathbb{Z} \mathbb{Q} \mathbb{Q}^n \mathbb{R} q \in \mathbb{Q} \pi \pi q,"['linear-algebra', 'vector-spaces', 'vectors']"
30,Product of two lower triangular matrices is a lower triangular matrix,Product of two lower triangular matrices is a lower triangular matrix,,"For my univerity studies I had to prove this: $ Let \,\, n \,\ \in N \,\, and \,\, L1,\, L2 \, \in R(n \times n) \,\, be \,\, both \,\, lower \,\, triangular \,\, matrices.$ $ Show \,\, that \,\, L := L1L2 \,\, is \,\, also \,\, a \,\, lower \,\, triangular \,\, matrix.$ I proved it like this (and I need some verification for the proof): $L_{ij} := (L1L2)_{ij}$ $Now \,\, just \,\, look \,\, at \,\, (L1L2)_{ij} \,\, where \,\, j > i.$ $(L1L2)_{ij} = \sum_{r=1}^n l1_{ir}l2_{rj} = \sum_{r=j}^i l1_{ir}l2_{rj} = \sum_{r=1}^n l1_{ir}l2_{rj} = \begin{cases} 0,  & \text{if $j>i$ is even} \\ a \in R, & \text{else} \end{cases} $ $\Rightarrow \text{L is lower triangular}$ $\square$",For my univerity studies I had to prove this: I proved it like this (and I need some verification for the proof):," Let \,\, n \,\ \in N \,\, and \,\, L1,\, L2 \, \in R(n \times n) \,\, be \,\, both \,\, lower \,\, triangular \,\, matrices.  Show \,\, that \,\, L := L1L2 \,\, is \,\, also \,\, a \,\, lower \,\, triangular \,\, matrix. L_{ij} := (L1L2)_{ij} Now \,\, just \,\, look \,\, at \,\, (L1L2)_{ij} \,\, where \,\, j > i. (L1L2)_{ij} = \sum_{r=1}^n l1_{ir}l2_{rj} = \sum_{r=j}^i l1_{ir}l2_{rj} = \sum_{r=1}^n l1_{ir}l2_{rj} = \begin{cases}
0,  & \text{if j>i is even} \\
a \in R, & \text{else}
\end{cases}
 \Rightarrow \text{L is lower triangular} \square","['linear-algebra', 'matrices', 'proof-verification']"
31,"Rank, dimension, basis","Rank, dimension, basis",,"I think I am a little bit confused with the terms in the title, so I hope you can correct me if I got it wrong... $$ \left\lbrace \begin{bmatrix} x_{1}\\0\\0 \end{bmatrix} : x_{1} \in \mathbb{R} \right\rbrace $$ is a vector space. So far so good. The dimension of the vector space is the number of vectors in the basis. In class, I wrote down that a basis is $$ B=(\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix}) $$ But this does not seem logical to me now, as it should be the smallest number of independent vectors that span the vector space. And shouldn’t that be just $ (\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix}) $ So this is the place where I'm not sure: what is the dimension of the vector space? It is a line in ${\mathbb R}^3$ , and it would seem logical that the dimension of a line is $1$ . And what would the rank be? I totally confused myself. Or is it like the dimension is $3$ and the rank is $1$ . so the above solution for the basis would be correct... Many thanks for your help","I think I am a little bit confused with the terms in the title, so I hope you can correct me if I got it wrong... is a vector space. So far so good. The dimension of the vector space is the number of vectors in the basis. In class, I wrote down that a basis is But this does not seem logical to me now, as it should be the smallest number of independent vectors that span the vector space. And shouldn’t that be just So this is the place where I'm not sure: what is the dimension of the vector space? It is a line in , and it would seem logical that the dimension of a line is . And what would the rank be? I totally confused myself. Or is it like the dimension is and the rank is . so the above solution for the basis would be correct... Many thanks for your help","
\left\lbrace \begin{bmatrix} x_{1}\\0\\0 \end{bmatrix} : x_{1} \in \mathbb{R} \right\rbrace
 
B=(\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix})
 
(\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix})
 {\mathbb R}^3 1 3 1",['linear-algebra']
32,"Can a set of vectors be linearly independent in one vector space, but be linearly dependent in another vector space?","Can a set of vectors be linearly independent in one vector space, but be linearly dependent in another vector space?",,"For example, let S = { $(x_1, x_1)| x_1 \in \mathbb R$ } be a subspace of $\mathbb R^2$ . By definition, dim(S) = 1, and dim( $\mathbb R^2$ ) = 2. Then the set {(1, 1)} only has one vector, so is it linearly independent in S, but is linearly dependent in $\mathbb R^2$ ? I know this doesn't make any sense, but we learned in class that a set of vectors can only be linearly independent if it spans the vector space that it is in. Since (1,1) is in both S and $\mathbb R^2$ , but the set {(1, 1)} only spans S, how come it is not only linearly independent in S and linearly dependent in $\mathbb R^2$ (since {(1,1)} does not span $\mathbb R^2$ ). Sorry for this stupid question","For example, let S = { } be a subspace of . By definition, dim(S) = 1, and dim( ) = 2. Then the set {(1, 1)} only has one vector, so is it linearly independent in S, but is linearly dependent in ? I know this doesn't make any sense, but we learned in class that a set of vectors can only be linearly independent if it spans the vector space that it is in. Since (1,1) is in both S and , but the set {(1, 1)} only spans S, how come it is not only linearly independent in S and linearly dependent in (since {(1,1)} does not span ). Sorry for this stupid question","(x_1, x_1)| x_1 \in \mathbb R \mathbb R^2 \mathbb R^2 \mathbb R^2 \mathbb R^2 \mathbb R^2 \mathbb R^2",['linear-algebra']
33,How to prove one-to-one and onto?,How to prove one-to-one and onto?,,"I am trying to prove $(-1, 1)$ is equinumerous to $\mathbb{R}$ by finding a bijection function from $(-1, 1)$ to $\mathbb{R}$. I've found a function $f(x) = \frac{x}{x^2-1}$, and it's a bijection obviously from its graph, but I don't know how to prove it is bijection rigorously. Any help, thanks!","I am trying to prove $(-1, 1)$ is equinumerous to $\mathbb{R}$ by finding a bijection function from $(-1, 1)$ to $\mathbb{R}$. I've found a function $f(x) = \frac{x}{x^2-1}$, and it's a bijection obviously from its graph, but I don't know how to prove it is bijection rigorously. Any help, thanks!",,[]
34,Is a vector space over a ring or over a field?,Is a vector space over a ring or over a field?,,"What is a vector space? I can see two different formulations, and between them there is one difference: commutativity. DEFINITION 1 (See here ) Let $(F, +_F, \times_F)$ be a division ring.   Let $(\mathcal{V}, +_\mathcal{V})$ be an abelian group.    Let $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ be a unitary module over $F$. Then $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ is a vector space over $F$. That is, a vector space is a unitary module over a ring, whose ring is a division ring. DEFINITION 2 Let $(F, +_F, \times_F)$ be a field.   Let $(\mathcal{V}, +_\mathcal{V})$ be an abelian group.    Let $\cdot: F\times \mathcal{V} \longrightarrow \mathcal{V}$ be a function. A vector space is $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ such that $\forall a,b, \in F$ and $\forall x,y \in \mathcal{V}$: $\cdot$ right distributive:  $(a +_F b) \cdot x = (a\cdot x) +_\mathcal{V} (b\cdot x)$ $\cdot$ left distributive: $\,\,\, a \cdot (x +_\mathcal{V} y) = (a\cdot x) +_\mathcal{V} (a\cdot y)$ $\cdot$ compatible with $\times_F$: $(a\times_F b) \cdot x = a \cdot (b\cdot x)$ $\times_F$ 's identity is $\cdot$'s identity: $1_F \cdot x = x$ There could also be other definitions,but for now it doesn't matter. What matter is that commutativity is not considered in the same way in both definitions! In the first definition, we ahve a division ring (not a commutative division ring, i.e. a field!), while in the second we have a field (i.e. a commutative division ring). Notice that the key difference on which I am struggling is that on one side we have a division ring and on the other side a commutative division ring. The first is an abelian group $(R, +_R)$ under the $+_R$ binary operation, however $(R, \times_R)$ is only a group (i.e. not abelian, i.e. not commutative).","What is a vector space? I can see two different formulations, and between them there is one difference: commutativity. DEFINITION 1 (See here ) Let $(F, +_F, \times_F)$ be a division ring.   Let $(\mathcal{V}, +_\mathcal{V})$ be an abelian group.    Let $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ be a unitary module over $F$. Then $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ is a vector space over $F$. That is, a vector space is a unitary module over a ring, whose ring is a division ring. DEFINITION 2 Let $(F, +_F, \times_F)$ be a field.   Let $(\mathcal{V}, +_\mathcal{V})$ be an abelian group.    Let $\cdot: F\times \mathcal{V} \longrightarrow \mathcal{V}$ be a function. A vector space is $(\mathcal{V}, +_\mathcal{V}, \cdot)_F$ such that $\forall a,b, \in F$ and $\forall x,y \in \mathcal{V}$: $\cdot$ right distributive:  $(a +_F b) \cdot x = (a\cdot x) +_\mathcal{V} (b\cdot x)$ $\cdot$ left distributive: $\,\,\, a \cdot (x +_\mathcal{V} y) = (a\cdot x) +_\mathcal{V} (a\cdot y)$ $\cdot$ compatible with $\times_F$: $(a\times_F b) \cdot x = a \cdot (b\cdot x)$ $\times_F$ 's identity is $\cdot$'s identity: $1_F \cdot x = x$ There could also be other definitions,but for now it doesn't matter. What matter is that commutativity is not considered in the same way in both definitions! In the first definition, we ahve a division ring (not a commutative division ring, i.e. a field!), while in the second we have a field (i.e. a commutative division ring). Notice that the key difference on which I am struggling is that on one side we have a division ring and on the other side a commutative division ring. The first is an abelian group $(R, +_R)$ under the $+_R$ binary operation, however $(R, \times_R)$ is only a group (i.e. not abelian, i.e. not commutative).",,"['linear-algebra', 'vector-spaces', 'definition']"
35,Minimum Least Squares Solution Using Pseudo Inverse (Derived from SVD) Is The Minimum Norm Solution - Extension from Vectors to Matrices,Minimum Least Squares Solution Using Pseudo Inverse (Derived from SVD) Is The Minimum Norm Solution - Extension from Vectors to Matrices,,"Given $A \in \mathbb{R}^{m \times n}$ , $B \in \mathbb{R}^{k \times \ell}$ , and $C\in \mathbb{R}^{m \times \ell}$ . Show that for $X \in \mathbb{R}^{n \times k}$ $$ {A}^{\dagger} C {B}^{\dagger} = \arg \min_{X} {\left\| C - A X B \right\|}_{F} $$ is the unique solution. Note: This is an extension of the minimum $2$ -norm least squares problem. Hint: Use the SVDs of $A$ and $B$ . This is a homework problem, but I got stuck. After I do SVD for both $A$ and $B$ , I discard the $0$ terms to get a slim SVD. Then I can factor out some things, but still don't see an explicit expression formulating. Could somebody guide me in the right direction? Edit: 1.""F norm"": Frobenius norm 2. $A^+$ or $A^\dagger$ : conjugate transpose, Moore–Penrose pseudoinverse","Given , , and . Show that for is the unique solution. Note: This is an extension of the minimum -norm least squares problem. Hint: Use the SVDs of and . This is a homework problem, but I got stuck. After I do SVD for both and , I discard the terms to get a slim SVD. Then I can factor out some things, but still don't see an explicit expression formulating. Could somebody guide me in the right direction? Edit: 1.""F norm"": Frobenius norm 2. or : conjugate transpose, Moore–Penrose pseudoinverse",A \in \mathbb{R}^{m \times n} B \in \mathbb{R}^{k \times \ell} C\in \mathbb{R}^{m \times \ell} X \in \mathbb{R}^{n \times k}  {A}^{\dagger} C {B}^{\dagger} = \arg \min_{X} {\left\| C - A X B \right\|}_{F}  2 A B A B 0 A^+ A^\dagger,"['linear-algebra', 'optimization', 'matrix-calculus', 'least-squares', 'svd']"
36,"What's an intuitive explanation behind cross products being vectors, when dot products are not?","What's an intuitive explanation behind cross products being vectors, when dot products are not?",,"As someone currently taking Multivariable Calculus but hasn't taken Linear Algebra, I've been trying to catch up on LA and build an intuition; simply knowing equations isn't really satisfying or useful. My understanding of dot product and cross product (in $\mathbb{R}^3$) is that they generalize multiplication; whereas one becomes multiplication of the norms when vectors are parallel, the other (or rather, its magnitude) becomes multiplication of the norms when vectors are orthogonal. Dot product is maximized via similarity in the direction of vectors, whereas the magnitude of cross product is maximized by differences in the directions. This answer did a nice job at explaining that concept. However, I'm still struggling to understand why cross products are vectors when dot products are scalars. The magnitude of cross product seems intuitive as an opposite to dot product, but I don't see how that is conceptually related to a vector being orthogonal to two others. I've read answers like this to try and understand it, but I still don't see the relationship between measuring the directional difference between two vectors, and creating a new one perpendicular to the two vectors. How can I intuitively grasp that?","As someone currently taking Multivariable Calculus but hasn't taken Linear Algebra, I've been trying to catch up on LA and build an intuition; simply knowing equations isn't really satisfying or useful. My understanding of dot product and cross product (in $\mathbb{R}^3$) is that they generalize multiplication; whereas one becomes multiplication of the norms when vectors are parallel, the other (or rather, its magnitude) becomes multiplication of the norms when vectors are orthogonal. Dot product is maximized via similarity in the direction of vectors, whereas the magnitude of cross product is maximized by differences in the directions. This answer did a nice job at explaining that concept. However, I'm still struggling to understand why cross products are vectors when dot products are scalars. The magnitude of cross product seems intuitive as an opposite to dot product, but I don't see how that is conceptually related to a vector being orthogonal to two others. I've read answers like this to try and understand it, but I still don't see the relationship between measuring the directional difference between two vectors, and creating a new one perpendicular to the two vectors. How can I intuitively grasp that?",,"['linear-algebra', 'vectors', 'intuition', 'cross-product']"
37,"When taking the exponential of a diagonalizable matrix, why can the eigenvector matrices be taken out of the exponential?","When taking the exponential of a diagonalizable matrix, why can the eigenvector matrices be taken out of the exponential?",,"Given diagonalizable matrix $A$ with diagonal form $D$, $A=PDP^{-1}$ where $P$ is the matrix of eigenvectors, why is it that that $e^A=e^{PDP^{-1}}=Pe^DP^{-1}$?","Given diagonalizable matrix $A$ with diagonal form $D$, $A=PDP^{-1}$ where $P$ is the matrix of eigenvectors, why is it that that $e^A=e^{PDP^{-1}}=Pe^DP^{-1}$?",,['linear-algebra']
38,Find complex number Z in $\lvert Z\rvert= Z+3-2i$,Find complex number Z in,\lvert Z\rvert= Z+3-2i,"$$\lvert Z\rvert = Z+ 3-2i$$ what I did so far is let $Z = a +bi$ so $$\sqrt{a^2 + b^2} = a+bi+3-2i$$ $$\sqrt{a^2 + b^2} = a+3 + i (b-2)$$ now what I'm thinking is squaring both sides but that doesn't work, any tips?","$$\lvert Z\rvert = Z+ 3-2i$$ what I did so far is let $Z = a +bi$ so $$\sqrt{a^2 + b^2} = a+bi+3-2i$$ $$\sqrt{a^2 + b^2} = a+3 + i (b-2)$$ now what I'm thinking is squaring both sides but that doesn't work, any tips?",,"['linear-algebra', 'complex-numbers']"
39,How to find the number of solutions to a system of equations with one parameter which cannot be reduced?,How to find the number of solutions to a system of equations with one parameter which cannot be reduced?,,"I have the system of linear equations where $k$ is the parameter: $$ \begin{cases}  x+ky+z = 0 \\ kx+y+kz=0 \\ (k+1)x-y+z=0 \end{cases} $$ For which the matrix is: $$ \begin{bmatrix} 1 & k & 1 \\ k & 1 & k \\ k+1 & -1 & 1 \end{bmatrix} $$ I managed to reduce the matrix to: $$ \begin{bmatrix} 1 & k & 1 \\ 0 & 1-k^2 & 0 \\ 0 & 2+k & k \end{bmatrix} $$ However, I don't see how this can be reduced further. As far as I know I need the echelon form to be able to determine how many solutions exist. Should I just plug in $k=-2$ in order to reduce it further? If yes then what can be said about the number of solutions if $k \neq =2$? EDIT: I'm only beginning linear math course so I can't use any advanced techniques (like determinants etc.)","I have the system of linear equations where $k$ is the parameter: $$ \begin{cases}  x+ky+z = 0 \\ kx+y+kz=0 \\ (k+1)x-y+z=0 \end{cases} $$ For which the matrix is: $$ \begin{bmatrix} 1 & k & 1 \\ k & 1 & k \\ k+1 & -1 & 1 \end{bmatrix} $$ I managed to reduce the matrix to: $$ \begin{bmatrix} 1 & k & 1 \\ 0 & 1-k^2 & 0 \\ 0 & 2+k & k \end{bmatrix} $$ However, I don't see how this can be reduced further. As far as I know I need the echelon form to be able to determine how many solutions exist. Should I just plug in $k=-2$ in order to reduce it further? If yes then what can be said about the number of solutions if $k \neq =2$? EDIT: I'm only beginning linear math course so I can't use any advanced techniques (like determinants etc.)",,"['linear-algebra', 'systems-of-equations']"
40,If $ A^3=A$ prove that $Ker\left(A-I\right)+Im\left(A-I\right)=V$,If  prove that, A^3=A Ker\left(A-I\right)+Im\left(A-I\right)=V,"If $ A^3=A$ prove that $Ker\left(A-I\right)+Im\left(A-I\right)=V$ I am not sure how to approach this problem, but first things first if we have $A^3=A$ that is $A^2=I$, what does that tell me (what does it imply) about $A$? The only thing I can tell at this point is that $A$ is it's own inverse and if a matrice is invertible it has a full rank which implies that the dimension of the image space is $n$ and the dimension of the null-space aka kernel is zero. Thus we would have that $Im(A)+Ker(A)=V$, but subtracting $I$ confuses me here. P.S. $A$ is the matrix representation of a linear operator and $V$ is the vector space for which the operator is defined.","If $ A^3=A$ prove that $Ker\left(A-I\right)+Im\left(A-I\right)=V$ I am not sure how to approach this problem, but first things first if we have $A^3=A$ that is $A^2=I$, what does that tell me (what does it imply) about $A$? The only thing I can tell at this point is that $A$ is it's own inverse and if a matrice is invertible it has a full rank which implies that the dimension of the image space is $n$ and the dimension of the null-space aka kernel is zero. Thus we would have that $Im(A)+Ker(A)=V$, but subtracting $I$ confuses me here. P.S. $A$ is the matrix representation of a linear operator and $V$ is the vector space for which the operator is defined.",,['linear-algebra']
41,"Why does $\operatorname{null}(A) = \operatorname{null}(A^TA)$, intuitively?","Why does , intuitively?",\operatorname{null}(A) = \operatorname{null}(A^TA),"It's easy to show that the nullspace of $A$ and the nullspace of $A^TA$ are the same. But intuitively what does that mean?  Or maybe the better question to ask first is, intuitively how does $A^TA$ relate to $A$?","It's easy to show that the nullspace of $A$ and the nullspace of $A^TA$ are the same. But intuitively what does that mean?  Or maybe the better question to ask first is, intuitively how does $A^TA$ relate to $A$?",,"['linear-algebra', 'matrices', 'intuition']"
42,Why do we define vector spaces over a general $\mathbb{F}$ (field) rather than $\mathbb{C}$ (complex numbers)?,Why do we define vector spaces over a general  (field) rather than  (complex numbers)?,\mathbb{F} \mathbb{C},"In my linear algebra class, many times we define vector spaces over the field $\mathbb{F}$ (i.e. $\mathbb{F}^n$) and then prove things about them. The instructor has defined $\mathbb{F}$ as ""either $\mathbb{C}$ or $\mathbb{R}$"". Because $\mathbb{R}$ is a subset of $\mathbb{C}$, why can we not simply define vector spaces over $\mathbb{C}$ instead of introducing new notation? Isn't it true that a property holding over  $\mathbb{C}$ immediately tell us it holds over $\mathbb{R}$ because $\mathbb{R}\subset \mathbb{C}$?","In my linear algebra class, many times we define vector spaces over the field $\mathbb{F}$ (i.e. $\mathbb{F}^n$) and then prove things about them. The instructor has defined $\mathbb{F}$ as ""either $\mathbb{C}$ or $\mathbb{R}$"". Because $\mathbb{R}$ is a subset of $\mathbb{C}$, why can we not simply define vector spaces over $\mathbb{C}$ instead of introducing new notation? Isn't it true that a property holding over  $\mathbb{C}$ immediately tell us it holds over $\mathbb{R}$ because $\mathbb{R}\subset \mathbb{C}$?",,"['linear-algebra', 'vector-spaces']"
43,Regarding the proof that similar matrices have the same characteristic polynomial,Regarding the proof that similar matrices have the same characteristic polynomial,,The proof is: $$|\lambda I-P^{-1}AP|=|P^{-1}(\lambda I)P-P^{-1}AP|=|P^{-1}((\lambda I)P-AP)|=|P^{-1}((\lambda I)-A)P)|=|P^{-1}|\cdot|((\lambda I)-A)|\cdot|P|=|\lambda I-A|$$ What I do not understand is why $|\lambda I-P^{-1}AP|=|P^{-1}(\lambda I)P-P^{-1}AP|$ why can we multiple $|\lambda I|=|P^{-1}(\lambda I)P|$?,The proof is: $$|\lambda I-P^{-1}AP|=|P^{-1}(\lambda I)P-P^{-1}AP|=|P^{-1}((\lambda I)P-AP)|=|P^{-1}((\lambda I)-A)P)|=|P^{-1}|\cdot|((\lambda I)-A)|\cdot|P|=|\lambda I-A|$$ What I do not understand is why $|\lambda I-P^{-1}AP|=|P^{-1}(\lambda I)P-P^{-1}AP|$ why can we multiple $|\lambda I|=|P^{-1}(\lambda I)P|$?,,['linear-algebra']
44,Prove or disprove: $\operatorname{Adj} (A)$ is diagonlizable $\implies A$ is diagonalizable,Prove or disprove:  is diagonlizable  is diagonalizable,\operatorname{Adj} (A) \implies A,For $2X2$: $$ A:\\ \begin{bmatrix} a & b \\ c & d  \end{bmatrix} $$ $$ \operatorname{Adj}(A):\\ \begin{bmatrix} d & -c \\ -b & a  \end{bmatrix} $$ So the statement is true. The problem comes when it's $3X3$: $$ \operatorname{Adj}(A):\\ \begin{bmatrix} \left |\begin{matrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{matrix} \right | & 0 & 0 \\ 0 &  \left |\begin{matrix} a_{11} & a_{13} \\ a_{31} & a_{33}  \end{matrix} \right |& 0 \\ 0 & 0 & \left |\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22}  \end{matrix} \right | \end{bmatrix} $$  I'm not sure this implies that A is diagonlizable. I've done some examples and the statement was true but I can't prove the general case. EDIT: I just realized that I confused diagonal with diagonalizable so you can ignore all my steps.,For $2X2$: $$ A:\\ \begin{bmatrix} a & b \\ c & d  \end{bmatrix} $$ $$ \operatorname{Adj}(A):\\ \begin{bmatrix} d & -c \\ -b & a  \end{bmatrix} $$ So the statement is true. The problem comes when it's $3X3$: $$ \operatorname{Adj}(A):\\ \begin{bmatrix} \left |\begin{matrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{matrix} \right | & 0 & 0 \\ 0 &  \left |\begin{matrix} a_{11} & a_{13} \\ a_{31} & a_{33}  \end{matrix} \right |& 0 \\ 0 & 0 & \left |\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22}  \end{matrix} \right | \end{bmatrix} $$  I'm not sure this implies that A is diagonlizable. I've done some examples and the statement was true but I can't prove the general case. EDIT: I just realized that I confused diagonal with diagonalizable so you can ignore all my steps.,,"['linear-algebra', 'matrices', 'adjoint-operators']"
45,Definition of Inverse in Linear and Abstract Algebra,Definition of Inverse in Linear and Abstract Algebra,,"In a linear algebra text, the following is the definition of the inverse of a matrix An $n\times n$ matrix $A$ is invertible when there exists an $n \times n$ matrix $B$ such that $$AB = BA = I_n$$ And likewise in an abstract algebra textbook, the definition of the inverse of a group is Given that $G$ is a group with operation $*$, for each $a \in G$, there exists an element $a^{-1}$ such that $$a*a^{-1} = a^{-1}*a = e,$$ where $e$ is the identity element in $G$. Such element is called the inverse of $a$ in $G$. Unfortunately, the second semester of abstract algebra didn't quite finalize due to low enrollment, so I'm doing independent self-study of topics I missed in Linear Algebra (as mind preparation). Here's my question: Is it sufficient to show that $AB = I_n \;\;\implies \;\;B = A^{-1}$ and $A = B^{-1}$? Or must you check both that $AB = I_n$ and $BA = I_n$ to completely conclude that $A = B^{-1}$ and $B = A^{-1}$? I remember on an exam, I had to prove that for a group homomorphism $\phi: G\to H$, for any $a \in G$, $\phi(a^{-1}) = [\phi(a)]^{-1}$ which I proved by asking the reader to observe that $\phi(a)\phi(a^{-1}) = \phi(aa^{-1}) = \phi(e_G) = e_H$ which because $\phi(a)\phi(a^{-1}) = e_H$, this can only mean that $\phi(a)^{-1} = \phi(a^{-1})$ by definition of inverse. And I got full points for it, but it leaves me wondering: am I supposed to check both arrangements to create the strongest possible argument?","In a linear algebra text, the following is the definition of the inverse of a matrix An $n\times n$ matrix $A$ is invertible when there exists an $n \times n$ matrix $B$ such that $$AB = BA = I_n$$ And likewise in an abstract algebra textbook, the definition of the inverse of a group is Given that $G$ is a group with operation $*$, for each $a \in G$, there exists an element $a^{-1}$ such that $$a*a^{-1} = a^{-1}*a = e,$$ where $e$ is the identity element in $G$. Such element is called the inverse of $a$ in $G$. Unfortunately, the second semester of abstract algebra didn't quite finalize due to low enrollment, so I'm doing independent self-study of topics I missed in Linear Algebra (as mind preparation). Here's my question: Is it sufficient to show that $AB = I_n \;\;\implies \;\;B = A^{-1}$ and $A = B^{-1}$? Or must you check both that $AB = I_n$ and $BA = I_n$ to completely conclude that $A = B^{-1}$ and $B = A^{-1}$? I remember on an exam, I had to prove that for a group homomorphism $\phi: G\to H$, for any $a \in G$, $\phi(a^{-1}) = [\phi(a)]^{-1}$ which I proved by asking the reader to observe that $\phi(a)\phi(a^{-1}) = \phi(aa^{-1}) = \phi(e_G) = e_H$ which because $\phi(a)\phi(a^{-1}) = e_H$, this can only mean that $\phi(a)^{-1} = \phi(a^{-1})$ by definition of inverse. And I got full points for it, but it leaves me wondering: am I supposed to check both arrangements to create the strongest possible argument?",,"['linear-algebra', 'abstract-algebra', 'proof-writing', 'inverse']"
46,"Show that if $AA^t = A^tA$, then $A=A^t$","Show that if , then",AA^t = A^tA A=A^t,"Suppose $A$ is a matrix with non-negative real entries. If $A^tA = AA^t$, show that $A=A^t$. My proof says: $AA^t = A^tA = (AA^t)^t$. I can't seem to get to the point of $A=A^t$ Edit: What if $A$ is a $2x2$ matrix?","Suppose $A$ is a matrix with non-negative real entries. If $A^tA = AA^t$, show that $A=A^t$. My proof says: $AA^t = A^tA = (AA^t)^t$. I can't seem to get to the point of $A=A^t$ Edit: What if $A$ is a $2x2$ matrix?",,"['linear-algebra', 'matrices']"
47,Is there something called the Reduced Column echleon form?,Is there something called the Reduced Column echleon form?,,"I recently asked a question where I couldn't find the rank of a matrix. The question is : Problem on Finding the rank from a Matrix which has a variable At the time I believed in the answer, and now when I went back to that problem I realized something weird. How did this answer work. The answer was a continuation from where I have reduced the matrix by reduce row echleon form. And the answer continues as adding individual columns to another column and with scalar multiples of a column added to another (like how we do for rows in echleon form). I have never learnt anything other than row reduction. I don't know what this is called, but how is this valid? Common sense suggest me that it's not valid. I even tried a Google search on ""Reduced Column Echleon form"" and nothing came up. Can someone please tell what's going on ? And also what is the name given to this process?","I recently asked a question where I couldn't find the rank of a matrix. The question is : Problem on Finding the rank from a Matrix which has a variable At the time I believed in the answer, and now when I went back to that problem I realized something weird. How did this answer work. The answer was a continuation from where I have reduced the matrix by reduce row echleon form. And the answer continues as adding individual columns to another column and with scalar multiples of a column added to another (like how we do for rows in echleon form). I have never learnt anything other than row reduction. I don't know what this is called, but how is this valid? Common sense suggest me that it's not valid. I even tried a Google search on ""Reduced Column Echleon form"" and nothing came up. Can someone please tell what's going on ? And also what is the name given to this process?",,"['linear-algebra', 'matrix-equations']"
48,A determinant problem,A determinant problem,,"If $f(n)=\alpha^n+\beta^n$ and $$A=\left| \begin{array}{ccc} 3 & 1+f(1) & 1+f(2) \\ 1+f(1) & 1+f(2) & 1+f(3) \\ 1+f(2) & 1+f(3) & 1+f(4) \end{array} \right|$$ $=k(1-\alpha)^2(1-\beta)^2(\alpha-\beta)^2$ then $k=$  $a) 1\:\:\:$ $b)-1\:\:\:$ $c) \alpha\beta\:\:\:$ $d)\alpha\beta\gamma$ I have done the sum, but an answer isn't provided, so please see if I'm correct. $$A=\left| \begin{array}{ccc} 3 & 1+\alpha+\beta & 1+\alpha^2+\beta^2 \\ 1+\alpha+\beta & 1+\alpha^2+\beta^2 & 1+\alpha^3+\beta^3 \\ 1+\alpha^2+\beta^2 & 1+\alpha^3+\beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$$ $=\left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ 1 & 1 & 1+\alpha^3+\beta^3 \\ 1 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+\left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ 1 & \alpha^2 & 1+\alpha^3+\beta^3 \\ 1 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ 1 & \beta^2 & 1+\alpha^3+\beta^3 \\ 1 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ \alpha & 1 & 1+\alpha^3+\beta^3 \\ \alpha^2 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \alpha & \alpha^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \alpha & \beta^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ \beta & 1 & 1+\alpha^3+\beta^3 \\ \beta^2 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \beta & \alpha^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \beta & \beta^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ Now, $\left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \beta & \beta^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ $\left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ 1 & 1 & 1+\alpha^3+\beta^3 \\ 1 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ $ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \alpha & \alpha^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ Putting that and taking common, we ultimately get: $A= (\alpha\beta^2-\beta\alpha^2-\beta^2+\beta+\alpha^2-\alpha) \cdot B$ where $B= \left| \begin{array}{ccc} 1 & 1 & 1 \\ 1 & \alpha & \beta \\ 1 & \alpha^2 & \beta^2 \end{array} \right|$ $=-(1-\alpha)(1-\beta)(\alpha-\beta)$ So finally, factorizing, $A= (1-\alpha)^2 (1-\beta)^2 (\alpha-\beta)^2$ So, $k=1$ Am I correct?","If $f(n)=\alpha^n+\beta^n$ and $$A=\left| \begin{array}{ccc} 3 & 1+f(1) & 1+f(2) \\ 1+f(1) & 1+f(2) & 1+f(3) \\ 1+f(2) & 1+f(3) & 1+f(4) \end{array} \right|$$ $=k(1-\alpha)^2(1-\beta)^2(\alpha-\beta)^2$ then $k=$  $a) 1\:\:\:$ $b)-1\:\:\:$ $c) \alpha\beta\:\:\:$ $d)\alpha\beta\gamma$ I have done the sum, but an answer isn't provided, so please see if I'm correct. $$A=\left| \begin{array}{ccc} 3 & 1+\alpha+\beta & 1+\alpha^2+\beta^2 \\ 1+\alpha+\beta & 1+\alpha^2+\beta^2 & 1+\alpha^3+\beta^3 \\ 1+\alpha^2+\beta^2 & 1+\alpha^3+\beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$$ $=\left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ 1 & 1 & 1+\alpha^3+\beta^3 \\ 1 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+\left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ 1 & \alpha^2 & 1+\alpha^3+\beta^3 \\ 1 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ 1 & \beta^2 & 1+\alpha^3+\beta^3 \\ 1 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ \alpha & 1 & 1+\alpha^3+\beta^3 \\ \alpha^2 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \alpha & \alpha^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \alpha & \beta^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ \beta & 1 & 1+\alpha^3+\beta^3 \\ \beta^2 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \beta & \alpha^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ $+ \left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \beta & \beta^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|$ Now, $\left| \begin{array}{ccc} 1 & \beta & 1+\alpha^2+\beta^2 \\ \beta & \beta^2 & 1+\alpha^3+\beta^3 \\ \beta^2 & \beta^3 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ $\left| \begin{array}{ccc} 1 & 1 & 1+\alpha^2+\beta^2 \\ 1 & 1 & 1+\alpha^3+\beta^3 \\ 1 & 1 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ $ \left| \begin{array}{ccc} 1 & \alpha & 1+\alpha^2+\beta^2 \\ \alpha & \alpha^2 & 1+\alpha^3+\beta^3 \\ \alpha^2 & \alpha^3 & 1+\alpha^4+\beta^4 \end{array} \right|=0$ Putting that and taking common, we ultimately get: $A= (\alpha\beta^2-\beta\alpha^2-\beta^2+\beta+\alpha^2-\alpha) \cdot B$ where $B= \left| \begin{array}{ccc} 1 & 1 & 1 \\ 1 & \alpha & \beta \\ 1 & \alpha^2 & \beta^2 \end{array} \right|$ $=-(1-\alpha)(1-\beta)(\alpha-\beta)$ So finally, factorizing, $A= (1-\alpha)^2 (1-\beta)^2 (\alpha-\beta)^2$ So, $k=1$ Am I correct?",,"['linear-algebra', 'proof-verification', 'determinant']"
49,Power series for a matrix inverse,Power series for a matrix inverse,,Is there a power series expansion for a matrix inverse of the form $$\left(\frac{1}{m}I+A\right)^{-1} \mbox{ where $m$ is a scalar?}$$ $A$ is not invertible but the expression above is defined. I don't want to embed $m$ into the $A$ matrix as I want the result to have $m$ in it explicitly. Thanks!,Is there a power series expansion for a matrix inverse of the form $$\left(\frac{1}{m}I+A\right)^{-1} \mbox{ where $m$ is a scalar?}$$ $A$ is not invertible but the expression above is defined. I don't want to embed $m$ into the $A$ matrix as I want the result to have $m$ in it explicitly. Thanks!,,"['linear-algebra', 'matrices', 'inverse', 'matrix-calculus']"
50,How to prove that $\det(A)$ can be expressed as a $n \times n$ determinant with entries $\operatorname{tr}(A^k)$,How to prove that  can be expressed as a  determinant with entries,\det(A) n \times n \operatorname{tr}(A^k),Let $A$ be an $n\times n$ non-singular matrix with real entries. How can I prove the following equation? Any references would be helpful.  $$ \det(A) = \frac 1{n!} \left| \begin{array}{cccccc}\operatorname{tr}(A) & 1 & 0 & \cdots &  \cdots & 0 \\ \operatorname{tr}(A^2) & \operatorname{tr}(A) & 2 & 0 & \cdots & 0 \\ \operatorname{tr}(A^3) & \operatorname{tr}(A^2) & \operatorname{tr}(A) & 3 & & \vdots \\ \vdots & & & & & n-1 \\ \operatorname{tr}(A^n) & \operatorname{tr}(A^{n-1}) &  \operatorname{tr}(A^{n-2}) & \cdots & \cdots & \operatorname{tr}(A)            \end{array}\right|$$,Let $A$ be an $n\times n$ non-singular matrix with real entries. How can I prove the following equation? Any references would be helpful.  $$ \det(A) = \frac 1{n!} \left| \begin{array}{cccccc}\operatorname{tr}(A) & 1 & 0 & \cdots &  \cdots & 0 \\ \operatorname{tr}(A^2) & \operatorname{tr}(A) & 2 & 0 & \cdots & 0 \\ \operatorname{tr}(A^3) & \operatorname{tr}(A^2) & \operatorname{tr}(A) & 3 & & \vdots \\ \vdots & & & & & n-1 \\ \operatorname{tr}(A^n) & \operatorname{tr}(A^{n-1}) &  \operatorname{tr}(A^{n-2}) & \cdots & \cdots & \operatorname{tr}(A)            \end{array}\right|$$,,"['linear-algebra', 'matrices', 'determinant']"
51,"How do you prove that vectors are linearly independent in $ \mathcal{C}[0,1]$?",How do you prove that vectors are linearly independent in ?," \mathcal{C}[0,1]","I'm presented with the question: Show that the given vectors are linearly independent in $\mathcal{C}[0,1]$ : $x^{3/2}, x^{5/2}$ I'm having a terrible time understanding linearly algebra in general. I think my part of my problem with this question is understanding what the $\mathcal{C}[0,1]$ notation means. Beyond that, I'm still not exactly sure how to show this. Any help would be greatly appreciated.","I'm presented with the question: Show that the given vectors are linearly independent in : I'm having a terrible time understanding linearly algebra in general. I think my part of my problem with this question is understanding what the notation means. Beyond that, I'm still not exactly sure how to show this. Any help would be greatly appreciated.","\mathcal{C}[0,1] x^{3/2}, x^{5/2} \mathcal{C}[0,1]",['linear-algebra']
52,Irreducible representation of dimension $5$ of $S_5$,Irreducible representation of dimension  of,5 S_5,i am searching for a concrete as possible description of the (there are two but the are obtained from each other by tensoring with the signature representation) irreducible representation of dimension 5 of the symmetric group $S_5$ on 5 elements. [Fulton Harris] to my knowledge only computes the character. Wikipedia says it has something to do with an exceptional transitive embedding of $S_5$ into $S_6$. Would love if someone could tell me a bit more about that.,i am searching for a concrete as possible description of the (there are two but the are obtained from each other by tensoring with the signature representation) irreducible representation of dimension 5 of the symmetric group $S_5$ on 5 elements. [Fulton Harris] to my knowledge only computes the character. Wikipedia says it has something to do with an exceptional transitive embedding of $S_5$ into $S_6$. Would love if someone could tell me a bit more about that.,,"['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'symmetric-groups']"
53,orthogonal complement of symmetric matrices,orthogonal complement of symmetric matrices,,"How do I can prove that the orthogonal complement of space of symmetric matrices is the space of skew-symmetric matrices? With the inner product $\langle A,B\rangle = \mbox{tr}(A^TB)$. Thanks in advance.","How do I can prove that the orthogonal complement of space of symmetric matrices is the space of skew-symmetric matrices? With the inner product $\langle A,B\rangle = \mbox{tr}(A^TB)$. Thanks in advance.",,"['linear-algebra', 'functional-analysis']"
54,Null space of the sum of two matrices,Null space of the sum of two matrices,,"If we have two square matrices A and B then can we claim that $N(A + B)\supset N(A) $ and $N(A + B)\supset N(B)$,  respectively ? I have this doubt. Could anybody help me with this? I would be very much thankful to you.","If we have two square matrices A and B then can we claim that $N(A + B)\supset N(A) $ and $N(A + B)\supset N(B)$,  respectively ? I have this doubt. Could anybody help me with this? I would be very much thankful to you.",,"['linear-algebra', 'matrices']"
55,"Given 2 linear maps T,S while $T^2 = S^2$. Does it necessarily mean that T=S or T=-S?","Given 2 linear maps T,S while . Does it necessarily mean that T=S or T=-S?",T^2 = S^2,"I am given 2 linear maps: $T,S$, both from $V$ to $V$, satisfying  $T^2 = S^2$. $T,S \ne id$, and $T,S \ne 0$. The question given is: Does it necessarily mean that $T=S$ or $T=-S$? (or not both?). Prove! I think that this is not necessarily true, but I can't find a counterexample to support my claim. Any ideas? thanks.","I am given 2 linear maps: $T,S$, both from $V$ to $V$, satisfying  $T^2 = S^2$. $T,S \ne id$, and $T,S \ne 0$. The question given is: Does it necessarily mean that $T=S$ or $T=-S$? (or not both?). Prove! I think that this is not necessarily true, but I can't find a counterexample to support my claim. Any ideas? thanks.",,['linear-algebra']
56,Why doesn't small perturbations of a matrix decrease its rank?,Why doesn't small perturbations of a matrix decrease its rank?,,"I want to show that if $F:\mathbb{R^n}\to\mathbb{R^m}$ has a derivative of rank = r at a point $p$, then there is a neighbourhood of that point where the rank of the function doesn't decrease, i.e. $\exists N=B(p, \delta)$ such that $rk(DF_x) \geq rk(DF_p) ~ \forall x\in N$","I want to show that if $F:\mathbb{R^n}\to\mathbb{R^m}$ has a derivative of rank = r at a point $p$, then there is a neighbourhood of that point where the rank of the function doesn't decrease, i.e. $\exists N=B(p, \delta)$ such that $rk(DF_x) \geq rk(DF_p) ~ \forall x\in N$",,"['calculus', 'linear-algebra']"
57,The distance between orthogonal matrices induced by the Frobenius norm,The distance between orthogonal matrices induced by the Frobenius norm,,"An orthogonal matrix is a matrix $A$ over the reals such that $A^t=A^{-1}$ (its transpose is its inverse). The Frobenius norm over $n\times n$ real matrices is given by $\|A\| = \sqrt{trace(A^tA)}$. I have come across the following claim: The distance (induced by the Frobenius norm) between any two (non equal) orthogonal matrices is $\sqrt{n}$. I can't find a proof for this claim, but no refutation either (of course, if the difference between two orthogonal matrices is itself an orthogonal matrix the claim is clear, but I don't know if that's true either).","An orthogonal matrix is a matrix $A$ over the reals such that $A^t=A^{-1}$ (its transpose is its inverse). The Frobenius norm over $n\times n$ real matrices is given by $\|A\| = \sqrt{trace(A^tA)}$. I have come across the following claim: The distance (induced by the Frobenius norm) between any two (non equal) orthogonal matrices is $\sqrt{n}$. I can't find a proof for this claim, but no refutation either (of course, if the difference between two orthogonal matrices is itself an orthogonal matrix the claim is clear, but I don't know if that's true either).",,"['linear-algebra', 'matrices']"
58,Why does the sum of eigenvalues equal to trace in terms of linear transformations?,Why does the sum of eigenvalues equal to trace in terms of linear transformations?,,"While studying eigenvectors, I was confronted with two statements: The product of the eigenvalues of some matrix $A$ is equal to the determinant of $A$ The trace of $A$ is equal to the sum of its eigenvalues The thing is I am trying to understand every topic I learn in terms of linear transformations. For example, the first statement made sense to me because since the determinant is how much area scaled after a linear transformation and we are stretching $2$ vectors by their eigenvalues therefore determinant equals the product of eigenvalues. But when I try to understand the second statement I can not relate the trace of a matrix and its eigenvalues because I can't also understand what the trace of a matrix tells us about a linear transformation. I would love it if you can give me an intuitive explanation or let me know if there are topics that I haven't studied that prevent me from understanding this.","While studying eigenvectors, I was confronted with two statements: The product of the eigenvalues of some matrix is equal to the determinant of The trace of is equal to the sum of its eigenvalues The thing is I am trying to understand every topic I learn in terms of linear transformations. For example, the first statement made sense to me because since the determinant is how much area scaled after a linear transformation and we are stretching vectors by their eigenvalues therefore determinant equals the product of eigenvalues. But when I try to understand the second statement I can not relate the trace of a matrix and its eigenvalues because I can't also understand what the trace of a matrix tells us about a linear transformation. I would love it if you can give me an intuitive explanation or let me know if there are topics that I haven't studied that prevent me from understanding this.",A A A 2,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'trace']"
59,Is the magnitude of a component of a vector always less than its norm?,Is the magnitude of a component of a vector always less than its norm?,,"Let $x\in \mathbb R^n$ and $\| \cdot\|$ a norm defined on $\mathbb R^n$ . Is it true that $|x_i| \leq \| x\|,\forall i \in \{1, 2, \dots, n\}$ ? I know this easily proved for $\|\cdot \|_2$ or $\| \cdot\|_{\infty}$ , and that the statement may be true using the fact that norms are equivalent on $\mathbb R^n$ but haven't been able to show how.","Let and a norm defined on . Is it true that ? I know this easily proved for or , and that the statement may be true using the fact that norms are equivalent on but haven't been able to show how.","x\in \mathbb R^n \| \cdot\| \mathbb R^n |x_i| \leq \| x\|,\forall i \in \{1, 2, \dots, n\} \|\cdot \|_2 \| \cdot\|_{\infty} \mathbb R^n","['linear-algebra', 'inequality', 'normed-spaces']"
60,$T:V→V$ is a linear transformation such that $T\circ T(x)$ is invertible. Prove that $T$ is also invertible.,is a linear transformation such that  is invertible. Prove that  is also invertible.,T:V→V T\circ T(x) T,"Let $V$ be a vector space with dimension $n\in\mathbb{N}$ and $T:V→V$ a linear transformation such that $T\circ T(x)$ is invertible. Prove that $T$ is also invertible. I'm thinking to use the Theorem that states: If $T:V→W$ is an invertible linear transformation with inverse $T^{-1}:W→V$ , then $T^{-1}$ is a linear transformation. Any tips on how I should go about this problem?","Let be a vector space with dimension and a linear transformation such that is invertible. Prove that is also invertible. I'm thinking to use the Theorem that states: If is an invertible linear transformation with inverse , then is a linear transformation. Any tips on how I should go about this problem?",V n\in\mathbb{N} T:V→V T\circ T(x) T T:V→W T^{-1}:W→V T^{-1},"['linear-algebra', 'linear-transformations', 'inverse']"
61,Where Does the Hessian Matrix Come from (Why Does it Work)?,Where Does the Hessian Matrix Come from (Why Does it Work)?,,"Why does the Hessian matrix $$\left( {\begin{array}{cc} \frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y} \\ \frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial y^2} \\ \end{array} } \right)$$ work and where does it come from? I just recently came across this in a multivaraible calculus course. It was used to determine whether an extremum of a function with 2 variables is a maximum or minimum or ""saddle point"". Can anyone explain why it pops up here and how it helps understand the properties of an extremum?","Why does the Hessian matrix work and where does it come from? I just recently came across this in a multivaraible calculus course. It was used to determine whether an extremum of a function with 2 variables is a maximum or minimum or ""saddle point"". Can anyone explain why it pops up here and how it helps understand the properties of an extremum?","\left( {\begin{array}{cc}
\frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y} \\
\frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial y^2} \\
\end{array} } \right)","['linear-algebra', 'multivariable-calculus', 'hessian-matrix']"
62,$\mathrm{rank}(M)=\mathrm{rank}(M^2)$ whenever $M$ is skew-symmetric,whenever  is skew-symmetric,\mathrm{rank}(M)=\mathrm{rank}(M^2) M,"On p.231 of Linear Algebra by Greub, it is stated that a real skew-symmetric matrix has the same rank as its square, i.e., $\mathrm{rank}(M)=\mathrm{rank}(M^2)$ whenever $M$ is real skew-symmetric. I tried to use the fact that skew-symmetric matrix is normal and some geometric properties of normal matrices, but cannot proceed. Any help is appreciated.","On p.231 of Linear Algebra by Greub, it is stated that a real skew-symmetric matrix has the same rank as its square, i.e., whenever is real skew-symmetric. I tried to use the fact that skew-symmetric matrix is normal and some geometric properties of normal matrices, but cannot proceed. Any help is appreciated.",\mathrm{rank}(M)=\mathrm{rank}(M^2) M,"['linear-algebra', 'matrices', 'matrix-rank']"
63,Given $A^2 = A - I$ find $A^{15}$,Given  find,A^2 = A - I A^{15},"We should find matrix A such that $A^2 = A - I$ find $A^{15}$ . I solved this with observing the pattern with raising A to different powers up to 6 and I realized that $A^{15} = -I$ . However I'm not sure if this is the correct method, or if the result is correct.","We should find matrix A such that find . I solved this with observing the pattern with raising A to different powers up to 6 and I realized that . However I'm not sure if this is the correct method, or if the result is correct.",A^2 = A - I A^{15} A^{15} = -I,"['linear-algebra', 'matrices', 'matrix-equations']"
64,Solve this specific large sparse system of linear equations,Solve this specific large sparse system of linear equations,,"I want to solve the system $Ax = b$ where $A \in \mathbb R^{n \times n}$ and $b \in \mathbb R^n$ with $n \approx 10^6$ . If $A$ would be a fully dense matrix this would be hopeless of course but luckily the matrix is sparse and highly structured. The matrix $A$ can be written as \begin{align} A = \begin{pmatrix} B_0 & B_1 &     &     &     \\     & C_1 & B_2 &     &     \\     &     & C_2 & B_3 &     \\     &     &     & C_3 & B_4 \\ D   &     &     &     & C_4 \\ \end{pmatrix} \end{align} where $B_i$ are upper triangular matrices, $C_i$ are diagonal with each entry being equal to $-1$ , and $D$ is a square matrix. The figure below provides a schematic overview of $A$ where every possibly nonzero entry is green and all zeroes are gray. The upper triangular matrices $B_i$ have approximately the same structure but do not necessarily have the same entry values. The submatrices $B_i$ , $C_i$ , and $D$ typically have dimension $m\times m$ with $m \approx 10^5$ . I don't know whether it is useful, but we also know for all non-zeroes with $i \neq j$ that $a_{ij} \in (0, 1]$ and the entries on the diagonal equal $-1$ except for the part that overlaps with $B_0$ , i.e., we have $C_i = -I$ . Note that the corresponding system $(A + I)y = b$ , where $I$ is an identity matrix, is relatively easy to solve as the submatrices $C_i$ become null matrices. Then we can first solve $D \hat x = \hat b$ where $\hat x$ and $\hat b$ are the last entries of $x$ and $b$ . Next, we can iteratively solve the triangular matrices one by one starting with the one closest to the bottom. Maybe we can use the solution $y$ (or a transformation of $y$ ) as an initial solution for $x$ for an iterative approach. Is there a better way to solve this system compared to naively feeding the system to a solver such as Matlab or LAPACK? EDIT 1: Green entries indicate a possibly positive value. However, for the square at the bottom, around half of the entries are expected to be zero. EDIT 2: Given the enormous size of this problem, approximate methods are also more than welcome.","I want to solve the system where and with . If would be a fully dense matrix this would be hopeless of course but luckily the matrix is sparse and highly structured. The matrix can be written as where are upper triangular matrices, are diagonal with each entry being equal to , and is a square matrix. The figure below provides a schematic overview of where every possibly nonzero entry is green and all zeroes are gray. The upper triangular matrices have approximately the same structure but do not necessarily have the same entry values. The submatrices , , and typically have dimension with . I don't know whether it is useful, but we also know for all non-zeroes with that and the entries on the diagonal equal except for the part that overlaps with , i.e., we have . Note that the corresponding system , where is an identity matrix, is relatively easy to solve as the submatrices become null matrices. Then we can first solve where and are the last entries of and . Next, we can iteratively solve the triangular matrices one by one starting with the one closest to the bottom. Maybe we can use the solution (or a transformation of ) as an initial solution for for an iterative approach. Is there a better way to solve this system compared to naively feeding the system to a solver such as Matlab or LAPACK? EDIT 1: Green entries indicate a possibly positive value. However, for the square at the bottom, around half of the entries are expected to be zero. EDIT 2: Given the enormous size of this problem, approximate methods are also more than welcome.","Ax = b A \in \mathbb R^{n \times n} b \in \mathbb R^n n \approx 10^6 A A \begin{align}
A = \begin{pmatrix}
B_0 & B_1 &     &     &     \\
    & C_1 & B_2 &     &     \\
    &     & C_2 & B_3 &     \\
    &     &     & C_3 & B_4 \\
D   &     &     &     & C_4 \\
\end{pmatrix}
\end{align} B_i C_i -1 D A B_i B_i C_i D m\times m m \approx 10^5 i \neq j a_{ij} \in (0, 1] -1 B_0 C_i = -I (A + I)y = b I C_i D \hat x = \hat b \hat x \hat b x b y y x","['linear-algebra', 'matrices', 'systems-of-equations', 'block-matrices', 'sparse-matrices']"
65,Calculating area of two vectors. (problems with getting calculations correct),Calculating area of two vectors. (problems with getting calculations correct),,"We have two vectors $u$ and $v$: $$ u=-3i+5j+2k  $$ $$ v=4i+3j-3k  $$ Cross product $u\times v$ gives us: $$u \times v =\begin{vmatrix}i & j & k \\ -3 & 5 & 2 \\ 4 & 3 & -3 \end{vmatrix} \\ =(5\times(-3)-2\times3)i \\ -(-3\times(-3)-4\times2)j \\ +(-3\times3-4\times5)k \\ u \times v =-21i-j-29k$$ these calculations on wolframalpha Now according to wikipedia area of two vectors $u$ and $v$ is $|u \times v|$ ($a$ and $b$ in this example) So i can think of at least two good ways to determine area formed by vectors $u$ and $v$. These are: $$ area_a=|u|\times|v| $$ $$ area_b=|u \times v|  $$ Now when i try to calculate area with these two methods. I get slightly different answers and i don't know why i would get two different areas ? Calculated areas are: $$ area_a = \sqrt{(-3)^2+(5)^2+(2)^2}\sqrt{(4)^2+(3)^2+(-3)^2}=2\sqrt{323}\approx35.95$$ $$ area_b = \sqrt{(-21)^2+(-1)^2+(-29)^2}=\sqrt{1283}\approx 35.82  $$ both areas should give same result? $$ area_a=area_b$$ But in this these are different. Now if someone could point out what I am missing in these calculations or calculating wrong that would be greatly appreciated. Thanks, Tuki","We have two vectors $u$ and $v$: $$ u=-3i+5j+2k  $$ $$ v=4i+3j-3k  $$ Cross product $u\times v$ gives us: $$u \times v =\begin{vmatrix}i & j & k \\ -3 & 5 & 2 \\ 4 & 3 & -3 \end{vmatrix} \\ =(5\times(-3)-2\times3)i \\ -(-3\times(-3)-4\times2)j \\ +(-3\times3-4\times5)k \\ u \times v =-21i-j-29k$$ these calculations on wolframalpha Now according to wikipedia area of two vectors $u$ and $v$ is $|u \times v|$ ($a$ and $b$ in this example) So i can think of at least two good ways to determine area formed by vectors $u$ and $v$. These are: $$ area_a=|u|\times|v| $$ $$ area_b=|u \times v|  $$ Now when i try to calculate area with these two methods. I get slightly different answers and i don't know why i would get two different areas ? Calculated areas are: $$ area_a = \sqrt{(-3)^2+(5)^2+(2)^2}\sqrt{(4)^2+(3)^2+(-3)^2}=2\sqrt{323}\approx35.95$$ $$ area_b = \sqrt{(-21)^2+(-1)^2+(-29)^2}=\sqrt{1283}\approx 35.82  $$ both areas should give same result? $$ area_a=area_b$$ But in this these are different. Now if someone could point out what I am missing in these calculations or calculating wrong that would be greatly appreciated. Thanks, Tuki",,"['linear-algebra', 'area']"
66,Similarity transform of a matrix preserves the determinant?,Similarity transform of a matrix preserves the determinant?,,"It is posited (in chapter $11$ of the book Numerical Recipes in C ) that doing a similarity transform of a matrix $A$ preserves its eigenvalues. This is how a similarity transform is defined: $$A \mapsto Z^{-1}AZ$$ for some transformation matrix $Z$ whose determinant is one. The proof given uses the characteristic equation. $$|Z^{-1} A Z - \lambda I| = |Z^{-1}(A-\lambda I)Z| $$ I don't understand how this comes about. Once we accept this, the result follows quite easily: $$= |Z||A-\lambda I||Z^{-1}| = |A-\lambda I|.$$","It is posited (in chapter $11$ of the book Numerical Recipes in C ) that doing a similarity transform of a matrix $A$ preserves its eigenvalues. This is how a similarity transform is defined: $$A \mapsto Z^{-1}AZ$$ for some transformation matrix $Z$ whose determinant is one. The proof given uses the characteristic equation. $$|Z^{-1} A Z - \lambda I| = |Z^{-1}(A-\lambda I)Z| $$ I don't understand how this comes about. Once we accept this, the result follows quite easily: $$= |Z||A-\lambda I||Z^{-1}| = |A-\lambda I|.$$",,"['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'proof-explanation']"
67,Given $a^2+b^2+c^2 = 1$ what type is this matrix?,Given  what type is this matrix?,a^2+b^2+c^2 = 1,"With $ a^2 + b^2 + c^2 = 1$, what type of matrix is $A$? $A = \begin{bmatrix} 0  & a  & -b \\ -a & 0  & c \\ b  & -c & 0  \end{bmatrix}$ So far I've tested $A$ for several types, I know that $A$ is non-orthogonal, obviously skew-symmetric and singular, as it's determinant equals $0$ and there is no chance to get an inverse. But nothing is related to the $ a^2 + b^2 + c^2 = 1$ condition, any ideas?","With $ a^2 + b^2 + c^2 = 1$, what type of matrix is $A$? $A = \begin{bmatrix} 0  & a  & -b \\ -a & 0  & c \\ b  & -c & 0  \end{bmatrix}$ So far I've tested $A$ for several types, I know that $A$ is non-orthogonal, obviously skew-symmetric and singular, as it's determinant equals $0$ and there is no chance to get an inverse. But nothing is related to the $ a^2 + b^2 + c^2 = 1$ condition, any ideas?",,"['linear-algebra', 'matrices', 'matrix-calculus', 'robotics']"
68,"Given a $2\times 2$ matrix $A$, compute $A^7$.","Given a  matrix , compute .",2\times 2 A A^7,"Let $A =         \begin{pmatrix}         e^{2x} & -1  \\         0 & e^{2x}-1  \\         \end{pmatrix} $. Compute $A^7$. I've tried the obvious way of multiplying $A$ with $A$, then $A^2$ with $A^2$, but I arrived at a messy result in the top right member of the matrix. Is there a general form to be noticed here?","Let $A =         \begin{pmatrix}         e^{2x} & -1  \\         0 & e^{2x}-1  \\         \end{pmatrix} $. Compute $A^7$. I've tried the obvious way of multiplying $A$ with $A$, then $A^2$ with $A^2$, but I arrived at a messy result in the top right member of the matrix. Is there a general form to be noticed here?",,"['linear-algebra', 'matrices']"
69,How to prove the positive-definiteness of this matrix?,How to prove the positive-definiteness of this matrix?,,"I have this matrix $$ C= \left( \begin{array}{c|c} A+\alpha I_m & -A-\alpha I_m \\ \hline -A-\alpha I_m & A+\alpha I_m \end{array} \right)\\ A= \left( \begin{array}{cccc} (y_1,y_1) &(y_2,y_1)& \cdots&(y_m,y_1)   \\ (y_1,y_2) &(y_2,y_2)&\cdots&(y_m,y_2) \\ \vdots  & \vdots  & \ddots & \vdots  \\ (y_1,y_m) &(y_2,y_m)&\cdots&(y_m,y_m) \\ \end{array} \right) $$   where $y_i\in L^2(\Omega)$ for all $i=1,\dots,m$ ,  $\alpha\geq 0$ and  $(\cdot,\cdot)$ denote the inner product in  $L^2(\Omega)$ with $\Omega\subset\mathbb{R}^2$. I would like to prove this matrix is Positive-definite, any suggestions?","I have this matrix $$ C= \left( \begin{array}{c|c} A+\alpha I_m & -A-\alpha I_m \\ \hline -A-\alpha I_m & A+\alpha I_m \end{array} \right)\\ A= \left( \begin{array}{cccc} (y_1,y_1) &(y_2,y_1)& \cdots&(y_m,y_1)   \\ (y_1,y_2) &(y_2,y_2)&\cdots&(y_m,y_2) \\ \vdots  & \vdots  & \ddots & \vdots  \\ (y_1,y_m) &(y_2,y_m)&\cdots&(y_m,y_m) \\ \end{array} \right) $$   where $y_i\in L^2(\Omega)$ for all $i=1,\dots,m$ ,  $\alpha\geq 0$ and  $(\cdot,\cdot)$ denote the inner product in  $L^2(\Omega)$ with $\Omega\subset\mathbb{R}^2$. I would like to prove this matrix is Positive-definite, any suggestions?",,"['linear-algebra', 'analysis', 'positive-definite']"
70,Special Orthogonal Group $SO(2)$,Special Orthogonal Group,SO(2),The special orthogonal group for $n=2$ is defined as: $$SO(2)=\big\{A\in O(2):\det A=1\big\}$$ I am trying to prove that if $A\in SO(2)$ then: $$A=\left(\begin{array}{cc} \cos\theta& -\sin\theta\\ \sin\theta&\cos\theta \end{array}\right)$$ My idea is show that $\Phi:S^1\to SO(2)$ defined as: $$z=e^{\theta i}\mapsto \Phi(z)=\left(\begin{array}{cc} \cos\theta& -\sin\theta\\ \sin\theta&\cos\theta \end{array}\right)$$ is an isomorphism of Lie groups. It is easy prove that is an monomorphism of Lie groups. How can I prove that is also surjective?,The special orthogonal group for $n=2$ is defined as: $$SO(2)=\big\{A\in O(2):\det A=1\big\}$$ I am trying to prove that if $A\in SO(2)$ then: $$A=\left(\begin{array}{cc} \cos\theta& -\sin\theta\\ \sin\theta&\cos\theta \end{array}\right)$$ My idea is show that $\Phi:S^1\to SO(2)$ defined as: $$z=e^{\theta i}\mapsto \Phi(z)=\left(\begin{array}{cc} \cos\theta& -\sin\theta\\ \sin\theta&\cos\theta \end{array}\right)$$ is an isomorphism of Lie groups. It is easy prove that is an monomorphism of Lie groups. How can I prove that is also surjective?,,"['linear-algebra', 'differential-geometry', 'lie-groups', 'orthogonal-matrices']"
71,How to prove that trace$(ABA^{-1}B^{-1})$=$3$,How to prove that trace=,(ABA^{-1}B^{-1}) 3,"If $A,B$ are two $3 \times 3$ square matrices and trace(A) is defined as the sum of all diagonal elements. trace$(ABA^{-1}B^{-1})$=$3$ I could easily verify the above for the identity matrix.But I couldn't generalise it. Please help me in this regard.thanks.","If $A,B$ are two $3 \times 3$ square matrices and trace(A) is defined as the sum of all diagonal elements. trace$(ABA^{-1}B^{-1})$=$3$ I could easily verify the above for the identity matrix.But I couldn't generalise it. Please help me in this regard.thanks.",,"['linear-algebra', 'matrices']"
72,Antisymmetric vs. alternating $k$-linear forms and wedge-product,Antisymmetric vs. alternating -linear forms and wedge-product,k,"In my linear algebra course we derived determinants via alternating $k$-linear forms $f: V^k \to K$. These alternating linear forms form a vector space $\Lambda^k(V^*)$. During this derivation we defined the wedgeproduct $$a \wedge b := \frac{k + l}{k!l!}\text{Alt}(a \otimes b)$$ for $a \in \Lambda^k(V^*)$ and $b\in \Lambda^l(V^*)$. Further $\text{Alt}(f)$ is an operator which produces an alternating linear form given a (non-alternating) linear form $f$. My notes say that the wedge product is not necessarily alternating. Moreover, $(a \wedge b) = (-1)^{kl}(b \wedge a)$. Why is that correct? $\text{Alt}(\cdot)$ scaled by a scalar should be alternating by default! In class we did not talk about exterior algebras at all but only used the notions above to derive determinants. This wikipedia mentions anti-symmetric maps ( here ). What exactly is that? I only know about anti-symmetric matrices . What is the difference between anti-symmetric and alternating maps? Is it true that an alterating bilinear form $b(x,y) = -b(y,x)$ can always be represented by an anti-symmetric matrix? I proved the converse (anti-symmetric matrix always induces an alternating bilinear form) but cannot find an angle to prove this direction. Thanks!","In my linear algebra course we derived determinants via alternating $k$-linear forms $f: V^k \to K$. These alternating linear forms form a vector space $\Lambda^k(V^*)$. During this derivation we defined the wedgeproduct $$a \wedge b := \frac{k + l}{k!l!}\text{Alt}(a \otimes b)$$ for $a \in \Lambda^k(V^*)$ and $b\in \Lambda^l(V^*)$. Further $\text{Alt}(f)$ is an operator which produces an alternating linear form given a (non-alternating) linear form $f$. My notes say that the wedge product is not necessarily alternating. Moreover, $(a \wedge b) = (-1)^{kl}(b \wedge a)$. Why is that correct? $\text{Alt}(\cdot)$ scaled by a scalar should be alternating by default! In class we did not talk about exterior algebras at all but only used the notions above to derive determinants. This wikipedia mentions anti-symmetric maps ( here ). What exactly is that? I only know about anti-symmetric matrices . What is the difference between anti-symmetric and alternating maps? Is it true that an alterating bilinear form $b(x,y) = -b(y,x)$ can always be represented by an anti-symmetric matrix? I proved the converse (anti-symmetric matrix always induces an alternating bilinear form) but cannot find an angle to prove this direction. Thanks!",,"['linear-algebra', 'determinant', 'multilinear-algebra', 'exterior-algebra']"
73,Can the magnitude of a vector sum ever equal the sum of the magnitudes?,Can the magnitude of a vector sum ever equal the sum of the magnitudes?,,"I'm currently taking a university course in Linear Algebra and Matrix Theory. A recent problem set included a question that asked, What can you say about two nonzero vectors $\vec{\alpha}$ and $\vec{\beta}$ that satisfy the equation:   $$\|\vec{\alpha}+\vec{\beta}\| \ = \ \|\vec{\alpha}\| + \|\vec{\beta}\| \  $$   $$\vec{\alpha},\vec{\beta} \in \mathbb{R}^n$$ I am attempting to solve this by finding a solution from this equation derived from the law of cosines: $$\|\vec{\alpha}+\vec{\beta}\|^2 \ = \ \|\vec{\alpha}\|^2 + \|\vec{\beta}\|^2 - \ 2\|\vec{\alpha}\| \|\vec{\beta}\|\cos(\pi-\theta)$$ ...so far I have been unable to find a valid solution and am tempted to assert that there exists no $\vec{\alpha}$ and $\vec{\beta}$ for which that equation is true. Is there any case in which the magnitude of the sum of two vectors equals the sum of the magnitudes?","I'm currently taking a university course in Linear Algebra and Matrix Theory. A recent problem set included a question that asked, What can you say about two nonzero vectors $\vec{\alpha}$ and $\vec{\beta}$ that satisfy the equation:   $$\|\vec{\alpha}+\vec{\beta}\| \ = \ \|\vec{\alpha}\| + \|\vec{\beta}\| \  $$   $$\vec{\alpha},\vec{\beta} \in \mathbb{R}^n$$ I am attempting to solve this by finding a solution from this equation derived from the law of cosines: $$\|\vec{\alpha}+\vec{\beta}\|^2 \ = \ \|\vec{\alpha}\|^2 + \|\vec{\beta}\|^2 - \ 2\|\vec{\alpha}\| \|\vec{\beta}\|\cos(\pi-\theta)$$ ...so far I have been unable to find a valid solution and am tempted to assert that there exists no $\vec{\alpha}$ and $\vec{\beta}$ for which that equation is true. Is there any case in which the magnitude of the sum of two vectors equals the sum of the magnitudes?",,"['linear-algebra', 'vectors']"
74,Sylow Conjugation Theorem is False,Sylow Conjugation Theorem is False,,"We have the following theorem: Theorem. Let $p$ be a prime and $G$ be a finite group whose order is divisible by $p$. Then the number of Sylow-$p$ subgroups of $G$ is congruent to $1$ modulo $p$. We are going to find a counterexample to this theorem. Let $G=SL_2(F_3)$, where $F_3$ denotes the finite field of order $3$. Then $|G|=24$. Let $n_3$ denote the number of Sylow-$3$ subgroups of $G$. Then the number of elements of order $3$ in $G$ is $2n_3$. Let $M\in G$ be a matrix of order $3$. Then $M$ satisfies the polynomial $x^3-1=(x-1)^3$. Thus the minimal polynomial of $M$ is either $x-1$, or it is $(x-1)^2=x^2+x+1$. But the minimal polynomial cannot be $x-1$ since the order of $M$ is $3$. So the minimal polynomial of $M$ has to be $x^2+x+1$. Since the characteristic polynomial of $M$ has degree $2$, we see that the minimal and the characteristic polynomial of $M$ coincide, and thus $M$ is a cyclic. This means that there is a vector $v\in F_3^2$ such that $\{v, Mv\}$ is a basis of $F_3^2$. So $M$ is similar to the matrix $N:=\begin{bmatrix} 0 & -1 \\ 1 & -1\end{bmatrix}$. What we have shown is that all matrices of order $3$ in $G$ are similar to $N$. It is clear that all matrices similar to $N$ are of order $3$. So the set of all the elements of order $3$ in $G$ is same as the conjugacy class of $N$. So now we set out to find the size of the conjugacy class of $N$. This is same as the index of the centralizer $C_G(N)$ of $N$. Let $A=\begin{bmatrix} a & b\\ c & d\end{bmatrix}$ be in $C_G(N)$. Then using $ANA^{-1}=N$, we have $b=-c$ and $d=a+b$, and using $\det(A)=1$ we have $a^2+ab+b^2=1$. The solutions to this are $(a, b)=(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)$. So $|C_G(N)|=6$. Therefore $\text{conj}(N)=|G:C_G(N)|=4$. So we have shown that there are $4$ elements of order $3$ in $G$, which yields there are $2$ Sylow-$3$ subgroups in $G$. This contradicts the theorem quoted in the beginning of the theorem. Can somebody please point out my mistake? I know Sylow theorems are not wrong :)","We have the following theorem: Theorem. Let $p$ be a prime and $G$ be a finite group whose order is divisible by $p$. Then the number of Sylow-$p$ subgroups of $G$ is congruent to $1$ modulo $p$. We are going to find a counterexample to this theorem. Let $G=SL_2(F_3)$, where $F_3$ denotes the finite field of order $3$. Then $|G|=24$. Let $n_3$ denote the number of Sylow-$3$ subgroups of $G$. Then the number of elements of order $3$ in $G$ is $2n_3$. Let $M\in G$ be a matrix of order $3$. Then $M$ satisfies the polynomial $x^3-1=(x-1)^3$. Thus the minimal polynomial of $M$ is either $x-1$, or it is $(x-1)^2=x^2+x+1$. But the minimal polynomial cannot be $x-1$ since the order of $M$ is $3$. So the minimal polynomial of $M$ has to be $x^2+x+1$. Since the characteristic polynomial of $M$ has degree $2$, we see that the minimal and the characteristic polynomial of $M$ coincide, and thus $M$ is a cyclic. This means that there is a vector $v\in F_3^2$ such that $\{v, Mv\}$ is a basis of $F_3^2$. So $M$ is similar to the matrix $N:=\begin{bmatrix} 0 & -1 \\ 1 & -1\end{bmatrix}$. What we have shown is that all matrices of order $3$ in $G$ are similar to $N$. It is clear that all matrices similar to $N$ are of order $3$. So the set of all the elements of order $3$ in $G$ is same as the conjugacy class of $N$. So now we set out to find the size of the conjugacy class of $N$. This is same as the index of the centralizer $C_G(N)$ of $N$. Let $A=\begin{bmatrix} a & b\\ c & d\end{bmatrix}$ be in $C_G(N)$. Then using $ANA^{-1}=N$, we have $b=-c$ and $d=a+b$, and using $\det(A)=1$ we have $a^2+ab+b^2=1$. The solutions to this are $(a, b)=(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)$. So $|C_G(N)|=6$. Therefore $\text{conj}(N)=|G:C_G(N)|=4$. So we have shown that there are $4$ elements of order $3$ in $G$, which yields there are $2$ Sylow-$3$ subgroups in $G$. This contradicts the theorem quoted in the beginning of the theorem. Can somebody please point out my mistake? I know Sylow theorems are not wrong :)",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'sylow-theory']"
75,Doubt with vectorial spaces (Basis and dimension),Doubt with vectorial spaces (Basis and dimension),,"Good night, i'm working in a problem, i need an basis and the dimension of the space. $a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ I make this: $\left[ \begin {array}{cccc} 1&0&0&-1\\ 2&1&1&0 \\ 1&1&1&1\\ 1&2&3&4 \\ 0&1&2&3\end {array} \right]$ and i apply gauss for reduce the matrix: $ \left[ \begin {array}{cccc} 1&0&0&-1\\ 0&1&0&1 \\ 0&0&1&1\\ 0&0&0&0 \\ 0&0&0&0\end {array} \right]$ now, i have 3 linearly independent vectors and my dimensions is 3. but, i can take any vector and they go linearly independent?  for example: $a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1)$ or $a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ and going to be linearly independent? thanks!!","Good night, i'm working in a problem, i need an basis and the dimension of the space. $a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ I make this: $\left[ \begin {array}{cccc} 1&0&0&-1\\ 2&1&1&0 \\ 1&1&1&1\\ 1&2&3&4 \\ 0&1&2&3\end {array} \right]$ and i apply gauss for reduce the matrix: $ \left[ \begin {array}{cccc} 1&0&0&-1\\ 0&1&0&1 \\ 0&0&1&1\\ 0&0&0&0 \\ 0&0&0&0\end {array} \right]$ now, i have 3 linearly independent vectors and my dimensions is 3. but, i can take any vector and they go linearly independent?  for example: $a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1)$ or $a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ and going to be linearly independent? thanks!!",,"['linear-algebra', 'matrices', 'vector-spaces']"
76,Is the matrix $A$ positive (negative) (semi-) definite?,Is the matrix  positive (negative) (semi-) definite?,A,"Given, $$A = \begin{bmatrix} 2 &-1  & -1\\   -1&2  & -1\\   -1&  -1& 2 \end{bmatrix}.$$ I want to see if the matrix $A$ positive (negative)  (semi-) definite. Define the quadratic form as $Q(x)=x'Ax$. Let $x \in \mathbb{R}^{3}$, with $x \neq 0$. So, $Q(x)=x'Ax = \begin{bmatrix} x_{1} &x_{2}  &x_{3}  \end{bmatrix} \begin{bmatrix} 2 &-1  & -1\\   -1&2  & -1\\   -1&  -1& 2 \end{bmatrix} \begin{bmatrix} x_{1}\\x_{2}  \\x_{3}  \end{bmatrix}$. After multiplying out the matrices I am left with $$Q(x) = 2(x_{1}^{2}+x_{2}^{2}+x_{3}^{2}-x_{1}x_{2} - x_{1}x_{3}-x_{2}x_{3}).$$ Not sure what I can do with this result. Any suggestions on how to proceed would be appreciated.","Given, $$A = \begin{bmatrix} 2 &-1  & -1\\   -1&2  & -1\\   -1&  -1& 2 \end{bmatrix}.$$ I want to see if the matrix $A$ positive (negative)  (semi-) definite. Define the quadratic form as $Q(x)=x'Ax$. Let $x \in \mathbb{R}^{3}$, with $x \neq 0$. So, $Q(x)=x'Ax = \begin{bmatrix} x_{1} &x_{2}  &x_{3}  \end{bmatrix} \begin{bmatrix} 2 &-1  & -1\\   -1&2  & -1\\   -1&  -1& 2 \end{bmatrix} \begin{bmatrix} x_{1}\\x_{2}  \\x_{3}  \end{bmatrix}$. After multiplying out the matrices I am left with $$Q(x) = 2(x_{1}^{2}+x_{2}^{2}+x_{3}^{2}-x_{1}x_{2} - x_{1}x_{3}-x_{2}x_{3}).$$ Not sure what I can do with this result. Any suggestions on how to proceed would be appreciated.",,"['linear-algebra', 'matrices', 'determinant', 'quadratic-forms']"
77,Unusual result to the addition,Unusual result to the addition,,"Question: Prove that (666... to n digits)^2 + (888... to n digits)=(444... to 2n digits) My way: I just proved the given equation for three values of n and written at the bottom. ""Since the equation satisfies for n=1, 2, and 3, the equation is true and hence proved."" Also I am seeing a regular pattern in (6666...to n digits)^2 $666^2=443556,6666^2=44435556,66666^2=4444355556$ So the pattern is first there are (n-1) $4's$ the one $3$ then (n-1) 5's the one 6. Is there a general way to solve this question?","Question: Prove that (666... to n digits)^2 + (888... to n digits)=(444... to 2n digits) My way: I just proved the given equation for three values of n and written at the bottom. ""Since the equation satisfies for n=1, 2, and 3, the equation is true and hence proved."" Also I am seeing a regular pattern in (6666...to n digits)^2 So the pattern is first there are (n-1) the one then (n-1) 5's the one 6. Is there a general way to solve this question?","666^2=443556,6666^2=44435556,66666^2=4444355556 4's 3","['linear-algebra', 'arithmetic']"
78,What is Homogeneous Coordinates? Why is it necessary in 2D transformation?,What is Homogeneous Coordinates? Why is it necessary in 2D transformation?,,"What is Homogeneous Coordinates? Why is it necessary in 2D transformation of objects in computer graphics? The concept of homogeneous coordinates in effect converts the 2D system a 3D one. So, why don't we just use a 3D system instead?","What is Homogeneous Coordinates? Why is it necessary in 2D transformation of objects in computer graphics? The concept of homogeneous coordinates in effect converts the 2D system a 3D one. So, why don't we just use a 3D system instead?",,['linear-algebra']
79,Show that every subspace of $\mathbb{R}^n$ is closed [closed],Show that every subspace of  is closed [closed],\mathbb{R}^n,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Show that every subspace of $\mathbb{R^n}$ is closed. I'm not sure how to do this or even what closed means. I don't even have a starting point. Any hints or solutions are greatly appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Show that every subspace of $\mathbb{R^n}$ is closed. I'm not sure how to do this or even what closed means. I don't even have a starting point. Any hints or solutions are greatly appreciated.",,"['real-analysis', 'linear-algebra', 'general-topology', 'analysis']"
80,"Give an example of a singular matrix in $M_{3×3}(Q)$ the entries of which are distinct prime positive integers, or show that no such matrix can exist.","Give an example of a singular matrix in  the entries of which are distinct prime positive integers, or show that no such matrix can exist.",M_{3×3}(Q),"I know that the matrix exist because the entries are primes but I don´t know how to explain, i need some help. Give an example of a singular matrix in $M_{3×3}(Q)$ the entries of which are distinct prime positive integers, or show that no such matrix can exist.","I know that the matrix exist because the entries are primes but I don´t know how to explain, i need some help. Give an example of a singular matrix in $M_{3×3}(Q)$ the entries of which are distinct prime positive integers, or show that no such matrix can exist.",,"['linear-algebra', 'matrices']"
81,Product of any two arbitrary positive definite matrices is positive definite or NOT? [duplicate],Product of any two arbitrary positive definite matrices is positive definite or NOT? [duplicate],,"This question already has an answer here : Positive definiteness of the matrix $A+B$ (1 answer) Closed 9 years ago . Suppose that , $A$ and $B$ are $n\times n$ positive definite matrices and > $I$ be $n\times n$ identity matrix. Then which of the followings are positive definite ? (i) $A+B$ (ii) $ABA$ (iii) $A^2+I$ (iv) $AB$ I know the definition of positive definite as : $\color{red}{A_{n\times n}}$ $\color{red}{\text{is positive definite if it's quadratic form}} $ $\color{red}{x^TAx>0}$ Since $A$ and $B$ are positive definite so, $x^TAx>0$ and $x^TBx>0$ . Then, $x^T(A+B)x=x^TAx+x^TBx>0.$ So $A+B$ is positive definite. I am confused about the product.. I saw a lot of questions in this site about the product of positive definiteness. But the answer in those questions it is assume that the matrices are symmetric . For example see the answer of this question. I want to know whether the product of any two arbitrary positive definite matrices is positive definite or NOT with a valid proof or counter example....","This question already has an answer here : Positive definiteness of the matrix $A+B$ (1 answer) Closed 9 years ago . Suppose that , and are positive definite matrices and > be identity matrix. Then which of the followings are positive definite ? (i) (ii) (iii) (iv) I know the definition of positive definite as : Since and are positive definite so, and . Then, So is positive definite. I am confused about the product.. I saw a lot of questions in this site about the product of positive definiteness. But the answer in those questions it is assume that the matrices are symmetric . For example see the answer of this question. I want to know whether the product of any two arbitrary positive definite matrices is positive definite or NOT with a valid proof or counter example....",A B n\times n I n\times n A+B ABA A^2+I AB \color{red}{A_{n\times n}} \color{red}{\text{is positive definite if it's quadratic form}}  \color{red}{x^TAx>0} A B x^TAx>0 x^TBx>0 x^T(A+B)x=x^TAx+x^TBx>0. A+B,"['linear-algebra', 'matrices', 'positive-definite']"
82,Polynomial: Is there a theorem that can save my proof when $K$ doesn't include $\mathbb C$,Polynomial: Is there a theorem that can save my proof when  doesn't include,K \mathbb C,"Suppose $f(x),g(x)\in K[x]$ ($K$ a number field), let $f(x)=x^{3m}+x^{3n+1}+x^{3p+2}$, where $m,n,p\in\mathbb N$, and let $g(x)=x^2+x+1$, prove:   $$g(x)\mid f(x)$$ I think this problem is not very easy unless I specify $K$ to be $\mathbb C$. If in $\mathbb C$, then it is plain to see  $$g(x)=(x-\omega_1)(x-\omega_2)$$ where $\omega_{1}=e^{i2\pi/3}$ and $\omega_{2}=e^{-i2\pi/3}$. And it's also obvious after simple calculation, that $$f(\omega_1)=f(\omega_2)=0$$ By the Factor Theorem, since $\omega_{1,2}$ are distinct, $$(x-\omega_1)(x-\omega_2)\mid f(x)$$ i.e. $$g(x)\mid f(x)$$ Nevertheless , all I have done so far is based on the assumption that $K$ is (or includes) $\mathbb C$, without which I think it will be ridiculous to write something like $(x-\omega_{1,2})$ as polynomials. However, I feel strongly that my approach, although based on a not quite reasonable assumption, is in fact a quick-and-easy way, whatever $K$ be. So, maybe there is a theorem unknown to me which can justify my method when $K$ doesn't include $\mathbb C$, say, $K=\mathbb R$? Of course, I know I am not allowed to factorize like this when $K=R$, but $g(x)\mid f(x)$ is still alright. And I think $g(x)\mid f(x)$ always holds whatever $K$ be, even though the factorization is not always true. So am I wrong in not giving up my proof? Or is there really a god-sent theorem that will save my proof?  Need some help. Best regards here.","Suppose $f(x),g(x)\in K[x]$ ($K$ a number field), let $f(x)=x^{3m}+x^{3n+1}+x^{3p+2}$, where $m,n,p\in\mathbb N$, and let $g(x)=x^2+x+1$, prove:   $$g(x)\mid f(x)$$ I think this problem is not very easy unless I specify $K$ to be $\mathbb C$. If in $\mathbb C$, then it is plain to see  $$g(x)=(x-\omega_1)(x-\omega_2)$$ where $\omega_{1}=e^{i2\pi/3}$ and $\omega_{2}=e^{-i2\pi/3}$. And it's also obvious after simple calculation, that $$f(\omega_1)=f(\omega_2)=0$$ By the Factor Theorem, since $\omega_{1,2}$ are distinct, $$(x-\omega_1)(x-\omega_2)\mid f(x)$$ i.e. $$g(x)\mid f(x)$$ Nevertheless , all I have done so far is based on the assumption that $K$ is (or includes) $\mathbb C$, without which I think it will be ridiculous to write something like $(x-\omega_{1,2})$ as polynomials. However, I feel strongly that my approach, although based on a not quite reasonable assumption, is in fact a quick-and-easy way, whatever $K$ be. So, maybe there is a theorem unknown to me which can justify my method when $K$ doesn't include $\mathbb C$, say, $K=\mathbb R$? Of course, I know I am not allowed to factorize like this when $K=R$, but $g(x)\mid f(x)$ is still alright. And I think $g(x)\mid f(x)$ always holds whatever $K$ be, even though the factorization is not always true. So am I wrong in not giving up my proof? Or is there really a god-sent theorem that will save my proof?  Need some help. Best regards here.",,"['linear-algebra', 'abstract-algebra', 'polynomials']"
83,If Matrix $A^3 = 0$ then what will be $I+A+A^2$,If Matrix  then what will be,A^3 = 0 I+A+A^2,"Matrix $A$ is a square matrix such that $$A^3=0$$ then what would be value of $$I+A+A^2=?$$ such that $I$ is unit matrix of same order as $A$ I firstly supposed sum to be $X$ that is  $X=I+A+A^2$ then  $$AX=A+A^2+A^3$$ $$AX=A+A^2$$ and multiplying both sides with $A^{-1}$ we get  $$K=I+A$$ but my answer was incorrect , what is correct solution to this problem?","Matrix $A$ is a square matrix such that $$A^3=0$$ then what would be value of $$I+A+A^2=?$$ such that $I$ is unit matrix of same order as $A$ I firstly supposed sum to be $X$ that is  $X=I+A+A^2$ then  $$AX=A+A^2+A^3$$ $$AX=A+A^2$$ and multiplying both sides with $A^{-1}$ we get  $$K=I+A$$ but my answer was incorrect , what is correct solution to this problem?",,"['linear-algebra', 'matrices']"
84,"Solve $2(x+y)+xy=x^2+y^2$ where $x,y \in \mathbb{Z}$",Solve  where,"2(x+y)+xy=x^2+y^2 x,y \in \mathbb{Z}",Solve the equation: $$2(x+y)+xy=x^2+y^2$$ How should I go about solving this? Any guidance appreciated. Thanks!,Solve the equation: $$2(x+y)+xy=x^2+y^2$$ How should I go about solving this? Any guidance appreciated. Thanks!,,"['linear-algebra', 'algebra-precalculus', 'elementary-number-theory']"
85,Prove that the characteristic polynomial of a nilpotent matrix is $x^n$,Prove that the characteristic polynomial of a nilpotent matrix is,x^n,"How can I prove that the char.pol. of a nilpotent matrix is of the form $x^k$? I'm trying to do it by contradiction but assuming that $p_{xA}=a_0+a_1x+\dots+a_mx^m+\dots+a_nx^n$ seems not giving any contradiction. I've proved that the eigenvalues of A has to be 0, which led to $det(A)=0$.","How can I prove that the char.pol. of a nilpotent matrix is of the form $x^k$? I'm trying to do it by contradiction but assuming that $p_{xA}=a_0+a_1x+\dots+a_mx^m+\dots+a_nx^n$ seems not giving any contradiction. I've proved that the eigenvalues of A has to be 0, which led to $det(A)=0$.",,"['linear-algebra', 'matrices', 'polynomials', 'nilpotence']"
86,Find a real $2\times 2$ matrix $A$ (other than $A = I$ ) such that $A^5 = I $.,Find a real  matrix  (other than  ) such that .,2\times 2 A A = I A^5 = I ,"I found the question in an online a source of challenging linear algebra problems, unfortunately there are no answers. Question: Find a real $2\times 2$ matrix $A$ (other than $A = I$ ) such that $A^5 = I$. I'm beginning to think no such matrix exists, but the way the question is posed it doesn't seem they would pull a trick like that.","I found the question in an online a source of challenging linear algebra problems, unfortunately there are no answers. Question: Find a real $2\times 2$ matrix $A$ (other than $A = I$ ) such that $A^5 = I$. I'm beginning to think no such matrix exists, but the way the question is posed it doesn't seem they would pull a trick like that.",,['linear-algebra']
87,Jordan Canonical Form of matrix,Jordan Canonical Form of matrix,,"I am having trouble figuring out computing Jordan Canonical Form. Can someone explain how to get there with this example matrix? $A=\begin{bmatrix}1&1&1\\0&2&0\\0&0&2\end{bmatrix}$ Also, what would the transformation matrix $D$ be, if $D^{-1}AD$ is in Jordan Form.","I am having trouble figuring out computing Jordan Canonical Form. Can someone explain how to get there with this example matrix? $A=\begin{bmatrix}1&1&1\\0&2&0\\0&0&2\end{bmatrix}$ Also, what would the transformation matrix $D$ be, if $D^{-1}AD$ is in Jordan Form.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
88,Show that $T(V)$ is finite-dimensional?,Show that  is finite-dimensional?,T(V),"Let $T: V\rightarrow V$ be the linear transformation defined as follows: If $f$ is in $T$, $g=T(f)$ is: $$ g(x) = \int_{-\pi}^{\pi} (1+\cos(x-t))f(t) dt $$ Prove $T(V)$ is finite-dimensional. edit : Forgot to put the definition of $V$: the linear space of all real functions continuous on the interval $[-\pi,\pi]$. edit 2 Dammit, thought the problem was written with a slash; re-edited to correct integrand.","Let $T: V\rightarrow V$ be the linear transformation defined as follows: If $f$ is in $T$, $g=T(f)$ is: $$ g(x) = \int_{-\pi}^{\pi} (1+\cos(x-t))f(t) dt $$ Prove $T(V)$ is finite-dimensional. edit : Forgot to put the definition of $V$: the linear space of all real functions continuous on the interval $[-\pi,\pi]$. edit 2 Dammit, thought the problem was written with a slash; re-edited to correct integrand.",,"['linear-algebra', 'functional-analysis']"
89,Dual space and inner/scalar product space,Dual space and inner/scalar product space,,"$V$ is vector space of finite dimension. $〈· , ·〉$ is an inner product on $V$.(Field $F$) We set transformation $T \colon V \rightarrow V^*$ as the following: $(T(v))(w) = 〈v , w〉$. Prove that $T$ is Isomorphism. I don't know how to prove that it is 1 on 1 and onto.  I mean, the dualic space is confusing me since I don't understand it properly. For 1-1 : I need to assume that $〈v_1,w_1〉 = 〈v_2,w_2〉$ and show that $v_1=v_2$ and $w_1 = w_2$? I'm not sure what is my domain... $T$ is from $V$ to $V^*$ so should I show that $v_1 = v_2$ only? For onto: I need to show that for every functional from $V^*$ there is $v$ from $V$ such that it equals? doesn't make sense because the inner product is a scalar from $F$. I feel very helpless about this, could someone help me please  ?","$V$ is vector space of finite dimension. $〈· , ·〉$ is an inner product on $V$.(Field $F$) We set transformation $T \colon V \rightarrow V^*$ as the following: $(T(v))(w) = 〈v , w〉$. Prove that $T$ is Isomorphism. I don't know how to prove that it is 1 on 1 and onto.  I mean, the dualic space is confusing me since I don't understand it properly. For 1-1 : I need to assume that $〈v_1,w_1〉 = 〈v_2,w_2〉$ and show that $v_1=v_2$ and $w_1 = w_2$? I'm not sure what is my domain... $T$ is from $V$ to $V^*$ so should I show that $v_1 = v_2$ only? For onto: I need to show that for every functional from $V^*$ there is $v$ from $V$ such that it equals? doesn't make sense because the inner product is a scalar from $F$. I feel very helpless about this, could someone help me please  ?",,"['linear-algebra', 'vector-spaces', 'inner-products']"
90,What's special about the first vector,What's special about the first vector,,"My linear algebra notes state the following lemma: If $(v_1, ...,v_m)$ is linearly dependent in $V$ and $v_1 \neq 0$ then there exists $j \in \{2,...,m\}$ such that $v_j \in span(v_1,...,v_{j-1})$ where $(...)$ denotes an ordered list. But if at least one $v_i$ is $\neq 0$ then the list can be reordered and the lemma applied. Is $v_1 \neq 0$ just another way of saying $v_i \neq 0$ for at least one $i$?","My linear algebra notes state the following lemma: If $(v_1, ...,v_m)$ is linearly dependent in $V$ and $v_1 \neq 0$ then there exists $j \in \{2,...,m\}$ such that $v_j \in span(v_1,...,v_{j-1})$ where $(...)$ denotes an ordered list. But if at least one $v_i$ is $\neq 0$ then the list can be reordered and the lemma applied. Is $v_1 \neq 0$ just another way of saying $v_i \neq 0$ for at least one $i$?",,"['linear-algebra', 'vector-spaces']"
91,Matrix commutator question,Matrix commutator question,,"Here's a nice question I heard on IRC, courtesy of ""tmyklebu."" Let $A$, $B$, and $C$ be $2\times 2$ complex matrices. Define the commutator $[X,Y]=XY-YX$ for any matrices $X$ and $Y$. Prove $$[[A,B]^2,C]=0.$$","Here's a nice question I heard on IRC, courtesy of ""tmyklebu."" Let $A$, $B$, and $C$ be $2\times 2$ complex matrices. Define the commutator $[X,Y]=XY-YX$ for any matrices $X$ and $Y$. Prove $$[[A,B]^2,C]=0.$$",,['linear-algebra']
92,Proof of matrix identity,Proof of matrix identity,,Is there a proof for the following identity? $\left(A^{-1}+B^{-1}\right)^{-1}=A(A+B)^{-1}B$,Is there a proof for the following identity? $\left(A^{-1}+B^{-1}\right)^{-1}=A(A+B)^{-1}B$,,"['linear-algebra', 'matrices']"
93,Orthogonal and symmetric Matrices,Orthogonal and symmetric Matrices,,What can one say about the set of all $n$-dimensional square matrices $A \in \text{GL}_n(\mathbb{C})$ that have an inverse with entries out of $\mathbb{C}$ with the properties: unitary $:\Leftrightarrow A^*= A^{-1}$ hermitian $:\Leftrightarrow A^* = A$ where $A^*$ is the conjugate transpose of $A$. What obviously follows is $$A^{-1} = A$$ The most simple matrix that is in this set is the identity matrix. Are there others? How do they look like?,What can one say about the set of all $n$-dimensional square matrices $A \in \text{GL}_n(\mathbb{C})$ that have an inverse with entries out of $\mathbb{C}$ with the properties: unitary $:\Leftrightarrow A^*= A^{-1}$ hermitian $:\Leftrightarrow A^* = A$ where $A^*$ is the conjugate transpose of $A$. What obviously follows is $$A^{-1} = A$$ The most simple matrix that is in this set is the identity matrix. Are there others? How do they look like?,,"['linear-algebra', 'matrices']"
94,Linear independence of columns and covering the whole space (algebraic rationale),Linear independence of columns and covering the whole space (algebraic rationale),,"It would be great if someone could explain me the following. If I understand the lecture correctly, given a system of, say, following equations: $x+2y-2z=1$ $2x+4y+z=3$ $4x+8y+4z=10$ We can say that the they do not describe the whole 3D space because the vectors formed by the first and second columns ( $\begin{pmatrix}  1 \\   2 \\   4   \end{pmatrix} $ and $\begin{pmatrix}  2 \\   4 \\   8   \end{pmatrix} $ respectively) are not linearly independent (meaning we can't solve the given equations for all possible RHS). From what I understand, it makes sense geometrically -- if we start at an arbitrary point in 3D, we can cover only a plane as we have only two directions we could move in. However, could someone please explain the meaning of the statement above in algebraic terms? What is it about the coefficients of a given variable that when they can be expressed via multiplication of some $c$ (where $c=2$ for the two aforementioned columns) by coefficients of some other variable, not all RHS values generate solutions? Thanks a lot!","It would be great if someone could explain me the following. If I understand the lecture correctly, given a system of, say, following equations: $x+2y-2z=1$ $2x+4y+z=3$ $4x+8y+4z=10$ We can say that the they do not describe the whole 3D space because the vectors formed by the first and second columns ( $\begin{pmatrix}  1 \\   2 \\   4   \end{pmatrix} $ and $\begin{pmatrix}  2 \\   4 \\   8   \end{pmatrix} $ respectively) are not linearly independent (meaning we can't solve the given equations for all possible RHS). From what I understand, it makes sense geometrically -- if we start at an arbitrary point in 3D, we can cover only a plane as we have only two directions we could move in. However, could someone please explain the meaning of the statement above in algebraic terms? What is it about the coefficients of a given variable that when they can be expressed via multiplication of some $c$ (where $c=2$ for the two aforementioned columns) by coefficients of some other variable, not all RHS values generate solutions? Thanks a lot!",,['linear-algebra']
95,Does a vector space over $F$ necessarily contain a field isomorphic to $F$?,Does a vector space over  necessarily contain a field isomorphic to ?,F F,"This is a question I had and answered myself, and I didn't find this anywhere on Google so I thought it would be interesting to share. Initially posted this in Math Overflow but deleted that question because users notified me that it didn't fit there, but I still thought it was an interesting question so I'm putting it here again. By definition, a vector space $V$ over $F$ is the tuple $(V, F, \cdot)$ where $F$ is a field, $(V, +)$ is an abelian group, and $\cdot: V \times F \rightarrow V$ satisfies associativity, distributivity, and compatibility with the multiplicative identity in $F$ . I always thought of vector spaces as a generalization of field extensions, but upon reflection I couldn't actually prove using the definition that $V$ must contain something isomorphic to $F$ . If I think about vector spaces like $F^n$ and any ring extension over $F$ , it seems almost 'obvious' that $V$ should contain something isomorphic to $F$ . Or am I just missing a counterexample?","This is a question I had and answered myself, and I didn't find this anywhere on Google so I thought it would be interesting to share. Initially posted this in Math Overflow but deleted that question because users notified me that it didn't fit there, but I still thought it was an interesting question so I'm putting it here again. By definition, a vector space over is the tuple where is a field, is an abelian group, and satisfies associativity, distributivity, and compatibility with the multiplicative identity in . I always thought of vector spaces as a generalization of field extensions, but upon reflection I couldn't actually prove using the definition that must contain something isomorphic to . If I think about vector spaces like and any ring extension over , it seems almost 'obvious' that should contain something isomorphic to . Or am I just missing a counterexample?","V F (V, F, \cdot) F (V, +) \cdot: V \times F \rightarrow V F V F F^n F V F","['linear-algebra', 'vector-spaces']"
96,"If $T$ is strictly upper triangular, can we find upper triangular $A,B \in M_n(\Bbb C)$ such that $T = AB- BA$?","If  is strictly upper triangular, can we find upper triangular  such that ?","T A,B \in M_n(\Bbb C) T = AB- BA","Consider $M_n(\Bbb C)$ , the vector space of matrices taking entries from $\Bbb C$ ,  for $n \ge 2$ . We know that the collection of upper triangular matrices in $M_n(\Bbb C)$ forms a subspace of $M_n(\Bbb C)$ . Additionally, if we start with two upper triangular matrices $A,B \in M_n(\Bbb C)$ , then the commutator $AB-BA$ is strictly upper triangular. I believe the converse is true, i.e., given a strictly upper triangular matrix $T \in M_n(\Bbb C)$ , we can find two upper triangular matrices $A,B \in M_n(\Bbb C)$ such that $T = AB - BA$ . I have thought of some examples (as follows) where this can be done, but I have not yet written a proof that takes care of all cases. If $T = 0$ , then any two commuting matrices $A,B \in M_n(\Bbb C)$ will do. In fact, we can just pick $A,B$ to be any two diagonal matrices. This already tells us that the choice of $A,B$ need not be unique. One of the simpler cases to consider is $n = 2$ . If $A_1 = \left(\begin{matrix} p_1 & q_1\\ 0 & r_1 \end{matrix}\right)$ and $A_2 = \left(\begin{matrix} p_2 & q_2\\ 0 & r_2 \end{matrix}\right)$ , then $$A_1A_2 - A_2A_1 = \left(\begin{matrix} 0 & p_1q_2 + q_1r_2-p_2q_1-q_2r_1\\ 0 & 0  \end{matrix}\right)$$ Given a strictly upper triangular matrix $T = \left(\begin{matrix} 0 & \lambda \\ 0 & 0 \end{matrix}\right)$ , we need to solve $$p_1q_2 + q_1r_2-p_2q_1-q_2r_1 = \lambda,$$ i.e., one equation in eight variables. This system is overdetermined, so we can pick values of $p_1, p_2, \ldots$ , etc. that solve the equation. There are infinitely many solutions! To repeat the same process for $M_n(\Bbb C)$ is cumbersome, and I'm hoping a proof by induction of some sort might do the trick. Thank you!","Consider , the vector space of matrices taking entries from ,  for . We know that the collection of upper triangular matrices in forms a subspace of . Additionally, if we start with two upper triangular matrices , then the commutator is strictly upper triangular. I believe the converse is true, i.e., given a strictly upper triangular matrix , we can find two upper triangular matrices such that . I have thought of some examples (as follows) where this can be done, but I have not yet written a proof that takes care of all cases. If , then any two commuting matrices will do. In fact, we can just pick to be any two diagonal matrices. This already tells us that the choice of need not be unique. One of the simpler cases to consider is . If and , then Given a strictly upper triangular matrix , we need to solve i.e., one equation in eight variables. This system is overdetermined, so we can pick values of , etc. that solve the equation. There are infinitely many solutions! To repeat the same process for is cumbersome, and I'm hoping a proof by induction of some sort might do the trick. Thank you!","M_n(\Bbb C) \Bbb C n \ge 2 M_n(\Bbb C) M_n(\Bbb C) A,B \in M_n(\Bbb C) AB-BA T \in M_n(\Bbb C) A,B \in M_n(\Bbb C) T = AB - BA T = 0 A,B \in M_n(\Bbb C) A,B A,B n = 2 A_1 = \left(\begin{matrix} p_1 & q_1\\ 0 & r_1 \end{matrix}\right) A_2 = \left(\begin{matrix} p_2 & q_2\\ 0 & r_2 \end{matrix}\right) A_1A_2 - A_2A_1 = \left(\begin{matrix} 0 & p_1q_2 + q_1r_2-p_2q_1-q_2r_1\\ 0 & 0  \end{matrix}\right) T = \left(\begin{matrix} 0 & \lambda \\ 0 & 0 \end{matrix}\right) p_1q_2 + q_1r_2-p_2q_1-q_2r_1 = \lambda, p_1, p_2, \ldots M_n(\Bbb C)","['linear-algebra', 'abstract-algebra', 'matrices', 'induction', 'matrix-equations']"
97,Why can't matrix commute?,Why can't matrix commute?,,"Sorry if this question sounds silly, and it probably is. I can prove easily that matrix multiplication is noncommutative; however, look at this 'fake' proof: $$AB=e^{\ln(A)}e^{\ln(B)}=e^{\ln(A)+\ln(B)}$$ $\ln(A)$ and $\ln(B)$ are also matrices, and matrix addition is comutative. So: $$e^{\ln(A)+\ln(B)}=e^{\ln(B)+\ln(A)}=e^{\ln(B)}e^{\ln(A)}=BA$$ What's the problem?","Sorry if this question sounds silly, and it probably is. I can prove easily that matrix multiplication is noncommutative; however, look at this 'fake' proof: and are also matrices, and matrix addition is comutative. So: What's the problem?",AB=e^{\ln(A)}e^{\ln(B)}=e^{\ln(A)+\ln(B)} \ln(A) \ln(B) e^{\ln(A)+\ln(B)}=e^{\ln(B)+\ln(A)}=e^{\ln(B)}e^{\ln(A)}=BA,"['linear-algebra', 'matrices', 'logarithms', 'exponentiation', 'fake-proofs']"
98,"Wronskian of functions $\sin(nx), n=1,2,...,k$.",Wronskian of functions .,"\sin(nx), n=1,2,...,k","Is it true that the Wronskian of the functions $\sin(nx), n=1,...,k$ is equal to $c(\sin(x))^p$ where $c$ is a constant and $p=1+2+...+k=k(k+1)/2$ ? That is true for $k=1,2,3,4,5$ . If it is true, how to find the constant $c=c(k)$ ?","Is it true that the Wronskian of the functions is equal to where is a constant and ? That is true for . If it is true, how to find the constant ?","\sin(nx), n=1,...,k c(\sin(x))^p c p=1+2+...+k=k(k+1)/2 k=1,2,3,4,5 c=c(k)","['linear-algebra', 'sequences-and-series', 'trigonometry', 'wronskian']"
99,Calculating $n$-th power of a matrix,Calculating -th power of a matrix,n,"I was doing an exercise of a past exam in which one of the things I had to do was calculating the $n$ th power of a Jordan matrix $$J=\begin{pmatrix} 2 & 1 & 0 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \end{pmatrix}.$$ I started calculating until the 5th power but I couldn't guess the expression for the $a_{1,2}$ , $a_{1,3}$ and $a_{2,3}$ components. When I looked it up in an online calculator it showed that: $$J^n=\begin{pmatrix} 2^n & \frac{2^n·n}{2} & \frac{2^n·(n^2-n)}{8} \\ 0 & 2^n & \frac{2^n·n}{2} \\ 0 & 0 & 2^n \end{pmatrix}.$$ My question is: when the relationships are as difficult as these　(especially the $a_{1,3}$ component), are there any tricks for figuring out the $n$ th power component? Because these kind of relationships are difficult to think in the middle of an exam.","I was doing an exercise of a past exam in which one of the things I had to do was calculating the th power of a Jordan matrix I started calculating until the 5th power but I couldn't guess the expression for the , and components. When I looked it up in an online calculator it showed that: My question is: when the relationships are as difficult as these　(especially the component), are there any tricks for figuring out the th power component? Because these kind of relationships are difficult to think in the middle of an exam.","n J=\begin{pmatrix}
2 & 1 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\end{pmatrix}. a_{1,2} a_{1,3} a_{2,3} J^n=\begin{pmatrix}
2^n & \frac{2^n·n}{2} & \frac{2^n·(n^2-n)}{8} \\
0 & 2^n & \frac{2^n·n}{2} \\
0 & 0 & 2^n
\end{pmatrix}. a_{1,3} n",['linear-algebra']
