,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Taking inequalities when dealing with sums,Taking inequalities when dealing with sums,,"I have from a problem im solving and so far I have that $|f_i(x) - f_i(y)| < \epsilon / \sqrt{n}$ where $\epsilon , n >0$ I'm now trying to use this inequality here: $$\|f(x) - f(y) \|_2  = [ \sum_i^n (f_i(x) - f_i(y))^2 ] ^{1/2} < \left ( \sum_i^n (\epsilon / \sqrt{n} )^2 \right )^{1/2} = \epsilon$$ This is the result I want, but I am not sure if I can just apply the inequality directly under the square root of the sum. Furthermore, the book I'm looking at directly wrote: $$[ \sum_i^n (f_i(x) - f_i(y))^2 ] ^{1/2} \leq n^{1/2} \text{max}_i | f_i(x) - f_i(y) | < n^{1/2} \epsilon / \sqrt{n} = \epsilon$$ I don't understand what they have done here, so I am asking: If someone could please explain what they've done If what I've done is at all correct, if not how I can correct it many thanks","I have from a problem im solving and so far I have that $|f_i(x) - f_i(y)| < \epsilon / \sqrt{n}$ where $\epsilon , n >0$ I'm now trying to use this inequality here: $$\|f(x) - f(y) \|_2  = [ \sum_i^n (f_i(x) - f_i(y))^2 ] ^{1/2} < \left ( \sum_i^n (\epsilon / \sqrt{n} )^2 \right )^{1/2} = \epsilon$$ This is the result I want, but I am not sure if I can just apply the inequality directly under the square root of the sum. Furthermore, the book I'm looking at directly wrote: $$[ \sum_i^n (f_i(x) - f_i(y))^2 ] ^{1/2} \leq n^{1/2} \text{max}_i | f_i(x) - f_i(y) | < n^{1/2} \epsilon / \sqrt{n} = \epsilon$$ I don't understand what they have done here, so I am asking: If someone could please explain what they've done If what I've done is at all correct, if not how I can correct it many thanks",,['analysis']
1,Uniform Convergence of Maximum of Sequence of Functions,Uniform Convergence of Maximum of Sequence of Functions,,"Let $K$ be a compact metric space, and $\{f_n\}_{n \in \mathbb{N}}$ is a uniformly bounded, equicontinuous family of functions. Define $$g_n(x) = \max \{f_1(x),f_2(x),\ldots,f_n(x)\}.$$ Prove that $g_n$ converges uniformly on $K$. I believe this stems from the fact that $g_n$ is equicontinuous (just take the smallest deltas for $g_n$) and $g_n$ is uniformly bounded. Ergo, it has a convergent subsequence by Arzela-Ascoli. It is then a simple argument to show that if some subsequence of $g_n$ converges, so does the entire sequence. Is this a fair argument?","Let $K$ be a compact metric space, and $\{f_n\}_{n \in \mathbb{N}}$ is a uniformly bounded, equicontinuous family of functions. Define $$g_n(x) = \max \{f_1(x),f_2(x),\ldots,f_n(x)\}.$$ Prove that $g_n$ converges uniformly on $K$. I believe this stems from the fact that $g_n$ is equicontinuous (just take the smallest deltas for $g_n$) and $g_n$ is uniformly bounded. Ergo, it has a convergent subsequence by Arzela-Ascoli. It is then a simple argument to show that if some subsequence of $g_n$ converges, so does the entire sequence. Is this a fair argument?",,"['real-analysis', 'analysis', 'proof-verification', 'proof-writing']"
2,"Prove if {$a_n$} $\rightarrow$ $\infty$, then {$a_n$} is not bounded above. Give an indirect proof.","Prove if {}  , then {} is not bounded above. Give an indirect proof.",a_n \rightarrow \infty a_n,"The book I am using for my Advance Calculus course is Introduction to Analysis by Arthur Mattuck. Prove if {$a_n$} $\rightarrow$ $\infty$, then {$a_n$} is not bounded above. Give an indirect proof. This is my rough proof to this question. I was wondering if anybody can look over it and see if I made a mistake or if there is a simpler way of doing this problem. I want to thank you ahead of time it is greatly appreciated.So lets begin: Proof:","The book I am using for my Advance Calculus course is Introduction to Analysis by Arthur Mattuck. Prove if {$a_n$} $\rightarrow$ $\infty$, then {$a_n$} is not bounded above. Give an indirect proof. This is my rough proof to this question. I was wondering if anybody can look over it and see if I made a mistake or if there is a simpler way of doing this problem. I want to thank you ahead of time it is greatly appreciated.So lets begin: Proof:",,"['real-analysis', 'analysis']"
3,Estimates for parabolic vs elliptic PDE,Estimates for parabolic vs elliptic PDE,,"Elliptic and parabolic PDE share many properties.  They each, for example, have an associated maximum principle and their value at any point depends on the entirety of the boundary data. I have been told that estimates for solutions to parabolic PDE typically mirror those for elliptic PDE, but are also more difficult to prove.  Since an elliptic PDE can be thought of as the steady state solution to a parabolic PDE, I am tempted to think of parabolic results as more general. My question: Is there some common method used to get results for elliptic PDE from results for parabolic PDE? My first guess would be to take $t\rightarrow \infty$ of a solution and, provided you have convergence in some appropriate sense, you get a solution to the associated steady-state problem.  However, I haven't found any source that does this.","Elliptic and parabolic PDE share many properties.  They each, for example, have an associated maximum principle and their value at any point depends on the entirety of the boundary data. I have been told that estimates for solutions to parabolic PDE typically mirror those for elliptic PDE, but are also more difficult to prove.  Since an elliptic PDE can be thought of as the steady state solution to a parabolic PDE, I am tempted to think of parabolic results as more general. My question: Is there some common method used to get results for elliptic PDE from results for parabolic PDE? My first guess would be to take $t\rightarrow \infty$ of a solution and, provided you have convergence in some appropriate sense, you get a solution to the associated steady-state problem.  However, I haven't found any source that does this.",,"['analysis', 'partial-differential-equations', 'elliptic-equations']"
4,"Complement of the union of countably many , mutually disjoint , non-empty open balls in $\mathbb R^n , (n >1) $ is path connected?","Complement of the union of countably many , mutually disjoint , non-empty open balls in  is path connected?","\mathbb R^n , (n >1) ","Let $n \ge 2$ and $\{B_m\}_{m=1}^\infty$ be countably infinitely many , mutually disjoint , non-empty  open balls in $\mathbb R^n$ , then is $\mathbb R^n \setminus \cup_{m=1}^\infty B_m$ path-connected ?","Let $n \ge 2$ and $\{B_m\}_{m=1}^\infty$ be countably infinitely many , mutually disjoint , non-empty  open balls in $\mathbb R^n$ , then is $\mathbb R^n \setminus \cup_{m=1}^\infty B_m$ path-connected ?",,['analysis']
5,Jacobi field strange condition.,Jacobi field strange condition.,,"I am currently reading a textbook (Kuehnel) saying that if $V,W \in T_pM$ are such that $\langle V,W \rangle =0$ and $\|V\|=\|W\|=1,$ then  $Y(t):=D \exp(tV)(tW)$ is a Jacobi field. The thing is, I don't understand why this textbook has all these conditions on $V,W$? Isn't it true that this also holds if we have any $V,W \in T_pM,$ cause all we should need is a variation of geodesics.","I am currently reading a textbook (Kuehnel) saying that if $V,W \in T_pM$ are such that $\langle V,W \rangle =0$ and $\|V\|=\|W\|=1,$ then  $Y(t):=D \exp(tV)(tW)$ is a Jacobi field. The thing is, I don't understand why this textbook has all these conditions on $V,W$? Isn't it true that this also holds if we have any $V,W \in T_pM,$ cause all we should need is a variation of geodesics.",,"['real-analysis', 'analysis', 'differential-geometry', 'manifolds', 'riemannian-geometry']"
6,Why does a differential form represent a vector field?,Why does a differential form represent a vector field?,,"I'm trying to learn the Divergence/Stoke's theorem and I can't wrap my head around the meaning of a differential form in this context. What does it mean that a differential form represents a vector field? What does it mean to derive a differential form? Why do we sometimes take an integral wherein of the inner product of a form and its derivative? What does it mean to integrate over a form? Lastly, sometimes in the context of a form I see dx ^ dy . What does the ^ mean? Can anyone give me some intuition about this? I've gone over my class notes and it all goes straight to formalities and I'm having trouble grasping these concepts.  Any help would be welcomed!","I'm trying to learn the Divergence/Stoke's theorem and I can't wrap my head around the meaning of a differential form in this context. What does it mean that a differential form represents a vector field? What does it mean to derive a differential form? Why do we sometimes take an integral wherein of the inner product of a form and its derivative? What does it mean to integrate over a form? Lastly, sometimes in the context of a form I see dx ^ dy . What does the ^ mean? Can anyone give me some intuition about this? I've gone over my class notes and it all goes straight to formalities and I'm having trouble grasping these concepts.  Any help would be welcomed!",,"['calculus', 'analysis', 'vector-spaces', 'differential-forms']"
7,Differentiation always easy?,Differentiation always easy?,,"There are many examples of real functions admitting antiderivatives (since e.g. continuous), but where computing a concrete antiderivative is a seriously hard problem even if an elementary one exists. What about differentiation? My experience is that the basic rules of calculus along with term-by-term differentiantion of power series make differentiation a just-do-it kind of problem for virtually all everyday kinds of functions. In fact, if we add the limit exchange trick for uniformly convergent sequence of derivatives, I cannot think of any examples, where finding a closed form for $f'$, given a closed form for $f$, is not a mechanical task. So the question is: are there any examples of real functions $f$, such that (1) $f$ is given in ""nice"" closed form $f(x)=\ldots$ (2) it is ""relatively easy"" to justify that $f$ is differentiable (3) computing the derivative of $f$ is actually hard. This isn't exactly a precise question, but there just might be a ""know it when I see it"" example.","There are many examples of real functions admitting antiderivatives (since e.g. continuous), but where computing a concrete antiderivative is a seriously hard problem even if an elementary one exists. What about differentiation? My experience is that the basic rules of calculus along with term-by-term differentiantion of power series make differentiation a just-do-it kind of problem for virtually all everyday kinds of functions. In fact, if we add the limit exchange trick for uniformly convergent sequence of derivatives, I cannot think of any examples, where finding a closed form for $f'$, given a closed form for $f$, is not a mechanical task. So the question is: are there any examples of real functions $f$, such that (1) $f$ is given in ""nice"" closed form $f(x)=\ldots$ (2) it is ""relatively easy"" to justify that $f$ is differentiable (3) computing the derivative of $f$ is actually hard. This isn't exactly a precise question, but there just might be a ""know it when I see it"" example.",,"['analysis', 'derivatives']"
8,A Chain of Subsets of $\mathbb{R}$ Without any Good Countable Subchain,A Chain of Subsets of  Without any Good Countable Subchain,\mathbb{R},"Consider which $\bigl{(} A_i \bigr{)}_{i\in I}$ is a chain of subsets of $\mathbb{R}$.  We say that a countable chain like $\bigl{(} B_n \bigr{)}_{n\in \mathbb{N}}$ is good if : for every $n\in \mathbb{N}$, the set $B_n$ be an element of the main chain $\bigl{(} A_i \bigr{)}_{i\in I}$ for every $i\in I$ there exists an $n\in \mathbb{N}$ that $A_i \subset B_n $. Give an example of $\bigl{(} A_i \bigr{)}_{i\in I}$ which for it, does not exist such a good chain $\bigl{(} B_n \bigr{)}_{n\in \mathbb{N}}$. Such an example surely exists and surely exists of subsets with lebesgue zero measure, because if there does not exist then we can find a maximal zero measure set by using Zorn's lemma.","Consider which $\bigl{(} A_i \bigr{)}_{i\in I}$ is a chain of subsets of $\mathbb{R}$.  We say that a countable chain like $\bigl{(} B_n \bigr{)}_{n\in \mathbb{N}}$ is good if : for every $n\in \mathbb{N}$, the set $B_n$ be an element of the main chain $\bigl{(} A_i \bigr{)}_{i\in I}$ for every $i\in I$ there exists an $n\in \mathbb{N}$ that $A_i \subset B_n $. Give an example of $\bigl{(} A_i \bigr{)}_{i\in I}$ which for it, does not exist such a good chain $\bigl{(} B_n \bigr{)}_{n\in \mathbb{N}}$. Such an example surely exists and surely exists of subsets with lebesgue zero measure, because if there does not exist then we can find a maximal zero measure set by using Zorn's lemma.",,['analysis']
9,About convergence in norm of the Fourier Transform,About convergence in norm of the Fourier Transform,,"Duoandikoetxea's Fourier Analysis , on page 59 (Corollary 3.7) says that: \begin{equation} \lim_{R \rightarrow \infty}\big\|S_{R}\,f - f\big\|_{p} = 0 \end{equation} for $1<p<\infty$, where $S_{R}\,f \:$ is such that $\widehat{S_{R}\,f}\left(y\right)$ = $\chi_{(-R,R)}\left(y\right) \,\hat{f}(y)$. He also proves that there is a constant $C_{p}$ such that $||\,S_{R}\,f\,||_{p} \le C_{p}\|\,f\|_{p}$, and that this $C_{p}$ doesn't depend on $R$. After that he says that this is not true when $p=1$, but he doesn't give a counterexample. What function would contradict this when $p=1$? Another thing, he obtains the limit above as a consequence of $\|\,S_{R}\,f\,\|_{p} \leq C_{p}\,\|f\|_{p}$. I can prove the limit above for all $f \in L^{p}$ if I know that it is valid for a dense subset, let's say, $S(\mathbb{R})$ (Schwartz class) or $C^{\infty}_{c}$ by approximating $f$ using such functions. How do I prove the limit above for $f$ in any of these spaces?","Duoandikoetxea's Fourier Analysis , on page 59 (Corollary 3.7) says that: \begin{equation} \lim_{R \rightarrow \infty}\big\|S_{R}\,f - f\big\|_{p} = 0 \end{equation} for $1<p<\infty$, where $S_{R}\,f \:$ is such that $\widehat{S_{R}\,f}\left(y\right)$ = $\chi_{(-R,R)}\left(y\right) \,\hat{f}(y)$. He also proves that there is a constant $C_{p}$ such that $||\,S_{R}\,f\,||_{p} \le C_{p}\|\,f\|_{p}$, and that this $C_{p}$ doesn't depend on $R$. After that he says that this is not true when $p=1$, but he doesn't give a counterexample. What function would contradict this when $p=1$? Another thing, he obtains the limit above as a consequence of $\|\,S_{R}\,f\,\|_{p} \leq C_{p}\,\|f\|_{p}$. I can prove the limit above for all $f \in L^{p}$ if I know that it is valid for a dense subset, let's say, $S(\mathbb{R})$ (Schwartz class) or $C^{\infty}_{c}$ by approximating $f$ using such functions. How do I prove the limit above for $f$ in any of these spaces?",,"['analysis', 'fourier-analysis']"
10,Function that satisfies $\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx$,Function that satisfies,\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx,"I was wondering if anyone would be able to help me find a function that satisfies this condition: $$\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx$$ It needs to be able to do this on the interval [0, 1]. I've tried a few functions that look similar to what I'm looking for, like $f(x) = \frac{x}{x-1}$, or $f(x) = \frac{x^2}{x-1}$, however none of them have been the solution. I'd appreciate help on this problem, I'm having some trouble figuring out where to start.","I was wondering if anyone would be able to help me find a function that satisfies this condition: $$\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx$$ It needs to be able to do this on the interval [0, 1]. I've tried a few functions that look similar to what I'm looking for, like $f(x) = \frac{x}{x-1}$, or $f(x) = \frac{x^2}{x-1}$, however none of them have been the solution. I'd appreciate help on this problem, I'm having some trouble figuring out where to start.",,"['real-analysis', 'integration', 'analysis']"
11,$p$-adics with the least upper-bound property,-adics with the least upper-bound property,p,"It is well-known (I believe?) that the $p$-adics do not admit an ordering in the 'usual sense', the ""usual sense"" being a total order that is compatible with the field operations.  I do not want to require that an ordering be a total order, however.  In fact, I only require that it be a pre-order (though I believe it turns out to be a partial order). Let $x\in \mathbb{Q}_p$ and define $x>0$ iff $x$ is a finite sum of powers of $p$ (the idea is that numbers which are non-zero absolute values should positive, and hence so should sums of them).  I believe this should be a partial order on $\mathbb{Q}_p$ that is compatible with the field operations (i.e., $x\leq y$ implies $x+z\leq y+z$ and $x,y\geq 0$ implies $xy\geq 0$). Does $\mathbb{Q}_p$ with this ordering have the least upper-bound property?  (I apologize in advance if this question is overly easy; my understanding of the $p$-adics is not as strong as it probably should be.)","It is well-known (I believe?) that the $p$-adics do not admit an ordering in the 'usual sense', the ""usual sense"" being a total order that is compatible with the field operations.  I do not want to require that an ordering be a total order, however.  In fact, I only require that it be a pre-order (though I believe it turns out to be a partial order). Let $x\in \mathbb{Q}_p$ and define $x>0$ iff $x$ is a finite sum of powers of $p$ (the idea is that numbers which are non-zero absolute values should positive, and hence so should sums of them).  I believe this should be a partial order on $\mathbb{Q}_p$ that is compatible with the field operations (i.e., $x\leq y$ implies $x+z\leq y+z$ and $x,y\geq 0$ implies $xy\geq 0$). Does $\mathbb{Q}_p$ with this ordering have the least upper-bound property?  (I apologize in advance if this question is overly easy; my understanding of the $p$-adics is not as strong as it probably should be.)",,"['analysis', 'order-theory', 'p-adic-number-theory']"
12,Bounded Derivatives and Uniformly Continuous Functions,Bounded Derivatives and Uniformly Continuous Functions,,"Prove or Disprove : Let $f:\mathbb{R} \to \mathbb{R}$ be a bounded uniformly continuous function that whose first and second derivative exists and is continuous, in other words $f \in C^2_{unif} (\mathbb{R},\mathbb{R})$. Then $f'(x)$ is bounded. This is a problem that I came across while working on a project. At first we felt that it wasn't true but we've been unable to find a counterexample. Any advice on how to prove it (if it is true) would be much appreciated. I apologize if this has been posted before.","Prove or Disprove : Let $f:\mathbb{R} \to \mathbb{R}$ be a bounded uniformly continuous function that whose first and second derivative exists and is continuous, in other words $f \in C^2_{unif} (\mathbb{R},\mathbb{R})$. Then $f'(x)$ is bounded. This is a problem that I came across while working on a project. At first we felt that it wasn't true but we've been unable to find a counterexample. Any advice on how to prove it (if it is true) would be much appreciated. I apologize if this has been posted before.",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
13,Discrete and Essential spectrum of Laplacian in $\mathbb R_{+}$ (with weird boundary conditions),Discrete and Essential spectrum of Laplacian in  (with weird boundary conditions),\mathbb R_{+},"I am given on Hilbert Space $\mathcal H=L^2(\mathbb R_{+})$ $$ Af(x)=-f''(x) $$  and Domain of A is  $$ D(A)=\{f\in H_2(\mathbb R_{+})\;\;| \;\;f'(0)+\alpha f(0)=0\} $$ for some $\alpha \in \mathbb R$ I need to find discrete and essential spectrum of this unbounded operator. I only know one way of finding the spectrum of Laplacian, that is by Fourier transform (and its variant for half line). I do not know how to handle this situation.","I am given on Hilbert Space $\mathcal H=L^2(\mathbb R_{+})$ $$ Af(x)=-f''(x) $$  and Domain of A is  $$ D(A)=\{f\in H_2(\mathbb R_{+})\;\;| \;\;f'(0)+\alpha f(0)=0\} $$ for some $\alpha \in \mathbb R$ I need to find discrete and essential spectrum of this unbounded operator. I only know one way of finding the spectrum of Laplacian, that is by Fourier transform (and its variant for half line). I do not know how to handle this situation.",,"['analysis', 'operator-theory', 'spectral-theory']"
14,Epsilon-Delta Proof of Divergence of $S_n = (-1)^n*n$,Epsilon-Delta Proof of Divergence of,S_n = (-1)^n*n,"I am self-learning Analysis (reading Spivak's Calculus) but I found this problem in Ross' Elementary Analysis that I found interesting. However, I am having some difficulty proving the statement. It is: Show that the following sequence does not converge, $$ S_n = (-1)^n*n $$. I am attempting a proof via contradiction; ie: assume $ \exists $ a limit L $ \mid \forall \epsilon > 0, \exists $ N $ \in \mathbb{N} \mid n > $ N$ \implies |S_n - L| < \epsilon. \\ \implies |(-1)^nn - L| < \epsilon. $ Now i am stuck here because i am unsure which epsilon to choose and what to work towards in order to obtain my contradiction. Am I required to show that for all L $ \in \mathbb{R}$ that there is an $ \epsilon > 0 $ such that for any $ N > 0 $ there is an $ n > N $ such that $ |S_n - L| \ge \epsilon $? Any help is appreciated.","I am self-learning Analysis (reading Spivak's Calculus) but I found this problem in Ross' Elementary Analysis that I found interesting. However, I am having some difficulty proving the statement. It is: Show that the following sequence does not converge, $$ S_n = (-1)^n*n $$. I am attempting a proof via contradiction; ie: assume $ \exists $ a limit L $ \mid \forall \epsilon > 0, \exists $ N $ \in \mathbb{N} \mid n > $ N$ \implies |S_n - L| < \epsilon. \\ \implies |(-1)^nn - L| < \epsilon. $ Now i am stuck here because i am unsure which epsilon to choose and what to work towards in order to obtain my contradiction. Am I required to show that for all L $ \in \mathbb{R}$ that there is an $ \epsilon > 0 $ such that for any $ N > 0 $ there is an $ n > N $ such that $ |S_n - L| \ge \epsilon $? Any help is appreciated.",,"['analysis', 'epsilon-delta']"
15,Integrability of fourier transform,Integrability of fourier transform,,"Let $f\in L^1(\mathbb{R})$ such that there exist $R,\delta >0$ for which $f$ is bounded in $[-\delta, \delta]$ and  $\hat{f}(\xi)\geq 0$ for $|\xi|\geq R$. Then $\hat{f}\in L^1(\mathbb{R})$. Roughly speaking if we were able to use the inversion formula we would have $\int |\hat{f}|=\int \hat{f}=f(0)$ and we would get the result from boundedness of $f$. I don't know how to make this idea rigorous or if it is the right way.","Let $f\in L^1(\mathbb{R})$ such that there exist $R,\delta >0$ for which $f$ is bounded in $[-\delta, \delta]$ and  $\hat{f}(\xi)\geq 0$ for $|\xi|\geq R$. Then $\hat{f}\in L^1(\mathbb{R})$. Roughly speaking if we were able to use the inversion formula we would have $\int |\hat{f}|=\int \hat{f}=f(0)$ and we would get the result from boundedness of $f$. I don't know how to make this idea rigorous or if it is the right way.",,"['analysis', 'fourier-analysis']"
16,Calculus Apostol Exercise 1.7,Calculus Apostol Exercise 1.7,,"A point (x,y) in the plane is called a lattice point if both coordinates x and y are integers. Let P be a polygon whose vertices are lattice points. The area of P is I + B/2 - 1, where I denotes the number of lattice points inside the polygon and B denotes the number on the boundary. (a) Prove that the formula is valid for rectangles with sides parallel to the coordinate axes.  (b) Prove that the formula is valid for right triangles and parallelograms.  (c) Use induction on the number of edges to construct a proof for general polygons. I have done part (a) but stuck on part (b) and (c). I have tried to represent the trapezoid and parallelogram as the difference between a rectangle and triangles, but no success as of now. Any suggestions?","A point (x,y) in the plane is called a lattice point if both coordinates x and y are integers. Let P be a polygon whose vertices are lattice points. The area of P is I + B/2 - 1, where I denotes the number of lattice points inside the polygon and B denotes the number on the boundary. (a) Prove that the formula is valid for rectangles with sides parallel to the coordinate axes.  (b) Prove that the formula is valid for right triangles and parallelograms.  (c) Use induction on the number of edges to construct a proof for general polygons. I have done part (a) but stuck on part (b) and (c). I have tried to represent the trapezoid and parallelogram as the difference between a rectangle and triangles, but no success as of now. Any suggestions?",,"['calculus', 'analysis']"
17,Analysis Question Involving Real Numbers,Analysis Question Involving Real Numbers,,"So I've been sick and I've missed a couple of lectures of my Analysis class. But I didn't want to be too behind in lecture tomorrow, so I was trying to catch up by reading my textbook and solving some problems. Here's one I was having some trouble with: Prove $\sqrt2\in\mathbb R$ by showing $x^2 = 2$ where $x = A|B$ is a cut w/ $A = \{r \in \mathbb Q : r \leqslant 0 \text{ or } r^2 < 2\}$. Any help would be appreciated! Thank you! :)","So I've been sick and I've missed a couple of lectures of my Analysis class. But I didn't want to be too behind in lecture tomorrow, so I was trying to catch up by reading my textbook and solving some problems. Here's one I was having some trouble with: Prove $\sqrt2\in\mathbb R$ by showing $x^2 = 2$ where $x = A|B$ is a cut w/ $A = \{r \in \mathbb Q : r \leqslant 0 \text{ or } r^2 < 2\}$. Any help would be appreciated! Thank you! :)",,"['real-analysis', 'analysis']"
18,A question regarding a double series.,A question regarding a double series.,,"Let $\{a_{mn}\}$ be a double series, where $a_{mn}>0$ for all $m,n\in\Bbb{N}$. If $\sum\limits_{i=1}^\infty{a_{ik}}$ is finite for all $k\in\Bbb{N}$ and $\sum\limits_{j=1}^\infty{a_{hj}}$ is finite for all $j\in\Bbb{N}$, then $\sum\limits_{i=1}^\infty \sum\limits_{j=1}^\infty{a_{ij}}=\sum\limits_{j=1}^\infty\sum\limits_{i=1}^\infty{a_{ij}}$. Is the above statement true? If it is, how does one go about proving it?","Let $\{a_{mn}\}$ be a double series, where $a_{mn}>0$ for all $m,n\in\Bbb{N}$. If $\sum\limits_{i=1}^\infty{a_{ik}}$ is finite for all $k\in\Bbb{N}$ and $\sum\limits_{j=1}^\infty{a_{hj}}$ is finite for all $j\in\Bbb{N}$, then $\sum\limits_{i=1}^\infty \sum\limits_{j=1}^\infty{a_{ij}}=\sum\limits_{j=1}^\infty\sum\limits_{i=1}^\infty{a_{ij}}$. Is the above statement true? If it is, how does one go about proving it?",,['analysis']
19,Convolution integral problem,Convolution integral problem,,"In the process of solving a certain PDE, I've arrived at a convolution integral: $$\int_{\mathbb{R}^3} G(x-y) \nabla p(y) dy$$ where $x \in \mathbb{R}^3$, $G(z)=\frac{1}{\| z \|}$ and $p(z) = \frac{z_1}{\| z \|^3}$. Note that $G$ and $p$ both vanish at $\infty$. To avoid computing this gradient, I integrated by parts, moving the $y$ derivative over to $G$. Computing the appropriate derivatives and using the vanishing at $\infty$, I get $$\int_{\mathbb{R}^3} \frac{x-y}{\| x - y \|^3} \frac{y_1}{\| y \|^3} dy.$$ I'm not sure where to go from here. If it helps any, I already know the final result from another source which is using a different derivation that I don't understand. At the end I should get $$\frac{2 \pi x}{\| x \|^3} x_1.$$","In the process of solving a certain PDE, I've arrived at a convolution integral: $$\int_{\mathbb{R}^3} G(x-y) \nabla p(y) dy$$ where $x \in \mathbb{R}^3$, $G(z)=\frac{1}{\| z \|}$ and $p(z) = \frac{z_1}{\| z \|^3}$. Note that $G$ and $p$ both vanish at $\infty$. To avoid computing this gradient, I integrated by parts, moving the $y$ derivative over to $G$. Computing the appropriate derivatives and using the vanishing at $\infty$, I get $$\int_{\mathbb{R}^3} \frac{x-y}{\| x - y \|^3} \frac{y_1}{\| y \|^3} dy.$$ I'm not sure where to go from here. If it helps any, I already know the final result from another source which is using a different derivation that I don't understand. At the end I should get $$\frac{2 \pi x}{\| x \|^3} x_1.$$",,"['analysis', 'multivariable-calculus', 'partial-differential-equations']"
20,Range of Influence of the Wave Equation?,Range of Influence of the Wave Equation?,,"Suppose $u$ is a solution of the two-dimensional wave equation $$u_{tt}-c^2 \Delta u=f(x)$$  with initial values $u(0),u_t(0)$ that have support on the disc $x_1^2+x_2^2 \le 1$. Up to what time can you be sure that $u=0$ at the point $(x_1,x_2)=(2,3)$? My attempt: I think it depends on the point farthest from $(2,3)$ in the disc, which is $(-\frac{-2 \sqrt {13}}{13},-\frac{-3 \sqrt {13}}{13})$. Then by the range of influence of wave equation, the time should be $t=\frac{14+2\sqrt {13}}{c}$, where $c$ is constant of two dimensional wave equation $u_{tt}-c^2 \Delta u=f(x)$, $14+2\sqrt {13}$ is the distance of $(2,3)$ and $(-\frac{-2 \sqrt {13}}{13},-\frac{-3 \sqrt {13}}{13})$. Is that right? Can anyone help? Thanks so much!:)","Suppose $u$ is a solution of the two-dimensional wave equation $$u_{tt}-c^2 \Delta u=f(x)$$  with initial values $u(0),u_t(0)$ that have support on the disc $x_1^2+x_2^2 \le 1$. Up to what time can you be sure that $u=0$ at the point $(x_1,x_2)=(2,3)$? My attempt: I think it depends on the point farthest from $(2,3)$ in the disc, which is $(-\frac{-2 \sqrt {13}}{13},-\frac{-3 \sqrt {13}}{13})$. Then by the range of influence of wave equation, the time should be $t=\frac{14+2\sqrt {13}}{c}$, where $c$ is constant of two dimensional wave equation $u_{tt}-c^2 \Delta u=f(x)$, $14+2\sqrt {13}$ is the distance of $(2,3)$ and $(-\frac{-2 \sqrt {13}}{13},-\frac{-3 \sqrt {13}}{13})$. Is that right? Can anyone help? Thanks so much!:)",,"['analysis', 'partial-differential-equations', 'wave-equation']"
21,"How find this function $f(x)\equiv 0,x\in R$?",How find this function ?,"f(x)\equiv 0,x\in R","Let $f:\mathbb R \to \mathbb R$ have the properties (1): for any prime number $p$ and any real number $x$,   $$\sum_{j=0}^{p-1}f\left(x+\dfrac{j}{p}\right)=0$$ (2): there exist real numbers $a$ and $b(>a)$ such that: $x\in (a,b) \implies f(x)=0$ Show that $f(x)\equiv 0$ Perhaps we can use this well known fact:  $$1+a+a^2+\cdots+a^{n-1}=0,a=\exp \left (\dfrac{k\pi\cdot i}{n}\right )$$ but I don't know how to use it here.","Let $f:\mathbb R \to \mathbb R$ have the properties (1): for any prime number $p$ and any real number $x$,   $$\sum_{j=0}^{p-1}f\left(x+\dfrac{j}{p}\right)=0$$ (2): there exist real numbers $a$ and $b(>a)$ such that: $x\in (a,b) \implies f(x)=0$ Show that $f(x)\equiv 0$ Perhaps we can use this well known fact:  $$1+a+a^2+\cdots+a^{n-1}=0,a=\exp \left (\dfrac{k\pi\cdot i}{n}\right )$$ but I don't know how to use it here.",,['analysis']
22,Bijection Between $\mathbb{Z}_+$ and a Subset of $\mathbb{Z}_+ \times \mathbb{Z}_+$,Bijection Between  and a Subset of,\mathbb{Z}_+ \mathbb{Z}_+ \times \mathbb{Z}_+,"Let $T=\{(a,b)\mid a,b\in\mathbb Z_+, b\leq a\}.$ Find a bijective function $f: T \to \mathbb{Z}_+$ I have tried to find a function but I can't, how does such function look like?","Let $T=\{(a,b)\mid a,b\in\mathbb Z_+, b\leq a\}.$ Find a bijective function $f: T \to \mathbb{Z}_+$ I have tried to find a function but I can't, how does such function look like?",,['analysis']
23,Question about Hilbert transform(applying plancherel theorem),Question about Hilbert transform(applying plancherel theorem),,"Let $f\in S(\mathbb{R})$(Schwartz function on real line). Then Hilbert transform $H$ of $f$ is defined by $\displaystyle Hf(x)=\lim\limits_{t\rightarrow0}\int_{|y|>t}\frac{1}{y}f(x-y)\,dy$ One can show that this integral exists if $f$ is schwartz. It is known that $H$ is of strong type $(p,p)$ for $p>1$ and weak type $(1,1)$. (It is also known that $H$ cannot be of strong type (1,1).) In most books and references that I looked up so far, they prove weak $L^1$ bound and strong $L^2$ bound of $H$ and use Marcinkiewicz interpolation for finishing touch. Especially in their proof of strong $L^2$ bound, They argue as follows. $\lVert Hf\rVert_2=\lVert (Hf)^\hat\ \rVert_2=\lVert f^\hat\ \rVert_2=\lVert f\rVert_2$ The first and third equality is by Plancherel's theorem, and the second equality is by the identity $(Hf)^\hat\ (\xi)=-i\rm{sgn}(\xi) f^\hat\ (\xi)$. My question is that, how can I apply Plancherel's theorem on the first equality? Shouldn't I guarantee that $Hf$ is in $L^1\cap L^2$ first to apply it? As written earlier, there is an example where $Hf$ fails to be in $L^1. (f=\chi_{[0,1]})$. I am confused.","Let $f\in S(\mathbb{R})$(Schwartz function on real line). Then Hilbert transform $H$ of $f$ is defined by $\displaystyle Hf(x)=\lim\limits_{t\rightarrow0}\int_{|y|>t}\frac{1}{y}f(x-y)\,dy$ One can show that this integral exists if $f$ is schwartz. It is known that $H$ is of strong type $(p,p)$ for $p>1$ and weak type $(1,1)$. (It is also known that $H$ cannot be of strong type (1,1).) In most books and references that I looked up so far, they prove weak $L^1$ bound and strong $L^2$ bound of $H$ and use Marcinkiewicz interpolation for finishing touch. Especially in their proof of strong $L^2$ bound, They argue as follows. $\lVert Hf\rVert_2=\lVert (Hf)^\hat\ \rVert_2=\lVert f^\hat\ \rVert_2=\lVert f\rVert_2$ The first and third equality is by Plancherel's theorem, and the second equality is by the identity $(Hf)^\hat\ (\xi)=-i\rm{sgn}(\xi) f^\hat\ (\xi)$. My question is that, how can I apply Plancherel's theorem on the first equality? Shouldn't I guarantee that $Hf$ is in $L^1\cap L^2$ first to apply it? As written earlier, there is an example where $Hf$ fails to be in $L^1. (f=\chi_{[0,1]})$. I am confused.",,"['analysis', 'harmonic-analysis']"
24,Every interval in real numbers has a rational and irrational,Every interval in real numbers has a rational and irrational,,"How can you prove the following: where $a\not= b$, show every $[a,b]$ of R has a rational and irrational number. The context for my question is as follows. In my intro calculus class, we were showing that the Dirichlet function is not integrable. This involved showing that the least upper bound of all upper estimates, U, and the greatest lower bound of all lower estimates, L, were not the same (on any partition of any interval.) This involved showing that L=0 because any subinterval $[x_{i-1},x_i]$ contains both a rational and irrational, so $m_i=o$ and thus every lower estimate will be 0. But the fact that every subinterval has a rational and irrational member, though intuitively obvious, was merely stated.","How can you prove the following: where $a\not= b$, show every $[a,b]$ of R has a rational and irrational number. The context for my question is as follows. In my intro calculus class, we were showing that the Dirichlet function is not integrable. This involved showing that the least upper bound of all upper estimates, U, and the greatest lower bound of all lower estimates, L, were not the same (on any partition of any interval.) This involved showing that L=0 because any subinterval $[x_{i-1},x_i]$ contains both a rational and irrational, so $m_i=o$ and thus every lower estimate will be 0. But the fact that every subinterval has a rational and irrational member, though intuitively obvious, was merely stated.",,"['calculus', 'analysis']"
25,Important topics in Matrix Analysis,Important topics in Matrix Analysis,,"I'm doing a course in Matrix analysis, and I'm supposed to prepare a presentation about any topic in Matrix theory. We already covered the book ""Matrix Analysis"" by Horn, so preferably I need a topic that extend the results in that book, or maybe something different. I have an engineering background but I have interest in pure and applied math. My research focus is on control theory, dynamical systems and optimization. I studied real analysis and fundamentals of functional analysis and measure theory. I also took courses in probability theory, Fourier analysis. I have modest knowledge of abstract algebra (structures). I'm looking for suggestion of topics that will relate the common domains of all these courses or perhaps give new insight. The presentation will be 10 mins. I'm expected to spend one day studying that topic.","I'm doing a course in Matrix analysis, and I'm supposed to prepare a presentation about any topic in Matrix theory. We already covered the book ""Matrix Analysis"" by Horn, so preferably I need a topic that extend the results in that book, or maybe something different. I have an engineering background but I have interest in pure and applied math. My research focus is on control theory, dynamical systems and optimization. I studied real analysis and fundamentals of functional analysis and measure theory. I also took courses in probability theory, Fourier analysis. I have modest knowledge of abstract algebra (structures). I'm looking for suggestion of topics that will relate the common domains of all these courses or perhaps give new insight. The presentation will be 10 mins. I'm expected to spend one day studying that topic.",,"['analysis', 'optimization']"
26,Prove there exists a point $c$ such thst $f(c)=c$ for the following function,Prove there exists a point  such thst  for the following function,c f(c)=c,"If $f:\mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function with $f(0)=2$ and $|f'(x)| \leq 1/2$ for all $x$ then there is a point $c$ such that $f(c)=c$ . My Attempt Let $$h(x)=f(x)-x$$ Now at the point $x=0$ $h(0)=f(0)-0=2$ now we need to find a point $x_0$ such that $h(x_0)<0$ and then we can apply the intermediate value theorem,. Take the interval $(0,5)$ by the mean value theorem there exists a $c \in (0,5)$ such that $$f'(c)=\dfrac{f(5)-f(0)}{5-0}=\dfrac{f(5)-2}{5}$$ We know from the question that $$\left|\dfrac{f(5)-2}{5}\right| \leq 1/2$$ and so solving gives us $$-1/2 \leq f(5) \leq 4.5$$ and so for any value in that range $$h(5)=f(5)-5 <0$$ and by intermediate value theorem there exists a $c \in (0,5)$ such that $h(c)=0 \iff f(c)=c$ Is this correct? Any help is much appreciated.","If $f:\mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function with $f(0)=2$ and $|f'(x)| \leq 1/2$ for all $x$ then there is a point $c$ such that $f(c)=c$ . My Attempt Let $$h(x)=f(x)-x$$ Now at the point $x=0$ $h(0)=f(0)-0=2$ now we need to find a point $x_0$ such that $h(x_0)<0$ and then we can apply the intermediate value theorem,. Take the interval $(0,5)$ by the mean value theorem there exists a $c \in (0,5)$ such that $$f'(c)=\dfrac{f(5)-f(0)}{5-0}=\dfrac{f(5)-2}{5}$$ We know from the question that $$\left|\dfrac{f(5)-2}{5}\right| \leq 1/2$$ and so solving gives us $$-1/2 \leq f(5) \leq 4.5$$ and so for any value in that range $$h(5)=f(5)-5 <0$$ and by intermediate value theorem there exists a $c \in (0,5)$ such that $h(c)=0 \iff f(c)=c$ Is this correct? Any help is much appreciated.",,"['analysis', 'continuity', 'proof-verification']"
27,Integral using spherical coordinates,Integral using spherical coordinates,,I am trying to compute the volume of the following set : intersection of cylinder $x^2 + y^2 \leq R$  and sphere $x^2 + y^2 + z^2 \leq 4R^2$. I am having trouble setting up the integral properly after transforming to spherical coordinates I am not sure where the sphere and the cylinder meet and how to compute the volume of that top part. I could use some help. Thank you,I am trying to compute the volume of the following set : intersection of cylinder $x^2 + y^2 \leq R$  and sphere $x^2 + y^2 + z^2 \leq 4R^2$. I am having trouble setting up the integral properly after transforming to spherical coordinates I am not sure where the sphere and the cylinder meet and how to compute the volume of that top part. I could use some help. Thank you,,"['calculus', 'integration', 'analysis', 'multivariable-calculus', 'spherical-coordinates']"
28,"Is there a word for describing ""smoothness"" quantitatively?","Is there a word for describing ""smoothness"" quantitatively?",,"I've long wondered how to ""quantitatively"" describe how smooth a function is. For instance, a 1000 term Fourier series for a the Heaviside step function is technically smooth, as it has infinite derivatives.  But, when compared to the first term, it is much ""spikier"". I often want to find myself saying the 1000-term Fourier series is ""less smooth"" than the single term... but is there a good word for that?","I've long wondered how to ""quantitatively"" describe how smooth a function is. For instance, a 1000 term Fourier series for a the Heaviside step function is technically smooth, as it has infinite derivatives.  But, when compared to the first term, it is much ""spikier"". I often want to find myself saying the 1000-term Fourier series is ""less smooth"" than the single term... but is there a good word for that?",,['analysis']
29,"Accumulation points of $\{ \sqrt{n} - \sqrt{m}: m,n \in \mathbb{N} \}$",Accumulation points of,"\{ \sqrt{n} - \sqrt{m}: m,n \in \mathbb{N} \}","This is my first post on MSE, so, pardon me if I'm not used to the site's rule yet. I'm trying to prepare myself for competitions in the future and I'm trying to improve my problem solving skills. I've come accross this problem from the book ""Problems in mathematical analysis"" by Witkowski and Piotr. It asks us to find the set of accumulation points of $$A=\{ \sqrt{n}-\sqrt{m}: m,n \in \mathbb{N} \}$$ Well, I guess $0$ is one of the accumlation points. Because if we set $n=m+1$ then we'll find a non-constant subsequence that converges $0$. I've been trying to find other non-constant sequences that converge to other real numbers, but I haven't succeeded so far. I'm very hesitant to think that $0$ is the only accumulation point of $A$. The main problem is that $n$ and $m$ must be natural numbers, that is very restricting for my intuition. I tried several subsequences of $\sqrt{n}-\sqrt{m}$ by hand or by using wolfram alpha but either they diverged to infinity or they converged to $0$. So, please, give me some hints. I'm NOT looking for a full solution YET! Just give me some ideas about how a problem solver might attack this problem. If this problem shows up in a competition, how should I analyze it? Should I start by trying several subsequences of it to get a picture or I should think more abstractly? If you're giving a hint, please try to explain how that idea has come to you. Thanks in advance.","This is my first post on MSE, so, pardon me if I'm not used to the site's rule yet. I'm trying to prepare myself for competitions in the future and I'm trying to improve my problem solving skills. I've come accross this problem from the book ""Problems in mathematical analysis"" by Witkowski and Piotr. It asks us to find the set of accumulation points of $$A=\{ \sqrt{n}-\sqrt{m}: m,n \in \mathbb{N} \}$$ Well, I guess $0$ is one of the accumlation points. Because if we set $n=m+1$ then we'll find a non-constant subsequence that converges $0$. I've been trying to find other non-constant sequences that converge to other real numbers, but I haven't succeeded so far. I'm very hesitant to think that $0$ is the only accumulation point of $A$. The main problem is that $n$ and $m$ must be natural numbers, that is very restricting for my intuition. I tried several subsequences of $\sqrt{n}-\sqrt{m}$ by hand or by using wolfram alpha but either they diverged to infinity or they converged to $0$. So, please, give me some hints. I'm NOT looking for a full solution YET! Just give me some ideas about how a problem solver might attack this problem. If this problem shows up in a competition, how should I analyze it? Should I start by trying several subsequences of it to get a picture or I should think more abstractly? If you're giving a hint, please try to explain how that idea has come to you. Thanks in advance.",,"['analysis', 'problem-solving']"
30,How to solve this system of 3 equations with 3 variables?,How to solve this system of 3 equations with 3 variables?,,"I stumbled upon this system with constants $a_{i,j}>0$ that I want to solve for $x,y,z \in\mathbb{R}$: \begin{align} a_{2,1}y+a_{3,1}z=& x(y+z) \\ a_{1,2}x+a_{3,2}z=& y(x+z) \\ a_{1,3}x+a_{2,3}y=& z(x+y) \end{align} I would appreciate help on the following questions: To which class of equations does it belong? Is the solution unique (except the trivial solution (0,0,0))? How do I solve it in theory resp. approximate the solution effectively? Can 1-3 be generalized to n instead of 3 dimensions? Edit: I transformed the system to a matrix equation: Let  \begin{equation} A= \begin{pmatrix} 0 & a_{1,2} & a_{1,3} \\ a_{2,1} & 0 & a_{2,3} \\ a_{3,1} & a_{3,2} & 0 \end{pmatrix}, \end{equation} \begin{equation} B= \begin{pmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} \end{equation} and $\pi=(x,y,z)$. Then the system is just \begin{equation} \langle\pi,\pi B\rangle=\pi A \end{equation}, where $\langle.,.\rangle$ is the elementwise multiplication of two vectors. I found that with an arbitrary starting vector $\pi_0$, the following sequence should converge to the unique solution $\pi$: \begin{equation} \pi_{i+1}:=\frac{\pi_{i}A}{\pi_{i}B}, \end{equation} where the fraction is again elementwise. Can anyone help me prove that? I could imagine a connection to the stationary distribution of irreducible Markov Chains, but I am not sure. Also thankful for references to literature or other posts. Thanks!","I stumbled upon this system with constants $a_{i,j}>0$ that I want to solve for $x,y,z \in\mathbb{R}$: \begin{align} a_{2,1}y+a_{3,1}z=& x(y+z) \\ a_{1,2}x+a_{3,2}z=& y(x+z) \\ a_{1,3}x+a_{2,3}y=& z(x+y) \end{align} I would appreciate help on the following questions: To which class of equations does it belong? Is the solution unique (except the trivial solution (0,0,0))? How do I solve it in theory resp. approximate the solution effectively? Can 1-3 be generalized to n instead of 3 dimensions? Edit: I transformed the system to a matrix equation: Let  \begin{equation} A= \begin{pmatrix} 0 & a_{1,2} & a_{1,3} \\ a_{2,1} & 0 & a_{2,3} \\ a_{3,1} & a_{3,2} & 0 \end{pmatrix}, \end{equation} \begin{equation} B= \begin{pmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} \end{equation} and $\pi=(x,y,z)$. Then the system is just \begin{equation} \langle\pi,\pi B\rangle=\pi A \end{equation}, where $\langle.,.\rangle$ is the elementwise multiplication of two vectors. I found that with an arbitrary starting vector $\pi_0$, the following sequence should converge to the unique solution $\pi$: \begin{equation} \pi_{i+1}:=\frac{\pi_{i}A}{\pi_{i}B}, \end{equation} where the fraction is again elementwise. Can anyone help me prove that? I could imagine a connection to the stationary distribution of irreducible Markov Chains, but I am not sure. Also thankful for references to literature or other posts. Thanks!",,"['linear-algebra', 'analysis', 'computer-science', 'numerical-linear-algebra']"
31,How prove $|f'(x)|\le 4$ if $|f(x)|\le 1$,How prove  if,|f'(x)|\le 4 |f(x)|\le 1,"Let $f(x)$ be differentiable on $\Bbb R$, and for any $x_{0}\in \Bbb R$,    $$0<f'(x_{0}+x)-f'(x_{0})<4x \qquad(x>0)$$   and if $|f(x)|\le 1$, show that   $|f'(x)|\le 4$. I tried to use Langrange’s theorem, but I couldn’t. This is an exam problem from yesterday. Thank you for you help!","Let $f(x)$ be differentiable on $\Bbb R$, and for any $x_{0}\in \Bbb R$,    $$0<f'(x_{0}+x)-f'(x_{0})<4x \qquad(x>0)$$   and if $|f(x)|\le 1$, show that   $|f'(x)|\le 4$. I tried to use Langrange’s theorem, but I couldn’t. This is an exam problem from yesterday. Thank you for you help!",,[]
32,Dirichlet series expansion?,Dirichlet series expansion?,,"When an analytic function $f(x)$ is given, we can easily obtain the coefficient of $x^n$ in a power series expansion of it. I'd like to know if there exists something similar for Dirichlet series. Is there a systematic way to get the coefficient of $n^{-x}$ when a function $f(x)$, if it can be represented as a Dirichlet series, is given?","When an analytic function $f(x)$ is given, we can easily obtain the coefficient of $x^n$ in a power series expansion of it. I'd like to know if there exists something similar for Dirichlet series. Is there a systematic way to get the coefficient of $n^{-x}$ when a function $f(x)$, if it can be represented as a Dirichlet series, is given?",,"['analysis', 'generating-functions']"
33,$m(E)=0$ then $m(\lbrace x^2 : x\in E\rbrace$?,then ?,m(E)=0 m(\lbrace x^2 : x\in E\rbrace,"Let E be a subset of $\mathbb{R}$ with lebesgue measure zero. How can I prove that $\lbrace x^2 : x\in E\rbrace$ also has lebesgue measure zero? Let $\epsilon>0$, I should find a cover of $\lbrace x^2 : x\in E\rbrace$ with total length at most $\epsilon$. since $m(E)=0$ so there exists a countable collection of intervals $\{ (a_k, b_k)\}$ such that $\sum b_k-a_k<\epsilon$. Then what is the next step? How can I find a cover for $\lbrace x^2 : x\in E\rbrace$?  If without loss of generality we think that $E\subset \mathbb{R}^+$ then $\bigcup (a_k^2, b_k^2)$ is a cover for $\lbrace x^2 : x\in E\rbrace$. But how to prove $\sum b_k^2-a_k^2<\epsilon$?","Let E be a subset of $\mathbb{R}$ with lebesgue measure zero. How can I prove that $\lbrace x^2 : x\in E\rbrace$ also has lebesgue measure zero? Let $\epsilon>0$, I should find a cover of $\lbrace x^2 : x\in E\rbrace$ with total length at most $\epsilon$. since $m(E)=0$ so there exists a countable collection of intervals $\{ (a_k, b_k)\}$ such that $\sum b_k-a_k<\epsilon$. Then what is the next step? How can I find a cover for $\lbrace x^2 : x\in E\rbrace$?  If without loss of generality we think that $E\subset \mathbb{R}^+$ then $\bigcup (a_k^2, b_k^2)$ is a cover for $\lbrace x^2 : x\in E\rbrace$. But how to prove $\sum b_k^2-a_k^2<\epsilon$?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
34,"$A(x) = \int_a^x f(t) dt$ in [a,b],let c in (a,b),if $f'$ is continuous at c then A' is continuous at c?","in [a,b],let c in (a,b),if  is continuous at c then A' is continuous at c?",A(x) = \int_a^x f(t) dt f',"this is an exercise in Apostol calculus I page 210. Given a function $f$ such that the integral $A(x) = \int_a^x f(t)dt$ exists for each $x$ in an interval $[a,b]$.Let $c$ be a point in $(a,b)$.Prove that if $f'$ is continuous at $c$.then $A'$ is continuous at $c$. Attempt: f' is continuous at c,so there's a interval $c- \delta<x<c+ \delta$ such that $f'(x)$ exists and  $|f'(x)-f'(c)|<\epsilon$, this imply $f$ is continuous in the interval.By the first fundamental theorem of calculus.$A'(x)=f(x)$ in $(c- \delta,c+ \delta)$.because $f(x)$ is continuous at $c$. So does A'. I feel my statement is quite strange and may have  something wrong.Hope someone could give a clear explaination to me.","this is an exercise in Apostol calculus I page 210. Given a function $f$ such that the integral $A(x) = \int_a^x f(t)dt$ exists for each $x$ in an interval $[a,b]$.Let $c$ be a point in $(a,b)$.Prove that if $f'$ is continuous at $c$.then $A'$ is continuous at $c$. Attempt: f' is continuous at c,so there's a interval $c- \delta<x<c+ \delta$ such that $f'(x)$ exists and  $|f'(x)-f'(c)|<\epsilon$, this imply $f$ is continuous in the interval.By the first fundamental theorem of calculus.$A'(x)=f(x)$ in $(c- \delta,c+ \delta)$.because $f(x)$ is continuous at $c$. So does A'. I feel my statement is quite strange and may have  something wrong.Hope someone could give a clear explaination to me.",,"['calculus', 'real-analysis', 'analysis']"
35,Commutativity of integration and Taylor expansion of the integrand in an integral,Commutativity of integration and Taylor expansion of the integrand in an integral,,"I am baffled with a seemingly a straightforward problem. Suppose we are given the following integral: \begin{equation}  f(a)\,=\,\int_{0}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}, \end{equation} and we want to determine the dependence of $f(a)$ on $a$ when $a\ll 1$. Apparently this integral can be solved using Mathematica. Taylor expanding the result, which is a Meijer G-function, it turns out that $f(a)$ is analytic in $a$. In the specific case of this integral, it's possible to use a trick so that one can directly Taylor expand the integrand (Taylor expanding the integrand of $f(a)-f(0)$ after $x\to x'=a x$). But I'm not interested in this particular integral and am mentioning this as a simple example. Now here is what I find paradoxical: Let's try to do this in a more pedestrian way by breaking up the integration range and Taylor expanding the exponential when x is small and the rest of the integrand when x is large. Interchanging the integration and summation is justified by Fubini's theorem (if I'm not mistaken, $\int \sum |c_n(x)| <\infty$ or $\sum\int |c_n(x)|<\infty$). Now, breaking up the integral can be done in two ways. Either, \begin{equation} f(a)=\int_{0}^{1} \frac{x^4}{x^4+a^4} e^{-x} + \int_{1}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,, \end{equation} or \begin{equation} f(a)=\int_{0}^{2a} \frac{x^4}{x^4+a^4} e^{-x} + \int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,. \end{equation} $\frac{x^4}{x^4+a^4}$ can be Taylor expanded and the integration ranges are within the convergence radius in both cases. The Taylor expansion in both cases results in a series that's uniformly convergent and therefore one should be able to interchange integration and summation. The former case, where the integration range is broken up at $1$, gives an analytic result in $a$. Curiously, the latter (breaking up the integral at $2a$) gives non-analytic terms (see below) and I cannot figure out how to reconcile this with the exact result. The lower integration ranges in both cases give analytic expressions in $a$. \begin{equation} \int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,=\, \sum_{n=0}^{\infty} \int_{2a}^{\infty} \frac{(-1)^n a^{4n}}{x^{4n}}e^{-x} \,=\, \sum_{n=0}^{\infty}(-1)^{n}a^{4n}\Gamma(1-4n,2a). \end{equation} Using the series expansion of the upper incomplete $\Gamma$-function, there will be terms of the form $\frac{-(-1)^n}{(4n-1)!} a^{4n} \ln(a)$. I would like to know whether the Taylor expansion is not justified (if so, why precisely), or, although hard to imagine, is it that somehow these non-analytic terms sum up to an analytic result. Thanks.","I am baffled with a seemingly a straightforward problem. Suppose we are given the following integral: \begin{equation}  f(a)\,=\,\int_{0}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}, \end{equation} and we want to determine the dependence of $f(a)$ on $a$ when $a\ll 1$. Apparently this integral can be solved using Mathematica. Taylor expanding the result, which is a Meijer G-function, it turns out that $f(a)$ is analytic in $a$. In the specific case of this integral, it's possible to use a trick so that one can directly Taylor expand the integrand (Taylor expanding the integrand of $f(a)-f(0)$ after $x\to x'=a x$). But I'm not interested in this particular integral and am mentioning this as a simple example. Now here is what I find paradoxical: Let's try to do this in a more pedestrian way by breaking up the integration range and Taylor expanding the exponential when x is small and the rest of the integrand when x is large. Interchanging the integration and summation is justified by Fubini's theorem (if I'm not mistaken, $\int \sum |c_n(x)| <\infty$ or $\sum\int |c_n(x)|<\infty$). Now, breaking up the integral can be done in two ways. Either, \begin{equation} f(a)=\int_{0}^{1} \frac{x^4}{x^4+a^4} e^{-x} + \int_{1}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,, \end{equation} or \begin{equation} f(a)=\int_{0}^{2a} \frac{x^4}{x^4+a^4} e^{-x} + \int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,. \end{equation} $\frac{x^4}{x^4+a^4}$ can be Taylor expanded and the integration ranges are within the convergence radius in both cases. The Taylor expansion in both cases results in a series that's uniformly convergent and therefore one should be able to interchange integration and summation. The former case, where the integration range is broken up at $1$, gives an analytic result in $a$. Curiously, the latter (breaking up the integral at $2a$) gives non-analytic terms (see below) and I cannot figure out how to reconcile this with the exact result. The lower integration ranges in both cases give analytic expressions in $a$. \begin{equation} \int_{2a}^{\infty} \frac{x^4}{x^4+a^4} e^{-x}\,=\, \sum_{n=0}^{\infty} \int_{2a}^{\infty} \frac{(-1)^n a^{4n}}{x^{4n}}e^{-x} \,=\, \sum_{n=0}^{\infty}(-1)^{n}a^{4n}\Gamma(1-4n,2a). \end{equation} Using the series expansion of the upper incomplete $\Gamma$-function, there will be terms of the form $\frac{-(-1)^n}{(4n-1)!} a^{4n} \ln(a)$. I would like to know whether the Taylor expansion is not justified (if so, why precisely), or, although hard to imagine, is it that somehow these non-analytic terms sum up to an analytic result. Thanks.",,"['real-analysis', 'analysis', 'analyticity']"
36,Proving continuity of exp(x),Proving continuity of exp(x),,"Well, my teacher went through a method of proving continuity of $\exp(x)$ which I don't like, so I tried to go about it a different way: We have proved the following (which I use) $\exp(x+y) = \exp(x)\exp(y)$ $\exp(0) = 1$ $\exp(x) \geq 1 + x \forall x \in \mathbb{R}$ firstly, I prove it's continuous at $ x = 0$ for $x \leq 0$ $\exp(-x) = \dfrac{1}{\exp(x)}$ $\exp(-x) = 1 + (-x) + \dfrac{(-x)^2}{2!} +... = 1 + $ +ve terms so $\exp(-x) \geq 1$ so $\exp(x) \leq 1$ $\Rightarrow  1 + x \leq \exp(x) \leq 1$ then by sandwich theorem $\lim_{x\to0^-}\exp(x) = 1$ for $x \geq 0 $ $-x \leq 0 \Rightarrow 1 - x \leq \exp(-x) = \dfrac{1}{\exp(x)} \leq 1$ by subbing in (-x) to the above therefore if $ 0 \leq x < 1 $ $\dfrac{1}{1-x} \geq \exp(x) \geq 1$ therefore $\lim_{x\to0^+}\exp(x) = 1$ therefore $\lim_{x\to 0} \exp(x) = 1$, I have a query at this point, can I consider $0 \leq x < 1$ and conclude $\lim_{x\to0+}\exp(x) = 1$? Moving on, showing it is continuous for any $c \in \mathbb{R}$ assume a sequence $(x_n)$ is a seq. with $x_n \to c$ so $(x_n - c) \to 0 \Rightarrow \exp(x_n -c) \to 1$ (by the composition function theorem, and the step above) $\exp(x_n) = \exp((x_n -c) + c) = \exp(x_n -c)\exp(c) \to \exp(c) $ how can I conclude from here?","Well, my teacher went through a method of proving continuity of $\exp(x)$ which I don't like, so I tried to go about it a different way: We have proved the following (which I use) $\exp(x+y) = \exp(x)\exp(y)$ $\exp(0) = 1$ $\exp(x) \geq 1 + x \forall x \in \mathbb{R}$ firstly, I prove it's continuous at $ x = 0$ for $x \leq 0$ $\exp(-x) = \dfrac{1}{\exp(x)}$ $\exp(-x) = 1 + (-x) + \dfrac{(-x)^2}{2!} +... = 1 + $ +ve terms so $\exp(-x) \geq 1$ so $\exp(x) \leq 1$ $\Rightarrow  1 + x \leq \exp(x) \leq 1$ then by sandwich theorem $\lim_{x\to0^-}\exp(x) = 1$ for $x \geq 0 $ $-x \leq 0 \Rightarrow 1 - x \leq \exp(-x) = \dfrac{1}{\exp(x)} \leq 1$ by subbing in (-x) to the above therefore if $ 0 \leq x < 1 $ $\dfrac{1}{1-x} \geq \exp(x) \geq 1$ therefore $\lim_{x\to0^+}\exp(x) = 1$ therefore $\lim_{x\to 0} \exp(x) = 1$, I have a query at this point, can I consider $0 \leq x < 1$ and conclude $\lim_{x\to0+}\exp(x) = 1$? Moving on, showing it is continuous for any $c \in \mathbb{R}$ assume a sequence $(x_n)$ is a seq. with $x_n \to c$ so $(x_n - c) \to 0 \Rightarrow \exp(x_n -c) \to 1$ (by the composition function theorem, and the step above) $\exp(x_n) = \exp((x_n -c) + c) = \exp(x_n -c)\exp(c) \to \exp(c) $ how can I conclude from here?",,"['analysis', 'continuity', 'exponential-function']"
37,Taylor expansions of harmonic functions.,Taylor expansions of harmonic functions.,,"Let $D \subset \mathbb{R}^2$ be the unit open disc. Note that any harmonic function on $D$ is real analytic. How can one prove that there exits a constant $C>0$ such that the Taylor expansion of any harmonic function on $D$ converges on the open disc of radius $C$? In other words, there is a uniform radius of convergence.","Let $D \subset \mathbb{R}^2$ be the unit open disc. Note that any harmonic function on $D$ is real analytic. How can one prove that there exits a constant $C>0$ such that the Taylor expansion of any harmonic function on $D$ converges on the open disc of radius $C$? In other words, there is a uniform radius of convergence.",,"['analysis', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
38,Reconciling the two statements,Reconciling the two statements,,"I had just recently picked up Functional Analysis so my problem may sound trivial. But I appreciate any help. I am having trouble to reconcile the two statements (said to be true in my notes): let $B \subset$ X*, dual space of X and define $B^o$ and $B^z$ to be the set of its annihilators and pre-annihilators respectively. 1) $(B^z)^o$ is the weak* closure of $R:=$  convex hull of $B$. 2) The norm closure of spanB is  a strict subset of $(B^z)^o$. I feel one of the two is wrong because 1) seems to contradict 2). Here's my reasoning. $R$ is a subset of spanB, hence norm closure of $R$ is contained in the norm closure of spanB. But the weak* topology is contained in the weak topology of X* i.e. the smallest topology to have X** to be continuous. So weak* closure of R is weak closed implying it's also normed closed, as R is convex. So the norm closure of $R$ is in norm closure of spanB, which conflicts with 2) if 1) is true. Thank you for any clarifications.","I had just recently picked up Functional Analysis so my problem may sound trivial. But I appreciate any help. I am having trouble to reconcile the two statements (said to be true in my notes): let $B \subset$ X*, dual space of X and define $B^o$ and $B^z$ to be the set of its annihilators and pre-annihilators respectively. 1) $(B^z)^o$ is the weak* closure of $R:=$  convex hull of $B$. 2) The norm closure of spanB is  a strict subset of $(B^z)^o$. I feel one of the two is wrong because 1) seems to contradict 2). Here's my reasoning. $R$ is a subset of spanB, hence norm closure of $R$ is contained in the norm closure of spanB. But the weak* topology is contained in the weak topology of X* i.e. the smallest topology to have X** to be continuous. So weak* closure of R is weak closed implying it's also normed closed, as R is convex. So the norm closure of $R$ is in norm closure of spanB, which conflicts with 2) if 1) is true. Thank you for any clarifications.",,['analysis']
39,Cannot understand some parts of proof for R be a countable union of closed sets,Cannot understand some parts of proof for R be a countable union of closed sets,,"Im reading chapter9 Category, Real Analysis, Carothers, 1ed, talking about discontinuous functions of metric space. Here is a proof for a theorem that R be a countable union of closed sets,: Baire's theorem involved is,: What is the ""entire open interval""? Is ""$E_n$ contains an interval"" the same as interior of $E_n$ is not empty? if $E_n$ has empty interior, that means complement of $G_n$ has empty interior, then equivalently, this will imply that $G_n$ is dense and make a contradiction here, is that right? What im still confusing about is how can we suppose if $\mathbb{R}$ = a countable union of closed sets?","Im reading chapter9 Category, Real Analysis, Carothers, 1ed, talking about discontinuous functions of metric space. Here is a proof for a theorem that R be a countable union of closed sets,: Baire's theorem involved is,: What is the ""entire open interval""? Is ""$E_n$ contains an interval"" the same as interior of $E_n$ is not empty? if $E_n$ has empty interior, that means complement of $G_n$ has empty interior, then equivalently, this will imply that $G_n$ is dense and make a contradiction here, is that right? What im still confusing about is how can we suppose if $\mathbb{R}$ = a countable union of closed sets?",,"['real-analysis', 'analysis', 'baire-category']"
40,Easy question about fourier transform,Easy question about fourier transform,,I think this is an easy one ... but I just can't find an answer. Assume $f : \mathbb{R} \to \mathbb{R}$ is a $n+2$ times differentiable function and and all drivatives up to the order $n+2$ are in $L_1$. From calculus we know  that $\mathcal{F}(D^{n+2} f )(x) =  (ix)^{n+2}\hat{f}$. Here $\mathcal{F}$ and $\hat{\cdot}$ denote the Fourier-transform. By the Riemann-Lebesgue lemma we have $(ix)^{n+2}\mathcal{F}(f) \to 0$ for $|x| \to \infty$. Thus by integration we can show that $x \mapsto (ix)^{n}\hat{f}(x)$ is a $L_1$ function. Question : I want to know if there are weaker conditions on the function $f$ that imply  $\int_{-\infty}^{\infty} |x|^{n}|\hat{f}(x)| dx < \infty$. Thank you !,I think this is an easy one ... but I just can't find an answer. Assume $f : \mathbb{R} \to \mathbb{R}$ is a $n+2$ times differentiable function and and all drivatives up to the order $n+2$ are in $L_1$. From calculus we know  that $\mathcal{F}(D^{n+2} f )(x) =  (ix)^{n+2}\hat{f}$. Here $\mathcal{F}$ and $\hat{\cdot}$ denote the Fourier-transform. By the Riemann-Lebesgue lemma we have $(ix)^{n+2}\mathcal{F}(f) \to 0$ for $|x| \to \infty$. Thus by integration we can show that $x \mapsto (ix)^{n}\hat{f}(x)$ is a $L_1$ function. Question : I want to know if there are weaker conditions on the function $f$ that imply  $\int_{-\infty}^{\infty} |x|^{n}|\hat{f}(x)| dx < \infty$. Thank you !,,"['analysis', 'fourier-analysis']"
41,"$\{(x,y):x\in{U},y>f(x)\}$ is an open subset of $\mathbb{R}^{n+1}$",is an open subset of,"\{(x,y):x\in{U},y>f(x)\} \mathbb{R}^{n+1}","Let $f$ be a continuous real-valued function defined on an open subset $U$ of $\mathbb{R}^n$. Show that $\{(x,y):x\in{U},y>f(x)\}$ is an open subset of $\mathbb{R}^{n+1}$ I known $f(x)$ is open by using theorem of equivalent between $f$ mapping $S$ is subset of $\mathbb{R}^n$ and $f$ is continuous on $S$. $f$ define on $U$ thus $f(x)$ is open of $\mathbb{R}^n$. Now, I want to know how to prove $f(x)$ is open subset of $\mathbb{R}^{n+1}$. Thanks.","Let $f$ be a continuous real-valued function defined on an open subset $U$ of $\mathbb{R}^n$. Show that $\{(x,y):x\in{U},y>f(x)\}$ is an open subset of $\mathbb{R}^{n+1}$ I known $f(x)$ is open by using theorem of equivalent between $f$ mapping $S$ is subset of $\mathbb{R}^n$ and $f$ is continuous on $S$. $f$ define on $U$ thus $f(x)$ is open of $\mathbb{R}^n$. Now, I want to know how to prove $f(x)$ is open subset of $\mathbb{R}^{n+1}$. Thanks.",,"['real-analysis', 'analysis']"
42,"If $f^{-1}(G)$ is open in $X$ for every open set $G$ in $Y$, then $f$ is continuous. Question on proof.","If  is open in  for every open set  in , then  is continuous. Question on proof.",f^{-1}(G) X G Y f,"Let $X,Y$ be metric spaces and $f:X\rightarrow Y$. If $f^{-1}(G)$ is open in $X$ for every open set $G$ in $Y$, then $f$ is continuous. The text I am using proves this proposition like so: Suppose that $f$ is discontinuous at some point $a\in X$. Then there is an $\epsilon >0$ such that for every $\delta>0$, there is an $x\in X$ with $d(x,a)<\delta$ and $d(f(x),f(a))\geq \epsilon.$ It follows that, although $a$ belongs to the inverse image of the open set $B_{\epsilon}(f(a))$ under $f$, the inverse image does not contain $B_{\delta}(a)$ for any $\delta>0$, so it is not open. I am having trouble understanding how $B_{\delta}(a)$ is not contained in $f^{-1}(B_{\epsilon}(f(a)))$. If $d(x,a)<\delta$ implies  $d(f(x),f(a))\geq \epsilon$. My questions are: 1) What does the preimage of $f^{-1}(B_{\epsilon}(f(a)))$ look like? Is it a set with radius greater than $\delta$ or less than $\delta$? How could I see this if I were to draw a picture? This answer may clarify questions 2) and 3). 2) If $B_{\delta}(a)$ is not contained in $f^{1}(B_{\epsilon}(f(a)))$ then $B_{\delta}(a)$ is a larger set than $f^{-1}(B_{\epsilon}(f(a)))$. Is this correct? 3) If so doesn't this mean that $f^{-1}(B_{\epsilon}(f(a)))$ maps points inside the $B_{\delta}(a)$? Thank you for any help and comments!","Let $X,Y$ be metric spaces and $f:X\rightarrow Y$. If $f^{-1}(G)$ is open in $X$ for every open set $G$ in $Y$, then $f$ is continuous. The text I am using proves this proposition like so: Suppose that $f$ is discontinuous at some point $a\in X$. Then there is an $\epsilon >0$ such that for every $\delta>0$, there is an $x\in X$ with $d(x,a)<\delta$ and $d(f(x),f(a))\geq \epsilon.$ It follows that, although $a$ belongs to the inverse image of the open set $B_{\epsilon}(f(a))$ under $f$, the inverse image does not contain $B_{\delta}(a)$ for any $\delta>0$, so it is not open. I am having trouble understanding how $B_{\delta}(a)$ is not contained in $f^{-1}(B_{\epsilon}(f(a)))$. If $d(x,a)<\delta$ implies  $d(f(x),f(a))\geq \epsilon$. My questions are: 1) What does the preimage of $f^{-1}(B_{\epsilon}(f(a)))$ look like? Is it a set with radius greater than $\delta$ or less than $\delta$? How could I see this if I were to draw a picture? This answer may clarify questions 2) and 3). 2) If $B_{\delta}(a)$ is not contained in $f^{1}(B_{\epsilon}(f(a)))$ then $B_{\delta}(a)$ is a larger set than $f^{-1}(B_{\epsilon}(f(a)))$. Is this correct? 3) If so doesn't this mean that $f^{-1}(B_{\epsilon}(f(a)))$ maps points inside the $B_{\delta}(a)$? Thank you for any help and comments!",,"['analysis', 'proof-writing', 'continuity', 'self-learning']"
43,"Analysis, Density of Rational Numbers","Analysis, Density of Rational Numbers",,"Suppose $p/q$ and $k/l$ are rational numbers with $\left|(p/q - k/l)\right|< {1}/{ql}$.  Prove $p/q = k/l$. Similarly, let $p/q$ be a fixed rational number and suppose $k/l$ is a rational number with $0 < \left|p/q - k/l \right| < 1/qm$ ... for some natural number m.  Show that $m < l$. The work I've done with these is just algebra to split up the absolute value inequalities but I can't logically work out the problems.  I'd prefer tips over a complete answer to the problem as these are for homework.","Suppose $p/q$ and $k/l$ are rational numbers with $\left|(p/q - k/l)\right|< {1}/{ql}$.  Prove $p/q = k/l$. Similarly, let $p/q$ be a fixed rational number and suppose $k/l$ is a rational number with $0 < \left|p/q - k/l \right| < 1/qm$ ... for some natural number m.  Show that $m < l$. The work I've done with these is just algebra to split up the absolute value inequalities but I can't logically work out the problems.  I'd prefer tips over a complete answer to the problem as these are for homework.",,"['real-analysis', 'analysis', 'rational-numbers']"
44,"If $a\ge b\ge-c\ge0$, is $\sqrt[3]{a-b-c}\ge\sqrt[3]{a}-\sqrt[3]{b}-\sqrt[3]{c}$?","If , is ?",a\ge b\ge-c\ge0 \sqrt[3]{a-b-c}\ge\sqrt[3]{a}-\sqrt[3]{b}-\sqrt[3]{c},Let $a\ge b\ge-c\ge0$. Is it true that $\sqrt[3]{a-b-c}\ge\sqrt[3]{a}-\sqrt[3]{b}-\sqrt[3]{c}$?,Let $a\ge b\ge-c\ge0$. Is it true that $\sqrt[3]{a-b-c}\ge\sqrt[3]{a}-\sqrt[3]{b}-\sqrt[3]{c}$?,,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'inequality']"
45,Inversion of a power series without a linear term,Inversion of a power series without a linear term,,"Could someone explain me how to invert $$ z = y e^{-y} = e^{-1} - \frac{1}{2e}(y - 1)^2 + \frac{1}{3e}(y - 1)^3 - \frac{1}{8e}(y - 1)^4 + \cdots $$ around $y=1, z=e^{-1}$, so that $y$ is expressed as a series of $(1 - ez)$ ? This is a part of example VI.8 in ""Analytic combinatorics"" by Flajolet and Sedgewick. They skips the details of the inversion process. Actually I am not so familiar with manipulating series expansions. If someone could give some details of the method, I'll greatly appreciate for it.","Could someone explain me how to invert $$ z = y e^{-y} = e^{-1} - \frac{1}{2e}(y - 1)^2 + \frac{1}{3e}(y - 1)^3 - \frac{1}{8e}(y - 1)^4 + \cdots $$ around $y=1, z=e^{-1}$, so that $y$ is expressed as a series of $(1 - ez)$ ? This is a part of example VI.8 in ""Analytic combinatorics"" by Flajolet and Sedgewick. They skips the details of the inversion process. Actually I am not so familiar with manipulating series expansions. If someone could give some details of the method, I'll greatly appreciate for it.",,"['calculus', 'analysis', 'generating-functions']"
46,For what kind of a subset its sums equal $\mathbb{R}^4$,For what kind of a subset its sums equal,\mathbb{R}^4,"For short, suppose $a,b$ are real numbers. Let $A=\{(\cos(at), \cos(bt), \sin(at), \sin(bt))\mid t\in \mathbb{R}\}$. Let $B=\sum A=\{\sum_{i=1}^n x_i\mid x_i\in A, n \geq 1\}$. For what values $a,b$, $B$ equals $\mathbb{R}^4$? In general, what conditions can we impose to a subset $A$  of $\mathbb{R}^n$,  such that the sums of $A$ is the whole space? Any references, suggestions are appreciated. Thanks!","For short, suppose $a,b$ are real numbers. Let $A=\{(\cos(at), \cos(bt), \sin(at), \sin(bt))\mid t\in \mathbb{R}\}$. Let $B=\sum A=\{\sum_{i=1}^n x_i\mid x_i\in A, n \geq 1\}$. For what values $a,b$, $B$ equals $\mathbb{R}^4$? In general, what conditions can we impose to a subset $A$  of $\mathbb{R}^n$,  such that the sums of $A$ is the whole space? Any references, suggestions are appreciated. Thanks!",,['analysis']
47,left-invariant n-form and metric on a Lie group,left-invariant n-form and metric on a Lie group,,"These two questions are from my exam practice question sets , which are quite similar. I got some problem understanding and solving both of them . For (a) , I can only substite $dx\wedge dy\wedge dz$ with $d(-x)\wedge d(e^{-x}(xz-y))\wedge d(-e^{-x}z)$ and it comes to $e^{-2x}dx\wedge dy \wedge dz$ by direct computation. I wonder if there is a more general method to solve the question. e.g compute a general form using left-variance property and then use the value of $\Omega_0$ to determine the exact form. But I coudnt figure it out. Thanks for any help ! (a)Let G denote the Lie group with underlying smooth manifold $R^3$ and group operation  $\star$ defined by $(x,y,z)\star(x',y',z'):=(x+x',y+e^xy'+ xe^xz',z+e^xz').$ Compute the unique n-form $\Omega \in \Gamma(\wedge^3 T^\ast M)$ such that $\Omega_0 = dx\wedge dy\wedge dz$ and $L^\ast_g\Omega=\Omega$.(Elementary algebra, which you need not reproduce here, gives that the inverse in G is given by $(x,y,z)^{-1}=(-x,e^{-x}(xz-y),-e^{-x}z$)). (b)Let $H_3$ denote the smooth manifold $R^3$ endowed with the group operation $(a,b,c)\ast(p,q,r):=(a+p,b+q,c+r+aq)$.Checking shows that $H_3$ is a Lie group with the identiy (0,0,0), which we just denote 0. Compute the left-invariant metric g on $H_3$ whose value at the identity is $g|_0 = (dx\otimes dx+dy\otimes dy+dz\otimes dz)$","These two questions are from my exam practice question sets , which are quite similar. I got some problem understanding and solving both of them . For (a) , I can only substite $dx\wedge dy\wedge dz$ with $d(-x)\wedge d(e^{-x}(xz-y))\wedge d(-e^{-x}z)$ and it comes to $e^{-2x}dx\wedge dy \wedge dz$ by direct computation. I wonder if there is a more general method to solve the question. e.g compute a general form using left-variance property and then use the value of $\Omega_0$ to determine the exact form. But I coudnt figure it out. Thanks for any help ! (a)Let G denote the Lie group with underlying smooth manifold $R^3$ and group operation  $\star$ defined by $(x,y,z)\star(x',y',z'):=(x+x',y+e^xy'+ xe^xz',z+e^xz').$ Compute the unique n-form $\Omega \in \Gamma(\wedge^3 T^\ast M)$ such that $\Omega_0 = dx\wedge dy\wedge dz$ and $L^\ast_g\Omega=\Omega$.(Elementary algebra, which you need not reproduce here, gives that the inverse in G is given by $(x,y,z)^{-1}=(-x,e^{-x}(xz-y),-e^{-x}z$)). (b)Let $H_3$ denote the smooth manifold $R^3$ endowed with the group operation $(a,b,c)\ast(p,q,r):=(a+p,b+q,c+r+aq)$.Checking shows that $H_3$ is a Lie group with the identiy (0,0,0), which we just denote 0. Compute the left-invariant metric g on $H_3$ whose value at the identity is $g|_0 = (dx\otimes dx+dy\otimes dy+dz\otimes dz)$",,"['analysis', 'differential-geometry', 'lie-algebras', 'riemannian-geometry']"
48,The geometric interpretation [duplicate],The geometric interpretation [duplicate],,"This question already has answers here : Stieltjes Integral meaning. (3 answers) Closed 11 years ago . In the course of mathematical analysis, there was one problem that i excited to know more about it: What is the geometric interpretation of $$ \int_a^b f(x)\,d(\alpha(x)) $$ and $\alpha(x)$ is function in $[a,b]$","This question already has answers here : Stieltjes Integral meaning. (3 answers) Closed 11 years ago . In the course of mathematical analysis, there was one problem that i excited to know more about it: What is the geometric interpretation of $$ \int_a^b f(x)\,d(\alpha(x)) $$ and $\alpha(x)$ is function in $[a,b]$",,"['analysis', 'integration']"
49,Question on Showing points of discontinuities of a function are removable (or not),Question on Showing points of discontinuities of a function are removable (or not),,"The question is as follows: Given function: $F(x,y)=\frac{x + 2y}{sin(x+y) - cos(x-y)}$ Tasks: a/ Find points of discontinuities b/ Decide if the points (of   discontinuities) from part a are removable Here is my work so far: (1) For part a, I think the points of discontinuities should have form $(0, \frac{\pi}{4} + n\pi)$ or $(\frac{\pi}{4} + n\pi, 0)$ , since they make the denominator undefined.  For convenience of part b, I choose to specifically deal with the point $(0, \frac{\pi}{4})$ (2) Recall definition: A point of discontinuity $x_0$ is removable if the limits of the function under certain path are equal to each other, as they are ""close"" to $x_0$.  In particular, if the function is 1 dimensional, we get the notion of ""left"" and ""right"" limits.  But here we talk about paths of any possible direction.  However, these limits are not equal to $f(x_0)$, which can be defined or undefined. (3) I'm having trouble of ""finding"" such paths @_@ I come across with these two, by fix x-coordinate and vary y-coordinate: $F(x, x^2 - \frac{\pi}{4})$ and $F(x, x^2 - x - \frac{\pi}{4})$ They both have limit to be $\frac{\pi}{2\sqrt(2)}$ as x approaches 0 (by my calculation) But what I can say about these results?  I feel that discontinuities of $F(x,y)$ should be not removable, but I don't know if my thought is correct. Would someone please help me on this question? Thank you in advance ^^","The question is as follows: Given function: $F(x,y)=\frac{x + 2y}{sin(x+y) - cos(x-y)}$ Tasks: a/ Find points of discontinuities b/ Decide if the points (of   discontinuities) from part a are removable Here is my work so far: (1) For part a, I think the points of discontinuities should have form $(0, \frac{\pi}{4} + n\pi)$ or $(\frac{\pi}{4} + n\pi, 0)$ , since they make the denominator undefined.  For convenience of part b, I choose to specifically deal with the point $(0, \frac{\pi}{4})$ (2) Recall definition: A point of discontinuity $x_0$ is removable if the limits of the function under certain path are equal to each other, as they are ""close"" to $x_0$.  In particular, if the function is 1 dimensional, we get the notion of ""left"" and ""right"" limits.  But here we talk about paths of any possible direction.  However, these limits are not equal to $f(x_0)$, which can be defined or undefined. (3) I'm having trouble of ""finding"" such paths @_@ I come across with these two, by fix x-coordinate and vary y-coordinate: $F(x, x^2 - \frac{\pi}{4})$ and $F(x, x^2 - x - \frac{\pi}{4})$ They both have limit to be $\frac{\pi}{2\sqrt(2)}$ as x approaches 0 (by my calculation) But what I can say about these results?  I feel that discontinuities of $F(x,y)$ should be not removable, but I don't know if my thought is correct. Would someone please help me on this question? Thank you in advance ^^",,['analysis']
50,About inequalities (general),About inequalities (general),,"I wonder if there is a sharper inequality than Hölder's inequality. I mean, we have $$\int_A |fg|d\mu \leq \left( \int_A |f|^p d\mu \right)^{1/p} \left( \int_A |g|^q d\mu \right)^{1/q}$$ for $\frac{1}{p}+\frac{1}{q}=1$ and $fg\in L^1(A, \mu)$. But, how accurate is this inequality? Are there others that might be better? Is there any book about inequalities and how optimal they are? Thank you very very much!!","I wonder if there is a sharper inequality than Hölder's inequality. I mean, we have $$\int_A |fg|d\mu \leq \left( \int_A |f|^p d\mu \right)^{1/p} \left( \int_A |g|^q d\mu \right)^{1/q}$$ for $\frac{1}{p}+\frac{1}{q}=1$ and $fg\in L^1(A, \mu)$. But, how accurate is this inequality? Are there others that might be better? Is there any book about inequalities and how optimal they are? Thank you very very much!!",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
51,Analysis proof for repeating digits of rational numbers,Analysis proof for repeating digits of rational numbers,,"""Every rational number is either a terminating or repeating decimal"". I knew there's a proof for this using number theory's theorems, but I wish to find a purely analysis proof, that is: the series $x = a_0 q^{0} + a_1 q^{-1} + ... + a_n q^{-n} + ...$ (with $0<= a_i <= q-1$ and $q$ is a natural number) converges to a rational value ONLY if the sequence $a_0 , a_1 , ...$ is periodic from some point. If this isn't possible then an analysis proof of a weaker result such as the case when each $a_i$ is either 0 or 1 would be appreciated. Thanks.","""Every rational number is either a terminating or repeating decimal"". I knew there's a proof for this using number theory's theorems, but I wish to find a purely analysis proof, that is: the series $x = a_0 q^{0} + a_1 q^{-1} + ... + a_n q^{-n} + ...$ (with $0<= a_i <= q-1$ and $q$ is a natural number) converges to a rational value ONLY if the sequence $a_0 , a_1 , ...$ is periodic from some point. If this isn't possible then an analysis proof of a weaker result such as the case when each $a_i$ is either 0 or 1 would be appreciated. Thanks.",,"['real-analysis', 'number-theory', 'analysis']"
52,Convergence in $L^2(\mathbb{R}_+)$,Convergence in,L^2(\mathbb{R}_+),"I'm struggling with the following question: Let $f \in L^2(\mathbb{R}_+)$ and let $r > 0$. Show that $\sum \limits_{n =0 }^{\infty} \frac{1}{r} \int_{[rn, r(n+1))} f(t)dt\textbf{1}_{[rn, r(n+1))}$ converges to $f$ in $L^2(\mathbb{R}_+)$ as $r \to 0^+$. Symbol $\textbf{1}_{[rn, r(n+1))}$ stands for an indicator function on $[rn, r(n+1))$. I was thinking to start with $f \in C_0(\mathbb{R}_+)$ - continuous functions with compact support which are dense in $L^2(\mathbb{R}_+)$, but I still can't show it. Thanks for any hints or answers.","I'm struggling with the following question: Let $f \in L^2(\mathbb{R}_+)$ and let $r > 0$. Show that $\sum \limits_{n =0 }^{\infty} \frac{1}{r} \int_{[rn, r(n+1))} f(t)dt\textbf{1}_{[rn, r(n+1))}$ converges to $f$ in $L^2(\mathbb{R}_+)$ as $r \to 0^+$. Symbol $\textbf{1}_{[rn, r(n+1))}$ stands for an indicator function on $[rn, r(n+1))$. I was thinking to start with $f \in C_0(\mathbb{R}_+)$ - continuous functions with compact support which are dense in $L^2(\mathbb{R}_+)$, but I still can't show it. Thanks for any hints or answers.",,"['real-analysis', 'analysis']"
53,how to prove this question about derivative and differentiation,how to prove this question about derivative and differentiation,,"Let  $$ f:\mathbb{R}\to \mathbb{R} $$  such that $f ',f'',f'''$ exist and $\lim_{x\to+\infty} f(x)=t$ exists if  $  \lim_{x\to+\infty} f'''(x)=0$. Then prove that $$ \lim_{x\to+\infty} f'(x) = \lim_{x\to+\infty} f''(x)=0. $$ Thanks in advance","Let  $$ f:\mathbb{R}\to \mathbb{R} $$  such that $f ',f'',f'''$ exist and $\lim_{x\to+\infty} f(x)=t$ exists if  $  \lim_{x\to+\infty} f'''(x)=0$. Then prove that $$ \lim_{x\to+\infty} f'(x) = \lim_{x\to+\infty} f''(x)=0. $$ Thanks in advance",,"['analysis', 'contest-math']"
54,Exercise from Evans' book.,Exercise from Evans' book.,,I am reading Evans' Partial differential Equations and am stuck with the following problem from chapter 12 on nonlinear wave equation. Suppose $u$ has a compact support and solves PDE of the form:$$u_{tt}-\sum_{i=1}^n(L_{p_i}(Du))_{x_i}=0.$$ Determine the appropriate energy $E(t)$ and  show that $(E(t))'=0.$ My first question is: can anybody clarify for me what $L_{p_i}$ stands for here? And what is the form of $L$ then? I was trying to find clarifications in the book but failed. Thanks!,I am reading Evans' Partial differential Equations and am stuck with the following problem from chapter 12 on nonlinear wave equation. Suppose $u$ has a compact support and solves PDE of the form:$$u_{tt}-\sum_{i=1}^n(L_{p_i}(Du))_{x_i}=0.$$ Determine the appropriate energy $E(t)$ and  show that $(E(t))'=0.$ My first question is: can anybody clarify for me what $L_{p_i}$ stands for here? And what is the form of $L$ then? I was trying to find clarifications in the book but failed. Thanks!,,"['analysis', 'partial-differential-equations']"
55,Trading localisation for regularity,Trading localisation for regularity,,"When reading about Schrödinger's fundamental solution in 1D, $$u(t,x)=\frac{1}{\sqrt{4\pi it}} \int_\mathbb{R} u_0(y) e^{\frac{i(x-y)^2}{4t}}dy$$ the author says thus Schrödinger evolution is instantaneously smoothing for localised data: if $u_0$ is so localised as to be absolutely integrable but is not smooth, the previous shows that at all other times $t>0$,  $u(t)$ is smooth (but not localised) . I see that when differentiating under the integral sign, if $u_0$ is compactly supported (not necessarily smooth), the integral converges and thus the interchange is legitimate. What can localised mean apart from compactly supported? Schwartz space would do, of course, but what non-smooth initial data are considered localised? I'd like a good interpretation of the following too, please: high frequencies (Fourier space) travel very fast (we know dispersion is proportional to frequency in Schrödinger equation) and radiate quickly away from the origin (of physical space?) where they are initially localised, leaving only the low frequencies, which are always smooth, to remain near the origin . I see this when breaking an initial data in a finite number of waves which ""sum"" to it, but it seems to me that smoothness away from the origin wouldn't be guaranteed.","When reading about Schrödinger's fundamental solution in 1D, $$u(t,x)=\frac{1}{\sqrt{4\pi it}} \int_\mathbb{R} u_0(y) e^{\frac{i(x-y)^2}{4t}}dy$$ the author says thus Schrödinger evolution is instantaneously smoothing for localised data: if $u_0$ is so localised as to be absolutely integrable but is not smooth, the previous shows that at all other times $t>0$,  $u(t)$ is smooth (but not localised) . I see that when differentiating under the integral sign, if $u_0$ is compactly supported (not necessarily smooth), the integral converges and thus the interchange is legitimate. What can localised mean apart from compactly supported? Schwartz space would do, of course, but what non-smooth initial data are considered localised? I'd like a good interpretation of the following too, please: high frequencies (Fourier space) travel very fast (we know dispersion is proportional to frequency in Schrödinger equation) and radiate quickly away from the origin (of physical space?) where they are initially localised, leaving only the low frequencies, which are always smooth, to remain near the origin . I see this when breaking an initial data in a finite number of waves which ""sum"" to it, but it seems to me that smoothness away from the origin wouldn't be guaranteed.",,"['analysis', 'partial-differential-equations']"
56,$(a_{2n})$ and $(a_{2n+1})$ converges then $(a_n)$ converges,and  converges then  converges,(a_{2n}) (a_{2n+1}) (a_n),"Whe had the following theorem in class: If $(a_{2n})_{n\in\mathbb N}$ and $(a_{2n+1})_{n\in\mathbb N}$ are convergent sequences with the same limit $a$ , then the sequence $(a_{n})_{n\in\mathbb N}$ converges. Proof: $(a_{2n})_{n\in\mathbb N}$ converges, so $\forall\varepsilon>0\exists   N_1\in\mathbb N:\forall n\geq N_1: |a_{2n}-a|<\varepsilon$ It follows $|a_m-a|<\varepsilon$ for all even $m\geq2N_1$ . $(a_{2n+1})_{n\in\mathbb N}$ converges, so $\forall\varepsilon>0\exists  N_2\in\mathbb N:\forall n\geq N_2: |a_{2n+1}-a|<\varepsilon$ It follows $|a_m-a|<\varepsilon$ for all odd $m\geq2N_2+1$ . Thus for all $m\in\mathbb N$ with $m\geq M:=\max\{2N_1,2N_2+1\}$ is $|a_m-a|<\varepsilon$ $\;\;\;\;\;\;\;\;\;\;\;$ q.e.d. Now, after trying to proove it myself again, I've just wondered myself about one thing: Why can't you choose $M=\max\{N_1,N_2\}$ and leave the second line in each number out? Why do you have to choose $2N_1$ and $2N_2+1$ ? Thanks for helping!","Whe had the following theorem in class: If and are convergent sequences with the same limit , then the sequence converges. Proof: converges, so It follows for all even . converges, so It follows for all odd . Thus for all with is q.e.d. Now, after trying to proove it myself again, I've just wondered myself about one thing: Why can't you choose and leave the second line in each number out? Why do you have to choose and ? Thanks for helping!","(a_{2n})_{n\in\mathbb N} (a_{2n+1})_{n\in\mathbb N} a (a_{n})_{n\in\mathbb N} (a_{2n})_{n\in\mathbb N} \forall\varepsilon>0\exists   N_1\in\mathbb N:\forall n\geq N_1: |a_{2n}-a|<\varepsilon |a_m-a|<\varepsilon m\geq2N_1 (a_{2n+1})_{n\in\mathbb N} \forall\varepsilon>0\exists  N_2\in\mathbb N:\forall n\geq N_2: |a_{2n+1}-a|<\varepsilon |a_m-a|<\varepsilon m\geq2N_2+1 m\in\mathbb N m\geq M:=\max\{2N_1,2N_2+1\} |a_m-a|<\varepsilon \;\;\;\;\;\;\;\;\;\;\; M=\max\{N_1,N_2\} 2N_1 2N_2+1",['real-analysis']
57,Showing a function has only one point of continuity.,Showing a function has only one point of continuity.,,"Let $$f(x) = \begin{cases}\;\;\, x\;\;,\;\text{ if } x \in \mathbb{Q}\\ -x\;\;,\; \text{ if } x \in \mathbb{R}\setminus \mathbb{Q} \end{cases}$$ (i)  Determine the point or points of continuity of $f$. (ii)  Show that the point of points of continuity of $f$ are the only points. Clearly its continuous at $0$.  I'm not sure how to prove it is continuous at $0$, but I know how to prove that it has no other points of continuity.","Let $$f(x) = \begin{cases}\;\;\, x\;\;,\;\text{ if } x \in \mathbb{Q}\\ -x\;\;,\; \text{ if } x \in \mathbb{R}\setminus \mathbb{Q} \end{cases}$$ (i)  Determine the point or points of continuity of $f$. (ii)  Show that the point of points of continuity of $f$ are the only points. Clearly its continuous at $0$.  I'm not sure how to prove it is continuous at $0$, but I know how to prove that it has no other points of continuity.",,['analysis']
58,Is there such a thing as function decomposability?,Is there such a thing as function decomposability?,,"I am not a mathematician, so what I ask might be trivial, however I couldn't find something relevant in the web. My question is the following: Is there a formal notation for functions that comply the following $$f(x_1, ..., x_n) = g(f(x_1), ..., g(f(x_n))$$ and if there is such a property, how is it called? The current, intuitive name I can think of is decomposable functions, but I don't know if formally there is such a thing. Example 1: An additive function would be decomposable. Example 2: A multiplicative function would also be decomposable","I am not a mathematician, so what I ask might be trivial, however I couldn't find something relevant in the web. My question is the following: Is there a formal notation for functions that comply the following $$f(x_1, ..., x_n) = g(f(x_1), ..., g(f(x_n))$$ and if there is such a property, how is it called? The current, intuitive name I can think of is decomposable functions, but I don't know if formally there is such a thing. Example 1: An additive function would be decomposable. Example 2: A multiplicative function would also be decomposable",,"['analysis', 'functions', 'terminology']"
59,Norm of operator $g\mapsto \int fg$,Norm of operator,g\mapsto \int fg,"Let $T_f:(C([a,b],\mathbb C), \lVert \cdot \rVert_1) \to \mathbb C$ with $g\mapsto \int_a^b f(x)g(x) dx$ for any given $f\in C([a,b],\mathbb C)$. I have to find the norm of $T_f$. I started with: $$|T_fg| = \left|\int_a^b f(x)g(x)dx\right| \leqslant\int_a^b |f(x)g(x)|dx \leqslant\lVert g \rVert _1 \lVert f \rVert _\infty.$$ For the last step I used Hölder. Now all I need is an example $g\in C([a,b],\mathbb C)$ with $\lVert g \rVert_1 = \int_a^b |g(x)|dx = 1$ and $|T_fg| = \lVert f \rVert _\infty$ (I suppose $\lVert f \rVert _\infty$ is the operator norm here). I have tried several functions, but none worked, maybe $\lVert T_f \rVert < \lVert f \rVert _\infty$?","Let $T_f:(C([a,b],\mathbb C), \lVert \cdot \rVert_1) \to \mathbb C$ with $g\mapsto \int_a^b f(x)g(x) dx$ for any given $f\in C([a,b],\mathbb C)$. I have to find the norm of $T_f$. I started with: $$|T_fg| = \left|\int_a^b f(x)g(x)dx\right| \leqslant\int_a^b |f(x)g(x)|dx \leqslant\lVert g \rVert _1 \lVert f \rVert _\infty.$$ For the last step I used Hölder. Now all I need is an example $g\in C([a,b],\mathbb C)$ with $\lVert g \rVert_1 = \int_a^b |g(x)|dx = 1$ and $|T_fg| = \lVert f \rVert _\infty$ (I suppose $\lVert f \rVert _\infty$ is the operator norm here). I have tried several functions, but none worked, maybe $\lVert T_f \rVert < \lVert f \rVert _\infty$?",,"['analysis', 'operator-theory', 'normed-spaces']"
60,"How to show that if $f$ or $g$ is continuous, then the convolution $f \star g$ of those functions is continuous?","How to show that if  or  is continuous, then the convolution  of those functions is continuous?",f g f \star g,"How to show that $f \star g$ is continuous if $f$ or $g$ is continuous? Do you use $\epsilon - \delta $ - approach in the proof? some hint. I define $$(f \star g)(x)=\frac{1}{2\pi} \int_{- \pi}^{\pi} f(y)g(x-y)dy,$$ if $f,g \in L^1[-\pi,\pi].$","How to show that $f \star g$ is continuous if $f$ or $g$ is continuous? Do you use $\epsilon - \delta $ - approach in the proof? some hint. I define $$(f \star g)(x)=\frac{1}{2\pi} \int_{- \pi}^{\pi} f(y)g(x-y)dy,$$ if $f,g \in L^1[-\pi,\pi].$",,['analysis']
61,Image of smooth manifold is a submanifold,Image of smooth manifold is a submanifold,,"It's know that if $M$ is a compact, smooth manifold of dimension $n$ and the map $f: M \to \mathbb{R^m}$ is injective, smooth, $n \le m$ and $Jf(a)$, the Jacobian, has rank $n$ for every $a \in M$, then $f(M)$ is a submanifold of $\mathbb{R^m}$.  I'm trying to think of a counterexample to this statement if we suppose $M$ is not compact, but haven't gotten anywhere.  Can anyone offer a hint?","It's know that if $M$ is a compact, smooth manifold of dimension $n$ and the map $f: M \to \mathbb{R^m}$ is injective, smooth, $n \le m$ and $Jf(a)$, the Jacobian, has rank $n$ for every $a \in M$, then $f(M)$ is a submanifold of $\mathbb{R^m}$.  I'm trying to think of a counterexample to this statement if we suppose $M$ is not compact, but haven't gotten anywhere.  Can anyone offer a hint?",,"['analysis', 'differential-topology']"
62,"""Constructions"" with a countably infinite number of steps","""Constructions"" with a countably infinite number of steps",,"Say I have a countably infinite initial set $A$ with $A \subset [0,1]$. My set satisfies the property that $\inf_{x\in A} |x-y| = 0,\,\forall{y\in [0,1]}$  (For example $A$ can be the set of rational numbers). I have a ""construction"" that works in the following manner: Step 1: Remove an arbitrary $x_1\in A$ from $A$ to come up with the set $A_1 = A - \{x_1\}$, and for $n=2,\ldots,N$, Step n: Remove an arbitrary $x_n \in A_{n-1}$ from $A_{n-1}$ to come up with the set $A_n = A- \{x_1, \ldots, x_n\}$. It can be shown that for any $n$, we have $\inf_{x\in A_n}|x-y| = 0,\,\forall{y\in [0,1]}$  as well. That is, removing finitely many terms from $A$, we still get $0$. I cannot guarantee though that repeating the above process infinitely many times I still get $0$, as I may have removed all the elements of $A$. My first question is why this is happening? Or to better put it, how can I avoid making mistakes when going from a finite number of steps to a countably infinite number of steps? Now, consider another ""construction""; at step $n$ I just remove an arbitrary element $x_n$ from $A$ to come up with the set $A_n = A - \{x_n\}$. Similar to the previous case we have $\inf_{x\in A_n} |x-y| = 0,\,\forall{y\in [0,1]}$ for any $n$. But now, I guess, even if I repeat the steps of this new construction infinitely many times, I would still get $0$ as my final answer. How can I prove this? In general, I have a construction in which I repeat some well-defined steps infinitely many times; how can I verify the correctness of my final answer? (You may suggest that I need to find a set-limit $\lim A_n$ for this case, but it appears to me that I need not, as I am ""sure"" that the final answer should be $0$ in any case).","Say I have a countably infinite initial set $A$ with $A \subset [0,1]$. My set satisfies the property that $\inf_{x\in A} |x-y| = 0,\,\forall{y\in [0,1]}$  (For example $A$ can be the set of rational numbers). I have a ""construction"" that works in the following manner: Step 1: Remove an arbitrary $x_1\in A$ from $A$ to come up with the set $A_1 = A - \{x_1\}$, and for $n=2,\ldots,N$, Step n: Remove an arbitrary $x_n \in A_{n-1}$ from $A_{n-1}$ to come up with the set $A_n = A- \{x_1, \ldots, x_n\}$. It can be shown that for any $n$, we have $\inf_{x\in A_n}|x-y| = 0,\,\forall{y\in [0,1]}$  as well. That is, removing finitely many terms from $A$, we still get $0$. I cannot guarantee though that repeating the above process infinitely many times I still get $0$, as I may have removed all the elements of $A$. My first question is why this is happening? Or to better put it, how can I avoid making mistakes when going from a finite number of steps to a countably infinite number of steps? Now, consider another ""construction""; at step $n$ I just remove an arbitrary element $x_n$ from $A$ to come up with the set $A_n = A - \{x_n\}$. Similar to the previous case we have $\inf_{x\in A_n} |x-y| = 0,\,\forall{y\in [0,1]}$ for any $n$. But now, I guess, even if I repeat the steps of this new construction infinitely many times, I would still get $0$ as my final answer. How can I prove this? In general, I have a construction in which I repeat some well-defined steps infinitely many times; how can I verify the correctness of my final answer? (You may suggest that I need to find a set-limit $\lim A_n$ for this case, but it appears to me that I need not, as I am ""sure"" that the final answer should be $0$ in any case).",,['analysis']
63,Reference requested: 'decomposition' of Haar measure on the adeles.,Reference requested: 'decomposition' of Haar measure on the adeles.,,"Since the adeles $\mathbb{A}$ (with addition) are a locally compact Hausdorff topological group there exists a Haar measure $\mu$. Now people claim that it can be normalized such that for every function of the form $f = \prod_p f_p$ such that $f_p = \mathbb{1}_{Z_p}$ for almost all $p$ and $f_p$ integrable for all the rest, one has $$\int_{\mathbb{A}} f = \prod_p \int_{Q_p} f_p(x_p) dx_p$$ where $dx_p$ is the additive Haar measure on $Q_p$, normalized such that the measure of $Z_p$ is one (the lebesgue measure up to some factor for $p=\infty$ respectively). My question is: why is this the case? In one of the books i tried to find the answer in, the author proceeds as follows: he defines simple sets to be sets of the form $M = \prod_p M_p$ where $M_p = Z_p$ for almost all $p$ and $M_p = U_p$ is open in $Q_p$ for all the rest. Then he defines another measure $\nu(M) := \prod_p \mu_p(M_p)$ where $\mu_p$ is the additive Haar measure on $Q_p$. Since simple sets are stable under finite intersections, one can continue this to a measure on the whole Borel-$\sigma$-algebra. The question here is: why is it a Radon measure, i.e. one has to show that $\nu(K) < \infty$ for compact $K$ and that it is outer regular for all borel sets and inner regular for open sets and sets of finite measure. How to do that? I guess that there is a better way to achieve this by starting with the abstract nice measure $\mu$ and then show that the relation above holds up to a factor. Does somebody know where to find that or can somebody point out a basic reference for the construction of the Haar-measure on the adeles? Thanks, Fabian Werner","Since the adeles $\mathbb{A}$ (with addition) are a locally compact Hausdorff topological group there exists a Haar measure $\mu$. Now people claim that it can be normalized such that for every function of the form $f = \prod_p f_p$ such that $f_p = \mathbb{1}_{Z_p}$ for almost all $p$ and $f_p$ integrable for all the rest, one has $$\int_{\mathbb{A}} f = \prod_p \int_{Q_p} f_p(x_p) dx_p$$ where $dx_p$ is the additive Haar measure on $Q_p$, normalized such that the measure of $Z_p$ is one (the lebesgue measure up to some factor for $p=\infty$ respectively). My question is: why is this the case? In one of the books i tried to find the answer in, the author proceeds as follows: he defines simple sets to be sets of the form $M = \prod_p M_p$ where $M_p = Z_p$ for almost all $p$ and $M_p = U_p$ is open in $Q_p$ for all the rest. Then he defines another measure $\nu(M) := \prod_p \mu_p(M_p)$ where $\mu_p$ is the additive Haar measure on $Q_p$. Since simple sets are stable under finite intersections, one can continue this to a measure on the whole Borel-$\sigma$-algebra. The question here is: why is it a Radon measure, i.e. one has to show that $\nu(K) < \infty$ for compact $K$ and that it is outer regular for all borel sets and inner regular for open sets and sets of finite measure. How to do that? I guess that there is a better way to achieve this by starting with the abstract nice measure $\mu$ and then show that the relation above holds up to a factor. Does somebody know where to find that or can somebody point out a basic reference for the construction of the Haar-measure on the adeles? Thanks, Fabian Werner",,"['analysis', 'p-adic-number-theory']"
64,About the remainder of Taylor expansion and Riemann-Liouville integral,About the remainder of Taylor expansion and Riemann-Liouville integral,,"Integral form of Taylor expansion looks like this:$$f(x)=\sum_{i=0}^k\frac{f^{(i)}(a)}{i!}(x-a)^i+\int_a^x\frac{f^{(k+1)}(t)}{k!}(x-t)^kdt$$ Riemann-Liouville integral is $$I^{\alpha}f=\frac{1}{\Gamma(\alpha)}\int_a^x{f(t)(x-t)^{(\alpha-1)}}dt$$ Q1: The integral form remainder of Taylor expansion is exactly $I^{k+1}f^{(k+1)}$. Why is that? Q2: As far as I know, Riemann-Liouville integral is basically $\alpha$th antiderivative of $f(x).$ Shouldn't $f(x)=I^{k+1}f^{(k+1)}$? Is that right?","Integral form of Taylor expansion looks like this:$$f(x)=\sum_{i=0}^k\frac{f^{(i)}(a)}{i!}(x-a)^i+\int_a^x\frac{f^{(k+1)}(t)}{k!}(x-t)^kdt$$ Riemann-Liouville integral is $$I^{\alpha}f=\frac{1}{\Gamma(\alpha)}\int_a^x{f(t)(x-t)^{(\alpha-1)}}dt$$ Q1: The integral form remainder of Taylor expansion is exactly $I^{k+1}f^{(k+1)}$. Why is that? Q2: As far as I know, Riemann-Liouville integral is basically $\alpha$th antiderivative of $f(x).$ Shouldn't $f(x)=I^{k+1}f^{(k+1)}$? Is that right?",,['analysis']
65,"$K$ compact and $\Omega$ is open, then $\inf\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} > 0$","compact and  is open, then","K \Omega \inf\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} > 0","I have to show the following: $(V,\rho)$ be a metric space, $K\subset V$ compact and $\Omega \subset V$ is open, then $d(K,\Omega^c) = \inf\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} > 0$. I had the following in mind. For every $x \in V$ $f_x: K \rightarrow \mathbb{R}: a \mapsto \rho(a,x)$ is continuous. Since $K$ is compact, $\min(f_x(K))$ exists. So I thought that it might be possible that $\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\}$ contains its infimum and therefore  $d(K,\Omega^c)$ cannot be $0$ since $K$ and $\Omega^c$ are disjoint. Also, $$\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} = \displaystyle\bigcup_{x\in \Omega^c} f_x(K)$$ But since the minimum of a infinite union does not necessarily exists I have no clue how to proceed. So I wonder if I am looking into the right direction. If so, could anyone give me a hint? If not, could anyone point me into the right direction. Please, DO NOT POST FULL ANSWERS!","I have to show the following: $(V,\rho)$ be a metric space, $K\subset V$ compact and $\Omega \subset V$ is open, then $d(K,\Omega^c) = \inf\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} > 0$. I had the following in mind. For every $x \in V$ $f_x: K \rightarrow \mathbb{R}: a \mapsto \rho(a,x)$ is continuous. Since $K$ is compact, $\min(f_x(K))$ exists. So I thought that it might be possible that $\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\}$ contains its infimum and therefore  $d(K,\Omega^c)$ cannot be $0$ since $K$ and $\Omega^c$ are disjoint. Also, $$\{\rho(x,x') \mid x \in K \textrm{ and } x' \in \Omega^c\} = \displaystyle\bigcup_{x\in \Omega^c} f_x(K)$$ But since the minimum of a infinite union does not necessarily exists I have no clue how to proceed. So I wonder if I am looking into the right direction. If so, could anyone give me a hint? If not, could anyone point me into the right direction. Please, DO NOT POST FULL ANSWERS!",,"['analysis', 'metric-spaces']"
66,"Give an example of a function $f:[0,1] \rightarrow \mathbb{R}$ that is...",Give an example of a function  that is...,"f:[0,1] \rightarrow \mathbb{R}","Give an example of a function $f:[0,1] \rightarrow \mathbb{R}$ such that... (a) $f$ is bounded, but not Riemann integrable on $[0,1]$. $$ f(x) := \begin{cases} 2x & \text{if $x$ is rational}\\  x & \text{if $x$ is irrational.} \end{cases} $$ (b) $f$ is Riemann integrable on $[0,1]$ but not monotone. $$f(x) := 2$$ (c) $f$ is Riemann integrable on $[0,1]$ but neither continuous nor monotone. $$f(x) := \begin{cases} 0 & \text{if $x$ is $0$}\\  2 & \text{otherwise.} \end{cases} $$ Is this correct? Thanks!","Give an example of a function $f:[0,1] \rightarrow \mathbb{R}$ such that... (a) $f$ is bounded, but not Riemann integrable on $[0,1]$. $$ f(x) := \begin{cases} 2x & \text{if $x$ is rational}\\  x & \text{if $x$ is irrational.} \end{cases} $$ (b) $f$ is Riemann integrable on $[0,1]$ but not monotone. $$f(x) := 2$$ (c) $f$ is Riemann integrable on $[0,1]$ but neither continuous nor monotone. $$f(x) := \begin{cases} 0 & \text{if $x$ is $0$}\\  2 & \text{otherwise.} \end{cases} $$ Is this correct? Thanks!",,['analysis']
67,Prove that a function is not Riemann integrable.,Prove that a function is not Riemann integrable.,,"Suppose $f: [-2,3] \longrightarrow \mathbb{R}$ is defined by $$ f(x) = \left\{ \begin{array}{l l} 2|x| + 1, & \text{if $x$ is rational}, \\ 0, & \text{if $x$ is irrational}. \end{array} \right.$$ Prove that $f$ is not Riemann integrable. We know that the lower integral is $0$ and the upper integral is $18$, then because they are not equal $f$ is not Riemann integrable. Is this correct? Thanks!","Suppose $f: [-2,3] \longrightarrow \mathbb{R}$ is defined by $$ f(x) = \left\{ \begin{array}{l l} 2|x| + 1, & \text{if $x$ is rational}, \\ 0, & \text{if $x$ is irrational}. \end{array} \right.$$ Prove that $f$ is not Riemann integrable. We know that the lower integral is $0$ and the upper integral is $18$, then because they are not equal $f$ is not Riemann integrable. Is this correct? Thanks!",,['analysis']
68,"Mixed $L^p$-norm Inequality on $[0,1]\times\mathbb{R}^2$.",Mixed -norm Inequality on .,"L^p [0,1]\times\mathbb{R}^2","Looking at $(t,x)\in[0,1]\times\mathbb{R}^2$, I came across the statement (for sufficiently smooth) real-valued $f$ that $$ \|f(t,x)\|_{L^\infty_tL^2_x} \lesssim \|f\|_{L^2_tL^2_x}^{1/2}\|\partial_t f\|_{L^2_tL^2_x}^{1/2} + \|f\|_{L^2_tL^2_x}, $$ which is a ""simple calculus identity."" I do not know how to prove this. I tried to let $t\in[0,1]$ and then bound $ \int f(t,x)^2dx$ by the above by writing $f(t,x)$ as $\int_0^t\partial_s f(s,x)ds+f(0,x)$ and similar tricks, but could not get it to work out. I also tried to use the Sobolev embedding of $W^{1,2}(\mathbb{R})$ into $L^\infty(\mathbb{R})$, but could not get that to work (nor is that ""simple calculus"" in my opinion). Any help would be greatly appreciated, thanks! (In case it is not standard, the mixed norm notation is $$  \|f(t,x)\|_{L^q_tL^p_x} = \left(\int\|f(t,\cdot)\|^q_{L^p}dt\right)^{1/q}. $$ ) (For reference, the original statement was found in the paper ""Sharp Trace Theorems for Null Hypersurfaces on Einstein Metrics with Finite Curvature Flux"" by S. Klainerman and I. Rodnianski, Geom. funct. anal. Vol. 16 (2006) 164-229.)","Looking at $(t,x)\in[0,1]\times\mathbb{R}^2$, I came across the statement (for sufficiently smooth) real-valued $f$ that $$ \|f(t,x)\|_{L^\infty_tL^2_x} \lesssim \|f\|_{L^2_tL^2_x}^{1/2}\|\partial_t f\|_{L^2_tL^2_x}^{1/2} + \|f\|_{L^2_tL^2_x}, $$ which is a ""simple calculus identity."" I do not know how to prove this. I tried to let $t\in[0,1]$ and then bound $ \int f(t,x)^2dx$ by the above by writing $f(t,x)$ as $\int_0^t\partial_s f(s,x)ds+f(0,x)$ and similar tricks, but could not get it to work out. I also tried to use the Sobolev embedding of $W^{1,2}(\mathbb{R})$ into $L^\infty(\mathbb{R})$, but could not get that to work (nor is that ""simple calculus"" in my opinion). Any help would be greatly appreciated, thanks! (In case it is not standard, the mixed norm notation is $$  \|f(t,x)\|_{L^q_tL^p_x} = \left(\int\|f(t,\cdot)\|^q_{L^p}dt\right)^{1/q}. $$ ) (For reference, the original statement was found in the paper ""Sharp Trace Theorems for Null Hypersurfaces on Einstein Metrics with Finite Curvature Flux"" by S. Klainerman and I. Rodnianski, Geom. funct. anal. Vol. 16 (2006) 164-229.)",,"['calculus', 'analysis', 'inequality']"
69,Identifying recursive polynomials,Identifying recursive polynomials,,"I need to evaluate the following function and want to proceed analytically as far as possible: $F(y) =e^{ i \beta \left ( y \frac{d}{d y} \right )^2}  y \, e^{-y^2/2}$ My plan is to expand into power series in $\beta$ and indentify the polynomials $\left (y \frac{d}{d y} \right )^k (y \, e^{-y^2/2})= p_k(y) \, y \, e^{-y^2/2}$ by the recursion relation I expect them to satisfy. Is this a sound strategy? Is there a more direct way to identify polynomials $p_k(y)$ and compute their ""generating function'' $F(y)$?","I need to evaluate the following function and want to proceed analytically as far as possible: $F(y) =e^{ i \beta \left ( y \frac{d}{d y} \right )^2}  y \, e^{-y^2/2}$ My plan is to expand into power series in $\beta$ and indentify the polynomials $\left (y \frac{d}{d y} \right )^k (y \, e^{-y^2/2})= p_k(y) \, y \, e^{-y^2/2}$ by the recursion relation I expect them to satisfy. Is this a sound strategy? Is there a more direct way to identify polynomials $p_k(y)$ and compute their ""generating function'' $F(y)$?",,"['analysis', 'polynomials', 'recurrence-relations']"
70,Showing $V_a^b\alpha = \sup \left\{\int_a^bfd\alpha:\|f\|_\infty\leq 1\right\}$,Showing,V_a^b\alpha = \sup \left\{\int_a^bfd\alpha:\|f\|_\infty\leq 1\right\},"Let $\alpha:[a,b]\to\mathbb{R}$ be of bounded variation and right-continuous. Given $\varepsilon>0$ and a partition $P$ of $[a,b]$, construct $f\in C[a,b]$ with $\|f\|_\infty \leq 1$ such that $\int_a^bfd\alpha\geq V(\alpha,P)-\varepsilon$. Conclude that $V_a^b\alpha = \sup \left\{\int_a^bfd\alpha:\|f\|_\infty\leq 1\right\}$. I am not sure how to show this but it seems to me that we want to construct $f$ that is some kind of characteristic function. Any hints would be appreciated.","Let $\alpha:[a,b]\to\mathbb{R}$ be of bounded variation and right-continuous. Given $\varepsilon>0$ and a partition $P$ of $[a,b]$, construct $f\in C[a,b]$ with $\|f\|_\infty \leq 1$ such that $\int_a^bfd\alpha\geq V(\alpha,P)-\varepsilon$. Conclude that $V_a^b\alpha = \sup \left\{\int_a^bfd\alpha:\|f\|_\infty\leq 1\right\}$. I am not sure how to show this but it seems to me that we want to construct $f$ that is some kind of characteristic function. Any hints would be appreciated.",,['analysis']
71,Integral equation and existence:  $g(x)=\int_{-\infty}^{\infty} \prod _{j=1}^nf(u-x_j)du$,Integral equation and existence:,g(x)=\int_{-\infty}^{\infty} \prod _{j=1}^nf(u-x_j)du,"I'd like to know how one would go about showing that the following function, $f$, that is almost everywhere positive exists: $$g(x_1,\cdots,x_n)=\int_{-\infty}^{\infty} \prod _{j=1}^nf(u-x_j)du$$ where $g:\mathbb{R}^n:\rightarrow \mathbb{R}$ satisfies: (1) $g(x_1,\cdots,x_n)$ is the derivative is the nth order partial derivative of  $\frac{\partial \ln(G(e^{x_1},\cdots,e^{x_n})}{\partial x_1x_2...x_n}$; where $G(e^{x_1},\cdots,e^{x_n})$ is symmetric, homogenous of degree 1, $G(0)=0$, $\lim G(y)\rightarrow \infty$ as $y \rightarrow \infty$, $G(y)>0$. (2) $g\ge0$ If this isn't possible, can you suggest of ways to add restrictions so that such an $f$ exists? I know this is asking a lot, but I was wondering if someone would be willing to give some direction. Thanks so much in advance!!!!!","I'd like to know how one would go about showing that the following function, $f$, that is almost everywhere positive exists: $$g(x_1,\cdots,x_n)=\int_{-\infty}^{\infty} \prod _{j=1}^nf(u-x_j)du$$ where $g:\mathbb{R}^n:\rightarrow \mathbb{R}$ satisfies: (1) $g(x_1,\cdots,x_n)$ is the derivative is the nth order partial derivative of  $\frac{\partial \ln(G(e^{x_1},\cdots,e^{x_n})}{\partial x_1x_2...x_n}$; where $G(e^{x_1},\cdots,e^{x_n})$ is symmetric, homogenous of degree 1, $G(0)=0$, $\lim G(y)\rightarrow \infty$ as $y \rightarrow \infty$, $G(y)>0$. (2) $g\ge0$ If this isn't possible, can you suggest of ways to add restrictions so that such an $f$ exists? I know this is asking a lot, but I was wondering if someone would be willing to give some direction. Thanks so much in advance!!!!!",,"['analysis', 'measure-theory', 'integral-equations']"
72,how to construct a polynomial,how to construct a polynomial,,"I have a question here: A finite sequence of real numbers $c_1, c_2, \dots, c_{n−1}$ is called saw– like if we have $(−1)^k(c_k − c_{k+1}) \leq 0$ for all $k = 1, \dots , n − 2$ or if we have $(−1)^k(c_k − c_{k+1}) \geq 0$ for all $k = 1, \dots , n − 2$. Prove that $c_1, c_2, \dots , c_{n−1}$ is a saw–like sequence if and only if there exist a polynomial $f (x)$ of degree $n$ with real coefficients such that (1) $x_1 \leq \dots \leq x_{n−1}$ are critical points of $f$ ; (2) $f (x_k) = c_k$ for $k = 1, \dots , n − 1$. I've proven one direction,which is ""if we have such a polynomial $f$, then the finite sequence {$c_k$} is saw-like."" But I don't know how to prove the other direction. I think I need to construct a polynomial $f$ then prove that it satisfies all the properties. But I don't know how to construct it? Use Lagrange interpolation formula? Thanks!","I have a question here: A finite sequence of real numbers $c_1, c_2, \dots, c_{n−1}$ is called saw– like if we have $(−1)^k(c_k − c_{k+1}) \leq 0$ for all $k = 1, \dots , n − 2$ or if we have $(−1)^k(c_k − c_{k+1}) \geq 0$ for all $k = 1, \dots , n − 2$. Prove that $c_1, c_2, \dots , c_{n−1}$ is a saw–like sequence if and only if there exist a polynomial $f (x)$ of degree $n$ with real coefficients such that (1) $x_1 \leq \dots \leq x_{n−1}$ are critical points of $f$ ; (2) $f (x_k) = c_k$ for $k = 1, \dots , n − 1$. I've proven one direction,which is ""if we have such a polynomial $f$, then the finite sequence {$c_k$} is saw-like."" But I don't know how to prove the other direction. I think I need to construct a polynomial $f$ then prove that it satisfies all the properties. But I don't know how to construct it? Use Lagrange interpolation formula? Thanks!",,['analysis']
73,"Is there an algebraic solution to $e^{-x/a}+e^{-x/b}=1$ ($a\neq b$, $a,b$ constants)?","Is there an algebraic solution to  (,  constants)?","e^{-x/a}+e^{-x/b}=1 a\neq b a,b","Is there an algebraic solution for the to find the intersection of the following two functions for values of $x\geq 0$: $$f_1(x)=1-2e^{-x/a}=f_2(x)=-1+2e^{-x/b}$$ $a$ and $b$ are positive constants. The equation can be simplified to: $$e^{-x/a}+e^{-x/b}=1$$ A Plot is here: I am searching for the $x$-value of the intersection in the second plot (this is for an inversion recovery experiment inf magnetic resonance). If there is no algebraic solution, can you suggest a numerical algorithm for this problem? Thanks","Is there an algebraic solution for the to find the intersection of the following two functions for values of $x\geq 0$: $$f_1(x)=1-2e^{-x/a}=f_2(x)=-1+2e^{-x/b}$$ $a$ and $b$ are positive constants. The equation can be simplified to: $$e^{-x/a}+e^{-x/b}=1$$ A Plot is here: I am searching for the $x$-value of the intersection in the second plot (this is for an inversion recovery experiment inf magnetic resonance). If there is no algebraic solution, can you suggest a numerical algorithm for this problem? Thanks",,"['analysis', 'numerical-methods', 'closed-form', 'transcendental-equations']"
74,Matrix problem -- powers,Matrix problem -- powers,,1) Why is true that there is an open ball in $M_n$ centered at the identity matrix and a continuous function $f$ defined on this open ball s.t. $f(M)^2=M$ for $M$ in the ball? 2) Extending the question a bit:  Would I be right in thinking that we cannot do the same for any arbitrary matrix $M\in M_n$ because considering $n=1$ this clearly fails for the negative numbers? 3) What about higher powers? The case $n=1$ clearly works for all odd powers. But not for the even ones. But perhaps there is a catch for the odd powers too? Thanks.,1) Why is true that there is an open ball in $M_n$ centered at the identity matrix and a continuous function $f$ defined on this open ball s.t. $f(M)^2=M$ for $M$ in the ball? 2) Extending the question a bit:  Would I be right in thinking that we cannot do the same for any arbitrary matrix $M\in M_n$ because considering $n=1$ this clearly fails for the negative numbers? 3) What about higher powers? The case $n=1$ clearly works for all odd powers. But not for the even ones. But perhaps there is a catch for the odd powers too? Thanks.,,"['linear-algebra', 'analysis']"
75,Intuitive understanding a theorem in analysis,Intuitive understanding a theorem in analysis,,"Is there a way to intuitively understand/visualize the following theorem in analysis? Let $(f_n)$ be a sequence of real functions differentiable in a finite/infinite open interval $(a,b)$. Suppose that $(f_n(x))$ converges for at least one $x\in(a,b)$ and that $(f_n')$ converges uniformly on every finite closed subinterval of $(a,b)$. Then (i) $(f_n)$ converges uniformly on every finite closed subinterval of $(a,b)$ (ii)$f=\lim_{n\to\infty}f_n$ is differentiable in $(a,b)$ and $\forall x\in(a,b)$, $f'(x)=\lim_{n\to\infty}f_n'(x)$ Thanks.","Is there a way to intuitively understand/visualize the following theorem in analysis? Let $(f_n)$ be a sequence of real functions differentiable in a finite/infinite open interval $(a,b)$. Suppose that $(f_n(x))$ converges for at least one $x\in(a,b)$ and that $(f_n')$ converges uniformly on every finite closed subinterval of $(a,b)$. Then (i) $(f_n)$ converges uniformly on every finite closed subinterval of $(a,b)$ (ii)$f=\lim_{n\to\infty}f_n$ is differentiable in $(a,b)$ and $\forall x\in(a,b)$, $f'(x)=\lim_{n\to\infty}f_n'(x)$ Thanks.",,"['real-analysis', 'analysis', 'visualization']"
76,Showing a continuous functions on a compact subset of $\mathbb{R}^3$ can be uniformly approximated by polynomials,Showing a continuous functions on a compact subset of  can be uniformly approximated by polynomials,\mathbb{R}^3,"$\displaystyle X= \left \{\frac{x^2}{2} + \frac{y^2}{3} + \frac{z^2}{6} \leq 1 \right \}$ is a compact set If $f(x,y,z)$ is continuous on $X$ , then for any $\epsilon \gt 0$ , there exists a polynomial $p(x,y,z)$ such that $|f - p|\lt \epsilon$ on $X$ . I need to prove this and I have no idea how.","is a compact set If is continuous on , then for any , there exists a polynomial such that on . I need to prove this and I have no idea how.","\displaystyle X= \left \{\frac{x^2}{2} + \frac{y^2}{3} + \frac{z^2}{6} \leq 1 \right \} f(x,y,z) X \epsilon \gt 0 p(x,y,z) |f - p|\lt \epsilon X","['real-analysis', 'analysis']"
77,Uses of Divergent Series and their summation-values in mathematics?,Uses of Divergent Series and their summation-values in mathematics?,,"When I was trying to find closed-form representations for odd zeta-values, I used  $$ \Gamma(z) = \frac{e^{-\gamma \cdot z}}{z} \prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big)^{-1} e^{\frac{z}{n}} $$  and rearranged it to  $$ \frac{\Gamma(z)}{e^{-\gamma \cdot z}} = \prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big)^{-1} e^{z/n}. $$ As we know that $$\prod_{n=1}^{\infty} e^{z/n} = e^{z + z/2 + z/3 + \cdots + z/n} = e^{\zeta(1) z},$$  we can state that  $$\prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big) = \frac{e^{z(\zeta(1) - \gamma)}}{z\Gamma(z)}\qquad\text{(1)}$$ I then stumbled upon the Wikipedia page of Ramanujan Summation (see the bottom of the page), which I used to set $\zeta(1) = \gamma$ (which was, admittedly, a rather dangerous move. Remarkably, things went well eventually. Please don't stop reading). The $z^3$ -coefficient of both sides can now be obtained. Consider  \begin{align*} (1-ax)(1-bx) &= 1 - (a+b)x + abx^2\\ &= 1-(a+b)x + (1/2)((a+b)^2-(a^2+b^2)) \end{align*} and \begin{align*}(1-ax)(1-bx)(1-cx) &= 1 - (a + b + c)x\\ &\qquad + (1/2)\Bigl((a + b + c)^2 - (a^2 + b^2 + c^2)\Bigr)x^2\\ &\qquad -(abc)x^3. \end{align*} We can also set  \begin{align*} (abc)x^3 &= (1/3)\Bigl((a^3 + b^3 + c^3) - (a + b + c)\Bigr)\\ &\qquad + (1/2)(a + b + c)^3 -(a + b + c)(a^2 + b^2 + c^2). \end{align*} It can be proved by induction that the x^3 term of $(1-ax)(1-bx)\cdots(1-nx)$ is equal to  \begin{align*} (1/3)&\Bigl((a^3 + b^3 + c^3 +\cdots + n^3) - (a + b + c + \cdots + n^3))\\ &\qquad+ (1/2)(a + b + c + \cdots + n)^3 -(a + b + c + \cdots + n)(a^2 + b^2 + c^2 + \cdots + n^2).\qquad\text{(2)} \end{align*} On the right side of equation (1), the $z^3$-term can be found by looking at the z^3 term of the Taylor expansion series of $1/(z \Gamma(z))$, which is $(1/3)\zeta(3) + (1/2)\zeta(2) + (1/6)\gamma^3$. We then use (2) to obtain the equality $$ (1/6)\gamma^3 - (1/2)\gamma \pi^2 - (1/6) \psi^{(2)}(1) = 1/3)\zeta(3) + (1/2)\zeta(2) + (1/6)\gamma^3$$  to find that  $$\zeta(3) = - (1/2) \psi^{(2)} (1),$$ (3) which is a true result that has been  known ( known should be a hyperlink but it isn't for some reason) for quite a long time. The important thing here is that I used $\zeta(1) = \gamma$, which isn't really true. Ramanujan assigned a summation value to the harmonic series (again, see Ramanujan Summation), and apparently it can be used to verify results and perhaps to prove other conjectures/solve problems. My first question is: Is this a legitimate way to prove (3) ? Generalizing this question: When and how are divergent series and their summation values used in mathematics? What are the 'rules' when dealing with summed divergent series and using them to (try to) find new results? Thanks, Max","When I was trying to find closed-form representations for odd zeta-values, I used  $$ \Gamma(z) = \frac{e^{-\gamma \cdot z}}{z} \prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big)^{-1} e^{\frac{z}{n}} $$  and rearranged it to  $$ \frac{\Gamma(z)}{e^{-\gamma \cdot z}} = \prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big)^{-1} e^{z/n}. $$ As we know that $$\prod_{n=1}^{\infty} e^{z/n} = e^{z + z/2 + z/3 + \cdots + z/n} = e^{\zeta(1) z},$$  we can state that  $$\prod_{n=1}^{\infty} \Big( 1 + \frac{z}{n} \Big) = \frac{e^{z(\zeta(1) - \gamma)}}{z\Gamma(z)}\qquad\text{(1)}$$ I then stumbled upon the Wikipedia page of Ramanujan Summation (see the bottom of the page), which I used to set $\zeta(1) = \gamma$ (which was, admittedly, a rather dangerous move. Remarkably, things went well eventually. Please don't stop reading). The $z^3$ -coefficient of both sides can now be obtained. Consider  \begin{align*} (1-ax)(1-bx) &= 1 - (a+b)x + abx^2\\ &= 1-(a+b)x + (1/2)((a+b)^2-(a^2+b^2)) \end{align*} and \begin{align*}(1-ax)(1-bx)(1-cx) &= 1 - (a + b + c)x\\ &\qquad + (1/2)\Bigl((a + b + c)^2 - (a^2 + b^2 + c^2)\Bigr)x^2\\ &\qquad -(abc)x^3. \end{align*} We can also set  \begin{align*} (abc)x^3 &= (1/3)\Bigl((a^3 + b^3 + c^3) - (a + b + c)\Bigr)\\ &\qquad + (1/2)(a + b + c)^3 -(a + b + c)(a^2 + b^2 + c^2). \end{align*} It can be proved by induction that the x^3 term of $(1-ax)(1-bx)\cdots(1-nx)$ is equal to  \begin{align*} (1/3)&\Bigl((a^3 + b^3 + c^3 +\cdots + n^3) - (a + b + c + \cdots + n^3))\\ &\qquad+ (1/2)(a + b + c + \cdots + n)^3 -(a + b + c + \cdots + n)(a^2 + b^2 + c^2 + \cdots + n^2).\qquad\text{(2)} \end{align*} On the right side of equation (1), the $z^3$-term can be found by looking at the z^3 term of the Taylor expansion series of $1/(z \Gamma(z))$, which is $(1/3)\zeta(3) + (1/2)\zeta(2) + (1/6)\gamma^3$. We then use (2) to obtain the equality $$ (1/6)\gamma^3 - (1/2)\gamma \pi^2 - (1/6) \psi^{(2)}(1) = 1/3)\zeta(3) + (1/2)\zeta(2) + (1/6)\gamma^3$$  to find that  $$\zeta(3) = - (1/2) \psi^{(2)} (1),$$ (3) which is a true result that has been  known ( known should be a hyperlink but it isn't for some reason) for quite a long time. The important thing here is that I used $\zeta(1) = \gamma$, which isn't really true. Ramanujan assigned a summation value to the harmonic series (again, see Ramanujan Summation), and apparently it can be used to verify results and perhaps to prove other conjectures/solve problems. My first question is: Is this a legitimate way to prove (3) ? Generalizing this question: When and how are divergent series and their summation values used in mathematics? What are the 'rules' when dealing with summed divergent series and using them to (try to) find new results? Thanks, Max",,"['analysis', 'sequences-and-series', 'big-list']"
78,Existence and Uniqueness of Equilibrium points for Concave N-Person Games,Existence and Uniqueness of Equilibrium points for Concave N-Person Games,,"I am reading a paper . I have a problems with understanding their lemma Lemma: The nonzero elements of every vector $u \in U(x)$ are given by a vector $\bar{u} \in E^k, \bar{k} \leqslant k$ , where $\bar{u}=-\left(\bar{H}^{\prime} \bar{H}\right)^{-1} \bar{H}^{\prime} g(x, \bar{r}) \geqslant 0$ . The $m \times \bar{k}$ matrix $\bar{H}=\bar{H}(x)$ consists of $\bar{k}$ linearly independent columns of $H(x)$ selected from $\nabla h_j(x)$ for $j \in J$ . Let me recall some useful notations We define an $m \times k$ matrix $H(x)$ whose $jth$ column is $\nabla h_j(x)$ $$H(x)=\left[\begin{array}{lll} \nabla h_1(x) & \nabla h_2(x) \ldots \nabla h_k(x) \end{array}\right]$$ $g(x,r)$ is a mapping of $E^m$ into itself $$g(x, r)=\left[\begin{array}{c} r_1 \nabla_1 \varphi_1(x) \\ r_2 \nabla_2 \varphi_2(x) \\ \vdots \\ r_n \nabla_n \varphi_n(x) \end{array}\right]$$ We define the mapping $f(x,u,r)$ of $E^{m+k} \to E^m$ for each fixed $\bar{r}>0$ as follows $$f(x, u, \bar{r})=g(x, \bar{r})+H(x) u$$ where $u \in U(x) \subset E^k$ such that $$U(x)=\left\{u \mid\|f(x, u, \bar{r})\|=\min _{\substack{v_j \geqslant 0, j \in J \\ v_j=0, j \neq J}}\|f(x, v, \bar{r})\|\right\}$$ $$J=J(x)=\left\{j \mid h_j(x) \leqslant 0\right\}$$ Thank you very much for your help. I appreciate it a lot","I am reading a paper . I have a problems with understanding their lemma Lemma: The nonzero elements of every vector are given by a vector , where . The matrix consists of linearly independent columns of selected from for . Let me recall some useful notations We define an matrix whose column is is a mapping of into itself We define the mapping of for each fixed as follows where such that Thank you very much for your help. I appreciate it a lot","u \in U(x) \bar{u} \in E^k, \bar{k} \leqslant k \bar{u}=-\left(\bar{H}^{\prime} \bar{H}\right)^{-1} \bar{H}^{\prime} g(x, \bar{r}) \geqslant 0 m \times \bar{k} \bar{H}=\bar{H}(x) \bar{k} H(x) \nabla h_j(x) j \in J m \times k H(x) jth \nabla h_j(x) H(x)=\left[\begin{array}{lll}
\nabla h_1(x) & \nabla h_2(x) \ldots \nabla h_k(x)
\end{array}\right] g(x,r) E^m g(x, r)=\left[\begin{array}{c}
r_1 \nabla_1 \varphi_1(x) \\
r_2 \nabla_2 \varphi_2(x) \\
\vdots \\
r_n \nabla_n \varphi_n(x)
\end{array}\right] f(x,u,r) E^{m+k} \to E^m \bar{r}>0 f(x, u, \bar{r})=g(x, \bar{r})+H(x) u u \in U(x) \subset E^k U(x)=\left\{u \mid\|f(x, u, \bar{r})\|=\min _{\substack{v_j \geqslant 0, j \in J \\ v_j=0, j \neq J}}\|f(x, v, \bar{r})\|\right\} J=J(x)=\left\{j \mid h_j(x) \leqslant 0\right\}","['analysis', 'optimization', 'convex-analysis', 'convex-optimization', 'karush-kuhn-tucker']"
79,The uniqueness of the damped Sine-Gordon.,The uniqueness of the damped Sine-Gordon.,,"The damped Sine-Gordon equation given : \begin{equation} \partial_{tt} u + \alpha \partial_t u - \Delta u + \beta \sin(u) = 0 \end{equation} for an unknown $u : D \times [0, \infty) \rightarrow \mathbb{R}$ with $D \subset \mathbb{R}^n$ a bounded open set with smooth boundary. Here $\alpha > 0$ and $\beta \in \mathbb{R}$ are given parameters. The equation is supplemented with Dirichlet boundary conditions: \begin{equation} u|_{\partial D} = 0 \end{equation} and (smooth) initial conditions: \begin{equation} u(0) = u_0, \quad \partial_t u(0) = v_0. \end{equation} (i) Show that if $u^{(1)}$ and $u^{(2)}$ are smooth solutions of the damped Sine-Gordon equation with the same initial conditions, namely if \begin{equation} u^{(1)}(0,x) = u^{(2)}(0,x), \quad \partial_t u^{(1)}(0,x) = \partial_t u^{(2)}(0,x) \quad \text{for all } x \in D \end{equation} then $u^{(1)} = u^{(2)}$ on all of $D \times [0, \infty).$ (ii) Now suppose that $u^{(1)}$ and $u^{(2)}$ are any two smooth solutions of the damped Sine-Gordon equation subject to the above boundary condition. Show that there exists a constant $\rho > 0,$ depending only on $D,$ such that the quantity \begin{equation} \mathcal{E}(t) := \int_D \big[(\partial_t( u^{(1)}(t,x) - u^{(2)}(t,x))^2 + |\nabla(u^{(1)}(t,x) - u^{(2)}(t,x))|^2\big] dx \end{equation} with an exponential rate in time whenever \begin{equation} \frac{|\beta|}{\alpha} < \rho. \end{equation} Proof: This is my proof for i) but I am not sure if it is correct and I am stuck on ii) Let $w = u^{(1)} -u^{(2)}$ then $w$ satisfies: \begin{equation} \partial_{tt} w + \alpha \partial_t w - \Delta w + \beta [\sin(u^{(1)})-\sin(u^{(2)})] = 0 \end{equation} \begin{equation} w|_{\partial D} = 0 \end{equation} \begin{equation} w(0,x) = \partial_t w(0,x) = 0 \end{equation} Let $E(t)=\dfrac{1}{2}\displaystyle \int_{D}(w_t)^2+|\nabla w|^2dx$ then \begin{align} \dot{E}(t)&=\displaystyle\int_D w_t(w_{tt}-\Delta w))\\ & = -\alpha \displaystyle\int_D (w_t)^2dx -\beta \displaystyle\int_D (w_t)[\sin(u^{(1)})-\sin(u^{(2)})]\\ & \leq  -\alpha \displaystyle\int_D (w_t)^2dx +|\beta|\displaystyle\int_D |w||w_t|dx \end{align} Apply Young- $\varepsilon$ and Poincare's inequality for $|\beta| \displaystyle\int_D |w||w_t|dx$ , we deduce $$\dot{E}(t) \leq \left(\dfrac{|\beta|\varepsilon}{2}-\alpha \right)\displaystyle\int_D (w_t)^2dx +\dfrac{|\beta|C_0}{2\varepsilon} \displaystyle\int_D |\nabla w|^2dx$$ Choose $C = \max \{\dfrac{|\beta| \varepsilon}{2}-\alpha;\dfrac{|\beta|C_0}{2\varepsilon}\}$ then $$\dot{E}(t) \leq C E(t)$$ By Gronwall's inequality,we get $$E(t) \leq e^C.E(0)=0$$ Thus $E(t)=0,$ which implies $w=0$ then we complete the proof. Am I correct for $i$ ? And for ii) I thought that we can do the same like i), for $\rho$ we can choose $\rho = \max \{\dfrac{|\beta| \varepsilon}{2}-\alpha;\dfrac{|\beta|C_0}{2\varepsilon}; \dfrac{|\beta|}{\alpha}\}$ but the problem is $\rho$ is not only dependent on $D$ but also $\beta$ and $\alpha,$ which does not satisfy the requirement. Could someone give me any better ideas? Because of ii) give the hint $\frac{|\beta|}{\alpha} <\rho$ , it makes me think my approach for i) may not be good. The constant $C$ I chose quite not well. Hint: The professor gave us the hint that: For i) If $w = u^1 - u^2$ consider the equation satisfied by $v^\epsilon = \partial_t w + \epsilon w,$ for an $\epsilon > 0$ a parameter to be tuned.  Then try to do an $L^2$ type energy estimate for $v^\epsilon.$ So it is different from what I did in the proof. But I haven't yet figured out the hint. For ii) You can also add two simplifications here to this part of the problem 1) that the domain yields a Poincare constant of 1 to simply calculations and 2) that you only need to prove the exponential decay for $\beta < \rho \min\{\alpha, 1\}$ for an appropriate choice of rho (I think something like $\rho=1/2$ will do).  It's just playing with epsilon young's inequality to get an appropriate differential inequality to apply Gronwall.","The damped Sine-Gordon equation given : for an unknown with a bounded open set with smooth boundary. Here and are given parameters. The equation is supplemented with Dirichlet boundary conditions: and (smooth) initial conditions: (i) Show that if and are smooth solutions of the damped Sine-Gordon equation with the same initial conditions, namely if then on all of (ii) Now suppose that and are any two smooth solutions of the damped Sine-Gordon equation subject to the above boundary condition. Show that there exists a constant depending only on such that the quantity with an exponential rate in time whenever Proof: This is my proof for i) but I am not sure if it is correct and I am stuck on ii) Let then satisfies: Let then Apply Young- and Poincare's inequality for , we deduce Choose then By Gronwall's inequality,we get Thus which implies then we complete the proof. Am I correct for ? And for ii) I thought that we can do the same like i), for we can choose but the problem is is not only dependent on but also and which does not satisfy the requirement. Could someone give me any better ideas? Because of ii) give the hint , it makes me think my approach for i) may not be good. The constant I chose quite not well. Hint: The professor gave us the hint that: For i) If consider the equation satisfied by for an a parameter to be tuned.  Then try to do an type energy estimate for So it is different from what I did in the proof. But I haven't yet figured out the hint. For ii) You can also add two simplifications here to this part of the problem 1) that the domain yields a Poincare constant of 1 to simply calculations and 2) that you only need to prove the exponential decay for for an appropriate choice of rho (I think something like will do).  It's just playing with epsilon young's inequality to get an appropriate differential inequality to apply Gronwall.","\begin{equation}
\partial_{tt} u + \alpha \partial_t u - \Delta u + \beta \sin(u) = 0
\end{equation} u : D \times [0, \infty) \rightarrow \mathbb{R} D \subset \mathbb{R}^n \alpha > 0 \beta \in \mathbb{R} \begin{equation}
u|_{\partial D} = 0
\end{equation} \begin{equation}
u(0) = u_0, \quad \partial_t u(0) = v_0.
\end{equation} u^{(1)} u^{(2)} \begin{equation}
u^{(1)}(0,x) = u^{(2)}(0,x), \quad \partial_t u^{(1)}(0,x) = \partial_t u^{(2)}(0,x) \quad \text{for all } x \in D
\end{equation} u^{(1)} = u^{(2)} D \times [0, \infty). u^{(1)} u^{(2)} \rho > 0, D, \begin{equation}
\mathcal{E}(t) := \int_D \big[(\partial_t( u^{(1)}(t,x) - u^{(2)}(t,x))^2 + |\nabla(u^{(1)}(t,x) - u^{(2)}(t,x))|^2\big] dx
\end{equation} \begin{equation}
\frac{|\beta|}{\alpha} < \rho.
\end{equation} w = u^{(1)} -u^{(2)} w \begin{equation}
\partial_{tt} w + \alpha \partial_t w - \Delta w + \beta [\sin(u^{(1)})-\sin(u^{(2)})] = 0
\end{equation} \begin{equation}
w|_{\partial D} = 0
\end{equation} \begin{equation}
w(0,x) = \partial_t w(0,x) = 0
\end{equation} E(t)=\dfrac{1}{2}\displaystyle \int_{D}(w_t)^2+|\nabla w|^2dx \begin{align}
\dot{E}(t)&=\displaystyle\int_D w_t(w_{tt}-\Delta w))\\
& = -\alpha \displaystyle\int_D (w_t)^2dx -\beta \displaystyle\int_D (w_t)[\sin(u^{(1)})-\sin(u^{(2)})]\\
& \leq  -\alpha \displaystyle\int_D (w_t)^2dx +|\beta|\displaystyle\int_D |w||w_t|dx
\end{align} \varepsilon |\beta| \displaystyle\int_D |w||w_t|dx \dot{E}(t) \leq \left(\dfrac{|\beta|\varepsilon}{2}-\alpha \right)\displaystyle\int_D (w_t)^2dx +\dfrac{|\beta|C_0}{2\varepsilon} \displaystyle\int_D |\nabla w|^2dx C = \max \{\dfrac{|\beta| \varepsilon}{2}-\alpha;\dfrac{|\beta|C_0}{2\varepsilon}\} \dot{E}(t) \leq C E(t) E(t) \leq e^C.E(0)=0 E(t)=0, w=0 i \rho \rho = \max \{\dfrac{|\beta| \varepsilon}{2}-\alpha;\dfrac{|\beta|C_0}{2\varepsilon}; \dfrac{|\beta|}{\alpha}\} \rho D \beta \alpha, \frac{|\beta|}{\alpha} <\rho C w = u^1 - u^2 v^\epsilon = \partial_t w + \epsilon w, \epsilon > 0 L^2 v^\epsilon. \beta < \rho \min\{\alpha, 1\} \rho=1/2","['analysis', 'partial-differential-equations', 'young-inequality', 'gronwall-type-inequality']"
80,Approximation using polynomials with integer coefficients,Approximation using polynomials with integer coefficients,,"The classic Weierstrass Theorem states that the set of polynomials are dense in $C[0,1]$ equipped with $|| \cdot ||_{\infty}$ . Bernstein's proof of Weierstrass Theorem gives an explicit form of polynomials that approximate a given function $f \in C[0,1]$ . $$B_n^f(x) := \sum_{k=0}^n f(\frac kn) \binom nk x^k (1-x)^{n-k}.$$ If we want to approximate a function by using polynomials of integer coefficients, we must have $f(0)$ and $f(1)$ are both integers. Otherwise, $||f - p||_\infty \ge \varepsilon >0$ for all polynomials $p$ with integer coefficients. My question is if the converse is true, that is, if $f(0)$ and $f(1)$ are both integers, then can we approximate $f$ by a sequence $\{p_n\}_{n \in \mathbb{N}}$ of polynomials with integer coefficients, $||p_n - f||_\infty \to 0$ as $n \to \infty$ . This can be reduced to the case that $f(0) = f(1) = 0$ . Assuming the statement is true for all $f \in C[0,1]$ with $f(0) = f(1) = 0$ . Given $g \in C[0,1]$ with $g(0) = a, g(1) = b$ both integers, we can approximate $f(x) = g(x) - (b-a)x - a$ by a sequence $\{p_n\}$ with integer coefficients by assumption, since $f(0) = g(0) - a = 0$ and $f(1) = g(1) - b = 0$ . Then $g(x)$ can be approximated by $p_n +(b-a)x + a$ .","The classic Weierstrass Theorem states that the set of polynomials are dense in equipped with . Bernstein's proof of Weierstrass Theorem gives an explicit form of polynomials that approximate a given function . If we want to approximate a function by using polynomials of integer coefficients, we must have and are both integers. Otherwise, for all polynomials with integer coefficients. My question is if the converse is true, that is, if and are both integers, then can we approximate by a sequence of polynomials with integer coefficients, as . This can be reduced to the case that . Assuming the statement is true for all with . Given with both integers, we can approximate by a sequence with integer coefficients by assumption, since and . Then can be approximated by .","C[0,1] || \cdot ||_{\infty} f \in C[0,1] B_n^f(x) := \sum_{k=0}^n f(\frac kn) \binom nk x^k (1-x)^{n-k}. f(0) f(1) ||f - p||_\infty \ge \varepsilon >0 p f(0) f(1) f \{p_n\}_{n \in \mathbb{N}} ||p_n - f||_\infty \to 0 n \to \infty f(0) = f(1) = 0 f \in C[0,1] f(0) = f(1) = 0 g \in C[0,1] g(0) = a, g(1) = b f(x) = g(x) - (b-a)x - a \{p_n\} f(0) = g(0) - a = 0 f(1) = g(1) - b = 0 g(x) p_n +(b-a)x + a","['real-analysis', 'analysis', 'polynomials']"
81,Infinite products that are equal to their geometric product integral,Infinite products that are equal to their geometric product integral,,"Background In the following question and references therein, a number of ""Sum equals integral"" identities are described. For instance, we have $$ \sum_{n = -\infty}^{+\infty} {\rm sinc} (x)^{N}\, = \int_{-\infty}^{+\infty} {\rm sinc} (x)^{N}\, dx =  \pi, \tag{1}\label{1}$$ for $1\leq N \leq 6$ , about which one can find more information in this paper (PDF) by Baillie, Borwein, and Borwein from 2008. Moreover, there is the binomial identity $$\sum_{n = -\infty}^{+\infty} \binom{\alpha}{n} e^{int} = \int_{-\infty}^{+\infty} \binom{\alpha}{x} e^{itx} \, dx = (1+e^{it})^\alpha, \; \alpha  >-1 \tag{2}\label{2}$$ which is due to Pollard & Shisha (1973). Finally, the authors Dominici, Gill, and Limpanuparb (2012) state the following identity involving the Bessel J function in their article (PDF) $$\sum_{t=-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}=\int_{-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}\, \text{d}t. \tag{3}\label{3}$$ Product integrals I wonder whether similar identities exist involving infinite products and their corresponding product integral . The latter is a continuous analogue of the discrete product operator. There are multiple types of product integrals. For the purposes of this particular question, we stick to product integrals that are referred to as Type II in the wiki page referenced above: \begin{align*} \prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline  &= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{4} \label{4} \end{align*} This is called the geometric product integral . So what I'm looking for are identities of the form $$ \prod_{n=a}^{\infty} f(n) = \prod_{a}^{\infty} f(x)^{dx}. \tag{5}\label{5}  $$ Here, $a=0$ , $a=1$ , or $a=-\infty$ . In other words, I'm looking for identities that satisfy $$ \prod_{n=a}^{\infty} f(n) = \exp \left( \int_{a}^{\infty} \ln f(x) \ dx \right). \tag{6}\label{6}$$ If the identity holds when the limits of the product (integral) need to be shifted slightly on either side of the equation to make it work, that would also be a good example in my eyes. Own work and Question I've gone through a number of possibilities listed on this page . One example that comes somewhat close, but not quite, is the one associated with equation (30). It states that $$ \prod_{n=1}^{\infty} \left(1+\frac{1}{n^3} \right) = \frac{1}{\pi} \cosh \left( \frac{1}{2} \pi \sqrt{3} \right). \tag{7}\label{7} $$ Moreover, we have $$ \exp \left( \int_{0}^{\infty} \ln \left[ 1+ \frac{1}{x^3} \right] \ dx \right) = \exp \left( \frac{2 \pi}{ \sqrt{3}} \right). \tag{8}\label{8} $$ We know that $\cosh(x) := \frac{\exp(x)+\exp(-x)}{2}$ , so it appears there are some similarities in the final expression. (Notice that, in this case, we have slightly shifted the limits of the geometric product integral.)  However, \eqref{7} and \eqref{8} do no amount to the same number. So there is more work to do. Therefore, my question is: Are there any infinite products that are equal to their geometric product integral, thus satisfying equation \eqref{6} or some slight variant of it?","Background In the following question and references therein, a number of ""Sum equals integral"" identities are described. For instance, we have for , about which one can find more information in this paper (PDF) by Baillie, Borwein, and Borwein from 2008. Moreover, there is the binomial identity which is due to Pollard & Shisha (1973). Finally, the authors Dominici, Gill, and Limpanuparb (2012) state the following identity involving the Bessel J function in their article (PDF) Product integrals I wonder whether similar identities exist involving infinite products and their corresponding product integral . The latter is a continuous analogue of the discrete product operator. There are multiple types of product integrals. For the purposes of this particular question, we stick to product integrals that are referred to as Type II in the wiki page referenced above: This is called the geometric product integral . So what I'm looking for are identities of the form Here, , , or . In other words, I'm looking for identities that satisfy If the identity holds when the limits of the product (integral) need to be shifted slightly on either side of the equation to make it work, that would also be a good example in my eyes. Own work and Question I've gone through a number of possibilities listed on this page . One example that comes somewhat close, but not quite, is the one associated with equation (30). It states that Moreover, we have We know that , so it appears there are some similarities in the final expression. (Notice that, in this case, we have slightly shifted the limits of the geometric product integral.)  However, \eqref{7} and \eqref{8} do no amount to the same number. So there is more work to do. Therefore, my question is: Are there any infinite products that are equal to their geometric product integral, thus satisfying equation \eqref{6} or some slight variant of it?"," \sum_{n = -\infty}^{+\infty} {\rm sinc} (x)^{N}\, = \int_{-\infty}^{+\infty} {\rm sinc} (x)^{N}\, dx =  \pi, \tag{1}\label{1} 1\leq N \leq 6 \sum_{n = -\infty}^{+\infty} \binom{\alpha}{n} e^{int} = \int_{-\infty}^{+\infty} \binom{\alpha}{x} e^{itx} \, dx = (1+e^{it})^\alpha, \; \alpha  >-1 \tag{2}\label{2} \sum_{t=-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}=\int_{-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}\, \text{d}t. \tag{3}\label{3} \begin{align*}
\prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline 
&= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{4} \label{4}
\end{align*}  \prod_{n=a}^{\infty} f(n) = \prod_{a}^{\infty} f(x)^{dx}. \tag{5}\label{5}   a=0 a=1 a=-\infty  \prod_{n=a}^{\infty} f(n) = \exp \left( \int_{a}^{\infty} \ln f(x) \ dx \right). \tag{6}\label{6}  \prod_{n=1}^{\infty} \left(1+\frac{1}{n^3} \right) = \frac{1}{\pi} \cosh \left( \frac{1}{2} \pi \sqrt{3} \right). \tag{7}\label{7}   \exp \left( \int_{0}^{\infty} \ln \left[ 1+ \frac{1}{x^3} \right] \ dx \right) = \exp \left( \frac{2 \pi}{ \sqrt{3}} \right). \tag{8}\label{8}  \cosh(x) := \frac{\exp(x)+\exp(-x)}{2}","['analysis', 'definite-integrals', 'products', 'big-list']"
82,Continuous analogue of the discrete simple continued fraction,Continuous analogue of the discrete simple continued fraction,,"Background The classical Riemann integral of a function $f : [a,b] \to \mathbb{R}$ can be defined by setting $$\int_{a}^{b} f(x) \ dx := \lim_{\Delta x \to 0} \sum f(x_{i}) \ \Delta x.  $$ Here, the limit is taken over all partitions of the interval $[a,b]$ whose norms approach zero. We can do something roughly similar with product integrals . They take the limit over a product instead of a sum, and can be interpreted as continuous analogues of discrete products. There are multiple types of product integrals. Type I is often refered to as Volterra's integral . It is defined as follows: \begin{align*} \prod_{a}^{b} \left(1+f(x) \ dx \right) &:= \lim_{\Delta x \to 0} \left(1 + f(x_{i}) \ \Delta x \right) \newline &= \exp \left( \int_{a}^{b} f(x) \ dx \right). \tag{1} \label{1} \end{align*} However, this is not a multiplicative operator. As an alternative, there is also Type II, the geometric integral. It is defined as \begin{align*} \prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline  &= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{2} \label{2} \end{align*} This does amount to a multiplicative operator. A third type, the bigeometric integral, is also an operator with this property. Question I wonder whether something similar can be done with other kinds of expressions ( infinite or finite). In particular, I am curious whether we can obtain a continuous analogue of the discrete simple continued fraction . In the discrete case, it is defined as: $$\underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} = \cfrac{1}{f(a) + \cfrac{1}{f(a+1)+\cfrac{1}{\ddots+\cfrac{1}{f(b-1) + \cfrac{1}{f(b)}}}}}$$ In other words, I am looking for a way to complete the following table, by finding a definition of what is described in the bottom right cell: \begin{array}{|c|c|c|} \hline & \text{additive} & \text{multiplicative} & \text{simple continued fraction} \\ \hline \text{discrete} & \sum_{i=a}^{b} f(i) & \prod_{i=a}^{b} f(i) & \underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} \\ \hline \text{continuous} & \int_{a}^{b} f(x) \ dx  & \prod_{a}^{b} f(x)^{dx}  & \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} \\ \hline \end{array} I think we could make a start by setting $$ \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} := \lim_{\Delta x \to 0} \large{\mathrm K} \frac{1}{f(x_{i})} \Delta x  .$$ However, I am not sure how this would translate into a formula that is similar to \eqref{1} or \eqref{2}. Is there a way to obtain such a formula (or perhaps multiple, as in the case of the product integral), and if so, what does it look like?","Background The classical Riemann integral of a function can be defined by setting Here, the limit is taken over all partitions of the interval whose norms approach zero. We can do something roughly similar with product integrals . They take the limit over a product instead of a sum, and can be interpreted as continuous analogues of discrete products. There are multiple types of product integrals. Type I is often refered to as Volterra's integral . It is defined as follows: However, this is not a multiplicative operator. As an alternative, there is also Type II, the geometric integral. It is defined as This does amount to a multiplicative operator. A third type, the bigeometric integral, is also an operator with this property. Question I wonder whether something similar can be done with other kinds of expressions ( infinite or finite). In particular, I am curious whether we can obtain a continuous analogue of the discrete simple continued fraction . In the discrete case, it is defined as: In other words, I am looking for a way to complete the following table, by finding a definition of what is described in the bottom right cell: I think we could make a start by setting However, I am not sure how this would translate into a formula that is similar to \eqref{1} or \eqref{2}. Is there a way to obtain such a formula (or perhaps multiple, as in the case of the product integral), and if so, what does it look like?","f : [a,b] \to \mathbb{R} \int_{a}^{b} f(x) \ dx := \lim_{\Delta x \to 0} \sum f(x_{i}) \ \Delta x.   [a,b] \begin{align*}
\prod_{a}^{b} \left(1+f(x) \ dx \right) &:= \lim_{\Delta x \to 0} \left(1 + f(x_{i}) \ \Delta x \right) \newline
&= \exp \left( \int_{a}^{b} f(x) \ dx \right). \tag{1} \label{1}
\end{align*} \begin{align*}
\prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline 
&= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{2} \label{2}
\end{align*} \underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} = \cfrac{1}{f(a) + \cfrac{1}{f(a+1)+\cfrac{1}{\ddots+\cfrac{1}{f(b-1) + \cfrac{1}{f(b)}}}}} \begin{array}{|c|c|c|}
\hline
& \text{additive} & \text{multiplicative} & \text{simple continued fraction} \\ \hline
\text{discrete} & \sum_{i=a}^{b} f(i) & \prod_{i=a}^{b} f(i) & \underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} \\ \hline
\text{continuous} & \int_{a}^{b} f(x) \ dx  & \prod_{a}^{b} f(x)^{dx}  & \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} \\ \hline
\end{array}  \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} := \lim_{\Delta x \to 0} \large{\mathrm K} \frac{1}{f(x_{i})} \Delta x  .","['integration', 'analysis', 'definition', 'products', 'continued-fractions']"
83,To what extent smooth mappings of an affine line into a manifold determine its differentiable structure?,To what extent smooth mappings of an affine line into a manifold determine its differentiable structure?,,"If $M$ is a (real) differentiable manifold, its differentiable structure is completely determined if it is known which mappings $M\to\mathbf{R}$ are smooth. How much can be said about the differentiable structure of $M$ if it is known which mappings $\mathbf{R}\to M$ are smooth? I suspect that this is not enough to determine the differentiable structure of $M$ . If so, can there be a number $k <\operatorname{dim}M$ such that the smooth mappings $\mathbf{R}^k\to M$ completely determine the differentiable structure of $M$ ? I think my first question can be rephrased as follows: if $f\colon\mathbf{R}^n\to\mathbf{R}$ has the property that for every smooth $\gamma\colon\mathbf{R}\to\mathbf{R}^n$ , the composition $f\circ\gamma\colon\mathbf{R}\to\mathbf{R}$ is smooth, does this imply that $f$ is smooth? I conjecture that if $f\colon\mathbf{R}^n\to\mathbf{R}$ has the property that for every smooth $\sigma\colon\mathbf{R}^2\to\mathbf{R}^n$ , the composition $f\circ\sigma\colon\mathbf{R}^2\to\mathbf{R}$ is smooth, then $f$ is smooth.","If is a (real) differentiable manifold, its differentiable structure is completely determined if it is known which mappings are smooth. How much can be said about the differentiable structure of if it is known which mappings are smooth? I suspect that this is not enough to determine the differentiable structure of . If so, can there be a number such that the smooth mappings completely determine the differentiable structure of ? I think my first question can be rephrased as follows: if has the property that for every smooth , the composition is smooth, does this imply that is smooth? I conjecture that if has the property that for every smooth , the composition is smooth, then is smooth.",M M\to\mathbf{R} M \mathbf{R}\to M M k <\operatorname{dim}M \mathbf{R}^k\to M M f\colon\mathbf{R}^n\to\mathbf{R} \gamma\colon\mathbf{R}\to\mathbf{R}^n f\circ\gamma\colon\mathbf{R}\to\mathbf{R} f f\colon\mathbf{R}^n\to\mathbf{R} \sigma\colon\mathbf{R}^2\to\mathbf{R}^n f\circ\sigma\colon\mathbf{R}^2\to\mathbf{R} f,"['differential-geometry', 'soft-question', 'manifolds', 'differential-topology', 'smooth-manifolds']"
84,Are there Examples of a function with elementary antiderivative that we know its antiderivative is too big too be written down?,Are there Examples of a function with elementary antiderivative that we know its antiderivative is too big too be written down?,,"EDIT: I asked this question on MO here . I recently learned that there are many very large numbers that have been defined, such as $TREE(3)$ and many others that we cannot write down. What made me interested is the idea that there is a function that take some finite and small number to an absolute beast of a number. So I wonder if there is some function with an elementary antiderivative that we know its antiderivative is too large to be written down, but we can write down the function itself. What I mean by writing down the antiderivative:  Is to write it without any shorthand notation like $\sum_n f(x_n)c_n$ Like how we can't layout the digits of $TREE(3)$ if we wanted to But the antiderivative is finite. What I mean to be too large is the humanity can't write it down without any shorthand notation because the antiderivative has a lot of terms (say for example $10^{10000}$ term) and composition of functions. What I mean by    writing down the function is: It is possible to write down its distributive form without any shorthand notation like $(1+x)^4$ i will count this as shorthand and its distributive form is $1+4x+6x^2+4x^3+x^4$ so functions like $x^{TREE(3)},\  {TREE(3)}  $ doesn't count.","EDIT: I asked this question on MO here . I recently learned that there are many very large numbers that have been defined, such as and many others that we cannot write down. What made me interested is the idea that there is a function that take some finite and small number to an absolute beast of a number. So I wonder if there is some function with an elementary antiderivative that we know its antiderivative is too large to be written down, but we can write down the function itself. What I mean by writing down the antiderivative:  Is to write it without any shorthand notation like Like how we can't layout the digits of if we wanted to But the antiderivative is finite. What I mean to be too large is the humanity can't write it down without any shorthand notation because the antiderivative has a lot of terms (say for example term) and composition of functions. What I mean by    writing down the function is: It is possible to write down its distributive form without any shorthand notation like i will count this as shorthand and its distributive form is so functions like doesn't count.","TREE(3) \sum_n f(x_n)c_n TREE(3) 10^{10000} (1+x)^4 1+4x+6x^2+4x^3+x^4 x^{TREE(3)},\  {TREE(3)}  ","['integration', 'analysis', 'soft-question', 'indefinite-integrals', 'examples-counterexamples']"
85,Is there a theorem which provides conditions under which a power series satisfies the reciprocal root sum law?,Is there a theorem which provides conditions under which a power series satisfies the reciprocal root sum law?,,"This  paper discusses how to prove that $\sum\limits_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6} $ . The first proof on this paper is Euler's original proof: $$\frac{\sin(\sqrt x)}{\sqrt x} = 1- \frac{x}{3!}+ \frac{x^2}{5!}- \frac{x^3}{7!}+\dots$$ The roots of $\frac{\sin(\sqrt x)}{\sqrt x}$ n are the numbers $\pi^2, 4\pi^2, 9\pi^2, 16\pi^2, \dots$ Now Euler knew that adding up the reciprocals of all the roots of a polynomial results in the negative of the ratio of the linear coefficient to the constant coefficient. In symbols, if $$(x − r_1)(x − r_2)···(x − r_n) = x^{n} + a_{n−1}x^{n−1} + \dots + a_1x + a_0$$ then $$\sum_{k=1}^n \frac{1}{r_k}= \frac{-a_1}{a_0}$$ Assuming that the same law must hold for a power series expansion, he applied it to $\frac{\sin(\sqrt x)}{\sqrt x}$ , concluding that $$\frac{1}{6}=\sum\limits_{n=1}^\infty \frac{1}{(\pi n)^2}$$ Why is this not considered a valid proof today? The problem is that power series are not polynomials, and do not share all the properties of polynomials. The next page the author wrote this : $$\frac{1}{1 − x}=1+ x + x^2 + x^3 + \dots$$ holds for all $x$ of absolute value less than $1$ . Now consider the function $g(x)=2 − 1/(1 − x)$ . Clearly, $g$ has a single root, $1/2$ . The power series expansion for $g(x$ ) is $ 1−x−x^2−x^3−···$ , so $a_0 = 1$ and $a_1 = −1$ . The sum of the reciprocal roots does not equal the ratio $−a_1/a_0$ . While this example shows that the reciprocal root sum law cannot be applied blindly to all power series, it does not imply that the law never holds. Indeed, the law must hold for the function $\frac{\sin(\sqrt x)}{\sqrt x}$ because we have independent proofs of Euler’s result. Notice the differences between $\frac{\sin(\sqrt x)}{\sqrt x}$ and the $g$ of the counterexample. The function $\frac{\sin(\sqrt x)}{\sqrt x}$ has an infinite number of roots, where $g$ has but one. And $\frac{\sin(\sqrt x)}{\sqrt x}$ has a power series that converges for all $x$ , where the series for $g$ only converges for $−1 <x< 1$ . $\color{red}{\text{Is there a theorem which provides conditions}}$ $ \color{red}{\text{under which a power series satisfies the reciprocal root sum law? I don’t know.}}$ Now I tried to search for a this theorem and I couldn't find any thing. Is there is a theorem like this somewhere ? Is there a proof for a  under which a power series satisfies the reciprocal root sum law?","This  paper discusses how to prove that . The first proof on this paper is Euler's original proof: The roots of n are the numbers Now Euler knew that adding up the reciprocals of all the roots of a polynomial results in the negative of the ratio of the linear coefficient to the constant coefficient. In symbols, if then Assuming that the same law must hold for a power series expansion, he applied it to , concluding that Why is this not considered a valid proof today? The problem is that power series are not polynomials, and do not share all the properties of polynomials. The next page the author wrote this : holds for all of absolute value less than . Now consider the function . Clearly, has a single root, . The power series expansion for ) is , so and . The sum of the reciprocal roots does not equal the ratio . While this example shows that the reciprocal root sum law cannot be applied blindly to all power series, it does not imply that the law never holds. Indeed, the law must hold for the function because we have independent proofs of Euler’s result. Notice the differences between and the of the counterexample. The function has an infinite number of roots, where has but one. And has a power series that converges for all , where the series for only converges for . Now I tried to search for a this theorem and I couldn't find any thing. Is there is a theorem like this somewhere ? Is there a proof for a  under which a power series satisfies the reciprocal root sum law?","\sum\limits_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6}  \frac{\sin(\sqrt x)}{\sqrt x} = 1- \frac{x}{3!}+ \frac{x^2}{5!}- \frac{x^3}{7!}+\dots \frac{\sin(\sqrt x)}{\sqrt x} \pi^2, 4\pi^2, 9\pi^2, 16\pi^2, \dots (x − r_1)(x − r_2)···(x − r_n) = x^{n} + a_{n−1}x^{n−1} + \dots + a_1x + a_0 \sum_{k=1}^n \frac{1}{r_k}= \frac{-a_1}{a_0} \frac{\sin(\sqrt x)}{\sqrt x} \frac{1}{6}=\sum\limits_{n=1}^\infty \frac{1}{(\pi n)^2} \frac{1}{1 − x}=1+ x + x^2 + x^3 + \dots x 1 g(x)=2 − 1/(1 − x) g 1/2 g(x  1−x−x^2−x^3−··· a_0 = 1 a_1 = −1 −a_1/a_0 \frac{\sin(\sqrt x)}{\sqrt x} \frac{\sin(\sqrt x)}{\sqrt x} g \frac{\sin(\sqrt x)}{\sqrt x} g \frac{\sin(\sqrt x)}{\sqrt x} x g −1 <x< 1 \color{red}{\text{Is there a theorem which provides
conditions}}  \color{red}{\text{under which a power series satisfies the reciprocal root sum law? I don’t know.}}","['real-analysis', 'sequences-and-series', 'analysis', 'reference-request', 'summation']"
86,Lagrange-mean like inequality,Lagrange-mean like inequality,,"I had this problem on an assignment a while ago, but I don't quite understand the formulation of the problem nor the purpose: Let $f : [a,b]→\mathbb{R}$ be a function that admits a derivative (not necessarily finite!) at any point of $[a,b]$ . Prove that there exists $x_0 \in [a,b]$ such that: $|\frac{f(b)-f(a)}{b-a}|\leq |f'(x_0)|$ The original solution was a bit convoluted, and was by constructing some nested intervals, but can't we just say that if the derivative is infinite at some point then we're done, otherwise $f$ is differentiable and we're done by Lagrange's mean theorem? What's wrong with this?","I had this problem on an assignment a while ago, but I don't quite understand the formulation of the problem nor the purpose: Let be a function that admits a derivative (not necessarily finite!) at any point of . Prove that there exists such that: The original solution was a bit convoluted, and was by constructing some nested intervals, but can't we just say that if the derivative is infinite at some point then we're done, otherwise is differentiable and we're done by Lagrange's mean theorem? What's wrong with this?","f : [a,b]→\mathbb{R} [a,b] x_0 \in [a,b] |\frac{f(b)-f(a)}{b-a}|\leq |f'(x_0)| f",['analysis']
87,"Show that $f(a,b,c)=(a+b+c)^3+(a+b)^2+a$ is injective.",Show that  is injective.,"f(a,b,c)=(a+b+c)^3+(a+b)^2+a","For a function $f : \mathbb{N} \times \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N}$ defined by $f(a,b,c)=(a+b+c)^3+(a+b)^2+a$ I want to show that $f$ is injective. How can I show this? I started by assuming $(a,b,c) \neq (d,e,f)$ and $f(a,b,c)=f(d,e,f)$ and seeking a contradiction. However, I am uncertain about the subsequent steps. Thank you.","For a function defined by I want to show that is injective. How can I show this? I started by assuming and and seeking a contradiction. However, I am uncertain about the subsequent steps. Thank you.","f : \mathbb{N} \times \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N} f(a,b,c)=(a+b+c)^3+(a+b)^2+a f (a,b,c) \neq (d,e,f) f(a,b,c)=f(d,e,f)","['analysis', 'functions', 'elementary-set-theory', 'natural-numbers']"
88,Transport equation and entropy conditon,Transport equation and entropy conditon,,"Consider the transport equation with smooth coefficient $a \in C^1(\mathbb{R}\times \mathbb{R}^+)$ given by \begin{align} u_t+(a(x,t)u)_x=0. \end{align} A weak solusolution $u \in C(\mathbb{R}^+;L^{1}(\mathbb{R}))$ satifies the PDE in the sense of distribution. Since it is transport equation, one would expect any weak solution should automatically satisfy the Kruzkov entropy condition. How to prove this rigorously for any $u \in C(\mathbb{R}^+;L^{1}(\mathbb{R}))$ ?. P.S. If $u \in C^1(\mathbb{R}\times \mathbb{R}^+)$ is a weak solution, then a standard computaion shows that $u$ satisfies enropy condition for any $\eta \in C^1(\mathbb{R})$ convex and then by a density argument we can choose $\eta(u)=|u-k|$ by a limiting argument.","Consider the transport equation with smooth coefficient given by A weak solusolution satifies the PDE in the sense of distribution. Since it is transport equation, one would expect any weak solution should automatically satisfy the Kruzkov entropy condition. How to prove this rigorously for any ?. P.S. If is a weak solution, then a standard computaion shows that satisfies enropy condition for any convex and then by a density argument we can choose by a limiting argument.","a \in C^1(\mathbb{R}\times \mathbb{R}^+) \begin{align}
u_t+(a(x,t)u)_x=0.
\end{align} u \in C(\mathbb{R}^+;L^{1}(\mathbb{R})) u \in C(\mathbb{R}^+;L^{1}(\mathbb{R})) u \in C^1(\mathbb{R}\times \mathbb{R}^+) u \eta \in C^1(\mathbb{R}) \eta(u)=|u-k|","['analysis', 'partial-differential-equations', 'fluid-dynamics', 'transport-equation']"
89,Determining the Big O Order for a Non-Standard Recurrence Relation,Determining the Big O Order for a Non-Standard Recurrence Relation,,"Question: Find the best possible order for $ T(n) $ , for sufficiently large values of $ n $ . $ T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1 $ My try: Here is my attempt at finding the solution using induction to prove $T(n) \le cn^{\log_{2 - \varepsilon}(3)} + b$ : $ \begin{align*} \varepsilon & > 0 \\\\ T(n) & \leq 3T \left( \frac{n}{2} + \sqrt{n} \right) + 1 \\\\ & \leq 3T \left( \frac{n}{2 - \varepsilon} \right) + 1 \quad \text{(For n )} \\\\ T(n) & \leq 3c \left( \frac{n}{2 - \varepsilon} \right)^{\log_2(3)} + 3b + 1 \\\\ & = \frac{3cn^{\log_{2 - \varepsilon}(3)}}{(2 - \varepsilon)^{\log_{2 - \varepsilon}(3)}} + 3b + 1 \\\\ T(n) & \leq cn^{\log_2(3)} + 3b + 1 \leq cn^{\log_2(3)} + b \quad \text{if } b < -\frac{1}{2} \\\\ \varepsilon & > 0 \Rightarrow T(n) \in O \left( n^{\log_{2 - \varepsilon}(3)} \right) \end{align*} $ What I can't find: Provide a proof that setting $\varepsilon = 0$ is permissible. $ T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1 \in O \left( n^{\log_{2}(3)} \right)$","Question: Find the best possible order for , for sufficiently large values of . My try: Here is my attempt at finding the solution using induction to prove : What I can't find: Provide a proof that setting is permissible."," T(n)   n   T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1  T(n) \le cn^{\log_{2 - \varepsilon}(3)} + b 
\begin{align*}
\varepsilon & > 0 \\\\
T(n) & \leq 3T \left( \frac{n}{2} + \sqrt{n} \right) + 1 \\\\
& \leq 3T \left( \frac{n}{2 - \varepsilon} \right) + 1 \quad \text{(For n )} \\\\
T(n) & \leq 3c \left( \frac{n}{2 - \varepsilon} \right)^{\log_2(3)} + 3b + 1 \\\\
& = \frac{3cn^{\log_{2 - \varepsilon}(3)}}{(2 - \varepsilon)^{\log_{2 - \varepsilon}(3)}} + 3b + 1 \\\\
T(n) & \leq cn^{\log_2(3)} + 3b + 1 \leq cn^{\log_2(3)} + b \quad \text{if } b < -\frac{1}{2} \\\\
\varepsilon & > 0 \Rightarrow T(n) \in O \left( n^{\log_{2 - \varepsilon}(3)} \right)
\end{align*}
 \varepsilon = 0  T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1 \in O \left( n^{\log_{2}(3)} \right)","['analysis', 'asymptotics', 'recurrence-relations', 'computational-complexity']"
90,Determining whether an operator has maximal rank,Determining whether an operator has maximal rank,,"Consider the following We showed that Lie point symmetries are useful for solving ODE's and PDE's. But how do we find them the first place? 5.4.1 Trivial Case Lets try to find Lie point symmetries of an 0th order ODE $$ \Delta[x, u]=0 . $$ By definition $g^{\varepsilon}:(x, u) \mapsto(\tilde{x}, \tilde{u})$ is a Lie point symmetry if $$ \Delta[x, u]=0 \quad \Longrightarrow \Delta[\tilde{x}, \tilde{u}]=\Delta\left[g^{\varepsilon}(x, u)\right]=0 . $$ Question: Can we reduce this to a statement about the generator of $g^{\varepsilon}$ ? The answer is yes, but will need to assume that $\Delta$ is of maximal rank. Defn (Maximal Rank) The operator $\Delta$ is of maximal rank if the matrix of derivatives $$ \frac{\partial \Delta_j}{\partial y_i} $$ is of maximal rank, where the $y_i$ runs over $x, u$ , and in general all coordinates. The definition makes no sense to me. The text gives $$     \Delta[x, u] = x^{2}. $$ as an operator which does not have maximal rank.  The issue for me is that this is not a vector, so how can I take its $i$ -th component? Question: Could someone clarify here what the partial derivatives mean with this indentation?","Consider the following We showed that Lie point symmetries are useful for solving ODE's and PDE's. But how do we find them the first place? 5.4.1 Trivial Case Lets try to find Lie point symmetries of an 0th order ODE By definition is a Lie point symmetry if Question: Can we reduce this to a statement about the generator of ? The answer is yes, but will need to assume that is of maximal rank. Defn (Maximal Rank) The operator is of maximal rank if the matrix of derivatives is of maximal rank, where the runs over , and in general all coordinates. The definition makes no sense to me. The text gives as an operator which does not have maximal rank.  The issue for me is that this is not a vector, so how can I take its -th component? Question: Could someone clarify here what the partial derivatives mean with this indentation?","
\Delta[x, u]=0 .
 g^{\varepsilon}:(x, u) \mapsto(\tilde{x}, \tilde{u}) 
\Delta[x, u]=0 \quad \Longrightarrow \Delta[\tilde{x}, \tilde{u}]=\Delta\left[g^{\varepsilon}(x, u)\right]=0 .
 g^{\varepsilon} \Delta \Delta 
\frac{\partial \Delta_j}{\partial y_i}
 y_i x, u 
    \Delta[x, u] = x^{2}.
 i","['analysis', 'partial-differential-equations', 'lie-algebras']"
91,Solving $\nabla f=g$,Solving,\nabla f=g,"For $f: \Omega \subset \mathbb{R}^n \rightarrow \mathbb{R}$ , let $\nabla f=(\partial_{x_1}f,\ldots,\partial_{x_n}f)$ denote its gradient. Under what conditions on $\boldsymbol{g}=(g_1,\ldots,g_n)$ , the equation $\nabla f=\boldsymbol{g}$ admits a solution? For $n=1$ , this problem is simple and for any continuous function $g:\Omega \rightarrow \mathbb{R}$ , the function $f(x):=\int_0^xf(s) d s$ does the job. The case $n>1$ , does not seem trivial to me. A precise necessary and sufficient conditon will be greatly appreciated.","For , let denote its gradient. Under what conditions on , the equation admits a solution? For , this problem is simple and for any continuous function , the function does the job. The case , does not seem trivial to me. A precise necessary and sufficient conditon will be greatly appreciated.","f: \Omega \subset \mathbb{R}^n \rightarrow \mathbb{R} \nabla f=(\partial_{x_1}f,\ldots,\partial_{x_n}f) \boldsymbol{g}=(g_1,\ldots,g_n) \nabla f=\boldsymbol{g} n=1 g:\Omega \rightarrow \mathbb{R} f(x):=\int_0^xf(s) d s n>1","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus']"
92,On the Definition of Smooth Map in Differential Topology,On the Definition of Smooth Map in Differential Topology,,"Let $X$ be a nonempty subset of $\mathbb{R}^n$ and $f:X \rightarrow \mathbb{R}^m$ . In differential topology (see e.g. Milnor, Topology from the Differentiable Viewpoint or Guillemin and Pollack, Differential Topology), we call $f$ smooth if for any $x \in X$ there is an open neighborhood $U$ of $x$ in $\mathbb{R}^n$ and a smooth (in the usual sense, that is with continuous partial derivatives of all orders) map $F:U \rightarrow \mathbb{R}^m$ such that $F$ and $f$ coincide on $U \cap X$ . This definition has clearly a local nature and it is for sure the right definition to give in differential topology. My curiosity is its relation with its global counterpart. Call $f:X \rightarrow \mathbb{R}^m$ globally smooth if there is an open subset of $\mathbb{R}^n$ containing $X$ and a smooth function $F:U \rightarrow \mathbb{R}^m$ such that $F|_X = f$ . The two definitions are very different in nature, but are they actually different? In other terms, is there some example of a smooth function which is not globally smooth? Thank you very much for your attention. NOTE . The fact that the definition of smooth function has a local nature is clear. We can make it even more clear by stating the following simple Proposition . Let $X$ be a nonempty subset of $\mathbb{R}^n$ and $f:X \rightarrow \mathbb{R}^m$ . $f$ is smooth if and only if for any $x \in X$ there is a neighborhood $N$ of $x$ in $X$ such that $f|_N$ is smooth. Proof . If $f$ is smooth, then for any nonempty subset $S$ of $X$ , $f|_S$ is smooth. Conversely, assume that for any $x \in X$ there is a neighborhood $N$ of $x$ in $X$ such that $f|_N$ is smooth. Let $x \in X$ and let $N$ be such a neighborhood. Then there exists an open neighborhood $U$ of $x$ in $\mathbb{R}^n$ and a smooth function $F:U \rightarrow \mathbb{R}^m$ such that $F$ and $f$ coincide on $U \cap N$ . If $V$ is an open neighborhood of $x$ in $X$ such that $V \subset N$ , there is an open subset $W$ of $\mathbb{R}^n$ such that $V= W \cap X$ . But then $U \cap W$ is an open neighborhood of $x$ in $\mathbb{R}^n$ , $F|_{U \cap W}$ is a smooth map, and $F|_{U \cap W}$ and $f$ coincide on $ U \cap W \cap X = U \cap V \subset U \cap N$ .","Let be a nonempty subset of and . In differential topology (see e.g. Milnor, Topology from the Differentiable Viewpoint or Guillemin and Pollack, Differential Topology), we call smooth if for any there is an open neighborhood of in and a smooth (in the usual sense, that is with continuous partial derivatives of all orders) map such that and coincide on . This definition has clearly a local nature and it is for sure the right definition to give in differential topology. My curiosity is its relation with its global counterpart. Call globally smooth if there is an open subset of containing and a smooth function such that . The two definitions are very different in nature, but are they actually different? In other terms, is there some example of a smooth function which is not globally smooth? Thank you very much for your attention. NOTE . The fact that the definition of smooth function has a local nature is clear. We can make it even more clear by stating the following simple Proposition . Let be a nonempty subset of and . is smooth if and only if for any there is a neighborhood of in such that is smooth. Proof . If is smooth, then for any nonempty subset of , is smooth. Conversely, assume that for any there is a neighborhood of in such that is smooth. Let and let be such a neighborhood. Then there exists an open neighborhood of in and a smooth function such that and coincide on . If is an open neighborhood of in such that , there is an open subset of such that . But then is an open neighborhood of in , is a smooth map, and and coincide on .",X \mathbb{R}^n f:X \rightarrow \mathbb{R}^m f x \in X U x \mathbb{R}^n F:U \rightarrow \mathbb{R}^m F f U \cap X f:X \rightarrow \mathbb{R}^m \mathbb{R}^n X F:U \rightarrow \mathbb{R}^m F|_X = f X \mathbb{R}^n f:X \rightarrow \mathbb{R}^m f x \in X N x X f|_N f S X f|_S x \in X N x X f|_N x \in X N U x \mathbb{R}^n F:U \rightarrow \mathbb{R}^m F f U \cap N V x X V \subset N W \mathbb{R}^n V= W \cap X U \cap W x \mathbb{R}^n F|_{U \cap W} F|_{U \cap W} f  U \cap W \cap X = U \cap V \subset U \cap N,"['real-analysis', 'analysis', 'differential-geometry', 'differential-topology']"
93,How does one show that the operator whose kernel is properly supported is a smoothing operator?,How does one show that the operator whose kernel is properly supported is a smoothing operator?,,"Proposition 1.7 (Properly supported smoothing operators). $L^{-\infty}=$ smoothing operators. Given $A \in L^{-\infty}$ with properly supported amplitude $a \in S^{-\infty}\left(\Omega \times \Omega \times \mathbb{R}^n\right)$ $$ A u(x)=\int_{\mathbb{R}^n} \int_{\Omega} e^{i\langle x-y, \xi\rangle} a(x, y, \xi) u(y) d y d \xi=\int_{\Omega}(\underbrace{\int_{\mathbb{R}^n} e^{i\langle x-y, \xi\rangle} a(x, y, \xi) d \xi}_{k(x, y)}) u(y) d y $$ Thus $$ A u(x)=\int_{\Omega} k(x, y) d y \quad \text { with } k \in C_{\text {proper }}^{\infty}(\Omega \times \Omega) $$ What about the converse? Given $k \in C_{\text {proper }}^{\infty}(\Omega \times \Omega)$ Is $A$ as above in $L^{-\infty}(\Omega)$ (1) Fix $\chi \in C_c^{\infty}\left(\mathbb{R}^n\right), \int_{\mathbb{R}^n} \chi=1, u \in \mathcal{E}(\Omega), x \in B \subset \subset \Omega$ (compact neighbourhood) $$ \begin{gathered} L:=\pi_2\left(\pi_1^{-1}(B) \cap \operatorname{supp} k\right) \quad \text { compact! }\\ \end{gathered} $$ $$ Au(x)=\int_L k(x, y) u(y) \int_{\mathbb{R}^n} \chi(\xi) e^{i\langle x-y, \xi\rangle} e^{-i\langle x-y, \xi\rangle} d \xi d y\\ =\int_{\mathbb{R}^n} \int_L e^{i\langle x-y, \xi\rangle} \underbrace{\left\{k(x, y) e^{-i\langle x-y, \xi\rangle} \chi(\xi)\right\}}_{a(x, y, \xi) \in S^{-\infty}\left(\Omega \times \Omega \times \mathbb{R}^n\right)} u(y) d y d \xi $$ Im going through this proof of that an operator whose kernel is properly supported is a smoothing operator. Here properly supported means that the projections to each component are proper maps. Im not quite sure why we can restrict the area of integration to L, where a priori it should be over all of omega.","Proposition 1.7 (Properly supported smoothing operators). smoothing operators. Given with properly supported amplitude Thus What about the converse? Given Is as above in (1) Fix (compact neighbourhood) Im going through this proof of that an operator whose kernel is properly supported is a smoothing operator. Here properly supported means that the projections to each component are proper maps. Im not quite sure why we can restrict the area of integration to L, where a priori it should be over all of omega.","L^{-\infty}= A \in L^{-\infty} a \in S^{-\infty}\left(\Omega \times \Omega \times \mathbb{R}^n\right) 
A u(x)=\int_{\mathbb{R}^n} \int_{\Omega} e^{i\langle x-y, \xi\rangle} a(x, y, \xi) u(y) d y d \xi=\int_{\Omega}(\underbrace{\int_{\mathbb{R}^n} e^{i\langle x-y, \xi\rangle} a(x, y, \xi) d \xi}_{k(x, y)}) u(y) d y
 
A u(x)=\int_{\Omega} k(x, y) d y \quad \text { with } k \in C_{\text {proper }}^{\infty}(\Omega \times \Omega)
 k \in C_{\text {proper }}^{\infty}(\Omega \times \Omega) A L^{-\infty}(\Omega) \chi \in C_c^{\infty}\left(\mathbb{R}^n\right), \int_{\mathbb{R}^n} \chi=1, u \in \mathcal{E}(\Omega), x \in B \subset \subset \Omega 
\begin{gathered}
L:=\pi_2\left(\pi_1^{-1}(B) \cap \operatorname{supp} k\right) \quad \text { compact! }\\
\end{gathered}
 
Au(x)=\int_L k(x, y) u(y) \int_{\mathbb{R}^n} \chi(\xi) e^{i\langle x-y, \xi\rangle} e^{-i\langle x-y, \xi\rangle} d \xi d y\\
=\int_{\mathbb{R}^n} \int_L e^{i\langle x-y, \xi\rangle} \underbrace{\left\{k(x, y) e^{-i\langle x-y, \xi\rangle} \chi(\xi)\right\}}_{a(x, y, \xi) \in S^{-\infty}\left(\Omega \times \Omega \times \mathbb{R}^n\right)} u(y) d y d \xi
","['analysis', 'pseudo-differential-operators', 'microlocal-analysis']"
94,Problem solving strategies for Measure theory (soft question),Problem solving strategies for Measure theory (soft question),,"I'm a beginner in learning Measure theory. I would like to ask are there some common problem solving tricks in Measure theory. For example, usually when I'm doing a measure theory problem, once I rewrite a set into simpler sets, the question is basically solved. So I guess an example trick would be ""measure theory problems are mostly about rewriting sets"". Also, the following seems to be a general procedure for proving integral identities/theorems: prove it for indicators $1_A$ linearity proves it for simple functions monotone convergence theorem proves it for non-negative measurable functions Are there any other tricks? Also, are my tricks common/useful? Many thanks in advance.","I'm a beginner in learning Measure theory. I would like to ask are there some common problem solving tricks in Measure theory. For example, usually when I'm doing a measure theory problem, once I rewrite a set into simpler sets, the question is basically solved. So I guess an example trick would be ""measure theory problems are mostly about rewriting sets"". Also, the following seems to be a general procedure for proving integral identities/theorems: prove it for indicators linearity proves it for simple functions monotone convergence theorem proves it for non-negative measurable functions Are there any other tricks? Also, are my tricks common/useful? Many thanks in advance.",1_A,"['real-analysis', 'analysis', 'measure-theory', 'soft-question', 'lebesgue-integral']"
95,"Do all perfect subsets of [0,1] contain a sequence such that the ratio of its increments converges?","Do all perfect subsets of [0,1] contain a sequence such that the ratio of its increments converges?",,"Fix a perfect set $D\subseteq[0,1]$ . Does there necessarily exist a monotone sequence $\{x_m\}_{m\in\mathbb{N}}\subseteq D$ such that $x_m\rightarrow x\in D$ with the property that: \begin{equation} \frac{x_m-x_{m-1}}{x_{m-1}-x_{m-2}}\rightarrow c \end{equation} for some $c\in(0,\infty)$ ? As all elements of $D$ are accumulation points, this seems natural, but I could not find a reference to such a property nor find a clear counterexample. Any help is much appreciated! For example, the Cantor set satisfies this property. Take $x_n=(1/3)^n$ . For all $n\in\mathbb{N}$ , $x_n$ is in the Cantor set. So too is $0$ . Moreover, the ratio of the increments is always c=1/3.","Fix a perfect set . Does there necessarily exist a monotone sequence such that with the property that: for some ? As all elements of are accumulation points, this seems natural, but I could not find a reference to such a property nor find a clear counterexample. Any help is much appreciated! For example, the Cantor set satisfies this property. Take . For all , is in the Cantor set. So too is . Moreover, the ratio of the increments is always c=1/3.","D\subseteq[0,1] \{x_m\}_{m\in\mathbb{N}}\subseteq D x_m\rightarrow x\in D \begin{equation}
\frac{x_m-x_{m-1}}{x_{m-1}-x_{m-2}}\rightarrow c
\end{equation} c\in(0,\infty) D x_n=(1/3)^n n\in\mathbb{N} x_n 0",['analysis']
96,What is the asymptotic density and Lebesgue density of two sets which partition the reals into subsets of positive measure?,What is the asymptotic density and Lebesgue density of two sets which partition the reals into subsets of positive measure?,,"Suppose we partition the reals into two sets $A$ and $B$ that are dense (with positive Lebesgue measure) in every non-empty sub-interval $(a,b)$ of $\mathbb{R}$ , where Lebesgue measure $\lambda$ restricts outer measure $\lambda^{*}$ to sets measurable in the Caratheodory sense . Does there exist an example (similar to this one ) where both: $$\lim\limits_{t\to\infty}\lambda(A\cap [-t,t])/(2t)$$ and $$\lim\limits_{t\to\infty}\lambda(B\cap [-t,t])/(2t)$$ are greater than zero but neither equal $1/2$ ? A bounty is also offered for this question here .","Suppose we partition the reals into two sets and that are dense (with positive Lebesgue measure) in every non-empty sub-interval of , where Lebesgue measure restricts outer measure to sets measurable in the Caratheodory sense . Does there exist an example (similar to this one ) where both: and are greater than zero but neither equal ? A bounty is also offered for this question here .","A B (a,b) \mathbb{R} \lambda \lambda^{*} \lim\limits_{t\to\infty}\lambda(A\cap [-t,t])/(2t) \lim\limits_{t\to\infty}\lambda(B\cap [-t,t])/(2t) 1/2","['real-analysis', 'analysis', 'measure-theory']"
97,"How to integrate $\int_{0}^{1}g'(x)f(g(x),x)dx$",How to integrate,"\int_{0}^{1}g'(x)f(g(x),x)dx","I know how to integrate $$\int_{0}^{1}g'(x)f(g(x))dx$$ for $g$ differentiable, I just need to take $s = g(x)$ and then I find $$\int_{g(0)}^{g(1)}f(s)ds$$ But what happens when $f$ depends explicitely on $x$ , i.e. how to do the integration for $$ \int_{0}^{1} g'(x)f(g(x),x)dx$$ For now, my only idea is to write this integral as $$ \int_{0}^{1}(g'(x), 1). (f(g(x),x),0)dx $$ so that I have $(g(x),x)'$ . But it does not seem to work ...","I know how to integrate for differentiable, I just need to take and then I find But what happens when depends explicitely on , i.e. how to do the integration for For now, my only idea is to write this integral as so that I have . But it does not seem to work ...","\int_{0}^{1}g'(x)f(g(x))dx g s = g(x) \int_{g(0)}^{g(1)}f(s)ds f x  \int_{0}^{1} g'(x)f(g(x),x)dx  \int_{0}^{1}(g'(x), 1). (f(g(x),x),0)dx  (g(x),x)'","['integration', 'analysis', 'contour-integration']"
98,Prove that $\lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a$.,Prove that .,\lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a,"Let $f$ be a function which definites on $\mathbb{R}$ and  has a third derivative, such that $$f'(0)=0,f''(0)=-2,f'''(0)>0,f'(1)=1,f''(1)=4,f'''(1)>0.$$ and for any $x\in (0,1)$ , $0<f(x)<f(0)=f(1)=1$ . If $a>1$ , then prove that $$\lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a.$$ In this problem $f$ requires so many conditions. Maybe we can choose a simple function $f$ to have a try, such as $f(x)=1-x^2+x^3$ , but I can't figure it out. In adition, I wonder why $a>1$ ?It seems that we can try to write $\ln (x+a)=\ln a+\ln (1+\frac{x}{a})$ and then expand $\ln(1+\frac{x}{a})$ at $0$ , and try to prove that for any $k\in \mathbb{N}$ , $$\lim_{n\to \infty}\frac{\int_0^1f^n(x)x^kdx}{\int_0^1f^n(x)dx}=0.$$","Let be a function which definites on and  has a third derivative, such that and for any , . If , then prove that In this problem requires so many conditions. Maybe we can choose a simple function to have a try, such as , but I can't figure it out. In adition, I wonder why ?It seems that we can try to write and then expand at , and try to prove that for any ,","f \mathbb{R} f'(0)=0,f''(0)=-2,f'''(0)>0,f'(1)=1,f''(1)=4,f'''(1)>0. x\in (0,1) 0<f(x)<f(0)=f(1)=1 a>1 \lim_{n\to \infty}\frac{\int_0^1f^n(x)\ln(x+a)dx}{\int_0^1f^n(x)dx}=\ln a. f f f(x)=1-x^2+x^3 a>1 \ln (x+a)=\ln a+\ln (1+\frac{x}{a}) \ln(1+\frac{x}{a}) 0 k\in \mathbb{N} \lim_{n\to \infty}\frac{\int_0^1f^n(x)x^kdx}{\int_0^1f^n(x)dx}=0.","['integration', 'sequences-and-series', 'analysis']"
99,Simple left earthquakes are dense,Simple left earthquakes are dense,,"i´ve been studying an article from W. P. Thurston about hyperbolic geometry, there, he defines something called left earthquake , whose definition is as follows: Definition. If $\lambda$ is a geodesic lamination on a hyperbolic plane, a $\lambda$ - left earthquake map $E$ is a (possibly discontinuous) biyective map from the hyperbolic plane onto itself which is an isometry on each stratum of $\lambda$ . Furthermore, the map $E$ satisfies the condition that for any two strata $A\neq B$ of $\lambda$ , the comparison isometry $$cmp(A,B)=(E|A)^{-1}\circ (E|B):\mathbb{H}^2\to\mathbb{H}^2$$ is a hyperbolic transformation whose axis weakly separates $A$ and $B$ and which translates to the left as viewed from $A$ . Of course, it is easily seen that any right earthquake induces a left earthquake and viceversa. Then, Thurston establishes the following: Theorem. Left earthquake maps with finite laminations are dense in the set of all left earthquake maps, in the topology of uniform convergence on compact sets. He starts the proof by first noticing that the image of a compact set by a left earthquake map has a bounded diameter . Of course, I believe that the proof relies on the fact that compact sets on a metric space are bounded, and thus, under a continuous function, the image of a compact set is bounded, i.e., has bounded diameter. (Thurston argues that the proof of this fact is in analogy to the proof of the following: The image of a compact interval under a monotone function is bounded , but I´m not able to figure out a proof of the latter). Thurston then gives the following argument: Blockquote Given any left $\lambda$ -earthquake $E$ , then for any compact subset $K$ of the hyperbolic plane, there is a finite subset of strata which intersect $K$ such that the union of the images of the images of this finite set of strata in the graph of $E\subseteq\mathbb{H}^2\times\mathbb{H}^2$ is $\varepsilon$ -dense in the graph of $E$ restricted to $K$ . This argument is not very clear to me, any help with trying to understand this would be very helpful","i´ve been studying an article from W. P. Thurston about hyperbolic geometry, there, he defines something called left earthquake , whose definition is as follows: Definition. If is a geodesic lamination on a hyperbolic plane, a - left earthquake map is a (possibly discontinuous) biyective map from the hyperbolic plane onto itself which is an isometry on each stratum of . Furthermore, the map satisfies the condition that for any two strata of , the comparison isometry is a hyperbolic transformation whose axis weakly separates and and which translates to the left as viewed from . Of course, it is easily seen that any right earthquake induces a left earthquake and viceversa. Then, Thurston establishes the following: Theorem. Left earthquake maps with finite laminations are dense in the set of all left earthquake maps, in the topology of uniform convergence on compact sets. He starts the proof by first noticing that the image of a compact set by a left earthquake map has a bounded diameter . Of course, I believe that the proof relies on the fact that compact sets on a metric space are bounded, and thus, under a continuous function, the image of a compact set is bounded, i.e., has bounded diameter. (Thurston argues that the proof of this fact is in analogy to the proof of the following: The image of a compact interval under a monotone function is bounded , but I´m not able to figure out a proof of the latter). Thurston then gives the following argument: Blockquote Given any left -earthquake , then for any compact subset of the hyperbolic plane, there is a finite subset of strata which intersect such that the union of the images of the images of this finite set of strata in the graph of is -dense in the graph of restricted to . This argument is not very clear to me, any help with trying to understand this would be very helpful","\lambda \lambda E \lambda E A\neq B \lambda cmp(A,B)=(E|A)^{-1}\circ (E|B):\mathbb{H}^2\to\mathbb{H}^2 A B A \lambda E K K E\subseteq\mathbb{H}^2\times\mathbb{H}^2 \varepsilon E K","['analysis', 'hyperbolic-geometry', 'low-dimensional-topology']"
