,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability density of Continuous uniform distribution over the unit circle,Probability density of Continuous uniform distribution over the unit circle,,"If we want to chose a point $(x,y)$ uniformly at random from a unit circle in a plane, why is the joint probability density of the random variable $f(x,y) = \frac{1}{\pi}$ for $x^2+y^2\leq1$? The question linked simply states this but doesn't prove it. Can someone give an explanation please? Continuous uniform distribution over a circle with radius R","If we want to chose a point $(x,y)$ uniformly at random from a unit circle in a plane, why is the joint probability density of the random variable $f(x,y) = \frac{1}{\pi}$ for $x^2+y^2\leq1$? The question linked simply states this but doesn't prove it. Can someone give an explanation please? Continuous uniform distribution over a circle with radius R",,"['probability', 'probability-theory', 'random', 'uniform-distribution']"
1,Central limit theorem confusion,Central limit theorem confusion,,"If a bunch of random variables $X_i$ are independently and identically distributed with an exponential distribution, their sum apparently follows a Gamma distribution. But doesn't the central limit theorem imply that (for $X_i$ of any distribution with mean zero and variance $\sigma^2$), the sum $\sum_{i=1}^n X_i$ will become approximately normally distributed $~N(0,n\sigma^2)$ for large enough $n$ ? Obviously I am missing something basic, but what's going on? How can the sum of i.i.d. exponential random variables have a Gamma distribution, but also be converging to normality?","If a bunch of random variables $X_i$ are independently and identically distributed with an exponential distribution, their sum apparently follows a Gamma distribution. But doesn't the central limit theorem imply that (for $X_i$ of any distribution with mean zero and variance $\sigma^2$), the sum $\sum_{i=1}^n X_i$ will become approximately normally distributed $~N(0,n\sigma^2)$ for large enough $n$ ? Obviously I am missing something basic, but what's going on? How can the sum of i.i.d. exponential random variables have a Gamma distribution, but also be converging to normality?",,"['probability', 'statistics', 'probability-distributions']"
2,Prove an Equivalent Criteria for $\sigma$-finite measures.,Prove an Equivalent Criteria for -finite measures.,\sigma,"A measure $\mu$ on (E,$M$) is $\sigma$-finite if and only if there exists a strictly positive function $f$ in M such that $\mu f<∞$  . Please help me out with the proof. Thank you.","A measure $\mu$ on (E,$M$) is $\sigma$-finite if and only if there exists a strictly positive function $f$ in M such that $\mu f<∞$  . Please help me out with the proof. Thank you.",,"['probability', 'measure-theory']"
3,Average area of choosing three points on a surface?,Average area of choosing three points on a surface?,,"Assume I choose three random points on the surface of a sphere. What is the average area? (Each point is independently chosen relative to a uniform distribution on the sphere) Also, what would be the average area if I choose three points on other types of surfaces? Such as 2-dimensional square/circle  or 1-dimensional line.","Assume I choose three random points on the surface of a sphere. What is the average area? (Each point is independently chosen relative to a uniform distribution on the sphere) Also, what would be the average area if I choose three points on other types of surfaces? Such as 2-dimensional square/circle  or 1-dimensional line.",,"['probability', 'geometry', 'geometric-probability']"
4,Matching Hats AND Coats Problem,Matching Hats AND Coats Problem,,"Suppose $N$ men throw their hats in a room AND their coats in an other room. Each man then randomly picks a hat and a coat. What is the probability that: None of the men select his own hat and his own coat Exactly $k$ of the men select his own hat and his own coat. More generally, if instead of $2$ items (hat and coat), the $N$ men threw $n$ items, how to find the probability mass function $P(k=0,1,2,\ldots,n)$ of the number of complete coincidences ($k$ men select their $n$ own items)?","Suppose $N$ men throw their hats in a room AND their coats in an other room. Each man then randomly picks a hat and a coat. What is the probability that: None of the men select his own hat and his own coat Exactly $k$ of the men select his own hat and his own coat. More generally, if instead of $2$ items (hat and coat), the $N$ men threw $n$ items, how to find the probability mass function $P(k=0,1,2,\ldots,n)$ of the number of complete coincidences ($k$ men select their $n$ own items)?",,"['probability', 'combinatorics']"
5,Doubts on Mutually exclusive and Independent events,Doubts on Mutually exclusive and Independent events,,"Problem: In a school competition,the probability of hitting the target bu Dick is $\frac{1}{2}$ ,by Betty is $\frac{1}{3}$ and by Joe is $\frac{3}{5}$ .If all of them fire independently,calculate the probability that the target will be hit. This general approach for solving is to find the complement of the probability that the target would not be hit . If the three events are mutually exclusive/independent, then why does summing the three probabilities not give the correct answer?","Problem: In a school competition,the probability of hitting the target bu Dick is ,by Betty is and by Joe is .If all of them fire independently,calculate the probability that the target will be hit. This general approach for solving is to find the complement of the probability that the target would not be hit . If the three events are mutually exclusive/independent, then why does summing the three probabilities not give the correct answer?",\frac{1}{2} \frac{1}{3} \frac{3}{5},"['probability', 'probability-theory']"
6,Probability and Infinity,Probability and Infinity,,"If the probability of an event is $\frac{1}{\infty}$ and $\infty$ trials are conducted, how many times will the event occur — $0$, $1$, or $\infty$?","If the probability of an event is $\frac{1}{\infty}$ and $\infty$ trials are conducted, how many times will the event occur — $0$, $1$, or $\infty$?",,"['probability', 'infinity']"
7,Expected path length in a ladder-like graph if edges can be randomly removed.,Expected path length in a ladder-like graph if edges can be randomly removed.,,"Question from an old exam: The King of Squares sets out to patrol the City of Squares. The City of Squares is an infinite ladder, i.e., a graph $(V, E)$ where $V = \mathbb{N} \times \{0,1\}$ and the vertex $(x, y)$ is connected to $(x-1, y)$ for $x>0,(x+1, y)$ and $(x, 1-y)$ . Unfortunately, due to the revolt, some parts of the streets are blocked. Each edge independently becomes blocked with probability $\frac{1}{2}$ . The king leaves his palace at the point $(0,0)$ . Let $X$ be the largest value of the coordinate $x$ that the King can reach without passing through the blocked streets. The king can look ahead, so he will choose the best route before he sets off. In the situation in the image below $X=5$ . (a) Find EX. (b) Find the probability that both points $(X, 0)$ and $(X, 1)$ are reachable. My attempt at (a): Let $$ X_i = \begin{cases}     i, & \text{if it's possible to reach $(i, 0)$ or $(i,1)$} \\     0, & \text{otherwise}. \end{cases} $$ Now we need to find the probability $p(i)$ that it is possible to reach the point $(i, 0)$ or $(i,1)$ . I have calculated that $p(i) = \frac{5}{8} p(i-1)$ , where $p(1) = \frac{5}{8}$ . This is derived from the idea that the probability of going from coordinate $ (x, 0)$ , to $(x+1, 0)$ or $(x+1, 1)$ is $\frac{5}{8}$ . Then $E[X]=\sum_{i=1}^{\infty}E[X_i]=\sum_{i=1}^{\infty}i\cdot(\frac{5}{8})^i = \frac{40}{9} 	\approx 4.44$ . Is this correct? How to do (b), because I don't know where to start?","Question from an old exam: The King of Squares sets out to patrol the City of Squares. The City of Squares is an infinite ladder, i.e., a graph where and the vertex is connected to for and . Unfortunately, due to the revolt, some parts of the streets are blocked. Each edge independently becomes blocked with probability . The king leaves his palace at the point . Let be the largest value of the coordinate that the King can reach without passing through the blocked streets. The king can look ahead, so he will choose the best route before he sets off. In the situation in the image below . (a) Find EX. (b) Find the probability that both points and are reachable. My attempt at (a): Let Now we need to find the probability that it is possible to reach the point or . I have calculated that , where . This is derived from the idea that the probability of going from coordinate , to or is . Then . Is this correct? How to do (b), because I don't know where to start?","(V, E) V = \mathbb{N} \times \{0,1\} (x, y) (x-1, y) x>0,(x+1, y) (x, 1-y) \frac{1}{2} (0,0) X x X=5 (X, 0) (X, 1) 
X_i = \begin{cases}
    i, & \text{if it's possible to reach (i, 0) or (i,1)} \\
    0, & \text{otherwise}.
\end{cases}
 p(i) (i, 0) (i,1) p(i) = \frac{5}{8} p(i-1) p(1) = \frac{5}{8}  (x, 0) (x+1, 0) (x+1, 1) \frac{5}{8} E[X]=\sum_{i=1}^{\infty}E[X_i]=\sum_{i=1}^{\infty}i\cdot(\frac{5}{8})^i = \frac{40}{9} 	\approx 4.44","['probability', 'expected-value']"
8,A complicated integral that Mathematica can't compute,A complicated integral that Mathematica can't compute,,"I have a very complicated distribution function of which I want to find the expected value. The distribution I got for a function having the cosine of the samples taken from a Gaussian distribution. $$ y = \cos(x) $$ Where $x$ values are drawn from a Gaussian distribution and we assume $x \in [-\pi, \pi]$ . $$ p(x) = \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(x - \mu_x)^2}{2\sigma_x^2}} $$ The distribution of $y$ hence can be written as below: So, now the distribution of $y$ becomes: $$ p(y) = \frac{2}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}} $$ I referred this article for the formulation. They do it when $x$ follows a uniform distribution. However, here I have a Gaussian distribution. The main question that I want to ask is to sum the random numbers one would generate using this distribution. $$ \sum_i y_i$$ Where $y_i$ are the samples drawn from $p(y)$ . I thought of solving this by the following integral. I thought like a Frequentist and found that the sum of a number of variables is the sum of the independent variables (samples) I have multiplied with the probability of those samples occurring out of some number of draws from the distribution. Please let me know if this thinking is not adequate enough. So, at the end it becomes: $$ \sum_i y_i = \sum_{i = 1}^{N_s} y_i p(y_i) $$ Where $N_s$ is the number of independent samples. This can be written in integral form as: $$ \int_{-1}^{+1} \frac{2y}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}} dy$$ I say $-1$ to $+1$ instead of a finite length here because I assume $N_s$ in the previous expression to be large enough. This looks like the expected value of the distribution. Is there a way to find a closed form solution to this equation (Or approximate closed form)? When it comes to Gaussian distribution, this quite nicely explained before and I have seen it here . When I tried this integral with the complicated distribution, it said that it can't converge. Is there way to do this? EDIT: I implemented a wrong function on mathematica. Now, I do the correct one. It doesn't say it can't converge anymore, however, it doesn't compute it now. It just returns with the integral expression. EDIT Version 2 ===================================================== I took $y = \cos(\theta)$ to solve the integral and I have now, $$  \int_{-\pi}^{\pi} \frac{2 \cos(\theta) e^{-\frac{\theta - \mu_x^2}{2\sigma_x^2}} }{\sqrt{2\pi \sigma_x^2}} d\theta$$ The solution I have from Mathematica looks like the following. $$ \frac{e^{-i \mu_x - \frac{1}{2\sigma_x^2}}}{2\sigma_x^2} \Bigg[ -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) + e^{i 2 \mu_x} \Bigg( -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) \Bigg) \Bigg)  \Bigg] \Bigg[ \operatorname{erf}[\frac{i + \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] -   \operatorname{erf}[\frac{i - \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] -    i e^{i2 \mu_x} (\operatorname{erfi}[\frac{1 + i \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] -       \operatorname{erfi}[\frac{1 - i \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] \Bigg] $$ Is there a relation between this and the nice solution given by @snoop ?","I have a very complicated distribution function of which I want to find the expected value. The distribution I got for a function having the cosine of the samples taken from a Gaussian distribution. Where values are drawn from a Gaussian distribution and we assume . The distribution of hence can be written as below: So, now the distribution of becomes: I referred this article for the formulation. They do it when follows a uniform distribution. However, here I have a Gaussian distribution. The main question that I want to ask is to sum the random numbers one would generate using this distribution. Where are the samples drawn from . I thought of solving this by the following integral. I thought like a Frequentist and found that the sum of a number of variables is the sum of the independent variables (samples) I have multiplied with the probability of those samples occurring out of some number of draws from the distribution. Please let me know if this thinking is not adequate enough. So, at the end it becomes: Where is the number of independent samples. This can be written in integral form as: I say to instead of a finite length here because I assume in the previous expression to be large enough. This looks like the expected value of the distribution. Is there a way to find a closed form solution to this equation (Or approximate closed form)? When it comes to Gaussian distribution, this quite nicely explained before and I have seen it here . When I tried this integral with the complicated distribution, it said that it can't converge. Is there way to do this? EDIT: I implemented a wrong function on mathematica. Now, I do the correct one. It doesn't say it can't converge anymore, however, it doesn't compute it now. It just returns with the integral expression. EDIT Version 2 ===================================================== I took to solve the integral and I have now, The solution I have from Mathematica looks like the following. Is there a relation between this and the nice solution given by @snoop ?"," y = \cos(x)  x x \in [-\pi, \pi]  p(x) = \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(x - \mu_x)^2}{2\sigma_x^2}}  y y  p(y) = \frac{2}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}}  x  \sum_i y_i y_i p(y)  \sum_i y_i = \sum_{i = 1}^{N_s} y_i p(y_i)  N_s  \int_{-1}^{+1} \frac{2y}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}} dy -1 +1 N_s y = \cos(\theta)   \int_{-\pi}^{\pi} \frac{2 \cos(\theta) e^{-\frac{\theta - \mu_x^2}{2\sigma_x^2}} }{\sqrt{2\pi \sigma_x^2}} d\theta  \frac{e^{-i \mu_x - \frac{1}{2\sigma_x^2}}}{2\sigma_x^2} \Bigg[ -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) + e^{i 2 \mu_x} \Bigg( -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) \Bigg) \Bigg)  \Bigg] \Bigg[ \operatorname{erf}[\frac{i + \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
 \operatorname{erf}[\frac{i - \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
  i e^{i2 \mu_x} (\operatorname{erfi}[\frac{1 + i \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
     \operatorname{erfi}[\frac{1 - i \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] \Bigg] ","['probability', 'integration', 'probability-distributions', 'closed-form', 'gaussian']"
9,"Given a random binary operation $\varphi$ on a finite set $G$, what is the probability that $(G,\varphi)$ is a group?","Given a random binary operation  on a finite set , what is the probability that  is a group?","\varphi G (G,\varphi)","I recently learned about the 5/8 theorem, which states that if the probability that any two random elements in a group commute exceeds 5/8, then the group must be abelian. I think its interesting seeing probability and group theory together, which led me to think of this question. This is my work so far, which has a good possibility of being flawed. First, consider all binary operations $\varphi : G\times G \to G$ , which given $|G|=n$ we can count as $|G|^{|G\times G|^2} = n^{n^2}$ . Next, consider all binary operations that fail to have an identity $e \in G$ . If $\varphi(g,e)\neq g$ (or $\varphi(e,g)\neq g$ ) for any single $g \in G$ , we can throw out all other possible functions given by the other elements in $G- \{g\}$ , which is given by $(n-1)^{(n-1)^2}$ operations. There are $n^2$ elements in $|G\times G|$ that each have $(n-1)$ ways to fail, so the number of binary operations that fail to have an identity element should be given by $$n^2(n-1)(n-1)^{(n-1)^2}=n^2(n-1)^{(n-1)^2+1}$$ Hence, we can get an upper bound for how many binary operations can form groups as $$n^{n^2}-n^2(n-1)^{(n-1)^2+1}$$ For inverses, I think the same argument can be made as the above, hence an upper bound after considering inverses is given by $$n^{n^2}-2n^2(n-1)^{(n-1)^2+1}$$ I have no idea how to even get started with associativity! Further, I have never been fantastic at counting problems, so I am sure I made a flaw in my argument thus far. Any progress or criticisms are very much welcome.","I recently learned about the 5/8 theorem, which states that if the probability that any two random elements in a group commute exceeds 5/8, then the group must be abelian. I think its interesting seeing probability and group theory together, which led me to think of this question. This is my work so far, which has a good possibility of being flawed. First, consider all binary operations , which given we can count as . Next, consider all binary operations that fail to have an identity . If (or ) for any single , we can throw out all other possible functions given by the other elements in , which is given by operations. There are elements in that each have ways to fail, so the number of binary operations that fail to have an identity element should be given by Hence, we can get an upper bound for how many binary operations can form groups as For inverses, I think the same argument can be made as the above, hence an upper bound after considering inverses is given by I have no idea how to even get started with associativity! Further, I have never been fantastic at counting problems, so I am sure I made a flaw in my argument thus far. Any progress or criticisms are very much welcome.","\varphi : G\times G \to G |G|=n |G|^{|G\times G|^2} = n^{n^2} e \in G \varphi(g,e)\neq g \varphi(e,g)\neq g g \in G G- \{g\} (n-1)^{(n-1)^2} n^2 |G\times G| (n-1) n^2(n-1)(n-1)^{(n-1)^2}=n^2(n-1)^{(n-1)^2+1} n^{n^2}-n^2(n-1)^{(n-1)^2+1} n^{n^2}-2n^2(n-1)^{(n-1)^2+1}","['probability', 'combinatorics', 'group-theory', 'finite-groups']"
10,Expected Value of flipping through a book,Expected Value of flipping through a book,,"Suppose that a book has $N$ pages, and we read through the book as follows. We start from page 0, and if we're on page $i$ , we randomly flip to a page $i + 1, i + 2, ..., N$ with equal probability. What is the expected value of number of flips we need to finish the book? Intuition tells me that we can, on average, expect to halve the number of pages remaining.  This  yields $\log_2(N)$ , but I'm having trouble formalizing it. If $N = 26$ , what is the probability we flip to page 13 at some point? Assume we start at page 0. I let $P_i$ be the probability we land on page 13 eventually, starting from page $i$ . Then, $P_{13} = 1$ , and in general, $$P_{i} = \frac{1}{26 - i}\sum_{k = i + 1}^{13}P_k$$ Evaluating terms like $P_{12}, P_{11}, P_{10}$ , I see that all of these values are $\frac{1}{14}$ , including $P_0$ . Is there a more intuitive reason for such a simple answer?","Suppose that a book has pages, and we read through the book as follows. We start from page 0, and if we're on page , we randomly flip to a page with equal probability. What is the expected value of number of flips we need to finish the book? Intuition tells me that we can, on average, expect to halve the number of pages remaining.  This  yields , but I'm having trouble formalizing it. If , what is the probability we flip to page 13 at some point? Assume we start at page 0. I let be the probability we land on page 13 eventually, starting from page . Then, , and in general, Evaluating terms like , I see that all of these values are , including . Is there a more intuitive reason for such a simple answer?","N i i + 1, i + 2, ..., N \log_2(N) N = 26 P_i i P_{13} = 1 P_{i} = \frac{1}{26 - i}\sum_{k = i + 1}^{13}P_k P_{12}, P_{11}, P_{10} \frac{1}{14} P_0","['probability', 'expected-value']"
11,$e$ showing up in expected value,showing up in expected value,e,"My question was inspired by this numberphile video on the maths of secret santa. So suppose you have a group of $n$ people who are all randomly choosing another person in the group at random. The probability the any given person chooses themselves is $p = 1/n$ and the expected value of $X$ (the number of people who choose themselves) is equal to $np = n \times 1/n = 1$ . If someone(s) chooses themselves, then everyone has to choose another person at random again. Let's define the random variable $Y$ as the number of attempts the group will have to make until everyone chooses someone who is not themselves. My question is, find the expected value of $Y$ , $E(Y)$ . I didn't know how to compute this mathematically but when I ran a bunch of simulations I found that the answer rounded to $e$ ( $2.71828\ldots$ )! Can someone please explain why $e$ is showing up here.","My question was inspired by this numberphile video on the maths of secret santa. So suppose you have a group of people who are all randomly choosing another person in the group at random. The probability the any given person chooses themselves is and the expected value of (the number of people who choose themselves) is equal to . If someone(s) chooses themselves, then everyone has to choose another person at random again. Let's define the random variable as the number of attempts the group will have to make until everyone chooses someone who is not themselves. My question is, find the expected value of , . I didn't know how to compute this mathematically but when I ran a bunch of simulations I found that the answer rounded to ( )! Can someone please explain why is showing up here.",n p = 1/n X np = n \times 1/n = 1 Y Y E(Y) e 2.71828\ldots e,"['probability', 'expected-value', 'eulers-number-e']"
12,"Beginner probability question about the phrase ""order doesn't matter""","Beginner probability question about the phrase ""order doesn't matter""",,"This is a lame question but it's been bugging me for a while... A father and his son are at a diner and each make one selection (randomly and independently) from a list of $10$ different dishes on a menu. What is the probability they choose different dishes? Let $E$ be the event that they choose separate dishes. Let's condition on the father ordering first (call the event $F$) then, $$P(E) = P(E \mid F) P(F) + P(E \mid F^c) P(F^c)=\left(\frac{1}{2}\right)\left(\frac{9}{10}\right)+\left(\frac{1}{2}\right)\left(\frac{9}{10}\right) = \frac{9}{10}$$ Question 1: I don't think you can say ""order doesn't matter"" for this problem. I think order does matter. Is that correct? You could say ""regardless of the order for this situation the conditional probabilities happen to be symmetrical, $P(E|F) = P(E|F^c) = 9/10$, and since the probability of the conditioning event is one half, the probability of the event in question is equal to just one of the conditional probabilities."" That is $P(E) = P(E|F)$. Is that the right way to think about this problem? Let's say I condition on the dish ordered by the first person $$\sum_{i=1}^{10} P(E \mid T=i)P(T=i) = \sum_{i=1}^{10} \frac{9}{10}\frac{1}{10} = 9/10$$ Question 2: Is this the same situation where ""order does matter"" it's just that the $P(E)$ is equal to either $P(E \mid T=i)P(T=i)$ summed for all $i$ whether the father or son goes first? Question 3: If order does matter, then how does the problem change if you say they choose them exactly simultaneously? Thanks.","This is a lame question but it's been bugging me for a while... A father and his son are at a diner and each make one selection (randomly and independently) from a list of $10$ different dishes on a menu. What is the probability they choose different dishes? Let $E$ be the event that they choose separate dishes. Let's condition on the father ordering first (call the event $F$) then, $$P(E) = P(E \mid F) P(F) + P(E \mid F^c) P(F^c)=\left(\frac{1}{2}\right)\left(\frac{9}{10}\right)+\left(\frac{1}{2}\right)\left(\frac{9}{10}\right) = \frac{9}{10}$$ Question 1: I don't think you can say ""order doesn't matter"" for this problem. I think order does matter. Is that correct? You could say ""regardless of the order for this situation the conditional probabilities happen to be symmetrical, $P(E|F) = P(E|F^c) = 9/10$, and since the probability of the conditioning event is one half, the probability of the event in question is equal to just one of the conditional probabilities."" That is $P(E) = P(E|F)$. Is that the right way to think about this problem? Let's say I condition on the dish ordered by the first person $$\sum_{i=1}^{10} P(E \mid T=i)P(T=i) = \sum_{i=1}^{10} \frac{9}{10}\frac{1}{10} = 9/10$$ Question 2: Is this the same situation where ""order does matter"" it's just that the $P(E)$ is equal to either $P(E \mid T=i)P(T=i)$ summed for all $i$ whether the father or son goes first? Question 3: If order does matter, then how does the problem change if you say they choose them exactly simultaneously? Thanks.",,"['probability', 'conditional-probability']"
13,Random walk on the lamplighter group,Random walk on the lamplighter group,,"Consider a random walk on the lamplighter group with the following generating set: move left, move right, and toggle lamp. Start at the origin, with all lamps off. What is the probability that, after $t$ steps, the lamp at the origin is on? I started by letting $g(b,k,t)$ denote the number of words of length $t$  that set the lamp at the origin to $b$ and the lamplighter at position $k$. Thus we have the recurrence relation \begin{align*}   g(b,k,0)&=[b=0][k=0] \\   g(b,k,t+1)&=g(b,k-1,t)+g(b,k+1,t)+g(b \oplus [k=0],k,t) \end{align*} where $[\cdot]$ is the Iverson bracket and $\oplus$ is xor . Then I let $f(b,t)$ denote the number of words of length $t$ that set the lamp at the origin to $b$: $$f(b,t) = \sum_{k \in \mathbb{Z}} g(b,k,t)$$ The answer to my question is $f(1,t) \cdot 3^{-t}$, since $3^t$ is the total number of words of length $t$. After some simplification, I obtained the following recurrence relation for $f$: \begin{align*}   f(b,0) &= [b=0] \\   f(b,t+1) &= 3 f(b,t) - g(b, 0, t) + g(b \oplus 1, 0, t) \end{align*} I'm trying to get rid of the remaining $g$ terms. $g(0,0,t)$ and $g(1,0,t)$ represent the number of $t$-length words that return to the origin while leaving its lamp off or on, respectively. I suspect I might be able to use Motzkin paths to solve for these. The number of $t$-length words that return to the origin is the $t$th central trinomial coefficient . That is, $$g(0,0,t)+g(1,0,t) = \sum_{i=0}^t \binom{t}{i} \binom{i}{t-i}$$ The first few coefficients are \begin{array}{c|c|c} & b = 0 & b = 1 \\ t=0 & 1 & 0 \\ t=1 & 0 & 1 \\ t=2 & 3 & 0 \\ t=3 & 2 & 5 \\ t=4 & 15 & 4 \\ t=5 & 22 & 29 \\ t=6 & 93 & 48 \\ t=7 & 196 & 197 \\ t=8 & 659 & 448 \\ t=9 & 1650 & 1489 \end{array} EDIT: Let $L_k$ denote the set of words that shift the lamplighter by $k$. Then $$[z^n]L_k(z) = \binom{n}{k}_2$$ where the RHS is the entry in row $n$ and column $k$ of the trinomial triangle . Let $L_k^-$ be the subset of $L_k$ such that the lamp at $k$ (or, equivalently in number, the lamp at the last position) is not toggled. Based on further investigation, it seems to be the case that $$L_k^-(z) = L_k(z) \frac{1+z L_0(z)}{1+2z L_0(z)}$$ Is there a simple explanation for this relationship? I sense some kind of recursive definition of $L_k^-$ in terms of $L_k$, $L_0$, and $L_k^-$ itself.","Consider a random walk on the lamplighter group with the following generating set: move left, move right, and toggle lamp. Start at the origin, with all lamps off. What is the probability that, after $t$ steps, the lamp at the origin is on? I started by letting $g(b,k,t)$ denote the number of words of length $t$  that set the lamp at the origin to $b$ and the lamplighter at position $k$. Thus we have the recurrence relation \begin{align*}   g(b,k,0)&=[b=0][k=0] \\   g(b,k,t+1)&=g(b,k-1,t)+g(b,k+1,t)+g(b \oplus [k=0],k,t) \end{align*} where $[\cdot]$ is the Iverson bracket and $\oplus$ is xor . Then I let $f(b,t)$ denote the number of words of length $t$ that set the lamp at the origin to $b$: $$f(b,t) = \sum_{k \in \mathbb{Z}} g(b,k,t)$$ The answer to my question is $f(1,t) \cdot 3^{-t}$, since $3^t$ is the total number of words of length $t$. After some simplification, I obtained the following recurrence relation for $f$: \begin{align*}   f(b,0) &= [b=0] \\   f(b,t+1) &= 3 f(b,t) - g(b, 0, t) + g(b \oplus 1, 0, t) \end{align*} I'm trying to get rid of the remaining $g$ terms. $g(0,0,t)$ and $g(1,0,t)$ represent the number of $t$-length words that return to the origin while leaving its lamp off or on, respectively. I suspect I might be able to use Motzkin paths to solve for these. The number of $t$-length words that return to the origin is the $t$th central trinomial coefficient . That is, $$g(0,0,t)+g(1,0,t) = \sum_{i=0}^t \binom{t}{i} \binom{i}{t-i}$$ The first few coefficients are \begin{array}{c|c|c} & b = 0 & b = 1 \\ t=0 & 1 & 0 \\ t=1 & 0 & 1 \\ t=2 & 3 & 0 \\ t=3 & 2 & 5 \\ t=4 & 15 & 4 \\ t=5 & 22 & 29 \\ t=6 & 93 & 48 \\ t=7 & 196 & 197 \\ t=8 & 659 & 448 \\ t=9 & 1650 & 1489 \end{array} EDIT: Let $L_k$ denote the set of words that shift the lamplighter by $k$. Then $$[z^n]L_k(z) = \binom{n}{k}_2$$ where the RHS is the entry in row $n$ and column $k$ of the trinomial triangle . Let $L_k^-$ be the subset of $L_k$ such that the lamp at $k$ (or, equivalently in number, the lamp at the last position) is not toggled. Based on further investigation, it seems to be the case that $$L_k^-(z) = L_k(z) \frac{1+z L_0(z)}{1+2z L_0(z)}$$ Is there a simple explanation for this relationship? I sense some kind of recursive definition of $L_k^-$ in terms of $L_k$, $L_0$, and $L_k^-$ itself.",,"['probability', 'combinatorics', 'group-theory', 'random-walk', 'cayley-graphs']"
14,Determining whether a sequence of coin flips is random,Determining whether a sequence of coin flips is random,,"Say we have observed a finite sequence of coin flips. Is there a metric for how likely this sequence is generated by a truly random coin flip. For example, if we flip a coin 1000 times presumably seeing 500 heads and then 500 tails is less likely than a random assortment of Heads and Tails. Also, presumably we would not see every even flip be a tails and every odd flip to be a heads.","Say we have observed a finite sequence of coin flips. Is there a metric for how likely this sequence is generated by a truly random coin flip. For example, if we flip a coin 1000 times presumably seeing 500 heads and then 500 tails is less likely than a random assortment of Heads and Tails. Also, presumably we would not see every even flip be a tails and every odd flip to be a heads.",,"['probability', 'sequences-and-series', 'probability-distributions']"
15,A question about probability (conflicting solutions),A question about probability (conflicting solutions),,"There is a square of side $n$ units. Join the diagonals. Now the square is divided into 4 regions of equal area. Each of them is coloured differently. Given 2 points that can lie within any of the four regions, what is the probability that two points are in the same coloured region? The problem: I and my friend get two different solutions using two different lines of reasoning. It has to do with the order of the points being placed: Solution 1 (mine) : The order in which the points are placed does not matter. The number of ways 2 identical objects can be placed in 4 distinct regions is 10 (stars and bars). Out of those, there are 4 cases which satisfy the condition. Hence the probability is $2/5$. Solution 2 (my friend's) : The order of the points being placed does matter. That means there are 16 ways to place the points (4 orientations × 4 regions). Out of those, 4 cases satisfy the condition. Hence the probability is $1/4$. Now of course, one line of reasoning must be wrong here. Which one is wrong? An explanation is appreciated.","There is a square of side $n$ units. Join the diagonals. Now the square is divided into 4 regions of equal area. Each of them is coloured differently. Given 2 points that can lie within any of the four regions, what is the probability that two points are in the same coloured region? The problem: I and my friend get two different solutions using two different lines of reasoning. It has to do with the order of the points being placed: Solution 1 (mine) : The order in which the points are placed does not matter. The number of ways 2 identical objects can be placed in 4 distinct regions is 10 (stars and bars). Out of those, there are 4 cases which satisfy the condition. Hence the probability is $2/5$. Solution 2 (my friend's) : The order of the points being placed does matter. That means there are 16 ways to place the points (4 orientations × 4 regions). Out of those, 4 cases satisfy the condition. Hence the probability is $1/4$. Now of course, one line of reasoning must be wrong here. Which one is wrong? An explanation is appreciated.",,['probability']
16,difference between expectation of a random variable in measure theory and in regular calculus probability books,difference between expectation of a random variable in measure theory and in regular calculus probability books,,"let X be a random variable with a density function $f_{X}(x)$. The expectation of X is defined as $E[X] = \int x f_{X}(x) dx$ While in the probability books that uses the measure theory it is defined as $E[X] = \int X dP$ how are these two definitions related? and if I have another random variable Y is its expectation defined in a similar manner, i.e., : $E[Y] = \int Y dP$ if yes, how can I know that I am integrating with respect to r.v X or Y ? any help is appreciated","let X be a random variable with a density function $f_{X}(x)$. The expectation of X is defined as $E[X] = \int x f_{X}(x) dx$ While in the probability books that uses the measure theory it is defined as $E[X] = \int X dP$ how are these two definitions related? and if I have another random variable Y is its expectation defined in a similar manner, i.e., : $E[Y] = \int Y dP$ if yes, how can I know that I am integrating with respect to r.v X or Y ? any help is appreciated",,"['probability', 'probability-theory', 'expectation']"
17,Conditional expectation of X given X is greater than Y,Conditional expectation of X given X is greater than Y,,"I wonder if someone can help me with this question: ""Let $X,Y \sim \mathcal{N}(0,1)$ where X and Y are independent. Find $E[X | X>Y]$."" I know that $E[X|Y=y]= \int_{-\infty}^{\infty} x f_{X|Y}(x,y)dx$, but in this case one is conditioning on an inequality, and I don't know how to treat that. Thanks in advance!","I wonder if someone can help me with this question: ""Let $X,Y \sim \mathcal{N}(0,1)$ where X and Y are independent. Find $E[X | X>Y]$."" I know that $E[X|Y=y]= \int_{-\infty}^{\infty} x f_{X|Y}(x,y)dx$, but in this case one is conditioning on an inequality, and I don't know how to treat that. Thanks in advance!",,"['probability', 'conditional-expectation']"
18,"Prove that $\mathbb P(X>Y) =\frac{b}{a + b}$ if $X, Y$ are exponentially distributed with parameters $a$ and $b$.",Prove that  if  are exponentially distributed with parameters  and .,"\mathbb P(X>Y) =\frac{b}{a + b} X, Y a b","Let $X, Y$ be an exponentially distributed random variables with parameters $a, b$. Then $X$ has pdf:    $$f_X(x) =\begin{cases} a e^{-a x},& x\geq 0\\ 0,& \text{otherwise}.\end{cases}$$ Suppose $X$ and $Y$ independent. Show that $$\mathbb P(X>Y) = \frac{b}{a+b}.$$ Now I thought the following: $$f(x,y) = f_X(x)\ f_Y(y) = abe^{-ax -by},\qquad\text{for } x,y > 0.$$ And then $$\mathbb P(X>Y) = \int_0^\infty \int_0^x a b e^{-ax -by}\,dydx$$ However, if I solve this (manually or using Wolframalpha), I can't seem to end up with $\frac{b}{a+b}$. Any ideas?","Let $X, Y$ be an exponentially distributed random variables with parameters $a, b$. Then $X$ has pdf:    $$f_X(x) =\begin{cases} a e^{-a x},& x\geq 0\\ 0,& \text{otherwise}.\end{cases}$$ Suppose $X$ and $Y$ independent. Show that $$\mathbb P(X>Y) = \frac{b}{a+b}.$$ Now I thought the following: $$f(x,y) = f_X(x)\ f_Y(y) = abe^{-ax -by},\qquad\text{for } x,y > 0.$$ And then $$\mathbb P(X>Y) = \int_0^\infty \int_0^x a b e^{-ax -by}\,dydx$$ However, if I solve this (manually or using Wolframalpha), I can't seem to end up with $\frac{b}{a+b}$. Any ideas?",,"['probability', 'integration', 'probability-distributions', 'independence', 'exponential-distribution']"
19,How to compute an expected value in shorter ways (when taking all possibilities into account isn't plausible.),How to compute an expected value in shorter ways (when taking all possibilities into account isn't plausible.),,"There is this question on which I have been spending a lot of time, trying to understand how to compute an expected value in a comprehensive way, as sorting out all the possibilities doesn't seem like that right thing to do, nor does it even seem possible. The question states: Dan tosses infinitely many standard, independent coins. The coins are tossed one by one. What is the expected number of tosses it will take Dan to arrive at two consecutive heads? The answer says it is 6, and I didn't understand what to do. This is my attempt: Firstly, I need to arrive at the first head. That for itself would have a geometric distribution with $\frac12$ , which requires at least $2$ expected steps to be made. Now, I either get another head and I am done, or I get tail, count one step, and then make 2 more expected steps. Now I am in my 6th move and how do I know I am expected to arrive at head? Is that because last time I arrived at tail? I feel like I am in the right direction, but I don't fully comprehend the properties of expected value.","There is this question on which I have been spending a lot of time, trying to understand how to compute an expected value in a comprehensive way, as sorting out all the possibilities doesn't seem like that right thing to do, nor does it even seem possible. The question states: Dan tosses infinitely many standard, independent coins. The coins are tossed one by one. What is the expected number of tosses it will take Dan to arrive at two consecutive heads? The answer says it is 6, and I didn't understand what to do. This is my attempt: Firstly, I need to arrive at the first head. That for itself would have a geometric distribution with , which requires at least expected steps to be made. Now, I either get another head and I am done, or I get tail, count one step, and then make 2 more expected steps. Now I am in my 6th move and how do I know I am expected to arrive at head? Is that because last time I arrived at tail? I feel like I am in the right direction, but I don't fully comprehend the properties of expected value.",\frac12 2,['probability']
20,Probability of asymmetric random walk returning to the origin,Probability of asymmetric random walk returning to the origin,,"Consider the random walk $S_n$ given by $ S_{n+1} =  \left\{   \begin{array}{l}     S_n+2 & \text{with probability $p$}\\     S_n - 1 & \text{with probability $1-p$}   \end{array} \right. \ $ Assume that $S_0 = n >0 $ . What is the probability of eventually reaching the the point $0$ ? Seems that the probability for this must be less than one; since it is not symmetric, it is not guaranteed to reach the origin if $p>0.5$ . It also seems like the positive starting value not being at the origin matters too, but I'm not sure how to compute the probability of eventually reaching the origin.","Consider the random walk given by Assume that . What is the probability of eventually reaching the the point ? Seems that the probability for this must be less than one; since it is not symmetric, it is not guaranteed to reach the origin if . It also seems like the positive starting value not being at the origin matters too, but I'm not sure how to compute the probability of eventually reaching the origin.","S_n 
S_{n+1} =  \left\{
  \begin{array}{l}
    S_n+2 & \text{with probability p}\\
    S_n - 1 & \text{with probability 1-p}
  \end{array}
\right.
\
 S_0 = n >0  0 p>0.5","['probability', 'probability-theory', 'random-walk']"
21,How can I solve this using permutations?,How can I solve this using permutations?,,"Delegates from 10 countries, including Russia, France, England and the United States are to be seated in a row. How many different seating arrangements are possible if the French and English delegates are to be seated next to each other and the Russian and American delegates are not to be next to each other? The approach I took was first determining the total possible arrangements and then subtracting out the ones that won't work as per the given conditions. So, the total possible arrangments is $10!$. Take the first condition where the French and English delegates must be seated next to each other. Pick a spot for the French delegate. Now, if the French delegate is at one of the end seats, then there is only $1$ spot for the English delegate. This constitutes $2$ possibilities for their seating. Now, if the French delegate is in one of the $8$ interior seats, the English delegate has $2$ possible seats for each one so $16$ pairs. A similar approach would be taken for the Russian and American delegates. Am I heading in the right direction? Any help would be greatly appreciated. Thanks!","Delegates from 10 countries, including Russia, France, England and the United States are to be seated in a row. How many different seating arrangements are possible if the French and English delegates are to be seated next to each other and the Russian and American delegates are not to be next to each other? The approach I took was first determining the total possible arrangements and then subtracting out the ones that won't work as per the given conditions. So, the total possible arrangments is $10!$. Take the first condition where the French and English delegates must be seated next to each other. Pick a spot for the French delegate. Now, if the French delegate is at one of the end seats, then there is only $1$ spot for the English delegate. This constitutes $2$ possibilities for their seating. Now, if the French delegate is in one of the $8$ interior seats, the English delegate has $2$ possible seats for each one so $16$ pairs. A similar approach would be taken for the Russian and American delegates. Am I heading in the right direction? Any help would be greatly appreciated. Thanks!",,['probability']
22,Card Problem from Gardner,Card Problem from Gardner,,"The following problem is from aha! Insight by Martin Gardner. On a draw of $n$ cards from a deck without replacement, what is the   probability that seven of the $n$ cards all have the same suit? It has been a while since I have done a probability computation so I am here to have my solution verified. Let $X_n$ be the random event of drawing at least seven cards of the same suit upon drawing $n$ cards. $$P(X_n) = \left\{      \begin{array}{ll}        0 & : n < 7\\        \dfrac{\binom{4}{1}\binom{13}{7}\binom{39}{n-7}}{\binom{52}{n}} & : 7 \le n < 25\\        1 & : 25 \le n      \end{array}    \right. $$ EDIT: So this counts the number of ways to draw exactly seven cards with $n$ draws. Robert's post gives the desired answer, but I am wondering if there is a simpler expression for the probability given solely in terms of $n$. EDIT2: Started bounty looking for a solution in terms of $n$. I would also accept an explanation as to why Robert's solution is as simple as it gets.","The following problem is from aha! Insight by Martin Gardner. On a draw of $n$ cards from a deck without replacement, what is the   probability that seven of the $n$ cards all have the same suit? It has been a while since I have done a probability computation so I am here to have my solution verified. Let $X_n$ be the random event of drawing at least seven cards of the same suit upon drawing $n$ cards. $$P(X_n) = \left\{      \begin{array}{ll}        0 & : n < 7\\        \dfrac{\binom{4}{1}\binom{13}{7}\binom{39}{n-7}}{\binom{52}{n}} & : 7 \le n < 25\\        1 & : 25 \le n      \end{array}    \right. $$ EDIT: So this counts the number of ways to draw exactly seven cards with $n$ draws. Robert's post gives the desired answer, but I am wondering if there is a simpler expression for the probability given solely in terms of $n$. EDIT2: Started bounty looking for a solution in terms of $n$. I would also accept an explanation as to why Robert's solution is as simple as it gets.",,"['probability', 'card-games']"
23,Bound for probability of the intersection of a set of events,Bound for probability of the intersection of a set of events,,"There are $N$ random variables $X_1,\dots X_N$ and $Pr(X_i=1)=p$ $\forall i\in N$. Can we upper bound the probability  that all random variables are $1$, i.e., $Pr(X_i=1,\forall i\in N)$. Note that the random variables are not independent. Edit: How about a lower bound? Looking for answers other than $0,1$. My attempt: I am thinking the product (as if independent) is a upperbound, but not sure.","There are $N$ random variables $X_1,\dots X_N$ and $Pr(X_i=1)=p$ $\forall i\in N$. Can we upper bound the probability  that all random variables are $1$, i.e., $Pr(X_i=1,\forall i\in N)$. Note that the random variables are not independent. Edit: How about a lower bound? Looking for answers other than $0,1$. My attempt: I am thinking the product (as if independent) is a upperbound, but not sure.",,['probability']
24,Expected value of the distance between 2 uniformly distributed points on circle,Expected value of the distance between 2 uniformly distributed points on circle,,"I have the following problem (related to Bertrand ): Given a circle of radius $a=1$. Choose 2 points randomly on the circle circumference. Then connect these points using a line with length $b$.   What is the expected length of this line? ($\mathbb{E}[b]$=..?) I have tried this: $x_i=\cos(\theta_i), y_i=\sin(\theta_i), \quad i=1,2$, where $\theta_i$ is uniformly distributed on $[0,2\pi]$ Then I tried to compute the squared distance. The squared distance between two points in the Eucledian space is: $$d^2=(\cos(\theta_1)-\cos(\theta_2))^2+(\sin(\theta_1)-\sin(\theta_2))^2 $$ Now taking expectations I got: $$E(d^2)=2-2 \ ( \ E(\cos(\theta_1)\cos(\theta_2) + E(\sin(\theta_1)\sin(\theta_2) \ )$$ (as $E(\cos^2(\theta_i))=E(\sin^2(\theta_j))$ Then $$E(\cos(\theta_1)\cos(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi}\theta_1 \theta_2\cos^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \cos^2(\frac{1}{2\pi})$$ and $$E(\sin(\theta_1)\sin(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi} \theta_1 \theta_2\sin^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \sin^2(\frac{1}{2\pi})$$ so that $$d^2=2-4 \pi^2 \left(\cos^2(\frac{1}{2 \pi}) + \sin^2(\frac{1}{2\pi})\right)=2-4 \pi^2$$ But that doesn't make sense since it is negative. Any help would be appreciated","I have the following problem (related to Bertrand ): Given a circle of radius $a=1$. Choose 2 points randomly on the circle circumference. Then connect these points using a line with length $b$.   What is the expected length of this line? ($\mathbb{E}[b]$=..?) I have tried this: $x_i=\cos(\theta_i), y_i=\sin(\theta_i), \quad i=1,2$, where $\theta_i$ is uniformly distributed on $[0,2\pi]$ Then I tried to compute the squared distance. The squared distance between two points in the Eucledian space is: $$d^2=(\cos(\theta_1)-\cos(\theta_2))^2+(\sin(\theta_1)-\sin(\theta_2))^2 $$ Now taking expectations I got: $$E(d^2)=2-2 \ ( \ E(\cos(\theta_1)\cos(\theta_2) + E(\sin(\theta_1)\sin(\theta_2) \ )$$ (as $E(\cos^2(\theta_i))=E(\sin^2(\theta_j))$ Then $$E(\cos(\theta_1)\cos(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi}\theta_1 \theta_2\cos^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \cos^2(\frac{1}{2\pi})$$ and $$E(\sin(\theta_1)\sin(\theta_2))\overset{uniform}=\int_0^{2\pi}\int_0^{2\pi} \theta_1 \theta_2\sin^2(\frac{1}{2\pi})\ \mathrm{d}\theta_1 \ \mathrm{d}\theta_2 = 4\pi^4 \sin^2(\frac{1}{2\pi})$$ so that $$d^2=2-4 \pi^2 \left(\cos^2(\frac{1}{2 \pi}) + \sin^2(\frac{1}{2\pi})\right)=2-4 \pi^2$$ But that doesn't make sense since it is negative. Any help would be appreciated",,"['probability', 'probability-theory', 'euclidean-geometry', 'paradoxes', 'geometric-probability']"
25,Using Recursion to Solve Coupon Collector,Using Recursion to Solve Coupon Collector,,"I read a brilliant answer by Mike Spivey on one of the questions  and I was wondering how I could use it to solve a coupon collectors problem. The problem is : There are coupons labelled 1,2,3...,10 how many coupons do I need to collect in order to have one of each labels. I know that the answer is $\displaystyle \sum_{i=1}^{10} \dfrac{10}{i}$ Here is my attempt : Let $X_i$  be the random variable corresponding to the number of coupons needed to be collected to have exactly $i$ unique labels. \begin{align} E(X_1)&=1\\ E(X_2|X_1)&=\dfrac{9}{10}.(X_1+1)+\dfrac{1}{10}.(E(X_2))\\ \implies E(X_2)&=\dfrac{9}{10}.(E(X_1)+1)+\dfrac{1}{10}.(E(X_2))\\ \text{Similarly,}\\ E(X_3)&=\dfrac{8}{10}.(E(X_2)+1)+\dfrac{2}{10}.(E(X_3))\\ \vdots \end{align} But this gives me the wrong answer. I know that there is a problem in my second equation but don't know why. My logic was as follows: Assuming I know how much it takes to get 1 coupon $(E(X_1))$ with probability 9/10 I find 2 in  $E(X_1)+1$ else, I just have $E(X_2)$. Neither does the formula in the aforementioned question work. Can someone help me set up the recursion equation? (If possible, please retain my Random Variables. I am more interesting in knowing why my logic is failing in designing the recursion than answering the original question)","I read a brilliant answer by Mike Spivey on one of the questions  and I was wondering how I could use it to solve a coupon collectors problem. The problem is : There are coupons labelled 1,2,3...,10 how many coupons do I need to collect in order to have one of each labels. I know that the answer is $\displaystyle \sum_{i=1}^{10} \dfrac{10}{i}$ Here is my attempt : Let $X_i$  be the random variable corresponding to the number of coupons needed to be collected to have exactly $i$ unique labels. \begin{align} E(X_1)&=1\\ E(X_2|X_1)&=\dfrac{9}{10}.(X_1+1)+\dfrac{1}{10}.(E(X_2))\\ \implies E(X_2)&=\dfrac{9}{10}.(E(X_1)+1)+\dfrac{1}{10}.(E(X_2))\\ \text{Similarly,}\\ E(X_3)&=\dfrac{8}{10}.(E(X_2)+1)+\dfrac{2}{10}.(E(X_3))\\ \vdots \end{align} But this gives me the wrong answer. I know that there is a problem in my second equation but don't know why. My logic was as follows: Assuming I know how much it takes to get 1 coupon $(E(X_1))$ with probability 9/10 I find 2 in  $E(X_1)+1$ else, I just have $E(X_2)$. Neither does the formula in the aforementioned question work. Can someone help me set up the recursion equation? (If possible, please retain my Random Variables. I am more interesting in knowing why my logic is failing in designing the recursion than answering the original question)",,"['probability', 'probability-theory', 'stochastic-processes']"
26,How many strings of 8 English letters are there...?,How many strings of 8 English letters are there...?,,"1) That contain at least one vowel, if letters can be repeated? $26^8-21^8$ 2) That contain exactly one vowel, if letters can be repeated? $8\cdot 5\cdot 21^7$ 3) That start with an X and contain at least one vowel, if letters can be repeated? $1\cdot 26^7-1\cdot 21^7$ Assume only upper-cased letters are used. I'm just trying to intuitively understand what's going on here.  Can anyone explain in a clear and concise manner? Thank you!","1) That contain at least one vowel, if letters can be repeated? $26^8-21^8$ 2) That contain exactly one vowel, if letters can be repeated? $8\cdot 5\cdot 21^7$ 3) That start with an X and contain at least one vowel, if letters can be repeated? $1\cdot 26^7-1\cdot 21^7$ Assume only upper-cased letters are used. I'm just trying to intuitively understand what's going on here.  Can anyone explain in a clear and concise manner? Thank you!",,['probability']
27,How would one rescale the variance and standard deviation of a set of data?,How would one rescale the variance and standard deviation of a set of data?,,"I have a set of data with a certain mean, variance, and standard deviation. I centered the mean around the origin the standard way by subtracting it from the data. Now how do I modify the data to make variance = 1?","I have a set of data with a certain mean, variance, and standard deviation. I centered the mean around the origin the standard way by subtracting it from the data. Now how do I modify the data to make variance = 1?",,"['probability', 'statistics', 'probability-distributions']"
28,Find the ordinary generating function $h(z)$ for a Gambler's Ruin variation.,Find the ordinary generating function  for a Gambler's Ruin variation.,h(z),"Assume we have a random walk starting at 1 with probability of moving left one space $q$, moving right one space $p$, and staying in the same place $r=1-p-q$. Let $T$ be the number of steps to reach 0. Find $h(z)$, the ordinary generating function. My idea was to break $T$ into two variables $Y_1,Y_2$ where $Y_1$ describes the number of times you stay in place and $Y_2$ the number of times we move forward or backward one. Then try to find a formula for $P(T=n)=P(Y_1+Y_2=n)=r_n$, but I'm getting really confused since there are multiple probabilities possible for each $T=n$ for $n\geq 3$. Once I have $r_n$ I can then use $h_T(z)=\sum_{n=1}^\infty r_n z^n$, but I'm not sure where to go from here.","Assume we have a random walk starting at 1 with probability of moving left one space $q$, moving right one space $p$, and staying in the same place $r=1-p-q$. Let $T$ be the number of steps to reach 0. Find $h(z)$, the ordinary generating function. My idea was to break $T$ into two variables $Y_1,Y_2$ where $Y_1$ describes the number of times you stay in place and $Y_2$ the number of times we move forward or backward one. Then try to find a formula for $P(T=n)=P(Y_1+Y_2=n)=r_n$, but I'm getting really confused since there are multiple probabilities possible for each $T=n$ for $n\geq 3$. Once I have $r_n$ I can then use $h_T(z)=\sum_{n=1}^\infty r_n z^n$, but I'm not sure where to go from here.",,"['probability', 'generating-functions']"
29,Expectation of the cardinality of the intersection of subsets,Expectation of the cardinality of the intersection of subsets,,"Let A be a set of n distinct elements, and let A' and A'' be independent permutations of A, where |A'| = |A''| = k.   What is the expectation for |A' ∩ A''| for any given k <= n?","Let A be a set of n distinct elements, and let A' and A'' be independent permutations of A, where |A'| = |A''| = k.   What is the expectation for |A' ∩ A''| for any given k <= n?",,"['probability', 'discrete-mathematics', 'permutations']"
30,New Two Children problem,New Two Children problem,,"The well known ""Two Children problem"" is answered in "" In a family with two children, what are the chances, if one of the children is a girl, that both children are girls?"" What about this variant: M. Smith says: I have two children and at least one is a girl named Jane . What are the chances that both children are girls if the probability that a girl is named Jane is p  ?","The well known ""Two Children problem"" is answered in "" In a family with two children, what are the chances, if one of the children is a girl, that both children are girls?"" What about this variant: M. Smith says: I have two children and at least one is a girl named Jane . What are the chances that both children are girls if the probability that a girl is named Jane is p  ?",,"['probability', 'faq']"
31,Probability that the row in the table contain only one bit one?,Probability that the row in the table contain only one bit one?,,"I am an electrical engineer that is currently working with an $m \times n$ matrix that only contains bit 0 and bit 1 in its cell. Assuming that bit 0 and bit 1 can appear in a cell with probability $p$ and $q$ respectively, where $p+q=1$ . Also, assume that the probability of 1 and 0 in each cell is independent of any other cell. 1/ How can I evaluate the probability that a row contain only one bit 1 ? For example, row 1, row 2, and final row  contain only one bit 1 2/ On average, how many row will contain only one bit 1 ? In my application, there is a row dropping action that can be describe as follows: Starting at the row that contain only bit one (row 5) and begin to look up and look down (blue arrow), if we see bit 1 above (same column) or bit 1 right below it (same column) then we will initiate a drop row action for the corresponding row. Note that I have marked the bit 1 above and bit 1 below in a blue rectangular cell. So, in total 4 rows will be dropped. That is row 1, 2,7 and 8. 3/ On average, how many row will be dropped ? I have a guts feeling that this may have something to do with the binomial distribution or geometric distribution but cannot get me head around it. Please help me with this ? Edit for clarity : 1/ For question 3, it is possible that there are many rows that contains one bit 1. 2/ For question 3, row that contain one bit 1 will never be dropped ! Thank you for your enthusiasm !","I am an electrical engineer that is currently working with an matrix that only contains bit 0 and bit 1 in its cell. Assuming that bit 0 and bit 1 can appear in a cell with probability and respectively, where . Also, assume that the probability of 1 and 0 in each cell is independent of any other cell. 1/ How can I evaluate the probability that a row contain only one bit 1 ? For example, row 1, row 2, and final row  contain only one bit 1 2/ On average, how many row will contain only one bit 1 ? In my application, there is a row dropping action that can be describe as follows: Starting at the row that contain only bit one (row 5) and begin to look up and look down (blue arrow), if we see bit 1 above (same column) or bit 1 right below it (same column) then we will initiate a drop row action for the corresponding row. Note that I have marked the bit 1 above and bit 1 below in a blue rectangular cell. So, in total 4 rows will be dropped. That is row 1, 2,7 and 8. 3/ On average, how many row will be dropped ? I have a guts feeling that this may have something to do with the binomial distribution or geometric distribution but cannot get me head around it. Please help me with this ? Edit for clarity : 1/ For question 3, it is possible that there are many rows that contains one bit 1. 2/ For question 3, row that contain one bit 1 will never be dropped ! Thank you for your enthusiasm !",m \times n p q p+q=1,"['probability', 'statistics']"
32,Probability that the next card is the ace of spades,Probability that the next card is the ace of spades,,"Say I have a standard $52$ count deck of cards in random order, and that I start flipping cards from the deck over until a king appears, which is card # $19$ . What's the probability the next card is the ace of spades? It's possible to brute force this using Bayes' theorem and slogging through a laborious calculation, but I was wondering if there were any tricks to arrive at the answer in a cleverer way.","Say I have a standard count deck of cards in random order, and that I start flipping cards from the deck over until a king appears, which is card # . What's the probability the next card is the ace of spades? It's possible to brute force this using Bayes' theorem and slogging through a laborious calculation, but I was wondering if there were any tricks to arrive at the answer in a cleverer way.",52 19,"['probability', 'conditional-probability', 'bayes-theorem', 'card-games']"
33,"Best strategy for 6 sided dice game: Roll as many dice as you want, you lose if at least one $1$ appears.","Best strategy for 6 sided dice game: Roll as many dice as you want, you lose if at least one  appears.",1,"I am new here because we got stuck with a question during the weekend's discussion. Let's imagine we are playing a dice game. You can roll as many 6-sided dice as you want, and so can your opponents, simultaneously. The highest sum wins, but if there is at least one "" $1$ "" in the roll, you score $0$ points in total. Is there ""the"" best strategy with how many dice to roll in order to statistically (unlimited repeats) win the game? If there is, how can we prove it? Our only starting point was the counterprobability for each throw. Each dice has a $5/6$ chance not to be a $1$ . So $1-(5/6 + 5/6 + \ldots)$ should be at least a "" $1$ "" in the throw. That's all we were able to manage, but we are missing the link to further calculations. Thanks a lot for your help. Edit: Thanks for the hint with 1−(5/6)^n We are playing one round with only 2 players. If it is a draw --> reroll with the selected number of dice The other player does not know how many dice the other player selects. The first win ends the game. If you roll ""1"" the whole round scores ""0"" To the question: Are you trying to win or trying to maximise the expected number of points? We are trying to find the best number of dice to win the game, but it never has occurred to me, the the number of dice of the other player is of relevance! Thanks for the input Also thanks a lot for all of your comments. I really appreciate it.","I am new here because we got stuck with a question during the weekend's discussion. Let's imagine we are playing a dice game. You can roll as many 6-sided dice as you want, and so can your opponents, simultaneously. The highest sum wins, but if there is at least one "" "" in the roll, you score points in total. Is there ""the"" best strategy with how many dice to roll in order to statistically (unlimited repeats) win the game? If there is, how can we prove it? Our only starting point was the counterprobability for each throw. Each dice has a chance not to be a . So should be at least a "" "" in the throw. That's all we were able to manage, but we are missing the link to further calculations. Thanks a lot for your help. Edit: Thanks for the hint with 1−(5/6)^n We are playing one round with only 2 players. If it is a draw --> reroll with the selected number of dice The other player does not know how many dice the other player selects. The first win ends the game. If you roll ""1"" the whole round scores ""0"" To the question: Are you trying to win or trying to maximise the expected number of points? We are trying to find the best number of dice to win the game, but it never has occurred to me, the the number of dice of the other player is of relevance! Thanks for the input Also thanks a lot for all of your comments. I really appreciate it.",1 0 5/6 1 1-(5/6 + 5/6 + \ldots) 1,"['probability', 'statistics', 'game-theory', 'dice', 'simulation']"
34,Upper bound of probability of not getting all values in independent draws,Upper bound of probability of not getting all values in independent draws,,"We draw $m$ uniform independent random values among $n$ , with $m\ge n$ . We consider the probability $p(m,n)$ that not all $n$ values have been drawn. We want an upper bound within a constant factor. The probability that a given value was not drawn is $q(m,n)=(1-1/n)^m$ . If these probabilities were independent, we could use $1-(1-q(m,n))^n$ for $p(m,n)$ . However the probabilities are dependent and that expression yields dead wrong values. How can we rigorously derive an upper bound? I needs not be tight, I'd be happy with something within any constant factor. The question's motivation is to prove that $\displaystyle\lim_{n\to+\infty}p(n\left\lceil\log_2(n)\right\rceil,n)=0$ , if possible with an explicit upper bound (towards answering an often-asked question about cryptographic hashes reaching their whole stated destination set, under a random oracle model). This itself is plausible because (as studied in the coupon collector's problem ) the expected number of draws to get all $n$ values is $n(\ln(n)+\gamma)+\frac12+\mathcal O(\frac1n)$ , and $n\left\lceil\log_2(n)\right\rceil$ grows faster than that, by a factor $\frac1{\ln(2)}>1$ .","We draw uniform independent random values among , with . We consider the probability that not all values have been drawn. We want an upper bound within a constant factor. The probability that a given value was not drawn is . If these probabilities were independent, we could use for . However the probabilities are dependent and that expression yields dead wrong values. How can we rigorously derive an upper bound? I needs not be tight, I'd be happy with something within any constant factor. The question's motivation is to prove that , if possible with an explicit upper bound (towards answering an often-asked question about cryptographic hashes reaching their whole stated destination set, under a random oracle model). This itself is plausible because (as studied in the coupon collector's problem ) the expected number of draws to get all values is , and grows faster than that, by a factor .","m n m\ge n p(m,n) n q(m,n)=(1-1/n)^m 1-(1-q(m,n))^n p(m,n) \displaystyle\lim_{n\to+\infty}p(n\left\lceil\log_2(n)\right\rceil,n)=0 n n(\ln(n)+\gamma)+\frac12+\mathcal O(\frac1n) n\left\lceil\log_2(n)\right\rceil \frac1{\ln(2)}>1","['probability', 'coupon-collector']"
35,Expected value of squared distance after $2016$ knight moves,Expected value of squared distance after  knight moves,2016,"From PUMaC 2016: https://static1.squarespace.com/static/570450471d07c094a39efaed/t/58b0dab8d1758e013286f070/1487985337234/PUMaC2016_CombinatoricsA.pdf A knight is placed at the origin of the Cartesian plane. Each turn, the knight moves in an chess L-shape ( $2$ units parallel to one axis and $1$ unit parallel to the other) to one of eight possible locations, chosen at random. After $2016$ such turns, what is the expected value of the square of the distance of the knight from the origin? Shouldn't this just be $0$ ? Imagine the $8$ possible moves each turn to be vectors with magnitude $\sqrt{5}$ , pair each one off with the one that's $180$ degrees in the other direction, and they all cancel out to $0$ . What am I missing? Edit: Honestly, I still don't understand the solutions given. At a turn starting from $(a, b)$ at most $1$ of the $8$ possible moves is parallel to the vector from $(0, 0)$ to $(a, b)$ , and that would increase the square of the distance to the origin by $5$ . But by the triangle inequality, the other $7$ necessarily increase the square of the distance to the origin by less than $5$ , and in some cases decrease it. So the idea that the average increase of the square of the distance to the origin is $5$ doesn't make sense to me, intuitively it should be less than $5$ . Can someone explain what's wrong with this thinking?","From PUMaC 2016: https://static1.squarespace.com/static/570450471d07c094a39efaed/t/58b0dab8d1758e013286f070/1487985337234/PUMaC2016_CombinatoricsA.pdf A knight is placed at the origin of the Cartesian plane. Each turn, the knight moves in an chess L-shape ( units parallel to one axis and unit parallel to the other) to one of eight possible locations, chosen at random. After such turns, what is the expected value of the square of the distance of the knight from the origin? Shouldn't this just be ? Imagine the possible moves each turn to be vectors with magnitude , pair each one off with the one that's degrees in the other direction, and they all cancel out to . What am I missing? Edit: Honestly, I still don't understand the solutions given. At a turn starting from at most of the possible moves is parallel to the vector from to , and that would increase the square of the distance to the origin by . But by the triangle inequality, the other necessarily increase the square of the distance to the origin by less than , and in some cases decrease it. So the idea that the average increase of the square of the distance to the origin is doesn't make sense to me, intuitively it should be less than . Can someone explain what's wrong with this thinking?","2 1 2016 0 8 \sqrt{5} 180 0 (a, b) 1 8 (0, 0) (a, b) 5 7 5 5 5","['probability', 'combinatorics', 'geometry', 'expected-value']"
36,2 dimensional symmetric random walk,2 dimensional symmetric random walk,,"Let $\left\{\left(X_{n}, Y_{n}\right)\right\}_{n=0}^{\infty}$ be a 2 -dimensional symmetric random walk. Namely, this is a Markov chain where $\left(X_{n+1}, Y_{n+1}\right)$ takes one of the following 4 values with equal probability: $$\left(X_{n}+1, Y_{n}\right),\left(X_{n}-1,Y_{n}\right),\left(X_{n}, Y_{n}+1\right),\left(X_{n}, Y_{n}-1\right)$$ Suppose that $X_{0}=Y_{0}=0$ . Define $T:=\inf \left\{n \geq 0: \max \left(\left|X_{n}\right|,\left|Y_{n}\right|\right)=3\right\} .$ I want to find the value of $\mathbb{E}[T]$ and $\mathbb{P}\left(X_{T}=3, Y_{T}=0\right)$ . Thanks for your help!","Let be a 2 -dimensional symmetric random walk. Namely, this is a Markov chain where takes one of the following 4 values with equal probability: Suppose that . Define I want to find the value of and . Thanks for your help!","\left\{\left(X_{n}, Y_{n}\right)\right\}_{n=0}^{\infty} \left(X_{n+1}, Y_{n+1}\right) \left(X_{n}+1, Y_{n}\right),\left(X_{n}-1,Y_{n}\right),\left(X_{n}, Y_{n}+1\right),\left(X_{n}, Y_{n}-1\right) X_{0}=Y_{0}=0 T:=\inf \left\{n \geq 0: \max \left(\left|X_{n}\right|,\left|Y_{n}\right|\right)=3\right\} . \mathbb{E}[T] \mathbb{P}\left(X_{T}=3, Y_{T}=0\right)","['probability', 'stochastic-processes', 'markov-chains', 'random-walk']"
37,Probability that 3 darts land in a same half of a dart board,Probability that 3 darts land in a same half of a dart board,,3 darts are thrown (equal probability of landing anywhere on the dart board). What's the probability that they all land on a same half of the dart board? Edit: I know the first 2 darts can land anywhere but don't know how to find the probability that the 3rd lands in an acceptable region.,3 darts are thrown (equal probability of landing anywhere on the dart board). What's the probability that they all land on a same half of the dart board? Edit: I know the first 2 darts can land anywhere but don't know how to find the probability that the 3rd lands in an acceptable region.,,['probability']
38,"Interpreting the word ""randomly""","Interpreting the word ""randomly""",,"Suppose that 3 indistinguishable balls are placed at random into 3 distinguishable cells. What is the probability that exactly one cell remains empty? The book's answer is $$\frac{3(3-1)}{3+3-1\choose{3}}=\frac{3}{5}$$ . In words, the number of ways of distributing the 3 balls so that there is exactly one cell that is empty divided by the number of ways to distributing the 3 balls. My question is, that if I interpret ""randomly"" to mean the probability that any ball falls into a cell is $ \frac{1}{3} $ . I think the answer comes out different. The probability that the first cell is empty is $\frac{6}{27}$ . So the probability that one of them is empty is $$3 *\frac{6}{27}=\frac{2}{3}$$ . Therefore is it right to say that there are two interpretations to this question and based on the wording we should infer the first interpretation?","Suppose that 3 indistinguishable balls are placed at random into 3 distinguishable cells. What is the probability that exactly one cell remains empty? The book's answer is . In words, the number of ways of distributing the 3 balls so that there is exactly one cell that is empty divided by the number of ways to distributing the 3 balls. My question is, that if I interpret ""randomly"" to mean the probability that any ball falls into a cell is . I think the answer comes out different. The probability that the first cell is empty is . So the probability that one of them is empty is . Therefore is it right to say that there are two interpretations to this question and based on the wording we should infer the first interpretation?",\frac{3(3-1)}{3+3-1\choose{3}}=\frac{3}{5}  \frac{1}{3}  \frac{6}{27} 3 *\frac{6}{27}=\frac{2}{3},['probability']
39,Example of a Non-negative Martingale Satisfying Certain Conditions,Example of a Non-negative Martingale Satisfying Certain Conditions,,"Question The question is to find an example of a non-negative martingale $(X_n)$ with $EX_n=1$ for all $n$ such that $X_n$ converges almost surely to a random variable $X$ where $EX\neq 1$ and $\text{Var}(X)>0$ . My attempt An example of a martingale that I thought could fit the bill was the product martingale with $X_n=\prod_{i=1}^n Y_i$ where $(Y_{i})$ are i.i.d non-negative random variables with mean $1$ and $P(Y_i=1)<1$ . Unfortunately $X_n\to 0$ a.s and hence the limit is degenerate. Other examples, I tried to cook up (e.g. branching process with one individual) all had degenerate limits. I am having trouble coming up with an example that does not have a degenerate limit.","Question The question is to find an example of a non-negative martingale with for all such that converges almost surely to a random variable where and . My attempt An example of a martingale that I thought could fit the bill was the product martingale with where are i.i.d non-negative random variables with mean and . Unfortunately a.s and hence the limit is degenerate. Other examples, I tried to cook up (e.g. branching process with one individual) all had degenerate limits. I am having trouble coming up with an example that does not have a degenerate limit.",(X_n) EX_n=1 n X_n X EX\neq 1 \text{Var}(X)>0 X_n=\prod_{i=1}^n Y_i (Y_{i}) 1 P(Y_i=1)<1 X_n\to 0,"['probability', 'probability-theory', 'convergence-divergence', 'martingales']"
40,Conditional probabilities with impossible outcomes,Conditional probabilities with impossible outcomes,,"Suppose we want to predict the outcome of a race between three runners $A$, $B$, $C$. We know the prior probabilities for head-to-head runs: $p_{A>B}$, $p_{A>C}$, $p_{B>C}$, where $A>B$ means that A finishes before B. How do we get the probability $p_{A>B>C}$? I thought I could just use a tree diagram but I run into the following problem: Some outcomes are impossible. For example, if my first edge is ""A finishes before B"" and my second edge is ""A finishes after C"", then ""B finishes before C"" is no longer possible. I thought about just setting probability of impossible outcomes to 0, but then my outcome probabilities are different depending on which of the three probabilities I start with. I think I'm making some basic mistake here. I've seen a lot of similar questions but I don't think they answer this. I've also tried looking at how horse race outcomes are predicted, but couldn't find an answer that I understood.","Suppose we want to predict the outcome of a race between three runners $A$, $B$, $C$. We know the prior probabilities for head-to-head runs: $p_{A>B}$, $p_{A>C}$, $p_{B>C}$, where $A>B$ means that A finishes before B. How do we get the probability $p_{A>B>C}$? I thought I could just use a tree diagram but I run into the following problem: Some outcomes are impossible. For example, if my first edge is ""A finishes before B"" and my second edge is ""A finishes after C"", then ""B finishes before C"" is no longer possible. I thought about just setting probability of impossible outcomes to 0, but then my outcome probabilities are different depending on which of the three probabilities I start with. I think I'm making some basic mistake here. I've seen a lot of similar questions but I don't think they answer this. I've also tried looking at how horse race outcomes are predicted, but couldn't find an answer that I understood.",,['probability']
41,World Cup: what group stage result (vector) is most likely?,World Cup: what group stage result (vector) is most likely?,,"In the World Cup 2018 group stage, a group consists of 4 teams, and every team plays every other team once, for a total of 6 games.  A team gets 3 points per win, 1 point per draw, 0 point per loss.  After all 6 games, the teams are ranked by total number of points. Let $v$ be the sorted 4-vector of total points.  E.g. $v=(9,6,3,0)$ represents a group where there are no draws, the top team beats all others, the 2nd top team beats the other two, and the 3rd top team only beats the worst team.  Similarly, $v=(3,3,3,3)$ represents a group where all 6 games are drawn. (Note: only a small number of different vectors are possible.  This was asked in World Cup Standings but has no posted answer.) My question is: which $v$ is most likely?  Obviously this depends on the probability model.  For the purpose of this question, assume (unrealistically) that each game is i.i.d., has probability $p$ of being drawn, and each team wins with equal probability ${1-p \over 2}$. What I seek: as a function of $p$, which $v$ is most likely? Further comments: Obviously, for any given value of $p$, the solution can be found numerically (exactly and/or Monte Carlo with high precision).  However, I'm hoping for a more intuitive answer using e.g. symmetry arguments, graph theory, entropy(?!) etc. I'm also interested in transitions as $p$ changes from $0$ to $1$. (E.g. as $p$ goes from $0$ to $1$, the $Prob(v=(3,3,3,3))$ also goes from $0$ to $1$, but at what point does $(3,3,3,3)$ become the most likely?) If you can't solve the general problem, it might still be interesting to know the answer for ""typical"" values of $d$.  E.g. as of this writing - the last games of Groups E & F just finished - there are 9 draws out of 44 games, so $p=9/44 \approx 0.2$. Lastly, one possible approach which I thought about for a bit but didnt make much progress: the problem might be easier if, instead of varying $p$, we vary the number of draws $D \in [0,6]$.  I.e. conditioned on $D$ draws out of 6 games, which $v$ is most likely?  This may provide an intermediate step to answering my original question (because $D$ is distributed $Binomial(6,p)$). [Off topic] Good luck to all remaining teams... and may all future VAR decisions be non-controversial!  :)","In the World Cup 2018 group stage, a group consists of 4 teams, and every team plays every other team once, for a total of 6 games.  A team gets 3 points per win, 1 point per draw, 0 point per loss.  After all 6 games, the teams are ranked by total number of points. Let $v$ be the sorted 4-vector of total points.  E.g. $v=(9,6,3,0)$ represents a group where there are no draws, the top team beats all others, the 2nd top team beats the other two, and the 3rd top team only beats the worst team.  Similarly, $v=(3,3,3,3)$ represents a group where all 6 games are drawn. (Note: only a small number of different vectors are possible.  This was asked in World Cup Standings but has no posted answer.) My question is: which $v$ is most likely?  Obviously this depends on the probability model.  For the purpose of this question, assume (unrealistically) that each game is i.i.d., has probability $p$ of being drawn, and each team wins with equal probability ${1-p \over 2}$. What I seek: as a function of $p$, which $v$ is most likely? Further comments: Obviously, for any given value of $p$, the solution can be found numerically (exactly and/or Monte Carlo with high precision).  However, I'm hoping for a more intuitive answer using e.g. symmetry arguments, graph theory, entropy(?!) etc. I'm also interested in transitions as $p$ changes from $0$ to $1$. (E.g. as $p$ goes from $0$ to $1$, the $Prob(v=(3,3,3,3))$ also goes from $0$ to $1$, but at what point does $(3,3,3,3)$ become the most likely?) If you can't solve the general problem, it might still be interesting to know the answer for ""typical"" values of $d$.  E.g. as of this writing - the last games of Groups E & F just finished - there are 9 draws out of 44 games, so $p=9/44 \approx 0.2$. Lastly, one possible approach which I thought about for a bit but didnt make much progress: the problem might be easier if, instead of varying $p$, we vary the number of draws $D \in [0,6]$.  I.e. conditioned on $D$ draws out of 6 games, which $v$ is most likely?  This may provide an intermediate step to answering my original question (because $D$ is distributed $Binomial(6,p)$). [Off topic] Good luck to all remaining teams... and may all future VAR decisions be non-controversial!  :)",,"['probability', 'combinatorics', 'recreational-mathematics']"
42,How does a probability density function evolve with time?,How does a probability density function evolve with time?,,"Let $f(x,t)$ be a probability density function of stochastic process $X_t$, where $X_t$ evolves over time deterministically according to the following ODE $$\dot x = g(x,t)$$ Then, how does $f(x)$ change with time? How to find  $\frac{df}{dt}$? For example, say $X_t$ is the height of a tree at time $t$. Tree heights are distributed according to $f(x,t)$ at a given time. Each tree grows according to  $\frac{dX_t}{dt}=g(X_t,t)$. Then what is  $\frac{df}{dt}$?","Let $f(x,t)$ be a probability density function of stochastic process $X_t$, where $X_t$ evolves over time deterministically according to the following ODE $$\dot x = g(x,t)$$ Then, how does $f(x)$ change with time? How to find  $\frac{df}{dt}$? For example, say $X_t$ is the height of a tree at time $t$. Tree heights are distributed according to $f(x,t)$ at a given time. Each tree grows according to  $\frac{dX_t}{dt}=g(X_t,t)$. Then what is  $\frac{df}{dt}$?",,"['probability', 'probability-theory', 'probability-distributions', 'partial-differential-equations', 'stochastic-processes']"
43,Is the numerator of $\sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}}$ always a power of $2$ in lowest terms?,Is the numerator of  always a power of  in lowest terms?,\sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}} 2,"Is the numerator of $$ \sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}} $$ always a power of $2$ in lowest terms, and if so, why? Is there a combinatorial or probabilistic proof of this?","Is the numerator of $$ \sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}} $$ always a power of $2$ in lowest terms, and if so, why? Is there a combinatorial or probabilistic proof of this?",,"['probability', 'combinatorics']"
44,Can the empirical mean and empirical variance be independent of each other?,Can the empirical mean and empirical variance be independent of each other?,,"Assume $n$ variables $\{X_i\}_{i=1}^n$ are independently drawn from the same Gaussian distribution. Then, we define the empirical mean by $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ and variance by $S^2=\frac{1}{n-1}(X_i-\bar{X})^2$. My question is that if $\bar{X}$ and $S^2$ are independent of each other? under which condition?","Assume $n$ variables $\{X_i\}_{i=1}^n$ are independently drawn from the same Gaussian distribution. Then, we define the empirical mean by $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ and variance by $S^2=\frac{1}{n-1}(X_i-\bar{X})^2$. My question is that if $\bar{X}$ and $S^2$ are independent of each other? under which condition?",,"['probability', 'statistics', 'statistical-inference']"
45,The additivity axiom of probabiity,The additivity axiom of probabiity,,"One of the axioms of probability is: If the sample space is finite and $A$ and $B$ are disjoint events then $Pr[A\cup B]=Pr[A]+Pr[B]$, and if the sample space is infinite , then for for any (possibly infinite number of) disjoint events, like $A_1,A_2,\ldots$, then $Pr[\cup_iA_i]=\sum_iPr[A_i]$ I cannot understand why the first one cannot imply the second one. I have seen several probability books and they have mentioned it and skipped its detail. Can anyone give an example that the union of just two disjoint sets does not imply the union of any (possibly infinite) number of disjoint events?","One of the axioms of probability is: If the sample space is finite and $A$ and $B$ are disjoint events then $Pr[A\cup B]=Pr[A]+Pr[B]$, and if the sample space is infinite , then for for any (possibly infinite number of) disjoint events, like $A_1,A_2,\ldots$, then $Pr[\cup_iA_i]=\sum_iPr[A_i]$ I cannot understand why the first one cannot imply the second one. I have seen several probability books and they have mentioned it and skipped its detail. Can anyone give an example that the union of just two disjoint sets does not imply the union of any (possibly infinite) number of disjoint events?",,"['probability', 'probability-theory', 'axioms']"
46,Show that $f_{\alpha}(t)$ is a p.d.f.,Show that  is a p.d.f.,f_{\alpha}(t),"Let $\displaystyle \phi(t)=\frac{1}{\sqrt{2\pi}}e^{-t^2/2}$,$t\in \Bbb R$ be the standard normal density function and $\displaystyle \Phi(x)=\int_{-\infty}^x\phi(t)\,dt$ be the standard normal distribution function. Let $f_{\alpha}(t)=2\phi(t)\Phi(\alpha t)$,$t\in \Bbb R$ where $\alpha \in \Bbb R$. Show that $f_{\alpha}$ is a probability density function. we have $\Phi'(x)=\phi(x)$. We have to show that $\displaystyle\int_{-\infty}^{\infty}f_{\alpha}(t)\,dt=1$. I tried by integration by-parts but I got the value is $0$. Dose there any other process or where is my mistake.? Edit : $\displaystyle \int_{-\infty}^{\infty} f_{\alpha}(t)dt= 2\int_{-\infty}^{\infty}\phi(t)\Phi(\alpha t)\,dt=2\left[\Phi(\alpha t)\int_{-\infty}^{\infty}\phi(t)\,dt\right]_{-\infty}^{\infty}-2\int_{-\infty}^{\infty}\left[\alpha\Phi'(\alpha t).\int_{-\infty}^{\infty}\phi(t)\,dt\right]\,dt=2[\Phi(\infty)-\Phi(-\infty)]-2\int_{-\infty}^{\infty}\alpha\phi(\alpha t)\,dt=\cdots=0$","Let $\displaystyle \phi(t)=\frac{1}{\sqrt{2\pi}}e^{-t^2/2}$,$t\in \Bbb R$ be the standard normal density function and $\displaystyle \Phi(x)=\int_{-\infty}^x\phi(t)\,dt$ be the standard normal distribution function. Let $f_{\alpha}(t)=2\phi(t)\Phi(\alpha t)$,$t\in \Bbb R$ where $\alpha \in \Bbb R$. Show that $f_{\alpha}$ is a probability density function. we have $\Phi'(x)=\phi(x)$. We have to show that $\displaystyle\int_{-\infty}^{\infty}f_{\alpha}(t)\,dt=1$. I tried by integration by-parts but I got the value is $0$. Dose there any other process or where is my mistake.? Edit : $\displaystyle \int_{-\infty}^{\infty} f_{\alpha}(t)dt= 2\int_{-\infty}^{\infty}\phi(t)\Phi(\alpha t)\,dt=2\left[\Phi(\alpha t)\int_{-\infty}^{\infty}\phi(t)\,dt\right]_{-\infty}^{\infty}-2\int_{-\infty}^{\infty}\left[\alpha\Phi'(\alpha t).\int_{-\infty}^{\infty}\phi(t)\,dt\right]\,dt=2[\Phi(\infty)-\Phi(-\infty)]-2\int_{-\infty}^{\infty}\alpha\phi(\alpha t)\,dt=\cdots=0$",,"['probability', 'probability-theory', 'probability-distributions']"
47,"What is the probability that a 1 was sent, given that 1 was received?","What is the probability that a 1 was sent, given that 1 was received?",,"I have the following information: $0$ is sent with probability $0.3$ $1$ is sent with probability $0.4$ $2$ is sent with probability $0.3$ Due to noise, $0$ is changed to $1$ during transmission with probability $0.2$ Due to noise, $0$ is changed to $2$ during transmission with probability $0.1$ Due to noise, $1$ is changed to $0$ during transmission with probability $0.2$ Due to noise, $1$ is changed to $2$ during transmission with probability $0.1$ Due to noise, $2$ is changed to $0$ during transmission with probability $0.2$ Due to noise, $2$ is changed to $1$ during transmission with probability $0.1$ Let A - the event that $\text{1 is received}$ , B - the event that $\text{1 is sent}$ I must find $P(B|A)$ : $$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$$ $P(B)$ = 0.4 $P(A|B)$ = P(1 is sent) $\times$ P(1 does not change ) P(1 does not change ) = [1 - P(1 becomes 0)] $\times$ [1 - P(1 becomes 2)] (right?) (1-0.2)(1-0.1) = 0.72 P(1 is sent) $\times$ 0.72 = (0.4)(0.72) = 0.288 P(A) = P(1 is sent and it stays 1) + P(0 is sent and it changes to 1) + P(2 is sent and it changes to 1) P(A) = 0.288 + (0.3)(0.2) + (0.3)(0.1) = 0.378 Final answer: $$\frac{0.288 \times 0.4}{0.378} = 0.30$$ Has this been done correctly?","I have the following information: is sent with probability is sent with probability is sent with probability Due to noise, is changed to during transmission with probability Due to noise, is changed to during transmission with probability Due to noise, is changed to during transmission with probability Due to noise, is changed to during transmission with probability Due to noise, is changed to during transmission with probability Due to noise, is changed to during transmission with probability Let A - the event that , B - the event that I must find : = 0.4 = P(1 is sent) P(1 does not change ) P(1 does not change ) = [1 - P(1 becomes 0)] [1 - P(1 becomes 2)] (right?) (1-0.2)(1-0.1) = 0.72 P(1 is sent) 0.72 = (0.4)(0.72) = 0.288 P(A) = P(1 is sent and it stays 1) + P(0 is sent and it changes to 1) + P(2 is sent and it changes to 1) P(A) = 0.288 + (0.3)(0.2) + (0.3)(0.1) = 0.378 Final answer: Has this been done correctly?",0 0.3 1 0.4 2 0.3 0 1 0.2 0 2 0.1 1 0 0.2 1 2 0.1 2 0 0.2 2 1 0.1 \text{1 is received} \text{1 is sent} P(B|A) P(B|A) = \frac{P(A|B)P(B)}{P(A)} P(B) P(A|B) \times \times \times \frac{0.288 \times 0.4}{0.378} = 0.30,"['probability', 'combinatorics', 'discrete-mathematics']"
48,prove that Doléans-Dade exponential is a local martingale,prove that Doléans-Dade exponential is a local martingale,,I want to prove that $Z_t$ the Doléans-Dade exponential is a local martingale i.e. that there exists a stopping time $\tau_n$ tending to infinity such that the stopping process $\mathbb{1}_{\tau_n>0}Z_{\tau_n}$ is a martingale. I don't know how to start and how to prove the existence of such stopping time ! any hint or help will be appreciated.. thank you for your time :) Doléans-Dade exponential: $$ Z_t=e^{-\int_0^t\beta_s dW_s-\frac{1}{2}\int_0^t \beta_s^2 ds} $$ where $W_t$ is a Brownian motion and $\beta_t$ is stochastic process.,I want to prove that $Z_t$ the Doléans-Dade exponential is a local martingale i.e. that there exists a stopping time $\tau_n$ tending to infinity such that the stopping process $\mathbb{1}_{\tau_n>0}Z_{\tau_n}$ is a martingale. I don't know how to start and how to prove the existence of such stopping time ! any hint or help will be appreciated.. thank you for your time :) Doléans-Dade exponential: $$ Z_t=e^{-\int_0^t\beta_s dW_s-\frac{1}{2}\int_0^t \beta_s^2 ds} $$ where $W_t$ is a Brownian motion and $\beta_t$ is stochastic process.,,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
49,Find the probability the largest number appearing on the coupon is 9.,Find the probability the largest number appearing on the coupon is 9.,,"15 coupons are numbered 1,2,3...15. 7 coupons are selected at random one at a time with replacement. Find probability that 9 is the largest number appearing on the coupon. I am having a problem finding $N(a)$ for this problem.  My working: $N(s)= 15^7$ (since you have 15 choices for each time you pick up a coupon) $N(a)= 9^7$  (since you have 9 permissible choices for each time you pickup a  coupon) But according to my textbook $N(a)= 9^7 -8^7$  but no further explanation is given.  So i would really appreciate it if someone could point out where i am going wrong. Thanks in advance :)","15 coupons are numbered 1,2,3...15. 7 coupons are selected at random one at a time with replacement. Find probability that 9 is the largest number appearing on the coupon. I am having a problem finding $N(a)$ for this problem.  My working: $N(s)= 15^7$ (since you have 15 choices for each time you pick up a coupon) $N(a)= 9^7$  (since you have 9 permissible choices for each time you pickup a  coupon) But according to my textbook $N(a)= 9^7 -8^7$  but no further explanation is given.  So i would really appreciate it if someone could point out where i am going wrong. Thanks in advance :)",,['probability']
50,Distribution of the sum of $N$ loaded dice rolls,Distribution of the sum of  loaded dice rolls,N,"I would like to calculate the probability distribution of the sum of all the faces of $N$ dice rolls. The face probabilities ${p_i}$ are know, but are not $1 \over 6$. I have found answers for the case of a fair dice (i.e. $p_i={1 \over 6}$) here and here For large $N$ I could apply the central limit theorem and use a normal distribution, but I don't know how to proceed for small $N$. (In particular, $N=2,4, 20$)","I would like to calculate the probability distribution of the sum of all the faces of $N$ dice rolls. The face probabilities ${p_i}$ are know, but are not $1 \over 6$. I have found answers for the case of a fair dice (i.e. $p_i={1 \over 6}$) here and here For large $N$ I could apply the central limit theorem and use a normal distribution, but I don't know how to proceed for small $N$. (In particular, $N=2,4, 20$)",,"['probability', 'probability-distributions', 'integer-partitions', 'dice']"
51,"Arranging problem: 4 couples, 8 seats in a row... Am I making this too simple?","Arranging problem: 4 couples, 8 seats in a row... Am I making this too simple?",,I am in a prob and stats course... haven't taken one in awhile and would like some help on these two problems.  I think I am probably making these a little two simple. Four married couples have bought 8 seats in a row for a concert.  In how many ways can they be seated if: a. if each couple sit together? 4!2!2!2!2!= 384 b. if all men sit together? I am thinking of this M1M2M3M4W1(Any women)(Any Women)(Any Women)(Any Women) so 4*3*2*1*4*3*2*1= 576,I am in a prob and stats course... haven't taken one in awhile and would like some help on these two problems.  I think I am probably making these a little two simple. Four married couples have bought 8 seats in a row for a concert.  In how many ways can they be seated if: a. if each couple sit together? 4!2!2!2!2!= 384 b. if all men sit together? I am thinking of this M1M2M3M4W1(Any women)(Any Women)(Any Women)(Any Women) so 4*3*2*1*4*3*2*1= 576,,['probability']
52,What is the probability of losing in the Taiwainese IMO team's game?,What is the probability of losing in the Taiwainese IMO team's game?,,"Evan Chen's recount of the Taiwanese IMO team's journey recorded a game the team members played at their free time, which runs as the following: There are $n$ team members (in the actual case $n=6$ but here we simply take $n\geq2$)Every team member points at another team member (whom must be different from him/herself) and thus we obtain a (directed) graph with $n$ vertices and $n$ edges. It has one more edge than a tree and therefore must contain a cycle. Any member who is a vertex of any cycle in this graph loses the game. (So it is possible that everyone loses but it's impossible that no one loses.) Assume everyone chooses the person s/he points at randomly, what is the probability of a player losing the game? Reference: Evan Chen's recount","Evan Chen's recount of the Taiwanese IMO team's journey recorded a game the team members played at their free time, which runs as the following: There are $n$ team members (in the actual case $n=6$ but here we simply take $n\geq2$)Every team member points at another team member (whom must be different from him/herself) and thus we obtain a (directed) graph with $n$ vertices and $n$ edges. It has one more edge than a tree and therefore must contain a cycle. Any member who is a vertex of any cycle in this graph loses the game. (So it is possible that everyone loses but it's impossible that no one loses.) Assume everyone chooses the person s/he points at randomly, what is the probability of a player losing the game? Reference: Evan Chen's recount",,"['probability', 'graph-theory']"
53,Uniform distribution on unit disk,Uniform distribution on unit disk,,"Let $(X, Y)$ be a random point chosen according to the uniform distribution in the disk of radius 1 centered at the origin. Compute the densities of $X$ and of $Y$. I know that the joint density of $X$ and $Y$ is $\frac{1}{\pi}$ since when we integrate $\frac{1}{\pi}$ over the unit circle, we get $1$. So if I wanted to find the density of $X$, I was thinking of finding the cumulative distribution of $X$ and the differentiate it to get its density. In order to get its cumulative distribution function, I was going to use the fact that $P(X<x)=P(X<x, -\infty < Y < \infty)$, but this integral doesn't seem nice to work with. Am I on the right track or is there a better way?","Let $(X, Y)$ be a random point chosen according to the uniform distribution in the disk of radius 1 centered at the origin. Compute the densities of $X$ and of $Y$. I know that the joint density of $X$ and $Y$ is $\frac{1}{\pi}$ since when we integrate $\frac{1}{\pi}$ over the unit circle, we get $1$. So if I wanted to find the density of $X$, I was thinking of finding the cumulative distribution of $X$ and the differentiate it to get its density. In order to get its cumulative distribution function, I was going to use the fact that $P(X<x)=P(X<x, -\infty < Y < \infty)$, but this integral doesn't seem nice to work with. Am I on the right track or is there a better way?",,"['probability', 'probability-distributions']"
54,Norm of covariance and precision matrices: is there any meaning?,Norm of covariance and precision matrices: is there any meaning?,,"Let $\Sigma$ be a covariance matrix of some distribution. Then $\Sigma^{-1}$ is the precision matrix. Question: Does $\|\Sigma\|$ or $\|\Sigma^{-1}\|$ have any meaning (for any norm, though I ask in particular for the spectral norm).?","Let $\Sigma$ be a covariance matrix of some distribution. Then $\Sigma^{-1}$ is the precision matrix. Question: Does $\|\Sigma\|$ or $\|\Sigma^{-1}\|$ have any meaning (for any norm, though I ask in particular for the spectral norm).?",,['probability']
55,How to solve 0.5 choose 4?,How to solve 0.5 choose 4?,,"I was solving this problem for homework. It says, in the problem, that if n is positive you use the generalized definition of binomial coefficients. In my case, n is positive so I just plugged n= 0.5 and r=4 into the equation n!/r!(n-r)!. However, now I'm having issues solving (n-r)! because I'm having to take the factorial of a negative number. Can someone just explain how I would go about solving this part?","I was solving this problem for homework. It says, in the problem, that if n is positive you use the generalized definition of binomial coefficients. In my case, n is positive so I just plugged n= 0.5 and r=4 into the equation n!/r!(n-r)!. However, now I'm having issues solving (n-r)! because I'm having to take the factorial of a negative number. Can someone just explain how I would go about solving this part?",,"['probability', 'binomial-coefficients']"
56,Rayleigh and Exponential distributions?,Rayleigh and Exponential distributions?,,If $X$ is Rayleigh distributed random variable. What is the distribution of $|X|^2$? If $X$ is Exponential distributed random variable. What is the distribution of $|X|^2$?,If $X$ is Rayleigh distributed random variable. What is the distribution of $|X|^2$? If $X$ is Exponential distributed random variable. What is the distribution of $|X|^2$?,,['probability']
57,"50/50 Joker of ""Who wants to be a Millionaire"" - A ""Monty Hall Problem"" variation?","50/50 Joker of ""Who wants to be a Millionaire"" - A ""Monty Hall Problem"" variation?",,"So the Monty Hall Problem itself is widely known and understood. Nonetheless, a friend of mine and I were wondering whether the the same strategy could affectively be applied by a participant of Who wants to be a Millionaire? when using the 50/50 Joker . Let's imagine the following scenario:  The participant P has no clue about the correct answer $ x \in \{A,B,C,D\} $ and wants to use the 50/50 Joker (eliminating two wrong answers). But instead of immediately going for it he first ""preselects"" one of the answers in his mind. There is no need to tell Quizmaster Q about his ""imaginary preselection"". Now P tells Q that he wants to use his joker and Q lets the computer eliminate two wrong answers. (1) In case  the answer P had preselected is eliminated he has no choice but to choose between the remaining two answers, effectively leaving him with a 50% chance of success - no magic happening here. (2) But what about the other case  when the answer P had preselected survives the elimination? According to the Monty Hall Problem it seems as if changing the selection (i.e. choosing the other remaining option P had not preseleted) seems to give him a 0.75 chance of success. Nevertheless, I find it hard to believe that this actually holds true, since the so called 50/50 (!) Joker would then not be p(success) = 0.5 after all. Additionally it seems unlikely that making an ""imaginary preselection"", no one else is told about, actually increases your odds. I know this problem is not exactly the same as Monty Hall since the the quizmaster does not always eliminate answers only from the ones the participant had not ""preselected"", meaning that the preselection itself could be eliminated, too, as it happens in (1) . Still the second case seems to actually be a just variation of it. So are we right and making a preselection and then going for the other remaining option is a valid strategy that increases the participant's odds of winning? If not, please help us understand our misconception.","So the Monty Hall Problem itself is widely known and understood. Nonetheless, a friend of mine and I were wondering whether the the same strategy could affectively be applied by a participant of Who wants to be a Millionaire? when using the 50/50 Joker . Let's imagine the following scenario:  The participant P has no clue about the correct answer $ x \in \{A,B,C,D\} $ and wants to use the 50/50 Joker (eliminating two wrong answers). But instead of immediately going for it he first ""preselects"" one of the answers in his mind. There is no need to tell Quizmaster Q about his ""imaginary preselection"". Now P tells Q that he wants to use his joker and Q lets the computer eliminate two wrong answers. (1) In case  the answer P had preselected is eliminated he has no choice but to choose between the remaining two answers, effectively leaving him with a 50% chance of success - no magic happening here. (2) But what about the other case  when the answer P had preselected survives the elimination? According to the Monty Hall Problem it seems as if changing the selection (i.e. choosing the other remaining option P had not preseleted) seems to give him a 0.75 chance of success. Nevertheless, I find it hard to believe that this actually holds true, since the so called 50/50 (!) Joker would then not be p(success) = 0.5 after all. Additionally it seems unlikely that making an ""imaginary preselection"", no one else is told about, actually increases your odds. I know this problem is not exactly the same as Monty Hall since the the quizmaster does not always eliminate answers only from the ones the participant had not ""preselected"", meaning that the preselection itself could be eliminated, too, as it happens in (1) . Still the second case seems to actually be a just variation of it. So are we right and making a preselection and then going for the other remaining option is a valid strategy that increases the participant's odds of winning? If not, please help us understand our misconception.",,"['probability', 'monty-hall']"
58,Recursion with Random number?,Recursion with Random number?,,"function foo(n)        if n = 1 then           return 1        else           return foo(rand(1, n))        end if      end function If foo is initially called with m as the parameter, what is the expected number times that rand() would be called ? BTW, rand(1,n) returns a uniformly distributed random integer in the range 1 to n .","function foo(n)        if n = 1 then           return 1        else           return foo(rand(1, n))        end if      end function If foo is initially called with m as the parameter, what is the expected number times that rand() would be called ? BTW, rand(1,n) returns a uniformly distributed random integer in the range 1 to n .",,"['probability', 'random', 'recursive-algorithms']"
59,Please explain to me why the Expected Value is $ E[X] = \int_{-\infty}^{\infty} x f_X(x) dx $,Please explain to me why the Expected Value is, E[X] = \int_{-\infty}^{\infty} x f_X(x) dx ,"For probability density functions (at least for the normal distribution and beta distribution) it holds that the expected value is given by $ E[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx $. I have to solve this for a homework for the beta distribution. Finding a solution on the internet wasn't that hard but I wouldn't mind understanding this calculation. I'd like to know first why $ f_X(x) $ is multiplied by $ x $. The same thing is done if the variance gets calculated - but in this case the function gets multiplied by $ x^2 $. So why is this the case? The other thing is that I don't even understand why this function gets integrated and not derived e.g.? Could anyone explain this to me? I am not looking for a solution, just an explaination. Thank you.","For probability density functions (at least for the normal distribution and beta distribution) it holds that the expected value is given by $ E[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx $. I have to solve this for a homework for the beta distribution. Finding a solution on the internet wasn't that hard but I wouldn't mind understanding this calculation. I'd like to know first why $ f_X(x) $ is multiplied by $ x $. The same thing is done if the variance gets calculated - but in this case the function gets multiplied by $ x^2 $. So why is this the case? The other thing is that I don't even understand why this function gets integrated and not derived e.g.? Could anyone explain this to me? I am not looking for a solution, just an explaination. Thank you.",,"['probability', 'integration', 'probability-distributions']"
60,Estimating the average number of passengers in cars in a parking lot.,Estimating the average number of passengers in cars in a parking lot.,,"All the workers at a certain company drive to work and park in the company’s lot. The company is interested in estimating the average number of workers in a car. Which of the following methods will enable the company to estimate this quantity? Randomly choose $n$ workers, find out how many were in the cars in which they were driven, and take the average of the $n$ values. Randomly choose $n$ cars in the lot, find out how many were driven in those cars, and take the average of the $n$ values. My intuition goes for number 2, but I'm not able to justify it formally.","All the workers at a certain company drive to work and park in the company’s lot. The company is interested in estimating the average number of workers in a car. Which of the following methods will enable the company to estimate this quantity? Randomly choose $n$ workers, find out how many were in the cars in which they were driven, and take the average of the $n$ values. Randomly choose $n$ cars in the lot, find out how many were driven in those cars, and take the average of the $n$ values. My intuition goes for number 2, but I'm not able to justify it formally.",,['probability']
61,Expected squared prediction error,Expected squared prediction error,,"I'm reading about statistical decision theory and on one point in my book the author defines the expected squared prediction error by: $$EPE = E(Y-g(X))^2 = \int(y -g(x))^2Pr(dx, dy)$$ I like to write this with the density function so that it stays more precise: $$EPE = \int\int(y-g(x))^2f(x,y)\;dx\;dy$$ Now on the other part the author says that by conditioning on $X$, $EPE$ can be written as: $$EPE = E_XE_{Y|X}([Y-g(X)]^2\;|\;X)$$ For some reason this notation confuses me...could someone write this conditional notation of $EPE$ more precisely, i.e. so that it would include the joint density function of random variables $X$ and $Y$ etc.? Just to be sure: $X$ is the variable we use to predict $Y$ and $g(X)$ is the function we are trying to solve, which minimizes $EPE$. Thank you for any help :)","I'm reading about statistical decision theory and on one point in my book the author defines the expected squared prediction error by: $$EPE = E(Y-g(X))^2 = \int(y -g(x))^2Pr(dx, dy)$$ I like to write this with the density function so that it stays more precise: $$EPE = \int\int(y-g(x))^2f(x,y)\;dx\;dy$$ Now on the other part the author says that by conditioning on $X$, $EPE$ can be written as: $$EPE = E_XE_{Y|X}([Y-g(X)]^2\;|\;X)$$ For some reason this notation confuses me...could someone write this conditional notation of $EPE$ more precisely, i.e. so that it would include the joint density function of random variables $X$ and $Y$ etc.? Just to be sure: $X$ is the variable we use to predict $Y$ and $g(X)$ is the function we are trying to solve, which minimizes $EPE$. Thank you for any help :)",,"['probability', 'statistics', 'conditional-probability']"
62,Probabilistic riddle [duplicate],Probabilistic riddle [duplicate],,"This question already has answers here : Multiple-choice question about the probability of a random answer to itself being correct (6 answers) Closed 6 years ago . If you choose an answer to this question at random, then what is the chance you will be correct? A) 25% B) 50% C) 60% D) 25% On internet you can find the problem here .","This question already has answers here : Multiple-choice question about the probability of a random answer to itself being correct (6 answers) Closed 6 years ago . If you choose an answer to this question at random, then what is the chance you will be correct? A) 25% B) 50% C) 60% D) 25% On internet you can find the problem here .",,"['probability', 'puzzle']"
63,Probability of sampling with and without replacement [duplicate],Probability of sampling with and without replacement [duplicate],,This question already has an answer here : What is the probability of recapturing a specified number of tagged elk? [closed] (1 answer) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved In sampling without replacement the probability of any fixed element in the population to be included in a random sample of size $r$ is $\frac{r}{n}$. In sampling with replacement the corresponding probability is $\left[1- \left(\frac{1}{1-n}\right)^r\right]$. Please help me show how this is proved.,This question already has an answer here : What is the probability of recapturing a specified number of tagged elk? [closed] (1 answer) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved In sampling without replacement the probability of any fixed element in the population to be included in a random sample of size $r$ is $\frac{r}{n}$. In sampling with replacement the corresponding probability is $\left[1- \left(\frac{1}{1-n}\right)^r\right]$. Please help me show how this is proved.,,"['probability', 'combinatorics', 'probability-distributions']"
64,What is the expected number of swaps performed by Bubblesort?,What is the expected number of swaps performed by Bubblesort?,,"The well-known Bubblesort algorithm sorts a list $a_1, a_2, . . . , a_n$ of numbers by repeatedly swapping adjacent numbers that are inverted (i.e., in the wrong relative order) until there are no remaining inversions. (Note that the number of swaps required does not depend on the order in which the swaps are made.) Suppose that the input to Bubblesort is a random permutation of the numbers $a_1, a_2, . . . , a_n$ , so that all $n!$ orderings are equally likely, and that all the numbers are distinct. What is the expected number of swaps performed by Bubblesort?","The well-known Bubblesort algorithm sorts a list $a_1, a_2, . . . , a_n$ of numbers by repeatedly swapping adjacent numbers that are inverted (i.e., in the wrong relative order) until there are no remaining inversions. (Note that the number of swaps required does not depend on the order in which the swaps are made.) Suppose that the input to Bubblesort is a random permutation of the numbers $a_1, a_2, . . . , a_n$ , so that all $n!$ orderings are equally likely, and that all the numbers are distinct. What is the expected number of swaps performed by Bubblesort?",,"['probability', 'discrete-mathematics', 'algorithms']"
65,Why is the Expected Value different from the number needed for 50% chance of success?,Why is the Expected Value different from the number needed for 50% chance of success?,,"An event with probability $p$ of being success is executed $\frac{1}{p}$ times. For example, if $p=5\%$, the event would then be executed $20$ times. The Expected Value for the total number of trials needed to get one success is $\frac{1}{p}$. In this case, it's $20$. What I'm confused is, as p approaches zero, the chance of having a success in the first $\frac{1}{p}$ trials always approaches to $1-\frac{1}{e}$, or about $63\%$. This means: $P$(at least $1$ success in all $\frac{1}{p}$ trials) is about $63\%$. This $63\%$ is higher than $50\%$. It seems to suggest that, if I take all $\frac{1}{p}$ trials and consider them as one big event, and do this big event multiple times, I'd get more successes than failures. But on the other hand, since $\frac{1}{p}$ times is the EV mentioned earlier, shouldn't the big event have an equal chance of being a success or a failure?","An event with probability $p$ of being success is executed $\frac{1}{p}$ times. For example, if $p=5\%$, the event would then be executed $20$ times. The Expected Value for the total number of trials needed to get one success is $\frac{1}{p}$. In this case, it's $20$. What I'm confused is, as p approaches zero, the chance of having a success in the first $\frac{1}{p}$ trials always approaches to $1-\frac{1}{e}$, or about $63\%$. This means: $P$(at least $1$ success in all $\frac{1}{p}$ trials) is about $63\%$. This $63\%$ is higher than $50\%$. It seems to suggest that, if I take all $\frac{1}{p}$ trials and consider them as one big event, and do this big event multiple times, I'd get more successes than failures. But on the other hand, since $\frac{1}{p}$ times is the EV mentioned earlier, shouldn't the big event have an equal chance of being a success or a failure?",,['probability']
66,Probability distribution of the subinterval lengths from a random interval division,Probability distribution of the subinterval lengths from a random interval division,,"For $a \in \mathbb{R}_{+}$ and $n \in \mathbb{N}_{+}$ draw $n-1$ points $X_1, \ldots, X_{n-1}$ independently, uniformly at random from the interval $I = [0, a]$. These points partition $I$ into $n$ disjoint subintervals $I_1 \, \dot\cup \ldots \dot\cup \, I_n = I$. Let $Y_i := |I_i| \in [0, a]$ denote the interval length of the $i$'th subinterval. How are the $Y_i$'s distributed? I am especially interested in the expectation and the variance. Here are my thoughts about this: I suppose that all $Y_i$'s are identically distributed, i.e., $Y_i \sim Y$ for some random variable $Y$. Further, I suppose that the expectation of $Y$ is $\mathbb{E}(Y) = \frac{a}{n}$. However, I have no clue about the variance, and I can neither prove my conjectures, nor find an answer on the internet. Can you help me on this?","For $a \in \mathbb{R}_{+}$ and $n \in \mathbb{N}_{+}$ draw $n-1$ points $X_1, \ldots, X_{n-1}$ independently, uniformly at random from the interval $I = [0, a]$. These points partition $I$ into $n$ disjoint subintervals $I_1 \, \dot\cup \ldots \dot\cup \, I_n = I$. Let $Y_i := |I_i| \in [0, a]$ denote the interval length of the $i$'th subinterval. How are the $Y_i$'s distributed? I am especially interested in the expectation and the variance. Here are my thoughts about this: I suppose that all $Y_i$'s are identically distributed, i.e., $Y_i \sim Y$ for some random variable $Y$. Further, I suppose that the expectation of $Y$ is $\mathbb{E}(Y) = \frac{a}{n}$. However, I have no clue about the variance, and I can neither prove my conjectures, nor find an answer on the internet. Can you help me on this?",,"['probability', 'probability-distributions']"
67,Expectation of an exponential function,Expectation of an exponential function,,"Why is the expectation of an exponential function:  $$\mathbb{E}[\exp(A x)] = \exp((1/2) A^2)\,?$$ I am struggling to find references that shows this, can anyone help me please? If anyone could enlighten me it would be great!","Why is the expectation of an exponential function:  $$\mathbb{E}[\exp(A x)] = \exp((1/2) A^2)\,?$$ I am struggling to find references that shows this, can anyone help me please? If anyone could enlighten me it would be great!",,['probability']
68,Question on probability using Bayes' theorem,Question on probability using Bayes' theorem,,"In a certain day care class, $30\%$ of the children have grey eyes, $50\%$ of them have blue and the other $20\%$'s eyes are in other colors. One day they play a game together. In the first run, $65\%$ of the grey eye ones, $82\%$ of the blue eyed ones and $50\%$ of the children with other eye color were selected. Now, if a child is selected randomly from the class, and we know that he/she was not in the first game, what is the probability that the child has blue eyes? My solution Let's say $B =$ blue, $G =$ grey and $O =$ ""Other color"" and $NR =$ ""not selected for the first run"" $$P(B \mid NR) = \frac{P(NR \mid B)P(B)}{P(G)P(NR \mid G) + P(B)P(NR \mid B) + P(O)P(NR \mid O)}$$ On substituting values $$P(B \mid NR) = \frac{0.5 \cdot (1-0.82)}{(0.3 \cdot (1-0.65)) + (0.5 \cdot (1-0.82)) + (0.2 \cdot (1-0.5))}$$ $$P(B \mid NR) = 0.305$$ Is this the right way to use bayes theorem?","In a certain day care class, $30\%$ of the children have grey eyes, $50\%$ of them have blue and the other $20\%$'s eyes are in other colors. One day they play a game together. In the first run, $65\%$ of the grey eye ones, $82\%$ of the blue eyed ones and $50\%$ of the children with other eye color were selected. Now, if a child is selected randomly from the class, and we know that he/she was not in the first game, what is the probability that the child has blue eyes? My solution Let's say $B =$ blue, $G =$ grey and $O =$ ""Other color"" and $NR =$ ""not selected for the first run"" $$P(B \mid NR) = \frac{P(NR \mid B)P(B)}{P(G)P(NR \mid G) + P(B)P(NR \mid B) + P(O)P(NR \mid O)}$$ On substituting values $$P(B \mid NR) = \frac{0.5 \cdot (1-0.82)}{(0.3 \cdot (1-0.65)) + (0.5 \cdot (1-0.82)) + (0.2 \cdot (1-0.5))}$$ $$P(B \mid NR) = 0.305$$ Is this the right way to use bayes theorem?",,"['probability', 'bayes-theorem']"
69,How do I find the cumulative distribution function of a binomial random variable?,How do I find the cumulative distribution function of a binomial random variable?,,How would I find the cumulative distribution function of a binomial? I know I'd have to integrate it with its given parameters but how would someone go about doing that?,How would I find the cumulative distribution function of a binomial? I know I'd have to integrate it with its given parameters but how would someone go about doing that?,,"['probability', 'probability-theory']"
70,Basic probability problem,Basic probability problem,,"Problem states: Consider two events $A$ and $B$, with $P(A) = 0.4$ and $Pr(B) = 0.7$. Determine the maximum and the minimum possible values for $P(A \& B)$ and the conditions under which each of these values is attained. To solve, I considered the event with the lowest probability $A$ to be a subset of the other, so maximum value is attained under that circumstance giving a probability of $0.4$. But the book states that the minimum is $0.1$, if $P(A \cup B) = 1$. I don't understand why! Because I thought that the minimum value is get when the two events are disjoint... So the minimum value must be $0$...","Problem states: Consider two events $A$ and $B$, with $P(A) = 0.4$ and $Pr(B) = 0.7$. Determine the maximum and the minimum possible values for $P(A \& B)$ and the conditions under which each of these values is attained. To solve, I considered the event with the lowest probability $A$ to be a subset of the other, so maximum value is attained under that circumstance giving a probability of $0.4$. But the book states that the minimum is $0.1$, if $P(A \cup B) = 1$. I don't understand why! Because I thought that the minimum value is get when the two events are disjoint... So the minimum value must be $0$...",,['probability']
71,Correlation Coefficient between these two random variables,Correlation Coefficient between these two random variables,,Suppose that $X$ is real-valued normal random variable with mean $\mu$ and variance $\sigma^2$. What is the correlation coefficient between $X$ and $X^2$?,Suppose that $X$ is real-valued normal random variable with mean $\mu$ and variance $\sigma^2$. What is the correlation coefficient between $X$ and $X^2$?,,['probability']
72,What is the motivation for the definition of the expected value?,What is the motivation for the definition of the expected value?,,"I have a general question about expected values: For a discrete random variable, $$E[X] = \sum_{i=1}^{\infty} x_{i}p_{i}$$ and $$E[X] = \int_{-\infty}^{\infty} xp(x) \ dx$$ for a continuous random variable $X$. But what is the motivation for these definitions? Is it essentially defined because of the following: Suppose I perform an experiment a large number of times and record the results. I then take the average of the results. I want to find what this average approaches as the number of trials increases. Thus by trial and error I find that the definition of $E(X)$ works. This is assuming I am taking a frequentist view of probability. What does this all have to do with computing the area under some function?","I have a general question about expected values: For a discrete random variable, $$E[X] = \sum_{i=1}^{\infty} x_{i}p_{i}$$ and $$E[X] = \int_{-\infty}^{\infty} xp(x) \ dx$$ for a continuous random variable $X$. But what is the motivation for these definitions? Is it essentially defined because of the following: Suppose I perform an experiment a large number of times and record the results. I then take the average of the results. I want to find what this average approaches as the number of trials increases. Thus by trial and error I find that the definition of $E(X)$ works. This is assuming I am taking a frequentist view of probability. What does this all have to do with computing the area under some function?",,"['probability', 'soft-question']"
73,Average time for a tetris field to fill,Average time for a tetris field to fill,,"I have a tetris field, infinitely wide (irrelevant) and N blocks high. Now random blocks start falling down, and I do not move them. Given that the field is N blocks high, the blocks aren't moved, the blocks fall one block per second and the blocks are the default blocks (they always ""spawn"" with the same orientation), how long does it take on average before the game is over?","I have a tetris field, infinitely wide (irrelevant) and N blocks high. Now random blocks start falling down, and I do not move them. Given that the field is N blocks high, the blocks aren't moved, the blocks fall one block per second and the blocks are the default blocks (they always ""spawn"" with the same orientation), how long does it take on average before the game is over?",,['probability']
74,Problem regarding coloring balls drawn from a bin,Problem regarding coloring balls drawn from a bin,,"I'm not sure if this has been asked before, but here I have the following problem: There are $n$ indistinguishable red balls in a bin. Each ""round,"" $k$ balls are randomly chosen from the bin with equal probability. All $k$ balls are colored blue and put back into the bin (this means that balls that are already blue are not changed). What is the expected number of rounds needed to color all $n$ balls blue? Is there an explicit formula for this problem? I've thought about this problem for a bit and quickly got stumped, but I realized that unless there was some insight for the problem, a recursive formula of some sort is probably needed as each round relies on the results of the previous rounds.  If the formula turns out to be recursive, is there some relatively ""simple"" heuristic that can estimate the expected number of rounds? There's also the tricky case of infinitely drawing blue balls after initially drawing some red balls, which would result in an infinite number of rounds. But my gut feeling says that the probability of this happening are small enough that the expected number of rounds should be finite. After searching a bit online, this seems to be (correct me if I'm wrong) the Coupon collector's problem but multiple coupons are drawn at once  instead of just one at a time, and that each of the coupons drawn are all distinct.  If that's the case, can any of the insights from that problem be used in the problem I have currently?","I'm not sure if this has been asked before, but here I have the following problem: There are indistinguishable red balls in a bin. Each ""round,"" balls are randomly chosen from the bin with equal probability. All balls are colored blue and put back into the bin (this means that balls that are already blue are not changed). What is the expected number of rounds needed to color all balls blue? Is there an explicit formula for this problem? I've thought about this problem for a bit and quickly got stumped, but I realized that unless there was some insight for the problem, a recursive formula of some sort is probably needed as each round relies on the results of the previous rounds.  If the formula turns out to be recursive, is there some relatively ""simple"" heuristic that can estimate the expected number of rounds? There's also the tricky case of infinitely drawing blue balls after initially drawing some red balls, which would result in an infinite number of rounds. But my gut feeling says that the probability of this happening are small enough that the expected number of rounds should be finite. After searching a bit online, this seems to be (correct me if I'm wrong) the Coupon collector's problem but multiple coupons are drawn at once  instead of just one at a time, and that each of the coupons drawn are all distinct.  If that's the case, can any of the insights from that problem be used in the problem I have currently?",n k k n,"['probability', 'expected-value', 'recreational-mathematics', 'balls-in-bins', 'coupon-collector']"
75,Probability to get 170 1's,Probability to get 170 1's,,"Suppose we throw a fair die 1000 times. What is the probability we get 170 1's? The exact number is $P={1000\choose 170}\bigg(\frac{1}{6}\bigg)^{170}\bigg(\frac{5}{6}\bigg)^{830}$ . We can approximate with normal distribution because we have independent events, a large and constant number of trials and an a-priori known probability with only two outcomes, success (get an 1) or failure (get anything else). We have $μ = np = 1000 \cdot\frac{1}{6} = 166.66$ and $σ = \sqrt npq = 11.785$ . $z = \frac{170 - 166.66}{11.785} = 0.2828$ Hence the probability is 0.6103. Is it correct? Thank you very much.","Suppose we throw a fair die 1000 times. What is the probability we get 170 1's? The exact number is . We can approximate with normal distribution because we have independent events, a large and constant number of trials and an a-priori known probability with only two outcomes, success (get an 1) or failure (get anything else). We have and . Hence the probability is 0.6103. Is it correct? Thank you very much.",P={1000\choose 170}\bigg(\frac{1}{6}\bigg)^{170}\bigg(\frac{5}{6}\bigg)^{830} μ = np = 1000 \cdot\frac{1}{6} = 166.66 σ = \sqrt npq = 11.785 z = \frac{170 - 166.66}{11.785} = 0.2828,"['probability', 'normal-distribution']"
76,"Abuse of notation when we denote $\mathcal{F}_n $-measurability of $X_n$ by ""$X_n \in \mathcal{F}_n$""?","Abuse of notation when we denote -measurability of  by """"?",\mathcal{F}_n  X_n X_n \in \mathcal{F}_n,"I am confused about a notation that I am seeing in many probability texts when authors write $X_n \in \mathcal{F}_n$ to express that $X_n$ is $\mathcal{F}_n$ -measurable. The elements in $\mathcal{F}_n$ are subsets of $\Omega$ , our probability space, so it does not make any sense for $X_n$ to be an element of $\mathcal{F}_n$ ... If anything, what we should write is $X^{-1}_n(\omega) \in \mathcal{F}_n$ for every $\omega \in \Omega$ to denote $\mathcal{F}_n$ -measurability. So my question is, do authors just write $X_n \in \mathcal{F}_n$ as some sort of shorthand convenience/abuse of notation, or am I missing something deeper here? Thanks in advance!","I am confused about a notation that I am seeing in many probability texts when authors write to express that is -measurable. The elements in are subsets of , our probability space, so it does not make any sense for to be an element of ... If anything, what we should write is for every to denote -measurability. So my question is, do authors just write as some sort of shorthand convenience/abuse of notation, or am I missing something deeper here? Thanks in advance!",X_n \in \mathcal{F}_n X_n \mathcal{F}_n \mathcal{F}_n \Omega X_n \mathcal{F}_n X^{-1}_n(\omega) \in \mathcal{F}_n \omega \in \Omega \mathcal{F}_n X_n \in \mathcal{F}_n,"['probability', 'probability-theory', 'measure-theory']"
77,Random variable $X$ is symmetric iff $\mathbb{E}\left(X / \left(1 + r^2X^2\right)\right) = 0$ for any $r \in \mathbb{R}$.,Random variable  is symmetric iff  for any .,X \mathbb{E}\left(X / \left(1 + r^2X^2\right)\right) = 0 r \in \mathbb{R},"Can you prove/disprove the following claim? Let $X$ be a random variable, which takes values in $\mathbb{R}$ . Assume that $\mathbb{E}\left(X / \left(1 + r^2X^2\right)\right)$ is defined and finite for any $r \in \mathbb{R}$ . The density of $X$ is symmetric about $0$ iff $$\mathbb{E}\left(\frac{X}{1 + r^2X^2}\right) = 0$$ for any $r \in \mathbb{R}$ . A couple of observations: If $X$ is symmetric about $0$ then the expectation is $0$ for any $r \in \mathbb{R}$ because $x/(1+r^2x^2)$ is an odd function. If the expectation condition was instead that $\mathbb{E}(g(X)) = 0$ for any odd function $g$ then it would be clear that $X$ must be symmetric, since we could just choose functions of the form $$g_s(x) = \begin{cases} -1 & \text{if } x \in (-s - \epsilon, -s),\\ 1 & \text{if } x \in (s, s + \epsilon),\\ 0 & \text{otherwise} \end{cases}$$ for any $s > 0$ and arbitrarily small $\epsilon$ 's and just check that the density of $X$ is symmetric. Context: I was watching a conference talk about e-values (more info about e-values here) and the speaker claimed (unless I misunderstood) that the following hypotheses are equivalent: $$X \textrm{ is symmetric},$$ and $$\mathbb{E}\left(1 + \frac{rX}{1 + r^2X^2}\right) \leq 1$$ for any $r \in \mathbb{R}$ . The speaker said that the claim can be shown but I couldn't find the proof in any of his citations.","Can you prove/disprove the following claim? Let be a random variable, which takes values in . Assume that is defined and finite for any . The density of is symmetric about iff for any . A couple of observations: If is symmetric about then the expectation is for any because is an odd function. If the expectation condition was instead that for any odd function then it would be clear that must be symmetric, since we could just choose functions of the form for any and arbitrarily small 's and just check that the density of is symmetric. Context: I was watching a conference talk about e-values (more info about e-values here) and the speaker claimed (unless I misunderstood) that the following hypotheses are equivalent: and for any . The speaker said that the claim can be shown but I couldn't find the proof in any of his citations.","X \mathbb{R} \mathbb{E}\left(X / \left(1 + r^2X^2\right)\right) r \in \mathbb{R} X 0 \mathbb{E}\left(\frac{X}{1 + r^2X^2}\right) = 0 r \in \mathbb{R} X 0 0 r \in \mathbb{R} x/(1+r^2x^2) \mathbb{E}(g(X)) = 0 g X g_s(x) = \begin{cases}
-1 & \text{if } x \in (-s - \epsilon, -s),\\
1 & \text{if } x \in (s, s + \epsilon),\\
0 & \text{otherwise}
\end{cases} s > 0 \epsilon X X \textrm{ is symmetric}, \mathbb{E}\left(1 + \frac{rX}{1 + r^2X^2}\right) \leq 1 r \in \mathbb{R}","['probability', 'statistics', 'probability-distributions']"
78,Expected number of cards turned over before seeing first Ace?,Expected number of cards turned over before seeing first Ace?,,"A standard 52-car deck is shuffled, and cards are turned over one-at-a-time starting with the top card. What is the expected number of cards that will be turned over before we see the first Ace? (Recall that there are 4 Aces in the deck.) There's a very clever way to do this. The $4$ aces partition the deck into $5$ components, with size on average ${{52 - 4}\over5} = 9.6$ . We then have to draw the first Ace, so the expected number of cards that'll be turned over before we see it is $9.6 + 1 = 10.6$ . However, for those out there who are stupid like myself (or more generously put, want to practice our computational fortitude), let's do it by brute force. We want to calculate $$1\left({4\over{52}}\right) + 2\left({{48}\over{52}}\right)\left({4\over{51}}\right) + 3 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{4}\over{50}}\right) + 4 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{46}\over{50}}\right)\left({{4}\over{49}}\right) + \ldots + 48\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{4}\over{5}}\right) + 49\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{1}\over{5}}\right)\left({{4}\over{4}}\right) = {4\over{52}} + \sum_{n = 2}^{49}\left(n \left(\prod_{i=1}^{n-1} {{49 - i}\over{53 - i}}\right) \left({4\over{53 - n}} \right)\right)$$ However, I'm not sure how to proceed with evaluating that expression. How on Earth can I get it to evaluate to $10.6$ ?","A standard 52-car deck is shuffled, and cards are turned over one-at-a-time starting with the top card. What is the expected number of cards that will be turned over before we see the first Ace? (Recall that there are 4 Aces in the deck.) There's a very clever way to do this. The aces partition the deck into components, with size on average . We then have to draw the first Ace, so the expected number of cards that'll be turned over before we see it is . However, for those out there who are stupid like myself (or more generously put, want to practice our computational fortitude), let's do it by brute force. We want to calculate However, I'm not sure how to proceed with evaluating that expression. How on Earth can I get it to evaluate to ?",4 5 {{52 - 4}\over5} = 9.6 9.6 + 1 = 10.6 1\left({4\over{52}}\right) + 2\left({{48}\over{52}}\right)\left({4\over{51}}\right) + 3 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{4}\over{50}}\right) + 4 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{46}\over{50}}\right)\left({{4}\over{49}}\right) + \ldots + 48\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{4}\over{5}}\right) + 49\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{1}\over{5}}\right)\left({{4}\over{4}}\right) = {4\over{52}} + \sum_{n = 2}^{49}\left(n \left(\prod_{i=1}^{n-1} {{49 - i}\over{53 - i}}\right) \left({4\over{53 - n}} \right)\right) 10.6,"['probability', 'combinatorics', 'card-games']"
79,What is the intuition behind linearity of expectations not requiring independence?,What is the intuition behind linearity of expectations not requiring independence?,,"I am confused as to the intuition behind the linearity of expectations not requiring events to be independent. Why is this true? I read that since the proof that shows expected values are linear does not use anything regarding independence, independence is not a requirement. I don't quite follow that step. Why would we not need to show that both independent and dependent events have this property? This also leaves me confused with questions regarding this property. For example, Suppose you toss a fair coin 12 times resulting in a sequence of heads (H) and tails (T). Let N be the number of times that the sequence HTHT appears.  For example, HTHT appears twice in HTHTHTTTTTTT. Find E(N) The answer to this problem is 9/16 , which comes from the fact that there is a 1/16 probability that HTHT occurs, starting at index n, with 1 <= n <= 9 , and the answer is 9 * 1/16 . Why is it that we can add the probability that the string HTHT occurs starting at any index? I ask this because say HTHT were to appear in the first four flips, then the probability that HTHT occurs starting at the second index is zero because T was the outcome of the second index. An explanation of the intuition of this property would be appreciated.","I am confused as to the intuition behind the linearity of expectations not requiring events to be independent. Why is this true? I read that since the proof that shows expected values are linear does not use anything regarding independence, independence is not a requirement. I don't quite follow that step. Why would we not need to show that both independent and dependent events have this property? This also leaves me confused with questions regarding this property. For example, Suppose you toss a fair coin 12 times resulting in a sequence of heads (H) and tails (T). Let N be the number of times that the sequence HTHT appears.  For example, HTHT appears twice in HTHTHTTTTTTT. Find E(N) The answer to this problem is 9/16 , which comes from the fact that there is a 1/16 probability that HTHT occurs, starting at index n, with 1 <= n <= 9 , and the answer is 9 * 1/16 . Why is it that we can add the probability that the string HTHT occurs starting at any index? I ask this because say HTHT were to appear in the first four flips, then the probability that HTHT occurs starting at the second index is zero because T was the outcome of the second index. An explanation of the intuition of this property would be appreciated.",,"['probability', 'probability-theory', 'intuition', 'expected-value']"
80,"If given $P(B\mid A) =4/5$, $P(B\mid A^\complement)= 2/5$ and $P(B)= 1/2$, what is the probability of $A$?","If given ,  and , what is the probability of ?",P(B\mid A) =4/5 P(B\mid A^\complement)= 2/5 P(B)= 1/2 A,"If given $P(B\mid A) =4/5$ , $P(B\mid A^\complement)= 2/5$ and $P(B)= 1/2$ , what is the probability of $A$ ? I know I need to apply Bayes theorem here to figure this out, but I'm struggling a bit to understand how. So far I've considered this formula: $$P(B\mid A) = \dfrac{P (B \cap A) }{ P (B \cap A) + P(B^\complement \cap A)}$$ From this formula, I understand that $P(B \cap A) = P(A) \cdot P(B\mid A)$ so I plug in the given values but then only find that $P(B^\complement |A)$ is $2/25$ . But this does not get me any closer to my goal, $P(A)$ . I imagine my understanding of this is quite backward. Any pointers would be helpful. Thank you","If given , and , what is the probability of ? I know I need to apply Bayes theorem here to figure this out, but I'm struggling a bit to understand how. So far I've considered this formula: From this formula, I understand that so I plug in the given values but then only find that is . But this does not get me any closer to my goal, . I imagine my understanding of this is quite backward. Any pointers would be helpful. Thank you",P(B\mid A) =4/5 P(B\mid A^\complement)= 2/5 P(B)= 1/2 A P(B\mid A) = \dfrac{P (B \cap A) }{ P (B \cap A) + P(B^\complement \cap A)} P(B \cap A) = P(A) \cdot P(B\mid A) P(B^\complement |A) 2/25 P(A),"['probability', 'conditional-probability', 'bayes-theorem']"
81,How much should I pay for a chance to win 100$?,How much should I pay for a chance to win 100$?,,"There are $4$ closed doors, with $100\$$ behind one of them. You can pay $X$ to open a door. If the money is there, you can keep it. If not you can pay another $X$ to open the next door, and so on. What is the most I can pay and still win on average? I think that it's $25\$$ . Since the money can be behind any door, you have as much of a chance of getting it on the first door as any other door. Since there are $4$ doors, and equal chance of being behind any door, $100 / 4$ = $25$ .","There are closed doors, with behind one of them. You can pay to open a door. If the money is there, you can keep it. If not you can pay another to open the next door, and so on. What is the most I can pay and still win on average? I think that it's . Since the money can be behind any door, you have as much of a chance of getting it on the first door as any other door. Since there are doors, and equal chance of being behind any door, = .",4 100\ X X 25\ 4 100 / 4 25,['probability']
82,Probability of getting at least one job offer based on interview odds,Probability of getting at least one job offer based on interview odds,,"I have recently interviewed for a number of jobs, and am wondering what the odds are of getting accepted for one based on the odds of each interview and the total number of interviews. I had $7$ interviews which had a $1$ in $10$ chance of securing a job ( $10$ interviewees for every applicant). I had another interview with a $6\%$ chance, another with a $5\%$ chance, and another with a $14\%$ chance. So in total, $10$ places with odds of $10\%$ , $10\%$ , $10\%$ , $10\%$ , $10\%$ , $10\%$ , $10\%$ , $14\%$ , $6\%$ , $5\%$ .  I know that I cannot simply add the numbers, but I am also stumped in that I am trying to find the odds of not one job offer, but a minimum of one offer....so the odds of either one positive return, two positive return, etc. vs the odds of all $10$ interviews coming up negative. Anyone know how to calculate this problem?","I have recently interviewed for a number of jobs, and am wondering what the odds are of getting accepted for one based on the odds of each interview and the total number of interviews. I had interviews which had a in chance of securing a job ( interviewees for every applicant). I had another interview with a chance, another with a chance, and another with a chance. So in total, places with odds of , , , , , , , , , .  I know that I cannot simply add the numbers, but I am also stumped in that I am trying to find the odds of not one job offer, but a minimum of one offer....so the odds of either one positive return, two positive return, etc. vs the odds of all interviews coming up negative. Anyone know how to calculate this problem?",7 1 10 10 6\% 5\% 14\% 10 10\% 10\% 10\% 10\% 10\% 10\% 10\% 14\% 6\% 5\% 10,['probability']
83,On an expected value inequality.,On an expected value inequality.,,"Given $X$ a random variable that takes values on all of $\mathbb{R}$ with associated probability density function $f$ is it true that for all $r > 0$ $$E \left[ \int_{X-r}^{X+r} f(x) dx \right] \ge E \left[ \int_{X-r}^{X+r} g(x) dx \right]$$ for any other probability density function $g$ ? This seems intuitively true to me and I imagine if it were to be true that it has been proven but I can't find a similar result on the standard textbooks, even a reference is welcome.","Given a random variable that takes values on all of with associated probability density function is it true that for all for any other probability density function ? This seems intuitively true to me and I imagine if it were to be true that it has been proven but I can't find a similar result on the standard textbooks, even a reference is welcome.",X \mathbb{R} f r > 0 E \left[ \int_{X-r}^{X+r} f(x) dx \right] \ge E \left[ \int_{X-r}^{X+r} g(x) dx \right] g,"['probability', 'integration', 'probability-theory', 'inequality']"
84,"Say ""red"" with playing cards","Say ""red"" with playing cards",,"Assume you have a set of playing cards, with 26 black and 26 red. You reveal one card at a time, and before every card is revealed you can say ""red"".(you only get one chance) If it red then you win, otherwise you lose. What's the strategy to maximise the winning chance? Some simple strategies(don't know if any of them works): first card $(50\%)$ when there are more red cards left (might not happen) after 25 red cards gone","Assume you have a set of playing cards, with 26 black and 26 red. You reveal one card at a time, and before every card is revealed you can say ""red"".(you only get one chance) If it red then you win, otherwise you lose. What's the strategy to maximise the winning chance? Some simple strategies(don't know if any of them works): first card $(50\%)$ when there are more red cards left (might not happen) after 25 red cards gone",,"['probability', 'combinatorics']"
85,Relationship between Laplace transform and moment generating function for queue,Relationship between Laplace transform and moment generating function for queue,,"(Again this is based on pp240 - 242 of the 1966 edition of Cox and Miller's ""The Theory of Stochastic Processes""). So we have, for a queue in equilibrium/stationary a probability density function for the delay (in virtual waiting time): $$p_0 + \int_0^\infty p(x)dx = 1$$ Where $x$ is the time taken for arriving customer to begin service and $p_0$ the probability that the wait will be zero. Now, the authors then state: $$p_0 + \int_0^\infty p(x)dx = p_0 + p^*(0) =1$$ Where the Laplace transform $\mathcal{L}\{p(x)\}$ = $p^*(s)$. That all seems fine to me, but they go on to say: $$w^*(s)=p_0+p^*(s) =\frac{...}{...}$$ (The rightmost term is not important here). They state ""we denote the m.g.f. of the equilibrium process by $w^*(s)$"". But isn't the mgf the (double sided) Laplace transform evaluated at $-s$? I don't suppose the double sized aspect matters much here: but does the $-s$ stipulation matter? (Perhaps not?) And how does $p_0$ fit in here? It doesn't seem to have been subject to any transform process, so why is it included in the mgf?","(Again this is based on pp240 - 242 of the 1966 edition of Cox and Miller's ""The Theory of Stochastic Processes""). So we have, for a queue in equilibrium/stationary a probability density function for the delay (in virtual waiting time): $$p_0 + \int_0^\infty p(x)dx = 1$$ Where $x$ is the time taken for arriving customer to begin service and $p_0$ the probability that the wait will be zero. Now, the authors then state: $$p_0 + \int_0^\infty p(x)dx = p_0 + p^*(0) =1$$ Where the Laplace transform $\mathcal{L}\{p(x)\}$ = $p^*(s)$. That all seems fine to me, but they go on to say: $$w^*(s)=p_0+p^*(s) =\frac{...}{...}$$ (The rightmost term is not important here). They state ""we denote the m.g.f. of the equilibrium process by $w^*(s)$"". But isn't the mgf the (double sided) Laplace transform evaluated at $-s$? I don't suppose the double sized aspect matters much here: but does the $-s$ stipulation matter? (Perhaps not?) And how does $p_0$ fit in here? It doesn't seem to have been subject to any transform process, so why is it included in the mgf?",,"['probability', 'stochastic-processes', 'laplace-transform', 'moment-generating-functions', 'queueing-theory']"
86,Generalization for Catalan number,Generalization for Catalan number,,"I know that the number of ways to open and close $n$ parenthesis is the $n$-th Catalan number. What about the number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis? Example: ))())() is a sequence of length 7 that contains exactly 2 well open and closed parenthesis. Basic fact: The number of sequences of open and closed parenthesis of length $k$ is equal to $2^k;$ The number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis is obviously greater that the the $n$-th Catalan number, but which is the precise value?","I know that the number of ways to open and close $n$ parenthesis is the $n$-th Catalan number. What about the number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis? Example: ))())() is a sequence of length 7 that contains exactly 2 well open and closed parenthesis. Basic fact: The number of sequences of open and closed parenthesis of length $k$ is equal to $2^k;$ The number of sequences of open and closed parenthesis of length $k$ that contains exactly $n$ well open and closed parenthesis is obviously greater that the the $n$-th Catalan number, but which is the precise value?",,"['probability', 'combinatorics', 'catalan-numbers']"
87,Least cost to guess a number between 0~100,Least cost to guess a number between 0~100,,"Here is the problem. There is a number randomly chosen between 1~100. The player tries to guess that. If the guess is larger than true value, the player is punished by 'a' dollar. If the guess is smaller than true value, the player is punished by 'b' dollar. How much money the play should prepare in order to hit the number in the worst case? 1) a = 1, b = 1; 2) a = 2, b = 1; 3) a = 1.5, b = 1; Here is where I am so far.  For the first sub-question, the player would use binary tree and the worst case would be 7 guessing times (6 wrong and 1 right). Hence, the punish money would be 6*1 = 6 dollars.  For the second sub-question, instead of separating the range by half, the player will separate the range according to the weight given by a and b, i.e. in this case the player would choose 33 for the first guess instead of 50. In this strategy, the worst case would cost the player 11 dollars(according to my calculation), while the binary method would cost the player 12 dollars. For the third question, the methodology is the same as second one. My doubt is: is there another method which is better? The doubt comes from this fact: my method applied same to sub-question 2 and sub-question 3, which means the existence of sub-question 3 is meaningless. Clearly an author of a problem would not put that kind of sub-question. appendix: I am not sure the problem in the link below would provide some hint, but they have something in common. http://datagenetics.com/blog/july22012/index.html","Here is the problem. There is a number randomly chosen between 1~100. The player tries to guess that. If the guess is larger than true value, the player is punished by 'a' dollar. If the guess is smaller than true value, the player is punished by 'b' dollar. How much money the play should prepare in order to hit the number in the worst case? 1) a = 1, b = 1; 2) a = 2, b = 1; 3) a = 1.5, b = 1; Here is where I am so far.  For the first sub-question, the player would use binary tree and the worst case would be 7 guessing times (6 wrong and 1 right). Hence, the punish money would be 6*1 = 6 dollars.  For the second sub-question, instead of separating the range by half, the player will separate the range according to the weight given by a and b, i.e. in this case the player would choose 33 for the first guess instead of 50. In this strategy, the worst case would cost the player 11 dollars(according to my calculation), while the binary method would cost the player 12 dollars. For the third question, the methodology is the same as second one. My doubt is: is there another method which is better? The doubt comes from this fact: my method applied same to sub-question 2 and sub-question 3, which means the existence of sub-question 3 is meaningless. Clearly an author of a problem would not put that kind of sub-question. appendix: I am not sure the problem in the link below would provide some hint, but they have something in common. http://datagenetics.com/blog/july22012/index.html",,"['probability', 'binary']"
88,"Uniformly random number on $[0,1]$ has zero entropy?",Uniformly random number on  has zero entropy?,"[0,1]","I am computing then entropy of the uniform distribution on $[0,1]$: $$ H(X) = \frac{1}{1-0} \int_0^1 (1 - 0) \log 1 \, dx = 0 $$ Does that mean at $X$ has zero entropy?","I am computing then entropy of the uniform distribution on $[0,1]$: $$ H(X) = \frac{1}{1-0} \int_0^1 (1 - 0) \log 1 \, dx = 0 $$ Does that mean at $X$ has zero entropy?",,"['probability', 'information-theory', 'entropy']"
89,Expectation of indicator variable squared,Expectation of indicator variable squared,,"Let $X$ be an indicator random variable with $P(X=1) = p$. My understanding is that $E(X) = p$, but why is it true that $E(X^2)=p$ as well?","Let $X$ be an indicator random variable with $P(X=1) = p$. My understanding is that $E(X) = p$, but why is it true that $E(X^2)=p$ as well?",,"['probability', 'random-variables']"
90,probability problem in Binomial distribution,probability problem in Binomial distribution,,"The question: A newsboy purchases papers at 12 cents and sells them at 16 cents.  However, he is not allowed to return unsold papers.  Suppose that his daily demand follows a Binomial distribution with n=10, p=1/3.  His mother suggests him to purchase 6 papers per day but his girlfriend suggests him to purchase 4 papers per day.  Who should the boy listen to so that he can obtain a higher expected profit? I am confused by this question.I tried the following: Define the probability distribution function by  $$f(x)=\sum_{i=0}^{10}\binom{10}{x}p^x(1-p)^{10-x}$$  then define $u(x)=0.04x+(n-x)(-0.12)$. For $n=6$, find $\sum_{x=0}^6u(x)f(x)$;for $n=10$, find $\sum_{x=0}^{10}u(x)f(x)$,then compare the two sums.  But I do not quite understand the question. Could anyone help?","The question: A newsboy purchases papers at 12 cents and sells them at 16 cents.  However, he is not allowed to return unsold papers.  Suppose that his daily demand follows a Binomial distribution with n=10, p=1/3.  His mother suggests him to purchase 6 papers per day but his girlfriend suggests him to purchase 4 papers per day.  Who should the boy listen to so that he can obtain a higher expected profit? I am confused by this question.I tried the following: Define the probability distribution function by  $$f(x)=\sum_{i=0}^{10}\binom{10}{x}p^x(1-p)^{10-x}$$  then define $u(x)=0.04x+(n-x)(-0.12)$. For $n=6$, find $\sum_{x=0}^6u(x)f(x)$;for $n=10$, find $\sum_{x=0}^{10}u(x)f(x)$,then compare the two sums.  But I do not quite understand the question. Could anyone help?",,[]
91,Alice and Bob are flipping coins...,Alice and Bob are flipping coins...,,"Alice and Bob are playing a game. They randomly determine who starts, then they take turns flipping a number of coins (N) and adding them to a growing pile. The first one to collect their target number of tails (T) wins. When Alice's variables are equal to Bob's ($N_{A} = N_{B}$, $T_{A} = T_{B}$), the odds of her winning are obviously 50%. However, for $N_{A} = 2, T_{A} = 20, N_{B} = 1, T_{B} = 10$ Alice's chances of victory appear to be slightly lower than 50%. This is based on running a few hundred thousand simulations of the game in Python. This outcome is, unfortunately, unintuitive to me. What is the mathematical reason for it? Note: This is a specific example chosen to highlight an issue I'm having in a more general problem. In the general problem, the players each have: Odds of an attempt getting them a point (O), number of attempts they get to make on their turn (N), and total number of collected points needed to win (T). If someone could also provide an equation that predicts the probability of Alice or Bob winning, given $O_{A}, N_{A}, T_{A}, O_{B}, N_{B}$, and $T_{B}$, I would be grateful.","Alice and Bob are playing a game. They randomly determine who starts, then they take turns flipping a number of coins (N) and adding them to a growing pile. The first one to collect their target number of tails (T) wins. When Alice's variables are equal to Bob's ($N_{A} = N_{B}$, $T_{A} = T_{B}$), the odds of her winning are obviously 50%. However, for $N_{A} = 2, T_{A} = 20, N_{B} = 1, T_{B} = 10$ Alice's chances of victory appear to be slightly lower than 50%. This is based on running a few hundred thousand simulations of the game in Python. This outcome is, unfortunately, unintuitive to me. What is the mathematical reason for it? Note: This is a specific example chosen to highlight an issue I'm having in a more general problem. In the general problem, the players each have: Odds of an attempt getting them a point (O), number of attempts they get to make on their turn (N), and total number of collected points needed to win (T). If someone could also provide an equation that predicts the probability of Alice or Bob winning, given $O_{A}, N_{A}, T_{A}, O_{B}, N_{B}$, and $T_{B}$, I would be grateful.",,['probability']
92,How to determine the number of coin tosses to identify one biased coin from another?,How to determine the number of coin tosses to identify one biased coin from another?,,"If coin $X$ and coin $Y$ are biased, and have the probability of turning up heads at $p$ and $q$ respectively, then given one of these coins at random, how many times must coin A be flipped in order to identify whether we're dealing with coin $X$ or $Y$? We assume a 0.5 chance that we can get either coin.","If coin $X$ and coin $Y$ are biased, and have the probability of turning up heads at $p$ and $q$ respectively, then given one of these coins at random, how many times must coin A be flipped in order to identify whether we're dealing with coin $X$ or $Y$? We assume a 0.5 chance that we can get either coin.",,"['probability', 'statistics', 'statistical-inference']"
93,Conditional probability question involving Bayes' theorem,Conditional probability question involving Bayes' theorem,,"A jar has 300 buttons in it. 299 buttons have one side red, the other side blue. One of the buttons has both sides blue. I randomly take a button from the jar and toss it 5 times getting 5 blues in a row. What is the probability I chose the special blue button? I did:  $$ \begin{align} P(\text{chose blue button}|\text{5 blue tosses}) &= \frac{P(\text{choose blue button and 5 blue tosses})}{P(\text{5 blue tosses})}\\ &= \frac{\frac{1}{300}\cdot 1^5}{\frac{299}{300} \left(\frac{1}{2}\right)^5 + \frac{1}{300}1^5} \end{align} $$ Is this correct?","A jar has 300 buttons in it. 299 buttons have one side red, the other side blue. One of the buttons has both sides blue. I randomly take a button from the jar and toss it 5 times getting 5 blues in a row. What is the probability I chose the special blue button? I did:  $$ \begin{align} P(\text{chose blue button}|\text{5 blue tosses}) &= \frac{P(\text{choose blue button and 5 blue tosses})}{P(\text{5 blue tosses})}\\ &= \frac{\frac{1}{300}\cdot 1^5}{\frac{299}{300} \left(\frac{1}{2}\right)^5 + \frac{1}{300}1^5} \end{align} $$ Is this correct?",,"['probability', 'bayes-theorem']"
94,Fun with combinatorics and 80 business customers,Fun with combinatorics and 80 business customers,,"In business with 80 workers, 7 of them are angry. If the business leader visits and picks 12 randomly, what is the probability of picking 12 where exactly 1 is angry? (7/80) (73/79) (72/78) (71/77) (70/76) (69/75) (68/74) (67/73) (66/72) (65/71) (64/70)*(63/69)*12=0.4134584151106464 What is the probability more than 2 are angry? My idea is to calculate the probability of 2,3,4,5,6, and 7 angry people just like did in the previous example and then add them altogether. In the previous example I can seat the one person 12 times. In all the different 12 spots, and then times by 12. The problem I have now is, how many times can I seat 2 people in 12 spots? If I use the combinatorics formula I will get a negative factorial. There must be a much easier way than this.","In business with 80 workers, 7 of them are angry. If the business leader visits and picks 12 randomly, what is the probability of picking 12 where exactly 1 is angry? (7/80) (73/79) (72/78) (71/77) (70/76) (69/75) (68/74) (67/73) (66/72) (65/71) (64/70)*(63/69)*12=0.4134584151106464 What is the probability more than 2 are angry? My idea is to calculate the probability of 2,3,4,5,6, and 7 angry people just like did in the previous example and then add them altogether. In the previous example I can seat the one person 12 times. In all the different 12 spots, and then times by 12. The problem I have now is, how many times can I seat 2 people in 12 spots? If I use the combinatorics formula I will get a negative factorial. There must be a much easier way than this.",,"['probability', 'combinatorics']"
95,"Distribution of $R^2 = X^2 +Y^2$ where (X,Y) is a point on the unit circle","Distribution of  where (X,Y) is a point on the unit circle",R^2 = X^2 +Y^2,"So I have a point $(X,Y)$ chosen from the unit disk with uniform distribution. And I'm attempting to find the distribution of $R^2$, where $R$ is the distance from the point to the origin. Now obviously the joint distribution: $f_{X,Y}(x,y)=\frac{1}{\pi}, \forall (x,y)\in C$ where $C=\{(x,y)\in\mathbb{R}:x^2+y^2\le1\}$ Now, I attempted to take the marginal distributions, square them, and then add them back, which gave the following: $F_X(x)=F_Y(x)=\frac{2}\pi\sqrt{1-x^2}, \forall x\in [-1,1]$ Applying the transformation, $g:[-1,1]\to\mathbb{R}, x\mapsto x^2$ \begin{align} f_{X^2}(x) & =f_{Y^2}(x)=f_X(\sqrt{x}) \left|\frac{1}{2}x^{-1/2}\right| +   \space f_X(-\sqrt{x}) \left| -\frac{1}{2}x^{-1/2}\right| \\[10pt] & =\frac{2}\pi\sqrt{\frac{1-x^2}{x}}, \forall x\in[0,1] \end{align} But then I realised I couldn't just add them as $X$ and $Y$ are dependent and as such so would $X^2$ and $Y^2$. So I assume I have to apply the transformation, $h:[-1,1]\times[-1,1] \to \mathbb{R}^2, (x,y)\mapsto(x^2,y^2)$ Which would entail me finding the Jacobian and inverse, etc. which I am fully able to do. However, $h$ is clearly not 1:1, and although I could get around that in the single variable case, I'm not sure what the anologue is for multy variables. So if anyone has any ideas on where to go from here, any assistance would be appreciated.","So I have a point $(X,Y)$ chosen from the unit disk with uniform distribution. And I'm attempting to find the distribution of $R^2$, where $R$ is the distance from the point to the origin. Now obviously the joint distribution: $f_{X,Y}(x,y)=\frac{1}{\pi}, \forall (x,y)\in C$ where $C=\{(x,y)\in\mathbb{R}:x^2+y^2\le1\}$ Now, I attempted to take the marginal distributions, square them, and then add them back, which gave the following: $F_X(x)=F_Y(x)=\frac{2}\pi\sqrt{1-x^2}, \forall x\in [-1,1]$ Applying the transformation, $g:[-1,1]\to\mathbb{R}, x\mapsto x^2$ \begin{align} f_{X^2}(x) & =f_{Y^2}(x)=f_X(\sqrt{x}) \left|\frac{1}{2}x^{-1/2}\right| +   \space f_X(-\sqrt{x}) \left| -\frac{1}{2}x^{-1/2}\right| \\[10pt] & =\frac{2}\pi\sqrt{\frac{1-x^2}{x}}, \forall x\in[0,1] \end{align} But then I realised I couldn't just add them as $X$ and $Y$ are dependent and as such so would $X^2$ and $Y^2$. So I assume I have to apply the transformation, $h:[-1,1]\times[-1,1] \to \mathbb{R}^2, (x,y)\mapsto(x^2,y^2)$ Which would entail me finding the Jacobian and inverse, etc. which I am fully able to do. However, $h$ is clearly not 1:1, and although I could get around that in the single variable case, I'm not sure what the anologue is for multy variables. So if anyone has any ideas on where to go from here, any assistance would be appreciated.",,"['probability', 'probability-distributions', 'transformation']"
96,Durrett Example 1.9 - Pairwise independence does not imply mutual independence?,Durrett Example 1.9 - Pairwise independence does not imply mutual independence?,,"The example in question is from Rick Durrett's ""Elementrary Probability for Applications"", and the setup is something like this: Let $A$ be the event ""Alice and Betty have the same birthday"", $B$ be the event ""Betty and Carol have the same birthday"", and $C$ be the event ""Carol and Alice have the same birthday"". Durrett goes on to demonstrate that each pair is independent, since for example, $P(A \cap B) = P(A)P(B)$. However, he concludes that $A, B$, and $C$ are not independent, since $P(A \cap B \cap C) = \frac{1}{365^2} \neq \frac{1}{365^3} = P(A)P(B)P(C)$. I understand the reasoning here, and that one can generally show that arbitrary events $X$ and $Y$ are not independent by showing that $P(X\cap Y) \neq P(X)P(Y)$. I am a little new to probability, though, and don't understand why exactly $P(A \cap B \cap C) = \frac{1}{365^2}$. My progress so far: I do see why $P(A) = P(B) =P(C) = \frac{1}{365}$, and thus why $P(A)P(B)P(C) = \frac{1}{365^3}$. It seems like the sample space $\Omega = \{ (a, b, c) \mid a,b,c \in [365] \}$ -- i.e., all of the possible triples of numbers from 1 to 365, where 1 denotes January 1st, 2 denotes January 2nd, etc. From that, I can conclude $|\Omega| = 365^3$, but I'm not sure where to go from here. It seems like once a single birthday is chosen, the rest are completely determined if they're all equal to each other - is this a good direction to go in?","The example in question is from Rick Durrett's ""Elementrary Probability for Applications"", and the setup is something like this: Let $A$ be the event ""Alice and Betty have the same birthday"", $B$ be the event ""Betty and Carol have the same birthday"", and $C$ be the event ""Carol and Alice have the same birthday"". Durrett goes on to demonstrate that each pair is independent, since for example, $P(A \cap B) = P(A)P(B)$. However, he concludes that $A, B$, and $C$ are not independent, since $P(A \cap B \cap C) = \frac{1}{365^2} \neq \frac{1}{365^3} = P(A)P(B)P(C)$. I understand the reasoning here, and that one can generally show that arbitrary events $X$ and $Y$ are not independent by showing that $P(X\cap Y) \neq P(X)P(Y)$. I am a little new to probability, though, and don't understand why exactly $P(A \cap B \cap C) = \frac{1}{365^2}$. My progress so far: I do see why $P(A) = P(B) =P(C) = \frac{1}{365}$, and thus why $P(A)P(B)P(C) = \frac{1}{365^3}$. It seems like the sample space $\Omega = \{ (a, b, c) \mid a,b,c \in [365] \}$ -- i.e., all of the possible triples of numbers from 1 to 365, where 1 denotes January 1st, 2 denotes January 2nd, etc. From that, I can conclude $|\Omega| = 365^3$, but I'm not sure where to go from here. It seems like once a single birthday is chosen, the rest are completely determined if they're all equal to each other - is this a good direction to go in?",,"['probability', 'independence']"
97,Marginal Distribution of Uniform Vector on Sphere,Marginal Distribution of Uniform Vector on Sphere,,"Suppose that $U=(U_1,\ldots,U_n)$ has the uniform distribution on the unit sphere $S_{n-1}=\{x\in\mathbb R^n:\|x\|_2=1\}$. I'm trying to understand the marginal distributions of individual components of the vector $U$, say, without loss of generality, $U_1$. So far, this is what I have: I know that $U$ is equal in distribution to $Z/\|Z\|_2$, where $Z$ is an $n$-dimensional standard gaussian vector. Thus, $U_1$ is equal in distribution to $Z_1/\|Z\|_2$. Then, we could in principle compute the distribution of $U_1$ as $$P[U_1\leq t]=P\big[Z_1\leq\|Z\|_2t\big],$$ but the right-hand side above using conventional means is horribly messy (i.e., nested integrals over the set $[x_1\leq\|x\|t]$). Is there a more practical/intuitive way of computing this distribution?","Suppose that $U=(U_1,\ldots,U_n)$ has the uniform distribution on the unit sphere $S_{n-1}=\{x\in\mathbb R^n:\|x\|_2=1\}$. I'm trying to understand the marginal distributions of individual components of the vector $U$, say, without loss of generality, $U_1$. So far, this is what I have: I know that $U$ is equal in distribution to $Z/\|Z\|_2$, where $Z$ is an $n$-dimensional standard gaussian vector. Thus, $U_1$ is equal in distribution to $Z_1/\|Z\|_2$. Then, we could in principle compute the distribution of $U_1$ as $$P[U_1\leq t]=P\big[Z_1\leq\|Z\|_2t\big],$$ but the right-hand side above using conventional means is horribly messy (i.e., nested integrals over the set $[x_1\leq\|x\|t]$). Is there a more practical/intuitive way of computing this distribution?",,"['probability', 'probability-distributions']"
98,Bound on variance of function of a random variable,Bound on variance of function of a random variable,,"Suppose $0\leq X\leq 1.$ Suppose we are given that $\mathrm{Var}(X)\leq a$ where $a$ is some small constant. What are the best upper bounds we can provide on $\mathrm{Var}(f(X))$ if a)  $f:[0,1]\mapsto\mathbb{R}$ is a Lipschitz function with Lipschitz constant $L$, such as say, $f(x) = x^2$ which has Lipschitz constant $2.$ b)  $f:[0,1]\mapsto\mathbb{R}$ is not Lipschitz but is a Hölder continuous function such as say $f(x) = \sqrt{x}.$ I am interested in upper bounds that go to zero as $a$ goes to zero.","Suppose $0\leq X\leq 1.$ Suppose we are given that $\mathrm{Var}(X)\leq a$ where $a$ is some small constant. What are the best upper bounds we can provide on $\mathrm{Var}(f(X))$ if a)  $f:[0,1]\mapsto\mathbb{R}$ is a Lipschitz function with Lipschitz constant $L$, such as say, $f(x) = x^2$ which has Lipschitz constant $2.$ b)  $f:[0,1]\mapsto\mathbb{R}$ is not Lipschitz but is a Hölder continuous function such as say $f(x) = \sqrt{x}.$ I am interested in upper bounds that go to zero as $a$ goes to zero.",,['probability']
99,Probability of rolling every face of a die at least once in a sequence of N throws.,Probability of rolling every face of a die at least once in a sequence of N throws.,,Say you have a die with $k$ faces (each face has a probability $1/k$). You throw this die $n$ times. What is the probability of having every single face showing at least once in that sequence?,Say you have a die with $k$ faces (each face has a probability $1/k$). You throw this die $n$ times. What is the probability of having every single face showing at least once in that sequence?,,"['probability', 'combinatorics']"
