,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve the Differential Equation : $y'' + \frac{L(x)}{2}y = 0 $,Solve the Differential Equation :,y'' + \frac{L(x)}{2}y = 0 ,"Solve the $2$nd order differential equation :   $$y'' + \frac{L(x)}{2}y = 0  $$ I have been looking at different methods as known integral , variation of parameters etc, but I don't know how to go about this equation. Please help.","Solve the $2$nd order differential equation :   $$y'' + \frac{L(x)}{2}y = 0  $$ I have been looking at different methods as known integral , variation of parameters etc, but I don't know how to go about this equation. Please help.",,['ordinary-differential-equations']
1,Kruzkov's change of variable,Kruzkov's change of variable,,Let $l:\Omega\to\mathbb{R}$ a sufficiently smooth function on an open set $\Omega$. Let the equations $$(I):\ \|\nabla u(x)\|=l(x)$$ $$(II):\ \|\nabla v(x)\|+l(x)v(x)=0$$ Prove that $u(x)$ is a viscosity solution of $(I)$ iff $v(x):=-e^{-u(x)}$ is a viscosity solution of $(II)$. I don't even know how to start. Any help would be appreciated.,Let $l:\Omega\to\mathbb{R}$ a sufficiently smooth function on an open set $\Omega$. Let the equations $$(I):\ \|\nabla u(x)\|=l(x)$$ $$(II):\ \|\nabla v(x)\|+l(x)v(x)=0$$ Prove that $u(x)$ is a viscosity solution of $(I)$ iff $v(x):=-e^{-u(x)}$ is a viscosity solution of $(II)$. I don't even know how to start. Any help would be appreciated.,,"['ordinary-differential-equations', 'optimal-control', 'viscosity-solutions']"
2,Strange attractors: what is the difference between a map and differential equation system?,Strange attractors: what is the difference between a map and differential equation system?,,"As far as I have been able to understand, there are two main ways of generating (or finding) a strange attractor : Using a map. E.g. the Hénon map (for a given $a,b$): $$x_{n+1} = 1 - a x_n^2 + y_n \ \ \ , \ \ \ \ y_{n+1} = b x_n$$ Using differential equations. E.g. the Lorentz system (for a given $\delta,\rho,\beta$): $$\dot x = \delta (y-x) \ \ \ , \ \ \ \ \dot y = x( \rho -z)-y \ \ \ , \ \ \ \ \dot z = x y - \beta z$$ What I do not understand very well is what differences are between a map and differential equations in this respect. This is what I guess: A map is always a discrete-time dynamical system , so no differential equations are required to generate the strange attractor. In the other hand, a differential equation system is per se a continuous-time dynamical system (due to the fact that it is based indeed on differential equations). Are the above assumptions correct, or are the differences between a map and a differential-equations-based dynamical system more than that? Can a differential equations system be converted into a map (probably adding some restrictions), or likewise, a map into a differential equations system and be able to reproduce the same strange attractor (or a restricted version of the same)?","As far as I have been able to understand, there are two main ways of generating (or finding) a strange attractor : Using a map. E.g. the Hénon map (for a given $a,b$): $$x_{n+1} = 1 - a x_n^2 + y_n \ \ \ , \ \ \ \ y_{n+1} = b x_n$$ Using differential equations. E.g. the Lorentz system (for a given $\delta,\rho,\beta$): $$\dot x = \delta (y-x) \ \ \ , \ \ \ \ \dot y = x( \rho -z)-y \ \ \ , \ \ \ \ \dot z = x y - \beta z$$ What I do not understand very well is what differences are between a map and differential equations in this respect. This is what I guess: A map is always a discrete-time dynamical system , so no differential equations are required to generate the strange attractor. In the other hand, a differential equation system is per se a continuous-time dynamical system (due to the fact that it is based indeed on differential equations). Are the above assumptions correct, or are the differences between a map and a differential-equations-based dynamical system more than that? Can a differential equations system be converted into a map (probably adding some restrictions), or likewise, a map into a differential equations system and be able to reproduce the same strange attractor (or a restricted version of the same)?",,"['ordinary-differential-equations', 'recurrence-relations', 'dynamical-systems']"
3,differential-equation with trigonometric functions,differential-equation with trigonometric functions,,"Is there a ""simple"" way to solve a differential-equation like the following example $f'(t)=\sin(f(t))+t^2$? I don't know how to approach this question, can someone help me?","Is there a ""simple"" way to solve a differential-equation like the following example $f'(t)=\sin(f(t))+t^2$? I don't know how to approach this question, can someone help me?",,"['calculus', 'ordinary-differential-equations', 'trigonometry']"
4,"Is there a pair of functions solving this ""trigonometric-like"" system of ODEs?","Is there a pair of functions solving this ""trigonometric-like"" system of ODEs?",,"The (ordinary) trigonometric functions are the solutions to the system of ordinary differential equations: \begin{align} c' &= -as, \\ s' &= ac, \end{align} with $c(0) = 1$ and $s(0) = 0$, for some constant $a$. Similarly, the hyperbolic functions are the solutions to the system \begin{align} c' &= as, \\ s' &= ac, \end{align} with the same initial conditions. Is there similarly some pair of functions satisfying \begin{align} c' &= \bar{z}s, \\ s' &= zc, \end{align} again with the same initial conditions, and where $z$ is complex and $\bar{z}$ is the complex conjugate of $z$? I have tried combining the trigonometric and hyperbolic functions in various ways, but cannot find a solution. If such a pair of functions does not exist, is there a method of finding e.g. a series solution?","The (ordinary) trigonometric functions are the solutions to the system of ordinary differential equations: \begin{align} c' &= -as, \\ s' &= ac, \end{align} with $c(0) = 1$ and $s(0) = 0$, for some constant $a$. Similarly, the hyperbolic functions are the solutions to the system \begin{align} c' &= as, \\ s' &= ac, \end{align} with the same initial conditions. Is there similarly some pair of functions satisfying \begin{align} c' &= \bar{z}s, \\ s' &= zc, \end{align} again with the same initial conditions, and where $z$ is complex and $\bar{z}$ is the complex conjugate of $z$? I have tried combining the trigonometric and hyperbolic functions in various ways, but cannot find a solution. If such a pair of functions does not exist, is there a method of finding e.g. a series solution?",,"['ordinary-differential-equations', 'special-functions']"
5,Boundedness of solution to a second order differential equation,Boundedness of solution to a second order differential equation,,"Suppose $f$ is a convex function with the property that level sets of $f$ are compact. I know that any solution of the differential equation $$\dot{x}=-\nabla f(x)$$ is bounded because $(d/dt) f(x) = - ||\nabla f(x)||^2$ so that $x_t$ always stays within the level set $\{ u ~| f(u) \leq f(x_0)\}$. My question is: suppose I have instead $$m \ddot{x} + c \dot{x} = - \nabla f(x),$$ where $m > 0$ and $c \geq 0$. It seems natural to guess that this will stay bounded as well. Is that true?","Suppose $f$ is a convex function with the property that level sets of $f$ are compact. I know that any solution of the differential equation $$\dot{x}=-\nabla f(x)$$ is bounded because $(d/dt) f(x) = - ||\nabla f(x)||^2$ so that $x_t$ always stays within the level set $\{ u ~| f(u) \leq f(x_0)\}$. My question is: suppose I have instead $$m \ddot{x} + c \dot{x} = - \nabla f(x),$$ where $m > 0$ and $c \geq 0$. It seems natural to guess that this will stay bounded as well. Is that true?",,['ordinary-differential-equations']
6,Must a one-dimensional conservative system with same period in spite of initial condition be a simple harmonic oscillator?,Must a one-dimensional conservative system with same period in spite of initial condition be a simple harmonic oscillator?,,"All the non-trivial solutions ( i.e. $x(t)\not\equiv 0$ )  of $$\frac{d^2x}{dt^2} = f(x)$$  has the same period, independent of the initial condition. Without loss of generality, set $$f(0)=0$$Does it imply that $$ f = -kx$$? If not, what  assumptions should be made to reach this conclusion? update: What if $f$ is restricted to be an odd smooth (or at least continuous) function, in which case,  we want to determine the continuous diffenrentiable function $F$ such that $$ \int_0^{x_0}\frac{1}{\sqrt{F(x_0)-F(s)}}\mathrm{d}s $$ is a constant, and let $f(x) = - F'(x)$.","All the non-trivial solutions ( i.e. $x(t)\not\equiv 0$ )  of $$\frac{d^2x}{dt^2} = f(x)$$  has the same period, independent of the initial condition. Without loss of generality, set $$f(0)=0$$Does it imply that $$ f = -kx$$? If not, what  assumptions should be made to reach this conclusion? update: What if $f$ is restricted to be an odd smooth (or at least continuous) function, in which case,  we want to determine the continuous diffenrentiable function $F$ such that $$ \int_0^{x_0}\frac{1}{\sqrt{F(x_0)-F(s)}}\mathrm{d}s $$ is a constant, and let $f(x) = - F'(x)$.",,"['analysis', 'ordinary-differential-equations', 'dynamical-systems']"
7,Combining Coefficients in a Fokker-Planck equation,Combining Coefficients in a Fokker-Planck equation,,"Question: What is the most appropriate way to combine two diffusion processes in a way that ""averages"" their behavior? Details A diffusion process can be parameterized by two functions $(\mu,\sigma)$. This corresponds to an Ito process: $$ dX_t = \mu(X_t)dt + \sigma(X_t)dW_t $$ as well as a Fokker-Planck equation : $$ \partial_t p(x,t) = -\sum_i\partial_i[\mu(x) p(x,t)]  +\frac{1}{2}\sum_i\sum_j \partial_{ij}[D_{ij}(x) p(x,t)]$$  where the diffusion tensor is $D(x)=\sigma(x)\,\sigma(x)^T$, which also appears in the equation for the infinitesimal generator . Furthermore, it is common to consider $D$ as the contravariant (inverse) metric tensor of a Riemannian manifold, i.e. $D=g^{-1}$. Ignoring $\mu$, notice there are three reasonable ways to define the variance of the process: $\sigma$, $D$, or $g$. My question is this: I have two diffusion processes, one with $\sigma$, $D$, and $g$, and another with $\hat{\sigma}$, $\hat{D}$, and $\hat{g}$. I want to create a new process with behaviour similar to the average between these two processes. So then my question is which variance measure should I combine ? I.e. which should I take:  (1) $\tilde{\sigma}=\sigma+\hat{\sigma}$,  (2) $ \tilde{g}=g+\hat{g}$, or  (3) $\tilde{D}=D+\hat{D}$? The application is not a physical one (in the experimental sense), but I would be happy with a physical reason. Indeed, I think any of the three would produce some kind of combined/average process, but it's not clear which one is the most sensible or natural. My initial thoughts: my most immediate thought is (1), because it is the most ""direct"" to the process. However, if we consider two metrics, it is sensible to add them (i.e. the sum of two metric tensors is still a metric), so perhaps (2) is reasonable as well.","Question: What is the most appropriate way to combine two diffusion processes in a way that ""averages"" their behavior? Details A diffusion process can be parameterized by two functions $(\mu,\sigma)$. This corresponds to an Ito process: $$ dX_t = \mu(X_t)dt + \sigma(X_t)dW_t $$ as well as a Fokker-Planck equation : $$ \partial_t p(x,t) = -\sum_i\partial_i[\mu(x) p(x,t)]  +\frac{1}{2}\sum_i\sum_j \partial_{ij}[D_{ij}(x) p(x,t)]$$  where the diffusion tensor is $D(x)=\sigma(x)\,\sigma(x)^T$, which also appears in the equation for the infinitesimal generator . Furthermore, it is common to consider $D$ as the contravariant (inverse) metric tensor of a Riemannian manifold, i.e. $D=g^{-1}$. Ignoring $\mu$, notice there are three reasonable ways to define the variance of the process: $\sigma$, $D$, or $g$. My question is this: I have two diffusion processes, one with $\sigma$, $D$, and $g$, and another with $\hat{\sigma}$, $\hat{D}$, and $\hat{g}$. I want to create a new process with behaviour similar to the average between these two processes. So then my question is which variance measure should I combine ? I.e. which should I take:  (1) $\tilde{\sigma}=\sigma+\hat{\sigma}$,  (2) $ \tilde{g}=g+\hat{g}$, or  (3) $\tilde{D}=D+\hat{D}$? The application is not a physical one (in the experimental sense), but I would be happy with a physical reason. Indeed, I think any of the three would produce some kind of combined/average process, but it's not clear which one is the most sensible or natural. My initial thoughts: my most immediate thought is (1), because it is the most ""direct"" to the process. However, if we consider two metrics, it is sensible to add them (i.e. the sum of two metric tensors is still a metric), so perhaps (2) is reasonable as well.",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'stochastic-processes', 'stochastic-calculus']"
8,Generalizing linear ODE's to Banach spaces,Generalizing linear ODE's to Banach spaces,,"The most general form of a linear IVP that was considered in my course is $$\dot x(t) = A(t)x(t)+b(t),\quad t\in J, \quad x(t_0)=x_0,$$ for $J$ an interval, $t_0\in J$, $A\in C(J,\mathbb{R}^{m\times m})$, and $b\in C(J,\mathbb{R}^m)$. The unique solution is derived using fundamental matrices and given as $$x(t) = X(t)\left(X^{-1}(t_0)x_0+\int_{t_0}^t X^{-1}(\tau)b(\tau)\operatorname{d}\tau\right)$$ with $X$ being a fundamental matrix. However, considering the same ODE in general Banach spaces, that is, $A\in C(J, \mathcal{L}(E))$, $b\in C(J, E)$, $E$ being a banach-space, what's the change? Is $$(\star)\quad x(t) = e^{\int_{t_0}^t A(\tau)\operatorname{d}\tau}x_0 +\int_{t_0}^t e^{\int_{\tau}^t A(s)\operatorname{d} s}b(\tau)\operatorname{d}\tau$$ the unique solution to the IVP? I understand the possibility of the space of solutions being of infinite dimension prohibits the use of fundamental matrices. Are fundamental matrices used to give concrete answers to concrete IVP's only? As I have worked through linear ODE's with constant $A\in\mathcal{L}(E)$ in banach-spaces, what's the difference coming from those ODE's? What becomes false after replacing ""$(t-t_0)A$"" by ""$\int_{t_0}^tA(\tau)\operatorname{d}\tau$""?","The most general form of a linear IVP that was considered in my course is $$\dot x(t) = A(t)x(t)+b(t),\quad t\in J, \quad x(t_0)=x_0,$$ for $J$ an interval, $t_0\in J$, $A\in C(J,\mathbb{R}^{m\times m})$, and $b\in C(J,\mathbb{R}^m)$. The unique solution is derived using fundamental matrices and given as $$x(t) = X(t)\left(X^{-1}(t_0)x_0+\int_{t_0}^t X^{-1}(\tau)b(\tau)\operatorname{d}\tau\right)$$ with $X$ being a fundamental matrix. However, considering the same ODE in general Banach spaces, that is, $A\in C(J, \mathcal{L}(E))$, $b\in C(J, E)$, $E$ being a banach-space, what's the change? Is $$(\star)\quad x(t) = e^{\int_{t_0}^t A(\tau)\operatorname{d}\tau}x_0 +\int_{t_0}^t e^{\int_{\tau}^t A(s)\operatorname{d} s}b(\tau)\operatorname{d}\tau$$ the unique solution to the IVP? I understand the possibility of the space of solutions being of infinite dimension prohibits the use of fundamental matrices. Are fundamental matrices used to give concrete answers to concrete IVP's only? As I have worked through linear ODE's with constant $A\in\mathcal{L}(E)$ in banach-spaces, what's the difference coming from those ODE's? What becomes false after replacing ""$(t-t_0)A$"" by ""$\int_{t_0}^tA(\tau)\operatorname{d}\tau$""?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'banach-spaces', 'initial-value-problems']"
9,solving a two dimensional stochastic differential equation,solving a two dimensional stochastic differential equation,,"This is exercise 5.8 from SDE by Oksendal. Solve the two dimensional SDE: $dX_1(t)=X_2(t)dt + \alpha  dB_1(t)$ $dX_2(t)= -X_1(t)dt + \beta dB_2(t)$ Where $(B_1(t),B_2(t))$ is two dimensional Brownian motion and $\alpha , \beta$ are constants. Is there any hint how to do it? Thanks a lot.","This is exercise 5.8 from SDE by Oksendal. Solve the two dimensional SDE: $dX_1(t)=X_2(t)dt + \alpha  dB_1(t)$ $dX_2(t)= -X_1(t)dt + \beta dB_2(t)$ Where $(B_1(t),B_2(t))$ is two dimensional Brownian motion and $\alpha , \beta$ are constants. Is there any hint how to do it? Thanks a lot.",,"['ordinary-differential-equations', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-analysis']"
10,Intuition for the Lipschitz-condition in Picard-Lindelöf,Intuition for the Lipschitz-condition in Picard-Lindelöf,,"In an exercise class I taught recently, someone came up to me after class and asked me about the intuition behind the Lipschitz-condition in Picard-Lindelöf. I am aware of the examples which show, that one cannot weaken the Lipschitz-condition in the Picard-Lindelöf Theorem. But why is the Lipschitz-condition really there? Is there some intuition from physics behind it? Or is it merely a technical tool to prove the theorem?","In an exercise class I taught recently, someone came up to me after class and asked me about the intuition behind the Lipschitz-condition in Picard-Lindelöf. I am aware of the examples which show, that one cannot weaken the Lipschitz-condition in the Picard-Lindelöf Theorem. But why is the Lipschitz-condition really there? Is there some intuition from physics behind it? Or is it merely a technical tool to prove the theorem?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
11,Which solutions to use for the following hypergeometric equation?,Which solutions to use for the following hypergeometric equation?,,"I am currently trying to understand the solution to a hypergeometric equation given in a paper on scalar fields and rotating black holes by S. Detweiler ( https://journals.aps.org/prd/abstract/10.1103/PhysRevD.22.2323 ). The equation I have trouble with is (17): $$z(z+1)\frac{d}{dz}[z(z+1)\frac{dR}{dz}]+[P^2-l(l+1)z(z+1)]R=0$$ When I try to solve it in mathematica I get associated Legendre polynomials: $$R(z) = C[1]LegendreP[l, 2 I P, 1 + 2 z] + C[2] LegendreQ[l, 2 I P, 1 + 2 z]$$ I know these can be written in terms of hypergeometric functions but they don't seem to match up when I try. In the paper, the author provides a solution of the following form: $$R(z)=(\frac{z}{z+1})^{iP}G(-l,l+1;1-2iP;z+1)$$ where G is any solution to the hypergeometric equation. From here he uses two independent hypergeometric functions $U_3$ and $U_4$ (which are in turn linear combinations of two other solutions $U_1$ and $U_5$): $$U_3 = (-z)^lF(-l,-l-2iP;-2l;-z^{-1})$$ $$U_4 = (-z)^{-l-1}F(l+1,l+1-2iP;2l+1;-z^{-1})$$ I am unsure how, given his solution in terms of G, the author knew which independent hypergeometric functions to use in the following steps to give the correct solution. Thank you in advance for any advice you can give me! EDIT: As I was unable to find them in the paper, I have calculated what I think are the correct boundary conditions. The requirement is that at the horizon, $r \rightarrow r_+$, there is only an ingoing wave solution. It is useful to know that in the above equations: $$ P = (am - 2Mr_+ \omega)/(r_+ - r_{-}) $$ $$ z = (r-r_+)/(r_+ - r_-)$$ then the boundary condition at $r \rightarrow r_+$ is $$R \sim (r-r_+)^{-i\alpha}$$ where $$ \alpha = \frac{Mr_+\omega-ma/2}{\sqrt{M^2-a^2}} $$","I am currently trying to understand the solution to a hypergeometric equation given in a paper on scalar fields and rotating black holes by S. Detweiler ( https://journals.aps.org/prd/abstract/10.1103/PhysRevD.22.2323 ). The equation I have trouble with is (17): $$z(z+1)\frac{d}{dz}[z(z+1)\frac{dR}{dz}]+[P^2-l(l+1)z(z+1)]R=0$$ When I try to solve it in mathematica I get associated Legendre polynomials: $$R(z) = C[1]LegendreP[l, 2 I P, 1 + 2 z] + C[2] LegendreQ[l, 2 I P, 1 + 2 z]$$ I know these can be written in terms of hypergeometric functions but they don't seem to match up when I try. In the paper, the author provides a solution of the following form: $$R(z)=(\frac{z}{z+1})^{iP}G(-l,l+1;1-2iP;z+1)$$ where G is any solution to the hypergeometric equation. From here he uses two independent hypergeometric functions $U_3$ and $U_4$ (which are in turn linear combinations of two other solutions $U_1$ and $U_5$): $$U_3 = (-z)^lF(-l,-l-2iP;-2l;-z^{-1})$$ $$U_4 = (-z)^{-l-1}F(l+1,l+1-2iP;2l+1;-z^{-1})$$ I am unsure how, given his solution in terms of G, the author knew which independent hypergeometric functions to use in the following steps to give the correct solution. Thank you in advance for any advice you can give me! EDIT: As I was unable to find them in the paper, I have calculated what I think are the correct boundary conditions. The requirement is that at the horizon, $r \rightarrow r_+$, there is only an ingoing wave solution. It is useful to know that in the above equations: $$ P = (am - 2Mr_+ \omega)/(r_+ - r_{-}) $$ $$ z = (r-r_+)/(r_+ - r_-)$$ then the boundary condition at $r \rightarrow r_+$ is $$R \sim (r-r_+)^{-i\alpha}$$ where $$ \alpha = \frac{Mr_+\omega-ma/2}{\sqrt{M^2-a^2}} $$",,"['ordinary-differential-equations', 'special-functions', 'mathematical-physics', 'hypergeometric-function']"
12,Generalization of Fuchs' Theorem for Differential Equations,Generalization of Fuchs' Theorem for Differential Equations,,"I am familiar with the method of Frobenius for solving second order linear DEs and have found a number of references to it online. To recap, it allows us to solve a DE like $\frac{d^2y}{dx^2} + p(x)\frac{d^2y}{dx^2} + q(x)y = 0$ with power series near a regular singular point assuming that p(x) and q(x) have poles of order at most 1 and 2 respectively. Fuchs' Theorem tells us that this method will always produce at least one answer. And I'm wondering if there is an analog to Fuchs' theorem to higher order linear DEs that anyone knows about and has a proof for. If we consider a higher order DE, $\frac{d^{n}y}{dx^{n}} + p_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}} + \cdots + p_{1}(x)\frac{dy}{dx} + p_{0}(x)y = 0$, we could begin to generalize the method. I imagine a singular point $x_{0}$ is called regular if $(x-x_0)^k p_{n-k}(x)$ is analytic for all $k = 1, 2, \cdots, n$. For this particular DE we would get an indical equation $r(r-1)\cdots (r-n+1) + r(r-1)\cdots (r-n+2)p_{n-1} + \cdots + p_{0} = 0$ where $p_k = lim_{x\rightarrow x_0}p_{k}(x)$ for each $k = 0, 1, \cdots, n-1$. Using solutions for $r$ we could then consider and work with solutions $y = \sum_{n=0}^{\infty}a_n (x-x_0)^{n+r}$. This seems like it would logically work, but I can't find much literature on this sort of generalization. I have only found one reference to this generalization at http://docsdrive.com/pdfs/sciencepublications/jmssp/2005/3-7.pdf and am curious as to whether there are others, perhaps an actual proof of these results and a characterization of the solutions in terms of the multiplicity of the solutions of the indical equation and their differences.","I am familiar with the method of Frobenius for solving second order linear DEs and have found a number of references to it online. To recap, it allows us to solve a DE like $\frac{d^2y}{dx^2} + p(x)\frac{d^2y}{dx^2} + q(x)y = 0$ with power series near a regular singular point assuming that p(x) and q(x) have poles of order at most 1 and 2 respectively. Fuchs' Theorem tells us that this method will always produce at least one answer. And I'm wondering if there is an analog to Fuchs' theorem to higher order linear DEs that anyone knows about and has a proof for. If we consider a higher order DE, $\frac{d^{n}y}{dx^{n}} + p_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}} + \cdots + p_{1}(x)\frac{dy}{dx} + p_{0}(x)y = 0$, we could begin to generalize the method. I imagine a singular point $x_{0}$ is called regular if $(x-x_0)^k p_{n-k}(x)$ is analytic for all $k = 1, 2, \cdots, n$. For this particular DE we would get an indical equation $r(r-1)\cdots (r-n+1) + r(r-1)\cdots (r-n+2)p_{n-1} + \cdots + p_{0} = 0$ where $p_k = lim_{x\rightarrow x_0}p_{k}(x)$ for each $k = 0, 1, \cdots, n-1$. Using solutions for $r$ we could then consider and work with solutions $y = \sum_{n=0}^{\infty}a_n (x-x_0)^{n+r}$. This seems like it would logically work, but I can't find much literature on this sort of generalization. I have only found one reference to this generalization at http://docsdrive.com/pdfs/sciencepublications/jmssp/2005/3-7.pdf and am curious as to whether there are others, perhaps an actual proof of these results and a characterization of the solutions in terms of the multiplicity of the solutions of the indical equation and their differences.",,['ordinary-differential-equations']
13,Solve differential equation $y''+y=\cos^2x$,Solve differential equation,y''+y=\cos^2x,"Solve differential equation  $y''+y=\cos^2x$ We are looking at a homogeneous equation: $$y''+y=0$$ $$\lambda^2+1=0\Rightarrow \lambda_1=-i,\lambda_2=i\Rightarrow y_h=C_1e^{0x}\cos x+C_2e^{0x}\sin x=C_1\cos x+C_2\sin x$$ Now we find a particular solution: $$y''+y=\cos^2x=\frac{1}{2}+\cos(2x)$$ $$y''+y=\frac{1}{2}$$ $$y_{p1}=1/2$$ $$y''+y=\cos(2x)$$ $$y_{p2}=A\sin 2x+B\cos 2x,y_{p2}''=-4A\sin 2x-4B\cos 2x$$ $$\Rightarrow A=0,B=-1/3, y_{p2}=-\frac{1}{3}\cos 2x$$ $$\Rightarrow y=y_h+y_{p1}+y_{p2}=C_1\cos x+C_2\sin x+\frac{1}{2}-\frac{1}{3}\cos 2x$$ Is this correct?","Solve differential equation  $y''+y=\cos^2x$ We are looking at a homogeneous equation: $$y''+y=0$$ $$\lambda^2+1=0\Rightarrow \lambda_1=-i,\lambda_2=i\Rightarrow y_h=C_1e^{0x}\cos x+C_2e^{0x}\sin x=C_1\cos x+C_2\sin x$$ Now we find a particular solution: $$y''+y=\cos^2x=\frac{1}{2}+\cos(2x)$$ $$y''+y=\frac{1}{2}$$ $$y_{p1}=1/2$$ $$y''+y=\cos(2x)$$ $$y_{p2}=A\sin 2x+B\cos 2x,y_{p2}''=-4A\sin 2x-4B\cos 2x$$ $$\Rightarrow A=0,B=-1/3, y_{p2}=-\frac{1}{3}\cos 2x$$ $$\Rightarrow y=y_h+y_{p1}+y_{p2}=C_1\cos x+C_2\sin x+\frac{1}{2}-\frac{1}{3}\cos 2x$$ Is this correct?",,['ordinary-differential-equations']
14,How do I actually choose poles?,How do I actually choose poles?,,"I have a control theory problem much akin to controlling the angle of an electric motor to a reference angle $\gamma_{ref}$ . (The ""electric motor"" exists in software only, and so has very little noise) Both the angle $x_1=\gamma$ and the speed $x_2=\dot{\gamma}$ are measureable, but only the angle acceleration $\ddot{\gamma}$ is controllable. The motor experiences a friction force $-b\dot{\gamma}$ and has inertia $1/c$ . Because I want to eliminate steady-state errors, I've introduced a dummy variable $x_3$ such that $\dot{x_3}=\gamma_{ref}-\gamma$ . My open loop system is: $$ \dot{x}= \begin{bmatrix}     0  & 1  & 0 \\     0  & -b & 0 \\     -1 & 0  & 0 \end{bmatrix} x +  \begin{bmatrix}     0   \\     c \\     0 \end{bmatrix} u + \begin{bmatrix}     0 \\     0 \\     1 \end{bmatrix} \gamma_{ref} $$ To put the control in canonical form, I've set: $$u=-l_1\gamma-l_2\dot{\gamma}-l_3x_3$$ and so $$ \begin{bmatrix}     0 \\     c \\     0 \end{bmatrix} u = \begin{bmatrix}     0 & 0 & 0 \\     -cl_1 & -cl_2 & -cl_3 \\     0 & 0 & 0 \end{bmatrix} x $$ which makes my closed loop system $$ \dot{x}= \begin{bmatrix}     0  & 1  & 0 \\     -cl_1  & -b-cl_2 & -cl_3 \\     -1 & 0  & 0 \end{bmatrix} x +  \begin{bmatrix}     0 \\     0 \\     1 \end{bmatrix} \gamma_{ref} = Ax+R\gamma_{ref} $$ To get the characteristic equation, I have to take $$\det{(A-\lambda I)}=0$$ which I've worked out to be $$\boxed{\lambda^3+(b+cl_2)\lambda^2+cl_1\lambda-cl_3=0}$$ PROBLEM. How do I actually choose the poles? Using my test environment in Java, I've concluded that for $b=0.1,c=1.0$ the poles $$ \lambda=-0.63\quad \lambda=-0.085-0.074i\quad \lambda=-0.085+0.074i $$ work fairly nicely... but this is a completely arbitrary trial-and-error method. I've seen some textbooks give answers like ""choose one pole -42b"" or whatever. But this is still not ideal. What general methods are available to me, where I can pick the poles based on some properties I want, like rise time and overshoot, and so on? I've tried reading about Bode plots but they sidetrack into theoretical stuff. Thanks a lot!!","I have a control theory problem much akin to controlling the angle of an electric motor to a reference angle . (The ""electric motor"" exists in software only, and so has very little noise) Both the angle and the speed are measureable, but only the angle acceleration is controllable. The motor experiences a friction force and has inertia . Because I want to eliminate steady-state errors, I've introduced a dummy variable such that . My open loop system is: To put the control in canonical form, I've set: and so which makes my closed loop system To get the characteristic equation, I have to take which I've worked out to be PROBLEM. How do I actually choose the poles? Using my test environment in Java, I've concluded that for the poles work fairly nicely... but this is a completely arbitrary trial-and-error method. I've seen some textbooks give answers like ""choose one pole -42b"" or whatever. But this is still not ideal. What general methods are available to me, where I can pick the poles based on some properties I want, like rise time and overshoot, and so on? I've tried reading about Bode plots but they sidetrack into theoretical stuff. Thanks a lot!!","\gamma_{ref} x_1=\gamma x_2=\dot{\gamma} \ddot{\gamma} -b\dot{\gamma} 1/c x_3 \dot{x_3}=\gamma_{ref}-\gamma 
\dot{x}=
\begin{bmatrix}
    0  & 1  & 0 \\
    0  & -b & 0 \\
    -1 & 0  & 0
\end{bmatrix}
x + 
\begin{bmatrix}
    0   \\
    c \\
    0
\end{bmatrix}
u +
\begin{bmatrix}
    0 \\
    0 \\
    1
\end{bmatrix}
\gamma_{ref}
 u=-l_1\gamma-l_2\dot{\gamma}-l_3x_3 
\begin{bmatrix}
    0 \\
    c \\
    0
\end{bmatrix}
u =
\begin{bmatrix}
    0 & 0 & 0 \\
    -cl_1 & -cl_2 & -cl_3 \\
    0 & 0 & 0
\end{bmatrix}
x
 
\dot{x}=
\begin{bmatrix}
    0  & 1  & 0 \\
    -cl_1  & -b-cl_2 & -cl_3 \\
    -1 & 0  & 0
\end{bmatrix}
x + 
\begin{bmatrix}
    0 \\
    0 \\
    1
\end{bmatrix}
\gamma_{ref}
=
Ax+R\gamma_{ref}
 \det{(A-\lambda I)}=0 \boxed{\lambda^3+(b+cl_2)\lambda^2+cl_1\lambda-cl_3=0} b=0.1,c=1.0 
\lambda=-0.63\quad
\lambda=-0.085-0.074i\quad
\lambda=-0.085+0.074i
","['ordinary-differential-equations', 'laplace-transform', 'control-theory', 'linear-control']"
15,Solving the differential equation $y' = y(1-y)e^{y}$,Solving the differential equation,y' = y(1-y)e^{y},"I'm trying to determine what the limit as $t$ goes to infinity of $y(t)$ is if $y(2016) = 2$ in the equation $y'= y(1-y)e^{y}$. I'm assuming the equation needs to be solved first, unless I'm missing some trick. However, I'm not sure how to solve this as if I try to solve it as I would any standard separable equation, the integral is impossible.","I'm trying to determine what the limit as $t$ goes to infinity of $y(t)$ is if $y(2016) = 2$ in the equation $y'= y(1-y)e^{y}$. I'm assuming the equation needs to be solved first, unless I'm missing some trick. However, I'm not sure how to solve this as if I try to solve it as I would any standard separable equation, the integral is impossible.",,['ordinary-differential-equations']
16,Why can't both of these functions be solutions to this differential equation?,Why can't both of these functions be solutions to this differential equation?,,"I am studying for an ODE exam, and working on this problem. Let $\displaystyle x_1 = \begin{bmatrix} 1 \\ t \\ \end{bmatrix}$ and $\displaystyle x_2 = \begin{bmatrix} t \\ 2t^2 \\ \end{bmatrix}$, and call by (H) the homogeneous differential equation $$x' = A(t) x$$ where $A$ is a continuous $2 \times 2$ matrix-valued function of $t$. I want to show that $x_1$ and $x_2$ cannot both be solutions to (H). Here is how I have gone about it: I know that $x_1$ and $x_2$ are linearly independent, so if they are both solutions to (H) then $W(t) \neq 0$ on the domain for $t$. And I've calculated the $W(t) = t^2$, where $W$ is the Wronskian of $\displaystyle \begin{bmatrix} 1 & t \\ t & 2t^2 \\ \end{bmatrix}$. But since I am not given a domain, couldn't both $x_1$ and $x_2$ be solutions of (H) as long as the domain for $t$ does not include $0$, as that would be the only point where $W(t) = 0$? The only thing I can think to apply is that the maximum interval of existence of each of these solutions is $\mathbb{R}$, which includes $0$. Should I generally assume that if I am not given a domain, I want the maximum interval of existence? (If anyone is wondering, this is a problem from Q. Kong's A Short Course in Ordinary Differential Equations . There is a second part but I am wondering the same thing about that part.) Edit: I've confirmed that it should be assumed that $A(t)$ is continuous on $\mathbb{R}$.","I am studying for an ODE exam, and working on this problem. Let $\displaystyle x_1 = \begin{bmatrix} 1 \\ t \\ \end{bmatrix}$ and $\displaystyle x_2 = \begin{bmatrix} t \\ 2t^2 \\ \end{bmatrix}$, and call by (H) the homogeneous differential equation $$x' = A(t) x$$ where $A$ is a continuous $2 \times 2$ matrix-valued function of $t$. I want to show that $x_1$ and $x_2$ cannot both be solutions to (H). Here is how I have gone about it: I know that $x_1$ and $x_2$ are linearly independent, so if they are both solutions to (H) then $W(t) \neq 0$ on the domain for $t$. And I've calculated the $W(t) = t^2$, where $W$ is the Wronskian of $\displaystyle \begin{bmatrix} 1 & t \\ t & 2t^2 \\ \end{bmatrix}$. But since I am not given a domain, couldn't both $x_1$ and $x_2$ be solutions of (H) as long as the domain for $t$ does not include $0$, as that would be the only point where $W(t) = 0$? The only thing I can think to apply is that the maximum interval of existence of each of these solutions is $\mathbb{R}$, which includes $0$. Should I generally assume that if I am not given a domain, I want the maximum interval of existence? (If anyone is wondering, this is a problem from Q. Kong's A Short Course in Ordinary Differential Equations . There is a second part but I am wondering the same thing about that part.) Edit: I've confirmed that it should be assumed that $A(t)$ is continuous on $\mathbb{R}$.",,['ordinary-differential-equations']
17,Kepler's Second Law of Planetary Motion. Solving for Theta at a known time in orbit.,Kepler's Second Law of Planetary Motion. Solving for Theta at a known time in orbit.,,The second law has been described to me above. I have taken a class on calculus and differential equations and am familiar with how to find a derivative. Given the earth's orbit I would say that the perihelion is theta of 0. Given an interval of 2592000 seconds (seconds per month) how can I use Kepler's second law to solve for the value of (THETA).Thank you. P.S. I am also interested in the values of r and r's corresponding height at time t as stated in the law's description.,The second law has been described to me above. I have taken a class on calculus and differential equations and am familiar with how to find a derivative. Given the earth's orbit I would say that the perihelion is theta of 0. Given an interval of 2592000 seconds (seconds per month) how can I use Kepler's second law to solve for the value of (THETA).Thank you. P.S. I am also interested in the values of r and r's corresponding height at time t as stated in the law's description.,,['ordinary-differential-equations']
18,Solution to $y''-\frac{2y'}x+cy=0$ by order reduction,Solution to  by order reduction,y''-\frac{2y'}x+cy=0,$$y''-\frac{2y'}x+cy=0$$ I have tried many ways to reduce this form to a first-order equation but they seem to be useless. Can anyone give me a hint? Is there any method that is better than changing to first-order?,$$y''-\frac{2y'}x+cy=0$$ I have tried many ways to reduce this form to a first-order equation but they seem to be useless. Can anyone give me a hint? Is there any method that is better than changing to first-order?,,['ordinary-differential-equations']
19,Why is $x'=\lambda x$ considered stiff for $\lambda<0$?,Why is  considered stiff for ?,x'=\lambda x \lambda<0,"Consider the ODE $x'(t)=\lambda x(t)$, $x(0)=1$ for $t\in[0,1]$, with $\lambda\in\mathbb{R}$. Applying the explicit Euler method yields approximations $x_N$ with $x_N(i/N)=(1+\frac{\lambda}{N})^i$, $i\in\{0,\dots,N\}$. The relative error to the exact solution $x(t)=\exp(\lambda t)$ is therefore $$ \epsilon_N(i/N)=\frac{\exp(\lambda i/N)-(1+\frac{\lambda}{N})^i}{\exp(\lambda i/N)}\\ =1-(1+\frac{\lambda}{N})^i\exp(-\lambda i/N)=1-\exp(i\log(1+\frac{\lambda}{N})-i\lambda/N)=1-\exp(-i\xi^2) $$ for some $\xi\in[0,|\lambda|/N]$ (as long as $N>-\lambda$). Therefore, $\sup_{i=1}^N|\epsilon_N|\leq \alpha$ requires (with $0<\alpha<1$ fixed and $\lambda\to \infty$) that $N\sim\lambda^2$. In the above derivation, the sign of $\lambda$ only appears once, when we require $N>-\lambda$ (which only is restrictive when $\lambda<0$). However, at the end we get $N\sim\lambda^2$ anyway. Therefore, I don't understand why the problem is considered stiff for $\lambda<0$ but not for $\lambda>0$.","Consider the ODE $x'(t)=\lambda x(t)$, $x(0)=1$ for $t\in[0,1]$, with $\lambda\in\mathbb{R}$. Applying the explicit Euler method yields approximations $x_N$ with $x_N(i/N)=(1+\frac{\lambda}{N})^i$, $i\in\{0,\dots,N\}$. The relative error to the exact solution $x(t)=\exp(\lambda t)$ is therefore $$ \epsilon_N(i/N)=\frac{\exp(\lambda i/N)-(1+\frac{\lambda}{N})^i}{\exp(\lambda i/N)}\\ =1-(1+\frac{\lambda}{N})^i\exp(-\lambda i/N)=1-\exp(i\log(1+\frac{\lambda}{N})-i\lambda/N)=1-\exp(-i\xi^2) $$ for some $\xi\in[0,|\lambda|/N]$ (as long as $N>-\lambda$). Therefore, $\sup_{i=1}^N|\epsilon_N|\leq \alpha$ requires (with $0<\alpha<1$ fixed and $\lambda\to \infty$) that $N\sim\lambda^2$. In the above derivation, the sign of $\lambda$ only appears once, when we require $N>-\lambda$ (which only is restrictive when $\lambda<0$). However, at the end we get $N\sim\lambda^2$ anyway. Therefore, I don't understand why the problem is considered stiff for $\lambda<0$ but not for $\lambda>0$.",,"['ordinary-differential-equations', 'numerical-methods']"
20,"Is there such a thing as ""total differential equations""?","Is there such a thing as ""total differential equations""?",,"As is well known, the theory of partial differential equations is much more difficult than the theory of ordinary differential equations, because PDEs don't behave as nicely. I was thinking, however, that the generalization of regular derivatives to the higher-dimensional case is usually best thought of as the total (i.e. Frechet) derivative, rather than partial derivatives. Question: Is there a theory of total differential equations? If not, why not? And if so, does the theory allow one to sidestep around a lot of the problems which arise in the theory of PDEs? The main problem I could think of is ""type-checking""; namely the original function will be a first-order tensor field, the first derivative would be a second-order tensor field, the second derivative would be a third order tensor field , and so on... Is there not a way to circumvent this problem? Note: Just so everyone is clear, I am not arguing against the usefulness of the  theory of PDEs; on the contrary, they were even used by Perelman to prove the Poincare Conjecture. I'm just wondering, if total differential equations would be easier to study, why no one has studied them. The existence of partial derivatives is only necessary but not sufficient for the existence of the total derivative. So my conjecture is that solving PDE's restricted to the solution space of total differential equations would be much easier and would have to account for far fewer pathological cases, since the existence of the total derivative implies far more regularity properties than one gets by assuming the existence of the partial derivatives alone. ( In fact, the existence of partial derivatives does not even imply the existence of all directional derivatives, which in turn does not even imply Gateaux differentiability, which in turn does not even imply total differentiability , so one should expect the general theory of PDEs to have far more pathological cases than the specific theory of total differential equations.) In other words, it seems like studying total differential equations might make it easier to identify which classes of PDE are easy to solve and would help explain in part why some PDEs are more difficult to solve, since it would make much more explicitly clear how analogies with ODEs break down in those cases. So even though total differential equations would arguably be a sub-case of the field of PDEs, the additional regularity properties would still make it a worthwhile object of study, the same way that the study of symmetric or diagonalizable matrices illuminates the whole of linear algebra.","As is well known, the theory of partial differential equations is much more difficult than the theory of ordinary differential equations, because PDEs don't behave as nicely. I was thinking, however, that the generalization of regular derivatives to the higher-dimensional case is usually best thought of as the total (i.e. Frechet) derivative, rather than partial derivatives. Question: Is there a theory of total differential equations? If not, why not? And if so, does the theory allow one to sidestep around a lot of the problems which arise in the theory of PDEs? The main problem I could think of is ""type-checking""; namely the original function will be a first-order tensor field, the first derivative would be a second-order tensor field, the second derivative would be a third order tensor field , and so on... Is there not a way to circumvent this problem? Note: Just so everyone is clear, I am not arguing against the usefulness of the  theory of PDEs; on the contrary, they were even used by Perelman to prove the Poincare Conjecture. I'm just wondering, if total differential equations would be easier to study, why no one has studied them. The existence of partial derivatives is only necessary but not sufficient for the existence of the total derivative. So my conjecture is that solving PDE's restricted to the solution space of total differential equations would be much easier and would have to account for far fewer pathological cases, since the existence of the total derivative implies far more regularity properties than one gets by assuming the existence of the partial derivatives alone. ( In fact, the existence of partial derivatives does not even imply the existence of all directional derivatives, which in turn does not even imply Gateaux differentiability, which in turn does not even imply total differentiability , so one should expect the general theory of PDEs to have far more pathological cases than the specific theory of total differential equations.) In other words, it seems like studying total differential equations might make it easier to identify which classes of PDE are easy to solve and would help explain in part why some PDEs are more difficult to solve, since it would make much more explicitly clear how analogies with ODEs break down in those cases. So even though total differential equations would arguably be a sub-case of the field of PDEs, the additional regularity properties would still make it a worthwhile object of study, the same way that the study of symmetric or diagonalizable matrices illuminates the whole of linear algebra.",,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
21,To find solution of $y''(x)=ay^3(x)+by^2(x)+cy(x)+d$,To find solution of,y''(x)=ay^3(x)+by^2(x)+cy(x)+d,"We know that $$g'(x)=g(x)$$ $g(0)=1$ Then $g(x)=e^x$ If we want to solve $y'=ay$, We can write general solution $y'=ay$ (or if derivative both sides $y''=a^2y$)  $$y=k.g(a x)=k.e^{ax}$$  where $k$ is a constant. We can express particular solution of many differential equation via using $e^{\beta x}$ but some we cannot. For example, we can find a particular solution for $y'=1+y^2$  (if we apply derivative both sides then we get  $y''=2y^3+2y$) and particular solution for $y''=2y^3+2y$ can be written: $$y=\tan(x)=-i\frac{g(ix)-g(-ix)}{g(ix)+g(-ix)}=-i\frac{e^{ix}-e^{-ix}}{e^{ix}+e^{-ix}}$$ but as I have known that we cannot solve the general differential equation ,$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$  (where a,b,c are constants) via only using  $e^{\beta x}$ . We need to define some non-elementery functions (elliptic functions) to find the solution. I thought to select an elliptic function to find a particular solution of general differential equation ,$$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$$ by using the selected elliptic function and elementary function $e^{\beta x}$ (my aim to find a closed form with limited terms in combination of both functions or only with the selected elliptic function). Let's select an elliptic function and try to use it a base function for this aim. $$f''(x)=-2f^3(x)$$ $f(0)=0$ $f'(0)=1$ $$(f'(x))^2=1-f^4(x)$$ The defined base function can be expressed $$x= \int_{0}^{f(x)} \frac{du}{\sqrt{1-u^4}}$$ Euler found an addition formula for this selected $f(x)$. The formula at page 6 in article that was written by Jose Barrios $$f(x+y)=\frac{f(x)\sqrt{1-f^4(y)}+f(y)\sqrt{1-f^4(x)}}{1+f^2(x)f^2(y)}$$ or we can rewrite it as: $$f(x+y)=\frac{f(x)f'(y)+f(y)f'(x)}{1+f^2(x)f^2(y)}$$ Is it possible to find a particular solution of the general differential equation $y''=ay^3+by^2+cy+d$ by using the combination of the selected  elliptic function $f(x)$  and $e^{x}$ in a closed form ? Note:Maybe we need to select another base $f(x)$ that would be easier for this aim. I selected randomly. I need references and hints about this idea. Thanks a lot for your helps EDIT:  15th August ,2016 I would like to  share my results: Let's assume that we have the equation: $$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$ If we solve the differential equation: General solution of the differential equation can be written as $y=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$ where $k$ is a constant. If we derivative both side $$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$ $$ y''=\frac{d^2y}{dx^2} = p (2y + m)y'$$ $$ y''= p^2 (2y + m)(y^2 + my + n)$$ $$ y''= 2p^2y^3+3p^2my^2+p^2(m^2+2n)y+p^2mn$$ If we compare with our general equation , We can find a condition that it has solution via $e^{\beta x}$. $$y''= ay^3+by^2+cy+d$$ The condition: $$2b^3+27a^2d=9abc$$ If the general equation satisfy this condition , the general equation has a solution with $e^{\beta x}$ but if the condition is not satisfy , we should express the solution with elliptic functions. It is a known fact that elliptic functions cannot be expressed by $e^{\beta x}$ in closed form. $p=\sqrt{a/2}$ ; $m=\frac{2b}{3a}$;$n=\frac{3d}{b}$ $y_p(x)=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$ where $k$ is a constant. I have noticed that we can express the solvable condition as: $y_p=A\cfrac{e^{\alpha x} +B}{e^{\alpha x} +C}$ Let's transform the general diff equation $y=A\cfrac{u +B}{u +C}$ and to check a condition for u $$y'=A\cfrac{C-B}{(u +C)^2}u'$$ $$y''= ay^3+by^2+cy+d$$ If we integrate both side, we get: $$ \frac12 \left( \frac{dy}{dx} \right)^2 =  \frac{a}{4} y^4+\frac{b}{3} y^3+\frac{c}{2} y^2+d y + e$$  where $e$ is a constant. $$  \frac{A^2(C-B)^2}{2(u +C)^4} u'^2 =  \frac{a}{4}\frac{(u +B)^4}{(u +C)^4}+\frac{b}{3} \frac{(u +B)^3}{(u +C)^3}+\frac{c}{2} \frac{(u +B)^2}{(u +C)^2}+d \frac{(u +B)}{(u +C)} + e$$ $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ I have been looking for a condition if we can transform it to one equation such as $$ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$$ where $a_n$ is any selected constant. In my question , I selected to convert it to $$ u'^2=  s(1-u^4)$$ but I am not sure if we can transform the general equation into this type. Can anybody prove that it is possible or not to convert it to a selected form such as $ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$? (where $a_n$ are selected constants.) EDIT:7th Sep 2016 $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ If We select $e=-(\frac{a}{4}+\frac{b}{3}+\frac{c}{2}+d)$ then $u^4$ term is canceled so we can get 3 degree polynomial form. $$ \frac{A^2(C-B)^2}{2} u'^2= b_3 u^3+b_2u^2+b_1u+b_0$$ We can do $x=\alpha t$ then  we can get the form, $$  u'^2=  u^3+k_2(A,B,C)u^2+k_1(A,B,C)u+k_0(A,B,C)$$ We still have 3 parameters ($A,B,C$) to select . We can transform into a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$. (where $a_n$ are  selected constants.) Can anybody help me to show if the transform to a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$  is possible or not?  Thanks a lot for helps","We know that $$g'(x)=g(x)$$ $g(0)=1$ Then $g(x)=e^x$ If we want to solve $y'=ay$, We can write general solution $y'=ay$ (or if derivative both sides $y''=a^2y$)  $$y=k.g(a x)=k.e^{ax}$$  where $k$ is a constant. We can express particular solution of many differential equation via using $e^{\beta x}$ but some we cannot. For example, we can find a particular solution for $y'=1+y^2$  (if we apply derivative both sides then we get  $y''=2y^3+2y$) and particular solution for $y''=2y^3+2y$ can be written: $$y=\tan(x)=-i\frac{g(ix)-g(-ix)}{g(ix)+g(-ix)}=-i\frac{e^{ix}-e^{-ix}}{e^{ix}+e^{-ix}}$$ but as I have known that we cannot solve the general differential equation ,$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$  (where a,b,c are constants) via only using  $e^{\beta x}$ . We need to define some non-elementery functions (elliptic functions) to find the solution. I thought to select an elliptic function to find a particular solution of general differential equation ,$$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$$ by using the selected elliptic function and elementary function $e^{\beta x}$ (my aim to find a closed form with limited terms in combination of both functions or only with the selected elliptic function). Let's select an elliptic function and try to use it a base function for this aim. $$f''(x)=-2f^3(x)$$ $f(0)=0$ $f'(0)=1$ $$(f'(x))^2=1-f^4(x)$$ The defined base function can be expressed $$x= \int_{0}^{f(x)} \frac{du}{\sqrt{1-u^4}}$$ Euler found an addition formula for this selected $f(x)$. The formula at page 6 in article that was written by Jose Barrios $$f(x+y)=\frac{f(x)\sqrt{1-f^4(y)}+f(y)\sqrt{1-f^4(x)}}{1+f^2(x)f^2(y)}$$ or we can rewrite it as: $$f(x+y)=\frac{f(x)f'(y)+f(y)f'(x)}{1+f^2(x)f^2(y)}$$ Is it possible to find a particular solution of the general differential equation $y''=ay^3+by^2+cy+d$ by using the combination of the selected  elliptic function $f(x)$  and $e^{x}$ in a closed form ? Note:Maybe we need to select another base $f(x)$ that would be easier for this aim. I selected randomly. I need references and hints about this idea. Thanks a lot for your helps EDIT:  15th August ,2016 I would like to  share my results: Let's assume that we have the equation: $$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$ If we solve the differential equation: General solution of the differential equation can be written as $y=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$ where $k$ is a constant. If we derivative both side $$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$ $$ y''=\frac{d^2y}{dx^2} = p (2y + m)y'$$ $$ y''= p^2 (2y + m)(y^2 + my + n)$$ $$ y''= 2p^2y^3+3p^2my^2+p^2(m^2+2n)y+p^2mn$$ If we compare with our general equation , We can find a condition that it has solution via $e^{\beta x}$. $$y''= ay^3+by^2+cy+d$$ The condition: $$2b^3+27a^2d=9abc$$ If the general equation satisfy this condition , the general equation has a solution with $e^{\beta x}$ but if the condition is not satisfy , we should express the solution with elliptic functions. It is a known fact that elliptic functions cannot be expressed by $e^{\beta x}$ in closed form. $p=\sqrt{a/2}$ ; $m=\frac{2b}{3a}$;$n=\frac{3d}{b}$ $y_p(x)=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$ where $k$ is a constant. I have noticed that we can express the solvable condition as: $y_p=A\cfrac{e^{\alpha x} +B}{e^{\alpha x} +C}$ Let's transform the general diff equation $y=A\cfrac{u +B}{u +C}$ and to check a condition for u $$y'=A\cfrac{C-B}{(u +C)^2}u'$$ $$y''= ay^3+by^2+cy+d$$ If we integrate both side, we get: $$ \frac12 \left( \frac{dy}{dx} \right)^2 =  \frac{a}{4} y^4+\frac{b}{3} y^3+\frac{c}{2} y^2+d y + e$$  where $e$ is a constant. $$  \frac{A^2(C-B)^2}{2(u +C)^4} u'^2 =  \frac{a}{4}\frac{(u +B)^4}{(u +C)^4}+\frac{b}{3} \frac{(u +B)^3}{(u +C)^3}+\frac{c}{2} \frac{(u +B)^2}{(u +C)^2}+d \frac{(u +B)}{(u +C)} + e$$ $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ I have been looking for a condition if we can transform it to one equation such as $$ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$$ where $a_n$ is any selected constant. In my question , I selected to convert it to $$ u'^2=  s(1-u^4)$$ but I am not sure if we can transform the general equation into this type. Can anybody prove that it is possible or not to convert it to a selected form such as $ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$? (where $a_n$ are selected constants.) EDIT:7th Sep 2016 $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ If We select $e=-(\frac{a}{4}+\frac{b}{3}+\frac{c}{2}+d)$ then $u^4$ term is canceled so we can get 3 degree polynomial form. $$ \frac{A^2(C-B)^2}{2} u'^2= b_3 u^3+b_2u^2+b_1u+b_0$$ We can do $x=\alpha t$ then  we can get the form, $$  u'^2=  u^3+k_2(A,B,C)u^2+k_1(A,B,C)u+k_0(A,B,C)$$ We still have 3 parameters ($A,B,C$) to select . We can transform into a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$. (where $a_n$ are  selected constants.) Can anybody help me to show if the transform to a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$  is possible or not?  Thanks a lot for helps",,"['ordinary-differential-equations', 'elliptic-functions', 'elliptic-integrals']"
22,How to tell if you have specified sufficient initial data for a differential equation?,How to tell if you have specified sufficient initial data for a differential equation?,,"I recently learnt that the following 'wave equation' is not well-posed $$ \begin{cases} \partial_{tt}u=\partial_{xx} u, & (0,1)\times\mathbb R\\ u(t,0)=u(t,1)=0,&t\in [0,1] \end{cases} $$ since the solution will not be unique. I was told that in this case it is sufficient that one specifies $\partial_t u(0,0)$ and $\partial_t u(0,1)$ in order to have a unique solution and I can understand the proof. But, could someone please explain to me what's going on here morally? Why is it that specifying only $u(\cdot,0)$ and $u(\cdot,1)$ is not enough, but further specifying the derivatives then is enough? I'd love an explanation that allows me to have a feeling for more general equations as to what would constitute sufficient initial data. If that's too vague, consider the concrete example: is the following heat-type equation well-posed  $$ \begin{cases} \partial_{t}u+\triangle^2u=0, & (0,\infty)\times\mathbb R^d\\ u(0,x)=f(x)\in C^\infty_c(\mathbb R^d\to\mathbb R) \end{cases} $$ provided we seek only solutions with sub-exponential growth (as with the usual heat equation) or do I need to specify more information about the derivatives of the solution at $t=0$ or something else entirely? And, how could you tell either way?","I recently learnt that the following 'wave equation' is not well-posed $$ \begin{cases} \partial_{tt}u=\partial_{xx} u, & (0,1)\times\mathbb R\\ u(t,0)=u(t,1)=0,&t\in [0,1] \end{cases} $$ since the solution will not be unique. I was told that in this case it is sufficient that one specifies $\partial_t u(0,0)$ and $\partial_t u(0,1)$ in order to have a unique solution and I can understand the proof. But, could someone please explain to me what's going on here morally? Why is it that specifying only $u(\cdot,0)$ and $u(\cdot,1)$ is not enough, but further specifying the derivatives then is enough? I'd love an explanation that allows me to have a feeling for more general equations as to what would constitute sufficient initial data. If that's too vague, consider the concrete example: is the following heat-type equation well-posed  $$ \begin{cases} \partial_{t}u+\triangle^2u=0, & (0,\infty)\times\mathbb R^d\\ u(0,x)=f(x)\in C^\infty_c(\mathbb R^d\to\mathbb R) \end{cases} $$ provided we seek only solutions with sub-exponential growth (as with the usual heat equation) or do I need to specify more information about the derivatives of the solution at $t=0$ or something else entirely? And, how could you tell either way?",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
23,ODE system solving by sequence of functions,ODE system solving by sequence of functions,,"Let $y' = Ay$ where $A = \begin{pmatrix} 0&1 \\ -1& 0 \end{pmatrix}$ and $y( 0 ) = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. Consider the map $$G: C(\mathbb{R},\mathbb{R}^2) \to C(\mathbb{R},\mathbb{R}^2),  G(\phi)(x) = \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \int_0^x A\phi(t) dt$$. With $\phi_0(x) = \begin{pmatrix} 1\\0\end{pmatrix}$ and $\phi_{n+1} = G(\phi_n)$, how to find and prove the general formula for $(\phi_n)_{n\in\mathbb{N}}$?","Let $y' = Ay$ where $A = \begin{pmatrix} 0&1 \\ -1& 0 \end{pmatrix}$ and $y( 0 ) = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. Consider the map $$G: C(\mathbb{R},\mathbb{R}^2) \to C(\mathbb{R},\mathbb{R}^2),  G(\phi)(x) = \begin{pmatrix} 1 \\ 0 \end{pmatrix} + \int_0^x A\phi(t) dt$$. With $\phi_0(x) = \begin{pmatrix} 1\\0\end{pmatrix}$ and $\phi_{n+1} = G(\phi_n)$, how to find and prove the general formula for $(\phi_n)_{n\in\mathbb{N}}$?",,"['ordinary-differential-equations', 'self-learning']"
24,Possibly new solution to equal-mass three-body problem; refinement required,Possibly new solution to equal-mass three-body problem; refinement required,,"(Since I didn't know which authorities to contact, I thought I'd post this here.) While messing around in this Wolfram Demonstrations applet , I found a suspicious pattern, in which I could see physical similarities between the path of the first object, the path of the second object at a later time, and even the path of the third object at an even later time. Through gradually tweaking that set of initial conditions, I was able to find an apparently new solution to the equal-mass three-body problem that wasn't listed here . Here are the initial conditions, with my restrictions in parentheses: $x_1(0)=0.7812$ $y_1(0)=-0.2465\;(\text{holding $x_1(0)^2+y_1(0)^2$ constant to avoid scaling})$ $x_2(0)=-0.2465\;(=y_1(0))$ $y_2(0)=0.7812\;(=x_1(0))$ $x_3(0)=-0.5347\;(=-x_1(0)-x_2(0))$ $y_3(0)=-0.5347\;(=x_3(0))$ $x_1'(0)=-0.6087$ $y_1'(0)=-0.286$ $x_2'(0)=0.286\;(=-y_1'(0))$ $y_2'(0)=0.6087\;(=-x_1'(0))$ $x_3'(0)=0.3227\;(=-x_1'(0)-x_2'(0))$ $y_3'(0)=-0.3227\;(=-x_3'(0))$ (I would estimate the uncertainty for all parameters to be roughly $0.01$ in either direction. It would be smaller, but a slight change in the position parameter can be practically canceled out by a slight change in one or both of the velocity parameters, apparently resulting in a relatively 2D structure in 3D parameter space.) This gives an approximate choreography with period $p\approx 17.0874$, shaped somewhat like a rosette: Going further in time, the entire system apparently rotates, but doesn't break apart: At this point, what algorithms or heuristics should I use to increase the precision of the parameters (for example, trying to make the paths match over increasing lengths of time)?","(Since I didn't know which authorities to contact, I thought I'd post this here.) While messing around in this Wolfram Demonstrations applet , I found a suspicious pattern, in which I could see physical similarities between the path of the first object, the path of the second object at a later time, and even the path of the third object at an even later time. Through gradually tweaking that set of initial conditions, I was able to find an apparently new solution to the equal-mass three-body problem that wasn't listed here . Here are the initial conditions, with my restrictions in parentheses: $x_1(0)=0.7812$ $y_1(0)=-0.2465\;(\text{holding $x_1(0)^2+y_1(0)^2$ constant to avoid scaling})$ $x_2(0)=-0.2465\;(=y_1(0))$ $y_2(0)=0.7812\;(=x_1(0))$ $x_3(0)=-0.5347\;(=-x_1(0)-x_2(0))$ $y_3(0)=-0.5347\;(=x_3(0))$ $x_1'(0)=-0.6087$ $y_1'(0)=-0.286$ $x_2'(0)=0.286\;(=-y_1'(0))$ $y_2'(0)=0.6087\;(=-x_1'(0))$ $x_3'(0)=0.3227\;(=-x_1'(0)-x_2'(0))$ $y_3'(0)=-0.3227\;(=-x_3'(0))$ (I would estimate the uncertainty for all parameters to be roughly $0.01$ in either direction. It would be smaller, but a slight change in the position parameter can be practically canceled out by a slight change in one or both of the velocity parameters, apparently resulting in a relatively 2D structure in 3D parameter space.) This gives an approximate choreography with period $p\approx 17.0874$, shaped somewhat like a rosette: Going further in time, the entire system apparently rotates, but doesn't break apart: At this point, what algorithms or heuristics should I use to increase the precision of the parameters (for example, trying to make the paths match over increasing lengths of time)?",,"['calculus', 'ordinary-differential-equations', 'reference-request', 'classical-mechanics']"
25,Modeling particles moving through a chamber,Modeling particles moving through a chamber,,"Consider the following phenomenon. Particles each traveling with speed $v_i$ metres per second enter a chamber at a rate of $r$ particles per second. Upon entering the chamber, each particle begins to reduce its speed from $v_i$ to $v_f$.  Each particle then exits the chamber. If I look at an individual particle entering the chamber at time $0$, it is travelling $Ce^{-At}$ metres per second after $t$ seconds.  The constants $C$ and $A$ are the same for every particle. I let this process run for some time.  Is there an expression $n(v,t)$ describing the number of particles in the chamber which are travelling at speed $v_i \leq v \leq v_f$ at time $t > 0$?","Consider the following phenomenon. Particles each traveling with speed $v_i$ metres per second enter a chamber at a rate of $r$ particles per second. Upon entering the chamber, each particle begins to reduce its speed from $v_i$ to $v_f$.  Each particle then exits the chamber. If I look at an individual particle entering the chamber at time $0$, it is travelling $Ce^{-At}$ metres per second after $t$ seconds.  The constants $C$ and $A$ are the same for every particle. I let this process run for some time.  Is there an expression $n(v,t)$ describing the number of particles in the chamber which are travelling at speed $v_i \leq v \leq v_f$ at time $t > 0$?",,"['ordinary-differential-equations', 'physics', 'mathematical-modeling']"
26,A question about a route of a point that travels in a particular way through the plane,A question about a route of a point that travels in a particular way through the plane,,"I don't know exactly how to classify this question. It is not from any homeworks, just something I've been wondering about. Let's say that in the beginning of an experiment ( the beginning is $t=0$ secs) we have two points on the plane: one on $(0,0)$ and one on $(0, 1)$. They start moving by the following rules: The point that was on $(0,0)$ moves right, on the $x$-axis, in a constant speed of $1~\text{m}/\text{s}$. The point that was on $(0,1)$ also moves in a constant speed of $1~\text{m}/\text{s}$, but its direction changes, so that the direction of its speed is always directed to the first point (like a missile that follows a moving object). I hope I made myself clear.  My question is: Is there a nice formula for the location of the second point on the plane for a given time $t$? How can it be derived? And what if we change the ratio between the constant speeds? Thank you for your time reading my question...","I don't know exactly how to classify this question. It is not from any homeworks, just something I've been wondering about. Let's say that in the beginning of an experiment ( the beginning is $t=0$ secs) we have two points on the plane: one on $(0,0)$ and one on $(0, 1)$. They start moving by the following rules: The point that was on $(0,0)$ moves right, on the $x$-axis, in a constant speed of $1~\text{m}/\text{s}$. The point that was on $(0,1)$ also moves in a constant speed of $1~\text{m}/\text{s}$, but its direction changes, so that the direction of its speed is always directed to the first point (like a missile that follows a moving object). I hope I made myself clear.  My question is: Is there a nice formula for the location of the second point on the plane for a given time $t$? How can it be derived? And what if we change the ratio between the constant speeds? Thank you for your time reading my question...",,"['calculus', 'ordinary-differential-equations']"
27,"""Shape"" of solutions of 2nd order homogeneous ODEs","""Shape"" of solutions of 2nd order homogeneous ODEs",,"Consider a second order homogeneous ODE: $$P(x)y''+Q(x)y'+R(x)y=0.$$ If $P,Q,R$ are constant functions, then we know that the general solution has the form $$y=c_1e^{r_1x}+c_2e^{r_2x},$$ $$y=c_1e^{r_1x}+c_2xe^{r_2x},$$ or $$y=e^{\alpha x}[c_1\cos(\beta x)+c_2\sin(\beta x)],$$ depending on the constants $P,Q,R$. If $P,Q,R$ are not constant, the solutions are not so simple. However, most resources I could find (such as this one ) do not discuss this case at all. What I was wondering is, even if we can't solve the equation directly, is it possible to get some meaningful information on the behaviour of the solutions? For example, is it possible to tell that the solution ""behaves like an exponential"" asymptotically, or oscillates like a sine/cosine function? Any reference discussing such questions would be welcome.","Consider a second order homogeneous ODE: $$P(x)y''+Q(x)y'+R(x)y=0.$$ If $P,Q,R$ are constant functions, then we know that the general solution has the form $$y=c_1e^{r_1x}+c_2e^{r_2x},$$ $$y=c_1e^{r_1x}+c_2xe^{r_2x},$$ or $$y=e^{\alpha x}[c_1\cos(\beta x)+c_2\sin(\beta x)],$$ depending on the constants $P,Q,R$. If $P,Q,R$ are not constant, the solutions are not so simple. However, most resources I could find (such as this one ) do not discuss this case at all. What I was wondering is, even if we can't solve the equation directly, is it possible to get some meaningful information on the behaviour of the solutions? For example, is it possible to tell that the solution ""behaves like an exponential"" asymptotically, or oscillates like a sine/cosine function? Any reference discussing such questions would be welcome.",,['ordinary-differential-equations']
28,Which singular perturbation method should be used for this system?,Which singular perturbation method should be used for this system?,,"Consider the system $$ \varepsilon \dfrac{dx}{dt} = -(x^3 - ax + b)$$ $$ \dfrac{db}{dt} = x - x_a$$ where $\varepsilon \ll 1$.  Applying regular perturbation methods isn't suitable because when $\varepsilon = 0$, the system turns into a differential - algebraic system. Which singular perturbation method is best for problems similar to this?","Consider the system $$ \varepsilon \dfrac{dx}{dt} = -(x^3 - ax + b)$$ $$ \dfrac{db}{dt} = x - x_a$$ where $\varepsilon \ll 1$.  Applying regular perturbation methods isn't suitable because when $\varepsilon = 0$, the system turns into a differential - algebraic system. Which singular perturbation method is best for problems similar to this?",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
29,Systems of ODE to High-Order converstion: Why?,Systems of ODE to High-Order converstion: Why?,,"I am the TA for a course in ODE, and one of my students asked me a question yesterday: why in the world do we bother with converting between (constant-coefficient) systems and higher-order ODE? I started to give what I think is the standard answer: we're really good at solving first order ODE but higher order stuff is hard, solution spaces, matrix exponentials, etc. etc. But I found myself at a complete loss for why we might want to go in the other direction. Are there advantages to having a single equation, that outweigh the fact that it is of high order?","I am the TA for a course in ODE, and one of my students asked me a question yesterday: why in the world do we bother with converting between (constant-coefficient) systems and higher-order ODE? I started to give what I think is the standard answer: we're really good at solving first order ODE but higher order stuff is hard, solution spaces, matrix exponentials, etc. etc. But I found myself at a complete loss for why we might want to go in the other direction. Are there advantages to having a single equation, that outweigh the fact that it is of high order?",,"['ordinary-differential-equations', 'systems-of-equations', 'education']"
30,Solving second-order ODE using an integrating factor,Solving second-order ODE using an integrating factor,,"Solve the ODE    $$ \frac{\partial^{2} u }{\partial \eta^{2}} + \frac{\eta}{2\nu} \frac{\partial}{\partial\eta} = 0 $$ The book uses integrating factor = $ e^{\int\frac{\eta}{2\nu} d\eta}$ Can someone explain from here, how to proceed? I get $ IF = e^\frac{\eta^{2}}{4\nu}$. Multiply this with the ODE. Not sure what to do next? Because the answer the book gives is  $$ \frac{\partial u}{\partial \eta} = A e^{\frac{-\eta^{2}}{4/nu}}$$  and I don't understand how to get this.","Solve the ODE    $$ \frac{\partial^{2} u }{\partial \eta^{2}} + \frac{\eta}{2\nu} \frac{\partial}{\partial\eta} = 0 $$ The book uses integrating factor = $ e^{\int\frac{\eta}{2\nu} d\eta}$ Can someone explain from here, how to proceed? I get $ IF = e^\frac{\eta^{2}}{4\nu}$. Multiply this with the ODE. Not sure what to do next? Because the answer the book gives is  $$ \frac{\partial u}{\partial \eta} = A e^{\frac{-\eta^{2}}{4/nu}}$$  and I don't understand how to get this.",,['ordinary-differential-equations']
31,Problem: differential equation,Problem: differential equation,,"Hi I try solve the following problem of differential equation $$ x''+tx'+\frac{1}{1+t+t^2}x=0\tag 1$$ when $$x(1)=0\ \ \ ;\ \ \ x'(1)=1 $$ is the solution analytic in $t_0=1$ and his convergence radius is $R>1$? Ok, I think I need put the differential equation like a frobenius differential equation, then I get, with the initial equation, that my solution is define by $$ \varphi_1(t)=\sum_{n=0}^{\infty} a_n(t-1)^{n+1}$$ $$\varphi_2(t)=C\varphi_1(t)\log(t-1)+\sum_{n=0}^{\infty} b_n(t-1)^n $$ I can not work with $\varphi_1$ in $(1)$, I do not know... someone could help me?","Hi I try solve the following problem of differential equation $$ x''+tx'+\frac{1}{1+t+t^2}x=0\tag 1$$ when $$x(1)=0\ \ \ ;\ \ \ x'(1)=1 $$ is the solution analytic in $t_0=1$ and his convergence radius is $R>1$? Ok, I think I need put the differential equation like a frobenius differential equation, then I get, with the initial equation, that my solution is define by $$ \varphi_1(t)=\sum_{n=0}^{\infty} a_n(t-1)^{n+1}$$ $$\varphi_2(t)=C\varphi_1(t)\log(t-1)+\sum_{n=0}^{\infty} b_n(t-1)^n $$ I can not work with $\varphi_1$ in $(1)$, I do not know... someone could help me?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'power-series', 'frobenius-method']"
32,Algebraic solutions of a differential equation.,Algebraic solutions of a differential equation.,,"Given a differential equation $y' = (1/x)(y^2 + y^3)$. My question is how does one go about finding the solutions of this differential equation which are algebraic over the field $\Bbb{C}(x)$,if any. Notation- $\Bbb{C}(x)$ is the quotient field of $\Bbb{C}[x]$.","Given a differential equation $y' = (1/x)(y^2 + y^3)$. My question is how does one go about finding the solutions of this differential equation which are algebraic over the field $\Bbb{C}(x)$,if any. Notation- $\Bbb{C}(x)$ is the quotient field of $\Bbb{C}[x]$.",,"['ordinary-differential-equations', 'galois-theory', 'differential-algebra']"
33,singular or ordinary point of a differential equation,singular or ordinary point of a differential equation,,Is $x=0$ singular or regular point of the following differential equation $p_2(x)y''+p_1(x)y'+p_0(x)y=0$ We know that $s_1=x$ and $s_2=x^2$ are two solutions of the equation I am having trouble figuring out how I can use the solutions to find out if it is a singular or ordinary point. Any help would help alot :),Is $x=0$ singular or regular point of the following differential equation $p_2(x)y''+p_1(x)y'+p_0(x)y=0$ We know that $s_1=x$ and $s_2=x^2$ are two solutions of the equation I am having trouble figuring out how I can use the solutions to find out if it is a singular or ordinary point. Any help would help alot :),,"['calculus', 'ordinary-differential-equations', 'homogeneous-equation']"
34,ODE solution as power series,ODE solution as power series,,"Consider the equation $$(x+3)y''+2y'-4(x+3)y=0.$$ I was trying to solve it by finding the solution in the form of power series. However, I stuck while trying to find any regularity in the coefficients, while Wolfram Alpha provides a pretty much nice answer. Could you please point out the direction?","Consider the equation $$(x+3)y''+2y'-4(x+3)y=0.$$ I was trying to solve it by finding the solution in the form of power series. However, I stuck while trying to find any regularity in the coefficients, while Wolfram Alpha provides a pretty much nice answer. Could you please point out the direction?",,"['ordinary-differential-equations', 'power-series']"
35,Check whether a functional has an extremal or NOT,Check whether a functional has an extremal or NOT,,"Find the extremal of the functional $$J(y)=\int_a^b F(x,y,y')\,dx$$where , $F(x,y,y')=y'+y$ , for admissible functions $y$. From Euler-Lagrange equation , $\displaystyle \frac{d}{dx}\left(\frac{\partial F}{\partial y'}\right)-\frac{\partial F}{\partial y}=0$ we get , $-1=0$ ( absurd ). So , we can conclude that the functional has NO extramal . Again we know when $x$ is absent in $F$ ,  then Euler-Lagrange equation is transformed into $\displaystyle F-y'\frac{\partial F}{\partial y'}=\text{ constant }$. Since in this problem $x$ is absent in $F$ so we can use it and using we get , $y=\text{constant }$ , which is the required extremal. Why these two process give different result ? Which is the correct answer and why ? Please explain properly.","Find the extremal of the functional $$J(y)=\int_a^b F(x,y,y')\,dx$$where , $F(x,y,y')=y'+y$ , for admissible functions $y$. From Euler-Lagrange equation , $\displaystyle \frac{d}{dx}\left(\frac{\partial F}{\partial y'}\right)-\frac{\partial F}{\partial y}=0$ we get , $-1=0$ ( absurd ). So , we can conclude that the functional has NO extramal . Again we know when $x$ is absent in $F$ ,  then Euler-Lagrange equation is transformed into $\displaystyle F-y'\frac{\partial F}{\partial y'}=\text{ constant }$. Since in this problem $x$ is absent in $F$ so we can use it and using we get , $y=\text{constant }$ , which is the required extremal. Why these two process give different result ? Which is the correct answer and why ? Please explain properly.",,"['ordinary-differential-equations', 'partial-differential-equations', 'calculus-of-variations', 'integral-equations']"
36,Changing dependent variable in 2nd order ODE,Changing dependent variable in 2nd order ODE,,"Consider the 2nd order ODE $\frac{d^2 u}{dx^2} + \left( \gamma + \delta x + \epsilon x^2 \right) \frac{du}{dx} + \alpha \left( x - x_0\right) u = 0$. If we change the dependent function to $w = \exp \left( \gamma x + \tfrac{1}{2} \delta x^2 + \tfrac{1}{3} \epsilon x^3 \right) \frac{du}{dx} $, then $w$ satisfies $\frac{d^2 w}{dx^2} - \left( \gamma + \delta x + \epsilon x^2 + \frac{1}{x-x_0} \right) \frac{dw}{dx} + \alpha \left( x - x_0\right) w = 0$. How to prove this? Naively, I get a third order ODE :-(","Consider the 2nd order ODE $\frac{d^2 u}{dx^2} + \left( \gamma + \delta x + \epsilon x^2 \right) \frac{du}{dx} + \alpha \left( x - x_0\right) u = 0$. If we change the dependent function to $w = \exp \left( \gamma x + \tfrac{1}{2} \delta x^2 + \tfrac{1}{3} \epsilon x^3 \right) \frac{du}{dx} $, then $w$ satisfies $\frac{d^2 w}{dx^2} - \left( \gamma + \delta x + \epsilon x^2 + \frac{1}{x-x_0} \right) \frac{dw}{dx} + \alpha \left( x - x_0\right) w = 0$. How to prove this? Naively, I get a third order ODE :-(",,"['calculus', 'ordinary-differential-equations']"
37,Solving $(y'+1)\ln{\frac{x+y}{x+3}} = \frac{x+y}{x+3}$,Solving,(y'+1)\ln{\frac{x+y}{x+3}} = \frac{x+y}{x+3},"Hello everyone :) This is another task I'm trying to solve and can't seem to get the same result as Wolfram Alpha. Solve the following differential equation: $$(y'+1)\ln{\frac{x+y}{x+3}} = \frac{x+y}{x+3}$$ My attempt: Use substitution $$e^z = \frac{x+y}{x+3}$$ and  $$y=e^z(x+3)-x$$ $$y=xe^z+3e^z-x$$  and $$y'=e^z+z'xe^z+3z'e^z-1$$ Hence: $$ (e^z+z'xe^z+3z'e^z)z = e^z $$ Deviding by $e^z$ $$ z+zz'x+3zz' = 1$$ $$ 1+z'x+3z' = \frac{1}{z} $$ $$ z'(x+3)= \frac{1-z}{z} $$ $$ \int \frac{z}{1-z}dz = \int \frac{1}{x+3} dx $$ $$ -\ln{\frac{x+y}{x+3}} - \ln{\bigg|1-\ln{\frac{x+y}{x+3}} \bigg|=\ln{(x+3)}} $$ EDIT: After fixing the solution, I got the above solution. But still, this solution does not correspond to that on Wolfram Alpha nor can i  find a way to transform them to be equal.","Hello everyone :) This is another task I'm trying to solve and can't seem to get the same result as Wolfram Alpha. Solve the following differential equation: $$(y'+1)\ln{\frac{x+y}{x+3}} = \frac{x+y}{x+3}$$ My attempt: Use substitution $$e^z = \frac{x+y}{x+3}$$ and  $$y=e^z(x+3)-x$$ $$y=xe^z+3e^z-x$$  and $$y'=e^z+z'xe^z+3z'e^z-1$$ Hence: $$ (e^z+z'xe^z+3z'e^z)z = e^z $$ Deviding by $e^z$ $$ z+zz'x+3zz' = 1$$ $$ 1+z'x+3z' = \frac{1}{z} $$ $$ z'(x+3)= \frac{1-z}{z} $$ $$ \int \frac{z}{1-z}dz = \int \frac{1}{x+3} dx $$ $$ -\ln{\frac{x+y}{x+3}} - \ln{\bigg|1-\ln{\frac{x+y}{x+3}} \bigg|=\ln{(x+3)}} $$ EDIT: After fixing the solution, I got the above solution. But still, this solution does not correspond to that on Wolfram Alpha nor can i  find a way to transform them to be equal.",,"['real-analysis', 'ordinary-differential-equations']"
38,"Need help solving system of ODEs $\frac{dx}{dt}=2y-z$, $\frac{dy}{dt}=3x-2z$, $\frac{dz}{dt}=5x-4y$","Need help solving system of ODEs , ,",\frac{dx}{dt}=2y-z \frac{dy}{dt}=3x-2z \frac{dz}{dt}=5x-4y,"Solve the following ODE system: $$\begin{array}{ld} \dfrac{dx}{dt}&=2y-z\\ \dfrac{dy}{dt}&=3x-2z\\ \dfrac{dz}{dt}&=5x-4y \end{array}$$ I tried solving it, but I have a feeling that the problem is wrong, but still maybe I just can't solve it.","Solve the following ODE system: $$\begin{array}{ld} \dfrac{dx}{dt}&=2y-z\\ \dfrac{dy}{dt}&=3x-2z\\ \dfrac{dz}{dt}&=5x-4y \end{array}$$ I tried solving it, but I have a feeling that the problem is wrong, but still maybe I just can't solve it.",,['ordinary-differential-equations']
39,Getting 0 solving Schrodinger equation with Dirac delta by Fourier transform,Getting 0 solving Schrodinger equation with Dirac delta by Fourier transform,,"I am attempting to solve the Schrödinger equation with the potential $V = - \delta (x)$. This leads to a differential equation $$ \alpha \psi''(x) + (E + \delta(x)) \psi(x) = 0 $$ where $$ \alpha \equiv \frac{\hbar^2}{2m} $$ My instinct to solve the equation was to use an integral transform as I didn't know what else to do with the delta function. Since I am concerned with all space, I chose the Fourier transform. To get the Fourier transform, I transform each term, $\alpha \psi''(x)$, $E \psi(x)$, $\delta(x) \psi(x)$, and $0$. I get these transforms: \begin{align} &\mathcal{F}\{ \alpha \psi''(x) \} = -4\alpha \pi^2 p^2 \hat{\psi}(p)\\ &\mathcal{F}\{ E \psi(x) \} = E \hat{\psi}(p) \\ &\mathcal{F}\{ \delta(x)\psi(x) \} = \hat{\psi}(p) \\ &\mathcal{F} \{0 \} = 0 \end{align} Where $\hat{\psi}(p)$ is the Fourier transform of $\psi(x)$. Unfortunately, when I combine this with the differential equation above, I get $$ (1 + E -4 \alpha \pi^2 p^2) \hat{\psi}(p) = 0 $$ which results in the $\hat{\psi}(p)$ term going away and leaving me with a function only of $p$, so I am not able to solve for $\psi(x)$ via an inverse Fourier transform like I expected. I did replace the right hand side with an arbitrary function and work through it, but that results in $\psi(x) = 0$ is that function is $0$. Is there something that I have done incorrectly in my approach? Is there a better approach to solving this equation? My professor and book both solved this question by solving on $(-\infty, 0)$ and $(0, \infty)$ and determining the condition at the origin by integrating the Schrödinger equation over $(-\epsilon, \epsilon)$, but I was hoping to be able to solve it in a more general way and plan to move to a Dirac comb next, which should be very similar using Fourier transformations.","I am attempting to solve the Schrödinger equation with the potential $V = - \delta (x)$. This leads to a differential equation $$ \alpha \psi''(x) + (E + \delta(x)) \psi(x) = 0 $$ where $$ \alpha \equiv \frac{\hbar^2}{2m} $$ My instinct to solve the equation was to use an integral transform as I didn't know what else to do with the delta function. Since I am concerned with all space, I chose the Fourier transform. To get the Fourier transform, I transform each term, $\alpha \psi''(x)$, $E \psi(x)$, $\delta(x) \psi(x)$, and $0$. I get these transforms: \begin{align} &\mathcal{F}\{ \alpha \psi''(x) \} = -4\alpha \pi^2 p^2 \hat{\psi}(p)\\ &\mathcal{F}\{ E \psi(x) \} = E \hat{\psi}(p) \\ &\mathcal{F}\{ \delta(x)\psi(x) \} = \hat{\psi}(p) \\ &\mathcal{F} \{0 \} = 0 \end{align} Where $\hat{\psi}(p)$ is the Fourier transform of $\psi(x)$. Unfortunately, when I combine this with the differential equation above, I get $$ (1 + E -4 \alpha \pi^2 p^2) \hat{\psi}(p) = 0 $$ which results in the $\hat{\psi}(p)$ term going away and leaving me with a function only of $p$, so I am not able to solve for $\psi(x)$ via an inverse Fourier transform like I expected. I did replace the right hand side with an arbitrary function and work through it, but that results in $\psi(x) = 0$ is that function is $0$. Is there something that I have done incorrectly in my approach? Is there a better approach to solving this equation? My professor and book both solved this question by solving on $(-\infty, 0)$ and $(0, \infty)$ and determining the condition at the origin by integrating the Schrödinger equation over $(-\epsilon, \epsilon)$, but I was hoping to be able to solve it in a more general way and plan to move to a Dirac comb next, which should be very similar using Fourier transformations.",,"['ordinary-differential-equations', 'fourier-analysis', 'dirac-delta']"
40,"How does the PDE $\,\dfrac{d^2u}{dx^2} = 0\,$ become $\,u=x\,f(y)+g(y)\,$ when integrated?",How does the PDE  become  when integrated?,"\,\dfrac{d^2u}{dx^2} = 0\, \,u=x\,f(y)+g(y)\,","Given that $u(x,y)$ can someone please explain to me how the result as asked in the question is achieved? Steps would be really appreciated, thanks.","Given that $u(x,y)$ can someone please explain to me how the result as asked in the question is achieved? Steps would be really appreciated, thanks.",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
41,System of Differential Equations- Asymmetric First-Price Auction,System of Differential Equations- Asymmetric First-Price Auction,,"I am working on a problem in my Auction Theory textbook regarding a two-player asymmetric first price auction. Assume the bidders are risk neutral. The problem statement is as follows: Suppose that bidder $1$'s value $X_{1}$ is distributed according to $F_{1}(x) = \frac{1}{4}(x-1)^{2}$ over $[1, 3]$, and bidder $2$'s value is distributed according to $\text{exp}(\frac{2}{3}x - 2)$ over $[0, 3]$. Show that $\beta_{1}(x) = x - 1$ and $\beta_{2}(x) = \frac{2}{3}x$ constitute equilibrium bidding strategies in a first price auction. I am trying to work on deriving $\beta_{1}$ and $\beta_{2}$. Unfortunately, my knowledge of differential equations isn't terribly strong. Would someone be able to double check my work and let me know if I have logic errors? I have derived the correct bidding functions, but am not entirely confident my work is sound. First, suppose the equilibrium bidding functions $\beta_{1} : [1, 3] \to \mathbb{R}_{+}, \beta_{2} : [0, 3] \to \mathbb{R}_{+}$ are strictly increasing and differentiable. Define $g_{1}(x) = \beta_{1}^{-1}(x)$ and $g_{2}(x) = \beta_{2}^{-1}(x)$. Player $i$ with valuation $v$ can only vary his bid, so he seeks to find the optimal bid given by the optimization problem below. $$\max_{b} F_{-i}(g_{-i}(b)) \cdot (v - b)$$ We consider the First Order Conditions: $$F_{-i}(g_{-i}(b)) = \dfrac{f_{-i}(g_{-i}(b))}{\beta_{-i}^{\prime}(g_{-i}(b))} \cdot (v-b)$$ At equilibrium, $v = g_{i}(b)$. Applying this and noting $\dfrac{1}{\beta_{-i}^{\prime}(g_{-i}(b))} = (g_{-i}(b))^{\prime}$, we have: $$(g_{-i}(b))^{\prime} = \dfrac{F_{-i}(g_{-i}(b))}{f_{-i}(g_{-i}(b))} \cdot \dfrac{1}{g_{i}(b) - b}$$ Plugging in each $F_{i}$, we obtain: $$g_{2}^{\prime}(b) = \dfrac{3}{2} \cdot \dfrac{1}{g_{1}(b) - b}$$ And: $$g_{1}^{\prime}(b) = \dfrac{1}{2} \cdot \dfrac{g_{1}(b) - 1}{g_{2}(b) - b}$$ At equilibrium, we have $\beta_{1}(3) = \beta_{2}(3)$. By individual rationality, $\beta_{2}(0) = 0 \implies g_{2}(0) = 0$. While I could obviously use the problem statement that $\beta_{1}(x) = x - 1$ to conclude that $g_{1}(0) = 1$, I don't know how to justify this boundary condition independently. Does anyone have any insights into this? Assuming this boundary condition though, I note: $$g_{2}^{\prime}(0) = \dfrac{3}{2} \cdot \dfrac{1}{1 - 0} = \dfrac{3}{2}$$ From here, I can wave my hand and guess that $g_{2}^{\prime}(b) = \dfrac{3}{2}$, which would imply $g_{2}(b) = \dfrac{3}{2}b$. I'm not sure how to formally derive this though. Would anyone have insights into this? Once I have $g_{2}(b) = \dfrac{3}{2}b$, I can plug into $g_{1}^{\prime}(b)$ to get: $$g_{1}^{\prime}(b) = \dfrac{1}{2} \cdot \dfrac{g_{1}(b) - 1}{\dfrac{3}{2}b - b} = \dfrac{g_{1}(b) - 1}{b}$$ Which is a first order linear differential equation, whose solution is: $g_{1}(b) = b + 1 \implies \beta_{1}(v) = v - 1$. And we have $\beta_{2}(v) = \dfrac{2}{3}v$. My work is certainly a little hand-wavy. I would greatly appreciate any help in solidifying the details. Thank you in advance for any help!","I am working on a problem in my Auction Theory textbook regarding a two-player asymmetric first price auction. Assume the bidders are risk neutral. The problem statement is as follows: Suppose that bidder $1$'s value $X_{1}$ is distributed according to $F_{1}(x) = \frac{1}{4}(x-1)^{2}$ over $[1, 3]$, and bidder $2$'s value is distributed according to $\text{exp}(\frac{2}{3}x - 2)$ over $[0, 3]$. Show that $\beta_{1}(x) = x - 1$ and $\beta_{2}(x) = \frac{2}{3}x$ constitute equilibrium bidding strategies in a first price auction. I am trying to work on deriving $\beta_{1}$ and $\beta_{2}$. Unfortunately, my knowledge of differential equations isn't terribly strong. Would someone be able to double check my work and let me know if I have logic errors? I have derived the correct bidding functions, but am not entirely confident my work is sound. First, suppose the equilibrium bidding functions $\beta_{1} : [1, 3] \to \mathbb{R}_{+}, \beta_{2} : [0, 3] \to \mathbb{R}_{+}$ are strictly increasing and differentiable. Define $g_{1}(x) = \beta_{1}^{-1}(x)$ and $g_{2}(x) = \beta_{2}^{-1}(x)$. Player $i$ with valuation $v$ can only vary his bid, so he seeks to find the optimal bid given by the optimization problem below. $$\max_{b} F_{-i}(g_{-i}(b)) \cdot (v - b)$$ We consider the First Order Conditions: $$F_{-i}(g_{-i}(b)) = \dfrac{f_{-i}(g_{-i}(b))}{\beta_{-i}^{\prime}(g_{-i}(b))} \cdot (v-b)$$ At equilibrium, $v = g_{i}(b)$. Applying this and noting $\dfrac{1}{\beta_{-i}^{\prime}(g_{-i}(b))} = (g_{-i}(b))^{\prime}$, we have: $$(g_{-i}(b))^{\prime} = \dfrac{F_{-i}(g_{-i}(b))}{f_{-i}(g_{-i}(b))} \cdot \dfrac{1}{g_{i}(b) - b}$$ Plugging in each $F_{i}$, we obtain: $$g_{2}^{\prime}(b) = \dfrac{3}{2} \cdot \dfrac{1}{g_{1}(b) - b}$$ And: $$g_{1}^{\prime}(b) = \dfrac{1}{2} \cdot \dfrac{g_{1}(b) - 1}{g_{2}(b) - b}$$ At equilibrium, we have $\beta_{1}(3) = \beta_{2}(3)$. By individual rationality, $\beta_{2}(0) = 0 \implies g_{2}(0) = 0$. While I could obviously use the problem statement that $\beta_{1}(x) = x - 1$ to conclude that $g_{1}(0) = 1$, I don't know how to justify this boundary condition independently. Does anyone have any insights into this? Assuming this boundary condition though, I note: $$g_{2}^{\prime}(0) = \dfrac{3}{2} \cdot \dfrac{1}{1 - 0} = \dfrac{3}{2}$$ From here, I can wave my hand and guess that $g_{2}^{\prime}(b) = \dfrac{3}{2}$, which would imply $g_{2}(b) = \dfrac{3}{2}b$. I'm not sure how to formally derive this though. Would anyone have insights into this? Once I have $g_{2}(b) = \dfrac{3}{2}b$, I can plug into $g_{1}^{\prime}(b)$ to get: $$g_{1}^{\prime}(b) = \dfrac{1}{2} \cdot \dfrac{g_{1}(b) - 1}{\dfrac{3}{2}b - b} = \dfrac{g_{1}(b) - 1}{b}$$ Which is a first order linear differential equation, whose solution is: $g_{1}(b) = b + 1 \implies \beta_{1}(v) = v - 1$. And we have $\beta_{2}(v) = \dfrac{2}{3}v$. My work is certainly a little hand-wavy. I would greatly appreciate any help in solidifying the details. Thank you in advance for any help!",,"['ordinary-differential-equations', 'game-theory', 'economics', 'auction-theory']"
42,Laplace Transform of a Heaviside function,Laplace Transform of a Heaviside function,,"Find the Laplace transform. $$g(t)= (t-1) u_1(t) - 2(t-2) u_2(t) + (t-3) u_3(t)$$ I understand that the $\mathcal{L}\{u_c(t) f(t-c)\} = e^{-cs}*F(s)$ Finding $F(s)$ is the hard part for me. My professor has used, for example,  $$f(t-2)=t^2$$ let $$s = t-2$$ $$t= s+2$$ $$f(s) = (s+2)^2$$ therefore $f(t) = (t+2)^2$ But then he said that $f(t-2) = 1$ therefore $f(t) = 1$. But why/how? By the previous logic if you let $s = t-2$ then $t= s+2$, and $f(s) = s+2$, so $f(t) = t+2$ not $1$. I'm having a tough time figuring this out.","Find the Laplace transform. $$g(t)= (t-1) u_1(t) - 2(t-2) u_2(t) + (t-3) u_3(t)$$ I understand that the $\mathcal{L}\{u_c(t) f(t-c)\} = e^{-cs}*F(s)$ Finding $F(s)$ is the hard part for me. My professor has used, for example,  $$f(t-2)=t^2$$ let $$s = t-2$$ $$t= s+2$$ $$f(s) = (s+2)^2$$ therefore $f(t) = (t+2)^2$ But then he said that $f(t-2) = 1$ therefore $f(t) = 1$. But why/how? By the previous logic if you let $s = t-2$ then $t= s+2$, and $f(s) = s+2$, so $f(t) = t+2$ not $1$. I'm having a tough time figuring this out.",,"['ordinary-differential-equations', 'laplace-transform']"
43,Solve the following differential equation $ u_{xx}-m^2u=\delta(x-x_0)$,Solve the following differential equation, u_{xx}-m^2u=\delta(x-x_0),"Find the solution of following equation $$ u_{xx}-m^2u=\delta(x-x_0),$$ $u(0)=0=u(L),\ x\in\mathbb R^2$ Actually, I don't know how to solve. Is there someone to help?","Find the solution of following equation $$ u_{xx}-m^2u=\delta(x-x_0),$$ $u(0)=0=u(L),\ x\in\mathbb R^2$ Actually, I don't know how to solve. Is there someone to help?",,['ordinary-differential-equations']
44,Completeness of the vector field $e^{-x} \frac{\partial}{\partial x} + \frac{\partial}{\partial y}$,Completeness of the vector field,e^{-x} \frac{\partial}{\partial x} + \frac{\partial}{\partial y},"I just want to bounce this off of the smart people on MSE to make sure I understand what's going on when we discuss complete vector fields. Consider the following field. $X = e^{-x} \frac{\partial}{\partial x} + \frac{\partial}{\partial y}$. We would like to determine if this vector field is complete. To do so, we solve the system of ODEs defined by it, and check to see if all the integral curves are defined for all $t$. The system of ODEs is $\frac{dx}{dt} = e^{-x}$, and $\frac{dy}{dt} = 1$.  Solving the second equation first it is easy to observe that $y=t+k$. On the other hand, the first equation implies that $e^x dx = dt$ and hence that $e^x = t +c$, or that $x = \ln (t+c)$. But this is not defined for some $t$. Am I correct in interpreting this to say that the integral curves can be thought of as tracing out the paths $e^x +c$ (of course the rate at which these curves are traced out goes down with $t)$? I believe that this vector field is not complete because this notion does not make sense for negative $t$ as we observed before. If I'm completely off base, how should I think about completness of vector fields, and how to check it? edit: corrected some typos as pointed out in comments/answers.","I just want to bounce this off of the smart people on MSE to make sure I understand what's going on when we discuss complete vector fields. Consider the following field. $X = e^{-x} \frac{\partial}{\partial x} + \frac{\partial}{\partial y}$. We would like to determine if this vector field is complete. To do so, we solve the system of ODEs defined by it, and check to see if all the integral curves are defined for all $t$. The system of ODEs is $\frac{dx}{dt} = e^{-x}$, and $\frac{dy}{dt} = 1$.  Solving the second equation first it is easy to observe that $y=t+k$. On the other hand, the first equation implies that $e^x dx = dt$ and hence that $e^x = t +c$, or that $x = \ln (t+c)$. But this is not defined for some $t$. Am I correct in interpreting this to say that the integral curves can be thought of as tracing out the paths $e^x +c$ (of course the rate at which these curves are traced out goes down with $t)$? I believe that this vector field is not complete because this notion does not make sense for negative $t$ as we observed before. If I'm completely off base, how should I think about completness of vector fields, and how to check it? edit: corrected some typos as pointed out in comments/answers.",,"['ordinary-differential-equations', 'differential-geometry', 'vector-fields']"
45,Integrating over a somewhat continuous function,Integrating over a somewhat continuous function,,"I have a function $q(t)$ that starts at $q(0)=q_0$ and needs to get to $q(1)=q_1>q_0$. I have a free parameter $z$ that I can wiggle around to control $q'(t)$. Namely, I have a function $$q'(t)=Q(t,q(t),z)>0.$$ I know that $\forall t$, $\forall q(t)$, $\lim_{z \rightarrow -\infty}Q(t,q(t),z)=0$, and $\lim_{z \rightarrow +\infty}Q(t,q(t),z)=+\infty$. I know that $Q(t,q(t),z)$ is increasing in $z$. 1) Do I understand correctly that, if $Q(t,q(t),z)$ is continuous in $z$, I can always find $z$ such that I will hit exactly $q(1)=q_1$? Is it obvious, or some sort of a proof is in order? 2) My $Q(t,q(t),z)$ is not continuous in $z$, but the jumps, if they happen, only happen upwards, and they are not ""frequent"" in the sense that if there is a jump for a given $z$ at a given $t$ and $q(t)$, there is no jump for all $z$ in a small neighborhood of my $z$ for the same $t$ and $q(t)$. Can I still argue that I can always hit $q(1)=q_1$? I mean, my $Q(t,q(t),z)$ is still continuous where there are no jumps... 2.1) Would it help if I could impose that, generically, if $Q(t,q(t),z)$ has a jump at $z$ for a given $t$ and $q(t)$, $Q(t,q(t)+\varepsilon,z)$ is locally continuous at the same $z$? 3) My $Q()$ turns out to be continuously differentiable, except for the points where it jumps. That probably implies Lipschitz continuity almost everywhere, which guarantees the existence of the solution to the initial value problem if I start from $q(0)=q_0$? upd I can probably only care about $q\in[q_0+\infty)$, in which case my $Q()$ is bounded, and therefore continuous differentiability in $q$, which I have, is sufficient for being Lipschitz? 4) This guy ( http://www.math.washington.edu/~burke/crs/555/555_notes/continuity.pdf , p23) says that for continuity in $z$, my $Q$ should have a Lipschitz constant independent of $z$. Would boundedness and continuous differentiability suffice to get a Lipschitz constant independent of z?","I have a function $q(t)$ that starts at $q(0)=q_0$ and needs to get to $q(1)=q_1>q_0$. I have a free parameter $z$ that I can wiggle around to control $q'(t)$. Namely, I have a function $$q'(t)=Q(t,q(t),z)>0.$$ I know that $\forall t$, $\forall q(t)$, $\lim_{z \rightarrow -\infty}Q(t,q(t),z)=0$, and $\lim_{z \rightarrow +\infty}Q(t,q(t),z)=+\infty$. I know that $Q(t,q(t),z)$ is increasing in $z$. 1) Do I understand correctly that, if $Q(t,q(t),z)$ is continuous in $z$, I can always find $z$ such that I will hit exactly $q(1)=q_1$? Is it obvious, or some sort of a proof is in order? 2) My $Q(t,q(t),z)$ is not continuous in $z$, but the jumps, if they happen, only happen upwards, and they are not ""frequent"" in the sense that if there is a jump for a given $z$ at a given $t$ and $q(t)$, there is no jump for all $z$ in a small neighborhood of my $z$ for the same $t$ and $q(t)$. Can I still argue that I can always hit $q(1)=q_1$? I mean, my $Q(t,q(t),z)$ is still continuous where there are no jumps... 2.1) Would it help if I could impose that, generically, if $Q(t,q(t),z)$ has a jump at $z$ for a given $t$ and $q(t)$, $Q(t,q(t)+\varepsilon,z)$ is locally continuous at the same $z$? 3) My $Q()$ turns out to be continuously differentiable, except for the points where it jumps. That probably implies Lipschitz continuity almost everywhere, which guarantees the existence of the solution to the initial value problem if I start from $q(0)=q_0$? upd I can probably only care about $q\in[q_0+\infty)$, in which case my $Q()$ is bounded, and therefore continuous differentiability in $q$, which I have, is sufficient for being Lipschitz? 4) This guy ( http://www.math.washington.edu/~burke/crs/555/555_notes/continuity.pdf , p23) says that for continuity in $z$, my $Q$ should have a Lipschitz constant independent of $z$. Would boundedness and continuous differentiability suffice to get a Lipschitz constant independent of z?",,"['integration', 'ordinary-differential-equations', 'definite-integrals', 'continuity', 'boundary-value-problem']"
46,Unique solution of Volterra integral equation of second kind,Unique solution of Volterra integral equation of second kind,,"Dear Maths Stackexchange, In the context of a physics problem, I am looking at a linear integral equation, more specifically a 2nd kind Volterra equation in the unknown $g(t)$: \begin{equation} \phantom{texttexttexttex}g(t) = f(t) + \int_0^t K(t,s) g(s) \mathrm{d}s \phantom{texttexttexttex}(1) \end{equation} The independent variable $t\geq0$ always. Now, the problem I have is that in my case $K(t,s)$ has the form $K(t,s)=K(t)K(s)$ where $K(t)$ is a (complicated) known function that has a pole of order 2 at $t=0$ (i.e. $t^2K(t)\to K_o<\infty$ as $t\to0$) but is smooth everywhere else. By contrast, the function $K(s)=s$ and $f(t)$ is smooth everywhere on $[0,\infty]$ and satisfies $\lim\limits_{t\to 0} f(t) = 0$. Because of this, $K(t,s)$ is not in $L^2([0,\epsilon]\times[0,\epsilon])$. Now, an easy way of solving an integral equation of type (1) is by converting it into an initial value problem. If I differentiate (1) w.r.t. after dividing by $K(t)$, then I find the first order linear ODE, \begin{equation} \phantom{texttext}g'(t) + \left(\frac{\mathrm{d}}{\mathrm{d}t} \log(\tilde{K}(t)) - tK(t)\right)g(t) = \tilde{f}'(t)K(t)\phantom{tetextxt}(2) \end{equation} where $\tilde{K}=\frac{1}{K(t)}$ and $\tilde{f}(t)=\frac{f(t)}{K(t)}$. This equation I can solve exactly, but I need an initial condition to do so. And here lies my problem. My first try would have been to require $g(0)=0$ as an intial condition. However, upon inspection of the coefficients at $t=0$, we see that they are singular there. Therefore, imposing any initial condition at $t=0$ fails. Now this leads me to the question: Does (1) have a unique solution despite the pole in $K(t)$ and which initial condition for $g(t)$ does this unique solution correspond to in (2)? Since the differential equation (2) follows from the integral equation (1), any equation of (1) must satisfy (2), but if there was a unique solution of (1), then obviously only one single out of the many solutions of (2) would correspond to that unique solution of (1). Can you  help me sort out my confusion? I would like to use the ODE formulation of the problem because it allows an easy analytical solution. Best regards Edit: clarified some ambiguities.","Dear Maths Stackexchange, In the context of a physics problem, I am looking at a linear integral equation, more specifically a 2nd kind Volterra equation in the unknown $g(t)$: \begin{equation} \phantom{texttexttexttex}g(t) = f(t) + \int_0^t K(t,s) g(s) \mathrm{d}s \phantom{texttexttexttex}(1) \end{equation} The independent variable $t\geq0$ always. Now, the problem I have is that in my case $K(t,s)$ has the form $K(t,s)=K(t)K(s)$ where $K(t)$ is a (complicated) known function that has a pole of order 2 at $t=0$ (i.e. $t^2K(t)\to K_o<\infty$ as $t\to0$) but is smooth everywhere else. By contrast, the function $K(s)=s$ and $f(t)$ is smooth everywhere on $[0,\infty]$ and satisfies $\lim\limits_{t\to 0} f(t) = 0$. Because of this, $K(t,s)$ is not in $L^2([0,\epsilon]\times[0,\epsilon])$. Now, an easy way of solving an integral equation of type (1) is by converting it into an initial value problem. If I differentiate (1) w.r.t. after dividing by $K(t)$, then I find the first order linear ODE, \begin{equation} \phantom{texttext}g'(t) + \left(\frac{\mathrm{d}}{\mathrm{d}t} \log(\tilde{K}(t)) - tK(t)\right)g(t) = \tilde{f}'(t)K(t)\phantom{tetextxt}(2) \end{equation} where $\tilde{K}=\frac{1}{K(t)}$ and $\tilde{f}(t)=\frac{f(t)}{K(t)}$. This equation I can solve exactly, but I need an initial condition to do so. And here lies my problem. My first try would have been to require $g(0)=0$ as an intial condition. However, upon inspection of the coefficients at $t=0$, we see that they are singular there. Therefore, imposing any initial condition at $t=0$ fails. Now this leads me to the question: Does (1) have a unique solution despite the pole in $K(t)$ and which initial condition for $g(t)$ does this unique solution correspond to in (2)? Since the differential equation (2) follows from the integral equation (1), any equation of (1) must satisfy (2), but if there was a unique solution of (1), then obviously only one single out of the many solutions of (2) would correspond to that unique solution of (1). Can you  help me sort out my confusion? I would like to use the ODE formulation of the problem because it allows an easy analytical solution. Best regards Edit: clarified some ambiguities.",,"['calculus', 'integration', 'ordinary-differential-equations']"
47,Finding a Lyapunov Function for a system involving a trigonometric function,Finding a Lyapunov Function for a system involving a trigonometric function,,"I'm dealing with determining if $(0,0)$ is stable or not for the following system via constructing a Lyapunov function. The system is $$ \begin{cases}        x'(t)=(1-x)y+x^2\sin{(x)}& \\        y'(t)=-(1-x)x+y^2\sin{(y)}&     \end{cases} $$ My initial guess was to choose $V(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$, however this leads to $\dot{V}=x^3 \sin{x}+y^3 \sin{y}$, which, for small $x$ and $y$, is positive. However, this does not agree with numerical evidence and also looking at the linearization method, which shows that the origin is indeed a stable node. Might someone have a suggestion for a Lyapunov function, as well as the domain to choose for it? I suppose I might as well ask if it would be valid to approximate the sine terms by its argument since we would be looking at a small neighborhood around the origin. EDIT: Here is a streamplot of the system in a neighborhood of the origin,it appears that the origin is unstable, so I guess I was wrong with my initial assumption. I guess this agrees with my choosing of a Lyapunov function because the function is positive for all x and y(except at the origin), implying instability. EDIT2: After thinking for a little bit, the Lyapunov function  $V(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$ work, with the domain $\Omega = \{ (x,y)\in \mathbb{R}^2 \vert -\pi < x < \pi$ and $-\pi < y <\pi \}$ so that $\dot{V}(x,y)>0$ in $\Omega$. This establishes instability of the origin.","I'm dealing with determining if $(0,0)$ is stable or not for the following system via constructing a Lyapunov function. The system is $$ \begin{cases}        x'(t)=(1-x)y+x^2\sin{(x)}& \\        y'(t)=-(1-x)x+y^2\sin{(y)}&     \end{cases} $$ My initial guess was to choose $V(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$, however this leads to $\dot{V}=x^3 \sin{x}+y^3 \sin{y}$, which, for small $x$ and $y$, is positive. However, this does not agree with numerical evidence and also looking at the linearization method, which shows that the origin is indeed a stable node. Might someone have a suggestion for a Lyapunov function, as well as the domain to choose for it? I suppose I might as well ask if it would be valid to approximate the sine terms by its argument since we would be looking at a small neighborhood around the origin. EDIT: Here is a streamplot of the system in a neighborhood of the origin,it appears that the origin is unstable, so I guess I was wrong with my initial assumption. I guess this agrees with my choosing of a Lyapunov function because the function is positive for all x and y(except at the origin), implying instability. EDIT2: After thinking for a little bit, the Lyapunov function  $V(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$ work, with the domain $\Omega = \{ (x,y)\in \mathbb{R}^2 \vert -\pi < x < \pi$ and $-\pi < y <\pi \}$ so that $\dot{V}(x,y)>0$ in $\Omega$. This establishes instability of the origin.",,"['real-analysis', 'ordinary-differential-equations', 'stability-in-odes']"
48,Using Multiple Scale Analysis to solve a non-linear differential equation,Using Multiple Scale Analysis to solve a non-linear differential equation,,"I would like to know if there are other methods to solve equations such as this one below. I don't really understand the theory behind the multiple scale analysis and why it works I understand some of the reasoning behind it just not the proof and theory. I kind of chose a differential equation for which I think it works and tried it out following a example from a similar equation. I think the answer seems strange but I was very thorough in my calculations. I welcome any comments or advice to problems like these thanks! \begin{equation}u^{''} +\omega_0^2u=-2\epsilon \mu u^{'}-\epsilon u^2 u^{'} + \epsilon k \cos \Omega t\end{equation} First we introduce two time variables (fast and slow) and define them as \begin{equation}T_0=t\;\;\;T_1=\epsilon t\end{equation} where $T_0$ is the fast time scale and $T_1$ is the slow time scale. The first order expansion of the problem is \begin{equation}u(t)=u_0(T_0,T_1)+\epsilon u_1(T_0,T_1)\end{equation} Differentiating the time derivative becomes \begin{align} \frac{du}{dt}=\frac{\partial u_0}{\partial T_0} \frac{dT_0}{dt}(1)+(\epsilon) \frac{\partial u_1}{\partial T_1} \frac{dT_1}{dt}=\frac{\partial u_0}{\partial T_0}+\epsilon \frac{\partial u_1}{\partial T_1} \end{align}  Define the linear operators, and functions \begin{equation} D_0=\dfrac{\partial}{\partial T_0},\hspace{10pt} D_1=\dfrac{\partial}{\partial T_1}\end{equation} The time derivative becomes $\dfrac{d}{dt}=D_0+\epsilon D_1$. The second order time derivative is then expressed as  \begin{equation}\frac{d^2}{dt^2}=(D_0+\epsilon D_1)^2=D_0^2+2D_0D_1 \epsilon +\epsilon ^2 D_1^2\end{equation} The $\epsilon ^2$ term is neglected. We are now ready to substitute our results in the original equation to obtain,  \pagebreak $$(D_0^2+2\epsilon D_0 D_1)(u_0+\epsilon u_1)+\omega^2_0(u_0+\epsilon u_1)=-2\epsilon \mu (D_0+ \epsilon D_1)(u_0+\epsilon u_1)-$$$$\epsilon (u_0+\epsilon u_1)^2(D_0+\epsilon D_1)(u_0+\epsilon u_1) + \epsilon k \cos \Omega t$$ $$\implies (D_0^2u_0+\epsilon D^2_0  u_1+2\epsilon D_1D_0 u_0 +2\epsilon ^2D_0 D_1 u_1^2)+\omega_0^2u_0+\epsilon \omega_0^2  u_1=-2 \epsilon \mu D_0u_0-2\epsilon^2\mu (...)$$ $$-\epsilon u^2_0D_0u_0+\epsilon^2(...)+\epsilon k \cos \Omega t $$ \begin{equation}\implies D_0 u_0+ \epsilon D_0 u_1 +2 \epsilon D_1 D_0 u_0+ \omega_0^2 u_0+\epsilon \omega_0^2 u_1=-2\epsilon \mu D_0 u_0-\epsilon u_0^2D_0u_0+\epsilon k \cos \Omega t\end{equation} Now we construct a system of differential equations in powers of $\epsilon$ called zero-order and first order problems and given by, \begin{align} D_0^2u_0+\omega_0^2u_0=0\end{align} \begin{align} D^2u_1 +\omega_0^2u_1=-2D_1D_0 u_0-2\mu D_0u_0-u_0^2D_0u_0+k \cos \Omega t \end{align} The first equation is a second order linear, homogeneous equation with constant coefficients and has general solution  \begin{equation}u_0(T_0,T_1)=A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i\omega_0T_0}\end{equation}  where $A(T_1)$ is the complex amplitude afterwards. Our next step is to investigate primary resonance of the system, which occurs when the actuation frequency $\Omega$ is near the natural frequency, $\omega_0$. This can be written as  \begin{align}\Omega=\omega_0+\epsilon \sigma\\ \Omega T_0=\omega T_0 +\epsilon T_0 \sigma\\\Omega t= \omega T_0+T_1\sigma \end{align} We substitute $u_0$ into the equation (2) to get,  \begin{equation*}D_0^2u_1+\omega_0^2u_1= -2D_0D_1[A(T_1)e^{i \omega_0 T_0}+\bar{A}(T_1)e^{-i \omega T_0}]-2\mu D_0[A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i \omega T_0}]- \end{equation*}\begin{equation} [A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i\omega T_0}]^2D_0u_0+k\frac{e^{i \omega_0 T_0}e^{i\sigma T_1}+e^{-i\omega_0T_1}e^{-i\sigma T_1}}{2}\end{equation} Thus applying the linear operations to the above to get $$-2[A^{'}e^{i\omega_0T_0}(i\omega_0)+\bar{A}^{'}e^{-i\omega_0T_0}(-i\omega_0)]-2\mu[Ae^{i\omega_0 T_0}(i\omega_0)+\bar{A}e^{-i\omega_0T_0}(-i\omega_0)]+$$$$\bigg(-A^2e^{2i\omega_0T_0}-2A\bar{A}-\bar{A}^2e^{-2i\omega T_0}\bigg)\bigg(Ae^{i\omega_0T_0}(i\omega_0)-\bar{A}e^{-i\omega_0T_0}(i\omega_0)\bigg)+\frac{k}{2}\bigg(e^{i(\omega T_0+\sigma T_1)}+e^{-i(\omega_0 T_0+\sigma T_1)}\bigg)$$  $$\implies -2A^{'}e^{i\omega_0T_0}(i\omega_0)+2\bar{A}^{'}e^{-i\omega_0T_0}(i\omega_0)-2\mu Ae^{i\omega_0 T_0}(i\omega_0)+2\bar{A}e^{-i\omega_0T_0}(i \omega_0)$$$$-A^3e^{3i\omega_0T_0}(i\omega_0)+A^2\bar{A}e^{i\omega_0T_0}(i\omega_0)-2A^2\bar{A}e^{i\omega_0T_0}(i\omega_0)+2A\bar{A}^2e^{-i\omega T_0}(i\omega_0)-A\bar{A}^2e^{-3i\omega_0T_0}(i\omega_0)+\bar{A}^3e^{-3i\omega_0T_0}(i\omega_0)$$\begin{equation}+\frac{k}{2}\bigg(e^{i\omega T_0}e^{i\sigma T_1}+e^{-i\omega_0} e^{-i\sigma T_1}\bigg)\end{equation} Consider $A(T_1)=\dfrac{1}{2}ae^{i\beta}$ where $a$ and $\beta$ are the real amplitude and phase from the secular\ terms equation. Secular terms are nonhomogenous terms that make the function unbounded they are inconsistent with the behavior of the physical system, thus they must be eliminated. A term is secular if it is a solution of the homogenous equation.  Therefore, the sum of the coefficient of $e^{i \omega T_0}$ must be $0$. $$-2A^{'}(i\omega_0)-2\mu A(i\omega_0)-A^2\bar{A}(i\omega_0)+\dfrac{k}{2}e^{i\sigma T_1}=0$$ $$e^{-i\beta}\bigg(-a^{'}(i\omega_0)e^{i\beta}+a\beta^{'}\omega_0e^{i\beta}-\mu ae^{i\beta}(i\omega_0)-\frac{1}{8}a^3e^{i\beta}(i\omega_0)+\frac{k}{2}e^{-i\sigma T_1})=0\bigg) $$ \begin{equation}-a^{'}(i\omega_0)+a\beta^{'}\omega_0-\mu a(i \omega_0)-\frac{1}{8}a^3(i \omega_0) +\frac{k}{2}\bigg(\cos(\sigma T_1-\beta)+i\sin(\sigma T-\beta)\bigg)=0\end{equation} For the entire function to be zero Real and Imaginary parts must be 0.  \begin{equation}a\beta^{'}\omega_0+\frac{k}{2}\cos(\sigma T_1-\beta)=0 \end{equation} \begin{equation}-a\omega_0-\frac{1}{8}a^3\omega_0 + \sin(\sigma T_1-\beta)-\mu a \omega_0=0\end{equation} We make a change of variables and set $\gamma=\sigma T_1-\beta$ so that $\beta^{'}=\sigma T_1-\gamma^{'}$. Therefore we have that, \begin{equation}a\gamma^{'}=a\sigma +\frac{k}{2\omega_0}\cos \gamma\end{equation} \begin{equation}a^{'}=-\mu a-\frac{1}{8}a^3+ \frac{\sin \gamma}{\omega_0}\end{equation} Steady states occur when $a\gamma^{'}=0$ and $a^{'}=0$ therefore we have that \begin{equation}-\mu a-\frac{1}{8}a^3 +\frac{\sin \gamma}{\omega_0}=0\end{equation} \begin{equation}a\sigma+\frac{k}{2\omega_0} \cos \gamma=0\end{equation} There fore the solution is  \begin{equation}\begin{cases} a+\dfrac{1}{8}a^3=\dfrac{\sin \gamma}{\mu \omega_0} \\ a\sigma=-\dfrac{k}{2\omega_0}\cos \gamma \end{cases}\end{equation}","I would like to know if there are other methods to solve equations such as this one below. I don't really understand the theory behind the multiple scale analysis and why it works I understand some of the reasoning behind it just not the proof and theory. I kind of chose a differential equation for which I think it works and tried it out following a example from a similar equation. I think the answer seems strange but I was very thorough in my calculations. I welcome any comments or advice to problems like these thanks! \begin{equation}u^{''} +\omega_0^2u=-2\epsilon \mu u^{'}-\epsilon u^2 u^{'} + \epsilon k \cos \Omega t\end{equation} First we introduce two time variables (fast and slow) and define them as \begin{equation}T_0=t\;\;\;T_1=\epsilon t\end{equation} where $T_0$ is the fast time scale and $T_1$ is the slow time scale. The first order expansion of the problem is \begin{equation}u(t)=u_0(T_0,T_1)+\epsilon u_1(T_0,T_1)\end{equation} Differentiating the time derivative becomes \begin{align} \frac{du}{dt}=\frac{\partial u_0}{\partial T_0} \frac{dT_0}{dt}(1)+(\epsilon) \frac{\partial u_1}{\partial T_1} \frac{dT_1}{dt}=\frac{\partial u_0}{\partial T_0}+\epsilon \frac{\partial u_1}{\partial T_1} \end{align}  Define the linear operators, and functions \begin{equation} D_0=\dfrac{\partial}{\partial T_0},\hspace{10pt} D_1=\dfrac{\partial}{\partial T_1}\end{equation} The time derivative becomes $\dfrac{d}{dt}=D_0+\epsilon D_1$. The second order time derivative is then expressed as  \begin{equation}\frac{d^2}{dt^2}=(D_0+\epsilon D_1)^2=D_0^2+2D_0D_1 \epsilon +\epsilon ^2 D_1^2\end{equation} The $\epsilon ^2$ term is neglected. We are now ready to substitute our results in the original equation to obtain,  \pagebreak $$(D_0^2+2\epsilon D_0 D_1)(u_0+\epsilon u_1)+\omega^2_0(u_0+\epsilon u_1)=-2\epsilon \mu (D_0+ \epsilon D_1)(u_0+\epsilon u_1)-$$$$\epsilon (u_0+\epsilon u_1)^2(D_0+\epsilon D_1)(u_0+\epsilon u_1) + \epsilon k \cos \Omega t$$ $$\implies (D_0^2u_0+\epsilon D^2_0  u_1+2\epsilon D_1D_0 u_0 +2\epsilon ^2D_0 D_1 u_1^2)+\omega_0^2u_0+\epsilon \omega_0^2  u_1=-2 \epsilon \mu D_0u_0-2\epsilon^2\mu (...)$$ $$-\epsilon u^2_0D_0u_0+\epsilon^2(...)+\epsilon k \cos \Omega t $$ \begin{equation}\implies D_0 u_0+ \epsilon D_0 u_1 +2 \epsilon D_1 D_0 u_0+ \omega_0^2 u_0+\epsilon \omega_0^2 u_1=-2\epsilon \mu D_0 u_0-\epsilon u_0^2D_0u_0+\epsilon k \cos \Omega t\end{equation} Now we construct a system of differential equations in powers of $\epsilon$ called zero-order and first order problems and given by, \begin{align} D_0^2u_0+\omega_0^2u_0=0\end{align} \begin{align} D^2u_1 +\omega_0^2u_1=-2D_1D_0 u_0-2\mu D_0u_0-u_0^2D_0u_0+k \cos \Omega t \end{align} The first equation is a second order linear, homogeneous equation with constant coefficients and has general solution  \begin{equation}u_0(T_0,T_1)=A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i\omega_0T_0}\end{equation}  where $A(T_1)$ is the complex amplitude afterwards. Our next step is to investigate primary resonance of the system, which occurs when the actuation frequency $\Omega$ is near the natural frequency, $\omega_0$. This can be written as  \begin{align}\Omega=\omega_0+\epsilon \sigma\\ \Omega T_0=\omega T_0 +\epsilon T_0 \sigma\\\Omega t= \omega T_0+T_1\sigma \end{align} We substitute $u_0$ into the equation (2) to get,  \begin{equation*}D_0^2u_1+\omega_0^2u_1= -2D_0D_1[A(T_1)e^{i \omega_0 T_0}+\bar{A}(T_1)e^{-i \omega T_0}]-2\mu D_0[A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i \omega T_0}]- \end{equation*}\begin{equation} [A(T_1)e^{i\omega_0T_0}+\bar{A}(T_1)e^{-i\omega T_0}]^2D_0u_0+k\frac{e^{i \omega_0 T_0}e^{i\sigma T_1}+e^{-i\omega_0T_1}e^{-i\sigma T_1}}{2}\end{equation} Thus applying the linear operations to the above to get $$-2[A^{'}e^{i\omega_0T_0}(i\omega_0)+\bar{A}^{'}e^{-i\omega_0T_0}(-i\omega_0)]-2\mu[Ae^{i\omega_0 T_0}(i\omega_0)+\bar{A}e^{-i\omega_0T_0}(-i\omega_0)]+$$$$\bigg(-A^2e^{2i\omega_0T_0}-2A\bar{A}-\bar{A}^2e^{-2i\omega T_0}\bigg)\bigg(Ae^{i\omega_0T_0}(i\omega_0)-\bar{A}e^{-i\omega_0T_0}(i\omega_0)\bigg)+\frac{k}{2}\bigg(e^{i(\omega T_0+\sigma T_1)}+e^{-i(\omega_0 T_0+\sigma T_1)}\bigg)$$  $$\implies -2A^{'}e^{i\omega_0T_0}(i\omega_0)+2\bar{A}^{'}e^{-i\omega_0T_0}(i\omega_0)-2\mu Ae^{i\omega_0 T_0}(i\omega_0)+2\bar{A}e^{-i\omega_0T_0}(i \omega_0)$$$$-A^3e^{3i\omega_0T_0}(i\omega_0)+A^2\bar{A}e^{i\omega_0T_0}(i\omega_0)-2A^2\bar{A}e^{i\omega_0T_0}(i\omega_0)+2A\bar{A}^2e^{-i\omega T_0}(i\omega_0)-A\bar{A}^2e^{-3i\omega_0T_0}(i\omega_0)+\bar{A}^3e^{-3i\omega_0T_0}(i\omega_0)$$\begin{equation}+\frac{k}{2}\bigg(e^{i\omega T_0}e^{i\sigma T_1}+e^{-i\omega_0} e^{-i\sigma T_1}\bigg)\end{equation} Consider $A(T_1)=\dfrac{1}{2}ae^{i\beta}$ where $a$ and $\beta$ are the real amplitude and phase from the secular\ terms equation. Secular terms are nonhomogenous terms that make the function unbounded they are inconsistent with the behavior of the physical system, thus they must be eliminated. A term is secular if it is a solution of the homogenous equation.  Therefore, the sum of the coefficient of $e^{i \omega T_0}$ must be $0$. $$-2A^{'}(i\omega_0)-2\mu A(i\omega_0)-A^2\bar{A}(i\omega_0)+\dfrac{k}{2}e^{i\sigma T_1}=0$$ $$e^{-i\beta}\bigg(-a^{'}(i\omega_0)e^{i\beta}+a\beta^{'}\omega_0e^{i\beta}-\mu ae^{i\beta}(i\omega_0)-\frac{1}{8}a^3e^{i\beta}(i\omega_0)+\frac{k}{2}e^{-i\sigma T_1})=0\bigg) $$ \begin{equation}-a^{'}(i\omega_0)+a\beta^{'}\omega_0-\mu a(i \omega_0)-\frac{1}{8}a^3(i \omega_0) +\frac{k}{2}\bigg(\cos(\sigma T_1-\beta)+i\sin(\sigma T-\beta)\bigg)=0\end{equation} For the entire function to be zero Real and Imaginary parts must be 0.  \begin{equation}a\beta^{'}\omega_0+\frac{k}{2}\cos(\sigma T_1-\beta)=0 \end{equation} \begin{equation}-a\omega_0-\frac{1}{8}a^3\omega_0 + \sin(\sigma T_1-\beta)-\mu a \omega_0=0\end{equation} We make a change of variables and set $\gamma=\sigma T_1-\beta$ so that $\beta^{'}=\sigma T_1-\gamma^{'}$. Therefore we have that, \begin{equation}a\gamma^{'}=a\sigma +\frac{k}{2\omega_0}\cos \gamma\end{equation} \begin{equation}a^{'}=-\mu a-\frac{1}{8}a^3+ \frac{\sin \gamma}{\omega_0}\end{equation} Steady states occur when $a\gamma^{'}=0$ and $a^{'}=0$ therefore we have that \begin{equation}-\mu a-\frac{1}{8}a^3 +\frac{\sin \gamma}{\omega_0}=0\end{equation} \begin{equation}a\sigma+\frac{k}{2\omega_0} \cos \gamma=0\end{equation} There fore the solution is  \begin{equation}\begin{cases} a+\dfrac{1}{8}a^3=\dfrac{\sin \gamma}{\mu \omega_0} \\ a\sigma=-\dfrac{k}{2\omega_0}\cos \gamma \end{cases}\end{equation}",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'perturbation-theory']"
49,Differential Equation with Cross Products [without separating into system of equations],Differential Equation with Cross Products [without separating into system of equations],,"I need to solve the following equation: $$ \frac{d m}{d t}=-m\wedge b-\alpha m\wedge (m\wedge b), $$ where $b$ is constant However, I was instructed specifically not to separate the calculation into a system of equations (which would not be linear and would be pretty complicated). I was told, instead, that there is a smarter way to do it. I managed to do some of it. I found the angle between m and b. And now I'm stuck. Here is what I did so far: $$\frac{dm}{dt} \cdot m = 0 \Rightarrow \frac{d\vert m \vert}{dt} = 0$$ Naming $u=|m\wedge b|^2$ and $v=m\cdot b$, we instantly get the relation: $$u+v^2=k^2$$ Where $k$ is a constant independent of time and of value $k=|b||m|$. We can rewrite the original equation as the following (triple product): $$ \frac{d m}{d t}=-m\wedge b-\alpha (m\cdot b)m + \alpha (m\cdot m)b $$ Then, $$\frac{dm}{dt} \cdot b = - \alpha(m\cdot b)(m\cdot b) + \alpha(m\cdot m)(b\cdot b)$$ $$\frac{dv}{dt} = - \alpha v^2 + \alpha k^2 $$ We also retrieve that $$\frac{dv}{dt} = - \alpha u $$ To solve the differential equation in v, we divide both sides by $ (k^2 - v^2)$, and then integrate. $$ln|v-k| - ln|v+k| = -2\alpha kt - 2C$$ $$|\frac{v-k}{v+k}| = e^{-2\alpha k t - 2C}$$ We know that $k>0$ and $v>0$, and that always $v<=k$, then $|\frac{v-k}{v+k}| = -\frac{v-k}{v+k}$ and the equation yields: $$v = -k\frac{1-e^{-2\alpha k t - 2C}}{1+e^{-2\alpha k t - 2C}} = -k\tanh(\alpha k t + C)$$ $$u = \frac{k^2}{\cosh^2(\alpha k t + C)}$$ We can extract: $$\cos(\phi) = \frac{k}{|m||b|\cosh(\alpha k t + C)}$$ $$\sin(\phi) = \frac{k \tanh(\alpha k t + C)}{|m||b|}$$ But $|m|$ and $|b|$ are constants.","I need to solve the following equation: $$ \frac{d m}{d t}=-m\wedge b-\alpha m\wedge (m\wedge b), $$ where $b$ is constant However, I was instructed specifically not to separate the calculation into a system of equations (which would not be linear and would be pretty complicated). I was told, instead, that there is a smarter way to do it. I managed to do some of it. I found the angle between m and b. And now I'm stuck. Here is what I did so far: $$\frac{dm}{dt} \cdot m = 0 \Rightarrow \frac{d\vert m \vert}{dt} = 0$$ Naming $u=|m\wedge b|^2$ and $v=m\cdot b$, we instantly get the relation: $$u+v^2=k^2$$ Where $k$ is a constant independent of time and of value $k=|b||m|$. We can rewrite the original equation as the following (triple product): $$ \frac{d m}{d t}=-m\wedge b-\alpha (m\cdot b)m + \alpha (m\cdot m)b $$ Then, $$\frac{dm}{dt} \cdot b = - \alpha(m\cdot b)(m\cdot b) + \alpha(m\cdot m)(b\cdot b)$$ $$\frac{dv}{dt} = - \alpha v^2 + \alpha k^2 $$ We also retrieve that $$\frac{dv}{dt} = - \alpha u $$ To solve the differential equation in v, we divide both sides by $ (k^2 - v^2)$, and then integrate. $$ln|v-k| - ln|v+k| = -2\alpha kt - 2C$$ $$|\frac{v-k}{v+k}| = e^{-2\alpha k t - 2C}$$ We know that $k>0$ and $v>0$, and that always $v<=k$, then $|\frac{v-k}{v+k}| = -\frac{v-k}{v+k}$ and the equation yields: $$v = -k\frac{1-e^{-2\alpha k t - 2C}}{1+e^{-2\alpha k t - 2C}} = -k\tanh(\alpha k t + C)$$ $$u = \frac{k^2}{\cosh^2(\alpha k t + C)}$$ We can extract: $$\cos(\phi) = \frac{k}{|m||b|\cosh(\alpha k t + C)}$$ $$\sin(\phi) = \frac{k \tanh(\alpha k t + C)}{|m||b|}$$ But $|m|$ and $|b|$ are constants.",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'cross-product']"
50,Maximum principle-estimation,Maximum principle-estimation,,"Let $S=\{x \in \mathbb{R}^2 \mid |x| <1\}$. Using the maximum principle I have to show that the solution of the problem $$-\Delta u(x)=f(x), x \in S \\ u(x)=0, x \in \partial{S}$$ satisfies the estimation $$|u(x)| \leq \frac{1}{4}\max_{x \in \overline{S}} |f(x)|, x \in S$$ To use the maximum principle shouldn't it stand that $$\Delta u \geq 0$$ ?? Do we have to take cases for $f$, if it is positive or negative?? EDIT : Is it as followed?? $$\max_{\overline{S}} u=\max_{\partial{S}}u =0$$ How can we use $f$ ?? How do we get an expression with $f$ at the inequality??","Let $S=\{x \in \mathbb{R}^2 \mid |x| <1\}$. Using the maximum principle I have to show that the solution of the problem $$-\Delta u(x)=f(x), x \in S \\ u(x)=0, x \in \partial{S}$$ satisfies the estimation $$|u(x)| \leq \frac{1}{4}\max_{x \in \overline{S}} |f(x)|, x \in S$$ To use the maximum principle shouldn't it stand that $$\Delta u \geq 0$$ ?? Do we have to take cases for $f$, if it is positive or negative?? EDIT : Is it as followed?? $$\max_{\overline{S}} u=\max_{\partial{S}}u =0$$ How can we use $f$ ?? How do we get an expression with $f$ at the inequality??",,"['ordinary-differential-equations', 'partial-differential-equations', 'maximum-principle']"
51,First order differential equation integrating factor is $e^{\int\frac{2}{x^2-1}}$,First order differential equation integrating factor is,e^{\int\frac{2}{x^2-1}},So i got the first order ode $$(x^2-1)\frac{dy}{dx}+2xy=x$$ I divided both sides by $x^2-1$ $$\frac{dy}{dx}+\frac{2}{x^2-1}xy=\frac{x}{x^2-1}$$ in the form $y' + p(x)y = q(x)$ So that means the integrand is... $$e^{\int\frac{2}{x^2-1}}$$ But i'm not sure what to do i think the $\int\frac{2}{x^2-1}$ = $-\log{(x-1)}+4\log{(x+1)}$ So it's $$e^{-\log{(x-1)}+4\log{(x+1)}}$$ $$e^{\log{(x-1)^{-1}}+\log{(x+1)^4}}$$ $$\frac{1}{x-1}+(x+1)^4$$ Is this right? and then just multiply both sides by this?,So i got the first order ode $$(x^2-1)\frac{dy}{dx}+2xy=x$$ I divided both sides by $x^2-1$ $$\frac{dy}{dx}+\frac{2}{x^2-1}xy=\frac{x}{x^2-1}$$ in the form $y' + p(x)y = q(x)$ So that means the integrand is... $$e^{\int\frac{2}{x^2-1}}$$ But i'm not sure what to do i think the $\int\frac{2}{x^2-1}$ = $-\log{(x-1)}+4\log{(x+1)}$ So it's $$e^{-\log{(x-1)}+4\log{(x+1)}}$$ $$e^{\log{(x-1)^{-1}}+\log{(x+1)^4}}$$ $$\frac{1}{x-1}+(x+1)^4$$ Is this right? and then just multiply both sides by this?,,"['calculus', 'ordinary-differential-equations']"
52,"roots of a linear DE $(D^2+2cD+k)y=0$ given $c<0,k>0, c^2>k$",roots of a linear DE  given,"(D^2+2cD+k)y=0 c<0,k>0, c^2>k","Let y(x) be a non-trivial solution of the second order linear differential equation $$\dfrac{d^2y}{dx^2}+2c\dfrac{dy}{dx}+ky=0 $$, where $c<0,k>0, c^2>k$. Then, (a) $|y(x)|\to\infty$ as $x\to\infty$ (b) $|y(x)|\to 0$ as $x\to\infty$ (c) $\lim\limits_{x\to\pm\infty}|y(x)|$ exists and is finite. (d) none of the above Now, solving the ODE, I get the roots as $-c\pm\sqrt{c^2-k}$. The two roots will be real, distinct and positive and hence, option (a) should be the answer. Did I solve this correctly ?","Let y(x) be a non-trivial solution of the second order linear differential equation $$\dfrac{d^2y}{dx^2}+2c\dfrac{dy}{dx}+ky=0 $$, where $c<0,k>0, c^2>k$. Then, (a) $|y(x)|\to\infty$ as $x\to\infty$ (b) $|y(x)|\to 0$ as $x\to\infty$ (c) $\lim\limits_{x\to\pm\infty}|y(x)|$ exists and is finite. (d) none of the above Now, solving the ODE, I get the roots as $-c\pm\sqrt{c^2-k}$. The two roots will be real, distinct and positive and hence, option (a) should be the answer. Did I solve this correctly ?",,['ordinary-differential-equations']
53,Finding an ODE given some of its solutions,Finding an ODE given some of its solutions,,"Find $a, b, f(x)$ such that $$y''+ay'+by = f(x)$$   Is satisfied by $g_{1}=\sin x + e^x$ and $g_{2}=\sin x - e^{-x}$ What I tried to do: First, I used the fact that if $g_{1}$ and $g_{2}$ are solutions for the nonhomogeneous ODE, than $g_{1}-g_{2}$ is a solution for the homogeneous correspondent ODE. So, $g = e^x + e^{-x}$ is a solution of $ y''+ay'+b=0$.  On the other hand, if I had $ y''+ay'+b=0$ and the discriminant $d = a^2-4b>0$, I would have solutions of the form: $$c_{1}e^{r_{1}x} + c_{1}e^{r_{2}x}$$, where $r_{1}$ and $r_{2}$ are solutions of $$r^2+ar+b=0$$ As in my case I have $r_{1} = 1$ and $r_{2} = -1$, the quadratic equation gives  $a=0$ and $b= -1$. I don't know how to find the function (or the functions) $f$ such that $$y''-y = f(x)$$ I was thinking about putting $y = \sin x$, as $\sin x$ appears in both the particular solutions $g_{1}$ and $g_{2}$ (that is, I expected that sine was a particular solution for the nonhomegeneous equation). I'm I doing this right? Does someone has a suggestion on how I could improve it?","Find $a, b, f(x)$ such that $$y''+ay'+by = f(x)$$   Is satisfied by $g_{1}=\sin x + e^x$ and $g_{2}=\sin x - e^{-x}$ What I tried to do: First, I used the fact that if $g_{1}$ and $g_{2}$ are solutions for the nonhomogeneous ODE, than $g_{1}-g_{2}$ is a solution for the homogeneous correspondent ODE. So, $g = e^x + e^{-x}$ is a solution of $ y''+ay'+b=0$.  On the other hand, if I had $ y''+ay'+b=0$ and the discriminant $d = a^2-4b>0$, I would have solutions of the form: $$c_{1}e^{r_{1}x} + c_{1}e^{r_{2}x}$$, where $r_{1}$ and $r_{2}$ are solutions of $$r^2+ar+b=0$$ As in my case I have $r_{1} = 1$ and $r_{2} = -1$, the quadratic equation gives  $a=0$ and $b= -1$. I don't know how to find the function (or the functions) $f$ such that $$y''-y = f(x)$$ I was thinking about putting $y = \sin x$, as $\sin x$ appears in both the particular solutions $g_{1}$ and $g_{2}$ (that is, I expected that sine was a particular solution for the nonhomegeneous equation). I'm I doing this right? Does someone has a suggestion on how I could improve it?",,"['calculus', 'ordinary-differential-equations']"
54,Nonlinear second-order ODE $yy'' - (y')^{2} = y^4$,Nonlinear second-order ODE,yy'' - (y')^{2} = y^4,"I have the following ODE to solve. $$ yy'' - (y')^{2} = y^4 $$ I tried to substitute $y'$ by $v$, and then I get the following: $$ yv' - v^{2} = y^4. $$ I can't go further. I can't see what I'm supposed to do in order to solve it. I saw a solution involving Bessel function. But, is it possible to transform the first ODE into an exact, linear, or Bernoulli equation? Any hint, please. Thanks.","I have the following ODE to solve. $$ yy'' - (y')^{2} = y^4 $$ I tried to substitute $y'$ by $v$, and then I get the following: $$ yv' - v^{2} = y^4. $$ I can't go further. I can't see what I'm supposed to do in order to solve it. I saw a solution involving Bessel function. But, is it possible to transform the first ODE into an exact, linear, or Bernoulli equation? Any hint, please. Thanks.",,['ordinary-differential-equations']
55,Quasilinear second order ODE,Quasilinear second order ODE,,"Consider a smooth $u\colon\mathbb{R}\rightarrow\mathbb{R}$ satisfying $$ u^{\prime\prime}+a\left(u^{\prime}\right)^{2}+bu=0\text{ on }\mathbb{R} $$ with  $$ u^{\prime}\left(x\right),u^{\prime\prime}\left(x\right)\rightarrow0\text{ as }\left|x\right|\rightarrow\infty. $$ I assume finding (reasonable) closed form expressions for this is difficult/impossible. Am I mistaken?","Consider a smooth $u\colon\mathbb{R}\rightarrow\mathbb{R}$ satisfying $$ u^{\prime\prime}+a\left(u^{\prime}\right)^{2}+bu=0\text{ on }\mathbb{R} $$ with  $$ u^{\prime}\left(x\right),u^{\prime\prime}\left(x\right)\rightarrow0\text{ as }\left|x\right|\rightarrow\infty. $$ I assume finding (reasonable) closed form expressions for this is difficult/impossible. Am I mistaken?",,['ordinary-differential-equations']
56,A basic non-autonomous O.D.E question,A basic non-autonomous O.D.E question,,"Consider the following non-autonomous O.D.E $$ \dot{x}(t) = h(x(t),g(t))$$ such that $h(.,.)$ is continuous but $g(.)$ is discontinuous(step function). Does the solution exist here ? I don't think so.","Consider the following non-autonomous O.D.E $$ \dot{x}(t) = h(x(t),g(t))$$ such that $h(.,.)$ is continuous but $g(.)$ is discontinuous(step function). Does the solution exist here ? I don't think so.",,"['real-analysis', 'ordinary-differential-equations']"
57,"How to solve linear, second order ODE with Frobenius method with a difficult recurrence relation?","How to solve linear, second order ODE with Frobenius method with a difficult recurrence relation?",,"The ODE in question is: $$4xy''+2y'+y=0$$ Shifting the power series of each term so that they are all raised to the power $(n+r)$ will yield this recurrence relation: $$a_{n+1}={a_n\over (n+r+1)(-2-4(n+r))}$$ with $$r=1/2, 0$$ If you plug values of $n$ into this recurrence relation it is nearly impossible to find a pattern for $a_n$, unless I'm missing something. Is there a way to continue to solve this ODE with the Frobenius method using this difficult recurrence relation, or any tricks to use earlier in the problem to avoid difficult recurrence relations?","The ODE in question is: $$4xy''+2y'+y=0$$ Shifting the power series of each term so that they are all raised to the power $(n+r)$ will yield this recurrence relation: $$a_{n+1}={a_n\over (n+r+1)(-2-4(n+r))}$$ with $$r=1/2, 0$$ If you plug values of $n$ into this recurrence relation it is nearly impossible to find a pattern for $a_n$, unless I'm missing something. Is there a way to continue to solve this ODE with the Frobenius method using this difficult recurrence relation, or any tricks to use earlier in the problem to avoid difficult recurrence relations?",,"['ordinary-differential-equations', 'recurrence-relations', 'power-series', 'pattern-recognition']"
58,How to find the inverse Laplace transform?,How to find the inverse Laplace transform?,,I'm trying to calculate $$\mathcal{L}^{-1}\left(\frac{3s^3-3s^2+3s-5}{s^2(s^2+2s+5)}\right)$$ But I am not sure how to go from here. I would be really grateful for any help. Thanks.,I'm trying to calculate $$\mathcal{L}^{-1}\left(\frac{3s^3-3s^2+3s-5}{s^2(s^2+2s+5)}\right)$$ But I am not sure how to go from here. I would be really grateful for any help. Thanks.,,"['ordinary-differential-equations', 'laplace-transform']"
59,"Questions regarding ""integrable systems""","Questions regarding ""integrable systems""",,"Consider a smooth differential equation on the plane   $$ x'=g(x,y),\quad y'=h(x,y). $$   Suppose there exists a function $D(x,y)$ such that   $$ (Dg)_x+(Dh)_y=0.  $$ Then $D$ is an integrating factor and the system is integrable. A quick search for ""integrable system"" on Google returns results not satisfying. Could anyone explain what the last sentence in the argument above means? [Added:] The question is motivated by reading a paper about the Bendixson-Dulac Theorem . In particular, $$ g(x,y)=ax+bx^2+cxy,\quad h(x,y)=dy+exy+fy^2 $$ and $D(x,y)=x^ry^s$ for some $r,s$. [Added:] I asked this question in MO. I don't understand though, there is an answer there: $X=g\partial_x + h\partial_y$ is the vector field whose flow lines are wanted. $\omega=hdx - gdy$ is a 1-form with kernel the span of $X$. Also $D\omega$ has kernel the span of $X$ for any function $D$ which does not vanish anywhere. If $d(D\omega)=0$ (this is your condition) then $D\omega$ is a closed 1-form, thus exact on simply connected sets. So $D\omega = dF$ for a function $F$ which can easily be computed by line integrals.   Thus the wanted flow lines are contained in the level sets of $F$. Finally,   the time dependence of the flow has to be computed extra. I would really appreciate it if anyone could explain what that answer means (in a more ""elementary"" way) here.","Consider a smooth differential equation on the plane   $$ x'=g(x,y),\quad y'=h(x,y). $$   Suppose there exists a function $D(x,y)$ such that   $$ (Dg)_x+(Dh)_y=0.  $$ Then $D$ is an integrating factor and the system is integrable. A quick search for ""integrable system"" on Google returns results not satisfying. Could anyone explain what the last sentence in the argument above means? [Added:] The question is motivated by reading a paper about the Bendixson-Dulac Theorem . In particular, $$ g(x,y)=ax+bx^2+cxy,\quad h(x,y)=dy+exy+fy^2 $$ and $D(x,y)=x^ry^s$ for some $r,s$. [Added:] I asked this question in MO. I don't understand though, there is an answer there: $X=g\partial_x + h\partial_y$ is the vector field whose flow lines are wanted. $\omega=hdx - gdy$ is a 1-form with kernel the span of $X$. Also $D\omega$ has kernel the span of $X$ for any function $D$ which does not vanish anywhere. If $d(D\omega)=0$ (this is your condition) then $D\omega$ is a closed 1-form, thus exact on simply connected sets. So $D\omega = dF$ for a function $F$ which can easily be computed by line integrals.   Thus the wanted flow lines are contained in the level sets of $F$. Finally,   the time dependence of the flow has to be computed extra. I would really appreciate it if anyone could explain what that answer means (in a more ""elementary"" way) here.",,[]
60,Solving PDE by Laplace Transform,Solving PDE by Laplace Transform,,"Use Laplace transforms to solve the boundary value problem $$Y_{xx}(x,t)-2Y_{tx}(x,t)+Y_{tt}(x,t)=0, \quad 0<x<1, t>0$$ $$Y(x,0)=Y_t(x,0)=0, \quad 0<x<1$$ $$Y(0,t)=0, \ Y(t,1)=F(t), \ t>0.$$ I am supposed to solve the PDE using Laplace transform. I know how to solve such PDE's like the wave equation using Laplace, but I dont know how to solve it for this problem where there is a mixed partial of $\frac{\partial^2}{\partial x\partial t}$. I tried searching online but I couldn't find anything. What I have so far is $$\frac{d^2U}{dx^2}-2s\frac{\partial U}{\partial x}=-s^2U$$ $$U(0,s)=0,\quad U(1,s)=F(s)$$.","Use Laplace transforms to solve the boundary value problem $$Y_{xx}(x,t)-2Y_{tx}(x,t)+Y_{tt}(x,t)=0, \quad 0<x<1, t>0$$ $$Y(x,0)=Y_t(x,0)=0, \quad 0<x<1$$ $$Y(0,t)=0, \ Y(t,1)=F(t), \ t>0.$$ I am supposed to solve the PDE using Laplace transform. I know how to solve such PDE's like the wave equation using Laplace, but I dont know how to solve it for this problem where there is a mixed partial of $\frac{\partial^2}{\partial x\partial t}$. I tried searching online but I couldn't find anything. What I have so far is $$\frac{d^2U}{dx^2}-2s\frac{\partial U}{\partial x}=-s^2U$$ $$U(0,s)=0,\quad U(1,s)=F(s)$$.",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
61,How to find the maximal interval of existence of the solution for the following initial value problem?,How to find the maximal interval of existence of the solution for the following initial value problem?,,"Consider the following initial value problem,  $$ \dot x = tx^3 \\ x(0) = x_{0} $$ We have the following theorem, . Since the hypotheses of the theorem are satisfied, we must have a solution on $[-a, a]$. To calculate $M$, it is easy to see that the maximum value attained by $t$ is $a$ and the one attained by $x$ in the ball of radius $b$ with center $x_0$ is $(x_0 + b)$. Hence, $$M = a(x_0 +b)^3$$ Solving for $a$ we get that $$ a = \pm \sqrt { \frac {b}{(x_0 + b)^3} } $$ Here is where I am stuck. I suppose that there exists a $b$ that maximizes $a$. Then, by maximizing $a$ we have thus found the maximal interval of existence for the solution. However, I cannot find a way to maximize the above expression for a fixed $x_0$. Is there something that I am missing or should another approach be taken to find the maximal interval of existence?","Consider the following initial value problem,  $$ \dot x = tx^3 \\ x(0) = x_{0} $$ We have the following theorem, . Since the hypotheses of the theorem are satisfied, we must have a solution on $[-a, a]$. To calculate $M$, it is easy to see that the maximum value attained by $t$ is $a$ and the one attained by $x$ in the ball of radius $b$ with center $x_0$ is $(x_0 + b)$. Hence, $$M = a(x_0 +b)^3$$ Solving for $a$ we get that $$ a = \pm \sqrt { \frac {b}{(x_0 + b)^3} } $$ Here is where I am stuck. I suppose that there exists a $b$ that maximizes $a$. Then, by maximizing $a$ we have thus found the maximal interval of existence for the solution. However, I cannot find a way to maximize the above expression for a fixed $x_0$. Is there something that I am missing or should another approach be taken to find the maximal interval of existence?",,['ordinary-differential-equations']
62,Differential Equation Mass Damped Spring,Differential Equation Mass Damped Spring,,"I have already attempted to find the natural frequency by taking the root of $k/\mu$ where $\mu = 2m$. I then divided that by $2\pi$ to get $6.74\cdot 10^{13}$, not sure how to answer the second question though. The $\mu$ for the Oxygen molecule (O2) is $1.33\cdot 10^{-26}$kg and $k =1195$N/m. What is the natural frequency of O2? What will happen to the molecule if it is forced by an external source to vibrate with a  frequency equal to its natural frequency? Explain in detail.","I have already attempted to find the natural frequency by taking the root of $k/\mu$ where $\mu = 2m$. I then divided that by $2\pi$ to get $6.74\cdot 10^{13}$, not sure how to answer the second question though. The $\mu$ for the Oxygen molecule (O2) is $1.33\cdot 10^{-26}$kg and $k =1195$N/m. What is the natural frequency of O2? What will happen to the molecule if it is forced by an external source to vibrate with a  frequency equal to its natural frequency? Explain in detail.",,['ordinary-differential-equations']
63,$(1-t^2)\frac{\mathrm{d}^2y}{\mathrm{d}t^2}-t\frac{\mathrm{d}y}{\mathrm{d}t}+(a+2q (1- 2t^2))y=0$,,(1-t^2)\frac{\mathrm{d}^2y}{\mathrm{d}t^2}-t\frac{\mathrm{d}y}{\mathrm{d}t}+(a+2q (1- 2t^2))y=0,"So I have to solve $$(1-t^2)\frac{\mathrm{d}^2y}{\mathrm{d}t^2} -t\frac{\mathrm{d}y}{\mathrm{d}t}+(a+2q (1-2t^2))y=0$$ All substitutions seem to fail, some trigonometric ones fail less than the rest, but they still don't reduce the DE to something solvable. I would also prefer to avoid series solution as I'm not good at it and I probably won't understand the answer, but if you can do it with series, be my guest.","So I have to solve $$(1-t^2)\frac{\mathrm{d}^2y}{\mathrm{d}t^2} -t\frac{\mathrm{d}y}{\mathrm{d}t}+(a+2q (1-2t^2))y=0$$ All substitutions seem to fail, some trigonometric ones fail less than the rest, but they still don't reduce the DE to something solvable. I would also prefer to avoid series solution as I'm not good at it and I probably won't understand the answer, but if you can do it with series, be my guest.",,"['ordinary-differential-equations', 'closed-form']"
64,Prove that maximal solutions are defined in $\mathbb{R}$,Prove that maximal solutions are defined in,\mathbb{R},"Let $f:\mathbb{R}\times\mathbb{R}^n\to \mathbb{R}^n$ be a continuously differentiable function. Suppose that exists $v:\mathbb{R}\to[0,\infty)$ continuous such that $||f(t,x)||\leq v(t)||x||\; \forall (t,x)\in \mathbb{R}\times\mathbb{R}^n$ .   Prove that all maximal solutions for $\dot{x}=f(t,x)$ are defined in $\mathbb{R}$ ; what happens if $f$ is bounded? I believe that the Picard–Lindelöf theorem is the way to go, but I don't know how to prove that $f$ satisfies the Lipschitz condition. In order to use it, if I understand correctly, I should prove that $v$ is bounded for every $t$ in a neighborhood of $(t_0,x_0)$ being the last a solution for the differential equation. If $\dot{x}=f(t,x)$ then $x(t)=\displaystyle\int f(t,x)dt$ . Considering the hypothesis, $$\displaystyle{||x||\leq\int||f(t,x)||dt\leq\int v(t) ||x||dt}\\1\leq\int||f(t,x)||dt\leq\int v(t)dt$$ which doesn't seem useful at all.... First update Alright, let's try it this way: The goal is to show that we always have a is defined in $\mathbb{R}$ for any IVP. Suppose $(t_0,x_0)$ is a maximal solution for $\dot{x}=f(t,x)$ with $x(t_0)=a$ , then I should prove that exists an interval $[t_0-c,t_0+c]$ in which the solution exists. $v$ is continuos, then for any $t_0$ we can pick $c$ and $[t_0-c,t_0+c]$ will be be bounded. Define $K=\operatorname{max}\{v(t):t\in [t_0-c,t_0+c]\}$ . Then $f$ is Lipschitz for every $(t,x)\in [t_0-c,t_0+c]\times \mathbb{R}^n$ , this is, $||f(t,x)||\leq v(t)||x||\leq K ||x||\;\forall (t,x)\in [t_0-c,t_0+c]\times\mathbb{R}^n$ By the Picard-Lindelöf theorem, the IVP has an unique maximal solution. And $t_0,c$ were arbitrary, can I conclude that all maximal solutions are defined? Second update Looking about extensibility I found these notes about differential equation. The extensibility theorem and the corollary seem to be enough to prove that the solutions exists for all $\mathbb{R}$ . I'll quote both here: Suppose that $f$ is $C^1$ on $\mathbb{R}^n$ .  Denote the unique solution by $x(t)$ and suppose $J:=(a,b)$ is the maximal interval of existence. Theorem 1.20 (Extensibility Theorem): For each compact set $K\subset \mathbb{R}^n$ there is a $t\in J$ such that $x(t)\notin K$ ;  thus, $$\lim_{t\to b^-}|x(t)|=\lim_{t\to a^+}|x(t)|=+\infty .$$ Corollary 1.21: Without loss of generality, if $f:\mathbb{R}^n\to \mathbb{R}^n$ is continuous, then the solutions to $\dot{x}=f(x)$ , $x(0)=x_0$ exist $\forall t\in \mathbb{R}$ . With the corollary and the continuity of $f:\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n$ , can I affirm that all maximal solutions can be defined in $\mathbb{R}$ for the equation $\dot{x}=f(t,x)$ ?","Let be a continuously differentiable function. Suppose that exists continuous such that .   Prove that all maximal solutions for are defined in ; what happens if is bounded? I believe that the Picard–Lindelöf theorem is the way to go, but I don't know how to prove that satisfies the Lipschitz condition. In order to use it, if I understand correctly, I should prove that is bounded for every in a neighborhood of being the last a solution for the differential equation. If then . Considering the hypothesis, which doesn't seem useful at all.... First update Alright, let's try it this way: The goal is to show that we always have a is defined in for any IVP. Suppose is a maximal solution for with , then I should prove that exists an interval in which the solution exists. is continuos, then for any we can pick and will be be bounded. Define . Then is Lipschitz for every , this is, By the Picard-Lindelöf theorem, the IVP has an unique maximal solution. And were arbitrary, can I conclude that all maximal solutions are defined? Second update Looking about extensibility I found these notes about differential equation. The extensibility theorem and the corollary seem to be enough to prove that the solutions exists for all . I'll quote both here: Suppose that is on .  Denote the unique solution by and suppose is the maximal interval of existence. Theorem 1.20 (Extensibility Theorem): For each compact set there is a such that ;  thus, Corollary 1.21: Without loss of generality, if is continuous, then the solutions to , exist . With the corollary and the continuity of , can I affirm that all maximal solutions can be defined in for the equation ?","f:\mathbb{R}\times\mathbb{R}^n\to \mathbb{R}^n v:\mathbb{R}\to[0,\infty) ||f(t,x)||\leq v(t)||x||\; \forall (t,x)\in \mathbb{R}\times\mathbb{R}^n \dot{x}=f(t,x) \mathbb{R} f f v t (t_0,x_0) \dot{x}=f(t,x) x(t)=\displaystyle\int f(t,x)dt \displaystyle{||x||\leq\int||f(t,x)||dt\leq\int v(t) ||x||dt}\\1\leq\int||f(t,x)||dt\leq\int v(t)dt \mathbb{R} (t_0,x_0) \dot{x}=f(t,x) x(t_0)=a [t_0-c,t_0+c] v t_0 c [t_0-c,t_0+c] K=\operatorname{max}\{v(t):t\in [t_0-c,t_0+c]\} f (t,x)\in [t_0-c,t_0+c]\times \mathbb{R}^n ||f(t,x)||\leq v(t)||x||\leq K ||x||\;\forall (t,x)\in [t_0-c,t_0+c]\times\mathbb{R}^n t_0,c \mathbb{R} f C^1 \mathbb{R}^n x(t) J:=(a,b) K\subset \mathbb{R}^n t\in J x(t)\notin K \lim_{t\to b^-}|x(t)|=\lim_{t\to a^+}|x(t)|=+\infty . f:\mathbb{R}^n\to \mathbb{R}^n \dot{x}=f(x) x(0)=x_0 \forall t\in \mathbb{R} f:\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n \mathbb{R} \dot{x}=f(t,x)",['ordinary-differential-equations']
65,Lyapunov-Schmidt reduction.,Lyapunov-Schmidt reduction.,,"Use Lyapunov-Schmidt reduction to find an expression, or   approximation, of the set of equilibria, as a function of the   parameter $\lambda$, of the planar vector field   $$f(x,y,\lambda)=(\lambda + 2x + y - x^2, 2x + (1+\lambda)y - xy)$$   near the equilibrium $(x,y) = (0,0)$ at $\lambda = 0$. So, usually, we tackle this problem by using the Implicit Function Theorem twice. Firstly, we define $f_1(x,y,\lambda) = \lambda + 2x + y - x^2, \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ $ $f_2(x,y,\lambda) =2x + (1+\lambda)y - xy$. Since $f_1(0,0,0) = 0$ and $D_x f_1(0,0,0)=2$, so it's invertible, the implicit function theorem states that there exist open neighborhoods $U \subseteq \mathbb{R}$ and $V \subseteq \mathbb{R}^2$ of $0$ and a function $x^*: V \to U$ such that $f_1(x,y,\lambda) = 0 \iff x^*(y,\lambda) = x$, with $x^*(0,0) = 0$. Furthermore, $D_y x^*(0,0) = -(D_x f_1 (0,0,0))^{-1} D_y f_1 (0,0,0) = -(2)^{-1}1 = -\tfrac{1}{2}$. Now, we substitute $x^*(y,\lambda)$ for $x$ in $f_2$, and again we want to apply the IFT to express $y$ in terms of $\lambda$. So we define $g(y,\lambda) = f_2(x^*(y,\lambda), y,  \lambda) = 2x^*(y,\lambda) + (1+\lambda)y - x^*(y,\lambda)y$. However, $D_y g(0,0) = 2D_y x^*(0,0) + 1 + 0 - D_y x^*(0,0)\cdot 0 -x^*(0,0)  =0$, so we can't apply the IFT. Is this just an ill-chosen example, or is there a different approach possible?","Use Lyapunov-Schmidt reduction to find an expression, or   approximation, of the set of equilibria, as a function of the   parameter $\lambda$, of the planar vector field   $$f(x,y,\lambda)=(\lambda + 2x + y - x^2, 2x + (1+\lambda)y - xy)$$   near the equilibrium $(x,y) = (0,0)$ at $\lambda = 0$. So, usually, we tackle this problem by using the Implicit Function Theorem twice. Firstly, we define $f_1(x,y,\lambda) = \lambda + 2x + y - x^2, \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ $ $f_2(x,y,\lambda) =2x + (1+\lambda)y - xy$. Since $f_1(0,0,0) = 0$ and $D_x f_1(0,0,0)=2$, so it's invertible, the implicit function theorem states that there exist open neighborhoods $U \subseteq \mathbb{R}$ and $V \subseteq \mathbb{R}^2$ of $0$ and a function $x^*: V \to U$ such that $f_1(x,y,\lambda) = 0 \iff x^*(y,\lambda) = x$, with $x^*(0,0) = 0$. Furthermore, $D_y x^*(0,0) = -(D_x f_1 (0,0,0))^{-1} D_y f_1 (0,0,0) = -(2)^{-1}1 = -\tfrac{1}{2}$. Now, we substitute $x^*(y,\lambda)$ for $x$ in $f_2$, and again we want to apply the IFT to express $y$ in terms of $\lambda$. So we define $g(y,\lambda) = f_2(x^*(y,\lambda), y,  \lambda) = 2x^*(y,\lambda) + (1+\lambda)y - x^*(y,\lambda)y$. However, $D_y g(0,0) = 2D_y x^*(0,0) + 1 + 0 - D_y x^*(0,0)\cdot 0 -x^*(0,0)  =0$, so we can't apply the IFT. Is this just an ill-chosen example, or is there a different approach possible?",,"['ordinary-differential-equations', 'implicit-function-theorem', 'bifurcation']"
66,Solving Inhomogeneous Differential Equations Using the Undetermined Coefficients Method,Solving Inhomogeneous Differential Equations Using the Undetermined Coefficients Method,,"I am trying to solve the following question in my homework: Use the method of undetermined coefficients to solve the following differential equation:   $$y'' + y = -\sin(x), \ y(0) = 0, \ y'(0) = 0.$$ I feel that I have a fairly good grasp of the concepts, that I need to find a general solution of the homogeneous equation and then a particular solution. Then I need to solve for the constants $c_1$ and $c_2$ by plugging in the initial conditions. However, my answer is not being accepted by the computer program my teachers use. Here is my answer: $$ y = -0.5\sin(x) + 0.5x\cos(x). $$ I have checked this answer several times by finding its first and second derivatives, plugging both into the original DE, and also plugging in the initial conditions. I am wondering if there is a typo, since my teachers create our homework online. Is anyone getting the same answer? I want to know if I need to approach my teachers and tell them that there is a possible typo in the homework. Thank you!","I am trying to solve the following question in my homework: Use the method of undetermined coefficients to solve the following differential equation:   $$y'' + y = -\sin(x), \ y(0) = 0, \ y'(0) = 0.$$ I feel that I have a fairly good grasp of the concepts, that I need to find a general solution of the homogeneous equation and then a particular solution. Then I need to solve for the constants $c_1$ and $c_2$ by plugging in the initial conditions. However, my answer is not being accepted by the computer program my teachers use. Here is my answer: $$ y = -0.5\sin(x) + 0.5x\cos(x). $$ I have checked this answer several times by finding its first and second derivatives, plugging both into the original DE, and also plugging in the initial conditions. I am wondering if there is a typo, since my teachers create our homework online. Is anyone getting the same answer? I want to know if I need to approach my teachers and tell them that there is a possible typo in the homework. Thank you!",,['ordinary-differential-equations']
67,Differential Equation Inequality,Differential Equation Inequality,,"Suppose that for all $t\in [0,1]$ $$ y' \leq 3t  + ty(t), \\ y(0) = 0. $$ I'm asked to find the maximum possible value of $y(1)$ and to give two examples of functions $y(t)$ which satisfy this inequality. The Hint Says that I should multiply the inequality by the integrating factor $u(t)$ one would normally use to solve this type of ODE and then to perform a definite integral. I'm lost on what exactly I'm supposed to do here.","Suppose that for all $t\in [0,1]$ $$ y' \leq 3t  + ty(t), \\ y(0) = 0. $$ I'm asked to find the maximum possible value of $y(1)$ and to give two examples of functions $y(t)$ which satisfy this inequality. The Hint Says that I should multiply the inequality by the integrating factor $u(t)$ one would normally use to solve this type of ODE and then to perform a definite integral. I'm lost on what exactly I'm supposed to do here.",,[]
68,Difficult differential equation,Difficult differential equation,,"So, someone challenged me to solve a differential equation, and this would be unorthodox, but MSE I need your help. It turned out impossible to solve by the methods I know. I would prefer hints over solutions. $$\frac{\mathrm{d}^2 y}{\mathrm{d}x^2}+\frac{\cos x}{\sin x} \frac{\mathrm{d} y}{\mathrm{d}x} +k(k+1)y=0$$","So, someone challenged me to solve a differential equation, and this would be unorthodox, but MSE I need your help. It turned out impossible to solve by the methods I know. I would prefer hints over solutions. $$\frac{\mathrm{d}^2 y}{\mathrm{d}x^2}+\frac{\cos x}{\sin x} \frac{\mathrm{d} y}{\mathrm{d}x} +k(k+1)y=0$$",,['ordinary-differential-equations']
69,How to solve $xy=2\int_1^xy(t)dt+5$?,How to solve ?,xy=2\int_1^xy(t)dt+5,"Could you please give me some hint how to solve this equation: $xy=2\int_1^xy(t)dt+5$. It is not known whether $y(x)$ is continuous or not, so I could not use Fundamental Theorem of Calculus for differentiating both sides of this equation and thus transform integral equation onto differential one. Thanks.","Could you please give me some hint how to solve this equation: $xy=2\int_1^xy(t)dt+5$. It is not known whether $y(x)$ is continuous or not, so I could not use Fundamental Theorem of Calculus for differentiating both sides of this equation and thus transform integral equation onto differential one. Thanks.",,"['ordinary-differential-equations', 'integral-equations']"
70,Stability analysis for a system of two differential equations,Stability analysis for a system of two differential equations,,"I have this system of differential equations: \begin{equation} \frac{dx}{dt}=\alpha x-\beta xy\\ \frac{dy}{dt}=\beta xy-\gamma y \end{equation} I need to find the critical points and then do a stability analysis. After this I need to find the solutions curves for this system. I have started like this: The critical points are $(0,0), (0,\frac{\alpha} {\beta}), (\frac{γ}{β},0)$. Is this correct? As I understand it I need to create a Jacobian matrix: $g(X)$= $\begin{pmatrix} \alpha & -\beta x  \\ \beta y & -\gamma  \end{pmatrix}$, where $X=(x,y)^T$ Then form: (also matrices) $A_1=g(0,0)$= $\begin{pmatrix} \alpha & 0  \\  0 & -\gamma  \end{pmatrix}$, $A_2=g\bigl(0,\frac{\alpha} {\beta}\bigr)$= $\begin{pmatrix} \alpha & 0  \\  \alpha & -\gamma  \end{pmatrix}$, $A_3=g\bigl(\frac{γ}{β},0\bigr)$= $\begin{pmatrix} \alpha & -\gamma  \\  0 & -\gamma  \end{pmatrix}$ My question is how do I know which one is stable? and then how do I form the solution curves.","I have this system of differential equations: \begin{equation} \frac{dx}{dt}=\alpha x-\beta xy\\ \frac{dy}{dt}=\beta xy-\gamma y \end{equation} I need to find the critical points and then do a stability analysis. After this I need to find the solutions curves for this system. I have started like this: The critical points are $(0,0), (0,\frac{\alpha} {\beta}), (\frac{γ}{β},0)$. Is this correct? As I understand it I need to create a Jacobian matrix: $g(X)$= $\begin{pmatrix} \alpha & -\beta x  \\ \beta y & -\gamma  \end{pmatrix}$, where $X=(x,y)^T$ Then form: (also matrices) $A_1=g(0,0)$= $\begin{pmatrix} \alpha & 0  \\  0 & -\gamma  \end{pmatrix}$, $A_2=g\bigl(0,\frac{\alpha} {\beta}\bigr)$= $\begin{pmatrix} \alpha & 0  \\  \alpha & -\gamma  \end{pmatrix}$, $A_3=g\bigl(\frac{γ}{β},0\bigr)$= $\begin{pmatrix} \alpha & -\gamma  \\  0 & -\gamma  \end{pmatrix}$ My question is how do I know which one is stable? and then how do I form the solution curves.",,"['ordinary-differential-equations', 'systems-of-equations', 'stability-in-odes']"
71,Volterra equation for a Bessel type IVP that appears in inverse scattering,Volterra equation for a Bessel type IVP that appears in inverse scattering,,"I have the following i.v.p. (Colton Kress-Inverse acoustic and electromagnetic scattering theory, Springer) $$y''(r)+(k^2n(r)-\frac{l(l+1)}{r^2})y(r)=0$$ $$y(0)=0, y'(0)=1$$ using the Liouville transformation: $z(ξ)=y(r)n(r)^{1/4},\ ξ=\int_0^{r}n(t)dt$,  the problem is transformed to $$z''(ξ)+(k^2-\frac{l(l+1)}{ξ^2}-g(ξ))z(ξ)=0$$ $$z(0)=0,\ z'(0)=\frac{1}{n(0)^{1/4}}$$ for a suitable function g. I want to write the solution as a Volterra integral equation. In the case that l=0, it is easy to find that $$z(ξ)=\frac{sin(kξ)}{kn(0)^{1/4}}+1/k\int_0^{ξ}sin(k(ξ-t))g(t)z(t)dt$$ If $l>0$, I would expect solutions in the form  $\sqrt{ξ} J_{l+1/2}(ξ)$ but I can't find the way.","I have the following i.v.p. (Colton Kress-Inverse acoustic and electromagnetic scattering theory, Springer) $$y''(r)+(k^2n(r)-\frac{l(l+1)}{r^2})y(r)=0$$ $$y(0)=0, y'(0)=1$$ using the Liouville transformation: $z(ξ)=y(r)n(r)^{1/4},\ ξ=\int_0^{r}n(t)dt$,  the problem is transformed to $$z''(ξ)+(k^2-\frac{l(l+1)}{ξ^2}-g(ξ))z(ξ)=0$$ $$z(0)=0,\ z'(0)=\frac{1}{n(0)^{1/4}}$$ for a suitable function g. I want to write the solution as a Volterra integral equation. In the case that l=0, it is easy to find that $$z(ξ)=\frac{sin(kξ)}{kn(0)^{1/4}}+1/k\int_0^{ξ}sin(k(ξ-t))g(t)z(t)dt$$ If $l>0$, I would expect solutions in the form  $\sqrt{ξ} J_{l+1/2}(ξ)$ but I can't find the way.",,"['ordinary-differential-equations', 'integral-equations', 'inverse-problems']"
72,Solution to $\frac{dy}{dx} + \frac{y}{x}=e^{xy}\cos^{2}x$,Solution to,\frac{dy}{dx} + \frac{y}{x}=e^{xy}\cos^{2}x,"Good evening, may I know if my solution to the a/m problem is correct? Thank you. Let $v=xy. $ Then $\dfrac{dv}{dx}= y+\dfrac{dy}{dx}x=xe^v\text{cos}^2{x}$ Hence, $\begin{align}\int e^{-v} dv = \int x\text{cos}^2x\end{align}dx \implies -e^{-xy}= \dfrac{x^2}{4}+\dfrac{x}{4} \text{sin}2x +\dfrac{1}{8}\text{cos}2x + C$","Good evening, may I know if my solution to the a/m problem is correct? Thank you. Let $v=xy. $ Then $\dfrac{dv}{dx}= y+\dfrac{dy}{dx}x=xe^v\text{cos}^2{x}$ Hence, $\begin{align}\int e^{-v} dv = \int x\text{cos}^2x\end{align}dx \implies -e^{-xy}= \dfrac{x^2}{4}+\dfrac{x}{4} \text{sin}2x +\dfrac{1}{8}\text{cos}2x + C$",,"['ordinary-differential-equations', 'proof-verification']"
73,Solution of the Legendre's ODE using Frobenius Method,Solution of the Legendre's ODE using Frobenius Method,,"This is the Legendre's differential equation given in my book: $(1-x)^{2}\ddot{y}-2x\dot{y}+k(k+1)y=0$ I solved this equation by taking: $y=x^{c}\{a_{0}+a_{1}x+a_{2}x^{2}+.....+a_{r}x^{r}+.....\}$ Therefore, each term in the equation becomes (I deliberately didn't use the sum notation to see it clearly): $k(k+1)y=k(k+1)a_{0}x^{c}+k(k+1)a_{1}x^{c+1}+k(k+1)a_{2}x^{c+2}+.....+k(k+1)a_{r}x^{c+r}+.....$ $-2x\dot{y}=-2ca_{0}x^{c}-2(c+1)a_{1}x^{c+1}-2(c+2)a_{2}x^{c+2}-.....-2(c+r)a_{r}x^{c+r}-.....$ $\ddot{y}=(c-1)ca_{0}x^{c-2}+c(c+1)a_{1}x^{c-1}+(c+1)(c+2)a_{2}x^{c}+.....+(c+r-1)(c+r)a_{r}x^{c+r-2}+.....$ $-x^{2}\ddot{y}=-(c-1)ca_{0}x^{c}-c(c+1)a_{1}x^{c+1}-(c+1)(c+2)a_{2}x^{c+2}-.....-(c+r-1)(c+r)a_{r}x^{c+r}-.....$ The indicial equation is $(c-1)ca_{0}x^{c-2}=0$. Therefore, there are two solutions for $c=0$ and $c=1$. Since, $(c+r+2)(c+r+1)a_{r+2}+k(k+1)a_{r}-2(c+r)a_{r}-(c+r)(c+r-1)a_{r}=0$ general recurrence equation is as follows: $a_{r+2}=\frac{[(c+r-k)(c+r+k+1)]a_{r}}{(c+r+2)(c+r+1)}$ There is no problem so far. But my book gives the following answer for $c=0$: $y=a_{0}\left\{ 1-\frac{k(k+1)}{2}x^{2}+\frac{k(k-2)(k+1)(k+3)}{4!}x^{4}-.....\right\}$ This is only possible for $a_1=0$. But I can't see that. I will be very glad if anyone can show me why $a_1$ should be equal to zero when $c=0$. Answer: I solved both for $c=0$ and $c=1$ as @Semiclassical suggested. For $c=0$, $a_1$ is indeterminate because of the term: $c(c+1)a_1x^{c-1}$ (It can be any value). Therefore, the solution is: $u=a_{0}\left\{ 1-\frac{k(k+1)}{2!}x^{2}+\frac{k(k-2)(k+1)(k+3)}{4!}x^{4}-.....\right\} +a_{1}\left\{ x-\frac{(k-1)(k+2)}{3!}x^{3}+\frac{(k-3)(k-1)(k+2)(k+4)}{5!}x^{5}-.....\right\}$ For $c=1$, $a_1=0$ because of the term: $c(c+1)a_1x^{c-1}$, thus the solution is: $w=a_{0}\left\{ x-\frac{(k-1)(k+2)}{3!}x^{3}+\frac{(k-3)(k-1)(k+2)(k+4)}{5!}x^{5}-.....\right\}$ As it is seen this is not a distinct solution. It is already available in the solution for $c=0$. Therefore, the solution for $c=0$ in my book is incomplete. Actually, it should give the complete solution as above. I learned that this happens when indicial roots differ by an integer value.","This is the Legendre's differential equation given in my book: $(1-x)^{2}\ddot{y}-2x\dot{y}+k(k+1)y=0$ I solved this equation by taking: $y=x^{c}\{a_{0}+a_{1}x+a_{2}x^{2}+.....+a_{r}x^{r}+.....\}$ Therefore, each term in the equation becomes (I deliberately didn't use the sum notation to see it clearly): $k(k+1)y=k(k+1)a_{0}x^{c}+k(k+1)a_{1}x^{c+1}+k(k+1)a_{2}x^{c+2}+.....+k(k+1)a_{r}x^{c+r}+.....$ $-2x\dot{y}=-2ca_{0}x^{c}-2(c+1)a_{1}x^{c+1}-2(c+2)a_{2}x^{c+2}-.....-2(c+r)a_{r}x^{c+r}-.....$ $\ddot{y}=(c-1)ca_{0}x^{c-2}+c(c+1)a_{1}x^{c-1}+(c+1)(c+2)a_{2}x^{c}+.....+(c+r-1)(c+r)a_{r}x^{c+r-2}+.....$ $-x^{2}\ddot{y}=-(c-1)ca_{0}x^{c}-c(c+1)a_{1}x^{c+1}-(c+1)(c+2)a_{2}x^{c+2}-.....-(c+r-1)(c+r)a_{r}x^{c+r}-.....$ The indicial equation is $(c-1)ca_{0}x^{c-2}=0$. Therefore, there are two solutions for $c=0$ and $c=1$. Since, $(c+r+2)(c+r+1)a_{r+2}+k(k+1)a_{r}-2(c+r)a_{r}-(c+r)(c+r-1)a_{r}=0$ general recurrence equation is as follows: $a_{r+2}=\frac{[(c+r-k)(c+r+k+1)]a_{r}}{(c+r+2)(c+r+1)}$ There is no problem so far. But my book gives the following answer for $c=0$: $y=a_{0}\left\{ 1-\frac{k(k+1)}{2}x^{2}+\frac{k(k-2)(k+1)(k+3)}{4!}x^{4}-.....\right\}$ This is only possible for $a_1=0$. But I can't see that. I will be very glad if anyone can show me why $a_1$ should be equal to zero when $c=0$. Answer: I solved both for $c=0$ and $c=1$ as @Semiclassical suggested. For $c=0$, $a_1$ is indeterminate because of the term: $c(c+1)a_1x^{c-1}$ (It can be any value). Therefore, the solution is: $u=a_{0}\left\{ 1-\frac{k(k+1)}{2!}x^{2}+\frac{k(k-2)(k+1)(k+3)}{4!}x^{4}-.....\right\} +a_{1}\left\{ x-\frac{(k-1)(k+2)}{3!}x^{3}+\frac{(k-3)(k-1)(k+2)(k+4)}{5!}x^{5}-.....\right\}$ For $c=1$, $a_1=0$ because of the term: $c(c+1)a_1x^{c-1}$, thus the solution is: $w=a_{0}\left\{ x-\frac{(k-1)(k+2)}{3!}x^{3}+\frac{(k-3)(k-1)(k+2)(k+4)}{5!}x^{5}-.....\right\}$ As it is seen this is not a distinct solution. It is already available in the solution for $c=0$. Therefore, the solution for $c=0$ in my book is incomplete. Actually, it should give the complete solution as above. I learned that this happens when indicial roots differ by an integer value.",,['ordinary-differential-equations']
74,How can I find a solution of second order ODE with variable coefficients?,How can I find a solution of second order ODE with variable coefficients?,,I want to find a solution of $$ \left(\frac{d^2}{dx^2} + (1+x^2)^{-1/2} \frac{d}{dx}  + c \right)f(x) = 0 $$ where $x \in \mathbb R$ and $c$ is a real constant.,I want to find a solution of $$ \left(\frac{d^2}{dx^2} + (1+x^2)^{-1/2} \frac{d}{dx}  + c \right)f(x) = 0 $$ where $x \in \mathbb R$ and $c$ is a real constant.,,['ordinary-differential-equations']
75,Estimating a dynamical system's behavior without using Liapunov theorem,Estimating a dynamical system's behavior without using Liapunov theorem,,"Assume that we have the following dynamical system $$x'=(\epsilon x+2y)(1+z)$$ $$y'=(-x+\epsilon y)(1+z)$$ $$z'=-z^3$$ Then how can I show that any solution that started from the region $z>-1$ tends to the only equilibrium of this system, origin? Since I'm trying to verify the Liapunov theorem for this case I want not to use Liapunov theorem to show this. My trial was letting $m=\frac{y}{x}$ so that $$x=Ce^{-{\epsilon\over\sqrt{2}}\arctan(\sqrt{2}m)}\frac{1}{\sqrt{2m^2+1}}$$ and $$y=mx$$ and used the fact that the solution keeps sprialing aroung the origin so $m$must be $\infty$ at some moment so that $x$ becomes zero. But this proof has a lot of gaps and is not so persuasive. How can I show it rigorously, or at least without serious gaps?","Assume that we have the following dynamical system $$x'=(\epsilon x+2y)(1+z)$$ $$y'=(-x+\epsilon y)(1+z)$$ $$z'=-z^3$$ Then how can I show that any solution that started from the region $z>-1$ tends to the only equilibrium of this system, origin? Since I'm trying to verify the Liapunov theorem for this case I want not to use Liapunov theorem to show this. My trial was letting $m=\frac{y}{x}$ so that $$x=Ce^{-{\epsilon\over\sqrt{2}}\arctan(\sqrt{2}m)}\frac{1}{\sqrt{2m^2+1}}$$ and $$y=mx$$ and used the fact that the solution keeps sprialing aroung the origin so $m$must be $\infty$ at some moment so that $x$ becomes zero. But this proof has a lot of gaps and is not so persuasive. How can I show it rigorously, or at least without serious gaps?",,"['ordinary-differential-equations', 'dynamical-systems']"
76,"Questions--Heat equation with $x>0,t>0$",Questions--Heat equation with,"x>0,t>0","I have the following problem: $$u_t=u_{xx}, x>0, t>0$$ $$u(x=0,t)=0 , t>0$$ $$u(x,t=0)=f(x), x>0$$ The solution of the problem is:  $$u(x,t)=\int_0^{+\infty} a(k) \sin(kx) e^{-k^2t} dk$$ $$u(x,0)=f(x)=\int_0^{+\infty} a(k) \sin(kx) dk$$ $$\sin(k'x) f(x)= \sin(k'x) \int_0^{+\infty} a(k) \sin(kx) dk \Rightarrow \int_{0}^{\infty}\sin(k'x) f(x) dx =  \int_0^{+\infty} a(k) \sin(kx) \sin(k'x) dk dx$$ We know the integral: $$\int_{-\infty}^{+\infty} e^{-i(k-k')x}dx= 2 \pi \delta(k-k')$$ $$e^{-ikx} e^{ik'x}=\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x)+ i(\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)) $$ Why do we know that $e^{-ikx} e^{ik'x}=$ is real,so  $\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)=0$ ? Also, why $\int_{-\infty}^{+\infty} (\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x))dx=2 \int_{\infty}^{+\infty} \sin(kx) \sin(k'x) dx$ ?","I have the following problem: $$u_t=u_{xx}, x>0, t>0$$ $$u(x=0,t)=0 , t>0$$ $$u(x,t=0)=f(x), x>0$$ The solution of the problem is:  $$u(x,t)=\int_0^{+\infty} a(k) \sin(kx) e^{-k^2t} dk$$ $$u(x,0)=f(x)=\int_0^{+\infty} a(k) \sin(kx) dk$$ $$\sin(k'x) f(x)= \sin(k'x) \int_0^{+\infty} a(k) \sin(kx) dk \Rightarrow \int_{0}^{\infty}\sin(k'x) f(x) dx =  \int_0^{+\infty} a(k) \sin(kx) \sin(k'x) dk dx$$ We know the integral: $$\int_{-\infty}^{+\infty} e^{-i(k-k')x}dx= 2 \pi \delta(k-k')$$ $$e^{-ikx} e^{ik'x}=\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x)+ i(\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)) $$ Why do we know that $e^{-ikx} e^{ik'x}=$ is real,so  $\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)=0$ ? Also, why $\int_{-\infty}^{+\infty} (\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x))dx=2 \int_{\infty}^{+\infty} \sin(kx) \sin(k'x) dx$ ?",,"['ordinary-differential-equations', 'partial-differential-equations']"
77,Checking Initial Values of an IVP,Checking Initial Values of an IVP,,"Show that the solution of the initial value problem $$y''+p(t)y'+q(t)y=g(t),y(t_0)=y_0, y'(t_0)=y_0'$$ can be written as $y=u(t)+v(t)$, where $u$ and $v$ are solutions of the two initial value problems $$u''+p(t)u'+q(t)u=0, u(t_0)=y_0, u'(t_0)=y_0'$$ $$v''+p(t)v'+q(t)v=g(t), v(t_0)=0, v'(t_0)=0$$ respectively. Be sure to check the initial values. I was able to show that the solution to the IVP can be written as $y=u(t)+v(t)$ (I believe). $$(u+v)''+p(t)(u+v)'+q(u+v)=[u''(t)+v''(t)]+[p(t)(u'(t)+v'(t))]+[q(t)(u(t)+v(t))]$$ $$=(u''+pu'+qu)+(v''+pv'+qv)$$ $$=0+g(t)$$ $$=g(t)$$ The problem I am having is checking the initial values. Generally, when it's a simple $$y''+y'-2y=2t, y(0)=0, y'(0)=1$$ type of problem, checking the initial values is very straightforward. For some reason or another, I can't quite seem to check the initial values on this one. I'm sure it is something silly that I'm having a mental block on because it looks different than normal. Note: this is problem #21 in section 3.7 from Elementary Differential Equations and Boundary Value Problems, Eighth Edition. Boyce, DiPrima.","Show that the solution of the initial value problem $$y''+p(t)y'+q(t)y=g(t),y(t_0)=y_0, y'(t_0)=y_0'$$ can be written as $y=u(t)+v(t)$, where $u$ and $v$ are solutions of the two initial value problems $$u''+p(t)u'+q(t)u=0, u(t_0)=y_0, u'(t_0)=y_0'$$ $$v''+p(t)v'+q(t)v=g(t), v(t_0)=0, v'(t_0)=0$$ respectively. Be sure to check the initial values. I was able to show that the solution to the IVP can be written as $y=u(t)+v(t)$ (I believe). $$(u+v)''+p(t)(u+v)'+q(u+v)=[u''(t)+v''(t)]+[p(t)(u'(t)+v'(t))]+[q(t)(u(t)+v(t))]$$ $$=(u''+pu'+qu)+(v''+pv'+qv)$$ $$=0+g(t)$$ $$=g(t)$$ The problem I am having is checking the initial values. Generally, when it's a simple $$y''+y'-2y=2t, y(0)=0, y'(0)=1$$ type of problem, checking the initial values is very straightforward. For some reason or another, I can't quite seem to check the initial values on this one. I'm sure it is something silly that I'm having a mental block on because it looks different than normal. Note: this is problem #21 in section 3.7 from Elementary Differential Equations and Boundary Value Problems, Eighth Edition. Boyce, DiPrima.",,['ordinary-differential-equations']
78,Simple differential equations problems,Simple differential equations problems,,"I have an ODE. $$ yy^{\prime\prime}-2(y^{\prime})^2+2y^{\prime}=0. $$ My lecturer got the answer $$ y=C, $$ where $C$ is an arbitrary constant, or $$ y=\frac{1}{\sqrt{C_1}}\tan(\sqrt{C_1}x+C_2), $$ where $C_1,C_2$ are arbitrary constants, $C_1>0$. But I think the solution may differ due to the sign of $C_1$. Let $y^{\prime}=v$, then $$ y^{\prime\prime}=\frac{dv}{dx}=\frac{dv}{dy}\frac{dy}{dx}=v\frac{dv}{dy}.  $$ So this ODE becomes  $$ vy\frac{dv}{dy}-2v^2+2v=0. $$ Case 1: $v=0$, then $y=C$, $C$ is an arbitrary constant; Case 2: if $v\ne0$, then separate variables, giving $$ \frac{1}{v-1}dv=\frac{2}{y}dy.  $$ Integrate both sides, we have  $$ \int\frac{1}{v-1}\mathrm{d}v=\int\frac{2}{y}\mathrm{d}y,  $$ i.e., $$ \ln|v-1|=\ln y^2+\ln|K|, $$ where $K$ is a non-zero constant. So we have $$ e^{\ln|v-1|}=e^{\ln|K|}e^{\ln{y^2}},  $$ and finally  $$ |v-1|=|K|y^2. $$ Therefore, $$ v-1=\pm|K|y^2. $$ Let $C_1=\pm|K|$, then  $$ v=C_1y^2+1, $$ where $C_1$ is a non-zero constant. Separate variables again, we get $$ \int\frac{1}{C_1y^2+1}\mathrm{d}y=\int \mathrm{d}x.  $$  Here is the problem: if $C_1<0$, we cannot use the integral $$ \int\frac{1}{x^2+a^2}\mathrm{d}x=\frac{1}{a}\arctan\frac{x}{a}+C,  $$ where $C$ is an arbitrary constant, instead, we need to use  $$ \int\frac{1}{ax^2+b}\mathrm{d}x=\frac{1}{2\sqrt{-ab}}\ln\left|\frac{\sqrt{a}x-\sqrt{-b}}{\sqrt{a}x+\sqrt{-b}}\right|+C,\ a>0\ \text{and}\ b<0 $$ to solve the ODE. Am I wrong or was my lecturer wrong? I always got confused with these constants in first order ODE's. Thank you.","I have an ODE. $$ yy^{\prime\prime}-2(y^{\prime})^2+2y^{\prime}=0. $$ My lecturer got the answer $$ y=C, $$ where $C$ is an arbitrary constant, or $$ y=\frac{1}{\sqrt{C_1}}\tan(\sqrt{C_1}x+C_2), $$ where $C_1,C_2$ are arbitrary constants, $C_1>0$. But I think the solution may differ due to the sign of $C_1$. Let $y^{\prime}=v$, then $$ y^{\prime\prime}=\frac{dv}{dx}=\frac{dv}{dy}\frac{dy}{dx}=v\frac{dv}{dy}.  $$ So this ODE becomes  $$ vy\frac{dv}{dy}-2v^2+2v=0. $$ Case 1: $v=0$, then $y=C$, $C$ is an arbitrary constant; Case 2: if $v\ne0$, then separate variables, giving $$ \frac{1}{v-1}dv=\frac{2}{y}dy.  $$ Integrate both sides, we have  $$ \int\frac{1}{v-1}\mathrm{d}v=\int\frac{2}{y}\mathrm{d}y,  $$ i.e., $$ \ln|v-1|=\ln y^2+\ln|K|, $$ where $K$ is a non-zero constant. So we have $$ e^{\ln|v-1|}=e^{\ln|K|}e^{\ln{y^2}},  $$ and finally  $$ |v-1|=|K|y^2. $$ Therefore, $$ v-1=\pm|K|y^2. $$ Let $C_1=\pm|K|$, then  $$ v=C_1y^2+1, $$ where $C_1$ is a non-zero constant. Separate variables again, we get $$ \int\frac{1}{C_1y^2+1}\mathrm{d}y=\int \mathrm{d}x.  $$  Here is the problem: if $C_1<0$, we cannot use the integral $$ \int\frac{1}{x^2+a^2}\mathrm{d}x=\frac{1}{a}\arctan\frac{x}{a}+C,  $$ where $C$ is an arbitrary constant, instead, we need to use  $$ \int\frac{1}{ax^2+b}\mathrm{d}x=\frac{1}{2\sqrt{-ab}}\ln\left|\frac{\sqrt{a}x-\sqrt{-b}}{\sqrt{a}x+\sqrt{-b}}\right|+C,\ a>0\ \text{and}\ b<0 $$ to solve the ODE. Am I wrong or was my lecturer wrong? I always got confused with these constants in first order ODE's. Thank you.",,['ordinary-differential-equations']
79,$x'=Ax$ has one periodic solution. Prove that all solutions are periodic.,has one periodic solution. Prove that all solutions are periodic.,x'=Ax,"I want to prove the following: 1) Suppose $$A_{2,2}=\begin{pmatrix} a_{1,1} & a_{1,2} \\ a_{2,1} & a_{2,2} \end{pmatrix}$$  real    and suppose the system of differential equations $$\begin{matrix}         x'=a_{1,1}x+a_{1,2}y\\         y'=a_{2,1}x+a_{2,2}y  \end{matrix}$$ has at least one periodic solution $\begin{pmatrix} x \\ y \end{pmatrix}=\begin{pmatrix} f(t) \\ g(t) \end{pmatrix}$.     Show that in this case, all solutions are periodic. 2) Suppose $A \in M_{n,n}(\mathbb{R})$ is invertible with $n$ odd. Show that there exists a solution to the system of equations $x'=Ax$ that is not periodic. I came up to the following. We proved in lecture that an ODE of this kind has a solution of the form $x(t)=\exp(At)\,x(0)$. As our solution is periodic of period $\omega$, we know: $$ \begin{pmatrix} f(t) \\ g(t) \end{pmatrix} =exp(At)\,\begin{pmatrix} f(0) \\ g(0) \end{pmatrix}=\exp(A(t+\omega))\,\begin{pmatrix} f(0) \\ g(0) \end{pmatrix}=\begin{pmatrix} f(t+\omega) \\ g(t+\omega) \end{pmatrix}$$ My ideas was now to show that $\exp(At)=\exp(A(t+\omega))$ and then deduce that all solutions have to be periodic. But I kinda stuck by how I should prove this.","I want to prove the following: 1) Suppose $$A_{2,2}=\begin{pmatrix} a_{1,1} & a_{1,2} \\ a_{2,1} & a_{2,2} \end{pmatrix}$$  real    and suppose the system of differential equations $$\begin{matrix}         x'=a_{1,1}x+a_{1,2}y\\         y'=a_{2,1}x+a_{2,2}y  \end{matrix}$$ has at least one periodic solution $\begin{pmatrix} x \\ y \end{pmatrix}=\begin{pmatrix} f(t) \\ g(t) \end{pmatrix}$.     Show that in this case, all solutions are periodic. 2) Suppose $A \in M_{n,n}(\mathbb{R})$ is invertible with $n$ odd. Show that there exists a solution to the system of equations $x'=Ax$ that is not periodic. I came up to the following. We proved in lecture that an ODE of this kind has a solution of the form $x(t)=\exp(At)\,x(0)$. As our solution is periodic of period $\omega$, we know: $$ \begin{pmatrix} f(t) \\ g(t) \end{pmatrix} =exp(At)\,\begin{pmatrix} f(0) \\ g(0) \end{pmatrix}=\exp(A(t+\omega))\,\begin{pmatrix} f(0) \\ g(0) \end{pmatrix}=\begin{pmatrix} f(t+\omega) \\ g(t+\omega) \end{pmatrix}$$ My ideas was now to show that $\exp(At)=\exp(A(t+\omega))$ and then deduce that all solutions have to be periodic. But I kinda stuck by how I should prove this.",,"['ordinary-differential-equations', 'periodic-functions']"
80,Boundary value problem-Is the solution correct?,Boundary value problem-Is the solution correct?,,"Having the following boundary value problem: $$u_{xx}+u_{yy}=0,\quad 0<x<a,\; 0<y<b, \tag{1}$$ $$u_x(0,y)=u_x(a,y)=0,\quad 0\leq y\leq b,$$ $$u(x,0)= \cos{(\frac{\pi x}{a})},\;\; u(x,b)=\cos^2{(\frac{\pi x}{a})},\;\;  0\leq x\leq a.$$ I have done the following: Using the method of separation of variables, the solution is of the form $u(x,y)=X(x)Y(y)$ $$(1) \Rightarrow X''Y+XY''=0 \Rightarrow \frac{X''}{X}+\frac{Y''}{Y}=0 \Rightarrow \frac{X''}{X}=- \frac{Y''}{Y}=- \lambda$$ So we get the following problems: $$\left.\begin{matrix} X''+ \lambda X=0, 0<x<a\\  X'(0)=X'(a)=0 \end{matrix}\right\}\tag{*}$$ $(*) \Rightarrow $ The eigenvalues are $\lambda =0$ and the corresponding eigenvalue is $X_0(x)=1$; $\lambda_n =\left(\frac{n \pi}{a}\right)^2$ and the eigenfunctions are $ X_n(x)= \cos\left(\frac{n \pi x}{a}\right)$. $$\left.\begin{matrix} Y_n''- \lambda_n Y_n=0,\quad 0<y<b\\  u(x,0)=\sum_{n=0}^{\infty} X_n(x)Y_n(0)= \cos{(\frac{\pi x}{a})}\\ u(x,b)=\sum_{n=0}^{\infty} X_n(x)Y_n(b)=\cos^2{(\frac{\pi x}{a})}\end{matrix}\right\} \tag{**}$$ $$(**) \Rightarrow Y_n(y)=\left\{\begin{matrix} a_0y+b_0, & n=0\\  a_n \sinh\left(\frac{n \pi y}{a}\right)+b_n \cosh\left(\frac{n \pi y}{a}\right) & n=1,2,3, \dots\,. \end{matrix}\right.$$ So $$u(x,y)=a_0y+b_0+ \sum_{n=1}^{\infty}{[a_n \sinh\left(\frac{n \pi y}{a}\right)+b_n \cosh\left(\frac{n \pi y}{a}\right) ] \cos{(\frac{n \pi x}{a})}}$$ By the condition $u(x,0)= \cos{(\frac{\pi x}{a})}$ we get: $b_0+ \sum_{n=1}^{\infty}{b_n  \cos{(\frac{n \pi x}{a})}}=\cos{(\frac{\pi x}{a})}$ So $$b_0=0, b_1=1, b_n=0(n \neq 1)$$ $$\Rightarrow u(x,y)=a_0y+  \cosh\left(\frac{ \pi y}{a}\right) \cos{(\frac{n \pi x}{a})}  +\sum_{n=1}^{\infty}{a_n \sinh\left(\frac{n \pi y}{a}\right) \cos{(\frac{n \pi x}{a})}}$$ By the condition $u(x,b)=\cos^2{(\frac{\pi x}{a})}$ we get: $a_0b+  \cosh\left(\frac{ \pi b}{a}\right) \cos{(\frac{n \pi x}{a})}  +\sum_{n=1}^{\infty}{a_n \sinh\left(\frac{n \pi b}{a}\right) \cos{(\frac{n \pi x}{a})}}=\cos^2{(\frac{\pi x}{a})}$ So $$a_0=\frac{1}{2b}, a_1=-\coth{(\frac{\pi b}{a})}, a_2=\frac{1}{2 \sinh{(\frac{2 \pi b}{a})}},\; a_n=0\;(n \geq 3).$$ So the solution is: $$u(x,y)=\frac{1}{2b}y+[-\coth{(\frac{\pi b}{a})} \sinh{(\frac{\pi y}{a})}+\cosh{(\frac{\pi y}{a})}] \cos{(\frac{\pi x}{a})}+\frac{1}{2 \sinh{(\frac{2 \pi b}{a})}} \sinh{(\frac{2 \pi y}{a})} \cos{(\frac{2 \pi x}{a})}$$ Are the coefficients that I have found correct?? Is the solution of the problem correct??","Having the following boundary value problem: $$u_{xx}+u_{yy}=0,\quad 0<x<a,\; 0<y<b, \tag{1}$$ $$u_x(0,y)=u_x(a,y)=0,\quad 0\leq y\leq b,$$ $$u(x,0)= \cos{(\frac{\pi x}{a})},\;\; u(x,b)=\cos^2{(\frac{\pi x}{a})},\;\;  0\leq x\leq a.$$ I have done the following: Using the method of separation of variables, the solution is of the form $u(x,y)=X(x)Y(y)$ $$(1) \Rightarrow X''Y+XY''=0 \Rightarrow \frac{X''}{X}+\frac{Y''}{Y}=0 \Rightarrow \frac{X''}{X}=- \frac{Y''}{Y}=- \lambda$$ So we get the following problems: $$\left.\begin{matrix} X''+ \lambda X=0, 0<x<a\\  X'(0)=X'(a)=0 \end{matrix}\right\}\tag{*}$$ $(*) \Rightarrow $ The eigenvalues are $\lambda =0$ and the corresponding eigenvalue is $X_0(x)=1$; $\lambda_n =\left(\frac{n \pi}{a}\right)^2$ and the eigenfunctions are $ X_n(x)= \cos\left(\frac{n \pi x}{a}\right)$. $$\left.\begin{matrix} Y_n''- \lambda_n Y_n=0,\quad 0<y<b\\  u(x,0)=\sum_{n=0}^{\infty} X_n(x)Y_n(0)= \cos{(\frac{\pi x}{a})}\\ u(x,b)=\sum_{n=0}^{\infty} X_n(x)Y_n(b)=\cos^2{(\frac{\pi x}{a})}\end{matrix}\right\} \tag{**}$$ $$(**) \Rightarrow Y_n(y)=\left\{\begin{matrix} a_0y+b_0, & n=0\\  a_n \sinh\left(\frac{n \pi y}{a}\right)+b_n \cosh\left(\frac{n \pi y}{a}\right) & n=1,2,3, \dots\,. \end{matrix}\right.$$ So $$u(x,y)=a_0y+b_0+ \sum_{n=1}^{\infty}{[a_n \sinh\left(\frac{n \pi y}{a}\right)+b_n \cosh\left(\frac{n \pi y}{a}\right) ] \cos{(\frac{n \pi x}{a})}}$$ By the condition $u(x,0)= \cos{(\frac{\pi x}{a})}$ we get: $b_0+ \sum_{n=1}^{\infty}{b_n  \cos{(\frac{n \pi x}{a})}}=\cos{(\frac{\pi x}{a})}$ So $$b_0=0, b_1=1, b_n=0(n \neq 1)$$ $$\Rightarrow u(x,y)=a_0y+  \cosh\left(\frac{ \pi y}{a}\right) \cos{(\frac{n \pi x}{a})}  +\sum_{n=1}^{\infty}{a_n \sinh\left(\frac{n \pi y}{a}\right) \cos{(\frac{n \pi x}{a})}}$$ By the condition $u(x,b)=\cos^2{(\frac{\pi x}{a})}$ we get: $a_0b+  \cosh\left(\frac{ \pi b}{a}\right) \cos{(\frac{n \pi x}{a})}  +\sum_{n=1}^{\infty}{a_n \sinh\left(\frac{n \pi b}{a}\right) \cos{(\frac{n \pi x}{a})}}=\cos^2{(\frac{\pi x}{a})}$ So $$a_0=\frac{1}{2b}, a_1=-\coth{(\frac{\pi b}{a})}, a_2=\frac{1}{2 \sinh{(\frac{2 \pi b}{a})}},\; a_n=0\;(n \geq 3).$$ So the solution is: $$u(x,y)=\frac{1}{2b}y+[-\coth{(\frac{\pi b}{a})} \sinh{(\frac{\pi y}{a})}+\cosh{(\frac{\pi y}{a})}] \cos{(\frac{\pi x}{a})}+\frac{1}{2 \sinh{(\frac{2 \pi b}{a})}} \sinh{(\frac{2 \pi y}{a})} \cos{(\frac{2 \pi x}{a})}$$ Are the coefficients that I have found correct?? Is the solution of the problem correct??",,"['ordinary-differential-equations', 'partial-differential-equations']"
81,"For $x' = y$, and $y' = -x - y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable.","For , and , Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable.",x' = y y' = -x - y,"For $x' = y$, and $y' = -x - y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable. I found that the equilibrium points are (0,0). Then I try to find the eigenvalues. The characteristic equation I found is $\lambda + \lambda^2+1=0$. I realize that they are complex eigenvalues...The real part of this complex eigenvalues are negative. Does this mean this is stable ? Is this also asymptotically stable? What are the differences between asymptotically stable and stable?","For $x' = y$, and $y' = -x - y$, Find all equilibrium points and decide whether they are stable, asymptotically stable, or unstable. I found that the equilibrium points are (0,0). Then I try to find the eigenvalues. The characteristic equation I found is $\lambda + \lambda^2+1=0$. I realize that they are complex eigenvalues...The real part of this complex eigenvalues are negative. Does this mean this is stable ? Is this also asymptotically stable? What are the differences between asymptotically stable and stable?",,"['ordinary-differential-equations', 'systems-of-equations']"
82,General and particular solution of differential equation,General and particular solution of differential equation,,"1) I need to find, in implicit form, the general solution of the differential equation  $$\frac{dy}{dx}=\frac{2y^4e^{2x}}{3(e^{2x}+7)^2}$$ 2) I then need to find the corresponding particular solution (in implicit form) that satisfies the initial condition $y=2$ and $x=0$. 3) I then need to find the explicit form of this particular solution. For the first part I came up with $$-\frac{3}{y^4}\frac{ dy}{dx}= \frac{-2e^{2x}}{(e^{2x}+7)^2}$$ which is $$\frac{d}{dx} (y^{-3})=\frac{d}{dx}\left(\frac{1}{e^{2x}+7}\right)$$ then $y^{-3}=\frac{1}{e^{2x}+7}  +c$ For part 2) i got $c=0$ so the particular solution would be $y^{-3}=\frac{1}{e^{2x}+7}.$ However I am confused as to how to do the 3rd part as the answer I got for part 2 seems to be in explicit form. I am not sure if I did the first part correctly even so need quite a bit of help!","1) I need to find, in implicit form, the general solution of the differential equation  $$\frac{dy}{dx}=\frac{2y^4e^{2x}}{3(e^{2x}+7)^2}$$ 2) I then need to find the corresponding particular solution (in implicit form) that satisfies the initial condition $y=2$ and $x=0$. 3) I then need to find the explicit form of this particular solution. For the first part I came up with $$-\frac{3}{y^4}\frac{ dy}{dx}= \frac{-2e^{2x}}{(e^{2x}+7)^2}$$ which is $$\frac{d}{dx} (y^{-3})=\frac{d}{dx}\left(\frac{1}{e^{2x}+7}\right)$$ then $y^{-3}=\frac{1}{e^{2x}+7}  +c$ For part 2) i got $c=0$ so the particular solution would be $y^{-3}=\frac{1}{e^{2x}+7}.$ However I am confused as to how to do the 3rd part as the answer I got for part 2 seems to be in explicit form. I am not sure if I did the first part correctly even so need quite a bit of help!",,"['calculus', 'integration', 'ordinary-differential-equations', 'implicit-differentiation']"
83,"The function $f(x)$ passes through the point $(-1,6)$ and $f'(x)=3x^2+2x+1$",The function  passes through the point  and,"f(x) (-1,6) f'(x)=3x^2+2x+1",a) find $f(x)$ I found that to be $f(x) = x^3+x^2+x+7$ b) Prove $f$ has a root between $x=-3$ and $x=-2$ I don't know how to do part b.,a) find $f(x)$ I found that to be $f(x) = x^3+x^2+x+7$ b) Prove $f$ has a root between $x=-3$ and $x=-2$ I don't know how to do part b.,,"['ordinary-differential-equations', 'math-software']"
84,which of the following statements are true?? ( Calculus and Differential equations)(NBHM-$2014$),which of the following statements are true?? ( Calculus and Differential equations)(NBHM-),2014,"Which of the following statements are ?? a. Let $\phi$ be a non-negative and continuously differentiable function on $(0,\infty)$ such that $\phi'(x)\le\phi(x)$ for all $x$ $\in (0,\infty)$. Then $$lim_{x\to \infty}\phi(x)=0$$ b. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and differentiable on     $(0,\infty)$ such that $\phi(0)=0$ and such that   $\phi'(x)\le\phi(x)$ for all $x$ $\in   (0,\infty)$. Then $\phi=0$. c. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and such that        $$\phi(x) \le \int_{0}^{x}\phi(t) dt$$ for all $x \in [0,\infty)$. Then $\phi=0$.","Which of the following statements are ?? a. Let $\phi$ be a non-negative and continuously differentiable function on $(0,\infty)$ such that $\phi'(x)\le\phi(x)$ for all $x$ $\in (0,\infty)$. Then $$lim_{x\to \infty}\phi(x)=0$$ b. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and differentiable on     $(0,\infty)$ such that $\phi(0)=0$ and such that   $\phi'(x)\le\phi(x)$ for all $x$ $\in   (0,\infty)$. Then $\phi=0$. c. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and such that        $$\phi(x) \le \int_{0}^{x}\phi(t) dt$$ for all $x \in [0,\infty)$. Then $\phi=0$.",,"['calculus', 'ordinary-differential-equations']"
85,Solving a differential equation with natural log,Solving a differential equation with natural log,,"I am given: $x\dfrac{dy}{dx}=\dfrac{1}{y^3}$ After separating and integrating, I have: $y^4/4=\ln x+C$ I am supposed to solve this equation, but I'm stuck here.  Should I solve explicitly so I can keep $C$? EDIT: A solution I came up with last night was: $y=(4\ln x+C)^{1/4}$","I am given: $x\dfrac{dy}{dx}=\dfrac{1}{y^3}$ After separating and integrating, I have: $y^4/4=\ln x+C$ I am supposed to solve this equation, but I'm stuck here.  Should I solve explicitly so I can keep $C$? EDIT: A solution I came up with last night was: $y=(4\ln x+C)^{1/4}$",,['ordinary-differential-equations']
86,Lowercase delta in differential-like equation,Lowercase delta in differential-like equation,,"Preface : The following question comes from an expression seen in a biophysics paper published in Nature protocols . I'm aware that in pure mathematical notation $\delta$ is never used in the context of differential equations, however, I have an expression with the following structure; $$\frac{\delta x}{\delta y} = G(x)$$ Where G is a function which depends on $x$. Rather than being a differential equation, I'm assuming this is a (somewhat misleading) way of looking at non-infinitesimal change. In the legend the following text appears ""$\delta x/ \delta y$ is the rate of change of ellipticity at any time t"" But in the original paper's context this $dt$ is likely to be value equal to or greater than 10 ms - i.e. this may not hold true as $\delta t \rightarrow 0$. This suggests that treating the expression as a differential equation would be inappropriate. Any thoughts/comments would be appreciated (even if it's just a 'yup - makes sense').","Preface : The following question comes from an expression seen in a biophysics paper published in Nature protocols . I'm aware that in pure mathematical notation $\delta$ is never used in the context of differential equations, however, I have an expression with the following structure; $$\frac{\delta x}{\delta y} = G(x)$$ Where G is a function which depends on $x$. Rather than being a differential equation, I'm assuming this is a (somewhat misleading) way of looking at non-infinitesimal change. In the legend the following text appears ""$\delta x/ \delta y$ is the rate of change of ellipticity at any time t"" But in the original paper's context this $dt$ is likely to be value equal to or greater than 10 ms - i.e. this may not hold true as $\delta t \rightarrow 0$. This suggests that treating the expression as a differential equation would be inappropriate. Any thoughts/comments would be appreciated (even if it's just a 'yup - makes sense').",,"['ordinary-differential-equations', 'notation', 'mathematical-physics']"
87,Nonlinear ODE strategy,Nonlinear ODE strategy,,"I encountered the following $2^{\text{nd}}$ -order, nonlinear ODE while working on a classical mechanics problem: $$ \frac{d^2r}{dt^2}-\frac{\alpha^2}{r^3}+\beta=0 $$ where $\alpha, \beta > 0$ are given. I'm really struggling to find an analytic solution. I tried taking the inverse Laplace Transform, but when I transformed back, I got nonsense. Any advice on a good strategy for solving this problem? Motivation Suppose you have a marble constrained to move along the inside surface of a cylindrical, frictionless, concave cone. Its Lagrangian is given by $$L=\frac{m}{2}\left(\dot r^2+(r\dot\phi)^2+\dot z^2\right) -mgz+\lambda(z-r\tan(\theta))$$ where $(r,\phi,z)$ are the normal cylindrical coordinates, $\theta$ is the inclination of the cone above horizontal, and $\lambda$ is the Lagrange multiplier. Solving Lagrange's equations to eliminate $\lambda$ and rewrite $z$ in terms of $r$ gives the above equation.","I encountered the following -order, nonlinear ODE while working on a classical mechanics problem: where are given. I'm really struggling to find an analytic solution. I tried taking the inverse Laplace Transform, but when I transformed back, I got nonsense. Any advice on a good strategy for solving this problem? Motivation Suppose you have a marble constrained to move along the inside surface of a cylindrical, frictionless, concave cone. Its Lagrangian is given by where are the normal cylindrical coordinates, is the inclination of the cone above horizontal, and is the Lagrange multiplier. Solving Lagrange's equations to eliminate and rewrite in terms of gives the above equation.","2^{\text{nd}}  \frac{d^2r}{dt^2}-\frac{\alpha^2}{r^3}+\beta=0  \alpha, \beta > 0 L=\frac{m}{2}\left(\dot r^2+(r\dot\phi)^2+\dot z^2\right) -mgz+\lambda(z-r\tan(\theta)) (r,\phi,z) \theta \lambda \lambda z r","['calculus', 'ordinary-differential-equations', 'classical-mechanics']"
88,Laplace transformations in differential equations,Laplace transformations in differential equations,,"Using Laplace transformations solve the differential equation $$y''+3y'-17y=e^{-3x}$$ $$y(0)=4 , y'(0)=7$$ I'm having some trouble understanding the basic concept and seeing an example done I'm sure will clear some confusion.","Using Laplace transformations solve the differential equation $$y''+3y'-17y=e^{-3x}$$ $$y(0)=4 , y'(0)=7$$ I'm having some trouble understanding the basic concept and seeing an example done I'm sure will clear some confusion.",,"['ordinary-differential-equations', 'laplace-transform']"
89,General solution to a differential equation with an initial condition,General solution to a differential equation with an initial condition,,"How can I find the general solution of this DE? where $D$, $R$, and $I$ are constant parameters. $$\frac{D}{2}\frac{\ d ^2 y}{\ d x^2 \,} + {(x-RI-D)}\frac{\ d y}{\ d x} + (1+\lambda){y}{} = 0$$ and the initial conditions: $$y(x_0)=a\space;\dot y(x_0)=b$$","How can I find the general solution of this DE? where $D$, $R$, and $I$ are constant parameters. $$\frac{D}{2}\frac{\ d ^2 y}{\ d x^2 \,} + {(x-RI-D)}\frac{\ d y}{\ d x} + (1+\lambda){y}{} = 0$$ and the initial conditions: $$y(x_0)=a\space;\dot y(x_0)=b$$",,['ordinary-differential-equations']
90,Solving homogeneous differential equation in symmetric form,Solving homogeneous differential equation in symmetric form,,"Let $g: \mathbb R \rightarrow \mathbb R$ be a differentiable and integrable function. The integral curve of the differential equation is: $(y + g(x))dx + (x - g(y))dy = 0$ that passes through the point $(1, 1)$ must also pass through which of the following points? $(0, 0),$ $(2, 1/2),$ $(1/2, 2),$ $(-1, -1),$ or  $(0, 1)$","Let $g: \mathbb R \rightarrow \mathbb R$ be a differentiable and integrable function. The integral curve of the differential equation is: $(y + g(x))dx + (x - g(y))dy = 0$ that passes through the point $(1, 1)$ must also pass through which of the following points? $(0, 0),$ $(2, 1/2),$ $(1/2, 2),$ $(-1, -1),$ or  $(0, 1)$",,['ordinary-differential-equations']
91,Convergence of Integral of Matrices,Convergence of Integral of Matrices,,"I have proved the following convergence, but I'm not convinced with my answer. Suppose we have the following $$\lim_{t \rightarrow \infty} \text{tr}\int_{0}^{t}e^{sQ^T}Pe^{sQ} ds$$ where tr is trace. I know $P$ is positive semi-definite, $Q$ is negative-definite. Let the Jordan normal form of $Q$ be $Q = SJS^{-1}$. Step 1 : bring trace inside and use cyclic permutation property $$\lim_{t \rightarrow \infty} \int_{0}^{t}\text{tr}(Pe^{sQ}e^{sQ^T}) ds$$ Step 2 : $P$ is positive semi-definite, $Q$ negative-definite, so $e^{sQ}$ is positive semi-definite. Space of PSD Matrices has an inner product, and tr can be one, so apply Cauchy-Schwarz inequality and get $\text{tr}(Pe^{sQ}e^{sQ^T}) \le \sqrt{\text{tr}(P^2)\text{tr}((e^{sQ}e^{sQ^T})^2)}$. But since they are PSD, we can take out the squares and get $$\text{tr}(Pe^{sQ}e^{sQ^T}) \le \sqrt{\text{tr}(P)^2\text{tr}((e^{sQ}e^{sQ^T}))^2} = {\text{tr}(P)\text{tr}((e^{sQ}e^{sQ^T}))}$$ Step 3 : Take out $\text{tr}(P)$ from the integral $$\lim_{t \rightarrow \infty} \int_{0}^{t}\text{tr}(Pe^{sQ}e^{sQ^T})ds \le \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ}e^{sQ^T})ds$$ Step 4 : Apply Step 2 to term inside integral $$\lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ}e^{sQ^T}) ds \le \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ})\text{tr}(e^{sQ^T})ds$$ Step 5 : Trace is sum of eigenvalues $$\lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ})\text{tr}(e^{sQ^T})ds  = \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t} 2\sum_{i=0}^N e^{s\lambda_i} $$ Step 6 : Swap integral and summation, and integrate $$\lim_{t \rightarrow \infty} \text{tr}(P) 2\sum_{i=0}^N \int_{0}^{t} e^{s\lambda_i} = \lim_{t \rightarrow \infty} \text{tr}(P) 2\sum_{i=0}^N \frac{e^{t\lambda_i} - 1}{\lambda_i}$$ Step 7 All $\lambda_i < 0$ $\Rightarrow \lim_{t \rightarrow \infty} e^{t\lambda_i} \rightarrow 0$. Take out $-1$ from all $\lambda$ and we get $$\le 2*\frac{tr(P)}{|tr(Q)|}$$ Is there anything wrong with this proof that I'm not seeing?","I have proved the following convergence, but I'm not convinced with my answer. Suppose we have the following $$\lim_{t \rightarrow \infty} \text{tr}\int_{0}^{t}e^{sQ^T}Pe^{sQ} ds$$ where tr is trace. I know $P$ is positive semi-definite, $Q$ is negative-definite. Let the Jordan normal form of $Q$ be $Q = SJS^{-1}$. Step 1 : bring trace inside and use cyclic permutation property $$\lim_{t \rightarrow \infty} \int_{0}^{t}\text{tr}(Pe^{sQ}e^{sQ^T}) ds$$ Step 2 : $P$ is positive semi-definite, $Q$ negative-definite, so $e^{sQ}$ is positive semi-definite. Space of PSD Matrices has an inner product, and tr can be one, so apply Cauchy-Schwarz inequality and get $\text{tr}(Pe^{sQ}e^{sQ^T}) \le \sqrt{\text{tr}(P^2)\text{tr}((e^{sQ}e^{sQ^T})^2)}$. But since they are PSD, we can take out the squares and get $$\text{tr}(Pe^{sQ}e^{sQ^T}) \le \sqrt{\text{tr}(P)^2\text{tr}((e^{sQ}e^{sQ^T}))^2} = {\text{tr}(P)\text{tr}((e^{sQ}e^{sQ^T}))}$$ Step 3 : Take out $\text{tr}(P)$ from the integral $$\lim_{t \rightarrow \infty} \int_{0}^{t}\text{tr}(Pe^{sQ}e^{sQ^T})ds \le \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ}e^{sQ^T})ds$$ Step 4 : Apply Step 2 to term inside integral $$\lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ}e^{sQ^T}) ds \le \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ})\text{tr}(e^{sQ^T})ds$$ Step 5 : Trace is sum of eigenvalues $$\lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t}\text{tr}(e^{sQ})\text{tr}(e^{sQ^T})ds  = \lim_{t \rightarrow \infty} \text{tr}(P)\int_{0}^{t} 2\sum_{i=0}^N e^{s\lambda_i} $$ Step 6 : Swap integral and summation, and integrate $$\lim_{t \rightarrow \infty} \text{tr}(P) 2\sum_{i=0}^N \int_{0}^{t} e^{s\lambda_i} = \lim_{t \rightarrow \infty} \text{tr}(P) 2\sum_{i=0}^N \frac{e^{t\lambda_i} - 1}{\lambda_i}$$ Step 7 All $\lambda_i < 0$ $\Rightarrow \lim_{t \rightarrow \infty} e^{t\lambda_i} \rightarrow 0$. Take out $-1$ from all $\lambda$ and we get $$\le 2*\frac{tr(P)}{|tr(Q)|}$$ Is there anything wrong with this proof that I'm not seeing?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
92,Finding the jacobian of a differential system with a piecewise function,Finding the jacobian of a differential system with a piecewise function,,"My system: $$\frac{\mathrm{dx} }{\mathrm{d} t}=-ax^2+y^2-\gamma z$$ $$\frac{\mathrm{dy} }{\mathrm{d} t}=- h(y)-\beta y $$ $$\frac{\mathrm{dz} }{\mathrm{d} t}=x+h(y)-\beta z $$ where $h$ is the piecewise function $$h(y)=\alpha(m_1 y+\frac{1}{2}(m_0 - m_1)(|y+1|-|y-1|)).$$ $$\frac{\mathrm{d} h}{\mathrm{d} y}=\alpha m_1+0.5(m_0-m_1)(\frac{y+1}{|y+1|}-\frac{y-1}{|y-1|})$$, is this right? Upon solving, I get the Jacobian $J$ $$J= \left( \begin{array}{ccc} -2ax & 2y  & -\gamma \\ 0 & \alpha m-\beta & 0 \\ 1 & \alpha m & -\beta \end{array} \right) $$ where, I've decided to generalize the Jacobian for all parts of the piecewise linear function by using the relation: $$ m = \begin{cases} m_1, |y|\geq 1 \\ m_0, |y|\leq 1\\ \end{cases} $$ Is this a right way to go about it? Q2.The system with values would look like: $$\frac{\mathrm{dx} }{\mathrm{d} t}=-0.2x^2+y^2-z$$ $$\frac{\mathrm{dy} }{\mathrm{d} t}=- h(y)-0.2 y $$ $$\frac{\mathrm{dz} }{\mathrm{d} t}=x+h(y)-0.2 z $$ with equilibrium points given by: $$x^*=(0;0.3140;-0.3140); y^*=(0;80/49;-80/49); z^*=(0;1.5699;-1.5699);$$ so,in the end how would I choose the distribution of the y* coordinates to find the equilibrium points?(as it is clearly shown,all equilibrium points are functions of y*). The only thing I can say for certain(as you guys have pointed out) is that y* cannot equal to 1.","My system: $$\frac{\mathrm{dx} }{\mathrm{d} t}=-ax^2+y^2-\gamma z$$ $$\frac{\mathrm{dy} }{\mathrm{d} t}=- h(y)-\beta y $$ $$\frac{\mathrm{dz} }{\mathrm{d} t}=x+h(y)-\beta z $$ where $h$ is the piecewise function $$h(y)=\alpha(m_1 y+\frac{1}{2}(m_0 - m_1)(|y+1|-|y-1|)).$$ $$\frac{\mathrm{d} h}{\mathrm{d} y}=\alpha m_1+0.5(m_0-m_1)(\frac{y+1}{|y+1|}-\frac{y-1}{|y-1|})$$, is this right? Upon solving, I get the Jacobian $J$ $$J= \left( \begin{array}{ccc} -2ax & 2y  & -\gamma \\ 0 & \alpha m-\beta & 0 \\ 1 & \alpha m & -\beta \end{array} \right) $$ where, I've decided to generalize the Jacobian for all parts of the piecewise linear function by using the relation: $$ m = \begin{cases} m_1, |y|\geq 1 \\ m_0, |y|\leq 1\\ \end{cases} $$ Is this a right way to go about it? Q2.The system with values would look like: $$\frac{\mathrm{dx} }{\mathrm{d} t}=-0.2x^2+y^2-z$$ $$\frac{\mathrm{dy} }{\mathrm{d} t}=- h(y)-0.2 y $$ $$\frac{\mathrm{dz} }{\mathrm{d} t}=x+h(y)-0.2 z $$ with equilibrium points given by: $$x^*=(0;0.3140;-0.3140); y^*=(0;80/49;-80/49); z^*=(0;1.5699;-1.5699);$$ so,in the end how would I choose the distribution of the y* coordinates to find the equilibrium points?(as it is clearly shown,all equilibrium points are functions of y*). The only thing I can say for certain(as you guys have pointed out) is that y* cannot equal to 1.",,"['ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems', 'mathematical-modeling']"
93,Uniqueness of weight function.,Uniqueness of weight function.,,"Let $L=p(x)\frac{d^2}{dx^2}+q(x)\frac{d}{dx}+r(x).$  Where L stands for differential operator.  Now inner product defined $(f,g)=\int_a^bf(x)g(x)w(x)dx$. Where $w(x)$ is a weight function.  Now $L$ is symmetric with respect to weight function if $(Lu,v)=(u,Lv)$. Need to prove if $L=p(x)\frac{d^2}{dx^2}+q(x)\frac{d}{dx}+r(x)$ then there is a weight function, and it is unique up to a constant, such that L is symmetric with respect to w.","Let $L=p(x)\frac{d^2}{dx^2}+q(x)\frac{d}{dx}+r(x).$  Where L stands for differential operator.  Now inner product defined $(f,g)=\int_a^bf(x)g(x)w(x)dx$. Where $w(x)$ is a weight function.  Now $L$ is symmetric with respect to weight function if $(Lu,v)=(u,Lv)$. Need to prove if $L=p(x)\frac{d^2}{dx^2}+q(x)\frac{d}{dx}+r(x)$ then there is a weight function, and it is unique up to a constant, such that L is symmetric with respect to w.",,"['ordinary-differential-equations', 'partial-differential-equations', 'special-functions']"
94,Water Systems — when can I use buckets of water to simulate an ODE?,Water Systems — when can I use buckets of water to simulate an ODE?,,"It is quite common to use physical systems to perform calculations (see here and here ). This is for a number of reasons: sometimes the physical system is efficient, sometimes it helps us understand the general principles of the physical system, and sometimes, because it can be a good way of demonstrating how a formal system works. In this case, I am interested in the latter case. I want to find a physical system that demonstrates how a system of ODEs works. Specifically, I'm interested in ODEs of a particular form: $$\frac{\partial x_i}{\partial t} = \sum_j A_{ij} x_j + B_i$$ I have a specific case in mind, but the general case is interesting. In particular I have been thinking about water models, which can model a subset of these equations, and about which I have some questions (see below). The following is an instantiation of the system: $$ \frac{\partial x}{\partial t} = A - Bx$$ $$ \frac{\partial y}{\partial t} = Bx - Cy$$ with appropriately chosen $A\propto a$, $B \propto b$ etc. Here is another system, choosing the appropriate $k \propto K$ (one can easily add a constant into this by changing the relative heights): $$\frac{\partial x}{\partial t} = k(y-x)$$ $$\frac{\partial y}{\partial t} = k(x-y)$$ My questions : Show whether the system $$ \dot{x} = a - bx + cy$$ $$\dot{y} = bx - dy$$ can be instantiated using pumps, taps, and holes (I'm fairly sure it cannot). More generally, using pumps, taps and holes, what are the constraints on $[A_{ij}]$ and $[B_i]$. Assuming that the equation in (1) cannot be instantiated, what physical modification could be used to make it possible. (there are quite a lot of possibilities, for example, this ) So far : For 2: The way I'm thinking of going about it is to define a ""water system"" inductively. Let $\mathcal{W}$ be the set of ""water systems"", made of a set of differential equations and a logical condition $\mathcal{C}$ under which they apply. This may or may not be correct... The pair containing $n$ diffential equations $\{\dot{x_i} = 0 \;|\; i=1\ldots n\}$ and $\mathcal{C}=T$ is a water system. Joining basins: If $\{\dot{x}_i = f_i(x_1 \ldots x_n)\}\in\mathcal{W}$, then systems transformed by $$f_i\rightarrow f_i + k(x_i - x_j + \Delta h)$$ $$f_j \rightarrow f_j + k(x_j - x_i - \Delta h)$$ $$C \rightarrow C \wedge (something??) $$ also belongs to $\mathcal{W}$. Leaks: If $\{\dot{x}_i = f_i(x_1 \ldots x_n)\}\in\mathcal{W}$, then systems transformed by $f_i\rightarrow f_i - k(x_i - h)$ is also in $\mathcal{W}$. Other stuff Anyway, not sure this is the right way of going about it.","It is quite common to use physical systems to perform calculations (see here and here ). This is for a number of reasons: sometimes the physical system is efficient, sometimes it helps us understand the general principles of the physical system, and sometimes, because it can be a good way of demonstrating how a formal system works. In this case, I am interested in the latter case. I want to find a physical system that demonstrates how a system of ODEs works. Specifically, I'm interested in ODEs of a particular form: $$\frac{\partial x_i}{\partial t} = \sum_j A_{ij} x_j + B_i$$ I have a specific case in mind, but the general case is interesting. In particular I have been thinking about water models, which can model a subset of these equations, and about which I have some questions (see below). The following is an instantiation of the system: $$ \frac{\partial x}{\partial t} = A - Bx$$ $$ \frac{\partial y}{\partial t} = Bx - Cy$$ with appropriately chosen $A\propto a$, $B \propto b$ etc. Here is another system, choosing the appropriate $k \propto K$ (one can easily add a constant into this by changing the relative heights): $$\frac{\partial x}{\partial t} = k(y-x)$$ $$\frac{\partial y}{\partial t} = k(x-y)$$ My questions : Show whether the system $$ \dot{x} = a - bx + cy$$ $$\dot{y} = bx - dy$$ can be instantiated using pumps, taps, and holes (I'm fairly sure it cannot). More generally, using pumps, taps and holes, what are the constraints on $[A_{ij}]$ and $[B_i]$. Assuming that the equation in (1) cannot be instantiated, what physical modification could be used to make it possible. (there are quite a lot of possibilities, for example, this ) So far : For 2: The way I'm thinking of going about it is to define a ""water system"" inductively. Let $\mathcal{W}$ be the set of ""water systems"", made of a set of differential equations and a logical condition $\mathcal{C}$ under which they apply. This may or may not be correct... The pair containing $n$ diffential equations $\{\dot{x_i} = 0 \;|\; i=1\ldots n\}$ and $\mathcal{C}=T$ is a water system. Joining basins: If $\{\dot{x}_i = f_i(x_1 \ldots x_n)\}\in\mathcal{W}$, then systems transformed by $$f_i\rightarrow f_i + k(x_i - x_j + \Delta h)$$ $$f_j \rightarrow f_j + k(x_j - x_i - \Delta h)$$ $$C \rightarrow C \wedge (something??) $$ also belongs to $\mathcal{W}$. Leaks: If $\{\dot{x}_i = f_i(x_1 \ldots x_n)\}\in\mathcal{W}$, then systems transformed by $f_i\rightarrow f_i - k(x_i - h)$ is also in $\mathcal{W}$. Other stuff Anyway, not sure this is the right way of going about it.",,"['ordinary-differential-equations', 'computer-science', 'mathematical-modeling']"
95,"Find the time span of snow plow operation, given that its speed is inversely proportional to the height of the snow","Find the time span of snow plow operation, given that its speed is inversely proportional to the height of the snow",,"One day snow began to fall before dawn and continued to fall at a constant rate. At midday a snowplot set out to clear a road. At 2pm it turned back, arriving to the starting point at 3pm.    If we suppose that the snowplow speed is inversely proportional to the height of the snow, at what time did it start snowing? At what time should the snowplot turn back in order to arrive to the starting point at 2pm? Let: $x(t)$ be the position of the snowplow at time $t$. $h(t)$ the height of the snow at time $t$ We denote $\Delta V $ the volume of snow removed by the snowplow in a time $\Delta t$ small enough to suppose that $h(t)$ constant in $\Delta x$. If L is the widht of the shovel of the snowplow we have: $\frac{\Delta V}{\Delta t} = h L \frac{\Delta x}{\Delta t}$ If we take  $\Delta t \rightarrow 0$: $\frac{dV}{dt} = h L \frac{dx}{dt}$ We know that $\frac{dV}{dt} = \alpha$ is constant, so we have $\frac{dx}{dt} = \frac{\alpha}{L h}$ As snow falls at a constant rate, we have $h(t) = c t$ then: $\frac{dx}{dt} = \frac{\alpha}{L c t}$ Calling $ A = \frac{\alpha}{L c}$ we have: $\frac{dx}{dt} = \frac{A}{t}$ and then: $x(t) = A Log(t) + C$ We have $x(T) = 0 \Longrightarrow x(t) = A Log(\frac{t}{T})$ This is clearly the position of the snowplow between 12pm and 2pm. I don't know how to continue from here because in the way back h(t) isn't as simple and depends on when the snowplow went over there the first time.","One day snow began to fall before dawn and continued to fall at a constant rate. At midday a snowplot set out to clear a road. At 2pm it turned back, arriving to the starting point at 3pm.    If we suppose that the snowplow speed is inversely proportional to the height of the snow, at what time did it start snowing? At what time should the snowplot turn back in order to arrive to the starting point at 2pm? Let: $x(t)$ be the position of the snowplow at time $t$. $h(t)$ the height of the snow at time $t$ We denote $\Delta V $ the volume of snow removed by the snowplow in a time $\Delta t$ small enough to suppose that $h(t)$ constant in $\Delta x$. If L is the widht of the shovel of the snowplow we have: $\frac{\Delta V}{\Delta t} = h L \frac{\Delta x}{\Delta t}$ If we take  $\Delta t \rightarrow 0$: $\frac{dV}{dt} = h L \frac{dx}{dt}$ We know that $\frac{dV}{dt} = \alpha$ is constant, so we have $\frac{dx}{dt} = \frac{\alpha}{L h}$ As snow falls at a constant rate, we have $h(t) = c t$ then: $\frac{dx}{dt} = \frac{\alpha}{L c t}$ Calling $ A = \frac{\alpha}{L c}$ we have: $\frac{dx}{dt} = \frac{A}{t}$ and then: $x(t) = A Log(t) + C$ We have $x(T) = 0 \Longrightarrow x(t) = A Log(\frac{t}{T})$ This is clearly the position of the snowplow between 12pm and 2pm. I don't know how to continue from here because in the way back h(t) isn't as simple and depends on when the snowplow went over there the first time.",,['ordinary-differential-equations']
96,solve the differential equation $y'=ye^{-y'}$,solve the differential equation,y'=ye^{-y'},how to solve differential equation like this $y'=ye^{-y'}$ I have no idea to solve problem like this is there any special function or special way to solve ?,how to solve differential equation like this $y'=ye^{-y'}$ I have no idea to solve problem like this is there any special function or special way to solve ?,,['calculus']
97,Using Fourier series techniques to solve $x'' + 3x = 7$ with $x'(0) = x'(5) = 0$,Using Fourier series techniques to solve  with,x'' + 3x = 7 x'(0) = x'(5) = 0,"$$x'' + 3x= 7$$ Given conditions $x'(0)=x'(5)=0$. I checked the list and I went through three books. I am doing intro to differential equations. I just don't know how to get the extensions... I was told if there is a derivative use $\cos$, but I can't hack it. If there is any simple explanation. Please feel free to expound. This is to simply find the formal solution of the said ODE using Fourier Series. What do I substitute at right hand side? Since I think left hand is simply summation $\cos n \pi/L$. ......I just know that $L=5$? And $7$ is $f(t)$ or $f(x)$? Maybe? Thanks.","$$x'' + 3x= 7$$ Given conditions $x'(0)=x'(5)=0$. I checked the list and I went through three books. I am doing intro to differential equations. I just don't know how to get the extensions... I was told if there is a derivative use $\cos$, but I can't hack it. If there is any simple explanation. Please feel free to expound. This is to simply find the formal solution of the said ODE using Fourier Series. What do I substitute at right hand side? Since I think left hand is simply summation $\cos n \pi/L$. ......I just know that $L=5$? And $7$ is $f(t)$ or $f(x)$? Maybe? Thanks.",,"['ordinary-differential-equations', 'fourier-series']"
98,"solve $y(x)=\cos \left(y'(x)\right) + y'(x)\sin (y'(x)), y(0)=1$",solve,"y(x)=\cos \left(y'(x)\right) + y'(x)\sin (y'(x)), y(0)=1","solve $$y(x)=\cos (y'(x)) + y'(x)\sin (y'(x)), y(0)=1$$ with wolfram alpha I got that a solution is $y(x)=x\arcsin x+\cos (\arcsin x)$ but I have no idea how to find it. I tried transforming into an exact equation by letting $u=y'$ then I get $y=\frac{u^2}{2}$ I just realised this is wrong (because $\frac{d}{dx}\frac{u^2}{2}=u'u\neq y'$) so there's no point posting the rest of my work. any hint?","solve $$y(x)=\cos (y'(x)) + y'(x)\sin (y'(x)), y(0)=1$$ with wolfram alpha I got that a solution is $y(x)=x\arcsin x+\cos (\arcsin x)$ but I have no idea how to find it. I tried transforming into an exact equation by letting $u=y'$ then I get $y=\frac{u^2}{2}$ I just realised this is wrong (because $\frac{d}{dx}\frac{u^2}{2}=u'u\neq y'$) so there's no point posting the rest of my work. any hint?",,['ordinary-differential-equations']
99,Can we use y=vx for non-homogeneous first order linear differential equation?,Can we use y=vx for non-homogeneous first order linear differential equation?,,"below is an example of a  non-homogeneous first order linear differential equation $yy'=x^3+y^2/x$ Yet, I found that I can solve it with y=vx, which as far as I know, is only for homogeneous cases. $$yy'=x^3+y^2/x\\ yy'=\frac{x^4+y^2}{x}\\ y'=\frac{x^4+y^2}{xy}\\ \frac{dy}{dx}=\frac{(αx)^4+(αy)^2}{(αx)(αy)}\\ \frac{dy}{dx}=\frac{(α^2)x^4+y^2}{xy}$$ There’s an extra $α^2$. Here comes the solution: Let $y=vx$ $$ \frac{dy}{dx}=v+x\frac{dv}{dx}\\ v+x\frac{dv}{dx}=\frac{x^4+v^2*x^2}{v*x^2}$$ Eliminate $x^2$, $$v+x\frac{dv}{dx}=\frac{x^2+v^2}{v}$$ Moving the v to the right, $$x\frac{dv}{dx}=\frac{x^2}{v}\\ v dv=x dx$$ Do the integration, $$v^2/2=(x^2/2)+ c$$,where c is a constant So, is there any exception for $y=vx$? Or am I wrong from the beginning where the differential equation is indeed homogeneous?","below is an example of a  non-homogeneous first order linear differential equation $yy'=x^3+y^2/x$ Yet, I found that I can solve it with y=vx, which as far as I know, is only for homogeneous cases. $$yy'=x^3+y^2/x\\ yy'=\frac{x^4+y^2}{x}\\ y'=\frac{x^4+y^2}{xy}\\ \frac{dy}{dx}=\frac{(αx)^4+(αy)^2}{(αx)(αy)}\\ \frac{dy}{dx}=\frac{(α^2)x^4+y^2}{xy}$$ There’s an extra $α^2$. Here comes the solution: Let $y=vx$ $$ \frac{dy}{dx}=v+x\frac{dv}{dx}\\ v+x\frac{dv}{dx}=\frac{x^4+v^2*x^2}{v*x^2}$$ Eliminate $x^2$, $$v+x\frac{dv}{dx}=\frac{x^2+v^2}{v}$$ Moving the v to the right, $$x\frac{dv}{dx}=\frac{x^2}{v}\\ v dv=x dx$$ Do the integration, $$v^2/2=(x^2/2)+ c$$,where c is a constant So, is there any exception for $y=vx$? Or am I wrong from the beginning where the differential equation is indeed homogeneous?",,['ordinary-differential-equations']
