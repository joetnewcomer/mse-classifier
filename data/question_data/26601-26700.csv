,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving that uncountable matrix multiplication is associative,Proving that uncountable matrix multiplication is associative,,"I have been looking for uncountable large structures that generalize the idea of matrix multiplication and noticed the following pattern Given functions $M_1(x,y) , M_2(x,y)$ defined on the unit square $[0,1] \times [0,1]$  one can define their ""matrix product"" as $$ M_3(x,y) = \int_{0}^{1} M_1 (s, y) M_2 (x, 1-s) ds  = ( M_1 M_2 )$$ If we try to approximate this integral with riemann sums, we find that the approximations are computationally equivalent to multiplying ever higher order matrices (to produce a third matrix), but it's not clear that associativity is preserved as we take the limit to forming an integral. My work: for 3 functions A,B,C I expressed  $A(BC)$ and $(AB)C$ in terms of nested integrals but now am not clear on how to show that the 2 expressions are equal. One idea was to perturb the definition above to: $ \int_{-\infty}^{\infty} M_1 (s, y) M_2 (x, -s) ds$ and then to show associativity for this simpler looking operation, [it then is easy  to show that maps sending the open unit square to the whole plane, composed with this operation, and then said map's inverse is equivalent to the earlier operation]","I have been looking for uncountable large structures that generalize the idea of matrix multiplication and noticed the following pattern Given functions $M_1(x,y) , M_2(x,y)$ defined on the unit square $[0,1] \times [0,1]$  one can define their ""matrix product"" as $$ M_3(x,y) = \int_{0}^{1} M_1 (s, y) M_2 (x, 1-s) ds  = ( M_1 M_2 )$$ If we try to approximate this integral with riemann sums, we find that the approximations are computationally equivalent to multiplying ever higher order matrices (to produce a third matrix), but it's not clear that associativity is preserved as we take the limit to forming an integral. My work: for 3 functions A,B,C I expressed  $A(BC)$ and $(AB)C$ in terms of nested integrals but now am not clear on how to show that the 2 expressions are equal. One idea was to perturb the definition above to: $ \int_{-\infty}^{\infty} M_1 (s, y) M_2 (x, -s) ds$ and then to show associativity for this simpler looking operation, [it then is easy  to show that maps sending the open unit square to the whole plane, composed with this operation, and then said map's inverse is equivalent to the earlier operation]",,"['linear-algebra', 'multivariable-calculus']"
1,Prove that the matrix $[\Gamma(\lambda_{i}+\mu_{j})]$ is nonsingular.,Prove that the matrix  is nonsingular.,[\Gamma(\lambda_{i}+\mu_{j})],Let $A$ be an $n\times n$ matrix whose entries are \begin{align*} a_{ij} = [\Gamma(\lambda_{i}+\mu_{j})] \end{align*} where $0 < \lambda_{1} < \ldots < \lambda_{n}$ and $0 < \mu_{1} < \ldots < \mu_{n}$ are real positive numbers and $\Gamma$ denotes the Gamma function given by $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt$ for $\operatorname{Re}(z)>0$. We need to show that matrix $A$ is non-singular. I have no idea how to start. Any hint or solution will be appreciated.,Let $A$ be an $n\times n$ matrix whose entries are \begin{align*} a_{ij} = [\Gamma(\lambda_{i}+\mu_{j})] \end{align*} where $0 < \lambda_{1} < \ldots < \lambda_{n}$ and $0 < \mu_{1} < \ldots < \mu_{n}$ are real positive numbers and $\Gamma$ denotes the Gamma function given by $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt$ for $\operatorname{Re}(z)>0$. We need to show that matrix $A$ is non-singular. I have no idea how to start. Any hint or solution will be appreciated.,,"['linear-algebra', 'matrices', 'determinant', 'gamma-function']"
2,$T^*$ is invertible whenever $T$ is invertible.,is invertible whenever  is invertible.,T^* T,"Is the following argument correct? In addition can someone please provide some insight concerning the second claim in the given proposition. Proposition . Let $V$ be finite-dimensional inner product space, and let $T$ be a linear operator on $V$. Prove that if $T$ is   invertible then $T^*$ is invertible and $(T^*)^{-1} = (T^{-1})^*$. Proof. Assume $T$ is invertible and $T^*w_0 = 0$ where $w_0\in V$ then by definition of adjoint $\langle Tv,w_0\rangle = 0,\forall v\in V$, but from hypothesis there exists a $v_0\in V$ such that $Tv_0 = w_0$ consequently $\langle Tv_0,w_0\rangle = \langle w_0,w_0\rangle = 0$ implying $w_0 = 0$.  Thus $\operatorname{null}T^* = \{0\}$ equivalently $T^*$ is injective and by theorem $\textbf{2.5}$ invertible. $\blacksquare$ Note: $\textbf{2.5}$ is the result that given an operator $T$ on a finite-dimensional vector space $V$ invertibility,injectivity and surjectivity are all equivalent.","Is the following argument correct? In addition can someone please provide some insight concerning the second claim in the given proposition. Proposition . Let $V$ be finite-dimensional inner product space, and let $T$ be a linear operator on $V$. Prove that if $T$ is   invertible then $T^*$ is invertible and $(T^*)^{-1} = (T^{-1})^*$. Proof. Assume $T$ is invertible and $T^*w_0 = 0$ where $w_0\in V$ then by definition of adjoint $\langle Tv,w_0\rangle = 0,\forall v\in V$, but from hypothesis there exists a $v_0\in V$ such that $Tv_0 = w_0$ consequently $\langle Tv_0,w_0\rangle = \langle w_0,w_0\rangle = 0$ implying $w_0 = 0$.  Thus $\operatorname{null}T^* = \{0\}$ equivalently $T^*$ is injective and by theorem $\textbf{2.5}$ invertible. $\blacksquare$ Note: $\textbf{2.5}$ is the result that given an operator $T$ on a finite-dimensional vector space $V$ invertibility,injectivity and surjectivity are all equivalent.",,"['linear-algebra', 'proof-verification', 'inner-products', 'adjoint-operators']"
3,trouble understanding lemma for normal basis theorem,trouble understanding lemma for normal basis theorem,,"I'm having trouble understanding the following proof: Lemma: Let $K \subseteq L$ be a Galois extension and the Galois group $G(L/K) = \{ \sigma_1, ..., \sigma _n \}$. Elements $x_1, ..., x_n \in L$ form a $K$-basis of $L$ if and only if $\det [\sigma_i(x_j)] \neq 0.$ Proof: The given elements are linearly dependent if and only if there are elements $a_1,...,a_n\in K$ not all equal to $0$ such that $a_1 x_1 + \dots + a_n x_n =0$. Letting all $\sigma _i$ act on this equality, we get \begin{align*} a_1 \sigma_1 (x_1) + \dots + a_n \sigma_1(x_n) &= 0 \\  a_1 \sigma_2 (x_1) + \dots + a_n \sigma_2(x_n) &= 0 \\ \dots \\ a_1 \sigma_n (x_1) + \dots + a_n \sigma_n(x_n) &= 0. \end{align*}   The above system of linear equations has a nonzero solution $a_1,...,a_n$ if and only if the determinant of the coefficient matrix (consisting of $\sigma_i(x_j) $) equals $0$. I understand that if there is a nonzero solution $a_1,...,a_n \in K$, then this determinant must be zero. Conversely, I do not understand why the determinant being zero implies there is a nonzero solution $a_1,...,a_n \in K$. Since the coefficients are in $L$, the only thing I should be able to deduce is that there is a nonzero solution $a_1,...,a_n \in L$, while in general $K \neq L$. What am I missing?","I'm having trouble understanding the following proof: Lemma: Let $K \subseteq L$ be a Galois extension and the Galois group $G(L/K) = \{ \sigma_1, ..., \sigma _n \}$. Elements $x_1, ..., x_n \in L$ form a $K$-basis of $L$ if and only if $\det [\sigma_i(x_j)] \neq 0.$ Proof: The given elements are linearly dependent if and only if there are elements $a_1,...,a_n\in K$ not all equal to $0$ such that $a_1 x_1 + \dots + a_n x_n =0$. Letting all $\sigma _i$ act on this equality, we get \begin{align*} a_1 \sigma_1 (x_1) + \dots + a_n \sigma_1(x_n) &= 0 \\  a_1 \sigma_2 (x_1) + \dots + a_n \sigma_2(x_n) &= 0 \\ \dots \\ a_1 \sigma_n (x_1) + \dots + a_n \sigma_n(x_n) &= 0. \end{align*}   The above system of linear equations has a nonzero solution $a_1,...,a_n$ if and only if the determinant of the coefficient matrix (consisting of $\sigma_i(x_j) $) equals $0$. I understand that if there is a nonzero solution $a_1,...,a_n \in K$, then this determinant must be zero. Conversely, I do not understand why the determinant being zero implies there is a nonzero solution $a_1,...,a_n \in K$. Since the coefficients are in $L$, the only thing I should be able to deduce is that there is a nonzero solution $a_1,...,a_n \in L$, while in general $K \neq L$. What am I missing?",,"['linear-algebra', 'galois-theory']"
4,Perturbation of injective map.,Perturbation of injective map.,,"I've got stuck on a result I am not able to prove. I will extrapolate it from its context: Let$\space$ $f$  be a linear injective map between $\mathbb{R}^n $ and $\mathbb{R}^m $ with $n \leq m$ and $g$ be a linear isomorphism in  $\mathbb{R}^n $. Let $ || \space ||$ be some consistent $m \times n$ matrix norm. Then exists $\epsilon >0$ such that for every $A$, $||A|| < \epsilon$ we have that: $f+Ag$ is still injective. Can someone help? Thanks :)","I've got stuck on a result I am not able to prove. I will extrapolate it from its context: Let$\space$ $f$  be a linear injective map between $\mathbb{R}^n $ and $\mathbb{R}^m $ with $n \leq m$ and $g$ be a linear isomorphism in  $\mathbb{R}^n $. Let $ || \space ||$ be some consistent $m \times n$ matrix norm. Then exists $\epsilon >0$ such that for every $A$, $||A|| < \epsilon$ we have that: $f+Ag$ is still injective. Can someone help? Thanks :)",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis', 'linear-transformations']"
5,"If $f$ is a non-unit element of an integral damain $A$, then $A[f^{-1}]$ is not finitely generated $A$-module.","If  is a non-unit element of an integral damain , then  is not finitely generated -module.",f A A[f^{-1}] A,"I would like to solve the following exercise: Let $A$ be an integral domain. Let $f$ be a non-unit element of $A$. Then show that $A[f^{-1}]$ is not finitely generated $A$-module. Here is my attempt: Let $K$ be the quotient field of $A$. Let $B$ be a subring of $K$ containing $A$ and $f^{-1}$. We have to show that $A[f^{-1}]$ is not finitely generated $A$-module. On contrary, suppose $A[f^{-1}]$ is finitely generated $A$-module. Then $f^{-1}$ is integral over $A$ by proposition 5.1 of the book Commutative Algebra by Atiyah. $i.e.,$ there are $a_{1}, a_{2},...,a_{n}\in A$ such that \begin{equation} f^{-n}+ a_{n}f^{-(n-1)}+...+ a_{1}=0 \end{equation} By multiplying with $f^{n}$ on both sides, we have  \begin{equation} 1+ a_{n}f+...+ a_{1}f^{n}=0 \end{equation} $i.e.,$ \begin{equation} -f(a_{n}+...+a_{1}f^{n-1})=1 \end{equation} Thus $f$ is a unit that yield a contradiction of the hypothesis. Is the above argument correct? Another solution would be highly appreciated.","I would like to solve the following exercise: Let $A$ be an integral domain. Let $f$ be a non-unit element of $A$. Then show that $A[f^{-1}]$ is not finitely generated $A$-module. Here is my attempt: Let $K$ be the quotient field of $A$. Let $B$ be a subring of $K$ containing $A$ and $f^{-1}$. We have to show that $A[f^{-1}]$ is not finitely generated $A$-module. On contrary, suppose $A[f^{-1}]$ is finitely generated $A$-module. Then $f^{-1}$ is integral over $A$ by proposition 5.1 of the book Commutative Algebra by Atiyah. $i.e.,$ there are $a_{1}, a_{2},...,a_{n}\in A$ such that \begin{equation} f^{-n}+ a_{n}f^{-(n-1)}+...+ a_{1}=0 \end{equation} By multiplying with $f^{n}$ on both sides, we have  \begin{equation} 1+ a_{n}f+...+ a_{1}f^{n}=0 \end{equation} $i.e.,$ \begin{equation} -f(a_{n}+...+a_{1}f^{n-1})=1 \end{equation} Thus $f$ is a unit that yield a contradiction of the hypothesis. Is the above argument correct? Another solution would be highly appreciated.",,"['linear-algebra', 'abstract-algebra', 'commutative-algebra']"
6,Describe the finite order integer matrices over complex field,Describe the finite order integer matrices over complex field,,"I stuck so much on this question! I need to describe finite order integer matrices without 1 eigenvalues over $\mathbb C$. I need description in terms of classes of equivalent matrices(such that exist complex matrix C such that $C^{-1}AC = B$) I came to this results. Let A be the integer matrix of order $m$. First of all A can be diagonalized so is $diag(\lambda_1, ..., \lambda_n)$ in some basis. If $\lambda$ is its eigenvalue, then $\lambda^{m} = 1$. The minimal polynomial $p$ for A divides $x^m - 1$ and belongs to $\mathbb Z[x]$ because $A \in Mat(\mathbb Z)$. Actually $p(\lambda_i) = 0$. So this should give some limitations on $(\lambda_1, ..., \lambda_n)$. I need to get all possible cases for $(\lambda_1, ..., \lambda_n)$. My hypothesis is ""$(\lambda_1, ..., \lambda_n)$ splits into groups where every group is all roots of $\frac{x^k - 1}{x-1}$"". Edit 1. I was really inaccurate with formulating my hypothesis. I really mean that $(\lambda_1, ..., \lambda_n)$ splits into groups where every group is all roots of some irreducible polynomial in decomposition of $\frac{x^k - 1}{x-1}$ Edit 2. I'm sorry if I messed you up, I'll try to give much more readable description of my question. I'm doing a research and found out that for some group all automorpmism are in 1-1 correspondence with the classes of simmilar integer matrices of finite order without 1-eigenvalues. So what I want is to somehow enumerate this classes. I divided the problem in two parts: 1) Prove that the set of classes(that I described above) is finite 2) Give a constructive way to enumerate this classes Hope now my question is more readable. Any help is appreciated!","I stuck so much on this question! I need to describe finite order integer matrices without 1 eigenvalues over $\mathbb C$. I need description in terms of classes of equivalent matrices(such that exist complex matrix C such that $C^{-1}AC = B$) I came to this results. Let A be the integer matrix of order $m$. First of all A can be diagonalized so is $diag(\lambda_1, ..., \lambda_n)$ in some basis. If $\lambda$ is its eigenvalue, then $\lambda^{m} = 1$. The minimal polynomial $p$ for A divides $x^m - 1$ and belongs to $\mathbb Z[x]$ because $A \in Mat(\mathbb Z)$. Actually $p(\lambda_i) = 0$. So this should give some limitations on $(\lambda_1, ..., \lambda_n)$. I need to get all possible cases for $(\lambda_1, ..., \lambda_n)$. My hypothesis is ""$(\lambda_1, ..., \lambda_n)$ splits into groups where every group is all roots of $\frac{x^k - 1}{x-1}$"". Edit 1. I was really inaccurate with formulating my hypothesis. I really mean that $(\lambda_1, ..., \lambda_n)$ splits into groups where every group is all roots of some irreducible polynomial in decomposition of $\frac{x^k - 1}{x-1}$ Edit 2. I'm sorry if I messed you up, I'll try to give much more readable description of my question. I'm doing a research and found out that for some group all automorpmism are in 1-1 correspondence with the classes of simmilar integer matrices of finite order without 1-eigenvalues. So what I want is to somehow enumerate this classes. I divided the problem in two parts: 1) Prove that the set of classes(that I described above) is finite 2) Give a constructive way to enumerate this classes Hope now my question is more readable. Any help is appreciated!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
7,Why is the gradient of a function in the dual space?,Why is the gradient of a function in the dual space?,,"Suppose $f(x)$ where $x \in D \subset R^n$ is a real-valued function. Why is the gradient of $f(x)$, i.e. $\nabla f(x)$, or subgradient of $f(x)$, i.e. $g \in \partial f(x)$, in the dual space?","Suppose $f(x)$ where $x \in D \subset R^n$ is a real-valued function. Why is the gradient of $f(x)$, i.e. $\nabla f(x)$, or subgradient of $f(x)$, i.e. $g \in \partial f(x)$, in the dual space?",,['linear-algebra']
8,Result needed: outside curve longer than convex inside curve,Result needed: outside curve longer than convex inside curve,,"In order to complete a proof, I need to show the following (explained using figure attached): I am given two points A and B in the two-dimensional plane connected by the direct black line. Points A and B are also connected by the green vectors. The corresponding green curve is convex. Finally, points A and B are also connected by the red vectors. The corresponding red curve is convex goes from A to B ""on the same side"" as the green curve (with respect to the black line) goes ""outside"" the green curve (with respect to the black line) I want to prove that the length of the red line is greater or equal to the green line. By looking at the picture this is obvious. In order to formally prove this, I could use a lengthy algebraic calculation. However I feel that what I want to prove should follow immediately from some result from the field of analysis for example (of which I am not an expert). Can someone think about a theorem that immediately implies that the red curve is at least as long as the green curve? Many thanks!","In order to complete a proof, I need to show the following (explained using figure attached): I am given two points A and B in the two-dimensional plane connected by the direct black line. Points A and B are also connected by the green vectors. The corresponding green curve is convex. Finally, points A and B are also connected by the red vectors. The corresponding red curve is convex goes from A to B ""on the same side"" as the green curve (with respect to the black line) goes ""outside"" the green curve (with respect to the black line) I want to prove that the length of the red line is greater or equal to the green line. By looking at the picture this is obvious. In order to formally prove this, I could use a lengthy algebraic calculation. However I feel that what I want to prove should follow immediately from some result from the field of analysis for example (of which I am not an expert). Can someone think about a theorem that immediately implies that the red curve is at least as long as the green curve? Many thanks!",,"['real-analysis', 'linear-algebra', 'curves']"
9,Limit of eigenvalues of a matrix sequence.,Limit of eigenvalues of a matrix sequence.,,"Suppose $H$ is an $n\times n$ symmetric positive definite matrix, $M_k$ is a sequence of $n \times n$ matrix ( not necessarily symmetric ) such that $M_k \to O$ where $O$ is the zero matrix. Let $\lambda_i(H),i=1,...,n$ denote the $i$th largest eigenvalue of $H$. My question is, is it true that $\mathop {\lim }\limits_{k \to \infty } \lambda_i (H + {M_k}) = \lambda_i (H),i=1,...,n$? If this is not true for every $i$, is it true for $i=1$ (largest eigenvalue) and $i=n$ (smallest eigenvalue)? ( My application only needs this one to hold ) Any explanation, counterexample or reference is helpful. Thanks!","Suppose $H$ is an $n\times n$ symmetric positive definite matrix, $M_k$ is a sequence of $n \times n$ matrix ( not necessarily symmetric ) such that $M_k \to O$ where $O$ is the zero matrix. Let $\lambda_i(H),i=1,...,n$ denote the $i$th largest eigenvalue of $H$. My question is, is it true that $\mathop {\lim }\limits_{k \to \infty } \lambda_i (H + {M_k}) = \lambda_i (H),i=1,...,n$? If this is not true for every $i$, is it true for $i=1$ (largest eigenvalue) and $i=n$ (smallest eigenvalue)? ( My application only needs this one to hold ) Any explanation, counterexample or reference is helpful. Thanks!",,"['linear-algebra', 'matrices', 'functional-analysis']"
10,Infinity norm quotient,Infinity norm quotient,,"Let $V = \mathbb{R}^n$ the canonical $n$ dimensional real vector space. We endow $V$ with the infinity norm defined as $\|x\|_\infty = \max_i |x_i|$ for any vector $x = (x_1, \dots, x_n) \in V$. Let then $V'$ be a strict subspace of $V$. One can consider the quotient  $$V/V'$$ which is naturally normed by the quotient norm: $$ \| x + V' \|_\infty = \inf_{v\in V'} \|x-v\|_\infty.$$ Is there a convenient way to compute generically this norm or at least get (non-trivial) bounds on it ? In the $\ell_2$ norm case, this reduces easily to the computation of the orthogonal projection of $x$ on the space $V'$, which can be performed by Gram-Schmidt orthogonalisation.","Let $V = \mathbb{R}^n$ the canonical $n$ dimensional real vector space. We endow $V$ with the infinity norm defined as $\|x\|_\infty = \max_i |x_i|$ for any vector $x = (x_1, \dots, x_n) \in V$. Let then $V'$ be a strict subspace of $V$. One can consider the quotient  $$V/V'$$ which is naturally normed by the quotient norm: $$ \| x + V' \|_\infty = \inf_{v\in V'} \|x-v\|_\infty.$$ Is there a convenient way to compute generically this norm or at least get (non-trivial) bounds on it ? In the $\ell_2$ norm case, this reduces easily to the computation of the orthogonal projection of $x$ on the space $V'$, which can be performed by Gram-Schmidt orthogonalisation.",,"['linear-algebra', 'banach-spaces', 'normed-spaces', 'quotient-spaces']"
11,Continuity of eigenvectors of self-adjoint matrices,Continuity of eigenvectors of self-adjoint matrices,,"What I have here is a matrix function $A(x,y)$, defined on $[0,1]\times [0,1]$, such that each $A(x,y)$ is a self-adjoint positive semi-definite $n\times n$-matrix (with possibly complex entries). Moreover, $(x,y)\mapsto A(x,y)$ is supposed to be continuous on $\mathbb R^2$ and it has constant rank, i.e., the multiplicity of the zero eigenvalue is constant and all the other eigenvalues remain in an interval $[a,b]$, where $a > 0$. Can I then find continuous matrix functions $U$ and $B$ such that $$ A(x,y) = U(x,y)B(x,y)U(x,y)^*, $$ $U(x,y)$ is unitary, and $B(x,y)$ is block diagonal of the form $$ B = \begin{pmatrix}0&0\\0&B_2\end{pmatrix}, $$ where $B_2(x,y)$ is a square matrix function of the size equal to the rank of $A(x,y)$?","What I have here is a matrix function $A(x,y)$, defined on $[0,1]\times [0,1]$, such that each $A(x,y)$ is a self-adjoint positive semi-definite $n\times n$-matrix (with possibly complex entries). Moreover, $(x,y)\mapsto A(x,y)$ is supposed to be continuous on $\mathbb R^2$ and it has constant rank, i.e., the multiplicity of the zero eigenvalue is constant and all the other eigenvalues remain in an interval $[a,b]$, where $a > 0$. Can I then find continuous matrix functions $U$ and $B$ such that $$ A(x,y) = U(x,y)B(x,y)U(x,y)^*, $$ $U(x,y)$ is unitary, and $B(x,y)$ is block diagonal of the form $$ B = \begin{pmatrix}0&0\\0&B_2\end{pmatrix}, $$ where $B_2(x,y)$ is a square matrix function of the size equal to the rank of $A(x,y)$?",,"['linear-algebra', 'matrices', 'continuity', 'eigenvalues-eigenvectors']"
12,"$\det (A+B)=\det (A-B)$, prove that $B^{-1}$ exists iff $b_{11}\neq b_{21}$",", prove that  exists iff",\det (A+B)=\det (A-B) B^{-1} b_{11}\neq b_{21},"Let $A=\begin{pmatrix} 0 & 1 & 2 \\ 0 & 1 & 2 \\ 0 & 2 & 3  \end{pmatrix}\in \mathbb{R}^{3\times 3}$ and let $B=\left [b_{ij}\right ]\in \mathbb{R}^{3\times 3}$ such that $\det \left (A+B\right )=\det \left (A-B\right )$. Prove that $B$ is invertible if and only if $b_{11}\neq b_{21}$. After simplifying by using properties of the determinant, the condition $\det \left (A+B\right )=\det \left (A-B\right )$ becomes $\det \begin{pmatrix} b_{11} & b_{12} & 2 \\ b_{21} & b_{22} & 2 \\ b_{31} & b_{23} & 3  \end{pmatrix}=-\det \begin{pmatrix} b_{11} & 1 & b_{13} \\ b_{21} & 1 & b_{23} \\ b_{31} & 2 & b_{33}  \end{pmatrix}$, providing that I did not commit any mistake. This does not seem concluding and I don't think it's the right approach to the solution. Any help?","Let $A=\begin{pmatrix} 0 & 1 & 2 \\ 0 & 1 & 2 \\ 0 & 2 & 3  \end{pmatrix}\in \mathbb{R}^{3\times 3}$ and let $B=\left [b_{ij}\right ]\in \mathbb{R}^{3\times 3}$ such that $\det \left (A+B\right )=\det \left (A-B\right )$. Prove that $B$ is invertible if and only if $b_{11}\neq b_{21}$. After simplifying by using properties of the determinant, the condition $\det \left (A+B\right )=\det \left (A-B\right )$ becomes $\det \begin{pmatrix} b_{11} & b_{12} & 2 \\ b_{21} & b_{22} & 2 \\ b_{31} & b_{23} & 3  \end{pmatrix}=-\det \begin{pmatrix} b_{11} & 1 & b_{13} \\ b_{21} & 1 & b_{23} \\ b_{31} & 2 & b_{33}  \end{pmatrix}$, providing that I did not commit any mistake. This does not seem concluding and I don't think it's the right approach to the solution. Any help?",,"['linear-algebra', 'matrices', 'determinant']"
13,Provig that the number $z_n$ of zero element of $A^{-1} $ satisfies $z_n\le n^2-2n$,Provig that the number  of zero element of  satisfies,z_n A^{-1}  z_n\le n^2-2n,"Let A be a $n\times n$ matrix $n\ge 2$ symmetric invertible with real positive entries. Then show that the number $z_n$ of zero element of $A^{-1} $ satisfies $$z_n\le n^2-2n$$   How many zero elements are in the inverse of the $n\times n$ matrix A given by    $$A=\begin{pmatrix} 1&1&1&1&\cdots&1 \\1&2&2&2&\cdots&2 \\1&2&1&1&\cdots&1\\ \cdots &\cdots&\cdots&\cdots&\cdots&\cdots \\1&2&1&2&\cdots&\cdots\end{pmatrix}$$ The try to use the multiplication formula for matrices which given by as follow $$(AB)_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj}$$ Then taking $B= A^{-1} = (b)_{i,j}$ we have the system, $$ (I_n)_{ij}=(AB)_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj} =\delta_{ij}$$ where $\delta_{ij}$ is the Kronecker symbol. From here I don't how to get further does any one ha an idea? for the second question the number of zeros elements seems to be $n^2-2n$ which prove that the inequality $z_n\le n^2-2n$ is sharp. see here","Let A be a $n\times n$ matrix $n\ge 2$ symmetric invertible with real positive entries. Then show that the number $z_n$ of zero element of $A^{-1} $ satisfies $$z_n\le n^2-2n$$   How many zero elements are in the inverse of the $n\times n$ matrix A given by    $$A=\begin{pmatrix} 1&1&1&1&\cdots&1 \\1&2&2&2&\cdots&2 \\1&2&1&1&\cdots&1\\ \cdots &\cdots&\cdots&\cdots&\cdots&\cdots \\1&2&1&2&\cdots&\cdots\end{pmatrix}$$ The try to use the multiplication formula for matrices which given by as follow $$(AB)_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj}$$ Then taking $B= A^{-1} = (b)_{i,j}$ we have the system, $$ (I_n)_{ij}=(AB)_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj} =\delta_{ij}$$ where $\delta_{ij}$ is the Kronecker symbol. From here I don't how to get further does any one ha an idea? for the second question the number of zeros elements seems to be $n^2-2n$ which prove that the inequality $z_n\le n^2-2n$ is sharp. see here",,"['linear-algebra', 'matrices', 'algebra-precalculus', 'contest-math']"
14,Sum of subspaces by setting determinant = 0,Sum of subspaces by setting determinant = 0,,"Given two subspaces of $R^4$ $W = \operatorname{Span}( (1,1,0,-1), (1,2,3,0) )$ $U = \operatorname{Span}( (1, 2, 2, -2), (2, 3, 2, -3) )$ Find the cartesian equations of the intersection. I would solve this by putting all the vectors in a matrix as columns and the coordinates x,y,z,t on the side and doing gaussian elimination (this follows straight from the definition). Instead I read a very strange (to me) solution to this problem. Given that $$ \det \left(\begin{bmatrix}1&1&1\\1&2&2\\0&3&2\end{bmatrix}\right) = -1 \neq 0$$ and that the last vector is the sum of the first and the third, a base of the sum is given by the first 3 vectors, so we can find cartesian equations for the sum by setting: $$ \det \left(\begin{bmatrix}x&y&z&t\\1&1&1&-1\\1&2&2&0\\0&3&2&-2\end{bmatrix}\right) = 0.$$ No more explanation is given could you please explain me how this procedure works?","Given two subspaces of $R^4$ $W = \operatorname{Span}( (1,1,0,-1), (1,2,3,0) )$ $U = \operatorname{Span}( (1, 2, 2, -2), (2, 3, 2, -3) )$ Find the cartesian equations of the intersection. I would solve this by putting all the vectors in a matrix as columns and the coordinates x,y,z,t on the side and doing gaussian elimination (this follows straight from the definition). Instead I read a very strange (to me) solution to this problem. Given that $$ \det \left(\begin{bmatrix}1&1&1\\1&2&2\\0&3&2\end{bmatrix}\right) = -1 \neq 0$$ and that the last vector is the sum of the first and the third, a base of the sum is given by the first 3 vectors, so we can find cartesian equations for the sum by setting: $$ \det \left(\begin{bmatrix}x&y&z&t\\1&1&1&-1\\1&2&2&0\\0&3&2&-2\end{bmatrix}\right) = 0.$$ No more explanation is given could you please explain me how this procedure works?",,"['linear-algebra', 'matrices', 'determinant']"
15,Eigenvalues of certain Hankel matrix,Eigenvalues of certain Hankel matrix,,"For each $n\in\mathbb N$, let $A_n$ be the $n\times n$ matrix with entries $$ A_n(k,j)=\begin{cases} 1,&\ k+j=n+1\\  1,&\ k+j=n+2,\\ 0,&\ \text{ otherwise} \end{cases} $$ So, for instance,  $$ A_4=\begin{bmatrix} 0&0&0&1\\ 0&0&1&1\\ 0&1&1&0\\ 1&1&0&0\end{bmatrix}. $$ I'm looking for a proof of the following: If $n=2m$, then $A_n$ has $m$ positive and $m$ negative eigenvalues. if $n=2m+1$, then $A_n$ has $m+1$ positive and $m$ negative eigenvalues. Finding the eigenvalues explicitly is definitely not an option, judging by the formulas already in the case $n=3$. If $p_n$ denotes the characteristic polynomial of $A_n$, then we have the recursion $$ p_{n+1}(t)=-t\,p_n(t)-p_{n-1}(t), $$ but I don't know if one can obtain meaningful information from this.","For each $n\in\mathbb N$, let $A_n$ be the $n\times n$ matrix with entries $$ A_n(k,j)=\begin{cases} 1,&\ k+j=n+1\\  1,&\ k+j=n+2,\\ 0,&\ \text{ otherwise} \end{cases} $$ So, for instance,  $$ A_4=\begin{bmatrix} 0&0&0&1\\ 0&0&1&1\\ 0&1&1&0\\ 1&1&0&0\end{bmatrix}. $$ I'm looking for a proof of the following: If $n=2m$, then $A_n$ has $m$ positive and $m$ negative eigenvalues. if $n=2m+1$, then $A_n$ has $m+1$ positive and $m$ negative eigenvalues. Finding the eigenvalues explicitly is definitely not an option, judging by the formulas already in the case $n=3$. If $p_n$ denotes the characteristic polynomial of $A_n$, then we have the recursion $$ p_{n+1}(t)=-t\,p_n(t)-p_{n-1}(t), $$ but I don't know if one can obtain meaningful information from this.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'operator-theory']"
16,Determinant of Block Tridiagonal Matrix,Determinant of Block Tridiagonal Matrix,,"I found in the following paper: Comments on ‘‘A note on a three-term recurrence for a tridiagonal matrix’’ that we can compute the determinant of a block tridiagonal matrix A via a recursion. In my particular case A is $4n\times4n$, $$\textbf{A}=\begin{pmatrix} \textbf{B}_L-h\textbf{R} & J\space\textbf{R} & \textbf{0} & \cdots & \textbf{0} \\ J\space \textbf{R} & -h\textbf{R} & J\space\textbf{R} &  & \textbf{0} \\ \textbf{0} & J\space \textbf{R} & -h\textbf{R} &\ddots &\vdots \\ \vdots &  &\ddots &\ddots & J\space\textbf{R}\\ \textbf{0} & \textbf{0} & \cdots & J \space\textbf{R} & \textbf{B}_R-h\textbf{R} \end{pmatrix}$$ where  $$\textbf{R}=\begin{pmatrix} 0&0&1&0\\ 0&0&0&1\\ -1&0&0&0\\ 0&-1&0&0\\  \end{pmatrix},\space \textbf{B}_{L,R}=\begin{pmatrix} 0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&\frac{1}{2}\Gamma_{-}^{\text{L,R}}\\ -\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0&\frac{1}{2}\Gamma_{-}^{\text{L,R}}&\frac{i}{2}\Gamma_{-}^{\text{L,R}}\\ \frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{1}{2}\Gamma_{-}^{\text{L,R}}&0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}\\ -\frac{1}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0\\ \end{pmatrix} $$ and $$J,h,\Gamma_{+}^{\text{L,R}},\Gamma_{-}^{\text{L,R}} \in \mathbb{R}$$ Now let me state the recursion mentioned in the above paper, $$\text{det}(\textbf{A})=\prod_{k=1}^{n}\text{det}(\Lambda_{k})\space\space\space\space(1)$$ where (in my case), $$ \Lambda_{1} = \textbf{B}_L-h\textbf{R}\\ \Lambda_{k} = -h\textbf{R}-J^{2}\textbf{R}\Lambda_{k-1}^{-1}\textbf{R}\\ \Lambda_{n}=\textbf{B}_R-h\textbf{R}-J^{2}\textbf{R}\Lambda_{n-1}^{-1}\textbf{R} $$  Now according to (1), the set of eigenvalues of $\textbf{A}$ should contain the eigenvalues of $\Lambda_{1} = \textbf{B}_L-h\textbf{R}$ since applying (1) to $\textbf{A}-\lambda I_{4n}$ gives $\Lambda_{1}^{'} = \textbf{B}_L-h\textbf{R}-\lambda I_{4}$. Now here comes my issue . I computed the spectrum of A in Mathematica for $n=50$ for the values $h=1,J=1.5,\Gamma_{+}^{\text{L}}=1.6,\Gamma_{+}^{\text{R}}=1.3,\Gamma_{-}^{\text{L}}=-0.4,\Gamma_{-}^{\text{R}}=-0.7$ I found that the spectrum did not contain the eigenvalues of $\textbf{B}_{L}-h\textbf{R}$. My question is: Is this recursion not applicable to my case, or am I incorrect in stating that the set of eigenvalues of A should contain the eigenvalues of $\Lambda_{1}$?","I found in the following paper: Comments on ‘‘A note on a three-term recurrence for a tridiagonal matrix’’ that we can compute the determinant of a block tridiagonal matrix A via a recursion. In my particular case A is $4n\times4n$, $$\textbf{A}=\begin{pmatrix} \textbf{B}_L-h\textbf{R} & J\space\textbf{R} & \textbf{0} & \cdots & \textbf{0} \\ J\space \textbf{R} & -h\textbf{R} & J\space\textbf{R} &  & \textbf{0} \\ \textbf{0} & J\space \textbf{R} & -h\textbf{R} &\ddots &\vdots \\ \vdots &  &\ddots &\ddots & J\space\textbf{R}\\ \textbf{0} & \textbf{0} & \cdots & J \space\textbf{R} & \textbf{B}_R-h\textbf{R} \end{pmatrix}$$ where  $$\textbf{R}=\begin{pmatrix} 0&0&1&0\\ 0&0&0&1\\ -1&0&0&0\\ 0&-1&0&0\\  \end{pmatrix},\space \textbf{B}_{L,R}=\begin{pmatrix} 0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&\frac{1}{2}\Gamma_{-}^{\text{L,R}}\\ -\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0&\frac{1}{2}\Gamma_{-}^{\text{L,R}}&\frac{i}{2}\Gamma_{-}^{\text{L,R}}\\ \frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{1}{2}\Gamma_{-}^{\text{L,R}}&0&\frac{i}{2}\Gamma_{+}^{\text{L,R}}\\ -\frac{1}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{-}^{\text{L,R}}&-\frac{i}{2}\Gamma_{+}^{\text{L,R}}&0\\ \end{pmatrix} $$ and $$J,h,\Gamma_{+}^{\text{L,R}},\Gamma_{-}^{\text{L,R}} \in \mathbb{R}$$ Now let me state the recursion mentioned in the above paper, $$\text{det}(\textbf{A})=\prod_{k=1}^{n}\text{det}(\Lambda_{k})\space\space\space\space(1)$$ where (in my case), $$ \Lambda_{1} = \textbf{B}_L-h\textbf{R}\\ \Lambda_{k} = -h\textbf{R}-J^{2}\textbf{R}\Lambda_{k-1}^{-1}\textbf{R}\\ \Lambda_{n}=\textbf{B}_R-h\textbf{R}-J^{2}\textbf{R}\Lambda_{n-1}^{-1}\textbf{R} $$  Now according to (1), the set of eigenvalues of $\textbf{A}$ should contain the eigenvalues of $\Lambda_{1} = \textbf{B}_L-h\textbf{R}$ since applying (1) to $\textbf{A}-\lambda I_{4n}$ gives $\Lambda_{1}^{'} = \textbf{B}_L-h\textbf{R}-\lambda I_{4}$. Now here comes my issue . I computed the spectrum of A in Mathematica for $n=50$ for the values $h=1,J=1.5,\Gamma_{+}^{\text{L}}=1.6,\Gamma_{+}^{\text{R}}=1.3,\Gamma_{-}^{\text{L}}=-0.4,\Gamma_{-}^{\text{R}}=-0.7$ I found that the spectrum did not contain the eigenvalues of $\textbf{B}_{L}-h\textbf{R}$. My question is: Is this recursion not applicable to my case, or am I incorrect in stating that the set of eigenvalues of A should contain the eigenvalues of $\Lambda_{1}$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'recursion', 'block-matrices']"
17,Simple formulae for matrices,Simple formulae for matrices,,"Can someone please explain me how to prove the following formula? $det(I +M) = \exp\;tr\;\ln(I+M)\;\,.$ Here $I$ and $M$ are a $2 \times 2$ identity matrix and an arbitrary $2 \times 2$ matrix, correspondingly. Also, how to derive from the above formula the following one? $− 2\,det\,M = tr(M^2) − (tr M)^2$ Do these formulae have counterparts of dimensions larger than $2 \times 2\,$? Somewhere in the literature I saw the relation $\ln\;det[M+Q] = \ln\;detM + Tr[M^{−1}\,Q] + O(Q^2)$ How to prove it? Many thanks!","Can someone please explain me how to prove the following formula? $det(I +M) = \exp\;tr\;\ln(I+M)\;\,.$ Here $I$ and $M$ are a $2 \times 2$ identity matrix and an arbitrary $2 \times 2$ matrix, correspondingly. Also, how to derive from the above formula the following one? $− 2\,det\,M = tr(M^2) − (tr M)^2$ Do these formulae have counterparts of dimensions larger than $2 \times 2\,$? Somewhere in the literature I saw the relation $\ln\;det[M+Q] = \ln\;detM + Tr[M^{−1}\,Q] + O(Q^2)$ How to prove it? Many thanks!",,['linear-algebra']
18,Inequality about Eigenvalues of Symmetric Block Matrix,Inequality about Eigenvalues of Symmetric Block Matrix,,"Let $ M = \left( \begin{array}{cc} A & B \\ B^T & C \end{array} \right)$ be a real symmetric matrix ($A, C$ are symmetric matrices). Let $\lambda_m$ and $\lambda_M$ denote the minimum and maximum eigenvalues of $M$, and $\mu_A$ and $\mu_C$ denote, respectively, the maximum eigenvalues of $A$ and $C$. Show that $\lambda_m+\lambda_M \leq \mu_A + \mu_C$. I've just got a hint from others: consider the positive semidefinite matrix $ M - \lambda_m I$. Maybe there are other methods.","Let $ M = \left( \begin{array}{cc} A & B \\ B^T & C \end{array} \right)$ be a real symmetric matrix ($A, C$ are symmetric matrices). Let $\lambda_m$ and $\lambda_M$ denote the minimum and maximum eigenvalues of $M$, and $\mu_A$ and $\mu_C$ denote, respectively, the maximum eigenvalues of $A$ and $C$. Show that $\lambda_m+\lambda_M \leq \mu_A + \mu_C$. I've just got a hint from others: consider the positive semidefinite matrix $ M - \lambda_m I$. Maybe there are other methods.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
19,About some differential identities of Vandermonde Determinant,About some differential identities of Vandermonde Determinant,,"I have three identities to prove: I proved (1) and (2), except (3) I know that f(x) can be factored out (xi-xj) terms(exactly n(n-1)/2 factors) , but this is all I know. Any help would be appreciated.","I have three identities to prove: I proved (1) and (2), except (3) I know that f(x) can be factored out (xi-xj) terms(exactly n(n-1)/2 factors) , but this is all I know. Any help would be appreciated.",,"['real-analysis', 'linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
20,How close are solutions of systems of homogeneous linear inequalities with close coefficients?,How close are solutions of systems of homogeneous linear inequalities with close coefficients?,,"Suppose I have two systems of $n$ homogeneous inequalities of $k$ variables:  $$Ax \geq 0$$ and  $$Bz \geq 0,$$ where both $A$ and $B$ are $n \times k$ matrices such that for any $i=1, \ldots, n$, and $j=1, \ldots, k$, we have $|a_{ij}-b_{ij}|\leq \epsilon$, where $\epsilon>0$ is very small. Can I conclude from here that there is some distance $\phi(\epsilon)$ (such that $\phi(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow 0$) with the following property: for any solution $x_0$ to the first system with $\|x_0\|=1$ there is a solution $z_0$ to the second system such that  $$\|x_0-z_0\|\leq \phi(\epsilon)$$  for some norm $\|\cdot\|$ in $\mathbb{R}^k$? I am interested in the form (or approximation) of such $\phi(\epsilon)$. EDIT. I am interested in situations when both systems have non-trivial (non-zero) solutions. I would be even willing to assume that both solution sets have non-empty interiors if this would help.","Suppose I have two systems of $n$ homogeneous inequalities of $k$ variables:  $$Ax \geq 0$$ and  $$Bz \geq 0,$$ where both $A$ and $B$ are $n \times k$ matrices such that for any $i=1, \ldots, n$, and $j=1, \ldots, k$, we have $|a_{ij}-b_{ij}|\leq \epsilon$, where $\epsilon>0$ is very small. Can I conclude from here that there is some distance $\phi(\epsilon)$ (such that $\phi(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow 0$) with the following property: for any solution $x_0$ to the first system with $\|x_0\|=1$ there is a solution $z_0$ to the second system such that  $$\|x_0-z_0\|\leq \phi(\epsilon)$$  for some norm $\|\cdot\|$ in $\mathbb{R}^k$? I am interested in the form (or approximation) of such $\phi(\epsilon)$. EDIT. I am interested in situations when both systems have non-trivial (non-zero) solutions. I would be even willing to assume that both solution sets have non-empty interiors if this would help.",,"['linear-algebra', 'inequality', 'linear-programming']"
21,Distance between line and point in homogeneous coordinates?,Distance between line and point in homogeneous coordinates?,,"I can't seem to find any definitive answer to this question. Assuming I have a 2D line in homogeneous coordinates defined by $$l = (a, b, c)^T$$ and a point in 2D space, $$x = (x, y)^T,$$ how do I find the perpendicular distance between the two. I know it has something to do with the dot product, but I can't remember exactly how it works. It would be nice if you could walk me through why your answer is correct.","I can't seem to find any definitive answer to this question. Assuming I have a 2D line in homogeneous coordinates defined by $$l = (a, b, c)^T$$ and a point in 2D space, $$x = (x, y)^T,$$ how do I find the perpendicular distance between the two. I know it has something to do with the dot product, but I can't remember exactly how it works. It would be nice if you could walk me through why your answer is correct.",,"['linear-algebra', 'geometry', 'projective-space', 'homogeneous-spaces']"
22,$A$ is consistent iff the augmented matrix has no pivot in last column.,is consistent iff the augmented matrix has no pivot in last column.,A,"A linear system of equations is inconsistent (does not have a solution) if and only   if there is a pivot in the last column of an echelon form of the   augmented matrix. I can understand the if part, that is because $0=1$ is impossible. But how do you prove the only if part? All of the posts I found on this site prove the if part only. My book says for only if part, ""If we don’t have such a row, we just make the reduced echelon form and then read the solution off it."" I don't think this is a proof to the only if part.","A linear system of equations is inconsistent (does not have a solution) if and only   if there is a pivot in the last column of an echelon form of the   augmented matrix. I can understand the if part, that is because is impossible. But how do you prove the only if part? All of the posts I found on this site prove the if part only. My book says for only if part, ""If we don’t have such a row, we just make the reduced echelon form and then read the solution off it."" I don't think this is a proof to the only if part.",0=1,['linear-algebra']
23,Moving from one Coupling to another,Moving from one Coupling to another,,"Let $X,Y$ be two discrete random variables. Two joint mass distributions (couplings) with marginals $X$ and $Y$ and with entries $p_{i,j}=\mathbb{P}_1(X=i,Y=j)$ and $p_{i,j}'=\mathbb{P}_2({X=i,Y=j})$  correspond to two matrices $(p_{i,j}), (p_{i,j}').$ Is there a linear transformation that maps $(p_{i,j})\mapsto(p_{i,j}')$? If not, is there a way to move from a given coupling of $X,Y$ to any other coupling of $X,Y$? The reason I ask is because I have a coupling of $X,Y$ and I wonder if a specific coupling exists. If there are any results that allow us to go from a   given coupling to any other coupling, perhaps this can be used to provide the desired coupling or show that it doesn't exist.","Let $X,Y$ be two discrete random variables. Two joint mass distributions (couplings) with marginals $X$ and $Y$ and with entries $p_{i,j}=\mathbb{P}_1(X=i,Y=j)$ and $p_{i,j}'=\mathbb{P}_2({X=i,Y=j})$  correspond to two matrices $(p_{i,j}), (p_{i,j}').$ Is there a linear transformation that maps $(p_{i,j})\mapsto(p_{i,j}')$? If not, is there a way to move from a given coupling of $X,Y$ to any other coupling of $X,Y$? The reason I ask is because I have a coupling of $X,Y$ and I wonder if a specific coupling exists. If there are any results that allow us to go from a   given coupling to any other coupling, perhaps this can be used to provide the desired coupling or show that it doesn't exist.",,"['linear-algebra', 'probability-theory', 'probability-distributions', 'linear-transformations', 'coupling']"
24,Formulate a Linear Program of minimizing maximum distance from a set of points to a line and formulate its dual,Formulate a Linear Program of minimizing maximum distance from a set of points to a line and formulate its dual,,"Say we define the distance from a point to a line in the plane as the length of the vertical distance ""you would have to walk"" from the point till you hit the line. Then find a line $L:y=ax+b$ , where $a,b\in \mathbb{R}$ in the plane that minimize the maximum distance from the points $(1,1),(2,2),(2,4),(3,2)$ to $L$. How would you formulate this problem as a linear problem and write down its dual LP?","Say we define the distance from a point to a line in the plane as the length of the vertical distance ""you would have to walk"" from the point till you hit the line. Then find a line $L:y=ax+b$ , where $a,b\in \mathbb{R}$ in the plane that minimize the maximum distance from the points $(1,1),(2,2),(2,4),(3,2)$ to $L$. How would you formulate this problem as a linear problem and write down its dual LP?",,"['linear-algebra', 'linear-programming', 'duality-theorems']"
25,Proof verification: $T\in\mathcal{L}(\mathcal{P}(\bf{R}))$ being injective and $\deg Tp\leq\deg p$ implies $T$ is surjective and $\deg Tp=\deg p$,Proof verification:  being injective and  implies  is surjective and,T\in\mathcal{L}(\mathcal{P}(\bf{R})) \deg Tp\leq\deg p T \deg Tp=\deg p,"Notations : $\mathcal P(\mathbf R)$ denotes the polynomials with real coefficients, while $\mathcal{L}(\mathcal P(\mathbf R))$ denotes the linear transformations within $\mathcal P(\mathbf R)$ with respect to the field of real numbers. Also, $\mathcal P_m(\mathbf R)$ denotes the polynomials with real coefficient and degree less than or equal to $m$. Are the proofs to the following propositions Correct ? Theorem. Given that $T\in\mathcal{L}(\mathcal{P}(\mathbf{R}))$ is injective and that $\deg Tp\leq\deg p$ for all $p\in\mathcal{P}(\mathbf{R})$ such that $p$ is a non-zero polynomial then $a)$ $T$ is surjective Proof. Let $p$ be arbitrary non-zero polynomial. We may assume without loss of generality that $\deg p = m$ where $m\in\{0,1,2,3,...\}$ Let us now examine the function $T|_{\mathcal{P}_m\mathbf{(R)}}$. The proposition $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ together with $T\in\mathcal{L}(\mathcal{P}\mathbf{(R)})$ implies that $T|_{\mathcal{P}_m\mathbf{(R)}}\in\mathcal{L}(\mathcal{P}_m\mathbf{(R)})$, moreover the injectivity of $T$ implies the the injectivity  of $T|_{\mathcal{P}_m\mathbf{(R)}}$ and since injectivity and surjectivity are equivalent for linear operators defined on a finite-dimensional vector space it follows that $T|_{\mathcal{P}_m\mathbf{(R)}}$ is surjective, consequently $\exists q\in \mathcal{P(\mathbf{R})}(Tq=p)$. $\blacksquare$ $b)$ $\deg Tp=\deg p$ for all non zero polynomial $p\in\mathcal{P}(\mathbf{R})$ Proof. We prove the above propostion by recourse to Mathematical-Induction . Basis-Step: Let $p$ be an arbitrary polynomial with $\deg p=0$ ($p$ is non-zero) we know that $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ thus in particular $\deg Tp\leq \deg p$. Now assume for the purpose of contradiction that $\deg Tp<\deg p$ which implies that $\deg Tp = -\infty$ but since $T$ is injective and therefore $\operatorname{null}T=\{0\}$ it must be that $p=0$ a contradiction consequently $\deg Tp=\deg p=0$. Inductive Step: Assume that $k$ is arbitrary and that $\deg Tp=\deg p$ where $\deg p\in\{0,1,2,3,...,k\}$. Consider now an arbitrary polynomial $q$ such that $\deg q=k+1$ assume further that $Tq=r$ and that $\deg r<\deg q$ therefore $\deg r\leq k$ since $T$ is surjective it follows that for some $s\in\mathcal{P(\mathbf{R})}$, $Ts=r$ and by inductive hypothesis $\deg s = \deg r \leq k$ but $Ts=r=Tq$ and $s\neq q$ contradicting the fact that $T$ is injective therefore it must be that $\deg q=\deg r$. $\blacksquare$","Notations : $\mathcal P(\mathbf R)$ denotes the polynomials with real coefficients, while $\mathcal{L}(\mathcal P(\mathbf R))$ denotes the linear transformations within $\mathcal P(\mathbf R)$ with respect to the field of real numbers. Also, $\mathcal P_m(\mathbf R)$ denotes the polynomials with real coefficient and degree less than or equal to $m$. Are the proofs to the following propositions Correct ? Theorem. Given that $T\in\mathcal{L}(\mathcal{P}(\mathbf{R}))$ is injective and that $\deg Tp\leq\deg p$ for all $p\in\mathcal{P}(\mathbf{R})$ such that $p$ is a non-zero polynomial then $a)$ $T$ is surjective Proof. Let $p$ be arbitrary non-zero polynomial. We may assume without loss of generality that $\deg p = m$ where $m\in\{0,1,2,3,...\}$ Let us now examine the function $T|_{\mathcal{P}_m\mathbf{(R)}}$. The proposition $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ together with $T\in\mathcal{L}(\mathcal{P}\mathbf{(R)})$ implies that $T|_{\mathcal{P}_m\mathbf{(R)}}\in\mathcal{L}(\mathcal{P}_m\mathbf{(R)})$, moreover the injectivity of $T$ implies the the injectivity  of $T|_{\mathcal{P}_m\mathbf{(R)}}$ and since injectivity and surjectivity are equivalent for linear operators defined on a finite-dimensional vector space it follows that $T|_{\mathcal{P}_m\mathbf{(R)}}$ is surjective, consequently $\exists q\in \mathcal{P(\mathbf{R})}(Tq=p)$. $\blacksquare$ $b)$ $\deg Tp=\deg p$ for all non zero polynomial $p\in\mathcal{P}(\mathbf{R})$ Proof. We prove the above propostion by recourse to Mathematical-Induction . Basis-Step: Let $p$ be an arbitrary polynomial with $\deg p=0$ ($p$ is non-zero) we know that $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ thus in particular $\deg Tp\leq \deg p$. Now assume for the purpose of contradiction that $\deg Tp<\deg p$ which implies that $\deg Tp = -\infty$ but since $T$ is injective and therefore $\operatorname{null}T=\{0\}$ it must be that $p=0$ a contradiction consequently $\deg Tp=\deg p=0$. Inductive Step: Assume that $k$ is arbitrary and that $\deg Tp=\deg p$ where $\deg p\in\{0,1,2,3,...,k\}$. Consider now an arbitrary polynomial $q$ such that $\deg q=k+1$ assume further that $Tq=r$ and that $\deg r<\deg q$ therefore $\deg r\leq k$ since $T$ is surjective it follows that for some $s\in\mathcal{P(\mathbf{R})}$, $Ts=r$ and by inductive hypothesis $\deg s = \deg r \leq k$ but $Ts=r=Tq$ and $s\neq q$ contradicting the fact that $T$ is injective therefore it must be that $\deg q=\deg r$. $\blacksquare$",,"['linear-algebra', 'proof-verification', 'linear-transformations']"
26,Can $Ax = b$ have infinitely many solutions for every $b$ in $ℝ^m$?,Can  have infinitely many solutions for every  in ?,Ax = b b ℝ^m,"I've been learning about linear independence/dependence and this question was asked: Suppose an m × n matrix A has n pivot columns. Explain why for each b in $ℝ^m$ the equation Ax=b has at most one solution? Hint: Explain why Ax=b cannot have infinitely many solutions. For reference: Let A be an m × n matrix.  Then the following statements are logically equivalent: For each b in $\Bbb R^m$, the equation Ax = b has a solution. Each b in $\Bbb R^m$ is a linear combination of the columns of A . The columns of A span $\Bbb R^m$. A has a pivot position in every row. I understand that each b in $ℝ^m$ has one solution, because, in the question, every column is a pivot column.  Therefore, there are no free variables, and Ax = b cannot have infinitely many solutions.  However, all this led to my question: Is there a circumstance where Ax = b has infinitely many solutions for every b in $ℝ^m$ , or, if there's a solution for every b in $ℝ^m$ , is it always unique (only one)? Edit: Would this matrix A have infinitely many solutions for every b ? \begin{array}{l}1&0&0&*\\0&1&0&*\\0&0&1&*\end{array} Asterisk = any number. If you added vector b to the right hand side of this matrix A , to make it an augmented matrix, there would still be a pivot position in every row, but there would also be a free variable $x_4$.  Would this be a circumstance where Ax = b has infinitely many solutions for every b in $ℝ^m$ ?","I've been learning about linear independence/dependence and this question was asked: Suppose an m × n matrix A has n pivot columns. Explain why for each b in $ℝ^m$ the equation Ax=b has at most one solution? Hint: Explain why Ax=b cannot have infinitely many solutions. For reference: Let A be an m × n matrix.  Then the following statements are logically equivalent: For each b in $\Bbb R^m$, the equation Ax = b has a solution. Each b in $\Bbb R^m$ is a linear combination of the columns of A . The columns of A span $\Bbb R^m$. A has a pivot position in every row. I understand that each b in $ℝ^m$ has one solution, because, in the question, every column is a pivot column.  Therefore, there are no free variables, and Ax = b cannot have infinitely many solutions.  However, all this led to my question: Is there a circumstance where Ax = b has infinitely many solutions for every b in $ℝ^m$ , or, if there's a solution for every b in $ℝ^m$ , is it always unique (only one)? Edit: Would this matrix A have infinitely many solutions for every b ? \begin{array}{l}1&0&0&*\\0&1&0&*\\0&0&1&*\end{array} Asterisk = any number. If you added vector b to the right hand side of this matrix A , to make it an augmented matrix, there would still be a pivot position in every row, but there would also be a free variable $x_4$.  Would this be a circumstance where Ax = b has infinitely many solutions for every b in $ℝ^m$ ?",,"['linear-algebra', 'matrices', 'matrix-equations']"
27,Distribution of the outer product of two Gaussian vectors,Distribution of the outer product of two Gaussian vectors,,"Assume that $\mathbf{x} \sim \mathcal{N}(0,\mathbf{I}_n)$ and $\mathbf{y}\sim \mathcal{N}(0,\mathbf{I}_p)$ are two independent standard Gaussian vectors. What is the distribution of their outer product $$\mathbf{x}\mathbf{y}^T=(x_iy_j)_{i\leq n, j\leq p},$$ which is a $n\times p$ matrix? In the simple case where $p=n=1$, we end up we the normal-product distribution , but in higher dimensions, things appear to get trickier and I don't know much about matrix variate distributions.","Assume that $\mathbf{x} \sim \mathcal{N}(0,\mathbf{I}_n)$ and $\mathbf{y}\sim \mathcal{N}(0,\mathbf{I}_p)$ are two independent standard Gaussian vectors. What is the distribution of their outer product $$\mathbf{x}\mathbf{y}^T=(x_iy_j)_{i\leq n, j\leq p},$$ which is a $n\times p$ matrix? In the simple case where $p=n=1$, we end up we the normal-product distribution , but in higher dimensions, things appear to get trickier and I don't know much about matrix variate distributions.",,"['linear-algebra', 'probability', 'probability-distributions', 'normal-distribution', 'outer-product']"
28,What is the Householder matrix for complex vector space?,What is the Householder matrix for complex vector space?,,"For $\Bbb R^n$, Householder matrix $Q=I-2vv^T$ is an operator that maps a vector to its reflection across a hyperplane of normal $v$. The following is an illustration for Householder operator of a general inner product space. For $\Bbb R^n$, use standard inner product, then we have $x-2<x,v>v=x-2vv^Tx=(I-2vv^T)x$, where $I-2vv^T$ is the Householder matrix. It satisfies the following However, I am wondering what would be the Householder matrix for complex vector space $\Bbb C^n$? It looks like $I-2vv^H$ is not right ($H$ denotes conjugate transpose), because by my calculation it does not satisfy above problem 5.7.3. Wikipedia suggests I tried, and I might be wrong, but it does not seem to satisfy problem 5.7.3 either. $Q(x+y)=(x + y) - \frac{{x - y}}{{{{\left\| {x - y} \right\|}^2}}}({(x - y)^H}(x + y) + \frac{{{x^H}(x - y)}}{{{{(x - y)}^H}x}}{(x - y)^H}(x + y))$ It looks impossible for $Q(x+y)=x+y$ unless $x^Hy=y^Hx$.","For $\Bbb R^n$, Householder matrix $Q=I-2vv^T$ is an operator that maps a vector to its reflection across a hyperplane of normal $v$. The following is an illustration for Householder operator of a general inner product space. For $\Bbb R^n$, use standard inner product, then we have $x-2<x,v>v=x-2vv^Tx=(I-2vv^T)x$, where $I-2vv^T$ is the Householder matrix. It satisfies the following However, I am wondering what would be the Householder matrix for complex vector space $\Bbb C^n$? It looks like $I-2vv^H$ is not right ($H$ denotes conjugate transpose), because by my calculation it does not satisfy above problem 5.7.3. Wikipedia suggests I tried, and I might be wrong, but it does not seem to satisfy problem 5.7.3 either. $Q(x+y)=(x + y) - \frac{{x - y}}{{{{\left\| {x - y} \right\|}^2}}}({(x - y)^H}(x + y) + \frac{{{x^H}(x - y)}}{{{{(x - y)}^H}x}}{(x - y)^H}(x + y))$ It looks impossible for $Q(x+y)=x+y$ unless $x^Hy=y^Hx$.",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
29,An alternative algorithm to find the Jordan form/basis for a complex matrix.,An alternative algorithm to find the Jordan form/basis for a complex matrix.,,"I am currently studying System Theory, and the exam involves a lot of finding Jordan forms/bases for state transition matrices. I know there is an algorithm for doing so which involves generalized eigenvectors and all, but that involves computing many powers of the matrix, which is tedious and prone to errors. I seem to have found an alternative method, but I can't find it stated anywhere else and I am wondering why it should be unsound, so I'll drop it here along with an example, and I hope someone will clarify what's wrong with it. Algorithm Let's say we have a matrix called $A$, and through the characteristic polynomial we find its eigenvalues $\{\lambda_i\}$ whose algebraic multiplicities are $\{m_i\}$. For each eigenvalue let $T_i$ be $A-\lambda_i I$: through it we can easily find $g_i = \dim \ker T_i$, the geometric multiplicity of the eigenvalue, and the eigenvectors of $A$,  $v_{i,1,1} \cdots v_{i,m_i,1}$, constituting a basis for $\ker T_i$. I'm calling these vectors like this because $v_{i,j,1}$ is the first vector in the partial basis that corresponds to the $j$-th Jordan block with eigenvalue $\lambda_i$. Then from the shape of the Jordan block we know that $A v_{i,j,k+1} = \lambda_i v_{i,j,k+1} + v_{i,j,k}$ or alternatively $T_i v_{i,j,k+1} = v_{i,j,k}$, so we can recursively find $v_{i,j,k+1}$ through a linear equation, or if we find no solution we know that the Jordan block with partial basis starting with $v_{i,j,1}$ is done. Then we can put together all the $v_{i,j,k}$'s to find the Jordan form and the desired basis. Example We wish to find the Jordan form of $A=\left( \begin{array}{cccc}  2 & 0 & 0 & 0 \\  0 & 2 & 1 & 0 \\  0 & 0 & 2 & 0 \\  1 & 0 & 0 & 2 \\ \end{array} \right)$. The characteristic polynomial is $\det(A-\lambda I) = (\lambda - 2)^4$, so we have $\lambda_1 = 2, m_1 = 4$. We take $\lambda_1 = 2$: by inspecting the kernel of $T_1 = A - 2I$ we find $v_{1,1,1} = \left( \begin{array}{c}  0 \\  1 \\  0 \\  0 \\ \end{array} \right)$ and $v_{1,2,1}=\left( \begin{array}{c}  0 \\  0 \\  0 \\  1 \\ \end{array} \right)$. We start from $v_{1,1,1}$, and we try to find $v_{1,1,2}$ by solving $T_1 v_{1,1,2} = v_{1,1,1} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  1 \\  0 \\  0 \\ \end{array} \right) \implies x=0, z=1$, so I choose $v_{1,1,2}=\left( \begin{array}{c}  0 \\  0 \\  1 \\  0 \\ \end{array} \right)$. Now we try to find $v_{1,1,3}$ by solving $T_1 v_{1,1,3} = v_{1,1,2} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  0 \\  1 \\  0 \\ \end{array} \right)$ which is impossible. So the basis for the first Jordan block is complete, and there must be $v_{1,2,2}$ that is the last vector we seek. We find it as usual by $T_1 v_{1,2,2} = v_{1,2,1} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  0 \\  0 \\  1 \\ \end{array} \right) \implies x=1, z=0$ so we choose $v_{1,2,2}=\left( \begin{array}{c}  1 \\  0 \\  0 \\  0 \\ \end{array} \right)$. So we have found $J=\left( \begin{array}{cccc}  2 & 1 & 0 & 0 \\  0 & 2 & 0 & 0 \\  0 & 0 & 2 & 1 \\  0 & 0 & 0 & 2 \\ \end{array} \right)$ by looking at the lengths of the chains we computed, and also we found $S=\left( \begin{array}{cccc}  0 & 0 & 0 & 1 \\  1 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 \\  0 & 0 & 1 & 0 \\ \end{array} \right)$ such that $J=S^{-1}AS$. This looks long because I wrote it all down, but most times many vectors can be computed mentally, and most importantly it doesn't involve computing $T_1^2$ and $T_1^3$.","I am currently studying System Theory, and the exam involves a lot of finding Jordan forms/bases for state transition matrices. I know there is an algorithm for doing so which involves generalized eigenvectors and all, but that involves computing many powers of the matrix, which is tedious and prone to errors. I seem to have found an alternative method, but I can't find it stated anywhere else and I am wondering why it should be unsound, so I'll drop it here along with an example, and I hope someone will clarify what's wrong with it. Algorithm Let's say we have a matrix called $A$, and through the characteristic polynomial we find its eigenvalues $\{\lambda_i\}$ whose algebraic multiplicities are $\{m_i\}$. For each eigenvalue let $T_i$ be $A-\lambda_i I$: through it we can easily find $g_i = \dim \ker T_i$, the geometric multiplicity of the eigenvalue, and the eigenvectors of $A$,  $v_{i,1,1} \cdots v_{i,m_i,1}$, constituting a basis for $\ker T_i$. I'm calling these vectors like this because $v_{i,j,1}$ is the first vector in the partial basis that corresponds to the $j$-th Jordan block with eigenvalue $\lambda_i$. Then from the shape of the Jordan block we know that $A v_{i,j,k+1} = \lambda_i v_{i,j,k+1} + v_{i,j,k}$ or alternatively $T_i v_{i,j,k+1} = v_{i,j,k}$, so we can recursively find $v_{i,j,k+1}$ through a linear equation, or if we find no solution we know that the Jordan block with partial basis starting with $v_{i,j,1}$ is done. Then we can put together all the $v_{i,j,k}$'s to find the Jordan form and the desired basis. Example We wish to find the Jordan form of $A=\left( \begin{array}{cccc}  2 & 0 & 0 & 0 \\  0 & 2 & 1 & 0 \\  0 & 0 & 2 & 0 \\  1 & 0 & 0 & 2 \\ \end{array} \right)$. The characteristic polynomial is $\det(A-\lambda I) = (\lambda - 2)^4$, so we have $\lambda_1 = 2, m_1 = 4$. We take $\lambda_1 = 2$: by inspecting the kernel of $T_1 = A - 2I$ we find $v_{1,1,1} = \left( \begin{array}{c}  0 \\  1 \\  0 \\  0 \\ \end{array} \right)$ and $v_{1,2,1}=\left( \begin{array}{c}  0 \\  0 \\  0 \\  1 \\ \end{array} \right)$. We start from $v_{1,1,1}$, and we try to find $v_{1,1,2}$ by solving $T_1 v_{1,1,2} = v_{1,1,1} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  1 \\  0 \\  0 \\ \end{array} \right) \implies x=0, z=1$, so I choose $v_{1,1,2}=\left( \begin{array}{c}  0 \\  0 \\  1 \\  0 \\ \end{array} \right)$. Now we try to find $v_{1,1,3}$ by solving $T_1 v_{1,1,3} = v_{1,1,2} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  0 \\  1 \\  0 \\ \end{array} \right)$ which is impossible. So the basis for the first Jordan block is complete, and there must be $v_{1,2,2}$ that is the last vector we seek. We find it as usual by $T_1 v_{1,2,2} = v_{1,2,1} \implies \left( \begin{array}{cccc}  0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 \\ \end{array} \right) \left( \begin{array}{c}  x \\  y \\  z \\  w \\ \end{array} \right) = \left( \begin{array}{c}  0 \\  0 \\  0 \\  1 \\ \end{array} \right) \implies x=1, z=0$ so we choose $v_{1,2,2}=\left( \begin{array}{c}  1 \\  0 \\  0 \\  0 \\ \end{array} \right)$. So we have found $J=\left( \begin{array}{cccc}  2 & 1 & 0 & 0 \\  0 & 2 & 0 & 0 \\  0 & 0 & 2 & 1 \\  0 & 0 & 0 & 2 \\ \end{array} \right)$ by looking at the lengths of the chains we computed, and also we found $S=\left( \begin{array}{cccc}  0 & 0 & 0 & 1 \\  1 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 \\  0 & 0 & 1 & 0 \\ \end{array} \right)$ such that $J=S^{-1}AS$. This looks long because I wrote it all down, but most times many vectors can be computed mentally, and most importantly it doesn't involve computing $T_1^2$ and $T_1^3$.",,"['linear-algebra', 'jordan-normal-form']"
30,"If $A$ is a non-square matrix with orthonormal columns, what is $A^+$?","If  is a non-square matrix with orthonormal columns, what is ?",A A^+,"If a matrix has orthonormal columns, they must be linearly independent, so $A^+ = (A^T A)^{−1} A^T$ . Also, the fact that its columns are orthonormal gives $A^T A = I$. Therefore, $$A^+ = (A^T A)^{−1} A^T = (I)^{-1}A^T = A^T$$ Thus, $A^+ = A^T$. Am I correct? Thank you.","If a matrix has orthonormal columns, they must be linearly independent, so $A^+ = (A^T A)^{−1} A^T$ . Also, the fact that its columns are orthonormal gives $A^T A = I$. Therefore, $$A^+ = (A^T A)^{−1} A^T = (I)^{-1}A^T = A^T$$ Thus, $A^+ = A^T$. Am I correct? Thank you.",,"['linear-algebra', 'matrices', 'least-squares', 'pseudoinverse']"
31,Eigenvalues of the linear system $XA +A^T X = 0$,Eigenvalues of the linear system,XA +A^T X = 0,"I am following a proof of the next theorem, rephrased from Theorem 8.5 of the book ""Introduction to the theory of differential inclusions"" by Georgi V. Smirnov. Let $\{\lambda_1,\dots,\lambda_n\}$ be eigenvalues of a constant matrix $A \in \mathbb{R}^{n\times n}$ and $X \in \mathbb{R}^{n\times n}$ be a variable in the set of symmetric matrices. Consider a linear system $XA + A^TX = 0$. It is equivalent to $Bx = 0$ where $x = (X_{11},X_{12},X_{22},\dots,X_{1n},\dots,X_{nn}) \in \mathbb{R}^{n(n+1)/2}$. Define   \begin{align} C &= \{\text{all eigenvalues of $B$}\}, \\ D &= \{m_1\lambda_1 + \cdots + m_n\lambda_n \mid m_i \in \{0,1,2\},\,m_1 + \cdots + m_n = 2\}. \end{align}   Then, C = D. I understood the part $D \subset C$ but I am stuck in the part $C\subset D$. If $|D| = n(n+1)/2$, then it is obvious but in the case $|D| < n(n+1)/2$ the book says only ""In the general case the result can be obtained taking the limit"". Would you give me any hint or reference how to use the limit? Edit I hope this is a correct proof of the part $C \subset D$ that uses the limit. Since $D\subset C$ and $|C|$ is at most $n(n+1)/2$, if $|D| = n(n+1)/2$, then $D = C$. Suppose $|D| < n(n+1)/2$. Consider a linear system $X(A + dA) + (A + dA)^TX = 0$ where $dA \in \mathbb{R}^{n\times n}$. It is equivalent to $(B + dB)x = 0$ and $\|dA\| \to 0 \Leftrightarrow \|dB\| \to 0$. Let $\{\lambda_1'(dA),\dots,\lambda_n'(dA)\}$ and $C = \{\rho_1,\dots,\rho_{n(n+1)/2}\}$ be eigenvalues of $A + dA$ and $B$, respectively. Define   \begin{align} C' &= \{\rho_1'(dB),\dots,\rho_{n(n+1)/2}'(dB)\} = \{\text{all eigenvalues of $B + dB$}\}, \\ D' &= \{m_1\lambda_1'(dA) + \cdots + m_n\lambda_n'(dA) \mid m_i \in \{0,1,2\},\,m_1 + \cdots + m_n = 2\}. \end{align}   Solutions of the characteristic equations $\det(A + dA - \lambda I) = 0$ and $\det(B + dB - \rho I) = 0$ are continuous functions with respect to parameters. So, for all $\varepsilon > 0$, there exists $\delta > 0$ such that $\max_i |\lambda_i'(dA) - \lambda_i| < \varepsilon$ and $\max_i|\rho_i'(dB) - \rho_i| < \varepsilon$ if $\|dA\| < \delta$ and $\|dB\|< \delta$; we assume that eigenvalues are properly listed. Also, there exists $\|dA\| < \delta$ such that all eigenvalues of $A + dA$ are distinct. Furthermore, there exists $\|dA\| < \delta$ satisfying $|D'| = n(n+1)/2$. Let $\rho \in C$. There exists $\rho' = m_1\lambda_1(dA) + \cdots + m_n\lambda_n(dA) \in C'$ such that   \begin{equation} |\rho - (m_1\lambda_1 + \cdots + m_n\lambda_n)| \le |\rho - \rho'| + |\rho' - (m_1\lambda_1 + \cdots + m_n\lambda_n)| < (1 + 2n)\varepsilon. \end{equation}   Since $\varepsilon$ is arbitrary, $\rho = m_1\lambda_1 + \cdots + m_n\lambda_n \in D$.","I am following a proof of the next theorem, rephrased from Theorem 8.5 of the book ""Introduction to the theory of differential inclusions"" by Georgi V. Smirnov. Let $\{\lambda_1,\dots,\lambda_n\}$ be eigenvalues of a constant matrix $A \in \mathbb{R}^{n\times n}$ and $X \in \mathbb{R}^{n\times n}$ be a variable in the set of symmetric matrices. Consider a linear system $XA + A^TX = 0$. It is equivalent to $Bx = 0$ where $x = (X_{11},X_{12},X_{22},\dots,X_{1n},\dots,X_{nn}) \in \mathbb{R}^{n(n+1)/2}$. Define   \begin{align} C &= \{\text{all eigenvalues of $B$}\}, \\ D &= \{m_1\lambda_1 + \cdots + m_n\lambda_n \mid m_i \in \{0,1,2\},\,m_1 + \cdots + m_n = 2\}. \end{align}   Then, C = D. I understood the part $D \subset C$ but I am stuck in the part $C\subset D$. If $|D| = n(n+1)/2$, then it is obvious but in the case $|D| < n(n+1)/2$ the book says only ""In the general case the result can be obtained taking the limit"". Would you give me any hint or reference how to use the limit? Edit I hope this is a correct proof of the part $C \subset D$ that uses the limit. Since $D\subset C$ and $|C|$ is at most $n(n+1)/2$, if $|D| = n(n+1)/2$, then $D = C$. Suppose $|D| < n(n+1)/2$. Consider a linear system $X(A + dA) + (A + dA)^TX = 0$ where $dA \in \mathbb{R}^{n\times n}$. It is equivalent to $(B + dB)x = 0$ and $\|dA\| \to 0 \Leftrightarrow \|dB\| \to 0$. Let $\{\lambda_1'(dA),\dots,\lambda_n'(dA)\}$ and $C = \{\rho_1,\dots,\rho_{n(n+1)/2}\}$ be eigenvalues of $A + dA$ and $B$, respectively. Define   \begin{align} C' &= \{\rho_1'(dB),\dots,\rho_{n(n+1)/2}'(dB)\} = \{\text{all eigenvalues of $B + dB$}\}, \\ D' &= \{m_1\lambda_1'(dA) + \cdots + m_n\lambda_n'(dA) \mid m_i \in \{0,1,2\},\,m_1 + \cdots + m_n = 2\}. \end{align}   Solutions of the characteristic equations $\det(A + dA - \lambda I) = 0$ and $\det(B + dB - \rho I) = 0$ are continuous functions with respect to parameters. So, for all $\varepsilon > 0$, there exists $\delta > 0$ such that $\max_i |\lambda_i'(dA) - \lambda_i| < \varepsilon$ and $\max_i|\rho_i'(dB) - \rho_i| < \varepsilon$ if $\|dA\| < \delta$ and $\|dB\|< \delta$; we assume that eigenvalues are properly listed. Also, there exists $\|dA\| < \delta$ such that all eigenvalues of $A + dA$ are distinct. Furthermore, there exists $\|dA\| < \delta$ satisfying $|D'| = n(n+1)/2$. Let $\rho \in C$. There exists $\rho' = m_1\lambda_1(dA) + \cdots + m_n\lambda_n(dA) \in C'$ such that   \begin{equation} |\rho - (m_1\lambda_1 + \cdots + m_n\lambda_n)| \le |\rho - \rho'| + |\rho' - (m_1\lambda_1 + \cdots + m_n\lambda_n)| < (1 + 2n)\varepsilon. \end{equation}   Since $\varepsilon$ is arbitrary, $\rho = m_1\lambda_1 + \cdots + m_n\lambda_n \in D$.",,['linear-algebra']
32,Baby Rudin Chapter 1 Problem 16,Baby Rudin Chapter 1 Problem 16,,"I am attempting to self-study through Baby Rudin and I have done every exercise in chapter 1 except for problem 16. Every solution I find on the internet for this problem uses theorems from linear algebra which I have no background in. I don't want to go on to chapter 2 without completing chapter 1. Can anyone provide an explanation of this problem that does not assume prerequisite knowledge of linear algebra? Suppose $k \geq 3$, $x, y \in R^k$, $|x-y| = d>0$, and $r>0$. Prove: (a) If $2r>d$ there are infinitely many $z \in R^k$ such that $|z-x| = |z-y| = r $ (b) If $2r = d $ there is exactly one such $z$. (c) If $2r < d$ there is no such $z$. My work so far: (c) is a trivial consequence of the triangle inequality. In about 2 weeks I have been unable to make any progress on (a) or (b).","I am attempting to self-study through Baby Rudin and I have done every exercise in chapter 1 except for problem 16. Every solution I find on the internet for this problem uses theorems from linear algebra which I have no background in. I don't want to go on to chapter 2 without completing chapter 1. Can anyone provide an explanation of this problem that does not assume prerequisite knowledge of linear algebra? Suppose $k \geq 3$, $x, y \in R^k$, $|x-y| = d>0$, and $r>0$. Prove: (a) If $2r>d$ there are infinitely many $z \in R^k$ such that $|z-x| = |z-y| = r $ (b) If $2r = d $ there is exactly one such $z$. (c) If $2r < d$ there is no such $z$. My work so far: (c) is a trivial consequence of the triangle inequality. In about 2 weeks I have been unable to make any progress on (a) or (b).",,"['real-analysis', 'linear-algebra']"
33,Finding the area of intersection and area of union between two sets of overlapping rectangles,Finding the area of intersection and area of union between two sets of overlapping rectangles,,"Suppose I have $N_r$ number of red rectangle boxes and $N_b$ rectangle boxes in an image of some arbitrary size. Each box is parameterized as $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$. I would like to find the area of intersection $A_I$ and the union $A_U$ between two sets of overlapping rectangles. This is because I would like to calculate the intersection over union between the two sets. That is, I would like to find the pink area in the two examples shown in the image below: I came up with a solution to find the intersection between two rectangles: $$ A_I = [\min(x_{\max,r},x_{\max,b}) - \max(x_{\min,r},x_{\min,b}) + 1] \cdot [\min(y_{\max,r},y_{\max,b}) - \max(y_{\min,r},y_{\min,b}) + 1] $$ $$ A_U = (x_{\max,r}-x_{\min,r})\cdot(y_{\max,r}-y_{\min,r}) + (x_{\max,b}-x_{\min,b})\cdot(y_{\max,b}-y_{\min,b}) - A_I$$ However, I am having trouble coming up with a solution for a problem with multiple boxes in each of the red and blue rectangle sets. It should be noted that the solution needs to be differentiable. This is because I am using it to calculate the error between the red and blue rectangles and trying to optimize the parameters in a neural network that would produce a set of the rectangles. So you can think of the red rectangles as the prediction and the blue as the ground truth.","Suppose I have $N_r$ number of red rectangle boxes and $N_b$ rectangle boxes in an image of some arbitrary size. Each box is parameterized as $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$. I would like to find the area of intersection $A_I$ and the union $A_U$ between two sets of overlapping rectangles. This is because I would like to calculate the intersection over union between the two sets. That is, I would like to find the pink area in the two examples shown in the image below: I came up with a solution to find the intersection between two rectangles: $$ A_I = [\min(x_{\max,r},x_{\max,b}) - \max(x_{\min,r},x_{\min,b}) + 1] \cdot [\min(y_{\max,r},y_{\max,b}) - \max(y_{\min,r},y_{\min,b}) + 1] $$ $$ A_U = (x_{\max,r}-x_{\min,r})\cdot(y_{\max,r}-y_{\min,r}) + (x_{\max,b}-x_{\min,b})\cdot(y_{\max,b}-y_{\min,b}) - A_I$$ However, I am having trouble coming up with a solution for a problem with multiple boxes in each of the red and blue rectangle sets. It should be noted that the solution needs to be differentiable. This is because I am using it to calculate the error between the red and blue rectangles and trying to optimize the parameters in a neural network that would produce a set of the rectangles. So you can think of the red rectangles as the prediction and the blue as the ground truth.",,"['calculus', 'linear-algebra', 'geometry', 'optimization', 'rectangles']"
34,Proofs involving a set of infinite dimensional vectors,Proofs involving a set of infinite dimensional vectors,,"Given: A = $\{  \vec{v} = (v_{1}, ....v_{k}, ...) \in \mathbb{R}^{\infty} | \sum_{i=1}^\infty v_i^2 \text{converges} \}$ Prove that the set is a subspace of $\mathbb{R}^\infty$ and: < $\vec{v}, \vec{u} > = \sum_{i=1}^\infty v_iu_i$ defines an inner product on A. I am really struggling with this problem, particularly showing closure under addition to show the set is a subspace, and proving positive definiteness of the inner product. I have been given the hint by a professor ""use the Cauchy-Schwarz inequality"" but I lack confidence in this advice, it is my understanding that a well defined inner product is a pre-condition for using Cauchy-Schwarz inequality. This is associated with an intro Linear Algebra take home exam, I have found a lot of information that leads me to believe this is regarding an $\mathscr{l}^2$ - Hilbert space, a concept that is not actually in my textbook. How to go about this proof? Ideally in a way that someone with 1 semester of linear algebra could understand.","Given: A = $\{  \vec{v} = (v_{1}, ....v_{k}, ...) \in \mathbb{R}^{\infty} | \sum_{i=1}^\infty v_i^2 \text{converges} \}$ Prove that the set is a subspace of $\mathbb{R}^\infty$ and: < $\vec{v}, \vec{u} > = \sum_{i=1}^\infty v_iu_i$ defines an inner product on A. I am really struggling with this problem, particularly showing closure under addition to show the set is a subspace, and proving positive definiteness of the inner product. I have been given the hint by a professor ""use the Cauchy-Schwarz inequality"" but I lack confidence in this advice, it is my understanding that a well defined inner product is a pre-condition for using Cauchy-Schwarz inequality. This is associated with an intro Linear Algebra take home exam, I have found a lot of information that leads me to believe this is regarding an $\mathscr{l}^2$ - Hilbert space, a concept that is not actually in my textbook. How to go about this proof? Ideally in a way that someone with 1 semester of linear algebra could understand.",,"['linear-algebra', 'proof-writing', 'hilbert-spaces']"
35,Show all matrices very similar to the identity matrix are invertible,Show all matrices very similar to the identity matrix are invertible,,"Please don't post completed solutions for 24 hours, as this is a homework problem, and I just need a bit of guidance. Given a $N\times N$ matrix A, let $|\mathbf{*}|_{2}$ be the sum of the squares of each element of such a matrix. If $|I-A|_2 \leq \frac{1}{2}$, then $A^{-1}$ must exist. Following is an overview of my attempts, not the question itself. From the assumption, it follows that the absolute value of every element of $|I-A|_2$ is less than $\frac{1}{2}$, or one is exactly $\frac{1}{2}$ and all others are $0$. I've attempted to show that such a matrix can be changed to row echelon form, by using the bounds of each element of $A$, showing that if I subtract some multiple of the first row from all following rows, $A_{2,2}$ must be positive, but it doesn't hold for the general case, specifically not for $A_{3,3}$. If the function $L_A$ (the linear map taking in vectors $v$ from $R^n$, and outputting $A\cdot v$) could be shown to map only $0_n$ to $0_n$, that would be sufficient, but my attempts to prove that have reduced to attempting to show that A can be reduced to ref form, and if I could show that directly, that would be sufficient proof. Similarly, if I could show $L_A$ to be injective or surjective, or that the kernel of $L_A$ was $\{0_n\}$, I could show that $A^{-1}$ must exist, but I have been unable to do so. Thank you in advance","Please don't post completed solutions for 24 hours, as this is a homework problem, and I just need a bit of guidance. Given a $N\times N$ matrix A, let $|\mathbf{*}|_{2}$ be the sum of the squares of each element of such a matrix. If $|I-A|_2 \leq \frac{1}{2}$, then $A^{-1}$ must exist. Following is an overview of my attempts, not the question itself. From the assumption, it follows that the absolute value of every element of $|I-A|_2$ is less than $\frac{1}{2}$, or one is exactly $\frac{1}{2}$ and all others are $0$. I've attempted to show that such a matrix can be changed to row echelon form, by using the bounds of each element of $A$, showing that if I subtract some multiple of the first row from all following rows, $A_{2,2}$ must be positive, but it doesn't hold for the general case, specifically not for $A_{3,3}$. If the function $L_A$ (the linear map taking in vectors $v$ from $R^n$, and outputting $A\cdot v$) could be shown to map only $0_n$ to $0_n$, that would be sufficient, but my attempts to prove that have reduced to attempting to show that A can be reduced to ref form, and if I could show that directly, that would be sufficient proof. Similarly, if I could show $L_A$ to be injective or surjective, or that the kernel of $L_A$ was $\{0_n\}$, I could show that $A^{-1}$ must exist, but I have been unable to do so. Thank you in advance",,"['linear-algebra', 'matrices', 'advice']"
36,Show this matrix is invertible,Show this matrix is invertible,,"Consider the matrix \begin{equation} S_n =  		\begin{bmatrix} 			\frac{1}{2!} & \frac{1}{3!} & \cdots & \frac{1}{(n+1)!} \\ 			\frac{1}{3!} & \frac{1}{4!} & \cdots & \frac{1}{(n+2)!} \\ 			\vdots & \vdots & \ddots & \vdots \\ 			\frac{1}{(n+1)!} & \frac{1}{(n+2)!} & \cdots & \frac{1}{2n!} \\ 		\end{bmatrix}. \end{equation} I am interested to know if $S_n$ is invertible for all $n$. I do not need the inverse explicitly, knowing it exists is sufficient (ie. non-zero determinant for all $n$ is enough). If this matrix has a special name, I would appreciate if someone could bring it to my attention.","Consider the matrix \begin{equation} S_n =  		\begin{bmatrix} 			\frac{1}{2!} & \frac{1}{3!} & \cdots & \frac{1}{(n+1)!} \\ 			\frac{1}{3!} & \frac{1}{4!} & \cdots & \frac{1}{(n+2)!} \\ 			\vdots & \vdots & \ddots & \vdots \\ 			\frac{1}{(n+1)!} & \frac{1}{(n+2)!} & \cdots & \frac{1}{2n!} \\ 		\end{bmatrix}. \end{equation} I am interested to know if $S_n$ is invertible for all $n$. I do not need the inverse explicitly, knowing it exists is sufficient (ie. non-zero determinant for all $n$ is enough). If this matrix has a special name, I would appreciate if someone could bring it to my attention.",,"['linear-algebra', 'matrices']"
37,Is there a slick proof for the identity that expresses the inner product of imaginary octonions in terms of the cross product?,Is there a slick proof for the identity that expresses the inner product of imaginary octonions in terms of the cross product?,,"Consider the octonions $\mathbb O$ and in particular their imaginary part $\operatorname{Im}\mathbb O$. Let $(-,-)$ be the scalar product induced by the identification of the imaginary octonions with $\Bbb R^7$. Furthermore, define the cross product of (any) octonions by $x\times y:= \frac{1}{2}(xy-yx)$. For imaginary octonions $x,y$, there is the nice identity $x\times y=(x,y)+x\cdot y$, where the dot denotes octonion multiplication. Define the linear map $L_{x,y}:\operatorname{Im}\mathbb O\to \operatorname{Im}\mathbb O$ by $L_{x,y}(v)=x\times (y\times v)$, where $x,y$ are also imaginary quaternions. It is easy to prove by explicit calculation (plugging in a basis) that there is the following identity: $$(x,y)=-\frac{1}{6}\operatorname{tr}L \qquad \qquad x,y\in \operatorname{Im}\mathbb O$$ This identity is useful because it shows that octonion multiplication of imaginary octonions only depends on the cross product, and thereby yields the equivalence of two common definitions of the exceptional Lie group $G_2$. I don't like the brute force proof, so I am left wondering whether there is a better way to see that this identity holds. Is anyone aware of a slick(er) proof? EDIT: It was just pointed out to me by Ted Shifrin that $\operatorname{tr}L$ is the would-be Killing form on $\Bbb R^7$ seen as an (almost-but-not-quite) Lie algebra (equipped with the cross product). This might point the way towards a nice proof (?)","Consider the octonions $\mathbb O$ and in particular their imaginary part $\operatorname{Im}\mathbb O$. Let $(-,-)$ be the scalar product induced by the identification of the imaginary octonions with $\Bbb R^7$. Furthermore, define the cross product of (any) octonions by $x\times y:= \frac{1}{2}(xy-yx)$. For imaginary octonions $x,y$, there is the nice identity $x\times y=(x,y)+x\cdot y$, where the dot denotes octonion multiplication. Define the linear map $L_{x,y}:\operatorname{Im}\mathbb O\to \operatorname{Im}\mathbb O$ by $L_{x,y}(v)=x\times (y\times v)$, where $x,y$ are also imaginary quaternions. It is easy to prove by explicit calculation (plugging in a basis) that there is the following identity: $$(x,y)=-\frac{1}{6}\operatorname{tr}L \qquad \qquad x,y\in \operatorname{Im}\mathbb O$$ This identity is useful because it shows that octonion multiplication of imaginary octonions only depends on the cross product, and thereby yields the equivalence of two common definitions of the exceptional Lie group $G_2$. I don't like the brute force proof, so I am left wondering whether there is a better way to see that this identity holds. Is anyone aware of a slick(er) proof? EDIT: It was just pointed out to me by Ted Shifrin that $\operatorname{tr}L$ is the would-be Killing form on $\Bbb R^7$ seen as an (almost-but-not-quite) Lie algebra (equipped with the cross product). This might point the way towards a nice proof (?)",,"['linear-algebra', 'lie-groups', 'octonions']"
38,Using eigenvectors to find the general solution from a system of equations,Using eigenvectors to find the general solution from a system of equations,,"\begin{bmatrix}-13&40&-48\\-8&23&-24\\0&0&3\end{bmatrix} Consider the matrix above. This corresponds to  \begin{align*} x_{1}'&=-13x_{1}+40x_{2}-48x_{3}\\ x_{2}'&=-8x_{1}+23x_{2}-24x_{3}\\ x_{3}'&=3x_{3} \end{align*} What I want to do is use eigenvectors to find the general solution. First I computed $\det(A-\lambda I)=0$. From this I got my eigenvalues to be $\lambda = 7$ and $\lambda = 3$ (this one is multiplicity 2). Next I would find $(A-\lambda I)x=0$ where $x$ is the eigenvector I am looking for. For $\lambda=7$, my eigenvector came out to be, \begin{bmatrix} 2\\1\\0 \end{bmatrix} So no problems here. On the other hand, I had some difficulties for when $\lambda = 3$. The matrix I would be solving for is \begin{bmatrix} -16&40&-48&0 \\ -8&20&-24&0 \\ 0&0&0&0 \end{bmatrix} When I do this I obtain, \begin{bmatrix} 1&-2.5&3&0 \\ 0&0&0&0 \\ 0&0&0&0 \end{bmatrix} so $$x_{1} -2.5x_{2}+3x_{3} = 0.$$ From here I am not sure how to get the two eigenvectors. The computer told me they are, \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix} and \begin{bmatrix} 5 \\ 2 \\ 0 \end{bmatrix} but I have no idea how to get these from what I have remaining. Also how would I write the general solution?","\begin{bmatrix}-13&40&-48\\-8&23&-24\\0&0&3\end{bmatrix} Consider the matrix above. This corresponds to  \begin{align*} x_{1}'&=-13x_{1}+40x_{2}-48x_{3}\\ x_{2}'&=-8x_{1}+23x_{2}-24x_{3}\\ x_{3}'&=3x_{3} \end{align*} What I want to do is use eigenvectors to find the general solution. First I computed $\det(A-\lambda I)=0$. From this I got my eigenvalues to be $\lambda = 7$ and $\lambda = 3$ (this one is multiplicity 2). Next I would find $(A-\lambda I)x=0$ where $x$ is the eigenvector I am looking for. For $\lambda=7$, my eigenvector came out to be, \begin{bmatrix} 2\\1\\0 \end{bmatrix} So no problems here. On the other hand, I had some difficulties for when $\lambda = 3$. The matrix I would be solving for is \begin{bmatrix} -16&40&-48&0 \\ -8&20&-24&0 \\ 0&0&0&0 \end{bmatrix} When I do this I obtain, \begin{bmatrix} 1&-2.5&3&0 \\ 0&0&0&0 \\ 0&0&0&0 \end{bmatrix} so $$x_{1} -2.5x_{2}+3x_{3} = 0.$$ From here I am not sure how to get the two eigenvectors. The computer told me they are, \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix} and \begin{bmatrix} 5 \\ 2 \\ 0 \end{bmatrix} but I have no idea how to get these from what I have remaining. Also how would I write the general solution?",,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
39,What is $\lim_{n\to\infty} \frac{x_n}{y_n}$ provided $x_n=2y_{n}-y_{n-1}$ and $y_n=3y_{n-2}-y_{n-2}$ with $x_0=y_0=1$?,What is  provided  and  with ?,\lim_{n\to\infty} \frac{x_n}{y_n} x_n=2y_{n}-y_{n-1} y_n=3y_{n-2}-y_{n-2} x_0=y_0=1,"Suppose $x_n$ and $y_n$ satisfy: $$         \begin{pmatrix}         x_n \\         y_n \\         \end{pmatrix}=\begin{pmatrix}         2&1 \\         1&1 \\         \end{pmatrix}\begin{pmatrix}         x_{n-1} \\         y_{n-1} \\         \end{pmatrix} $$  with $x_0=y_0=1$. What is $\displaystyle\lim_{n\to\infty} \frac{x_n}{y_n}$ assuming this limit exists? After doing some algebra, I've got $x_n=2y_{n}-y_{n-1}$ and $y_n=3y_{n-2}-y_{n-2}$, but cannot go further.","Suppose $x_n$ and $y_n$ satisfy: $$         \begin{pmatrix}         x_n \\         y_n \\         \end{pmatrix}=\begin{pmatrix}         2&1 \\         1&1 \\         \end{pmatrix}\begin{pmatrix}         x_{n-1} \\         y_{n-1} \\         \end{pmatrix} $$  with $x_0=y_0=1$. What is $\displaystyle\lim_{n\to\infty} \frac{x_n}{y_n}$ assuming this limit exists? After doing some algebra, I've got $x_n=2y_{n}-y_{n-1}$ and $y_n=3y_{n-2}-y_{n-2}$, but cannot go further.",,"['calculus', 'linear-algebra', 'sequences-and-series', 'limits']"
40,Mathematical name for the flipped matrix and the subsequent matrix dot product in a discrete convolution,Mathematical name for the flipped matrix and the subsequent matrix dot product in a discrete convolution,,"In full-disclosure I asked this question on Quora anonymously, because I thought the answer was going to be embarrassingly self-evident, but given the single, tangential answer (albeit very interesting), I want to maximize my chances by posting the question here with a follow-up immediately related secondary question. Before convolving a filter with an image, or a kernel with a layer in convolutional neural networks, the filter (or kernel) is flipped in its rows and columns. I am looking for the name of this flipped matrix. The matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}\color{red}{\blacksquare}&\color{blue}{\blacksquare}\\\color{green}{\blacksquare}&\color{aqua}{\blacksquare}\end{bmatrix}$ with flipped columns and rows would be $\begin{bmatrix}d&c\\b&a\end{bmatrix}=\begin{bmatrix}\color{aqua}{\blacksquare}&\color{green}{\blacksquare}\\\color{blue}{\blacksquare}&\color{red}{\blacksquare}\end{bmatrix}.$ To be clear (given prior experience - I know it is by and large completely unnecessary), I am not asking for the transpose: $\begin{bmatrix}a&b\\c&d\end{bmatrix}^\top=\begin{bmatrix}a&c\\b&d\end{bmatrix}=\begin{bmatrix}\color{red}{\blacksquare}&\color{green}{\blacksquare}\\\color{blue}{\blacksquare}&\color{aqua}{\blacksquare}\end{bmatrix}.$ Immediately after getting this flipped filter or kernel the convolution consists of a sum of all the entries of a Hadamard product , which really is sort of a ""dot product of matrices"". What is the name of this matrix operation in general: $$\text{elementwise}\sum\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\text{elementwise}\sum\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}=az+bw+cv+dy$$ ? Thanks to @Omnomnomnom the answer to the second question is the Frobenius inner product : For $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ and $\begin{bmatrix}z&w\\v &y\end{bmatrix}$, the Frobenius inner product, $\langle A,B \rangle_\mathbf F$ takes two matrices and returns a number. $$\small\sum_{\text{el.wise}}\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right) =  tr\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}^\top\cdot \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=tr\left(\begin{bmatrix}a&c\\b&d\end{bmatrix}\cdot \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\tiny az+cv+bw+dy$$ and thanks to @Hurkyl $$\small\sum_{\text{el.wise}}\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\small\sum_{\text{el.wise}}\left(\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}\right)=\begin{bmatrix}1&1\end{bmatrix}\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=\tiny az+cv+bw+dy $$","In full-disclosure I asked this question on Quora anonymously, because I thought the answer was going to be embarrassingly self-evident, but given the single, tangential answer (albeit very interesting), I want to maximize my chances by posting the question here with a follow-up immediately related secondary question. Before convolving a filter with an image, or a kernel with a layer in convolutional neural networks, the filter (or kernel) is flipped in its rows and columns. I am looking for the name of this flipped matrix. The matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}\color{red}{\blacksquare}&\color{blue}{\blacksquare}\\\color{green}{\blacksquare}&\color{aqua}{\blacksquare}\end{bmatrix}$ with flipped columns and rows would be $\begin{bmatrix}d&c\\b&a\end{bmatrix}=\begin{bmatrix}\color{aqua}{\blacksquare}&\color{green}{\blacksquare}\\\color{blue}{\blacksquare}&\color{red}{\blacksquare}\end{bmatrix}.$ To be clear (given prior experience - I know it is by and large completely unnecessary), I am not asking for the transpose: $\begin{bmatrix}a&b\\c&d\end{bmatrix}^\top=\begin{bmatrix}a&c\\b&d\end{bmatrix}=\begin{bmatrix}\color{red}{\blacksquare}&\color{green}{\blacksquare}\\\color{blue}{\blacksquare}&\color{aqua}{\blacksquare}\end{bmatrix}.$ Immediately after getting this flipped filter or kernel the convolution consists of a sum of all the entries of a Hadamard product , which really is sort of a ""dot product of matrices"". What is the name of this matrix operation in general: $$\text{elementwise}\sum\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\text{elementwise}\sum\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}=az+bw+cv+dy$$ ? Thanks to @Omnomnomnom the answer to the second question is the Frobenius inner product : For $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ and $\begin{bmatrix}z&w\\v &y\end{bmatrix}$, the Frobenius inner product, $\langle A,B \rangle_\mathbf F$ takes two matrices and returns a number. $$\small\sum_{\text{el.wise}}\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right) =  tr\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}^\top\cdot \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=tr\left(\begin{bmatrix}a&c\\b&d\end{bmatrix}\cdot \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\tiny az+cv+bw+dy$$ and thanks to @Hurkyl $$\small\sum_{\text{el.wise}}\left(\begin{bmatrix}a&b\\c&d\end{bmatrix}\circ \begin{bmatrix}z&w\\v &y\end{bmatrix}\right)=\small\sum_{\text{el.wise}}\left(\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}\right)=\begin{bmatrix}1&1\end{bmatrix}\begin{bmatrix}az&bw\\cv&dy\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=\tiny az+cv+bw+dy $$",,"['linear-algebra', 'terminology', 'convolution', 'machine-learning']"
41,Discriminant of an almost cyclotomic field,Discriminant of an almost cyclotomic field,,"Let $k$ be a positive even square-free integer and let $$L:=\mathbb{Q}(\zeta_k,\sqrt{2}).$$ This is the maximal abelian extension of $L$ contained in $\mathbb{Q}(\zeta_k,2^{\frac{1}{k}})$. I would like to ask whether there is an explicit formula for the discriminant of $L$ only in terms of $k$, as the one that is given here for example.","Let $k$ be a positive even square-free integer and let $$L:=\mathbb{Q}(\zeta_k,\sqrt{2}).$$ This is the maximal abelian extension of $L$ contained in $\mathbb{Q}(\zeta_k,2^{\frac{1}{k}})$. I would like to ask whether there is an explicit formula for the discriminant of $L$ only in terms of $k$, as the one that is given here for example.",,"['linear-algebra', 'number-theory', 'algebraic-number-theory', 'cyclotomic-fields']"
42,Conversion of symmetric matrix into skew-symmetric matrix by flipping signs of elements,Conversion of symmetric matrix into skew-symmetric matrix by flipping signs of elements,,"We are given an $n \times n$ matrix $A=(a_{ij})_{i,j\in\{1,2,\ldots,n\}}$ with the following properties : $A$ is symmetric, $A$ has a zero diagonal, every element of $A$ is a number in $\{0,1,2\}$, every row sum of $A$ is an odd number. We say that we flip the sign of an element of $A$ if we change the element from some $a_{ij}$ to $-a_{ij}$. Prove that it is always possible to perform a finite number of sign flips on $A$ to to obtain a new matrix $B=(b_{ij})_{i,j\in\{1,2,\ldots,n\}}$ such that $B$ is skew-symmetric, and each row sum of $B$ is either $1$ or $-1$.","We are given an $n \times n$ matrix $A=(a_{ij})_{i,j\in\{1,2,\ldots,n\}}$ with the following properties : $A$ is symmetric, $A$ has a zero diagonal, every element of $A$ is a number in $\{0,1,2\}$, every row sum of $A$ is an odd number. We say that we flip the sign of an element of $A$ if we change the element from some $a_{ij}$ to $-a_{ij}$. Prove that it is always possible to perform a finite number of sign flips on $A$ to to obtain a new matrix $B=(b_{ij})_{i,j\in\{1,2,\ldots,n\}}$ such that $B$ is skew-symmetric, and each row sum of $B$ is either $1$ or $-1$.",,"['linear-algebra', 'combinatorics', 'matrices']"
43,notation to avoid repetition in matrix expressions (e.g xx' and x'Ax),notation to avoid repetition in matrix expressions (e.g xx' and x'Ax),,"Is there a notation to express expressions such as $x^{T}Ax$, $xx^{T}$, $X^TAX$ with only one instance of $x$, or $X$ appearing in each case? This is to reduce repetition, which becomes more severe when in place of $x$ or $X$ is a longer expression. For $x^{T}Ax$, I have seen $||x||_{A}^2$, which I like, despite possibly being misleading in the cases where it is not a norm (if $A$ is not positive definite).","Is there a notation to express expressions such as $x^{T}Ax$, $xx^{T}$, $X^TAX$ with only one instance of $x$, or $X$ appearing in each case? This is to reduce repetition, which becomes more severe when in place of $x$ or $X$ is a longer expression. For $x^{T}Ax$, I have seen $||x||_{A}^2$, which I like, despite possibly being misleading in the cases where it is not a norm (if $A$ is not positive definite).",,"['linear-algebra', 'notation']"
44,An eigenvalue optimization problem,An eigenvalue optimization problem,,"Given a positive definite matrix $A \in \mathbb{R}^{n \times n}$, I would like to compute $$\max_{x \in \Delta_n} \lambda_{\rm min} \left( {\rm diag}(x) A \right)$$ where $\Delta_n$ is the simplex, i.e., $\Delta_n  = \{x \in \mathbb{R}^n ~|~ x_i \geq 0, \sum_i x_i = 1\}$ ${\rm diag}(x)$ is the standard operation which transforms the vector $x \in \mathbb{R}^n$ into a diagonal matrix by putting the elements of $x$ on the diagonal. $\lambda_{\rm min}(W)$ returns the smallest eigenvalue of the matrix $W$. In this case, since ${\rm diag}(x)A$ is conjugate to ${\rm    diag(x)}^{1/2} A {\rm diag(x)}^{1/2}$ and $A$ is positive definite, we have that ${\rm diag}(x)A$ has real eigenvalues, and it makes sense to talk about the smallest eigenvalue. I have a vague suspicion there might be an explicit formula for the solution of this problem, though I'm not sure. If that was the case, that would be great. If not, is there a polynomial-time algorithm which computes the optimal solution? Can this maybe be reduced to a semi-definite program? Something to go on is that $\lambda_{\rm min}(W)$ is a concave function of the symmetric matrix $W$. On the other hand, I haven't been able to translate this into concavity or convexity of $\lambda_{\rm min}({\rm diag}(x)A)$ as a function of $x$; the issue is that ${\rm diag}(x)A$ is not symmetric, so I'm not sure how useful this is.","Given a positive definite matrix $A \in \mathbb{R}^{n \times n}$, I would like to compute $$\max_{x \in \Delta_n} \lambda_{\rm min} \left( {\rm diag}(x) A \right)$$ where $\Delta_n$ is the simplex, i.e., $\Delta_n  = \{x \in \mathbb{R}^n ~|~ x_i \geq 0, \sum_i x_i = 1\}$ ${\rm diag}(x)$ is the standard operation which transforms the vector $x \in \mathbb{R}^n$ into a diagonal matrix by putting the elements of $x$ on the diagonal. $\lambda_{\rm min}(W)$ returns the smallest eigenvalue of the matrix $W$. In this case, since ${\rm diag}(x)A$ is conjugate to ${\rm    diag(x)}^{1/2} A {\rm diag(x)}^{1/2}$ and $A$ is positive definite, we have that ${\rm diag}(x)A$ has real eigenvalues, and it makes sense to talk about the smallest eigenvalue. I have a vague suspicion there might be an explicit formula for the solution of this problem, though I'm not sure. If that was the case, that would be great. If not, is there a polynomial-time algorithm which computes the optimal solution? Can this maybe be reduced to a semi-definite program? Something to go on is that $\lambda_{\rm min}(W)$ is a concave function of the symmetric matrix $W$. On the other hand, I haven't been able to translate this into concavity or convexity of $\lambda_{\rm min}({\rm diag}(x)A)$ as a function of $x$; the issue is that ${\rm diag}(x)A$ is not symmetric, so I'm not sure how useful this is.",,"['linear-algebra', 'matrices', 'optimization', 'algorithms', 'nonlinear-optimization']"
45,Operators and Tensor Products,Operators and Tensor Products,,"For clarity I've re-written this question. Let $V, \text{End} (V), V \otimes V, \text{End} (V\otimes V), \text{and } \text{End} (V) \otimes \text{End} (V)$ be a vector space, the space of linear operators on it, the tensor product of  $V$  with itself, the space of linear operators on $V \otimes V$, and the tensor product of the space of $\text{End}(V)$ with itself. There is no topology involved in  this question. Define a map $\phi: \text{End}(V) \times \text{End}(V) \to \text{End}(V \otimes V)$ by $\phi(S, T)(u \otimes v) = S(u) \otimes T(v)$. Then $\phi$ is bilinear: e.g. $\phi(S_1 + S_2, T)(u \otimes  v) = (S_1(u) + S_2(u)) \otimes T(v) = S_1(u) \otimes T(v) + S_2(u) \otimes T(v) = \phi(S_1 , T)(u \otimes v) + \phi(S_2, T)(u \otimes v)$ So by the universal property of the tensor product $\text{End}(V) \otimes \text{End}(V)$ there is a linear map $\overline \phi: \text{End}(V) \otimes \text{End}(V) \to \text{End}(V \otimes V)$ where for all $S, T \in \text{End}(V) $we have $\overline \phi (S \otimes T) (u \otimes v) = S(u) \otimes T(v)$ according to Operators on a Tensor Product Space and its answers, if $V$ is finite dimensional then $\overline \phi$ is a canonical isomorphism. Note that if $V$ has finite dimension $n$ then $Dim(\text{End}(V)) = n^2$; $Dim (\text{End}(V) \otimes \text{End}(V)) = n^ 4 = Dim(\text{End}(V \otimes V))$. So the spaces are isomorphic (but is $\overline \phi$ an isomorphism ?) If $\overline \phi$ is an isomorphism then it has an inverse, $\overline \phi^{-1}: \text{End}(V \otimes V) \to \text{End}(V) \otimes \text{End}(V)$ Consider the map $f: V \times V \to V \otimes V$ where $f(u, v) = v \otimes u$. This is also bilinear, e.g. $f(u, v_1 + v_2) = (v_1 + v_2) \otimes u = v_1 \otimes u + v_2 \otimes u = f(u, v_1) + f(u, v_2)$ By the universal property of $V \otimes V$ there is a linear map $\overline f \in \text{End}(V\otimes V)$ where for all $u, v \in V$ we have $\overline f(u \otimes v) = v \otimes u$. So, my question is if (V is finite dimensional and) $\overline \phi $ is an isomorphism, then what is $\overline \phi ^{-1}(\overline f)$ ? The answer has been provided below by @martini The original question wording is .... I may be mistaken in my understanding of what follows..... The answers to this question Operators on a Tensor Product Space say that  for finite dimensional spaces there is a canonical isomporphism between $\text{End}(V \otimes W)$ and $\text{End}(V) \otimes \text{End}(W)$ based on extending $\phi(S,T)(u\otimes v):=Su\otimes Tv.$ On the other hand I am reading http://www.physik.uni-leipzig.de/~schmidtm/qm/tepr.pdf which includes the following statement which appears to conflict with this Remark: There are many operators on V ⊗W which are not of the form A ⊗ B with A and B being operators on V and W, respectively. For example, if W = V , the mapping V ×V → V ⊗V defined by (v,w) 7→ (w, v) induces an operator on V ⊗V (check this) which is not of that form. This suggests (to me) that a mapping $\text{End}(V) \otimes \text{End}(W) \to \text{End}(V \otimes W)$ can't be surjective and so there can't be an isomorphism ? I would appreciate help with this.","For clarity I've re-written this question. Let $V, \text{End} (V), V \otimes V, \text{End} (V\otimes V), \text{and } \text{End} (V) \otimes \text{End} (V)$ be a vector space, the space of linear operators on it, the tensor product of  $V$  with itself, the space of linear operators on $V \otimes V$, and the tensor product of the space of $\text{End}(V)$ with itself. There is no topology involved in  this question. Define a map $\phi: \text{End}(V) \times \text{End}(V) \to \text{End}(V \otimes V)$ by $\phi(S, T)(u \otimes v) = S(u) \otimes T(v)$. Then $\phi$ is bilinear: e.g. $\phi(S_1 + S_2, T)(u \otimes  v) = (S_1(u) + S_2(u)) \otimes T(v) = S_1(u) \otimes T(v) + S_2(u) \otimes T(v) = \phi(S_1 , T)(u \otimes v) + \phi(S_2, T)(u \otimes v)$ So by the universal property of the tensor product $\text{End}(V) \otimes \text{End}(V)$ there is a linear map $\overline \phi: \text{End}(V) \otimes \text{End}(V) \to \text{End}(V \otimes V)$ where for all $S, T \in \text{End}(V) $we have $\overline \phi (S \otimes T) (u \otimes v) = S(u) \otimes T(v)$ according to Operators on a Tensor Product Space and its answers, if $V$ is finite dimensional then $\overline \phi$ is a canonical isomorphism. Note that if $V$ has finite dimension $n$ then $Dim(\text{End}(V)) = n^2$; $Dim (\text{End}(V) \otimes \text{End}(V)) = n^ 4 = Dim(\text{End}(V \otimes V))$. So the spaces are isomorphic (but is $\overline \phi$ an isomorphism ?) If $\overline \phi$ is an isomorphism then it has an inverse, $\overline \phi^{-1}: \text{End}(V \otimes V) \to \text{End}(V) \otimes \text{End}(V)$ Consider the map $f: V \times V \to V \otimes V$ where $f(u, v) = v \otimes u$. This is also bilinear, e.g. $f(u, v_1 + v_2) = (v_1 + v_2) \otimes u = v_1 \otimes u + v_2 \otimes u = f(u, v_1) + f(u, v_2)$ By the universal property of $V \otimes V$ there is a linear map $\overline f \in \text{End}(V\otimes V)$ where for all $u, v \in V$ we have $\overline f(u \otimes v) = v \otimes u$. So, my question is if (V is finite dimensional and) $\overline \phi $ is an isomorphism, then what is $\overline \phi ^{-1}(\overline f)$ ? The answer has been provided below by @martini The original question wording is .... I may be mistaken in my understanding of what follows..... The answers to this question Operators on a Tensor Product Space say that  for finite dimensional spaces there is a canonical isomporphism between $\text{End}(V \otimes W)$ and $\text{End}(V) \otimes \text{End}(W)$ based on extending $\phi(S,T)(u\otimes v):=Su\otimes Tv.$ On the other hand I am reading http://www.physik.uni-leipzig.de/~schmidtm/qm/tepr.pdf which includes the following statement which appears to conflict with this Remark: There are many operators on V ⊗W which are not of the form A ⊗ B with A and B being operators on V and W, respectively. For example, if W = V , the mapping V ×V → V ⊗V defined by (v,w) 7→ (w, v) induces an operator on V ⊗V (check this) which is not of that form. This suggests (to me) that a mapping $\text{End}(V) \otimes \text{End}(W) \to \text{End}(V \otimes W)$ can't be surjective and so there can't be an isomorphism ? I would appreciate help with this.",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'tensor-products']"
46,"Maximum determinant of a non-negative matrix, given the sum of all entries","Maximum determinant of a non-negative matrix, given the sum of all entries",,"Let $M$ be an $n\times n$ matrix such that all of its entries are non-negative and sum to $A$. Can we prove $\det(M)\leq \frac{A^n}{n^n}$? I am looking for cool solutions, please go for it. My boring solution: We proceed via induction, Let $A_j$ be the matrix obtained by removing the $n$th row and $j$th column.Let $b_1,b_2,\dots b_n$ be the determinants of these matrices, then the determinant of $M$ is $\sum\limits_{j=1}^n(-1)^{n+j}a_ib_i$, if $a_1+a_2+\dots+a_n=s$ and the maximum of the absolute values of $b_1,b_2,\dots b_n$ is $m$ then clearly this sum is less than or equal to $sm$. Notice that the sum of the terms inside the matrix corresponding to $m$ is at most $A-s$, so by the induction hypothesis this product is at most $\frac{(A-s)^{n-1}}{(n-1)^{n-1}}s$. It is easy to show with calculus that this is maximized when $s=\frac{A}{n}$.","Let $M$ be an $n\times n$ matrix such that all of its entries are non-negative and sum to $A$. Can we prove $\det(M)\leq \frac{A^n}{n^n}$? I am looking for cool solutions, please go for it. My boring solution: We proceed via induction, Let $A_j$ be the matrix obtained by removing the $n$th row and $j$th column.Let $b_1,b_2,\dots b_n$ be the determinants of these matrices, then the determinant of $M$ is $\sum\limits_{j=1}^n(-1)^{n+j}a_ib_i$, if $a_1+a_2+\dots+a_n=s$ and the maximum of the absolute values of $b_1,b_2,\dots b_n$ is $m$ then clearly this sum is less than or equal to $sm$. Notice that the sum of the terms inside the matrix corresponding to $m$ is at most $A-s$, so by the induction hypothesis this product is at most $\frac{(A-s)^{n-1}}{(n-1)^{n-1}}s$. It is easy to show with calculus that this is maximized when $s=\frac{A}{n}$.",,"['calculus', 'linear-algebra', 'optimization', 'determinant']"
47,"$Ax=b$ is solvable, then it has the same solutions of $A^TAx=A^Tb$","is solvable, then it has the same solutions of",Ax=b A^TAx=A^Tb,"I have to prove that given a matrix $A \in \mathbb{R}_{m\times n}$ and $b \in \mathbb{R}^n$. Suppose that the system $Ax=b$ is solvable, x is solution of $Ax=b$ If and only if x is solution of $A^TAx=A^Tb$. It's easy to show that if x is solution of $Ax=b$ them it is also solution if $A^tAx=A^Tb$, but I can't figure out why I have the other direction even if $A$ doesn't have full rank (that partícular case is easy to solve).","I have to prove that given a matrix $A \in \mathbb{R}_{m\times n}$ and $b \in \mathbb{R}^n$. Suppose that the system $Ax=b$ is solvable, x is solution of $Ax=b$ If and only if x is solution of $A^TAx=A^Tb$. It's easy to show that if x is solution of $Ax=b$ them it is also solution if $A^tAx=A^Tb$, but I can't figure out why I have the other direction even if $A$ doesn't have full rank (that partícular case is easy to solve).",,"['linear-algebra', 'systems-of-equations', 'least-squares']"
48,Relationship between properties of linear transformations algebraically and visually,Relationship between properties of linear transformations algebraically and visually,,"I learned from 3Blue1Brown's Linear Algebra videos that a 2-D transformation is linear if it follows these rules: lines remain lines without getting curved the origin remains fixed in place grid lines remain parallel and evenly spaced I'm now going through linear algebra from a textbook, which lays out this definition of a linear transformation: T(u+v) = T(u) + T(v) T(cu) = cT(u) I'm wondering, is there a connection between these two ways of thinking of linear transformations? Do the visual ways of seeing 2-D linear transformations correspond to the formal definition when in 2-D?","I learned from 3Blue1Brown's Linear Algebra videos that a 2-D transformation is linear if it follows these rules: lines remain lines without getting curved the origin remains fixed in place grid lines remain parallel and evenly spaced I'm now going through linear algebra from a textbook, which lays out this definition of a linear transformation: T(u+v) = T(u) + T(v) T(cu) = cT(u) I'm wondering, is there a connection between these two ways of thinking of linear transformations? Do the visual ways of seeing 2-D linear transformations correspond to the formal definition when in 2-D?",,['linear-algebra']
49,"Given the matrix A and vector X, what is the partial derivative of AX with respect to A?","Given the matrix A and vector X, what is the partial derivative of AX with respect to A?",,"If A has the dimension [m$\times$n] and X [n$\times$1], $\frac{\partial AX}{\partial A}$ is said to be $X^T$ (according to Section 2.3.1 here ). However, shouldn't $\frac{\partial AX}{\partial A}$ have the dimension [m$\times$m$\times$n] instead of [1$\times$n]? How is this result derived?","If A has the dimension [m$\times$n] and X [n$\times$1], $\frac{\partial AX}{\partial A}$ is said to be $X^T$ (according to Section 2.3.1 here ). However, shouldn't $\frac{\partial AX}{\partial A}$ have the dimension [m$\times$m$\times$n] instead of [1$\times$n]? How is this result derived?",,"['linear-algebra', 'partial-derivative', 'matrix-calculus']"
50,On solutions of a certain $n\times n$ linear system whose coefficients are all $\pm1$,On solutions of a certain  linear system whose coefficients are all,n\times n \pm1,"Let $n>2$ and $A \in M_n(\mathbb{R})$ be a $\{-1,1\}$-matrix (whose elements are $-1$ or $1$). Let $b\in\mathbb R^n$ contains the row-wise counts of minus ones in $A$ (i.e. $b_i$ is the number of minus ones in the $i$-th row of $A$). Suppose $Av = b$. Prove or disprove that if $Ax=0$ has only the trivial solution, then $v$ has at least two identical elements. For example, $$ A = \pmatrix{1 & 1 & 1 \\ -1 & 1 & 1\\ -1 & -1 & 1}, \ b=\pmatrix{0\\ 1\\ 2}, \ v=\pmatrix{-\frac12\\ -\frac12\\ 1}. $$ I couldn't find a counterexample, so for the moment I'm assuming it is indeed true. Perhaps the more pertinent issue I'm having is not knowing where to start, given the peculiar structure of the matrix and of $b$. I considered proving the contrapositive (if $v_1 = \ ...\  = v_n$, then there exists some nonzero/nontrivial solution to $Ax = 0$), but I didn't get far with that. I appreciate all help, even if it's just a nudge in the right direction (or if anyone finds a counterexample). Thank you kindly! Edit: In addition, I have not found a counterexample for the case where each $a_{ij}$ is either $-\delta$ or $\delta$ for $\delta \in \mathbb{R}$, so if the initial statement is indeed true, then something that would point to this fact would be optimal (or at the very least interesting). Thanks again!","Let $n>2$ and $A \in M_n(\mathbb{R})$ be a $\{-1,1\}$-matrix (whose elements are $-1$ or $1$). Let $b\in\mathbb R^n$ contains the row-wise counts of minus ones in $A$ (i.e. $b_i$ is the number of minus ones in the $i$-th row of $A$). Suppose $Av = b$. Prove or disprove that if $Ax=0$ has only the trivial solution, then $v$ has at least two identical elements. For example, $$ A = \pmatrix{1 & 1 & 1 \\ -1 & 1 & 1\\ -1 & -1 & 1}, \ b=\pmatrix{0\\ 1\\ 2}, \ v=\pmatrix{-\frac12\\ -\frac12\\ 1}. $$ I couldn't find a counterexample, so for the moment I'm assuming it is indeed true. Perhaps the more pertinent issue I'm having is not knowing where to start, given the peculiar structure of the matrix and of $b$. I considered proving the contrapositive (if $v_1 = \ ...\  = v_n$, then there exists some nonzero/nontrivial solution to $Ax = 0$), but I didn't get far with that. I appreciate all help, even if it's just a nudge in the right direction (or if anyone finds a counterexample). Thank you kindly! Edit: In addition, I have not found a counterexample for the case where each $a_{ij}$ is either $-\delta$ or $\delta$ for $\delta \in \mathbb{R}$, so if the initial statement is indeed true, then something that would point to this fact would be optimal (or at the very least interesting). Thanks again!",,"['linear-algebra', 'matrices', 'systems-of-equations']"
51,When is an outer product $ab^\top$ diagonalisable? [duplicate],When is an outer product  diagonalisable? [duplicate],ab^\top,"This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 7 years ago . If $a$ and $b$ are $n\times 1$ vectors and $A=ab^\top$, why is A diagonalizable if and only if $a^\top b \ne 0$ ? From my understanding, $a^\top b$ is the sum of the diagonal of A. How does this relate to diagonalizability?","This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 7 years ago . If $a$ and $b$ are $n\times 1$ vectors and $A=ab^\top$, why is A diagonalizable if and only if $a^\top b \ne 0$ ? From my understanding, $a^\top b$ is the sum of the diagonal of A. How does this relate to diagonalizability?",,"['linear-algebra', 'matrices', 'diagonalization']"
52,Does there exist an infinite dimensional real normed linear space which can be written as a countable union of one dimensional subspaces?,Does there exist an infinite dimensional real normed linear space which can be written as a countable union of one dimensional subspaces?,,"Does there exist an infinite dimensional real NLS which can be written as a countable union of one dimensional subspaces ? I can only tell that if there is a NLS such that it is possible then it cannot be a Banach space by Baire category theorem ( as any one dimensional subspace is closed and being a proper subspace , has empty interior ) . Please help . Thanks in advance","Does there exist an infinite dimensional real NLS which can be written as a countable union of one dimensional subspaces ? I can only tell that if there is a NLS such that it is possible then it cannot be a Banach space by Baire category theorem ( as any one dimensional subspace is closed and being a proper subspace , has empty interior ) . Please help . Thanks in advance",,['linear-algebra']
53,Submodule maximal if and only if quotient module simple?,Submodule maximal if and only if quotient module simple?,,"Why is the case that a submodule $L$ of a module $L$ is maximal, among submodules of $M$ distinct from $M$, if and only if the quotient module $M/L$ is simple?","Why is the case that a submodule $L$ of a module $L$ is maximal, among submodules of $M$ distinct from $M$, if and only if the quotient module $M/L$ is simple?",,['linear-algebra']
54,How to find the transition matrix for ordered basis of 2x2 diagonal matrices,How to find the transition matrix for ordered basis of 2x2 diagonal matrices,,"The problem: For the vector space of lower triangular matrices with zero trace, given ordered basis: $B=$ { $$         \begin{bmatrix}         -5 & 0 \\         4 & 5 \\         \end{bmatrix}, $$ \begin{bmatrix}     -1 & 0 \\     1 & 1 \\     \end{bmatrix} } and $C=$ { $$     \begin{bmatrix}     -5 & 0 \\     -4 & 5 \\     \end{bmatrix}, $$ \begin{bmatrix}     -1 & 0 \\     5 & 1 \\     \end{bmatrix} } find the transition matrix $C$ to $B$ . I know how to find a transition matrix when the basis consists of $n \times 1 $ vectors, but my textbook doesn't address this scenario where the basis consists of a set of $2 \times 2$ matrices and haven't found applicable guidance online.","The problem: For the vector space of lower triangular matrices with zero trace, given ordered basis: { } and { } find the transition matrix to . I know how to find a transition matrix when the basis consists of vectors, but my textbook doesn't address this scenario where the basis consists of a set of matrices and haven't found applicable guidance online.","B= 
        \begin{bmatrix}
        -5 & 0 \\
        4 & 5 \\
        \end{bmatrix},
 \begin{bmatrix}
    -1 & 0 \\
    1 & 1 \\
    \end{bmatrix} C= 
    \begin{bmatrix}
    -5 & 0 \\
    -4 & 5 \\
    \end{bmatrix},
 \begin{bmatrix}
    -1 & 0 \\
    5 & 1 \\
    \end{bmatrix} C B n \times 1  2 \times 2","['linear-algebra', 'matrices', 'change-of-basis']"
55,Does $c\cdot f(x) = f(cx) \implies f$ is linear? [duplicate],Does  is linear? [duplicate],c\cdot f(x) = f(cx) \implies f,"This question already has answers here : Are all multiplicative functions additive? (2 answers) Closed 7 years ago . A linear functions $f$ is a function that has the following two properties: $c\cdot f(x) = f(cx)$ $f(x+y) = f(x) + f(y)$ But does $c\cdot f(x) = f(cx) \implies f$ is linear? Can we assume property $2$ above, if we have a function $f$ satisfying property $1$?","This question already has answers here : Are all multiplicative functions additive? (2 answers) Closed 7 years ago . A linear functions $f$ is a function that has the following two properties: $c\cdot f(x) = f(cx)$ $f(x+y) = f(x) + f(y)$ But does $c\cdot f(x) = f(cx) \implies f$ is linear? Can we assume property $2$ above, if we have a function $f$ satisfying property $1$?",,"['linear-algebra', 'functions']"
56,Using 8x8 Binary Matrices as a hash,Using 8x8 Binary Matrices as a hash,,"I had the idea of computing a 64 bit hash of a text string by assigning a unique binary 8x8 matrix to each character, and computing the hashes of larger strings by multiplying the matrices corresponding to the substrings.  In this system both addition and multiplication of matrix elements would be modulo 2.  If this works, it would have the benefit that the hash of two concatenated strings would be the product of their hashes. This method would completely fall apart if some matrices were much more likely to turn up than others, if a large subset of the space of matrices were unusable, or if there were a risk of generating a degenerate matrix from the product of two non-degenerate matrices, causing the whole product to collapse.  As a result, I have a few questions. 1) What percentage of the space of 64 bit values are usable as hashes, i.e. the matrix they represent does not have determinant zero? 2) Given two matrices chosen randomly from the set of usable hashes, is their product equally likely to generate any other matrix in the usable set, or are some products more likely than others? 3) Will the determinant of the product of two matrices with non-zero determinants ever be zero? EDIT: My initial idea was to turn each 8 bit character with bits $abcdefgh$ into the following matrix: $\begin{bmatrix} a \oplus h & a \oplus g & a \oplus f & a \oplus e & a \oplus d & a \oplus c & a \oplus b & 1 \\ b \oplus h & b \oplus g & b \oplus f & b \oplus e & b \oplus d & b \oplus c & 1 & 0 \\ c \oplus h & c \oplus g & c \oplus f & c \oplus e & c \oplus d & 1 & 0 & 0 \\ d \oplus h & d \oplus g & d \oplus f & d \oplus e & 1 & 0 & 0 & 0 \\ e \oplus h & e \oplus g & e \oplus f & 1 & 0 & 0 & 0 & 0 \\ f \oplus h & f \oplus g & 1 & 0 & 0 & 0 & 0 & 0 \\ g \oplus h & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}$","I had the idea of computing a 64 bit hash of a text string by assigning a unique binary 8x8 matrix to each character, and computing the hashes of larger strings by multiplying the matrices corresponding to the substrings.  In this system both addition and multiplication of matrix elements would be modulo 2.  If this works, it would have the benefit that the hash of two concatenated strings would be the product of their hashes. This method would completely fall apart if some matrices were much more likely to turn up than others, if a large subset of the space of matrices were unusable, or if there were a risk of generating a degenerate matrix from the product of two non-degenerate matrices, causing the whole product to collapse.  As a result, I have a few questions. 1) What percentage of the space of 64 bit values are usable as hashes, i.e. the matrix they represent does not have determinant zero? 2) Given two matrices chosen randomly from the set of usable hashes, is their product equally likely to generate any other matrix in the usable set, or are some products more likely than others? 3) Will the determinant of the product of two matrices with non-zero determinants ever be zero? EDIT: My initial idea was to turn each 8 bit character with bits $abcdefgh$ into the following matrix: $\begin{bmatrix} a \oplus h & a \oplus g & a \oplus f & a \oplus e & a \oplus d & a \oplus c & a \oplus b & 1 \\ b \oplus h & b \oplus g & b \oplus f & b \oplus e & b \oplus d & b \oplus c & 1 & 0 \\ c \oplus h & c \oplus g & c \oplus f & c \oplus e & c \oplus d & 1 & 0 & 0 \\ d \oplus h & d \oplus g & d \oplus f & d \oplus e & 1 & 0 & 0 & 0 \\ e \oplus h & e \oplus g & e \oplus f & 1 & 0 & 0 & 0 & 0 \\ f \oplus h & f \oplus g & 1 & 0 & 0 & 0 & 0 & 0 \\ g \oplus h & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}$",,"['linear-algebra', 'probability', 'binary']"
57,how to express the result of triple cross product of two vectors in spherical coordinates (unit vector),how to express the result of triple cross product of two vectors in spherical coordinates (unit vector),,"I have a system of differential equations like below: (real problem is more complex, here is just an example) $$ \frac{\mathrm{d}\textbf{M}_1}{\mathrm{d}t}=\frac{A}{M_{1}M_{2}}[\textbf{M}_1\times(\textbf{M}_1\times\textbf{M}_2)]+\frac{B}{M_1}(\textbf{M}_1\times\textbf{M}_2) \\ \frac{\mathrm{d}\textbf{M}_2}{\mathrm{d}t}=\frac{C}{M_{2}M_{1}}[\textbf{M}_2\times(\textbf{M}_2\times\textbf{M}_1)]+\frac{D}{M_2}(\textbf{M}_2\times\textbf{M}_1) $$ A, B, C, and D are just constants. $\textbf{M}_1$ and $\textbf{M}_2$ are two vectors and their derivative over time (t) has such relation. Since the magnitude of $\textbf{M}_1$ and $\textbf{M}_2$ is unchanged over time, and I am only interested in the change of $\theta$ and $\phi$ of both vectors, so I want to express the equations in spherical coordinates and use unit vectors of $\theta$ and $\phi$. Thus, the equations can be separated to 4 equations, each of which deals with only one parameter: $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$. (In my case, $\theta_{1,2}$ is the angle between $\textbf{M}_{1,2}$ and +z direction, ${\phi}_{1,2}$ is the angle of $\textbf{M}_{1,2}$ with +x direction on x-y plane.) The left side of the equations can thus be re-written as: $$ \frac{\mathrm{d}\textbf{M}_1}{\mathrm{d}t}=M_1(\frac{\mathrm{d}\theta_1}{\mathrm{d}t}\hat{\theta}_1+\frac{\mathrm{d}\phi_1}{\mathrm{d}t}\sin\theta_1\hat{\phi}_1) \\ \frac{\mathrm{d}\textbf{M}_2}{\mathrm{d}t}=M_2(\frac{\mathrm{d}\theta_2}{\mathrm{d}t}\hat{\theta}_2+\frac{\mathrm{d}\phi_2}{\mathrm{d}t}\sin\theta_2\hat{\phi}_2) $$ But I can’t express the right side of the equations to similar expression with $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$. So my question is, can we express the items on the right side as similar as the left side of the differential equations? I’ve tried this equation: $\textbf{a}\times(\textbf{b}\times\textbf{c})=\textbf{b}(\textbf{a}\cdot\textbf{c})-\textbf{c}(\textbf{a}\cdot{\textbf{b}})$ for that triple cross product. But it results in an equation with $\textbf{M}_1$ and $\textbf{M}_2$, and seems not to be expressed by $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$ further. For $\textbf{M}_1\times\textbf{M}_2$, I have no clue... Thanks a lot for your help! Best regards,","I have a system of differential equations like below: (real problem is more complex, here is just an example) $$ \frac{\mathrm{d}\textbf{M}_1}{\mathrm{d}t}=\frac{A}{M_{1}M_{2}}[\textbf{M}_1\times(\textbf{M}_1\times\textbf{M}_2)]+\frac{B}{M_1}(\textbf{M}_1\times\textbf{M}_2) \\ \frac{\mathrm{d}\textbf{M}_2}{\mathrm{d}t}=\frac{C}{M_{2}M_{1}}[\textbf{M}_2\times(\textbf{M}_2\times\textbf{M}_1)]+\frac{D}{M_2}(\textbf{M}_2\times\textbf{M}_1) $$ A, B, C, and D are just constants. $\textbf{M}_1$ and $\textbf{M}_2$ are two vectors and their derivative over time (t) has such relation. Since the magnitude of $\textbf{M}_1$ and $\textbf{M}_2$ is unchanged over time, and I am only interested in the change of $\theta$ and $\phi$ of both vectors, so I want to express the equations in spherical coordinates and use unit vectors of $\theta$ and $\phi$. Thus, the equations can be separated to 4 equations, each of which deals with only one parameter: $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$. (In my case, $\theta_{1,2}$ is the angle between $\textbf{M}_{1,2}$ and +z direction, ${\phi}_{1,2}$ is the angle of $\textbf{M}_{1,2}$ with +x direction on x-y plane.) The left side of the equations can thus be re-written as: $$ \frac{\mathrm{d}\textbf{M}_1}{\mathrm{d}t}=M_1(\frac{\mathrm{d}\theta_1}{\mathrm{d}t}\hat{\theta}_1+\frac{\mathrm{d}\phi_1}{\mathrm{d}t}\sin\theta_1\hat{\phi}_1) \\ \frac{\mathrm{d}\textbf{M}_2}{\mathrm{d}t}=M_2(\frac{\mathrm{d}\theta_2}{\mathrm{d}t}\hat{\theta}_2+\frac{\mathrm{d}\phi_2}{\mathrm{d}t}\sin\theta_2\hat{\phi}_2) $$ But I can’t express the right side of the equations to similar expression with $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$. So my question is, can we express the items on the right side as similar as the left side of the differential equations? I’ve tried this equation: $\textbf{a}\times(\textbf{b}\times\textbf{c})=\textbf{b}(\textbf{a}\cdot\textbf{c})-\textbf{c}(\textbf{a}\cdot{\textbf{b}})$ for that triple cross product. But it results in an equation with $\textbf{M}_1$ and $\textbf{M}_2$, and seems not to be expressed by $\hat{\theta}_1$, $\hat{\phi}_1$, $\hat{\theta}_2$ and $\hat{\phi}_2$ further. For $\textbf{M}_1\times\textbf{M}_2$, I have no clue... Thanks a lot for your help! Best regards,",,['linear-algebra']
58,Geometric interpretation of positive semi-definiteness of sum of matrices,Geometric interpretation of positive semi-definiteness of sum of matrices,,"I am trying to solve some convex optimization problems. $$ C = \{x \in R^n \mid x^TAx + b^Tx + c \leq 0\} $$ where $A\in S^n, b\in R^n$ and $c\in R$ Question is to show that the intersection of $C$ and the hyperplane defined by $g^Tx + h = 0$ with $g\neq 0$ is convex iff $A + \lambda gg^T \succeq 0$ for some $\lambda \in R$ I am having a hard time to geometrically interpret that. Like how $A + \lambda gg^T$ can become positive semi-definite. I have solution to it here (Page # 8 Exercise 2.10 b) I am thinking it like using toy example in $R^2$ let  $$A = \left[ \begin{array}{ccc} -1 & 0 \\ 0 & -1  \end{array} \right], g = \left[ \begin{array}{ccc}  2 & 2 \end{array}  \right]^T$$ Though I did some basic sum of  $A + \lambda gg^T$ and it makes it positive semi-definite (psd) but I want to understand geometric intuition behind it. Also I understood some part of solution where it is taking intersection of $C$ and $H$, but then I have no clue how he shifted it to sum of matrices. One thing I know from previous exercise that if a matrix is psd then set generated by its quadratic inequality would be convex. It would be great help if: 1) Someone could help me in understanding how summation of two matrices which are not necessarily psd, can make it psd? 2) ""How"" the information from eigendecomposition of $A$ can be used here? 3) Also I noticed that other matrix has linearly dependent rows so is it necessary that it should be linearly dependent or the other matrix could make the sum psd. P.S. This is not a homework. This is my understanding to this question that they want us to know that sum of matrices can make the matrix psd and the set generated by it, convex. Other intuition behind this exercise would be great help as well. Thanks.","I am trying to solve some convex optimization problems. $$ C = \{x \in R^n \mid x^TAx + b^Tx + c \leq 0\} $$ where $A\in S^n, b\in R^n$ and $c\in R$ Question is to show that the intersection of $C$ and the hyperplane defined by $g^Tx + h = 0$ with $g\neq 0$ is convex iff $A + \lambda gg^T \succeq 0$ for some $\lambda \in R$ I am having a hard time to geometrically interpret that. Like how $A + \lambda gg^T$ can become positive semi-definite. I have solution to it here (Page # 8 Exercise 2.10 b) I am thinking it like using toy example in $R^2$ let  $$A = \left[ \begin{array}{ccc} -1 & 0 \\ 0 & -1  \end{array} \right], g = \left[ \begin{array}{ccc}  2 & 2 \end{array}  \right]^T$$ Though I did some basic sum of  $A + \lambda gg^T$ and it makes it positive semi-definite (psd) but I want to understand geometric intuition behind it. Also I understood some part of solution where it is taking intersection of $C$ and $H$, but then I have no clue how he shifted it to sum of matrices. One thing I know from previous exercise that if a matrix is psd then set generated by its quadratic inequality would be convex. It would be great help if: 1) Someone could help me in understanding how summation of two matrices which are not necessarily psd, can make it psd? 2) ""How"" the information from eigendecomposition of $A$ can be used here? 3) Also I noticed that other matrix has linearly dependent rows so is it necessary that it should be linearly dependent or the other matrix could make the sum psd. P.S. This is not a homework. This is my understanding to this question that they want us to know that sum of matrices can make the matrix psd and the set generated by it, convex. Other intuition behind this exercise would be great help as well. Thanks.",,"['linear-algebra', 'convex-analysis', 'convex-optimization', 'positive-semidefinite']"
59,The expected distortion of a linear transformation (continued),The expected distortion of a linear transformation (continued),,"Let $A: \mathbb{R}^n \to \mathbb{R}^n$ be a linear transformation. I am interested in the ""average distortion"" caused by the action of $A$ on vectors. Consider the uniform distribution on $\mathbb{S}^{n-1}$, and the random variable $X:\mathbb{S}^{n-1} \to \mathbb{R}$ defined by $X(x)=\|A(x)\|_2$. Question: What is the expectation of $X$? (Is there a closed formula?) Using SVD, the problem reduces to $A$ being a diagonal matrix with non-negative entries. So, the question amounts to calculating $$\int_{\mathbb{S}^{n-1}} \sqrt{\sum_{i=1}^n (\sigma_ix_i)^2} $$ (and dividing by the volume of $\mathbb{S}^{n-1}$). This question is related to these two , which ask about the expected distortion of the square of the norm (which is easier, since no square roots are involved). For the above problems, a successful approach was to use standard normal variables, in order to generate a unit random vector (see here ). However, it does not seem to help in this case.","Let $A: \mathbb{R}^n \to \mathbb{R}^n$ be a linear transformation. I am interested in the ""average distortion"" caused by the action of $A$ on vectors. Consider the uniform distribution on $\mathbb{S}^{n-1}$, and the random variable $X:\mathbb{S}^{n-1} \to \mathbb{R}$ defined by $X(x)=\|A(x)\|_2$. Question: What is the expectation of $X$? (Is there a closed formula?) Using SVD, the problem reduces to $A$ being a diagonal matrix with non-negative entries. So, the question amounts to calculating $$\int_{\mathbb{S}^{n-1}} \sqrt{\sum_{i=1}^n (\sigma_ix_i)^2} $$ (and dividing by the volume of $\mathbb{S}^{n-1}$). This question is related to these two , which ask about the expected distortion of the square of the norm (which is easier, since no square roots are involved). For the above problems, a successful approach was to use standard normal variables, in order to generate a unit random vector (see here ). However, it does not seem to help in this case.",,"['linear-algebra', 'probability', 'matrices', 'random-variables']"
60,Prove a matrix expression leads to an invertible matrix?,Prove a matrix expression leads to an invertible matrix?,,"I want to prove matrix $C$ is invertible: $$C=I-A^TB(B^TB)^{-1}B^TA(A^TA)^{-1},$$ where $I$ is an identity matrix of appropriate dimensions, and $(A^TA)^{-1}$ and $(B^TB)^{-1}$ imply both $A$ and $B$ have linearly independent columns. I tried to achieve my goal by proving $\det(C)\neq0$ but got stuck.","I want to prove matrix $C$ is invertible: $$C=I-A^TB(B^TB)^{-1}B^TA(A^TA)^{-1},$$ where $I$ is an identity matrix of appropriate dimensions, and $(A^TA)^{-1}$ and $(B^TB)^{-1}$ imply both $A$ and $B$ have linearly independent columns. I tried to achieve my goal by proving $\det(C)\neq0$ but got stuck.",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
61,Creating a tight frame of $\mathbb{R}^{n}$ when already knowing some of its vectors.,Creating a tight frame of  when already knowing some of its vectors.,\mathbb{R}^{n},"I'm wondering whether or not there's an optimal way for adding rows to a given matrix $S\in\mathbb{R}^{m\times mn}$, $m\leq n$, so that the columns of the resulting matrix form an orthogonal system of vectors of equal norm. I've been struggling with this problem for quite some time now and the reason is that I don't want to change my starting matrix $S$. This is equivalent to creating a tight frame of $\mathbb{R}^{mn+v}$, $v\geq 0$  when you already know some of its vectors. Up to this point, I have succeeded in creating a tight frame given any number of its vectors but I don't know how to minimize $v$.","I'm wondering whether or not there's an optimal way for adding rows to a given matrix $S\in\mathbb{R}^{m\times mn}$, $m\leq n$, so that the columns of the resulting matrix form an orthogonal system of vectors of equal norm. I've been struggling with this problem for quite some time now and the reason is that I don't want to change my starting matrix $S$. This is equivalent to creating a tight frame of $\mathbb{R}^{mn+v}$, $v\geq 0$  when you already know some of its vectors. Up to this point, I have succeeded in creating a tight frame given any number of its vectors but I don't know how to minimize $v$.",,"['linear-algebra', 'matrices']"
62,Show that the map is onto,Show that the map is onto,,"Suppose $R=\{A\in GL(n,\mathbb C):A^{-1}=\bar A\}$, where $\bar A$ is the conjugate matrix of $A$. Show that $$b:GL(n,\mathbb C)\to R,\quad A\mapsto A\bar{A}^{-1}$$ is onto. I don't know where to start. Is it possible to construct directly for any $M\in R$ a matrix $A$ such that $b(A)=M$? (The problem comes from a description of totally real subspaces of $\mathbb C^n$.)","Suppose $R=\{A\in GL(n,\mathbb C):A^{-1}=\bar A\}$, where $\bar A$ is the conjugate matrix of $A$. Show that $$b:GL(n,\mathbb C)\to R,\quad A\mapsto A\bar{A}^{-1}$$ is onto. I don't know where to start. Is it possible to construct directly for any $M\in R$ a matrix $A$ such that $b(A)=M$? (The problem comes from a description of totally real subspaces of $\mathbb C^n$.)",,"['linear-algebra', 'matrices']"
63,Find the value of special tridiagonal determinant,Find the value of special tridiagonal determinant,,"Let $A_{n}$ be the following  tridiagonal determinant  of order $n:$ \begin{vmatrix}  a_{0}+a_{1}&  a_{1}&  0&  0& \cdots&  0& \quad0\\   a_{1}&  a_{1}+a_{2}&  a_{2}&  0&  \cdots&  0& \quad0\\   0&  a_{2}&  a_{2}+a_{3}&  a_{3}&  \cdots&  0& \quad0\\   \vdots&  \vdots&  \vdots&  \vdots&  &  \vdots& \quad\vdots\\   0&  0&  0&  0&  \cdots&  a_{n-1}&   \quad a_{n-1}+a_{n} \end{vmatrix} Find the value of $A_{n}.$ As we know,$$A_{n}=(a_{n-1}+a_{n})A_{n-1}-a^{2}_{n-1}A_{n-2}\Longrightarrow$$ $$\begin{bmatrix}  A_{n}\\  A_{n-1} \end{bmatrix}=\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  A_{n-1}\\  A_{n-2} \end{bmatrix}=$$$$\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  a_{n-2}+a_{n-1}& -a^{2}_{n-2}\\  1& 0 \end{bmatrix}\cdots\begin{bmatrix}  a_{2}+a_{3}& -a^{2}_{2}\\  1& 0 \end{bmatrix}\begin{bmatrix}  A_{2}\\  A_{1} \end{bmatrix}.$$ But it is  not easy to deal with  $$\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  a_{n-2}+a_{n-1}& -a^{2}_{n-2}\\  1& 0 \end{bmatrix}\cdots\begin{bmatrix}  a_{2}+a_{3}& -a^{2}_{2}\\  1& 0 \end{bmatrix}$$","Let $A_{n}$ be the following  tridiagonal determinant  of order $n:$ \begin{vmatrix}  a_{0}+a_{1}&  a_{1}&  0&  0& \cdots&  0& \quad0\\   a_{1}&  a_{1}+a_{2}&  a_{2}&  0&  \cdots&  0& \quad0\\   0&  a_{2}&  a_{2}+a_{3}&  a_{3}&  \cdots&  0& \quad0\\   \vdots&  \vdots&  \vdots&  \vdots&  &  \vdots& \quad\vdots\\   0&  0&  0&  0&  \cdots&  a_{n-1}&   \quad a_{n-1}+a_{n} \end{vmatrix} Find the value of $A_{n}.$ As we know,$$A_{n}=(a_{n-1}+a_{n})A_{n-1}-a^{2}_{n-1}A_{n-2}\Longrightarrow$$ $$\begin{bmatrix}  A_{n}\\  A_{n-1} \end{bmatrix}=\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  A_{n-1}\\  A_{n-2} \end{bmatrix}=$$$$\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  a_{n-2}+a_{n-1}& -a^{2}_{n-2}\\  1& 0 \end{bmatrix}\cdots\begin{bmatrix}  a_{2}+a_{3}& -a^{2}_{2}\\  1& 0 \end{bmatrix}\begin{bmatrix}  A_{2}\\  A_{1} \end{bmatrix}.$$ But it is  not easy to deal with  $$\begin{bmatrix}  a_{n-1}+a_{n}& -a^{2}_{n-1}\\  1& 0 \end{bmatrix}\begin{bmatrix}  a_{n-2}+a_{n-1}& -a^{2}_{n-2}\\  1& 0 \end{bmatrix}\cdots\begin{bmatrix}  a_{2}+a_{3}& -a^{2}_{2}\\  1& 0 \end{bmatrix}$$",,['linear-algebra']
64,How to compute $\det(A+J)$?,How to compute ?,\det(A+J),If $A$  be an $n\times n$ matrix and $J$ be a matrix of same order with all entries $1$ then Show that $\det( A + J)=\det A$ + sum of all cofactors of $A$. I have tried using Laplace Expansion but I am not getting it. Please give some hints at doing the same.,If $A$  be an $n\times n$ matrix and $J$ be a matrix of same order with all entries $1$ then Show that $\det( A + J)=\det A$ + sum of all cofactors of $A$. I have tried using Laplace Expansion but I am not getting it. Please give some hints at doing the same.,,"['linear-algebra', 'matrices', 'determinant']"
65,Numerical Calculation of Eigenvalues of a large real Symmetric tridiagonal matrix,Numerical Calculation of Eigenvalues of a large real Symmetric tridiagonal matrix,,"If I have an $N \times N$ matrix where every entry is zero except for along the super-diagonal and sub-diagonal, where the each entry is the conjugate of the last, like the following $5 \times 5$ matrix $$\begin{bmatrix}0 & (1-t) & 0 & 0 & 0 \\(1-t) & 0 & (1+t) & 0 & 0 \\0 & (1+t) & 0 & (1-t) & 0 \\0 & 0 & (1-t) & 0 & (1+t)\\0 & 0 & 0 & (1+t) & 0 \end{bmatrix}$$ where $t$ is some parameter. Imagine a much larger matrix following the same pattern. Is there any way, numerical or otherwise to find the eigenvalues?","If I have an $N \times N$ matrix where every entry is zero except for along the super-diagonal and sub-diagonal, where the each entry is the conjugate of the last, like the following $5 \times 5$ matrix $$\begin{bmatrix}0 & (1-t) & 0 & 0 & 0 \\(1-t) & 0 & (1+t) & 0 & 0 \\0 & (1+t) & 0 & (1-t) & 0 \\0 & 0 & (1-t) & 0 & (1+t)\\0 & 0 & 0 & (1+t) & 0 \end{bmatrix}$$ where $t$ is some parameter. Imagine a much larger matrix following the same pattern. Is there any way, numerical or otherwise to find the eigenvalues?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
66,"Weyl group, bilinear form, and character/cocharacter pairing. Many questions!","Weyl group, bilinear form, and character/cocharacter pairing. Many questions!",,"Let $G$ be a connected linear algebraic group, $T$ a maximal torus of $G$, and $\alpha$ a weight of $T$ such that $G_{\alpha} = Z_G(S)$ is not solvable, where $S = (\textrm{Ker } \alpha)^0$.  I have been really confused about a few things concerning the Weyl group $W(G,T) = N_G(T)/Z_G(T)$, which injects into the group of automorphisms of the group of characters of $T$, as $(nZ_G(T) \cdot \chi)(t) = \chi(ntn^{-1})$. I had asked a question earlier ( Why is $s_{\alpha}$ a Euclidean reflection? ) and user yisishoujo had given me a detailed answer.  Unfortunately I was not able to understand all of his answer, although most of it was very helpful.  I feel like there is something very subtle about this topic which everyone else realizes but that I am not aware of.  So I want to break this question into several questions in the hope someone can point out to me what it is I'm missing. Notation: $X, Y$ are the groups of characters/cocharacters of $T$. $V = \mathbb{R} \otimes_{\mathbb{Z}} X$ $V ^\wedge{} = \mathbb{R} \otimes_{\mathbb{Z}} Y$. We can identify $X$ as an additive subgroup of $V$ as $\chi \mapsto 1 \otimes \chi$.  Similarly for $Y$ and $V^{\wedge}$. Relevant facts: 1 . There is a canonical pairing $\langle -, -\rangle:X \times Y \rightarrow \mathbb{Z}$, where for $\chi \in X, \gamma \in Y$, $\langle \chi, \gamma \rangle$ is the unique integer for which $\chi \circ \gamma(x) = x^{\langle \chi, \gamma \rangle}$ for all $x \in k^{\ast}$. 2 . If $\gamma \in Y$, $\gamma$ induces an abelian group homomorphism of $X$ into $\mathbb{Z}$ by $\chi \mapsto \langle \chi, \gamma \rangle$.  The assignment $\gamma \mapsto \langle -, \gamma \rangle$ is an isomorphism of abelian groups $Y \rightarrow \textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$.  We identify $Y$ with $\textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$. 3 . Identifying $Y$ as above, there is a canonical isomorphism of real vector spaces from $V^{\wedge}$ onto the dual of $V$, where to any generator $\lambda \otimes \gamma$, we associate the map $V \rightarrow \mathbb{R}$ defined on generators by $r \otimes \chi \mapsto r \lambda \langle \chi, \gamma \rangle$. 4 . Any abelian group automorphism of $X$ extends to a linear isomorphism of $V$, the same for $Y$ and $V^{\wedge}$.  The subgroup of the Weyl group, $W_{\alpha} := Z_G(S) \cap N_G(T)/Z_G(T)$, can be shown to have order exactly two.  Choosing an $n \in Z_G(S) \cap N_G(T)$ which is not in $Z_G(T)$, we obtain an automorphism $s$ of $V$ which has order exactly $2$. 5 .  The Weyl group also injects into the group of automorphisms of $Y$, as $(nZ_G(T) \cdot \gamma)(x) = n^{-1}\gamma(x)n$.  Under this action, we have $\langle \chi, \gamma \rangle = \langle w \cdot \chi, w \cdot \gamma \rangle$ for any $w$ in the Weyl group, $\chi \in X, \gamma \in Y$.  The actions of the Weyl group on $X, Y$ extend respectively to actions on $V, V^{\wedge}$.  The pairing $\langle -, - \rangle$ extends to a Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ Questions: 1 . Why does $s(\alpha) \neq \alpha$? I know it is not the case that $s(\chi) = \chi$ for all characters $\chi$, but it is not clear what happens to $\alpha$.  If I can show that $s(\alpha) \neq \alpha$, yisishoujo explained why we must have $s(\alpha) =- \alpha$.  Indeed, conjugation by $n$ (the $n$ as in Fact 4) induces an automorphism of algebraic groups $T/S$.  But $T/S$ is isomorphic to $k^{\ast}$, so there are only two such automorphisms, the identity map and the inverse map.  If $\textrm{Int } n$ induced the identity, then we would have $ntn^{-1}S = tS$ for all $t \in T$, i.e. $\alpha(ntn^{-1}) = \alpha(t)$ for all $t$, which implies $s \alpha = \alpha$, contradiction.  It follows that $ntn^{-1}S = t^{-1}S$ for all $t$, whence $s(\alpha) = - \alpha$. 2 .  Take $s, n$ as in Fact 4.  We can think of $s$ as an automorphism on $Y$, even of $V^{\wedge}$, as in Fact 5. Why does there exist a cocharacter $\gamma$ such that $s \cdot \gamma = - \gamma$? Here is my attempt at a proof.  We know $s$ extends to an automorphism of $V^{\wedge}$, and still $s^2 = 1_{V^{\wedge}}$.  Choosing a basis for $Y$, we can think of $s$ as a matrix with entries in $\mathbb{Z}$.  It follows that the only eigenvalues of $s$ as $\pm 1$.  If I could show $-1$ was an eigenvalue, then the nullspace of $s + 1_{V^{\wedge}}$ would be nontrivial.  Since solving a system of linear equations doesn't depend on the ambient field, we would find a solution with entries in $\mathbb{Z}$, i.e. a solution $\gamma \in Y$ with $s \cdot \gamma = - \gamma$.  But why must $-1$ be an eigenvalue? 3 .  Take $s, n$ as in Question 2. Supposing that Question 2 has been answered, is it possible to choose $\gamma \in Y$ such that $s \cdot \gamma = -\gamma$ as well as such that $\langle \alpha, \gamma \rangle \neq 0$? yisishoujo explains, given such a cocharacter, how to show the following: ($\ast$) If $\chi$ is a character of $T$, and $\langle \chi, \lambda \rangle = 0$, then $s \cdot \chi = \chi$. 4 .  The Weyl group acts as a group of automorphisms of $V$ (Fact 5).  By the standard averaging process, one can show there exists a symmetric, positive define bilinear form on $V$ which is invariant under the action of the Weyl group $(s \cdot v, s \cdot w) = (v,w)$ for $s$ in the Weyl group, $v, w \in V$. Is such a bilinear form unique up to a scalar? 5 .  Similar to question 4, we identified a canonical Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ as in Facts 1, 5. Is such a pairing unique up to scalar? 6 . What is the relationship between a Weyl group-invariant form on $V$ and the canonical pairing $V \times V^{\wedge}$?  Can they in any sense be 'identified?'  If so, does any possible nonuniqueness of the form on $V$ (as in Question 4) come into play? I know that GIVEN a Weyl group invariant form $(-,-)$ on $V$, there is a isomorphism of $V$ into its dual by sending $v$ to the functional $(v,-)$.  Composing that with the isomorphism identified in Fact 3, we get an isomorphism $V \rightarrow V^{\wedge}$.  This isomorphism induces an action of the Weyl group on $V^{\wedge}$ (which I do not believe is necessarily the canonical action as in Fact 5, however).  Letting $\Phi$ be the inverse map $V^{\wedge} \rightarrow V$, we get a Weyl group invariant pairing $\langle -,-\rangle_1 V \times V^{\wedge} \rightarrow \mathbb{R}$ by setting $\langle v, \eta \rangle_1 = (v, \Phi(\eta))$. And finally, the main question which all of this is intended to be put together to answer: given any Weyl group invariant bilinear form $(-,-)$ on $V$, and given $n,s$ as in Fact 4, why is it the case that $s$ is a Euclidean reflection of $V$ on $\alpha$?  In other words, why is it the case that $s(\alpha) =- \alpha$?  And if $v$ is any element of $V$ with $(\alpha, v) = 0$, why is it the case that $s \cdot v = v$? This main question which has been so difficult for me is an offhand comment in Springer, Linear Algebraic Groups , 7.17.  User yisishoujo indicated that the result $s \cdot v = v$ for all $v$ satisfying $(\alpha, v) = 0$ is a result of the statement $(\ast$) given in Fact 3, but I do not understand why.  Maybe I need to understand the relationship between the bilinear form and the pairing (Fact 6) better.  Thank you for your time.","Let $G$ be a connected linear algebraic group, $T$ a maximal torus of $G$, and $\alpha$ a weight of $T$ such that $G_{\alpha} = Z_G(S)$ is not solvable, where $S = (\textrm{Ker } \alpha)^0$.  I have been really confused about a few things concerning the Weyl group $W(G,T) = N_G(T)/Z_G(T)$, which injects into the group of automorphisms of the group of characters of $T$, as $(nZ_G(T) \cdot \chi)(t) = \chi(ntn^{-1})$. I had asked a question earlier ( Why is $s_{\alpha}$ a Euclidean reflection? ) and user yisishoujo had given me a detailed answer.  Unfortunately I was not able to understand all of his answer, although most of it was very helpful.  I feel like there is something very subtle about this topic which everyone else realizes but that I am not aware of.  So I want to break this question into several questions in the hope someone can point out to me what it is I'm missing. Notation: $X, Y$ are the groups of characters/cocharacters of $T$. $V = \mathbb{R} \otimes_{\mathbb{Z}} X$ $V ^\wedge{} = \mathbb{R} \otimes_{\mathbb{Z}} Y$. We can identify $X$ as an additive subgroup of $V$ as $\chi \mapsto 1 \otimes \chi$.  Similarly for $Y$ and $V^{\wedge}$. Relevant facts: 1 . There is a canonical pairing $\langle -, -\rangle:X \times Y \rightarrow \mathbb{Z}$, where for $\chi \in X, \gamma \in Y$, $\langle \chi, \gamma \rangle$ is the unique integer for which $\chi \circ \gamma(x) = x^{\langle \chi, \gamma \rangle}$ for all $x \in k^{\ast}$. 2 . If $\gamma \in Y$, $\gamma$ induces an abelian group homomorphism of $X$ into $\mathbb{Z}$ by $\chi \mapsto \langle \chi, \gamma \rangle$.  The assignment $\gamma \mapsto \langle -, \gamma \rangle$ is an isomorphism of abelian groups $Y \rightarrow \textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$.  We identify $Y$ with $\textrm{Hom}_{\mathbb{Z}}(X,\mathbb{Z})$. 3 . Identifying $Y$ as above, there is a canonical isomorphism of real vector spaces from $V^{\wedge}$ onto the dual of $V$, where to any generator $\lambda \otimes \gamma$, we associate the map $V \rightarrow \mathbb{R}$ defined on generators by $r \otimes \chi \mapsto r \lambda \langle \chi, \gamma \rangle$. 4 . Any abelian group automorphism of $X$ extends to a linear isomorphism of $V$, the same for $Y$ and $V^{\wedge}$.  The subgroup of the Weyl group, $W_{\alpha} := Z_G(S) \cap N_G(T)/Z_G(T)$, can be shown to have order exactly two.  Choosing an $n \in Z_G(S) \cap N_G(T)$ which is not in $Z_G(T)$, we obtain an automorphism $s$ of $V$ which has order exactly $2$. 5 .  The Weyl group also injects into the group of automorphisms of $Y$, as $(nZ_G(T) \cdot \gamma)(x) = n^{-1}\gamma(x)n$.  Under this action, we have $\langle \chi, \gamma \rangle = \langle w \cdot \chi, w \cdot \gamma \rangle$ for any $w$ in the Weyl group, $\chi \in X, \gamma \in Y$.  The actions of the Weyl group on $X, Y$ extend respectively to actions on $V, V^{\wedge}$.  The pairing $\langle -, - \rangle$ extends to a Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ Questions: 1 . Why does $s(\alpha) \neq \alpha$? I know it is not the case that $s(\chi) = \chi$ for all characters $\chi$, but it is not clear what happens to $\alpha$.  If I can show that $s(\alpha) \neq \alpha$, yisishoujo explained why we must have $s(\alpha) =- \alpha$.  Indeed, conjugation by $n$ (the $n$ as in Fact 4) induces an automorphism of algebraic groups $T/S$.  But $T/S$ is isomorphic to $k^{\ast}$, so there are only two such automorphisms, the identity map and the inverse map.  If $\textrm{Int } n$ induced the identity, then we would have $ntn^{-1}S = tS$ for all $t \in T$, i.e. $\alpha(ntn^{-1}) = \alpha(t)$ for all $t$, which implies $s \alpha = \alpha$, contradiction.  It follows that $ntn^{-1}S = t^{-1}S$ for all $t$, whence $s(\alpha) = - \alpha$. 2 .  Take $s, n$ as in Fact 4.  We can think of $s$ as an automorphism on $Y$, even of $V^{\wedge}$, as in Fact 5. Why does there exist a cocharacter $\gamma$ such that $s \cdot \gamma = - \gamma$? Here is my attempt at a proof.  We know $s$ extends to an automorphism of $V^{\wedge}$, and still $s^2 = 1_{V^{\wedge}}$.  Choosing a basis for $Y$, we can think of $s$ as a matrix with entries in $\mathbb{Z}$.  It follows that the only eigenvalues of $s$ as $\pm 1$.  If I could show $-1$ was an eigenvalue, then the nullspace of $s + 1_{V^{\wedge}}$ would be nontrivial.  Since solving a system of linear equations doesn't depend on the ambient field, we would find a solution with entries in $\mathbb{Z}$, i.e. a solution $\gamma \in Y$ with $s \cdot \gamma = - \gamma$.  But why must $-1$ be an eigenvalue? 3 .  Take $s, n$ as in Question 2. Supposing that Question 2 has been answered, is it possible to choose $\gamma \in Y$ such that $s \cdot \gamma = -\gamma$ as well as such that $\langle \alpha, \gamma \rangle \neq 0$? yisishoujo explains, given such a cocharacter, how to show the following: ($\ast$) If $\chi$ is a character of $T$, and $\langle \chi, \lambda \rangle = 0$, then $s \cdot \chi = \chi$. 4 .  The Weyl group acts as a group of automorphisms of $V$ (Fact 5).  By the standard averaging process, one can show there exists a symmetric, positive define bilinear form on $V$ which is invariant under the action of the Weyl group $(s \cdot v, s \cdot w) = (v,w)$ for $s$ in the Weyl group, $v, w \in V$. Is such a bilinear form unique up to a scalar? 5 .  Similar to question 4, we identified a canonical Weyl group-invariant pairing $V \times V^{\wedge} \rightarrow \mathbb{R}$ as in Facts 1, 5. Is such a pairing unique up to scalar? 6 . What is the relationship between a Weyl group-invariant form on $V$ and the canonical pairing $V \times V^{\wedge}$?  Can they in any sense be 'identified?'  If so, does any possible nonuniqueness of the form on $V$ (as in Question 4) come into play? I know that GIVEN a Weyl group invariant form $(-,-)$ on $V$, there is a isomorphism of $V$ into its dual by sending $v$ to the functional $(v,-)$.  Composing that with the isomorphism identified in Fact 3, we get an isomorphism $V \rightarrow V^{\wedge}$.  This isomorphism induces an action of the Weyl group on $V^{\wedge}$ (which I do not believe is necessarily the canonical action as in Fact 5, however).  Letting $\Phi$ be the inverse map $V^{\wedge} \rightarrow V$, we get a Weyl group invariant pairing $\langle -,-\rangle_1 V \times V^{\wedge} \rightarrow \mathbb{R}$ by setting $\langle v, \eta \rangle_1 = (v, \Phi(\eta))$. And finally, the main question which all of this is intended to be put together to answer: given any Weyl group invariant bilinear form $(-,-)$ on $V$, and given $n,s$ as in Fact 4, why is it the case that $s$ is a Euclidean reflection of $V$ on $\alpha$?  In other words, why is it the case that $s(\alpha) =- \alpha$?  And if $v$ is any element of $V$ with $(\alpha, v) = 0$, why is it the case that $s \cdot v = v$? This main question which has been so difficult for me is an offhand comment in Springer, Linear Algebraic Groups , 7.17.  User yisishoujo indicated that the result $s \cdot v = v$ for all $v$ satisfying $(\alpha, v) = 0$ is a result of the statement $(\ast$) given in Fact 3, but I do not understand why.  Maybe I need to understand the relationship between the bilinear form and the pairing (Fact 6) better.  Thank you for your time.",,"['linear-algebra', 'algebraic-geometry', 'lie-algebras', 'algebraic-groups']"
67,Inequality in normed linear space implies independence,Inequality in normed linear space implies independence,,"Suppose $X$ is a normed linear space over $\mathbb{R}$, and $v_1,  \cdots, v_n \in X$ are unit vectors. Furthermore, assume that there    exists $\epsilon \in (0, 1/2)$ such that for any constants $c_i \in  \mathbb{R}$, we have $$\left \| \sum_{i=1}^{n} c_i v_i \right \| \le  (1+\epsilon) \max_{1\le i \le n} |c_i|.$$ Prove that the collection    $\{v_i\}$ is linearly independent. My first thought is to try bounding $\left \|\sum_{i=1}^{n} c_i v_i \right \|$ below in terms of $\epsilon$ and $\max_{1\le i \le n} |c_i|$ but I'm unsure how to go about doing this. If this can be done such that the bound is always positive, then linear independence would of course follow. Any help would be much appreciated!","Suppose $X$ is a normed linear space over $\mathbb{R}$, and $v_1,  \cdots, v_n \in X$ are unit vectors. Furthermore, assume that there    exists $\epsilon \in (0, 1/2)$ such that for any constants $c_i \in  \mathbb{R}$, we have $$\left \| \sum_{i=1}^{n} c_i v_i \right \| \le  (1+\epsilon) \max_{1\le i \le n} |c_i|.$$ Prove that the collection    $\{v_i\}$ is linearly independent. My first thought is to try bounding $\left \|\sum_{i=1}^{n} c_i v_i \right \|$ below in terms of $\epsilon$ and $\max_{1\le i \le n} |c_i|$ but I'm unsure how to go about doing this. If this can be done such that the bound is always positive, then linear independence would of course follow. Any help would be much appreciated!",,"['linear-algebra', 'functional-analysis']"
68,Inverting an $n \times n$ matrix using determinant,Inverting an  matrix using determinant,n \times n,"We're asked to invert the following matrix with the help of guided questions. $$\begin{pmatrix} 1 + a_1 & 1 & \cdots & 1 \\ 1 & 1+a_2 & \ddots & \vdots \\ \vdots & \ddots & \ddots & 1 \\ 1 & \cdots & 1 & 1 + a_n \end{pmatrix}$$ To do that, the problem considered the following determinant : Let $(r_1,r_2,\cdots,r_n)$ be distinct real numbers and $(a,b)\in\mathbb{R}^2$ $$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b+x & r_2+x & \ddots & \vdots \\ \vdots & \ddots & \ddots & a+x \\ b+x & \cdots & b+x & r_n + x \end{vmatrix}$$ Then we are asked to prove there exists two real numbers A and B such that for all real numbers $x$ $$\Delta(x) = Ax + B$$ I have done a row reduction on the matrix to simplify it. For $i$ going from $n-1$ to $1$ $L_{i+1} \leftarrow L_{i+1} - L_i$ $$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b-r_1 & r_2-a & 0  & 0 \\ 0 & \ddots & \ddots & 0 \\ 0 & 0 & b- r_n & r_n-a\end{vmatrix}$$ Then I developped with respect to the first row. Let $\Delta_{i,j}$ be the minor of $\Delta$ then $$\Delta(x) = (r_1 + x)\Delta_{1,1} + (a+x)\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ Rearranging the forula gives A and B independent of $x$ which proves their existence : $$\Delta(x) = x[\Delta_{1,1} + \sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j}] + r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ The real problem i'm facing is the next question:  Given the polynome $P(X) = \prod\limits_{k=1}^n(r_k-X)$ and supposing $a \neq b$ we need to prove that  $$\Delta(0) = \frac{aP(b)-bP(a)}{a-b}$$ To use the first question I noticed $\Delta(0) = B$ and simplified it $$    B = r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j}  \\ \Delta_{1,1} = \prod\limits_{i=2}^n (r_k-a)  $$ Since $\Delta_{1,1}$ is an inferiour triangular matrix, then similarly : $$\Delta_{1,n} = \prod\limits_{i=1}^{n-1} (b-r_k) $$ For other minors I noticed that we can split them into diagonal inferiour and superior triangular blocks $$ \Delta_{i,j}= \left| \begin{array}{c|c} B' & O \\ \hline O & A' \end{array} \right| $$ and thus $\forall j\in \{2,\cdots,n-1\} $ $$\Delta_{1,j} = \prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)$$ So i ended up with this  $$\Delta(0) = B = r_1\prod\limits_{i=2}^n (r_k-a) + a\left(\sum\limits_{j=2}^{n-1} (-1)^{1+j}\prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)\right) + \\ a\times(-1)^{1+n}\times\prod\limits_{i=1}^{n-1} (b-r_k) $$ I feel like i am close to getting $P(b)$ and $P(a)$ from the first and last terms but i can't seem to see how? Sorry for any mistakes in advance, i'm new around here.","We're asked to invert the following matrix with the help of guided questions. $$\begin{pmatrix} 1 + a_1 & 1 & \cdots & 1 \\ 1 & 1+a_2 & \ddots & \vdots \\ \vdots & \ddots & \ddots & 1 \\ 1 & \cdots & 1 & 1 + a_n \end{pmatrix}$$ To do that, the problem considered the following determinant : Let $(r_1,r_2,\cdots,r_n)$ be distinct real numbers and $(a,b)\in\mathbb{R}^2$ $$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b+x & r_2+x & \ddots & \vdots \\ \vdots & \ddots & \ddots & a+x \\ b+x & \cdots & b+x & r_n + x \end{vmatrix}$$ Then we are asked to prove there exists two real numbers A and B such that for all real numbers $x$ $$\Delta(x) = Ax + B$$ I have done a row reduction on the matrix to simplify it. For $i$ going from $n-1$ to $1$ $L_{i+1} \leftarrow L_{i+1} - L_i$ $$\Delta(x) = \begin{vmatrix} r_1 + x & a+x & \cdots & a+x \\ b-r_1 & r_2-a & 0  & 0 \\ 0 & \ddots & \ddots & 0 \\ 0 & 0 & b- r_n & r_n-a\end{vmatrix}$$ Then I developped with respect to the first row. Let $\Delta_{i,j}$ be the minor of $\Delta$ then $$\Delta(x) = (r_1 + x)\Delta_{1,1} + (a+x)\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ Rearranging the forula gives A and B independent of $x$ which proves their existence : $$\Delta(x) = x[\Delta_{1,1} + \sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j}] + r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j} $$ The real problem i'm facing is the next question:  Given the polynome $P(X) = \prod\limits_{k=1}^n(r_k-X)$ and supposing $a \neq b$ we need to prove that  $$\Delta(0) = \frac{aP(b)-bP(a)}{a-b}$$ To use the first question I noticed $\Delta(0) = B$ and simplified it $$    B = r_1\Delta_{1,1} + a\sum\limits_{j=2}^n (-1)^{1+j}\Delta_{1,j}  \\ \Delta_{1,1} = \prod\limits_{i=2}^n (r_k-a)  $$ Since $\Delta_{1,1}$ is an inferiour triangular matrix, then similarly : $$\Delta_{1,n} = \prod\limits_{i=1}^{n-1} (b-r_k) $$ For other minors I noticed that we can split them into diagonal inferiour and superior triangular blocks $$ \Delta_{i,j}= \left| \begin{array}{c|c} B' & O \\ \hline O & A' \end{array} \right| $$ and thus $\forall j\in \{2,\cdots,n-1\} $ $$\Delta_{1,j} = \prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)$$ So i ended up with this  $$\Delta(0) = B = r_1\prod\limits_{i=2}^n (r_k-a) + a\left(\sum\limits_{j=2}^{n-1} (-1)^{1+j}\prod\limits_{i=2}^{j-1} (b-r_k)\prod\limits_{i=j+1}^{n} (r_k-a)\right) + \\ a\times(-1)^{1+n}\times\prod\limits_{i=1}^{n-1} (b-r_k) $$ I feel like i am close to getting $P(b)$ and $P(a)$ from the first and last terms but i can't seem to see how? Sorry for any mistakes in advance, i'm new around here.",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
69,Which matricies are a product of projections?,Which matricies are a product of projections?,,"I believe that if $A$ is an $n$ by $n$ matrix over $\mathbb{R}$, then $A$ can be written as a product of projections as long as $A$ is not invertible. However, I don't know how to prove this. By projection I mean a matrix $P$ which satisfies $P^2 = P$. I know how the show this if $n = 2$, however the method involves explicitly calculating such a representation. I think it would be extremely messy to generalize this and hope that someone finds a simpler solution.","I believe that if $A$ is an $n$ by $n$ matrix over $\mathbb{R}$, then $A$ can be written as a product of projections as long as $A$ is not invertible. However, I don't know how to prove this. By projection I mean a matrix $P$ which satisfies $P^2 = P$. I know how the show this if $n = 2$, however the method involves explicitly calculating such a representation. I think it would be extremely messy to generalize this and hope that someone finds a simpler solution.",,['linear-algebra']
70,Geometric interpretation for eigenvalues and eigenvectors of the cross product's representation as a linear map,Geometric interpretation for eigenvalues and eigenvectors of the cross product's representation as a linear map,,"Fix ${\bf x} = (x_1,x_2,x_3) \in \Bbb R^3\setminus\{{\bf 0}\}$. We can look at the cross product as a linear map ${\bf x}\times: \Bbb R^3 \to \Bbb R^3$ which is represented in the standard basis by $$\begin{bmatrix} 0 & -x_3 & x_2 \\ x_3 & 0 & -x_1 \\ -x_2 & x_1 & 0\end{bmatrix}.$$Also, it is easy to compute its charcteristic polynomial $p(t) = t(t^2 + \|{\bf x}\|^2)$. Then $0$ is an eigenvalue for which the associated eigenspace is the line spanned by ${\bf x}$ itself. But we can write $$p(t) = t(t-i\|{\bf x}\|)(t+i\|{\bf x}\|),$$and continue the analysis. Assuming I didn't screw up computations, I get that the a complex eigenvector associated to $i\|{\bf x}\|$ is $${\bf v} = \left(-x_1x_3 - x_2\|{\bf x}\|i, -x_2x_3+x_1\|{\bf x}\|i, x_1^2+x_2^2\right).$$We have $${\rm Re}({\bf v}) = (-x_1x_3,-x_2x_3,x_1^2+x_2^2) \quad\mbox{and}\quad {\rm Im}({\bf v}) = (-x_2\|{\bf x}\|,x_1\|{\bf x}\|,0).$$Then I noticed that: $${\bf x}\times {\rm Re}({\bf v})  = \|{\bf x}\|\,{\rm Im}({\bf v}) \quad\mbox{and}\quad {\bf x}\times {\rm Im}({\bf v}) = -\|{\bf x}\|\,{\rm Re}({\bf v}).  $$ Even more, ${\rm Re}({\bf v})$ and ${\rm Im}({\bf v})$ are orthogonal. I am bewildered by my little discovery. However, I can't quite interpret this geometrically, and I guess that extra factor of $\|{\bf x}\|$ is related to the $i\|{\bf x}\|$ eigenvalue. Can someone explain what's behind these computations? I just found this question, and I apologize for not searching well enough before asking - yet, there is no satisfactory answer there, and they didn't use real and imaginary parts of the eigenvectors like I pointed here - it might make something easier to see, so please don't vote to close as duplicate (yet?).","Fix ${\bf x} = (x_1,x_2,x_3) \in \Bbb R^3\setminus\{{\bf 0}\}$. We can look at the cross product as a linear map ${\bf x}\times: \Bbb R^3 \to \Bbb R^3$ which is represented in the standard basis by $$\begin{bmatrix} 0 & -x_3 & x_2 \\ x_3 & 0 & -x_1 \\ -x_2 & x_1 & 0\end{bmatrix}.$$Also, it is easy to compute its charcteristic polynomial $p(t) = t(t^2 + \|{\bf x}\|^2)$. Then $0$ is an eigenvalue for which the associated eigenspace is the line spanned by ${\bf x}$ itself. But we can write $$p(t) = t(t-i\|{\bf x}\|)(t+i\|{\bf x}\|),$$and continue the analysis. Assuming I didn't screw up computations, I get that the a complex eigenvector associated to $i\|{\bf x}\|$ is $${\bf v} = \left(-x_1x_3 - x_2\|{\bf x}\|i, -x_2x_3+x_1\|{\bf x}\|i, x_1^2+x_2^2\right).$$We have $${\rm Re}({\bf v}) = (-x_1x_3,-x_2x_3,x_1^2+x_2^2) \quad\mbox{and}\quad {\rm Im}({\bf v}) = (-x_2\|{\bf x}\|,x_1\|{\bf x}\|,0).$$Then I noticed that: $${\bf x}\times {\rm Re}({\bf v})  = \|{\bf x}\|\,{\rm Im}({\bf v}) \quad\mbox{and}\quad {\bf x}\times {\rm Im}({\bf v}) = -\|{\bf x}\|\,{\rm Re}({\bf v}).  $$ Even more, ${\rm Re}({\bf v})$ and ${\rm Im}({\bf v})$ are orthogonal. I am bewildered by my little discovery. However, I can't quite interpret this geometrically, and I guess that extra factor of $\|{\bf x}\|$ is related to the $i\|{\bf x}\|$ eigenvalue. Can someone explain what's behind these computations? I just found this question, and I apologize for not searching well enough before asking - yet, there is no satisfactory answer there, and they didn't use real and imaginary parts of the eigenvectors like I pointed here - it might make something easier to see, so please don't vote to close as duplicate (yet?).",,"['linear-algebra', 'analytic-geometry', 'linear-transformations', 'diagonalization', 'cross-product']"
71,I have a answer to a question about trace. Is there an easier answer to this question?,I have a answer to a question about trace. Is there an easier answer to this question?,,"Let $A\in M_n(\mathbb{C})$. Show that $$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ My answer: It is easy to see that $$tr\left(\frac{A+A^*}{2}\right)=\text{Re}(tr(A))\qquad and\qquad tr((A^*A)^{1/2})=\sum_{i=1}^n\sigma_i(A),$$ where $\sigma(A)$ is the singular value of $A$. On the other hand, based on Polar decomposition theorem, there exists a unitary matrix $P$ such that $A=P((A^*A)^{1/2})$. Now, according to [Theorem 8.7.6, Page 551, R. Horn, C. Johnson, Matrix analysis, Second edition], we have: $$\begin{eqnarray}  \text{Re}(tr(A))&=\text{Re}(tr(P((A^*A)^{1/2})))\\ &\leq\sum_{i=1}^n \sigma_i(P)\sigma_i ((A^*A)^{1/2}).  \end{eqnarray} $$ Since $P$ is unitary, $\sigma_i(P)=1$. We have also $\sigma_i ((A^*A)^{1/2})=\sigma_i(A)$. So, $$ \text{Re}(tr(A))\leq\sum_{i=1}^n \sigma_i(A).$$ Therefore, we get $$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ Now,  is there an easier answer to this question?","Let $A\in M_n(\mathbb{C})$. Show that $$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ My answer: It is easy to see that $$tr\left(\frac{A+A^*}{2}\right)=\text{Re}(tr(A))\qquad and\qquad tr((A^*A)^{1/2})=\sum_{i=1}^n\sigma_i(A),$$ where $\sigma(A)$ is the singular value of $A$. On the other hand, based on Polar decomposition theorem, there exists a unitary matrix $P$ such that $A=P((A^*A)^{1/2})$. Now, according to [Theorem 8.7.6, Page 551, R. Horn, C. Johnson, Matrix analysis, Second edition], we have: $$\begin{eqnarray}  \text{Re}(tr(A))&=\text{Re}(tr(P((A^*A)^{1/2})))\\ &\leq\sum_{i=1}^n \sigma_i(P)\sigma_i ((A^*A)^{1/2}).  \end{eqnarray} $$ Since $P$ is unitary, $\sigma_i(P)=1$. We have also $\sigma_i ((A^*A)^{1/2})=\sigma_i(A)$. So, $$ \text{Re}(tr(A))\leq\sum_{i=1}^n \sigma_i(A).$$ Therefore, we get $$tr\left(\frac{A+A^*}{2}\right)\leq tr((A^*A)^{1/2}).$$ Now,  is there an easier answer to this question?",,"['linear-algebra', 'matrices', 'analysis']"
72,SVD - Decomposed Matrix Sizes,SVD - Decomposed Matrix Sizes,,"I had a question about SVD. Specifically about the size of matrices $U$, $\Sigma$ and $V$ decomposed from the $m\times n$ matrix $X$ using the formula $$X = U \Sigma V^T $$ Most of the the tutorial literature says that the resulting sizes are $U$ is $m \times m$ $\Sigma$ is $m \times n$ $V$ is $n \times n$ However, there have been quite few times when the sizes given are $U$ is $m \times n$ $\Sigma$ is $n \times n$ $V$ is $n \times n$ In other words instead of $\Sigma$ being the matrix with possibly different number of rows and columns, its $U$ with the different number of rows and columns. The math works out, so why (and in what cases) is this less frequent version used?","I had a question about SVD. Specifically about the size of matrices $U$, $\Sigma$ and $V$ decomposed from the $m\times n$ matrix $X$ using the formula $$X = U \Sigma V^T $$ Most of the the tutorial literature says that the resulting sizes are $U$ is $m \times m$ $\Sigma$ is $m \times n$ $V$ is $n \times n$ However, there have been quite few times when the sizes given are $U$ is $m \times n$ $\Sigma$ is $n \times n$ $V$ is $n \times n$ In other words instead of $\Sigma$ being the matrix with possibly different number of rows and columns, its $U$ with the different number of rows and columns. The math works out, so why (and in what cases) is this less frequent version used?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
73,Principal Minors of $B(AB)^{-1}A$ and Cauchy-Binet Terms,Principal Minors of  and Cauchy-Binet Terms,B(AB)^{-1}A,"I am looking for a proof for the following conjecture. I think the result follows from applying a generalization of the Cauchy-Binet formula to the matrix $\mathbf{M}$ defined bellow. I've tested it as much as I could using Mathematica and am convinced it is true, but I haven't been able to prove it. Any help will be much appreciated. Setup Suppose $\mathbf{A}$ and $\mathbf{B}$ are $(n \times m)$ and $(m \times n)$ matrices respectively, with $n<m$ and $\operatorname{rank}(\mathbf{A})=\operatorname{rank}(\mathbf{B})=n$. Let $K \equiv \{1,\dots,m\}$, $\mathbf{X}_{k}$ denote the matrix $\mathbf{X}$ keeping only columns and rows in $k$, $\mathbf{X}_{rk}$ denote the matrix $\mathbf{X}$ keeping only rows in $k$, and $\mathbf{X}_{ck}$ denote the matrix $\mathbf{X}$ keeping only columns in $k$. Also, for any $k \subset K$, let $K_n$ be the set of subsets of $K$ with $n$ elements. Then, using the Cauchy-Binet formula, the determinant of the matrix $\mathbf{A}\mathbf{B}$, denoted by $\Delta$, can be written as $$\Delta \equiv \det(\mathbf{A}\mathbf{B}) = \sum_{k\in K_n}{\det(\mathbf{A}_{ck})\det(\mathbf{B}_{rk})}. $$ Denote each element of this sum by $$ d_k \equiv \det(\mathbf{A}_{ck})\det(\mathbf{B}_{rk}), \;\;\;\;\text{for all }k \in K_n.$$ Consider the matrix $\mathbf{M}$ given by $$ \mathbf{M} = \mathbf{B}(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}, $$ with principal minors given by $\det(\mathbf{M}_{k})$ for any $k \in P(K)$, where $P(K)$ is the power set of $K$ (the set of all subsets of $K$). Conjecture I want to show that   $$ \det(\mathbf{M}_{k}) = \frac{1}{\Delta}\sum_{j\in \{ i \in K_n : k \subset i \}}d_j, \;\;\;\;\text{for all }k \in P(K) $$ Example to clarify notation Suppose that $m=3$ and $n=2$. Then, from the definitions we have that \begin{align} P(K) =& \{\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \\[3ex] K_n =& \{\{1,2\},\{1,3\},\{2,3\}\} \\[3ex] \mathbf{A} =& \left[\begin{matrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \end{matrix}\right],  \;\;\; \mathbf{B} = \left[\begin{matrix} b_{11} & b_{12}  \\ b_{21} & b_{22}  \\ b_{31} & b_{32} \end{matrix}\right] \\[3ex] \mathbf{A}\mathbf{B} =& \left[\begin{matrix} a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31}	& a_{11}b_{12}+a_{12}b_{22}+a_{13}b_{32} \\ a_{21}b_{11}+a_{22}b_{21}+a_{23}b_{31} &	a_{21}b_{12}+a_{22}b_{22}+a_{23}b_{32}\end{matrix}\right] \end{align} and the Cauchy-Binet formula implies \begin{align} \det(\mathbf{A}\mathbf{B}) =& \det(\mathbf{A}_{c\{1,2\}})\det(\mathbf{B}_{r\{1,2\}}) + \det(\mathbf{A}_{c\{1,3\}})\det(\mathbf{B}_{r\{1,3\}}) + \det(\mathbf{A}_{c\{2,3\}})\det(\mathbf{B}_{r\{2,3\}}) \\[2ex] =& \det\left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]\det\left[\begin{matrix} b_{11} & b_{12}  \\ b_{21} & b_{22}  \end{matrix}\right] + \det\left[\begin{matrix} a_{11} & a_{13} \\ a_{21} & a_{23} \end{matrix}\right]\det\left[\begin{matrix} b_{11} & b_{12}  \\ b_{31} & b_{32} \end{matrix}\right] + \\[1ex]  & \det\left[\begin{matrix}  a_{12} & a_{13} \\ a_{22} & a_{23} \end{matrix}\right]\det\left[\begin{matrix} b_{21} & b_{22}  \\ b_{31} & b_{32} \end{matrix}\right]. \end{align} Again, using the definitions above we have that \begin{align} d_{\{1,2\}} \equiv & \; \det(\mathbf{A}_{c\{1,2\}})\det(\mathbf{B}_{r\{1,2\}}) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\[1ex] d_{\{1,3\}} \equiv & \; \det(\mathbf{A}_{c\{1,3\}})\det(\mathbf{B}_{r\{1,3\}}) \\[1ex] d_{\{2,3\}} \equiv & \; \det(\mathbf{A}_{c\{2,3\}})\det(\mathbf{B}_{r\{2,3\}}) \\[3ex] \Delta \equiv & \; d_{\{1,2\}}+d_{\{1,3\}}+d_{\{2,3\}} \end{align} and, with some algebra, it is easy to verify that the principal minors of $\mathbf{M}$ can be written as \begin{align} \det(\mathbf{M}_{\{1,2\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{1,2\}}d_j =\; \frac{1}{\Delta}d_{\{1,2\}} \\[2ex] \det(\mathbf{M}_{\{1,3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{1,3\}}d_j =\; \frac{1}{\Delta}d_{\{1,3\}} \\[2ex] \det(\mathbf{M}_{\{2,3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{2,3\}}d_j =\; \frac{1}{\Delta}d_{\{2,3\}} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \\[2ex] \det(\mathbf{M}_{\{3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,3\},\{2,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,3\}}+d_{\{2,3\}}) \\[2ex] \det(\mathbf{M}_{\{2\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,2\},\{2,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,2\}}+d_{\{2,3\}}) \\[2ex] \det(\mathbf{M}_{\{1\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,2\},\{1,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,2\}}+d_{\{1,3\}}) \end{align} Moreover, $\det(\mathbf{M}_{\{1,2,3\}}) =0$ follows from the fact that $\det(\mathbf{M}_{\{1,2,3\}})$ is a principal minor of order $3>2=n$. All principal minors of $\mathbf{M}$ of order greater than $n$, are equal to zero, since $\operatorname{rank}(\mathbf{M})=n$. Attempts at the proof using the suggestions by @darijgrinberg Fix $k \in P(K)$ and let $v\equiv |k|$, i.e. the number of elements in $k$. First suppose that $v \gt n$, so that $\det(\mathbf{M}_k)=0$, since $\operatorname{rank}(\mathbf{M}_k)=0$, and $\{ i \in K_n : k \subset i \} = \emptyset$, so that the conjecture holds trivially. Next, suppose that $v \le n$. What follows is tentative. Working with the definition of $\mathbf{M}_{k}$: By definition, $$ \mathbf{M}_{k}=\mathbf{B}_{rk}(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{ck} $$ where $\mathbf{B}_{rk}$ and $(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{ck}$ are $(v \times n)$ and $(n \times v)$ matrices respectively. Let $L\equiv \{1,\dots,n\}$ and $L_v$ be the set of subsets of $L$ with $v$ elements, and from the Cauchy-Binet formula we have that $$ \det(\mathbf{M}_{k})=\sum_{j \in L_v}\det\left(\mathbf{B}_{rk,cj}\right)\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right), \tag1$$ and $$	\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right)=\sum_{i \in L_v}\det\left((\mathbf{A}\mathbf{B})_{rj,ci}^{-1}\right)\det\left(\mathbf{A}_{ri,ck}\right).$$ So that $$ \det(\mathbf{M}_{k})=\sum_{i,j \in L_v}\det\left((\mathbf{A}\mathbf{B})_{rj,ci}^{-1}\right)\det\left(\mathbf{A}_{ri,ck}\right)\det\left(\mathbf{B}_{rk,cj}\right),$$ and since $$ (\mathbf{A}\mathbf{B})^{-1}=\frac{1}{\Delta}\operatorname{adj}(\mathbf{A}\mathbf{B}) \implies (\mathbf{A}\mathbf{B})_{rj,ci}^{-1}=\frac{1}{\Delta}\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci} $$ it follows that $$ \det(\mathbf{M}_{k})=\frac{1}{\Delta^v} \sum_{i,j \in L_v}\det\left(\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci}\right)\det\left(\mathbf{A}_{ri,ck}\right)\det\left(\mathbf{B}_{rk,cj}\right).$$ Next, for each $i,j \in L_{v}$ define $i'\equiv \{1,\dots,n\}\setminus i$ and $j'\equiv \{1,\dots,n\}\setminus j$, then it follows from Jacobi's theorem that $$ \det(\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci})=(-1)^{\sigma_{ij}}\det(((\mathbf{A}\mathbf{B})^{\top})_{rj',ci'})\Delta^{v-1}=(-1)^{\sigma_{ij}}\det(\mathbf{A}_{rj'}\mathbf{B}_{ci'}) $$ where $$ \sigma_{ij} \equiv i_{v+1}'+\cdots+i_{n}'+j_{v+1}'+\cdots+j_{n}' $$ and, therefore, $$ \det(\mathbf{M}_{k})=\frac{1}{\Delta}\sum_{i,j \in L_{v}}(-1)^{\sigma_{ij}}\det(\mathbf{A}_{rj'}\mathbf{B}_{ci'})\det(\mathbf{A}_{ri,ck})\det(\mathbf{B}_{rk,cj}) $$ So far I haven't been able to proceed from here. Working with the conjecture equation: Notice that for any $j \in K_{n}$, $$ \det((\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{cj})=\frac{1}{\Delta}\det(\mathbf{A}_{cj}), $$ so that the conjecture can be rewritten as $$ \det(\mathbf{M}_{k}) = \sum_{j\in \{ i \in K_n : k \subset i \}}\det(\mathbf{B}_{rj})\det((\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{cj}), \tag2$$ which looks similar equation $(1)$, rewritten here for convenience $$ \det(\mathbf{M}_{k})=\sum_{j \in L_v}\det\left(\mathbf{B}_{rk,cj}\right)\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right). $$ I am not sure how to deal with the difference in the indexes between the two equations. My guess is that I will need to use a Laplace expansion to equation $(2)$.","I am looking for a proof for the following conjecture. I think the result follows from applying a generalization of the Cauchy-Binet formula to the matrix $\mathbf{M}$ defined bellow. I've tested it as much as I could using Mathematica and am convinced it is true, but I haven't been able to prove it. Any help will be much appreciated. Setup Suppose $\mathbf{A}$ and $\mathbf{B}$ are $(n \times m)$ and $(m \times n)$ matrices respectively, with $n<m$ and $\operatorname{rank}(\mathbf{A})=\operatorname{rank}(\mathbf{B})=n$. Let $K \equiv \{1,\dots,m\}$, $\mathbf{X}_{k}$ denote the matrix $\mathbf{X}$ keeping only columns and rows in $k$, $\mathbf{X}_{rk}$ denote the matrix $\mathbf{X}$ keeping only rows in $k$, and $\mathbf{X}_{ck}$ denote the matrix $\mathbf{X}$ keeping only columns in $k$. Also, for any $k \subset K$, let $K_n$ be the set of subsets of $K$ with $n$ elements. Then, using the Cauchy-Binet formula, the determinant of the matrix $\mathbf{A}\mathbf{B}$, denoted by $\Delta$, can be written as $$\Delta \equiv \det(\mathbf{A}\mathbf{B}) = \sum_{k\in K_n}{\det(\mathbf{A}_{ck})\det(\mathbf{B}_{rk})}. $$ Denote each element of this sum by $$ d_k \equiv \det(\mathbf{A}_{ck})\det(\mathbf{B}_{rk}), \;\;\;\;\text{for all }k \in K_n.$$ Consider the matrix $\mathbf{M}$ given by $$ \mathbf{M} = \mathbf{B}(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}, $$ with principal minors given by $\det(\mathbf{M}_{k})$ for any $k \in P(K)$, where $P(K)$ is the power set of $K$ (the set of all subsets of $K$). Conjecture I want to show that   $$ \det(\mathbf{M}_{k}) = \frac{1}{\Delta}\sum_{j\in \{ i \in K_n : k \subset i \}}d_j, \;\;\;\;\text{for all }k \in P(K) $$ Example to clarify notation Suppose that $m=3$ and $n=2$. Then, from the definitions we have that \begin{align} P(K) =& \{\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \\[3ex] K_n =& \{\{1,2\},\{1,3\},\{2,3\}\} \\[3ex] \mathbf{A} =& \left[\begin{matrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \end{matrix}\right],  \;\;\; \mathbf{B} = \left[\begin{matrix} b_{11} & b_{12}  \\ b_{21} & b_{22}  \\ b_{31} & b_{32} \end{matrix}\right] \\[3ex] \mathbf{A}\mathbf{B} =& \left[\begin{matrix} a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31}	& a_{11}b_{12}+a_{12}b_{22}+a_{13}b_{32} \\ a_{21}b_{11}+a_{22}b_{21}+a_{23}b_{31} &	a_{21}b_{12}+a_{22}b_{22}+a_{23}b_{32}\end{matrix}\right] \end{align} and the Cauchy-Binet formula implies \begin{align} \det(\mathbf{A}\mathbf{B}) =& \det(\mathbf{A}_{c\{1,2\}})\det(\mathbf{B}_{r\{1,2\}}) + \det(\mathbf{A}_{c\{1,3\}})\det(\mathbf{B}_{r\{1,3\}}) + \det(\mathbf{A}_{c\{2,3\}})\det(\mathbf{B}_{r\{2,3\}}) \\[2ex] =& \det\left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]\det\left[\begin{matrix} b_{11} & b_{12}  \\ b_{21} & b_{22}  \end{matrix}\right] + \det\left[\begin{matrix} a_{11} & a_{13} \\ a_{21} & a_{23} \end{matrix}\right]\det\left[\begin{matrix} b_{11} & b_{12}  \\ b_{31} & b_{32} \end{matrix}\right] + \\[1ex]  & \det\left[\begin{matrix}  a_{12} & a_{13} \\ a_{22} & a_{23} \end{matrix}\right]\det\left[\begin{matrix} b_{21} & b_{22}  \\ b_{31} & b_{32} \end{matrix}\right]. \end{align} Again, using the definitions above we have that \begin{align} d_{\{1,2\}} \equiv & \; \det(\mathbf{A}_{c\{1,2\}})\det(\mathbf{B}_{r\{1,2\}}) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\[1ex] d_{\{1,3\}} \equiv & \; \det(\mathbf{A}_{c\{1,3\}})\det(\mathbf{B}_{r\{1,3\}}) \\[1ex] d_{\{2,3\}} \equiv & \; \det(\mathbf{A}_{c\{2,3\}})\det(\mathbf{B}_{r\{2,3\}}) \\[3ex] \Delta \equiv & \; d_{\{1,2\}}+d_{\{1,3\}}+d_{\{2,3\}} \end{align} and, with some algebra, it is easy to verify that the principal minors of $\mathbf{M}$ can be written as \begin{align} \det(\mathbf{M}_{\{1,2\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{1,2\}}d_j =\; \frac{1}{\Delta}d_{\{1,2\}} \\[2ex] \det(\mathbf{M}_{\{1,3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{1,3\}}d_j =\; \frac{1}{\Delta}d_{\{1,3\}} \\[2ex] \det(\mathbf{M}_{\{2,3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{2,3\}}d_j =\; \frac{1}{\Delta}d_{\{2,3\}} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \\[2ex] \det(\mathbf{M}_{\{3\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,3\},\{2,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,3\}}+d_{\{2,3\}}) \\[2ex] \det(\mathbf{M}_{\{2\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,2\},\{2,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,2\}}+d_{\{2,3\}}) \\[2ex] \det(\mathbf{M}_{\{1\}}) =& \; \frac{1}{\Delta}\sum_{j\in \{\{1,2\},\{1,3\}\}}d_j =\; \frac{1}{\Delta}(d_{\{1,2\}}+d_{\{1,3\}}) \end{align} Moreover, $\det(\mathbf{M}_{\{1,2,3\}}) =0$ follows from the fact that $\det(\mathbf{M}_{\{1,2,3\}})$ is a principal minor of order $3>2=n$. All principal minors of $\mathbf{M}$ of order greater than $n$, are equal to zero, since $\operatorname{rank}(\mathbf{M})=n$. Attempts at the proof using the suggestions by @darijgrinberg Fix $k \in P(K)$ and let $v\equiv |k|$, i.e. the number of elements in $k$. First suppose that $v \gt n$, so that $\det(\mathbf{M}_k)=0$, since $\operatorname{rank}(\mathbf{M}_k)=0$, and $\{ i \in K_n : k \subset i \} = \emptyset$, so that the conjecture holds trivially. Next, suppose that $v \le n$. What follows is tentative. Working with the definition of $\mathbf{M}_{k}$: By definition, $$ \mathbf{M}_{k}=\mathbf{B}_{rk}(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{ck} $$ where $\mathbf{B}_{rk}$ and $(\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{ck}$ are $(v \times n)$ and $(n \times v)$ matrices respectively. Let $L\equiv \{1,\dots,n\}$ and $L_v$ be the set of subsets of $L$ with $v$ elements, and from the Cauchy-Binet formula we have that $$ \det(\mathbf{M}_{k})=\sum_{j \in L_v}\det\left(\mathbf{B}_{rk,cj}\right)\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right), \tag1$$ and $$	\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right)=\sum_{i \in L_v}\det\left((\mathbf{A}\mathbf{B})_{rj,ci}^{-1}\right)\det\left(\mathbf{A}_{ri,ck}\right).$$ So that $$ \det(\mathbf{M}_{k})=\sum_{i,j \in L_v}\det\left((\mathbf{A}\mathbf{B})_{rj,ci}^{-1}\right)\det\left(\mathbf{A}_{ri,ck}\right)\det\left(\mathbf{B}_{rk,cj}\right),$$ and since $$ (\mathbf{A}\mathbf{B})^{-1}=\frac{1}{\Delta}\operatorname{adj}(\mathbf{A}\mathbf{B}) \implies (\mathbf{A}\mathbf{B})_{rj,ci}^{-1}=\frac{1}{\Delta}\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci} $$ it follows that $$ \det(\mathbf{M}_{k})=\frac{1}{\Delta^v} \sum_{i,j \in L_v}\det\left(\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci}\right)\det\left(\mathbf{A}_{ri,ck}\right)\det\left(\mathbf{B}_{rk,cj}\right).$$ Next, for each $i,j \in L_{v}$ define $i'\equiv \{1,\dots,n\}\setminus i$ and $j'\equiv \{1,\dots,n\}\setminus j$, then it follows from Jacobi's theorem that $$ \det(\operatorname{adj}(\mathbf{A}\mathbf{B})_{rj,ci})=(-1)^{\sigma_{ij}}\det(((\mathbf{A}\mathbf{B})^{\top})_{rj',ci'})\Delta^{v-1}=(-1)^{\sigma_{ij}}\det(\mathbf{A}_{rj'}\mathbf{B}_{ci'}) $$ where $$ \sigma_{ij} \equiv i_{v+1}'+\cdots+i_{n}'+j_{v+1}'+\cdots+j_{n}' $$ and, therefore, $$ \det(\mathbf{M}_{k})=\frac{1}{\Delta}\sum_{i,j \in L_{v}}(-1)^{\sigma_{ij}}\det(\mathbf{A}_{rj'}\mathbf{B}_{ci'})\det(\mathbf{A}_{ri,ck})\det(\mathbf{B}_{rk,cj}) $$ So far I haven't been able to proceed from here. Working with the conjecture equation: Notice that for any $j \in K_{n}$, $$ \det((\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{cj})=\frac{1}{\Delta}\det(\mathbf{A}_{cj}), $$ so that the conjecture can be rewritten as $$ \det(\mathbf{M}_{k}) = \sum_{j\in \{ i \in K_n : k \subset i \}}\det(\mathbf{B}_{rj})\det((\mathbf{A}\mathbf{B})^{-1}\mathbf{A}_{cj}), \tag2$$ which looks similar equation $(1)$, rewritten here for convenience $$ \det(\mathbf{M}_{k})=\sum_{j \in L_v}\det\left(\mathbf{B}_{rk,cj}\right)\det\left((\mathbf{A}\mathbf{B})_{rj}^{-1}\mathbf{A}_{ck}\right). $$ I am not sure how to deal with the difference in the indexes between the two equations. My guess is that I will need to use a Laplace expansion to equation $(2)$.",,"['linear-algebra', 'matrices', 'determinant']"
74,Existence of polynomials in the Diagonal-nilpotent decomposition of a matrix,Existence of polynomials in the Diagonal-nilpotent decomposition of a matrix,,"Let $V$ be a finite dimensional vector space over $\mathbb{C}$ and $T: V \rightarrow V$ linear operator. Show that there exist polynomials, without constant terms, $g, h \in \mathbb{C}$ such that $g(T)=D_T$ and $h(T)=N_T$, where $D_T$ is a diagonalizable matrix and $N_T$ nilpotent matrix such that $T=D_T+N_T$ and $D_TN_T=N_TD_T$. I have proved that $D_T$ and $N_T$ actually exist by using Jordan Canonical form and now I want to show the existence of the polynomials described above. I want to use the following result to prove the claim: If $f_T=\prod_{i=1}^{r}(t-\lambda_i)^{e_i}$ in $F[t]$ and all the $\lambda_i$ are distinct, then there exists a polynomial $P \in F[t]$ such that $(t-\lambda_i)^{e_i}\mid (P-\lambda_i)$ for all $i$ and $t \mid P$ in $F[t]$. From this result I know that $f_T \mid \prod_{i=1}^{r}(P-\lambda_i)$ and $t^r \mid P^r$. But I don't know how to proceed, any ideas?","Let $V$ be a finite dimensional vector space over $\mathbb{C}$ and $T: V \rightarrow V$ linear operator. Show that there exist polynomials, without constant terms, $g, h \in \mathbb{C}$ such that $g(T)=D_T$ and $h(T)=N_T$, where $D_T$ is a diagonalizable matrix and $N_T$ nilpotent matrix such that $T=D_T+N_T$ and $D_TN_T=N_TD_T$. I have proved that $D_T$ and $N_T$ actually exist by using Jordan Canonical form and now I want to show the existence of the polynomials described above. I want to use the following result to prove the claim: If $f_T=\prod_{i=1}^{r}(t-\lambda_i)^{e_i}$ in $F[t]$ and all the $\lambda_i$ are distinct, then there exists a polynomial $P \in F[t]$ such that $(t-\lambda_i)^{e_i}\mid (P-\lambda_i)$ for all $i$ and $t \mid P$ in $F[t]$. From this result I know that $f_T \mid \prod_{i=1}^{r}(P-\lambda_i)$ and $t^r \mid P^r$. But I don't know how to proceed, any ideas?",,['linear-algebra']
75,Trace of matrix that is a product of 2 others.,Trace of matrix that is a product of 2 others.,,"We consider that $A,B$ are two square matrices.  I would like to know if there is a proof that  $$tr(AB)=tr(BA)$$ I seek for special kind of proof without using sigma notation and matrices multiplication definition because it is obvious then. Is there a more deep meaning for this trace property.? Thanks!!","We consider that $A,B$ are two square matrices.  I would like to know if there is a proof that  $$tr(AB)=tr(BA)$$ I seek for special kind of proof without using sigma notation and matrices multiplication definition because it is obvious then. Is there a more deep meaning for this trace property.? Thanks!!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
76,Proving dimension formula in linear algebra,Proving dimension formula in linear algebra,,"Let $V$ and $W$ be finite dimensional vector spaces and let $T:V \to W$ be a linear transformation. (a) Prove that if $\dim(V) < \dim(W)$ then $T$ cannot be onto. (b) Prove that if $\dim(V) > \dim(W)$ then $T$ cannot be one-to-one. What I tried: (a) Proving by contradiction. Suppose that $T$ is onto. Then, since we are also given that $T$ is linear, then $T$ has to be one-to-one. Thus $T$ is both one-to-one and onto which means $\dim(V) = \dim(W)$ hence contradiction the fact that $\dim(V) < \dim(W)$. (b) Again proving by contradiction, suppose that $T$ is one-to-one. Then we know that $\dim N(T) = 0$. And since $\dim R(T) + \dim N(T) = \dim(V)$, this makes $\dim R(T) = \dim(W)$, and thus $V$ maps onto $W$, which contradicts the fact that $\dim(V) > \dim(W)$ and hence proving the statement. Is my prove correct? Could anyone explain? Also could anyone show me how to do the prove directly instead of using contradiction? Thanks","Let $V$ and $W$ be finite dimensional vector spaces and let $T:V \to W$ be a linear transformation. (a) Prove that if $\dim(V) < \dim(W)$ then $T$ cannot be onto. (b) Prove that if $\dim(V) > \dim(W)$ then $T$ cannot be one-to-one. What I tried: (a) Proving by contradiction. Suppose that $T$ is onto. Then, since we are also given that $T$ is linear, then $T$ has to be one-to-one. Thus $T$ is both one-to-one and onto which means $\dim(V) = \dim(W)$ hence contradiction the fact that $\dim(V) < \dim(W)$. (b) Again proving by contradiction, suppose that $T$ is one-to-one. Then we know that $\dim N(T) = 0$. And since $\dim R(T) + \dim N(T) = \dim(V)$, this makes $\dim R(T) = \dim(W)$, and thus $V$ maps onto $W$, which contradicts the fact that $\dim(V) > \dim(W)$ and hence proving the statement. Is my prove correct? Could anyone explain? Also could anyone show me how to do the prove directly instead of using contradiction? Thanks",,['linear-algebra']
77,Prove that $u\cdot v = 1/4||u+v||^2 - 1/4||u-v||^2$ for all vectors $u$ and $v$ in $\mathbb{R}^n$,Prove that  for all vectors  and  in,u\cdot v = 1/4||u+v||^2 - 1/4||u-v||^2 u v \mathbb{R}^n,"I need some help figuring out how to work through this problem. Prove that $ u \cdot v  = 1/4 ||u + v||^2 - 1/4||u - v||^2$ for all vectors $u$ and $v$ in $\mathbb{R}^n$. Sorry, forgot to include my work so far: I decided to ignore the 1/4 and deal with it later once I had a better understanding of the question. $= ||u+v||^2 - ||u-v||^2$ $= (u+v)(u+v) - (u-v)(u-v)$ $= u(u+v) + v(u+v) - u(u-v) + v(u-v)$ $= uu + uv + uv + vv - uu + uv + uv - vv$ $u \cdot v= 3(uv)$ This is as far as I've gotten, not sure if I'm on the right track or where to go next.","I need some help figuring out how to work through this problem. Prove that $ u \cdot v  = 1/4 ||u + v||^2 - 1/4||u - v||^2$ for all vectors $u$ and $v$ in $\mathbb{R}^n$. Sorry, forgot to include my work so far: I decided to ignore the 1/4 and deal with it later once I had a better understanding of the question. $= ||u+v||^2 - ||u-v||^2$ $= (u+v)(u+v) - (u-v)(u-v)$ $= u(u+v) + v(u+v) - u(u-v) + v(u-v)$ $= uu + uv + uv + vv - uu + uv + uv - vv$ $u \cdot v= 3(uv)$ This is as far as I've gotten, not sure if I'm on the right track or where to go next.",,"['linear-algebra', 'euclidean-geometry']"
78,A linear transformation whose domain has higher dimension than the target space,A linear transformation whose domain has higher dimension than the target space,,"I have a doubt in this question. I would like to check if my answer for letter a is correct and a hint for letter b. Let $T:U\rightarrow V$ be a linear transformation where $U$ and $V$ are vector spaces such that $\dim_\mathbb{K}V<\dim_\mathbb{K}U < \infty$. a) Prove the existence of a nonzero element $u \in U$ such that $T(u) = 0$. b) Let $\mathbb{B}$ be an arbitrary basis of $U$. Does there always exists a vector $u \in \mathbb{B}$ such that $T(u) = 0$? Prove or give a counterexample. My attempt: a) Let $\dim_\mathbb{K}V = n$, $\dim_\mathbb{K}U = m$ and $B_U= \{ u_1, ..., u_n\}$ be a basis of U, so $u$ = $\sum_{i=1}^m \alpha_i u_i \Longrightarrow T(u)  =  \sum_{i=1}^m \alpha_i T(u_i)$ $n = \dim_\mathbb{K}V < dim_\mathbb{K}U = m$, so all lineary independent sets in $V$ have at most 'n' elements, but $A = \{ T(u_1), ..., T(u_m) \}$ has #$A = m > n$, therefore $A$ is lineary dependent. In other words, $\sum_{i=1}^m \alpha_i T(u_i) = 0$ for some $0<i<m+1$. So, there exists a nonzero $u \in U$ such that $T(u) = 0$. b) For this question I have no  idea, but I think I need to use my development in 'a'.","I have a doubt in this question. I would like to check if my answer for letter a is correct and a hint for letter b. Let $T:U\rightarrow V$ be a linear transformation where $U$ and $V$ are vector spaces such that $\dim_\mathbb{K}V<\dim_\mathbb{K}U < \infty$. a) Prove the existence of a nonzero element $u \in U$ such that $T(u) = 0$. b) Let $\mathbb{B}$ be an arbitrary basis of $U$. Does there always exists a vector $u \in \mathbb{B}$ such that $T(u) = 0$? Prove or give a counterexample. My attempt: a) Let $\dim_\mathbb{K}V = n$, $\dim_\mathbb{K}U = m$ and $B_U= \{ u_1, ..., u_n\}$ be a basis of U, so $u$ = $\sum_{i=1}^m \alpha_i u_i \Longrightarrow T(u)  =  \sum_{i=1}^m \alpha_i T(u_i)$ $n = \dim_\mathbb{K}V < dim_\mathbb{K}U = m$, so all lineary independent sets in $V$ have at most 'n' elements, but $A = \{ T(u_1), ..., T(u_m) \}$ has #$A = m > n$, therefore $A$ is lineary dependent. In other words, $\sum_{i=1}^m \alpha_i T(u_i) = 0$ for some $0<i<m+1$. So, there exists a nonzero $u \in U$ such that $T(u) = 0$. b) For this question I have no  idea, but I think I need to use my development in 'a'.",,"['linear-algebra', 'proof-verification', 'linear-transformations']"
79,Find equation of plane given plane point,Find equation of plane given plane point,,"So I am given one plane point $M(5,2,0)$, also two points which are not plane points: $P(6,1,-1)$ distance to plane $1$, also point $Q(0,5,4)$ distance to plane $3$. How find equation of plane with the information given?","So I am given one plane point $M(5,2,0)$, also two points which are not plane points: $P(6,1,-1)$ distance to plane $1$, also point $Q(0,5,4)$ distance to plane $3$. How find equation of plane with the information given?",,"['linear-algebra', 'algebra-precalculus', 'geometry', 'solid-geometry']"
80,A real symmetric matrix $A$ positive definite if all its eigenvalues are positive,A real symmetric matrix  positive definite if all its eigenvalues are positive,A,"Let $A\in \mathbb R^{n \times n},\ A^T=A$ and the eigenvalues $\lambda_i>0$. Then $v^TAv>0$ for every nonzero vector $v$. I know how to prove the above statement by using the fact that if $A$ is real symmetric then there exists an orthonormal basis $\mathcal B=\{v_1,v_2,....,v_n \}$ that consists of eigenvectors of $A$. Is there a proof where one doesn't need $\mathcal B$ in order to prove that $A$ is positive definite?","Let $A\in \mathbb R^{n \times n},\ A^T=A$ and the eigenvalues $\lambda_i>0$. Then $v^TAv>0$ for every nonzero vector $v$. I know how to prove the above statement by using the fact that if $A$ is real symmetric then there exists an orthonormal basis $\mathcal B=\{v_1,v_2,....,v_n \}$ that consists of eigenvectors of $A$. Is there a proof where one doesn't need $\mathcal B$ in order to prove that $A$ is positive definite?",,"['linear-algebra', 'matrices', 'alternative-proof', 'positive-definite']"
81,Inferring Jordan Block structure from dimension of null spaces,Inferring Jordan Block structure from dimension of null spaces,,"Suppose $T- \lambda I$ is a nilpotent linear transformation on vector space $V$ with index of nilpotency $m < \mathrm{dim}V$. Assume we know $n_k = \mathrm{dim}(T - \lambda I)^k$ for $k = 1, 2, \cdots m$. I know that the number of Jordan sub-blocks is $n_ 1 = \mathrm{dim}(T - \lambda I)$. I also understand that the maximum length of a chain or cycle will be $ m $ , in other words, max size of the Jordan sub-blocks is $m$. In many places, additional formulae are stated that give the distribution of Jordan blocks of various sizes if all the $n_k$ are known. What is the proof of such results ? In other words, can someone explain with a proof what information about the Jordan block does $n_2 = \mathrm{dim}(T - \lambda I)^2$ carry? To me, that only means that there are $n_2$ basis vectors of  $\mathrm{dim}(T - \lambda I)^2$, of which $n_1$ are  basis vectors of $\mathrm{dim}(T - \lambda I)$. But how does all this tie up with the number of Jordan blocks of size $2 \times 2$ ?","Suppose $T- \lambda I$ is a nilpotent linear transformation on vector space $V$ with index of nilpotency $m < \mathrm{dim}V$. Assume we know $n_k = \mathrm{dim}(T - \lambda I)^k$ for $k = 1, 2, \cdots m$. I know that the number of Jordan sub-blocks is $n_ 1 = \mathrm{dim}(T - \lambda I)$. I also understand that the maximum length of a chain or cycle will be $ m $ , in other words, max size of the Jordan sub-blocks is $m$. In many places, additional formulae are stated that give the distribution of Jordan blocks of various sizes if all the $n_k$ are known. What is the proof of such results ? In other words, can someone explain with a proof what information about the Jordan block does $n_2 = \mathrm{dim}(T - \lambda I)^2$ carry? To me, that only means that there are $n_2$ basis vectors of  $\mathrm{dim}(T - \lambda I)^2$, of which $n_1$ are  basis vectors of $\mathrm{dim}(T - \lambda I)$. But how does all this tie up with the number of Jordan blocks of size $2 \times 2$ ?",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'nilpotence']"
82,Inversion of n x n matrix. [closed],Inversion of n x n matrix. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is the inverse of this $n \times n$ matrix, which is an $n \times n$ matrix of $1$s minus the $n \times n$ identity matrix.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is the inverse of this $n \times n$ matrix, which is an $n \times n$ matrix of $1$s minus the $n \times n$ identity matrix.",,"['linear-algebra', 'matrices']"
83,Finding matrix such that it has a given kernel.,Finding matrix such that it has a given kernel.,,"Find matrix $A$ over $\Bbb Z_3$ such that $\ker A = \left\langle\begin{pmatrix}   1\\   2\\   0\\   1\\   2\\   \end{pmatrix},\begin{pmatrix}   1\\   1\\   1\\   1\\   1\\   \end{pmatrix},\begin{pmatrix}   1\\   0\\   2\\   1\\   0\\   \end{pmatrix}\right\rangle$ . I am confused by this problem. I guess I am not sure what the task is. I know how to calculate $\ker A$ for a matrix but what is this telling me? $\ker A$ means $Ax = 0$ but how do you proceed when you have the set of solutions like this instead of the matrix? Thank you.",Find matrix over such that . I am confused by this problem. I guess I am not sure what the task is. I know how to calculate for a matrix but what is this telling me? means but how do you proceed when you have the set of solutions like this instead of the matrix? Thank you.,"A \Bbb Z_3 \ker A = \left\langle\begin{pmatrix}
  1\\
  2\\
  0\\
  1\\
  2\\
  \end{pmatrix},\begin{pmatrix}
  1\\
  1\\
  1\\
  1\\
  1\\
  \end{pmatrix},\begin{pmatrix}
  1\\
  0\\
  2\\
  1\\
  0\\
  \end{pmatrix}\right\rangle \ker A \ker A Ax = 0","['linear-algebra', 'matrices', 'finite-fields']"
84,Trace of Product of Powers of $A$ and $A^\ast$,Trace of Product of Powers of  and,A A^\ast,"Let $n$ be odd, $\displaystyle v=1,...,\frac{n-1}{2}$ and $\displaystyle \zeta=e^{2\pi i/n}$. Define the following matrices: $$A(0,v)=\left(\begin{array}{cc}1+\zeta^{-v} & \zeta^v+\zeta^{2v}\\ \zeta^{-v}+\zeta^{-2v}&1+\zeta^{v}\end{array}\right),$$ $$A(1,v)=\left(\begin{array}{cc}\zeta^{-1}+\zeta^{-v} & \zeta^{v}\\ \zeta^{-v}&\zeta^{-1}+\zeta^{v}\end{array}\right).$$ $$A(n-1,v)=\left(\begin{array}{cc}\zeta+\zeta^{-v} & \zeta^{2v}\\ \zeta^{-2v}&\zeta+\zeta^v\end{array}\right).$$ I am hoping to calculate for each of these $A$ $$\text{Tr}\left[\left(A^k\right)^*A^k\right]=\text{Tr}\left[\left(A^*\right)^kA^k\right].$$ All I have is that $A$ and $A^*$ in general do not commute so I can't simultaneously diagonalise them necessarily. I do know that if we write $A=D+(A-D)$ (with $D$ diagonal), that $$A^*=\overline{D}+(A-D).$$ I suppose anybody who knows anything about calculating $$\text{Tr}(A^kB^k)$$ can help. Context: I need to calculate or rather bound these traces to calculate a distance to random for the convolution powers of a $\nu\in M_p(\mathbb{G}_n)$ for $\mathbb{G}_n$ a series of quantum groups of dimension $2n^2$ ($n$ odd). For $u=2,...,k-2$, $A(u,v)$ is diagonal so no problems there.","Let $n$ be odd, $\displaystyle v=1,...,\frac{n-1}{2}$ and $\displaystyle \zeta=e^{2\pi i/n}$. Define the following matrices: $$A(0,v)=\left(\begin{array}{cc}1+\zeta^{-v} & \zeta^v+\zeta^{2v}\\ \zeta^{-v}+\zeta^{-2v}&1+\zeta^{v}\end{array}\right),$$ $$A(1,v)=\left(\begin{array}{cc}\zeta^{-1}+\zeta^{-v} & \zeta^{v}\\ \zeta^{-v}&\zeta^{-1}+\zeta^{v}\end{array}\right).$$ $$A(n-1,v)=\left(\begin{array}{cc}\zeta+\zeta^{-v} & \zeta^{2v}\\ \zeta^{-2v}&\zeta+\zeta^v\end{array}\right).$$ I am hoping to calculate for each of these $A$ $$\text{Tr}\left[\left(A^k\right)^*A^k\right]=\text{Tr}\left[\left(A^*\right)^kA^k\right].$$ All I have is that $A$ and $A^*$ in general do not commute so I can't simultaneously diagonalise them necessarily. I do know that if we write $A=D+(A-D)$ (with $D$ diagonal), that $$A^*=\overline{D}+(A-D).$$ I suppose anybody who knows anything about calculating $$\text{Tr}(A^kB^k)$$ can help. Context: I need to calculate or rather bound these traces to calculate a distance to random for the convolution powers of a $\nu\in M_p(\mathbb{G}_n)$ for $\mathbb{G}_n$ a series of quantum groups of dimension $2n^2$ ($n$ odd). For $u=2,...,k-2$, $A(u,v)$ is diagonal so no problems there.",,"['linear-algebra', 'matrices', 'roots-of-unity', 'quantum-groups']"
85,What is the difference between Linear Least Squares and Ordinary Least Squares?,What is the difference between Linear Least Squares and Ordinary Least Squares?,,"My understanding is that Ordinary Least Squares (Usually taught in Statistics classes) uses the vertical distance only when minimizing error/residuals (see Wikipedia for Ordinary Least Squares ) with a modeled line. On the other hand, Linear Least Squares (Usually taught in Linear Algebra classes) uses vertical and horizontal distance components when minimizing the error/residuals (See Wikipedia for Linear Least Squares ) with the modeled line, in effect minimizing the ""closest"" distance. Is this correct? Normally would one expect to get the same estimation of parameters for a linear model?","My understanding is that Ordinary Least Squares (Usually taught in Statistics classes) uses the vertical distance only when minimizing error/residuals (see Wikipedia for Ordinary Least Squares ) with a modeled line. On the other hand, Linear Least Squares (Usually taught in Linear Algebra classes) uses vertical and horizontal distance components when minimizing the error/residuals (See Wikipedia for Linear Least Squares ) with the modeled line, in effect minimizing the ""closest"" distance. Is this correct? Normally would one expect to get the same estimation of parameters for a linear model?",,"['linear-algebra', 'statistics', 'regression', 'least-squares']"
86,"For a set of symmetric matrices $A_i$ of order p, show that if the sum of their ranks is p, $A_iA_j=0$","For a set of symmetric matrices  of order p, show that if the sum of their ranks is p,",A_i A_iA_j=0,"Here's what I know. Matrices $A_i$ for $i=1,...,k$ are all symmetric p by p matrices. $\sum\limits_{i=1}^k A_i = I_p$ where $I_p$ is the p by p identity matrix $\sum\limits_{i=1}^k rank(A_i) = p$ With this, I have to find a way to show that for all $i \neq j$, $A_iA_j=0$. I assume this is solved by showing that $rank(A_iA_j)=0$ but every attempt that I've made to do that goes nowhere. Can anyone point me in the right direction?","Here's what I know. Matrices $A_i$ for $i=1,...,k$ are all symmetric p by p matrices. $\sum\limits_{i=1}^k A_i = I_p$ where $I_p$ is the p by p identity matrix $\sum\limits_{i=1}^k rank(A_i) = p$ With this, I have to find a way to show that for all $i \neq j$, $A_iA_j=0$. I assume this is solved by showing that $rank(A_iA_j)=0$ but every attempt that I've made to do that goes nowhere. Can anyone point me in the right direction?",,"['linear-algebra', 'matrices', 'vector-spaces', 'equivalence-relations', 'matrix-rank']"
87,The $2$-norm of a submatrix does not exceed the norm of the matrix,The -norm of a submatrix does not exceed the norm of the matrix,2,"Let $A \in \mathbb{R}^{n\times n}$ and let $1\le  i_1 \le i_2 \le n$. Let $B$ be the submatrix with indices $(i_1:i_2,i_1:i_2)$. Prove that $\|B\|2\le \|A\|_2$. Can I say since $B$ is submatrix of $A$ then $\|B\|_2\le \|A\|_2$? Or should I prove more? And how?","Let $A \in \mathbb{R}^{n\times n}$ and let $1\le  i_1 \le i_2 \le n$. Let $B$ be the submatrix with indices $(i_1:i_2,i_1:i_2)$. Prove that $\|B\|2\le \|A\|_2$. Can I say since $B$ is submatrix of $A$ then $\|B\|_2\le \|A\|_2$? Or should I prove more? And how?",,"['linear-algebra', 'matrices', 'normed-spaces']"
88,Curiosity about the wedge product and the Levi-Civita symbol,Curiosity about the wedge product and the Levi-Civita symbol,,"Let us use the definition of the wedge product of two vectors: $$\vec{u}\wedge\vec{v} = \vec{u}\otimes\vec{v} - \vec{v}\otimes\vec{u}$$ writing $\vec{u}$ and $\vec{v}$ in dyadic form as $\vec{u} = u_a\vec{e}^a$ and $\vec{v} = v_a\vec{e}^a$ the wedge product above yields: $$\vec{u}\wedge\vec{v} = u_av_b\vec{e}^a \otimes \vec{e}^b - u_av_b\vec{e}^b \otimes \vec{e}^a$$ $$ = u_av_b(\delta_l^a\delta_m^b - \delta_l^b\delta_m^a)\vec{e}^l \otimes \vec{e}^m$$ $$ = u_av_b\epsilon_{lm}^{ab}\vec{e}^l \otimes \vec{e}^m = u_av_b\epsilon^{ab}\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m$$ Assuming all the above is correct, expanding the $a,b$ contraction above in say $\Bbb{R}^2$ does indeed give $u_av_b\epsilon^{ab} = u_1v_2 - u_2v_1$ and on the other hand the second contraction on the basis gives $\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m = \vec{e}^1\otimes\vec{e}^2 - \vec{e}^2\otimes\vec{e}^1 = \vec{e}^1\wedge\vec{e}^2$. And I believe the above still holds even when the indices are in $\{1,2,3\}$ My question(s) is(are): following a similar analogy and trying to stick to as little 'new' notation as possible, extending the above to the wedge product of multiple vectors is proving problematic for me. And I cant seem to find any text that connects the wedge product to the Levi-Civita symbol intimately. So what is the definition for the product $\vec{u}\wedge\vec{v}\wedge\vec{w}$ were the vectors are in $\Bbb{R}^3$ in similar notation to above if correct?","Let us use the definition of the wedge product of two vectors: $$\vec{u}\wedge\vec{v} = \vec{u}\otimes\vec{v} - \vec{v}\otimes\vec{u}$$ writing $\vec{u}$ and $\vec{v}$ in dyadic form as $\vec{u} = u_a\vec{e}^a$ and $\vec{v} = v_a\vec{e}^a$ the wedge product above yields: $$\vec{u}\wedge\vec{v} = u_av_b\vec{e}^a \otimes \vec{e}^b - u_av_b\vec{e}^b \otimes \vec{e}^a$$ $$ = u_av_b(\delta_l^a\delta_m^b - \delta_l^b\delta_m^a)\vec{e}^l \otimes \vec{e}^m$$ $$ = u_av_b\epsilon_{lm}^{ab}\vec{e}^l \otimes \vec{e}^m = u_av_b\epsilon^{ab}\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m$$ Assuming all the above is correct, expanding the $a,b$ contraction above in say $\Bbb{R}^2$ does indeed give $u_av_b\epsilon^{ab} = u_1v_2 - u_2v_1$ and on the other hand the second contraction on the basis gives $\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m = \vec{e}^1\otimes\vec{e}^2 - \vec{e}^2\otimes\vec{e}^1 = \vec{e}^1\wedge\vec{e}^2$. And I believe the above still holds even when the indices are in $\{1,2,3\}$ My question(s) is(are): following a similar analogy and trying to stick to as little 'new' notation as possible, extending the above to the wedge product of multiple vectors is proving problematic for me. And I cant seem to find any text that connects the wedge product to the Levi-Civita symbol intimately. So what is the definition for the product $\vec{u}\wedge\vec{v}\wedge\vec{w}$ were the vectors are in $\Bbb{R}^3$ in similar notation to above if correct?",,"['linear-algebra', 'differential-forms', 'tensors', 'multilinear-algebra']"
89,Property of elements of a positive definite matrix,Property of elements of a positive definite matrix,,"I am stuck in proving a property of a positive definite matrix. Let $A$ be a positive definite matrix. Then:   $$|A_{ij}| \le \sqrt{A_{ii}A_{jj}} \le \frac{A_{ii}+A_{jj}}{2}.$$ My work: As for every non zero x, $x^TAx \gt 0$, consider $x^T = \begin{bmatrix}0& 0 &0 &\cdots& 0 &1& 0&\cdots&0 &-1& 0&\cdots&0 \end{bmatrix}$ with $1$  at position $i$ and $-1$ at position $j$.  $$x^TAx = A_{ii}+A_{jj}-2*A_{ij} \gt 0.$$ Need help in completing the proof.","I am stuck in proving a property of a positive definite matrix. Let $A$ be a positive definite matrix. Then:   $$|A_{ij}| \le \sqrt{A_{ii}A_{jj}} \le \frac{A_{ii}+A_{jj}}{2}.$$ My work: As for every non zero x, $x^TAx \gt 0$, consider $x^T = \begin{bmatrix}0& 0 &0 &\cdots& 0 &1& 0&\cdots&0 &-1& 0&\cdots&0 \end{bmatrix}$ with $1$  at position $i$ and $-1$ at position $j$.  $$x^TAx = A_{ii}+A_{jj}-2*A_{ij} \gt 0.$$ Need help in completing the proof.",,"['linear-algebra', 'matrices', 'inequality']"
90,Fibonacci sequence and eigenvalues,Fibonacci sequence and eigenvalues,,"I'm learning about eigenvectors and values, and one of the excercises in my book tackles the fibonacci recursion from this angle. Let $F = \begin{bmatrix}1&1\\1&0\end{bmatrix}^n \quad\text{ then }\quad \begin{bmatrix}x_{n+1}\\x_n\end{bmatrix} = F^n \begin{bmatrix}x_1\\x_0\end{bmatrix} $ To obtain a closed formula for $x_n$, I've obtained the eigenvalues and vectors of $F$, diagonalized it into $PDP^{-1}$ (where $P$ is the eigenvector matrix and $D$ is $\operatorname{diag}(\operatorname{eig}(F))$ and calculated the product of $PD^nP^{-1}$. I feel like I'm missing something.  The eigenvalues are there in the closed formula. Is there a way to deduce the formula from the eigenvalues without calculating  $P^{-1}$ and $PDP^{-1}$?","I'm learning about eigenvectors and values, and one of the excercises in my book tackles the fibonacci recursion from this angle. Let $F = \begin{bmatrix}1&1\\1&0\end{bmatrix}^n \quad\text{ then }\quad \begin{bmatrix}x_{n+1}\\x_n\end{bmatrix} = F^n \begin{bmatrix}x_1\\x_0\end{bmatrix} $ To obtain a closed formula for $x_n$, I've obtained the eigenvalues and vectors of $F$, diagonalized it into $PDP^{-1}$ (where $P$ is the eigenvector matrix and $D$ is $\operatorname{diag}(\operatorname{eig}(F))$ and calculated the product of $PD^nP^{-1}$. I feel like I'm missing something.  The eigenvalues are there in the closed formula. Is there a way to deduce the formula from the eigenvalues without calculating  $P^{-1}$ and $PDP^{-1}$?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'fibonacci-numbers']"
91,Is it a coincidence that the jacobian matrix of differentiable complex functions is also the matrix isomorphic to complex numbers?,Is it a coincidence that the jacobian matrix of differentiable complex functions is also the matrix isomorphic to complex numbers?,,"You can ""represent"" complex numbers with 2x2 matrices, with the isomorphism between the fields $(\Bbb C,+,\times)$ and $(\begin{pmatrix} a & -b \\ b & a \end{pmatrix},+,\times)$ : $$f:a+bi\mapsto\begin{pmatrix} a & -b \\ b & a \end{pmatrix}$$ with $a,b\in\Bbb R$ . Additionally, a complex function can be represented by $f(x+iy) = u +iv$ , or that $f$ is composed of the two functions $u(x,y)$ and $v(x,y)$ . The differentiable function $f$ has the property that its jacobian matrix $\begin{pmatrix} \partial u/\partial x & \partial v/\partial x \\ \partial u/\partial y & \partial v/\partial y \end{pmatrix}$ must be of the form $$\begin{pmatrix} a & -b \\ b & a \end{pmatrix}$$ Is this just a coincidence, or can we generalize this? For example, a split complex number $a+jb$ is represented by $\begin{pmatrix} a & b \\ b & a \end{pmatrix}$ . Would this matrix also have to do with the partial derivatives of a split-complex function? (Although I have not a slightest idea what it might mean to take the derivative of such a thing.)","You can ""represent"" complex numbers with 2x2 matrices, with the isomorphism between the fields and : with . Additionally, a complex function can be represented by , or that is composed of the two functions and . The differentiable function has the property that its jacobian matrix must be of the form Is this just a coincidence, or can we generalize this? For example, a split complex number is represented by . Would this matrix also have to do with the partial derivatives of a split-complex function? (Although I have not a slightest idea what it might mean to take the derivative of such a thing.)","(\Bbb C,+,\times) (\begin{pmatrix} a & -b \\ b & a \end{pmatrix},+,\times) f:a+bi\mapsto\begin{pmatrix} a & -b \\ b & a \end{pmatrix} a,b\in\Bbb R f(x+iy) = u +iv f u(x,y) v(x,y) f \begin{pmatrix} \partial u/\partial x & \partial v/\partial x \\ \partial u/\partial y & \partial v/\partial y \end{pmatrix} \begin{pmatrix} a & -b \\ b & a \end{pmatrix} a+jb \begin{pmatrix} a & b \\ b & a \end{pmatrix}","['linear-algebra', 'complex-numbers']"
92,Jordan normal form over $\mathbb{C}$,Jordan normal form over,\mathbb{C},"Let there be $T:\mathbb{C}^8\rightarrow \mathbb{C}^8$ Such that $ T\left(\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \\ x_{5} \\ x_{6} \\ x_{7} \\ x_{8} \end{array}\right)=\left(\begin{array}{c} 2\,x_{8}+x_{7}+3\,x_{6}+5\,x_{5}-x_{4}+3\,x_{3}+x_{2}+5\,x_{1} \\ 4\,x_{8}+5\,x_{7}+3\,x_{6}+4\,x_{5}-3\,x_{4}-2\,x_{3}+3\,x_{2}-3\,x_{1} \\ 5\,x_{8}+x_{7}-2\,x_{6}-3\,x_{5}+3\,x_{4}+5\,x_{3}-3\,x_{2}-3\,x_{1} \\ -2\,x_{8}+2\,x_{7}-x_{6}+2\,x_{5}-2\,x_{4}+5\,x_{3}+5\,x_{2}+x_{1} \\ -x_{8}-2\,x_{7}-3\,x_{6}+5\,x_{5}+x_{4}+4\,x_{3}-5\,x_{2}-3\,x_{1} \\ 5\,x_{8}-x_{7}+5\,x_{6}-3\,x_{5}-5\,x_{4}-3\,x_{3}-5\,x_{2}-3\,x_{1} \\ -3\,x_{8}+4\,x_{7}-4\,x_{6}+4\,x_{5}+5\,x_{4}+4\,x_{3}+2\,x_{2}-2\,x_{1} \\ -5\,x_{8}-2\,x_{7}+4\,x_{6}+x_{5}+4\,x_{4}-3\,x_{3}+5\,x_{2}-x_{1} \end{array}\right)$ Does $T$ has a Jordan normal form over $\mathbb{C}$? Doesn't over $\mathbb{C}$ all matrices have a Jordan normal form?","Let there be $T:\mathbb{C}^8\rightarrow \mathbb{C}^8$ Such that $ T\left(\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \\ x_{5} \\ x_{6} \\ x_{7} \\ x_{8} \end{array}\right)=\left(\begin{array}{c} 2\,x_{8}+x_{7}+3\,x_{6}+5\,x_{5}-x_{4}+3\,x_{3}+x_{2}+5\,x_{1} \\ 4\,x_{8}+5\,x_{7}+3\,x_{6}+4\,x_{5}-3\,x_{4}-2\,x_{3}+3\,x_{2}-3\,x_{1} \\ 5\,x_{8}+x_{7}-2\,x_{6}-3\,x_{5}+3\,x_{4}+5\,x_{3}-3\,x_{2}-3\,x_{1} \\ -2\,x_{8}+2\,x_{7}-x_{6}+2\,x_{5}-2\,x_{4}+5\,x_{3}+5\,x_{2}+x_{1} \\ -x_{8}-2\,x_{7}-3\,x_{6}+5\,x_{5}+x_{4}+4\,x_{3}-5\,x_{2}-3\,x_{1} \\ 5\,x_{8}-x_{7}+5\,x_{6}-3\,x_{5}-5\,x_{4}-3\,x_{3}-5\,x_{2}-3\,x_{1} \\ -3\,x_{8}+4\,x_{7}-4\,x_{6}+4\,x_{5}+5\,x_{4}+4\,x_{3}+2\,x_{2}-2\,x_{1} \\ -5\,x_{8}-2\,x_{7}+4\,x_{6}+x_{5}+4\,x_{4}-3\,x_{3}+5\,x_{2}-x_{1} \end{array}\right)$ Does $T$ has a Jordan normal form over $\mathbb{C}$? Doesn't over $\mathbb{C}$ all matrices have a Jordan normal form?",,"['linear-algebra', 'jordan-normal-form']"
93,3D projection coordinates onto 2D plane to determine transformation matrix?,3D projection coordinates onto 2D plane to determine transformation matrix?,,"I'm not sure if there is an actual solution to this problem or not, but thought I would give it a shot here to see if anyone has any ideas. So here goes: I basically have three vertices of a rigid triangle with known 3D coordinates. The vertices are projected onto a 2D plane (by projection, I mean that each vertex would basically have a fixed line drawn from it to the 2D plane, and that ""line"" would also stay rigid to the triangle so that the lines would move along with the triangle if it is transformed), in which I also know the 2D coordinates. A transformation matrix is applied to the original three points (can be a combination of rotation and translation) and I now know the new 2D projection coordinates. Is it possible to obtain either the unknown transformation matrix or the new coordinates? Any ideas are much appreciated. Thanks!","I'm not sure if there is an actual solution to this problem or not, but thought I would give it a shot here to see if anyone has any ideas. So here goes: I basically have three vertices of a rigid triangle with known 3D coordinates. The vertices are projected onto a 2D plane (by projection, I mean that each vertex would basically have a fixed line drawn from it to the 2D plane, and that ""line"" would also stay rigid to the triangle so that the lines would move along with the triangle if it is transformed), in which I also know the 2D coordinates. A transformation matrix is applied to the original three points (can be a combination of rotation and translation) and I now know the new 2D projection coordinates. Is it possible to obtain either the unknown transformation matrix or the new coordinates? Any ideas are much appreciated. Thanks!",,"['linear-algebra', 'geometry', 'projective-geometry']"
94,An efficient way to find anagrams,An efficient way to find anagrams,,"Consider a set of words where you want to divide the set into subsets of words, where all members of each subset are anagrams (same letters, different arrangement). You can do this computationally in several ways, the ""best"" way is sorting the letters from A through Z in each word. If two words are equal when their respective letters are sorted, then they must be anagrams. Otherwise, they are not. For the general case we cannot do any better. The sorting, which is $\mathcal{O}(M \log M)$ for each word of length $M$, is the computationally hardest part. But here is something I've been discussing. Suppose we know there is a longest word with $M$ characters. Then imagine we have a so-called ""hash function"" $f$ which maps a word to a positive integer. Let each letter A,B,C,...,Z have a specific weight $a_1,a_2,a_3,\dots,a_{26}$. For a word $x$, the value $f(x)$ is the sum of the corresponding weights without regard to their position within the word. $f(ZABC) = a_{26} + a_1 + a_2 + a_3$. This has linear complexity, $\mathcal{O}(M)$ for each word. If we can select these weights such that only words that are anagrams map to the same value, then a computer can easily calculate whether two words are anagrams by looking at their respective function value. The question is of course, how do we choose the weights ? I can formulate the problem as such: For fixed $M$ and $N$, find $\{a_1,a_2,\dots,a_{N}\}$, where $a_1 < a_2 < \dots < a_{N}$, such that $$c_ia_i \neq c_1a_1 + c_2a_2 + \dots + c_{i-1}a_{i-1} + c_{i+1}a_{i+1} + \dots + c_{N}a_{N}$$ for any combination of $c_j \in \{0,1,2,\dots,M\}$ except $c_1=c_2=\dots=c_{N}=0$. Basically, we want to find a linearly independent $N$-subset of $\Bbb Z_{>0}$ subject to a constraint on the coefficients. Is there a clever method for this? A very simple solution is to let $a_1 = 1$ and recursively define $a_{i+1} = Ma_i + 1$ but this grows very quickly and $a_{26}$ is too large for use in computers. For small $M$ and $N$ I have been able to find better solutions. Is there a better solution for $N=26$ and say $M \approx 15$? What is the smallest possible $a_{26}$?","Consider a set of words where you want to divide the set into subsets of words, where all members of each subset are anagrams (same letters, different arrangement). You can do this computationally in several ways, the ""best"" way is sorting the letters from A through Z in each word. If two words are equal when their respective letters are sorted, then they must be anagrams. Otherwise, they are not. For the general case we cannot do any better. The sorting, which is $\mathcal{O}(M \log M)$ for each word of length $M$, is the computationally hardest part. But here is something I've been discussing. Suppose we know there is a longest word with $M$ characters. Then imagine we have a so-called ""hash function"" $f$ which maps a word to a positive integer. Let each letter A,B,C,...,Z have a specific weight $a_1,a_2,a_3,\dots,a_{26}$. For a word $x$, the value $f(x)$ is the sum of the corresponding weights without regard to their position within the word. $f(ZABC) = a_{26} + a_1 + a_2 + a_3$. This has linear complexity, $\mathcal{O}(M)$ for each word. If we can select these weights such that only words that are anagrams map to the same value, then a computer can easily calculate whether two words are anagrams by looking at their respective function value. The question is of course, how do we choose the weights ? I can formulate the problem as such: For fixed $M$ and $N$, find $\{a_1,a_2,\dots,a_{N}\}$, where $a_1 < a_2 < \dots < a_{N}$, such that $$c_ia_i \neq c_1a_1 + c_2a_2 + \dots + c_{i-1}a_{i-1} + c_{i+1}a_{i+1} + \dots + c_{N}a_{N}$$ for any combination of $c_j \in \{0,1,2,\dots,M\}$ except $c_1=c_2=\dots=c_{N}=0$. Basically, we want to find a linearly independent $N$-subset of $\Bbb Z_{>0}$ subject to a constraint on the coefficients. Is there a clever method for this? A very simple solution is to let $a_1 = 1$ and recursively define $a_{i+1} = Ma_i + 1$ but this grows very quickly and $a_{26}$ is too large for use in computers. For small $M$ and $N$ I have been able to find better solutions. Is there a better solution for $N=26$ and say $M \approx 15$? What is the smallest possible $a_{26}$?",,"['linear-algebra', 'combinatorics', 'computational-mathematics', 'hash-function']"
95,"A result about commuting matrices in $ M(n, \mathbb{C} ) $",A result about commuting matrices in," M(n, \mathbb{C} ) ","Let $ A $ be a matrix in $ M(n, \mathbb{C} ) $ and let $ A^{*} $ be its Hermitian adjoint. Suppose that the matrices $ A $ and $ AA^{*}-A^{*}A $ commute. Show that $ AA^{*} = A^{*}A $. Here is a sketch of my solution : Lemma : Let $ F $ be a field of characteristic $ 0 $ and $ S,T \in M(n, F) $. Suppose that $ ST-TS $ commutes with $ S $. Then, $ ST-TS $ is nilpotent. We have that $ A $ and $ B = AA^{*} - A^{*}A $ commute, so by the lemma, $ B $ is nilpotent. Also, $ B^{*} = AA^{*} - A^{*}A = B $, so $ B $ is Hermitian. But, $ B $ being both, nilpotent and Hermitian $ \implies $ $ B=0 $ $ \implies $ $ AA^{*} = A^{*}A $ I was wondering whether there is a simpler solution as mine uses some machinery. Any hints please?","Let $ A $ be a matrix in $ M(n, \mathbb{C} ) $ and let $ A^{*} $ be its Hermitian adjoint. Suppose that the matrices $ A $ and $ AA^{*}-A^{*}A $ commute. Show that $ AA^{*} = A^{*}A $. Here is a sketch of my solution : Lemma : Let $ F $ be a field of characteristic $ 0 $ and $ S,T \in M(n, F) $. Suppose that $ ST-TS $ commutes with $ S $. Then, $ ST-TS $ is nilpotent. We have that $ A $ and $ B = AA^{*} - A^{*}A $ commute, so by the lemma, $ B $ is nilpotent. Also, $ B^{*} = AA^{*} - A^{*}A = B $, so $ B $ is Hermitian. But, $ B $ being both, nilpotent and Hermitian $ \implies $ $ B=0 $ $ \implies $ $ AA^{*} = A^{*}A $ I was wondering whether there is a simpler solution as mine uses some machinery. Any hints please?",,['linear-algebra']
96,Double integral of a product in calculus of variations,Double integral of a product in calculus of variations,,"Let's say I have an integral of the form $$   V(u) = \iint\limits_{[0,T]^2}f(x,y)u(x)u(y)\mathrm dx\mathrm dy $$ which I would like to optimize over smooth functions $u$. For the variation I get $$   \delta V(u) \approx \iint\limits_{[0,T]^2}f(x,y)[u(x)\delta u(y) + u(y)\delta u(x)]\mathrm dx\mathrm dy $$ since I hope the term $\delta u(x)\delta u(y)$ can be disregarded. How can I find the optimal $u$ now? In the 1d case I'd have $$   F(u) = \int_0^T G(x,u(x))\mathrm dx \quad \implies \quad \delta F(u) \approx \int_0^TG_u(x, u(x))\delta u(x)\mathrm dx $$ and by fundamental lemma of calculus of variations I would get $G_u(x, u(x)) = 0$. However, I am not sure whether this passage is extendable to my case. Fwiw, my original case is a bit more complicated: I deal with an integral $$   W(u) = V(u) + F(u), $$ and since I know how to variate the latter part, I hope it is enough to know how to variate the former. Would be happy to hear any suggestions. I tag it as LA, since the $V$ functional is a bilinear form, so maybe there are some results in LA known for that case.","Let's say I have an integral of the form $$   V(u) = \iint\limits_{[0,T]^2}f(x,y)u(x)u(y)\mathrm dx\mathrm dy $$ which I would like to optimize over smooth functions $u$. For the variation I get $$   \delta V(u) \approx \iint\limits_{[0,T]^2}f(x,y)[u(x)\delta u(y) + u(y)\delta u(x)]\mathrm dx\mathrm dy $$ since I hope the term $\delta u(x)\delta u(y)$ can be disregarded. How can I find the optimal $u$ now? In the 1d case I'd have $$   F(u) = \int_0^T G(x,u(x))\mathrm dx \quad \implies \quad \delta F(u) \approx \int_0^TG_u(x, u(x))\delta u(x)\mathrm dx $$ and by fundamental lemma of calculus of variations I would get $G_u(x, u(x)) = 0$. However, I am not sure whether this passage is extendable to my case. Fwiw, my original case is a bit more complicated: I deal with an integral $$   W(u) = V(u) + F(u), $$ and since I know how to variate the latter part, I hope it is enough to know how to variate the former. Would be happy to hear any suggestions. I tag it as LA, since the $V$ functional is a bilinear form, so maybe there are some results in LA known for that case.",,"['linear-algebra', 'optimization', 'calculus-of-variations']"
97,Show that no $T\in M_{5\times 5}(\mathbb{Q})$ has order $8$.,Show that no  has order .,T\in M_{5\times 5}(\mathbb{Q}) 8,"Before I get too far, I'll say that I think the above statement is incorrect. Assume that $T\in M_{5\times 5}(\mathbb{Q})$, with $|T|=8$, and let $f(x)=x^8-1$. Since $f(T) = 0$, it follows that, if $m(x)$ is the minimal polynomial of $T$, then we must have $m(x)$ dividing $f(x)=x^8-1=(x-1)(x+1)(x^2+1)(x^4+1)$. Since $T\in M_{5\times 5}(\mathbb{Q})$, we must also have $\deg(m)\leq 5$, and so $m(x)=(x-1)^{e_1}(x+1)^{e_2}(x^2+1)^{e_3}(x^4+1)^{e_4}$, with each $e_i \in \{0,1\}$. If $e_4 = 0$, then $|T|\leq 4$; so $x^4 + 1$ must divide $m(x)$. As the largest of $T$'s invariant factors, $m(x)$ must be divisible by any smaller invariant factors; however, $x^4+1$ is irreducible in $\mathbb{Q}[x]$. And so, recalling our constraint on the degree of $m$, we must have $m(x) = (x-1)^{e_1}(x+1)^{e_2}(x^4+1)$, where $e_1+e_2 =1$. (By the way, $m(x)=p(x)$, the characteristic polynomial of $T$.) At this point, my first time through the problem, I still thought I was proving the statement and wanted to show a contradiction. I wasn't sure what to do, so, using each of the possible minimal polynomials, I put $T$ in rational canonical form and plugged it into my TI-82, raising it to the eighth power, and sure enough got the identity matrix. Moreover, $T^4 \neq I$. I've spent the summer feeling like a big stupid in algebra, so although my reasoning seems pretty tight to me, I can't help but think I'm missing something. Am I? (The question comes from a qual-prep seminar I'm sitting in on.) If the statement is false, does my logic check out? And what conditions would make it a true statement? It doesn't look like it works if $T$ must have order $6$; it seems as though it does hold for $T\in M_{3\times 3}(\mathbb{Q})$ (again, with $|T|=8$).","Before I get too far, I'll say that I think the above statement is incorrect. Assume that $T\in M_{5\times 5}(\mathbb{Q})$, with $|T|=8$, and let $f(x)=x^8-1$. Since $f(T) = 0$, it follows that, if $m(x)$ is the minimal polynomial of $T$, then we must have $m(x)$ dividing $f(x)=x^8-1=(x-1)(x+1)(x^2+1)(x^4+1)$. Since $T\in M_{5\times 5}(\mathbb{Q})$, we must also have $\deg(m)\leq 5$, and so $m(x)=(x-1)^{e_1}(x+1)^{e_2}(x^2+1)^{e_3}(x^4+1)^{e_4}$, with each $e_i \in \{0,1\}$. If $e_4 = 0$, then $|T|\leq 4$; so $x^4 + 1$ must divide $m(x)$. As the largest of $T$'s invariant factors, $m(x)$ must be divisible by any smaller invariant factors; however, $x^4+1$ is irreducible in $\mathbb{Q}[x]$. And so, recalling our constraint on the degree of $m$, we must have $m(x) = (x-1)^{e_1}(x+1)^{e_2}(x^4+1)$, where $e_1+e_2 =1$. (By the way, $m(x)=p(x)$, the characteristic polynomial of $T$.) At this point, my first time through the problem, I still thought I was proving the statement and wanted to show a contradiction. I wasn't sure what to do, so, using each of the possible minimal polynomials, I put $T$ in rational canonical form and plugged it into my TI-82, raising it to the eighth power, and sure enough got the identity matrix. Moreover, $T^4 \neq I$. I've spent the summer feeling like a big stupid in algebra, so although my reasoning seems pretty tight to me, I can't help but think I'm missing something. Am I? (The question comes from a qual-prep seminar I'm sitting in on.) If the statement is false, does my logic check out? And what conditions would make it a true statement? It doesn't look like it works if $T$ must have order $6$; it seems as though it does hold for $T\in M_{3\times 3}(\mathbb{Q})$ (again, with $|T|=8$).",,"['linear-algebra', 'solution-verification']"
98,"Does it make sense to talk about complex matrices over the field of real numbers, R?","Does it make sense to talk about complex matrices over the field of real numbers, R?",,"I don't see an issue with considering a vector space of complex matrices over R -- addition of matrices makes sense, but scalar multiplication will be done with real numbers. But I wanted to ask, just in case I may have it wrong. Thanks, Edit: As far as I know, the entries being complex does not make it a vector space over C - it is the numbers that we are using to scale the matrices that determine the ground field.  (But like I said, I may have it wrong...)","I don't see an issue with considering a vector space of complex matrices over R -- addition of matrices makes sense, but scalar multiplication will be done with real numbers. But I wanted to ask, just in case I may have it wrong. Thanks, Edit: As far as I know, the entries being complex does not make it a vector space over C - it is the numbers that we are using to scale the matrices that determine the ground field.  (But like I said, I may have it wrong...)",,"['linear-algebra', 'field-theory']"
99,Proof of determinant formula,Proof of determinant formula,,"I have just started to learn how to construct proofs. That is, I am not really good at it (yet). In this thread I will work through a problem from my Linear Algebra textbook. First i will give you my ""solution"" and then, hopefully, you can tell me where I went wrong. If my proof strategy for this case is wrong I would love to hear why it's wrong (if it is possible) since I think that is how I will become better :) My textbook says an often good proof strategy of different determinant formulas is by Mathematical induction and I think it also works in this case, but as i said earlier, I am not too good at constructing proofs yet. Problem: Let $X, Y$ be column-vectors. Show that $det(I+XY^T)=1+Y^TX$, where the last product is interpreted as a number. Ok so here is my attempt to solve the problem: Proof strategy: Induction 1. Base case: The statement is true when n=2, since: $$I=\left( \begin{array}{ccc} 1 & 0 \\ 0 & 1 \end{array} \right), XY^T=\left( \begin{array}{ccc} x_1y_1 & x_1y_2 \\ x_2y_1 & x_2y_2 \end{array} \right)$$ and $|I+XY^T|=\begin{vmatrix} x_1y_1+1 & x_1y_2\\ x_2y_1 & x_2y_2+1 \end{vmatrix}$ When we expand the determinant, we get: $(x_1y_1+1)(x_2y_2+1)-x_1y_2x_2y_1= 1+(x_1y_1x_2y_2+x_1y_1+x_2y_2-x_1y_2x_2y_1)=1+(x_1y_1+x_2y_2)=1+Y^TX$ 2. induction hypothesis: Suppose it's true for the value $n-1$ and now I want to prove it's true for n. 3. The inductive step: $det(I+XY^T)=x_1y_1+x_2y_2+...+x_{n-1}y_{n-1}+x_ny_n + 1$ And here is where i pretty much get stuck. I don't know where to go from here. It's kinda hard for me to grasp the idea behind mathematical induction. I don't really know what to do when I come to this step. What can I do to finish the proof? (well, if what I have done so far is correct, that is).","I have just started to learn how to construct proofs. That is, I am not really good at it (yet). In this thread I will work through a problem from my Linear Algebra textbook. First i will give you my ""solution"" and then, hopefully, you can tell me where I went wrong. If my proof strategy for this case is wrong I would love to hear why it's wrong (if it is possible) since I think that is how I will become better :) My textbook says an often good proof strategy of different determinant formulas is by Mathematical induction and I think it also works in this case, but as i said earlier, I am not too good at constructing proofs yet. Problem: Let $X, Y$ be column-vectors. Show that $det(I+XY^T)=1+Y^TX$, where the last product is interpreted as a number. Ok so here is my attempt to solve the problem: Proof strategy: Induction 1. Base case: The statement is true when n=2, since: $$I=\left( \begin{array}{ccc} 1 & 0 \\ 0 & 1 \end{array} \right), XY^T=\left( \begin{array}{ccc} x_1y_1 & x_1y_2 \\ x_2y_1 & x_2y_2 \end{array} \right)$$ and $|I+XY^T|=\begin{vmatrix} x_1y_1+1 & x_1y_2\\ x_2y_1 & x_2y_2+1 \end{vmatrix}$ When we expand the determinant, we get: $(x_1y_1+1)(x_2y_2+1)-x_1y_2x_2y_1= 1+(x_1y_1x_2y_2+x_1y_1+x_2y_2-x_1y_2x_2y_1)=1+(x_1y_1+x_2y_2)=1+Y^TX$ 2. induction hypothesis: Suppose it's true for the value $n-1$ and now I want to prove it's true for n. 3. The inductive step: $det(I+XY^T)=x_1y_1+x_2y_2+...+x_{n-1}y_{n-1}+x_ny_n + 1$ And here is where i pretty much get stuck. I don't know where to go from here. It's kinda hard for me to grasp the idea behind mathematical induction. I don't really know what to do when I come to this step. What can I do to finish the proof? (well, if what I have done so far is correct, that is).",,['linear-algebra']
