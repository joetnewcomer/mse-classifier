,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Real symmetric matrix similar to diagonal matrix,Real symmetric matrix similar to diagonal matrix,,"Let $A$ be a $2\times 2$ matrix with real entries, which is symmetric. Prove that $A$ is similar over $\Bbb{R}$ to a diagonal matrix.","Let $A$ be a $2\times 2$ matrix with real entries, which is symmetric. Prove that $A$ is similar over $\Bbb{R}$ to a diagonal matrix.",,"['linear-algebra', 'matrices']"
1,Lie group of matrices commuting with a fixed set of matrices,Lie group of matrices commuting with a fixed set of matrices,,"Given a set of $S$ of $ n\times n$ Hermitian matrices with entries in $\mathbb{C}$, the set of all $n\times n$ unitary matrices that commute with $S$ forms a Lie group $G$. My question is what Lie groups can we get from this process by choice of $n$ and $S$? We can get any products of unitary groups of the form $U(p)\times U(q)\times U(r)...$  by taking S to be a matrix with $p$ eigenvalues equal to $0$, $q$ eigenvalues equal to $1$ and $r$ eigenvalues equal to $2$ and so on. Is that the only possibility? Or can I get a group isomorphic to $U(1)\times Sp(m)$ or something? (We're always going to have a factor of $U(1)$  since the identity matrix commutes with everything. Like literally everything.) I can think of neither a counterexample nor a proof. If I can get other Lie groups than the natural follow up questions would be: What Lie groups can I get? Is there a nice construction get a set $S$ given a Lie group $G$? What representations of $G$ can I get?","Given a set of $S$ of $ n\times n$ Hermitian matrices with entries in $\mathbb{C}$, the set of all $n\times n$ unitary matrices that commute with $S$ forms a Lie group $G$. My question is what Lie groups can we get from this process by choice of $n$ and $S$? We can get any products of unitary groups of the form $U(p)\times U(q)\times U(r)...$  by taking S to be a matrix with $p$ eigenvalues equal to $0$, $q$ eigenvalues equal to $1$ and $r$ eigenvalues equal to $2$ and so on. Is that the only possibility? Or can I get a group isomorphic to $U(1)\times Sp(m)$ or something? (We're always going to have a factor of $U(1)$  since the identity matrix commutes with everything. Like literally everything.) I can think of neither a counterexample nor a proof. If I can get other Lie groups than the natural follow up questions would be: What Lie groups can I get? Is there a nice construction get a set $S$ given a Lie group $G$? What representations of $G$ can I get?",,"['linear-algebra', 'representation-theory', 'lie-groups']"
2,Linear Algebra: finding a basis for a subspace of $\mathbb{R}^4$,Linear Algebra: finding a basis for a subspace of,\mathbb{R}^4,So I am stuck on this example from my Into. Linear Algerbra book I'm not exactly sure how I'm supposed to find the basis in this case. Am I just supposed to use a random t and s value and call the single vector a basis? (There were no previous examples in the book that were similar) Thank you,So I am stuck on this example from my Into. Linear Algerbra book I'm not exactly sure how I'm supposed to find the basis in this case. Am I just supposed to use a random t and s value and call the single vector a basis? (There were no previous examples in the book that were similar) Thank you,,[]
3,Solving Coupled Eigenvalue equations,Solving Coupled Eigenvalue equations,,"I wish to solve the following set of coupled eigenvalue equations. How should I do it? For real matrices $A$,$B$,$D$ and vectors $x \in R^m$, $y \in R^n$ $$ A x + B y = \lambda x $$ $$ B^T x + D y = \mu y $$ where, $A$ and $D$ are symmetric. Background: I am trying to solve the following optimization problem: $$ \min x^T A x + y^T D y + x^T By $$ $$ \textrm{such that } \, x^T x = 1 , y^T y = 1 $$ This leads to the above eigenvalue problem. $\lambda$ and $\mu$ are the Langrangian parameters for the constraints. EDIT: I need to actually find numerical solutions for these equations given $A$, $B$, $D$","I wish to solve the following set of coupled eigenvalue equations. How should I do it? For real matrices $A$,$B$,$D$ and vectors $x \in R^m$, $y \in R^n$ $$ A x + B y = \lambda x $$ $$ B^T x + D y = \mu y $$ where, $A$ and $D$ are symmetric. Background: I am trying to solve the following optimization problem: $$ \min x^T A x + y^T D y + x^T By $$ $$ \textrm{such that } \, x^T x = 1 , y^T y = 1 $$ This leads to the above eigenvalue problem. $\lambda$ and $\mu$ are the Langrangian parameters for the constraints. EDIT: I need to actually find numerical solutions for these equations given $A$, $B$, $D$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
4,Does $M_n^{-1}$ converge for a series of growing matrices $M_n$?,Does  converge for a series of growing matrices ?,M_n^{-1} M_n,"$M_n$ is a $n\times n$ matrix with $M_{n+1}=\begin{pmatrix}M_n & a_n \\ b_n^T & c_n\end{pmatrix}$ and $a_n, b_n, c_n \to 0$ for $n\to \infty$. Is this sufficient to state $$ \lim_{n\to\infty}(M_n^{-1}) = (\lim_{n\to\infty}M_n)^{-1} ?$$","$M_n$ is a $n\times n$ matrix with $M_{n+1}=\begin{pmatrix}M_n & a_n \\ b_n^T & c_n\end{pmatrix}$ and $a_n, b_n, c_n \to 0$ for $n\to \infty$. Is this sufficient to state $$ \lim_{n\to\infty}(M_n^{-1}) = (\lim_{n\to\infty}M_n)^{-1} ?$$",,"['linear-algebra', 'matrices', 'convergence-divergence']"
5,What are norms of sub-matrices invariant under a block diagonal similarity transformation of a block matrix?,What are norms of sub-matrices invariant under a block diagonal similarity transformation of a block matrix?,,"Say $M := \begin{pmatrix} A & B\\ C & D \end{pmatrix}$ is a block matrix with $A, D$ being square matrices and this $B$ and $C^T$ having the same shape. Is there any norm characterizing the collection of $B$ and $C$ that is invariant under all block diagonal similarity transformations $M\to S M S^{-1}$ with $S=\begin{pmatrix}E & 0\\ 0 & F\end{pmatrix}$ of the whole block matrix? Were $B$ and $C$ square matrices I'd use e.g. the sum of squared eigenvalues, but is there something similar for non-square matrices? In brief, the requirements for the block norms I seek are: the norm of the square blocks $A$ and $D$ must be invariant (under these similarity transformations of $M$) respectively either there are individual norms for $B$ and $C$ which are invariant, or the is one combined norm depending on $B$ and $C$ which is invariant Is the latter requirement possible? Due to the block diagonal structure of $S$, all blocks transform independently, so there should be independent norms for $B$ and $C$. But since $B\to F B E^{-1}$ and $C\to E C F^{-1}$ are no similarity transformations, I couldn't even use eigenvalues if $B$ and $C$ were square matrices. However, $BC \to F BC F^{-1}$ and $CB \to E CB E^{-1}$ are similarity transformations, the remaining question is what kind of norm to use and maybe also how to decide whether $BC$ or $CB$ or both are ""significant""...","Say $M := \begin{pmatrix} A & B\\ C & D \end{pmatrix}$ is a block matrix with $A, D$ being square matrices and this $B$ and $C^T$ having the same shape. Is there any norm characterizing the collection of $B$ and $C$ that is invariant under all block diagonal similarity transformations $M\to S M S^{-1}$ with $S=\begin{pmatrix}E & 0\\ 0 & F\end{pmatrix}$ of the whole block matrix? Were $B$ and $C$ square matrices I'd use e.g. the sum of squared eigenvalues, but is there something similar for non-square matrices? In brief, the requirements for the block norms I seek are: the norm of the square blocks $A$ and $D$ must be invariant (under these similarity transformations of $M$) respectively either there are individual norms for $B$ and $C$ which are invariant, or the is one combined norm depending on $B$ and $C$ which is invariant Is the latter requirement possible? Due to the block diagonal structure of $S$, all blocks transform independently, so there should be independent norms for $B$ and $C$. But since $B\to F B E^{-1}$ and $C\to E C F^{-1}$ are no similarity transformations, I couldn't even use eigenvalues if $B$ and $C$ were square matrices. However, $BC \to F BC F^{-1}$ and $CB \to E CB E^{-1}$ are similarity transformations, the remaining question is what kind of norm to use and maybe also how to decide whether $BC$ or $CB$ or both are ""significant""...",,"['linear-algebra', 'matrices', 'normed-spaces', 'block-matrices', 'similar-matrices']"
6,A kind of generalized eigenvalue problem,A kind of generalized eigenvalue problem,,"Let $A$ , $B$ and $C$ denote three complex $2 \times 2$ matrices. Find necessary and sufficient conditions on $A$ , $B$ and $C$ for the existence of a nonzero $v \in \mathbb{C}^2$ such that $Av$ , $Bv$ and $Cv$ are all nonzero, and $Av$ , $Bv$ and $Cv$ are all multiples of each other. Motivation: I was playing around with a problem and was led to formulate the problem above. Well, actually, I am trying to prove a statement by contradiction. I assumed the statement was false, and then was able to show that it would imply the existence of a $v$ such as above for some specific $A$ , $B$ and $C$ . This led me to ask the question above, which is a kind of generalized eigenvalue problem, and can be formulated independently. Edit: I am thinking that the problem can be reformulated as to whether or not some rational cubic curve in $\mathbb{P}^3_\mathbb{C}$ intersects non-trivially the ""standard"" twisted cubic. Indeed, consider the map $\gamma$ from $\mathbb{C}^2$ to $\operatorname{Sym}^3(\mathbb{C}^2)$ which is defined by $$ \gamma(v) = Av \odot Bv \odot Cv. $$ This induces a cubic rational map $\tilde{\gamma}$ from $\mathbb{P}^1_\mathbb{C}$ to $\mathbb{P}^3_\mathbb{C}$ . I think that the image of this map can be written as an intersection of quadrics in $\mathbb{P}^3_\mathbb{C}$ , but I am not entirely sure. On the other hand, the twisted cubic is (up to some binomial coefficients) induced by the map $$ v \mapsto v \odot v \odot v.$$ And the image of the twisted cubic in $\mathbb{P}^3_\mathbb{C}$ is known to be the vanishing locus of an ideal which is generated by $4$ quadrics (it is not a complete intersection). So the problem is equivalent to whether or not the image of $\tilde{\gamma}$ intersects non-trivially the image of the twisted cubic. So I think it amounts to whether or not the common vanishing locus of finitely many quadrics in $\mathbb{P}^3_\mathbb{C}$ is non-empty. Also note that a similar question was asked before here, see @Theo Bendit's comment below. There is an answer in the linked post but I am hoping to get an alternative solution which is more algebro-geometric.","Let , and denote three complex matrices. Find necessary and sufficient conditions on , and for the existence of a nonzero such that , and are all nonzero, and , and are all multiples of each other. Motivation: I was playing around with a problem and was led to formulate the problem above. Well, actually, I am trying to prove a statement by contradiction. I assumed the statement was false, and then was able to show that it would imply the existence of a such as above for some specific , and . This led me to ask the question above, which is a kind of generalized eigenvalue problem, and can be formulated independently. Edit: I am thinking that the problem can be reformulated as to whether or not some rational cubic curve in intersects non-trivially the ""standard"" twisted cubic. Indeed, consider the map from to which is defined by This induces a cubic rational map from to . I think that the image of this map can be written as an intersection of quadrics in , but I am not entirely sure. On the other hand, the twisted cubic is (up to some binomial coefficients) induced by the map And the image of the twisted cubic in is known to be the vanishing locus of an ideal which is generated by quadrics (it is not a complete intersection). So the problem is equivalent to whether or not the image of intersects non-trivially the image of the twisted cubic. So I think it amounts to whether or not the common vanishing locus of finitely many quadrics in is non-empty. Also note that a similar question was asked before here, see @Theo Bendit's comment below. There is an answer in the linked post but I am hoping to get an alternative solution which is more algebro-geometric.",A B C 2 \times 2 A B C v \in \mathbb{C}^2 Av Bv Cv Av Bv Cv v A B C \mathbb{P}^3_\mathbb{C} \gamma \mathbb{C}^2 \operatorname{Sym}^3(\mathbb{C}^2)  \gamma(v) = Av \odot Bv \odot Cv.  \tilde{\gamma} \mathbb{P}^1_\mathbb{C} \mathbb{P}^3_\mathbb{C} \mathbb{P}^3_\mathbb{C}  v \mapsto v \odot v \odot v. \mathbb{P}^3_\mathbb{C} 4 \tilde{\gamma} \mathbb{P}^3_\mathbb{C},"['linear-algebra', 'eigenvalues-eigenvectors']"
7,Can you explain how to project this vector onto the complex plane to someone who hasn't learned Differential Geometry or Topology?,Can you explain how to project this vector onto the complex plane to someone who hasn't learned Differential Geometry or Topology?,,"Background I am trying to understand the paper ""On $C^2$ -smooth Surfaces of Constant Width"" by Brendan Guilfoyle and Wilhelm Klingenberg . However, despite my efforts to decipher the paper, I can't follow everything it is describing. This paper is written around complex differential geometry and topology, and appears either to play fast and loose with notation or to rely on conventions I haven't encountered before. I have no prior experience with differential geometry or topology in general, and the (seemingly) flexible use of notation makes it difficult to follow without intimate familiarity with the conventions of the fields in question. I have been unable to find other resources on the same topic that are easier to parse, or lessons on how to understand the concepts in the paper that aren't either behind a paywall or also way over my head. What I Understand So Far In Section 2.1: The Space of Oriented Lines , the authors say Let $ð•ƒ$ be the set of oriented lines, or rays, in $ð”¼^3$ . Such a line $Î³$ is uniquely determined by its unit direction vector $\vec{U}$ and the vector $\vec{V}$ joining the origin to the point on the line that lies closest to the origin. That is, $$ Î³ = \{ \vec{V} + r \; \vec{U} âˆˆ ð”¼^3 \; | \; r âˆˆ â„ \} $$ where r is an affine parameter along the line. By parallel translation, we move $\vec{U}$ to the origin and $\vec{V}$ to the head of $\vec{U}$ . Thus, we obtain a vector that is tangent to the unit 2-dimensional sphere in $ð”¼^3$ . The mapping is one-to-one and so it identifies the space of oriented lines with the tangent bundle of the 2-sphere $TS^2$ (see Figure 1). $$ ð•ƒ = \{ (\vec{U}, \vec{V}) âˆˆ ð”¼^3 Ã— ð”¼^3 \; | \quad |\vec{U}| = 1 \quad \vec{U} â‹… \vec{V} = 0\} $$ $$ \text{FÉªÉ¢á´œÊ€á´‡ 1.} $$ So far, so good. This all makes sense. In Section 2.2: Coordinates on ð•ƒ , the authors introduce $Î¾$ as ""the local complex coordinate on the unit 2-sphere in $ð”¼^3$ obtained by stereographic projection from the south pole"" in terms of standard spherical polar angles $(Î¸, Ï†)$ with the definition $Î¾ = \tan\left(\frac{Î¸}{2}\right) e^{i Ï†}$ . That all checks out. The next thing they say is We convert from coordinates $(Î¾, \bar{Î¾})$ back to $(Î¸, Ï†)$ using $$ \cos(Î¸) = \frac{1 - Î¾ \bar{Î¾}}{1 + Î¾ \bar{Î¾}},\quad \sin(Î¸) = \frac{2 \sqrt{Î¾ \bar{Î¾}}}{1 + Î¾ \bar{Î¾}},\quad \cos(Ï†) = \frac{Î¾ + \bar{Î¾}}{2 \sqrt{Î¾ \bar{Î¾}}},\quad \sin(Ï†) = \frac{Î¾ - \bar{Î¾}}{2 i \sqrt{Î¾ \bar{Î¾}}} $$ I know the conversion from unit-spherical coordinates $(Î¸, Ï†)$ to Cartesian coordinates is $(\sin(Î¸) \cos(Ï†), \sin(Î¸) \sin(Ï†), \cos(Î¸))$ , and these equations all check out. Where I Get Confused Continuing farther into Section 2.2 is where I run into trouble, as the authors add another complex variable $Î·$ without defining it in a way I understand: This can be extended to complex coordinates $(Î¾, Î·)$ on ð•ƒ minus the tangent space over the south pole, as follows. First note that a tangent vector $\vec{X}$ to the 2-sphere can always be expressed as a linear combination of the tangent vectors generated by $Î¸$ and $Ï†$ : $$ \vec{X} = X^Î¸ \frac{âˆ‚}{âˆ‚ Î¸} + X^Ï† \frac{âˆ‚}{âˆ‚ Ï†}. $$ In our complex formalism, we have the natural complex tangent vector $$ \frac{âˆ‚}{âˆ‚ Î¾} = \cos\left(\frac{Î¸}{2}\right)^2 \left(\frac{âˆ‚}{âˆ‚ Î¸} - \frac{i}{2 \cos\left(\frac{Î¸}{2}\right) \sin\left(\frac{Î¸}{2}\right)} \frac{âˆ‚}{âˆ‚ Ï†}\right) e^{-i Ï†}, $$ and any real tangent vector can be written as $$ \vec{X} = Î· \frac{âˆ‚}{âˆ‚ Î¾} + \bar{Î·} \frac{âˆ‚}{âˆ‚ \bar{Î¾}}. $$ for a complex number $Î·$ . We identify the real tangent vector $\vec{X}$ on the 2-sphere (and hence the ray in $ð”¼^3$ ) with the two complex numbers $(Î¾, Î·)$ . Loosely speaking, $Î¾$ determines the direction of the ray, and $Î·$ determines its perpendicular distance vector to the origin â€” complex representations of the vectors $\vec{U}$ and $\vec{V}$ . I get that $\vec{U}$ is being projected from $ð”¼^3$ into the complex plane as $Î¾$ , since $\vec{U}$ is a unit vector with its base on the origin and its head on the unit sphere at position $(Î¸, Ï†)$ . However, I don't understand how $\vec{V}$ gets converted to $Î·$ . $\vec{V}$ is not necessarily of unit length, so it can't be as simple as $\vec{U}$ 's projection. There seem to be a number of different ways to project $\vec{V}$ into the complex plane, and I'm not sure which is correct. Can you explain the conversion between $\vec{V}$ and $Î·$ to someone whose formal math education hasn't gone past Calc 1? Specifically, based on the next sections of the paper, I need to be able to convert from coordinates $(Î¾, Î·)$ to the vectors $\vec{U}$ and $\vec{V}$ . I completely understand the definition for $\vec{U}(Î¾)$ , so I just need to know how to derive a definition for $\vec{V}(Î·)$ or $\vec{V}(Î¾, Î·)$ .","Background I am trying to understand the paper ""On -smooth Surfaces of Constant Width"" by Brendan Guilfoyle and Wilhelm Klingenberg . However, despite my efforts to decipher the paper, I can't follow everything it is describing. This paper is written around complex differential geometry and topology, and appears either to play fast and loose with notation or to rely on conventions I haven't encountered before. I have no prior experience with differential geometry or topology in general, and the (seemingly) flexible use of notation makes it difficult to follow without intimate familiarity with the conventions of the fields in question. I have been unable to find other resources on the same topic that are easier to parse, or lessons on how to understand the concepts in the paper that aren't either behind a paywall or also way over my head. What I Understand So Far In Section 2.1: The Space of Oriented Lines , the authors say Let be the set of oriented lines, or rays, in . Such a line is uniquely determined by its unit direction vector and the vector joining the origin to the point on the line that lies closest to the origin. That is, where r is an affine parameter along the line. By parallel translation, we move to the origin and to the head of . Thus, we obtain a vector that is tangent to the unit 2-dimensional sphere in . The mapping is one-to-one and so it identifies the space of oriented lines with the tangent bundle of the 2-sphere (see Figure 1). So far, so good. This all makes sense. In Section 2.2: Coordinates on ð•ƒ , the authors introduce as ""the local complex coordinate on the unit 2-sphere in obtained by stereographic projection from the south pole"" in terms of standard spherical polar angles with the definition . That all checks out. The next thing they say is We convert from coordinates back to using I know the conversion from unit-spherical coordinates to Cartesian coordinates is , and these equations all check out. Where I Get Confused Continuing farther into Section 2.2 is where I run into trouble, as the authors add another complex variable without defining it in a way I understand: This can be extended to complex coordinates on ð•ƒ minus the tangent space over the south pole, as follows. First note that a tangent vector to the 2-sphere can always be expressed as a linear combination of the tangent vectors generated by and : In our complex formalism, we have the natural complex tangent vector and any real tangent vector can be written as for a complex number . We identify the real tangent vector on the 2-sphere (and hence the ray in ) with the two complex numbers . Loosely speaking, determines the direction of the ray, and determines its perpendicular distance vector to the origin â€” complex representations of the vectors and . I get that is being projected from into the complex plane as , since is a unit vector with its base on the origin and its head on the unit sphere at position . However, I don't understand how gets converted to . is not necessarily of unit length, so it can't be as simple as 's projection. There seem to be a number of different ways to project into the complex plane, and I'm not sure which is correct. Can you explain the conversion between and to someone whose formal math education hasn't gone past Calc 1? Specifically, based on the next sections of the paper, I need to be able to convert from coordinates to the vectors and . I completely understand the definition for , so I just need to know how to derive a definition for or .","C^2 ð•ƒ ð”¼^3 Î³ \vec{U} \vec{V} 
Î³ = \{ \vec{V} + r \; \vec{U} âˆˆ ð”¼^3 \; | \; r âˆˆ â„ \}
 \vec{U} \vec{V} \vec{U} ð”¼^3 TS^2 
ð•ƒ = \{ (\vec{U}, \vec{V}) âˆˆ ð”¼^3 Ã— ð”¼^3 \; | \quad |\vec{U}| = 1 \quad \vec{U} â‹… \vec{V} = 0\}
 
\text{FÉªÉ¢á´œÊ€á´‡ 1.}
 Î¾ ð”¼^3 (Î¸, Ï†) Î¾ = \tan\left(\frac{Î¸}{2}\right) e^{i Ï†} (Î¾, \bar{Î¾}) (Î¸, Ï†) 
\cos(Î¸) = \frac{1 - Î¾ \bar{Î¾}}{1 + Î¾ \bar{Î¾}},\quad \sin(Î¸) = \frac{2 \sqrt{Î¾ \bar{Î¾}}}{1 + Î¾ \bar{Î¾}},\quad \cos(Ï†) = \frac{Î¾ + \bar{Î¾}}{2 \sqrt{Î¾ \bar{Î¾}}},\quad \sin(Ï†) = \frac{Î¾ - \bar{Î¾}}{2 i \sqrt{Î¾ \bar{Î¾}}}
 (Î¸, Ï†) (\sin(Î¸) \cos(Ï†), \sin(Î¸) \sin(Ï†), \cos(Î¸)) Î· (Î¾, Î·) \vec{X} Î¸ Ï† 
\vec{X} = X^Î¸ \frac{âˆ‚}{âˆ‚ Î¸} + X^Ï† \frac{âˆ‚}{âˆ‚ Ï†}.
 
\frac{âˆ‚}{âˆ‚ Î¾} = \cos\left(\frac{Î¸}{2}\right)^2 \left(\frac{âˆ‚}{âˆ‚ Î¸} - \frac{i}{2 \cos\left(\frac{Î¸}{2}\right) \sin\left(\frac{Î¸}{2}\right)} \frac{âˆ‚}{âˆ‚ Ï†}\right) e^{-i Ï†},
 
\vec{X} = Î· \frac{âˆ‚}{âˆ‚ Î¾} + \bar{Î·} \frac{âˆ‚}{âˆ‚ \bar{Î¾}}.
 Î· \vec{X} ð”¼^3 (Î¾, Î·) Î¾ Î· \vec{U} \vec{V} \vec{U} ð”¼^3 Î¾ \vec{U} (Î¸, Ï†) \vec{V} Î· \vec{V} \vec{U} \vec{V} \vec{V} Î· (Î¾, Î·) \vec{U} \vec{V} \vec{U}(Î¾) \vec{V}(Î·) \vec{V}(Î¾, Î·)","['linear-algebra', 'differential-geometry']"
8,Least squares solution to underdetermined Lyapunov equation,Least squares solution to underdetermined Lyapunov equation,,"I need to solve an underdetermined Lyapunov equation for unknown $n\times n$ matrix $X$ . $$AX + XA = B$$ The naive method is to vectorize $x=\operatorname{vec}(X)$ and use a least squares solver on the following equation to find the least-squares solution. $$(I\otimes A + A\otimes I)x = \operatorname{vec}B$$ Experimentally I found that when $A$ and $B$ are positive semidefinite, I get the same solution by expressing $X$ in terms of $U,s$ , the eigenvectors and eigenvalues of $A$ as below ( proof ) and modifying pointwise (Hadamard) division to skip division by zero, like how pseudo-inverse implementations do it. $$X=U \left( \frac{U' BU}{s + s'} \right) U'$$ Does this appear in the literature, or does someone see a way to prove that this recovers least-squares solution? Example $$\text{A=}\left( \begin{array}{ccc}  8 & -8 & -8 \\  -8 & 9 & 8 \\  -8 & 8 & 8 \\ \end{array} \right)$$ $$\text{B=}\left( \begin{array}{ccc}  5 & 5 & -5 \\  5 & 9 & -3 \\  -5 & -3 & 6 \\ \end{array} \right)$$ This equation is underdetermined because $A$ and $B$ are singular, hence standard Lyapunov solver fails. However, both least squares and truncated spectral decomposition methods succeed with the same answer: $$X=\frac{1}{640}\left( \begin{array}{ccc}  1789 & 2928 & -1329 \\  2928 & 4672 & -1968 \\  -1329 & -1968 & 869 \\ \end{array} \right)$$ Notebook","I need to solve an underdetermined Lyapunov equation for unknown matrix . The naive method is to vectorize and use a least squares solver on the following equation to find the least-squares solution. Experimentally I found that when and are positive semidefinite, I get the same solution by expressing in terms of , the eigenvectors and eigenvalues of as below ( proof ) and modifying pointwise (Hadamard) division to skip division by zero, like how pseudo-inverse implementations do it. Does this appear in the literature, or does someone see a way to prove that this recovers least-squares solution? Example This equation is underdetermined because and are singular, hence standard Lyapunov solver fails. However, both least squares and truncated spectral decomposition methods succeed with the same answer: Notebook","n\times n X AX + XA = B x=\operatorname{vec}(X) (I\otimes A + A\otimes I)x = \operatorname{vec}B A B X U,s A X=U \left( \frac{U' BU}{s + s'} \right) U' \text{A=}\left(
\begin{array}{ccc}
 8 & -8 & -8 \\
 -8 & 9 & 8 \\
 -8 & 8 & 8 \\
\end{array}
\right) \text{B=}\left(
\begin{array}{ccc}
 5 & 5 & -5 \\
 5 & 9 & -3 \\
 -5 & -3 & 6 \\
\end{array}
\right) A B X=\frac{1}{640}\left(
\begin{array}{ccc}
 1789 & 2928 & -1329 \\
 2928 & 4672 & -1968 \\
 -1329 & -1968 & 869 \\
\end{array}
\right)","['linear-algebra', 'matrix-equations']"
9,Extension of Wahba's problem: finding two unknown rotations surrounding a known rotation,Extension of Wahba's problem: finding two unknown rotations surrounding a known rotation,,"Wahba's Problem seeks to find the $3\times3$ rotation matrix which minimises: $$ J(\boldsymbol{\mathrm{R}}) = \frac{1}{2} \sum_{k}|| \boldsymbol{{w}}_k - \boldsymbol{\mathrm{R}}\boldsymbol{v}_k||^2 $$ I have a similar but potentially more difficult problem, where my transformation between two vectors is given by $$ \boldsymbol{w}_k \approx \boldsymbol{\mathrm{R}}_1\, \boldsymbol{\mathrm{R}}_{2,k}\, \boldsymbol{\mathrm{R}}_3\, \boldsymbol{v}_k $$ where $\boldsymbol{\mathrm{R}}_{2,k}$ is a known Euler rotation about two axes, and $k=1, 2, \cdots, N$ . I wish to determine the best fit rotations $\boldsymbol{\mathrm{R}}_1$ , and $\boldsymbol{\mathrm{R}}_3$ which describe the two sets of vectors. With the exception of using a large set of data in a nonlinear optimisation, is there any way to estimate these two rotation matrices? Any help is appreciated - Thank you!","Wahba's Problem seeks to find the rotation matrix which minimises: I have a similar but potentially more difficult problem, where my transformation between two vectors is given by where is a known Euler rotation about two axes, and . I wish to determine the best fit rotations , and which describe the two sets of vectors. With the exception of using a large set of data in a nonlinear optimisation, is there any way to estimate these two rotation matrices? Any help is appreciated - Thank you!","3\times3  J(\boldsymbol{\mathrm{R}}) = \frac{1}{2} \sum_{k}|| \boldsymbol{{w}}_k - \boldsymbol{\mathrm{R}}\boldsymbol{v}_k||^2   \boldsymbol{w}_k \approx \boldsymbol{\mathrm{R}}_1\, \boldsymbol{\mathrm{R}}_{2,k}\, \boldsymbol{\mathrm{R}}_3\, \boldsymbol{v}_k  \boldsymbol{\mathrm{R}}_{2,k} k=1, 2, \cdots, N \boldsymbol{\mathrm{R}}_1 \boldsymbol{\mathrm{R}}_3","['linear-algebra', 'matrices', 'geometry', 'linear-transformations', 'rotations']"
10,Number of unpaired $x$ and $y$ samples needed to determine $A\in\mathbb{R}^{2\times2}$,Number of unpaired  and  samples needed to determine,x y A\in\mathbb{R}^{2\times2},"This is a question from an exam on deep learning that I took not so long ago, and I'd like to know the answer. The Setting There is a Gaussian distribution from which we sample $x \in \mathbb{R}^{2}$ values, but what we observe is $y \in \mathbb{R}^{2}$ , where $y = A x$ . We have an oracle that takes as input a number $n \in \mathbb{N}$ and returns $n$ different $x$ values sampled from the Gaussian and $n$ $y$ values corresponding to the $x$ values, but they are unpaired (meaning that if we label the $x, y$ values as $$\begin{array}{c} x_{1},\ldots,x_{n}\\ y_{1},\ldots,y_{n} \end{array}$$ there is some permutation $\pi$ such that for all $i$ we have $y_{i}=Ax_{\pi\left(i\right)}$ . The Question What is the minimal number $n$ that we need to give to the oracle, in order to determine $A$ (prove your answer). If it isn't possible with any finite (or even infinite) number of samples - prove it. Are the cases where $A$ is invertible different from the case where it isn't? My Attempt My initial gut feeling was that it's impossible, but then I had the following Idea. Clearly, if we had two paired samples $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right)$ we could determine $A$ easily. But since they are unpaired, what we could maybe do is choose $n=3$ and go over all different permutations, using the first two pairs to determine $A$ and check if it fits the 3rd pair. However, I am not sure if this is correct, as perhaps I could find two permutations yielding different $A$ 's. I wasn't able to prove this nor find a counter-example","This is a question from an exam on deep learning that I took not so long ago, and I'd like to know the answer. The Setting There is a Gaussian distribution from which we sample values, but what we observe is , where . We have an oracle that takes as input a number and returns different values sampled from the Gaussian and values corresponding to the values, but they are unpaired (meaning that if we label the values as there is some permutation such that for all we have . The Question What is the minimal number that we need to give to the oracle, in order to determine (prove your answer). If it isn't possible with any finite (or even infinite) number of samples - prove it. Are the cases where is invertible different from the case where it isn't? My Attempt My initial gut feeling was that it's impossible, but then I had the following Idea. Clearly, if we had two paired samples we could determine easily. But since they are unpaired, what we could maybe do is choose and go over all different permutations, using the first two pairs to determine and check if it fits the 3rd pair. However, I am not sure if this is correct, as perhaps I could find two permutations yielding different 's. I wasn't able to prove this nor find a counter-example","x \in \mathbb{R}^{2} y \in \mathbb{R}^{2} y = A x n \in \mathbb{N} n x n y x x, y \begin{array}{c}
x_{1},\ldots,x_{n}\\
y_{1},\ldots,y_{n}
\end{array} \pi i y_{i}=Ax_{\pi\left(i\right)} n A A \left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right) A n=3 A A","['linear-algebra', 'matrices', 'permutations', 'sampling', 'decidability']"
11,Linear Programming conceptual question,Linear Programming conceptual question,,"Linear programming involves solving a linear problem subject to the constraint $x \geq  0$ , and minimizing a Cost function for the specific case. The book (Gilbert Strang, Linear Algebra, section on Linear Programming), states that in Linear Programming, if we have $m$ equations, $n$ unknowns ( $n > m$ as an under determined system of equations). Then the number of corners is $n!/m!(n-m)!$ . Taking a plane $x1 + x2 + x3 = 4$ , a corner-example would be $(0,4,0)$ . (useful for reading the extract.) When it is stated that for $m$ equations there is a large number of corners with $m$ non-zeroes, where does this come from? Maybe a single example for 2 3D planes would help me understand better. PS: in this case, according to the definition, corner would always be $(x,0,0)$ , $(0,y,0)$ , $(0,0,z)$ but here is an intuition from the book It is a vector $x \geq 0$ such that at most $m$ components are non-zero, and is a solution to $Ax=b$ . Applied to the example, with a single equation, at most 1 component is non-zero.","Linear programming involves solving a linear problem subject to the constraint , and minimizing a Cost function for the specific case. The book (Gilbert Strang, Linear Algebra, section on Linear Programming), states that in Linear Programming, if we have equations, unknowns ( as an under determined system of equations). Then the number of corners is . Taking a plane , a corner-example would be . (useful for reading the extract.) When it is stated that for equations there is a large number of corners with non-zeroes, where does this come from? Maybe a single example for 2 3D planes would help me understand better. PS: in this case, according to the definition, corner would always be , , but here is an intuition from the book It is a vector such that at most components are non-zero, and is a solution to . Applied to the example, with a single equation, at most 1 component is non-zero.","x \geq  0 m n n > m n!/m!(n-m)! x1 + x2 + x3 = 4 (0,4,0) m m (x,0,0) (0,y,0) (0,0,z) x \geq 0 m Ax=b","['linear-algebra', 'linear-programming']"
12,Is a projected subspace equivalent to a truncated subspace?,Is a projected subspace equivalent to a truncated subspace?,,"I am from physics with humble mathematical background so apologies if you find this trivial. Given a vector $v=\begin{pmatrix} a \\ \alpha \\ b \\ \beta \end{pmatrix}$ that lives in $\mathbb{R}^{4\times 1}$ , I define a projector $P$ with $P_{11}=P_{33}=1$ and all other elements zero, such that $u:=Pv = \begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix}$ . My question is whether the following replacement is true: $$\begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix} a \\   b   \end{pmatrix}=:w$$ Since $u$ and $w$ belong to different spaces, to me it seems that this equivalence is not true. I ""invented"" the word truncated for the vector $w$ , only to help me frame the question title. Also, can a similar thing be said about the matrices, e.g. $$ P \begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\                   a_{21} & a_{22} & a_{23} & a_{24}\\                  a_{31} & a_{32} & a_{33} & a_{34}\\                  a_{41} & a_{42} & a_{43} & a_{44}   \end{pmatrix}P = \begin{pmatrix} a_{11} & 0 & a_{13} & 0 \\                   0 & 0 & 0 & 0\\                  a_{31} & 0 & a_{33} & 0\\                  0 & 0 & 0 & 0   \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix}  a_{11} & a_{13} \\ a_{31} & a_{33} \end{pmatrix}$$ EDIT: Thanks to the comment by @ronno and answer by @Nils. What I mean by the ""equivalence""  is the that expectation values should be the same in the two representations.","I am from physics with humble mathematical background so apologies if you find this trivial. Given a vector that lives in , I define a projector with and all other elements zero, such that . My question is whether the following replacement is true: Since and belong to different spaces, to me it seems that this equivalence is not true. I ""invented"" the word truncated for the vector , only to help me frame the question title. Also, can a similar thing be said about the matrices, e.g. EDIT: Thanks to the comment by @ronno and answer by @Nils. What I mean by the ""equivalence""  is the that expectation values should be the same in the two representations.","v=\begin{pmatrix} a \\ \alpha \\ b \\ \beta \end{pmatrix} \mathbb{R}^{4\times 1} P P_{11}=P_{33}=1 u:=Pv = \begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix} \begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix} a \\   b   \end{pmatrix}=:w u w w  P \begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\
                  a_{21} & a_{22} & a_{23} & a_{24}\\
                 a_{31} & a_{32} & a_{33} & a_{34}\\
                 a_{41} & a_{42} & a_{43} & a_{44}   \end{pmatrix}P = \begin{pmatrix} a_{11} & 0 & a_{13} & 0 \\
                  0 & 0 & 0 & 0\\
                 a_{31} & 0 & a_{33} & 0\\
                 0 & 0 & 0 & 0   \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix}  a_{11} & a_{13} \\ a_{31} & a_{33} \end{pmatrix}","['linear-algebra', 'linear-transformations', 'projective-space', 'projection']"
13,Why is curl given by a cross product?,Why is curl given by a cross product?,,"I've googled a lot but in most places, they usually only show how to compute it which I by now can do. But I wanna understand why the cross product between nabla and the vector give us the curl. So far I've managed to understand that when we are talking about the curl we are interested in the vector that points upwards or downwards (perpendicular) to the actual curl (rotation) like the green vector down here: And then since the cross product between two vectors give us another vector orthogonal to both I finally understood why we do $\nabla\times \vec{F}$ . Am I correct so far? Otherwise feel free to correct me (I actually want you to). But then my first question is why does this ""green"" vector in the picture define the curl? For example in Stoke's theorem we use curl to calculate higher dimension of ""Green"", but in Green's theorem we don't take any perpendicular vectors (obviously because z is non existent). This brings me to my second question which is how is the curl related to Green's thorem? Until now I had assumed that the green's theorem was just like line integral where we are only interested in the the field vector's component's that are parallel to the curve's tangents. But in general I am confused and I'd really appreciate it if someone could just clear these things up for me?","I've googled a lot but in most places, they usually only show how to compute it which I by now can do. But I wanna understand why the cross product between nabla and the vector give us the curl. So far I've managed to understand that when we are talking about the curl we are interested in the vector that points upwards or downwards (perpendicular) to the actual curl (rotation) like the green vector down here: And then since the cross product between two vectors give us another vector orthogonal to both I finally understood why we do . Am I correct so far? Otherwise feel free to correct me (I actually want you to). But then my first question is why does this ""green"" vector in the picture define the curl? For example in Stoke's theorem we use curl to calculate higher dimension of ""Green"", but in Green's theorem we don't take any perpendicular vectors (obviously because z is non existent). This brings me to my second question which is how is the curl related to Green's thorem? Until now I had assumed that the green's theorem was just like line integral where we are only interested in the the field vector's component's that are parallel to the curve's tangents. But in general I am confused and I'd really appreciate it if someone could just clear these things up for me?",\nabla\times \vec{F},"['calculus', 'linear-algebra', 'multivariable-calculus', 'curl', 'divergence-theorem']"
14,Proximal operator of squared $\ell_1$-norm,Proximal operator of squared -norm,\ell_1,"For any $a \in \mathbb R^d$ and $t \ge 0$ , let $p_t(a)$ be the unique minimizer of $f_t(x;a) := \|x-a\|_2^2 + t\|x\|_1^2$ over $x \in \mathbb R^d$ . Question. Is there an analytic formula for $p_t(a)$ ? Observation 1. By the subdifferential characterization of prox-operators, we know that $p_t(a) = (1+t\partial f)^{-1}(a)$ , where $\partial f$ is the subdifferential operator of $f$ . Thus, if $R := \|p_t(a)\|_1$ , then $p_t(a)$ satisfies $p_t(a) = (1+Rt\partial \|\cdot\|_1)^{-1}a = \mbox{prox}_{Rt\|\cdot\|_1}(a)$ , i.e the components of $p_t(a)$ are given by $$ (p_t(a))_j = \mathrm{ST}(a_j;Rt), \tag{1} \label{1} $$ Here, $\mathrm{ST}(u;\lambda)$ is the well-known soft-thresholding operator defined by $$ \mathrm{ST}(u;\lambda) := \mbox{sign}(u)(|u| - \lambda)_+. $$ However, the issue is that $R$ is unknown, since it depends on the sought-for $p_t(a)$ . Observation 2. From \eqref{1}, we deduce that: if $Rt \ge \|a\|_\infty := \max_j |a_j|$ , then $p_t(a) = 0$ . Thus, we may assume $Rt \le \|a\|_\infty$ . We may therefore search for $p_t(a)$ in form \eqref{1} with $Rt$ restricted to the compact interval $[0,\|a\|_\infty]$ . This is one-dimensional problem! Unfortunately, it's still not a closed-form solution...","For any and , let be the unique minimizer of over . Question. Is there an analytic formula for ? Observation 1. By the subdifferential characterization of prox-operators, we know that , where is the subdifferential operator of . Thus, if , then satisfies , i.e the components of are given by Here, is the well-known soft-thresholding operator defined by However, the issue is that is unknown, since it depends on the sought-for . Observation 2. From \eqref{1}, we deduce that: if , then . Thus, we may assume . We may therefore search for in form \eqref{1} with restricted to the compact interval . This is one-dimensional problem! Unfortunately, it's still not a closed-form solution...","a \in \mathbb R^d t \ge 0 p_t(a) f_t(x;a) := \|x-a\|_2^2 + t\|x\|_1^2 x \in \mathbb R^d p_t(a) p_t(a) = (1+t\partial f)^{-1}(a) \partial f f R := \|p_t(a)\|_1 p_t(a) p_t(a) = (1+Rt\partial \|\cdot\|_1)^{-1}a = \mbox{prox}_{Rt\|\cdot\|_1}(a) p_t(a) 
(p_t(a))_j = \mathrm{ST}(a_j;Rt),
\tag{1}
\label{1}
 \mathrm{ST}(u;\lambda) 
\mathrm{ST}(u;\lambda) :=
\mbox{sign}(u)(|u| - \lambda)_+.
 R p_t(a) Rt \ge \|a\|_\infty := \max_j |a_j| p_t(a) = 0 Rt \le \|a\|_\infty p_t(a) Rt [0,\|a\|_\infty]","['linear-algebra', 'convex-analysis', 'convex-optimization', 'signal-processing', 'proximal-operators']"
15,Why does this substitution give an incomplete particular integral solution for this linear ODE?,Why does this substitution give an incomplete particular integral solution for this linear ODE?,,"Trying to solve the following linear nonhomogeneous ODE $$ (D + 2)^2y = \cosh 2x $$ We can easily find the complementary function $$ y_c = c_1 e^{-2x} + c_2 {x} e^{-2x} $$ Using the inverse differential operator $$y_p = \frac{1}{(D+2)^2}\cosh 2x$$ I tried two methods to solve it but one of them gives an incomplete particular solution, I first used the fact that $\cosh 2x = ({e^{2x} + e^{-2x}})/{2}$ , and then applied the shift theorem $$y_p = \frac{1}{(D+2)^2} \frac{e^{2x} + e^{-2x}}{2} = \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{1}{(D+2)^2}e^{-2x}\right) = \frac{1}{2}\left(\frac{1}{16}e^{2x} + e^{-2x}\frac{1}{D^2}(1)\right)  = \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{x^2}{2}e^{-2x}\right)$$ $$y_p = \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x}$$ which gives a complete particular solution, so the general solution is $$y = c_1 e^{-2x} + c_2 {x} e^{-2x} + \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x}$$ But when I tried to expand $(D+2)^2$ then substitute $2^2$ in place of $D^2$ , and lastly apply the shift theorem, I got $$y_p = \frac{1}{D^2+4D+4}\cosh 2x = \frac{1}{4}\left(\frac{1}{D+2}\cosh 2x\right) = \frac{1}{4}\left(\frac{1}{D+2} \frac{e^{2x} + e^{-2x}}{2}\right) = \frac{1}{32}e^{2x} + \frac{1}{8}\left(\frac{1}{D+2}e^{-2x}\right)$$ $$y_p = \frac{1}{32}e^{2x} + \frac{1}{8}xe^{-2x}$$ but the term $\dfrac{x^2}{4}e^{-2x}$ is missing in this particular solution while the other $\dfrac{1}{8}xe^{-2x}$ is already there in the complementary function. So my question is, what exactly did I do wrong?, and when can I be sure that the method I am using will actually give the right complete answer?","Trying to solve the following linear nonhomogeneous ODE We can easily find the complementary function Using the inverse differential operator I tried two methods to solve it but one of them gives an incomplete particular solution, I first used the fact that , and then applied the shift theorem which gives a complete particular solution, so the general solution is But when I tried to expand then substitute in place of , and lastly apply the shift theorem, I got but the term is missing in this particular solution while the other is already there in the complementary function. So my question is, what exactly did I do wrong?, and when can I be sure that the method I am using will actually give the right complete answer?"," (D + 2)^2y = \cosh 2x   y_c = c_1 e^{-2x} + c_2 {x} e^{-2x}  y_p = \frac{1}{(D+2)^2}\cosh 2x \cosh 2x = ({e^{2x} + e^{-2x}})/{2} y_p = \frac{1}{(D+2)^2} \frac{e^{2x} + e^{-2x}}{2}
= \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{1}{(D+2)^2}e^{-2x}\right) = \frac{1}{2}\left(\frac{1}{16}e^{2x} + e^{-2x}\frac{1}{D^2}(1)\right)  = \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{x^2}{2}e^{-2x}\right) y_p = \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x} y = c_1 e^{-2x} + c_2 {x} e^{-2x} + \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x} (D+2)^2 2^2 D^2 y_p = \frac{1}{D^2+4D+4}\cosh 2x = \frac{1}{4}\left(\frac{1}{D+2}\cosh 2x\right) = \frac{1}{4}\left(\frac{1}{D+2} \frac{e^{2x} + e^{-2x}}{2}\right) = \frac{1}{32}e^{2x} + \frac{1}{8}\left(\frac{1}{D+2}e^{-2x}\right) y_p = \frac{1}{32}e^{2x} + \frac{1}{8}xe^{-2x} \dfrac{x^2}{4}e^{-2x} \dfrac{1}{8}xe^{-2x}","['linear-algebra', 'ordinary-differential-equations', 'differential-operators']"
16,"If we expand the definition of the general quadratic $ax^{2}+bx+c=0$ to include the case $a=0$, can we arrive at a general solution?","If we expand the definition of the general quadratic  to include the case , can we arrive at a general solution?",ax^{2}+bx+c=0 a=0,"Through a simple mathematical substitution, I have stumbled upon an alternative formula for solving a quadratic equation: $$x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}}$$ (Please refer to my formula derivation here ; this is my first question, so cannot include images yet). I am not aware if this has been mentioned elsewhere. I was just curious for a formula that reduces to the linear solution ( $x=-\frac{c}{b}$ ) for the standard quadratic $ax^2 + bx + c = 0$ , if we allow the case $a=0$ . The standard solution formula $x=\frac{-b \pm \sqrt{b^{2}-4ac}}{2a}$ crashes for $a=0$ because division by zero is not allowed. This alternative formula $x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}}$ covers the case $a=0$ , wherein it reduces to the linear equation $(bx + c = 0)$ solution ( $x=-\frac{c}{b}$ ) for the negative square root of the discriminant, but the positive square root of the discriminant needs to be ignored/is a problem in that case. What are your thoughts? Can we arrive at a general formula for a quadratic equation which includes all the cases ( $a=0$ and/or $b=0$ and/or $c=0$ )?","Through a simple mathematical substitution, I have stumbled upon an alternative formula for solving a quadratic equation: (Please refer to my formula derivation here ; this is my first question, so cannot include images yet). I am not aware if this has been mentioned elsewhere. I was just curious for a formula that reduces to the linear solution ( ) for the standard quadratic , if we allow the case . The standard solution formula crashes for because division by zero is not allowed. This alternative formula covers the case , wherein it reduces to the linear equation solution ( ) for the negative square root of the discriminant, but the positive square root of the discriminant needs to be ignored/is a problem in that case. What are your thoughts? Can we arrive at a general formula for a quadratic equation which includes all the cases ( and/or and/or )?",x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}} x=-\frac{c}{b} ax^2 + bx + c = 0 a=0 x=\frac{-b \pm \sqrt{b^{2}-4ac}}{2a} a=0 x=\frac{2c}{-b \pm \sqrt{b^{2}-4ac}} a=0 (bx + c = 0) x=-\frac{c}{b} a=0 b=0 c=0,"['linear-algebra', 'quadratics']"
17,Is there analog of angle in complex domain?,Is there analog of angle in complex domain?,,"Given a real vector space with an inner product, i. e. a positive definite symmetric bilinear form $\beta$ , I can define angle between any two nonzero vectors $v_1$ , $v_2$ as $\arccos\frac{\beta(v_1,v_2)}{\sqrt{\beta(v_1,v_1)\beta(v_2,v_2)}}$ . The angle will not change if I multiply the vectors by nonzero real numbers of the same sign, and will become replaced by the complementary angle if these numbers have opposite signs. The angle is also invariant under the action of the group $\operatorname{O}(\beta)$ of $\beta$ -preserving linear transformations. What are analogs of all this for complex vector spaces? I suppose that one has to take a Hermitian form for $\beta$ but the above expression under $\arccos$ will now be a complex number. Shall I take its argument? What happens under the action of the unitary group?","Given a real vector space with an inner product, i. e. a positive definite symmetric bilinear form , I can define angle between any two nonzero vectors , as . The angle will not change if I multiply the vectors by nonzero real numbers of the same sign, and will become replaced by the complementary angle if these numbers have opposite signs. The angle is also invariant under the action of the group of -preserving linear transformations. What are analogs of all this for complex vector spaces? I suppose that one has to take a Hermitian form for but the above expression under will now be a complex number. Shall I take its argument? What happens under the action of the unitary group?","\beta v_1 v_2 \arccos\frac{\beta(v_1,v_2)}{\sqrt{\beta(v_1,v_1)\beta(v_2,v_2)}} \operatorname{O}(\beta) \beta \beta \arccos","['linear-algebra', 'complex-numbers', 'euclidean-geometry', 'hilbert-spaces', 'inner-products']"
18,Find diagonal matrix $D$ so that $AD + B$ has specific eigenvalues.,Find diagonal matrix  so that  has specific eigenvalues.,D AD + B,"Problem I have the following matrix equation ( $A,B,D$ are all square, $D$ is diagonal): $$ C=AD + B $$ Assuming $A$ and $B$ are completely known and the eigenvalues ( $\lambda_i$ ) of $C$ are completely known, how can I go about solving for the diagonal matrix $D$ (numerically or otherwise)? Out of curiosity, to make this more general, if we don't restrict $D$ to be diagonal, can we solve for its eigenvalues given the same information? Edit : To be clear, $C$ is not known, only its eigenvalues are known. I'm essentially trying to solve for $C$ by first solving for $D$ . What I've Tried Since I know the eigenvalues of $C$ , I can set up a system of equations in terms of the unknown diagonal elements $\gamma_i$ using the following relationship and the known eigenvalues $\lambda_i$ : $$ \det \left( A  \begin{bmatrix}      \gamma_1 & \dots  & 0\\     \vdots & \ddots & \vdots\\     0 & \dots  & \gamma_n      \end{bmatrix} + B + \lambda_i I \right) = 0 $$ However, this system is non-linear and is cumbersome to set up and solve when the the matrices get large. Does anyone know of a better way?","Problem I have the following matrix equation ( are all square, is diagonal): Assuming and are completely known and the eigenvalues ( ) of are completely known, how can I go about solving for the diagonal matrix (numerically or otherwise)? Out of curiosity, to make this more general, if we don't restrict to be diagonal, can we solve for its eigenvalues given the same information? Edit : To be clear, is not known, only its eigenvalues are known. I'm essentially trying to solve for by first solving for . What I've Tried Since I know the eigenvalues of , I can set up a system of equations in terms of the unknown diagonal elements using the following relationship and the known eigenvalues : However, this system is non-linear and is cumbersome to set up and solve when the the matrices get large. Does anyone know of a better way?","A,B,D D 
C=AD + B
 A B \lambda_i C D D C C D C \gamma_i \lambda_i 
\det \left( A 
\begin{bmatrix} 
    \gamma_1 & \dots  & 0\\
    \vdots & \ddots & \vdots\\
    0 & \dots  & \gamma_n 
    \end{bmatrix}
+ B + \lambda_i I \right) = 0
","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
19,Infimum of the inner product between the gradient of a least square loss and the direction,Infimum of the inner product between the gradient of a least square loss and the direction,,"Let $A\in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^{m}$ . The gradient of a least square loss ( $f(x)=\frac{1}{2}\|Ax-b\|^2$ ) is $A^{\top}(Ax-b)$ . Is it possible to find a $\delta>0$ for every given $\epsilon>0$ such that the following holds for any $x \in \mathbb{R}^n$ when $x_0$ is given: $$ \inf_{\|x-x_0\|\geq \epsilon} \frac{\langle x-x_0, A^{\top}(Ax-b) \rangle}{\|x-x_0\|^2 } \geq \delta $$ My try $$ \begin{aligned} \frac{\langle x-x_0, A^{\top}(A(x-x_0+x_0)-b) \rangle}{\|x-x_0\|^2 }  &= \frac{\langle x-x_0, A^{\top}A(x-x_0) \rangle}{\|x-x_0\|^2 }  + \frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } \\ &\geq \frac{\lambda_{\min}(A^{\top}A)\|x-x_0\|^2 }{\|x-x_0\|^2 } + \frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } \\ &= \lambda_{\min}(A^{\top}A) \end{aligned}. $$ Now there are two situations: $A$ is full columns rank. Hence, $(\lambda_{\min}(A^{\top}A)>0)$ . Then, how do I know $\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > \lambda_{\min}(A^{\top}A) $ ? $A$ is not full columns rank. Hence, $(\lambda_{\min}(A^{\top}A)=0)$ . Then, how do I know $\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > 0 $ ?","Let and . The gradient of a least square loss ( ) is . Is it possible to find a for every given such that the following holds for any when is given: My try Now there are two situations: is full columns rank. Hence, . Then, how do I know ? is not full columns rank. Hence, . Then, how do I know ?","A\in \mathbb{R}^{m \times n} b \in \mathbb{R}^{m} f(x)=\frac{1}{2}\|Ax-b\|^2 A^{\top}(Ax-b) \delta>0 \epsilon>0 x \in \mathbb{R}^n x_0 
\inf_{\|x-x_0\|\geq \epsilon} \frac{\langle x-x_0, A^{\top}(Ax-b) \rangle}{\|x-x_0\|^2 } \geq \delta
 
\begin{aligned}
\frac{\langle x-x_0, A^{\top}(A(x-x_0+x_0)-b) \rangle}{\|x-x_0\|^2 } 
&=
\frac{\langle x-x_0, A^{\top}A(x-x_0) \rangle}{\|x-x_0\|^2 } 
+
\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 }
\\
&\geq
\frac{\lambda_{\min}(A^{\top}A)\|x-x_0\|^2 }{\|x-x_0\|^2 }
+
\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } \\
&=
\lambda_{\min}(A^{\top}A)
\end{aligned}.
 A (\lambda_{\min}(A^{\top}A)>0) \frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > \lambda_{\min}(A^{\top}A)  A (\lambda_{\min}(A^{\top}A)=0) \frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > 0 ","['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors']"
20,Prove that if the intersection of $n+1$ distinct spheres is non-empty then it has at most $n$ elements in $\mathbb{R}^n$,Prove that if the intersection of  distinct spheres is non-empty then it has at most  elements in,n+1 n \mathbb{R}^n,"Let $C \in \mathbb{R}^n$ , $D > 0$ , and $C_1,\ldots,C_{n+1}$ some distinct points with $\|C - C_k\| = D$ . Consider the balls $\mathcal{B}(C_k,r_k)$ for some $r_k > 0$ . Can we prove that $$\left| \bigcap_{k=1}^{n+1} \partial \mathcal{B}(C_k,r_k) \right| \leq n$$ i.e the intersection has at most $n$ elements? Later edit: It might be true even for an intersection of $3$ such spheres ...","Let , , and some distinct points with . Consider the balls for some . Can we prove that i.e the intersection has at most elements? Later edit: It might be true even for an intersection of such spheres ...","C \in \mathbb{R}^n D > 0 C_1,\ldots,C_{n+1} \|C - C_k\| = D \mathcal{B}(C_k,r_k) r_k > 0 \left| \bigcap_{k=1}^{n+1} \partial \mathcal{B}(C_k,r_k) \right| \leq n n 3","['linear-algebra', 'euclidean-geometry']"
21,Conditions for $I-x A$ to be a convergent matrix for some $x\in \mathbb{R}$,Conditions for  to be a convergent matrix for some,I-x A x\in \mathbb{R},"I'm looking for interpretable necessary/sufficient conditions on $A$ which guarantee that $(I-x A)$ is a convergent matrix for some $x\in \mathbb{R}$ For instance, $A$ normal with non-zero eigenvalues having real parts of same sign seems sufficient . What about non-normal $A$ ? For instance, let $A=\left(\begin{matrix}1&2\\0&1\end{matrix}\right)$ , we can visualize trajectories of $\left(I-x  \left(\begin{matrix}1&2\\0&1\end{matrix}\right)\right)^k$ below. $A$ satisfies this condition because of spectral radius condition, $\rho(I-xA)<1$ for any $0<x<2$ Following this it seems the example above is provably convergent because this map is contractive under coordinate change $y=Px$ with $$P=\left( \begin{array}{cc}  \frac{2 \sqrt{\frac{29}{3}}}{3} & -\frac{4}{\sqrt{87}} \\  0 & \frac{10}{\sqrt{87}} \\ \end{array} \right)$$ Coordinate transformation $P$ under which $x=Ax$ becomes contractive for any stable $A$ can be obtained by solving the following (comes from discrete Lyapunov equation) $$P^T P = T\left(\frac{T^{-1}A^{-T}U}{\lambda(A)-\lambda(A^{-1})^T}\right)U^{-1}$$ with $$\begin{align} T&=\text{columns of right eigenvectors of } A\\ U&=\text{columns of left eigenvectors of } A^{-1}\\ \lambda&=\text{column vector of eigenvalues}\\ a-b&=\text{subtraction with numpy broadcasting rules}\\ a/b&=\text{componentwise (Hadamard) division} \end{align} $$ notebook","I'm looking for interpretable necessary/sufficient conditions on which guarantee that is a convergent matrix for some For instance, normal with non-zero eigenvalues having real parts of same sign seems sufficient . What about non-normal ? For instance, let , we can visualize trajectories of below. satisfies this condition because of spectral radius condition, for any Following this it seems the example above is provably convergent because this map is contractive under coordinate change with Coordinate transformation under which becomes contractive for any stable can be obtained by solving the following (comes from discrete Lyapunov equation) with notebook","A (I-x A) x\in \mathbb{R} A A A=\left(\begin{matrix}1&2\\0&1\end{matrix}\right) \left(I-x  \left(\begin{matrix}1&2\\0&1\end{matrix}\right)\right)^k A \rho(I-xA)<1 0<x<2 y=Px P=\left(
\begin{array}{cc}
 \frac{2 \sqrt{\frac{29}{3}}}{3} & -\frac{4}{\sqrt{87}} \\
 0 & \frac{10}{\sqrt{87}} \\
\end{array}
\right) P x=Ax A P^T P = T\left(\frac{T^{-1}A^{-T}U}{\lambda(A)-\lambda(A^{-1})^T}\right)U^{-1} \begin{align}
T&=\text{columns of right eigenvectors of } A\\
U&=\text{columns of left eigenvectors of } A^{-1}\\
\lambda&=\text{column vector of eigenvalues}\\
a-b&=\text{subtraction with numpy broadcasting rules}\\
a/b&=\text{componentwise (Hadamard) division}
\end{align}
","['linear-algebra', 'dynamical-systems', 'fixed-point-theorems', 'contraction-operator']"
22,"Given matrix $B$, find all possible matrices $A$ satisfying $ A(A-2B) = -(A-2B)A $","Given matrix , find all possible matrices  satisfying",B A  A(A-2B) = -(A-2B)A ,"Let $A, B \in \Bbb R^{3 \times 3}$ such that $A(A-2B) = -(A-2B)A$ . Given $$ B = \begin{pmatrix} 2 & -2 & 1 \\ -1 & 3 & -1 \\ 2 & -4 & 3 \end{pmatrix} $$ find all possible matrices $A$ satisfying the equation above. I hope there is some way faster than compute directly...",Let such that . Given find all possible matrices satisfying the equation above. I hope there is some way faster than compute directly...,"A, B \in \Bbb R^{3 \times 3} A(A-2B) = -(A-2B)A  B = \begin{pmatrix} 2 & -2 & 1 \\ -1 & 3 & -1 \\ 2 & -4 & 3 \end{pmatrix}  A","['linear-algebra', 'matrices', 'matrix-equations']"
23,Prove uniqueness of solutions of different OLS matrix cases,Prove uniqueness of solutions of different OLS matrix cases,,"Let $D = \{(x_1, y_2), (x_2, y_2), \ldots , (x_n, y_n)\}$ where $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$ . One may use linear regression to predict $y$ as $w^Tx$ for some parameter vector $w \in \mathbb{R}^{d}$ . Consider matrix $X \in \mathbb{R}^{n\times d}$ with $x_i$ as rows and the vector $y \in \mathbb{R}^n$ as the vector of $y_i$ . Given an OLS loss function $$ \arg\min_w \hat{R}(w)=\arg\min_w \sum^n_{i=1}(y_i - w^Tx_i)^2$$ a) Show that for $n < d$ that the OLS loss function does not admit a unique solution b) Under what assumptions on $X$ does this equation admit a unique solution $w^*$ ? i. There exists a unique solution if $n \geq d$ and the columns of $X$ are independent ii. There exists a unique solution $\iff$ $X^TX$ is invertible iii. For $n > d$ , there will always be a unique solution if $X$ is full rank. I'm a little unsure how to write a proof on this question. My initial idea is to derive a unique solution under the assumption that $X^TX$ is invertible and use the properties of linear systems to that a) will have $\mathrm{rank}(w^*) < d$ and thus be inconsistent and b) will hold under assumptions ii. Is there a better/more formal way for proving a) and b)?","Let where and . One may use linear regression to predict as for some parameter vector . Consider matrix with as rows and the vector as the vector of . Given an OLS loss function a) Show that for that the OLS loss function does not admit a unique solution b) Under what assumptions on does this equation admit a unique solution ? i. There exists a unique solution if and the columns of are independent ii. There exists a unique solution is invertible iii. For , there will always be a unique solution if is full rank. I'm a little unsure how to write a proof on this question. My initial idea is to derive a unique solution under the assumption that is invertible and use the properties of linear systems to that a) will have and thus be inconsistent and b) will hold under assumptions ii. Is there a better/more formal way for proving a) and b)?","D = \{(x_1, y_2), (x_2, y_2), \ldots , (x_n, y_n)\} x_i \in \mathbb{R}^d y_i \in \mathbb{R} y w^Tx w \in \mathbb{R}^{d} X \in \mathbb{R}^{n\times d} x_i y \in \mathbb{R}^n y_i  \arg\min_w \hat{R}(w)=\arg\min_w \sum^n_{i=1}(y_i - w^Tx_i)^2 n < d X w^* n \geq d X \iff X^TX n > d X X^TX \mathrm{rank}(w^*) < d","['linear-algebra', 'statistics', 'matrix-equations', 'matrix-rank', 'linear-regression']"
24,Help proving that signed volume of n-parallelepiped is multilinear,Help proving that signed volume of n-parallelepiped is multilinear,,"Overview I am trying to build some intuition about the volumes of parallelepipeds and determinants. I would like to define the determinant as the unique function of $N$ vectors in $\mathbb{R}^N$ which is multilinear, anti-symmetric, and normalized and then show that the signed volume of the $N$ -parallelepiped satisfies these conditions without using any facts about determinants . The challenging one here for me is multi-linearity. Furthermore, I would like the signed volume to be defined in terms of Lebesgue integrals. Problem Setup Let $v_1, \ldots, v_N \in \mathbb{R}^N$ . A parallelepiped $P = P(v_1, \ldots, v_N) \subset \mathbb{R}^N$ is the set $$ P = P(v_1, \ldots, v_N) = \left\{\sum_{i=1}^N t_i v_i \mid 0 \le t_i \le 1 \text{ for $i$ from 1 to $N$} \right\} $$ My goal is to define a function $\text{svol}(P) = \text{svol}(v_1,\ldots, v_N)$ which is the signed volume of the parallelepiped $P$ . The volume $\text{vol}$ of the parallelepiped is defined as follows. Let $1_P$ be the indicator function on the set $P$ . $$ \text{vol}(P) = \text{vol}(v_1, \ldots, v_N) = \int_{\mathbb{R}^N} 1_P dV $$ Where the integral is the Lebesgue integral. We should have $$ |\text{svol}(P)| = \text{vol}(P) $$ We must additionally define, or provide an algorithm, to set the sign of $\text{svol}(P)$ . It is apparently the case, once everything above has been set up correctly, that for $a, b \in \mathbb{R}$ and $w \in \mathbb{R}^N$ \begin{equation} \tag{*} \text{svol}(av_1 + bw, v_2, \ldots v_N) = a\cdot \text{svol}(v_1, v_2, \ldots, v_N) + b\cdot \text{svol}(w, v_2, \ldots v_N) \end{equation} The Direct Questions My questions are How should $\text{svol}(v_1,\ldots, v_N)$ be defined such that (1) it is defined in terms of an integral over $\mathbb{R}^N$ and (2) we are able to prove $(*)$ . Given the definition that answers the previous question, how do we prove $(*)$ ? An Illustrative Image The answer to this question includes a beautiful illustration the depicts exactly what I am trying to prove. It is intuitively clear to me from this diagram and Cavalieri's principle that $(*)$ should hold. But of course, for me, the illustration does not constitute a fully rigorous proof following the conditions I laid out above. An Almost Solution The closest I have come to a satisfactory proof is as follows. It is possible to define the volume of the $n$ -parallelepiped inductively. We say the volume of the 1-parallelepiped $\overline{\text{vol}}(P) = ||v||$ . The volume of the $n$ -parallelepiped is then defined by $\overline{\text{vol}}_N(v_1, \ldots, v_N) = \overline{\text{vol}}_{N-1}(v_1, \ldots, v_{N-1}) \cdot ||v_{N, \perp}||$ where $v_{N, \perp}$ is the component of $v_N$ which is orthogonal to the span of $\{v_1, \ldots, v_{N-1}\}$ . This component may be called the $N^{\text{th}}$ altitude of the parallelepiped. Under this definition the proof follows because, from $(*)$ , $||v_{1, \perp}|| + ||w_{\perp}|| = ||(v_1 + w)_{\perp}||$ . This proof is essentially a conversion of the image above into a more rigorous definition and proof. I think the quick proof I've given here lacks something defining the sign of $\overline{\text{vol}_N}$ . The problems with this proof are the lack of control of the sign of the volume and that the definition of $\overline{\text{vol}}_N$ is not based on an integral. One appropriate answer to the question I am asking in this thread would be a way to control the sign of $\overline{\text{vol}_N}$ and relate $\overline{\text{vol}_N}$ to the integration based definition of volume. This would essentially be an equation relating $\text{svol}$ and $\overline{\text{vol}_N}$ . Comment About Sign I realize that to define $\text{sgn}(\text{svol}(v_1,\ldots, v_N))$ that we must make some convention choice. This convention choice should be consistent with $\text{sgn}(\text{svol}(e_1,\ldots,e_N)) = +1$ where $\{e_1, \ldots, e_N\}$ is the standard basis for $\mathbb{R}^N$ . Beyond that, I'm not sure how to define the sign in a way that does not use any facts about determinants . I don't know if there's a way to determine if two sets of $N$ vectors, $\{v_1, \ldots, v_N\}$ and $\{w_1, \ldots, w_N\}$ have the same orientation other than computing the sign of the determinant. If someone has an answer here I would appreciate it. If not I would be comfortable with the following. Let $$ D(v_1, \ldots, v_N) = \sum_{\sigma \in S_N} \text{sgn}(\sigma) \prod_{i=1}^N v_{i, \sigma(i)} $$ where $S_N$ is the symmetric group of size $N$ and if $j=\sigma(i)$ then $v_{i, j}$ indicates the $j^{\text{th}}$ component of vector $v_i$ . Then let $$ \text{sgn}(\text{svol}(v_1, \ldots, v_N)) = \text{sgn}(D(v_1, \ldots, v_N)) $$ so that $$ \text{svol}(v_1, \ldots, v_N) = \text{sgn}(D(v_1, \ldots, v_N)) \cdot \text{vol}(v_1, \ldots, v_N) $$ This last section essentially serves as an answer to my first question above. The question of how to prove $(*)$ from this definition still remains.","Overview I am trying to build some intuition about the volumes of parallelepipeds and determinants. I would like to define the determinant as the unique function of vectors in which is multilinear, anti-symmetric, and normalized and then show that the signed volume of the -parallelepiped satisfies these conditions without using any facts about determinants . The challenging one here for me is multi-linearity. Furthermore, I would like the signed volume to be defined in terms of Lebesgue integrals. Problem Setup Let . A parallelepiped is the set My goal is to define a function which is the signed volume of the parallelepiped . The volume of the parallelepiped is defined as follows. Let be the indicator function on the set . Where the integral is the Lebesgue integral. We should have We must additionally define, or provide an algorithm, to set the sign of . It is apparently the case, once everything above has been set up correctly, that for and The Direct Questions My questions are How should be defined such that (1) it is defined in terms of an integral over and (2) we are able to prove . Given the definition that answers the previous question, how do we prove ? An Illustrative Image The answer to this question includes a beautiful illustration the depicts exactly what I am trying to prove. It is intuitively clear to me from this diagram and Cavalieri's principle that should hold. But of course, for me, the illustration does not constitute a fully rigorous proof following the conditions I laid out above. An Almost Solution The closest I have come to a satisfactory proof is as follows. It is possible to define the volume of the -parallelepiped inductively. We say the volume of the 1-parallelepiped . The volume of the -parallelepiped is then defined by where is the component of which is orthogonal to the span of . This component may be called the altitude of the parallelepiped. Under this definition the proof follows because, from , . This proof is essentially a conversion of the image above into a more rigorous definition and proof. I think the quick proof I've given here lacks something defining the sign of . The problems with this proof are the lack of control of the sign of the volume and that the definition of is not based on an integral. One appropriate answer to the question I am asking in this thread would be a way to control the sign of and relate to the integration based definition of volume. This would essentially be an equation relating and . Comment About Sign I realize that to define that we must make some convention choice. This convention choice should be consistent with where is the standard basis for . Beyond that, I'm not sure how to define the sign in a way that does not use any facts about determinants . I don't know if there's a way to determine if two sets of vectors, and have the same orientation other than computing the sign of the determinant. If someone has an answer here I would appreciate it. If not I would be comfortable with the following. Let where is the symmetric group of size and if then indicates the component of vector . Then let so that This last section essentially serves as an answer to my first question above. The question of how to prove from this definition still remains.","N \mathbb{R}^N N v_1, \ldots, v_N \in \mathbb{R}^N P = P(v_1, \ldots, v_N) \subset \mathbb{R}^N 
P = P(v_1, \ldots, v_N) = \left\{\sum_{i=1}^N t_i v_i \mid 0 \le t_i \le 1 \text{ for i from 1 to N} \right\}
 \text{svol}(P) = \text{svol}(v_1,\ldots, v_N) P \text{vol} 1_P P 
\text{vol}(P) = \text{vol}(v_1, \ldots, v_N) = \int_{\mathbb{R}^N} 1_P dV
 
|\text{svol}(P)| = \text{vol}(P)
 \text{svol}(P) a, b \in \mathbb{R} w \in \mathbb{R}^N \begin{equation}
\tag{*}
\text{svol}(av_1 + bw, v_2, \ldots v_N) = a\cdot \text{svol}(v_1, v_2, \ldots, v_N) + b\cdot \text{svol}(w, v_2, \ldots v_N)
\end{equation} \text{svol}(v_1,\ldots, v_N) \mathbb{R}^N (*) (*) (*) n \overline{\text{vol}}(P) = ||v|| n \overline{\text{vol}}_N(v_1, \ldots, v_N) = \overline{\text{vol}}_{N-1}(v_1, \ldots, v_{N-1}) \cdot ||v_{N, \perp}|| v_{N, \perp} v_N \{v_1, \ldots, v_{N-1}\} N^{\text{th}} (*) ||v_{1, \perp}|| + ||w_{\perp}|| = ||(v_1 + w)_{\perp}|| \overline{\text{vol}_N} \overline{\text{vol}}_N \overline{\text{vol}_N} \overline{\text{vol}_N} \text{svol} \overline{\text{vol}_N} \text{sgn}(\text{svol}(v_1,\ldots, v_N)) \text{sgn}(\text{svol}(e_1,\ldots,e_N)) = +1 \{e_1, \ldots, e_N\} \mathbb{R}^N N \{v_1, \ldots, v_N\} \{w_1, \ldots, w_N\} 
D(v_1, \ldots, v_N) = \sum_{\sigma \in S_N} \text{sgn}(\sigma) \prod_{i=1}^N v_{i, \sigma(i)}
 S_N N j=\sigma(i) v_{i, j} j^{\text{th}} v_i 
\text{sgn}(\text{svol}(v_1, \ldots, v_N)) = \text{sgn}(D(v_1, \ldots, v_N))
 
\text{svol}(v_1, \ldots, v_N) = \text{sgn}(D(v_1, \ldots, v_N)) \cdot \text{vol}(v_1, \ldots, v_N)
 (*)","['linear-algebra', 'integration', 'determinant', 'volume']"
25,Matrices and influence of small error on their inverse matrices.,Matrices and influence of small error on their inverse matrices.,,"Find (nontrivial) matrix $A\in M_n$ such, that only small error in its element has small influence on inverse matrix, it means that this matrix will be different from $A^{-1}$ only little bit. Then find (nontrivial) matrix $B\in M_n$ such, that only small error in its element has bigg influence on inverse matrix, it means that this matrix will be different from $B^{-1}$ a lot. I have already find these matrices. Consider matrix real matrix $A$ $$A=\begin{pmatrix}0 & 1\\ 1 & 1\end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$A^{-1}=\begin{pmatrix}-1 & 1\\ 1 & 0\end{pmatrix}.$$ Now consider matrix $A+\Delta A$ in the form $$A+\Delta A=\begin{pmatrix}\frac{1}{1000} & 1\\ 1 & 1\end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$(A+\Delta A)^{-1}=\begin{pmatrix}-\frac{1000}{999} & \frac{1000}{999}\\ \frac{1000}{999} & -\frac{1}{999}\end{pmatrix}.$$ It means that $A^{-1}$ and $(A+\Delta A)^{-1}$ differ only little bit. But when I consider matrix $B$ $$B=\begin{pmatrix}\frac{1001}{1000} & 1\\ 1 & 1 \end{pmatrix},$$ then the inverse matrix $B^{-1}$ is $$B^{-1}=\begin{pmatrix}1000 & -1000\\ -1000 & 1001\end{pmatrix}.$$ Now consider matrix $B+\Delta B$ in the form $$B+\Delta B=\begin{pmatrix}\frac{1002}{1000} & 1\\ 1 & 1 \end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$(B+\Delta B)^{-1}=\begin{pmatrix}500 & -500\\ -500 & 501\end{pmatrix}.$$ It means that $B^{-1}$ and $(B+\Delta B)^{-1}$ differ a lot. But furthermore what I should do, it is not only to find matrices, but also formally justify it for any matrix. The hint is that I should use knowledges about decompositions of matrices and about condition number, also I can use this relation $$\frac{||(A+\Delta A)^{-1}-A^{-1}||}{||A^{-1}||}\leq \kappa(A)\frac{||\Delta A||}{||A||},$$ where $\kappa(A)$ is condition number of matrix $A$ . I think that it can be something with regularity and singularity of matrix. My matrix $A$ is  ""strong"" regular and difference between these two inverse matrices was small. On the other hand the matrix $B$ was near to singular and the difference between inverse matrices was bigg. I have not idea how to deal with formally and more generally. Any help will be appreciated. Thank you very much.","Find (nontrivial) matrix such, that only small error in its element has small influence on inverse matrix, it means that this matrix will be different from only little bit. Then find (nontrivial) matrix such, that only small error in its element has bigg influence on inverse matrix, it means that this matrix will be different from a lot. I have already find these matrices. Consider matrix real matrix then the inverse matrix is Now consider matrix in the form then the inverse matrix is It means that and differ only little bit. But when I consider matrix then the inverse matrix is Now consider matrix in the form then the inverse matrix is It means that and differ a lot. But furthermore what I should do, it is not only to find matrices, but also formally justify it for any matrix. The hint is that I should use knowledges about decompositions of matrices and about condition number, also I can use this relation where is condition number of matrix . I think that it can be something with regularity and singularity of matrix. My matrix is  ""strong"" regular and difference between these two inverse matrices was small. On the other hand the matrix was near to singular and the difference between inverse matrices was bigg. I have not idea how to deal with formally and more generally. Any help will be appreciated. Thank you very much.","A\in M_n A^{-1} B\in M_n B^{-1} A A=\begin{pmatrix}0 & 1\\
1 & 1\end{pmatrix}, A^{-1} A^{-1}=\begin{pmatrix}-1 & 1\\
1 & 0\end{pmatrix}. A+\Delta A A+\Delta A=\begin{pmatrix}\frac{1}{1000} & 1\\
1 & 1\end{pmatrix}, A^{-1} (A+\Delta A)^{-1}=\begin{pmatrix}-\frac{1000}{999} & \frac{1000}{999}\\
\frac{1000}{999} & -\frac{1}{999}\end{pmatrix}. A^{-1} (A+\Delta A)^{-1} B B=\begin{pmatrix}\frac{1001}{1000} & 1\\
1 & 1 \end{pmatrix}, B^{-1} B^{-1}=\begin{pmatrix}1000 & -1000\\
-1000 & 1001\end{pmatrix}. B+\Delta B B+\Delta B=\begin{pmatrix}\frac{1002}{1000} & 1\\
1 & 1 \end{pmatrix}, A^{-1} (B+\Delta B)^{-1}=\begin{pmatrix}500 & -500\\
-500 & 501\end{pmatrix}. B^{-1} (B+\Delta B)^{-1} \frac{||(A+\Delta A)^{-1}-A^{-1}||}{||A^{-1}||}\leq \kappa(A)\frac{||\Delta A||}{||A||}, \kappa(A) A A B","['linear-algebra', 'matrices', 'matrix-decomposition', 'condition-number']"
26,Finding the least point of numerical range which lies on real axis,Finding the least point of numerical range which lies on real axis,,"Given $A\in\mathbb{C}^{n\times n}$ , its numerical range is $W(A):=\{(\Re (x^*Ax),\Im(x^*Ax)) :x^*x=1\}\subseteq \mathbb{C}^2$ . $W(A)$ is a convex set. An example of $W(A)$ plot is given in the figure below (red line is the boundary of $W(A)$ ). $\hspace{6cm}$ Given $A$ , I want to find the point on the boundary of $W(A)$ , the point which is minimum on the real-axis (blue point on the figure). I think we need to minimize real-coordinate and make a constraint that imaginary-coordinate is zero: \begin{array}{ll} \underset{x\in\mathbb{C}^{n}}{\text{min}} & \Re(x^*Ax)\\ \text{s.t.} & \Im(x^*Ax)=0,\\&x^*x=1.\end{array} Since $\Re(x^*Ax)=\frac{A+A^*}{2}$ and $\Im(x^*Ax)=\frac{A-A^*}{2i}$ , we get this problem \begin{array}{ll} \underset{x\in\mathbb{C}^{n}}{\text{min}} & x^*(\frac{A+A^*}{2})x\qquad=&\underset{x\in\mathbb{C}^{n}}{\text{min}} & \frac{1}{2}(x^*Ax+x^*A^*x)\qquad=&\underset{x\in\mathbb{C}^{n}}{\text{min}} & x^*Ax\\ \text{ s.t.} & x^*(\frac{A-A^*}{2i})x=0,&\text{ s.t.} & x^*Ax=x^*A^*x,&\text{ s.t.} & x^*x=1\\&x^*x=1&&x^*x=1\end{array} But $x^*Ax$ might be complex in general and minimization of $x^*Ax$ is not possible then, I couldn't find where I am doing wrong.","Given , its numerical range is . is a convex set. An example of plot is given in the figure below (red line is the boundary of ). Given , I want to find the point on the boundary of , the point which is minimum on the real-axis (blue point on the figure). I think we need to minimize real-coordinate and make a constraint that imaginary-coordinate is zero: Since and , we get this problem But might be complex in general and minimization of is not possible then, I couldn't find where I am doing wrong.","A\in\mathbb{C}^{n\times n} W(A):=\{(\Re (x^*Ax),\Im(x^*Ax)) :x^*x=1\}\subseteq \mathbb{C}^2 W(A) W(A) W(A) \hspace{6cm} A W(A) \begin{array}{ll} \underset{x\in\mathbb{C}^{n}}{\text{min}} & \Re(x^*Ax)\\ \text{s.t.} & \Im(x^*Ax)=0,\\&x^*x=1.\end{array} \Re(x^*Ax)=\frac{A+A^*}{2} \Im(x^*Ax)=\frac{A-A^*}{2i} \begin{array}{ll} \underset{x\in\mathbb{C}^{n}}{\text{min}} & x^*(\frac{A+A^*}{2})x\qquad=&\underset{x\in\mathbb{C}^{n}}{\text{min}} & \frac{1}{2}(x^*Ax+x^*A^*x)\qquad=&\underset{x\in\mathbb{C}^{n}}{\text{min}} & x^*Ax\\ \text{ s.t.} & x^*(\frac{A-A^*}{2i})x=0,&\text{ s.t.} & x^*Ax=x^*A^*x,&\text{ s.t.} & x^*x=1\\&x^*x=1&&x^*x=1\end{array} x^*Ax x^*Ax","['linear-algebra', 'optimization']"
27,Does there exist $n\times n$ matrices such that $BABCB-BCBAB=B$?,Does there exist  matrices such that ?,n\times n BABCB-BCBAB=B,"Found this past paper exercise and I can't figure out how to go with it. Suppose $n \times n$ matrix $B$ , non invertible, non zero. Does there exist $n\times n$ matrices $A,C$ such that: a) $BABCB - BCBAB = B$ b) $ACB - CAB = B$ My try: a) $BABCB - BCBAB = B$ $B(ABC-CBA)B=B$ Then, $ABC-CBA$ is the left inverse or right inverse of $B$ . But I can not continue from here. Also, tried the trace cycle property, which leads nowhere. The determinant properties give no important results, since $det(B)=0$ .","Found this past paper exercise and I can't figure out how to go with it. Suppose matrix , non invertible, non zero. Does there exist matrices such that: a) b) My try: a) Then, is the left inverse or right inverse of . But I can not continue from here. Also, tried the trace cycle property, which leads nowhere. The determinant properties give no important results, since .","n \times n B n\times n A,C BABCB - BCBAB = B ACB - CAB = B BABCB - BCBAB = B B(ABC-CBA)B=B ABC-CBA B det(B)=0","['linear-algebra', 'matrices', 'matrix-equations']"
28,Natural inner product on the Hom space.,Natural inner product on the Hom space.,,"Let $P$ and $V$ be vector spaces of dimension $k$ and $n$ , respectively. I want to know if it's possible to endow $\text{Hom}(P,V)$ with a natural inner product, but without using the (non-canonical) isomorphism between $V$ and $V^*$ and then considering something like $\langle{\phi},{\psi}\rangle = \text{tr}(\phi^{\top} \circ \psi)$ . What comes to my mind and what is more suitable to the setup I'm working on is something like this: assume that $P$ and $V$ have inner products $\langle \cdot, \cdot \rangle_P$ and $\langle \cdot, \cdot \rangle_V$ , respectively. Then, we can identify $P \otimes V$ with $\text{Hom}(P,V)$ using the map $p \otimes v \mapsto \langle \cdot, p \rangle_P v$ and we can consider the canonical inner product on $P \otimes V$ given by $\langle p \otimes v, q \otimes w \rangle_{P \otimes V} := \langle p, q \rangle_P \langle v, w \rangle_P$ . But how this translates to the Hom space? I've tried this: choosing basis $\{p_1,\ldots,p_k\}$ and $\{v_1,\ldots,v_n\}$ (and I can't assume those basis are orthogonal) and defining $G$ as the Gram matrix of $V$ , ie, $G = (\langle v_i, v_j \rangle_V)_{i,j}$ , I've identified $\text{Hom}(P,V)$ as the space of matrices $n \times k$ written in those basis and then defined the inner product as $\text{tr}(A^{\top}GB)$ . But this formula is wrong because on $P \otimes V$ we have $\langle{p_i \otimes v_r},{p_j \otimes v_s}\rangle_{P \otimes V} = \langle{p_i},{p_j}\rangle_P \langle{v_r},{v_s}\rangle_V$ , but the inner product $\text{tr}(A^{\top}GB)$ on $\text{Hom}(P,V)$ computed on the corresponding matrices of the linear transformations $A := \langle \cdot,p_i \rangle_P v_r$ and $B := \langle \cdot,p_j \rangle_P v_s$ gives me $$\sum_{\lambda=1}^k \langle p_{\lambda},p_i \rangle_P \langle v_r,v_s \rangle_V \langle p_{\lambda},p_j \rangle_V$$ and this is not what I expected. Any idea how should I proceed?","Let and be vector spaces of dimension and , respectively. I want to know if it's possible to endow with a natural inner product, but without using the (non-canonical) isomorphism between and and then considering something like . What comes to my mind and what is more suitable to the setup I'm working on is something like this: assume that and have inner products and , respectively. Then, we can identify with using the map and we can consider the canonical inner product on given by . But how this translates to the Hom space? I've tried this: choosing basis and (and I can't assume those basis are orthogonal) and defining as the Gram matrix of , ie, , I've identified as the space of matrices written in those basis and then defined the inner product as . But this formula is wrong because on we have , but the inner product on computed on the corresponding matrices of the linear transformations and gives me and this is not what I expected. Any idea how should I proceed?","P V k n \text{Hom}(P,V) V V^* \langle{\phi},{\psi}\rangle = \text{tr}(\phi^{\top} \circ \psi) P V \langle \cdot, \cdot \rangle_P \langle \cdot, \cdot \rangle_V P \otimes V \text{Hom}(P,V) p \otimes v \mapsto \langle \cdot, p \rangle_P v P \otimes V \langle p \otimes v, q \otimes w \rangle_{P \otimes V} := \langle p, q \rangle_P \langle v, w \rangle_P \{p_1,\ldots,p_k\} \{v_1,\ldots,v_n\} G V G = (\langle v_i, v_j \rangle_V)_{i,j} \text{Hom}(P,V) n \times k \text{tr}(A^{\top}GB) P \otimes V \langle{p_i \otimes v_r},{p_j \otimes v_s}\rangle_{P \otimes V} = \langle{p_i},{p_j}\rangle_P \langle{v_r},{v_s}\rangle_V \text{tr}(A^{\top}GB) \text{Hom}(P,V) A := \langle \cdot,p_i \rangle_P v_r B := \langle \cdot,p_j \rangle_P v_s \sum_{\lambda=1}^k \langle p_{\lambda},p_i \rangle_P \langle v_r,v_s \rangle_V \langle
p_{\lambda},p_j \rangle_V",['linear-algebra']
29,Graduate level linear algebra?,Graduate level linear algebra?,,"Basically, I have a few related questions. The first one may seem a little naive, so please forgive me. Is there much linear algebra to be taught beyond the level of books like Axler or Friedberg, Insel, and Spence? Is linear algebra typically taught beyond this advanced undergraduate/early graduate level, or would it just start to become abstract algebra and/or functional analysis? If linear algebra is indeed taught at the graduate level, I would love some recommendations for some of the most commonly used textbooks. I hope I phrased my question clearly and correctly, this is my first time posting here.","Basically, I have a few related questions. The first one may seem a little naive, so please forgive me. Is there much linear algebra to be taught beyond the level of books like Axler or Friedberg, Insel, and Spence? Is linear algebra typically taught beyond this advanced undergraduate/early graduate level, or would it just start to become abstract algebra and/or functional analysis? If linear algebra is indeed taught at the graduate level, I would love some recommendations for some of the most commonly used textbooks. I hope I phrased my question clearly and correctly, this is my first time posting here.",,"['linear-algebra', 'book-recommendation']"
30,Solutions to equation of $6$ variables,Solutions to equation of  variables,6,"Preliminaries: Let $M$ be any $4\times 4$ unitary matrix. Now, define the matrices $A$ and $B$ by $$A=M^\dagger\left[ \begin{pmatrix} \cos(\theta) & -e^{i\lambda}\sin(\theta)\\  e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta) \end{pmatrix}\otimes \begin{pmatrix} 1 & 0\\  0 &1  \end{pmatrix}\right]M$$ $$B=M^\dagger\left[\begin{pmatrix} 1 & 0\\  0 &1  \end{pmatrix}\otimes \begin{pmatrix} \cos(\theta) & -e^{i\lambda}\sin(\theta)\\  e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta) \end{pmatrix}\right]M$$ We may then define $f:\mathbb{R}^6\to \mathbb{R}$ by $$f(\theta,\lambda,\phi,\delta,\alpha,\beta)=\prod_{C\in\{A,B\}}\left(\left|\begin{pmatrix} 0\\  0\\  1\\  0 \end{pmatrix}^TC\begin{pmatrix} \cos(\delta)e^{i\alpha}\\  \sin(\delta)e^{i\beta}\\\  0\\  0 \end{pmatrix}\right|+\left|\begin{pmatrix} 0\\  0\\  0\\  1 \end{pmatrix}^TC\begin{pmatrix} \cos(\delta)e^{i\alpha}\\  \sin(\delta)e^{i\beta}\\\  0\\  0 \end{pmatrix}\right|\right)$$ Now, what are the zeros of this function? It is easy to show that any element of the set $$S=\{(k_1\pi ,\lambda, 2\pi k_2-\lambda,\delta,\alpha,\beta):k_1,k_2\in\mathbb{Z}\text{ and }\lambda,\delta,\alpha,\beta\in\mathbb{R}\}$$ is a solution to $f(\theta,\lambda,\phi,\delta,\alpha,\beta)=0$ . This is because $$\begin{pmatrix} \cos(\theta) & -e^{i\lambda}\sin(\theta)\\  e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta) \end{pmatrix}\Bigg|_{\theta=k_1 \pi,\phi=2\pi k_2-\lambda}=(-1)^{k_1}\begin{pmatrix} 1 & 0\\  0 & 1 \end{pmatrix}$$ and the matrix $A$ collapses down to $$A=M^\dagger[(-1)^{k_1}I_4]M=(-1)^{k_1}M^\dagger M=(-1)^{k_1}I_4$$ (where $I_4$ is the $4\times 4$ identity matrix), which in turn means all the inner products are zero. My question: Is there some $4\times 4$ unitary matrix $M$ such that $S$ is all the solutions to the equation $f(\theta,\lambda,\phi,\delta,\alpha,\beta)=0$ ? Motivation: If I could prove this question in the negative (there are no such matrices $M$ ), then I could show that at least $3$ qubits are required for an error detection code I am developing for a quantum computer. I won't bore you with all the details (unless someone wants to know more), but my problem has simplified to the linear algebra question above. Work: So far, I have tried a lot of different ways of proving the above without much success. For any set matrix $M$ , I can always find solutions that are not in $S$ . However, it does seem like almost all the degrees of freedom are required because there are some $M$ where the solution seems to lie on a line in $\mathbb{R}^6$ space (basically it depends on the difference between $\alpha$ and $\beta$ ). That is, it does not form a family of solutions on a plane/hyper-plane in the space. My current work is to try to use different generators of $U(4)$ and see if I can say something if I decompose $M$ into constituent parts (Here are two different papers I have been using for said generators). Unfortunately, this has not simplified my problem to the point where I can get a satisfactory solution.","Preliminaries: Let be any unitary matrix. Now, define the matrices and by We may then define by Now, what are the zeros of this function? It is easy to show that any element of the set is a solution to . This is because and the matrix collapses down to (where is the identity matrix), which in turn means all the inner products are zero. My question: Is there some unitary matrix such that is all the solutions to the equation ? Motivation: If I could prove this question in the negative (there are no such matrices ), then I could show that at least qubits are required for an error detection code I am developing for a quantum computer. I won't bore you with all the details (unless someone wants to know more), but my problem has simplified to the linear algebra question above. Work: So far, I have tried a lot of different ways of proving the above without much success. For any set matrix , I can always find solutions that are not in . However, it does seem like almost all the degrees of freedom are required because there are some where the solution seems to lie on a line in space (basically it depends on the difference between and ). That is, it does not form a family of solutions on a plane/hyper-plane in the space. My current work is to try to use different generators of and see if I can say something if I decompose into constituent parts (Here are two different papers I have been using for said generators). Unfortunately, this has not simplified my problem to the point where I can get a satisfactory solution.","M 4\times 4 A B A=M^\dagger\left[ \begin{pmatrix}
\cos(\theta) & -e^{i\lambda}\sin(\theta)\\ 
e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta)
\end{pmatrix}\otimes \begin{pmatrix}
1 & 0\\ 
0 &1 
\end{pmatrix}\right]M B=M^\dagger\left[\begin{pmatrix}
1 & 0\\ 
0 &1 
\end{pmatrix}\otimes \begin{pmatrix}
\cos(\theta) & -e^{i\lambda}\sin(\theta)\\ 
e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta)
\end{pmatrix}\right]M f:\mathbb{R}^6\to \mathbb{R} f(\theta,\lambda,\phi,\delta,\alpha,\beta)=\prod_{C\in\{A,B\}}\left(\left|\begin{pmatrix}
0\\ 
0\\ 
1\\ 
0
\end{pmatrix}^TC\begin{pmatrix}
\cos(\delta)e^{i\alpha}\\ 
\sin(\delta)e^{i\beta}\\\ 
0\\ 
0
\end{pmatrix}\right|+\left|\begin{pmatrix}
0\\ 
0\\ 
0\\ 
1
\end{pmatrix}^TC\begin{pmatrix}
\cos(\delta)e^{i\alpha}\\ 
\sin(\delta)e^{i\beta}\\\ 
0\\ 
0
\end{pmatrix}\right|\right) S=\{(k_1\pi ,\lambda, 2\pi k_2-\lambda,\delta,\alpha,\beta):k_1,k_2\in\mathbb{Z}\text{ and }\lambda,\delta,\alpha,\beta\in\mathbb{R}\} f(\theta,\lambda,\phi,\delta,\alpha,\beta)=0 \begin{pmatrix}
\cos(\theta) & -e^{i\lambda}\sin(\theta)\\ 
e^{i\phi}\sin(\theta) & e^{i(\lambda+\phi)}\cos(\theta)
\end{pmatrix}\Bigg|_{\theta=k_1 \pi,\phi=2\pi k_2-\lambda}=(-1)^{k_1}\begin{pmatrix}
1 & 0\\ 
0 & 1
\end{pmatrix} A A=M^\dagger[(-1)^{k_1}I_4]M=(-1)^{k_1}M^\dagger M=(-1)^{k_1}I_4 I_4 4\times 4 4\times 4 M S f(\theta,\lambda,\phi,\delta,\alpha,\beta)=0 M 3 M S M \mathbb{R}^6 \alpha \beta U(4) M","['linear-algebra', 'unitary-matrices']"
31,Eigenvalues and Eigenvectors of $A=I-\alpha vv^{T}$,Eigenvalues and Eigenvectors of,A=I-\alpha vv^{T},"Consider the matrix $A$ given by $A=I-\alpha vv^{T}$ with $v\neq0$ and $v\in\mathbb{R}^{n}$ and $\alpha\neq0$ . we want to show that there are two distinct eigenvalues $\lambda_{1},\lambda_{2}$ to be found with their corresponding eigenvectors $x_{\lambda_{1}}$ and $x_{\lambda_{2}}$ . My attempt : By definition, we have that $Ax=\lambda x$ thus : $$ (I-\alpha vv^{T})x=x-\alpha vv^{T}x=x-(\alpha v^{T}x)v $$ One can easily notice that $v$ is nothing but a scalar multiple of $x$ that is to say $x=\beta v$ and thus we have that for $x=v$ we get : $$ Av=(1-\alpha v^{T}v)v $$ Thus, an eigenvalue of $A$ is $\lambda_{1}=1-\alpha v^{T}v$ . I am unable to find the second eigenvalue nor the corresponding eigenvectors. I would truly appreciate help as I am lost in the process.","Consider the matrix given by with and and . we want to show that there are two distinct eigenvalues to be found with their corresponding eigenvectors and . My attempt : By definition, we have that thus : One can easily notice that is nothing but a scalar multiple of that is to say and thus we have that for we get : Thus, an eigenvalue of is . I am unable to find the second eigenvalue nor the corresponding eigenvectors. I would truly appreciate help as I am lost in the process.","A A=I-\alpha vv^{T} v\neq0 v\in\mathbb{R}^{n} \alpha\neq0 \lambda_{1},\lambda_{2} x_{\lambda_{1}} x_{\lambda_{2}} Ax=\lambda x 
(I-\alpha vv^{T})x=x-\alpha vv^{T}x=x-(\alpha v^{T}x)v
 v x x=\beta v x=v 
Av=(1-\alpha v^{T}v)v
 A \lambda_{1}=1-\alpha v^{T}v",['linear-algebra']
32,"Is there an established notation for $A_1A_2\dots A_n$, a product of matrices?","Is there an established notation for , a product of matrices?",A_1A_2\dots A_n,"I've taken a course or two in numerical analysis and often several matrices in a family- say $A_i\in \mathbb{C}^{m\times m},1\le i\le n$ - need to be multiplied. I haven't encountered an established notation for this, so I often defined and used the usual notation for products with success: $$ A_1 A_2\dots A_{n-1}A_n = \prod_{k=1}^{n}A_k $$ If instead we are multiplying in the opposite order, one could just 'flip' the product: $$ A_n A_{n-1}\dots A_2 A_1 = \coprod_{k=1}^{n} A_k \left(=\prod_{k=1}^{n}A_{n+1-k}\right) $$ Is there a more conventional notation for this procedure?","I've taken a course or two in numerical analysis and often several matrices in a family- say - need to be multiplied. I haven't encountered an established notation for this, so I often defined and used the usual notation for products with success: If instead we are multiplying in the opposite order, one could just 'flip' the product: Is there a more conventional notation for this procedure?","A_i\in \mathbb{C}^{m\times m},1\le i\le n 
A_1 A_2\dots A_{n-1}A_n = \prod_{k=1}^{n}A_k
 
A_n A_{n-1}\dots A_2 A_1 = \coprod_{k=1}^{n} A_k \left(=\prod_{k=1}^{n}A_{n+1-k}\right)
","['linear-algebra', 'matrices', 'notation', 'numerical-linear-algebra']"
33,Confusion in Notation for Array and Dimension,Confusion in Notation for Array and Dimension,,"I have a vector $\mathbf{x} = (x_1,x_2)$ , how do I represent the array of such vector? Is it $\mathbf{x} \in \mathbf{X}$ where $\mathbf{X}$ = $\bigcup _{{1}}^{k}\mathbf{x}^s$ for $s \in (1,..k)$ length of $k$ vectors? Something seems off here! Also I want to index a vector in array then is this correct way to represent the index ${\mathbf{x}^1}$ ? and the corresponding dimensional representation of this index shall be $(x_1^1,x_2^1)$ ? Is the usage of ""hat"" appropriate here ? such as "" $\hat{x}$ ""?","I have a vector , how do I represent the array of such vector? Is it where = for length of vectors? Something seems off here! Also I want to index a vector in array then is this correct way to represent the index ? and the corresponding dimensional representation of this index shall be ? Is the usage of ""hat"" appropriate here ? such as "" ""?","\mathbf{x} = (x_1,x_2) \mathbf{x} \in \mathbf{X} \mathbf{X} \bigcup _{{1}}^{k}\mathbf{x}^s s \in (1,..k) k {\mathbf{x}^1} (x_1^1,x_2^1) \hat{x}","['linear-algebra', 'notation', 'index-notation']"
34,Orthogonal and Orthonormal Matrix,Orthogonal and Orthonormal Matrix,,I know that the columns of an orthogonal matrix are perpendicular to each other and additionally if the columns have unit length then they are orthonormal. But my professor states that the columns of an orthogonal matrix form an orthonormal basis? Is this right? Then what is the difference between orthogonal and orthonormal matrix?,I know that the columns of an orthogonal matrix are perpendicular to each other and additionally if the columns have unit length then they are orthonormal. But my professor states that the columns of an orthogonal matrix form an orthonormal basis? Is this right? Then what is the difference between orthogonal and orthonormal matrix?,,['linear-algebra']
35,Proof of the spectral decomposition theorem for normal operators on a finite-dimensional vector space,Proof of the spectral decomposition theorem for normal operators on a finite-dimensional vector space,,"So, I was reading the book Nielsen and Chuang and it's introductory chapter on Quantum Mechanics and it had a theorem called the ""Spectral Decomposition Theorem"" which states that an Operator $M$ is a normal operator if and only if it can be diagonalized in the orthonormal basis (which turn out to be the eigen-vectors). Now, I have some trouble understanding the forward proof, which was to prove that if I have a Normal operator, it can be diagonalized. I am attaching the proof that is written in the book. Ultimately, it uses Projectors onto eigenspaces of a particular eigen value $P_\lambda=\sum_i |\lambda ; i\rangle \langle\lambda;i |$ and it's Orthogonal complement $Q_\lambda \equiv I-P_\lambda$ and rewrite $M=(P_\lambda+Q_\lambda)M(P_\lambda+Q_\lambda)$ which then simplifies to $M=P_\lambda MP_\lambda +Q_\lambda M Q_\lambda$ and using principle of mathematical induction, one can indeed prove that M can be diagonalized with respect to some orthonormal basis. The part of the proof that I don't understand is that somehow, it implies that the operator $M$ can be diagonalized with respect to eigenvectors of this Normal Operator and that they are necessarily orthogonal. Also, if I try to write down $M$ in outer-product representation, it somehow simplifies as $M=\sum_i \lambda_i |e_i\rangle\langle e_i|$ where $\lambda_i$ 's are the eigen-values and $|e_i\rangle$ 's are the eigenvectors. I have literally no idea how this decomposition was implied by the above arguments. Any sort of help in the understanding of this is appreciated. P.S.- I would really appreciate if you could explain the arguments with respect to the way that it is given in the proof in this book.","So, I was reading the book Nielsen and Chuang and it's introductory chapter on Quantum Mechanics and it had a theorem called the ""Spectral Decomposition Theorem"" which states that an Operator is a normal operator if and only if it can be diagonalized in the orthonormal basis (which turn out to be the eigen-vectors). Now, I have some trouble understanding the forward proof, which was to prove that if I have a Normal operator, it can be diagonalized. I am attaching the proof that is written in the book. Ultimately, it uses Projectors onto eigenspaces of a particular eigen value and it's Orthogonal complement and rewrite which then simplifies to and using principle of mathematical induction, one can indeed prove that M can be diagonalized with respect to some orthonormal basis. The part of the proof that I don't understand is that somehow, it implies that the operator can be diagonalized with respect to eigenvectors of this Normal Operator and that they are necessarily orthogonal. Also, if I try to write down in outer-product representation, it somehow simplifies as where 's are the eigen-values and 's are the eigenvectors. I have literally no idea how this decomposition was implied by the above arguments. Any sort of help in the understanding of this is appreciated. P.S.- I would really appreciate if you could explain the arguments with respect to the way that it is given in the proof in this book.",M P_\lambda=\sum_i |\lambda ; i\rangle \langle\lambda;i | Q_\lambda \equiv I-P_\lambda M=(P_\lambda+Q_\lambda)M(P_\lambda+Q_\lambda) M=P_\lambda MP_\lambda +Q_\lambda M Q_\lambda M M M=\sum_i \lambda_i |e_i\rangle\langle e_i| \lambda_i |e_i\rangle,['quantum-mechanics']
36,Why does MINRES converge in 3 iterations on matrices of specific form?,Why does MINRES converge in 3 iterations on matrices of specific form?,,"The MINRES algorithm for solving $Ax = b$ for symmetric $A$ can be described as follows: The $k$ -th iterate of the algorithm is $$x_k = \arg\min_{K_k(A)} \lVert Ax-b \rVert_2$$ where $K_k(A)=\text{span}\{A^ib\mid i < k\}$ is the $k$ -th Krylov subspace of $A$ By this definition, it is clear that it converges to the exact solution in $n$ iterations, if $A$ is an $n\times n$ matrix. By using this solver on matrices with a very specific structure, I noticed that in that case, the solver converges in just 3 iterations. The matrices are of the form $$ A = \begin{bmatrix} I_{n-1} & v\\ v^T & 0 \end{bmatrix} $$ where $v$ is a column vector and and $I_{n-1}$ is the $(n-1) \times (n-1)$ identity matrix. How can this early convergence be explained? My thoughts The matrix $A$ can in this case be seen as an Identity matrix plus 2 rank-one corrections: $$A = I_n + \begin{bmatrix} 0 & v\\ 0 & -\frac{1}{2} \end{bmatrix} + \begin{bmatrix} 0 & 0\\ v^T & -\frac{1}{2} \end{bmatrix}$$ This means that we can exactly invert the matrix using the Sherman-Morrison formula twice. I currently avoid doing this explicitly as it leads to instabilities, while 3 MINRES iterations do not. Maybe MINRES algorithm implicitly exploits the fact that we are just a rank-two matrix away from the identity? You can verify this behaviour with this example python snipet .","The MINRES algorithm for solving for symmetric can be described as follows: The -th iterate of the algorithm is where is the -th Krylov subspace of By this definition, it is clear that it converges to the exact solution in iterations, if is an matrix. By using this solver on matrices with a very specific structure, I noticed that in that case, the solver converges in just 3 iterations. The matrices are of the form where is a column vector and and is the identity matrix. How can this early convergence be explained? My thoughts The matrix can in this case be seen as an Identity matrix plus 2 rank-one corrections: This means that we can exactly invert the matrix using the Sherman-Morrison formula twice. I currently avoid doing this explicitly as it leads to instabilities, while 3 MINRES iterations do not. Maybe MINRES algorithm implicitly exploits the fact that we are just a rank-two matrix away from the identity? You can verify this behaviour with this example python snipet .","Ax = b A k x_k = \arg\min_{K_k(A)} \lVert Ax-b \rVert_2 K_k(A)=\text{span}\{A^ib\mid i < k\} k A n A n\times n 
A = \begin{bmatrix}
I_{n-1} & v\\
v^T & 0
\end{bmatrix}
 v I_{n-1} (n-1) \times (n-1) A A = I_n +
\begin{bmatrix}
0 & v\\
0 & -\frac{1}{2}
\end{bmatrix} +
\begin{bmatrix}
0 & 0\\
v^T & -\frac{1}{2}
\end{bmatrix}","['linear-algebra', 'numerical-linear-algebra', 'sparse-matrices']"
37,Kernel of $(I-A)^2$ where $A$ has unique real eigenvalue $1$,Kernel of  where  has unique real eigenvalue,(I-A)^2 A 1,"Assume I have a real square matrix $A$ with a simple eigenvalue $1$ and corresponding eigenvector $v$ . Then $v$ should span the kernel of the matrix $I-A$ , where $I$ is the identity matrix. Now I am wondering the following: does $v$ also span the kernel of $(I-A)^2$ ? Writing $(I-A)^2 x = 0 \iff (I-A)x = A(I-A)x$ this question should be equivalent to asking whether $\operatorname {span}(v) = \ker(I-A)$ can lie in the image of $I-A$ . Thanks in advance for any help on this matter.","Assume I have a real square matrix with a simple eigenvalue and corresponding eigenvector . Then should span the kernel of the matrix , where is the identity matrix. Now I am wondering the following: does also span the kernel of ? Writing this question should be equivalent to asking whether can lie in the image of . Thanks in advance for any help on this matter.",A 1 v v I-A I v (I-A)^2 (I-A)^2 x = 0 \iff (I-A)x = A(I-A)x \operatorname {span}(v) = \ker(I-A) I-A,"['linear-algebra', 'linear-transformations']"
38,Recursion for Characteristic Polynomial - Proof?,Recursion for Characteristic Polynomial - Proof?,,"In the book ""Computational Complexity of Counting and Sampling"" I have found the following theorem: It gives a recursion formula for a division-free algorithm for the determinant in $O(n^4)$ . Now, we have the feeling that each trace $\mathrm{tr}(w(\cdot,\cdot,k))$ yields the $k$ -th homogeneous coefficient of the characteristic polynomial. Examples up to 5 confirm this conjecture. Though, a proof is missing still. I already tried various things. It would be good if one can show that $\mathrm{tr}(w(\cdot,\cdot,k))$ is invariant under change of basis of $M$ . Then one can consider w.l.o.g. diagonal matrices. For these matrices, the proof is easy. However, an induction seems not helpful here. Moreover, when one iteratively insert the recursion, the expressions quickly get complicated. I don't need the full proof here. A hint/direction or useful identity is already appreciated.","In the book ""Computational Complexity of Counting and Sampling"" I have found the following theorem: It gives a recursion formula for a division-free algorithm for the determinant in . Now, we have the feeling that each trace yields the -th homogeneous coefficient of the characteristic polynomial. Examples up to 5 confirm this conjecture. Though, a proof is missing still. I already tried various things. It would be good if one can show that is invariant under change of basis of . Then one can consider w.l.o.g. diagonal matrices. For these matrices, the proof is easy. However, an induction seems not helpful here. Moreover, when one iteratively insert the recursion, the expressions quickly get complicated. I don't need the full proof here. A hint/direction or useful identity is already appreciated.","O(n^4) \mathrm{tr}(w(\cdot,\cdot,k)) k \mathrm{tr}(w(\cdot,\cdot,k)) M","['linear-algebra', 'computational-complexity']"
39,Conditions on the characteristic polynomial of a matrix with sines and cosines to has integer coefficients,Conditions on the characteristic polynomial of a matrix with sines and cosines to has integer coefficients,,"Let $A\in \mathsf{GL}(4,\mathbb{R})$ be the following matrix: $$A=\begin{pmatrix} \cos a&-\sin a&0&0\\ \sin a&\cos a&0&0\\0&0&\cos b&-\sin b\\0&0&\sin b&\cos b \end{pmatrix}$$ Assume that $A$ has finite order, i.e. $a$ and $b$ are rational multiples of $\pi$ . The problem is to give necessary conditions on the characteristic polynomial of $A$ to make it integer, i.e. $P_A(\lambda)\in\mathbb{Z}[\lambda]$ The characteristic polynomial is $P_A(\lambda)=\lambda^4-2\lambda^3(\cos a+\cos b)+\lambda^2(2+4\cos a\cos b)-2\lambda(\cos a+\cos b)+1$ (it is a symmetric polynomial). Now we want that \begin{cases} 2(\cos a+\cos b)\in\mathbb{Z}\\ 4\cos a\cos b\in \mathbb{Z} \end{cases} Question: In this case I've solved the system by hand and found the possible values of $a$ and $b$ but I wonder if there is a more efficient way to do this, or if there is some deeper theory involved, because I want to generalize to higher values (for example when there are three blocks, $a$ , $b$ , $c$ and so on). I see that in someway elementary symmetric polynomials appear but I don't know if this helps. Any comment or suggestion will be appreciated!","Let be the following matrix: Assume that has finite order, i.e. and are rational multiples of . The problem is to give necessary conditions on the characteristic polynomial of to make it integer, i.e. The characteristic polynomial is (it is a symmetric polynomial). Now we want that Question: In this case I've solved the system by hand and found the possible values of and but I wonder if there is a more efficient way to do this, or if there is some deeper theory involved, because I want to generalize to higher values (for example when there are three blocks, , , and so on). I see that in someway elementary symmetric polynomials appear but I don't know if this helps. Any comment or suggestion will be appreciated!","A\in \mathsf{GL}(4,\mathbb{R}) A=\begin{pmatrix}
\cos a&-\sin a&0&0\\ \sin a&\cos a&0&0\\0&0&\cos b&-\sin b\\0&0&\sin b&\cos b
\end{pmatrix} A a b \pi A P_A(\lambda)\in\mathbb{Z}[\lambda] P_A(\lambda)=\lambda^4-2\lambda^3(\cos a+\cos b)+\lambda^2(2+4\cos a\cos b)-2\lambda(\cos a+\cos b)+1 \begin{cases}
2(\cos a+\cos b)\in\mathbb{Z}\\
4\cos a\cos b\in \mathbb{Z}
\end{cases} a b a b c","['linear-algebra', 'matrices', 'symmetric-polynomials', 'characteristic-polynomial']"
40,Express $\operatorname{trace}(B'XB)$ in terms of $A$ and $B$,Express  in terms of  and,\operatorname{trace}(B'XB) A B,"Given $A\in\Bbb R^{n\times n}$ , $B\in\Bbb R^{n\times m}$ , and $X>0$ , s. t. $X=A'XA-A'XB(I+B'XB)^{-1}B'XA,$ where $A'$ is $A$ transpose. Is it possible to express $\operatorname{trace}(B'XB)$ in terms of $A$ and $B$ only (without $X$ )? If it helps, $(A,B)$ is stabilizable . Even for diagonal $A$ , the answer is not obvious. My attempt: I only have few equalities that I managed to deduce: $\operatorname{trace}(B'XB)=\operatorname{trace}(AX^{-1}A'X)-\operatorname{trace}(I)=\operatorname{trace}(AX^{-1}A'X)-n.$ $\operatorname{trace}(B'XB)=\sum\limits_{i=1}^m(B_i'XB_i)$ , where $B_i$ is the $i$ 'th column of $B$ . Let $A=\begin{bmatrix}a_1&&\\&a_2&\\ &&a_2\end{bmatrix}$ , then $\operatorname{trace}(B'XB)=a_1^2a_2^2 + a_2^2 -2$ . (i.e. independent of $B$ ) $\det(A_1)^2+\cdots+\det(A_m)^2\geqslant \operatorname{trace}(AX^{-1}A'X)\geqslant m\sqrt[m]{\det(A)^2}$ . To prove this part, we can do Wonham decomposition on $(A,B)$ then use 1 and 2 together with geometric mean. Is there any tighter bound than 4?","Given , , and , s. t. where is transpose. Is it possible to express in terms of and only (without )? If it helps, is stabilizable . Even for diagonal , the answer is not obvious. My attempt: I only have few equalities that I managed to deduce: , where is the 'th column of . Let , then . (i.e. independent of ) . To prove this part, we can do Wonham decomposition on then use 1 and 2 together with geometric mean. Is there any tighter bound than 4?","A\in\Bbb R^{n\times n} B\in\Bbb R^{n\times m} X>0 X=A'XA-A'XB(I+B'XB)^{-1}B'XA, A' A \operatorname{trace}(B'XB) A B X (A,B) A \operatorname{trace}(B'XB)=\operatorname{trace}(AX^{-1}A'X)-\operatorname{trace}(I)=\operatorname{trace}(AX^{-1}A'X)-n. \operatorname{trace}(B'XB)=\sum\limits_{i=1}^m(B_i'XB_i) B_i i B A=\begin{bmatrix}a_1&&\\&a_2&\\
&&a_2\end{bmatrix} \operatorname{trace}(B'XB)=a_1^2a_2^2 + a_2^2 -2 B \det(A_1)^2+\cdots+\det(A_m)^2\geqslant \operatorname{trace}(AX^{-1}A'X)\geqslant m\sqrt[m]{\det(A)^2} (A,B)","['linear-algebra', 'control-theory']"
41,Solving Recurrent Relations using Backtracking,Solving Recurrent Relations using Backtracking,,"The following formula has been provided: $a(n) = a(n-1) + a(n-2)\quad \mbox{with initial states}\quad 0 , 2  $ After some research the formula is found to be a Binet's Formula. It is required to convert the above recursive formula to an explicit formula using the Backtracking method. This is what i have done: \begin{align} a(n) &= a(n-1) + a(n-2) = (a(n-2) + a(n-3)) + a(n-2)       \\ & = 2a(n-2) + a(n-3) = 2(a(n-3) + a(n-4)) + a(n-3)        \\ & = 3a(n-3) + 2a(n-4) = 3(a(n-4) + a(n-5)) + 2a(n-4)       \\ & = 5a(n-4) + 3a(n-5) = 5(a(n-5) + a(n-6)) + 3a(n-5)       \\ & = 8a(n-5) + 5a(n-6)      \\ & \vdots     \\ & \mbox{and so on}\ldots \end{align} As it can be seen, there is a relationship with Pascal triangle.","The following formula has been provided: After some research the formula is found to be a Binet's Formula. It is required to convert the above recursive formula to an explicit formula using the Backtracking method. This is what i have done: As it can be seen, there is a relationship with Pascal triangle.","a(n) = a(n-1) + a(n-2)\quad \mbox{with initial states}\quad 0 , 2   \begin{align}
a(n) &= a(n-1) + a(n-2) = (a(n-2) + a(n-3)) + a(n-2)  
    \\ & = 2a(n-2) + a(n-3) = 2(a(n-3) + a(n-4)) + a(n-3)  
     \\ & = 3a(n-3) + 2a(n-4) = 3(a(n-4) + a(n-5)) + 2a(n-4)  
    \\ & = 5a(n-4) + 3a(n-5) = 5(a(n-5) + a(n-6)) + 3a(n-5)  
    \\ & = 8a(n-5) + 5a(n-6)
     \\ & \vdots
    \\ & \mbox{and so on}\ldots
\end{align}","['linear-algebra', 'discrete-mathematics', 'recurrence-relations', 'fibonacci-numbers']"
42,"Linear algebra - Prove that for any isomorphism there is an ""identity basis(?)""","Linear algebra - Prove that for any isomorphism there is an ""identity basis(?)""",,"I'd appreciate if you could help me with this question. What I thought about so far is showing that for any basis $B$ , we know that $[T]_B$ is invertible so there are $k$ elementary matrices such that $E^k*...*E^1*[T]_B=I$ . I'm not sure if it's a good direction. Here is the question: Let $V$ be a vector space over $F$ . Prove that for any isomorphism $T : V \rightarrow F^n$ , there exists a basis $B$ for $V$ such that $T=Is_B$ , meaning $T(v)=[v]_B$ Note: if you see a solution I'd be happy if you could write a hint in addition to your solution, so i can try it myself first. Also, I'm not a native english speaker so if anything is not clear please tell me and I'll fix it.","I'd appreciate if you could help me with this question. What I thought about so far is showing that for any basis , we know that is invertible so there are elementary matrices such that . I'm not sure if it's a good direction. Here is the question: Let be a vector space over . Prove that for any isomorphism , there exists a basis for such that , meaning Note: if you see a solution I'd be happy if you could write a hint in addition to your solution, so i can try it myself first. Also, I'm not a native english speaker so if anything is not clear please tell me and I'll fix it.",B [T]_B k E^k*...*E^1*[T]_B=I V F T : V \rightarrow F^n B V T=Is_B T(v)=[v]_B,"['linear-algebra', 'vector-space-isomorphism']"
43,$\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0$ for a $C^\infty$ function $f:\Bbb R^n\to \Bbb R^n$,for a  function,\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0 C^\infty f:\Bbb R^n\to \Bbb R^n,"Let $f,g:\Bbb R^n \to \Bbb R^n$ be two $C^\infty$ functions. I am trying to prove the following statements: (1) $\displaystyle\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0$ $(1\leq i\leq n)$ , where $Df$ is the derivative of $f$ (with $ij$ -entry given by $\frac{\partial f_i}{\partial x_j}$ ), and $\text{cof}(A)$ is the cofactor matrix of $A$ . (2) If $U$ is a bounded open connected subset of $\Bbb R^n$ having smooth boundary, and if $f=g$ on $\partial U$ , then $\int_U \det(Df)dx=\int_U \det (Dg)dx$ . For (1), by definition of the cofactor matrix, we have $(\text{cof}(Df))_{ij}= (-1)^{i+j} \frac{\partial f_i}{\partial x_j}\det(M_{ij})$ , where $M_{ij}$ is the $ij$ -minor of $\text{cof}(Df)$ . But I can't see how to proceed. For (2), I think I should use some kind of Stoke's theorem, but I have no idea. Any hints for these? Thanks in advance.","Let be two functions. I am trying to prove the following statements: (1) , where is the derivative of (with -entry given by ), and is the cofactor matrix of . (2) If is a bounded open connected subset of having smooth boundary, and if on , then . For (1), by definition of the cofactor matrix, we have , where is the -minor of . But I can't see how to proceed. For (2), I think I should use some kind of Stoke's theorem, but I have no idea. Any hints for these? Thanks in advance.","f,g:\Bbb R^n \to \Bbb R^n C^\infty \displaystyle\sum_{j=1}^n \frac{\partial}{\partial x_j} (\text{cof}(Df))_{ij}=0 (1\leq i\leq n) Df f ij \frac{\partial f_i}{\partial x_j} \text{cof}(A) A U \Bbb R^n f=g \partial U \int_U \det(Df)dx=\int_U \det (Dg)dx (\text{cof}(Df))_{ij}= (-1)^{i+j} \frac{\partial f_i}{\partial x_j}\det(M_{ij}) M_{ij} ij \text{cof}(Df)","['real-analysis', 'linear-algebra', 'integration', 'analysis', 'smooth-functions']"
44,Positive-definiteness of matrix with $0 \leq a_{ij} \leq \frac{1}{ij}$,Positive-definiteness of matrix with,0 \leq a_{ij} \leq \frac{1}{ij},"Let $A$ be a $N\times N$ -matrix with elements $$ a_{ii}=1 \quad\text{and}\quad a_{ij} = \frac{1}{ij}  \quad\text{for}~ i\neq j. $$ Then $A$ is positive-definite, as can be easily seen from $$ x^T A x = \sum_i x_i^2 + \sum_{i \neq j} \frac{x_i x_j}{ij}  \geq \sum_i \frac{x_i^2}{i^2} + \sum_{i \neq j} \frac{x_i x_j}{ij}  = \left(\sum_i \frac{x_i}{i}\right)^2 \geq 0. $$ Assume now that $A$ is a real symmetric $N\times N$ -matrix with elements $$ \tag{1} a_{ii}=1 \quad\text{and}\quad 0 \leq a_{ij} \leq \frac{1}{ij}  \quad\text{for}~ i\neq j. $$ Is it possible to show that $A$ is also positive-definite (or positive-semidefinite)? It is quite easy to obtain the result if $N$ is not large (e.g., $N \leq 4$ ) by estimating the corresponding leading principal minors. However, the case of arbitrary $N$ is unclear. This question is a refinement of my previous question , where the assumption $(1)$ was weakened by assuming $|a_{ij}| \leq \frac{1}{ij}$ for $i \neq j$ . Under this weaker assumption, a counterexample was presented in a comment. In another related question , the assumption $(1)$ was weakened by assuming $0 \leq |a_{ij}| \leq 1$ for $i \neq j$ , and a counterexample was also given. However, a similar construction does not seem to work in the case of the assumption $(1)$ .","Let be a -matrix with elements Then is positive-definite, as can be easily seen from Assume now that is a real symmetric -matrix with elements Is it possible to show that is also positive-definite (or positive-semidefinite)? It is quite easy to obtain the result if is not large (e.g., ) by estimating the corresponding leading principal minors. However, the case of arbitrary is unclear. This question is a refinement of my previous question , where the assumption was weakened by assuming for . Under this weaker assumption, a counterexample was presented in a comment. In another related question , the assumption was weakened by assuming for , and a counterexample was also given. However, a similar construction does not seem to work in the case of the assumption .","A N\times N 
a_{ii}=1 \quad\text{and}\quad
a_{ij} = \frac{1}{ij}  \quad\text{for}~ i\neq j.
 A 
x^T A x = \sum_i x_i^2 + \sum_{i \neq j} \frac{x_i x_j}{ij} 
\geq
\sum_i \frac{x_i^2}{i^2} + \sum_{i \neq j} \frac{x_i x_j}{ij} 
=
\left(\sum_i \frac{x_i}{i}\right)^2 \geq 0.
 A N\times N 
\tag{1}
a_{ii}=1 \quad\text{and}\quad
0 \leq a_{ij} \leq \frac{1}{ij}  \quad\text{for}~ i\neq j.
 A N N \leq 4 N (1) |a_{ij}| \leq \frac{1}{ij} i \neq j (1) 0 \leq |a_{ij}| \leq 1 i \neq j (1)","['linear-algebra', 'matrices', 'polynomials', 'positive-definite', 'positive-semidefinite']"
45,Inequality for the matrix infinity norm,Inequality for the matrix infinity norm,,"Consider the matrix $\ell_{\infty} \to \ell_{\infty}$ operator norm for some matrix $A \in \mathbb{R}^{m \times n}$ , given by $$ \| A \|_{\infty} := \sup_{x: \| x \|_{\infty} = 1} \| A x \|_{\infty} := \max_{j \in [m]} \| A_{i, :} \|_1. $$ Question : Prove (or disprove via counterexample) the following inequality: $$ \left\| V \begin{bmatrix} I_{k_1} & 0 \\ 0 & -I_{k_2} \end{bmatrix}V^\top \right\|_{\infty} \leq C \| V V^\top \|_{\infty}, $$ for $V$ satisfying $V^\top V = I$ and $V \in \mathbb{R}^{n \times k}$ , with $k_1 + k_2 = k$ . This is not a homework problem, and I have been unable to come up with a counterexample (for randomly generated $V$ , I find $C < 2$ ). I'm looking for some $C$ which is ideally in the range $o(\sqrt{k})$ .","Consider the matrix operator norm for some matrix , given by Question : Prove (or disprove via counterexample) the following inequality: for satisfying and , with . This is not a homework problem, and I have been unable to come up with a counterexample (for randomly generated , I find ). I'm looking for some which is ideally in the range .","\ell_{\infty} \to \ell_{\infty} A \in \mathbb{R}^{m \times n} 
\| A \|_{\infty} := \sup_{x: \| x \|_{\infty} = 1} \| A x \|_{\infty} :=
\max_{j \in [m]} \| A_{i, :} \|_1.
 
\left\| V \begin{bmatrix} I_{k_1} & 0 \\ 0 & -I_{k_2} \end{bmatrix}V^\top \right\|_{\infty} \leq C \| V V^\top \|_{\infty},
 V V^\top V = I V \in \mathbb{R}^{n \times k} k_1 + k_2 = k V C < 2 C o(\sqrt{k})","['linear-algebra', 'matrices', 'inequality', 'normed-spaces', 'matrix-norms']"
46,Prove that the reduced row echelon form (rref) of an $n$ by $n$ matrix either is the identity matrix ðˆ or contains at least one row of zeroes.,Prove that the reduced row echelon form (rref) of an  by  matrix either is the identity matrix ðˆ or contains at least one row of zeroes.,n n,"I'm trying to prove the following proposition Prove that the reduced row echelon form (rref) of an $n$ by $n$ matrix either is the identity matrix $\bf I$ or contains at least one row of zeroes. Firstly, I will quote the definition of the rref from the same book where the proposition was given: A matrix is in reduced row echelon form, normally abbreviated to rref,   if it satisfies all the following conditions: If there are any rows containing only zero entries then they are located in the bottom part of the matrix. If a row contains non-zero entries then the first non-zero entry is a 1. This 1 is called a leading 1. The leading 1â€™s of two consecutive non-zero rows go strictly from top left to bottom right of the matrix. The only non-zero entry in a column containing a leading 1 is the leading 1. Now, my attempt: Assume $\bf A$ is $n$ x $n$ matrix, where $\bf R$ is rref of $\bf A$ . Suppose $\bf R â‰  I$ . Then $\bf R$ must have a leading $1$ (call it $x_{i,j}$ ) which is located in ith row and jth column and $j > i$ . Since $\bf R$ is in rref, then all leading $1$ must go strictly to the bottom right of the matrix. We are left with the $n - j$ columns and $n - i$ rows. because $j > i$ , then $n - i > n - j$ and thus there must be at least $j-i$ zero rows. Now suppose $\bf R$ doesn't have row of zeros. In this case, if $x_{i,j}$ = 1, then $i = j$ , because we've shown that if $j > i$ then $\bf R$ will have row of zeros. And by definition of the identity matrix, we can conclude that $\bf R = I$ $\Box$ . Although I have a lot of doubts, but I will ask: is it correct? The proposition is kind of intuitive, however, it was a struggle for me to formalize my thoughts. If you have any remarks/suggestions about the proof above, I'd be glad to hear them!","I'm trying to prove the following proposition Prove that the reduced row echelon form (rref) of an by matrix either is the identity matrix or contains at least one row of zeroes. Firstly, I will quote the definition of the rref from the same book where the proposition was given: A matrix is in reduced row echelon form, normally abbreviated to rref,   if it satisfies all the following conditions: If there are any rows containing only zero entries then they are located in the bottom part of the matrix. If a row contains non-zero entries then the first non-zero entry is a 1. This 1 is called a leading 1. The leading 1â€™s of two consecutive non-zero rows go strictly from top left to bottom right of the matrix. The only non-zero entry in a column containing a leading 1 is the leading 1. Now, my attempt: Assume is x matrix, where is rref of . Suppose . Then must have a leading (call it ) which is located in ith row and jth column and . Since is in rref, then all leading must go strictly to the bottom right of the matrix. We are left with the columns and rows. because , then and thus there must be at least zero rows. Now suppose doesn't have row of zeros. In this case, if = 1, then , because we've shown that if then will have row of zeros. And by definition of the identity matrix, we can conclude that . Although I have a lot of doubts, but I will ask: is it correct? The proposition is kind of intuitive, however, it was a struggle for me to formalize my thoughts. If you have any remarks/suggestions about the proof above, I'd be glad to hear them!","n n \bf I \bf A n n \bf R \bf A \bf R â‰  I \bf R 1 x_{i,j} j > i \bf R 1 n - j n - i j > i n - i > n - j j-i \bf R x_{i,j} i = j j > i \bf R \bf R = I \Box","['linear-algebra', 'proof-verification', 'proof-writing']"
47,Explanation for Eigenvalues or Characteristic values of Projection Operator,Explanation for Eigenvalues or Characteristic values of Projection Operator,,Question: I need to give what are possible eigenvalues/characteristic value of projection operator with an explanation. My Attempt: Let E be any projection on vector space V . Assume R be the range of E and N be null space of E My Doubt: I have got c=1 and c=0; there is and in between by the condition of independence so how does this imply c is either 1 or 0 Please help me in understanding how the above equation gives that c has possible eigenvalue either 0 or 1 .,Question: I need to give what are possible eigenvalues/characteristic value of projection operator with an explanation. My Attempt: Let E be any projection on vector space V . Assume R be the range of E and N be null space of E My Doubt: I have got c=1 and c=0; there is and in between by the condition of independence so how does this imply c is either 1 or 0 Please help me in understanding how the above equation gives that c has possible eigenvalue either 0 or 1 .,,"['linear-algebra', 'eigenvalues-eigenvectors', 'independence', 'projection-matrices']"
48,Show The Jordan Normal Form Of $\varphi$.,Show The Jordan Normal Form Of .,\varphi,"Fix a nonnegative integer $n$ , and consider the linear space $$\mathbb{R}_n\left [x,y \right ] := \left\{  \sum_{\substack{  i_1,i_2;\\ i_1+i_2\leq n}}a_{i_1i_2}x^{i_1}y^{i_2}\quad\Big|{}_{\quad}a_{i_1i_2}\in \mathbb{R} ; \ i_1,i_2 \text{ are non-negative integers}\right \}$$ over $\mathbb{R}$ where two   operations, addition and scalar multiplication , are defined as usual. $\\$ A linear map $\varphi$ from $\mathbb{R}_n\left [x,y \right ]$ to $\mathbb{R}_n\left[x,y \right ]$ defined as following: $$\forall f(x,y)\in \mathbb{R}_n\left [x,y \right ],\quad\varphi(f):=2\frac{\partial f }{\partial x}+ \frac{\partial f }{\partial y}.\quad$$ $\\$ Show  the  jordan normal form of $\varphi$ . When $n=1,$ $$span\{2,x,-\frac{1}{2}x+y\}=\mathbb{R}_1\left [x,y \right ], $$ $$\varphi(2,x,-\frac{1}{2}x+y)=(2,x,-\frac{1}{2}x+y)\left(\begin{array}{cc|cc}  0 &  1& 0\\   0&  0& 0\\   \hline 0&  0& 0 \end{array}\right).$$ When $n=2,$ $$span\{1,x,y,xy,x^2,y^2\}=\mathbb{R}_2\left [x,y \right ], $$ it is not difficult to calculate the  jordan normal form of $\varphi$ is $$\left(\begin{array}{ccc|cc|c} 0& 1& 0& 0& 0& 0\\ 0&  0&  1&  0&  0& 0\\ 0&  0&  0&  0&  0& 0\\ \hline0&  0& 0 &  0&  1& 0\\  0&  0& 0 &  0&  0& 0\\  \hline 0&  0& 0 &  0&  0& 0\\  \end{array}\right).$$ But how to generalize it to  any  integer $n$ and  prove the generalization is  correct ï¼Ÿ","Fix a nonnegative integer , and consider the linear space over where two   operations, addition and scalar multiplication , are defined as usual. A linear map from to defined as following: Show  the  jordan normal form of . When When it is not difficult to calculate the  jordan normal form of is But how to generalize it to  any  integer and  prove the generalization is  correct ï¼Ÿ","n \mathbb{R}_n\left [x,y \right ] := \left\{ 
\sum_{\substack{
 i_1,i_2;\\
i_1+i_2\leq n}}a_{i_1i_2}x^{i_1}y^{i_2}\quad\Big|{}_{\quad}a_{i_1i_2}\in \mathbb{R} ; \ i_1,i_2 \text{ are non-negative integers}\right \} \mathbb{R} \\ \varphi \mathbb{R}_n\left [x,y \right ] \mathbb{R}_n\left[x,y \right ] \forall f(x,y)\in \mathbb{R}_n\left [x,y \right ],\quad\varphi(f):=2\frac{\partial f }{\partial x}+ \frac{\partial f }{\partial y}.\quad \\ \varphi n=1, span\{2,x,-\frac{1}{2}x+y\}=\mathbb{R}_1\left [x,y \right ],  \varphi(2,x,-\frac{1}{2}x+y)=(2,x,-\frac{1}{2}x+y)\left(\begin{array}{cc|cc} 
0 &  1& 0\\ 
 0&  0& 0\\ 
 \hline 0&  0& 0
\end{array}\right). n=2, span\{1,x,y,xy,x^2,y^2\}=\mathbb{R}_2\left [x,y \right ],  \varphi \left(\begin{array}{ccc|cc|c} 0& 1& 0& 0& 0& 0\\ 0&  0&  1&  0&  0& 0\\ 0&  0&  0&  0&  0& 0\\ \hline0&  0& 0 &  0&  1& 0\\  0&  0& 0 &  0&  0& 0\\  \hline 0&  0& 0 &  0&  0& 0\\ 
\end{array}\right). n","['linear-algebra', 'matrices', 'partial-differential-equations', 'jordan-normal-form']"
49,Obtaining positive eigenvalues of the matrix $A$?,Obtaining positive eigenvalues of the matrix ?,A,"Let us consider the matrix $A$ which has three parameters $R,C1,C3$ . This is from the Ikeda map in real form. It is defined as $$x \rightarrow R+(x \cos(\tau)-y \sin(\tau))$$ $$y \rightarrow x\sin(\tau)+y\cos(\tau)$$ The Jacobain matrix is given by: \begin{equation*} A = \begin{bmatrix}  \cos(\tau) + x \frac{\partial}{\partial x} \cos \tau - y\frac{\partial}{\partial x}\sin\tau & x\frac{\partial}{\partial y} \cos \tau - \sin \tau - y \frac{\partial}{\partial y} \sin \tau\\ \sin\tau + x \frac{\partial}{\partial x} \sin \tau + y \frac{\partial}{\partial x} \cos \tau & x \frac{\partial}{\partial y}\sin\tau + \cos \tau + y\frac{\partial}{\partial y}\cos \tau \end{bmatrix} \end{equation*} where $$\tau = C_{1} - \frac{C_{3}} {1+x^2+y^2}$$ $x,y$ are solutions of the non-linear equation \begin{equation} R+x \cos \tau - y \sin \tau = x\\ x\sin \tau + y \cos \tau = y \end{equation} After calculating the determinant of the matrix $A$ , we get $det(A)=1$ (so product of eigenvalues is 1) using $$\frac{\partial \tau}{\partial x} = \frac{2C_{3}x}{(1+x^2+y^2)^2}$$ $$\frac{\partial \tau}{\partial y} = \frac{2C_{3}y}{(1+x^2 + y^2)^2}$$ I am wondering for which values of $R,C_{1},C_{3}$ I can obtain positive eigenvalues? as I see after trying many values I get complex eigenvalues or negative eigenvalues. I am thinking whether the above matrix can have any positive eigenvalues at all? Any sharp hawk eye observations to this? EDIT - If suppose $R=0$ , then we see that $x=0,y=0$ satisfies the non linear equation and if we obtain the trace of the matrix at $(0,0)$ we get the trace as $2\cos \tau$ and now for the eigen values to be real and positive we need $\cos \tau > 1$ which is not possible so we can eliminate the $R=0$ case. Now I am thinking whether if for $R \neq 0$ , can we have positive eigenvalues for the Jacobian matrix?","Let us consider the matrix which has three parameters . This is from the Ikeda map in real form. It is defined as The Jacobain matrix is given by: where are solutions of the non-linear equation After calculating the determinant of the matrix , we get (so product of eigenvalues is 1) using I am wondering for which values of I can obtain positive eigenvalues? as I see after trying many values I get complex eigenvalues or negative eigenvalues. I am thinking whether the above matrix can have any positive eigenvalues at all? Any sharp hawk eye observations to this? EDIT - If suppose , then we see that satisfies the non linear equation and if we obtain the trace of the matrix at we get the trace as and now for the eigen values to be real and positive we need which is not possible so we can eliminate the case. Now I am thinking whether if for , can we have positive eigenvalues for the Jacobian matrix?","A R,C1,C3 x \rightarrow R+(x \cos(\tau)-y \sin(\tau)) y \rightarrow x\sin(\tau)+y\cos(\tau) \begin{equation*}
A =
\begin{bmatrix}
 \cos(\tau) + x \frac{\partial}{\partial x} \cos \tau - y\frac{\partial}{\partial x}\sin\tau & x\frac{\partial}{\partial y} \cos \tau - \sin \tau - y \frac{\partial}{\partial y} \sin \tau\\
\sin\tau + x \frac{\partial}{\partial x} \sin \tau + y \frac{\partial}{\partial x} \cos \tau & x \frac{\partial}{\partial y}\sin\tau + \cos \tau + y\frac{\partial}{\partial y}\cos \tau
\end{bmatrix}
\end{equation*} \tau = C_{1} - \frac{C_{3}} {1+x^2+y^2} x,y \begin{equation}
R+x \cos \tau - y \sin \tau = x\\
x\sin \tau + y \cos \tau = y
\end{equation} A det(A)=1 \frac{\partial \tau}{\partial x} = \frac{2C_{3}x}{(1+x^2+y^2)^2} \frac{\partial \tau}{\partial y} = \frac{2C_{3}y}{(1+x^2 + y^2)^2} R,C_{1},C_{3} R=0 x=0,y=0 (0,0) 2\cos \tau \cos \tau > 1 R=0 R \neq 0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'dynamical-systems', 'computational-mathematics']"
50,"Finding $ \max \mathrm{rk} \, A'(0)$ among all $A\in C^1(\mathbb{R}, M_n)$ with $ \mathrm{rk} A(t) \leq r$",Finding  among all  with," \max \mathrm{rk} \, A'(0) A\in C^1(\mathbb{R}, M_n)  \mathrm{rk} A(t) \leq r","From the admission test for Normale di Pisa: Let $0<r<n$ be integers and let $M_n$ be the space of real valued matrices. The problem is to find: $$ \max \mathrm{rk} \, A'(0) $$ among all the $A \in C^1(\mathbb{R}, M_n)$ meeting the condition $ \mathrm{rk} A(t) \leq r$ I have some ideas. For example, if the matrix $A(t)$ is diagonal for each $t$ , than the above maximum should be $r$ , thanks to some easy continuity arguments. The same arguments work for the triangular case. I don't know how to generalize this in the general case: does observing that any matrix can be put in triangular form help? Thanks in advance for any answers!","From the admission test for Normale di Pisa: Let be integers and let be the space of real valued matrices. The problem is to find: among all the meeting the condition I have some ideas. For example, if the matrix is diagonal for each , than the above maximum should be , thanks to some easy continuity arguments. The same arguments work for the triangular case. I don't know how to generalize this in the general case: does observing that any matrix can be put in triangular form help? Thanks in advance for any answers!","0<r<n M_n 
\max \mathrm{rk} \, A'(0)
 A \in C^1(\mathbb{R}, M_n)  \mathrm{rk} A(t) \leq r A(t) t r","['linear-algebra', 'geometry', 'analysis']"
51,Induction step of proof of 'Every element of $O(n)$ is product of hyperplane reflections',Induction step of proof of 'Every element of  is product of hyperplane reflections',O(n),"The following came up in induction step of proof of 'Every element of $O(n)$ is product of hyperplane reflections'. An element $A$ in $O(n)$ is called hyperplane reflection if $$A=Pdiag(1,\cdots , 1,-1)P^T$$ where $P\in O(n)$ . If $$A'=P \begin{bmatrix} A_{n-1} & \\  & \pm 1  \end{bmatrix} P^T $$ and $A_{n-1}$ is in $O(n-1)$ then $A'$ is product of hyperplane reflections in $O(n)$ . Why is this true? All we know is $A_{n-1}$ is product of hyperplane reflections. How does this give result?",The following came up in induction step of proof of 'Every element of is product of hyperplane reflections'. An element in is called hyperplane reflection if where . If and is in then is product of hyperplane reflections in . Why is this true? All we know is is product of hyperplane reflections. How does this give result?,"O(n) A O(n) A=Pdiag(1,\cdots , 1,-1)P^T P\in O(n) A'=P \begin{bmatrix}
A_{n-1} & \\
 & \pm 1 
\end{bmatrix} P^T
 A_{n-1} O(n-1) A' O(n) A_{n-1}","['linear-algebra', 'matrices', 'group-theory', 'lie-groups', 'orthogonal-matrices']"
52,"Rewrite $(\det A)^{1/n}=\min\left\{\frac{\mathrm{tr}(AC)}{n}:C \in \Bbb{C}^{nÃ—n},C>0,\det C=1\right\}$ in terms of $\frac{\rm{tr}(CAC)}{n}$",Rewrite  in terms of,"(\det A)^{1/n}=\min\left\{\frac{\mathrm{tr}(AC)}{n}:C \in \Bbb{C}^{nÃ—n},C>0,\det C=1\right\} \frac{\rm{tr}(CAC)}{n}","Given $$ (\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(AC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label1\tag1 $$ Question Show that the formula can be rewritten as $$ (\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(CAC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label{2}\tag{2} $$ Then, show by an example that the formula \eqref{1} is false if $A$ is singular. My approach , would love to get your opinions: The determinant and the trace are two quite different beasts, little relation can be found among them. If the matrix is not only symmetric (hermitic) but also positive semi-definite , then its eigenvalues are real and non-negative.  Hence, given the properties ${\rm tr}(AC)=\sum \lambda_c$ and ${\rm det}(A)=\prod \lambda_C$ , and recalling the AM GM inequality, we get the following (probably not very useful) inequality: $$\frac{\operatorname{tr}(AC)}{n} \ge {\det}(A)^{1/n}.$$ (equality holds iff $M = \lambda I$ for some $\lambda \ge  0$ ) My Solution: If $\det C = 1$ , could I say that $C$ is either an unity matrix or identity matrix, and hence it won't have any other max. or min. occurrences, thus the formula can be re-expressed as stated above? Could I say that $AC = A = $ matrix with all zeros and just eigenvalues on the diagonal $\lambda_1 ,..., \lambda_i,..., \lambda_n$ , then $$ \frac{\lambda_1 +...+\lambda_n}{n} \ge (\lambda_1 \cdot ... \cdot \lambda_n)^{1/n}$$ and $$ \frac{\lambda_1 +...+\lambda_n}{(\lambda_1 \cdot ... \cdot \lambda_n)^{1/n}} \ge n\:? $$ Then, I say that if $A$ is singular then $\det A = 0$ thus one of the $\lambda_i = 0$ , therefore the product of all $\lambda_i = 0$ , thus, could I say that $\frac{0}{0} \ge n$ (although it's undefined), therefore, it contradicts the assumption and the formula doesn't hold for a singular matrix? What I'm struggling with: where I bolded ""could I say that"" - I'm not sure that it's correct and would love to know your opinion.","Given Question Show that the formula can be rewritten as Then, show by an example that the formula \eqref{1} is false if is singular. My approach , would love to get your opinions: The determinant and the trace are two quite different beasts, little relation can be found among them. If the matrix is not only symmetric (hermitic) but also positive semi-definite , then its eigenvalues are real and non-negative.  Hence, given the properties and , and recalling the AM GM inequality, we get the following (probably not very useful) inequality: (equality holds iff for some ) My Solution: If , could I say that is either an unity matrix or identity matrix, and hence it won't have any other max. or min. occurrences, thus the formula can be re-expressed as stated above? Could I say that matrix with all zeros and just eigenvalues on the diagonal , then and Then, I say that if is singular then thus one of the , therefore the product of all , thus, could I say that (although it's undefined), therefore, it contradicts the assumption and the formula doesn't hold for a singular matrix? What I'm struggling with: where I bolded ""could I say that"" - I'm not sure that it's correct and would love to know your opinion.","
(\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(AC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label1\tag1
 
(\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(CAC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label{2}\tag{2}
 A {\rm tr}(AC)=\sum \lambda_c {\rm det}(A)=\prod \lambda_C \frac{\operatorname{tr}(AC)}{n} \ge {\det}(A)^{1/n}. M = \lambda I \lambda \ge  0 \det C = 1 C AC = A =  \lambda_1 ,..., \lambda_i,..., \lambda_n 
\frac{\lambda_1 +...+\lambda_n}{n} \ge (\lambda_1 \cdot ... \cdot \lambda_n)^{1/n} 
\frac{\lambda_1 +...+\lambda_n}{(\lambda_1 \cdot ... \cdot \lambda_n)^{1/n}} \ge n\:?
 A \det A = 0 \lambda_i = 0 \lambda_i = 0 \frac{0}{0} \ge n","['linear-algebra', 'determinant', 'trace']"
53,"Why does this trick to derive the formula for $[A^n,B]$ in terms of repeated commutators work so well?",Why does this trick to derive the formula for  in terms of repeated commutators work so well?,"[A^n,B]","It is a known result that, given generically noncommuting operators $A,B$ , we have $$ A^n B=\sum_{k=0}^n \binom{n}{k} \operatorname{ad}^k(A)(B) A^{n-k},\tag A $$ where $\operatorname{ad}^k(A)(B)\equiv[\underbrace{A,[A,[\dots,[A}_k,B]\dots]] $ . This can be proved for example via induction with not too much work. However, while trying to get a better understanding of this formula, I realised that there is a much easier way to derive it, at least on a formal, intuitive level. The trick Let $\hat{\mathcal S}$ and $\hat{\mathcal C}$ (standing for ""shift"" and ""commute"", respectively) denote operators that act on expressions of the form $A^k D^j A^\ell$ (denoting for simplicity $D^j\equiv\operatorname{ad}^j(A)(B)$ ) as follows: \begin{align} \hat{\mathcal S} (A^k D^j A^\ell)  &= A^{k-1} D^j A^{\ell+1}, \\ \hat{\mathcal C} (A^{k} D^{j} A^\ell) &= A^{k-1} D^{j+1} A^{\ell}. \end{align} In other words, $\hat{\mathcal S}$ ""moves"" the central $D$ block on the left, while $\hat{\mathcal C}$ makes it ""eat"" the neighboring $A$ factor. It is not hard to see that $\hat{\mathcal S}+\hat{\mathcal C}=\mathbb 1$ , which is but another way to state the identity $$A[A,B]=[A,B]A+[A,[A,B]].$$ Moreover, crucially, $\hat{\mathcal S}$ and $\hat{\mathcal C}$ commute. Because of this, I can write $$A^n B=(\hat{\mathcal S}+\hat{\mathcal C})^n (A^n B)=\sum_{k=0}^n\binom{n}{k} \hat{\mathcal S}^{n-k} \hat{\mathcal C}^{k}(A^n B),$$ which immediately gives me (A) without any need for recursion or other tricks. The question Now, this is all fine and dandy, but it leaves me wondering as to why does this kind of thing work ? It looks like I am somehow bypassing the nuisance of having to deal with non-commuting operations by switching to a space of ""superoperators"", in which the same operation can be expressed in terms of commuting ""superoperators"". I am not even sure how one could go in formalising this ""superoperators"" $\hat{\mathcal S},\hat{\mathcal C}$ , as they seem to be objects acting on ""strings of operators"" more than on the elements of the operator algebra themselves. Is there a way to formalise this way of handling the expressions? Is this a well-known method in this context (I had never seen it but I am not well-versed in this kinds of manipulations)?","It is a known result that, given generically noncommuting operators , we have where . This can be proved for example via induction with not too much work. However, while trying to get a better understanding of this formula, I realised that there is a much easier way to derive it, at least on a formal, intuitive level. The trick Let and (standing for ""shift"" and ""commute"", respectively) denote operators that act on expressions of the form (denoting for simplicity ) as follows: In other words, ""moves"" the central block on the left, while makes it ""eat"" the neighboring factor. It is not hard to see that , which is but another way to state the identity Moreover, crucially, and commute. Because of this, I can write which immediately gives me (A) without any need for recursion or other tricks. The question Now, this is all fine and dandy, but it leaves me wondering as to why does this kind of thing work ? It looks like I am somehow bypassing the nuisance of having to deal with non-commuting operations by switching to a space of ""superoperators"", in which the same operation can be expressed in terms of commuting ""superoperators"". I am not even sure how one could go in formalising this ""superoperators"" , as they seem to be objects acting on ""strings of operators"" more than on the elements of the operator algebra themselves. Is there a way to formalise this way of handling the expressions? Is this a well-known method in this context (I had never seen it but I am not well-versed in this kinds of manipulations)?","A,B  A^n B=\sum_{k=0}^n \binom{n}{k} \operatorname{ad}^k(A)(B) A^{n-k},\tag A  \operatorname{ad}^k(A)(B)\equiv[\underbrace{A,[A,[\dots,[A}_k,B]\dots]]  \hat{\mathcal S} \hat{\mathcal C} A^k D^j A^\ell D^j\equiv\operatorname{ad}^j(A)(B) \begin{align}
\hat{\mathcal S} (A^k D^j A^\ell) 
&= A^{k-1} D^j A^{\ell+1}, \\
\hat{\mathcal C} (A^{k} D^{j} A^\ell)
&= A^{k-1} D^{j+1} A^{\ell}.
\end{align} \hat{\mathcal S} D \hat{\mathcal C} A \hat{\mathcal S}+\hat{\mathcal C}=\mathbb 1 A[A,B]=[A,B]A+[A,[A,B]]. \hat{\mathcal S} \hat{\mathcal C} A^n B=(\hat{\mathcal S}+\hat{\mathcal C})^n (A^n B)=\sum_{k=0}^n\binom{n}{k} \hat{\mathcal S}^{n-k} \hat{\mathcal C}^{k}(A^n B), \hat{\mathcal S},\hat{\mathcal C}","['linear-algebra', 'matrices', 'operator-algebras', 'noncommutative-algebra']"
54,Can we obtain an explicit and efficient analytic interpolation of tetration by this method?,Can we obtain an explicit and efficient analytic interpolation of tetration by this method?,,"I am curious about this. It has been a very long time since I have ever toyed with this topic but it was an old interest of mine quite some time ago - maybe 8 years (250 megaseconds) ago at least, and I never really got to a conclusion, nor do I think anyone did entirely satisfactorily, and some comments on a Youtube video I was watching inspired me to dust it off and try once more to give it another thwack. And the question is, basically, how can one construct a ""reasonable"" interpolation of the ""tetration"" operation, also called a ""power tower"", which for those who have not heard of it is defined for natural $n > 1$ and real $a > 1$ by $$^{n} a = a \uparrow \uparrow n := \underbrace{a^{a^{a^{...^a}}}}_{\mbox{$n$ copies of $a$}}$$ where the nested exponentiations on the left are evaluated in rightassociative fashion, so the deepest (""highest"") layer is done first, e.g. $$^{3} 3 = 3^{3^3} = 3^{27} = 7\ 625\ 597\ 484\ 987$$ and not left-associative, i.e. $$^{3} 3 \ne (3^3)^3 = 3^{3 \cdot 3} = 3^9 = 19\ 683$$ What the ""interpolation"" part means is basically, given that this definition clearly only works for values of the second argument, $n$ (called as such by analogy with exponentiation even though it is written first in the ""left-superscript"" notation just introduced), usually called the ""height"" of the tetration or power tower for obvious reasons, that are natural numbers at least 1, since we have to have a whole number of ""copies of $a$ "" - half a copy, say, wouldn't make much sense, as while you can literally write a half-written "" $a$ "", that is formally nonsense and has no mathematical meaning, although it may have other forms of meaning from other angles of human understanding and analysis, e.g. perhaps as a form of artsey commentary. Mathematical meaning is, though, of course, what we're interested in. We can, of course, naturally extend this in a way similar to extending exponentiation to the integers by noting that $$^{n+1} a = a^{^n a}$$ and thus $$^{n-1} a = \log_a(^n a)$$ and if we do this we can at least extend that $^0 a = 1$ , similar to exponentiation, and $^{(-1)} a = 0$ , a rather interesting result when viewed in contrast to exponentiation given that the first negative exponentiation of a number is not a constant but instead its reciprocal. Of course, we cannot extend now to $^{(-2)} a$ , as then we get $\log_a(0)$ which is undefined (though of course if you want to stretch the maths a bit and expand the codomain to the extended reals, you can say $^{(-2)}a = -\infty$ . In any case though, $^{(-3)} a$ and further are definitely, really undefined, since no real exponential can be negative, much less negative infinity!). So this peters out. Of course, the most interesting bit - as hinted at with the ""half a copy"" business above - is trying to extend the height $n$ to real values, presumably in $(-2, \infty)$ at least. And there have been a number of methods fielded in those past epochs which attempt to do this as well as some interesting ones regarding the conditions which are required to produce a suitably ""natural"" extension, given that it is trivially obvious that one can, of course, ""interpolate"" a given sparse sequence of points in any way that one desires and, moreover, even with the identities $^{n+1} a = a^{^n a}$ , they only suffice to make it unique insofar as whole-number increments of the tower are concerned - fill any unit interval with anything you like, and the identity will extend to provide an interpolant that will satisfy it. For exponentiation, this non-uniqueness is much less of a problem because we also have the additional identity $a^{n+m} = a^n a^m$ , which lets us extend to rational values, however no such identity exists for tetration. In this regard, extension of tetration is similar to the question of extension of the factorial, which is similarly impoverished of identities, with new ones interestingly only coming about after the extension was done by Euler in the form of the gamma function, to meet a challenge originally proposed by Bernoulli to do exactly this. The gamma function, however, is still ostensibly more ""natural"" simply because a) it often crops up and b) it has some very cute integral representations, esp. the darlin' $$\Gamma(x) = \int_{0}^{1} [-\log(u)]^{n-1}\ du$$ (Though with regard to objection a), one could say this may be simply because we have not yet found such an expression, and thus places where it might be useful, could be currently written off as ""unsolvable"".) Yet clearly, that doesn't seem to have been the case for tetration, either. Moreover, in all these past discussions, many of the extension methods proposed are in fact extremely cumbersome and computationally intensive to approximate, involving elaborate constructs like Riemann mappings, infinite limits of integral equations, and so forth - all things that are, while mathematically valid, both inelegant and also not something you're going be able to program into a software pack like Mathematica and have it spit out 2500 digits of $^{1/2} 2$ in the blink of an eye. But nonetheless, one particular method out of these proposed methods seems be both fairly simple and like that it might possible be amenable to more detailed analysis, and that is the ""Carleman matrix"" operator method. This method is most succinctly expressed for the specific case $a = e$ , to construct the ""natural tetrational"" $\mathrm{tet}(x) :=\ ^x e$ with real height $x$ , so we'll just focus on that for now. But basically it is based on the following two observations. The first is that one can consider the result of the height- $n$ power tower of $e$ as the iterated exponential evaluated at $1$ , namely $$^n e = \exp^n(1)$$ or perhaps more nicely for what we're about to do, $$^{n-1} e = \exp^n(0)$$ which has some interesting gamma-function like quality about it with the offset. And the second one is the following. If we let $\exp$ be given by its power series, $$\exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$ then we can actually represent such iterated exponentials using what is called its Carleman matrix , basically the infinite-order ""matrix"" with entries $$C[\exp]_{ij} = \frac{i^j}{j!}$$ such that if we have the infinite vector of exponential coefficients $\mathbf{a} = \left[ \frac{1}{1!}\ \frac{1}{2!}\ \frac{1}{3!}\ \cdots \right]^T$ then the vector $\mathbf{b}_n = (\mathbf{C}[\exp])^n \mathbf{a}$ is the coefficients of $\exp^n$ . In particular, if we sum the top row, we get exactly what we want: $^{n-1} e$ . Now the question is, though, how can we compute this matrix power for fractional $n$ in some explicit form? It seems one possible way to do this, and the way that I saw when this method was suggested (by Gottfried Helms, who was here a long time ago, not sure if they're still so) was to try to diagonalize the matrix, so that you can use the fact that if a matrix $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}$ for some diagonal matrix $\mathbf{D}$ (which happens to be the matrix of eigenvalues) then $\mathbf{A}^t = \mathbf{P}\mathbf{D}^t\mathbf{P}^{-1}$ where that the inner power is easy to compute as you just take exponents of the diagonal terms. And numerically this seems to work for at least finite truncations of the matrix $\mathbf{C}[\exp]$ , but analytically it may be on shaky ground, as I see with this thread here that I just saw when trying to dig into this once more: Diagonalization of an infinite matrix and moreover it's still not super efficient as we'd like - we're not computing 2500 digits of $^{1/2} e$ and I want them computed, dammit! However, it seems that in this case some of the objections raised in that thread do not perhaps apply here. In particular, it is mentioned how that an infinite matrix diagonalization is ambiguous (seems to mirror the situation with the tetration interpolation generally where there is great freedom) due to choice of suitable normed vector space over which to make sense of it, and moreover in that question it was pointed that the usual most ""natural"" space, $\ell^2$ , did not work for that questioner's particular matrix, because in particular it would ""map"" most ""vectors"" of $\ell^2$ to effectively outside of the space. However , this Carleman matrix seems better behaved - in particular, due to the fact that any vector $[\ a_0\ a_1\ a_2\ \cdots\ ]^T \in \ell^2$ by definition must have terms that converge absolutely as an infinite sum, then that owing to the factorials in the matrix when we multiply by it we should also get an (even more) convergent series, as is illustrated by considering the bounding ""vector"" $[\ 1\ 1\ 1\ \cdots\ ]^T$ as ""formally"" acted upon by $\mathbf{C}[\exp]$ . So it seems that in that regard, we are in better shape against at least the objections raised in that thread in this case than for that poster's scenario. Thus the question I have is, if we take the relevant target space as $\ell^2$ , can we find a ""natural"" infinite diagonalization and thus matrix-power for this case, and moreover express it somehow in terms of at least some sort of infinite combinatorial sums or otherwise easily-manipulated expressions for its coefficients? Moreover, another interesting and seemingly natural question provoked by this is, exactly how sensitive is the method to the choice of at least norm used to interpret the matrix power, i.e. can we have absolute freedom to interpolate $^n e$ in any way we please, and if not, then just how much do we get? I suspect ""a lot"", but there are some ""lots"" that are more than others in mathematics, even when infinities are concerned, thanks to Cantor. And can we do the inverse - i.e. if I fill in $^{n} e$ with some freely-chosen interpolant in the interval $[0, 1]$ (for the purpose of making things easy I'll assume it's continuous and moreover equals $1$ at $x = 0$ and $e$ at $x = 1$ ) - can we find a norm such that the associated Carleman matrix power will produce that interpolant? Does it have to be analytic (seems right, but keep in mind that we are summing the top row , not necessarily creating a power series valid for all or even any inputs, though again it also ""seems"" right that if not analytic, it'll diverge)? If so, what's the proof? ADD (epoch time 1545.46 Ms): In the quest for an at least summatory-formula shot at the diagonalization, I note this matrix has the interesting relations among rows and columns given by $$C[\exp]_{(i+1)j} = \sum_{k=0}^{j} \frac{1}{(j - k)!} C_{ik}$$ and $$C[\exp]_{i(j+1)} = \frac{i}{j+1} C_{ij}$$ Not sure if this helps anything, though. But at least it shows there is structure and thus we're not just dealing with effectively purely random matrices and thus in theory it might somehow be exploited in some fashion to simplify things. ADD 2 (same time): You should actually sum the second row of the matrix power to get the tetration $^n e$ , not the first to get $^{n-1} e$ . The first row always sums to 1. ADD 3 (ue+1545.47 Ms): The first row formula above allows us to derive the interesting property that if $\mathbf{C}[\exp]$ is right-multiplied by the factorial matrix $$\mathbf{D} := \begin{bmatrix} 1 & \frac{1}{1!} & \frac{1}{2!} & \frac{1}{3!} & \cdots \\ 0 & 1 & \frac{1}{1!} & \frac{1}{2!} & \cdots \\ 0 & 0 & 1 & \frac{1}{1!} & \cdots \\ 0 & 0 & 0 & 1 & \cdots \\ & & \cdots & & \end{bmatrix}$$ where $D_{kj} = \frac{1}{(j-k)!}$ (with negative-argument factorials ""naturally"" extended as $\infty$ so these $D$ terms are $0$ ), it shifts up by one row. ADD 4 (sim. time): It looks like we can move both up and down, in particular the matrix $\mathbf{D}$ with entries $D_{kj} = (-1)^{k-j} \frac{1}{(k-j)!}$ will shift down, while the other matrix above, perhaps better called $\mathbf{U}$ instead will shift up. Shifting left and right is possible as well but via the Hadamard product and not the ordinary product, as the second set of relations between columns indicates. In particular, the Hadamard product with the matrix $\mathbf{L}$ with entries $L_{ij} = \frac{i}{j+1}$ will shift left, and the matrix $\mathbf{R}$ with entries $R_{ij} = \frac{j}{i}$ will shift right. Thus we have some interesting system for ""moving"" the matrix about like some kind of tableau or grid - not sure what these symmetry properties do though for easing the analytic solution/explicit formula of the matrixpower as a summation.","I am curious about this. It has been a very long time since I have ever toyed with this topic but it was an old interest of mine quite some time ago - maybe 8 years (250 megaseconds) ago at least, and I never really got to a conclusion, nor do I think anyone did entirely satisfactorily, and some comments on a Youtube video I was watching inspired me to dust it off and try once more to give it another thwack. And the question is, basically, how can one construct a ""reasonable"" interpolation of the ""tetration"" operation, also called a ""power tower"", which for those who have not heard of it is defined for natural and real by where the nested exponentiations on the left are evaluated in rightassociative fashion, so the deepest (""highest"") layer is done first, e.g. and not left-associative, i.e. What the ""interpolation"" part means is basically, given that this definition clearly only works for values of the second argument, (called as such by analogy with exponentiation even though it is written first in the ""left-superscript"" notation just introduced), usually called the ""height"" of the tetration or power tower for obvious reasons, that are natural numbers at least 1, since we have to have a whole number of ""copies of "" - half a copy, say, wouldn't make much sense, as while you can literally write a half-written "" "", that is formally nonsense and has no mathematical meaning, although it may have other forms of meaning from other angles of human understanding and analysis, e.g. perhaps as a form of artsey commentary. Mathematical meaning is, though, of course, what we're interested in. We can, of course, naturally extend this in a way similar to extending exponentiation to the integers by noting that and thus and if we do this we can at least extend that , similar to exponentiation, and , a rather interesting result when viewed in contrast to exponentiation given that the first negative exponentiation of a number is not a constant but instead its reciprocal. Of course, we cannot extend now to , as then we get which is undefined (though of course if you want to stretch the maths a bit and expand the codomain to the extended reals, you can say . In any case though, and further are definitely, really undefined, since no real exponential can be negative, much less negative infinity!). So this peters out. Of course, the most interesting bit - as hinted at with the ""half a copy"" business above - is trying to extend the height to real values, presumably in at least. And there have been a number of methods fielded in those past epochs which attempt to do this as well as some interesting ones regarding the conditions which are required to produce a suitably ""natural"" extension, given that it is trivially obvious that one can, of course, ""interpolate"" a given sparse sequence of points in any way that one desires and, moreover, even with the identities , they only suffice to make it unique insofar as whole-number increments of the tower are concerned - fill any unit interval with anything you like, and the identity will extend to provide an interpolant that will satisfy it. For exponentiation, this non-uniqueness is much less of a problem because we also have the additional identity , which lets us extend to rational values, however no such identity exists for tetration. In this regard, extension of tetration is similar to the question of extension of the factorial, which is similarly impoverished of identities, with new ones interestingly only coming about after the extension was done by Euler in the form of the gamma function, to meet a challenge originally proposed by Bernoulli to do exactly this. The gamma function, however, is still ostensibly more ""natural"" simply because a) it often crops up and b) it has some very cute integral representations, esp. the darlin' (Though with regard to objection a), one could say this may be simply because we have not yet found such an expression, and thus places where it might be useful, could be currently written off as ""unsolvable"".) Yet clearly, that doesn't seem to have been the case for tetration, either. Moreover, in all these past discussions, many of the extension methods proposed are in fact extremely cumbersome and computationally intensive to approximate, involving elaborate constructs like Riemann mappings, infinite limits of integral equations, and so forth - all things that are, while mathematically valid, both inelegant and also not something you're going be able to program into a software pack like Mathematica and have it spit out 2500 digits of in the blink of an eye. But nonetheless, one particular method out of these proposed methods seems be both fairly simple and like that it might possible be amenable to more detailed analysis, and that is the ""Carleman matrix"" operator method. This method is most succinctly expressed for the specific case , to construct the ""natural tetrational"" with real height , so we'll just focus on that for now. But basically it is based on the following two observations. The first is that one can consider the result of the height- power tower of as the iterated exponential evaluated at , namely or perhaps more nicely for what we're about to do, which has some interesting gamma-function like quality about it with the offset. And the second one is the following. If we let be given by its power series, then we can actually represent such iterated exponentials using what is called its Carleman matrix , basically the infinite-order ""matrix"" with entries such that if we have the infinite vector of exponential coefficients then the vector is the coefficients of . In particular, if we sum the top row, we get exactly what we want: . Now the question is, though, how can we compute this matrix power for fractional in some explicit form? It seems one possible way to do this, and the way that I saw when this method was suggested (by Gottfried Helms, who was here a long time ago, not sure if they're still so) was to try to diagonalize the matrix, so that you can use the fact that if a matrix for some diagonal matrix (which happens to be the matrix of eigenvalues) then where that the inner power is easy to compute as you just take exponents of the diagonal terms. And numerically this seems to work for at least finite truncations of the matrix , but analytically it may be on shaky ground, as I see with this thread here that I just saw when trying to dig into this once more: Diagonalization of an infinite matrix and moreover it's still not super efficient as we'd like - we're not computing 2500 digits of and I want them computed, dammit! However, it seems that in this case some of the objections raised in that thread do not perhaps apply here. In particular, it is mentioned how that an infinite matrix diagonalization is ambiguous (seems to mirror the situation with the tetration interpolation generally where there is great freedom) due to choice of suitable normed vector space over which to make sense of it, and moreover in that question it was pointed that the usual most ""natural"" space, , did not work for that questioner's particular matrix, because in particular it would ""map"" most ""vectors"" of to effectively outside of the space. However , this Carleman matrix seems better behaved - in particular, due to the fact that any vector by definition must have terms that converge absolutely as an infinite sum, then that owing to the factorials in the matrix when we multiply by it we should also get an (even more) convergent series, as is illustrated by considering the bounding ""vector"" as ""formally"" acted upon by . So it seems that in that regard, we are in better shape against at least the objections raised in that thread in this case than for that poster's scenario. Thus the question I have is, if we take the relevant target space as , can we find a ""natural"" infinite diagonalization and thus matrix-power for this case, and moreover express it somehow in terms of at least some sort of infinite combinatorial sums or otherwise easily-manipulated expressions for its coefficients? Moreover, another interesting and seemingly natural question provoked by this is, exactly how sensitive is the method to the choice of at least norm used to interpret the matrix power, i.e. can we have absolute freedom to interpolate in any way we please, and if not, then just how much do we get? I suspect ""a lot"", but there are some ""lots"" that are more than others in mathematics, even when infinities are concerned, thanks to Cantor. And can we do the inverse - i.e. if I fill in with some freely-chosen interpolant in the interval (for the purpose of making things easy I'll assume it's continuous and moreover equals at and at ) - can we find a norm such that the associated Carleman matrix power will produce that interpolant? Does it have to be analytic (seems right, but keep in mind that we are summing the top row , not necessarily creating a power series valid for all or even any inputs, though again it also ""seems"" right that if not analytic, it'll diverge)? If so, what's the proof? ADD (epoch time 1545.46 Ms): In the quest for an at least summatory-formula shot at the diagonalization, I note this matrix has the interesting relations among rows and columns given by and Not sure if this helps anything, though. But at least it shows there is structure and thus we're not just dealing with effectively purely random matrices and thus in theory it might somehow be exploited in some fashion to simplify things. ADD 2 (same time): You should actually sum the second row of the matrix power to get the tetration , not the first to get . The first row always sums to 1. ADD 3 (ue+1545.47 Ms): The first row formula above allows us to derive the interesting property that if is right-multiplied by the factorial matrix where (with negative-argument factorials ""naturally"" extended as so these terms are ), it shifts up by one row. ADD 4 (sim. time): It looks like we can move both up and down, in particular the matrix with entries will shift down, while the other matrix above, perhaps better called instead will shift up. Shifting left and right is possible as well but via the Hadamard product and not the ordinary product, as the second set of relations between columns indicates. In particular, the Hadamard product with the matrix with entries will shift left, and the matrix with entries will shift right. Thus we have some interesting system for ""moving"" the matrix about like some kind of tableau or grid - not sure what these symmetry properties do though for easing the analytic solution/explicit formula of the matrixpower as a summation.","n > 1 a > 1 ^{n} a = a \uparrow \uparrow n := \underbrace{a^{a^{a^{...^a}}}}_{\mbox{n copies of a}} ^{3} 3 = 3^{3^3} = 3^{27} = 7\ 625\ 597\ 484\ 987 ^{3} 3 \ne (3^3)^3 = 3^{3 \cdot 3} = 3^9 = 19\ 683 n a a ^{n+1} a = a^{^n a} ^{n-1} a = \log_a(^n a) ^0 a = 1 ^{(-1)} a = 0 ^{(-2)} a \log_a(0) ^{(-2)}a = -\infty ^{(-3)} a n (-2, \infty) ^{n+1} a = a^{^n a} a^{n+m} = a^n a^m \Gamma(x) = \int_{0}^{1} [-\log(u)]^{n-1}\ du ^{1/2} 2 a = e \mathrm{tet}(x) :=\ ^x e x n e 1 ^n e = \exp^n(1) ^{n-1} e = \exp^n(0) \exp \exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!} C[\exp]_{ij} = \frac{i^j}{j!} \mathbf{a} = \left[ \frac{1}{1!}\ \frac{1}{2!}\ \frac{1}{3!}\ \cdots \right]^T \mathbf{b}_n = (\mathbf{C}[\exp])^n \mathbf{a} \exp^n ^{n-1} e n \mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1} \mathbf{D} \mathbf{A}^t = \mathbf{P}\mathbf{D}^t\mathbf{P}^{-1} \mathbf{C}[\exp] ^{1/2} e \ell^2 \ell^2 [\ a_0\ a_1\ a_2\ \cdots\ ]^T \in \ell^2 [\ 1\ 1\ 1\ \cdots\ ]^T \mathbf{C}[\exp] \ell^2 ^n e ^{n} e [0, 1] 1 x = 0 e x = 1 C[\exp]_{(i+1)j} = \sum_{k=0}^{j} \frac{1}{(j - k)!} C_{ik} C[\exp]_{i(j+1)} = \frac{i}{j+1} C_{ij} ^n e ^{n-1} e \mathbf{C}[\exp] \mathbf{D} := \begin{bmatrix} 1 & \frac{1}{1!} & \frac{1}{2!} & \frac{1}{3!} & \cdots \\
0 & 1 & \frac{1}{1!} & \frac{1}{2!} & \cdots \\
0 & 0 & 1 & \frac{1}{1!} & \cdots \\
0 & 0 & 0 & 1 & \cdots \\
& & \cdots & & \end{bmatrix} D_{kj} = \frac{1}{(j-k)!} \infty D 0 \mathbf{D} D_{kj} = (-1)^{k-j} \frac{1}{(k-j)!} \mathbf{U} \mathbf{L} L_{ij} = \frac{i}{j+1} \mathbf{R} R_{ij} = \frac{j}{i}","['real-analysis', 'linear-algebra', 'combinatorics', 'functional-analysis', 'tetration']"
55,Finding a set of matrices based on eigenvalues and eigenvectors with constraints,Finding a set of matrices based on eigenvalues and eigenvectors with constraints,,"I'm trying to solve the following problem and hope for some helpful insights on how to approach this: In the 3-dimensional case, for and a given set of eigenvalues and eigenvectors and chosen values $a$ and $b$ , $c$ , find the set of 3 by 3 matrices that have the corresponding eigenvalues and eigenvectors (if it exists). Some elements of the matrix and eigenvectors are fixed values. For example in the 3-dimensional case: Eigenvalues: $ \lambda_1=3 $ , $\lambda_2 = -1$ and $\lambda_3 = 2$ Eigenvectors: $v_1=\begin{bmatrix} 1 \\ 2 \\a  \end{bmatrix} $ , $ v_2=\begin{bmatrix} 2 \\ b \\1  \end{bmatrix} $ and $ v_3=\begin{bmatrix} c \\ 0 \\-1  \end{bmatrix} $ The matrix has the following fixed elements: $$ \begin{bmatrix} x_{11} & 2 & x_{13} \\ x_{21} & x_{22} & 1 \\ x_{31} & 3 & x_{33} \end{bmatrix} $$ In other words: Let's say I set $a=2$ and $b=-1$ and $c=1$ . What are the possible matrices with fixed values $x_{12}=2$ , $x_{23}=1$ and $x_{32}=3$ with eigenvalues $ \lambda_1=3 $ , $\lambda_2 = -1$ and $\lambda_3 = 2$ and eigenvectors $v_1=\begin{bmatrix} 1 \\ 2 \\2  \end{bmatrix} $ , $ v_2=\begin{bmatrix} 2 \\ -1 \\1  \end{bmatrix} $ and $ v_3=\begin{bmatrix} 1 \\ 0 \\-1  \end{bmatrix} $ . Also, it would be interesting to know under what circumstances solutions exist and how many possible solutions there are? My first idea was to solve the eigenvalue equations for each eigenvalue and eigenvector: $$ (A-\lambda I)v=0 $$ for example for $\lambda_1$ and $v_1 $ : $$ \begin{bmatrix} x_{11}-3 & 2 & x_{13} \\ x_{21} & x_{22}-3 & 1 \\ x_{31} & 3 & x_{33}-3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\2  \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\0  \end{bmatrix} $$ However, the values of $x$ must be true for all eigenvectors and not just for one. So I got stuck (or I'm missing something). My second idea was to use the fact that a matrix $M$ can be obtained when knowing the eigenvectors and eigenvalues by $M=PDP^{-1}$ where $P$ is the matrix with the eigenvectors as columns and $D$ the diagonal matrix of eigenvalues. However, this does not take care of the constraints. Stuck again.","I'm trying to solve the following problem and hope for some helpful insights on how to approach this: In the 3-dimensional case, for and a given set of eigenvalues and eigenvectors and chosen values and , , find the set of 3 by 3 matrices that have the corresponding eigenvalues and eigenvectors (if it exists). Some elements of the matrix and eigenvectors are fixed values. For example in the 3-dimensional case: Eigenvalues: , and Eigenvectors: , and The matrix has the following fixed elements: In other words: Let's say I set and and . What are the possible matrices with fixed values , and with eigenvalues , and and eigenvectors , and . Also, it would be interesting to know under what circumstances solutions exist and how many possible solutions there are? My first idea was to solve the eigenvalue equations for each eigenvalue and eigenvector: for example for and : However, the values of must be true for all eigenvectors and not just for one. So I got stuck (or I'm missing something). My second idea was to use the fact that a matrix can be obtained when knowing the eigenvectors and eigenvalues by where is the matrix with the eigenvectors as columns and the diagonal matrix of eigenvalues. However, this does not take care of the constraints. Stuck again.",a b c  \lambda_1=3  \lambda_2 = -1 \lambda_3 = 2 v_1=\begin{bmatrix} 1 \\ 2 \\a  \end{bmatrix}   v_2=\begin{bmatrix} 2 \\ b \\1  \end{bmatrix}   v_3=\begin{bmatrix} c \\ 0 \\-1  \end{bmatrix}   \begin{bmatrix} x_{11} & 2 & x_{13} \\ x_{21} & x_{22} & 1 \\ x_{31} & 3 & x_{33} \end{bmatrix}  a=2 b=-1 c=1 x_{12}=2 x_{23}=1 x_{32}=3  \lambda_1=3  \lambda_2 = -1 \lambda_3 = 2 v_1=\begin{bmatrix} 1 \\ 2 \\2  \end{bmatrix}   v_2=\begin{bmatrix} 2 \\ -1 \\1  \end{bmatrix}   v_3=\begin{bmatrix} 1 \\ 0 \\-1  \end{bmatrix}   (A-\lambda I)v=0  \lambda_1 v_1   \begin{bmatrix} x_{11}-3 & 2 & x_{13} \\ x_{21} & x_{22}-3 & 1 \\ x_{31} & 3 & x_{33}-3 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\2  \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\0  \end{bmatrix}  x M M=PDP^{-1} P D,"['linear-algebra', 'eigenvalues-eigenvectors']"
56,Positive semidefiniteness of symmetric matrix with diagonal = 1 and non-diagonal elements less than 1,Positive semidefiniteness of symmetric matrix with diagonal = 1 and non-diagonal elements less than 1,,Does anyone know any useful results with respect to symmetric matrices with constant diagonals (specifically with respect to whether all eigenvalues are greater than $0$ )? I am working on a set of general $n \times n$ matrices with diagonal entries all equal to $1$ and non-diagonal entries between $0$ and $1$ and I am attempting to figure out whether or not this is a subset of positive semidefinite matrices,Does anyone know any useful results with respect to symmetric matrices with constant diagonals (specifically with respect to whether all eigenvalues are greater than )? I am working on a set of general matrices with diagonal entries all equal to and non-diagonal entries between and and I am attempting to figure out whether or not this is a subset of positive semidefinite matrices,0 n \times n 1 0 1,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
57,"if two matrices have the same solution space, do they have the same nullspace?","if two matrices have the same solution space, do they have the same nullspace?",,"Let $A, B âˆˆ M_{mÃ—n}(F)$ , and suppose $b âˆˆ F^m$ . If the solution sets $\{x : Ax = b\}$ and $\{x : Bx = b\}$ are nonempty and equal, does it follow that $N(A) = N(B)$ ? Unless $b=0$ , I don't think this is true. How can I prove this for some fixed non-zero b?","Let , and suppose . If the solution sets and are nonempty and equal, does it follow that ? Unless , I don't think this is true. How can I prove this for some fixed non-zero b?","A, B âˆˆ M_{mÃ—n}(F) b âˆˆ F^m \{x : Ax = b\} \{x : Bx = b\} N(A) = N(B) b=0",['linear-algebra']
58,Deducing the null space from column relations,Deducing the null space from column relations,,"Let $A=\begin{bmatrix} a_{1} & a_{2} & a_{3} \end{bmatrix}$ and $3a_{1}+2a_{2}+a_{3}=0$ where $a_{i}$ are matrix columns. Find the nullspace of $A$ . So, my initial thought was that the nullspace is the span of the vector $\begin{bmatrix} 3\\  2\\  1 \end{bmatrix}$ , but I'm not quite sure.","Let and where are matrix columns. Find the nullspace of . So, my initial thought was that the nullspace is the span of the vector , but I'm not quite sure.","A=\begin{bmatrix}
a_{1} & a_{2} & a_{3}
\end{bmatrix} 3a_{1}+2a_{2}+a_{3}=0 a_{i} A \begin{bmatrix}
3\\ 
2\\ 
1
\end{bmatrix}","['linear-algebra', 'matrices']"
59,Points of continuity in $ M_n (k) $ of minimal polynomials,Points of continuity in  of minimal polynomials, M_n (k) ,"The goal is to show that $\Gamma $ the set of points of continuity of $ M \mapsto \pi_M $ is $ \{ M \in M_n(k) , \chi_M = \pi_M \} $ .  Where $ \pi_M $ is the minimal polynomial of $M$ and $ \chi_M $ is the characteristic polynomial of $M$ . With the field being $ k = \mathbb{R} , \mathbb{C} $ so that we can talk about topology. $ \bullet $ First we know that $ M \mapsto \chi_M $ is continuous because Newton's identities expresses the coefficients of $ \chi_M $ as polynomial functions of $tr(A^k) $ for $  k \leq n $ and that's a finite dimensional multilinear form. $ \bullet $ Then we see that $ M \mapsto \pi_M $ is not continuous because there exists a sequence $ (A_p)_p \subset M_n(k) $ which converges to A but $ \pi_{A_p} $ does not converge to $ \pi_A $ . $ \bullet $ So one of the two inclusions is direct. We now need to prove that : $$ \Gamma \subset  \{ M \in M_n(k) , \chi_M = \pi_M \} $$ The indication is to show that this last set in an open set of $ M_n(k) $ .... Thanks in advance for any help",The goal is to show that the set of points of continuity of is .  Where is the minimal polynomial of and is the characteristic polynomial of . With the field being so that we can talk about topology. First we know that is continuous because Newton's identities expresses the coefficients of as polynomial functions of for and that's a finite dimensional multilinear form. Then we see that is not continuous because there exists a sequence which converges to A but does not converge to . So one of the two inclusions is direct. We now need to prove that : The indication is to show that this last set in an open set of .... Thanks in advance for any help,"\Gamma   M \mapsto \pi_M   \{ M \in M_n(k) , \chi_M = \pi_M \}   \pi_M  M  \chi_M  M  k = \mathbb{R} , \mathbb{C}   \bullet   M \mapsto \chi_M   \chi_M  tr(A^k)    k \leq n   \bullet   M \mapsto \pi_M   (A_p)_p \subset M_n(k)   \pi_{A_p}   \pi_A   \bullet   \Gamma \subset  \{ M \in M_n(k) , \chi_M = \pi_M \}   M_n(k) ","['linear-algebra', 'abstract-algebra', 'general-topology', 'matrix-calculus']"
60,Matrix eigenvalues,Matrix eigenvalues,,"Consider the matrix $$A_n=\begin{bmatrix} 	a & b & 0 & 0 & 0 & \dots & 0 & 0 & 0 \\ 	c & a & b & 0 & 0 & \dots & 0 & 0 & 0 \\ 	0 & c & a & b & 0 & \dots & 0 & 0 & 0 \\ 	0 & 0 & c & a & b & \dots & 0 & 0 & 0 \\ 	0 & 0 & 0 & c & a & \dots & 0 & 0 & 0 \\ 	\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 	0 & 0 & 0 & 0 & 0  & \dots & a & b & 0 \\ 	0 & 0 & 0 & 0 & 0  & \dots & c & a & b \\ 	0 & 0 & 0 & 0 & 0  & \dots & 0 & c & a   	\end{bmatrix}_{n\times n}$$ The matrix with $a=2$ and $b=c=-1$ is encountered in finite difference discretization of $u_{xx}.$ (a) If $D_n = \det(A_n),$ show that $D_n = aD_{n-1}-bcD_{n-2}.$ (b) Solve the recurrence analytically to obtain $D_n$ as a function of $n.$ (and ofcourse $D_n$ will also depend on $a, b, c.$) (c) Obtain the eigenvalues of $A_n.$ (Hint: Replace $a$ by $a-\lambda$) $$ $$ $$ $$(a)Part can be shown easily by just simple Laplace expansion. (b)We see that $D_0=1, D_1=a$. Let $D_n=r^n$ be a solution of the recurrence relation \begin{equation} D_n=aD_{n-1}-bcD_{n-2} \end{equation} Then characteristic equation corresponding to (1) \begin{alignat*}{3} &\quad & r^n-ar^{n-1}+bcr^{n-2} &=0 \\&\implies &r^2-ar+bc &=0 \\&\implies &r_1=\tfrac{a-\sqrt{a^2-4bc}}{2}, r_2 &=\tfrac{a+\sqrt{a^2-4bc}}{2} \end{alignat*}$ $ Case 1: $a^2-4bc=0$ $r_1=r_2=\frac{a}{2}$ General solution of (1) : $D_n=(C_1+nC_2)(\frac{a}{2})^n$, where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1=D_0=1$. For $n=1$, we get $(C_1+C_2)\frac{a}{2}=D_1=a\implies C_2=1$ Hence $D_n=(1+n)(\frac{a}{2})^n$  $$ $$ Case 2: $a^2-4bc\neq0$ General solution of (1) : $D_n=C_1r_1^n+C_2r_2^n$, with where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1+C_2=D_0=1$ For $n=1$, we get $C_1r_1+C_2r_2=D_1=a\implies (C_1+C_2)\frac{a}{2}+(C_2-C_1)\frac{\sqrt{a^2-4bc}}{2}=a   \implies 2C_2-1=\frac{a}{\sqrt{a^2-4bc}}   \implies C_2=\frac{r_2}{\sqrt{a^2-4bc}}$ $\therefore C_1=\frac{-r_1}{\sqrt{a^2-4bc}}$ Hence $D_n=\frac{r_2^{n+1}-r_1^{n+1}}{\sqrt{a^2-4bc}}=\frac{1}{2^{n+1}\sqrt{a^2-4bc}}[(a+\sqrt{a^2-4bc})^{n+1}-(a-\sqrt{a^2-4bc})^{n+1}]$ $$------------------------------------$$ I have done this far, but I'm stuck now. Is there any simpler expression for $D_n$? How to obtain eigenvalues, if we consider replacing $a$ by $a-\lambda$?","Consider the matrix $$A_n=\begin{bmatrix} 	a & b & 0 & 0 & 0 & \dots & 0 & 0 & 0 \\ 	c & a & b & 0 & 0 & \dots & 0 & 0 & 0 \\ 	0 & c & a & b & 0 & \dots & 0 & 0 & 0 \\ 	0 & 0 & c & a & b & \dots & 0 & 0 & 0 \\ 	0 & 0 & 0 & c & a & \dots & 0 & 0 & 0 \\ 	\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 	0 & 0 & 0 & 0 & 0  & \dots & a & b & 0 \\ 	0 & 0 & 0 & 0 & 0  & \dots & c & a & b \\ 	0 & 0 & 0 & 0 & 0  & \dots & 0 & c & a   	\end{bmatrix}_{n\times n}$$ The matrix with $a=2$ and $b=c=-1$ is encountered in finite difference discretization of $u_{xx}.$ (a) If $D_n = \det(A_n),$ show that $D_n = aD_{n-1}-bcD_{n-2}.$ (b) Solve the recurrence analytically to obtain $D_n$ as a function of $n.$ (and ofcourse $D_n$ will also depend on $a, b, c.$) (c) Obtain the eigenvalues of $A_n.$ (Hint: Replace $a$ by $a-\lambda$) $$ $$ $$ $$(a)Part can be shown easily by just simple Laplace expansion. (b)We see that $D_0=1, D_1=a$. Let $D_n=r^n$ be a solution of the recurrence relation \begin{equation} D_n=aD_{n-1}-bcD_{n-2} \end{equation} Then characteristic equation corresponding to (1) \begin{alignat*}{3} &\quad & r^n-ar^{n-1}+bcr^{n-2} &=0 \\&\implies &r^2-ar+bc &=0 \\&\implies &r_1=\tfrac{a-\sqrt{a^2-4bc}}{2}, r_2 &=\tfrac{a+\sqrt{a^2-4bc}}{2} \end{alignat*}$ $ Case 1: $a^2-4bc=0$ $r_1=r_2=\frac{a}{2}$ General solution of (1) : $D_n=(C_1+nC_2)(\frac{a}{2})^n$, where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1=D_0=1$. For $n=1$, we get $(C_1+C_2)\frac{a}{2}=D_1=a\implies C_2=1$ Hence $D_n=(1+n)(\frac{a}{2})^n$  $$ $$ Case 2: $a^2-4bc\neq0$ General solution of (1) : $D_n=C_1r_1^n+C_2r_2^n$, with where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1+C_2=D_0=1$ For $n=1$, we get $C_1r_1+C_2r_2=D_1=a\implies (C_1+C_2)\frac{a}{2}+(C_2-C_1)\frac{\sqrt{a^2-4bc}}{2}=a   \implies 2C_2-1=\frac{a}{\sqrt{a^2-4bc}}   \implies C_2=\frac{r_2}{\sqrt{a^2-4bc}}$ $\therefore C_1=\frac{-r_1}{\sqrt{a^2-4bc}}$ Hence $D_n=\frac{r_2^{n+1}-r_1^{n+1}}{\sqrt{a^2-4bc}}=\frac{1}{2^{n+1}\sqrt{a^2-4bc}}[(a+\sqrt{a^2-4bc})^{n+1}-(a-\sqrt{a^2-4bc})^{n+1}]$ $$------------------------------------$$ I have done this far, but I'm stuck now. Is there any simpler expression for $D_n$? How to obtain eigenvalues, if we consider replacing $a$ by $a-\lambda$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
61,"Show that if some base can show as linear combination, then vectors in linear combination is linear indepedent","Show that if some base can show as linear combination, then vectors in linear combination is linear indepedent",,"Let $Bs=\{e_1,e_2,...,e_n\}$ is standard base $\mathbb R^{n}$. If $x_1,x_2,...,x_n$ vectors from space $R^{n}$ such that $e_i\in L(x_1,x_2,...,x_n)$, $i=1:n$ then set $\{x_1,x_2,...x_n\}$ is base of space $\mathbb R^{n}$?. My answer is yes. First I write $\alpha_1e_1+\alpha_2e_2+...+\alpha_ne_n=0$. Since $e_i\in L(x_1,x_2,...,x_n)$ we can write every vector in base $Bs$ as linear combination of $\{x_1,x_2,...,x_n\}$. For example: $\begin{matrix}  e_1=a_{11}x_1+a_{21}x_2+...+a_{n1}x_n\\ e_2=a_{12}x_1+a_{22}x_2+...+a_{n2}x_n\\ .........................\\ e_n=a_{1n}x_1+a_{2n}x_2+...+a_{nn}x_n \end{matrix}$ then $0=x1(\alpha_1a_{11}+\alpha_2a_{12}+...+\alpha_na_{1n})+x2(\alpha_1a_{21}+\alpha_2a_{22}+...+\alpha_na_{2n})+...+xn(\alpha_1a_{n1}+\alpha_2a_{n2}+...+\alpha_na_{nn})$ If we suppose opposite that this vectors is linear dependent, then one vector can be write as linear combination of other vectors, for example if I use $x_n$ to write as linear combination, then $(\alpha_1a_{n1}+\alpha_2a_{n2}+...+\alpha_{nn}a_{1n})\not=0$ because we can not divide something with zero, if we put  $(\alpha_10+\alpha_20+...+\alpha_na_{nn})\not=0$ then $\alpha_n \not=0$,so now we have $0e_1+0e_2+...+\alpha_ne_n=0$, then $e_n=0$ but it is not true, so $ L(x_1,x_2,...,x_n)$ is base of $\mathbb R^n$, is this ok?","Let $Bs=\{e_1,e_2,...,e_n\}$ is standard base $\mathbb R^{n}$. If $x_1,x_2,...,x_n$ vectors from space $R^{n}$ such that $e_i\in L(x_1,x_2,...,x_n)$, $i=1:n$ then set $\{x_1,x_2,...x_n\}$ is base of space $\mathbb R^{n}$?. My answer is yes. First I write $\alpha_1e_1+\alpha_2e_2+...+\alpha_ne_n=0$. Since $e_i\in L(x_1,x_2,...,x_n)$ we can write every vector in base $Bs$ as linear combination of $\{x_1,x_2,...,x_n\}$. For example: $\begin{matrix}  e_1=a_{11}x_1+a_{21}x_2+...+a_{n1}x_n\\ e_2=a_{12}x_1+a_{22}x_2+...+a_{n2}x_n\\ .........................\\ e_n=a_{1n}x_1+a_{2n}x_2+...+a_{nn}x_n \end{matrix}$ then $0=x1(\alpha_1a_{11}+\alpha_2a_{12}+...+\alpha_na_{1n})+x2(\alpha_1a_{21}+\alpha_2a_{22}+...+\alpha_na_{2n})+...+xn(\alpha_1a_{n1}+\alpha_2a_{n2}+...+\alpha_na_{nn})$ If we suppose opposite that this vectors is linear dependent, then one vector can be write as linear combination of other vectors, for example if I use $x_n$ to write as linear combination, then $(\alpha_1a_{n1}+\alpha_2a_{n2}+...+\alpha_{nn}a_{1n})\not=0$ because we can not divide something with zero, if we put  $(\alpha_10+\alpha_20+...+\alpha_na_{nn})\not=0$ then $\alpha_n \not=0$,so now we have $0e_1+0e_2+...+\alpha_ne_n=0$, then $e_n=0$ but it is not true, so $ L(x_1,x_2,...,x_n)$ is base of $\mathbb R^n$, is this ok?",,"['linear-algebra', 'vector-spaces']"
62,On the eigenvalues of a block-symmetric matrix,On the eigenvalues of a block-symmetric matrix,,"I'm trying to find some kind of property that can tell me something about the invertibility of the matrix $$ M = \begin{bmatrix} A & B \\ -B^{T} & A \end{bmatrix}$$ Where both $A$ and $B$ are square-invertible (but not symmetric) and the eigenvalues of $A$ are pure-imaginary (I don't know if this is a plus or not). Is there any ""easy"" condition to say that $M$ is going to be invertible? Thanks in advance!","I'm trying to find some kind of property that can tell me something about the invertibility of the matrix $$ M = \begin{bmatrix} A & B \\ -B^{T} & A \end{bmatrix}$$ Where both $A$ and $B$ are square-invertible (but not symmetric) and the eigenvalues of $A$ are pure-imaginary (I don't know if this is a plus or not). Is there any ""easy"" condition to say that $M$ is going to be invertible? Thanks in advance!",,"['linear-algebra', 'matrices', 'linear-transformations', 'inverse', 'symmetric-matrices']"
63,Natural isomorphism to dual space of an inner product space in complex case.,Natural isomorphism to dual space of an inner product space in complex case.,,"$k$ is a field and $V$ is a finite dimensional $k$ vector space that has an inner product $\langle - , - \rangle$. If $k = \mathbb{R}$, there is a natural isomorphism $\phi \colon V \rightarrow V^*$, $v \mapsto \langle v, - \rangle$. However, if $k = \mathbb{C}$, $\langle v, - \rangle$ is not linear because $\langle v, cx \rangle = \overline{c} \langle v, x \rangle$. $\langle -, v \rangle$ is linear but in this case  $V \rightarrow V^*$, $v \mapsto \langle -, v \rangle$ is not linear. Are there no natural isomorphism between $V$ and $V^*$ when $k=\mathbb{C}$?","$k$ is a field and $V$ is a finite dimensional $k$ vector space that has an inner product $\langle - , - \rangle$. If $k = \mathbb{R}$, there is a natural isomorphism $\phi \colon V \rightarrow V^*$, $v \mapsto \langle v, - \rangle$. However, if $k = \mathbb{C}$, $\langle v, - \rangle$ is not linear because $\langle v, cx \rangle = \overline{c} \langle v, x \rangle$. $\langle -, v \rangle$ is linear but in this case  $V \rightarrow V^*$, $v \mapsto \langle -, v \rangle$ is not linear. Are there no natural isomorphism between $V$ and $V^*$ when $k=\mathbb{C}$?",,[]
64,What can we say about invertible matrix $P$ under a condition that $A=PAP^{-1}$?,What can we say about invertible matrix  under a condition that ?,P A=PAP^{-1},"Given an integral matrix $A$, we shall consider the set $$\{P:\text{invertible}|PAP^{-1}=A\}.$$ We can check that the set is a group under the matrix multiplication. Can we say something algebraic for $P$? I am considering the concept of commutator to find some property for $P$. If you have another viewpoint or some comment, can you give it to me?","Given an integral matrix $A$, we shall consider the set $$\{P:\text{invertible}|PAP^{-1}=A\}.$$ We can check that the set is a group under the matrix multiplication. Can we say something algebraic for $P$? I am considering the concept of commutator to find some property for $P$. If you have another viewpoint or some comment, can you give it to me?",,"['linear-algebra', 'abstract-algebra', 'inverse']"
65,"If $E$ is an infinite vector space, how to interprete $\sum_{i}x_ie_i$?","If  is an infinite vector space, how to interprete ?",E \sum_{i}x_ie_i,"Let $E$ a vector space of infinite dimension and let $(e_i)_{i\mathbb N}$ a basis. How is interpreted  $$\sum_{i\in\mathbb N}x_ie_i \ \ ?$$ Suppose now that $E$ is a Banach space with norm $\|\cdot \|$. I suppose that $$\sum_{i\in\mathbb N}x_ie_i=\lim_{n\to \infty }\sum_{i=1}^n x_ie_i,$$ in the $\|\cdot \|$ sense, i.e. $$\forall \varepsilon>0, \exists N: \forall n\in \mathbb N, n\geq N\implies \left\|\sum_{i\in\mathbb N}x_ie_i-\sum_{i=1}^nx_ie_i\right\|<\varepsilon.$$ But since $E$ is a vector space, $\sum_{i\in\mathbb N}x_ie_i\in E$ and thus exist (by definition of a vector space), but I guess it could happen that $$\lim_{n\to \infty }\sum_{i=1}^n x_ie_i$$ doesn't exist or is infinite, does it ? So how can we manage this case ?","Let $E$ a vector space of infinite dimension and let $(e_i)_{i\mathbb N}$ a basis. How is interpreted  $$\sum_{i\in\mathbb N}x_ie_i \ \ ?$$ Suppose now that $E$ is a Banach space with norm $\|\cdot \|$. I suppose that $$\sum_{i\in\mathbb N}x_ie_i=\lim_{n\to \infty }\sum_{i=1}^n x_ie_i,$$ in the $\|\cdot \|$ sense, i.e. $$\forall \varepsilon>0, \exists N: \forall n\in \mathbb N, n\geq N\implies \left\|\sum_{i\in\mathbb N}x_ie_i-\sum_{i=1}^nx_ie_i\right\|<\varepsilon.$$ But since $E$ is a vector space, $\sum_{i\in\mathbb N}x_ie_i\in E$ and thus exist (by definition of a vector space), but I guess it could happen that $$\lim_{n\to \infty }\sum_{i=1}^n x_ie_i$$ doesn't exist or is infinite, does it ? So how can we manage this case ?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'summation']"
66,Evaluating trivial integral with linear algebra,Evaluating trivial integral with linear algebra,,$$\int_{-\infty}^{\infty} \frac{\text{d}x}{ax^2+bx+c} = \frac{\pi}{\sqrt{\det(A)}}$$ where $A = \begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix}$ The connection to matrix quadratic form is given via the equivalence: $$ax^2 + bx + c \equiv \begin{bmatrix}x&1\end{bmatrix}\begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix} \begin{bmatrix}x\\1\end{bmatrix} $$ What standard results from Linear Algebra make this integral a linear transformation of the standard integral? $$\int_{-\infty}^{\infty} \frac{\text{d}x}{x^2 + 1} = \pi$$,$$\int_{-\infty}^{\infty} \frac{\text{d}x}{ax^2+bx+c} = \frac{\pi}{\sqrt{\det(A)}}$$ where $A = \begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix}$ The connection to matrix quadratic form is given via the equivalence: $$ax^2 + bx + c \equiv \begin{bmatrix}x&1\end{bmatrix}\begin{bmatrix}a&\frac{b}{2}\\\frac{b}{2}&c\end{bmatrix} \begin{bmatrix}x\\1\end{bmatrix} $$ What standard results from Linear Algebra make this integral a linear transformation of the standard integral? $$\int_{-\infty}^{\infty} \frac{\text{d}x}{x^2 + 1} = \pi$$,,"['linear-algebra', 'integration', 'definite-integrals', 'linear-transformations', 'determinant']"
67,Proof every operator has an upper-triangular matrix,Proof every operator has an upper-triangular matrix,,"I am having trouble understanding this proof that every operator has an upper-triangular matrix. $\lambda=$ is an eigenvalue of $T$, for $T \in L(V)$ where $V$ is a vector space on $F^n$, they say : suppose $U = \mathcal{R}(T-\lambda I)$, then $\dim(U)<\dim(V)$ because $U$ is not surjective. $Tu=(T-\lambda I)u + \lambda u$ shows $T$ is invariant under $U$, because both terms are in U. So above they are showing that since $T$ is an operator on $U$, $T$ has an upper triangular matrix with respect to some basis of $U$, $u_1,...u_m$ because in this is a claim they are trying to prove  by induction. Because $T$ has an upper triangular matrix, this means $Tu_j=(T|_u)(u_j)\in \operatorname{span}(u_1...u_j)$ I think I get all of this so far. Here is what I don't understand : Extend $u_1...u_m$ to a basis of $V$, $u_1,...u_m,v_1,...,v_n$. For each $k$, $Tv_k=(T-\lambda I)v_k + \lambda v_k$ $(T-\lambda I)v_k \in U = \operatorname{span}(u_1,...,u_m) => Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)$ I guess $(T-\lambda I)v_k \in U $ because it equals zero, and zero is in $U?$ So why does it mean $Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)?$ $\lambda v_k$ is an eigenvector/eigenvalue, but is it true that there is only 1 independent eigenvector per eigenvalue? This one eigenvalue works on $v_1...v_k?$","I am having trouble understanding this proof that every operator has an upper-triangular matrix. $\lambda=$ is an eigenvalue of $T$, for $T \in L(V)$ where $V$ is a vector space on $F^n$, they say : suppose $U = \mathcal{R}(T-\lambda I)$, then $\dim(U)<\dim(V)$ because $U$ is not surjective. $Tu=(T-\lambda I)u + \lambda u$ shows $T$ is invariant under $U$, because both terms are in U. So above they are showing that since $T$ is an operator on $U$, $T$ has an upper triangular matrix with respect to some basis of $U$, $u_1,...u_m$ because in this is a claim they are trying to prove  by induction. Because $T$ has an upper triangular matrix, this means $Tu_j=(T|_u)(u_j)\in \operatorname{span}(u_1...u_j)$ I think I get all of this so far. Here is what I don't understand : Extend $u_1...u_m$ to a basis of $V$, $u_1,...u_m,v_1,...,v_n$. For each $k$, $Tv_k=(T-\lambda I)v_k + \lambda v_k$ $(T-\lambda I)v_k \in U = \operatorname{span}(u_1,...,u_m) => Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)$ I guess $(T-\lambda I)v_k \in U $ because it equals zero, and zero is in $U?$ So why does it mean $Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)?$ $\lambda v_k$ is an eigenvector/eigenvalue, but is it true that there is only 1 independent eigenvector per eigenvalue? This one eigenvalue works on $v_1...v_k?$",,['linear-algebra']
68,"What are the semi-norms on $M_n(\mathbb C)$ such that $\| AB\| = \| BA\|$, for all $A$ and $B$?","What are the semi-norms on  such that , for all  and ?",M_n(\mathbb C) \| AB\| = \| BA\| A B,"What are the semi-norms on $M_n(\mathbb C)$ such that $\| AB\| = \| BA\|$, for all $A$ and $B$ in $M_n(\mathbb C)$, $n â‰¥ 2$? I came across this exercise (an oral exercise), and I have thought about this : Is there a semi-norm that respects matrix similitary? Do you have another idea to solve this problem? It does not seem really natural to directly think of similitary and does not allow to conclude apparently: are these the only such semi-norms?","What are the semi-norms on $M_n(\mathbb C)$ such that $\| AB\| = \| BA\|$, for all $A$ and $B$ in $M_n(\mathbb C)$, $n â‰¥ 2$? I came across this exercise (an oral exercise), and I have thought about this : Is there a semi-norm that respects matrix similitary? Do you have another idea to solve this problem? It does not seem really natural to directly think of similitary and does not allow to conclude apparently: are these the only such semi-norms?",,['linear-algebra']
69,Random matrices: stochastic dominance of trace and determinant,Random matrices: stochastic dominance of trace and determinant,,"For this question, if $X$ and $Y$ are 2 real random scalars, then $X$ has first-order stochastic dominance over $Y$ if for any $x$, $\Pr[X\leq x]\leq\Pr[Y\leq x]$. Let's write $X\succeq Y$. Suppose $A$ and $B$ are $n\times n$ random positive semidefinite real matrices and I'm interested in sufficient conditions that allow me to say $\operatorname{tr}(A)\succeq\operatorname{tr}(B)$ or $\det(A)\succeq\det(B)$. In particular, can I say something if I know $c'Ac\succeq c'Bc$ for all $n\times 1$ nonstochastic $c$?","For this question, if $X$ and $Y$ are 2 real random scalars, then $X$ has first-order stochastic dominance over $Y$ if for any $x$, $\Pr[X\leq x]\leq\Pr[Y\leq x]$. Let's write $X\succeq Y$. Suppose $A$ and $B$ are $n\times n$ random positive semidefinite real matrices and I'm interested in sufficient conditions that allow me to say $\operatorname{tr}(A)\succeq\operatorname{tr}(B)$ or $\det(A)\succeq\det(B)$. In particular, can I say something if I know $c'Ac\succeq c'Bc$ for all $n\times 1$ nonstochastic $c$?",,"['linear-algebra', 'statistics', 'reference-request', 'random-matrices']"
70,Square Roots of a Matrix: Diagonalisable Solutions.,Square Roots of a Matrix: Diagonalisable Solutions.,,"I am trying to solve the following problem, Find all diagonalisable matrices $B$ such that $$B^2 = \left(\begin{matrix}     3    & 1 \\     -2       & 0  \end{matrix}\right)$$ I diagonalised the matrix on the RHS, so we can write $$B^2 = \left(\begin{matrix}     1    & 1 \\     -1     & -2  \end{matrix}\right) \left(\begin{matrix}     2    & 0 \\     0       & 1  \end{matrix}\right) \left(\begin{matrix}     2    & 1 \\     -1       & 1  \end{matrix}\right) $$ We know $B$ is diagonalisable, so we can write $$P_1 D_1^2 P_1^{-1} = \left(\begin{matrix}     1    & 1 \\     -1     & -2  \end{matrix}\right) \left(\begin{matrix}     2    & 0 \\     0       & 1  \end{matrix}\right) \left(\begin{matrix}     2    & 1 \\     -1       & 1  \end{matrix}\right) $$ So, I found the following four solutions, all involving $P_1 = P$, with $$ D_1 = \left(\begin{matrix}     \pm \sqrt{2}    & 0 \\     0       & \pm 1  \end{matrix}\right)$$ My question is, are there any more solutions? How can we be sure that there are/aren't more solutions? I think that if matrix diagonalisation is unique, then these should be the only solutions. Is this a valid idea? Edit: I found out that diagonalisation is unique up to permutations of the order of eigenvalues and eigenvectors. This implies switching the columns in $P_1$ and in $D_1$ are also valid solutions, but when $B$ is expanded, the same solutions arise as presented above.","I am trying to solve the following problem, Find all diagonalisable matrices $B$ such that $$B^2 = \left(\begin{matrix}     3    & 1 \\     -2       & 0  \end{matrix}\right)$$ I diagonalised the matrix on the RHS, so we can write $$B^2 = \left(\begin{matrix}     1    & 1 \\     -1     & -2  \end{matrix}\right) \left(\begin{matrix}     2    & 0 \\     0       & 1  \end{matrix}\right) \left(\begin{matrix}     2    & 1 \\     -1       & 1  \end{matrix}\right) $$ We know $B$ is diagonalisable, so we can write $$P_1 D_1^2 P_1^{-1} = \left(\begin{matrix}     1    & 1 \\     -1     & -2  \end{matrix}\right) \left(\begin{matrix}     2    & 0 \\     0       & 1  \end{matrix}\right) \left(\begin{matrix}     2    & 1 \\     -1       & 1  \end{matrix}\right) $$ So, I found the following four solutions, all involving $P_1 = P$, with $$ D_1 = \left(\begin{matrix}     \pm \sqrt{2}    & 0 \\     0       & \pm 1  \end{matrix}\right)$$ My question is, are there any more solutions? How can we be sure that there are/aren't more solutions? I think that if matrix diagonalisation is unique, then these should be the only solutions. Is this a valid idea? Edit: I found out that diagonalisation is unique up to permutations of the order of eigenvalues and eigenvectors. This implies switching the columns in $P_1$ and in $D_1$ are also valid solutions, but when $B$ is expanded, the same solutions arise as presented above.",,['linear-algebra']
71,How to prove this trace matrix inequalityï¼Ÿ,How to prove this trace matrix inequalityï¼Ÿ,,Given: $A$ is a diagonal positive definite matrix; $\operatorname{Tr}(A)=1$ and $\operatorname{Tr}(A^{2})\leq 1$; $B$ is a Hermitian matrix; $AB\neq BA$. How to prove the following: $$\operatorname{Tr}(AABB)-\operatorname{Tr}(ABAB)\leq \operatorname{Tr}(A^{2})\left[ \operatorname{Tr}(ABB)-\operatorname{Tr}(AB)\operatorname{Tr}(AB)% \right]? $$,Given: $A$ is a diagonal positive definite matrix; $\operatorname{Tr}(A)=1$ and $\operatorname{Tr}(A^{2})\leq 1$; $B$ is a Hermitian matrix; $AB\neq BA$. How to prove the following: $$\operatorname{Tr}(AABB)-\operatorname{Tr}(ABAB)\leq \operatorname{Tr}(A^{2})\left[ \operatorname{Tr}(ABB)-\operatorname{Tr}(AB)\operatorname{Tr}(AB)% \right]? $$,,"['linear-algebra', 'matrices', 'inequality']"
72,Is this $0v=0$ proof correct?,Is this  proof correct?,0v=0,"I'm taking my first linear algebra course at university and recently I've been introduced to vector spaces. Now, the teacher has asked us to prove that $0v=0$ for any $v$ using only the definition of a vector space. I've came up with a proof, but I'm unsure whether it's correct or not, as I'm not familiar with proofs in Math. $$ \begin{align}(1+1)v &= 2v \\ (1+1)v-2v &= 2v-2v\\ [(1+1)-2]v &= 2(v-v) \\ 0v &= 0 \end{align} $$ If this is correct I can go on proving other properties of vector spaces. Any feedback appreciated! EDIT: Ok, taking into account JosÃ©'s answer, I came up with this one $$ (1+0)v=v \\ 1v+0v=v \\ -v+v+0v=-v+v \\ 0v=0 $$","I'm taking my first linear algebra course at university and recently I've been introduced to vector spaces. Now, the teacher has asked us to prove that $0v=0$ for any $v$ using only the definition of a vector space. I've came up with a proof, but I'm unsure whether it's correct or not, as I'm not familiar with proofs in Math. $$ \begin{align}(1+1)v &= 2v \\ (1+1)v-2v &= 2v-2v\\ [(1+1)-2]v &= 2(v-v) \\ 0v &= 0 \end{align} $$ If this is correct I can go on proving other properties of vector spaces. Any feedback appreciated! EDIT: Ok, taking into account JosÃ©'s answer, I came up with this one $$ (1+0)v=v \\ 1v+0v=v \\ -v+v+0v=-v+v \\ 0v=0 $$",,"['linear-algebra', 'proof-verification', 'vector-spaces']"
73,Is every linear transformation with at least two eigenvalues decomposable?,Is every linear transformation with at least two eigenvalues decomposable?,,"(A linear transformation $T : V \rightarrow V$ is decomposable if $V$ can be written as the direct sum of two proper $T$-invariant subspaces) Some background for this question: I have been going back over linear algebra recently, with the intention of pushing standard theorems and proofs to their bare essentials, so I can get an idea of when a property is ""necessary"" and when it is only ""sufficient"". For example the theorem which states ""Every finite-dimensional inner product space $V$ induces a natural isomorphism $V \rightarrow V^*$, given by $v \mapsto <v,->$"" has nothing to do with symmetry or the full power of positive definiteness. So this can be generalised from an inner product to a non-degenerate bilinear form. However this theorem does depend on the finite dimensionality of $V$, and so any proof must make use of this property. Now I have been trying to prove the following theorem (or find a counter example to it). ""Given that $V$ is a vector space, and $T : V \rightarrow V$ is a linear transformation with at least two distinct eigenvalues. Then $T$ is decomposable"". This theorem can be proven for the finite dimensional case by considering the Frobenius normal form of some matrix of $T$, but this proof explicitly uses the property of finite dimensionality. I have tried hard to produce either a more general proof that doesn't use finiteness, or a counter example in the infinite case, but I feel like I have hit a brick wall. Any help would be greatly appreciated, so thank you in advance.","(A linear transformation $T : V \rightarrow V$ is decomposable if $V$ can be written as the direct sum of two proper $T$-invariant subspaces) Some background for this question: I have been going back over linear algebra recently, with the intention of pushing standard theorems and proofs to their bare essentials, so I can get an idea of when a property is ""necessary"" and when it is only ""sufficient"". For example the theorem which states ""Every finite-dimensional inner product space $V$ induces a natural isomorphism $V \rightarrow V^*$, given by $v \mapsto <v,->$"" has nothing to do with symmetry or the full power of positive definiteness. So this can be generalised from an inner product to a non-degenerate bilinear form. However this theorem does depend on the finite dimensionality of $V$, and so any proof must make use of this property. Now I have been trying to prove the following theorem (or find a counter example to it). ""Given that $V$ is a vector space, and $T : V \rightarrow V$ is a linear transformation with at least two distinct eigenvalues. Then $T$ is decomposable"". This theorem can be proven for the finite dimensional case by considering the Frobenius normal form of some matrix of $T$, but this proof explicitly uses the property of finite dimensionality. I have tried hard to produce either a more general proof that doesn't use finiteness, or a counter example in the infinite case, but I feel like I have hit a brick wall. Any help would be greatly appreciated, so thank you in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'matrix-decomposition']"
74,Using Jordan Normal Form to determine when characteristic and minimal polynomials are identical,Using Jordan Normal Form to determine when characteristic and minimal polynomials are identical,,"Say I want to immediately write down a matrix with an identical minimal and characteristic polynomial. Say, $$ (t-1)^{3}(t-2). $$ My first instinct is to write down Jordan Blocks in a block diagonal matrix with blocks  $$ J_{3}(1), \text{ }J_{1}(2)$$  to give a matrix $$ \begin{bmatrix} 1 & 1  & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 2 \end{bmatrix}. $$ Clearly this gives the required characteristic polynomial specified above. However, since we know that the minimal polynomial divides the characteristic polynomial, and that distinct roots must show up in the minimal polynomial, using the ease of Jordan Normal Form when multiplying matrices, we can quickly check each case  $(t-1)(t-2)$, $(t-1)^{2}(t-2)$, $(t-1)^{3}(t-2)$ and show that the last one is the only such zero matrix required. However, is it sufficient to argue from the get go, that because we've included only one Jordan Block of each eigenvalue of maximum size, we can immediately say that the characteristic and minimal polynomial are the same (due to the minimal polynomial of each individual block being the same as each of the factors in the characteristic polynomial)? In other words, if the minimal and characteristic polynomial are the same and is $$ \prod_{i=1}^{k} (t- \lambda_{i})^{n_{i}}, $$ then a matrix which immediately satisfies this is $$\begin{bmatrix} J_{n_{1}}(\lambda_{1}) & 0 & \dots & 0 \\ 0 & \ddots & &   \\ 0 & 0 & 0 & J_{n_{k}}(\lambda_{k}) \end{bmatrix}. $$ Does this seem reasonable? Thanks in advance.","Say I want to immediately write down a matrix with an identical minimal and characteristic polynomial. Say, $$ (t-1)^{3}(t-2). $$ My first instinct is to write down Jordan Blocks in a block diagonal matrix with blocks  $$ J_{3}(1), \text{ }J_{1}(2)$$  to give a matrix $$ \begin{bmatrix} 1 & 1  & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 2 \end{bmatrix}. $$ Clearly this gives the required characteristic polynomial specified above. However, since we know that the minimal polynomial divides the characteristic polynomial, and that distinct roots must show up in the minimal polynomial, using the ease of Jordan Normal Form when multiplying matrices, we can quickly check each case  $(t-1)(t-2)$, $(t-1)^{2}(t-2)$, $(t-1)^{3}(t-2)$ and show that the last one is the only such zero matrix required. However, is it sufficient to argue from the get go, that because we've included only one Jordan Block of each eigenvalue of maximum size, we can immediately say that the characteristic and minimal polynomial are the same (due to the minimal polynomial of each individual block being the same as each of the factors in the characteristic polynomial)? In other words, if the minimal and characteristic polynomial are the same and is $$ \prod_{i=1}^{k} (t- \lambda_{i})^{n_{i}}, $$ then a matrix which immediately satisfies this is $$\begin{bmatrix} J_{n_{1}}(\lambda_{1}) & 0 & \dots & 0 \\ 0 & \ddots & &   \\ 0 & 0 & 0 & J_{n_{k}}(\lambda_{k}) \end{bmatrix}. $$ Does this seem reasonable? Thanks in advance.",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'minimal-polynomials']"
75,Integration with Cos function in denominator,Integration with Cos function in denominator,,"I need to solve following integration: $$\int_0^{\frac{\pi }{2}} \frac{1}{\left(a \cos ^2(x)+1\right)^m} \, dx$$ In Mathematica, the solution is given with $\text{Hypergeometric2F1}$ function. However, I want to know how to derive this step-by-steps. Can someone please guide me? I started by applying the negative binomial expansion, however it works only when $a \cos ^2(x)<1$. I am wondering that if there is any general expression !!!","I need to solve following integration: $$\int_0^{\frac{\pi }{2}} \frac{1}{\left(a \cos ^2(x)+1\right)^m} \, dx$$ In Mathematica, the solution is given with $\text{Hypergeometric2F1}$ function. However, I want to know how to derive this step-by-steps. Can someone please guide me? I started by applying the negative binomial expansion, however it works only when $a \cos ^2(x)<1$. I am wondering that if there is any general expression !!!",,"['linear-algebra', 'integration']"
76,Rank of a Polynomial function over Finite Fields,Rank of a Polynomial function over Finite Fields,,"Consider a function $f:\mathbb{R}^k \to \mathbb{R}^n$ where $f=(f_1,f_2,\dots,f_n)$ with each $f_i$ being a polynomial function of variables $t_1,t_2,\dots,t_k$. Now if the polynomials $f_1,f_2,\dots,f_k$ were linear functions of $t_1,\dots,t_k$, then $f$ is a linear map from $\mathbb{R}^k$ to $\mathbb{R}^n$. In this case we can define a rank of the map, and we have the rank-nullity theorem relating the rank and the dimension of its kernel. What if the functions $f_1,f_2,\dots,f_n$ were of higher degree? What would be the natural generalization of the ""rank"" of the map $f$? In one context, I noticed that they defined the rank of $f$ to be the rank of the Jacobian matrix of $f$ (whose entries are treated as elements of the rational function field over $\mathbb{R}$. Is this a standard notion? It does generalize the notion of rank of a linear map. Do we have some analogous rank-nullity theorem in this case, relating the rank and the dimension of the variety $f=0$? I am aware that there are standard rigorous notions of dimension for varieties. I am especially interested in such a notion of rank when the underlying field is not $\mathbb{R}$ but a small prime field, say $\mathbb{F}_2$. In this case, the Jacobian seems a bit restrictive, since partial derivatives over $\mathbb{F}_2$ are themselves not very useful. Or maybe I am wrong here.  Another notion I have seen is that of algebraic independence, which seems reasonable as well. Would that agree with a rank-nullity theorem? More importantly, would that agree with the rank of the Jacobian? So my essential question is: What would be a notion of rank for a polynomial function $f:\mathbb{F}_2^k \to \mathbb{F}_2^n$?","Consider a function $f:\mathbb{R}^k \to \mathbb{R}^n$ where $f=(f_1,f_2,\dots,f_n)$ with each $f_i$ being a polynomial function of variables $t_1,t_2,\dots,t_k$. Now if the polynomials $f_1,f_2,\dots,f_k$ were linear functions of $t_1,\dots,t_k$, then $f$ is a linear map from $\mathbb{R}^k$ to $\mathbb{R}^n$. In this case we can define a rank of the map, and we have the rank-nullity theorem relating the rank and the dimension of its kernel. What if the functions $f_1,f_2,\dots,f_n$ were of higher degree? What would be the natural generalization of the ""rank"" of the map $f$? In one context, I noticed that they defined the rank of $f$ to be the rank of the Jacobian matrix of $f$ (whose entries are treated as elements of the rational function field over $\mathbb{R}$. Is this a standard notion? It does generalize the notion of rank of a linear map. Do we have some analogous rank-nullity theorem in this case, relating the rank and the dimension of the variety $f=0$? I am aware that there are standard rigorous notions of dimension for varieties. I am especially interested in such a notion of rank when the underlying field is not $\mathbb{R}$ but a small prime field, say $\mathbb{F}_2$. In this case, the Jacobian seems a bit restrictive, since partial derivatives over $\mathbb{F}_2$ are themselves not very useful. Or maybe I am wrong here.  Another notion I have seen is that of algebraic independence, which seems reasonable as well. Would that agree with a rank-nullity theorem? More importantly, would that agree with the rank of the Jacobian? So my essential question is: What would be a notion of rank for a polynomial function $f:\mathbb{F}_2^k \to \mathbb{F}_2^n$?",,"['linear-algebra', 'abstract-algebra', 'algebraic-geometry', 'polynomials', 'finite-fields']"
77,Operator norm induced by Frobenius norm,Operator norm induced by Frobenius norm,,"Suppose $T \colon \mathbb{R}^n \to \mathbb{R}^n$ is a linear operator. In following, we will consider $T$ as a matrix. The operator norm induced by $2$-norm on $\mathbb R^n$ is given by $\|T\|_{op,2} = \max_{\|x\|_2 = 1} \|Tx\|_2$. Let $T$ act by matrix multiplication on the vector space $\mathcal{M}(n \times m, \mathbb{R})$, i.e., the space of $n \times m$ matrices. What is operator norm $\|T\|_{op,F}$ induced by Frobenius norm $\| \cdot \|_F$ on $\mathcal {M}(n \times m)$? Here is my thought: For a given matrix $A \in \mathcal{M}(n \times m, \mathbb{R})$, let $A = (a_1, \dots, a_m)$ where $a_j \in \mathbb R^n$ \begin{align*} \| TA\|_F^2 &= \|(Ta_1, \dots, Ta_m)\|_F^2 \\ &= \| Ta_1\|_2^2 + \dots \|Ta_m\|_2^2 \le \|T\|_{op,2}^2 \|a_1\|_2^2 + \dots + \|T\|_{op,2}^2 \|a_m\|_2^2 \\&= \|T\|_{op,2}^2 \|A\|_F^2. \end{align*} It seems like the operator norm $\|T\|_{op,F}$ should be upper bounded by $\|T\|_{op,2}$. Are they indeed equal? Thanks. EDIT: @erfink points to this link . Indeed, I am asking a quite different question from this. I am not asking the Frobenius norm of $T$ but the operator norm induced by Frobenius norm of $\mathcal{M}(n \times m)$ when $T$ acts on this space by matrix multiplication. $T$ itself even is not an element of space but an elemnt of $\mathcal{M} (n \times n)$.","Suppose $T \colon \mathbb{R}^n \to \mathbb{R}^n$ is a linear operator. In following, we will consider $T$ as a matrix. The operator norm induced by $2$-norm on $\mathbb R^n$ is given by $\|T\|_{op,2} = \max_{\|x\|_2 = 1} \|Tx\|_2$. Let $T$ act by matrix multiplication on the vector space $\mathcal{M}(n \times m, \mathbb{R})$, i.e., the space of $n \times m$ matrices. What is operator norm $\|T\|_{op,F}$ induced by Frobenius norm $\| \cdot \|_F$ on $\mathcal {M}(n \times m)$? Here is my thought: For a given matrix $A \in \mathcal{M}(n \times m, \mathbb{R})$, let $A = (a_1, \dots, a_m)$ where $a_j \in \mathbb R^n$ \begin{align*} \| TA\|_F^2 &= \|(Ta_1, \dots, Ta_m)\|_F^2 \\ &= \| Ta_1\|_2^2 + \dots \|Ta_m\|_2^2 \le \|T\|_{op,2}^2 \|a_1\|_2^2 + \dots + \|T\|_{op,2}^2 \|a_m\|_2^2 \\&= \|T\|_{op,2}^2 \|A\|_F^2. \end{align*} It seems like the operator norm $\|T\|_{op,F}$ should be upper bounded by $\|T\|_{op,2}$. Are they indeed equal? Thanks. EDIT: @erfink points to this link . Indeed, I am asking a quite different question from this. I am not asking the Frobenius norm of $T$ but the operator norm induced by Frobenius norm of $\mathcal{M}(n \times m)$ when $T$ acts on this space by matrix multiplication. $T$ itself even is not an element of space but an elemnt of $\mathcal{M} (n \times n)$.",,"['linear-algebra', 'matrices', 'linear-transformations', 'normed-spaces', 'matrix-norms']"
78,When is product of right-invertible matrix and left-invertible matrix invertible?,When is product of right-invertible matrix and left-invertible matrix invertible?,,"I have a strong suspicion this is a textbook linear algebra problem, but I have been unsuccessful in finding an answer. Let $A$ be an $n \times m$ matrix and let $B$ be an $m \times n$ matrix where $n < m$.  Suppose than $A$ has rank $n$ ($A$ has a right-inverse) and $B$ also has rank $n$ ($B$ has a left-inverse). When is $AB$ invertible? Here is what I have so far.  Say that $AB$ is invertible and the inverse is $C$.  Then, $\left(CA\right) B = I_{n}$ And, $A\left(B C\right) = I_{n}$. Or: $BC$ must be a right-inverse of $A$ and $CA$ must be a left-inverse of $B$. $CA = B^{-1}_{left} \implies C = B^{-1}_{left} A^{-1}_{right}$ $BC = A^{-1}_{right} \implies C = B^{-1}_{left}A^{-1}_{right}$ So, if $C$ exists, then I know what it is.  If I know there is a $C$ such that $CA$ is a left-inverse of $B$ and $BC$ is a right-inverse of $A$, then the product is invertible.  What I'm looking for are more primitive conditions on the matrices $A$ and $B$ that guarantee such a matrix exists.","I have a strong suspicion this is a textbook linear algebra problem, but I have been unsuccessful in finding an answer. Let $A$ be an $n \times m$ matrix and let $B$ be an $m \times n$ matrix where $n < m$.  Suppose than $A$ has rank $n$ ($A$ has a right-inverse) and $B$ also has rank $n$ ($B$ has a left-inverse). When is $AB$ invertible? Here is what I have so far.  Say that $AB$ is invertible and the inverse is $C$.  Then, $\left(CA\right) B = I_{n}$ And, $A\left(B C\right) = I_{n}$. Or: $BC$ must be a right-inverse of $A$ and $CA$ must be a left-inverse of $B$. $CA = B^{-1}_{left} \implies C = B^{-1}_{left} A^{-1}_{right}$ $BC = A^{-1}_{right} \implies C = B^{-1}_{left}A^{-1}_{right}$ So, if $C$ exists, then I know what it is.  If I know there is a $C$ such that $CA$ is a left-inverse of $B$ and $BC$ is a right-inverse of $A$, then the product is invertible.  What I'm looking for are more primitive conditions on the matrices $A$ and $B$ that guarantee such a matrix exists.",,['linear-algebra']
79,What is the coequalizer of two $m \times n$ matrices in $\text{Matr$_K$}$?,What is the coequalizer of two  matrices in _K?,m \times n \text{Matr },"What is the coequalizer of two $m \times n$ matrices in $\text{Matr$_K$}$?$(K$ is a commutative ring, objects are positive integers, and arrows are $m \times n$ matricies$)$ If $A_{m \times n},B_{m \times n}$ are matrices (arrows $A,B: n \rightarrow m)$, then I would have to find a matrix $E_{e \times m}$ with $EA=EB$ such that for every $H_{d \times m}$ there's a unique $H'_{d \times e}$ such that $H' \circ E = H$. I first tried to work with $E$ being the zero matrix but it doesn't satisfy the above composition. Anyone have any ideas?","What is the coequalizer of two $m \times n$ matrices in $\text{Matr$_K$}$?$(K$ is a commutative ring, objects are positive integers, and arrows are $m \times n$ matricies$)$ If $A_{m \times n},B_{m \times n}$ are matrices (arrows $A,B: n \rightarrow m)$, then I would have to find a matrix $E_{e \times m}$ with $EA=EB$ such that for every $H_{d \times m}$ there's a unique $H'_{d \times e}$ such that $H' \circ E = H$. I first tried to work with $E$ being the zero matrix but it doesn't satisfy the above composition. Anyone have any ideas?",,"['linear-algebra', 'category-theory']"
80,Dimension of $End(V)$ with $V$ countable dimension irreducible module over a complex algebra,Dimension of  with  countable dimension irreducible module over a complex algebra,End(V) V,"Let $A$ be a $\mathbb{C}$-algebra and $V$ be an irreducible $A$-module with countable dimension. What is the dimension of $End(V)$ as $A$-module? Note that every endomorphism must be injective and surjective, because of the irreducibility of $V$. My claim (or at least my hope) is that $End(V)$ has countable dimension, but I can't see a proof of this fact.","Let $A$ be a $\mathbb{C}$-algebra and $V$ be an irreducible $A$-module with countable dimension. What is the dimension of $End(V)$ as $A$-module? Note that every endomorphism must be injective and surjective, because of the irreducibility of $V$. My claim (or at least my hope) is that $End(V)$ has countable dimension, but I can't see a proof of this fact.",,"['linear-algebra', 'abstract-algebra', 'cardinals', 'division-algebras']"
81,What to teach in a lecture about diagonalization of matrices in 50 minutes?,What to teach in a lecture about diagonalization of matrices in 50 minutes?,,"I have to give a lecture of 50 minutes with the theme ""diagonalization of matrices"" (basic undergrad level) for a public contest. I would like suggestions about the contents of this class. I've been thinking to give the pages 181-187 from Hoffman and Kunze's linear algebra book (maybe I will not have time to do all these pages). What do you think is the best approach? I don't know if I start this class with eigenvalues as Hoffman and Kunze do and goes through very basic theorems about eigenvalues and diagonalizable matrices or begin with the definition of diagonalizable matrices and prove deeper theorems.","I have to give a lecture of 50 minutes with the theme ""diagonalization of matrices"" (basic undergrad level) for a public contest. I would like suggestions about the contents of this class. I've been thinking to give the pages 181-187 from Hoffman and Kunze's linear algebra book (maybe I will not have time to do all these pages). What do you think is the best approach? I don't know if I start this class with eigenvalues as Hoffman and Kunze do and goes through very basic theorems about eigenvalues and diagonalizable matrices or begin with the definition of diagonalizable matrices and prove deeper theorems.",,"['linear-algebra', 'soft-question']"
82,Finding all invariant subspaces of a matrix,Finding all invariant subspaces of a matrix,,"Let $\alpha$ be a real number. Find all invariant subspaces for the matrix $$   \begin{pmatrix}   \cos \alpha & -\sin \alpha & 0 \\   \sin \alpha & \cos \alpha & 0 \\    0 & 0 & 1 \\   \end{pmatrix}. $$ How does the result depend on $\alpha$ ? I am a bit confused about how to find all the spaces. I can see that if $\alpha=0$ then every subspace is invariant, but what do I do in other cases? Should I find the eigenspaces?","Let be a real number. Find all invariant subspaces for the matrix How does the result depend on ? I am a bit confused about how to find all the spaces. I can see that if then every subspace is invariant, but what do I do in other cases? Should I find the eigenspaces?","\alpha 
  \begin{pmatrix}
  \cos \alpha & -\sin \alpha & 0 \\
  \sin \alpha & \cos \alpha & 0 \\ 
  0 & 0 & 1 \\
  \end{pmatrix}.
 \alpha \alpha=0","['linear-algebra', 'matrices', 'invariant-subspace']"
83,For which $n$ does there exist a surjective homomorphism from $SL_n(\mathbf{R})\rightarrow PGL_n(\mathbf{R})$?,For which  does there exist a surjective homomorphism from ?,n SL_n(\mathbf{R})\rightarrow PGL_n(\mathbf{R}),"Also, how does the situation change when replacing $\mathbf{R}$ with $\mathbf{Q}$? I have only very basic tools to approach this problem. My attempt at understanding it is that $PGL_n$ is the set of linear transformations which leave the norms of vectors the same, while $SL_n$ is the set of linear transformations which preserve volumes and their orientations. But I must have some misunderstanding because then one could consider in $PGL_4$ the matrix $$\left(\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0  &0 \\ 0& 0&1&0\\0&0&0&1 \end{array}\right)$$ for which I do not think you can come up with a matrix in $SL_4$ to be its preimage. But I have high confidence that this approach is incorrect so I would like to be pointed in a better direction.","Also, how does the situation change when replacing $\mathbf{R}$ with $\mathbf{Q}$? I have only very basic tools to approach this problem. My attempt at understanding it is that $PGL_n$ is the set of linear transformations which leave the norms of vectors the same, while $SL_n$ is the set of linear transformations which preserve volumes and their orientations. But I must have some misunderstanding because then one could consider in $PGL_4$ the matrix $$\left(\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0  &0 \\ 0& 0&1&0\\0&0&0&1 \end{array}\right)$$ for which I do not think you can come up with a matrix in $SL_4$ to be its preimage. But I have high confidence that this approach is incorrect so I would like to be pointed in a better direction.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'group-homomorphism', 'infinite-groups']"
84,Is there a metric function defined on $M_{n}\left( \mathbb{R} \right)$ such that the determinant function is not continuousï¼Ÿ,Is there a metric function defined on  such that the determinant function is not continuousï¼Ÿ,M_{n}\left( \mathbb{R} \right),"Is there a metric function $d \colon M_{n}(\mathbb{R}) \times M_{n}(\mathbb{R}) \mapsto \mathbb{R}$ , such that the determinant function $\det \colon M_{n}(\mathbb{R}) \mapsto \mathbb{R}$ is not a continuous function on the metric space $\left( M_{n}(\mathbb{R}), d( \cdot , \cdot ) \right)$ ? Of course, $M_{n}( \mathbb{R} )$ can be given the metric induced by a norm. And all norms on a finite dimensional space are equivalent. Thus, we can prove that $\det ( \cdot )$ is always continuous on any metric space $\left( M_{n}(\mathbb{R}), d( \cdot , \cdot ) \right)$ induced by a norm space $\left( M_{n}(\mathbb{R}), \left\Vert \cdot \right\Vert \right)$ by defining a proper norm. I want to know whether there do exist such a metric space, it cannot be a norm space.","Is there a metric function , such that the determinant function is not a continuous function on the metric space ? Of course, can be given the metric induced by a norm. And all norms on a finite dimensional space are equivalent. Thus, we can prove that is always continuous on any metric space induced by a norm space by defining a proper norm. I want to know whether there do exist such a metric space, it cannot be a norm space.","d \colon M_{n}(\mathbb{R}) \times M_{n}(\mathbb{R}) \mapsto \mathbb{R} \det \colon M_{n}(\mathbb{R}) \mapsto \mathbb{R} \left( M_{n}(\mathbb{R}), d( \cdot , \cdot ) \right) M_{n}( \mathbb{R} ) \det ( \cdot ) \left( M_{n}(\mathbb{R}), d( \cdot , \cdot ) \right) \left( M_{n}(\mathbb{R}), \left\Vert \cdot \right\Vert \right)","['linear-algebra', 'matrices', 'functional-analysis', 'determinant']"
85,Finding all possible Jordan forms of an $ 8\times 8$ matrix given the minimal polynomial,Finding all possible Jordan forms of an  matrix given the minimal polynomial, 8\times 8,Find all possible Jordan forms of an $ 8\times 8$ matrix given that $$t^2(t-1)^3$$ is the minimal polynomial. I don't really know where to start so all help would be appreciated,Find all possible Jordan forms of an $ 8\times 8$ matrix given that $$t^2(t-1)^3$$ is the minimal polynomial. I don't really know where to start so all help would be appreciated,,"['linear-algebra', 'matrices', 'jordan-normal-form', 'minimal-polynomials']"
86,Expansion of the KÃ¤hler metric of Grassmannian / A simple question on linear algebra,Expansion of the KÃ¤hler metric of Grassmannian / A simple question on linear algebra,,"Here's a dumb question from a physicist. The problem comes from expanding the KÃ¤hler potential of the Grassmannian manifold in the vicinity of a certain point. Consider two matrices, $\{A^i_j\}_{N\times N}$ and $\{\tilde{A}^\alpha_\beta\}_{M\times M}$, defined as: \begin{alignat}{9} \begin{alignedat}{9} A^i_j = \delta^i_j + \phi^i_\alpha\bar{\phi}^\alpha_j\\ \tilde{A}^\alpha_\beta = \delta^\alpha_\beta + \bar{\phi}^\alpha_i\phi^i_\beta \end{alignedat}\quad,\qquad \begin{alignedat}{9} i,j&=1\ldots N\\ \alpha,\beta&=1\ldots M \end{alignedat} \end{alignat} Here bar denotes Hermitean conjugation. It's straightforward to show that $\operatorname{Tr{}} \ln(A) = \tilde{\operatorname{Tr{}}} \ln(\tilde{A}) \equiv K$, where the tilde over the trace reminds that the trace is taken over the greek indices. Indeed, since the logarithm is defined through a series, we have: \begin{equation}     \operatorname{Tr{}} \ln(A) = \operatorname{Tr{}} f_k (\phi^i_\alpha\bar{\phi}^\alpha_j)^k     = f_k \operatorname{Tr{}} (\phi^i_\alpha\bar{\phi}^\alpha_j)^k     = f_k \tilde{\operatorname{Tr{}}} (\bar{\phi}^\alpha_i\phi^i_\beta)^k     = \tilde{\operatorname{Tr{}}} f_k (\bar{\phi}^\alpha_i\phi^i_\beta)^k     = \tilde{\operatorname{Tr{}}} \ln(\tilde{A}) \end{equation} Here $f_k$ are the coefficients in the Taylor expansion on the logarithm. As I am constructing the small-$\phi$ expansion of the following quantity (the generalisation of the Fubini-Study metric): $$ G_{i\beta}^{j\alpha}= \dfrac{\partial}{\partial\phi^i_\alpha}\dfrac{\partial}{\partial\bar{\phi}_j^\beta} K $$ Depending on which definition of $K$ I'm using, I'm getting different results. The definition above renders: \begin{alignat}{9} {}^{(I)}G_{i\beta}^{j\alpha}&=\operatorname{Tr{}} (A^{-1}) \delta_i^j \delta^\alpha_\beta - \operatorname{Tr{}}(A^{-2}) \phi^j_\beta \bar{\phi}_i^\alpha \\ {}^{(II)}G_{i\beta}^{j\alpha}&=\tilde{\operatorname{Tr{}}} (\tilde{A}^{-1}) \delta_i^j \delta^\alpha_\beta - \tilde{\operatorname{Tr{}}}(\tilde{A}^{-2}) \phi^j_\beta \bar{\phi}_i^\alpha \end{alignat} OK, looks like the quantities $\operatorname{Tr{}} (A^{-1})$  and $\tilde{\operatorname{Tr{}}} (\tilde{A}^{-1})$ should coincide. However: \begin{alignat}{9} (A^{-1})^i_j &\approx \delta^i_j - \phi^i_\alpha \bar{\phi}^\alpha_j \\ (\tilde{A}^{-1})^\alpha_\beta &\approx \delta^\alpha_\beta - \bar{\phi}^\alpha_i  \phi^i_\beta \end{alignat} Which gives for the traces: \begin{alignat}{9} \begin{alignedat}{9} \operatorname{Tr{}} (A^{-1}) &= N - \phi^i_\alpha \bar{\phi}^\alpha_i \\ \tilde{\operatorname{Tr{}}} (\tilde{A}^{-1}) &= M - \phi^i_\alpha \bar{\phi}^\alpha_i \end{alignedat} \quad,\qquad \begin{alignedat}{9} \operatorname{Tr{}} (A^{-2}) &= N - 2\phi^i_\alpha \bar{\phi}^\alpha_i \\ \tilde{\operatorname{Tr{}}} (\tilde{A}^{-2}) &= M - 2\phi^i_\alpha \bar{\phi}^\alpha_i \end{alignedat} \end{alignat} Clearly, the expansions ${}^{(I)}G_{i\beta}^{j\alpha}$ and ${}^{(II)}G_{i\beta}^{j\alpha}$ differ, and only match upon replacing $N$ with $M$. What am I doing wrong?","Here's a dumb question from a physicist. The problem comes from expanding the KÃ¤hler potential of the Grassmannian manifold in the vicinity of a certain point. Consider two matrices, $\{A^i_j\}_{N\times N}$ and $\{\tilde{A}^\alpha_\beta\}_{M\times M}$, defined as: \begin{alignat}{9} \begin{alignedat}{9} A^i_j = \delta^i_j + \phi^i_\alpha\bar{\phi}^\alpha_j\\ \tilde{A}^\alpha_\beta = \delta^\alpha_\beta + \bar{\phi}^\alpha_i\phi^i_\beta \end{alignedat}\quad,\qquad \begin{alignedat}{9} i,j&=1\ldots N\\ \alpha,\beta&=1\ldots M \end{alignedat} \end{alignat} Here bar denotes Hermitean conjugation. It's straightforward to show that $\operatorname{Tr{}} \ln(A) = \tilde{\operatorname{Tr{}}} \ln(\tilde{A}) \equiv K$, where the tilde over the trace reminds that the trace is taken over the greek indices. Indeed, since the logarithm is defined through a series, we have: \begin{equation}     \operatorname{Tr{}} \ln(A) = \operatorname{Tr{}} f_k (\phi^i_\alpha\bar{\phi}^\alpha_j)^k     = f_k \operatorname{Tr{}} (\phi^i_\alpha\bar{\phi}^\alpha_j)^k     = f_k \tilde{\operatorname{Tr{}}} (\bar{\phi}^\alpha_i\phi^i_\beta)^k     = \tilde{\operatorname{Tr{}}} f_k (\bar{\phi}^\alpha_i\phi^i_\beta)^k     = \tilde{\operatorname{Tr{}}} \ln(\tilde{A}) \end{equation} Here $f_k$ are the coefficients in the Taylor expansion on the logarithm. As I am constructing the small-$\phi$ expansion of the following quantity (the generalisation of the Fubini-Study metric): $$ G_{i\beta}^{j\alpha}= \dfrac{\partial}{\partial\phi^i_\alpha}\dfrac{\partial}{\partial\bar{\phi}_j^\beta} K $$ Depending on which definition of $K$ I'm using, I'm getting different results. The definition above renders: \begin{alignat}{9} {}^{(I)}G_{i\beta}^{j\alpha}&=\operatorname{Tr{}} (A^{-1}) \delta_i^j \delta^\alpha_\beta - \operatorname{Tr{}}(A^{-2}) \phi^j_\beta \bar{\phi}_i^\alpha \\ {}^{(II)}G_{i\beta}^{j\alpha}&=\tilde{\operatorname{Tr{}}} (\tilde{A}^{-1}) \delta_i^j \delta^\alpha_\beta - \tilde{\operatorname{Tr{}}}(\tilde{A}^{-2}) \phi^j_\beta \bar{\phi}_i^\alpha \end{alignat} OK, looks like the quantities $\operatorname{Tr{}} (A^{-1})$  and $\tilde{\operatorname{Tr{}}} (\tilde{A}^{-1})$ should coincide. However: \begin{alignat}{9} (A^{-1})^i_j &\approx \delta^i_j - \phi^i_\alpha \bar{\phi}^\alpha_j \\ (\tilde{A}^{-1})^\alpha_\beta &\approx \delta^\alpha_\beta - \bar{\phi}^\alpha_i  \phi^i_\beta \end{alignat} Which gives for the traces: \begin{alignat}{9} \begin{alignedat}{9} \operatorname{Tr{}} (A^{-1}) &= N - \phi^i_\alpha \bar{\phi}^\alpha_i \\ \tilde{\operatorname{Tr{}}} (\tilde{A}^{-1}) &= M - \phi^i_\alpha \bar{\phi}^\alpha_i \end{alignedat} \quad,\qquad \begin{alignedat}{9} \operatorname{Tr{}} (A^{-2}) &= N - 2\phi^i_\alpha \bar{\phi}^\alpha_i \\ \tilde{\operatorname{Tr{}}} (\tilde{A}^{-2}) &= M - 2\phi^i_\alpha \bar{\phi}^\alpha_i \end{alignedat} \end{alignat} Clearly, the expansions ${}^{(I)}G_{i\beta}^{j\alpha}$ and ${}^{(II)}G_{i\beta}^{j\alpha}$ differ, and only match upon replacing $N$ with $M$. What am I doing wrong?",,"['linear-algebra', 'matrices', 'tensor-products', 'tensors', 'kahler-manifolds']"
87,Differential operator acting on a Vandermonde determinant - an identity,Differential operator acting on a Vandermonde determinant - an identity,,"in my endeavors I've stumbled upon the following identity: $$ \prod_{i=1}^N \left ( 1 + \frac{\partial}{\partial a_i}\right ) \left (   \Delta(a) \prod_{i=1}^N a_i \right ) = \Delta(a) \int_0^\infty dt e^{-t} \prod_{i=1}^N (t+a_i)  $$ with Vandermonde determinant $\Delta (a) = \det \left ( a_j^{i-1} \right )_{i,j=1...N}$. I find it surprisingly hard to prove, maybe someone knows this formula or could suggest any way to derive it? I've tried to utilize Schur polynomials as the LHS is the $s_{(1_N)}$ but to no avail. EDIT:  My attempt of a proof started from the RHS where I use the generating functional of elementary symmetric functions: $$ \prod_{i=1}^N (t + a_i ) = \sum_{n=0}^N \sigma_n (a) t^{N-n} $$ with elementary symmetric functions defined as $$ \sigma_n(a) = \sum_{1\leq i_1 < i_2 < ... < i_n \leq N} a_{i_1} a_{i_2} ... a_{i_n} $$ which gives  $$ \text{RHS} = \Delta(a) \sum_{n=0}^N (N-n)!\sigma_n(a)  $$","in my endeavors I've stumbled upon the following identity: $$ \prod_{i=1}^N \left ( 1 + \frac{\partial}{\partial a_i}\right ) \left (   \Delta(a) \prod_{i=1}^N a_i \right ) = \Delta(a) \int_0^\infty dt e^{-t} \prod_{i=1}^N (t+a_i)  $$ with Vandermonde determinant $\Delta (a) = \det \left ( a_j^{i-1} \right )_{i,j=1...N}$. I find it surprisingly hard to prove, maybe someone knows this formula or could suggest any way to derive it? I've tried to utilize Schur polynomials as the LHS is the $s_{(1_N)}$ but to no avail. EDIT:  My attempt of a proof started from the RHS where I use the generating functional of elementary symmetric functions: $$ \prod_{i=1}^N (t + a_i ) = \sum_{n=0}^N \sigma_n (a) t^{N-n} $$ with elementary symmetric functions defined as $$ \sigma_n(a) = \sum_{1\leq i_1 < i_2 < ... < i_n \leq N} a_{i_1} a_{i_2} ... a_{i_n} $$ which gives  $$ \text{RHS} = \Delta(a) \sum_{n=0}^N (N-n)!\sigma_n(a)  $$",,"['linear-algebra', 'multivariable-calculus', 'random-matrices']"
88,When does a generating set for a free module contain a basis?,When does a generating set for a free module contain a basis?,,"Let $A$ be a commutative ring with identity and $M$ a free $A$-module. Now suppose that $T$ generates $M$. When does $T$ contain a basis of $M$? It partly seems like a ""dual"" to the following result: Let $A$ be a commutative ring with identity and $M$ a free $A$-module. If $N$ is a free submodule of $M$, then the basis of $N$ can be extended to a basis of $M$ if and only if $M/N$, the quotient module, is free. So I'm guessing that we can obtain a similar result, that is, a necessary and sufficient condition under which the question is true.","Let $A$ be a commutative ring with identity and $M$ a free $A$-module. Now suppose that $T$ generates $M$. When does $T$ contain a basis of $M$? It partly seems like a ""dual"" to the following result: Let $A$ be a commutative ring with identity and $M$ a free $A$-module. If $N$ is a free submodule of $M$, then the basis of $N$ can be extended to a basis of $M$ if and only if $M/N$, the quotient module, is free. So I'm guessing that we can obtain a similar result, that is, a necessary and sufficient condition under which the question is true.",,"['linear-algebra', 'commutative-algebra', 'modules']"
89,Minimizing the Schatten 1-norm over symmetric matrices.,Minimizing the Schatten 1-norm over symmetric matrices.,,"Let $X$ be an $n \times n$ Hermitian matrix, it follows that we can write $X=A+iB$ where $A$ is symmetric and $B$ is skew-symmetric. Let $S$ be the set of $n \times n$ symmetric matrices and define the Schatten $1$-norm (also known as trace norm) as $\lVert M \rVert_1=\sum_j \sigma_j(M)$, where $ \sigma_j(M)$ are the singular values of $M$. To measure how close $X$ is to the subspace of symmetric the matrices, we can define the distance measure $$ D(X) = \min_{Y\in S} \lVert X-Y \rVert_1 .$$ Expanding $X$ into symmetric and skew-symmetric parts, we get that $$D(X)= \min_{Y\in S} \lVert (A-Y)+iB \rVert_1$$ From this it seems intuitively that the minimum occurs when $Y=A$, in which case $D(X)=\lVert B\rVert_1$ is simply the $1$-norm of the imaginary part of $X$. How would one show that this quantity is minimized for $A=Y$?","Let $X$ be an $n \times n$ Hermitian matrix, it follows that we can write $X=A+iB$ where $A$ is symmetric and $B$ is skew-symmetric. Let $S$ be the set of $n \times n$ symmetric matrices and define the Schatten $1$-norm (also known as trace norm) as $\lVert M \rVert_1=\sum_j \sigma_j(M)$, where $ \sigma_j(M)$ are the singular values of $M$. To measure how close $X$ is to the subspace of symmetric the matrices, we can define the distance measure $$ D(X) = \min_{Y\in S} \lVert X-Y \rVert_1 .$$ Expanding $X$ into symmetric and skew-symmetric parts, we get that $$D(X)= \min_{Y\in S} \lVert (A-Y)+iB \rVert_1$$ From this it seems intuitively that the minimum occurs when $Y=A$, in which case $D(X)=\lVert B\rVert_1$ is simply the $1$-norm of the imaginary part of $X$. How would one show that this quantity is minimized for $A=Y$?",,"['linear-algebra', 'matrices', 'functional-analysis', 'normed-spaces', 'spectral-theory']"
90,$\dim V = \dim V/U+\dim U$,,\dim V = \dim V/U+\dim U,"Is the following Proof Correct? Theorem. Given that $U$ is a subspace of $V$ and $\{v_1+U,v_2+U,\ldots,v_m+U\}$ is a basis for $V/U$ and $\{u_1,u_2,...,u_n\}$ is a basis for $U$ prove that $\{v_1,v_2,\ldots,v_m,u_1,u_2,\ldots,u_n\}$ is a basis for $V$ . Proof. We know that the dimension of a quotient space is determined as follows $$\dim V/U = \dim V-\dim U$$ which implies that $\dim V = \dim V/U+\dim U$ therefore the list $v_1,v_2,\ldots v_m,u_1,u_2,\ldots,u_n$ is of the right length i.e. $m+n$ . To establish that the list is a basis we need only prove that either the list is linearly independent or spans $V$ , we choose the latter option. Let $v\in V$ be arbitrary, evidently $v+U\in V/U$ and therefore for some $\alpha_1,\alpha_2,\ldots,\alpha_m\in\mathbf{F}$ $$v+U = \sum_{j=1}^{m}\alpha_j(v_j+U) = \left(\sum_{j=1}^{m}\alpha_jv_j\right)+U$$ which may be equivalently stated as follows $$v-\left(\sum_{j=1}^{m}\alpha_jv_j\right)\in U$$ and consequently for some $\beta_1,\beta_2,\ldots,\beta_n\in\mathbf{F}$ we have $$v-\left(\sum_{j=1}^{m}\alpha_jv_j\right) = \sum_{i=1}^{n}\beta_i u_i$$ which implies that $$v = \sum_{j=1}^{m}\alpha_jv_j+\sum_{i=1}^{n}\beta_i u_i$$ since $v$ was arbitrary it follows that $\operatorname{span}\{v_1,v_2,\ldots,v_m,u_1,u_2,\ldots,u_n\} = V$ . $\blacksquare$","Is the following Proof Correct? Theorem. Given that is a subspace of and is a basis for and is a basis for prove that is a basis for . Proof. We know that the dimension of a quotient space is determined as follows which implies that therefore the list is of the right length i.e. . To establish that the list is a basis we need only prove that either the list is linearly independent or spans , we choose the latter option. Let be arbitrary, evidently and therefore for some which may be equivalently stated as follows and consequently for some we have which implies that since was arbitrary it follows that .","U V \{v_1+U,v_2+U,\ldots,v_m+U\} V/U \{u_1,u_2,...,u_n\} U \{v_1,v_2,\ldots,v_m,u_1,u_2,\ldots,u_n\} V \dim V/U = \dim V-\dim U \dim V = \dim V/U+\dim U v_1,v_2,\ldots v_m,u_1,u_2,\ldots,u_n m+n V v\in V v+U\in V/U \alpha_1,\alpha_2,\ldots,\alpha_m\in\mathbf{F} v+U = \sum_{j=1}^{m}\alpha_j(v_j+U) = \left(\sum_{j=1}^{m}\alpha_jv_j\right)+U v-\left(\sum_{j=1}^{m}\alpha_jv_j\right)\in U \beta_1,\beta_2,\ldots,\beta_n\in\mathbf{F} v-\left(\sum_{j=1}^{m}\alpha_jv_j\right) = \sum_{i=1}^{n}\beta_i u_i v = \sum_{j=1}^{m}\alpha_jv_j+\sum_{i=1}^{n}\beta_i u_i v \operatorname{span}\{v_1,v_2,\ldots,v_m,u_1,u_2,\ldots,u_n\} = V \blacksquare","['linear-algebra', 'vector-spaces', 'solution-verification']"
91,Dimension of $\mathbb{Q}$-vector space if a nonsingular linear transformation $T$ exists such that $T^{-1} = T^{2} + T$,Dimension of -vector space if a nonsingular linear transformation  exists such that,\mathbb{Q} T T^{-1} = T^{2} + T,"We're given $V$ a finite dimensional vector space over $\mathbb{Q}$, $T$ a non-singular linear transformation of $V$ such that $T^{-1} = T^{2} + T$. The question has two parts. If I understand part (a), I should be able to get part (b), so right now I'm looking for a hint on part (a): (a) Prove that the dimension of $V$ is divisible by 3. (b) Prove that if the dimension is exactly 3, then all such transformations $T$ are similar. I'm trying to think of $T$ as a matrix. I can get $v = A^{3}v + A^{2}v$ for any $v$, but I don't know where to go from there.","We're given $V$ a finite dimensional vector space over $\mathbb{Q}$, $T$ a non-singular linear transformation of $V$ such that $T^{-1} = T^{2} + T$. The question has two parts. If I understand part (a), I should be able to get part (b), so right now I'm looking for a hint on part (a): (a) Prove that the dimension of $V$ is divisible by 3. (b) Prove that if the dimension is exactly 3, then all such transformations $T$ are similar. I'm trying to think of $T$ as a matrix. I can get $v = A^{3}v + A^{2}v$ for any $v$, but I don't know where to go from there.",,"['linear-algebra', 'abstract-algebra', 'linear-transformations']"
92,Describing the image and nullspace of a linear map,Describing the image and nullspace of a linear map,,"Let $V$ be the space of all continuous functions on $[a,b$]. Define the map $T$ according to $$T[f](x)=\int_a^b f(t) \sin(x-t) \mathrm{d} t. $$ I want describe the image and kernel (nullspace) of this map.  In order to do this, I've first noticed that $$T[f](x)=\left(-\int_a^b f(t) \sin(t) \mathrm{d} t \right) \cos(x)+\left( \int_a^b f(t) \cos(t) \right) \sin(x) .$$ This means that the image $T[V]$ is a subspace of the space spanned by $\{ \cos(x),\sin(x)\}$. The kernel consists of all functions $f$ such that $$\int_a^b f(t) \sin(t) \mathrm{d}t=\int_a^b f(t) \cos(t) \mathrm{d} t=0. $$ I'd like some help on showing that $T[V]=\operatorname{span} \{\cos(x),\sin(x)\}$, and perhaps getting a clearer characterization of the kernel. Notice that if $[a,b]=[-\pi,\pi]$ we can use the orthogonality of $\sin$ and $\cos$, but I'm interested in a general interval. Thanks!","Let $V$ be the space of all continuous functions on $[a,b$]. Define the map $T$ according to $$T[f](x)=\int_a^b f(t) \sin(x-t) \mathrm{d} t. $$ I want describe the image and kernel (nullspace) of this map.  In order to do this, I've first noticed that $$T[f](x)=\left(-\int_a^b f(t) \sin(t) \mathrm{d} t \right) \cos(x)+\left( \int_a^b f(t) \cos(t) \right) \sin(x) .$$ This means that the image $T[V]$ is a subspace of the space spanned by $\{ \cos(x),\sin(x)\}$. The kernel consists of all functions $f$ such that $$\int_a^b f(t) \sin(t) \mathrm{d}t=\int_a^b f(t) \cos(t) \mathrm{d} t=0. $$ I'd like some help on showing that $T[V]=\operatorname{span} \{\cos(x),\sin(x)\}$, and perhaps getting a clearer characterization of the kernel. Notice that if $[a,b]=[-\pi,\pi]$ we can use the orthogonality of $\sin$ and $\cos$, but I'm interested in a general interval. Thanks!",,"['linear-algebra', 'linear-transformations']"
93,Finding eigenvector of $ \left[ \begin{smallmatrix} 0 & B \\ B & 0 \end{smallmatrix} \right] $,Finding eigenvector of, \left[ \begin{smallmatrix} 0 & B \\ B & 0 \end{smallmatrix} \right] ,"$A \in M^{2n\times 2n}(\mathbb{R})$ $$A = \left[  \begin{matrix}   0 &  B \\   B & 0   \end{matrix} \right] $$ where $B$ is symmetric matrix   ($B^T=B$). Is it possible to find a pair $(\lambda, x)$ where $\lambda$ is an   eigenvalue and $x$ is an eigenvector and $|\lambda|$ is highest among   all eigenvalues. If you can use only power iteration? If it is possible then how you should do it? I think that it is not possible.  Here is the intuition I have: Let's suppose that we can solve the task. Then we can take arbitrary $x_0$ and  build a sequence of $x_i = A^ix_0$. However the structure of A implicates that if $x_0=[x_0^1, x_0^2]$ then every iteration swap $x_0^1$ and $x_0^2$, so the sequences  $B^nx_0^1$  and $B^nx_0^2$ should have the same limit, however since $x_0$ is totally arbitrary vector then only possible limit is $0$ which is not an eigenvector. Am I right and how to prove it properly?","$A \in M^{2n\times 2n}(\mathbb{R})$ $$A = \left[  \begin{matrix}   0 &  B \\   B & 0   \end{matrix} \right] $$ where $B$ is symmetric matrix   ($B^T=B$). Is it possible to find a pair $(\lambda, x)$ where $\lambda$ is an   eigenvalue and $x$ is an eigenvector and $|\lambda|$ is highest among   all eigenvalues. If you can use only power iteration? If it is possible then how you should do it? I think that it is not possible.  Here is the intuition I have: Let's suppose that we can solve the task. Then we can take arbitrary $x_0$ and  build a sequence of $x_i = A^ix_0$. However the structure of A implicates that if $x_0=[x_0^1, x_0^2]$ then every iteration swap $x_0^1$ and $x_0^2$, so the sequences  $B^nx_0^1$  and $B^nx_0^2$ should have the same limit, however since $x_0$ is totally arbitrary vector then only possible limit is $0$ which is not an eigenvector. Am I right and how to prove it properly?",,"['linear-algebra', 'numerical-methods', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
94,Covariance matrix $P$ for an Extended Kalman Filter not symmetric,Covariance matrix  for an Extended Kalman Filter not symmetric,P,"I am trying to implement an Extended Kalman Filter (EKF) and it is becoming harder than I thought. I have one question. I noticed that the covariance matrix which should get updated over each iteration is not symmetric. I am debugging through MATLAB.  I know that P should be symmetric and stay symmetric. What does it mean if the covariance matrix is not symmetric? Where it could be the error? EDIT: Any covariance matrix MUST be symmetric no matter what. The symmetry comes from its definition. The covariance tells you how two variables are related and therefore if $x$ is related to $y$ also $y$ will be related to $x$ and of course in the same way. My problem was that I was checking with the MATLAB function issymmetric(A) if the matrix $A$ was symmetric.  Apparently that function checks for exact symmetry and therefore if the involved matrixes are computed numerically and therefore it is not exactly symmetric it will give you false / 0 as a result. But, if for the same matrix I checked A-A' I had something of the order of 1e-15 . Thanks in advance.","I am trying to implement an Extended Kalman Filter (EKF) and it is becoming harder than I thought. I have one question. I noticed that the covariance matrix which should get updated over each iteration is not symmetric. I am debugging through MATLAB.  I know that P should be symmetric and stay symmetric. What does it mean if the covariance matrix is not symmetric? Where it could be the error? EDIT: Any covariance matrix MUST be symmetric no matter what. The symmetry comes from its definition. The covariance tells you how two variables are related and therefore if $x$ is related to $y$ also $y$ will be related to $x$ and of course in the same way. My problem was that I was checking with the MATLAB function issymmetric(A) if the matrix $A$ was symmetric.  Apparently that function checks for exact symmetry and therefore if the involved matrixes are computed numerically and therefore it is not exactly symmetric it will give you false / 0 as a result. But, if for the same matrix I checked A-A' I had something of the order of 1e-15 . Thanks in advance.",,"['linear-algebra', 'matlab', 'covariance', 'kalman-filter']"
95,Antisymmetric matrix operating on $\mathbb R_{\ge 0}^n$,Antisymmetric matrix operating on,\mathbb R_{\ge 0}^n,"While looking at something related to game theory, I came across this problem. Given an antisymmetric matrix $\mathbf A$, show that there is a vector $\mathbf t \ne \mathbf 0$ with only nonnegative entries such that $\mathbf{At}$ has only nonpositive entries. I've managed to prove some things about $\mathbf t$. In particular, for all $i$, at most one of $t_i$ and $[At]_i$ can be nonzero. However, I can't seem to prove that $\mathbf t$ necessarily exists. Any tips on how I might prove this? Or, for that matter, is there a counterexample?","While looking at something related to game theory, I came across this problem. Given an antisymmetric matrix $\mathbf A$, show that there is a vector $\mathbf t \ne \mathbf 0$ with only nonnegative entries such that $\mathbf{At}$ has only nonpositive entries. I've managed to prove some things about $\mathbf t$. In particular, for all $i$, at most one of $t_i$ and $[At]_i$ can be nonzero. However, I can't seem to prove that $\mathbf t$ necessarily exists. Any tips on how I might prove this? Or, for that matter, is there a counterexample?",,['linear-algebra']
96,A Fibonacci identity and $f(Av)=\det(A)f(v)$,A Fibonacci identity and,f(Av)=\det(A)f(v),"Given a diagonalizable linear transformation $A:V\to V$ over an algebraically closed field $k$, and an eigenbasis $v_1,\ldots v_n$ with $Av_i=\lambda_i v_i$, consider the following procedure to define a function $f_A$. Given a vector $v\in V$, express it as as $v=\sum a_i v_i$.  Let $f_A(v)=\prod a_i$.  This satisfies the nice property that $f_A(Av)=\det(A)f_A(v)$. I came across this construction when trying to find invariants for generalized Fibonacci sequences.  In particular, define $G_0=a, G_1=b, G_{n+1}=G_n+G_{n-1}$ for $n>1$.  Applying this construction to the equation $$ \pmatrix{1 & 1\\ 1 & 0}^n\pmatrix{b \\ a} =\pmatrix{G_n \\ G_{n-1}}$$ yields that $G_{n}G_{n+2}-G_{n+1}^2=(-1)^n(G_0G_2-G_1^2)=(-1)^n(a(a+b)-b^2)$.  In particular, $|G_{n}G_{n+2}-G_{n+1}^2|$ is independent of $n$, so if one wanted to determine whether $(c,d)=(G_n,G_{n+1})$ for some $n$, then a necessary condition is that $c(c+d)-d^2=(-1)^n(a(a+b)-b^2)$. This identity is similar to but more general than the classical identity $F_{n-1}F_{n+1}-F_n^2=(-1)^n$ My question: Has this construction been studied before? Is there a cleaner approach to it?  Does it generalize?  Are there other, better ways to generate functions that satisfy $f(Av)=\det(A)f(v)$? Update: The construction can be rephrased (up to a constant) as $\det(M(v))$ where $M(v)$ is the matrix whose columns are the $A$-eignevectors which sum to $v$.  The property $f(Av)=\det(A)f(v)$ comes from the property that $M(Av)=AM(v)$.  Therefore, this question could be answered by a classification of the maps $V\to \hom(V,V)$ which commute with $A$. I feel like this kind of tensor construction has a better chance of having been studied.","Given a diagonalizable linear transformation $A:V\to V$ over an algebraically closed field $k$, and an eigenbasis $v_1,\ldots v_n$ with $Av_i=\lambda_i v_i$, consider the following procedure to define a function $f_A$. Given a vector $v\in V$, express it as as $v=\sum a_i v_i$.  Let $f_A(v)=\prod a_i$.  This satisfies the nice property that $f_A(Av)=\det(A)f_A(v)$. I came across this construction when trying to find invariants for generalized Fibonacci sequences.  In particular, define $G_0=a, G_1=b, G_{n+1}=G_n+G_{n-1}$ for $n>1$.  Applying this construction to the equation $$ \pmatrix{1 & 1\\ 1 & 0}^n\pmatrix{b \\ a} =\pmatrix{G_n \\ G_{n-1}}$$ yields that $G_{n}G_{n+2}-G_{n+1}^2=(-1)^n(G_0G_2-G_1^2)=(-1)^n(a(a+b)-b^2)$.  In particular, $|G_{n}G_{n+2}-G_{n+1}^2|$ is independent of $n$, so if one wanted to determine whether $(c,d)=(G_n,G_{n+1})$ for some $n$, then a necessary condition is that $c(c+d)-d^2=(-1)^n(a(a+b)-b^2)$. This identity is similar to but more general than the classical identity $F_{n-1}F_{n+1}-F_n^2=(-1)^n$ My question: Has this construction been studied before? Is there a cleaner approach to it?  Does it generalize?  Are there other, better ways to generate functions that satisfy $f(Av)=\det(A)f(v)$? Update: The construction can be rephrased (up to a constant) as $\det(M(v))$ where $M(v)$ is the matrix whose columns are the $A$-eignevectors which sum to $v$.  The property $f(Av)=\det(A)f(v)$ comes from the property that $M(Av)=AM(v)$.  Therefore, this question could be answered by a classification of the maps $V\to \hom(V,V)$ which commute with $A$. I feel like this kind of tensor construction has a better chance of having been studied.",,"['linear-algebra', 'reference-request', 'tensor-products']"
97,Bound on $x^TAx$ for SPD matrix $A$ in terms of its main diagonal,Bound on  for SPD matrix  in terms of its main diagonal,x^TAx A,"Let $A\in\mathbb{R}^{n\times n}$ be a symmetric positive definite matrix and $x\in\mathbb{R}^n$ some vector. I want to find a bound of the form $$x^T Ax \leq c \sum_{i=1}^n a_{ii} x_i^2$$ with a constant $c>0$ . If $\lambda_\text{min}$ and $\lambda_\text{max}$ denote the smallest and largest eigenvalue of $A$ (both are positive due to spd assumption), then one bound that holds is $$x^T Ax \leq \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n a_{ii} x_i^2.$$ Proof: $$x^TAx \leq \lambda_\text{max} \Vert x\Vert^2 = \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n \lambda_\text{min} x_i^2 \leq \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n a_{ii} x_i^2$$ where we used that $a_{ii} \geq \lambda_\text{min}$ for all $i$ . However, I think there should be a tighter bound. For example, the inequality holds with $c=1$ if $A$ is diagonal. Any ideas?","Let be a symmetric positive definite matrix and some vector. I want to find a bound of the form with a constant . If and denote the smallest and largest eigenvalue of (both are positive due to spd assumption), then one bound that holds is Proof: where we used that for all . However, I think there should be a tighter bound. For example, the inequality holds with if is diagonal. Any ideas?",A\in\mathbb{R}^{n\times n} x\in\mathbb{R}^n x^T Ax \leq c \sum_{i=1}^n a_{ii} x_i^2 c>0 \lambda_\text{min} \lambda_\text{max} A x^T Ax \leq \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n a_{ii} x_i^2. x^TAx \leq \lambda_\text{max} \Vert x\Vert^2 = \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n \lambda_\text{min} x_i^2 \leq \frac{\lambda_\text{max}}{\lambda_\text{min}} \sum_{i=1}^n a_{ii} x_i^2 a_{ii} \geq \lambda_\text{min} i c=1 A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'symmetric-matrices']"
98,"Let $U$ and $V$ be vector spaces, and $T_1$ and $T_2$ be linear maps from $U$ to $V$ and $V$ to $U$ respectively, and are onto maps.","Let  and  be vector spaces, and  and  be linear maps from  to  and  to  respectively, and are onto maps.",U V T_1 T_2 U V V U,"Let $U$ and $V$ be vector spaces, and $T_1$ and $T_2$ be linear maps from $U$ to $V$ and $V$ to $U$ respectively, and are onto maps. Are $U$ and $V$ isomorphic? If both space are finite dimensional then they are isomorphic. But in other cases? I think they are not isomorphic. But then I am finding it hard to find a counterexample. Any suggestion?","Let $U$ and $V$ be vector spaces, and $T_1$ and $T_2$ be linear maps from $U$ to $V$ and $V$ to $U$ respectively, and are onto maps. Are $U$ and $V$ isomorphic? If both space are finite dimensional then they are isomorphic. But in other cases? I think they are not isomorphic. But then I am finding it hard to find a counterexample. Any suggestion?",,['linear-algebra']
99,"Finding $n\dim B$ linearly independent invertible matrices in $M(n,B)$?",Finding  linearly independent invertible matrices in ?,"n\dim B M(n,B)","I'm trying to solve the following exercise: If $A$ is a finite-dimensional $k$-algebra, then a minimal representation of $A$ is a representation of minimal $k$-dimension. Show that if $B$ is a finite-dimensional division algebra over $k$ and $A=M(n,B)$, then the action of $A$ on $B^n$ is a minimal representation. To show this, we need to show that if we have any representation $A\to\text{End}(V)$ for a $k$-space $V$, then $\dim V\ge n\dim B$. I will write this map as $E\mapsto\varphi_E$ for $E\in A$. Now, if I suppose that $A$ has $nm$ linearly independent invertible matrices $E_1,\dots,E_{nm}$, where $m=\dim B$ then I will be done. By taking some nonzero $v\in V$, I let $v_i=\varphi_{E_i}(v)$ then this is nonzero since $\varphi_{E_i}$ is invertible (since $E_i$ is), and I can show that $\{v_1,\dots,v_{nm}\}$ is linearly independent using the linear independence of $E_1,\dots,E_{nm}$ in $A$. So my problem just lies now in finding $nm$ linearly independent, invertible elements of $A$ (the latter of which is the same as having $\det E\neq0$ since $B$ is a division algebra). Does anybody have a suggestion on how to obtain these?","I'm trying to solve the following exercise: If $A$ is a finite-dimensional $k$-algebra, then a minimal representation of $A$ is a representation of minimal $k$-dimension. Show that if $B$ is a finite-dimensional division algebra over $k$ and $A=M(n,B)$, then the action of $A$ on $B^n$ is a minimal representation. To show this, we need to show that if we have any representation $A\to\text{End}(V)$ for a $k$-space $V$, then $\dim V\ge n\dim B$. I will write this map as $E\mapsto\varphi_E$ for $E\in A$. Now, if I suppose that $A$ has $nm$ linearly independent invertible matrices $E_1,\dots,E_{nm}$, where $m=\dim B$ then I will be done. By taking some nonzero $v\in V$, I let $v_i=\varphi_{E_i}(v)$ then this is nonzero since $\varphi_{E_i}$ is invertible (since $E_i$ is), and I can show that $\{v_1,\dots,v_{nm}\}$ is linearly independent using the linear independence of $E_1,\dots,E_{nm}$ in $A$. So my problem just lies now in finding $nm$ linearly independent, invertible elements of $A$ (the latter of which is the same as having $\det E\neq0$ since $B$ is a division algebra). Does anybody have a suggestion on how to obtain these?",,"['linear-algebra', 'abstract-algebra', 'representation-theory']"
