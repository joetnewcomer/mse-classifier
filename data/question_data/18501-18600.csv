,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Fast algorithm for solving system of linear equations,Fast algorithm for solving system of linear equations,,"I have a system of $N$ linear equations, $Ax=b$, in $N$ unknowns (where $N$ is large). If I am interested in the solution for only one of the unknowns, what are the best approaches? For example, assume $N=50,000$. We want the solution for $x_1$ through $x_{100}$ only. Is there any trick that does not require $O(n^{3})$ (or $O$(matrix inversion))?","I have a system of $N$ linear equations, $Ax=b$, in $N$ unknowns (where $N$ is large). If I am interested in the solution for only one of the unknowns, what are the best approaches? For example, assume $N=50,000$. We want the solution for $x_1$ through $x_{100}$ only. Is there any trick that does not require $O(n^{3})$ (or $O$(matrix inversion))?",,"['linear-algebra', 'systems-of-equations', 'numerical-linear-algebra']"
1,Quick way to check if a matrix is diagonalizable.,Quick way to check if a matrix is diagonalizable.,,"Is there any quick way to check whether a matrix is diagonalizable or not? In exam if a question is asked like ""Which of the following matrix is diagonalizable?"" and four options are given then how can one check it quickly? I hope my question makes sense.","Is there any quick way to check whether a matrix is diagonalizable or not? In exam if a question is asked like ""Which of the following matrix is diagonalizable?"" and four options are given then how can one check it quickly? I hope my question makes sense.",,"['linear-algebra', 'diagonalization']"
2,Probability that a random binary matrix is invertible? [duplicate],Probability that a random binary matrix is invertible? [duplicate],,"This question already has an answer here : Probability of a random $n \times n$ matrix over $\mathbb F_2$ being nonsingular (1 answer) Closed 4 years ago . What is the probability that a random $\{0,1\}$, $n \times n$ matrix is invertible? Assume the 0 and 1 are each present in an entry with probability $\frac{1}{2}$. Is there an explicit formula as a function of $n$?  Does it tend to 1 as $n$ grows large? I'm sure this is all known... Thanks!","This question already has an answer here : Probability of a random $n \times n$ matrix over $\mathbb F_2$ being nonsingular (1 answer) Closed 4 years ago . What is the probability that a random $\{0,1\}$, $n \times n$ matrix is invertible? Assume the 0 and 1 are each present in an entry with probability $\frac{1}{2}$. Is there an explicit formula as a function of $n$?  Does it tend to 1 as $n$ grows large? I'm sure this is all known... Thanks!",,"['linear-algebra', 'probability', 'matrices', 'random-matrices']"
3,Order of matrices in $GL_2(\mathbb{Z})$,Order of matrices in,GL_2(\mathbb{Z}),"Let $A\in GL_2\left(\mathbb{Z}\right)$, the group of invertible matrices with integer coefficients, and denote by $\omega(A)$ the order of $A$. How we prove that $$\left\{\omega(A);A\in GL_2\left(\mathbb{Z}\right)\right\}=\{1,2,3,4,6,\infty\}.$$","Let $A\in GL_2\left(\mathbb{Z}\right)$, the group of invertible matrices with integer coefficients, and denote by $\omega(A)$ the order of $A$. How we prove that $$\left\{\omega(A);A\in GL_2\left(\mathbb{Z}\right)\right\}=\{1,2,3,4,6,\infty\}.$$",,['linear-algebra']
4,"Reverse engineering ""nice"" matrices from the eigenvalues","Reverse engineering ""nice"" matrices from the eigenvalues",,"Before beginning, I should note some similarities to the question: Building matrices from eigenvalues .  However, I have a different objective. The Setup When writing problems for an introductory linear algebra course, a generic question would be Find the eigenvalues of $A= \begin{bmatrix} \dots \end{bmatrix}$ . From an instructor perspective, it is generally desirable that the computation be reasonably straightforward (e.g., not tasking students with finding roots of an irreducible quintic). Furthermore, the goal of the question is whether students can correctly find the characteristic polynomial and solve for the roots, rather than the ability to carefully add numerous fractions. As such, matrices $A$ with integer entries are generally preferable, perhaps along with a bias towards positive entries that aren't too large. However, we don't want to make the problem too easy. For example, tasking students with finding the eigenvalues of a diagonal/triangular matrix doesn't make for a particularly good question (unless it is specifically a question of recognizing this property). As such, it is probably best to write such a question by starting with the eigenvalues and working backwards to generate a matrix $A$ with those eigenvalues. These thoughts lead me to an (admittedly vague) definition: A "" nifty matrix "" $A$ should Have a given set of eigenvalues $\{\lambda_1, \dots, \lambda_n\}$ . Contain only non-zero integers: $A = (a_{ij}) \in \{\mathbb{Z}\setminus\{0\}\}^{n \times n}$ Note that our conditions necessitate that the eigenvalues be algebraic integers and that the list of eigenvalues must contain all of the Galois conjugates. For example, $$ D= \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}, \quad T= \begin{bmatrix} 3 & 7 \\ 0 & 2 \end{bmatrix}, \quad \text{ and } A=\begin{bmatrix} 1 & 1 \\ -2 & 4 \end{bmatrix} $$ all have the same eigenvalues, but only $A$ is ""nifty matrix"" for the eigenvalues $\{3,2\}$ . Similarly, $$A= \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}$$ is a ""nifty matrix"" for the eigenvalues $\lambda_\pm = \frac{3 \pm \sqrt{5}}{2}$ . The Goal Imagine that you're stuck in a textbook factory and need to write hundreds of ""compute the eigenvalues"" problems. How would you set about your task? Since you are picking the eigenvalues at the start, you can also control the characteristic and minimal polynomials of $A$ , $\chi_A$ and $\mu_A$ . The way I envision this working is a function / algorithm that gives a (perhaps non-unique) mapping $$\{\text{eigenvalues & min. or char. poly} \} \longrightarrow \text{nifty matrix}. $$ A simple approach Start with a list of integer eigenvalues. Form a diagonal matrix (or Jordan block matrix or upper-triangular matrix) $M$ that has the desired eigenvalues. Build a matrix $P$ by doing some row operations on the identity matrix, without rescaling rows or adding non-integer copies of rows. By Cramer's rule, $P^{-1}$ will only have integer entries as well. Form $A= P M P^{-1}$ . See how we did. Keep trying different $P$ matrices until something works. A slightly more sophisticated approach. This approach has the benefit of starting with the characteristic polynomial, which allows you to generate nifty matrices for eigenvalues that aren't necessarily integers (unlike the above approach). Pick a suitable characteristic polynomial $\chi$ and minimal polynomial $\mu$ , each with integer coefficients. Build a matrix $C$ that will be the rational canonical form of $A$ using the characteristic polynomial and minimal polynomial. Note that $C$ will have integer entries. Use our trick from the previous algorithm: $A=P C P^{-1}$ where $\det(P)= \pm 1$ . See how we did. Keep trying different $P$ matrices until something works. The Question Is there a better way? Note that both of these algorithms are ill-defined--how do we find a $P$ that ensures $A$ has no zero entries? As an extension of this problem, suppose we defined a super nifty matrix to be a nifty matrix  that minimizes $f(A) = \max_{i,j} \{|a_{ij|} \}$ over all similar nifty matrices? How could we find a (probably non-unique) minimizer? What if we minimized with respect to $g(A) = \sum_{i,j} |a_{ij}|$ or any other suitable norm on the entries? What if we decided that negative numbers are too messy and wanted $A=(a_{ij}) \in \mathbb{N}^{n\times n}$ ? What conditions would this impose on the eigenvalues and our algorithms? Edited to Add: I should note that such ""nifty matrices"" truly aren't nice problems, outside of the $2 \times 2$ case. This is part of why I didn't call them ""nice matrices,"" but rather ""nifty"" as in ""Hey! This messy looking matrix of integers has my desired eigenvalues/ characteristic polynomial! Isn't that nifty?!"" I am primarily interested in this problem as a mathematical construct; generating a clever linear algebra problem is only a motivating idea. For another, perhaps more ""realistic"" motivation, you can recast the problem as: Given a list of $n$ suitable eigenvalues, is it possible to created a complete graph on $n$ vertices such that each edge has a non-zero (possibly negative) integer weight and the weighted transition matrix has the desired eigenvalues? What is the ""simplest"" such graph? How can you algorithmically construct this graph / weighted transition matrix?","Before beginning, I should note some similarities to the question: Building matrices from eigenvalues .  However, I have a different objective. The Setup When writing problems for an introductory linear algebra course, a generic question would be Find the eigenvalues of . From an instructor perspective, it is generally desirable that the computation be reasonably straightforward (e.g., not tasking students with finding roots of an irreducible quintic). Furthermore, the goal of the question is whether students can correctly find the characteristic polynomial and solve for the roots, rather than the ability to carefully add numerous fractions. As such, matrices with integer entries are generally preferable, perhaps along with a bias towards positive entries that aren't too large. However, we don't want to make the problem too easy. For example, tasking students with finding the eigenvalues of a diagonal/triangular matrix doesn't make for a particularly good question (unless it is specifically a question of recognizing this property). As such, it is probably best to write such a question by starting with the eigenvalues and working backwards to generate a matrix with those eigenvalues. These thoughts lead me to an (admittedly vague) definition: A "" nifty matrix "" should Have a given set of eigenvalues . Contain only non-zero integers: Note that our conditions necessitate that the eigenvalues be algebraic integers and that the list of eigenvalues must contain all of the Galois conjugates. For example, all have the same eigenvalues, but only is ""nifty matrix"" for the eigenvalues . Similarly, is a ""nifty matrix"" for the eigenvalues . The Goal Imagine that you're stuck in a textbook factory and need to write hundreds of ""compute the eigenvalues"" problems. How would you set about your task? Since you are picking the eigenvalues at the start, you can also control the characteristic and minimal polynomials of , and . The way I envision this working is a function / algorithm that gives a (perhaps non-unique) mapping A simple approach Start with a list of integer eigenvalues. Form a diagonal matrix (or Jordan block matrix or upper-triangular matrix) that has the desired eigenvalues. Build a matrix by doing some row operations on the identity matrix, without rescaling rows or adding non-integer copies of rows. By Cramer's rule, will only have integer entries as well. Form . See how we did. Keep trying different matrices until something works. A slightly more sophisticated approach. This approach has the benefit of starting with the characteristic polynomial, which allows you to generate nifty matrices for eigenvalues that aren't necessarily integers (unlike the above approach). Pick a suitable characteristic polynomial and minimal polynomial , each with integer coefficients. Build a matrix that will be the rational canonical form of using the characteristic polynomial and minimal polynomial. Note that will have integer entries. Use our trick from the previous algorithm: where . See how we did. Keep trying different matrices until something works. The Question Is there a better way? Note that both of these algorithms are ill-defined--how do we find a that ensures has no zero entries? As an extension of this problem, suppose we defined a super nifty matrix to be a nifty matrix  that minimizes over all similar nifty matrices? How could we find a (probably non-unique) minimizer? What if we minimized with respect to or any other suitable norm on the entries? What if we decided that negative numbers are too messy and wanted ? What conditions would this impose on the eigenvalues and our algorithms? Edited to Add: I should note that such ""nifty matrices"" truly aren't nice problems, outside of the case. This is part of why I didn't call them ""nice matrices,"" but rather ""nifty"" as in ""Hey! This messy looking matrix of integers has my desired eigenvalues/ characteristic polynomial! Isn't that nifty?!"" I am primarily interested in this problem as a mathematical construct; generating a clever linear algebra problem is only a motivating idea. For another, perhaps more ""realistic"" motivation, you can recast the problem as: Given a list of suitable eigenvalues, is it possible to created a complete graph on vertices such that each edge has a non-zero (possibly negative) integer weight and the weighted transition matrix has the desired eigenvalues? What is the ""simplest"" such graph? How can you algorithmically construct this graph / weighted transition matrix?","A= \begin{bmatrix} \dots \end{bmatrix} A A A \{\lambda_1, \dots, \lambda_n\} A = (a_{ij}) \in \{\mathbb{Z}\setminus\{0\}\}^{n \times n}  D= \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}, \quad T= \begin{bmatrix} 3 & 7 \\ 0 & 2 \end{bmatrix}, \quad \text{ and } A=\begin{bmatrix} 1 & 1 \\ -2 & 4 \end{bmatrix}  A \{3,2\} A= \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix} \lambda_\pm = \frac{3 \pm \sqrt{5}}{2} A \chi_A \mu_A \{\text{eigenvalues & min. or char. poly} \} \longrightarrow \text{nifty matrix}.  M P P^{-1} A= P M P^{-1} P \chi \mu C A C A=P C P^{-1} \det(P)= \pm 1 P P A f(A) = \max_{i,j} \{|a_{ij|} \} g(A) = \sum_{i,j} |a_{ij}| A=(a_{ij}) \in \mathbb{N}^{n\times n} 2 \times 2 n n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
5,Eigenvalues and eigenvectors of Hadamard product of two positive definite matrices,Eigenvalues and eigenvectors of Hadamard product of two positive definite matrices,,"The component-wise product (Hadamard product) of two positive definite matrices is a positive definite matrix ( Schur product theorem ). I encountered the following proof of it: $A=(a_{ij})$ and $B=(b_{ij})$ are positive definite. Let $a_{ij} = \displaystyle \sum_{k=1}^N \lambda_k t_{ik} t_{jk}$ where $T=(t_{ij})$ is an orthogonal matrix and $\lambda_k$ are the eigenvalues of $A$. $$\sum_{i,j=1}^N a_{ij} b_{ij} x_i x_j = \sum_{i,j=1}^N b_{ij} x_i x_j \left( \sum_{k=1}^N \lambda_k t_{ik} t_{jk} \right) = \sum_{k=1}^N \lambda_k \left( \sum_{i,j=1}^N b_{ij} x_i t_{ik} x_j t_{jk} \right)$$ and so then showing $$\sum_{i,j=1}^N b_{ij} x_i t_{ik} x_j t_{jk} >0$$ is enough. Is there a simple geometrical relationship between the eigenvalues and eigenvectors of $C=(a_{ij} b_{ij})$ and the eigenvalues and eigenvectors of $A$ and $B$ that explains the geometrical motivation behind this proof?","The component-wise product (Hadamard product) of two positive definite matrices is a positive definite matrix ( Schur product theorem ). I encountered the following proof of it: $A=(a_{ij})$ and $B=(b_{ij})$ are positive definite. Let $a_{ij} = \displaystyle \sum_{k=1}^N \lambda_k t_{ik} t_{jk}$ where $T=(t_{ij})$ is an orthogonal matrix and $\lambda_k$ are the eigenvalues of $A$. $$\sum_{i,j=1}^N a_{ij} b_{ij} x_i x_j = \sum_{i,j=1}^N b_{ij} x_i x_j \left( \sum_{k=1}^N \lambda_k t_{ik} t_{jk} \right) = \sum_{k=1}^N \lambda_k \left( \sum_{i,j=1}^N b_{ij} x_i t_{ik} x_j t_{jk} \right)$$ and so then showing $$\sum_{i,j=1}^N b_{ij} x_i t_{ik} x_j t_{jk} >0$$ is enough. Is there a simple geometrical relationship between the eigenvalues and eigenvectors of $C=(a_{ij} b_{ij})$ and the eigenvalues and eigenvectors of $A$ and $B$ that explains the geometrical motivation behind this proof?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'hadamard-product']"
6,Inverse of a Toeplitz matrix,Inverse of a Toeplitz matrix,,"A Toeplitz matrix or diagonal-constant matrix is a matrix in which each descending diagonal from left to right is constant. For instance, the following matrix is an $n\times n$ Toeplitz matrix: $$ A = \begin{bmatrix} a_{0} & a_{-1} & a_{-2} & \ldots & \ldots &a_{-n+1} \\\  a_{1} & a_0 & a_{-1} & \ddots & & \vdots \\ a_{2} & a_{1} & \ddots & \ddots & \ddots& \vdots \\\ \vdots & \ddots & \ddots & \ddots & a_{-1} & a_{-2}\\\ \vdots & & \ddots & a_{1} & a_{0}& a_{-1} \\ a_{n-1} & \ldots & \ldots & a_{2} & a_{1} & a_{0} \end{bmatrix}  $$ I'm interested in the self-adjoint case ( $a_{-k}=a_{k}\in\mathbb{R}$ ). My questions are: Is there a relatively simple criterion to know when these matrices are invertible by just analyzing the sequence $\{a_{0},\ldots,a_{n-1}\}$ ? In the invertible case, what is known about its inverse? About its determinant? Thanks!","A Toeplitz matrix or diagonal-constant matrix is a matrix in which each descending diagonal from left to right is constant. For instance, the following matrix is an Toeplitz matrix: I'm interested in the self-adjoint case ( ). My questions are: Is there a relatively simple criterion to know when these matrices are invertible by just analyzing the sequence ? In the invertible case, what is known about its inverse? About its determinant? Thanks!","n\times n 
A = \begin{bmatrix} a_{0} & a_{-1} & a_{-2} & \ldots & \ldots &a_{-n+1} \\\ 
a_{1} & a_0 & a_{-1} & \ddots & & \vdots \\ a_{2} & a_{1} & \ddots & \ddots & \ddots& \vdots \\\
\vdots & \ddots & \ddots & \ddots & a_{-1} & a_{-2}\\\
\vdots & & \ddots & a_{1} & a_{0}& a_{-1} \\ a_{n-1} & \ldots & \ldots & a_{2} & a_{1} & a_{0} \end{bmatrix} 
 a_{-k}=a_{k}\in\mathbb{R} \{a_{0},\ldots,a_{n-1}\}","['linear-algebra', 'matrices', 'determinant', 'inverse', 'toeplitz-matrices']"
7,Description of Levi factors and unipotent radicals of parabolic subgroups in classical groups,Description of Levi factors and unipotent radicals of parabolic subgroups in classical groups,,"For an algebraic group $G$ over an algebraically closed field $k$, a parabolic subgroup $P$ has factorization $P = Q \rtimes L$, where $Q$ is the unipotent radical of $P$ and $L$ is some Levi factor of $P$. If $G = GL(V)$ with $\dim V = n$, then $$P = \{ \pmatrix{A_{n_1} &  & & * \\  & A_{n_2} & & \\  &  & \ddots & \\ 0 &  & & A_{n_t} } \}$$ for some $n = n_1 + \cdots + n_t$ and $$Q = \{ \pmatrix{I_{n_1} &  & & * \\  & I_{n_2} & & \\  &  & \ddots & \\ 0 &  & & I_{n_t} } \}$$ and for example $$L = \{ \pmatrix{A_{n_1} &  & & 0 \\  & A_{n_2} & & \\  &  & \ddots & \\ 0 &  & & A_{n_t} } \}$$ So $L \cong GL_{n_1} \times GL_{n_2} \times \cdots \times GL_{n_t}$. What about when $G = Sp(V)$ or $G = SO(V)$ in characteristic $p \neq 2$? Then $P$ is a stabilizer of a flag of totally singular subspaces, how does one describe $Q$ and $L$? Is there a good reference for this?","For an algebraic group $G$ over an algebraically closed field $k$, a parabolic subgroup $P$ has factorization $P = Q \rtimes L$, where $Q$ is the unipotent radical of $P$ and $L$ is some Levi factor of $P$. If $G = GL(V)$ with $\dim V = n$, then $$P = \{ \pmatrix{A_{n_1} &  & & * \\  & A_{n_2} & & \\  &  & \ddots & \\ 0 &  & & A_{n_t} } \}$$ for some $n = n_1 + \cdots + n_t$ and $$Q = \{ \pmatrix{I_{n_1} &  & & * \\  & I_{n_2} & & \\  &  & \ddots & \\ 0 &  & & I_{n_t} } \}$$ and for example $$L = \{ \pmatrix{A_{n_1} &  & & 0 \\  & A_{n_2} & & \\  &  & \ddots & \\ 0 &  & & A_{n_t} } \}$$ So $L \cong GL_{n_1} \times GL_{n_2} \times \cdots \times GL_{n_t}$. What about when $G = Sp(V)$ or $G = SO(V)$ in characteristic $p \neq 2$? Then $P$ is a stabilizer of a flag of totally singular subspaces, how does one describe $Q$ and $L$? Is there a good reference for this?",,"['linear-algebra', 'group-theory', 'lie-algebras', 'algebraic-groups']"
8,Prove that if $AB$ is invertible then $B$ is invertible.,Prove that if  is invertible then  is invertible.,AB B,I know this proof is short but a bit tricky. So I suppose that $AB$ is invertible then $(AB)^{-1}$ exists. We also know $(AB)^{-1}=B^{-1}A^{-1}$. If we let $C=(B^{-1}A^{-1}A)$ then by the invertible matrix theorem we see that since $CA=I$(left inverse) then $B$ is invertible. Would this be correct? Edit Suppose $AB$ is invertible. There exists a matrix call it $X$ such that $XAB=I$. Let $C=XA$ Then $CB=I$ and it follows that $B$ is invertible by the invertible matrix theorem.,I know this proof is short but a bit tricky. So I suppose that $AB$ is invertible then $(AB)^{-1}$ exists. We also know $(AB)^{-1}=B^{-1}A^{-1}$. If we let $C=(B^{-1}A^{-1}A)$ then by the invertible matrix theorem we see that since $CA=I$(left inverse) then $B$ is invertible. Would this be correct? Edit Suppose $AB$ is invertible. There exists a matrix call it $X$ such that $XAB=I$. Let $C=XA$ Then $CB=I$ and it follows that $B$ is invertible by the invertible matrix theorem.,,"['linear-algebra', 'matrices', 'inverse']"
9,"How to prove $1$,$\sqrt{2},\sqrt{3}$ and $\sqrt{6}$ are linearly independent over $\mathbb{Q}$?","How to prove , and  are linearly independent over ?","1 \sqrt{2},\sqrt{3} \sqrt{6} \mathbb{Q}","How do I prove that $1$,$\sqrt{2},\sqrt{3}$ and $\sqrt{6}$ are linearly independent over $\mathbb{Q}$? $\mathbb{Q}$ is the rational field. I want to know the detail about the proof. Thanks in advance. Actually I know any two of them and three of them are linearly independent.","How do I prove that $1$,$\sqrt{2},\sqrt{3}$ and $\sqrt{6}$ are linearly independent over $\mathbb{Q}$? $\mathbb{Q}$ is the rational field. I want to know the detail about the proof. Thanks in advance. Actually I know any two of them and three of them are linearly independent.",,"['linear-algebra', 'abstract-algebra', 'radicals']"
10,There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent.,There exist an infinite subset  such that any three vectors in  are linearly independent.,S\subseteq\mathbb{R}^3 S,Could anyone just give me hint for this one? There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent. True or false?,Could anyone just give me hint for this one? There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent. True or false?,,['linear-algebra']
11,How would you explain a tensor to a computer scientist?,How would you explain a tensor to a computer scientist?,,"How would you explain a tensor to a computer scientist? My friend, who studies computer science, recently asked me what a tensor was. I study physics, and I tried my best to explain what a tensor is, and I said something along the lines of ""a mathematical object that is described in between the mappings of vector spaces"", and he wasn't quite about that definition. I understood why, since it is a pretty wordy definition I gave, so I decided to give a more, down to earth, definition, describing a tensor as some array, with n-dimensions. However, he was still kind of confused by this. Can anyone synthesise a decent definition, tailored to a computer scientist's understanding?","How would you explain a tensor to a computer scientist? My friend, who studies computer science, recently asked me what a tensor was. I study physics, and I tried my best to explain what a tensor is, and I said something along the lines of ""a mathematical object that is described in between the mappings of vector spaces"", and he wasn't quite about that definition. I understood why, since it is a pretty wordy definition I gave, so I decided to give a more, down to earth, definition, describing a tensor as some array, with n-dimensions. However, he was still kind of confused by this. Can anyone synthesise a decent definition, tailored to a computer scientist's understanding?",,"['linear-algebra', 'vector-analysis', 'tensors']"
12,How to compute the determinant of a tridiagonal Toeplitz matrix?,How to compute the determinant of a tridiagonal Toeplitz matrix?,,"How to show that the determinant of the following $(n\times n)$ matrix $$\begin{pmatrix} 5 & 2 & 0 & 0 & 0 & \cdots & 0 \\ 2 & 5 & 2 & 0 & 0 & \cdots & 0 \\ 0 & 2 & 5 & 2 & 0 & \cdots & 0 \\ \vdots & \vdots& \vdots& \vdots & \vdots & \vdots & \vdots \\ 0 & \cdots & \cdots & 0 & 2 & 5 & 2 \\ 0 & \cdots & \cdots & \cdots & \cdots & 2 & 5 \end{pmatrix}$$ is equal to $\frac13(4^{n+1}-1)$? More generally: How does one compute the determinant of the following tridiagonal matrix (where the three diagonals are constant)? $$M_n(a,b,c) = \begin{pmatrix} a & b & 0 & 0 & 0 & \cdots & 0 \\ c & a & b & 0 & 0 & \cdots & 0 \\ 0 & c & a & b & 0 & \cdots & 0 \\ \vdots & \vdots& \vdots& \vdots & \vdots& \vdots & \vdots \\ 0 & \cdots & \cdots & 0 & c & a & b \\ 0 & \cdots & \cdots & \cdots & \cdots & c & a \end{pmatrix}$$ Here $a,b,c$ can be taken to be real numbers, or complex numbers. In other words, the matrix $M_n(a,b,c) = (m_{ij})_{1 \le i,j \le n}$ is such that $$m_{ij} = \begin{cases} a & i = j, \\ b & i = j - 1, \\ c & i = j + 1, \\ 0 & \text{otherwise.} \end{cases}$$ There does not seem to be an easy pattern to use induction: the matrix is not a diagonal block matrix of the type $M = \bigl(\begin{smallmatrix} A & C \\ 0 & B \end{smallmatrix}\bigr)$ (where we could use $\det(M) = \det(A) \det(B)$ for the induction step), and there are no lines or columns with only one nonzero entry, so Laplace expansion gets complicated quickly. Is there a general pattern that one could use? Or is the answer only known on a case-by-case basis? It's possible to compute the determinant by hand for small $n$: $$\begin{align} \det(M_1(a,b,c)) & = \begin{vmatrix} a \end{vmatrix} = a  \\ \det(M_2(a,b,c)) & = \begin{vmatrix} a & b \\ c & a \end{vmatrix} = a^2 - bc \\ \det(M_3(a,b,c)) & = \begin{vmatrix} a & b & 0 \\ c & a & b \\ 0 & c & a \end{vmatrix} = a^3 - 2abc \end{align}$$ But there is no readily apparent pattern and the computation becomes very difficult when $n$ gets large.","How to show that the determinant of the following $(n\times n)$ matrix $$\begin{pmatrix} 5 & 2 & 0 & 0 & 0 & \cdots & 0 \\ 2 & 5 & 2 & 0 & 0 & \cdots & 0 \\ 0 & 2 & 5 & 2 & 0 & \cdots & 0 \\ \vdots & \vdots& \vdots& \vdots & \vdots & \vdots & \vdots \\ 0 & \cdots & \cdots & 0 & 2 & 5 & 2 \\ 0 & \cdots & \cdots & \cdots & \cdots & 2 & 5 \end{pmatrix}$$ is equal to $\frac13(4^{n+1}-1)$? More generally: How does one compute the determinant of the following tridiagonal matrix (where the three diagonals are constant)? $$M_n(a,b,c) = \begin{pmatrix} a & b & 0 & 0 & 0 & \cdots & 0 \\ c & a & b & 0 & 0 & \cdots & 0 \\ 0 & c & a & b & 0 & \cdots & 0 \\ \vdots & \vdots& \vdots& \vdots & \vdots& \vdots & \vdots \\ 0 & \cdots & \cdots & 0 & c & a & b \\ 0 & \cdots & \cdots & \cdots & \cdots & c & a \end{pmatrix}$$ Here $a,b,c$ can be taken to be real numbers, or complex numbers. In other words, the matrix $M_n(a,b,c) = (m_{ij})_{1 \le i,j \le n}$ is such that $$m_{ij} = \begin{cases} a & i = j, \\ b & i = j - 1, \\ c & i = j + 1, \\ 0 & \text{otherwise.} \end{cases}$$ There does not seem to be an easy pattern to use induction: the matrix is not a diagonal block matrix of the type $M = \bigl(\begin{smallmatrix} A & C \\ 0 & B \end{smallmatrix}\bigr)$ (where we could use $\det(M) = \det(A) \det(B)$ for the induction step), and there are no lines or columns with only one nonzero entry, so Laplace expansion gets complicated quickly. Is there a general pattern that one could use? Or is the answer only known on a case-by-case basis? It's possible to compute the determinant by hand for small $n$: $$\begin{align} \det(M_1(a,b,c)) & = \begin{vmatrix} a \end{vmatrix} = a  \\ \det(M_2(a,b,c)) & = \begin{vmatrix} a & b \\ c & a \end{vmatrix} = a^2 - bc \\ \det(M_3(a,b,c)) & = \begin{vmatrix} a & b & 0 \\ c & a & b \\ 0 & c & a \end{vmatrix} = a^3 - 2abc \end{align}$$ But there is no readily apparent pattern and the computation becomes very difficult when $n$ gets large.",,"['linear-algebra', 'matrices', 'determinant', 'toeplitz-matrices', 'faq']"
13,"The lore of the game Numenera mentions ""an irrational number that may be a four-dimensional equivalent of $\pi$"". What could this mean?","The lore of the game Numenera mentions ""an irrational number that may be a four-dimensional equivalent of "". What could this mean?",\pi,"There's an RPG (Role-Playing Game) called Numenera , set on the Earth a billion years in the future, which is covered in the partially-functional and generally weird and mysterious technological ruins of a billion years of advanced civilizations. In one of its sourcebooks, it lists possible fragmentary transmissions that a player character might receive from its global ""datasphere"", one of which is listed as the following: An irrational number that may be a four-dimensional equivalent of $\pi$ . When I saw this, my first thought was ""I'm pretty sure that's probably just pi multiplied by a constant"", followed by ""If it's not, I'm sure mathematicians have already worked it out."" When I did I Google search, I couldn't find anything obvious. So, what is the equivalent to pi for a four-dimensional hypersphere?","There's an RPG (Role-Playing Game) called Numenera , set on the Earth a billion years in the future, which is covered in the partially-functional and generally weird and mysterious technological ruins of a billion years of advanced civilizations. In one of its sourcebooks, it lists possible fragmentary transmissions that a player character might receive from its global ""datasphere"", one of which is listed as the following: An irrational number that may be a four-dimensional equivalent of . When I saw this, my first thought was ""I'm pretty sure that's probably just pi multiplied by a constant"", followed by ""If it's not, I'm sure mathematicians have already worked it out."" When I did I Google search, I couldn't find anything obvious. So, what is the equivalent to pi for a four-dimensional hypersphere?",\pi,"['linear-algebra', 'geometry', 'pi']"
14,Practical uses of matrix multiplication,Practical uses of matrix multiplication,,"Usually, the use of matrix multiplication is initially given with graphics — scalings, translations, rotations, etc. Then, there are more in-depth examples such as counting the number of walks between nodes in a graph using the power of the graph's adjacency matrix. What are other good examples of using matrix multiplication in various contexts?","Usually, the use of matrix multiplication is initially given with graphics — scalings, translations, rotations, etc. Then, there are more in-depth examples such as counting the number of walks between nodes in a graph using the power of the graph's adjacency matrix. What are other good examples of using matrix multiplication in various contexts?",,"['linear-algebra', 'matrices', 'intuition', 'big-list', 'education']"
15,Why are infinite-dimensional vector spaces usually equipped with additional structure?,Why are infinite-dimensional vector spaces usually equipped with additional structure?,,"In a first course in linear algebra, it is common for instructors to mostly restrict their attention to finite-dimensional vector spaces. These vector spaces are usually not assumed to be equipped with any additional structure, such as an inner product, norm, or a topology. On the other hand, it seems that when infinite-dimensional vector spaces are encountered in later courses, it is much more common to equip with them additional structure. Why is this? A partial answer might be that infinite-dimensional vector spaces are often studied in functional analysis, where extra structure is needed to properly define analytical concepts such as infinite series. However, I would have expected that ""pure"" infinite-dimensional vector spaces have a use in some area of mathematics. I have now also asked this on MathOverflow .","In a first course in linear algebra, it is common for instructors to mostly restrict their attention to finite-dimensional vector spaces. These vector spaces are usually not assumed to be equipped with any additional structure, such as an inner product, norm, or a topology. On the other hand, it seems that when infinite-dimensional vector spaces are encountered in later courses, it is much more common to equip with them additional structure. Why is this? A partial answer might be that infinite-dimensional vector spaces are often studied in functional analysis, where extra structure is needed to properly define analytical concepts such as infinite series. However, I would have expected that ""pure"" infinite-dimensional vector spaces have a use in some area of mathematics. I have now also asked this on MathOverflow .",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'soft-question']"
16,Dimension of the sum of two vector subspaces,Dimension of the sum of two vector subspaces,,"$\dim(U_1+U_2) = \dim U_1 +\dim U_2 - \dim(U_1\cap U_2).$ I want to make sure that my intuition is correct. Suppose we have two planes $U_1,U_2$ though the origin in $\mathbb{R^3}$. Since the planes meet at the origin, they also intersect, which in this case is a one-dimensional line in $\mathbb{R^3}$. To obtain the dimension of $U_1$ and $U_2$, we add the dimensions of the planes (4), and the subtract the dimensions of the line (1), which results in (3). *additional question(s): Can we generalize this notion to $\mathbb{F^{n}}$? Suppose we have an additional case where $U_1$ and $U_2$ are planes in $\mathbb{R^3}$, but $U_1 \subseteq U_2$. In this instance, $dim(U_1 + U_2) < 3$, because the first two-dimensional plane is contained in the second and as a result, the dimensions of the subspaces when summed cannot exceed two. Since both subspaces $U_1,U_2$ are two dimensional and $U_1 \subseteq U_2$, then their intersection is also two-dimensional, concluding $dim(U_1+U_2)=2+2-2 = 2$. Is this proper intuition?","$\dim(U_1+U_2) = \dim U_1 +\dim U_2 - \dim(U_1\cap U_2).$ I want to make sure that my intuition is correct. Suppose we have two planes $U_1,U_2$ though the origin in $\mathbb{R^3}$. Since the planes meet at the origin, they also intersect, which in this case is a one-dimensional line in $\mathbb{R^3}$. To obtain the dimension of $U_1$ and $U_2$, we add the dimensions of the planes (4), and the subtract the dimensions of the line (1), which results in (3). *additional question(s): Can we generalize this notion to $\mathbb{F^{n}}$? Suppose we have an additional case where $U_1$ and $U_2$ are planes in $\mathbb{R^3}$, but $U_1 \subseteq U_2$. In this instance, $dim(U_1 + U_2) < 3$, because the first two-dimensional plane is contained in the second and as a result, the dimensions of the subspaces when summed cannot exceed two. Since both subspaces $U_1,U_2$ are two dimensional and $U_1 \subseteq U_2$, then their intersection is also two-dimensional, concluding $dim(U_1+U_2)=2+2-2 = 2$. Is this proper intuition?",,['linear-algebra']
17,What is the intuitive interpretation of the transpose compared to the inverse?,What is the intuitive interpretation of the transpose compared to the inverse?,,"I've been thinking about this question already for a long time and I've just encountered it again in the following lemma: $$f(x) = g(Ax + b) \implies \nabla f = A^T \nabla g(Ax + b) $$ This lemma makes intuitive sense if you think of it as taking the $x$ to the space $Ax$, calculating the gradient and then taking the result back to the original space. But why is ""taking the result back"" realised as $A^T$ and not $A^{-1}$? By doing the calculations you get $A^T$, no doubt, but I always expect an inverse. In general, when should I expect a transpose and when an inverse? Where are they similar and where do they differ?","I've been thinking about this question already for a long time and I've just encountered it again in the following lemma: $$f(x) = g(Ax + b) \implies \nabla f = A^T \nabla g(Ax + b) $$ This lemma makes intuitive sense if you think of it as taking the $x$ to the space $Ax$, calculating the gradient and then taking the result back to the original space. But why is ""taking the result back"" realised as $A^T$ and not $A^{-1}$? By doing the calculations you get $A^T$, no doubt, but I always expect an inverse. In general, when should I expect a transpose and when an inverse? Where are they similar and where do they differ?",,"['linear-algebra', 'matrices', 'inverse', 'intuition', 'transpose']"
18,Understanding the properties and use of the Laplacian matrix (and its norm),Understanding the properties and use of the Laplacian matrix (and its norm),,"I am reading the wikipedia article on the Laplacian matrix: https://en.wikipedia.org/wiki/Laplacian_matrix I don't understand what is the particular use of it; having the diagonals as the degree and why the negative adjacency elements off the diagonal? What use would this have? Then on reading about its norm, first of all what does a norm really mean? And what is the norm for the Laplacian matrix delivering? This norm does not result in a matrix whose terms cancel out or sum to one. Or that the determinant is equal to any consistent value. Any insight? Best,","I am reading the wikipedia article on the Laplacian matrix: https://en.wikipedia.org/wiki/Laplacian_matrix I don't understand what is the particular use of it; having the diagonals as the degree and why the negative adjacency elements off the diagonal? What use would this have? Then on reading about its norm, first of all what does a norm really mean? And what is the norm for the Laplacian matrix delivering? This norm does not result in a matrix whose terms cancel out or sum to one. Or that the determinant is equal to any consistent value. Any insight? Best,",,"['linear-algebra', 'graph-theory', 'normed-spaces']"
19,Intuition on spectral theorem,Intuition on spectral theorem,,"In the last month I studied the spectral theorems and I formally understood them. But I would like some intuition about them. If you didn’t know spectral theorems, how would you come up with the idea that symmetric/normal endomorphisms are the only orthogonally diagonalizable endomorphisms in the real/complex case. How would you even come up with the idea of studying the adjoint?","In the last month I studied the spectral theorems and I formally understood them. But I would like some intuition about them. If you didn’t know spectral theorems, how would you come up with the idea that symmetric/normal endomorphisms are the only orthogonally diagonalizable endomorphisms in the real/complex case. How would you even come up with the idea of studying the adjoint?",,"['linear-algebra', 'intuition', 'spectral-theory', 'diagonalization']"
20,Understanding the difference between Span and Basis,Understanding the difference between Span and Basis,,"I've been reading a bit around MSE and I've stumbled upon some similar questions as mine. However, most of them do not have a concrete explanation to what I'm looking for. I understand that the Span of a Vector Space $V$ is the linear combination of all the vectors in $V$. I also understand that the Basis of a Vector Space V is a set of vectors ${v_{1}, v_{2}, ..., v_{n}}$ which is linearly independent and whose span is all of $V$. Now, from my understanding the basis is a combination of vectors which are linearly independent, for example, $(1,0)$ and $(0,1)$. But why? The other question I have is, what do they mean by ""whose span is all of $V$"" ? On a final note, I would really appreciate a good definition of Span and Basis along with a concrete example of each which will really help to reinforce my understanding. Thanks.","I've been reading a bit around MSE and I've stumbled upon some similar questions as mine. However, most of them do not have a concrete explanation to what I'm looking for. I understand that the Span of a Vector Space $V$ is the linear combination of all the vectors in $V$. I also understand that the Basis of a Vector Space V is a set of vectors ${v_{1}, v_{2}, ..., v_{n}}$ which is linearly independent and whose span is all of $V$. Now, from my understanding the basis is a combination of vectors which are linearly independent, for example, $(1,0)$ and $(0,1)$. But why? The other question I have is, what do they mean by ""whose span is all of $V$"" ? On a final note, I would really appreciate a good definition of Span and Basis along with a concrete example of each which will really help to reinforce my understanding. Thanks.",,"['linear-algebra', 'vector-spaces', 'span']"
21,Why does polynomial factorization generalize to matrices,Why does polynomial factorization generalize to matrices,,"I'm reading about linear algebra and I came across with the following theorem where I have a problem convincing myself: Theorem 2.1 $\,$ Every linear operator on a finite-dimensional complex vector space has an eigenvalue. Proof: To show that $T$ (our linear operator on $V$) has an   eigenvalue, fix any nonzero vector $v \in V$. The vectors $v, Tv,  T^2v,..., T^nv$ cannot be linearly independent, because $V$ has   dimension $n$ and we have $n + 1$ vectors. Thus there exist complex   numbers $a_0,...,a_n$, not all $0$, such that $$a_0v + a_1Tv + ··· + a_nT^nv = 0.$$ Make the $a$’s the coefficients   of a polynomial, which can be written in factored form as $$a_0 + a_1z  + ··· + a_nz^n = c(z − r_1)\cdots(z − r_m),$$ where $c$ is a non-zero complex number, each $r_j$ is complex, and the equation holds for all   complex $z$. We then have $${\color{red}{ 0=(a_0I + a_1T + ··· + a_nT^n)v= c(T − r_1I)\cdots(T −  r_mI)v}},$$ which means that $T − r_j$ I is not injective for at least   one $j$. In other words, $T$ has an eigenvalue.$\;\blacksquare$ I'm having trouble with the factorized form of the matrix polynomial (in red). I understood that the factorization holds for a polynomial by the fundamental theorem of algebra but why does this also hold for matrices? In other words, why is the the part I highlighted true? Does such an factorization always exist? Could I have some help to see this? Thank you =) P.S. here is my reference (page 3). UPDATE: Someone else also has asked the same question before it seems.","I'm reading about linear algebra and I came across with the following theorem where I have a problem convincing myself: Theorem 2.1 $\,$ Every linear operator on a finite-dimensional complex vector space has an eigenvalue. Proof: To show that $T$ (our linear operator on $V$) has an   eigenvalue, fix any nonzero vector $v \in V$. The vectors $v, Tv,  T^2v,..., T^nv$ cannot be linearly independent, because $V$ has   dimension $n$ and we have $n + 1$ vectors. Thus there exist complex   numbers $a_0,...,a_n$, not all $0$, such that $$a_0v + a_1Tv + ··· + a_nT^nv = 0.$$ Make the $a$’s the coefficients   of a polynomial, which can be written in factored form as $$a_0 + a_1z  + ··· + a_nz^n = c(z − r_1)\cdots(z − r_m),$$ where $c$ is a non-zero complex number, each $r_j$ is complex, and the equation holds for all   complex $z$. We then have $${\color{red}{ 0=(a_0I + a_1T + ··· + a_nT^n)v= c(T − r_1I)\cdots(T −  r_mI)v}},$$ which means that $T − r_j$ I is not injective for at least   one $j$. In other words, $T$ has an eigenvalue.$\;\blacksquare$ I'm having trouble with the factorized form of the matrix polynomial (in red). I understood that the factorization holds for a polynomial by the fundamental theorem of algebra but why does this also hold for matrices? In other words, why is the the part I highlighted true? Does such an factorization always exist? Could I have some help to see this? Thank you =) P.S. here is my reference (page 3). UPDATE: Someone else also has asked the same question before it seems.",,['linear-algebra']
22,Why are fields with characteristic 2 so pathological?,Why are fields with characteristic 2 so pathological?,,"For example, over fields with characteristic 2, there exist nonzero symmetric nilpotent matrices, and nonzero matrices could be simultaneously symmetric and anti-symmetric. I wonder why characteristic 2 makes such fields so special.","For example, over fields with characteristic 2, there exist nonzero symmetric nilpotent matrices, and nonzero matrices could be simultaneously symmetric and anti-symmetric. I wonder why characteristic 2 makes such fields so special.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory']"
23,Calculating the number of operations in matrix multiplication,Calculating the number of operations in matrix multiplication,,Is there a formula to calculate the number of multiplications that take place when multiplying 2 matrices? For example $$\begin{pmatrix}1&2\\3&4\end{pmatrix} \times \begin{pmatrix}5&6\\7&8\end{pmatrix}  = \text{8 multiplications and 4 additions} $$,Is there a formula to calculate the number of multiplications that take place when multiplying 2 matrices? For example $$\begin{pmatrix}1&2\\3&4\end{pmatrix} \times \begin{pmatrix}5&6\\7&8\end{pmatrix}  = \text{8 multiplications and 4 additions} $$,,['linear-algebra']
24,Square root of Positive Definite Matrix,Square root of Positive Definite Matrix,,Let $A$ be an $n\times n$ positive definite matrix. Show that there exists a unique positive definite matrix $B$ such that $B^2=A$. I do know the existence. But what about the uniqueness? Would you help me out? Thank you.,Let $A$ be an $n\times n$ positive definite matrix. Show that there exists a unique positive definite matrix $B$ such that $B^2=A$. I do know the existence. But what about the uniqueness? Would you help me out? Thank you.,,"['linear-algebra', 'matrices', 'diagonalization']"
25,Is $AA^T$ a positive-definite symmetric matrix?,Is  a positive-definite symmetric matrix?,AA^T,Let $A\in M_{n\times n}(\mathbb{R})$ be a matrix. Is it true that $AA^{T}$ is positive-definite? Clearly $AA^{T}$ is symmetric. I have shown that a symmetric matrix $S\in M_{n\times n}(\mathbb{R})$ is positive-definite if and only if $S$ has only positive eigenvalues. Can this be helpful?,Let $A\in M_{n\times n}(\mathbb{R})$ be a matrix. Is it true that $AA^{T}$ is positive-definite? Clearly $AA^{T}$ is symmetric. I have shown that a symmetric matrix $S\in M_{n\times n}(\mathbb{R})$ is positive-definite if and only if $S$ has only positive eigenvalues. Can this be helpful?,,"['linear-algebra', 'matrices']"
26,Relation between determinant and matrix rank,Relation between determinant and matrix rank,,"Let $A$ a square matrix with the size of $n \times n$. I know that if the rank of the matrix is $<n$, then there must be a ""zeroes-line"", therefore $\det(A)=0$. What about $\text{rank}(A)=n$? Why does it imply $\det(A)\ne0$? Of course, there is no ""zeroes-line"", but that doesn't prove it yet. I've seen a proof in a book which does this conclusion immediately, but IMHO this alone, doesn't prove it. What's the missing part?","Let $A$ a square matrix with the size of $n \times n$. I know that if the rank of the matrix is $<n$, then there must be a ""zeroes-line"", therefore $\det(A)=0$. What about $\text{rank}(A)=n$? Why does it imply $\det(A)\ne0$? Of course, there is no ""zeroes-line"", but that doesn't prove it yet. I've seen a proof in a book which does this conclusion immediately, but IMHO this alone, doesn't prove it. What's the missing part?",,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
27,Book recommendations for linear algebra [duplicate],Book recommendations for linear algebra [duplicate],,"This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Closed 6 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I have been wanting to learn about linear algebra (specifically about vector spaces) for a long time, but I am not sure what book to buy, any suggestions?","This question already has answers here : Where to start learning Linear Algebra? [closed] (16 answers) Closed 6 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I have been wanting to learn about linear algebra (specifically about vector spaces) for a long time, but I am not sure what book to buy, any suggestions?",,"['linear-algebra', 'abstract-algebra']"
28,Rank-nullity theorem for free $\mathbb Z$-modules,Rank-nullity theorem for free -modules,\mathbb Z,"From linear algebra we know that given vector spaces $V$, $W$ over a field $k$ and a linear map $f\colon V\to W$ we have $$\dim V = \dim \operatorname{im} f + \dim \ker f.$$ Is this still true when we consider free $\mathbb Z$-modules (i.e. free abelian groups) instead of vector spaces? Given a homomorphism $f\colon G\to H$ between free $\mathbb Z$-modules, do we have $$\operatorname{rk}(G) = \operatorname{rk}\operatorname{im} f + \operatorname{rk}\ker f?$$ To provide some context: This question comes up when computing homology groups of free chain complexes, where we need to check if some generating set of a kernel is a basis. Using the rank nullity theorem for free $\mathbb Z$-modules is an appealing way to do this.","From linear algebra we know that given vector spaces $V$, $W$ over a field $k$ and a linear map $f\colon V\to W$ we have $$\dim V = \dim \operatorname{im} f + \dim \ker f.$$ Is this still true when we consider free $\mathbb Z$-modules (i.e. free abelian groups) instead of vector spaces? Given a homomorphism $f\colon G\to H$ between free $\mathbb Z$-modules, do we have $$\operatorname{rk}(G) = \operatorname{rk}\operatorname{im} f + \operatorname{rk}\ker f?$$ To provide some context: This question comes up when computing homology groups of free chain complexes, where we need to check if some generating set of a kernel is a basis. Using the rank nullity theorem for free $\mathbb Z$-modules is an appealing way to do this.",,"['linear-algebra', 'modules', 'homological-algebra', 'abelian-groups']"
29,Let $A$ and $B$ be $n \times n$ real matrices such that $AB=BA=0$ and $A+B$ is invertible,Let  and  be  real matrices such that  and  is invertible,A B n \times n AB=BA=0 A+B,I came across the following problem that says: Let $A$ and $B$ be $n \times n$ real matrices such that $AB=BA=0$ and $A+B$ is invertible.  Then how can I prove the following: rank $A$ + rank $B$ = $n$ nullity $A$ + nullity $B$ = $n$ $A-B$ is invertible. Can someone point me in the right direction? Thanks in advance for your time.,I came across the following problem that says: Let and be real matrices such that and is invertible.  Then how can I prove the following: rank + rank = nullity + nullity = is invertible. Can someone point me in the right direction? Thanks in advance for your time.,A B n \times n AB=BA=0 A+B A B n A B n A-B,"['linear-algebra', 'matrices']"
30,Cross product in $\mathbb R^n$,Cross product in,\mathbb R^n,I read that the cross product can't be generalized to $\mathbb R^n$. Then I found that in $n=7$ there is a Cross product: https://en.wikipedia.org/wiki/Seven-dimensional_cross_product Why is it not possible to define a cross product for other dimensions $ \ge 4$?,I read that the cross product can't be generalized to $\mathbb R^n$. Then I found that in $n=7$ there is a Cross product: https://en.wikipedia.org/wiki/Seven-dimensional_cross_product Why is it not possible to define a cross product for other dimensions $ \ge 4$?,,"['linear-algebra', 'vector-spaces', 'vectors', 'cross-product', 'binary-operations']"
31,Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$ [duplicate],Show  [duplicate],\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B),"This question already has answers here : Show that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$ (6 answers) Closed 2 years ago . Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$, where $A,B \in M_{m\times n}(\mathbb{F})$. I'm trying to think in terms of linear transformations. We can define $T_a, T_b:\mathbb{F}^n\rightarrow \mathbb{F}^m$. I know that $\dim_{\mathbb F}\operatorname{Im} T_a, \dim_{\mathbb F}\operatorname{Im} T_b \le m$. What should I do next?","This question already has answers here : Show that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$ (6 answers) Closed 2 years ago . Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$, where $A,B \in M_{m\times n}(\mathbb{F})$. I'm trying to think in terms of linear transformations. We can define $T_a, T_b:\mathbb{F}^n\rightarrow \mathbb{F}^m$. I know that $\dim_{\mathbb F}\operatorname{Im} T_a, \dim_{\mathbb F}\operatorname{Im} T_b \le m$. What should I do next?",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
32,How to self study Linear Algebra,How to self study Linear Algebra,,"I have no idea if this question is appropriate for this forum, but I hope you guys can overlook the fact that I asked it on a wrong forum (if I did) and still help me answer it (of course, if this is indeed the wrong forum for the type of question I'm about to ask, do please say so). I am a 16 year old guy who is passionate about physics and as a result wants to increase his knowledge in mathematics, the language of physics. I've read and heard a lot about Linear Algebra and how crucial it is to physics and I am deeply motivated to self study this intriguing part of mathematics. However, I have limited knowledge of maths. I (for example) know basic algebra, trig, diff/int calc and some analytical geometry, but I wouldn't say I master these subjects past the high school curriculum. Now my question is: Would you kind people say I am able to self study Linear algebra or is it too tough and/or does it require too much of a math background? And are there any good books out there for BEGINNERS in LA? I found this (free) e-book called: Elementary Linear Algebra by Kenneth Kuttler and another one called 'Linear Algebra: Theory and its applications' also by Kuttler? Are these any good? Or would you recommend other books? If you guys have any tips regarding books for LA but also tips in general, please, I'd appreciate them!","I have no idea if this question is appropriate for this forum, but I hope you guys can overlook the fact that I asked it on a wrong forum (if I did) and still help me answer it (of course, if this is indeed the wrong forum for the type of question I'm about to ask, do please say so). I am a 16 year old guy who is passionate about physics and as a result wants to increase his knowledge in mathematics, the language of physics. I've read and heard a lot about Linear Algebra and how crucial it is to physics and I am deeply motivated to self study this intriguing part of mathematics. However, I have limited knowledge of maths. I (for example) know basic algebra, trig, diff/int calc and some analytical geometry, but I wouldn't say I master these subjects past the high school curriculum. Now my question is: Would you kind people say I am able to self study Linear algebra or is it too tough and/or does it require too much of a math background? And are there any good books out there for BEGINNERS in LA? I found this (free) e-book called: Elementary Linear Algebra by Kenneth Kuttler and another one called 'Linear Algebra: Theory and its applications' also by Kuttler? Are these any good? Or would you recommend other books? If you guys have any tips regarding books for LA but also tips in general, please, I'd appreciate them!",,"['linear-algebra', 'reference-request', 'self-learning']"
33,Generalized Cross Product,Generalized Cross Product,,"I know that the cross product can be generalized as $$\text{cross}(x_0,...,x_{n-1})=\det\begin{vmatrix}&x_0&\\&x_1&\\&\vdots&\\e_1&\cdots&e_n\end{vmatrix}$$ where $e_i$ is the $i$'th standard unit vector. We have $n-1$ vectors in $n$-dimensional Euclidean Space, so there is a one-dimensional orthogonal complement to that set (if they are independent) and the cross product above gives a vector in that subspace. I also  know that the ""area""/""n-volume"" of an n-parallelopiped spanned by the vectors $v_1,...,v_n$ is given by $$\sqrt{\det A^TA}$$ where $A=\begin{bmatrix}v_0&\cdots&v_n\end{bmatrix}$. In three dimensions this reduces to $$\sqrt{\det\begin{bmatrix}a_0&a_1&a_2\\b_0&b_1&b_2\end{bmatrix}\begin{bmatrix}a_0&b_0\\a_1&b_1\\a_2&b_2\end{bmatrix}}=\sqrt{||a||^2||b||^2-(a\cdot b)^2}=||a\times b||$$ I am wondering if it is true in general that, taking the cross product as defined above, $$||\text{cross}(x_0,...,x_{n-1})||=\sqrt{\det A^TA}\;\;\;\;\;\; A=\begin{bmatrix}x_0&\cdots&x_{n-1}\end{bmatrix}$$ The algebra seems horrific but I can't find any nice way to prove (or disprove) it.","I know that the cross product can be generalized as $$\text{cross}(x_0,...,x_{n-1})=\det\begin{vmatrix}&x_0&\\&x_1&\\&\vdots&\\e_1&\cdots&e_n\end{vmatrix}$$ where $e_i$ is the $i$'th standard unit vector. We have $n-1$ vectors in $n$-dimensional Euclidean Space, so there is a one-dimensional orthogonal complement to that set (if they are independent) and the cross product above gives a vector in that subspace. I also  know that the ""area""/""n-volume"" of an n-parallelopiped spanned by the vectors $v_1,...,v_n$ is given by $$\sqrt{\det A^TA}$$ where $A=\begin{bmatrix}v_0&\cdots&v_n\end{bmatrix}$. In three dimensions this reduces to $$\sqrt{\det\begin{bmatrix}a_0&a_1&a_2\\b_0&b_1&b_2\end{bmatrix}\begin{bmatrix}a_0&b_0\\a_1&b_1\\a_2&b_2\end{bmatrix}}=\sqrt{||a||^2||b||^2-(a\cdot b)^2}=||a\times b||$$ I am wondering if it is true in general that, taking the cross product as defined above, $$||\text{cross}(x_0,...,x_{n-1})||=\sqrt{\det A^TA}\;\;\;\;\;\; A=\begin{bmatrix}x_0&\cdots&x_{n-1}\end{bmatrix}$$ The algebra seems horrific but I can't find any nice way to prove (or disprove) it.",,['linear-algebra']
34,How to find the eigenvalues of tridiagonal Toeplitz matrix?,How to find the eigenvalues of tridiagonal Toeplitz matrix?,,"Assume the tridiagonal matrix $T$ is in this form: $$ T = \begin{bmatrix} a       & c        &              &              &   &\\ b       & a        & c            &              &   &\\         & b        & a            & c            &   &\\         &          &              &\ddots        &   &\\         &          &              & b            & a & c\\         &          &              &              & b & a\\ \end{bmatrix} $$ we must show that its eigenvalues are of the form $$a + 2 \sqrt{bc} \, \cos \left( \frac{k \pi}{n+1} \right)$$ where $$a=qh^2−1, ~~ b=1- \frac{ph}{2}, ~~ c =1+\frac{ph}{2} , ~~q \leq 0.$$","Assume the tridiagonal matrix $T$ is in this form: $$ T = \begin{bmatrix} a       & c        &              &              &   &\\ b       & a        & c            &              &   &\\         & b        & a            & c            &   &\\         &          &              &\ddots        &   &\\         &          &              & b            & a & c\\         &          &              &              & b & a\\ \end{bmatrix} $$ we must show that its eigenvalues are of the form $$a + 2 \sqrt{bc} \, \cos \left( \frac{k \pi}{n+1} \right)$$ where $$a=qh^2−1, ~~ b=1- \frac{ph}{2}, ~~ c =1+\frac{ph}{2} , ~~q \leq 0.$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'toeplitz-matrices']"
35,Block diagonal matrix diagonalizable,Block diagonal matrix diagonalizable,,"I am trying to prove that: The matrix $C = \left(\begin{smallmatrix}A& 0\\0 & B\end{smallmatrix}\right)$ is diagonalizable, if only if $A$ and $B$ are diagonalizable. If $A\in GL(\mathbb{C}^n)$ and $B\in GL(\mathbb{C}^m)$ are diagonalizable, then is easy to check the $C\in GL(\mathbb{C}^{n+m})$ is diagonalizable. But if I suppose that $C$ is diagonalizable, then exists $S = [S_1, S_2, \ldots, S_{n+m}]$ , $S_i\in\mathbb{C}^{m+n}$ , such that $S^{-1}CS = \mbox{diag}(\lambda_i)$ . Now $CS_i = \lambda_iS_i$ , and  if $S_i = \left(\begin{smallmatrix}x_i\\y_i\end{smallmatrix}\right)$ , $x_i\in\mathbb{C}^n$ and $y_i\in\mathbb{C}^m$ , then $$Ax_i = \lambda_ix_i\quad\mbox{ and }\quad By_i = \lambda_iy_i.$$ So, if I can justify that $\{x_1,\ldots,x_{n+m}\}$ have exactly $n$ linear independent vectors and $\{y_1,\ldots,y_{n+m}\}$ have $m$ linear independent vectors, I will prove that $A$ and $B$ are diagonalizables, but I don't know how to prove that? Please, anyone have an idea? Thanks in advance.","I am trying to prove that: The matrix is diagonalizable, if only if and are diagonalizable. If and are diagonalizable, then is easy to check the is diagonalizable. But if I suppose that is diagonalizable, then exists , , such that . Now , and  if , and , then So, if I can justify that have exactly linear independent vectors and have linear independent vectors, I will prove that and are diagonalizables, but I don't know how to prove that? Please, anyone have an idea? Thanks in advance.","C = \left(\begin{smallmatrix}A& 0\\0 & B\end{smallmatrix}\right) A B A\in GL(\mathbb{C}^n) B\in GL(\mathbb{C}^m) C\in GL(\mathbb{C}^{n+m}) C S = [S_1, S_2, \ldots, S_{n+m}] S_i\in\mathbb{C}^{m+n} S^{-1}CS = \mbox{diag}(\lambda_i) CS_i = \lambda_iS_i S_i = \left(\begin{smallmatrix}x_i\\y_i\end{smallmatrix}\right) x_i\in\mathbb{C}^n y_i\in\mathbb{C}^m Ax_i = \lambda_ix_i\quad\mbox{ and }\quad By_i = \lambda_iy_i. \{x_1,\ldots,x_{n+m}\} n \{y_1,\ldots,y_{n+m}\} m A B","['linear-algebra', 'matrices', 'diagonalization', 'block-matrices']"
36,What is the dimension of a matrix?,What is the dimension of a matrix?,,"I have been under the impression that the dimension of a matrix is simply whatever dimension it lives in. More precisely, if a vector space contained the vectors $(v_1, v_2,...,v_n)$, where each vector contained $3$ components $(a,b,c)$ (for some $a$, $b$ and $c$), then its dimension would be $\Bbb R^3$. Recently I was told this is not true, and the dimension of this vector space would be $\Bbb R^n$. In other words, I was under the belief that the dimension is the number of elements that compose the vectors in our vector space, but the dimension is how many vectors the vector space contains?! Where am I going wrong?","I have been under the impression that the dimension of a matrix is simply whatever dimension it lives in. More precisely, if a vector space contained the vectors $(v_1, v_2,...,v_n)$, where each vector contained $3$ components $(a,b,c)$ (for some $a$, $b$ and $c$), then its dimension would be $\Bbb R^3$. Recently I was told this is not true, and the dimension of this vector space would be $\Bbb R^n$. In other words, I was under the belief that the dimension is the number of elements that compose the vectors in our vector space, but the dimension is how many vectors the vector space contains?! Where am I going wrong?",,"['linear-algebra', 'vector-spaces']"
37,The formal definition of “angle”,The formal definition of “angle”,,"My main question is about the definition of ""angle"". Many linear algebra textbooks define the angle between two vectors in terms of their inner product. I superficially understand that this corresponds to the law of cosine in Euclidean geometry. But since axiomatic geometry and analytic geometry seem to have completely different approaches in dealing with geometric concepts, I would like to know what exactly motivates this definition, and what are the results of this definition. First of all, in order for this definition to work, we need to define the sine and cosine function. Usually, these functions are first introduced and therefore defined and only defined within a purely geometric context. In particular, most elementary definitions of these two functions require a previous understanding of the idea of an angle, such as the definition using a unit circle. Therefore, my question is, how can we have a valid cosine function that works outside of axiomatic geometry before we can even define ""angle"" in $\mathbb{R}^n$ via the cosine function? I know cosine function can be defined via a series, but again, how do we justify this definition if we were to use it to define ""angle""? Secondly, using these definitions, along with some other definitions for elementary geometric concepts such as planes, can one prove everything that is provable via Euclid's axioms of geometry using only algebra in $\mathbb{R}^n$ without any of those axioms? If not, what other axioms must we include in order to do so? Thanks!","My main question is about the definition of ""angle"". Many linear algebra textbooks define the angle between two vectors in terms of their inner product. I superficially understand that this corresponds to the law of cosine in Euclidean geometry. But since axiomatic geometry and analytic geometry seem to have completely different approaches in dealing with geometric concepts, I would like to know what exactly motivates this definition, and what are the results of this definition. First of all, in order for this definition to work, we need to define the sine and cosine function. Usually, these functions are first introduced and therefore defined and only defined within a purely geometric context. In particular, most elementary definitions of these two functions require a previous understanding of the idea of an angle, such as the definition using a unit circle. Therefore, my question is, how can we have a valid cosine function that works outside of axiomatic geometry before we can even define ""angle"" in via the cosine function? I know cosine function can be defined via a series, but again, how do we justify this definition if we were to use it to define ""angle""? Secondly, using these definitions, along with some other definitions for elementary geometric concepts such as planes, can one prove everything that is provable via Euclid's axioms of geometry using only algebra in without any of those axioms? If not, what other axioms must we include in order to do so? Thanks!",\mathbb{R}^n \mathbb{R}^n,"['linear-algebra', 'geometry', 'euclidean-geometry', 'analytic-geometry']"
38,What is a form?,What is a form?,,"I have read about differential forms, bilinear forms, quadratic forms and some other r-linear forms but I still have this shred of doubt in my mind on what exactly is a form. I have an assumption that it is to a ring what a vector is to a field. I apologise if it is a repost or something though.","I have read about differential forms, bilinear forms, quadratic forms and some other r-linear forms but I still have this shred of doubt in my mind on what exactly is a form. I have an assumption that it is to a ring what a vector is to a field. I apologise if it is a repost or something though.",,"['linear-algebra', 'differential-geometry', 'differential-forms', 'quadratic-forms', 'modular-forms']"
39,Question about Axler's proof that every linear operator has an eigenvalue,Question about Axler's proof that every linear operator has an eigenvalue,,"I am puzzled by Sheldon Axler's proof that every linear operator on a finite dimensional complex vector space has an eigenvalue (theorem 5.10 in ""Linear Algebra Done Right"").  In particular, it's his maneuver in the last set of displayed equations where he substitutes the linear operator $T$ for the complex variable $z$.  See below. What principle allows Axler to make this substitution of linear operator for complex variable and still claim that the RHS equals the LHS, and that the LHS goes from being the number zero (in the preceding set of equations) to the zero vector (in the final set of equations)? As a possible answer to my own question, on the preceding page he offers these remarks: Is linearity of the map from ""polynomial over complex variable"" to ""polynomial over T"" the property that makes this work?  If so, I don't feel very enlightened.  Any further insight here is greatly appreciated, especially since Axler makes a side comment that this proof is much clearer than the usual proof via the determinant.","I am puzzled by Sheldon Axler's proof that every linear operator on a finite dimensional complex vector space has an eigenvalue (theorem 5.10 in ""Linear Algebra Done Right"").  In particular, it's his maneuver in the last set of displayed equations where he substitutes the linear operator $T$ for the complex variable $z$.  See below. What principle allows Axler to make this substitution of linear operator for complex variable and still claim that the RHS equals the LHS, and that the LHS goes from being the number zero (in the preceding set of equations) to the zero vector (in the final set of equations)? As a possible answer to my own question, on the preceding page he offers these remarks: Is linearity of the map from ""polynomial over complex variable"" to ""polynomial over T"" the property that makes this work?  If so, I don't feel very enlightened.  Any further insight here is greatly appreciated, especially since Axler makes a side comment that this proof is much clearer than the usual proof via the determinant.",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'eigenvalues-eigenvectors', 'determinant']"
40,Why are vector spaces sometimes called linear spaces?,Why are vector spaces sometimes called linear spaces?,,"I have never come across the term 'linear space' as a synonym for 'vector space' and it seems from the book I am using (Linear Algebra by Kostrikin and Manin) that the term linear space is more familiar to the authors as opposed to using vector space. This book was translated from the Russian edition into English so it seems that the term linear space is/was more predominant in the Russian speaking countries? So I was wondering what is the intuition/motivation behind choosing such a term for the concept of a vector space.    Why have the word 'linear space' for vector spaces?    What is so ""linear"" about vector spaces?   Is it possible to have a ""non-linear"" vector space?    Why should we distinguish between ""linear"" and ""non-linear"" if such a term non-linear space exists? I know that I have not had enough linear algebra and exposure to higher mathematics to have a feel for why such a term is used for vector spaces and it would be great if someone could give an exposition.","I have never come across the term 'linear space' as a synonym for 'vector space' and it seems from the book I am using (Linear Algebra by Kostrikin and Manin) that the term linear space is more familiar to the authors as opposed to using vector space. This book was translated from the Russian edition into English so it seems that the term linear space is/was more predominant in the Russian speaking countries? So I was wondering what is the intuition/motivation behind choosing such a term for the concept of a vector space.    Why have the word 'linear space' for vector spaces?    What is so ""linear"" about vector spaces?   Is it possible to have a ""non-linear"" vector space?    Why should we distinguish between ""linear"" and ""non-linear"" if such a term non-linear space exists? I know that I have not had enough linear algebra and exposure to higher mathematics to have a feel for why such a term is used for vector spaces and it would be great if someone could give an exposition.",,"['linear-algebra', 'terminology', 'vector-spaces', 'definition', 'math-history']"
41,Positive integer matrices satisfying $A^3 + B^3 = C^3$,Positive integer matrices satisfying,A^3 + B^3 = C^3,"From the 3rd edition of the book ""The Linear Algebra a Beginning Graduate Student Ought to Know"" by Jonathan S. Golan, we find the following exercise (number 475) under chapter 9: ""Find infinitely-many triples $(A, B, C$ ) of nonzero matrices in $M_{3×3}(\mathbb{Q})$ , the entries of which are nonnegative integers, satisfying the condition $A^3 + B^3 = C^3.$ "" Now we if understand ""non-negative integers"" to include $0$ , then easily we can take $A$ , $B$ and $C$ to be diagonal matrices such that: $$ A = \begin{pmatrix} a & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix} , B =  \begin{pmatrix} 0 & 0 & 0 \\ 0 & b & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}, C = \begin{pmatrix} a & 0 & 0 \\ 0 & b & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}$$ However, if we interpret the term ""non-negative"" to mean ""strictly positive"" (s.t. a positive matrix is defined in the sense given in the entry: https://en.wikipedia.org/wiki/Nonnegative_matrix ), the question becomes harder... I suspect that the equation never holds, not only in the case of $n=3$ , but for all positive integers $n$ . I.e. we cannot find positive matrices $A,B,C$ in $M_{n×n}(\mathbb{Q})$ such that $A^k + B^k = C^k.$ where $k>2$ is an integer."" I conjecture this because a few constructions I tried for finding solutions all failed, and I imagine a way to prove that it is impossible would be by contradiction... i.e. show that any valid triplet would imply a rational/integer solution to the integer equation $a^k + b^k = c^k$ (where $a,b,c \in \mathbb{Q}$ ) which can't exist by Fermat's last theorem. However, I haven't found a promising trick yet and I wonder if the conjecture, if true, can be proven so easily, be it via contradiction or via other means...","From the 3rd edition of the book ""The Linear Algebra a Beginning Graduate Student Ought to Know"" by Jonathan S. Golan, we find the following exercise (number 475) under chapter 9: ""Find infinitely-many triples ) of nonzero matrices in , the entries of which are nonnegative integers, satisfying the condition "" Now we if understand ""non-negative integers"" to include , then easily we can take , and to be diagonal matrices such that: However, if we interpret the term ""non-negative"" to mean ""strictly positive"" (s.t. a positive matrix is defined in the sense given in the entry: https://en.wikipedia.org/wiki/Nonnegative_matrix ), the question becomes harder... I suspect that the equation never holds, not only in the case of , but for all positive integers . I.e. we cannot find positive matrices in such that where is an integer."" I conjecture this because a few constructions I tried for finding solutions all failed, and I imagine a way to prove that it is impossible would be by contradiction... i.e. show that any valid triplet would imply a rational/integer solution to the integer equation (where ) which can't exist by Fermat's last theorem. However, I haven't found a promising trick yet and I wonder if the conjecture, if true, can be proven so easily, be it via contradiction or via other means...","(A, B, C M_{3×3}(\mathbb{Q}) A^3 + B^3 = C^3. 0 A B C  A =
\begin{pmatrix}
a & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}
, B = 
\begin{pmatrix}
0 & 0 & 0 \\
0 & b & 0 \\
0 & 0 & 0 \\
\end{pmatrix},
C = \begin{pmatrix}
a & 0 & 0 \\
0 & b & 0 \\
0 & 0 & 0 \\
\end{pmatrix} n=3 n A,B,C M_{n×n}(\mathbb{Q}) A^k + B^k = C^k. k>2 a^k + b^k = c^k a,b,c \in \mathbb{Q}","['linear-algebra', 'matrices', 'matrix-equations']"
42,Showing that matrix $Q=UV^T$ is the nearest orthogonal matrix to $A$.,Showing that matrix  is the nearest orthogonal matrix to .,Q=UV^T A,"Let $A$ be an $m \times n$ matrix with a singular value decomposition $A=U\Sigma V^T$. Show that the matrix $Q=UV^T$ is the nearest orthogonal matrix to $A$, i.e., $$\min_{Q^TQ=I_{n \times n}} \|A-Q\|_F$$","Let $A$ be an $m \times n$ matrix with a singular value decomposition $A=U\Sigma V^T$. Show that the matrix $Q=UV^T$ is the nearest orthogonal matrix to $A$, i.e., $$\min_{Q^TQ=I_{n \times n}} \|A-Q\|_F$$",,"['linear-algebra', 'matrices', 'optimization', 'orthogonal-matrices', 'procrustes-problem']"
43,Finding the coordinate vector relative to a given basis,Finding the coordinate vector relative to a given basis,,"Given the basis $\beta = \{(1,-1,3),(-3,4,9),(2,-2,4)\}$ and $x = (8, -9, 6)$, I am to find the corresponding coordinate vector $[x]_\beta$. I claim that the coordinate vectors entries $x_1,x_2,x_3$ meet the following criterion: $$x_1(1,-1,3)+x_2(-3,4,9)+x_3(2,-2,4) = (8,-9,6)$$ This is equivalent to solving the augmented matrix \begin{bmatrix} 1 &-3 &2 & 8\\-1 & 4 & -2 & -9\\3 & 9 & 4 & 6 \end{bmatrix} which is row equivalent to \begin{bmatrix} 1 &-3 &2 & 8\\0 & 1 & 0 & -1\\0 & 0 & -1 & 0 \end{bmatrix} which gives $x_3 = 0$, $x_2 = -1$ and $x_1 = 3x_2 + 8 = 5$, thus the coordinate vector $(5, -1, 0)$ There is an error here, seeing as the text claims a different answer. What is the error? In particular, is it computational or simply an error in my understanding of the question? Edit: fix typo in equation","Given the basis $\beta = \{(1,-1,3),(-3,4,9),(2,-2,4)\}$ and $x = (8, -9, 6)$, I am to find the corresponding coordinate vector $[x]_\beta$. I claim that the coordinate vectors entries $x_1,x_2,x_3$ meet the following criterion: $$x_1(1,-1,3)+x_2(-3,4,9)+x_3(2,-2,4) = (8,-9,6)$$ This is equivalent to solving the augmented matrix \begin{bmatrix} 1 &-3 &2 & 8\\-1 & 4 & -2 & -9\\3 & 9 & 4 & 6 \end{bmatrix} which is row equivalent to \begin{bmatrix} 1 &-3 &2 & 8\\0 & 1 & 0 & -1\\0 & 0 & -1 & 0 \end{bmatrix} which gives $x_3 = 0$, $x_2 = -1$ and $x_1 = 3x_2 + 8 = 5$, thus the coordinate vector $(5, -1, 0)$ There is an error here, seeing as the text claims a different answer. What is the error? In particular, is it computational or simply an error in my understanding of the question? Edit: fix typo in equation",,['linear-algebra']
44,To show that orthogonal complement of a set A is closed.,To show that orthogonal complement of a set A is closed.,,"To show that orthogonal complement of a set A is closed. My try: I first show that the inner product is a continuous map. Let $X$ be an inner product space. For all $x_1,x_2,y_1,y_2 \in X$, by Cauchy-Schwarz inequality we get, $$|\langle x_1,y_1\rangle - \langle x_2,y_2\rangle| = |\langle x_1- x_2,y_1\rangle + \langle x_2, y_1-y_2\rangle| $$ $$\leq \|x_1- x_2\|\cdot\|y_1\| +\|x_2\|\cdot\| y_1-y_2\|$$ This implies continuity of inner products. Let $A \subset X$ and $y \in A^\perp$. To show that $ A^\perp$ is closed, we have to show that if $(y_n)$ is convergent sequence in $ A^\perp$, then the limit $y$ also belong to $ A^\perp$. Let $x \in A$, then using that inner product is a continuous map, $$\langle x,y\rangle = \langle x, \lim_{n\to \infty} (y_n)\rangle = \lim_{n\to \infty} \langle x, y_n\rangle = 0.$$ Since $\langle x, y_n\rangle = 0$ for all $x \in A$ and $y_n \in A^\perp$. Hence $y \in A^\perp$. Is the approach\the proof correct?? Thank You!!","To show that orthogonal complement of a set A is closed. My try: I first show that the inner product is a continuous map. Let $X$ be an inner product space. For all $x_1,x_2,y_1,y_2 \in X$, by Cauchy-Schwarz inequality we get, $$|\langle x_1,y_1\rangle - \langle x_2,y_2\rangle| = |\langle x_1- x_2,y_1\rangle + \langle x_2, y_1-y_2\rangle| $$ $$\leq \|x_1- x_2\|\cdot\|y_1\| +\|x_2\|\cdot\| y_1-y_2\|$$ This implies continuity of inner products. Let $A \subset X$ and $y \in A^\perp$. To show that $ A^\perp$ is closed, we have to show that if $(y_n)$ is convergent sequence in $ A^\perp$, then the limit $y$ also belong to $ A^\perp$. Let $x \in A$, then using that inner product is a continuous map, $$\langle x,y\rangle = \langle x, \lim_{n\to \infty} (y_n)\rangle = \lim_{n\to \infty} \langle x, y_n\rangle = 0.$$ Since $\langle x, y_n\rangle = 0$ for all $x \in A$ and $y_n \in A^\perp$. Hence $y \in A^\perp$. Is the approach\the proof correct?? Thank You!!",,"['linear-algebra', 'vector-spaces', 'inner-products']"
45,Is there an easy way to find the sign of the determinant of an orthogonal matrix?,Is there an easy way to find the sign of the determinant of an orthogonal matrix?,,"I just learned that if a matrix is orthogonal, its determinant can only be valued 1 or -1. Now, if I were presented with a large matrix where it would take a lot of effort to calculate its determinant, but I know it's orthogonal, is there a simpler way to find out whether the determinant is positive or negative than the standard way of calculating the determinant?","I just learned that if a matrix is orthogonal, its determinant can only be valued 1 or -1. Now, if I were presented with a large matrix where it would take a lot of effort to calculate its determinant, but I know it's orthogonal, is there a simpler way to find out whether the determinant is positive or negative than the standard way of calculating the determinant?",,"['linear-algebra', 'matrices']"
46,A surprising result about the product of Blaschke matrices,A surprising result about the product of Blaschke matrices,,"I have verified analytically the conjecture described bellow up to $n=4$, but have had no success trying to prove it. Any help would be much appreciated. Setup Let $\{\lambda_i\}_{i=1}^n$ be real numbers and $g_i:\mathbb R \to \mathbb R$ for all $i\in \mathbb N $. Consider the following recurrence $$ g_{i+1}(x)=\frac{1+g_{i}(\lambda_{i})g_{i}(x)}{g_{i}(\lambda_{i})-g_{i}(x)}\frac{x-\lambda_{i}}{1-\lambda_{i}x}. $$ with $$ g_1(x) = a+bx .$$ Next, let $$ \mathbf M_i(x) = \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_i}{1-\lambda_i x}\end{bmatrix}\begin{bmatrix} g_i(\lambda_i) & -1 \\ 1 & g_i(\lambda_i)\end{bmatrix}, $$ and $$ S(x) = \begin{bmatrix} 0 & 1 \end{bmatrix}\begin{bmatrix} g_{n+1}(0) & -1 \\ 1 & g_{n+1}(0)\end{bmatrix}\left[\mathbf M_n(x)\mathbf M_{n-1}(x)\dots\mathbf M_{2}(x)\mathbf M_1(x)\right]\begin{bmatrix} 0 \\ 1\end{bmatrix}.$$ Conjecture For all $n\ge 2$ and for all $i,j\in\{1,\dots,n\}$, $$\frac{S(\lambda_i)}{S(\lambda_j)} = \frac{\lambda_j\left(1+g_1(\lambda_j)g_1(1/\lambda_j)\right)}{\lambda_i\left(1+g_1(\lambda_i)g_1(1/\lambda_i)\right)}.$$ The reason I consider this surprising is that the analytical formulas for $S(x)$ become extremely complicated very quickly as $n$ increases, yet these ratios continue to satisfy this simple equation. Background The motivation for this question comes from the following. Suppose $\mathbf A (x)$ is a $2\times 2$ matrix such that $\det (\mathbf A (x))$ has $n+1$ roots (inside the unit circle): $\{\lambda_i\}_{i=1}^n$ and $0$. To solve some forecasting problems (in which $\mathbf A (x)$ represents the signal structure) it is useful to find a matrix $\mathbf B (x)$ such that $\det(\mathbf A (x)\mathbf B (x))$ has no roots (inside the unit circle). A procedure to obtain the matrix $\mathbf B (x)$ is as follows. To remove the first root, $\lambda_1$, you multiply $\mathbf A (x)$ by an unitary matrix times the Blaschke matrix for this root, i.e. $$ \mathbf A^* (x) = \mathbf A (x) \mathbf W_1 \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_1}{1-\lambda_1 x}\end{bmatrix}$$ where $\mathbf W_1$ is a matrix whose columns are the left singular vectors of the of $\mathbf A(\lambda_1)$. To remove the second root, $\lambda_2$, you repeat this step using $\mathbf A^*(x)$ instead of $\mathbf A(x)$. Then, one can simply repeat this step for each of the other roots. The matrices $\mathbf M_i(x)$ above are (up to a constant), the transpose of the matrices that are multiplied to $\mathbf A^*(x)$ in each step described above. One simplifying aspect of the problem I am working with is that the second row of $\mathbf A(x)$ is equal to $0$ when evaluated at any of the roots.","I have verified analytically the conjecture described bellow up to $n=4$, but have had no success trying to prove it. Any help would be much appreciated. Setup Let $\{\lambda_i\}_{i=1}^n$ be real numbers and $g_i:\mathbb R \to \mathbb R$ for all $i\in \mathbb N $. Consider the following recurrence $$ g_{i+1}(x)=\frac{1+g_{i}(\lambda_{i})g_{i}(x)}{g_{i}(\lambda_{i})-g_{i}(x)}\frac{x-\lambda_{i}}{1-\lambda_{i}x}. $$ with $$ g_1(x) = a+bx .$$ Next, let $$ \mathbf M_i(x) = \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_i}{1-\lambda_i x}\end{bmatrix}\begin{bmatrix} g_i(\lambda_i) & -1 \\ 1 & g_i(\lambda_i)\end{bmatrix}, $$ and $$ S(x) = \begin{bmatrix} 0 & 1 \end{bmatrix}\begin{bmatrix} g_{n+1}(0) & -1 \\ 1 & g_{n+1}(0)\end{bmatrix}\left[\mathbf M_n(x)\mathbf M_{n-1}(x)\dots\mathbf M_{2}(x)\mathbf M_1(x)\right]\begin{bmatrix} 0 \\ 1\end{bmatrix}.$$ Conjecture For all $n\ge 2$ and for all $i,j\in\{1,\dots,n\}$, $$\frac{S(\lambda_i)}{S(\lambda_j)} = \frac{\lambda_j\left(1+g_1(\lambda_j)g_1(1/\lambda_j)\right)}{\lambda_i\left(1+g_1(\lambda_i)g_1(1/\lambda_i)\right)}.$$ The reason I consider this surprising is that the analytical formulas for $S(x)$ become extremely complicated very quickly as $n$ increases, yet these ratios continue to satisfy this simple equation. Background The motivation for this question comes from the following. Suppose $\mathbf A (x)$ is a $2\times 2$ matrix such that $\det (\mathbf A (x))$ has $n+1$ roots (inside the unit circle): $\{\lambda_i\}_{i=1}^n$ and $0$. To solve some forecasting problems (in which $\mathbf A (x)$ represents the signal structure) it is useful to find a matrix $\mathbf B (x)$ such that $\det(\mathbf A (x)\mathbf B (x))$ has no roots (inside the unit circle). A procedure to obtain the matrix $\mathbf B (x)$ is as follows. To remove the first root, $\lambda_1$, you multiply $\mathbf A (x)$ by an unitary matrix times the Blaschke matrix for this root, i.e. $$ \mathbf A^* (x) = \mathbf A (x) \mathbf W_1 \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_1}{1-\lambda_1 x}\end{bmatrix}$$ where $\mathbf W_1$ is a matrix whose columns are the left singular vectors of the of $\mathbf A(\lambda_1)$. To remove the second root, $\lambda_2$, you repeat this step using $\mathbf A^*(x)$ instead of $\mathbf A(x)$. Then, one can simply repeat this step for each of the other roots. The matrices $\mathbf M_i(x)$ above are (up to a constant), the transpose of the matrices that are multiplied to $\mathbf A^*(x)$ in each step described above. One simplifying aspect of the problem I am working with is that the second row of $\mathbf A(x)$ is equal to $0$ when evaluated at any of the roots.",,"['linear-algebra', 'matrices', 'complex-analysis', 'recurrence-relations']"
47,Proving any linear transformation can be represented as a matrix,Proving any linear transformation can be represented as a matrix,,"I'm trying to prove that Theorem. Consider a linear transformation $T : \mathbb R^n \to \mathbb R^n$ .   The transformation $T$ can be represented as a matrix product $\mathbf x \mapsto A \mathbf x$ , for some matrix $A \in \mathbb R^{n \times n}$ . Here's my attempt at a constructive proof. Proof. Consider a matrix $\mathbf x \in \mathbb R^n$ given by \begin{align*}   \mathbf x &=   \begin{bmatrix}     x_1 \\     x_2 \\     \vdots \\     x_n   \end{bmatrix}. \end{align*} We will construct a matrix $A \in \mathbb R^{n \times n}$ such that $T(\mathbf x) = A \mathbf x$ . The vector $\mathbf x$ can also be written as \begin{align*}   \mathbf x &=   x_1   \begin{bmatrix}     1 \\     0 \\     \vdots \\     0   \end{bmatrix}   +   x_2   \begin{bmatrix}     0 \\     1 \\     \vdots \\     0   \end{bmatrix}   + \dotsb +   x_n   \begin{bmatrix}     0 \\     0 \\     \vdots \\     1   \end{bmatrix} \\   &= x_1 \mathbf{e}_{1} + x_2 \mathbf{e}_{2} + \dotsb + x_n \mathbf{e}_{n} \\   &= \sum_{i=1}^{n} x_i \mathbf{e}_{i}, \end{align*} where $\mathbf{e}_{i}$ are the standard basis vectors in $\mathbb R^n$ . Consider the transformation $T(\mathbf x)$ .   Rewriting $\mathbf x$ as above, we have \begin{align}   T(\mathbf x) &= T \left( \sum_{i=1}^{n} x_i \mathbf{e}_{i} \right) \\   &= \sum_{i=1}^{n} T(x_i \mathbf{e}_{i}) \\   T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{1} \end{align} Let the matrix $A \in \mathbb R^{n \times n}$ be defined by \begin{align*}   A &=   \begin{bmatrix}     T(\mathbf{e}_{1}) &     T(\mathbf{e}_{2}) &     \cdots &     T(\mathbf{e}_{n}) &   \end{bmatrix} \\   &=   \begin{bmatrix}     a_{11} & \cdots & a_{1n} \\     \vdots & \ddots & \vdots \\     a_{n1} & \cdots & a_{nn}   \end{bmatrix}, \end{align*} where each $T(\mathbf{e}_{i})$ is a column of $A$ , and each $a_{ij} = T(\mathbf{e}_{i}) \cdot \mathbf{e}_{j}$ is the $j$ th component of $T(\mathbf{e}_{i})$ .   Then, by the definition of matrix-vector multiplication, we have \begin{align*}   A \mathbf x &=   \begin{bmatrix}     a_{11} & \cdots & a_{1n} \\     \vdots & \ddots & \vdots \\     a_{n1} & \cdots & a_{nn}   \end{bmatrix}   \begin{bmatrix}     x_1 \\     \vdots \\     x_n   \end{bmatrix} \\   &=   \begin{bmatrix}     x_1 a_{11} + \dotsb + x_n a_{1n} \\     \vdots \\     x_1 a_{n1} + \dotsb + x_n a_{nn} \\   \end{bmatrix} \\   &=   x_1   \begin{bmatrix}     a_{11} \\     \vdots \\     a_{n1}   \end{bmatrix}   + \dotsb +   x_n   \begin{bmatrix}     a_{n1} \\     \vdots \\     a_{nn}   \end{bmatrix} \\   &= x_1 T(\mathbf{e}_{1}) + \dotsb + x_n T(\mathbf{e}_{n}) \\   A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{2} \end{align*} Therefore, by eqs. (1) and (2), we have that \begin{align*}   T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}) &   A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}), \end{align*} and we reach $T(\mathbf x) = A \mathbf x$ , as was to be shown. Any thoughts or suggestions would be appreciated.","I'm trying to prove that Theorem. Consider a linear transformation .   The transformation can be represented as a matrix product , for some matrix . Here's my attempt at a constructive proof. Proof. Consider a matrix given by We will construct a matrix such that . The vector can also be written as where are the standard basis vectors in . Consider the transformation .   Rewriting as above, we have Let the matrix be defined by where each is a column of , and each is the th component of .   Then, by the definition of matrix-vector multiplication, we have Therefore, by eqs. (1) and (2), we have that and we reach , as was to be shown. Any thoughts or suggestions would be appreciated.","T : \mathbb R^n \to \mathbb R^n T \mathbf x \mapsto A \mathbf x A \in \mathbb R^{n \times n} \mathbf x \in \mathbb R^n \begin{align*}
  \mathbf x &=
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
  \end{bmatrix}.
\end{align*} A \in \mathbb R^{n \times n} T(\mathbf x) = A \mathbf x \mathbf x \begin{align*}
  \mathbf x &=
  x_1
  \begin{bmatrix}
    1 \\
    0 \\
    \vdots \\
    0
  \end{bmatrix}
  +
  x_2
  \begin{bmatrix}
    0 \\
    1 \\
    \vdots \\
    0
  \end{bmatrix}
  + \dotsb +
  x_n
  \begin{bmatrix}
    0 \\
    0 \\
    \vdots \\
    1
  \end{bmatrix} \\
  &= x_1 \mathbf{e}_{1} + x_2 \mathbf{e}_{2} + \dotsb + x_n \mathbf{e}_{n} \\
  &= \sum_{i=1}^{n} x_i \mathbf{e}_{i},
\end{align*} \mathbf{e}_{i} \mathbb R^n T(\mathbf x) \mathbf x \begin{align}
  T(\mathbf x) &= T \left( \sum_{i=1}^{n} x_i \mathbf{e}_{i} \right) \\
  &= \sum_{i=1}^{n} T(x_i \mathbf{e}_{i}) \\
  T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{1}
\end{align} A \in \mathbb R^{n \times n} \begin{align*}
  A &=
  \begin{bmatrix}
    T(\mathbf{e}_{1}) &
    T(\mathbf{e}_{2}) &
    \cdots &
    T(\mathbf{e}_{n}) &
  \end{bmatrix} \\
  &=
  \begin{bmatrix}
    a_{11} & \cdots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \cdots & a_{nn}
  \end{bmatrix},
\end{align*} T(\mathbf{e}_{i}) A a_{ij} = T(\mathbf{e}_{i}) \cdot \mathbf{e}_{j} j T(\mathbf{e}_{i}) \begin{align*}
  A \mathbf x &=
  \begin{bmatrix}
    a_{11} & \cdots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \cdots & a_{nn}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
  \end{bmatrix} \\
  &=
  \begin{bmatrix}
    x_1 a_{11} + \dotsb + x_n a_{1n} \\
    \vdots \\
    x_1 a_{n1} + \dotsb + x_n a_{nn} \\
  \end{bmatrix} \\
  &=
  x_1
  \begin{bmatrix}
    a_{11} \\
    \vdots \\
    a_{n1}
  \end{bmatrix}
  + \dotsb +
  x_n
  \begin{bmatrix}
    a_{n1} \\
    \vdots \\
    a_{nn}
  \end{bmatrix} \\
  &= x_1 T(\mathbf{e}_{1}) + \dotsb + x_n T(\mathbf{e}_{n}) \\
  A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}). \tag{2}
\end{align*} \begin{align*}
  T(\mathbf x) &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}) &
  A \mathbf x &= \sum_{i=1}^{n} x_i T(\mathbf{e}_{i}),
\end{align*} T(\mathbf x) = A \mathbf x","['linear-algebra', 'vector-spaces', 'solution-verification']"
48,Is there an infinite-dimensional Jordan decomposition?,Is there an infinite-dimensional Jordan decomposition?,,"I just noticed this embarrassing gap in my understanding of linear algebra. This question seems to be asking something similar, although the questioner doesn't quite say it explicitly. And this question asks a version of the question which I think is a little more naive than what I'd like to say. In finite dimensions, the Jordan decomposition of a linear endomorphism $T$ is the unique way to express $T = T_{ss} + T_n$ where $T_{ss}$ is semisimple, $T_n$ is nilpotent, and $T_{ss}$ commutes with $T_n$. I'm wondering whether something similar holds in infinite dimensions. For a complex Banach space $X$, I think a semisimple operator $T_{ss}$ should be one which commutes with a set of projection operators $P_i$ with $P_i P_j = \delta_{ij} P_i$ with $\oplus_i P_i(X)$ dense in $X$, such that the restriction of $T_{ss}$ to each $P_i(X)$ is a scalar $\lambda_i$. Please let me know if this is the wrong definition. Instead of a nilpotent operator $T_n$, we'll consider a quasinilpotent operator $T_{qn}$, i.e. an operator whose spectrum is $\{0\}$, or equivalently $\lim_{k \to \infty} \|T_{qn}^k\|^{1/k} = 0$. Again let me know if there's a better notion to use here. So then the question is: for an arbitrary bounded operator $T$ on a complex Banach space $X$, do there exist commuting bounded operators $T_{ss}, T_{qn}$ such that $T_{ss}$ is semisimple, $T_{qn}$ is quasinilpotent, and $T=T_{ss}+T_{qn}$? If the answer is no, I'd like a counterexample, and also any results establishing a positive answer on special classes of Banach spaces. It does worry my that a positive answer to my question would immediately imply that any counterexample to the invariant subspace problem differs by a scalar from a quasinilpotent operator. Is this true/known? EDIT As Mariano implicitly points out, the shift operator seems to scuttle this idea since it has no obvious ""diagonal part"" and it is not itself quasinilpotent. But here's a modification to the question that just might be worth making. The quasinilpotent operators are exactly the limits of nilpotent operators in the norm topology. Instead we could take limits of nilpotent operators in the strong topology. Note that the shift operator is of this type. I haven't thought through the ramifications of this defintion -- If there's anything interesting to know about this class of operators, or if they have a name, I'd like to hear it. So: does every operator $T$ on a Banach space decompose as $T = T_{ss} + T'$ where $T_{ss}$ is diagonal, $T'$ is a limit of nilpotent operators in the strong topology, and everything commutes? EDIT As Ivan points out in his answer, although the ""backward shift"" operator on $\ell_p(\mathbb{N})$, say, is ""strong-quasinilpotent"", the ""forward shift"" operator is not. In fact, on $\ell_\infty(\mathbb{N})$, even the backward shift operator is not strong-quasinilpotent. Since the whole point of widening the definition is to accommodate various types of shift operators, the idea really doesn't work. A last-ditch effort would be to restrict to Hilbert spaces and consider ""weak-quasinilpotent"" operators $T$, i.e. those such that $T^n \to 0$ in the weak topology. At least the usual shift operators on $\ell_2(\mathbb{N})$ and $\ell_2(\mathbb{Z})$ are weak-quasinilpotent. Or maybe there's some other approach entirely to characterizing ""shift-type"" operators? In the case where there is no topology, as Ivan points out, it's possible that simply specifying the matrix similarity class of a ""shift operator"" on a space with a countable algebraic basis might yield a sufficiently robust notion. But I don't know whether we can get a ""diagonal - shift"" decomposition with this definition.","I just noticed this embarrassing gap in my understanding of linear algebra. This question seems to be asking something similar, although the questioner doesn't quite say it explicitly. And this question asks a version of the question which I think is a little more naive than what I'd like to say. In finite dimensions, the Jordan decomposition of a linear endomorphism $T$ is the unique way to express $T = T_{ss} + T_n$ where $T_{ss}$ is semisimple, $T_n$ is nilpotent, and $T_{ss}$ commutes with $T_n$. I'm wondering whether something similar holds in infinite dimensions. For a complex Banach space $X$, I think a semisimple operator $T_{ss}$ should be one which commutes with a set of projection operators $P_i$ with $P_i P_j = \delta_{ij} P_i$ with $\oplus_i P_i(X)$ dense in $X$, such that the restriction of $T_{ss}$ to each $P_i(X)$ is a scalar $\lambda_i$. Please let me know if this is the wrong definition. Instead of a nilpotent operator $T_n$, we'll consider a quasinilpotent operator $T_{qn}$, i.e. an operator whose spectrum is $\{0\}$, or equivalently $\lim_{k \to \infty} \|T_{qn}^k\|^{1/k} = 0$. Again let me know if there's a better notion to use here. So then the question is: for an arbitrary bounded operator $T$ on a complex Banach space $X$, do there exist commuting bounded operators $T_{ss}, T_{qn}$ such that $T_{ss}$ is semisimple, $T_{qn}$ is quasinilpotent, and $T=T_{ss}+T_{qn}$? If the answer is no, I'd like a counterexample, and also any results establishing a positive answer on special classes of Banach spaces. It does worry my that a positive answer to my question would immediately imply that any counterexample to the invariant subspace problem differs by a scalar from a quasinilpotent operator. Is this true/known? EDIT As Mariano implicitly points out, the shift operator seems to scuttle this idea since it has no obvious ""diagonal part"" and it is not itself quasinilpotent. But here's a modification to the question that just might be worth making. The quasinilpotent operators are exactly the limits of nilpotent operators in the norm topology. Instead we could take limits of nilpotent operators in the strong topology. Note that the shift operator is of this type. I haven't thought through the ramifications of this defintion -- If there's anything interesting to know about this class of operators, or if they have a name, I'd like to hear it. So: does every operator $T$ on a Banach space decompose as $T = T_{ss} + T'$ where $T_{ss}$ is diagonal, $T'$ is a limit of nilpotent operators in the strong topology, and everything commutes? EDIT As Ivan points out in his answer, although the ""backward shift"" operator on $\ell_p(\mathbb{N})$, say, is ""strong-quasinilpotent"", the ""forward shift"" operator is not. In fact, on $\ell_\infty(\mathbb{N})$, even the backward shift operator is not strong-quasinilpotent. Since the whole point of widening the definition is to accommodate various types of shift operators, the idea really doesn't work. A last-ditch effort would be to restrict to Hilbert spaces and consider ""weak-quasinilpotent"" operators $T$, i.e. those such that $T^n \to 0$ in the weak topology. At least the usual shift operators on $\ell_2(\mathbb{N})$ and $\ell_2(\mathbb{Z})$ are weak-quasinilpotent. Or maybe there's some other approach entirely to characterizing ""shift-type"" operators? In the case where there is no topology, as Ivan points out, it's possible that simply specifying the matrix similarity class of a ""shift operator"" on a space with a countable algebraic basis might yield a sufficiently robust notion. But I don't know whether we can get a ""diagonal - shift"" decomposition with this definition.",,"['linear-algebra', 'functional-analysis']"
49,equidistant points in $\mathbb{R}^n$ equipped with $\|.\|_p$,equidistant points in  equipped with,\mathbb{R}^n \|.\|_p,"My question is inspired by another question that was asked here. Question How many points $x_1,x_2,\dots,x_m \in \mathbb{R}^n$ can I find such that $\|x_i-x_j\|_p = 1$ for all $i\neq j$. For $p=2$ the answer is $n+1$. you can find the prove in the thread of my inspiration . However I though that the result holds for an arbitrary $p\in[1,+\infty]$. Then I found a counterexample for $p=1$ and $p=\infty$: you can choose the points \begin{align*} x_1 = \left(\begin{matrix} 0  \\ 0\end{matrix}\right),\; x_2 = \left(\begin{matrix} 1 \\  0\end{matrix}\right),\; x_3 = \left(\begin{matrix} 0 \\  1\end{matrix}\right)\;\text{and}\; x_4 = \left(\begin{matrix} 1 \\  1\end{matrix}\right) \end{align*} then you have four points in    $\mathbb{R}^2$ which fulfill $\|x_i-x_j\|_\infty = 1$ for $i\neq j$. There is a very similar example for $p=1$. I think that this follows from the special geometry of the balls in these norms. So I think there are three cases $p = 1$ $p = \infty$ $p \in (1,+\infty)$ In the first case I think the solution is $m\leq 2n$ in the second it is $m\leq 2^n$ and in the third $m\leq n+1$. Unfortunately I couldn't prove it maybe someone has a tipp. Update I found out that the cases $p=1$ and $p=\infty$ have different solutions as I already have edited. I can also prove that my guesses are lower bounds for the maximum of points that can exist. One has just to regard the corners of a ball $B_r(x)$ with radius $r=\frac{1}{2}$ in the desired norm.","My question is inspired by another question that was asked here. Question How many points $x_1,x_2,\dots,x_m \in \mathbb{R}^n$ can I find such that $\|x_i-x_j\|_p = 1$ for all $i\neq j$. For $p=2$ the answer is $n+1$. you can find the prove in the thread of my inspiration . However I though that the result holds for an arbitrary $p\in[1,+\infty]$. Then I found a counterexample for $p=1$ and $p=\infty$: you can choose the points \begin{align*} x_1 = \left(\begin{matrix} 0  \\ 0\end{matrix}\right),\; x_2 = \left(\begin{matrix} 1 \\  0\end{matrix}\right),\; x_3 = \left(\begin{matrix} 0 \\  1\end{matrix}\right)\;\text{and}\; x_4 = \left(\begin{matrix} 1 \\  1\end{matrix}\right) \end{align*} then you have four points in    $\mathbb{R}^2$ which fulfill $\|x_i-x_j\|_\infty = 1$ for $i\neq j$. There is a very similar example for $p=1$. I think that this follows from the special geometry of the balls in these norms. So I think there are three cases $p = 1$ $p = \infty$ $p \in (1,+\infty)$ In the first case I think the solution is $m\leq 2n$ in the second it is $m\leq 2^n$ and in the third $m\leq n+1$. Unfortunately I couldn't prove it maybe someone has a tipp. Update I found out that the cases $p=1$ and $p=\infty$ have different solutions as I already have edited. I can also prove that my guesses are lower bounds for the maximum of points that can exist. One has just to regard the corners of a ball $B_r(x)$ with radius $r=\frac{1}{2}$ in the desired norm.",,"['linear-algebra', 'geometry']"
50,What do you call the product of a matrix's diagonal elements?,What do you call the product of a matrix's diagonal elements?,,"The trace of $A$, an $N\times N$ matrix, is $\displaystyle\sum_{i=1}^N A_{ii}$. What do you call $\displaystyle\prod_{i=1}^N A_{ii}$?","The trace of $A$, an $N\times N$ matrix, is $\displaystyle\sum_{i=1}^N A_{ii}$. What do you call $\displaystyle\prod_{i=1}^N A_{ii}$?",,"['linear-algebra', 'matrices', 'terminology']"
51,How can I justify this without determining the determinant?,How can I justify this without determining the determinant?,,"I need to justify the following equation is true: $$     \begin{vmatrix}     a_1+b_1x & a_1x+b_1 & c_1 \\     a_2+b_2x & a_2x+b_2 & c_2 \\     a_3+b_3x & a_3x+b_3 & c_3 \\     \end{vmatrix} = (1-x^2)\cdot\begin{vmatrix}     a_1 & b_1 & c_1 \\     a_2 & b_2 & c_2 \\     a_3 & b_3 & c_3 \\     \end{vmatrix} $$ I tried dividing the determinant of the first matrix in the sum of two, so the first would not have $b's$ and the second wouldn't have $a's$ . Then I'd multiply by $\frac 1x$ in the first column of the second matrix and the first column of the second, so I'd have $x^2$ times the sum of the determinants of the two matrices. I could then subtract column 1 to column 2 in both matrices, and we'd have a column of zeros in both, hence the determinant is zero on both and times $x^2$ would still be zero, so I didn't prove anything. What did I do wrong?","I need to justify the following equation is true: I tried dividing the determinant of the first matrix in the sum of two, so the first would not have and the second wouldn't have . Then I'd multiply by in the first column of the second matrix and the first column of the second, so I'd have times the sum of the determinants of the two matrices. I could then subtract column 1 to column 2 in both matrices, and we'd have a column of zeros in both, hence the determinant is zero on both and times would still be zero, so I didn't prove anything. What did I do wrong?","
    \begin{vmatrix}
    a_1+b_1x & a_1x+b_1 & c_1 \\
    a_2+b_2x & a_2x+b_2 & c_2 \\
    a_3+b_3x & a_3x+b_3 & c_3 \\
    \end{vmatrix} = (1-x^2)\cdot\begin{vmatrix}
    a_1 & b_1 & c_1 \\
    a_2 & b_2 & c_2 \\
    a_3 & b_3 & c_3 \\
    \end{vmatrix}
 b's a's \frac 1x x^2 x^2","['linear-algebra', 'determinant']"
52,What's an intuitive way of looking at quotient spaces?,What's an intuitive way of looking at quotient spaces?,,"I understand the concept of $\mathbb{Z}/n\mathbb{Z}$, but I am having a really hard time understanding how this concept of quotients applies to vector spaces. Suppose $V = \mathbb{F}[x]$ is a vector space and $U \le V$. What exactly does $V/U$ represent?","I understand the concept of $\mathbb{Z}/n\mathbb{Z}$, but I am having a really hard time understanding how this concept of quotients applies to vector spaces. Suppose $V = \mathbb{F}[x]$ is a vector space and $U \le V$. What exactly does $V/U$ represent?",,"['linear-algebra', 'intuition']"
53,Eigenvalues are unique?,Eigenvalues are unique?,,"I'm studying eigenvector and eigenvalue but there are some confusing things to me. (1) Eigenvectors are not unique (2) If eigenvectors come from distinct eigenvalues, then eigenvectors are unique. Here is my question. Then, eigenvalues are unique all the time? Or there are any restrictions to make eigenvalues to be unique just like the case of eigenvectors? Also, when we diagonalize matrix A=$S\lambda S^{-1}$, the eigenvector matrix S is not unique. It means if we multiply each column of S by nonzero constant we can make a new $S'$. Why is that? If we can diagonalize matrix A, it means that eigenvectors are unique. But the eigenvetor matrix S is not unique? Then with the new eigenvector matrix$S'$,  $S'\lambda S'^{-1}$ makes the same matrix A? My last question is that. If eigenvalues are not unique, then eigenvalue matrix $\lambda$ is not unique too? But $S\lambda'S^{-1}$ makes same matrix A?","I'm studying eigenvector and eigenvalue but there are some confusing things to me. (1) Eigenvectors are not unique (2) If eigenvectors come from distinct eigenvalues, then eigenvectors are unique. Here is my question. Then, eigenvalues are unique all the time? Or there are any restrictions to make eigenvalues to be unique just like the case of eigenvectors? Also, when we diagonalize matrix A=$S\lambda S^{-1}$, the eigenvector matrix S is not unique. It means if we multiply each column of S by nonzero constant we can make a new $S'$. Why is that? If we can diagonalize matrix A, it means that eigenvectors are unique. But the eigenvetor matrix S is not unique? Then with the new eigenvector matrix$S'$,  $S'\lambda S'^{-1}$ makes the same matrix A? My last question is that. If eigenvalues are not unique, then eigenvalue matrix $\lambda$ is not unique too? But $S\lambda'S^{-1}$ makes same matrix A?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
54,'Free Vector Space' and 'Vector Space','Free Vector Space' and 'Vector Space',,"In this very nice book , author has defined vector space as set of functions $f : S \rightarrow F$ where $S$ is a finite set and $F$ is a field. It turns out that this definition is closely resemble definition of Free Vector Space . There are one or two threads on other online community regarding this. While I try to undersand it, could anyone point out some differences between vector spaces defined this way and defined in standard ways e.g. Hoffman and Kunze. Can one give a small concrete example of constructing a vector space using this definition and its implication. Any tutorial or link will be appreciated.","In this very nice book , author has defined vector space as set of functions where is a finite set and is a field. It turns out that this definition is closely resemble definition of Free Vector Space . There are one or two threads on other online community regarding this. While I try to undersand it, could anyone point out some differences between vector spaces defined this way and defined in standard ways e.g. Hoffman and Kunze. Can one give a small concrete example of constructing a vector space using this definition and its implication. Any tutorial or link will be appreciated.",f : S \rightarrow F S F,['linear-algebra']
55,What are the properties of eigenvalues of permutation matrices?,What are the properties of eigenvalues of permutation matrices?,,"Up till now, the only things I was able to come up/prove are the following properties: $\prod\lambda_i = \pm 1$ $ 0 \leq \sum \lambda_i \leq n$, where $n$ is the size of the matrix eigenvalues of the permutation matrix lie on the unit circle I am curious whether there exist some other interesting properties.","Up till now, the only things I was able to come up/prove are the following properties: $\prod\lambda_i = \pm 1$ $ 0 \leq \sum \lambda_i \leq n$, where $n$ is the size of the matrix eigenvalues of the permutation matrix lie on the unit circle I am curious whether there exist some other interesting properties.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'permutation-cycles', 'permutation-matrices']"
56,Bilinearity: what does it mean?,Bilinearity: what does it mean?,,"What does bilinear really mean?  Everytime I heard the word, I think it should be ""linear in 2 ways?"" For example, from the definition of inner product (taken from Appendix A of ""Wavelets For Computer Graphics"" by Stollnitz): An inner product on a vector space V is any map from $ V \times V $ to $\mathbb{R}$ that is: Symmetric $ \langle u | v \rangle = \langle v | u \rangle $ Bilinear $ \langle au + bv | w \rangle = a \langle u | w \rangle + b \langle v | w \rangle $ Positive definite $ \langle u | u \rangle > 0 $ for all $ u \ne 0 $ But how is bilinearity ""linear in 2 ways"" , (if bilinear really does mean $2\times$ linear!)","What does bilinear really mean?  Everytime I heard the word, I think it should be ""linear in 2 ways?"" For example, from the definition of inner product (taken from Appendix A of ""Wavelets For Computer Graphics"" by Stollnitz): An inner product on a vector space V is any map from $ V \times V $ to $\mathbb{R}$ that is: Symmetric $ \langle u | v \rangle = \langle v | u \rangle $ Bilinear $ \langle au + bv | w \rangle = a \langle u | w \rangle + b \langle v | w \rangle $ Positive definite $ \langle u | u \rangle > 0 $ for all $ u \ne 0 $ But how is bilinearity ""linear in 2 ways"" , (if bilinear really does mean $2\times$ linear!)",,"['linear-algebra', 'terminology', 'intuition', 'bilinear-form']"
57,"Prove that if $A$ is normal, then eigenvectors corresponding to distinct eigenvalues are necessarily orthogonal (alternative proof)","Prove that if  is normal, then eigenvectors corresponding to distinct eigenvalues are necessarily orthogonal (alternative proof)",A,"The problem statement is as follows: Prove that for a normal matrix $A$, eigenvectors corresponding to different eigenvalues are necessarily orthogonal. I can certainly prove that this is the case, using the spectral theorem.  The gist of my proof is presented below. If possible, I would like to find a simpler proof.  I was hoping that there might be some sort of manipulation along these lines , noting that $$ \langle Av_1,A v_2\rangle  = \langle v_1,A^*Av_2\rangle  = \langle v_1,AA^*v_2\rangle  = \langle A^* v_1,A^* v_2 \rangle $$ Any ideas here would be appreciated. My proof: Let $\{v_{\lambda,i}\}$ be an orthonormal basis of eigenvectors (as guaranteed by the spectral theorem) such that $$ A v_{\lambda,i} = \lambda v_{\lambda,i} $$ Let $v_1,\lambda_1$ and $v_2,\lambda_2$ be eigenpairs with $\lambda_1 \neq \lambda_2$.  We may write $ v_1 = \sum_{i,\lambda}a_{i,\lambda}v_{i,\lambda} .$ We then have $$ 0 = Av_1 - \lambda_1 v_1 = \sum_{i,\lambda}(\lambda - \lambda_1)a_{i,\lambda}v_{i,\lambda} $$ So that $a_{i,\lambda} = 0$ when $\lambda \neq \lambda_1$.  Similarly, we may write $v_2 = \sum_{i,\lambda}b_{i,\lambda}v_{i,\lambda}$, and note that $b_{i,\lambda} = 0$ when $\lambda \neq \lambda_2$.  From there, we have $$ \langle v_1,v_2 \rangle = \sum_{i,\lambda}a_{i,\lambda}b_{i,\lambda} $$ the above must be zero since for each pair $i,\lambda$, either $a_{i,\lambda}=0$ or $b_{i,\lambda} = 0$.","The problem statement is as follows: Prove that for a normal matrix $A$, eigenvectors corresponding to different eigenvalues are necessarily orthogonal. I can certainly prove that this is the case, using the spectral theorem.  The gist of my proof is presented below. If possible, I would like to find a simpler proof.  I was hoping that there might be some sort of manipulation along these lines , noting that $$ \langle Av_1,A v_2\rangle  = \langle v_1,A^*Av_2\rangle  = \langle v_1,AA^*v_2\rangle  = \langle A^* v_1,A^* v_2 \rangle $$ Any ideas here would be appreciated. My proof: Let $\{v_{\lambda,i}\}$ be an orthonormal basis of eigenvectors (as guaranteed by the spectral theorem) such that $$ A v_{\lambda,i} = \lambda v_{\lambda,i} $$ Let $v_1,\lambda_1$ and $v_2,\lambda_2$ be eigenpairs with $\lambda_1 \neq \lambda_2$.  We may write $ v_1 = \sum_{i,\lambda}a_{i,\lambda}v_{i,\lambda} .$ We then have $$ 0 = Av_1 - \lambda_1 v_1 = \sum_{i,\lambda}(\lambda - \lambda_1)a_{i,\lambda}v_{i,\lambda} $$ So that $a_{i,\lambda} = 0$ when $\lambda \neq \lambda_1$.  Similarly, we may write $v_2 = \sum_{i,\lambda}b_{i,\lambda}v_{i,\lambda}$, and note that $b_{i,\lambda} = 0$ when $\lambda \neq \lambda_2$.  From there, we have $$ \langle v_1,v_2 \rangle = \sum_{i,\lambda}a_{i,\lambda}b_{i,\lambda} $$ the above must be zero since for each pair $i,\lambda$, either $a_{i,\lambda}=0$ or $b_{i,\lambda} = 0$.",,"['linear-algebra', 'matrices', 'alternative-proof']"
58,Covectors and Vectors,Covectors and Vectors,,"I have a general question about vector/covectors: Background. A vector (for our purposes) is a physical object in each basis of $\mathbb{R}^3$ represented by three numbers such that these numbers obey certain transformation rules when we change the basis. Let $\textbf{x}$ be an arbitrary vector and $\textbf{e}_1, \textbf{e}_2, \textbf{e}_3$ and $\tilde{\textbf{e}}_1, \tilde{\textbf{e}}_2, \tilde{\textbf{e}}_3$ be two bases. These transformation/inverse transformation rules are the following: $$\tilde{x}^{j} = \sum_{j=1}^{3} T_{i}^{j} x^i$$ and $$x^{j} = \sum_{i=1}^{3} S_{i}^{j} \tilde{x}^i$$ Question. Vectors satisfy the above properties. Now if I imagine some other set of objects that satisfy the above properties why do we call them covectors? What are covectors and how are they different from vectors if they satisfy the same properties?","I have a general question about vector/covectors: Background. A vector (for our purposes) is a physical object in each basis of $\mathbb{R}^3$ represented by three numbers such that these numbers obey certain transformation rules when we change the basis. Let $\textbf{x}$ be an arbitrary vector and $\textbf{e}_1, \textbf{e}_2, \textbf{e}_3$ and $\tilde{\textbf{e}}_1, \tilde{\textbf{e}}_2, \tilde{\textbf{e}}_3$ be two bases. These transformation/inverse transformation rules are the following: $$\tilde{x}^{j} = \sum_{j=1}^{3} T_{i}^{j} x^i$$ and $$x^{j} = \sum_{i=1}^{3} S_{i}^{j} \tilde{x}^i$$ Question. Vectors satisfy the above properties. Now if I imagine some other set of objects that satisfy the above properties why do we call them covectors? What are covectors and how are they different from vectors if they satisfy the same properties?",,"['linear-algebra', 'vector-spaces']"
59,The eigenvalues of a block matrix of the form $\pmatrix{B&C\\0&D}$,The eigenvalues of a block matrix of the form,\pmatrix{B&C\\0&D},"It is known that all $B$, $C$ and $D$ are $3 \times 3$ matrices. And the eigenvalues of $B$ are $1, 2, 3$; $C$ are $4, 5, 6$; and $D$ are $7, 8, 9$. What are the eigenvalues of the $6 \times 6$ matrix  $$\begin{pmatrix} B & C\\0 & D \end{pmatrix}$$ where $0$ is the $3 \times 3$ matrix whose entries are all $0$.  From my intuition, I think the eigenvalues of the new $6 \times 6$ matrix are the eigenvalues of $B$ and $D$. But how can I show that?","It is known that all $B$, $C$ and $D$ are $3 \times 3$ matrices. And the eigenvalues of $B$ are $1, 2, 3$; $C$ are $4, 5, 6$; and $D$ are $7, 8, 9$. What are the eigenvalues of the $6 \times 6$ matrix  $$\begin{pmatrix} B & C\\0 & D \end{pmatrix}$$ where $0$ is the $3 \times 3$ matrix whose entries are all $0$.  From my intuition, I think the eigenvalues of the new $6 \times 6$ matrix are the eigenvalues of $B$ and $D$. But how can I show that?",,['linear-algebra']
60,"A matrix is diagonalizable, so what?","A matrix is diagonalizable, so what?",,"I mean, you can say it's similar to a diagonal matrix, it has $n$ independent eigenvectors, etc., but what's the big deal of having diagonalizability? Can I solidly perceive the differences between two linear transformation one of which is diagonalizable and the other is not, either by visualization or figurative description? For example, invertibility can be perceived. Because non-invertible transformation must compress the space in one or more certain direction to $0$ . Like crashing a space flat.","I mean, you can say it's similar to a diagonal matrix, it has independent eigenvectors, etc., but what's the big deal of having diagonalizability? Can I solidly perceive the differences between two linear transformation one of which is diagonalizable and the other is not, either by visualization or figurative description? For example, invertibility can be perceived. Because non-invertible transformation must compress the space in one or more certain direction to . Like crashing a space flat.",n 0,"['linear-algebra', 'matrices', 'diagonalization']"
61,Why do we assume that a matrix in quadratic form is symmetric?,Why do we assume that a matrix in quadratic form is symmetric?,,"I am looking to the review document for linear algebra (Zico Kolter (updated by Chuong Do), Linear Algebra Review and Reference ) , and the part of the quadratic form  (pg17) mentions about an assumption of being symmetric for a matrix in quadratic form. It also includes some declarative equality for that proposed argument. What is the practical reason to assume that the matrix describing a quadratic form (over $\mathbb{R}$ ) is symmetric? I also do not get the idea proposed by the argument? Can someone light me about?","I am looking to the review document for linear algebra (Zico Kolter (updated by Chuong Do), Linear Algebra Review and Reference ) , and the part of the quadratic form  (pg17) mentions about an assumption of being symmetric for a matrix in quadratic form. It also includes some declarative equality for that proposed argument. What is the practical reason to assume that the matrix describing a quadratic form (over ) is symmetric? I also do not get the idea proposed by the argument? Can someone light me about?",\mathbb{R},"['linear-algebra', 'matrices', 'quadratic-forms']"
62,What is the difference between orthogonal and orthonormal in terms of vectors and vector space?,What is the difference between orthogonal and orthonormal in terms of vectors and vector space?,,I am beginner to linear algebra. I want to know detailed explanation of what is the difference between these two and geometrically how these two are interpreted?,I am beginner to linear algebra. I want to know detailed explanation of what is the difference between these two and geometrically how these two are interpreted?,,"['linear-algebra', 'matrices', 'orthogonality']"
63,Decomposing an Affine transformation,Decomposing an Affine transformation,,"An affine transformation is composed of rotations, translations, scaling and shearing. In 2D, such a transformation can be represented using an augmented matrix by  $$ \begin{bmatrix} \vec{y} \\ 1 \end{bmatrix} =\begin{bmatrix} A & \vec{b} \ \\ 0, \ldots, 0 & 1 \end{bmatrix} \begin{bmatrix} \vec{x} \\ 1 \end{bmatrix} $$ vector b represents the translation. Bu how can I decompose A into rotation, scaling and shearing? I am trying to ""interpolate"" an affine transform so that, if I have points in a frame 1 and in a frame 3 such that a transform $T$ takes points from frame 1 to points in frame 3, then the interpolated transform would take my points from frame 1 to frame 2. If, for example, an 60° rotation is needed to go from frame 1 to frame 3, then a 30° one would be used to go from frame 1 to frame 2. Thank you for your help.","An affine transformation is composed of rotations, translations, scaling and shearing. In 2D, such a transformation can be represented using an augmented matrix by  $$ \begin{bmatrix} \vec{y} \\ 1 \end{bmatrix} =\begin{bmatrix} A & \vec{b} \ \\ 0, \ldots, 0 & 1 \end{bmatrix} \begin{bmatrix} \vec{x} \\ 1 \end{bmatrix} $$ vector b represents the translation. Bu how can I decompose A into rotation, scaling and shearing? I am trying to ""interpolate"" an affine transform so that, if I have points in a frame 1 and in a frame 3 such that a transform $T$ takes points from frame 1 to points in frame 3, then the interpolated transform would take my points from frame 1 to frame 2. If, for example, an 60° rotation is needed to go from frame 1 to frame 3, then a 30° one would be used to go from frame 1 to frame 2. Thank you for your help.",,"['linear-algebra', 'geometry']"
64,Strang's proof of SVD and intuition behind matrices $U$ and $V$,Strang's proof of SVD and intuition behind matrices  and,U V,"In lecture 29 of MIT 18.06, Professor Gilbert Strang ""proves"" the singular value decomposition (SVD) by assuming that we can write $A = U\Sigma V^T$ and then deriving what $U$, $\Sigma$, and $V$ must be based on the eigendecomposition of $$ AA^T = U\Sigma ^2 U^T$$ and $$ A^TA = V\Sigma ^2 V^T$$ My intuition tells me there's something wrong with first assuming that we can write $A$ in this form. As in, we are finding $U$, $\Sigma$ and $V$ for those matrices that have this form, but what if some matrices couldn't be written in this form in the first place? Also, is there some intuition to be found about $U$ and $V$ by only thinking about them as eigenvectors of $AA^T$ and $A^TA$? In the sense that we should be able to know why $V$ will have its properties (be mapped orthogonally by $A$) by just looking at it as an eigenvector of $A^TA$ (which somehow implicitly ""encodes"" the basis $U$). Not sure if this intuition makes sense either.","In lecture 29 of MIT 18.06, Professor Gilbert Strang ""proves"" the singular value decomposition (SVD) by assuming that we can write $A = U\Sigma V^T$ and then deriving what $U$, $\Sigma$, and $V$ must be based on the eigendecomposition of $$ AA^T = U\Sigma ^2 U^T$$ and $$ A^TA = V\Sigma ^2 V^T$$ My intuition tells me there's something wrong with first assuming that we can write $A$ in this form. As in, we are finding $U$, $\Sigma$ and $V$ for those matrices that have this form, but what if some matrices couldn't be written in this form in the first place? Also, is there some intuition to be found about $U$ and $V$ by only thinking about them as eigenvectors of $AA^T$ and $A^TA$? In the sense that we should be able to know why $V$ will have its properties (be mapped orthogonally by $A$) by just looking at it as an eigenvector of $A^TA$ (which somehow implicitly ""encodes"" the basis $U$). Not sure if this intuition makes sense either.",,"['linear-algebra', 'matrices', 'svd']"
65,How does the dot product convert a matrix into a scalar?,How does the dot product convert a matrix into a scalar?,,"I am learning linear algebra, and I am a bit confused by the dot product and how the answer to the process turns out to be a scalar rather than a matrix. For $2$ vectors with $2$ components, I learned that dot product is equivalent to a $1 \times 2$ row vector left multiplied by a $2 \times 1$ column vector.   The result of such a multiplication should result in a $ 1 \times 1 $ vector.   But I am learning that the dot product somehow transforms the result into a scalar, rather than a $ 1 \times 1 $ vector. Maybe I am missing something but the difference between a $ 1 \times 1 $ vector and a scalar seems important, because you can multiply a scalar by a matrix of any size, but you can only left-multiply a $ 1 \times 1 $ vector by another matrix with $1$ row. Thanks for any help in understanding this.","I am learning linear algebra, and I am a bit confused by the dot product and how the answer to the process turns out to be a scalar rather than a matrix. For $2$ vectors with $2$ components, I learned that dot product is equivalent to a $1 \times 2$ row vector left multiplied by a $2 \times 1$ column vector.   The result of such a multiplication should result in a $ 1 \times 1 $ vector.   But I am learning that the dot product somehow transforms the result into a scalar, rather than a $ 1 \times 1 $ vector. Maybe I am missing something but the difference between a $ 1 \times 1 $ vector and a scalar seems important, because you can multiply a scalar by a matrix of any size, but you can only left-multiply a $ 1 \times 1 $ vector by another matrix with $1$ row. Thanks for any help in understanding this.",,['linear-algebra']
66,What is inverse of $I+A$?,What is inverse of ?,I+A,"Assume $A$ is a square invertible matrix and we have $A^{-1}$. If we know that $I+A$ is also invertible, do we have a close form for $(I+A)^{-1}$ in terms of $A^{-1}$ and $A$? Does it make it any easier if we know that sum of all rows are equal?","Assume $A$ is a square invertible matrix and we have $A^{-1}$. If we know that $I+A$ is also invertible, do we have a close form for $(I+A)^{-1}$ in terms of $A^{-1}$ and $A$? Does it make it any easier if we know that sum of all rows are equal?",,"['linear-algebra', 'matrices', 'inverse']"
67,"$\sin(A)$, where $A$ is a matrix",", where  is a matrix",\sin(A) A,"If $A$ is an $n\times n$ matrix with elements $a_{ij}$ $i=$i'th row, $j=$j'th column. Then  $e^A$ is also a matrix as can be seen by expanding it in a power series.Is $e^A$ always convergent and defined for any $n\times n$ matrix? What are the elements $e^A_{ij}$ in terms of $a_{ij}$ ? $\sin(x)$ for real numbers $x$ can be interpreted easily geometrically by looking at the unit circle. Is there any geometrical interpretation of $\sin(A)$ when $A$ is a matrix? What applications does it have? Slight addition: Is $\sin(A)$ periodic in any sense, i.e. is there a matrix $B$ not $0$ such that $\sin(A+B)=\sin(A)$ for all matrices $A$? Does $\sin(A)^2+\cos(A)^2=I$ hold? And do all the regular rules from algebra transfer, i.e. $e^Ae^B=e^{(A+B)}$ ? Is there a consistent definition of $M/N$ for matrices $M,N$, such that $e^A/e^B=e^{(A-B)}$ holds?","If $A$ is an $n\times n$ matrix with elements $a_{ij}$ $i=$i'th row, $j=$j'th column. Then  $e^A$ is also a matrix as can be seen by expanding it in a power series.Is $e^A$ always convergent and defined for any $n\times n$ matrix? What are the elements $e^A_{ij}$ in terms of $a_{ij}$ ? $\sin(x)$ for real numbers $x$ can be interpreted easily geometrically by looking at the unit circle. Is there any geometrical interpretation of $\sin(A)$ when $A$ is a matrix? What applications does it have? Slight addition: Is $\sin(A)$ periodic in any sense, i.e. is there a matrix $B$ not $0$ such that $\sin(A+B)=\sin(A)$ for all matrices $A$? Does $\sin(A)^2+\cos(A)^2=I$ hold? And do all the regular rules from algebra transfer, i.e. $e^Ae^B=e^{(A+B)}$ ? Is there a consistent definition of $M/N$ for matrices $M,N$, such that $e^A/e^B=e^{(A-B)}$ holds?",,"['linear-algebra', 'matrices', 'trigonometry', 'exponentiation']"
68,"Prove that $\operatorname{SL}(n,\Bbb R)$ is connected.",Prove that  is connected.,"\operatorname{SL}(n,\Bbb R)","Prove that $\operatorname{SL}(n, \Bbb R)$ is connected. The problem is I know only topological groups from Munkres only. Again Just started fundamental groups. So if anyone can explain to me how it is true in a lucid language and in an easy way such that it remains in my boundary of knowledge then it would be a great help. I have mentioned what I know. Again if tag this in the wrong field. Please forgive me.",Prove that is connected. The problem is I know only topological groups from Munkres only. Again Just started fundamental groups. So if anyone can explain to me how it is true in a lucid language and in an easy way such that it remains in my boundary of knowledge then it would be a great help. I have mentioned what I know. Again if tag this in the wrong field. Please forgive me.,"\operatorname{SL}(n, \Bbb R)","['linear-algebra', 'general-topology', 'topological-groups']"
69,What does it mean for two matrices to be orthogonal?,What does it mean for two matrices to be orthogonal?,,"Firstly, please bear in mind that I do not have much knowledge about linear algebra. Secondly, I am not asking about some mathematical definitions but rather a more physical definition of this mathematical meaning. So, the problem is that I can understand the meaning of orthogonality between two vectors, they are just ""lines"" perpendicular to each other, but I can not physically perceive what orthogonality means for matrices .(like a $3\times 3$ one). I mean if vectors are like lines in space, those being orthogonal is an easy concept to visualize. But how to visualize two orthogonal matrices? I think that this is easier to explain to me if you first explain how does a matrix look like in space (visualization): For example, for a $2 \times 2$ , is it the area that its two contained vectors form in space? If it is that, then two orthogonal matrices are simply those two areas that are perpendicular to each other. If not, give me an analogous explanation.*","Firstly, please bear in mind that I do not have much knowledge about linear algebra. Secondly, I am not asking about some mathematical definitions but rather a more physical definition of this mathematical meaning. So, the problem is that I can understand the meaning of orthogonality between two vectors, they are just ""lines"" perpendicular to each other, but I can not physically perceive what orthogonality means for matrices .(like a one). I mean if vectors are like lines in space, those being orthogonal is an easy concept to visualize. But how to visualize two orthogonal matrices? I think that this is easier to explain to me if you first explain how does a matrix look like in space (visualization): For example, for a , is it the area that its two contained vectors form in space? If it is that, then two orthogonal matrices are simply those two areas that are perpendicular to each other. If not, give me an analogous explanation.*",3\times 3 2 \times 2,"['linear-algebra', 'orthogonality']"
70,$A^{-1}$ has integer entries if and only if the ${\rm det}\ (A) =\pm 1$,has integer entries if and only if the,A^{-1} {\rm det}\ (A) =\pm 1,"So, $A$ is a $n \times n$ matrix with integer entries. The question is to prove that $A^{-1}$ has all integer entries if and only if ${\rm det}\ (A) =\pm 1$ . I know that $A^{-1}= {\rm adj}(A)/{\rm det}(A)$ , but I have no idea where to go from there for the forward direction. Any help here would be greatly appreciated. For the backwards direction, I think I am okay. I plugged the 1 and the -1 options into $ {\rm adj}(A)/{\rm det}(A)$ . So, I know that $A^{-1}= {\rm adj}(A)$ or $-{\rm adj}(A)$ . Then I said that because the ${\rm adj}(A)$ is simply the matrix of co-factors of $A$ , and because $A$ has all integer entries, then ${\rm adj}(A)$ will have all integer entries, which will mean that $A^{-1}$ will have all integer entries. Is that okay? Or am I missing something else? Thank you!!","So, is a matrix with integer entries. The question is to prove that has all integer entries if and only if . I know that , but I have no idea where to go from there for the forward direction. Any help here would be greatly appreciated. For the backwards direction, I think I am okay. I plugged the 1 and the -1 options into . So, I know that or . Then I said that because the is simply the matrix of co-factors of , and because has all integer entries, then will have all integer entries, which will mean that will have all integer entries. Is that okay? Or am I missing something else? Thank you!!",A n \times n A^{-1} {\rm det}\ (A) =\pm 1 A^{-1}= {\rm adj}(A)/{\rm det}(A)  {\rm adj}(A)/{\rm det}(A) A^{-1}= {\rm adj}(A) -{\rm adj}(A) {\rm adj}(A) A A {\rm adj}(A) A^{-1},"['linear-algebra', 'matrices', 'determinant', 'inverse', 'adjoint-operators']"
71,"If the Wronskian is zero at some point, does this imply linear dependency of functions?","If the Wronskian is zero at some point, does this imply linear dependency of functions?",,"We know that If for functions $f$ and $g$ , the Wronskian $W(f,g)(x_0)$ is nonzero for some $x_0$ in $[a,b]$ then $f$ and $g$ are linearly independent on $[a,b]$ . If $f$ and $g$ are linearly dependent then the Wronskian is zero for all $x_0$ in $[a,b]$ . My doubt is : If for some $x$ , $W(f,g)(x)$ is zero, can we conclude that Wronskian is identically zero as we know that wronskian is zero or never zero. In one problem, Wronskian $W$ was coming as $-x^2$ on $(\infty,-\infty)$ . Since $W$ is $0$ for $x=0$ can we say Wronskian is identically zero OR using point 1 we may conclude that we are getting more than one point where Wronskian is not zero and hence functions are linearly independent.","We know that If for functions and , the Wronskian is nonzero for some in then and are linearly independent on . If and are linearly dependent then the Wronskian is zero for all in . My doubt is : If for some , is zero, can we conclude that Wronskian is identically zero as we know that wronskian is zero or never zero. In one problem, Wronskian was coming as on . Since is for can we say Wronskian is identically zero OR using point 1 we may conclude that we are getting more than one point where Wronskian is not zero and hence functions are linearly independent.","f g W(f,g)(x_0) x_0 [a,b] f g [a,b] f g x_0 [a,b] x W(f,g)(x) W -x^2 (\infty,-\infty) W 0 x=0","['linear-algebra', 'ordinary-differential-equations', 'analysis', 'wronskian']"
72,Do characteristic polynomials exhaust all monic polynomials?,Do characteristic polynomials exhaust all monic polynomials?,,"Let $A$ be an $n\times n$ matrix, then $\mathrm{char}_A(x):=\det(xI-A)$ is a monic polynomial of degree $n$. It is called the characteristic polynomial of $A$. My question is the converse: Let $p(x)$ be a monic polynomial of degree $n$. Can we always find an $n\times n$ matrix such that $p(x)=\mathrm{char}_A(x)$?","Let $A$ be an $n\times n$ matrix, then $\mathrm{char}_A(x):=\det(xI-A)$ is a monic polynomial of degree $n$. It is called the characteristic polynomial of $A$. My question is the converse: Let $p(x)$ be a monic polynomial of degree $n$. Can we always find an $n\times n$ matrix such that $p(x)=\mathrm{char}_A(x)$?",,"['linear-algebra', 'matrices', 'polynomials', 'characteristic-polynomial']"
73,Tensors in the context of engineering mechanics: can they be explained in an intuitive way?,Tensors in the context of engineering mechanics: can they be explained in an intuitive way?,,"I've spent a few weeks scouring the internet for a an explanation of tensors in the context of engineering mechanics. You know, the ones every engineering student know and love (stress, strain, etc.). But I cannot find any explanations of tensors without running into abstract formalisms like ""homomorphisms"" and ""inner product spaces"". I'm not looking for an explanation of tensors using abstract algebra or infinite, generalized vector spaces. I just want some clarification on what they actually mean and are doing in the nice 3D, Euclidean space, especially in the context of mechanics. There are a few questions that have been bugging me that I'm hoping all you smart people here can answer: What's the difference between a linear transformation and a tensor? Somehow they can both be represented by a $3\times 3$ matrix, but they do different things when acting on a vector? Like the columns of a $3 \times 3$ matrix of a linear transformation tell you where the basis vectors end up, but the same columns of a tensor don't represent basis vectors at all? Furthermore, a linear transformation transforms all of space but a tensor is defined at every point in space? Does a tensor act on vectors the same way as linear transformations do? What is the difference between a tensor product, dyadic product, and outer product and why are engineering tensors like the Cauchy stress built from the tensor product of two vectors (i.e. traction vector and normal vector)? Is it true that scalars and vectors are just $0^\mathrm{th}$ order and $1^\mathrm{st}$ order tensors, respectively? How are all these things related to each other? What topics and/or subtopics of linear algebra are essential to grasp the essence of tensors in the context of physics and engineering? Are they really just objects that act on vectors to produce other vectors (or numbers) or are they something more? I have plenty more questions, but I figure the answers to these could already be enough to fill a whole textbook. Just to note, I have already searched Math.StackExchange for tensors but haven't found any explanations that make sense to me yet. Thanks!","I've spent a few weeks scouring the internet for a an explanation of tensors in the context of engineering mechanics. You know, the ones every engineering student know and love (stress, strain, etc.). But I cannot find any explanations of tensors without running into abstract formalisms like ""homomorphisms"" and ""inner product spaces"". I'm not looking for an explanation of tensors using abstract algebra or infinite, generalized vector spaces. I just want some clarification on what they actually mean and are doing in the nice 3D, Euclidean space, especially in the context of mechanics. There are a few questions that have been bugging me that I'm hoping all you smart people here can answer: What's the difference between a linear transformation and a tensor? Somehow they can both be represented by a $3\times 3$ matrix, but they do different things when acting on a vector? Like the columns of a $3 \times 3$ matrix of a linear transformation tell you where the basis vectors end up, but the same columns of a tensor don't represent basis vectors at all? Furthermore, a linear transformation transforms all of space but a tensor is defined at every point in space? Does a tensor act on vectors the same way as linear transformations do? What is the difference between a tensor product, dyadic product, and outer product and why are engineering tensors like the Cauchy stress built from the tensor product of two vectors (i.e. traction vector and normal vector)? Is it true that scalars and vectors are just $0^\mathrm{th}$ order and $1^\mathrm{st}$ order tensors, respectively? How are all these things related to each other? What topics and/or subtopics of linear algebra are essential to grasp the essence of tensors in the context of physics and engineering? Are they really just objects that act on vectors to produce other vectors (or numbers) or are they something more? I have plenty more questions, but I figure the answers to these could already be enough to fill a whole textbook. Just to note, I have already searched Math.StackExchange for tensors but haven't found any explanations that make sense to me yet. Thanks!",,"['linear-algebra', 'tensor-products', 'tensors', 'classical-mechanics']"
74,$AB=BA$ implies $AB^T=B^TA$ when $A$ is normal,implies  when  is normal,AB=BA AB^T=B^TA A,"I am looking for an elementary proof (if such exists) of the following: $$ AB=BA \quad\Longrightarrow\quad AB^T=B^TA, $$ where $A$ and $B$ are $n\times n$ real matrices, and $A$ is a normal matrix, i.e., $AA^T=A^TA$  - it is true for complex matrices as well, with $A^T$ replaced by $A^*$. There is a non-elementary proof of this using the exponential of a matrix and properties of entire functions. Update. In the first version of the question, both $A$ and $B$ were supposed to be normal, but as Shlomi correctly pointed out, this is true even in the case when only one of them is normal.","I am looking for an elementary proof (if such exists) of the following: $$ AB=BA \quad\Longrightarrow\quad AB^T=B^TA, $$ where $A$ and $B$ are $n\times n$ real matrices, and $A$ is a normal matrix, i.e., $AA^T=A^TA$  - it is true for complex matrices as well, with $A^T$ replaced by $A^*$. There is a non-elementary proof of this using the exponential of a matrix and properties of entire functions. Update. In the first version of the question, both $A$ and $B$ were supposed to be normal, but as Shlomi correctly pointed out, this is true even in the case when only one of them is normal.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
75,Invertible matrices over a commutative ring and their determinants,Invertible matrices over a commutative ring and their determinants,,"Why is it true that a matrix $A \in \operatorname{Mat}_n(R)$, where $R$ is a commutative ring, is invertible iff its determinant is invertible? Since $\det(A)I_n = A\operatorname{adj}(A) = \operatorname{adj}(A)A$ then I can see why the determinant being invertible implies the inverse exists, since the adjoint always exists, but I can't see why it's true the other way around.","Why is it true that a matrix $A \in \operatorname{Mat}_n(R)$, where $R$ is a commutative ring, is invertible iff its determinant is invertible? Since $\det(A)I_n = A\operatorname{adj}(A) = \operatorname{adj}(A)A$ then I can see why the determinant being invertible implies the inverse exists, since the adjoint always exists, but I can't see why it's true the other way around.",,"['linear-algebra', 'abstract-algebra', 'determinant']"
76,Showing that an Isometry on the Euclidean Plane fixing the origin is Linear,Showing that an Isometry on the Euclidean Plane fixing the origin is Linear,,"Suppose $f$ is an isometric (i.e., distance preserving) function on $\mathbb{E}^2$ such that $f(0,0) = (0,0)$.  Then I want to show that $f$ is necessarily linear.  Now $f$ is linear iff $f$ is both additive and homogenous. The following is an attempted proof for the homogeneity of f (missing the last step); still more, I have no idea how to argue for the additivity of $f$. Any ideas? Let $x \in \mathbb{E}^2$ and $\alpha \in \mathbb{R}$ We know that $\forall x \in \mathbb{E}^2$, $\Vert x - 0 \Vert = \Vert f(x) - f(0)\Vert = \Vert f(x) - 0 \Vert$ so that $\Vert x\Vert = \Vert f(x)\Vert$. From this we immediately have the following facts: $\Vert x \Vert = \Vert f(x) \Vert$ $\Vert \alpha x \Vert = \Vert f(\alpha x) \Vert$ We can then argue that since $\Vert \alpha x\Vert = |\alpha| \Vert x \Vert = |\alpha| \Vert f(x) \Vert = \Vert \alpha f(x) \Vert$, we also have that $\Vert f(\alpha x) \Vert = \Vert \alpha f(x)\Vert$. Finally, we have that $\Vert \alpha x - x \Vert = \Vert \alpha f(x) - f(x) \Vert$ iff $ \Vert (\alpha - 1)x \Vert = \Vert (\alpha - 1) f(x) \Vert$ iff $|\alpha - 1| \Vert x \Vert = |\alpha - 1| \Vert f(x) \Vert$ Since the last of these statements is in fact true, we now have $\Vert \alpha x - x \Vert = \Vert \alpha f(x) - f(x) \Vert$ as desired. Now at this point it seems like I have all of the facts required to assert that $f(\alpha x) = \alpha f(x)$, but I can't figure out how to formally state why without illegally appealing to visual intuition. Any ideas?","Suppose $f$ is an isometric (i.e., distance preserving) function on $\mathbb{E}^2$ such that $f(0,0) = (0,0)$.  Then I want to show that $f$ is necessarily linear.  Now $f$ is linear iff $f$ is both additive and homogenous. The following is an attempted proof for the homogeneity of f (missing the last step); still more, I have no idea how to argue for the additivity of $f$. Any ideas? Let $x \in \mathbb{E}^2$ and $\alpha \in \mathbb{R}$ We know that $\forall x \in \mathbb{E}^2$, $\Vert x - 0 \Vert = \Vert f(x) - f(0)\Vert = \Vert f(x) - 0 \Vert$ so that $\Vert x\Vert = \Vert f(x)\Vert$. From this we immediately have the following facts: $\Vert x \Vert = \Vert f(x) \Vert$ $\Vert \alpha x \Vert = \Vert f(\alpha x) \Vert$ We can then argue that since $\Vert \alpha x\Vert = |\alpha| \Vert x \Vert = |\alpha| \Vert f(x) \Vert = \Vert \alpha f(x) \Vert$, we also have that $\Vert f(\alpha x) \Vert = \Vert \alpha f(x)\Vert$. Finally, we have that $\Vert \alpha x - x \Vert = \Vert \alpha f(x) - f(x) \Vert$ iff $ \Vert (\alpha - 1)x \Vert = \Vert (\alpha - 1) f(x) \Vert$ iff $|\alpha - 1| \Vert x \Vert = |\alpha - 1| \Vert f(x) \Vert$ Since the last of these statements is in fact true, we now have $\Vert \alpha x - x \Vert = \Vert \alpha f(x) - f(x) \Vert$ as desired. Now at this point it seems like I have all of the facts required to assert that $f(\alpha x) = \alpha f(x)$, but I can't figure out how to formally state why without illegally appealing to visual intuition. Any ideas?",,"['linear-algebra', 'euclidean-geometry']"
77,Finding a basis of an infinite-dimensional vector space?,Finding a basis of an infinite-dimensional vector space?,,"The other day, my teacher was talking infinite-dimensional vector spaces and complications that arise when trying to find a basis for those. He mentioned that it's been proven that some (or all, do not quite remember) infinite-dimensional vector spaces have a basis (the result uses an Axiom of Choice, if I remember correctly), that is, an infinite list of linearly independent vectors, such that any element in the space can be written as a finite linear combination of them. However, my teacher mentioned that actually finding one is really complicated, and I got a sense that it was basically impossible, which reminded me of Banach-Tarski paradox, where it's technically 'possible' to decompose the sphere in a given paradoxical way, but this cannot be actually exhibited. So my question is, is the basis situation analogous to that, or is it actually possible to explicitly find a basis for infinite-dimensional vector spaces?","The other day, my teacher was talking infinite-dimensional vector spaces and complications that arise when trying to find a basis for those. He mentioned that it's been proven that some (or all, do not quite remember) infinite-dimensional vector spaces have a basis (the result uses an Axiom of Choice, if I remember correctly), that is, an infinite list of linearly independent vectors, such that any element in the space can be written as a finite linear combination of them. However, my teacher mentioned that actually finding one is really complicated, and I got a sense that it was basically impossible, which reminded me of Banach-Tarski paradox, where it's technically 'possible' to decompose the sphere in a given paradoxical way, but this cannot be actually exhibited. So my question is, is the basis situation analogous to that, or is it actually possible to explicitly find a basis for infinite-dimensional vector spaces?",,"['linear-algebra', 'hamel-basis']"
78,"On the order of elements of $\mathrm{GL}(2,q)$.",On the order of elements of .,"\mathrm{GL}(2,q)","There's a particular property of the elements of $\mathrm{GL}(2,q)$ , the general linear group of $n\times n$ matrices over a finite field of order $q$ , that I don't understand. I know that the order of $\mathrm{GL}(2,q)$ is $(q^2-1)(q^2-q)=q(q+1)(q-1)^2$ , since there are $q^2-1$ possible vectors for the first column, excluding the $0$ vector, and $q^2-q$ possible vector for the second column, excluding all multiples of the first. So the order of any element must divide $q(q+1)(q-1)^2$ by Langrange. However, there is a further detail that any element of $\mathrm{GL}(2,q)$ must have order dividing $q(q-1)$ or $(q-1)(q+1)$ . Is there a reason why one can narrow down the order to divide one of those smaller factors of $q(q+1)(q-1)^2$ ? Thanks!","There's a particular property of the elements of , the general linear group of matrices over a finite field of order , that I don't understand. I know that the order of is , since there are possible vectors for the first column, excluding the vector, and possible vector for the second column, excluding all multiples of the first. So the order of any element must divide by Langrange. However, there is a further detail that any element of must have order dividing or . Is there a reason why one can narrow down the order to divide one of those smaller factors of ? Thanks!","\mathrm{GL}(2,q) n\times n q \mathrm{GL}(2,q) (q^2-1)(q^2-q)=q(q+1)(q-1)^2 q^2-1 0 q^2-q q(q+1)(q-1)^2 \mathrm{GL}(2,q) q(q-1) (q-1)(q+1) q(q+1)(q-1)^2","['linear-algebra', 'abstract-algebra', 'finite-groups', 'finite-fields']"
79,Two Definitions of the Special Orthogonal Lie Algebra,Two Definitions of the Special Orthogonal Lie Algebra,,"I am encountering two definitions of the special orthogonal lie algebra, and I would like to know if they are equivalent, and if there are advantages to working with one over the other. If we begin with an $n$-dimensional vector space $V$ over a field $k$ and a chosen basis, we can define a bilinear form on $V$ by a matrix $S\in M_n(k)$, ie, let $\langle v,w\rangle=v^tSw$ for all $v,w\in V$.  Now $g\in GL_n(k)$ preserves the form ($\langle g(v),g(w)\rangle=\langle v,w\rangle$) if and only if $g^tSg=S$, so all such $g$ form a linear algebraic group $G$.  The tangent space at the identity of $G$ will be contained in that of $GL_n(k)$, so $T_eG\subset T_eGL_n(k)=M_n(k)$, and in fact, $T_eG=\{B\in M_n(k)\mid B^tS+SB=0\}$.  $T_eG$ becomes a lie algebra, $Lie(G)$, if we define the bracket to be the commutator of two matrices. Now, if $S=I_n$, it follows that $G=O_n(k)$ is the orthogonal group of matrices satisfying $g^tg=I_n$, and $Lie(G)=\mathfrak{so}_n$ is the lie algebra of antisymmetric matrices. In Humphrey's Introduction to Lie Algebras and Representation Theory , he defines $\mathfrak{so}_n$ to be all matrices $B$ satisyfing $B^tS+SB=0$, where $$ S=\begin{pmatrix} 1&0&0\\ 0&O&I_l\\ 0&I_l&O \end{pmatrix} \hspace{.5in}\text{or}\hspace{.5in} S=\begin{pmatrix} O&I_l\\ I_l&O \end{pmatrix} $$ depending on the parity of $n$.  The matrices obtained in this way are not antisymmetric, nor is the group $G$ preserving the form defined by $S$ the orthogonal group $O_n(k)$. Are the two groups obtained from considering different $S$ isomorphic?  Are the two lie algebras isomorphic?  If so, why would one prefer one form to the other?","I am encountering two definitions of the special orthogonal lie algebra, and I would like to know if they are equivalent, and if there are advantages to working with one over the other. If we begin with an $n$-dimensional vector space $V$ over a field $k$ and a chosen basis, we can define a bilinear form on $V$ by a matrix $S\in M_n(k)$, ie, let $\langle v,w\rangle=v^tSw$ for all $v,w\in V$.  Now $g\in GL_n(k)$ preserves the form ($\langle g(v),g(w)\rangle=\langle v,w\rangle$) if and only if $g^tSg=S$, so all such $g$ form a linear algebraic group $G$.  The tangent space at the identity of $G$ will be contained in that of $GL_n(k)$, so $T_eG\subset T_eGL_n(k)=M_n(k)$, and in fact, $T_eG=\{B\in M_n(k)\mid B^tS+SB=0\}$.  $T_eG$ becomes a lie algebra, $Lie(G)$, if we define the bracket to be the commutator of two matrices. Now, if $S=I_n$, it follows that $G=O_n(k)$ is the orthogonal group of matrices satisfying $g^tg=I_n$, and $Lie(G)=\mathfrak{so}_n$ is the lie algebra of antisymmetric matrices. In Humphrey's Introduction to Lie Algebras and Representation Theory , he defines $\mathfrak{so}_n$ to be all matrices $B$ satisyfing $B^tS+SB=0$, where $$ S=\begin{pmatrix} 1&0&0\\ 0&O&I_l\\ 0&I_l&O \end{pmatrix} \hspace{.5in}\text{or}\hspace{.5in} S=\begin{pmatrix} O&I_l\\ I_l&O \end{pmatrix} $$ depending on the parity of $n$.  The matrices obtained in this way are not antisymmetric, nor is the group $G$ preserving the form defined by $S$ the orthogonal group $O_n(k)$. Are the two groups obtained from considering different $S$ isomorphic?  Are the two lie algebras isomorphic?  If so, why would one prefer one form to the other?",,"['linear-algebra', 'differential-geometry', 'lie-groups', 'lie-algebras']"
80,Proving that the coefficients of the characteristic polynomial are the traces of the exterior powers,Proving that the coefficients of the characteristic polynomial are the traces of the exterior powers,,"Let $T$ be an endomorphism of a finite-dimensional vector space $V$. Let $$f(x)=x^n+c_1x^{n-1}+ \dots + c_n$$ be the characteristic polynomial of $T$. It is well known that $c_m=(-1)^m\text{tr}(\bigwedge^mT)$. If the base field is $\mathbf{C}$, then we can prove it using a density argument. The statement is true for diagonalizable matrices, which are dense in $M_n(\mathbf{C})$. This actually enough to prove it general, but I don't find it very illuminating. I would like to see an abstract proof of this result. Thank you!","Let $T$ be an endomorphism of a finite-dimensional vector space $V$. Let $$f(x)=x^n+c_1x^{n-1}+ \dots + c_n$$ be the characteristic polynomial of $T$. It is well known that $c_m=(-1)^m\text{tr}(\bigwedge^mT)$. If the base field is $\mathbf{C}$, then we can prove it using a density argument. The statement is true for diagonalizable matrices, which are dense in $M_n(\mathbf{C})$. This actually enough to prove it general, but I don't find it very illuminating. I would like to see an abstract proof of this result. Thank you!",,"['linear-algebra', 'multilinear-algebra']"
81,"Difference between Gilbert Strang's ""Introduction to Linear Algebra"" and his ""Linear Algebra and Its Applications""?","Difference between Gilbert Strang's ""Introduction to Linear Algebra"" and his ""Linear Algebra and Its Applications""?",,"Could someone please explain the difference between Gilbert Strang's ""Introduction to Linear Algebra"" and his ""Linear Algebra and Its Applications""? Thank you.","Could someone please explain the difference between Gilbert Strang's ""Introduction to Linear Algebra"" and his ""Linear Algebra and Its Applications""? Thank you.",,"['linear-algebra', 'reference-request']"
82,Intuition for complex eigenvalues,Intuition for complex eigenvalues,,"The eigenvalues of a rotation matrix are complex numbers. I understand that they cannot be real numbers because when you rotate something no direction stays the same. My question What is the intuition that the eigenvalues are complex? Why do they exist at all for rotation matrices? I mean it is not so that every time a calculation is not possible the result is complex (dividing by 0 is not possible at all - the result is not real, but it is not complex either!). The complex numbers seem to cover some middle ground here, but I don't understand how and why they come into play - there don't seem to be any square roots taken from negative numbers...)","The eigenvalues of a rotation matrix are complex numbers. I understand that they cannot be real numbers because when you rotate something no direction stays the same. My question What is the intuition that the eigenvalues are complex? Why do they exist at all for rotation matrices? I mean it is not so that every time a calculation is not possible the result is complex (dividing by 0 is not possible at all - the result is not real, but it is not complex either!). The complex numbers seem to cover some middle ground here, but I don't understand how and why they come into play - there don't seem to be any square roots taken from negative numbers...)",,"['linear-algebra', 'matrices', 'intuition', 'complex-numbers', 'eigenvalues-eigenvectors']"
83,"Is there any distinction between these products: scalar, dot, inner?","Is there any distinction between these products: scalar, dot, inner?",,"I hope you will forgive a math question that comes up in physics contexts where language is loose.  This question migrated from Physics SE. I'm finding that I sometimes don't know what kind of product a physics author means. There is a product which can be defined on a vector space that takes two vectors and returns a scalar.  There is a product that takes a vector and a covector and returns a scalar. Is there an agreed-upon language for distinctions between the three words ""scalar product"", ""dot product"", and ""inner product""? I can mostly gloss over the language as it's usually clear from context.  But not always. Addition: I think we can all agree what a scalar product is:  a map taking two vectors and returning a scalar.  A vector space does not have to have a scalar product.  To have a scalar product, a vector space needs a metric. And we can all agree that there is a "" contraction "" taking one member of a vector space, and one member of it dual, and retuning a scalar, where the dual can be considered to be a scalar-valued linear function on the vector space.  Does this ""product"" have a name? What I would like to know: what are the definitions of ""dot product"" and ""inner product""? Addition #2: A commenter (@Hunter) cited a link that points out that physics authors do not all agree on what an inner product is.   The following quotation is cited:  ""If the inner product is taken of two vectors, one must be a contravariant vector and the other a covariant vector. The inner product of two covariant or two contravariant vectors is not defined."" [Spherical Astronomy, Robin Michael Green page 495.]  Some say the inner product takes $V^*\times V$ into scalars, others say $V \times V$.  (Consensus among physicists here is that ""inner"" = ""scalar"", i.e. the domain for both is $V \times V$., and ""dot"" = ""scalar"", usually reserved for Euclidean geometry.) Furthermore, most quantum mechanics texts call $\langle\psi\mid\chi\rangle$ an inner product, whereas by my understanding this is a mapping of one bra and one ket to scalars:  a contraction of a vector with its dual.  I understand that there is an isomorphism in this case, so there is no ambiguity, but the terminology adds to confusion. One thing is certain: some authors don't tell us which definition they are using, and it's sometimes not clear from context. Read more: http://www.physicsforums.com/showthread.php?t=735158","I hope you will forgive a math question that comes up in physics contexts where language is loose.  This question migrated from Physics SE. I'm finding that I sometimes don't know what kind of product a physics author means. There is a product which can be defined on a vector space that takes two vectors and returns a scalar.  There is a product that takes a vector and a covector and returns a scalar. Is there an agreed-upon language for distinctions between the three words ""scalar product"", ""dot product"", and ""inner product""? I can mostly gloss over the language as it's usually clear from context.  But not always. Addition: I think we can all agree what a scalar product is:  a map taking two vectors and returning a scalar.  A vector space does not have to have a scalar product.  To have a scalar product, a vector space needs a metric. And we can all agree that there is a "" contraction "" taking one member of a vector space, and one member of it dual, and retuning a scalar, where the dual can be considered to be a scalar-valued linear function on the vector space.  Does this ""product"" have a name? What I would like to know: what are the definitions of ""dot product"" and ""inner product""? Addition #2: A commenter (@Hunter) cited a link that points out that physics authors do not all agree on what an inner product is.   The following quotation is cited:  ""If the inner product is taken of two vectors, one must be a contravariant vector and the other a covariant vector. The inner product of two covariant or two contravariant vectors is not defined."" [Spherical Astronomy, Robin Michael Green page 495.]  Some say the inner product takes $V^*\times V$ into scalars, others say $V \times V$.  (Consensus among physicists here is that ""inner"" = ""scalar"", i.e. the domain for both is $V \times V$., and ""dot"" = ""scalar"", usually reserved for Euclidean geometry.) Furthermore, most quantum mechanics texts call $\langle\psi\mid\chi\rangle$ an inner product, whereas by my understanding this is a mapping of one bra and one ket to scalars:  a contraction of a vector with its dual.  I understand that there is an isomorphism in this case, so there is no ambiguity, but the terminology adds to confusion. One thing is certain: some authors don't tell us which definition they are using, and it's sometimes not clear from context. Read more: http://www.physicsforums.com/showthread.php?t=735158",,"['terminology', 'vectors', 'linear-algebra']"
84,Why can I solve an impossible equation using linear algebra?,Why can I solve an impossible equation using linear algebra?,,"I am currently learning matlab and linear algebra side by side and I stumbled upon this example from mathworks A = [1 2 0; 0 4 3]; b = [8; 18]; x = A\b  x = 3×1       0 4.0000 0.6667 which in my mind translates to $$ A = \left[ \begin{matrix} 1 & 2 & 0 \\ 0 & 4 & 3 \end{matrix}\right] B = \left[ \begin{matrix} 8 \\ 18 \end{matrix}\right] x = \left[ \begin{matrix} a \\ b \\ c \end{matrix}\right] $$ $$ Ax = \left[ \begin{matrix} 1 & 2 & 0 \\ 0 & 4 & 3 \end{matrix}\right] \times \left[ \begin{matrix} a \\ b \\ c \end{matrix}\right]  = \left[ \begin{matrix} a + 2b \\ 4b + 3c \end{matrix}\right] $$ which boils down to $$ \left[ \begin{matrix} a + 2b \\ 4b + 3c \end{matrix}\right] = \left[ \begin{matrix} 8\\ 18 \end{matrix}\right] \Rightarrow \begin{matrix}a + 2b = 8 \\4b + 3c = 18\end{matrix} $$ which is an equation with 3 unknown (a, b and c) with two equations, which is impossible! Yes there is a solution $$ x = \left[ \begin{matrix} 0 \\ 4 \\ 2/3 \end{matrix}\right] $$ How can I solve an impossible equation (three unknown and two equations) using linear algebra?","I am currently learning matlab and linear algebra side by side and I stumbled upon this example from mathworks A = [1 2 0; 0 4 3]; b = [8; 18]; x = A\b  x = 3×1       0 4.0000 0.6667 which in my mind translates to which boils down to which is an equation with 3 unknown (a, b and c) with two equations, which is impossible! Yes there is a solution How can I solve an impossible equation (three unknown and two equations) using linear algebra?","
A = \left[
\begin{matrix}
1 & 2 & 0 \\
0 & 4 & 3
\end{matrix}\right]
B = \left[
\begin{matrix}
8 \\ 18
\end{matrix}\right]
x = \left[
\begin{matrix}
a \\ b \\ c
\end{matrix}\right]
 
Ax = \left[
\begin{matrix}
1 & 2 & 0 \\
0 & 4 & 3
\end{matrix}\right] \times \left[
\begin{matrix}
a \\ b \\ c
\end{matrix}\right] 
= \left[
\begin{matrix}
a + 2b \\ 4b + 3c
\end{matrix}\right]
 
\left[
\begin{matrix}
a + 2b \\ 4b + 3c
\end{matrix}\right] = \left[
\begin{matrix}
8\\ 18
\end{matrix}\right] \Rightarrow \begin{matrix}a + 2b = 8 \\4b + 3c = 18\end{matrix}
 
x = \left[
\begin{matrix}
0 \\ 4 \\ 2/3
\end{matrix}\right]
",['linear-algebra']
85,Show that $ e^{A+B}=e^A e^B$,Show that, e^{A+B}=e^A e^B,"If $A$ and $B$ are $n\times n$ matrices such that $AB = BA$ (that is, $A$ and $B$ commute), show that $$ e^{A+B}=e^A e^B$$ Note that $A$ and $B$ do NOT have to be diagonalizable.","If $A$ and $B$ are $n\times n$ matrices such that $AB = BA$ (that is, $A$ and $B$ commute), show that $$ e^{A+B}=e^A e^B$$ Note that $A$ and $B$ do NOT have to be diagonalizable.",,"['linear-algebra', 'sequences-and-series', 'matrices', 'exponential-function']"
86,Method to reverse a Kronecker product,Method to reverse a Kronecker product,,"Let's say I have two simple vectors: $[0, 1]$ and $[1, 0]$. Their Kronecker product would be $[0, 0, 1, 0]$. Let's say I have only the Kronecker product. How can I find the two initial vectors back? If my two vectors are written as : $[a, b]$ and $[c, d]$, the (given) Kronecker product is: $$[ac, ad, bc, bd] = [k_0, k_1, k_2, k_3]$$ So I have a system of four non linear equations that I wish to solve: $$\begin{align*} ac &= k_0\\ ad&= k_1\\ bc&= k_2\\ bd &=k_3. \end{align*}$$ I am looking for a general way to solve this problem for any number of initial vectors in $\mathbb{C}^2$ (leading my number of variables to $2n$ and my equations to $2^n$ if I have $n$ vectors). So here are a few specific questions: What is the common name of this problem? If a general solution is known, what is its complexity class? Does the fact that I have more and more equations when $n$ goes up compared to the number of variables help? (Note: I really didn't know what to put as a tag.)","Let's say I have two simple vectors: $[0, 1]$ and $[1, 0]$. Their Kronecker product would be $[0, 0, 1, 0]$. Let's say I have only the Kronecker product. How can I find the two initial vectors back? If my two vectors are written as : $[a, b]$ and $[c, d]$, the (given) Kronecker product is: $$[ac, ad, bc, bd] = [k_0, k_1, k_2, k_3]$$ So I have a system of four non linear equations that I wish to solve: $$\begin{align*} ac &= k_0\\ ad&= k_1\\ bc&= k_2\\ bd &=k_3. \end{align*}$$ I am looking for a general way to solve this problem for any number of initial vectors in $\mathbb{C}^2$ (leading my number of variables to $2n$ and my equations to $2^n$ if I have $n$ vectors). So here are a few specific questions: What is the common name of this problem? If a general solution is known, what is its complexity class? Does the fact that I have more and more equations when $n$ goes up compared to the number of variables help? (Note: I really didn't know what to put as a tag.)",,['linear-algebra']
87,Show that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$,Show that,\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B),"I know about the fact that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$, where $A$ and $B$ are $m \times n$ matrices. But somehow, I don't find this as intuitive as the multiplication version of this fact. The rank of $A$ plus the rank of $B$ could have well more than the columns of $(A+B)$! How can I show to prove that this really is true?","I know about the fact that $\operatorname{rank}(A+B) \leq \operatorname{rank}(A) + \operatorname{rank}(B)$, where $A$ and $B$ are $m \times n$ matrices. But somehow, I don't find this as intuitive as the multiplication version of this fact. The rank of $A$ plus the rank of $B$ could have well more than the columns of $(A+B)$! How can I show to prove that this really is true?",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
88,Orthogonal projection of a point onto a line,Orthogonal projection of a point onto a line,,"How would I go about solving the following problem? Find an orthogonal projection of a point T$(-4,5)$ onto a line $\frac{x}{3}+\frac{y}{-5}=1$.","How would I go about solving the following problem? Find an orthogonal projection of a point T$(-4,5)$ onto a line $\frac{x}{3}+\frac{y}{-5}=1$.",,"['linear-algebra', 'geometry', 'matrices', 'analytic-geometry']"
89,Is the closure of any linear subspace of a normed space $X$ again a linear subspace of $X$?,Is the closure of any linear subspace of a normed space  again a linear subspace of ?,X X,Let $X$ be a normed linear space with norm $||\cdot||$ and $A \neq \emptyset$ a linear subspace of $X$ . Prove that $\bar{A}$ is also a linear subspace of $X$ . I'm not able to visualize the additive and multiplicative closure of the new two points added to the subspace $\bar{A}$ .,Let be a normed linear space with norm and a linear subspace of . Prove that is also a linear subspace of . I'm not able to visualize the additive and multiplicative closure of the new two points added to the subspace .,X ||\cdot|| A \neq \emptyset X \bar{A} X \bar{A},"['linear-algebra', 'general-topology', 'vector-spaces', 'normed-spaces']"
90,What is the notation for the set of all $m\times n$ matrices?,What is the notation for the set of all  matrices?,m\times n,"Given that $\mathbb{R}^n$ is the notation used for $n$ -dimensional vectors, is there an accepted equivalent notation for matrices?","Given that is the notation used for -dimensional vectors, is there an accepted equivalent notation for matrices?",\mathbb{R}^n n,"['linear-algebra', 'matrices', 'notation']"
91,Prove that $n+1$ vectors in $\mathbb{R}^n$ cannot be linearly independent,Prove that  vectors in  cannot be linearly independent,n+1 \mathbb{R}^n,"I was looking for a short snazzy proof on the following statement: n+1 vectors in $\mathbb{R}^n$ cannot be linearly independent A student of mine asked this today morning and I couldn't come up with a proof solely from the definition of linear independence . From a higher level perspective, I explained that if I put the vectors in a matrix then if the only null space entry is the zero vector, then the vectors are independent but since we have one extra column than row and that the row and column rank are equal, there is no way we can have $n+1$ as the rank of the matrix and hence from Rank-Nullity theorem, the dimension of the Nullspace is at least one which implies that there is a combination of the vectors where not all the scalar multiples in the definition are 0 but yet we get a zero as the linear combination. The student hasn't completely learnt the fundamental subspaces yet so I am not sure he grasped what I was saying. Is there a cleaner proof? EDIT: I am stunned how many beautiful answers I got with so much diversity.","I was looking for a short snazzy proof on the following statement: n+1 vectors in $\mathbb{R}^n$ cannot be linearly independent A student of mine asked this today morning and I couldn't come up with a proof solely from the definition of linear independence . From a higher level perspective, I explained that if I put the vectors in a matrix then if the only null space entry is the zero vector, then the vectors are independent but since we have one extra column than row and that the row and column rank are equal, there is no way we can have $n+1$ as the rank of the matrix and hence from Rank-Nullity theorem, the dimension of the Nullspace is at least one which implies that there is a combination of the vectors where not all the scalar multiples in the definition are 0 but yet we get a zero as the linear combination. The student hasn't completely learnt the fundamental subspaces yet so I am not sure he grasped what I was saying. Is there a cleaner proof? EDIT: I am stunned how many beautiful answers I got with so much diversity.",,['linear-algebra']
92,Linear independency before and after Linear Transformation,Linear independency before and after Linear Transformation,,"If we are given some linearly dependent vectors, would the T of those vectors necessarily be dependent (given a transformation from $R^n$ to $R^p$)? And if we are given some linearly independent vectors, would T of those vectors necessarily be independent (given a transformation from $R^n$ to $R^p$)? The answers are presumably no and no, but I am struggling to figure out why. Thanks!","If we are given some linearly dependent vectors, would the T of those vectors necessarily be dependent (given a transformation from $R^n$ to $R^p$)? And if we are given some linearly independent vectors, would T of those vectors necessarily be independent (given a transformation from $R^n$ to $R^p$)? The answers are presumably no and no, but I am struggling to figure out why. Thanks!",,['linear-algebra']
93,shortcut for finding a inverse  of matrix,shortcut for finding a inverse  of matrix,,"I need tricks or shortcuts to find the inverse of  $2 \times 2$ and $3 \times 3$ matrices. I have to take a time-based exam, in which I have to find the inverse of square matrices.","I need tricks or shortcuts to find the inverse of  $2 \times 2$ and $3 \times 3$ matrices. I have to take a time-based exam, in which I have to find the inverse of square matrices.",,"['linear-algebra', 'matrices']"
94,What is the implication of Perron Frobenius Theorem?,What is the implication of Perron Frobenius Theorem?,,"The Perron Frobenius theorem states: Any square matrix $A$ with positive entries has a unique eigenvector with   positive entries (up to a multiplication by a positive scalar), and   the corresponding eigenvalue has multiplicity one and is strictly   greater than the absolute value of any other eigenvalue. So I tempted fate using this matrix: $$ A =\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$$ I find my eigenvalues to be $\lambda_1 = 0, \lambda_2 = 2$ Now I find my eigenvector, taking $v_{11} = 1$, $v_{21} = 1$ I find  $v_1 = \begin{bmatrix} v_{11} & v_{12} \end{bmatrix}$ = $\begin{bmatrix} 1 & -1 \end{bmatrix}$ $v_1 = \begin{bmatrix} v_{21} & v_{22} \end{bmatrix}$ = $\begin{bmatrix} 1 & 1 \end{bmatrix}$ This verifies the Perron-Frobenius theorem. Now what is the great implication that every positive square matrix has a real eigenvector with an eigenvalue that is the largest of all eigenvalues? Can someone show me an application of this theorem?","The Perron Frobenius theorem states: Any square matrix $A$ with positive entries has a unique eigenvector with   positive entries (up to a multiplication by a positive scalar), and   the corresponding eigenvalue has multiplicity one and is strictly   greater than the absolute value of any other eigenvalue. So I tempted fate using this matrix: $$ A =\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$$ I find my eigenvalues to be $\lambda_1 = 0, \lambda_2 = 2$ Now I find my eigenvector, taking $v_{11} = 1$, $v_{21} = 1$ I find  $v_1 = \begin{bmatrix} v_{11} & v_{12} \end{bmatrix}$ = $\begin{bmatrix} 1 & -1 \end{bmatrix}$ $v_1 = \begin{bmatrix} v_{21} & v_{22} \end{bmatrix}$ = $\begin{bmatrix} 1 & 1 \end{bmatrix}$ This verifies the Perron-Frobenius theorem. Now what is the great implication that every positive square matrix has a real eigenvector with an eigenvalue that is the largest of all eigenvalues? Can someone show me an application of this theorem?",,"['linear-algebra', 'matrices']"
95,Determinant of matrix times a constant.,Determinant of matrix times a constant.,,Prove that $\det(kA) = k^n \det(A)$ for and ($n \times n$) matrix. I have tried looking at this a couple of ways but can't figure out where to start. It's confusing to me since the equation for a determinant is such a weird summation.,Prove that $\det(kA) = k^n \det(A)$ for and ($n \times n$) matrix. I have tried looking at this a couple of ways but can't figure out where to start. It's confusing to me since the equation for a determinant is such a weird summation.,,"['linear-algebra', 'matrices']"
96,What is the use of the Dot Product of two vectors?,What is the use of the Dot Product of two vectors?,,"Suppose you have two vectors a and b that you want to take the dot product of, now this is done quite simply by taking each corresponding coordinate of each vector, multiplying them and then adding the result together. At the end of performing our operation we are left with a constant number. My question therefore is what can we do with this number,why do we calculate it so to speak? I mean it seems almost useless to me compared with the cross product of two vectors (where you end up with an actual vector).","Suppose you have two vectors a and b that you want to take the dot product of, now this is done quite simply by taking each corresponding coordinate of each vector, multiplying them and then adding the result together. At the end of performing our operation we are left with a constant number. My question therefore is what can we do with this number,why do we calculate it so to speak? I mean it seems almost useless to me compared with the cross product of two vectors (where you end up with an actual vector).",,['linear-algebra']
97,Proving the relation $\det(I + xy^T ) = 1 + x^Ty$,Proving the relation,\det(I + xy^T ) = 1 + x^Ty,Let $x$ and $y$ denote two length-$n$ column vectors. Prove that $$\det(I + xy^T ) = 1 + x^Ty$$ Is Sylvester's determinant theorem an extension of the problem? Is the approach the same?,Let $x$ and $y$ denote two length-$n$ column vectors. Prove that $$\det(I + xy^T ) = 1 + x^Ty$$ Is Sylvester's determinant theorem an extension of the problem? Is the approach the same?,,"['linear-algebra', 'matrices', 'determinant']"
98,Intuitive explanation of why $\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V$,Intuitive explanation of why,\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V,"I'm having a hard time truly understanding the meaning of $\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V$ where $V$ is the domain of a linear transformation $T:V\to W$. I've used this equation several times in many problems, and I've gone over the proof and I believe that I fully understand it, but I don't understand the intuitive reasoning behind it. I'd appreciate an intuitive explanation of it. Just to be clear, I do understand the equation itself, I am able to use it, and I know how to prove it; my question is what is the meaning of this equation from a linear algebra perspective.","I'm having a hard time truly understanding the meaning of $\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V$ where $V$ is the domain of a linear transformation $T:V\to W$. I've used this equation several times in many problems, and I've gone over the proof and I believe that I fully understand it, but I don't understand the intuitive reasoning behind it. I'd appreciate an intuitive explanation of it. Just to be clear, I do understand the equation itself, I am able to use it, and I know how to prove it; my question is what is the meaning of this equation from a linear algebra perspective.",,['linear-algebra']
99,Relation of this antisymmetric matrix $r = \left(\begin{smallmatrix}0 &1\\-1&0\end{smallmatrix}\right)$ to $i$,Relation of this antisymmetric matrix  to,r = \left(\begin{smallmatrix}0 &1\\-1&0\end{smallmatrix}\right) i,"I was reviewing some matrices and found this interesting if $r = \begin{pmatrix} 0&1\\ -1&0 \end{pmatrix}$ then $rr=-I$, also $$\exp{(\theta r)} = \cos\theta I + \sin\theta r$$ No wonder, the matrix $R(\theta) = e^{\theta r}$ is the 2d rotation matrix, just like $e^{i\theta}$ rotates a vector in the Argand plane. I have a very cursory knowledge of complex analysis, so I would like to know where I can find the details, i.e what is the unifying theme and in which literature can it be found.","I was reviewing some matrices and found this interesting if $r = \begin{pmatrix} 0&1\\ -1&0 \end{pmatrix}$ then $rr=-I$, also $$\exp{(\theta r)} = \cos\theta I + \sin\theta r$$ No wonder, the matrix $R(\theta) = e^{\theta r}$ is the 2d rotation matrix, just like $e^{i\theta}$ rotates a vector in the Argand plane. I have a very cursory knowledge of complex analysis, so I would like to know where I can find the details, i.e what is the unifying theme and in which literature can it be found.",,"['linear-algebra', 'matrices', 'reference-request', 'complex-numbers']"
