,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,General expressions for $\mathcal{L}(n)=\int_{0}^{\infty}\operatorname{Ci}(x)^n\text{d}x$,General expressions for,\mathcal{L}(n)=\int_{0}^{\infty}\operatorname{Ci}(x)^n\text{d}x,"Define $$\operatorname{Ci}(x)=-\int_{x}^{ \infty} \frac{\cos(y)}{y}\text{d}y.$$ It is easy to show $$ \mathcal{L}(1)=\int_{0}^{\infty}\operatorname{Ci}(x)\text{d}x=0 $$ and $$\mathcal{L}(2)=\int_{0}^{\infty}\operatorname{Ci}(x)^2\text{d}x =\frac{\pi}{2}.$$ $\mathcal{L}(3),\mathcal{L}(4)$ is a little bit non-trivial. We have two claims(take a look here to find more details): $$\begin{aligned} &\mathcal{L}(3)=-\frac{3\pi}{2}\ln2 \\ &\mathcal{L}(4)=3\pi\operatorname{Li}_2 \left ( \frac{2}{3}  \right )+\frac{3\pi}{2}\ln^23 \end{aligned}$$ Where $\operatorname{Li}$ are polylogarithms , they are defined by $\displaystyle{\operatorname{Li}_n(z) =\sum_{k=1}^{\infty} \frac{z^k}{k^n}}$ for $|z|<1$ . $\mathcal{L}(5)$ is much more non-trivial. We have $$ \mathcal{L}(5)=\int_{0}^{\infty}\operatorname{Ci}(x)^5\text{d}x =-\frac{15\pi^3}{8}\ln(2)-\frac{15\pi}{2}\ln(2)^3 -\frac{45\pi}{4}\operatorname{Li}_2\left ( \frac{1}{4}  \right )\ln(2) -\frac{45\pi}{4}\operatorname{Li}_3\left ( \frac{1}{4}  \right )  -\frac{15\pi}{16}\zeta(3). $$ Where $\zeta(n)=\operatorname{Li}_n(1)$ for $\Re(n)>1$ . My question: How can we find alternate generalizations? I believe that $\mathcal{L}(6)$ can be expressed by using ordinary polylogarithms( $\mathcal{L}(7)$ seems impossible). We can also find the closed-forms of following integrals: $$\int_{0}^{\infty}\operatorname{Ci}(x)^4\cos(x)\text{d}x,\int_{0}^{\infty}\operatorname{Ci}(x)^2\frac{\operatorname{Si}(2x)}{x} \cos(x)\text{d}x$$ where $\displaystyle{\operatorname{Si}(x)=\int_{0}^{x} \frac{\sin(t)}{t}\text{d}t}.$ Update 1: Define $\operatorname{si}(x)+\operatorname{Si}(x)=\frac{\pi}{2}$ . Here are some results: $$\begin{aligned} &\int_{0}^{\infty}\operatorname{si}(x)\text{d}x=1\\ &\int_{0}^{\infty}\operatorname{si}(x)^2\text{d}x=\frac{\pi}{2}\\ &\int_{0}^{\infty}\operatorname{si}(x)^3\text{d}x=\frac{\pi^2}{4} -\frac{3}{2}\ln^22-\frac{3}{4}  \operatorname{Li}_2\left ( \frac{1}{4}  \right )\\ &\int_{0}^{\infty}\operatorname{si}(x)^4\text{d}x= \frac{\pi^3}{4} -3\pi\ln^22-\frac{3\pi}{2}  \operatorname{Li}_2\left ( \frac{1}{4}  \right ) \end{aligned}$$ Update 2: A useful fourier transform $$\int_{0}^{\infty}\operatorname{Ci}(x)^3\cos(a x)\text{d}x =\begin{cases} \color{Red}{\frac{\pi  \text{Li}_2\left(\frac{1-a}{3}\right)}{4 a}}+\frac{\pi  \text{Li}_2\left(\frac{a-1}{a-2}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{a+1}{3 (a-1)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a-1}{a+1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{a+2}\right)}{2 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{3}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a-1}{3 (a+1)}\right)}{4 a}+\frac{\pi  \log ^2(2-a)}{4 a}-\frac{\pi  \log ^2(a+2)}{4 a}+\frac{\pi  \log (3) \log (a-1)}{4 a}+\frac{\pi  \log (3) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log (a-2)}{4 a}-\frac{\pi  \log (3) \tanh ^{-1}\left(\frac{a}{2}\right)}{2 a} &  (0\le a\le1),\\ \color{Red}{\frac{\pi  \text{Li}_2\left(\frac{a^2}{a^2-1}\right)}{4 a}}+\frac{\pi  \log \left(-\frac{a}{a+1}\right) \log \left(\frac{1}{1-a^2}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{a}{2}\right)}{2 a}+\frac{\pi  \text{Li}_2(1-a)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (1-a)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{3}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{a}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (a+1)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{1}{2}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{1}{1-a}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{1}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2(-a)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{1}{a+1}\right)}{4 a}-\frac{3 \pi  \text{Li}_2\left(\frac{a}{a+1}\right)}{4 a}-\frac{7 \pi ^3}{24 a}+\frac{3 \pi  \log ^2(2)}{8 a}+\frac{\pi  \log ^2(a)}{8 a}-\frac{\pi  \log ^2(a+1)}{2 a}+\frac{\pi  \log (2) \log (a-1)}{4 a}+\frac{\pi  \log (2) \log (a+1)}{4 a}-\frac{\pi  \log (2) \log (a)}{2 a}-\frac{\pi  \log (2) \log (a+2)}{2 a}+\frac{\pi  \log \left(\frac{a+2}{a+1}\right) \log \left(\frac{1}{(a+1)^2}\right)}{4 a}+\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}+\frac{\pi  \log (3) \log (a+2)}{4 a}+\frac{\pi  \log (a) \log (a+2)}{2 a}-\frac{\pi  \log (a) \log (a+1)}{2 a}-\frac{i \pi ^2 \log \left(\frac{1}{1-a}\right)}{4 a}-\frac{\pi  \log (3) \log (a-1)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(-\frac{a}{a+1}\right)}{4 a}-\frac{\pi  \log (a-1) \log (a+2)}{4 a}-\frac{\pi  \log (a+1) \log (a+2)}{4 a}  & (1\le a\le3),  \\   \color{Red}{-\frac{\pi  \text{Li}_2\left(\frac{a^2}{a^2-1}\right)}{4 a}}-\frac{\pi  \log \left(-\frac{a}{a+1}\right) \log \left(\frac{1}{1-a^2}\right)}{4 a}+\frac{\pi  \text{Li}_2(-2)}{4 a}+\frac{\pi  \text{Li}_2(2)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{2}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{1-a}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{a+1}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{a}{a+1}\right)}{2 a}-\frac{\pi  \text{Li}_2\left(-\frac{a}{2}\right)}{2 a}-\frac{\pi  \text{Li}_2(1-a)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (1-a)}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a-2}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{3}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2(a-1)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (a+1)}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}+\frac{\pi ^3}{3 a}-\frac{\pi  \log ^2(2)}{4 a}+\frac{\pi  \log ^2(a+1)}{2 a}+\frac{i \pi ^2 \log (2)}{4 a}+\frac{\pi  \log (2) \log (a)}{2 a}+\frac{\pi  \log (2) \log (a+2)}{2 a}-\frac{\pi  \log (2) \log (a-1)}{4 a}-\frac{\pi  \log (2) \log (a+1)}{4 a}+\frac{i \pi ^2 \log \left(\frac{1}{1-a}\right)}{4 a}+\frac{\pi  \log (3) \log (a-2)}{4 a}+\frac{\pi  \log (a-2) \log (a-1)}{4 a}+\frac{\pi  \log \left(\frac{a+2}{a+1}\right) \log \left(-\frac{1}{a+1}\right)}{4 a}+\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(-\frac{a}{a+1}\right)}{4 a}+\frac{\pi  \log (a-1) \log (a+2)}{4 a}+\frac{\pi  \log (a+1) \log (a+2)}{4 a}-\frac{\pi  \log (a) \log (a+2)}{2 a}-\frac{\pi  \log (a-2) \log \left(\frac{1}{a-1}\right)}{4 a}-\frac{\pi  \log (3) \log \left(\frac{a+2}{a-1}\right)}{4 a}-\frac{\pi  \log (2-a) \log (a-1)}{4 a}-\frac{\pi  \log (a) \log \left(\frac{1}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log \left(\frac{a-2}{a+1}\right)}{4 a}-\frac{\pi  \log \left(\frac{1}{(a+1)^2}\right) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log (a+1)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}& (a\ge3). \end{cases}$$ Update 3: Common fourier transforms $$\begin{aligned} &1.\int_{0}^{\infty}\operatorname{Ci}(x)\cos(\omega x)\text{d}x= \begin{cases}   0 &(0\le\omega<1), \\ \displaystyle{ -\frac{\pi}{4}  }&(\omega=1), \\ \displaystyle{ -\frac{\pi}{2\omega}  }&(\omega>1). \end{cases}\\ &2.\int_{0}^{\infty}\operatorname{Ci}(x)\sin(\omega x)\text{d}x= \begin{cases}  \displaystyle{-\frac{\ln(1-\omega^2)}{2\omega}}  &(0\le\omega<1), \\ \displaystyle{ +\infty  }&(\omega=1), \\ \displaystyle{-\frac{\ln(\omega^2-1)}{2\omega} }&(\omega>1). \end{cases}\\ &3.\int_{0}^{\infty}\operatorname{Ci}(x)^2\cos(\omega x)\text{d}x= \begin{cases} \displaystyle{ \frac{\pi\ln(1+\omega)}{2\omega} }&(0\le\omega\le2), \\ \displaystyle{ \frac{\pi\ln(\omega^2-1)}{2\omega} }&(\omega\ge2). \end{cases}\\ &4.\int_{0}^{\infty}\operatorname{si}(x)\sin(\omega x)\text{d}x= \begin{cases}   0 &(0\le\omega<1), \\ \displaystyle{ \frac{\pi}{4}  }&(\omega=1), \\ \displaystyle{ \frac{\pi}{2\omega}  }&(\omega>1). \end{cases}\\ &5.\int_{0}^{\infty}\operatorname{si}(x)\cos(\omega x)\text{d}x= \begin{cases}  \displaystyle{\frac{1}{2\omega}\ln\left ( \frac{1+\omega}{1-\omega}  \right ) }  &(0\le\omega<1), \\ \displaystyle{ +\infty  }&(\omega=1), \\ \displaystyle{\frac{1}{2\omega}\ln\left ( \frac{\omega+1}{\omega-1}  \right ) }&(\omega>1). \end{cases}\\ &6.\int_{0}^{\infty}\operatorname{si}(x)^2\cos(\omega x)\text{d}x= \begin{cases} \displaystyle{ \frac{\pi\ln(1+\omega)}{2\omega} }&(0\le\omega\le2), \\ \displaystyle{ \frac{\pi}{2\omega}\ln\left ( \frac{\omega+1}{\omega-1}  \right ) }&(\omega\ge2). \end{cases}\\ &7.\int_{0}^{\infty}\frac{\operatorname{Si}(x)}{x}\cos(\omega x)\text{d}x= \begin{cases}  \displaystyle{-\frac{\pi}{2}\ln(\omega)}  &(0<\omega\le1), \\ \displaystyle{0 }&(\omega\ge1). \end{cases}\\ \end{aligned}$$ Definition: Functions $\operatorname{Si}_n(x)$ are defined by $$\operatorname{Si}_0(x)=\sin(x),\operatorname{Si}_n(x) =\int_{0}^{x} \frac{\operatorname{Si}_{n-1}(t)}{t}\text{d}t.$$ And we are able to get $$ \int_{0}^{\infty}\frac{\operatorname{Si}_2(x)\operatorname{si}(x)^2}{x} \text{d} x=\frac{7\pi^5}{1440}. $$","Define It is easy to show and is a little bit non-trivial. We have two claims(take a look here to find more details): Where are polylogarithms , they are defined by for . is much more non-trivial. We have Where for . My question: How can we find alternate generalizations? I believe that can be expressed by using ordinary polylogarithms( seems impossible). We can also find the closed-forms of following integrals: where Update 1: Define . Here are some results: Update 2: A useful fourier transform Update 3: Common fourier transforms Definition: Functions are defined by And we are able to get","\operatorname{Ci}(x)=-\int_{x}^{
\infty} \frac{\cos(y)}{y}\text{d}y. 
\mathcal{L}(1)=\int_{0}^{\infty}\operatorname{Ci}(x)\text{d}x=0
 \mathcal{L}(2)=\int_{0}^{\infty}\operatorname{Ci}(x)^2\text{d}x
=\frac{\pi}{2}. \mathcal{L}(3),\mathcal{L}(4) \begin{aligned}
&\mathcal{L}(3)=-\frac{3\pi}{2}\ln2 \\
&\mathcal{L}(4)=3\pi\operatorname{Li}_2
\left ( \frac{2}{3}  \right )+\frac{3\pi}{2}\ln^23
\end{aligned} \operatorname{Li} \displaystyle{\operatorname{Li}_n(z)
=\sum_{k=1}^{\infty} \frac{z^k}{k^n}} |z|<1 \mathcal{L}(5) 
\mathcal{L}(5)=\int_{0}^{\infty}\operatorname{Ci}(x)^5\text{d}x
=-\frac{15\pi^3}{8}\ln(2)-\frac{15\pi}{2}\ln(2)^3
-\frac{45\pi}{4}\operatorname{Li}_2\left ( \frac{1}{4}  \right )\ln(2)
-\frac{45\pi}{4}\operatorname{Li}_3\left ( \frac{1}{4}  \right ) 
-\frac{15\pi}{16}\zeta(3).
 \zeta(n)=\operatorname{Li}_n(1) \Re(n)>1 \mathcal{L}(6) \mathcal{L}(7) \int_{0}^{\infty}\operatorname{Ci}(x)^4\cos(x)\text{d}x,\int_{0}^{\infty}\operatorname{Ci}(x)^2\frac{\operatorname{Si}(2x)}{x} \cos(x)\text{d}x \displaystyle{\operatorname{Si}(x)=\int_{0}^{x} \frac{\sin(t)}{t}\text{d}t}. \operatorname{si}(x)+\operatorname{Si}(x)=\frac{\pi}{2} \begin{aligned}
&\int_{0}^{\infty}\operatorname{si}(x)\text{d}x=1\\
&\int_{0}^{\infty}\operatorname{si}(x)^2\text{d}x=\frac{\pi}{2}\\
&\int_{0}^{\infty}\operatorname{si}(x)^3\text{d}x=\frac{\pi^2}{4} -\frac{3}{2}\ln^22-\frac{3}{4} 
\operatorname{Li}_2\left ( \frac{1}{4}  \right )\\
&\int_{0}^{\infty}\operatorname{si}(x)^4\text{d}x=
\frac{\pi^3}{4} -3\pi\ln^22-\frac{3\pi}{2} 
\operatorname{Li}_2\left ( \frac{1}{4}  \right )
\end{aligned} \int_{0}^{\infty}\operatorname{Ci}(x)^3\cos(a x)\text{d}x
=\begin{cases}
\color{Red}{\frac{\pi  \text{Li}_2\left(\frac{1-a}{3}\right)}{4 a}}+\frac{\pi  \text{Li}_2\left(\frac{a-1}{a-2}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{a+1}{3 (a-1)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a-1}{a+1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{a+2}\right)}{2 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{3}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+1}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a-1}{3 (a+1)}\right)}{4 a}+\frac{\pi  \log ^2(2-a)}{4 a}-\frac{\pi  \log ^2(a+2)}{4 a}+\frac{\pi  \log (3) \log (a-1)}{4 a}+\frac{\pi  \log (3) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log (a-2)}{4 a}-\frac{\pi  \log (3) \tanh ^{-1}\left(\frac{a}{2}\right)}{2 a} &  (0\le a\le1),\\
\color{Red}{\frac{\pi  \text{Li}_2\left(\frac{a^2}{a^2-1}\right)}{4 a}}+\frac{\pi  \log \left(-\frac{a}{a+1}\right) \log \left(\frac{1}{1-a^2}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{a}{2}\right)}{2 a}+\frac{\pi  \text{Li}_2(1-a)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (1-a)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{3}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{a}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (a+1)}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{1}{2}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{1}{1-a}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{1}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2(-a)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{1}{a+1}\right)}{4 a}-\frac{3 \pi  \text{Li}_2\left(\frac{a}{a+1}\right)}{4 a}-\frac{7 \pi ^3}{24 a}+\frac{3 \pi  \log ^2(2)}{8 a}+\frac{\pi  \log ^2(a)}{8 a}-\frac{\pi  \log ^2(a+1)}{2 a}+\frac{\pi  \log (2) \log (a-1)}{4 a}+\frac{\pi  \log (2) \log (a+1)}{4 a}-\frac{\pi  \log (2) \log (a)}{2 a}-\frac{\pi  \log (2) \log (a+2)}{2 a}+\frac{\pi  \log \left(\frac{a+2}{a+1}\right) \log \left(\frac{1}{(a+1)^2}\right)}{4 a}+\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}+\frac{\pi  \log (3) \log (a+2)}{4 a}+\frac{\pi  \log (a) \log (a+2)}{2 a}-\frac{\pi  \log (a) \log (a+1)}{2 a}-\frac{i \pi ^2 \log \left(\frac{1}{1-a}\right)}{4 a}-\frac{\pi  \log (3) \log (a-1)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(-\frac{a}{a+1}\right)}{4 a}-\frac{\pi  \log (a-1) \log (a+2)}{4 a}-\frac{\pi  \log (a+1) \log (a+2)}{4 a}  & (1\le a\le3),  \\
  \color{Red}{-\frac{\pi  \text{Li}_2\left(\frac{a^2}{a^2-1}\right)}{4 a}}-\frac{\pi  \log \left(-\frac{a}{a+1}\right) \log \left(\frac{1}{1-a^2}\right)}{4 a}+\frac{\pi  \text{Li}_2(-2)}{4 a}+\frac{\pi  \text{Li}_2(2)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{2}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{1-a}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{a}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(-\frac{1}{a-1}\right)}{4 a}+\frac{\pi  \text{Li}_2\left(\frac{1}{a+1}\right)}{2 a}+\frac{\pi  \text{Li}_2\left(\frac{a}{a+1}\right)}{2 a}-\frac{\pi  \text{Li}_2\left(-\frac{a}{2}\right)}{2 a}-\frac{\pi  \text{Li}_2(1-a)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (1-a)}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a-2}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(-\frac{3}{a-1}\right)}{4 a}-\frac{\pi  \text{Li}_2(a-1)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a+2}{2 (a+1)}\right)}{4 a}-\frac{\pi  \text{Li}_2\left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}+\frac{\pi ^3}{3 a}-\frac{\pi  \log ^2(2)}{4 a}+\frac{\pi  \log ^2(a+1)}{2 a}+\frac{i \pi ^2 \log (2)}{4 a}+\frac{\pi  \log (2) \log (a)}{2 a}+\frac{\pi  \log (2) \log (a+2)}{2 a}-\frac{\pi  \log (2) \log (a-1)}{4 a}-\frac{\pi  \log (2) \log (a+1)}{4 a}+\frac{i \pi ^2 \log \left(\frac{1}{1-a}\right)}{4 a}+\frac{\pi  \log (3) \log (a-2)}{4 a}+\frac{\pi  \log (a-2) \log (a-1)}{4 a}+\frac{\pi  \log \left(\frac{a+2}{a+1}\right) \log \left(-\frac{1}{a+1}\right)}{4 a}+\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(-\frac{a}{a+1}\right)}{4 a}+\frac{\pi  \log (a-1) \log (a+2)}{4 a}+\frac{\pi  \log (a+1) \log (a+2)}{4 a}-\frac{\pi  \log (a) \log (a+2)}{2 a}-\frac{\pi  \log (a-2) \log \left(\frac{1}{a-1}\right)}{4 a}-\frac{\pi  \log (3) \log \left(\frac{a+2}{a-1}\right)}{4 a}-\frac{\pi  \log (2-a) \log (a-1)}{4 a}-\frac{\pi  \log (a) \log \left(\frac{1}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log \left(\frac{a-2}{a+1}\right)}{4 a}-\frac{\pi  \log \left(\frac{1}{(a+1)^2}\right) \log \left(\frac{a+2}{a+1}\right)}{4 a}-\frac{\pi  \log (3) \log (a+1)}{4 a}-\frac{\pi  \log \left(-\frac{1}{a+1}\right) \log \left(\frac{a (a+2)}{(a+1)^2}\right)}{4 a}& (a\ge3).
\end{cases} \begin{aligned}
&1.\int_{0}^{\infty}\operatorname{Ci}(x)\cos(\omega x)\text{d}x=
\begin{cases}
  0 &(0\le\omega<1), \\
\displaystyle{ -\frac{\pi}{4}  }&(\omega=1), \\
\displaystyle{ -\frac{\pi}{2\omega}  }&(\omega>1).
\end{cases}\\
&2.\int_{0}^{\infty}\operatorname{Ci}(x)\sin(\omega x)\text{d}x=
\begin{cases}
 \displaystyle{-\frac{\ln(1-\omega^2)}{2\omega}}  &(0\le\omega<1), \\
\displaystyle{ +\infty  }&(\omega=1), \\
\displaystyle{-\frac{\ln(\omega^2-1)}{2\omega} }&(\omega>1).
\end{cases}\\
&3.\int_{0}^{\infty}\operatorname{Ci}(x)^2\cos(\omega x)\text{d}x=
\begin{cases}
\displaystyle{ \frac{\pi\ln(1+\omega)}{2\omega} }&(0\le\omega\le2), \\
\displaystyle{ \frac{\pi\ln(\omega^2-1)}{2\omega} }&(\omega\ge2).
\end{cases}\\
&4.\int_{0}^{\infty}\operatorname{si}(x)\sin(\omega x)\text{d}x=
\begin{cases}
  0 &(0\le\omega<1), \\
\displaystyle{ \frac{\pi}{4}  }&(\omega=1), \\
\displaystyle{ \frac{\pi}{2\omega}  }&(\omega>1).
\end{cases}\\
&5.\int_{0}^{\infty}\operatorname{si}(x)\cos(\omega x)\text{d}x=
\begin{cases}
 \displaystyle{\frac{1}{2\omega}\ln\left ( \frac{1+\omega}{1-\omega}  \right ) }  &(0\le\omega<1), \\
\displaystyle{ +\infty  }&(\omega=1), \\
\displaystyle{\frac{1}{2\omega}\ln\left ( \frac{\omega+1}{\omega-1}  \right ) }&(\omega>1).
\end{cases}\\
&6.\int_{0}^{\infty}\operatorname{si}(x)^2\cos(\omega x)\text{d}x=
\begin{cases}
\displaystyle{ \frac{\pi\ln(1+\omega)}{2\omega} }&(0\le\omega\le2), \\
\displaystyle{ \frac{\pi}{2\omega}\ln\left ( \frac{\omega+1}{\omega-1}  \right ) }&(\omega\ge2).
\end{cases}\\
&7.\int_{0}^{\infty}\frac{\operatorname{Si}(x)}{x}\cos(\omega x)\text{d}x=
\begin{cases}
 \displaystyle{-\frac{\pi}{2}\ln(\omega)}  &(0<\omega\le1), \\
\displaystyle{0 }&(\omega\ge1).
\end{cases}\\
\end{aligned} \operatorname{Si}_n(x) \operatorname{Si}_0(x)=\sin(x),\operatorname{Si}_n(x)
=\int_{0}^{x} \frac{\operatorname{Si}_{n-1}(t)}{t}\text{d}t. 
\int_{0}^{\infty}\frac{\operatorname{Si}_2(x)\operatorname{si}(x)^2}{x}
\text{d} x=\frac{7\pi^5}{1440}.
","['real-analysis', 'calculus', 'integration', 'trigonometric-integrals', 'polylogarithm']"
1,"Find out functions of the form $g(x,y) = \int f(x,t) f(y,t) \lambda(dt)$",Find out functions of the form,"g(x,y) = \int f(x,t) f(y,t) \lambda(dt)","I am interested in the following question. Given a symmetric function $g: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R$ or $\mathbb R_{+}^{n}\times \mathbb R_{+}^{n} \rightarrow 0$ .  I am interested in finding out whether $g$ can be written as the following form: $$g(x,y) = \int f(x,t) f(y,t) \lambda(dt),$$ where $\lambda$ is some measure but not necessarily the standard Lebesgue measure. For example, $g(x,y) = \min\{|x|,|y|\}$ can be written as the above form for $f(x,t) = \mathbb I(0<t<|x|)$ . $g(x,y) = \frac{1}{|x|+|y|}$ can also be written as above for $f(x,t) = e^{-|x|t}\mathbb I(t>0)$ . I am wondering if there is any necessary or sufficient condition to describe the set of functions which satisfies the above assumption. One necessary condition is $g(x,y)$ needs to be positive symmetric definite. Thanks a lot!!","I am interested in the following question. Given a symmetric function or .  I am interested in finding out whether can be written as the following form: where is some measure but not necessarily the standard Lebesgue measure. For example, can be written as the above form for . can also be written as above for . I am wondering if there is any necessary or sufficient condition to describe the set of functions which satisfies the above assumption. One necessary condition is needs to be positive symmetric definite. Thanks a lot!!","g: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R \mathbb R_{+}^{n}\times \mathbb R_{+}^{n} \rightarrow 0 g g(x,y) = \int f(x,t) f(y,t) \lambda(dt), \lambda g(x,y) = \min\{|x|,|y|\} f(x,t) = \mathbb I(0<t<|x|) g(x,y) = \frac{1}{|x|+|y|} f(x,t) = e^{-|x|t}\mathbb I(t>0) g(x,y)","['real-analysis', 'probability', 'functional-analysis', 'symmetric-functions']"
2,"$\dfrac{\partial^2 f}{\partial x \partial y} = 0 \nRightarrow f(x,y) = g(x) + h(y)$",,"\dfrac{\partial^2 f}{\partial x \partial y} = 0 \nRightarrow f(x,y) = g(x) + h(y)","I am working through Ted Shifrin's book Multivariable Mathematics. There is an exercise problem that is meant to demonstrate that one can have $\dfrac{\partial^2 f}{\partial x \partial y} = 0$ but $ f(x,y) \neq  g(x) + h(y)$ . The question (3.6.11) is as follows: $$ \mathrm{Given} \; f(x, y) = \begin{cases} 0, \; x < 0\; \mathrm{or} \; y < 0 \\x^3, \; x \geq 0 \; \mathrm{and} \; y > 0 \end{cases} $$ Show that $f$ is $C^2$ Show that $\dfrac{\partial^2 f}{\partial x \partial y} = 0$ Show that $f(x,y)$ cannot be written as $g(x) + h(y)$ for appropriate functions $g, h$ . I see that the domain is the entire plane except the x-axis, that is $\mathbb{R}^2 -\{y=0\}$ . The function is then 0 in all quadrants except the first, where it is $x^3$ . I could show 1. and 2. above, but I am puzzled by two things. Q1 What's wrong with writing $f(x,y) = g(x) + h(y)$ piecewise in each quadrant ? Q2. Will anything change if the domain allows the line $y=0$ also ? Q3. What is the takeaway from this problem ? I do not  understand that.","I am working through Ted Shifrin's book Multivariable Mathematics. There is an exercise problem that is meant to demonstrate that one can have but . The question (3.6.11) is as follows: Show that is Show that Show that cannot be written as for appropriate functions . I see that the domain is the entire plane except the x-axis, that is . The function is then 0 in all quadrants except the first, where it is . I could show 1. and 2. above, but I am puzzled by two things. Q1 What's wrong with writing piecewise in each quadrant ? Q2. Will anything change if the domain allows the line also ? Q3. What is the takeaway from this problem ? I do not  understand that.","\dfrac{\partial^2 f}{\partial x \partial y} = 0  f(x,y) \neq  g(x) + h(y)  \mathrm{Given} \; f(x, y) = \begin{cases} 0, \; x < 0\; \mathrm{or} \; y < 0 \\x^3, \; x \geq 0 \; \mathrm{and} \; y > 0 \end{cases}  f C^2 \dfrac{\partial^2 f}{\partial x \partial y} = 0 f(x,y) g(x) + h(y) g, h \mathbb{R}^2 -\{y=0\} x^3 f(x,y) = g(x) + h(y) y=0","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
3,Is Lebesgue integral w.r.t. counting measure the same thing as sum (on an arbitrary set)?,Is Lebesgue integral w.r.t. counting measure the same thing as sum (on an arbitrary set)?,,"TL:DR; For arbitrary sets (not necessarily countable) we have two notions: Lebesgue integral w.r.t. the counting measure and sum of family indexed by this set. Are these two notions equivalent? For an arbitrary set $X$ we can define counting measure simply by putting $\mu(A)=|A|$ . (I.e., if $A$ is finite then $\mu(A)$ is simply number of elements of $A$ ; otherwise it is $+\infty$ .) In this way we get a $\sigma$ -additive measure on $\mathcal P(X)$ and it is possible to work with Lebesgue integral with respect to this measure. If the integral $$\int f \;\mathrm{d} \mu$$ of a function $f\colon X\to\mathbb R$ exists, it is natural to interpret this integral as a sum of the values $f(x)$ over all $x\in X$ . There is also a (more-or-less standard) notion of a sum of values on a given set which includes uncountable sets. Let me briefly recall the definition. (Below I will add a few links to other posts on this site where this definition can be found.) Definition. Let $f\colon X\to\mathbb R$ be a function and $S\in\mathbb R$ . We say that $$\sum_{x\in X} f(x) = S$$ if and only if for every $\varepsilon>0$ there exists a finite set $F_0$ such that for all finite sets $F\supseteq F_0$ we have $\left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon$ . $$(\forall \varepsilon>0) (\exists F_0\text{ finite }) \left(F\text{ is finite and }F\supseteq F_0 \Rightarrow \left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon \right)$$ Some further remarks: We can modify the above definition in a natural way to be able to say when $\sum f(x)=+\infty$ and $\sum f(x)=-\infty$ . If we work with non-negative values, i.e., $f(x)\ge0$ , then we get a much simpler equivalent definition $$\sum_{x\in X} f(x) = \sup \{\sum_{x\in F} f(x); F\text{ is finite}\}.$$ This type of sum is also defined in the Wikipedia article about series: Summations over arbitrary index sets ( current revision ). This definition can be interpreted nicely using convergence of nets. We take the directed set consisting of finite subsets of $X$ ordered by inclusion. For every such finite set we have the value $s_F=\sum_{x\in F} f(x)$ . The sum as defined above is equal to $S$ iff $S$ is the limit of this net. With this definition, the distinction between conditional and absolute convergence no longer makes sense. (Which is natural, since we do not take any kind of ordering on $X$ into account.) In particular, in the case $X=\mathbb N$ this correspond to definition of sum of absolutely convergent series. (At least if we work with real values. In more general contexts, it can happen that unconditional convergence and absolute convergence might be different. This definition of sum corresponds to unconditional convergence.) A variant of Cauchy's criterion for such sums can be shown. The same definition can be used in more general settings. (You still probably need the structure to be at least a topological abelian group if you expect the sum to behave reasonably.) Question. Is the sum as defined above equivalent to the notion of Lebesgue integral with respect to the counting measure? In particular I would like to know: Are there some problems that arise if we work with uncountable sets, rather than just with countable ones? Are there any specific problems if I also allow negative values? I would be grateful for both references to some texts which deal with relationship between these two notions. And, of course, for a proof (or sketch of a proof) if this is sufficiently simple to fit into a post on this site. I have checked whether something about this is mentioned in the Wikipedia article Counting measure . This kind of sum is mentioned there, but in a slightly different context. The current revision of the Wikipedia article says that: The counting measure is a special case of a more general construct.  With the notation as above, any function $f \colon X \to [0, \infty)$ defines a measure $\mu$ on $(X, \Sigma)$ via $$\mu(A):=\sum_{a \in A} f(a)\, \forall A\subseteq X,$$ where the possibly uncountable sum of real numbers is defined to be the sup of the sums over all finite subsets, i.e., $$\sum_{y \in Y \subseteq \mathbb R} y := \sup_{F \subseteq Y, |F| < \infty} \left\{ \sum_{y \in F} y  \right\}.$$ Taking $f(x)=1$ for all $x$ ' in $X$ produces the counting measure. Some related links: There are several posts where the definition of sum over arbitrary (possibly uncountable) sum is given, including some properties and references. For example, in the following questions: The sum of an uncountable number of positive numbers... , Does uncountable summation, with a finite sum, ever occur in mathematics? , Use of $\sum $ for uncountable indexing set , Looking for a reference (textbook) for an elementary analysis problem on uncountable sums , Reference for series on arbitrary infinite sets . (And for each of them you can find a lot of further posts about this topic if you look at related and linked questions in the sidebar.) Proof that counting measure and summation coincide for non-negative functions on $\mathbb N$ can be found here: Integration with respect to counting measure. Although this answer claims that counting measure is not $\sigma$ -additive if $X$ is not countable, I do not really see why this should be the case. (I am not sure, but it is possible that the poster meant to say $\sigma$ -finite...?) In fact, there is another question on this site where the answer gives a proof for countable additivity: Counting measure proof .","TL:DR; For arbitrary sets (not necessarily countable) we have two notions: Lebesgue integral w.r.t. the counting measure and sum of family indexed by this set. Are these two notions equivalent? For an arbitrary set we can define counting measure simply by putting . (I.e., if is finite then is simply number of elements of ; otherwise it is .) In this way we get a -additive measure on and it is possible to work with Lebesgue integral with respect to this measure. If the integral of a function exists, it is natural to interpret this integral as a sum of the values over all . There is also a (more-or-less standard) notion of a sum of values on a given set which includes uncountable sets. Let me briefly recall the definition. (Below I will add a few links to other posts on this site where this definition can be found.) Definition. Let be a function and . We say that if and only if for every there exists a finite set such that for all finite sets we have . Some further remarks: We can modify the above definition in a natural way to be able to say when and . If we work with non-negative values, i.e., , then we get a much simpler equivalent definition This type of sum is also defined in the Wikipedia article about series: Summations over arbitrary index sets ( current revision ). This definition can be interpreted nicely using convergence of nets. We take the directed set consisting of finite subsets of ordered by inclusion. For every such finite set we have the value . The sum as defined above is equal to iff is the limit of this net. With this definition, the distinction between conditional and absolute convergence no longer makes sense. (Which is natural, since we do not take any kind of ordering on into account.) In particular, in the case this correspond to definition of sum of absolutely convergent series. (At least if we work with real values. In more general contexts, it can happen that unconditional convergence and absolute convergence might be different. This definition of sum corresponds to unconditional convergence.) A variant of Cauchy's criterion for such sums can be shown. The same definition can be used in more general settings. (You still probably need the structure to be at least a topological abelian group if you expect the sum to behave reasonably.) Question. Is the sum as defined above equivalent to the notion of Lebesgue integral with respect to the counting measure? In particular I would like to know: Are there some problems that arise if we work with uncountable sets, rather than just with countable ones? Are there any specific problems if I also allow negative values? I would be grateful for both references to some texts which deal with relationship between these two notions. And, of course, for a proof (or sketch of a proof) if this is sufficiently simple to fit into a post on this site. I have checked whether something about this is mentioned in the Wikipedia article Counting measure . This kind of sum is mentioned there, but in a slightly different context. The current revision of the Wikipedia article says that: The counting measure is a special case of a more general construct.  With the notation as above, any function defines a measure on via where the possibly uncountable sum of real numbers is defined to be the sup of the sums over all finite subsets, i.e., Taking for all ' in produces the counting measure. Some related links: There are several posts where the definition of sum over arbitrary (possibly uncountable) sum is given, including some properties and references. For example, in the following questions: The sum of an uncountable number of positive numbers... , Does uncountable summation, with a finite sum, ever occur in mathematics? , Use of for uncountable indexing set , Looking for a reference (textbook) for an elementary analysis problem on uncountable sums , Reference for series on arbitrary infinite sets . (And for each of them you can find a lot of further posts about this topic if you look at related and linked questions in the sidebar.) Proof that counting measure and summation coincide for non-negative functions on can be found here: Integration with respect to counting measure. Although this answer claims that counting measure is not -additive if is not countable, I do not really see why this should be the case. (I am not sure, but it is possible that the poster meant to say -finite...?) In fact, there is another question on this site where the answer gives a proof for countable additivity: Counting measure proof .","X \mu(A)=|A| A \mu(A) A +\infty \sigma \mathcal P(X) \int f \;\mathrm{d} \mu f\colon X\to\mathbb R f(x) x\in X f\colon X\to\mathbb R S\in\mathbb R \sum_{x\in X} f(x) = S \varepsilon>0 F_0 F\supseteq F_0 \left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon (\forall \varepsilon>0) (\exists F_0\text{ finite }) \left(F\text{ is finite and }F\supseteq F_0 \Rightarrow \left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon \right) \sum f(x)=+\infty \sum f(x)=-\infty f(x)\ge0 \sum_{x\in X} f(x) = \sup \{\sum_{x\in F} f(x); F\text{ is finite}\}. X s_F=\sum_{x\in F} f(x) S S X X=\mathbb N f \colon X \to [0, \infty) \mu (X, \Sigma) \mu(A):=\sum_{a \in A} f(a)\, \forall A\subseteq X, \sum_{y \in Y \subseteq \mathbb R} y := \sup_{F \subseteq Y, |F| < \infty} \left\{ \sum_{y \in F} y  \right\}. f(x)=1 x X \sum  \mathbb N \sigma X \sigma","['real-analysis', 'sequences-and-series', 'measure-theory', 'summation', 'lebesgue-integral']"
4,"""Lebesgue"" measurabillity on Riemannian manifolds","""Lebesgue"" measurabillity on Riemannian manifolds",,"Let $X$ be a smooth oriented manifold of positive dimension; Let $g_1,g_2$  be two Riemannian metrics on $X$. Define $\Lambda_1,\Lambda_2:C_c(X) \to \mathbb{R}$ by $$ \Lambda_i(f)=\int_X f \, Vol_{g_i},$$ where $Vol_{g_i}$ is the Riemannian volume form of $g_i$. The discussion here implies that for each $i$, there is a unique $\sigma$-algebra $\mathfrak B_i$, and a unique measure $\mu_i$ such that $I(f)=\int_X f d \mu$ for all $f \in C_c(X)$, and the conditions (a)-(f)** are satisfied. (This is essentially Riesz Representation theorem, with the additional observation that $X$ is $\sigma$-compact $\Rightarrow$ uniqueness of the $\mathfrak B_i$). Questions: (1) Is it true that $\mathfrak B_1= \mathfrak B_2$? (independence of the metric chosen) Assuming this is true, is there a way to define $\mathfrak B_i$ without passing through a Riemannian metric? (This is a natural expectation now since $\mathfrak B_i$ is an invariant of $X$ as a differentiable manifold, thus unrelated to the added Riemannian structure) My guess is that $\mathfrak B_i$ will be the completion of the Borel $\sigma$-algebra w.r.t a suitable measure (which should be any one of the ""Lebesgue"" measures $\mu_i$ mentioned above. On a first glance, this does seem to go through a Riemannian metric, since the $\mu_i$ was induced by it. However, the completion w.r.t a measure $\mu$ is dependent only on the  subsets that have $\mu$-measure zero , and this is independent of the Riemannian metric chosen, and can be defined invariantly (see Lee's book on smooth manifolds, chapter 6: A subset of a manifold has measure zero if its image under every coordinate chart has measure zero  in $\mathbb{R}^n$) (2) A function $f:X \to \mathbb{R}$ is measurable w.tr.t $\mathfrak B_i$ if and only if it is measurable after (pre)composing with a coordinate chart diffeomorphism? (3) Is this the standard way to define $L^p(X)$ spaces (from the perspective of measurability, are the elements of $L^p(X)$ exactly the measurable functions in the sense above, with the additional requirement of finiteness of the integral?) ** (a) $\mathfrak{B_i}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{B}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{B}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{B}$.","Let $X$ be a smooth oriented manifold of positive dimension; Let $g_1,g_2$  be two Riemannian metrics on $X$. Define $\Lambda_1,\Lambda_2:C_c(X) \to \mathbb{R}$ by $$ \Lambda_i(f)=\int_X f \, Vol_{g_i},$$ where $Vol_{g_i}$ is the Riemannian volume form of $g_i$. The discussion here implies that for each $i$, there is a unique $\sigma$-algebra $\mathfrak B_i$, and a unique measure $\mu_i$ such that $I(f)=\int_X f d \mu$ for all $f \in C_c(X)$, and the conditions (a)-(f)** are satisfied. (This is essentially Riesz Representation theorem, with the additional observation that $X$ is $\sigma$-compact $\Rightarrow$ uniqueness of the $\mathfrak B_i$). Questions: (1) Is it true that $\mathfrak B_1= \mathfrak B_2$? (independence of the metric chosen) Assuming this is true, is there a way to define $\mathfrak B_i$ without passing through a Riemannian metric? (This is a natural expectation now since $\mathfrak B_i$ is an invariant of $X$ as a differentiable manifold, thus unrelated to the added Riemannian structure) My guess is that $\mathfrak B_i$ will be the completion of the Borel $\sigma$-algebra w.r.t a suitable measure (which should be any one of the ""Lebesgue"" measures $\mu_i$ mentioned above. On a first glance, this does seem to go through a Riemannian metric, since the $\mu_i$ was induced by it. However, the completion w.r.t a measure $\mu$ is dependent only on the  subsets that have $\mu$-measure zero , and this is independent of the Riemannian metric chosen, and can be defined invariantly (see Lee's book on smooth manifolds, chapter 6: A subset of a manifold has measure zero if its image under every coordinate chart has measure zero  in $\mathbb{R}^n$) (2) A function $f:X \to \mathbb{R}$ is measurable w.tr.t $\mathfrak B_i$ if and only if it is measurable after (pre)composing with a coordinate chart diffeomorphism? (3) Is this the standard way to define $L^p(X)$ spaces (from the perspective of measurability, are the elements of $L^p(X)$ exactly the measurable functions in the sense above, with the additional requirement of finiteness of the integral?) ** (a) $\mathfrak{B_i}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{B}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{B}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{B}$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'smooth-manifolds']"
5,Examples of smooth fractals,Examples of smooth fractals,,"A classic example of a fractal curve is the Koch Snowflake .  This is a topological manifold (as opposed to many other fractals which are not), but it also clearly not smooth. Question : Are there any curve-type fractals that are actually smooth?  Or does the infinite self-similarity eventually pose an insurmountable barrier to smoothness? Technically speaking, $\mathbb{R}$ is a smooth fractal too, so for the above question, I'd only introduce the caveat that the curve be 'interesting' as a fractal (or at least non-trivial). Intuitively, I see no reason for such objects to not exist, but this is far from any area of math I'm familiar with.","A classic example of a fractal curve is the Koch Snowflake .  This is a topological manifold (as opposed to many other fractals which are not), but it also clearly not smooth. Question : Are there any curve-type fractals that are actually smooth?  Or does the infinite self-similarity eventually pose an insurmountable barrier to smoothness? Technically speaking, $\mathbb{R}$ is a smooth fractal too, so for the above question, I'd only introduce the caveat that the curve be 'interesting' as a fractal (or at least non-trivial). Intuitively, I see no reason for such objects to not exist, but this is far from any area of math I'm familiar with.",,"['real-analysis', 'geometry', 'plane-curves', 'fractals']"
6,When is a function of the largest eigenvalue continuous and/or differentiable?,When is a function of the largest eigenvalue continuous and/or differentiable?,,"I want to understand why the following function, the largest eigenvalue of a symmetric linear operator, is continuous and Gâteaux differentiable . \begin{equation*} \lambda(V)=\sup_{f \in \ell^2(I):\ \rVert f \lVert_2} \langle (A+V)f, f \rangle, \qquad V \in \ell^2(I) \end{equation*} where $I$ is a finite index set (subset of $\mathbb Z^d$ in fact) $A: \ell^2(I) \rightarrow \ell^2(I)$ is a symmetric linear operator that is nonnegative outside its diagonal, so $-A$ is positive definite $V \in \ell^2(I)$ multiplies like the diagonal matrix $\mathrm{diag}(V_1, \dots, V_n)$. I encountered this statement in a probability theory proof where it simply states that this follows easily from the Perron-Frobenius theorem and basic linear algebra. So we should have (1) \begin{equation*} \lim_{n\rightarrow\infty} \sup_{\lVert f \rVert_2=1} \langle(A+V_n)f,f\rangle =\sup_{\lVert f \rVert_2=1} \langle(Af,f\rangle, \qquad \mathrm{\ where\ } V_n \rightarrow 0 \mathrm{\ pointwise} \end{equation*} and the existence of (2)\begin{equation*} \lim_{t \rightarrow 0}\frac{1}{t} \left( \lambda(V+hg)-\lambda(V) \right) =\lim_{t \rightarrow 0}\frac{1}{t} \left( \sup_{\lVert f \rVert_2=1} \langle(A+V+hg)f,f\rangle - \sup_{\lVert f \rVert_2=1} \langle(A+V)f,f\rangle \right). \end{equation*} In (1) the problem is that it's not obvious to me that we may swap the limit and the supremum and I don't see any good reason for this to be true. For (2) I'm simply puzzled. The Perron-Frobenius theorem says that the largest eigenvalue of $A+V$ is simple and that it has a positive eigenfunction. But I don't see how to conclude the existence of the Gâteaux derivative from there. I guess there must be some theorem from linear algebra, but so far my research didn't give me an answer either. Some more context on how I encountered this problem The operator $A$ is the generator $\Delta$ of a symmetric random walk $(X_t)_{t\geq0}$ on $\mathbb{Z}^d$, restricted to a finite, connected subset: \begin{equation*} \Delta_I f(x) = \sum_{y\in\mathbb{Z}^d:\ |x-y|=1} \omega_{xy} [f(y)-f(x)], \qquad x\in I,\ f: \mathbb{Z}^d \rightarrow \mathbb{R},\ \mathrm{supp}(f)\subset I \end{equation*} where $\omega_{xy}=\omega_{yx}\in(0,\infty)$ are symmetric weights. Then I need $\Lambda(V):=\lambda(V)-\lambda(0)$ to be Gâteaux differentiable and continuous with respect to pointwise convergence in order to apply a large deviations principle with rate function $\Lambda$.","I want to understand why the following function, the largest eigenvalue of a symmetric linear operator, is continuous and Gâteaux differentiable . \begin{equation*} \lambda(V)=\sup_{f \in \ell^2(I):\ \rVert f \lVert_2} \langle (A+V)f, f \rangle, \qquad V \in \ell^2(I) \end{equation*} where $I$ is a finite index set (subset of $\mathbb Z^d$ in fact) $A: \ell^2(I) \rightarrow \ell^2(I)$ is a symmetric linear operator that is nonnegative outside its diagonal, so $-A$ is positive definite $V \in \ell^2(I)$ multiplies like the diagonal matrix $\mathrm{diag}(V_1, \dots, V_n)$. I encountered this statement in a probability theory proof where it simply states that this follows easily from the Perron-Frobenius theorem and basic linear algebra. So we should have (1) \begin{equation*} \lim_{n\rightarrow\infty} \sup_{\lVert f \rVert_2=1} \langle(A+V_n)f,f\rangle =\sup_{\lVert f \rVert_2=1} \langle(Af,f\rangle, \qquad \mathrm{\ where\ } V_n \rightarrow 0 \mathrm{\ pointwise} \end{equation*} and the existence of (2)\begin{equation*} \lim_{t \rightarrow 0}\frac{1}{t} \left( \lambda(V+hg)-\lambda(V) \right) =\lim_{t \rightarrow 0}\frac{1}{t} \left( \sup_{\lVert f \rVert_2=1} \langle(A+V+hg)f,f\rangle - \sup_{\lVert f \rVert_2=1} \langle(A+V)f,f\rangle \right). \end{equation*} In (1) the problem is that it's not obvious to me that we may swap the limit and the supremum and I don't see any good reason for this to be true. For (2) I'm simply puzzled. The Perron-Frobenius theorem says that the largest eigenvalue of $A+V$ is simple and that it has a positive eigenfunction. But I don't see how to conclude the existence of the Gâteaux derivative from there. I guess there must be some theorem from linear algebra, but so far my research didn't give me an answer either. Some more context on how I encountered this problem The operator $A$ is the generator $\Delta$ of a symmetric random walk $(X_t)_{t\geq0}$ on $\mathbb{Z}^d$, restricted to a finite, connected subset: \begin{equation*} \Delta_I f(x) = \sum_{y\in\mathbb{Z}^d:\ |x-y|=1} \omega_{xy} [f(y)-f(x)], \qquad x\in I,\ f: \mathbb{Z}^d \rightarrow \mathbb{R},\ \mathrm{supp}(f)\subset I \end{equation*} where $\omega_{xy}=\omega_{yx}\in(0,\infty)$ are symmetric weights. Then I need $\Lambda(V):=\lambda(V)-\lambda(0)$ to be Gâteaux differentiable and continuous with respect to pointwise convergence in order to apply a large deviations principle with rate function $\Lambda$.",,"['real-analysis', 'linear-algebra', 'continuity', 'hilbert-spaces', 'supremum-and-infimum']"
7,Asymptotics for a series of products,Asymptotics for a series of products,,"I am trying to solve the following problem: Define the following functions for $x>0$:   $$f_n(x):=\prod_{k=0}^{n}\frac{1}{x+k}$$ Show that the function   $$f(x):=\sum_{n=0}^{+\infty}f_n(x)$$   is well defined for $x>0$. Calculate its value in $1$. Study the function $f(x)$ and give asymptotic estimates for $x \to 0^+$ and $x\to +\infty$. Prove that the following equivalence holds:   $$f(x)=e \sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!}$$ I am having a hard time proving the equality in the third point. What I have done for now: $\textbf{Part 1}$ Using the ratio test, $$\lim_{n\to +\infty}\frac{\prod_{k=0}^{n+1}\frac{1}{x+k}}{\prod_{k=0}^{n}\frac{1}{x+k}}=\lim_{n\to +\infty}\frac{1}{x+n+1}=0$$  the series converges for $x>0$. The value of the function in $1$ is $$f(1)=\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{k+1}=\sum_{n=0}^{+\infty}\frac{1}{(n+1)!}=e-1$$ $\textbf{Part 2}$ First of all, $f$ is positive for every $x>0$. Its monotonicity is immediate: if $x_2>x_1$, $$\begin{align} \quad \qquad \frac{1}{x_2+k}<\frac{1}{x_1+k} \end{align} \\   \implies f(x_2)=\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x_2+k}\leq\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x_1+k}=f(x_1)$$ The general term of the series $f$ must be zero, because it converges; hence in an interval $[M,+\infty)$ with $M>0$ $$||f_n(x) ||_{\infty}=\prod_{k=0}^{n}\frac{1}{M+k}$$ $$\implies \sum_{n=0}^{+\infty}||f_n(x)|| \text{  is convergent}$$ so the series is uniformly convergent on every interval of the type $[M,+\infty)$. $f$ is asymptotic to $\frac 1x$ for $x\to +\infty$: in fact $$\lim_{x\to \infty}\frac{f(x)}{\frac{1}{x}}= \lim_{x\to \infty}  x\left (\frac{1}{x}+ \sum_{n=1}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}\right )= 1 $$ because the series converges in a neighbourhood of $+\infty$. In a neighbourhood of $0$, the function acts similarly: we can notice that $$\lim_{x\to 0^+}\frac{f(x)}{\frac{1}{x}}=\lim_{x\to 0^+} x\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}=\lim_{x \to 0^+} x\left (\frac{1}{x}+ \sum_{n=1}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}\right )=  \lim_{x\to 0^+}  1 + \sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{x+k}$$ but $\sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{x+k}$ converges in $x=0$ and is continuous, so the limit is $$\lim_{x\to 0^+}\frac{f(x)}{\frac{1}{x}} = 1+  \sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{k}=e$$ hence $f \sim \frac{e}{x}$ Monotonicity and limits of this function imply that $f$ is a bijection of $(0,+\infty)$ in itself. $\textbf{Part 3}$ I have tried to manipulate the sums: writing a single fraction instead of the product does not seem to work: it leads to $$\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}=\frac{1}{x}+\frac{1}{x}\frac{1}{x(x+1)}+\dots=\lim_{n\to +\infty}\frac{\sum_{h=0}^{n}\prod_{k=0}^h(x+k)}{\prod_{k=0}^{n}(x+k)}$$ It does not seem very familiar, even dividing it by $e=\sum_{n=0}^{+\infty}\frac{1}{n!}=f(1)$ Another idea that came to mind was to use the Cauchy product series and the Cauchy series product on the RHS : it leads to $$\sum_{i=0}^{+\infty}\frac{1}{i!}\sum_{j=0}^{+\infty}\frac{(-1)^j}{(x+j)j!}=\sum_{k=0}^{+\infty}\sum_{l=0}^{k}\frac{(-1)^{k-l}}{(x+k-l)l!(k-l)!}$$ Things seem as complicated as before. Integrating or derivating $f(x)$ term by term would require to know a general form for the integral/derivative of $f_n(x)=\prod_{k=0}^{n}\frac{1}{x+k}$: it does not appear impossible to find it, but I think it would not be of great practical use; moreover, the series does not converge uniformly on  the whole interval $(0,+\infty)$. The same goes for the series on the RHS . Working backwards, I thought of finding its integral/series on the interval $[M,+\infty)$ : I obtained $$\int \left (e\sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!} \right ) dx =e\sum_{n=0}^{+\infty} \int \frac{(-1)^n}{(x+n)n!}  dx=e\sum_{n=0}^{+\infty}  \frac{(-1)^n}{n!}\log(x+n)+C $$ I can't get far from here, and I am not even sure if what I have done is correct. Question : Are the two first parts correct? What could be a good way of proving the equality in the third part?","I am trying to solve the following problem: Define the following functions for $x>0$:   $$f_n(x):=\prod_{k=0}^{n}\frac{1}{x+k}$$ Show that the function   $$f(x):=\sum_{n=0}^{+\infty}f_n(x)$$   is well defined for $x>0$. Calculate its value in $1$. Study the function $f(x)$ and give asymptotic estimates for $x \to 0^+$ and $x\to +\infty$. Prove that the following equivalence holds:   $$f(x)=e \sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!}$$ I am having a hard time proving the equality in the third point. What I have done for now: $\textbf{Part 1}$ Using the ratio test, $$\lim_{n\to +\infty}\frac{\prod_{k=0}^{n+1}\frac{1}{x+k}}{\prod_{k=0}^{n}\frac{1}{x+k}}=\lim_{n\to +\infty}\frac{1}{x+n+1}=0$$  the series converges for $x>0$. The value of the function in $1$ is $$f(1)=\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{k+1}=\sum_{n=0}^{+\infty}\frac{1}{(n+1)!}=e-1$$ $\textbf{Part 2}$ First of all, $f$ is positive for every $x>0$. Its monotonicity is immediate: if $x_2>x_1$, $$\begin{align} \quad \qquad \frac{1}{x_2+k}<\frac{1}{x_1+k} \end{align} \\   \implies f(x_2)=\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x_2+k}\leq\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x_1+k}=f(x_1)$$ The general term of the series $f$ must be zero, because it converges; hence in an interval $[M,+\infty)$ with $M>0$ $$||f_n(x) ||_{\infty}=\prod_{k=0}^{n}\frac{1}{M+k}$$ $$\implies \sum_{n=0}^{+\infty}||f_n(x)|| \text{  is convergent}$$ so the series is uniformly convergent on every interval of the type $[M,+\infty)$. $f$ is asymptotic to $\frac 1x$ for $x\to +\infty$: in fact $$\lim_{x\to \infty}\frac{f(x)}{\frac{1}{x}}= \lim_{x\to \infty}  x\left (\frac{1}{x}+ \sum_{n=1}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}\right )= 1 $$ because the series converges in a neighbourhood of $+\infty$. In a neighbourhood of $0$, the function acts similarly: we can notice that $$\lim_{x\to 0^+}\frac{f(x)}{\frac{1}{x}}=\lim_{x\to 0^+} x\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}=\lim_{x \to 0^+} x\left (\frac{1}{x}+ \sum_{n=1}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}\right )=  \lim_{x\to 0^+}  1 + \sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{x+k}$$ but $\sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{x+k}$ converges in $x=0$ and is continuous, so the limit is $$\lim_{x\to 0^+}\frac{f(x)}{\frac{1}{x}} = 1+  \sum_{n=1}^{+\infty}\prod_{k=1}^{n}\frac{1}{k}=e$$ hence $f \sim \frac{e}{x}$ Monotonicity and limits of this function imply that $f$ is a bijection of $(0,+\infty)$ in itself. $\textbf{Part 3}$ I have tried to manipulate the sums: writing a single fraction instead of the product does not seem to work: it leads to $$\sum_{n=0}^{+\infty}\prod_{k=0}^{n}\frac{1}{x+k}=\frac{1}{x}+\frac{1}{x}\frac{1}{x(x+1)}+\dots=\lim_{n\to +\infty}\frac{\sum_{h=0}^{n}\prod_{k=0}^h(x+k)}{\prod_{k=0}^{n}(x+k)}$$ It does not seem very familiar, even dividing it by $e=\sum_{n=0}^{+\infty}\frac{1}{n!}=f(1)$ Another idea that came to mind was to use the Cauchy product series and the Cauchy series product on the RHS : it leads to $$\sum_{i=0}^{+\infty}\frac{1}{i!}\sum_{j=0}^{+\infty}\frac{(-1)^j}{(x+j)j!}=\sum_{k=0}^{+\infty}\sum_{l=0}^{k}\frac{(-1)^{k-l}}{(x+k-l)l!(k-l)!}$$ Things seem as complicated as before. Integrating or derivating $f(x)$ term by term would require to know a general form for the integral/derivative of $f_n(x)=\prod_{k=0}^{n}\frac{1}{x+k}$: it does not appear impossible to find it, but I think it would not be of great practical use; moreover, the series does not converge uniformly on  the whole interval $(0,+\infty)$. The same goes for the series on the RHS . Working backwards, I thought of finding its integral/series on the interval $[M,+\infty)$ : I obtained $$\int \left (e\sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!} \right ) dx =e\sum_{n=0}^{+\infty} \int \frac{(-1)^n}{(x+n)n!}  dx=e\sum_{n=0}^{+\infty}  \frac{(-1)^n}{n!}\log(x+n)+C $$ I can't get far from here, and I am not even sure if what I have done is correct. Question : Are the two first parts correct? What could be a good way of proving the equality in the third part?",,"['real-analysis', 'sequences-and-series', 'asymptotics', 'infinite-product']"
8,Borel measurability is a local property,Borel measurability is a local property,,"I am looking at Exercise 5.2 (page 44) in ""Real Analysis for Graduate Students"" by Richard Bass. Let $f:(0, 1)\to \mathbb{R}$ be a function such that for every $x\in (0, 1)$,   there exist $r>0$ and a Borel measurable function $g$, both depending   on $x$, such that $f$ and $g$ agree on $(x-r, x+r)\cap (0, 1)$. Prove   that $f$ is Borel measurable. Attempt. For each $x\in (0, 1)$, let's denote $r_{x}$ and $g_{x}$ to be the quantities given in the problem statement. By choosing $r_x$ sufficiently small, we may assume that $(x-r_{x}, x+r_{x})\subseteq (0, 1)$. Then for $a\in\mathbb{R}$, we have $$ f^{-1}((a, \infty))=\{y: f(y)>a\}=\bigcup_{x\in (0, 1)} \{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x}) $$ Since $g_{x}$ is Borel measurable, we know that $\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$ is a Borel set. But the union above is uncountable; so how do we show that $f^{-1}((a, \infty))$ is a Borel set? Attempt 2. Since $\mathbb{Q}$ is countable and dense in $(0, 1)$, we can try looking at the points $x\in\mathbb{Q}\cap (0, 1)$. But then the intervals $(x-r_{x}, x+r_{x})$ need not cover $(0, 1)$. The standard example is as follows. Suppose $\{q_1, q_2, …\}$ is enumeration of rationals in $(0, 1)$, and let $\varepsilon>0$. If we let $r_{q_{j}}$ to be $\varepsilon 2^{-j}$, then the measure of $\bigcup_{j=1}^{\infty} (q_j-r_{q_{j}}, q_{j}+r_{q_{j}})$ is at most $2\varepsilon$. Attempt 3. If each $g_{x}$ is continuous, we would be done. In that case, $$\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$$ would be an open set; so as a union of open sets, $f^{-1}((a, \infty))$ would be an open set, and hence Borel.","I am looking at Exercise 5.2 (page 44) in ""Real Analysis for Graduate Students"" by Richard Bass. Let $f:(0, 1)\to \mathbb{R}$ be a function such that for every $x\in (0, 1)$,   there exist $r>0$ and a Borel measurable function $g$, both depending   on $x$, such that $f$ and $g$ agree on $(x-r, x+r)\cap (0, 1)$. Prove   that $f$ is Borel measurable. Attempt. For each $x\in (0, 1)$, let's denote $r_{x}$ and $g_{x}$ to be the quantities given in the problem statement. By choosing $r_x$ sufficiently small, we may assume that $(x-r_{x}, x+r_{x})\subseteq (0, 1)$. Then for $a\in\mathbb{R}$, we have $$ f^{-1}((a, \infty))=\{y: f(y)>a\}=\bigcup_{x\in (0, 1)} \{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x}) $$ Since $g_{x}$ is Borel measurable, we know that $\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$ is a Borel set. But the union above is uncountable; so how do we show that $f^{-1}((a, \infty))$ is a Borel set? Attempt 2. Since $\mathbb{Q}$ is countable and dense in $(0, 1)$, we can try looking at the points $x\in\mathbb{Q}\cap (0, 1)$. But then the intervals $(x-r_{x}, x+r_{x})$ need not cover $(0, 1)$. The standard example is as follows. Suppose $\{q_1, q_2, …\}$ is enumeration of rationals in $(0, 1)$, and let $\varepsilon>0$. If we let $r_{q_{j}}$ to be $\varepsilon 2^{-j}$, then the measure of $\bigcup_{j=1}^{\infty} (q_j-r_{q_{j}}, q_{j}+r_{q_{j}})$ is at most $2\varepsilon$. Attempt 3. If each $g_{x}$ is continuous, we would be done. In that case, $$\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$$ would be an open set; so as a union of open sets, $f^{-1}((a, \infty))$ would be an open set, and hence Borel.",,"['real-analysis', 'measure-theory']"
9,Gronwall's Lemma (Discrete version),Gronwall's Lemma (Discrete version),,I have the following exercise: Gronwall's lemma (Discrete version): Let $(u_n)$ and $(w_n)$ be nonnegative sequences satisfying $$ u_n \leq \alpha + \sum_{k=0}^{n-1}u_kw_k \quad \forall n. $$ Then for all $n$ it holds $$ u_n \leq \alpha \exp\biggl( \sum_{k=0}^{n-1} w_k \biggr). $$ Proof the lemma by the following steps: (i) Verify the identity $$ 1+\sum_{k=0}^{n-1}\biggl( \prod_{l = 0}^{k-1} (1+w_l) \biggr)w_k \leq \prod_{k = 0}^{n-1} (1+w_k). $$ (ii) Proof by induction that for all $n$ it holds $$ u_n \leq \alpha \prod_{k = 0}^{n-1} (1+w_k). $$ (iii) Deduce the lemma. I tried to solve this exercise by myself without results on points (ii) and (iii) (I already know how to proof the first point). Can someone provide some ideas or a step-by-step solution?,I have the following exercise: Gronwall's lemma (Discrete version): Let $(u_n)$ and $(w_n)$ be nonnegative sequences satisfying $$ u_n \leq \alpha + \sum_{k=0}^{n-1}u_kw_k \quad \forall n. $$ Then for all $n$ it holds $$ u_n \leq \alpha \exp\biggl( \sum_{k=0}^{n-1} w_k \biggr). $$ Proof the lemma by the following steps: (i) Verify the identity $$ 1+\sum_{k=0}^{n-1}\biggl( \prod_{l = 0}^{k-1} (1+w_l) \biggr)w_k \leq \prod_{k = 0}^{n-1} (1+w_k). $$ (ii) Proof by induction that for all $n$ it holds $$ u_n \leq \alpha \prod_{k = 0}^{n-1} (1+w_k). $$ (iii) Deduce the lemma. I tried to solve this exercise by myself without results on points (ii) and (iii) (I already know how to proof the first point). Can someone provide some ideas or a step-by-step solution?,,['real-analysis']
10,Convergence of the series $\sum_{n=1}^\infty \frac{(2n)!!}{(2n+1)!!} $,Convergence of the series,\sum_{n=1}^\infty \frac{(2n)!!}{(2n+1)!!} ,Study the convergence of the next series: $$\sum_{n=1}^\infty \frac{(2n)!!}{(2n+1)!!} $$ My solution: since $$\frac{(2n)!!}{(2n+2)!!} \leq \frac{(2n)!!}{(2n+1)!!}$$ forall $n \in \mathbb{N}$ and since $$\sum_{n=1}^\infty \frac{(2n)!!}{(2n+2)!!} = \sum_{n=1}^\infty \frac{1}{2n+2} = \infty$$ then the first series diverges. Is there anything wrong? Thanks in advance.,Study the convergence of the next series: $$\sum_{n=1}^\infty \frac{(2n)!!}{(2n+1)!!} $$ My solution: since $$\frac{(2n)!!}{(2n+2)!!} \leq \frac{(2n)!!}{(2n+1)!!}$$ forall $n \in \mathbb{N}$ and since $$\sum_{n=1}^\infty \frac{(2n)!!}{(2n+2)!!} = \sum_{n=1}^\infty \frac{1}{2n+2} = \infty$$ then the first series diverges. Is there anything wrong? Thanks in advance.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
11,Who is responsible for the analytical/topological proof of FTA?,Who is responsible for the analytical/topological proof of FTA?,,"The fundamental theorem of algebra asserts: Theorem Let $P$ be a polynomial of degree $\geq 1$ in $\Bbb C$. Then there exists a $z_1\in\Bbb C$ such that $P(z_1)=0$. The proof sketch goes as follows: Lemma 1 For each $A>0$ there is an $R>0$ such that $|z|\geq R$ implies $|P(z)|\geq A$ Proof For $z\neq 0$, write $$P(z)=a_0z^n Q(z)$$ where $$Q(z)=\sum_{k=0}^n \frac{a_k}{a_0}z^{-k}$$ Then $Q\to 1$ for $z\to \infty$ so there is an $R_1$ for which $|z|\geq R_1$ implies $|Q(z)|\geq 1/2$. Thus for $|z|\geq R_1$ we have $$|P(z)|=|a_0||z|^n| Q(z)|\geq \frac 1 2 |a_0||z|^n$$ From this it is clear we can make $|P(z)|>A$ by taking $|z|>R$ where $$R=\max\left\{R_1,\left(\frac{2A}{|a_0|}\right)^{1/n}\right\}$$ Lemma 2 If $P$ is a polynomial of degree $\geq 1$ and $P(z_1)\neq 0$, given $\delta_0 >0$ there is a $z_2$ such that $|z_1-z_2|<\delta$ and $|P(z_2)|<|P(z_1|$ Proof Consider the particular case $P(0)=1$. We may then write $$P(z)=a_nz^n+a_{n-1}z^{n-1}+\cdots+a_1z+1$$ Now let $a_k$ by the first coefficient $a_0,\dots,a_{n-1}$ that is nonvanishing. Then $$P(z)=1+a_kz^k+\cdots a_nz^n$$ so we may write $$P(z)=1+a_k z^k(1+H(z))$$ where $$H(z)=\frac{a_{k-1}}{a_k}z+\cdots+^\frac{a_0}{a_k}z^{n-k}$$ Since $H(0)=0$; by continuity $|H(z)|<\frac 1 2 $ for each $z$ with $|z|\leq \delta <\delta_0$, where $\delta$ is such that $\delta^k|a_k|<1$. We choose $z_2$ as a solution of $$z^k=-\delta^k\frac{|a_k|}{a_k}$$ Then clearly $|z_2|=\delta$ and $$|P(z_2)|=|1-\delta^k|a_k|-\delta^k|a_k||H(z_2)||\\ \leq 1-\delta^k|a_k|+\delta^k|a_k||H(z_2)|\\ \leq 1-\delta^k|a_k|+\frac 1 2\delta^k|a_k|<1$$ which is what we wanted. In the general case where $P(z_0)\neq 0$, we write $P(z)=\sum a_k z^k$ as $P(z)=\sum b_k(z-z_0)^k$ and note that $P(z)=b_n P_1(z)$ with $P_1(0)=1$, so we profit from what we did before. Finally, the proof PROOF Since $P$ is continuous so is $|P|$, and $|P|\geq 0$. Set thus $\alpha =\inf_{z\in \Bbb C}|P(z)|$. Since we can take, from the first lemma, an $R$ such that $|P(z)|\geq \alpha +1$ we may discard the region $|z|> R$ and write $$\alpha =\inf_{|z|\leq R}|P(z)|$$ By continuity of $|P|$ and compactness of the disk $K=\{z:|z|\leq R\}$, Weierstrass' theorem asserts that for some $z_1$ we have $|P(z_1)|=\alpha$. Since $|P(z_1)|=\alpha<\alpha+1$, $z_1$ is an interior point of the disk $K$, whence there is some $\delta_0$-ball around it. If $\alpha \neq 0$, the second lemma implies we can take some $z_2$ with $|z_1-z_2|<\delta_0$ and $|P(z_2)|<\alpha$ contrary to $\alpha$ being the infimum, whence $|P(z_1)|=\alpha=0$, which implies $P(z_1)=0$ and the theorem is proven. Does anyone know where this proof first appeared? Who produced it? I've seen it already in Spivak's ""Calculus"" and E.G. Shilov's ""Elementary Real and Complex Analysis"".","The fundamental theorem of algebra asserts: Theorem Let $P$ be a polynomial of degree $\geq 1$ in $\Bbb C$. Then there exists a $z_1\in\Bbb C$ such that $P(z_1)=0$. The proof sketch goes as follows: Lemma 1 For each $A>0$ there is an $R>0$ such that $|z|\geq R$ implies $|P(z)|\geq A$ Proof For $z\neq 0$, write $$P(z)=a_0z^n Q(z)$$ where $$Q(z)=\sum_{k=0}^n \frac{a_k}{a_0}z^{-k}$$ Then $Q\to 1$ for $z\to \infty$ so there is an $R_1$ for which $|z|\geq R_1$ implies $|Q(z)|\geq 1/2$. Thus for $|z|\geq R_1$ we have $$|P(z)|=|a_0||z|^n| Q(z)|\geq \frac 1 2 |a_0||z|^n$$ From this it is clear we can make $|P(z)|>A$ by taking $|z|>R$ where $$R=\max\left\{R_1,\left(\frac{2A}{|a_0|}\right)^{1/n}\right\}$$ Lemma 2 If $P$ is a polynomial of degree $\geq 1$ and $P(z_1)\neq 0$, given $\delta_0 >0$ there is a $z_2$ such that $|z_1-z_2|<\delta$ and $|P(z_2)|<|P(z_1|$ Proof Consider the particular case $P(0)=1$. We may then write $$P(z)=a_nz^n+a_{n-1}z^{n-1}+\cdots+a_1z+1$$ Now let $a_k$ by the first coefficient $a_0,\dots,a_{n-1}$ that is nonvanishing. Then $$P(z)=1+a_kz^k+\cdots a_nz^n$$ so we may write $$P(z)=1+a_k z^k(1+H(z))$$ where $$H(z)=\frac{a_{k-1}}{a_k}z+\cdots+^\frac{a_0}{a_k}z^{n-k}$$ Since $H(0)=0$; by continuity $|H(z)|<\frac 1 2 $ for each $z$ with $|z|\leq \delta <\delta_0$, where $\delta$ is such that $\delta^k|a_k|<1$. We choose $z_2$ as a solution of $$z^k=-\delta^k\frac{|a_k|}{a_k}$$ Then clearly $|z_2|=\delta$ and $$|P(z_2)|=|1-\delta^k|a_k|-\delta^k|a_k||H(z_2)||\\ \leq 1-\delta^k|a_k|+\delta^k|a_k||H(z_2)|\\ \leq 1-\delta^k|a_k|+\frac 1 2\delta^k|a_k|<1$$ which is what we wanted. In the general case where $P(z_0)\neq 0$, we write $P(z)=\sum a_k z^k$ as $P(z)=\sum b_k(z-z_0)^k$ and note that $P(z)=b_n P_1(z)$ with $P_1(0)=1$, so we profit from what we did before. Finally, the proof PROOF Since $P$ is continuous so is $|P|$, and $|P|\geq 0$. Set thus $\alpha =\inf_{z\in \Bbb C}|P(z)|$. Since we can take, from the first lemma, an $R$ such that $|P(z)|\geq \alpha +1$ we may discard the region $|z|> R$ and write $$\alpha =\inf_{|z|\leq R}|P(z)|$$ By continuity of $|P|$ and compactness of the disk $K=\{z:|z|\leq R\}$, Weierstrass' theorem asserts that for some $z_1$ we have $|P(z_1)|=\alpha$. Since $|P(z_1)|=\alpha<\alpha+1$, $z_1$ is an interior point of the disk $K$, whence there is some $\delta_0$-ball around it. If $\alpha \neq 0$, the second lemma implies we can take some $z_2$ with $|z_1-z_2|<\delta_0$ and $|P(z_2)|<\alpha$ contrary to $\alpha$ being the infimum, whence $|P(z_1)|=\alpha=0$, which implies $P(z_1)=0$ and the theorem is proven. Does anyone know where this proof first appeared? Who produced it? I've seen it already in Spivak's ""Calculus"" and E.G. Shilov's ""Elementary Real and Complex Analysis"".",,"['real-analysis', 'complex-analysis', 'math-history']"
12,$\lim\limits_{x\to\infty}f(x)^{1/x}$ where $f(x)=\sum\limits_{k=0}^{\infty}\cfrac{x^{a_k}}{a_k!}$.,where .,\lim\limits_{x\to\infty}f(x)^{1/x} f(x)=\sum\limits_{k=0}^{\infty}\cfrac{x^{a_k}}{a_k!},"Does the following limit exist? What is the value of it if it exists? $$\lim\limits_{x\to\infty}f(x)^{1/x}$$ where $f(x)=\sum\limits_{k=0}^{\infty}\cfrac{x^{a_k}}{a_k!}$ and $\{a_k\}\subset\mathbb{N}$ satisfies $a_k<a_{k+1},k=0,1,\cdots$ $\bf{EDIT:}$ I'll show that $f(x)^{1/x}$ is not necessarily monotonically increasing for $x>0$. Since $\lim\limits_{x\to+\infty}\big(x+2\big)^{1/x}=1$, for any $M>0$, we can find some $L > M$ such that $\big(2+L\big)^{1/L}<\sqrt{3}$. It is easy to see that: $$\sum_{k=N}^\infty \frac{x^k}{k!} = \frac{e^{\theta x}}{N!}x^N\leq \frac{x^N}{N!}e^x,\quad \theta\in(0,1)$$ Hence we can choose $N$ big enough such that for any $x\in[0,L]$ $$\sum_{k=N}^\infty \frac{x^k}{k!} \leq 1$$ Now, we let $$a_k=\begin{cases}k,& k=0,1\\ 0,& 2\leq k <N\\ k,& k\geq N\end{cases}$$ Then $f(x)= 1+x+\sum\limits_{k=N}^\infty\frac{x^k}{k!}$ and  $$f(2)^{1/2} \geq \sqrt{3} > (2+L)^{1/L} \geq f(L)^{1/L}$$ which shows that $f(x)^{1/x}$ is not monotonically increasing on $[2,L]$.","Does the following limit exist? What is the value of it if it exists? $$\lim\limits_{x\to\infty}f(x)^{1/x}$$ where $f(x)=\sum\limits_{k=0}^{\infty}\cfrac{x^{a_k}}{a_k!}$ and $\{a_k\}\subset\mathbb{N}$ satisfies $a_k<a_{k+1},k=0,1,\cdots$ $\bf{EDIT:}$ I'll show that $f(x)^{1/x}$ is not necessarily monotonically increasing for $x>0$. Since $\lim\limits_{x\to+\infty}\big(x+2\big)^{1/x}=1$, for any $M>0$, we can find some $L > M$ such that $\big(2+L\big)^{1/L}<\sqrt{3}$. It is easy to see that: $$\sum_{k=N}^\infty \frac{x^k}{k!} = \frac{e^{\theta x}}{N!}x^N\leq \frac{x^N}{N!}e^x,\quad \theta\in(0,1)$$ Hence we can choose $N$ big enough such that for any $x\in[0,L]$ $$\sum_{k=N}^\infty \frac{x^k}{k!} \leq 1$$ Now, we let $$a_k=\begin{cases}k,& k=0,1\\ 0,& 2\leq k <N\\ k,& k\geq N\end{cases}$$ Then $f(x)= 1+x+\sum\limits_{k=N}^\infty\frac{x^k}{k!}$ and  $$f(2)^{1/2} \geq \sqrt{3} > (2+L)^{1/L} \geq f(L)^{1/L}$$ which shows that $f(x)^{1/x}$ is not monotonically increasing on $[2,L]$.",,"['real-analysis', 'limits', 'power-series']"
13,Under what conditions can I interchange the order of limits for a function of two variable?,Under what conditions can I interchange the order of limits for a function of two variable?,,"Suppose I have $f:\mathbb{R}^2 \to \mathbb{R}$.   What conditions do I need to say that $$\lim_{x \to a} \lim_{y \to b} f(x,y) = \lim_{y \to b} \lim_{x \to a} f(x,y)$$ ? What about in a more general case, by taking $X,Y$ and $Z$ topological (Hausdorff) spaces and $f$ from $X \times Y$ to $Z$ ? Thank you","Suppose I have $f:\mathbb{R}^2 \to \mathbb{R}$.   What conditions do I need to say that $$\lim_{x \to a} \lim_{y \to b} f(x,y) = \lim_{y \to b} \lim_{x \to a} f(x,y)$$ ? What about in a more general case, by taking $X,Y$ and $Z$ topological (Hausdorff) spaces and $f$ from $X \times Y$ to $Z$ ? Thank you",,"['real-analysis', 'general-topology', 'limits']"
14,Limit of a decreasing sequence of outer measures is an outer measure?,Limit of a decreasing sequence of outer measures is an outer measure?,,"The problem given to me on my homework is: Prove that the limit of a decreasing family of outer measures is an outer measure. Doing out the ""obvious"" approach, we quickly reach the problem of wanting to say that $$\lim_{j\to\infty}\;\mu_j^*\left(\bigcup_{i=1}^\infty A_i\right)\leq\lim_{j\to\infty}\left(\sum_{i=1}^\infty\;\mu_j^*(A_i)\right)\underbrace{\leq}_{\text{PROBLEM}}\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\mu_j^*(A_i)\right)$$ The problem, as I see it, is that Fatou's Lemma has the inequality going the opposite way of what we want here (by the way, this course hasn't actually gotten to integration yet). In fact, I think the claim is false, because I can imagine very well having the following scenario: $\{A_i\}_{i=1}^\infty\subset \mathcal{P}(X)$ is a family of subsets of $X$ (disjoint, probably) $\mu_j^*:\mathcal{P}(X)\to[0,\infty]$ are decreasing family of outer measures on $X$ with the property that $$\mu_j^*(A_i)=\begin{cases}1\text{ if }j\leq i\\ 0\text{ if }j>i \end{cases}$$ (This certainly would not be in conflict with the assumption that the $\mu_j^*$ are decreasing, i.e. that $\mu_j^*(A)\geq\mu_{j+1}^*(A)$ for all $j\in\mathbb{N}$ and $A\subseteq X$.) Letting $\mu^*:\mathcal{P}(X)\to[0,\infty]$ be defined by $\mu^*(A)=\lim_{j\to\infty}\mu_j^*(A)$, we conclude immediately that $\mu^*(\varnothing)=0$ and that $\mu^*(A)\leq\mu^*(B)$ when $A\subseteq B$, but we then have that $$\mu^*\left(\bigcup_{i=1}^\infty A_i\right)=\lim_{j\to\infty}\;\mu_j^*\left(\bigcup_{i=1}^\infty A_i\right)\geq\lim_{j\to\infty}\;\mu_j^*(A_j)=\lim_{j\to\infty}\;1=1$$ while $$\sum_{i=1}^\infty\;\mu^*(A_i)=\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\mu_j^*(A_i)\right)=\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\;{1\text{ if }j\leq i\atop 0\text{ if }j>i}\right)=\sum_{i=1}^\infty\;0=0 $$ so that  $$\mu^*\left(\bigcup_{i=1}^\infty A_i\right)\not\leq\sum_{i=1}^\infty\;\mu^*(A_i)$$ and therefore $\mu^*$ is not an outer measure. So, giving as little away as possible (as this is a homework question), is the scenario I proposed above impossible? I tried to construct examples with $X=\mathbb{N}$ and the value of $\mu_j^*(A)$ depending on whether $A\cap\{1,\ldots,j\}=\varnothing$, but that didn't get anywhere.","The problem given to me on my homework is: Prove that the limit of a decreasing family of outer measures is an outer measure. Doing out the ""obvious"" approach, we quickly reach the problem of wanting to say that $$\lim_{j\to\infty}\;\mu_j^*\left(\bigcup_{i=1}^\infty A_i\right)\leq\lim_{j\to\infty}\left(\sum_{i=1}^\infty\;\mu_j^*(A_i)\right)\underbrace{\leq}_{\text{PROBLEM}}\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\mu_j^*(A_i)\right)$$ The problem, as I see it, is that Fatou's Lemma has the inequality going the opposite way of what we want here (by the way, this course hasn't actually gotten to integration yet). In fact, I think the claim is false, because I can imagine very well having the following scenario: $\{A_i\}_{i=1}^\infty\subset \mathcal{P}(X)$ is a family of subsets of $X$ (disjoint, probably) $\mu_j^*:\mathcal{P}(X)\to[0,\infty]$ are decreasing family of outer measures on $X$ with the property that $$\mu_j^*(A_i)=\begin{cases}1\text{ if }j\leq i\\ 0\text{ if }j>i \end{cases}$$ (This certainly would not be in conflict with the assumption that the $\mu_j^*$ are decreasing, i.e. that $\mu_j^*(A)\geq\mu_{j+1}^*(A)$ for all $j\in\mathbb{N}$ and $A\subseteq X$.) Letting $\mu^*:\mathcal{P}(X)\to[0,\infty]$ be defined by $\mu^*(A)=\lim_{j\to\infty}\mu_j^*(A)$, we conclude immediately that $\mu^*(\varnothing)=0$ and that $\mu^*(A)\leq\mu^*(B)$ when $A\subseteq B$, but we then have that $$\mu^*\left(\bigcup_{i=1}^\infty A_i\right)=\lim_{j\to\infty}\;\mu_j^*\left(\bigcup_{i=1}^\infty A_i\right)\geq\lim_{j\to\infty}\;\mu_j^*(A_j)=\lim_{j\to\infty}\;1=1$$ while $$\sum_{i=1}^\infty\;\mu^*(A_i)=\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\mu_j^*(A_i)\right)=\sum_{i=1}^\infty\left(\lim_{j\to\infty}\;\;{1\text{ if }j\leq i\atop 0\text{ if }j>i}\right)=\sum_{i=1}^\infty\;0=0 $$ so that  $$\mu^*\left(\bigcup_{i=1}^\infty A_i\right)\not\leq\sum_{i=1}^\infty\;\mu^*(A_i)$$ and therefore $\mu^*$ is not an outer measure. So, giving as little away as possible (as this is a homework question), is the scenario I proposed above impossible? I tried to construct examples with $X=\mathbb{N}$ and the value of $\mu_j^*(A)$ depending on whether $A\cap\{1,\ldots,j\}=\varnothing$, but that didn't get anywhere.",,"['real-analysis', 'measure-theory']"
15,Measurability of the composition of a measurable map with a surjective map satisfying an expansion condition,Measurability of the composition of a measurable map with a surjective map satisfying an expansion condition,,"I am trying to figure out the following problem in measure theory and am stuck.  It seems like it should be very easy, so I must be missing something. Let $g: \mathbb{R} \to \mathbb{R}$ be a mapping of $\mathbb{R}$ onto $\mathbb{R}$ for which there is a constant $c > 0$ for which $$ |g(u) - g(v)| \geq c \cdot |u-v| \text{ for all } u, v \in \mathbb{R}. $$ (Note to avoid confusion: this function is NOT Lipschitz and not supposed to be.) Show that if $f: \mathbb{R} \to \mathbb{R}$ is Lebesgue measurable, then so is the composition $f \circ g$. I see that we need to show that $g$ maps measurable sets to measurable sets.  I know how to show $g$ is injective and that bounded sets are mapped to and from bounded sets... but I'm not sure where to go from there. I'd appreciate a nudge in the right direction.  Please do not give away the whole problem, if possible.","I am trying to figure out the following problem in measure theory and am stuck.  It seems like it should be very easy, so I must be missing something. Let $g: \mathbb{R} \to \mathbb{R}$ be a mapping of $\mathbb{R}$ onto $\mathbb{R}$ for which there is a constant $c > 0$ for which $$ |g(u) - g(v)| \geq c \cdot |u-v| \text{ for all } u, v \in \mathbb{R}. $$ (Note to avoid confusion: this function is NOT Lipschitz and not supposed to be.) Show that if $f: \mathbb{R} \to \mathbb{R}$ is Lebesgue measurable, then so is the composition $f \circ g$. I see that we need to show that $g$ maps measurable sets to measurable sets.  I know how to show $g$ is injective and that bounded sets are mapped to and from bounded sets... but I'm not sure where to go from there. I'd appreciate a nudge in the right direction.  Please do not give away the whole problem, if possible.",,"['real-analysis', 'measure-theory']"
16,Prove $\int_{0}^{1} \frac{k^{\frac34}}{(1-k^2)^\frac38} K(k)\text{d}k=\frac{\pi^2}{12}\sqrt{5+\frac{1}{\sqrt{2} } }$,Prove,\int_{0}^{1} \frac{k^{\frac34}}{(1-k^2)^\frac38} K(k)\text{d}k=\frac{\pi^2}{12}\sqrt{5+\frac{1}{\sqrt{2} } },The paper mentioned a proposition: $$ \int_{0}^{1} \frac{k^{\frac34}}{(1-k^2)^\frac38} K(k)\text{d}k=\frac{\pi^2}{12}\sqrt{5+\frac{1}{\sqrt{2} } }. $$ Its equivalent is $$ \int_{0}^{\infty}\vartheta_2(q)^3\vartheta_4(q)^2 \sqrt{\vartheta_2(q)\vartheta_4(q)}\text{d}x =\frac{1}{3} \sqrt{5+\frac{1}{\sqrt{2} } }. $$ I think that there is a $L$ -series satisfying $$ \int_{0}^{\infty}x^{s-1}\vartheta_2(q)^3\vartheta_4(q)^2 \sqrt{\vartheta_2(q)\vartheta_4(q)}\text{d}x =L_f(s)\Gamma(s)\times\text{other components}. $$,The paper mentioned a proposition: Its equivalent is I think that there is a -series satisfying,"
\int_{0}^{1} \frac{k^{\frac34}}{(1-k^2)^\frac38}
K(k)\text{d}k=\frac{\pi^2}{12}\sqrt{5+\frac{1}{\sqrt{2} } }.
 
\int_{0}^{\infty}\vartheta_2(q)^3\vartheta_4(q)^2
\sqrt{\vartheta_2(q)\vartheta_4(q)}\text{d}x
=\frac{1}{3} \sqrt{5+\frac{1}{\sqrt{2} } }.
 L 
\int_{0}^{\infty}x^{s-1}\vartheta_2(q)^3\vartheta_4(q)^2
\sqrt{\vartheta_2(q)\vartheta_4(q)}\text{d}x
=L_f(s)\Gamma(s)\times\text{other components}.
","['real-analysis', 'integration', 'definite-integrals', 'elliptic-integrals', 'dirichlet-series']"
17,A math analysis problem.,A math analysis problem.,,"Suppose $f(x)$ is a differentiable function on $\mathbb R$ with continuous derivative. For any $x \in \mathbb R$ , $f’(x)>f(f(x))$ . Prove that for any $x\ge 0$ , $f(f(f(x)))\le 0$ . I don’t know how to use the continuity of the function’s derivative in this problem. The only thing I get right now is that $f(f(f(x)))\le f’(f(x))$ by substituting $f(x)$ into $x$ , but I can’t prove $f’(f(x))\le 0$ .","Suppose is a differentiable function on with continuous derivative. For any , . Prove that for any , . I don’t know how to use the continuity of the function’s derivative in this problem. The only thing I get right now is that by substituting into , but I can’t prove .",f(x) \mathbb R x \in \mathbb R f’(x)>f(f(x)) x\ge 0 f(f(f(x)))\le 0 f(f(f(x)))\le f’(f(x)) f(x) x f’(f(x))\le 0,['real-analysis']
18,When is the derivative of $f(g(x))$ equal to $g(f'(x))$?,When is the derivative of  equal to ?,f(g(x)) g(f'(x)),"By the chain rule, we know that the derivative of $f(g(x))$ is $f'(g(x))g'(x)$ . Question : When is the derivative of $f(g(x))$ equal to $g(f'(x))$ ? Trivial solutions include the following: Let $f$ be any differentiable function and $g$ be the identity function ( $g(x)=x$ ) or the zero function ( $g(x)=0$ ). Let $f$ be any constant function and $g$ be any differentiable function that fixes zero. On the other hand, $f(x)=x^2$ and $g(x)=\sin x$ form a nontrivial solution (by the double angle formula for sine). If $g(x)=x+a$ , where $a$ is a constant, then $f'(x+a)=f'(x)+a$ , so $f(x)=\frac{1}{2}x^2+bx+c$ would work for any two constants $b$ and $c$ . Finally, if $f(x)=mx+b$ , where $m$ and $b$ are constants, then $mg'(x)=g(m)$ , so $g'(x)=\frac{g(m)}{m}$ and $g(x)=\frac{g(m)}{m}x+c$ for some constant $c$ . But this must in particular be true for $x=m$ , so $c$ must be zero and $g(x)=nx$ would then work for any constant $n$ . But I do not know any other solutions. Can anyone help find one? Note that $f$ may be replaced by any other function with the same derivative (i.e. differing from $f$ by a constant) without changing the validity of the equation, so we may assume without lost of generality that $f$ fixes zero (assuming, of course, that zero is in the domain of $f$ ).","By the chain rule, we know that the derivative of is . Question : When is the derivative of equal to ? Trivial solutions include the following: Let be any differentiable function and be the identity function ( ) or the zero function ( ). Let be any constant function and be any differentiable function that fixes zero. On the other hand, and form a nontrivial solution (by the double angle formula for sine). If , where is a constant, then , so would work for any two constants and . Finally, if , where and are constants, then , so and for some constant . But this must in particular be true for , so must be zero and would then work for any constant . But I do not know any other solutions. Can anyone help find one? Note that may be replaced by any other function with the same derivative (i.e. differing from by a constant) without changing the validity of the equation, so we may assume without lost of generality that fixes zero (assuming, of course, that zero is in the domain of ).",f(g(x)) f'(g(x))g'(x) f(g(x)) g(f'(x)) f g g(x)=x g(x)=0 f g f(x)=x^2 g(x)=\sin x g(x)=x+a a f'(x+a)=f'(x)+a f(x)=\frac{1}{2}x^2+bx+c b c f(x)=mx+b m b mg'(x)=g(m) g'(x)=\frac{g(m)}{m} g(x)=\frac{g(m)}{m}x+c c x=m c g(x)=nx n f f f f,"['real-analysis', 'functions', 'derivatives', 'chain-rule']"
19,On the infinity of $\{p\in \mathbb {N}:\exists n\in\mathbb{N}~p| \left \lfloor{r^n}\right \rfloor\}$,On the infinity of,\{p\in \mathbb {N}:\exists n\in\mathbb{N}~p| \left \lfloor{r^n}\right \rfloor\},"For which $r\in\mathbb{R}$ is the set $\mathscr{P}_r=\{p \in \mathbb{P}:\ (\exists n\in\mathbb{N})(p\mid\lfloor{r^n}\rfloor)\}$ infinite? Of course, if $r\in \mathbb{Z}$ then $\mathscr{P}_r $ is finite. We can prove, for example, that for all non-constant $f\in\mathbb{Z}[x]$ the set $\{p \text{ prime}:\ (\exists n\in\mathbb{N})(p\mid f(n))\}$ is infinite (refer to this question ). Actually this works somewhat more generally. Proposition. Let $f:\mathbb{N}\rightarrow\mathbb{N}$ s.t. $(\forall k\in \mathbb{N})(\exists n\in\mathbb{N})(n>\log_2 (\max_{x\leq n}f(x))^k)$ and $f$ is uniformly quasi-injective, that is $(\exists M)(\forall n\in\Bbb{N})(|f^{-1}(n)|\leq M)$ . Then $\mathscr{P}=\{p \text{ prime}:\ (\exists n\in\mathbb{N})(p\mid f(n))\}$ is infinite. Proof. Let $f$ be such a function, and let's assume, for simplicity, that $f$ is injective.  Let's suppose by contradiction that $\mathscr{P}=\{p_1,...,p_k\}$ .   Let $F_n=\{f(x):\ x\leq n\}$ . Let's define $\phi :F_n\rightarrow S$ where S is the set of strings of natural numbers of length k such that every entry is less or equal to $\log_2 (\max_{x\leq n}f(x))$ , and $x=p_1^{\phi(x)_1}...p_k^{\phi(x)_k}$ . By the fundamental theorem of arithmetic, and the fact that $2$ is the smallest positive prime, we conclude that $\phi$ is injective and well-defined. So $\phi \circ f$ is also injective.   Thus, $n\leq|S|\leq \log_2 (\max_{x\leq n}f(x))^k$ , a contradiction. However, we are not in the hypothesis of the above proposition. The natural conjecture would  be that Conjecture. $\mathscr{P}_r$ is infinite $\iff r\in \mathbb{R}\setminus \mathbb{Z}$ There is some empirical evidence that supports this conjecture. Indeed, by implementing the following code on Octave we can check what the elements of $\mathscr{P}_{r,m}=\{p\in\mathbb{P}:\ (\exists n\in\mathbb{N})(n\leq m)(p\mid\lfloor{r^n} \rfloor)\}$ are. function p=weirdcount(r,m) p=[]; v=[]; for k=1:m   p=union(p,factor(floor((r)^k)));   v(k)=length(p);   endfor plot(v,'*');   end The function also plots $|\mathscr{P}_{r,k}|$ against $k$ . The following are four examples of graphs with $r=1.5,e,\pi, \sqrt{2}$ and $m=30,30,30,60$ These graphs seem to suggest that Conjecture. $r\in \mathbb{R}\setminus \mathbb{Z} \iff |\mathscr{P}_{r,k}|=\mathcal{O}(k)$ A preliminary result toward the proof of these conjectures may be that Proposition. Let $r\in \mathbb{R}^{+}$ and $n\in\mathbb{N}, n\geq 2$ , such that $\lfloor {r^a} \rfloor$ is a power of $n$ for all $a\in \mathbb{N}$ . Then $r$ is a power of $n$ . Proof. Lengthy but easy. Moreover, we can prove that Proposition. $2\in \mathscr{P}_r$ almost always for $|r|>1$ Proof. Let $\mathscr{R}_p=\{r\in \mathbb{R}:p\in \mathscr{P}_r\}$ . We have to prove that $\forall 1<a<b~~b-a=|\mathscr{R}_2\cap (a,b)|$ .    If we prove that, given an interval $(a,b)$ like above $\forall \epsilon >0$ , there is always a plurinterval (a finite union of intervals) $I\subset \mathscr{R}_2\cap (a,b)$ such that $|I|\geq \frac{b-a}{2}-\epsilon$ we are finished.   That, however, is implied by $\lim_{n\to\infty}\int_{a}^{b}(-1)^{\lfloor x^n\rfloor}dx=0$ .   We observe that (with a slight abuse of notation) $$\int_{a}^{b}(-1)^{\lfloor x^n\rfloor}dx \sim \sum_{k=a^n}^{b^n}(-1)^n((k+1)^{\frac{1}{n}}-k^{\frac{1}{n}})$$ $$\sim \sum_{k=\frac{a^n}{2}}^{\frac{b^n}{2}}2(2k+1)^{\frac{1}{n}}-(2k)^{\frac{1}{n}}-(2k+2)^{\frac{1}{n}}$$ By applying Lagrange's Theorem twice, we obtain $$0<2(2k+1)^{\frac{1}{n}}-(2k)^{\frac{1}{n}}-(2k+2)^{\frac{1}{n}}<2(\frac{1}{n}-\frac{1}{n^2})(2k+2)^{\frac{1}{n}-2}$$ So the last summation is asymptotically limited from above by $$h(n)\int_{\frac{a^n}{2}+1}^{\frac{b^n}{2}+1}x^{\frac{1}{n}-2}dx$$ (where $h(n)$ is a function s.t. $\lim_{n\to\infty}h(n)=0$ ) which equals. setting $m=\frac{1}{n}-1$ $$h(\frac{1}{m+1})\frac{(\frac{b^{\frac{1}{m+1}}}{2}+1)^m-(\frac{a^{\frac{1}{m+1}}}{2}+1)^m}{m}$$ which, as $n$ goes to $\infty$ (and so $m$ goes to $-1$ ), has limit equal to $0$ . So, we are done. This method can be easily adapted to get this more general, but still quite weak result. Proposition. $ \forall p\in \mathbb{P} \liminf_{x\to\infty}\frac{|\mathscr{R}_p\cap(1,x)|}{x-1}\geq \frac{1}{p-1}$ Also, the above proof shows that $\forall p\in \mathbb{P}~\mathscr{R}_p$ is dense in $(-\infty,1] \cup [1,\infty)$ . Of course, if we were able to prove that $\forall p\in\mathbb{P}~p\in \mathscr{P}_r$ almost always for $|r|>1$ , this would imply a nice corollary, ie, that $\mathscr{P}_r$ is almost always infinite, for |r|>1.","For which is the set infinite? Of course, if then is finite. We can prove, for example, that for all non-constant the set is infinite (refer to this question ). Actually this works somewhat more generally. Proposition. Let s.t. and is uniformly quasi-injective, that is . Then is infinite. Proof. Let be such a function, and let's assume, for simplicity, that is injective.  Let's suppose by contradiction that .   Let . Let's define where S is the set of strings of natural numbers of length k such that every entry is less or equal to , and . By the fundamental theorem of arithmetic, and the fact that is the smallest positive prime, we conclude that is injective and well-defined. So is also injective.   Thus, , a contradiction. However, we are not in the hypothesis of the above proposition. The natural conjecture would  be that Conjecture. is infinite There is some empirical evidence that supports this conjecture. Indeed, by implementing the following code on Octave we can check what the elements of are. function p=weirdcount(r,m) p=[]; v=[]; for k=1:m   p=union(p,factor(floor((r)^k)));   v(k)=length(p);   endfor plot(v,'*');   end The function also plots against . The following are four examples of graphs with and These graphs seem to suggest that Conjecture. A preliminary result toward the proof of these conjectures may be that Proposition. Let and , such that is a power of for all . Then is a power of . Proof. Lengthy but easy. Moreover, we can prove that Proposition. almost always for Proof. Let . We have to prove that .    If we prove that, given an interval like above , there is always a plurinterval (a finite union of intervals) such that we are finished.   That, however, is implied by .   We observe that (with a slight abuse of notation) By applying Lagrange's Theorem twice, we obtain So the last summation is asymptotically limited from above by (where is a function s.t. ) which equals. setting which, as goes to (and so goes to ), has limit equal to . So, we are done. This method can be easily adapted to get this more general, but still quite weak result. Proposition. Also, the above proof shows that is dense in . Of course, if we were able to prove that almost always for , this would imply a nice corollary, ie, that is almost always infinite, for |r|>1.","r\in\mathbb{R} \mathscr{P}_r=\{p \in \mathbb{P}:\ (\exists n\in\mathbb{N})(p\mid\lfloor{r^n}\rfloor)\} r\in \mathbb{Z} \mathscr{P}_r  f\in\mathbb{Z}[x] \{p \text{ prime}:\ (\exists n\in\mathbb{N})(p\mid f(n))\} f:\mathbb{N}\rightarrow\mathbb{N} (\forall k\in \mathbb{N})(\exists n\in\mathbb{N})(n>\log_2 (\max_{x\leq n}f(x))^k) f (\exists M)(\forall n\in\Bbb{N})(|f^{-1}(n)|\leq M) \mathscr{P}=\{p \text{ prime}:\ (\exists n\in\mathbb{N})(p\mid f(n))\} f f \mathscr{P}=\{p_1,...,p_k\} F_n=\{f(x):\ x\leq n\} \phi :F_n\rightarrow S \log_2 (\max_{x\leq n}f(x)) x=p_1^{\phi(x)_1}...p_k^{\phi(x)_k} 2 \phi \phi \circ f n\leq|S|\leq \log_2 (\max_{x\leq n}f(x))^k \mathscr{P}_r \iff r\in \mathbb{R}\setminus \mathbb{Z} \mathscr{P}_{r,m}=\{p\in\mathbb{P}:\ (\exists n\in\mathbb{N})(n\leq m)(p\mid\lfloor{r^n} \rfloor)\} |\mathscr{P}_{r,k}| k r=1.5,e,\pi, \sqrt{2} m=30,30,30,60 r\in \mathbb{R}\setminus \mathbb{Z} \iff |\mathscr{P}_{r,k}|=\mathcal{O}(k) r\in \mathbb{R}^{+} n\in\mathbb{N}, n\geq 2 \lfloor {r^a} \rfloor n a\in \mathbb{N} r n 2\in \mathscr{P}_r |r|>1 \mathscr{R}_p=\{r\in \mathbb{R}:p\in \mathscr{P}_r\} \forall 1<a<b~~b-a=|\mathscr{R}_2\cap (a,b)| (a,b) \forall \epsilon >0 I\subset \mathscr{R}_2\cap (a,b) |I|\geq \frac{b-a}{2}-\epsilon \lim_{n\to\infty}\int_{a}^{b}(-1)^{\lfloor x^n\rfloor}dx=0 \int_{a}^{b}(-1)^{\lfloor x^n\rfloor}dx \sim \sum_{k=a^n}^{b^n}(-1)^n((k+1)^{\frac{1}{n}}-k^{\frac{1}{n}}) \sim \sum_{k=\frac{a^n}{2}}^{\frac{b^n}{2}}2(2k+1)^{\frac{1}{n}}-(2k)^{\frac{1}{n}}-(2k+2)^{\frac{1}{n}} 0<2(2k+1)^{\frac{1}{n}}-(2k)^{\frac{1}{n}}-(2k+2)^{\frac{1}{n}}<2(\frac{1}{n}-\frac{1}{n^2})(2k+2)^{\frac{1}{n}-2} h(n)\int_{\frac{a^n}{2}+1}^{\frac{b^n}{2}+1}x^{\frac{1}{n}-2}dx h(n) \lim_{n\to\infty}h(n)=0 m=\frac{1}{n}-1 h(\frac{1}{m+1})\frac{(\frac{b^{\frac{1}{m+1}}}{2}+1)^m-(\frac{a^{\frac{1}{m+1}}}{2}+1)^m}{m} n \infty m -1 0  \forall p\in \mathbb{P} \liminf_{x\to\infty}\frac{|\mathscr{R}_p\cap(1,x)|}{x-1}\geq \frac{1}{p-1} \forall p\in \mathbb{P}~\mathscr{R}_p (-\infty,1] \cup [1,\infty) \forall p\in\mathbb{P}~p\in \mathscr{P}_r |r|>1 \mathscr{P}_r","['real-analysis', 'number-theory']"
20,Identity operator on $L^2(\mathbb{R}^d)$,Identity operator on,L^2(\mathbb{R}^d),"I want to show that the identity operator on $L^2(\mathbb{R}^d)$ cannot be given by an absolutely convergent integral operator. That is, if $K(x,y)$ is a measurable function on $\mathbb{R}^d \times \mathbb{R}^d$ such that for each $f \in L^2(\mathbb{R}^d)$ and $T(f)(x) = \int_{\mathbb{R}^d} K(x,y)f(y)dy$ converges for almost every $x$, then $T(f) \neq f$ for some $f$. Therefore, suppose that $T(f)(x)$ converges absolutely for almost every $x$ and $T(f) = f$ for all $f$. Then $$f(x) = \int_{\mathbb{R}^d} K(x,y) f(y) dy \leq \int_{\mathbb{R}^d} \left| K(x,y)f(y) \right|dy< \infty.$$ I don't really know how to proceed from here.","I want to show that the identity operator on $L^2(\mathbb{R}^d)$ cannot be given by an absolutely convergent integral operator. That is, if $K(x,y)$ is a measurable function on $\mathbb{R}^d \times \mathbb{R}^d$ such that for each $f \in L^2(\mathbb{R}^d)$ and $T(f)(x) = \int_{\mathbb{R}^d} K(x,y)f(y)dy$ converges for almost every $x$, then $T(f) \neq f$ for some $f$. Therefore, suppose that $T(f)(x)$ converges absolutely for almost every $x$ and $T(f) = f$ for all $f$. Then $$f(x) = \int_{\mathbb{R}^d} K(x,y) f(y) dy \leq \int_{\mathbb{R}^d} \left| K(x,y)f(y) \right|dy< \infty.$$ I don't really know how to proceed from here.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'operator-theory', 'integral-operators']"
21,Alternate Characterization of Linearity,Alternate Characterization of Linearity,,"This question is prompted by this video on matrices and linear transformations , which I highly recommend as a pedagogical tool.  In it, the author characterizes linear transformations in the following way (I'm paraphrasing and formalizing) Define a line in a vector space to be a set of the form $L = \{u + t\,v: t \in \Bbb R\}$ for some vectors $u$ and $v$ (note: $L$ may consist of a single point) .  That is, $S$ is an affine subspace of dimension at most $1$ . A function $T: \Bbb R^n \to \Bbb R^m$ is linear if: $T(0) = 0$ For any line $L \subset \Bbb R^n$ , the image $T(L)$ is a line in $\Bbb R^m$ I like this definition because of its geometric appeal and the fact that it manages to ""put the line in linear"". Of course, the traditional definition of a linear map is one which preserves linear combinations. My Question: How should one prove that a function satisfying this definition preserves linear combinations? Can this be proven in a beginner-friendly way? I'll admit I haven't really banged my head against this one, but here are my thoughts: it is equivalent to prove that a function that satisfies only the second condition (i.e. maps lines to lines) is an affine transformation , i.e. that it preserves affine combinations .  From there, it would suffice to note that if $T$ is affine, then $x \mapsto T(x) - T(0)$ is linear. That being said, I don't see a quick way to handle that proof off the top of my head.  Moreover, if this really is the quickest way to reach a proof, it seems that proving this in linear algebra 101 is a bit too ambitious (which is not to say that this fact fails to be pedagogically useful).  I'm guessing a little real-analysis might have to come in at some point. A note for myself and future visitors: If we are also given that 𝑇 is bijective, then this is a consequence of the fundamental theorem of projective geometry .","This question is prompted by this video on matrices and linear transformations , which I highly recommend as a pedagogical tool.  In it, the author characterizes linear transformations in the following way (I'm paraphrasing and formalizing) Define a line in a vector space to be a set of the form for some vectors and (note: may consist of a single point) .  That is, is an affine subspace of dimension at most . A function is linear if: For any line , the image is a line in I like this definition because of its geometric appeal and the fact that it manages to ""put the line in linear"". Of course, the traditional definition of a linear map is one which preserves linear combinations. My Question: How should one prove that a function satisfying this definition preserves linear combinations? Can this be proven in a beginner-friendly way? I'll admit I haven't really banged my head against this one, but here are my thoughts: it is equivalent to prove that a function that satisfies only the second condition (i.e. maps lines to lines) is an affine transformation , i.e. that it preserves affine combinations .  From there, it would suffice to note that if is affine, then is linear. That being said, I don't see a quick way to handle that proof off the top of my head.  Moreover, if this really is the quickest way to reach a proof, it seems that proving this in linear algebra 101 is a bit too ambitious (which is not to say that this fact fails to be pedagogically useful).  I'm guessing a little real-analysis might have to come in at some point. A note for myself and future visitors: If we are also given that 𝑇 is bijective, then this is a consequence of the fundamental theorem of projective geometry .","L = \{u + t\,v: t \in \Bbb R\} u v L S 1 T: \Bbb R^n \to \Bbb R^m T(0) = 0 L \subset \Bbb R^n T(L) \Bbb R^m T x \mapsto T(x) - T(0)","['real-analysis', 'linear-algebra', 'matrices', 'geometry', 'education']"
22,"What are the ""right"" spaces for the Laplace transform","What are the ""right"" spaces for the Laplace transform",,"There are for example several canonical spaces to define the Fourier transform (i.e. Schwartz's space). Is there also a particularly suitable space to define the Laplace transform, so that the Laplace transform is at least bijective?","There are for example several canonical spaces to define the Fourier transform (i.e. Schwartz's space). Is there also a particularly suitable space to define the Laplace transform, so that the Laplace transform is at least bijective?",,"['real-analysis', 'analysis']"
23,Deriving Fourier inversion formula from Fourier series,Deriving Fourier inversion formula from Fourier series,,"Let $g\in C_0^{\infty}(\mathbb{R})$ (infinitely differentiable with compact support), and let $$\hat{g}(y)=\int_{-\infty}^\infty g(x)e^{-ixy}dx$$ Assume that $\hat{g}$ is in the Schwartz class. Prove that $$g(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{g}(y)e^{ixy}dy$$ We may use the result that if $f\in C^{\infty}(\mathbb{R})$ is a periodic function of period $2L$ , then $$\hat{f}(x)=\sum_{n=-\infty}^\infty \left(\dfrac{1}{2L}\int_{-L}^Lf(y)e^{-in\pi y/L}dy\right)e^{i\pi nx/L}$$ I'm trying to follow Steven Stadnicki 's hint. Since $g$ has compact support, let $N$ be such that $g(x)=0$ for all $|x|>N$ . Choose $L>N$ , and let $f_L(x)=g(x)$ for $|x|\leq N$ and extend $f_L(x)$ periodically with period $2L$ to all of $\mathbb{R}$ . Then we have $$\hat{f_L}(x)=\sum_{n=-\infty}^\infty \left(\dfrac{1}{2L}\int_{-L}^Lf(y)e^{-in\pi y/L}dy\right)e^{i\pi nx/L}$$ If I send $L$ to $\infty$ , in some sense I get the function $g$ . But I'm still confused how the Fourier coefficients of $f_L$ will translate to the coefficients of $g$ .","Let (infinitely differentiable with compact support), and let Assume that is in the Schwartz class. Prove that We may use the result that if is a periodic function of period , then I'm trying to follow Steven Stadnicki 's hint. Since has compact support, let be such that for all . Choose , and let for and extend periodically with period to all of . Then we have If I send to , in some sense I get the function . But I'm still confused how the Fourier coefficients of will translate to the coefficients of .",g\in C_0^{\infty}(\mathbb{R}) \hat{g}(y)=\int_{-\infty}^\infty g(x)e^{-ixy}dx \hat{g} g(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{g}(y)e^{ixy}dy f\in C^{\infty}(\mathbb{R}) 2L \hat{f}(x)=\sum_{n=-\infty}^\infty \left(\dfrac{1}{2L}\int_{-L}^Lf(y)e^{-in\pi y/L}dy\right)e^{i\pi nx/L} g N g(x)=0 |x|>N L>N f_L(x)=g(x) |x|\leq N f_L(x) 2L \mathbb{R} \hat{f_L}(x)=\sum_{n=-\infty}^\infty \left(\dfrac{1}{2L}\int_{-L}^Lf(y)e^{-in\pi y/L}dy\right)e^{i\pi nx/L} L \infty g f_L g,"['real-analysis', 'integration', 'fourier-analysis']"
24,"If $(c_n)$ is a decreasing sequence of positive real numbers and if $\sum_n c_n\sin{nx}$ is uniformly convergent, then $\lim{(nc_n)}=0$","If  is a decreasing sequence of positive real numbers and if  is uniformly convergent, then",(c_n) \sum_n c_n\sin{nx} \lim{(nc_n)}=0,"Let $(c_n)$ be a decreasing sequence of positive real numbers. If $\sum_n c_n\sin{nx}$ is uniformly convergent, then show that $\lim{(nc_n)}=0$. This problem is from the book Introduction to Real Analysis by Bartle and Sherbert. I attempted it several times but never could get anywhere near the solution. Any help? Thanks and regards.","Let $(c_n)$ be a decreasing sequence of positive real numbers. If $\sum_n c_n\sin{nx}$ is uniformly convergent, then show that $\lim{(nc_n)}=0$. This problem is from the book Introduction to Real Analysis by Bartle and Sherbert. I attempted it several times but never could get anywhere near the solution. Any help? Thanks and regards.",,"['real-analysis', 'sequences-and-series']"
25,prove that every continuous function is integrable,prove that every continuous function is integrable,,"Can someone tell me whether this is correct thank you! We know that if a function f is continuous on $[a,b]$, a closed finite interval, then f is uniformly continuous on that interval. This means that if we're given any $\varepsilon > 0$, there exists $\delta > 0$ such that if $x$ and $y$ are any two points in $[a,b]$ with $|x-y| < \delta$, then $|f(x) - f(y)| < \varepsilon$. So let's say we're given an epsilon. To show that a continuous function f is integrable, we must find a delta such that: For all partitions $\Gamma= \{x_0< \ldots < x_n\}$ of $[a,b]$ with $|\Gamma|:=\max \{x_{i+1} - x_i\} < \delta$ we have  $$\mathrm{S}_{\delta} - \mathrm{s}_{\delta} < \varepsilon,$$ where,  $$\mathrm{S}_{\delta}:=\inf \Sigma\  M_i(x_{i+1} - x_i)\ \mathrm{and}\ \mathrm{s}_{\delta}:=\inf \Sigma\  m_i(x_{i+1} - x_i),$$ over all partitions $\Gamma$ that satisfies $|\Gamma|<\delta$, with  $M_i:=\max f|_{[x_i, x_i+1]}$ and $m_i:=\min f|_{[x_i, x_i+1]}$. So, to recap, for a given epsilon we must find a delta such that $\mathrm{S}_{\delta} - \mathrm{s}_{\delta} < \varepsilon$. Since $f$ is uniformly continuous on $[a,b]$, we can choose $\delta$ such that  $$|x-y| < \delta  \ \Rightarrow \ |f(x) - f(y)| < \varepsilon/(b-a).$$ Then, for this $\delta$, we have, for any partition $\Gamma$ with $|\Gamma| <\delta$, that $M_i - m_i < \epsilon/(b-a)$. Therefore, $$\mathrm{S}_{\delta}-\mathrm{s}_{\delta} < \frac\varepsilon{(b-a)} (b-a) = \varepsilon.$$","Can someone tell me whether this is correct thank you! We know that if a function f is continuous on $[a,b]$, a closed finite interval, then f is uniformly continuous on that interval. This means that if we're given any $\varepsilon > 0$, there exists $\delta > 0$ such that if $x$ and $y$ are any two points in $[a,b]$ with $|x-y| < \delta$, then $|f(x) - f(y)| < \varepsilon$. So let's say we're given an epsilon. To show that a continuous function f is integrable, we must find a delta such that: For all partitions $\Gamma= \{x_0< \ldots < x_n\}$ of $[a,b]$ with $|\Gamma|:=\max \{x_{i+1} - x_i\} < \delta$ we have  $$\mathrm{S}_{\delta} - \mathrm{s}_{\delta} < \varepsilon,$$ where,  $$\mathrm{S}_{\delta}:=\inf \Sigma\  M_i(x_{i+1} - x_i)\ \mathrm{and}\ \mathrm{s}_{\delta}:=\inf \Sigma\  m_i(x_{i+1} - x_i),$$ over all partitions $\Gamma$ that satisfies $|\Gamma|<\delta$, with  $M_i:=\max f|_{[x_i, x_i+1]}$ and $m_i:=\min f|_{[x_i, x_i+1]}$. So, to recap, for a given epsilon we must find a delta such that $\mathrm{S}_{\delta} - \mathrm{s}_{\delta} < \varepsilon$. Since $f$ is uniformly continuous on $[a,b]$, we can choose $\delta$ such that  $$|x-y| < \delta  \ \Rightarrow \ |f(x) - f(y)| < \varepsilon/(b-a).$$ Then, for this $\delta$, we have, for any partition $\Gamma$ with $|\Gamma| <\delta$, that $M_i - m_i < \epsilon/(b-a)$. Therefore, $$\mathrm{S}_{\delta}-\mathrm{s}_{\delta} < \frac\varepsilon{(b-a)} (b-a) = \varepsilon.$$",,"['real-analysis', 'proof-verification']"
26,Equality condition in Minkowski's inequality for $L^{\infty}$,Equality condition in Minkowski's inequality for,L^{\infty},"I am trying to find out when equality holds in Minkowski's inequality for $L^{\infty}$ (i.e. a necessary and sufficient condition for equality). I did a search and there was a discussion for the case where $1<p<\infty$ but not when $p=\infty$ so I am hoping to get some ideas or for someone to point me to a source where this is discussed. I will list a couple of observations I made while working this out (though I'm not sure whether I'm right with these): If $\mu(\{x:|f(x)|\geq\|f+g\|_{\infty}-\|g\|_{\infty}\})=0$ (or with $f$ and $g$ interchanged), then I have the reverse inequality. If I pick $a,b$ such that $\|f\|_{\infty}\leq a<\|f\|_{\infty}+\varepsilon$, $\|g\|_{\infty}\leq b<\|g\|_{\infty}+\varepsilon$, $\mu(\{x:|f(x)|>a\})=0$, $\mu(\{x:|g(x)|>b\})=0$, and for all $c<a+b$ I have $\mu(\{x:|f(x)+g(x)|>c\})>0$, then I also have the reverse inequality.","I am trying to find out when equality holds in Minkowski's inequality for $L^{\infty}$ (i.e. a necessary and sufficient condition for equality). I did a search and there was a discussion for the case where $1<p<\infty$ but not when $p=\infty$ so I am hoping to get some ideas or for someone to point me to a source where this is discussed. I will list a couple of observations I made while working this out (though I'm not sure whether I'm right with these): If $\mu(\{x:|f(x)|\geq\|f+g\|_{\infty}-\|g\|_{\infty}\})=0$ (or with $f$ and $g$ interchanged), then I have the reverse inequality. If I pick $a,b$ such that $\|f\|_{\infty}\leq a<\|f\|_{\infty}+\varepsilon$, $\|g\|_{\infty}\leq b<\|g\|_{\infty}+\varepsilon$, $\mu(\{x:|f(x)|>a\})=0$, $\mu(\{x:|g(x)|>b\})=0$, and for all $c<a+b$ I have $\mu(\{x:|f(x)+g(x)|>c\})>0$, then I also have the reverse inequality.",,['real-analysis']
27,"Is there error in the answer to Spivak's Calculus, problem 5-3(iv)?","Is there error in the answer to Spivak's Calculus, problem 5-3(iv)?",,"I'm puzzled by the answer to a problem for Spivak's Calculus (4E) provided in his Combined Answer Book . Problem 5-3(iv) (p. 108) asks the reader to prove that $\mathop{\lim}\limits_{x \to a} x^{4} =a^{4}$ (for arbitrary $a$) by using some techniques in the text to find a $\delta$ such that $\lvert x^{4} - a^{4} \rvert<\varepsilon$ for all $x$ satisfying $0<\lvert x-a\rvert<\delta$. The answer book begins (p. 67) by using one of these techniques (p. 93) to show that $$\lvert x^{4} - a^{4} \rvert = \lvert (x^{2})^{2} - (a^{2})^{2} \rvert<\varepsilon$$ for $$\lvert x^{2} - a^{2} \rvert <\min \left({\frac{\varepsilon}{2\lvert a^{2}\rvert+1},1}\right) = \delta_{2} .$$ In my answer, I  use the same approach to show that $$\lvert x^{2} - a^{2} \rvert <\delta_{2}$$ for $$\lvert x - a \rvert <\min \left({\frac{\delta_{2}}{2\lvert a\rvert+1},1}\right) = \delta_{1} ,$$ so that $$\lvert x^{4} - a^{4} \rvert<\varepsilon$$ when $$\delta = \delta_{1}=\min \left({\frac{\delta_{2}}{2\lvert a\rvert+1},1}\right). \Box$$ But Spivak's answer book has $$\delta =\min \left({\frac{\delta_{1}}{2\lvert a\rvert+1},1}\right),$$ which I believe is an error.","I'm puzzled by the answer to a problem for Spivak's Calculus (4E) provided in his Combined Answer Book . Problem 5-3(iv) (p. 108) asks the reader to prove that $\mathop{\lim}\limits_{x \to a} x^{4} =a^{4}$ (for arbitrary $a$) by using some techniques in the text to find a $\delta$ such that $\lvert x^{4} - a^{4} \rvert<\varepsilon$ for all $x$ satisfying $0<\lvert x-a\rvert<\delta$. The answer book begins (p. 67) by using one of these techniques (p. 93) to show that $$\lvert x^{4} - a^{4} \rvert = \lvert (x^{2})^{2} - (a^{2})^{2} \rvert<\varepsilon$$ for $$\lvert x^{2} - a^{2} \rvert <\min \left({\frac{\varepsilon}{2\lvert a^{2}\rvert+1},1}\right) = \delta_{2} .$$ In my answer, I  use the same approach to show that $$\lvert x^{2} - a^{2} \rvert <\delta_{2}$$ for $$\lvert x - a \rvert <\min \left({\frac{\delta_{2}}{2\lvert a\rvert+1},1}\right) = \delta_{1} ,$$ so that $$\lvert x^{4} - a^{4} \rvert<\varepsilon$$ when $$\delta = \delta_{1}=\min \left({\frac{\delta_{2}}{2\lvert a\rvert+1},1}\right). \Box$$ But Spivak's answer book has $$\delta =\min \left({\frac{\delta_{1}}{2\lvert a\rvert+1},1}\right),$$ which I believe is an error.",,"['calculus', 'real-analysis', 'limits']"
28,When is a uniformly dense family of functions dense in L^p?,When is a uniformly dense family of functions dense in L^p?,,"Suppose $\mathcal{A}\subset L^p(\mathbb{R})$ is an algebra of functions with the following property: For every compact $K\subset\mathbb{R}$, $\mathcal{A}$ is dense in $\mathcal{C}(K)$ with respect to the uniform norm $\|\cdot\|_{\infty}$, where $\mathcal{C}(K)$ is the collection of real continuous functions on $K$. The uniform norm I'm referring to is $$\|f\|_{\infty}=\sup_{t\in K}|f(t)|.$$ (See http://en.wikipedia.org/wiki/Uniform_norm .) Can we conclude that $\mathcal{A}$ is dense in $L^p(\mathbb{R})$ (with respect to the $L^p$ norm)? I became interested in this question while investigating a special case, the $L^2$-density of finite linear combinations of Gaussians: $$\sum_{i=1}^n\alpha_ie^{-k_i(x-x_i)^2},\qquad\alpha_i,k_i,x_i\in\mathbb{R},k_i>0.$$ The question above occurred to me because I can imagine it being useful in cases like this to, say, verify the hypotheses of the Stone-Weierstrass theorem for a given family of functions rather than to explicitly approximate functions in $L^p$.","Suppose $\mathcal{A}\subset L^p(\mathbb{R})$ is an algebra of functions with the following property: For every compact $K\subset\mathbb{R}$, $\mathcal{A}$ is dense in $\mathcal{C}(K)$ with respect to the uniform norm $\|\cdot\|_{\infty}$, where $\mathcal{C}(K)$ is the collection of real continuous functions on $K$. The uniform norm I'm referring to is $$\|f\|_{\infty}=\sup_{t\in K}|f(t)|.$$ (See http://en.wikipedia.org/wiki/Uniform_norm .) Can we conclude that $\mathcal{A}$ is dense in $L^p(\mathbb{R})$ (with respect to the $L^p$ norm)? I became interested in this question while investigating a special case, the $L^2$-density of finite linear combinations of Gaussians: $$\sum_{i=1}^n\alpha_ie^{-k_i(x-x_i)^2},\qquad\alpha_i,k_i,x_i\in\mathbb{R},k_i>0.$$ The question above occurred to me because I can imagine it being useful in cases like this to, say, verify the hypotheses of the Stone-Weierstrass theorem for a given family of functions rather than to explicitly approximate functions in $L^p$.",,['real-analysis']
29,Does there exist $( x_n)_{n\in\mathbb{N}}\subset \mathbb{R}$ such that $\sum_{k=1}^{\infty} x_{kn} =1$ for every $n\in\mathbb{N}?$,Does there exist  such that  for every,( x_n)_{n\in\mathbb{N}}\subset \mathbb{R} \sum_{k=1}^{\infty} x_{kn} =1 n\in\mathbb{N}?,"Does there exist $( x_n)_{n\in\mathbb{N}}\subset \mathbb{R}$ such that $\displaystyle\sum_{k=1}^{\infty} x_{kn} =1$ for every $n\in\mathbb{N}?$ I suspect so, but I am struggling to actually construct such a sequence. All I can say so far is that there must be infinitely many members of $( x_n)_{n\in\mathbb{N}}$ that are positive and infinitely many members of $( x_n)_{n\in\mathbb{N}}$ that are negative. Other than this, I have not made much progress. Can someone sketch a construction or provide any hints of a disproof please?","Does there exist such that for every I suspect so, but I am struggling to actually construct such a sequence. All I can say so far is that there must be infinitely many members of that are positive and infinitely many members of that are negative. Other than this, I have not made much progress. Can someone sketch a construction or provide any hints of a disproof please?",( x_n)_{n\in\mathbb{N}}\subset \mathbb{R} \displaystyle\sum_{k=1}^{\infty} x_{kn} =1 n\in\mathbb{N}? ( x_n)_{n\in\mathbb{N}} ( x_n)_{n\in\mathbb{N}},"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
30,Can we define the fractional derivative by mapping a function to a sinusoidal?,Can we define the fractional derivative by mapping a function to a sinusoidal?,,"For integer $n$ we have that: $$\frac{d^n}{dx^n} \sin(x) = \sin\left(x+\frac{n \pi}{2}\right)$$ For any function $f(x)$ (ignoring domain restrictions for the time being), let: $$f(x) = \sin(u) \ \ [1]$$ $$u = \arcsin\left(f(x)\right)  \ \ [2]$$ Then, take the $n$ th derivative wrt $u$ of both sides of $[1]$ : $$\frac{d^n}{du^n} f(x) = \frac{d^n}{du^n} \sin(u)$$ $$\frac{d^n}{du^n} f(x) = \sin\left(u+\frac{n \pi}{2}\right)$$ Now we can solve for $du$ and substitute: $$f'(x) \ dx = \cos(u) \ du$$ $$du = \frac{f'(x)}{\cos(u)} \ dx  = \frac{f'(x)}{\cos\left(\arcsin(f(x)\right)} \ dx = \frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx $$ Then: $$\frac{d^n}{du^n} f(x) = \frac{d^n}{\left(\frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx \right)^n} \ f(x) = \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{-n} \frac{d^n}{dx^n} f(x)$$ Finally we can solve for the $n$ th derivative in terms of $x$ and $n$ alone, using $[2]$ : $$\frac{d^n}{dx^n} f(x) = \sin\left(\arcsin\left(f(x)\right)+\frac{n \pi}{2}\right) \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{n}$$ And now there is no issue with letting $n$ vary continuously. Now from this formula we have that $|f(x)| < 1$ , so it obviously doesn't work everywhere for all functions. I made a Desmos graph , try inputting different choices of $f$ and see how it interpolates between the function and its first derivative. My questions are the following: Why is there sometimes not a smooth transition between the function and its derivative? Try letting $f = x^{x}$ and see what happens when you let $n$ go from $0$ to $1$ , the left half flips back and forth. Is this due to the exponentiation not being defined? How do we extend the domain? We can just make the transformation $f(x) \to f(x - c)$ to center the derivative around a point other than 0, but it's not clear how to make this accept functions with a magnitude greater than $1$ . Is this always exact for integer n so long as $|f(x)| < 1$ ? It seems like there might be a bit of error, but can't tell if that is numerical or due to the domain of the functions involved. Has anything like this been done before? I'd like to see if there is any existing literature that uses this sort of idea.","For integer we have that: For any function (ignoring domain restrictions for the time being), let: Then, take the th derivative wrt of both sides of : Now we can solve for and substitute: Then: Finally we can solve for the th derivative in terms of and alone, using : And now there is no issue with letting vary continuously. Now from this formula we have that , so it obviously doesn't work everywhere for all functions. I made a Desmos graph , try inputting different choices of and see how it interpolates between the function and its first derivative. My questions are the following: Why is there sometimes not a smooth transition between the function and its derivative? Try letting and see what happens when you let go from to , the left half flips back and forth. Is this due to the exponentiation not being defined? How do we extend the domain? We can just make the transformation to center the derivative around a point other than 0, but it's not clear how to make this accept functions with a magnitude greater than . Is this always exact for integer n so long as ? It seems like there might be a bit of error, but can't tell if that is numerical or due to the domain of the functions involved. Has anything like this been done before? I'd like to see if there is any existing literature that uses this sort of idea.",n \frac{d^n}{dx^n} \sin(x) = \sin\left(x+\frac{n \pi}{2}\right) f(x) f(x) = \sin(u) \ \ [1] u = \arcsin\left(f(x)\right)  \ \ [2] n u [1] \frac{d^n}{du^n} f(x) = \frac{d^n}{du^n} \sin(u) \frac{d^n}{du^n} f(x) = \sin\left(u+\frac{n \pi}{2}\right) du f'(x) \ dx = \cos(u) \ du du = \frac{f'(x)}{\cos(u)} \ dx  = \frac{f'(x)}{\cos\left(\arcsin(f(x)\right)} \ dx = \frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx  \frac{d^n}{du^n} f(x) = \frac{d^n}{\left(\frac{f'(x)}{\sqrt{1-f(x)^2}} \ dx \right)^n} \ f(x) = \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{-n} \frac{d^n}{dx^n} f(x) n x n [2] \frac{d^n}{dx^n} f(x) = \sin\left(\arcsin\left(f(x)\right)+\frac{n \pi}{2}\right) \left(\frac{f'(x)}{\sqrt{1-f(x)^2}}\right)^{n} n |f(x)| < 1 f f = x^{x} n 0 1 f(x) \to f(x - c) 1 |f(x)| < 1,"['real-analysis', 'complex-analysis', 'fractional-calculus']"
31,Show that Riemann sum equals integral,Show that Riemann sum equals integral,,"Suppose we have some continuous function $\textbf{F}\colon \mathbb{R}^{n} \to \mathbb{R}^n$ (i.e. a force field) and a curve $\gamma\colon [a, b]\subseteq \mathbb{R} \to \mathbb{R}^{n}$ in $C^1$ . Considering the partition $P$ of the interval $[a, b]$ \begin{align*} P= \left\{t_{0}=a, t_{1}=a + \frac{b-a}{n}, a + 2 \frac{b-a}{n} , \ldots,t_{n}= b\right\} \implies \Delta t = \frac{b-a}{n} .\end{align*} I want to show that \begin{align*}     \lim_{n \to\infty} \sum_{k=1}^{n}\left\langle      \textbf{F}\left(\gamma(t_{k}^{*})\right), \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)\right\rangle      \Delta t &= \int_{a}^{b}\left\langle  \textbf{F}\left(\gamma(t)\right),\gamma'(t)\right\rangle \mathrm{d}t  \\  &= \lim_{n \to\infty} \sum_{k=1}^{n}\left\langle   \textbf{F}\left(\gamma(t_{k}^{*})\right),\gamma'(t_{k}^{*}) \right\rangle\Delta t .\end{align*} So let $\epsilon>0$ be arbitrary. We can apply the mean value theorem componentwise, namely for all $1\le i\le m$ we have \begin{align*}     \forall x,y \in [a, b] \colon \gamma_{i}(y)-\gamma_{i}(x)=\gamma_{i}'(\xi_i) (y-x), \quad \xi_i \in [x, y] .\end{align*} By uniform continuity of $\gamma'$ there exists some $\delta$ s.t. for all $x$ and for all $y \in (x-\delta, x+\delta)$ \begin{align*}    \left\|\gamma'(y)-\gamma'(x) \right\|_{2}   <\epsilon .\end{align*} Now, we choose $N>0$ s.t. \begin{align*} \frac{b-a}{N}<  \delta .\end{align*} Then, for all $n\ge N$ we find \begin{align*} &\left|   \sum_{k=1}^{n}\left(\left\langle  \textbf{F}\left(\gamma(t_{k}^{*})\right),         \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)\right\rangle \Delta t -\left\langle  \textbf{F}\left(\gamma(t_{k}^{*})\right),\gamma'(t_{k}^{*}) \right\rangle\Delta t \right) \right |  \\ =    &\left|   \sum_{k=1}^{n}\Delta t\left\langle \textbf{F} \left(\gamma(t_{k}^{*})\right), \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)     -\gamma'(t_{k-1})+\gamma'(t_{k-1}) -\gamma'(t_{k}^{*})\right\rangle  \right |  \\ \le  &   \Delta t\sum_{k=1}^{n}\left\| \textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2}\left\|  \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)-\gamma'(t_{k-1})+\gamma'(t_{k-1}) -\gamma'(t_{k}^{*})\right\|_{2} \\ \le  &\Delta t\sum_{k=1}^{n} \left\| \textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2}\left(\left\|  \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)-\gamma'(t_{k-1})\right\|_2+\left\|\gamma'(t_{k-1}) -\gamma'(t_{k}^{*})\right\|_{2}\right) \\  < &\Delta t \sum_{k=1}^{n} \left\|\textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2} \cdot \epsilon(\sqrt{m} +1)  .\end{align*} The last inequality follows from the fact that \begin{align*}     \frac{\gamma_{i}(t_{k})-\gamma_{i}(t_{k-1})}{t_{k}-t_{k-1}} = \gamma_{i}'(\xi_{i}), \quad \xi_{i} \in (t_{k-1}, t_{k}) \end{align*} and since $\xi_i-t_{k-1}<\delta$ we have \begin{align*}     |\gamma_{i}'(\xi_{i})-\gamma_{i}'(t_{k-1})| < \epsilon \end{align*} for all $1\le i\le m$ . Since $\left\|\textbf{F}\left(\gamma(t)\right)\right\|_{2} $ is a continuous function on a compact set it takes a maximal value, let this be $M$ . It follows that \begin{align*}     \Delta t \sum_{k=1}^{n} \left\|\textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2} \cdot \epsilon(\sqrt{m} +1)     \le (b-a)^{2} m\cdot M\cdot \epsilon(\sqrt{m} +1) .\end{align*} Choosing $\epsilon$ in the beginning accordingly concludes the proof.","Suppose we have some continuous function (i.e. a force field) and a curve in . Considering the partition of the interval I want to show that So let be arbitrary. We can apply the mean value theorem componentwise, namely for all we have By uniform continuity of there exists some s.t. for all and for all Now, we choose s.t. Then, for all we find The last inequality follows from the fact that and since we have for all . Since is a continuous function on a compact set it takes a maximal value, let this be . It follows that Choosing in the beginning accordingly concludes the proof.","\textbf{F}\colon \mathbb{R}^{n} \to \mathbb{R}^n \gamma\colon [a, b]\subseteq \mathbb{R} \to \mathbb{R}^{n} C^1 P [a, b] \begin{align*}
P= \left\{t_{0}=a, t_{1}=a + \frac{b-a}{n}, a + 2 \frac{b-a}{n} , \ldots,t_{n}= b\right\} \implies \Delta t = \frac{b-a}{n}
.\end{align*} \begin{align*}
    \lim_{n \to\infty} \sum_{k=1}^{n}\left\langle 
    \textbf{F}\left(\gamma(t_{k}^{*})\right), \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)\right\rangle 
    \Delta t
&= \int_{a}^{b}\left\langle  \textbf{F}\left(\gamma(t)\right),\gamma'(t)\right\rangle \mathrm{d}t 
\\ 
&=
\lim_{n \to\infty} \sum_{k=1}^{n}\left\langle   \textbf{F}\left(\gamma(t_{k}^{*})\right),\gamma'(t_{k}^{*}) \right\rangle\Delta t
.\end{align*} \epsilon>0 1\le i\le m \begin{align*}
    \forall x,y \in [a, b] \colon \gamma_{i}(y)-\gamma_{i}(x)=\gamma_{i}'(\xi_i) (y-x), \quad \xi_i \in [x, y]
.\end{align*} \gamma' \delta x y \in (x-\delta, x+\delta) \begin{align*}
   \left\|\gamma'(y)-\gamma'(x) \right\|_{2}   <\epsilon
.\end{align*} N>0 \begin{align*}
\frac{b-a}{N}<  \delta
.\end{align*} n\ge N \begin{align*}
&\left|  
\sum_{k=1}^{n}\left(\left\langle  \textbf{F}\left(\gamma(t_{k}^{*})\right),
        \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)\right\rangle \Delta t
-\left\langle  \textbf{F}\left(\gamma(t_{k}^{*})\right),\gamma'(t_{k}^{*}) \right\rangle\Delta t
\right)
\right | 
\\
=   
&\left|  
\sum_{k=1}^{n}\Delta t\left\langle \textbf{F} \left(\gamma(t_{k}^{*})\right), \left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)
    -\gamma'(t_{k-1})+\gamma'(t_{k-1})
-\gamma'(t_{k}^{*})\right\rangle 
\right | 
\\
\le 
&  
\Delta t\sum_{k=1}^{n}\left\| \textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2}\left\| 
\left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)-\gamma'(t_{k-1})+\gamma'(t_{k-1})
-\gamma'(t_{k}^{*})\right\|_{2}
\\
\le 
&\Delta t\sum_{k=1}^{n} \left\| \textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2}\left(\left\| 
\left(\frac{\gamma(t_{k})-\gamma(t_{k-1})}{t_{k}-t_{k-1}}\right)-\gamma'(t_{k-1})\right\|_2+\left\|\gamma'(t_{k-1})
-\gamma'(t_{k}^{*})\right\|_{2}\right)
\\ 
<
&\Delta t \sum_{k=1}^{n} \left\|\textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2} \cdot \epsilon(\sqrt{m} +1) 
.\end{align*} \begin{align*}
    \frac{\gamma_{i}(t_{k})-\gamma_{i}(t_{k-1})}{t_{k}-t_{k-1}} = \gamma_{i}'(\xi_{i}), \quad \xi_{i} \in (t_{k-1}, t_{k})
\end{align*} \xi_i-t_{k-1}<\delta \begin{align*}
    |\gamma_{i}'(\xi_{i})-\gamma_{i}'(t_{k-1})| < \epsilon
\end{align*} 1\le i\le m \left\|\textbf{F}\left(\gamma(t)\right)\right\|_{2}  M \begin{align*}
    \Delta t \sum_{k=1}^{n} \left\|\textbf{F}\left(\gamma(t_{k}^{*})\right)\right\|_{2} \cdot \epsilon(\sqrt{m} +1)
    \le (b-a)^{2} m\cdot M\cdot \epsilon(\sqrt{m} +1)
.\end{align*} \epsilon","['real-analysis', 'calculus', 'integration', 'multivariable-calculus', 'solution-verification']"
32,"Is there a ""global"" convexity locally around a minimum?","Is there a ""global"" convexity locally around a minimum?",,"$$\newcommand{\til}{\tilde}$$ Let $F:(0,\infty) \to [0,\infty)$ be a continuous function satisfying $F(1)=0$ , which is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Suppose also that $F|_{[1-\epsilon,1+\epsilon]}$ is strictly convex, for some $\epsilon>0$ . Question: Does there exist a $\delta>0$ such that $F$ is convex at every point $y \in (1-\delta,1)$ ? By convexity at $y\,\,$ I mean that for any $x_1,x_2>0, \alpha \in [0,1]$ satisfying $\alpha x_1 + (1- \alpha)x_2 =y$ , $$ F(y)=F\big(\alpha x_1 + (1- \alpha)x_2 \big) \leq \alpha F(x_1) + (1-\alpha)F(x_2). \tag{1} $$ Equivalently , $F$ admits a supporting line at $y$ , i.e. $\exists m \in \mathbb{R}$ such that $$ F(x) \ge F(y)+m (x-y) \, \, \, \text{ for every } \, \, x \in (0,\infty). \tag{2} $$ Edit: I have a proof for a positive answer if $F \in C^1$ , and an incomplete argument for the case where we do not assume $F \in C^1$ . I am interested to know the answer when $F$ is merely continuous. (Feel free to skip over my proofs/arguments below. The question stands as it is.) A (hopefully correct) proof assuming $F \in C^1$ : If there is no such $\delta$ , there exists $y_n \in (0,1)$ , $y_n \to 1$ such that $F$ is not convex at $y_n$ . Thus, $\exists x_n \in (0,\infty)$ such that $$ F(x_n) < F(y_n)+F'(y_n) (x_n-y_n) \tag{3} $$ The tangent at $y_n$ is below the $x$ -axis for $x>1$ (since $F$ is convex at $(y_n,1]$ it decreases more slowly than its tangent after the tangency point). Thus, $F>0$ is above its tangent which is negative for $x>1$ , which implies $x_n \in (0,1)$ . We may assume that $x_n \to x_0$ ; taking limits on both sides of $(3)$ , we obtain $F(x_0) \le F(1)=0$ , since $|F'(y_n) (x_n-y_n)| \le |F'(y_n)| \to 0$ . This implies $x_0=1$ , so both $x_n,y_n \to 1$ . Looking again at inequality $(3)$ , this contradicts the convexity of $F|_{[1-\epsilon,1+\epsilon]}$ . We have used $F'(y_n) \to F'(1)=0$ . If we do not assume $F \in C^1$ , we need to replace $F'(y_n)$ with slopes of supporting lines $m_n$ , which do not necessarily converge to zero . An incomplete proof without assuming $F \in C^1$ : Assume there is no such $\delta$ . Then $\exists s_n \in [0,1]$ , $s_n \to 1$ such that $F$ is not convex at $s_n$ . Thus $\exists x_n,y_n \in (0,\infty), \alpha_n \in [0,1]$ , $x_n \le s_n \le y_n$ such that $$ s_n=\alpha_n x_n + (1- \alpha_n)y_n, \, \, \text{ and } \, \,  F\left(s_n \right) > \alpha_n F(x_n) + (1-\alpha_n)F(y_n). \tag{4} $$ $x_n \le s_n \le 1$ so $x_n$ is bounded. W.L.O.G we may assume that $y_n \le 1$ . Indeed, if $y_n >1$ , we can replace it with $\til y_n=1$ , and choose $\til x_n$ such that $\alpha_n \til x_n + (1- \alpha_n)\til y_n=s_n$ . Then $x_n \le \til x_n \le s_n \le 1=\til y_n \le y_n$ , hence $F(\til x_n) \le F(x_n), F(\til y_n) \le F(y_n)$ . Then inequality $(4)$ holds with $x_n,y_n$ replaced by $\til x_n,\til y_n$ . We now have $x_n \le s_n \le y_n \le 1$ , and $s_n \to 1$ so we may assume that $x_n \to x, y_n \to 1, \alpha_n \to\alpha$ . Taking limits of inequality $(4)$ we get $$ 0=F(1) \ge \alpha F(x) + (1-\alpha)F(1) = \alpha F(x)\ge 0, $$ so $\alpha F(x)=0$ . If $\alpha>0$ , then $F(x)=0$ and $x=1$ , so $x_n,y_n,s_n \to 1$ , thus they eventually lie at $(1-\epsilon,1]$ , which together with inequality $(4)$ contradicts the convexity of $F|_{[1-\epsilon,1+\epsilon]}$ . The problem is that if $\alpha=0$ we cannot deduce that $x=1$ !","Let be a continuous function satisfying , which is strictly increasing on , and strictly decreasing on . Suppose also that is strictly convex, for some . Question: Does there exist a such that is convex at every point ? By convexity at I mean that for any satisfying , Equivalently , admits a supporting line at , i.e. such that Edit: I have a proof for a positive answer if , and an incomplete argument for the case where we do not assume . I am interested to know the answer when is merely continuous. (Feel free to skip over my proofs/arguments below. The question stands as it is.) A (hopefully correct) proof assuming : If there is no such , there exists , such that is not convex at . Thus, such that The tangent at is below the -axis for (since is convex at it decreases more slowly than its tangent after the tangency point). Thus, is above its tangent which is negative for , which implies . We may assume that ; taking limits on both sides of , we obtain , since . This implies , so both . Looking again at inequality , this contradicts the convexity of . We have used . If we do not assume , we need to replace with slopes of supporting lines , which do not necessarily converge to zero . An incomplete proof without assuming : Assume there is no such . Then , such that is not convex at . Thus , such that so is bounded. W.L.O.G we may assume that . Indeed, if , we can replace it with , and choose such that . Then , hence . Then inequality holds with replaced by . We now have , and so we may assume that . Taking limits of inequality we get so . If , then and , so , thus they eventually lie at , which together with inequality contradicts the convexity of . The problem is that if we cannot deduce that !","\newcommand{\til}{\tilde} F:(0,\infty) \to [0,\infty) F(1)=0 [1,\infty) (0,1] F|_{[1-\epsilon,1+\epsilon]} \epsilon>0 \delta>0 F y \in (1-\delta,1) y\,\, x_1,x_2>0, \alpha \in [0,1] \alpha x_1 + (1- \alpha)x_2 =y 
F(y)=F\big(\alpha x_1 + (1- \alpha)x_2 \big) \leq \alpha F(x_1) + (1-\alpha)F(x_2). \tag{1}
 F y \exists m \in \mathbb{R} 
F(x) \ge F(y)+m (x-y) \, \, \, \text{ for every } \, \, x \in (0,\infty). \tag{2}
 F \in C^1 F \in C^1 F F \in C^1 \delta y_n \in (0,1) y_n \to 1 F y_n \exists x_n \in (0,\infty) 
F(x_n) < F(y_n)+F'(y_n) (x_n-y_n) \tag{3}
 y_n x x>1 F (y_n,1] F>0 x>1 x_n \in (0,1) x_n \to x_0 (3) F(x_0) \le F(1)=0 |F'(y_n) (x_n-y_n)| \le |F'(y_n)| \to 0 x_0=1 x_n,y_n \to 1 (3) F|_{[1-\epsilon,1+\epsilon]} F'(y_n) \to F'(1)=0 F \in C^1 F'(y_n) m_n F \in C^1 \delta \exists s_n \in [0,1] s_n \to 1 F s_n \exists x_n,y_n \in (0,\infty), \alpha_n \in [0,1] x_n \le s_n \le y_n 
s_n=\alpha_n x_n + (1- \alpha_n)y_n, \, \, \text{ and } \, \, 
F\left(s_n \right) > \alpha_n F(x_n) + (1-\alpha_n)F(y_n). \tag{4}
 x_n \le s_n \le 1 x_n y_n \le 1 y_n >1 \til y_n=1 \til x_n \alpha_n \til x_n + (1- \alpha_n)\til y_n=s_n x_n \le \til x_n \le s_n \le 1=\til y_n \le y_n F(\til x_n) \le F(x_n), F(\til y_n) \le F(y_n) (4) x_n,y_n \til x_n,\til y_n x_n \le s_n \le y_n \le 1 s_n \to 1 x_n \to x, y_n \to 1, \alpha_n \to\alpha (4) 
0=F(1) \ge \alpha F(x) + (1-\alpha)F(1) = \alpha F(x)\ge 0,
 \alpha F(x)=0 \alpha>0 F(x)=0 x=1 x_n,y_n,s_n \to 1 (1-\epsilon,1] (4) F|_{[1-\epsilon,1+\epsilon]} \alpha=0 x=1","['real-analysis', 'calculus', 'probability', 'convex-analysis', 'solution-verification']"
33,"proof verification: $f(x) = 1/x$ is not uniformly continuous on the open interval (0,1).","proof verification:  is not uniformly continuous on the open interval (0,1).",f(x) = 1/x,"I've written a proof that $f\left(x\right)=\frac{1}{x}$ is not uniformly continuous on the interval $(0,1)$ and would like to know if it is correct. Here's what I've got. In order to show a function $f$ is not uniformly continuous on $A$ , it   suffices to show there exist two sequences $(x_n)$ and $(y_n)$ in $A$ and an $\epsilon_0>0$ satisfying $\lim(|x_n-y_n|)=0$ but $|f(x_n)-f(y_n)|\ge\epsilon_0$ . Let $x_n=\frac{1}{n}$ and $y_n=\frac{2}{n}$ , with $n\ge3$ , and set $\epsilon_0=\frac{3}{2}$ . Then $\lim(|x_n-y_n|)=0$ , but $\left|\frac{1}{x_n}-\frac{1}{y_n}\right|=\left|n-\frac{n}{2}\right|=\frac{n}{2}\ge\epsilon_0=\frac{3}{2}$ ,   as desired.","I've written a proof that is not uniformly continuous on the interval and would like to know if it is correct. Here's what I've got. In order to show a function is not uniformly continuous on , it   suffices to show there exist two sequences and in and an satisfying but . Let and , with , and set . Then , but ,   as desired.","f\left(x\right)=\frac{1}{x} (0,1) f A (x_n) (y_n) A \epsilon_0>0 \lim(|x_n-y_n|)=0 |f(x_n)-f(y_n)|\ge\epsilon_0 x_n=\frac{1}{n} y_n=\frac{2}{n} n\ge3 \epsilon_0=\frac{3}{2} \lim(|x_n-y_n|)=0 \left|\frac{1}{x_n}-\frac{1}{y_n}\right|=\left|n-\frac{n}{2}\right|=\frac{n}{2}\ge\epsilon_0=\frac{3}{2}","['real-analysis', 'proof-verification', 'continuity']"
34,Defining an unusual subspace of $c_0$,Defining an unusual subspace of,c_0,"This is going to be a long post, so I'm giving a description first: I recently came across the following exercise: Let $(X,\mathcal{A},\mu)$ be a measure space. If $(f_n)\subset L^p(\mu)$ and $f\in L^p(\mu)$ , $p\geq 1$ , such that $\|f_n-f\|_p\leq n^{-c}$ with $c>1/p$ , prove that $f_n\to f$ a.e. on $X$ .  The solution is not a big deal, but leads to an interesting question: which rates are good enough for convergence in norm to imply convergence a.e.? Recall that with no further assumptions, convergence in norm only implies the existence of a subsequence that converges a.e. Anyway, the following definition is necessary: Define $\text{gr}^p(\mu)$ as the set $$gr^p(\mu):=\{(a_n)\in c_0|\text{ for all } (f_n)\subset L^p(\mu): \big{(}\forall n\in\mathbb{N}: \|f_n\|_p\leq |a_n|\big{)}\implies f_n\to0\text{ a.e.}\}$$ What I would like is to describe this set. My progress is the following: 1) For any measure space and any $p\geq 1$ it is $(0)\in\text{gr}^p(\mu)$ , therefore this set is never empty. 2) If $(a_n)\in\text{gr}^p(\mu)$ and $\lambda\in\mathbb{C}$ then $\lambda\cdot(a_n)\in\text{gr}^p(\mu)$ . Indeed, if $\lambda=0$ it is obvious; otherwise if $(f_n)\subset L^p(\mu)$ with $\|f_n\|_p\leq|\lambda a_n|$ for all $n$ we have that $\displaystyle{\|\frac{1}{\lambda}f_n\|_p\leq|a_n|}$ for all $n$ therefore $\frac{1}{\lambda}f_n\to 0$ a.e. which is true iff $f_n\to 0$ a.e. 3) For any measure space, $\ell^p\subset\text{gr}^p(\mu)$ . Let $(a_n)\in\ell^p$ and $(f_n)\subset L^p(\mu)$ with $\|f_n\|_p\leq|a_n|$ for all $n$ . We have $\displaystyle{\int_X|f_n|^pd\mu\leq|a_n|^p}$ for all $n$ and by summing and using the Monotone convergence theorem we have that $\displaystyle{\int_X\sum_{n}|f_n|^pd\mu\leq\|(a_n)\|_{\ell^p}<\infty}$ , therefore the series $\sum_n|f_n|^p$ converges a.e. hence $|f_n|^p\to0$ a.e. which implies $f_n\to 0$ a.e. 4) In any measure space, if $(a_n)\in\text{gr}^p(\mu)$ and $(a_{n_k})\subset(a_n)$ , we have $(a_{n_k})\in\text{gr}^p(\mu)$ . Let $(a_{n_k})\subset(a_n)\in\text{gr}^p(\mu)$ and $(f_k)\subset L^p(\mu)$ s.t. for all $k$ it is $\|f_k\|_p\leq |a_{n_k}|$ ; Define $g_n$ as $0$ if $n\not\in\{n_k: k\in\mathbb{N}\}$ and $g_{n_k}=f_k$ for all $k$ . Then $\|g_n\|_p\leq |a_n|$ for all $n$ , hence $g_n\to 0$ a.e. which of course implies $f_k\to0$ a.e. 5) $\text{gr}^p(\mu)$ is a linear subspace of $c_0$ . We need only to prove that it is closed under addition. Let $(a_n),(b_n)\in\text{gr}^p(\mu)$ and $(f_n)\subset L^p(\mu)$ with $\|f_n\|_p\leq|a_n+b_n|$ for all $n$ . We partition $\mathbb{N}$ in $S=\{n: a_n=0\}$ and its complement $\mathbb{N}-S$ . Case 1: $S$ is an infinite set. By 4), $(b_n)_{n\in S}\in\text{gr}^p(\mu)$ and $\|f_n\|_p\leq|b_n|$ for all $n$ in $S$ ; therefore the subsequence $(f_n)_{n\in S}$ converges a.e. to $0$ . Now for $n\not\in S$ we can find $\lambda_n\in\mathbb{C}$ such that $b_n=\lambda_n\cdot a_n$ . We have to deal with two sub-cases: Sub-case 1: There exists $M>0$ s.t. for all $n\in\mathbb{N}-S$ it is $|\lambda_n|\leq M$ . In this sub-case, for $n\not\in S$ we have $\|f_n\|_p\leq |a_n|+|b_n|\leq (1+M)|b_n|$ . But $(b_n)_{n\in\mathbb{N}-S}\in\text{gr}^p(\mu)$ by 4), and by 2) we have $((1+M)b_n)_{n\in\mathbb{N}-S}\in\text{gr}^p(\mu)$ . Hence $(f_n)_{n\in\mathbb{N}-S}$ converges to $0$ a.e. Sub-case 2: $|\lambda_n|\to\infty$ as $n\to\infty$ through $\mathbb{N}-S$ (note that if $\mathbb{N}-S$ is finite we are automatically in sub-case 1). We can find $n_0\in\mathbb{N}$ such that for all $n\geq n_0$ and $n\not\in S$ it is $|\lambda_n|>1$ . For those $n$ it is $a_n=\frac{1}{\lambda_n}b_n$ therefore $\|f_n\|_p\leq|1+1/\lambda_n|\cdot|b_n|\leq2|b_n|$ ; now since $(b_n)_{n\geq n_0, n\in\mathbb{N}-S}\in\text{gr}^p(\mu)$ it is $(f_n)_{n\geq n_0, n\in\mathbb{N}-S}\to0$ a.e. and we are done. Case 2: S is finite; we can do exactly what we did in the two sub-cases above for $\mathbb{N}-S$ and we are done. Anyway , my questions to the community are these: Are these spaces any interesting in your opinion? What would be a good norm for these spaces? I can't think of anything that is of interest. In this post I prove that for a series of Dirac point-mass measures the space $\text{gr}^p(\mu)$ is the entire $c_0$ for all $p$ and that for the measure space $(\mathbb{R}^d, \mathcal{L}^d, \lambda_d)$ the space $\text{gr}^p(\mu)$ is only $\ell^p$ . EDIT : An easy argument that shows that $\text{gr}^p(\mu)$ is not closed under the supremum norm, unless $\text{gr}^p(\mu)=c_0$ : obviously $c_{00}\subset\text{gr}^p(\mu)$ , where $c_{00}$ denotes the subspace of sequences that are eventually $0$ . Thus $\text{gr}^p(\mu)$ is dense in $c_0$ with the supremum norm.","This is going to be a long post, so I'm giving a description first: I recently came across the following exercise: Let be a measure space. If and , , such that with , prove that a.e. on .  The solution is not a big deal, but leads to an interesting question: which rates are good enough for convergence in norm to imply convergence a.e.? Recall that with no further assumptions, convergence in norm only implies the existence of a subsequence that converges a.e. Anyway, the following definition is necessary: Define as the set What I would like is to describe this set. My progress is the following: 1) For any measure space and any it is , therefore this set is never empty. 2) If and then . Indeed, if it is obvious; otherwise if with for all we have that for all therefore a.e. which is true iff a.e. 3) For any measure space, . Let and with for all . We have for all and by summing and using the Monotone convergence theorem we have that , therefore the series converges a.e. hence a.e. which implies a.e. 4) In any measure space, if and , we have . Let and s.t. for all it is ; Define as if and for all . Then for all , hence a.e. which of course implies a.e. 5) is a linear subspace of . We need only to prove that it is closed under addition. Let and with for all . We partition in and its complement . Case 1: is an infinite set. By 4), and for all in ; therefore the subsequence converges a.e. to . Now for we can find such that . We have to deal with two sub-cases: Sub-case 1: There exists s.t. for all it is . In this sub-case, for we have . But by 4), and by 2) we have . Hence converges to a.e. Sub-case 2: as through (note that if is finite we are automatically in sub-case 1). We can find such that for all and it is . For those it is therefore ; now since it is a.e. and we are done. Case 2: S is finite; we can do exactly what we did in the two sub-cases above for and we are done. Anyway , my questions to the community are these: Are these spaces any interesting in your opinion? What would be a good norm for these spaces? I can't think of anything that is of interest. In this post I prove that for a series of Dirac point-mass measures the space is the entire for all and that for the measure space the space is only . EDIT : An easy argument that shows that is not closed under the supremum norm, unless : obviously , where denotes the subspace of sequences that are eventually . Thus is dense in with the supremum norm.","(X,\mathcal{A},\mu) (f_n)\subset L^p(\mu) f\in L^p(\mu) p\geq 1 \|f_n-f\|_p\leq n^{-c} c>1/p f_n\to f X \text{gr}^p(\mu) gr^p(\mu):=\{(a_n)\in c_0|\text{ for all } (f_n)\subset L^p(\mu): \big{(}\forall n\in\mathbb{N}: \|f_n\|_p\leq |a_n|\big{)}\implies f_n\to0\text{ a.e.}\} p\geq 1 (0)\in\text{gr}^p(\mu) (a_n)\in\text{gr}^p(\mu) \lambda\in\mathbb{C} \lambda\cdot(a_n)\in\text{gr}^p(\mu) \lambda=0 (f_n)\subset L^p(\mu) \|f_n\|_p\leq|\lambda a_n| n \displaystyle{\|\frac{1}{\lambda}f_n\|_p\leq|a_n|} n \frac{1}{\lambda}f_n\to 0 f_n\to 0 \ell^p\subset\text{gr}^p(\mu) (a_n)\in\ell^p (f_n)\subset L^p(\mu) \|f_n\|_p\leq|a_n| n \displaystyle{\int_X|f_n|^pd\mu\leq|a_n|^p} n \displaystyle{\int_X\sum_{n}|f_n|^pd\mu\leq\|(a_n)\|_{\ell^p}<\infty} \sum_n|f_n|^p |f_n|^p\to0 f_n\to 0 (a_n)\in\text{gr}^p(\mu) (a_{n_k})\subset(a_n) (a_{n_k})\in\text{gr}^p(\mu) (a_{n_k})\subset(a_n)\in\text{gr}^p(\mu) (f_k)\subset L^p(\mu) k \|f_k\|_p\leq |a_{n_k}| g_n 0 n\not\in\{n_k: k\in\mathbb{N}\} g_{n_k}=f_k k \|g_n\|_p\leq |a_n| n g_n\to 0 f_k\to0 \text{gr}^p(\mu) c_0 (a_n),(b_n)\in\text{gr}^p(\mu) (f_n)\subset L^p(\mu) \|f_n\|_p\leq|a_n+b_n| n \mathbb{N} S=\{n: a_n=0\} \mathbb{N}-S S (b_n)_{n\in S}\in\text{gr}^p(\mu) \|f_n\|_p\leq|b_n| n S (f_n)_{n\in S} 0 n\not\in S \lambda_n\in\mathbb{C} b_n=\lambda_n\cdot a_n M>0 n\in\mathbb{N}-S |\lambda_n|\leq M n\not\in S \|f_n\|_p\leq |a_n|+|b_n|\leq (1+M)|b_n| (b_n)_{n\in\mathbb{N}-S}\in\text{gr}^p(\mu) ((1+M)b_n)_{n\in\mathbb{N}-S}\in\text{gr}^p(\mu) (f_n)_{n\in\mathbb{N}-S} 0 |\lambda_n|\to\infty n\to\infty \mathbb{N}-S \mathbb{N}-S n_0\in\mathbb{N} n\geq n_0 n\not\in S |\lambda_n|>1 n a_n=\frac{1}{\lambda_n}b_n \|f_n\|_p\leq|1+1/\lambda_n|\cdot|b_n|\leq2|b_n| (b_n)_{n\geq n_0, n\in\mathbb{N}-S}\in\text{gr}^p(\mu) (f_n)_{n\geq n_0, n\in\mathbb{N}-S}\to0 \mathbb{N}-S \text{gr}^p(\mu) c_0 p (\mathbb{R}^d, \mathcal{L}^d, \lambda_d) \text{gr}^p(\mu) \ell^p \text{gr}^p(\mu) \text{gr}^p(\mu)=c_0 c_{00}\subset\text{gr}^p(\mu) c_{00} 0 \text{gr}^p(\mu) c_0","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
35,"If $f'+f''\geq f^2$ show that $\frac{f'}{f''}\leq 1$ for all $x\in(0,+\infty)$.",If  show that  for all .,"f'+f''\geq f^2 \frac{f'}{f''}\leq 1 x\in(0,+\infty)","I have a question about this : Let a function $f$ with domain $]0,+\infty[$ and codomain $]0,+\infty[$ and twice differentiable with the following inequality :   $$f'+f''\geq f^2$$ Show that we have $\dfrac{f'}{f''}\leq 1$ for all $x\in$  $]0,+\infty[$ I have no idea to prove or disprove this . Thanks.","I have a question about this : Let a function $f$ with domain $]0,+\infty[$ and codomain $]0,+\infty[$ and twice differentiable with the following inequality :   $$f'+f''\geq f^2$$ Show that we have $\dfrac{f'}{f''}\leq 1$ for all $x\in$  $]0,+\infty[$ I have no idea to prove or disprove this . Thanks.",,"['calculus', 'real-analysis']"
36,Proving an integral is finite,Proving an integral is finite,,"I have the following integral: $$\displaystyle \int_{\mathbb{R}^2} \left( \int_{\mathbb{R}^2} \frac{J_{1}(|\alpha|)J_{1}(|k- \alpha|)}{|\alpha||k-\alpha|} \ \mathrm{d}\alpha \right)^2 \ \mathrm{d}k,$$ where both $\alpha$ and $k$ are vectors in $\mathbb{R}^2$, with $k \neq 0$, and $J_{\nu}$ denotes the Bessel function of the first kind. I'm having some trouble with the best way to approach this integral. If we focus on the inner integral first, then using the fact that for sufficiently large, positive $z$ we have $|J_{\nu}(z)| \leqslant C|z|^{-1/2},$ then the inner integral can be reduced to $$\displaystyle \int_{\mathbb{R}^2} |\alpha|^{-3/2}|k-\alpha|^{-3/2} \ \mathrm{d}\alpha.$$ However, as can be seen in this answer , this integral is $O(|k|^{-1})$, which, after squaring, is clearly not integrable over all $|k| \geqslant 1$ after switching to polar co-ordinates (obviously not including $0$ in the lower limit of the outer integral). We would need an estimate of at least $O(|k|^{-1 - \epsilon})$ for any $\epsilon > 0$ to guarantee convergence of the outer integral. One idea might be to try to bring the outer integral inside (though one would need to justify interchanging the order of integration). Using the asymptotics for the Bessel functions gives a product of cosines, and then one can use polar co-ordinates (taking $r = |\alpha|$). This would cancel out the $|\alpha|$ in the denominator, but then the $|k-\alpha|$ terms get very messy, which seems to make things worse. The Bessel functions appear to cause the most trouble. Does anyone have any ideas on how to proceed? One idea is to notice (as someone suggested in the comments) that the above is the $L^2$ norm of a convolution, and also that (up to a constant) we have $$\displaystyle f(\xi) = \frac{J_{d/2}(|\xi|)}{|\xi|^{d/2}} = \mathcal{F}(\chi)(\xi),$$ where $\chi$ is the characteristic function of the unit ball in $\mathbb{R}^d$. It can be shown that $f \in L^2(\mathbb{R}^d)$ and even $L^{p}(\mathbb{R}^d)$ for any $p \geqslant 2$. This lets us write the entire expression as $$\|f \ast f\|_2^2 = \|\mathcal{F}(\chi) \ast \mathcal{F}(\chi)\|_2^2,$$ but unfortunately there is no kind of convolution theorem for functions on $L^2(\mathbb{R}^d)$ that I am aware of, without going into the theory of distributions. Moreover, $f$ does not even belong to $S(\mathbb{R}^d)$ for any $d$, so we cannot say much about the convolution. Thus, the problem is equivalent to asserting the finiteness of the above norm, $\|f \ast f\|_2$. If anyone has any ideas on how else the problem could be approached, then I would be very keen to hear about them.","I have the following integral: $$\displaystyle \int_{\mathbb{R}^2} \left( \int_{\mathbb{R}^2} \frac{J_{1}(|\alpha|)J_{1}(|k- \alpha|)}{|\alpha||k-\alpha|} \ \mathrm{d}\alpha \right)^2 \ \mathrm{d}k,$$ where both $\alpha$ and $k$ are vectors in $\mathbb{R}^2$, with $k \neq 0$, and $J_{\nu}$ denotes the Bessel function of the first kind. I'm having some trouble with the best way to approach this integral. If we focus on the inner integral first, then using the fact that for sufficiently large, positive $z$ we have $|J_{\nu}(z)| \leqslant C|z|^{-1/2},$ then the inner integral can be reduced to $$\displaystyle \int_{\mathbb{R}^2} |\alpha|^{-3/2}|k-\alpha|^{-3/2} \ \mathrm{d}\alpha.$$ However, as can be seen in this answer , this integral is $O(|k|^{-1})$, which, after squaring, is clearly not integrable over all $|k| \geqslant 1$ after switching to polar co-ordinates (obviously not including $0$ in the lower limit of the outer integral). We would need an estimate of at least $O(|k|^{-1 - \epsilon})$ for any $\epsilon > 0$ to guarantee convergence of the outer integral. One idea might be to try to bring the outer integral inside (though one would need to justify interchanging the order of integration). Using the asymptotics for the Bessel functions gives a product of cosines, and then one can use polar co-ordinates (taking $r = |\alpha|$). This would cancel out the $|\alpha|$ in the denominator, but then the $|k-\alpha|$ terms get very messy, which seems to make things worse. The Bessel functions appear to cause the most trouble. Does anyone have any ideas on how to proceed? One idea is to notice (as someone suggested in the comments) that the above is the $L^2$ norm of a convolution, and also that (up to a constant) we have $$\displaystyle f(\xi) = \frac{J_{d/2}(|\xi|)}{|\xi|^{d/2}} = \mathcal{F}(\chi)(\xi),$$ where $\chi$ is the characteristic function of the unit ball in $\mathbb{R}^d$. It can be shown that $f \in L^2(\mathbb{R}^d)$ and even $L^{p}(\mathbb{R}^d)$ for any $p \geqslant 2$. This lets us write the entire expression as $$\|f \ast f\|_2^2 = \|\mathcal{F}(\chi) \ast \mathcal{F}(\chi)\|_2^2,$$ but unfortunately there is no kind of convolution theorem for functions on $L^2(\mathbb{R}^d)$ that I am aware of, without going into the theory of distributions. Moreover, $f$ does not even belong to $S(\mathbb{R}^d)$ for any $d$, so we cannot say much about the convolution. Thus, the problem is equivalent to asserting the finiteness of the above norm, $\|f \ast f\|_2$. If anyone has any ideas on how else the problem could be approached, then I would be very keen to hear about them.",,"['real-analysis', 'integration', 'asymptotics', 'estimation', 'bessel-functions']"
37,Points of differentiability of $f(x) = \sum\limits_{n : q_n < x} c_n$,Points of differentiability of,f(x) = \sum\limits_{n : q_n < x} c_n,"Let, $\{q_n\}_{n \in \mathbb{N}}$ be an enumeration of rational numbers. Consider the function $f : \mathbb{R} \to \mathbb{R}$ given by, $$\displaystyle f(x) = \sum\limits_{n : q_n < x} c_n$$ where, $\displaystyle \sum\limits_{n=1}^{\infty} c_n$ is an absolutely convergent positive series. The function is clearly monotone increasing, discontinuous at rationals (with jump exactly $c_n$ at $x = q_n$) and continuous at irrationals. $1.$ I wish to ask about the points of differentiability of $f$? Since $f$ is monotone it should be differentiable a.e. but how do we identify these points of differentiability? (as in a way of representing this set in a compact way) Intuitively, it seems they should be related to the particular enumeration of the rationals $\{q_n\}$ at hand. For example if we have an enumeration such that for $\alpha \in \mathbb{R \setminus Q}$, we have $q_n \notin (\alpha - \delta_N , \alpha + \delta_N)$ for $1 \le n \le N$ (i.e., say $|q_n - \alpha| > \delta_n$ for $n \in \mathbb{N}$ where, $\delta_n \downarrow 0^{+}$ as $n \to \infty$) and now if we impose further the 'nice' property: $\displaystyle \frac{f(\alpha + \delta_N) - f(\alpha)}{\delta_N} = \frac{1}{\delta_N}\sum\limits_{n : q_n \in (\alpha, \alpha + \delta_N)} c_n \to \lambda$, (as $N \to \infty$) and similarly one for the left derivative, we have $f'(\alpha) = \lambda$. So, intuitively I can see how to choose an enumeration that makes the derivative equal $\lambda$ at $x = \alpha$ (or blows up at $\alpha$, i.e., $\lambda = + \infty$). To clarify what I am asking: Given an enumeration of rationals, how do we come up with relevant definitions/concepts relating to said enumeration, which helps us identify which $\alpha$'s we should expect to be a point of differentiablity. $2.$ Is there a way to estimate the derivative at these points? Has these questions been addressed/answered in literature before? I'd love it if I could get some reference in this matter. Thanks! :-)","Let, $\{q_n\}_{n \in \mathbb{N}}$ be an enumeration of rational numbers. Consider the function $f : \mathbb{R} \to \mathbb{R}$ given by, $$\displaystyle f(x) = \sum\limits_{n : q_n < x} c_n$$ where, $\displaystyle \sum\limits_{n=1}^{\infty} c_n$ is an absolutely convergent positive series. The function is clearly monotone increasing, discontinuous at rationals (with jump exactly $c_n$ at $x = q_n$) and continuous at irrationals. $1.$ I wish to ask about the points of differentiability of $f$? Since $f$ is monotone it should be differentiable a.e. but how do we identify these points of differentiability? (as in a way of representing this set in a compact way) Intuitively, it seems they should be related to the particular enumeration of the rationals $\{q_n\}$ at hand. For example if we have an enumeration such that for $\alpha \in \mathbb{R \setminus Q}$, we have $q_n \notin (\alpha - \delta_N , \alpha + \delta_N)$ for $1 \le n \le N$ (i.e., say $|q_n - \alpha| > \delta_n$ for $n \in \mathbb{N}$ where, $\delta_n \downarrow 0^{+}$ as $n \to \infty$) and now if we impose further the 'nice' property: $\displaystyle \frac{f(\alpha + \delta_N) - f(\alpha)}{\delta_N} = \frac{1}{\delta_N}\sum\limits_{n : q_n \in (\alpha, \alpha + \delta_N)} c_n \to \lambda$, (as $N \to \infty$) and similarly one for the left derivative, we have $f'(\alpha) = \lambda$. So, intuitively I can see how to choose an enumeration that makes the derivative equal $\lambda$ at $x = \alpha$ (or blows up at $\alpha$, i.e., $\lambda = + \infty$). To clarify what I am asking: Given an enumeration of rationals, how do we come up with relevant definitions/concepts relating to said enumeration, which helps us identify which $\alpha$'s we should expect to be a point of differentiablity. $2.$ Is there a way to estimate the derivative at these points? Has these questions been addressed/answered in literature before? I'd love it if I could get some reference in this matter. Thanks! :-)",,"['real-analysis', 'sequences-and-series', 'analysis']"
38,Evaluate the following integral involving $\sin \pi x$,Evaluate the following integral involving,\sin \pi x,"Let $F: \Bbb{R} \to \Bbb{R}$ be defined by $$F(s)=\begin{cases}1, & \text{if }s\ge \dfrac12 \\[0.2cm]0, & \text{if }s< \dfrac12 \end{cases}$$   I need to evaluate $$\int^{1}_{0} F(\sin \pi x) dx\,$$ I noticed that $\sin (\pi x)$ is greater than $\frac{1}{2}$ for $\frac{1}{6}\le x \le \frac{5}{6}$ thus integral reduces to $\int^{\frac{5}{6}}_{\frac{1}{6}} 1 dx=\frac{2}{3}$. Is it okay?","Let $F: \Bbb{R} \to \Bbb{R}$ be defined by $$F(s)=\begin{cases}1, & \text{if }s\ge \dfrac12 \\[0.2cm]0, & \text{if }s< \dfrac12 \end{cases}$$   I need to evaluate $$\int^{1}_{0} F(\sin \pi x) dx\,$$ I noticed that $\sin (\pi x)$ is greater than $\frac{1}{2}$ for $\frac{1}{6}\le x \le \frac{5}{6}$ thus integral reduces to $\int^{\frac{5}{6}}_{\frac{1}{6}} 1 dx=\frac{2}{3}$. Is it okay?",,"['real-analysis', 'integration']"
39,Floor function and convergence of the sequence,Floor function and convergence of the sequence,,"Sequence $\{a(n)\}$ of real numbers is such that $\forall\space\lambda\in(1,2)$ sequence $a(\lfloor{\lambda}^n\rfloor)$ has a finite limit. Does it follow that $\{a(n)\}$ is convergent?","Sequence $\{a(n)\}$ of real numbers is such that $\forall\space\lambda\in(1,2)$ sequence $a(\lfloor{\lambda}^n\rfloor)$ has a finite limit. Does it follow that $\{a(n)\}$ is convergent?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
40,Let $D$ be a bounded domain (open connected) in $ \mathbb C$ and assume that complement of $D$ is connected.Then show that $\partial D$ is connected,Let  be a bounded domain (open connected) in  and assume that complement of  is connected.Then show that  is connected,D  \mathbb C D \partial D,"I am trying to prove the following famous result in Point Set Topology. Let $D$ be a bounded domain (open connected) in $ \mathbb C$ and assume that complement of $D$ is connected. Then show that  $\partial D$ is connected. I know there are proofs of this result using Fundamental Group and Algebraic Topology but I don't know much Algebraic Topology so I am trying to prove this result using Elementary Topology only. I am planning to use following basic result of Point Set Topology: Let $(K_i)_{i\in I}$ be an indexed family of decreasing, connected, compact sets in a Topological space X. Then $B = \bigcap_i K_i$ is connected. I defined  $K_n$= {$ z \in D : dist (z, \partial D) \leq \frac{1}{n}$}. Now as distance is a continuous function therefore each $K_n$ is compact and also $K_n$ is a decreasing sequence. Furthermore, $ \bigcap K_i$=$\partial D$. So I think now its enough to show that each $K_i$ is connected. I personally believe that each $K_i$ is path connected, but I am unable to prove this. Feel free to scold me If I am doing something wrong here. Please help with my idea or give some different proof using elementary topology only!","I am trying to prove the following famous result in Point Set Topology. Let $D$ be a bounded domain (open connected) in $ \mathbb C$ and assume that complement of $D$ is connected. Then show that  $\partial D$ is connected. I know there are proofs of this result using Fundamental Group and Algebraic Topology but I don't know much Algebraic Topology so I am trying to prove this result using Elementary Topology only. I am planning to use following basic result of Point Set Topology: Let $(K_i)_{i\in I}$ be an indexed family of decreasing, connected, compact sets in a Topological space X. Then $B = \bigcap_i K_i$ is connected. I defined  $K_n$= {$ z \in D : dist (z, \partial D) \leq \frac{1}{n}$}. Now as distance is a continuous function therefore each $K_n$ is compact and also $K_n$ is a decreasing sequence. Furthermore, $ \bigcap K_i$=$\partial D$. So I think now its enough to show that each $K_i$ is connected. I personally believe that each $K_i$ is path connected, but I am unable to prove this. Feel free to scold me If I am doing something wrong here. Please help with my idea or give some different proof using elementary topology only!",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness', 'connectedness']"
41,"Find $f:C\to\mathbb{R}^2$ continuous and bijective but not open, $C\subset\mathbb{R} ^2$ is closed and connected","Find  continuous and bijective but not open,  is closed and connected",f:C\to\mathbb{R}^2 C\subset\mathbb{R} ^2,"Are there a closed connected subspace  $C$ of $\mathbb{R}^2$ and a continuous, bijective function  $f:C\to\mathbb{R}^2$ that is not open? If we remove the condition for $C$ to be connected, we have the function  $f:\big([0,+\infty)\times\mathbb{R}\big)\cup\{(-1,0)\}\to\mathbb{R}^2$ defined by \begin{equation} f(x,y)= \begin{cases} &\bigg(e^y\cos\big(\frac{2\pi x}{x+1}\big),e^y\sin\big(\frac{2\pi x}{x+1}\big)\bigg)&,&(x,y)\neq (-1,0)\\ &(0,0)&,&(x,y)=(-1,0) \end{cases}   \end{equation} which satisfy the rest of the conditions. See justification here. But what is the answer if $C$ is closed and connected?","Are there a closed connected subspace  $C$ of $\mathbb{R}^2$ and a continuous, bijective function  $f:C\to\mathbb{R}^2$ that is not open? If we remove the condition for $C$ to be connected, we have the function  $f:\big([0,+\infty)\times\mathbb{R}\big)\cup\{(-1,0)\}\to\mathbb{R}^2$ defined by \begin{equation} f(x,y)= \begin{cases} &\bigg(e^y\cos\big(\frac{2\pi x}{x+1}\big),e^y\sin\big(\frac{2\pi x}{x+1}\big)\bigg)&,&(x,y)\neq (-1,0)\\ &(0,0)&,&(x,y)=(-1,0) \end{cases}   \end{equation} which satisfy the rest of the conditions. See justification here. But what is the answer if $C$ is closed and connected?",,"['real-analysis', 'general-topology', 'examples-counterexamples']"
42,Egorov's theorem for this Lebesgue integral,Egorov's theorem for this Lebesgue integral,,"I want to prove Egorov's theorem using this Lebesgue integral defined by the upper integral $$\int^*f:=\left\{\int h ; h \ge f \text{ and h upper-continuous }\right\}$$ $$\int_*f:=\left\{\int h ; h \le f \text{ and h lower-continuous }\right\}$$ So a Lebesgue integral of a function $ f : \mathbb{R}^n \rightarrow \mathbb{R}$ exists $\int f \Leftrightarrow \int^*f = \int_*f$. I am also allowed to use the following theorems: $L^p$ is a Banach space; Dominated convergence theorem; Monotone convergence theorem; $C^{\infty}$ is dense in the Lebesgue-functions. But the huge problem is: We don't know what Borel-sets are and we don't have anything like measures so far. Therefore, all standard proofs of this theorem are not applicable to this situation. Hence, I wanted to find out whether anybody here knows a way how to do it? Maybe I should say more about how this integral is defined: Every semincontinuous function is the limit of a monotone sequence of continuous functions with finite support $g_n$. The integral over these kind of functions is defined via n-times 1 dimensional integration over all variables and then $\int h:= \lim_{n \rightarrow \infty} \int g_n$. (But this is probably not that relevant to this proof). If anything is unclear, please let me know","I want to prove Egorov's theorem using this Lebesgue integral defined by the upper integral $$\int^*f:=\left\{\int h ; h \ge f \text{ and h upper-continuous }\right\}$$ $$\int_*f:=\left\{\int h ; h \le f \text{ and h lower-continuous }\right\}$$ So a Lebesgue integral of a function $ f : \mathbb{R}^n \rightarrow \mathbb{R}$ exists $\int f \Leftrightarrow \int^*f = \int_*f$. I am also allowed to use the following theorems: $L^p$ is a Banach space; Dominated convergence theorem; Monotone convergence theorem; $C^{\infty}$ is dense in the Lebesgue-functions. But the huge problem is: We don't know what Borel-sets are and we don't have anything like measures so far. Therefore, all standard proofs of this theorem are not applicable to this situation. Hence, I wanted to find out whether anybody here knows a way how to do it? Maybe I should say more about how this integral is defined: Every semincontinuous function is the limit of a monotone sequence of continuous functions with finite support $g_n$. The integral over these kind of functions is defined via n-times 1 dimensional integration over all variables and then $\int h:= \lim_{n \rightarrow \infty} \int g_n$. (But this is probably not that relevant to this proof). If anything is unclear, please let me know",,"['calculus', 'real-analysis']"
43,How prove this $\lim_{n\to \infty}\sin{n^m}$ divergent.,How prove this  divergent.,\lim_{n\to \infty}\sin{n^m},"show that $$\lim_{n\to \infty}\sin{(n^m)}$$  divergent,where $m\in N^{+}$ I have kown $$\lim_{n\to\infty}\sin{n^2}$$  to be divergent and dense in $[-1,1]$. This is very  famous problem,the problem is first is post $AMM$( The American Mathematical Monthly,1970-1975) problem,and after some years  $AMM$ post $\sin{(n^{14})}$ in $[-1,1]$ was dense? and this is open problem.so I Guess  $$\sin{(n^m)}$$ is dense in $[-1,1]$.","show that $$\lim_{n\to \infty}\sin{(n^m)}$$  divergent,where $m\in N^{+}$ I have kown $$\lim_{n\to\infty}\sin{n^2}$$  to be divergent and dense in $[-1,1]$. This is very  famous problem,the problem is first is post $AMM$( The American Mathematical Monthly,1970-1975) problem,and after some years  $AMM$ post $\sin{(n^{14})}$ in $[-1,1]$ was dense? and this is open problem.so I Guess  $$\sin{(n^m)}$$ is dense in $[-1,1]$.",,"['calculus', 'real-analysis', 'limits']"
44,"Consider the sequence defined: $a_1=0, a_{n+1}=3+\sqrt{11+a_n}$, show that is bounded above and increasing using induction.","Consider the sequence defined: , show that is bounded above and increasing using induction.","a_1=0, a_{n+1}=3+\sqrt{11+a_n}","Consider the sequence defined: $a_1=0, a_{n+1}=3+\sqrt{11+a_n}$ a) Show, using induction, that this sequence is bounded above by 14; b) prove that the sequence is increasing; c) Why must it converge?; d)Find the limit. So for part a), I have: Let $P(n)$ be the statement that $a_n \leq 14$. Consider $P(1)$, where $n=1$.  Then $0 \leq 14$, which is indeed true. Now assume that $P(k)$ is true for some $k\in \mathbb{N}$, i.e. $a_k \leq 14$  Then for $P(k+1)$: $a_{k+1}=3+\sqrt{11+a_k}=3+\sqrt{11+14}=8 \leq 14$.  Thus the statement holds for $P(k+1)$ is true.  Therefore the statement holds for all $n \in \mathbb{N}$. For part b), I also use induction. Let $S(n)$ be the statement: $a_{n+1} \geq a_n, \forall n \in \mathbb{N}$.  Then for $n=1$, we have that $S(1)$ is $a_2=3+\sqrt{11} \geq a_1=0$, which is indeed true.  Assume $S(k)$ is true for some $k \in \mathbb{N}$, i.e. $a_{k+1} \geq a_k$.  Then for $S(k+1)$, $a_{k+2}=3+\sqrt{11+a_{k+1}} \geq 3+\sqrt{11+a_k}=a_{k+1}$.  Thus $S(k+1)$ is true.  Thus $S(n)$ is true for all $n \in \mathbb{N}$ c) The sequence must converge since this sequence is bounded and monotonically increasing. d) For this part do I say that $L=3+\sqrt{11+L}$, and then solve for $L$? I just want to see if I am doing this correctly.  I think that part b is incorrect because of the induction step. Any help and feedback is appreciated. Thanks in advance.","Consider the sequence defined: $a_1=0, a_{n+1}=3+\sqrt{11+a_n}$ a) Show, using induction, that this sequence is bounded above by 14; b) prove that the sequence is increasing; c) Why must it converge?; d)Find the limit. So for part a), I have: Let $P(n)$ be the statement that $a_n \leq 14$. Consider $P(1)$, where $n=1$.  Then $0 \leq 14$, which is indeed true. Now assume that $P(k)$ is true for some $k\in \mathbb{N}$, i.e. $a_k \leq 14$  Then for $P(k+1)$: $a_{k+1}=3+\sqrt{11+a_k}=3+\sqrt{11+14}=8 \leq 14$.  Thus the statement holds for $P(k+1)$ is true.  Therefore the statement holds for all $n \in \mathbb{N}$. For part b), I also use induction. Let $S(n)$ be the statement: $a_{n+1} \geq a_n, \forall n \in \mathbb{N}$.  Then for $n=1$, we have that $S(1)$ is $a_2=3+\sqrt{11} \geq a_1=0$, which is indeed true.  Assume $S(k)$ is true for some $k \in \mathbb{N}$, i.e. $a_{k+1} \geq a_k$.  Then for $S(k+1)$, $a_{k+2}=3+\sqrt{11+a_{k+1}} \geq 3+\sqrt{11+a_k}=a_{k+1}$.  Thus $S(k+1)$ is true.  Thus $S(n)$ is true for all $n \in \mathbb{N}$ c) The sequence must converge since this sequence is bounded and monotonically increasing. d) For this part do I say that $L=3+\sqrt{11+L}$, and then solve for $L$? I just want to see if I am doing this correctly.  I think that part b is incorrect because of the induction step. Any help and feedback is appreciated. Thanks in advance.",,"['real-analysis', 'sequences-and-series']"
45,The uniqueness of the Gamma Function,The uniqueness of the Gamma Function,,"It is a theorem that any function $f$ defined for positive real numbers satisfying $f(1)=1$ $f(x+1)=x\cdot f(x)$ $f$ is log convex is identically equal to the gamma function. (Condition 2 means that this function interpolates a shifted factorial function.) Now, a beginner (such as myself) might ask: What if we weaken condition 2 by instead requiring $f$ to be merely convex, not log convex? I would imagine that such functions would look not too different, since intuitively, I can't wildly deviate the graph of the gamma function if I want to maintain condition 2 and stay convex. Just a follow-up musing---What if instead of condition 3, we require convexity and infinite differentiability? Do we still uniquely determine the gamma function?","It is a theorem that any function $f$ defined for positive real numbers satisfying $f(1)=1$ $f(x+1)=x\cdot f(x)$ $f$ is log convex is identically equal to the gamma function. (Condition 2 means that this function interpolates a shifted factorial function.) Now, a beginner (such as myself) might ask: What if we weaken condition 2 by instead requiring $f$ to be merely convex, not log convex? I would imagine that such functions would look not too different, since intuitively, I can't wildly deviate the graph of the gamma function if I want to maintain condition 2 and stay convex. Just a follow-up musing---What if instead of condition 3, we require convexity and infinite differentiability? Do we still uniquely determine the gamma function?",,"['real-analysis', 'special-functions', 'gamma-function']"
46,What is the probability that the absolute value of the roots of a polynomial of degree $n$ is greater than $x$?,What is the probability that the absolute value of the roots of a polynomial of degree  is greater than ?,n x,"Note : Posted in MO since it is unanswered in MSE. Let $f(x) = 0$ be an equation of degree $n$ . WLOG we can assume that the its coefficients are in $(-1,1)$ . This is because we can divide each coefficient by the coefficient with the largest magnitude to make each one of them fall in the interval $(-1,1)$ . Assume that the coefficients are uniformly random in $(-1,1)$ . It is well known that most of absolute values of the roots have value close to one i.e. the roots tend to form a unit circle around the origin as shown in the figure below for a polynomial of degree $n = 666$ . It can be observed from the graph that while most of the roots are close to the unit circle, some of them are outside it. I want to find the probability $P(n,x)$ that a roots of a polynomial of degree $n$ lies at a distance $x > 1$ from the origin. To do this, I run a simulation generating polynomial of degree $n$ and its $n$ roots in each trial and counted the total number of roots across all the trials whose absolute value is $\ge x$ . Question : Experimental data shows that $P(n,1) = \frac{1}{2}$ and for $x > 1$ , $\displaystyle n P(n,x) \to  \frac{1}{2x}$ as $n \to \infty$ . Can this be proved or disproved? Related : Is the root of a polynomial with the largest modulus more likely to be real?","Note : Posted in MO since it is unanswered in MSE. Let be an equation of degree . WLOG we can assume that the its coefficients are in . This is because we can divide each coefficient by the coefficient with the largest magnitude to make each one of them fall in the interval . Assume that the coefficients are uniformly random in . It is well known that most of absolute values of the roots have value close to one i.e. the roots tend to form a unit circle around the origin as shown in the figure below for a polynomial of degree . It can be observed from the graph that while most of the roots are close to the unit circle, some of them are outside it. I want to find the probability that a roots of a polynomial of degree lies at a distance from the origin. To do this, I run a simulation generating polynomial of degree and its roots in each trial and counted the total number of roots across all the trials whose absolute value is . Question : Experimental data shows that and for , as . Can this be proved or disproved? Related : Is the root of a polynomial with the largest modulus more likely to be real?","f(x) = 0 n (-1,1) (-1,1) (-1,1) n = 666 P(n,x) n x > 1 n n \ge x P(n,1) = \frac{1}{2} x > 1 \displaystyle n P(n,x) \to  \frac{1}{2x} n \to \infty","['real-analysis', 'probability', 'algebra-precalculus', 'polynomials', 'roots']"
47,"Approximate the sum of a non $C^1(0,1)$ function by its integral",Approximate the sum of a non  function by its integral,"C^1(0,1)","Consider the function $f: [0,1] \to \mathbb{C}$ defined by $$ f(x)=\sum_{n=1}^{9} e^{2\pi n i x}, $$ so that $$ |f(x)|=\bigg|\frac{\sin(9\pi x)}{\sin(\pi x)}\bigg|. $$ I'm interested in approximating $\frac{1}{q}\sum_{0 \lt a \lt q} |f(\frac{a}{q})|$ by $\int_{0}^{1} |f(t)|dt$ . The first idea that comes to mind is using the Euler Mac Laurin formula. The problem is that the function $|f(x)|$ is not continuosly differentiable. Another approach I tried was the following: By partial integration, for any $\alpha\in\mathbb{R}$ and $\delta\gt 0$ , $$ \int_{\alpha}^{\alpha+\delta}(x-\alpha)f'(x)dx=\delta f(\alpha+\delta)-\int_{\alpha}^{\alpha+\delta} f(x)dx. $$ Therefore, by taking $\delta=1/q$ and $\alpha=a/q$ , we have $$ \frac{1}{q}f\Big(\frac{a+1}{q}\Big)=\int_{\frac{a}{q}}^{\frac{a}{q}+\frac{1}{q}}f(x)dx+\int_{\frac{a}{q}}^{\frac{a}{q}+\frac{1}{q}}\Big(x-\frac{a}{q}\Big)f'(x)dx. $$ Hence, by taking absolute value and summing with respect to $a$ from $0$ to $q-1$ , we have $$ \frac{1}{q}\sum_{0 \lt a \le q} |f\Big(\frac{a}{q}\Big)|\le \int_{0}^{1}|f(x)|dx + \frac{1}{q}\int_{0}^{1} |f'(x)| dx $$ The problem is that I would like a much more precise approximation because $\int_{0}^{1} |f'(x)|dx$ is quite big compared to $\int_{0}^{1}|f(x)|dx$ , and I know that as $q$ gets bigger this is less of a problem, but I would also like to know how big does $q$ has to be in order for this sum to be well approximated by the integral. Is it possible to obtain a similar lower bound in terms of the integral plus an error term? And is there any way to bound the error term in terms of $q$ but that it doens't involve the integral of the absolute value of the derivative?","Consider the function defined by so that I'm interested in approximating by . The first idea that comes to mind is using the Euler Mac Laurin formula. The problem is that the function is not continuosly differentiable. Another approach I tried was the following: By partial integration, for any and , Therefore, by taking and , we have Hence, by taking absolute value and summing with respect to from to , we have The problem is that I would like a much more precise approximation because is quite big compared to , and I know that as gets bigger this is less of a problem, but I would also like to know how big does has to be in order for this sum to be well approximated by the integral. Is it possible to obtain a similar lower bound in terms of the integral plus an error term? And is there any way to bound the error term in terms of but that it doens't involve the integral of the absolute value of the derivative?","f: [0,1] \to \mathbb{C} 
f(x)=\sum_{n=1}^{9} e^{2\pi n i x},
 
|f(x)|=\bigg|\frac{\sin(9\pi x)}{\sin(\pi x)}\bigg|.
 \frac{1}{q}\sum_{0 \lt a \lt q} |f(\frac{a}{q})| \int_{0}^{1} |f(t)|dt |f(x)| \alpha\in\mathbb{R} \delta\gt 0 
\int_{\alpha}^{\alpha+\delta}(x-\alpha)f'(x)dx=\delta f(\alpha+\delta)-\int_{\alpha}^{\alpha+\delta} f(x)dx.
 \delta=1/q \alpha=a/q 
\frac{1}{q}f\Big(\frac{a+1}{q}\Big)=\int_{\frac{a}{q}}^{\frac{a}{q}+\frac{1}{q}}f(x)dx+\int_{\frac{a}{q}}^{\frac{a}{q}+\frac{1}{q}}\Big(x-\frac{a}{q}\Big)f'(x)dx.
 a 0 q-1 
\frac{1}{q}\sum_{0 \lt a \le q} |f\Big(\frac{a}{q}\Big)|\le \int_{0}^{1}|f(x)|dx + \frac{1}{q}\int_{0}^{1} |f'(x)| dx
 \int_{0}^{1} |f'(x)|dx \int_{0}^{1}|f(x)|dx q q q","['real-analysis', 'calculus', 'analysis', 'analytic-number-theory']"
48,"""Taylor series"" is to ""Volterra series"" as ""Padé approximant"" is to _________?","""Taylor series"" is to ""Volterra series"" as ""Padé approximant"" is to _________?",,"Padé approximants are often better than Taylor series at representing a function. Given a Taylor series, one can use Wynn's epsilon algorithm to easily produce the Padé approximants to it. Volterra series are a generalization of Taylor series that can also model ""memory"" phenomena. Does there exist a similar generalization of Padé approximants that can model these phenomena, or an algorithm like Wynn's to compute them from the Volterra series?","Padé approximants are often better than Taylor series at representing a function. Given a Taylor series, one can use Wynn's epsilon algorithm to easily produce the Padé approximants to it. Volterra series are a generalization of Taylor series that can also model ""memory"" phenomena. Does there exist a similar generalization of Padé approximants that can model these phenomena, or an algorithm like Wynn's to compute them from the Volterra series?",,"['real-analysis', 'sequences-and-series', 'terminology', 'rational-functions', 'approximation-theory']"
49,Convergence of a Stochastic Process - Am I missing something obvious?,Convergence of a Stochastic Process - Am I missing something obvious?,,"In the paper On the Convergence of Stochastic Iterative Dynamic Programming Algorithms (Jaakkola et al. 1994) the authors claim that a statement is ""easy to show"". Now I am starting to suspect that it isn't easy and they might have just wanted to avoid having to show it. But I thought I would post this to see if I missed something obvious. What I have so far: Since $X_n$ is bounded in a compact interval it certainly has convergent subsequences $|X_{n+1}-X_n|\le (\alpha_n+\gamma \beta_n)C_1$ which implies that it converges to zero, but that isn't enough for it to be a Cauchy sequence even with the first statement $\lim\inf X_n\ge 0$ $$ \begin{align} X_{n+1}(x)&=(1-\alpha_n(x))X_n(x) + \gamma\beta_n(x)\|X_n\| \\ &\ge (1-\alpha_n(x))X_n(x)\\ &=\prod^n_{k=0}(1-\alpha_k(x))X_0 \to 0 \end{align}$$ since $\sum \alpha_n=\infty$ (c.f. Infinite Product ). Because of $\alpha_n(x) \to 0$ we know that $(1-\alpha_n(x))\ge 0$ for almost all $n$ , and if $X_n(x)\ge 0$ then $$ X_{n+1}(x) = \underbrace{(1-\alpha_n(x))}_{\ge 0} \underbrace{X_n(x)}_{\ge 0} +\underbrace{\gamma\beta_n(x)\|X_n\|}_{\ge 0} $$ thus if one element of the sequence is positive all following elements will be positive too. The sequences which stay negative converge to zero ( $\lim\inf X_n\ge 0$ ). The other sequences will be positive for almost all n. For $\|X_n\|$ not to converge $\|X_n\|=\max_x X_n(x)$ for an infinite amount of n. If it was equal to the maximum of the negative sequences for almost all n it would converge. $$\|X_n\|=\max_x -X_n(x) \le \max_x - \prod_{k=0}^n (1-\alpha_k) X_0 \to 0 $$ If we set $\beta_n=0$ we have $$X_m=\prod_{k=n}^{m-1} (1-\alpha_k)X_n \to 0$$ So my intuition is: since $\beta_n$ is smaller than $\alpha_n$ (on average) replacing $\beta_n$ with $\alpha_n$ should probably be fine, since you introduce a larger difference to zero. So I think going in the direction $$X_{n+1}\sim (1-\alpha_n)X_n +\gamma \alpha_n X_n = (1-(1-\gamma)\alpha_n)X_n$$ Which is fine since $\sum(1-\gamma)\alpha_n =\infty$ for $\gamma\in(0,1)$ But I still need to formalize replacing $\beta_n$ with $\alpha_n$ which only works if I take the expected value. And I don't know if the expected value leaves the infinite sums intact. I also have to justify replacing the norm with just one element. I think I can assume that the norm is the max norm without disrupting later proofs. And since $\lim\inf X_n\ge 0$ , $|X_n|$ is basically equal to $X_n$ . I am also a bit confused since the approach I am currently following would show that it converges to 0 instantly while the proof wants me to show that it converges to some $X^*$ and then continuous with arguments on how to show that it converges to $0$ from there. Which makes me think, that I am not on the ""intended proof path"". So maybe I am missing something obvious which could save me a lot of trouble. Especially since they claim it should be easy.","In the paper On the Convergence of Stochastic Iterative Dynamic Programming Algorithms (Jaakkola et al. 1994) the authors claim that a statement is ""easy to show"". Now I am starting to suspect that it isn't easy and they might have just wanted to avoid having to show it. But I thought I would post this to see if I missed something obvious. What I have so far: Since is bounded in a compact interval it certainly has convergent subsequences which implies that it converges to zero, but that isn't enough for it to be a Cauchy sequence even with the first statement since (c.f. Infinite Product ). Because of we know that for almost all , and if then thus if one element of the sequence is positive all following elements will be positive too. The sequences which stay negative converge to zero ( ). The other sequences will be positive for almost all n. For not to converge for an infinite amount of n. If it was equal to the maximum of the negative sequences for almost all n it would converge. If we set we have So my intuition is: since is smaller than (on average) replacing with should probably be fine, since you introduce a larger difference to zero. So I think going in the direction Which is fine since for But I still need to formalize replacing with which only works if I take the expected value. And I don't know if the expected value leaves the infinite sums intact. I also have to justify replacing the norm with just one element. I think I can assume that the norm is the max norm without disrupting later proofs. And since , is basically equal to . I am also a bit confused since the approach I am currently following would show that it converges to 0 instantly while the proof wants me to show that it converges to some and then continuous with arguments on how to show that it converges to from there. Which makes me think, that I am not on the ""intended proof path"". So maybe I am missing something obvious which could save me a lot of trouble. Especially since they claim it should be easy.","X_n |X_{n+1}-X_n|\le (\alpha_n+\gamma \beta_n)C_1 \lim\inf X_n\ge 0 
\begin{align}
X_{n+1}(x)&=(1-\alpha_n(x))X_n(x) + \gamma\beta_n(x)\|X_n\| \\
&\ge (1-\alpha_n(x))X_n(x)\\
&=\prod^n_{k=0}(1-\alpha_k(x))X_0 \to 0
\end{align} \sum \alpha_n=\infty \alpha_n(x) \to 0 (1-\alpha_n(x))\ge 0 n X_n(x)\ge 0 
X_{n+1}(x) = \underbrace{(1-\alpha_n(x))}_{\ge 0}
\underbrace{X_n(x)}_{\ge 0} +\underbrace{\gamma\beta_n(x)\|X_n\|}_{\ge 0}
 \lim\inf X_n\ge 0 \|X_n\| \|X_n\|=\max_x X_n(x) \|X_n\|=\max_x -X_n(x) \le \max_x - \prod_{k=0}^n (1-\alpha_k) X_0 \to 0  \beta_n=0 X_m=\prod_{k=n}^{m-1} (1-\alpha_k)X_n \to 0 \beta_n \alpha_n \beta_n \alpha_n X_{n+1}\sim (1-\alpha_n)X_n +\gamma \alpha_n X_n = (1-(1-\gamma)\alpha_n)X_n \sum(1-\gamma)\alpha_n =\infty \gamma\in(0,1) \beta_n \alpha_n \lim\inf X_n\ge 0 |X_n| X_n X^* 0","['real-analysis', 'stochastic-processes', 'stochastic-analysis', 'stochastic-approximation']"
50,Rigorous Proof of Slutsky's Theorem,Rigorous Proof of Slutsky's Theorem,,"I was hoping to type up my proof of Slutsky's Theorem and get confirmation on the excruciating details being all correct... Statement of Slutsky's Theorem: $$\text{Let }X_n, \ X,\ Y_n,\ Y,\text{ share the same Probability Space }(\Omega,\mathcal{F},P).$$ $$\text{If }Y_n\xrightarrow{\text{prob}}c, \text{ for any constant } c \text{, and }X_n\xrightarrow{\text{dist}}X\text{ then:}$$ $$1.) \ X_n+Y_n\xrightarrow{\text{dist}}X_n+c$$ $$2.) \ X_nY_n\xrightarrow{\text{dist}}cX. \ \ \ \ \ \ \ \ \ \ $$ Proof of 1.) Let $x$ be a point such that $x-c$ is a point of continuity of $F_x$ and pick $\epsilon$ such that $x-c+\epsilon$ is another point of continuity of $F_x$ . By the Law of Total Probability: $$P(X_n+Y_n\leq x)=P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|\leq\epsilon\})+P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\})$$ However, as $$\{ \ \{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\}\ \}\subseteq\{|Y_n-c|>\epsilon\}$$ $$\Rightarrow P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\})\leq P(|Y_n-c|>\epsilon)$$ $$\Rightarrow P(X_n+Y_n\leq x)\leq P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|\leq\epsilon\})+P(|Y_n-c|>\epsilon)$$ Now note that $$\{|Y_n-c|\leq\epsilon\}\equiv\{c-\epsilon \leq Y_n \leq c+\epsilon\}$$ Thus $$P(X_n+Y_n\leq x) \leq P(X_n+c-\epsilon\leq x)+P(|Y_n-c|>\epsilon)$$ This inequality holds because we then have $$X_n+c-\epsilon\leq X_n+Y_n \leq X_n+c+\epsilon$$ so $$P(X_n+Y_n\leq x)\leq P(X_n+c-\epsilon\leq x)$$ and doing our limits $$\displaystyle\limsup_{n}P(X_n+Y_n\leq x) \leq \displaystyle\limsup_{n}P(\{X_n+c-\epsilon\leq x\})+\displaystyle\limsup_{n}P(|Y_n-c|>\epsilon)$$ The far right term is zero by $Y_n\xrightarrow{prob}c$ which gives us $$\displaystyle\limsup_{n}P(X_n+Y_n\leq x) \leq \displaystyle\limsup_{n}P(X_n+c\leq x+\epsilon)$$ As $\epsilon$ is only on the right side, and can be as small as needed and $x-c$ a point of continuity of $F_x$ , we get $$\displaystyle\limsup_{n}P(X_n+Y_n\leq x) = P(X+c\leq x)$$ The $\displaystyle\liminf_{n}$ follows similarly giving the same type of result $$\displaystyle\liminf_{n}P(X_n+Y_n\leq x) = P(X+c\leq x)$$ Thus by their equivalence $$\displaystyle\lim_{n\rightarrow\infty}P(X_n+Y_n\leq x) = P(X+c\leq x)\ \ \square$$ Proof of 2.) Assuming $\pm\epsilon / \delta$ are points of continuity of $F_x$ , and WLOG let $Y_n\xrightarrow{prob}0$ : $$P(|X_nY_n| > \epsilon)=P(\{|X_nY_n|>\epsilon\}\cap\{|Y_n|>\delta\}) + P(\{|X_nY_n>\epsilon\}\cap\{|Y_n|\leq\delta\})$$ Therefore as $$|Y_n|\leq\delta\Rightarrow\frac{1}{|Y_N|}\geq\frac{1}{\delta}\Rightarrow\frac{|X_nY_n|}{|Y_N|}\geq\frac{\epsilon}{\delta}\Rightarrow|X_n|\geq\frac{\epsilon}{\delta}$$ Also noting $$\{|X_nY_n|>\epsilon\}\cap\{|Y_n|>\delta\}\subseteq\{|Y_n|>\delta\}$$ $$\Rightarrow P(|X_nY_n| > \epsilon) \leq P(|Y_n|>\delta)+P(|X_n|>\frac{\epsilon}{\delta})$$ And so by subadditivity $$\leq P(|Y_n|>\delta) + P(X_n>\frac{\epsilon}{\delta}) + P(X_n\leq\frac{-\epsilon}{\delta}) $$ $$ = P(|Y_n|>\delta) + 1 - P(X_n\leq\frac{\epsilon}{\delta}) + P(X_n\leq\frac{-\epsilon}{\delta})$$ $$=P(|Y_n|>\delta) + 1 - F_n(\frac{\epsilon}{\delta})+F_n(\frac{-\epsilon}{\delta}) $$ Thus $$\limsup_{n}P(|X_nY_n|>\epsilon)\leq\limsup_{n}P(|Y_n|>\delta) + \limsup_{n}1 - \limsup_{n}F_n(\frac{\epsilon}{\delta})+\limsup_{n}F_n(\frac{-\epsilon}{\delta})$$ Using $Y_n\xrightarrow{prob}0$ we simplify to $$\limsup_{n}P(|X_nY_n|>\epsilon)\leq 1 - \limsup_{n}F_n(\frac{\epsilon}{\delta})+\limsup_{n}F_n(\frac{-\epsilon}{\delta})$$ $$=1 - F(\frac{\epsilon}{\delta})+F(\frac{-\epsilon}{\delta})$$ Let $\delta\rightarrow 0$ and so by properties of a CDF $$F(\frac{\epsilon}{\delta})\rightarrow 1 \text{ and }F(\frac{-\epsilon}{\delta})\rightarrow0$$ $$\Rightarrow P(|X_nY_n|>\epsilon)\rightarrow0$$ If $Y_n$ converges to a non zero constant the proof remains by just substitution $Y_n=Z_n+c$ . The $\liminf$ again follows similarly.","I was hoping to type up my proof of Slutsky's Theorem and get confirmation on the excruciating details being all correct... Statement of Slutsky's Theorem: Proof of 1.) Let be a point such that is a point of continuity of and pick such that is another point of continuity of . By the Law of Total Probability: However, as Now note that Thus This inequality holds because we then have so and doing our limits The far right term is zero by which gives us As is only on the right side, and can be as small as needed and a point of continuity of , we get The follows similarly giving the same type of result Thus by their equivalence Proof of 2.) Assuming are points of continuity of , and WLOG let : Therefore as Also noting And so by subadditivity Thus Using we simplify to Let and so by properties of a CDF If converges to a non zero constant the proof remains by just substitution . The again follows similarly.","\text{Let }X_n, \ X,\ Y_n,\ Y,\text{ share the same Probability Space }(\Omega,\mathcal{F},P). \text{If }Y_n\xrightarrow{\text{prob}}c, \text{ for any constant } c \text{, and }X_n\xrightarrow{\text{dist}}X\text{ then:} 1.) \ X_n+Y_n\xrightarrow{\text{dist}}X_n+c 2.) \ X_nY_n\xrightarrow{\text{dist}}cX. \ \ \ \ \ \ \ \ \ \  x x-c F_x \epsilon x-c+\epsilon F_x P(X_n+Y_n\leq x)=P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|\leq\epsilon\})+P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\}) \{ \ \{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\}\ \}\subseteq\{|Y_n-c|>\epsilon\} \Rightarrow P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|>\epsilon\})\leq P(|Y_n-c|>\epsilon) \Rightarrow P(X_n+Y_n\leq x)\leq P(\{X_n+Y_n\leq x\}\cap\{|Y_n-c|\leq\epsilon\})+P(|Y_n-c|>\epsilon) \{|Y_n-c|\leq\epsilon\}\equiv\{c-\epsilon \leq Y_n \leq c+\epsilon\} P(X_n+Y_n\leq x) \leq P(X_n+c-\epsilon\leq x)+P(|Y_n-c|>\epsilon) X_n+c-\epsilon\leq X_n+Y_n \leq X_n+c+\epsilon P(X_n+Y_n\leq x)\leq P(X_n+c-\epsilon\leq x) \displaystyle\limsup_{n}P(X_n+Y_n\leq x) \leq \displaystyle\limsup_{n}P(\{X_n+c-\epsilon\leq x\})+\displaystyle\limsup_{n}P(|Y_n-c|>\epsilon) Y_n\xrightarrow{prob}c \displaystyle\limsup_{n}P(X_n+Y_n\leq x) \leq \displaystyle\limsup_{n}P(X_n+c\leq x+\epsilon) \epsilon x-c F_x \displaystyle\limsup_{n}P(X_n+Y_n\leq x) = P(X+c\leq x) \displaystyle\liminf_{n} \displaystyle\liminf_{n}P(X_n+Y_n\leq x) = P(X+c\leq x) \displaystyle\lim_{n\rightarrow\infty}P(X_n+Y_n\leq x) = P(X+c\leq x)\ \ \square \pm\epsilon / \delta F_x Y_n\xrightarrow{prob}0 P(|X_nY_n| > \epsilon)=P(\{|X_nY_n|>\epsilon\}\cap\{|Y_n|>\delta\}) + P(\{|X_nY_n>\epsilon\}\cap\{|Y_n|\leq\delta\}) |Y_n|\leq\delta\Rightarrow\frac{1}{|Y_N|}\geq\frac{1}{\delta}\Rightarrow\frac{|X_nY_n|}{|Y_N|}\geq\frac{\epsilon}{\delta}\Rightarrow|X_n|\geq\frac{\epsilon}{\delta} \{|X_nY_n|>\epsilon\}\cap\{|Y_n|>\delta\}\subseteq\{|Y_n|>\delta\} \Rightarrow P(|X_nY_n| > \epsilon) \leq P(|Y_n|>\delta)+P(|X_n|>\frac{\epsilon}{\delta}) \leq P(|Y_n|>\delta) + P(X_n>\frac{\epsilon}{\delta}) + P(X_n\leq\frac{-\epsilon}{\delta})   = P(|Y_n|>\delta) + 1 - P(X_n\leq\frac{\epsilon}{\delta}) + P(X_n\leq\frac{-\epsilon}{\delta}) =P(|Y_n|>\delta) + 1 - F_n(\frac{\epsilon}{\delta})+F_n(\frac{-\epsilon}{\delta})  \limsup_{n}P(|X_nY_n|>\epsilon)\leq\limsup_{n}P(|Y_n|>\delta) + \limsup_{n}1 - \limsup_{n}F_n(\frac{\epsilon}{\delta})+\limsup_{n}F_n(\frac{-\epsilon}{\delta}) Y_n\xrightarrow{prob}0 \limsup_{n}P(|X_nY_n|>\epsilon)\leq 1 - \limsup_{n}F_n(\frac{\epsilon}{\delta})+\limsup_{n}F_n(\frac{-\epsilon}{\delta}) =1 - F(\frac{\epsilon}{\delta})+F(\frac{-\epsilon}{\delta}) \delta\rightarrow 0 F(\frac{\epsilon}{\delta})\rightarrow 1 \text{ and }F(\frac{-\epsilon}{\delta})\rightarrow0 \Rightarrow P(|X_nY_n|>\epsilon)\rightarrow0 Y_n Y_n=Z_n+c \liminf","['real-analysis', 'probability', 'probability-theory', 'statistics']"
51,"Is the property ""being a derivative"" preserved under multiplication and composition?","Is the property ""being a derivative"" preserved under multiplication and composition?",,"Since differentiation is linear, we therefore have that if $f, g: I\to \mathbb{R}$ is a derivative (where $I\subset \mathbb{R}$ is an interval), then so does their linear combination. What if we consider their multiplication and composition? Due to the forms of the product rule of differentiation of product function and chain rule of differentiation of composition, I highly doubt their product or composition necessarily is still a derivative, but I cannot construct counterexamples.","Since differentiation is linear, we therefore have that if $f, g: I\to \mathbb{R}$ is a derivative (where $I\subset \mathbb{R}$ is an interval), then so does their linear combination. What if we consider their multiplication and composition? Due to the forms of the product rule of differentiation of product function and chain rule of differentiation of composition, I highly doubt their product or composition necessarily is still a derivative, but I cannot construct counterexamples.",,"['real-analysis', 'examples-counterexamples']"
52,A very useful lemma for Henstock-Stieltjes integration,A very useful lemma for Henstock-Stieltjes integration,,"I'd like to see a proof (or hints and outlines) for the following lemma, which is very useful to prove some interesting properties, including an Integration by Parts theorem for Henstock-Stieltjes integrals: Let $f$, $g$ and $\varphi$ be (normally real) functions defined on   $[a,b]$ and $f$ is $\varphi$-Henstock-Stieltjes integrable with   $F(x)=\int_a^x f d\varphi$. Then $fg$ is $\varphi$-Henstock-Stieltjes   integrable if and only if $g$ is  $F$-Henstock-Stieltjes integrable   and so we have $$\int_a^b fg d \varphi =\int_a^b g d F.$$ I think I'm able to show a simple version of this lemma where $\varphi(t)=t$ following the proof of 9.17 (image) given in The Integrals of Lebesgue, Denjoy, Perron and Henstock by Gordon A Russel, which is even more simple: he also takes $g(t)=1$. I'd like to know if it's possible to adapt this proof for what I want, or if I have to start from scratch. This definition may be required:","I'd like to see a proof (or hints and outlines) for the following lemma, which is very useful to prove some interesting properties, including an Integration by Parts theorem for Henstock-Stieltjes integrals: Let $f$, $g$ and $\varphi$ be (normally real) functions defined on   $[a,b]$ and $f$ is $\varphi$-Henstock-Stieltjes integrable with   $F(x)=\int_a^x f d\varphi$. Then $fg$ is $\varphi$-Henstock-Stieltjes   integrable if and only if $g$ is  $F$-Henstock-Stieltjes integrable   and so we have $$\int_a^b fg d \varphi =\int_a^b g d F.$$ I think I'm able to show a simple version of this lemma where $\varphi(t)=t$ following the proof of 9.17 (image) given in The Integrals of Lebesgue, Denjoy, Perron and Henstock by Gordon A Russel, which is even more simple: he also takes $g(t)=1$. I'd like to know if it's possible to adapt this proof for what I want, or if I have to start from scratch. This definition may be required:",,"['real-analysis', 'integration', 'measure-theory', 'stieltjes-integral', 'gauge-integral']"
53,Variation of the Kempner series – convergence of series $\sum\frac{1}{n}$ where $9$ is not a digit of $1/n$.,Variation of the Kempner series – convergence of series  where  is not a digit of .,\sum\frac{1}{n} 9 1/n,"It is easy to argue that the Kempner series converges:  $$  \sum\limits_{\substack{n \text{ s.t. 9 is}\\\text{ not a digit} \\\text{ of } n}}  \frac{1}{n} < \infty$$ Let $E \subset \Bbb N_{>0}$ the subset of the positive integers  such that $9$ is not a digit of the decimal expansion of $1/n$ (the decimal expansion is not allowed to have a trailing infinite sequence of ""$9$""s. For instance $0.24999...$ is not allowed). Here are the first numbers that don't belong to $E$ : $11,13,17,19,21,23,29,31,34,38,41,…$ (not known by the OEIS, by the way). My question is: Does the series    $$ \sum\limits_{n \in E} \frac{1}{n} \tag 1$$ converge? My attempt is : Let $1/n = 0,a_1 a_2 \dots a_k \overline{b_1 b_2 \dots b_m}$ with $n \in E$. Since $1/n$ has no digit ""9"", we have at most $9^{k+m}$ possibilities for the $a_i$'s and $b_j$'s. Moreover, $1/n ≥ 0,00...0\overline{00...01}≥1/10^{k+m}$. But then I can only bound my series $(1)$ from below, by some real number. So, this is not a clue for the divergence of the series. Apparently, the numbers of the form $n=10k+1$ don't belong to $E$. Maybe we can find sufficiently many numbers that have $9$ in the decimal representation of their reciprocals, so that $(1)$ could converge... Any comment will be appreciated !","It is easy to argue that the Kempner series converges:  $$  \sum\limits_{\substack{n \text{ s.t. 9 is}\\\text{ not a digit} \\\text{ of } n}}  \frac{1}{n} < \infty$$ Let $E \subset \Bbb N_{>0}$ the subset of the positive integers  such that $9$ is not a digit of the decimal expansion of $1/n$ (the decimal expansion is not allowed to have a trailing infinite sequence of ""$9$""s. For instance $0.24999...$ is not allowed). Here are the first numbers that don't belong to $E$ : $11,13,17,19,21,23,29,31,34,38,41,…$ (not known by the OEIS, by the way). My question is: Does the series    $$ \sum\limits_{n \in E} \frac{1}{n} \tag 1$$ converge? My attempt is : Let $1/n = 0,a_1 a_2 \dots a_k \overline{b_1 b_2 \dots b_m}$ with $n \in E$. Since $1/n$ has no digit ""9"", we have at most $9^{k+m}$ possibilities for the $a_i$'s and $b_j$'s. Moreover, $1/n ≥ 0,00...0\overline{00...01}≥1/10^{k+m}$. But then I can only bound my series $(1)$ from below, by some real number. So, this is not a clue for the divergence of the series. Apparently, the numbers of the form $n=10k+1$ don't belong to $E$. Maybe we can find sufficiently many numbers that have $9$ in the decimal representation of their reciprocals, so that $(1)$ could converge... Any comment will be appreciated !",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'decimal-expansion']"
54,Easier ways to prove $\int_0^1 \frac{\log^2 x-2}{x^x}dx<0$,Easier ways to prove,\int_0^1 \frac{\log^2 x-2}{x^x}dx<0,"Prove that $$\int_0^1 \frac{\log^2 x-2}{x^x}dx<0$$ One way to do this is use the idea in the proof of Sophomore's dream. We have $$x^{-x}=\exp(-x\log x)=\sum_{n=0}^\infty\frac{(-1)^nx^n\log^n x}{n!}$$ Therefore, using the change of variable $x=\exp(-t/(n+1))$ we have $$\begin{aligned}\int_0^1 \frac{\log^2 x}{x^x}dx=&\sum_{n=0}^\infty\frac{(-1)^n}{n!}\int_0^1x^n\log^{n+2}xdx\\ =&\sum_{n=0}^\infty\frac{(n+1)^{-(n+3)}}{n!}\int_0^\infty t^{n+2}e^{-t}dt\\ =&\sum_{n=0}^\infty\frac{(n+1)^{-(n+3)}(n+2)!}{n!}\\ =&\sum_{n=0}^\infty(n+1)^{-(n+2)}(n+2)\\ =&\sum_{n=1}^\infty n^{-(n+1)}(n+1)\\ =&\sum_{n=1}^\infty n^{-n}+n^{-(n+1)}\\ <&2\sum_{n=1}^\infty n^{-n}\\ =&\int_0^1\frac{2}{x^x}dx \end{aligned}$$ Hence the result follows. I am curious if there are any other methods to prove this, especially I am interested in easier approaches. P.S. This was a bonus problem in an assignment from a multivariable calculus class.","Prove that $$\int_0^1 \frac{\log^2 x-2}{x^x}dx<0$$ One way to do this is use the idea in the proof of Sophomore's dream. We have $$x^{-x}=\exp(-x\log x)=\sum_{n=0}^\infty\frac{(-1)^nx^n\log^n x}{n!}$$ Therefore, using the change of variable $x=\exp(-t/(n+1))$ we have $$\begin{aligned}\int_0^1 \frac{\log^2 x}{x^x}dx=&\sum_{n=0}^\infty\frac{(-1)^n}{n!}\int_0^1x^n\log^{n+2}xdx\\ =&\sum_{n=0}^\infty\frac{(n+1)^{-(n+3)}}{n!}\int_0^\infty t^{n+2}e^{-t}dt\\ =&\sum_{n=0}^\infty\frac{(n+1)^{-(n+3)}(n+2)!}{n!}\\ =&\sum_{n=0}^\infty(n+1)^{-(n+2)}(n+2)\\ =&\sum_{n=1}^\infty n^{-(n+1)}(n+1)\\ =&\sum_{n=1}^\infty n^{-n}+n^{-(n+1)}\\ <&2\sum_{n=1}^\infty n^{-n}\\ =&\int_0^1\frac{2}{x^x}dx \end{aligned}$$ Hence the result follows. I am curious if there are any other methods to prove this, especially I am interested in easier approaches. P.S. This was a bonus problem in an assignment from a multivariable calculus class.",,"['real-analysis', 'sequences-and-series', 'improper-integrals', 'integral-inequality']"
55,norm of a singular integral operator,norm of a singular integral operator,,"My question is from Harmonic Analysis, about the study of singular kernels (in the Calderon Zygmund sense.) Suppose that a kernel $K$ is a singular kernel, extending to a bounded operator on $L^p$, for any $1\leq p<\infty$. On $L^2$ it is easy to compute the norm using Plancherel. But I am not sure how to compute the norm in general of this induced operator as a bounded operator from $L^1$ to $L^1_w$. The most common 'norm' I have seen (for instance in Stein) is that of $||\hat{K}||_{L^\infty} + C$, where the $\hat{K}$ denotes the distributional Fourier transform, and $C$ is the constant such that $$|K(x)|\leq\frac{C}{|x|^n}$$  I know that $K$ commutes with translations, but am wondering about general transformations. A norm I am interested in is the constant $A$ such that $$\int\limits_{\mathbb{R}^n\setminus B_{2\delta}(y)} |K(x-y)-K(x-\bar{y}) |dx\leq A$$ whenever $\bar{y}\in B_\delta(y)$, some $\delta>1$. How does one prove that, if $K$ satisfies the above decay and regularity estimates, and if $T$ is a general transformation matrix on $\mathbb{R}^n$, then $$\int\limits_{\mathbb{R}^n\setminus B_{2\delta}(y)} |K(T(x-y))-K(T(x-\bar{y})) ||\det(T)|dx\leq p(1+||T||||T^{-1}||)A?$$  where $p(x)$ is some $L^1_{loc}$ function?  Any help would be appreciated.","My question is from Harmonic Analysis, about the study of singular kernels (in the Calderon Zygmund sense.) Suppose that a kernel $K$ is a singular kernel, extending to a bounded operator on $L^p$, for any $1\leq p<\infty$. On $L^2$ it is easy to compute the norm using Plancherel. But I am not sure how to compute the norm in general of this induced operator as a bounded operator from $L^1$ to $L^1_w$. The most common 'norm' I have seen (for instance in Stein) is that of $||\hat{K}||_{L^\infty} + C$, where the $\hat{K}$ denotes the distributional Fourier transform, and $C$ is the constant such that $$|K(x)|\leq\frac{C}{|x|^n}$$  I know that $K$ commutes with translations, but am wondering about general transformations. A norm I am interested in is the constant $A$ such that $$\int\limits_{\mathbb{R}^n\setminus B_{2\delta}(y)} |K(x-y)-K(x-\bar{y}) |dx\leq A$$ whenever $\bar{y}\in B_\delta(y)$, some $\delta>1$. How does one prove that, if $K$ satisfies the above decay and regularity estimates, and if $T$ is a general transformation matrix on $\mathbb{R}^n$, then $$\int\limits_{\mathbb{R}^n\setminus B_{2\delta}(y)} |K(T(x-y))-K(T(x-\bar{y})) ||\det(T)|dx\leq p(1+||T||||T^{-1}||)A?$$  where $p(x)$ is some $L^1_{loc}$ function?  Any help would be appreciated.",,"['real-analysis', 'analysis', 'functional-analysis', 'harmonic-analysis', 'singular-integrals']"
56,Topology of convergence in measure,Topology of convergence in measure,,"Currently I am doing some measure theory (on $X=[0,1]$ with the Borel-Sigma algebra and the Lebesgue measure), and I am looking at sets $A \subset L^p$, such that for all $q \in (0,p)$, the topologies induced by the $L^q$ and $L^p$ norm are equivalent.  Okay, then my book said: Hey, if you know this for your set $A$, then you also know that the topologies induced by $L^p$ and $L^0$ agree on $A$ (and this is my main question: why?), where the latter topology is supposed to be induced by the convergence in measure.  I am aware of the concept of convergence in measure from Stochastics, but I do not see a relationship to this problem. Of course, I read the Wikipedia article and as far as I can see, this topology only defined for measure space that have a finite measure, right? Apparently, it is induced by a metric and so from a functional analytic point of view it would be interesting to know if this space is complete. Nevertheless, I do not directly see the relationship between the definition of convergence in measure and what Wikipedia tries to define as the respective topology, is there anybody here who could elaborate on this? Also, I am puzzled in what sense we can think of this definition in terms of a formal limit of the $L^p$-norms. Is there a relationship that tells us, why this is the natural topology on $L^0$? If anything is unclear, please let me know.","Currently I am doing some measure theory (on $X=[0,1]$ with the Borel-Sigma algebra and the Lebesgue measure), and I am looking at sets $A \subset L^p$, such that for all $q \in (0,p)$, the topologies induced by the $L^q$ and $L^p$ norm are equivalent.  Okay, then my book said: Hey, if you know this for your set $A$, then you also know that the topologies induced by $L^p$ and $L^0$ agree on $A$ (and this is my main question: why?), where the latter topology is supposed to be induced by the convergence in measure.  I am aware of the concept of convergence in measure from Stochastics, but I do not see a relationship to this problem. Of course, I read the Wikipedia article and as far as I can see, this topology only defined for measure space that have a finite measure, right? Apparently, it is induced by a metric and so from a functional analytic point of view it would be interesting to know if this space is complete. Nevertheless, I do not directly see the relationship between the definition of convergence in measure and what Wikipedia tries to define as the respective topology, is there anybody here who could elaborate on this? Also, I am puzzled in what sense we can think of this definition in terms of a formal limit of the $L^p$-norms. Is there a relationship that tells us, why this is the natural topology on $L^0$? If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
57,Function whose inverse is also its derivative?,Function whose inverse is also its derivative?,,"What are some good examples of a function  $f : \mathbb{R} \to \mathbb{R}$ where its derivative is equal to its inverse? I attempted to find a monomial that satisfied it by starting with $f(x) = ax^b$ and showing that $f^{-1}(x) = f'(x) \implies b-1=\frac{1}{b} \implies b=\phi$ and got $$f(x) = \frac{x^\phi}{\sqrt[\phi]{\phi}}$$ Which seems to work according to WolframAlpha, but I'm having trouble double-checking it. Any other ideas?","What are some good examples of a function  $f : \mathbb{R} \to \mathbb{R}$ where its derivative is equal to its inverse? I attempted to find a monomial that satisfied it by starting with $f(x) = ax^b$ and showing that $f^{-1}(x) = f'(x) \implies b-1=\frac{1}{b} \implies b=\phi$ and got $$f(x) = \frac{x^\phi}{\sqrt[\phi]{\phi}}$$ Which seems to work according to WolframAlpha, but I'm having trouble double-checking it. Any other ideas?",,"['ordinary-differential-equations', 'functional-equations']"
58,Proving $x+\sin x-2\ln{(1+x)}\geqslant0$,Proving,x+\sin x-2\ln{(1+x)}\geqslant0,"Question: Let $x>-1$ , show that $$x+\sin x-2\ln{(1+x)}\geqslant 0.$$ This is true. See http://www.wolframalpha.com/input/?i=x%2Bsinx-2ln%281%2Bx%29 My try: For $$f(x)=x+\sin x-2\ln{(1+x)},\\ f'(x)=1+\cos{x}-\dfrac{2}{1+x}=\dfrac{x-1}{1+x}+\cos{x}=0\Longrightarrow\cos{x}=\dfrac{1-x}{1+x}.$$ So $$\sin x=\pm\sqrt{1-{\cos^2{x}}}=\pm \dfrac{2\sqrt{x}}{1+x}$$ If $\sin x=+\dfrac{2\sqrt{x}}{1+x}$ , I can prove it. But if $\sin x=-\dfrac{2\sqrt{x}}{1+x}$ , I cannot. See also http://www.wolframalpha.com/input/?i=%28x-1%29%2F%28x%2B1%29%2Bcosx This inequality seems nice, but it is not easy to prove. Thank you.","Question: Let , show that This is true. See http://www.wolframalpha.com/input/?i=x%2Bsinx-2ln%281%2Bx%29 My try: For So If , I can prove it. But if , I cannot. See also http://www.wolframalpha.com/input/?i=%28x-1%29%2F%28x%2B1%29%2Bcosx This inequality seems nice, but it is not easy to prove. Thank you.","x>-1 x+\sin x-2\ln{(1+x)}\geqslant 0. f(x)=x+\sin x-2\ln{(1+x)},\\
f'(x)=1+\cos{x}-\dfrac{2}{1+x}=\dfrac{x-1}{1+x}+\cos{x}=0\Longrightarrow\cos{x}=\dfrac{1-x}{1+x}. \sin x=\pm\sqrt{1-{\cos^2{x}}}=\pm \dfrac{2\sqrt{x}}{1+x} \sin x=+\dfrac{2\sqrt{x}}{1+x} \sin x=-\dfrac{2\sqrt{x}}{1+x}","['real-analysis', 'inequality', 'logarithms', 'maxima-minima']"
59,Proving that Continuous Open Functions are Strictly Monotonic,Proving that Continuous Open Functions are Strictly Monotonic,,"It is a fact from analysis that a continuous and open real-valued function of a real variable is strictly monotonic. The proof I know runs something like this: Suppose $f$ is an open and continuous map but is not  strictly monotonic. Consequently, there exist three numbers $a < c < b$ such that either $$ f(a) \geq f(c) \leq f(b) \;\;\;\; (1) $$ or $$ f(a) \leq f(c) \geq f(b) \;\;\;\; (2) $$ If $(1)$ holds then the exteme value theorem guarantees that $f$ attains its infimum on $[a,b]$; but by assumption, the infimum is at least as small as $f(c)$ so in fact $f$ attains its infimum on $(a,b)$. Also by assumption, $f$ carries open intervals to open intervals. With this though we have a contradiction since an open interval cannot contain it's own infimum. A similar argument yields considering suprema yields an analagous contradiction. Therefore, $f$ is strictly monotonic. My question is, Is there a more constructive way to prove this that doesn't involve contradiction? Although I think  the proof given is nice, I don't think I could have come up with it own my own because the consequences of $f$ not being strictly monotonic as exhibited in (1) and (2) would not have occurred to me. So, it would be good to see a direct way of proving this.","It is a fact from analysis that a continuous and open real-valued function of a real variable is strictly monotonic. The proof I know runs something like this: Suppose $f$ is an open and continuous map but is not  strictly monotonic. Consequently, there exist three numbers $a < c < b$ such that either $$ f(a) \geq f(c) \leq f(b) \;\;\;\; (1) $$ or $$ f(a) \leq f(c) \geq f(b) \;\;\;\; (2) $$ If $(1)$ holds then the exteme value theorem guarantees that $f$ attains its infimum on $[a,b]$; but by assumption, the infimum is at least as small as $f(c)$ so in fact $f$ attains its infimum on $(a,b)$. Also by assumption, $f$ carries open intervals to open intervals. With this though we have a contradiction since an open interval cannot contain it's own infimum. A similar argument yields considering suprema yields an analagous contradiction. Therefore, $f$ is strictly monotonic. My question is, Is there a more constructive way to prove this that doesn't involve contradiction? Although I think  the proof given is nice, I don't think I could have come up with it own my own because the consequences of $f$ not being strictly monotonic as exhibited in (1) and (2) would not have occurred to me. So, it would be good to see a direct way of proving this.",,['real-analysis']
60,Is indefinite integration suspect?,Is indefinite integration suspect?,,"In this post , Qiaochu Yuan remarks that 'it is convenient but misleading to write $$ \int f(x) \, dx=g(x) $$ [where the derivative of $g$ is $f$ ]'. This sentiment seems to be shared by many contributors here, and I don't understand why. To me, both definite and indefinite integration are both valid operations you can perform on a function, and there is nothing suspect about indefinite integration. I know about the fundamental theorem of calculus, which (as far as I understand) explains the link between indefinite and definite integration. If by integration we mean computing the area under the graph, the fundamental theorem of calculus shows us that integration is the opposite of differentiation, since $$ \frac{d}{dx} \int_{a}^{x} f(t) \, dt = f(x) $$ This shows that every continuous function has an antiderivative. Since a clear link between integration and antidifferentiation has been established, we give the antiderivative the convenient label 'indefinite integral'. (This also explains why the definite and indefinite integration notations are so similar.) This label is fine, so long as we remember that integration is defined as finding the area under the graph, while antidifferentiation is defined as finding the inverse of the derivative. Another result of the fundamental theorem of calculus is that $$ \int_{a}^{x}f(t) \, dt=\int f(x) \, dx $$ So obviously every indefinite integral can be rewritten in terms of definite integrals, but I don't understand the motivation behind this. If $F$ is an antiderivative of $f$ , then why is it more correct to write $$ \int_{a}^{x} f(t) \, dt = F(x) \, , $$ compared to $$ \int f(x) \, dx = F(x) \, ? $$","In this post , Qiaochu Yuan remarks that 'it is convenient but misleading to write [where the derivative of is ]'. This sentiment seems to be shared by many contributors here, and I don't understand why. To me, both definite and indefinite integration are both valid operations you can perform on a function, and there is nothing suspect about indefinite integration. I know about the fundamental theorem of calculus, which (as far as I understand) explains the link between indefinite and definite integration. If by integration we mean computing the area under the graph, the fundamental theorem of calculus shows us that integration is the opposite of differentiation, since This shows that every continuous function has an antiderivative. Since a clear link between integration and antidifferentiation has been established, we give the antiderivative the convenient label 'indefinite integral'. (This also explains why the definite and indefinite integration notations are so similar.) This label is fine, so long as we remember that integration is defined as finding the area under the graph, while antidifferentiation is defined as finding the inverse of the derivative. Another result of the fundamental theorem of calculus is that So obviously every indefinite integral can be rewritten in terms of definite integrals, but I don't understand the motivation behind this. If is an antiderivative of , then why is it more correct to write compared to","
\int f(x) \, dx=g(x)
 g f 
\frac{d}{dx} \int_{a}^{x} f(t) \, dt = f(x)
 
\int_{a}^{x}f(t) \, dt=\int f(x) \, dx
 F f 
\int_{a}^{x} f(t) \, dt = F(x) \, ,
 
\int f(x) \, dx = F(x) \, ?
","['real-analysis', 'calculus', 'integration']"
61,Antiderivative of $\log(x)$ without Parts,Antiderivative of  without Parts,\log(x),"I understand how the antiderivative  of $\log(x)$ can be obtained by Integration by Parts (i.e. product rule), but I was wondering how-if at all- it could be obtained only using sum/difference rule and substitution/chain rule.","I understand how the antiderivative  of $\log(x)$ can be obtained by Integration by Parts (i.e. product rule), but I was wondering how-if at all- it could be obtained only using sum/difference rule and substitution/chain rule.",,"['real-analysis', 'calculus', 'integration', 'logarithms']"
62,Showing that the exponential expression $e^x (x-1) + 1$ is positive,Showing that the exponential expression  is positive,e^x (x-1) + 1,"I'm looking at $$ f(x) = e^x (x-1) + 1$$ I'm having the feeling (based on the application where I am using it), that $f(x)$ should be strictly positive for $x > 0$. Indeed, Wolfram Alpha plots it as such, with a global minimum of ($f(0)x=0$). However, I fail to show this. It is trivial for $x \geq 1$, but what for $x < 1$?","I'm looking at $$ f(x) = e^x (x-1) + 1$$ I'm having the feeling (based on the application where I am using it), that $f(x)$ should be strictly positive for $x > 0$. Indeed, Wolfram Alpha plots it as such, with a global minimum of ($f(0)x=0$). However, I fail to show this. It is trivial for $x \geq 1$, but what for $x < 1$?",,"['calculus', 'real-analysis', 'inequality', 'exponential-function']"
63,Slightly changing the formal definition of continuity of $f: \mathbb{R} \to \mathbb{R}$?,Slightly changing the formal definition of continuity of ?,f: \mathbb{R} \to \mathbb{R},"I'm curious for some perspectives on why it would be wrong to change the definition of continuity of $f: \Bbb R \to \Bbb R$ in the following way: Original definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \epsilon > 0$ $\exists \delta > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . Altered definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \delta > 0$ $\exists \epsilon > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . The altered definition is more in line with what I think when I think about continuity intuitively: nearby points are sent to nearby points.  It only makes sense to me to be able to choose ""nearness"" in the domain (i.e., $\forall \delta > 0$ ) and show there is nearness in the codomain (i.e., $\exists \epsilon > 0$ ) to prove intuitively that ""nearby points are sent to nearby points"". Similarly , if $X, Y$ are topological spaces, we say $f: X \to Y$ is continuous if the preimages of open sets are open.  What would be wrong about changing the definition to say that a map is continuous if the images of open sets are open (i.e., $f$ is continuous if it is an open map)?  This is more inline with the intuitive idea of ""nearby points being sent to nearby points"" -- you pick nearness in the domain (i.e., an arbitrary open set) an show nearness in the codomain (i.e., the image is open). Does anyone have any useful remarks?","I'm curious for some perspectives on why it would be wrong to change the definition of continuity of in the following way: Original definition. is said to be continuous at if such that . Altered definition. is said to be continuous at if such that . The altered definition is more in line with what I think when I think about continuity intuitively: nearby points are sent to nearby points.  It only makes sense to me to be able to choose ""nearness"" in the domain (i.e., ) and show there is nearness in the codomain (i.e., ) to prove intuitively that ""nearby points are sent to nearby points"". Similarly , if are topological spaces, we say is continuous if the preimages of open sets are open.  What would be wrong about changing the definition to say that a map is continuous if the images of open sets are open (i.e., is continuous if it is an open map)?  This is more inline with the intuitive idea of ""nearby points being sent to nearby points"" -- you pick nearness in the domain (i.e., an arbitrary open set) an show nearness in the codomain (i.e., the image is open). Does anyone have any useful remarks?","f: \Bbb R \to \Bbb R f : \Bbb R \to \Bbb R x \in \Bbb R \forall \epsilon > 0 \exists \delta > 0 |x - a| < \delta \implies |f(x) - f(a)| < \epsilon f : \Bbb R \to \Bbb R x \in \Bbb R \forall \delta > 0 \exists \epsilon > 0 |x - a| < \delta \implies |f(x) - f(a)| < \epsilon \forall \delta > 0 \exists \epsilon > 0 X, Y f: X \to Y f","['real-analysis', 'general-topology', 'continuity']"
64,Convergence of the series $\sqrt[n]n-1$,Convergence of the series,\sqrt[n]n-1,Let $a_n=\sqrt[n]n-1$. Does $\sum_{n=1}^\infty a_n$ converge?,Let $a_n=\sqrt[n]n-1$. Does $\sum_{n=1}^\infty a_n$ converge?,,"['real-analysis', 'sequences-and-series']"
65,"If $\sum\frac1{a_n}$ is convergent, then irrational?","If  is convergent, then irrational?",\sum\frac1{a_n},"$\{a_n\}$ is a  strictly increasing sequence of positive integers such that $$\lim_{n\to\infty}\frac{a_{n+1}}{ a_n}=1$$ If $\sum\limits_{n=1}^\infty\frac1{a_n}$ is convergent, can one conclude that $\sum\limits_{n=1}^\infty\frac1{a_n}$ is an irrational number? a transcendental  number? A special case is $\zeta(n)(n\geq2)$ . so, the question, if true, may be difficult. Does someone suggest a counter-example? Thanks a lot!","is a  strictly increasing sequence of positive integers such that If is convergent, can one conclude that is an irrational number? a transcendental  number? A special case is . so, the question, if true, may be difficult. Does someone suggest a counter-example? Thanks a lot!",\{a_n\} \lim_{n\to\infty}\frac{a_{n+1}}{ a_n}=1 \sum\limits_{n=1}^\infty\frac1{a_n} \sum\limits_{n=1}^\infty\frac1{a_n} \zeta(n)(n\geq2),"['real-analysis', 'number-theory', 'irrational-numbers']"
66,Every bounded monotone sequence converges,Every bounded monotone sequence converges,,I am trying to prove: If a sequence is monotone and bounded then it converges. My idea is: Assume $a_n$ is monotone and not converges and then show that it is not bounded. But: my problem is that I fail to prove it is not bounded. Please can you help me? I found a different proof in a book but I want to know if my proof can work.,I am trying to prove: If a sequence is monotone and bounded then it converges. My idea is: Assume $a_n$ is monotone and not converges and then show that it is not bounded. But: my problem is that I fail to prove it is not bounded. Please can you help me? I found a different proof in a book but I want to know if my proof can work.,,"['real-analysis', 'sequences-and-series']"
67,Prove $\exp(x+y) = \exp(x) \exp(y)$ for $\exp(x) = \sum_{n=0}^\infty \frac {x^n}{n!}$,Prove  for,\exp(x+y) = \exp(x) \exp(y) \exp(x) = \sum_{n=0}^\infty \frac {x^n}{n!},"I am trying to prove $\exp(x+y) = \exp(x) \exp(y)$. I may use that $$\exp(x) = \sum_{n=0}^\infty \frac {x^n}{n!}$$ I further know how to multiply two power series in one point, i.e. if $f(x) = \sum_{n=0}^\infty c_n(x-a)^n$ and $g(x) = \sum_{k=0}^\infty d_n(x-a)^n$ then $$ f(x)g(x) = \sum_{n=0}^\infty e_n(x-a)^n $$ with $$ e_n = \sum_{m=0}^n c_md_{n-m} $$","I am trying to prove $\exp(x+y) = \exp(x) \exp(y)$. I may use that $$\exp(x) = \sum_{n=0}^\infty \frac {x^n}{n!}$$ I further know how to multiply two power series in one point, i.e. if $f(x) = \sum_{n=0}^\infty c_n(x-a)^n$ and $g(x) = \sum_{k=0}^\infty d_n(x-a)^n$ then $$ f(x)g(x) = \sum_{n=0}^\infty e_n(x-a)^n $$ with $$ e_n = \sum_{m=0}^n c_md_{n-m} $$",,['real-analysis']
68,How to prove a polynomial can be written as Taylor-style?,How to prove a polynomial can be written as Taylor-style?,,"I know that by Taylor's theorem, a function $f$ under some assumptions, can be computed by $$f(x)=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{(n+1)}$$ If $f$ itself is a polynomial of degree $n$, then $$f(x)=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n.$$ This can be directly deduced from Taylor's theorem as I mentioned. However, since this is a much simpler result, can we prove it without using that theorem? And is there intuitive understanding of the above equation?","I know that by Taylor's theorem, a function $f$ under some assumptions, can be computed by $$f(x)=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{(n+1)}$$ If $f$ itself is a polynomial of degree $n$, then $$f(x)=f(a)+f'(a)(x-a)+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n.$$ This can be directly deduced from Taylor's theorem as I mentioned. However, since this is a much simpler result, can we prove it without using that theorem? And is there intuitive understanding of the above equation?",,"['calculus', 'real-analysis', 'derivatives', 'power-series', 'taylor-expansion']"
69,Why dense subsets are convenient to prove theorems,Why dense subsets are convenient to prove theorems,,"Could you please explain the following concept (preferably by examples) about dense subsets: If you want to prove that every point in $A$ has a certain property that is preserved under limits, then it suffices to prove that every point in a dense subset $B$ of $A$ has that property. What are the examples of properties that are preserved under limits? Why is it sufficient to prove such properties for a dense subset $B\subseteq A\subseteq closure(B)$?","Could you please explain the following concept (preferably by examples) about dense subsets: If you want to prove that every point in $A$ has a certain property that is preserved under limits, then it suffices to prove that every point in a dense subset $B$ of $A$ has that property. What are the examples of properties that are preserved under limits? Why is it sufficient to prove such properties for a dense subset $B\subseteq A\subseteq closure(B)$?",,['real-analysis']
70,Prove that the limit of $\sin n$ as $n \rightarrow \infty$ does not exist [duplicate],Prove that the limit of  as  does not exist [duplicate],\sin n n \rightarrow \infty,"This question already has answers here : Prove the divergence of the sequence $\left\{ \sin(n) \right\}_{n=1}^{\infty}$. (9 answers) Closed 3 years ago . Using only the delta definition of a limit, how can we prove that the sequence $\{a_n\}$, where $a_n = \sin n$, as $n$ tends to infinity does not have a limit? Thanks!","This question already has answers here : Prove the divergence of the sequence $\left\{ \sin(n) \right\}_{n=1}^{\infty}$. (9 answers) Closed 3 years ago . Using only the delta definition of a limit, how can we prove that the sequence $\{a_n\}$, where $a_n = \sin n$, as $n$ tends to infinity does not have a limit? Thanks!",,['calculus']
71,Can a function have a derivative where it has no value?,Can a function have a derivative where it has no value?,,"Consider the following function: $$f(x) = x^2 \mid x \in \mathbb{R}, \ x \ne 0$$ The derivative at $x=0$ seems to want to be zero, in the same way that $\lim \limits_{x \to 0} f(x) = 0$ However, when I look at the definition of the derivative, this doesn't seem to work: $$f'(x) =  \lim \limits_{\Delta x \to 0} \frac{f(x+\Delta x )-f(x)}{\Delta x}.$$ The function isn't defined at $f(x)$, so $f'(x)$ is also undefined. Would it make any sense to replace the $f(x)$ in the definition with $\lim \limits_{x \to 0} f(x) = 0$? Then, I suppose we'd have $\lim \limits_{x \to 0} f'(x) = 0$? Would it be permissible? Would there be any point?","Consider the following function: $$f(x) = x^2 \mid x \in \mathbb{R}, \ x \ne 0$$ The derivative at $x=0$ seems to want to be zero, in the same way that $\lim \limits_{x \to 0} f(x) = 0$ However, when I look at the definition of the derivative, this doesn't seem to work: $$f'(x) =  \lim \limits_{\Delta x \to 0} \frac{f(x+\Delta x )-f(x)}{\Delta x}.$$ The function isn't defined at $f(x)$, so $f'(x)$ is also undefined. Would it make any sense to replace the $f(x)$ in the definition with $\lim \limits_{x \to 0} f(x) = 0$? Then, I suppose we'd have $\lim \limits_{x \to 0} f'(x) = 0$? Would it be permissible? Would there be any point?",,"['calculus', 'real-analysis']"
72,"Evaluate $\int_{0}^{\infty} \frac{{(1+x)}^{-n}}{\log^2 x+\pi^2} \ dx, \space n\ge1$",Evaluate,"\int_{0}^{\infty} \frac{{(1+x)}^{-n}}{\log^2 x+\pi^2} \ dx, \space n\ge1","Evaluate the integral $$\int_{0}^{\infty} \frac{{(1+x)}^{-n}}{\log^2 x+\pi^2} \ dx, \space n\ge1$$","Evaluate the integral $$\int_{0}^{\infty} \frac{{(1+x)}^{-n}}{\log^2 x+\pi^2} \ dx, \space n\ge1$$",,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
73,"Real Analysis Methodologies to show $\gamma =2\int_0^\infty \frac{\cos(x^2)-\cos(x)}{x}\,dx$",Real Analysis Methodologies to show,"\gamma =2\int_0^\infty \frac{\cos(x^2)-\cos(x)}{x}\,dx","In THIS ANSWER , I used straightforward complex analysis to show that $$\gamma =2\int_0^\infty \frac{\cos(x^2)-\cos(x)}{x}\,dx \tag 1$$ where $\gamma =-\int_0^\infty \log(x) e^{-x}\,dx$ is the Euler-Mascheroni Constant . The key in the derivation of $(1)$ was to transform the cosine terms into real exponential ones. To date, I have been unable to use strictly real analysis, without appealing to tabulated results of special functions (e.g., use of the $\text{Cin}(x)$ and $\text{Ci}(x)$ functions), to prove $(1)$. I have tried introducing a parameter and using ""Feynman's Trick to augment the integral into something manageable.  Or somewhat equivalently, rewriting the integral in $(1)$ as a double integral and proceeding by exploiting Fubini-Tonelli. QUESTION:  What are ways to prove $(1)$ without relying on complex analysis and without simply appealing to tabulated relationships of special functions.  For example, stating that the $Ci(x)$ function is defined as $\text{Ci}(x)\equiv -\int_x^\infty\frac{\cos(t)}{t}\,dt=\gamma +\log(x) +\int_0^x \frac{\cos(t)-1}{t}\,dt$ is unsatisfactory unless one proves the latter equality.","In THIS ANSWER , I used straightforward complex analysis to show that $$\gamma =2\int_0^\infty \frac{\cos(x^2)-\cos(x)}{x}\,dx \tag 1$$ where $\gamma =-\int_0^\infty \log(x) e^{-x}\,dx$ is the Euler-Mascheroni Constant . The key in the derivation of $(1)$ was to transform the cosine terms into real exponential ones. To date, I have been unable to use strictly real analysis, without appealing to tabulated results of special functions (e.g., use of the $\text{Cin}(x)$ and $\text{Ci}(x)$ functions), to prove $(1)$. I have tried introducing a parameter and using ""Feynman's Trick to augment the integral into something manageable.  Or somewhat equivalently, rewriting the integral in $(1)$ as a double integral and proceeding by exploiting Fubini-Tonelli. QUESTION:  What are ways to prove $(1)$ without relying on complex analysis and without simply appealing to tabulated relationships of special functions.  For example, stating that the $Ci(x)$ function is defined as $\text{Ci}(x)\equiv -\int_x^\infty\frac{\cos(t)}{t}\,dt=\gamma +\log(x) +\int_0^x \frac{\cos(t)-1}{t}\,dt$ is unsatisfactory unless one proves the latter equality.",,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'euler-mascheroni-constant']"
74,A continuous function with positive and negative values but never zero?,A continuous function with positive and negative values but never zero?,,"Well, it is easy to prove that $e^z$ is never zero and $z$ is any complex number. Also, $e^z$ can be both positive and negative. On the other hand, $e^z$ is continuous. How that's possible that a continuous function can be negative and positive but never meets zero? Detailed simple explanations would be much appreciated.","Well, it is easy to prove that $e^z$ is never zero and $z$ is any complex number. Also, $e^z$ can be both positive and negative. On the other hand, $e^z$ is continuous. How that's possible that a continuous function can be negative and positive but never meets zero? Detailed simple explanations would be much appreciated.",,['real-analysis']
75,Prove that the sum of two compact sets in $\mathbb R^n$ is compact.,Prove that the sum of two compact sets in  is compact.,\mathbb R^n,"Given two sets $S_1$ and $S_2$ in $\mathbb R^n$ define their sum by    $$S_1+S_2=\{x\in\mathbb R^n; x=x_1+x_2, x_1\in S_1, x_2\in S_2\}.$$   Prove that if $S_1$ and $S_2$ are compact, $S_1+S_2$ is also compact. Prove that the sum of two compact sets in $\mathbb R^n$ is compact. Compact set is the one which is both bounded and closed. The finite union of closed sets is closed. But union is not the same as defined in the task. I so not know how to proceed. I do understand that I need to show that the resulting set is both bounded and closed, but I do know how to do that.","Given two sets $S_1$ and $S_2$ in $\mathbb R^n$ define their sum by    $$S_1+S_2=\{x\in\mathbb R^n; x=x_1+x_2, x_1\in S_1, x_2\in S_2\}.$$   Prove that if $S_1$ and $S_2$ are compact, $S_1+S_2$ is also compact. Prove that the sum of two compact sets in $\mathbb R^n$ is compact. Compact set is the one which is both bounded and closed. The finite union of closed sets is closed. But union is not the same as defined in the task. I so not know how to proceed. I do understand that I need to show that the resulting set is both bounded and closed, but I do know how to do that.",,"['real-analysis', 'compactness', 'sumset']"
76,Continuous functions are bounded,Continuous functions are bounded,,"How one demonstrates this claim: Let $F:\mathbb{R}^m \rightarrow \mathbb{R}^n$ be a continuous function, if $X \subset \mathbb{R}^m$ is bounded then $F(X)$ is bounded.","How one demonstrates this claim: Let $F:\mathbb{R}^m \rightarrow \mathbb{R}^n$ be a continuous function, if $X \subset \mathbb{R}^m$ is bounded then $F(X)$ is bounded.",,['real-analysis']
77,Another evaluating limit question: $\lim\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}$,Another evaluating limit question:,\lim\frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n},How do I begin to evaluate this limit: $$\lim_{n\to \infty}\ \frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}\;?$$ Thanks a lot.,How do I begin to evaluate this limit: $$\lim_{n\to \infty}\ \frac{1\cdot3\cdot5\cdot\ldots\cdot(2n-1)}{2\cdot4\cdot6\cdot\ldots\cdot2n}\;?$$ Thanks a lot.,,"['calculus', 'real-analysis', 'limits', 'factorial', 'infinite-product']"
78,"If $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=1$, is $f$ asymptotic to $\exp$?","If , is  asymptotic to ?",\lim_{x\to\infty}\frac{f'(x)}{f(x)}=1 f \exp,"For a differentiable and nonzero function $f:(a,\infty)\to\mathbb R$ , it seems like the local and end behavior of $f'(x)/f(x)$ gives a measure for how similar $f$ is to an exponential function. This is a reasonable guess because the quantity $f'(x)/f(x)$ gives the derivative of $\ln\circ f$ at $x$ . My suspicion began when I noticed that for any exponential $a^x$ , we have $$\frac{f'(x)}{f(x)}=\frac{a^x\ln(a)}{a^x}=\ln(a)$$ indicating that exponential functions are the only functions for which $f'/f$ is constant. Exploring this further with the hyperbolic cosine $\cosh(x)$ , a function which is basically identical to $\exp(x)$ as $x\to\infty$ , I arrived at $$\frac{f'(x)}{f(x)}=\frac{\sinh(x)}{\cosh(x)}=\tanh(x)\to 1\text{ as }x\to\infty$$ These examples were enough to convince me, perhaps erroneously, that $f'/f$ gives a measure for how similar a function is to an exponential, leading me to the following conjecture: Suppose $f:(a,\infty)\to\mathbb R$ is a differentiable function that is never zero. If $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=L$ , does it follow that for some constant $C$ , $f(x)\sim C\exp(Lx)$ as $x\to\infty$ ? For simplicity, I focused my attention to the case where $f'(x)/f(x)\to 1$ , $f$ is strictly positive, and $f'/f$ is ""eventually integrable"", i.e. there is a constant $c$ such that for every $x$ greater than $c$ , $f'/f$ is integrable over $[c,x]$ . Unraveling $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=1$ with the definition of a limit and leveraging these simplifying assumptions, I was able to deduce the following: For every $\varepsilon>0$ , there is a $\delta\in\mathbb R$ such that for some constants $C_1,C_2>0$ , $$C_1\exp\left((1-\varepsilon)x\right)<f(x)<C_2\exp\left((1+\varepsilon)x\right)\text{ for every }x\in\text{dom}[f'/f]\text{ with }x>\delta$$ As we make $\varepsilon$ smaller and smaller, the functions $\exp\left((1-\varepsilon)x\right)$ and $\exp\left((1+\varepsilon)x\right)$ will look more and more like $\exp(x)$ , suggesting that the ultimate goal of $\lim_{x\to\infty}\frac{f(x)}{C\exp(x)}=1$ might be true. However, it doesn't seem like my result is sufficient to arrive at this conclusion. The constants $C_1,C_2>0$ depend on $\varepsilon$ and $\delta$ , so I'm afraid they may fluctuate enough to ruin any hopes of asymptotic equivalence. Is my simplified conjecture, equipped with the integrability assumption, actually true? If so, how can I reach the goal? If not, what other assumptions on $f$ are needed to ensure asymptotic equivalence?","For a differentiable and nonzero function , it seems like the local and end behavior of gives a measure for how similar is to an exponential function. This is a reasonable guess because the quantity gives the derivative of at . My suspicion began when I noticed that for any exponential , we have indicating that exponential functions are the only functions for which is constant. Exploring this further with the hyperbolic cosine , a function which is basically identical to as , I arrived at These examples were enough to convince me, perhaps erroneously, that gives a measure for how similar a function is to an exponential, leading me to the following conjecture: Suppose is a differentiable function that is never zero. If , does it follow that for some constant , as ? For simplicity, I focused my attention to the case where , is strictly positive, and is ""eventually integrable"", i.e. there is a constant such that for every greater than , is integrable over . Unraveling with the definition of a limit and leveraging these simplifying assumptions, I was able to deduce the following: For every , there is a such that for some constants , As we make smaller and smaller, the functions and will look more and more like , suggesting that the ultimate goal of might be true. However, it doesn't seem like my result is sufficient to arrive at this conclusion. The constants depend on and , so I'm afraid they may fluctuate enough to ruin any hopes of asymptotic equivalence. Is my simplified conjecture, equipped with the integrability assumption, actually true? If so, how can I reach the goal? If not, what other assumptions on are needed to ensure asymptotic equivalence?","f:(a,\infty)\to\mathbb R f'(x)/f(x) f f'(x)/f(x) \ln\circ f x a^x \frac{f'(x)}{f(x)}=\frac{a^x\ln(a)}{a^x}=\ln(a) f'/f \cosh(x) \exp(x) x\to\infty \frac{f'(x)}{f(x)}=\frac{\sinh(x)}{\cosh(x)}=\tanh(x)\to 1\text{ as }x\to\infty f'/f f:(a,\infty)\to\mathbb R \lim_{x\to\infty}\frac{f'(x)}{f(x)}=L C f(x)\sim C\exp(Lx) x\to\infty f'(x)/f(x)\to 1 f f'/f c x c f'/f [c,x] \lim_{x\to\infty}\frac{f'(x)}{f(x)}=1 \varepsilon>0 \delta\in\mathbb R C_1,C_2>0 C_1\exp\left((1-\varepsilon)x\right)<f(x)<C_2\exp\left((1+\varepsilon)x\right)\text{ for every }x\in\text{dom}[f'/f]\text{ with }x>\delta \varepsilon \exp\left((1-\varepsilon)x\right) \exp\left((1+\varepsilon)x\right) \exp(x) \lim_{x\to\infty}\frac{f(x)}{C\exp(x)}=1 C_1,C_2>0 \varepsilon \delta f","['real-analysis', 'calculus', 'limits', 'asymptotics', 'exponential-function']"
79,Suppose $(a_n)$ is a sequence such that $a_n=\frac{1!+2!+\cdots+n!}{n!}$. Show that $\lim{a_n}=1$,Suppose  is a sequence such that . Show that,(a_n) a_n=\frac{1!+2!+\cdots+n!}{n!} \lim{a_n}=1,"Suppose $(a_n)$ is a sequence such that $$a_n=\frac{1!+2!+\cdots+n!}{n!} \, .$$ Show that $\lim_{n \rightarrow \infty}{a_n}=1$. My attempt is to formulate an inequality and then use the Squeeze Theorem. Since we know that $$\frac{1}{n!}+\frac{2!}{n!}+\cdots+1>1$$ as $\frac{i!}{n!}>0$ for all $1 \leq i \leq n-1$, we have $a_n \geq1$. Then I'm stuck at formulating another side. Can anyone help?","Suppose $(a_n)$ is a sequence such that $$a_n=\frac{1!+2!+\cdots+n!}{n!} \, .$$ Show that $\lim_{n \rightarrow \infty}{a_n}=1$. My attempt is to formulate an inequality and then use the Squeeze Theorem. Since we know that $$\frac{1}{n!}+\frac{2!}{n!}+\cdots+1>1$$ as $\frac{i!}{n!}>0$ for all $1 \leq i \leq n-1$, we have $a_n \geq1$. Then I'm stuck at formulating another side. Can anyone help?",,['real-analysis']
80,"Evaluate $\int_0^{{\pi}/{2}} \log(1+\cos x)\, dx$",Evaluate,"\int_0^{{\pi}/{2}} \log(1+\cos x)\, dx","Find the value of $\displaystyle \int_0^{{\pi}/{2}} \log(1+\cos x)\ dx$ I tried to put $1+ \cos x = 2 \cos^2 \frac{x}{2} $, but I am unable to proceed further. I think the following integral can be helpful:  $\displaystyle \int_0^{{\pi}/{2}} \log(\cos x)\ dx =-\frac{\pi}{2} \log2 $.","Find the value of $\displaystyle \int_0^{{\pi}/{2}} \log(1+\cos x)\ dx$ I tried to put $1+ \cos x = 2 \cos^2 \frac{x}{2} $, but I am unable to proceed further. I think the following integral can be helpful:  $\displaystyle \int_0^{{\pi}/{2}} \log(\cos x)\ dx =-\frac{\pi}{2} \log2 $.",,"['calculus', 'real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
81,Limit of the absolute value of a function,Limit of the absolute value of a function,,"Say I have a real valued function $f(x)$, is it true that if $\lim_{x \rightarrow c} |f(x)| = 0$ then $\lim_{x \rightarrow c} f(x) = 0$?, where $c$ can be a real number or $\pm \infty$.","Say I have a real valued function $f(x)$, is it true that if $\lim_{x \rightarrow c} |f(x)| = 0$ then $\lim_{x \rightarrow c} f(x) = 0$?, where $c$ can be a real number or $\pm \infty$.",,"['real-analysis', 'absolute-value']"
82,"For $a_n>0$ such that $\sum a_n $ converges, show that there exist $c_n>0$ such that $c_n\to \infty$ and $\sum a_n c_n$ is finite.","For  such that  converges, show that there exist  such that  and  is finite.",a_n>0 \sum a_n  c_n>0 c_n\to \infty \sum a_n c_n,A problem From PhD Pre lims Exam: Let $ a_{n} > 0 $ for all $ n\in\mathbb{N}$ such that $\sum a_{n} $ converges. Show that there exist $ c_{n} > 0 $ ($n\in\mathbb{N}$) such that $ \lim \limits_{n\to\infty} c_{n}= \infty $ and $\sum a_{n}c_{n} $ is finite.,A problem From PhD Pre lims Exam: Let $ a_{n} > 0 $ for all $ n\in\mathbb{N}$ such that $\sum a_{n} $ converges. Show that there exist $ c_{n} > 0 $ ($n\in\mathbb{N}$) such that $ \lim \limits_{n\to\infty} c_{n}= \infty $ and $\sum a_{n}c_{n} $ is finite.,,"['real-analysis', 'sequences-and-series']"
83,"can polynomial functions with rational coefficients approximate any continuous function on $[a,b]$.",can polynomial functions with rational coefficients approximate any continuous function on .,"[a,b]","The Stone-Weierstrass theorem states that for any continuous function $f$ on $[a,b]$ there exists a polynomial function $p$ such that $\|f-p\|<\varepsilon$ for any $\varepsilon>0$. Where $\|\cdot\|$ is the sup norm. If I replace ""a polynomial function"" with ""a polynomial function with rational coefficient"" from the above statement, will the statement still hold true?","The Stone-Weierstrass theorem states that for any continuous function $f$ on $[a,b]$ there exists a polynomial function $p$ such that $\|f-p\|<\varepsilon$ for any $\varepsilon>0$. Where $\|\cdot\|$ is the sup norm. If I replace ""a polynomial function"" with ""a polynomial function with rational coefficient"" from the above statement, will the statement still hold true?",,['real-analysis']
84,Example of a countable compact set in the real numbers [closed],Example of a countable compact set in the real numbers [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Can someone give me an example or a hint to come up with a countable compact set in the real line with infinitely many accumulation points? Thank you in advance!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Can someone give me an example or a hint to come up with a countable compact set in the real line with infinitely many accumulation points? Thank you in advance!",,"['real-analysis', 'metric-spaces']"
85,Is there a sequence with an uncountable number of accumulation points?,Is there a sequence with an uncountable number of accumulation points?,,Let $(x_{n})_{n \geq 1}$ sequence in $\mathbb{R}$. Is there a sequence with an uncountable number of accumulation points? Thank you!,Let $(x_{n})_{n \geq 1}$ sequence in $\mathbb{R}$. Is there a sequence with an uncountable number of accumulation points? Thank you!,,"['real-analysis', 'sequences-and-series', 'general-topology']"
86,What is a continuous extension?,What is a continuous extension?,,"The continuous extension of $f(x)$ at $x=c$ makes the function continuous at that point. Can you elaborate some more? I wasn't able to find very much on ""continuous extension"" throughout the web. How can you turn a point of discontinuity into a point of continuity? How is the function being ""extended"" into continuity? Thank you.","The continuous extension of $f(x)$ at $x=c$ makes the function continuous at that point. Can you elaborate some more? I wasn't able to find very much on ""continuous extension"" throughout the web. How can you turn a point of discontinuity into a point of continuity? How is the function being ""extended"" into continuity? Thank you.",,"['calculus', 'real-analysis', 'continuity']"
87,Why no simple proof by contradiction for mean value theorem?,Why no simple proof by contradiction for mean value theorem?,,"I'm reading the Wikipedia proof for the MVT, and it uses Rolle's theorem. In fact, many other websites that prove MVT do the same. When I first read the statement of the mean value theorem, I thought it must obviously be true because the alternative, that $f'(x) > \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b)$ was absurd (and same for $f'(x)$ strictly less than the right hand side), because the rate of increase of the function being higher than the average rate of increase at all points contradicts the very definition of the average rate of increase. By this, I mean that if the function $f$ was increasing at the average rate $f'(x) = \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b)$ then it would exactly go straight from $(a, f(a))$ to $(b, f(b))$ (this is the definition of average), and so if it's always increasing strictly faster , then surely it increases ""too much"" to be able to get down to $(b, f(b))$ in time, so to speak. Yet, I do not see sites formalizing this to give a proof by contradiction of the mean value theorem in a line or two, so I imagine this must be going wrong somewhere. Could someone tell me where?","I'm reading the Wikipedia proof for the MVT, and it uses Rolle's theorem. In fact, many other websites that prove MVT do the same. When I first read the statement of the mean value theorem, I thought it must obviously be true because the alternative, that was absurd (and same for strictly less than the right hand side), because the rate of increase of the function being higher than the average rate of increase at all points contradicts the very definition of the average rate of increase. By this, I mean that if the function was increasing at the average rate then it would exactly go straight from to (this is the definition of average), and so if it's always increasing strictly faster , then surely it increases ""too much"" to be able to get down to in time, so to speak. Yet, I do not see sites formalizing this to give a proof by contradiction of the mean value theorem in a line or two, so I imagine this must be going wrong somewhere. Could someone tell me where?","f'(x) > \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b) f'(x) f f'(x) = \frac{f(b)-f(a)}{b-a} \ \forall x \in (a,b) (a, f(a)) (b, f(b)) (b, f(b))",['real-analysis']
88,"Dimension of vector space, countable, uncountable?","Dimension of vector space, countable, uncountable?",,"In set theory, when we talk about the cardinality of a set we have notions of finite, countable and uncountably infinite sets. Main Question Let's talk about the dimension of a vector space. In linear algebra I have heard that vector spaces are either of finite dimension (for example $\mathbb{R}^n$ ) or infinite dimension (for example $C[0,1]$ ). Why don't we have notions of countably infinite dimensional and uncountably infinite dimensional vector spaces? Maybe, I am missing the bigger picture. Extras P.S. A long time ago, I attended a talk given on enumerative algebraic geometry and the professor said, ""I always think of a positive natural number as the dimension of some vector space"". Can this idea then be extended to vector spaces of uncountably infinite dimension by considering transfinite numbers as denoting the dimension of some vector space?","In set theory, when we talk about the cardinality of a set we have notions of finite, countable and uncountably infinite sets. Main Question Let's talk about the dimension of a vector space. In linear algebra I have heard that vector spaces are either of finite dimension (for example ) or infinite dimension (for example ). Why don't we have notions of countably infinite dimensional and uncountably infinite dimensional vector spaces? Maybe, I am missing the bigger picture. Extras P.S. A long time ago, I attended a talk given on enumerative algebraic geometry and the professor said, ""I always think of a positive natural number as the dimension of some vector space"". Can this idea then be extended to vector spaces of uncountably infinite dimension by considering transfinite numbers as denoting the dimension of some vector space?","\mathbb{R}^n C[0,1]","['real-analysis', 'linear-algebra', 'cardinals']"
89,Infinite Series $\sum\limits_{k=1}^{\infty}\frac{k^n}{k!}$,Infinite Series,\sum\limits_{k=1}^{\infty}\frac{k^n}{k!},"How can I find the value of the sum $\sum_{k=1}^{\infty}\frac{k^n}{k!}$? for example for $n=6$, we have $$\sum_{k=1}^{\infty}\frac{k^6}{k!}=203e.$$","How can I find the value of the sum $\sum_{k=1}^{\infty}\frac{k^n}{k!}$? for example for $n=6$, we have $$\sum_{k=1}^{\infty}\frac{k^6}{k!}=203e.$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'summation', 'closed-form']"
90,Estimating the integral $\sqrt{n}\cdot \int\limits_0^\pi \left( \frac{1 + \cos t}{2} \right)^n dt$,Estimating the integral,\sqrt{n}\cdot \int\limits_0^\pi \left( \frac{1 + \cos t}{2} \right)^n dt,"Consider the sequence $\{a_n\}$ defined by $$ a_n = \sqrt{n}\cdot \int_{0}^{\pi} \left( \frac{1 + \cos t}{2} \right)^n dt.$$ An exercise in Rudin, Real and Complex Analysis, requires showing that this sequence is convergent to a real number $a$, with $ a > 0$. I don't have any idea of how to prove this.  I only obtained the following estimation $$ \begin{align*}  \int_{0}^{\pi} \left( \frac{1 + \cos t}{2} \right)^n  dt &= 2 \int_{0}^{\frac{\pi}{2}} \left( 1 - \sin^2 t \right)^n dt \\  &> 2 \int_{0}^{\frac{1}{\sqrt{n}}} (1 - t^2)^n dt \\  &> 2 \int_{0}^{\frac{1}{\sqrt{n}}} (1 - n t^2) dt \\ & = \frac{4}{3 \sqrt{n}},  \end{align*}$$ which shows that $ a_n > \frac{4}{3}$. Thank you very much in advance for any help.","Consider the sequence $\{a_n\}$ defined by $$ a_n = \sqrt{n}\cdot \int_{0}^{\pi} \left( \frac{1 + \cos t}{2} \right)^n dt.$$ An exercise in Rudin, Real and Complex Analysis, requires showing that this sequence is convergent to a real number $a$, with $ a > 0$. I don't have any idea of how to prove this.  I only obtained the following estimation $$ \begin{align*}  \int_{0}^{\pi} \left( \frac{1 + \cos t}{2} \right)^n  dt &= 2 \int_{0}^{\frac{\pi}{2}} \left( 1 - \sin^2 t \right)^n dt \\  &> 2 \int_{0}^{\frac{1}{\sqrt{n}}} (1 - t^2)^n dt \\  &> 2 \int_{0}^{\frac{1}{\sqrt{n}}} (1 - n t^2) dt \\ & = \frac{4}{3 \sqrt{n}},  \end{align*}$$ which shows that $ a_n > \frac{4}{3}$. Thank you very much in advance for any help.",,"['real-analysis', 'sequences-and-series', 'integration']"
91,(Non)Existence of limits,(Non)Existence of limits,,When we say that a limit of a function does not exist in $\mathbb{R}$ (or some metric space) does it make sense to say that it might exist somewhere else? [I am trying to think along lines of existence of imaginary roots] If yes. Then give examples especially regarding $\mathbb{R}$.,When we say that a limit of a function does not exist in $\mathbb{R}$ (or some metric space) does it make sense to say that it might exist somewhere else? [I am trying to think along lines of existence of imaginary roots] If yes. Then give examples especially regarding $\mathbb{R}$.,,"['real-analysis', 'analysis', 'limits', 'functions', 'metric-spaces']"
92,"Real sequences which sum to 0, multiply to 1.","Real sequences which sum to 0, multiply to 1.",,"Does there exist two sequences $(x_n)_n$, $(y_n)_n$ of real numbers such that $\lim_n x_n-y_n\neq 0$ (may not exist), but $\lim_n x_n+y_n=0$ and $\lim_n x_ny_n=1$? Notice that it cannot be the case that both sequences are convergent, or even bounded, since by taking convergent subsequences, say with limits $x$ and $y$, we would have that $x+y=0$ and $xy=1$, which has no real solutions.","Does there exist two sequences $(x_n)_n$, $(y_n)_n$ of real numbers such that $\lim_n x_n-y_n\neq 0$ (may not exist), but $\lim_n x_n+y_n=0$ and $\lim_n x_ny_n=1$? Notice that it cannot be the case that both sequences are convergent, or even bounded, since by taking convergent subsequences, say with limits $x$ and $y$, we would have that $x+y=0$ and $xy=1$, which has no real solutions.",,['real-analysis']
93,Differentiability of $f(x) = x^2 \sin{\frac{1}{x}}$ and $f'$,Differentiability of  and,f(x) = x^2 \sin{\frac{1}{x}} f',"Let $f(x) = x^2 \sin{\frac{1}{x}}$ for $x\neq 0$ and $f(0) =0$ . (a) Use the basic properties of the derivative, and the Chain Rule to show that $f$ is differentiable at each $a\neq 0$ and calculate $f'(a)$ . You may use without proof that $\sin$ is differentiable and that $\sin' =\cos$ . Not even sure what this is asking. (b) Show that $f$ is differentiable at $0$ and that $f'(0) =0$ . $\frac {f(x)-f(0)}{x-0} \to \lim_{x \to 0} x \sin(1/x)$ . $x \sin(1/x) \leq |x|$ and $\lim_{x \to 0} |x|=0$ . Thus $f(x)$ is differentiable at $0$ ; moreover $f^{'}(0)=0$ . (c) Show that $f'$ is not continuous at $0$ . $f{'}(x)=x^{2} \cos(1/x) (-x^{-2}) + 2x \sin (1/x)$ . In pieces: $\lim_{x \to 0} \cos (1/x)$ . $f^{'}(0-)$ nor $f{'}(0+)$ exists as $x \to 0$ $f^{'}(x)$ oscillates infinity between $-1$ and $1$ with ever increase frequency as $x \rightarrow 0$ for any $p>0$ $[-p,0]$ , $[-p,p]$ or $[0,p]$ $f$ is not continuous. Question : How to show more rigorously?","Let for and . (a) Use the basic properties of the derivative, and the Chain Rule to show that is differentiable at each and calculate . You may use without proof that is differentiable and that . Not even sure what this is asking. (b) Show that is differentiable at and that . . and . Thus is differentiable at ; moreover . (c) Show that is not continuous at . . In pieces: . nor exists as oscillates infinity between and with ever increase frequency as for any , or is not continuous. Question : How to show more rigorously?","f(x) = x^2 \sin{\frac{1}{x}} x\neq 0 f(0) =0 f a\neq 0 f'(a) \sin \sin' =\cos f 0 f'(0) =0 \frac {f(x)-f(0)}{x-0} \to \lim_{x \to 0} x \sin(1/x) x \sin(1/x) \leq |x| \lim_{x \to 0} |x|=0 f(x) 0 f^{'}(0)=0 f' 0 f{'}(x)=x^{2} \cos(1/x) (-x^{-2}) + 2x \sin (1/x) \lim_{x \to 0} \cos (1/x) f^{'}(0-) f{'}(0+) x \to 0 f^{'}(x) -1 1 x \rightarrow 0 p>0 [-p,0] [-p,p] [0,p] f","['calculus', 'real-analysis', 'derivatives']"
94,I feel like epsilon-delta is reversed,I feel like epsilon-delta is reversed,,"The lim is about ""when x approachs a, then y approachs L"". Then, shouldn't the epsilon and delta be like ""For all delta, no matter how small the delta is, you can always find an epsilon that makes ε < f(x)-L < ε""? But, the conventional explanation says like ""for all epsilon, you find delta"", which feels like to me, ""when y approachs L, x goes to a"".","The lim is about ""when x approachs a, then y approachs L"". Then, shouldn't the epsilon and delta be like ""For all delta, no matter how small the delta is, you can always find an epsilon that makes ε < f(x)-L < ε""? But, the conventional explanation says like ""for all epsilon, you find delta"", which feels like to me, ""when y approachs L, x goes to a"".",,"['real-analysis', 'limits', 'definition', 'epsilon-delta']"
95,Fat Cantor Set with large complement???,Fat Cantor Set with large complement???,,"I'm encontering fat Cantor sets for the first time, and I found the formula for the length of the complement on Wikipedia (you know, like you do) as $\mu(I \setminus C_\alpha) = \alpha \sum_{n=1}^\infty (2\alpha)^n = \frac{\alpha}{1-2\alpha}$ . This makes sense and is a pretty intuitive extension of the normal Cantor set formula, but also it seems like I can make it arbitrarily large for an $\alpha$ arbitrarily close to $\frac{1}{2}$ , which doesn't make much sense for a subset of $[0,1]$ . That said, given the construction of the set, I'm not sure why I'm not allowed to take intervals of length $3^{-n} < \alpha^n < 2^{-n}$ and still use this formula to compute the length of the complement. I feel like I'm missing someting small and silly, so if anyone can set me right on this, I'd appreciate it.","I'm encontering fat Cantor sets for the first time, and I found the formula for the length of the complement on Wikipedia (you know, like you do) as . This makes sense and is a pretty intuitive extension of the normal Cantor set formula, but also it seems like I can make it arbitrarily large for an arbitrarily close to , which doesn't make much sense for a subset of . That said, given the construction of the set, I'm not sure why I'm not allowed to take intervals of length and still use this formula to compute the length of the complement. I feel like I'm missing someting small and silly, so if anyone can set me right on this, I'd appreciate it.","\mu(I \setminus C_\alpha) = \alpha \sum_{n=1}^\infty (2\alpha)^n = \frac{\alpha}{1-2\alpha} \alpha \frac{1}{2} [0,1] 3^{-n} < \alpha^n < 2^{-n}","['real-analysis', 'measure-theory', 'cantor-set']"
96,"How to prove that càdlàg (RCLL) functions on $[0,1]$ are bounded?",How to prove that càdlàg (RCLL) functions on  are bounded?,"[0,1]","While studying the space $\mathbb{D}[0,1]$ of right continuous functions with left hand limits (i.e. càdlàg functions) on $[0,1]$, I came across the following theorem: Theorem. If $f$ is càdlàg on $[0,1]$, it is bounded. My proof attempt: I am aware that if a function has both left and right hand limits on $[0,1]$, then the set of discontinuities is at most countable. Hence I tackled this in two parts, one where the discontinuities are finite, the other infinite. I got the finite discontinuity one. But I am stuck at the infinite discontinuity part. My guess is that there is something special about the countable discontinuities (e.g. they cannot be dense in $[0,1]$) and somewhere, I'll have to use the sequential compactness property to get a contradiction, but I am unable to collect my ideas. I request any starting hints on this. A sketch of the proof would also be appreciated.","While studying the space $\mathbb{D}[0,1]$ of right continuous functions with left hand limits (i.e. càdlàg functions) on $[0,1]$, I came across the following theorem: Theorem. If $f$ is càdlàg on $[0,1]$, it is bounded. My proof attempt: I am aware that if a function has both left and right hand limits on $[0,1]$, then the set of discontinuities is at most countable. Hence I tackled this in two parts, one where the discontinuities are finite, the other infinite. I got the finite discontinuity one. But I am stuck at the infinite discontinuity part. My guess is that there is something special about the countable discontinuities (e.g. they cannot be dense in $[0,1]$) and somewhere, I'll have to use the sequential compactness property to get a contradiction, but I am unable to collect my ideas. I request any starting hints on this. A sketch of the proof would also be appreciated.",,"['real-analysis', 'continuity']"
97,Entire function dominated by another entire function is a constant multiple,Entire function dominated by another entire function is a constant multiple,,"These two questions I didn't even find the way to solve So please if you can help Suppose $f (z)$ is entire with $|f(z)| \le |\exp(z)|$ for every $z$ I want to prove that $f(z) = k\exp(z)$  for some $|k| \le 1$ Can a non constant entire function be bounded in half a plane? Prove if yes , example if not.","These two questions I didn't even find the way to solve So please if you can help Suppose $f (z)$ is entire with $|f(z)| \le |\exp(z)|$ for every $z$ I want to prove that $f(z) = k\exp(z)$  for some $|k| \le 1$ Can a non constant entire function be bounded in half a plane? Prove if yes , example if not.",,"['real-analysis', 'complex-analysis', 'analyticity']"
98,Sufficiency to prove the convergence of a sequence using even and odd terms,Sufficiency to prove the convergence of a sequence using even and odd terms,,"Given a sequence $a_{n}$, if I know that the sequence of even terms converges to the same limit as the subsequence of odd terms: $$\lim_{n\rightarrow\infty} a_{2n}=\lim_{n\to\infty} a_{2n-1}=L$$ Is this sufficient to prove that the $\lim_{n\to\infty}a_{n}=L$? If so, how can I make this more rigorous?  Is there a theorem I can state that covers this case?","Given a sequence $a_{n}$, if I know that the sequence of even terms converges to the same limit as the subsequence of odd terms: $$\lim_{n\rightarrow\infty} a_{2n}=\lim_{n\to\infty} a_{2n-1}=L$$ Is this sufficient to prove that the $\lim_{n\to\infty}a_{n}=L$? If so, how can I make this more rigorous?  Is there a theorem I can state that covers this case?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
99,"If $b_n$ is a bounded sequence and $\lim a_n = 0$, show that $\lim(a_nb_n) = 0$","If  is a bounded sequence and , show that",b_n \lim a_n = 0 \lim(a_nb_n) = 0,"This is a real-analysis homework question so I of course have to be very precise and justify anything or any theorem I use. If $b_n$ is a bounded sequence and $\lim(a_n) = 0$, show that $\lim(a_nb_n) = 0$ Intuitively, since $b_n$ is bounded, then sup($b_n$) is some finite number and therefore we can take an $N$ natural number as large as we need such that for all $n\gt N$ $b_na_n$ approaches $0$. At first I thought to use the limit theorems, but since $a_n$ is not bounded, the general limit theorems do not reply. (I am referring to $\lim(X + Y) = \lim X + \lim Y$ for $X,Y$ sequences etc). I was thinking then to use the definition of the limit somehow to show that since $b_n$ is bounded we can take as intuitively stated above $N$ large enough to show the statement is true.  I'm not sure how to proceed with this. Thank you for your replies in advance!","This is a real-analysis homework question so I of course have to be very precise and justify anything or any theorem I use. If $b_n$ is a bounded sequence and $\lim(a_n) = 0$, show that $\lim(a_nb_n) = 0$ Intuitively, since $b_n$ is bounded, then sup($b_n$) is some finite number and therefore we can take an $N$ natural number as large as we need such that for all $n\gt N$ $b_na_n$ approaches $0$. At first I thought to use the limit theorems, but since $a_n$ is not bounded, the general limit theorems do not reply. (I am referring to $\lim(X + Y) = \lim X + \lim Y$ for $X,Y$ sequences etc). I was thinking then to use the definition of the limit somehow to show that since $b_n$ is bounded we can take as intuitively stated above $N$ large enough to show the statement is true.  I'm not sure how to proceed with this. Thank you for your replies in advance!",,"['real-analysis', 'limits']"
