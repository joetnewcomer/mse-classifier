,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding the sum $\frac{x}{x+1} + \frac{2x^2}{x^2+1} + \frac{4x^4}{x^4+1} + \cdots$,Finding the sum,\frac{x}{x+1} + \frac{2x^2}{x^2+1} + \frac{4x^4}{x^4+1} + \cdots,Suppose $|x| < 1$. Can you give any ideas on how to find the following sum? $$ \frac{x}{x+1} + \frac{2x^2}{x^2+1} + \frac{4x^4}{x^4+1} + \frac{8x^8}{x^8+1} + \cdots $$,Suppose $|x| < 1$. Can you give any ideas on how to find the following sum? $$ \frac{x}{x+1} + \frac{2x^2}{x^2+1} + \frac{4x^4}{x^4+1} + \frac{8x^8}{x^8+1} + \cdots $$,,"['calculus', 'sequences-and-series', 'summation', 'power-series']"
1,Strategies For Summing Harmonic Numbers,Strategies For Summing Harmonic Numbers,,"Lately, I have found several interesting problems involving Harmonic numbers such as \begin{equation*}\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n^2}=\frac{7\pi^4}{360}\end{equation*} I am not familiar with computing sums involving Harmonic numbers. Is there a general strategy for tackling such problems? How can this series be evaluated by operating on the generating function for $H_n^{(2)}$?","Lately, I have found several interesting problems involving Harmonic numbers such as \begin{equation*}\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n^2}=\frac{7\pi^4}{360}\end{equation*} I am not familiar with computing sums involving Harmonic numbers. Is there a general strategy for tackling such problems? How can this series be evaluated by operating on the generating function for $H_n^{(2)}$?",,"['calculus', 'sequences-and-series', 'harmonic-numbers']"
2,Problems with limit of factorials: $\;\lim_{n\to\infty}\frac{1!+2!+\ldots+n!}{n!}\;$,Problems with limit of factorials:,\;\lim_{n\to\infty}\frac{1!+2!+\ldots+n!}{n!}\;,"I'm having problems with following limit: $$\lim_{n\to\infty}\frac{1!+2!+\ldots+n!}{n!}$$ This is what I did now: I already proved this sequence is monotone descending, at least for $\;n\ge4\;$ , and since zero , or even one, is a low bound limit exists. Also think limit is $\;1\;$ since $\;n!\;$ is $\;n\;$ times greater than $\;(n-1)!\;$ , so in numerator we can like omit all summands except last one, but of course I need something formalizing more. I don't know of Stirling approximations or integrals, but someone told me that won't help in this. Any help is appreciated.","I'm having problems with following limit: $$\lim_{n\to\infty}\frac{1!+2!+\ldots+n!}{n!}$$ This is what I did now: I already proved this sequence is monotone descending, at least for $\;n\ge4\;$ , and since zero , or even one, is a low bound limit exists. Also think limit is $\;1\;$ since $\;n!\;$ is $\;n\;$ times greater than $\;(n-1)!\;$ , so in numerator we can like omit all summands except last one, but of course I need something formalizing more. I don't know of Stirling approximations or integrals, but someone told me that won't help in this. Any help is appreciated.",,['calculus']
3,Evaluate $\int_0^\infty \frac{\arctan(3x) - \arctan(9x)}{x} {dx}$,Evaluate,\int_0^\infty \frac{\arctan(3x) - \arctan(9x)}{x} {dx},Evaluate the integral $$\int_0^\infty \frac{\arctan(3x) - \arctan(9x)}{x} {dx}.$$ I tried to split this into 2 integrals and then using the substitution $t = \arctan(3x)$ but I got nowhere.,Evaluate the integral $$\int_0^\infty \frac{\arctan(3x) - \arctan(9x)}{x} {dx}.$$ I tried to split this into 2 integrals and then using the substitution $t = \arctan(3x)$ but I got nowhere.,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
4,The antiderivative of $\sin(1/x)$,The antiderivative of,\sin(1/x),"How to prove that the function $f(x)=\sin\frac{1}{x}$ for $x\neq 0,f(0)=0$ has an antiderivative? This means $F(x)=\int^{x}_{0}\sin(1/t)dt$ has derivative $0$ at $x=0$, but I have no idea how to prove it.","How to prove that the function $f(x)=\sin\frac{1}{x}$ for $x\neq 0,f(0)=0$ has an antiderivative? This means $F(x)=\int^{x}_{0}\sin(1/t)dt$ has derivative $0$ at $x=0$, but I have no idea how to prove it.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
5,"Evaluate $\int \sqrt{ \frac {\sin(x-\alpha)} {\sin(x+\alpha)} }\,\operatorname d\!x$?",Evaluate ?,"\int \sqrt{ \frac {\sin(x-\alpha)} {\sin(x+\alpha)} }\,\operatorname d\!x","How to go about evaluating the following integral? $$ I = \int \sqrt{ \dfrac {\sin(x-\alpha)} {\sin(x+\alpha)} }\,\operatorname d\!x$$ What I have done so far: $$ I = \int \sqrt{ 1-\tan\alpha\cdot\cot x }\,\operatorname d\!x$$ Let $ t^2 = 1-\tan\alpha\cdot\cot x $ $$ \begin{align}   2t\,\operatorname d\!t &= \tan\alpha \cdot \csc^2x\,\operatorname d\!x \\  & = \tan\alpha \cdot \Bigg(1 + \Big(\dfrac{1-t^2}{\tan\alpha}\Big)^2\Bigg)\,\operatorname d\!x \\  & = \dfrac{\Big(\tan^2 \alpha + (1-t^2)^2\Big)}{\tan \alpha}dx \end{align}$$ So, from that: $$\begin{align}  I &= \int \sqrt{ 1-\tan\alpha\cdot\cot x }\,dx \\  & = \int \dfrac{2t^2\tan\alpha}{\Big(\tan^2 \alpha + (1-t^2)^2\Big)}\, \operatorname d\!t \\ \end{align}$$ What to do next? Edit : I had thought of doing a substitution: $u = 1-t^2$ but that doesn't work as you need one more $t$ term in the numerator.","How to go about evaluating the following integral? $$ I = \int \sqrt{ \dfrac {\sin(x-\alpha)} {\sin(x+\alpha)} }\,\operatorname d\!x$$ What I have done so far: $$ I = \int \sqrt{ 1-\tan\alpha\cdot\cot x }\,\operatorname d\!x$$ Let $ t^2 = 1-\tan\alpha\cdot\cot x $ $$ \begin{align}   2t\,\operatorname d\!t &= \tan\alpha \cdot \csc^2x\,\operatorname d\!x \\  & = \tan\alpha \cdot \Bigg(1 + \Big(\dfrac{1-t^2}{\tan\alpha}\Big)^2\Bigg)\,\operatorname d\!x \\  & = \dfrac{\Big(\tan^2 \alpha + (1-t^2)^2\Big)}{\tan \alpha}dx \end{align}$$ So, from that: $$\begin{align}  I &= \int \sqrt{ 1-\tan\alpha\cdot\cot x }\,dx \\  & = \int \dfrac{2t^2\tan\alpha}{\Big(\tan^2 \alpha + (1-t^2)^2\Big)}\, \operatorname d\!t \\ \end{align}$$ What to do next? Edit : I had thought of doing a substitution: $u = 1-t^2$ but that doesn't work as you need one more $t$ term in the numerator.",,"['calculus', 'integration', 'indefinite-integrals']"
6,"From a mathematician's point of view, what is the purpose of '$dx$' in $\int f(x)\ dx$?","From a mathematician's point of view, what is the purpose of '' in ?",dx \int f(x)\ dx,"I've done a bit of searching and found a fairly well written explanation, but at the end, the author noted that this explanation seems to work fine for a physicist's purposes - but a mathematician would groan at it due to oversimplifications or inaccuracies. Since I first posted this paper, two different people have emailed me   to tell me that Real Mathematicians don't do this. Playing with dx in   the ways described in this paper is apparently one of those smarmy   tricks that physicists use to give headaches to mathematicians. http://www4.ncsu.edu/unity/lockers/users/f/felder/public/kenny/papers/dx.html I was also confused when reading it because my Calculus prof last semester said that the chain rule $$\frac{dy}{dz}=\frac{dy}{dx}\cdot \frac{dx}{dz}$$ cannot be treated as fractions, despite the fact that they look like it, and the $dx$ would not cancel out between the two since you can't do that with differentials. But this article said just the opposite.","I've done a bit of searching and found a fairly well written explanation, but at the end, the author noted that this explanation seems to work fine for a physicist's purposes - but a mathematician would groan at it due to oversimplifications or inaccuracies. Since I first posted this paper, two different people have emailed me   to tell me that Real Mathematicians don't do this. Playing with dx in   the ways described in this paper is apparently one of those smarmy   tricks that physicists use to give headaches to mathematicians. http://www4.ncsu.edu/unity/lockers/users/f/felder/public/kenny/papers/dx.html I was also confused when reading it because my Calculus prof last semester said that the chain rule $$\frac{dy}{dz}=\frac{dy}{dx}\cdot \frac{dx}{dz}$$ cannot be treated as fractions, despite the fact that they look like it, and the $dx$ would not cancel out between the two since you can't do that with differentials. But this article said just the opposite.",,"['calculus', 'notation']"
7,Definite integral with a strange power,Definite integral with a strange power,,"I've been struggling for a while now on evaluating this disgusting integral: $$(\ln2)\int_0^{(\ln2)^{1/\ln2}}2^{\ln x}\cdot x^\left(\frac{x^{\ln2}+1}{\ln x}-1\right)dx$$ My maths teacher gave our class this question a while ago, and he said that we should be able to do it (I am in high-school, and we have only been taught a fairly basic level of integration). So today I spent many hours applying every integration technique I know to this monster, but I got absolutely nowhere. It got to a point where I couldn't think of another variable to use as a substitution because I had already made so many. I eventually decided to plug this into an integral calculator and received a surprisingly nice result of $e$ , however there was no further information and so I was not able to view any of the steps in how they got there. I am so stuck on this problem :( Does anyone know how they got there? What are the steps in finding its indefinite form?","I've been struggling for a while now on evaluating this disgusting integral: My maths teacher gave our class this question a while ago, and he said that we should be able to do it (I am in high-school, and we have only been taught a fairly basic level of integration). So today I spent many hours applying every integration technique I know to this monster, but I got absolutely nowhere. It got to a point where I couldn't think of another variable to use as a substitution because I had already made so many. I eventually decided to plug this into an integral calculator and received a surprisingly nice result of , however there was no further information and so I was not able to view any of the steps in how they got there. I am so stuck on this problem :( Does anyone know how they got there? What are the steps in finding its indefinite form?",(\ln2)\int_0^{(\ln2)^{1/\ln2}}2^{\ln x}\cdot x^\left(\frac{x^{\ln2}+1}{\ln x}-1\right)dx e,"['calculus', 'integration', 'definite-integrals', 'exponentiation']"
8,Why ignoring the function $f(x)=1/(1+x^2)$?,Why ignoring the function ?,f(x)=1/(1+x^2),"I heard that the function $$f(x)=e^{-x^2}$$ Is extremely important in probability and statistics, because it looks like the normal distribution (or something like that). But i noticed that the graph of this function is similar to the function $$g(x)=\frac{1}{1+x^2}$$ Furthermore, the area under this curve is pretty easy to calculate, since $$\int_{0}^z \frac{\,dx}{1+x^2}=\arctan(z).$$ So why the first function is more important than the second one, although they look pretty similar?","I heard that the function Is extremely important in probability and statistics, because it looks like the normal distribution (or something like that). But i noticed that the graph of this function is similar to the function Furthermore, the area under this curve is pretty easy to calculate, since So why the first function is more important than the second one, although they look pretty similar?","f(x)=e^{-x^2} g(x)=\frac{1}{1+x^2} \int_{0}^z \frac{\,dx}{1+x^2}=\arctan(z).","['calculus', 'probability']"
9,"What is the dot in ""$1.2.4$""?","What is the dot in """"?",1.2.4,"I am not a mathematician.  I did additional maths O’level back in the stone age but did not pursue maths further (much to my regret). I am reading David Acheson’s fascinating book ‘The Story of Calculus’ and have just about kept up till I got a use of ‘ $\cdot$ ' (dot) that I do not understand.  It is in his Chapter $14$ ‘an Enigma’ and first occurs here in the context of chain rule :- Suppose, for instance, that $y$ is some function of $x$ , and that $x$ itself is a function of some other variable - say $t$ .  Then we can, if we wish, consider $y$ as a function of $t$ , and then $\frac{dy}{dt}=\frac{dy}{dx}\cdot \frac{dx}{dt}$ What is the dot doing?  I looked at the suggested previous questions about the dot without success.  Does it mean $\&$ (as it does in propositional logic, where $P.Q$ stands for $P \& Q$ ? The (or a ) mysterious dot corps up again in Chapter $23$ , about $e$ numbers, on the topic of the Taylor series .   Here we find the series $$e^x=1+x+\frac{x^2}{1.2}+\frac{x^3}{1.2.3}+...$$ What is the ' $.$ ' doing here, please?  Is it in some way a concatenation?  Or what is it?","I am not a mathematician.  I did additional maths O’level back in the stone age but did not pursue maths further (much to my regret). I am reading David Acheson’s fascinating book ‘The Story of Calculus’ and have just about kept up till I got a use of ‘ ' (dot) that I do not understand.  It is in his Chapter ‘an Enigma’ and first occurs here in the context of chain rule :- Suppose, for instance, that is some function of , and that itself is a function of some other variable - say .  Then we can, if we wish, consider as a function of , and then What is the dot doing?  I looked at the suggested previous questions about the dot without success.  Does it mean (as it does in propositional logic, where stands for ? The (or a ) mysterious dot corps up again in Chapter , about numbers, on the topic of the Taylor series .   Here we find the series What is the ' ' doing here, please?  Is it in some way a concatenation?  Or what is it?",\cdot 14 y x x t y t \frac{dy}{dt}=\frac{dy}{dx}\cdot \frac{dx}{dt} \& P.Q P \& Q 23 e e^x=1+x+\frac{x^2}{1.2}+\frac{x^3}{1.2.3}+... .,"['calculus', 'sequences-and-series', 'limits', 'notation']"
10,How to integrate the dilogarithms?,How to integrate the dilogarithms?,,"$\def\Li{\operatorname{Li}}$ How can you integrate $\Li_2$? I tried from $0 \to 1$ $\displaystyle \int_{0}^{1} \Li_2(z) \,dz = \sum_{n=1}^{\infty} \frac{1}{n^2(n+1)}$ $$\frac{An + B}{n^2} + \frac{D}{n+1} = \frac{1}{n^2(n+1)}$$ $$(An + B)(n+1) + D(n^2) = 1$$ Let $n = -1, \implies D = 1$ Let $n = 0, \implies B = 1$ Let $n = 1, \implies A = -1$ $$\frac{-n + 1}{n^2} + \frac{1}{n+1} = \frac{1}{n^2(n+1)}$$ $$= \sum_{n=1}^{\infty} \frac{-n + 1}{n^2} + \frac{1}{n+1} = \sum_{n=1}^{\infty} \frac{1}{n^2(n+1)} = \sum_{n=1}^{\infty} \frac{1}{n^2} - \frac{1}{n} + \frac{1}{n+1} $$ The $1/n$ is the problem, it is the harmonic series, which diverges.","$\def\Li{\operatorname{Li}}$ How can you integrate $\Li_2$? I tried from $0 \to 1$ $\displaystyle \int_{0}^{1} \Li_2(z) \,dz = \sum_{n=1}^{\infty} \frac{1}{n^2(n+1)}$ $$\frac{An + B}{n^2} + \frac{D}{n+1} = \frac{1}{n^2(n+1)}$$ $$(An + B)(n+1) + D(n^2) = 1$$ Let $n = -1, \implies D = 1$ Let $n = 0, \implies B = 1$ Let $n = 1, \implies A = -1$ $$\frac{-n + 1}{n^2} + \frac{1}{n+1} = \frac{1}{n^2(n+1)}$$ $$= \sum_{n=1}^{\infty} \frac{-n + 1}{n^2} + \frac{1}{n+1} = \sum_{n=1}^{\infty} \frac{1}{n^2(n+1)} = \sum_{n=1}^{\infty} \frac{1}{n^2} - \frac{1}{n} + \frac{1}{n+1} $$ The $1/n$ is the problem, it is the harmonic series, which diverges.",,"['calculus', 'integration', 'sequences-and-series', 'special-functions', 'polylogarithm']"
11,Derivative of exponential function proof,Derivative of exponential function proof,,"I'm looking for a straight forward proof using the definition of a derivative applied to the exponential function and substitution of one of the limit definitions of $e$, starting with $e = \lim_{h\to \infty}\left({1+\dfrac{1}{h}}\right)^h$ or $e=\sum_{h=0}^{\infty}{\dfrac{1}{h!}}$ and $\dfrac{d}{dx}\left( e^x \right) = \lim_{h\to 0}\left({\dfrac{e^{x+h}-e^{x}}{h}}\right)$ I found a proof I sort of liked here (which is sort of along the lines of a proof I'd like to use): http://www.math.brown.edu/UTRA/explog.html My only problem is that he combines the dummy variable, $h$, for the limit definition of $e$ and the dummy variable, $h$, used for the derivative. To me, it seems like it's not quite valid to do such a thing because it assumes both values are equal. Can anyone provide a better proof or justification for why the dummy variables can be combined? EDIT: I guess I'd also like to have a proof of why: $\lim_{h\to 0}\left({\dfrac{e^{h}-1}{h}}\right) = 1$ using one of the limit definitions of $e$ shown above.","I'm looking for a straight forward proof using the definition of a derivative applied to the exponential function and substitution of one of the limit definitions of $e$, starting with $e = \lim_{h\to \infty}\left({1+\dfrac{1}{h}}\right)^h$ or $e=\sum_{h=0}^{\infty}{\dfrac{1}{h!}}$ and $\dfrac{d}{dx}\left( e^x \right) = \lim_{h\to 0}\left({\dfrac{e^{x+h}-e^{x}}{h}}\right)$ I found a proof I sort of liked here (which is sort of along the lines of a proof I'd like to use): http://www.math.brown.edu/UTRA/explog.html My only problem is that he combines the dummy variable, $h$, for the limit definition of $e$ and the dummy variable, $h$, used for the derivative. To me, it seems like it's not quite valid to do such a thing because it assumes both values are equal. Can anyone provide a better proof or justification for why the dummy variables can be combined? EDIT: I guess I'd also like to have a proof of why: $\lim_{h\to 0}\left({\dfrac{e^{h}-1}{h}}\right) = 1$ using one of the limit definitions of $e$ shown above.",,"['calculus', 'sequences-and-series', 'limits', 'derivatives', 'exponential-function']"
12,Applications of Residue Theorem in complex analysis?,Applications of Residue Theorem in complex analysis?,,"Does anyone know the applications of Residue Theorem in complex analysis? I would like to do a quick paper on the matter, but am not sure where to start. The residue theorem The residue theorem, sometimes called Cauchy's residue theorem (one of many things named after Augustin-Louis Cauchy), is a powerful tool to evaluate line integrals of analytic functions over closed curves; it can often be used to compute real integrals as well. It generalizes the Cauchy integral theorem and Cauchy's integral formula. From a geometrical perspective, it is a special case of the generalized Stokes' theorem. Illustration of the setting The statement is as follows:   Suppose $U$ is a simply connected open subset of the complex plane, and $a_1,\ldots,a_n$ are finitely many points of $U$ and $f$ is a function which is defined and holomorphic on $U\setminus\{a_1,\ldots,a_n\}$. If $\gamma$ is a rectifiable curve in $U$ which does not meet any of the $a_k$, and whose start point equals its endpoint, then    $$\oint_\gamma f(z)\,dz=2\pi i\sum_{k=1}^n I(\gamma,a_k)\mathrm{Res}(f,a_k)$$ I'm sure many complex analysis experts are very familiar with this theorem. I was just hoping someone could enlighten me on its many applications for my paper. Thank you!","Does anyone know the applications of Residue Theorem in complex analysis? I would like to do a quick paper on the matter, but am not sure where to start. The residue theorem The residue theorem, sometimes called Cauchy's residue theorem (one of many things named after Augustin-Louis Cauchy), is a powerful tool to evaluate line integrals of analytic functions over closed curves; it can often be used to compute real integrals as well. It generalizes the Cauchy integral theorem and Cauchy's integral formula. From a geometrical perspective, it is a special case of the generalized Stokes' theorem. Illustration of the setting The statement is as follows:   Suppose $U$ is a simply connected open subset of the complex plane, and $a_1,\ldots,a_n$ are finitely many points of $U$ and $f$ is a function which is defined and holomorphic on $U\setminus\{a_1,\ldots,a_n\}$. If $\gamma$ is a rectifiable curve in $U$ which does not meet any of the $a_k$, and whose start point equals its endpoint, then    $$\oint_\gamma f(z)\,dz=2\pi i\sum_{k=1}^n I(\gamma,a_k)\mathrm{Res}(f,a_k)$$ I'm sure many complex analysis experts are very familiar with this theorem. I was just hoping someone could enlighten me on its many applications for my paper. Thank you!",,"['calculus', 'complex-analysis', 'residue-calculus']"
13,Show that $|\sqrt{x}-\sqrt{y}| \le \sqrt{|x-y|}$,Show that,|\sqrt{x}-\sqrt{y}| \le \sqrt{|x-y|},"In a solution for a test, I came upon the following: we now use $|\sqrt{x}-\sqrt{y}| \le \sqrt{|x-y|}$ (prove) . I've been unable to solve this - I've looked at the proof of the triangle inequality, but I haven't been able to apply the same concepts here. I'd appreciate any help.","In a solution for a test, I came upon the following: we now use $|\sqrt{x}-\sqrt{y}| \le \sqrt{|x-y|}$ (prove) . I've been unable to solve this - I've looked at the proof of the triangle inequality, but I haven't been able to apply the same concepts here. I'd appreciate any help.",,"['calculus', 'inequality']"
14,Proof of an estimate for the tail of a normal distribution,Proof of an estimate for the tail of a normal distribution,,My advisor told me to look up the proof of the following standard estimate so that we can adapt it to the case where we are dealing with something similar but including the addition of a polynomial integrand. I have yet to find a reference containing the proof and was wondering what the reference was or if someone knew a quick way to prove the following estimate: How do you show  $\exists c > 0$ so that $\int_u^\infty \exp(-z^2)\;\mathrm dz < \frac{c}{u} \exp{(-u^2)}$?,My advisor told me to look up the proof of the following standard estimate so that we can adapt it to the case where we are dealing with something similar but including the addition of a polynomial integrand. I have yet to find a reference containing the proof and was wondering what the reference was or if someone knew a quick way to prove the following estimate: How do you show  $\exists c > 0$ so that $\int_u^\infty \exp(-z^2)\;\mathrm dz < \frac{c}{u} \exp{(-u^2)}$?,,"['calculus', 'probability', 'faq']"
15,"Volume of a pyramid, using an integral","Volume of a pyramid, using an integral",,"I have a calculus exam tomorrow and this is a possible question. However, I don't know how to handle this question. Suppose you have 3 points in space: $p_1=(a,0,0)$, $p_2=(0,b,0)$ and $p_3=(0,0,c)$, $a,b,c \gt 0$. If we connect these points we get a pyramid in the first octant, with the origin as a top. (i) Proove that the volume of this pyramid is given by $V = \frac{1}{6}abc$ by using a volume integral. Use the formula we made for non-revolution-solids. (Integrate $A(x)$ from $a$ to $b$, where $A(x)$ is the area of the intersection of the solid with the plane perpendicular to the $x$-axis in $x$. (Maybe $A(y)$ is better suitable in this problem.)) Hint: Calculate the surface of a slice at height $z = z_0$, with $z_0$ is a constant. (Maybe I should find $A(z)$?) (ii) If you know the equation of the plane $V$ through these points is given by $V \leftrightarrow \frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, and that $p=(1,2,3)$ is an element of $V$, find the minimal volume of the pyramid cut off from the first octant. Explain physically why this has to be a minimum. Notes: Excuse my English, it's not my native language. The original question is in Dutch. We use the textbook Calculus 6E, metrical edition, by James Stewart. Which is fortunately in English, so I should understand most of your answers. Thanks a lot already!","I have a calculus exam tomorrow and this is a possible question. However, I don't know how to handle this question. Suppose you have 3 points in space: $p_1=(a,0,0)$, $p_2=(0,b,0)$ and $p_3=(0,0,c)$, $a,b,c \gt 0$. If we connect these points we get a pyramid in the first octant, with the origin as a top. (i) Proove that the volume of this pyramid is given by $V = \frac{1}{6}abc$ by using a volume integral. Use the formula we made for non-revolution-solids. (Integrate $A(x)$ from $a$ to $b$, where $A(x)$ is the area of the intersection of the solid with the plane perpendicular to the $x$-axis in $x$. (Maybe $A(y)$ is better suitable in this problem.)) Hint: Calculate the surface of a slice at height $z = z_0$, with $z_0$ is a constant. (Maybe I should find $A(z)$?) (ii) If you know the equation of the plane $V$ through these points is given by $V \leftrightarrow \frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$, and that $p=(1,2,3)$ is an element of $V$, find the minimal volume of the pyramid cut off from the first octant. Explain physically why this has to be a minimum. Notes: Excuse my English, it's not my native language. The original question is in Dutch. We use the textbook Calculus 6E, metrical edition, by James Stewart. Which is fortunately in English, so I should understand most of your answers. Thanks a lot already!",,"['calculus', 'integration', 'definite-integrals', 'solid-geometry']"
16,Testing continuity of the function $f(x) = \lim\limits_{n \to \infty} \frac{x}{(2\sin{x})^{2n}+1} \ \text{for} \ x \in \mathbb{R}$,Testing continuity of the function,f(x) = \lim\limits_{n \to \infty} \frac{x}{(2\sin{x})^{2n}+1} \ \text{for} \ x \in \mathbb{R},"My Question is: Examine the continuity of $$f(x) = \lim_{n \to \infty} \frac{x}{(2\sin{x})^{2n}+1} \qquad \text{for} \ x \in \mathbb{R}$$ How can I do this? Honestly, speaking I have $\text{no idea}$ of doing, this as this seems to be a tough limit. What I only know is that as $n \to \infty$ $\sin{n}$ $\text{oscillates}$ between $-1$ and $1$.","My Question is: Examine the continuity of $$f(x) = \lim_{n \to \infty} \frac{x}{(2\sin{x})^{2n}+1} \qquad \text{for} \ x \in \mathbb{R}$$ How can I do this? Honestly, speaking I have $\text{no idea}$ of doing, this as this seems to be a tough limit. What I only know is that as $n \to \infty$ $\sin{n}$ $\text{oscillates}$ between $-1$ and $1$.",,['calculus']
17,How to find $\int_0^{\infty}\frac{dx}{(1+x^2)^4}$,How to find,\int_0^{\infty}\frac{dx}{(1+x^2)^4},How would you compute for the definite integral of $$\int_0^{\infty}\frac{dx}{(1+x^2)^4}$$ I know that integral of $\displaystyle \frac1{(1+x^2)}$ equals $\tan^{-1}x$. I tried using integration by parts without much luck.  My teacher pointed me to special functions by which I found out about the hypergeometric distribution. Although I don't know how to apply it to this problem. Anybody know how to use special functions or how to go about this problem?,How would you compute for the definite integral of $$\int_0^{\infty}\frac{dx}{(1+x^2)^4}$$ I know that integral of $\displaystyle \frac1{(1+x^2)}$ equals $\tan^{-1}x$. I tried using integration by parts without much luck.  My teacher pointed me to special functions by which I found out about the hypergeometric distribution. Although I don't know how to apply it to this problem. Anybody know how to use special functions or how to go about this problem?,,['calculus']
18,How to integrate $ \int\frac{x-2}{(7x^2-36x+48)\sqrt{x^2-2x-1}}dx$?,How to integrate ?, \int\frac{x-2}{(7x^2-36x+48)\sqrt{x^2-2x-1}}dx,How to integrate $$ I=\int\frac{x-2}{\left(7x^2-36x+48\right)\sqrt{x^2-2x-1}}dx$$ The given answer is $$ I=-\dfrac{1}{\sqrt{33}}\cdot \tan^{-1}\left(\frac{\sqrt{3x^2-6x-3}}{\sqrt{11}\cdot (x-3)}\right)+\mathcal{C}$$ I tried by different substitutions i.e $$\dfrac{x^2 - 2x -1}{x-3} = t$$ but I am not getting my desired answer.,How to integrate The given answer is I tried by different substitutions i.e but I am not getting my desired answer., I=\int\frac{x-2}{\left(7x^2-36x+48\right)\sqrt{x^2-2x-1}}dx  I=-\dfrac{1}{\sqrt{33}}\cdot \tan^{-1}\left(\frac{\sqrt{3x^2-6x-3}}{\sqrt{11}\cdot (x-3)}\right)+\mathcal{C} \dfrac{x^2 - 2x -1}{x-3} = t,"['calculus', 'integration', 'indefinite-integrals']"
19,Why isn't Fourier Series taught in calculus 2? [closed],Why isn't Fourier Series taught in calculus 2? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question I am self taught and have read the book ""Essential Calculus ETF"" by Larson and Hosteller from cover to cover and have since been evaluating the more difficult problems in calculus. During my venture in solving difficult integrals using series expansions via the monotone convergence theorem, I noticed that some integrals were tackled easily with the Fourier series. Now, why isn't this topic included in the same sections where power, taylor and maclaurin series are being discussed? Not that I'm complaining (though I favor maclaurin and this is my hobby, so I'm learning something new), but shouldn't this series also be included in Calculus 2?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question I am self taught and have read the book ""Essential Calculus ETF"" by Larson and Hosteller from cover to cover and have since been evaluating the more difficult problems in calculus. During my venture in solving difficult integrals using series expansions via the monotone convergence theorem, I noticed that some integrals were tackled easily with the Fourier series. Now, why isn't this topic included in the same sections where power, taylor and maclaurin series are being discussed? Not that I'm complaining (though I favor maclaurin and this is my hobby, so I'm learning something new), but shouldn't this series also be included in Calculus 2?",,"['calculus', 'fourier-series', 'self-learning']"
20,"True or false: If $f(x)$ is continuous on $[0, 2]$ and $f(0)=f(2)$, then there exists a number $c\in [0, 1]$ such that $f(c) = f(c + 1)$.","True or false: If  is continuous on  and , then there exists a number  such that .","f(x) [0, 2] f(0)=f(2) c\in [0, 1] f(c) = f(c + 1)","I was solving past exams of calculus course and I've encounter with a problem, which I couldn't figure out how to solve.The question is following; Prove that whether the given statement is true or false: Suppose that $f(x)$ is continuous on $[0, 2]$ and $f(0)=f(2)$. Then there exists a number $c\in [0, 1]$ such that $f(c) = f(c + 1)$. By just reading the question, you want to say ""use Mean Value Theorem or Rolle's theorem"" but we don't know whether $f(x)$ is differentiable on the interval, at least it is now given, so I am totally stuck.I would appreciate any help.","I was solving past exams of calculus course and I've encounter with a problem, which I couldn't figure out how to solve.The question is following; Prove that whether the given statement is true or false: Suppose that $f(x)$ is continuous on $[0, 2]$ and $f(0)=f(2)$. Then there exists a number $c\in [0, 1]$ such that $f(c) = f(c + 1)$. By just reading the question, you want to say ""use Mean Value Theorem or Rolle's theorem"" but we don't know whether $f(x)$ is differentiable on the interval, at least it is now given, so I am totally stuck.I would appreciate any help.",,['calculus']
21,Find $\int_0^1\frac{\ln^2(1-x)}{x}\ dx$,Find,\int_0^1\frac{\ln^2(1-x)}{x}\ dx,"In solving $\displaystyle\int_0^\frac{\pi}{4}\dfrac{\ln(\sin x)\ln(\cos x)}{\sin x\cos x}\ dx,$ I have found that this is equal to $\dfrac{1}{16}\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx.$ WolframAlpha says that the desired value is $\dfrac{\zeta(3)}{8},$ so I suspect a conversion to a series is necessary. How do I prove $\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx=\displaystyle\sum_{n=1}^\infty\dfrac{2}{n^3}$? Note that the above integral can also be given as $\displaystyle\int_0^1\dfrac{\ln^2x}{1-x}\ dx$, which I know is equal to $\displaystyle\sum_{n=0}^\infty x^n\ln^2x.$ Also for reference, here is a picture of my original work to get to this point.","In solving $\displaystyle\int_0^\frac{\pi}{4}\dfrac{\ln(\sin x)\ln(\cos x)}{\sin x\cos x}\ dx,$ I have found that this is equal to $\dfrac{1}{16}\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx.$ WolframAlpha says that the desired value is $\dfrac{\zeta(3)}{8},$ so I suspect a conversion to a series is necessary. How do I prove $\displaystyle\int_0^1\dfrac{\ln^2(1-x)}{x}\ dx=\displaystyle\sum_{n=1}^\infty\dfrac{2}{n^3}$? Note that the above integral can also be given as $\displaystyle\int_0^1\dfrac{\ln^2x}{1-x}\ dx$, which I know is equal to $\displaystyle\sum_{n=0}^\infty x^n\ln^2x.$ Also for reference, here is a picture of my original work to get to this point.",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
22,100th derivative of $e^{-x^2}$ at point $0$,100th derivative of  at point,e^{-x^2} 0,Problem:  Find $\frac{\mathrm d^{100}}{\mathrm dx^{100}}e^{-x^2}$ at point $0$. My attempt: $y'=-2xe^{-x^2}$ I tried to use General Leibniz rule and I didn't get much better information. Without: Taylor series,Problem:  Find $\frac{\mathrm d^{100}}{\mathrm dx^{100}}e^{-x^2}$ at point $0$. My attempt: $y'=-2xe^{-x^2}$ I tried to use General Leibniz rule and I didn't get much better information. Without: Taylor series,,"['calculus', 'derivatives']"
23,Show that $(1+\frac{1}{n})^n+\frac{1}{n}$ is eventually increasing,Show that  is eventually increasing,(1+\frac{1}{n})^n+\frac{1}{n},"I would like to find a  way to show that the sequence $a_n=\big(1+\frac{1}{n}\big)^n+\frac{1}{n}$ is eventually increasing. $\hspace{.3 in}$(Numerical evidence suggests that $a_n<a_{n+1}$ for $n\ge6$.) I was led to this problem by trying to prove by induction that $\big(1+\frac{1}{n}\big)^n\le3-\frac{1}{n}$, as in $\hspace{.4 in}$ A simple proof that $\bigl(1+\frac1n\bigr)^n\leq3-\frac1n$?","I would like to find a  way to show that the sequence $a_n=\big(1+\frac{1}{n}\big)^n+\frac{1}{n}$ is eventually increasing. $\hspace{.3 in}$(Numerical evidence suggests that $a_n<a_{n+1}$ for $n\ge6$.) I was led to this problem by trying to prove by induction that $\big(1+\frac{1}{n}\big)^n\le3-\frac{1}{n}$, as in $\hspace{.4 in}$ A simple proof that $\bigl(1+\frac1n\bigr)^n\leq3-\frac1n$?",,"['calculus', 'sequences-and-series']"
24,Using differentiation to solve equations [duplicate],Using differentiation to solve equations [duplicate],,"This question already has answers here : Differentiating both sides of a non-differential equation (3 answers) Closed 8 years ago . Lets say that I have an equation that can't really be solved via elementary means, for e.g: $$ e^x = 4x$$ Logically, what is wrong with me using equating derivatives (or integrals for that matter)? For e.g: $$ \dfrac{d}{dx} (e^x) = \dfrac{d}{dx} (4x) $$ or $$ \int{e^x}{dx} = \int{4x}{dx}  $$","This question already has answers here : Differentiating both sides of a non-differential equation (3 answers) Closed 8 years ago . Lets say that I have an equation that can't really be solved via elementary means, for e.g: $$ e^x = 4x$$ Logically, what is wrong with me using equating derivatives (or integrals for that matter)? For e.g: $$ \dfrac{d}{dx} (e^x) = \dfrac{d}{dx} (4x) $$ or $$ \int{e^x}{dx} = \int{4x}{dx}  $$",,['calculus']
25,"For which values of $a$, $b$ and $c$, if $a + b = c$, then $\frac{1}{a} + \frac{1}{b} = \frac{1}{c}$?","For which values of ,  and , if , then ?",a b c a + b = c \frac{1}{a} + \frac{1}{b} = \frac{1}{c},"I have a problem in my homework, which I have tried to solve, but I have just ideas, no real mathematical solutions. The problem is the following: Suppose we have three real numbers $a$, $b$, and $c$ which satisfy the   equation: $$a + b = c$$ Is it then also true that: $$\frac{1}{a} + \frac{1}{b} = \frac{1}{c}$$ or not? Or is it only true for some particular choice of $a$, $b$, and   $c$, and which would that be? My ideas: I noticed immediately that all $a$, $b$ and $c$ must be different from $0$, because otherwise we would have $\frac{1}{0}$ in the second equation, and that's not defined, as everybody knows. I tried to form a system of equations with the equations given in the specification of the problem: $$\begin{cases} a + b = c \\ \frac{1}{a} + \frac{1}{b} = \frac{1}{c} \end{cases}$$ Since we have 3 variables ($a$, $b$ and $c$), I am not sure if this system of equations can bring me to some solution. I have tried to replace $a + b$ in the second equation: $$b(a + b) + a(a + b) = ab$$ $$ba + b^2 + a^2 + ba = ab$$ We can simplify to: $$ba + b^2 + a^2 = 0$$ Now, I would not know how to proceed, and sincerely I don't know if my solution (ideas) is correct or not, or how far is it from the real solution. My guess is that there's no values for $a$, $b$ or $c$ such that the 2 equations are valid.","I have a problem in my homework, which I have tried to solve, but I have just ideas, no real mathematical solutions. The problem is the following: Suppose we have three real numbers $a$, $b$, and $c$ which satisfy the   equation: $$a + b = c$$ Is it then also true that: $$\frac{1}{a} + \frac{1}{b} = \frac{1}{c}$$ or not? Or is it only true for some particular choice of $a$, $b$, and   $c$, and which would that be? My ideas: I noticed immediately that all $a$, $b$ and $c$ must be different from $0$, because otherwise we would have $\frac{1}{0}$ in the second equation, and that's not defined, as everybody knows. I tried to form a system of equations with the equations given in the specification of the problem: $$\begin{cases} a + b = c \\ \frac{1}{a} + \frac{1}{b} = \frac{1}{c} \end{cases}$$ Since we have 3 variables ($a$, $b$ and $c$), I am not sure if this system of equations can bring me to some solution. I have tried to replace $a + b$ in the second equation: $$b(a + b) + a(a + b) = ab$$ $$ba + b^2 + a^2 + ba = ab$$ We can simplify to: $$ba + b^2 + a^2 = 0$$ Now, I would not know how to proceed, and sincerely I don't know if my solution (ideas) is correct or not, or how far is it from the real solution. My guess is that there's no values for $a$, $b$ or $c$ such that the 2 equations are valid.",,[]
26,Proving $\left(\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}}\right)\left(\sqrt{x-\sqrt{x-\sqrt{x-\sqrt{x+\cdots}}}}\right)=x$,Proving,\left(\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}}\right)\left(\sqrt{x-\sqrt{x-\sqrt{x-\sqrt{x+\cdots}}}}\right)=x,How can I prove this equality? $$\left(\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}}\right)\left(\sqrt{x-\sqrt{x-\sqrt{x-\sqrt{x+\cdots}}}}\right)=x$$,How can I prove this equality?,\left(\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\cdots}}}}\right)\left(\sqrt{x-\sqrt{x-\sqrt{x-\sqrt{x+\cdots}}}}\right)=x,"['calculus', 'sequences-and-series', 'limits', 'radicals', 'nested-radicals']"
27,"Evaluate $\int\frac{1}{1+x^6} \,dx$",Evaluate,"\int\frac{1}{1+x^6} \,dx","I came across following problem Evaluate $$\int\frac{1}{1+x^6} \,dx$$ When I asked my teacher for hint he said first evaluate $$\int\frac{1}{1+x^4} \,dx$$ I've tried to factorize $1+x^6$ as $$1+x^6=(x^2 + 1)(x^4 - x^2 + 1)$$ and then writing $$I=\int\frac{1}{1+x^6} \,dx=\int\frac{1}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx=\int\frac{1+x^2-x^2}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx$$ $$I=\int\frac{1}{x^4 - x^2 + 1} \,dx-\int\frac{x^2}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx$$ However $$x^4-x^2+1=\left(x^2-\frac12\right)^2+\frac{3}{4}$$ But I can't see how it helps I've also tried to reverse engineer the solution given by Wolfram Alpha And I need to have terms similar to $$\frac{x^2-1}{x^4-x^2+1} \quad , \quad \frac{1}{1+x^2} \quad , \quad \frac{1}{(x+c)^2+1}\quad , \quad \frac{1}{(x+c)^2+1}$$ in integrand, How can I transform my cute looking integrand into these huge terms? Since in exams I will neither have access to WA nor time to reverse engineer the solution moreover it does not seem intuitive,is there any way to solve this problem with some nice tricks or maybe substitutions?","I came across following problem Evaluate $$\int\frac{1}{1+x^6} \,dx$$ When I asked my teacher for hint he said first evaluate $$\int\frac{1}{1+x^4} \,dx$$ I've tried to factorize $1+x^6$ as $$1+x^6=(x^2 + 1)(x^4 - x^2 + 1)$$ and then writing $$I=\int\frac{1}{1+x^6} \,dx=\int\frac{1}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx=\int\frac{1+x^2-x^2}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx$$ $$I=\int\frac{1}{x^4 - x^2 + 1} \,dx-\int\frac{x^2}{(x^2 + 1)(x^4 - x^2 + 1)} \,dx$$ However $$x^4-x^2+1=\left(x^2-\frac12\right)^2+\frac{3}{4}$$ But I can't see how it helps I've also tried to reverse engineer the solution given by Wolfram Alpha And I need to have terms similar to $$\frac{x^2-1}{x^4-x^2+1} \quad , \quad \frac{1}{1+x^2} \quad , \quad \frac{1}{(x+c)^2+1}\quad , \quad \frac{1}{(x+c)^2+1}$$ in integrand, How can I transform my cute looking integrand into these huge terms? Since in exams I will neither have access to WA nor time to reverse engineer the solution moreover it does not seem intuitive,is there any way to solve this problem with some nice tricks or maybe substitutions?",,['calculus']
28,Proving double derivatives with the chain rule (I think?),Proving double derivatives with the chain rule (I think?),,"Hey StackExchange I'm having trouble understating where to start with this problem, I'm supposed to prove something about double derivatives and the chain rule but I'm having trouble understanding exactly what I'm being asked and how to go about doing it. If $y = f(u)$ and $u = g(x)$, where f and g are twice differentiable functions show that: $$\frac{d^2y}{dx^2} = \frac{d^2y}{du^2}\left(\frac{du}{dx}\right)^2 + \frac{dy}{du}\left(\frac{d^2u}{dx^2}\right)$$","Hey StackExchange I'm having trouble understating where to start with this problem, I'm supposed to prove something about double derivatives and the chain rule but I'm having trouble understanding exactly what I'm being asked and how to go about doing it. If $y = f(u)$ and $u = g(x)$, where f and g are twice differentiable functions show that: $$\frac{d^2y}{dx^2} = \frac{d^2y}{du^2}\left(\frac{du}{dx}\right)^2 + \frac{dy}{du}\left(\frac{d^2u}{dx^2}\right)$$",,"['calculus', 'algebra-precalculus', 'derivatives']"
29,Why is Simpson's rule exact for cubics?,Why is Simpson's rule exact for cubics?,,I can't understand: Why is Simpson's rule exact for cubic polynomials?,I can't understand: Why is Simpson's rule exact for cubic polynomials?,,"['calculus', 'integration', 'approximation']"
30,Nth derivative of $\tan^m x$,Nth derivative of,\tan^m x,"$m$ is positive integer, $n$ is non-negative integer. $$f_n(x)=\frac {d^n}{dx^n} (\tan ^m(x))$$ $P_n(x)=f_n(\arctan(x))$ I would like to find the polynomials that are defined as above $P_0(x)=x^m$ $P_1(x)=mx^{m+1}+mx^{m-1}$ $P_2(x)=m(m+1)x^{m+2}+2m^2x^{m}+m(m-1)x^{m-2}$ $P_3(x)=(m^3+3m^2+2m)x^{m+3}+(3m^3+3m^2+2m)x^{m+1}+(3m^3-3m^2+2m)x^{m-1}+(m^3-3m^2+2m)x^{m-3}$ I wonder how to  find general formula of $P_n(x)$? I also wish to know if any orthogonal relation  can be found for that polynomials or not? Thanks for answers EDIT: I proved Robert Isreal's generating function. I would like to share it. $$ g(x,z) = \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^m(x) = \tan^m(x+z)  $$ $$ \frac {d}{dz} (\tan^m(x+z))=m \tan^{m-1}(x+z)+m \tan^{m+1}(x+z)=m \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^{m-1}(x)+m \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^{m+1}(x)= \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} (m\tan^{m-1}(x)+m\tan^{m+1}(x))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} (\dfrac{d}{dx}(\tan^{m}(x)))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^{n+1}}{dx^{n+1}} (\tan^{m}(x))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^{n+1}}{dx^{n+1}} (\tan^{m}(x))$$ $$ \frac {d}{dz} ( \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^m(x) )= \sum_{n=1}^\infty \dfrac{z^{n-1}}{(n-1)!} \dfrac{d^n}{dx^n} \tan^m(x) = \sum_{n=1}^\infty \dfrac{z^{n-1}}{(n-1)!} \dfrac{d^n}{dx^n} \tan^m(x)=\sum_{k=0}^\infty \dfrac{z^{k}}{k!} \dfrac{d^{k+1}}{dx^{k+1}} \tan^m(x)$$ I also understood that it can be written for any function  as shown  below .(Thanks a lot to Robert Isreal) $$ \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} h^m(x) = h^m(x+z)  $$ I also wrote $P_n(x)$ as the closed form shown below by using Robert Israel's  answer. $$P_n(x)=\frac{n!}{2 \pi i}\int_0^{2 \pi i} e^{nz}\left(\dfrac{x+\tan(e^{-z})}{1-x \tan(e^{-z})}\right)^m dz$$ I do not know next step how to find if any orthogonal relation exist between the polynomials or not. Maybe second order differential equation can be found  by using the relations above.  Thanks for advice.","$m$ is positive integer, $n$ is non-negative integer. $$f_n(x)=\frac {d^n}{dx^n} (\tan ^m(x))$$ $P_n(x)=f_n(\arctan(x))$ I would like to find the polynomials that are defined as above $P_0(x)=x^m$ $P_1(x)=mx^{m+1}+mx^{m-1}$ $P_2(x)=m(m+1)x^{m+2}+2m^2x^{m}+m(m-1)x^{m-2}$ $P_3(x)=(m^3+3m^2+2m)x^{m+3}+(3m^3+3m^2+2m)x^{m+1}+(3m^3-3m^2+2m)x^{m-1}+(m^3-3m^2+2m)x^{m-3}$ I wonder how to  find general formula of $P_n(x)$? I also wish to know if any orthogonal relation  can be found for that polynomials or not? Thanks for answers EDIT: I proved Robert Isreal's generating function. I would like to share it. $$ g(x,z) = \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^m(x) = \tan^m(x+z)  $$ $$ \frac {d}{dz} (\tan^m(x+z))=m \tan^{m-1}(x+z)+m \tan^{m+1}(x+z)=m \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^{m-1}(x)+m \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^{m+1}(x)= \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} (m\tan^{m-1}(x)+m\tan^{m+1}(x))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} (\dfrac{d}{dx}(\tan^{m}(x)))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^{n+1}}{dx^{n+1}} (\tan^{m}(x))=\sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^{n+1}}{dx^{n+1}} (\tan^{m}(x))$$ $$ \frac {d}{dz} ( \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} \tan^m(x) )= \sum_{n=1}^\infty \dfrac{z^{n-1}}{(n-1)!} \dfrac{d^n}{dx^n} \tan^m(x) = \sum_{n=1}^\infty \dfrac{z^{n-1}}{(n-1)!} \dfrac{d^n}{dx^n} \tan^m(x)=\sum_{k=0}^\infty \dfrac{z^{k}}{k!} \dfrac{d^{k+1}}{dx^{k+1}} \tan^m(x)$$ I also understood that it can be written for any function  as shown  below .(Thanks a lot to Robert Isreal) $$ \sum_{n=0}^\infty \dfrac{z^n}{n!} \dfrac{d^n}{dx^n} h^m(x) = h^m(x+z)  $$ I also wrote $P_n(x)$ as the closed form shown below by using Robert Israel's  answer. $$P_n(x)=\frac{n!}{2 \pi i}\int_0^{2 \pi i} e^{nz}\left(\dfrac{x+\tan(e^{-z})}{1-x \tan(e^{-z})}\right)^m dz$$ I do not know next step how to find if any orthogonal relation exist between the polynomials or not. Maybe second order differential equation can be found  by using the relations above.  Thanks for advice.",,"['calculus', 'trigonometry', 'polynomials', 'derivatives']"
31,How to write an integral as a limit?,How to write an integral as a limit?,,"We know that the derivative of a function called $f(x)$ can be written as a limit, just like here:$$\frac{d}{dx}f(x)=\lim\limits_{\Delta x\to0}\frac{f(x+\Delta x)-f(x)}{\Delta x}$$ but Is there any definition of integrals in the form of limits?","We know that the derivative of a function called $f(x)$ can be written as a limit, just like here:$$\frac{d}{dx}f(x)=\lim\limits_{\Delta x\to0}\frac{f(x+\Delta x)-f(x)}{\Delta x}$$ but Is there any definition of integrals in the form of limits?",,"['calculus', 'integration']"
32,$\int dx/(x^{10000}-1)$,,\int dx/(x^{10000}-1),"Is there any way to evaluate this indefinite integral using pencil and paper? A closed-form solution exists, because $1/(x^{10000}-1)$ can be expressed as a partial fraction decomposition of the form $\sum c_m/(x-a_m)$, where the $a_m$ are the 10,000-th roots of unity. But brute-force computation of the $c_m$ is the kind of fool's errand that a human would never embark on, and that software stupidly attempts and fails to accomplish. (Maxima, Yacas, and Wolfram Alpha all try and fail.) This is not homework.","Is there any way to evaluate this indefinite integral using pencil and paper? A closed-form solution exists, because $1/(x^{10000}-1)$ can be expressed as a partial fraction decomposition of the form $\sum c_m/(x-a_m)$, where the $a_m$ are the 10,000-th roots of unity. But brute-force computation of the $c_m$ is the kind of fool's errand that a human would never embark on, and that software stupidly attempts and fails to accomplish. (Maxima, Yacas, and Wolfram Alpha all try and fail.) This is not homework.",,['calculus']
33,Reduction formula for $I_{n}=\int {\cos{nx} \over \cos{x}}\rm{d}x$,Reduction formula for,I_{n}=\int {\cos{nx} \over \cos{x}}\rm{d}x,"What would be a simple method to compute a reduction formula for the following? $\displaystyle I_{n}=\int {\cos{nx} \over \cos{x}} \rm{d}x~$ where $n$ is a positive integer I understand that it may involve splitting the numerator into $\cos(n-2+2)x~$ (or something similar to this form...), but how would one intuitively recognize that manipulating the expression into such a random arrangement is the way to proceed on this question? Moreover, are there alternative methods, and possibly even some way of directly computing this integral without the need for a reduction formula?","What would be a simple method to compute a reduction formula for the following? $\displaystyle I_{n}=\int {\cos{nx} \over \cos{x}} \rm{d}x~$ where $n$ is a positive integer I understand that it may involve splitting the numerator into $\cos(n-2+2)x~$ (or something similar to this form...), but how would one intuitively recognize that manipulating the expression into such a random arrangement is the way to proceed on this question? Moreover, are there alternative methods, and possibly even some way of directly computing this integral without the need for a reduction formula?",,"['calculus', 'integration', 'trigonometry', 'recurrence-relations', 'reduction-formula']"
34,How to evaluate $\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}}?$,How to evaluate,\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}}?,$$\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}}$$ How to evaluate the above integral? I am trying this question by substituting $x = \tan^{2 / 3} y$ . Then the denominator will become $$\sqrt{\tan^{2/3} y+\sqrt{\tan^{4/3} y+ \tan y}} .$$ But how to proceed further?,How to evaluate the above integral? I am trying this question by substituting . Then the denominator will become But how to proceed further?,\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}} x = \tan^{2 / 3} y \sqrt{\tan^{2/3} y+\sqrt{\tan^{4/3} y+ \tan y}} .,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
35,Prove $\sum_{i=0}^n (-1)^{n-i} \binom{n+1}{i} (i+1)^n = (n+2)^n$,Prove,\sum_{i=0}^n (-1)^{n-i} \binom{n+1}{i} (i+1)^n = (n+2)^n,"I found through simulations that $$\sum_{i=0}^n (-1)^{n-i} \binom{n+1}{i} (i+1)^n  = (n+2)^n$$ Is there any proof of this?  I've tried to solve it by: Induction, but it gets too messy. Binomial theorem, but I got nowhere. Any help with this is highly appreciated.","I found through simulations that $$\sum_{i=0}^n (-1)^{n-i} \binom{n+1}{i} (i+1)^n  = (n+2)^n$$ Is there any proof of this?  I've tried to solve it by: Induction, but it gets too messy. Binomial theorem, but I got nowhere. Any help with this is highly appreciated.",,"['calculus', 'sequences-and-series', 'combinatorics', 'summation', 'power-series']"
36,Evaluation of $\lim_{n \to \infty} \int_{0}^\infty \frac{1}{1+x^n} dx$,Evaluation of,\lim_{n \to \infty} \int_{0}^\infty \frac{1}{1+x^n} dx,"$$L=\lim_{n \to \infty} \int_{0}^\infty \frac{1}{1+x^n} dx$$ $$\phi(x)=\lim_{n \to \infty}\frac{1}{1+x^n}$$ So I was just playing around with $\int_{0}^\infty \frac{1}{1+x^2} dx $ and it equals to $\frac{\pi}{2}$ . So I thought if this integral converges then higher powers of x it must also. And for that matter what is the limit as power becomes infinitely large? 1) I tried graphing $\phi(x)$ and I have a at x=0 the function equals 1 but it also equals 1 for all $x \in [0,1)$ 2) And at 1 it equals 1/2 and it equals 0 for, $x \in (1,\infty)$ Based on 1) and 2) I have a hunch that $L=1$ . Any rigorous proofs? Thanks in advance. And tell me about this question's or this type's formal name because I didn't know what to search on google.","So I was just playing around with and it equals to . So I thought if this integral converges then higher powers of x it must also. And for that matter what is the limit as power becomes infinitely large? 1) I tried graphing and I have a at x=0 the function equals 1 but it also equals 1 for all 2) And at 1 it equals 1/2 and it equals 0 for, Based on 1) and 2) I have a hunch that . Any rigorous proofs? Thanks in advance. And tell me about this question's or this type's formal name because I didn't know what to search on google.","L=\lim_{n \to \infty} \int_{0}^\infty \frac{1}{1+x^n} dx \phi(x)=\lim_{n \to \infty}\frac{1}{1+x^n} \int_{0}^\infty \frac{1}{1+x^2} dx  \frac{\pi}{2} \phi(x) x \in [0,1) x \in (1,\infty) L=1","['calculus', 'limits']"
37,"If $f$ is differentiable and bounded and $\lim\limits_{x\to\infty}f'(x)=0$, does $\lim\limits_{x\to\infty}f(x)$ exists?","If  is differentiable and bounded and , does  exists?",f \lim\limits_{x\to\infty}f'(x)=0 \lim\limits_{x\to\infty}f(x),"Let $f$ be differentiable and bounded, and $\lim\limits_{x\to\infty}f'(x)=0$. I want to know if $\lim\limits_{x\to\infty}f(x)$ exists. It is easy (with MVT) to show that $\lim\limits_{x\to\infty}(f(x+1)-f(x))=0$, and other questions (e.g. here ) show that $\lim\limits_{x\to\infty}\frac{f(x)}x=0$. My intuition says that $\lim\limits_{x\to\infty}f(x)$ should exist, but that does not mean anything.","Let $f$ be differentiable and bounded, and $\lim\limits_{x\to\infty}f'(x)=0$. I want to know if $\lim\limits_{x\to\infty}f(x)$ exists. It is easy (with MVT) to show that $\lim\limits_{x\to\infty}(f(x+1)-f(x))=0$, and other questions (e.g. here ) show that $\lim\limits_{x\to\infty}\frac{f(x)}x=0$. My intuition says that $\lim\limits_{x\to\infty}f(x)$ should exist, but that does not mean anything.",,['calculus']
38,Mean value theorem understanding,Mean value theorem understanding,,"So I have this question here which says: Say that f is differentiable and $f'(x)\neq1$ on $(-\infty,\infty)$. Show that there is at most one real number $a$ such that $f(a)=a$ I'm supposed to use the mean value theorem with the function $g(x)=f(x)-x$ but i'm not really sure how I'm supposed to incorporated the mean value theorem into this... I tried to substitute for $f(b)$ with $g(b)+b$ and simplify things but i'm not getting anywhere. Any help on this please?","So I have this question here which says: Say that f is differentiable and $f'(x)\neq1$ on $(-\infty,\infty)$. Show that there is at most one real number $a$ such that $f(a)=a$ I'm supposed to use the mean value theorem with the function $g(x)=f(x)-x$ but i'm not really sure how I'm supposed to incorporated the mean value theorem into this... I tried to substitute for $f(b)$ with $g(b)+b$ and simplify things but i'm not getting anywhere. Any help on this please?",,"['calculus', 'derivatives']"
39,Why is continuous differentiability important?,Why is continuous differentiability important?,,"In calculus, I would presume that the notion of continuous differentiability is important, which is why we have classes $C^1, C^2,\ldots,C^n$ which are defined in terms of having a continuous $n$th derivative. But why? Why is the derivative being continuous relevant at all? What is the motivation for defining $C^n$ in terms of not merely being $n$ times differentiable, but $n$ times continuously differentiable? For which (important) theorems in single and multivariable calculus is the hypothesis of continuous differentiability absolutely required? It is not required for either the fundamental theorem of calculus or integration by substitution, though it is often presented as being such.","In calculus, I would presume that the notion of continuous differentiability is important, which is why we have classes $C^1, C^2,\ldots,C^n$ which are defined in terms of having a continuous $n$th derivative. But why? Why is the derivative being continuous relevant at all? What is the motivation for defining $C^n$ in terms of not merely being $n$ times differentiable, but $n$ times continuously differentiable? For which (important) theorems in single and multivariable calculus is the hypothesis of continuous differentiability absolutely required? It is not required for either the fundamental theorem of calculus or integration by substitution, though it is often presented as being such.",,"['calculus', 'derivatives']"
40,Finding Delta Algebraically for a Given Epsilon?,Finding Delta Algebraically for a Given Epsilon?,,"For the limit $$\lim_{x\to 5}\sqrt{x-1}=2$$ find a $\delta>0$ that works for $\epsilon=1$. In another words, find a $\delta>0$ such that for all $x$, $$0<|x-5|<\delta \implies |\sqrt{x-1}-2|<1$$ Ok so here's what I did... $$|\sqrt{x-1}-2|<1$$ $$-1<\sqrt{x-1}-2<1$$ $$1<\sqrt{x-1}<3$$ $$1<x-1<9$$ $$2<x<10$$ Since I have to get $x$ in the simplified inequality above to change to $x-5$, I decided to subtract 5 on all sides like this: $$2-5<x-5<10-5$$ $$-3<x-5<5$$ But I don't know if that's right or not. Please help?","For the limit $$\lim_{x\to 5}\sqrt{x-1}=2$$ find a $\delta>0$ that works for $\epsilon=1$. In another words, find a $\delta>0$ such that for all $x$, $$0<|x-5|<\delta \implies |\sqrt{x-1}-2|<1$$ Ok so here's what I did... $$|\sqrt{x-1}-2|<1$$ $$-1<\sqrt{x-1}-2<1$$ $$1<\sqrt{x-1}<3$$ $$1<x-1<9$$ $$2<x<10$$ Since I have to get $x$ in the simplified inequality above to change to $x-5$, I decided to subtract 5 on all sides like this: $$2-5<x-5<10-5$$ $$-3<x-5<5$$ But I don't know if that's right or not. Please help?",,"['calculus', 'algebra-precalculus', 'limits', 'continuity', 'epsilon-delta']"
41,Number of points of accumulation of a sequence,Number of points of accumulation of a sequence,,Can a sequence have infinitely many points of accumulation i.e. we can extract infinitely many subsequences from it s.t. they all converge to their respective point of accumulation?  I have the feeling it would mean that the period of repetition of something could be infinitely big.,Can a sequence have infinitely many points of accumulation i.e. we can extract infinitely many subsequences from it s.t. they all converge to their respective point of accumulation?  I have the feeling it would mean that the period of repetition of something could be infinitely big.,,"['calculus', 'sequences-and-series']"
42,How to solve $\lim_{n\to \infty}\sin(1)\times \sin(2)\times\sin(3)\times\ldots\times\sin(n)$,How to solve,\lim_{n\to \infty}\sin(1)\times \sin(2)\times\sin(3)\times\ldots\times\sin(n),"The limits I'm trying to solve are: $$\lim_{n\to \infty}\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$ $$\lim_{n\to \infty}n\times\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$ For the former limit, my (probably incorrect) solution is that $\sin(1)\times\sin(2)\times\sin(3)\ldots$ are constants, so the limit can be written as $$\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n-1)\cdot \lim_{n\to \infty}\sin(n)$$ and $\lim_{n\to \infty}\sin(n)$ simply does not exist, because $\sin(n)$ does not settle on a single value when ${n\to \infty}$.","The limits I'm trying to solve are: $$\lim_{n\to \infty}\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$ $$\lim_{n\to \infty}n\times\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n)$$ For the former limit, my (probably incorrect) solution is that $\sin(1)\times\sin(2)\times\sin(3)\ldots$ are constants, so the limit can be written as $$\sin(1)\times\sin(2)\times\sin(3)\times\ldots\times\sin(n-1)\cdot \lim_{n\to \infty}\sin(n)$$ and $\lim_{n\to \infty}\sin(n)$ simply does not exist, because $\sin(n)$ does not settle on a single value when ${n\to \infty}$.",,"['calculus', 'limits']"
43,When does $\ln(x)=\sin(x)$?,When does ?,\ln(x)=\sin(x),"This isn't a very easy equation to solve, and I really don't know where to start. I need the answer, not hints, because I know this is too hard for my level and I won't be able to do it even with hints. I'd like the answer in fractions, not decimals (I could easily use a graphing calculator to get the decimal answer). Use $\pi$ and e symbols as necessary. Thanks! Note: if you use any complicated functions in your answer, please explain what they are.","This isn't a very easy equation to solve, and I really don't know where to start. I need the answer, not hints, because I know this is too hard for my level and I won't be able to do it even with hints. I'd like the answer in fractions, not decimals (I could easily use a graphing calculator to get the decimal answer). Use $\pi$ and e symbols as necessary. Thanks! Note: if you use any complicated functions in your answer, please explain what they are.",,"['calculus', 'transcendental-equations']"
44,Which expansion of $e$ is more accurate?,Which expansion of  is more accurate?,e,We have two forms  of $e^x$ $$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+....$$   and   $$e^x=\frac{1}{\displaystyle 1-x+\frac{x^2}{2!}-\frac{x^3}{3!}+....}$$   The second form comes from $e^x=1/e^{-x}$ Which one is more accurate if I want to find any value of $e^x$,We have two forms  of $e^x$ $$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+....$$   and   $$e^x=\frac{1}{\displaystyle 1-x+\frac{x^2}{2!}-\frac{x^3}{3!}+....}$$   The second form comes from $e^x=1/e^{-x}$ Which one is more accurate if I want to find any value of $e^x$,,"['calculus', 'summation', 'taylor-expansion', 'approximation']"
45,Why is calculating the area under a curve required or rather what usage it would provide,Why is calculating the area under a curve required or rather what usage it would provide,,I understand Integration and Differentiation and see a lot of Physics / Electrical Theory using them. Take for example a sine wave. So area for me means the space any object would occupy. So what's usage it comes to find the area of a sine curve? There are so a many formulas that calculates the area by Integration - but why calculation is required - I mean what information we can get (isn't it just space occupied) or rather what data we can find by calculating area of a curve via Integration?,I understand Integration and Differentiation and see a lot of Physics / Electrical Theory using them. Take for example a sine wave. So area for me means the space any object would occupy. So what's usage it comes to find the area of a sine curve? There are so a many formulas that calculates the area by Integration - but why calculation is required - I mean what information we can get (isn't it just space occupied) or rather what data we can find by calculating area of a curve via Integration?,,"['calculus', 'integration']"
46,Integral $ \int_{-\pi/2}^{\pi/2} \frac{1}{2007^x+1}\cdot \frac{\sin^{2008}x}{\sin^{2008}x+\cos^{2008}x}dx $,Integral, \int_{-\pi/2}^{\pi/2} \frac{1}{2007^x+1}\cdot \frac{\sin^{2008}x}{\sin^{2008}x+\cos^{2008}x}dx ,"I am trying to solve this integral $$ \int_{-\pi/2}^{\pi/2} \frac{1}{2007^x+1}\cdot \frac{\sin^{2008}x}{\sin^{2008}x+\cos^{2008}x}dx $$ A closed form does exist despite the looks of the integrand.  This problem is from some old high school IMO training courses.  I am not sure how to solve it. The only information that may be of help is $$ \sin x=\sum_{n=1}^\infty \frac{(-1)^n x^{2n+1}}{(2n+1)!}, \quad \cos x=\sum_{n=1}^\infty \frac{ x^{2n}}{(2n)!} \quad \forall \ x $$ Thanks, I am looking for a complete solution, not a description as of what to do.","I am trying to solve this integral $$ \int_{-\pi/2}^{\pi/2} \frac{1}{2007^x+1}\cdot \frac{\sin^{2008}x}{\sin^{2008}x+\cos^{2008}x}dx $$ A closed form does exist despite the looks of the integrand.  This problem is from some old high school IMO training courses.  I am not sure how to solve it. The only information that may be of help is $$ \sin x=\sum_{n=1}^\infty \frac{(-1)^n x^{2n+1}}{(2n+1)!}, \quad \cos x=\sum_{n=1}^\infty \frac{ x^{2n}}{(2n)!} \quad \forall \ x $$ Thanks, I am looking for a complete solution, not a description as of what to do.",,"['calculus', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
47,Doesn't the constant matter in $\int\frac{1}{x}dx=\ln(kx)+C$ instead of writing $\ln(x)+C$?,Doesn't the constant matter in  instead of writing ?,\int\frac{1}{x}dx=\ln(kx)+C \ln(x)+C,If I know that $\frac{d}{dx}\ln(5x)=\frac{1}{x}$ then shouldn't I be taught that $\int\frac{1}{x}dx=\ln(kx)+C$ instead of $\ln(x)+C$? Is there a reason why we don't care about the constant inside the $\ln$ function?,If I know that $\frac{d}{dx}\ln(5x)=\frac{1}{x}$ then shouldn't I be taught that $\int\frac{1}{x}dx=\ln(kx)+C$ instead of $\ln(x)+C$? Is there a reason why we don't care about the constant inside the $\ln$ function?,,"['calculus', 'integration', 'indefinite-integrals']"
48,A sufficient condition for linearity?,A sufficient condition for linearity?,,"If $f$ is a linear function (defined on $\mathbb{R}$), then for each $x$, $f(x) – xf’(x) = f(0)$. Is the converse true? That is, is it true that if $f$ is a differentiable function defined on $\mathbb{R}$ such that for each $x$, $f(x) – xf’(x) = f(0)$, then $f$ is linear?","If $f$ is a linear function (defined on $\mathbb{R}$), then for each $x$, $f(x) – xf’(x) = f(0)$. Is the converse true? That is, is it true that if $f$ is a differentiable function defined on $\mathbb{R}$ such that for each $x$, $f(x) – xf’(x) = f(0)$, then $f$ is linear?",,['calculus']
49,Ranges and the Fundamental Theorem of Calculus 1,Ranges and the Fundamental Theorem of Calculus 1,,I'm going over a chapter by chapter review for my calculus final and discovered this problem: $$y=\int_{\sqrt{x}}^{x^3}\sqrt{t}\sin{t}\;\mathrm dt$$ They split it up so that it became: $$-\int_1^{\sqrt{x}}\sqrt{t}\sin{t}\;\mathrm dt + \int_1^{x^3}\sqrt{t}\sin{t}\;\mathrm dt $$ Why 1? How was that determined?,I'm going over a chapter by chapter review for my calculus final and discovered this problem: $$y=\int_{\sqrt{x}}^{x^3}\sqrt{t}\sin{t}\;\mathrm dt$$ They split it up so that it became: $$-\int_1^{\sqrt{x}}\sqrt{t}\sin{t}\;\mathrm dt + \int_1^{x^3}\sqrt{t}\sin{t}\;\mathrm dt $$ Why 1? How was that determined?,,['calculus']
50,Why can we linearize the exponential regression?,Why can we linearize the exponential regression?,,"When we are using observed data $(x_1,y_1)\ldots(x_m,y_m)$ for the exponential model $$y(x)=a_1\mathrm{e}^{a_2x}~(a_1>0),$$ it is natural to think about the linearized model $$\ln y=\ln a_1+a_2x.$$ It is not hard to understand this approach, but my question is, how can we prove rigorously that we are obtaining the same results? I.e., the following two optimization problems for $a_1$ and $a_2$ $$\text{minimize}\sum_{k=1}^m(a_1\mathrm{e}^{a_2 x_k}-y_k)^2$$ and $$\text{minimize}\sum_{k=1}^m(\ln a_1+a_2 x_k-\ln y_k)^2$$ yield the same $a_1$ , $a_2$ ? My attempts using multivariate calculus failed, and I can only say that since the problems are ""equivalent"" with unique solution, the answer should be unique. Is that acceptable?","When we are using observed data for the exponential model it is natural to think about the linearized model It is not hard to understand this approach, but my question is, how can we prove rigorously that we are obtaining the same results? I.e., the following two optimization problems for and and yield the same , ? My attempts using multivariate calculus failed, and I can only say that since the problems are ""equivalent"" with unique solution, the answer should be unique. Is that acceptable?","(x_1,y_1)\ldots(x_m,y_m) y(x)=a_1\mathrm{e}^{a_2x}~(a_1>0), \ln y=\ln a_1+a_2x. a_1 a_2 \text{minimize}\sum_{k=1}^m(a_1\mathrm{e}^{a_2 x_k}-y_k)^2 \text{minimize}\sum_{k=1}^m(\ln a_1+a_2 x_k-\ln y_k)^2 a_1 a_2","['calculus', 'statistics', 'optimization', 'nonlinear-optimization', 'regression']"
51,Applications of Ramanujan's Master Theorem,Applications of Ramanujan's Master Theorem,,"Ramanujan's Master Theorem is really neat. Unfortunately, however I have only used it once before, and I want to use it more. I would like a list of integrals to which I may apply this beautiful theorem. The Theorem: (Taken from Wikipedia ) If $f(x)$ is a complex valued function with a series representation in the form $$f(x)=\sum_{n\geq0}\frac{\phi(n)}{n!}(-x)^n$$ Then $$\int_0^\infty x^{s-1}f(x)\mathrm dx=\Gamma(s)\phi(-s)$$ Where $\Gamma(s)$ is the Gamma function. Cheers!","Ramanujan's Master Theorem is really neat. Unfortunately, however I have only used it once before, and I want to use it more. I would like a list of integrals to which I may apply this beautiful theorem. The Theorem: (Taken from Wikipedia ) If is a complex valued function with a series representation in the form Then Where is the Gamma function. Cheers!",f(x) f(x)=\sum_{n\geq0}\frac{\phi(n)}{n!}(-x)^n \int_0^\infty x^{s-1}f(x)\mathrm dx=\Gamma(s)\phi(-s) \Gamma(s),"['calculus', 'integration', 'definite-integrals']"
52,"Explaining $\int_{-1}^1\frac{1}{1+x^2}\,dx = \frac{\pi}{2}$.",Explaining .,"\int_{-1}^1\frac{1}{1+x^2}\,dx = \frac{\pi}{2}","How would you explain to a student that $$ \int_{-1}^1\frac{1}{1+x^2}\,dx = \arctan(1) - \arctan(-1) = \frac{\pi}{2} $$ and not $$ \int_{-1}^1\frac{1}{1+x^2}\,dx = \arctan(1) - \arctan(-1) \neq \frac{\pi}{4} - \frac{3\pi}{4} = -\frac{\pi}{2}$$ besides the obvious fact that $\arctan x$ cannot map to two distinct values?","How would you explain to a student that $$ \int_{-1}^1\frac{1}{1+x^2}\,dx = \arctan(1) - \arctan(-1) = \frac{\pi}{2} $$ and not $$ \int_{-1}^1\frac{1}{1+x^2}\,dx = \arctan(1) - \arctan(-1) \neq \frac{\pi}{4} - \frac{3\pi}{4} = -\frac{\pi}{2}$$ besides the obvious fact that $\arctan x$ cannot map to two distinct values?",,"['calculus', 'integration']"
53,Finding the general Taylor Series of a function,Finding the general Taylor Series of a function,,"I have to find the general Taylor Series Expansion, about $0$, of the following function. $$ \sqrt{x^4 -6x^2+1} $$ I have tried to use the identity $$ \left(1+t\right)^{1/2} = \sum_{n\ge 0}\frac{(-1)^{n+1}}{4^n (2n-1)} \binom{2n}{n}t^n $$. and then substitute for $t$ accordingly, but to no avail, since the resulting expression becomes messy. Any help will be appreciated. Thanks.","I have to find the general Taylor Series Expansion, about $0$, of the following function. $$ \sqrt{x^4 -6x^2+1} $$ I have tried to use the identity $$ \left(1+t\right)^{1/2} = \sum_{n\ge 0}\frac{(-1)^{n+1}}{4^n (2n-1)} \binom{2n}{n}t^n $$. and then substitute for $t$ accordingly, but to no avail, since the resulting expression becomes messy. Any help will be appreciated. Thanks.",,"['calculus', 'taylor-expansion']"
54,Showing a function is injective using that $f'(x)\ne0$,Showing a function is injective using that,f'(x)\ne0,"Given a differentiable function $f\colon \mathbb R\to\mathbb R$ which we must prove to be injective, does it suffice to show $f'(x)≠0$ for all $x$ (for which the function is defined)? It makes sense, of course, due to the mean value theorem, but maybe there's some subtleties I am missing since this method doesn't seem to be used very often.","Given a differentiable function $f\colon \mathbb R\to\mathbb R$ which we must prove to be injective, does it suffice to show $f'(x)≠0$ for all $x$ (for which the function is defined)? It makes sense, of course, due to the mean value theorem, but maybe there's some subtleties I am missing since this method doesn't seem to be used very often.",,"['calculus', 'functions']"
55,"How do I find $\int_{0}^{\infty} \frac{\sin^4 x}{x^2}\,dx$?",How do I find ?,"\int_{0}^{\infty} \frac{\sin^4 x}{x^2}\,dx","I need help evaluating the following integral $$\int_{0}^{\infty} \frac{\sin^4 x}{x^2}\,dx$$ which should probably be equal to $\frac{\pi}{4}$ Using some trigonometric manipulations I got $\frac{3}{8} - \frac{\cos{2x}}{2} + \frac{\cos{4x}}{8}$ which using integration by parts doesn't lead me to anything pretty. Update : Not sure if I should post this as a separate question but getting explanation why $\int_{0}^{\infty} \frac{\sin{ax}}{x} = \frac{\pi}{2}$ for a positive integer $a$ could help me solve this question.","I need help evaluating the following integral $$\int_{0}^{\infty} \frac{\sin^4 x}{x^2}\,dx$$ which should probably be equal to $\frac{\pi}{4}$ Using some trigonometric manipulations I got $\frac{3}{8} - \frac{\cos{2x}}{2} + \frac{\cos{4x}}{8}$ which using integration by parts doesn't lead me to anything pretty. Update : Not sure if I should post this as a separate question but getting explanation why $\int_{0}^{\infty} \frac{\sin{ax}}{x} = \frac{\pi}{2}$ for a positive integer $a$ could help me solve this question.",,"['calculus', 'integration']"
56,"Prove that if a function $f$ has a jump at an interior point of the interval $[a,b]$ then it cannot be the derivative of any function.",Prove that if a function  has a jump at an interior point of the interval  then it cannot be the derivative of any function.,"f [a,b]","Prove that if a function $f$ has a jump at an interior point of the interval $[a,b]$ then it cannot be the derivative of any function. I know that for $f$ is differentiable in $(a,b)$ and that it has one-sided derivative $f_+' (a)≠f_-' (b)$ at the endpoints. If $C$ is a real number between $f_+' (a)$  and $f_-' (b)$, then there exists $c∈(a,b)$ such that $f' (c)=C $. How can I use this to prove the above?","Prove that if a function $f$ has a jump at an interior point of the interval $[a,b]$ then it cannot be the derivative of any function. I know that for $f$ is differentiable in $(a,b)$ and that it has one-sided derivative $f_+' (a)≠f_-' (b)$ at the endpoints. If $C$ is a real number between $f_+' (a)$  and $f_-' (b)$, then there exists $c∈(a,b)$ such that $f' (c)=C $. How can I use this to prove the above?",,"['calculus', 'proof-writing']"
57,What needs to be true of $f$ for $f_{xy}=f_{yx}$?,What needs to be true of  for ?,f f_{xy}=f_{yx},"What conditions does a function $f$ have to fulfill in order that: $\frac{\partial}{\partial x}(\frac{\partial f}{\partial y})=\frac{\partial}{\partial y}(\frac{\partial f}{\partial x}) $? I am trying to prove something else, and I have just got it down to this bit, which is usually true, but I'm not sure what was to be true of $f$ for this to be true. Continuity? Differentiable?","What conditions does a function $f$ have to fulfill in order that: $\frac{\partial}{\partial x}(\frac{\partial f}{\partial y})=\frac{\partial}{\partial y}(\frac{\partial f}{\partial x}) $? I am trying to prove something else, and I have just got it down to this bit, which is usually true, but I'm not sure what was to be true of $f$ for this to be true. Continuity? Differentiable?",,['calculus']
58,Integral $\int_{-\infty}^{\infty}\frac{e^{r \arctan(ax)}+e^{-r \arctan(ax)}}{1+x^2}\cos \left( \frac{r}{2}\log(1+a^2x^2)\right)dx$,Integral,\int_{-\infty}^{\infty}\frac{e^{r \arctan(ax)}+e^{-r \arctan(ax)}}{1+x^2}\cos \left( \frac{r}{2}\log(1+a^2x^2)\right)dx,How can I show that $$\int_{-\infty}^{\infty}\frac{e^{r \arctan(ax)}+e^{-r \arctan(ax)}}{1+x^2}\cos \left( \frac{r}{2}\log(1+a^2x^2)\right)dx = 2\pi \cos \left( r\log(1+a)\right)$$ where $a \in \mathbb{R}^+$ and $r \in \mathbb{R}$? The relatively simple result suggests that it might be possible to evaluate the integral using contour integration if one could find the right function and contour combination.,How can I show that $$\int_{-\infty}^{\infty}\frac{e^{r \arctan(ax)}+e^{-r \arctan(ax)}}{1+x^2}\cos \left( \frac{r}{2}\log(1+a^2x^2)\right)dx = 2\pi \cos \left( r\log(1+a)\right)$$ where $a \in \mathbb{R}^+$ and $r \in \mathbb{R}$? The relatively simple result suggests that it might be possible to evaluate the integral using contour integration if one could find the right function and contour combination.,,"['calculus', 'complex-analysis', 'integration', 'improper-integrals']"
59,Does little Bézout theorem hold for smooth functions?,Does little Bézout theorem hold for smooth functions?,,"As a special case of little Bézout theorem, if we have a polynomail $f(x)$ with $f(0)=0$, then there exists another polynomial $g(x)$ such that $f(x)=xg(x)$. It's easy to see that this fact generalizes to analytic functions because we have Taylor expansion. Now my question is whether little Bézout theorem holds for smooth functions. More precisely, my question is: If $f(x)\in C^{\infty}(\mathbb{R})$ with $f(0)=0$, does there exist $g(x)\in C^{\infty}(\mathbb{R})$ such that $f(x)=xg(x)$? EDIT: As pointed out by Jason, the above question has positive answer and it actually holds in arbitrary dimensions, that is, If $f(x)\in C^{\infty}(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in C^{\infty}(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. Multiplying a cutoff function on both sides, one obtains, If $f(x)\in C^{\infty}_c(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in C^{\infty}_c(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. With a cleverer use of cutoff function, one can also obtain, If $f(x)\in \mathscr{S}(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in \mathscr{S}(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. Here $\mathscr{S}((\mathbb{R^n})$ denotes the Schwartz space.","As a special case of little Bézout theorem, if we have a polynomail $f(x)$ with $f(0)=0$, then there exists another polynomial $g(x)$ such that $f(x)=xg(x)$. It's easy to see that this fact generalizes to analytic functions because we have Taylor expansion. Now my question is whether little Bézout theorem holds for smooth functions. More precisely, my question is: If $f(x)\in C^{\infty}(\mathbb{R})$ with $f(0)=0$, does there exist $g(x)\in C^{\infty}(\mathbb{R})$ such that $f(x)=xg(x)$? EDIT: As pointed out by Jason, the above question has positive answer and it actually holds in arbitrary dimensions, that is, If $f(x)\in C^{\infty}(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in C^{\infty}(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. Multiplying a cutoff function on both sides, one obtains, If $f(x)\in C^{\infty}_c(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in C^{\infty}_c(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. With a cleverer use of cutoff function, one can also obtain, If $f(x)\in \mathscr{S}(\mathbb{R^n})$ with $f(0)=0$, then there exist $g_i(x)\in \mathscr{S}(\mathbb{R^n})$ such that $f(x)=\sum_{i=1}^{n} x_ig_i(x)$. Here $\mathscr{S}((\mathbb{R^n})$ denotes the Schwartz space.",,"['calculus', 'analysis', 'smooth-functions']"
60,$\lim_{n \to +\infty } \left \{ en! \right \}$,,\lim_{n \to +\infty } \left \{ en! \right \},"I'd love your help this time with the following limit: $\lim_{n \to +\infty } \left \{ en! \right \}$ when $\{ a \}=a-[a].$ Honestly, I don't have a clue. Thank you.","I'd love your help this time with the following limit: $\lim_{n \to +\infty } \left \{ en! \right \}$ when $\{ a \}=a-[a].$ Honestly, I don't have a clue. Thank you.",,[]
61,Understanding Limits of Integration in Integration-by-Parts,Understanding Limits of Integration in Integration-by-Parts,,"My understanding of integration-by-parts is a little shaky.  In particular, I'm not totally certain that I understand how to properly calculate the limits of integration. For example, the formula I have is: $\int_{v_1}^{v_2}{u dv} = (u_2 v_2 - u_1 v_1) - \int_{u_1}^{u_2}{v du}$ I'd like to see how to calculate $u_1$ and $u_2$, preferably in a complete example (that solves a definite integral.)  I'm really interested in an example where the limits of integration change; i.e. $u_1$ and $u_2$ are different than $v_1$ and $v_2$, if possible.","My understanding of integration-by-parts is a little shaky.  In particular, I'm not totally certain that I understand how to properly calculate the limits of integration. For example, the formula I have is: $\int_{v_1}^{v_2}{u dv} = (u_2 v_2 - u_1 v_1) - \int_{u_1}^{u_2}{v du}$ I'd like to see how to calculate $u_1$ and $u_2$, preferably in a complete example (that solves a definite integral.)  I'm really interested in an example where the limits of integration change; i.e. $u_1$ and $u_2$ are different than $v_1$ and $v_2$, if possible.",,"['calculus', 'integration', 'definite-integrals']"
62,Show that $\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right)$,Show that,\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right),"Given that $a,b,c,d\in\mathbb{R}$ and that $c$ and $d$ are not both $0$ , show that $$\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right).$$ I was trying to answer a question and ended up with this integral. I don't think it helped me answer that other question, but I thought this integral is interesting by itself. I teased out the RHS expression by first noticing that $\tan \left(\frac{1}{\pi}\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx\right)$ seemed to be always rational when $a,b,c,d\in \mathbb{Z}$ , then setting two of $a,b,c,d$ equal to $1$ and then looking for patterns in the value of the integral in terms of the other two. That's all the progress I've made so far.","Given that and that and are not both , show that I was trying to answer a question and ended up with this integral. I don't think it helped me answer that other question, but I thought this integral is interesting by itself. I teased out the RHS expression by first noticing that seemed to be always rational when , then setting two of equal to and then looking for patterns in the value of the integral in terms of the other two. That's all the progress I've made so far.","a,b,c,d\in\mathbb{R} c d 0 \int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right). \tan \left(\frac{1}{\pi}\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx\right) a,b,c,d\in \mathbb{Z} a,b,c,d 1","['calculus', 'integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
63,Prove the following integral is constant: $\int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx$,Prove the following integral is constant:,\int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx,"There is a famous improper integral with exact solution given by $$\int_{0}^{+1} \frac{ \ln x }{\sqrt{1 - x^2}} dx = -\frac{\pi}{2} \ln 2$$ From this, it is pretty easy to generalize to the following variation: $$\int_{-1}^{+1} \frac{ \ln |x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2$$ From what I can tell, we can even extend this further to show that: $$\int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2$$ for any $|a| \leq 1$ . Unfortunately, I have only been able to verify this numerically (i.e., testing it for various values in Wolfram Alpha). However, I cannot think of a straightforward way to prove it generally. The identity has an interesting physics application I am working on. It shows that a surface charge density with the form $$\sigma(x) = \frac{1}{\sqrt{1 - x^2}}$$ will also result in a constant potential along a thin strip from [-1,1]; an interesting result for transmission line models and a nice validation for simulation methods. I would love to see a straightforward proof of this. I suspect it has been done, as I have seen it mentioned in the literature. I just can't find a reference to it, and I do not see any easy way to replicate it myself. Thanks in advance!","There is a famous improper integral with exact solution given by From this, it is pretty easy to generalize to the following variation: From what I can tell, we can even extend this further to show that: for any . Unfortunately, I have only been able to verify this numerically (i.e., testing it for various values in Wolfram Alpha). However, I cannot think of a straightforward way to prove it generally. The identity has an interesting physics application I am working on. It shows that a surface charge density with the form will also result in a constant potential along a thin strip from [-1,1]; an interesting result for transmission line models and a nice validation for simulation methods. I would love to see a straightforward proof of this. I suspect it has been done, as I have seen it mentioned in the literature. I just can't find a reference to it, and I do not see any easy way to replicate it myself. Thanks in advance!",\int_{0}^{+1} \frac{ \ln x }{\sqrt{1 - x^2}} dx = -\frac{\pi}{2} \ln 2 \int_{-1}^{+1} \frac{ \ln |x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2 \int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2 |a| \leq 1 \sigma(x) = \frac{1}{\sqrt{1 - x^2}},"['calculus', 'integration']"
64,"Why is this integral diverging? $\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx$",Why is this integral diverging?,"\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx","$$\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx$$ I can easily prove that this integral is diverging by taking the limit over the following proper integral, $$\lim_{R_1,R_2\to \infty} \int\limits^{R_2}_{-R_1} \,\frac{x}{x^2+1} dx$$ Mathematically, this makes sense to me, but intuitively I am not able to absorb this. If we observe the function $y=\frac{x}{x^2+1}$ , it is clearly an odd function. And since, integrals return signed areas, an integral to an odd function having limits that are the negatives of each other, should evaluate to zero. As clearly seen by the following plot. Can someone please clarify this? Thank you.","I can easily prove that this integral is diverging by taking the limit over the following proper integral, Mathematically, this makes sense to me, but intuitively I am not able to absorb this. If we observe the function , it is clearly an odd function. And since, integrals return signed areas, an integral to an odd function having limits that are the negatives of each other, should evaluate to zero. As clearly seen by the following plot. Can someone please clarify this? Thank you.","\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx \lim_{R_1,R_2\to \infty} \int\limits^{R_2}_{-R_1} \,\frac{x}{x^2+1} dx y=\frac{x}{x^2+1}","['calculus', 'integration', 'convergence-divergence', 'definite-integrals']"
65,"How to calculate $\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x}$.",How to calculate .,"\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x}","Calculate: $$\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x}$$ My attempt: Let $$ A=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x},B=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\sin x\, \text{d}x} $$ then $$ A+B=\frac{\pi ^2\ln 2}{4}-\frac{7}{8}\zeta \left( 3 \right) +\underset{I}{\underbrace{\int_0^{\frac{\pi}{2}}{x^2\cot x\ln \left( \sin 2x \right)}\, \text{d}x}} $$ $$ A-B=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cot x\, \text{d}x} $$ Define $$ J\left( a,b \right) =\int_0^{\frac{\pi}{2}}{\frac{\sin \left( 2ax \right)}{\sin ^b\left( 2x \right)}}\, \text{d}x $$ I‘ve been stuck here for a long time, I can't figure out $I$ by the derivative of $J(a,b)$. Maybe I'm doing the wrong way. If so, how can I figure it out? THX!","Calculate: $$\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x}$$ My attempt: Let $$ A=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cos x\, \text{d}x},B=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\sin x\, \text{d}x} $$ then $$ A+B=\frac{\pi ^2\ln 2}{4}-\frac{7}{8}\zeta \left( 3 \right) +\underset{I}{\underbrace{\int_0^{\frac{\pi}{2}}{x^2\cot x\ln \left( \sin 2x \right)}\, \text{d}x}} $$ $$ A-B=\int_0^{\frac{\pi}{2}}{x^2\cot x\ln\cot x\, \text{d}x} $$ Define $$ J\left( a,b \right) =\int_0^{\frac{\pi}{2}}{\frac{\sin \left( 2ax \right)}{\sin ^b\left( 2x \right)}}\, \text{d}x $$ I‘ve been stuck here for a long time, I can't figure out $I$ by the derivative of $J(a,b)$. Maybe I'm doing the wrong way. If so, how can I figure it out? THX!",,"['calculus', 'integration', 'definite-integrals', 'harmonic-numbers', 'digamma-function']"
66,How to find the general solution to $\int f^{-1}(x){\rm d}x$ in terms of $\int f(x){\rm d}x$,How to find the general solution to  in terms of,\int f^{-1}(x){\rm d}x \int f(x){\rm d}x,"I am trying to find a general proof of $\int f^{-1}(x)\,{\rm d}x$ in terms of $\int f(x)\,{\rm d}x$. The first step that I took was to piece apart what it means for a function to have an inverse. So I know the way an inverse function works, but I don’t know how it works in integration like in proving this. I am very interested in seeing the solution to this because knowing what this is would help to solve integrals where you know what the integral for the inverse function is.","I am trying to find a general proof of $\int f^{-1}(x)\,{\rm d}x$ in terms of $\int f(x)\,{\rm d}x$. The first step that I took was to piece apart what it means for a function to have an inverse. So I know the way an inverse function works, but I don’t know how it works in integration like in proving this. I am very interested in seeing the solution to this because knowing what this is would help to solve integrals where you know what the integral for the inverse function is.",,"['calculus', 'integration', 'inverse-function']"
67,How is the concept of the limit the foundation of calculus?,How is the concept of the limit the foundation of calculus?,,"My casual study of mathematics and calculus introduced me to the notion that calculus didn't find a firm foundation until Cauchy, Weierstrauss (et al) developed set theory some ~100 years after Newton and Leibniz. The concept of limits (and their epsilon-delta proofs) was what allowed calculus to get past the shaky logic of infinitesimals. (The above is just me laying out what I believe to be true, but please feel free to take issue with any/all of it.) HERE IS MY ACTUAL QUESTION (more or less). How exactly do limits save calculus? When thinking about the derivative, we want to contemplate the rate of change of a function wrt its input. Instead of using an infinitesimal change in the input, we take the limit as the change in the input goes to zero. When we say ""take the limit"" we mean finding a band around the function value (+/- ""epsilon"") and derive a band around the input value (+/- ""delta"") such that any value of the input beyond a certain point is guaranteed to generate a function value within our desired band (i.e., +/- epsilon). This is all rather loose, but hopefully you catch the drift. BUT HOW DOES THIS HELP SAVE CALCULUS? Is it because by saying we can take any ARBITRARY epsilon, we are in effect , saying we can take EVERY epsilon > 0? We exhaust every epsilon all the way down to (but not including) 0. This action reminds me a lot of the idea that 0.999... = 1. Sorry if this isn't clear, but if I had to put this in simplest terms, I guess it boils down to: is ""arbitrarily many"" the same as ""infinitely many""?","My casual study of mathematics and calculus introduced me to the notion that calculus didn't find a firm foundation until Cauchy, Weierstrauss (et al) developed set theory some ~100 years after Newton and Leibniz. The concept of limits (and their epsilon-delta proofs) was what allowed calculus to get past the shaky logic of infinitesimals. (The above is just me laying out what I believe to be true, but please feel free to take issue with any/all of it.) HERE IS MY ACTUAL QUESTION (more or less). How exactly do limits save calculus? When thinking about the derivative, we want to contemplate the rate of change of a function wrt its input. Instead of using an infinitesimal change in the input, we take the limit as the change in the input goes to zero. When we say ""take the limit"" we mean finding a band around the function value (+/- ""epsilon"") and derive a band around the input value (+/- ""delta"") such that any value of the input beyond a certain point is guaranteed to generate a function value within our desired band (i.e., +/- epsilon). This is all rather loose, but hopefully you catch the drift. BUT HOW DOES THIS HELP SAVE CALCULUS? Is it because by saying we can take any ARBITRARY epsilon, we are in effect , saying we can take EVERY epsilon > 0? We exhaust every epsilon all the way down to (but not including) 0. This action reminds me a lot of the idea that 0.999... = 1. Sorry if this isn't clear, but if I had to put this in simplest terms, I guess it boils down to: is ""arbitrarily many"" the same as ""infinitely many""?",,"['calculus', 'limits', 'infinitesimals', 'big-picture']"
68,What does the $L^p$ norm tend to as $p\to 0$? [duplicate],What does the  norm tend to as ? [duplicate],L^p p\to 0,"This question already has answers here : Limit of $L^p$ norm when $p\to0$ (4 answers) Closed 5 years ago . This is something I was thinking about, so I'm going to post it as a question and post my own answer. I hope that anyone who wants will comment, correct me if I'm wrong, and add their own knowledge and thoughts. Suppose $f:[a,b]\to \mathbb{R}$ is a positive, continuous function. The $L^p$ norms of $f$ are the analogs to the weighted power means of a group of positive numbers $a_1, \dotsc, a_n$. (Perhaps it would be more precise to say that the expressions $\left(\int_a^b\frac{f^p}{b-a}dx\right)^{1/p}$ are the analogs.) The power means are given by $$M_p(a_1, \dotsc, a_n)=(w_1a_1^p + \dotsb + w_na_n^p)^{1/p},$$ where $w_i$ such that $\sum w_i =1$ are any weights we like (the simplest case is when $w_i=1/n$ for all $i$). It is well-known (see here ) that the power means $M_p$ tend to the geometric mean as $p\to 0$. What does $$\left( \frac{\int_a^b f(x)^p}{(b-a)}\right)^{1/p}$$ tend to as $p\to 0$?","This question already has answers here : Limit of $L^p$ norm when $p\to0$ (4 answers) Closed 5 years ago . This is something I was thinking about, so I'm going to post it as a question and post my own answer. I hope that anyone who wants will comment, correct me if I'm wrong, and add their own knowledge and thoughts. Suppose $f:[a,b]\to \mathbb{R}$ is a positive, continuous function. The $L^p$ norms of $f$ are the analogs to the weighted power means of a group of positive numbers $a_1, \dotsc, a_n$. (Perhaps it would be more precise to say that the expressions $\left(\int_a^b\frac{f^p}{b-a}dx\right)^{1/p}$ are the analogs.) The power means are given by $$M_p(a_1, \dotsc, a_n)=(w_1a_1^p + \dotsb + w_na_n^p)^{1/p},$$ where $w_i$ such that $\sum w_i =1$ are any weights we like (the simplest case is when $w_i=1/n$ for all $i$). It is well-known (see here ) that the power means $M_p$ tend to the geometric mean as $p\to 0$. What does $$\left( \frac{\int_a^b f(x)^p}{(b-a)}\right)^{1/p}$$ tend to as $p\to 0$?",,"['calculus', 'integration', 'lp-spaces', 'average']"
69,Integrating $\exp (\exp (x))$,Integrating,\exp (\exp (x)),How am I able to integrate $e^{e^x}$? $$\int e^{e^x}dx$$ Am I suppose to use $u$ substitution?  But what should I let $x$ be?  And what should $dx$ be? Thanks for the help!,How am I able to integrate $e^{e^x}$? $$\int e^{e^x}dx$$ Am I suppose to use $u$ substitution?  But what should I let $x$ be?  And what should $dx$ be? Thanks for the help!,,"['calculus', 'integration']"
70,Compute $\int_0^1 \frac{\mathrm{d}x}{\sqrt{1-x^4}} \cdot \int_0^1 \frac{x^2\mathrm{d}x}{\sqrt{1-x^4}}$,Compute,\int_0^1 \frac{\mathrm{d}x}{\sqrt{1-x^4}} \cdot \int_0^1 \frac{x^2\mathrm{d}x}{\sqrt{1-x^4}},"Compute $$\int_0^1 \dfrac{\mathrm{d}x}{\sqrt{1-x^4}} \cdot \int_0^1 \dfrac{x^2\mathrm{d}x}{\sqrt{1-x^4}}$$ . The primitive function of each one seems not elementary function. I tried like this $$ \int_0^1 \dfrac{\mathrm{d}x}{\sqrt{1-x^4}} + \int_0^1 \dfrac{x^2\mathrm{d}x}{\sqrt{1-x^4}}=\int_0^1\dfrac{\mathrm{d}(x-\frac{1}{x})}{\sqrt{\frac{1}{x^2}-x^2}}, $$ but it still does not work. Actually, I can compute it by Gamma and Beta function. But I wonder is there other approach? Appreciate any help!","Compute . The primitive function of each one seems not elementary function. I tried like this but it still does not work. Actually, I can compute it by Gamma and Beta function. But I wonder is there other approach? Appreciate any help!","\int_0^1 \dfrac{\mathrm{d}x}{\sqrt{1-x^4}} \cdot \int_0^1 \dfrac{x^2\mathrm{d}x}{\sqrt{1-x^4}} 
\int_0^1 \dfrac{\mathrm{d}x}{\sqrt{1-x^4}} + \int_0^1 \dfrac{x^2\mathrm{d}x}{\sqrt{1-x^4}}=\int_0^1\dfrac{\mathrm{d}(x-\frac{1}{x})}{\sqrt{\frac{1}{x^2}-x^2}},
","['calculus', 'integration', 'definite-integrals']"
71,Show convergence of series $\sum_{k=1}^{\infty}\frac{k!}{k^k} $,Show convergence of series,\sum_{k=1}^{\infty}\frac{k!}{k^k} ,"I want to prove that $\sum_{k=1}^{\infty}\frac{k!}{k^k} $ converges. My idea is that, for any integer $k \ge 1 $ , we have \begin{align*} \frac{k!}{k^k} &= \frac{1}{k}\cdot\frac{2}{k}\cdots\frac{k-2}{k}\cdot\frac{k-1}{k}\cdot\frac{k}{k} \\ &= \frac{1}{k}\cdot\frac{2}{k}\cdots\frac{k-2}{k}\cdot\frac{k-1}{k}\\ &\leq \frac{1}{k}\cdot\frac{2}{k}\cdots\cdot\frac{k-2}{k} \\ &\qquad \vdots \\ &\leq \frac{1}{k}\cdot\frac{2}{k}  = \frac{2}{k^2} \end{align*} That is $$\sum_{k=1}^{\infty}\frac{k!}{k^k} \leq \sum_{k=1}^{\infty}\frac{2}{k^2} $$ And since right-hand side of the inequality is finite, so is left-hand side and therefore the series is convergent. However, I dont find this way of solving the assignment elegant and I believe there is a cleaner way. Appreciates all help I can get.","I want to prove that converges. My idea is that, for any integer , we have That is And since right-hand side of the inequality is finite, so is left-hand side and therefore the series is convergent. However, I dont find this way of solving the assignment elegant and I believe there is a cleaner way. Appreciates all help I can get.","\sum_{k=1}^{\infty}\frac{k!}{k^k}  k \ge 1  \begin{align*}
\frac{k!}{k^k}
&= \frac{1}{k}\cdot\frac{2}{k}\cdots\frac{k-2}{k}\cdot\frac{k-1}{k}\cdot\frac{k}{k} \\
&= \frac{1}{k}\cdot\frac{2}{k}\cdots\frac{k-2}{k}\cdot\frac{k-1}{k}\\
&\leq \frac{1}{k}\cdot\frac{2}{k}\cdots\cdot\frac{k-2}{k} \\
&\qquad \vdots \\
&\leq \frac{1}{k}\cdot\frac{2}{k}
 = \frac{2}{k^2}
\end{align*} \sum_{k=1}^{\infty}\frac{k!}{k^k} \leq \sum_{k=1}^{\infty}\frac{2}{k^2} ","['calculus', 'sequences-and-series']"
72,"If $\int_0^x f^2(t)dt \le f(x)$ for all $x \in [0,1]$, then $\min_{[0,1]} f(x) \le 1$?","If  for all , then ?","\int_0^x f^2(t)dt \le f(x) x \in [0,1] \min_{[0,1]} f(x) \le 1","Suppose that $f$ is a continuous function on $[0,1]$ and $$\int_0^x [f(t)]^2dt \le f(x) \quad \text{for all} \quad x \in[0,1].$$ Prove or disprove $$\min_{0\le x\le 1} f(x) \le 1.$$ In case the desired inequality does not hold, what is the best upper bound? Thanks.","Suppose that is a continuous function on and Prove or disprove In case the desired inequality does not hold, what is the best upper bound? Thanks.","f [0,1] \int_0^x [f(t)]^2dt \le f(x) \quad \text{for all} \quad x \in[0,1]. \min_{0\le x\le 1} f(x) \le 1.",['calculus']
73,The limit of $\frac{n^3-3}{2n^2+n-1}$,The limit of,\frac{n^3-3}{2n^2+n-1},"I have to find the limit of the sequence above. Firstly, I tried to multiply out $n^3$, as it has the largest exponent. $$\lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} =  \lim_{n\to\infty}\frac{n^3(1-\frac{3}{n^3})}{n^3(\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3})} =  \lim_{n\to\infty}\frac{1-\frac{3}{n^3}}{\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3}}$$ $$ \begin{align} \lim_{n\to\infty}1-\frac{3}{n^3} = 1 \\[1ex] \lim_{n\to\infty}\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3} = 0 \\[1ex] \lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \frac{1}{0} \end{align} $$ Then, after realizing $\frac{1}{0}$ might not be a plausible limit, I tried to multiply out the variable with the largest exponent in both the dividend and the divisor. $$\lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \lim_{n\to\infty}\frac{n^3(1 - \frac{3}{n^3})}{n^2(2 + \frac{1}{n} - \frac{1}{n^2})} =  \lim_{n\to\infty}n\cdot\frac{1 - \frac{3}{n^3}}{2 + \frac{1}{n} - \frac{1}{n^2}}$$ $$ \begin{align} \lim_{n\to\infty}1-\frac{3}{n^3} = 1 \\ \lim_{n\to\infty}2 + \frac{1}{n} - \frac{1}{n^2} = 2 \\ \lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \frac{1}{2} \\ \lim_{n\to\infty}n = \infty \end{align} $$ So, my questions about this problem: Could $\frac{1}{0}$ be a valid limit? Does $\infty\cdot\frac{1}{2}$ equal to $\infty$? In conclusion, what is the limit of the sequence above? $\infty?$ Thank you!","I have to find the limit of the sequence above. Firstly, I tried to multiply out $n^3$, as it has the largest exponent. $$\lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} =  \lim_{n\to\infty}\frac{n^3(1-\frac{3}{n^3})}{n^3(\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3})} =  \lim_{n\to\infty}\frac{1-\frac{3}{n^3}}{\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3}}$$ $$ \begin{align} \lim_{n\to\infty}1-\frac{3}{n^3} = 1 \\[1ex] \lim_{n\to\infty}\frac{2}{n} + \frac{1}{n^2} - \frac{1}{n^3} = 0 \\[1ex] \lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \frac{1}{0} \end{align} $$ Then, after realizing $\frac{1}{0}$ might not be a plausible limit, I tried to multiply out the variable with the largest exponent in both the dividend and the divisor. $$\lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \lim_{n\to\infty}\frac{n^3(1 - \frac{3}{n^3})}{n^2(2 + \frac{1}{n} - \frac{1}{n^2})} =  \lim_{n\to\infty}n\cdot\frac{1 - \frac{3}{n^3}}{2 + \frac{1}{n} - \frac{1}{n^2}}$$ $$ \begin{align} \lim_{n\to\infty}1-\frac{3}{n^3} = 1 \\ \lim_{n\to\infty}2 + \frac{1}{n} - \frac{1}{n^2} = 2 \\ \lim_{n\to\infty}\frac{n^3-3}{2n^2+n-1} = \frac{1}{2} \\ \lim_{n\to\infty}n = \infty \end{align} $$ So, my questions about this problem: Could $\frac{1}{0}$ be a valid limit? Does $\infty\cdot\frac{1}{2}$ equal to $\infty$? In conclusion, what is the limit of the sequence above? $\infty?$ Thank you!",,"['calculus', 'limits', 'infinity']"
74,How to prove that $\int_{0}^{\infty}{\sin^4(x)\ln(x)}\cdot{\mathrm dx\over x^2}={\pi\over 4}\cdot(1-\gamma)?$,How to prove that,\int_{0}^{\infty}{\sin^4(x)\ln(x)}\cdot{\mathrm dx\over x^2}={\pi\over 4}\cdot(1-\gamma)?,"How to prove that   $$\int_{0}^{\infty}{\sin^4(x)\ln(x)}\cdot{\mathrm dx\over x^2}={\pi\over 4}\cdot(1-\gamma).\tag1$$ Here is my attempt: $$I(a)=\int_{0}^{\infty}{\ln(x)\sin^4(x)\over x^a}\,\mathrm dx\tag2$$ $$I'(a)=\int_{0}^{\infty}{\sin^4(x)\over x^a}\,\mathrm dx\tag3$$ $$I'(2)=\int_{0}^{\infty}{\sin^4(x)\over x^2}\,\mathrm dx\tag4$$ $$I'(2)=\int_{0}^{\infty}{\sin^2(x)\over x^2}\,\mathrm dx-{1\over 4}\int_{0}^{\infty}{\sin^2(2x)\over x^2}\,\mathrm dx=-{\pi\over 2}\tag5$$ Why is this way wrong? How to prove (1)?","How to prove that   $$\int_{0}^{\infty}{\sin^4(x)\ln(x)}\cdot{\mathrm dx\over x^2}={\pi\over 4}\cdot(1-\gamma).\tag1$$ Here is my attempt: $$I(a)=\int_{0}^{\infty}{\ln(x)\sin^4(x)\over x^a}\,\mathrm dx\tag2$$ $$I'(a)=\int_{0}^{\infty}{\sin^4(x)\over x^a}\,\mathrm dx\tag3$$ $$I'(2)=\int_{0}^{\infty}{\sin^4(x)\over x^2}\,\mathrm dx\tag4$$ $$I'(2)=\int_{0}^{\infty}{\sin^2(x)\over x^2}\,\mathrm dx-{1\over 4}\int_{0}^{\infty}{\sin^2(2x)\over x^2}\,\mathrm dx=-{\pi\over 2}\tag5$$ Why is this way wrong? How to prove (1)?",,"['calculus', 'integration', 'improper-integrals']"
75,Asymptotic estimation problem about $\sum\limits_{j = 1}^n {\sum\limits_{i = 1}^n {\frac{{i + j}}{{{i^2} + {j^2}}}} } $,Asymptotic estimation problem about,\sum\limits_{j = 1}^n {\sum\limits_{i = 1}^n {\frac{{i + j}}{{{i^2} + {j^2}}}} } ,"How to get$$\mathop {\lim }\limits_{n \to \infty } n\left( {\frac{\pi }{2} + \ln 2 - \frac{1}{n}\sum\limits_{j = 1}^n {\sum\limits_{i = 1}^n {\frac{{i + j}}{{{i^2} + {j^2}}}} } } \right).$$ I think we can use Euler–Maclaurin formula $$\sum_{n=a}^b f(n) \sim \int_a^b f(x)\,\mathrm{d}x + \frac{f(b) + f(a)}{2} + \sum_{k=1}^\infty \frac{B_{2k}}{(2k)!} \left(f^{(2k - 1)}(b) - f^{(2k - 1)}(a)\right),$$ where $a,b$ are both integers. But it seems difficult because of the double summation!","How to get$$\mathop {\lim }\limits_{n \to \infty } n\left( {\frac{\pi }{2} + \ln 2 - \frac{1}{n}\sum\limits_{j = 1}^n {\sum\limits_{i = 1}^n {\frac{{i + j}}{{{i^2} + {j^2}}}} } } \right).$$ I think we can use Euler–Maclaurin formula $$\sum_{n=a}^b f(n) \sim \int_a^b f(x)\,\mathrm{d}x + \frac{f(b) + f(a)}{2} + \sum_{k=1}^\infty \frac{B_{2k}}{(2k)!} \left(f^{(2k - 1)}(b) - f^{(2k - 1)}(a)\right),$$ where $a,b$ are both integers. But it seems difficult because of the double summation!",,"['calculus', 'integration', 'analysis', 'limits']"
76,"Prove that if integral of a squared function is zero, then function is zero function","Prove that if integral of a squared function is zero, then function is zero function",,"I almost got this proof done but I can't seem to justify a little step. It goes: Let $f$ be a real-valued, continuous function on $[a,b]$. Prove that if $$\int_a^b [f(x)]²\ dx = 0$$ then $$f(x)=0 \,\,\,\,\,\,\,\,\forall x \in [a,b]$$ I start by defining $$F(x) = \int_a^x [f(t)]²\ dt $$ Since $f$ is continuous, then $F'(x)=[f(x)]²\ge0 \;\;\;\;\; \forall x \in [a,b]$. Thus $F$ is increasing in $[a,b]$. It's clear that $F(a)=0$ and by hypothesis $F(b)=0$. If I could justify why this means that $F$ must be a constant function in $[a,b]$ then my proof would be completed, since that would mean $F'(x)=0\;\;\forall x\in[a,b]$ and therefore $f(x)=0\;\;\forall x\in[a,b]$. Could anyone please tell me if there's a theorem or anything that would let me justify the key step???","I almost got this proof done but I can't seem to justify a little step. It goes: Let $f$ be a real-valued, continuous function on $[a,b]$. Prove that if $$\int_a^b [f(x)]²\ dx = 0$$ then $$f(x)=0 \,\,\,\,\,\,\,\,\forall x \in [a,b]$$ I start by defining $$F(x) = \int_a^x [f(t)]²\ dt $$ Since $f$ is continuous, then $F'(x)=[f(x)]²\ge0 \;\;\;\;\; \forall x \in [a,b]$. Thus $F$ is increasing in $[a,b]$. It's clear that $F(a)=0$ and by hypothesis $F(b)=0$. If I could justify why this means that $F$ must be a constant function in $[a,b]$ then my proof would be completed, since that would mean $F'(x)=0\;\;\forall x\in[a,b]$ and therefore $f(x)=0\;\;\forall x\in[a,b]$. Could anyone please tell me if there's a theorem or anything that would let me justify the key step???",,"['calculus', 'integration', 'functions', 'definite-integrals']"
77,How to solve this limit: $\lim_{x\to\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1}$ [duplicate],How to solve this limit:  [duplicate],\lim_{x\to\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1},This question already has answers here : Find the value of : $\lim_{x\to\infty} \sqrt{x+\sqrt{x}}-\sqrt{x}$ (3 answers) Closed 8 years ago . Compute the limit $\lim\limits_{x\to+\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1}$ my attempt: I tried to multiply top and bottom by the conjugate $$\begin{align} \lim_{x\to+\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1}&=\lim_{x\to+\infty}\left(\sqrt{x+\sqrt{x}}-\sqrt{x-1}\right)\frac{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{\left(\sqrt{x+\sqrt{x}}\right)^2-\left(\sqrt{x-1}\right)^2}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{(x+\sqrt{x})-(x-1)}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{1+\sqrt{x}}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}} \end{align}$$ But I don't know what I can do after this.,This question already has answers here : Find the value of : $\lim_{x\to\infty} \sqrt{x+\sqrt{x}}-\sqrt{x}$ (3 answers) Closed 8 years ago . Compute the limit $\lim\limits_{x\to+\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1}$ my attempt: I tried to multiply top and bottom by the conjugate $$\begin{align} \lim_{x\to+\infty}\sqrt{x+\sqrt{x}}-\sqrt{x-1}&=\lim_{x\to+\infty}\left(\sqrt{x+\sqrt{x}}-\sqrt{x-1}\right)\frac{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{\left(\sqrt{x+\sqrt{x}}\right)^2-\left(\sqrt{x-1}\right)^2}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{(x+\sqrt{x})-(x-1)}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}}\\ &=\lim_{x\to+\infty}\frac{1+\sqrt{x}}{\sqrt{x+\sqrt{x}}+\sqrt{x-1}} \end{align}$$ But I don't know what I can do after this.,,"['calculus', 'limits', 'radicals']"
78,"$\int^{\infty}_{-\infty}u(x,y) \,d y$ independent of x",independent of x,"\int^{\infty}_{-\infty}u(x,y) \,d y","I need to prove that $$I = \int^{\infty}_{-\infty}u(x,y) \,dy$$ is independent of $x$ and find its value, where $$u(x,y) = \frac{1}{2\pi}\exp\left(+x^2/2-y^2/2\right)K_0\left(\sqrt{(x-y)^2+(-x^2/2+y^2/2)^2}\right)$$ and $K_0$ is the modified Bessel function of the second kind with order zero. Evaluating the integral numerically with Mathematica for different values of $x$ gives the result of $2.38$ , but I want to know if it is possible to show analytically. Increasing $x$ results in an increase of the exponential term on the left, but it also then strongly increases the argument of modified Bessel function, thus reducing its value. To show that integral is independant of $x$ , it is sufficient to show that $\int^{\infty}_{-\infty}\frac{\, d}{\, dx}u(x,y) = 0$ but any differentiation looks more and more ugly. EDIT Mathematica test: x = 100     NIntegrate[     (1/(2 Pi))* Exp[x*x/2 - y*y/2] BesselK[0,         Sqrt[(x - y)*(x - y) + (x*x/2 - y*y/2)*(x*x/2 -              y*y/2 )]], {y, -Infinity, x, Infinity}, MaxRecursion -> 22] This gives an answer of $0.378936$ independent of the choice of $x$ . In the earlier calculation I missed the factor $\frac{1}{2\pi}$ .","I need to prove that is independent of and find its value, where and is the modified Bessel function of the second kind with order zero. Evaluating the integral numerically with Mathematica for different values of gives the result of , but I want to know if it is possible to show analytically. Increasing results in an increase of the exponential term on the left, but it also then strongly increases the argument of modified Bessel function, thus reducing its value. To show that integral is independant of , it is sufficient to show that but any differentiation looks more and more ugly. EDIT Mathematica test: x = 100     NIntegrate[     (1/(2 Pi))* Exp[x*x/2 - y*y/2] BesselK[0,         Sqrt[(x - y)*(x - y) + (x*x/2 - y*y/2)*(x*x/2 -              y*y/2 )]], {y, -Infinity, x, Infinity}, MaxRecursion -> 22] This gives an answer of independent of the choice of . In the earlier calculation I missed the factor .","I = \int^{\infty}_{-\infty}u(x,y) \,dy x u(x,y) = \frac{1}{2\pi}\exp\left(+x^2/2-y^2/2\right)K_0\left(\sqrt{(x-y)^2+(-x^2/2+y^2/2)^2}\right) K_0 x 2.38 x x \int^{\infty}_{-\infty}\frac{\, d}{\, dx}u(x,y) = 0 0.378936 x \frac{1}{2\pi}","['calculus', 'integration', 'analysis', 'definite-integrals', 'bessel-functions']"
79,Compute $\sum\limits_{n = 1}^{\infty} \frac{1}{n^4}$.,Compute .,\sum\limits_{n = 1}^{\infty} \frac{1}{n^4},"Compute the Fourier series for $x^3$ and use it to compute the value of $\sum\limits_{n = 1}^{\infty} \frac{1}{n^4}$. I determined the coefficients of the Fourier series, which are $$a_0 = \dfrac{\pi^3}{2}; \qquad a_n = \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}$$ Then, I get $$x^3 = \dfrac{\pi^3}{4} + \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}\cos(nx)$$ If $x = \pi$, then $$\begin{aligned} \pi^3 &= \dfrac{\pi^3}{4} + \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}\cos(n\pi)\\ \dfrac{3\pi^3}{4} &= \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}(-1)^n \end{aligned}$$ I'm stuck.  It's easy to compute $\sum\limits_{n = 1}^{\infty} \frac{1}{n^2}$, using the Fourier series, but for this type of problem I'm stuck. Any comments or suggestions?  By the way, I know that $$\sum\limits_{n = 1}^{\infty} \dfrac{1}{n^4} = \dfrac{\pi^4}{90}$$ I need to know how to get there.","Compute the Fourier series for $x^3$ and use it to compute the value of $\sum\limits_{n = 1}^{\infty} \frac{1}{n^4}$. I determined the coefficients of the Fourier series, which are $$a_0 = \dfrac{\pi^3}{2}; \qquad a_n = \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}$$ Then, I get $$x^3 = \dfrac{\pi^3}{4} + \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}\cos(nx)$$ If $x = \pi$, then $$\begin{aligned} \pi^3 &= \dfrac{\pi^3}{4} + \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}\cos(n\pi)\\ \dfrac{3\pi^3}{4} &= \sum\limits_{n = 1}^{\infty} \dfrac{6(\pi^2 n^2 - 2)(-1)^n + 12}{\pi n^4}(-1)^n \end{aligned}$$ I'm stuck.  It's easy to compute $\sum\limits_{n = 1}^{\infty} \frac{1}{n^2}$, using the Fourier series, but for this type of problem I'm stuck. Any comments or suggestions?  By the way, I know that $$\sum\limits_{n = 1}^{\infty} \dfrac{1}{n^4} = \dfrac{\pi^4}{90}$$ I need to know how to get there.",,"['calculus', 'sequences-and-series', 'partial-differential-equations', 'fourier-series']"
80,Find asymptotic of recurrence sequence,Find asymptotic of recurrence sequence,,"Given a sequence $x_1=\frac{1}{2}$, $x_{n+1}=x_n-x_n^2$. It's easy to see that it limits to $0$. The question is: is there exists an $\alpha$ such, that $\lim\limits_{n\to\infty}n^\alpha x_n\neq0$. I tried to find explicit formula for $x_n$, but did not succeed. I stucked at that point: if $a_1=(\frac{1}{4})^2$, $a_n=(a_{n-1}+\frac{1}{4})^2$, then $x_n=-a_{n-2}+\frac{1}{4}$ for $n\ge3$. Thanks for any ideas and help.","Given a sequence $x_1=\frac{1}{2}$, $x_{n+1}=x_n-x_n^2$. It's easy to see that it limits to $0$. The question is: is there exists an $\alpha$ such, that $\lim\limits_{n\to\infty}n^\alpha x_n\neq0$. I tried to find explicit formula for $x_n$, but did not succeed. I stucked at that point: if $a_1=(\frac{1}{4})^2$, $a_n=(a_{n-1}+\frac{1}{4})^2$, then $x_n=-a_{n-2}+\frac{1}{4}$ for $n\ge3$. Thanks for any ideas and help.",,"['calculus', 'sequences-and-series', 'limits', 'asymptotics', 'recurrence-relations']"
81,Prove $\frac{1}{1 \cdot 3} + \frac{1}{3 \cdot 5} + \frac{1}{5 \cdot 7} + \cdots$ converges to $\frac 1 2 $,Prove  converges to,\frac{1}{1 \cdot 3} + \frac{1}{3 \cdot 5} + \frac{1}{5 \cdot 7} + \cdots \frac 1 2 ,"Show that $$\frac{1}{1 \cdot 3} + \frac{1}{3 \cdot 5} + \frac{1}{5 \cdot 7} + \cdots = \frac{1}{2}.$$ I'm not exactly sure what to do here, it seems awfully similar to Zeno's paradox. If the series continues infinitely then each term is just going to get smaller and smaller. Is this an example where I should be making a Riemann sum and then taking the limit which would end up being $1/2$?","Show that $$\frac{1}{1 \cdot 3} + \frac{1}{3 \cdot 5} + \frac{1}{5 \cdot 7} + \cdots = \frac{1}{2}.$$ I'm not exactly sure what to do here, it seems awfully similar to Zeno's paradox. If the series continues infinitely then each term is just going to get smaller and smaller. Is this an example where I should be making a Riemann sum and then taking the limit which would end up being $1/2$?",,"['calculus', 'sequences-and-series', 'telescopic-series']"
82,Equality with Euler–Mascheroni constant,Equality with Euler–Mascheroni constant,,"While trying to prove $\int_0^{\infty } \frac{\log (x)}{e^x+1} \, dx = -\frac{1}{2} \log ^2(2)$ How to show? in an alternative way, I came to this solution: $$\sum_{k=0}^{+\infty}(-1)^{k+1}\frac{\log (k+1)+\gamma }{(k+1)}.$$ As both solutions have to be the same, the following equality should be valid: $$\sum_{k=0}^{+\infty}(-1)^{k+1}\frac{\log (k+1)+\gamma }{(k+1)}=- \frac{1}{2}{{\log }^2(2)}. $$ Can anyone give me some advice on how to prove this equality. p.s. You can be sure that the equality is correct, as I checked it numerically.","While trying to prove $\int_0^{\infty } \frac{\log (x)}{e^x+1} \, dx = -\frac{1}{2} \log ^2(2)$ How to show? in an alternative way, I came to this solution: $$\sum_{k=0}^{+\infty}(-1)^{k+1}\frac{\log (k+1)+\gamma }{(k+1)}.$$ As both solutions have to be the same, the following equality should be valid: $$\sum_{k=0}^{+\infty}(-1)^{k+1}\frac{\log (k+1)+\gamma }{(k+1)}=- \frac{1}{2}{{\log }^2(2)}. $$ Can anyone give me some advice on how to prove this equality. p.s. You can be sure that the equality is correct, as I checked it numerically.",,"['calculus', 'power-series', 'euler-mascheroni-constant']"
83,Derive $\frac{d}{dx} \left[\sin^{-1} x\right] = \frac{1}{\sqrt{1-x^2}}$,Derive,\frac{d}{dx} \left[\sin^{-1} x\right] = \frac{1}{\sqrt{1-x^2}},"Derive $\frac{d}{dx} \left[\sin^{-1} x\right] = \frac{1}{\sqrt{1-x^2}}$ (Hint: set $x = \sin y$ and use implicit differentiation) So, I tried to use the hint and I got: $x = \sin y$ $\frac{d}{dx}\left[x\right] = \sin y\frac{d}{dx}$ $\frac{dx}{dx} = \cos y \frac{dy}{dx}$ $\frac{dy}{dx} = \frac{1}{\cos y}$ $\frac{dy}{dx} = \sec y$ From here I need a little help. Did I do the implicit differentiation correctly? How do I use this to help with the original question?","Derive $\frac{d}{dx} \left[\sin^{-1} x\right] = \frac{1}{\sqrt{1-x^2}}$ (Hint: set $x = \sin y$ and use implicit differentiation) So, I tried to use the hint and I got: $x = \sin y$ $\frac{d}{dx}\left[x\right] = \sin y\frac{d}{dx}$ $\frac{dx}{dx} = \cos y \frac{dy}{dx}$ $\frac{dy}{dx} = \frac{1}{\cos y}$ $\frac{dy}{dx} = \sec y$ From here I need a little help. Did I do the implicit differentiation correctly? How do I use this to help with the original question?",,"['calculus', 'trigonometry']"
84,"If $f$ continuous and $f(x^2) = f(x)$, then $f$ is a const","If  continuous and , then  is a const",f f(x^2) = f(x) f,"Problem: Given $f:[0,1] \rightarrow \mathbb{R}$ ($f$ continuous ) and $f(x^2) = f(x)$ $\forall x \in [0,1]$. Show that function $f$ is a const.","Problem: Given $f:[0,1] \rightarrow \mathbb{R}$ ($f$ continuous ) and $f(x^2) = f(x)$ $\forall x \in [0,1]$. Show that function $f$ is a const.",,['calculus']
85,"Integral from yesterdays test: $\int_0^\pi {{1+\sin^2x}\over{6-\cos^2x+\left|\cos x\right|}} \sin x \cdot x \, dx$",Integral from yesterdays test:,"\int_0^\pi {{1+\sin^2x}\over{6-\cos^2x+\left|\cos x\right|}} \sin x \cdot x \, dx","This was the integral we had on the test yesterday: $$\int_0^\pi {{1+\sin^2x}\over{6-\cos^2x+\left|\cos x\right|}} \sin x \cdot x \, dx$$ None of my friends including me managed to solve it. Does anyone know how to solve this integral? I tried literally everything but couldn't manage to solve it ... By the way, do you think that this integral is way too hard to be on the test? (First year of college, 90 min test )","This was the integral we had on the test yesterday: $$\int_0^\pi {{1+\sin^2x}\over{6-\cos^2x+\left|\cos x\right|}} \sin x \cdot x \, dx$$ None of my friends including me managed to solve it. Does anyone know how to solve this integral? I tried literally everything but couldn't manage to solve it ... By the way, do you think that this integral is way too hard to be on the test? (First year of college, 90 min test )",,"['calculus', 'integration', 'definite-integrals']"
86,Solve $\int_{0}^{1} \log(x)\log(1-x) dx$ without convolution,Solve  without convolution,\int_{0}^{1} \log(x)\log(1-x) dx,"Maybe it's too much to ask for, but is there a way to solve $\int \limits_{0}^{1} \log(x)\log(1-x) dx$ without convolution? Note that $\log x =\log_e x$.","Maybe it's too much to ask for, but is there a way to solve $\int \limits_{0}^{1} \log(x)\log(1-x) dx$ without convolution? Note that $\log x =\log_e x$.",,"['calculus', 'integration', 'definite-integrals', 'logarithms']"
87,Why does the higher order derivative test work?,Why does the higher order derivative test work?,,"I'm an AP Calculus BC student, so all I know about derivatives is the increasing/decreasing/ relative max/min function relation with first derivative (first derivative test), concavity (second derivative test). I showed that you can create a third derivative test to my teacher (by showing that if the critical numbers of the second derivative = 0, and if the third derivative is non-zero, then it must be a point of inflection as the function is either increasing or decreasing on the second derivative graph), and he said it was right and to think about derivatives at higher orders like x^5 only has a derivative at x = 0 with the fifth derivative). When i googled it, i found the higher order derivative test. Is there a logical explanation for why this test works? I dont understand why if n-1 derivatives = 0 and the nth is non-zero, you can find max/mins/inflection points.","I'm an AP Calculus BC student, so all I know about derivatives is the increasing/decreasing/ relative max/min function relation with first derivative (first derivative test), concavity (second derivative test). I showed that you can create a third derivative test to my teacher (by showing that if the critical numbers of the second derivative = 0, and if the third derivative is non-zero, then it must be a point of inflection as the function is either increasing or decreasing on the second derivative graph), and he said it was right and to think about derivatives at higher orders like x^5 only has a derivative at x = 0 with the fifth derivative). When i googled it, i found the higher order derivative test. Is there a logical explanation for why this test works? I dont understand why if n-1 derivatives = 0 and the nth is non-zero, you can find max/mins/inflection points.",,['calculus']
88,"Prove $1.43 < \int_0^1 e^{x^2}\,\mathrm{d}x < \frac{e+1}{2}$",Prove,"1.43 < \int_0^1 e^{x^2}\,\mathrm{d}x < \frac{e+1}{2}","Prove $$1.43<\int_0^1 e^{x^2}\,\mathrm{d}x<\frac{e+1}{2}$$ What I did: As I have no idea how to approach the left inequality I work with $$\int_0^1 e^{x^2} \mathrm{d}x<\frac{e+1}{2} \iff \int_0^1 e^{x^2} \mathrm{d}x<\int_0^1 \frac12 (e+x)\mathrm{d}x \iff e^{x^2}<\frac12(e+x) \iff x^2<\log (e+x)-\log 2$$ I don't know how to proceed.",Prove What I did: As I have no idea how to approach the left inequality I work with I don't know how to proceed.,"1.43<\int_0^1 e^{x^2}\,\mathrm{d}x<\frac{e+1}{2} \int_0^1 e^{x^2} \mathrm{d}x<\frac{e+1}{2} \iff \int_0^1 e^{x^2} \mathrm{d}x<\int_0^1 \frac12 (e+x)\mathrm{d}x \iff e^{x^2}<\frac12(e+x) \iff x^2<\log (e+x)-\log 2","['calculus', 'inequality', 'integral-inequality']"
89,Is it true that $\left\lfloor\sum_{s=1}^n\operatorname{Li}_s\left(\frac 1k \right)\right\rfloor\stackrel{?}{=}\left\lfloor\frac nk \right\rfloor$,Is it true that,\left\lfloor\sum_{s=1}^n\operatorname{Li}_s\left(\frac 1k \right)\right\rfloor\stackrel{?}{=}\left\lfloor\frac nk \right\rfloor,"While studying polylogarithms I observed the following. Let $n>0$ and $k>1$ be integers. Is the following statement true? $$\left\lfloor \sum_{s=1}^n \operatorname{Li}_s\left( \frac{1}{k} \right) \right\rfloor \stackrel{?}{=} \left\lfloor \frac{n}{k} \right\rfloor $$ If it is, then how could we prove it? If not, give a counterexample.","While studying polylogarithms I observed the following. Let $n>0$ and $k>1$ be integers. Is the following statement true? $$\left\lfloor \sum_{s=1}^n \operatorname{Li}_s\left( \frac{1}{k} \right) \right\rfloor \stackrel{?}{=} \left\lfloor \frac{n}{k} \right\rfloor $$ If it is, then how could we prove it? If not, give a counterexample.",,"['calculus', 'summation', 'ceiling-and-floor-functions', 'polylogarithm']"
90,Intuitive and convincing argument that functions are vectors,Intuitive and convincing argument that functions are vectors,,"Back to school time again. As I'm discussing all the mathy stuff and insights gained over the summer, I cannot help but notice that many of my peers in second or third year undergrad cannot bridge the gap between vectors and functions. When I ask them why, the answer I most often get are as follows: Vectors are something learned in linear algebra, and they are basically pointy arrows in $R^2$ and written with brackets $\{\}$ or sometimes with arrow on the top Functions are completely different, they are the object under study in calculus, never drawn as an arrow, doesn't satisfy some axioms of vector space, can be drawn in a lot of ways (piecewise or continuous, with crazy oscillations). You can do a lot with functions, such as taking the derivative of it, or the integral of it. You can find the inverse of a function. No authority has ever said functions are vectors, if they did I would believe them Yes, functions are sometimes contained in brackets, but that is just a vector of functions, not a function Can someone please provide a good example may bridge this gap? Also, is there any books that explicitly bridge the two concepts? Thank you for your inputs!","Back to school time again. As I'm discussing all the mathy stuff and insights gained over the summer, I cannot help but notice that many of my peers in second or third year undergrad cannot bridge the gap between vectors and functions. When I ask them why, the answer I most often get are as follows: Vectors are something learned in linear algebra, and they are basically pointy arrows in $R^2$ and written with brackets $\{\}$ or sometimes with arrow on the top Functions are completely different, they are the object under study in calculus, never drawn as an arrow, doesn't satisfy some axioms of vector space, can be drawn in a lot of ways (piecewise or continuous, with crazy oscillations). You can do a lot with functions, such as taking the derivative of it, or the integral of it. You can find the inverse of a function. No authority has ever said functions are vectors, if they did I would believe them Yes, functions are sometimes contained in brackets, but that is just a vector of functions, not a function Can someone please provide a good example may bridge this gap? Also, is there any books that explicitly bridge the two concepts? Thank you for your inputs!",,"['calculus', 'linear-algebra', 'functions', 'vectors']"
91,A function satisfying $f(\frac1{x+1})\cdot x=f(x)-1$ and $f(1)=1$?,A function satisfying  and ?,f(\frac1{x+1})\cdot x=f(x)-1 f(1)=1,"$f:[0,\infty)\to\mathbb{R}$ is a continuous function which satisfies $f(1)=1$ and: $$f(\frac1{x+1})\cdot x=f(x)-1$$ Does there exist such a function, if they do, are there infinitely many? And is there any explicit example? I have been able to derive these: $f(1)\cdot0=f(0)-1$, which means $f(0)=1$. $f(\frac1{2})=f(1)-1=0$ Similarly, $f(\frac23)=-2$ and then $f(\frac35)=\frac{-9}2$ and so on. This is going on without any pattern I can see. Is there anything special in the sequence $(1,\frac12,\frac23,...,a_n,...)$ where $a_n=\frac1{a_{n-1}+1}$? I have not been able to use continuity as well. Please help!","$f:[0,\infty)\to\mathbb{R}$ is a continuous function which satisfies $f(1)=1$ and: $$f(\frac1{x+1})\cdot x=f(x)-1$$ Does there exist such a function, if they do, are there infinitely many? And is there any explicit example? I have been able to derive these: $f(1)\cdot0=f(0)-1$, which means $f(0)=1$. $f(\frac1{2})=f(1)-1=0$ Similarly, $f(\frac23)=-2$ and then $f(\frac35)=\frac{-9}2$ and so on. This is going on without any pattern I can see. Is there anything special in the sequence $(1,\frac12,\frac23,...,a_n,...)$ where $a_n=\frac1{a_{n-1}+1}$? I have not been able to use continuity as well. Please help!",,"['calculus', 'functions', 'functional-equations']"
92,$n$th derivative of $e^x \sin x$,th derivative of,n e^x \sin x,"Can someone check this for me, please? The exercise is just to find a expression to the nth derivative of $f(x) = e^x \cdot \sin x$. I have done the following: Write $\sin x = \dfrac{e^{ix} - e^{-ix}}{2i}$, then we have $f(x) = \dfrac{1}{2i} \cdot (e^{(1+i)x} - e^{(1-i)x})$. Taking the derivatives: $f^{(n)}(x) = \dfrac{1}{2i} \cdot ((1+i)^n e^{(1+i)x} - (1-i)^n e^{(1-i)x})$ Now, I use that: $$ (1+i)^n = {\sqrt{2}}^n \cdot \left(\cos\dfrac{n \pi}{4} + i \sin\dfrac{n \pi}{4}\right) \\(1 - i)^n = \sqrt{2}^n \cdot \left( \cos \dfrac{-n \pi}{4} + i \sin \dfrac{-n \pi}{4} \right)$$ Plugging that mess, I get: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot \left(\left(\cos \dfrac{n \pi}{4} + i \sin \dfrac{n \pi}{4}\right) e^{ix} - \left(\cos \dfrac{-n \pi}{4} + i \sin \dfrac{- n \pi}{4}\right) e^{-ix} \right)$$ But, $e^{ix} = \cos x + i \sin x$, and using Moivre's theorem, that makes: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot \left(\cos \left( x + \frac{n \pi}{4}\right) + i \sin \left( x + \frac{n \pi}{4}\right) - \left(\cos \left( - x  - \frac{n \pi}{4}\right) + i \sin \left( -x -\frac{ n \pi}{4}\right)\right)\right)$$ and since $\cos$ is an even function, and $\sin$ is odd, we get: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot 2i \sin \left(x + \dfrac{n \pi}{4}\right)$$ Simplifying, the answer would be $f^{(n)}(x) = e^x \cdot \sqrt{2}^n \cdot \sin\left(x + \dfrac{n \pi}{4}\right)$. I'm almost positive that this is it, but I just want to be sure. Thank you in advance!","Can someone check this for me, please? The exercise is just to find a expression to the nth derivative of $f(x) = e^x \cdot \sin x$. I have done the following: Write $\sin x = \dfrac{e^{ix} - e^{-ix}}{2i}$, then we have $f(x) = \dfrac{1}{2i} \cdot (e^{(1+i)x} - e^{(1-i)x})$. Taking the derivatives: $f^{(n)}(x) = \dfrac{1}{2i} \cdot ((1+i)^n e^{(1+i)x} - (1-i)^n e^{(1-i)x})$ Now, I use that: $$ (1+i)^n = {\sqrt{2}}^n \cdot \left(\cos\dfrac{n \pi}{4} + i \sin\dfrac{n \pi}{4}\right) \\(1 - i)^n = \sqrt{2}^n \cdot \left( \cos \dfrac{-n \pi}{4} + i \sin \dfrac{-n \pi}{4} \right)$$ Plugging that mess, I get: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot \left(\left(\cos \dfrac{n \pi}{4} + i \sin \dfrac{n \pi}{4}\right) e^{ix} - \left(\cos \dfrac{-n \pi}{4} + i \sin \dfrac{- n \pi}{4}\right) e^{-ix} \right)$$ But, $e^{ix} = \cos x + i \sin x$, and using Moivre's theorem, that makes: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot \left(\cos \left( x + \frac{n \pi}{4}\right) + i \sin \left( x + \frac{n \pi}{4}\right) - \left(\cos \left( - x  - \frac{n \pi}{4}\right) + i \sin \left( -x -\frac{ n \pi}{4}\right)\right)\right)$$ and since $\cos$ is an even function, and $\sin$ is odd, we get: $$f^{(n)}(x) = \dfrac{e^x}{2i} \sqrt{2}^n \cdot 2i \sin \left(x + \dfrac{n \pi}{4}\right)$$ Simplifying, the answer would be $f^{(n)}(x) = e^x \cdot \sqrt{2}^n \cdot \sin\left(x + \dfrac{n \pi}{4}\right)$. I'm almost positive that this is it, but I just want to be sure. Thank you in advance!",,"['calculus', 'derivatives', 'complex-numbers']"
93,Integral of $e^{-x^2}\cos(x^2)$ using residues,Integral of  using residues,e^{-x^2}\cos(x^2),"I want to solve the following integral: $$\int_0^{\infty} \!\! \operatorname{e}^{-x^2}\!\cos(x^2) \, \operatorname{d}\!x$$ I have seen this in a section about residues, so my guess is that I would need to compute an appropriate contour integral. However, it does not seems to be like the usual forms of integrals that can be computed using an appropriate complex integral. So I don't know what to do. Any help would be appreciated. Thanks!","I want to solve the following integral: $$\int_0^{\infty} \!\! \operatorname{e}^{-x^2}\!\cos(x^2) \, \operatorname{d}\!x$$ I have seen this in a section about residues, so my guess is that I would need to compute an appropriate contour integral. However, it does not seems to be like the usual forms of integrals that can be computed using an appropriate complex integral. So I don't know what to do. Any help would be appreciated. Thanks!",,"['calculus', 'complex-analysis', 'integration', 'contour-integration']"
94,finding the expansion of $\arcsin(z)^2$,finding the expansion of,\arcsin(z)^2,Is there a fast and nice way to find the expansion of $\arcsin(z)^2$ without squaring expansion of $\arcsin(z)$ ? For $|z|<1$ show that    $$(\sin^{-1}(z))^2 = z^2 + \frac{2}{3}\cdot \frac{z^4}{2} + \frac{2}{3}\cdot\frac{4}{5}\cdot \frac{z^6}{3}+ \frac{2}{3}\cdot\frac{4}{5}\cdot\frac{6}{7}\cdot \frac{z^8}{4} + \dots$$ It should have something like $c_{2n} = \frac{2^{2n}n!^2}{(2n+1)!n}$ as coefficient maybe we could use Residue theorem to evaluate it.,Is there a fast and nice way to find the expansion of $\arcsin(z)^2$ without squaring expansion of $\arcsin(z)$ ? For $|z|<1$ show that    $$(\sin^{-1}(z))^2 = z^2 + \frac{2}{3}\cdot \frac{z^4}{2} + \frac{2}{3}\cdot\frac{4}{5}\cdot \frac{z^6}{3}+ \frac{2}{3}\cdot\frac{4}{5}\cdot\frac{6}{7}\cdot \frac{z^8}{4} + \dots$$ It should have something like $c_{2n} = \frac{2^{2n}n!^2}{(2n+1)!n}$ as coefficient maybe we could use Residue theorem to evaluate it.,,"['calculus', 'trigonometry', 'laurent-series']"
95,$f(x^2) = 2f(x)$ and $f(x)$ continuous,and  continuous,f(x^2) = 2f(x) f(x),"I ran into a problem recently where I obtained the following constraint on a function. $$f(x^2) = 2f(x) \,\,\,\, \forall x \geq 0$$ and the function $f(x)$ is continuous. Can we conclude that $f(x)$ is of following form? $$f(x) = \begin{cases} a\log(x) & \text{for } x \geq 1\\ 0 & \text{for  }x \in [0,1]\end{cases} \tag{$\star$}$$ where $a \in \mathbb{R}$. It is easy to conclude for $x \in [0,1)$ it should be zero, since $f(0) = 0$ and $f(x) = \dfrac{f(x^{2^n})}{2^n}$. Now letting $n \to \infty$, thanks to continuity, we can conclude that $f(x) = 0$ for $x \in [0,1)$. But how do I show $(\star)$ for $x \geq 1$? If not under what further constraints, will I be able to conclude ($\star$)? This is similar to the Cauchy functional equation $f(xy) = f(x) + f(y)$, but with $y=x$.","I ran into a problem recently where I obtained the following constraint on a function. $$f(x^2) = 2f(x) \,\,\,\, \forall x \geq 0$$ and the function $f(x)$ is continuous. Can we conclude that $f(x)$ is of following form? $$f(x) = \begin{cases} a\log(x) & \text{for } x \geq 1\\ 0 & \text{for  }x \in [0,1]\end{cases} \tag{$\star$}$$ where $a \in \mathbb{R}$. It is easy to conclude for $x \in [0,1)$ it should be zero, since $f(0) = 0$ and $f(x) = \dfrac{f(x^{2^n})}{2^n}$. Now letting $n \to \infty$, thanks to continuity, we can conclude that $f(x) = 0$ for $x \in [0,1)$. But how do I show $(\star)$ for $x \geq 1$? If not under what further constraints, will I be able to conclude ($\star$)? This is similar to the Cauchy functional equation $f(xy) = f(x) + f(y)$, but with $y=x$.",,['calculus']
96,How to evaluate the following integral using hypergeometric function?,How to evaluate the following integral using hypergeometric function?,,May I know how this integral was evaluated using  hypergeometric function? $$\int \sin^n x\ dx$$ Wolframalpha showed this result but with no steps Thanks in advance.,May I know how this integral was evaluated using  hypergeometric function? $$\int \sin^n x\ dx$$ Wolframalpha showed this result but with no steps Thanks in advance.,,"['calculus', 'integration', 'special-functions']"
97,"integrating the secant function, who figured this out? [duplicate]","integrating the secant function, who figured this out? [duplicate]",,"This question already has answers here : Tables and histories of methods of finding $\int\sec x\,dx$? (2 answers) Closed 8 months ago . I was looking at how the secant function is integrated.  The process is not obvious, and I don't expect it to be but I wanted to know if anyone knows who figured this out.  Here's what I'm talking about: $$\begin{align*}\int \sec(x)dx &=\int \sec(x)\cdot \frac{\sec(x)+\tan(x)}{\sec(x)+\tan(x)}dx\\ &=\int \frac{\sec^2(x)+\tan(x)\sec(x)}{\sec(x)+\tan(x)}dx. \end{align*}$$ If $f(x) = \frac{1}{x}$, $g(x)=\sec(x)+\tan(x)$, $g'(x)=\sec^2(x)+\tan(x)\sec(x)$ Then $\int \sec(x)dx = \int f(g(x))\cdot g'(x)dx=\int \frac{1}{u}du$, where $u=g(x)$ $=\ln|\sec(x)+\tan(x)|+c$ So my question is, who first realised how to do this?  Who figured out step 2?  It's clever and not that obvious. (and my sub-question is: why does Arturo insist on re-formatting my questions so that my first statements are centre aligned?! :)","This question already has answers here : Tables and histories of methods of finding $\int\sec x\,dx$? (2 answers) Closed 8 months ago . I was looking at how the secant function is integrated.  The process is not obvious, and I don't expect it to be but I wanted to know if anyone knows who figured this out.  Here's what I'm talking about: $$\begin{align*}\int \sec(x)dx &=\int \sec(x)\cdot \frac{\sec(x)+\tan(x)}{\sec(x)+\tan(x)}dx\\ &=\int \frac{\sec^2(x)+\tan(x)\sec(x)}{\sec(x)+\tan(x)}dx. \end{align*}$$ If $f(x) = \frac{1}{x}$, $g(x)=\sec(x)+\tan(x)$, $g'(x)=\sec^2(x)+\tan(x)\sec(x)$ Then $\int \sec(x)dx = \int f(g(x))\cdot g'(x)dx=\int \frac{1}{u}du$, where $u=g(x)$ $=\ln|\sec(x)+\tan(x)|+c$ So my question is, who first realised how to do this?  Who figured out step 2?  It's clever and not that obvious. (and my sub-question is: why does Arturo insist on re-formatting my questions so that my first statements are centre aligned?! :)",,"['calculus', 'integration', 'math-history']"
98,Does $\int_{0}^{\infty}\frac{dx}{1+(x\sin5x)^2}$ converge?,Does  converge?,\int_{0}^{\infty}\frac{dx}{1+(x\sin5x)^2},"I would like your help with deciding whether the following integral converges or not: $$\int_{0}^{\infty}\frac{dx}{1+(x\sin5x)^2}.$$ I tried to compare it to other functions and to change the variables, but it didn't work for me. Thanks a lot!","I would like your help with deciding whether the following integral converges or not: $$\int_{0}^{\infty}\frac{dx}{1+(x\sin5x)^2}.$$ I tried to compare it to other functions and to change the variables, but it didn't work for me. Thanks a lot!",,['calculus']
99,Derivative of $f(x) = (x+x)$,Derivative of,f(x) = (x+x),"I'm trying to teach myself algebra and derivatives. I learned the derivative for $f(x) = x^2$ from a lesson, and now I thought I would see if I could figure out the derivative of $f(x) = x+x$ on my own. I know the formula for derivatives is: $$\lim_{\delta\rightarrow 0}\frac{f(x+\delta)          - f(x)}{\delta}$$ So my attempt at algebra amounted to this: $$\frac{((x+x) + (\delta + \delta))          - (x+x)}{\delta}$$ $$=\frac{\delta + \delta}{\delta}$$ Which doesn't seem right. (Isn't the derivative supposed to NOT contain the delta term?)","I'm trying to teach myself algebra and derivatives. I learned the derivative for $f(x) = x^2$ from a lesson, and now I thought I would see if I could figure out the derivative of $f(x) = x+x$ on my own. I know the formula for derivatives is: $$\lim_{\delta\rightarrow 0}\frac{f(x+\delta)          - f(x)}{\delta}$$ So my attempt at algebra amounted to this: $$\frac{((x+x) + (\delta + \delta))          - (x+x)}{\delta}$$ $$=\frac{\delta + \delta}{\delta}$$ Which doesn't seem right. (Isn't the derivative supposed to NOT contain the delta term?)",,"['calculus', 'limits', 'derivatives']"
