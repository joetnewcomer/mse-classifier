,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Cubic Spline Interpolation math,Cubic Spline Interpolation math,,"I am writing a code snippet in Python to do an interpolation using cubic splines. I have first done the math, and then attempted to implement the pseudo code in Python. However, I think i might have messed up with the running index or a coefficient. Would someone please be kind enough to check my math ? The resulting curve is not smooth, does not fit the interior points and is all over the place. Let $(x_{0},y_{0}),(x_{1},y_{1}),\ldots,(x_{n},y_{n})$ be $n+1$ points in $\mathbb{R}^2$. A spline is a piece-wise polynomial function of the form - $$S(x)=\begin{cases}         S_{0}(x),& \text{if } x_{0}\le{x}\lt x_{1}\\         \vdots\\         S_{i}(x),& \text{if } x_{i}\le{x}\lt x_{i+1}\\         \vdots\\         S_{n-1}(x),& \text{if } x_{n-1}\le{x}\lt x_{n}\\ \end{cases}$$ $S_{i}(x)$ is a cubic polynomial with $4$ four coefficients, $\forall{i}$. There are $n$ intervals and a total of $4n$ unknowns. So, we need $4n$ conditions. Suppose $S_{i}(x)$ has the form $S_{i}(x)=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}$. The first and second derivatives of the cubic polynomials are: $\begin{aligned} S_{i}(x)&=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}\\ S_{i}'(x)&=3A_{i}(x-x_{i})^2+2B_{i}(x-x_{i})+C_{i}\\ S_{i}''(x)&=6A_{i}(x-x_{i})+2B_{i}\\ \end{aligned}$ Also, $\begin{aligned} S_{i}(x_{i})&=D_{i}\\ S_{i}'(x_{i})&=C_{i}\\ S_{i}''(x_{i})&=2B_{i}\\ \end{aligned}$ We define $h_{i}=x_{i}-x_{i-1}$. We have: $\begin{aligned} S_{i-1}(x_{i})&=A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}\\ S_{i-1}'(x_{i})&=3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}\\ S_{i-1}''(x_{i})&=6A_{i-1}h_{i}+2B_{i-1}\\ \end{aligned}$ Four properties of cubic splines The spline should satisfy meet the below criteria - The function $S(x)$ will interpolate all data points. $S(x)$ must be continuous. And so in each interval, $S_{i}(x_{i})=y_{i}$ and $S_{i-1}(x_{i})=y_{i}$. The curve $S(x)$ should be smooth without jumps. $S'(x)$ must be continuous on the interval $[x_{i},x_{i+1}]$. Therefore, the slopes at each interior points must match. $S_{i}'(x_{i})=S_{i-1}'(x_{i})$. The curve $S(x)$ should be not have any abrupt changes in its bentness or convexity. $S''(x)$ will be continuous on the interval $[x_{i},x_{i+1}]$. $S_{i}''(x_{i})=S_{i-1}''(x_{i})$. A choice of one of the following two conditions at the end points $(x_{0},y_{0})$ and $(x_{n},y_{n})$ (a) The natural spline: $S'_{0}(x_{0})=0=S'_{n-1}(x_{n})$ (b) The clamped cubic spline : $S'_{0}(x_{0})=f'(x_{0})$ and $S'_{n-1}(x_{n})=f'(x_{n})$ where $f$ is presumably the function, we are trying to approximate. Let's determine the $4n$ conditions. Evaluation of the coefficients $A_i,B_{i},C_{i},D_{i}$ 1) The first condition yields $A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}=D_{i}$ 2) The second condition yields $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$ 3) The third condition yields $6A_{i-1}h_{i}+2B_{i-1}=2B_{i}$ The above equations can be somwhat simplified, if we substitute $S_{i}''(x_{i})=2B_{i}=z_{i}$. Thus, we have $B_{i}=z_{i}/2$. 1) The last equation becomes : $6A_{i-1}h_{i}=2(z_{i}/2)-2(z_{i-1}/2)=z_{i}-z_{i-1}$. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ 2) The first equation becomes : $\begin{aligned} C_{i-1}h_{i}&=y_{i}-y_{i-1}-A_{i-1}h_{i}^3-B_{i-1}h_{i}^2\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-(A_{i-1}h_{i}^2+B_{i-1}h_{i})\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-\left(\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^2+\frac{z_{i-1}}{2}h_{i}\right)\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)\\ \end{aligned}$ We define $b_{i}=\frac{y_{i}-y_{i-1}}{h_{i}}$. In all of the above the equations, the running index $i$ goes from $1$ to $n$. Thus, we now have our equations for determining the coefficients. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ $B_{i-1}=\frac{z_{i-1}}{2}$ $C_{i-1}=b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)$ $D_{i}=y_{i}$ The system of equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ If we substitute these values in the second equation $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$, we should get a recurrence relation between $z_{i}$ - $\begin{aligned} 3\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^{2}+2\frac{z_{i-1}}{2}h_{i}+b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)&=b_{i+1}-h_{i+1}\left(\frac{z_{i+1}+2z_{i}}{6}\right)\\ \left(\frac{2z_{i}+z_{i-1}}{6}\right)h_{i}+\left(\frac{z_{i+1}+2z_{i}}{6}\right)h_{i+1}&=b_{i+1}-b_{i}\\ h_{i+1}z_{i+1}+2z_{i}(h_{i}+h_{i+1})+z_{i-1}h_{i}&=6(b_{i+1}-b_{i}) \end{aligned}$ for $i=1,2,3,\ldots,n-1$ This system of linear equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ can be represented in the matrix form as - $\begin{bmatrix} h_{1} & 2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\ 0 & h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\ 0 & 0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\ \vdots & & & & \ddots & \\ 0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n}) & h_{n} \end{bmatrix} \begin{bmatrix} z_{0}\\ z_{1}\\ z_{2}\\ \vdots\\ z_{n-1} \end{bmatrix} =\begin{bmatrix} 6(b_{2}-b_{1})\\ 6(b_{3}-b_{2})\\ 6(b_{4}-b_{3})\\ \vdots\\ 6(b_{n}-b_{n-1}) \end{bmatrix}$ This matrix has $n-1$ rows and $n+1$ columns. So, we need two additional conditions. For natural splines, $z_{0}=0=z_{n}$. The first column and the last column in the above system of linear equations can be eliminated, resulting in, $\begin{bmatrix} 2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\ h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\ 0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\ \vdots & & & \ddots & \\ 0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n}) \end{bmatrix} \begin{bmatrix} z_{1}\\ z_{2}\\ \vdots\\ z_{n-1} \end{bmatrix} =\begin{bmatrix} 6(b_{2}-b_{1})\\ 6(b_{3}-b_{2})\\ 6(b_{4}-b_{3})\\ \vdots\\ 6(b_{n}-b_{n-1}) \end{bmatrix}$","I am writing a code snippet in Python to do an interpolation using cubic splines. I have first done the math, and then attempted to implement the pseudo code in Python. However, I think i might have messed up with the running index or a coefficient. Would someone please be kind enough to check my math ? The resulting curve is not smooth, does not fit the interior points and is all over the place. Let $(x_{0},y_{0}),(x_{1},y_{1}),\ldots,(x_{n},y_{n})$ be $n+1$ points in $\mathbb{R}^2$. A spline is a piece-wise polynomial function of the form - $$S(x)=\begin{cases}         S_{0}(x),& \text{if } x_{0}\le{x}\lt x_{1}\\         \vdots\\         S_{i}(x),& \text{if } x_{i}\le{x}\lt x_{i+1}\\         \vdots\\         S_{n-1}(x),& \text{if } x_{n-1}\le{x}\lt x_{n}\\ \end{cases}$$ $S_{i}(x)$ is a cubic polynomial with $4$ four coefficients, $\forall{i}$. There are $n$ intervals and a total of $4n$ unknowns. So, we need $4n$ conditions. Suppose $S_{i}(x)$ has the form $S_{i}(x)=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}$. The first and second derivatives of the cubic polynomials are: $\begin{aligned} S_{i}(x)&=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}\\ S_{i}'(x)&=3A_{i}(x-x_{i})^2+2B_{i}(x-x_{i})+C_{i}\\ S_{i}''(x)&=6A_{i}(x-x_{i})+2B_{i}\\ \end{aligned}$ Also, $\begin{aligned} S_{i}(x_{i})&=D_{i}\\ S_{i}'(x_{i})&=C_{i}\\ S_{i}''(x_{i})&=2B_{i}\\ \end{aligned}$ We define $h_{i}=x_{i}-x_{i-1}$. We have: $\begin{aligned} S_{i-1}(x_{i})&=A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}\\ S_{i-1}'(x_{i})&=3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}\\ S_{i-1}''(x_{i})&=6A_{i-1}h_{i}+2B_{i-1}\\ \end{aligned}$ Four properties of cubic splines The spline should satisfy meet the below criteria - The function $S(x)$ will interpolate all data points. $S(x)$ must be continuous. And so in each interval, $S_{i}(x_{i})=y_{i}$ and $S_{i-1}(x_{i})=y_{i}$. The curve $S(x)$ should be smooth without jumps. $S'(x)$ must be continuous on the interval $[x_{i},x_{i+1}]$. Therefore, the slopes at each interior points must match. $S_{i}'(x_{i})=S_{i-1}'(x_{i})$. The curve $S(x)$ should be not have any abrupt changes in its bentness or convexity. $S''(x)$ will be continuous on the interval $[x_{i},x_{i+1}]$. $S_{i}''(x_{i})=S_{i-1}''(x_{i})$. A choice of one of the following two conditions at the end points $(x_{0},y_{0})$ and $(x_{n},y_{n})$ (a) The natural spline: $S'_{0}(x_{0})=0=S'_{n-1}(x_{n})$ (b) The clamped cubic spline : $S'_{0}(x_{0})=f'(x_{0})$ and $S'_{n-1}(x_{n})=f'(x_{n})$ where $f$ is presumably the function, we are trying to approximate. Let's determine the $4n$ conditions. Evaluation of the coefficients $A_i,B_{i},C_{i},D_{i}$ 1) The first condition yields $A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}=D_{i}$ 2) The second condition yields $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$ 3) The third condition yields $6A_{i-1}h_{i}+2B_{i-1}=2B_{i}$ The above equations can be somwhat simplified, if we substitute $S_{i}''(x_{i})=2B_{i}=z_{i}$. Thus, we have $B_{i}=z_{i}/2$. 1) The last equation becomes : $6A_{i-1}h_{i}=2(z_{i}/2)-2(z_{i-1}/2)=z_{i}-z_{i-1}$. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ 2) The first equation becomes : $\begin{aligned} C_{i-1}h_{i}&=y_{i}-y_{i-1}-A_{i-1}h_{i}^3-B_{i-1}h_{i}^2\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-(A_{i-1}h_{i}^2+B_{i-1}h_{i})\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-\left(\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^2+\frac{z_{i-1}}{2}h_{i}\right)\\ C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)\\ \end{aligned}$ We define $b_{i}=\frac{y_{i}-y_{i-1}}{h_{i}}$. In all of the above the equations, the running index $i$ goes from $1$ to $n$. Thus, we now have our equations for determining the coefficients. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ $B_{i-1}=\frac{z_{i-1}}{2}$ $C_{i-1}=b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)$ $D_{i}=y_{i}$ The system of equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ If we substitute these values in the second equation $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$, we should get a recurrence relation between $z_{i}$ - $\begin{aligned} 3\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^{2}+2\frac{z_{i-1}}{2}h_{i}+b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)&=b_{i+1}-h_{i+1}\left(\frac{z_{i+1}+2z_{i}}{6}\right)\\ \left(\frac{2z_{i}+z_{i-1}}{6}\right)h_{i}+\left(\frac{z_{i+1}+2z_{i}}{6}\right)h_{i+1}&=b_{i+1}-b_{i}\\ h_{i+1}z_{i+1}+2z_{i}(h_{i}+h_{i+1})+z_{i-1}h_{i}&=6(b_{i+1}-b_{i}) \end{aligned}$ for $i=1,2,3,\ldots,n-1$ This system of linear equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ can be represented in the matrix form as - $\begin{bmatrix} h_{1} & 2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\ 0 & h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\ 0 & 0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\ \vdots & & & & \ddots & \\ 0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n}) & h_{n} \end{bmatrix} \begin{bmatrix} z_{0}\\ z_{1}\\ z_{2}\\ \vdots\\ z_{n-1} \end{bmatrix} =\begin{bmatrix} 6(b_{2}-b_{1})\\ 6(b_{3}-b_{2})\\ 6(b_{4}-b_{3})\\ \vdots\\ 6(b_{n}-b_{n-1}) \end{bmatrix}$ This matrix has $n-1$ rows and $n+1$ columns. So, we need two additional conditions. For natural splines, $z_{0}=0=z_{n}$. The first column and the last column in the above system of linear equations can be eliminated, resulting in, $\begin{bmatrix} 2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\ h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\ 0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\ \vdots & & & \ddots & \\ 0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n}) \end{bmatrix} \begin{bmatrix} z_{1}\\ z_{2}\\ \vdots\\ z_{n-1} \end{bmatrix} =\begin{bmatrix} 6(b_{2}-b_{1})\\ 6(b_{3}-b_{2})\\ 6(b_{4}-b_{3})\\ \vdots\\ 6(b_{n}-b_{n-1}) \end{bmatrix}$",,"['calculus', 'interpolation']"
1,"What can be said about the set of points that makes the MVT ""works""?","What can be said about the set of points that makes the MVT ""works""?",,"Consider a (real-valued) function $f$ that is differentiable on an open interval $I$ of $\mathbb R$. For $a\neq b$ in $\mathbb R$, we consider $S_{a,b}=\left\{ x\in I\; \middle|\; f'(x)=\frac{f(b)-f(a)}{b-a}\;  \right\}$. $S_{a,b}$ is non empty by the Mean Value Theorem. Let now $\displaystyle S=\bigcup_{a\neq b \in I}S_{a,b}$ What can be said about $S$ ? Has this set been studied before? Is $S$ an interval? Is $S$ open? Clearly, $S$ is a subset of $I$ but the inclusion can be strict. For example, if $f(x)=x^3$, we have $0\notin S$.  This example also shows that $S$ is not always an interval. Note: feel free to add conditions on $f$ if it allows $S$ to have nice properties.","Consider a (real-valued) function $f$ that is differentiable on an open interval $I$ of $\mathbb R$. For $a\neq b$ in $\mathbb R$, we consider $S_{a,b}=\left\{ x\in I\; \middle|\; f'(x)=\frac{f(b)-f(a)}{b-a}\;  \right\}$. $S_{a,b}$ is non empty by the Mean Value Theorem. Let now $\displaystyle S=\bigcup_{a\neq b \in I}S_{a,b}$ What can be said about $S$ ? Has this set been studied before? Is $S$ an interval? Is $S$ open? Clearly, $S$ is a subset of $I$ but the inclusion can be strict. For example, if $f(x)=x^3$, we have $0\notin S$.  This example also shows that $S$ is not always an interval. Note: feel free to add conditions on $f$ if it allows $S$ to have nice properties.",,"['calculus', 'general-topology', 'reference-request']"
2,Calculating the limit $\lim\limits_{x \to 0^+} \frac{\sqrt{\sin x}-\sin\sqrt{ x}}{x\sqrt{x}}$,Calculating the limit,\lim\limits_{x \to 0^+} \frac{\sqrt{\sin x}-\sin\sqrt{ x}}{x\sqrt{x}},Calculate $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}$$   without use Taylor serie and L'Hôpital. $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}\cdot\dfrac{\sqrt{\sin x}+\sin \sqrt{x}}{\sqrt{\sin x}+\sin \sqrt{x}}=\lim\limits_{x \to 0^+} \dfrac{\sin x-\sin^2\sqrt{x}}{x\sqrt{x}(\sqrt{\sin x}+\sin \sqrt{x})}$$ now what ?,Calculate $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}$$   without use Taylor serie and L'Hôpital. $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}\cdot\dfrac{\sqrt{\sin x}+\sin \sqrt{x}}{\sqrt{\sin x}+\sin \sqrt{x}}=\lim\limits_{x \to 0^+} \dfrac{\sin x-\sin^2\sqrt{x}}{x\sqrt{x}(\sqrt{\sin x}+\sin \sqrt{x})}$$ now what ?,,"['calculus', 'limits', 'trigonometry', 'derivatives', 'limits-without-lhopital']"
3,Pointwise convergence of monotonic functions to a continuous function implies Uniform Convergence,Pointwise convergence of monotonic functions to a continuous function implies Uniform Convergence,,"Let $f_{n}\colon\left[0,1\right]\to\mathbb{R}$ be a set of functions such that $f_{n}$ is monotonically increasing for all $n$. Moreover, let's assume that $f_{n}$ converges pointwise to a continuous function $f$. Prove that $f_{n}$ uniformly converges to $f$. Here is my attempt: $f\colon$$\left[0,1\right]\to\mathbb{R}$ is a continuous function, therefore from Cantor's Theorem we can say the it is uniformly continuous. That means that there is $\delta>0$ such that for all $x,y\in\left[0,1\right]$ that are close enough $\left|x-y\right|<\delta$ it follows that $\left|f(x)-f(y)\right|<\frac{\varepsilon}{3}$. Now for each $x\in\left[0,1\right]$ let $U_{x}$=$\left(x-\delta,x+\delta\right)$. From the Hiene-Borel Theorem we can conclude that there is a finite set $\left\{ x_{0},\ldots,x_{M}\right\} \subset\left[0,1\right]$ such that $\left[0,1\right]\subset\bigcup_{i=0}^{M}U_{x_{i}}$. From pointwise convergence we know that for every $0\leq i\leq M$ there is a number $N_{i}$ such that for all $n>N_{i}$ it follows that $\left|f_{n}(x_{i})-f(x_{i})\right|<\frac{\varepsilon}{3}$. Lets define $N=\max\left\{ N_{0},\ldots,N_{M}\right\} $. All I can conclude for now is that for all $n>N$ and for all $x\in\left[0,1\right]$ there is $0\leq i\leq M$ such that $x\in U_{x_{i}}$ and therefore: $$ \begin{align*} \left|f_{n}(x)-f(x)\right| & \leq\left|f_{n}(x)-f_{n}(x_{i})\right|+\left|f_{n}(x_{i})-f(x_{i})\right|+\left|f(x_{i})-f(x)\right|<\\  & <\left|f_{n}(x)-f_{n}(x_{i})\right|+\frac{2\varepsilon}{3} \end{align*} $$ How can I estimate this $\left|f_{n}(x)-f_{n}(x_{i})\right|$? I know I haven't used the monotony of those $f_{n}$'s yet and it probably has to do something with it. Is this going somewhere?","Let $f_{n}\colon\left[0,1\right]\to\mathbb{R}$ be a set of functions such that $f_{n}$ is monotonically increasing for all $n$. Moreover, let's assume that $f_{n}$ converges pointwise to a continuous function $f$. Prove that $f_{n}$ uniformly converges to $f$. Here is my attempt: $f\colon$$\left[0,1\right]\to\mathbb{R}$ is a continuous function, therefore from Cantor's Theorem we can say the it is uniformly continuous. That means that there is $\delta>0$ such that for all $x,y\in\left[0,1\right]$ that are close enough $\left|x-y\right|<\delta$ it follows that $\left|f(x)-f(y)\right|<\frac{\varepsilon}{3}$. Now for each $x\in\left[0,1\right]$ let $U_{x}$=$\left(x-\delta,x+\delta\right)$. From the Hiene-Borel Theorem we can conclude that there is a finite set $\left\{ x_{0},\ldots,x_{M}\right\} \subset\left[0,1\right]$ such that $\left[0,1\right]\subset\bigcup_{i=0}^{M}U_{x_{i}}$. From pointwise convergence we know that for every $0\leq i\leq M$ there is a number $N_{i}$ such that for all $n>N_{i}$ it follows that $\left|f_{n}(x_{i})-f(x_{i})\right|<\frac{\varepsilon}{3}$. Lets define $N=\max\left\{ N_{0},\ldots,N_{M}\right\} $. All I can conclude for now is that for all $n>N$ and for all $x\in\left[0,1\right]$ there is $0\leq i\leq M$ such that $x\in U_{x_{i}}$ and therefore: $$ \begin{align*} \left|f_{n}(x)-f(x)\right| & \leq\left|f_{n}(x)-f_{n}(x_{i})\right|+\left|f_{n}(x_{i})-f(x_{i})\right|+\left|f(x_{i})-f(x)\right|<\\  & <\left|f_{n}(x)-f_{n}(x_{i})\right|+\frac{2\varepsilon}{3} \end{align*} $$ How can I estimate this $\left|f_{n}(x)-f_{n}(x_{i})\right|$? I know I haven't used the monotony of those $f_{n}$'s yet and it probably has to do something with it. Is this going somewhere?",,"['calculus', 'uniform-convergence']"
4,"Integral $\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x$",Integral,"\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x","Inspired by this topic , can I easily prove the result below? $$\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x=\frac{13}{36}$$ The elementary antiderivative exists, but it seems masochist if one wants compute it.","Inspired by this topic , can I easily prove the result below? $$\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x=\frac{13}{36}$$ The elementary antiderivative exists, but it seems masochist if one wants compute it.",,"['calculus', 'integration', 'definite-integrals']"
5,L'Hospital's Rule but the process does not stop,L'Hospital's Rule but the process does not stop,,Let $\Psi:\mathbb{C}\rightarrow \mathbb{C}$ be an infinitely differentiable function at $\Psi^{-1}(a).$ What is the limit: $$\lim_{w\rightarrow \Psi^{-1}(a)} \frac{1}{(\Psi(w)-a)(\Psi')^{1/2}(w)}+\frac{1}{(\Psi'(\Psi^{-1}(a)))^{3/2}(\Psi^{-1}(a)-w)}?$$ I kept using L Hôpital's rule but the process cannot be stopped. Thank you.,Let $\Psi:\mathbb{C}\rightarrow \mathbb{C}$ be an infinitely differentiable function at $\Psi^{-1}(a).$ What is the limit: $$\lim_{w\rightarrow \Psi^{-1}(a)} \frac{1}{(\Psi(w)-a)(\Psi')^{1/2}(w)}+\frac{1}{(\Psi'(\Psi^{-1}(a)))^{3/2}(\Psi^{-1}(a)-w)}?$$ I kept using L Hôpital's rule but the process cannot be stopped. Thank you.,,"['calculus', 'limits']"
6,"Prove Fourier Transformation: $\int_{-\infty}^{\infty} \Theta(t) \sin (w_0 t) e^{- i w t} \,dt =- \frac{w_0}{ w^2-w_0^2 +i \text{sgn}(w)0^{+}}$",Prove Fourier Transformation:,"\int_{-\infty}^{\infty} \Theta(t) \sin (w_0 t) e^{- i w t} \,dt =- \frac{w_0}{ w^2-w_0^2 +i \text{sgn}(w)0^{+}}","Prove: $$\begin{align}\int_{-\infty}^{\infty} \Theta(t) \sin (w_0 t) e^{- i w t} \,dt &=- \frac{w_0}{ w^2-w_0^2 +i \text{sgn}(w)0^{+}}\\\\ &=- \frac{w_0}{w^2-w_0^2}+ \frac{i \pi }{2}(\delta(w-w_0)-\delta(w+w_0))\end{align}$$ where $\delta(x)$ is the Dirac function , $\Theta(x)$ is the Heaviside step function , $\text{sgn}(x)$ is the Sign function . I think that the 2nd equation may   need to use Sokhotski–Plemelj theorem , but there is $\text{sgn}(w)$ in the  denominator, so I don't know how to use the identity.","Prove: $$\begin{align}\int_{-\infty}^{\infty} \Theta(t) \sin (w_0 t) e^{- i w t} \,dt &=- \frac{w_0}{ w^2-w_0^2 +i \text{sgn}(w)0^{+}}\\\\ &=- \frac{w_0}{w^2-w_0^2}+ \frac{i \pi }{2}(\delta(w-w_0)-\delta(w+w_0))\end{align}$$ where $\delta(x)$ is the Dirac function , $\Theta(x)$ is the Heaviside step function , $\text{sgn}(x)$ is the Sign function . I think that the 2nd equation may   need to use Sokhotski–Plemelj theorem , but there is $\text{sgn}(w)$ in the  denominator, so I don't know how to use the identity.",,"['calculus', 'integration', 'improper-integrals', 'fourier-transform']"
7,Evaluation of $\int\frac{1}{x+ \sqrt{x^2-x+1}}dx$,Evaluation of,\int\frac{1}{x+ \sqrt{x^2-x+1}}dx,"Evaluate : $$\int\frac{1}{x+ \sqrt{x^2-x+1}}dx$$ After multiplying the denominator with $x-\sqrt{x^2-x+1}$ , I get $$x+\ln |x-1|-\int \frac{\sqrt{x^2-x+1}}{x-1}dx$$ Is $\int \frac{\sqrt{x^2-x+1}}{x-1}dx$ is integrable in terms of elementary functions?","Evaluate : After multiplying the denominator with , I get Is is integrable in terms of elementary functions?",\int\frac{1}{x+ \sqrt{x^2-x+1}}dx x-\sqrt{x^2-x+1} x+\ln |x-1|-\int \frac{\sqrt{x^2-x+1}}{x-1}dx \int \frac{\sqrt{x^2-x+1}}{x-1}dx,"['calculus', 'integration', 'indefinite-integrals']"
8,How does this converging function appear to diverge?,How does this converging function appear to diverge?,,"I found something while trying to analyze the integral $\displaystyle\int_0^\infty \frac{\sin(mx)} {x}\mathrm  dx $ Let us suppose we already know that the value of the integral is $\pi/2$ when $m>0$, $0$ when $m=0$ and $-\pi/2$ when $m<0$. Let $f(m)=\displaystyle\int_0^\infty \dfrac{\sin(mx)} {x} \mathrm dx$ Thus $f'(m)=\displaystyle\dfrac{\mathrm d}{\mathrm dm}\int_0^\infty \frac{\sin(mx)} {x} \mathrm dx=\int_0^\infty \frac{\partial}{\partial m}\dfrac{\sin(mx)} {x}\mathrm  dx=\int_0^\infty \cos(mx)\mathrm  dx=\lim\limits_{x \to \infty} \frac{\sin(mx)}{m}$, which does not converge. However, $f'(m)$ should be $0$ where $m\neq0$, as value of the integral remains constant So we get $0= \lim\limits_{x \to \infty} \dfrac{\sin(mx)}{m}$, where $m\neq0$ How does this happen? Please note that I am a high school student and do not know complex analysis. Can we assign value $0$ to $\lim\limits_{x \to \infty}{\sin(x)}$ as the average value of Sine function remains zero even when $x$ approaches infinity?","I found something while trying to analyze the integral $\displaystyle\int_0^\infty \frac{\sin(mx)} {x}\mathrm  dx $ Let us suppose we already know that the value of the integral is $\pi/2$ when $m>0$, $0$ when $m=0$ and $-\pi/2$ when $m<0$. Let $f(m)=\displaystyle\int_0^\infty \dfrac{\sin(mx)} {x} \mathrm dx$ Thus $f'(m)=\displaystyle\dfrac{\mathrm d}{\mathrm dm}\int_0^\infty \frac{\sin(mx)} {x} \mathrm dx=\int_0^\infty \frac{\partial}{\partial m}\dfrac{\sin(mx)} {x}\mathrm  dx=\int_0^\infty \cos(mx)\mathrm  dx=\lim\limits_{x \to \infty} \frac{\sin(mx)}{m}$, which does not converge. However, $f'(m)$ should be $0$ where $m\neq0$, as value of the integral remains constant So we get $0= \lim\limits_{x \to \infty} \dfrac{\sin(mx)}{m}$, where $m\neq0$ How does this happen? Please note that I am a high school student and do not know complex analysis. Can we assign value $0$ to $\lim\limits_{x \to \infty}{\sin(x)}$ as the average value of Sine function remains zero even when $x$ approaches infinity?",,"['calculus', 'convergence-divergence', 'definite-integrals']"
9,Prove $\sum_{k=1}^{\infty }\frac{\left ( -1 \right )^{k}}{k}\sum_{j=1}^{2k}\frac{\left ( -1 \right )^{j}}{j}=\frac{\pi ^{2}}{48}+\frac{1}{4}\ln^22$.,Prove .,\sum_{k=1}^{\infty }\frac{\left ( -1 \right )^{k}}{k}\sum_{j=1}^{2k}\frac{\left ( -1 \right )^{j}}{j}=\frac{\pi ^{2}}{48}+\frac{1}{4}\ln^22,"How to prove the following series, $$\sum_{k=1}^{\infty }\frac{\left ( -1 \right )^{k}}{k}\sum_{j=1}^{2k}\frac{\left ( -1 \right )^{j}}{j}=\frac{\pi ^{2}}{48}+\frac{1}{4}\ln^22$$ I know a formula which might be usful. $$\sum_{j=1}^{n}\frac{\left ( -1 \right )^{j-1}}{j}=\ln 2+\left ( -1 \right )^{n-1}\int_{0}^{1}\frac{x^{n}}{1+x}\mathrm{d}x$$ any hint will be appreciate.","How to prove the following series, $$\sum_{k=1}^{\infty }\frac{\left ( -1 \right )^{k}}{k}\sum_{j=1}^{2k}\frac{\left ( -1 \right )^{j}}{j}=\frac{\pi ^{2}}{48}+\frac{1}{4}\ln^22$$ I know a formula which might be usful. $$\sum_{j=1}^{n}\frac{\left ( -1 \right )^{j-1}}{j}=\ln 2+\left ( -1 \right )^{n-1}\int_{0}^{1}\frac{x^{n}}{1+x}\mathrm{d}x$$ any hint will be appreciate.",,"['calculus', 'integration']"
10,$\int_0^4\frac{\log x}{\sqrt{4x-x^2}} dx=0$ [duplicate],[duplicate],\int_0^4\frac{\log x}{\sqrt{4x-x^2}} dx=0,This question already has answers here : Prove that $\int_0^4 \frac{\ln x}{\sqrt{4x-x^2}}~dx=0$ (without trigonometric substitution) (5 answers) Closed 7 years ago . I am having trouble proving that it is equal to zero analytically. I have tried plotting and know that for $0<x<1$ the integrand is negative and positive otherwise. I have tried substitution $u\to \sqrt{x}$ but I cannot proceed further.,This question already has answers here : Prove that $\int_0^4 \frac{\ln x}{\sqrt{4x-x^2}}~dx=0$ (without trigonometric substitution) (5 answers) Closed 7 years ago . I am having trouble proving that it is equal to zero analytically. I have tried plotting and know that for $0<x<1$ the integrand is negative and positive otherwise. I have tried substitution $u\to \sqrt{x}$ but I cannot proceed further.,,['calculus']
11,Is the function differentiable at $0$?,Is the function differentiable at ?,0,"Let $$f(x) = \begin{cases}\begin{align*}&\cos{\dfrac{1}{x}}, &x \neq0 \\ &0, &x=0. \end{align*}\end{cases}$$ Is the function $F(x) = \displaystyle \int_{0}^x f dx$ differentiable at $0$ ? We can see that the function $f(x)$ is continuous everywhere except $x=0$ . In order to show differentiability we will need to show the derivatives from the left and right are equal. So we need to show that $$\lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h}.$$ The derivative from the right is $$\lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^+} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h}$$ and the derivative from the left is $$\lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h}.$$ I am not sure how to proceed.",Let Is the function differentiable at ? We can see that the function is continuous everywhere except . In order to show differentiability we will need to show the derivatives from the left and right are equal. So we need to show that The derivative from the right is and the derivative from the left is I am not sure how to proceed.,"f(x) = \begin{cases}\begin{align*}&\cos{\dfrac{1}{x}}, &x \neq0 \\ &0, &x=0. \end{align*}\end{cases} F(x) = \displaystyle \int_{0}^x f dx 0 f(x) x=0 \lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h}. \lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^+} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h} \lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h}.",['calculus']
12,How could one solve $\int_{0}^{\infty} \frac{1}{1-t^4}dt$ with special functions?,How could one solve  with special functions?,\int_{0}^{\infty} \frac{1}{1-t^4}dt,"How could one solve $$\int_0^\infty \frac{1}{1-t^4} \, dt\,?$$ I have to apply special functions, so I thought that I have to use the change variable $$u=t^4,$$ but $$du=4t^3\,dt$$ and when $$t\rightarrow0\qquad u\rightarrow0\ $$ I get $$t\rightarrow\infty\qquad u\rightarrow\infty, $$ whereas beta function is $\int_{0}^{1} t^{x-1}(1-t)^{y-1}dt$ so I cannot use that change. Help?","How could one solve $$\int_0^\infty \frac{1}{1-t^4} \, dt\,?$$ I have to apply special functions, so I thought that I have to use the change variable $$u=t^4,$$ but $$du=4t^3\,dt$$ and when $$t\rightarrow0\qquad u\rightarrow0\ $$ I get $$t\rightarrow\infty\qquad u\rightarrow\infty, $$ whereas beta function is $\int_{0}^{1} t^{x-1}(1-t)^{y-1}dt$ so I cannot use that change. Help?",,"['calculus', 'integration', 'special-functions', 'gamma-function', 'beta-function']"
13,Ellipse Perimeter,Ellipse Perimeter,,"I've seen lots of methods of getting an approximation of the perimeter of an ellipse, however, I was wondering if there is an exact method that exists, no matter how complex.","I've seen lots of methods of getting an approximation of the perimeter of an ellipse, however, I was wondering if there is an exact method that exists, no matter how complex.",,"['calculus', 'conic-sections']"
14,Polar to Rectangular Coordinates,Polar to Rectangular Coordinates,,"Problem: Transform the following equation from polar to rectangular coordinates. \begin{eqnarray*} \rho &=& \frac{2}{1 - \cos \theta} \\ \end{eqnarray*} Answer: Recall that: \begin{eqnarray*} \rho &=& {(x^2+y^2)} ^ {\frac{1}{2}} \\ \cos \theta &=&  \frac{x}{\rho} =\frac{x}{(x^2+y^2)^{\frac{1}{2}}} \\ \end{eqnarray*} This gives us: \begin{eqnarray*} {(x^2+y^2)}^{\frac{1}{2}} &=& \frac{2}{1 - \frac{x}{(x^2+y^2)^{\frac{1}{2}}}} \\ 2 &=& {(x^2+y^2)^\frac{1}{2}} - x \\ x + 2 &=& (x^2+y^2)^\frac{1}{2} \\ (x + 2)^2 &=& x^2+y^2 \\ x^2 + 4x + 4 &=& x^2+y^2 \\ 4x + 4 &=& y^2 \\ y^2 &=& 4x + 4 \end{eqnarray*} However, the book gets: \begin{eqnarray*} y^2 &=& 4(x + 2) \end{eqnarray*} What am I missing?","Problem: Transform the following equation from polar to rectangular coordinates. \begin{eqnarray*} \rho &=& \frac{2}{1 - \cos \theta} \\ \end{eqnarray*} Answer: Recall that: \begin{eqnarray*} \rho &=& {(x^2+y^2)} ^ {\frac{1}{2}} \\ \cos \theta &=&  \frac{x}{\rho} =\frac{x}{(x^2+y^2)^{\frac{1}{2}}} \\ \end{eqnarray*} This gives us: \begin{eqnarray*} {(x^2+y^2)}^{\frac{1}{2}} &=& \frac{2}{1 - \frac{x}{(x^2+y^2)^{\frac{1}{2}}}} \\ 2 &=& {(x^2+y^2)^\frac{1}{2}} - x \\ x + 2 &=& (x^2+y^2)^\frac{1}{2} \\ (x + 2)^2 &=& x^2+y^2 \\ x^2 + 4x + 4 &=& x^2+y^2 \\ 4x + 4 &=& y^2 \\ y^2 &=& 4x + 4 \end{eqnarray*} However, the book gets: \begin{eqnarray*} y^2 &=& 4(x + 2) \end{eqnarray*} What am I missing?",,"['calculus', 'functions', 'trigonometry']"
15,Proving a Lipschitz constant does not exist.,Proving a Lipschitz constant does not exist.,,"So I wish to find for each of these functions a Lipschitz constant or prove that none exists. So my definition for a function to be Lipschitz is: A function $f:[a,b] \rightarrow \mathbb{R}$ is Lipschitz if there exists a $L$ such that $|f(x) - f(y)| \leq L|x-y|$ for all $x,y \in [a,b]$. $f(x) = \frac{1}{x}$ for $x \in (0, 1]$ $f(x) = e^x$ for $x \in \mathbb{R}$ $f(x) = \sqrt{1-x^2}$ for $x \in [-1,1]$ My attempt for 1) to prove $f$ is not Lipschitz is via contradiction. Suppose that $x, y \in (0,1]$ and $f$ is Lipschitz. Then there exists a $L$ such that, $$|\frac{1}{x} -\frac{1}{y}| = \frac{|x-y|}{|xy|} \leq L |x-y|$$ for all $x,y \in (0, 1]$. This would imply that, $\frac{1}{|xy|} \leq L$ for all $x,y \in (0, 1]$. But such a $L$ cannot exist since we can make $x, y$ as small as we like, the fraction will grow. So in conclusion I attempted 1) but not sure if I am correct. I am stuck on 2 and 3. Anybody can give me some hints? I'm thinking of using the Mean Value Theorem on question 2. Other than that I have no idea how to start.","So I wish to find for each of these functions a Lipschitz constant or prove that none exists. So my definition for a function to be Lipschitz is: A function $f:[a,b] \rightarrow \mathbb{R}$ is Lipschitz if there exists a $L$ such that $|f(x) - f(y)| \leq L|x-y|$ for all $x,y \in [a,b]$. $f(x) = \frac{1}{x}$ for $x \in (0, 1]$ $f(x) = e^x$ for $x \in \mathbb{R}$ $f(x) = \sqrt{1-x^2}$ for $x \in [-1,1]$ My attempt for 1) to prove $f$ is not Lipschitz is via contradiction. Suppose that $x, y \in (0,1]$ and $f$ is Lipschitz. Then there exists a $L$ such that, $$|\frac{1}{x} -\frac{1}{y}| = \frac{|x-y|}{|xy|} \leq L |x-y|$$ for all $x,y \in (0, 1]$. This would imply that, $\frac{1}{|xy|} \leq L$ for all $x,y \in (0, 1]$. But such a $L$ cannot exist since we can make $x, y$ as small as we like, the fraction will grow. So in conclusion I attempted 1) but not sure if I am correct. I am stuck on 2 and 3. Anybody can give me some hints? I'm thinking of using the Mean Value Theorem on question 2. Other than that I have no idea how to start.",,['calculus']
16,Limit of $\sqrt[2n+1]{n^2+n}$,Limit of,\sqrt[2n+1]{n^2+n},"Prove $a_n=\sqrt[2n+1]{n^2+n}$ tends to $1$ as $n$ tends to infinity. In the textbook, a hint was given: Let $a_n=1+h$, then $n^2+n=(1+h)^{2n+1}\gt \binom{2n+1}{3}(h^3)$ Then the consecutive steps are some algebraic manipulation, I managed to prove that $$\sqrt[3]{\frac{1}{n+1}+\frac{1}{n^2}}\gt \sqrt[2n+1]{n^2+n}-1\gt0$$ So $a_n$ tends to $1$ as $n$ gets sufficiently large. But I don't understand why $a_n$ was set as $1+h$ at the first place, in retrospective, this does make calculations a lot convenient. And how did the author know when to use the binomial coefficient and compare the fourth term, it seems the author just plucked it out of thin air.","Prove $a_n=\sqrt[2n+1]{n^2+n}$ tends to $1$ as $n$ tends to infinity. In the textbook, a hint was given: Let $a_n=1+h$, then $n^2+n=(1+h)^{2n+1}\gt \binom{2n+1}{3}(h^3)$ Then the consecutive steps are some algebraic manipulation, I managed to prove that $$\sqrt[3]{\frac{1}{n+1}+\frac{1}{n^2}}\gt \sqrt[2n+1]{n^2+n}-1\gt0$$ So $a_n$ tends to $1$ as $n$ gets sufficiently large. But I don't understand why $a_n$ was set as $1+h$ at the first place, in retrospective, this does make calculations a lot convenient. And how did the author know when to use the binomial coefficient and compare the fourth term, it seems the author just plucked it out of thin air.",,"['calculus', 'limits']"
17,What's the theoretical basis for integration using partial fractions?,What's the theoretical basis for integration using partial fractions?,,"Exercises involving integration using partial fractions depend on expressing a rational function $\frac{P(x)}{Q(x)}$ (where the degree of $P$ is less than the degree of $Q$) as a sum of $$\frac{A}{(x+a)^k}$$ and $$ \frac{Bx+C}{(x^2+bx+c)^m}$$ I didn't see a reasonable explanation for why this is possible (especially for why it's necessary to put a linear polynomial $Bx+c$ in the numerator when there are ""repeated"" quadratical terms in the denominator). OBS.: I'm just a calculus student, so the theory which explains this might not be accessible to me. I asked this however because maybe someone could show me a way to look to it.","Exercises involving integration using partial fractions depend on expressing a rational function $\frac{P(x)}{Q(x)}$ (where the degree of $P$ is less than the degree of $Q$) as a sum of $$\frac{A}{(x+a)^k}$$ and $$ \frac{Bx+C}{(x^2+bx+c)^m}$$ I didn't see a reasonable explanation for why this is possible (especially for why it's necessary to put a linear polynomial $Bx+c$ in the numerator when there are ""repeated"" quadratical terms in the denominator). OBS.: I'm just a calculus student, so the theory which explains this might not be accessible to me. I asked this however because maybe someone could show me a way to look to it.",,"['calculus', 'integration', 'ring-theory', 'commutative-algebra', 'field-theory']"
18,Problem with definite integral $\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$,Problem with definite integral,\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx,$$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ The question says 'evaluate the integral using the suggested substitution. It gives $u=\cos x$. But I think Let $u=1-2\sin x$ is better. $$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ $$u=1-2\sin x$$ $$du=-2\cos x dx$$ $$=-\frac{1}{2}\int_{1}^{0}\sqrt{u}du$$ $$=\frac{1}{2}\int_{0}^{1}\sqrt{u}du$$ $$=\left | \frac{u^{\frac{3}{2}}}{3} \right |_{0}^{1}$$ $$=\frac{1}{3}$$ My question is how to solve it by using $u=\cos x$. Can anyone show the solution for it? Thanks a lot!,$$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ The question says 'evaluate the integral using the suggested substitution. It gives $u=\cos x$. But I think Let $u=1-2\sin x$ is better. $$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ $$u=1-2\sin x$$ $$du=-2\cos x dx$$ $$=-\frac{1}{2}\int_{1}^{0}\sqrt{u}du$$ $$=\frac{1}{2}\int_{0}^{1}\sqrt{u}du$$ $$=\left | \frac{u^{\frac{3}{2}}}{3} \right |_{0}^{1}$$ $$=\frac{1}{3}$$ My question is how to solve it by using $u=\cos x$. Can anyone show the solution for it? Thanks a lot!,,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
19,Proving that $\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}=f''(x)$,Proving that,\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}=f''(x),"Prove that $$\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}=f''(x)$$ Is the following a correct proof: $f''(x)=$$\lim\limits_{h\to 0}\frac{f'(x)-f'(x-h)}{h}=\lim\limits_{h\to 0}\frac{\frac{f(x+h)-f(x)}{h}-\frac{f(x)-f(x-h)}{h}}{h}=\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}$ I would really love input on this proof. The book ""Berkeley Problems in Mathematics"" solves it differently.","Prove that $$\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}=f''(x)$$ Is the following a correct proof: $f''(x)=$$\lim\limits_{h\to 0}\frac{f'(x)-f'(x-h)}{h}=\lim\limits_{h\to 0}\frac{\frac{f(x+h)-f(x)}{h}-\frac{f(x)-f(x-h)}{h}}{h}=\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}$ I would really love input on this proof. The book ""Berkeley Problems in Mathematics"" solves it differently.",,"['calculus', 'derivatives']"
20,Evaluating sums using residues $(-1)^n/n^2$ [duplicate],Evaluating sums using residues  [duplicate],(-1)^n/n^2,"This question already has answers here : Complex Analysis Solution to the Basel Problem ($\sum_{k=1}^\infty \frac{1}{k^2}$) [duplicate] (4 answers) Closed 9 years ago . I am an alien towards compelx analysis, with very little know I am posing a question, who someone may want to help with. Evaluate: $$\frac{1}{4}\cdot \sum_{n=1}^{\infty} \frac{(-1)^n}{n^2}$$ In disguise this is similar to $\zeta(2)$  but how can this be done using residues, and complex analysis? I need some help. I am just interested. The answer is $\displaystyle \frac{\pi^2}{48}$","This question already has answers here : Complex Analysis Solution to the Basel Problem ($\sum_{k=1}^\infty \frac{1}{k^2}$) [duplicate] (4 answers) Closed 9 years ago . I am an alien towards compelx analysis, with very little know I am posing a question, who someone may want to help with. Evaluate: $$\frac{1}{4}\cdot \sum_{n=1}^{\infty} \frac{(-1)^n}{n^2}$$ In disguise this is similar to $\zeta(2)$  but how can this be done using residues, and complex analysis? I need some help. I am just interested. The answer is $\displaystyle \frac{\pi^2}{48}$",,"['calculus', 'sequences-and-series', 'complex-analysis', 'analysis', 'residue-calculus']"
21,Evaluate $\int_0^\infty e^{-x}\sin e^{-x}\cos e^{-x} \ln e^{-x}\;dx$,Evaluate,\int_0^\infty e^{-x}\sin e^{-x}\cos e^{-x} \ln e^{-x}\;dx,How to evaluate the following integral? $$\int_0^\infty e^{-x}\sin e^{-x}\cos e^{-x} \ln e^{-x}\;dx$$,How to evaluate the following integral? $$\int_0^\infty e^{-x}\sin e^{-x}\cos e^{-x} \ln e^{-x}\;dx$$,,"['calculus', 'integration']"
22,Show by induction that $2!4!6!...(2n)! \geq ((n+1)!)^n$,Show by induction that,2!4!6!...(2n)! \geq ((n+1)!)^n,"Show by induction that $2!4!6!...(2n)! \geq ((n+1)!)^n$ I stuck at $((n+1)!)^n (2(n+1))! \geq ((n+1+1)!)^{n+1}$, but cant progress to next step It will be great in someone can demonstrate how to solve this problem. Thanks","Show by induction that $2!4!6!...(2n)! \geq ((n+1)!)^n$ I stuck at $((n+1)!)^n (2(n+1))! \geq ((n+1+1)!)^{n+1}$, but cant progress to next step It will be great in someone can demonstrate how to solve this problem. Thanks",,"['calculus', 'discrete-mathematics']"
23,What's a good primer from linear algebra to spherical harmonics?,What's a good primer from linear algebra to spherical harmonics?,,"I need a topic, a primer, that will be able to introduce me to spherical harmonics and how to translate and use them with the usual tools of linear algebra and calculus, namely matrices, polynomials and derivatives for example . In other words, I would like to know enough to handle and compute harmonics. What topics do you suggest I should touch to get up and running with spherical harmonics starting with a linear algebra and calculus background ?","I need a topic, a primer, that will be able to introduce me to spherical harmonics and how to translate and use them with the usual tools of linear algebra and calculus, namely matrices, polynomials and derivatives for example . In other words, I would like to know enough to handle and compute harmonics. What topics do you suggest I should touch to get up and running with spherical harmonics starting with a linear algebra and calculus background ?",,"['calculus', 'linear-algebra', 'reference-request', 'harmonic-analysis', 'spherical-harmonics']"
24,Why can we only talk about derivatives on an open interval?,Why can we only talk about derivatives on an open interval?,,"For instance, in my calculus class, all theorems are in the following form: For example, Rolle's theorem: If $f(x)$ is continuous on $[a,b]$, differentiable on $(a,b)$ ... (etc) My question is, when presented with a closed interval, why must we talk about its derivatives on an open interval? Why doesn't Rolle's theorem say `differentiable on $[a,b]$' instead of $(a,b)$","For instance, in my calculus class, all theorems are in the following form: For example, Rolle's theorem: If $f(x)$ is continuous on $[a,b]$, differentiable on $(a,b)$ ... (etc) My question is, when presented with a closed interval, why must we talk about its derivatives on an open interval? Why doesn't Rolle's theorem say `differentiable on $[a,b]$' instead of $(a,b)$",,['calculus']
25,Why is $e^{-x^2}$ such a big deal?,Why is  such a big deal?,e^{-x^2},"It seems like integrating it is a big deal and everything, but I don't understand why. My teacher is making it seem really important in my calculus class for seemingly no reason. Help would be appreciated.","It seems like integrating it is a big deal and everything, but I don't understand why. My teacher is making it seem really important in my calculus class for seemingly no reason. Help would be appreciated.",,['calculus']
26,Evaluating a series of hypergeometric functions,Evaluating a series of hypergeometric functions,,"I would like to prove (or disprove) the following statement: $$ \sum_{n=0}^\infty \left[\frac{{}_2{\rm F}_1\left(\frac{1}{2},\frac{1-n}{2};\frac{3}{2};1\right)}{n!}\right] = \frac{\pi}{2} \left[ \sum_{a=0}^\infty \frac{1}{4^a (a!)^2} + \frac{1}{2} \sum_{b=0}^\infty \frac{\left(\frac{1}{2}\right)^{2b}}{\Gamma\left(b+\frac{3}{2}\right)^2}\right] $$ also can be seen as $$ \sum_{n=0}^\infty \left[\frac{{}_2{\rm F}_1\left(\frac{1}{2},\frac{1-n}{2};\frac{3}{2};1\right)}{n!}\right] = \frac{\pi}{2} \bigg[ I_0(1) + L_0(1) \bigg] $$ Where $I_0(1)$ is the modified bessel function of the first kind and $L_0(1)$ is the modified struve function. Major brownie points on the line here!","I would like to prove (or disprove) the following statement: $$ \sum_{n=0}^\infty \left[\frac{{}_2{\rm F}_1\left(\frac{1}{2},\frac{1-n}{2};\frac{3}{2};1\right)}{n!}\right] = \frac{\pi}{2} \left[ \sum_{a=0}^\infty \frac{1}{4^a (a!)^2} + \frac{1}{2} \sum_{b=0}^\infty \frac{\left(\frac{1}{2}\right)^{2b}}{\Gamma\left(b+\frac{3}{2}\right)^2}\right] $$ also can be seen as $$ \sum_{n=0}^\infty \left[\frac{{}_2{\rm F}_1\left(\frac{1}{2},\frac{1-n}{2};\frac{3}{2};1\right)}{n!}\right] = \frac{\pi}{2} \bigg[ I_0(1) + L_0(1) \bigg] $$ Where $I_0(1)$ is the modified bessel function of the first kind and $L_0(1)$ is the modified struve function. Major brownie points on the line here!",,"['calculus', 'sequences-and-series', 'summation', 'special-functions', 'hypergeometric-function']"
27,how find all the zeroes of the polynomial,how find all the zeroes of the polynomial,,Find all the zeroes of the polynomial $f(x) = 2x^7  - 17x^6  -45x^5  +390x^4  + 28x^3  + 1832x^2  +960x$ this is my try $f(x)= 2x^7  - 16x^6-x^6-42x^5-3x^5+390x^4  + 28x^3  + 1820x^2+12x^2  +960x$ $f(x)=2x^6 (x - 8)-x^5 (x+42)-3x^4 (x-130)+28x^2 (x + 1820)+12x(x+80)$,Find all the zeroes of the polynomial $f(x) = 2x^7  - 17x^6  -45x^5  +390x^4  + 28x^3  + 1832x^2  +960x$ this is my try $f(x)= 2x^7  - 16x^6-x^6-42x^5-3x^5+390x^4  + 28x^3  + 1820x^2+12x^2  +960x$ $f(x)=2x^6 (x - 8)-x^5 (x+42)-3x^4 (x-130)+28x^2 (x + 1820)+12x(x+80)$,,"['calculus', 'algebra-precalculus']"
28,"Evaluate $\int_{0}^{1} \frac{\left[\rm{Li}_2\left(\frac{1}{2} \right)-\rm{Li}_2\left(\frac{1 + x}{2}\right)\right]\ln( 1 - x)}{1 + x}\,dx$",Evaluate,"\int_{0}^{1} \frac{\left[\rm{Li}_2\left(\frac{1}{2} \right)-\rm{Li}_2\left(\frac{1 + x}{2}\right)\right]\ln( 1 - x)}{1 + x}\,dx","$\def\Li{{\rm{Li}}}$How to evaluate the following integral$${\large\int_0^1} {\frac{{\left[ {\Li_2\left( {\frac{1}{2}} \right) - \Li_2\left( \frac{1 + x}{2} \right)} \right]\ln \left( {1 - x} \right)}}{{1 + x}}}\, dx$$","$\def\Li{{\rm{Li}}}$How to evaluate the following integral$${\large\int_0^1} {\frac{{\left[ {\Li_2\left( {\frac{1}{2}} \right) - \Li_2\left( \frac{1 + x}{2} \right)} \right]\ln \left( {1 - x} \right)}}{{1 + x}}}\, dx$$",,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'closed-form']"
29,Evaluation of $ \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx$,Evaluation of, \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx,"Evaluation of $\displaystyle \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx$ $\bf{My\; Solution:}$ Using $\displaystyle (x\cdot \sin x+n\cdot \cos x) = \sqrt{x^2+n^2}\left\{\frac{x}{\sqrt{x^2+n^2}}\cdot \sin x+\frac{n}{\sqrt{x^2+n^2}}\cdot \cos x\right\}$ $$\displaystyle = \sqrt{x^2+n^2}\cdot \cos\left(x-\phi\right)\;,$$ where $\displaystyle \sin \phi = \frac{x}{\sqrt{x^2+n^2}}$ and $\displaystyle \cos \phi = \frac{n}{\sqrt{x^2+n^2}}$ and $\displaystyle \tan \phi = \frac{x}{n}\Rightarrow \phi = \tan^{-1}\left(\frac{x}{n}\right)$ So Integral is $$\displaystyle = \int \sec^2(x-\phi)\cdot \left(\frac{x^2+n(n-1)}{x^2+n^2}\right)dx$$ Now Let $$\displaystyle (x-\phi) = y\Rightarrow \left(x-\tan^{-1}\left(\frac{x}{n}\right)\right)=y$$. Then $$\displaystyle \left(\frac{x^2+n(n-1)}{x^2+n^2}\right)dx = dy$$ So Integral is $$\displaystyle \int \sec^2(y)dy = \tan y +\mathbb{C} = \tan\left(x-\tan^{-1}\left(\frac{x}{n}\right)\right)+\mathbb{C}$$ So $$\displaystyle \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx = \left(\frac{n\cdot \tan x-x}{n+x\cdot \tan x}\right)+\mathbb{C}$$ My Question is , Is there is any other solution other then that, because It is very Complex and Lengthy. If Yes, The please write here Thanks","Evaluation of $\displaystyle \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx$ $\bf{My\; Solution:}$ Using $\displaystyle (x\cdot \sin x+n\cdot \cos x) = \sqrt{x^2+n^2}\left\{\frac{x}{\sqrt{x^2+n^2}}\cdot \sin x+\frac{n}{\sqrt{x^2+n^2}}\cdot \cos x\right\}$ $$\displaystyle = \sqrt{x^2+n^2}\cdot \cos\left(x-\phi\right)\;,$$ where $\displaystyle \sin \phi = \frac{x}{\sqrt{x^2+n^2}}$ and $\displaystyle \cos \phi = \frac{n}{\sqrt{x^2+n^2}}$ and $\displaystyle \tan \phi = \frac{x}{n}\Rightarrow \phi = \tan^{-1}\left(\frac{x}{n}\right)$ So Integral is $$\displaystyle = \int \sec^2(x-\phi)\cdot \left(\frac{x^2+n(n-1)}{x^2+n^2}\right)dx$$ Now Let $$\displaystyle (x-\phi) = y\Rightarrow \left(x-\tan^{-1}\left(\frac{x}{n}\right)\right)=y$$. Then $$\displaystyle \left(\frac{x^2+n(n-1)}{x^2+n^2}\right)dx = dy$$ So Integral is $$\displaystyle \int \sec^2(y)dy = \tan y +\mathbb{C} = \tan\left(x-\tan^{-1}\left(\frac{x}{n}\right)\right)+\mathbb{C}$$ So $$\displaystyle \int \frac{x^2+n(n-1)}{(x\cdot \sin x+n\cdot \cos x)^2}dx = \left(\frac{n\cdot \tan x-x}{n+x\cdot \tan x}\right)+\mathbb{C}$$ My Question is , Is there is any other solution other then that, because It is very Complex and Lengthy. If Yes, The please write here Thanks",,['calculus']
30,How is $ \sum_{n=1}^{\infty}\left(\psi(\alpha n)-\log(\alpha n)+\frac{1}{2\alpha n}\right)$ when $\alpha$ is great?,How is  when  is great?, \sum_{n=1}^{\infty}\left(\psi(\alpha n)-\log(\alpha n)+\frac{1}{2\alpha n}\right) \alpha,"Let $\psi := \Gamma'/\Gamma$ denote the digamma function. Could you find, as $\alpha$ tends to $+\infty$, an equivalent term for the following series? $$ \sum_{n=1}^{\infty}  \left( \psi (\alpha n) - \log (\alpha n) + \frac{1}{2\alpha n} \right) $$ Please I do have an answer, I'm curious about different approaches. Thanks.","Let $\psi := \Gamma'/\Gamma$ denote the digamma function. Could you find, as $\alpha$ tends to $+\infty$, an equivalent term for the following series? $$ \sum_{n=1}^{\infty}  \left( \psi (\alpha n) - \log (\alpha n) + \frac{1}{2\alpha n} \right) $$ Please I do have an answer, I'm curious about different approaches. Thanks.",,"['calculus', 'sequences-and-series', 'logarithms', 'asymptotics', 'polygamma']"
31,Finding the uniquely determined region of a PDE,Finding the uniquely determined region of a PDE,,"(a) Solve the equation $yu_x+xu_y=0$  with the condition $u(0,y) = e^{-y^2}$. (b) In which region of the xy plane is the solution uniquely determined? I did the first part but I don't understand how to do part b.  I read this. But I don't understand how the argument is based. A more clear explanation is appreciated","(a) Solve the equation $yu_x+xu_y=0$  with the condition $u(0,y) = e^{-y^2}$. (b) In which region of the xy plane is the solution uniquely determined? I did the first part but I don't understand how to do part b.  I read this. But I don't understand how the argument is based. A more clear explanation is appreciated",,"['calculus', 'partial-differential-equations']"
32,Solving $\int_{-\infty}^{\infty}\frac{x^2e^x}{(1+e^x)^2}dx$,Solving,\int_{-\infty}^{\infty}\frac{x^2e^x}{(1+e^x)^2}dx,"I am attempting to use residues to solve $\int_{-\infty}^{\infty}\frac{x^2e^x}{(1+e^x)^2}dx$; the answer is $\frac{\pi^2}{3}$. I have tried to split $\frac{x^2e^x}{(1+e^x)^2}$ into two parts $$\frac{x^2}{e^x+1}-\frac{x^2}{\left(e^x+1\right)^2},$$ however no progress was to be made. I could not solve $$ \int_{-\infty}^{\infty}\frac{z^2}{e^z+1}-\frac{z^2}{\left(e^z+1\right)^2}dz. $$ any easier than the problem at hand. As a matter of fact neither of these two integrals converge individually. The way I approached the problem was by finding the singularities, namely when $e^z=-1$. So I got, when we let $z=x+iy$, that $z_k=i\pi+2\pi k$ for $k\in\mathbb{Z}$. This is my attempt, $$ \int_{C}\frac{z^2e^{z}}{(e^z+1)^2}=2i\pi\sum_{\forall n}(Res(f,z_n)). $$ Since $\{-1\}$ is a second order pole, our residue is given by $$ \lim_{z\to z_k}\frac{d}{dz}[(z-z_k)^2\frac{z^2e^{z}}{(e^z+1)^2}]. $$ I found these with the aid of Mathematica. However if we were to calculate all the residues we would get an alternating sequence, if arranged properly. It becomes more apparent when we solve the equation that is split. For instance if we take $z_{-4}$ we get  $$ 2 \pi  i \left(\lim_{z\to z_{-4}} \, \frac{z^2 (z-z_{-4})}{e^z+1}\right)\ \text{and}\ 2 \pi  i \left(\lim_{z\to z_{-4}} \, \frac{\partial }{\partial z}\frac{\left(z-z_{-4}\right){}^2 \left(-z^2\right)}{\left(e^z+1\right)^2}\right) $$ or, $$ 98i\pi^3 \text{ and } 14 i (-7 \pi +2 i) \pi ^2. $$ So the total contribution would be $-28 \pi ^2$. Were as if we take $z_4$ we would get $$ 162 i \pi ^3 \text{ and } -18 i \pi ^2 (9 \pi +2 i) $$ and the contribution would be $36 \pi ^2$. One dilemma is finding the summation of all these residues. Is my work flawed? Is there a better contour to make this problem work? Thank you for the assistance.","I am attempting to use residues to solve $\int_{-\infty}^{\infty}\frac{x^2e^x}{(1+e^x)^2}dx$; the answer is $\frac{\pi^2}{3}$. I have tried to split $\frac{x^2e^x}{(1+e^x)^2}$ into two parts $$\frac{x^2}{e^x+1}-\frac{x^2}{\left(e^x+1\right)^2},$$ however no progress was to be made. I could not solve $$ \int_{-\infty}^{\infty}\frac{z^2}{e^z+1}-\frac{z^2}{\left(e^z+1\right)^2}dz. $$ any easier than the problem at hand. As a matter of fact neither of these two integrals converge individually. The way I approached the problem was by finding the singularities, namely when $e^z=-1$. So I got, when we let $z=x+iy$, that $z_k=i\pi+2\pi k$ for $k\in\mathbb{Z}$. This is my attempt, $$ \int_{C}\frac{z^2e^{z}}{(e^z+1)^2}=2i\pi\sum_{\forall n}(Res(f,z_n)). $$ Since $\{-1\}$ is a second order pole, our residue is given by $$ \lim_{z\to z_k}\frac{d}{dz}[(z-z_k)^2\frac{z^2e^{z}}{(e^z+1)^2}]. $$ I found these with the aid of Mathematica. However if we were to calculate all the residues we would get an alternating sequence, if arranged properly. It becomes more apparent when we solve the equation that is split. For instance if we take $z_{-4}$ we get  $$ 2 \pi  i \left(\lim_{z\to z_{-4}} \, \frac{z^2 (z-z_{-4})}{e^z+1}\right)\ \text{and}\ 2 \pi  i \left(\lim_{z\to z_{-4}} \, \frac{\partial }{\partial z}\frac{\left(z-z_{-4}\right){}^2 \left(-z^2\right)}{\left(e^z+1\right)^2}\right) $$ or, $$ 98i\pi^3 \text{ and } 14 i (-7 \pi +2 i) \pi ^2. $$ So the total contribution would be $-28 \pi ^2$. Were as if we take $z_4$ we would get $$ 162 i \pi ^3 \text{ and } -18 i \pi ^2 (9 \pi +2 i) $$ and the contribution would be $36 \pi ^2$. One dilemma is finding the summation of all these residues. Is my work flawed? Is there a better contour to make this problem work? Thank you for the assistance.",,"['calculus', 'integration', 'analysis']"
33,Find the limit of $(\sqrt2-\root3\of2 )(\sqrt2 -\root4\of2 )...(\sqrt 2-\root{n}\of2 )$ without using the squeezing principle,Find the limit of  without using the squeezing principle,(\sqrt2-\root3\of2 )(\sqrt2 -\root4\of2 )...(\sqrt 2-\root{n}\of2 ),"$$\mathop {\lim }\limits_{n \to \infty } (\sqrt 2  - \root 3 \of 2 )(\sqrt 2  - \root 4 \of 2 )...(\sqrt 2  - \root n \of 2 )$$ I was able easily to find the limit of this series using the squeezing principle, but how can you find it without it? I'm guessing it's an algebra trick :)","$$\mathop {\lim }\limits_{n \to \infty } (\sqrt 2  - \root 3 \of 2 )(\sqrt 2  - \root 4 \of 2 )...(\sqrt 2  - \root n \of 2 )$$ I was able easily to find the limit of this series using the squeezing principle, but how can you find it without it? I'm guessing it's an algebra trick :)",,"['calculus', 'limits']"
34,Need a general formula for $\frac{d^n}{dx^n}\left(f(x)^m\right)$,Need a general formula for,\frac{d^n}{dx^n}\left(f(x)^m\right),"Let $m,n\in\mathbb{N}$. I need to express the derivative $\displaystyle\frac{d^n}{dx^n}\left(f(x)^m\right)$ in terms of sums/products of the derivatives of the function $f$ itself. Here are results for several small fixed values of $n$: $$\frac{d}{dx}\left(f(x)^m\right)=m \cdot f(x)^{m-1} \cdot f^{'}(x)$$ $$\frac{d^2}{dx^2}\left(f(x)^m\right)=m \cdot (m-1) \cdot f(x)^{m-2} \cdot f^{'}(x)^2+m \cdot f(x)^{m-1} \cdot f^{''}(x)$$ $$\frac{d^3}{dx^3}\left(f(x)^m\right)=m \cdot (m-1) \cdot (m-2) \cdot f(x)^{m-3} \cdot f^{'}(x)^3+3 \cdot m \cdot (m-1) \cdot f(x)^{m-2} \cdot f^{'}(x) \cdot f^{''}(x) + m \cdot f(x)^{m-1} \cdot f^{'''}(x)$$ I tried to find a common pattern for these expressions, but failed. Question: Is it possible to find a general formula for the derivative of any given order $n$?","Let $m,n\in\mathbb{N}$. I need to express the derivative $\displaystyle\frac{d^n}{dx^n}\left(f(x)^m\right)$ in terms of sums/products of the derivatives of the function $f$ itself. Here are results for several small fixed values of $n$: $$\frac{d}{dx}\left(f(x)^m\right)=m \cdot f(x)^{m-1} \cdot f^{'}(x)$$ $$\frac{d^2}{dx^2}\left(f(x)^m\right)=m \cdot (m-1) \cdot f(x)^{m-2} \cdot f^{'}(x)^2+m \cdot f(x)^{m-1} \cdot f^{''}(x)$$ $$\frac{d^3}{dx^3}\left(f(x)^m\right)=m \cdot (m-1) \cdot (m-2) \cdot f(x)^{m-3} \cdot f^{'}(x)^3+3 \cdot m \cdot (m-1) \cdot f(x)^{m-2} \cdot f^{'}(x) \cdot f^{''}(x) + m \cdot f(x)^{m-1} \cdot f^{'''}(x)$$ I tried to find a common pattern for these expressions, but failed. Question: Is it possible to find a general formula for the derivative of any given order $n$?",,"['calculus', 'derivatives', 'exponentiation']"
35,Why does the derivative not exist at a cusp?,Why does the derivative not exist at a cusp?,,"I'm trying to grasp what's going on at a cusp geometrically. For instance, $y^2=x^3$ is not differentiable at the origin. In $y$ things appear fine: differentiate $y = \pm x^{3/2}$ and we get $y'=0$ regardless of approach from the top or bottom branch. But in $x$, we get the slope $x'$ is $\pm\infty$ depending on approach. But then again, for the circle $y^2+x^2=1$, we get $y' = \pm\infty$ at (1,0) depending on approach from the top or bottom. Yet it's clear that $dy/dx$ exists since, among many reasons, the Jacobian matrix has full rank everywhere. On the surface the two cases seem the same; at some point the derivative wrt to the other variable is 0, and flipping the variable we get two different slopes for the vertical tangent. Either there is some subtle difference or I'm missing something glaringly obvious. I think given any parameterization of the cuspidal cubic that tangent vectors at the origin only point in the negative x direction, while we can get opposite pointing tangent vectors everywhere on the circle depending on our parameterizations, but I'm not sure how to interpret this formally.","I'm trying to grasp what's going on at a cusp geometrically. For instance, $y^2=x^3$ is not differentiable at the origin. In $y$ things appear fine: differentiate $y = \pm x^{3/2}$ and we get $y'=0$ regardless of approach from the top or bottom branch. But in $x$, we get the slope $x'$ is $\pm\infty$ depending on approach. But then again, for the circle $y^2+x^2=1$, we get $y' = \pm\infty$ at (1,0) depending on approach from the top or bottom. Yet it's clear that $dy/dx$ exists since, among many reasons, the Jacobian matrix has full rank everywhere. On the surface the two cases seem the same; at some point the derivative wrt to the other variable is 0, and flipping the variable we get two different slopes for the vertical tangent. Either there is some subtle difference or I'm missing something glaringly obvious. I think given any parameterization of the cuspidal cubic that tangent vectors at the origin only point in the negative x direction, while we can get opposite pointing tangent vectors everywhere on the circle depending on our parameterizations, but I'm not sure how to interpret this formally.",,"['calculus', 'geometry', 'multivariable-calculus', 'differential-geometry']"
36,Integral of $e^{y^2}$,Integral of,e^{y^2},"Why can't be we take the antiderivative of $e^{y^2}$ with respect to y? My textbook just says it is not possible. That's it. No explanation. I could not find any one explanation on the web where it was explained either. Might seem like a dorky question but any explanation would be appreciated. But if we can, I guess the next natural question would be, ""what is the anti-derivative?""","Why can't be we take the antiderivative of $e^{y^2}$ with respect to y? My textbook just says it is not possible. That's it. No explanation. I could not find any one explanation on the web where it was explained either. Might seem like a dorky question but any explanation would be appreciated. But if we can, I guess the next natural question would be, ""what is the anti-derivative?""",,"['calculus', 'multivariable-calculus']"
37,Compute $\lim_{n\to\infty} nx_n$,Compute,\lim_{n\to\infty} nx_n,"Let $(x_n)_{n\ge2}$, $x_2>0$, that satisfies recurrence $x_{n+1}=\sqrt[n]{1+n x_n}-1, n\ge 2$. Compute $\lim_{n\to\infty} nx_n$. It's clear that $x_n\to 0$, and probably Stolz theorem would be helpful. Is it really necessary to use this theorem?","Let $(x_n)_{n\ge2}$, $x_2>0$, that satisfies recurrence $x_{n+1}=\sqrt[n]{1+n x_n}-1, n\ge 2$. Compute $\lim_{n\to\infty} nx_n$. It's clear that $x_n\to 0$, and probably Stolz theorem would be helpful. Is it really necessary to use this theorem?",,['calculus']
38,How to prove these integral inequalities?,How to prove these integral inequalities?,,"a) $f(x)>0$ and $f(x)\in C[a,b]$ Prove $$\left(\int_a^bf(x)\sin x\,dx\right)^2 +\left(\int_a^bf(x)\cos x\,dx\right)^2 \le \left(\int_a^bf(x)\,dx\right)^2$$ I have tried Cauchy-Schwarz inequality but failed to prove. b) $f(x)$ is differentiable in $[0,1]$ Prove  $$|f(0)|\le \int_0^1|f(x)|\,dx+\int_0^1|f'(x)|dx$$ Any Helps or Tips,Thanks","a) $f(x)>0$ and $f(x)\in C[a,b]$ Prove $$\left(\int_a^bf(x)\sin x\,dx\right)^2 +\left(\int_a^bf(x)\cos x\,dx\right)^2 \le \left(\int_a^bf(x)\,dx\right)^2$$ I have tried Cauchy-Schwarz inequality but failed to prove. b) $f(x)$ is differentiable in $[0,1]$ Prove  $$|f(0)|\le \int_0^1|f(x)|\,dx+\int_0^1|f'(x)|dx$$ Any Helps or Tips,Thanks",,['calculus']
39,Extreme Value Theorem Proof (Spivak),Extreme Value Theorem Proof (Spivak),,"Them: If $f$ is continuous on $[a,b]$ , then there is a $y$ in $[a,b]$ such that $f(y) \geq f(x)$ for each $x \in [a,b]$ Proof . We already know that $f$ is bounded on $[a,b]$ , which means that the set $$\{ f(x):x\text{ in }[a,b]\}$$ is bounded. This set is obviously not $\varnothing$ , so it has a least upper bound $\alpha$ . Since $\alpha\geqslant f(x)$ for $x$ in $[a,b]$ it suffices to show that $\alpha=f(y)$ for some $y$ in $[a,b]$ . Suppose instead that $\alpha\neq f(y)$ for all $y$ in $[a,b]$ . Then the function $g$ defined by $$g(x)=\dfrac1{\alpha-f(x)},\quad x\text{ in }[a,b]$$ is continuous on $[a,b]$ , since the denominator of the right side is never $0$ . On the other hand, $\alpha$ is the least upper bound of $\{f(x):x\text{ in }[a,b]\}$ ; this means that $$\text{for every $\epsilon\gt0$ there is $x$ in $[a,b]$ with $\alpha-f(x)\lt\epsilon$}.$$ This, in turn, means that $$\text{for every $\epsilon\gt0$ there is $x$ in $[a,b]$ with $g(x)\gt1/\epsilon$}.$$ But this means that $g$ is not bounded on $[a,b]$ , contradicting the previous theorem. $\Rule{0.3em}{0.87em}{0.1em}$ OKay first of all how on earth does one come up with $g(x)$ ? It just feels like it comes out of nowhere. On the other hand what does This means that: for every $\epsilon > 0$ , there is x in $[a,b]$ with $\alpha - f(x) < \epsilon$ even mean? This statement is true, I agree, but I have absolutely no feeling for it. I also do not understand the last line with $g$ . He could have chosen $\epsilon  = \alpha - f(x)$ and be done it no?","Them: If is continuous on , then there is a in such that for each Proof . We already know that is bounded on , which means that the set is bounded. This set is obviously not , so it has a least upper bound . Since for in it suffices to show that for some in . Suppose instead that for all in . Then the function defined by is continuous on , since the denominator of the right side is never . On the other hand, is the least upper bound of ; this means that This, in turn, means that But this means that is not bounded on , contradicting the previous theorem. OKay first of all how on earth does one come up with ? It just feels like it comes out of nowhere. On the other hand what does This means that: for every , there is x in with even mean? This statement is true, I agree, but I have absolutely no feeling for it. I also do not understand the last line with . He could have chosen and be done it no?","f [a,b] y [a,b] f(y) \geq f(x) x \in [a,b] f [a,b] \{ f(x):x\text{ in }[a,b]\} \varnothing \alpha \alpha\geqslant f(x) x [a,b] \alpha=f(y) y [a,b] \alpha\neq f(y) y [a,b] g g(x)=\dfrac1{\alpha-f(x)},\quad x\text{ in }[a,b] [a,b] 0 \alpha \{f(x):x\text{ in }[a,b]\} \text{for every \epsilon\gt0 there is x in [a,b] with \alpha-f(x)\lt\epsilon}. \text{for every \epsilon\gt0 there is x in [a,b] with g(x)\gt1/\epsilon}. g [a,b] \Rule{0.3em}{0.87em}{0.1em} g(x) \epsilon > 0 [a,b] \alpha - f(x) < \epsilon g \epsilon  = \alpha - f(x)",['calculus']
40,Taylor polynomial approximation,Taylor polynomial approximation,,"How do you determine if adding more terms to the Taylor polynomial will improve its approximation of $f(p)$ or in other words, how do you determine if a Taylor series converges for a particular value of $x$? I understand the concept but not conceptually ""enough."" A clear explanation to help clear the road would be appreciated.","How do you determine if adding more terms to the Taylor polynomial will improve its approximation of $f(p)$ or in other words, how do you determine if a Taylor series converges for a particular value of $x$? I understand the concept but not conceptually ""enough."" A clear explanation to help clear the road would be appreciated.",,"['calculus', 'algebra-precalculus', 'taylor-expansion', 'approximation-theory']"
41,Definition of continuity at a point,Definition of continuity at a point,,"In ""Principles of mathematical analysis"" Walter Rudin gives the definition of $$\lim_{x \rightarrow p} f(x)=q$$ for $f: X \supset E \rightarrow Y$ with $X,Y$ metric spaces, in this way: for every $\epsilon>0$ there exists a $\delta >0$ such that $$d_Y(f(x),q)<\epsilon$$ for all points $x \in E$ for which $$0<d_X(x,p)<\delta$$ where $d_X$ and $d_Y$ are the distances in $X$ and $Y$. After few pages he gives the definition of continuity: $f$ is said to be continuous at $p$ if for every $\epsilon>0$ there   exists a $\delta >0$ such that $$d_Y(f(x),f(p))<\epsilon$$ for all points $x \in E$ for which $$d_X(x,p)<\delta$$ My question is, why did he drop the $0<\dots$ in $d_X(x,p)<\delta$? I though that continuity in $p$ means that the limit of the function in $p$ exists and is equal to $f(p)$, so one should simply replace $q$ with $f(p)$ in the above definition of limit, why changing the condition on $x$? Doing so a function like $$f: \{1\} \rightarrow \mathbb{R}$$ which maps 1 to 5, for example, would be continous in 1, since $x=p$ is allowed, but what is the limit of $f$ in 1? It doesn't exist, right? Is there a deep reason for such a counterintuitive definition? EDIT: Maybe I should better clarify my question. I know that this definition works, I am asking why considering a function continuos at isolated points (which is an immediate consequence of this defintion ---> It turned out I was wrong about this). The only reason I can think of is that this definition agrees with the topological one for continuity, but it doesn't seem to me a good reason since topology came after (and would have agreed no matter the convention).","In ""Principles of mathematical analysis"" Walter Rudin gives the definition of $$\lim_{x \rightarrow p} f(x)=q$$ for $f: X \supset E \rightarrow Y$ with $X,Y$ metric spaces, in this way: for every $\epsilon>0$ there exists a $\delta >0$ such that $$d_Y(f(x),q)<\epsilon$$ for all points $x \in E$ for which $$0<d_X(x,p)<\delta$$ where $d_X$ and $d_Y$ are the distances in $X$ and $Y$. After few pages he gives the definition of continuity: $f$ is said to be continuous at $p$ if for every $\epsilon>0$ there   exists a $\delta >0$ such that $$d_Y(f(x),f(p))<\epsilon$$ for all points $x \in E$ for which $$d_X(x,p)<\delta$$ My question is, why did he drop the $0<\dots$ in $d_X(x,p)<\delta$? I though that continuity in $p$ means that the limit of the function in $p$ exists and is equal to $f(p)$, so one should simply replace $q$ with $f(p)$ in the above definition of limit, why changing the condition on $x$? Doing so a function like $$f: \{1\} \rightarrow \mathbb{R}$$ which maps 1 to 5, for example, would be continous in 1, since $x=p$ is allowed, but what is the limit of $f$ in 1? It doesn't exist, right? Is there a deep reason for such a counterintuitive definition? EDIT: Maybe I should better clarify my question. I know that this definition works, I am asking why considering a function continuos at isolated points (which is an immediate consequence of this defintion ---> It turned out I was wrong about this). The only reason I can think of is that this definition agrees with the topological one for continuity, but it doesn't seem to me a good reason since topology came after (and would have agreed no matter the convention).",,"['calculus', 'continuity']"
42,How to calculate number of digits of a large number?,How to calculate number of digits of a large number?,,Does anyone know any efficient ways of finding the number of digits in the large number $N = 4^{4^{4^4}}$? Thanks.,Does anyone know any efficient ways of finding the number of digits in the large number $N = 4^{4^{4^4}}$? Thanks.,,"['calculus', 'logarithms']"
43,"Alternative ways to show $\int_{0}^{\infty}f(x)\, dx = \int_{0}^{1}f^{-1}(y)\, dy$",Alternative ways to show,"\int_{0}^{\infty}f(x)\, dx = \int_{0}^{1}f^{-1}(y)\, dy","Let $f$ be a continuous, strictly decreasing, real-valued function such that $\int_{0}^{\infty}f(x)\, dx$ is finite and $f(0) = 1$. In terms of $f^{-1}$, we see that $$\int_{0}^{\infty}f(x)\, dx = \int_{0}^{1}f^{-1}(y)\, dy.$$ The way I saw this was by drawing an $f(x)$ and noticing that if I integrate in $y$ instead, the left handed integral becomes the right handed integral. Is there a way to do this with change of variables/$u$-substitution or other ways (namely ones that don't require drawing a picture of $f$)?","Let $f$ be a continuous, strictly decreasing, real-valued function such that $\int_{0}^{\infty}f(x)\, dx$ is finite and $f(0) = 1$. In terms of $f^{-1}$, we see that $$\int_{0}^{\infty}f(x)\, dx = \int_{0}^{1}f^{-1}(y)\, dy.$$ The way I saw this was by drawing an $f(x)$ and noticing that if I integrate in $y$ instead, the left handed integral becomes the right handed integral. Is there a way to do this with change of variables/$u$-substitution or other ways (namely ones that don't require drawing a picture of $f$)?",,"['calculus', 'integration', 'improper-integrals', 'alternative-proof', 'gre-exam']"
44,Limit with a summation,Limit with a summation,,"I'm getting crazy with the next limit: $$\lim_{n\rightarrow\infty}\frac{\sum \limits_{k=1}^n(k·a_k)}{n^2}$$ The exercise also says that is known that: $$\lim_{n\rightarrow\infty}a_n = L$$ I suppose that I have to answer in function of ""$L$"", but I still can't see anything. Any ideas?","I'm getting crazy with the next limit: $$\lim_{n\rightarrow\infty}\frac{\sum \limits_{k=1}^n(k·a_k)}{n^2}$$ The exercise also says that is known that: $$\lim_{n\rightarrow\infty}a_n = L$$ I suppose that I have to answer in function of ""$L$"", but I still can't see anything. Any ideas?",,"['calculus', 'sequences-and-series', 'limits']"
45,Finding limit for the function,Finding limit for the function,,"I have problem with showing that the limit of the following function $$\frac{  \sqrt{\frac{3 \pi}{2n}} -  \int_0^{\sqrt 6}(  1-\frac{x^2}{6}  +\frac{x^4}{120})^ndx}{\frac{3}{20}\frac 1n \sqrt{\frac{3 \pi}{2n}}}$$ equal to $1$, with $n \to \infty$.","I have problem with showing that the limit of the following function $$\frac{  \sqrt{\frac{3 \pi}{2n}} -  \int_0^{\sqrt 6}(  1-\frac{x^2}{6}  +\frac{x^4}{120})^ndx}{\frac{3}{20}\frac 1n \sqrt{\frac{3 \pi}{2n}}}$$ equal to $1$, with $n \to \infty$.",,"['calculus', 'limits']"
46,"Continuity of an ""SVD"" operator","Continuity of an ""SVD"" operator",,"Let $A_n$ be a series of matrices, and let $A$ be another matrix. Let $S(B)$ be an SVD operator that takes a matrix and returns the left singular vectors matrix ordered by largest singular value to smallest singular value. Also, assume all singular values for $A$ are unique. Is there some matrix norm $\| \cdot \|$ under which if $\|A_n - A\| \to 0$ as $n \to \infty$ then $\|S(A_n) - S(A)\| \to 0$? Does it happen for the Frobenius norm?","Let $A_n$ be a series of matrices, and let $A$ be another matrix. Let $S(B)$ be an SVD operator that takes a matrix and returns the left singular vectors matrix ordered by largest singular value to smallest singular value. Also, assume all singular values for $A$ are unique. Is there some matrix norm $\| \cdot \|$ under which if $\|A_n - A\| \to 0$ as $n \to \infty$ then $\|S(A_n) - S(A)\| \to 0$? Does it happen for the Frobenius norm?",,"['calculus', 'linear-algebra', 'continuity', 'matrix-calculus', 'svd']"
47,What if $f^{(n)}(a)=0$ for all $n\geq 0$?,What if  for all ?,f^{(n)}(a)=0 n\geq 0,"This morning I was trying to imagine what a function would look like if all it's derivatives were zero at a point $a$ (assuming it is $C^\infty$). My first thought was that it should be identically zero in a neighborhood of $a$, but this is only true if the function is analytic. So my question is this: What can we say in general about a smooth function (i.e. has derivatives of all orders) whose derivatives are all zero at a single point $a$? It seems like the function $f$ should approach zero at $a$ faster than any polynomial because of Taylor's theorem. In other words, for every $k\geq 1$, there exists some remainder function $r_k(x)$ with $$\frac{f(x)}{(x-a)^k}=r_k(x)\qquad \left( \text{and}\quad \lim_{x\rightarrow a}\ r_k(x)= 0\right).$$ So can we conclude that $f$ approaches zero faster than any polynomial? If so, how is this made precise? And is this all we can say about these functions? (These questions can all be subordinated to the main question)","This morning I was trying to imagine what a function would look like if all it's derivatives were zero at a point $a$ (assuming it is $C^\infty$). My first thought was that it should be identically zero in a neighborhood of $a$, but this is only true if the function is analytic. So my question is this: What can we say in general about a smooth function (i.e. has derivatives of all orders) whose derivatives are all zero at a single point $a$? It seems like the function $f$ should approach zero at $a$ faster than any polynomial because of Taylor's theorem. In other words, for every $k\geq 1$, there exists some remainder function $r_k(x)$ with $$\frac{f(x)}{(x-a)^k}=r_k(x)\qquad \left( \text{and}\quad \lim_{x\rightarrow a}\ r_k(x)= 0\right).$$ So can we conclude that $f$ approaches zero faster than any polynomial? If so, how is this made precise? And is this all we can say about these functions? (These questions can all be subordinated to the main question)",,"['calculus', 'taylor-expansion']"
48,A tricky series problem,A tricky series problem,,"The problem is the following: If $\sum \limits_{n=1} ^{\infty} a_n$ converges, where $a_n$ are real numbers, then there exists $b_n \to \infty$ so that $\sum \limits_{n=1} ^{\infty} a_n b_n$ is still convergent. I know that the above statement is true if $a_n$ are nonnegative (setting $b_n=\frac1{\sqrt{R_{n-1}}+ \sqrt{R_{n}}}$, where $R_n=\sum \limits_{k=n} ^{\infty} a_k$). But for the general $a_n$, I have no idea how to prove it. Any ideas are welcome. Thanks!","The problem is the following: If $\sum \limits_{n=1} ^{\infty} a_n$ converges, where $a_n$ are real numbers, then there exists $b_n \to \infty$ so that $\sum \limits_{n=1} ^{\infty} a_n b_n$ is still convergent. I know that the above statement is true if $a_n$ are nonnegative (setting $b_n=\frac1{\sqrt{R_{n-1}}+ \sqrt{R_{n}}}$, where $R_n=\sum \limits_{k=n} ^{\infty} a_k$). But for the general $a_n$, I have no idea how to prove it. Any ideas are welcome. Thanks!",,"['calculus', 'sequences-and-series']"
49,Neglecting higher order terms in expansion,Neglecting higher order terms in expansion,,"Suppose we have a function $v$ of $x$ with a minimum at $x=0$. We have, for $x$ close to zero, $$v'(x) = v'(0) +xv''(0) +\frac{x^2}{2}v'''(0)+\cdots$$ Then as $v'(0)=0$ $$v'(x)\approx xv''(0)$$ if $$|xv'''(0)|\ll v''(0)$$ Which is fine. I am unable to understand this statement: Typically each extra derivative will bring with it a factor of $1/L $   where $L$ is the distance over which the function changes by a large   fraction. So $$x\ll L$$ This is extracted from a physics derivation, and I cannot get how they tacked on a factor of $1/L$","Suppose we have a function $v$ of $x$ with a minimum at $x=0$. We have, for $x$ close to zero, $$v'(x) = v'(0) +xv''(0) +\frac{x^2}{2}v'''(0)+\cdots$$ Then as $v'(0)=0$ $$v'(x)\approx xv''(0)$$ if $$|xv'''(0)|\ll v''(0)$$ Which is fine. I am unable to understand this statement: Typically each extra derivative will bring with it a factor of $1/L $   where $L$ is the distance over which the function changes by a large   fraction. So $$x\ll L$$ This is extracted from a physics derivation, and I cannot get how they tacked on a factor of $1/L$",,"['calculus', 'approximation']"
50,Evaluate $\int \cos^3 x\;\sin^2 xdx$,Evaluate,\int \cos^3 x\;\sin^2 xdx,"Is this correct? I thought it would be but when I entered it into wolfram alpha, I got a different answer. $$\int (\cos^3x)(\sin^2x)dx  = \int(\cos x)(\cos^2x)(\sin^2x)dx  = \int (\cos x)(1-\sin^2x)(\sin^2x)dx.$$ let $u = \sin x$, $du = \cos xdx$ $$\int(1-u^2)u^2du = \int(u^2-u^4)du  = \frac{u^3}{3} - \frac{u^5}{5} +C$$ Plugging in back $u$, we get $\displaystyle\frac{\sin^3 x}{3} - \frac{\sin^5 x}{5}$ + C","Is this correct? I thought it would be but when I entered it into wolfram alpha, I got a different answer. $$\int (\cos^3x)(\sin^2x)dx  = \int(\cos x)(\cos^2x)(\sin^2x)dx  = \int (\cos x)(1-\sin^2x)(\sin^2x)dx.$$ let $u = \sin x$, $du = \cos xdx$ $$\int(1-u^2)u^2du = \int(u^2-u^4)du  = \frac{u^3}{3} - \frac{u^5}{5} +C$$ Plugging in back $u$, we get $\displaystyle\frac{\sin^3 x}{3} - \frac{\sin^5 x}{5}$ + C",,"['calculus', 'integration']"
51,How to Evaluate the Integral $\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy?$,How to Evaluate the Integral,\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy?,"Question: How to Evaluate the Integral $$\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy?$$ My attempt I'm looking for a method to evaluate it. I've attempted a substitution to simplify the expression, but I'm not sure how to proceed further. Here’s what I've done so far: I used the substitution $\sqrt{1+\sin(y)} \rightarrow x$ . This transforms the integral into: $$-2\int_{1}^{\sqrt{2}}\frac{\ln(x^{2}-1)}{x^{2}-2}dx.$$ I further decomposed the logarithm: $$-2\int_{1}^{\sqrt{2}}\frac{\ln(x+1)+\ln(x-1)}{(x-\sqrt{2})(x+\sqrt{2})}dx.$$ I'm stuck at this point. Edit ;The integral was given by our professor, who mentioned it has a very beautiful closed form. I have already emailed our professor, who gave the integral to my friends and me. If I receive any updates, I'll let you know.","Question: How to Evaluate the Integral My attempt I'm looking for a method to evaluate it. I've attempted a substitution to simplify the expression, but I'm not sure how to proceed further. Here’s what I've done so far: I used the substitution . This transforms the integral into: I further decomposed the logarithm: I'm stuck at this point. Edit ;The integral was given by our professor, who mentioned it has a very beautiful closed form. I have already emailed our professor, who gave the integral to my friends and me. If I receive any updates, I'll let you know.",\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{1+\sin(y)}\ln(\sin(y))}{\cos(y)}dy? \sqrt{1+\sin(y)} \rightarrow x -2\int_{1}^{\sqrt{2}}\frac{\ln(x^{2}-1)}{x^{2}-2}dx. -2\int_{1}^{\sqrt{2}}\frac{\ln(x+1)+\ln(x-1)}{(x-\sqrt{2})(x+\sqrt{2})}dx.,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
52,"Evaluating $\int_{1}^{\infty} \frac{\log(x)}{x^2(x^2-1)} \, dx$ at Its Upper Boundary",Evaluating  at Its Upper Boundary,"\int_{1}^{\infty} \frac{\log(x)}{x^2(x^2-1)} \, dx","Background : $$ \mbox{I am trying to solve the following integral:}\quad \int_{1}^{\infty}\frac{\log\left(x\right)}{x^{2}\left(x^{2} - 1\right)}\,{\rm d}x $$ This is a follow-up question from this post , and the indefinite integral of said integral was answered by Claude Leibovici with the following answer: $\int\frac{\log (x)}{x^2 \left(x^2-1\right)}\,dx = f(x) = -\frac{1}{2} [\text{Li}_2(1-x)+\text{Li}_2(-x)]-\frac{1}{2} \log (x+1) \log (x)+\frac{\log (x)}{x}+\frac{1}{x}.$ I am new to the dilogarithm, so I studied some of its identities, which are: $\begin{align*} &\mathrm{Li}_2(x)+\mathrm{Li}_2(-x)=\frac{1}{2} \mathrm{Li}_2(x^2),\\ &\mathrm{Li}_2(1-x)+\mathrm{Li}_2\left(1-\frac{1}{x}\right)=-\frac{\log^2(x)}{2},\\ &\mathrm{Li}_2(x)+\mathrm{Li}_2(1-x)=\frac{\pi^2}{6}-\log(x)\log(1-x),\\&\mathrm{Li}_2(-x)-\mathrm{Li}_2(1-x)+\frac{1}{2}\mathrm{Li}_2(1-x^2)=-\frac{\pi^2}{12}-\log(x)\log(x+1),\\ &\mathrm{Li}_2(x)+\mathrm{Li}_2\left(\frac{1}{x}\right)=-\frac{\pi^2}{6}-\frac{\log^2(-x)}{2}. \end{align*}$ I also know that $\begin{align*} & \mathrm{Li}_2(0) = 0,\\ & \mathrm{Li}_2(1) = \frac{\pi^2}{6},\\ & \mathrm{Li}_2(-1) = -\frac{\pi^2}{12}. \end{align*}$ Question So, I am now trying to prove that $\lim_{x \to \infty} f(x) = \frac{\pi^2}{6}.$ To be more precise, I am trying to show that $\lim_{x \to \infty} \left[ \mathrm{Li}_2(1-x) + \mathrm{Li}_2(-x) + \log(x) \log(x+1) \right] = -\frac{\pi^2}{3}. \tag{1}$ I have proved that $\lim_{x \to 1} f(x) = \frac{\pi^2}{24}+1$ , but I still can't figure out how to use the dilogarithm identities to prove equation (1). Can anyone help me with this problem?","Background : This is a follow-up question from this post , and the indefinite integral of said integral was answered by Claude Leibovici with the following answer: I am new to the dilogarithm, so I studied some of its identities, which are: I also know that Question So, I am now trying to prove that To be more precise, I am trying to show that I have proved that , but I still can't figure out how to use the dilogarithm identities to prove equation (1). Can anyone help me with this problem?","
\mbox{I am trying to solve the following integral:}\quad
\int_{1}^{\infty}\frac{\log\left(x\right)}{x^{2}\left(x^{2} - 1\right)}\,{\rm d}x
 \int\frac{\log (x)}{x^2 \left(x^2-1\right)}\,dx = f(x) = -\frac{1}{2} [\text{Li}_2(1-x)+\text{Li}_2(-x)]-\frac{1}{2} \log (x+1) \log (x)+\frac{\log (x)}{x}+\frac{1}{x}. \begin{align*}
&\mathrm{Li}_2(x)+\mathrm{Li}_2(-x)=\frac{1}{2} \mathrm{Li}_2(x^2),\\
&\mathrm{Li}_2(1-x)+\mathrm{Li}_2\left(1-\frac{1}{x}\right)=-\frac{\log^2(x)}{2},\\
&\mathrm{Li}_2(x)+\mathrm{Li}_2(1-x)=\frac{\pi^2}{6}-\log(x)\log(1-x),\\&\mathrm{Li}_2(-x)-\mathrm{Li}_2(1-x)+\frac{1}{2}\mathrm{Li}_2(1-x^2)=-\frac{\pi^2}{12}-\log(x)\log(x+1),\\
&\mathrm{Li}_2(x)+\mathrm{Li}_2\left(\frac{1}{x}\right)=-\frac{\pi^2}{6}-\frac{\log^2(-x)}{2}.
\end{align*} \begin{align*}
& \mathrm{Li}_2(0) = 0,\\
& \mathrm{Li}_2(1) = \frac{\pi^2}{6},\\
& \mathrm{Li}_2(-1) = -\frac{\pi^2}{12}.
\end{align*} \lim_{x \to \infty} f(x) = \frac{\pi^2}{6}. \lim_{x \to \infty} \left[ \mathrm{Li}_2(1-x) + \mathrm{Li}_2(-x) + \log(x) \log(x+1) \right] = -\frac{\pi^2}{3}. \tag{1} \lim_{x \to 1} f(x) = \frac{\pi^2}{24}+1","['calculus', 'integration', 'limits', 'improper-integrals']"
53,Maximising an integral.,Maximising an integral.,,"Given that $\{x_1,x_2,x_3,x_4,x_5,x_6,x_7\}$ is any permutation of $\{1,2,3,4,5,6,7\}$ , I am asked to maximise $$\int_{\int_{x_1}^{x_2} x_3}^{\int_{x_4}^{x_5} x_6} x_7 \ \ dx$$ My work : Knowing that $\int c \ \ dx = cx$ , the integral in question becomes $$x_7 \cdot \int^{x_6(x_5-x_4)}_{x_3(x_2-x_1)} 1 \ \ dx$$ This becomes: $$x_7(x_6(x_5-x_4)-x_3(x_2-x_1))$$ So we are supposed to maximise this expression over distinct integers from $1$ through $7$ . I do not know how to proceed, with a pen and paper approach, so I will appreciate help!","Given that is any permutation of , I am asked to maximise My work : Knowing that , the integral in question becomes This becomes: So we are supposed to maximise this expression over distinct integers from through . I do not know how to proceed, with a pen and paper approach, so I will appreciate help!","\{x_1,x_2,x_3,x_4,x_5,x_6,x_7\} \{1,2,3,4,5,6,7\} \int_{\int_{x_1}^{x_2} x_3}^{\int_{x_4}^{x_5} x_6} x_7 \ \ dx \int c \ \ dx = cx x_7 \cdot \int^{x_6(x_5-x_4)}_{x_3(x_2-x_1)} 1 \ \ dx x_7(x_6(x_5-x_4)-x_3(x_2-x_1)) 1 7","['calculus', 'algebra-precalculus', 'optimization']"
54,Prove that $\sum_{n=1}^\infty \dfrac{1}{\log_2(a_n)}$ converges.,Prove that  converges.,\sum_{n=1}^\infty \dfrac{1}{\log_2(a_n)},"Let $a_0 = 2.$ For $n\ge 1,$ let $a_n$ be the smallest positive integer so that $\sum_{j=0}^n 1/a_j < 1$ . Prove that $\sum_{n=1}^\infty \dfrac{1}{\log_2(a_n)}$ converges. I think one can come up with a recurrence relation for the $a_n$ 's.  Computing a few values of $a_n,$ we get that the sequence starts with $2,3,7,43.$ Now if we guess that the sequence is quadratic so that $a_{n+1} = A a_n^2 + B a_n + C$ for some constants A,B,C, then we can arrive at the formula $a_{n+1} = a_n^2 - a_n+1.$ This formula can be proved by induction if we additionally prove that $\sum_{j=0}^n 1/a_j = 1 - 1/(a_n^2 - a_n)$ , with the base case of $n=1$ being obvious. Now assume the formula holds for some $n\ge 1.$ Then $a_{n+1} = a_n^2 - a_n + 1$ by the inductive hypothesis, and $\sum_{j=0}^{n+1} 1/a_j = 1 - 1/((a_n^2 - a_n)(a_n^2 - a_n+1)) = 1 - 1/(a_{n+1}(a_{n+1}-1)).$ So $a_{n+2} = a_{n+1}^2 - a_{n+1}+1$ . Hence the result follows by induction. Now that we have a useful recurrence relation for the $a_n$ 's, we could finish the problem if we could show that $a_n \ge 2^{n^p}$ for some $p > 1$ , but I'm not sure which choice of p would work. Or one could find an alternative lower bound for the $a_n$ 's that'll guarantee the convergence of the given series.","Let For let be the smallest positive integer so that . Prove that converges. I think one can come up with a recurrence relation for the 's.  Computing a few values of we get that the sequence starts with Now if we guess that the sequence is quadratic so that for some constants A,B,C, then we can arrive at the formula This formula can be proved by induction if we additionally prove that , with the base case of being obvious. Now assume the formula holds for some Then by the inductive hypothesis, and So . Hence the result follows by induction. Now that we have a useful recurrence relation for the 's, we could finish the problem if we could show that for some , but I'm not sure which choice of p would work. Or one could find an alternative lower bound for the 's that'll guarantee the convergence of the given series.","a_0 = 2. n\ge 1, a_n \sum_{j=0}^n 1/a_j < 1 \sum_{n=1}^\infty \dfrac{1}{\log_2(a_n)} a_n a_n, 2,3,7,43. a_{n+1} = A a_n^2 + B a_n + C a_{n+1} = a_n^2 - a_n+1. \sum_{j=0}^n 1/a_j = 1 - 1/(a_n^2 - a_n) n=1 n\ge 1. a_{n+1} = a_n^2 - a_n + 1 \sum_{j=0}^{n+1} 1/a_j = 1 - 1/((a_n^2 - a_n)(a_n^2 - a_n+1)) = 1 - 1/(a_{n+1}(a_{n+1}-1)). a_{n+2} = a_{n+1}^2 - a_{n+1}+1 a_n a_n \ge 2^{n^p} p > 1 a_n","['calculus', 'sequences-and-series', 'inequality', 'convergence-divergence', 'induction']"
55,How to integrate the above integrand? Please help me out.,How to integrate the above integrand? Please help me out.,,"How to integrate $$\int_0^1 \frac{x^{49}}{1+x+x^2+\cdots+ x^{100}}\;dx$$ I am trying by breaking the summation in the denominator. $$\sum_{i=0}^{100} x^i=\frac{1-x^{101}}{1-x}$$ Therefore the above integrand will become $$\frac{x^{49}(1-x)}{1-x^{101}}$$ But how to approach further? As far as I can guess  the only way to proceed after this is by applying the Maclaurian series expansion of $1/(1-x)$ ; which is $$\sum_{i=0}^n x^i$$ . But the Series expansion of $1/(1-x)$ is applicable only when $-1 \le x \lt 1$ . In the integration, the limit is from $0$ to $1$ , i.e. we have to find the area in the interval $[0,1]$ . So, how to proceed further? Please help me out.","How to integrate I am trying by breaking the summation in the denominator. Therefore the above integrand will become But how to approach further? As far as I can guess  the only way to proceed after this is by applying the Maclaurian series expansion of ; which is . But the Series expansion of is applicable only when . In the integration, the limit is from to , i.e. we have to find the area in the interval . So, how to proceed further? Please help me out.","\int_0^1 \frac{x^{49}}{1+x+x^2+\cdots+ x^{100}}\;dx \sum_{i=0}^{100} x^i=\frac{1-x^{101}}{1-x} \frac{x^{49}(1-x)}{1-x^{101}} 1/(1-x) \sum_{i=0}^n x^i 1/(1-x) -1 \le x \lt 1 0 1 [0,1]","['calculus', 'definite-integrals']"
56,Evaluate the integral $\int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx} $,Evaluate the integral,\int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx} ,"I am trying to evaluate this integral $$I = \int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx} $$ Here is my try: $${\text{Let}}:x \to 1 - 2x \Rightarrow I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)}}{{1 - x}}dx} $$ Since the antiderivative of $${\frac{{\ln \left( x \right)}}{{1 - x}}}$$ is $${{\text{L}}{{\text{i}}_2}\left( { 1-x} \right)}$$ then $$I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\left( {{\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right) - \frac{{{\pi ^2}}}{6}} \right)$$ I am stucking at the remaining integral, I tried to use series expansion of: $\displaystyle \frac{1}{{1 - x}} = \sum\limits_{n = 0}^\infty  {{x^n}}$ but after changing summation and integration, the integral became more complicated. May I ask for help? Thank you so much. By the way, the closed form is $$\frac{{3{\text{L}}{{\text{i}}_3}\left( {\frac{1}{4}} \right)}}{4} - {\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right)\ln (2) + {\text{L}}{{\text{i}}_2}\left( {\frac{1}{4}} \right)\ln (2) + \frac{{\zeta (3)}}{8} + {\ln ^3}(2) + \frac{{{{\ln }^3}(3)}}{3} - {\ln ^2}(3)\ln (2) + \frac{1}{{12}}{\pi ^2}\ln (2)$$","I am trying to evaluate this integral Here is my try: Since the antiderivative of is then I am stucking at the remaining integral, I tried to use series expansion of: but after changing summation and integration, the integral became more complicated. May I ask for help? Thank you so much. By the way, the closed form is",I = \int\limits_{\frac{1}{3}}^{\frac{1}{2}} {\frac{{\ln \left( {1 - x} \right)\ln \left( {1 - 2x} \right)}}{x}dx}  {\text{Let}}:x \to 1 - 2x \Rightarrow I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)}}{{1 - x}}dx}  {\frac{{\ln \left( x \right)}}{{1 - x}}} {{\text{L}}{{\text{i}}_2}\left( { 1-x} \right)} I = \int\limits_0^{\frac{1}{3}} {\frac{{\ln \left( x \right)\ln \left( {1 + x} \right)}}{{1 - x}}dx}  - \ln \left( 2 \right)\left( {{\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right) - \frac{{{\pi ^2}}}{6}} \right) \displaystyle \frac{1}{{1 - x}} = \sum\limits_{n = 0}^\infty  {{x^n}} \frac{{3{\text{L}}{{\text{i}}_3}\left( {\frac{1}{4}} \right)}}{4} - {\text{L}}{{\text{i}}_2}\left( {\frac{2}{3}} \right)\ln (2) + {\text{L}}{{\text{i}}_2}\left( {\frac{1}{4}} \right)\ln (2) + \frac{{\zeta (3)}}{8} + {\ln ^3}(2) + \frac{{{{\ln }^3}(3)}}{3} - {\ln ^2}(3)\ln (2) + \frac{1}{{12}}{\pi ^2}\ln (2),"['calculus', 'integration']"
57,Show that the function is strictly increasing.,Show that the function is strictly increasing.,,"How should I show that the function $f$ defined below is strictly increasing for $x\in(0,1)$ ? I have considered its first derivative, but it seems too complicated to deduce $f'>0$ from there. $f(x)=\frac{3x^2+4x(1-x)+4(1-x)^2}{4x^2}\ln^2{(1-x)}$ $f'(x)=\dfrac{\ln\left(1-x\right)\left(3x^3+\left(2\ln\left(1-x\right)-4\right)x^2+\left(4-6\ln\left(1-x\right)\right)x+4\ln\left(1-x\right)\right)}{2\left(x-1\right)x^3}$","How should I show that the function defined below is strictly increasing for ? I have considered its first derivative, but it seems too complicated to deduce from there.","f x\in(0,1) f'>0 f(x)=\frac{3x^2+4x(1-x)+4(1-x)^2}{4x^2}\ln^2{(1-x)} f'(x)=\dfrac{\ln\left(1-x\right)\left(3x^3+\left(2\ln\left(1-x\right)-4\right)x^2+\left(4-6\ln\left(1-x\right)\right)x+4\ln\left(1-x\right)\right)}{2\left(x-1\right)x^3}","['calculus', 'algebra-precalculus', 'limits', 'functions']"
58,Evaluating $\int \sqrt{\frac{2x+3}{2x-3}}dx$,Evaluating,\int \sqrt{\frac{2x+3}{2x-3}}dx,"I was evaluating $\int \sqrt{\frac{2x+3}{2x-3}}dx$ and got an answer which, I think, is not correct as it is different from wolframalpha's answer. Here's my work: $$\begin{align}\int \sqrt\frac{2x+3}{2x-3} dx & = \int \frac{2x + 3}{\sqrt{4x^2 - 9}} \ dx\tag{1}\\& =\int \frac{x}{\sqrt{x^2 - (3/2)^2}}\ dx  + \frac32\int \frac{dx}{\sqrt{x^2 - (3/2)^2}}\tag{2}\\&= \sqrt{x^2 - (3/2)^2 } + \frac{3}{2}\cosh^{-1}\left(\frac{2x}{3}\right) + C\tag{3}\end{align}$$ Steps: $(1.)$ Rationalized the numerator. $(2.)$ Applied linearity. $(3.)$ The first integral is done by substituting $x^2 - (3/2)^2 = t$ and the  second one is inverse hyperbolic cosine. WolframAlpha shows this . I also tried differentiating both the answers, but still it's different from mine.","I was evaluating and got an answer which, I think, is not correct as it is different from wolframalpha's answer. Here's my work: Steps: Rationalized the numerator. Applied linearity. The first integral is done by substituting and the  second one is inverse hyperbolic cosine. WolframAlpha shows this . I also tried differentiating both the answers, but still it's different from mine.",\int \sqrt{\frac{2x+3}{2x-3}}dx \begin{align}\int \sqrt\frac{2x+3}{2x-3} dx & = \int \frac{2x + 3}{\sqrt{4x^2 - 9}} \ dx\tag{1}\\& =\int \frac{x}{\sqrt{x^2 - (3/2)^2}}\ dx  + \frac32\int \frac{dx}{\sqrt{x^2 - (3/2)^2}}\tag{2}\\&= \sqrt{x^2 - (3/2)^2 } + \frac{3}{2}\cosh^{-1}\left(\frac{2x}{3}\right) + C\tag{3}\end{align} (1.) (2.) (3.) x^2 - (3/2)^2 = t,"['calculus', 'integration', 'indefinite-integrals']"
59,Show that $\int_0^1 (\sin{x}-4/9)^3dx<0$ without a calculator.,Show that  without a calculator.,\int_0^1 (\sin{x}-4/9)^3dx<0,"In this age of electronic devices, sometimes I like to challenge myself to find clever ways to find solutions to math problems that seem to require a calculator, without a calculator. ( Here is a favorite example.) I recently came up with this: Without a calculator, show that $$\int_0^1 \left(\sin{x}-\frac{4}{9}\right)^3dx<0$$ It's a pretty close shave: my computer says the LHS $\approx −0.0000050...$ . The exact value of the LHS is: $$\frac{1}{729}\left(972(2+\cos{1})\sin^4{0.5}-243(2-\sin{2})+432(1-\cos{1})-64\right)$$ I tried to use Maclaurin series, but that seems to be futile. I also tried to find some kind of useful symmetry of the graph of $y=\sin{x}-\frac{4}{9}$ , but to no avail. Any clever way to do this? Just curious.","In this age of electronic devices, sometimes I like to challenge myself to find clever ways to find solutions to math problems that seem to require a calculator, without a calculator. ( Here is a favorite example.) I recently came up with this: Without a calculator, show that It's a pretty close shave: my computer says the LHS . The exact value of the LHS is: I tried to use Maclaurin series, but that seems to be futile. I also tried to find some kind of useful symmetry of the graph of , but to no avail. Any clever way to do this? Just curious.",\int_0^1 \left(\sin{x}-\frac{4}{9}\right)^3dx<0 \approx −0.0000050... \frac{1}{729}\left(972(2+\cos{1})\sin^4{0.5}-243(2-\sin{2})+432(1-\cos{1})-64\right) y=\sin{x}-\frac{4}{9},"['calculus', 'integration', 'inequality']"
60,Find the derivative of the following function w.r.t $x:$ $y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}}$,Find the derivative of the following function w.r.t,x: y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}},"Find $\frac{dy}{dx},$ if $y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}}$ My attempt: If the base and power were the same function,then we could find it's derivative with respect to $x$ as $y=x^y.$ But,here in the question, $\sin x$ and $\cos x$ are two different functions.So,I have no idea how to differentiate it with respect to $x.$ Also,I don't know if this function exist or not.If exist,then find its derivative. Any help would be appreciated.Thank you!","Find if My attempt: If the base and power were the same function,then we could find it's derivative with respect to as But,here in the question, and are two different functions.So,I have no idea how to differentiate it with respect to Also,I don't know if this function exist or not.If exist,then find its derivative. Any help would be appreciated.Thank you!","\frac{dy}{dx}, y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}} x y=x^y. \sin x \cos x x.","['calculus', 'derivatives']"
61,Why is the bizarre $f^{-1}(y)=y+\sum_{n=1}^\infty\frac{1}{n!}\frac{d^{n-1}}{dy^{n-1}}[y-f(y)]^n$ equivalent to the Lagrange inversion formula?,Why is the bizarre  equivalent to the Lagrange inversion formula?,f^{-1}(y)=y+\sum_{n=1}^\infty\frac{1}{n!}\frac{d^{n-1}}{dy^{n-1}}[y-f(y)]^n,"$\newcommand{\d}{\mathrm{d}}$ EDIT: In the nontrivial example $f(x)=x-\frac{1}{4}x^3$ and using either of the two series to produce a result for $f^{-1}(3/4)$ , I find that $(\ast)$ and $(1)$ produce the same result. It would appear that they actually are, by some unseen machinery, equivalent. Why? Op: A collection of past ""Step"" exam questions includes a question based on the following assertion: If $y=f(x)$ , the inverse of $f$ is given by Lagrange's identity : $$\tag{$\ast$}f^{-1}(y)=y+\sum_{n=1}^\infty\frac{1}{n!}\frac{\d^{n-1}}{\d y^{n-1}}[y-f(y)]^n$$ Of course this is not very formally phrased, since the targeted syllabus is pre-university, but let's infer that $f$ is to be a real analytic injection and $y$ in its image (that being said, it still feels as if far too many $y$ s are floating around - is $y$ fixed? Where did $x$ go?) With that, I am still suspicious of this identity. The Lagrange Inversion Theorem as I know it has two forms: Suppose $f:\Bbb C\to\Bbb C$ is analytic at a point $a$ and $f'(a)\neq0$ ; then there is a neighbourhood $V\subseteq\Bbb C$ of $f(a)$ in which the function $g:V\to\Bbb C$ defined by: $$\tag{1}z\mapsto a+\sum_{n=1}^\infty\frac{1}{n!}(z-f(a))^n\cdot\lim_{w\to a}\frac{\d^{n-1}}{\d w^{n-1}}\left[\left(\frac{w-a}{f(w)-f(a)}\right)^n\right]$$ Is a local analytic inverse of $f$ - $f(z)=w\iff z=g(w)$ if $w\in V,z\in f^{-1}(V)$ . There is also a cute combinatorial version, Lagrange-Burmann: Suppose $f:\Bbb C\to\Bbb C$ satisfies $f(w)=w/\phi(w)$ for some analytic $\phi:\Bbb C\to\Bbb C$ with $\phi(0)\neq 0$ . There is a similarly defined as above inverse to $f$ , $g$ , with coefficient formula: $$\tag{2}[w^n]g(w)=\frac{1}{n}[z^{n-1}]\phi(w)^n$$ Let's note that $(\ast)$ is broadly similar but technically different to both $(1)$ and $(2)$ . My question is: I know for a fact $(1),(2)$ are equivalent and correct after many hours of painful research, but $(\ast)$ I have never seen before - how is it equivalent to $(1)$ ? We note that if they are equivalent, this would imply (I think, anyway - the excessive usage of $y$ s is baffling): $$(y-f(x))^n\cdot\lim_{a\to x}\frac{\d^{n-1}}{\d a^{n-1}}\left[\left(\frac{a-x}{f(a)-f(x)}\right)^n\right]\equiv\frac{\d^{n-1}}{\d y^{n-1}}[y-f(y)]^n$$ But this seems ridiculous, especially since no fixed point $x$ is defined, on the right hand side. Moreover a direct equation of coefficients reveals that the centrepoint $x$ should be taken as $y$ , which again seems nonsensical. Is the examiner abusing notation, am I making some mistake, or is $(\ast)$ nonsense as my gut feeling suggests? I surely hope it is not the latter.","EDIT: In the nontrivial example and using either of the two series to produce a result for , I find that and produce the same result. It would appear that they actually are, by some unseen machinery, equivalent. Why? Op: A collection of past ""Step"" exam questions includes a question based on the following assertion: If , the inverse of is given by Lagrange's identity : Of course this is not very formally phrased, since the targeted syllabus is pre-university, but let's infer that is to be a real analytic injection and in its image (that being said, it still feels as if far too many s are floating around - is fixed? Where did go?) With that, I am still suspicious of this identity. The Lagrange Inversion Theorem as I know it has two forms: Suppose is analytic at a point and ; then there is a neighbourhood of in which the function defined by: Is a local analytic inverse of - if . There is also a cute combinatorial version, Lagrange-Burmann: Suppose satisfies for some analytic with . There is a similarly defined as above inverse to , , with coefficient formula: Let's note that is broadly similar but technically different to both and . My question is: I know for a fact are equivalent and correct after many hours of painful research, but I have never seen before - how is it equivalent to ? We note that if they are equivalent, this would imply (I think, anyway - the excessive usage of s is baffling): But this seems ridiculous, especially since no fixed point is defined, on the right hand side. Moreover a direct equation of coefficients reveals that the centrepoint should be taken as , which again seems nonsensical. Is the examiner abusing notation, am I making some mistake, or is nonsense as my gut feeling suggests? I surely hope it is not the latter.","\newcommand{\d}{\mathrm{d}} f(x)=x-\frac{1}{4}x^3 f^{-1}(3/4) (\ast) (1) y=f(x) f \tag{\ast}f^{-1}(y)=y+\sum_{n=1}^\infty\frac{1}{n!}\frac{\d^{n-1}}{\d y^{n-1}}[y-f(y)]^n f y y y x f:\Bbb C\to\Bbb C a f'(a)\neq0 V\subseteq\Bbb C f(a) g:V\to\Bbb C \tag{1}z\mapsto a+\sum_{n=1}^\infty\frac{1}{n!}(z-f(a))^n\cdot\lim_{w\to a}\frac{\d^{n-1}}{\d w^{n-1}}\left[\left(\frac{w-a}{f(w)-f(a)}\right)^n\right] f f(z)=w\iff z=g(w) w\in V,z\in f^{-1}(V) f:\Bbb C\to\Bbb C f(w)=w/\phi(w) \phi:\Bbb C\to\Bbb C \phi(0)\neq 0 f g \tag{2}[w^n]g(w)=\frac{1}{n}[z^{n-1}]\phi(w)^n (\ast) (1) (2) (1),(2) (\ast) (1) y (y-f(x))^n\cdot\lim_{a\to x}\frac{\d^{n-1}}{\d a^{n-1}}\left[\left(\frac{a-x}{f(a)-f(x)}\right)^n\right]\equiv\frac{\d^{n-1}}{\d y^{n-1}}[y-f(y)]^n x x y (\ast)","['calculus', 'complex-analysis', 'power-series', 'inverse-function', 'lagrange-inversion']"
62,"How to evaluate the integral $\int_{0}^{\frac{\pi}{2}}\sqrt[n]{\tan\theta} d \theta$, where $n\geq 2$?","How to evaluate the integral , where ?",\int_{0}^{\frac{\pi}{2}}\sqrt[n]{\tan\theta} d \theta n\geq 2,"In my post , I started to investigate the integral $\int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta$ and then $\int \sqrt[3]{\tan \theta} d \theta$ in post . After encountering the Beta Functions, I want to try to apply it to the integral. $$ \begin{aligned} \int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta &=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{2}} \theta \cos ^{-\frac{1}{2}} \theta d \theta \\ &=\int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{3}{4}\right)-1} \theta \cos ^{2\left(\frac{1}{4}\right)-1} \theta d \theta \\ &=\frac{1}{2} B\left(\frac{3}{4}, \frac{1}{4}\right) \\&=\frac{\pi}{\sqrt2}  \end{aligned} $$ I then go further to $$ I_n=\int_{0}^{\frac{\pi}{2} }\sqrt[n]{\tan \theta} d\theta. $$ Similarly $$ \begin{aligned} I_n&=\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{n}  } \theta d \theta \\ &=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{n} } \theta \cos ^{-\frac{1}{n} } \theta d \theta \\ &= \int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{n+1}{2 n}\right)-1} \theta \cos ^{2\left(\frac{n-1}{2 n}\right)-1} d \theta \\ &=\frac{1}{2} B\left(\frac{n+1}{2 n}, \frac{n-1}{2 n}\right) \end{aligned} $$ Applying the theorem $$ B(x, 1-x)=\pi \csc (\pi x), \textrm{ where } x\notin Z $$ gives $$ \boxed{\int_{0}^{\frac{\pi}{2}} \sqrt[n]{\tan \theta} d \theta =\frac{\pi}{2} \csc \left(\frac{n+1}{2 n} \pi\right)=\frac{\pi}{2} \sec \left(\frac{\pi}{2 n}\right)} $$ which is unexpectedly beautiful and decent. Furthermore Replacing $\frac{1}{n}$ by $a$ yields $$\boxed{ \int_{0}^{\frac{\pi}{2}} \tan ^{a} \theta d \theta =\frac{\pi}{2} \csc \left(\frac{a+1}{2} \pi\right)=\frac{\pi}{2} \sec \frac{a \pi}{2}} $$ For example, $$ \int_{0}^{\frac{\pi}{2}} \sqrt[3]{\tan \theta} d \theta =I\left(\frac{1}{3}\right) =\frac{\pi}{2} \sec \left(\frac{ \pi}{6}\right)=\frac{\pi}{\sqrt{3}} $$ $$\int_{0}^{\frac{\pi}{2}} \sqrt[6]{\tan \theta} d \theta =I\left(\frac{1}{6} \right)=\frac{\pi}{2} \sec\left(\frac{\pi}{12}\right)=\pi \sqrt{2-\sqrt{3}}$$ $$ \int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{e}}\theta d\theta =\frac{\pi}{2} \sec \frac{\pi}{2 e} $$ $$ \int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{\pi}} \theta d \theta=\frac{\pi}{2} \sec \frac{1}{2} $$ checked by Wolframalpha. My question : Is there a method without using Beta Functions?","In my post , I started to investigate the integral and then in post . After encountering the Beta Functions, I want to try to apply it to the integral. I then go further to Similarly Applying the theorem gives which is unexpectedly beautiful and decent. Furthermore Replacing by yields For example, checked by Wolframalpha. My question : Is there a method without using Beta Functions?","\int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta \int \sqrt[3]{\tan \theta} d \theta 
\begin{aligned}
\int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta &=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{2}} \theta \cos ^{-\frac{1}{2}} \theta d \theta \\
&=\int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{3}{4}\right)-1} \theta \cos ^{2\left(\frac{1}{4}\right)-1} \theta d \theta \\
&=\frac{1}{2} B\left(\frac{3}{4}, \frac{1}{4}\right) \\&=\frac{\pi}{\sqrt2} 
\end{aligned}
 
I_n=\int_{0}^{\frac{\pi}{2} }\sqrt[n]{\tan \theta} d\theta.
 
\begin{aligned}
I_n&=\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{n}  } \theta d \theta \\
&=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{n} } \theta \cos ^{-\frac{1}{n} } \theta d \theta \\
&= \int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{n+1}{2 n}\right)-1} \theta \cos ^{2\left(\frac{n-1}{2 n}\right)-1} d \theta \\
&=\frac{1}{2} B\left(\frac{n+1}{2 n}, \frac{n-1}{2 n}\right)
\end{aligned}
 
B(x, 1-x)=\pi \csc (\pi x), \textrm{ where } x\notin Z
 
\boxed{\int_{0}^{\frac{\pi}{2}} \sqrt[n]{\tan \theta} d \theta =\frac{\pi}{2} \csc \left(\frac{n+1}{2 n} \pi\right)=\frac{\pi}{2} \sec \left(\frac{\pi}{2 n}\right)}
 \frac{1}{n} a \boxed{
\int_{0}^{\frac{\pi}{2}} \tan ^{a} \theta d \theta =\frac{\pi}{2} \csc \left(\frac{a+1}{2} \pi\right)=\frac{\pi}{2} \sec \frac{a \pi}{2}}
 
\int_{0}^{\frac{\pi}{2}} \sqrt[3]{\tan \theta} d \theta =I\left(\frac{1}{3}\right) =\frac{\pi}{2} \sec \left(\frac{ \pi}{6}\right)=\frac{\pi}{\sqrt{3}}  \int_{0}^{\frac{\pi}{2}} \sqrt[6]{\tan \theta} d \theta =I\left(\frac{1}{6} \right)=\frac{\pi}{2} \sec\left(\frac{\pi}{12}\right)=\pi \sqrt{2-\sqrt{3}} 
\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{e}}\theta d\theta =\frac{\pi}{2} \sec \frac{\pi}{2 e}
 
\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{\pi}} \theta d \theta=\frac{\pi}{2} \sec \frac{1}{2}
","['calculus', 'integration', 'definite-integrals', 'beta-function']"
63,Is my method for computing $\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6}$ valid?,Is my method for computing  valid?,\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6},"I know that if $\lim_{x \to a}g(x)$ exists and $f(x)$ is continuous at $\lim_{x \to a}g(x)$ , then we can interchange the limit with $f(x)$ . That is, $$ \lim_{x\to a}f(g(x))=f(\lim_{x\to a}g(x)). $$ So onto the limit in question: I'd like to show that $$ \lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6}. $$ I start off by letting $$ L = \lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}. $$ Next, I apply the natural logarithm to both sides of the equation to give $$ \ln L = \ln\left(\lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}\right). $$ But now, I would like to interchange the natural log with the limit on the right side of the equation. However, this seems not justified because I don't know, a priori, that $\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2}$ exists and if it does exist, that the natural log is even defined at the limit (what if it's $0$ or negative?). Thus, my method of interchanging the natural log with the limit seems circular. I need to assume the limit exists and is neither $0$ nor negative before proceeding with my computation. So here are my questions : Is it actually justified to interchange the natural log with the limit in this type of computation? Is it commonplace and accepted to assume the limit exists when going about these types of computations? If not, can someone propsose an alternative method for computing this limit without using the epsilon-delta definition?","I know that if exists and is continuous at , then we can interchange the limit with . That is, So onto the limit in question: I'd like to show that I start off by letting Next, I apply the natural logarithm to both sides of the equation to give But now, I would like to interchange the natural log with the limit on the right side of the equation. However, this seems not justified because I don't know, a priori, that exists and if it does exist, that the natural log is even defined at the limit (what if it's or negative?). Thus, my method of interchanging the natural log with the limit seems circular. I need to assume the limit exists and is neither nor negative before proceeding with my computation. So here are my questions : Is it actually justified to interchange the natural log with the limit in this type of computation? Is it commonplace and accepted to assume the limit exists when going about these types of computations? If not, can someone propsose an alternative method for computing this limit without using the epsilon-delta definition?","\lim_{x \to a}g(x) f(x) \lim_{x \to a}g(x) f(x) 
\lim_{x\to a}f(g(x))=f(\lim_{x\to a}g(x)).
 
\lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} = e^{-1/6}.
 
L = \lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}.
 
\ln L = \ln\left(\lim_{x \to 0^+}\left(\dfrac{\sin x}{x} \right)^{1/x^2}\right).
 \lim_{x \to 0^+} \left( \dfrac{\sin x}{x} \right)^{1/x^2} 0 0","['calculus', 'limits']"
64,How do I change the order of integration for this parabolic wedge triple integral?,How do I change the order of integration for this parabolic wedge triple integral?,,"The question at hand asked me to change the order of the triple integral $$\int_{-1}^{1}\int_{x^{2}}^{1}\int_{0}^{y} f(x,y,z)dz dy dx$$ to $dy dz dx$ . It seemed straight forward at first, seeing that y is bounded by $y=z$ and $y=1$ , and my region in the x-z plane would look like this: But this region seems to say that z is bound by $z=0$ and $z=1$ , and x bound by $x=-1$ and $x=1$ . But putting those as my bounds would just express a rectangular block cut in half and not the solid. I suppose my initial assumption of y being bound by $y=z$ and $y=1$ doesn't allow me to take into the account the $y=x^{2}$ also binding it. How should I approach this?","The question at hand asked me to change the order of the triple integral to . It seemed straight forward at first, seeing that y is bounded by and , and my region in the x-z plane would look like this: But this region seems to say that z is bound by and , and x bound by and . But putting those as my bounds would just express a rectangular block cut in half and not the solid. I suppose my initial assumption of y being bound by and doesn't allow me to take into the account the also binding it. How should I approach this?","\int_{-1}^{1}\int_{x^{2}}^{1}\int_{0}^{y} f(x,y,z)dz dy dx dy dz dx y=z y=1 z=0 z=1 x=-1 x=1 y=z y=1 y=x^{2}","['calculus', 'integration']"
65,How do you solve the following integral?,How do you solve the following integral?,,"How do you solve the following integral $$\int_0^{2\pi}\sqrt{\frac{5}{4} + \cos(t)} \, dt.$$ It has been a while since I have done integration and I think somehow I could multpily by the conjugate but I am unsure.  Any assistance would be appreciated. This problem is part of a larger problem to find the length of the cardiod. $$r(\theta)=\frac{1}{2}+\cos(\theta).$$ This integral is the length integral and I originally just found a numerical approximation but after reviewing my problem my teacher said there is a nice way to antidifferentiate but I was unsure how to go about that.",How do you solve the following integral It has been a while since I have done integration and I think somehow I could multpily by the conjugate but I am unsure.  Any assistance would be appreciated. This problem is part of a larger problem to find the length of the cardiod. This integral is the length integral and I originally just found a numerical approximation but after reviewing my problem my teacher said there is a nice way to antidifferentiate but I was unsure how to go about that.,"\int_0^{2\pi}\sqrt{\frac{5}{4} + \cos(t)} \, dt. r(\theta)=\frac{1}{2}+\cos(\theta).","['calculus', 'integration']"
66,Nested sums - reciprocal of a sum : Find exact value,Nested sums - reciprocal of a sum : Find exact value,,"$$\text{Find:} ~~~~~~ \sum_{k=1}^{\infty} \frac{1}{ \left (  \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2}$$ (Beware of the bounds of $j$ , it does not always start from $1$ , but starts at $k$ and goes to $k^2$ ) At first I thought it has something to do with Riemann's zeta function $\zeta$ and thus the solution is: $$ \zeta \left( \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}} \right ) $$ However this totally seems incorrect because we cannot find this sum as we have an unknown, $k$ . Which also is being summed to $\infty$ . I thought (however was unsure) of the fact that: $$ \left ( \sum f \right )^2 \ge  \sum f^2  $$ Thus (maybe) it is trivial the sum is converging to some value, which according to a little program I wrote is about: $$ \sum_{k=1}^{\infty} \frac{1}{ \left (  \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2} \approx 1.596$$ But this is not a rigorous proof, is there a way to solve this so we get a solution by hand - calculating the exact value of this sum if it is finite (if not, why? ) Any mathematical tools (not programs) are acceptable, because this is not a question from a specific course ( I don't have context for this).","(Beware of the bounds of , it does not always start from , but starts at and goes to ) At first I thought it has something to do with Riemann's zeta function and thus the solution is: However this totally seems incorrect because we cannot find this sum as we have an unknown, . Which also is being summed to . I thought (however was unsure) of the fact that: Thus (maybe) it is trivial the sum is converging to some value, which according to a little program I wrote is about: But this is not a rigorous proof, is there a way to solve this so we get a solution by hand - calculating the exact value of this sum if it is finite (if not, why? ) Any mathematical tools (not programs) are acceptable, because this is not a question from a specific course ( I don't have context for this).","\text{Find:} ~~~~~~ \sum_{k=1}^{\infty} \frac{1}{ \left (
 \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2} j 1 k k^2 \zeta  \zeta \left( \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}} \right )  k \infty  \left ( \sum f \right )^2 \ge  \sum f^2    \sum_{k=1}^{\infty} \frac{1}{ \left (
 \sum_{j=k}^{k^2} \frac{1}{\sqrt{~j~}}  \right )^2} \approx 1.596","['calculus', 'sequences-and-series', 'summation', 'riemann-zeta']"
67,"What is a condition for two real functions $f,g$ to ""commute"", so $f(g(x))=g(f(x))$?","What is a condition for two real functions  to ""commute"", so ?","f,g f(g(x))=g(f(x))","Say I'm given two functions $f,g$ . Can I tell if they ""commute"" without actually trying them in the formula $f(g(x))=g(f(x))$ ? And given a function $f$ , is there a way to find all functions $g$ such that $f(g(x))=g(f(x))$ ? I've tried using derivatives and the chain rule but haven't got anything interesting yet.","Say I'm given two functions . Can I tell if they ""commute"" without actually trying them in the formula ? And given a function , is there a way to find all functions such that ? I've tried using derivatives and the chain rule but haven't got anything interesting yet.","f,g f(g(x))=g(f(x)) f g f(g(x))=g(f(x))","['calculus', 'algebra-precalculus', 'functions', 'real-numbers']"
68,Evaluate $\int x^2 \sin(7x^3)dx$,Evaluate,\int x^2 \sin(7x^3)dx,"Evaluate $$\int x^2 \sin(7x^3)dx.$$ Can somebody check my solution? Thanks! Let $u = 7x^3$ , then $du = 21x^2dx$ and so $\dfrac{du}{21x^2}=dx$ . Thus we have: $$\int x^2 \sin(7x^3)dx=\frac{1}{21} \int \sin(u)du = \frac{-1}{21}\cos(u)+C = \frac{-1}{21}\cos(7x^3)+C$$","Evaluate Can somebody check my solution? Thanks! Let , then and so . Thus we have:",\int x^2 \sin(7x^3)dx. u = 7x^3 du = 21x^2dx \dfrac{du}{21x^2}=dx \int x^2 \sin(7x^3)dx=\frac{1}{21} \int \sin(u)du = \frac{-1}{21}\cos(u)+C = \frac{-1}{21}\cos(7x^3)+C,['calculus']
69,How to show :$\int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt \frac {2018}{2020}$,How to show :,\int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt \frac {2018}{2020},I tried to solve this problem as $$0\lt x \lt 1 \implies 0\lt x^{2018} \lt 1 \implies 0\lt (1-x^{2018}) \lt 1$$ this means $$\int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt   \int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}\right] dx \lt \int_0^1 1^{{1\over 2020}} dx$$ as inequality can be integrated. So I have come as far as proving the given  expression is less than 1 but I cannot proceed further. Can someone show me how to proceed? (Please try give answers which can be understood by those in elementary calculus courses),I tried to solve this problem as this means as inequality can be integrated. So I have come as far as proving the given  expression is less than 1 but I cannot proceed further. Can someone show me how to proceed? (Please try give answers which can be understood by those in elementary calculus courses),0\lt x \lt 1 \implies 0\lt x^{2018} \lt 1 \implies 0\lt (1-x^{2018}) \lt 1 \int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}- \left(1-x^{2020}\right)^{1\over 2018} \right] dx \lt   \int_0^1 \left[\left(1-x^{2018}\right)^{1\over 2020}\right] dx \lt \int_0^1 1^{{1\over 2020}} dx,"['calculus', 'integration', 'inequality', 'definite-integrals']"
70,Does the sum $\frac{1}{3} + \frac{1}{3^{1+1/2}}+\cdots$ have a closed form?,Does the sum  have a closed form?,\frac{1}{3} + \frac{1}{3^{1+1/2}}+\cdots,"Evaluate the sum $$\frac{1}{3} + \frac{1}{3^{1+\frac{1}{2}}}+\frac{1}{3^{1+\frac{1}{2}+\frac{1}{3}}}+\cdots$$ It seems that $1 + \dfrac{1}{2} + \dfrac{1}{3} + \cdots + \dfrac{1}{n}$ approaches $\ln n$ as $n\to \infty$ , but I'm not sure if this is useful. Also, $3^{\ln n} =e^{\ln n\cdot \ln 3}= n^{\ln 3}$ , but I'm also not sure how this is useful. edit: I know how to prove that it converges, but I was wondering if there was a closed form for this sum.","Evaluate the sum It seems that approaches as , but I'm not sure if this is useful. Also, , but I'm also not sure how this is useful. edit: I know how to prove that it converges, but I was wondering if there was a closed form for this sum.",\frac{1}{3} + \frac{1}{3^{1+\frac{1}{2}}}+\frac{1}{3^{1+\frac{1}{2}+\frac{1}{3}}}+\cdots 1 + \dfrac{1}{2} + \dfrac{1}{3} + \cdots + \dfrac{1}{n} \ln n n\to \infty 3^{\ln n} =e^{\ln n\cdot \ln 3}= n^{\ln 3},['calculus']
71,Can we prove the irrationality of pi from its expression as a limiting product with the nested square roots?,Can we prove the irrationality of pi from its expression as a limiting product with the nested square roots?,,"Is it possible to prove the irrationality of $ \pi $ from the expression $ \pi = Lim_{m \to \infty} [2^m(\sqrt(2-\sqrt(2+\sqrt(2+...)...)))]$ , where there are $m$ square roots in the nested expression? This expression is obtained by considering the ratio of the perimeter of a regular $n$ -sided polygon to the diameter of its circumcircle and then letting $ n \to \infty$ . This expression was found by Archimedes I think.But didn't see Archimedes' name as one of those proving the irrationality. Makes me suspect whether this can be done. Can anyone please provide me a method (if it exists)? Any indirect method starting from here would be fine.","Is it possible to prove the irrationality of from the expression , where there are square roots in the nested expression? This expression is obtained by considering the ratio of the perimeter of a regular -sided polygon to the diameter of its circumcircle and then letting . This expression was found by Archimedes I think.But didn't see Archimedes' name as one of those proving the irrationality. Makes me suspect whether this can be done. Can anyone please provide me a method (if it exists)? Any indirect method starting from here would be fine.", \pi   \pi = Lim_{m \to \infty} [2^m(\sqrt(2-\sqrt(2+\sqrt(2+...)...)))] m n  n \to \infty,"['calculus', 'geometry', 'number-theory']"
72,Solving $\sin(x) = \ln(x)$,Solving,\sin(x) = \ln(x),"While tutoring a student in introductory calculus, I inquired as to how she would set up an integral to determine the volume of the solid created by revolving the region bounded by the functions $\sin(x)$ , $\ln(x)$ , and the $x$ -axis about the line $y=5$ . She set up the integral as follows: $$\pi\int_{1}^{a} 25-(\ln(x))^2 dx + \pi\int_{a}^{\pi} 25-\sin^2(x)dx $$ where $a$ is the solution to $\sin(x) = \ln(x)$ . However, I couldn't give an explanation as to how to find this point. Is this something that could be solved given two years of introductory calculus, or does it require a larger toolkit? In searching for an answer to this problem online, I was bombarded by hits on $\ln(x)$ differentiation resources which obviously weren't helpful. Any light you could shed would be appreciated!","While tutoring a student in introductory calculus, I inquired as to how she would set up an integral to determine the volume of the solid created by revolving the region bounded by the functions , , and the -axis about the line . She set up the integral as follows: where is the solution to . However, I couldn't give an explanation as to how to find this point. Is this something that could be solved given two years of introductory calculus, or does it require a larger toolkit? In searching for an answer to this problem online, I was bombarded by hits on differentiation resources which obviously weren't helpful. Any light you could shed would be appreciated!",\sin(x) \ln(x) x y=5 \pi\int_{1}^{a} 25-(\ln(x))^2 dx + \pi\int_{a}^{\pi} 25-\sin^2(x)dx  a \sin(x) = \ln(x) \ln(x),"['calculus', 'integration', 'definite-integrals']"
73,Evaluating $\sum_{k=1}^{\infty} 2\ln{(2k)} - \ln{(2k-1)} - \ln{(2k+1)} $,Evaluating,\sum_{k=1}^{\infty} 2\ln{(2k)} - \ln{(2k-1)} - \ln{(2k+1)} ,"I was trying to evaluate the following series, which I know converges: $$\sum_{k=1}^{\infty} 2\ln{(2k)} - \ln{(2k-1)} - \ln{(2k+1)} \tag{1}\label{1} $$ In a telescoping fashion, I began writing out the terms in hopes to find a pattern: $$= (2\ln{2} - \ln{1} - \ln{3}) + (2\ln{4} - \ln{3} - \ln{5}) + (2\ln{6} - \ln{5} - \ln{7}) + \ldots \tag{2}\label{2}$$ while nothing canceled out, I grouped terms together: $$ = 2\ln{2} - 2\ln{3} + 2\ln{4} - 2\ln{5} + 2\ln{6} - 2\ln{7} + \ldots \tag{3}\label{3}$$ $$ = 2 \left[ \ln{2} - \ln{3} + \ln{4} - \ln{5} + \ln{6} - \ln{7} + \ldots \right] \tag{4}\label{4}$$ which left me with the following divergent series: $$ = 2 \sum _{k=2} ^{\infty} (-1)^k \ln{k} \tag{5}\label{5}$$ Clearly, $\eqref{5}$ can't be equivalent to $\eqref{1}$. I'm pretty new to calculus, and while I've covered telescoping series, it seems that this technique cannot be applied here. Though, I don't know why. Where did I go wrong?","I was trying to evaluate the following series, which I know converges: $$\sum_{k=1}^{\infty} 2\ln{(2k)} - \ln{(2k-1)} - \ln{(2k+1)} \tag{1}\label{1} $$ In a telescoping fashion, I began writing out the terms in hopes to find a pattern: $$= (2\ln{2} - \ln{1} - \ln{3}) + (2\ln{4} - \ln{3} - \ln{5}) + (2\ln{6} - \ln{5} - \ln{7}) + \ldots \tag{2}\label{2}$$ while nothing canceled out, I grouped terms together: $$ = 2\ln{2} - 2\ln{3} + 2\ln{4} - 2\ln{5} + 2\ln{6} - 2\ln{7} + \ldots \tag{3}\label{3}$$ $$ = 2 \left[ \ln{2} - \ln{3} + \ln{4} - \ln{5} + \ln{6} - \ln{7} + \ldots \right] \tag{4}\label{4}$$ which left me with the following divergent series: $$ = 2 \sum _{k=2} ^{\infty} (-1)^k \ln{k} \tag{5}\label{5}$$ Clearly, $\eqref{5}$ can't be equivalent to $\eqref{1}$. I'm pretty new to calculus, and while I've covered telescoping series, it seems that this technique cannot be applied here. Though, I don't know why. Where did I go wrong?",,"['calculus', 'sequences-and-series']"
74,"Can I use Leibniz' rule to differentiate when integral is $\int_0^\gamma f(\gamma,s)dF(s)$, instead of $\int_0^\gamma f(\gamma,s)ds$ ($F$ is a CDF)","Can I use Leibniz' rule to differentiate when integral is , instead of  ( is a CDF)","\int_0^\gamma f(\gamma,s)dF(s) \int_0^\gamma f(\gamma,s)ds F","Specifically, suppose I have a CDF $F$ and an integral $$ \int_0^\gamma (\gamma-s)dF(s) $$ and I want to find $$ \frac{d}{d\gamma} \int_0^\gamma (\gamma-s)dF(s) $$ (Note that the upper limit $\gamma$ indicates that $s=\gamma$) If the integral was $\int_0^\gamma (\gamma-s)ds$ then I can apply Liebniz rule. Can I still use Liebniz rule even though my integral is of the form $\int_0^\gamma f(\gamma,s)dF(s)$, instead of $\int_0^\gamma f(\gamma,s)ds$ (if $F$ is differentiable than I can (rigorously/correctly) write $dF(s) = f(s)ds$ and have the standard form. But if $F$ has mass point(s)... then I don't know)","Specifically, suppose I have a CDF $F$ and an integral $$ \int_0^\gamma (\gamma-s)dF(s) $$ and I want to find $$ \frac{d}{d\gamma} \int_0^\gamma (\gamma-s)dF(s) $$ (Note that the upper limit $\gamma$ indicates that $s=\gamma$) If the integral was $\int_0^\gamma (\gamma-s)ds$ then I can apply Liebniz rule. Can I still use Liebniz rule even though my integral is of the form $\int_0^\gamma f(\gamma,s)dF(s)$, instead of $\int_0^\gamma f(\gamma,s)ds$ (if $F$ is differentiable than I can (rigorously/correctly) write $dF(s) = f(s)ds$ and have the standard form. But if $F$ has mass point(s)... then I don't know)",,"['calculus', 'integration', 'derivatives']"
75,Sum of Independent Half-Normal Distributions with unequal variance,Sum of Independent Half-Normal Distributions with unequal variance,,"$\DeclareMathOperator{\erf}{erf}$ I want to find the cumulative distribution function (CDF) of the random variable $Z = |X| + |Y|$ where $X$ and $Y$ are two independent random variables which are normally distributed with mean 0 but different variance $\sigma_X^2$ and $\sigma^2_{Y}$ . $|X|$ and $|Y|$ follow the so-called Half-Normal distribution, which is a special case of the Folded-Normal distribution, when $\mu = 0$ . The probability density functions (PDF) of $|X|$ and $|Y|$ for $x > 0$ are defined as follows ( wikipedia ): \begin{align} \label{pdf} f_{|X|}(x, \sigma_{X}) =  \frac{\sqrt{2}}{\sigma_{X}  \sqrt{\pi}}  \exp \left(- \frac{x^2}{2\sigma_{X}^2}\right) && f_{|Y|}(y, \sigma_{Y}) =  \frac{\sqrt{2}}{\sigma_{Y}  \sqrt{\pi}} \exp\left(- \frac{y^2}{2\sigma_{Y}^2}\right) \end{align} And their CDF: \begin{align} \label{cdf} F_{|X|}(x, \sigma_{X}) = \erf\left(\frac{x}{\sigma_{X} \sqrt{2}}\right) && F_{|Y|}(y, \sigma_{Y}) = \erf\left(\frac{y}{\sigma_{Y} \sqrt{2}}\right) \end{align} @DilipSarwate gave a very elegant solution for the case $\sigma_{Y} = \sigma_{X}$ here , but I am interested in the general case when this is not true, i.e. one of the variable has a higher weight in the final value (bigger scale). So far, I have determined the PDF of $Z$ by doing the convolution of $f_{|X|}$ and $f_{|Y|}$ . Since $f_{|X|}$ and $f_{|Y|}$ are defined only for $x > 0$ , we can truncate the convolution to the interval $[0,z]$ : \begin{align} f_Z(z) &= \int_0^z f_{|Y|}(z-x) f_{|X|}(x) \, dx \\ f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \int_0^z \exp\left(\frac{-(z-x)^2}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\ f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \int_0^z  \exp\left(\frac{-(x^2 - 2xz + z^2)}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\ f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_0^z \exp\left(\frac{-x^2+2xz}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\ f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_0^z \exp\left(\frac{-(\sigma_X^2 + \sigma_Y^2)x^2 + 2z\sigma_X^2 x}{2 \sigma_X^2 \sigma_Y^2}\right) \, dx  \end{align} We define $\sigma_Z = \sqrt{\left(\sigma_X^2 + \sigma_Y^2\right)}$ : \begin{align} f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 x^2 + 2z\sigma_X^2 x}{2 \sigma_X^2 \sigma_Y^2}\right) dx  \end{align} We complete the square $ax^2 + bx$ to the form $a(x-h)^2 + k$ with $h = -\frac{b}{2a}$ and $k = - \frac{b^2}{4a}$ , i.e., $h = \frac{z\sigma_X^2}{\sigma_Z^2}$ and $k = \frac{(z \sigma_X^2)^2}{\sigma_Z^2}$ . This results in: \begin{align} f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right)  \int_{0}^{z}  \exp\left(\frac{-\sigma_Z^2\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2  	+ \frac{(z \sigma_X^2)^2}{\sigma_Z^2}}{2\sigma_X^2 \sigma_Y^2}\right) dx \\ f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \exp\left(\frac{(z\sigma_X^2)^2}{2\sigma_X^2 \sigma_Y^2 \sigma_Z^2}\right) \int_{0}^{z}  \exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx \\ f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2(\sigma_Z^2 - \sigma_X^2)}{2\sigma_Y^2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx \\ f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2} \right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx  \end{align} \begin{align} f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\frac{\sigma_X^2 \sigma_Y^2}{\sigma_Z^2}}\right) dx \\ \end{align} We add the term $\frac{\sqrt{2 \pi (\frac{\sigma_X \sigma_Y}{\sigma_Z})^2}}{\sqrt{2 \pi (\frac{\sigma_X \sigma_Y}{\sigma_Z})^2}}$ : \begin{align} f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi}  \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right)  \sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}  \int_{0}^{z}  \frac{1}{\sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}\right) dx \\ f_{Z}(z) &= \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right)  \int_{0}^{z}  \frac{1}{\sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}}  \exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}\right) dx  \end{align} As we can see, the expression in the integral represents the density of a Gaussian distribution with mean $\frac{z \sigma_X^2}{\sigma_Z^2}$ and standard deviation $\frac{\sigma_X \sigma_Y}{\sigma_Z}$ . As a results we can evaluate it as follows: \begin{align} f_{Z}(z) &= \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \left[\Phi\left(z, \frac{z \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right) - \Phi\left(0, \frac{z \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right)\right] \end{align} where $\Phi(x,m,s)$ is the value of the cumulative distribution function at $x$ of a Gaussian distribution with mean $m$ and standard deviation $s$ . Now we can compute the cumulative distribution function $F_Z$ : \begin{align} F_{Z}(z) &= \int_{0}^{z} f_{Z}(x) dx \\ F_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[\Phi\left(x, \frac{x \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right) - \Phi\left(0, \frac{x \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right)\right] dx \\ F_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[ \frac{1}{2} \left( 1 + \erf \left(\frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) \right) -  \frac{1}{2} \left( 1 + \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) \right) \right] dx  \\ F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[ \erf \left( \frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)  -  \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) \right] dx  \end{align} Since the integral of the sum is equal to the sum of the integrals: \begin{align} F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[  \int_{0}^{z}  \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)  \erf \left( \frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) dx  -  \int_{0}^{z}  \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)  \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)  dx \right] \\ F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[   \int_{0}^{z}   \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)   \erf \left( \frac{x \left(1-\frac{\sigma_X^2}{\sigma_Z^2} \right)}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) dx  -   \int_{0}^{z}   \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)   \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)  dx \right] \\  F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[   \int_{0}^{z}   \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)   \erf \left( x \frac{\sigma_Y}{\sqrt{2} \sigma_Z \sigma_X } \right) dx  -   \int_{0}^{z}   \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)   \erf \left(- x \frac{\sigma_X}{\sqrt{2} \sigma_Z \sigma_Y } \right)  dx \right] \end{align} We know that $\erf(x)$ is an odd function and that $\exp(x)$ is an even function, as a result, their product is odd, and thus: \begin{align} F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[  \int_{0}^{z}  \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)  \erf \left( x \frac{\sigma_Y}{\sqrt{2} \sigma_Z \sigma_X } \right) dx  + \int_{0}^{z}  \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)  \erf \left(x \frac{\sigma_X}{\sqrt{2} \sigma_Z \sigma_Y } \right) dx \right] \end{align} This is where I am stuck. It seems that the integrals are not elementary. What can I do? Just to verify my results in the case of equal variance, I am going to assume that $\sigma_X = \sigma_Y$ . From 3.3 (41) in this paper , We know that: $$ \int \exp \left( -a^2 x^2 \right) \left[ \erf(ax)\right]^n \, dx = \frac{\sqrt{\pi}}{2a(n+1)} \left[\erf(ax)\right]^{n+1} $$ Thus: \begin{align} F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}   \int_{0}^{z}  \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right)  \erf \left( \frac{x}{\sqrt{2} \sigma_Z } \right) dx \\ F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}   \frac{\sqrt{\pi}}{4 \frac{1}{\sqrt{2} \sigma_Z}} \erf(\frac{z}{\sqrt{2} \sigma_Z})^2 \\ F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \erf(\frac{z}{\sqrt{2} \sigma_Z})^2 \end{align} Which is consistent with the solution from here . However, as I said, I don't know how to solve the problem when $\sigma_X \neq \sigma_Y$ . Any suggestions? I think I have no choice but to use a numerical approximation method, such as Simpson's rule.","I want to find the cumulative distribution function (CDF) of the random variable where and are two independent random variables which are normally distributed with mean 0 but different variance and . and follow the so-called Half-Normal distribution, which is a special case of the Folded-Normal distribution, when . The probability density functions (PDF) of and for are defined as follows ( wikipedia ): And their CDF: @DilipSarwate gave a very elegant solution for the case here , but I am interested in the general case when this is not true, i.e. one of the variable has a higher weight in the final value (bigger scale). So far, I have determined the PDF of by doing the convolution of and . Since and are defined only for , we can truncate the convolution to the interval : We define : We complete the square to the form with and , i.e., and . This results in: We add the term : As we can see, the expression in the integral represents the density of a Gaussian distribution with mean and standard deviation . As a results we can evaluate it as follows: where is the value of the cumulative distribution function at of a Gaussian distribution with mean and standard deviation . Now we can compute the cumulative distribution function : Since the integral of the sum is equal to the sum of the integrals: We know that is an odd function and that is an even function, as a result, their product is odd, and thus: This is where I am stuck. It seems that the integrals are not elementary. What can I do? Just to verify my results in the case of equal variance, I am going to assume that . From 3.3 (41) in this paper , We know that: Thus: Which is consistent with the solution from here . However, as I said, I don't know how to solve the problem when . Any suggestions? I think I have no choice but to use a numerical approximation method, such as Simpson's rule.","\DeclareMathOperator{\erf}{erf} Z = |X| + |Y| X Y \sigma_X^2 \sigma^2_{Y} |X| |Y| \mu = 0 |X| |Y| x > 0 \begin{align}
\label{pdf}
f_{|X|}(x, \sigma_{X}) =  \frac{\sqrt{2}}{\sigma_{X}  \sqrt{\pi}}  \exp \left(- \frac{x^2}{2\sigma_{X}^2}\right) && f_{|Y|}(y, \sigma_{Y}) =  \frac{\sqrt{2}}{\sigma_{Y}  \sqrt{\pi}} \exp\left(- \frac{y^2}{2\sigma_{Y}^2}\right)
\end{align} \begin{align}
\label{cdf}
F_{|X|}(x, \sigma_{X}) = \erf\left(\frac{x}{\sigma_{X} \sqrt{2}}\right) && F_{|Y|}(y, \sigma_{Y}) = \erf\left(\frac{y}{\sigma_{Y} \sqrt{2}}\right)
\end{align} \sigma_{Y} = \sigma_{X} Z f_{|X|} f_{|Y|} f_{|X|} f_{|Y|} x > 0 [0,z] \begin{align}
f_Z(z) &= \int_0^z f_{|Y|}(z-x) f_{|X|}(x) \, dx \\
f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \int_0^z \exp\left(\frac{-(z-x)^2}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\
f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \int_0^z 
\exp\left(\frac{-(x^2 - 2xz + z^2)}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\
f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_0^z \exp\left(\frac{-x^2+2xz}{2\sigma_Y^2}\right) \exp\left(\frac{-x^2}{2\sigma_X^2}\right) \, dx \\
f_Z(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_0^z \exp\left(\frac{-(\sigma_X^2 + \sigma_Y^2)x^2 + 2z\sigma_X^2 x}{2 \sigma_X^2 \sigma_Y^2}\right) \, dx 
\end{align} \sigma_Z = \sqrt{\left(\sigma_X^2 + \sigma_Y^2\right)} \begin{align}
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2\sigma_Y^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 x^2 + 2z\sigma_X^2 x}{2 \sigma_X^2 \sigma_Y^2}\right) dx 
\end{align} ax^2 + bx a(x-h)^2 + k h = -\frac{b}{2a} k = - \frac{b^2}{4a} h = \frac{z\sigma_X^2}{\sigma_Z^2} k = \frac{(z \sigma_X^2)^2}{\sigma_Z^2} \begin{align}
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi}
\exp\left(\frac{-z^2}{2\sigma_Y^2}\right) 
\int_{0}^{z} 
\exp\left(\frac{-\sigma_Z^2\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2 
	+ \frac{(z \sigma_X^2)^2}{\sigma_Z^2}}{2\sigma_X^2 \sigma_Y^2}\right) dx \\
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi}
\exp\left(\frac{-z^2}{2\sigma_Y^2}\right)
\exp\left(\frac{(z\sigma_X^2)^2}{2\sigma_X^2 \sigma_Y^2 \sigma_Z^2}\right) \int_{0}^{z} 
\exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx \\
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2(\sigma_Z^2 - \sigma_X^2)}{2\sigma_Y^2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx \\
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\sigma_Z^2 \left(x - \frac{z \sigma_X^2}{\sigma_Z^2} \right)^2}{2\sigma_X^2 \sigma_Y^2}\right) dx 
\end{align} \begin{align}
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \int_{0}^{z} \exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\frac{\sigma_X^2 \sigma_Y^2}{\sigma_Z^2}}\right) dx \\
\end{align} \frac{\sqrt{2 \pi (\frac{\sigma_X \sigma_Y}{\sigma_Z})^2}}{\sqrt{2 \pi (\frac{\sigma_X \sigma_Y}{\sigma_Z})^2}} \begin{align}
f_{Z}(z) &= \frac{2}{\sigma_X \sigma_Y \pi} 
\exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) 
\sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2} 
\int_{0}^{z} 
\frac{1}{\sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}}
\exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}\right) dx \\
f_{Z}(z) &= \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}} 
\exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) 
\int_{0}^{z} 
\frac{1}{\sqrt{2 \pi \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} 
\exp\left(\frac{-\left(x - \frac{z \sigma_X^2}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}\right) dx 
\end{align} \frac{z \sigma_X^2}{\sigma_Z^2} \frac{\sigma_X \sigma_Y}{\sigma_Z} \begin{align}
f_{Z}(z) &= \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}} \exp\left(\frac{-z^2}{2 \sigma_Z^2}\right) \left[\Phi\left(z, \frac{z \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right) - \Phi\left(0, \frac{z \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right)\right]
\end{align} \Phi(x,m,s) x m s F_Z \begin{align}
F_{Z}(z) &= \int_{0}^{z} f_{Z}(x) dx \\
F_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[\Phi\left(x, \frac{x \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right) - \Phi\left(0, \frac{x \sigma_X^2}{\sigma_Z^2}, \frac{\sigma_X \sigma_Y}{\sigma_Z}\right)\right] dx \\
F_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[
\frac{1}{2} \left( 1 + \erf \left(\frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) \right) - 
\frac{1}{2} \left( 1 + \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) \right)
\right] dx 
\\
F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}}  \int_{0}^{z} \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) \left[
\erf \left( \frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)  - 
\erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)
\right] dx 
\end{align} \begin{align}
F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[ 
\int_{0}^{z} 
\exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
\erf \left( \frac{x-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) dx  - 
\int_{0}^{z} 
\exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
\erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)
 dx \right] \\
F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[ 
 \int_{0}^{z} 
 \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
 \erf \left( \frac{x \left(1-\frac{\sigma_X^2}{\sigma_Z^2} \right)}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right) dx  - 
 \int_{0}^{z} 
 \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
 \erf \left( \frac{-\frac{x \sigma_X^2}{\sigma_Z^2}}{\frac{\sigma_X \sigma_Y}{\sigma_Z} \sqrt{2}} \right)
 dx \right] \\
 F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[ 
 \int_{0}^{z} 
 \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
 \erf \left( x \frac{\sigma_Y}{\sqrt{2} \sigma_Z \sigma_X } \right) dx  - 
 \int_{0}^{z} 
 \exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
 \erf \left(- x \frac{\sigma_X}{\sqrt{2} \sigma_Z \sigma_Y } \right)
 dx \right]
\end{align} \erf(x) \exp(x) \begin{align}
F_{Z}(z) &=  \frac{\sqrt{2}}{\sigma_Z \sqrt{\pi}} \left[ 
\int_{0}^{z} 
\exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
\erf \left( x \frac{\sigma_Y}{\sqrt{2} \sigma_Z \sigma_X } \right) dx  +
\int_{0}^{z} 
\exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
\erf \left(x \frac{\sigma_X}{\sqrt{2} \sigma_Z \sigma_Y } \right)
dx \right]
\end{align} \sigma_X = \sigma_Y 
\int \exp \left( -a^2 x^2 \right) \left[ \erf(ax)\right]^n \, dx = \frac{\sqrt{\pi}}{2a(n+1)} \left[\erf(ax)\right]^{n+1}
 \begin{align}
F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  
\int_{0}^{z} 
\exp\left(\frac{-x^2}{2 \sigma_Z^2}\right) 
\erf \left( \frac{x}{\sqrt{2} \sigma_Z } \right) dx \\
F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \frac{2 \sqrt{2}}{\sigma_Z \sqrt{\pi}}  
\frac{\sqrt{\pi}}{4 \frac{1}{\sqrt{2} \sigma_Z}} \erf(\frac{z}{\sqrt{2} \sigma_Z})^2 \\
F^{\sigma_X = \sigma_Y}_{Z}(z) &=  \erf(\frac{z}{\sqrt{2} \sigma_Z})^2
\end{align} \sigma_X \neq \sigma_Y","['calculus', 'probability-distributions', 'random-variables', 'normal-distribution', 'convolution']"
76,"Computing the Integral $\int\tanh[b(x-a)]\cos\beta x\,dx$",Computing the Integral,"\int\tanh[b(x-a)]\cos\beta x\,dx","$1$. Recently, I encountered the following integral in a physical problem $$I(a,b,\beta)=\int\tanh[b(x-a)]\cos\beta x\,dx,$$ where $a,\,b,\,\beta$ are some real numbers. Mathematica gives this results which looks really complicated. I want to know how the result of mathematica can be obtained? Also, the Re and FullSimplify commands in Mathematica didn't make any further simplification. Can the result of mathematica be put in terms of real numbers? The imaginary $i$ seating there makes me un-comfortable for further use. I should substitute this result into many other equations I am hopeful to get a simpler form for the anti-derivative as I believe that humans do much better than machines and programs (see this post as a proof!) :) My thought was to write a Taylor expansion for $\cos\beta x$ and then going through. In that case we encounter the following integrals $$J_{2n}(a,b,\beta)=\int x^{2n}\tanh[b(x-a)]\,dx,\qquad n=0,1,2,\dots$$ which I couldn't manage to get a nice closed form for them. $2$. If there is no way to get a simpler anti-derivative then I am interested to obtain a simple form for the following definite integral $$I=\int_{-l}^{l}\tanh[b(x-a)]\cos\beta x\,dx,$$ where $\beta=\frac{n\pi}{l}$ and $n$ is a positive integer so obviously $\cos\beta l=(-1)^n,\,\sin\beta l=0$. Any help or hint is appreciated. :)","$1$. Recently, I encountered the following integral in a physical problem $$I(a,b,\beta)=\int\tanh[b(x-a)]\cos\beta x\,dx,$$ where $a,\,b,\,\beta$ are some real numbers. Mathematica gives this results which looks really complicated. I want to know how the result of mathematica can be obtained? Also, the Re and FullSimplify commands in Mathematica didn't make any further simplification. Can the result of mathematica be put in terms of real numbers? The imaginary $i$ seating there makes me un-comfortable for further use. I should substitute this result into many other equations I am hopeful to get a simpler form for the anti-derivative as I believe that humans do much better than machines and programs (see this post as a proof!) :) My thought was to write a Taylor expansion for $\cos\beta x$ and then going through. In that case we encounter the following integrals $$J_{2n}(a,b,\beta)=\int x^{2n}\tanh[b(x-a)]\,dx,\qquad n=0,1,2,\dots$$ which I couldn't manage to get a nice closed form for them. $2$. If there is no way to get a simpler anti-derivative then I am interested to obtain a simple form for the following definite integral $$I=\int_{-l}^{l}\tanh[b(x-a)]\cos\beta x\,dx,$$ where $\beta=\frac{n\pi}{l}$ and $n$ is a positive integer so obviously $\cos\beta l=(-1)^n,\,\sin\beta l=0$. Any help or hint is appreciated. :)",,"['calculus', 'integration', 'definite-integrals', 'indefinite-integrals', 'hypergeometric-function']"
77,Lagrange: Show that there are two points where $f'$ equals zero,Lagrange: Show that there are two points where  equals zero,f',"Let $f: \mathbb R \to \Bbb R$ a differentiable function. Let $T>0 \in R$ such that $f(x+T)=f(x) \forall x \in \Bbb R$ Show that the interval $[0,T)$ has two points where the function $f'$ get equals to $0$(means that $f'(x) = 0$ What I've done : If I substitute $0$ I get $f(T) = f(0)$ and by Rolle Theorem - If a real-valued function f is continuous on a proper closed interval $[a, b]$, differentiable on the open interval $(a, b)$, and $f(a) = f(b)$, then there exists at least one $c$ in the open interval $(a, b)$ such that $f'(c)=0$. Now I'm trying to find the second point I used Lagrange Theorem(Mean value theorem - https://en.wikipedia.org/wiki/Mean_value_theorem ) I get $f'(c) =$ $f(T) - f(0) \over T-0$ = $f(T) - f(T) \over T$ = $0$ but how can I confirm that it's a different point. Can someone help me please? Thanks in advance","Let $f: \mathbb R \to \Bbb R$ a differentiable function. Let $T>0 \in R$ such that $f(x+T)=f(x) \forall x \in \Bbb R$ Show that the interval $[0,T)$ has two points where the function $f'$ get equals to $0$(means that $f'(x) = 0$ What I've done : If I substitute $0$ I get $f(T) = f(0)$ and by Rolle Theorem - If a real-valued function f is continuous on a proper closed interval $[a, b]$, differentiable on the open interval $(a, b)$, and $f(a) = f(b)$, then there exists at least one $c$ in the open interval $(a, b)$ such that $f'(c)=0$. Now I'm trying to find the second point I used Lagrange Theorem(Mean value theorem - https://en.wikipedia.org/wiki/Mean_value_theorem ) I get $f'(c) =$ $f(T) - f(0) \over T-0$ = $f(T) - f(T) \over T$ = $0$ but how can I confirm that it's a different point. Can someone help me please? Thanks in advance",,['calculus']
78,The integral of $\left|\frac{\cos x}x\right|$,The integral of,\left|\frac{\cos x}x\right|,"I'm looking to determine whether the following function is unbounded or not: $$ F(x) = \int_1^x\left|\frac{\cos t}{t}\right|\text{d} t $$ I can't seem to do much with it because of the $|\cos(t)|$. I thought of using the fact that $\int |f| \ge |\int f|$, but the problem is that the integral of $\frac{\cos t}t$ (without the absolute values) is bounded, and so that doesn't prove that $F(x)$ is unbounded or bounded. I tried re-expressing this as a cosine integral (the function $\text{Ci}(x)$) but to no avail. I'm not sure where else to go with this; the main problem seems to be the fact that its very difficult to derive an inequality with the $|\cos(t)|$ without a $|\cos(t)|$ on the other side of the inequality (or at least some trig function). Any help would be appreciated.","I'm looking to determine whether the following function is unbounded or not: $$ F(x) = \int_1^x\left|\frac{\cos t}{t}\right|\text{d} t $$ I can't seem to do much with it because of the $|\cos(t)|$. I thought of using the fact that $\int |f| \ge |\int f|$, but the problem is that the integral of $\frac{\cos t}t$ (without the absolute values) is bounded, and so that doesn't prove that $F(x)$ is unbounded or bounded. I tried re-expressing this as a cosine integral (the function $\text{Ci}(x)$) but to no avail. I'm not sure where else to go with this; the main problem seems to be the fact that its very difficult to derive an inequality with the $|\cos(t)|$ without a $|\cos(t)|$ on the other side of the inequality (or at least some trig function). Any help would be appreciated.",,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
79,Is the Risch-algorithm more powerful than the usual integration methods?,Is the Risch-algorithm more powerful than the usual integration methods?,,"Suppose, a function $f(x)$ has an elementary antiderivate. Is it always possible to find the antiderivate with the usual integration mehods (integration by parts, substitution, and so on) or can the Risch algorithm be necessary to find it ? If I understand it right, the Risch-algorithm is nearly always successful, but I have no idea how the algorithm actually works. Many integrals (assuming that an elementary antiderivate exists) are solveable with the usual methods as well, but I think there are cases which are too hard, so that we actually need the Risch-algorithm. Additional question : How efficient is the Risch-algorithm in practice ? Are there cases prcatically infeasible becuase the Risch-agorithm would take too long (Of course, the length of the antiderivate should not be too large) ?","Suppose, a function $f(x)$ has an elementary antiderivate. Is it always possible to find the antiderivate with the usual integration mehods (integration by parts, substitution, and so on) or can the Risch algorithm be necessary to find it ? If I understand it right, the Risch-algorithm is nearly always successful, but I have no idea how the algorithm actually works. Many integrals (assuming that an elementary antiderivate exists) are solveable with the usual methods as well, but I think there are cases which are too hard, so that we actually need the Risch-algorithm. Additional question : How efficient is the Risch-algorithm in practice ? Are there cases prcatically infeasible becuase the Risch-agorithm would take too long (Of course, the length of the antiderivate should not be too large) ?",,"['calculus', 'integration', 'closed-form', 'elementary-functions']"
80,Can the Risch-algorithm actually prove that $e^{-x^2}$ has no closed-form antiderivative?,Can the Risch-algorithm actually prove that  has no closed-form antiderivative?,e^{-x^2},"The Risch algorithm is used to find closed-form antiderivatives. If I understand the article right, only heuristics are known. On the other hand, I came across the claim that it is known that $e^{-x^2}$ has no closed-form antiderivative. Is the Risch-algorithm successful in the case $f(x)=e^{-x^2}$ ? Is it actually proven that no closed-form antiderivative exist or just very probable because no form has been found using the Risch algorithm ? This question could well be a duplicate, but I am not sure whether the aspect of decidability has been asked.","The Risch algorithm is used to find closed-form antiderivatives. If I understand the article right, only heuristics are known. On the other hand, I came across the claim that it is known that $e^{-x^2}$ has no closed-form antiderivative. Is the Risch-algorithm successful in the case $f(x)=e^{-x^2}$ ? Is it actually proven that no closed-form antiderivative exist or just very probable because no form has been found using the Risch algorithm ? This question could well be a duplicate, but I am not sure whether the aspect of decidability has been asked.",,"['calculus', 'integration', 'symbolic-computation']"
81,How to prove the parallel projection of an ellipsoid is an ellipse?,How to prove the parallel projection of an ellipsoid is an ellipse?,,"Take the following ellipsoid in implicit form as an example: $$x^2 + 2 y^2 + 3 z^2 + x y + y z - 2 xz = 5$$ which shows: The parallel projection of the ellipsoid onto $xoy$ coordinate plane can be obtained as: $$ 8 x^2 + 16 x y+23 y^2=60$$ Is it possible to prove: The parallel projection of an ellipsoid is always an ellipse and how? I guess this should be able to be generalized into: the perspective projection of an ellipsoid is a conic curve. How to prove it? In prjective geometry, the quadratic form of conics is useful in such proof. This one seems a little more difficult.","Take the following ellipsoid in implicit form as an example: $$x^2 + 2 y^2 + 3 z^2 + x y + y z - 2 xz = 5$$ which shows: The parallel projection of the ellipsoid onto $xoy$ coordinate plane can be obtained as: $$ 8 x^2 + 16 x y+23 y^2=60$$ Is it possible to prove: The parallel projection of an ellipsoid is always an ellipse and how? I guess this should be able to be generalized into: the perspective projection of an ellipsoid is a conic curve. How to prove it? In prjective geometry, the quadratic form of conics is useful in such proof. This one seems a little more difficult.",,"['calculus', 'analytic-geometry', 'surfaces', 'projective-geometry', 'ellipsoids']"
82,"Can the Substitution Rule be Interpreted as a ""Change of Measure""?","Can the Substitution Rule be Interpreted as a ""Change of Measure""?",,"I just started learning measure and rigorous integration theory on my own along side my calculus class and I've noticed that with the substitution rule, you have something that looks like this $$ \int^{b}_{a} f(g(x))g'(x) \, dx=\int^{u(b)}_{u(a)} f(u)  \, du $$ where $u(x)=g(x)$. I don't know how to formally connect this with measure theory yet but when I was listening to the lecture in class, my instincts/intuition were screaming at me that this has to connect directly to measure theory. I know the statement of the Radon-Nikodym theorem, though I haven't been able to quite grasp the significance but that would be my first guess but a google search only yielded some feint allusions to this connection that were rather unsatisfactory. I just feel like the change of the bounds from $[a,b]$ to $[u(a),u(b)]$ has to be some sort of ""change of measure"" or something to that effect. I apologize if this is a very elementary or stupid question but I can't get it off my mind and I'm not making much progress trying to formalize it myself. Any ideas or just some hints that will point me in the right direction would be awesome. Thanks in advance.","I just started learning measure and rigorous integration theory on my own along side my calculus class and I've noticed that with the substitution rule, you have something that looks like this $$ \int^{b}_{a} f(g(x))g'(x) \, dx=\int^{u(b)}_{u(a)} f(u)  \, du $$ where $u(x)=g(x)$. I don't know how to formally connect this with measure theory yet but when I was listening to the lecture in class, my instincts/intuition were screaming at me that this has to connect directly to measure theory. I know the statement of the Radon-Nikodym theorem, though I haven't been able to quite grasp the significance but that would be my first guess but a google search only yielded some feint allusions to this connection that were rather unsatisfactory. I just feel like the change of the bounds from $[a,b]$ to $[u(a),u(b)]$ has to be some sort of ""change of measure"" or something to that effect. I apologize if this is a very elementary or stupid question but I can't get it off my mind and I'm not making much progress trying to formalize it myself. Any ideas or just some hints that will point me in the right direction would be awesome. Thanks in advance.",,"['calculus', 'integration', 'measure-theory', 'lebesgue-integral', 'substitution']"
83,Is there a rearrangement theorem for conditionally convergent improper integrals?,Is there a rearrangement theorem for conditionally convergent improper integrals?,,"The famous Riemann rearrangement theorem states that for a conditionally convergent real number series, we can rearrange the order of summation to make it converge to any prescribed number in the extended real line. In particular, this result sheds much light on the significance of absolute convergence , without which it would be quite dangerous to manipulate a convergent series. For improper (Riemann) integrals, we can also distinguish conditionally convergent integrals from absolutely convergent ones using analogous definitions. I'm wondering, however, if there also exists an analogous ""rearrangement theorem"" for improper integrals which reveals the essential difference (like eligibility for rearrangement, in the case of numerical series) between the two kinds of convergent integrals? Indeed, can we even define an integral version of ""rearrangement""? Of course, one distinction I'm already aware of is that absolutely convergent integrals are also integrable in the Lebesgue sense while conditionally convergent ones fail to be. But this is not what I want, since it doesn't appear nearly as striking as what is exhibited in the series version. PS: as far as I can tell, one probable way to see why eligibility for rearrangement matters is how we define a valid expectation for a numerical random variable. We require expectations (numerical series for discrete  random variables, and integrals for continuous ones) to be absolutely convergent, for if they were not, then they would undesirably depend on the ""chronological order"" in which we observe events, violating our basic principle that expectations should be stable and inherent in the random variable itself, rather than affected by how each event chronologically arises.","The famous Riemann rearrangement theorem states that for a conditionally convergent real number series, we can rearrange the order of summation to make it converge to any prescribed number in the extended real line. In particular, this result sheds much light on the significance of absolute convergence , without which it would be quite dangerous to manipulate a convergent series. For improper (Riemann) integrals, we can also distinguish conditionally convergent integrals from absolutely convergent ones using analogous definitions. I'm wondering, however, if there also exists an analogous ""rearrangement theorem"" for improper integrals which reveals the essential difference (like eligibility for rearrangement, in the case of numerical series) between the two kinds of convergent integrals? Indeed, can we even define an integral version of ""rearrangement""? Of course, one distinction I'm already aware of is that absolutely convergent integrals are also integrable in the Lebesgue sense while conditionally convergent ones fail to be. But this is not what I want, since it doesn't appear nearly as striking as what is exhibited in the series version. PS: as far as I can tell, one probable way to see why eligibility for rearrangement matters is how we define a valid expectation for a numerical random variable. We require expectations (numerical series for discrete  random variables, and integrals for continuous ones) to be absolutely convergent, for if they were not, then they would undesirably depend on the ""chronological order"" in which we observe events, violating our basic principle that expectations should be stable and inherent in the random variable itself, rather than affected by how each event chronologically arises.",,"['calculus', 'integration', 'convergence-divergence', 'soft-question', 'riemann-integration']"
84,Evaluation of $\int_{0}^{10 \pi} ([\sec ^{-1}x]+[\cot^{-1} x])~\mathrm dx$ [closed],Evaluation of  [closed],\int_{0}^{10 \pi} ([\sec ^{-1}x]+[\cot^{-1} x])~\mathrm dx,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find the value of the integral $$\int_{0}^{10 \pi} (\lfloor\sec ^{-1}x\rfloor+\lfloor\cot^{-1} x\rfloor)~\mathrm dx$$ where $\lfloor . \rfloor$ denotes greatest integer function. Could some help me with this? I cannot break greatest integer function here into different interval. Please provide some insight.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find the value of the integral $$\int_{0}^{10 \pi} (\lfloor\sec ^{-1}x\rfloor+\lfloor\cot^{-1} x\rfloor)~\mathrm dx$$ where $\lfloor . \rfloor$ denotes greatest integer function. Could some help me with this? I cannot break greatest integer function here into different interval. Please provide some insight.",,"['calculus', 'integration', 'definite-integrals']"
85,Integral of $\frac{1}{x\sqrt{x^2-1}}$,Integral of,\frac{1}{x\sqrt{x^2-1}},"I am very confused by this. I know that the derivative of $\text{arcsec}(x)$ is $\dfrac{1}{|x|\sqrt{x^2-1}}$. However, if you plug in the integral of $\dfrac{1}{x\sqrt{x^2-1}}$ into wolfram alpha it gives some other answer with an inverse tangent: $$ \int \dfrac{1}{x\sqrt{x^2-1}}dx = - \tan^{-1}\Bigg(\frac{1}{\sqrt{x^2-1}} \Bigg) +C $$ I was just wondering why this is, or why wolfram is giving something totally different.  Are they equivalent?","I am very confused by this. I know that the derivative of $\text{arcsec}(x)$ is $\dfrac{1}{|x|\sqrt{x^2-1}}$. However, if you plug in the integral of $\dfrac{1}{x\sqrt{x^2-1}}$ into wolfram alpha it gives some other answer with an inverse tangent: $$ \int \dfrac{1}{x\sqrt{x^2-1}}dx = - \tan^{-1}\Bigg(\frac{1}{\sqrt{x^2-1}} \Bigg) +C $$ I was just wondering why this is, or why wolfram is giving something totally different.  Are they equivalent?",,"['calculus', 'integration', 'indefinite-integrals', 'wolfram-alpha']"
86,"Order of integration can be swapped if limits are constants, right?","Order of integration can be swapped if limits are constants, right?",,"The order of integration can be easily swapped if the limits are constants, right? $$\int_{a}^{b}\int_{c}^{d}f(x,y)dydx=\int_{c}^{d}\int_{a}^{b}f(x,y)dxdy$$ It only gets computationally hard if the limits are functions of each other, right? $$\int_{a}^{b}\int_{c(x)}^{d(x)}f(x,y)dydx \neq \int_{c(x)}^{d(x)}\int_{a}^{b}f(x,y)dxdy$$ Sorry for the potentially trivial answer. Just doing a reality check over here.","The order of integration can be easily swapped if the limits are constants, right? It only gets computationally hard if the limits are functions of each other, right? Sorry for the potentially trivial answer. Just doing a reality check over here.","\int_{a}^{b}\int_{c}^{d}f(x,y)dydx=\int_{c}^{d}\int_{a}^{b}f(x,y)dxdy \int_{a}^{b}\int_{c(x)}^{d(x)}f(x,y)dydx \neq \int_{c(x)}^{d(x)}\int_{a}^{b}f(x,y)dxdy","['calculus', 'integration', 'multiple-integral']"
87,Integral of quotients of $\sin$ function,Integral of quotients of  function,\sin,"I am trying to calculate the definite integral $$\int_0^\pi \frac{\sin(\frac{21}{2}x)}{\sin(\frac{1}{2}x)} dx.$$ Wolfram Alpha says here that the answer is $\pi$. I replaced 21 by other constants and think that in general, $\int_0^\pi \frac{\sin(\frac{n}{2}x)}{\sin(\frac{1}{2}x)} dx = \pi$ for all odd $n \in \mathbb Z$. However I have no idea how to approach this problem. I tried substituting $u = x/2$ to simplify the integral a bit to $$2\int_0^{\pi/2} \frac{\sin(nu)}{\sin(u)}.$$ Then I thought that I could maybe use the identity $\sin(nx) = \sin(x)\cos( (n-1)x) + \sin((n-1)x)\cos(x)$. For instance, since \begin{align*} \sin(3x) &= \sin(2x)\cos(x) + \cos(2x)\sin(x) \\ &= 2\sin(x) \cos^2(x) + \cos^2(x)\sin(x) - \sin^3(x) \\ &= \sin(x)(3\cos^2(x) - \sin^2(x)) \end{align*} the integral would become $$ 2\int_0^{\pi/2} 3\cos^2(x) -\sin^2(x) dx $$ which I am able to solve: \begin{align*} 2\int_0^{\pi/2} 3\cos^2(x) -\sin^2(x) dx &= 2[\frac{3}{2}(x+\sin(x)\cos(x)) - \frac{1}{2}(x - \sin(x)\cos(x))]^{\pi/2}_0 \\ &= [2x + 4\sin(x)\cos(x)]^{\pi/2}_0 \\ &= \pi. \end{align*} However I don't know how to generalize this approach because the expansion for $\sin(21x)$ would have lots of unwieldy terms. Is there another way to do this problem that I am missing?","I am trying to calculate the definite integral $$\int_0^\pi \frac{\sin(\frac{21}{2}x)}{\sin(\frac{1}{2}x)} dx.$$ Wolfram Alpha says here that the answer is $\pi$. I replaced 21 by other constants and think that in general, $\int_0^\pi \frac{\sin(\frac{n}{2}x)}{\sin(\frac{1}{2}x)} dx = \pi$ for all odd $n \in \mathbb Z$. However I have no idea how to approach this problem. I tried substituting $u = x/2$ to simplify the integral a bit to $$2\int_0^{\pi/2} \frac{\sin(nu)}{\sin(u)}.$$ Then I thought that I could maybe use the identity $\sin(nx) = \sin(x)\cos( (n-1)x) + \sin((n-1)x)\cos(x)$. For instance, since \begin{align*} \sin(3x) &= \sin(2x)\cos(x) + \cos(2x)\sin(x) \\ &= 2\sin(x) \cos^2(x) + \cos^2(x)\sin(x) - \sin^3(x) \\ &= \sin(x)(3\cos^2(x) - \sin^2(x)) \end{align*} the integral would become $$ 2\int_0^{\pi/2} 3\cos^2(x) -\sin^2(x) dx $$ which I am able to solve: \begin{align*} 2\int_0^{\pi/2} 3\cos^2(x) -\sin^2(x) dx &= 2[\frac{3}{2}(x+\sin(x)\cos(x)) - \frac{1}{2}(x - \sin(x)\cos(x))]^{\pi/2}_0 \\ &= [2x + 4\sin(x)\cos(x)]^{\pi/2}_0 \\ &= \pi. \end{align*} However I don't know how to generalize this approach because the expansion for $\sin(21x)$ would have lots of unwieldy terms. Is there another way to do this problem that I am missing?",,"['calculus', 'integration']"
88,$2005$th derivative of $f$ at $0$,th derivative of  at,2005 f 0,"So I tried using Leibnitz formula to solve by recurrence, but I can just get to one point and then it's a mess again. Problem is Let $f(x)=\frac{1}{1+2x+3x^2+\ldots+2005x^{2004}}$. Find $f^{[2005]}(0)$. What I did is to notice that $f'=-f^2g$, where $g$ is the polynomial, and then tried to expand the result to higher order derivatives. By Leibnitz formula and the last expression, I got $f^{[2005]}(0)=\displaystyle\sum_{k=0}^{2004}\binom{2004}{k}\left(f^2\right)^{[k]}(0)g^{[2004-k]}(0)=\sum_{k=0}^{2004}\binom{2004}{k}\left(f^2\right)^{[k]}(0)(2004-k)! =\sum_{k=0}^{2004}\frac{2004!}{k!}\left(f^2\right)^{[k]}(0)$ I'd really appreciate some help here, please.","So I tried using Leibnitz formula to solve by recurrence, but I can just get to one point and then it's a mess again. Problem is Let $f(x)=\frac{1}{1+2x+3x^2+\ldots+2005x^{2004}}$. Find $f^{[2005]}(0)$. What I did is to notice that $f'=-f^2g$, where $g$ is the polynomial, and then tried to expand the result to higher order derivatives. By Leibnitz formula and the last expression, I got $f^{[2005]}(0)=\displaystyle\sum_{k=0}^{2004}\binom{2004}{k}\left(f^2\right)^{[k]}(0)g^{[2004-k]}(0)=\sum_{k=0}^{2004}\binom{2004}{k}\left(f^2\right)^{[k]}(0)(2004-k)! =\sum_{k=0}^{2004}\frac{2004!}{k!}\left(f^2\right)^{[k]}(0)$ I'd really appreciate some help here, please.",,"['calculus', 'derivatives', 'induction', 'recurrence-relations']"
89,$f(x)$ is an analytic function in $\mathbb{R}$ such that $f(-x)f(x)=1$. What else can we find out about $f(x)$?,is an analytic function in  such that . What else can we find out about ?,f(x) \mathbb{R} f(-x)f(x)=1 f(x),"Well, I know that there are some easy things we can say immediately: $f(0)= \pm 1$, follows immediately $f(x)=\pm 1$ is the obvious solution, so let's look for other solutions. Moreover, let's consider only the case $f(0)=1$ for now The obvious identities, such as $$f(x)^2=\frac{f(x)}{f(-x)}$$ And now for the series: $$f(x)=a_0+a_1 x+ a_2 x^2+a_3 x^3+\dots$$ We can immediately see by multiplying the series for $f(x)$ and $f(-x)$ that (for the case $f(0)>0$): $$a_0=1$$ $$a_2=\frac{a_1^2}{2}$$ $$a_4=a_1 a_3-\frac{a_1^4}{8}$$ $$a_6=a_1 a_5+\frac{a_3(a_3-a_1^3)}{2}+\frac{a_1^6}{16}$$ And so on. The coefficients for the even powers will be related to the ones for the odd powers. But that's the extent of what we can really say, or so I think. What else can we say about $f(x)$ based on these two restrictions only? And what is the weakest restriction we need to get $f(x)=c^x$?","Well, I know that there are some easy things we can say immediately: $f(0)= \pm 1$, follows immediately $f(x)=\pm 1$ is the obvious solution, so let's look for other solutions. Moreover, let's consider only the case $f(0)=1$ for now The obvious identities, such as $$f(x)^2=\frac{f(x)}{f(-x)}$$ And now for the series: $$f(x)=a_0+a_1 x+ a_2 x^2+a_3 x^3+\dots$$ We can immediately see by multiplying the series for $f(x)$ and $f(-x)$ that (for the case $f(0)>0$): $$a_0=1$$ $$a_2=\frac{a_1^2}{2}$$ $$a_4=a_1 a_3-\frac{a_1^4}{8}$$ $$a_6=a_1 a_5+\frac{a_3(a_3-a_1^3)}{2}+\frac{a_1^6}{16}$$ And so on. The coefficients for the even powers will be related to the ones for the odd powers. But that's the extent of what we can really say, or so I think. What else can we say about $f(x)$ based on these two restrictions only? And what is the weakest restriction we need to get $f(x)=c^x$?",,"['calculus', 'exponential-function', 'functional-equations']"
90,Derivative of functions of the form $f(x)^{g(x)}$,Derivative of functions of the form,f(x)^{g(x)},"I am trying to find the derivative of the function $h(x)=f(x)^{g(x)}$. I just wanted to be sure my derivation was correct: We proceed by using logarithmic differentiation. $h(x)=f(x)^{g(x)}$ $\log (h(x))=g(x) \log (f(x))$ $\frac{h'(x)}{h(x)}=g'(x) \log (f(x))+\frac{g(x)f'(x)}{f(x)}$ Thus, $h'(x)=h(x)\left(g'(x) \log (f(x))+\frac{g(x)f'(x)}{f(x)}\right)$ Does this look correct?","I am trying to find the derivative of the function $h(x)=f(x)^{g(x)}$. I just wanted to be sure my derivation was correct: We proceed by using logarithmic differentiation. $h(x)=f(x)^{g(x)}$ $\log (h(x))=g(x) \log (f(x))$ $\frac{h'(x)}{h(x)}=g'(x) \log (f(x))+\frac{g(x)f'(x)}{f(x)}$ Thus, $h'(x)=h(x)\left(g'(x) \log (f(x))+\frac{g(x)f'(x)}{f(x)}\right)$ Does this look correct?",,"['calculus', 'derivatives', 'proof-verification']"
91,Convergent subsequence of $\sin(n)$,Convergent subsequence of,\sin(n),According to theorem every bounded sequence in $R$ has a convergent subsequence.Can anybody make a convergent subsequence of $\sin{n}$.,According to theorem every bounded sequence in $R$ has a convergent subsequence.Can anybody make a convergent subsequence of $\sin{n}$.,,"['calculus', 'analysis']"
92,"Indefinite integral $\int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx $",Indefinite integral,"\int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx ","For my engineering math course I got a couple of exercises about indefinite integrals. I ran trought all of them but stumbled upon the following problem. $$\int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx $$ We can write $1+x-2x^2$ as $(1-x)(2x+1)$ So I got: $$ \int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx = \int \frac{1-x}{\sqrt{(1-x)(2x+1)}}\,dx  $$ We can also replace $1-x$ in the denominator with $\sqrt{(1-x)^2}$ $$ \int \frac{1-x}{\sqrt{(1-x)(2x+1)}}\,dx = \int \frac{\sqrt{(1-x)^2}}{\sqrt{(1-x)(2x+1)}}\,dx $$ If we simplify this fraction we get: $$ \int \frac{\sqrt{1-x}}{\sqrt{2x+1}}\,dx $$ Next we apply the following substitutions $$ u = -x $$ so : $-du = dx$ We can rewrite the integral as following: $$-\int \frac{\sqrt{1+u}}{\sqrt{1-2u}}\,du$$ Then we apply another substitution:  $\sqrt{1+u} = t $ so $ \frac{1}{2\sqrt{1+u}} = dt $ We rewrite: $ \sqrt{1+u} $ to $\frac{1}{2}t^2 \,dt $ We can also replace $\sqrt{1-2u} $ as following: $$\sqrt{-2t^2+3}=\sqrt{-2(1+u)+3}=\sqrt{1-2u}$$ With al these substitutions the integral has now the following form: $$-\frac{1}{2}\int \frac{t^2}{\sqrt{-2t^2+3}}\,dt$$ Next we try to ''clean'' up the numerator: $$-\frac{1}{2} \int \frac{t^2}{\sqrt{\frac{1}{2}(6-t^2)}} \, dt$$ $$-\frac{\sqrt{2}}{2} \int \frac{t^2}{\sqrt{6-t^2}} \, dt$$ And that's where I got stuck. I can clearly see that an arcsin is showing up in the integral but don't know how to get rid of the $t^2$.","For my engineering math course I got a couple of exercises about indefinite integrals. I ran trought all of them but stumbled upon the following problem. $$\int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx $$ We can write $1+x-2x^2$ as $(1-x)(2x+1)$ So I got: $$ \int \frac{1-x}{\sqrt{1+x-2x^2}}\,dx = \int \frac{1-x}{\sqrt{(1-x)(2x+1)}}\,dx  $$ We can also replace $1-x$ in the denominator with $\sqrt{(1-x)^2}$ $$ \int \frac{1-x}{\sqrt{(1-x)(2x+1)}}\,dx = \int \frac{\sqrt{(1-x)^2}}{\sqrt{(1-x)(2x+1)}}\,dx $$ If we simplify this fraction we get: $$ \int \frac{\sqrt{1-x}}{\sqrt{2x+1}}\,dx $$ Next we apply the following substitutions $$ u = -x $$ so : $-du = dx$ We can rewrite the integral as following: $$-\int \frac{\sqrt{1+u}}{\sqrt{1-2u}}\,du$$ Then we apply another substitution:  $\sqrt{1+u} = t $ so $ \frac{1}{2\sqrt{1+u}} = dt $ We rewrite: $ \sqrt{1+u} $ to $\frac{1}{2}t^2 \,dt $ We can also replace $\sqrt{1-2u} $ as following: $$\sqrt{-2t^2+3}=\sqrt{-2(1+u)+3}=\sqrt{1-2u}$$ With al these substitutions the integral has now the following form: $$-\frac{1}{2}\int \frac{t^2}{\sqrt{-2t^2+3}}\,dt$$ Next we try to ''clean'' up the numerator: $$-\frac{1}{2} \int \frac{t^2}{\sqrt{\frac{1}{2}(6-t^2)}} \, dt$$ $$-\frac{\sqrt{2}}{2} \int \frac{t^2}{\sqrt{6-t^2}} \, dt$$ And that's where I got stuck. I can clearly see that an arcsin is showing up in the integral but don't know how to get rid of the $t^2$.",,"['calculus', 'integration', 'indefinite-integrals', 'substitution']"
93,Definition of Unsigned Definite Integral,Definition of Unsigned Definite Integral,,"In Terence Tao's paper Differential Forms and Integration , he mentions that there are $3$ distinct notions of integration when discussing functions $f: \Bbb R \to \Bbb R$ Indefinite Integrals: $\int f(x)\ dx$ Unsigned Definite Integrals: $\int_{[a,b]} f(x)\ dx$ Signed Definite Integrals: $\int_a^b f(x)\ dx$ I know well the definitions of indefinite integral -- $\int f(x)\ dx = F(x) \iff F'(x) = f(x)$ -- and the signed definite integral -- via the Darboux or Riemann sum definitions.  But I've never heard of an unsigned definite integral and I can't find a rigorous definition of it. What is the definition of the unsigned definite integral?","In Terence Tao's paper Differential Forms and Integration , he mentions that there are $3$ distinct notions of integration when discussing functions $f: \Bbb R \to \Bbb R$ Indefinite Integrals: $\int f(x)\ dx$ Unsigned Definite Integrals: $\int_{[a,b]} f(x)\ dx$ Signed Definite Integrals: $\int_a^b f(x)\ dx$ I know well the definitions of indefinite integral -- $\int f(x)\ dx = F(x) \iff F'(x) = f(x)$ -- and the signed definite integral -- via the Darboux or Riemann sum definitions.  But I've never heard of an unsigned definite integral and I can't find a rigorous definition of it. What is the definition of the unsigned definite integral?",,"['calculus', 'integration']"
94,"If $ I = \int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{201} xdx$ and $J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}xdx\;,$ Then $ \frac{I}{J}$",If  and  Then," I = \int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{201} xdx J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}xdx\;,  \frac{I}{J}","If $\displaystyle I = \int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{201}\cdot xdx$ and $J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}\cdot xdx\;,$ Then value of $\displaystyle \frac{I}{J}$ $\bf{My\; Try::}$ Given $$\displaystyle J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}\cdot xdx\;,$$ Now Let $(1-x^2) = t\;,$ Then $2xdx = dt$ and Changing Limit We get $$\displaystyle I = -\frac{1}{2}\int_{1}^{0}(1-t^{100})^{201}dt = \frac{1}{2}\int_{0}^{1}(1-t^{100})^{201}dt$$ And we get $$\displaystyle J = -\frac{1}{2}\int_{1}^{0}(1-t^{100})^{202}dt = \frac{1}{2}\int_{0}^{1}(1-t^{100})^{202}\cdot 1dt$$ Using Integration by parts, We get $$\displaystyle J = \frac{1}{2}\left[(1-t^{100})^{202}\cdot t\right]_{0}^{1}-\frac{202}{2}\int (1-t^{100})^{201}\cdot (-100t^{100})dt = -10100\int_{0}^{1}(1-t^2)^{201}\left[(1-t^{100})-1\right]dt$$ So Integral $$\displaystyle J = -10100\int_{0}^{1}(1-t^{100})^{202}dt+10100\int_{0}^{1}(1-t^{100})^{201}dt$$ So we get $$\displaystyle J=-10100\cdot 2J+202\cdot 2I\Rightarrow 20201J = 404I\Rightarrow \frac{I}{J} = \frac{20201}{20200}$$ My question is can we solve it using $\bf{Trigonometric\; Substution}$ Or any $\bf{Other\;  method,}$ If yes then plz explain here. Thanks","If $\displaystyle I = \int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{201}\cdot xdx$ and $J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}\cdot xdx\;,$ Then value of $\displaystyle \frac{I}{J}$ $\bf{My\; Try::}$ Given $$\displaystyle J=\int_{0}^{1}\left[1-(1-x^2)^{100}\right]^{202}\cdot xdx\;,$$ Now Let $(1-x^2) = t\;,$ Then $2xdx = dt$ and Changing Limit We get $$\displaystyle I = -\frac{1}{2}\int_{1}^{0}(1-t^{100})^{201}dt = \frac{1}{2}\int_{0}^{1}(1-t^{100})^{201}dt$$ And we get $$\displaystyle J = -\frac{1}{2}\int_{1}^{0}(1-t^{100})^{202}dt = \frac{1}{2}\int_{0}^{1}(1-t^{100})^{202}\cdot 1dt$$ Using Integration by parts, We get $$\displaystyle J = \frac{1}{2}\left[(1-t^{100})^{202}\cdot t\right]_{0}^{1}-\frac{202}{2}\int (1-t^{100})^{201}\cdot (-100t^{100})dt = -10100\int_{0}^{1}(1-t^2)^{201}\left[(1-t^{100})-1\right]dt$$ So Integral $$\displaystyle J = -10100\int_{0}^{1}(1-t^{100})^{202}dt+10100\int_{0}^{1}(1-t^{100})^{201}dt$$ So we get $$\displaystyle J=-10100\cdot 2J+202\cdot 2I\Rightarrow 20201J = 404I\Rightarrow \frac{I}{J} = \frac{20201}{20200}$$ My question is can we solve it using $\bf{Trigonometric\; Substution}$ Or any $\bf{Other\;  method,}$ If yes then plz explain here. Thanks",,"['calculus', 'integration', 'definite-integrals', 'beta-function']"
95,Name for kind of big O notation with leading coefficient,Name for kind of big O notation with leading coefficient,,"Context: As known the big O notation $O(f(n))$ describes a function $g(n)$ such that there is a constant $C \ge 0$ with $\limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le C$ (I assume that $f(n)$ is never zero). Thus the big O notation can be used to characterize the speed of convergence (for example for an algorithm). So I can write $$a(n) = a + O\left(\frac 1n\right)$$ which means that the function / algorithm $a(n)$ calculates in almost all calculation steps $n$ the desired number $a$ with an error less than $\frac Cn$ for a constant $C\ge 0$. Problem: Whereby I can compare two algorithms by their speed of convergence with the big O notation, the above notation $a(n) = a + O\left(\frac 1n\right)$ does not say anything about the actual error in the n-th step because I do not know the constant $C\ge 0$. My solution: One may introduce a new notation, lets say the big Psi notation $\Psi(f(n))$ which is defined as $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| < 1$$ For example one may write $$a(n) = a + \Psi\left(\frac{42}{n}\right)$$ so that $a(n)\in O\left(\frac 1n\right)$ with the constant $C=42$. There are also arithmetic rules for the big Psi notation similar to the rules for the big notation, for example: $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b+ab}{n}\right)$$ My question: Because the proposed solution is simple and somehow straightforward I guess there was already a mathematician who wrote about it. Can you point me to a textbook/paper where this notation is discussed, please? How is this notation called in mathematics? So far I have only found the big Omega notation and the big Theta notation ... Update: After rethinking the notation I would now define $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le 1$$ such that $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b}{n}\right)$$ because $$\limsup_{n\to\infty} \frac{\frac{a+b}{n}}{\frac{a+b}{n}+\frac{ab}{n^2}} = 1$$","Context: As known the big O notation $O(f(n))$ describes a function $g(n)$ such that there is a constant $C \ge 0$ with $\limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le C$ (I assume that $f(n)$ is never zero). Thus the big O notation can be used to characterize the speed of convergence (for example for an algorithm). So I can write $$a(n) = a + O\left(\frac 1n\right)$$ which means that the function / algorithm $a(n)$ calculates in almost all calculation steps $n$ the desired number $a$ with an error less than $\frac Cn$ for a constant $C\ge 0$. Problem: Whereby I can compare two algorithms by their speed of convergence with the big O notation, the above notation $a(n) = a + O\left(\frac 1n\right)$ does not say anything about the actual error in the n-th step because I do not know the constant $C\ge 0$. My solution: One may introduce a new notation, lets say the big Psi notation $\Psi(f(n))$ which is defined as $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| < 1$$ For example one may write $$a(n) = a + \Psi\left(\frac{42}{n}\right)$$ so that $a(n)\in O\left(\frac 1n\right)$ with the constant $C=42$. There are also arithmetic rules for the big Psi notation similar to the rules for the big notation, for example: $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b+ab}{n}\right)$$ My question: Because the proposed solution is simple and somehow straightforward I guess there was already a mathematician who wrote about it. Can you point me to a textbook/paper where this notation is discussed, please? How is this notation called in mathematics? So far I have only found the big Omega notation and the big Theta notation ... Update: After rethinking the notation I would now define $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le 1$$ such that $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b}{n}\right)$$ because $$\limsup_{n\to\infty} \frac{\frac{a+b}{n}}{\frac{a+b}{n}+\frac{ab}{n^2}} = 1$$",,"['calculus', 'sequences-and-series', 'numerical-methods', 'asymptotics', 'definition']"
96,Is standard eigenvalue optimization problem convex,Is standard eigenvalue optimization problem convex,,"For any arbitrary  symmetric matrix A , is the standard eigenvalue problem convex $ \lambda_{max}(A)= \max_{\|x\| \leq1} x^{T}Ax$","For any arbitrary  symmetric matrix A , is the standard eigenvalue problem convex $ \lambda_{max}(A)= \max_{\|x\| \leq1} x^{T}Ax$",,"['calculus', 'optimization', 'convex-optimization', 'nonlinear-optimization']"
97,$ \int_1^2\int_1^2 \int_1^2 \int_1^2 \frac{x_1+x_2+x_3-x_4}{x_1+x_2+x_3+x_4}dx_1dx_2dx_3dx_4 $,, \int_1^2\int_1^2 \int_1^2 \int_1^2 \frac{x_1+x_2+x_3-x_4}{x_1+x_2+x_3+x_4}dx_1dx_2dx_3dx_4 ,Evaluate $$I=  \int_1^2\int_1^2 \int_1^2 \int_1^2  \frac{x_1+x_2+x_3-x_4}{x_1+x_2+x_3+x_4}dx_1dx_2dx_3dx_4$$ Answer Options: $1$ $\frac{1}{2}$ $\frac{1}{3}$ $\frac{1}{4}$ I need some suggestion here. I tried to evaluate one by one but it becomes messy. I think there some trick involved here.,Evaluate $$I=  \int_1^2\int_1^2 \int_1^2 \int_1^2  \frac{x_1+x_2+x_3-x_4}{x_1+x_2+x_3+x_4}dx_1dx_2dx_3dx_4$$ Answer Options: $1$ $\frac{1}{2}$ $\frac{1}{3}$ $\frac{1}{4}$ I need some suggestion here. I tried to evaluate one by one but it becomes messy. I think there some trick involved here.,,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
98,What are the connections between the three Mertens' theorem?,What are the connections between the three Mertens' theorem?,,"In number theory the three Mertens' theorems are the following. Mertens' $1$st theorem. For all $n\geq2$ $$\left\lvert\sum_{p\leqslant n} \frac{\ln p}{p} - \ln n\right\rvert \leq 2.$$ Mertens' $2$nd theroem. $$\lim_{n\to\infty}\left(\sum_{p\le n}\frac1p -\ln\ln n-M\right) =0,$$ where $M$ is the Meissel–Mertens constant . Mertens' $3$rd theroem. $$\lim_{n\to\infty}\ln n\prod_{p\le n}\left(1-\frac1p\right)=e^{-\gamma},$$ where $\gamma$ is the Euler–Mascheroni constant . What connections are there between in this three theorems, beside that all of them about prime series and products? What relationships are behind the scenes? I know that the $2$nd theorem is connected to prime number theorem , and the other two theorems? The motivation of the question is this really nice answer .","In number theory the three Mertens' theorems are the following. Mertens' $1$st theorem. For all $n\geq2$ $$\left\lvert\sum_{p\leqslant n} \frac{\ln p}{p} - \ln n\right\rvert \leq 2.$$ Mertens' $2$nd theroem. $$\lim_{n\to\infty}\left(\sum_{p\le n}\frac1p -\ln\ln n-M\right) =0,$$ where $M$ is the Meissel–Mertens constant . Mertens' $3$rd theroem. $$\lim_{n\to\infty}\ln n\prod_{p\le n}\left(1-\frac1p\right)=e^{-\gamma},$$ where $\gamma$ is the Euler–Mascheroni constant . What connections are there between in this three theorems, beside that all of them about prime series and products? What relationships are behind the scenes? I know that the $2$nd theorem is connected to prime number theorem , and the other two theorems? The motivation of the question is this really nice answer .",,"['calculus', 'sequences-and-series', 'number-theory', 'prime-numbers']"
99,What comes before precalculus [closed],What comes before precalculus [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I studied in Europe and followed mostly European academic ways. I'm tutoring a young person , and I will to know what math class comes before pre calculus. Because soon we will be starting some precalculus exercises","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I studied in Europe and followed mostly European academic ways. I'm tutoring a young person , and I will to know what math class comes before pre calculus. Because soon we will be starting some precalculus exercises",,"['calculus', 'algebra-precalculus']"
