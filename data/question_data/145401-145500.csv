,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit of $\frac{N^{n}(N!)^{n}}{(n-1)!}$ as $n$ tends to infinity,Limit of  as  tends to infinity,\frac{N^{n}(N!)^{n}}{(n-1)!} n,How to prove that $$\lim_{n\rightarrow\infty}\frac{N^{n}(N!)^{n}}{(n-1)!}=0?$$ Any help would be greatly appreciated.,How to prove that Any help would be greatly appreciated.,\lim_{n\rightarrow\infty}\frac{N^{n}(N!)^{n}}{(n-1)!}=0?,"['calculus', 'limits', 'factorial']"
1,Applying L'Hopital's rule for $\lim_{x\to\infty} x((1+1/x)^x-e)$ [duplicate],Applying L'Hopital's rule for  [duplicate],\lim_{x\to\infty} x((1+1/x)^x-e),"This question already has answers here : Find this limit using L`Hopitals rule [duplicate] (2 answers) Closed 4 years ago . I know the limit is $-e/2$ but I can't get there. I know I should be using L'Hopitals Rule here, I tried both $0/0$ and $\infty/\infty$ , either way it's a big mess. Please help. Maybe you can use the $e^{log(...)}$ trick but I haven't found it to be useful. Edit: I'm not familiar with the Big-O notation","This question already has answers here : Find this limit using L`Hopitals rule [duplicate] (2 answers) Closed 4 years ago . I know the limit is but I can't get there. I know I should be using L'Hopitals Rule here, I tried both and , either way it's a big mess. Please help. Maybe you can use the trick but I haven't found it to be useful. Edit: I'm not familiar with the Big-O notation",-e/2 0/0 \infty/\infty e^{log(...)},"['calculus', 'limits']"
2,Proof of limit of exponential function,Proof of limit of exponential function,,"I was doing a proof that for $a\in(0,1)$ $$\lim_{x\to\infty}a^x=0$$ Proof For arbitrary $\epsilon>0$ we wish to find $x_0\in \mathbb{R}$ such that $\forall x\in \mathbb{R}$ if $x>x_0$ then $|a^x|<\epsilon$ . By some analysis we set $x_0= \frac{\ln{\epsilon}}{\ln{a}}$ . Now, if we take $x>x_0$ we have \begin{align*} x>\frac{\ln{\epsilon}}{\ln{a}}\\ x\ln{a}<\ln{\epsilon}\\ \ln{a^x}<\ln{\epsilon}\\ a^x=|a^x|<\epsilon \end{align*} Here i saw that the fact that $a\in (0,1)$ helps to convert the $<$ to $>$ . Now, obviously for $a>1$ this shouldn't hold, so in this case i wanted to show that $\lim_{x\to\infty}a^x\neq0$ that is there exists $\epsilon >0$ such that for any $x_0\in \mathbb{R}$ there is $x\in \mathbb{R}$ we have both $x>x_0$ and $a^x\geq \epsilon$ . Now, i have no idea what should i pick, it seems reasonable to me that $\epsilon$ should somehow depend on $x_0$ but I can't do that according to the order of the quantifiers. So, maybe take $\epsilon = 1/2$ . Now consider arbitrary $x_0$ , we wish to find some $x$ such that if $x>x_0$ we get that $a^x\geq \epsilon$ . We know that $a^x$ is always positive. Take $x=x_0+1$ to obtain $$a^x=a^{x_0+1}=a^{x_0}+a\geq 0+a>1>1/2>\epsilon$$ I am wondering, if this is the correct approach and possible if there can be something more elegant done? Thanks","I was doing a proof that for Proof For arbitrary we wish to find such that if then . By some analysis we set . Now, if we take we have Here i saw that the fact that helps to convert the to . Now, obviously for this shouldn't hold, so in this case i wanted to show that that is there exists such that for any there is we have both and . Now, i have no idea what should i pick, it seems reasonable to me that should somehow depend on but I can't do that according to the order of the quantifiers. So, maybe take . Now consider arbitrary , we wish to find some such that if we get that . We know that is always positive. Take to obtain I am wondering, if this is the correct approach and possible if there can be something more elegant done? Thanks","a\in(0,1) \lim_{x\to\infty}a^x=0 \epsilon>0 x_0\in \mathbb{R} \forall x\in \mathbb{R} x>x_0 |a^x|<\epsilon x_0= \frac{\ln{\epsilon}}{\ln{a}} x>x_0 \begin{align*}
x>\frac{\ln{\epsilon}}{\ln{a}}\\
x\ln{a}<\ln{\epsilon}\\
\ln{a^x}<\ln{\epsilon}\\
a^x=|a^x|<\epsilon
\end{align*} a\in (0,1) < > a>1 \lim_{x\to\infty}a^x\neq0 \epsilon >0 x_0\in \mathbb{R} x\in \mathbb{R} x>x_0 a^x\geq \epsilon \epsilon x_0 \epsilon = 1/2 x_0 x x>x_0 a^x\geq \epsilon a^x x=x_0+1 a^x=a^{x_0+1}=a^{x_0}+a\geq 0+a>1>1/2>\epsilon","['real-analysis', 'limits', 'proof-verification', 'alternative-proof']"
3,Does $\lim_{z \rightarrow 3-4i}\frac{(\overline{z}-3-4i)^4}{|z-3+4i|^4}$ exist? Justify your answer.,Does  exist? Justify your answer.,\lim_{z \rightarrow 3-4i}\frac{(\overline{z}-3-4i)^4}{|z-3+4i|^4},"I  managed to manipulate the expression by changing $$\begin{align*} |z-3+4i|^4 &= |x + iy - 3 + 4i|^{4}\\ & = (x-3+i(y+4))^2(x-3-i(y+4))^2\\ & = (z-3+4i)^2( \overline{z}-3-4i)^2 \end{align*}$$ I'm not sure where I should go from here. I'm aware I can find two curves on which the function value differs while approaching the limit to prove it doesn't exist, but I can't seem to think of the two curves.","I  managed to manipulate the expression by changing $$\begin{align*} |z-3+4i|^4 &= |x + iy - 3 + 4i|^{4}\\ & = (x-3+i(y+4))^2(x-3-i(y+4))^2\\ & = (z-3+4i)^2( \overline{z}-3-4i)^2 \end{align*}$$ I'm not sure where I should go from here. I'm aware I can find two curves on which the function value differs while approaching the limit to prove it doesn't exist, but I can't seem to think of the two curves.",,['limits']
4,Evaluate $ \lim _{x \to 0} \left[{\frac{x^2}{\sin x \tan x}} \right]$ where $[\cdot]$ denotes the greatest integer function. [duplicate],Evaluate  where  denotes the greatest integer function. [duplicate], \lim _{x \to 0} \left[{\frac{x^2}{\sin x \tan x}} \right] [\cdot],"This question already has answers here : Calculating $\lim_{x\to0} \left\lfloor\frac{x^2}{\sin x \tan x}\right\rfloor$ (6 answers) Closed 5 years ago . Evaluate $$\lim _{x \to 0} \left[{\frac{x^2}{\sin x \tan x}} \right]$$ where $[\cdot]$ denotes the greatest integer function. Can anyone give me a hint to proceed? I know that $$\frac {\sin x}{x} < 1$$ for all $x \in (-\pi/2 ,\pi/2) \setminus \{0\}$ and $$\frac {\tan x}{x} > 1$$ for all $x \in (-\pi/2 ,\pi/2) \setminus \{0\}$. But will these two inequalities be helpful here?","This question already has answers here : Calculating $\lim_{x\to0} \left\lfloor\frac{x^2}{\sin x \tan x}\right\rfloor$ (6 answers) Closed 5 years ago . Evaluate $$\lim _{x \to 0} \left[{\frac{x^2}{\sin x \tan x}} \right]$$ where $[\cdot]$ denotes the greatest integer function. Can anyone give me a hint to proceed? I know that $$\frac {\sin x}{x} < 1$$ for all $x \in (-\pi/2 ,\pi/2) \setminus \{0\}$ and $$\frac {\tan x}{x} > 1$$ for all $x \in (-\pi/2 ,\pi/2) \setminus \{0\}$. But will these two inequalities be helpful here?",,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
5,"Evaluating $\lim_{n\to\infty}\frac1{n}\int_{0}^{\pi /2}{\frac{\sin^2nx}{\sin^2 x}f(x)\;dx}$, for continuous $f$","Evaluating , for continuous",\lim_{n\to\infty}\frac1{n}\int_{0}^{\pi /2}{\frac{\sin^2nx}{\sin^2 x}f(x)\;dx} f,"Let $f:\left[ 0,{\pi }/{2}\; \right]\to \mathbb{R}$ be a continuous function. What is the value of the following limit? $$\underset{n\to \infty }{\mathop{\lim }}\,\frac{1}{n}\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}f\left( x \right)dx}$$ I strongly believe that the solution depends on this integral $$\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}dx}=\frac{n\pi }{2}$$ and using Weierstrass Approximation Theorem .","Let $f:\left[ 0,{\pi }/{2}\; \right]\to \mathbb{R}$ be a continuous function. What is the value of the following limit? $$\underset{n\to \infty }{\mathop{\lim }}\,\frac{1}{n}\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}f\left( x \right)dx}$$ I strongly believe that the solution depends on this integral $$\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}dx}=\frac{n\pi }{2}$$ and using Weierstrass Approximation Theorem .",,"['real-analysis', 'integration']"
6,why the integral $\int_{n=0}^{\infty} \frac{dx}{x^8 + \sqrt{x}}$ is converge?,why the integral  is converge?,\int_{n=0}^{\infty} \frac{dx}{x^8 + \sqrt{x}},"How to show that this integral $\int_{n=0}^{\infty} \frac{dx}{x^8 + \sqrt{x}}$ is divergent? I try using $g(x)=\frac{1}{x^8}$ and then $\lim_{x \to {\infty}} \frac{\frac{dx}{x^8 + \sqrt{x}}}{\frac{1}{x^8}}$ = L (L-constant), but  $\int_{n=0}^{\infty} \frac{dx}{x^8}$ is divergent, so my answer is diverge, but the correct one is converge, Why?","How to show that this integral $\int_{n=0}^{\infty} \frac{dx}{x^8 + \sqrt{x}}$ is divergent? I try using $g(x)=\frac{1}{x^8}$ and then $\lim_{x \to {\infty}} \frac{\frac{dx}{x^8 + \sqrt{x}}}{\frac{1}{x^8}}$ = L (L-constant), but  $\int_{n=0}^{\infty} \frac{dx}{x^8}$ is divergent, so my answer is diverge, but the correct one is converge, Why?",,"['calculus', 'integration', 'limits', 'infinity']"
7,"How to prove that $x\ln\left(\frac{e^x+1}{e^x-1}\right)$ tends to $0$ as $x\to0,\infty$",How to prove that  tends to  as,"x\ln\left(\frac{e^x+1}{e^x-1}\right) 0 x\to0,\infty","How could it be shown that $$\lim_{x\to0}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=\lim_{x\to\infty}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=0\quad?$$ Note that when $x=0$, we have $x=0$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(\frac20\right)$ which is indeterminate and when $x\to\infty$, we have $x\to\infty$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(1+\frac2{e^x-1}\right)\to\ln1=0$. So it is not possible to just multiply both. I can't see L'Hopital working as the fraction in $\ln$ expands to a sum, not a fraction. Here's a plot of the function in Desmos. Any approaches?","How could it be shown that $$\lim_{x\to0}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=\lim_{x\to\infty}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=0\quad?$$ Note that when $x=0$, we have $x=0$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(\frac20\right)$ which is indeterminate and when $x\to\infty$, we have $x\to\infty$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(1+\frac2{e^x-1}\right)\to\ln1=0$. So it is not possible to just multiply both. I can't see L'Hopital working as the fraction in $\ln$ expands to a sum, not a fraction. Here's a plot of the function in Desmos. Any approaches?",,['limits']
8,A strange thing when evaluating a basic limit,A strange thing when evaluating a basic limit,,"I have attempted to evaluate the limit $\lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)$. I understand, intuitively, that the limit is equal to zero, but when trying to show it, something strange happened. I've tried to evaluate the limit based on a technique that uses the fact that $\lim _{x\to \infty }\left(\frac{1}{x^n}\right)\:=\:0,\:n>0$, and divided both the numerator and the denominator of the fraction by $x$: \begin{align} & \lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)=\lim _{x\to \infty } \left(\frac{\frac{1}{x}}{\frac{\sqrt{x-1}}{x}}\right) =\lim_{x\to \:\infty} \left(\frac{\frac{1}{x}}{\frac{\sqrt{x-1}}{\sqrt{x^2}}}\right) =\lim_{x\to \:\:\infty \:\:}\left(\frac{\frac{1}{x}}{\sqrt{\frac{x-1}{x^2}}}\right) \\[10pt] = {} & \lim_{x\to \:\:\:\infty \:\:\:} \left(\frac{\frac{1}{x}}{\sqrt{\frac{1}{x}-\frac{1}{x^2}}}\right)=\frac{\frac 1 \infty}{\sqrt{\frac 1 \infty -\frac{1}{\infty ^2}}}=\frac{0}{\sqrt{0-0}}=\frac{0}{0}= \text{indeterminate} \end{align} Moreover, when I divided the numerator and the denominator by $\sqrt{x}$, I've actually gotten the correct result: $$\lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)=\lim _{x\to \infty } \left(\frac{\frac{1}{\sqrt{x}}}{\frac{\sqrt{x-1}}{\sqrt{x}}}\right)=\lim _{x\to \infty }\left(\frac{\frac{1}{\sqrt{x}}}{\sqrt{\frac{x-1}{x}}}\right)=\lim_{x\to \:\infty \:}\left(\frac{\frac{1}{\sqrt{x}}}{\sqrt{1-\frac{1}{x}}} \right) = \frac{\frac{1}{\sqrt{\infty }}}{\sqrt{1-\frac{1}{\infty }}}=\frac{0}{\sqrt{1-0}} = \frac{0}{1}=0$$  Why is there a difference in the results? Why does dividing by $\sqrt{x}$ is allowed, but not by dividing by $x$, or have I done an arithmetic/algebraic mistake that caused the result to be wrong? Help will be much appreciated :)","I have attempted to evaluate the limit $\lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)$. I understand, intuitively, that the limit is equal to zero, but when trying to show it, something strange happened. I've tried to evaluate the limit based on a technique that uses the fact that $\lim _{x\to \infty }\left(\frac{1}{x^n}\right)\:=\:0,\:n>0$, and divided both the numerator and the denominator of the fraction by $x$: \begin{align} & \lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)=\lim _{x\to \infty } \left(\frac{\frac{1}{x}}{\frac{\sqrt{x-1}}{x}}\right) =\lim_{x\to \:\infty} \left(\frac{\frac{1}{x}}{\frac{\sqrt{x-1}}{\sqrt{x^2}}}\right) =\lim_{x\to \:\:\infty \:\:}\left(\frac{\frac{1}{x}}{\sqrt{\frac{x-1}{x^2}}}\right) \\[10pt] = {} & \lim_{x\to \:\:\:\infty \:\:\:} \left(\frac{\frac{1}{x}}{\sqrt{\frac{1}{x}-\frac{1}{x^2}}}\right)=\frac{\frac 1 \infty}{\sqrt{\frac 1 \infty -\frac{1}{\infty ^2}}}=\frac{0}{\sqrt{0-0}}=\frac{0}{0}= \text{indeterminate} \end{align} Moreover, when I divided the numerator and the denominator by $\sqrt{x}$, I've actually gotten the correct result: $$\lim _{x\to \infty }\left(\frac{1}{\sqrt{x-1}}\right)=\lim _{x\to \infty } \left(\frac{\frac{1}{\sqrt{x}}}{\frac{\sqrt{x-1}}{\sqrt{x}}}\right)=\lim _{x\to \infty }\left(\frac{\frac{1}{\sqrt{x}}}{\sqrt{\frac{x-1}{x}}}\right)=\lim_{x\to \:\infty \:}\left(\frac{\frac{1}{\sqrt{x}}}{\sqrt{1-\frac{1}{x}}} \right) = \frac{\frac{1}{\sqrt{\infty }}}{\sqrt{1-\frac{1}{\infty }}}=\frac{0}{\sqrt{1-0}} = \frac{0}{1}=0$$  Why is there a difference in the results? Why does dividing by $\sqrt{x}$ is allowed, but not by dividing by $x$, or have I done an arithmetic/algebraic mistake that caused the result to be wrong? Help will be much appreciated :)",,"['calculus', 'limits', 'radicals', 'limits-without-lhopital']"
9,How Gelfond find his limit for $\exp(\pi) $? [duplicate],How Gelfond find his limit for ? [duplicate],\exp(\pi) ,"This question already has an answer here : Origin of rapidly converging sequence for $e^\pi$ (1 answer) Closed 5 years ago . $$ a_0 = \frac{1}{\sqrt 2} $$ $$ a_{n+1} = \frac{( \sqrt {1 - a_n^2} -1)^2}{a_n^2} $$ $$ \lim_{n \to \infty} \frac{4^{\frac{1}{2^n}}}{a_{n+1}^{\frac{1}{2^n}}} = \exp(\pi) $$ How did Gelfond find this nice result ? And how to prove it ?  The 4 is trivial , but the rest is not. Is this related to trigonometry ? Is this related to continued fractions ? Are there analogues known for cube roots ? Notice a proof alone might not explain how he found the result.","This question already has an answer here : Origin of rapidly converging sequence for $e^\pi$ (1 answer) Closed 5 years ago . $$ a_0 = \frac{1}{\sqrt 2} $$ $$ a_{n+1} = \frac{( \sqrt {1 - a_n^2} -1)^2}{a_n^2} $$ $$ \lim_{n \to \infty} \frac{4^{\frac{1}{2^n}}}{a_{n+1}^{\frac{1}{2^n}}} = \exp(\pi) $$ How did Gelfond find this nice result ? And how to prove it ?  The 4 is trivial , but the rest is not. Is this related to trigonometry ? Is this related to continued fractions ? Are there analogues known for cube roots ? Notice a proof alone might not explain how he found the result.",,"['limits', 'roots', 'fixed-point-theorems', 'pi']"
10,"Switching ""For all $\epsilon$, there exists $\delta$, ..."" to ""For all $\delta$, there exists $\epsilon$, ..."" in the definition of function's limit [duplicate]","Switching ""For all , there exists , ..."" to ""For all , there exists , ..."" in the definition of function's limit [duplicate]",\epsilon \delta \delta \epsilon,"This question already has answers here : What's wrong with this ""backwards"" definition of limit? (5 answers) Closed 6 months ago . Formal accepted definition: For all $\epsilon >0$, there exist $\delta >0$ such that if $0<\vert x-a\vert<\delta$, then $\vert f(x)-L\vert<\epsilon$. The other statement: For all $\delta >0$, there exist $\epsilon >0$ such that if $0<\vert x-a\vert<\delta$, then $\vert f(x)-L\vert<\epsilon$. I know that in quantifier logic, switching quantifiers might change the statement itself, but both of the above statements seem non-contradicting to me so must it be the same?","This question already has answers here : What's wrong with this ""backwards"" definition of limit? (5 answers) Closed 6 months ago . Formal accepted definition: For all $\epsilon >0$, there exist $\delta >0$ such that if $0<\vert x-a\vert<\delta$, then $\vert f(x)-L\vert<\epsilon$. The other statement: For all $\delta >0$, there exist $\epsilon >0$ such that if $0<\vert x-a\vert<\delta$, then $\vert f(x)-L\vert<\epsilon$. I know that in quantifier logic, switching quantifiers might change the statement itself, but both of the above statements seem non-contradicting to me so must it be the same?",,"['real-analysis', 'limits', 'definition']"
11,Trigonometric functions limit to complex infinity,Trigonometric functions limit to complex infinity,,"It is well-known that trigonometric functions oscillate on the real axis and the limit does not exist as the argument approaches infinity. However, I suspect that a limiting value exist if the argument approaches any complex infinity that is not real, i.e. $$\lim_{r\to\infty} f(re^{i\theta})$$ is suspected to exist for $\theta \ne n\pi$ , where $f$ is a trigonometric function. I confirmed it is the case for $\tan (z)$ by decomposing it into real and imaginary parts. For real part: $$\lim_{r\to\infty}\frac{\sin 2r\cos\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=0$$ which is straightforward. For imaginary part: $$\lim_{r\to\infty}\frac{\sinh 2r\sin\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=\text{sgn}(\sin\theta)$$ Thus $$\lim_{r\to\infty}\tan(re^{i\theta})= \text{sgn}(\sin\theta)i$$ My questions are: Are the above calculations correct? Is there an easier way to compute the limit, other than decomposing it into real and imaginary parts? Indeed, the limits should be well known. Are there some reliable references that summarize the results for various trigonometric functions? Thanks in advance.","It is well-known that trigonometric functions oscillate on the real axis and the limit does not exist as the argument approaches infinity. However, I suspect that a limiting value exist if the argument approaches any complex infinity that is not real, i.e. is suspected to exist for , where is a trigonometric function. I confirmed it is the case for by decomposing it into real and imaginary parts. For real part: which is straightforward. For imaginary part: Thus My questions are: Are the above calculations correct? Is there an easier way to compute the limit, other than decomposing it into real and imaginary parts? Indeed, the limits should be well known. Are there some reliable references that summarize the results for various trigonometric functions? Thanks in advance.",\lim_{r\to\infty} f(re^{i\theta}) \theta \ne n\pi f \tan (z) \lim_{r\to\infty}\frac{\sin 2r\cos\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=0 \lim_{r\to\infty}\frac{\sinh 2r\sin\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=\text{sgn}(\sin\theta) \lim_{r\to\infty}\tan(re^{i\theta})= \text{sgn}(\sin\theta)i,"['limits', 'trigonometry', 'reference-request']"
12,Find $\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx$,Find,\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx,Evaluate $\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx$ I tried using the substitution $u=\frac {\pi}{2}-x$ and maybe thought this function might be symmetrical in some way and got: $$\lim_{n\to\infty}\int_{\frac {\pi}{6}}^{\frac {\pi}{2}}\frac {\cos^nx}{\sin^nx+\cos^nx}dx$$ but it's not really helping me... any other ideas?,Evaluate $\lim_{n\to\infty}\int_0^{\frac {\pi}{3}}\frac {\sin^nx}{\sin^nx+\cos^nx}dx$ I tried using the substitution $u=\frac {\pi}{2}-x$ and maybe thought this function might be symmetrical in some way and got: $$\lim_{n\to\infty}\int_{\frac {\pi}{6}}^{\frac {\pi}{2}}\frac {\cos^nx}{\sin^nx+\cos^nx}dx$$ but it's not really helping me... any other ideas?,,"['integration', 'limits', 'definite-integrals', 'limits-without-lhopital']"
13,"Prove $\lim_{y \to 0^+} \frac{y}{\pi} \int_{-\infty}^{+\infty} \frac{f(x) \, \mathrm{d}x}{x^2+y^2} = f(0)$ with $f$ continuous, bounded","Prove  with  continuous, bounded","\lim_{y \to 0^+} \frac{y}{\pi} \int_{-\infty}^{+\infty} \frac{f(x) \, \mathrm{d}x}{x^2+y^2} = f(0) f","I have been trying to solve the following problem, given $f$ continuous and bounded prove that $$\lim_{y \to 0^+} \frac{y}{\pi} \int_{-\infty}^{+\infty} \frac{f(x) \, \mathrm{d}x}{x^2+y^2} = f(0)$$ I tried first solving this by parts by integrating $\frac{1}{x^2+y^2}$ and differentiating $f(x)$ but this doesn't seem to work or bring me anywhere.","I have been trying to solve the following problem, given $f$ continuous and bounded prove that $$\lim_{y \to 0^+} \frac{y}{\pi} \int_{-\infty}^{+\infty} \frac{f(x) \, \mathrm{d}x}{x^2+y^2} = f(0)$$ I tried first solving this by parts by integrating $\frac{1}{x^2+y^2}$ and differentiating $f(x)$ but this doesn't seem to work or bring me anywhere.",,"['calculus', 'limits', 'improper-integrals']"
14,Find the derivative of $h(x) = 1/x^2$ by definition,Find the derivative of  by definition,h(x) = 1/x^2,"Can someone please explain the process of finding the derivative $h'(x)$ of  $h(x) = \dfrac{1}{x^2} $ using the delta ($\Delta$) notation. What I managed: $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{h(x+\Delta x)- h(x)}{\Delta x}$ $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{1/(x+\Delta x)^2 - 1/x^2}{\Delta x}$ $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{x^2-(x+\Delta x)^2}{(x+\Delta x)^2x^2\Delta x}$ And after this point, I do not know how to proceed. The next steps have been given as: $h '(x) \displaystyle  = \lim _{\Delta x\rightarrow 0} \frac{-2x\Delta x - (\Delta x)^2}{(x+\Delta x)^2x^2\Delta x}$ $h '(x) \displaystyle  = \lim _{\Delta x\rightarrow 0} \frac{-2x - \Delta x}{(x+\Delta x)^2x^2}$ With the answer being $h'(x) = -\dfrac{2}{x^{3}}$ Can someone please explain the process between steps 3 and 4?","Can someone please explain the process of finding the derivative $h'(x)$ of  $h(x) = \dfrac{1}{x^2} $ using the delta ($\Delta$) notation. What I managed: $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{h(x+\Delta x)- h(x)}{\Delta x}$ $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{1/(x+\Delta x)^2 - 1/x^2}{\Delta x}$ $h '(x) \displaystyle = \lim _{\Delta x\rightarrow 0} \frac{x^2-(x+\Delta x)^2}{(x+\Delta x)^2x^2\Delta x}$ And after this point, I do not know how to proceed. The next steps have been given as: $h '(x) \displaystyle  = \lim _{\Delta x\rightarrow 0} \frac{-2x\Delta x - (\Delta x)^2}{(x+\Delta x)^2x^2\Delta x}$ $h '(x) \displaystyle  = \lim _{\Delta x\rightarrow 0} \frac{-2x - \Delta x}{(x+\Delta x)^2x^2}$ With the answer being $h'(x) = -\dfrac{2}{x^{3}}$ Can someone please explain the process between steps 3 and 4?",,"['calculus', 'limits', 'derivatives']"
15,A situation with limits,A situation with limits,,"In a proof, I encountered the following situation: For every $n \in \mathbb{N}$ between $k^2 \leq n \leq (k+1)^2$, we have: $$a_k \leq b_n \le c_{k+1}$$ where $a_k, c_k \to 0$ if $k \to \infty$. They then conclude that $\lim_{n \to \infty} b_n = 0$. I can see this intuitively but can't write it out rigorously.","In a proof, I encountered the following situation: For every $n \in \mathbb{N}$ between $k^2 \leq n \leq (k+1)^2$, we have: $$a_k \leq b_n \le c_{k+1}$$ where $a_k, c_k \to 0$ if $k \to \infty$. They then conclude that $\lim_{n \to \infty} b_n = 0$. I can see this intuitively but can't write it out rigorously.",,['real-analysis']
16,Show that a linear projection is continuous if it is continuous in 0,Show that a linear projection is continuous if it is continuous in 0,,"Here is a perfectly fine answer to the question: Linear functional $f$ is continuous at $x_0=0$ if and only if $f$ is continuous $\forall x\in X$? However, I am in a set and topology course and my professor proved this using sequences. I am going over the proof and am unsure of one step. Let X with $\lVert{X}\rVert$ and Y with $\lVert{Y}\rVert$ be normed vector spaces(Over ${\rm I\!R}$). Let T: X -> Y be a linear projection. Show that if T is continuous in $\overline{0}$ it is continuous*. Solution: Let a $\in$ X. Let $x_{n}$ -> a. $x_{n}$ - a -> $\overline{0}$ $\quad$ Because X is a vector space and $x_{n}$ converges to a. T($x_{n}$ - a) -> $\overline{0}$ $\quad$ ** T($x_{n}$) - T(a) -> $\overline{0}$ $\quad$ b/c T is a linear projection T($x_{n}$) -> T(a) $\quad$ Which implies T is continuous in a. *In every point I suppose ** Why is this ok? Is there some clever rule I'm missing? /Regards","Here is a perfectly fine answer to the question: Linear functional $f$ is continuous at $x_0=0$ if and only if $f$ is continuous $\forall x\in X$? However, I am in a set and topology course and my professor proved this using sequences. I am going over the proof and am unsure of one step. Let X with $\lVert{X}\rVert$ and Y with $\lVert{Y}\rVert$ be normed vector spaces(Over ${\rm I\!R}$). Let T: X -> Y be a linear projection. Show that if T is continuous in $\overline{0}$ it is continuous*. Solution: Let a $\in$ X. Let $x_{n}$ -> a. $x_{n}$ - a -> $\overline{0}$ $\quad$ Because X is a vector space and $x_{n}$ converges to a. T($x_{n}$ - a) -> $\overline{0}$ $\quad$ ** T($x_{n}$) - T(a) -> $\overline{0}$ $\quad$ b/c T is a linear projection T($x_{n}$) -> T(a) $\quad$ Which implies T is continuous in a. *In every point I suppose ** Why is this ok? Is there some clever rule I'm missing? /Regards",,"['general-topology', 'limits', 'continuity', 'linear-transformations', 'normed-spaces']"
17,Find $\lim\limits_{n\to \infty}x_{n}$ if for all $n\ge 1$ $\{x_n\}$denotes a sequence of real numbers where $x_{n+1}=\frac{1}{2}(x_n+\frac{5}{x_n})$,Find  if for all  denotes a sequence of real numbers where,\lim\limits_{n\to \infty}x_{n} n\ge 1 \{x_n\} x_{n+1}=\frac{1}{2}(x_n+\frac{5}{x_n}),"Also given Let x be a positive number. $x_{1}=\dfrac{1}{2}(x+\dfrac{5}{x})$ , $x_{2}=\dfrac{1}{2}(x_1+\dfrac{5}{x_1})$ I have shown that for all $n\ge 1$ , $\dfrac{x_n-\sqrt{5}}{x_n+\sqrt{5}}={\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}}$ But from here I get $x_n=\dfrac{\sqrt{5}\cdot 2 \cdot \bigg({\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}}+1\bigg)}{1-{\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}}}$ But from here how do I get $lim_{n\to \infty}x_{n}$ L'Hospital does not help me here as I think. It is a GREAT GRAND SHAME ON MYSELF that I could not think of the problem in a simpler way. I should edit this: We know $x_{n}=\dfrac{1}{2}(x_{n-1}+\dfrac{5}{x_{n-1}})$ $\implies \dfrac{1}{2}(x_{n-1}+\dfrac{5}{x_{n-1}})\ge \bigg(x_{n-1}\cdot \dfrac{5}{x_{n-1}} \bigg)^{1/2}$ [By the A.M-G.M inequality] $\implies x_n \ge \sqrt{5}$ Thus $\lim\limits_{n\to \infty}x_{n}= \sqrt{5}$ Done!!I should die right now!!","Also given Let x be a positive number. , I have shown that for all , But from here I get But from here how do I get L'Hospital does not help me here as I think. It is a GREAT GRAND SHAME ON MYSELF that I could not think of the problem in a simpler way. I should edit this: We know [By the A.M-G.M inequality] Thus Done!!I should die right now!!",x_{1}=\dfrac{1}{2}(x+\dfrac{5}{x}) x_{2}=\dfrac{1}{2}(x_1+\dfrac{5}{x_1}) n\ge 1 \dfrac{x_n-\sqrt{5}}{x_n+\sqrt{5}}={\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}} x_n=\dfrac{\sqrt{5}\cdot 2 \cdot \bigg({\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}}+1\bigg)}{1-{\bigg(\dfrac{x-\sqrt{5}}{x+\sqrt{5}}}\bigg)^{2^{n}}} lim_{n\to \infty}x_{n} x_{n}=\dfrac{1}{2}(x_{n-1}+\dfrac{5}{x_{n-1}}) \implies \dfrac{1}{2}(x_{n-1}+\dfrac{5}{x_{n-1}})\ge \bigg(x_{n-1}\cdot \dfrac{5}{x_{n-1}} \bigg)^{1/2} \implies x_n \ge \sqrt{5} \lim\limits_{n\to \infty}x_{n}= \sqrt{5},"['sequences-and-series', 'limits', 'recursion']"
18,Proof Verification: $\lim_{x \to 0} \cos(1/x) $ does not converge,Proof Verification:  does not converge,\lim_{x \to 0} \cos(1/x) ,"This is one of the questions from Bartle. While attempting this question, I used the sequential criteria as follows: take $\;(x_n)=1/n$ Then, as $ n \to \infty$ $(x_n) \to 0$ Now, $\cos(1/x)  = \cos (1/1/n) = \cos(n)$ diverges as it oscillates between -1 and 1 Can anyone please verify if this approach is correct? The reason why I'm a bit doubtful is because the solution manual uses a different approach that makes use of $\pi$ and what not and was worried if this rather simplistic approach is ok? Thank you in advance.","This is one of the questions from Bartle. While attempting this question, I used the sequential criteria as follows: take $\;(x_n)=1/n$ Then, as $ n \to \infty$ $(x_n) \to 0$ Now, $\cos(1/x)  = \cos (1/1/n) = \cos(n)$ diverges as it oscillates between -1 and 1 Can anyone please verify if this approach is correct? The reason why I'm a bit doubtful is because the solution manual uses a different approach that makes use of $\pi$ and what not and was worried if this rather simplistic approach is ok? Thank you in advance.",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
19,Proving limits for fractions using epsilon-delta definition,Proving limits for fractions using epsilon-delta definition,,"Using the $\epsilon - \delta $ definition of the limit, prove that: $$\lim_{x\to 0} \frac{(2x+1)(x-2)}{3x+1} = -2$$ I firstly notice that my delta can never be greater than $\frac{1}{3}$ because there is a discontinuity at $x=-\frac{1}{3}$. I applied the standard steps as follows: $\vert \frac{(2x+1)(x-2)}{3x+1}  +2 \vert = \vert\frac{2x+3}{3x+1}\vert \vert x\vert$ Right now I need to restrict $x$ to some number, but I am not sure which value should I choose in order to easily bound my fraction, any help on choosing the correct delta is appreciated!","Using the $\epsilon - \delta $ definition of the limit, prove that: $$\lim_{x\to 0} \frac{(2x+1)(x-2)}{3x+1} = -2$$ I firstly notice that my delta can never be greater than $\frac{1}{3}$ because there is a discontinuity at $x=-\frac{1}{3}$. I applied the standard steps as follows: $\vert \frac{(2x+1)(x-2)}{3x+1}  +2 \vert = \vert\frac{2x+3}{3x+1}\vert \vert x\vert$ Right now I need to restrict $x$ to some number, but I am not sure which value should I choose in order to easily bound my fraction, any help on choosing the correct delta is appreciated!",,"['calculus', 'limits', 'epsilon-delta']"
20,A sequence $p_n(x)$ that converges for infinitely many values of $x$,A sequence  that converges for infinitely many values of,p_n(x) x,"Let $(a_n)_{n \geq 1}, (b_n)_{n \geq 1}, (c_n)_{n \geq 1}$ be sequences of real numbers. Knowing that the sequence $$p_n(x)=(x-a_n)(x-b_n)(x-c_n)$$   converges for infinitely many values of $x$, prove that it converges for every $x \in \mathbb{R}$. This is very similar to what happens to a polynomial when it is involved in something which happens ""for infinitely many values"": it actually happens for all values. Starting from this, I tried to write $$p_n(x)=x^3-(a_n+b_n+c_n)x^2+(a_nb_n+b_nc_n+c_na_n)x-a_nb_nc_n$$ But from here, I don't know anything about these $3$ sequences and I couldn't proceed further.","Let $(a_n)_{n \geq 1}, (b_n)_{n \geq 1}, (c_n)_{n \geq 1}$ be sequences of real numbers. Knowing that the sequence $$p_n(x)=(x-a_n)(x-b_n)(x-c_n)$$   converges for infinitely many values of $x$, prove that it converges for every $x \in \mathbb{R}$. This is very similar to what happens to a polynomial when it is involved in something which happens ""for infinitely many values"": it actually happens for all values. Starting from this, I tried to write $$p_n(x)=x^3-(a_n+b_n+c_n)x^2+(a_nb_n+b_nc_n+c_na_n)x-a_nb_nc_n$$ But from here, I don't know anything about these $3$ sequences and I couldn't proceed further.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
21,Calculating $\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3}$.,Calculating .,\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3},"I have a difficulty in calculating this limit: $$\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3},$$ I have tried $\tan x = \frac{\sin x}{\cos x}$, then I unified the denominator of the numerator of the given limit problem finally I got $$\lim_{x \rightarrow 0} \frac{\sin x}{x^{3} \cos x} - \lim_{x \rightarrow 0} \frac{ \sin x}{x^3},$$ Then I got stucked, could anyone help me in solving it?","I have a difficulty in calculating this limit: $$\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3},$$ I have tried $\tan x = \frac{\sin x}{\cos x}$, then I unified the denominator of the numerator of the given limit problem finally I got $$\lim_{x \rightarrow 0} \frac{\sin x}{x^{3} \cos x} - \lim_{x \rightarrow 0} \frac{ \sin x}{x^3},$$ Then I got stucked, could anyone help me in solving it?",,['calculus']
22,Proof Verification: $\lim{C^{1/n}}=1$ for $C>0$,Proof Verification:  for,\lim{C^{1/n}}=1 C>0,"Claim: $\lim{C^{1/n}}=1$ for $C>0$ Working:  $|C^{1/n}-1| < \epsilon$ $ \implies C^{1/n} < \epsilon +1$ $\implies {(1/n)}\ln{C} < \ln{(\epsilon +1)}$ $ \implies n> {\ln{C}/\ln{(\epsilon +1)}}$ Proof: Let $ \epsilon >0$ be given. Choose $N>{\ln{C}/\ln{(\epsilon +1)}}$ Then for any $n>N$,  this implies that  $|C^{1/n} -1|<\epsilon$ Hence, $\lim{C^{1/n}} =1$ for all $C>0$ Can anyone please verify this proof?","Claim: $\lim{C^{1/n}}=1$ for $C>0$ Working:  $|C^{1/n}-1| < \epsilon$ $ \implies C^{1/n} < \epsilon +1$ $\implies {(1/n)}\ln{C} < \ln{(\epsilon +1)}$ $ \implies n> {\ln{C}/\ln{(\epsilon +1)}}$ Proof: Let $ \epsilon >0$ be given. Choose $N>{\ln{C}/\ln{(\epsilon +1)}}$ Then for any $n>N$,  this implies that  $|C^{1/n} -1|<\epsilon$ Hence, $\lim{C^{1/n}} =1$ for all $C>0$ Can anyone please verify this proof?",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
23,Is this result correct? Limits don't depend on norm.,Is this result correct? Limits don't depend on norm.,,"Let $\|\cdot\|: \mathbb R \to \mathbb R$ be some norm on $\mathbb R$ which is not the standard absolute value $|\cdot|$ we all know and love. $f: \mathbb R \to \mathbb R, L \in \mathbb R.$ Suppose that $\lim_{t \to 0} f(t) = L$ with respect to $\| \cdot \|$ This means that $\forall \epsilon > 0 \exists \delta >0: \|t\| \leq \delta \implies\|f(t)-L\| \leq \epsilon$ I would like to show that this is also true with respect to $|\cdot |$. The limit does not depend on with respect to which norm does $t$ approach zero. What I did: Fix $\epsilon >0 $ and choose a $\delta$ such that $\|t\| \leq \delta \implies\|f(t)-f(0)\| \leq \epsilon$. All norms are equivalent, so there is $c >0$ such that $c|t| \leq \|t\| \leq \delta$ and $c|f(t)-L| \leq \|f(t)-L\| \leq \epsilon$ So $|t| \leq \frac{\delta}{c} = \delta'$ and $|f(t)-L| \leq \frac{\epsilon}{c} = \epsilon'$ Finally, $|t| \leq \delta ' \implies\frac{\|t\|}{c} \leq\frac{\delta}{c} \implies \|t\| \leq \delta$ That implies by our initial assumption that $\|f(t)-L\| \leq \epsilon$, but then $|f(t)-L| \leq \frac{\epsilon}{c} = \epsilon'$. Since our $\epsilon$ is as small as we would like, so is $\epsilon'$, and that concludes the proof. Is this proof correct? Question number 2 The differential of a function activated on unit vector $h$ is defined to be $D_f(a)h = \lim_{t \to 0} \frac{f(a+th)-f(a)}{t}$. Suppose my proof is correct and it does not matter with respect to which norm does $t$ approach zero here. Does it matter with respect to which norm is $h$ a unit vector? In other words- Does the differential of a function at all depend on the choice of the norms and inner products we work with? If so - why? if not - why not?","Let $\|\cdot\|: \mathbb R \to \mathbb R$ be some norm on $\mathbb R$ which is not the standard absolute value $|\cdot|$ we all know and love. $f: \mathbb R \to \mathbb R, L \in \mathbb R.$ Suppose that $\lim_{t \to 0} f(t) = L$ with respect to $\| \cdot \|$ This means that $\forall \epsilon > 0 \exists \delta >0: \|t\| \leq \delta \implies\|f(t)-L\| \leq \epsilon$ I would like to show that this is also true with respect to $|\cdot |$. The limit does not depend on with respect to which norm does $t$ approach zero. What I did: Fix $\epsilon >0 $ and choose a $\delta$ such that $\|t\| \leq \delta \implies\|f(t)-f(0)\| \leq \epsilon$. All norms are equivalent, so there is $c >0$ such that $c|t| \leq \|t\| \leq \delta$ and $c|f(t)-L| \leq \|f(t)-L\| \leq \epsilon$ So $|t| \leq \frac{\delta}{c} = \delta'$ and $|f(t)-L| \leq \frac{\epsilon}{c} = \epsilon'$ Finally, $|t| \leq \delta ' \implies\frac{\|t\|}{c} \leq\frac{\delta}{c} \implies \|t\| \leq \delta$ That implies by our initial assumption that $\|f(t)-L\| \leq \epsilon$, but then $|f(t)-L| \leq \frac{\epsilon}{c} = \epsilon'$. Since our $\epsilon$ is as small as we would like, so is $\epsilon'$, and that concludes the proof. Is this proof correct? Question number 2 The differential of a function activated on unit vector $h$ is defined to be $D_f(a)h = \lim_{t \to 0} \frac{f(a+th)-f(a)}{t}$. Suppose my proof is correct and it does not matter with respect to which norm does $t$ approach zero here. Does it matter with respect to which norm is $h$ a unit vector? In other words- Does the differential of a function at all depend on the choice of the norms and inner products we work with? If so - why? if not - why not?",,"['calculus', 'limits', 'proof-verification']"
24,"If $2\tan^2x - 5\sec x = 1$ has exactly $7$ distinct solutions for $x\in[0,\frac{n\pi}{2}]$, $n\in N$, then the greatest value of $n$ is?","If  has exactly  distinct solutions for , , then the greatest value of  is?","2\tan^2x - 5\sec x = 1 7 x\in[0,\frac{n\pi}{2}] n\in N n","If $2\tan^2x - 5\sec x = 1$ has exactly $7$ distinct solutions for $x\in[0,\frac{n\pi}{2}]$, $n\in N$, then the greatest value of $n$ is? My attempt: Solving the above quadratic equation, we get $\cos x = \frac{1}{3}$ The general solution of the equation is given by $\cos x = 2n\pi \pm \cos^{-1}\frac{1}{3}$ For having $7$ distinct solutions, $n$ can have value = 0,1,2,3 So, from here we can conclude that $n$ is anything but greater than $6$. So, according to the options given in the questions, the greatest value of $n$ should be $13$. But the answer given is $14$. Can anyone justify?","If $2\tan^2x - 5\sec x = 1$ has exactly $7$ distinct solutions for $x\in[0,\frac{n\pi}{2}]$, $n\in N$, then the greatest value of $n$ is? My attempt: Solving the above quadratic equation, we get $\cos x = \frac{1}{3}$ The general solution of the equation is given by $\cos x = 2n\pi \pm \cos^{-1}\frac{1}{3}$ For having $7$ distinct solutions, $n$ can have value = 0,1,2,3 So, from here we can conclude that $n$ is anything but greater than $6$. So, according to the options given in the questions, the greatest value of $n$ should be $13$. But the answer given is $14$. Can anyone justify?",,"['limits', 'trigonometry']"
25,What is the limit of |x| as x approaches 0?,What is the limit of |x| as x approaches 0?,,"Alright, I feel like this is simpler than I think it is... So the limit we want to know is $$\lim_{x \to 0}  f(x)=|x|$$ First we look at the piecewise definition which is given by $$|x| = \begin{cases} x,  & x\geqslant 0 \\ -x, & x\lt0 \end{cases}$$ We then look at the one sided limits, for the limit to 0 from above, we consider the case where $$x\geqslant 0$$ such that $$\lim_{x \to 0^+}  x$$ Yet this leaves us with just an x, which as it goes to 0... is 0? Yet the solutions I have calculate it in the followin way, $$\lim_{x \to 0^+}  \frac{|x|}{x} = 1$$ Why is it divided by x, where does that come from? And furthermore, why does it approach 1 when there are other numbers that exist between 1 and 0 that x could approach... I guess I'm also missing an intuitive understanding?","Alright, I feel like this is simpler than I think it is... So the limit we want to know is $$\lim_{x \to 0}  f(x)=|x|$$ First we look at the piecewise definition which is given by $$|x| = \begin{cases} x,  & x\geqslant 0 \\ -x, & x\lt0 \end{cases}$$ We then look at the one sided limits, for the limit to 0 from above, we consider the case where $$x\geqslant 0$$ such that $$\lim_{x \to 0^+}  x$$ Yet this leaves us with just an x, which as it goes to 0... is 0? Yet the solutions I have calculate it in the followin way, $$\lim_{x \to 0^+}  \frac{|x|}{x} = 1$$ Why is it divided by x, where does that come from? And furthermore, why does it approach 1 when there are other numbers that exist between 1 and 0 that x could approach... I guess I'm also missing an intuitive understanding?",,"['calculus', 'limits']"
26,"Computing $\lim\limits_{n \to \infty} \int_0^{\frac{\pi}{2}}{\frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\mathrm{d}x} $",Computing,"\lim\limits_{n \to \infty} \int_0^{\frac{\pi}{2}}{\frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\mathrm{d}x} ","$\def\d{\mathrm{d}}$I would like to compute the following limit,  $$\displaystyle{\lim_{n \to \infty} \int_0^{\frac{\pi}{2}} \frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\d x} .$$ I am looking for a high school answer. I tried writing $$\lim_{n \to \infty} \int_0^{\frac{\pi}{2}}{\frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\d x = \lim_{n \to \infty} \lim_{ε \to \frac{\pi}{2}}\int_0^ε{\frac{(\sin(x))^n}{1-\sin(x)}}\,\d x},$$ but it doesn't help me, since $1 - \sin(x) \leq 1, \forall x \in \left[0,  \dfrac{\pi}{2}\right]$.","$\def\d{\mathrm{d}}$I would like to compute the following limit,  $$\displaystyle{\lim_{n \to \infty} \int_0^{\frac{\pi}{2}} \frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\d x} .$$ I am looking for a high school answer. I tried writing $$\lim_{n \to \infty} \int_0^{\frac{\pi}{2}}{\frac{(\sin(x))^{n}}{1-\sin{(x)}}\,\d x = \lim_{n \to \infty} \lim_{ε \to \frac{\pi}{2}}\int_0^ε{\frac{(\sin(x))^n}{1-\sin(x)}}\,\d x},$$ but it doesn't help me, since $1 - \sin(x) \leq 1, \forall x \in \left[0,  \dfrac{\pi}{2}\right]$.",,"['real-analysis', 'limits', 'definite-integrals', 'improper-integrals', 'trigonometric-integrals']"
27,The sum of finitely many geometric sequences has a limit iff every base is less than 1 in absolute value,The sum of finitely many geometric sequences has a limit iff every base is less than 1 in absolute value,,"It is well known that a geometric sequence $x_n = c b^n$ with a base $b\ne 1$ has a limit if and only if $|b|<1$. (Note that $b$ might be complex.) I'd like to prove a more general fact: a finite sum of such sequences, with distinct bases $b_1, \dots, b_r$, not equal to $1$, has a limit if and only if $|b_1|, \dots, |b_r|<1$. More precisely: if $$x_n = c_1 b_1^n + c_2 b_2^n + \dots + c_r b_r^n$$ where $r$ is a fixed integer, $c_k$ are fixed nonzero complex numbers, and $b_k$ are fixed distinct complex numbers not equal to $1$, then $\lim_{n\to\infty} x_n$ exists if and only if $\max_k |b_k|<1$. Progress . Sufficiency is obvious, the question is necessity. The terms with larger modulus dominate the rest, thus we may assume $|b_1|=\dots = |b_r|$.  Also, it suffices to consider the case $|b_1|=\dots = |b_r| = 1$ because if the common modulus is $M>1$ and the limit exists, then multiplying the sequence by $M^{-n}$ will result in another convergent sequence. So, $b_k = \exp(2\pi i \theta_k)$ for some $\theta_k\in (0, 1)$. If all $\theta_k$ are rational, the sequence $x_n$ is periodic (and nonconstant), so there is no limit. How to deal with the case when some $\theta_k$ are irrational?","It is well known that a geometric sequence $x_n = c b^n$ with a base $b\ne 1$ has a limit if and only if $|b|<1$. (Note that $b$ might be complex.) I'd like to prove a more general fact: a finite sum of such sequences, with distinct bases $b_1, \dots, b_r$, not equal to $1$, has a limit if and only if $|b_1|, \dots, |b_r|<1$. More precisely: if $$x_n = c_1 b_1^n + c_2 b_2^n + \dots + c_r b_r^n$$ where $r$ is a fixed integer, $c_k$ are fixed nonzero complex numbers, and $b_k$ are fixed distinct complex numbers not equal to $1$, then $\lim_{n\to\infty} x_n$ exists if and only if $\max_k |b_k|<1$. Progress . Sufficiency is obvious, the question is necessity. The terms with larger modulus dominate the rest, thus we may assume $|b_1|=\dots = |b_r|$.  Also, it suffices to consider the case $|b_1|=\dots = |b_r| = 1$ because if the common modulus is $M>1$ and the limit exists, then multiplying the sequence by $M^{-n}$ will result in another convergent sequence. So, $b_k = \exp(2\pi i \theta_k)$ for some $\theta_k\in (0, 1)$. If all $\theta_k$ are rational, the sequence $x_n$ is periodic (and nonconstant), so there is no limit. How to deal with the case when some $\theta_k$ are irrational?",,['calculus']
28,"Limits with two variables, finding k such that limit exists","Limits with two variables, finding k such that limit exists",,"Find the biggest number $k$, such that the limit $$ \lim_{(x,y)\to(0.0)} \frac{x^{15}y^{23}}{(x^2 +y^2)^p} $$ exists for all $p < k $ I was thinking that if we're left with x's and/or y's in the numerator and denominator, we have an expression in an undetermined form, $0 \over 0$ so i though that $k = 7.5$ would be correct, seeing as that would give a $0\over 0 $ expression, atleast from what I've calculated.","Find the biggest number $k$, such that the limit $$ \lim_{(x,y)\to(0.0)} \frac{x^{15}y^{23}}{(x^2 +y^2)^p} $$ exists for all $p < k $ I was thinking that if we're left with x's and/or y's in the numerator and denominator, we have an expression in an undetermined form, $0 \over 0$ so i though that $k = 7.5$ would be correct, seeing as that would give a $0\over 0 $ expression, atleast from what I've calculated.",,"['limits', 'multivariable-calculus']"
29,Find the limit of $a_{n+1}=\cos(a_n)$ [duplicate],Find the limit of  [duplicate],a_{n+1}=\cos(a_n),"This question already has answers here : Does $\lim_{n\to\infty}\underset{n}{\underbrace{\cos(\cos(...\cos x))}}$ exist? [duplicate] (1 answer) If $a_{n+1}=\cos(a_n)$ for $n\ge0$ and $a_0 \in [-\pi/2,\pi/2]$, find $\lim_{n \to \infty}a_n$ if it exists (2 answers) Closed 6 years ago . Let $\beta\in[-1,1]$ and let $a_n$ defined by $ \begin{cases} a_1=\beta,  \\ a_{n+1}=\cos(a_n) \end{cases}$ Let $c\in \mathbb{R}$, s.a $\cos(c)=c$. Prove that : $\lim\limits_{n\to\infty}a_n=c$ My work so far: I know that $c\in(0,1)$. $|\cos(a_n)-c|=|\cos(a_n)-\cos(c)|=|-2\sin(\frac{a_n+c}{2})\sin(\frac{a_n-c}{2})|\le\frac{1}{2}|a_n+c||a_n-c|$ But I don't know how to continue from here.","This question already has answers here : Does $\lim_{n\to\infty}\underset{n}{\underbrace{\cos(\cos(...\cos x))}}$ exist? [duplicate] (1 answer) If $a_{n+1}=\cos(a_n)$ for $n\ge0$ and $a_0 \in [-\pi/2,\pi/2]$, find $\lim_{n \to \infty}a_n$ if it exists (2 answers) Closed 6 years ago . Let $\beta\in[-1,1]$ and let $a_n$ defined by $ \begin{cases} a_1=\beta,  \\ a_{n+1}=\cos(a_n) \end{cases}$ Let $c\in \mathbb{R}$, s.a $\cos(c)=c$. Prove that : $\lim\limits_{n\to\infty}a_n=c$ My work so far: I know that $c\in(0,1)$. $|\cos(a_n)-c|=|\cos(a_n)-\cos(c)|=|-2\sin(\frac{a_n+c}{2})\sin(\frac{a_n-c}{2})|\le\frac{1}{2}|a_n+c||a_n-c|$ But I don't know how to continue from here.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
30,$\lim\limits_{x\to\infty}\frac{\sqrt{4x^2+x^4}+3x^2}{x^2-5x}$,,\lim\limits_{x\to\infty}\frac{\sqrt{4x^2+x^4}+3x^2}{x^2-5x},"Can anyone help me solve this? I know the answer is 4, but I don't really know how do I find the biggest power of $x$ when there's a square root. $$\lim_{x\to\infty}\frac{\sqrt{4x^2+x^4}+3x^2}{x^2-5x}$$","Can anyone help me solve this? I know the answer is 4, but I don't really know how do I find the biggest power of $x$ when there's a square root. $$\lim_{x\to\infty}\frac{\sqrt{4x^2+x^4}+3x^2}{x^2-5x}$$",,"['calculus', 'limits']"
31,A problem in using theorem for finding limit,A problem in using theorem for finding limit,,"Let $a_n = \arctan ( \ln n )$ , find $\lim_{ n \to \infty} a_n$ if any . I think we can't apply the theorem (i.e $\lim_{ n \to \infty} \arctan ( \ln n ) = \arctan (\lim_{ n \to \infty} \ln n) = \pi /2 )$ . The condition of theorem is $a_n \to L$  but here $a_n$ diverges to infinity . Although , we know that $\pi / 2$ is the right answer since $\lim_{x \to \infty} \arctan x = \pi /2  $ and instead of $a_n$ we can put any other sequence that diverges to infinity .","Let $a_n = \arctan ( \ln n )$ , find $\lim_{ n \to \infty} a_n$ if any . I think we can't apply the theorem (i.e $\lim_{ n \to \infty} \arctan ( \ln n ) = \arctan (\lim_{ n \to \infty} \ln n) = \pi /2 )$ . The condition of theorem is $a_n \to L$  but here $a_n$ diverges to infinity . Although , we know that $\pi / 2$ is the right answer since $\lim_{x \to \infty} \arctan x = \pi /2  $ and instead of $a_n$ we can put any other sequence that diverges to infinity .",,"['sequences-and-series', 'limits', 'limits-without-lhopital']"
32,convergence of square summable sequence,convergence of square summable sequence,,"I have seen a very interensting but for me confusing statement. It says that if $\sum\limits_{n=1}^{\infty} a_{n}^{2} < \infty,$ then necessarily holds that $\lim\limits_{n \rightarrow \infty} a_{n}=0$ (from neccesary condition of convergence of sequence). But i don't understand, how is connected mentioned condition and convergence of squared summable sequence? I mean i know, that if the series of the sequence $a_{n}$ converges then $\lim\limits_{n \rightarrow \infty} a_{n}=0,$ but does it hold for the mentioned statement as well?","I have seen a very interensting but for me confusing statement. It says that if $\sum\limits_{n=1}^{\infty} a_{n}^{2} < \infty,$ then necessarily holds that $\lim\limits_{n \rightarrow \infty} a_{n}=0$ (from neccesary condition of convergence of sequence). But i don't understand, how is connected mentioned condition and convergence of squared summable sequence? I mean i know, that if the series of the sequence $a_{n}$ converges then $\lim\limits_{n \rightarrow \infty} a_{n}=0,$ but does it hold for the mentioned statement as well?",,"['real-analysis', 'limits', 'convergence-divergence', 'summation']"
33,3 sequences which are convergent,3 sequences which are convergent,,"Let $a_0, b_0, c_0\in \mathbb{R}$ and $k>0$. If   \begin{align} a_{n+1}=\frac{1}{k+1}b_n+\frac{k}{k+1}c_n\\ b_{n+1}=\frac{1}{k+1}c_n+\frac{k}{k+1}a_n\\ c_{n+1}=\frac{1}{k+1}a_n+\frac{k}{k+1}b_n \end{align}   Prove that $(a_n), (b_n),(c_n)$ are convergent. I know that they are all convergent to $\frac{a_0+b_0+c_0}{3}$ and I have a geometrical solution. However, I'm interested in a purely analytical one. I got that $(a_n+b_n+c_n)$ is constant and then I tried to get a relation only in terms of say $(a_n)$, but this worked only when $k=1$.","Let $a_0, b_0, c_0\in \mathbb{R}$ and $k>0$. If   \begin{align} a_{n+1}=\frac{1}{k+1}b_n+\frac{k}{k+1}c_n\\ b_{n+1}=\frac{1}{k+1}c_n+\frac{k}{k+1}a_n\\ c_{n+1}=\frac{1}{k+1}a_n+\frac{k}{k+1}b_n \end{align}   Prove that $(a_n), (b_n),(c_n)$ are convergent. I know that they are all convergent to $\frac{a_0+b_0+c_0}{3}$ and I have a geometrical solution. However, I'm interested in a purely analytical one. I got that $(a_n+b_n+c_n)$ is constant and then I tried to get a relation only in terms of say $(a_n)$, but this worked only when $k=1$.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
34,Can we find the limit $\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{\left(-1\right)^nx^2}{n^2+x^2}$ without evaluating the sum?,Can we find the limit  without evaluating the sum?,\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{\left(-1\right)^nx^2}{n^2+x^2},"How to find the limit   $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}x^{2}}{n^{2}+x^{2}}$   if we don't evaluate the sum? I know the sum is actually an elementary function which we can find it using Fourier series or other methods, but I'm just curious about if there exists some alternative ways to find this limit. I tried to write it as this form: $$\displaystyle\lim_{x\rightarrow\infty}x\sum_{n=1}^{\infty}\left(\frac{x}{\left(2n\right)^{2}+x^{2}}-\frac{x}{\left(2n-1\right)^{2}+x^{2}}\right).$$ As we know, $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{x}{\left(2n\right)^{2}+x^{2}}$ and $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{x}{\left(2n-1\right)^{2}+x^{2}}$ must get a same value (we don't need to care about what the exact value is) , so this is in the form $``0\cdot\infty""$, which cannot be evaluated directly. This is where I get stucked. After days of thinking, I'm getting closer to the answer. We can use easy algebra to get that $$\left |\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right |\leq\frac{1}{n^2}\quad\forall n\in\mathbb{Z^+},x\in\mathbb{R}$$ Hence the series below converges uniformly on $\mathbb{R}$:$$\displaystyle\sum_{n=1}^{\infty}\left(\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right)$$ Changing the order of sum and limit, we can get:$$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right)=0$$ which is$$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n\right)^{2}+x^{2}}-\frac{x^2}{\left(2n-1\right)^{2}+x^{2}}\right)=\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n+1\right)^{2}+x^{2}}-\frac{x^2}{\left(2n\right)^{2}+x^{2}}\right)$$ and we also know $$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n\right)^{2}+x^{2}}-\frac{x^2}{\left(2n-1\right)^{2}+x^{2}}\right)=\lim_{x\rightarrow\infty}\left(-\frac{x^2}{1+x^2}-\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n+1\right)^{2}+x^{2}}-\frac{x^2}{\left(2n\right)^{2}+x^{2}}\right)\right)$$ If the limit exists, there must be an equation for the limit $L=-1-L$ which solves $L=-1/2$. So everything needed is to prove that the limit exists. This would require a bit of analysis. I’m going to prove it via Cauchy’s rule ($\displaystyle\lim_{x\rightarrow+\infty}f\left(x\right)\ exists\Leftrightarrow\forall\epsilon>0\exists X>0 \forall x_1,x_2>X, \left|f(x_1)-f(x_2)\right|<\epsilon$).","How to find the limit   $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}x^{2}}{n^{2}+x^{2}}$   if we don't evaluate the sum? I know the sum is actually an elementary function which we can find it using Fourier series or other methods, but I'm just curious about if there exists some alternative ways to find this limit. I tried to write it as this form: $$\displaystyle\lim_{x\rightarrow\infty}x\sum_{n=1}^{\infty}\left(\frac{x}{\left(2n\right)^{2}+x^{2}}-\frac{x}{\left(2n-1\right)^{2}+x^{2}}\right).$$ As we know, $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{x}{\left(2n\right)^{2}+x^{2}}$ and $\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{x}{\left(2n-1\right)^{2}+x^{2}}$ must get a same value (we don't need to care about what the exact value is) , so this is in the form $``0\cdot\infty""$, which cannot be evaluated directly. This is where I get stucked. After days of thinking, I'm getting closer to the answer. We can use easy algebra to get that $$\left |\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right |\leq\frac{1}{n^2}\quad\forall n\in\mathbb{Z^+},x\in\mathbb{R}$$ Hence the series below converges uniformly on $\mathbb{R}$:$$\displaystyle\sum_{n=1}^{\infty}\left(\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right)$$ Changing the order of sum and limit, we can get:$$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{2x^2}{\left(2n\right)^2+x^2}-\frac{x^2}{\left(2n-1\right)^2+x^2}-\frac{x^2}{\left(2n+1\right)^2+x^2}\right)=0$$ which is$$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n\right)^{2}+x^{2}}-\frac{x^2}{\left(2n-1\right)^{2}+x^{2}}\right)=\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n+1\right)^{2}+x^{2}}-\frac{x^2}{\left(2n\right)^{2}+x^{2}}\right)$$ and we also know $$\displaystyle\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n\right)^{2}+x^{2}}-\frac{x^2}{\left(2n-1\right)^{2}+x^{2}}\right)=\lim_{x\rightarrow\infty}\left(-\frac{x^2}{1+x^2}-\sum_{n=1}^{\infty}\left(\frac{x^2}{\left(2n+1\right)^{2}+x^{2}}-\frac{x^2}{\left(2n\right)^{2}+x^{2}}\right)\right)$$ If the limit exists, there must be an equation for the limit $L=-1-L$ which solves $L=-1/2$. So everything needed is to prove that the limit exists. This would require a bit of analysis. I’m going to prove it via Cauchy’s rule ($\displaystyle\lim_{x\rightarrow+\infty}f\left(x\right)\ exists\Leftrightarrow\forall\epsilon>0\exists X>0 \forall x_1,x_2>X, \left|f(x_1)-f(x_2)\right|<\epsilon$).",,"['sequences-and-series', 'analysis', 'limits']"
35,Summary of my understanding of sequences and series' convergence and divergence?,Summary of my understanding of sequences and series' convergence and divergence?,,"I'm trying to summarise my understanding of infinite sequences, series, and their relationships with respect to convergence at the fundamental level. Here is what I know. How much of this is correct? First off, here's a table of the notations that I use, and their corresponding meaning. My understanding is that: The sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$ converges if $$\lim\limits_{n\to\infty}a_n=L_{a}.$$ The infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges if its sequence of partial sums,  $\lbrace s_n \rbrace _{n=0}^{\infty}$, has a limit, i.e.$$\lim\limits_{n\to\infty}s_n=L_{s}.$$ If the infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges, then the limit of the sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$ is $0$, i.e. $$\sum\limits_{n=0}^{\infty}a_n \: converges \rightarrow    \lim\limits_{n\to\infty}a_n=0.$$ The divergence test: If the the limit of the sequence $\lbrace a_n \rbrace    _{n=0}^{\infty}$ is NOT $0$ or does not exist, then the infinite series diverges, i.e. $$\lim\limits_{n\to\infty}a_n\neq0 \rightarrow \sum\limits_{n=0}^{\infty}a_n \: diverges$$ Would seriously appreciate it if anyone could verify whether the above is accurate or incorrect in any way. EDIT: I've modified the two limits notation that were mentioned in the comments and answers below, as well as adding the additional condition (limit does not exist or does not equal zero) for the divergence test. I appreciate all the answers/comments.","I'm trying to summarise my understanding of infinite sequences, series, and their relationships with respect to convergence at the fundamental level. Here is what I know. How much of this is correct? First off, here's a table of the notations that I use, and their corresponding meaning. My understanding is that: The sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$ converges if $$\lim\limits_{n\to\infty}a_n=L_{a}.$$ The infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges if its sequence of partial sums,  $\lbrace s_n \rbrace _{n=0}^{\infty}$, has a limit, i.e.$$\lim\limits_{n\to\infty}s_n=L_{s}.$$ If the infinite series $\sum\limits_{n=0}^{\infty}a_n$ converges, then the limit of the sequence $\lbrace a_n \rbrace _{n=0}^{\infty}$ is $0$, i.e. $$\sum\limits_{n=0}^{\infty}a_n \: converges \rightarrow    \lim\limits_{n\to\infty}a_n=0.$$ The divergence test: If the the limit of the sequence $\lbrace a_n \rbrace    _{n=0}^{\infty}$ is NOT $0$ or does not exist, then the infinite series diverges, i.e. $$\lim\limits_{n\to\infty}a_n\neq0 \rightarrow \sum\limits_{n=0}^{\infty}a_n \: diverges$$ Would seriously appreciate it if anyone could verify whether the above is accurate or incorrect in any way. EDIT: I've modified the two limits notation that were mentioned in the comments and answers below, as well as adding the additional condition (limit does not exist or does not equal zero) for the divergence test. I appreciate all the answers/comments.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
36,"By using Wallis’s product, prove $\zeta(2)=\frac{\pi^2}{6}$","By using Wallis’s product, prove",\zeta(2)=\frac{\pi^2}{6},"$\sin x = x\left(1-\frac{x^2}{\pi^2}\right)\left(1-\frac{x^2}{(2\pi)^2}\right)\cdots$ This equal is used in both the solution of  Basel’s problem and the proof of Wallis’s product. So I wonder if I can prove one by using the other. Help me solve this problem. Or if you are sure this assumption is wrong, tell me the reason. I’m sorry if I put wrong tugs.","$\sin x = x\left(1-\frac{x^2}{\pi^2}\right)\left(1-\frac{x^2}{(2\pi)^2}\right)\cdots$ This equal is used in both the solution of  Basel’s problem and the proof of Wallis’s product. So I wonder if I can prove one by using the other. Help me solve this problem. Or if you are sure this assumption is wrong, tell me the reason. I’m sorry if I put wrong tugs.",,"['calculus', 'number-theory', 'limits']"
37,Limit of $m/(x^m - 1) - n/(x^n - 1)$ as $x\rightarrow 1$ without l'Hôpital's rule? [duplicate],Limit of  as  without l'Hôpital's rule? [duplicate],m/(x^m - 1) - n/(x^n - 1) x\rightarrow 1,"This question already has answers here : Showing that $\lim_{x \to 1} \left(\frac{23}{1-x^{23}}-\frac{11}{1-x^{11}} \right)=6$ (11 answers) Closed 6 years ago . Let $m,n\in\mathbb N$. How can I solve this limit without using l'Hopital's rule, please? $$\lim_{x\to1}\left(\frac{m}{1-x^m}-\frac{n}{1-x^n}\right)$$","This question already has answers here : Showing that $\lim_{x \to 1} \left(\frac{23}{1-x^{23}}-\frac{11}{1-x^{11}} \right)=6$ (11 answers) Closed 6 years ago . Let $m,n\in\mathbb N$. How can I solve this limit without using l'Hopital's rule, please? $$\lim_{x\to1}\left(\frac{m}{1-x^m}-\frac{n}{1-x^n}\right)$$",,"['real-analysis', 'analysis', 'limits', 'limits-without-lhopital']"
38,Find $\lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x}$,Find,\lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x},Find $$\lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x}$$ My work so far: $$\lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x}=\frac{\ln3-\ln5}{\ln4-\ln10}$$ Is correct? Add: I used $a^x\sim 1+x\ln a$ for $x\rightarrow 0$,Find My work so far: Is correct? Add: I used for,\lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x} \lim_{x\rightarrow0}\frac{3^x-5^x}{4^x-10^x}=\frac{\ln3-\ln5}{\ln4-\ln10} a^x\sim 1+x\ln a x\rightarrow 0,"['real-analysis', 'limits']"
39,What is $\lim\limits_{n\to\infty}\frac{\Gamma(n+1)\Gamma(\alpha-n+1)}{n^{\alpha+1} }$,What is,\lim\limits_{n\to\infty}\frac{\Gamma(n+1)\Gamma(\alpha-n+1)}{n^{\alpha+1} },"Let $\alpha\in\mathbb{R}$  then, I would like to compute    $$\lim_{n\to\infty}\frac{\Gamma(n+1)\Gamma(\alpha-n+1)}{n^{\alpha+1} }$$   where $\Gamma$ is the standard gamma function.","Let $\alpha\in\mathbb{R}$  then, I would like to compute    $$\lim_{n\to\infty}\frac{\Gamma(n+1)\Gamma(\alpha-n+1)}{n^{\alpha+1} }$$   where $\Gamma$ is the standard gamma function.",,"['real-analysis', 'limits', 'asymptotics', 'gamma-function']"
40,"What is the significance of the ""0<"" in the definition of limits?","What is the significance of the ""0<"" in the definition of limits?",,"$∀ε>0,∃δ>0 $ s.t. $0< |x-a|<δ⇒|f(x)-L|<ε$ In the part where it says: ""$0< |x-a|<δ$"", what is the purpose of ""$0<$""? What if it was not there? How would that change the definition?","$∀ε>0,∃δ>0 $ s.t. $0< |x-a|<δ⇒|f(x)-L|<ε$ In the part where it says: ""$0< |x-a|<δ$"", what is the purpose of ""$0<$""? What if it was not there? How would that change the definition?",,"['limits', 'epsilon-delta']"
41,Evaluate $\lim_{n \to \infty} \sqrt{n}x_n$ with $x_{n+1}=x_{n} (1-x_{n}^2)$,Evaluate  with,\lim_{n \to \infty} \sqrt{n}x_n x_{n+1}=x_{n} (1-x_{n}^2),"Question: Let $(x_n)_{n\geq1}$ be a sequence such that $x_1 \in (0,1)$ and $$x_{n+1}=x_{n} (1-x_{n}^2)\tag{1}$$ for $n \geq 1$.  Evaluate $\lim_{n \to \infty} \sqrt{n}x_n$. The Hint Given: Fine upper and lower bound My attempt: Working backwards, the sequence is stationary at $x_n = 1/\sqrt{2}$, so I suspect that to be the limit as well. But I'm not able to find any upper or lower bound.  I'm thinking of an upper bound $$x_n < \frac{1}{\sqrt{2n+2}},\tag{2}$$ but I'm struggling to show this by induction. Any hints is appreciated","Question: Let $(x_n)_{n\geq1}$ be a sequence such that $x_1 \in (0,1)$ and $$x_{n+1}=x_{n} (1-x_{n}^2)\tag{1}$$ for $n \geq 1$.  Evaluate $\lim_{n \to \infty} \sqrt{n}x_n$. The Hint Given: Fine upper and lower bound My attempt: Working backwards, the sequence is stationary at $x_n = 1/\sqrt{2}$, so I suspect that to be the limit as well. But I'm not able to find any upper or lower bound.  I'm thinking of an upper bound $$x_n < \frac{1}{\sqrt{2n+2}},\tag{2}$$ but I'm struggling to show this by induction. Any hints is appreciated",,"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
42,Epsilon-Delta Proof limit $\frac{3x^2y}{ x^2 + y^2}$,Epsilon-Delta Proof limit,\frac{3x^2y}{ x^2 + y^2},"I have just learned about the Epsilon-Delta definition of a limit. I understand that if a function has a limit, then given any $ \epsilon > 0$, there is a $\delta > 0$, such that for all $x$ within $ \delta $ of $c$, $f(x)$ is within $\epsilon of L$. In other words, $|x - c| < \delta \rightarrow |f(x) - L| < \epsilon$ The task in my textbook was to find the limit of $3x^2y / (x^2 + y^2)$ as $(x,y) \rightarrow 0$. To prove that the limit of this function is $0$, we need to find $\delta > 0$ that confirms the inequalities specified above. As I set out to answer the question, I wrote down $0 < |x - 0| < \delta \rightarrow 3x^2y / (x^2 + y^2) - 0< \epsilon $ However, the book began with $0 < \sqrt{x^2 + y^2} < \delta \rightarrow 3x^2y / (x^2 + y^2) - 0< \epsilon $ Where do they get $\sqrt{x^2 + y^2}$ ? Please explain in the simplest way you can - I am very, very new to this!","I have just learned about the Epsilon-Delta definition of a limit. I understand that if a function has a limit, then given any $ \epsilon > 0$, there is a $\delta > 0$, such that for all $x$ within $ \delta $ of $c$, $f(x)$ is within $\epsilon of L$. In other words, $|x - c| < \delta \rightarrow |f(x) - L| < \epsilon$ The task in my textbook was to find the limit of $3x^2y / (x^2 + y^2)$ as $(x,y) \rightarrow 0$. To prove that the limit of this function is $0$, we need to find $\delta > 0$ that confirms the inequalities specified above. As I set out to answer the question, I wrote down $0 < |x - 0| < \delta \rightarrow 3x^2y / (x^2 + y^2) - 0< \epsilon $ However, the book began with $0 < \sqrt{x^2 + y^2} < \delta \rightarrow 3x^2y / (x^2 + y^2) - 0< \epsilon $ Where do they get $\sqrt{x^2 + y^2}$ ? Please explain in the simplest way you can - I am very, very new to this!",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
43,How can e^x seemingly approach 0+ as x approaches negative infinity?,How can e^x seemingly approach 0+ as x approaches negative infinity?,,"The specific problem with regards to this graph is: $$\lim_{x \to -\infty} g(2 + e^x)$$ Now it's true that we technically can't apply ""the limit of a sum is the sum of the limits"" since the function is not continuous. But to me, this intuitively seemed like the solution would just be: $$ g(2 + 0) $$ $$ = g(2) $$ $$ = 0.5 $$ In hindsight, this wass grounded on foolish assumptions. But even if my approach isn't mathematically sound, I don't get why the provided approach on the answer key should be sound: $$ u = 2 + e^x $$ $$ as\ x\to - \infty, u\to2^+ $$ $$ \lim_{u \to 2+} g(u) = -2 $$ I understand that, when you look at the graph of $e^x$ as it approaches negative infinity, it gradually diminishes towards zero, so I somewhat understand how the value it approaches is sort of like approaching $0$ from the right ( $0^+$ ) -- but to me, this provided solution looks like it's defying the fact that $ \lim_{x \to -\infty} e^x = 0 $ , and it seems really alien to me. One thing I'm wondering is that things are different since we're working inside of the function $g$ , but I'm unable to find any such rule or explanation for this.","The specific problem with regards to this graph is: Now it's true that we technically can't apply ""the limit of a sum is the sum of the limits"" since the function is not continuous. But to me, this intuitively seemed like the solution would just be: In hindsight, this wass grounded on foolish assumptions. But even if my approach isn't mathematically sound, I don't get why the provided approach on the answer key should be sound: I understand that, when you look at the graph of as it approaches negative infinity, it gradually diminishes towards zero, so I somewhat understand how the value it approaches is sort of like approaching from the right ( ) -- but to me, this provided solution looks like it's defying the fact that , and it seems really alien to me. One thing I'm wondering is that things are different since we're working inside of the function , but I'm unable to find any such rule or explanation for this.","\lim_{x \to -\infty} g(2 + e^x)  g(2 + 0)   = g(2)   = 0.5   u = 2 + e^x   as\ x\to - \infty, u\to2^+   \lim_{u \to 2+} g(u) = -2  e^x 0 0^+  \lim_{x \to -\infty} e^x = 0  g","['limits', 'limits-without-lhopital']"
44,Switching order of limits,Switching order of limits,,"I'm stuck on the following exercise (from T.Tao's Analysis 2 book): ""Let $f\colon\mathbb{R}^2\to\mathbb{R}$ be a function from $\mathbb{R}^2$ to $\mathbb{R}$ and $(x_0,y_0)$ be a point in $\mathbb{R}^2$. If $f$ is continuous at $(x_0,y_0)$, show that $\lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=\lim_{y\to y_0}\limsup_{x\to x_0}f(x,y)=f(x_0,y_0)$ and $\lim_{x\to x_0}\liminf_{y\to y_0}f(x,y)=\lim_{y\to y_0}\liminf_{x\to x_0}f(x,y)=f(x_0,y_0)$"". (note: $\limsup_{x\to x_0}f(x):=\inf_{r>0}\sup_{|x-x_0|<r}f(x)$ and $\liminf_{x\to x_0}f(x):=\sup_{r>0}\inf_{|x-x_0|<r}f(x)$) I've first tried to use the hypothesis that $f$ is continuous but I haven't got anywhere so I tried to argue by contradiction but this approach too didn't led me anywhere and now I'm stuck. Any hints?","I'm stuck on the following exercise (from T.Tao's Analysis 2 book): ""Let $f\colon\mathbb{R}^2\to\mathbb{R}$ be a function from $\mathbb{R}^2$ to $\mathbb{R}$ and $(x_0,y_0)$ be a point in $\mathbb{R}^2$. If $f$ is continuous at $(x_0,y_0)$, show that $\lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=\lim_{y\to y_0}\limsup_{x\to x_0}f(x,y)=f(x_0,y_0)$ and $\lim_{x\to x_0}\liminf_{y\to y_0}f(x,y)=\lim_{y\to y_0}\liminf_{x\to x_0}f(x,y)=f(x_0,y_0)$"". (note: $\limsup_{x\to x_0}f(x):=\inf_{r>0}\sup_{|x-x_0|<r}f(x)$ and $\liminf_{x\to x_0}f(x):=\sup_{r>0}\inf_{|x-x_0|<r}f(x)$) I've first tried to use the hypothesis that $f$ is continuous but I haven't got anywhere so I tried to argue by contradiction but this approach too didn't led me anywhere and now I'm stuck. Any hints?",,"['real-analysis', 'limits', 'limsup-and-liminf']"
45,$(x_{n})$ is a real sequence with $x_{n}\ne0$. Assume $\lim\frac{x_{n+1}}{x_{n}}=\ell$. Show $|\ell|<1\implies\lim{x_{n}}=0$.,is a real sequence with . Assume . Show .,(x_{n}) x_{n}\ne0 \lim\frac{x_{n+1}}{x_{n}}=\ell |\ell|<1\implies\lim{x_{n}}=0,"The book's (A Problem Book in Real Analysis by Aksoy and Khamsi) solution goes like this: $\lim\frac{x_{n+1}}{x_{n}}=\ell\implies\lim\left|\frac{x_{n+1}}{x_{n}}\right|=|\ell|$. So given $\varepsilon=\frac{1-|\ell|}{2}$, there exists $N_{0}\ge1$ such that for any $n\ge N_{0}$ we have $$\left| \frac{|x_{n+1}|}{x_{n}}-|\ell|\right|<\varepsilon\\\implies|\ell|-\varepsilon<\frac{|x_{n+1}|}{|x_{n}|}<|\ell|+\varepsilon$$ Then by definition of $\varepsilon$ we have $$\frac{|x_{n+1}|}{|x_{n}|}<\frac{|\ell|+1}{2}<1 \ \ \ \ \ \ \ (*)$$ For any $n\ge N_{0}$ $$|x_{n+1}|<\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}| \ \ \ \ \ \ \ (**)$$ Then $$\lim\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}=0\implies\lim|x_{n}|=0\implies\lim x_{n}=0$$ What I don't get: 1) How did they come up with $\varepsilon=\frac{1-|\ell|}{2}$? By $(*)$ it looks like they wanted something that when added to $|\ell|$ is $<1$, but I don't know why. 2) By multiplying $(*)$ by $|x_{n}|$ I see you can get something that looks like $(**)$, but how do you know $\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}|$ won't be smaller than $|x_{n+1}|$? How they did they know to raise $\frac{|\ell|+1}{2}$ to the power $n-N_{0}+1$? 3) Why does $0\le |x_{n+1}|<\text{sequence that converges to 0}$ allow us to conclude $\lim|x_{n}|=0$? How did they even know $\lim|{x_{n}}|$ exists? Thanks in advance for any help.","The book's (A Problem Book in Real Analysis by Aksoy and Khamsi) solution goes like this: $\lim\frac{x_{n+1}}{x_{n}}=\ell\implies\lim\left|\frac{x_{n+1}}{x_{n}}\right|=|\ell|$. So given $\varepsilon=\frac{1-|\ell|}{2}$, there exists $N_{0}\ge1$ such that for any $n\ge N_{0}$ we have $$\left| \frac{|x_{n+1}|}{x_{n}}-|\ell|\right|<\varepsilon\\\implies|\ell|-\varepsilon<\frac{|x_{n+1}|}{|x_{n}|}<|\ell|+\varepsilon$$ Then by definition of $\varepsilon$ we have $$\frac{|x_{n+1}|}{|x_{n}|}<\frac{|\ell|+1}{2}<1 \ \ \ \ \ \ \ (*)$$ For any $n\ge N_{0}$ $$|x_{n+1}|<\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}| \ \ \ \ \ \ \ (**)$$ Then $$\lim\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}=0\implies\lim|x_{n}|=0\implies\lim x_{n}=0$$ What I don't get: 1) How did they come up with $\varepsilon=\frac{1-|\ell|}{2}$? By $(*)$ it looks like they wanted something that when added to $|\ell|$ is $<1$, but I don't know why. 2) By multiplying $(*)$ by $|x_{n}|$ I see you can get something that looks like $(**)$, but how do you know $\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}|$ won't be smaller than $|x_{n+1}|$? How they did they know to raise $\frac{|\ell|+1}{2}$ to the power $n-N_{0}+1$? 3) Why does $0\le |x_{n+1}|<\text{sequence that converges to 0}$ allow us to conclude $\lim|x_{n}|=0$? How did they even know $\lim|{x_{n}}|$ exists? Thanks in advance for any help.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
46,Why can $z^z$ take any value when $|z| \rightarrow 0$?,Why can  take any value when ?,z^z |z| \rightarrow 0,"In the real numbers the expression $x^x$ converges to 1 coming from ""any"" direction (which here means from the positive side or from the negative side). Now I feel to remember that this does not hold in the complex plane, because there the expression $z^z$ should be able to take any value (when $|z|$ goes to 0) depending on the direction/angle you're coming from. Now I tried to make sense of that, but couldn't complete the picture. I will demonstrate how far I got. Let $z = r \cdot e^{i \phi}$ then $z^z = (r \cdot e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot (e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot e^{i \phi \cdot (r \cdot e^{i \phi})}$. With this parametrization, when we look at $|z| \rightarrow 0$, this just means $r \rightarrow 0$. My intuition (and Wolframalpha) would now say that the term with $r$ as the base would approach 1 as $r$ approaches 0. But now Wolframalpha also says that the second term with $e$ as the base also approaches 1 as $r$ approaches 0, which would mean the combined limit is 1, not depending on $\phi$. This would then mean that $z^z$ does indeed also approach 1 in the complex plane from whereever we are coming (as the limit is independent of $\phi$). Now, where is the flaw? Does $z^z \rightarrow 1$ as $|z| \rightarrow 0$ for all $z \in \mathbb{C}$? Is there a flaw in my calculations? (either in an intermediate step or the reasoning at the end)","In the real numbers the expression $x^x$ converges to 1 coming from ""any"" direction (which here means from the positive side or from the negative side). Now I feel to remember that this does not hold in the complex plane, because there the expression $z^z$ should be able to take any value (when $|z|$ goes to 0) depending on the direction/angle you're coming from. Now I tried to make sense of that, but couldn't complete the picture. I will demonstrate how far I got. Let $z = r \cdot e^{i \phi}$ then $z^z = (r \cdot e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot (e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot e^{i \phi \cdot (r \cdot e^{i \phi})}$. With this parametrization, when we look at $|z| \rightarrow 0$, this just means $r \rightarrow 0$. My intuition (and Wolframalpha) would now say that the term with $r$ as the base would approach 1 as $r$ approaches 0. But now Wolframalpha also says that the second term with $e$ as the base also approaches 1 as $r$ approaches 0, which would mean the combined limit is 1, not depending on $\phi$. This would then mean that $z^z$ does indeed also approach 1 in the complex plane from whereever we are coming (as the limit is independent of $\phi$). Now, where is the flaw? Does $z^z \rightarrow 1$ as $|z| \rightarrow 0$ for all $z \in \mathbb{C}$? Is there a flaw in my calculations? (either in an intermediate step or the reasoning at the end)",,"['limits', 'complex-numbers']"
47,Finding the mistake in the limit,Finding the mistake in the limit,,"Please, help me with finding the mistake, where did i go wrong: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ I tried the squeeze theorem, and I can see that $L \in [\frac{1}{2},1]$. I found the solution, as $$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{\sqrt{n}\sqrt{n+i}}}$$ $$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{{n}\sqrt{1+\frac{i}{n}}}}$$ We can look at the limit as a way to calculate the area of a function of reals, so we have $$f(x)=\frac{1}{\sqrt{1+x}}$$ $$L=\int_{0}^1f(x)=...=2(\sqrt2-1)$$ Is there a way to do it without integration? Another way, a better way? Second part of the question: Trying another way of finding solution, I made a mistake that shows lack of fundamental understanding: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ Now we take $ln$ of both sides: $$\ln{L}=\ln\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=\lim_{n \to \infty}(\ln\frac{1}{\sqrt{n}\sqrt{n+1}}+\ln\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\ln\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}\ln{\sqrt{{(n^n)\prod_{i=1}^n(n+i)}}}$$ There are indeterminate forms here, but they are clearly diverging to infinity. $$\ln{L}=-\infty$$ $$L=0$$ Where did I go wrong in the other procedure?","Please, help me with finding the mistake, where did i go wrong: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ I tried the squeeze theorem, and I can see that $L \in [\frac{1}{2},1]$. I found the solution, as $$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{\sqrt{n}\sqrt{n+i}}}$$ $$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{{n}\sqrt{1+\frac{i}{n}}}}$$ We can look at the limit as a way to calculate the area of a function of reals, so we have $$f(x)=\frac{1}{\sqrt{1+x}}$$ $$L=\int_{0}^1f(x)=...=2(\sqrt2-1)$$ Is there a way to do it without integration? Another way, a better way? Second part of the question: Trying another way of finding solution, I made a mistake that shows lack of fundamental understanding: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ Now we take $ln$ of both sides: $$\ln{L}=\ln\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=\lim_{n \to \infty}(\ln\frac{1}{\sqrt{n}\sqrt{n+1}}+\ln\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\ln\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}\ln{\sqrt{{(n^n)\prod_{i=1}^n(n+i)}}}$$ There are indeterminate forms here, but they are clearly diverging to infinity. $$\ln{L}=-\infty$$ $$L=0$$ Where did I go wrong in the other procedure?",,['limits']
48,Problem involving limit and sum,Problem involving limit and sum,,"Let $a_n$ be a sequence satisfying $$\sum_{n=1}^\infty \left(na_n-\frac{n^2+1}{2n+1} \right)=3$$ Compute $\lim_{n\rightarrow\infty}(a_n^2+2a_n+2)$ . I have tried $(\lim_{n\rightarrow\infty}(a_n))^2+2\lim_{n\rightarrow\infty}(a_n)+2$ using several limit properities, but what can I do further?","Let be a sequence satisfying Compute . I have tried using several limit properities, but what can I do further?",a_n \sum_{n=1}^\infty \left(na_n-\frac{n^2+1}{2n+1} \right)=3 \lim_{n\rightarrow\infty}(a_n^2+2a_n+2) (\lim_{n\rightarrow\infty}(a_n))^2+2\lim_{n\rightarrow\infty}(a_n)+2,"['limits', 'summation']"
49,Unlikely equation involving combinations,Unlikely equation involving combinations,,"I was playing with my Python implementation of some combinatorial functions, specifically those that return the number of combinations of $r$ elements from a set of cardinality $n$: Without repetition: $\frac{n!}{r!(n-r)!}$ With repetition: $\frac{(n+r-1)!}{r!(n-1)!}$ For convenience, I will refer to the former function as $C(n, r)$, and the latter as $Cr(n, r)$. While doing this, I found something fascinating: $$\lim_{a\to\infty} {Cr(a^2, a) \over C(a^2, a)} = e$$ Those who know Python can verify the correctness of my implementation of the functions: def permutation_count(n, r, repetitions=False):     """"""Returns the number of permutations of 'r' elements from a set of     cardinality 'n'.""""""     if repetitions:         return n**r     else:         return reduce(lambda x, y: x * y, range(n, n - r, -1))  def combination_count(n, r, repetitions=False):     """"""Returns the number of combinations of 'r' elements from a set of     cardinality 'n'.""""""     if repetitions:         n = n + r - 1     return permutation_count(n, r) // factorial(r)  # My function: def f(a):     return combination_count(a**2, a, True) / combination_count(a**2, a, False) I observed the above result by simply evaluating this function for increasingly large integer values. I will send any who request it a text file containing the results of the function for said values. I posted this here for these reasons: 1. There's the possibility, however small, that I am the first to discover this equation. 2. I enjoy math and I'm fairly knowledgeable regarding it, but I'm definitely not an expert, and I have no idea what the significance of this discovery is. Someone more knowledgeable than myself might consider it trivial, as it may logically proceed from some known mathematical property. 3. If this discovery is in fact significant, it could be useful to those more knowledgeable than myself. I would also like to understand why it is significant, in that case.","I was playing with my Python implementation of some combinatorial functions, specifically those that return the number of combinations of $r$ elements from a set of cardinality $n$: Without repetition: $\frac{n!}{r!(n-r)!}$ With repetition: $\frac{(n+r-1)!}{r!(n-1)!}$ For convenience, I will refer to the former function as $C(n, r)$, and the latter as $Cr(n, r)$. While doing this, I found something fascinating: $$\lim_{a\to\infty} {Cr(a^2, a) \over C(a^2, a)} = e$$ Those who know Python can verify the correctness of my implementation of the functions: def permutation_count(n, r, repetitions=False):     """"""Returns the number of permutations of 'r' elements from a set of     cardinality 'n'.""""""     if repetitions:         return n**r     else:         return reduce(lambda x, y: x * y, range(n, n - r, -1))  def combination_count(n, r, repetitions=False):     """"""Returns the number of combinations of 'r' elements from a set of     cardinality 'n'.""""""     if repetitions:         n = n + r - 1     return permutation_count(n, r) // factorial(r)  # My function: def f(a):     return combination_count(a**2, a, True) / combination_count(a**2, a, False) I observed the above result by simply evaluating this function for increasingly large integer values. I will send any who request it a text file containing the results of the function for said values. I posted this here for these reasons: 1. There's the possibility, however small, that I am the first to discover this equation. 2. I enjoy math and I'm fairly knowledgeable regarding it, but I'm definitely not an expert, and I have no idea what the significance of this discovery is. Someone more knowledgeable than myself might consider it trivial, as it may logically proceed from some known mathematical property. 3. If this discovery is in fact significant, it could be useful to those more knowledgeable than myself. I would also like to understand why it is significant, in that case.",,"['combinatorics', 'limits', 'exponential-function']"
50,Give me some hints in calculation this limit.,Give me some hints in calculation this limit.,,$$\lim_{x\to2\  \\ y\to2}\frac{x^{6}+ \tan (x^{2}-y^{2}) - y^{6}}{\sin(x^{6}-y^{6}) - x^{5}y +xy^{5} + \arctan(x^{2}y -xy^{2})}$$ I used a fact that $$\tan \alpha \sim \alpha \\ \arctan \alpha \sim \alpha \\  \sin\alpha \sim \alpha$$ Since now we have $$\lim_{x\to2\ \\ y\to2}\frac{x^{6}+ x^{2}-y^{2} - y^{6}}{x^{6}-y^{6} - x^{5}y +xy^{5} + x^{2}y -xy^{2}}$$ Then $$\lim_{x\to2\ y\to2}\frac{x^{6}+ x^{2}-y^{2} - y^{6}}{x^{6}-y^{6} - x^{5}y +xy^{5} + x^{2}y -xy^{2}}=\\ \\  =\lim_{x\to2\ \\ y\to2}\frac{(x+ y)(x-y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})+(x+y)(x-y)}   {(x+ y)(x-y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})-xy(x^{2}-y^{2})(x^{2}+y^{2}) - xy(x-y)}=\\=\lim_{x\to2\ \\y\to2}\frac{(x+ y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})+(x+y)}   {(x+ y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})-xy(x+y)(x^{2}+y^{2}) - xy} $$  And what to do next what multipliers to group? Help please.,$$\lim_{x\to2\  \\ y\to2}\frac{x^{6}+ \tan (x^{2}-y^{2}) - y^{6}}{\sin(x^{6}-y^{6}) - x^{5}y +xy^{5} + \arctan(x^{2}y -xy^{2})}$$ I used a fact that $$\tan \alpha \sim \alpha \\ \arctan \alpha \sim \alpha \\  \sin\alpha \sim \alpha$$ Since now we have $$\lim_{x\to2\ \\ y\to2}\frac{x^{6}+ x^{2}-y^{2} - y^{6}}{x^{6}-y^{6} - x^{5}y +xy^{5} + x^{2}y -xy^{2}}$$ Then $$\lim_{x\to2\ y\to2}\frac{x^{6}+ x^{2}-y^{2} - y^{6}}{x^{6}-y^{6} - x^{5}y +xy^{5} + x^{2}y -xy^{2}}=\\ \\  =\lim_{x\to2\ \\ y\to2}\frac{(x+ y)(x-y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})+(x+y)(x-y)}   {(x+ y)(x-y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})-xy(x^{2}-y^{2})(x^{2}+y^{2}) - xy(x-y)}=\\=\lim_{x\to2\ \\y\to2}\frac{(x+ y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})+(x+y)}   {(x+ y)(x^{2}-xy +y^{2})(x^{2}+xy +y^{2})-xy(x+y)(x^{2}+y^{2}) - xy} $$  And what to do next what multipliers to group? Help please.,,"['limits', 'limits-without-lhopital']"
51,How to represent this limit:$\lim\limits_{n\to\infty}\left(\frac{n^n}{n!}\prod_{k=1}^n\frac{x+\frac{n}{k}}{x^2+\frac{n^2}{k^2}}\right)^{\frac{x}{n}}$?,How to represent this limit:?,\lim\limits_{n\to\infty}\left(\frac{n^n}{n!}\prod_{k=1}^n\frac{x+\frac{n}{k}}{x^2+\frac{n^2}{k^2}}\right)^{\frac{x}{n}},"$$f(x)= \lim_{n\rightarrow\infty} \left(\dfrac{n^n(x+n)(x+\dfrac{n}{2})\cdots(x+\dfrac{n}{n})}{n!(x^2+n^2)(x^2+\dfrac{n^2}{2^2})\cdots(x^2+\dfrac{n^2}{n^2})}\right)^{x/n}  , \quad x>0$$ How can I represent this limit in a simple form? I tried that above fomula $\left(\dfrac{\prod\limits_{k=1}^n \left(\dfrac{kx}{n}+1\right)} {\prod\limits_{k=1}^n \left(\dfrac{k^2 x^2}{n^2}+1\right)}\right)^{x/n}$ help me.",How can I represent this limit in a simple form? I tried that above fomula help me.,"f(x)= \lim_{n\rightarrow\infty} \left(\dfrac{n^n(x+n)(x+\dfrac{n}{2})\cdots(x+\dfrac{n}{n})}{n!(x^2+n^2)(x^2+\dfrac{n^2}{2^2})\cdots(x^2+\dfrac{n^2}{n^2})}\right)^{x/n}
 , \quad x>0 \left(\dfrac{\prod\limits_{k=1}^n \left(\dfrac{kx}{n}+1\right)} {\prod\limits_{k=1}^n \left(\dfrac{k^2 x^2}{n^2}+1\right)}\right)^{x/n}","['real-analysis', 'calculus', 'limits']"
52,How to find $\lim\limits_{x \to 8} \frac{\frac{1}{\sqrt{x +1}} - \frac 13}{x-8}$,How to find,\lim\limits_{x \to 8} \frac{\frac{1}{\sqrt{x +1}} - \frac 13}{x-8},I am trying to find the limit as $x\to 8$ of the following function. What follows is the function and then the work I've done on it. $$ \lim_{x\to 8}\frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8}$$ \begin{align}\frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8} &= \frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8} \times \frac{\frac{1}{\sqrt{x +1}} + \frac{1}{3}}{\frac{1}{\sqrt{x +1}} + \frac{1}{3}} \\\\ & = \frac{\frac{1}{x+1}-\frac{1}{9}}{(x-8)\left(\frac{1}{\sqrt{x +1}} + \frac{1}{3}\right)}\\\\ & = \frac{8-x}{(x-8)\left(\frac{1}{\sqrt{x +1}} + \frac{1}{3}\right)}\\\\ & = \frac {-1}{\frac{1}{\sqrt{x +1}} + \frac{1}{3}}\end{align} At this point I try direct substitution and get: $$ = \frac{-1}{\frac{2}{3}}$$ This is not the answer. Could someone please help me figure out where I've gone wrong?,I am trying to find the limit as $x\to 8$ of the following function. What follows is the function and then the work I've done on it. $$ \lim_{x\to 8}\frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8}$$ \begin{align}\frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8} &= \frac{\frac{1}{\sqrt{x +1}} - \frac{1}{3}} {x-8} \times \frac{\frac{1}{\sqrt{x +1}} + \frac{1}{3}}{\frac{1}{\sqrt{x +1}} + \frac{1}{3}} \\\\ & = \frac{\frac{1}{x+1}-\frac{1}{9}}{(x-8)\left(\frac{1}{\sqrt{x +1}} + \frac{1}{3}\right)}\\\\ & = \frac{8-x}{(x-8)\left(\frac{1}{\sqrt{x +1}} + \frac{1}{3}\right)}\\\\ & = \frac {-1}{\frac{1}{\sqrt{x +1}} + \frac{1}{3}}\end{align} At this point I try direct substitution and get: $$ = \frac{-1}{\frac{2}{3}}$$ This is not the answer. Could someone please help me figure out where I've gone wrong?,,"['calculus', 'limits']"
53,Prove that a differentiable function f with $f'(x) \geq f(x)^2$ is non positive and has limit $0$,Prove that a differentiable function f with  is non positive and has limit,f'(x) \geq f(x)^2 0,"Let $f:(0,\infty) \to \mathbb{R}$, with $f'(x) \geq f(x)^2$, be a differentiable function. I must prove that $f(x)\leq 0$ and $\lim_{x \to \infty}f(x) = 0$ As $f'(x) \geq f(x)^2 \geq 0$, $f$ is non decreasing. So, if I have $\lim_{x \to \infty} = 0$, it is easy to prove that $f(x) \leq 0$. But I'm stuck on proving $\lim_{x \to \infty} = 0$ I'd appreciate some help.","Let $f:(0,\infty) \to \mathbb{R}$, with $f'(x) \geq f(x)^2$, be a differentiable function. I must prove that $f(x)\leq 0$ and $\lim_{x \to \infty}f(x) = 0$ As $f'(x) \geq f(x)^2 \geq 0$, $f$ is non decreasing. So, if I have $\lim_{x \to \infty} = 0$, it is easy to prove that $f(x) \leq 0$. But I'm stuck on proving $\lim_{x \to \infty} = 0$ I'd appreciate some help.",,"['real-analysis', 'limits', 'derivatives']"
54,Paradox: Two completely different values for $\lim_{n\to\infty} \left(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}\right)$,Paradox: Two completely different values for,\lim_{n\to\infty} \left(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}\right),"I am asked to evaluate the following question: $$\lim_{n\to\infty} \left(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}\right)$$ I did the following: Using the property: Limit of a sum is the sum of limits, I take on each term separately. $$\lim_{n\to\infty}(\frac{1}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{1}{n^2}}{\frac{1}{n^2}-1}) = \frac{0}{-1} = 0.$$ Similarly each term evaluates to $0$ despite the dependence on n: $$\lim_{n\to\infty}(\frac{n}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{n}{n^2}}{\frac{1}{n^2}-1}) = 0.$$ I checked the answer and it said -$\frac{1}{2}.$ I checked the complete solution: $$\lim_{n\to\infty}(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}) = \lim_{n\to\infty}(\frac{1+2+...+n}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{n(n-1)}{2}}{1-n^2}) = -\frac12.$$ Obviously, only one answer is ryt [hoping that the limit of an expression is a uniques number]. Both the deductions seem quite logical to me. The only plausible mistake i have noticed is $1+2+...+n = \frac{n(n-1)}{2}$ is only true for $n$ $\in$ $\Bbb Z$. Solve the paradox.","I am asked to evaluate the following question: $$\lim_{n\to\infty} \left(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}\right)$$ I did the following: Using the property: Limit of a sum is the sum of limits, I take on each term separately. $$\lim_{n\to\infty}(\frac{1}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{1}{n^2}}{\frac{1}{n^2}-1}) = \frac{0}{-1} = 0.$$ Similarly each term evaluates to $0$ despite the dependence on n: $$\lim_{n\to\infty}(\frac{n}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{n}{n^2}}{\frac{1}{n^2}-1}) = 0.$$ I checked the answer and it said -$\frac{1}{2}.$ I checked the complete solution: $$\lim_{n\to\infty}(\frac{1}{1-n^2}+\frac{2}{1-n^2}+...+\frac{n}{1-n^2}) = \lim_{n\to\infty}(\frac{1+2+...+n}{1-n^2}) = \lim_{n\to\infty}(\frac{\frac{n(n-1)}{2}}{1-n^2}) = -\frac12.$$ Obviously, only one answer is ryt [hoping that the limit of an expression is a uniques number]. Both the deductions seem quite logical to me. The only plausible mistake i have noticed is $1+2+...+n = \frac{n(n-1)}{2}$ is only true for $n$ $\in$ $\Bbb Z$. Solve the paradox.",,"['sequences-and-series', 'limits', 'fake-proofs', 'paradoxes']"
55,How to evaluate this limit??,How to evaluate this limit??,,Evaluate: $$\ f(x)= \lim_{n\rightarrow \infty}\left( \dfrac{n^n(x+n)\left( x+\dfrac{n}{2}\right)\left( x+\dfrac{n}{3}\right)... \left( x+\dfrac{n}{n}\right)}{n!(x^2+n^2)\left( x^2+\dfrac{n^2}{4}\right)\left( x^2+\dfrac{n^2}{9}\right)...\left( x^2+\dfrac{n^2}{n^2}\right)}\right)^{\dfrac{x}{n}}$$ $x\in R^+$ Find the coordinates of the maxima of $f(x)$ .,Evaluate: Find the coordinates of the maxima of .,\ f(x)= \lim_{n\rightarrow \infty}\left( \dfrac{n^n(x+n)\left( x+\dfrac{n}{2}\right)\left( x+\dfrac{n}{3}\right)... \left( x+\dfrac{n}{n}\right)}{n!(x^2+n^2)\left( x^2+\dfrac{n^2}{4}\right)\left( x^2+\dfrac{n^2}{9}\right)...\left( x^2+\dfrac{n^2}{n^2}\right)}\right)^{\dfrac{x}{n}} x\in R^+ f(x),"['calculus', 'integration', 'limits', 'derivatives', 'riemann-sum']"
56,Finding sum $\frac{1}{2} x^2 + \frac{1}{2} \frac{1}{3} x^3 + \frac{1}{4} \frac{1}{3} x^4 + ... $,Finding sum,\frac{1}{2} x^2 + \frac{1}{2} \frac{1}{3} x^3 + \frac{1}{4} \frac{1}{3} x^4 + ... ,"I need to find following: for $0 < x < 1$ $$\frac{1}{2} x^2 + \frac{1}{2} \frac{1}{3} x^3 + \frac{1}{4} \frac{1}{3} x^4  + ... $$ My attempt: I can see that the sum is composed of two infinite sums, one is $\sum_{n=1}^{\infty} \frac{1}{n(n+1)} = \sum_{n=1}^{\infty} \left( \frac{1}{n} -  \frac{1}{n+1} \right)$ (Telescoping ) and another is, $\sum_{n=2}^{\infty} x^n$ (it's a G.P.). How can I use these for solving the sum in question? Any hints will be appreciated...","I need to find following: for $0 < x < 1$ $$\frac{1}{2} x^2 + \frac{1}{2} \frac{1}{3} x^3 + \frac{1}{4} \frac{1}{3} x^4  + ... $$ My attempt: I can see that the sum is composed of two infinite sums, one is $\sum_{n=1}^{\infty} \frac{1}{n(n+1)} = \sum_{n=1}^{\infty} \left( \frac{1}{n} -  \frac{1}{n+1} \right)$ (Telescoping ) and another is, $\sum_{n=2}^{\infty} x^n$ (it's a G.P.). How can I use these for solving the sum in question? Any hints will be appreciated...",,"['real-analysis', 'sequences-and-series', 'limits']"
57,Prove that $\lim_{k\to \infty} x_{n_{k}} = c$ for any subsequence $x_{n_{k}}$ of $(x_n)$,Prove that  for any subsequence  of,\lim_{k\to \infty} x_{n_{k}} = c x_{n_{k}} (x_n),"Question: Suppose $\lim_{n\to\infty} x_{n} = c$. Prove that $\lim_{k\to\infty} x_{n_{k}} = c$ for any subsequence $(x_{n_{k}})$ of $(x_{n})$. [Hint: $n_{k} \geq k$.] Could you please help me how can I prove it can I use the following definition ? Definition: Let $(n_{k})_{k=1}^{\infty}$ be a strictly increasing sequence of positive integers; that is, $n_1 < n_2 < n_3 < \dotsb$ . If $(x_n)_{n=1}^{\infty}$ is a sequence, then $(x_{n_{k}})_{k=1}^{\infty} = (x_{n_{1}},x_{n_{2}},x_{n_{3}},\dotsc)$ is called a subsequence of $(x_n)$.","Question: Suppose $\lim_{n\to\infty} x_{n} = c$. Prove that $\lim_{k\to\infty} x_{n_{k}} = c$ for any subsequence $(x_{n_{k}})$ of $(x_{n})$. [Hint: $n_{k} \geq k$.] Could you please help me how can I prove it can I use the following definition ? Definition: Let $(n_{k})_{k=1}^{\infty}$ be a strictly increasing sequence of positive integers; that is, $n_1 < n_2 < n_3 < \dotsb$ . If $(x_n)_{n=1}^{\infty}$ is a sequence, then $(x_{n_{k}})_{k=1}^{\infty} = (x_{n_{1}},x_{n_{2}},x_{n_{3}},\dotsc)$ is called a subsequence of $(x_n)$.",,"['calculus', 'sequences-and-series', 'analysis', 'limits', 'convergence-divergence']"
58,Prove $\liminf (a_n) \leq \limsup (a_n)$,Prove,\liminf (a_n) \leq \limsup (a_n),"I've been working on this for a bit and haven't got far :/ So the full question is: Suppose $(a_n)_{n=1}^{\infty}$ is a bounded sequence of real numbers. Prove that $$\liminf (a_n) \leq \lim\sup (a_n)$$ (Hint: apply the result of the previous question to a difference of two sequences.) So the previous question was basically: $(a_n)_{n=1}^{\infty}$ is a convergent sequence and $a_n \in [0, \infty) \forall n$ . Prove the limit lies in $[0, \infty)$ . How I solved it was fixed $\epsilon > 0$ then $\exists$ $N \in \mathbb{N}$ $\forall$ $n\geq N$ I let the limit $=a$ . $\therefore$ $|a_n - a| < \epsilon$ Since $a_n$ took finitely many values $\implies$ $M_1 = \max\{|a_1|,...,|a_{N-1}|\}$ , let $M_2 = |a| + \epsilon$ Since it converges it's bounded by some M $\implies$ $(a_n)^{\infty}_{n=1}$ within $[0, M]$ Since $[0, M]$ is closed, it contained its limit points $\implies$ $\lim_{n\to\infty} a_n \in [0,M] \subset [0, \infty)$ . So far I've said; $(a_n)_{n=1}^{\infty} \in [\mathbb{R}]$ $M_1 = \inf\{a_n\}, M_2 = \sup\{a_n\}$ (From previous question) Therefore, $\sup\{a_n\} = |a_n| + \epsilon$ $\inf{a_n} = \max\{a_1, a_2, ..., a_{N-1}\}$ $|a_n|<M_1 \implies |a_n| < \inf\{a_n\}$ $|a_N| < |a| + \epsilon \implies |a_N|< \inf\{a_n\}$ That's all I have not sure if it actually means anything but yeah as I said any help would GREATLY be appreciated. Thanks ;)","I've been working on this for a bit and haven't got far :/ So the full question is: Suppose is a bounded sequence of real numbers. Prove that (Hint: apply the result of the previous question to a difference of two sequences.) So the previous question was basically: is a convergent sequence and . Prove the limit lies in . How I solved it was fixed then I let the limit . Since took finitely many values , let Since it converges it's bounded by some M within Since is closed, it contained its limit points . So far I've said; (From previous question) Therefore, That's all I have not sure if it actually means anything but yeah as I said any help would GREATLY be appreciated. Thanks ;)","(a_n)_{n=1}^{\infty} \liminf (a_n) \leq \lim\sup (a_n) (a_n)_{n=1}^{\infty} a_n \in [0, \infty) \forall n [0, \infty) \epsilon > 0 \exists N \in \mathbb{N} \forall n\geq N =a \therefore |a_n - a| < \epsilon a_n \implies M_1 = \max\{|a_1|,...,|a_{N-1}|\} M_2 = |a| + \epsilon \implies (a_n)^{\infty}_{n=1} [0, M] [0, M] \implies \lim_{n\to\infty} a_n \in [0,M] \subset [0, \infty) (a_n)_{n=1}^{\infty} \in [\mathbb{R}] M_1 = \inf\{a_n\}, M_2 = \sup\{a_n\} \sup\{a_n\} = |a_n| + \epsilon \inf{a_n} = \max\{a_1, a_2, ..., a_{N-1}\} |a_n|<M_1 \implies |a_n| < \inf\{a_n\} |a_N| < |a| + \epsilon \implies |a_N|< \inf\{a_n\}","['sequences-and-series', 'limits']"
59,L'hopitals rule with limits,L'hopitals rule with limits,,"Given the following limit, $$\begin{align} \lim_{x\to 0}\frac{e^{-1/x^2}-0}{x-0}\\\\ & \end{align}$$ How do I calculate it? when pluggin in 0 I would get $\frac{0}{0}$","Given the following limit, $$\begin{align} \lim_{x\to 0}\frac{e^{-1/x^2}-0}{x-0}\\\\ & \end{align}$$ How do I calculate it? when pluggin in 0 I would get $\frac{0}{0}$",,"['calculus', 'limits']"
60,Evaluation of the limit $\lim_{n\to\infty}\sum_{k=1}^n\frac k{k^2+n^2}$ [duplicate],Evaluation of the limit  [duplicate],\lim_{n\to\infty}\sum_{k=1}^n\frac k{k^2+n^2},"This question already has answers here : How do you calculate this limit $\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{n^2+k^2}$? (3 answers) Closed 7 years ago . $$\lim_{n\to\infty}\sum_{k=1}^n\frac k{k^2+n^2}$$ I need to evaluate this limit, but I don't know how to start. Should i take $1/n^2$ out?  Help required. Thank you.","This question already has answers here : How do you calculate this limit $\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{n^2+k^2}$? (3 answers) Closed 7 years ago . $$\lim_{n\to\infty}\sum_{k=1}^n\frac k{k^2+n^2}$$ I need to evaluate this limit, but I don't know how to start. Should i take $1/n^2$ out?  Help required. Thank you.",,"['calculus', 'limits', 'summation', 'infinity']"
61,Interesting limit with log function,Interesting limit with log function,,Compute the following limit:$$ \lim _{n \to \infty} \frac{1}{n}\sum_{k=0}^{n-1}\left[\frac{k+1}{n}-\frac{1}{2}\right]\log(2n-2k-1) .$$ I do not know how to start as I am new in this subject. Can you help me?,Compute the following limit:$$ \lim _{n \to \infty} \frac{1}{n}\sum_{k=0}^{n-1}\left[\frac{k+1}{n}-\frac{1}{2}\right]\log(2n-2k-1) .$$ I do not know how to start as I am new in this subject. Can you help me?,,"['sequences-and-series', 'limits']"
62,Proving the limit of $\frac{1}{n^2+n}$ = 0 using the $\epsilon$ - N definition,Proving the limit of  = 0 using the  - N definition,\frac{1}{n^2+n} \epsilon,"So, I'm stuck on this question, and have been working on it for a few hours, probably because I'm not 100% in understanding the definition. But here's the question: Using the $\epsilon$ - N definition of the limit prove that, $\lim_{x \to ∞}{1\over(n^2+n)}$ = $0$ (sorry I don't know how to write fractions in limits). In other words, given $\epsilon$ > $0$, find explicitly a natural number N which satisfies the statement in the definition of the limit. So the definition I received in lectures is: Let $(a_n)_1^∞$ ($1$ should be $n=1$, I couldn't get it to work, sorry) be a sequence of real numbers $(a_n)_1^∞$ = {$a_1, a_2, a_3,...$} . Definition: We write $\lim_{n \to ∞} a_n = a$, and say the limit of $(a_n)_1^∞$ equals $a$, if for every $\epsilon$ > 0 there exists N ∈ $\mathbb{N}$ such that if $n ≥ N$, then $|a_n - a| < \epsilon$. So far this is what I've done: |$\frac{1}{n^2+n}$ + 1| < $\epsilon$ $\therefore$ |$\frac{1-n^2+n}{n^2+n}$ + 1| < $\epsilon$ I did try other ways like disregarding the 'n' in the denominator since $n^2$ is more significant then it, but I don't think I'm supposed to do that. I'm really just stuck on what to do next as I don't really understand the process. Any help would be GREATLY appreciated, thanks! :)","So, I'm stuck on this question, and have been working on it for a few hours, probably because I'm not 100% in understanding the definition. But here's the question: Using the $\epsilon$ - N definition of the limit prove that, $\lim_{x \to ∞}{1\over(n^2+n)}$ = $0$ (sorry I don't know how to write fractions in limits). In other words, given $\epsilon$ > $0$, find explicitly a natural number N which satisfies the statement in the definition of the limit. So the definition I received in lectures is: Let $(a_n)_1^∞$ ($1$ should be $n=1$, I couldn't get it to work, sorry) be a sequence of real numbers $(a_n)_1^∞$ = {$a_1, a_2, a_3,...$} . Definition: We write $\lim_{n \to ∞} a_n = a$, and say the limit of $(a_n)_1^∞$ equals $a$, if for every $\epsilon$ > 0 there exists N ∈ $\mathbb{N}$ such that if $n ≥ N$, then $|a_n - a| < \epsilon$. So far this is what I've done: |$\frac{1}{n^2+n}$ + 1| < $\epsilon$ $\therefore$ |$\frac{1-n^2+n}{n^2+n}$ + 1| < $\epsilon$ I did try other ways like disregarding the 'n' in the denominator since $n^2$ is more significant then it, but I don't think I'm supposed to do that. I'm really just stuck on what to do next as I don't really understand the process. Any help would be GREATLY appreciated, thanks! :)",,"['sequences-and-series', 'limits', 'propositional-calculus', 'proof-explanation', 'epsilon-delta']"
63,Proof that $a_n < b_n \implies \lim(a_n) \le \lim(b_n)$,Proof that,a_n < b_n \implies \lim(a_n) \le \lim(b_n),"First of all, I'm aware that there are many questions like this on the site, but they all seem to be related to either $\limsup$ or $\liminf$ and I couldn't find anything that would help me with my problem. I've done some Googling and found some great resources, but I'm still not quite sure how to get to some steps and would like your assistance. The problem is as follows: Given $a_n < b_n$ prove that $\lim_{n\to \infty}(a_n) \le \lim_{n\to\infty}(b_n)$ . The proof is then done by contradiction, assuming that $a = \lim_{n\to \infty}(a_n) > b =\lim_{n\to\infty}(b_n)$ . We take an $\epsilon = \frac{a-b} 2$ , so that the $\epsilon$ -neighborhoods of $a$ and $b$ are disjoint. From the definition of limits, we now know that there is such a $N$ , so that $\forall n > N : |a_n-a|<\frac\epsilon2$ and $|b_n-b|<\frac\epsilon2$ . The next step is absolutely always confusing. Two variants I've found are either: $a_n>a-\epsilon=a-\left(\frac{a-b} 2\right)=b+\left(\frac{a-b} 2\right)=b+\epsilon>b_n$ In which I do not understand why any two terms of that (in)equality are like that, or it is said that if $a > b$ , there must be such an $\epsilon$ so that $a - \epsilon > b + \epsilon$ . Then, $a - \epsilon > b + \epsilon > b_n, a_n > a - \epsilon > b + \epsilon > b_n$ , which contradicts $a_n \le b_n$ . Here I simply cannot comprehend how we came to the conclusion that $a_n > a - \epsilon$ and $b + \epsilon > b_n$ . The definition of the limit uses absolute values everywhere, so surely the values depends on the signs of $a_n$ and $a$ , and $b_n$ and $b$ . Please help me understand what is it that I'm missing here. Thanks in advance!","First of all, I'm aware that there are many questions like this on the site, but they all seem to be related to either or and I couldn't find anything that would help me with my problem. I've done some Googling and found some great resources, but I'm still not quite sure how to get to some steps and would like your assistance. The problem is as follows: Given prove that . The proof is then done by contradiction, assuming that . We take an , so that the -neighborhoods of and are disjoint. From the definition of limits, we now know that there is such a , so that and . The next step is absolutely always confusing. Two variants I've found are either: In which I do not understand why any two terms of that (in)equality are like that, or it is said that if , there must be such an so that . Then, , which contradicts . Here I simply cannot comprehend how we came to the conclusion that and . The definition of the limit uses absolute values everywhere, so surely the values depends on the signs of and , and and . Please help me understand what is it that I'm missing here. Thanks in advance!","\limsup \liminf a_n < b_n \lim_{n\to \infty}(a_n) \le \lim_{n\to\infty}(b_n) a = \lim_{n\to \infty}(a_n) > b =\lim_{n\to\infty}(b_n) \epsilon = \frac{a-b} 2 \epsilon a b N \forall n > N : |a_n-a|<\frac\epsilon2 |b_n-b|<\frac\epsilon2 a_n>a-\epsilon=a-\left(\frac{a-b} 2\right)=b+\left(\frac{a-b} 2\right)=b+\epsilon>b_n a > b \epsilon a - \epsilon > b + \epsilon a - \epsilon > b + \epsilon > b_n, a_n > a - \epsilon > b + \epsilon > b_n a_n \le b_n a_n > a - \epsilon b + \epsilon > b_n a_n a b_n b","['real-analysis', 'limits', 'analysis', 'inequality']"
64,prove that lnx is continuous at 1 and at any positive real number a,prove that lnx is continuous at 1 and at any positive real number a,,"Deﬁnition :. We say that a function f is continuous at a provided that for any ε > 0, there exists a δ > 0 such that if |x−a| < δ then |f(x)−f(a)| < ε. (a) Use the deﬁnition of continuity to prove that lnx is continuous at 1. [Hint: You may want to use the fact |lnx| < ε ⇔−ε < lnx < ε to ﬁnd a δ.] (b) Use part (a) to prove that lnx is continuous at any positive real number a. [Hint: ln(x) = ln(x/a) + ln(a). This is a combination of functions which are continuous at a. Be sure to explain how you know that ln(x/a) is continuous at a.] for part a how can I find  δ that works for if |x−a| < δ then |f(x)−f(a)| < ε. and for part b how can I show that ln(x/a) is continuous at a ? please help me with part a and b","Deﬁnition :. We say that a function f is continuous at a provided that for any ε > 0, there exists a δ > 0 such that if |x−a| < δ then |f(x)−f(a)| < ε. (a) Use the deﬁnition of continuity to prove that lnx is continuous at 1. [Hint: You may want to use the fact |lnx| < ε ⇔−ε < lnx < ε to ﬁnd a δ.] (b) Use part (a) to prove that lnx is continuous at any positive real number a. [Hint: ln(x) = ln(x/a) + ln(a). This is a combination of functions which are continuous at a. Be sure to explain how you know that ln(x/a) is continuous at a.] for part a how can I find  δ that works for if |x−a| < δ then |f(x)−f(a)| < ε. and for part b how can I show that ln(x/a) is continuous at a ? please help me with part a and b",,"['calculus', 'real-analysis', 'limits', 'convergence-divergence', 'continuity']"
65,Compute the given limit,Compute the given limit,,"$$\lim_{x\rightarrow0^{+}} x^{0.7}(\ln(e^{x}  - 1))$$ Since $\ln(0)$ is undefined, I know I need to simplify that expression, as of now what I did was take $\ln(e^{x}(1- e^{-x})$. ie Take $e^{x}$ out and then use the property $\ln(ab) = \ln a + \ln b$ which still leaves me with $\ln(1 - e^{-x})$. Is there any way to simplify this expression?","$$\lim_{x\rightarrow0^{+}} x^{0.7}(\ln(e^{x}  - 1))$$ Since $\ln(0)$ is undefined, I know I need to simplify that expression, as of now what I did was take $\ln(e^{x}(1- e^{-x})$. ie Take $e^{x}$ out and then use the property $\ln(ab) = \ln a + \ln b$ which still leaves me with $\ln(1 - e^{-x})$. Is there any way to simplify this expression?",,"['calculus', 'limits']"
66,"When and When Not to Use ""Limit Arithmetic""","When and When Not to Use ""Limit Arithmetic""",,"Off the top of my head, here are examples that do and don't let you use ""limit arithmetic"" (addition in both cases). Ex. 1: $\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})}$ When solving, you cannot do this: $\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})} = \lim\limits_{x \to \infty}{\sqrt{x - a}} - \lim\limits_{x \to \infty}{\sqrt{x}}$ Ex. 2: $\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}}$ Yet, here you can do this: $\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}} = \lim\limits_{x \to 0}{(\frac{e^{ax}-1}{x} - \frac{e^{bx}-1}{x})} = \lim\limits_{x \to 0}{\frac{e^{ax}-1}{x}} - \lim\limits_{x \to 0}{\frac{e^{bx}-1}{x}}$ Why is limit arithmetic is valid in only some cases? When can you use it?","Off the top of my head, here are examples that do and don't let you use ""limit arithmetic"" (addition in both cases). Ex. 1: $\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})}$ When solving, you cannot do this: $\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})} = \lim\limits_{x \to \infty}{\sqrt{x - a}} - \lim\limits_{x \to \infty}{\sqrt{x}}$ Ex. 2: $\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}}$ Yet, here you can do this: $\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}} = \lim\limits_{x \to 0}{(\frac{e^{ax}-1}{x} - \frac{e^{bx}-1}{x})} = \lim\limits_{x \to 0}{\frac{e^{ax}-1}{x}} - \lim\limits_{x \to 0}{\frac{e^{bx}-1}{x}}$ Why is limit arithmetic is valid in only some cases? When can you use it?",,"['calculus', 'real-analysis', 'limits']"
67,Convergence in Probability where Yn= the max of n independent exponentially distributed rv's,Convergence in Probability where Yn= the max of n independent exponentially distributed rv's,,"Let $X_1, X_2, \cdots$ be independent random variables, each with exponential distribution with parameter $\lambda = 1$. For any $n ≥ 1$, let $Y_n := \max(X_1,\cdots, X_n)$. Let $0 < a < 1 < b$. Show that $P(Y_n ≤ a \log n) \to 0$ as $n \to \infty$, and $P(Y_n ≤ b \log n) \to 1$ as $n \to \infty$. Conclude that $Y_n/ \log n$ converges to $1$ in probability as $n\to \infty$. I have no idea how to approach this problem. I know the formula for convergence in probability, but this seems foreign to me.","Let $X_1, X_2, \cdots$ be independent random variables, each with exponential distribution with parameter $\lambda = 1$. For any $n ≥ 1$, let $Y_n := \max(X_1,\cdots, X_n)$. Let $0 < a < 1 < b$. Show that $P(Y_n ≤ a \log n) \to 0$ as $n \to \infty$, and $P(Y_n ≤ b \log n) \to 1$ as $n \to \infty$. Conclude that $Y_n/ \log n$ converges to $1$ in probability as $n\to \infty$. I have no idea how to approach this problem. I know the formula for convergence in probability, but this seems foreign to me.",,"['probability', 'limits', 'convergence-divergence', 'exponential-distribution']"
68,Find the limit $\lim_{x\to0}\frac{\arcsin x -x}{x^2}$,Find the limit,\lim_{x\to0}\frac{\arcsin x -x}{x^2},"How can I find $\lim_{x\to0}\frac{\arcsin x -x}{x^2}$? I've tried using the Lhopital rule and it got me here: $\lim_{x\to0}\frac{\arcsin x -x}{x^2} = \lim_{x\to0}\frac{\frac{1}{\sqrt{1-x^2}}-1}{2x} = \lim_{x\to0}\frac{x}{2\sqrt{1-x^2}\cdot (1+\sqrt{1-x^2})}$ This doesn't make life much easier, unless I could say that $\frac{x}{2\sqrt{1-x^2}\cdot (1+\sqrt{1-x^2})}$ is continuous at $x=0$.. Is there a better way to approach this?","How can I find $\lim_{x\to0}\frac{\arcsin x -x}{x^2}$? I've tried using the Lhopital rule and it got me here: $\lim_{x\to0}\frac{\arcsin x -x}{x^2} = \lim_{x\to0}\frac{\frac{1}{\sqrt{1-x^2}}-1}{2x} = \lim_{x\to0}\frac{x}{2\sqrt{1-x^2}\cdot (1+\sqrt{1-x^2})}$ This doesn't make life much easier, unless I could say that $\frac{x}{2\sqrt{1-x^2}\cdot (1+\sqrt{1-x^2})}$ is continuous at $x=0$.. Is there a better way to approach this?",,['limits']
69,How to find the limit of a function involving two variables.,How to find the limit of a function involving two variables.,,"How would I find the limit of the function $\frac{x^2-4y^2}{x+2y}$ as $(x,y) \to (2,-1)$. So far I have considered lines approaching the point from different direction like $x=-2y$ but every time I substitute it into the function I get $0/0$. Does this mean the limit doesn't exist?  Many thanks for your help","How would I find the limit of the function $\frac{x^2-4y^2}{x+2y}$ as $(x,y) \to (2,-1)$. So far I have considered lines approaching the point from different direction like $x=-2y$ but every time I substitute it into the function I get $0/0$. Does this mean the limit doesn't exist?  Many thanks for your help",,"['limits', 'multivariable-calculus']"
70,Weird limit: $\lim_{n\to\infty}\left(\frac{f(1)+f(2)+...+f(n)}{n}\right)^n$,Weird limit:,\lim_{n\to\infty}\left(\frac{f(1)+f(2)+...+f(n)}{n}\right)^n,"I couldn't find the right path for this limit to solve. $$\lim_{n\to\infty}\left(\dfrac{f(1)+f(2)+...+f(n)}{n}\right)^n$$ $$f:\mathbb{R}\to\mathbb{R}, f(x)=\sqrt[3]{x^3+3x^2+2x+1}-\sqrt[3]{x^3-x+1}$$ I know that we have the indeterminate $1^{\infty}$, but apart from that...I didn't get very far","I couldn't find the right path for this limit to solve. $$\lim_{n\to\infty}\left(\dfrac{f(1)+f(2)+...+f(n)}{n}\right)^n$$ $$f:\mathbb{R}\to\mathbb{R}, f(x)=\sqrt[3]{x^3+3x^2+2x+1}-\sqrt[3]{x^3-x+1}$$ I know that we have the indeterminate $1^{\infty}$, but apart from that...I didn't get very far",,"['limits', 'summation', 'radicals', 'square-numbers']"
71,Does f(x) is differentiable,Does f(x) is differentiable,,I need to determine whether the following function is differentiable at $x_0 \ne 0$ according to the derivative definition : $$ f(x) = x^\frac{1}{3} $$ So I started by looking at the definition but not sure how to proceed: $$ f'(x_0) =\lim_{x \to x_0} \frac{x^\frac{1}{3} - x_0^\frac{1}{3}}{x - x_0} $$ Thank you,I need to determine whether the following function is differentiable at $x_0 \ne 0$ according to the derivative definition : $$ f(x) = x^\frac{1}{3} $$ So I started by looking at the definition but not sure how to proceed: $$ f'(x_0) =\lim_{x \to x_0} \frac{x^\frac{1}{3} - x_0^\frac{1}{3}}{x - x_0} $$ Thank you,,"['limits', 'derivatives', 'limits-without-lhopital']"
72,Limit exists on a differentiable function?,Limit exists on a differentiable function?,,"Let f be a differentiable function a $x = 1$ such that $f(1) = 1$, $f'(1) = 4$. Compute the following limits, or prove that they don't exist: a. $\lim_{x\to 1} \frac{1-f(x)}{x-1}$ b. $\lim_{x\to 1} \frac{f^2(x)-f(x)}{x-1}$ I am quite divided on this. According to the definition of the derivative, I get that a = -4 and b = 4. However, if I check the limit of $\lim_{x\to 1+}$ and $\lim_{x\to 1-}$ on b, I get that the limit are different (One is positive and the other is negative). Which step am I doing wrong? What am I missing? Thanks.","Let f be a differentiable function a $x = 1$ such that $f(1) = 1$, $f'(1) = 4$. Compute the following limits, or prove that they don't exist: a. $\lim_{x\to 1} \frac{1-f(x)}{x-1}$ b. $\lim_{x\to 1} \frac{f^2(x)-f(x)}{x-1}$ I am quite divided on this. According to the definition of the derivative, I get that a = -4 and b = 4. However, if I check the limit of $\lim_{x\to 1+}$ and $\lim_{x\to 1-}$ on b, I get that the limit are different (One is positive and the other is negative). Which step am I doing wrong? What am I missing? Thanks.",,"['calculus', 'limits']"
73,Incorrect solution in limits,Incorrect solution in limits,,"$$ \lim_{n\to \infty}\left(\frac{1}{n^4}{+3^{\frac{2}{2+n}}}\right)^{n} $$ So i re-write it like: $\lim_{n\to \infty}e^{n\ln{\frac{1}{n^4}\ln3^{\frac{2}{2+n}}}}$ $=$  $e^{\frac{2n}{2+n}\ln{\frac{1}{n^4}\ln3}}=e^{{2}\ln{\frac{1}{n^4}\ln3}}$ So here, $\ln{\frac{1}{n^4}}$ give  us minus infinity, but i think that somewhere i lost a way, so i need some hints","$$ \lim_{n\to \infty}\left(\frac{1}{n^4}{+3^{\frac{2}{2+n}}}\right)^{n} $$ So i re-write it like: $\lim_{n\to \infty}e^{n\ln{\frac{1}{n^4}\ln3^{\frac{2}{2+n}}}}$ $=$  $e^{\frac{2n}{2+n}\ln{\frac{1}{n^4}\ln3}}=e^{{2}\ln{\frac{1}{n^4}\ln3}}$ So here, $\ln{\frac{1}{n^4}}$ give  us minus infinity, but i think that somewhere i lost a way, so i need some hints",,['limits']
74,Prove that $\lim_{x \to 1}(2x+3)=5$,Prove that,\lim_{x \to 1}(2x+3)=5,Please check my proof There exist $\delta $ such that $0<x<\delta \rightarrow |2x+3-5|=|2x-2|<\epsilon $ $$0<x<\delta \rightarrow 2|x-1|< \epsilon $$ $$ \rightarrow |x-1|<\frac{\epsilon }{2}$$ Choose $\delta =\frac{\epsilon }{2}$ For x is real number and  $$0<x<\delta \rightarrow |2x+3-5|=|5-5|=0<\delta =\frac{\epsilon }{2}$$ Then limit is 5,Please check my proof There exist $\delta $ such that $0<x<\delta \rightarrow |2x+3-5|=|2x-2|<\epsilon $ $$0<x<\delta \rightarrow 2|x-1|< \epsilon $$ $$ \rightarrow |x-1|<\frac{\epsilon }{2}$$ Choose $\delta =\frac{\epsilon }{2}$ For x is real number and  $$0<x<\delta \rightarrow |2x+3-5|=|5-5|=0<\delta =\frac{\epsilon }{2}$$ Then limit is 5,,"['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
75,"Is $f(x,y)=(xy)^{2/3}$ differentiable at $(0,0)$?",Is  differentiable at ?,"f(x,y)=(xy)^{2/3} (0,0)","I looked at the graph of $f(x,y)=(xy)^{2/3}$ at the origin, and no matter how far I zoom in it has four corners along $x=\pm y$, so it doesn't look like it is differentiable as $z=0$ doesn't look like a tangent plane. I also found that its partial derivatives are not continuous at $(0,0)$, e.g.  $$ \lim_{(ky^2,y) \to (0,0)} \frac{\partial f}{\partial x} = \lim_{(ky^2,y) \to (0,0)} \frac{2}{3}x^{-\frac{1}{3}}y^{\frac{2}{3}} = \frac{2}{3}k^{-\frac{1}{3}} $$ (although $\frac{\partial f}{\partial x}(0,0) = \frac{\partial f}{\partial y}(0,0) = 0$) I also tried to prove/disprove its differentiability by definition, and for it to be differentiable I need $$ \lim_{(x,y) \to (0,0)} \frac{f(x,y)-0}{\sqrt{x^2+y^2}} =0 $$ but then I got stuck. Am I even on the right track? [UPDATE] I need to do the same thing for $f(x,y)=(xy)^{\frac{1}{3}}$... I think the answer is that $(xy)^{2/3}$ is differentiable but $(xy)^{1/3}$ is not, according to my professor but I'm not sure","I looked at the graph of $f(x,y)=(xy)^{2/3}$ at the origin, and no matter how far I zoom in it has four corners along $x=\pm y$, so it doesn't look like it is differentiable as $z=0$ doesn't look like a tangent plane. I also found that its partial derivatives are not continuous at $(0,0)$, e.g.  $$ \lim_{(ky^2,y) \to (0,0)} \frac{\partial f}{\partial x} = \lim_{(ky^2,y) \to (0,0)} \frac{2}{3}x^{-\frac{1}{3}}y^{\frac{2}{3}} = \frac{2}{3}k^{-\frac{1}{3}} $$ (although $\frac{\partial f}{\partial x}(0,0) = \frac{\partial f}{\partial y}(0,0) = 0$) I also tried to prove/disprove its differentiability by definition, and for it to be differentiable I need $$ \lim_{(x,y) \to (0,0)} \frac{f(x,y)-0}{\sqrt{x^2+y^2}} =0 $$ but then I got stuck. Am I even on the right track? [UPDATE] I need to do the same thing for $f(x,y)=(xy)^{\frac{1}{3}}$... I think the answer is that $(xy)^{2/3}$ is differentiable but $(xy)^{1/3}$ is not, according to my professor but I'm not sure",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus', 'derivatives']"
76,Sequence of norms,Sequence of norms,,"Suppose that we have a sequence of norms $(\|\cdot\|_n)_{n\geq 1}$ on a finite dimensional space $X$ such that, as functions from $X$ to $\mathbb{R}_+$, they point-wise converge to a limit norm $\|\cdot\|_\infty$. Since we are in a finite-dimensional setting, all of the norms are equivalent among them, and, in particular, all of the norms in the sequence are equivalent to $\|\cdot\|_\infty$. An implication of this fact is that there is a sequence of constants $c_n$ such that $$ \forall n \geq 1,  \forall x\in X, \quad \|x\|_n \geq c_n\|x\|_\infty. $$ Now, the equivalence constants $c_n$ are not necessarily unique, but, since the norms $\|\cdot\|_n$ approach $\|\cdot\|_\infty$ in the limit, I am wondering how can one prove that they can be chosen to have $1$ as a limit (i.e. $\lim_{n\to\infty} c_n = 1$) ?","Suppose that we have a sequence of norms $(\|\cdot\|_n)_{n\geq 1}$ on a finite dimensional space $X$ such that, as functions from $X$ to $\mathbb{R}_+$, they point-wise converge to a limit norm $\|\cdot\|_\infty$. Since we are in a finite-dimensional setting, all of the norms are equivalent among them, and, in particular, all of the norms in the sequence are equivalent to $\|\cdot\|_\infty$. An implication of this fact is that there is a sequence of constants $c_n$ such that $$ \forall n \geq 1,  \forall x\in X, \quad \|x\|_n \geq c_n\|x\|_\infty. $$ Now, the equivalence constants $c_n$ are not necessarily unique, but, since the norms $\|\cdot\|_n$ approach $\|\cdot\|_\infty$ in the limit, I am wondering how can one prove that they can be chosen to have $1$ as a limit (i.e. $\lim_{n\to\infty} c_n = 1$) ?",,"['sequences-and-series', 'limits', 'normed-spaces']"
77,"Showing that $f_n:[0,1]\to\mathbb{R}$ when $f_n=nx(1-x)^n$ converges pointwise but not uniformly",Showing that  when  converges pointwise but not uniformly,"f_n:[0,1]\to\mathbb{R} f_n=nx(1-x)^n","Suppose $f_n:[0,1]\to\mathbb{R}$ such that $f_n=nx(1-x)^n$, then I need to show that $f_n$ converges pointwise but not uniformly. I can see that since $f$ is defined on $[0,1]$, the part $(1-x)^n$ is smaller than $1$ and therefore will converge, but what about $nx$? This will go to infinity... Therefore, I can't even see that it converges pointwise, but let's suppose it converges to some function, how do I show that it doesn't converge uniformly? I should find an $\epsilon$ such that it's not valid for all $n>n_0(\epsilon)$ that $|f_n-something|<\epsilon$, right?","Suppose $f_n:[0,1]\to\mathbb{R}$ such that $f_n=nx(1-x)^n$, then I need to show that $f_n$ converges pointwise but not uniformly. I can see that since $f$ is defined on $[0,1]$, the part $(1-x)^n$ is smaller than $1$ and therefore will converge, but what about $nx$? This will go to infinity... Therefore, I can't even see that it converges pointwise, but let's suppose it converges to some function, how do I show that it doesn't converge uniformly? I should find an $\epsilon$ such that it's not valid for all $n>n_0(\epsilon)$ that $|f_n-something|<\epsilon$, right?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
78,Is this a valid transformation of a expression?,Is this a valid transformation of a expression?,,I need to check if this function is continuous: $$f(x)= \lim_{n\to\infty}\sqrt[n]{4+x^{6n}}$$ So I did this: $$f(x)= \lim_{n\to\infty}\sqrt[n]{4+x^{6n}} = \lim_{n\to\infty}(4+x^{6n})^{1/n} =\lim_{n\to\infty} \left(x^{6n}\cdot \left(\frac{4}{x^{6n}}+ 1 \right) \right)^{1/n}$$ $$f(x) = \lim_{n\to\infty}x^6 \cdot \left(\frac 4 {x^{6n}}+ 1\right)^{1/n} = x^6$$ Is this valid? Can I now just check the continuity of: $$f(x) = x^6$$,I need to check if this function is continuous: $$f(x)= \lim_{n\to\infty}\sqrt[n]{4+x^{6n}}$$ So I did this: $$f(x)= \lim_{n\to\infty}\sqrt[n]{4+x^{6n}} = \lim_{n\to\infty}(4+x^{6n})^{1/n} =\lim_{n\to\infty} \left(x^{6n}\cdot \left(\frac{4}{x^{6n}}+ 1 \right) \right)^{1/n}$$ $$f(x) = \lim_{n\to\infty}x^6 \cdot \left(\frac 4 {x^{6n}}+ 1\right)^{1/n} = x^6$$ Is this valid? Can I now just check the continuity of: $$f(x) = x^6$$,,"['limits', 'functions', 'continuity']"
79,Evaluating limit of sum of all terms of a given series,Evaluating limit of sum of all terms of a given series,,"Let $$T_n ={\Big(\frac{n!}{1\cdot 3\cdot 5\cdot 7 \cdots (2n+1)}\Big)}^2,$$ compute $$\lim_{ n \to \infty} ( T_1+T_2+ \cdots +T_n)$$ I tried to find out a recurrence relation with the help of the given series but it did not help me. Provided that the limit exists, I tried to check the upper and lower bound for it but could not succeed. Please help me in this regard. Thanks.","Let $$T_n ={\Big(\frac{n!}{1\cdot 3\cdot 5\cdot 7 \cdots (2n+1)}\Big)}^2,$$ compute $$\lim_{ n \to \infty} ( T_1+T_2+ \cdots +T_n)$$ I tried to find out a recurrence relation with the help of the given series but it did not help me. Provided that the limit exists, I tried to check the upper and lower bound for it but could not succeed. Please help me in this regard. Thanks.",,['limits']
80,"find $\lim_{(x,y)\to (0,0)} \frac{x^2y^2}{x^4+y^4}$",find,"\lim_{(x,y)\to (0,0)} \frac{x^2y^2}{x^4+y^4}","if $$f(x,y)=\frac{x^2y^2}{x^4+y^4}$$ is a 2 variable function, find $$\lim_{(x,y)\to (0,0)} \frac{x^2y^2}{x^4+y^4}$$ I really don't understand 2 variable limits. I understand that if limits from 2 or more paths aren't the same, the limit doesn't exist, but I don't know how to find the limit since there is an infinite number of paths possible.","if $$f(x,y)=\frac{x^2y^2}{x^4+y^4}$$ is a 2 variable function, find $$\lim_{(x,y)\to (0,0)} \frac{x^2y^2}{x^4+y^4}$$ I really don't understand 2 variable limits. I understand that if limits from 2 or more paths aren't the same, the limit doesn't exist, but I don't know how to find the limit since there is an infinite number of paths possible.",,"['calculus', 'limits', 'multivariable-calculus']"
81,What is $\lim_{x\to0}\frac{|x+3|(\sqrt{ax+b}-2)}x$ when it exists?,What is  when it exists?,\lim_{x\to0}\frac{|x+3|(\sqrt{ax+b}-2)}x,"For what values of $a$ and $b$ does the following limit exist and what is the limit in those cases?   $$\lim_{x\to0}\frac{|x+3|(\sqrt{ax+b}-2)}x$$ Actually this is an assignment which I need to do, but I have no idea where to start, please help!","For what values of $a$ and $b$ does the following limit exist and what is the limit in those cases?   $$\lim_{x\to0}\frac{|x+3|(\sqrt{ax+b}-2)}x$$ Actually this is an assignment which I need to do, but I have no idea where to start, please help!",,"['calculus', 'limits']"
82,Proving the continued fraction expansion of $\sqrt2$ [duplicate],Proving the continued fraction expansion of  [duplicate],\sqrt2,"This question already has answers here : Proving the continued fraction representation of $\sqrt{2}$ (3 answers) Closed 7 years ago . I am working on a multi-part problem, which shows that $$\sqrt{2} = 1+\frac{1}{2+\frac{1}{2+...}}$$ I have shown that given some rational approximation $\frac{m}{n}$ to $\sqrt2$ , we can take $\frac{m'}{n'} = \frac{m+2n}{m+n}$ and get a better approximation. I also know that this iteration goes back and forth between being larger and smaller than $\sqrt2$. Say if we start with $\frac{m}{n}<\sqrt2$, then iterating will give us $$\frac{m}{n}<\frac{m''}{n''} = \frac{3m+4n}{2m+3n}<\sqrt2$$ Finally, I am asked to prove that the sequence obtained by iterating in this fashion, starting with $$1,\frac{3}{2},\frac{7}{5},...$$ which I have shown is given recursively by $$q_1=1, q_{n+1} = 1+\frac{1}{1+q_n} $$ converges to $\sqrt2$. My professor's ""hint"" is to consider the ""odd"" and ""even"" subsequences, and show that they both converge to the same limit. This makes sense to me, since both sequences are clearly monotone and bounded. Most (all?) of my classmates simply wrote that because they are both monotone and bounded by $\sqrt2$, then they must both converge to $\sqrt2$. However, just because they are bounded above and below, respectively by $\sqrt2$, it does not necessarily mean that $\sqrt2$ is the limit for both (or either!) of them. Some classmates have tried to prove by contradiction that if there were some other limit for one of the subsequences, we could iterate again and get closer to $\sqrt2$. I think that this is a faulty argument, because it assumes that we reach the limit. Remember: the sequence is ""fixed"" as soon as we choose to start it at $q_1 = 1$ ! My counterargument towards most of my classmates has been the sequence given my $\frac{1}{n}$. The sequence is bounded below by $-99$, and no matter how many times we ""iterate"" (in this case, just taking the next value of $n$) we can always get closer and closer to $-99$, but that doesn't mean that $-99$ is the limit. If someone could offer me a resolution to this problem I would be hugely appreciative. It's been driving me crazy for the past week. EDIT: I realize this post has been marked as a duplicate, however I think it should be left up, as the answer given is a bit different from that in the ""original"".","This question already has answers here : Proving the continued fraction representation of $\sqrt{2}$ (3 answers) Closed 7 years ago . I am working on a multi-part problem, which shows that $$\sqrt{2} = 1+\frac{1}{2+\frac{1}{2+...}}$$ I have shown that given some rational approximation $\frac{m}{n}$ to $\sqrt2$ , we can take $\frac{m'}{n'} = \frac{m+2n}{m+n}$ and get a better approximation. I also know that this iteration goes back and forth between being larger and smaller than $\sqrt2$. Say if we start with $\frac{m}{n}<\sqrt2$, then iterating will give us $$\frac{m}{n}<\frac{m''}{n''} = \frac{3m+4n}{2m+3n}<\sqrt2$$ Finally, I am asked to prove that the sequence obtained by iterating in this fashion, starting with $$1,\frac{3}{2},\frac{7}{5},...$$ which I have shown is given recursively by $$q_1=1, q_{n+1} = 1+\frac{1}{1+q_n} $$ converges to $\sqrt2$. My professor's ""hint"" is to consider the ""odd"" and ""even"" subsequences, and show that they both converge to the same limit. This makes sense to me, since both sequences are clearly monotone and bounded. Most (all?) of my classmates simply wrote that because they are both monotone and bounded by $\sqrt2$, then they must both converge to $\sqrt2$. However, just because they are bounded above and below, respectively by $\sqrt2$, it does not necessarily mean that $\sqrt2$ is the limit for both (or either!) of them. Some classmates have tried to prove by contradiction that if there were some other limit for one of the subsequences, we could iterate again and get closer to $\sqrt2$. I think that this is a faulty argument, because it assumes that we reach the limit. Remember: the sequence is ""fixed"" as soon as we choose to start it at $q_1 = 1$ ! My counterargument towards most of my classmates has been the sequence given my $\frac{1}{n}$. The sequence is bounded below by $-99$, and no matter how many times we ""iterate"" (in this case, just taking the next value of $n$) we can always get closer and closer to $-99$, but that doesn't mean that $-99$ is the limit. If someone could offer me a resolution to this problem I would be hugely appreciative. It's been driving me crazy for the past week. EDIT: I realize this post has been marked as a duplicate, however I think it should be left up, as the answer given is a bit different from that in the ""original"".",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'continued-fractions']"
83,Series $\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)}$ Convergence or Divergence Using The Ratio Test,Series  Convergence or Divergence Using The Ratio Test,\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)},"I am trying to determine if the following series converges or diverges by using the ratio test, which I believe can be summarized as the following: $$L=\lim_{n\to \infty}  \left| \frac{a_{n+1}}{a_n} \right|$$ If $L < 1$ then the series converges, if $L > 1$, then it diverges, and if $L = 1$, then it is ambiguous. The series is below: $$\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)}$$ I understand the ratio test in theory, but am not sure how to put it into practice for a series like this.","I am trying to determine if the following series converges or diverges by using the ratio test, which I believe can be summarized as the following: $$L=\lim_{n\to \infty}  \left| \frac{a_{n+1}}{a_n} \right|$$ If $L < 1$ then the series converges, if $L > 1$, then it diverges, and if $L = 1$, then it is ambiguous. The series is below: $$\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)}$$ I understand the ratio test in theory, but am not sure how to put it into practice for a series like this.",,"['sequences-and-series', 'limits', 'convergence-divergence']"
84,Spivak alternative limit solution fallacy,Spivak alternative limit solution fallacy,,I can't find where the fallacy of the following proof lies!,I can't find where the fallacy of the following proof lies!,,['limits']
85,Evaluate the Limit of the Quotient of two exponential functions,Evaluate the Limit of the Quotient of two exponential functions,,Say you have the following function: $$\frac{a^x}{b^x}$$ and want to evaluate it's limit as $x$ approaches infinity. Intuition tells me that the limit will approach $0$ if $b > a$ and will approach $\infty$ if $a>b$ and plugging various equations into a graphing calculator bears this out. However I am at a loss for how to definitively prove that this is the case. Is there a specific identity or theorem for evaluating such a limit?,Say you have the following function: $$\frac{a^x}{b^x}$$ and want to evaluate it's limit as $x$ approaches infinity. Intuition tells me that the limit will approach $0$ if $b > a$ and will approach $\infty$ if $a>b$ and plugging various equations into a graphing calculator bears this out. However I am at a loss for how to definitively prove that this is the case. Is there a specific identity or theorem for evaluating such a limit?,,"['limits', 'exponential-function']"
86,When do limits at infinity not exist?,When do limits at infinity not exist?,,"Here is a limit at infinity$$\lim_{x \to \infty}f(x)$$ A limit fails to exist for one of the four reasons: The one-sided limits are not equal The function doesn't approach a finite value The function oscillates The $x$ value is approaching the endpoint of a closed interval Here are my questions: How does this work for limits at infinity? I am under the impression that we can only approach infinity from one side, therefore causing all limit at infinity to fail this test. So the only case where this rule applies is if the limit evaluates to either $+\infty$ or $-\infty$? No questions. Is this case essentially a subset of case 1?","Here is a limit at infinity$$\lim_{x \to \infty}f(x)$$ A limit fails to exist for one of the four reasons: The one-sided limits are not equal The function doesn't approach a finite value The function oscillates The $x$ value is approaching the endpoint of a closed interval Here are my questions: How does this work for limits at infinity? I am under the impression that we can only approach infinity from one side, therefore causing all limit at infinity to fail this test. So the only case where this rule applies is if the limit evaluates to either $+\infty$ or $-\infty$? No questions. Is this case essentially a subset of case 1?",,"['calculus', 'limits']"
87,How can I find the limit of a function that is inside a limit?,How can I find the limit of a function that is inside a limit?,,"For example: Given $\lim_{x\to 4} \frac{5xf(x)−1}{x−4}= 8$, find $\lim_{x\to 4} f(x)$. A good hint would be appreciated!","For example: Given $\lim_{x\to 4} \frac{5xf(x)−1}{x−4}= 8$, find $\lim_{x\to 4} f(x)$. A good hint would be appreciated!",,"['calculus', 'limits']"
88,Prove that $\lim_{n \to \infty}\dfrac{\alpha(n)}{n} = 0$,Prove that,\lim_{n \to \infty}\dfrac{\alpha(n)}{n} = 0,"Prove that $\displaystyle \lim_{n \to \infty}\dfrac{\alpha(n)}{n} = 0$ where $\alpha(n)$ is the number of primes which divide $n$. I think we should get an upper bound on $\alpha(n)$ by using the fact that each prime is greater than or equal to $2$, but I am not sure who to get the bound. Also, do they mean $\alpha(n)$ to be the number of distinct primes which divide $n$","Prove that $\displaystyle \lim_{n \to \infty}\dfrac{\alpha(n)}{n} = 0$ where $\alpha(n)$ is the number of primes which divide $n$. I think we should get an upper bound on $\alpha(n)$ by using the fact that each prime is greater than or equal to $2$, but I am not sure who to get the bound. Also, do they mean $\alpha(n)$ to be the number of distinct primes which divide $n$",,"['calculus', 'limits', 'prime-factorization']"
89,Prove: If $\lim_{x \to \infty}g(x)=0$ $ \Rightarrow \lim_{x \to \infty} f(x)=0$,Prove: If,\lim_{x \to \infty}g(x)=0  \Rightarrow \lim_{x \to \infty} f(x)=0,"$\forall x \in \mathbb R$ $g(x)\gt 0 $ and $\displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)}=L \gt0$ Prove that 1.) If $\displaystyle \lim_{x \to \infty}g(x)=0$ then $\displaystyle \lim_{x \to \infty} f(x)=0$ 2.) Conclude that $\displaystyle \lim_{x \to \infty}g(x)=0 \iff \lim_{x \to \infty} f(x)=0$ 1.) Using the definition of limits: $\forall \epsilon_1 \gt 0$ , $N_1 \gt 0$ $$ x\gt N_1  \Rightarrow |g(x)| \lt \epsilon_1$$ $\forall \epsilon_0 \gt 0$ , $N_0 \gt 0$ $$ x \gt N_0  \Rightarrow \left|\frac{f(x)}{g(x)}-L\right| \lt \epsilon_0$$ Now doing a little bit of algebra: $$g(x)(-\epsilon_0+L)\lt f(x) \lt g(x)(\epsilon_0+L)$$ And using the fact that $|g(x)| \lt \epsilon_1$ : $$-\epsilon_1(-\epsilon_0+L) \le g(x)(-\epsilon_0+L)\lt f(x) \lt g(x)(\epsilon_0+L)\le \epsilon_1(\epsilon_0+L)$$ This turns into: $$-\epsilon_1(-\epsilon_0+L) \lt f(x) \lt \epsilon_1(\epsilon_0+L)$$ Are my steps correct thus far? Leaving the only thing left is picking a value for $\epsilon_1$ so that I end up with $|f(x)|\lt \epsilon$ ?","and Prove that 1.) If then 2.) Conclude that 1.) Using the definition of limits: , , Now doing a little bit of algebra: And using the fact that : This turns into: Are my steps correct thus far? Leaving the only thing left is picking a value for so that I end up with ?",\forall x \in \mathbb R g(x)\gt 0  \displaystyle \lim_{x \to \infty} \frac{f(x)}{g(x)}=L \gt0 \displaystyle \lim_{x \to \infty}g(x)=0 \displaystyle \lim_{x \to \infty} f(x)=0 \displaystyle \lim_{x \to \infty}g(x)=0 \iff \lim_{x \to \infty} f(x)=0 \forall \epsilon_1 \gt 0 N_1 \gt 0  x\gt N_1  \Rightarrow |g(x)| \lt \epsilon_1 \forall \epsilon_0 \gt 0 N_0 \gt 0  x \gt N_0  \Rightarrow \left|\frac{f(x)}{g(x)}-L\right| \lt \epsilon_0 g(x)(-\epsilon_0+L)\lt f(x) \lt g(x)(\epsilon_0+L) |g(x)| \lt \epsilon_1 -\epsilon_1(-\epsilon_0+L) \le g(x)(-\epsilon_0+L)\lt f(x) \lt g(x)(\epsilon_0+L)\le \epsilon_1(\epsilon_0+L) -\epsilon_1(-\epsilon_0+L) \lt f(x) \lt \epsilon_1(\epsilon_0+L) \epsilon_1 |f(x)|\lt \epsilon,"['calculus', 'limits']"
90,Prove that $\lim \limits_{x \to 5}\left(4x^2-7\right)=93$,Prove that,\lim \limits_{x \to 5}\left(4x^2-7\right)=93,"So I first need to determine the limit and then prove it: $\lim \limits_{x \to 5}\left(4x^2-7\right)$ So $L=93$ And thus $\left|f(x)-L\right|=\epsilon$ and $\left|x-c\right|=\delta$ Plugging in the values... $\left|\left(4x^2-7\right)-93\right| \lt \epsilon$, which when factored gives $4(x+5)\left|x-5\right|\lt \epsilon$ So does this mean: $\left|x-5\right|\lt 4(x+5)\delta$","So I first need to determine the limit and then prove it: $\lim \limits_{x \to 5}\left(4x^2-7\right)$ So $L=93$ And thus $\left|f(x)-L\right|=\epsilon$ and $\left|x-c\right|=\delta$ Plugging in the values... $\left|\left(4x^2-7\right)-93\right| \lt \epsilon$, which when factored gives $4(x+5)\left|x-5\right|\lt \epsilon$ So does this mean: $\left|x-5\right|\lt 4(x+5)\delta$",,"['real-analysis', 'limits', 'proof-writing', 'epsilon-delta']"
91,Is the limit $\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]}$ a one sided limit or not?,Is the limit  a one sided limit or not?,\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]},"Is the limit $\displaystyle\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]}$ a one sided limit or not? Here $[\, \cdot\, ]$ is the greatest integer function. According to me the right hand limit will be not defined and the left hand limit will be $\dfrac{-\sin1}{-1}$.","Is the limit $\displaystyle\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]}$ a one sided limit or not? Here $[\, \cdot\, ]$ is the greatest integer function. According to me the right hand limit will be not defined and the left hand limit will be $\dfrac{-\sin1}{-1}$.",,"['limits', 'trigonometry', 'ceiling-and-floor-functions']"
92,Help finding the limit of ln(2-x) if x tends to infinity,Help finding the limit of ln(2-x) if x tends to infinity,,"This is my problem. I've also put some work for the formatting! Find the limit if it exists: $$\lim_{x\to\infty}f(x)=\frac{ln(2-x)}{\sqrt[3]{1-6x}}$$ I've tried solving it, but I can't seem to figure it out. Using hospital's rule: I know that  $$\frac{1}{\sqrt[3]{1-6x}} = (1-6x)^{-1/3}$$ so  $$\lim_{x\to\infty}f(x)=\frac{\frac{d}{dx}ln(2-x)}{\frac{d}{dx}\sqrt[3]{1-6x}}$$ $$\lim_{x\to\infty}f(x)=\frac{\frac{-1}{(2-x)}}{(-1/3)*(1-6x)^{-4/3}*6}$$ After this I get really confused... I've tried stuff for hours now. Thank you guys.","This is my problem. I've also put some work for the formatting! Find the limit if it exists: $$\lim_{x\to\infty}f(x)=\frac{ln(2-x)}{\sqrt[3]{1-6x}}$$ I've tried solving it, but I can't seem to figure it out. Using hospital's rule: I know that  $$\frac{1}{\sqrt[3]{1-6x}} = (1-6x)^{-1/3}$$ so  $$\lim_{x\to\infty}f(x)=\frac{\frac{d}{dx}ln(2-x)}{\frac{d}{dx}\sqrt[3]{1-6x}}$$ $$\lim_{x\to\infty}f(x)=\frac{\frac{-1}{(2-x)}}{(-1/3)*(1-6x)^{-4/3}*6}$$ After this I get really confused... I've tried stuff for hours now. Thank you guys.",,"['calculus', 'limits']"
93,$\sum (-1)^{n+1}\log\left(1+\frac{1}{n}\right)$ convergent but not absolutely convergent,convergent but not absolutely convergent,\sum (-1)^{n+1}\log\left(1+\frac{1}{n}\right),"I need to prove that: $$\sum_{n=1}^{\infty} (-1)^{n+1}\log\left(1+\frac{1}{n}\right)$$ is convergent, but not absolutely convergent. I tried the ratio test: $$\frac{a_{n+1}}{a_n} = -\frac{\log\left(1+\frac{1}{n+1}\right)}{\log\left(1+\frac{1}{n}\right)} = -\log\left({\frac{1}{n+1}-\frac{1}{n}}\right)$$ I know that the thing inside the $\log$ converges to $1$, so $-\log$ converges to $0$? This is not right, I cannot conclude that this series is divergent. Also, for the sequence without the $(-1)^{n+1}$ it would give $0$ too.","I need to prove that: $$\sum_{n=1}^{\infty} (-1)^{n+1}\log\left(1+\frac{1}{n}\right)$$ is convergent, but not absolutely convergent. I tried the ratio test: $$\frac{a_{n+1}}{a_n} = -\frac{\log\left(1+\frac{1}{n+1}\right)}{\log\left(1+\frac{1}{n}\right)} = -\log\left({\frac{1}{n+1}-\frac{1}{n}}\right)$$ I know that the thing inside the $\log$ converges to $1$, so $-\log$ converges to $0$? This is not right, I cannot conclude that this series is divergent. Also, for the sequence without the $(-1)^{n+1}$ it would give $0$ too.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
94,Can a polygon with an infinite number of sides be viewed as a line?,Can a polygon with an infinite number of sides be viewed as a line?,,"The inner angles of a polygon approach 180º as the number of sides (N) of the polygon increases. So, if N approaches infinity, we would have a circle. But... At infinity, we would also have a set of sides with 180º between each other, i.e., a straight line. So, is it a circle, line or both? This seems strange, but how could this be explained? Is this reasoning wrong? Thanks :)","The inner angles of a polygon approach 180º as the number of sides (N) of the polygon increases. So, if N approaches infinity, we would have a circle. But... At infinity, we would also have a set of sides with 180º between each other, i.e., a straight line. So, is it a circle, line or both? This seems strange, but how could this be explained? Is this reasoning wrong? Thanks :)",,"['limits', 'circles', 'polygons']"
95,Limit of $f^{-1}(f(x))$ as $f(x)$ diverges,Limit of  as  diverges,f^{-1}(f(x)) f(x),"Let $f(x)$ be a continuous, real-valued, one-to-one function on some domain $(a,b)$ which diverges at one end of the domain (say $b$). As $f$ is one-to-one I expect to be able to define a inverse function $f^{-1}$ such that $f^{-1}(f(x)) = x$ for $x$ in the domain of $f$. Is the limit $$ \lim_{x\to b} f^{-1}(f(x)) \,,$$ well defined (and presumably equal to $b$), and if so is there a general procedure for evaluating $$ \lim_{x\to b} f^{-1}(c \, f(x)) \,,$$ for some constant $c$? I have in mind $$\lim_{x\to \pi/2} \arctan( c \, \tan(x) )$$ in particular if there is some further condition that $f$ must meet to proceed.","Let $f(x)$ be a continuous, real-valued, one-to-one function on some domain $(a,b)$ which diverges at one end of the domain (say $b$). As $f$ is one-to-one I expect to be able to define a inverse function $f^{-1}$ such that $f^{-1}(f(x)) = x$ for $x$ in the domain of $f$. Is the limit $$ \lim_{x\to b} f^{-1}(f(x)) \,,$$ well defined (and presumably equal to $b$), and if so is there a general procedure for evaluating $$ \lim_{x\to b} f^{-1}(c \, f(x)) \,,$$ for some constant $c$? I have in mind $$\lim_{x\to \pi/2} \arctan( c \, \tan(x) )$$ in particular if there is some further condition that $f$ must meet to proceed.",,"['real-analysis', 'limits', 'inverse-function']"
96,Evaluate $\lim_{n\to \infty}{\sqrt{(1-\cos(1/n))\sqrt{(1-\cos(1/n))\dots}}}$,Evaluate,\lim_{n\to \infty}{\sqrt{(1-\cos(1/n))\sqrt{(1-\cos(1/n))\dots}}},"Evaluate the limit:$$\lim_{n\to \infty}{\sqrt{(1-\cos(1/n))\sqrt{(1-\cos(1/n))\dots}}}$$ My attempt: let $l$ be equal to that limit so i can write $$l=\lim_{n\to \infty}{(1-cos(1/n))^{1/2+1/2^2+1/2^3.....\infty}}$$ As i know $$1/2+1/2^2+1/2^3....\infty=1$$ therefore i can write $$l=\lim_{n\to \infty}(1-\cos(1/n))^1$$ as $n\to \infty, 1/n \to 0$ so $$l=(1-1)=0$$ i don't know why but this is wrong, the correct answer is $1/2$. Can anyone say why i am wrong and show the correct procedure!","Evaluate the limit:$$\lim_{n\to \infty}{\sqrt{(1-\cos(1/n))\sqrt{(1-\cos(1/n))\dots}}}$$ My attempt: let $l$ be equal to that limit so i can write $$l=\lim_{n\to \infty}{(1-cos(1/n))^{1/2+1/2^2+1/2^3.....\infty}}$$ As i know $$1/2+1/2^2+1/2^3....\infty=1$$ therefore i can write $$l=\lim_{n\to \infty}(1-\cos(1/n))^1$$ as $n\to \infty, 1/n \to 0$ so $$l=(1-1)=0$$ i don't know why but this is wrong, the correct answer is $1/2$. Can anyone say why i am wrong and show the correct procedure!",,"['real-analysis', 'sequences-and-series', 'limits']"
97,How to prove $\lim\limits_{n\to \infty}(a_n+2b_n)=L+2M$ using the formal definition of limit of sequence?,How to prove  using the formal definition of limit of sequence?,\lim\limits_{n\to \infty}(a_n+2b_n)=L+2M,"Let $\{a_n\}^{\infty}_{n=1},\{b_n\}^{\infty}_{n=1}$ be sequence. Assume $\lim\limits_{n\to \infty}a_n=L,\ \ \lim\limits_{n\to \infty}b_n=M$, how to prove $\lim\limits_{n\to \infty}(a_n+2b_n)=L+2M$ using the formal definition of limit of sequence? I remember the formal definition goes as following(not sure for sequence though): $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |a_n-L|\lt \epsilon$ Then we can set $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |a_n-L|\lt \frac12\epsilon$ $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |b_n-M|\lt \frac14\epsilon$ Then $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |2b_n-2M|\lt \frac12\epsilon$ Then $|a_n-L|+|2b_n-2M|\lt \epsilon$. But I am not sure write a proof, could someone help?","Let $\{a_n\}^{\infty}_{n=1},\{b_n\}^{\infty}_{n=1}$ be sequence. Assume $\lim\limits_{n\to \infty}a_n=L,\ \ \lim\limits_{n\to \infty}b_n=M$, how to prove $\lim\limits_{n\to \infty}(a_n+2b_n)=L+2M$ using the formal definition of limit of sequence? I remember the formal definition goes as following(not sure for sequence though): $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |a_n-L|\lt \epsilon$ Then we can set $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |a_n-L|\lt \frac12\epsilon$ $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |b_n-M|\lt \frac14\epsilon$ Then $\forall \epsilon\gt 0,\exists N\in\Bbb{R}:x\gt N\implies |2b_n-2M|\lt \frac12\epsilon$ Then $|a_n-L|+|2b_n-2M|\lt \epsilon$. But I am not sure write a proof, could someone help?",,"['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'epsilon-delta']"
98,"Limit $\lim\limits_{(x,y)\to(0,0)}\frac{\sin(x^2-y^2)}{x^2-y^2}$",Limit,"\lim\limits_{(x,y)\to(0,0)}\frac{\sin(x^2-y^2)}{x^2-y^2}","I'm trying to understand the following limit: $$\lim_{(x,y)\to(0,0)}\frac{\sin(x^2-y^2)}{x^2-y^2}$$ The $\lim_{(x,y)\to (0,0)} f(x,y)$ is undefined. Why it is not equal to $1$? Let's suppose $t = x^2-y^2$. Then as $(x,y)$ approaches $(0,0)$, $t$ approaches $0$, so $$\lim_{t\to 0} \frac{\sin t}{t} = 1 $$ I can see that the path $x=y$ is undefined,does this mean that the limit does not exist ? I though that I needed to find two different defined paths that gives two different results to disprove that the limit exists. If so this means that I can't really rely on the ""t substitution"" to determined the limit. I tried to pass it into polar coordinates but It didn't help.. How can I determine a limit of a two variable function ? I can't check all the possibles paths like in one variable (left and right ). I understood that the safest way is to use the squeeze theorem  or to pass the coordinates into polar coordinates and then use the squeeze theorem. I would appreciate any sort of help in the matter , Thanks.","I'm trying to understand the following limit: $$\lim_{(x,y)\to(0,0)}\frac{\sin(x^2-y^2)}{x^2-y^2}$$ The $\lim_{(x,y)\to (0,0)} f(x,y)$ is undefined. Why it is not equal to $1$? Let's suppose $t = x^2-y^2$. Then as $(x,y)$ approaches $(0,0)$, $t$ approaches $0$, so $$\lim_{t\to 0} \frac{\sin t}{t} = 1 $$ I can see that the path $x=y$ is undefined,does this mean that the limit does not exist ? I though that I needed to find two different defined paths that gives two different results to disprove that the limit exists. If so this means that I can't really rely on the ""t substitution"" to determined the limit. I tried to pass it into polar coordinates but It didn't help.. How can I determine a limit of a two variable function ? I can't check all the possibles paths like in one variable (left and right ). I understood that the safest way is to use the squeeze theorem  or to pass the coordinates into polar coordinates and then use the squeeze theorem. I would appreciate any sort of help in the matter , Thanks.",,"['calculus', 'limits', 'multivariable-calculus']"
99,Alternatives to pure quantifier logic,Alternatives to pure quantifier logic,,"Are there some alternatives for pure quantifier logic? Pure quantifier logic is axioms and rules of inference added to proposition logic to result first order logic. Are there other axioms that do not use notions ""for all"" or ""there exists"", but can be applied to extend propositional logic, and can capture many things (at least quantification over $\Bbb R^n$ ) captured by pure quantifier logic. Is it possible to use limits to define quantifiers (that would allow at least quantification over $\Bbb R^n$ )? I know that limits are defined in terms of quantifiers, but can they be redefined somehow?","Are there some alternatives for pure quantifier logic? Pure quantifier logic is axioms and rules of inference added to proposition logic to result first order logic. Are there other axioms that do not use notions ""for all"" or ""there exists"", but can be applied to extend propositional logic, and can capture many things (at least quantification over ) captured by pure quantifier logic. Is it possible to use limits to define quantifiers (that would allow at least quantification over )? I know that limits are defined in terms of quantifiers, but can they be redefined somehow?",\Bbb R^n \Bbb R^n,"['limits', 'logic', 'quantifiers', 'foundations']"
