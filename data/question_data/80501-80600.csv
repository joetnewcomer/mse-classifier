,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,SVD of “almost” block diagonal matrix,SVD of “almost” block diagonal matrix,,"It is possible to show that SVD of block diagonal matrix is equivalent to independent SVDs of each block. I am wondering if there is something interesting to say on the case where the matrix is composed of 2 blocks of size $N$ , with $k$ rows and $k$ columns of size $2N+k$ (overlapping in a $k\times k$ corner) added on. I am particularly interested in the influence of the “shared” rows on the reconstruction of the entire matrix when using truncated SVD. An example: \begin{pmatrix}   \begin{matrix}   X_1   \end{matrix}   & 0 & \vdots \\ \   0& X_2 & \vec{u} \\  \cdots & \vec{v} &\vdots \end{pmatrix} Here, $X_1$ and $X_2$ are the two diagonal blocks; each block is $N \times N$ . I concatenate to them a single row $\vec{v}$ with $2N+1$ entries and a single column $\vec{u}$ with $2N+1$ entries, and aim to understand how the SVD of the complete matrix relates to the SVD of the block diagonal sub-matrix. Thanks","It is possible to show that SVD of block diagonal matrix is equivalent to independent SVDs of each block. I am wondering if there is something interesting to say on the case where the matrix is composed of 2 blocks of size , with rows and columns of size (overlapping in a corner) added on. I am particularly interested in the influence of the “shared” rows on the reconstruction of the entire matrix when using truncated SVD. An example: Here, and are the two diagonal blocks; each block is . I concatenate to them a single row with entries and a single column with entries, and aim to understand how the SVD of the complete matrix relates to the SVD of the block diagonal sub-matrix. Thanks","N k k 2N+k k\times k \begin{pmatrix}
  \begin{matrix}
  X_1
  \end{matrix}
  & 0 & \vdots \\
\
  0& X_2 & \vec{u} \\ 
\cdots & \vec{v} &\vdots
\end{pmatrix} X_1 X_2 N \times N \vec{v} 2N+1 \vec{u} 2N+1","['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'block-matrices']"
1,Alternative proof for multiplicativity of resultant?,Alternative proof for multiplicativity of resultant?,,"The formula $R(fg,h)=R(f,h)R(g,h)$ follows easily if you express each resultant in terms of the roots of the polynomials involved. It can also be obtained by relating $R(f,h)$ to the determinant of multiplication by $f$ on the space $K[x]/(h)$ . But is there a  way to prove the above  formula by extending the three Sylvester matrices involved by cleverly chosen blocks  so that the determinants don't change, the three matrices become square matrices of equal size, and one matrix is the product of the other two?","The formula follows easily if you express each resultant in terms of the roots of the polynomials involved. It can also be obtained by relating to the determinant of multiplication by on the space . But is there a  way to prove the above  formula by extending the three Sylvester matrices involved by cleverly chosen blocks  so that the determinants don't change, the three matrices become square matrices of equal size, and one matrix is the product of the other two?","R(fg,h)=R(f,h)R(g,h) R(f,h) f K[x]/(h)","['matrices', 'polynomials', 'determinant', 'resultant']"
2,Change of variables when integrating over matrices,Change of variables when integrating over matrices,,"I have the following integral \begin{align}     \int f \left(U(X)\right) g \left(U(X) \right) dX \end{align} Where, $X,U(X) \in \mathbb{R}^{n \times d}$ ; $f,g: \mathbb{R}^{n \times d} \xrightarrow{} \mathbb{R}$ . $f$ is a PDF and $g(X)$ is a positive function for every $X$ . The connection between $X$ and $U(X)$ is given as follow: \begin{equation} \label{Eq: relation}     U\left( X \right)^{T} U\left( X \right)     =X^{T} X + B \end{equation} I would like to change the integral such that the integration variable will be $V=U(X)$ . It should have the general form of \begin{align}     \int f \left(U(X)\right) g \left(U(X) \right) dX     &=\int f \left(V\right) g \left(V \right)      \det (J\left(X,V \right)) dV \end{align} $J\left(X,V \right)$ is some sort of Jacobian matrix, but I can't figure out what should it be and how to calculate its determinant. Any help will be appreciated.","I have the following integral Where, ; . is a PDF and is a positive function for every . The connection between and is given as follow: I would like to change the integral such that the integration variable will be . It should have the general form of is some sort of Jacobian matrix, but I can't figure out what should it be and how to calculate its determinant. Any help will be appreciated.","\begin{align}
    \int f \left(U(X)\right) g \left(U(X) \right) dX
\end{align} X,U(X) \in \mathbb{R}^{n \times d} f,g: \mathbb{R}^{n \times d} \xrightarrow{} \mathbb{R} f g(X) X X U(X) \begin{equation}
\label{Eq: relation}
    U\left( X \right)^{T} U\left( X \right)
    =X^{T} X + B
\end{equation} V=U(X) \begin{align}
    \int f \left(U(X)\right) g \left(U(X) \right) dX
    &=\int f \left(V\right) g \left(V \right) 
    \det (J\left(X,V \right)) dV
\end{align} J\left(X,V \right)","['integration', 'matrices', 'change-of-variable']"
3,similarity symmetric block-tridiagonal matrix,similarity symmetric block-tridiagonal matrix,,"I am solving a problem, in the middle of which I reached the following symmetric block-tridiagonal matrix. ‎ \begin{bmatrix}‎ 0 & B & &0  \\‎ B^T & 0‎ & ‎\ddots‎  \\‎ ‎&‎\ddots‎& ‎\ddots‎&B \\‎  0 ‎& &  B^T & 0 ‎\end{bmatrix} ‎ Where $B$ is an invertible square matrix and size $0$ is equal to size $B$ (matrix B is not necessarily normal). Can I make this matrix similar to the following  tridiagonal block matrix as shown below? \begin{bmatrix}‎ 0 & C & &0  \\‎ C & 0‎ & ‎\ddots‎  \\‎ ‎&‎\ddots‎& ‎\ddots‎&C \\‎  0 ‎& &  C & 0 ‎\end{bmatrix} ‎ in which size $C$ and $0$ equals $B$ .","I am solving a problem, in the middle of which I reached the following symmetric block-tridiagonal matrix. ‎ ‎ Where is an invertible square matrix and size is equal to size (matrix B is not necessarily normal). Can I make this matrix similar to the following  tridiagonal block matrix as shown below? ‎ in which size and equals .","\begin{bmatrix}‎
0 & B & &0  \\‎
B^T & 0‎ & ‎\ddots‎  \\‎
‎&‎\ddots‎& ‎\ddots‎&B \\‎
 0 ‎& &  B^T & 0
‎\end{bmatrix} B 0 B \begin{bmatrix}‎
0 & C & &0  \\‎
C & 0‎ & ‎\ddots‎  \\‎
‎&‎\ddots‎& ‎\ddots‎&C \\‎
 0 ‎& &  C & 0
‎\end{bmatrix} C 0 B","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'symmetric-matrices', 'similar-matrices']"
4,Visualizing the meaning of a complex determinant,Visualizing the meaning of a complex determinant,,"Context: A common visualization of the meaning of a (real) determinant is the area (or volume, in higher dimensions) enclosed by the image of basis vectors under matrix transformation: Question: But what if the matrix and determinant in question are complex valued?  How do we visualize the meaning of the determinant of these cases? Example: For example, the determinant of a unitary matrix $U$ satisfies $$ \det(U) = e^{i\phi} $$ for some $\phi$ .  Is there a way to visualize what the complex determinant of $U$ is, in this case?","Context: A common visualization of the meaning of a (real) determinant is the area (or volume, in higher dimensions) enclosed by the image of basis vectors under matrix transformation: Question: But what if the matrix and determinant in question are complex valued?  How do we visualize the meaning of the determinant of these cases? Example: For example, the determinant of a unitary matrix satisfies for some .  Is there a way to visualize what the complex determinant of is, in this case?","U 
\det(U) = e^{i\phi}
 \phi U","['matrices', 'determinant', 'visualization']"
5,Characterize hypersurface given by $\det(\sum_{k=1}^n x_k A_k) = 0$,Characterize hypersurface given by,\det(\sum_{k=1}^n x_k A_k) = 0,"Consider $n$ matrices $A_k \in \mathbb R^{n \times n}$ . The equation $\det\left(\sum_{k=1}^n x_k A_k\right) = 0$ describes (in general) a hypersurface when $x \in \mathbb R^n$ . Below is an example of such surface in $\mathbb R^3$ given by $\det \begin{pmatrix} x_1 + x_2 & x_3 & 0\\ x_2 & x_2 + x_3 & x_1\\ x_2 & 0 & x_1 + x_3 \end{pmatrix} = 0$ Since $\det\left(\sum_{k=1}^n x_k A_k\right) = 0$ implies $\det\left(\sum_{k=1}^n (\lambda x_k) A_k\right) = 0$ it is enough to consider this equation on the unit sphere $x^2 = 1$ . I'm wondering if these surfaces are known and studied. Particularly I'm interested in testing whether for given $A_k$ and starting and terminating points $A, B$ there exists a continuous path $x(t)$ such that $x(0) = A, \quad x(1) = B$ which never crosses the surface: $\det (A_k x_k(t)) \neq 0, \forall t \in [0, 1]$ . My observations: If all matrices are diagonal: $A_k = \operatorname{diag}_i(d_i^{(k)})$ the determinant is easy to compute: $$ 0 = \det (\sum_{k=1}^n x_k A_k) = \prod_{i=1}^n \sum_{k=1}^n x_k d_i^{(k)} $$ The solution is a union of $n$ planes given by $\sum_{k=1}^n x_k d_i^{(k)} = 0$ . For nonsingular $S, T \in \mathbb R^{n \times n}$ the substitution $A_k \to S A_k T$ does not change the equation. Combined with the first observation this gives a solution in case when all $A_k$ can be simultaneously diagonalized by a pair of matrices. If matrices are symmetric and positive definite then $\mathbb R_+^n$ does not intersect with the surface. This problem arose when I tried to solve numerically a quadratic system of equations given by $x^\top B_k x = c_k$ and observed that the numerical method ""hits a wall"" at a point where jacobian of the system vanishes. In my particular case matrices $B_k$ are indefinite.","Consider matrices . The equation describes (in general) a hypersurface when . Below is an example of such surface in given by Since implies it is enough to consider this equation on the unit sphere . I'm wondering if these surfaces are known and studied. Particularly I'm interested in testing whether for given and starting and terminating points there exists a continuous path such that which never crosses the surface: . My observations: If all matrices are diagonal: the determinant is easy to compute: The solution is a union of planes given by . For nonsingular the substitution does not change the equation. Combined with the first observation this gives a solution in case when all can be simultaneously diagonalized by a pair of matrices. If matrices are symmetric and positive definite then does not intersect with the surface. This problem arose when I tried to solve numerically a quadratic system of equations given by and observed that the numerical method ""hits a wall"" at a point where jacobian of the system vanishes. In my particular case matrices are indefinite.","n A_k \in \mathbb R^{n \times n} \det\left(\sum_{k=1}^n x_k A_k\right) = 0 x \in \mathbb R^n \mathbb R^3 \det \begin{pmatrix}
x_1 + x_2 & x_3 & 0\\
x_2 & x_2 + x_3 & x_1\\
x_2 & 0 & x_1 + x_3
\end{pmatrix} = 0 \det\left(\sum_{k=1}^n x_k A_k\right) = 0 \det\left(\sum_{k=1}^n (\lambda x_k) A_k\right) = 0 x^2 = 1 A_k A, B x(t) x(0) = A, \quad x(1) = B \det (A_k x_k(t)) \neq 0, \forall t \in [0, 1] A_k = \operatorname{diag}_i(d_i^{(k)}) 
0 = \det (\sum_{k=1}^n x_k A_k) = \prod_{i=1}^n \sum_{k=1}^n x_k d_i^{(k)}
 n \sum_{k=1}^n x_k d_i^{(k)} = 0 S, T \in \mathbb R^{n \times n} A_k \to S A_k T A_k \mathbb R_+^n x^\top B_k x = c_k B_k","['matrices', 'manifolds', 'determinant', 'quadratic-forms', 'jacobian']"
6,Matrix Equation $A^* B A = C $ solved for $A$,Matrix Equation  solved for,A^* B A = C  A,"Is there a standardized way to solve $A^* B A = C $ for $A$ if $A$ is a complex and square matrice, and $B$ and $C$ are real-valued and square matrices. $A^*$ is the conjugate transpose of $A$ . Is there a special name for such kinds of equations? Edit: Information that might help: The Matrix B and C are conjugate transpose auto-correlation matrices $B^* = b \cdot b^*  \\ C^* = c \cdot c^* $","Is there a standardized way to solve for if is a complex and square matrice, and and are real-valued and square matrices. is the conjugate transpose of . Is there a special name for such kinds of equations? Edit: Information that might help: The Matrix B and C are conjugate transpose auto-correlation matrices","A^* B A = C  A A B C A^* A B^* = b \cdot b^*  \\
C^* = c \cdot c^*
","['linear-algebra', 'matrices', 'matrix-equations']"
7,Show that a certain space of banded matrices with nonnegative determinants is connected and closed,Show that a certain space of banded matrices with nonnegative determinants is connected and closed,,"Let $n$ and $k$ be two positive integers, with $n \geq 2$ and $k \geq 1$ . Let $l=\{{l_i}\}$ be an $nk$ -dimensional positive real vector. Let $A_{n,k}(l)={(a_{i,j})}_{1 \leq i,j \leq nk}$ be the following $nk \times nk$ matrix: \begin{equation} a_{i,j}= \begin{cases} l_i&\text{if }\quad i=j,\\ -1&\text{if }\quad 1 \leq j-i \leq n-1,\\ -1&\text{if }\quad i \geq nk-n+2\ \text{ and }\ j+nk-i \leq n-1,\\ 0&\text{otherwise}. \end{cases} \end{equation} That is, $$ A_{n,k}(l) = \pmatrix{ l_1 & -1 & \cdots &-1 &0 &\cdots &0\\ 0 & l_2 & -1 &\ddots &-1 &\ddots &\vdots\\ \vdots & 0 & l_3 &-1 &\ddots &\ddots &0\\ 0 &\ddots &\ddots &\ddots &\ddots &\ddots &-1\\ -1 &0 &\ddots &\ddots &\ddots &\ddots &\vdots \\ \vdots& \ddots& \ddots&\ddots&0&l_{kn-1}&-1\\ -1 & \cdots & -1 & 0 & \cdots & 0  & l_{kn}}, $$ where the lower triangular bottom-left block has size $(n-1)\times(n-1)$ . Denote the set of $l$ that $\det(A_{n,k}(l)) \geq 0$ be $L$ . I want to show $L$ is a connected and closed set. Any comment and suggestion are welcome.","Let and be two positive integers, with and . Let be an -dimensional positive real vector. Let be the following matrix: That is, where the lower triangular bottom-left block has size . Denote the set of that be . I want to show is a connected and closed set. Any comment and suggestion are welcome.","n k n \geq 2 k \geq 1 l=\{{l_i}\} nk A_{n,k}(l)={(a_{i,j})}_{1 \leq i,j \leq nk} nk \times nk \begin{equation}
a_{i,j}=
\begin{cases}
l_i&\text{if }\quad i=j,\\
-1&\text{if }\quad 1 \leq j-i \leq n-1,\\
-1&\text{if }\quad i \geq nk-n+2\ \text{ and }\ j+nk-i \leq n-1,\\
0&\text{otherwise}.
\end{cases}
\end{equation} 
A_{n,k}(l) = \pmatrix{
l_1 & -1 & \cdots &-1 &0 &\cdots &0\\
0 & l_2 & -1 &\ddots &-1 &\ddots &\vdots\\
\vdots & 0 & l_3 &-1 &\ddots &\ddots &0\\
0 &\ddots &\ddots &\ddots &\ddots &\ddots &-1\\
-1 &0 &\ddots &\ddots &\ddots &\ddots &\vdots \\
\vdots& \ddots& \ddots&\ddots&0&l_{kn-1}&-1\\
-1 & \cdots & -1 & 0 & \cdots & 0  & l_{kn}},
 (n-1)\times(n-1) l \det(A_{n,k}(l)) \geq 0 L L","['linear-algebra', 'general-topology', 'matrices']"
8,"Similar concept for ""nilpotent"" matrix but gives the identity and not 0?","Similar concept for ""nilpotent"" matrix but gives the identity and not 0?",,"Is there a term for a square matrix $E$ such that $E^k=I$ for some positive integer $k$ ? To provide context: I was experimenting with permutation matrices and discovered that they satisfy the interesting property above. I have not proved/disproved this claim, I am looking for some hints and suspect that it has a nice name. Initially I thought it would be called ""unipotent"" (like how ""nilpotent"" is defined) but that's not it.","Is there a term for a square matrix such that for some positive integer ? To provide context: I was experimenting with permutation matrices and discovered that they satisfy the interesting property above. I have not proved/disproved this claim, I am looking for some hints and suspect that it has a nice name. Initially I thought it would be called ""unipotent"" (like how ""nilpotent"" is defined) but that's not it.",E E^k=I k,"['matrices', 'permutations', 'nilpotence']"
9,Integral matrices commute modulo $p^k$,Integral matrices commute modulo,p^k,"Given an $n \times n$ integral matrix $A \in M_n(\mathbb{Z})$ , denote by $A_k \in M_n(\mathbb{Z}/p^k \mathbb{Z})$ its reduction modulo $p^k$ . Now let $A, B \in M_n(\mathbb{Z})$ be be such that $A_k, B_k$ commute in $M_n(\mathbb{Z}/p^k\mathbb{Z})$ . What can we say about $A$ and $B$ ? I am looking for a statement of the form: there exist $A', B' \in M_n(\mathbb{Z})$ such that $A_k = A'_k, B_k = B'_k$ and $A'$ commutes with $B'$ . Or maybe this is too much to ask but we can still find commuting $A', B'$ such that $A_l = A'_l, B_l = B'_l$ , where $l$ is some function of $k$ ? What if we further require that $A_k, B_k \in GL_n(\mathbb{Z}/p^k\mathbb{Z})$ ? The question of whether almost-commuting matrices are close to commuting matrices has an enormous literature, but in everything I could find the matrices are complex and the ""almost"" and ""close"" is formalized in terms of the operator, Hilbert-Schmidt or Frobenius norm. Here I am asking the same question, but for integral matrices, where the ""almost"" and ""closed"" are formalized via equality modulo $p^k$ .","Given an integral matrix , denote by its reduction modulo . Now let be be such that commute in . What can we say about and ? I am looking for a statement of the form: there exist such that and commutes with . Or maybe this is too much to ask but we can still find commuting such that , where is some function of ? What if we further require that ? The question of whether almost-commuting matrices are close to commuting matrices has an enormous literature, but in everything I could find the matrices are complex and the ""almost"" and ""close"" is formalized in terms of the operator, Hilbert-Schmidt or Frobenius norm. Here I am asking the same question, but for integral matrices, where the ""almost"" and ""closed"" are formalized via equality modulo .","n \times n A \in M_n(\mathbb{Z}) A_k \in M_n(\mathbb{Z}/p^k \mathbb{Z}) p^k A, B \in M_n(\mathbb{Z}) A_k, B_k M_n(\mathbb{Z}/p^k\mathbb{Z}) A B A', B' \in M_n(\mathbb{Z}) A_k = A'_k, B_k = B'_k A' B' A', B' A_l = A'_l, B_l = B'_l l k A_k, B_k \in GL_n(\mathbb{Z}/p^k\mathbb{Z}) p^k","['linear-algebra', 'abstract-algebra', 'matrices', 'modular-arithmetic', 'matrix-equations']"
10,Numerical methods to minimize a matrix function,Numerical methods to minimize a matrix function,,"I'm faced with the problem \begin{align*} \min_{A\in\mathbb{R}^{n\times m}}\left\{f(A)+\lambda\lvert\lvert A\rvert\rvert_{S_p}^p\right\}, \end{align*} where $f:\mathbb{R}^{n\times m}\to\mathbb{R}$ is some generic matrix function, $\lambda>0$ is a fixed constant and $\lvert\lvert A\rvert\rvert_{S_p}$ is the $p$ -Schatten norm of $A$ defined as \begin{align*} \lvert\lvert A\rvert\rvert_{S_p} = \left(\sum_j|\sigma_j(A)|^p\right)^{1/p}, \end{align*} where $\sigma_j(A)$ are the singular values of $A$ (its a norm ony when $p\geq 1$ ; when $p=1$ , is the nuclear norm). There are some special cases when the problem has an explicit and unique solution. For example, if $p=1$ and $f(A)=g(A):=\lvert\lvert A-X\rvert\rvert_F^2$ for some fixed $X$ , where $\lvert\lvert \cdot\rvert\rvert_F$ is the Frobenius norm. Also, there are papers that describe numerical method to solve the problem when $f=g$ and $p<1$ . My question is: Do you know some generic algorithm to solve this problem for any $f$ and any $p$ ? I'm specially interested in the case $p<1$ , when the Schatten norm is not convex (and not a norm).","I'm faced with the problem where is some generic matrix function, is a fixed constant and is the -Schatten norm of defined as where are the singular values of (its a norm ony when ; when , is the nuclear norm). There are some special cases when the problem has an explicit and unique solution. For example, if and for some fixed , where is the Frobenius norm. Also, there are papers that describe numerical method to solve the problem when and . My question is: Do you know some generic algorithm to solve this problem for any and any ? I'm specially interested in the case , when the Schatten norm is not convex (and not a norm).","\begin{align*}
\min_{A\in\mathbb{R}^{n\times m}}\left\{f(A)+\lambda\lvert\lvert A\rvert\rvert_{S_p}^p\right\},
\end{align*} f:\mathbb{R}^{n\times m}\to\mathbb{R} \lambda>0 \lvert\lvert A\rvert\rvert_{S_p} p A \begin{align*}
\lvert\lvert A\rvert\rvert_{S_p} = \left(\sum_j|\sigma_j(A)|^p\right)^{1/p},
\end{align*} \sigma_j(A) A p\geq 1 p=1 p=1 f(A)=g(A):=\lvert\lvert A-X\rvert\rvert_F^2 X \lvert\lvert \cdot\rvert\rvert_F f=g p<1 f p p<1","['matrices', 'optimization', 'reference-request', 'numerical-optimization', 'matrix-norms']"
11,What can be said about the DFT given the following information?,What can be said about the DFT given the following information?,,"We are given the following information:- Let $z,w \in \mathbb{C^n}$ be signals, $z_{\delta} = \Omega_nz$ and $w_{\delta} = \Omega_nw$ be the discrete Fourier transformation of $z$ and $w$ . We define $\Omega_n$ as the following: $$\Omega_n = \frac{1}{\sqrt n} \begin{bmatrix}1&1&1&1&...&1\\1&\omega&\omega^2&\omega^3&...&\omega^{n-1}\\1&\omega^2&\omega^4&\omega^6&...&\omega^{2(n-1)}\\1&\omega^3&\omega^6&\omega^9&...&\omega^{3(n-1)}\\...&...&...&...&...&...\\1&\omega^{n-1}&\omega^{2(n-1)}&\omega^{3(n-1)}&...&\omega^{(n-1)(n-1)}\end{bmatrix}$$ Given the fact that the columns of the DFT matrix are normalized, which of the following statements are true? We can say that $z_{{\delta}_{n-1}}$ describes the share of the highest frequencies of of the signal $z$ . In the rows of the DFT matrix $\Omega_n$ , you will find the eigenvectors in a circulant matrix. The convolution of $z$ and $w$ correspond to the elemental multiplication of the transformed signals: $z \cdot w= z_{\delta}⋅w_{\delta}$ For $2.$ I'm pretty sure that it's false because when we take the case of a four point clockwise DFT matrix, we can see that the rows don't contain the eigenvectors in a circulant matrix. For $3.$ I have a feeling that it's true but I'm not sure if I can explain it.","We are given the following information:- Let be signals, and be the discrete Fourier transformation of and . We define as the following: Given the fact that the columns of the DFT matrix are normalized, which of the following statements are true? We can say that describes the share of the highest frequencies of of the signal . In the rows of the DFT matrix , you will find the eigenvectors in a circulant matrix. The convolution of and correspond to the elemental multiplication of the transformed signals: For I'm pretty sure that it's false because when we take the case of a four point clockwise DFT matrix, we can see that the rows don't contain the eigenvectors in a circulant matrix. For I have a feeling that it's true but I'm not sure if I can explain it.","z,w \in \mathbb{C^n} z_{\delta} = \Omega_nz w_{\delta} = \Omega_nw z w \Omega_n \Omega_n = \frac{1}{\sqrt n} \begin{bmatrix}1&1&1&1&...&1\\1&\omega&\omega^2&\omega^3&...&\omega^{n-1}\\1&\omega^2&\omega^4&\omega^6&...&\omega^{2(n-1)}\\1&\omega^3&\omega^6&\omega^9&...&\omega^{3(n-1)}\\...&...&...&...&...&...\\1&\omega^{n-1}&\omega^{2(n-1)}&\omega^{3(n-1)}&...&\omega^{(n-1)(n-1)}\end{bmatrix} z_{{\delta}_{n-1}} z \Omega_n z w z \cdot w= z_{\delta}⋅w_{\delta} 2. 3.","['real-analysis', 'linear-algebra', 'matrices', 'fourier-analysis', 'transformation']"
12,Cartesian coordinates and Linear Transformation,Cartesian coordinates and Linear Transformation,,Equation 3.1(a) is a linear transformation but what is the meaning of 3.1(b) and 3.1(c)? Why should it satisfy these conditions?,Equation 3.1(a) is a linear transformation but what is the meaning of 3.1(b) and 3.1(c)? Why should it satisfy these conditions?,,"['matrices', 'linear-transformations', 'coordinate-systems']"
13,About the eigenvalues of real symmetric matrix,About the eigenvalues of real symmetric matrix,,"Let $S$ be an $n×n$ symmetric real matrix, and for some $v≠0$ , $\|Sv-αv\|<ε\|v\|$ ( $α$ is a real number). Then how can we prove that $S$ has at least one eigenvalue $λ$ with $|λ-α|<ε$ ? I am aware that the eigenvectors associated with different eigenvalues are orthogonal, but how can I apply this?  Thank you in advance.","Let be an symmetric real matrix, and for some , ( is a real number). Then how can we prove that has at least one eigenvalue with ? I am aware that the eigenvectors associated with different eigenvalues are orthogonal, but how can I apply this?  Thank you in advance.",S n×n v≠0 \|Sv-αv\|<ε\|v\| α S λ |λ-α|<ε,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
14,Confusion regarding the proof that a matrix that preserves angles between vectors from Poole's Linear Algebra: A Modern Introduction,Confusion regarding the proof that a matrix that preserves angles between vectors from Poole's Linear Algebra: A Modern Introduction,,"I'm working my way through Poole's Linear Algebra: A Modern Introduction , and I'm having some trouble fully wrapping my head around his solution to a problem regarding orthogonal matrices and their preservation of angles. The problem states: If $Q$ is an $n\times n$ matrix such that the angels $\angle(Q\mathbf{x},Q\mathbf{y})$ and $\angle(\mathbf{x},\mathbf{y})$ are equal for all vectors $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{R}^n$ , prove that $Q$ is an orthogonal matrix. Poole's solution is as follows: The given condition means that for all vectors $\mathbf{x}$ and $\mathbf{y}$ , $$\frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert}$$ Let $\mathbf{q}_i$ be the $i^\text{th}$ column of $Q$ . Then $\mathbf{q}_i=Q\mathbf{e}_i$ , where the $\mathbf{e}_i$ are the standard basis vectors. Since the $\mathbf{e}_i$ are orthonormal and the $\mathbf{q}_i$ are unit vectors, we have $$\mathbf{q}_i\cdot \mathbf{q}_j=\frac{\mathbf{q}_i\cdot \mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert}=\frac{\mathbf{e}_i\cdot \mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\mathbf{e}_i\cdot \mathbf{e}_j=\begin{cases}0 & i\neq j \\ 1 & i=j\end{cases}$$ It follows that $Q$ is an orthogonal matrix. Nearly all of this reasoning makes sense to me; given that the $\mathbf{q}_i$ are unit vectors, the fact that $Q$ follows logically from the requirement that $$\frac{\mathbf{e}_i\cdot\mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\frac{Q\mathbf{e}_i\cdot Q\mathbf{e}_j}{\Vert Q\mathbf{e}_i\Vert\,\Vert Q\mathbf{e}_j\Vert}=\frac{\mathbf{q}_i\cdot\mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert}$$ and the definition of an orthogonal matrix. However, I'm not sure what actually requires that the $\mathbf{q}_i$ be unit vectors. The only other stipulation on $Q$ in the premise for the problem was that it be a square matrix, so I'm assuming that the condition that the $\mathbf{q}_i$ be unit vectors is somehow implicit in the relationship $\frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert}$ for all $\mathbf{x}$ and $\mathbf{y}$ . Can anybody help me understand Poole's justification for the $\mathbf{q}_i$ being unit vectors?","I'm working my way through Poole's Linear Algebra: A Modern Introduction , and I'm having some trouble fully wrapping my head around his solution to a problem regarding orthogonal matrices and their preservation of angles. The problem states: If is an matrix such that the angels and are equal for all vectors and in , prove that is an orthogonal matrix. Poole's solution is as follows: The given condition means that for all vectors and , Let be the column of . Then , where the are the standard basis vectors. Since the are orthonormal and the are unit vectors, we have It follows that is an orthogonal matrix. Nearly all of this reasoning makes sense to me; given that the are unit vectors, the fact that follows logically from the requirement that and the definition of an orthogonal matrix. However, I'm not sure what actually requires that the be unit vectors. The only other stipulation on in the premise for the problem was that it be a square matrix, so I'm assuming that the condition that the be unit vectors is somehow implicit in the relationship for all and . Can anybody help me understand Poole's justification for the being unit vectors?","Q n\times n \angle(Q\mathbf{x},Q\mathbf{y}) \angle(\mathbf{x},\mathbf{y}) \mathbf{x} \mathbf{y} \mathbb{R}^n Q \mathbf{x} \mathbf{y} \frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert} \mathbf{q}_i i^\text{th} Q \mathbf{q}_i=Q\mathbf{e}_i \mathbf{e}_i \mathbf{e}_i \mathbf{q}_i \mathbf{q}_i\cdot \mathbf{q}_j=\frac{\mathbf{q}_i\cdot \mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert}=\frac{\mathbf{e}_i\cdot \mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\mathbf{e}_i\cdot \mathbf{e}_j=\begin{cases}0 & i\neq j \\ 1 & i=j\end{cases} Q \mathbf{q}_i Q \frac{\mathbf{e}_i\cdot\mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\frac{Q\mathbf{e}_i\cdot Q\mathbf{e}_j}{\Vert Q\mathbf{e}_i\Vert\,\Vert Q\mathbf{e}_j\Vert}=\frac{\mathbf{q}_i\cdot\mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert} \mathbf{q}_i Q \mathbf{q}_i \frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert} \mathbf{x} \mathbf{y} \mathbf{q}_i","['linear-algebra', 'matrices', 'linear-transformations', 'orthogonality', 'orthogonal-matrices']"
15,Matrices with special properties and their relation to an interesting problem about cards.,Matrices with special properties and their relation to an interesting problem about cards.,,"For $k\in \mathbb{N}$ , $A$ , an $n$ x $n$ matrix has the following properties: Every entry belongs to the set $\{0,1,2,...,k \}$ , and, Every individual row and column adds up to $k$ . Question: Can we always permute just the rows (or just the columns) of $A$ to obtain a matrix which has all non-zero diagonal entries? Further, I encountered this question while solving the following recreational question on cards. Say I have an usual pack of $52$ cards ( $13$ cards of each type numbered from $1$ to $13$ ). In any arbitrary distribution of these cards into $13$ groups, can I always find an ordering of the groups such that group $i$ contains a card numbered $i$ ? I observed that if I form a matrix with the mentioned properties, where $n=13$ , and $k=4$ , and the $A_{ij}$ entry is the number of $j$ numbered cards in $i^{th}$ group, then the question on the matrix property is an equivalent of the question on cards. Finally, I have proved it is possible for any $k$ , and $n=3$ and $4$ . I would really appreciate further help. Thank you.","For , , an x matrix has the following properties: Every entry belongs to the set , and, Every individual row and column adds up to . Question: Can we always permute just the rows (or just the columns) of to obtain a matrix which has all non-zero diagonal entries? Further, I encountered this question while solving the following recreational question on cards. Say I have an usual pack of cards ( cards of each type numbered from to ). In any arbitrary distribution of these cards into groups, can I always find an ordering of the groups such that group contains a card numbered ? I observed that if I form a matrix with the mentioned properties, where , and , and the entry is the number of numbered cards in group, then the question on the matrix property is an equivalent of the question on cards. Finally, I have proved it is possible for any , and and . I would really appreciate further help. Thank you.","k\in \mathbb{N} A n n \{0,1,2,...,k \} k A 52 13 1 13 13 i i n=13 k=4 A_{ij} j i^{th} k n=3 4","['matrices', 'discrete-mathematics', 'card-games']"
16,Projection of a symmetric matrix onto the positive semidefinite (PSD) cone under the nuclear norm,Projection of a symmetric matrix onto the positive semidefinite (PSD) cone under the nuclear norm,,"Question: Given a symmetric matrix, $S$ , what is the solution to the optimization problem $$ \arg\min_{P \in \mathcal{S}_{\ge 0}} \| S - P \|_N $$ where $\| \cdot \|_N$ denotes the nuclear norm, i.e. the sum of the singular values, or equivalently in the case of a symmetric matrix, the sum of the absolute value of the eigenvalues? In other words, given a symmetric matrix $S$ , what is the closest matrix $P \succeq 0$ in the positive semidefinite cone (denoted $\mathcal{S}_{\ge 0}$ ) with respect to the nuclear norm ? Guess: I would guess that the answer is the same as for the spectral norm and the Frobenius norm, provided that the nuclear norm is unitarily invariant (is it? I don't know, and I can't find a reference). Namely the answer in those cases is apparently: $$ V \Lambda^+ V^\top $$ where $V \Lambda V^\top$ is the eigenvalue decomposition of $S$ , and $V^+$ is the diagonal matrix that is the same as $\Lambda$ except that any negative entries have been replaced with zero. The following argument is obviously wrong, but imagine we would have that for any symmetric matrix $R = U K U^\top$ , after applying a rotation to get $\tilde{R} = V K V^\top$ , we would still have that (in fantasy land) $||S - R||_N = ||S - \tilde{R}||_N$ , then we could assume without loss of generality that $U = V = I$ , and then so calculating the nuclear norm of the difference would reduce to finding the nuclear norm $|| \Lambda - K ||_N$ . But this is easy, since it's just the sum of the absolute values of the diagonals, $\sum_i| \lambda_i - \kappa_i|$ , and then the only way to minimize this (is the following claim actually true?), while ensuring that all of the $\kappa_i \ge 0$ , is to take $\kappa_i = \max\{ \lambda_i , 0 \}$ , i.e. $K = \Lambda^+$ . Of course the claim about taking $V=U=I$ without loss of generality is obviously wrong, since then there would be no unique minimizer/no unique solution to the optimization problem, but the PSD cone is convex and the nuclear norm is a convex function. However, it would seem to suffice to be able to show that $$ || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N$$ for any $U$ orthogonal and $U \not=V$ . So I guess two claims of questionable merit: Claim 1: The minimizer of $$ \min_{i, \kappa_i \ge 0} \sum_{i} | \lambda_i - \kappa_i | $$ is $\kappa_i = \max \{0, \lambda_i\}$ for all $i$ . Claim 2: $$ || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N$$ for any $U$ orthogonal and $U \not=V$ . (And any diagonal matrix $K$ .) The following questions are all related but don't answer my question, because they either involve projection onto a set different than the PSD cone, or projection with respect to a distance different from that induced by the nuclear norm (or w.r.t. an unspecified distance). So this question does not seem to be a duplicate of any of these. Orthogonal projection on nuclear-norm ball How to project a symmetric matrix onto the cone of positive semidefinite (PSD) matrices? Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices Matrix projection onto positive semidefinite cone with respect to the spectral norm Projection of a matrix onto the spectral norm ball Projection of a real symmetric matrix to the cone of positive semidefinite (PSD) matrices ($ \mathcal{S}_{+} $)","Question: Given a symmetric matrix, , what is the solution to the optimization problem where denotes the nuclear norm, i.e. the sum of the singular values, or equivalently in the case of a symmetric matrix, the sum of the absolute value of the eigenvalues? In other words, given a symmetric matrix , what is the closest matrix in the positive semidefinite cone (denoted ) with respect to the nuclear norm ? Guess: I would guess that the answer is the same as for the spectral norm and the Frobenius norm, provided that the nuclear norm is unitarily invariant (is it? I don't know, and I can't find a reference). Namely the answer in those cases is apparently: where is the eigenvalue decomposition of , and is the diagonal matrix that is the same as except that any negative entries have been replaced with zero. The following argument is obviously wrong, but imagine we would have that for any symmetric matrix , after applying a rotation to get , we would still have that (in fantasy land) , then we could assume without loss of generality that , and then so calculating the nuclear norm of the difference would reduce to finding the nuclear norm . But this is easy, since it's just the sum of the absolute values of the diagonals, , and then the only way to minimize this (is the following claim actually true?), while ensuring that all of the , is to take , i.e. . Of course the claim about taking without loss of generality is obviously wrong, since then there would be no unique minimizer/no unique solution to the optimization problem, but the PSD cone is convex and the nuclear norm is a convex function. However, it would seem to suffice to be able to show that for any orthogonal and . So I guess two claims of questionable merit: Claim 1: The minimizer of is for all . Claim 2: for any orthogonal and . (And any diagonal matrix .) The following questions are all related but don't answer my question, because they either involve projection onto a set different than the PSD cone, or projection with respect to a distance different from that induced by the nuclear norm (or w.r.t. an unspecified distance). So this question does not seem to be a duplicate of any of these. Orthogonal projection on nuclear-norm ball How to project a symmetric matrix onto the cone of positive semidefinite (PSD) matrices? Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices Matrix projection onto positive semidefinite cone with respect to the spectral norm Projection of a matrix onto the spectral norm ball Projection of a real symmetric matrix to the cone of positive semidefinite (PSD) matrices ($ \mathcal{S}_{+} $)","S  \arg\min_{P \in \mathcal{S}_{\ge 0}} \| S - P \|_N  \| \cdot \|_N S P \succeq 0 \mathcal{S}_{\ge 0}  V \Lambda^+ V^\top  V \Lambda V^\top S V^+ \Lambda R = U K U^\top \tilde{R} = V K V^\top ||S - R||_N = ||S - \tilde{R}||_N U = V = I || \Lambda - K ||_N \sum_i| \lambda_i - \kappa_i| \kappa_i \ge 0 \kappa_i = \max\{ \lambda_i , 0 \} K = \Lambda^+ V=U=I  || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N U U \not=V  \min_{i, \kappa_i \ge 0} \sum_{i} | \lambda_i - \kappa_i |  \kappa_i = \max \{0, \lambda_i\} i  || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N U U \not=V K","['linear-algebra', 'matrices', 'convex-optimization', 'positive-semidefinite', 'nuclear-norm']"
17,How to prove that the singular value of product of two orthonormal matrix is related to the principal angles between their columns space?,How to prove that the singular value of product of two orthonormal matrix is related to the principal angles between their columns space?,,"Assume that $A$ , $B$ $\in R^{p\times d}$ both have orthonormal columns, then the vector of $d$ principal angles between their column spaces is give by $(\cos^{-1}\sigma_1,\cos^{-1}\sigma_2, \dots, \cos^{-1}\sigma_d)^T$ , where $\sigma_1 \ge \dots \ge \sigma_d$ . For the definition of principal angles, it's the copy from wiki. Let $V$ be an inner product space. Given two subspaces $\mathcal{U},\mathcal{W}$ with $\dim(\mathcal{U})=k\leq \dim(\mathcal{W}):=\ell$ , there exists then a sequence of $k$ angles $ 0 \le \theta_1 \le \theta_2 \le \cdots \le \theta_k \le \pi/2$ called the principal angles, the first one defined as $\theta_1:=\min \left\{ \arccos \left( \left. \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U}, w\in \mathcal{W}\right\}=\angle(u_1,w_1),$ where $\langle \cdot , \cdot \rangle $ is the inner product and $\|\cdot\|$ the induced norm. The vectors $u_1$ and $w_1$ are the corresponding ''principal vectors.'' The other principal angles and vectors are then defined recursively via $\theta_i:=\min \left\{ \left. \arccos \left( \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U},~w\in \mathcal{W},~u\perp u_j,~w \perp w_j \quad \forall j\in \{1,\ldots,i-1\} \right\}.$ The question is that how can I prove that $\sigma_1 \ge \dots \ge \sigma_d$ are actually the singular values of $B^TA$ ?","Assume that , both have orthonormal columns, then the vector of principal angles between their column spaces is give by , where . For the definition of principal angles, it's the copy from wiki. Let be an inner product space. Given two subspaces with , there exists then a sequence of angles called the principal angles, the first one defined as where is the inner product and the induced norm. The vectors and are the corresponding ''principal vectors.'' The other principal angles and vectors are then defined recursively via The question is that how can I prove that are actually the singular values of ?","A B \in R^{p\times d} d (\cos^{-1}\sigma_1,\cos^{-1}\sigma_2, \dots, \cos^{-1}\sigma_d)^T \sigma_1 \ge \dots \ge \sigma_d V \mathcal{U},\mathcal{W} \dim(\mathcal{U})=k\leq \dim(\mathcal{W}):=\ell k  0 \le \theta_1 \le \theta_2 \le \cdots \le \theta_k \le \pi/2 \theta_1:=\min \left\{ \arccos \left( \left. \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U}, w\in \mathcal{W}\right\}=\angle(u_1,w_1), \langle \cdot , \cdot \rangle  \|\cdot\| u_1 w_1 \theta_i:=\min \left\{ \left. \arccos \left( \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U},~w\in \mathcal{W},~u\perp u_j,~w \perp w_j \quad \forall j\in \{1,\ldots,i-1\} \right\}. \sigma_1 \ge \dots \ge \sigma_d B^TA","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
18,Show $\mathrm{det}(M)$ is well defined.,Show  is well defined.,\mathrm{det}(M),"One intuitive way to approach studying the determinant of a given matrix $M$ is to inspire its formal definition in the signed volume of applying the corresponding transformation to the unit $n$ -cube. By noticing that $M = E_1...E_nM^R$ where $E_i$ is an elementary matrix and $M^R$ is the row reduced echelon form of $M$ one could define $\mathrm{det}(M)$ as $\frac{\prod^{}_{} \mathrm{diag}(M^R)}{p}$ , where $p = \prod^{}_{}\mathrm{det}(E_i)$ . The issue is that one could arrive at $M^R$ in more than one way, so that the elementary matrices may differ, so might $p$ . How could one then prove that regardless of how one computes $M^R$ the product of the resulting elementary matrices will always be the same, that is, how could one prove that the above definition of the determinant is well defined? Would appreciate any help.","One intuitive way to approach studying the determinant of a given matrix is to inspire its formal definition in the signed volume of applying the corresponding transformation to the unit -cube. By noticing that where is an elementary matrix and is the row reduced echelon form of one could define as , where . The issue is that one could arrive at in more than one way, so that the elementary matrices may differ, so might . How could one then prove that regardless of how one computes the product of the resulting elementary matrices will always be the same, that is, how could one prove that the above definition of the determinant is well defined? Would appreciate any help.",M n M = E_1...E_nM^R E_i M^R M \mathrm{det}(M) \frac{\prod^{}_{} \mathrm{diag}(M^R)}{p} p = \prod^{}_{}\mathrm{det}(E_i) M^R p M^R,"['linear-algebra', 'matrices', 'proof-writing', 'linear-transformations', 'determinant']"
19,Complex symmetric matrices are normal,Complex symmetric matrices are normal,,"In the taxonomy at https://en.wikipedia.org/wiki/List_of_matrices , it says that complex symmetric matrices are normal. For real symmetric matrices, you can prove that they are hermitian hence normal. How would you prove the normality for a complex symmetric matrix ?","In the taxonomy at https://en.wikipedia.org/wiki/List_of_matrices , it says that complex symmetric matrices are normal. For real symmetric matrices, you can prove that they are hermitian hence normal. How would you prove the normality for a complex symmetric matrix ?",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
20,Useful theorems on eigenvalues of matrices summation,Useful theorems on eigenvalues of matrices summation,,"Suppose we have $M\geq 3$ numbers of symmetric matrices $\mathbf{A}_1, \mathbf{A}_2, \cdots, \mathbf{A}_M \in \mathbb{R}^{n\times n}$ . Each of them has $n$ numbers of real eigenvalues: $$ \lambda_1^1\leq \lambda^1_2 \cdots\leq \lambda^1_n\\ \lambda_1^2\leq \lambda^2_2 \cdots\leq \lambda^2_n\\ \vdots\\ \lambda_1^M\leq \lambda^M_2 \cdots\leq \lambda^M_n, $$ fortunately, we also have $\lambda^1_1>0$ and $\lambda^M_1>0$ ( $\mathbf{A}_1$ and $\mathbf{A}_M$ are positive definite). Then, we are interested in the eigenvalues of $$ \mathbf{S} = \mathbf{A}_1 + \mathbf{A}_2+ \cdots+ \mathbf{A}_M. $$ I have questions: If I want $\lambda_\text{min}(\mathbf{S})$ to be positive, are there any sufficient and necessary conditions on the eigenvalues of $\mathbf{A}_1, \mathbf{A}_2, \cdots, \mathbf{A}_M$ ? (i.e. Find the lower bound of $\lambda_\text{min}(\mathbf{S})$ ) more soft question: are there any useful theorems for the analysis of this?","Suppose we have numbers of symmetric matrices . Each of them has numbers of real eigenvalues: fortunately, we also have and ( and are positive definite). Then, we are interested in the eigenvalues of I have questions: If I want to be positive, are there any sufficient and necessary conditions on the eigenvalues of ? (i.e. Find the lower bound of ) more soft question: are there any useful theorems for the analysis of this?","M\geq 3 \mathbf{A}_1, \mathbf{A}_2, \cdots, \mathbf{A}_M \in \mathbb{R}^{n\times n} n 
\lambda_1^1\leq \lambda^1_2 \cdots\leq \lambda^1_n\\
\lambda_1^2\leq \lambda^2_2 \cdots\leq \lambda^2_n\\
\vdots\\
\lambda_1^M\leq \lambda^M_2 \cdots\leq \lambda^M_n,
 \lambda^1_1>0 \lambda^M_1>0 \mathbf{A}_1 \mathbf{A}_M 
\mathbf{S} = \mathbf{A}_1 + \mathbf{A}_2+ \cdots+ \mathbf{A}_M.
 \lambda_\text{min}(\mathbf{S}) \mathbf{A}_1, \mathbf{A}_2, \cdots, \mathbf{A}_M \lambda_\text{min}(\mathbf{S})","['real-analysis', 'linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
21,Matrices with rows from an arbitrary vector space (and multiplication by number matrices),Matrices with rows from an arbitrary vector space (and multiplication by number matrices),,"Matrices consist of elements of some field. However, if we have a matrix $A\in M_{m,n}(F)$ , it is sometimes useful to look at each row as a vector from $F^n$ , i.e., we can view the matrix as $$A=\begin{pmatrix}\vec r_1\\\vec r_2\\\vdots\\\vec r_m\end{pmatrix}.$$ (Doing the same thing with columns make sense, too. I will describe stuff in this post with rows, it can be easily changed for columns.) Sometimes it might be useful to do same thing with vectors from arbitrary vector space $V$ over a field $F$ . I.e., we can use notation $$\mathbf{B}=\begin{pmatrix}\vec v_1\\\vec v_2\\\vdots\\\vec v_m\end{pmatrix}.$$ I will use bold for ""matrices consisting of vectors"". This is just a different notation for ordered $n$ -tuple of vectors. But at least in some ways they are similar to matrices. For example, we can multiply such matrix by $A\in M_{k,m}(F)$ from the left to get $$A\cdot \mathbf{B},$$ which is the matrix where the $i$ -th row is the linear combination $\sum_{j=1}^m a_{ij}\vec v_j$ . (If we choose to work with columns, we would multiply from the right.) We can also add these matrices and multiply them by a scalar. With these definitions several properties of usual multiplication of matrices still hold - for the products that are allowed. For example, associativity $A(B\mathbf{C})=A(B\mathbf{C})$ or distributivity - both $(A+B)\mathbf{C}$ and $A(\mathbf{C}+\mathbf{D})$ . Also some properties which are valid for rank are still valid for dimension of the vector space generated by the rows. (For example, if $A$ is invertible then the ""rank"" of $\mathbf B$ and $A\mathbf B$ is the same. ""Rank"" of $A\mathbf B$ is bounded from above by the rank of $A$ and also by the ""rank"" of $\mathbf B$ .) We cannot multiply from the right, but we still can ""cancel"" on the right in the sense that if rows of $\mathbf B$ are linearly independent then $A\mathbf{B}=\mathbf{0}$ implies $A=0$ and $A_1\mathbf{B}=A_2\mathbf{B}$ implies $A_1=A_2$ . This notation can be used, for example, to make a compact notation for transition matrix between two bases by writing $\mathbf B_2=M\mathbf{B_1}$ . (And some proofs about transition matrices could be written in quite a compact way using this notation. Another possible advantage of this notation is that if we are careful only to do ""allowed"" multiplications, than we can use many properties of the usual matrix multiplication - which after some time spend with linear algebra and matrices are used almost automatically.) Question. Are there some text which use this formalism, where we can multiply by ""non-numerical"" matrices with rows (or columns) consists of vectors from arbitrary vector space (not necessary $n$ -tuples? Are there some situations when using this approach brings some advantages? Remark 1. In a sense, the above considerations can be bypassed easily. If we work with the type of ""matrices"" as above, we can simply take the finite dimensional subspace $S$ which contains rows of all matrices which we need at the moment. (For example, all rows which appear in some ""matrix"" identity we are looking at. Or if $V$ is finitely-dimensional, we can simply take $S=V$ .) If we fix some basis for $S$ , this induces and isomorphism between $S$ and $F^n$ , where $n=\dim(S)$ and we get a natural map $\mathbf{B} \mapsto B\in M_{m,n}$ . Once we fixed a basis for $S$ , any result concerning multiplication of ""matrices"" is now just the usual multiplication - we just need to transfer everything through this isomorphism. Still, I was curious whether sometimes it might be useful to avoid the need to fix a base and transfer things using the corresponding isomorphism. Remark 2. Matrix summability methods can be viewed as a multiplication of an infinite matrix (of dimensions "" $\mathbb N\times\mathbb N$ "") by a sequence considered as an infinite vector. Although in such ""matrices"" the rows do not have finitely many coordinates, this is different from what I have in mind, since I work here with matrices that have finitely many rows.","Matrices consist of elements of some field. However, if we have a matrix , it is sometimes useful to look at each row as a vector from , i.e., we can view the matrix as (Doing the same thing with columns make sense, too. I will describe stuff in this post with rows, it can be easily changed for columns.) Sometimes it might be useful to do same thing with vectors from arbitrary vector space over a field . I.e., we can use notation I will use bold for ""matrices consisting of vectors"". This is just a different notation for ordered -tuple of vectors. But at least in some ways they are similar to matrices. For example, we can multiply such matrix by from the left to get which is the matrix where the -th row is the linear combination . (If we choose to work with columns, we would multiply from the right.) We can also add these matrices and multiply them by a scalar. With these definitions several properties of usual multiplication of matrices still hold - for the products that are allowed. For example, associativity or distributivity - both and . Also some properties which are valid for rank are still valid for dimension of the vector space generated by the rows. (For example, if is invertible then the ""rank"" of and is the same. ""Rank"" of is bounded from above by the rank of and also by the ""rank"" of .) We cannot multiply from the right, but we still can ""cancel"" on the right in the sense that if rows of are linearly independent then implies and implies . This notation can be used, for example, to make a compact notation for transition matrix between two bases by writing . (And some proofs about transition matrices could be written in quite a compact way using this notation. Another possible advantage of this notation is that if we are careful only to do ""allowed"" multiplications, than we can use many properties of the usual matrix multiplication - which after some time spend with linear algebra and matrices are used almost automatically.) Question. Are there some text which use this formalism, where we can multiply by ""non-numerical"" matrices with rows (or columns) consists of vectors from arbitrary vector space (not necessary -tuples? Are there some situations when using this approach brings some advantages? Remark 1. In a sense, the above considerations can be bypassed easily. If we work with the type of ""matrices"" as above, we can simply take the finite dimensional subspace which contains rows of all matrices which we need at the moment. (For example, all rows which appear in some ""matrix"" identity we are looking at. Or if is finitely-dimensional, we can simply take .) If we fix some basis for , this induces and isomorphism between and , where and we get a natural map . Once we fixed a basis for , any result concerning multiplication of ""matrices"" is now just the usual multiplication - we just need to transfer everything through this isomorphism. Still, I was curious whether sometimes it might be useful to avoid the need to fix a base and transfer things using the corresponding isomorphism. Remark 2. Matrix summability methods can be viewed as a multiplication of an infinite matrix (of dimensions "" "") by a sequence considered as an infinite vector. Although in such ""matrices"" the rows do not have finitely many coordinates, this is different from what I have in mind, since I work here with matrices that have finitely many rows.","A\in M_{m,n}(F) F^n A=\begin{pmatrix}\vec r_1\\\vec r_2\\\vdots\\\vec r_m\end{pmatrix}. V F \mathbf{B}=\begin{pmatrix}\vec v_1\\\vec v_2\\\vdots\\\vec v_m\end{pmatrix}. n A\in M_{k,m}(F) A\cdot \mathbf{B}, i \sum_{j=1}^m a_{ij}\vec v_j A(B\mathbf{C})=A(B\mathbf{C}) (A+B)\mathbf{C} A(\mathbf{C}+\mathbf{D}) A \mathbf B A\mathbf B A\mathbf B A \mathbf B \mathbf B A\mathbf{B}=\mathbf{0} A=0 A_1\mathbf{B}=A_2\mathbf{B} A_1=A_2 \mathbf B_2=M\mathbf{B_1} n S V S=V S S F^n n=\dim(S) \mathbf{B} \mapsto B\in M_{m,n} S \mathbb N\times\mathbb N","['linear-algebra', 'matrices', 'reference-request', 'vector-spaces', 'change-of-basis']"
22,Making a matrix positive definite,Making a matrix positive definite,,"Suppose $p \times p$ matrix $X$ is symmetric and not positive definite (PD). I want to find the minimum value for $\mu$ such that $X + \mu I_p$ is positive definite. To this end, I suppose we can solve a semidefinite program (SDP) of the form $$\begin{array}{ll} \text{minimize} & \mu\\ \text{subject to} & X + \mu I_p  \succeq 0\\ & \mu \geq 0\end{array}$$ where $A \succeq 0$ denotes that matrix $A$ is PD, and $I_p$ is the $p \times p$ identity matrix. Is there a more elegant approach to find $\mu$ using linear algebra? I am also curious if $\mu$ has any interpretation or not. Thanks for possible feedback!","Suppose matrix is symmetric and not positive definite (PD). I want to find the minimum value for such that is positive definite. To this end, I suppose we can solve a semidefinite program (SDP) of the form where denotes that matrix is PD, and is the identity matrix. Is there a more elegant approach to find using linear algebra? I am also curious if has any interpretation or not. Thanks for possible feedback!",p \times p X \mu X + \mu I_p \begin{array}{ll} \text{minimize} & \mu\\ \text{subject to} & X + \mu I_p  \succeq 0\\ & \mu \geq 0\end{array} A \succeq 0 A I_p p \times p \mu \mu,"['linear-algebra', 'matrices', 'optimization', 'positive-definite', 'symmetric-matrices']"
23,Properties of function $f(A) = \text{tr}(A^{2})$ on set of real matrices,Properties of function  on set of real matrices,f(A) = \text{tr}(A^{2}),"There is function $f$ on set $M$ , such that $$ f(A) = \text{tr}(A^{2}), \space M = \{A \in \text{Mat}_{2\times2}(\mathbb{R}) \space | \space A^{T} = A, \space \text{det} A \neq 0\} $$ Is it bounded below or above? If so, does it reach its min/max? My ideas are the following: Let $ A =  \left( \begin{array}{ccc}  a & c \\  c & d \\ \end{array} \right) $ . Since $\text{det} A \neq 0 $ , we have $ad-c^{2} \neq 0 $ . Our function is $f(A) = \text{tr}(A^{2}) = a^2 + 2c^{2} + d^{2} $ . It is bounded from below by zero, but can't reach its min because $a, c, d$ can't all equal zero at the same time since $ad-c^{2} \neq 0 $ . It is not bounded from above because we can let $a = d = 0$ and any $ c \in \mathbb{R}\setminus\{0\}$ . Is this a correct solution and does it fully answer to given questions?","There is function on set , such that Is it bounded below or above? If so, does it reach its min/max? My ideas are the following: Let . Since , we have . Our function is . It is bounded from below by zero, but can't reach its min because can't all equal zero at the same time since . It is not bounded from above because we can let and any . Is this a correct solution and does it fully answer to given questions?","f M  f(A) = \text{tr}(A^{2}), \space M = \{A \in \text{Mat}_{2\times2}(\mathbb{R}) \space | \space A^{T} = A, \space \text{det} A \neq 0\}   A =  \left(
\begin{array}{ccc}
 a & c \\
 c & d \\
\end{array}
\right)  \text{det} A \neq 0  ad-c^{2} \neq 0  f(A) = \text{tr}(A^{2}) = a^2 + 2c^{2} + d^{2}  a, c, d ad-c^{2} \neq 0  a = d = 0  c \in \mathbb{R}\setminus\{0\}","['calculus', 'linear-algebra', 'matrices']"
24,Question on elementary Linear Algebra product,Question on elementary Linear Algebra product,,"I came across this in a problem: $$\frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}$$ Then, in order to simplify this, I thought the following: \begin{gather} \frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\color{red}{\left(\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\right)}\mathbf{v}^{T}\mathbf{A}^{-1}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\ \frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\color{red}{\left(\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\right)}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\ \frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\left(1+\color{red}{\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\right)}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\ \mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1} \end{gather} I figured that this could be done because of associativity and as the terms in $\color{red}{\text{red}}$ result in a scalar, it could be 'moved around'. Is this correct? If yes, does this always hold or are there any conditions on the vectors and matrices involved?","I came across this in a problem: Then, in order to simplify this, I thought the following: I figured that this could be done because of associativity and as the terms in result in a scalar, it could be 'moved around'. Is this correct? If yes, does this always hold or are there any conditions on the vectors and matrices involved?","\frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}} \begin{gather}
\frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\color{red}{\left(\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\right)}\mathbf{v}^{T}\mathbf{A}^{-1}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\
\frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}+\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\color{red}{\left(\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}\right)}}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\
\frac{\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}\left(1+\color{red}{\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\right)}{1+\mathbf{v}^{T}\mathbf{A}^{-1}\mathbf{u}}\\
\mathbf{u}\mathbf{v}^{T}\mathbf{A}^{-1}
\end{gather} \color{red}{\text{red}}","['matrices', 'vectors']"
25,Rank of a matrix over field extension.,Rank of a matrix over field extension.,,Let $A$ be a matrix over a field $\mathbb{F}$ and $\mathbb{K}$ be a field extension  of $\mathbb{F}$ . As I know that characteristic and minimal polynomial of $A$ over $\mathbb{F}$ and $\mathbb{K}$ are same. Now can I say that rank of the matrix over both field will be same? I tried with particular matrices and got the same rank. Please suggest. Thanks.,Let be a matrix over a field and be a field extension  of . As I know that characteristic and minimal polynomial of over and are same. Now can I say that rank of the matrix over both field will be same? I tried with particular matrices and got the same rank. Please suggest. Thanks.,A \mathbb{F} \mathbb{K} \mathbb{F} A \mathbb{F} \mathbb{K},"['matrices', 'matrix-rank']"
26,Calculate the Jordan normal form,Calculate the Jordan normal form,,"I have the matrix $A=\begin{bmatrix}  -2 & -3 & 6 \\ 1 & 2 & -2\\ -1 & -1 &3 \end{bmatrix}$ and I have to find the transformation matrix and its Jordan normal form. This is what I did so far: Char. polynomial: $p_A=(\lambda-1)^3$ so I have eigenvalue $\lambda=1$ . Then I calculated the kernel: $$\ker(A-I_3)=\ker\begin{pmatrix} -3 & -3 & 6 \\ 1 & 1 & -2 \\ -1 & -1 &2 \end{pmatrix} = \operatorname{span}\left\{\begin{pmatrix} -1\\1\\0\end{pmatrix};\begin{pmatrix} 2\\0\\1\end{pmatrix}\right\}$$ Then I have to calculate a third vector $v_3$ , such that: $(A-I_3)v_3=v_2$ but the system doesn't give me a solution for this vector, am I missing something?","I have the matrix and I have to find the transformation matrix and its Jordan normal form. This is what I did so far: Char. polynomial: so I have eigenvalue . Then I calculated the kernel: Then I have to calculate a third vector , such that: but the system doesn't give me a solution for this vector, am I missing something?",A=\begin{bmatrix}  -2 & -3 & 6 \\ 1 & 2 & -2\\ -1 & -1 &3 \end{bmatrix} p_A=(\lambda-1)^3 \lambda=1 \ker(A-I_3)=\ker\begin{pmatrix} -3 & -3 & 6 \\ 1 & 1 & -2 \\ -1 & -1 &2 \end{pmatrix} = \operatorname{span}\left\{\begin{pmatrix} -1\\1\\0\end{pmatrix};\begin{pmatrix} 2\\0\\1\end{pmatrix}\right\} v_3 (A-I_3)v_3=v_2,"['linear-algebra', 'matrices', 'jordan-normal-form']"
27,Changes in Perron eigenvector given a particular perturbation,Changes in Perron eigenvector given a particular perturbation,,"Let $A$ be a matrix with real and strictly positive elements ( $a_{ij}>0$ ). The row or columns of A do not add to one or to the same number. Let $x^A$ be the Perron vector of the matrix, normalized so that $\sum_i x_i^A = 1$ . Let $B$ be a new matrix with real and positive elements formed by perturbing $A$ in the following way: $b_{1j} > a_{1j}$ and $b_{ij} < a_{ij}$ for all $i \geq 2$ . Let $x^B$ be the Perron vector under the same normalization. Is it possible to show that $x_1^B > x_1^A$ and $x_i^B < x_i^A$ for all $i \geq 2$ ? Under what restrictions or conditions (properties of $A$ and $B$ ) would this be satisfied?","Let be a matrix with real and strictly positive elements ( ). The row or columns of A do not add to one or to the same number. Let be the Perron vector of the matrix, normalized so that . Let be a new matrix with real and positive elements formed by perturbing in the following way: and for all . Let be the Perron vector under the same normalization. Is it possible to show that and for all ? Under what restrictions or conditions (properties of and ) would this be satisfied?",A a_{ij}>0 x^A \sum_i x_i^A = 1 B A b_{1j} > a_{1j} b_{ij} < a_{ij} i \geq 2 x^B x_1^B > x_1^A x_i^B < x_i^A i \geq 2 A B,"['linear-algebra', 'matrices']"
28,A formula for the trace of matrix monomials,A formula for the trace of matrix monomials,,"I am looking for a general formula for the trace of monomials of the complex matrices $X=A+A^T$ and $P=i(-A+A^T)$ , where $$ A=\begin{pmatrix}      0 &\sqrt{1}  & 0         & 0         & \dots  & 0         \\       0 & 0        & \sqrt{2} & 0         & \dots  & 0          \\      0 & 0        & 0         &\sqrt{3}   & \dots  & 0         \\      0 & 0        &         0 & 0         & \ddots &\vdots     \\ \vdots &   \vdots & \vdots    &\vdots     & \ddots &\sqrt{n-1}   \\  0      &    0     &0          &0          &\dots   &0              \end{pmatrix}. $$ I became intrigued by the algebraic patterns exhibited by traces of monomials in $X$ and $P$ . Before list them, let me define the degree in $X$ (or $P$ ) of a matrix $M$ written as monomial, $\text{deg}_X(M)$ , as the the sum of exponents of $X$ . For example, $\text{deg}_X(X^2PX^3P^7X)=6$ and $\text{deg}_P(X^2PX^3P^7X)=8$ . Now a partial list: The trace is nonzero only if the degree in $X$ and $P$ is even and it is always an integer. The trace is invariant if $X$ and $P$ are exchanged. So, for example, $\text{trace}(P^2)=\text{trace}(X^2)$ . For $3\times 3$ matrices, $\text{trace}(X^{2n})=\text{trace}(P^{2n})=2\cdot 3^n$ , where $n$ is positive. There is no direct generalization for bigger matrices. For $4\times 4$ matrices, for example, $\text{trace}(X^{2n})= 12, 60, 324, 1764$ with $n=1,2,3,4$ . For a $n\times n$ matrix monomial, $\text{trace}(X^2)=n\cdot(n-1)$ . For $2\times 2$ matrix monomials, the nonzero traces are always $\pm 2$ . If the degree in $X$ and $P$ are fixed, the monomials have different nonzero traces only if they cannot be transformed into each other by a cyclic permutation or an exchange of $X$ and $P$ . Thus, for example $\text{trace}(XPXP)=-2$ and $\text{trace}(X^2P^2)=10$ for $3\times 3$ matrix monomials.","I am looking for a general formula for the trace of monomials of the complex matrices and , where I became intrigued by the algebraic patterns exhibited by traces of monomials in and . Before list them, let me define the degree in (or ) of a matrix written as monomial, , as the the sum of exponents of . For example, and . Now a partial list: The trace is nonzero only if the degree in and is even and it is always an integer. The trace is invariant if and are exchanged. So, for example, . For matrices, , where is positive. There is no direct generalization for bigger matrices. For matrices, for example, with . For a matrix monomial, . For matrix monomials, the nonzero traces are always . If the degree in and are fixed, the monomials have different nonzero traces only if they cannot be transformed into each other by a cyclic permutation or an exchange of and . Thus, for example and for matrix monomials.","X=A+A^T P=i(-A+A^T) 
A=\begin{pmatrix}
     0 &\sqrt{1}  & 0         & 0         & \dots  & 0         \\ 
     0 & 0        & \sqrt{2} & 0         & \dots  & 0          \\
     0 & 0        & 0         &\sqrt{3}   & \dots  & 0         \\
     0 & 0        &         0 & 0         & \ddots &\vdots     \\
\vdots &   \vdots & \vdots    &\vdots     & \ddots &\sqrt{n-1}   \\ 
0      &    0     &0          &0          &\dots   &0              \end{pmatrix}.
 X P X P M \text{deg}_X(M) X \text{deg}_X(X^2PX^3P^7X)=6 \text{deg}_P(X^2PX^3P^7X)=8 X P X P \text{trace}(P^2)=\text{trace}(X^2) 3\times 3 \text{trace}(X^{2n})=\text{trace}(P^{2n})=2\cdot 3^n n 4\times 4 \text{trace}(X^{2n})= 12, 60, 324, 1764 n=1,2,3,4 n\times n \text{trace}(X^2)=n\cdot(n-1) 2\times 2 \pm 2 X P X P \text{trace}(XPXP)=-2 \text{trace}(X^2P^2)=10 3\times 3","['linear-algebra', 'matrices', 'polynomials', 'trace']"
29,Cosine of a Linear System,Cosine of a Linear System,,"Let $\mathbf{A}$ be a $m\times n$ matrix, $\mathbf{y}$ and $\mathbf{b}$ are vectors of dimension $n\times 1$ . All of them have real entries. I am interested in solving the system of equations $$\mathbf{y} = \cos(\mathbf{Ax}+\mathbf{b})$$ where cosine is applied element wise on the vector $\mathbf{Ax}+\mathbf{b}$ . If cosine is not there, the solution is straightforward and I know this. The problem is that $\arccos(\mathbf{y})$ is one-to-many mapping. Thus for each $\mathbf{y}$ among that, we need to consider whether a solution exists or not. How can I solve this problem? I tried the following and got stuck after a while. Let $\mathbf{z} = \arccos(\mathbf{y})$ . Since $\cos(2d\pi+x)=\cos(x)$ for any integer $d$ , it follows that $$z_i = 2d_i\pi + r_i$$ for every positive integer $d_i$ and $r_i\in(0,2\pi)$ . Thus, the system of equations (after rearrangement in vector form) becomes $$2\pi\mathbf{d}+\mathbf{r}-\mathbf{b} = \mathbf{Ax}$$ Thus, we are now interested in solving a system of equations to find the integer vector $\mathbf{d}$ and $\mathbf{x}$ such that above equation is satisfied. Is this correct? Or intuitively, is there a vector in the column space of $\mathbf{A}$ such that it can be expressed as the LHS above for some integer vector $\mathbf{d}$ ? Are all this steps correct?","Let be a matrix, and are vectors of dimension . All of them have real entries. I am interested in solving the system of equations where cosine is applied element wise on the vector . If cosine is not there, the solution is straightforward and I know this. The problem is that is one-to-many mapping. Thus for each among that, we need to consider whether a solution exists or not. How can I solve this problem? I tried the following and got stuck after a while. Let . Since for any integer , it follows that for every positive integer and . Thus, the system of equations (after rearrangement in vector form) becomes Thus, we are now interested in solving a system of equations to find the integer vector and such that above equation is satisfied. Is this correct? Or intuitively, is there a vector in the column space of such that it can be expressed as the LHS above for some integer vector ? Are all this steps correct?","\mathbf{A} m\times n \mathbf{y} \mathbf{b} n\times 1 \mathbf{y} = \cos(\mathbf{Ax}+\mathbf{b}) \mathbf{Ax}+\mathbf{b} \arccos(\mathbf{y}) \mathbf{y} \mathbf{z} = \arccos(\mathbf{y}) \cos(2d\pi+x)=\cos(x) d z_i = 2d_i\pi + r_i d_i r_i\in(0,2\pi) 2\pi\mathbf{d}+\mathbf{r}-\mathbf{b} = \mathbf{Ax} \mathbf{d} \mathbf{x} \mathbf{A} \mathbf{d}","['linear-algebra', 'matrices']"
30,Woodbury Matrix Inversion,Woodbury Matrix Inversion,,"I am trying to invert a matrix using Woodbury identity. The inversion using Cholesky decomposition has the following pseudo-code: For $t=1,2,...$ $(1)\;\; \text{Read}\;x_t\in\mathbb{R}^n$ $(2)\;\;D_{t-1}=diag(|\theta_t^1|,...,|\theta_t^n|)$ $(3)\;\;A_t=A_t+x_tx_t'$ $(4)\;\;A_{t}^{-1}=\sqrt{D_{{t-1}}}\left(a\mathbf{I}+\sqrt{D_{{t-1}}}A_t\sqrt{D_{{t-1}}}\right)^{-1}\sqrt{D_{{t-1}}}$ $(5)\;\;\text{Read}\;y_t\in\mathbb{R}$ $(6)\;\;b=b+y_tx_t$ $(7)\;\;\theta_t=A_{t}^{-1}b$ End For The well known application of Sherman-Morrison on Recursive Least Squares is as follows: $$A_t^{-1}=(aI+x_tx_t')^{-1}=A_{t-1}^{-1} - \frac{(A_{t-1}^{-1}x_t)(A_{t-1}^{-1}x_t)'}{1+x_t'A_{t-1}^{-1}x_t}$$ where $A_0^{-1}=\frac{1}{a}I$ and we can set $A_t = A_{t-1}+\sum_{t=1}^Tx_tx_t'$ , which will lead to time complexity of $O(n^2)$ . The above technique is mentioned here . The two implementation in $\texttt{R}$ are as follows: X <-matrix(runif(1000),20,10) Y<-rnorm(20) a<- 0.1  Cholsky<-function(X,Y,a){   X <- as.matrix(X)   Y <- as.matrix(Y)   T <- nrow(X)   N <- ncol(X)   aI<- diag(a,N)   bt<- matrix(0,ncol=1,nrow=N)   for (t in 1:T){     xt<-X[t,]     At <- aI + (xt %*% t(xt))     InvA<-chol2inv(chol(At))     bt <- bt + (Y[t] * xt)     theta<- InvA %*% bt   }   return(theta) } Cholsky(X,Y,a)   Morrison<-function(X,Y,a){   X <- as.matrix(X)   Y <- as.matrix(Y)   T <- nrow(X)   N <- ncol(X)   At<-diag(1/a,N)   bt<- matrix(0,ncol=1,nrow=N)   for (t in 1:T){     xt<-X[t,]     At <- At + (xt %*% t(xt))     InvA <- At - ((t(xt%*%At)%*%(as.matrix(xt%*%At)))                     /as.numeric(xt%*%At%*%xt+1))     bt <- bt + (Y[t] * xt)     theta<- InvA %*% bt   }   return(theta) } Morrison(X,Y,a) They don't give the same result. So, perhaps I should not expect the implementations to be equivalent. I was wondering if I could invert the following (for the above case) more efficiently: $$A_t^{-1}=\left(D_{t-1}^{\frac{1}{2}}A_tD_{t-1}^{\frac{1}{2}}+aI\right)^{-1}$$ where $A_t=A_{t-1}+x_tx_t'$ and $A_0=\mathbf{0}$ . Essentially, I want to invert: $$M_t=\left(a\mathbf{I}+\sqrt{D_{{t-1}}}x_tx_t'\sqrt{D_{{t-1}}}\right)$$ I say: $$M_{t}^{-1}=M_{t-1}^{-1}-\frac{(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)'}{1+x_t'D_{t-1}^{\frac{1}{2}}M_{t-1}D_{t-1}^{\frac{1}{2}}x_t}$$ where $D_0=diag(\mathbf{1}$ ). So, $M^{-1}_0=\frac{1}{a}I$ RLS identity given above $(aI+u_tv_t)^{-1}$ uses $u=A_{t-1}^{-1}x_t,v_t=u_t'$ ,   I am using $u=M_{t-1}^{-1}D_{t-1}^{\frac{1}{2}}x_t,v_t=u_t'$ One may write the implementations in $\texttt{R}$ as follows: X <-matrix(runif(1000),20,10) Y<-rnorm(20) a<- 0.1  #Cholesky implementation X <- as.matrix(X) Y <- as.matrix(Y) T <- nrow(X) N <- ncol(X) bt<- matrix(0,ncol=1,nrow=N) At<- diag(0,N) I<- diag(a,N);Mt<-diag(1/a,N) theta0<- rep(1,N) for (t in 1:2){   xt<-X[t,]   Dt <- diag(sqrt(abs(as.numeric(theta0))))   At <- At + (xt %*% t(xt))   Mt <-  I + (Dt%*%At%*%Dt)   InvA <- chol2inv(chol( Mt ))    AAt<- Dt %*%InvA%*% Dt   bt <- bt + (Y[t] * xt)   theta0 <- AAt %*% bt   print(theta0) } Above is the correct implementation of the pseudo code. If I swap the following lines. I don't get the same answer. Mt <-  Mt + (Dt%*%At%*%Dt) InvA<- Mt - ((t(xt%*%Dt%*%Mt)%*%(as.matrix(xt%*%Dt%*%Mt)))                    /as.numeric(xt%*%Dt%*%Mt%*%t(as.matrix(xt%*%Dt))+1)) Why is that?","I am trying to invert a matrix using Woodbury identity. The inversion using Cholesky decomposition has the following pseudo-code: For End For The well known application of Sherman-Morrison on Recursive Least Squares is as follows: where and we can set , which will lead to time complexity of . The above technique is mentioned here . The two implementation in are as follows: X <-matrix(runif(1000),20,10) Y<-rnorm(20) a<- 0.1  Cholsky<-function(X,Y,a){   X <- as.matrix(X)   Y <- as.matrix(Y)   T <- nrow(X)   N <- ncol(X)   aI<- diag(a,N)   bt<- matrix(0,ncol=1,nrow=N)   for (t in 1:T){     xt<-X[t,]     At <- aI + (xt %*% t(xt))     InvA<-chol2inv(chol(At))     bt <- bt + (Y[t] * xt)     theta<- InvA %*% bt   }   return(theta) } Cholsky(X,Y,a)   Morrison<-function(X,Y,a){   X <- as.matrix(X)   Y <- as.matrix(Y)   T <- nrow(X)   N <- ncol(X)   At<-diag(1/a,N)   bt<- matrix(0,ncol=1,nrow=N)   for (t in 1:T){     xt<-X[t,]     At <- At + (xt %*% t(xt))     InvA <- At - ((t(xt%*%At)%*%(as.matrix(xt%*%At)))                     /as.numeric(xt%*%At%*%xt+1))     bt <- bt + (Y[t] * xt)     theta<- InvA %*% bt   }   return(theta) } Morrison(X,Y,a) They don't give the same result. So, perhaps I should not expect the implementations to be equivalent. I was wondering if I could invert the following (for the above case) more efficiently: where and . Essentially, I want to invert: I say: where ). So, RLS identity given above uses ,   I am using One may write the implementations in as follows: X <-matrix(runif(1000),20,10) Y<-rnorm(20) a<- 0.1  #Cholesky implementation X <- as.matrix(X) Y <- as.matrix(Y) T <- nrow(X) N <- ncol(X) bt<- matrix(0,ncol=1,nrow=N) At<- diag(0,N) I<- diag(a,N);Mt<-diag(1/a,N) theta0<- rep(1,N) for (t in 1:2){   xt<-X[t,]   Dt <- diag(sqrt(abs(as.numeric(theta0))))   At <- At + (xt %*% t(xt))   Mt <-  I + (Dt%*%At%*%Dt)   InvA <- chol2inv(chol( Mt ))    AAt<- Dt %*%InvA%*% Dt   bt <- bt + (Y[t] * xt)   theta0 <- AAt %*% bt   print(theta0) } Above is the correct implementation of the pseudo code. If I swap the following lines. I don't get the same answer. Mt <-  Mt + (Dt%*%At%*%Dt) InvA<- Mt - ((t(xt%*%Dt%*%Mt)%*%(as.matrix(xt%*%Dt%*%Mt)))                    /as.numeric(xt%*%Dt%*%Mt%*%t(as.matrix(xt%*%Dt))+1)) Why is that?","t=1,2,... (1)\;\; \text{Read}\;x_t\in\mathbb{R}^n (2)\;\;D_{t-1}=diag(|\theta_t^1|,...,|\theta_t^n|) (3)\;\;A_t=A_t+x_tx_t' (4)\;\;A_{t}^{-1}=\sqrt{D_{{t-1}}}\left(a\mathbf{I}+\sqrt{D_{{t-1}}}A_t\sqrt{D_{{t-1}}}\right)^{-1}\sqrt{D_{{t-1}}} (5)\;\;\text{Read}\;y_t\in\mathbb{R} (6)\;\;b=b+y_tx_t (7)\;\;\theta_t=A_{t}^{-1}b A_t^{-1}=(aI+x_tx_t')^{-1}=A_{t-1}^{-1} - \frac{(A_{t-1}^{-1}x_t)(A_{t-1}^{-1}x_t)'}{1+x_t'A_{t-1}^{-1}x_t} A_0^{-1}=\frac{1}{a}I A_t = A_{t-1}+\sum_{t=1}^Tx_tx_t' O(n^2) \texttt{R} A_t^{-1}=\left(D_{t-1}^{\frac{1}{2}}A_tD_{t-1}^{\frac{1}{2}}+aI\right)^{-1} A_t=A_{t-1}+x_tx_t' A_0=\mathbf{0} M_t=\left(a\mathbf{I}+\sqrt{D_{{t-1}}}x_tx_t'\sqrt{D_{{t-1}}}\right) M_{t}^{-1}=M_{t-1}^{-1}-\frac{(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)'}{1+x_t'D_{t-1}^{\frac{1}{2}}M_{t-1}D_{t-1}^{\frac{1}{2}}x_t} D_0=diag(\mathbf{1} M^{-1}_0=\frac{1}{a}I (aI+u_tv_t)^{-1} u=A_{t-1}^{-1}x_t,v_t=u_t' u=M_{t-1}^{-1}D_{t-1}^{\frac{1}{2}}x_t,v_t=u_t' \texttt{R}","['linear-algebra', 'matrices', 'inverse', 'numerical-linear-algebra']"
31,Eigenvalues of a matrix given its derivative.,Eigenvalues of a matrix given its derivative.,,"As noted in the comments, the following notations will be used: $\lambda_i$ is the $i^{th}$ eigenvalue (presumably listed by descending magnitude) $|v_i\rangle$ is the $i^{th}$ column eigenvector $\langle v_i|$ is the $i^{th}$ row eigenvector Suppose I have a large matrix $A(x,t)$ depending on two free parameters (so each input of $A$ is a continuous function of $x$ and $t$ ).  Let $$ B(x,t)=\frac \partial {\partial t}A(x,t)\\ C(x,t)=\frac \partial {\partial  x}B(x,t) $$ If I know all the eigenvalues and eigenvectors of $A$ , and I know all the eigenvalues and eigenvectors of $C$ , then can I use that information to find the complete eigenvalues and eigenvectors of $B$ ? I know that if such a relationship exists that it is non-trivial, since, for example, if $$ A=\sum \lambda_i |v_i\rangle\langle v_i|, $$ then $B$ is given by $$ B=\sum (\frac \partial {\partial t}\lambda_i) |v_i\rangle\langle v_i|+ \lambda_i  |\frac \partial {\partial t}v_i\rangle\langle v_i|+ \lambda_i  |v_i\rangle\langle \frac \partial {\partial t}v_i|, $$ which shows that $B$ is not diagonalized the same as $A$ , but surely a relationship must exist between them.","As noted in the comments, the following notations will be used: is the eigenvalue (presumably listed by descending magnitude) is the column eigenvector is the row eigenvector Suppose I have a large matrix depending on two free parameters (so each input of is a continuous function of and ).  Let If I know all the eigenvalues and eigenvectors of , and I know all the eigenvalues and eigenvectors of , then can I use that information to find the complete eigenvalues and eigenvectors of ? I know that if such a relationship exists that it is non-trivial, since, for example, if then is given by which shows that is not diagonalized the same as , but surely a relationship must exist between them.","\lambda_i i^{th} |v_i\rangle i^{th} \langle v_i| i^{th} A(x,t) A x t 
B(x,t)=\frac \partial {\partial t}A(x,t)\\
C(x,t)=\frac \partial {\partial  x}B(x,t)
 A C B 
A=\sum \lambda_i |v_i\rangle\langle v_i|,
 B 
B=\sum (\frac \partial {\partial t}\lambda_i) |v_i\rangle\langle v_i|+ \lambda_i  |\frac \partial {\partial t}v_i\rangle\langle v_i|+ \lambda_i  |v_i\rangle\langle \frac \partial {\partial t}v_i|,
 B A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
32,Limit of matrix function,Limit of matrix function,,"Let $A\in\mathbb{R}^{n\times n}$ be a matrix whose real eigenvalues have negative real part, and $X=X^\top\in\mathbb{R}^{n\times n}$ be a positive semidefinite matrix, i.e., $X\succeq 0$ . Consider the following matrix-valued function $$\tag{1}\label{1} F(X) = \left(\int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t\right)^{-1/2} X \left(\int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t\right)^{-1/2} $$ which maps positive (semi)definite matrices into a positive (semi)definite matrices. (Here $\cdot^{1/2}$ denotes the symmetric square root of a positive semidefinite matrix and $e^{\cdot}$ is the matrix exponential). Note that \eqref{1} is a continuous function which is not defined for any $X\succeq 0$ such that $\int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t$ is singular. However, I wonder whether it is possible to define a continuous extension of $F(\cdot)$ in the set of positive semidefinite matrices. In more formal terms, let $\bar{X}\succeq 0$ be such that $\int_0^{\infty} e^{At} \bar{X} e^{A^\top t} \mathrm{d} t$ is singular, and let $\{X_n\}_{n\ge 0}$ , $X_n\succ 0$ , be any sequence such that $\lim_{n\to \infty} X_n = \bar{X}$ . Does $ \lim_{n\to \infty} F(X_n)$ exist and is finite? My question is motivated by the special case of scalar matrices $A=\alpha I$ , $\alpha<0$ , for which it is easy to see that the above limit exists and is finite. For general $A$ 's, however, it is not clear to me whether this limit still exists. Numerical simulations suggest that the answer is again in the affirmative, but I was not able to prove it.","Let be a matrix whose real eigenvalues have negative real part, and be a positive semidefinite matrix, i.e., . Consider the following matrix-valued function which maps positive (semi)definite matrices into a positive (semi)definite matrices. (Here denotes the symmetric square root of a positive semidefinite matrix and is the matrix exponential). Note that \eqref{1} is a continuous function which is not defined for any such that is singular. However, I wonder whether it is possible to define a continuous extension of in the set of positive semidefinite matrices. In more formal terms, let be such that is singular, and let , , be any sequence such that . Does exist and is finite? My question is motivated by the special case of scalar matrices , , for which it is easy to see that the above limit exists and is finite. For general 's, however, it is not clear to me whether this limit still exists. Numerical simulations suggest that the answer is again in the affirmative, but I was not able to prove it.","A\in\mathbb{R}^{n\times n} X=X^\top\in\mathbb{R}^{n\times n} X\succeq 0 \tag{1}\label{1}
F(X) = \left(\int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t\right)^{-1/2} X \left(\int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t\right)^{-1/2}
 \cdot^{1/2} e^{\cdot} X\succeq 0 \int_0^{\infty} e^{At} X e^{A^\top t} \mathrm{d} t F(\cdot) \bar{X}\succeq 0 \int_0^{\infty} e^{At} \bar{X} e^{A^\top t} \mathrm{d} t \{X_n\}_{n\ge 0} X_n\succ 0 \lim_{n\to \infty} X_n = \bar{X} 
\lim_{n\to \infty} F(X_n) A=\alpha I \alpha<0 A","['linear-algebra', 'matrices', 'limits', 'convergence-divergence', 'matrix-calculus']"
33,Matrices with positive permutation products,Matrices with positive permutation products,,"Let $A=(a_{ij})$ be a $n\times n$ real matrix such that $$ \operatorname{sign}(\sigma) \cdot a_{1,\sigma(1)} a_{2,\sigma(2)} \ldots a_{n,\sigma(n)}\ge 0 $$ for all $\sigma\in S_n$ .  Is there a name for this rather small subclass of matrices?",Let be a real matrix such that for all .  Is there a name for this rather small subclass of matrices?,"A=(a_{ij}) n\times n 
\operatorname{sign}(\sigma) \cdot a_{1,\sigma(1)} a_{2,\sigma(2)} \ldots a_{n,\sigma(n)}\ge 0
 \sigma\in S_n","['linear-algebra', 'matrices', 'permutations', 'terminology', 'determinant']"
34,"If $M$ is symmetric posdef, then all diagonal band matrices derived from $M$ are also posdef?","If  is symmetric posdef, then all diagonal band matrices derived from  are also posdef?",M M,"Let $M$ be a positive definite and symmetric matrix: $$M = \left(\begin{array}{cccc}   a_{11} & a_{12} & \cdots & a_{1 n}\\   a_{21} & a_{22} & \cdots & a_{2 n}\\   \vdots & \vdots & \ddots & \vdots\\   a_{n 1} & a_{n 2} & \cdots & a_{n n} \end{array}\right)$$ where $a_{ij} = a_{ji}$ . Consider the band matrices $B^{(d)}$ with components $b_{i j}^{(d)} = a_{i j}$ if $|i-j| < d$ and $b_{i j}^{(d)}=0$ otherwise. Thus $B^{(d)}$ is a band matrix of ""width"" $d$ . For example $$B^{(2)} = \left(\begin{array}{ccccccc}   a_{11} & a_{12} & 0 & 0 & \cdots & 0 & 0\\   a_{21} & a_{22} & a_{32} & 0 & \cdots & 0 & 0\\   0 & a_{32} & a_{33} & a_{43} & \cdots & 0 & 0\\   0 & 0 & a_{34} & a_{44} & \cdots & 0 & 0\\   \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\   0 & 0 & 0 & 0 & \cdots & a_{n - 1, n - 1} & a_{n - 1, n}\\   0 & 0 & 0 & 0 & \cdots & a_{n, n - 1} & a_{n n} \end{array}\right)$$ Is $B^{(d)}$ positive definite? Motivation: I am trying to construct a preconditioner for an optimization problem. Computing the full Hessian is computationally expensive, but I can compute a few diagonals without problems. I want to use a band approximation like this as a preconditioner, but I need to be sure it will be posdef.","Let be a positive definite and symmetric matrix: where . Consider the band matrices with components if and otherwise. Thus is a band matrix of ""width"" . For example Is positive definite? Motivation: I am trying to construct a preconditioner for an optimization problem. Computing the full Hessian is computationally expensive, but I can compute a few diagonals without problems. I want to use a band approximation like this as a preconditioner, but I need to be sure it will be posdef.","M M = \left(\begin{array}{cccc}
  a_{11} & a_{12} & \cdots & a_{1 n}\\
  a_{21} & a_{22} & \cdots & a_{2 n}\\
  \vdots & \vdots & \ddots & \vdots\\
  a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right) a_{ij} = a_{ji} B^{(d)} b_{i j}^{(d)} = a_{i j} |i-j| < d b_{i j}^{(d)}=0 B^{(d)} d B^{(2)} = \left(\begin{array}{ccccccc}
  a_{11} & a_{12} & 0 & 0 & \cdots & 0 & 0\\
  a_{21} & a_{22} & a_{32} & 0 & \cdots & 0 & 0\\
  0 & a_{32} & a_{33} & a_{43} & \cdots & 0 & 0\\
  0 & 0 & a_{34} & a_{44} & \cdots & 0 & 0\\
  \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
  0 & 0 & 0 & 0 & \cdots & a_{n - 1, n - 1} & a_{n - 1, n}\\
  0 & 0 & 0 & 0 & \cdots & a_{n, n - 1} & a_{n n}
\end{array}\right) B^{(d)}","['linear-algebra', 'matrices', 'positive-definite']"
35,Are the eigenvalues of the sum of two positive definite matrices increased?,Are the eigenvalues of the sum of two positive definite matrices increased?,,"Let $A$ and $B$ be two $n \times n$ (symmetric) positive definite matrices, and denote the $k$ th smallest eigenvalue of a general $n \times n$ matrix by $\lambda_k(X)$ , $k = 1, 2, \ldots, n$ so that $$\lambda_1(X) \leq \lambda_2(X) \leq \cdots \leq \lambda_n(X).$$ I guess the following relation holds: $$\lambda_k(A + B) > \max\{\lambda_k(A), \lambda_k(B)\}, \; k = 1, 2, \ldots, n.$$ This looks intuitive but I have difficulty to prove it, any hints?","Let and be two (symmetric) positive definite matrices, and denote the th smallest eigenvalue of a general matrix by , so that I guess the following relation holds: This looks intuitive but I have difficulty to prove it, any hints?","A B n \times n k n \times n \lambda_k(X) k = 1, 2, \ldots, n \lambda_1(X) \leq \lambda_2(X) \leq \cdots \leq \lambda_n(X). \lambda_k(A + B) > \max\{\lambda_k(A), \lambda_k(B)\}, \; k = 1, 2, \ldots, n.","['linear-algebra', 'eigenvalues-eigenvectors']"
36,Finding the minimal value of a $4\times 4$ determinant,Finding the minimal value of a  determinant,4\times 4,"The question. Let $\xi=(\xi_1,\xi_2,\xi_3,\xi_4)\in\mathbb R^4$ be a vector with irrational coordinates. I am interested in finding the minimal value $\mu_\xi$ of $$\left\vert \det \begin{pmatrix} a_1 & a_2 & 0 & 1 \\ b_1 & b_2 & 1 & 0 \\ c_1 & c_2 & \xi_1 & \xi_3 \\ d_1 & d_2 & \xi_2 & \xi_4 \end{pmatrix} \right\vert,$$ for $a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2\in\mathbb Z$ , in terms of the area of the parallelepiped formed by the two vectors $X_i:=(a_i,b_i,c_i,d_i)$ , $i=1,2$ (which I assume linearly independent), in $\mathbb R^4$ . Let's call this area $D(X_1,X_2)$ . I know we have $$\begin{align*} D(X_1,X_2)^2 &= \Vert X_1\Vert^2\Vert X_2\Vert^2-(X_1\cdot X_2)^2 \end{align*},$$ but I have no clue on how to proceed from here. The conjecture. My hope (which would help the construction of another proof a lot) would be that if we chose the $\xi_i$ properly, we can show that the minimal value verifies \begin{equation} \mu_\xi\geqslant \frac c{D(X_1,X_2)^2}\qquad\qquad (1) \end{equation} where $c$ is a constant (it may depends on $\xi$ ). Final remarks. Despite the fact that I strongly believe that $(1)$ is true, any proof that would show that $$\mu_\xi\geqslant \frac c{D(X_1,X_2)^\gamma}$$ for a $\gamma<4$ would be of great interest.","The question. Let be a vector with irrational coordinates. I am interested in finding the minimal value of for , in terms of the area of the parallelepiped formed by the two vectors , (which I assume linearly independent), in . Let's call this area . I know we have but I have no clue on how to proceed from here. The conjecture. My hope (which would help the construction of another proof a lot) would be that if we chose the properly, we can show that the minimal value verifies where is a constant (it may depends on ). Final remarks. Despite the fact that I strongly believe that is true, any proof that would show that for a would be of great interest.","\xi=(\xi_1,\xi_2,\xi_3,\xi_4)\in\mathbb R^4 \mu_\xi \left\vert \det \begin{pmatrix} a_1 & a_2 & 0 & 1 \\ b_1 & b_2 & 1 & 0 \\ c_1 & c_2 & \xi_1 & \xi_3 \\ d_1 & d_2 & \xi_2 & \xi_4 \end{pmatrix} \right\vert, a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2\in\mathbb Z X_i:=(a_i,b_i,c_i,d_i) i=1,2 \mathbb R^4 D(X_1,X_2) \begin{align*} D(X_1,X_2)^2 &= \Vert X_1\Vert^2\Vert X_2\Vert^2-(X_1\cdot X_2)^2 \end{align*}, \xi_i \begin{equation}
\mu_\xi\geqslant \frac c{D(X_1,X_2)^2}\qquad\qquad (1)
\end{equation} c \xi (1) \mu_\xi\geqslant \frac c{D(X_1,X_2)^\gamma} \gamma<4","['real-analysis', 'matrices', 'determinant', 'maxima-minima', 'diophantine-approximation']"
37,Similarity transform with respect to a metric?,Similarity transform with respect to a metric?,,"Does this type of matrix expression have a name? $$D'S=CDC^{-1}$$ All the matrices are $n\times n$ and $S$ is a matrix consisting of the inner products of the basis vectors of $D'$ , so $D'$ is in general defined with respect to a nonorthonormal basis. Obviously, the matrix $D'S=E$ is a similar matrix to $D$ and if $S=\mathbf{1}$ (ie $D'$ is in an orthonormal basis) then $D'$ itself is just a similarity transform of $D$ , but is there terminology for the general case, something akin to "" $D'$ is similar to $D$ with respect to the metric $S$ ."" As a matter of context, I was trying to answer a question on Chem SE  about the relationship between atomic and molecular basis density matrices , when I arrived at an expression similar to the one above. I was hoping to find a way to describe how the bases of $D'$ and $D$ are related, as in the physical context they describe very similar properties. It seems to have something to do with nonorthogonality of the basis of the matrix $D'$ .","Does this type of matrix expression have a name? All the matrices are and is a matrix consisting of the inner products of the basis vectors of , so is in general defined with respect to a nonorthonormal basis. Obviously, the matrix is a similar matrix to and if (ie is in an orthonormal basis) then itself is just a similarity transform of , but is there terminology for the general case, something akin to "" is similar to with respect to the metric ."" As a matter of context, I was trying to answer a question on Chem SE  about the relationship between atomic and molecular basis density matrices , when I arrived at an expression similar to the one above. I was hoping to find a way to describe how the bases of and are related, as in the physical context they describe very similar properties. It seems to have something to do with nonorthogonality of the basis of the matrix .",D'S=CDC^{-1} n\times n S D' D' D'S=E D S=\mathbf{1} D' D' D D' D S D' D D',"['matrices', 'linear-transformations', 'mathematical-physics', 'chemistry']"
38,proof about commutative operators and T-cyclic vectors,proof about commutative operators and T-cyclic vectors,,"Let $V$ be a finite dimensional vector space over $F$. Let $T:V \to V$ be a linear operator. Prove that if every linear operator $U$ which commutes with $T$ is a polynomial of $T$, than $T$ has a $T$-cyclic vector. I don't really know where to start... can someone please point me in the right direction?","Let $V$ be a finite dimensional vector space over $F$. Let $T:V \to V$ be a linear operator. Prove that if every linear operator $U$ which commutes with $T$ is a polynomial of $T$, than $T$ has a $T$-cyclic vector. I don't really know where to start... can someone please point me in the right direction?",,['linear-algebra']
39,A matrix problem about egienvalue and trace,A matrix problem about egienvalue and trace,,"Consider an  $m\times m$ positive definite and Hermitian matrix $\mathbf{M}$ and an arbitrary $m\times n (m>n)$  para-unitary matrix  $\mathbf{R}$, i.e., $\mathbf{R}^H\mathbf{R}=\mathbf{I}_n$. then， is there any strict proof for: Let $\mu_1,...,\mu_{n}$ and $\lambda_1,...,\lambda_{n}$ denote the eigenvalues in descending order of $(\mathbf{R}^H\mathbf{M}\mathbf{R})^{-1}$ and $\mathbf{R}^H\mathbf{M}^{-1}\mathbf{R}$, respectively,  we have $\mu_k \le \lambda_k, \forall k$. ? I use matlab and generate many random matrices to check the result. I think it must be true. However, how to prove it?","Consider an  $m\times m$ positive definite and Hermitian matrix $\mathbf{M}$ and an arbitrary $m\times n (m>n)$  para-unitary matrix  $\mathbf{R}$, i.e., $\mathbf{R}^H\mathbf{R}=\mathbf{I}_n$. then， is there any strict proof for: Let $\mu_1,...,\mu_{n}$ and $\lambda_1,...,\lambda_{n}$ denote the eigenvalues in descending order of $(\mathbf{R}^H\mathbf{M}\mathbf{R})^{-1}$ and $\mathbf{R}^H\mathbf{M}^{-1}\mathbf{R}$, respectively,  we have $\mu_k \le \lambda_k, \forall k$. ? I use matlab and generate many random matrices to check the result. I think it must be true. However, how to prove it?",,"['matrices', 'matrix-equations', 'matrix-calculus', 'hessian-matrix']"
40,Show that a matrix is a multiple of the identity [duplicate],Show that a matrix is a multiple of the identity [duplicate],,"This question already has answers here : If $C$ commutes with certain matrices $A$ and $B$, why is $C$ a scalar multiple of the identity? (3 answers) Closed 5 years ago . Let $A$, $B$, $C$ three complex 2x2 matrices such that $$A^2=B^3=I\;(A\neq I\neq B),\quad ABA=B^{-1},\quad AC=CA,\quad BC=CB.$$ Show that $C=rI$ for some $r\in\mathbb{C}$. I got the minimal polynomials of $A$ and $B$, but I can't go further to find the minimal polynomial of $C$. Any suggestion?","This question already has answers here : If $C$ commutes with certain matrices $A$ and $B$, why is $C$ a scalar multiple of the identity? (3 answers) Closed 5 years ago . Let $A$, $B$, $C$ three complex 2x2 matrices such that $$A^2=B^3=I\;(A\neq I\neq B),\quad ABA=B^{-1},\quad AC=CA,\quad BC=CB.$$ Show that $C=rI$ for some $r\in\mathbb{C}$. I got the minimal polynomials of $A$ and $B$, but I can't go further to find the minimal polynomial of $C$. Any suggestion?",,"['linear-algebra', 'matrices']"
41,On the invertibility of the adjacency matrix of a graph,On the invertibility of the adjacency matrix of a graph,,"Which are the sufficient and necessary conditions for an undirected graph with no self edges (i.e. no loop of length $1$) to have an invertible adjacency matrix? In this case, the adjacency matrix is symmetric (i.e. $A = A^\top$). Moreover, all the diagonal elements are $0$ and there is a $1$ in both the entries $(i,j)$ and $(j,i)$, with $i\neq j$, if and only if vertices $i$ and $j$ are connected. A first necessary condition is the following: all vertices must have at least one connection, otherwise the relative row of $A$ is null. But what else?","Which are the sufficient and necessary conditions for an undirected graph with no self edges (i.e. no loop of length $1$) to have an invertible adjacency matrix? In this case, the adjacency matrix is symmetric (i.e. $A = A^\top$). Moreover, all the diagonal elements are $0$ and there is a $1$ in both the entries $(i,j)$ and $(j,i)$, with $i\neq j$, if and only if vertices $i$ and $j$ are connected. A first necessary condition is the following: all vertices must have at least one connection, otherwise the relative row of $A$ is null. But what else?",,"['matrices', 'graph-theory', 'spectral-graph-theory', 'adjacency-matrix']"
42,Proof about eigenspace of eigenvalue in power of matrix,Proof about eigenspace of eigenvalue in power of matrix,,"Upon studying the Jordan normal form I came across the problem of determining the Jordan normal form of powers of a single base matrix and in that context I was wondering what happens to the eigenspaces. I am not yet entirely sure if the statement below is correct which is why I tried to prove it myself and would now like to know if the below proof is actually correct. Also, since I have not written many proofs yet as of now, please let me know if I have done any notational mistakes or if anything could be stated in a more elegant way. Show that given any square-matrix $A\in \mathbb{K}^{n\times n}, \mathbb{K}$ is a field, $k \in \mathbb{N}$ the following statement holds true: $E_{\mu}(A^k) = \oplus_i E_{\lambda_i}(A)$ where $\lambda_i$ is an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$. ""$\supseteq$"": Let $\lambda_i$ be an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$ and $v \in E_{\lambda_i}(A)$. Then: $Av = \lambda_iv$. $A^kv = A^{k-1}Av = \lambda_iA^{k-1}v = \ldots = {\lambda_i}^kv \implies v \in E_{{\lambda_i}^k}(A^k) = E_{\mu}(A^k)$. ""$\subseteq$"": Assume $E_\mu(A^k) \supset \oplus_iE_{\lambda_i}(A)$. Then $\exists v \in E_\mu(A^k) \setminus \oplus_iE_{\lambda_i}(A)$. $A^kv = {\mu}v \implies A^kv^k = A^kvv^{k-1} = {\mu}vv^{k-1} = {\mu}v^k$ therefore $v^k$ is an eigenvector of $A^k$ and $\mu$. Note that $Av \neq \lambda_iv$ for any eigenvalue $\lambda_i$ of $A$. Thus: $(Av)^k \neq (\lambda_iv)^k \Leftrightarrow A^kv^k \neq \lambda_i^kv^k = \mu v^k$ which means that $v^k$ cannot be an eigenvector of $A^k$ and $\mu$ which is a contradiction. Such a vector $v$ can therefore not exist, implying that $E_\mu(A^k) \subseteq \oplus_iE_{\lambda_i}(A)$.","Upon studying the Jordan normal form I came across the problem of determining the Jordan normal form of powers of a single base matrix and in that context I was wondering what happens to the eigenspaces. I am not yet entirely sure if the statement below is correct which is why I tried to prove it myself and would now like to know if the below proof is actually correct. Also, since I have not written many proofs yet as of now, please let me know if I have done any notational mistakes or if anything could be stated in a more elegant way. Show that given any square-matrix $A\in \mathbb{K}^{n\times n}, \mathbb{K}$ is a field, $k \in \mathbb{N}$ the following statement holds true: $E_{\mu}(A^k) = \oplus_i E_{\lambda_i}(A)$ where $\lambda_i$ is an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$. ""$\supseteq$"": Let $\lambda_i$ be an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$ and $v \in E_{\lambda_i}(A)$. Then: $Av = \lambda_iv$. $A^kv = A^{k-1}Av = \lambda_iA^{k-1}v = \ldots = {\lambda_i}^kv \implies v \in E_{{\lambda_i}^k}(A^k) = E_{\mu}(A^k)$. ""$\subseteq$"": Assume $E_\mu(A^k) \supset \oplus_iE_{\lambda_i}(A)$. Then $\exists v \in E_\mu(A^k) \setminus \oplus_iE_{\lambda_i}(A)$. $A^kv = {\mu}v \implies A^kv^k = A^kvv^{k-1} = {\mu}vv^{k-1} = {\mu}v^k$ therefore $v^k$ is an eigenvector of $A^k$ and $\mu$. Note that $Av \neq \lambda_iv$ for any eigenvalue $\lambda_i$ of $A$. Thus: $(Av)^k \neq (\lambda_iv)^k \Leftrightarrow A^kv^k \neq \lambda_i^kv^k = \mu v^k$ which means that $v^k$ cannot be an eigenvector of $A^k$ and $\mu$ which is a contradiction. Such a vector $v$ can therefore not exist, implying that $E_\mu(A^k) \subseteq \oplus_iE_{\lambda_i}(A)$.",,"['matrices', 'eigenvalues-eigenvectors', 'exponentiation']"
43,Multiplicity of the zero eigenvalue when a symmetric matrix has $m$ identical rows,Multiplicity of the zero eigenvalue when a symmetric matrix has  identical rows,m,Let $A$ be a real symmetric matrix of order $n$.   Show that if $A$ has $m$ identical rows then $A$ has a zero eigenvalue of multiplicity at least $m-1$. My try : Since $A$ has $m$ identical rows by elementary row operations we have $m-1$ rows of $A$  equal to $0$. Hence $A$ has geometric multiplicity of $0$ to be equal to atleast $m-1$. Since $A$ is symmetric so the algebraic multiplicity of $0$ equals its geometric multiplicity and so it is atleast $m-1$. But how to show that it is equal to exactly $m-1$? Will you please help?,Let $A$ be a real symmetric matrix of order $n$.   Show that if $A$ has $m$ identical rows then $A$ has a zero eigenvalue of multiplicity at least $m-1$. My try : Since $A$ has $m$ identical rows by elementary row operations we have $m-1$ rows of $A$  equal to $0$. Hence $A$ has geometric multiplicity of $0$ to be equal to atleast $m-1$. Since $A$ is symmetric so the algebraic multiplicity of $0$ equals its geometric multiplicity and so it is atleast $m-1$. But how to show that it is equal to exactly $m-1$? Will you please help?,,['linear-algebra']
44,Equivalent forms Optimization,Equivalent forms Optimization,,"We were told to assume in class that the below optimization formulations are equivalent- $$\min_w\max_{\delta:||\delta||_F\leq\epsilon}||(X+\delta)w-y||_2^2$$ $$\min_{w}||Xw-y||_2^2+\lambda||w||_2^2 $$ for appropriately chosen $\lambda$. $X,\delta\in R^{m\times n},~w\in R^{n\times1},~y\in R^{m\times1}$ Can someone please explain why this is true? A reference paper pointing this out would also be appreciated.","We were told to assume in class that the below optimization formulations are equivalent- $$\min_w\max_{\delta:||\delta||_F\leq\epsilon}||(X+\delta)w-y||_2^2$$ $$\min_{w}||Xw-y||_2^2+\lambda||w||_2^2 $$ for appropriately chosen $\lambda$. $X,\delta\in R^{m\times n},~w\in R^{n\times1},~y\in R^{m\times1}$ Can someone please explain why this is true? A reference paper pointing this out would also be appreciated.",,"['matrices', 'optimization', 'convex-optimization', 'least-squares', 'quadratic-programming']"
45,When is the exponential map between matrices injective? [duplicate],When is the exponential map between matrices injective? [duplicate],,"This question already has an answer here : Is $\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n$ injective? (1 answer) Closed 4 years ago . Consider a complex matrix $A$. When does $e^A=e^B$ imply $A=B$? Is there any general statement that can be made as to when this holds? It is clearly not true in general, a trivial example being when $A$ and $B$ are diagonal in the same basis but with eigenvalues differing by $2\pi i\mathbb Z$. This is also mentioned in this question . This answer also seems to provide an answer to this question, but I'm not familiar with the theory of Lie algebras so I'm not sure, and I wouldn't know how to translate it into more elementary statements about matrices (if that is even possible).","This question already has an answer here : Is $\exp:\overline{\mathbb{M}}_n\to\mathbb{M}_n$ injective? (1 answer) Closed 4 years ago . Consider a complex matrix $A$. When does $e^A=e^B$ imply $A=B$? Is there any general statement that can be made as to when this holds? It is clearly not true in general, a trivial example being when $A$ and $B$ are diagonal in the same basis but with eigenvalues differing by $2\pi i\mathbb Z$. This is also mentioned in this question . This answer also seems to provide an answer to this question, but I'm not familiar with the theory of Lie algebras so I'm not sure, and I wouldn't know how to translate it into more elementary statements about matrices (if that is even possible).",,"['linear-algebra', 'matrices', 'exponential-function', 'matrix-exponential']"
46,Postive-semidefiniteness of matrix with entries $1/(a_i+a_j)$,Postive-semidefiniteness of matrix with entries,1/(a_i+a_j),"Let $a_1, \ldots, a_n$ be a set of positive numbers. Define a matrix $M_{ij} = \frac{1}{a_i+a_j}$. I'm trying to prove that $M$ is positive-semidefinite. The hint says to use the fact that $\int_{0}^{\infty} e^{-sx}\; dx = \frac{1}{s}$ if $s > 0$. However I don't know how this hint is useful. I've tried choosing an arbitrary vector $x$ and substituting $x^{\intercal}Mx = \sum_{i}\sum_{j} \frac{x_ix_j}{a_i+a_j}$ into $s$ and using properties of exponents to simplify the equation into something that is clearly positive, but without any luck. The denominator $\frac{1}{a_i+a_j}$ is simply too difficult to work with. At this point I think I'm just missing some trick that I don't know. Any help would be appreciated.","Let $a_1, \ldots, a_n$ be a set of positive numbers. Define a matrix $M_{ij} = \frac{1}{a_i+a_j}$. I'm trying to prove that $M$ is positive-semidefinite. The hint says to use the fact that $\int_{0}^{\infty} e^{-sx}\; dx = \frac{1}{s}$ if $s > 0$. However I don't know how this hint is useful. I've tried choosing an arbitrary vector $x$ and substituting $x^{\intercal}Mx = \sum_{i}\sum_{j} \frac{x_ix_j}{a_i+a_j}$ into $s$ and using properties of exponents to simplify the equation into something that is clearly positive, but without any luck. The denominator $\frac{1}{a_i+a_j}$ is simply too difficult to work with. At this point I think I'm just missing some trick that I don't know. Any help would be appreciated.",,"['linear-algebra', 'positive-definite']"
47,Existence of matrix with certain property over a finite field,Existence of matrix with certain property over a finite field,,"Let $F_p = \{0,1,2,...,p-1\}$ be field for a prime $p$. Does there exist a matrix $A$ with entries in $F_p$ such that $tr(A^k) = 0$ if $k=2,3,\ldots,p-1$ and $tr(A^k) \neq 0$ if $k=1$?","Let $F_p = \{0,1,2,...,p-1\}$ be field for a prime $p$. Does there exist a matrix $A$ with entries in $F_p$ such that $tr(A^k) = 0$ if $k=2,3,\ldots,p-1$ and $tr(A^k) \neq 0$ if $k=1$?",,"['abstract-algebra', 'matrices', 'finite-fields', 'trace']"
48,How to find the eigen values of the given matrix,How to find the eigen values of the given matrix,,Given the matrix \begin{bmatrix} 5&1&1&1&1&1\\1&5&1&1&1&1\\1&1&5&1&1&1\\1&1&1&4&1&0\\1&1&1&1&4&0\\1&1&1&0&0&3 \end{bmatrix} find its eigen values(preferably by elementary row/column operations). Since I don't know any other method other than elementary operations to find eigen values so I tried writing the characteristic polynomial of the matrix  which is follows: \begin{bmatrix} x-5&-1&-1&-1&-1&-1\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3 \end{bmatrix} Using $R1=R1-(R2+R3+R4+R5+R6)$ \begin{bmatrix} x&-x+8&-x+8&-x+6&-x+6&-x+4\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3 \end{bmatrix},Given the matrix \begin{bmatrix} 5&1&1&1&1&1\\1&5&1&1&1&1\\1&1&5&1&1&1\\1&1&1&4&1&0\\1&1&1&1&4&0\\1&1&1&0&0&3 \end{bmatrix} find its eigen values(preferably by elementary row/column operations). Since I don't know any other method other than elementary operations to find eigen values so I tried writing the characteristic polynomial of the matrix  which is follows: \begin{bmatrix} x-5&-1&-1&-1&-1&-1\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3 \end{bmatrix} Using $R1=R1-(R2+R3+R4+R5+R6)$ \begin{bmatrix} x&-x+8&-x+8&-x+6&-x+6&-x+4\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3 \end{bmatrix},,"['linear-algebra', 'matrices']"
49,Prove that n is a multiple of $16$.,Prove that n is a multiple of .,16,"Let $n\ge2$ be a natural number and $A,B$ two real matrices of dimension $5$ so that $A^2+B^2=\sqrt[n]{2+\sqrt[n]{2}}AB$ and $\det(AB-BA)>0$ . Prove that $n$ is a multiple of $16$ . I don't know how to solve it. I just know that the trace of $AB-BA$ is $0$ and also then, from Cayley-Hamilton, I have that $(AB-BA)^2<0$ which is false; from here I have no idea.","Let be a natural number and two real matrices of dimension so that and . Prove that is a multiple of . I don't know how to solve it. I just know that the trace of is and also then, from Cayley-Hamilton, I have that which is false; from here I have no idea.","n\ge2 A,B 5 A^2+B^2=\sqrt[n]{2+\sqrt[n]{2}}AB \det(AB-BA)>0 n 16 AB-BA 0 (AB-BA)^2<0",['linear-algebra']
50,Show that $\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{2n}{4n}-1}$ [closed],Show that  [closed],\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{2n}{4n}-1},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question For any real matrix $A$ of dimension $3$ and for any $n \geqslant 2$, show that$$\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{4n}{2n}-1}.$$ I know that $\det(A-xI_3)$ is equal to zero polynomial from Cayley-Hamilton, but I do not know what I should use to deal with the binomial and to solve it. Any help please? Thanks in advance.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question For any real matrix $A$ of dimension $3$ and for any $n \geqslant 2$, show that$$\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{4n}{2n}-1}.$$ I know that $\det(A-xI_3)$ is equal to zero polynomial from Cayley-Hamilton, but I do not know what I should use to deal with the binomial and to solve it. Any help please? Thanks in advance.",,"['linear-algebra', 'matrices']"
51,"Any two complex square matrices are triangulable in the same basis, albeit perhaps in different orders","Any two complex square matrices are triangulable in the same basis, albeit perhaps in different orders",,"Let $E$ be a finite-dimensional vector-space on $\mathbb C$. Take any $a$, $b$ endomorphisms of $E$. Show there exist a basis $(e_1, ... e_n)$ and a permutation $\sigma$   such that the matrix of $a$ in $(e_1, ... e_n)$ and the matrix of $b$   in $(e_{\sigma(1)}, ... ,e_{\sigma(n)})$ are upper-triangular. Here some thoughts. Induction does not seem appropriate. Let $(a_1, ... a_n)$ be a triangular basis for $a$ and $(b_1, ... b_n)$ be a triangular basis for $b$. The first step is easy: take $a_1$ as $e_1$ which is an eigen-vector of $a$ and $b_1$ an eigen-vector of $b$. One can choose $i$ such that $b_1 = e_i$ chossing $i \neq 1$ minimal. Then $Vect(e_1, a_2, ... , a_{i-1}, e_i) \cap Vect(b_2,...b_n) \neq$ $\{0_E\}$ So let $x$ in  this intersection. $x$ can be colinear to $e_1$ ...","Let $E$ be a finite-dimensional vector-space on $\mathbb C$. Take any $a$, $b$ endomorphisms of $E$. Show there exist a basis $(e_1, ... e_n)$ and a permutation $\sigma$   such that the matrix of $a$ in $(e_1, ... e_n)$ and the matrix of $b$   in $(e_{\sigma(1)}, ... ,e_{\sigma(n)})$ are upper-triangular. Here some thoughts. Induction does not seem appropriate. Let $(a_1, ... a_n)$ be a triangular basis for $a$ and $(b_1, ... b_n)$ be a triangular basis for $b$. The first step is easy: take $a_1$ as $e_1$ which is an eigen-vector of $a$ and $b_1$ an eigen-vector of $b$. One can choose $i$ such that $b_1 = e_i$ chossing $i \neq 1$ minimal. Then $Vect(e_1, a_2, ... , a_{i-1}, e_i) \cap Vect(b_2,...b_n) \neq$ $\{0_E\}$ So let $x$ in  this intersection. $x$ can be colinear to $e_1$ ...",,"['linear-algebra', 'matrices']"
52,Multivariate polynomials as matrix products?,Multivariate polynomials as matrix products?,,"Sorry if this is a silly question.  But if I have a polynomial in two variables, with the maximum degree of each individual variable no greater than N, $$p(x, y)=c_{00}+c_{10}x+c_{01}y+c_{11}xy+c_{20}x^2+c_{02}y^2+... =\sum_{i=0}^N{\sum_{j=0}^N{c_{ij}x^iy^j}}$$ ...then I can write this as a product, $$p(x,y)=\textbf{x}'A\textbf{y},$$ where $$\textbf{x}=\begin{matrix}\begin{pmatrix}1&x&x^2&...&x^N\end{pmatrix}\end{matrix}'$$ $$\textbf{y}=\begin{matrix}\begin{pmatrix}1&y&y^2&...&y^N\end{pmatrix}\end{matrix}'$$ $$A=\begin{matrix}\begin{pmatrix}c_{00}&c_{01}&c_{02}&...&c_{0N} \\c_{10}&c_{11}&c_{12}&...&c_{1N} \\c_{20}&c_{21}&c_{22}&...&c_{2N} \\... \\c_{N0}&c_{N1}&c_{N2}&...&c_{NN}\end{pmatrix}\end{matrix}$$ This seems convenient, since the elements of A correspond exactly to the coefficients on each term of the function. My questions are: 1) Is this a ""standard"" way of representing a polynomial of two variables? 2) Is there some convenient way to extend this to polynomials of more than two variables? I don't know if there's a more general way to state this question, or if there is some theory out there regarding multivariate polynomials that I could learn, but any answers or links to resources would be helpful...I'm not quite sure what to Google for this.","Sorry if this is a silly question.  But if I have a polynomial in two variables, with the maximum degree of each individual variable no greater than N, $$p(x, y)=c_{00}+c_{10}x+c_{01}y+c_{11}xy+c_{20}x^2+c_{02}y^2+... =\sum_{i=0}^N{\sum_{j=0}^N{c_{ij}x^iy^j}}$$ ...then I can write this as a product, $$p(x,y)=\textbf{x}'A\textbf{y},$$ where $$\textbf{x}=\begin{matrix}\begin{pmatrix}1&x&x^2&...&x^N\end{pmatrix}\end{matrix}'$$ $$\textbf{y}=\begin{matrix}\begin{pmatrix}1&y&y^2&...&y^N\end{pmatrix}\end{matrix}'$$ $$A=\begin{matrix}\begin{pmatrix}c_{00}&c_{01}&c_{02}&...&c_{0N} \\c_{10}&c_{11}&c_{12}&...&c_{1N} \\c_{20}&c_{21}&c_{22}&...&c_{2N} \\... \\c_{N0}&c_{N1}&c_{N2}&...&c_{NN}\end{pmatrix}\end{matrix}$$ This seems convenient, since the elements of A correspond exactly to the coefficients on each term of the function. My questions are: 1) Is this a ""standard"" way of representing a polynomial of two variables? 2) Is there some convenient way to extend this to polynomials of more than two variables? I don't know if there's a more general way to state this question, or if there is some theory out there regarding multivariate polynomials that I could learn, but any answers or links to resources would be helpful...I'm not quite sure what to Google for this.",,"['matrices', 'polynomials', 'matrix-decomposition']"
53,Inverse of matrix after updating diagonal?,Inverse of matrix after updating diagonal?,,"Let $A$ be a real symmetric positive-definite matrix, with known inverse $A^{-1}$. Is there an efficient algorithm to compute $(A+R)^{-1}$, where $R$ is a real diagonal matrix? Assume that $A+R$ is also positive-definite. Failed idea with Cholesky decomposition I have read about the Cholesky decomposition and how it can be updated efficiently when the matrix changes by a rank-one modification. Since adding $R$ is the same as many rank-one modifications (one for each entry in the diagonal), in principle I could use this algoritm to compute the updated Cholesky decomposition. However this turns out to be $O(n^3)$ (where $n$ is the dimension of the matrices), no better than matrix inversion, because each rank-one update of the Cholesky decomposition is $O(n^2)$, and there are $n$ entries in the diagonal. Related This is closely related to Efficient diagonal update of matrix inverse . However in that question, the diagonal update is homogeneous. Here the diagonal entries $R_{nn}$ are in principle distinct for each $n$.","Let $A$ be a real symmetric positive-definite matrix, with known inverse $A^{-1}$. Is there an efficient algorithm to compute $(A+R)^{-1}$, where $R$ is a real diagonal matrix? Assume that $A+R$ is also positive-definite. Failed idea with Cholesky decomposition I have read about the Cholesky decomposition and how it can be updated efficiently when the matrix changes by a rank-one modification. Since adding $R$ is the same as many rank-one modifications (one for each entry in the diagonal), in principle I could use this algoritm to compute the updated Cholesky decomposition. However this turns out to be $O(n^3)$ (where $n$ is the dimension of the matrices), no better than matrix inversion, because each rank-one update of the Cholesky decomposition is $O(n^2)$, and there are $n$ entries in the diagonal. Related This is closely related to Efficient diagonal update of matrix inverse . However in that question, the diagonal update is homogeneous. Here the diagonal entries $R_{nn}$ are in principle distinct for each $n$.",,"['linear-algebra', 'matrices', 'inverse', 'positive-definite']"
54,Is the converse of Cayley-Hamilton Theorem true?,Is the converse of Cayley-Hamilton Theorem true?,,"The question is motivated from the following problem: Let $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times 2$ matrix. If $A=A^{-1}$, then the trace of $A$ is $$ (A) 2 \quad(B)1 \quad(C)0 \quad (D)-1 \quad (E)-2$$ Since $A=A^{-1}$, $A^2=I$. If the converse of Cayley-Hamilton Theorem is true, then $\lambda^2=1$ and thus $\lambda=\pm1$. And then $\rm{trace}(A)=1+(-1)=0$. Here are my questions: Is  $C$ the answer to the quoted problem? Is the converse of Cayley-Hamilton Theorem, i.e.,""for the square real matrix $A$, if $p(A)=0$, then $p(\lambda)$ is the characteristic polynomial of the matrix $A$"" true? If it is not, then what's the right method to solve the problem above?","The question is motivated from the following problem: Let $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times 2$ matrix. If $A=A^{-1}$, then the trace of $A$ is $$ (A) 2 \quad(B)1 \quad(C)0 \quad (D)-1 \quad (E)-2$$ Since $A=A^{-1}$, $A^2=I$. If the converse of Cayley-Hamilton Theorem is true, then $\lambda^2=1$ and thus $\lambda=\pm1$. And then $\rm{trace}(A)=1+(-1)=0$. Here are my questions: Is  $C$ the answer to the quoted problem? Is the converse of Cayley-Hamilton Theorem, i.e.,""for the square real matrix $A$, if $p(A)=0$, then $p(\lambda)$ is the characteristic polynomial of the matrix $A$"" true? If it is not, then what's the right method to solve the problem above?",,['linear-algebra']
55,Eiegenspectrum on subtracting a diagonal matrix,Eiegenspectrum on subtracting a diagonal matrix,,"Suppose I have a (psd) matrix $A$ and a diagonal matrix $\Sigma$. I want to know how the eigenvectors and eigenvalues of $A-\Sigma$ behave. The elements of $\Sigma$ are very small (compared to the eigenvalues of $A$). I know that when $\Sigma$ is a multiple of identity, it only shifts the spectrum of $A$. Such a thing does not hold for other diagonal matrices. But are their results from matrix perturbation which tells how the eigenvectors change (upper and lower bounds) on such perturbation to the matrix?","Suppose I have a (psd) matrix $A$ and a diagonal matrix $\Sigma$. I want to know how the eigenvectors and eigenvalues of $A-\Sigma$ behave. The elements of $\Sigma$ are very small (compared to the eigenvalues of $A$). I know that when $\Sigma$ is a multiple of identity, it only shifts the spectrum of $A$. Such a thing does not hold for other diagonal matrices. But are their results from matrix perturbation which tells how the eigenvectors change (upper and lower bounds) on such perturbation to the matrix?",,"['linear-algebra', 'matrices']"
56,Rotation invariants for higher degree homogeneous polynomials (like Tr$(P^m)$ for degree 2)?,Rotation invariants for higher degree homogeneous polynomials (like Tr for degree 2)?,(P^m),"Treating rotation in $\mathbb{R}^n$ as $x\to Ox$ for orthogonal $O^T O=O O^T=1$, we can easily get complete sets of independent rotation invariants for degree 1 and 2 homogeneous polynomials : Degree 1: for $p(x)=\sum_i P_i x_i$, rotation: $P_i\to \sum_a P_a O_{ai}$ has single invariant: $\sum_i P_i^2$: $$\sum_i \left(\sum_a P_a O_{ai}\right)^2 = \sum_{ia\alpha} P_a O_{ai} P_\alpha O_{\alpha i} = \sum_{a\alpha} P_a P_\alpha \delta_{a\alpha} = \sum_a P_a^2$$ Degree 2: $p(x)=\sum_{ij} P_{ij} x_i x_j$ has $n$ independent rotation invariants: eigenspectrum of (symmetric) $P=P^T$, or equivalently $\{0,\ldots,n-1\}$ coefficients of characteristic polynomial $\det(P-\lambda I)$, or equivalently $\textrm{Tr}(P^m)$ for $m\in \{1,\ldots,n\}$. Let's check the last one: $$\textrm{Tr}((O^T P O)^m)=\textrm{Tr}(O^T P^m O)=\sum_{iab} O_{ai}(P^m)_{ab}O_{bi}=\sum_{ab}(P^m)_{ab} \delta_{ab}=\textrm{Tr}(P^m).$$ For degree 3 such rotation would analogously mean: $$P_{ijk}\to\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}$$ How to construct rotation invariants for degree 3 and higher? How many independent invariants should we expect? My motivation is testing graph isomorphism problem, which after looking at eigenspaces of adjacency matrix becomes question if two sets of points differ only by rotation, and theses sets can be described using homogeneous polynomials ( stack ) - efficient testing for degree 3 or 4 should be sufficient. Update 1: In analogy to degree 1, here is one for degree 3: $\sum_{ijk} P_{ijk}^2$ (and the same for higher): $$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right)^2= \sum_{ijkabc\alpha\beta\gamma} P_{abc} O_{ai} O_{bj} O_{ck}  P_{\alpha\beta\gamma} O_{\alpha i} O_{\beta j} O_{\gamma k}=\sum_{abc}P_{abc}^2$$ Update 2: In analogy to Tr$(P^2)=\sum_{ij} P_{ij} P_{ji}$ for degree 2, for degree 3 we can get invariant $\sum_{ijk} P_{ijk} P_{jki}$: $$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right) \left(\sum_{\alpha\beta\gamma} P_{\beta\gamma\alpha} O_{\beta j} O_{\gamma k} O_{\alpha i}\right) =\sum_{abc} P_{abc} P_{bca} $$ As every $ijk$ summation leads to one Kronecker delta $\delta$. Analogously $\sum_{ijk} P_{ijk} P_{kij}$. Update 3 (22.12.2017): We can analogously go with lager sets of variables, turning pairs of $O$ into Kronecker deltas: every variable should appear in exactly two terms. For example $\sum_{abcdef} P_{abc} P_{ade} P_{bcf} P_{def} $ is rotation invariant this way. We get nice combinatorics of indexes (like in free probability) - leading to final questions: For degree 2 we would take Tr$(P^{m})$ for $m=1\ldots n$, the next one: Tr$(P^{n+1})$ would be dependent - how to generally choose the largest independent set of rotation invariants? Looking at degree 2, I suspect what is crucial is determining its size (kind of size of eigenbasis) - then ""many""(?) sets of invariants of this size should be right (""determine eigenbasis""). Are they really all rotation invariants for homogeneous polynomials? (we know it is true for degree 1 and 2) If all required invariants agree, can we effectively determine the rotation? (I suspect the set of possible rotations might be nasty (?) ) Update 4: Diagrammatic representation of some first rotation invariants for degree 1,2,3,4: Degree of polynomial is also degree of vertex. Edge corresponds to summed index. Vertices and edges are unlabeled. Disconnected graphs gives invariants being products over its components. Update 5: We can analogously get rotation invariants for general polynomials:  $$p(x)=p+\sum_i p_i x_i +\sum_{ij} p_{ij} x_i x_j + \ldots $$ Beside the homogeneous terms like in the above diagram, there are also additional mixing terms corresponding to graphs with vertices of varying degree, starting with $\sum_{ab} p_a p_{ab} p_b$. It is much better than what is offered by standard e.g. rotationally invariant spherical harmonics , which give only 1 rotation invariant per degree (angular momentum $l$), and don't allow to test mixing between them. Here we probably get a complete set of rotation invariants - such that their agreement ensures that polynomials differ only by rotation. How to choose such complete sets of invariants - completely defining polynomial of given degree modulo rotation? For degree 2: $p(x)=x^T A x + b^Tx+ c$ describes e.g. paraboloid. Rotation invariants are: $c$, $\sum_i b_i^2$, Tr$(A^k)$ for $k=1\ldots n$. Adding  $\sum_{ij} b_i (A^k)_{ij} b_j$ for $k=1...n-1$ invariants should completely define this paraboloid modulo rotation. Update 6: For a complete set of invariants (determining polynomial modulo rotation), there is needed an automatic procedure to calculate a lot of them. A simple way to do it is to build e.g. matrix like $M_{ab}=\sum_{cd} p_{acd} p_{cdb}$ and then construct Tr$(M^k)$ invariants for $k=1...n$. This was for ""-<>-"" graph with two external edges. Analogously we could construct ""-<=|=|...|=>-"" ladder-like graphs, constructed by expanding it step by step, and in each step getting $n$ invariants from closing such graph. The question is if such construction can be systematized to get a complete set of invariants? For degree $d$ there should be ${n+d-1 \choose d}-n(n-1)/2$ of them, plus $n(n-1)/2$ mixed invariants describing relative rotation to the lower degree terms. Update: I decided to write separate paper about these invariants, especially from perspective of machine learning applications (they are much stronger than standard approaches): https://arxiv.org/pdf/1801.01058","Treating rotation in $\mathbb{R}^n$ as $x\to Ox$ for orthogonal $O^T O=O O^T=1$, we can easily get complete sets of independent rotation invariants for degree 1 and 2 homogeneous polynomials : Degree 1: for $p(x)=\sum_i P_i x_i$, rotation: $P_i\to \sum_a P_a O_{ai}$ has single invariant: $\sum_i P_i^2$: $$\sum_i \left(\sum_a P_a O_{ai}\right)^2 = \sum_{ia\alpha} P_a O_{ai} P_\alpha O_{\alpha i} = \sum_{a\alpha} P_a P_\alpha \delta_{a\alpha} = \sum_a P_a^2$$ Degree 2: $p(x)=\sum_{ij} P_{ij} x_i x_j$ has $n$ independent rotation invariants: eigenspectrum of (symmetric) $P=P^T$, or equivalently $\{0,\ldots,n-1\}$ coefficients of characteristic polynomial $\det(P-\lambda I)$, or equivalently $\textrm{Tr}(P^m)$ for $m\in \{1,\ldots,n\}$. Let's check the last one: $$\textrm{Tr}((O^T P O)^m)=\textrm{Tr}(O^T P^m O)=\sum_{iab} O_{ai}(P^m)_{ab}O_{bi}=\sum_{ab}(P^m)_{ab} \delta_{ab}=\textrm{Tr}(P^m).$$ For degree 3 such rotation would analogously mean: $$P_{ijk}\to\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}$$ How to construct rotation invariants for degree 3 and higher? How many independent invariants should we expect? My motivation is testing graph isomorphism problem, which after looking at eigenspaces of adjacency matrix becomes question if two sets of points differ only by rotation, and theses sets can be described using homogeneous polynomials ( stack ) - efficient testing for degree 3 or 4 should be sufficient. Update 1: In analogy to degree 1, here is one for degree 3: $\sum_{ijk} P_{ijk}^2$ (and the same for higher): $$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right)^2= \sum_{ijkabc\alpha\beta\gamma} P_{abc} O_{ai} O_{bj} O_{ck}  P_{\alpha\beta\gamma} O_{\alpha i} O_{\beta j} O_{\gamma k}=\sum_{abc}P_{abc}^2$$ Update 2: In analogy to Tr$(P^2)=\sum_{ij} P_{ij} P_{ji}$ for degree 2, for degree 3 we can get invariant $\sum_{ijk} P_{ijk} P_{jki}$: $$\sum_{ijk}\left(\sum_{abc} P_{abc} O_{ai} O_{bj} O_{ck}\right) \left(\sum_{\alpha\beta\gamma} P_{\beta\gamma\alpha} O_{\beta j} O_{\gamma k} O_{\alpha i}\right) =\sum_{abc} P_{abc} P_{bca} $$ As every $ijk$ summation leads to one Kronecker delta $\delta$. Analogously $\sum_{ijk} P_{ijk} P_{kij}$. Update 3 (22.12.2017): We can analogously go with lager sets of variables, turning pairs of $O$ into Kronecker deltas: every variable should appear in exactly two terms. For example $\sum_{abcdef} P_{abc} P_{ade} P_{bcf} P_{def} $ is rotation invariant this way. We get nice combinatorics of indexes (like in free probability) - leading to final questions: For degree 2 we would take Tr$(P^{m})$ for $m=1\ldots n$, the next one: Tr$(P^{n+1})$ would be dependent - how to generally choose the largest independent set of rotation invariants? Looking at degree 2, I suspect what is crucial is determining its size (kind of size of eigenbasis) - then ""many""(?) sets of invariants of this size should be right (""determine eigenbasis""). Are they really all rotation invariants for homogeneous polynomials? (we know it is true for degree 1 and 2) If all required invariants agree, can we effectively determine the rotation? (I suspect the set of possible rotations might be nasty (?) ) Update 4: Diagrammatic representation of some first rotation invariants for degree 1,2,3,4: Degree of polynomial is also degree of vertex. Edge corresponds to summed index. Vertices and edges are unlabeled. Disconnected graphs gives invariants being products over its components. Update 5: We can analogously get rotation invariants for general polynomials:  $$p(x)=p+\sum_i p_i x_i +\sum_{ij} p_{ij} x_i x_j + \ldots $$ Beside the homogeneous terms like in the above diagram, there are also additional mixing terms corresponding to graphs with vertices of varying degree, starting with $\sum_{ab} p_a p_{ab} p_b$. It is much better than what is offered by standard e.g. rotationally invariant spherical harmonics , which give only 1 rotation invariant per degree (angular momentum $l$), and don't allow to test mixing between them. Here we probably get a complete set of rotation invariants - such that their agreement ensures that polynomials differ only by rotation. How to choose such complete sets of invariants - completely defining polynomial of given degree modulo rotation? For degree 2: $p(x)=x^T A x + b^Tx+ c$ describes e.g. paraboloid. Rotation invariants are: $c$, $\sum_i b_i^2$, Tr$(A^k)$ for $k=1\ldots n$. Adding  $\sum_{ij} b_i (A^k)_{ij} b_j$ for $k=1...n-1$ invariants should completely define this paraboloid modulo rotation. Update 6: For a complete set of invariants (determining polynomial modulo rotation), there is needed an automatic procedure to calculate a lot of them. A simple way to do it is to build e.g. matrix like $M_{ab}=\sum_{cd} p_{acd} p_{cdb}$ and then construct Tr$(M^k)$ invariants for $k=1...n$. This was for ""-<>-"" graph with two external edges. Analogously we could construct ""-<=|=|...|=>-"" ladder-like graphs, constructed by expanding it step by step, and in each step getting $n$ invariants from closing such graph. The question is if such construction can be systematized to get a complete set of invariants? For degree $d$ there should be ${n+d-1 \choose d}-n(n-1)/2$ of them, plus $n(n-1)/2$ mixed invariants describing relative rotation to the lower degree terms. Update: I decided to write separate paper about these invariants, especially from perspective of machine learning applications (they are much stronger than standard approaches): https://arxiv.org/pdf/1801.01058",,"['matrices', 'polynomials', 'rotations', 'multilinear-algebra', 'invariance']"
57,Why block-diagonal form for nilpotent matrices?,Why block-diagonal form for nilpotent matrices?,,"I am currently reading Jim Hefferon's Linear Algebra . In chapter 5, nilpotence, strings, he goes through the process of finding a string basis of a map, and proves that there exists a string basis for any nilpotent transformation. He then suddenly states that from this string basis, you can create a matrix consisting of only 0's, except subdiagonal 1's in blocks. Also this block-diagonal form, is the canonical form of similar nilpotent matrices. He doesn't go into many details, and latter on the exercises, he says that the canonical form can be ""immediately"" found just by knowing the amount of strings and their length (not even knowing the actual basis vectors). So my two questions: 1) Is it obvious that the canonical form is a block-diagonal? Or does it need a rigorous proof that the text simply ignores? 2) How do we find the canonical form, if we know the strings and their lengths?","I am currently reading Jim Hefferon's Linear Algebra . In chapter 5, nilpotence, strings, he goes through the process of finding a string basis of a map, and proves that there exists a string basis for any nilpotent transformation. He then suddenly states that from this string basis, you can create a matrix consisting of only 0's, except subdiagonal 1's in blocks. Also this block-diagonal form, is the canonical form of similar nilpotent matrices. He doesn't go into many details, and latter on the exercises, he says that the canonical form can be ""immediately"" found just by knowing the amount of strings and their length (not even knowing the actual basis vectors). So my two questions: 1) Is it obvious that the canonical form is a block-diagonal? Or does it need a rigorous proof that the text simply ignores? 2) How do we find the canonical form, if we know the strings and their lengths?",,"['linear-algebra', 'matrices', 'linear-transformations', 'jordan-normal-form', 'nilpotence']"
58,$A^3\cdot B^3=X^3+Y^3$.,.,A^3\cdot B^3=X^3+Y^3,"Prove that if $A,B\in M_n (\mathbb {C}) $ then there exists $X,Y \in M_n (\mathbb {C}) $ s.t. $A^3\cdot B^3=X^3+Y^3$. If $A,B $ commutes then it's clear. But what about the other case?","Prove that if $A,B\in M_n (\mathbb {C}) $ then there exists $X,Y \in M_n (\mathbb {C}) $ s.t. $A^3\cdot B^3=X^3+Y^3$. If $A,B $ commutes then it's clear. But what about the other case?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
59,Maximize expectation of concave function with respect to unitary matrix,Maximize expectation of concave function with respect to unitary matrix,,"Let $\mathbf{x} \sim \mathcal{N}(\mathbf{m},\mathbf{C})$ and let $\mathbf{D}$ be a diagonal matrix with positive entries and of the same dimension as $\mathbf{C}$. Let $f(z)$ be a strictly increasing and concave function in $z$. Considering all the possible unitary matrices $\mathbf{U}$ (i.e., $\mathbf{U} \mathbf{U}^{\mathrm{T}}=\mathbf{I}$), I suspect the following holds: \begin{align} \mathrm{argmax}_{\mathbf{U}} \mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big] = \mathrm{argmax}_{\mathbf{U}} f \big( \mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big] \big). \end{align} However, I don't know how to prove it formally. My intuition is that, by optimizing over unitary matrices $\mathbf{U}$, we are just playing with the ""directions"" (note that $\mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}}$ can be seen as the eigendecomposition of a symmetric and positive definite matrix), and I suspect that the directions that maximize $\mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big]$ should be the ones maximizing $\mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big]$. It would be great if someone could confirm this. Note: to make things easier, we can consider $f(z) = \log(z)$ (which is strictly increasing and concave).","Let $\mathbf{x} \sim \mathcal{N}(\mathbf{m},\mathbf{C})$ and let $\mathbf{D}$ be a diagonal matrix with positive entries and of the same dimension as $\mathbf{C}$. Let $f(z)$ be a strictly increasing and concave function in $z$. Considering all the possible unitary matrices $\mathbf{U}$ (i.e., $\mathbf{U} \mathbf{U}^{\mathrm{T}}=\mathbf{I}$), I suspect the following holds: \begin{align} \mathrm{argmax}_{\mathbf{U}} \mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big] = \mathrm{argmax}_{\mathbf{U}} f \big( \mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big] \big). \end{align} However, I don't know how to prove it formally. My intuition is that, by optimizing over unitary matrices $\mathbf{U}$, we are just playing with the ""directions"" (note that $\mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}}$ can be seen as the eigendecomposition of a symmetric and positive definite matrix), and I suspect that the directions that maximize $\mathbb{E}_{\mathbf{x}} \big[ f \big( \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big) \big]$ should be the ones maximizing $\mathbb{E}_{\mathbf{x}} \big[ \mathbf{x}^{\mathrm{T}} \mathbf{U} \mathbf{D} \mathbf{U}^{\mathrm{T}} \mathbf{x} \big]$. It would be great if someone could confirm this. Note: to make things easier, we can consider $f(z) = \log(z)$ (which is strictly increasing and concave).",,"['matrices', 'normal-distribution', 'expectation', 'convex-optimization']"
60,Rank of a matrix of L.T. which is not one one,Rank of a matrix of L.T. which is not one one,,If we take a linear transformation $T$ from $R^n$ to $R^n$ and assume that it is not one-one. Can we find what exactly its rank is? If it is one-one then $\ker T =\{0\}$ and the matrix of the L.T. must be non singular and therefore rank will be $n$. But if it is not one-one then rank must be less than $n$. But can we get the exact rank of the matrix of L.T.?,If we take a linear transformation $T$ from $R^n$ to $R^n$ and assume that it is not one-one. Can we find what exactly its rank is? If it is one-one then $\ker T =\{0\}$ and the matrix of the L.T. must be non singular and therefore rank will be $n$. But if it is not one-one then rank must be less than $n$. But can we get the exact rank of the matrix of L.T.?,,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-rank']"
61,Pseudo-inverse with minimal number of non-zero entries,Pseudo-inverse with minimal number of non-zero entries,,"I'm looking for a way to compute the pseudo-inverse of a matrix (not the Moore-Penrose, but any other) with the minimal number of non-zero entries (maximum number of zero entries). In MATLAB, the the mldivide() operator does that, but I cannot find any documentation on how this operator evaluates my matrix so that it inds this particular pseudo-inverse. Is there a way to do this ""by hand""? Note: I know, Moore-Penrose minimize the Euclidean norm, but I explicitely do not want to minimize that norm, but to maximize the zero-entries in my pseudo-inverse.","I'm looking for a way to compute the pseudo-inverse of a matrix (not the Moore-Penrose, but any other) with the minimal number of non-zero entries (maximum number of zero entries). In MATLAB, the the mldivide() operator does that, but I cannot find any documentation on how this operator evaluates my matrix so that it inds this particular pseudo-inverse. Is there a way to do this ""by hand""? Note: I know, Moore-Penrose minimize the Euclidean norm, but I explicitely do not want to minimize that norm, but to maximize the zero-entries in my pseudo-inverse.",,"['linear-algebra', 'matrices', 'inverse', 'matlab', 'pseudoinverse']"
62,Conditions for the invertibility of doubly stochastic matrix,Conditions for the invertibility of doubly stochastic matrix,,"I am trying to find conditions for the invertibility of the matrix resulting from the convex combination of all possible permutation matrices of dimension $n \times n$ (to put it in context, and in case it helps, each of the permutation matrices identifies a different order in which an agent would rank n distinct alternatives), where: a permutation matrix is a square matrix for which each column/row has exactly one element equal to 1, and takes value zero elsewhere. a doubly stochastic matrix is a square matrix of non negative real numbers for which the row sum and the column sum is equal to 1 a convex combination is a linear combination whose coefficients add up to 1 So for instance for $n=3$ the object of study would be: $$ \tau_{1}\left[\begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1 \end{array}\right]+\tau_{2}\left[\begin{array}{ccc} 1 & 0 & 0\\ 0 & 0 & 1\\ 0 & 1 & 0 \end{array}\right]+\tau_{3}\left[\begin{array}{ccc} 0 & 1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 1 \end{array}\right]+\tau_{4}\left[\begin{array}{ccc} 0 & 1 & 0\\ 0 & 0 & 1\\ 1 & 0 & 0 \end{array}\right]+\tau_{5}\left[\begin{array}{ccc} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{array}\right]+\tau_{6}\left[\begin{array}{ccc} 0 & 0 & 1\\ 0 & 1 & 0\\ 1 & 0 & 0 \end{array}\right] = \left[\begin{array}{ccc} \tau_1+\tau_2 & \tau_3+\tau_4 & \tau_5+\tau_6\\ \tau_3+\tau_5 & \tau_1+\tau_6 & \tau_2+\tau_4\\ \tau_4+\tau_6 & \tau_2+\tau_5 & \tau_1+\tau_3 \end{array}\right] $$ where $\sum_{i=1}^{6}\tau_{i}=1$ and $\tau_{i}\geq0$ for all $i=1,...,6$. Of course I am looking for answers in the case of generic $n$. Even conditions under which the resulting matrix is non-singular for a finite number of values would still do it for me. Is there a standard reference for this? I could not find any, but as I am not a mathematician I suspect it may be something really obvious that is dealt with e.g. in problem sets. Thank you!","I am trying to find conditions for the invertibility of the matrix resulting from the convex combination of all possible permutation matrices of dimension $n \times n$ (to put it in context, and in case it helps, each of the permutation matrices identifies a different order in which an agent would rank n distinct alternatives), where: a permutation matrix is a square matrix for which each column/row has exactly one element equal to 1, and takes value zero elsewhere. a doubly stochastic matrix is a square matrix of non negative real numbers for which the row sum and the column sum is equal to 1 a convex combination is a linear combination whose coefficients add up to 1 So for instance for $n=3$ the object of study would be: $$ \tau_{1}\left[\begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1 \end{array}\right]+\tau_{2}\left[\begin{array}{ccc} 1 & 0 & 0\\ 0 & 0 & 1\\ 0 & 1 & 0 \end{array}\right]+\tau_{3}\left[\begin{array}{ccc} 0 & 1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 1 \end{array}\right]+\tau_{4}\left[\begin{array}{ccc} 0 & 1 & 0\\ 0 & 0 & 1\\ 1 & 0 & 0 \end{array}\right]+\tau_{5}\left[\begin{array}{ccc} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0 \end{array}\right]+\tau_{6}\left[\begin{array}{ccc} 0 & 0 & 1\\ 0 & 1 & 0\\ 1 & 0 & 0 \end{array}\right] = \left[\begin{array}{ccc} \tau_1+\tau_2 & \tau_3+\tau_4 & \tau_5+\tau_6\\ \tau_3+\tau_5 & \tau_1+\tau_6 & \tau_2+\tau_4\\ \tau_4+\tau_6 & \tau_2+\tau_5 & \tau_1+\tau_3 \end{array}\right] $$ where $\sum_{i=1}^{6}\tau_{i}=1$ and $\tau_{i}\geq0$ for all $i=1,...,6$. Of course I am looking for answers in the case of generic $n$. Even conditions under which the resulting matrix is non-singular for a finite number of values would still do it for me. Is there a standard reference for this? I could not find any, but as I am not a mathematician I suspect it may be something really obvious that is dealt with e.g. in problem sets. Thank you!",,"['linear-algebra', 'matrices', 'matrix-rank']"
63,Show that partial pivoting leads to an $LU$ decomposition of $PA$.,Show that partial pivoting leads to an  decomposition of .,LU PA,"Exercise: show that for every non-singular matrix $A$, partial pivoting leads to an $LU$ decomposition of $PA$ so: $PA = LU$. I have the following theorems I can use: Theorem 1: Assume that the coefficient matrix $A\in\mathbb{R}^{n\times n}$ of the linear system $Au = f$ is non-singular and that it can be brought to its upper triangular form $U$ using $n-1$ row operations without scaling and without interchanges in such a way that pivot elements $a_{k,k}^{(k-1)}$ for $k = 1,...,n-1$ are non-zero. Then the $n-1$ Gaussian transformation $M_k$ for $k=1, ..., n-1$ exist such that   \begin{equation} \begin{split} M_{n-1}M_{n-2}...M_1A = U\Leftrightarrow  A &= (M_{n-1}...M_1)^{-1}U\\ \Leftrightarrow A &= LU \end{split} \end{equation} and Theorem 2: If Gaussian elimination with partial pivoting is used to compute the upper triangularization   \begin{equation} M_{n-1}P_{n-1}...M_1P_1A = U, \end{equation}    then    \begin{equation} PA = LU, \end{equation}   where $P = P_{n-1}...P_1$ and $L$ is a unit lower triangular matrix with $\left|l_{ij}\right| \leq1$. What I think I should do: From Theorem 2 I know that if the $LU$ decomposition exists, Gaussian elimination with partial pivoting will lead to an $LU$ decomposition of $PA$. So I need to show that every non-singular matrix $A$ has a $LU$ decomposition. Theorem 1 states that if a non-singular matrix $A$ can be brought to its upper triangular form $U$ then an $LU$ decomposition exists. So I think that I need to show that every non-singular matrix A can be brought to its upper triangular form $U$. Question: How do I solve this exercise? Am I on the right track? Thanks in advance!","Exercise: show that for every non-singular matrix $A$, partial pivoting leads to an $LU$ decomposition of $PA$ so: $PA = LU$. I have the following theorems I can use: Theorem 1: Assume that the coefficient matrix $A\in\mathbb{R}^{n\times n}$ of the linear system $Au = f$ is non-singular and that it can be brought to its upper triangular form $U$ using $n-1$ row operations without scaling and without interchanges in such a way that pivot elements $a_{k,k}^{(k-1)}$ for $k = 1,...,n-1$ are non-zero. Then the $n-1$ Gaussian transformation $M_k$ for $k=1, ..., n-1$ exist such that   \begin{equation} \begin{split} M_{n-1}M_{n-2}...M_1A = U\Leftrightarrow  A &= (M_{n-1}...M_1)^{-1}U\\ \Leftrightarrow A &= LU \end{split} \end{equation} and Theorem 2: If Gaussian elimination with partial pivoting is used to compute the upper triangularization   \begin{equation} M_{n-1}P_{n-1}...M_1P_1A = U, \end{equation}    then    \begin{equation} PA = LU, \end{equation}   where $P = P_{n-1}...P_1$ and $L$ is a unit lower triangular matrix with $\left|l_{ij}\right| \leq1$. What I think I should do: From Theorem 2 I know that if the $LU$ decomposition exists, Gaussian elimination with partial pivoting will lead to an $LU$ decomposition of $PA$. So I need to show that every non-singular matrix $A$ has a $LU$ decomposition. Theorem 1 states that if a non-singular matrix $A$ can be brought to its upper triangular form $U$ then an $LU$ decomposition exists. So I think that I need to show that every non-singular matrix A can be brought to its upper triangular form $U$. Question: How do I solve this exercise? Am I on the right track? Thanks in advance!",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
64,Eigenvalue problem with symmetric matrix with diagonal diagonal blocks,Eigenvalue problem with symmetric matrix with diagonal diagonal blocks,,"I would like to efficiently compute all the eigenvalues and eigenvectors of a real matrix A for which the structure is as follows: $A =\begin{bmatrix} D_1 & C\\C^T & D_2 \end{bmatrix}$ , in which $D_1$ is a $n_1 \times n_1$ diagonal matrix with strictly positive elements and $D_2$ is a $n_2 \times n_2$ diagonal matrix with strictly positive elements, and where $C$ is a fully-populated $n_1 \times n_2$ matrix with full rank. I would like to know if there is a way to exploit the fact that $D_1$ and $D_2$ are diagonal, in order to speed-up the calculation. For now, I am directly using MATLAB routine eig applied to A: [V,D] = eig(A) and it isn't faster than for a fully-populated real symmetric positive-definite matrix of the same size, even for the case $n_1 \ll n_2$ for which $C$ is very thin. Are you aware of any way to exploit such a structure? Thanks very much, Olivier","I would like to efficiently compute all the eigenvalues and eigenvectors of a real matrix A for which the structure is as follows: $A =\begin{bmatrix} D_1 & C\\C^T & D_2 \end{bmatrix}$ , in which $D_1$ is a $n_1 \times n_1$ diagonal matrix with strictly positive elements and $D_2$ is a $n_2 \times n_2$ diagonal matrix with strictly positive elements, and where $C$ is a fully-populated $n_1 \times n_2$ matrix with full rank. I would like to know if there is a way to exploit the fact that $D_1$ and $D_2$ are diagonal, in order to speed-up the calculation. For now, I am directly using MATLAB routine eig applied to A: [V,D] = eig(A) and it isn't faster than for a fully-populated real symmetric positive-definite matrix of the same size, even for the case $n_1 \ll n_2$ for which $C$ is very thin. Are you aware of any way to exploit such a structure? Thanks very much, Olivier",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'symmetric-matrices']"
65,Is a principal submatrix of a diagonalizable matrix diagonalizable?,Is a principal submatrix of a diagonalizable matrix diagonalizable?,,"Let $A$ be diagonalizable, i.e., $A=X \Lambda X^{-1}$ for some diagonal matrix $\Lambda$. Consider $B$ which is a principal submatrix of $A$. Does there exist an invertible matrix $Y$ and a diagonal matrix $D$ such that  $B=Y D Y^{-1}$ ? $D$ and $\Lambda$ can be related through the interlacing property. Can $X$ and $Y$ also be related to each other. Specifically, if $X$ has small condition number, does $Y$ also have a small condition number? Any thoughts/pointers are appreciated!","Let $A$ be diagonalizable, i.e., $A=X \Lambda X^{-1}$ for some diagonal matrix $\Lambda$. Consider $B$ which is a principal submatrix of $A$. Does there exist an invertible matrix $Y$ and a diagonal matrix $D$ such that  $B=Y D Y^{-1}$ ? $D$ and $\Lambda$ can be related through the interlacing property. Can $X$ and $Y$ also be related to each other. Specifically, if $X$ has small condition number, does $Y$ also have a small condition number? Any thoughts/pointers are appreciated!",,"['linear-algebra', 'matrices', 'diagonalization', 'matrix-decomposition']"
66,The inverse matrix of $a_{ij}=i^{j}$ [duplicate],The inverse matrix of  [duplicate],a_{ij}=i^{j},"This question already has an answer here : Proof of Vandermonde Matrix Inverse Formula (1 answer) Closed 6 years ago . This is my first question, so I’m sorry if I made mistakes. $\left(\begin{array}{ccccc}1&1&1&\cdots&1\\2&2^2&2^3&\cdots&2^n\\3&3^2&3^3&\cdots&3^n\\\vdots&\vdots&\vdots&\ddots&\vdots\\n&n^2&n^3&\cdots&n^n\end{array}\right)$ In order to solve the above inverse matrix,  I tried to solve the below liner simultaneous equations in n-th unknowns. $kx_1+k^2x_2+\cdots+k^nx_n=1+2^{n-1}+\cdots+k^{n-1}$ $k=1,2,\cdots,n$ Inductively, I know the solution of the liner simultaneous equations. $x_k=\frac{B_{n-k}}{n-k} {n-1 \choose n-k-1} (1≦k<n-1)$ $x_{n-1}=\frac{1}{2}$ $x_n=\frac{1}{n}$ But even though I solved the solutions of the equations, I have no ideas how to use it. I thought Vandermonde’s déterminant may be effective, but I have no ideas how to use it.","This question already has an answer here : Proof of Vandermonde Matrix Inverse Formula (1 answer) Closed 6 years ago . This is my first question, so I’m sorry if I made mistakes. $\left(\begin{array}{ccccc}1&1&1&\cdots&1\\2&2^2&2^3&\cdots&2^n\\3&3^2&3^3&\cdots&3^n\\\vdots&\vdots&\vdots&\ddots&\vdots\\n&n^2&n^3&\cdots&n^n\end{array}\right)$ In order to solve the above inverse matrix,  I tried to solve the below liner simultaneous equations in n-th unknowns. $kx_1+k^2x_2+\cdots+k^nx_n=1+2^{n-1}+\cdots+k^{n-1}$ $k=1,2,\cdots,n$ Inductively, I know the solution of the liner simultaneous equations. $x_k=\frac{B_{n-k}}{n-k} {n-1 \choose n-k-1} (1≦k<n-1)$ $x_{n-1}=\frac{1}{2}$ $x_n=\frac{1}{n}$ But even though I solved the solutions of the equations, I have no ideas how to use it. I thought Vandermonde’s déterminant may be effective, but I have no ideas how to use it.",,"['linear-algebra', 'matrices']"
67,Solving an upper triangular system of linear equations,Solving an upper triangular system of linear equations,,"Given $$(I+T_1T_2T_3)\,x = b$$ where $I$ is the identity matrix and $T_1$, $T_2$ and $T_3$ are invertible upper triangular matrices. Matrix $(I+T_1T_2T_3)$ is also invertible. I want to know what the fastest method to find vector $x$ is. I know that it is easy to Find $A = I + T_1T_2T_3$ with $\mathcal{O(n^3)}$ basic arithmetic operations. Find $A^{-1}b$ with $\mathcal{O(n^2)}$ basic arithmetic operations. As a result it needs  $\mathcal{O(n^3)}$ operations. I wonder if it is possible to do it with $\mathcal{O(n^2)}$ operations? Note: I updated the question by adding the invertible matrix $T_3$. For case of two matrices, we can write the LHS, in form of $T_1^{-1}(T_1^{-1}+T_2)x$, which needs $\mathcal{O}(n^2)$ operations to solve.","Given $$(I+T_1T_2T_3)\,x = b$$ where $I$ is the identity matrix and $T_1$, $T_2$ and $T_3$ are invertible upper triangular matrices. Matrix $(I+T_1T_2T_3)$ is also invertible. I want to know what the fastest method to find vector $x$ is. I know that it is easy to Find $A = I + T_1T_2T_3$ with $\mathcal{O(n^3)}$ basic arithmetic operations. Find $A^{-1}b$ with $\mathcal{O(n^2)}$ basic arithmetic operations. As a result it needs  $\mathcal{O(n^3)}$ operations. I wonder if it is possible to do it with $\mathcal{O(n^2)}$ operations? Note: I updated the question by adding the invertible matrix $T_3$. For case of two matrices, we can write the LHS, in form of $T_1^{-1}(T_1^{-1}+T_2)x$, which needs $\mathcal{O}(n^2)$ operations to solve.",,"['linear-algebra', 'matrices', 'asymptotics', 'systems-of-equations', 'numerical-linear-algebra']"
68,Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are arbitrary matrices,Eigenvalues of  and  where  and  are arbitrary matrices,AB BA A B,"This question is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices which itself is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices . Let $A$ be an $m \times n$ matrix and B and $n \times k$ matrix. Obviously, the matrix product $AB$ is possible, whereas the product $BA$ is not. Assume $n<k<m$, such that $AB$ is a large matrix. Is there anything we can do to either matrix $A$ or $B$, such that the product $BA$ becomes possible and such that the eigenvalues of $BA$ say something about the eigenvalues of the original $AB$? I am thinking of procedures such as: Truncating $A$ (making it $k \times n$) Appending some values to $B$ (making it $n \times m$) Interpolating values in $B$ Taking random samples etc. Motivation 1 (theoretical): The matrix $AB$ is large and clearly degenerate. Therefore, there must be a smaller matrix which captures the same information as $AB$ (i.e. has the same eigenvalues). If $k=m$, then $BA$ would be such a smaller matrix, as discussed in Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices . Motivation 2 (practical): The eigendecomposition of a very large matrix is computationally expensive and may require special hardware. If the problem can be simplified, e.g. by decomposing the smaller $BA$, then the analysis can be performed more efficiently. Alternatively, is there anything we can say about the eigenvalues of $AB$ without performing the product, i.e. based on analyses of $A$ and $B$ separately.","This question is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices which itself is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices . Let $A$ be an $m \times n$ matrix and B and $n \times k$ matrix. Obviously, the matrix product $AB$ is possible, whereas the product $BA$ is not. Assume $n<k<m$, such that $AB$ is a large matrix. Is there anything we can do to either matrix $A$ or $B$, such that the product $BA$ becomes possible and such that the eigenvalues of $BA$ say something about the eigenvalues of the original $AB$? I am thinking of procedures such as: Truncating $A$ (making it $k \times n$) Appending some values to $B$ (making it $n \times m$) Interpolating values in $B$ Taking random samples etc. Motivation 1 (theoretical): The matrix $AB$ is large and clearly degenerate. Therefore, there must be a smaller matrix which captures the same information as $AB$ (i.e. has the same eigenvalues). If $k=m$, then $BA$ would be such a smaller matrix, as discussed in Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices . Motivation 2 (practical): The eigendecomposition of a very large matrix is computationally expensive and may require special hardware. If the problem can be simplified, e.g. by decomposing the smaller $BA$, then the analysis can be performed more efficiently. Alternatively, is there anything we can say about the eigenvalues of $AB$ without performing the product, i.e. based on analyses of $A$ and $B$ separately.",,"['matrices', 'matrix-decomposition', 'matrix-rank']"
69,Condition for eigenvalues to have negative real parts (Hurwitz) for specific matrix structure,Condition for eigenvalues to have negative real parts (Hurwitz) for specific matrix structure,,"Let $$ A=\begin{bmatrix} P & \alpha x\\ -y^\top & 0\end{bmatrix}$$ where $P \in \mathbb{R}^{n \times n}$ is Hurwitz (the eigenvalues of $P$ have strictly negative real parts), $x, y \in \mathbb{R}^{n}$ , and $\alpha$ is a real positive scalar. Find the condition for $A$ to be Hurwitz for sufficiently small $\alpha > 0$ . I have analyzed that this is true whenever $y^\top P^{-1} x < 0$ holds. However, I am not able to prove this fact. Help in this regard would be appreciated.","Let where is Hurwitz (the eigenvalues of have strictly negative real parts), , and is a real positive scalar. Find the condition for to be Hurwitz for sufficiently small . I have analyzed that this is true whenever holds. However, I am not able to prove this fact. Help in this regard would be appreciated."," A=\begin{bmatrix} P & \alpha x\\ -y^\top & 0\end{bmatrix} P \in \mathbb{R}^{n \times n} P x, y \in \mathbb{R}^{n} \alpha A \alpha > 0 y^\top P^{-1} x < 0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'control-theory', 'hurwitz-matrices']"
70,Prove $\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B$ using only minors,Prove  using only minors,\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B,"Let $A,B$ be two $(m,n)$ matrices. Prove that $$\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B$$ using only the definition of $\rk$ involving non-zero minors. I expressed every minor $\Delta_r$ of $A+B$ as a linear combination of minors of $A$ and $B$, each with order not less than $\lceil \frac{r}{2} \rceil$, but this approach didn't get me to the result. I am a beginner in linear algebra and that is the only definition of $\rk$ I have learnt, so I would highly appreciate any solution using it. EDIT: I think I managed to find a solution. Let $A=(a_{ij}), B=(b_{ij})$ with $i\leq m$ and $j\leq n$. Also, $\rk A=r$ and $\rk B=s$. Then $A+B=(a_{ij}+b_{ij})$ and let $\Delta_{r+s+1}$ be a minor of $A+B$ of order $(r+s+1)$. Let $k=r+s+1$. $$\Delta_{k}=\det \begin{pmatrix} a_{i_1j_1}+b_{i_1j_1} & a_{i_1j_2}+b_{i_1j_2} &... &a_{i_1j_{k}}b_{i_1j_{k}} \\ ...&...&...&...\\ a_{i_{k}j_1}+b_{i_{k}j_1}& a_{i_kj_2}+b_{i_kj_2}& ...& a_{i_kj_k}+b_{i_kj_k}\end{pmatrix}$$ Using the properties of determinants, we can develop the one above as a linear combination of determinants with the form $$\det \begin{pmatrix} a_{i_1p_1} & a_{i_1p_2}&... &a_{i_1p_{x}} & b_{i_1q_1} &b_{i_1q_2} &... &b_{i_1q_y} \\ a_{i_2p_1} & a_{i_2p_2}&... &a_{i_2p_{x}} & b_{i_2q_1} &b_{i_2q_2} &... &b_{i_2q_y} \\ ...&...&...&...&...&...&...&...\\ a_{i_kp_1}& a_{i_kp_2}& ...& a_{i_kp_x} & b_{i_kq_1} & b_{i_kq_2}   & ... &b_{i_kq_y}\end{pmatrix}$$ where $x+y=k=r+s+1$ and $ \{ p_1,p_2,...p_x,q_1,q_2,...q_y \} = \{j_1,j_2,...j_k \}$ If $x \leq r$ then $y \geq s+1$ and we can write the last determinant as a linear combination of $(k-x)$order determinants consisting only of elements of  $B$. Since $k-x=y$, these will be minors of $B$ with order $y \geq s+1$ and since $\rk B=s$ it means that all these minors are $0$. The same holds true if $x\geq r+1$, when we write the determinant as a linear combination of minors of $A$ with order $x$, thus all equal to $0$. In conclusion, all the determinants with the form as above are $0$ and so $\Delta_k=0$, which means that $\rk(A+B) \leq r+s=\rk A+\rk B$","Let $A,B$ be two $(m,n)$ matrices. Prove that $$\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B$$ using only the definition of $\rk$ involving non-zero minors. I expressed every minor $\Delta_r$ of $A+B$ as a linear combination of minors of $A$ and $B$, each with order not less than $\lceil \frac{r}{2} \rceil$, but this approach didn't get me to the result. I am a beginner in linear algebra and that is the only definition of $\rk$ I have learnt, so I would highly appreciate any solution using it. EDIT: I think I managed to find a solution. Let $A=(a_{ij}), B=(b_{ij})$ with $i\leq m$ and $j\leq n$. Also, $\rk A=r$ and $\rk B=s$. Then $A+B=(a_{ij}+b_{ij})$ and let $\Delta_{r+s+1}$ be a minor of $A+B$ of order $(r+s+1)$. Let $k=r+s+1$. $$\Delta_{k}=\det \begin{pmatrix} a_{i_1j_1}+b_{i_1j_1} & a_{i_1j_2}+b_{i_1j_2} &... &a_{i_1j_{k}}b_{i_1j_{k}} \\ ...&...&...&...\\ a_{i_{k}j_1}+b_{i_{k}j_1}& a_{i_kj_2}+b_{i_kj_2}& ...& a_{i_kj_k}+b_{i_kj_k}\end{pmatrix}$$ Using the properties of determinants, we can develop the one above as a linear combination of determinants with the form $$\det \begin{pmatrix} a_{i_1p_1} & a_{i_1p_2}&... &a_{i_1p_{x}} & b_{i_1q_1} &b_{i_1q_2} &... &b_{i_1q_y} \\ a_{i_2p_1} & a_{i_2p_2}&... &a_{i_2p_{x}} & b_{i_2q_1} &b_{i_2q_2} &... &b_{i_2q_y} \\ ...&...&...&...&...&...&...&...\\ a_{i_kp_1}& a_{i_kp_2}& ...& a_{i_kp_x} & b_{i_kq_1} & b_{i_kq_2}   & ... &b_{i_kq_y}\end{pmatrix}$$ where $x+y=k=r+s+1$ and $ \{ p_1,p_2,...p_x,q_1,q_2,...q_y \} = \{j_1,j_2,...j_k \}$ If $x \leq r$ then $y \geq s+1$ and we can write the last determinant as a linear combination of $(k-x)$order determinants consisting only of elements of  $B$. Since $k-x=y$, these will be minors of $B$ with order $y \geq s+1$ and since $\rk B=s$ it means that all these minors are $0$. The same holds true if $x\geq r+1$, when we write the determinant as a linear combination of minors of $A$ with order $x$, thus all equal to $0$. In conclusion, all the determinants with the form as above are $0$ and so $\Delta_k=0$, which means that $\rk(A+B) \leq r+s=\rk A+\rk B$",,"['linear-algebra', 'matrices', 'matrix-rank']"
71,Need to improve upper bound for $\| (uv^T + B)^{-1} \|$ (Sherman-Morrison formula),Need to improve upper bound for  (Sherman-Morrison formula),\| (uv^T + B)^{-1} \|,"I have a matrix $A \in \mathbb{C}^{n \times n}$ in the form $A = uv^T + B$, where $u,v \in \mathbb{C}^n$ and $B \in \mathbb{C}^{n \times n}$. I know $A$ is invertible and want to find an upper bound for the norm of its inverse using the vectors $u,v$ and the matrix $B$. More precisely, I want a bound for $$\| A^{-1} \| = \| (uv^T + B)^{-1} \|,$$ where the norm can be the spectral or the Frobenius norm. I'm trying to come up with something in terms of $u,v,B$, but with no success. Also, searching here I found some posts about sum of psd matrices (like here and here ). This is not my case, unfortunately, but I still hope there is some idea out there to help me on this problem. The matrices $u^Tv$ and $B$ may be assumed to be invertible too. EDIT: In fact, I was able to get a bound, it is close but still not exactly what I need. Using the Sherman-Morrison formula we get $$\|(uv^T + B)^{-1}\| = \left\|B^{-1} - \frac{B^{-1}uv^TB^{-1}}{1+v^TB^{-1}u}\right\| \leq \|B^{-1}\| + \left\|\frac{B^{-1}uv^TB^{-1}}{1+v^TB^{-1}u}\right\| \leq $$ $$\leq \|B^{-1}\| + \frac{\|B^{-1}\| \|u\| \|v^T\| \|B^{-1}\|}{|1+v^TB^{-1}u|} = \|B^{-1}\| \left( 1+ \frac{\|B^{-1}\| \|u\| \|v^T\|}{|1+v^TB^{-1}u|} \right).$$ In order to have something useful for my purposes I need throw away or bound the factor $v^TB^{-1}u$ in the denominator. Suppose we have the norms of $u,v,B,B^{-1}$. Is there a way to use this and make the factor $v^TB^{-1}u$ disappear (I don't want to compute the inverse, only the norm of the inverse is accessible). Thank you.","I have a matrix $A \in \mathbb{C}^{n \times n}$ in the form $A = uv^T + B$, where $u,v \in \mathbb{C}^n$ and $B \in \mathbb{C}^{n \times n}$. I know $A$ is invertible and want to find an upper bound for the norm of its inverse using the vectors $u,v$ and the matrix $B$. More precisely, I want a bound for $$\| A^{-1} \| = \| (uv^T + B)^{-1} \|,$$ where the norm can be the spectral or the Frobenius norm. I'm trying to come up with something in terms of $u,v,B$, but with no success. Also, searching here I found some posts about sum of psd matrices (like here and here ). This is not my case, unfortunately, but I still hope there is some idea out there to help me on this problem. The matrices $u^Tv$ and $B$ may be assumed to be invertible too. EDIT: In fact, I was able to get a bound, it is close but still not exactly what I need. Using the Sherman-Morrison formula we get $$\|(uv^T + B)^{-1}\| = \left\|B^{-1} - \frac{B^{-1}uv^TB^{-1}}{1+v^TB^{-1}u}\right\| \leq \|B^{-1}\| + \left\|\frac{B^{-1}uv^TB^{-1}}{1+v^TB^{-1}u}\right\| \leq $$ $$\leq \|B^{-1}\| + \frac{\|B^{-1}\| \|u\| \|v^T\| \|B^{-1}\|}{|1+v^TB^{-1}u|} = \|B^{-1}\| \left( 1+ \frac{\|B^{-1}\| \|u\| \|v^T\|}{|1+v^TB^{-1}u|} \right).$$ In order to have something useful for my purposes I need throw away or bound the factor $v^TB^{-1}u$ in the denominator. Suppose we have the norms of $u,v,B,B^{-1}$. Is there a way to use this and make the factor $v^TB^{-1}u$ disappear (I don't want to compute the inverse, only the norm of the inverse is accessible). Thank you.",,"['matrices', 'normed-spaces', 'inverse', 'upper-lower-bounds']"
72,Taylor approximation to a matrix logarithm of a product of matrix exponentials,Taylor approximation to a matrix logarithm of a product of matrix exponentials,,"Let $\mu_i,\nu_i\in\mathbb{R}^{n\times n}$ for $i\in{\{1,\dots,I\}}$, and let $\sigma\in\mathbb{R}$. Assume that $\log{\prod_{i=1}^I{\exp{(\mu_i)}}}$ exists with real entries. It must be the case that for some matrix $M\in\mathbb{R}^{n\times n}$: $$\log{\prod_{i=1}^I{\exp{(\mu_i+\sigma\nu_i)}}}=\log{\prod_{i=1}^I{\exp{(\mu_i)}}}+M\sigma+O(\sigma ^2 )$$ as $\sigma\rightarrow 0$. What is $M$? The derivative of an exponential of an arbitrary matrix function seems to be rather messy, but I was hoping  there would be a simpler answer thanks to linearity, and only being interested in first order terms. Further, by vectorising and using the properties of Kronecker products and commutation matrices, can one express $M$ in the form: $$\operatorname{vec}{M}=\sum_{i=1}^I{f_i(\mu_1,\dots,\mu_I)\operatorname{vec}{\nu_i}}\space\space\space\space\space ?$$ P.S. I've tagged this with the Lie group and Lie algebra tags as it's abundantly clear that much of the relevant results are in the more general case. But my knowledge of Lie theory is minimal, so it would be appreciated if answers could keep things as specific to real matrices as possible.","Let $\mu_i,\nu_i\in\mathbb{R}^{n\times n}$ for $i\in{\{1,\dots,I\}}$, and let $\sigma\in\mathbb{R}$. Assume that $\log{\prod_{i=1}^I{\exp{(\mu_i)}}}$ exists with real entries. It must be the case that for some matrix $M\in\mathbb{R}^{n\times n}$: $$\log{\prod_{i=1}^I{\exp{(\mu_i+\sigma\nu_i)}}}=\log{\prod_{i=1}^I{\exp{(\mu_i)}}}+M\sigma+O(\sigma ^2 )$$ as $\sigma\rightarrow 0$. What is $M$? The derivative of an exponential of an arbitrary matrix function seems to be rather messy, but I was hoping  there would be a simpler answer thanks to linearity, and only being interested in first order terms. Further, by vectorising and using the properties of Kronecker products and commutation matrices, can one express $M$ in the form: $$\operatorname{vec}{M}=\sum_{i=1}^I{f_i(\mu_1,\dots,\mu_I)\operatorname{vec}{\nu_i}}\space\space\space\space\space ?$$ P.S. I've tagged this with the Lie group and Lie algebra tags as it's abundantly clear that much of the relevant results are in the more general case. But my knowledge of Lie theory is minimal, so it would be appreciated if answers could keep things as specific to real matrices as possible.",,"['matrices', 'lie-groups', 'lie-algebras', 'matrix-calculus', 'perturbation-theory']"
73,How to 'diagonalise' this special $(N+1)\times(N)$ matrix (described in the text)?,How to 'diagonalise' this special  matrix (described in the text)?,(N+1)\times(N),"Here is a special $(N+1)\times N$ matrix: $$A=\begin{pmatrix}a_1&a_2&a_3&\ldots&a_N\\b_1&0&0&\ldots&0\\0&b_2&0&\ldots&0\\0&0&b_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&b_N\end{pmatrix}_{(N+1)\times N}$$ Where the matrix elements $a_i$, $b_i$ are all real and positive. Does it exist two orthogonal matrices $U_{(N+1)\times(N+1)}$ and $V_{N\times N}$, satisfying $$UAV=\begin{pmatrix}0&0&0&\ldots&0\\\xi_1&0&0&\ldots&0\\0&\xi_2&0&\ldots&0\\0&0&\xi_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&\xi_N\end{pmatrix}_{(N+1)\times N}$$? For the case $a_i\ll b_i$, I can find the answer (within the accuracy of $\frac{a_i}{b_i}$); but for the general case, I have no idea. I am not sure if this problem is a kind of typical exercise in the linear algebra textbook. Thanks for everyone who help me to solve this problem or give me some hints.","Here is a special $(N+1)\times N$ matrix: $$A=\begin{pmatrix}a_1&a_2&a_3&\ldots&a_N\\b_1&0&0&\ldots&0\\0&b_2&0&\ldots&0\\0&0&b_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&b_N\end{pmatrix}_{(N+1)\times N}$$ Where the matrix elements $a_i$, $b_i$ are all real and positive. Does it exist two orthogonal matrices $U_{(N+1)\times(N+1)}$ and $V_{N\times N}$, satisfying $$UAV=\begin{pmatrix}0&0&0&\ldots&0\\\xi_1&0&0&\ldots&0\\0&\xi_2&0&\ldots&0\\0&0&\xi_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&\xi_N\end{pmatrix}_{(N+1)\times N}$$? For the case $a_i\ll b_i$, I can find the answer (within the accuracy of $\frac{a_i}{b_i}$); but for the general case, I have no idea. I am not sure if this problem is a kind of typical exercise in the linear algebra textbook. Thanks for everyone who help me to solve this problem or give me some hints.",,"['linear-algebra', 'matrices', 'diagonalization']"
74,Companion matrices for multivariate polynomials?,Companion matrices for multivariate polynomials?,,"I am aware of companion matrices for single variable polynomials: $$p(x) = x^k+c_{k-1}x^{k-1}+\cdots +c_0$$ $${\bf C_p} = \left[\begin{array}{ll}{\bf 0}^T & -c_0 \\ {\bf I}_{k-1} & {\bf c}_{1:k}\end{array}\right]$$ $\bf C_p$ will have the same eigenvalues as the roots of $p(x)=0$ Are there any ways to build companion matrices for multivariate polynomials (in some sense)? EDIT for example what I would like is that if I have $f(x,y) = (x+1)^2+(y+1)^2-1$ I can (by some recipe or algorithm) build a matrix that has the same properties in the sense of approximating roots of the polynomial when iterating matrix multiplication in the same way as one variable do.","I am aware of companion matrices for single variable polynomials: $$p(x) = x^k+c_{k-1}x^{k-1}+\cdots +c_0$$ $${\bf C_p} = \left[\begin{array}{ll}{\bf 0}^T & -c_0 \\ {\bf I}_{k-1} & {\bf c}_{1:k}\end{array}\right]$$ $\bf C_p$ will have the same eigenvalues as the roots of $p(x)=0$ Are there any ways to build companion matrices for multivariate polynomials (in some sense)? EDIT for example what I would like is that if I have $f(x,y) = (x+1)^2+(y+1)^2-1$ I can (by some recipe or algorithm) build a matrix that has the same properties in the sense of approximating roots of the polynomial when iterating matrix multiplication in the same way as one variable do.",,"['linear-algebra', 'matrices', 'polynomials', 'multivariate-polynomial', 'companion-matrices']"
75,Proof of the Weighted Generalized Inverse matrix,Proof of the Weighted Generalized Inverse matrix,,"I am having trouble understanding how to get the weighted generalized inverse of a matrix. Let me start from the beginning. Suppose $$a=Xb$$ Where $a$ is a vector with m elements, $b$ is a vector of n elements and $X$ is a matrix with mxn elements. Solving for $b$ we get $$b={X}^{+}a$$ Where ${X}^{+}$ is the Moore-Penrose inverse defined as $$X^+ = X^T (XX^T)^{-1}$$ Where $X^T$ is the transpose of $X$. The weighted generalized inverse is given by $$X_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}$$ Making the weighted solution of $b$ $$b_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}a$$ The proofs that I have found start from places that I don't understand how they end up there to begin with. I probably don't know something relatively basic, but can someone tell me where I can find the proof without taking any assumption for granted? Any help would be appreciated.","I am having trouble understanding how to get the weighted generalized inverse of a matrix. Let me start from the beginning. Suppose $$a=Xb$$ Where $a$ is a vector with m elements, $b$ is a vector of n elements and $X$ is a matrix with mxn elements. Solving for $b$ we get $$b={X}^{+}a$$ Where ${X}^{+}$ is the Moore-Penrose inverse defined as $$X^+ = X^T (XX^T)^{-1}$$ Where $X^T$ is the transpose of $X$. The weighted generalized inverse is given by $$X_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}$$ Making the weighted solution of $b$ $$b_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}a$$ The proofs that I have found start from places that I don't understand how they end up there to begin with. I probably don't know something relatively basic, but can someone tell me where I can find the proof without taking any assumption for granted? Any help would be appreciated.",,['matrices']
76,How many independent parameters are in a skew-symmetric as well as orthogonal matrix?,How many independent parameters are in a skew-symmetric as well as orthogonal matrix?,,"I'm currently trying to parameterize a given real and square matrix $A$ with the properties $A^T=-A$ and $A^TA=\textbf{1}_N$, for even $N$. I don't know how many independent parameters I would have for a given choice of $N$ and I would like to have a formula and a proof for this, but I don't know how. For instance, a general orthogonal matrix has $N(N-1)/2$ independent parameters. I've found that if $A$ is a $4 \times 4$, then I have $2$ independent parameters, if there are no further restrictions.","I'm currently trying to parameterize a given real and square matrix $A$ with the properties $A^T=-A$ and $A^TA=\textbf{1}_N$, for even $N$. I don't know how many independent parameters I would have for a given choice of $N$ and I would like to have a formula and a proof for this, but I don't know how. For instance, a general orthogonal matrix has $N(N-1)/2$ independent parameters. I've found that if $A$ is a $4 \times 4$, then I have $2$ independent parameters, if there are no further restrictions.",,"['linear-algebra', 'matrices', 'orthogonal-matrices']"
77,Partial fractions to find $A^n$,Partial fractions to find,A^n,"I'm confused on how to evaluate the individual parts to get the values for $c_1, c_2$ Given the matrix $$A = \begin{bmatrix}2 & 3\\ 3 & 2 \end{bmatrix},$$ I know that $$\det(xI-A) = (x+1)(x-5).$$ Suppose $$\frac{x^n}{f(x)} = q(x)+\frac{c_1}{x+1}+\frac{c_2}{x-5},$$ the solution is shown as: \begin{align*} c_1 &= \left.\frac{x^n}{x-5}\right|_{x=-1} = \frac{1}{6} \cdot (-1)^{n+1},\\ c_2 &= \left.\frac{x^n}{x+1}\right|_{x=5} = \frac{1}{6} \cdot 5^{n}. \end{align*} I'm not too sure why the first numerator goes to $(-1)^{n+1}$ and not just $(-1)^n$ so some clarification would be nice. Also if we had $(x-5)^2$ what would be the difference in evaluating that?","I'm confused on how to evaluate the individual parts to get the values for $c_1, c_2$ Given the matrix $$A = \begin{bmatrix}2 & 3\\ 3 & 2 \end{bmatrix},$$ I know that $$\det(xI-A) = (x+1)(x-5).$$ Suppose $$\frac{x^n}{f(x)} = q(x)+\frac{c_1}{x+1}+\frac{c_2}{x-5},$$ the solution is shown as: \begin{align*} c_1 &= \left.\frac{x^n}{x-5}\right|_{x=-1} = \frac{1}{6} \cdot (-1)^{n+1},\\ c_2 &= \left.\frac{x^n}{x+1}\right|_{x=5} = \frac{1}{6} \cdot 5^{n}. \end{align*} I'm not too sure why the first numerator goes to $(-1)^{n+1}$ and not just $(-1)^n$ so some clarification would be nice. Also if we had $(x-5)^2$ what would be the difference in evaluating that?",,"['linear-algebra', 'matrices']"
78,SU(n) is a manifold via charts,SU(n) is a manifold via charts,,"I'm trying to construct an atlas the painful way for $SU(n)$, using charts. According to Wiki, SU(n) is a real manifold of real dimension $n^2-1$; I can believe this because you can perform a Cayley Transformation for any $U \in SU(n)$ such that the map $C:M_n(\mathbb{C})^{*}\longrightarrow M_n(\mathbb{C})$ $$ C : U \longmapsto C(U) = (1_n - U)(1_n+U)^{-1}  $$ Which is well defined because any unimodular unitary matrices are non exceptional (that is that have $\det(1_n+U)\neq 0$). Thus the Cayley transformation defines a homeomorphism, given that you can use the Lie algebra $$\mathfrak{su}(n) = \{X \in GL_n(\mathbb{C}): X^{\dagger}+X=0,\; Tr(X)=0\} $$ to explicitly compute the dimension. But what I want to try is to find explicit charts for these guys. This is what I've tried so far (restricting to $SU(3)$ to see the machinery involved); given that $$SU(3) = \left\{\;\;\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right) \; \left| \right. \left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)^{\dagger}\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)=\left(\begin{smallmatrix}1&0&0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{smallmatrix}\right) \; \& \;|ei-hf|^2+|gf-di|^2+|dh-ge|^2 +|dh-ge|^2=1    \;\;\right\} $$ Where you get a whole bunch of conditions from the unitary conditions, and thus a whole set of equivalent conditions for the determinant. I got these by setting the inverse equal to the hermitian conjugate: Unitary Conditions \begin{eqnarray} a=\overline{ei-hf}\\ b=\overline{gf-di}\\ c=\overline{dh-ge}\\ d=\overline{ch-bi}\\ e=\overline{ai-gc}\\ f=\overline{bg-ah}\\ g=\overline{bf-ce}\\ h=\overline{dc-af}\\ i=\overline{ae-bd} \end{eqnarray} Determinant Examples \begin{eqnarray} a(ei-hf)-b(di-gf)+c(dh-ge)=1\\ |ei-hf|^2+|gf-di|^2+|dh-ge|^2=1\\ |a|^2+|b|^2+|c|^2=1 \;\;\; \text{etc.} \end{eqnarray} Now in trying to construct a chart, going by the fact $a,\ldots,i \in \mathbb{C}$, there are 18 real degrees of freedom/9 complex. Since $\mathbb{C}^4 \cong \mathbb{R}^8$, I think it may be easier to work with $\mathbb{C}^4$, since you can always express the matrix entries explicitly using four entries. And then this is where I think it best to write out a chart, say for $U_a :=\{ei-hf\neq 0 \Leftrightarrow a\neq 0\}$, which is open, as  $$\phi_a : U_a \longrightarrow \phi_a(U_a) \subseteq GL_2(\mathbb{C}) \subseteq \mathbb{C}^4  $$ $$\phi_a (\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)) = (e,f,h,i) \;\;\;\text{equivalently} \left(\begin{smallmatrix}e & f\\h& i\end{smallmatrix}\right) $$ Then, trying to invert it, things go a bit awry, and when I try to recalculate the explicit inverse, my construction seems to fail, since everytime I try to include the unitary condition and determinant condition in general, things go funny. Any ideas would be really appreciated!","I'm trying to construct an atlas the painful way for $SU(n)$, using charts. According to Wiki, SU(n) is a real manifold of real dimension $n^2-1$; I can believe this because you can perform a Cayley Transformation for any $U \in SU(n)$ such that the map $C:M_n(\mathbb{C})^{*}\longrightarrow M_n(\mathbb{C})$ $$ C : U \longmapsto C(U) = (1_n - U)(1_n+U)^{-1}  $$ Which is well defined because any unimodular unitary matrices are non exceptional (that is that have $\det(1_n+U)\neq 0$). Thus the Cayley transformation defines a homeomorphism, given that you can use the Lie algebra $$\mathfrak{su}(n) = \{X \in GL_n(\mathbb{C}): X^{\dagger}+X=0,\; Tr(X)=0\} $$ to explicitly compute the dimension. But what I want to try is to find explicit charts for these guys. This is what I've tried so far (restricting to $SU(3)$ to see the machinery involved); given that $$SU(3) = \left\{\;\;\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right) \; \left| \right. \left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)^{\dagger}\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)=\left(\begin{smallmatrix}1&0&0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{smallmatrix}\right) \; \& \;|ei-hf|^2+|gf-di|^2+|dh-ge|^2 +|dh-ge|^2=1    \;\;\right\} $$ Where you get a whole bunch of conditions from the unitary conditions, and thus a whole set of equivalent conditions for the determinant. I got these by setting the inverse equal to the hermitian conjugate: Unitary Conditions \begin{eqnarray} a=\overline{ei-hf}\\ b=\overline{gf-di}\\ c=\overline{dh-ge}\\ d=\overline{ch-bi}\\ e=\overline{ai-gc}\\ f=\overline{bg-ah}\\ g=\overline{bf-ce}\\ h=\overline{dc-af}\\ i=\overline{ae-bd} \end{eqnarray} Determinant Examples \begin{eqnarray} a(ei-hf)-b(di-gf)+c(dh-ge)=1\\ |ei-hf|^2+|gf-di|^2+|dh-ge|^2=1\\ |a|^2+|b|^2+|c|^2=1 \;\;\; \text{etc.} \end{eqnarray} Now in trying to construct a chart, going by the fact $a,\ldots,i \in \mathbb{C}$, there are 18 real degrees of freedom/9 complex. Since $\mathbb{C}^4 \cong \mathbb{R}^8$, I think it may be easier to work with $\mathbb{C}^4$, since you can always express the matrix entries explicitly using four entries. And then this is where I think it best to write out a chart, say for $U_a :=\{ei-hf\neq 0 \Leftrightarrow a\neq 0\}$, which is open, as  $$\phi_a : U_a \longrightarrow \phi_a(U_a) \subseteq GL_2(\mathbb{C}) \subseteq \mathbb{C}^4  $$ $$\phi_a (\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)) = (e,f,h,i) \;\;\;\text{equivalently} \left(\begin{smallmatrix}e & f\\h& i\end{smallmatrix}\right) $$ Then, trying to invert it, things go a bit awry, and when I try to recalculate the explicit inverse, my construction seems to fail, since everytime I try to include the unitary condition and determinant condition in general, things go funny. Any ideas would be really appreciated!",,"['matrices', 'differential-geometry', 'representation-theory', 'lie-groups', 'lie-algebras']"
79,What's the probability of that matrix multiplication can exchange? [closed],What's the probability of that matrix multiplication can exchange? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Given two n-order matrices $A,B$,whose elements follow the Gaussian distribution on R.what's the probability that $AB=BA$?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Given two n-order matrices $A,B$,whose elements follow the Gaussian distribution on R.what's the probability that $AB=BA$?",,"['linear-algebra', 'matrices']"
80,Layout convention in matrix derivation,Layout convention in matrix derivation,,"In a book i'm reading there are these formulas, $y$ and $\beta$ are vectors of same dimensions, $X$ is a matrix: $$RSS(\beta) = (y - X\beta)^T(y - X\beta)$$ $$\frac{\partial RSS(\beta)}{\partial \beta} = -2X^T(y - X\beta)$$ $$\frac{\partial^2 RSS(\beta)}{\partial \beta \partial \beta^T} = 2X^TX$$ From what I understand, when he uses $\partial \beta$ as denominator, it uses the numerator layout convention, meaning the derivative is actually calculated w.r.t $\partial \beta^T$, and when $\partial \beta^T$ is used explicitly, it is just the same as using $\partial \beta$ with the numerator layout. If that is correct, then why is he using two layouts at the same time in the second order derivative, why not just write $\partial \beta^2$, or even $\partial \beta^T\partial \beta$ ? If using $\partial \beta$ or $\partial \beta^T$ as denominator produces the same result, why not always be explicit and use $\partial \beta^T$ to avoid confusion ? Is there any advantage to have multiple layouts for matrix derivatives ?","In a book i'm reading there are these formulas, $y$ and $\beta$ are vectors of same dimensions, $X$ is a matrix: $$RSS(\beta) = (y - X\beta)^T(y - X\beta)$$ $$\frac{\partial RSS(\beta)}{\partial \beta} = -2X^T(y - X\beta)$$ $$\frac{\partial^2 RSS(\beta)}{\partial \beta \partial \beta^T} = 2X^TX$$ From what I understand, when he uses $\partial \beta$ as denominator, it uses the numerator layout convention, meaning the derivative is actually calculated w.r.t $\partial \beta^T$, and when $\partial \beta^T$ is used explicitly, it is just the same as using $\partial \beta$ with the numerator layout. If that is correct, then why is he using two layouts at the same time in the second order derivative, why not just write $\partial \beta^2$, or even $\partial \beta^T\partial \beta$ ? If using $\partial \beta$ or $\partial \beta^T$ as denominator produces the same result, why not always be explicit and use $\partial \beta^T$ to avoid confusion ? Is there any advantage to have multiple layouts for matrix derivatives ?",,"['calculus', 'matrices']"
81,"Tangent space of smooth manifold of ""matrices with same rank""","Tangent space of smooth manifold of ""matrices with same rank""",,"Define $$M(n,M\times N)=\{X\in \mathbb{R}^{M\times N} \mid \operatorname{rank} X = n\}$$  We know: $M(n,M\times N)$ is a smooth and connected manifold. The tangent space of $M(n,M\times N)$ at an element $X$ is  $$T_X M(n,M\times N) = \{AX + XB \mid A\in \mathbb{R}^{M\times M}, B\in \mathbb{R}^{N\times N}\}$$ My question is the following lemma: Does the green part means $$A(t)X(t) + X(t)B(t)$$ is of rank $n$? If yes, why? I cannot follow this.","Define $$M(n,M\times N)=\{X\in \mathbb{R}^{M\times N} \mid \operatorname{rank} X = n\}$$  We know: $M(n,M\times N)$ is a smooth and connected manifold. The tangent space of $M(n,M\times N)$ at an element $X$ is  $$T_X M(n,M\times N) = \{AX + XB \mid A\in \mathbb{R}^{M\times M}, B\in \mathbb{R}^{N\times N}\}$$ My question is the following lemma: Does the green part means $$A(t)X(t) + X(t)B(t)$$ is of rank $n$? If yes, why? I cannot follow this.",,"['matrices', 'ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds', 'matrix-rank']"
82,"Linear transformation matrix with respect to basis, using transition matrices","Linear transformation matrix with respect to basis, using transition matrices",,"I have linear transformation $ \Bbb{R}^3\to \Bbb{R}^2 $, matrix of LT with respect to bases $B = \{(1;1;1),(0;1;0),(1;1;0)\}$ and $B'=\{(1;1),(0;1)\}$ is: $$A = \begin{bmatrix}2 & 1 & 3\\3 & 1 & -3\end{bmatrix}$$ My task is to compute matrix $A'$ (LT with respect to standard bases), and using transition matrices. I suppose I have to compute transition matrix from std basis $S$ to basis $B$ in $ \Bbb{R}^3$, so $T_{SB}$ , then ${T'}_{B'S'}$ in $ \Bbb{R}^2$ and get the matrix $A'$ using formula $A'=T'\cdot A\cdot T$. Is it correct? I am just not able to get correct solution this way.","I have linear transformation $ \Bbb{R}^3\to \Bbb{R}^2 $, matrix of LT with respect to bases $B = \{(1;1;1),(0;1;0),(1;1;0)\}$ and $B'=\{(1;1),(0;1)\}$ is: $$A = \begin{bmatrix}2 & 1 & 3\\3 & 1 & -3\end{bmatrix}$$ My task is to compute matrix $A'$ (LT with respect to standard bases), and using transition matrices. I suppose I have to compute transition matrix from std basis $S$ to basis $B$ in $ \Bbb{R}^3$, so $T_{SB}$ , then ${T'}_{B'S'}$ in $ \Bbb{R}^2$ and get the matrix $A'$ using formula $A'=T'\cdot A\cdot T$. Is it correct? I am just not able to get correct solution this way.",,"['linear-algebra', 'matrices', 'linear-transformations']"
83,Auto Path in Dokapon Kingdom - how do they do it?,Auto Path in Dokapon Kingdom - how do they do it?,,"In the video game Dokapon Kingdom ( here's an example video on it ), the objective is to go around saving towns while simultaneously screwing over your friends (who will no longer be your friends). (It's been described as "" Mario Party meets an RPG"".) Prominently featured within that game is the Auto Path function, which determines what spaces are reachable from the current space using a path of length $n$, namely, what the player rolled. Said path can loop multiple times, and can even end up turning around on itself, but it cannot traverse the same edge twice in a row (i.e. no sudden U-turns). Translating the game board to a graph (and its associated adjacency matrix) is very easy, but how does one enforce the no sudden U-turns rule? Using the squared matrix with its diagonal zeroed out doesn't work, as that still allows a U-turn after every other edge taken.","In the video game Dokapon Kingdom ( here's an example video on it ), the objective is to go around saving towns while simultaneously screwing over your friends (who will no longer be your friends). (It's been described as "" Mario Party meets an RPG"".) Prominently featured within that game is the Auto Path function, which determines what spaces are reachable from the current space using a path of length $n$, namely, what the player rolled. Said path can loop multiple times, and can even end up turning around on itself, but it cannot traverse the same edge twice in a row (i.e. no sudden U-turns). Translating the game board to a graph (and its associated adjacency matrix) is very easy, but how does one enforce the no sudden U-turns rule? Using the squared matrix with its diagonal zeroed out doesn't work, as that still allows a U-turn after every other edge taken.",,"['matrices', 'graph-theory']"
84,Integral polynomial interpolation and the Vandermonde matrix,Integral polynomial interpolation and the Vandermonde matrix,,"I'm thinking about the following problem: Given $n$ pairs of integers $(x_i, y_i)$, when is there a polynomial $f \in \Bbb Z[x]$ with $f(x_i) = y_i$ for each $i$? We can always do this with a rational polynomial, and the following procedure gives a way to determine if it is possible with integer polynomials or not: Suppose that such an $f = f_0$ exists. By vertical/horizontal shifts, we may assume that $(x_n, y_n) = (0,0)$. Then $f_0(0) = 0$ so $f_0 = xf_1$ for some $f_1 \in \Bbb Z[x]$ that must satisfy $f_1(x_i - x_n) = \frac{y_i-y_n}{x_i-x_n}$ for $i = 1, \ldots, n-1$. Now induct. All of the steps are reversible, so this shows that if such an integer polynomial exists, we can take it to be of degree $n-1$. Instead of prescribing the $y_i$ at the beginning, let's ask ``What $n$-tuples of integers are of the form $(f(x_1), \ldots, f(x_n))$ for some $f \in \Bbb Z[x]$ and some fixed $x_i$?"" Equivalently, what is the image of the natural map $\Bbb Z[x] \to \Bbb Z^n$ given by evaluation at those $n$ points? Since I'm really interested in the failure of this map to be surjective, the object I care about is the cokernel. To set some notation, if $S$ is a finite set of integers (let's say it's ordered) with $|S| = n$, let $V_S$ be the cokernel of the map $\Bbb Z[x] \to \Bbb Z^n$ given by evaluating at the elements of $S$. The observation above shows that WLOG we can replace $\Bbb Z[x]$ by the group $\Bbb Z[x]_n$ of polynomials of degree at most $n-1$, and the map $\Bbb Z[x]_n \to \Bbb Z^n$ is given by the Vandermonde matrix with parameters the elements of $S$. This immediately tells us that the size of $V_S$ is the determinant of the Vandermonde matrix. Also, the procedure above realizes $V_S$ as an extension \begin{equation*} 0 \to V_{(S-a_n) \setminus \{0\}} \to V_S \to \bigoplus_{i=1}^{n-1} \Bbb Z/(a_i-a_n)\Bbb Z \to 0 \end{equation*} which has been helpful for small $n$. Is anything else known about the structure of $V_S$, and in particular, the map $\Bbb Z^n \to V_S$? Maybe we can determine the corresponding element of an Ext group (coming from the above SES) in some inductive way? I'm also interested in multivariable polynomials, but the problem seems less tractable there.","I'm thinking about the following problem: Given $n$ pairs of integers $(x_i, y_i)$, when is there a polynomial $f \in \Bbb Z[x]$ with $f(x_i) = y_i$ for each $i$? We can always do this with a rational polynomial, and the following procedure gives a way to determine if it is possible with integer polynomials or not: Suppose that such an $f = f_0$ exists. By vertical/horizontal shifts, we may assume that $(x_n, y_n) = (0,0)$. Then $f_0(0) = 0$ so $f_0 = xf_1$ for some $f_1 \in \Bbb Z[x]$ that must satisfy $f_1(x_i - x_n) = \frac{y_i-y_n}{x_i-x_n}$ for $i = 1, \ldots, n-1$. Now induct. All of the steps are reversible, so this shows that if such an integer polynomial exists, we can take it to be of degree $n-1$. Instead of prescribing the $y_i$ at the beginning, let's ask ``What $n$-tuples of integers are of the form $(f(x_1), \ldots, f(x_n))$ for some $f \in \Bbb Z[x]$ and some fixed $x_i$?"" Equivalently, what is the image of the natural map $\Bbb Z[x] \to \Bbb Z^n$ given by evaluation at those $n$ points? Since I'm really interested in the failure of this map to be surjective, the object I care about is the cokernel. To set some notation, if $S$ is a finite set of integers (let's say it's ordered) with $|S| = n$, let $V_S$ be the cokernel of the map $\Bbb Z[x] \to \Bbb Z^n$ given by evaluating at the elements of $S$. The observation above shows that WLOG we can replace $\Bbb Z[x]$ by the group $\Bbb Z[x]_n$ of polynomials of degree at most $n-1$, and the map $\Bbb Z[x]_n \to \Bbb Z^n$ is given by the Vandermonde matrix with parameters the elements of $S$. This immediately tells us that the size of $V_S$ is the determinant of the Vandermonde matrix. Also, the procedure above realizes $V_S$ as an extension \begin{equation*} 0 \to V_{(S-a_n) \setminus \{0\}} \to V_S \to \bigoplus_{i=1}^{n-1} \Bbb Z/(a_i-a_n)\Bbb Z \to 0 \end{equation*} which has been helpful for small $n$. Is anything else known about the structure of $V_S$, and in particular, the map $\Bbb Z^n \to V_S$? Maybe we can determine the corresponding element of an Ext group (coming from the above SES) in some inductive way? I'm also interested in multivariable polynomials, but the problem seems less tractable there.",,"['abstract-algebra', 'matrices', 'polynomials']"
85,what does inequality between traces of inverse of matrices imply?,what does inequality between traces of inverse of matrices imply?,,"Let $A$ and $B$ be positive definite matrices. In which case is the following relation correct please? $$ Tr(A^{-1})\geq Tr(B^{-1})\implies Tr(A)\leq Tr(B), $$ where $Tr(A)$ means the trace of the matrix $A$ .",Let and be positive definite matrices. In which case is the following relation correct please? where means the trace of the matrix .,"A B 
Tr(A^{-1})\geq Tr(B^{-1})\implies Tr(A)\leq Tr(B),
 Tr(A) A","['linear-algebra', 'matrices', 'trace']"
86,Prove matrix $A^TD-C^TB=I$,Prove matrix,A^TD-C^TB=I,"Let $A$, $B$, $C$ and $D$ be square matrices $n\times n$ over $ \mathbb{R} $  . Assume that $AB^T$ and $CD^T$ are symmetric . and $AD^T-BC^T=I$ Prove that:  $A^TD-C^TB=I$ I know this question has answer here: Prove that $A^TD-C^TB=I$ But I don't understand the accepted answer(Where that equation came from?): Hint: The given condition says that   $$ \pmatrix{A&-B\\ -C&D}\pmatrix{D^T&B^T\\ C^T&A^T} = \pmatrix{I&0\\ 0&I}. $$   Now, note that $XY=I$ implies that $YX=I$ and in turn $X^TY^T=I$. Is there any simpler way? Do $A^TD $ and $C^TB$ must be symmetric? And if so, how to prove it?","Let $A$, $B$, $C$ and $D$ be square matrices $n\times n$ over $ \mathbb{R} $  . Assume that $AB^T$ and $CD^T$ are symmetric . and $AD^T-BC^T=I$ Prove that:  $A^TD-C^TB=I$ I know this question has answer here: Prove that $A^TD-C^TB=I$ But I don't understand the accepted answer(Where that equation came from?): Hint: The given condition says that   $$ \pmatrix{A&-B\\ -C&D}\pmatrix{D^T&B^T\\ C^T&A^T} = \pmatrix{I&0\\ 0&I}. $$   Now, note that $XY=I$ implies that $YX=I$ and in turn $X^TY^T=I$. Is there any simpler way? Do $A^TD $ and $C^TB$ must be symmetric? And if so, how to prove it?",,"['linear-algebra', 'matrices']"
87,How can we prove Sylvester's determinant identity?,How can we prove Sylvester's determinant identity?,,"Sylvester's determinant identity states that if $A$ and $B$ are matrices of sizes $m\times n$ and $n\times m$, then $$ \det(I_m+AB) = \det(I_n+BA)$$ where $I_m$ and $I_n$ denote the $m \times m$ and $n \times n$ identity matrices, respectively. Could you sketch a proof for me, or point to an accessible reference?","Sylvester's determinant identity states that if $A$ and $B$ are matrices of sizes $m\times n$ and $n\times m$, then $$ \det(I_m+AB) = \det(I_n+BA)$$ where $I_m$ and $I_n$ denote the $m \times m$ and $n \times n$ identity matrices, respectively. Could you sketch a proof for me, or point to an accessible reference?",,"['linear-algebra', 'matrices', 'determinant']"
88,Matrix product calculation of coefficients of squared polynomial,Matrix product calculation of coefficients of squared polynomial,,"Answers to this question describe how, if we square an order- $m$ polynomial with coefficients $a_k$ , $$ p(x) = \left( \sum_{k=0}^{m} a_k x^k \right)^2 = \sum_{k=0}^{2m} c_k x^k $$ the coefficients $c_k$ of the resulting order- $2m$ polynomial can be calculated as $$ c_k = \sum_{j=0}^{k} a_j a_{k-j} $$ provided we take $a_j = 0$ in cases where $j \notin \{ 0, \dots, m\}$ . Question: If we let $\mathbf{c}$ be a length $2m+1$ vector containing all the $c_k$ , and $\mathbf{a}$ be a length $m+1$ vector containing all the $a_k$ , is it possible to express $\mathbf{c}$ as a matrix product of $\mathbf{a}$ with other matrices? I've been thinking about constructing some matrices to pad the $\mathbf{a}$ with zeros up to the length of $\mathbf{c}$ , then making a reversed copy, but haven't been able to get any further. Thanks!","Answers to this question describe how, if we square an order- polynomial with coefficients , the coefficients of the resulting order- polynomial can be calculated as provided we take in cases where . Question: If we let be a length vector containing all the , and be a length vector containing all the , is it possible to express as a matrix product of with other matrices? I've been thinking about constructing some matrices to pad the with zeros up to the length of , then making a reversed copy, but haven't been able to get any further. Thanks!","m a_k  p(x) = \left( \sum_{k=0}^{m} a_k x^k \right)^2 = \sum_{k=0}^{2m} c_k x^k  c_k 2m  c_k = \sum_{j=0}^{k} a_j a_{k-j}  a_j = 0 j \notin \{ 0, \dots, m\} \mathbf{c} 2m+1 c_k \mathbf{a} m+1 a_k \mathbf{c} \mathbf{a} \mathbf{a} \mathbf{c}","['matrices', 'polynomials', 'convolution']"
89,The set of all real square roots of a real matrix cannot be countably infinite,The set of all real square roots of a real matrix cannot be countably infinite,,Let $A \in End(\mathbb R^n)$ be any endomorphism. Prove that the set of solutions of equation $B^2 = A$ is finite or uncountable. Any help would be much appreciated.,Let $A \in End(\mathbb R^n)$ be any endomorphism. Prove that the set of solutions of equation $B^2 = A$ is finite or uncountable. Any help would be much appreciated.,,"['linear-algebra', 'abstract-algebra', 'matrices']"
90,Convergence of a sequence of complex matrices,Convergence of a sequence of complex matrices,,Suppose that $A \in \mathcal M_n(\mathbb C)$ is a complex matrix such that the sequence $(A^p)_{p \in \mathbb N}$ is bounded. Denote $$B_p = \frac{1}{p} \sum_{0 \le k \le p-1} A^k$$ How to prove that the sequence $(B^p)_{p \in \mathbb N}$ converges towards a matrix $B$ of a projector ? I'm able to notice that $B_p(I_n- A)= \frac{1}{p} (I_n-A^p)$ and therefore that the sequence $(B_p(I_n-A))_{p \in \mathbb N}$ converges to $0$... But not much more than that.,Suppose that $A \in \mathcal M_n(\mathbb C)$ is a complex matrix such that the sequence $(A^p)_{p \in \mathbb N}$ is bounded. Denote $$B_p = \frac{1}{p} \sum_{0 \le k \le p-1} A^k$$ How to prove that the sequence $(B^p)_{p \in \mathbb N}$ converges towards a matrix $B$ of a projector ? I'm able to notice that $B_p(I_n- A)= \frac{1}{p} (I_n-A^p)$ and therefore that the sequence $(B_p(I_n-A))_{p \in \mathbb N}$ converges to $0$... But not much more than that.,,"['linear-algebra', 'sequences-and-series', 'matrices', 'convergence-divergence']"
91,Max dimension of a subspace of singular $n\times n$ matrices,Max dimension of a subspace of singular  matrices,n\times n,"I am sure the answer to this is (kind of) well known. I've searched the web and the site for a proof and found nothing, and if this is a duplicate, I'm sorry. The following question was given in a contest I took part. I had an approach but it didn't solve the problem. Consider $V$ a linear subspace of the real vector space $\mathcal{M}_n(\Bbb{R})$ ($n\times n$ real entries matrices) such that $V$ contains only singular matrices (i.e matrices with determinant equal to $0$). What is the maximal dimension of $V$? A quick guess would be $n^2-n$ since if we consider $W$ the set of $n\times n$ real matrices with last line equal to $0$ then this space has dimension $n^2-n$ and it is a linear space of singular matrices. Now the only thing there is to prove is that if $V$ is a subspace of $\mathcal{M}_n(\Bbb{R})$ of dimension $k > n^2-n$ then $V$ contains a non-singular matrix. The official proof was unsatisfactory for me, because it was a combinatorial one, and seemed to have few things in common with linear algebra. I was hoping for a pure linear algebra proof. My approach was to search for a permutation matrix in $V$, but I used some 'false theorem' in between, which I am ashamed to post here.","I am sure the answer to this is (kind of) well known. I've searched the web and the site for a proof and found nothing, and if this is a duplicate, I'm sorry. The following question was given in a contest I took part. I had an approach but it didn't solve the problem. Consider $V$ a linear subspace of the real vector space $\mathcal{M}_n(\Bbb{R})$ ($n\times n$ real entries matrices) such that $V$ contains only singular matrices (i.e matrices with determinant equal to $0$). What is the maximal dimension of $V$? A quick guess would be $n^2-n$ since if we consider $W$ the set of $n\times n$ real matrices with last line equal to $0$ then this space has dimension $n^2-n$ and it is a linear space of singular matrices. Now the only thing there is to prove is that if $V$ is a subspace of $\mathcal{M}_n(\Bbb{R})$ of dimension $k > n^2-n$ then $V$ contains a non-singular matrix. The official proof was unsatisfactory for me, because it was a combinatorial one, and seemed to have few things in common with linear algebra. I was hoping for a pure linear algebra proof. My approach was to search for a permutation matrix in $V$, but I used some 'false theorem' in between, which I am ashamed to post here.",,"['linear-algebra', 'matrices']"
92,Inequality about modified Jordan form,Inequality about modified Jordan form,,"I'm trying to prove an inequality, which in turn I think is equivalent to this inequality, $$(|\lambda_1|-\epsilon)^2\bar{y}^Ty\le\bar{y}^T\left(\overline{(\lambda_rI_r+\epsilon F_r)_r}^T(\lambda_rI_r+\epsilon F_r)_r\right)y\le(|\lambda_d|+\epsilon)^2\bar{y}^Ty$$ for all $y\in\mathbb{C}^d$, for any $0\le\epsilon\le|\lambda_1|$. where $(\lambda_rI_r+\epsilon F_r)_r$ is a slightly modified Jordan form of a complex-valued matrix $M$. $\lambda_rI_r+\epsilon F_r$ is the modified $r$-th Jordan block with $I_r$ being identity matrix and $F_r$ being $1$s right above diagonal. $M$ has eigenvalues $\lambda_1,...,\lambda_d$ with order that $|\lambda_1|\le|\lambda_2|\le...\le|\lambda_d|$. I think the $M$ is not needed here, and more advanced knowledge about Jordan form except the definition neither. I assume the Jordan form of $M$ has $n$ Jordan blocks. Then I wish I could prove that $$(|\lambda_1|-\epsilon)^2\bar{y_r}^Ty_r\le\bar{y_r}^T\overline{(\lambda_rI_r+\epsilon F_r)}^T(\lambda_rI_r+\epsilon F_r)y_r\le(|\lambda_d|+\epsilon)^2\bar{y_r}^Ty_r$$ for $r=1,...,n$, where $y_r$ is the corresponding $r$-th block of $y$. Then the middle part can be expanded to $$\bar{y_r}^T(|\lambda_r|^2I_r+\bar{\lambda_r}\epsilon F_r+\lambda_r\epsilon F_r^T+\epsilon^2F_r^TF_r)y_r.$$ Then the inequality is equivalent to $$|\lambda_1|^2\sum_{i=1}^{s_r}\bar{y_i}y_i-2|\lambda_1|\epsilon\sum_{i=1}^{s_r}\bar{y_i}y_i+\epsilon^2\sum_{i=1}^{s_r}\bar{y_i}y_i\le|\lambda_r|^2\sum_{i=1}^{s_r}\bar{y_i}y_i+\lambda_r\epsilon\sum_{i=2}^{s_r}\bar{y_i}y_{i-1}+\overline{\lambda_r\epsilon\sum_{i=2}^{s_r}\bar{y_i}y_{i-1}}+\epsilon^2\sum_{i=2}^{s_r}\bar{y_i}y_i\le|\lambda_d|^2\sum_{i=1}^{s_r}\bar{y_i}y_i+2|\lambda_d|\epsilon\sum_{i=1}^{s_r}\bar{y_i}y_i+\epsilon^2\sum_{i=1}^{s_r}\bar{y_i}y_i$$ I can see the close similarity between the tree formulas separated by $\le$, but I am stuck in to show the inequality. Especially how can we handle the conjugate terms ? Or the second inequality which I want to prove is true or not ? Important Update : I'm so sorry that I missed to impose an essential condition that $0\le\epsilon\le|\lambda_1|$","I'm trying to prove an inequality, which in turn I think is equivalent to this inequality, $$(|\lambda_1|-\epsilon)^2\bar{y}^Ty\le\bar{y}^T\left(\overline{(\lambda_rI_r+\epsilon F_r)_r}^T(\lambda_rI_r+\epsilon F_r)_r\right)y\le(|\lambda_d|+\epsilon)^2\bar{y}^Ty$$ for all $y\in\mathbb{C}^d$, for any $0\le\epsilon\le|\lambda_1|$. where $(\lambda_rI_r+\epsilon F_r)_r$ is a slightly modified Jordan form of a complex-valued matrix $M$. $\lambda_rI_r+\epsilon F_r$ is the modified $r$-th Jordan block with $I_r$ being identity matrix and $F_r$ being $1$s right above diagonal. $M$ has eigenvalues $\lambda_1,...,\lambda_d$ with order that $|\lambda_1|\le|\lambda_2|\le...\le|\lambda_d|$. I think the $M$ is not needed here, and more advanced knowledge about Jordan form except the definition neither. I assume the Jordan form of $M$ has $n$ Jordan blocks. Then I wish I could prove that $$(|\lambda_1|-\epsilon)^2\bar{y_r}^Ty_r\le\bar{y_r}^T\overline{(\lambda_rI_r+\epsilon F_r)}^T(\lambda_rI_r+\epsilon F_r)y_r\le(|\lambda_d|+\epsilon)^2\bar{y_r}^Ty_r$$ for $r=1,...,n$, where $y_r$ is the corresponding $r$-th block of $y$. Then the middle part can be expanded to $$\bar{y_r}^T(|\lambda_r|^2I_r+\bar{\lambda_r}\epsilon F_r+\lambda_r\epsilon F_r^T+\epsilon^2F_r^TF_r)y_r.$$ Then the inequality is equivalent to $$|\lambda_1|^2\sum_{i=1}^{s_r}\bar{y_i}y_i-2|\lambda_1|\epsilon\sum_{i=1}^{s_r}\bar{y_i}y_i+\epsilon^2\sum_{i=1}^{s_r}\bar{y_i}y_i\le|\lambda_r|^2\sum_{i=1}^{s_r}\bar{y_i}y_i+\lambda_r\epsilon\sum_{i=2}^{s_r}\bar{y_i}y_{i-1}+\overline{\lambda_r\epsilon\sum_{i=2}^{s_r}\bar{y_i}y_{i-1}}+\epsilon^2\sum_{i=2}^{s_r}\bar{y_i}y_i\le|\lambda_d|^2\sum_{i=1}^{s_r}\bar{y_i}y_i+2|\lambda_d|\epsilon\sum_{i=1}^{s_r}\bar{y_i}y_i+\epsilon^2\sum_{i=1}^{s_r}\bar{y_i}y_i$$ I can see the close similarity between the tree formulas separated by $\le$, but I am stuck in to show the inequality. Especially how can we handle the conjugate terms ? Or the second inequality which I want to prove is true or not ? Important Update : I'm so sorry that I missed to impose an essential condition that $0\le\epsilon\le|\lambda_1|$",,"['linear-algebra', 'matrices', 'inequality', 'complex-numbers', 'jordan-normal-form']"
93,$A$ similar to $-A$ implies direct sum decomposition,similar to  implies direct sum decomposition,A -A,"I'm working on linear algebra/Jordan form questions, studying out of Dummit and Foote. Problem:  Let $A \in M_{n}(\mathbb{C})$ and $P \in GL_{n}(\mathbb{C})$ such that $AP = -PA$.  Show that $\mathbb{C}^{n} = V_{1} \oplus V_{2}$ where $V_1$ and $V_2$ are subspaces such that $AV_{1} \subset V_2$ and $AV_{2} \subset V_{1}$. For doing this problem, I have made only a few observatios, but don't know how to proceed.  First, every linear transformation over a $\mathbb{C}$-vector space has a Jordan canonical form, so $\mathbb{C}^{n}$ has a Jordan basis.  If $\{\lambda_1,...,\lambda_k\}$ are the distinct eigenvalues, then $\mathbb{C}^{n} = E_{\lambda_1} \oplus ... \oplus E_{\lambda_k}$ where $E_{\lambda_i}$ is the generalized eigenspace for $\lambda_{i}$. If $AP = -PA$, then $A$ is similar to $-A$, so they have the same Jordan canonical form (except possibly for conjugating by a permutation matrix).  In particular, the eigenvalues of $A$ is the same set as the eigenvalues of $-A$.  So if $w \ne 0$ is a generalized eigenvector with eigenvalue $\lambda$, we have $(A - \lambda I)^{k}w = 0$ for some $k \geq 1$.  Then $(-A + \lambda I)^{k}w = 0$, so $w$ is a generalized eigenvector of $-A$ with eigenvalue of $-\lambda$.  By similarity, $-\lambda$ is an eigenvalue of $A$, so there exists $v \ne 0$ linearly independent of the vectors in $E_{\lambda}$ such that $(A + \lambda I)^{l}v \ne 0$.  This says that each $E_{\lambda}$ has an independent $E_{-\lambda}$ in the eigendecomposition of $\mathbb{C}^{n}$, whenever $\lambda \ne 0$. I am inclined to say that the decomposition into $V_1$ and $V_2$ must be based on this separation into positive and negative eigenvalues, but $A$-invariance of the eigenspaces (i.e. $AE_{\lambda} \subset E_{\lambda}$ for any $\lambda$) makes me think this won't work.  I appreciate any help that can be provided.","I'm working on linear algebra/Jordan form questions, studying out of Dummit and Foote. Problem:  Let $A \in M_{n}(\mathbb{C})$ and $P \in GL_{n}(\mathbb{C})$ such that $AP = -PA$.  Show that $\mathbb{C}^{n} = V_{1} \oplus V_{2}$ where $V_1$ and $V_2$ are subspaces such that $AV_{1} \subset V_2$ and $AV_{2} \subset V_{1}$. For doing this problem, I have made only a few observatios, but don't know how to proceed.  First, every linear transformation over a $\mathbb{C}$-vector space has a Jordan canonical form, so $\mathbb{C}^{n}$ has a Jordan basis.  If $\{\lambda_1,...,\lambda_k\}$ are the distinct eigenvalues, then $\mathbb{C}^{n} = E_{\lambda_1} \oplus ... \oplus E_{\lambda_k}$ where $E_{\lambda_i}$ is the generalized eigenspace for $\lambda_{i}$. If $AP = -PA$, then $A$ is similar to $-A$, so they have the same Jordan canonical form (except possibly for conjugating by a permutation matrix).  In particular, the eigenvalues of $A$ is the same set as the eigenvalues of $-A$.  So if $w \ne 0$ is a generalized eigenvector with eigenvalue $\lambda$, we have $(A - \lambda I)^{k}w = 0$ for some $k \geq 1$.  Then $(-A + \lambda I)^{k}w = 0$, so $w$ is a generalized eigenvector of $-A$ with eigenvalue of $-\lambda$.  By similarity, $-\lambda$ is an eigenvalue of $A$, so there exists $v \ne 0$ linearly independent of the vectors in $E_{\lambda}$ such that $(A + \lambda I)^{l}v \ne 0$.  This says that each $E_{\lambda}$ has an independent $E_{-\lambda}$ in the eigendecomposition of $\mathbb{C}^{n}$, whenever $\lambda \ne 0$. I am inclined to say that the decomposition into $V_1$ and $V_2$ must be based on this separation into positive and negative eigenvalues, but $A$-invariance of the eigenspaces (i.e. $AE_{\lambda} \subset E_{\lambda}$ for any $\lambda$) makes me think this won't work.  I appreciate any help that can be provided.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces']"
94,Pseudoinverse of the sum of matrices,Pseudoinverse of the sum of matrices,,"Similarly to the question posted here Inverse of the sum of matrices but in case of non-square matrices. If I want to compute the pseudoinverse of (A+B) and matrices A,B pinv(A) is known is there a way to compute (or make an approximation) of the new pseudoinverse?? Matrices A and B are full rank.","Similarly to the question posted here Inverse of the sum of matrices but in case of non-square matrices. If I want to compute the pseudoinverse of (A+B) and matrices A,B pinv(A) is known is there a way to compute (or make an approximation) of the new pseudoinverse?? Matrices A and B are full rank.",,"['matrices', 'pseudoinverse']"
95,Prove the direct sum of generalized eigenspaces is the whole vector space,Prove the direct sum of generalized eigenspaces is the whole vector space,,"Given a $n\times n$ matrix $A$ over an algebraically closed field, let $\lambda_1,...,\lambda_k$ be its eigenvalues, and let $V_{\lambda_i}$ be the generalized eigenspace of $\lambda_i$. The question is to prove $⨁_{i=1}^dV_{λ_i }=V $. The generalized eigenspace is defined as the following, $V_{\lambda_i}=\{x:(A-\lambda_i I)^{m(\lambda_i)}x=0\}$ where $m(\lambda_i)$ is the algebraic multiplicity of $\lambda_i$. A proof from the textbook is as the following, Let $d_i$ = $\dim V_{\lambda_i}. $Suppose $⨁_{i=1}^dV_{λ_i }\neq V $, then $\sum_{i=1}^k d_i<n$. Let $S_i$ be a basis of $V_{λ_i }$ containing $d_i$ linear independent vectors, and extend $(S_1,...,S_k)$ to a basis of $V$ by adding $(n-\sum_{i=1}^k d_i<n)$ linear independent vectors $S'$, then $S=(S_1,...,S_k,S')$ is a basis of $V$. Note generalized eigenspaces are invariant subspaces, i.e. $AV_{\lambda_i}\subseteq V_{\lambda_i}$, thus every column of $AS_i$    is still in $V_{\lambda_i}$ and hence a linear combination of columns of $S_i$ for $1\le i\le k$, thus the following holds (NOTE the ""*""s could be non-zero elements), where $AS_i=SA_i$ for $i=1,...,k$ Then I get lost with the proof. The remaining part is the following, I get the idea that the proof wants to find an eigenvector $x$ s.t. $Sx$ is a linear combination of only vectors in $S'$, then show $Sx$ is an eigenvector of some eigenvalue $\lambda_i$, and hence the contradiction because $Sx$ would be a vector in $V_{\lambda_i}$ that cannot be expressed as a linear combination of $S_i$. However, I don't understand why this proof is true, especially for the highlightened parts.","Given a $n\times n$ matrix $A$ over an algebraically closed field, let $\lambda_1,...,\lambda_k$ be its eigenvalues, and let $V_{\lambda_i}$ be the generalized eigenspace of $\lambda_i$. The question is to prove $⨁_{i=1}^dV_{λ_i }=V $. The generalized eigenspace is defined as the following, $V_{\lambda_i}=\{x:(A-\lambda_i I)^{m(\lambda_i)}x=0\}$ where $m(\lambda_i)$ is the algebraic multiplicity of $\lambda_i$. A proof from the textbook is as the following, Let $d_i$ = $\dim V_{\lambda_i}. $Suppose $⨁_{i=1}^dV_{λ_i }\neq V $, then $\sum_{i=1}^k d_i<n$. Let $S_i$ be a basis of $V_{λ_i }$ containing $d_i$ linear independent vectors, and extend $(S_1,...,S_k)$ to a basis of $V$ by adding $(n-\sum_{i=1}^k d_i<n)$ linear independent vectors $S'$, then $S=(S_1,...,S_k,S')$ is a basis of $V$. Note generalized eigenspaces are invariant subspaces, i.e. $AV_{\lambda_i}\subseteq V_{\lambda_i}$, thus every column of $AS_i$    is still in $V_{\lambda_i}$ and hence a linear combination of columns of $S_i$ for $1\le i\le k$, thus the following holds (NOTE the ""*""s could be non-zero elements), where $AS_i=SA_i$ for $i=1,...,k$ Then I get lost with the proof. The remaining part is the following, I get the idea that the proof wants to find an eigenvector $x$ s.t. $Sx$ is a linear combination of only vectors in $S'$, then show $Sx$ is an eigenvector of some eigenvalue $\lambda_i$, and hence the contradiction because $Sx$ would be a vector in $V_{\lambda_i}$ that cannot be expressed as a linear combination of $S_i$. However, I don't understand why this proof is true, especially for the highlightened parts.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
96,Contradicting Answer when calculating determinant in two different ways,Contradicting Answer when calculating determinant in two different ways,,"So I'm supposed to decide which $x \in \mathbb{R}$ gives a non-zero determinant: \begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix} It works well when when I re-write the matrix using Gaussian Elimination, only adding/subtracting rows to other rows. However, out of interest, I tried adding two rows to each other ""simultaneously"", because I suspected that it would bring me to the solution quicker. Basically, I tried subtracting row 3 with row 2, and row 2 with row 3 and replace them ""at the same time"". It's kind of difficult to explain, so I'll show you instead: $$\begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&-1&1&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&0&0&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=0$$ Apparently this suggests that all values on $x$ produces a zero-valued determinant, which isn't true. I'm suspecting that there is something wrong in cross-adding/subtracting rows to other rows, but I can't quite figure out why. My wild guess is that there's some equivalence to multiplying a row by zero. In order to pin-point this, I tried to compare it to a set of equations: $$\begin{cases} x + y = 3 \\ 2x + y = 5\end{cases}$$ From this you obtain the matrix: $$\begin{pmatrix}1&1&3\\2&1&5\end{pmatrix}\sim \begin{pmatrix}1&0&2\\0&1&1\end{pmatrix}$$ Which gives the solution $\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}2\\1\end{pmatrix}$ However, if I multiply row 1 with 2 and subtract it from row 2, and at the same time multiply row 2 with 1/2 and subtract that to row 1, I obtain: $$\begin{pmatrix}0&1/2&1/2\\0&-1&-1\end{pmatrix}\sim\begin{pmatrix}0&1&1\\0&0&0\end{pmatrix}$$ So basically, I've lost information about $x$, which makes sense. However, what bothers me is that I get confused when this is applied generally, such as in the case with the determinant. My questions are therefore: In what way does this type of ""cross-addition/cross-subtraction"" of matrices lose information? Maybe a vague question, but is there some way of seeing where/how the information is lost? Can it be compared to multiplying by 0 somehow? Is there any general rule for how you cannot perform operations on rows in matrices to avoid losing information? I know that you can't multiply a whole row by 0, but are there other things to look out for? Basically, I'm just curious about this, since it hasn't been mention anywhere in my book or on any lecture. Maybe the answer is obvious and I'm just blind, but even if that's the case, I'd appreciate some guidelines! Thanks a head! EDIT: From experimenting a bit, I've started to realize that doing this cross-addition is similar to completely removing a row, which is similar, and even equivalent, to multiplying by 0. (That's why you always end up with at least one 0-row, or if you continue, a 0-matrix.) Maybe that was the connection I was looking for? It seems to make sense, but I may have missed something.","So I'm supposed to decide which $x \in \mathbb{R}$ gives a non-zero determinant: \begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix} It works well when when I re-write the matrix using Gaussian Elimination, only adding/subtracting rows to other rows. However, out of interest, I tried adding two rows to each other ""simultaneously"", because I suspected that it would bring me to the solution quicker. Basically, I tried subtracting row 3 with row 2, and row 2 with row 3 and replace them ""at the same time"". It's kind of difficult to explain, so I'll show you instead: $$\begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&-1&1&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&0&0&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=0$$ Apparently this suggests that all values on $x$ produces a zero-valued determinant, which isn't true. I'm suspecting that there is something wrong in cross-adding/subtracting rows to other rows, but I can't quite figure out why. My wild guess is that there's some equivalence to multiplying a row by zero. In order to pin-point this, I tried to compare it to a set of equations: $$\begin{cases} x + y = 3 \\ 2x + y = 5\end{cases}$$ From this you obtain the matrix: $$\begin{pmatrix}1&1&3\\2&1&5\end{pmatrix}\sim \begin{pmatrix}1&0&2\\0&1&1\end{pmatrix}$$ Which gives the solution $\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}2\\1\end{pmatrix}$ However, if I multiply row 1 with 2 and subtract it from row 2, and at the same time multiply row 2 with 1/2 and subtract that to row 1, I obtain: $$\begin{pmatrix}0&1/2&1/2\\0&-1&-1\end{pmatrix}\sim\begin{pmatrix}0&1&1\\0&0&0\end{pmatrix}$$ So basically, I've lost information about $x$, which makes sense. However, what bothers me is that I get confused when this is applied generally, such as in the case with the determinant. My questions are therefore: In what way does this type of ""cross-addition/cross-subtraction"" of matrices lose information? Maybe a vague question, but is there some way of seeing where/how the information is lost? Can it be compared to multiplying by 0 somehow? Is there any general rule for how you cannot perform operations on rows in matrices to avoid losing information? I know that you can't multiply a whole row by 0, but are there other things to look out for? Basically, I'm just curious about this, since it hasn't been mention anywhere in my book or on any lecture. Maybe the answer is obvious and I'm just blind, but even if that's the case, I'd appreciate some guidelines! Thanks a head! EDIT: From experimenting a bit, I've started to realize that doing this cross-addition is similar to completely removing a row, which is similar, and even equivalent, to multiplying by 0. (That's why you always end up with at least one 0-row, or if you continue, a 0-matrix.) Maybe that was the connection I was looking for? It seems to make sense, but I may have missed something.",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
97,Left and right multiplication of a matrix A with an upper triangular matrix U,Left and right multiplication of a matrix A with an upper triangular matrix U,,Let A and $U$ are two square matrices where A is invertible. If $$AU=UA$$ In partitioned form  $$\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}\begin{bmatrix}U_{11}&0_{12}\\0_{21}&U_{22}\end{bmatrix}=\begin{bmatrix}U_{11}&0_{12}\\0_{21}&U_{22}\end{bmatrix}\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}$$ $\implies$ $$A_{21}U_{11}=U_{22}A_{21}$$ where $U_{11}$ and $U_{22}$ are upper triangular square matrices (dimension can be different) and $U_{11}$ and $U_{22}$ cannot have same diagonal elements Then how to prove $A_{21}$ is a zero matrix and similarly $A_{12}$,Let A and $U$ are two square matrices where A is invertible. If $$AU=UA$$ In partitioned form  $$\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}\begin{bmatrix}U_{11}&0_{12}\\0_{21}&U_{22}\end{bmatrix}=\begin{bmatrix}U_{11}&0_{12}\\0_{21}&U_{22}\end{bmatrix}\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}$$ $\implies$ $$A_{21}U_{11}=U_{22}A_{21}$$ where $U_{11}$ and $U_{22}$ are upper triangular square matrices (dimension can be different) and $U_{11}$ and $U_{22}$ cannot have same diagonal elements Then how to prove $A_{21}$ is a zero matrix and similarly $A_{12}$,,['matrices']
98,Relationship between eigenvalues and matrix elements (Note: eigenspectrum gap and rescaling),Relationship between eigenvalues and matrix elements (Note: eigenspectrum gap and rescaling),,"Let $\textbf D$ be a square matrix with eigenvalues separated into two or more groups using their magnitude (say the first group of eigenvalues is $\{1,2,\dots,10\}$ and the second group has eigenvalues $\{1010,1021,1051,\dots,10000\}$). Then consider the eigenvalue relation of transition matrix $\textbf{D}$, $$\textbf{D}\textbf{R}=\textbf{R}\Lambda$$ Partitioning $\Lambda$ gives $$\textbf{D}\textbf{R}=\textbf{R} \begin{bmatrix}\Lambda_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\Lambda_{mm}\end{bmatrix}$$ wheere $\Lambda_{dd}$ has the smallest group and $\Lambda_{mm}$ has the largest group of eigenvalues. Rescale the system using $k$ (a value with magnitude greater than the small group of eigenvalues and less than large group of eigenvalues - say 100 or 1000 in the previous example of eigengroups) as $$\frac{1}{|k|}\textbf{D}\textbf{R}=\frac{1}{|k|}\textbf{R} \begin{bmatrix}\Lambda_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\Lambda_{mm}\end{bmatrix}$$ $$\frac{1}{|k|}\textbf{D}\textbf{R}=\textbf{R} \begin{bmatrix}\frac{\Lambda_{dd}}{|k|}&\textbf{0}_{dm}\\\textbf{0}_{md}&\frac{\Lambda_{mm}}{|k|}\end{bmatrix}$$ Assume that $k$ infinitely large, then $\lim\limits_{|k|\rightarrow \infty}\frac{\Lambda_{dd}}{|k|}=\textbf{0}_{dd}$, $$ \tilde{\textbf{D}}\textbf{R}=\textbf{R}\begin{bmatrix}\textbf{0}_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\frac{\Lambda_{mm}}{|k|}\end{bmatrix}$$ However, to rescale, I need to know what is the separation between eigenvalues to find a suitable k. This is hard in large dimensional matrices due to the need of computing a large number of eigenvalues. Then instead of checking the gap in eigenvalues, How can confirm that rescaling using the gap in elements of D will rescale the eigenvalues to zero. A worked out example is given below In the first figure, elements of D is sorted by their magnitude and second figure shows the eigenvalues. If I rescale D using $10^4$ (A  value in the second gap in the element spectrum. first gap is not useful in the case), I will get the first group of eigenvalues rescaled to zero. Then by $10^6$, I may (sometimes there will be the same number of zero eigenvalues by stepping up) rescale the second group to zero and so on. How can I confirm this is true in general or for some specific cases under some conditions?. A small observation about infinity. What is infinity? When we say a value $x$ is infinite, what are we meant by that?. To my knowledge: if $x$ is defined as $\infty$, then $1/x=0$. But we know for any large number, no $x$ will satisfy this condition. There will be a decimal value at some point in $1/x$. The question is, are we considering this decimal point or not. If we are truncating it, then we can say $x=\infty$ which is an approximation. If someone says, 100 is infinite for him, then $1/100=0.01$ is a zero for him. By using this fact I trucated the matrix D by removing the decimal point which satisfies the limit condition on $k$.","Let $\textbf D$ be a square matrix with eigenvalues separated into two or more groups using their magnitude (say the first group of eigenvalues is $\{1,2,\dots,10\}$ and the second group has eigenvalues $\{1010,1021,1051,\dots,10000\}$). Then consider the eigenvalue relation of transition matrix $\textbf{D}$, $$\textbf{D}\textbf{R}=\textbf{R}\Lambda$$ Partitioning $\Lambda$ gives $$\textbf{D}\textbf{R}=\textbf{R} \begin{bmatrix}\Lambda_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\Lambda_{mm}\end{bmatrix}$$ wheere $\Lambda_{dd}$ has the smallest group and $\Lambda_{mm}$ has the largest group of eigenvalues. Rescale the system using $k$ (a value with magnitude greater than the small group of eigenvalues and less than large group of eigenvalues - say 100 or 1000 in the previous example of eigengroups) as $$\frac{1}{|k|}\textbf{D}\textbf{R}=\frac{1}{|k|}\textbf{R} \begin{bmatrix}\Lambda_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\Lambda_{mm}\end{bmatrix}$$ $$\frac{1}{|k|}\textbf{D}\textbf{R}=\textbf{R} \begin{bmatrix}\frac{\Lambda_{dd}}{|k|}&\textbf{0}_{dm}\\\textbf{0}_{md}&\frac{\Lambda_{mm}}{|k|}\end{bmatrix}$$ Assume that $k$ infinitely large, then $\lim\limits_{|k|\rightarrow \infty}\frac{\Lambda_{dd}}{|k|}=\textbf{0}_{dd}$, $$ \tilde{\textbf{D}}\textbf{R}=\textbf{R}\begin{bmatrix}\textbf{0}_{dd}&\textbf{0}_{dm}\\\textbf{0}_{md}&\frac{\Lambda_{mm}}{|k|}\end{bmatrix}$$ However, to rescale, I need to know what is the separation between eigenvalues to find a suitable k. This is hard in large dimensional matrices due to the need of computing a large number of eigenvalues. Then instead of checking the gap in eigenvalues, How can confirm that rescaling using the gap in elements of D will rescale the eigenvalues to zero. A worked out example is given below In the first figure, elements of D is sorted by their magnitude and second figure shows the eigenvalues. If I rescale D using $10^4$ (A  value in the second gap in the element spectrum. first gap is not useful in the case), I will get the first group of eigenvalues rescaled to zero. Then by $10^6$, I may (sometimes there will be the same number of zero eigenvalues by stepping up) rescale the second group to zero and so on. How can I confirm this is true in general or for some specific cases under some conditions?. A small observation about infinity. What is infinity? When we say a value $x$ is infinite, what are we meant by that?. To my knowledge: if $x$ is defined as $\infty$, then $1/x=0$. But we know for any large number, no $x$ will satisfy this condition. There will be a decimal value at some point in $1/x$. The question is, are we considering this decimal point or not. If we are truncating it, then we can say $x=\infty$ which is an approximation. If someone says, 100 is infinite for him, then $1/100=0.01$ is a zero for him. By using this fact I trucated the matrix D by removing the decimal point which satisfies the limit condition on $k$.",,"['matrices', 'eigenvalues-eigenvectors', 'sparse-matrices']"
99,A generalization of holomorphic functions,A generalization of holomorphic functions,,Let's fix a  matrix $A\in M_{2}(\mathbb{R})$.  Assume that the  following vector space of  smooth  functions is  closed under complex  multiplication: $$\mathcal{S}_{A}=\{f:\mathbb{C}\to \mathbb{C}\mid Df.A=A.Df  \}$$ Here  $Df$  is  the  Jacobian of  $f:\mathbb{R}^{2}\to \mathbb{R}^{2}$ (We  identify  $\mathbb{C}$  with $\mathbb{R}^{2}$). Does  this  imply that  $A$ is  in the  form $A=\begin{pmatrix} a&-b\\b&a \end{pmatrix}$? Note that For $A=\begin{pmatrix} 0&-1\\1&0 \end{pmatrix}$  the  relation $Df.A=A.Df$  is equivalent to the Cauchy Riemann equations for $f=u+iv$ so we obtain the class of  holomorphic  functions.,Let's fix a  matrix $A\in M_{2}(\mathbb{R})$.  Assume that the  following vector space of  smooth  functions is  closed under complex  multiplication: $$\mathcal{S}_{A}=\{f:\mathbb{C}\to \mathbb{C}\mid Df.A=A.Df  \}$$ Here  $Df$  is  the  Jacobian of  $f:\mathbb{R}^{2}\to \mathbb{R}^{2}$ (We  identify  $\mathbb{C}$  with $\mathbb{R}^{2}$). Does  this  imply that  $A$ is  in the  form $A=\begin{pmatrix} a&-b\\b&a \end{pmatrix}$? Note that For $A=\begin{pmatrix} 0&-1\\1&0 \end{pmatrix}$  the  relation $Df.A=A.Df$  is equivalent to the Cauchy Riemann equations for $f=u+iv$ so we obtain the class of  holomorphic  functions.,,"['matrices', 'complex-analysis', 'partial-differential-equations', 'complex-numbers', 'matrix-calculus']"
