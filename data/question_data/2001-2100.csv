,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What do you get with this equivalence relationship for all $\mathbb{Q}$ sequences,What do you get with this equivalence relationship for all  sequences,\mathbb{Q},Consider all $\mathbb{Q}$ Cauchy sequences with this equivalence relationship $\{x_n\} \sim \{y_n\} \iff \{x_n-y_n\} \rightarrow 0$ Then you get all real numbers as an equivalence class with this relationship. Let's consider now all $\mathbb{Q}$ sequences with this equivalence relationship. We get a set which contains all real numbers and many more. What can we say about those new elements? Does that set represent some known set? Can we get some interesting topological or analytic results about it?,Consider all $\mathbb{Q}$ Cauchy sequences with this equivalence relationship $\{x_n\} \sim \{y_n\} \iff \{x_n-y_n\} \rightarrow 0$ Then you get all real numbers as an equivalence class with this relationship. Let's consider now all $\mathbb{Q}$ sequences with this equivalence relationship. We get a set which contains all real numbers and many more. What can we say about those new elements? Does that set represent some known set? Can we get some interesting topological or analytic results about it?,,"['real-analysis', 'general-topology', 'equivalence-relations', 'real-numbers']"
1,"If $\int_a^b f(x) \ \mathrm{d}x = \int_a^b g(x) \ \mathrm{d}x$ then $\exists x \in [a,b]$ with $f(x) = g(x).$",If  then  with,"\int_a^b f(x) \ \mathrm{d}x = \int_a^b g(x) \ \mathrm{d}x \exists x \in [a,b] f(x) = g(x).","I am trying to prove the following: Take $f, g:[a,b] \to \mathbb{R}$ such that $f$ and $g$ are continuous. If $$\int_a^b f(x) \ \mathrm{d}x = \int_a^b g(x) \ \mathrm{d}x,$$ then there exists some $c \in [a,b]$ such that $f(c) = g(c).$ Here's my current proof. I'd welcome any feedback regarding correctness and clarity. Current Proof Assume that there exists no such $c$. There are then three possibilities. First, it is possible that $f(x) > g(x)$ $\forall$ $x \in [a,b]$. However, this cannot be, since then we would have $$\int_a^b f(x) \ \mathrm{d}x > \int_a^b g(x) \ \mathrm{d}.x$$ Similarly, we cannot have $g(x) > f(x)$ $\forall$ $x \in [a,b]$, since then $$\int_a^b f(x) \ \mathrm{d}x < \int_a^b g(x) \ \mathrm{d}x.$$ Thus, there exists some $x \in [a,b]$ such that $f(x) > g(x)$ and some $y \neq x$ such that $g(y) > f(y).$ Assume without loss of generality that $x < y.$ Consider a new function $h:[a,b] \to \mathbb{R}$ defined by $$h(x) = f(x) - g(x).$$ Clearly, $h$ is continuous, as the difference of two continuous functions. From the above, we have that $h(x) > 0$ and $h(y) < 0.$ Apply the Intermediate Value Theorem to $h$ on the interval $(x,y).$ Thus, there exists some $c \in (x, y)$ such that $f(c) = 0$. Since $(x, y) \subset [a, b]$, we have found an element of $[a,b]$ such that $h(c) = 0 \implies f(x) = g(x).$","I am trying to prove the following: Take $f, g:[a,b] \to \mathbb{R}$ such that $f$ and $g$ are continuous. If $$\int_a^b f(x) \ \mathrm{d}x = \int_a^b g(x) \ \mathrm{d}x,$$ then there exists some $c \in [a,b]$ such that $f(c) = g(c).$ Here's my current proof. I'd welcome any feedback regarding correctness and clarity. Current Proof Assume that there exists no such $c$. There are then three possibilities. First, it is possible that $f(x) > g(x)$ $\forall$ $x \in [a,b]$. However, this cannot be, since then we would have $$\int_a^b f(x) \ \mathrm{d}x > \int_a^b g(x) \ \mathrm{d}.x$$ Similarly, we cannot have $g(x) > f(x)$ $\forall$ $x \in [a,b]$, since then $$\int_a^b f(x) \ \mathrm{d}x < \int_a^b g(x) \ \mathrm{d}x.$$ Thus, there exists some $x \in [a,b]$ such that $f(x) > g(x)$ and some $y \neq x$ such that $g(y) > f(y).$ Assume without loss of generality that $x < y.$ Consider a new function $h:[a,b] \to \mathbb{R}$ defined by $$h(x) = f(x) - g(x).$$ Clearly, $h$ is continuous, as the difference of two continuous functions. From the above, we have that $h(x) > 0$ and $h(y) < 0.$ Apply the Intermediate Value Theorem to $h$ on the interval $(x,y).$ Thus, there exists some $c \in (x, y)$ such that $f(c) = 0$. Since $(x, y) \subset [a, b]$, we have found an element of $[a,b]$ such that $h(c) = 0 \implies f(x) = g(x).$",,"['real-analysis', 'analysis', 'integration']"
2,Differential Equations with Deviating Argument,Differential Equations with Deviating Argument,,"Is there literature available on solving differential equations of the type $$f(x,y(x),y(\kappa x),y'(x))=0,$$ where $\kappa$ is a given constant? I know about the book Introduction to the Theory and Application of Differential Equations with Deviating Arguments by L.E. El'sgol'ts and S.B. Norkin from the year 1973 [1], but I wonder if there are more recently published books available as well. Specifically, I would be interested in solving for example $$u(2t)-2u'(t)u(t)=0$$  without guesswork. [1] Introduction to the Theory and Application of Differential Equations with Deviating Arguments, L.E. El'sgol'ts and S.B. Norkin, Mathematics in Science and Engineering, Volume 105, Academic Press, New York, 1973","Is there literature available on solving differential equations of the type $$f(x,y(x),y(\kappa x),y'(x))=0,$$ where $\kappa$ is a given constant? I know about the book Introduction to the Theory and Application of Differential Equations with Deviating Arguments by L.E. El'sgol'ts and S.B. Norkin from the year 1973 [1], but I wonder if there are more recently published books available as well. Specifically, I would be interested in solving for example $$u(2t)-2u'(t)u(t)=0$$  without guesswork. [1] Introduction to the Theory and Application of Differential Equations with Deviating Arguments, L.E. El'sgol'ts and S.B. Norkin, Mathematics in Science and Engineering, Volume 105, Academic Press, New York, 1973",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
3,"Open map which ""almost fixes"" the boundary of an open ball","Open map which ""almost fixes"" the boundary of an open ball",,"We have a continuous function $f:\bar{B}\to\mathbb{R}^n$, where $\bar{B}=\{x\in\mathbb{R}^n:\|x\|\le 1\}$, such that if $\|x\|=1$ then $\|f(x)-x\|<\epsilon$, for a fixed $\epsilon\in(0,1)$. We have to prove that $B(0,1-\epsilon)\subseteq f(\bar{B})$. This appears as a lemma in Rudin's Real and Complex Analysis . The author claims that it is possible to prove it without Brouwer's fixed point theorem , under the additional hypothesis that $f$ is open . So far I've only observed that the problem reduces to showing that $f(\bar{B})\cap B(0,1-\epsilon)$ is not empty. Any ideas?","We have a continuous function $f:\bar{B}\to\mathbb{R}^n$, where $\bar{B}=\{x\in\mathbb{R}^n:\|x\|\le 1\}$, such that if $\|x\|=1$ then $\|f(x)-x\|<\epsilon$, for a fixed $\epsilon\in(0,1)$. We have to prove that $B(0,1-\epsilon)\subseteq f(\bar{B})$. This appears as a lemma in Rudin's Real and Complex Analysis . The author claims that it is possible to prove it without Brouwer's fixed point theorem , under the additional hypothesis that $f$ is open . So far I've only observed that the problem reduces to showing that $f(\bar{B})\cap B(0,1-\epsilon)$ is not empty. Any ideas?",,"['real-analysis', 'general-topology']"
4,"Does there exist any $p >0$ such that $\frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty$?",Does there exist any  such that ?,"p >0 \frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty","Does there exist any $p >0$ such that \begin{equation*}     \frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty \;? \end{equation*} If there is one, what's the infimum of those $p$ ? Is it also a minimum? I started wondering about it since the set of limit points of $\big(\sin(n)\big)_{n \in \mathbb{N}}$ is the whole interval $[-1,1]$ , so it seems interesting to quantify how fast this sequence clusters around $0$ .","Does there exist any such that If there is one, what's the infimum of those ? Is it also a minimum? I started wondering about it since the set of limit points of is the whole interval , so it seems interesting to quantify how fast this sequence clusters around .","p >0 \begin{equation*}
    \frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty \;?
\end{equation*} p \big(\sin(n)\big)_{n \in \mathbb{N}} [-1,1] 0","['real-analysis', 'sequences-and-series', 'limits', 'elementary-number-theory']"
5,Is there any condition that makes a measure zero set necessarily countable?,Is there any condition that makes a measure zero set necessarily countable?,,"Background : Let us consider the Lebesgue measure space $(\Bbb{R}, \mathcal{L}(\Bbb{R}),m) $ . Here measurable set means Lebesgue measurable and measure means Lebesgue measure. $\mathcal{S}\subset \mathcal{P}(\Bbb{R}) $ is called a class of small sets ( or a $\sigma$ -ideal) if $\emptyset \in \mathcal{S}$ $A\in\mathcal{S}$ and $B\subset A$ implies $B\in \mathcal{S}$ $\bigcup_{n\in \Bbb{N}} A_n\in\mathcal{S}$ whenever $A_n\in\mathcal{S}$ for all $n\in \Bbb{N}$ The class of countable sets, null sets, meager sets are all small in one sense and other. In this thread, we want to study the relationship between sets which are small in sense of cardinality and measure. Small in sense of cardinality implies small in sense of measure: $A\subset \Bbb{R}$ is countable implies $m(A)=0$ . Small in sense measure may not implies small in sense of cardinality: Example: The Cantor set . Question: Is there any condition that makes a measure zero set necessarily countable? UPDATE : Strong measure zero set : $A\subset \Bbb{R}$ is strong measure $0$ set if for every sequence $(\delta_n)\subset \Bbb{R}^+$ there exists a sequence $(I_n)$ of intervals such that $\ell(I_n) < \delta_n$ for all $n$ and $A$ is contained in the union of the $I_n$ . From the definition, it is clear that a strong measure zero set is a null set. The Cantor set is a measure $0$ set but fails to be countable.But the Cantor set is not strong measure zero set(see here ). In a celebrated paper, Borel conjectured that every Strong measure zero set of reals was countable. This statement known as Borel Conjecture . It is now known that this statement is independent of ZFC.","Background : Let us consider the Lebesgue measure space . Here measurable set means Lebesgue measurable and measure means Lebesgue measure. is called a class of small sets ( or a -ideal) if and implies whenever for all The class of countable sets, null sets, meager sets are all small in one sense and other. In this thread, we want to study the relationship between sets which are small in sense of cardinality and measure. Small in sense of cardinality implies small in sense of measure: is countable implies . Small in sense measure may not implies small in sense of cardinality: Example: The Cantor set . Question: Is there any condition that makes a measure zero set necessarily countable? UPDATE : Strong measure zero set : is strong measure set if for every sequence there exists a sequence of intervals such that for all and is contained in the union of the . From the definition, it is clear that a strong measure zero set is a null set. The Cantor set is a measure set but fails to be countable.But the Cantor set is not strong measure zero set(see here ). In a celebrated paper, Borel conjectured that every Strong measure zero set of reals was countable. This statement known as Borel Conjecture . It is now known that this statement is independent of ZFC.","(\Bbb{R}, \mathcal{L}(\Bbb{R}),m)  \mathcal{S}\subset \mathcal{P}(\Bbb{R})  \sigma \emptyset \in \mathcal{S} A\in\mathcal{S} B\subset A B\in \mathcal{S} \bigcup_{n\in \Bbb{N}} A_n\in\mathcal{S} A_n\in\mathcal{S} n\in \Bbb{N} A\subset \Bbb{R} m(A)=0 A\subset \Bbb{R} 0 (\delta_n)\subset \Bbb{R}^+ (I_n) \ell(I_n) < \delta_n n A I_n 0","['real-analysis', 'measure-theory', 'metric-spaces', 'lebesgue-measure', 'descriptive-set-theory']"
6,About the inequality : $x^{x^{x^{x^{x^x}}}}\geq x^{x^{x^{((e-2)(1+e))x\left(1+\sqrt{x}\left((\sqrt{x})^3-1\right)\right)}}}\geq x^{x^{\frac{16}{27}}}$,About the inequality :,x^{x^{x^{x^{x^x}}}}\geq x^{x^{x^{((e-2)(1+e))x\left(1+\sqrt{x}\left((\sqrt{x})^3-1\right)\right)}}}\geq x^{x^{\frac{16}{27}}},"This inequality is due to user RiverLi : Let $0<x\leq 1$ then we have : $$x^{x^{x^{x^{x^x}}}}\geq x^{x^{\frac{16}{27}}} \geq 0.5x^2+0.5$$ I propose  another one wich states : Let $0<x\leq 1$ then prove or disprove we have : $$x^{x^{x^{x^{x^x}}}}\geq x^{x^{x^{\left(\left(e-2\right)\left(1+e\right)\right)\left(x+x^{3}-x^{\frac{3}{2}}\right)}}}\geq x^{x^{\frac{16}{27}}} \geq 0.5x^2+0.5$$ Some background : Some days ago user RiverLi proposed an inequality (see reference) with a really nice and clever proof . So the inequality : $$x^{x^{x^{x^{x^x}}}} \geq 0.5x^2+0.5$$ is already proved . So there many tricks we can use to show the refinement I propose .It seems that  each inequality is a steps with two exponents . My attempt : The first LHS is equivalent to : $$x^{x^{x}}\leq \left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right)$$ So we can use the lemma (see the reference [1] p136): Let $0<a\leq 1$ and $c>0$ then we have : $$a^c\leq(1−c)^2+ac(2−c)−ac(1−c)\ln(a)$$ But it doesn't works... So if we substitute $y^2=x$ the function : $$h(y)=\left(\left(e-2\right)\left(e+1\right)y^{2}\left(1+y\left(y^{3}-1\right)\right)\right)$$ Is convex for $y\in[0,1]$ .So we can use the same argument as below.A good value is $y=0.51$ it gives us for $x\in(0.14,0.9)$ : $$\left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right)\ge 1.193075\left(x^{0.5}-0.51\right)+0.387383\ge x^{x^x}$$ So next we can take the log on both side and use the power series of $x^x$ around $x=1$ wich is well-know .Furthermore I build an inequality wich states : let $x\in[\frac{1}{5},\frac{3}{5}]$ it seems we have : $$x^x\leq \left(x\ln\left(x\right)+1+\left(\left(x\ln\left(x\right)\right)^{2}+\left(x\ln\left(x\right)\right)^{3}\right)\right)^{\frac{1}{3e-e\ln\left(-1+e-e^{2}+e^{3}\right)}}$$ Unfortunately we cannot use the inequality above .Anyway since $f(x)=x^x$ is convex on $(0,1)$ we can use the same way to get someting like $x,x_0\in(0.14,0.9)$ : $$x^{x^x}\leq\exp(\ln(x)(f'(x_0)(x-x_0)+f(x_0))\leq^{?} 1.193075\left(x^{0.5}-0.51\right)+0.387383$$ To works we need to have $x$ near $x_0$ wich is a little bit embarrassing . Edit : We introduce the function : $$t(x)=x^{2x^{x}}$$ It seems that $t(x)$ is convex on $(0,1)$ so we can get inequality like $x\in(0.15,0.19)$ : $$x^{2x^x}\leq \left(\frac{\left(t\left(0.15\right)-t\left(0.19\right)\right)}{0.15-0.19}\left(x-0.15\right)+t\left(0.15\right)\right)\leq \left(1.193075\left(x^{\left(0.5\right)}-0.51\right)+0.387383\right)^{2}$$ Remains to show the convexity of this function $t(x)$ .To show it we stop at the first derivative wich seems to be the product of two increasing function we have : $$a(x)=x^{2x^{\left(x\right)}+x-1.376}$$ It seems that $a(x)$ is increasing on $(0,1)$ And : $$b(x)=x^{1.376}(\ln(x))^{2}+\ln(x)x^{1.376}+x^{0.376}$$ It seems that $b(x)$ is increasing on $(0,1)$ . And we have : $$t'(x)=a(x)* b(x)$$ . Edit 2 : Some ideas to show if $a(x)$ and $b(x)$ are increasing : For $a(x)$ : We use the formula over the reals : $$(a+b)(a-b)=a^2-b^2$$ Then take the logarithm . For $b(x)$ differentiate and we can substitute as $x=y^{\frac{1}{1.376}}$ before. Correct me if I'm wrong For the second it's equivalent to : $$g(x)=\left(x^{\left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right)}\right)>\frac{16}{27}$$ It seems that the function $g(x)$ is convex for $x\in(0,1]$ .So we can use the fact , convex functions lie above their supporting lines . A related result :Let $x\in(0,1)$ then it seems we have : $$x(x^{2}-2x+2)\geq x^{x^{x}}$$ To show it we can use the convexity on $(0,1)$ of : $$v(x)=\frac{\ln\left(x(x^{2}-2x+2)\right)}{\ln\left(x\right)}$$ And : $$j(x)=x^x$$ We can improve the inequality we have on $(0,1)$ : $$\left(x^{1.15}\left(x^{2}-2x+2\right)\right)^{0.85}\geq x^{x^x}$$ Question : How to (dis)prove it ? Reference : About the inequality $x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12$ [1] : Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938 https://www.planetmath.org/convexfunctionslieabovetheirsupportinglines","This inequality is due to user RiverLi : Let then we have : I propose  another one wich states : Let then prove or disprove we have : Some background : Some days ago user RiverLi proposed an inequality (see reference) with a really nice and clever proof . So the inequality : is already proved . So there many tricks we can use to show the refinement I propose .It seems that  each inequality is a steps with two exponents . My attempt : The first LHS is equivalent to : So we can use the lemma (see the reference [1] p136): Let and then we have : But it doesn't works... So if we substitute the function : Is convex for .So we can use the same argument as below.A good value is it gives us for : So next we can take the log on both side and use the power series of around wich is well-know .Furthermore I build an inequality wich states : let it seems we have : Unfortunately we cannot use the inequality above .Anyway since is convex on we can use the same way to get someting like : To works we need to have near wich is a little bit embarrassing . Edit : We introduce the function : It seems that is convex on so we can get inequality like : Remains to show the convexity of this function .To show it we stop at the first derivative wich seems to be the product of two increasing function we have : It seems that is increasing on And : It seems that is increasing on . And we have : . Edit 2 : Some ideas to show if and are increasing : For : We use the formula over the reals : Then take the logarithm . For differentiate and we can substitute as before. Correct me if I'm wrong For the second it's equivalent to : It seems that the function is convex for .So we can use the fact , convex functions lie above their supporting lines . A related result :Let then it seems we have : To show it we can use the convexity on of : And : We can improve the inequality we have on : Question : How to (dis)prove it ? Reference : About the inequality $x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12$ [1] : Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"", The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938 https://www.planetmath.org/convexfunctionslieabovetheirsupportinglines","0<x\leq 1 x^{x^{x^{x^{x^x}}}}\geq x^{x^{\frac{16}{27}}} \geq 0.5x^2+0.5 0<x\leq 1 x^{x^{x^{x^{x^x}}}}\geq x^{x^{x^{\left(\left(e-2\right)\left(1+e\right)\right)\left(x+x^{3}-x^{\frac{3}{2}}\right)}}}\geq x^{x^{\frac{16}{27}}} \geq 0.5x^2+0.5 x^{x^{x^{x^{x^x}}}} \geq 0.5x^2+0.5 x^{x^{x}}\leq \left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right) 0<a\leq 1 c>0 a^c\leq(1−c)^2+ac(2−c)−ac(1−c)\ln(a) y^2=x h(y)=\left(\left(e-2\right)\left(e+1\right)y^{2}\left(1+y\left(y^{3}-1\right)\right)\right) y\in[0,1] y=0.51 x\in(0.14,0.9) \left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right)\ge 1.193075\left(x^{0.5}-0.51\right)+0.387383\ge x^{x^x} x^x x=1 x\in[\frac{1}{5},\frac{3}{5}] x^x\leq \left(x\ln\left(x\right)+1+\left(\left(x\ln\left(x\right)\right)^{2}+\left(x\ln\left(x\right)\right)^{3}\right)\right)^{\frac{1}{3e-e\ln\left(-1+e-e^{2}+e^{3}\right)}} f(x)=x^x (0,1) x,x_0\in(0.14,0.9) x^{x^x}\leq\exp(\ln(x)(f'(x_0)(x-x_0)+f(x_0))\leq^{?} 1.193075\left(x^{0.5}-0.51\right)+0.387383 x x_0 t(x)=x^{2x^{x}} t(x) (0,1) x\in(0.15,0.19) x^{2x^x}\leq \left(\frac{\left(t\left(0.15\right)-t\left(0.19\right)\right)}{0.15-0.19}\left(x-0.15\right)+t\left(0.15\right)\right)\leq \left(1.193075\left(x^{\left(0.5\right)}-0.51\right)+0.387383\right)^{2} t(x) a(x)=x^{2x^{\left(x\right)}+x-1.376} a(x) (0,1) b(x)=x^{1.376}(\ln(x))^{2}+\ln(x)x^{1.376}+x^{0.376} b(x) (0,1) t'(x)=a(x)* b(x) a(x) b(x) a(x) (a+b)(a-b)=a^2-b^2 b(x) x=y^{\frac{1}{1.376}} g(x)=\left(x^{\left(e-2\right)\left(e+1\right)x\left(1+x^{0.5}\left(x^{1.5}-1\right)\right)}\right)>\frac{16}{27} g(x) x\in(0,1] x\in(0,1) x(x^{2}-2x+2)\geq x^{x^{x}} (0,1) v(x)=\frac{\ln\left(x(x^{2}-2x+2)\right)}{\ln\left(x\right)} j(x)=x^x (0,1) \left(x^{1.15}\left(x^{2}-2x+2\right)\right)^{0.85}\geq x^{x^x}","['real-analysis', 'inequality', 'power-towers']"
7,Function of two sets,Function of two sets,,"Let $U$ be the set of all nonempty subsets of $[0,1]$ that are a union of finitely many  closed intervals (where an ""interval"" that is a single point does not count as an interval). Does there exist a function $f:U\times U\rightarrow U$ such that for any $A,B\in U$ : (a) $f(A,B) = f(B,A)$ (b) $f(A,B)$ has length (i.e. Lebesgue measure) less than $0.0001$ . (c) $f(A,B)\cap A$ has positive length. (d)  The length of $f(X,B)\cap A$ is maximized at $X=A$ . This is a variant of this question with more restrictive conditions, so my guess would be that the answer is no.","Let be the set of all nonempty subsets of that are a union of finitely many  closed intervals (where an ""interval"" that is a single point does not count as an interval). Does there exist a function such that for any : (a) (b) has length (i.e. Lebesgue measure) less than . (c) has positive length. (d)  The length of is maximized at . This is a variant of this question with more restrictive conditions, so my guess would be that the answer is no.","U [0,1] f:U\times U\rightarrow U A,B\in U f(A,B) = f(B,A) f(A,B) 0.0001 f(A,B)\cap A f(X,B)\cap A X=A","['real-analysis', 'optimization', 'lebesgue-measure']"
8,Putnam 2018 - Exercise A.5 - proof check,Putnam 2018 - Exercise A.5 - proof check,,"The problem statement is as follows. Let $f:\Bbb R \to \Bbb R$ be an infinitely differentiable function satisfying $f(0) = 0$ and $f(1) = 1$ , and $f(x) \geq 0$ for all $x \in \Bbb R$ . Show that there exist a positive integer $n$ and a real number $x$ such that $f^{(n)}(x)< 0$ . My proof. Suppose $f^{(n)}(x) \geq 0$ for all $x\in \Bbb R$ , for all $n \in \Bbb Z^+$ . If there exists a $c < 0$ such that $f(c)>0$ , then the Mean Value Theorem would give a negative derivative at some point between $c$ and $0$ . So $f(x) = 0$ for $x \leq 0$ , and thus, since for all $n$ $$\lim_{x\to 0^-}f^{(n)}(x) = 0.$$ we must have $f^{(n)}(0) = 0$ . (So far it is all like in some of the ""official"" proofs.) By Taylor's Theorem (with ""starting point"" $0$ ) we have $$f(x) = \frac{f^{(n)}(\eta_n(x))}{n!}x^n,$$ for some $\eta_n(x) \in (0,x)$ . In particular $$f(1) = \frac{f^{(n)}(\eta_n(1))}{n!} = 1,$$ which implies that there is a point $\eta_n(1)\in (0,1)$ such that $f^{(n)}(\eta_n(1)) = n!$ . By monotonicity of $f^{(n)}(x)$ , therefore, we must have $$f^{(n)}(1) \geq n!.$$ Using again Taylor's Theorem (starting at $1$ , this time) yields $$f(x) = \sum_{k=0}^n \frac{f^{(k)}(1)}{k!}(x-1)^k+ \frac{f^{(n+1)}(\xi_n(x))}{(n+1)!}(x-1)^{n+1},$$ for some $\xi_n(x) \in (1,x)$ , and where $f^{(0)}(x) = f(x)$ . This gives $$f(2) \geq n$$ for all $n\in \Bbb Z^+$ , a contradiction. Is my proof correct?","The problem statement is as follows. Let be an infinitely differentiable function satisfying and , and for all . Show that there exist a positive integer and a real number such that . My proof. Suppose for all , for all . If there exists a such that , then the Mean Value Theorem would give a negative derivative at some point between and . So for , and thus, since for all we must have . (So far it is all like in some of the ""official"" proofs.) By Taylor's Theorem (with ""starting point"" ) we have for some . In particular which implies that there is a point such that . By monotonicity of , therefore, we must have Using again Taylor's Theorem (starting at , this time) yields for some , and where . This gives for all , a contradiction. Is my proof correct?","f:\Bbb R \to \Bbb R f(0) = 0 f(1) = 1 f(x) \geq 0 x \in \Bbb R n x f^{(n)}(x)< 0 f^{(n)}(x) \geq 0 x\in \Bbb R n \in \Bbb Z^+ c < 0 f(c)>0 c 0 f(x) = 0 x \leq 0 n \lim_{x\to 0^-}f^{(n)}(x) = 0. f^{(n)}(0) = 0 0 f(x) = \frac{f^{(n)}(\eta_n(x))}{n!}x^n, \eta_n(x) \in (0,x) f(1) = \frac{f^{(n)}(\eta_n(1))}{n!} = 1, \eta_n(1)\in (0,1) f^{(n)}(\eta_n(1)) = n! f^{(n)}(x) f^{(n)}(1) \geq n!. 1 f(x) = \sum_{k=0}^n \frac{f^{(k)}(1)}{k!}(x-1)^k+ \frac{f^{(n+1)}(\xi_n(x))}{(n+1)!}(x-1)^{n+1}, \xi_n(x) \in (1,x) f^{(0)}(x) = f(x) f(2) \geq n n\in \Bbb Z^+","['real-analysis', 'solution-verification', 'taylor-expansion']"
9,Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$,Theorem 6.12 (a) in Baby Rudin:,\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha,"Here is part (a) of Theorem 6.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f_1 \in \mathscr{R}(\alpha)$ and $f_2 \in \mathscr{R}(\alpha)$ , then $$f_1 + f_2 \in \mathscr{R}(\alpha), $$ and $$ \int_a^b \left( f_1 + f_2 \right) d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha,$$ And, here is Rudin's proof. If $f = f_1 + f_2$ and $P$ is any partition of $[a, b]$ , we have $$ \tag{20} L\left(P, f_1, \alpha \right) + L\left(P, f_2, \alpha \right) \leq L\left(P, f, \alpha \right) \leq U\left(P, f, \alpha \right) \leq U\left(P, f_1, \alpha \right) + U\left(P, f_2, \alpha \right).$$ If $f_1 \in \mathscr{R}(\alpha)$ and $f_2 \in \mathscr{R}(\alpha)$ , let $\varepsilon > 0$ be given. There are partitions $P_j$ $(j = 1, 2)$ such that $$ U \left( P_j, f_j, \alpha \right) -  L \left( P_j, f_j, \alpha \right) < \varepsilon. $$ These inequalities persist if $P_1$ and $P_2$ are replaced by their common refinement $P$ . Then (20) implies $$ U (P, f, \alpha) - L (P, f, \alpha) < 2 \varepsilon, $$ which proves that $f \in \mathscr{R}(\alpha)$ . With this same $P$ we have $$ U \left( P, f_j , \alpha \right) < \int f_j d \alpha + \varepsilon \qquad (j = 1, 2); $$ hence (20) implies $$ \int f d \alpha \leq U ( P, f, \alpha) < \int f_1 d \alpha + \int f_2 d \alpha + 2 \varepsilon. $$ Since $\varepsilon$ was arbitrary, we conclude that $$ \tag{21}  \int f d \alpha \leq \int f_1 d \alpha + \int f_2 d \alpha. $$ If we replace $f_1$ and $f_2$ in (21) by $- f_1$ and $- f_2$ , the inequality is reversed, and the equality is proved. Now here is my reading of Rudin's proof. Let $f = f_1 + f_2$ , and let $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ , where $$a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b,$$ be any partition of $[a, b]$ . For each $i = 1, \ldots, n$ , if $m_{1i}$ , $m_{2i}$ , and $m_i$ are the infima of $f_1$ , $f_2$ , and $f$ , respectively, on $\left[ x_{i-1}, x_i \right]$ , then we see that $$ m_{1i} \leq f_1(x), \qquad m_{2i} \leq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ and so $$ m_{1i} + m_{2i} \leq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ which in turn implies that $$ m_{1i} + m_{2i} \leq m_i \qquad ( i = 1, \ldots, n  ),$$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $$  m_{1i} \Delta \alpha_i + m_{2i}  \Delta \alpha_i \leq m_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ),$$ where $ \Delta \alpha_i = \alpha\left( x_i \right) - \alpha\left( x_{i-1} \right)$ . Adding all the inequalities for $i = 1, \ldots, n$ , we thus obtain $$ L\left( P, f_1, \alpha \right) +  L\left( P, f_2, \alpha \right) \leq L ( P, f, \alpha). \tag{20(1)} $$ Similarly, for each $i = 1, \ldots, n$ , if $M_{1i}$ , $M_{2i}$ , and $M_i$ are the suprema of $f_1$ , $f_2$ , and $f$ , respectively, on $\left[ x_{i-1}, x_i \right]$ , then we see that $$ M_{1i} \geq f_1(x), \qquad M_{2i} \geq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ and so $$ M_{1i} + M_{2i} \geq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),$$ which in turn implies that $$ M_{1i} + M_{2i} \geq M_i \qquad ( i = 1, \ldots, n  ),$$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $$  M_{1i} \Delta \alpha_i + M_{2i}  \Delta \alpha_i \geq M_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ),$$ and, adding together all these inequalities for $i = 1, \ldots, n$ , we thus obtain $$ U\left( P, f_1, \alpha \right) +  U\left( P, f_2, \alpha \right) \geq U ( P, f, \alpha). \tag{20(2)} $$ From (20(1)) and (20(2)) we obtain (20) in Rudin's proof. Let $\varepsilon > 0$ be given. As $f_1$ and $f_2$ are Riemann-integrable with respect to $\alpha$ over $[a, b]$ , so (by Theorem 6.6 in Baby Rudin, 3rd edition) we can find partitions $P_1$ and $P_2$ of $[a, b]$ such that $$ U \left( P_1, f_1, \alpha \right) - L \left( P_1, f_1, \alpha \right) < \frac{\varepsilon}{2}, \qquad  U \left( P_2, f_2, \alpha \right) - L \left( P_2, f_2, \alpha \right) < \frac{\varepsilon}{2}. \tag{1} $$ Now let $P = P_1 \cup P_2$ . Then $P \supset P_1$ and $P \supset P_2$ . Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have $$ L \left( P_1, f_1, \alpha \right) \leq L (P, f_1, \alpha) \leq U(P, f_1, \alpha) \leq U(P_1, f_1, \alpha), $$ and $$  L \left( P_2, f_2, \alpha \right) \leq L (P, f_2, \alpha) \leq U(P, f_2, \alpha) \leq U(P_2, f_2, \alpha), $$ and so we can conclude that $$ U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \leq U ( P_1, f_1, \alpha) - L(P_1, f_1, \alpha) < { \varepsilon \over 2 }, $$ and $$  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) \leq U ( P_2, f_2, \alpha) - L(P_2, f_2, \alpha) < { \varepsilon \over 2 }, $$ which implies $$ U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) < { \varepsilon \over 2 }, \qquad  \mbox{ and  } \qquad  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) < { \varepsilon \over 2 }, \tag{2} $$ and this in turn implies that $$ \begin{align} &\ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right)  \right] \\  &= \left[  U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \right] + \left[  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right)   \right] \\ &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\ &= \varepsilon.  \end{align} \tag{3}  $$ But (20) implies that $$  \begin{align} & U ( P, f, \alpha) - L(P, f, \alpha) \\ &\leq  \left[ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) \right] - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right) \right]. \tag{4}  \end{align} $$ Now (3) and (4) together imply that $$ U(P, f, \alpha) - L(P, f, \alpha ) < \varepsilon. \tag{5} $$ Thus for every real number $\varepsilon > 0$ we can find a partition $P$ of $[a, b]$ such that (5) holds.  So by Theorem 6.6 in Baby Rudin we can conclude that $f \in \mathscr{R}(\alpha)$ . Thus $$ \underline{\int}_a^b f \ d \alpha = \overline{\int}_a^b f \ d \alpha, $$ that is, $$ \sup \left\{ \ L(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\} =  \inf \left\{ \ U(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\}. $$ Moreover, this common value is denoted by $\int_a^b f \ d \alpha$ . Now from (2) we find that $$ U\left( P, f_1, \alpha \right) < L \left( P, f_1, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_1 d \alpha + { \varepsilon \over 2 },  $$ and $$  U\left( P, f_2, \alpha \right) < L \left( P, f_2, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_2 d \alpha + { \varepsilon \over 2 },   $$ and so $$  U\left( P, f_1, \alpha \right) < \int_a^b f_1 d \alpha + { \varepsilon \over 2 },  \qquad \mbox{ and } \qquad  U\left( P, f_2, \alpha \right) < \int_a^b f_2 d \alpha + { \varepsilon \over 2 },  $$ and upone adding the last two inequalities we obtain $$ U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right) < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon. \tag{6} $$ Now using  (20) we can also conclude $$ \int_a^b f d \alpha \leq U(P, f, \alpha) \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right), $$ which implies $$ \int_a^b f d \alpha \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right). \tag{7} $$ Therefore  from (6) and (7) we can conclude that for every real number $\varepsilon > 0$ , we have $$ \int_a^b f d \alpha < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon, $$ which implies that $$ \int_a^b f d \alpha \leq  \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha. \tag{A}$$ Again from (2) we see that $$ \int_a^b f_1 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_1, \alpha \right) - { \varepsilon \over 2 } <   L \left( P, f_1, \alpha \right), $$ and $$ \int_a^b f_2 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_2, \alpha \right) - { \varepsilon \over 2 } <   L \left( P, f_2, \alpha \right), $$ which imply $$ \int_a^b f_1 d \alpha - { \varepsilon \over 2 } < L \left( P, f_1, \alpha \right), \qquad \mbox{ and } \qquad  \int_a^b f_2 d \alpha - { \varepsilon \over 2 } < L \left( P, f_2, \alpha \right), $$ and upon adding the last two inequalities we obtain $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < L \left( P, f_1, \alpha \right) +  L \left( P, f_2, \alpha \right). \tag{8}$$ But from (20) we obtain $$ L \left( P, f_1, \alpha \right) +  L \left( P, f_2, \alpha \right) \leq L(P, f, \alpha) \leq \int_a^b f d \alpha,$$ which implies $$ L \left( P, f_1, \alpha \right) +  L \left( P, f_2, \alpha \right) \leq \int_a^b f d \alpha. \tag{9} $$ From (8) and (9) we have $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < \int_a^b f d \alpha, $$ and so $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  < \int_a^b f d \alpha + \varepsilon  $$ for every real number $\varepsilon > 0$ . Therefore $$ \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  \leq  \int_a^b f d \alpha. \tag{B}  $$ Now from (A) and (B) we can conclude that $$ \int_a^b f d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha, $$ as required. Is my rendering of Rudin's proof correct (and as intended by Rudin)?","Here is part (a) of Theorem 6.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If and , then and And, here is Rudin's proof. If and is any partition of , we have If and , let be given. There are partitions such that These inequalities persist if and are replaced by their common refinement . Then (20) implies which proves that . With this same we have hence (20) implies Since was arbitrary, we conclude that If we replace and in (21) by and , the inequality is reversed, and the equality is proved. Now here is my reading of Rudin's proof. Let , and let , where be any partition of . For each , if , , and are the infima of , , and , respectively, on , then we see that and so which in turn implies that and as is a monotonically increasing function on , so where . Adding all the inequalities for , we thus obtain Similarly, for each , if , , and are the suprema of , , and , respectively, on , then we see that and so which in turn implies that and as is a monotonically increasing function on , so and, adding together all these inequalities for , we thus obtain From (20(1)) and (20(2)) we obtain (20) in Rudin's proof. Let be given. As and are Riemann-integrable with respect to over , so (by Theorem 6.6 in Baby Rudin, 3rd edition) we can find partitions and of such that Now let . Then and . Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have and and so we can conclude that and which implies and this in turn implies that But (20) implies that Now (3) and (4) together imply that Thus for every real number we can find a partition of such that (5) holds.  So by Theorem 6.6 in Baby Rudin we can conclude that . Thus that is, Moreover, this common value is denoted by . Now from (2) we find that and and so and upone adding the last two inequalities we obtain Now using  (20) we can also conclude which implies Therefore  from (6) and (7) we can conclude that for every real number , we have which implies that Again from (2) we see that and which imply and upon adding the last two inequalities we obtain But from (20) we obtain which implies From (8) and (9) we have and so for every real number . Therefore Now from (A) and (B) we can conclude that as required. Is my rendering of Rudin's proof correct (and as intended by Rudin)?","f_1 \in \mathscr{R}(\alpha) f_2 \in \mathscr{R}(\alpha) f_1 + f_2 \in \mathscr{R}(\alpha),   \int_a^b \left( f_1 + f_2 \right) d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha, f = f_1 + f_2 P [a, b]  \tag{20} L\left(P, f_1, \alpha \right) + L\left(P, f_2, \alpha \right) \leq L\left(P, f, \alpha \right) \leq U\left(P, f, \alpha \right) \leq U\left(P, f_1, \alpha \right) + U\left(P, f_2, \alpha \right). f_1 \in \mathscr{R}(\alpha) f_2 \in \mathscr{R}(\alpha) \varepsilon > 0 P_j (j = 1, 2)  U \left( P_j, f_j, \alpha \right) -  L \left( P_j, f_j, \alpha \right) < \varepsilon.  P_1 P_2 P  U (P, f, \alpha) - L (P, f, \alpha) < 2 \varepsilon,  f \in \mathscr{R}(\alpha) P  U \left( P, f_j , \alpha \right) < \int f_j d \alpha + \varepsilon \qquad (j = 1, 2);   \int f d \alpha \leq U ( P, f, \alpha) < \int f_1 d \alpha + \int f_2 d \alpha + 2 \varepsilon.  \varepsilon  \tag{21}  \int f d \alpha \leq \int f_1 d \alpha + \int f_2 d \alpha.  f_1 f_2 - f_1 - f_2 f = f_1 + f_2 P = \left\{ x_0, x_1, \ldots, x_n \right\} a = x_0 \leq x_1 \leq \cdots \leq x_{n-1} \leq x_n = b, [a, b] i = 1, \ldots, n m_{1i} m_{2i} m_i f_1 f_2 f \left[ x_{i-1}, x_i \right]  m_{1i} \leq f_1(x), \qquad m_{2i} \leq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),  m_{1i} + m_{2i} \leq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),  m_{1i} + m_{2i} \leq m_i \qquad ( i = 1, \ldots, n  ), \alpha [a, b]   m_{1i} \Delta \alpha_i + m_{2i}  \Delta \alpha_i \leq m_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ),  \Delta \alpha_i = \alpha\left( x_i \right) - \alpha\left( x_{i-1} \right) i = 1, \ldots, n  L\left( P, f_1, \alpha \right) +  L\left( P, f_2, \alpha \right) \leq L ( P, f, \alpha). \tag{20(1)}  i = 1, \ldots, n M_{1i} M_{2i} M_i f_1 f_2 f \left[ x_{i-1}, x_i \right]  M_{1i} \geq f_1(x), \qquad M_{2i} \geq f_2(x) \qquad (x_{i-1} \leq x \leq x_i ),  M_{1i} + M_{2i} \geq f_1(x) + f_2(x) = f(x) \qquad (x_{i-1} \leq x \leq x_i ),  M_{1i} + M_{2i} \geq M_i \qquad ( i = 1, \ldots, n  ), \alpha [a, b]   M_{1i} \Delta \alpha_i + M_{2i}  \Delta \alpha_i \geq M_i  \Delta \alpha_i \qquad ( i = 1, \ldots, n  ), i = 1, \ldots, n  U\left( P, f_1, \alpha \right) +  U\left( P, f_2, \alpha \right) \geq U ( P, f, \alpha). \tag{20(2)}  \varepsilon > 0 f_1 f_2 \alpha [a, b] P_1 P_2 [a, b]  U \left( P_1, f_1, \alpha \right) - L \left( P_1, f_1, \alpha \right) < \frac{\varepsilon}{2}, \qquad  U \left( P_2, f_2, \alpha \right) - L \left( P_2, f_2, \alpha \right) < \frac{\varepsilon}{2}. \tag{1}  P = P_1 \cup P_2 P \supset P_1 P \supset P_2  L \left( P_1, f_1, \alpha \right) \leq L (P, f_1, \alpha) \leq U(P, f_1, \alpha) \leq U(P_1, f_1, \alpha),    L \left( P_2, f_2, \alpha \right) \leq L (P, f_2, \alpha) \leq U(P, f_2, \alpha) \leq U(P_2, f_2, \alpha),   U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \leq U ( P_1, f_1, \alpha) - L(P_1, f_1, \alpha) < { \varepsilon \over 2 },    U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) \leq U ( P_2, f_2, \alpha) - L(P_2, f_2, \alpha) < { \varepsilon \over 2 },   U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) < { \varepsilon \over 2 }, \qquad  \mbox{ and  } \qquad  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right) < { \varepsilon \over 2 }, \tag{2}  
\begin{align}
&\ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right)  \right] \\ 
&= \left[  U \left(P, f_1, \alpha \right) - L \left(P, f_1, \alpha \right) \right] + \left[  U \left(P, f_2, \alpha \right) - L \left(P, f_2, \alpha \right)   \right] \\
&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
&= \varepsilon. 
\end{align} \tag{3} 
  
\begin{align}
& U ( P, f, \alpha) - L(P, f, \alpha) \\
&\leq  \left[ U \left(P, f_1, \alpha \right) + U \left(P, f_2, \alpha \right) \right] - \left[ L \left(P, f_1, \alpha \right) + L \left(P, f_2, \alpha \right) \right]. \tag{4} 
\end{align}
  U(P, f, \alpha) - L(P, f, \alpha ) < \varepsilon. \tag{5}  \varepsilon > 0 P [a, b] f \in \mathscr{R}(\alpha)  \underline{\int}_a^b f \ d \alpha = \overline{\int}_a^b f \ d \alpha,   \sup \left\{ \ L(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\} =  \inf \left\{ \ U(Q, f, \alpha) \ \colon \ Q \mbox{ is a partition of } [a, b] \ \right\}.  \int_a^b f \ d \alpha  U\left( P, f_1, \alpha \right) < L \left( P, f_1, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_1 d \alpha + { \varepsilon \over 2 },     U\left( P, f_2, \alpha \right) < L \left( P, f_2, \alpha \right) + { \varepsilon \over 2 } \leq \int_a^b f_2 d \alpha + { \varepsilon \over 2 },      U\left( P, f_1, \alpha \right) < \int_a^b f_1 d \alpha + { \varepsilon \over 2 },  \qquad \mbox{ and } \qquad  U\left( P, f_2, \alpha \right) < \int_a^b f_2 d \alpha + { \varepsilon \over 2 },    U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right) < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon. \tag{6}   \int_a^b f d \alpha \leq U(P, f, \alpha) \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right),   \int_a^b f d \alpha \leq  U\left( P, f_1, \alpha \right) + U\left( P, f_2, \alpha \right). \tag{7}  \varepsilon > 0  \int_a^b f d \alpha < \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha + \varepsilon,   \int_a^b f d \alpha \leq  \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha. \tag{A}  \int_a^b f_1 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_1, \alpha \right) - { \varepsilon \over 2 } <  
L \left( P, f_1, \alpha \right),   \int_a^b f_2 d \alpha - { \varepsilon \over 2 } \leq U \left( P, f_2, \alpha \right) - { \varepsilon \over 2 } <  
L \left( P, f_2, \alpha \right),   \int_a^b f_1 d \alpha - { \varepsilon \over 2 } < L \left( P, f_1, \alpha \right), \qquad \mbox{ and } \qquad 
\int_a^b f_2 d \alpha - { \varepsilon \over 2 } < L \left( P, f_2, \alpha \right),   \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right). \tag{8}  L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right) \leq L(P, f, \alpha) \leq \int_a^b f d \alpha,  L \left( P, f_1, \alpha \right) + 
L \left( P, f_2, \alpha \right) \leq \int_a^b f d \alpha. \tag{9}   \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha -  \varepsilon < \int_a^b f d \alpha,   \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  < \int_a^b f d \alpha + \varepsilon   \varepsilon > 0  \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha  \leq  \int_a^b f d \alpha. \tag{B}    \int_a^b f d \alpha = \int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha, ","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'solution-verification']"
10,Integrate functions from point 'a' to point 'a' proof,Integrate functions from point 'a' to point 'a' proof,,"I understand that the integral of any function from and to the same point must equal zero, such; $$\int_a^a f(x) \,dx= 0$$ It makes sense, area from one point to the same point should be zero. But, how is this shown in a mathematical sense, proving it fully, without just saying 'it makes sense'?","I understand that the integral of any function from and to the same point must equal zero, such; $$\int_a^a f(x) \,dx= 0$$ It makes sense, area from one point to the same point should be zero. But, how is this shown in a mathematical sense, proving it fully, without just saying 'it makes sense'?",,"['calculus', 'real-analysis', 'integration']"
11,How to prove that $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $,How to prove that  is dense in, {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R}) ,This question is inspired by an old question on here. Prove: $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $ Proof: $ {\mathbf{GL}_{n}}(\mathbb{R}) = \{A \in {\mathbf{M}_{n}}(\mathbb{R}) : \det A \neq 0\}$ where $ {\mathbf{M}_{n}}(\mathbb{R}) $ consists of all $n \times n$ matrices with real entries. We want to prove that $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $ . $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $ if every matrix $A \in {\mathbf{M}_{n}}(\mathbb{R}) $ either belongs to $ {\mathbf{GL}_{n}}(\mathbb{R}) $ or is a limit point of $ {\mathbf{GL}_{n}}(\mathbb{R}) $ . A matrix $P$ is a limit point of $ {\mathbf{GL}_{n}}(\mathbb{R}) $ in $ {\mathbf{M}_{n}}(\mathbb{R}) $ if and only if every open set $U$ containing $P$ also contains  a point of $ {\mathbf{GL}_{n}}(\mathbb{R}) $ different from $P$ . And now when we should start to think I am not sure what to do as I have never worked with $ {\mathbf{GL}_{n}}(\mathbb{R}) $ or $ {\mathbf{M}_{n}}(\mathbb{R}) $ before. I would appreciate a hint or a few on how to continue.,This question is inspired by an old question on here. Prove: is dense in Proof: where consists of all matrices with real entries. We want to prove that is dense in . is dense in if every matrix either belongs to or is a limit point of . A matrix is a limit point of in if and only if every open set containing also contains  a point of different from . And now when we should start to think I am not sure what to do as I have never worked with or before. I would appreciate a hint or a few on how to continue., {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R})   {\mathbf{GL}_{n}}(\mathbb{R}) = \{A \in {\mathbf{M}_{n}}(\mathbb{R}) : \det A \neq 0\}  {\mathbf{M}_{n}}(\mathbb{R})  n \times n  {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R})   {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R})  A \in {\mathbf{M}_{n}}(\mathbb{R})   {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{GL}_{n}}(\mathbb{R})  P  {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R})  U P  {\mathbf{GL}_{n}}(\mathbb{R})  P  {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R}) ,"['real-analysis', 'general-topology', 'matrices']"
12,Proving $\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4} $,Proving,\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4} ,"So this integral reminds me of the Dirichlet integral but I am not sure if I can use similar methods to solve it. I want to prove $$\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4} $$ I tried parameterizing with $$ I(a) := \int_0^{\infty} \sin(ax)\frac{\sin^2(x)}{x^2}dx$$ or $$ I(a) := \int_0^{\infty} \frac{\sin^3(x)}{x^2}e^{-ax}dx$$ But none of them worked out for me. Not sure what to do. I would really like to use real methods and not complex analysis, since I haven’t learned it yet.","So this integral reminds me of the Dirichlet integral but I am not sure if I can use similar methods to solve it. I want to prove I tried parameterizing with or But none of them worked out for me. Not sure what to do. I would really like to use real methods and not complex analysis, since I haven’t learned it yet.",\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4}   I(a) := \int_0^{\infty} \sin(ax)\frac{\sin^2(x)}{x^2}dx  I(a) := \int_0^{\infty} \frac{\sin^3(x)}{x^2}e^{-ax}dx,"['real-analysis', 'calculus', 'integration', 'definite-integrals']"
13,Counterexample to Leibniz criterion for alternating series,Counterexample to Leibniz criterion for alternating series,,"I have this statement and I need to say if it is true or false: Let $\{a_n\}$ be a real sequence. $$\lim_{n\to +\infty} a_n = 0 \quad \implies \quad \sum_{n=1}^{\infty}(-1)^na_n \quad \text{converges}$$ I know, from the Leibniz criterion that: If $a_n \to 0$, $a_n$ is decreasing and positive then $\sum_{n=1}^{\infty}(-1)^na_n$ converges From this fact I believe that the statement is false but I couldn't come up with an infinitesimal sequence that isn't decreasing and for that reason is divergent. I tried some function with $sin(\frac{1}{n})$ without any luck. Any help would be very appreciated, thank you!","I have this statement and I need to say if it is true or false: Let $\{a_n\}$ be a real sequence. $$\lim_{n\to +\infty} a_n = 0 \quad \implies \quad \sum_{n=1}^{\infty}(-1)^na_n \quad \text{converges}$$ I know, from the Leibniz criterion that: If $a_n \to 0$, $a_n$ is decreasing and positive then $\sum_{n=1}^{\infty}(-1)^na_n$ converges From this fact I believe that the statement is false but I couldn't come up with an infinitesimal sequence that isn't decreasing and for that reason is divergent. I tried some function with $sin(\frac{1}{n})$ without any luck. Any help would be very appreciated, thank you!",,"['real-analysis', 'sequences-and-series']"
14,Does $\lim_{n \to \infty}a_n^{1/n} = 1$ imply $\lim_{n \to \infty} \frac{a_{n + 1}}{a_n} = 1$?,Does  imply ?,\lim_{n \to \infty}a_n^{1/n} = 1 \lim_{n \to \infty} \frac{a_{n + 1}}{a_n} = 1,Supose I have a sequence $\{a_n\}$ of positive real numbers such that $\lim\limits_{n \to \infty}a_n^{1/n} = 1$. Is it true that $\lim\limits_{n \to \infty} \frac{a_{n + 1}}{a_n} = 1$ or depends of the sequence that a choose?,Supose I have a sequence $\{a_n\}$ of positive real numbers such that $\lim\limits_{n \to \infty}a_n^{1/n} = 1$. Is it true that $\lim\limits_{n \to \infty} \frac{a_{n + 1}}{a_n} = 1$ or depends of the sequence that a choose?,,['real-analysis']
15,Computig the series $\sum\limits_{n=2}^\infty \ln\left(1-\frac{1}{n^2}\right)$,Computig the series,\sum\limits_{n=2}^\infty \ln\left(1-\frac{1}{n^2}\right),"So I have this problem for midterm reviews: $$\sum_{n=2}^\infty \ln\left(1-\frac{1}{n^2}\right)=\text{ ?}$$ I know that you can find the series form of a natural log, as shown here: $$\ln\left(1-\frac{1}{n^2}\right)=-\sum_{k=2}^\infty \left(\frac{1}{n^{4k}}\right)\left(\frac{1}{2k}\right) $$ But the above doesn't seem to help very much since it results in two summation notations mushed together. Is there a somewhat nontedious way to go about this? Thanks! All help appreciated.","So I have this problem for midterm reviews: $$\sum_{n=2}^\infty \ln\left(1-\frac{1}{n^2}\right)=\text{ ?}$$ I know that you can find the series form of a natural log, as shown here: $$\ln\left(1-\frac{1}{n^2}\right)=-\sum_{k=2}^\infty \left(\frac{1}{n^{4k}}\right)\left(\frac{1}{2k}\right) $$ But the above doesn't seem to help very much since it results in two summation notations mushed together. Is there a somewhat nontedious way to go about this? Thanks! All help appreciated.",,"['calculus', 'real-analysis', 'sequences-and-series', 'summation']"
16,"Example where $f\circ g$ is bijective, but neither $f$ nor $g$ is bijective","Example where  is bijective, but neither  nor  is bijective",f\circ g f g,"Can anyone come up with an explicit example of two functions $f$ and $g$ such that: $f\circ g$ is bijective, but neither $f$ nor $g$ is bijective? I tried the following: $$f:\mathbb{R}\rightarrow \mathbb{R^{+}} $$ $$f(x)=x^{2}$$ and  $$g:\mathbb{R^{+}}\rightarrow \mathbb{R}$$ $$g(x)=\sqrt{x}$$ $f$ is not injective, and $g$ is not surjective, but $f\circ g$ is bijective Any other examples?","Can anyone come up with an explicit example of two functions $f$ and $g$ such that: $f\circ g$ is bijective, but neither $f$ nor $g$ is bijective? I tried the following: $$f:\mathbb{R}\rightarrow \mathbb{R^{+}} $$ $$f(x)=x^{2}$$ and  $$g:\mathbb{R^{+}}\rightarrow \mathbb{R}$$ $$g(x)=\sqrt{x}$$ $f$ is not injective, and $g$ is not surjective, but $f\circ g$ is bijective Any other examples?",,"['real-analysis', 'calculus', 'functions', 'examples-counterexamples', 'function-and-relation-composition']"
17,What rational number is between these two real numbers?,What rational number is between these two real numbers?,,"According to several texts and professors, there exists a rational number between any two real numbers. But suppose you had two real numbers which had the same digits in the same places up to some place, where they differed by one digit - say the smaller one had a $4$ and the bigger one had a $5$. The smaller number's digits following the four are all $9$s, and the bigger number's digits after the five are all $0$s. For example: $$ \beta=1.235\overline{0} \\ \alpha=1.234\overline{9}$$ What rational number is between $\alpha$ and $\beta$?","According to several texts and professors, there exists a rational number between any two real numbers. But suppose you had two real numbers which had the same digits in the same places up to some place, where they differed by one digit - say the smaller one had a $4$ and the bigger one had a $5$. The smaller number's digits following the four are all $9$s, and the bigger number's digits after the five are all $0$s. For example: $$ \beta=1.235\overline{0} \\ \alpha=1.234\overline{9}$$ What rational number is between $\alpha$ and $\beta$?",,['real-analysis']
18,Why does this iterative way of solving an equation work?,Why does this iterative way of solving an equation work?,,"I was solving some semiconductor physics problem and in order to get the temperature I got this nasty equation: $$ T = \dfrac{7020}{\dfrac{3}{2}\ln(T)+12}.$$ It seems that I can solve this kind of equation by simply guessing a solution for $T$ and then substituting that answer back into the equation and then again substituting the new answer back into equation and so on until I am satisfied by the precision of result. Somehow this method works. Concretely for my example, my first guess was $T=1$ and I got this sequence of numbers $(585.0, 325.6419704169386, 339.4797907885183, 338.4580701961562, 338.53186591337385,338.52652733834424, ...)$ and they really seem to solve equation better and better. Questions. 1) What is an intuitive way to see why this method works? 2) How can I show rigorously that this method actually converges to a solution of the equation? 3) An obvious generalization for which the method might work seems to be: $$ x = \dfrac{a}{b\ln(x)+c}. $$ For which $a,b,c$ will this method work? Is this equation a special case of some natural generalization of this equation? What are some similar equations which I can solve by the method described? 4) When will the sequence of numbers in the iteration process converge in  finitely many steps to an exact solution to the equation? Does that case exist? Is a solution to: $$ x = \dfrac{a}{b\ln(x)+c} $$ irrational for every $a,b,c$ ? Is it transcendental? If not, for which $a,b,c$ will that be the case? Thank you for any help.","I was solving some semiconductor physics problem and in order to get the temperature I got this nasty equation: It seems that I can solve this kind of equation by simply guessing a solution for and then substituting that answer back into the equation and then again substituting the new answer back into equation and so on until I am satisfied by the precision of result. Somehow this method works. Concretely for my example, my first guess was and I got this sequence of numbers and they really seem to solve equation better and better. Questions. 1) What is an intuitive way to see why this method works? 2) How can I show rigorously that this method actually converges to a solution of the equation? 3) An obvious generalization for which the method might work seems to be: For which will this method work? Is this equation a special case of some natural generalization of this equation? What are some similar equations which I can solve by the method described? 4) When will the sequence of numbers in the iteration process converge in  finitely many steps to an exact solution to the equation? Does that case exist? Is a solution to: irrational for every ? Is it transcendental? If not, for which will that be the case? Thank you for any help."," T = \dfrac{7020}{\dfrac{3}{2}\ln(T)+12}. T T=1 (585.0, 325.6419704169386, 339.4797907885183, 338.4580701961562, 338.53186591337385,338.52652733834424, ...)  x = \dfrac{a}{b\ln(x)+c}.  a,b,c  x = \dfrac{a}{b\ln(x)+c}  a,b,c a,b,c","['real-analysis', 'sequences-and-series', 'recurrence-relations', 'irrational-numbers']"
19,Are there continuous functions for which the epsilon-delta property doesn't hold? [duplicate],Are there continuous functions for which the epsilon-delta property doesn't hold? [duplicate],,"This question already has an answer here : What does ""if and only if"" mean in definitions? (1 answer) Closed 7 years ago . The standard definition of continuity is as follows: A function is continuous if $$\forall \varepsilon > 0\  \exists \delta > 0\ \text{s.t. } 0 < |x - x_0| < \delta \implies |f(x) - f(x_0)| < \varepsilon $$ This may sound silly, but is the converse true? In other words, is there an example of a function which is continuous, but for which this property doesn't hold? I thought I remembered reading something about the exponential function being an example of a continuous function for which the epsilon-delta implication is false, since it gets arbitrarily steep as x approaches infinity, but I could be wrong. Thanks!","This question already has an answer here : What does ""if and only if"" mean in definitions? (1 answer) Closed 7 years ago . The standard definition of continuity is as follows: A function is continuous if $$\forall \varepsilon > 0\  \exists \delta > 0\ \text{s.t. } 0 < |x - x_0| < \delta \implies |f(x) - f(x_0)| < \varepsilon $$ This may sound silly, but is the converse true? In other words, is there an example of a function which is continuous, but for which this property doesn't hold? I thought I remembered reading something about the exponential function being an example of a continuous function for which the epsilon-delta implication is false, since it gets arbitrarily steep as x approaches infinity, but I could be wrong. Thanks!",,"['calculus', 'real-analysis', 'limits', 'continuity', 'epsilon-delta']"
20,Prove that $\lim_{n\to\infty} (\sqrt{n^2+n}-n) = \frac{1}{2}$,Prove that,\lim_{n\to\infty} (\sqrt{n^2+n}-n) = \frac{1}{2},"Here's the question: Prove that $\lim_{n \to \infty} (\sqrt{n^2+n}-n) = \frac{1}{2}.$ Here's my attempt at a solution, but for some reason, the $N$ that I arrive at is incorrect (I ran a computer program to test my solution against some test cases, and it spits out an error). Can anyone spot the error for me? $\left|\sqrt{n^2+n}-n-\frac{1}{2}\right| < \epsilon$ $\Rightarrow \left|\frac{n}{\sqrt{n^2+n}+n} - \frac{1}{2}\right| < \epsilon$ $\Rightarrow \frac{1}{2}-\frac{1}{\sqrt{1+\frac{1}{n}}+1} < \epsilon$ $\Rightarrow \frac{1}{\sqrt{1+\frac{1}{n}}+1} > \frac{1}{2} - \epsilon = \frac{1-2 \epsilon}{2}$ $\Rightarrow \frac{1}{\sqrt{1+\frac{1}{n}}} > \frac{1-2 \epsilon}{2}$ $\Rightarrow \frac{1}{\sqrt{\frac{1}{n}}} > \frac{1-2 \epsilon}{2}$ $\Rightarrow \sqrt{n} > \frac{1-2 \epsilon}{2}$ $\Rightarrow n > \frac{4 {\epsilon}^2-4 \epsilon +1}{4}$","Here's the question: Prove that Here's my attempt at a solution, but for some reason, the that I arrive at is incorrect (I ran a computer program to test my solution against some test cases, and it spits out an error). Can anyone spot the error for me?",\lim_{n \to \infty} (\sqrt{n^2+n}-n) = \frac{1}{2}. N \left|\sqrt{n^2+n}-n-\frac{1}{2}\right| < \epsilon \Rightarrow \left|\frac{n}{\sqrt{n^2+n}+n} - \frac{1}{2}\right| < \epsilon \Rightarrow \frac{1}{2}-\frac{1}{\sqrt{1+\frac{1}{n}}+1} < \epsilon \Rightarrow \frac{1}{\sqrt{1+\frac{1}{n}}+1} > \frac{1}{2} - \epsilon = \frac{1-2 \epsilon}{2} \Rightarrow \frac{1}{\sqrt{1+\frac{1}{n}}} > \frac{1-2 \epsilon}{2} \Rightarrow \frac{1}{\sqrt{\frac{1}{n}}} > \frac{1-2 \epsilon}{2} \Rightarrow \sqrt{n} > \frac{1-2 \epsilon}{2} \Rightarrow n > \frac{4 {\epsilon}^2-4 \epsilon +1}{4},"['real-analysis', 'limits', 'radicals']"
21,How do you calculate this limit $\mathop {\lim }\limits_{x \to 0} \frac{{\sin (\sin x)}}{x}$?,How do you calculate this limit ?,\mathop {\lim }\limits_{x \to 0} \frac{{\sin (\sin x)}}{x},How do you calculate this limit $$\mathop {\lim }\limits_{x \to 0} \frac{{\sin (\sin x)}}{x}?$$ without derivatives please. Thanks.,How do you calculate this limit $$\mathop {\lim }\limits_{x \to 0} \frac{{\sin (\sin x)}}{x}?$$ without derivatives please. Thanks.,,"['calculus', 'real-analysis', 'trigonometry', 'limits']"
22,"Give an example of a function $h$ that is discontinuous at every point of $[0,1]$, but with $|h|$ continuous on $[0,1]$","Give an example of a function  that is discontinuous at every point of , but with  continuous on","h [0,1] |h| [0,1]","Give an example of a function $h:[0,1]\to\mathbb{R}$ that is discontinuous at every point of $[0,1]$, but such that the function $| h |$ that is continuous on $[0,1]$. I don't really even know where to start with this one. I would have to prove that the function $| h |$ is continuous on $[0,1]$, ie if we're given any $\varepsilon>0$, there exists $\delta>0$ such that if $x$ and $c$ are any two points in $[0,1]$ with $|x-c| < \delta$, then $|f(x) - f(c)| < \varepsilon$. Alternatively I could use the limit definition. But I can only think of functions that are discontinuous at some points in  $[0,1]$ rather than all... I feel like I'm missing something obvious here, but any help is greatly appreciated. This is a question on a past final.","Give an example of a function $h:[0,1]\to\mathbb{R}$ that is discontinuous at every point of $[0,1]$, but such that the function $| h |$ that is continuous on $[0,1]$. I don't really even know where to start with this one. I would have to prove that the function $| h |$ is continuous on $[0,1]$, ie if we're given any $\varepsilon>0$, there exists $\delta>0$ such that if $x$ and $c$ are any two points in $[0,1]$ with $|x-c| < \delta$, then $|f(x) - f(c)| < \varepsilon$. Alternatively I could use the limit definition. But I can only think of functions that are discontinuous at some points in  $[0,1]$ rather than all... I feel like I'm missing something obvious here, but any help is greatly appreciated. This is a question on a past final.",,"['real-analysis', 'continuity', 'examples-counterexamples']"
23,"How to show that every $\alpha$-Hölder function, with $\alpha>1$, is constant?","How to show that every -Hölder function, with , is constant?",\alpha \alpha>1,"Suppose $f:(a,b) \to \mathbb{R} $ satisfy $|f(x) - f(y) | \le M |x-y|^\alpha$ for some  $\alpha >1$ and all $x,y \in (a,b) $. Prove that $f$ is constant on $(a,b)$. I'm not sure which theorem should I look to prove this question. Can you guys give me a bit of hint? First of all how to prove some function $f(x)$ is constant on $(a,b)$? Just show $f'(x) = 0$?","Suppose $f:(a,b) \to \mathbb{R} $ satisfy $|f(x) - f(y) | \le M |x-y|^\alpha$ for some  $\alpha >1$ and all $x,y \in (a,b) $. Prove that $f$ is constant on $(a,b)$. I'm not sure which theorem should I look to prove this question. Can you guys give me a bit of hint? First of all how to prove some function $f(x)$ is constant on $(a,b)$? Just show $f'(x) = 0$?",,"['real-analysis', 'calculus', 'holder-spaces']"
24,Is the union of two nowhere dense sets nowhere dense?,Is the union of two nowhere dense sets nowhere dense?,,"Is the union of two nowhere dense sets nowhere dense? Using the following definition: A nowhere dense set is a subset $E\subset X$ of a metric space (or topological space) $X$ such that $(\overline{E})^o=\emptyset$. I tried using topological properties like ""union of closure of sets is the closure of union"", and others. I tried also using the fact that $(\overline{A})^c={(A^c)}^o$ and other complement elementary-set-theory identities.","Is the union of two nowhere dense sets nowhere dense? Using the following definition: A nowhere dense set is a subset $E\subset X$ of a metric space (or topological space) $X$ such that $(\overline{E})^o=\emptyset$. I tried using topological properties like ""union of closure of sets is the closure of union"", and others. I tried also using the fact that $(\overline{A})^c={(A^c)}^o$ and other complement elementary-set-theory identities.",,"['real-analysis', 'general-topology', 'baire-category']"
25,Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$,Proving that:,\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab},Let $a$ and $b$ be positive reals. Show that $$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$,Let $a$ and $b$ be positive reals. Show that $$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$,,"['real-analysis', 'limits', 'real-numbers', 'radicals', 'means']"
26,Counting the Number of Real Roots of A Polynomial,Counting the Number of Real Roots of A Polynomial,,"I am interested in solving problems which involve finding the number of real roots of any polynomial. Suppose I take a function $$f(x)=x^6+x^5+x^4+x^3+x^2+x+1$$ This does not have any real roots but I am trying to figure out if there is some analytical way that does not involve graphing to come to this conclusion. Using Descartes' Rule of Signs , there are zero sign changes in $f$ so by virtue of which there are no positive roots to the polynomial. Considering $$f(-x) = x^6-x^5+x^4-x^3+x^2-x+1$$ I concluded that there are either 6 negative, 4 negative, 2 negative or zero negative roots. So I have 4 cases to consider : 0 positive roots, 6 negative roots, 0 complex roots 0 positive roots, 4 negative roots, 2 complex roots 0 positive roots, 2 negative roots, 4 complex roots 0 positive roots, 0 negative roots, 6 complex roots (The correct case) I tried differentiating $f$ but the derivative is equally bad $$f'(x) = 6x^5+5x^4+4x^3+3x^2+2x+1$$ I am unable to conclude anything from this. I tried going about the problem the other way. If a polynomial with an even degree is always positive or negative depending on the leading coefficient, it will not have any real roots but then again, finding the extrema of the function is proving to be extremely difficult. I have tried using Bolzano's Intermediate Value Theorem . It guarantees the existence of at least one root but then again, there is a possibility that there might be more than one which can only be eliminated by monotonicity which again brings me back to the bad derivative. I believe there need to be some general rules by virtue of which, we are able to calculate the number of roots for any polynomial. Is graphing the best technique for polynomials like these and if it is, are there any ways by which a quick but accurate plot can be drawn? While reading about the relevant theory, I came across Sturm's Method and the Newton-Raphson Method but haven't touched these yet. Is it absolutely required to know these concepts to effectively draw conclusions? Have I missed something?","I am interested in solving problems which involve finding the number of real roots of any polynomial. Suppose I take a function This does not have any real roots but I am trying to figure out if there is some analytical way that does not involve graphing to come to this conclusion. Using Descartes' Rule of Signs , there are zero sign changes in so by virtue of which there are no positive roots to the polynomial. Considering I concluded that there are either 6 negative, 4 negative, 2 negative or zero negative roots. So I have 4 cases to consider : 0 positive roots, 6 negative roots, 0 complex roots 0 positive roots, 4 negative roots, 2 complex roots 0 positive roots, 2 negative roots, 4 complex roots 0 positive roots, 0 negative roots, 6 complex roots (The correct case) I tried differentiating but the derivative is equally bad I am unable to conclude anything from this. I tried going about the problem the other way. If a polynomial with an even degree is always positive or negative depending on the leading coefficient, it will not have any real roots but then again, finding the extrema of the function is proving to be extremely difficult. I have tried using Bolzano's Intermediate Value Theorem . It guarantees the existence of at least one root but then again, there is a possibility that there might be more than one which can only be eliminated by monotonicity which again brings me back to the bad derivative. I believe there need to be some general rules by virtue of which, we are able to calculate the number of roots for any polynomial. Is graphing the best technique for polynomials like these and if it is, are there any ways by which a quick but accurate plot can be drawn? While reading about the relevant theory, I came across Sturm's Method and the Newton-Raphson Method but haven't touched these yet. Is it absolutely required to know these concepts to effectively draw conclusions? Have I missed something?",f(x)=x^6+x^5+x^4+x^3+x^2+x+1 f f(-x) = x^6-x^5+x^4-x^3+x^2-x+1 f f'(x) = 6x^5+5x^4+4x^3+3x^2+2x+1,"['real-analysis', 'calculus', 'algebra-precalculus', 'derivatives', 'polynomials']"
27,Proof that $\sum_{1}^{\infty} \frac{1}{n^2} <2$,Proof that,\sum_{1}^{\infty} \frac{1}{n^2} <2,"I know how to prove that $$\sum_1^{\infty} \frac{1}{n^2}<2$$ because $$\sum_1^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}<2$$ But I wanted to prove it using only inequalities. Is there a way to do it? Can you think of an inequality such that you can calculate the limit of both sides, and the limit of the rigth side is $2$? Is there a good book about inequalities that helps to prove that a sum is less than a given quantity? This is not a homework problem, its a self posed problem that I was thinking about :)","I know how to prove that $$\sum_1^{\infty} \frac{1}{n^2}<2$$ because $$\sum_1^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}<2$$ But I wanted to prove it using only inequalities. Is there a way to do it? Can you think of an inequality such that you can calculate the limit of both sides, and the limit of the rigth side is $2$? Is there a good book about inequalities that helps to prove that a sum is less than a given quantity? This is not a homework problem, its a self posed problem that I was thinking about :)",,"['calculus', 'real-analysis', 'summation']"
28,"The derivative of $e^x$ using the definition of derivative as a limit and the definition $e^x = \lim_{n\to\infty}(1+x/n)^n$, without L'Hôpital's rule","The derivative of  using the definition of derivative as a limit and the definition , without L'Hôpital's rule",e^x e^x = \lim_{n\to\infty}(1+x/n)^n,"Let's define  $$ e^x := \lim_{n\to\infty}\left(1+\frac{x} {n}\right)^n, \forall x\in\Bbb R $$ and $$ \frac{d} {dx} f(x) := \lim_{\Delta x\to0} \frac{f(x+\Delta x) - f(x)} {\Delta x} $$ Prove that $$ \frac{d} {dx} e^x = e^x $$ using the definition of $e^x$ and derivation above, without using L'Hôpital's rule or the "" logarithm trick "" and/or the ""inverse function derivative trick"". $$ \left( \frac{d} {dx} f^{-1}(x)= \frac{1} {\left(\frac{d}{d(f^{-1}(x))}f(x)\right)(f^{-1}(x))}\right) $$ Or equivalently prove that the following two definiton of $e$ are identical $$ 1)\space\space\space\space e =\lim_{n\to\infty}(1+\frac{1} {n})^n $$ $$ 2) \space\space\space\space e\in\Bbb R,\space\space(\frac{d} {dx} e^x)(x=0) = 1   $$ What I've got is $$ \frac{d}{dx}e^x=e^x \lim_{\Delta x\to0}\frac{e^{\Delta x} - 1} {\Delta x} = e^x \lim_{\Delta x\to0}\lim_{n\to\infty}\frac{\left(1+\frac{\Delta x}{n}\right)^{n}-1}{\Delta x} =  e^x \lim_{\Delta x\to0}\frac{e^{0+\Delta x}-e^0}{\Delta x}  $$ If i assume that $n\in\Bbb N$ I could use binomial theorem but I didn't got much out of it. Wolframalpha just uses L'Hospital rule to solve it, but I am looking for an elementary solution. What I'm interested in is basically is the equivalence of the two definition of $e$ mentioned above. And I'd like to get a direct proof rather than an indirect(I mean which involves logarithms or the derivatives of invers functions). I look forward getting your aswers.","Let's define  $$ e^x := \lim_{n\to\infty}\left(1+\frac{x} {n}\right)^n, \forall x\in\Bbb R $$ and $$ \frac{d} {dx} f(x) := \lim_{\Delta x\to0} \frac{f(x+\Delta x) - f(x)} {\Delta x} $$ Prove that $$ \frac{d} {dx} e^x = e^x $$ using the definition of $e^x$ and derivation above, without using L'Hôpital's rule or the "" logarithm trick "" and/or the ""inverse function derivative trick"". $$ \left( \frac{d} {dx} f^{-1}(x)= \frac{1} {\left(\frac{d}{d(f^{-1}(x))}f(x)\right)(f^{-1}(x))}\right) $$ Or equivalently prove that the following two definiton of $e$ are identical $$ 1)\space\space\space\space e =\lim_{n\to\infty}(1+\frac{1} {n})^n $$ $$ 2) \space\space\space\space e\in\Bbb R,\space\space(\frac{d} {dx} e^x)(x=0) = 1   $$ What I've got is $$ \frac{d}{dx}e^x=e^x \lim_{\Delta x\to0}\frac{e^{\Delta x} - 1} {\Delta x} = e^x \lim_{\Delta x\to0}\lim_{n\to\infty}\frac{\left(1+\frac{\Delta x}{n}\right)^{n}-1}{\Delta x} =  e^x \lim_{\Delta x\to0}\frac{e^{0+\Delta x}-e^0}{\Delta x}  $$ If i assume that $n\in\Bbb N$ I could use binomial theorem but I didn't got much out of it. Wolframalpha just uses L'Hospital rule to solve it, but I am looking for an elementary solution. What I'm interested in is basically is the equivalence of the two definition of $e$ mentioned above. And I'd like to get a direct proof rather than an indirect(I mean which involves logarithms or the derivatives of invers functions). I look forward getting your aswers.",,"['calculus', 'real-analysis', 'derivatives', 'definition']"
29,Combinatorial proof: $e^x=\sum\limits_{k=0}^{\infty}\frac{x^k}{k!}$,Combinatorial proof:,e^x=\sum\limits_{k=0}^{\infty}\frac{x^k}{k!},"Using notion of derivative of functions from Taylor formula follow that $$e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ Is there any elementary combinatorial proof of this formula here is my proof for $x$ natural number Denote by $P_k^m$ number of $k$-permutations with unlimited repetitions of elements from a $m$-set then we can prove that $$P_k^m=\sum_{r_0+r_1+...+r_{m-1}=k}\frac{k!}{r_0 !...r_{m-1}!}$$ also is valid $$P_k^m=m^k$$ Based on first formula we can derive that $$\sum_{k=0}^{\infty}P_k^m\frac{x^k}{k!}=\left(\sum_{k=0}^{\infty}\frac{x^k}{k!}\right)^m$$ from second formula $$\sum_{k=0}^{\infty}P_k^m\frac{x^k}{k!}=\sum_{k=0}^{\infty}\frac{(mx)^k}{k!}$$ now is clear that  $$\sum_{k=0}^{\infty}\frac{(mx)^k}{k!}=\left(\sum_{k=0}^{\infty}\frac{x^k}{k!}\right)^m$$ from last equation for $x=1$ taking in account that $$\sum_{k=0}^{\infty}\frac{1}{k!}=e=2,71828...$$ we have finally that for natural number $m$ is valid formula $$e^m=\sum_{k=0}^{\infty}\frac{m^k}{k!}$$","Using notion of derivative of functions from Taylor formula follow that $$e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ Is there any elementary combinatorial proof of this formula here is my proof for $x$ natural number Denote by $P_k^m$ number of $k$-permutations with unlimited repetitions of elements from a $m$-set then we can prove that $$P_k^m=\sum_{r_0+r_1+...+r_{m-1}=k}\frac{k!}{r_0 !...r_{m-1}!}$$ also is valid $$P_k^m=m^k$$ Based on first formula we can derive that $$\sum_{k=0}^{\infty}P_k^m\frac{x^k}{k!}=\left(\sum_{k=0}^{\infty}\frac{x^k}{k!}\right)^m$$ from second formula $$\sum_{k=0}^{\infty}P_k^m\frac{x^k}{k!}=\sum_{k=0}^{\infty}\frac{(mx)^k}{k!}$$ now is clear that  $$\sum_{k=0}^{\infty}\frac{(mx)^k}{k!}=\left(\sum_{k=0}^{\infty}\frac{x^k}{k!}\right)^m$$ from last equation for $x=1$ taking in account that $$\sum_{k=0}^{\infty}\frac{1}{k!}=e=2,71828...$$ we have finally that for natural number $m$ is valid formula $$e^m=\sum_{k=0}^{\infty}\frac{m^k}{k!}$$",,"['real-analysis', 'combinatorics', 'analysis', 'exponential-function', 'combinatorial-proofs']"
30,Why is it true that every finite-dimensional inner product space is a Hilbert space?,Why is it true that every finite-dimensional inner product space is a Hilbert space?,,"A Hilbert space is, by definition, a complete inner product space. If $(V,|.|)$ is finite dimensional inner product space of dimension $n$ then it is (topologically) isomorphic to $\mathbb{R}^n$ which is of course complete. My instinct here is to say ""and therefore, $V$ is also a Hilbert space"". I'm not sure about this step however since completeness depends on the norm and the norm depends on the selected inner product (assuming the induced norm $||x|| = (x | x)^{1/2}$ is used). So, must we place some condition on the inner product or is the topological isomorphism between $\mathbb{R}^n$ enough to guarantee that V is Hilbert?","A Hilbert space is, by definition, a complete inner product space. If $(V,|.|)$ is finite dimensional inner product space of dimension $n$ then it is (topologically) isomorphic to $\mathbb{R}^n$ which is of course complete. My instinct here is to say ""and therefore, $V$ is also a Hilbert space"". I'm not sure about this step however since completeness depends on the norm and the norm depends on the selected inner product (assuming the induced norm $||x|| = (x | x)^{1/2}$ is used). So, must we place some condition on the inner product or is the topological isomorphism between $\mathbb{R}^n$ enough to guarantee that V is Hilbert?",,"['real-analysis', 'functional-analysis']"
31,How to learn without looking at solutions? (real analysis),How to learn without looking at solutions? (real analysis),,"I have 2 weeks to do real analysis HW set, I work on them everyday, but many questions I spend hours on and cannot figure out. In the end, I google them (and feel horrible), read the proofs, and think there was no way I could've come up with that proof. I know I need to not google solutions to learn, but I can't learn without googling solutions. What to do? Or at what point is googling ok? And in an ideal world I'd spend all the time in the world on a question, but reality is my hours are limited in college. I literally count the hours I have each day, and spend them accordingly, and at some point not googling a solution is simply shooting myself in the foot for other classes.","I have 2 weeks to do real analysis HW set, I work on them everyday, but many questions I spend hours on and cannot figure out. In the end, I google them (and feel horrible), read the proofs, and think there was no way I could've come up with that proof. I know I need to not google solutions to learn, but I can't learn without googling solutions. What to do? Or at what point is googling ok? And in an ideal world I'd spend all the time in the world on a question, but reality is my hours are limited in college. I literally count the hours I have each day, and spend them accordingly, and at some point not googling a solution is simply shooting myself in the foot for other classes.",,"['real-analysis', 'advice']"
32,Lebesgue density theorem in the line,Lebesgue density theorem in the line,,"Suppose $A \subseteq \mathbb{R} $, $m(A) > 0 $. Then for almost all $x \in A $ we have $$ \lim_{\epsilon \to 0^+ } \frac{ m(A \cap (x - \epsilon, x + \epsilon))}{2 \epsilon} = 1.$$ Can someone help me with this problem? $m$ is the Lebesgue measure thanks","Suppose $A \subseteq \mathbb{R} $, $m(A) > 0 $. Then for almost all $x \in A $ we have $$ \lim_{\epsilon \to 0^+ } \frac{ m(A \cap (x - \epsilon, x + \epsilon))}{2 \epsilon} = 1.$$ Can someone help me with this problem? $m$ is the Lebesgue measure thanks",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
33,Function grows slower than $\ln(x)$,Function grows slower than,\ln(x),What function grows slower than $\ln(x)$ as $x \rightarrow\infty$ ? How am I supposed to find it besides just trying finding limits of all known functions? I am looking for functions that are unbounded and strictly increasing for real $x>0$ and grow slower than $\ln(x)$,What function grows slower than as ? How am I supposed to find it besides just trying finding limits of all known functions? I am looking for functions that are unbounded and strictly increasing for real and grow slower than,\ln(x) x \rightarrow\infty x>0 \ln(x),"['calculus', 'real-analysis', 'limits']"
34,"Is $\int\limits_0^\infty\frac{\sin y}{y^{s+1}}dy=-\Gamma(-s)\sin(\frac{\pi s}{2})$ for $\operatorname{Re}(s)\in (-1,0)$ obvious?",Is  for  obvious?,"\int\limits_0^\infty\frac{\sin y}{y^{s+1}}dy=-\Gamma(-s)\sin(\frac{\pi s}{2}) \operatorname{Re}(s)\in (-1,0)","This is a part of computation in Titchmash, Theories of Zeta Functions which I do not find obvious but there is no explanation. I did figure out the computation. $$\int_0^\infty\frac{\sin(y)}{y^{s+1}}dy=-\Gamma(-s)\sin\left(\frac{\pi s}{2}\right)$$ Q: There is no explanation in the book for this step. Why is this obvious without explanation? My recipe goes as the following. It suffices to restrict to real axis part with $s\in (-1,0)$ region. Now integral is real valued in this region. Here I need $\Gamma(-s)=\frac{\Gamma(-s+1)}{s}$ extension to obtain real valuedness. Consider the integral as the imaginary part of $\int_0^{i\infty} \frac{e^{z}}{i^s z^{s+1}}dz$ where I have already rotated axis by $i$ multiplication. Now to obtain $\Gamma$ function, close contour from $(+\infty,0)$ axis portion and connect to $(0,i\infty)$ portion. Then close the contour by arc. The arc contour contribution is $0$ via exponential suppresion. Then apply residue theorem easily as the whole thing is holomorphic by $s\in (-1,0)$ region. Hence equality follows. This is not $1-2$ line naive computation though not hard. However, it did take me a while to figure out.","This is a part of computation in Titchmash, Theories of Zeta Functions which I do not find obvious but there is no explanation. I did figure out the computation. Q: There is no explanation in the book for this step. Why is this obvious without explanation? My recipe goes as the following. It suffices to restrict to real axis part with region. Now integral is real valued in this region. Here I need extension to obtain real valuedness. Consider the integral as the imaginary part of where I have already rotated axis by multiplication. Now to obtain function, close contour from axis portion and connect to portion. Then close the contour by arc. The arc contour contribution is via exponential suppresion. Then apply residue theorem easily as the whole thing is holomorphic by region. Hence equality follows. This is not line naive computation though not hard. However, it did take me a while to figure out.","\int_0^\infty\frac{\sin(y)}{y^{s+1}}dy=-\Gamma(-s)\sin\left(\frac{\pi s}{2}\right) s\in (-1,0) \Gamma(-s)=\frac{\Gamma(-s+1)}{s} \int_0^{i\infty} \frac{e^{z}}{i^s z^{s+1}}dz i \Gamma (+\infty,0) (0,i\infty) 0 s\in (-1,0) 1-2","['real-analysis', 'complex-analysis', 'number-theory', 'analysis']"
35,prove that any finite set in a metric space is compact,prove that any finite set in a metric space is compact,,Prove that any finite set in a metric space is compact It is obvious that any finite set is bounded. I don't understand if any finite set is closed? And is it ok to say any finite set is compact because they are bounded and closed?,Prove that any finite set in a metric space is compact It is obvious that any finite set is bounded. I don't understand if any finite set is closed? And is it ok to say any finite set is compact because they are bounded and closed?,,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
36,Why a non-diagonalizable matrix can be approximated by an infinite sequence of diagonalizable matrices?,Why a non-diagonalizable matrix can be approximated by an infinite sequence of diagonalizable matrices?,,"It is known that any non-diagonalizable matrix, $A$, can be approximated by a set of diagonalizable matrices, e.g. $A \simeq \lim_{k \rightarrow \infty} A_k$. Why this is true? Note: I was faced with it for the first time at a note about a simple proof for Cayley-Hamilton theorem, but I was not able to find it in my books nor in the internet to the extent I've googled.","It is known that any non-diagonalizable matrix, $A$, can be approximated by a set of diagonalizable matrices, e.g. $A \simeq \lim_{k \rightarrow \infty} A_k$. Why this is true? Note: I was faced with it for the first time at a note about a simple proof for Cayley-Hamilton theorem, but I was not able to find it in my books nor in the internet to the extent I've googled.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'vector-spaces', 'diagonalization']"
37,Pointwise limit of integrable function,Pointwise limit of integrable function,,"Question: Show that the pointwise limit of integrable functions is not necessarily integrable. I am stuck on this question. Here is what I know. Let $(f_n)^{\infty}_{n=1}$ be a series of integrable functions, and let $$\lim_{n \to \infty} f_n(x)=Z$$ I need to show that $Z$ is not necessarily integrable. Should I be looking for a specific example? A function that is integrable, but as $n \to \infty $ the function is no longer integrable.","Question: Show that the pointwise limit of integrable functions is not necessarily integrable. I am stuck on this question. Here is what I know. Let $(f_n)^{\infty}_{n=1}$ be a series of integrable functions, and let $$\lim_{n \to \infty} f_n(x)=Z$$ I need to show that $Z$ is not necessarily integrable. Should I be looking for a specific example? A function that is integrable, but as $n \to \infty $ the function is no longer integrable.",,['real-analysis']
38,Compute the series : $\sum_{n=1}^{\infty} \frac{4^n n!}{(2n)!}$,Compute the series :,\sum_{n=1}^{\infty} \frac{4^n n!}{(2n)!},How would you compute the following series? I'm interested in some easy approaches that would allow me to work it out. $$\sum_{n=1}^{\infty} \frac{4^n n!}{(2n)!}$$,How would you compute the following series? I'm interested in some easy approaches that would allow me to work it out. $$\sum_{n=1}^{\infty} \frac{4^n n!}{(2n)!}$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
39,How do we prove that $\lfloor0.999\cdots\rfloor = \lfloor 1 \rfloor$?,How do we prove that ?,\lfloor0.999\cdots\rfloor = \lfloor 1 \rfloor,"Are the floor functions of $0.999\cdots$ and 1 equal? It is true that $0.999\cdots=1$ but how does one justifies the integer part of $0.999\cdots$ being 1 , where it is not, or alternatively without using $0.999\cdots=1$ how can we show that  $\lfloor0.999\cdots\rfloor = \lfloor 1 \rfloor$ ?","Are the floor functions of $0.999\cdots$ and 1 equal? It is true that $0.999\cdots=1$ but how does one justifies the integer part of $0.999\cdots$ being 1 , where it is not, or alternatively without using $0.999\cdots=1$ how can we show that  $\lfloor0.999\cdots\rfloor = \lfloor 1 \rfloor$ ?",,"['real-analysis', 'examples-counterexamples', 'ceiling-and-floor-functions', 'decimal-expansion']"
40,"If f is integrable, is it finite almost everywhere?","If f is integrable, is it finite almost everywhere?",,"If $\int_\Omega f d\mu<\infty$, and $f$ is non-negative, can we conclude that $f$ is finite a.e. on $\Omega$? Is being finite a.e. the same as having a finite essential supremum, i.e. there exists an $M$ such that $f\leq M$ a.e.? I know that $f$ integrable does not mean that $f$ has a finite essential supremum, from the counter example $f=1/\sqrt x$ on $(0,1]$. Thanks for help.","If $\int_\Omega f d\mu<\infty$, and $f$ is non-negative, can we conclude that $f$ is finite a.e. on $\Omega$? Is being finite a.e. the same as having a finite essential supremum, i.e. there exists an $M$ such that $f\leq M$ a.e.? I know that $f$ integrable does not mean that $f$ has a finite essential supremum, from the counter example $f=1/\sqrt x$ on $(0,1]$. Thanks for help.",,"['real-analysis', 'measure-theory']"
41,"How can I show that $\sup(AB)\geq\sup A\sup B$ for $A,B\subset\mathbb{R}$ where $A\cup B$ is positive and bounded?",How can I show that  for  where  is positive and bounded?,"\sup(AB)\geq\sup A\sup B A,B\subset\mathbb{R} A\cup B","The question is based on the following exercise in real analysis: Assume that $A,B\subset{\Bbb R}$ are both bounded and $x>0$ for all $x\in A\cup B$. Show that $$ \sup(AB)=\sup A\sup B $$   where    $$ AB:=\{ab\in{\Bbb R}:a\in A, b\in B\}. $$ Since $0<a\leq\sup A$ and $0<b\leq\sup B$ for all $a\in A$ and $b\in B$, we have $$ ab\leq\sup A\sup B $$ for all $ab\in AB$ which implies that $\sup AB\leq\sup A\sup B$. I have trouble with another direction: $$ \sup AB\geq\sup A\sup B $$ I was trying to show that for every $\epsilon >0$, $\sup AB-\epsilon \geq \sup A\sup B$. If one uses the definition of supremum, one has the estimates that for every $\epsilon>0$, $$ \sup A-\epsilon\leq a, \quad \sup B-\epsilon\leq b $$ for some $a\in A,\ b\in B$. It follows that $$ \sup A\sup B\leq (a+\epsilon)(b+\epsilon)=ab+\epsilon(a+b)+\epsilon^2\leq \sup AB+\epsilon (a+b)+\epsilon^2 $$ which seems quite close to what I want. How can I go on?","The question is based on the following exercise in real analysis: Assume that $A,B\subset{\Bbb R}$ are both bounded and $x>0$ for all $x\in A\cup B$. Show that $$ \sup(AB)=\sup A\sup B $$   where    $$ AB:=\{ab\in{\Bbb R}:a\in A, b\in B\}. $$ Since $0<a\leq\sup A$ and $0<b\leq\sup B$ for all $a\in A$ and $b\in B$, we have $$ ab\leq\sup A\sup B $$ for all $ab\in AB$ which implies that $\sup AB\leq\sup A\sup B$. I have trouble with another direction: $$ \sup AB\geq\sup A\sup B $$ I was trying to show that for every $\epsilon >0$, $\sup AB-\epsilon \geq \sup A\sup B$. If one uses the definition of supremum, one has the estimates that for every $\epsilon>0$, $$ \sup A-\epsilon\leq a, \quad \sup B-\epsilon\leq b $$ for some $a\in A,\ b\in B$. It follows that $$ \sup A\sup B\leq (a+\epsilon)(b+\epsilon)=ab+\epsilon(a+b)+\epsilon^2\leq \sup AB+\epsilon (a+b)+\epsilon^2 $$ which seems quite close to what I want. How can I go on?",,[]
42,"Proof that rational functions are an ordered field, but non-archimedean - Bartle's elements of real analysis","Proof that rational functions are an ordered field, but non-archimedean - Bartle's elements of real analysis",,"I read that the set of rational functions with rational coefficients forms an ordered field, yet it is non-archimedean. I tried googling this, but I don't think I understood the solution. How does one define an order on rational functions of the form $\mathbb{Q}(x)=p(x)/q(x)$ ? How do you show that $\mathbb{Q}(x)$ is non-archimedean? That there is no natural number $n$ , such that $n>\mathbb{Q}(x)$ ? Do I substitute numerical values of $x$ and show that $\mathbb{Q}(x)$ is unbounded or something?","I read that the set of rational functions with rational coefficients forms an ordered field, yet it is non-archimedean. I tried googling this, but I don't think I understood the solution. How does one define an order on rational functions of the form ? How do you show that is non-archimedean? That there is no natural number , such that ? Do I substitute numerical values of and show that is unbounded or something?",\mathbb{Q}(x)=p(x)/q(x) \mathbb{Q}(x) n n>\mathbb{Q}(x) x \mathbb{Q}(x),"['real-analysis', 'rational-functions', 'ordered-fields']"
43,Explaining Green's Theorem for Undergraduates,Explaining Green's Theorem for Undergraduates,,"I taught (undergraduates) the theory of Riemann integration with motivation from the notion of ""area"" and as an application, proved how it really represents area, by computing area of circle, rectangle. Then students also got interest in the ""Theory"" of integration. Next, I will teach Green's theorem. But almost all the text-books on analysis/calculus give a detailed description ""proof"" of this theorem, but no motivation. If I gave it (proof) to read to an undergraduate, he would not enjoy it beyond computations. What could be a good motivation, or any simple problem motivating towards Green's theorem, which would create interest (in theory) for undergraduates?","I taught (undergraduates) the theory of Riemann integration with motivation from the notion of ""area"" and as an application, proved how it really represents area, by computing area of circle, rectangle. Then students also got interest in the ""Theory"" of integration. Next, I will teach Green's theorem. But almost all the text-books on analysis/calculus give a detailed description ""proof"" of this theorem, but no motivation. If I gave it (proof) to read to an undergraduate, he would not enjoy it beyond computations. What could be a good motivation, or any simple problem motivating towards Green's theorem, which would create interest (in theory) for undergraduates?",,"['calculus', 'real-analysis', 'soft-question']"
44,Can infinity be a supremum? Can it be a maximum?,Can infinity be a supremum? Can it be a maximum?,,"If you consider all the real numbers, is infinity the supremum? What about the maximum? I know the supremum does not have to be in the set and the maximum does, but I'm confused as to how to answer these questions. Are the real numbers still technically bounded above even though they go on forever?","If you consider all the real numbers, is infinity the supremum? What about the maximum? I know the supremum does not have to be in the set and the maximum does, but I'm confused as to how to answer these questions. Are the real numbers still technically bounded above even though they go on forever?",,['real-analysis']
45,"Evaluating the reception of (epsilon, delta) definitions","Evaluating the reception of (epsilon, delta) definitions",,"There is much discussion both in the education community and the mathematics community concerning the challenge of (epsilon, delta) type definitions in real analysis and the student reception of it. My impression has been that the mathematical community often holds an upbeat opinion on the success of student reception of this, whereas the education community often stresses difficulties and their ""baffling"" and ""inhibitive"" effect (see below). A typical educational perspective on this was recently expressed by Paul Dawkins in the following terms: 2.3. Student difficulties with real analysis definitions. The concepts of limit and continuity have posed well-documented difficulties for students both at the calculus and analysis level of instructions (e.g. Cornu, 1991; Cottrill et al., 1996; Ferrini-Mundy & Graham, 1994; Tall & Vinner, 1981; Williams, 1991). Researchers identified difficulties stemming from a number of issues: the language of limits (Cornu, 1991; Williams, 1991), multiple quantification in the formal definition (Dubinsky, Elderman, & Gong, 1988; Dubinsky & Yiparaki, 2000; Swinyard & Lockwood, 2007), implicit dependencies among quantities in the definition (Roh & Lee, 2011a, 2011b), and persistent notions pertaining to the existence of infinitesimal quantities (Ely, 2010). Limits and continuity are often couched as formalizations of approaching and connectedness respectively. However, the standard, formal definitions display much more subtlety and complexity. That complexity often baffles students who cannot perceive the necessity for so many moving parts. Thus learning the concepts and formal definitions in real analysis are fraught both with need to acquire proficiency with conceptual tools such as quantification and to help students perceive conceptual necessity for these tools. This means students often cannot coordinate their concept image with the concept definition, inhibiting their acculturation to advanced mathematical practice, which emphasizes concept definitions. See http://dx.doi.org/10.1016/j.jmathb.2013.10.002 for the entire article (note that the online article provides links to the papers cited above). To summarize, in the field of education, researchers decidedly have not come to the conclusion that epsilon, delta definitions are either ""simple"", ""clear"", or ""common sense"". Meanwhile, mathematicians often express contrary sentiments. Two examples are given below. ...one cannot teach the concept of limit without using the epsilon-delta definition. Teaching such ideas intuitively does not make it easier for the student it makes it harder to understand. Bertrand Russell has called the rigorous definition of limit and convergence the greatest achievement of the human intellect in 2000 years! The Greeks were puzzled by paradoxes involving motion; now they all become clear, because we have complete understanding of limits and convergence. Without the proper definition, things are difficult. With the definition, they are simple and clear. (see Kleinfeld, Margaret; Calculus: Reformed or Deformed? Amer. Math. Monthly 103 (1996), no. 3, 230-232.) I always tell my calculus students that mathematics is not esoteric: It is common sense. (Even the notorious epsilon, delta definition of limit is common sense, and moreover is central to the important practical problems of approximation and estimation.) (see Bishop, Errett; Book Review: Elementary calculus. Bull. Amer. Math. Soc. 83 (1977), no. 2, 205--208.) When one compares the upbeat assessment common in the mathematics community and the somber assessments common in the education community, sometimes one wonders whether they are talking about the same thing. How does one bridge the gap between the two assessments? Are they perhaps dealing with distinct student populations? Are there perhaps education studies providing more upbeat assessments than Dawkins' article would suggest? Note 1. See also https://mathoverflow.net/questions/158145/assessing-effectiveness-of-epsilon-delta-definitions Note 2. Two approaches have been proposed to account for this difference of perception between the education community and the math community: (a) sample bias: mathematicians tend to base their appraisal of the effectiveness of these definitions in terms of the most active students in their classes, which are often the best students; (b) student/professor gap: mathematicians base their appraisal on their own scientific appreciation of these definitions as the ""right"" ones, arrived at after a considerable investment of time and removed from the original experience of actually learning those definitions.  Both of these sound plausible, but it would be instructive to have field research in support of these approaches. We recently published an article reporting the result of student polling concerning the comparative educational merits of epsilon-delta definitions and infinitesimal definitions of key concepts like continuity and convergence, with students favoring the infinitesimal definitions by large margins.","There is much discussion both in the education community and the mathematics community concerning the challenge of (epsilon, delta) type definitions in real analysis and the student reception of it. My impression has been that the mathematical community often holds an upbeat opinion on the success of student reception of this, whereas the education community often stresses difficulties and their ""baffling"" and ""inhibitive"" effect (see below). A typical educational perspective on this was recently expressed by Paul Dawkins in the following terms: 2.3. Student difficulties with real analysis definitions. The concepts of limit and continuity have posed well-documented difficulties for students both at the calculus and analysis level of instructions (e.g. Cornu, 1991; Cottrill et al., 1996; Ferrini-Mundy & Graham, 1994; Tall & Vinner, 1981; Williams, 1991). Researchers identified difficulties stemming from a number of issues: the language of limits (Cornu, 1991; Williams, 1991), multiple quantification in the formal definition (Dubinsky, Elderman, & Gong, 1988; Dubinsky & Yiparaki, 2000; Swinyard & Lockwood, 2007), implicit dependencies among quantities in the definition (Roh & Lee, 2011a, 2011b), and persistent notions pertaining to the existence of infinitesimal quantities (Ely, 2010). Limits and continuity are often couched as formalizations of approaching and connectedness respectively. However, the standard, formal definitions display much more subtlety and complexity. That complexity often baffles students who cannot perceive the necessity for so many moving parts. Thus learning the concepts and formal definitions in real analysis are fraught both with need to acquire proficiency with conceptual tools such as quantification and to help students perceive conceptual necessity for these tools. This means students often cannot coordinate their concept image with the concept definition, inhibiting their acculturation to advanced mathematical practice, which emphasizes concept definitions. See http://dx.doi.org/10.1016/j.jmathb.2013.10.002 for the entire article (note that the online article provides links to the papers cited above). To summarize, in the field of education, researchers decidedly have not come to the conclusion that epsilon, delta definitions are either ""simple"", ""clear"", or ""common sense"". Meanwhile, mathematicians often express contrary sentiments. Two examples are given below. ...one cannot teach the concept of limit without using the epsilon-delta definition. Teaching such ideas intuitively does not make it easier for the student it makes it harder to understand. Bertrand Russell has called the rigorous definition of limit and convergence the greatest achievement of the human intellect in 2000 years! The Greeks were puzzled by paradoxes involving motion; now they all become clear, because we have complete understanding of limits and convergence. Without the proper definition, things are difficult. With the definition, they are simple and clear. (see Kleinfeld, Margaret; Calculus: Reformed or Deformed? Amer. Math. Monthly 103 (1996), no. 3, 230-232.) I always tell my calculus students that mathematics is not esoteric: It is common sense. (Even the notorious epsilon, delta definition of limit is common sense, and moreover is central to the important practical problems of approximation and estimation.) (see Bishop, Errett; Book Review: Elementary calculus. Bull. Amer. Math. Soc. 83 (1977), no. 2, 205--208.) When one compares the upbeat assessment common in the mathematics community and the somber assessments common in the education community, sometimes one wonders whether they are talking about the same thing. How does one bridge the gap between the two assessments? Are they perhaps dealing with distinct student populations? Are there perhaps education studies providing more upbeat assessments than Dawkins' article would suggest? Note 1. See also https://mathoverflow.net/questions/158145/assessing-effectiveness-of-epsilon-delta-definitions Note 2. Two approaches have been proposed to account for this difference of perception between the education community and the math community: (a) sample bias: mathematicians tend to base their appraisal of the effectiveness of these definitions in terms of the most active students in their classes, which are often the best students; (b) student/professor gap: mathematicians base their appraisal on their own scientific appreciation of these definitions as the ""right"" ones, arrived at after a considerable investment of time and removed from the original experience of actually learning those definitions.  Both of these sound plausible, but it would be instructive to have field research in support of these approaches. We recently published an article reporting the result of student polling concerning the comparative educational merits of epsilon-delta definitions and infinitesimal definitions of key concepts like continuity and convergence, with students favoring the infinitesimal definitions by large margins.",,"['calculus', 'real-analysis', 'limits', 'education', 'math-history']"
46,Geometric proof of existence of irrational numbers.,Geometric proof of existence of irrational numbers.,,"It is easy, using only straightedge and compass, to construct irrational lengths, is there a way to prove, using only straightedge and compass, that there are constructible lengths which are irrational? Ie a geometric proof. And is it possible to construct an (unending) sequence of rational lengths or areas, such that they can get arbitrarily close to the area or circumference of a circle? If not, then does this provide evidence that the real numbers are not sufficiently refined to capture exactly the circumference or area of an idealized circle? (The idea being that the reals can be constructed from equivalence classes of infinite sequences of rationals, so if the circumference cant be approached arbitrarily by rationals then its not necessarily a real number)","It is easy, using only straightedge and compass, to construct irrational lengths, is there a way to prove, using only straightedge and compass, that there are constructible lengths which are irrational? Ie a geometric proof. And is it possible to construct an (unending) sequence of rational lengths or areas, such that they can get arbitrarily close to the area or circumference of a circle? If not, then does this provide evidence that the real numbers are not sufficiently refined to capture exactly the circumference or area of an idealized circle? (The idea being that the reals can be constructed from equivalence classes of infinite sequences of rationals, so if the circumference cant be approached arbitrarily by rationals then its not necessarily a real number)",,"['real-analysis', 'pi', 'geometric-construction', 'meta-math']"
47,"For an infinite sequence of real numbers that monotonically grows, is infinity considered a limit?","For an infinite sequence of real numbers that monotonically grows, is infinity considered a limit?",,"My question is on the topic of ""infinity"" as a limit. Does a sequence like $1,2,3,4,5,6,7,\ldots$ have a limit of infinity, or is it considered as not having a limit. If infinity is considered a limit, is this sequence considered convergent then? My reason for this question is to further distinct these two sequences: $1,2,1,3,1,4,1,5,1,6,1,\ldots$ and $1,2,4,3,7,4,10,5,\ldots$ As per my last question from a previous post My professor stated that the first sequence is considered as ""not going to infinity"", while the second one does. He hasn't given a formal definition of what that means, but I'm going to assume that means $\lim a_n = \infty$ and $\lim a_n$ does not exist. If infinity is considered a limit point, would that make the second sequence convergent, converging to infinity, while the first sequence as being divergent, with the limit not existing? In that case, wouldn't this clash with some definitions? In a wikipedia article, it says ""Zaporedje $2, 4, 6, 8, 10,\cdots$ je divergentno in ima nepravo stekališče neskončno"" which in translation means ""The sequence $2, 4, 6, 8,\cdots$ is divergent and it has the false limit point of infinity"" But as previously stated, this form of sequence would have indeed a limit, of infinity, and thus would be convergent?","My question is on the topic of ""infinity"" as a limit. Does a sequence like have a limit of infinity, or is it considered as not having a limit. If infinity is considered a limit, is this sequence considered convergent then? My reason for this question is to further distinct these two sequences: and As per my last question from a previous post My professor stated that the first sequence is considered as ""not going to infinity"", while the second one does. He hasn't given a formal definition of what that means, but I'm going to assume that means and does not exist. If infinity is considered a limit point, would that make the second sequence convergent, converging to infinity, while the first sequence as being divergent, with the limit not existing? In that case, wouldn't this clash with some definitions? In a wikipedia article, it says ""Zaporedje je divergentno in ima nepravo stekališče neskončno"" which in translation means ""The sequence is divergent and it has the false limit point of infinity"" But as previously stated, this form of sequence would have indeed a limit, of infinity, and thus would be convergent?","1,2,3,4,5,6,7,\ldots 1,2,1,3,1,4,1,5,1,6,1,\ldots 1,2,4,3,7,4,10,5,\ldots \lim a_n = \infty \lim a_n 2, 4, 6, 8, 10,\cdots 2, 4, 6, 8,\cdots","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'metric-spaces']"
48,"Asymptotic expansion of $\int_0^{+\infty} \frac{ne^{-\sqrt{t}}}{1+n^2t^2}\,dt$",Asymptotic expansion of,"\int_0^{+\infty} \frac{ne^{-\sqrt{t}}}{1+n^2t^2}\,dt","I would like to get a general formula to get the asymptotic expansion at the order $n$ (so at whatever precision I want) of the following integral : $$I = \int_0^{+\infty} \frac{ne^{-\sqrt{t}}}{1+n^2t^2} \mathrm{d}t$$ Some thoughts : For now I am able to find an asymptotic expansion of order $1$ . First intuitively we have : $$I = O \left ( \frac{1}{n} \right )$$ From the expression : $$\frac{ne^{-\sqrt{t}}}{1+n^2t^2}$$ The change of variable : $\tan(\theta) = nt$ suggests itself and the problem boils down to calculating an asymptotic expansion of : $$\int_0^{\pi/2} e^{-\sqrt{\frac{\tan(\theta)}{n}}} \mathrm{d}\theta$$ Now it is possible to use the dominated convergence theorem because we have : $$e^{-\sqrt{\frac{\tan(\theta)}{n}}} \leq 1$$ Yet there is still a problem since $I$ is an improper and integral, so by using the dominated convergence theorem we only get an asymptotic expansion and not the exact value. That’s why I get the following asymptotic (of order $1$ ): $$\frac{\pi}{2} + O \left ( \frac{1}{n} \right )$$ Now is it possible to expand this in order to get a general formula for the asymptotic expansion of $I$ at whatever order we want ?","I would like to get a general formula to get the asymptotic expansion at the order (so at whatever precision I want) of the following integral : Some thoughts : For now I am able to find an asymptotic expansion of order . First intuitively we have : From the expression : The change of variable : suggests itself and the problem boils down to calculating an asymptotic expansion of : Now it is possible to use the dominated convergence theorem because we have : Yet there is still a problem since is an improper and integral, so by using the dominated convergence theorem we only get an asymptotic expansion and not the exact value. That’s why I get the following asymptotic (of order ): Now is it possible to expand this in order to get a general formula for the asymptotic expansion of at whatever order we want ?",n I = \int_0^{+\infty} \frac{ne^{-\sqrt{t}}}{1+n^2t^2} \mathrm{d}t 1 I = O \left ( \frac{1}{n} \right ) \frac{ne^{-\sqrt{t}}}{1+n^2t^2} \tan(\theta) = nt \int_0^{\pi/2} e^{-\sqrt{\frac{\tan(\theta)}{n}}} \mathrm{d}\theta e^{-\sqrt{\frac{\tan(\theta)}{n}}} \leq 1 I 1 \frac{\pi}{2} + O \left ( \frac{1}{n} \right ) I,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'asymptotics']"
49,How can I pick up analysis quickly? [closed],How can I pick up analysis quickly? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 6 months ago . Improve this question I have a 2-3 week recess from university for winter break. In this time, I would like to learn analysis, starting with Walter Rudin's Principles of Mathematical Analysis , and then, if at all possible, continuing with Walter Rudin's Real and Complex Analysis . If necessary, I would be willing to complete the second book after returning to college (that is, outside of the 2-3 week time frame). A few questions come to mind: How reasonable are these goals? My background in maths is an elementary Moore method single-variable calculus course, and the beginning of (undergraduate) introductory real analysis. However, most of my time during the break will be available for mathematics. Is only the goal of completing the first book reasonable, with the second book requiring additional time? Is Principles of Mathematical Analysis sufficient for reading Real and Complex Analysis ? If not, what else should I know? What advice can you give me? I'm reading these primarily for entertainment, and I hope with this to learn enough mathematics to do interesting things. (I am a maths student in college, but have just started undergraduate analysis. My courses do not use either Rudin book.) This does not need to be advice on the books themselves, perhaps it could be advice on how to learn math quickly (and properly) if one has sufficient time to think about it exclusively.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 6 months ago . Improve this question I have a 2-3 week recess from university for winter break. In this time, I would like to learn analysis, starting with Walter Rudin's Principles of Mathematical Analysis , and then, if at all possible, continuing with Walter Rudin's Real and Complex Analysis . If necessary, I would be willing to complete the second book after returning to college (that is, outside of the 2-3 week time frame). A few questions come to mind: How reasonable are these goals? My background in maths is an elementary Moore method single-variable calculus course, and the beginning of (undergraduate) introductory real analysis. However, most of my time during the break will be available for mathematics. Is only the goal of completing the first book reasonable, with the second book requiring additional time? Is Principles of Mathematical Analysis sufficient for reading Real and Complex Analysis ? If not, what else should I know? What advice can you give me? I'm reading these primarily for entertainment, and I hope with this to learn enough mathematics to do interesting things. (I am a maths student in college, but have just started undergraduate analysis. My courses do not use either Rudin book.) This does not need to be advice on the books themselves, perhaps it could be advice on how to learn math quickly (and properly) if one has sufficient time to think about it exclusively.",,"['real-analysis', 'analysis', 'soft-question', 'self-learning', 'learning']"
50,Closed subsets of $\mathbb{R}$ characterization,Closed subsets of  characterization,\mathbb{R},"I remember the characterization of open subsets of $\mathbb{R}$ as a countable union of disjoint open intervals. I was thinking about whether this allows us to characterize closed subsets as a countable union of disjoint closed intervals. As we know, closed subsets are just complements of open subsets. I had an idea of starting with an open interval at $(a,b)$, then looking at the next open interval to the right at $(c,d)$, and concluding that $[b,c]$ belongs to the closed subset. But this idea can break down, for example, if the open subset contains the intervals $(-1,0), (1/2,1), (1/8,1/4), (1/32,1/16), \ldots$. In this case, there is no next open interval to the right of $(-1,0)$. How can we modify this idea to characterize closed subsets?","I remember the characterization of open subsets of $\mathbb{R}$ as a countable union of disjoint open intervals. I was thinking about whether this allows us to characterize closed subsets as a countable union of disjoint closed intervals. As we know, closed subsets are just complements of open subsets. I had an idea of starting with an open interval at $(a,b)$, then looking at the next open interval to the right at $(c,d)$, and concluding that $[b,c]$ belongs to the closed subset. But this idea can break down, for example, if the open subset contains the intervals $(-1,0), (1/2,1), (1/8,1/4), (1/32,1/16), \ldots$. In this case, there is no next open interval to the right of $(-1,0)$. How can we modify this idea to characterize closed subsets?",,['real-analysis']
51,"Show that $f(x)$ is a constant function if $\lim\limits_{h\to 0}\frac{1}{h^3}\int_{-h}^{h}f(x+t)\cdot t\,dt=0$ for all $x$",Show that  is a constant function if  for all,"f(x) \lim\limits_{h\to 0}\frac{1}{h^3}\int_{-h}^{h}f(x+t)\cdot t\,dt=0 x","Let $f(x)$ be a continuous function on $\mathbb{R}$ , such that for any real number $x$ we have: $$\lim_{h\to 0}\dfrac{1}{h^3}\int_{-h}^{h}f(x+t)\cdot t\,dt=0.$$ Show that $f(x)$ is a constant function. Maybe we can use the following lemma? Lemma . If $g$ is a continuous function, then $$\lim_{h\to0}\frac{1}{2\,h}\int_{x-h}^{x+h}g(s)\,ds=g(x).$$ Proof . We may assume $h>0$ . $$ \left|g(x)-\frac{1}{2\,h}\int_{x-h}^{x+h}g(s)\,ds\right|=\frac{1}{2\,h}\left|\int_{x-h}^{x+h}(g(x)-g(s))\,ds\right|\le\frac{1}{2\,h}\int_{x-h}^{x+h}\left|g(x)-g(s)\right|\,ds.$$ Use that $g$ is continuous at $x$ to show that the last expression converges to $0$ as $h\to0$ .","Let be a continuous function on , such that for any real number we have: Show that is a constant function. Maybe we can use the following lemma? Lemma . If is a continuous function, then Proof . We may assume . Use that is continuous at to show that the last expression converges to as .","f(x) \mathbb{R} x \lim_{h\to 0}\dfrac{1}{h^3}\int_{-h}^{h}f(x+t)\cdot t\,dt=0. f(x) g \lim_{h\to0}\frac{1}{2\,h}\int_{x-h}^{x+h}g(s)\,ds=g(x). h>0 
\left|g(x)-\frac{1}{2\,h}\int_{x-h}^{x+h}g(s)\,ds\right|=\frac{1}{2\,h}\left|\int_{x-h}^{x+h}(g(x)-g(s))\,ds\right|\le\frac{1}{2\,h}\int_{x-h}^{x+h}\left|g(x)-g(s)\right|\,ds. g x 0 h\to0","['real-analysis', 'integration', 'limits', 'continuity']"
52,Is topology just a generalization of real analysis?,Is topology just a generalization of real analysis?,,"While trying to learn undergraduate topology, I came across this lecture by Dr. Zimmerman who claims ""Topology is a generalization of real analysis, a lot of topology anyway."" They are obviously related and topology does seem more general, but this statement still surprised me. Can all of real (and complex) analysis be recast in the framework of topology? Edit: Could I say that real analysis is just studying the topology of $\mathbb{R}$ ?","While trying to learn undergraduate topology, I came across this lecture by Dr. Zimmerman who claims ""Topology is a generalization of real analysis, a lot of topology anyway."" They are obviously related and topology does seem more general, but this statement still surprised me. Can all of real (and complex) analysis be recast in the framework of topology? Edit: Could I say that real analysis is just studying the topology of ?",\mathbb{R},"['real-analysis', 'general-topology']"
53,Infinite series of nth root of n factorial,Infinite series of nth root of n factorial,,"Why is this not correct: $$ \begin{align} \lim_{n\to \infty}\sqrt[n]{n!} &= \lim_{n\to \infty}\sqrt[n]{n(n-1)(n-2)(n-3)\cdots(1)} \\ &=\lim_{n\to \infty}\sqrt[n]{n} \cdot \lim_{n\to \infty}\sqrt[n]{n-1} \cdot \lim_{n\to \infty}\sqrt[n]{n-2}\cdots \lim_{n\to \infty}\sqrt[n]{1} \\ &=1 \cdot 1 \cdot 1 \cdot 1 \cdots 1 \\ &=1 \end{align} $$ Therefore, $\lim_{n\to \infty} \sqrt[n]{n!}=1$. It is clear that $\lim_{n\to \infty} \sqrt[n]{n}= 1$ as and that $n! = n(n-1)!$ Yet wolframalpha gives me infinity as the limit and not $1$! If you have Rudin's Principles of Mathematical Analysis refer to Theorem $3.3$ c) and Theorem $3.20$ c)","Why is this not correct: $$ \begin{align} \lim_{n\to \infty}\sqrt[n]{n!} &= \lim_{n\to \infty}\sqrt[n]{n(n-1)(n-2)(n-3)\cdots(1)} \\ &=\lim_{n\to \infty}\sqrt[n]{n} \cdot \lim_{n\to \infty}\sqrt[n]{n-1} \cdot \lim_{n\to \infty}\sqrt[n]{n-2}\cdots \lim_{n\to \infty}\sqrt[n]{1} \\ &=1 \cdot 1 \cdot 1 \cdot 1 \cdots 1 \\ &=1 \end{align} $$ Therefore, $\lim_{n\to \infty} \sqrt[n]{n!}=1$. It is clear that $\lim_{n\to \infty} \sqrt[n]{n}= 1$ as and that $n! = n(n-1)!$ Yet wolframalpha gives me infinity as the limit and not $1$! If you have Rudin's Principles of Mathematical Analysis refer to Theorem $3.3$ c) and Theorem $3.20$ c)",,['real-analysis']
54,Mapping the Real Line to the Unit Interval,Mapping the Real Line to the Unit Interval,,"What is a continuous mapping of the real line $(-\infty, \infty)$ to the interval $[0, 1]$? I'm trying out logs and exponentials but they don't seem to work?","What is a continuous mapping of the real line $(-\infty, \infty)$ to the interval $[0, 1]$? I'm trying out logs and exponentials but they don't seem to work?",,"['calculus', 'real-analysis']"
55,Finding the value of the following double integral,Finding the value of the following double integral,,"The following question appeared in the American Mathematical Monthly (AMM), problem 12247, Vol.128, April 2021. For positive real constants $a$ , $b$ , and $c$ , prove $$\int_0^{\pi} \int_0^{\infty} \frac{a}{\pi(x^2+a^2)^{3/2}} \frac{x}{\sqrt{x^2+b^2+c^2-2cx\cos \theta}}~dx~d\theta=\frac{1}{\sqrt{(a+b)^2+c^2}}.$$ I tried to use the substitution $x=a\tan\theta$ . But it does not help a lot. Please guide. Any answer will be highly appreciated. Solutions posted in MSE are based on Fourier transform methods which sometimes are beyond the scope of undergraduate courses. I would like to see if solutions based on more traditional Calculus methods can be found.","The following question appeared in the American Mathematical Monthly (AMM), problem 12247, Vol.128, April 2021. For positive real constants , , and , prove I tried to use the substitution . But it does not help a lot. Please guide. Any answer will be highly appreciated. Solutions posted in MSE are based on Fourier transform methods which sometimes are beyond the scope of undergraduate courses. I would like to see if solutions based on more traditional Calculus methods can be found.",a b c \int_0^{\pi} \int_0^{\infty} \frac{a}{\pi(x^2+a^2)^{3/2}} \frac{x}{\sqrt{x^2+b^2+c^2-2cx\cos \theta}}~dx~d\theta=\frac{1}{\sqrt{(a+b)^2+c^2}}. x=a\tan\theta,['real-analysis']
56,What is the difference between Rudin's *Principles of Mathematical Analysis* and *Real and Complex Analysis* books?,What is the difference between Rudin's *Principles of Mathematical Analysis* and *Real and Complex Analysis* books?,,What's the difference between these two Rudin's books: Principles of Mathematical Analysis Real and Complex Analysis ? I want to reread by myself undergraduate analysis (single and multivariable analysis) to remember my undergraduate courses. Is one better to self-study than another? Isn't Principle is just a synthesis of the Real and Complex analysis Book? Are the exercises hard in both books?,What's the difference between these two Rudin's books: Principles of Mathematical Analysis Real and Complex Analysis ? I want to reread by myself undergraduate analysis (single and multivariable analysis) to remember my undergraduate courses. Is one better to self-study than another? Isn't Principle is just a synthesis of the Real and Complex analysis Book? Are the exercises hard in both books?,,"['real-analysis', 'complex-analysis', 'analysis', 'reference-request', 'book-recommendation']"
57,Nested Interval Property implies Axiom of Completeness,Nested Interval Property implies Axiom of Completeness,,"This is second attempt of me to prove: The Nested Interval Property implies the Axiom of Completeness of the real numbers. Nested interval property: If $I_1 \supseteq I_2 \supseteq I_3 \dots$ are closed intervals then $\bigcap_n I_n$ is not empty. Axiom of completeness: If $S$ is a non-empty set in $\mathbb R$ that has an upper bound then $S$ has a least upper bound. A first attempt is here . Please can you check my proof again? Proof: Let $K$ be an upper bound of $S$. Pick $s \in S$. Let $I_1 = [s,K]$. If $K$ is not the least upper bound there is a smaller upper bound $K_2$. Let $I_2 = [s,K_2]$. And so on. If no $K_n$ is a least upper bound for $S$ then because of nested interval property the intersection $I=\bigcap_n I_n$ is non-empty. Also, it is closed. Then the maximum $M$ of $I$ is a least upper bound of $S$: For all $K_n$ it holds that all $s \in S$ are $\le K_n$. The $M$ is the limit of the sequence $K_n$ therefore also $s \le M$ for every $s$. Also $M$ is the least upper bound because if it is not the least upper bound then by the construction $K_n = M$ for some $n$ and there is a smaller upper bound $K_{n+1}$. Then $M \notin \bigcap_n I_n$ which contradicts that $M$ is the maximum in the closed set $\bigcap_n I_n$.","This is second attempt of me to prove: The Nested Interval Property implies the Axiom of Completeness of the real numbers. Nested interval property: If $I_1 \supseteq I_2 \supseteq I_3 \dots$ are closed intervals then $\bigcap_n I_n$ is not empty. Axiom of completeness: If $S$ is a non-empty set in $\mathbb R$ that has an upper bound then $S$ has a least upper bound. A first attempt is here . Please can you check my proof again? Proof: Let $K$ be an upper bound of $S$. Pick $s \in S$. Let $I_1 = [s,K]$. If $K$ is not the least upper bound there is a smaller upper bound $K_2$. Let $I_2 = [s,K_2]$. And so on. If no $K_n$ is a least upper bound for $S$ then because of nested interval property the intersection $I=\bigcap_n I_n$ is non-empty. Also, it is closed. Then the maximum $M$ of $I$ is a least upper bound of $S$: For all $K_n$ it holds that all $s \in S$ are $\le K_n$. The $M$ is the limit of the sequence $K_n$ therefore also $s \le M$ for every $s$. Also $M$ is the least upper bound because if it is not the least upper bound then by the construction $K_n = M$ for some $n$ and there is a smaller upper bound $K_{n+1}$. Then $M \notin \bigcap_n I_n$ which contradicts that $M$ is the maximum in the closed set $\bigcap_n I_n$.",,"['real-analysis', 'proof-verification']"
58,What's wrong with this definition of continuity?,What's wrong with this definition of continuity?,,"Consider this definitions: A function $f:X \to Y$ is continuous at $x\in X$ iff for any open neighborhood $V_{f(x)}$ of $f(x)$ there is an open neighborhood $U_{x}$ of $x$ that gets mapped by $f$ into $V_{f(x)}$ (or, in other words, there is an open neighborhood $U_{x}$ of $x$ such that $f[U_x]\subseteq V_{f(x)}$). A function $f:X \to Y$ is continuous iff it is continuous at all $x \in X$. This definition of continuity seems to me equivalent to the ""standard"" definition in terms of inverse images (namely $f:X\to Y\;$ is continuous iff for any open set $V\subseteq Y$, the set $f^{-1}(V)\subseteq X$ is open). Am I wrong? Assuming that I'm correct, I am baffled by the prevalence of the currently standard definition, since the one above looks to me far more natural.  It is certainly more obviously a generalization of the $\epsilon$-$\delta$ definition of continuity in metric spaces (just replace $V_{f(x)}$ and $U_{x}$ by open balls $B(f(x), \epsilon)$ and $B(x, \delta)$, respectively), which, in turn, is an obvious generalization of the $\epsilon$-$\delta$ definition of continuity for functions $\mathbb{R}\to\mathbb{R}$ (just replace the open balls $B(f(x), \epsilon)$ and $B(x, \delta)$ by open intervals $(f(x) - \epsilon, f(x) + \epsilon)$ and $(x-\delta, x+\delta)$, respectively). Given these considerations, why is the standard definition the generally preferred one? Edit: the replies I've gotten so far have focused on the fact that the alternative definition depends on an auxiliary definition of continuity at a point, but this is a very minor aspect of the alternative definition.  I chose this two-part approach only to make the wording of the definition of continuity slightly less awkward, but it is not required.  I could have just as well written: A function $f:X \to Y$ is continuous iff for all $x \in X$ and any open neighborhood $V_{f(x)}$ of $f(x)$ there is an open neighborhood $U_{x}$ of $x$ such that $f[U_{x}]\subseteq V_{f(x)}$. Also, these replies suggest that, when it comes to defining terms, brevity trumps clarity.  I find this hard to take: a definition, by definition , is introducing a concept, so its intended audience is one that will appreciate clarity over brevity.  An equivalent characterization of the same concept whose only advantage is greater brevity should be relegated to a theorem, IMO.","Consider this definitions: A function $f:X \to Y$ is continuous at $x\in X$ iff for any open neighborhood $V_{f(x)}$ of $f(x)$ there is an open neighborhood $U_{x}$ of $x$ that gets mapped by $f$ into $V_{f(x)}$ (or, in other words, there is an open neighborhood $U_{x}$ of $x$ such that $f[U_x]\subseteq V_{f(x)}$). A function $f:X \to Y$ is continuous iff it is continuous at all $x \in X$. This definition of continuity seems to me equivalent to the ""standard"" definition in terms of inverse images (namely $f:X\to Y\;$ is continuous iff for any open set $V\subseteq Y$, the set $f^{-1}(V)\subseteq X$ is open). Am I wrong? Assuming that I'm correct, I am baffled by the prevalence of the currently standard definition, since the one above looks to me far more natural.  It is certainly more obviously a generalization of the $\epsilon$-$\delta$ definition of continuity in metric spaces (just replace $V_{f(x)}$ and $U_{x}$ by open balls $B(f(x), \epsilon)$ and $B(x, \delta)$, respectively), which, in turn, is an obvious generalization of the $\epsilon$-$\delta$ definition of continuity for functions $\mathbb{R}\to\mathbb{R}$ (just replace the open balls $B(f(x), \epsilon)$ and $B(x, \delta)$ by open intervals $(f(x) - \epsilon, f(x) + \epsilon)$ and $(x-\delta, x+\delta)$, respectively). Given these considerations, why is the standard definition the generally preferred one? Edit: the replies I've gotten so far have focused on the fact that the alternative definition depends on an auxiliary definition of continuity at a point, but this is a very minor aspect of the alternative definition.  I chose this two-part approach only to make the wording of the definition of continuity slightly less awkward, but it is not required.  I could have just as well written: A function $f:X \to Y$ is continuous iff for all $x \in X$ and any open neighborhood $V_{f(x)}$ of $f(x)$ there is an open neighborhood $U_{x}$ of $x$ such that $f[U_{x}]\subseteq V_{f(x)}$. Also, these replies suggest that, when it comes to defining terms, brevity trumps clarity.  I find this hard to take: a definition, by definition , is introducing a concept, so its intended audience is one that will appreciate clarity over brevity.  An equivalent characterization of the same concept whose only advantage is greater brevity should be relegated to a theorem, IMO.",,"['calculus', 'real-analysis', 'general-topology', 'metric-spaces']"
59,Polynomial outputs containing a particular Integer sequence,Polynomial outputs containing a particular Integer sequence,,"Does there exist a polynomial $P$ of degree greater than $1$ and with integer coefficients such that for every natural number $m$ there exists a natural number $n$ such that $P(n)=2^m$ ? This question seems very tricky and interesting, I guess some kind of interpolation might be of help but I could not figure out a proper solution. Comparing growth rate in this problem seems to be of no use as its an existence problem and polynomial does tend to infinity. Help. What about proving there does not exist such a monic polynomial?","Does there exist a polynomial of degree greater than and with integer coefficients such that for every natural number there exists a natural number such that ? This question seems very tricky and interesting, I guess some kind of interpolation might be of help but I could not figure out a proper solution. Comparing growth rate in this problem seems to be of no use as its an existence problem and polynomial does tend to infinity. Help. What about proving there does not exist such a monic polynomial?",P 1 m n P(n)=2^m,"['real-analysis', 'number-theory', 'polynomials']"
60,Rudin's proof on the Analytic Incompleteness of Rationals [duplicate],Rudin's proof on the Analytic Incompleteness of Rationals [duplicate],,"This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 6 years ago . In Rudin's classical ""Principles of Mathematical Analysis,"" he gave a proof like this: Claim: Let $A= \{p\in \mathbb{Q} | p>0, p^2 <2\}$ . Then A contains no largest number. Proof: Given any $p\in A$ . Let $q = p-\frac{p^2 -2}{p+2}$ . Later Rudin claimed that $q^2<2,$ and $q>p$ . My instructor asks us to think about a question on our own: Why is such $q$ a natural choice in this proof? I can see that in this way, $q>p$ is for sure. However, how does it become a natural choice?","This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 6 years ago . In Rudin's classical ""Principles of Mathematical Analysis,"" he gave a proof like this: Claim: Let . Then A contains no largest number. Proof: Given any . Let . Later Rudin claimed that and . My instructor asks us to think about a question on our own: Why is such a natural choice in this proof? I can see that in this way, is for sure. However, how does it become a natural choice?","A= \{p\in \mathbb{Q} | p>0, p^2 <2\} p\in A q = p-\frac{p^2 -2}{p+2} q^2<2, q>p q q>p","['real-analysis', 'proof-explanation', 'irrational-numbers']"
61,Prove the supremum of the set of affine functions is convex,Prove the supremum of the set of affine functions is convex,,"Let $\langle f_i \rangle _{i \in I}$ be a family of affine functions on a convex and compact set $\Omega \subset \mathbb{R^d}$ such that $f_i = a_i.x +b_i$ for $x \in \Omega$. Prove that f , defined by $f(x) = sup_{i \in I}f_i(x)$ for $x \in \Omega$ is a convex function. Explain this geometrically. I understand that since f is in $C^1(\Omega)$, f is convex if $sup(a_ix + b_i) \le sup(a_iy+b_i) + a_i(x-y)$, but i am having trouble proving this.","Let $\langle f_i \rangle _{i \in I}$ be a family of affine functions on a convex and compact set $\Omega \subset \mathbb{R^d}$ such that $f_i = a_i.x +b_i$ for $x \in \Omega$. Prove that f , defined by $f(x) = sup_{i \in I}f_i(x)$ for $x \in \Omega$ is a convex function. Explain this geometrically. I understand that since f is in $C^1(\Omega)$, f is convex if $sup(a_ix + b_i) \le sup(a_iy+b_i) + a_i(x-y)$, but i am having trouble proving this.",,"['real-analysis', 'multivariable-calculus', 'optimization', 'convex-optimization', 'nonlinear-optimization']"
62,Non-decreasing sequence of random variable convergence in probability implies it also converges almost surely.,Non-decreasing sequence of random variable convergence in probability implies it also converges almost surely.,,"The problem stated as follow: Suppose $X_1 \leq X_2 \leq \cdots$ and $X_n \xrightarrow[]{p} X$. Show that $X_n \to X$ a.s. I'm think about may be use the continuity of probability measure, but I don't know if that's correct.","The problem stated as follow: Suppose $X_1 \leq X_2 \leq \cdots$ and $X_n \xrightarrow[]{p} X$. Show that $X_n \to X$ a.s. I'm think about may be use the continuity of probability measure, but I don't know if that's correct.",,"['real-analysis', 'probability', 'measure-theory', 'convergence-divergence']"
63,The diameter of a compact set.,The diameter of a compact set.,,"Let $(X,d)$ be a metric space and define the diameter of a set $A$ as follows: $\operatorname{diam}A$ = $\sup\{d(x,y)\mathrel|x,y\in A \}$ if this quantity exists. Show that if $A$ is compact, then there are $a_1,a_2 \in A$ such that $\operatorname{diam}A = d(a_1,a_2)$.","Let $(X,d)$ be a metric space and define the diameter of a set $A$ as follows: $\operatorname{diam}A$ = $\sup\{d(x,y)\mathrel|x,y\in A \}$ if this quantity exists. Show that if $A$ is compact, then there are $a_1,a_2 \in A$ such that $\operatorname{diam}A = d(a_1,a_2)$.",,['real-analysis']
64,Bounding $\int_0^1 f(x) dx $ under the condition $\int_0^1 f'(x)^2 dx \le 1$,Bounding  under the condition,\int_0^1 f(x) dx  \int_0^1 f'(x)^2 dx \le 1,"Any tips on how to solve this? Problem 1.1.28 (Fa87) Let $S$ be the set of all real $C^1$ functions $f$ on $[0, 1]$ such that $f(0) = 0$ and $$\int_0^1 f'(x)^2 dx \le 1 \;. $$ Define $$J(f) = \int_0^1 f(x) dx \; .$$ Show that the function $J$ is bounded on $S$ , and compute its supremum. Is there a function $f_0 \in S$ at which $J$ attains its maximum value? If so, what is $f_0$ ? I tried using Cauchy-Schwartz and got a bound of $\frac23$ but it doesn't seem strong enough.","Any tips on how to solve this? Problem 1.1.28 (Fa87) Let be the set of all real functions on such that and Define Show that the function is bounded on , and compute its supremum. Is there a function at which attains its maximum value? If so, what is ? I tried using Cauchy-Schwartz and got a bound of but it doesn't seem strong enough.","S C^1 f [0, 1] f(0) = 0 \int_0^1 f'(x)^2 dx \le 1 \;.  J(f) = \int_0^1 f(x) dx \; . J S f_0 \in S J f_0 \frac23","['calculus', 'real-analysis', 'functional-analysis']"
65,'Fixed Point' Irrationals,'Fixed Point' Irrationals,,"I found this interesting problem which turns out to be more difficult than it first appears: Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is a function such that $f(f(x))=x$ for all $x \in \mathbb{R}$. Prove that there exists an irrational number $t$ such that $f(t)$ is irrational. The $f(f(x))=x$ condition reminds me of fixed point problems but as nothing else about $f$ is known I'm not sure how to apply this. Instead, I thought about the standard 'irrational to irrational power being rational' problem. So I thought about trying something along the lines of taking $x \in \mathbb{R}$ irrational then looking at $f(x)=y$. If $y$ is irrational we are done. If not, then I feel like trying something like $\sqrt{2}y$ as an input would work but nothing really panned out. Then I observed if $g(x)=(f\circ f)(x)$, we have $$ g(xy)=xy=g(x)g(y) $$ and $$ g(x+y)=x+y=g(x)+g(y) $$ but am unsure what this gets me. Any clues as to how I might proceed or perhaps an alternative route?","I found this interesting problem which turns out to be more difficult than it first appears: Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is a function such that $f(f(x))=x$ for all $x \in \mathbb{R}$. Prove that there exists an irrational number $t$ such that $f(t)$ is irrational. The $f(f(x))=x$ condition reminds me of fixed point problems but as nothing else about $f$ is known I'm not sure how to apply this. Instead, I thought about the standard 'irrational to irrational power being rational' problem. So I thought about trying something along the lines of taking $x \in \mathbb{R}$ irrational then looking at $f(x)=y$. If $y$ is irrational we are done. If not, then I feel like trying something like $\sqrt{2}y$ as an input would work but nothing really panned out. Then I observed if $g(x)=(f\circ f)(x)$, we have $$ g(xy)=xy=g(x)g(y) $$ and $$ g(x+y)=x+y=g(x)+g(y) $$ but am unsure what this gets me. Any clues as to how I might proceed or perhaps an alternative route?",,"['real-analysis', 'problem-solving']"
66,$f : S^1 \to\mathbb R$ is continuous then $f(x)=f(-x)$ for some $x\in S^1$,is continuous then  for some,f : S^1 \to\mathbb R f(x)=f(-x) x\in S^1,"Question is to prove : $f : S^1 \to \mathbb R$ is continuous then $f(x)=f(-x)$ for some $x\in S^1$ I guess it would be helpful to use intermediate value theorem Assuming $f(x)\neq f(-x)$ then given any $p\in (f(-x),f(x))$ (assuming $f(-x)<f(x)$) there exists $y\in (-x, x)$ such that $f(y)=p$ I am not very sure of how to use this.. It would be helpful if someone can give some hint which would help me to solve this.. Thank you.","Question is to prove : $f : S^1 \to \mathbb R$ is continuous then $f(x)=f(-x)$ for some $x\in S^1$ I guess it would be helpful to use intermediate value theorem Assuming $f(x)\neq f(-x)$ then given any $p\in (f(-x),f(x))$ (assuming $f(-x)<f(x)$) there exists $y\in (-x, x)$ such that $f(y)=p$ I am not very sure of how to use this.. It would be helpful if someone can give some hint which would help me to solve this.. Thank you.",,['real-analysis']
67,Showing that $\{x\in\mathbb R^n: \|x\|=\pi\}\cup\{0\}$ is not connected,Showing that  is not connected,\{x\in\mathbb R^n: \|x\|=\pi\}\cup\{0\},"I do have problems with connected sets so I got the following exercise: $X:=\{x\in\mathbb{R}^n: \|x\|=\pi\}\cup\{0\}\subset\mathbb{R}^n$. Why is $X$ not connected? My attempt: I have to find disjoint open sets $U,V\ne\emptyset$ such that $U\cup V=X$ . Let $U=\{x\in\mathbb{R}^n: \|x\|=\pi\}$ and $V=\{0\}$. Then $V$ is relative open since $$V=\{x\in\mathbb{R}^n:\|x\|<1\}\cap X$$ and $\{x\in\mathbb{R}^n:\|x\|<1\}$ is open in $\mathbb{R}^n$. Is this right? and why is $U$ open?","I do have problems with connected sets so I got the following exercise: $X:=\{x\in\mathbb{R}^n: \|x\|=\pi\}\cup\{0\}\subset\mathbb{R}^n$. Why is $X$ not connected? My attempt: I have to find disjoint open sets $U,V\ne\emptyset$ such that $U\cup V=X$ . Let $U=\{x\in\mathbb{R}^n: \|x\|=\pi\}$ and $V=\{0\}$. Then $V$ is relative open since $$V=\{x\in\mathbb{R}^n:\|x\|<1\}\cap X$$ and $\{x\in\mathbb{R}^n:\|x\|<1\}$ is open in $\mathbb{R}^n$. Is this right? and why is $U$ open?",,['real-analysis']
68,Is the complement of a countable set in $\mathbb{R}$ dense? Application to convergence of probability distribution functions.,Is the complement of a countable set in  dense? Application to convergence of probability distribution functions.,\mathbb{R},"I am wondering if we have a set $A\in\mathbb{R}$ that is countable, whether $A^{c}$ is dense in $\mathbb{R}$? I thought I saw this quoted somewhere on google but I can not find it again! I am working through a proof relating to uniqueness of weak limits of sequences distribution functions. The set of discontinuity points of distribution functions are countable, and the proof suggests the complement of this set is dense in $\mathbb{R}$, and I am unsure how this conclusion is arrived at. Any help would be greatly appreciated.","I am wondering if we have a set $A\in\mathbb{R}$ that is countable, whether $A^{c}$ is dense in $\mathbb{R}$? I thought I saw this quoted somewhere on google but I can not find it again! I am working through a proof relating to uniqueness of weak limits of sequences distribution functions. The set of discontinuity points of distribution functions are countable, and the proof suggests the complement of this set is dense in $\mathbb{R}$, and I am unsure how this conclusion is arrived at. Any help would be greatly appreciated.",,"['real-analysis', 'general-topology', 'metric-spaces']"
69,Why can't epsilon depend on delta instead?,Why can't epsilon depend on delta instead?,,"When presented with $\lim_{x\to a}f(x) = L$, we are usually taught to intuitively think of $x$ approaching the value $a$ from both sides, with $f(x)$ getting closer and closer to the value $L$. For example, to guess the value of $\lim_{x\to 3}(x+3)$, we plug in $2.9$, $2.999999$, or $3.01$, $3.00000001$, and see what happens. Or we draw a graph. This was in high school calculus. However, when rigorously proving that a limit exists, the notion of 'getting closer and closer to a value' is replaced with $\epsilon$-$\delta$ language. Intuitively, given $\epsilon>0$, no matter how small your 'strip' is around $L$, if I can always find a corresponding strip which ensures that the values of $f(x)$ will be within the strip around $L$, then I've proven that the limit exists. The rigorous definition requires that $\epsilon$ be given first. This makes sense. But if we challenge someone with $\delta$, and if our opponent fails to provide an $\epsilon$ so that $0<|x-a|<\delta$, wouldn't that prove that the limit doesn't exist? Why can't limits be defined this way instead of the other way round? I think this is more natural, because in the intuitive definition, we vary $x$ and observe what happens to $f(x)$. Suddenly, in the rigorous definition, we do the reverse: Pick values around $L$ and observe whether there are $x$'s which map to those values. What is wrong with my reasoning?","When presented with $\lim_{x\to a}f(x) = L$, we are usually taught to intuitively think of $x$ approaching the value $a$ from both sides, with $f(x)$ getting closer and closer to the value $L$. For example, to guess the value of $\lim_{x\to 3}(x+3)$, we plug in $2.9$, $2.999999$, or $3.01$, $3.00000001$, and see what happens. Or we draw a graph. This was in high school calculus. However, when rigorously proving that a limit exists, the notion of 'getting closer and closer to a value' is replaced with $\epsilon$-$\delta$ language. Intuitively, given $\epsilon>0$, no matter how small your 'strip' is around $L$, if I can always find a corresponding strip which ensures that the values of $f(x)$ will be within the strip around $L$, then I've proven that the limit exists. The rigorous definition requires that $\epsilon$ be given first. This makes sense. But if we challenge someone with $\delta$, and if our opponent fails to provide an $\epsilon$ so that $0<|x-a|<\delta$, wouldn't that prove that the limit doesn't exist? Why can't limits be defined this way instead of the other way round? I think this is more natural, because in the intuitive definition, we vary $x$ and observe what happens to $f(x)$. Suddenly, in the rigorous definition, we do the reverse: Pick values around $L$ and observe whether there are $x$'s which map to those values. What is wrong with my reasoning?",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
70,$f$ is continuous $ \iff $ $f^{-1}$ is continuous?,is continuous   is continuous?,f  \iff  f^{-1},"Is the following true? Let $A, B \subseteq \Bbb R$ and let $f : A \to B$ be a bijective map.   Then $\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;f$ is continuous on $A$  $  \iff $ $f^{-1}$ is continuous on $B$ It seems like such an obvious result but I can't seem to prove it. At least is the result true if we restrict $A$ and $B$ to be intervals? My Attempt at a Proof: $\implies $: Let $b \in B$. I was going nowhere after considering an arbitrary sequence $ (x_n) $ in $B$ which converges to $b$. So I instead assumed that $V$ was any neighbourhood around $f^{-1}(b)$. Now suppose there is no neighbourhood $U$ of $b$ such that $ x \in U \implies f^{-1}(x) \in V $. But for every neighbourhood $U'$ of $b$ there is a neighbourhood $V'$ of $f^{-1}(b)$ such that $ f^{-1}(x) \in V' \implies x \in U' $. This is by considering $b = f(f^{-1}(b))$ and since $f$ is continuous. If one of these $V'$s is a subset of $V$ then we are done. But suppose not. I cannot proceed further. I considered arbitrarily small neighbourhoods $U'_n = \{ x \ | \ |x - b| \lt \frac 1 n \}$ but still got nowhere. Any help is appreciated.","Is the following true? Let $A, B \subseteq \Bbb R$ and let $f : A \to B$ be a bijective map.   Then $\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;f$ is continuous on $A$  $  \iff $ $f^{-1}$ is continuous on $B$ It seems like such an obvious result but I can't seem to prove it. At least is the result true if we restrict $A$ and $B$ to be intervals? My Attempt at a Proof: $\implies $: Let $b \in B$. I was going nowhere after considering an arbitrary sequence $ (x_n) $ in $B$ which converges to $b$. So I instead assumed that $V$ was any neighbourhood around $f^{-1}(b)$. Now suppose there is no neighbourhood $U$ of $b$ such that $ x \in U \implies f^{-1}(x) \in V $. But for every neighbourhood $U'$ of $b$ there is a neighbourhood $V'$ of $f^{-1}(b)$ such that $ f^{-1}(x) \in V' \implies x \in U' $. This is by considering $b = f(f^{-1}(b))$ and since $f$ is continuous. If one of these $V'$s is a subset of $V$ then we are done. But suppose not. I cannot proceed further. I considered arbitrarily small neighbourhoods $U'_n = \{ x \ | \ |x - b| \lt \frac 1 n \}$ but still got nowhere. Any help is appreciated.",,"['real-analysis', 'continuity', 'inverse']"
71,Another way of expressing $\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1}$,Another way of expressing,\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1},Do you know any nice way of expressing $$\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1}$$ ? Some simple manipulations involving the integrals lead to an expression that also uses the hypergeometric series. Is there any way of getting a form that doesn't use the  HG function?,Do you know any nice way of expressing $$\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1}$$ ? Some simple manipulations involving the integrals lead to an expression that also uses the hypergeometric series. Is there any way of getting a form that doesn't use the  HG function?,,"['real-analysis', 'calculus', 'sequences-and-series', 'complex-analysis', 'harmonic-numbers']"
72,Value of $(-1)^x$ for $x$ irrational,Value of  for  irrational,(-1)^x x,"I was working on an analysis problem when this question arose in one my proofs. I think it may be either $-1$ or $1$, but it seems like there can only be an arbitrary way to assign this. So is there an agreed upon method for determining $(-1)^x$ for irrational values of $x$?","I was working on an analysis problem when this question arose in one my proofs. I think it may be either $-1$ or $1$, but it seems like there can only be an arbitrary way to assign this. So is there an agreed upon method for determining $(-1)^x$ for irrational values of $x$?",,"['real-analysis', 'complex-analysis', 'exponentiation']"
73,Proving the rationals are dense in R,Proving the rationals are dense in R,,"I know this is a common proof. I'm following Rudin's proof and I'm following everything except for one step. Suppose $x, y \in \Bbb R$ and $x < y$. Then there exists an $n \in \Bbb N$ such that $n(y-x) > 1$. Again by the Archimedean property, there exist $m_{1}, m_{2} \in \Bbb N$ such that $m_{1} > nx$ and $m_{2} > -nx$, i.e. $$ -m_{2} < nx < m_{1} $$ From here, Rudin says there must be an $m \in \Bbb Z$ with $-m_{2} \le m \le m_{1}$ and that $$ m-1 \le nx < m $$ I'm confused about these two steps. If $-m_{2} < nx < m_{1}$, then isn't $-m_{2} < m_{1}$? edit: to be clear, I follow everything up until the introduction of $m$.","I know this is a common proof. I'm following Rudin's proof and I'm following everything except for one step. Suppose $x, y \in \Bbb R$ and $x < y$. Then there exists an $n \in \Bbb N$ such that $n(y-x) > 1$. Again by the Archimedean property, there exist $m_{1}, m_{2} \in \Bbb N$ such that $m_{1} > nx$ and $m_{2} > -nx$, i.e. $$ -m_{2} < nx < m_{1} $$ From here, Rudin says there must be an $m \in \Bbb Z$ with $-m_{2} \le m \le m_{1}$ and that $$ m-1 \le nx < m $$ I'm confused about these two steps. If $-m_{2} < nx < m_{1}$, then isn't $-m_{2} < m_{1}$? edit: to be clear, I follow everything up until the introduction of $m$.",,"['real-analysis', 'rational-numbers']"
74,Does there exist a 3-dimensional subspace of real functions consisting only of monotone functions?,Does there exist a 3-dimensional subspace of real functions consisting only of monotone functions?,,"This is Exercise 1.O from the book  Van Rooij, Schikhof: A Second Course on Real Functions . The set of the monotone functions on $[0,1]$ contains all polynomial   functions of degree $\le 1$. These form a two-dimensional vector space. Does the set of   all monotone functions contain a three-dimensional vector space?","This is Exercise 1.O from the book  Van Rooij, Schikhof: A Second Course on Real Functions . The set of the monotone functions on $[0,1]$ contains all polynomial   functions of degree $\le 1$. These form a two-dimensional vector space. Does the set of   all monotone functions contain a three-dimensional vector space?",,"['linear-algebra', 'real-analysis']"
75,Finding the power series of a rational function,Finding the power series of a rational function,,"In many combinatorial enumeration problems it is possible to find a rational generating function (i.e. the quotient of two polynomials) for the sequence in question. The question is - given the generating function, how can we find (algorithmically) the values of the sequence, i.e. the coefficients of the corresponding power series? I know that for a rational generating function, the sequence satisfies a recurrence relation given by the coefficients of the polynomial in the denominator, so it's really just the question of finding the finite initial values.","In many combinatorial enumeration problems it is possible to find a rational generating function (i.e. the quotient of two polynomials) for the sequence in question. The question is - given the generating function, how can we find (algorithmically) the values of the sequence, i.e. the coefficients of the corresponding power series? I know that for a rational generating function, the sequence satisfies a recurrence relation given by the coefficients of the polynomial in the denominator, so it's really just the question of finding the finite initial values.",,"['real-analysis', 'combinatorics', 'generating-functions']"
76,"Is possible to use ""Feynman's trick"" (differentiate under the integral or Leibniz integral rule) to calculate $\int_0^1 \frac{\ln(1-x)}{x}dx\:?$","Is possible to use ""Feynman's trick"" (differentiate under the integral or Leibniz integral rule) to calculate",\int_0^1 \frac{\ln(1-x)}{x}dx\:?,"I heard that the equivalent integral: $-\int_0^\infty \frac{x}{e^x-1}dx$ can be done using Contour integration (I never studied this). Also that sometimes ""Leibniz integral rule"" is used instead of Contour integration. So can ""Feynman's trick"" be used to show that $\int_0^1 \frac{\ln(1-x)}{x}dx = -\frac{\pi^2}{6}$   $\:\:?$","I heard that the equivalent integral: $-\int_0^\infty \frac{x}{e^x-1}dx$ can be done using Contour integration (I never studied this). Also that sometimes ""Leibniz integral rule"" is used instead of Contour integration. So can ""Feynman's trick"" be used to show that $\int_0^1 \frac{\ln(1-x)}{x}dx = -\frac{\pi^2}{6}$   $\:\:?$",,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'contour-integration']"
77,"Evaluation of $\int_0^1 \frac{x^3}{2(2-x^2)(1+x^2) + 3\sqrt{(2-x^2)(1+x^2)}}\,\mathrm dx$",Evaluation of,"\int_0^1 \frac{x^3}{2(2-x^2)(1+x^2) + 3\sqrt{(2-x^2)(1+x^2)}}\,\mathrm dx","How does one evaluate the following integral? $$\int_0^1 \frac{x^3}{2(2-x^2)(1+x^2) + 3\sqrt{(2-x^2)(1+x^2)}}\,\mathrm dx$$ This is a homework problem and I have been evaluating this integral for hours yet no success so far. I have tried to rationalize the integrand by multiplying it with $$\frac{2(2-x^2)(1+x^2) - 3\sqrt{(2-x^2)(1+x^2)}}{2(2-x^2)(1+x^2) - 3\sqrt{(2-x^2)(1+x^2)}}$$ but the integrand is getting worse. I have tried to use trigonometric substitutions like $x=\tan\theta$ and $x=\sqrt{2}\sin\theta$, but I cannot rid off the square root form. I have also tried to use hyperbolic trigonometric substitutions but the thing does not get any easier neither also substitution $y=x^2$ nor $y=\sqrt{(2-x^2)(1+x^2)}$. Using integration by parts is almost impossible for this one. I have also tried to use the tricks from this thread , but still did not get anything. No clue is given. My professor said, we must use clever substitutions but I cannot find them. Any idea or hint? Any help would be appreciated. Thanks in advance. Edit : The answer I got from my Prof is $\dfrac{3-2\sqrt{2}}{6}$.","How does one evaluate the following integral? $$\int_0^1 \frac{x^3}{2(2-x^2)(1+x^2) + 3\sqrt{(2-x^2)(1+x^2)}}\,\mathrm dx$$ This is a homework problem and I have been evaluating this integral for hours yet no success so far. I have tried to rationalize the integrand by multiplying it with $$\frac{2(2-x^2)(1+x^2) - 3\sqrt{(2-x^2)(1+x^2)}}{2(2-x^2)(1+x^2) - 3\sqrt{(2-x^2)(1+x^2)}}$$ but the integrand is getting worse. I have tried to use trigonometric substitutions like $x=\tan\theta$ and $x=\sqrt{2}\sin\theta$, but I cannot rid off the square root form. I have also tried to use hyperbolic trigonometric substitutions but the thing does not get any easier neither also substitution $y=x^2$ nor $y=\sqrt{(2-x^2)(1+x^2)}$. Using integration by parts is almost impossible for this one. I have also tried to use the tricks from this thread , but still did not get anything. No clue is given. My professor said, we must use clever substitutions but I cannot find them. Any idea or hint? Any help would be appreciated. Thanks in advance. Edit : The answer I got from my Prof is $\dfrac{3-2\sqrt{2}}{6}$.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
78,Value of $f'(0)$ if $f(x)=\frac{x}{1+\frac{x}{1+\frac{x}{1+\ddots}}}$,Value of  if,f'(0) f(x)=\frac{x}{1+\frac{x}{1+\frac{x}{1+\ddots}}},"Consider the function $$f(x)=\cfrac{x}{1+\cfrac{x}{1+\cfrac{x}{1+\ddots}}} $$ Determine the value of $f'(0)$. I tried to differentiate $f(x)$ but it is not subject to chain rule, and now I'm stuck. How would I solve this? Also how to rapidly evaluate $f(x)$ for a given $x$?","Consider the function $$f(x)=\cfrac{x}{1+\cfrac{x}{1+\cfrac{x}{1+\ddots}}} $$ Determine the value of $f'(0)$. I tried to differentiate $f(x)$ but it is not subject to chain rule, and now I'm stuck. How would I solve this? Also how to rapidly evaluate $f(x)$ for a given $x$?",,"['real-analysis', 'derivatives', 'continued-fractions']"
79,An uncountable family of measurable sets with positive measure,An uncountable family of measurable sets with positive measure,,"Is there a family of measurable sets $\{E_\alpha\}_{\alpha \in A}$ , disjoint, uncountable and each one with positive measure?","Is there a family of measurable sets $\{E_\alpha\}_{\alpha \in A}$ , disjoint, uncountable and each one with positive measure?",,"['real-analysis', 'measure-theory']"
80,The subset of non-measurable set,The subset of non-measurable set,,"If $A$ is a non-measurable set in $\mathbb R^n$ (in the sense of Lebesgue), does it necessarily contain a positive measurable subset?","If $A$ is a non-measurable set in $\mathbb R^n$ (in the sense of Lebesgue), does it necessarily contain a positive measurable subset?",,"['real-analysis', 'measure-theory']"
81,"An interesting exercise about converging positive series, involving $\sum_{n\geq 1}a_n^{\frac{n-1}{n}}$","An interesting exercise about converging positive series, involving",\sum_{n\geq 1}a_n^{\frac{n-1}{n}},"Yesterday I stumbled across an interesting exercise (Indam test 2014, Exercise B3): (Ex) Given a positive sequence $\{a_n\}_{n\geq 1}$ such that $\sum_{n\geq  1}a_n$ is convergent, prove that $$ \sum_{n\geq  1}a_n^{\frac{n-1}{n}}$$ is convergent, too. My proof exploits an idea from Carleman's inequality. We have: $$ a_n^{\frac{n-1}{n}}=\text{GM}\left(\frac{1}{n},2a_n,\frac{3}{2}a_n,\ldots,\frac{n}{n-1}a_n\right) $$ and by the AM-GM inequality $$ a_n^{\frac{n-1}{n}}\leq \frac{1}{n}\left(\frac{1}{n}+a_n\sum_{k=1}^{n-1}\frac{k+1}{k}\right)\leq \frac{1}{n^2}+\left(1+\frac{\log n}{n}\right)a_n $$ hence $$ \sum_{n\geq 1}a_n^{\frac{n-1}{n}}\color{red}{\leq} \frac{\pi^2}{6}+\left(1+\frac{1}{e}\right)\sum_{n\geq 1}a_n.$$ Now my actual Question: Is there a simpler proof of (Ex) , maybe through Holder's inequality, maybe exploiting the approximations   $$ \sum_{m<n\leq 2m}a_n^{\frac{2m-1}{2m}}\approx \sum_{m<n\leq 2m}a_n^{\frac{n-1}{n}}\approx \sum_{m<n\leq 2m}a_n^{\frac{m-1}{m}}$$   ""blocking"" the exponents over small summation sub-ranges?","Yesterday I stumbled across an interesting exercise (Indam test 2014, Exercise B3): (Ex) Given a positive sequence $\{a_n\}_{n\geq 1}$ such that $\sum_{n\geq  1}a_n$ is convergent, prove that $$ \sum_{n\geq  1}a_n^{\frac{n-1}{n}}$$ is convergent, too. My proof exploits an idea from Carleman's inequality. We have: $$ a_n^{\frac{n-1}{n}}=\text{GM}\left(\frac{1}{n},2a_n,\frac{3}{2}a_n,\ldots,\frac{n}{n-1}a_n\right) $$ and by the AM-GM inequality $$ a_n^{\frac{n-1}{n}}\leq \frac{1}{n}\left(\frac{1}{n}+a_n\sum_{k=1}^{n-1}\frac{k+1}{k}\right)\leq \frac{1}{n^2}+\left(1+\frac{\log n}{n}\right)a_n $$ hence $$ \sum_{n\geq 1}a_n^{\frac{n-1}{n}}\color{red}{\leq} \frac{\pi^2}{6}+\left(1+\frac{1}{e}\right)\sum_{n\geq 1}a_n.$$ Now my actual Question: Is there a simpler proof of (Ex) , maybe through Holder's inequality, maybe exploiting the approximations   $$ \sum_{m<n\leq 2m}a_n^{\frac{2m-1}{2m}}\approx \sum_{m<n\leq 2m}a_n^{\frac{n-1}{n}}\approx \sum_{m<n\leq 2m}a_n^{\frac{m-1}{m}}$$   ""blocking"" the exponents over small summation sub-ranges?",,"['real-analysis', 'calculus', 'sequences-and-series', 'inequality']"
82,What does the centre dot notation mean? $P(\cdot)$,What does the centre dot notation mean?,P(\cdot),"I think the notation has something to do with probability. I saw the symbol somewhere and I cannot seem to find anything on it. If I were to read what it looks like, I would say: probability as a function of dot. It looks like this: $P(\cdot)$ Thanks in advance!","I think the notation has something to do with probability. I saw the symbol somewhere and I cannot seem to find anything on it. If I were to read what it looks like, I would say: probability as a function of dot. It looks like this: Thanks in advance!",P(\cdot),"['real-analysis', 'probability', 'notation']"
83,"Meaning of ""occurs for infinitely many"" and ""all but finitely many""","Meaning of ""occurs for infinitely many"" and ""all but finitely many""",,Suppose $P(n)$ is certain statement. I hear all the time that my teachers say $$ P(n) \; \; \text{occurs for infinitely many} \; \; \;n $$ $$ P(n) \; \; \text{for all but finitely many} \; \; n $$ MY question: What are the precise definitions of this statements ?,Suppose $P(n)$ is certain statement. I hear all the time that my teachers say $$ P(n) \; \; \text{occurs for infinitely many} \; \; \;n $$ $$ P(n) \; \; \text{for all but finitely many} \; \; n $$ MY question: What are the precise definitions of this statements ?,,[]
84,"Differentiable and Continuous functions on [0,1] with 'weird' conditions.","Differentiable and Continuous functions on [0,1] with 'weird' conditions.",,"I've been stuck on this one for a while. Comes from an analysis qual question. Let f be a function that is continuous on $\left[0,1\right]$ and differentiable on $(0,1)$. Show that if $f(0)=0$ and $|f'(x)| \leq |f(x)|$ for all $x \in (0,1)$, then $f(x)=0$ for all $x \in \left[0,1\right]$. What I've tried doing so far is see if there was anything I could do with MVT. I didn't really see anything to do with definitions either..to which I have a feeling I'll be playing around with them. Drawing a picture was a little difficult with these conditions as well Any hints/suggestions?","I've been stuck on this one for a while. Comes from an analysis qual question. Let f be a function that is continuous on $\left[0,1\right]$ and differentiable on $(0,1)$. Show that if $f(0)=0$ and $|f'(x)| \leq |f(x)|$ for all $x \in (0,1)$, then $f(x)=0$ for all $x \in \left[0,1\right]$. What I've tried doing so far is see if there was anything I could do with MVT. I didn't really see anything to do with definitions either..to which I have a feeling I'll be playing around with them. Drawing a picture was a little difficult with these conditions as well Any hints/suggestions?",,['real-analysis']
85,Show that $\sum\limits_{k=1}^{\infty}\frac{\zeta(2k)}{(k+1)(2k+1)}=\frac12$,Show that,\sum\limits_{k=1}^{\infty}\frac{\zeta(2k)}{(k+1)(2k+1)}=\frac12,I was doing a integral which ends up with a tough series part: $$\sum_{k=1}^{\infty}\frac{\zeta(2k)}{(k+1)(2k+1)}$$ Mathematica says $$\frac12$$ Which agrees with the anwer...Anyone know how to evaluate this?,I was doing a integral which ends up with a tough series part: $$\sum_{k=1}^{\infty}\frac{\zeta(2k)}{(k+1)(2k+1)}$$ Mathematica says $$\frac12$$ Which agrees with the anwer...Anyone know how to evaluate this?,,"['calculus', 'real-analysis', 'sequences-and-series', 'riemann-zeta']"
86,Reference request (famous mathematicians for High School),Reference request (famous mathematicians for High School),,"Some students of an High School asked me some books from famous mathematicians that they can read (so advanced high school level focused mainly on real-analysis). They were asking things like Cauchy, Fermat... but I think the language would be technical in an akward ancient way for them so that probably will not be suitable. I thought that maybe Riemann dissertation could do but maybe it's too advanced. I then thought Galois, but again the original papers are quite difficult to understand if you don't already have the right picture in your mind. I'm not sure there's effectively a book from an historically famous mathematician that could fit the request. They didn't specifically requested that the argument should be mathematical even if I think they implied this, otherwise I could suggest something of Poincaré which rather philosophical but at least readable. They didin't specify the period (even if I think they might want to already know the name of the writer). In modern period maybe I would suggest Mumford the Indra's Pearls. But I'm quite sure they don't know Mumford... I'm now thinking that maybe some kind of physicist would be better. But they were asking mathematicians. I really don't have a clue of what to suggest. Please help me! Edit. Someone correctly asked me why I'm "" limiting to Mathematicians they've heard of "". I totally agree with who's asking. Old mathematics would be kind of akward I think. The problem is not that "" I'm limiting "", the problem is that "" this is what they asked "". It's something like "" We have seen their theorems, heard a lot about them, we would like to read something written by them "". As pointed out is that probably this request cannot be fulfilled entirely or is not a good idea to fulfill their request. So any suggestion will be take into account. If it was physics Schroedinger ""What is Life"" and some Heisenberg essays would be in order... in mathematics I've no clue if exists something similar... I think maybe Poincaré is the only one...","Some students of an High School asked me some books from famous mathematicians that they can read (so advanced high school level focused mainly on real-analysis). They were asking things like Cauchy, Fermat... but I think the language would be technical in an akward ancient way for them so that probably will not be suitable. I thought that maybe Riemann dissertation could do but maybe it's too advanced. I then thought Galois, but again the original papers are quite difficult to understand if you don't already have the right picture in your mind. I'm not sure there's effectively a book from an historically famous mathematician that could fit the request. They didn't specifically requested that the argument should be mathematical even if I think they implied this, otherwise I could suggest something of Poincaré which rather philosophical but at least readable. They didin't specify the period (even if I think they might want to already know the name of the writer). In modern period maybe I would suggest Mumford the Indra's Pearls. But I'm quite sure they don't know Mumford... I'm now thinking that maybe some kind of physicist would be better. But they were asking mathematicians. I really don't have a clue of what to suggest. Please help me! Edit. Someone correctly asked me why I'm "" limiting to Mathematicians they've heard of "". I totally agree with who's asking. Old mathematics would be kind of akward I think. The problem is not that "" I'm limiting "", the problem is that "" this is what they asked "". It's something like "" We have seen their theorems, heard a lot about them, we would like to read something written by them "". As pointed out is that probably this request cannot be fulfilled entirely or is not a good idea to fulfill their request. So any suggestion will be take into account. If it was physics Schroedinger ""What is Life"" and some Heisenberg essays would be in order... in mathematics I've no clue if exists something similar... I think maybe Poincaré is the only one...",,"['real-analysis', 'reference-request', 'soft-question']"
87,"What is the difference between advanced calculus, vector calculus, multivariable calculus, multivariable real analysis and vector analysis?","What is the difference between advanced calculus, vector calculus, multivariable calculus, multivariable real analysis and vector analysis?",,"What is the difference between advanced calculus, vector calculus, multivariable calculus, multivariable real analysis and vector analysis? What I think I know Vector calculus and multivariable calculus are the same. Multivariable real analysis and vector analysis are the same and both are the formalization of multivariable/vector calculus. Am I right? what's the difference between advanced calculus and these other subjects?","What is the difference between advanced calculus, vector calculus, multivariable calculus, multivariable real analysis and vector analysis? What I think I know Vector calculus and multivariable calculus are the same. Multivariable real analysis and vector analysis are the same and both are the formalization of multivariable/vector calculus. Am I right? what's the difference between advanced calculus and these other subjects?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
88,Difference between boundary point & limit point.,Difference between boundary point & limit point.,,"A limit point is just a accumulation point whose neighbourhood contains infinitely many elements of the sequence. Is there any difference between boundary point & limit point? I've read in another question here that all boudary points are limit points, but is the converse true?","A limit point is just a accumulation point whose neighbourhood contains infinitely many elements of the sequence. Is there any difference between boundary point & limit point? I've read in another question here that all boudary points are limit points, but is the converse true?",,[]
89,"Prove that $\int_0^1\frac{1-x}{1-x^6}\ln^4x\,dx=\frac{16\sqrt{3}}{729}\pi^5+\frac{605}{54}\zeta(5)$",Prove that,"\int_0^1\frac{1-x}{1-x^6}\ln^4x\,dx=\frac{16\sqrt{3}}{729}\pi^5+\frac{605}{54}\zeta(5)","This integral comes from a well-known site (I am sorry, the site is classified due to regarding the OP.) $$\int_0^1\frac{1-x}{1-x^6}\ln^4x\,dx$$ I can calculate the integral using the help of geometric series and I get the answer \begin{align} \sum_{n=0}^\infty\left(\frac{24}{(6n+1)^5}-\frac{24}{(6n+2)^5}\right) &=\frac{1}{6^5}\left(\Psi^{(4)}\left(\frac{1}{3}\right)-\Psi^{(4)}\left(\frac{1}{6}\right)\right)\\ &=\frac{16\sqrt{3}}{729}\pi^5+\frac{605}{54}\zeta(5) \end{align} To be honest, I use Wolfram Alpha to calculate the sum of series. The problem is I don't think this is the correct way to calculate the integral because I use a machine to help me. I tried another way, I used partial fraction to decompose the integrand as $$\frac{\ln^4x}{3(x+1)}+\frac{\ln^4x}{2(x^2+x+1)}-\frac{2x-1}{6(x^2-x+1)}\ln^4x$$ but none of them seemed easy to calculate. Could anyone here please help me to calculate the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.","This integral comes from a well-known site (I am sorry, the site is classified due to regarding the OP.) $$\int_0^1\frac{1-x}{1-x^6}\ln^4x\,dx$$ I can calculate the integral using the help of geometric series and I get the answer \begin{align} \sum_{n=0}^\infty\left(\frac{24}{(6n+1)^5}-\frac{24}{(6n+2)^5}\right) &=\frac{1}{6^5}\left(\Psi^{(4)}\left(\frac{1}{3}\right)-\Psi^{(4)}\left(\frac{1}{6}\right)\right)\\ &=\frac{16\sqrt{3}}{729}\pi^5+\frac{605}{54}\zeta(5) \end{align} To be honest, I use Wolfram Alpha to calculate the sum of series. The problem is I don't think this is the correct way to calculate the integral because I use a machine to help me. I tried another way, I used partial fraction to decompose the integrand as $$\frac{\ln^4x}{3(x+1)}+\frac{\ln^4x}{2(x^2+x+1)}-\frac{2x-1}{6(x^2-x+1)}\ln^4x$$ but none of them seemed easy to calculate. Could anyone here please help me to calculate the integral preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'improper-integrals']"
90,liminf and limsup with characteristic (indicator) function,liminf and limsup with characteristic (indicator) function,,"So first let me state my homework problem: Let $X$ be a set, let $\{A_k\}$ be a sequence of subsets of $X$ , let $B = \bigcup_{n=1}^{+\infty} \bigcap_{k=n}^{+\infty} A_k$ , and let $C = \bigcap_{n=1}^{+\infty} \bigcup_{k=n}^{+\infty} A_k$ . Show that (a) $\liminf_k\; {\chi_A}_{_k} = \chi_B$ , and $(b)$ $\limsup_k \;{\chi_A}_{_k} = \chi_C.$ I know that, in the context I am familiar with, that $$\liminf_{k\to +\infty}\; X_k = \bigcup_{k=1}^{+\infty} \bigcap_{n=k}^{+\infty} X_n$$ and $$\limsup_{k\to +\infty}\;X_k = \bigcap_{k=1}^{+\infty} \bigcup_{n=k}^{+\infty} X_n.$$ I also know that the characteristics (indicator) function is defined as $\chi_A(x) = \begin{cases} 1, & x \in A \\ 0, & x \notin A .\end{cases} $ So I wrote out $B$ in some of its `glory': $B= (A_1 \cap A_2 \cap A_3 \cap \cdots) \cup (A_2 \cap A_3 \cap \cdots) \cup (A_3 \cap A_4 \cap \cdots) \cup \cdots$ , and as the first argument is the smallest, with increasing size to the right, the last term in the expression for $B$ would be $B$ , which would be the largest. So if I replace the $X_k$ 's above with $\chi$ 's I still don't see how I can get the correct answer - though it looks pretty clear from the definition of $B$ and that of $\liminf$ being basically the same, except in this case for the $\chi$ . Any direction would be greatly appreciated. By the way, I have checked out limsup and liminf of a sequence of subsets of a set but I was somewhat confused by the topology, the meets/joins, etc. Thanks much, Nate","So first let me state my homework problem: Let be a set, let be a sequence of subsets of , let , and let . Show that (a) , and I know that, in the context I am familiar with, that and I also know that the characteristics (indicator) function is defined as So I wrote out in some of its `glory': , and as the first argument is the smallest, with increasing size to the right, the last term in the expression for would be , which would be the largest. So if I replace the 's above with 's I still don't see how I can get the correct answer - though it looks pretty clear from the definition of and that of being basically the same, except in this case for the . Any direction would be greatly appreciated. By the way, I have checked out limsup and liminf of a sequence of subsets of a set but I was somewhat confused by the topology, the meets/joins, etc. Thanks much, Nate","X \{A_k\} X B = \bigcup_{n=1}^{+\infty} \bigcap_{k=n}^{+\infty} A_k C = \bigcap_{n=1}^{+\infty} \bigcup_{k=n}^{+\infty} A_k \liminf_k\; {\chi_A}_{_k} = \chi_B (b) \limsup_k \;{\chi_A}_{_k} = \chi_C. \liminf_{k\to +\infty}\; X_k = \bigcup_{k=1}^{+\infty} \bigcap_{n=k}^{+\infty} X_n \limsup_{k\to +\infty}\;X_k = \bigcap_{k=1}^{+\infty} \bigcup_{n=k}^{+\infty} X_n. \chi_A(x) = \begin{cases} 1, & x \in A \\ 0, & x \notin A .\end{cases}  B B= (A_1 \cap A_2 \cap A_3 \cap \cdots) \cup (A_2 \cap A_3 \cap \cdots) \cup (A_3 \cap A_4 \cap \cdots) \cup \cdots B B X_k \chi B \liminf \chi","['real-analysis', 'elementary-set-theory', 'limsup-and-liminf', 'measurable-functions']"
91,"For an integrable function $f$, do continuity conditions on its integral affect continuity of $f$?","For an integrable function , do continuity conditions on its integral affect continuity of ?",f f,"In this question, whenever I say ""integrable"" I mean ""Riemann-integrable"". The (first) fundamental theorem of calculus states: if $f\in C[a,b]$, then the function $F(x)=\int_a^x f(t)\, dt$ is continuous in $[a,b]$, differentiable in $(a,b)$ and $F'(x)=f(x)$ in $(a,b)$. Now, a (sort of?) converse would read: if $f$ is an integrable function in $[a,b]$ such that the function $F(x)=\int_a^x f(t)\, dt$ is $C^1[a,b]$, then $f$ is continuous. This is not true, as the answers below put it, it suffices to alter a continuous function on a point, which does not alter the $C^1$-ness of its integral. Now I ask this: are these all the counterexamples? Does there exist a counterexample which is not of this kind, i.e. a continuous function save for a removable discontinuity? To clarify: a function $f:[a,b]\to \mathbb{R}$ is said to have a removable discontinuity in a point $c\in [a,b]$ if both one-sided limits (or the one that is defined, in case of $a$ or $b$) exist and are equal.","In this question, whenever I say ""integrable"" I mean ""Riemann-integrable"". The (first) fundamental theorem of calculus states: if $f\in C[a,b]$, then the function $F(x)=\int_a^x f(t)\, dt$ is continuous in $[a,b]$, differentiable in $(a,b)$ and $F'(x)=f(x)$ in $(a,b)$. Now, a (sort of?) converse would read: if $f$ is an integrable function in $[a,b]$ such that the function $F(x)=\int_a^x f(t)\, dt$ is $C^1[a,b]$, then $f$ is continuous. This is not true, as the answers below put it, it suffices to alter a continuous function on a point, which does not alter the $C^1$-ness of its integral. Now I ask this: are these all the counterexamples? Does there exist a counterexample which is not of this kind, i.e. a continuous function save for a removable discontinuity? To clarify: a function $f:[a,b]\to \mathbb{R}$ is said to have a removable discontinuity in a point $c\in [a,b]$ if both one-sided limits (or the one that is defined, in case of $a$ or $b$) exist and are equal.",,"['calculus', 'real-analysis']"
92,Find $\int_0^1 \frac{f(x)}{\sqrt{1+x^2}}dx$,Find,\int_0^1 \frac{f(x)}{\sqrt{1+x^2}}dx,"Let $f(x)$ be continuous on $[0;1]$ , with $f(0) = 0; f(1) = 1$ and $$\int_0^1 [f'(x)]^2 \sqrt{1+x^2} \,dx = \dfrac{1}{\ln\left(1+\sqrt{2}\right)}$$ Find ${\displaystyle \int_0^1} \dfrac{f(x)}{\sqrt{1+x^2}} \,dx$ Attempt: I tried to use Cauchy-Scharwz as below: $$\int_0^1 [f'(x)]^2 \sqrt{1+x^2} \,dx \cdot \int_0^1 \dfrac{f(x)}{\sqrt{1+x^2}} \,dx \geq \left(\int_0^1 \sqrt{[f'(x)]^2 \sqrt{1+x^2}} \cdot \sqrt{\dfrac{f(x)}{\sqrt{1+x^2}}} \,dx\right)^2$$ $$\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,=\left(\int_0^1 f'(x) \sqrt{f(x)} \,dx\right)^2$$ I was able to find ${\displaystyle \int_0^1} f'(x) \sqrt{f(x)} \,dx = \dfrac{2}{3}$ , but the problem is I can't show if the equality is happen or not, so my attempt isn't helpful at all. Is there a better way to approach this?","Let be continuous on , with and Find Attempt: I tried to use Cauchy-Scharwz as below: I was able to find , but the problem is I can't show if the equality is happen or not, so my attempt isn't helpful at all. Is there a better way to approach this?","f(x) [0;1] f(0) = 0; f(1) = 1 \int_0^1 [f'(x)]^2 \sqrt{1+x^2} \,dx = \dfrac{1}{\ln\left(1+\sqrt{2}\right)} {\displaystyle \int_0^1} \dfrac{f(x)}{\sqrt{1+x^2}} \,dx \int_0^1 [f'(x)]^2 \sqrt{1+x^2} \,dx \cdot \int_0^1 \dfrac{f(x)}{\sqrt{1+x^2}} \,dx \geq \left(\int_0^1 \sqrt{[f'(x)]^2 \sqrt{1+x^2}} \cdot \sqrt{\dfrac{f(x)}{\sqrt{1+x^2}}} \,dx\right)^2 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,=\left(\int_0^1 f'(x) \sqrt{f(x)} \,dx\right)^2 {\displaystyle \int_0^1} f'(x) \sqrt{f(x)} \,dx = \dfrac{2}{3}","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
93,prove $\int_0^\infty \frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}{8}$ with real methods,prove  with real methods,\int_0^\infty \frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}{8},"Context: I looked up ""complex residue"" on google images, and saw this integral. I, being unfamiliar with the use of contour integration, decided to try proving the result without complex analysis. Seeing as I was stuck, I decided to ask you for help. I am attempting to prove that $$J=\int_0^\infty\frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}8$$ With real methods because I do not know complex analysis. I have started with the substitution $x=\tan u$ : $$J=\int_0^{\pi/2}\log^2(\tan x)\mathrm dx$$ $$J=\int_0^{\pi/2}\log^2(\cos x)\mathrm dx-2\int_{0}^{\pi/2}\log(\cos x)\log(\sin x)\mathrm dx+\int_0^{\pi/2}\log^2(\sin x)\mathrm dx$$ But frankly, this is basically worse. Could I have some help? Thanks. Update: Wait I think I actually found a viable method $$F(\alpha)=\int_0^\infty \frac{x^{\alpha}}{x^2+1}\mathrm dx$$ As I have shown in other posts of mine, $$\int_0^\infty\frac{x^{2b-1}}{(1+x^2)^{a+b}}\mathrm dx=\frac12\mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{2\Gamma(a+b)}$$ so $$F(\alpha)=\frac12\Gamma\left(\frac{1+\alpha}2\right)\Gamma\left(\frac{1-\alpha}2\right)$$ And from $$\Gamma(1-s)\Gamma(s)=\frac\pi{\sin \pi s}$$ we see that $$F(\alpha)=\frac\pi{2\cos\frac{\pi \alpha}{2}}$$ So $$J=F''(0)=\frac{\pi^3}8$$ Okay while I have just found a proof, I would like to see which ways you did it.","Context: I looked up ""complex residue"" on google images, and saw this integral. I, being unfamiliar with the use of contour integration, decided to try proving the result without complex analysis. Seeing as I was stuck, I decided to ask you for help. I am attempting to prove that With real methods because I do not know complex analysis. I have started with the substitution : But frankly, this is basically worse. Could I have some help? Thanks. Update: Wait I think I actually found a viable method As I have shown in other posts of mine, so And from we see that So Okay while I have just found a proof, I would like to see which ways you did it.","J=\int_0^\infty\frac{\log^2(x)}{x^2+1}\mathrm dx=\frac{\pi^3}8 x=\tan u J=\int_0^{\pi/2}\log^2(\tan x)\mathrm dx J=\int_0^{\pi/2}\log^2(\cos x)\mathrm dx-2\int_{0}^{\pi/2}\log(\cos x)\log(\sin x)\mathrm dx+\int_0^{\pi/2}\log^2(\sin x)\mathrm dx F(\alpha)=\int_0^\infty \frac{x^{\alpha}}{x^2+1}\mathrm dx \int_0^\infty\frac{x^{2b-1}}{(1+x^2)^{a+b}}\mathrm dx=\frac12\mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{2\Gamma(a+b)} F(\alpha)=\frac12\Gamma\left(\frac{1+\alpha}2\right)\Gamma\left(\frac{1-\alpha}2\right) \Gamma(1-s)\Gamma(s)=\frac\pi{\sin \pi s} F(\alpha)=\frac\pi{2\cos\frac{\pi \alpha}{2}} J=F''(0)=\frac{\pi^3}8","['real-analysis', 'calculus', 'integration']"
94,Inner regularity of Lebesgue measurable sets,Inner regularity of Lebesgue measurable sets,,"This is an exercise in real analysis: Let $E\subset{\Bbb R}^d$ be Lebesgue measurable. Show that    $$ m(E)=\sup\{m(K):K\subset E, K \text{compact}\}. $$ When $E$ is bounded, this can be done by the following proposition: $E\subset{\Bbb R}^d$ is Lebesgue measurable if and only if for every $\varepsilon>0$, one can find a closed set $F$ contained in $E$ with $m^*(E\setminus F)\leq\varepsilon$. How can I deal with the case that $E$ is unbounded?","This is an exercise in real analysis: Let $E\subset{\Bbb R}^d$ be Lebesgue measurable. Show that    $$ m(E)=\sup\{m(K):K\subset E, K \text{compact}\}. $$ When $E$ is bounded, this can be done by the following proposition: $E\subset{\Bbb R}^d$ is Lebesgue measurable if and only if for every $\varepsilon>0$, one can find a closed set $F$ contained in $E$ with $m^*(E\setminus F)\leq\varepsilon$. How can I deal with the case that $E$ is unbounded?",,['real-analysis']
95,Finding $\displaystyle \lim_{n \to \infty} \int_0^\infty \frac{e^{-x}\cos{x}}{nx^2 + \frac{1}{n}}dx$,Finding,\displaystyle \lim_{n \to \infty} \int_0^\infty \frac{e^{-x}\cos{x}}{nx^2 + \frac{1}{n}}dx,Prove that $\displaystyle \lim_{n \to \infty} \int_0^\infty \frac{e^{-x}\cos{x}}{nx^2 + \frac{1}{n}}dx$ exists and determine its value.,Prove that $\displaystyle \lim_{n \to \infty} \int_0^\infty \frac{e^{-x}\cos{x}}{nx^2 + \frac{1}{n}}dx$ exists and determine its value.,,"['real-analysis', 'measure-theory']"
96,What's the component interval?,What's the component interval?,,"In Apostol, page $51$, he defines what he calls the component interval. I can't find any reference to it on the web. I have some problems with the definition: Let $S \subseteq \mathbb{R}$. An open interval $I$ of $S$ is a component interval if there does not exist an open interval $J$ of $S$ such that $I \subset J$. Intuitively, I get that $I$ is the largest possible open interval that is contained in $S$. I think that the set of all rationals between the end points of $S$, $\{\alpha \in (A,B)\ |\ \alpha \in \mathbb{Q}\}$, is a component interval. Is that true? If $D$ is dense in $S$, is $D$ in general a component interval of $S$?","In Apostol, page $51$, he defines what he calls the component interval. I can't find any reference to it on the web. I have some problems with the definition: Let $S \subseteq \mathbb{R}$. An open interval $I$ of $S$ is a component interval if there does not exist an open interval $J$ of $S$ such that $I \subset J$. Intuitively, I get that $I$ is the largest possible open interval that is contained in $S$. I think that the set of all rationals between the end points of $S$, $\{\alpha \in (A,B)\ |\ \alpha \in \mathbb{Q}\}$, is a component interval. Is that true? If $D$ is dense in $S$, is $D$ in general a component interval of $S$?",,['real-analysis']
97,How is the Riemann integral a special case of the Stieltjes integral?,How is the Riemann integral a special case of the Stieltjes integral?,,"From Rudin's Principles of mathematical analysis , 6.2 Definition Let $\alpha$ be a monotonically increasing function on $[a,b]$ . ... Corresponding to each partition $P$ of $[a,b]$ , we write $$\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}).$$ He then goes on to define the Riemann Stieltjes integral of $f$ with respect to $\alpha$ , over the interval $[a,b]$ . The Riemann integral is then pointed out to be a special case of this when $\alpha(x)=x$ . With $\alpha(x)=x$ , I understand $\Delta x = x_i - x_{i-1}$ to represent the directed magnitude of the ""base of the approximating rectangle"" that we then multiply by the value of $f$ taken somewhere within this interval, thus obtaining the area of an approximating rectangle. I don't know where to begin to interpret the case where $\alpha(x) \not\equiv x$ .","From Rudin's Principles of mathematical analysis , 6.2 Definition Let be a monotonically increasing function on . ... Corresponding to each partition of , we write He then goes on to define the Riemann Stieltjes integral of with respect to , over the interval . The Riemann integral is then pointed out to be a special case of this when . With , I understand to represent the directed magnitude of the ""base of the approximating rectangle"" that we then multiply by the value of taken somewhere within this interval, thus obtaining the area of an approximating rectangle. I don't know where to begin to interpret the case where .","\alpha [a,b] P [a,b] \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}). f \alpha [a,b] \alpha(x)=x \alpha(x)=x \Delta x = x_i - x_{i-1} f \alpha(x) \not\equiv x","['real-analysis', 'riemann-integration', 'stieltjes-integral']"
98,"""Let"" in math texts","""Let"" in math texts",,"I'm trying to do the following exercise for my Real Analysis class: Let $p$ be a given natural number. Give an example of a sequence $\left(x_{n}\right)$ that is not a Cauchy sequence, but that satisfies $\lim \left|x_{n+p}-x_{n}\right|=0$ However, I am in constant doubt in regard to the ""let"" word in math texts. Can I choose, say $p = 1$ , or when one says ""let"" I am supposed to stick with $p \in \mathbb{N}$ and nothing more? What kind of ""control"" do I have over $p$ ? For instance, when one says ""Given $\epsilon \gt 0$ "", I usually see things like: Let $\epsilon = \frac{\epsilon}{2}$ so one can finish a certain argument. Can someone help me? I need to solve this doubt once and for all, it bothers me very often.","I'm trying to do the following exercise for my Real Analysis class: Let be a given natural number. Give an example of a sequence that is not a Cauchy sequence, but that satisfies However, I am in constant doubt in regard to the ""let"" word in math texts. Can I choose, say , or when one says ""let"" I am supposed to stick with and nothing more? What kind of ""control"" do I have over ? For instance, when one says ""Given "", I usually see things like: Let so one can finish a certain argument. Can someone help me? I need to solve this doubt once and for all, it bothers me very often.",p \left(x_{n}\right) \lim \left|x_{n+p}-x_{n}\right|=0 p = 1 p \in \mathbb{N} p \epsilon \gt 0 \epsilon = \frac{\epsilon}{2},"['real-analysis', 'proof-writing']"
99,Is it true that a continuous function with compact support is uniformly continuous?,Is it true that a continuous function with compact support is uniformly continuous?,,"I've been trying to prove the given $f:\mathbb R\rightarrow \mathbb C$ continuous with compact support, $f$ is uniformly continuous. I don't know if it's true or not, but it is highly plausible and it's interesting. It's obvious that it's uniformly continuous on the support (because it is continuous on compact set), and outside the support (cause it's constant), separately. Could I use the continuity to show that it's uniformly continuous over all of $\mathbb R$?","I've been trying to prove the given $f:\mathbb R\rightarrow \mathbb C$ continuous with compact support, $f$ is uniformly continuous. I don't know if it's true or not, but it is highly plausible and it's interesting. It's obvious that it's uniformly continuous on the support (because it is continuous on compact set), and outside the support (cause it's constant), separately. Could I use the continuity to show that it's uniformly continuous over all of $\mathbb R$?",,"['real-analysis', 'complex-analysis']"
