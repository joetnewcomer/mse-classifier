,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $\det(AB-BA) × \det(AC-CA) \geq 0$ if $A^2 = -BC$,Show that  if,\det(AB-BA) × \det(AC-CA) \geq 0 A^2 = -BC,"We have $A,B,C$ three $n×n$ matrices with real entries. We know that $$   A^2 = -BC $$ and we want to show that $$   \det(AB-BA) × \det(AC-CA) \geq 0 \,. $$ We can easily show that for $n=2k$ we have $\det(B) × \det(C) \geq 0$ , and for $n = 2k + 1$ we have $\det(B) × \det(C) \leq0$ . Then for $n = 2k$ we can expand $\det(AB-BA)$ like $$   2 × \det(A) × \det(B) - c_1 + c_2 - c_3 + c_4 - \dotsb +c_{n-2} - c_{n-1} $$ and expand $\det(AC-CA)$ like $$   2 × \det(A) × \det(C) - d_1 + d_2 - d_3 + d_4 - \dotsb + d_{n-2} - d_{n-1} \,. $$ For $n = 2k + 1$ , we can expand $\det(AB - BA)$ like $$   c_1 - c_2 + c_3 - c_4 + \dotsb + c_{n-2} - c_{n-1} $$ and $\det(AC - CA)$ like $$   d_1 - d_2 + d_3 - d_4 + \dotsb + d_{n-2} - d_{n-1} \,. $$ If we multiply these two expanded forms in every case $n = 2k$ and $n = 2k+1$ , then we get an ugly answer and I don’t know we can check the sign like that. I am now stuck. Maybe we need to use eigenvalues? Please someone help me solve this problem.","We have three matrices with real entries. We know that and we want to show that We can easily show that for we have , and for we have . Then for we can expand like and expand like For , we can expand like and like If we multiply these two expanded forms in every case and , then we get an ugly answer and I don’t know we can check the sign like that. I am now stuck. Maybe we need to use eigenvalues? Please someone help me solve this problem.","A,B,C n×n 
  A^2 = -BC
 
  \det(AB-BA) × \det(AC-CA) \geq 0 \,.
 n=2k \det(B) × \det(C) \geq 0 n = 2k + 1 \det(B) × \det(C) \leq0 n = 2k \det(AB-BA) 
  2 × \det(A) × \det(B) - c_1 + c_2 - c_3 + c_4 - \dotsb +c_{n-2} - c_{n-1}
 \det(AC-CA) 
  2 × \det(A) × \det(C) - d_1 + d_2 - d_3 + d_4 - \dotsb + d_{n-2} - d_{n-1} \,.
 n = 2k + 1 \det(AB - BA) 
  c_1 - c_2 + c_3 - c_4 + \dotsb + c_{n-2} - c_{n-1}
 \det(AC - CA) 
  d_1 - d_2 + d_3 - d_4 + \dotsb + d_{n-2} - d_{n-1} \,.
 n = 2k n = 2k+1","['matrices', 'eigenvalues-eigenvectors', 'determinant', 'matrix-decomposition', 'characteristic-polynomial']"
1,Expected determinant of a random symmetric matrix,Expected determinant of a random symmetric matrix,,"The three distinct entries of a $2 \times 2$ symmetric matrix are drawn from the uniform distribution over $[-60, 60]$. What is the expected determinant of the matrix? I assume it is $0$ but I am not good at proving it efficiently. Thanks.","The three distinct entries of a $2 \times 2$ symmetric matrix are drawn from the uniform distribution over $[-60, 60]$. What is the expected determinant of the matrix? I assume it is $0$ but I am not good at proving it efficiently. Thanks.",,"['matrices', 'determinant', 'uniform-distribution', 'random-matrices', 'symmetric-matrices']"
2,Tensor Notation Upper and Lower Indices,Tensor Notation Upper and Lower Indices,,"I want to ask what the difference between the tensors $T_i^{\; j}$ , $T_j^{\; i}$ , $T_{\; i}^{ j}$ , and $T_{\;i}^{j}$ are. In particular I am asking about the matrix representations of these tensors and their relationships.","I want to ask what the difference between the tensors $T_i^{\; j}$ , $T_j^{\; i}$ , $T_{\; i}^{ j}$ , and $T_{\;i}^{j}$ are. In particular I am asking about the matrix representations of these tensors and their relationships.",,"['matrices', 'tensors']"
3,Can A be singular? [duplicate],Can A be singular? [duplicate],,This question already has answers here : Can $A$ be singular (3 answers) Closed 9 years ago . Let $A\in \mathbb{C}^{n\times n}$ satisfy $$A^{2}+A+I=0 $$ Can A be singular? So I have: $$ (A-I)(A^{2}+A+I)=0\\ A^{3} = I \\ (\det A^{3}) = \det(I) \\ (\det A)^{3} = 1\\ \det A\neq 0 $$ So $A$ is not singular. Is this correct the way I've done this?,This question already has answers here : Can $A$ be singular (3 answers) Closed 9 years ago . Let $A\in \mathbb{C}^{n\times n}$ satisfy $$A^{2}+A+I=0 $$ Can A be singular? So I have: $$ (A-I)(A^{2}+A+I)=0\\ A^{3} = I \\ (\det A^{3}) = \det(I) \\ (\det A)^{3} = 1\\ \det A\neq 0 $$ So $A$ is not singular. Is this correct the way I've done this?,,"['matrices', 'inverse']"
4,Eigenvectors of a Rotation Matrix,Eigenvectors of a Rotation Matrix,,"The eigenvector of the rotation matrix corresponding to eigenvalue 1 is the axis of rotation. The remaining eigenvalues are complex conjugates of each other and so are the corresponding eigenvectors. The two complex eigenvectors can be manipulated to determine a plane perpendicular to the first real eigen vector. I did this in matlab for many non-diagonal, non-identity matrices and found that eigenvectors were both unit and mutually orthonormal. I dont think this is a coincidence. Why is it so? Section 3.1 of the link below:  robotics.caltech.edu/~jwb/courses/ME115/handouts/rotation.pdf","The eigenvector of the rotation matrix corresponding to eigenvalue 1 is the axis of rotation. The remaining eigenvalues are complex conjugates of each other and so are the corresponding eigenvectors. The two complex eigenvectors can be manipulated to determine a plane perpendicular to the first real eigen vector. I did this in matlab for many non-diagonal, non-identity matrices and found that eigenvectors were both unit and mutually orthonormal. I dont think this is a coincidence. Why is it so? Section 3.1 of the link below:  robotics.caltech.edu/~jwb/courses/ME115/handouts/rotation.pdf",,"['matrices', 'eigenvalues-eigenvectors', 'rotations', 'orthogonality']"
5,Positive semidefinite cone is generated by all rank-$1$ matrices.,Positive semidefinite cone is generated by all rank- matrices.,1,"The positive semidefinite cone is generated by all rank-$1$ matrices $xx^T$, which form the extreme rays of the cone. Positive definite matrices lie in the interior of the cone. Positive semidefinite matrices with at least one zero eigenvalue are on the boundary. I am unable to justify why the statements above are true. Please give some hints.","The positive semidefinite cone is generated by all rank-$1$ matrices $xx^T$, which form the extreme rays of the cone. Positive definite matrices lie in the interior of the cone. Positive semidefinite matrices with at least one zero eigenvalue are on the boundary. I am unable to justify why the statements above are true. Please give some hints.",,"['matrices', 'positive-definite', 'symmetric-matrices', 'positive-semidefinite']"
6,How to find the orthonormal transformation that will rotate a vector to the x axis?,How to find the orthonormal transformation that will rotate a vector to the x axis?,,I am having trouble remembering linear algebra. I need to find the orthonormal transformation that will rotate a 3-dimensional vector to the x axis. I could not find any similar question on the net. Any tips?,I am having trouble remembering linear algebra. I need to find the orthonormal transformation that will rotate a 3-dimensional vector to the x axis. I could not find any similar question on the net. Any tips?,,"['matrices', 'transformation', 'orthonormal']"
7,Matrix Differential Equation with a Skew-Symmetric Matrix,Matrix Differential Equation with a Skew-Symmetric Matrix,,"From a bank of masters exams: Say the position of a particle moving   in $\mathbb{R}^n$ is given by a smooth   vector-valued function $\vec{x}(t)$.   Suppose that $\vec{x}(t)$ satisfies a   differential equation,   $$ \frac{d\vec{x}}{dt} = A(t)\vec{x},$$     where $A(t)$ is a   real anti-symmetric matrix depending   smoothly on $t$. Show that this   particle moves on a sphere, that is,   $||\vec{x}(t)||$ is constant. By the spectral theorem, $A$ is normal and therefore has a complete basis of eigenvectors in $\mathbb{C}^n$. I am familiar with the ""standard"" method of solving for matrix exponentials, i.e. finding the eigenvalues and eigenvectors of $A$, and then using linear combinations of $e^{\lambda t}\vec{x}$ as the solutions, but there is not a complete basis of eigenvectors in $\mathbb{R}$. Taking the matrix exponential $e^A$ doesn't seem to do anything.","From a bank of masters exams: Say the position of a particle moving   in $\mathbb{R}^n$ is given by a smooth   vector-valued function $\vec{x}(t)$.   Suppose that $\vec{x}(t)$ satisfies a   differential equation,   $$ \frac{d\vec{x}}{dt} = A(t)\vec{x},$$     where $A(t)$ is a   real anti-symmetric matrix depending   smoothly on $t$. Show that this   particle moves on a sphere, that is,   $||\vec{x}(t)||$ is constant. By the spectral theorem, $A$ is normal and therefore has a complete basis of eigenvectors in $\mathbb{C}^n$. I am familiar with the ""standard"" method of solving for matrix exponentials, i.e. finding the eigenvalues and eigenvectors of $A$, and then using linear combinations of $e^{\lambda t}\vec{x}$ as the solutions, but there is not a complete basis of eigenvectors in $\mathbb{R}$. Taking the matrix exponential $e^A$ doesn't seem to do anything.",,"['matrices', 'ordinary-differential-equations', 'spectral-theory']"
8,Prove $\det((AB)^{n}-(BA)^{n})$ is a perfect cube.,Prove  is a perfect cube.,\det((AB)^{n}-(BA)^{n}),"We have $A,B$ two $3×3$ matrices with integer numbers. We know that $(AB)^{2}+BA=(BA)^2+AB$ . a) Show that $\det((AB)^{n}-(BA)^{n})$ is divisible by $det(AB-BA)$ . b) Show that if $\det(AB-BA)=1$ , then $\det((AB)^{n}-(BA)^{n})$ is a perfect cube. I have tried taking the trace in the first  equality but nothing interesting. Maybe will help us rewriting $\det(AB-BA)$ as $Tr((AB-BA)^{3})/3$ ? It feels like we need to use some induction here but I dont know what is the ""induction general form"". I see we can rewrite $\det(AB-BA)=\det((AB)^{2}-(BA)^{2})$ . I think that perfect cube will come from a determinant rewriting in polynomial form, but I dont know how we can rewrite it as a polynomial.","We have two matrices with integer numbers. We know that . a) Show that is divisible by . b) Show that if , then is a perfect cube. I have tried taking the trace in the first  equality but nothing interesting. Maybe will help us rewriting as ? It feels like we need to use some induction here but I dont know what is the ""induction general form"". I see we can rewrite . I think that perfect cube will come from a determinant rewriting in polynomial form, but I dont know how we can rewrite it as a polynomial.","A,B 3×3 (AB)^{2}+BA=(BA)^2+AB \det((AB)^{n}-(BA)^{n}) det(AB-BA) \det(AB-BA)=1 \det((AB)^{n}-(BA)^{n}) \det(AB-BA) Tr((AB-BA)^{3})/3 \det(AB-BA)=\det((AB)^{2}-(BA)^{2})","['matrices', 'determinant', 'matrix-decomposition', 'characteristic-polynomial', 'cayley-hamilton']"
9,"I heard that tensors are a generalization of scalars, vectors, and matrices. But tensors don't look like matrices at all.","I heard that tensors are a generalization of scalars, vectors, and matrices. But tensors don't look like matrices at all.",,"I am reading ""Analysis on Manifolds"" by James R. Munkres. Definition: Let $V$ be a vector space. Let $V^k = V \times \cdots \times V$ denote the set of all $k$ -tuples $(v_1, \cdots, v_k)$ of vectors of $V$ . A function $f : V^k \to \mathbb{R}$ is said to be linear in the $i$ th variable if, given fixed vectors $v_j$ for $j \ne i$ , the function $T : V \to \mathbb{R}$ defined by $$T(v) = f(v_1, \cdots, v_{i-1}, v, v_{i+1}, \cdots, v_k)$$ is linear. The function $f$ is said to be multilinear if it is linear in the $i$ th variable for each $i$ . Such a function $f$ is also called a $k$ -tensor, or a tensor of order $k$ , on $V$ . This is the definition of tensors. I heard that tensors are a generalization of scalars, vectors, and matrices. But tensors don't look like scalars, vectors, and matrices at all. For example, please show me a tensor which corresponds to a matrix.","I am reading ""Analysis on Manifolds"" by James R. Munkres. Definition: Let be a vector space. Let denote the set of all -tuples of vectors of . A function is said to be linear in the th variable if, given fixed vectors for , the function defined by is linear. The function is said to be multilinear if it is linear in the th variable for each . Such a function is also called a -tensor, or a tensor of order , on . This is the definition of tensors. I heard that tensors are a generalization of scalars, vectors, and matrices. But tensors don't look like scalars, vectors, and matrices at all. For example, please show me a tensor which corresponds to a matrix.","V V^k = V \times \cdots \times V k (v_1, \cdots, v_k) V f : V^k \to \mathbb{R} i v_j j \ne i T : V \to \mathbb{R} T(v) = f(v_1, \cdots, v_{i-1}, v, v_{i+1}, \cdots, v_k) f i i f k k V","['matrices', 'tensors', 'multilinear-algebra']"
10,Symmetric Grothendieck inequality,Symmetric Grothendieck inequality,,"Grothendieck's inequality states that for all $n \times n$ matrices $(a_{ij})$ such that $$\max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij}\, x_i\, y_j\right| \leq 1,$$ there exists a universal constant $K$ , such that for $u_i, v_j$ in any Hilbert space, $$ \max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \langle u_i , v_j \rangle \right| \leq K. $$ I would like to prove the symmetric statement.  For all symmetric matrices $(a_{ij})$ such that $$ \max_{x \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \,x_i\, x_j\right| \leq 1,  $$ there exists a universal constant such that $$ \max_{x \in \{\pm 1\}^n} \left|\sum_{i,j} a_{ij} \langle u_i , v_j \rangle \right| \leq 2K $$ for $u_i, v_j$ in any Hilbert space.  This should be a consequence of the original inequality.  I tried to use the polarization identity $$ \langle Ax, y\rangle = \langle Au, u\rangle - \langle Av, v \rangle $$ where $u = (x+y)/2$ and $ v = (x-y)/2$ .  However, as $x$ and $y$ vary over $\pm 1$ vectors, $u$ and $v$ can be vectors in $\{\pm 1, 0\}$ .","Grothendieck's inequality states that for all matrices such that there exists a universal constant , such that for in any Hilbert space, I would like to prove the symmetric statement.  For all symmetric matrices such that there exists a universal constant such that for in any Hilbert space.  This should be a consequence of the original inequality.  I tried to use the polarization identity where and .  However, as and vary over vectors, and can be vectors in .","n \times n (a_{ij}) \max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij}\, x_i\, y_j\right| \leq 1, K u_i, v_j 
\max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \langle u_i , v_j \rangle \right| \leq K.
 (a_{ij}) 
\max_{x \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \,x_i\, x_j\right| \leq 1, 
 
\max_{x \in \{\pm 1\}^n} \left|\sum_{i,j} a_{ij} \langle u_i , v_j \rangle \right| \leq 2K
 u_i, v_j 
\langle Ax, y\rangle = \langle Au, u\rangle - \langle Av, v \rangle
 u = (x+y)/2  v = (x-y)/2 x y \pm 1 u v \{\pm 1, 0\}","['matrices', 'inequality', 'optimization']"
11,Conditional Number growth of Hilbert Matrix: Theoretical vs MatLab.,Conditional Number growth of Hilbert Matrix: Theoretical vs MatLab.,,"I need to investigate how the condition number of the Hilbert matrix grows with the size N. The Matlab command is: ""cond(hilb(N),2)"" Compute the condition number of the Hilbert matrices Hn ∈ R, N×N, for all N = 1, . . . , 50. Using Matlab to calculate the log of condition number of Hn versus N. (The blue 'x') Compare this with the anticipated theoretical growth (The red line) of $$O\left(\frac{(1+\sqrt{2})^{4N}}{\sqrt{N}}\right) $$ I got a plot like this: When N = 13, the Condition Number reaches the maximum. The Condition Number does not continue to grow when N>13. Why does the Condition Number stop growing after N=13? % generate Hilbert matrices and compute cond number with 2-norm  N=50; % maximum size of a matrix condofH = []; % conditional number of Hilbert Matrix N_it= zeros(1,N);    % compute the cond number of Hn for n = 1:N     Hn = hilb(n);     N_it(n)=n;     condofH = [condofH cond(Hn,2)]; end  % at this point we have a vector condofH that contains the condition % number of the Hilber matrices from 1x1 to 50x50. % plot on the same graph the theoretical growth line.    % Theoretical growth of condofH x = 1:50; y = (1+sqrt(2)).^(4*x)./(sqrt(x));  % plot plot(N_it, log(y)); plot(N_it, log(condofH),'x', N_it,log(y));   % plot labels plot(N_it, log(condofH),'x', N_it,log(y)) title('Conditional Number growth of Hilbert Matrix: Theoretical vs Matlab') xlabel('N', 'fontsize', 16) ylabel('log(cond(Hn))','fontsize', 16) lgd = legend ('Location', 'northwest') legend('MatLab', 'Theoretical') legend('show')","I need to investigate how the condition number of the Hilbert matrix grows with the size N. The Matlab command is: ""cond(hilb(N),2)"" Compute the condition number of the Hilbert matrices Hn ∈ R, N×N, for all N = 1, . . . , 50. Using Matlab to calculate the log of condition number of Hn versus N. (The blue 'x') Compare this with the anticipated theoretical growth (The red line) of $$O\left(\frac{(1+\sqrt{2})^{4N}}{\sqrt{N}}\right) $$ I got a plot like this: When N = 13, the Condition Number reaches the maximum. The Condition Number does not continue to grow when N>13. Why does the Condition Number stop growing after N=13? % generate Hilbert matrices and compute cond number with 2-norm  N=50; % maximum size of a matrix condofH = []; % conditional number of Hilbert Matrix N_it= zeros(1,N);    % compute the cond number of Hn for n = 1:N     Hn = hilb(n);     N_it(n)=n;     condofH = [condofH cond(Hn,2)]; end  % at this point we have a vector condofH that contains the condition % number of the Hilber matrices from 1x1 to 50x50. % plot on the same graph the theoretical growth line.    % Theoretical growth of condofH x = 1:50; y = (1+sqrt(2)).^(4*x)./(sqrt(x));  % plot plot(N_it, log(y)); plot(N_it, log(condofH),'x', N_it,log(y));   % plot labels plot(N_it, log(condofH),'x', N_it,log(y)) title('Conditional Number growth of Hilbert Matrix: Theoretical vs Matlab') xlabel('N', 'fontsize', 16) ylabel('log(cond(Hn))','fontsize', 16) lgd = legend ('Location', 'northwest') legend('MatLab', 'Theoretical') legend('show')",,"['matrices', 'matlab', 'hilbert-matrices']"
12,Properties of entrywise L1 matrix norm,Properties of entrywise L1 matrix norm,,"Consider the entrywise $L_1$ norm on matrices, given by $$\|M\|_1 = \sum_{i,j} |M_{i,j}|.$$ I'm looking for useful properties this norm might have.  Is there anything we can say about $\|A \cdot B \|_1$, in terms of the matrices $A,B$? (e.g., upper-bound $\|A \cdot B\|_1$, based on some quantities about $A$ and $B$?)","Consider the entrywise $L_1$ norm on matrices, given by $$\|M\|_1 = \sum_{i,j} |M_{i,j}|.$$ I'm looking for useful properties this norm might have.  Is there anything we can say about $\|A \cdot B \|_1$, in terms of the matrices $A,B$? (e.g., upper-bound $\|A \cdot B\|_1$, based on some quantities about $A$ and $B$?)",,"['matrices', 'normed-spaces']"
13,Show that the Area of image = Area of object $\cdot |\det(T)|$? Where $T$ is a linear transformation from $R^2 \rightarrow R^2$,Show that the Area of image = Area of object ? Where  is a linear transformation from,\cdot |\det(T)| T R^2 \rightarrow R^2,"Prove that the area of an image in $2d$ cartesian coordinates is equal to the determinant of the linear transformation times the area of the initial shape. I've tried to formulate general expression for area given lots of points, but it feels like that's barking up the wrong tree. I've also proved it for transformations which are combinations of rotations and enlargements. If that is the case, the distance between each point in the shape will increase by a constant, which has to be the same no matter what the initial shape is. So we can take a unit square which is the easiest case, and it is trivial to show that the resulting area is $\det(T)$. But because these transformations stretch lengths by the same constant, it must stretch areas by the same constant - $\det(T)$. Similarly, is it possible to show that all linear transformations (i.e. shears and compressions) have some property that allows us to deduce that the area of the image must transform by some constant for any image?","Prove that the area of an image in $2d$ cartesian coordinates is equal to the determinant of the linear transformation times the area of the initial shape. I've tried to formulate general expression for area given lots of points, but it feels like that's barking up the wrong tree. I've also proved it for transformations which are combinations of rotations and enlargements. If that is the case, the distance between each point in the shape will increase by a constant, which has to be the same no matter what the initial shape is. So we can take a unit square which is the easiest case, and it is trivial to show that the resulting area is $\det(T)$. But because these transformations stretch lengths by the same constant, it must stretch areas by the same constant - $\det(T)$. Similarly, is it possible to show that all linear transformations (i.e. shears and compressions) have some property that allows us to deduce that the area of the image must transform by some constant for any image?",,"['matrices', 'determinant', 'linear-transformations', 'area']"
14,How to find a onto homomorphism between two groups?,How to find a onto homomorphism between two groups?,,"Consider the following subgroups of $\text{SL}(2,\mathbb{Z})$ : $A$ the subgroup of  matrices with determinant $1$ : \begin{bmatrix}4\mathbb{Z}+1&8\mathbb{Z}\\4\mathbb{Z}&4\mathbb{Z}+1\end{bmatrix} $B$ the subgroup of  matrices with determinant $1$ : \begin{bmatrix}2\mathbb{Z}+1&8\mathbb{Z}\\4\mathbb{Z}&2\mathbb{Z}+1\end{bmatrix} I want some onto homomorphism from $B$ to $A$ whose kernel is \begin{bmatrix}1&0\\0&1\end{bmatrix}\begin{bmatrix}-1&0\\0&-1\end{bmatrix} How to get this? I have no idea how to find the map.","Consider the following subgroups of $\text{SL}(2,\mathbb{Z})$ : $A$ the subgroup of  matrices with determinant $1$ : \begin{bmatrix}4\mathbb{Z}+1&8\mathbb{Z}\\4\mathbb{Z}&4\mathbb{Z}+1\end{bmatrix} $B$ the subgroup of  matrices with determinant $1$ : \begin{bmatrix}2\mathbb{Z}+1&8\mathbb{Z}\\4\mathbb{Z}&2\mathbb{Z}+1\end{bmatrix} I want some onto homomorphism from $B$ to $A$ whose kernel is \begin{bmatrix}1&0\\0&1\end{bmatrix}\begin{bmatrix}-1&0\\0&-1\end{bmatrix} How to get this? I have no idea how to find the map.",,"['matrices', 'group-theory']"
15,Minimize Frobenius norm with unitary constraint,Minimize Frobenius norm with unitary constraint,,"I am trying to find a unitary tramsformation, $M$, that minimizes $\Vert MA-B \Vert_F^2$  where $A$ and $B$ are $N\times L,\;L\ge N$. I know how to solve it without the unitary constraint. I thought using Lagrange multipliers with the constraint $\Vert M^HM-I \Vert_F^2 = tr\left\{ \left( M^HM-I \right)^H \left( M^HM-I \right)\right\} = 0$ but the it is quite difficult to solve. Is there any simpler way? Thanks.","I am trying to find a unitary tramsformation, $M$, that minimizes $\Vert MA-B \Vert_F^2$  where $A$ and $B$ are $N\times L,\;L\ge N$. I know how to solve it without the unitary constraint. I thought using Lagrange multipliers with the constraint $\Vert M^HM-I \Vert_F^2 = tr\left\{ \left( M^HM-I \right)^H \left( M^HM-I \right)\right\} = 0$ but the it is quite difficult to solve. Is there any simpler way? Thanks.",,"['matrices', 'optimization', 'lagrange-multiplier']"
16,Are Toeplitz matrices always square?,Are Toeplitz matrices always square?,,"Possibly a stupid question, but are Toeplitz matrices always square? Wikipedia seems to suggest so, as the Toeplitz page says ""... Any $n \times n$ matrix $A$ of the form..."" But Matlab suggests otherwise: >> toeplitz([0,1,2,3],[0,4,5])  ans =       0     4     5      1     0     4      2     1     0      3     2     1 Is Matlab just being helpful here or is that a valid Toeplitz matrix?","Possibly a stupid question, but are Toeplitz matrices always square? Wikipedia seems to suggest so, as the Toeplitz page says ""... Any matrix of the form..."" But Matlab suggests otherwise: >> toeplitz([0,1,2,3],[0,4,5])  ans =       0     4     5      1     0     4      2     1     0      3     2     1 Is Matlab just being helpful here or is that a valid Toeplitz matrix?",n \times n A,"['matrices', 'toeplitz-matrices']"
17,Notation for sum over element wise multiplication,Notation for sum over element wise multiplication,,"Im looking for  a  typical notation  for  the  sum  over the elements after an element-wise  multiplication  of two matrices $A$ , $B$ (hadamard  product). Is  it  correct  to  write $\sum  A \odot B$ without  further  specifiying  what $\sum$ is doing  or  do  i  need  to  use  row  and column  indices? Thanks!","Im looking for  a  typical notation  for  the  sum  over the elements after an element-wise  multiplication  of two matrices , (hadamard  product). Is  it  correct  to  write without  further  specifiying  what is doing  or  do  i  need  to  use  row  and column  indices? Thanks!",A B \sum  A \odot B \sum,"['matrices', 'hadamard-product']"
18,Normalizing a quasi-rotation matrix,Normalizing a quasi-rotation matrix,,"I have a rotation matrix $R$ generated after a lot of multiplications, inverting and so on. However, the outcome is not completely normalized, e.g., $R R^T \neq I$ , but close. How can I fully normalize it?","I have a rotation matrix generated after a lot of multiplications, inverting and so on. However, the outcome is not completely normalized, e.g., , but close. How can I fully normalize it?",R R R^T \neq I,"['matrices', 'rotations']"
19,A and B are two matrices such that $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ then $ AB=BA$,A and B are two matrices such that  then,(A+B)^3=A^3+3A^2B+3AB^2+B^3  AB=BA,Let $A$ and $B$ be two invertible matrices in $M_2(\mathbb{R})$such that $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ then prove or disprove that $ AB=BA$ My working: $$(A+B)^3=A^3+3A^2B+3AB^2+B^3$$ $$\implies BA^2+B^2A+ABA+BAB =2A^2B+2AB^2$$ Now what should I do?,Let $A$ and $B$ be two invertible matrices in $M_2(\mathbb{R})$such that $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ then prove or disprove that $ AB=BA$ My working: $$(A+B)^3=A^3+3A^2B+3AB^2+B^3$$ $$\implies BA^2+B^2A+ABA+BAB =2A^2B+2AB^2$$ Now what should I do?,,['matrices']
20,consider the following subsets of complex plane,consider the following subsets of complex plane,,"$$\Omega_1=\left\{c\in\Bbb C:\begin{bmatrix}1&c\\\bar c&1\\ \end{bmatrix}\text{ is non-negative definite } \right\} $$ $$\Omega_2= \left\{c\in\Bbb C:         \begin{bmatrix}         1 & c & c \\         \bar c & 1 & c \\         \bar c & \bar c & 1 \\         \end{bmatrix} \text{ is non-negative definite } \right\}$$ Let $$\bar D=\{z\in\Bbb C:|z|\le1\}$$ Then 1) $\Omega_1=\bar D,\Omega_2=\bar D$ 2). $\Omega_1\neq\bar D, \Omega_2=\bar D$ 3). $\Omega_1=\bar D, \Omega_2\neq\bar D$ 4). $\Omega_1\neq\bar D, \Omega_2\neq\bar D$ How to proceed? Thank you.","$$\Omega_1=\left\{c\in\Bbb C:\begin{bmatrix}1&c\\\bar c&1\\ \end{bmatrix}\text{ is non-negative definite } \right\} $$ $$\Omega_2= \left\{c\in\Bbb C:         \begin{bmatrix}         1 & c & c \\         \bar c & 1 & c \\         \bar c & \bar c & 1 \\         \end{bmatrix} \text{ is non-negative definite } \right\}$$ Let $$\bar D=\{z\in\Bbb C:|z|\le1\}$$ Then 1) $\Omega_1=\bar D,\Omega_2=\bar D$ 2). $\Omega_1\neq\bar D, \Omega_2=\bar D$ 3). $\Omega_1=\bar D, \Omega_2\neq\bar D$ 4). $\Omega_1\neq\bar D, \Omega_2\neq\bar D$ How to proceed? Thank you.",,"['complex-analysis', 'matrices']"
21,Frobenius norm of a matrix [closed],Frobenius norm of a matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I know that Frobenius norm of a matrix A is equal to the square root of the trace of (A*conjugate transpose(A)). But how do I prove it mathematically?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I know that Frobenius norm of a matrix A is equal to the square root of the trace of (A*conjugate transpose(A)). But how do I prove it mathematically?",,"['matrices', 'normed-spaces']"
22,A special case: determinant of a $n\times n$ matrix,A special case: determinant of a  matrix,n\times n,"I would like to solve for the determinant of a $n\times n$ matrix $V$ defined as: $$ V_{i,j}= \begin{cases} v_{i}+v_{j} & \text{if} & i \neq j \\[2mm] (2-\beta_{i}) v_{i} & \text{if} & i = j \\ \end{cases} $$ Here, $v_i, v_j \in(0,1)$, and $\beta_i>0$.  Any suggestions or thoughts are highly appreciated.","I would like to solve for the determinant of a $n\times n$ matrix $V$ defined as: $$ V_{i,j}= \begin{cases} v_{i}+v_{j} & \text{if} & i \neq j \\[2mm] (2-\beta_{i}) v_{i} & \text{if} & i = j \\ \end{cases} $$ Here, $v_i, v_j \in(0,1)$, and $\beta_i>0$.  Any suggestions or thoughts are highly appreciated.",,"['matrices', 'determinant', 'matrix-equations', 'matrix-decomposition', 'matrix-rank']"
23,A real life application for QR decomposition,A real life application for QR decomposition,,"I need to use the QR decomposition of a matrix for a real life application, (use it on a particular matrix form) and I have no idea what to do. Can you suggest me a real life application for this? thank you.","I need to use the QR decomposition of a matrix for a real life application, (use it on a particular matrix form) and I have no idea what to do. Can you suggest me a real life application for this? thank you.",,"['matrices', 'applications', 'matrix-decomposition']"
24,Inverting the sum of a Dense and Diagonal matrix,Inverting the sum of a Dense and Diagonal matrix,,"So, lets assume that we have two matrices given to us: $A \in \mathbb{\Re}^{M \times K}$: just some dense, real, arbitrary valued matrix. $L \in \Re^{K \times K}$: A positive diagonal matrix Now, lets say we have the following situation: $\left( A^T A + L \right)^{-1}$ Is it possible to rephrase this statement in such a way as to make use of the knowledge of  $\left( A^T A \right)^{-1}$ so as to avoid this addition inside the inverse? I'm sorry if this is a little vague :P Just running into an analytical hiccup and trying to figure out how to break this statement apart. Edit: Would it be possible to analyze this expression using an eigen decomposition? That is, if we say $A^T A = Q \Lambda Q^{-1}$ Then, if $L$ is diagonal: $\left( A^T A + L \right) = Q \left( \Lambda + L \right) Q^{-1}$ Which would make $\left( A^T A + L \right)^{-1} = Q \left( \Lambda + L \right)^{-1} Q^{-1}$ Which might be usable somehow because $\left( \Lambda + L \right)$ is a diagonal matrix and allowing you to calculate the inverse directly by taking $1$ over the diagonal entries, right? Or is this completely off? Edit 2 : @Rcollyer So, I changed around my formulation entirely by only looking at rank 1 updates of $A^T A$ and was able to use the Sherman-Morrison (a form of the Woodbury) formula to break it apart and get at what I wanted :P","So, lets assume that we have two matrices given to us: $A \in \mathbb{\Re}^{M \times K}$: just some dense, real, arbitrary valued matrix. $L \in \Re^{K \times K}$: A positive diagonal matrix Now, lets say we have the following situation: $\left( A^T A + L \right)^{-1}$ Is it possible to rephrase this statement in such a way as to make use of the knowledge of  $\left( A^T A \right)^{-1}$ so as to avoid this addition inside the inverse? I'm sorry if this is a little vague :P Just running into an analytical hiccup and trying to figure out how to break this statement apart. Edit: Would it be possible to analyze this expression using an eigen decomposition? That is, if we say $A^T A = Q \Lambda Q^{-1}$ Then, if $L$ is diagonal: $\left( A^T A + L \right) = Q \left( \Lambda + L \right) Q^{-1}$ Which would make $\left( A^T A + L \right)^{-1} = Q \left( \Lambda + L \right)^{-1} Q^{-1}$ Which might be usable somehow because $\left( \Lambda + L \right)$ is a diagonal matrix and allowing you to calculate the inverse directly by taking $1$ over the diagonal entries, right? Or is this completely off? Edit 2 : @Rcollyer So, I changed around my formulation entirely by only looking at rank 1 updates of $A^T A$ and was able to use the Sherman-Morrison (a form of the Woodbury) formula to break it apart and get at what I wanted :P",,['matrices']
25,"$SL(2,\mathbb R)$ and $SO(2,1)$ isomorphic - or not?",and  isomorphic - or not?,"SL(2,\mathbb R) SO(2,1)","In this wikipedia article about $SU(1,1)$ , it is stated that This group [ $SU(1,1)$ ] is isomorphic to $SO(2,1)$ and $SL(2,\mathbb ℝ)^{[17]}$ I'm confused by the relation of $SO(2,1)$ and $SL(2,\mathbb R)$ . I know that they are locally isomorphic as Lie groups (as their Lie algebras are isomorphic). Topologically, they are different - $SL(2,\mathbb R)$ is connected (by row-echelon-reduction), where $SO(2,1)$ is not connected (there is the component fixing the upper hyperboloid and the component interchanging the hyperboloids). Therefore, they are not isomorphic as Lie groups. So, are $SO(2,1)$ and $SL(2,\mathbb R)$ isomorphic as abstract groups ? Why (not)? The source $^{[17]}$ (Gilmore's Lie Groups, Lie Algebras and some of their Applications , p.201-205) from wikipedia seems only to show an isomorphism between the Lie algebras. This question does not show that there is no isomorphism, but gives a 2-1-map, which is not surjective.","In this wikipedia article about , it is stated that This group [ ] is isomorphic to and I'm confused by the relation of and . I know that they are locally isomorphic as Lie groups (as their Lie algebras are isomorphic). Topologically, they are different - is connected (by row-echelon-reduction), where is not connected (there is the component fixing the upper hyperboloid and the component interchanging the hyperboloids). Therefore, they are not isomorphic as Lie groups. So, are and isomorphic as abstract groups ? Why (not)? The source (Gilmore's Lie Groups, Lie Algebras and some of their Applications , p.201-205) from wikipedia seems only to show an isomorphism between the Lie algebras. This question does not show that there is no isomorphism, but gives a 2-1-map, which is not surjective.","SU(1,1) SU(1,1) SO(2,1) SL(2,\mathbb ℝ)^{[17]} SO(2,1) SL(2,\mathbb R) SL(2,\mathbb R) SO(2,1) SO(2,1) SL(2,\mathbb R) ^{[17]}","['matrices', 'group-theory', 'lie-groups']"
26,Easy way to determine matrix positive / negative definiteness,Easy way to determine matrix positive / negative definiteness,,"So I have this math final coming up on Wednesday, and recently we have been finding critical points for two variable function using the Hessian matrix, and we didn't really explicitly learn how to find out the definiteness of the Hessian matrix in order to determine whether the point is a min/max etc. I was wondering if there is a good way to be able to find this out, something quick and simple that I could apply in the exam. So far in the couple of example we had, the prof. said that if the value of the top left element of the matrix is greater than 0, as well as the determinant, then it's positive semi-definite. But I think this only works for positive matrices. We're more than likely only going to be dealing with real numbers, rather than complex numbers, so I'm looking for something applicable for real matrices. Thank you!","So I have this math final coming up on Wednesday, and recently we have been finding critical points for two variable function using the Hessian matrix, and we didn't really explicitly learn how to find out the definiteness of the Hessian matrix in order to determine whether the point is a min/max etc. I was wondering if there is a good way to be able to find this out, something quick and simple that I could apply in the exam. So far in the couple of example we had, the prof. said that if the value of the top left element of the matrix is greater than 0, as well as the determinant, then it's positive semi-definite. But I think this only works for positive matrices. We're more than likely only going to be dealing with real numbers, rather than complex numbers, so I'm looking for something applicable for real matrices. Thank you!",,"['matrices', 'multivariable-calculus', 'positive-definite', 'hessian-matrix']"
27,Matrix Calculus: Derivative of Vectorized Symmetric Positive Definite Matrix w.r.t. its Vectorized Matrix Logarithm,Matrix Calculus: Derivative of Vectorized Symmetric Positive Definite Matrix w.r.t. its Vectorized Matrix Logarithm,,"Setup: Let $k\in{}\mathbb{N},$ and let $\mathrm{M}_{k,k}(\mathbb{}{R})$ denote the set of $k\times{}k$ matrices over the field of real numbers. Let $X\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ be a symmetric, positive definite matrix. Then $X$ has $k$ positive eigenvalues $\lambda_{1},\dots{},\lambda_{k}$ with corresponding eigenvectors $v_{1},\dots{},v_{k}$. The eigendecomposition/spectral decomposition of $X$ is: $$X = V\Lambda{}V^{-1} = V\Lambda{}V',$$ where $\Lambda{}=\mathrm{diag}(\lambda_{1},\dots{},\lambda_{k})\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ is the diagonal matrix with the $k$ eigenvalues on the main diagonal and $V=(v_{1},\dots{},v_{k})\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ is the matrix whose $k$ columns are the orthonormal eigenvectors. We define the natural matrix logarithm of $X$, denoted $\log{}(X)$, to be $$\log{}(X)=V\log{}(\Lambda{})V',$$ where $\log{}(\Lambda{})=\mathrm{diag}(\log{}(\lambda_{1}),\dots{},\log{}(\lambda_{k}))\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$. Question: What, if it can be found, is the analytical form of the $[k(k+1)/2] \times{} [k(k+1)/2]$ Jacobian matrix $$\frac{\partial{}\mathrm{vec}(X)}{\partial{}\mathrm{vec}(\log{}(X))'}$$ where $\mathrm{vec}(\cdot{})$ is the half-vectorization operator that stacks the lower triangular part of its square argument matrix. (Background: This is a recurring problem in multivariate statistics when one adopts a ""log-parameterization"" of a covariance or precision matrix, which are both, by definition, symmetric and positive (semi-)definite.)","Setup: Let $k\in{}\mathbb{N},$ and let $\mathrm{M}_{k,k}(\mathbb{}{R})$ denote the set of $k\times{}k$ matrices over the field of real numbers. Let $X\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ be a symmetric, positive definite matrix. Then $X$ has $k$ positive eigenvalues $\lambda_{1},\dots{},\lambda_{k}$ with corresponding eigenvectors $v_{1},\dots{},v_{k}$. The eigendecomposition/spectral decomposition of $X$ is: $$X = V\Lambda{}V^{-1} = V\Lambda{}V',$$ where $\Lambda{}=\mathrm{diag}(\lambda_{1},\dots{},\lambda_{k})\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ is the diagonal matrix with the $k$ eigenvalues on the main diagonal and $V=(v_{1},\dots{},v_{k})\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$ is the matrix whose $k$ columns are the orthonormal eigenvectors. We define the natural matrix logarithm of $X$, denoted $\log{}(X)$, to be $$\log{}(X)=V\log{}(\Lambda{})V',$$ where $\log{}(\Lambda{})=\mathrm{diag}(\log{}(\lambda_{1}),\dots{},\log{}(\lambda_{k}))\in{}\mathrm{M}_{k,k}(\mathbb{}{R})$. Question: What, if it can be found, is the analytical form of the $[k(k+1)/2] \times{} [k(k+1)/2]$ Jacobian matrix $$\frac{\partial{}\mathrm{vec}(X)}{\partial{}\mathrm{vec}(\log{}(X))'}$$ where $\mathrm{vec}(\cdot{})$ is the half-vectorization operator that stacks the lower triangular part of its square argument matrix. (Background: This is a recurring problem in multivariate statistics when one adopts a ""log-parameterization"" of a covariance or precision matrix, which are both, by definition, symmetric and positive (semi-)definite.)",,"['matrices', 'logarithms', 'matrix-calculus', 'positive-definite', 'symmetric-matrices']"
28,Decompose a matrix into diagonal term and low-rank approximation,Decompose a matrix into diagonal term and low-rank approximation,,"For a matrix $A$ the Singular Values Decomposition allows getting the closest low-rank approximation $$A_K=\sum_i^K\sigma_i \vec{v}_i \vec{u}_i^T$$ so that $\|A-A_k\|_F$ is minimal. I'd like to do the same but allow for an additional diagonal term; that is, for a given square matrix $A$ and an positive integer $K$ find a diagonal matrix $D$ and low-rank approximation  $$A_K=D+\sum_i^K\sigma_i \vec{v}_i \vec{u}_i^T$$ so that like above $\|A-A_k\|_F$ is minimal. The problem originated in the context of correlations matrices. Thus answers which further assume $A$ is symmetric, positive semi-definite are also welcome.","For a matrix $A$ the Singular Values Decomposition allows getting the closest low-rank approximation $$A_K=\sum_i^K\sigma_i \vec{v}_i \vec{u}_i^T$$ so that $\|A-A_k\|_F$ is minimal. I'd like to do the same but allow for an additional diagonal term; that is, for a given square matrix $A$ and an positive integer $K$ find a diagonal matrix $D$ and low-rank approximation  $$A_K=D+\sum_i^K\sigma_i \vec{v}_i \vec{u}_i^T$$ so that like above $\|A-A_k\|_F$ is minimal. The problem originated in the context of correlations matrices. Thus answers which further assume $A$ is symmetric, positive semi-definite are also welcome.",,"['matrices', 'matrix-decomposition', 'svd']"
29,"For matrices, why is the Lie algebra equal to the tangent space?","For matrices, why is the Lie algebra equal to the tangent space?",,"For a matrix Lie group $G\subset GL_n(\mathbb C)$ we define the Lie algebra to be the set of matrices $X\in M_n(\mathbb C)$ such that for all $t\in \mathbb R$ we have  $\quad \exp(tX)\in G$. For clarity, I define the exponential map by $$\exp(X) = \sum_{k=0}^\infty \dfrac{X^k}{k!}$$ The tangent space $T_I(G)$ at the group identity $I$ is the set of all matrices $X\in M_n(\mathbb C)$ for which there exists a differentiable path $$\gamma : (-\epsilon,\epsilon) \to G$$ where $\gamma(0)=I$ and $\gamma'(0)=X$. It is proposed that Lie algebra and the tangent space are the same. It is clear from the differentiability of exponentiation that if $\quad \exp(tX)\in G \quad$ at every $t$ then the path $\gamma$ defined by $\gamma(t) = \exp(tX)$ shows that $X$ is in the tangent space. The other direction is not as obvious to me. If $X$ is in the tangent space (ie. there exists a path $\gamma$ satisfying the conditions above) then $X$ is in the Lie algebra. Can somebody please explain why this is so?","For a matrix Lie group $G\subset GL_n(\mathbb C)$ we define the Lie algebra to be the set of matrices $X\in M_n(\mathbb C)$ such that for all $t\in \mathbb R$ we have  $\quad \exp(tX)\in G$. For clarity, I define the exponential map by $$\exp(X) = \sum_{k=0}^\infty \dfrac{X^k}{k!}$$ The tangent space $T_I(G)$ at the group identity $I$ is the set of all matrices $X\in M_n(\mathbb C)$ for which there exists a differentiable path $$\gamma : (-\epsilon,\epsilon) \to G$$ where $\gamma(0)=I$ and $\gamma'(0)=X$. It is proposed that Lie algebra and the tangent space are the same. It is clear from the differentiability of exponentiation that if $\quad \exp(tX)\in G \quad$ at every $t$ then the path $\gamma$ defined by $\gamma(t) = \exp(tX)$ shows that $X$ is in the tangent space. The other direction is not as obvious to me. If $X$ is in the tangent space (ie. there exists a path $\gamma$ satisfying the conditions above) then $X$ is in the Lie algebra. Can somebody please explain why this is so?",,"['matrices', 'exponential-function', 'lie-groups', 'lie-algebras']"
30,Extension fields isomorphic to fields of matrices,Extension fields isomorphic to fields of matrices,,"Suppose $K \subset L$ is a finite field extension of degree $m$. Is it true that there exists some natural $n$ such that $L$ is isomorphic to a subfield of $M^{n\times n}(K)$, the ring of $n\times n$ matrices with entries in $K$? What is the smallest dimension (as a function of $m$) for which an isomorphism exists? The example I had in mind was the 2-dimensional extension $\mathbb{R} \subset \mathbb{C}$ with $\mathbb{C}$ being isomorphic to $\{ \begin{pmatrix}a&-b\\b&a\end{pmatrix} : a, b \in \mathbb{R}\}$.","Suppose $K \subset L$ is a finite field extension of degree $m$. Is it true that there exists some natural $n$ such that $L$ is isomorphic to a subfield of $M^{n\times n}(K)$, the ring of $n\times n$ matrices with entries in $K$? What is the smallest dimension (as a function of $m$) for which an isomorphism exists? The example I had in mind was the 2-dimensional extension $\mathbb{R} \subset \mathbb{C}$ with $\mathbb{C}$ being isomorphic to $\{ \begin{pmatrix}a&-b\\b&a\end{pmatrix} : a, b \in \mathbb{R}\}$.",,"['matrices', 'field-theory', 'extension-field']"
31,Prove that determinant of matrix equal $\pm1$ or $0$,Prove that determinant of matrix equal  or,\pm1 0,We are given square binary matrix $A_n$. Data contained by A comply the following rule: if row has any 1's then they would appear there only successively (row $(1\space 1\space0\space1 )$ is impossible). For example: $$ \left|\left( \begin{array}_ 1 & 1 & 1  \\ 1 & 0 & 0  \\ 0 & 0 & 1 \end{array} \right)\right| = -1$$ Task is to prove that determinant of matrix equal $\pm1$ or 0. Thank you for suggestions.,We are given square binary matrix $A_n$. Data contained by A comply the following rule: if row has any 1's then they would appear there only successively (row $(1\space 1\space0\space1 )$ is impossible). For example: $$ \left|\left( \begin{array}_ 1 & 1 & 1  \\ 1 & 0 & 0  \\ 0 & 0 & 1 \end{array} \right)\right| = -1$$ Task is to prove that determinant of matrix equal $\pm1$ or 0. Thank you for suggestions.,,"['matrices', 'determinant']"
32,"Spectral radius, second induced norm","Spectral radius, second induced norm",,"In my textbook there are few facts left without any sign of a proof, which really bugs me, and I was thinking maybe someone can help me: $A\in \mathbb{R}^{m,n} \Rightarrow \ \|A\|_2 = \sqrt{\rho(A^TA)}$ where $\rho(A^TA)$ denotes spectral radius of a matrix $A^TA$ i.e.   maximum absolute value of an eigenvalue of $A^TA$ (in this case all   eigenvalues are real and non-negative) If $A\in\mathbb{R}^{n,n}$ and $A^T=A$ then $\|A\|_2=\rho(A)$ $\|A\|_2\le\sqrt{\|A^TA\|_{\infty}}$ I suppose these are well known facts, yet I couldn't find their proofs on the Internet. Eigenvalues are very new for me, maybe that is why I couldn't (and why I am not able to prove it alone, I hope they are not too complicated, I prefer simple proofs). In first also fact that eigenvalues of $A^TA$ are real and non-negative is not easy for me. Additionally I have several questions: I suppose that (yet I don't know a lot about it) $\|A\|_2=\max_{\vec{x}\neq\vec{0}} \frac{\|A\vec{x}\|_2}{\|\vec{x}\|_2}$ is not easy to compute. Is this second definition of $\|A\|_2$ (with spectral radius) helpful? I mean easier to compute? Can one easy and quite fast find eigenvalues, at least numerically? Is this inequality in the third point very good, that is $\|A\|_2$ is close to given upper bound?","In my textbook there are few facts left without any sign of a proof, which really bugs me, and I was thinking maybe someone can help me: $A\in \mathbb{R}^{m,n} \Rightarrow \ \|A\|_2 = \sqrt{\rho(A^TA)}$ where $\rho(A^TA)$ denotes spectral radius of a matrix $A^TA$ i.e.   maximum absolute value of an eigenvalue of $A^TA$ (in this case all   eigenvalues are real and non-negative) If $A\in\mathbb{R}^{n,n}$ and $A^T=A$ then $\|A\|_2=\rho(A)$ $\|A\|_2\le\sqrt{\|A^TA\|_{\infty}}$ I suppose these are well known facts, yet I couldn't find their proofs on the Internet. Eigenvalues are very new for me, maybe that is why I couldn't (and why I am not able to prove it alone, I hope they are not too complicated, I prefer simple proofs). In first also fact that eigenvalues of $A^TA$ are real and non-negative is not easy for me. Additionally I have several questions: I suppose that (yet I don't know a lot about it) $\|A\|_2=\max_{\vec{x}\neq\vec{0}} \frac{\|A\vec{x}\|_2}{\|\vec{x}\|_2}$ is not easy to compute. Is this second definition of $\|A\|_2$ (with spectral radius) helpful? I mean easier to compute? Can one easy and quite fast find eigenvalues, at least numerically? Is this inequality in the third point very good, that is $\|A\|_2$ is close to given upper bound?",,"['matrices', 'eigenvalues-eigenvectors', 'normed-spaces', 'spectral-radius']"
33,Spinor Mapping is Surjective,Spinor Mapping is Surjective,,"I'm (still) trying to prove that $SL(2,\mathbb{C})$ is the universal covering group the the proper orthochronous Lorentz group $L$. I have completed the following steps. (1) Prove that the vector space of $2\times 2$ Hermitian matrices $H$ is isomorphic to Minkowski space. (2) Demonstrate that the action of $SU(2)$ on $H$ by $X\mapsto AXA^{\dagger}$ induces a group homomorphism $SL(2,\mathbb{C})\to L$. (3) Prove this is 2:1 by observing that every $2\times 2$ complex matrix can be written as $X+iY$ with $X$ and $Y$ Hermitian. I still need to prove that this map is surjective though. Here I am completely stuck. All the books and internet resources I have found either gloss over it, or state that it's true but don't prove it. Could someone possibly give me a proper proof, with mathematician's rigour?! P.S. My own attempts at a proof have fallen down. I tried the following (a) Derive a formula for the inverse map locally. I can't see any good way to attack this though. (b) Prove that the associated Lie algebra homomorphism is an isomorphism. I know theoretically this should be possible, but practically it seems a nightmare!","I'm (still) trying to prove that $SL(2,\mathbb{C})$ is the universal covering group the the proper orthochronous Lorentz group $L$. I have completed the following steps. (1) Prove that the vector space of $2\times 2$ Hermitian matrices $H$ is isomorphic to Minkowski space. (2) Demonstrate that the action of $SU(2)$ on $H$ by $X\mapsto AXA^{\dagger}$ induces a group homomorphism $SL(2,\mathbb{C})\to L$. (3) Prove this is 2:1 by observing that every $2\times 2$ complex matrix can be written as $X+iY$ with $X$ and $Y$ Hermitian. I still need to prove that this map is surjective though. Here I am completely stuck. All the books and internet resources I have found either gloss over it, or state that it's true but don't prove it. Could someone possibly give me a proper proof, with mathematician's rigour?! P.S. My own attempts at a proof have fallen down. I tried the following (a) Derive a formula for the inverse map locally. I can't see any good way to attack this though. (b) Prove that the associated Lie algebra homomorphism is an isomorphism. I know theoretically this should be possible, but practically it seems a nightmare!",,"['matrices', 'representation-theory', 'lie-groups', 'lie-algebras', 'spin-geometry']"
34,Real jordan form to complex jordan form then compute P matrix.,Real jordan form to complex jordan form then compute P matrix.,,"I have the matrix  $$A =  \begin{bmatrix} 5 &  0 & 1 &  0 & 0 & -6 \\ 3 & -1 & 3 &  1 & 0 & -6 \\ 6 & -6 & 5 &  0 & 1 & -6 \\ 7 & -7 & 4 & -2 & 4 & -7 \\ 6 & -6 & 6 & -6 & 5 & -6 \\ 2 &  1 & 0 &  0 & 0 &  0 \end{bmatrix}$$ This can be brought in the following Jordan form, i.e. $A = TJT^{-1}$. $$J =  \begin{bmatrix} 2-3j & 1 & 0 & 0 & 0 & 0 \\ 0 & 2-3j & 1 & 0 & 0 & 0 \\ 0 & 0 & 2-3j & 0 & 0 & 0 \\ 0 & 0 & 0 & 2+3j & 1 & 0 \\ 0 & 0 & 0 & 0 & 2+3j & 1 \\ 0 & 0 & 0 & 0 & 0 & 2+3j  \end{bmatrix}$$ $$T =  \begin{bmatrix} 2j & 2j & 1+j & -2j & -2j & 1-j \\ 1+j & 2j & j & 1-j & -2j & -j \\ 0 & 2j & 2j & 0 & -2j & -2j \\ 0 & 1+j & 2j & 0 & 1-j & -2j \\ 0 & 0 & 2j & 0 & 0 & -2j \\ -1+j & -1+j & j & -1-j & -1-j & -j \end{bmatrix}  $$ Now I have to bring A into its real Jordan form. This is easy: $$J^{R} =  \begin{bmatrix} 2 & 3 & 1 & 0 & 0 & 0 \\ -3 & 2 & 0 & 1 & 0 & 0 \\ 0 & 0 & 2 & 3 & 1 & 0 \\ 0 & 0 & -3 & 2 & 0 & 1 \\ 0 & 0 & 0 & 0 & 2 & 3 \\ 0 & 0 & 0 & 0 & -3 & 2   \end{bmatrix} $$ Now I have to compute $V$ such that $VJ^RV^{-1} = A$. My question now is how do I compute this $V$? For real valued jordan forms this is easy. I just have to compute the eigenvectors of A, $\{T_i\}$ and then $T = \begin{bmatrix} T_1 | T_2 | \ldots | T_n\end{bmatrix}$.","I have the matrix  $$A =  \begin{bmatrix} 5 &  0 & 1 &  0 & 0 & -6 \\ 3 & -1 & 3 &  1 & 0 & -6 \\ 6 & -6 & 5 &  0 & 1 & -6 \\ 7 & -7 & 4 & -2 & 4 & -7 \\ 6 & -6 & 6 & -6 & 5 & -6 \\ 2 &  1 & 0 &  0 & 0 &  0 \end{bmatrix}$$ This can be brought in the following Jordan form, i.e. $A = TJT^{-1}$. $$J =  \begin{bmatrix} 2-3j & 1 & 0 & 0 & 0 & 0 \\ 0 & 2-3j & 1 & 0 & 0 & 0 \\ 0 & 0 & 2-3j & 0 & 0 & 0 \\ 0 & 0 & 0 & 2+3j & 1 & 0 \\ 0 & 0 & 0 & 0 & 2+3j & 1 \\ 0 & 0 & 0 & 0 & 0 & 2+3j  \end{bmatrix}$$ $$T =  \begin{bmatrix} 2j & 2j & 1+j & -2j & -2j & 1-j \\ 1+j & 2j & j & 1-j & -2j & -j \\ 0 & 2j & 2j & 0 & -2j & -2j \\ 0 & 1+j & 2j & 0 & 1-j & -2j \\ 0 & 0 & 2j & 0 & 0 & -2j \\ -1+j & -1+j & j & -1-j & -1-j & -j \end{bmatrix}  $$ Now I have to bring A into its real Jordan form. This is easy: $$J^{R} =  \begin{bmatrix} 2 & 3 & 1 & 0 & 0 & 0 \\ -3 & 2 & 0 & 1 & 0 & 0 \\ 0 & 0 & 2 & 3 & 1 & 0 \\ 0 & 0 & -3 & 2 & 0 & 1 \\ 0 & 0 & 0 & 0 & 2 & 3 \\ 0 & 0 & 0 & 0 & -3 & 2   \end{bmatrix} $$ Now I have to compute $V$ such that $VJ^RV^{-1} = A$. My question now is how do I compute this $V$? For real valued jordan forms this is easy. I just have to compute the eigenvectors of A, $\{T_i\}$ and then $T = \begin{bmatrix} T_1 | T_2 | \ldots | T_n\end{bmatrix}$.",,['matrices']
35,Maximizing symmetric matrices v.s. non-symmetric matrices,Maximizing symmetric matrices v.s. non-symmetric matrices,,"Quick clarification on the following will be appreciated. I know that for a real symmetric matrix $M$, the maximum of $x^TMx$ over all unit vectors $x$ gives the largest eigenvalue of $M$. Why is the ""symmetry"" condition necessary? What if my matrix is not symmetric? Isn't the maximum of $x^TMx=$ still largest eigenvalue of $M$? Thanks.","Quick clarification on the following will be appreciated. I know that for a real symmetric matrix $M$, the maximum of $x^TMx$ over all unit vectors $x$ gives the largest eigenvalue of $M$. Why is the ""symmetry"" condition necessary? What if my matrix is not symmetric? Isn't the maximum of $x^TMx=$ still largest eigenvalue of $M$? Thanks.",,"['matrices', 'optimization']"
36,Minimizing the diagonal product of a special orthogonal matrix,Minimizing the diagonal product of a special orthogonal matrix,,"For any $n > 1$ , define $f: \textrm{SO}(n) \rightarrow \mathbb{R}$ as the diagonal product $$f(A) = \prod_{i=1}^n A_{ii}$$ Based on some numerical experiments, it seems that $$\min_{A \in \textrm{SO}(n)} f(A) = -\left(1-\frac2n\right)^n,$$ and this minimum is attained on matrices $(a_{ij})$ satisfying $|a_{ii}| = 1-2/n$ (with odd number of negative $a_{ii}$ 's, of course) and $|a_{ij}| = 2/n$ for $i \neq j$ . For illustration, here is one of such ""diagonal-product-minimizing"" matrices for the $n=5$ case: $$\frac{1}{5} \left( \begin{array}{rrrrr} -3 & -2 & -2 & -2 & -2 \\ 2 & 3 & -2 & -2 & -2 \\ 2 & -2 & 3 & -2 & -2 \\ 2 & -2 & -2 & 3 & -2 \\ 2 & -2 & -2 & -2 & 3 \end{array} \right)$$ Is that a known (let alone true) result? How can one prove it? I've managed to handle only the trivial $n=2$ case and the somewhat less trivial $n=3$ case using the Euler angle parametrization.","For any , define as the diagonal product Based on some numerical experiments, it seems that and this minimum is attained on matrices satisfying (with odd number of negative 's, of course) and for . For illustration, here is one of such ""diagonal-product-minimizing"" matrices for the case: Is that a known (let alone true) result? How can one prove it? I've managed to handle only the trivial case and the somewhat less trivial case using the Euler angle parametrization.","n > 1 f: \textrm{SO}(n) \rightarrow \mathbb{R} f(A) = \prod_{i=1}^n A_{ii} \min_{A \in \textrm{SO}(n)} f(A) = -\left(1-\frac2n\right)^n, (a_{ij}) |a_{ii}| = 1-2/n a_{ii} |a_{ij}| = 2/n i \neq j n=5 \frac{1}{5} \left( \begin{array}{rrrrr} -3 & -2 & -2 & -2 & -2 \\ 2 & 3 & -2 & -2 & -2 \\ 2 & -2 & 3 & -2 & -2 \\ 2 & -2 & -2 & 3 & -2 \\ 2 & -2 & -2 & -2 & 3 \end{array} \right) n=2 n=3","['matrices', 'optimization', 'orthogonal-matrices', 'combinatorial-designs']"
37,Derivative of the exponential function (of matrix functions) by a strange integral and a function object which does not commute with its derivative,Derivative of the exponential function (of matrix functions) by a strange integral and a function object which does not commute with its derivative,,"At the bottom of this section of an article, Wikipedia claims: $$\frac{d}{dt}\exp(X(t))=\int_0^1\exp(\alpha X(t))\frac{dX(t)}{dt}\exp((1-\alpha)X(t))\,d\alpha$$ For any general $t$ -dependent object $X$ . I can't get access to the paper they referenced from. My attempts to find this are as follows: When $X(t)$ commutes with $\frac{d}{dt}X(t)$ the result of that integral is trivially what would be expected from the chain rule. When $X$ does not commute however, there is a problem! I let $\Delta$ denote the object, be it a matrix (what I'm most interested in) or something else, representing the direction derivative of $X$ . $$\begin{align}\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{d}{dt}\frac{(X(t)^n)}{n!}=\sum_{n=0}^\infty\frac{1}{n!}\frac{d}{dt}(X(t)^n)\\&=\sum_{n=0}^\infty\frac{1}{n!}\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}\cdot\frac{dX(t)}{dt} \\\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}&=\lim_{h\to0}\frac{h(\Delta X^{n-1}+X\Delta X^{n-2}+X^2\Delta X^{n-3}+\cdots)+o(h^2)}{h} \\&=\Delta X^{n-1}+X\Delta X^{n-2}+\cdots \\&=\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)} \\\therefore\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{1}{n!}\cdot\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)}\cdot\frac{dX(t)}{dt}\end{align}$$ And I have no idea whether any of this is right, nor how to make my "" $\Delta$ "" anything meaningful (I would assume it is $\frac{d(X(t))}{dt}$ , but I don't know). I don't even know if it should be in the above calculations. I more importantly have no idea how to calculate the integral formula at the top of this question or turn my workings into an integral, and I've just been showing my failed attempts thus far. I would greatly appreciate any references, hints or answers, as this seems like a very important (at the very least interesting) formula but not one that I will be able to solve with my own knowledge. Many thanks.","At the bottom of this section of an article, Wikipedia claims: For any general -dependent object . I can't get access to the paper they referenced from. My attempts to find this are as follows: When commutes with the result of that integral is trivially what would be expected from the chain rule. When does not commute however, there is a problem! I let denote the object, be it a matrix (what I'm most interested in) or something else, representing the direction derivative of . And I have no idea whether any of this is right, nor how to make my "" "" anything meaningful (I would assume it is , but I don't know). I don't even know if it should be in the above calculations. I more importantly have no idea how to calculate the integral formula at the top of this question or turn my workings into an integral, and I've just been showing my failed attempts thus far. I would greatly appreciate any references, hints or answers, as this seems like a very important (at the very least interesting) formula but not one that I will be able to solve with my own knowledge. Many thanks.","\frac{d}{dt}\exp(X(t))=\int_0^1\exp(\alpha X(t))\frac{dX(t)}{dt}\exp((1-\alpha)X(t))\,d\alpha t X X(t) \frac{d}{dt}X(t) X \Delta X \begin{align}\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{d}{dt}\frac{(X(t)^n)}{n!}=\sum_{n=0}^\infty\frac{1}{n!}\frac{d}{dt}(X(t)^n)\\&=\sum_{n=0}^\infty\frac{1}{n!}\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}\cdot\frac{dX(t)}{dt}
\\\lim_{h\to0}\frac{(X+h\Delta)^n-X^n}{h}&=\lim_{h\to0}\frac{h(\Delta X^{n-1}+X\Delta X^{n-2}+X^2\Delta X^{n-3}+\cdots)+o(h^2)}{h}
\\&=\Delta X^{n-1}+X\Delta X^{n-2}+\cdots
\\&=\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)}
\\\therefore\frac{d}{dt}\exp(X(t))&=\sum_{n=0}^\infty\frac{1}{n!}\cdot\sum_{\alpha=0}^{n-1}X^{\alpha}\Delta X^{n-(1+\alpha)}\cdot\frac{dX(t)}{dt}\end{align} \Delta \frac{d(X(t))}{dt}","['matrices', 'derivatives', 'exponential-function', 'matrix-calculus', 'matrix-exponential']"
38,Proving a level set is a submanifold or is empty,Proving a level set is a submanifold or is empty,,"Let $A$ be an invertible, symmetric $n\times n$ matrix and define $F:\mathbb R^n\rightarrow\mathbb R$ by $F(x)=x\cdot Ax$ For $k>0$ , how can I show that the set $T_k=\{x\in\mathbb R^n:F(x)=k\}$ is either empty or a submanifold of $\mathbb R^n$ ? I know that if $rank(\delta F(x))=1$ for all $x\in T_k$ , then $T_k$ is a submanifold of $\mathbb R^n$ $\delta F(x)=(\delta_1 F(x)...\delta_n F(x))$ The rank of $\delta F(x)$ is zero iff all the partial derivatives are zero, why does this imply that $T_k$ is empty?","Let be an invertible, symmetric matrix and define by For , how can I show that the set is either empty or a submanifold of ? I know that if for all , then is a submanifold of The rank of is zero iff all the partial derivatives are zero, why does this imply that is empty?",A n\times n F:\mathbb R^n\rightarrow\mathbb R F(x)=x\cdot Ax k>0 T_k=\{x\in\mathbb R^n:F(x)=k\} \mathbb R^n rank(\delta F(x))=1 x\in T_k T_k \mathbb R^n \delta F(x)=(\delta_1 F(x)...\delta_n F(x)) \delta F(x) T_k,"['matrices', 'multivariable-calculus', 'differential-topology']"
39,About a sum that looks like a determinant,About a sum that looks like a determinant,,"I want to prove the following equality. $$\sum_{\sigma\in S_n}\frac{\text{sgn}(\sigma)}{|\text{Fix}(\sigma)|+1}=(-1)^{n+1}\frac{n}{n+1},$$   where $\sigma$ is a permutation on $n$ elements and $\text{sgn}, \text{Fix}$ stand for the sign of the permutation and the fixed points of the permutation. The sum reminds me of a determinant since, but I can't see how would $$\prod_{i=1}^na_{i,\sigma(i)}=\frac{1}{|\text{Fix}(\sigma)|+1}.$$ I tried also looking at the element on the right hand side. The $\frac{1}{n+1}$ reminds me of two things, one could be an alternating geometric series and the other one is the integral of $x^n$. My instinct tells me that matrix whose determinant is this must not be very complicated, I feel there must be some symmetries as well. If you can provide any insight or hints it would be very much appreciated.","I want to prove the following equality. $$\sum_{\sigma\in S_n}\frac{\text{sgn}(\sigma)}{|\text{Fix}(\sigma)|+1}=(-1)^{n+1}\frac{n}{n+1},$$   where $\sigma$ is a permutation on $n$ elements and $\text{sgn}, \text{Fix}$ stand for the sign of the permutation and the fixed points of the permutation. The sum reminds me of a determinant since, but I can't see how would $$\prod_{i=1}^na_{i,\sigma(i)}=\frac{1}{|\text{Fix}(\sigma)|+1}.$$ I tried also looking at the element on the right hand side. The $\frac{1}{n+1}$ reminds me of two things, one could be an alternating geometric series and the other one is the integral of $x^n$. My instinct tells me that matrix whose determinant is this must not be very complicated, I feel there must be some symmetries as well. If you can provide any insight or hints it would be very much appreciated.",,"['matrices', 'permutations', 'determinant']"
40,Nearest semi-orthogonal matrix using the entry-wise $ {\ell}_{1} $ norm,Nearest semi-orthogonal matrix using the entry-wise  norm, {\ell}_{1} ,"Given an $m \times n$ matrix $M$ ( $m \geq n$ ), the nearest semi-orthogonal matrix problem in $m \times n$ matrix $R$ is $$\begin{array}{ll} \text{minimize} & \| M - R \|_F\\ \text{subject to} & R^T R = I_n\end{array}$$ A solution can be found by using Lagrangian or polar decomposition , and is known to be $$\hat{R} := M(M^TM)^{-1/2}$$ If $\|\cdot\|_F$ is replaced by the entry-wise $1$ -norm $$\|A\|_1 := \|\operatorname{vec}(A)\|_1 = \sum_{i,j} |A_{i,j}|$$ the problem becomes $$\boxed{\begin{array}{ll} \text{minimize} & \| M - R \|_1\\ \text{subject to} & R^T R = I_n\end{array}}$$ What do we know about the solutions in this case? Is $\hat{R}$ still a solution? If the solution is something else, do analytic forms or approximations exist? Any insight or direction to literature is appreciated.","Given an matrix ( ), the nearest semi-orthogonal matrix problem in matrix is A solution can be found by using Lagrangian or polar decomposition , and is known to be If is replaced by the entry-wise -norm the problem becomes What do we know about the solutions in this case? Is still a solution? If the solution is something else, do analytic forms or approximations exist? Any insight or direction to literature is appreciated.","m \times n M m \geq n m \times n R \begin{array}{ll} \text{minimize} & \| M - R \|_F\\ \text{subject to} & R^T R = I_n\end{array} \hat{R} := M(M^TM)^{-1/2} \|\cdot\|_F 1 \|A\|_1 := \|\operatorname{vec}(A)\|_1 = \sum_{i,j} |A_{i,j}| \boxed{\begin{array}{ll} \text{minimize} & \| M - R \|_1\\ \text{subject to} & R^T R = I_n\end{array}} \hat{R}","['matrices', 'optimization', 'orthogonal-matrices', 'non-convex-optimization', 'stiefel-manifolds']"
41,Finding the general formula for the powers of a matrix,Finding the general formula for the powers of a matrix,,"Let $A=\begin{bmatrix}0&1\\1&0\end{bmatrix}$. Find the formula for $A^n$ and prove it. The way I tried to solve it is like this: If we find $A^2$, $A^3$ and so on you will notice this patern: If $n$ is odd $A^n=\begin{bmatrix}0&1\\1&0\end{bmatrix}$. If $n$ is even then $A^n=I$. To prove this I used mathematical induction. We see that the base case is $A^1$ and $A^2$. Now we suppose the statement holds for a natural number $n$. If $n+1$ is odd then this means that n is even and we see that the matrix $A^{n+1}=A^nA=\begin{bmatrix}0&1\\1&0\end{bmatrix}$, so the statement holds in this case. If $n+1$ is even then this means that $n$ is odd and we see that the matrix $A^{n+1}=A^nA=I$, so the statement holds in this case also. As a consequence the statement holds for all natural numbers. I want to know if this proof is right or not? Can someone help me?","Let $A=\begin{bmatrix}0&1\\1&0\end{bmatrix}$. Find the formula for $A^n$ and prove it. The way I tried to solve it is like this: If we find $A^2$, $A^3$ and so on you will notice this patern: If $n$ is odd $A^n=\begin{bmatrix}0&1\\1&0\end{bmatrix}$. If $n$ is even then $A^n=I$. To prove this I used mathematical induction. We see that the base case is $A^1$ and $A^2$. Now we suppose the statement holds for a natural number $n$. If $n+1$ is odd then this means that n is even and we see that the matrix $A^{n+1}=A^nA=\begin{bmatrix}0&1\\1&0\end{bmatrix}$, so the statement holds in this case. If $n+1$ is even then this means that $n$ is odd and we see that the matrix $A^{n+1}=A^nA=I$, so the statement holds in this case also. As a consequence the statement holds for all natural numbers. I want to know if this proof is right or not? Can someone help me?",,"['matrices', 'proof-verification', 'induction']"
42,Solving a first-order inhomogeneous matrix differential equation?,Solving a first-order inhomogeneous matrix differential equation?,,"Given non-commuting matrices $A$ and $B$ of order $n$, is there a closed-form solution to the differential equation $$\frac{dX}{dt} = AX + tBX$$ with $X(0) = I$? I know that for the reals, $x = a \exp{\int f(t)}$ is the general solution to $\dot{x} = f(t)x$, but I'm also 99% certain this relies on the commutivity of the reals. I'm more specifically looking to numerically compute $X(T)$ given the more general differential equation $$\frac{dX}{dt} = f(t)X,\;\;X(0)=I$$ but in circumstances where $f'(t)$ may be large and a $1$st order piecewise approximation would be far more accurate than $0$th order for any given $\Delta t$. Ultimately my concern is computing $X(T)$ as quickly as possible. Are there better techniques for accomplishing this?","Given non-commuting matrices $A$ and $B$ of order $n$, is there a closed-form solution to the differential equation $$\frac{dX}{dt} = AX + tBX$$ with $X(0) = I$? I know that for the reals, $x = a \exp{\int f(t)}$ is the general solution to $\dot{x} = f(t)x$, but I'm also 99% certain this relies on the commutivity of the reals. I'm more specifically looking to numerically compute $X(T)$ given the more general differential equation $$\frac{dX}{dt} = f(t)X,\;\;X(0)=I$$ but in circumstances where $f'(t)$ may be large and a $1$st order piecewise approximation would be far more accurate than $0$th order for any given $\Delta t$. Ultimately my concern is computing $X(T)$ as quickly as possible. Are there better techniques for accomplishing this?",,"['matrices', 'ordinary-differential-equations', 'dynamical-systems']"
43,Automorphisms of matrix rings,Automorphisms of matrix rings,,"I am trying to classify automorphisms of matrix rings.  I was surprised to be able to find little on the subject by searching (here or elsewhere) though this could be an indication of my poor searching skills rather than the lack of material. For the moment, I am looking at matrices over a field but I am trying to be as general as possible about the field: not necessarily $\mathbb{R}$ or $\mathbb{C}$ or even necessarily of characteristic $0$ . I am not sure whether it is standard but I am only interested in automorphisms which in a sense preserve the field.  More exactly, they should have this property: $$\phi(\lambda M) = \lambda\phi(M)$$ I am most interested in automorphisms due to the matrix structure rather than ones which are in a sense inherited from the field. One obvious class of automorphisms is those of the form: $$A \rightarrow P^{-1}AP$$ where $P$ is invertible. Now, I need to either find others or prove that all must have this form. After trying and failing for a while to prove that all must have this form, I started to look for counterexamples.  I found a partial one.  For $M_2(\mathbb{R})$ , the automorphism: $$ \left(\begin{matrix}         x_{11} & x_{12} \\         x_{21} & x_{22} \\         \end{matrix}\right)  \rightarrow \left(\begin{matrix}         x_{11} & -x_{12} \\         -x_{21} & x_{22} \\         \end{matrix}\right)  $$ [This was originally mistyped giving the appearance that it was only defined for symmetric matrices.] cannot be expressed in this form but if I extend to $M_2(\mathbb{C})$ it can with $$ P =  \left(\begin{matrix}         i & 0 \\         0 & -i \\         \end{matrix}\right)  $$ So, I may need to consider whether allowing extensions of the field would cover all automorphisms.  The question now becomes: are all automorphisms of this form if field extensions are allowed or can I find a counterexample to that. My attempts mostly involve considering possible images of simple matrices e.g. ones with a single entry of $1$ and otherwise $0$ . This is not homework, I am long past that.  It is an old man doing self-imposed brain exercise. Some hints rather than a complete answer would be appreciated since the purpose is to exercise my brain.","I am trying to classify automorphisms of matrix rings.  I was surprised to be able to find little on the subject by searching (here or elsewhere) though this could be an indication of my poor searching skills rather than the lack of material. For the moment, I am looking at matrices over a field but I am trying to be as general as possible about the field: not necessarily or or even necessarily of characteristic . I am not sure whether it is standard but I am only interested in automorphisms which in a sense preserve the field.  More exactly, they should have this property: I am most interested in automorphisms due to the matrix structure rather than ones which are in a sense inherited from the field. One obvious class of automorphisms is those of the form: where is invertible. Now, I need to either find others or prove that all must have this form. After trying and failing for a while to prove that all must have this form, I started to look for counterexamples.  I found a partial one.  For , the automorphism: [This was originally mistyped giving the appearance that it was only defined for symmetric matrices.] cannot be expressed in this form but if I extend to it can with So, I may need to consider whether allowing extensions of the field would cover all automorphisms.  The question now becomes: are all automorphisms of this form if field extensions are allowed or can I find a counterexample to that. My attempts mostly involve considering possible images of simple matrices e.g. ones with a single entry of and otherwise . This is not homework, I am long past that.  It is an old man doing self-imposed brain exercise. Some hints rather than a complete answer would be appreciated since the purpose is to exercise my brain.","\mathbb{R} \mathbb{C} 0 \phi(\lambda M) = \lambda\phi(M) A \rightarrow P^{-1}AP P M_2(\mathbb{R}) 
\left(\begin{matrix}
        x_{11} & x_{12} \\
        x_{21} & x_{22} \\
        \end{matrix}\right) 
\rightarrow
\left(\begin{matrix}
        x_{11} & -x_{12} \\
        -x_{21} & x_{22} \\
        \end{matrix}\right) 
 M_2(\mathbb{C})  P = 
\left(\begin{matrix}
        i & 0 \\
        0 & -i \\
        \end{matrix}\right) 
 1 0","['matrices', 'ring-theory', 'field-theory']"
44,Orthogonal Matrix with Determinant 1 is a Rotation Matrix,Orthogonal Matrix with Determinant 1 is a Rotation Matrix,,"I am confused with how to show that an orthogonal matrix with determinant 1 must always be a rotation matrix. My approach to proving this was to take a general matrix $\begin{bmatrix}a&b \\c&d\end{bmatrix}$ and using the definition of a matrix being orthogonal, work out some restrictions on $a,b,c,d$ such that the matrix must be a rotation matrix. Doing this; I end up with $\begin{bmatrix}a&b \\c&d\end{bmatrix}^T\begin{bmatrix}a&b \\c&d\end{bmatrix}=\begin{bmatrix}a^2+c^2&ab+cd \\ab+cd&b^2+d^2\end{bmatrix}=\begin{bmatrix}1&0 \\0&1\end{bmatrix}$ We also know $ad-bc=1$ from the determinant restriction. I thought we could say since $a^2+c^2=1$ then we could say $a=\cos(\theta)$ and $c=\sin(\theta)$. From here we could chose the second column of the matrix to a vector such that $b=-\cos(\pi/2-\theta)$ and $d=\sin(\pi/2-\theta)$ which gives $b=-\sin(\theta)$ and d=$\cos(\theta)$. This would give $\begin{bmatrix}\cos(\theta)&-\sin(\theta) \\\sin(\theta)&\cos(\theta)\end{bmatrix}$ which is the rotation matrix I need to show. Is this correct and is there a better approach to help me prove that any orthogonal matrix with determinant 1 must be a rotation matrix? EDIT: We can say $a=\cos(\theta)$ and $c=\sin(\theta)$ as we know $a^2+c^2=1$ and so $a$ and $c$ must lie on the unit circle, hence we can parameterise the variables in the matrix as such.","I am confused with how to show that an orthogonal matrix with determinant 1 must always be a rotation matrix. My approach to proving this was to take a general matrix $\begin{bmatrix}a&b \\c&d\end{bmatrix}$ and using the definition of a matrix being orthogonal, work out some restrictions on $a,b,c,d$ such that the matrix must be a rotation matrix. Doing this; I end up with $\begin{bmatrix}a&b \\c&d\end{bmatrix}^T\begin{bmatrix}a&b \\c&d\end{bmatrix}=\begin{bmatrix}a^2+c^2&ab+cd \\ab+cd&b^2+d^2\end{bmatrix}=\begin{bmatrix}1&0 \\0&1\end{bmatrix}$ We also know $ad-bc=1$ from the determinant restriction. I thought we could say since $a^2+c^2=1$ then we could say $a=\cos(\theta)$ and $c=\sin(\theta)$. From here we could chose the second column of the matrix to a vector such that $b=-\cos(\pi/2-\theta)$ and $d=\sin(\pi/2-\theta)$ which gives $b=-\sin(\theta)$ and d=$\cos(\theta)$. This would give $\begin{bmatrix}\cos(\theta)&-\sin(\theta) \\\sin(\theta)&\cos(\theta)\end{bmatrix}$ which is the rotation matrix I need to show. Is this correct and is there a better approach to help me prove that any orthogonal matrix with determinant 1 must be a rotation matrix? EDIT: We can say $a=\cos(\theta)$ and $c=\sin(\theta)$ as we know $a^2+c^2=1$ and so $a$ and $c$ must lie on the unit circle, hence we can parameterise the variables in the matrix as such.",,"['matrices', 'rotations', 'orthogonal-matrices']"
45,Derivative of the matrix exponential with respect to its matrix argument,Derivative of the matrix exponential with respect to its matrix argument,,"I was trying to find the Frechet derivative of $f = \exp(X)$, where $X \in \mathbb{R}^{n\times n}$ is positive definite. I thought it ought to be $\exp(X)$. I see results where the derivative is with respect to a scalar argument, but this question has not been asked before. I tried to see if I could find $Df_X$ starting with  $Df_X[H] = \exp(X+H) - \exp(X)$.  If I can show that the right hand side evaluates to $I + XH + X^2H/2 + \ldots = \exp(X)H$, I am done. After I use the series definition, however,  I am lost  because I see no reason to  assume that $A$ and $H$ commute. Please help. EDIT Following the suggestion in the comment, I try to compute the Gateaux derivative as $\exp(X + tH)$ by writing down the first few terms. $\exp(X+tH) = I + (X + tH) + (X^2 + tXH + tHX + t^2H^2)/2 + \cdots$ $\dfrac{d}{dt}\exp(X+tH)\Big|_{t=0} = H + (XH+HX)/2 + \cdots$ And now am stuck again. It seems the expression on the right cannot be rearranged to give what I want. I think it  is the derivative of the trace of the exponential, not the exponential itself that yields $\exp(X)$","I was trying to find the Frechet derivative of $f = \exp(X)$, where $X \in \mathbb{R}^{n\times n}$ is positive definite. I thought it ought to be $\exp(X)$. I see results where the derivative is with respect to a scalar argument, but this question has not been asked before. I tried to see if I could find $Df_X$ starting with  $Df_X[H] = \exp(X+H) - \exp(X)$.  If I can show that the right hand side evaluates to $I + XH + X^2H/2 + \ldots = \exp(X)H$, I am done. After I use the series definition, however,  I am lost  because I see no reason to  assume that $A$ and $H$ commute. Please help. EDIT Following the suggestion in the comment, I try to compute the Gateaux derivative as $\exp(X + tH)$ by writing down the first few terms. $\exp(X+tH) = I + (X + tH) + (X^2 + tXH + tHX + t^2H^2)/2 + \cdots$ $\dfrac{d}{dt}\exp(X+tH)\Big|_{t=0} = H + (XH+HX)/2 + \cdots$ And now am stuck again. It seems the expression on the right cannot be rearranged to give what I want. I think it  is the derivative of the trace of the exponential, not the exponential itself that yields $\exp(X)$",,"['matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential', 'gateaux-derivative']"
46,Unsure about a maths symbol,Unsure about a maths symbol,,"Help, help, help! I've come across this maths symbol, $[n(i,j)]^{0.5}$ where $n$ is a square matrix. Does this mean that it is the $(i,j)$ element of $n^{0.5}$? or $n(i,j)^{0.5}$? source: http://outobox.cs.umn.edu/Random_Walks_Collaborative_Recommendation_Fouss.pdf 2nd page, fifth line from the top of left column: Thanks a million in advance! Eh... after looking at some notation he used at page 367, my guess is element-wise square-root?","Help, help, help! I've come across this maths symbol, $[n(i,j)]^{0.5}$ where $n$ is a square matrix. Does this mean that it is the $(i,j)$ element of $n^{0.5}$? or $n(i,j)^{0.5}$? source: http://outobox.cs.umn.edu/Random_Walks_Collaborative_Recommendation_Fouss.pdf 2nd page, fifth line from the top of left column: Thanks a million in advance! Eh... after looking at some notation he used at page 367, my guess is element-wise square-root?",,"['matrices', 'notation']"
47,How to tell if a directed graph is acyclic from the adjacency matrix?,How to tell if a directed graph is acyclic from the adjacency matrix?,,"Suppose you have an adjacency matrix $A$ for a directed graph $G=\{V,E\}$ , so $A_{ij} = 1$ if $V_i\rightarrow V_j \in E$ , and $A_{ij}=0$ otherwise. Many properties of the graph can be derived from this adjacency matrix. For instance, $(A^n)_{ij}$ tells you the number of paths of length $n$ going from $V_i$ to $V_j$ , and for an undirected graph the number of connected components is equal to the number of eigenvalues of $A$ equal to zero. Is there a nice linear-algebraic quantity that can tell you whether or not this graph has cycles? My gut says that $A^* = I + A + A^2 + A^3 + \ldots = (I-A)^{-1}$ being finite (that is, $I-A$ being nonsingular) is a necessary and sufficient condition, because it means there are a finite number of paths of arbitrary length between any two vertices. Is this correct?","Suppose you have an adjacency matrix for a directed graph , so if , and otherwise. Many properties of the graph can be derived from this adjacency matrix. For instance, tells you the number of paths of length going from to , and for an undirected graph the number of connected components is equal to the number of eigenvalues of equal to zero. Is there a nice linear-algebraic quantity that can tell you whether or not this graph has cycles? My gut says that being finite (that is, being nonsingular) is a necessary and sufficient condition, because it means there are a finite number of paths of arbitrary length between any two vertices. Is this correct?","A G=\{V,E\} A_{ij} = 1 V_i\rightarrow V_j \in E A_{ij}=0 (A^n)_{ij} n V_i V_j A A^* = I + A + A^2 + A^3 + \ldots = (I-A)^{-1} I-A","['matrices', 'graph-theory', 'adjacency-matrix', 'nonnegative-matrices']"
48,Generalized variance,Generalized variance,,Generalized variance is the determinant of correlation matrix. Does increasing the off-diagonal entries (correlation coefficients) decreases the determinant? Is a proof available? All elements are positive. Can we deduce from Hadamard inequality of determinant?,Generalized variance is the determinant of correlation matrix. Does increasing the off-diagonal entries (correlation coefficients) decreases the determinant? Is a proof available? All elements are positive. Can we deduce from Hadamard inequality of determinant?,,"['matrices', 'correlation', 'determinant']"
49,Olympiad linear algebra problem,Olympiad linear algebra problem,,"This is a problem from an olympiad I took today. I tried but couldn't solve it. Let $A$ and $B$ rectangular matrices with real entries, of dimensions $k\times n$ and $m\times n$ respectively. Prove that if for every $n\times l$ matrix $X$ ($l\in \mathbb{Z}^+$) the equality $AX=0$ implies $BX=0$, then there exists a matrix $C$ such that $B=CA$. I tried to define a commutative diagram, but failed. Anyhow, I'd prefer a solution that works with the matrices explicitly. But I'll welcome and appreciate any solutions.","This is a problem from an olympiad I took today. I tried but couldn't solve it. Let $A$ and $B$ rectangular matrices with real entries, of dimensions $k\times n$ and $m\times n$ respectively. Prove that if for every $n\times l$ matrix $X$ ($l\in \mathbb{Z}^+$) the equality $AX=0$ implies $BX=0$, then there exists a matrix $C$ such that $B=CA$. I tried to define a commutative diagram, but failed. Anyhow, I'd prefer a solution that works with the matrices explicitly. But I'll welcome and appreciate any solutions.",,['matrices']
50,Confusion regarding vector/matrix multiplication in index notation,Confusion regarding vector/matrix multiplication in index notation,,"I came across this question and answer (sorry I don't have an electronic source for it, only a paper copy). After reading the answer it had me questioning the notation one uses to denote row/column vectors in tensor expressions: Consider the following contravariant vectors: $a^\mu=(1,1,0,0)$ , $b^\mu=(0,1,0,0)$ and $c^\mu=(0,0,0,1)$ . Derive the following quantities: $\phi=a_\mu b^\mu$ , $\psi=a_\mu a^\mu$ , $V_\mu=a_\nu b_\mu a^\nu$ , $W^{\nu\mu}=c^\nu a^\mu$ , $P_{\mu\nu}=a_\mu b_\nu$ and $Q_\mu^{\,\,\,\nu}=b_\mu c^\nu$ . Here are the solutions: First we can construct the covariant versions of these vectors, $a_\mu=(-1,1,0,0)$ , $b_\mu=(0,1,0,0)$ and $c_\mu=(0,0,0,1)$ , then we have $$\phi=a_\mu b^\mu=a_0b^0+a_1b^1+a_2b^2 +a_3b^3=1\tag{a}$$ $$\psi=a_\mu a^\mu =a_0a^0+a_1a^1+a_2a^2+a_3a^3=-1+1=0\tag{b}$$ $$V_\mu=a_\nu b_\mu a^\nu=\left(a_\nu a^\nu\right)b_\mu=0\qquad \text{(using the result (b))}\tag{c}$$ $$W^{\nu\mu}=c^\nu a^\mu = \begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1&1&0&0\end{pmatrix}\tag{d}$$ $$P_{\mu\nu}=a_\mu b_\nu = \begin{pmatrix}0&-1&0&0\\0&1&0&0\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{e}$$ $$Q_\mu^{\,\,\,\nu}=b_\mu c^\nu=\begin{pmatrix}0&0&0&0\\0&0&0&1\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{f}$$ In the above extract the metric signature used is $\eta_{\mu\nu}=\text{diag}(-1,1,1,1)$ and for consistency I will follow this convention in the rest of this post. That's the end of the solutions, my problem is that I just don't understand why both the $a^\mu,\,b^\mu,\,c^\mu$ and $a_\mu,\,b_\mu,\,c_\mu$ are written as row vectors. I think either the contravariant or covariant vectors must be written as column vectors for matrix multiplication to even make sense. For example, to compute $a_\mu$ I could write $$a_\mu=\eta_{\nu\mu}a^\nu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}\begin{pmatrix}1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}^T\tag{1}$$ which is in the right form for matrix multiplication to apply, a $(4\times 4)\times(4\times 1)$ matrix product. In the second equality I used the fact that the Minkowski metric is symmetric, $\eta_{\mu\nu}=\eta_{\nu\mu}$ Now this is the part that really confuses me , I'm to understand that tensor expressions are commutative - since all the information is captured in the repeated (contracted) indices, just like in expression $(\mathrm{c})$ the $b_\mu$ was commuted past the $a^\nu$ . So if what I'm saying is really true, I may also write $$a_\mu=a^\nu\eta_{\nu\mu}=\begin{pmatrix}1&1&0&0\end{pmatrix}\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}\tag{2}$$ which is also in the right form for matrix multiplication to apply, a $(1\times 4)\times(4\times 4)$ matrix product. This also turns out to be the same expression the author got for $a_\mu$ . But $(1)\ne(2)$ , so why don't I get the same result? Remark: At first sight this question may look so trivial and stupid that it should just be downvoted without mercy. But I ask you to please bear with me, this is not obvious to me at all. Update in response to comments by @JackozeeHakkiuz and @Kurt G. In what follows I'm going to write out every little tedious step in the calculation, this may be painful or incredibly annoying to read for those of you well-acquainted with tensor expression manipulations. But I am a beginner to this, and it is clear that I do not understand how to work with tensors, so I feel I must labour the point. I'm sorry about this. Firstly to address the commutativity of $a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$ . Now I mentioned in the comments that I do not wish to compute these four-vectors using matrix-multiplication (as that was what lead to the confusion to begin with). Instead I will use the Einstein summation convention and then try to explain what the problem is: So, starting with $a^\nu\eta_{\mu\nu}$ , $$a_\mu=a^\nu\eta_{\mu\nu}=a^\nu\eta_{\nu\mu}=\begin{pmatrix}a^0\eta_{00} + a^1 \eta_{10} + a^2\eta_{20}+ a^3\eta_{30}\\a^0\eta_{01} + a^1 \eta_{11} + a^2\eta_{21}+ a^3\eta_{31}\\a^0\eta_{02} + a^1 \eta_{12} + a^2\eta_{22}+ a^3\eta_{23}\\a^0\eta_{03} + a^1 \eta_{13} + a^2\eta_{23}+ a^3\eta_{33}\end{pmatrix}$$ $$=\begin{pmatrix}a^0\eta_{00} + 0 + 0 + 0\\0 + a^1 \eta_{11} + 0 + 0\\0 + 0 + a^2\eta_{22}+ 0\\0 + 0 + 0+ a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}a^0\eta_{00}\\a^1 \eta_{11} \\a^2\eta_{22}\\a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}1\times (-1)\\ 1\times (1) \\0 \times (1)\\ 0\times (1)\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix}$$ Where I have used the four-vector $a^\mu=(1,1,0,0)$ and the fact that $\eta$ is diagonal: $\eta_{00}=-1$ , $\eta_{11}=\eta_{22}=\eta_{33}=1$ which indeed matches the expression for $a_\mu$ in the author's solution above. Now for the reverse order, $\eta_{\mu\nu}a^\nu$ , $$a_\mu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}\eta_{00}a^0 + \eta_{01}a^1 + \eta_{02}a^2+ \eta_{03}a^3\\\eta_{10}a^0 + \eta_{11}a^1 + \eta_{12}a^2+ \eta_{13}a^3\\\eta_{20}a^0 + \eta_{21}a^1 + \eta_{22}a^2+ \eta_{23}a^3\\\eta_{30}a^0 + \eta_{31}a^1 + \eta_{32}a^2+ \eta_{33}a^3\end{pmatrix}$$ $$=\begin{pmatrix}\eta_{00}a^0 + 0 + 0 + 0\\0 +  \eta_{11}a^1 + 0 + 0\\0 + 0 + \eta_{22}a^2+ 0\\0 + 0 + 0+ \eta_{33}a^3\end{pmatrix}=\begin{pmatrix}\eta_{00}a^0\\ \eta_{11}a^1 \\\eta_{22}a^2\\\eta_{33}a^3\end{pmatrix}=\begin{pmatrix}(-1)\times 1\\ (1)\times 1 \\(1) \times 0\\ (1)\times 0\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix}$$ Which shows that $a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$ (tensor factors commute). But do factors in tensor expressions always commute? For instance, I am going to use eqn. $(\mathrm{d})$ in the author's solution as an example. I could compute $W^{\nu\mu}$ in two different ways: Firstly, I could write $$W^{\nu\mu}=c^\nu \color{red}{a^\mu} = \begin{pmatrix}c^0a^0&c^0a^1&c^0a^2&c^0a^3\\c^1a^0&c^1a^1&c^1a^2&c^1a^3\\c^2a^0&c^2a^1&c^2a^2&c^3a^3\\c^3a^0&c^3a^1&c^3a^2&c^3a^3\end{pmatrix}$$ $$=\begin{pmatrix}0\times (1)& 0\times(1)&0\times (0)&1\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\1\times (1)& 1\times(1)&1\times (0)&1\times (0)\end{pmatrix}=\begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1& 1&0&0\end{pmatrix}$$ Great, that matches the expression $(\mathrm{d})$ as written by the author, where I used $c^\mu=(0,0,0,1)$ and $a^\mu=(1,1,0,0)$ . But I'm to understand that factors in tensor expressions are commutative, so what is to stop me from writing $$W^{\nu\mu}\stackrel{\color{blue}{?}}{=}\color{red}{a^\mu} c^\nu = \begin{pmatrix}a^0c^0&a^0c^1&a^0c^2&a^0c^3\\a^1c^0&a^1c^1&a^1c^2&a^1c^3\\a^2c^0&a^2c^1&a^2c^2&a^2c^3\\a^3c^0&a^3c^1&a^3c^2&a^3c^3\end{pmatrix}$$ $$=\begin{pmatrix}(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\end{pmatrix}=\begin{pmatrix}0&0&0&1\\0&0&0&1\\0&0&0&0\\0& 0&0&0\end{pmatrix}?$$ So $$\bbox[5px,border:2px solid darkgreen] {W^{\nu\mu}\stackrel{\color{blue}{?}}{=}c^\nu \color{red}{a^\mu}\ne \color{red}{a^\mu} c^\nu}$$ I think I'm misunderstanding the comments regarding the commutativity, but from what was written it was my impression that all information is captured in the tensor indices, hence I can commute tensor factors at will in any tensor expression. This was mentioned for the case of $$a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu$$ I assumed this statement generalized to any tensor expression. Clearly this is not the case for $W^{\nu\mu}$ so there is something else going on here I don't understand. CORRECTIONS: Sorry for the late edit, I've just realised I made a typo for second expression involving $W^{\nu\mu}$ ; in the previous version I (mistakenly) just switched the order of the indices, $\nu$ and $\mu$ instead of the four-vectors $a^\nu$ and $c^\mu$ . I have marked the relevant areas in red and it is the equation in the green box that I am questioning. Apologies for the excessively long post.","I came across this question and answer (sorry I don't have an electronic source for it, only a paper copy). After reading the answer it had me questioning the notation one uses to denote row/column vectors in tensor expressions: Consider the following contravariant vectors: , and . Derive the following quantities: , , , , and . Here are the solutions: First we can construct the covariant versions of these vectors, , and , then we have In the above extract the metric signature used is and for consistency I will follow this convention in the rest of this post. That's the end of the solutions, my problem is that I just don't understand why both the and are written as row vectors. I think either the contravariant or covariant vectors must be written as column vectors for matrix multiplication to even make sense. For example, to compute I could write which is in the right form for matrix multiplication to apply, a matrix product. In the second equality I used the fact that the Minkowski metric is symmetric, Now this is the part that really confuses me , I'm to understand that tensor expressions are commutative - since all the information is captured in the repeated (contracted) indices, just like in expression the was commuted past the . So if what I'm saying is really true, I may also write which is also in the right form for matrix multiplication to apply, a matrix product. This also turns out to be the same expression the author got for . But , so why don't I get the same result? Remark: At first sight this question may look so trivial and stupid that it should just be downvoted without mercy. But I ask you to please bear with me, this is not obvious to me at all. Update in response to comments by @JackozeeHakkiuz and @Kurt G. In what follows I'm going to write out every little tedious step in the calculation, this may be painful or incredibly annoying to read for those of you well-acquainted with tensor expression manipulations. But I am a beginner to this, and it is clear that I do not understand how to work with tensors, so I feel I must labour the point. I'm sorry about this. Firstly to address the commutativity of . Now I mentioned in the comments that I do not wish to compute these four-vectors using matrix-multiplication (as that was what lead to the confusion to begin with). Instead I will use the Einstein summation convention and then try to explain what the problem is: So, starting with , Where I have used the four-vector and the fact that is diagonal: , which indeed matches the expression for in the author's solution above. Now for the reverse order, , Which shows that (tensor factors commute). But do factors in tensor expressions always commute? For instance, I am going to use eqn. in the author's solution as an example. I could compute in two different ways: Firstly, I could write Great, that matches the expression as written by the author, where I used and . But I'm to understand that factors in tensor expressions are commutative, so what is to stop me from writing So I think I'm misunderstanding the comments regarding the commutativity, but from what was written it was my impression that all information is captured in the tensor indices, hence I can commute tensor factors at will in any tensor expression. This was mentioned for the case of I assumed this statement generalized to any tensor expression. Clearly this is not the case for so there is something else going on here I don't understand. CORRECTIONS: Sorry for the late edit, I've just realised I made a typo for second expression involving ; in the previous version I (mistakenly) just switched the order of the indices, and instead of the four-vectors and . I have marked the relevant areas in red and it is the equation in the green box that I am questioning. Apologies for the excessively long post.","a^\mu=(1,1,0,0) b^\mu=(0,1,0,0) c^\mu=(0,0,0,1) \phi=a_\mu b^\mu \psi=a_\mu a^\mu V_\mu=a_\nu b_\mu a^\nu W^{\nu\mu}=c^\nu a^\mu P_{\mu\nu}=a_\mu b_\nu Q_\mu^{\,\,\,\nu}=b_\mu c^\nu a_\mu=(-1,1,0,0) b_\mu=(0,1,0,0) c_\mu=(0,0,0,1) \phi=a_\mu b^\mu=a_0b^0+a_1b^1+a_2b^2 +a_3b^3=1\tag{a} \psi=a_\mu a^\mu =a_0a^0+a_1a^1+a_2a^2+a_3a^3=-1+1=0\tag{b} V_\mu=a_\nu b_\mu a^\nu=\left(a_\nu a^\nu\right)b_\mu=0\qquad \text{(using the result (b))}\tag{c} W^{\nu\mu}=c^\nu a^\mu = \begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1&1&0&0\end{pmatrix}\tag{d} P_{\mu\nu}=a_\mu b_\nu = \begin{pmatrix}0&-1&0&0\\0&1&0&0\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{e} Q_\mu^{\,\,\,\nu}=b_\mu c^\nu=\begin{pmatrix}0&0&0&0\\0&0&0&1\\0&0&0&0\\0&0&0&0\end{pmatrix}\tag{f} \eta_{\mu\nu}=\text{diag}(-1,1,1,1) a^\mu,\,b^\mu,\,c^\mu a_\mu,\,b_\mu,\,c_\mu a_\mu a_\mu=\eta_{\nu\mu}a^\nu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}\begin{pmatrix}1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1\\1\\0\\0\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}^T\tag{1} (4\times 4)\times(4\times 1) \eta_{\mu\nu}=\eta_{\nu\mu} (\mathrm{c}) b_\mu a^\nu a_\mu=a^\nu\eta_{\nu\mu}=\begin{pmatrix}1&1&0&0\end{pmatrix}\begin{pmatrix}-1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}=\begin{pmatrix}-1&1&0&0\end{pmatrix}\tag{2} (1\times 4)\times(4\times 4) a_\mu (1)\ne(2) a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu a^\nu\eta_{\mu\nu} a_\mu=a^\nu\eta_{\mu\nu}=a^\nu\eta_{\nu\mu}=\begin{pmatrix}a^0\eta_{00} + a^1 \eta_{10} + a^2\eta_{20}+ a^3\eta_{30}\\a^0\eta_{01} + a^1 \eta_{11} + a^2\eta_{21}+ a^3\eta_{31}\\a^0\eta_{02} + a^1 \eta_{12} + a^2\eta_{22}+ a^3\eta_{23}\\a^0\eta_{03} + a^1 \eta_{13} + a^2\eta_{23}+ a^3\eta_{33}\end{pmatrix} =\begin{pmatrix}a^0\eta_{00} + 0 + 0 + 0\\0 + a^1 \eta_{11} + 0 + 0\\0 + 0 + a^2\eta_{22}+ 0\\0 + 0 + 0+ a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}a^0\eta_{00}\\a^1 \eta_{11} \\a^2\eta_{22}\\a^3\eta_{33}\end{pmatrix}=\begin{pmatrix}1\times (-1)\\ 1\times (1) \\0 \times (1)\\ 0\times (1)\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix} a^\mu=(1,1,0,0) \eta \eta_{00}=-1 \eta_{11}=\eta_{22}=\eta_{33}=1 a_\mu \eta_{\mu\nu}a^\nu a_\mu=\eta_{\mu\nu}a^\nu=\begin{pmatrix}\eta_{00}a^0 + \eta_{01}a^1 + \eta_{02}a^2+ \eta_{03}a^3\\\eta_{10}a^0 + \eta_{11}a^1 + \eta_{12}a^2+ \eta_{13}a^3\\\eta_{20}a^0 + \eta_{21}a^1 + \eta_{22}a^2+ \eta_{23}a^3\\\eta_{30}a^0 + \eta_{31}a^1 + \eta_{32}a^2+ \eta_{33}a^3\end{pmatrix} =\begin{pmatrix}\eta_{00}a^0 + 0 + 0 + 0\\0 +  \eta_{11}a^1 + 0 + 0\\0 + 0 + \eta_{22}a^2+ 0\\0 + 0 + 0+ \eta_{33}a^3\end{pmatrix}=\begin{pmatrix}\eta_{00}a^0\\ \eta_{11}a^1 \\\eta_{22}a^2\\\eta_{33}a^3\end{pmatrix}=\begin{pmatrix}(-1)\times 1\\ (1)\times 1 \\(1) \times 0\\ (1)\times 0\end{pmatrix}=\begin{pmatrix}-1\\ 1 \\ 0 \\ 0\end{pmatrix} a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu (\mathrm{d}) W^{\nu\mu} W^{\nu\mu}=c^\nu \color{red}{a^\mu} = \begin{pmatrix}c^0a^0&c^0a^1&c^0a^2&c^0a^3\\c^1a^0&c^1a^1&c^1a^2&c^1a^3\\c^2a^0&c^2a^1&c^2a^2&c^3a^3\\c^3a^0&c^3a^1&c^3a^2&c^3a^3\end{pmatrix} =\begin{pmatrix}0\times (1)& 0\times(1)&0\times (0)&1\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\0\times (1)& 0\times(1)&0\times (0)&0\times (0)\\1\times (1)& 1\times(1)&1\times (0)&1\times (0)\end{pmatrix}=\begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&0\\1& 1&0&0\end{pmatrix} (\mathrm{d}) c^\mu=(0,0,0,1) a^\mu=(1,1,0,0) W^{\nu\mu}\stackrel{\color{blue}{?}}{=}\color{red}{a^\mu} c^\nu = \begin{pmatrix}a^0c^0&a^0c^1&a^0c^2&a^0c^3\\a^1c^0&a^1c^1&a^1c^2&a^1c^3\\a^2c^0&a^2c^1&a^2c^2&a^2c^3\\a^3c^0&a^3c^1&a^3c^2&a^3c^3\end{pmatrix} =\begin{pmatrix}(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(1)\times 0& (1)\times 0&(1)\times 0&(1)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\\(0)\times 0& (0)\times 0&(0)\times 0&(0)\times 1\end{pmatrix}=\begin{pmatrix}0&0&0&1\\0&0&0&1\\0&0&0&0\\0& 0&0&0\end{pmatrix}? \bbox[5px,border:2px solid darkgreen]
{W^{\nu\mu}\stackrel{\color{blue}{?}}{=}c^\nu \color{red}{a^\mu}\ne \color{red}{a^\mu} c^\nu} a_\mu=a^\nu\eta_{\mu\nu}=\eta_{\mu\nu}a^\nu W^{\nu\mu} W^{\nu\mu} \nu \mu a^\nu c^\mu","['matrices', 'solution-verification', 'vectors', 'intuition', 'tensors']"
51,Is $\exp(\log(A))=A$ for any matrix $A$ where $\log(A)$ is defined?,Is  for any matrix  where  is defined?,\exp(\log(A))=A A \log(A),"It is possible to show that: $$\exp(\log(A)) = A$$ When $A$ is diagonalizable (and the logarithm exists). Furthermore it is possible to show that when $\| A - I\| < 1$ then $\exp(\log(A))=A$ . The proof for that relies of the fact that any matrix can be approximated by a series of diagonalizable matrices, and on the fact that when $\| A - I\| < 1$ the logarithm always exists. This let's our approximation $A_n \to A$ eventually (for large enough $n$ ) lie in the ball of radius $1$ around $I$ which allows us to invoke the continuity of $\exp, \log$ and finish the theorem. My question is, is the equality $\exp(\log(A)) = A$ always true when $\log(A)$ exists? The same proof could be applied only if the $\log$ exists in some open neighborhood of $A$ , but that's not always the case. For example taking $A = 2\cdot I$ then for any $\epsilon$ we have $\log((2 + \epsilon)\cdot I)$ doesn't exist. Despite that, $\exp(\log(2\cdot I)) = 2\cdot I$ . Is there a proof that $\exp(\log(A)) = A$ when $\log(A)$ exists? Is there a counterexample? In this question the matrix logarithm is defined via the power series: $$\log(A) = \sum_{n=1}^\infty (-1)^{n+1} \frac{(A-I)^n}{n}$$","It is possible to show that: When is diagonalizable (and the logarithm exists). Furthermore it is possible to show that when then . The proof for that relies of the fact that any matrix can be approximated by a series of diagonalizable matrices, and on the fact that when the logarithm always exists. This let's our approximation eventually (for large enough ) lie in the ball of radius around which allows us to invoke the continuity of and finish the theorem. My question is, is the equality always true when exists? The same proof could be applied only if the exists in some open neighborhood of , but that's not always the case. For example taking then for any we have doesn't exist. Despite that, . Is there a proof that when exists? Is there a counterexample? In this question the matrix logarithm is defined via the power series:","\exp(\log(A)) = A A \| A - I\| < 1 \exp(\log(A))=A \| A - I\| < 1 A_n \to A n 1 I \exp, \log \exp(\log(A)) = A \log(A) \log A A = 2\cdot I \epsilon \log((2 + \epsilon)\cdot I) \exp(\log(2\cdot I)) = 2\cdot I \exp(\log(A)) = A \log(A) \log(A) = \sum_{n=1}^\infty (-1)^{n+1} \frac{(A-I)^n}{n}","['matrices', 'exponential-function', 'matrix-exponential']"
52,How to find the maximum of $\boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}$ subject to $\boldsymbol{q}^T \boldsymbol{x}=1$?,How to find the maximum of  subject to ?,\boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x} \boldsymbol{q}^T \boldsymbol{x}=1,"I want to solve the following problem in $\boldsymbol{x} \in \mathbb R^{n}$ $$\begin{array}{ll} \text{maximize} & \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}\\ \text{subject to} & \boldsymbol{q}^T \boldsymbol{x} = 1\\ & x_i \geq 0\end{array}$$ where matrix $\boldsymbol{A}$ is positive definite matrix and $x_i$ denotes the $i$ -th entry of $\boldsymbol{x}$ . Actually, I have tried to use Lagrangian multiplier. I directly transformed the objective function to $-\boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x} + \lambda ( \boldsymbol{q}^T \boldsymbol{x} - 1 )$ and take its first derivative and set that to zero. However, the solution obtained did not maximize the objective function, it just makes $\boldsymbol{x}^T\boldsymbol{A} \boldsymbol{x}$ smaller and smaller. Then I found that the solution of $\min_{\boldsymbol{x}} \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}$ with the same constraints is the same with that of $\max_{\boldsymbol{x}} \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}$ . Any comments would be appreciated! Update As comments suggested, I changed the situation to $x_i \geq 0, \forall i$ . Thus for example, when $\boldsymbol{A}= \left[\begin{matrix} {2 \; 0\\ 0 \;1 }\end{matrix} \right]$ and $\boldsymbol{q} = [1,1]^T$ . The problem has a solution $\boldsymbol{x} = [1 ,0]^T$ that maximize the objective function. Can this extend to more general case?","I want to solve the following problem in where matrix is positive definite matrix and denotes the -th entry of . Actually, I have tried to use Lagrangian multiplier. I directly transformed the objective function to and take its first derivative and set that to zero. However, the solution obtained did not maximize the objective function, it just makes smaller and smaller. Then I found that the solution of with the same constraints is the same with that of . Any comments would be appreciated! Update As comments suggested, I changed the situation to . Thus for example, when and . The problem has a solution that maximize the objective function. Can this extend to more general case?","\boldsymbol{x} \in \mathbb R^{n} \begin{array}{ll} \text{maximize} & \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x}\\ \text{subject to} & \boldsymbol{q}^T \boldsymbol{x} = 1\\ & x_i \geq 0\end{array} \boldsymbol{A} x_i i \boldsymbol{x} -\boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x} + \lambda ( \boldsymbol{q}^T \boldsymbol{x} - 1 ) \boldsymbol{x}^T\boldsymbol{A} \boldsymbol{x} \min_{\boldsymbol{x}} \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x} \max_{\boldsymbol{x}} \boldsymbol{x}^T \boldsymbol{A} \boldsymbol{x} x_i \geq 0, \forall i \boldsymbol{A}= \left[\begin{matrix} {2 \; 0\\ 0 \;1 }\end{matrix} \right] \boldsymbol{q} = [1,1]^T \boldsymbol{x} = [1 ,0]^T","['matrices', 'optimization', 'quadratic-programming', 'non-convex-optimization']"
53,Eigenvalues and elementary row operations,Eigenvalues and elementary row operations,,"We know that elementary row operations do not change the determinant of a matrix but may change the associated eigenvalues. Consider an example, say two $5 \times 5$ matrix are given: $$A = \begin{pmatrix} 0 & 1 & 0 & 0 & 0\\ a & b & 0 & 0 & 0\\ 0 & 0 & p & q & r\\ 0 & 0 & s & t & u\\ 0 & 0 & v & w & x\\ \end{pmatrix}, \hspace{2cm}    B = \begin{pmatrix} 0 & 1 & 0 & 0 & 0\\ a & b & 0 & 0 & 0\\ 0 & 0 & p & q & r\\ 0 & 0 & s & t & u\\ ka & kb & v & w & x\\ \end{pmatrix} $$  Now $B$ can easily be reduced to $A$ by using the following operation on $B$ $$R_5 - kR_2$$ Now these two have the same eigenvalues. It is cumbersome to try to symbolically calculate the eigenvalues to show they are indeed same (I have tried tons of matrices with random numbers in Mathematica ). $A$ is a block diagonal matrix and $B$ is reduceable to one. If you talked about systems, $A$ shows two decoupled spaces (of dimensions $2$ and $3$) within the $5-D$ vector space of $A$. Can anyone prove that such a pair of matrices always have same eigenvalues? Is there any property that says so? Does $A$ being a block diagonal matrix have to do anything with the eigenvalues being the same? Any insight or discussion is welcome! Please correct me if I used any term loosely or wrongly.","We know that elementary row operations do not change the determinant of a matrix but may change the associated eigenvalues. Consider an example, say two $5 \times 5$ matrix are given: $$A = \begin{pmatrix} 0 & 1 & 0 & 0 & 0\\ a & b & 0 & 0 & 0\\ 0 & 0 & p & q & r\\ 0 & 0 & s & t & u\\ 0 & 0 & v & w & x\\ \end{pmatrix}, \hspace{2cm}    B = \begin{pmatrix} 0 & 1 & 0 & 0 & 0\\ a & b & 0 & 0 & 0\\ 0 & 0 & p & q & r\\ 0 & 0 & s & t & u\\ ka & kb & v & w & x\\ \end{pmatrix} $$  Now $B$ can easily be reduced to $A$ by using the following operation on $B$ $$R_5 - kR_2$$ Now these two have the same eigenvalues. It is cumbersome to try to symbolically calculate the eigenvalues to show they are indeed same (I have tried tons of matrices with random numbers in Mathematica ). $A$ is a block diagonal matrix and $B$ is reduceable to one. If you talked about systems, $A$ shows two decoupled spaces (of dimensions $2$ and $3$) within the $5-D$ vector space of $A$. Can anyone prove that such a pair of matrices always have same eigenvalues? Is there any property that says so? Does $A$ being a block diagonal matrix have to do anything with the eigenvalues being the same? Any insight or discussion is welcome! Please correct me if I used any term loosely or wrongly.",,['matrices']
54,What are all the uses of the determinant?,What are all the uses of the determinant?,,"I've learned how to calculate the determinant but what is the determinant used for? So far, I only know that there is no inverse if the determinant is 0.","I've learned how to calculate the determinant but what is the determinant used for? So far, I only know that there is no inverse if the determinant is 0.",,"['matrices', 'big-list']"
55,Is the determinant of a RREF matrix equal to the determinant of the original matrix?,Is the determinant of a RREF matrix equal to the determinant of the original matrix?,,"Prove or disprove: If $R$ is the reduced row echelon form (RREF) of $A$ , then $\det A = \det R$ , where $A$ is an $n \times n$ matrix.","Prove or disprove: If is the reduced row echelon form (RREF) of , then , where is an matrix.",R A \det A = \det R A n \times n,"['matrices', 'determinant', 'gaussian-elimination']"
56,generalized ordering of positive semi-definite matrix by eigenvalues,generalized ordering of positive semi-definite matrix by eigenvalues,,"I know positive semi-definite matrices are generalizations of non-negative numbers. So ""ordering"" of the two systems should be pretty much like each other. How to prove the following theorem? For two symmetric $X$ and $Y$, if $X \geq Y$, then $\lambda_i(X) \geq \lambda_i(Y)$, for every $i$. $\lambda_i(\cdot)$ denotes the $i$-th largest eigenvalue. And what about the converse statement? Is it true? Thanks a lot.","I know positive semi-definite matrices are generalizations of non-negative numbers. So ""ordering"" of the two systems should be pretty much like each other. How to prove the following theorem? For two symmetric $X$ and $Y$, if $X \geq Y$, then $\lambda_i(X) \geq \lambda_i(Y)$, for every $i$. $\lambda_i(\cdot)$ denotes the $i$-th largest eigenvalue. And what about the converse statement? Is it true? Thanks a lot.",,['matrices']
57,Every Lie algebra endomorphism of $\mathfrak{so}(3)$ is given by the anticommutator with a symmetric matrix.,Every Lie algebra endomorphism of  is given by the anticommutator with a symmetric matrix.,\mathfrak{so}(3),"I want to prove that for every Lie algebra endomorphism $T$ on $\mathfrak g=\mathfrak{so}(3)$ , there exists a symmetric $3\times 3$ matrix $B$ such that $T( x)=Bx+xB$ for all $x \in \mathfrak g$ . I cannot figure this out. Edit: This is a problem in the book Quantum Mechanics for Mathematicians by Takhtajan Edit The endomorphism is assumed to be symmetric.","I want to prove that for every Lie algebra endomorphism on , there exists a symmetric matrix such that for all . I cannot figure this out. Edit: This is a problem in the book Quantum Mechanics for Mathematicians by Takhtajan Edit The endomorphism is assumed to be symmetric.",T \mathfrak g=\mathfrak{so}(3) 3\times 3 B T( x)=Bx+xB x \in \mathfrak g,"['matrices', 'lie-algebras', 'symmetric-matrices', 'semisimple-lie-algebras']"
58,Vertices in a graph with the same number of closed walks,Vertices in a graph with the same number of closed walks,,"A graph is called walk regular if the number of closed walks starting from vertex $u$ of length $k$ does not depend on $u$ . If $A$ is the adjacency matrix of the graph, this means that $A^k$ has equal diagonal values for every $k>0$ . I'm interested in a weaker condition, that there exists two vertices $u,v$ such that the number of closed walks of length $k$ is the same for $u$ and $v$ . Are there results or references about this? In other words, when does $[A^k]_{uu} = [A^k]_{vv}$ hold? For example, it would be nice to have necessary/sufficient conditions for this property to hold. If there is a graph automorphism that maps $u\to v$ then certainly $u$ and $v$ have the same number of closed walks, though the existence of such an automorphism is not necessary (an example is the Folkman graph which is walk-regular but not vertex-transitive). I'm aware that if the graph has $n$ vertices and $u,v$ have the same number of closed walks for $k=1,2,\ldots n-1$ , then by Hamilton-Cayley's theorem it is true also for all $k\geq n$ . I'm asking in the case of a simple undirected graph, but if there are generalizations for directed and/or weighted graphs they're highly welcome.","A graph is called walk regular if the number of closed walks starting from vertex of length does not depend on . If is the adjacency matrix of the graph, this means that has equal diagonal values for every . I'm interested in a weaker condition, that there exists two vertices such that the number of closed walks of length is the same for and . Are there results or references about this? In other words, when does hold? For example, it would be nice to have necessary/sufficient conditions for this property to hold. If there is a graph automorphism that maps then certainly and have the same number of closed walks, though the existence of such an automorphism is not necessary (an example is the Folkman graph which is walk-regular but not vertex-transitive). I'm aware that if the graph has vertices and have the same number of closed walks for , then by Hamilton-Cayley's theorem it is true also for all . I'm asking in the case of a simple undirected graph, but if there are generalizations for directed and/or weighted graphs they're highly welcome.","u k u A A^k k>0 u,v k u v [A^k]_{uu} = [A^k]_{vv} u\to v u v n u,v k=1,2,\ldots n-1 k\geq n","['matrices', 'graph-theory', 'reference-request', 'adjacency-matrix']"
59,A sort of AM-GM inequality for matrices,A sort of AM-GM inequality for matrices,,"Let $A, B$ be symmetric, positive definite matrices. Is it true that $$ tr \left( (A B^2 A)^{1/2}\right) \leq \frac{1}{4} \| A + B\|_F^2? $$ In the diagonal case, the left-hand side is $tr(AB)$ and, denoting by $\lambda_i$ and $\mu_i$ the diagonal elements of $A$ and $B$ , the inequality follows from the usual AM-GM inequality: $$ tr(AB) =\sum_i \mu_i \lambda_i \leq \frac{1}{4} \sum_i (\mu_i + \lambda_i)^2 = \frac 14 \| A + B\|_F^2. $$ To convince ourselves that the inequality is true, we can run the following python code, which prints 0.99999375. import numpy as np import scipy.linalg as la  n, d, ratios = 100000, 3, [] for i in range(n):     A = np.random.randn(d, d)     B = np.random.randn(d, d)     A, B = A.dot(A.T), B.dot(B.T)     lhs = np.matrix.trace(la.sqrtm(A.dot(B).dot(B).dot(A)))     rhs = (1/4)*la.norm(A + B, ord='fro')**2     ratios.append(lhs/rhs)  print(np.max(ratios)) After a little literature search, it seems that a slightly modified version of the inequality, with a factor $1/2$ instead of $1/4$ in the right-hand side, is a consequence of Theorem IX.4.2 in the textbook Matrix Analysis by Rajendra Bhatia. Statement of Theorem IX.4.2: for any two matrices, $$s_j(A^*B) \leq \frac{1}{2} \, s_j(AA^* + BB^*),$$ where $s_j$ , $j = 1, \dots, n$ , denotes the $j$ -th singular value. Proof that this partially answers the question: we can rewrite the left-hand side as $$ GM(A, B) := tr \left( (A B^2 A)^{1/2}\right) = tr \left(\left((BA)^*BA\right)^{1/2} \right) = \sum_j s_j(BA). $$ Using the quoted result, we therefore obtain \begin{align} GM(A, B) &\leq \frac{1}{2} \sum_j s_j(A^2 + B^2) = \frac{1}{2} tr(A^2 + B^2) \\ &= \frac 14 tr((A + B)^2 + (A - B)^2) \\ &= \frac 14 tr(A + B)^2 + \frac{1}{4} tr(A - B)^2 \\ &= \frac 14 \|A + B\|_F^2 + \frac 14 \|A - B\|_F^2 \\ &\leq \frac 12 \|A + B\|_F^2 =: 2 \, AM(A, B). \end{align} Let us note that numerical experiments show that the inequality $$ \frac{1}{2} \sum_j s_j(A^2 + B^2) \leq 2 \, AM(A, B), $$ is sharp.","Let be symmetric, positive definite matrices. Is it true that In the diagonal case, the left-hand side is and, denoting by and the diagonal elements of and , the inequality follows from the usual AM-GM inequality: To convince ourselves that the inequality is true, we can run the following python code, which prints 0.99999375. import numpy as np import scipy.linalg as la  n, d, ratios = 100000, 3, [] for i in range(n):     A = np.random.randn(d, d)     B = np.random.randn(d, d)     A, B = A.dot(A.T), B.dot(B.T)     lhs = np.matrix.trace(la.sqrtm(A.dot(B).dot(B).dot(A)))     rhs = (1/4)*la.norm(A + B, ord='fro')**2     ratios.append(lhs/rhs)  print(np.max(ratios)) After a little literature search, it seems that a slightly modified version of the inequality, with a factor instead of in the right-hand side, is a consequence of Theorem IX.4.2 in the textbook Matrix Analysis by Rajendra Bhatia. Statement of Theorem IX.4.2: for any two matrices, where , , denotes the -th singular value. Proof that this partially answers the question: we can rewrite the left-hand side as Using the quoted result, we therefore obtain Let us note that numerical experiments show that the inequality is sharp.","A, B 
tr \left( (A B^2 A)^{1/2}\right) \leq \frac{1}{4} \| A + B\|_F^2?
 tr(AB) \lambda_i \mu_i A B 
tr(AB) =\sum_i \mu_i \lambda_i \leq \frac{1}{4} \sum_i (\mu_i + \lambda_i)^2 = \frac 14 \| A + B\|_F^2.
 1/2 1/4 s_j(A^*B) \leq \frac{1}{2} \, s_j(AA^* + BB^*), s_j j = 1, \dots, n j 
GM(A, B) := tr \left( (A B^2 A)^{1/2}\right) = tr \left(\left((BA)^*BA\right)^{1/2} \right) = \sum_j s_j(BA).
 \begin{align}
GM(A, B) &\leq \frac{1}{2} \sum_j s_j(A^2 + B^2) = \frac{1}{2} tr(A^2 + B^2) \\
&= \frac 14 tr((A + B)^2 + (A - B)^2) \\
&= \frac 14 tr(A + B)^2 + \frac{1}{4} tr(A - B)^2 \\
&= \frac 14 \|A + B\|_F^2 + \frac 14 \|A - B\|_F^2 \\
&\leq \frac 12 \|A + B\|_F^2 =: 2 \, AM(A, B).
\end{align} 
\frac{1}{2} \sum_j s_j(A^2 + B^2) \leq 2 \, AM(A, B),
","['matrices', 'a.m.-g.m.-inequality']"
60,"A is an antisymmetric matrix (of even size). B is another matrix such that $b_{i,j}=a_{i,j}+c$. Prove that |A|=|B|",A is an antisymmetric matrix (of even size). B is another matrix such that . Prove that |A|=|B|,"b_{i,j}=a_{i,j}+c",I know that B would look something like this: $$\begin{bmatrix} c & a_{12}+c  &...&&a_{1n}+c \\ -a_{12}+c & c &...&&a_{2n}+c  \\ . \\ . \\ . \\ -a_{1n}+c & -a_{2n}+c & &...&c  \end{bmatrix}$$ And that if it was of uneven size the determinant would be $0$. I also know that $|B|=|B^t|$ and $|A|=|A^t|=|-A|=(-1)^n|A|$. Any hint?,I know that B would look something like this: $$\begin{bmatrix} c & a_{12}+c  &...&&a_{1n}+c \\ -a_{12}+c & c &...&&a_{2n}+c  \\ . \\ . \\ . \\ -a_{1n}+c & -a_{2n}+c & &...&c  \end{bmatrix}$$ And that if it was of uneven size the determinant would be $0$. I also know that $|B|=|B^t|$ and $|A|=|A^t|=|-A|=(-1)^n|A|$. Any hint?,,"['matrices', 'determinant']"
61,$\text{vec}\left(A\otimes B\right)$ is not $\text{vec}\left(A\right) \otimes \text{vec}\left(B\right)$,is not,\text{vec}\left(A\otimes B\right) \text{vec}\left(A\right) \otimes \text{vec}\left(B\right),"Let $A$ and $B$ be two square matrices of dimension $a$ and $b$. $\text{vec}\left(\cdot\right)$ is the vectorization of a matrix . Now $v_0=\text{vec}\left(A\otimes B\right)$ is not $v_1=\text{vec}\left(A\right) \otimes \text{vec}\left(B\right)$, but the set of vector elements in both is equal, but $v_0$ and $v_1$ seem to be related by a permutation of elements: $v_0 = P_{a,b} v_1$ Can this permutation matrix $P_{a,b}$ be described in general?","Let $A$ and $B$ be two square matrices of dimension $a$ and $b$. $\text{vec}\left(\cdot\right)$ is the vectorization of a matrix . Now $v_0=\text{vec}\left(A\otimes B\right)$ is not $v_1=\text{vec}\left(A\right) \otimes \text{vec}\left(B\right)$, but the set of vector elements in both is equal, but $v_0$ and $v_1$ seem to be related by a permutation of elements: $v_0 = P_{a,b} v_1$ Can this permutation matrix $P_{a,b}$ be described in general?",,"['matrices', 'kronecker-product', 'vectorization']"
62,Derivative of determinant of symmetric matrix wrt a scalar,Derivative of determinant of symmetric matrix wrt a scalar,,"For a given square symmetric invertible matrix $\mathbf{X}$ and scalar $\alpha$ (such that the entries of $\mathbf{X}$ depend on $\alpha$), I would like to use the following well-known expression for the derivative of the determinant wrt a scalar (e.g. see wikipedia ): $$ \frac{\partial}{\partial \alpha} \det(\mathbf{X}) = \det(\mathbf{X}) \operatorname{tr} \left( \mathbf{X}^{-1} \frac{\partial \mathbf{X}}{\partial \alpha} \right) $$ However, I am concerned that this expression may not apply when there exists any kind of special ""structure"" in the matrix (e.g. an $n \times n$ symmetric matrix contains fewer than $n^2$ independent entries). Consider for a moment another situation, where such structure matters: differentiating the determinant wrt the matrix entries (instead of a scalar). First, let $\mathbf{Y}$ be a square invertible ""unstructured"" matrix (i.e. all elements are independent). It is well-known (e.g. see The Matrix Cookbook section 2.1.2, or wikipedia ) that, in this case: $$ \frac{\partial}{\partial \mathbf{Y}} \det(\mathbf{Y}) = \det(\mathbf{Y}) \, (\mathbf{Y}^{-1})^T $$ Now consider $\mathbf{X}$ again, a square symmetric matrix. In this case, the entries are no longer independent (as described in The Matrix Cookbook , beginning of section 2.8), so we have instead (see section 2.8.2 for the symmetric case): $$ \frac{\partial}{\partial \mathbf{X}} \det(\mathbf{X}) = \det(\mathbf{X}) \, (2 \mathbf{X}^{-1} - (\mathbf{X}^{-1} \circ \mathbf{I})) $$ So, when differentiating the determinant wrt matrix entries, the matrix structure clearly matters. Here is my question: when differentiating a matrix determinant wrt a scalar, does the matrix structure matter? And if so, what is the correct expression for $\frac{\partial}{\partial \alpha} \det(\mathbf{X})$ for the specific case where $\mathbf{X}$ is symmetric?","For a given square symmetric invertible matrix $\mathbf{X}$ and scalar $\alpha$ (such that the entries of $\mathbf{X}$ depend on $\alpha$), I would like to use the following well-known expression for the derivative of the determinant wrt a scalar (e.g. see wikipedia ): $$ \frac{\partial}{\partial \alpha} \det(\mathbf{X}) = \det(\mathbf{X}) \operatorname{tr} \left( \mathbf{X}^{-1} \frac{\partial \mathbf{X}}{\partial \alpha} \right) $$ However, I am concerned that this expression may not apply when there exists any kind of special ""structure"" in the matrix (e.g. an $n \times n$ symmetric matrix contains fewer than $n^2$ independent entries). Consider for a moment another situation, where such structure matters: differentiating the determinant wrt the matrix entries (instead of a scalar). First, let $\mathbf{Y}$ be a square invertible ""unstructured"" matrix (i.e. all elements are independent). It is well-known (e.g. see The Matrix Cookbook section 2.1.2, or wikipedia ) that, in this case: $$ \frac{\partial}{\partial \mathbf{Y}} \det(\mathbf{Y}) = \det(\mathbf{Y}) \, (\mathbf{Y}^{-1})^T $$ Now consider $\mathbf{X}$ again, a square symmetric matrix. In this case, the entries are no longer independent (as described in The Matrix Cookbook , beginning of section 2.8), so we have instead (see section 2.8.2 for the symmetric case): $$ \frac{\partial}{\partial \mathbf{X}} \det(\mathbf{X}) = \det(\mathbf{X}) \, (2 \mathbf{X}^{-1} - (\mathbf{X}^{-1} \circ \mathbf{I})) $$ So, when differentiating the determinant wrt matrix entries, the matrix structure clearly matters. Here is my question: when differentiating a matrix determinant wrt a scalar, does the matrix structure matter? And if so, what is the correct expression for $\frac{\partial}{\partial \alpha} \det(\mathbf{X})$ for the specific case where $\mathbf{X}$ is symmetric?",,"['matrices', 'derivatives', 'determinant', 'trace', 'matrix-calculus']"
63,Geometry of Commuting Hermitian Matrices,Geometry of Commuting Hermitian Matrices,,"I am a physicist working on a project dedicated to the quantisation of commuting matrix models. In the appropriate formalism this problem is reduced to a quantisation in a curved space -- the space of commuting matrices. The general prescription for quantisation in curved space involves ambiguity of the Hamiltonian operator proportional to the scalar curvature of the curved space - hence my question. A set of $p$ commuting $n\times n$ hermitian matrices $X^{\mu}$ for $\mu=1,\dots p$, is parametrised in terms of a set of $p$ diagonal matrices $\Lambda^{\mu}$ and an unitary matrix $U$ via: $X^{\mu}=U\,\Lambda^{\mu}\,U^{\dagger}~~$   for $~\mu=1\dots p$, clearly not all degrees of U contribute to this parametrisation, for example a reparametrisation $U'= D\,U$, where $D$ is a diagonal unitary matrix would result in the same set of commuting matrices. In other words only the elements of the quotient space $U(n)\,/\,U(1)^n$, which is the maximal flag manifold $F_n$, contribute to the parametrisation. The metric on the resulting curved manifold can be calculated as a pull-back of the metric on the space of hermitian matrices defined as: $ds_{X}^2=Tr\,\left( dX^{\mu}dX^{\mu}\right) $ , Using that $U^{\dagger}d X^{\mu} U=d\Lambda^{\mu}+[\theta,\Lambda^{\mu}]~~$, where $\theta$ is the Maurer-Cartan form $\theta=U^{\dagger}dU$, one can write the induced metric as: $ds^2=\sum\limits_{i=1}^n(d\vec\lambda_i)^2+2\sum\limits_{i<j}(\vec\lambda_i-\vec\lambda_j)^2\theta_{ij}\bar{\theta}_{ij}~~$, where $~~\vec \lambda_i =(\Lambda^1_{ii},\dots,\Lambda^p_{ii})$ . Now I need the Riemann curvature of the above metric. It seems that it is convenient to work in tetrad formalism, using tetrads $E_{ij}=|\vec\lambda_i-\vec\lambda_j|\,\theta_{ij}$, for $i<j$. The problem is that $d E_{ij}$ will now contain a term proportional to $(\theta_{ii}-\theta_{jj})\wedge\theta_{ij}$ and since $\theta_{ii}$ are not part of the basis the spin curvature cannot be written easily without using the explicit parametrisation of $U(n)$. Intuitively, I know that the scalar curvature should depend only on the lambdas ($\vec\lambda_i$), and I have verified that explicitly for $SU(2)$ and $SU(3)$, however a general result seems to require some invariant way to express the pullback of the term $(\theta_{ii}-\theta_{jj})\wedge\theta_{ij}$ on the submanifold spanned by the off diagonal $\theta$'s. I was wondering if mathematicians have explored the manifold of commuting hermitian matrices. In fact even a reference to a convenient parametrisation of the maximal flag manifold $F_n$ would greatly help me in deriving a general expression for the scalar curvature. Any comments/suggestions are welcomed.","I am a physicist working on a project dedicated to the quantisation of commuting matrix models. In the appropriate formalism this problem is reduced to a quantisation in a curved space -- the space of commuting matrices. The general prescription for quantisation in curved space involves ambiguity of the Hamiltonian operator proportional to the scalar curvature of the curved space - hence my question. A set of $p$ commuting $n\times n$ hermitian matrices $X^{\mu}$ for $\mu=1,\dots p$, is parametrised in terms of a set of $p$ diagonal matrices $\Lambda^{\mu}$ and an unitary matrix $U$ via: $X^{\mu}=U\,\Lambda^{\mu}\,U^{\dagger}~~$   for $~\mu=1\dots p$, clearly not all degrees of U contribute to this parametrisation, for example a reparametrisation $U'= D\,U$, where $D$ is a diagonal unitary matrix would result in the same set of commuting matrices. In other words only the elements of the quotient space $U(n)\,/\,U(1)^n$, which is the maximal flag manifold $F_n$, contribute to the parametrisation. The metric on the resulting curved manifold can be calculated as a pull-back of the metric on the space of hermitian matrices defined as: $ds_{X}^2=Tr\,\left( dX^{\mu}dX^{\mu}\right) $ , Using that $U^{\dagger}d X^{\mu} U=d\Lambda^{\mu}+[\theta,\Lambda^{\mu}]~~$, where $\theta$ is the Maurer-Cartan form $\theta=U^{\dagger}dU$, one can write the induced metric as: $ds^2=\sum\limits_{i=1}^n(d\vec\lambda_i)^2+2\sum\limits_{i<j}(\vec\lambda_i-\vec\lambda_j)^2\theta_{ij}\bar{\theta}_{ij}~~$, where $~~\vec \lambda_i =(\Lambda^1_{ii},\dots,\Lambda^p_{ii})$ . Now I need the Riemann curvature of the above metric. It seems that it is convenient to work in tetrad formalism, using tetrads $E_{ij}=|\vec\lambda_i-\vec\lambda_j|\,\theta_{ij}$, for $i<j$. The problem is that $d E_{ij}$ will now contain a term proportional to $(\theta_{ii}-\theta_{jj})\wedge\theta_{ij}$ and since $\theta_{ii}$ are not part of the basis the spin curvature cannot be written easily without using the explicit parametrisation of $U(n)$. Intuitively, I know that the scalar curvature should depend only on the lambdas ($\vec\lambda_i$), and I have verified that explicitly for $SU(2)$ and $SU(3)$, however a general result seems to require some invariant way to express the pullback of the term $(\theta_{ii}-\theta_{jj})\wedge\theta_{ij}$ on the submanifold spanned by the off diagonal $\theta$'s. I was wondering if mathematicians have explored the manifold of commuting hermitian matrices. In fact even a reference to a convenient parametrisation of the maximal flag manifold $F_n$ would greatly help me in deriving a general expression for the scalar curvature. Any comments/suggestions are welcomed.",,"['matrices', 'reference-request', 'manifolds', 'curvature']"
64,Preservation of Positive-Definiteness from Small Perturbations,Preservation of Positive-Definiteness from Small Perturbations,,"Let $A$ with real positive entries be a Hermitian positive definite matrix. I'm wondering if one perturbs $A$, e.g., $\hat{A}=A+\Delta A$, would the matrix still be positive definite? I'm told this is particularly true so long as the perturbation satisfies the condition number bound $$\frac{\|\Delta A\|}{\|A\|} < \frac{1}{K(A)},$$ where $K(A)$ is the condition number $K(A)=\|A^{-1}\|\|A\|$ (all in the Frobenius norm). I'm stuck figuring out how to prove this. Since $A$ is positive definite, it has a unique Cholesky decomposition $A=LL^*$ and so $\hat A=LL^*+\Delta A$. But this leads me nowhere in proving that $\hat A$ too has unique Cholesky decomposition dependent on the condition number.","Let $A$ with real positive entries be a Hermitian positive definite matrix. I'm wondering if one perturbs $A$, e.g., $\hat{A}=A+\Delta A$, would the matrix still be positive definite? I'm told this is particularly true so long as the perturbation satisfies the condition number bound $$\frac{\|\Delta A\|}{\|A\|} < \frac{1}{K(A)},$$ where $K(A)$ is the condition number $K(A)=\|A^{-1}\|\|A\|$ (all in the Frobenius norm). I'm stuck figuring out how to prove this. Since $A$ is positive definite, it has a unique Cholesky decomposition $A=LL^*$ and so $\hat A=LL^*+\Delta A$. But this leads me nowhere in proving that $\hat A$ too has unique Cholesky decomposition dependent on the condition number.",,"['matrices', 'numerical-linear-algebra', 'perturbation-theory']"
65,What is the isomorphism function in $M_m(M_n(\mathbb R))\cong M_{mn}(\mathbb R)$?,What is the isomorphism function in ?,M_m(M_n(\mathbb R))\cong M_{mn}(\mathbb R),"What is the isomorphism function in $M_m(M_n(\mathbb R))\cong M_{mn}(\mathbb R)$. I tried this $[[a_{ij}]_{kl}]\mapsto[a_{ijkl}]$ , but I couldn't prove all steps.","What is the isomorphism function in $M_m(M_n(\mathbb R))\cong M_{mn}(\mathbb R)$. I tried this $[[a_{ij}]_{kl}]\mapsto[a_{ijkl}]$ , but I couldn't prove all steps.",,['matrices']
66,The derivative of a matrix transpose with respect to the original matrix,The derivative of a matrix transpose with respect to the original matrix,,I am just stepping into matrix calculus and I wonder what the following differential is. Thanks. $$ \frac{\partial(A^T)}{\partial A} $$,I am just stepping into matrix calculus and I wonder what the following differential is. Thanks. $$ \frac{\partial(A^T)}{\partial A} $$,,"['matrices', 'derivatives', 'matrix-calculus']"
67,Properties of the Floquet Generator,Properties of the Floquet Generator,,"Consider the differential equation $$ \vec x'(t) = A(t) \vec x(t) , $$ where $A(t)$ is periodic: $A(t+T) = A(t)$. Let $\Phi(t)$ be its fundamental solution, i.e. $\Phi(0) = I$ and $\Phi'(t) = A(t) \cdot \Phi(t)$. The Floquet Theorem tells us that $$ \Phi(t) = Q(t) \cdot \mathrm e^{B t} , $$ where $Q(t+T) = Q(t)$ and $B$ is the time-independent Floquet generator. (To prove the theorem, choose $B$ such that $\mathrm e^{BT} = \Phi(T)$ and define $Q(t) = \Phi(t) \cdot \mathrm e^{-B t}$.) My question is: What is known about the properties of $B$ depending on the properties of $A$? For example, If every $A(t)$ is skew-adjoint, I know that $\Phi(T)$ is unitary and I can choose $B$ to be skew-adjoint as well. If every $A(t)$ is a right (doubly) stochastic matrix, can $B$ always be chosen to be a right (doubly) stochastic matrix? If the spectrum of $A(t)$ is in the left-half plane $\{ z \in \mathbb C: \Re(z) < 0 \}$ for every $t$, does the same hold for $B$? Or similarly, if every $A(t)$ is the generator of a contractive semigroup, is $B$ as well? Are there any general results or techniques?","Consider the differential equation $$ \vec x'(t) = A(t) \vec x(t) , $$ where $A(t)$ is periodic: $A(t+T) = A(t)$. Let $\Phi(t)$ be its fundamental solution, i.e. $\Phi(0) = I$ and $\Phi'(t) = A(t) \cdot \Phi(t)$. The Floquet Theorem tells us that $$ \Phi(t) = Q(t) \cdot \mathrm e^{B t} , $$ where $Q(t+T) = Q(t)$ and $B$ is the time-independent Floquet generator. (To prove the theorem, choose $B$ such that $\mathrm e^{BT} = \Phi(T)$ and define $Q(t) = \Phi(t) \cdot \mathrm e^{-B t}$.) My question is: What is known about the properties of $B$ depending on the properties of $A$? For example, If every $A(t)$ is skew-adjoint, I know that $\Phi(T)$ is unitary and I can choose $B$ to be skew-adjoint as well. If every $A(t)$ is a right (doubly) stochastic matrix, can $B$ always be chosen to be a right (doubly) stochastic matrix? If the spectrum of $A(t)$ is in the left-half plane $\{ z \in \mathbb C: \Re(z) < 0 \}$ for every $t$, does the same hold for $B$? Or similarly, if every $A(t)$ is the generator of a contractive semigroup, is $B$ as well? Are there any general results or techniques?",,"['matrices', 'analysis', 'ordinary-differential-equations', 'periodic-functions']"
68,"Extending representation of $\operatorname{GL}(n,\mathbb{Z})$ to $\operatorname{GL}(n,\mathbb{Q})$",Extending representation of  to,"\operatorname{GL}(n,\mathbb{Z}) \operatorname{GL}(n,\mathbb{Q})","Given a representation $\rho\colon\operatorname{GL}(n;\mathbb{Z}) \rightarrow\operatorname{GL}(V)$ on a finite-dimensional, complex vector space $V$ , I am trying to understand a condition that would let me find a representation $$\tilde \rho\colon\operatorname{GL}(n;\mathbb{Q}) \rightarrow \operatorname{GL}(V)$$ with $\tilde \rho|_{\operatorname{GL}(n;\mathbb{Z})} = \rho.$ This is easy enough when $n=1$ . Any representation $\rho$ can be extended to $\mathbb{Q}^{\times}$ by defining $$\rho(x) := \begin{cases} \rho(1): & x > 0; \\ \rho(-1): & x < 0. \end{cases}$$ In other words, pulling back along the sign homomorphism $$\mathrm{sgn} : \mathbb{Q}^{\times} \rightarrow \mathbb{Z}^{\times}, \; x \mapsto \begin{cases} 1: & x > 0; \\ -1: & x < 0. \end{cases}$$ I can't think of a surjective homomorphism $\operatorname{GL}(n;\mathbb{Q}) \rightarrow\operatorname{GL}(n;\mathbb{Z})$ for $n > 1$ so I can't use this argument anymore.","Given a representation on a finite-dimensional, complex vector space , I am trying to understand a condition that would let me find a representation with This is easy enough when . Any representation can be extended to by defining In other words, pulling back along the sign homomorphism I can't think of a surjective homomorphism for so I can't use this argument anymore.","\rho\colon\operatorname{GL}(n;\mathbb{Z}) \rightarrow\operatorname{GL}(V) V \tilde \rho\colon\operatorname{GL}(n;\mathbb{Q}) \rightarrow \operatorname{GL}(V) \tilde \rho|_{\operatorname{GL}(n;\mathbb{Z})} = \rho. n=1 \rho \mathbb{Q}^{\times} \rho(x) := \begin{cases} \rho(1): & x > 0; \\ \rho(-1): & x < 0. \end{cases} \mathrm{sgn} : \mathbb{Q}^{\times} \rightarrow \mathbb{Z}^{\times}, \; x \mapsto \begin{cases} 1: & x > 0; \\ -1: & x < 0. \end{cases} \operatorname{GL}(n;\mathbb{Q}) \rightarrow\operatorname{GL}(n;\mathbb{Z}) n > 1","['matrices', 'group-theory', 'representation-theory']"
69,How to prove that this matrix is total unimodular,How to prove that this matrix is total unimodular,,"This matrix is total unimodular (tested by a computer program). 1 1 1 1 -1 -1 -1 -1 0 1 1 1  0 -1 -1 -1 0 0 1 1  0  0 -1 -1 0 0 0 1  0  0  0 -1 1 0 0 0 -1  0  0  0 1 1 0 0 -1 -1  0  0 1 1 1 0 -1 -1 -1  0 1 1 1 1 -1 -1 -1 -1 Is there way to prove theoretical that this matrix is total unimodular? I already tried some stuff from the wikipedia article, but the one criteria fails, because it has more than at most two non-zero entries per column .","This matrix is total unimodular (tested by a computer program). 1 1 1 1 -1 -1 -1 -1 0 1 1 1  0 -1 -1 -1 0 0 1 1  0  0 -1 -1 0 0 0 1  0  0  0 -1 1 0 0 0 -1  0  0  0 1 1 0 0 -1 -1  0  0 1 1 1 0 -1 -1 -1  0 1 1 1 1 -1 -1 -1 -1 Is there way to prove theoretical that this matrix is total unimodular? I already tried some stuff from the wikipedia article, but the one criteria fails, because it has more than at most two non-zero entries per column .",,"['matrices', 'determinant', 'integer-programming']"
70,An interesting property of symmetric real matrices with row and column sums zero,An interesting property of symmetric real matrices with row and column sums zero,,"Let $A$ be an $n \times n$ real symmetric matrix with row and column sums zero. For example, $$ A=\begin{bmatrix}1 & -2 & 1\\ -2 & 1 & 1\\ 1 & 1 & -2 \end{bmatrix}. $$ I have the following interesting observation about $A$ in general. Claim: Suppose $\mathrm{rank}(A)=n-1$, and let $v_1, v_2,\dots ,v_{n-1}$ be the $n-1$ normalized eigenvectors (with unit length) corresponding to the $n-1$ nonzero eigenvalues. Let $\mathbf{V}=[v_1,\dots,v_{n-1}]$ be an $n\times(n-1)$ matrix of which each column $i$ is the eigenvector $v_i$. We have $$ I-\mathbf{V}\mathbf{V}^{T}=\frac{1}{n}\begin{bmatrix}1 & \dots & 1\\ \vdots & \vdots & \vdots\\ 1 & \dots & 1 \end{bmatrix}, $$ where $I$ is the identity matrix, and $n$ is the number of columns. As for our particular $A$ in the display, we have $$ \mathbf{V}=\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\ 0 & -\frac{2}{\sqrt{6}} \end{bmatrix}. $$ One can easily verify the above claim for this example. I have randomly generated many such matrices, and the claim holds. So it might be correct. My question is how to prove it. After spending many hours, I have made little progress so far. The only thing meaningful I have found is that any eigenvector of $A$ must sum to be zero, because $0=1^TAv=\lambda 1^Tv$. Here $\{\lambda, v\}$ denotes a generic pair of eigenvalue and eigenvector. An additional observation is that all cofactors of $A$ are identical. But these observations are far from enough to understand this claim. Any thought is welcomed. Thanks.","Let $A$ be an $n \times n$ real symmetric matrix with row and column sums zero. For example, $$ A=\begin{bmatrix}1 & -2 & 1\\ -2 & 1 & 1\\ 1 & 1 & -2 \end{bmatrix}. $$ I have the following interesting observation about $A$ in general. Claim: Suppose $\mathrm{rank}(A)=n-1$, and let $v_1, v_2,\dots ,v_{n-1}$ be the $n-1$ normalized eigenvectors (with unit length) corresponding to the $n-1$ nonzero eigenvalues. Let $\mathbf{V}=[v_1,\dots,v_{n-1}]$ be an $n\times(n-1)$ matrix of which each column $i$ is the eigenvector $v_i$. We have $$ I-\mathbf{V}\mathbf{V}^{T}=\frac{1}{n}\begin{bmatrix}1 & \dots & 1\\ \vdots & \vdots & \vdots\\ 1 & \dots & 1 \end{bmatrix}, $$ where $I$ is the identity matrix, and $n$ is the number of columns. As for our particular $A$ in the display, we have $$ \mathbf{V}=\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\ 0 & -\frac{2}{\sqrt{6}} \end{bmatrix}. $$ One can easily verify the above claim for this example. I have randomly generated many such matrices, and the claim holds. So it might be correct. My question is how to prove it. After spending many hours, I have made little progress so far. The only thing meaningful I have found is that any eigenvector of $A$ must sum to be zero, because $0=1^TAv=\lambda 1^Tv$. Here $\{\lambda, v\}$ denotes a generic pair of eigenvalue and eigenvector. An additional observation is that all cofactors of $A$ are identical. But these observations are far from enough to understand this claim. Any thought is welcomed. Thanks.",,['matrices']
71,Formula for position in an upper triangular matrix,Formula for position in an upper triangular matrix,,"I'm currently working on the Travelling Salesman's Problem in a computer science module. I have implemented some linear programming techniques using the software lp_solve. I've ended up with an upper triangular matrix, that is missing the diagonal, consisting of $1$s and $0$s which indicate whether the route between two specific towns took place. Here's an example of what I'm talking about with 5 towns: $$\begin{bmatrix}\cdot & 0&0&0&1\\\cdot&\cdot & 0&0&1\\\cdot&\cdot&\cdot&0&1\\\cdot&\cdot&\cdot&\cdot&1\\\cdot&\cdot&\cdot&\cdot&\cdot\end{bmatrix}$$ I wanted to derive a formula that maps the position ""single dimensional coordinate"" to its corresponding matrix position vector. An example for clarity, again in a system of 5 towns: $$0\mapsto(0,1) \\ 1\mapsto(0,2) \\ 2\mapsto(0,3) \\ 3\mapsto(0,4) \\ 4\mapsto(1,2) \\ 5\mapsto(1,3) \\ 6\mapsto(1,4)\\ 7\mapsto(2,3)\\ 8\mapsto(2,4)\\ 9\mapsto(3,4)$$ where the ""single-dimensional coordinate"" I mentioned above is the index that gets mapped to the position vector. So my question: how can I derive a formula that defines the map I've expressed above?","I'm currently working on the Travelling Salesman's Problem in a computer science module. I have implemented some linear programming techniques using the software lp_solve. I've ended up with an upper triangular matrix, that is missing the diagonal, consisting of $1$s and $0$s which indicate whether the route between two specific towns took place. Here's an example of what I'm talking about with 5 towns: $$\begin{bmatrix}\cdot & 0&0&0&1\\\cdot&\cdot & 0&0&1\\\cdot&\cdot&\cdot&0&1\\\cdot&\cdot&\cdot&\cdot&1\\\cdot&\cdot&\cdot&\cdot&\cdot\end{bmatrix}$$ I wanted to derive a formula that maps the position ""single dimensional coordinate"" to its corresponding matrix position vector. An example for clarity, again in a system of 5 towns: $$0\mapsto(0,1) \\ 1\mapsto(0,2) \\ 2\mapsto(0,3) \\ 3\mapsto(0,4) \\ 4\mapsto(1,2) \\ 5\mapsto(1,3) \\ 6\mapsto(1,4)\\ 7\mapsto(2,3)\\ 8\mapsto(2,4)\\ 9\mapsto(3,4)$$ where the ""single-dimensional coordinate"" I mentioned above is the index that gets mapped to the position vector. So my question: how can I derive a formula that defines the map I've expressed above?",,"['matrices', 'graph-theory', 'linear-programming']"
72,"If $\mathrm{tr}(A)=0$, then we have $A=BC-CB?$","If , then we have",\mathrm{tr}(A)=0 A=BC-CB?,"For any matrix $A_{n\times n}$ with $\mathrm{tr}(A)=0$ show that there exist two matrices $B$ and $C$ such that $$A=BC-CB.$$ I know to prove this: if $A=BC-CB$, then we have $\mathrm{tr}(A)=0$ because $$\mathrm{tr}(BC)=\mathrm{tr}(CB)$$ so $$\mathrm{tr}(A)=\mathrm{tr}(BC-CB)=\mathrm{tr}(BC)-\mathrm{tr}(CB)=0.$$ But my problem is that I can't prove it.","For any matrix $A_{n\times n}$ with $\mathrm{tr}(A)=0$ show that there exist two matrices $B$ and $C$ such that $$A=BC-CB.$$ I know to prove this: if $A=BC-CB$, then we have $\mathrm{tr}(A)=0$ because $$\mathrm{tr}(BC)=\mathrm{tr}(CB)$$ so $$\mathrm{tr}(A)=\mathrm{tr}(BC-CB)=\mathrm{tr}(BC)-\mathrm{tr}(CB)=0.$$ But my problem is that I can't prove it.",,['matrices']
73,Maximum number of different diagonals obtained by column permutations,Maximum number of different diagonals obtained by column permutations,,"Consider a n x n matrix with entries being only '0' and '1'. For example: $\left( \begin{array}{ccc} 1 & 0 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ We then consider all column permutations of the matrix, and count the different types of diagonals starting from bottom left to upper right: Number 1) $\left( \begin{array}{ccc} 1 & 0 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ Diagonal: 011 Number 2)$\left( \begin{array}{ccc} 1 & 1 & 0\\1 & 0 & 1\\0&1&0\end{array}\right)$ Diagonal: 000 Number 3)$\left( \begin{array}{ccc} 0 & 1 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ Diagonal: 011 Number 4)$\left( \begin{array}{ccc} 0 & 1 & 1\\1 & 0 & 1\\0&1&0\end{array}\right)$Diagonal: 001 Number 5)$\left( \begin{array}{ccc} 1 & 1 & 0\\0 & 1 & 1\\1&0&0\end{array}\right)$ Diagonal: 110 Number 6)$\left( \begin{array}{ccc} 1 & 0 & 1\\0 & 1 & 1\\1&0&0\end{array}\right)$ Diagonal: 111 There are a total number of 5 different possible diagonals: 011, 000, 001, 110, 111. The question is, given any such matrix with entries being '0' and '1', what is the maximum number of possible diagonals? I have tried some conjectures and seems that the formula of maximum diagonals seem to be $2^n-n$. For instance $2^3-3=5$. Thanks for any help in finding the general formula for the maximum number of diagonals, among all n x n matrices (for a fixed n, among all matrices)! The matrix can be any n x n matrix, as long as it yields the maximum number of different diagonals with respect to column permutations.","Consider a n x n matrix with entries being only '0' and '1'. For example: $\left( \begin{array}{ccc} 1 & 0 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ We then consider all column permutations of the matrix, and count the different types of diagonals starting from bottom left to upper right: Number 1) $\left( \begin{array}{ccc} 1 & 0 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ Diagonal: 011 Number 2)$\left( \begin{array}{ccc} 1 & 1 & 0\\1 & 0 & 1\\0&1&0\end{array}\right)$ Diagonal: 000 Number 3)$\left( \begin{array}{ccc} 0 & 1 & 1\\1 & 1 & 0\\0&0&1\end{array}\right)$ Diagonal: 011 Number 4)$\left( \begin{array}{ccc} 0 & 1 & 1\\1 & 0 & 1\\0&1&0\end{array}\right)$Diagonal: 001 Number 5)$\left( \begin{array}{ccc} 1 & 1 & 0\\0 & 1 & 1\\1&0&0\end{array}\right)$ Diagonal: 110 Number 6)$\left( \begin{array}{ccc} 1 & 0 & 1\\0 & 1 & 1\\1&0&0\end{array}\right)$ Diagonal: 111 There are a total number of 5 different possible diagonals: 011, 000, 001, 110, 111. The question is, given any such matrix with entries being '0' and '1', what is the maximum number of possible diagonals? I have tried some conjectures and seems that the formula of maximum diagonals seem to be $2^n-n$. For instance $2^3-3=5$. Thanks for any help in finding the general formula for the maximum number of diagonals, among all n x n matrices (for a fixed n, among all matrices)! The matrix can be any n x n matrix, as long as it yields the maximum number of different diagonals with respect to column permutations.",,['matrices']
74,Solution of a linear matrix differential equation,Solution of a linear matrix differential equation,,"Consider a linear matrix differential equation of the form $$\frac{\mathrm{d} C}{\mathrm{d} t} = A C + C A^{\mathrm{T}}$$ where $C$ is a symmetric $n \times n$ matrix and $A$ is a $n \times n$ matrix. Find $C(t)$ . Is there a formal solution for the above equation? This is in principle linear equation if we treat the matrix $C$ and $A$ as a $n^2$ vector. However, it does not seem to be practical way to solve the problem. This kind of differential equations for matrices is quite new to me. Besides the formal solution let me know some books considering similar topic. Thanks.","Consider a linear matrix differential equation of the form where is a symmetric matrix and is a matrix. Find . Is there a formal solution for the above equation? This is in principle linear equation if we treat the matrix and as a vector. However, it does not seem to be practical way to solve the problem. This kind of differential equations for matrices is quite new to me. Besides the formal solution let me know some books considering similar topic. Thanks.",\frac{\mathrm{d} C}{\mathrm{d} t} = A C + C A^{\mathrm{T}} C n \times n A n \times n C(t) C A n^2,"['matrices', 'ordinary-differential-equations', 'matrix-equations']"
75,How to find the derivative of the adjugate operator?,How to find the derivative of the adjugate operator?,,"How can I find the general derivative of the function $\mathbf{A} \to \mathrm{adj}(\mathbf{A})$ ? Where $\mathbf{A}$ is invertible I got the following: $\mathrm{D}f_{\mathrm{adj}(\mathbf{A})(H)} = \mathrm{tr}(\mathrm{adj}(\mathbf{A}) \cdot H)\cdot \mathrm{adj}(\mathbf{A})^{-1}   -   (\mathrm{adj}(\mathbf{A})) \cdot  H \cdot\mathrm{adj}(\mathbf{A})^{-1}$ Is it right? (And if it is, it's still not enough for every $\mathbf{A}$..)","How can I find the general derivative of the function $\mathbf{A} \to \mathrm{adj}(\mathbf{A})$ ? Where $\mathbf{A}$ is invertible I got the following: $\mathrm{D}f_{\mathrm{adj}(\mathbf{A})(H)} = \mathrm{tr}(\mathrm{adj}(\mathbf{A}) \cdot H)\cdot \mathrm{adj}(\mathbf{A})^{-1}   -   (\mathrm{adj}(\mathbf{A})) \cdot  H \cdot\mathrm{adj}(\mathbf{A})^{-1}$ Is it right? (And if it is, it's still not enough for every $\mathbf{A}$..)",,"['matrices', 'derivatives', 'matrix-calculus']"
76,General question about matrix calculus with specific example (with attempted answer),General question about matrix calculus with specific example (with attempted answer),,"I'm struggling to find the right way to approach matrix calculus problems generally . As an example of a problem that is bothering me, I would like to calculate the derivative of $||Ax||$ (Euclidean vector norm) with respect to the matrix $A$. How can I discover this via first principles? The natural thing seems to be to consider $||(A+H)x||-||Ax||$ as $||H||$ goes to zero but I don't see how to get something tangible from it. Addendum: This question is getting little attention. I am really looking for a general approach for solving these sorts of matrix calculus problems. In particular, finding the derivative with respect to a matrix of certain vector quantities. This comes up all the time in convex optimization algorithms like gradient descent and so on.\ Further: If we look at the derivative of $||Ax||^2$ with respect to $A$ we see that this expression can be written as trace$(Axx^TA^T$), so the derivative with respect to $A$ is $2xx^TU^T$. Edit : I don't know if this is the Frechet derivative per se, but I guess we can just notice that $||Ax||^p=(||Ax||^2)^{\frac{p}{2}}$, so by the power rule we get that the derivative of this is $p\cdot\frac{xx^T U^T}{||Ax||^{p/2 - 1}}$. Is this correct??","I'm struggling to find the right way to approach matrix calculus problems generally . As an example of a problem that is bothering me, I would like to calculate the derivative of $||Ax||$ (Euclidean vector norm) with respect to the matrix $A$. How can I discover this via first principles? The natural thing seems to be to consider $||(A+H)x||-||Ax||$ as $||H||$ goes to zero but I don't see how to get something tangible from it. Addendum: This question is getting little attention. I am really looking for a general approach for solving these sorts of matrix calculus problems. In particular, finding the derivative with respect to a matrix of certain vector quantities. This comes up all the time in convex optimization algorithms like gradient descent and so on.\ Further: If we look at the derivative of $||Ax||^2$ with respect to $A$ we see that this expression can be written as trace$(Axx^TA^T$), so the derivative with respect to $A$ is $2xx^TU^T$. Edit : I don't know if this is the Frechet derivative per se, but I guess we can just notice that $||Ax||^p=(||Ax||^2)^{\frac{p}{2}}$, so by the power rule we get that the derivative of this is $p\cdot\frac{xx^T U^T}{||Ax||^{p/2 - 1}}$. Is this correct??",,['matrices']
77,Convergence of Matrix Series,Convergence of Matrix Series,,"I would just like a quick sanity check. If I have a matrix $ M $, then the series $ 1 + M + M^2 + M^3 \cdots $ converges to $ (1-M)^{-1} $ if the operator norm $ \lVert M \rVert_{\mathrm{op}} < 1$. Is it sufficient to show that each column vector $ v $ of $ M $ has norm $ \lVert v\rVert_{L^2} < 1 $?","I would just like a quick sanity check. If I have a matrix $ M $, then the series $ 1 + M + M^2 + M^3 \cdots $ converges to $ (1-M)^{-1} $ if the operator norm $ \lVert M \rVert_{\mathrm{op}} < 1$. Is it sufficient to show that each column vector $ v $ of $ M $ has norm $ \lVert v\rVert_{L^2} < 1 $?",,"['matrices', 'analysis']"
78,Invariant Inner Product on Lie Algebra,Invariant Inner Product on Lie Algebra,,"Let $G$ be a Lie group, $\frak{g}$ its Lie algebra. Suppose $\mathcal{D}$ a representation of $G$ on $V$, $d$ the associated Lie algebra representation. Suppose $V$ is endowed with an inner product. When (if at all) is the following statement true? The inner product is invariant under $\mathcal{D}$ iff it is invariant under $d$ . I can't find any way of proving it, nor can I think of a counterexample. The question was motivated by the fact that many books prove that the Killing form on a Lie algebra is $ad$-invariant by stating that it's obviously $Ad$-invariant. Any insight would be much appreciated!","Let $G$ be a Lie group, $\frak{g}$ its Lie algebra. Suppose $\mathcal{D}$ a representation of $G$ on $V$, $d$ the associated Lie algebra representation. Suppose $V$ is endowed with an inner product. When (if at all) is the following statement true? The inner product is invariant under $\mathcal{D}$ iff it is invariant under $d$ . I can't find any way of proving it, nor can I think of a counterexample. The question was motivated by the fact that many books prove that the Killing form on a Lie algebra is $ad$-invariant by stating that it's obviously $Ad$-invariant. Any insight would be much appreciated!",,"['matrices', 'differential-geometry', 'representation-theory', 'lie-groups', 'lie-algebras']"
79,Derivative of function including matrix logarithm,Derivative of function including matrix logarithm,,"Is the following equation a first order approximation or incorrect for general matrix Lie groups? And what are the higher order terms? $$\frac{\partial}{\partial\mathbf x} (\log(\mathtt A\cdot\exp(\widehat{\mathbf x})\cdot \mathtt B))^\vee \approx \mathtt{Ad_A}$$ with $\mathtt{A,B}$ are element of a Matrix Lie group $G(n)$ (=$n\times n$-matrices), $\mathbf x$ being a n-vector, $\exp(\cdot)$ being the matrix exponential, $\log(\cdot)$ being the matrix logarithm, $\widehat{\cdot}$ being an operator which maps an $n$-vector to a Lie algebra element (=$n\times n$-matrix), v being the corresponding inverse, and $\mathtt{Ad_A}$ is the adjoint of $\mathtt A$ (in matrix form). Before I start with my approach let me first talk about the definitions and some underlying lemmas in more detail. You might want to skip this and jump directly to my attempt below the line (=======). The operator $\widehat{\cdot}$ maps a vector onto the Lie algebra element $\widehat{\cdot}:\mathbb R^n\rightarrow g, \widehat{\mathbf x} = \sum_i x_i \mathtt G_i$, with $\mathtt G_i$ being the generators of the Lie algebra $g$. The vee operator $(\cdot)^\vee: g\rightarrow \mathbb R^n$ is the corresponding inverse. Example for SO3: $\widehat{\mathbf x} = \begin{bmatrix} 0&-x_3& x_2\\ x_3&0,&-x_1\\-x_2&x_1&0\end{bmatrix}$ $(\mathtt R)^\vee= \begin{bmatrix}R_{3,2}\\R_{1,3}\\R_{2,1}\end{bmatrix}=\frac{1}{2} \begin{bmatrix}R_{3,2}-R_{2,3}\\R_{1,3}-R_{3,1}\\R_{2,1}-R_{1,2}\end{bmatrix} = -\begin{bmatrix}R_{2,3}\\R_{3,1}\\R_{1,2}\end{bmatrix} $ Now let us look at the definition of the adjoint: $Adj_\mathtt A(\widehat{\mathbf x}) := \mathtt A \widehat{\omega}  \mathtt A^{-1}$ with $\mathbf x$ being an $n$-vector and $\mathtt A$ a matrix Lie group element. $Adj_\mathtt A$ can be seen as linear operator. Thus, there exists a $n\times n$ matrix $\mathtt{Ad_A}$ such at that $\mathtt{Ad_A}\cdot \mathbf x = (Adj_\mathtt A(\widehat{\mathbf x}))^\vee.$ Example for SO3: $\mathtt{Ad_R}\mathbf x = \mathtt R \widehat{\mathbf x} \mathtt R^\top$ $\Rightarrow$ $\mathtt{Ad_R=R}$ Since $\mathtt A\exp(\mathtt B)\mathtt A^{-1}=\exp(\mathtt{ABA}^{-1})$ it is true that $\exp(\widehat{\mathtt{Ad_A} \mathbf x}) = \mathtt{A} \exp(\widehat{\mathbf x}) \mathtt{A}^{-1}.$  Thus: $$\exp(\widehat{\mathtt{Ad_A} \mathbf x}) \mathtt A= \mathtt{A} \exp(\widehat{\mathbf x}) \quad (1)$$ Let us look at the formula $\text{BCH}(\mathtt{A,B) := \log(\exp(A),\exp(B))}$. (2) We know, if $\mathtt {A,B}$ commute, $\text{BCH}(\mathtt{A,B}) = \mathtt{A+B}$. Otherwise, it can be approximated with the Baker-Campell-Hausdorff (BCH) formula. The first two terms are: $t_1 = \mathtt{A+B}$ $t_2 = \frac{1}{2}(\mathtt{AB-BA})$ =============================================== My approach: Let $\mathbf c := \log(\mathtt{AB})$ $\frac{\partial}{\partial\mathbf x} (\log(\mathtt A\cdot\exp(\widehat{\mathbf x})\cdot \mathtt B))^\vee = \frac{\partial}{\partial\mathbf x} (\log(\exp(\widehat{\mathtt {Ad_A}\mathbf x})\mathtt A\mathtt B))^\vee$ (using (1)) $= \frac{\partial}{\partial\mathbf x} (\text{BCH}(\widehat{\mathtt {Ad_A}\mathbf x},\mathbf c))^\vee$ (using (2)) Now if $\mathtt {A,B}$ are elements of commutative group (thus, $\widehat{\mathtt{Ad_A}\mathbf x}$ and $\mathbf c$ are elements of a commutative algebra) , we simply get: $= \frac{\partial}{\partial\mathbf x}  \mathtt{Ad_A}\mathbf x + \frac{\partial}{\partial\mathbf x} (\mathbf c)^\vee$  $=   \mathtt{Ad_A} + \mathtt O= \mathtt{Ad_A}$ (Edit: Actually, in this case $\mathtt{Ad_A}=\mathtt I$ always.) However, if $\mathtt {A,B}$ are elements of a general (non-commutative) matrix group, we have to use the BCH formula...","Is the following equation a first order approximation or incorrect for general matrix Lie groups? And what are the higher order terms? $$\frac{\partial}{\partial\mathbf x} (\log(\mathtt A\cdot\exp(\widehat{\mathbf x})\cdot \mathtt B))^\vee \approx \mathtt{Ad_A}$$ with $\mathtt{A,B}$ are element of a Matrix Lie group $G(n)$ (=$n\times n$-matrices), $\mathbf x$ being a n-vector, $\exp(\cdot)$ being the matrix exponential, $\log(\cdot)$ being the matrix logarithm, $\widehat{\cdot}$ being an operator which maps an $n$-vector to a Lie algebra element (=$n\times n$-matrix), v being the corresponding inverse, and $\mathtt{Ad_A}$ is the adjoint of $\mathtt A$ (in matrix form). Before I start with my approach let me first talk about the definitions and some underlying lemmas in more detail. You might want to skip this and jump directly to my attempt below the line (=======). The operator $\widehat{\cdot}$ maps a vector onto the Lie algebra element $\widehat{\cdot}:\mathbb R^n\rightarrow g, \widehat{\mathbf x} = \sum_i x_i \mathtt G_i$, with $\mathtt G_i$ being the generators of the Lie algebra $g$. The vee operator $(\cdot)^\vee: g\rightarrow \mathbb R^n$ is the corresponding inverse. Example for SO3: $\widehat{\mathbf x} = \begin{bmatrix} 0&-x_3& x_2\\ x_3&0,&-x_1\\-x_2&x_1&0\end{bmatrix}$ $(\mathtt R)^\vee= \begin{bmatrix}R_{3,2}\\R_{1,3}\\R_{2,1}\end{bmatrix}=\frac{1}{2} \begin{bmatrix}R_{3,2}-R_{2,3}\\R_{1,3}-R_{3,1}\\R_{2,1}-R_{1,2}\end{bmatrix} = -\begin{bmatrix}R_{2,3}\\R_{3,1}\\R_{1,2}\end{bmatrix} $ Now let us look at the definition of the adjoint: $Adj_\mathtt A(\widehat{\mathbf x}) := \mathtt A \widehat{\omega}  \mathtt A^{-1}$ with $\mathbf x$ being an $n$-vector and $\mathtt A$ a matrix Lie group element. $Adj_\mathtt A$ can be seen as linear operator. Thus, there exists a $n\times n$ matrix $\mathtt{Ad_A}$ such at that $\mathtt{Ad_A}\cdot \mathbf x = (Adj_\mathtt A(\widehat{\mathbf x}))^\vee.$ Example for SO3: $\mathtt{Ad_R}\mathbf x = \mathtt R \widehat{\mathbf x} \mathtt R^\top$ $\Rightarrow$ $\mathtt{Ad_R=R}$ Since $\mathtt A\exp(\mathtt B)\mathtt A^{-1}=\exp(\mathtt{ABA}^{-1})$ it is true that $\exp(\widehat{\mathtt{Ad_A} \mathbf x}) = \mathtt{A} \exp(\widehat{\mathbf x}) \mathtt{A}^{-1}.$  Thus: $$\exp(\widehat{\mathtt{Ad_A} \mathbf x}) \mathtt A= \mathtt{A} \exp(\widehat{\mathbf x}) \quad (1)$$ Let us look at the formula $\text{BCH}(\mathtt{A,B) := \log(\exp(A),\exp(B))}$. (2) We know, if $\mathtt {A,B}$ commute, $\text{BCH}(\mathtt{A,B}) = \mathtt{A+B}$. Otherwise, it can be approximated with the Baker-Campell-Hausdorff (BCH) formula. The first two terms are: $t_1 = \mathtt{A+B}$ $t_2 = \frac{1}{2}(\mathtt{AB-BA})$ =============================================== My approach: Let $\mathbf c := \log(\mathtt{AB})$ $\frac{\partial}{\partial\mathbf x} (\log(\mathtt A\cdot\exp(\widehat{\mathbf x})\cdot \mathtt B))^\vee = \frac{\partial}{\partial\mathbf x} (\log(\exp(\widehat{\mathtt {Ad_A}\mathbf x})\mathtt A\mathtt B))^\vee$ (using (1)) $= \frac{\partial}{\partial\mathbf x} (\text{BCH}(\widehat{\mathtt {Ad_A}\mathbf x},\mathbf c))^\vee$ (using (2)) Now if $\mathtt {A,B}$ are elements of commutative group (thus, $\widehat{\mathtt{Ad_A}\mathbf x}$ and $\mathbf c$ are elements of a commutative algebra) , we simply get: $= \frac{\partial}{\partial\mathbf x}  \mathtt{Ad_A}\mathbf x + \frac{\partial}{\partial\mathbf x} (\mathbf c)^\vee$  $=   \mathtt{Ad_A} + \mathtt O= \mathtt{Ad_A}$ (Edit: Actually, in this case $\mathtt{Ad_A}=\mathtt I$ always.) However, if $\mathtt {A,B}$ are elements of a general (non-commutative) matrix group, we have to use the BCH formula...",,"['matrices', 'optimization', 'lie-groups', 'lie-algebras']"
80,Efficient algorithm for computing Vandermonde determinant,Efficient algorithm for computing Vandermonde determinant,,The determinant of Vandermonde matrix $$V=\left[\begin{matrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{n-1} \\ 1 & x_3 & x_3^2 & \cdots & x_3^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \cdots & x_n^{n-1} \\ \end{matrix}\right]$$ can be represented as a product of pairwise differences between $x_i$ s: $$\det(V)=\prod_{i<j}x_j-x_i.$$ Is there an algorithm that computes this determinant more quickly than by trivially multiplying those $O(n^2)$ differences?,The determinant of Vandermonde matrix can be represented as a product of pairwise differences between s: Is there an algorithm that computes this determinant more quickly than by trivially multiplying those differences?,"V=\left[\begin{matrix}
1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
1 & x_2 & x_2^2 & \cdots & x_2^{n-1} \\
1 & x_3 & x_3^2 & \cdots & x_3^{n-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^{n-1} \\
\end{matrix}\right] x_i \det(V)=\prod_{i<j}x_j-x_i. O(n^2)","['matrices', 'algorithms', 'determinant', 'numerical-linear-algebra']"
81,"Square root of the determinant of AB+I where A, B are skew-symmetric","Square root of the determinant of AB+I where A, B are skew-symmetric",,"Imagine I have two skew-symmetric square matrices $A$, $B$. (So $A^\intercal = -A$, etc.) Now I am interested in the square root of the determinant of $AB+I$, where $I$ is the identity matrix, $$ x = \sqrt{ \det \left( AB + I \right) } $$ As quick inspection for small matrices suggests that this $x$ is a polynomial of the elements of $A$ and $B$, for example for $3 \times 3$ matrices we find $$ x =  1 - a_{12} b_{12} - a_{13} b_{13} - a_{23} b_{23} $$ and I checked this analytically for matrices up to $6 \times 6$. It reminds me of the pfaffian of a skew-symmetric matrix, which is also a 'square root of a determinant' but nonetheless a polynomial in the matrix elements. Now my questions are: Does anyone know a proof that $x$ is a polynomial in the elements of $A$ and $B$, and if so, what is that polynomial? Does anyone know an efficient (so not $O(n!)$) algorithm to compute $x$?","Imagine I have two skew-symmetric square matrices $A$, $B$. (So $A^\intercal = -A$, etc.) Now I am interested in the square root of the determinant of $AB+I$, where $I$ is the identity matrix, $$ x = \sqrt{ \det \left( AB + I \right) } $$ As quick inspection for small matrices suggests that this $x$ is a polynomial of the elements of $A$ and $B$, for example for $3 \times 3$ matrices we find $$ x =  1 - a_{12} b_{12} - a_{13} b_{13} - a_{23} b_{23} $$ and I checked this analytically for matrices up to $6 \times 6$. It reminds me of the pfaffian of a skew-symmetric matrix, which is also a 'square root of a determinant' but nonetheless a polynomial in the matrix elements. Now my questions are: Does anyone know a proof that $x$ is a polynomial in the elements of $A$ and $B$, and if so, what is that polynomial? Does anyone know an efficient (so not $O(n!)$) algorithm to compute $x$?",,"['matrices', 'determinant', 'radicals']"
82,Hermitian Matrices with Repeated Eigenvalues has Codimension 3?,Hermitian Matrices with Repeated Eigenvalues has Codimension 3?,,"It is sometimes claimed that the space of $n\times n$ Hermitian matrices with at least one repeated eigenvalue has codimension 3. (See link exercise 10.) The proof of this in dimension two is very simple: by diagonalizing a $2\times 2$ Hermitian matrix $H$, we easily conclude that if $H$ has the same eigenvalue twice, then $H$ must be a constant multiple of the $2\times 2$ identity matrix, and this space has dimension 1, versus 4 for the space of $2\times 2$ Hermitian matrices. However, I'm having difficulties with (what I believe to be) the standard demonstration of this in higher dimensions (provided in the paper ""On the Behaviour of Eigenvalues in Adiabatic Processes"" by Von Neumann and Wigner). From what I can tell, the demonstration (outline) goes goes as follows: An $n\times n$ Hermitian matrix $H$ can be written as $UDU^*$, where $U$ is unitary and $D$ is diagonal and contains the eigenvalues, say in increasing order. The decomposition $UDU^*$ is not unique, but if we have $WDW^*$ for another unitary matrix $W$, then we can write $W=UV$, where $V$ commutes with $D$, and so one could argue we can fix $U$ and $D$, and then write $H=(UV)D(UV)^*$, where $V$ can be any unitary matrix that commutes with $D$. Thus, the number of real parameters one needs to specify the matrix $H$ would be $n^2+f-v$, where $n^2$ is the number of real parameters to specify a fixed unitary matrix $U$, $f$ is the number of parameters to specify the real eigenvalues of $D$, and $v$ is the number of parameters to specify $V$. We can show that, if $f$ is the number of eigenvalues of $H$, and $g_1,\ldots,g_f$ are the multiplicities of the eigenvalues (i.e., $g_1+\cdots+g_f=n$), then $v=g_1^2+g_2^2+\cdots+g_f^2$. Therefore, if there is a repeated eigenvalue ($g_i\geq 2$ for at least one $i$), the number of parameters will be at most $n^2-3$, hence the space of Hermitian matrices with repeated eigenvalues has codimension 3. My problems with this are the following: In the case of $2\times 2$ matrices, it is clear that the space of Hermitian matrices with repeated eigenvalues is a vector space, as it consists of the multiples of the identity matrix. However, it is not at all clear to me that this is the case in higher dimensions. In step 3. above, I don't understand how one just removes the parameters used to specify $V$ from the total amount of parameters. If it were argued that we have double counted some parameters and that we must now remove them, this would be fine, but it seems that we start with $n^2+f$ parameters and that we then somehow remove parameters that are completely unrelated to the previous ones. Finally, I'm not sure I understand the connection between parameters and dimension.  While it is true that Unitary matrices can be specified with $n^2$ numbers, we can't say that the space of Unitary matrices has dimension $n^2$ because unitary matrices do not form a linear subspace. So, I'm not sure how this business of counting parameters rigorously coincides with dimension of subspaces.","It is sometimes claimed that the space of $n\times n$ Hermitian matrices with at least one repeated eigenvalue has codimension 3. (See link exercise 10.) The proof of this in dimension two is very simple: by diagonalizing a $2\times 2$ Hermitian matrix $H$, we easily conclude that if $H$ has the same eigenvalue twice, then $H$ must be a constant multiple of the $2\times 2$ identity matrix, and this space has dimension 1, versus 4 for the space of $2\times 2$ Hermitian matrices. However, I'm having difficulties with (what I believe to be) the standard demonstration of this in higher dimensions (provided in the paper ""On the Behaviour of Eigenvalues in Adiabatic Processes"" by Von Neumann and Wigner). From what I can tell, the demonstration (outline) goes goes as follows: An $n\times n$ Hermitian matrix $H$ can be written as $UDU^*$, where $U$ is unitary and $D$ is diagonal and contains the eigenvalues, say in increasing order. The decomposition $UDU^*$ is not unique, but if we have $WDW^*$ for another unitary matrix $W$, then we can write $W=UV$, where $V$ commutes with $D$, and so one could argue we can fix $U$ and $D$, and then write $H=(UV)D(UV)^*$, where $V$ can be any unitary matrix that commutes with $D$. Thus, the number of real parameters one needs to specify the matrix $H$ would be $n^2+f-v$, where $n^2$ is the number of real parameters to specify a fixed unitary matrix $U$, $f$ is the number of parameters to specify the real eigenvalues of $D$, and $v$ is the number of parameters to specify $V$. We can show that, if $f$ is the number of eigenvalues of $H$, and $g_1,\ldots,g_f$ are the multiplicities of the eigenvalues (i.e., $g_1+\cdots+g_f=n$), then $v=g_1^2+g_2^2+\cdots+g_f^2$. Therefore, if there is a repeated eigenvalue ($g_i\geq 2$ for at least one $i$), the number of parameters will be at most $n^2-3$, hence the space of Hermitian matrices with repeated eigenvalues has codimension 3. My problems with this are the following: In the case of $2\times 2$ matrices, it is clear that the space of Hermitian matrices with repeated eigenvalues is a vector space, as it consists of the multiples of the identity matrix. However, it is not at all clear to me that this is the case in higher dimensions. In step 3. above, I don't understand how one just removes the parameters used to specify $V$ from the total amount of parameters. If it were argued that we have double counted some parameters and that we must now remove them, this would be fine, but it seems that we start with $n^2+f$ parameters and that we then somehow remove parameters that are completely unrelated to the previous ones. Finally, I'm not sure I understand the connection between parameters and dimension.  While it is true that Unitary matrices can be specified with $n^2$ numbers, we can't say that the space of Unitary matrices has dimension $n^2$ because unitary matrices do not form a linear subspace. So, I'm not sure how this business of counting parameters rigorously coincides with dimension of subspaces.",,['matrices']
83,Is S a group under matrix addition,Is S a group under matrix addition,,"Another matrix question! Let $$S=\{A \in M_2(\mathbb{R}):f(A)=0\}\text{ and }f\left(\begin{bmatrix}a&b\\c&d \end{bmatrix}\right)=b$$ Is S a group under matrix addition. Either prove that (S,+) is a group or show that one of the group axioms fails. Group axioms: (G1) the set G is closed under the operation $*$ (G2) The operation $*$ is associative - $(a*b)*c=a*(b*c)$ (G3) There exists an element $e \in G$ such that $a*e=a=e*a$ for all $a \in G$ (G4) for each $a \in G$ there exists an element $a^{-1}$ of G such that $a*a^{-1}=e=a^{-1}*a$ Here's what I've got so far: (G1) let $A=\begin{bmatrix}a&0\\c&d\end{bmatrix}$ and $B=\begin{bmatrix}e&0\\g&h \end{bmatrix}$ where $A, B \in S$ then $A+B=\begin{bmatrix}a+e&0\\c+g&d+h \end{bmatrix}$ which is $\in S$ so it is closed. - (G1) is satisfied (G2) using $A$ and $B$ as above and $C=\begin{bmatrix}j&0\\m&n \end{bmatrix},\\ (A+B)+C = \begin{bmatrix}a+e&0\\c+g&d+h \end{bmatrix}+\begin{bmatrix}j&0\\m&n \end{bmatrix} = \begin{bmatrix}a+e+j&0\\c+g+m&d+h+n\end{bmatrix}$ and $A+(B+C)=\begin{bmatrix}a&0\\c&d\end{bmatrix}+\begin{bmatrix}e+j&0\\g+m&h+n\end{bmatrix}=\begin{bmatrix}a+e+j&0\\c+g+m&d+h+n\end{bmatrix}$ so $(A+B)+C=A+(B+C)$ - (G2) is satisfied (G3) let $e=\begin{bmatrix}0&0\\0&0\end{bmatrix}$ pretty easy to see that $A+e=A=e+A$ - (G3) satisfied (G4) let $A^{-1}=\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}$ $A+A^{-1}=\begin{bmatrix}a&0\\c&d\end{bmatrix}+\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}=e$ and  $A^{-1}+A=\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}+\begin{bmatrix}a&0\\c&d\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}=e$ (G4) satisfied So I have proved all the axioms (hopefully correctly?!) therefore we can say that (S,+) is a group. I know that's a bit of reading but I would really appreciate your confirmation that this is right / where I went wrong if it isn't. The one I'm really worried about is G1, and I know then that if this is wrong and (S,+) is not closed then I don't need to worry about the rest. And also if there is another way to do it?","Another matrix question! Let $$S=\{A \in M_2(\mathbb{R}):f(A)=0\}\text{ and }f\left(\begin{bmatrix}a&b\\c&d \end{bmatrix}\right)=b$$ Is S a group under matrix addition. Either prove that (S,+) is a group or show that one of the group axioms fails. Group axioms: (G1) the set G is closed under the operation $*$ (G2) The operation $*$ is associative - $(a*b)*c=a*(b*c)$ (G3) There exists an element $e \in G$ such that $a*e=a=e*a$ for all $a \in G$ (G4) for each $a \in G$ there exists an element $a^{-1}$ of G such that $a*a^{-1}=e=a^{-1}*a$ Here's what I've got so far: (G1) let $A=\begin{bmatrix}a&0\\c&d\end{bmatrix}$ and $B=\begin{bmatrix}e&0\\g&h \end{bmatrix}$ where $A, B \in S$ then $A+B=\begin{bmatrix}a+e&0\\c+g&d+h \end{bmatrix}$ which is $\in S$ so it is closed. - (G1) is satisfied (G2) using $A$ and $B$ as above and $C=\begin{bmatrix}j&0\\m&n \end{bmatrix},\\ (A+B)+C = \begin{bmatrix}a+e&0\\c+g&d+h \end{bmatrix}+\begin{bmatrix}j&0\\m&n \end{bmatrix} = \begin{bmatrix}a+e+j&0\\c+g+m&d+h+n\end{bmatrix}$ and $A+(B+C)=\begin{bmatrix}a&0\\c&d\end{bmatrix}+\begin{bmatrix}e+j&0\\g+m&h+n\end{bmatrix}=\begin{bmatrix}a+e+j&0\\c+g+m&d+h+n\end{bmatrix}$ so $(A+B)+C=A+(B+C)$ - (G2) is satisfied (G3) let $e=\begin{bmatrix}0&0\\0&0\end{bmatrix}$ pretty easy to see that $A+e=A=e+A$ - (G3) satisfied (G4) let $A^{-1}=\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}$ $A+A^{-1}=\begin{bmatrix}a&0\\c&d\end{bmatrix}+\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}=e$ and  $A^{-1}+A=\begin{bmatrix}-a&0\\-c&-d\end{bmatrix}+\begin{bmatrix}a&0\\c&d\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}=e$ (G4) satisfied So I have proved all the axioms (hopefully correctly?!) therefore we can say that (S,+) is a group. I know that's a bit of reading but I would really appreciate your confirmation that this is right / where I went wrong if it isn't. The one I'm really worried about is G1, and I know then that if this is wrong and (S,+) is not closed then I don't need to worry about the rest. And also if there is another way to do it?",,"['group-theory', 'matrices']"
84,Natural matrix norm of an inverse matrix,Natural matrix norm of an inverse matrix,,"Let $\left\|\cdot\right\| : \text{GL}(n,\mathbb{R})\to\mathbb{R}_{\ge 0}$ denote the natural matrix norm, i.e. $$\left\|A\right\|:=\max_{x\ne 0}\frac{\left\|Ax\right\|}{\left\|x\right\|}=\max_{\left\|x\right\|=1}\left\|Ax\right\|$$ is induced by a vector norm $\left\|\cdot\right\| : \mathbb{R}^n\to\mathbb{R}_{\ge 0}$. I want to show, that it holds $$\left\|A^{-1}\right\|=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1}$$ Proof :  \begin{equation} \begin{split} \left\|A^{-1}\right\|&=\max_{\left\|x\right\|=1}\left\|A^{-1}x\right\|\\ &=\max_{\left\|Ay\right\|=1}\left\|y\right\|\\ &=\left(\min_{\left\|Ay\right\|=1}\left\|y\right\|^{-1}\right)^{-1}\\ &=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1} \end{split} \end{equation} How does the last step work?","Let $\left\|\cdot\right\| : \text{GL}(n,\mathbb{R})\to\mathbb{R}_{\ge 0}$ denote the natural matrix norm, i.e. $$\left\|A\right\|:=\max_{x\ne 0}\frac{\left\|Ax\right\|}{\left\|x\right\|}=\max_{\left\|x\right\|=1}\left\|Ax\right\|$$ is induced by a vector norm $\left\|\cdot\right\| : \mathbb{R}^n\to\mathbb{R}_{\ge 0}$. I want to show, that it holds $$\left\|A^{-1}\right\|=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1}$$ Proof :  \begin{equation} \begin{split} \left\|A^{-1}\right\|&=\max_{\left\|x\right\|=1}\left\|A^{-1}x\right\|\\ &=\max_{\left\|Ay\right\|=1}\left\|y\right\|\\ &=\left(\min_{\left\|Ay\right\|=1}\left\|y\right\|^{-1}\right)^{-1}\\ &=\left(\min_{\left\|x\right\|=1}\left\|Ax\right\|\right)^{-1} \end{split} \end{equation} How does the last step work?",,"['matrices', 'normed-spaces', 'inverse', 'matrix-norms']"
85,How to understand Gimbal Lock? [duplicate],How to understand Gimbal Lock? [duplicate],,"This question already has answers here : Euler angles and gimbal lock (5 answers) Closed 2 years ago . I have read so many articles on the web about Gimbal Lock and also many Q&A on Stackexchange/Stackoverflow, but nevertheless I cannot fully understand why Gimbal Lock should occur and why it should be a problem. Let our Euler Angles formalism the following: at the beginning the object reference frame and the world reference frame are aligned (right-hand rule) the first rotation occurs about the (world or object) z-axys (yaw) the second rotation occurs about the object y-axis which is different from the world's one (pitch) the third rotation occurs about the object x-axis which is different from the world's one (roll) Note that I am using what Wikipedia calls intrinsic formalism (ZYX or z,y',x'') to highlight that the pitch and roll rotations occur about new axis. From several animations with physical gimbals I understood that when the yaw-gimbal and the roll-gimbal are aligned (and it happens when the object pitches of $\pm\frac\pi2$ ) then the rotation about those gimbals are the same. Note that in this case we are facing with a different formalism because for example when the object pitches then the yaw gimbal doesn't follow the object orientation (see for example this top cited video at 0:47). Anyway mathematically speaking I cannot understand why gimbal lock should occur. I try to explain what i mean in the following. Denoting with $$R_X=\begin{bmatrix}1&0&0\\ 0&\cos(\varphi)&-\sin(\varphi)\\ 0&\sin(\varphi)&\cos(\varphi)\end{bmatrix}$$ $$R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\ 0&1&0\\ -\sin(\theta)&0&\cos(\theta)\end{bmatrix}$$ $$R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\ \sin(\psi)&\cos(\psi)&0\\ 0&0&1\end{bmatrix}$$ the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles $(yaw,pitch,roll)=(\psi,\theta,\varphi)$ is given by: $$R=R_Z\cdot R_Y\cdot R_X$$ (note the order of rotations ZYX because I am working in the object reference frame see this ). Suppose now that i perform the following elementary rotations: angle $\psi$ about z-axis (yaw) angle $\theta = -\frac\pi2$ about y-axis (pitch which should cause gimbal lock) angle $\varphi$ about x-axis (yaw) Composing these rotation using the formula above I obtain: $$R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\ 0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\ 1&0&0\end{bmatrix}$$ and in different articles they say that the fact that the only angle that appear in that expression is $(\varphi+\psi)$ means that yaw and roll are linked and so we lose a degree of freedom. But it sounds strange to me. The only thing I can deduce from this result is that there are infinite sequences yaw-pitch-roll (or better yaw- $\left(-\frac\pi2\right)$ -roll) which lead to the same orientation. Also I could note that given this orientation I cannot uniquely determine which sequence leads to it, but I cannot see any other problem. Some articles say that when I am in a gimbal lock situation, to reach some position near the object I need to make a non-intuitive path (and this could be a problem in rendering applications, I have seen some animations in which the object overturns after being in a gimbal lock situations and before to reach another orientation near the gimbal lock one) but I don't know why it should happen). Let be in this situation: $$R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\ 0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\ 1&0&0\end{bmatrix}$$ and suppose $\varphi+\psi = 0$ . I obtain $$R_0=\begin{bmatrix}0&0&-1\\ 0&1&0\\ 1&0&0\end{bmatrix}$$ Perturbing $\varphi$ or $\psi$ by a small $\varepsilon>0$ , we have $$R_{\varepsilon}=\begin{bmatrix}0&-\sin(\varepsilon)&-\cos(\varepsilon)\\ 0&\cos(\varepsilon)&-\sin(\varepsilon)\\ 1&0&0\end{bmatrix}$$ which could be approximated by $$R_{\varepsilon}=\begin{bmatrix}0&-\varepsilon&\varepsilon^2-1\\ 0&1-\varepsilon^2&-\varepsilon\\ 1&0&0\end{bmatrix}$$ that seems to me a trasformation ""near"" to $R_0$ . So please help me to understand what I am missing.","This question already has answers here : Euler angles and gimbal lock (5 answers) Closed 2 years ago . I have read so many articles on the web about Gimbal Lock and also many Q&A on Stackexchange/Stackoverflow, but nevertheless I cannot fully understand why Gimbal Lock should occur and why it should be a problem. Let our Euler Angles formalism the following: at the beginning the object reference frame and the world reference frame are aligned (right-hand rule) the first rotation occurs about the (world or object) z-axys (yaw) the second rotation occurs about the object y-axis which is different from the world's one (pitch) the third rotation occurs about the object x-axis which is different from the world's one (roll) Note that I am using what Wikipedia calls intrinsic formalism (ZYX or z,y',x'') to highlight that the pitch and roll rotations occur about new axis. From several animations with physical gimbals I understood that when the yaw-gimbal and the roll-gimbal are aligned (and it happens when the object pitches of ) then the rotation about those gimbals are the same. Note that in this case we are facing with a different formalism because for example when the object pitches then the yaw gimbal doesn't follow the object orientation (see for example this top cited video at 0:47). Anyway mathematically speaking I cannot understand why gimbal lock should occur. I try to explain what i mean in the following. Denoting with the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles is given by: (note the order of rotations ZYX because I am working in the object reference frame see this ). Suppose now that i perform the following elementary rotations: angle about z-axis (yaw) angle about y-axis (pitch which should cause gimbal lock) angle about x-axis (yaw) Composing these rotation using the formula above I obtain: and in different articles they say that the fact that the only angle that appear in that expression is means that yaw and roll are linked and so we lose a degree of freedom. But it sounds strange to me. The only thing I can deduce from this result is that there are infinite sequences yaw-pitch-roll (or better yaw- -roll) which lead to the same orientation. Also I could note that given this orientation I cannot uniquely determine which sequence leads to it, but I cannot see any other problem. Some articles say that when I am in a gimbal lock situation, to reach some position near the object I need to make a non-intuitive path (and this could be a problem in rendering applications, I have seen some animations in which the object overturns after being in a gimbal lock situations and before to reach another orientation near the gimbal lock one) but I don't know why it should happen). Let be in this situation: and suppose . I obtain Perturbing or by a small , we have which could be approximated by that seems to me a trasformation ""near"" to . So please help me to understand what I am missing.","\pm\frac\pi2 R_X=\begin{bmatrix}1&0&0\\
0&\cos(\varphi)&-\sin(\varphi)\\
0&\sin(\varphi)&\cos(\varphi)\end{bmatrix} R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\
0&1&0\\
-\sin(\theta)&0&\cos(\theta)\end{bmatrix} R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\
\sin(\psi)&\cos(\psi)&0\\
0&0&1\end{bmatrix} (yaw,pitch,roll)=(\psi,\theta,\varphi) R=R_Z\cdot R_Y\cdot R_X \psi \theta = -\frac\pi2 \varphi R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\
0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\
1&0&0\end{bmatrix} (\varphi+\psi) \left(-\frac\pi2\right) R=\begin{bmatrix}0&-\sin(\varphi+\psi)&-\cos(\varphi+\psi)\\
0&\cos(\varphi+\psi)&-\sin(\varphi+\psi)\\
1&0&0\end{bmatrix} \varphi+\psi = 0 R_0=\begin{bmatrix}0&0&-1\\
0&1&0\\
1&0&0\end{bmatrix} \varphi \psi \varepsilon>0 R_{\varepsilon}=\begin{bmatrix}0&-\sin(\varepsilon)&-\cos(\varepsilon)\\
0&\cos(\varepsilon)&-\sin(\varepsilon)\\
1&0&0\end{bmatrix} R_{\varepsilon}=\begin{bmatrix}0&-\varepsilon&\varepsilon^2-1\\
0&1-\varepsilon^2&-\varepsilon\\
1&0&0\end{bmatrix} R_0","['matrices', 'rotations', 'angle']"
86,Deforming an approximate algebra into an exact algebra,Deforming an approximate algebra into an exact algebra,,"Consider a linear subspace of matrices, $M \subset \textrm{Mat}_n(\mathbb{C})$ , which is $\epsilon$ - approximately closed under multiplication, i.e. for all $x,y \in M$ , there exists $z \in M$ such that $\lVert{xy-z}\rVert < \epsilon \lVert x \rVert \lVert y \rVert$ . Assume $M$ is closed under Hermitian adjoints.  Then you might say $M$ forms an "" $\epsilon$ -approximate"" $*$ -subalgebra of $\textrm{Mat}_n(\mathbb{C})$ .  (We might also assume $M \ni 1$ and $M$ is approximately closed under inverses.) Have approximate $*$ -subalgebras of this sort been studied? I am familiar with some notions of Ulam stability, approximate homomorphisms, and approximate representations, but the setup here appears slightly different. ( $M$ is not given as the image of some approximate homomorphism from an actual algebra.) In particular, I'm interested in the following: Is it known whether $M$ may be slightly deformed so that it becomes an exact $*$ -subalgebra?  E.g., does there exist a subspace $N \subset \textrm{Mat}_n(\mathbb{C})$ which is an exact $*$ -algebra, and which is nearby to the subspace $M$ , with distance controlled by $\epsilon$ ? If anyone's interested, I'll offer an example of a conjecture along these lines. For a von Neumann algebra $\mathcal{A}$ , call a linear subspace $M \subset \mathcal{A}$ $\epsilon$ -approximately multiplicatively closed if it is closed under $*$ , contains the identity, and satisfies the property that for all $x,y \in M$ , $\exists z \in M$ s.t. $\lVert{xy-z}\rVert < \epsilon \lVert x \rVert \lVert y \rVert$ . Also define a distance between two subspaces: for any linear subspaces $X,Y$ , let $d(X,Y)$ be the Hausdorff distance between their unit balls. Then one might conjecture: For every $\epsilon>0$ , there exists $\delta>0$ such that for any finite-dimensional von Neumann algebra $\mathcal{A}$ , for any $\delta$ -approximately multiplicatively closed subspace $M \subset \mathcal{A}$ , there exists a von Neumann subalgebra $\mathcal{N} \subset \mathcal{A}$ such that $d(M,\mathcal{N})<\epsilon$ . The related literature I know seems to focus on maps between algebras which are approximately multiplicative, e.g. "" Approximately Multiplicative Maps Between Banach Algebras "" (Johnson, 1986).  Here, I'm wondering about linear subspaces that are approximately multiplicatively closed.  (So these are like ""approximate sub-algebras,"" rather than ""approximate homomorphisms."")","Consider a linear subspace of matrices, , which is - approximately closed under multiplication, i.e. for all , there exists such that . Assume is closed under Hermitian adjoints.  Then you might say forms an "" -approximate"" -subalgebra of .  (We might also assume and is approximately closed under inverses.) Have approximate -subalgebras of this sort been studied? I am familiar with some notions of Ulam stability, approximate homomorphisms, and approximate representations, but the setup here appears slightly different. ( is not given as the image of some approximate homomorphism from an actual algebra.) In particular, I'm interested in the following: Is it known whether may be slightly deformed so that it becomes an exact -subalgebra?  E.g., does there exist a subspace which is an exact -algebra, and which is nearby to the subspace , with distance controlled by ? If anyone's interested, I'll offer an example of a conjecture along these lines. For a von Neumann algebra , call a linear subspace -approximately multiplicatively closed if it is closed under , contains the identity, and satisfies the property that for all , s.t. . Also define a distance between two subspaces: for any linear subspaces , let be the Hausdorff distance between their unit balls. Then one might conjecture: For every , there exists such that for any finite-dimensional von Neumann algebra , for any -approximately multiplicatively closed subspace , there exists a von Neumann subalgebra such that . The related literature I know seems to focus on maps between algebras which are approximately multiplicative, e.g. "" Approximately Multiplicative Maps Between Banach Algebras "" (Johnson, 1986).  Here, I'm wondering about linear subspaces that are approximately multiplicatively closed.  (So these are like ""approximate sub-algebras,"" rather than ""approximate homomorphisms."")","M \subset \textrm{Mat}_n(\mathbb{C}) \epsilon x,y \in M z \in M \lVert{xy-z}\rVert < \epsilon \lVert x \rVert \lVert y \rVert M M \epsilon * \textrm{Mat}_n(\mathbb{C}) M \ni 1 M * M M * N \subset \textrm{Mat}_n(\mathbb{C}) * M \epsilon \mathcal{A} M \subset \mathcal{A} \epsilon * x,y \in M \exists z \in M \lVert{xy-z}\rVert < \epsilon \lVert x \rVert \lVert y \rVert X,Y d(X,Y) \epsilon>0 \delta>0 \mathcal{A} \delta M \subset \mathcal{A} \mathcal{N} \subset \mathcal{A} d(M,\mathcal{N})<\epsilon","['matrices', 'operator-algebras', 'von-neumann-algebras']"
87,Convex hull of rank-$1$ matrices is the nuclear norm unit ball,Convex hull of rank- matrices is the nuclear norm unit ball,1,"Let $$A := \left\{ u v^T : u \in \mathbb{R}^m, v \in \mathbb{R}^n, \|u\|_2 = \|v\|_2 = 1 \right\}$$ I would like to show that $$\textrm{conv}(A) = B_* := \left\{ X \in \mathbb{R}^{m \times n}: \|X\|_* = \textrm{Tr} \left( \sqrt{XX^T} \right) \le 1 \right\}$$ I have shown that $\textrm{conv}(A) \subset B_*$ . However, I have been stuck proving the opposite inclusion $ B_* \subset \textrm{conv}(A)$ . Does anyone have any tips on how to show this inclusion? One idea that I have is to take the SVD of an arbitrary matrix $X \in B_*$ such that $X=U\Sigma V^T$ where the sum of the elements of the diagonal matrix $\Sigma \le 1$ . We also know that $U,V^T$ are orthonormal square matrices. However, I'm not sure where to go from there.","Let I would like to show that I have shown that . However, I have been stuck proving the opposite inclusion . Does anyone have any tips on how to show this inclusion? One idea that I have is to take the SVD of an arbitrary matrix such that where the sum of the elements of the diagonal matrix . We also know that are orthonormal square matrices. However, I'm not sure where to go from there.","A := \left\{ u v^T : u \in \mathbb{R}^m, v \in \mathbb{R}^n, \|u\|_2 = \|v\|_2 = 1 \right\} \textrm{conv}(A) = B_* := \left\{ X \in \mathbb{R}^{m \times n}: \|X\|_* = \textrm{Tr} \left( \sqrt{XX^T} \right) \le 1 \right\} \textrm{conv}(A) \subset B_*  B_* \subset \textrm{conv}(A) X \in B_* X=U\Sigma V^T \Sigma \le 1 U,V^T","['matrices', 'convex-analysis', 'convex-hulls', 'nuclear-norm', 'rank-1-matrices']"
88,An interesting puzzle in an otherwise boring game,An interesting puzzle in an otherwise boring game,,"I recently had the displeasure of playing the Xbox 360 game Tower Bloxx Deluxe. You've played a version of it if not it itself; you build towers by dropping floors on top of the increasingly swaying tower you've already built. That's not interesting. What is interesting is the game's city building. It gives you a grid of plots on which you build towers for people to live in and maximize your population. There are 5 tiers of tower, each taller thus more populous than the last. The trick is that a tier 2 tower needs to have a tier 1 tower adjacent to it before it can be built. A tier 3 tower needs a tier 1 tower and a tier 2 tower adjacent to it before it can be built. Eventually, a tier 5 tower needs to have all the lower tiers adjacent to it before it can be built. Higher tier towers do not satisfy requirements for the lower tiers. Once a tower is built, these restrictions no longer apply, so you can bulldoze everything around a tower and it will still stand. Reworking this as a math puzzle, consider an n by m matrix M of 1's. Each iteration, you may replace one value in M with a counting number s if every counting number less than s is accounted for in the cells adjacent to it (The game only allows orthogonal adjacency, but allowing diagonal could be interesting too). What is the maximum sum of the values in M? What is the minimum number of iterations required to reach that value? What about expanding this to a graph instead of a matrix? It's fairly obvious that no 5's can ever be next to each other, and that any final grid must have a 1 in it somewhere. I whipped up some Python code to brute force the problem, and it found that the optimal matrices of their sizes, up to symmetry, are $$\begin{vmatrix} 1 \end{vmatrix}, \begin{vmatrix} 2 & 1 \end{vmatrix},  \begin{vmatrix} 2 & 3 \\ 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 \\ 3 & 4 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 4 & 3 \\ 4 & 2 & 4 \\ 3 & 4 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 4 & 1 \\ 3 & 4 & 2 & 3 \end{vmatrix}, \begin{vmatrix} 2 & 4 & 3 & 2 \\ 4 & 1 & 5 & 4 \\ 3 & 4 & 2 & 3 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 4 & 3 & 2 \\ 3 & 4 & 2 & 4 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 4 & 1 & 4 & 3 & 2 \\ 3 & 3 & 4 & 2 & 4 & 3 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}, \begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}$$ Matrices of area greater than 12 cause even my fairly beefy computer to run out of memory. I'm surprised that a 5 only appears for the first time in a 4x3. The nx1's also follow a clear pattern. I'm struggling to make any headway beyond this. What can you all come up with? If anyone else wants to use it, here's the code I wrote (meant to be fast to write, not necessarily easily readable, sorry): from queue import SimpleQueue as Q   def modification(grid, row, col, val):     return grid[:row] + (grid[row][:col] + (val,) + grid[row][col + 1:],) + grid[row + 1:]           def maximize(w, h, p):     q = Q()     s = set()     q.put(tuple((0,) * w for i in range(h)))     return _maximize(q, s, p)           def matsum(grid):     return sum(sum(row) for row in grid)       def _maximize(q, s, p):     G = ((0,),)     r = -1     while not q.empty():         r = (r + 1) % p         if r == 0:             print(""Queue length: "" + str(q.qsize()) +                   ""\tSeen grids: "" + str(len(s)) + "" ""*15, end='\r')         g = q.get()         s.add(g)         if matsum(g) > matsum(G):             G = g         w = len(g[0])         h = len(g)         for x in range(w):             for y in range(h):                 # try setting it to 0                 m = modification(g, y, x, 0)                 if m not in s:                     q.put(m)                 for v in range(1, 5):                     if ((x > 0 and g[y][x - 1] == v - 1)                             or (y > 0 and g[y - 1][x] == v - 1)                             or (x < w - 1 and g[y][x + 1] == v - 1)                             or (y < h - 1 and g[y + 1][x] == v - 1)):                         m = modification(g, y, x, v)                         if m not in s:                             s.add(m)                             q.put(m)                     else:                         break     print(""Queue length: "" + str(q.qsize()) +           ""\tSeen grids: "" + str(len(s)) + "" ""*15)     return G            for i in range(1, 13):     for j in range(1, i+1):         if i*j < 13:             print(str(i) + ""x"" + str(j) + ':')             print(maximize(i, j, 5000)) The following code has been slightly modified to prioritize higher-sum matrices at the cost of performance. The result is that it will find high-sum matrices quickly, but for sizes larger than say 4x3, it will take a prohibitively long time to finish and output the actual highest sum matrix (though it will eventually finish). import heapq   def modification(grid, row, col, val):     return grid[:row] + (grid[row][:col] + (val,) + grid[row][col + 1:],) + grid[row + 1:]   def maximize(w, h, p):     q = []     s = set()     heapq.heappush(q, (-w*h, tuple((1,) * w for i in range(h))))     return _maximize(q, s, p)   def matsum(grid):     return sum(sum(row) for row in grid)   def _maximize(q, s, p):     G = ((0,),)     sG = matsum(G)     r = -1     while len(q) != 0:         r = (r + 1) % p         if r == 0:             print(""len(q)="" + str(len(q)) +                   "" len(s)="" + str(len(s)) +                   "" G="" + str(G) +                    "" matsum(G)="" + str(sG)+"" ""*10, end='\r')         sg, g = heapq.heappop(q)         s.add(g)         if -sg > sG:             sG, G = -sg, g         w = len(g[0])         h = len(g)         for x in range(w):             for y in range(h):                 # try setting it to 1                 m = modification(g, y, x, 1)                 if m not in s:                     heapq.heappush(q, (-matsum(m), m))                     s.add(m)                 for v in range(2, 6):                     if ((x > 0 and g[y][x - 1] == v - 1)                             or (y > 0 and g[y - 1][x] == v - 1)                             or (x < w - 1 and g[y][x + 1] == v - 1)                             or (y < h - 1 and g[y + 1][x] == v - 1)):                         m = modification(g, y, x, v)                         if m not in s:                             s.add(m)                             heapq.heappush(q, (-matsum(m), m))                     else:                         break     print(""len(q)="" + str(len(q)) +                   "" len(s)="" + str(len(s)) +                   "" G="" + str(G) +                    "" matsum(G)="" + str(sG)+"" ""*10)     return G   print(maximize(3, 3, 1000)) # to prove it works #print(maximize(4, 4, 10000))","I recently had the displeasure of playing the Xbox 360 game Tower Bloxx Deluxe. You've played a version of it if not it itself; you build towers by dropping floors on top of the increasingly swaying tower you've already built. That's not interesting. What is interesting is the game's city building. It gives you a grid of plots on which you build towers for people to live in and maximize your population. There are 5 tiers of tower, each taller thus more populous than the last. The trick is that a tier 2 tower needs to have a tier 1 tower adjacent to it before it can be built. A tier 3 tower needs a tier 1 tower and a tier 2 tower adjacent to it before it can be built. Eventually, a tier 5 tower needs to have all the lower tiers adjacent to it before it can be built. Higher tier towers do not satisfy requirements for the lower tiers. Once a tower is built, these restrictions no longer apply, so you can bulldoze everything around a tower and it will still stand. Reworking this as a math puzzle, consider an n by m matrix M of 1's. Each iteration, you may replace one value in M with a counting number s if every counting number less than s is accounted for in the cells adjacent to it (The game only allows orthogonal adjacency, but allowing diagonal could be interesting too). What is the maximum sum of the values in M? What is the minimum number of iterations required to reach that value? What about expanding this to a graph instead of a matrix? It's fairly obvious that no 5's can ever be next to each other, and that any final grid must have a 1 in it somewhere. I whipped up some Python code to brute force the problem, and it found that the optimal matrices of their sizes, up to symmetry, are Matrices of area greater than 12 cause even my fairly beefy computer to run out of memory. I'm surprised that a 5 only appears for the first time in a 4x3. The nx1's also follow a clear pattern. I'm struggling to make any headway beyond this. What can you all come up with? If anyone else wants to use it, here's the code I wrote (meant to be fast to write, not necessarily easily readable, sorry): from queue import SimpleQueue as Q   def modification(grid, row, col, val):     return grid[:row] + (grid[row][:col] + (val,) + grid[row][col + 1:],) + grid[row + 1:]           def maximize(w, h, p):     q = Q()     s = set()     q.put(tuple((0,) * w for i in range(h)))     return _maximize(q, s, p)           def matsum(grid):     return sum(sum(row) for row in grid)       def _maximize(q, s, p):     G = ((0,),)     r = -1     while not q.empty():         r = (r + 1) % p         if r == 0:             print(""Queue length: "" + str(q.qsize()) +                   ""\tSeen grids: "" + str(len(s)) + "" ""*15, end='\r')         g = q.get()         s.add(g)         if matsum(g) > matsum(G):             G = g         w = len(g[0])         h = len(g)         for x in range(w):             for y in range(h):                 # try setting it to 0                 m = modification(g, y, x, 0)                 if m not in s:                     q.put(m)                 for v in range(1, 5):                     if ((x > 0 and g[y][x - 1] == v - 1)                             or (y > 0 and g[y - 1][x] == v - 1)                             or (x < w - 1 and g[y][x + 1] == v - 1)                             or (y < h - 1 and g[y + 1][x] == v - 1)):                         m = modification(g, y, x, v)                         if m not in s:                             s.add(m)                             q.put(m)                     else:                         break     print(""Queue length: "" + str(q.qsize()) +           ""\tSeen grids: "" + str(len(s)) + "" ""*15)     return G            for i in range(1, 13):     for j in range(1, i+1):         if i*j < 13:             print(str(i) + ""x"" + str(j) + ':')             print(maximize(i, j, 5000)) The following code has been slightly modified to prioritize higher-sum matrices at the cost of performance. The result is that it will find high-sum matrices quickly, but for sizes larger than say 4x3, it will take a prohibitively long time to finish and output the actual highest sum matrix (though it will eventually finish). import heapq   def modification(grid, row, col, val):     return grid[:row] + (grid[row][:col] + (val,) + grid[row][col + 1:],) + grid[row + 1:]   def maximize(w, h, p):     q = []     s = set()     heapq.heappush(q, (-w*h, tuple((1,) * w for i in range(h))))     return _maximize(q, s, p)   def matsum(grid):     return sum(sum(row) for row in grid)   def _maximize(q, s, p):     G = ((0,),)     sG = matsum(G)     r = -1     while len(q) != 0:         r = (r + 1) % p         if r == 0:             print(""len(q)="" + str(len(q)) +                   "" len(s)="" + str(len(s)) +                   "" G="" + str(G) +                    "" matsum(G)="" + str(sG)+"" ""*10, end='\r')         sg, g = heapq.heappop(q)         s.add(g)         if -sg > sG:             sG, G = -sg, g         w = len(g[0])         h = len(g)         for x in range(w):             for y in range(h):                 # try setting it to 1                 m = modification(g, y, x, 1)                 if m not in s:                     heapq.heappush(q, (-matsum(m), m))                     s.add(m)                 for v in range(2, 6):                     if ((x > 0 and g[y][x - 1] == v - 1)                             or (y > 0 and g[y - 1][x] == v - 1)                             or (x < w - 1 and g[y][x + 1] == v - 1)                             or (y < h - 1 and g[y + 1][x] == v - 1)):                         m = modification(g, y, x, v)                         if m not in s:                             s.add(m)                             heapq.heappush(q, (-matsum(m), m))                     else:                         break     print(""len(q)="" + str(len(q)) +                   "" len(s)="" + str(len(s)) +                   "" G="" + str(G) +                    "" matsum(G)="" + str(sG)+"" ""*10)     return G   print(maximize(3, 3, 1000)) # to prove it works #print(maximize(4, 4, 10000))","\begin{vmatrix} 1 \end{vmatrix},
\begin{vmatrix} 2 & 1 \end{vmatrix}, 
\begin{vmatrix} 2 & 3 \\ 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 \\ 3 & 4 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 4 & 3 \\ 4 & 2 & 4 \\ 3 & 4 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 4 & 1 \\ 3 & 4 & 2 & 3 \end{vmatrix},
\begin{vmatrix} 2 & 4 & 3 & 2 \\ 4 & 1 & 5 & 4 \\ 3 & 4 & 2 & 3 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 4 & 3 & 2 \\ 3 & 4 & 2 & 4 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 4 & 1 & 4 & 3 & 2 \\ 3 & 3 & 4 & 2 & 4 & 3 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix},
\begin{vmatrix} 2 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 2 & 3 & 1 \end{vmatrix}","['matrices', 'optimization', 'puzzle']"
89,Simplifying Trace of a Matrix Expression ${\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)$,Simplifying Trace of a Matrix Expression,{\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right),Let $B$ be a symmetric invertible matrix  and let $A$ be a diagonal matrix with non-zero entries on the main diagonal. I am trying to simplify the following expression  \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right) \end{align} where $D=I+ ABA$ Case that I did: In the one dimensional case we get  \begin{align} \frac{B}{(1+BA^2)^2}. \end{align} Also in the case that $A = a I$ (all diagonal elements are the same) we get \begin{align} (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T = (I+ BA^2)^{-1} B (I+ BA^2)^{-1} \end{align}  and therefore the trace becomes \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right). \end{align} My question is what can we say about a more general diagonal $A$? Can we show also that  \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right) \end{align} Note that another way to look at this problem is to look through the norm operator \begin{align} \| I- D^{-1} BA^2 \| \end{align} I wounder if there is a software that can take care of this simplification.,Let $B$ be a symmetric invertible matrix  and let $A$ be a diagonal matrix with non-zero entries on the main diagonal. I am trying to simplify the following expression  \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right) \end{align} where $D=I+ ABA$ Case that I did: In the one dimensional case we get  \begin{align} \frac{B}{(1+BA^2)^2}. \end{align} Also in the case that $A = a I$ (all diagonal elements are the same) we get \begin{align} (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T = (I+ BA^2)^{-1} B (I+ BA^2)^{-1} \end{align}  and therefore the trace becomes \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right). \end{align} My question is what can we say about a more general diagonal $A$? Can we show also that  \begin{align} {\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right) \end{align} Note that another way to look at this problem is to look through the norm operator \begin{align} \| I- D^{-1} BA^2 \| \end{align} I wounder if there is a software that can take care of this simplification.,,"['matrices', 'matrix-equations', 'matrix-decomposition', 'trace']"
90,Upper bound on infinity norm of inverse of a positive definite matrix [closed],Upper bound on infinity norm of inverse of a positive definite matrix [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Consider a positive definite matrix, $A$, and the following quantity: \begin{align} \|A^{-1}\|_\infty \end{align} Are there any upper bounds on the above normed term? EDIT: The bound should be in terms of $A$, such as the elements or eigenvalues of $A$.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Consider a positive definite matrix, $A$, and the following quantity: \begin{align} \|A^{-1}\|_\infty \end{align} Are there any upper bounds on the above normed term? EDIT: The bound should be in terms of $A$, such as the elements or eigenvalues of $A$.",,"['matrices', 'normed-spaces']"
91,Calculating Norms for the Transpose,Calculating Norms for the Transpose,,"How does one show that the transpose mapping $T: \mathcal{M}_{n} \to \mathcal{M}_{n}$, given by $T(a)=a^{t}$, has norm $||T||=1$ but completely bounded norm $||T||_{CB}=n$? Notation. $\mathcal{M}_{n}$ denotes the space of $n \times n$ matrices. Reference: Example 1.1.6 from http://arxiv.org/pdf/1410.7188v3.pdf There's a proof included in the reference. However, I am interested in alternate ways to calculate the norms above.","How does one show that the transpose mapping $T: \mathcal{M}_{n} \to \mathcal{M}_{n}$, given by $T(a)=a^{t}$, has norm $||T||=1$ but completely bounded norm $||T||_{CB}=n$? Notation. $\mathcal{M}_{n}$ denotes the space of $n \times n$ matrices. Reference: Example 1.1.6 from http://arxiv.org/pdf/1410.7188v3.pdf There's a proof included in the reference. However, I am interested in alternate ways to calculate the norms above.",,"['matrices', 'operator-theory', 'normed-spaces', 'operator-algebras']"
92,Upper bound on the distance of orthogonal matrices,Upper bound on the distance of orthogonal matrices,,"Dear math stackexchange users, I have a question on orthogonal matrices: suppose I have a matrix $X\in\mathbb{R}^{n\times n}$ and I consider the orbit of the orthogonal group $O(n)$ acting from the left on $X$:  $$Orb= \{Y\in\mathbb{R}^{n\times n}\ |\ Y=OX, O\in O(n)\}.$$ Now let $\epsilon>0$ and suppose I have $OX\in Orb$ satisfying $\|X-OX\|_F\leq \epsilon$. If $I$ is the identity matrix, is there any way to give an upper bound on the distance $\|I-O\|_F$ in terms of $\|X\|_F$ and $\epsilon$? Of course, if I have an upper bound on $\|I-O\|_F$, then by submultiplicativity of the Frobenius norm this gives an upper bound on $\|X-OX\|$. However, is there anything for the other direction? The orthogonal group is a compact set and this gives a trivial upper bound. This one however, is not applicable in my problem. Thank you very much!","Dear math stackexchange users, I have a question on orthogonal matrices: suppose I have a matrix $X\in\mathbb{R}^{n\times n}$ and I consider the orbit of the orthogonal group $O(n)$ acting from the left on $X$:  $$Orb= \{Y\in\mathbb{R}^{n\times n}\ |\ Y=OX, O\in O(n)\}.$$ Now let $\epsilon>0$ and suppose I have $OX\in Orb$ satisfying $\|X-OX\|_F\leq \epsilon$. If $I$ is the identity matrix, is there any way to give an upper bound on the distance $\|I-O\|_F$ in terms of $\|X\|_F$ and $\epsilon$? Of course, if I have an upper bound on $\|I-O\|_F$, then by submultiplicativity of the Frobenius norm this gives an upper bound on $\|X-OX\|$. However, is there anything for the other direction? The orthogonal group is a compact set and this gives a trivial upper bound. This one however, is not applicable in my problem. Thank you very much!",,"['matrices', 'differential-geometry', 'analytic-geometry', 'riemannian-geometry', 'matrix-calculus']"
93,Birkhoff representation of a stochastic matrix,Birkhoff representation of a stochastic matrix,,"From the Birkhoff theorem , it is known that every doubly stochastic matrix can be written as a convex combination of permutation matrices, although this representation might not be unique. Assume that a stochastic matrix is given. How can I find a permutation matrix that has a nonzero weight in at least one convex representation of the given stochastic matrix? For example, if $$P = \begin{bmatrix} \frac 12 & \frac 12\\ \frac 12 & \frac 12\end{bmatrix}$$ then $P$ can be written as follows $$P = \frac 12 \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix} + \frac 12 \begin{bmatrix} 0 & 1\\ 1 & 0\end{bmatrix}$$ In this case, both permutation matrices in the right-hand side have the property that I wish because they receive nonzero weight in at least one convex representation of $P$. Is there any simple algorithm to rapidly find at least one such permutation matrix for a given stochastic matrix?","From the Birkhoff theorem , it is known that every doubly stochastic matrix can be written as a convex combination of permutation matrices, although this representation might not be unique. Assume that a stochastic matrix is given. How can I find a permutation matrix that has a nonzero weight in at least one convex representation of the given stochastic matrix? For example, if $$P = \begin{bmatrix} \frac 12 & \frac 12\\ \frac 12 & \frac 12\end{bmatrix}$$ then $P$ can be written as follows $$P = \frac 12 \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix} + \frac 12 \begin{bmatrix} 0 & 1\\ 1 & 0\end{bmatrix}$$ In this case, both permutation matrices in the right-hand side have the property that I wish because they receive nonzero weight in at least one convex representation of $P$. Is there any simple algorithm to rapidly find at least one such permutation matrix for a given stochastic matrix?",,"['matrices', 'permutations', 'convex-hulls', 'stochastic-matrices', 'birkhoff-polytopes']"
94,Continuous subgroup of SO(3)?,Continuous subgroup of SO(3)?,,"I read from a paper arXiv: cond-mat/0602109 by a theoretical physicist, Prof. Frank Bais,  close subgroups of $SO(3)$ is given by ${C_n,D_n,T,O,I,SO(2)\rtimes Z_2}$, where $C_n$ is the  cyclic group of order $n$, $D_n$ is dihedral group of order $n$, $T$ is the tetrahedral group, $O$ is the octahedral group, and $I$ is the icosahedral group. However, I have some questions to understand this. Is $SO(2)$ a subgroup of $SO(3)$? The subgroups listed in the paper mentioned above is about close subgroups. What's the meaning of 'close' here? If $SO(2)$ is a subgroup of $SO(3)$, can it be parameterized as a $3\times 3$ matrix as \begin{align} \left( \begin{array}{ccc}  \cos\theta & -\sin\theta & 0 \\  \sin\theta & \cos\theta & 0 \\  0 & 0 & 1 \end{array} \right) \end{align} with a imagined '$z$' axis? Can the subgroup $SO(2)\rtimes Z_2$ be parameterized as a $3\times 3$ matrix? E.g, \begin{align} \left( \begin{array}{ccc}  \cos\theta & -\sin\theta & 0 \\  \sin\theta & \cos\theta & 0 \\  0 & 0 & \ 1 \end{array} \right), \left( \begin{array}{ccc}  \cos\theta & \sin\theta & 0 \\  \sin\theta & -\cos\theta & 0 \\  0 & 0 & \ -1 \end{array} \right) \end{align} Moreover, any suggestions of tutorial references or textbooks that I can find the list of subgroups or the way I can calculate them by myself are also warmly welcome. I am not familiar with continuous groups.","I read from a paper arXiv: cond-mat/0602109 by a theoretical physicist, Prof. Frank Bais,  close subgroups of $SO(3)$ is given by ${C_n,D_n,T,O,I,SO(2)\rtimes Z_2}$, where $C_n$ is the  cyclic group of order $n$, $D_n$ is dihedral group of order $n$, $T$ is the tetrahedral group, $O$ is the octahedral group, and $I$ is the icosahedral group. However, I have some questions to understand this. Is $SO(2)$ a subgroup of $SO(3)$? The subgroups listed in the paper mentioned above is about close subgroups. What's the meaning of 'close' here? If $SO(2)$ is a subgroup of $SO(3)$, can it be parameterized as a $3\times 3$ matrix as \begin{align} \left( \begin{array}{ccc}  \cos\theta & -\sin\theta & 0 \\  \sin\theta & \cos\theta & 0 \\  0 & 0 & 1 \end{array} \right) \end{align} with a imagined '$z$' axis? Can the subgroup $SO(2)\rtimes Z_2$ be parameterized as a $3\times 3$ matrix? E.g, \begin{align} \left( \begin{array}{ccc}  \cos\theta & -\sin\theta & 0 \\  \sin\theta & \cos\theta & 0 \\  0 & 0 & \ 1 \end{array} \right), \left( \begin{array}{ccc}  \cos\theta & \sin\theta & 0 \\  \sin\theta & -\cos\theta & 0 \\  0 & 0 & \ -1 \end{array} \right) \end{align} Moreover, any suggestions of tutorial references or textbooks that I can find the list of subgroups or the way I can calculate them by myself are also warmly welcome. I am not familiar with continuous groups.",,"['group-theory', 'matrices']"
95,term for a sum of diagonal and skew-symmetric matrix?,term for a sum of diagonal and skew-symmetric matrix?,,"Is there a term for a matrix that is a sum of a diagonal and a skew-symmetric matrix? One particular example of this is a 2x2 matrix of the form $$ M = \begin{bmatrix} a & b \\ -b & a \end{bmatrix} $$ where a and b are real coefficients. There's something special about this form vs. general matrices with off-diagonal coefficients, in the context of multivariable differential equations, that I can't put my finger on (something to do with the symmetry of the eigenvalues and eigenvectors), but maybe I can figure it out if I know what search term to use. edit: magic property encountered here in the 2x2 case: the eigenvalues are $\lambda = a\pm jb$ and the eigenvectors are $v=\begin{bmatrix}1\\\pm j\end{bmatrix} $. (p.s. yes, I'm an engineer, I use $j=\sqrt{-1}$ instead of $i$)","Is there a term for a matrix that is a sum of a diagonal and a skew-symmetric matrix? One particular example of this is a 2x2 matrix of the form $$ M = \begin{bmatrix} a & b \\ -b & a \end{bmatrix} $$ where a and b are real coefficients. There's something special about this form vs. general matrices with off-diagonal coefficients, in the context of multivariable differential equations, that I can't put my finger on (something to do with the symmetry of the eigenvalues and eigenvectors), but maybe I can figure it out if I know what search term to use. edit: magic property encountered here in the 2x2 case: the eigenvalues are $\lambda = a\pm jb$ and the eigenvectors are $v=\begin{bmatrix}1\\\pm j\end{bmatrix} $. (p.s. yes, I'm an engineer, I use $j=\sqrt{-1}$ instead of $i$)",,"['matrices', 'terminology']"
96,Hermitian positive semi-definite matrix is a Gram matrix,Hermitian positive semi-definite matrix is a Gram matrix,,"I showed that every Gram matrix, i.e. a $n \times n$ matrix $A$ with $A_{ij} = \langle x_i,x_j\rangle$ where $x_1,...,x_n$ are vectors in an inner product vector space $V$ , is Hermitian and positive semi-definite. But how to show the converse: For every Hermitian positive semi-definite matrix there is a inner product space $V$ and vectors $x_1,...,x_n$ such that $\langle  x_i,x_j\rangle = A_{ij}$ ? Any help is appreciated.","I showed that every Gram matrix, i.e. a matrix with where are vectors in an inner product vector space , is Hermitian and positive semi-definite. But how to show the converse: For every Hermitian positive semi-definite matrix there is a inner product space and vectors such that ? Any help is appreciated.","n \times n A A_{ij} = \langle x_i,x_j\rangle x_1,...,x_n V V x_1,...,x_n \langle  x_i,x_j\rangle = A_{ij}","['matrices', 'inner-products']"
97,Linear Systems - Matrix Powers - Determinants,Linear Systems - Matrix Powers - Determinants,,Is there a simple way of determining the determinant of a matrix of the following form? $$ P=\left[x \mid Ax \mid A^2x \mid \cdots \mid A^{(n-1)}x \right] $$ Here $A$ is an $n\times n$ matrix and $x$ is a $n\times 1$ vector. Can we represent $\det(P)$ as a function of $A$ and $x$?,Is there a simple way of determining the determinant of a matrix of the following form? $$ P=\left[x \mid Ax \mid A^2x \mid \cdots \mid A^{(n-1)}x \right] $$ Here $A$ is an $n\times n$ matrix and $x$ is a $n\times 1$ vector. Can we represent $\det(P)$ as a function of $A$ and $x$?,,['matrices']
98,0 as the unique eigenvalue of a matrix,0 as the unique eigenvalue of a matrix,,"I want to prove the following statement : Let $B$ be a matrix, such that $B$ has the eigenvalue $0$ and no other eigenvalue. Then $B^2=0$. In the context of the statement, $B$ is of size $2$. Is this hypothesis necessary for the statement to hold ? How to prove the statement ?","I want to prove the following statement : Let $B$ be a matrix, such that $B$ has the eigenvalue $0$ and no other eigenvalue. Then $B^2=0$. In the context of the statement, $B$ is of size $2$. Is this hypothesis necessary for the statement to hold ? How to prove the statement ?",,"['matrices', 'eigenvalues-eigenvectors']"
99,If $A$ is a square matrix such that $A^{27}=A^{64}=I$ then $A=I$,If  is a square matrix such that  then,A A^{27}=A^{64}=I A=I,"If $A$ is a square matrix such that $A^{27}=A^{64}=I$ then $A=I$. What I did is to subtract I from both sides of the equation: $$A^{27}-I=A^{64}-I=0$$ then: \begin{align*} A^{27}-I &= (A-I)(A+A^2+A^3+\dots+A^{26})=0\\ A^{64}-I &= (A-I)(A+A^2+A^3+\dots+A^{63})=0. \end{align*} So from what I understand, either $A=I$ (as needed) or $A+A^2+A^3+\dots+A^{26}=0$ or $A+A^2+A^3+\dots+A^{63}=0$. At this point I got stuck. By the way, I found out that $A$ is an invertible matrix because if $A^{27}=I$ then also $A^{26}A=AA^{26}=I$ then $A^{26}=A^{-1}$. Also I thought to use the contradiction proving by assuming that $A+A^2+A^3+\dots+A^{63}=0$, but because $A^{27}=I$, then: $$A+A^2+A^3+\dots+A^{26}+I+A^{28}+\dots+A^{53}+I+A^{55}+\dots+A^{63}=0$$ but yet nothing. Would appreciate your guidance, thanks!","If $A$ is a square matrix such that $A^{27}=A^{64}=I$ then $A=I$. What I did is to subtract I from both sides of the equation: $$A^{27}-I=A^{64}-I=0$$ then: \begin{align*} A^{27}-I &= (A-I)(A+A^2+A^3+\dots+A^{26})=0\\ A^{64}-I &= (A-I)(A+A^2+A^3+\dots+A^{63})=0. \end{align*} So from what I understand, either $A=I$ (as needed) or $A+A^2+A^3+\dots+A^{26}=0$ or $A+A^2+A^3+\dots+A^{63}=0$. At this point I got stuck. By the way, I found out that $A$ is an invertible matrix because if $A^{27}=I$ then also $A^{26}A=AA^{26}=I$ then $A^{26}=A^{-1}$. Also I thought to use the contradiction proving by assuming that $A+A^2+A^3+\dots+A^{63}=0$, but because $A^{27}=I$, then: $$A+A^2+A^3+\dots+A^{26}+I+A^{28}+\dots+A^{53}+I+A^{55}+\dots+A^{63}=0$$ but yet nothing. Would appreciate your guidance, thanks!",,['matrices']
