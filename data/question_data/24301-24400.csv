,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the general expression for a unitary $3 \times 3$ matrix?,What is the general expression for a unitary  matrix?,3 \times 3,The Wikipedia page for unitary matrices gives a general expression for the $2 \times 2$ unitary matrix: \begin{pmatrix} a & b \\ -e^{i\phi}b^* & e^{i\phi}a^* \end{pmatrix} with $|a^2| + |b^2| = 1$ . Is there a similar general construction for the $3 \times 3$ unitary matrix?,The Wikipedia page for unitary matrices gives a general expression for the unitary matrix: with . Is there a similar general construction for the unitary matrix?,"2 \times 2 \begin{pmatrix}
a & b \\
-e^{i\phi}b^* & e^{i\phi}a^*
\end{pmatrix} |a^2| + |b^2| = 1 3 \times 3","['linear-algebra', 'matrices', 'unitary-matrices', 'linear-groups']"
1,System of linear equations with positive solutions / intersection of quadrics in $\mathbb{R}^6$,System of linear equations with positive solutions / intersection of quadrics in,\mathbb{R}^6,"Given any 12 real constants $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ all $>0$ I'm asking myself if is it always possible to find positive reals $x,y,z,t,u,v$ such that $$a_1x+a_2y+a_3z=b_1t+b_2u+b_3x=c_1t+c_2y+c_3v=d_1v+d_2u+d_3z$$ But I can not find an answer since I don't know how to work with the condition of $x,y,z,t,u,v$ positive. What do you think? If the answer is ""no"" is there a condition on $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ to guarantee that there exist such $x,y,z,t,u,v$? As pointed out by user Marty Cohen one could substitute the variables $x,y,\dots$ with the variables $x^2,y^2,\dots$ and get quadratic equation. At this point one could introduce another variable $k\in \mathbb{R}$ and ask if the following system $$a_1x^2+a_2y^2+a_3z^2=k\\ b_1t^2+b_2u^2+b_3x^2=k\\ c_1t^2+c_2y^2+c_3v^2=k\\ d_1v^2+d_2u^2+d_3z^2=k$$ has a solution such that all $x,y,z,t,u,v$ are $\neq 0$. Note that this is the intersection of 4 quadrics of $\mathbb{R}^6$ Edit: I'm searching conditions on the $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ which guarantee the existence of $x,y,zt,u,v>0$, I don't want to solve the system with a linear program","Given any 12 real constants $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ all $>0$ I'm asking myself if is it always possible to find positive reals $x,y,z,t,u,v$ such that $$a_1x+a_2y+a_3z=b_1t+b_2u+b_3x=c_1t+c_2y+c_3v=d_1v+d_2u+d_3z$$ But I can not find an answer since I don't know how to work with the condition of $x,y,z,t,u,v$ positive. What do you think? If the answer is ""no"" is there a condition on $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ to guarantee that there exist such $x,y,z,t,u,v$? As pointed out by user Marty Cohen one could substitute the variables $x,y,\dots$ with the variables $x^2,y^2,\dots$ and get quadratic equation. At this point one could introduce another variable $k\in \mathbb{R}$ and ask if the following system $$a_1x^2+a_2y^2+a_3z^2=k\\ b_1t^2+b_2u^2+b_3x^2=k\\ c_1t^2+c_2y^2+c_3v^2=k\\ d_1v^2+d_2u^2+d_3z^2=k$$ has a solution such that all $x,y,z,t,u,v$ are $\neq 0$. Note that this is the intersection of 4 quadrics of $\mathbb{R}^6$ Edit: I'm searching conditions on the $a_1,a_2,a_3,b_1,b_2,b_3,c_1,c_2,c_3,d_1,d_2,d_3$ which guarantee the existence of $x,y,zt,u,v>0$, I don't want to solve the system with a linear program",,"['linear-algebra', 'systems-of-equations']"
2,How to find an approximate solution of a three dimensional DE system,How to find an approximate solution of a three dimensional DE system,,"Suppose A is a $3$ by $3$ matrix. How can I find the approximated solution as $t$ goes to $\infty$ of the system $$X' = AX$$ I think it has something to do with the Jordan decomposition of A, but how? If $A = SJS^{-1}$ can I just take the solution to the system $X' = JX$?","Suppose A is a $3$ by $3$ matrix. How can I find the approximated solution as $t$ goes to $\infty$ of the system $$X' = AX$$ I think it has something to do with the Jordan decomposition of A, but how? If $A = SJS^{-1}$ can I just take the solution to the system $X' = JX$?",,"['linear-algebra', 'ordinary-differential-equations']"
3,Conditions for positive definiteness: matrix inequality,Conditions for positive definiteness: matrix inequality,,"Let $0<\alpha<1$ and $A,B\in\mathbb{R}^{n\times n}$. I am trying to find conditions on $A$ and $B$ such that \begin{equation} I_n-\frac{1}{\alpha}B^{\rm T}B-\frac{1}{4\alpha(1-\alpha)}( A^{\rm T}B+A)^{\rm T}( A^{\rm T}B+A)>0. \end{equation} However, I do not know how to proceed. Any idea or suggestion is appreciated.","Let $0<\alpha<1$ and $A,B\in\mathbb{R}^{n\times n}$. I am trying to find conditions on $A$ and $B$ such that \begin{equation} I_n-\frac{1}{\alpha}B^{\rm T}B-\frac{1}{4\alpha(1-\alpha)}( A^{\rm T}B+A)^{\rm T}( A^{\rm T}B+A)>0. \end{equation} However, I do not know how to proceed. Any idea or suggestion is appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-theory', 'positive-definite']"
4,Blockwise cofactor matrix identity,Blockwise cofactor matrix identity,,"Wikipedia gives an identity for blockwise inversion, assuming the appropriate inverses exist: $$\begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}^{-1} = \begin{bmatrix} \mathbf{A}^{-1}+\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \end{bmatrix}$$ Is there a corresponding general formula for the matrix of cofactors: $$\operatorname{adj}^T\begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}$$ for when $\mathbf{A}^{-1}$ or $(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}$ do not necessarily exist? Is there at least one for bordered matrices, i.e. $\mathbf{D} \in \mathbb{C}^{1\times 1}$?","Wikipedia gives an identity for blockwise inversion, assuming the appropriate inverses exist: $$\begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}^{-1} = \begin{bmatrix} \mathbf{A}^{-1}+\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \end{bmatrix}$$ Is there a corresponding general formula for the matrix of cofactors: $$\operatorname{adj}^T\begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}$$ for when $\mathbf{A}^{-1}$ or $(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}$ do not necessarily exist? Is there at least one for bordered matrices, i.e. $\mathbf{D} \in \mathbb{C}^{1\times 1}$?",,"['linear-algebra', 'matrices', 'reference-request', 'matrix-decomposition', 'pseudoinverse']"
5,Intuition for bases having the same orientation,Intuition for bases having the same orientation,,"Let $V$ be a finite dimensional real vector space.  I'm trying to develop an intuitive understanding of what it means for two ordered bases $\alpha = (u_1,\ldots,u_n)$ and $\beta = (v_1,\ldots,v_n)$ of $V$ to have the same orientation. A popular definition is that $\alpha$ and $\beta$ have the same orientation if and only if the change of basis matrix from $\alpha$ to $\beta$ has a positive determinant.  (For example, on p. 238 [section 21.1] of An Introduction to Manifolds by Tu, there is a statement, ""We say that two ordered bases are equivalent if the change-of-basis matrix $A$ has positive determinant."") However, this definition does not seem very intuitively clear to me. I can't see directly what this definition means in an intuitive way. Here is an attempt at a more intuitive definition.  Ordered bases $\alpha$ and $\beta$ have the same orientation if and only if there exist continuous functions $w_i:[0,1] \to V$ (for $i = 1,\ldots, n$) such that: The set $\{w_1(t),\ldots,w_n(t)\}$ is linearly independent for all $t \in [0,1]$. $w_i(0) = u_i$ for $i = 1,\ldots, n$. $w_i(1) = v_i$ for $i = 1,\ldots, n$. Intuitively, this definition means that $\alpha$ can be ""continuously deformed"" into $\beta$ without passing through a degenerate configuration.  I like this because it matches my intuition from $\mathbb R^3$: when I try to imagine deforming a right-handed basis into a left-handed basis, I always have to pass through a degenerate configuration.  I can also visualize one and two dimensional examples. I would like to show that this definition is equivalent to the more standard definition that I gave at the beginning.  Here is an attempted proof.  First of all, suppose that $\alpha$ and $\beta$ have the same orientation according to my proposed definition.  Let $R(t)$ be the change of basis matrix from $\alpha$ to the ordered basis $(w_1(t),\ldots, w_n(t))$.  Because $\det R(t)$ is a continuous function that is initially equal to $1$ and is never $0$, it must be true that $\det R(t) > 0$ for all $t \in [0,1]$.  In particular, $\det R(1) > 0$.  This shows that the change of basis matrix from $\alpha$ to $\beta$ has positive determinant. Conversely, I need to show that if $\alpha$ and $\beta$ have the same orientation according to the standard definition (i.e., the change of basis matrix from $\alpha$ to $\beta$ has positive determinant), then $\alpha$ and $\beta$ have the same orientation according to my proposed definition. My question: How do I show this? I've considered some ideas based on using the SVD of the change of basis matrix, or the QR factorization, but I'm not sure they will work.  (Also, I suspect a simpler approach should be available.) Additional question: Please provide any comments that you think might be relevant, or tangentially relevant -- anything that you suspect might improve my understanding of this topic.  Maybe a completely different way of looking at orientation would be more intuitive or enlightening.","Let $V$ be a finite dimensional real vector space.  I'm trying to develop an intuitive understanding of what it means for two ordered bases $\alpha = (u_1,\ldots,u_n)$ and $\beta = (v_1,\ldots,v_n)$ of $V$ to have the same orientation. A popular definition is that $\alpha$ and $\beta$ have the same orientation if and only if the change of basis matrix from $\alpha$ to $\beta$ has a positive determinant.  (For example, on p. 238 [section 21.1] of An Introduction to Manifolds by Tu, there is a statement, ""We say that two ordered bases are equivalent if the change-of-basis matrix $A$ has positive determinant."") However, this definition does not seem very intuitively clear to me. I can't see directly what this definition means in an intuitive way. Here is an attempt at a more intuitive definition.  Ordered bases $\alpha$ and $\beta$ have the same orientation if and only if there exist continuous functions $w_i:[0,1] \to V$ (for $i = 1,\ldots, n$) such that: The set $\{w_1(t),\ldots,w_n(t)\}$ is linearly independent for all $t \in [0,1]$. $w_i(0) = u_i$ for $i = 1,\ldots, n$. $w_i(1) = v_i$ for $i = 1,\ldots, n$. Intuitively, this definition means that $\alpha$ can be ""continuously deformed"" into $\beta$ without passing through a degenerate configuration.  I like this because it matches my intuition from $\mathbb R^3$: when I try to imagine deforming a right-handed basis into a left-handed basis, I always have to pass through a degenerate configuration.  I can also visualize one and two dimensional examples. I would like to show that this definition is equivalent to the more standard definition that I gave at the beginning.  Here is an attempted proof.  First of all, suppose that $\alpha$ and $\beta$ have the same orientation according to my proposed definition.  Let $R(t)$ be the change of basis matrix from $\alpha$ to the ordered basis $(w_1(t),\ldots, w_n(t))$.  Because $\det R(t)$ is a continuous function that is initially equal to $1$ and is never $0$, it must be true that $\det R(t) > 0$ for all $t \in [0,1]$.  In particular, $\det R(1) > 0$.  This shows that the change of basis matrix from $\alpha$ to $\beta$ has positive determinant. Conversely, I need to show that if $\alpha$ and $\beta$ have the same orientation according to the standard definition (i.e., the change of basis matrix from $\alpha$ to $\beta$ has positive determinant), then $\alpha$ and $\beta$ have the same orientation according to my proposed definition. My question: How do I show this? I've considered some ideas based on using the SVD of the change of basis matrix, or the QR factorization, but I'm not sure they will work.  (Also, I suspect a simpler approach should be available.) Additional question: Please provide any comments that you think might be relevant, or tangentially relevant -- anything that you suspect might improve my understanding of this topic.  Maybe a completely different way of looking at orientation would be more intuitive or enlightening.",,"['linear-algebra', 'smooth-manifolds']"
6,How many elements with rank $r$ in the space $\mathbb{M}_n(\mathbb{F}_p)$?,How many elements with rank  in the space ?,r \mathbb{M}_n(\mathbb{F}_p),"Let $\mathbb{F}_p$ be a field with $p$ elements and $\mathbb{M}_n(\mathbb{F}_p)$ is the set of all $n \times n$ matrices over the field $\mathbb{F}_p$ . Now, we know that $|\mathbb{M}_n(\mathbb{F}_p)| = p^{n^2}$ . The number of matrix with rank $0$ is $1$ , namely the null matrix of order $n$ . Number of matrices with rank $n$ is $$\prod_{i=0}^{n-1} (p^n-p^i),$$ namley $GL(n, \mathbb{F}_p)$ . How many elements with rank $r~ (0 \leq r \leq n)$ in $\mathbb{M}_n(\mathbb{F}_p)$ ? Note: If we denote the number of elements with rank $i~ (0 \leq i \leq n)$ is $R_i$ in $\mathbb{M}_n(\mathbb{F}_p)$ then $$\sum_{i=0}^{n} R_i= p^{n^2}.$$","Let be a field with elements and is the set of all matrices over the field . Now, we know that . The number of matrix with rank is , namely the null matrix of order . Number of matrices with rank is namley . How many elements with rank in ? Note: If we denote the number of elements with rank is in then","\mathbb{F}_p p \mathbb{M}_n(\mathbb{F}_p) n \times n \mathbb{F}_p |\mathbb{M}_n(\mathbb{F}_p)| = p^{n^2} 0 1 n n \prod_{i=0}^{n-1} (p^n-p^i), GL(n, \mathbb{F}_p) r~ (0 \leq r \leq n) \mathbb{M}_n(\mathbb{F}_p) i~ (0 \leq i \leq n) R_i \mathbb{M}_n(\mathbb{F}_p) \sum_{i=0}^{n} R_i= p^{n^2}.","['linear-algebra', 'abstract-algebra', 'coding-theory']"
7,Checking if matrix is diagonalizable,Checking if matrix is diagonalizable,,"When trying to check if $$C=\begin{pmatrix} 1&1&1\\ 1&1&1\\ 1&1&1\\ \end{pmatrix}\in\Bbb R^{3\times3}$$ is diagonalizable, we find $c_C(x)=-x^2(x-3) \Rightarrow λ=0,3$ with $m(0)=2,m(3)=1$. So to find eigenvectors we solve $(C-λI)X=0$ which give $$\begin{pmatrix}x\\y\\z\\\end{pmatrix}\in\Bbb R^{3\times1}\setminus\{0\},\begin{pmatrix}x\\x\\x\\\end{pmatrix}\in\Bbb R^{3\times1}\setminus\{0\}$$ for $0$ and $3$ respectively. Is it this so far correct? If so then how do I proceed to finding if there exists a basis of $\Bbb R^{3\times1}$ from these eigenvectors?","When trying to check if $$C=\begin{pmatrix} 1&1&1\\ 1&1&1\\ 1&1&1\\ \end{pmatrix}\in\Bbb R^{3\times3}$$ is diagonalizable, we find $c_C(x)=-x^2(x-3) \Rightarrow λ=0,3$ with $m(0)=2,m(3)=1$. So to find eigenvectors we solve $(C-λI)X=0$ which give $$\begin{pmatrix}x\\y\\z\\\end{pmatrix}\in\Bbb R^{3\times1}\setminus\{0\},\begin{pmatrix}x\\x\\x\\\end{pmatrix}\in\Bbb R^{3\times1}\setminus\{0\}$$ for $0$ and $3$ respectively. Is it this so far correct? If so then how do I proceed to finding if there exists a basis of $\Bbb R^{3\times1}$ from these eigenvectors?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
8,Every cyclic subspace contains an eigenvector,Every cyclic subspace contains an eigenvector,,"The question is : Let $X$ be a non-null vector.Then there exists an eigenvector $Y$ of $A$ belonging to the span of $\{X, AX, A^{2} X, ... \}$. I have tried to the best of my ability to solve it. But I don't find any right way to proceed.Please help me. I want to add a solution of my own question which I find just now in a pdf. Here's this : Let $k$ be the least positive integer such that $X, AX, A^{2} X, ... , A^{k} X$ are linearly dependent. Now let us consider a relation $\sum_{i=0}^{k} c_{i} A^{i} X = 0$. Then we must have $c_k \neq 0$. Now let us consider a polynomial $g(t) = \sum_{i=0}^{k} c_{i} t^{i}$. Let $\beta_{1}, \beta_{2}, ... , \beta_{k}$ be the roots of the polynomial $g(t)$. Then $g(t) = c_{k} \prod_{i=1}^{k} (t - \beta_{i})$. Hence, $\sum_{i=0}^{k} c_{i} A^{i} = g(A) = c_{k} \prod_{i=1}^{k} (A - \beta_{i} I)$.Taking $Y = (\prod_{i=2}^{k} (A - \beta_{i} I)) X$ , it is easy to see that $Y \neq 0$ by the minimality of $k$ and $(A - \beta_{1} I) Y = 0$. Hence the result follows. But at last it is not clear to me why is $Y$ in the span of $\{X, A X, A^{2} X, ... \}$? Please anyone suggest me what is the trick behind it. Thank you in advance.","The question is : Let $X$ be a non-null vector.Then there exists an eigenvector $Y$ of $A$ belonging to the span of $\{X, AX, A^{2} X, ... \}$. I have tried to the best of my ability to solve it. But I don't find any right way to proceed.Please help me. I want to add a solution of my own question which I find just now in a pdf. Here's this : Let $k$ be the least positive integer such that $X, AX, A^{2} X, ... , A^{k} X$ are linearly dependent. Now let us consider a relation $\sum_{i=0}^{k} c_{i} A^{i} X = 0$. Then we must have $c_k \neq 0$. Now let us consider a polynomial $g(t) = \sum_{i=0}^{k} c_{i} t^{i}$. Let $\beta_{1}, \beta_{2}, ... , \beta_{k}$ be the roots of the polynomial $g(t)$. Then $g(t) = c_{k} \prod_{i=1}^{k} (t - \beta_{i})$. Hence, $\sum_{i=0}^{k} c_{i} A^{i} = g(A) = c_{k} \prod_{i=1}^{k} (A - \beta_{i} I)$.Taking $Y = (\prod_{i=2}^{k} (A - \beta_{i} I)) X$ , it is easy to see that $Y \neq 0$ by the minimality of $k$ and $(A - \beta_{1} I) Y = 0$. Hence the result follows. But at last it is not clear to me why is $Y$ in the span of $\{X, A X, A^{2} X, ... \}$? Please anyone suggest me what is the trick behind it. Thank you in advance.",,['linear-algebra']
9,Matrices whose all principal $k\times k$ sub-matrices are positive semidefinite,Matrices whose all principal  sub-matrices are positive semidefinite,k\times k,"I would like to know whether the set of $n\times n$ Hermitian matrices  whose all ${{n}\choose{k}}$ principal $k\times k$ sub-matrices---the matrices obtained by removing $n-k$ columns as well as the corresponding rows---are positive semidefinite is a well-studied set, and if so under which name. This is for a given $1\leq k\leq n$. If $k=1$, this is the set of $n\times n$ matrices whose diagonal entries are non-negative, and, if $k=n$, it is the set of $n\times n$ positive-semidefinite matrices. I am interested in what is known for the general case $1\leq k\leq n$, in particular for the non-trivial case $1<k<n$, for $n\geq 3$.","I would like to know whether the set of $n\times n$ Hermitian matrices  whose all ${{n}\choose{k}}$ principal $k\times k$ sub-matrices---the matrices obtained by removing $n-k$ columns as well as the corresponding rows---are positive semidefinite is a well-studied set, and if so under which name. This is for a given $1\leq k\leq n$. If $k=1$, this is the set of $n\times n$ matrices whose diagonal entries are non-negative, and, if $k=n$, it is the set of $n\times n$ positive-semidefinite matrices. I am interested in what is known for the general case $1\leq k\leq n$, in particular for the non-trivial case $1<k<n$, for $n\geq 3$.",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
10,Is a faithful representation of the orthogonal group on a vector space equivalent to a choice of inner product?,Is a faithful representation of the orthogonal group on a vector space equivalent to a choice of inner product?,,"Given an $n$-dimensional real vector space $V$, is a choice of faithful representation of the orthogonal group $O(n)$ on $V$ equivalent to a choice of inner product on $V$? I think this should be true. Certainly a choice of inner product on $V$ determines a group $O(V)$ which is isomorphic to $O(n)$ via a choice of orthonormal basis on $V$. However, I am not sure how to define an inner product on $V$ using a given faithful representation of $O(n)$.","Given an $n$-dimensional real vector space $V$, is a choice of faithful representation of the orthogonal group $O(n)$ on $V$ equivalent to a choice of inner product on $V$? I think this should be true. Certainly a choice of inner product on $V$ determines a group $O(V)$ which is isomorphic to $O(n)$ via a choice of orthonormal basis on $V$. However, I am not sure how to define an inner product on $V$ using a given faithful representation of $O(n)$.",,"['linear-algebra', 'group-theory', 'representation-theory', 'lie-groups']"
11,The set of real sequences has no countable spanning set,The set of real sequences has no countable spanning set,,"I'm working through some old Harvard Math 55 problem sets, and one problem in particular has got me stumped: If $F$ is field, then prove that the vector space $F^\infty$ over $F$ (the space of infinite tuples or sequences) has no countable spanning set. In the case that $F$ is countable or finite, we can show that the space spanned by a countable set is necessarily itself countable and therefore cannot span $F^\infty$ , which is uncountable. However, that argument fails when the field is uncountable, since the span of even a single vector is an uncountable set. Any hints on how to proceed?","I'm working through some old Harvard Math 55 problem sets, and one problem in particular has got me stumped: If is field, then prove that the vector space over (the space of infinite tuples or sequences) has no countable spanning set. In the case that is countable or finite, we can show that the space spanned by a countable set is necessarily itself countable and therefore cannot span , which is uncountable. However, that argument fails when the field is uncountable, since the span of even a single vector is an uncountable set. Any hints on how to proceed?",F F^\infty F F F^\infty,"['linear-algebra', 'vector-spaces']"
12,Should I put commas after my matrices in a paper?,Should I put commas after my matrices in a paper?,,"When I need to put a matrix in my paper, I usually use $$  to center it on my page, and I usually describe something about the matrix immediately after, e.g., ""which has real entries..."" should I put a comma after the matrix?  Is that the standard thing to do when writing a paper? There's a bit of spacing between the matrix and the comma, so it looks a bit weird, but I also want to be grammatically correct, too. What do you think? Thanks,","When I need to put a matrix in my paper, I usually use $$  to center it on my page, and I usually describe something about the matrix immediately after, e.g., ""which has real entries..."" should I put a comma after the matrix?  Is that the standard thing to do when writing a paper? There's a bit of spacing between the matrix and the comma, so it looks a bit weird, but I also want to be grammatically correct, too. What do you think? Thanks,",,"['linear-algebra', 'matrices', 'notation', 'article-writing']"
13,Standard hermitian metric on a complex manifold,Standard hermitian metric on a complex manifold,,"I have some trouble in the definition of the hermitian metric on a hermitian manifold: Let $M$ be a riemannian manifold of real dimension $n>1$ with real coordinates $x_1,\dots,x_n$. $M$ has a smoothly varying bilinear form on each tangent space, $ds^2=\sum_{i,j}g_{ij}dx_i\otimes dx_j$. Now suppose that $ds^2$ is the standard bilinear form of $\mathbb{R}^n$, then we can write $ds^2=\sum_{i=1}^ndx_i\otimes dx_i$ and it's such that, given the tangent vector $v=\sum_{j=1}^nv_i\frac{\partial}{\partial x_j}$, $ds^2(v,v)=\sum_{i=1}^ndx_i(\sum_{j=1}^nv_i\frac{\partial}{\partial x_j})\otimes dx_i(\sum_{j=1}^nv_i\frac{\partial}{\partial x_j})=\sum_{i=1}^nv_i^2$. I can't do this same thing on a hermitian manifold: Let $X$ be a hermitian manifold of complex dimension $k>1$ with complex coordinates $z_1,\dots,z_k$. $X$ has a smoothly varying hermitian metric $h=\sum_{i,j}h_{i,j}dz_i\otimes d\overline{z_j}$ on each tangent space. As before, suppose that $h$ is the standard hermitian metric of $\mathbb{C}^k$, then $h=\sum_{i=1}^kdz_i\otimes d\overline{z_i}$. Given a tangent vector $w=\sum_{j=1}^kw_j\frac{\partial}{\partial z_j}$ it should be of course $h(w,w)=\sum_{j=1}^kw_j\overline{w_j}=\sum_{j=1}^k|w_j|^2$. But what I get is $h(w,w)=\sum_{i=1}^kdz_i(\sum_{j=1}^kw_j\frac{\partial}{\partial z_j})\otimes d\overline{z_i}(\sum_{j=1}^kw_j\frac{\partial}{\partial z_j})=0$ since $d\overline{z_i}(\frac{\partial}{\partial z_i})=\frac 1 2(dx_i-idy_i)(\frac{\partial}{\partial x_i}-i\frac{\partial}{\partial y_i})=0$. There must be an error, can someone explain it to me? Also, I don't understand what do the conjugate tangent vectors $\frac{\partial}{\partial \overline{z_j}}$ stand for . What should be the result of $h( \frac{\partial}{\partial \overline{z_j}},\frac{\partial}{\partial \overline{z_j}})$ and $h(\frac{\partial}{\partial z_j},\frac{\partial}{\partial \overline{z_j}})$ if $h$ is the standard hermitian metric?","I have some trouble in the definition of the hermitian metric on a hermitian manifold: Let $M$ be a riemannian manifold of real dimension $n>1$ with real coordinates $x_1,\dots,x_n$. $M$ has a smoothly varying bilinear form on each tangent space, $ds^2=\sum_{i,j}g_{ij}dx_i\otimes dx_j$. Now suppose that $ds^2$ is the standard bilinear form of $\mathbb{R}^n$, then we can write $ds^2=\sum_{i=1}^ndx_i\otimes dx_i$ and it's such that, given the tangent vector $v=\sum_{j=1}^nv_i\frac{\partial}{\partial x_j}$, $ds^2(v,v)=\sum_{i=1}^ndx_i(\sum_{j=1}^nv_i\frac{\partial}{\partial x_j})\otimes dx_i(\sum_{j=1}^nv_i\frac{\partial}{\partial x_j})=\sum_{i=1}^nv_i^2$. I can't do this same thing on a hermitian manifold: Let $X$ be a hermitian manifold of complex dimension $k>1$ with complex coordinates $z_1,\dots,z_k$. $X$ has a smoothly varying hermitian metric $h=\sum_{i,j}h_{i,j}dz_i\otimes d\overline{z_j}$ on each tangent space. As before, suppose that $h$ is the standard hermitian metric of $\mathbb{C}^k$, then $h=\sum_{i=1}^kdz_i\otimes d\overline{z_i}$. Given a tangent vector $w=\sum_{j=1}^kw_j\frac{\partial}{\partial z_j}$ it should be of course $h(w,w)=\sum_{j=1}^kw_j\overline{w_j}=\sum_{j=1}^k|w_j|^2$. But what I get is $h(w,w)=\sum_{i=1}^kdz_i(\sum_{j=1}^kw_j\frac{\partial}{\partial z_j})\otimes d\overline{z_i}(\sum_{j=1}^kw_j\frac{\partial}{\partial z_j})=0$ since $d\overline{z_i}(\frac{\partial}{\partial z_i})=\frac 1 2(dx_i-idy_i)(\frac{\partial}{\partial x_i}-i\frac{\partial}{\partial y_i})=0$. There must be an error, can someone explain it to me? Also, I don't understand what do the conjugate tangent vectors $\frac{\partial}{\partial \overline{z_j}}$ stand for . What should be the result of $h( \frac{\partial}{\partial \overline{z_j}},\frac{\partial}{\partial \overline{z_j}})$ and $h(\frac{\partial}{\partial z_j},\frac{\partial}{\partial \overline{z_j}})$ if $h$ is the standard hermitian metric?",,"['linear-algebra', 'differential-geometry', 'riemannian-geometry', 'complex-geometry']"
14,Is a Rotation Matrix Still a Rotation Matrix under a Non-Standard Basis?,Is a Rotation Matrix Still a Rotation Matrix under a Non-Standard Basis?,,"Let us work in $\mathbb{R}^2$. Consider the following rotation matrix: $$ R = \begin{bmatrix}  \cos \theta & - \sin \theta \\  \sin \theta & \cos \theta \end{bmatrix} $$ I agree that this represents a rotation of angle $\theta$ in $\mathbb{R}^2$ under the standard basis of $\beta = \{(1,0), (0,1)\}$. But would it still represent a rotation if we changed the basis to something non-standard (in both the domain and the codomain), say $\beta_1 = \{1, 1), (1, -1) \}$? That is, would we have that $$ \mathbf{\vec{b}}^t R \mathbf{c} $$ represents a rotation if $\mathbf{\vec{b}}$ represents our non-standard basis and $\mathbf{c}$ represents an arbitrary element in $\mathbb{R}^2$ with respect to the non-standard basis?","Let us work in $\mathbb{R}^2$. Consider the following rotation matrix: $$ R = \begin{bmatrix}  \cos \theta & - \sin \theta \\  \sin \theta & \cos \theta \end{bmatrix} $$ I agree that this represents a rotation of angle $\theta$ in $\mathbb{R}^2$ under the standard basis of $\beta = \{(1,0), (0,1)\}$. But would it still represent a rotation if we changed the basis to something non-standard (in both the domain and the codomain), say $\beta_1 = \{1, 1), (1, -1) \}$? That is, would we have that $$ \mathbf{\vec{b}}^t R \mathbf{c} $$ represents a rotation if $\mathbf{\vec{b}}$ represents our non-standard basis and $\mathbf{c}$ represents an arbitrary element in $\mathbb{R}^2$ with respect to the non-standard basis?",,"['linear-algebra', 'rotations']"
15,"Every finite subgroup of $GL(n,\mathbb{R})$ is conjugate to a subgroup of $O(n)$",Every finite subgroup of  is conjugate to a subgroup of,"GL(n,\mathbb{R}) O(n)","A finite subgroup $G$ of $GL(n,\mathbb{R})$ is conjugate to a subgroup of $O(n)$. Proof. Let's define $\beta \colon \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ such that  $$ \beta(v,w) = \sum_{g\in G} gv \cdot gw $$ where $\cdot$ is the usual dot product. Then $\beta$ is a bilinear symmetric form. Moreover $$\beta(v,v) = \sum_{g\in G} \|gv\|^2 > 0, \quad \forall v\neq \mathbf 0 $$ and $$ \beta(hv,hw) = \sum_{g\in G} ghv \cdot ghw = \sum_{k\in Gh=G} kv \cdot kw = \beta(v,w)$$ Hence $\beta$ is a $G$-invariant inner product and then $G$ is a subgroup of a conjugate of $O(n)$. I can't see how the sentences '$\beta$ is a $G$-invariant inner product' and '$G$ is a subgroup of a conjugate of $O(n)$' are related.","A finite subgroup $G$ of $GL(n,\mathbb{R})$ is conjugate to a subgroup of $O(n)$. Proof. Let's define $\beta \colon \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ such that  $$ \beta(v,w) = \sum_{g\in G} gv \cdot gw $$ where $\cdot$ is the usual dot product. Then $\beta$ is a bilinear symmetric form. Moreover $$\beta(v,v) = \sum_{g\in G} \|gv\|^2 > 0, \quad \forall v\neq \mathbf 0 $$ and $$ \beta(hv,hw) = \sum_{g\in G} ghv \cdot ghw = \sum_{k\in Gh=G} kv \cdot kw = \beta(v,w)$$ Hence $\beta$ is a $G$-invariant inner product and then $G$ is a subgroup of a conjugate of $O(n)$. I can't see how the sentences '$\beta$ is a $G$-invariant inner product' and '$G$ is a subgroup of a conjugate of $O(n)$' are related.",,"['linear-algebra', 'inner-products']"
16,The large-$N$ limit of eigenvalues of matrices with non-diagonal elements scaling as $1/N$,The large- limit of eigenvalues of matrices with non-diagonal elements scaling as,N 1/N,"Define a series of matrices$$H_N= \begin{bmatrix} 1&1/N&1/N&\cdots&1/N\\ 1/N&2&1/N&\cdots&1/N\\ 1/N&1/N&3&\cdots&1/N\\ \vdots&\vdots&\vdots&&\vdots\\ 1/N&1/N&1/N&\cdots&N \end{bmatrix}$$ My question is, when $N\to+\infty$, would the eigenvalues of $H$ be different from $\{1,\ldots,N\}$ ? The answer is not obvious, as,  for the matrix series $$G_N=\begin{bmatrix} 1&1/N&1/N&\cdots&1/N\\ 1/N&1&1/N&\cdots&1/N\\ 1/N&1/N&1&\cdots&1/N\\ \vdots&\vdots&\vdots&&\vdots\\ 1/N&1/N&1/N&\cdots&1 \end{bmatrix}$$ You can verify that $G_N$ has an eigenvalue of $2$.","Define a series of matrices$$H_N= \begin{bmatrix} 1&1/N&1/N&\cdots&1/N\\ 1/N&2&1/N&\cdots&1/N\\ 1/N&1/N&3&\cdots&1/N\\ \vdots&\vdots&\vdots&&\vdots\\ 1/N&1/N&1/N&\cdots&N \end{bmatrix}$$ My question is, when $N\to+\infty$, would the eigenvalues of $H$ be different from $\{1,\ldots,N\}$ ? The answer is not obvious, as,  for the matrix series $$G_N=\begin{bmatrix} 1&1/N&1/N&\cdots&1/N\\ 1/N&1&1/N&\cdots&1/N\\ 1/N&1/N&1&\cdots&1/N\\ \vdots&\vdots&\vdots&&\vdots\\ 1/N&1/N&1/N&\cdots&1 \end{bmatrix}$$ You can verify that $G_N$ has an eigenvalue of $2$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
17,Show that an integer matrix with following conditions is the identity $I$,Show that an integer matrix with following conditions is the identity,I,every entries of $A$ is integer every entries of $A-I$ is multiple of a prime $p$ ($p\geq3$) there exists $n\ge1$ such that $A^n=I$ show that $A=I$ I tried $A=I+p^kB$ where not every entries of $B$ is multiple of $p$. then $(I+p^kB)^n=I+np^kB+{n(n-1)\over2}p^{2k}B^2+...+p^{nk}B^n $ but how should I proceed? thanks in advance,every entries of $A$ is integer every entries of $A-I$ is multiple of a prime $p$ ($p\geq3$) there exists $n\ge1$ such that $A^n=I$ show that $A=I$ I tried $A=I+p^kB$ where not every entries of $B$ is multiple of $p$. then $(I+p^kB)^n=I+np^kB+{n(n-1)\over2}p^{2k}B^2+...+p^{nk}B^n $ but how should I proceed? thanks in advance,,"['linear-algebra', 'algebra-precalculus', 'elementary-number-theory']"
18,Possible ranks of a $n!\times n$ matrix with permuted rows,Possible ranks of a  matrix with permuted rows,n!\times n,"Let $a_1,\cdots,a_n$ be $n$ arbitrary real numbers. Form the $n! \times n$ matrix $M$ whose rows are obtained by permuting the $n$ numbers given. Find all the possible ranks of such a matrix. Obviously, the order of listing out these permutation does not affect the rank. When $n=3$, a possible such $M$ is   $$\begin{pmatrix} a_1 & a_2 & a_3 \\ a_1 & a_3 & a_2\\ a_2 & a_3 & a_1 \\ a_2 & a_1 & a_3 \\ a_3 & a_1 & a_2 \\ a_3 & a_2 & a_1\end{pmatrix}$$ For a general $n$, I have proved the existence of ranks with value $0,1,n-1,n$: $$\eqalign{   & {\text{For }}\operatorname{rank} M = 0,{\text{ put all }}{a_i} = 0  \cr    & {\text{For }}\operatorname{rank} M = 1,{\text{ put all }}{a_i} = 1  \cr    & {\text{For rank }}M = n - 1,{\text{ put }}{a_1} = {a_2} = ... = {a_{n - 1}} = 1{\text{ and }}{a_n} =  - n + 1  \cr    & {\text{For rank }}M = n,{\text{ put }}{a_1} = 1{\text{ and all other }}{a_i} = 0 \cr} $$ All except the third one are trivial observations, to prove the third one, note that when ${a_1} = {a_2} = ... = {a_{n - 1}} = 1{\text{ and }}{a_n} =  - n + 1$, the sum of the $n$ columns of $M$ is zero, hence its columns are linear dependent, we have ${\text{rank }}M \leqslant n - 1$. However, note that the $(n - 1) \times (n - 1)$ matrix $${\left( {\begin{array}{*{20}{c}}   { - n + 1}&1& \cdots &1 \\    1&{ - n + 1}& \cdots &1 \\     \vdots &1& \ddots &1 \\    1&1&1&{ - n + 1}  \end{array}} \right)}$$ is a submatrix of $M$, and by formula for $n \times n$ matrix with $x$ on diagonal and $y$ elsewhere, $$\left| {\begin{array}{*{20}{c}} x&y& \cdots &y\\ y&x& \cdots &y\\  \vdots &y& \ddots &y\\ y&y&y&x \end{array}} \right| = {(x - y)^{n - 1}}\left[ {x + (n - 1)y} \right]$$ this submatrix is invertible and thus has rank $n-1$. Hence the original $M$ satisfies ${\text{rank }}M \geqslant n - 1$ However, I can't prove or disprove the existence of other values of rank. Even the simplest case when $M$ is a $4! \times 4$, I cannot assert or rule out the existence of $M$ such that ${\text{rank }}M = 2$ There is a similar question in Stackexchange, but it only tells the facts that I had already knew, and does not give any insight. This is a question from Artin's Algebra so I think this is not very hard. Any help will be greatly appreciated.","Let $a_1,\cdots,a_n$ be $n$ arbitrary real numbers. Form the $n! \times n$ matrix $M$ whose rows are obtained by permuting the $n$ numbers given. Find all the possible ranks of such a matrix. Obviously, the order of listing out these permutation does not affect the rank. When $n=3$, a possible such $M$ is   $$\begin{pmatrix} a_1 & a_2 & a_3 \\ a_1 & a_3 & a_2\\ a_2 & a_3 & a_1 \\ a_2 & a_1 & a_3 \\ a_3 & a_1 & a_2 \\ a_3 & a_2 & a_1\end{pmatrix}$$ For a general $n$, I have proved the existence of ranks with value $0,1,n-1,n$: $$\eqalign{   & {\text{For }}\operatorname{rank} M = 0,{\text{ put all }}{a_i} = 0  \cr    & {\text{For }}\operatorname{rank} M = 1,{\text{ put all }}{a_i} = 1  \cr    & {\text{For rank }}M = n - 1,{\text{ put }}{a_1} = {a_2} = ... = {a_{n - 1}} = 1{\text{ and }}{a_n} =  - n + 1  \cr    & {\text{For rank }}M = n,{\text{ put }}{a_1} = 1{\text{ and all other }}{a_i} = 0 \cr} $$ All except the third one are trivial observations, to prove the third one, note that when ${a_1} = {a_2} = ... = {a_{n - 1}} = 1{\text{ and }}{a_n} =  - n + 1$, the sum of the $n$ columns of $M$ is zero, hence its columns are linear dependent, we have ${\text{rank }}M \leqslant n - 1$. However, note that the $(n - 1) \times (n - 1)$ matrix $${\left( {\begin{array}{*{20}{c}}   { - n + 1}&1& \cdots &1 \\    1&{ - n + 1}& \cdots &1 \\     \vdots &1& \ddots &1 \\    1&1&1&{ - n + 1}  \end{array}} \right)}$$ is a submatrix of $M$, and by formula for $n \times n$ matrix with $x$ on diagonal and $y$ elsewhere, $$\left| {\begin{array}{*{20}{c}} x&y& \cdots &y\\ y&x& \cdots &y\\  \vdots &y& \ddots &y\\ y&y&y&x \end{array}} \right| = {(x - y)^{n - 1}}\left[ {x + (n - 1)y} \right]$$ this submatrix is invertible and thus has rank $n-1$. Hence the original $M$ satisfies ${\text{rank }}M \geqslant n - 1$ However, I can't prove or disprove the existence of other values of rank. Even the simplest case when $M$ is a $4! \times 4$, I cannot assert or rule out the existence of $M$ such that ${\text{rank }}M = 2$ There is a similar question in Stackexchange, but it only tells the facts that I had already knew, and does not give any insight. This is a question from Artin's Algebra so I think this is not very hard. Any help will be greatly appreciated.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'permutations']"
19,Exponential map for the Lie group of upper triangular matrices,Exponential map for the Lie group of upper triangular matrices,,Let $G$ be the Lie group of all upper triangular real matrices (over $\mathbb{R}$) with positive diagonal elements . Denote $\mathfrak{g}$ its Lie algebra. Do we have surjectivity of $\exp : \mathfrak{g}\to G$ ? and what about injectivity? Is $\mathfrak{g}$ equal to the algebra of all upper triangular matrices?,Let $G$ be the Lie group of all upper triangular real matrices (over $\mathbb{R}$) with positive diagonal elements . Denote $\mathfrak{g}$ its Lie algebra. Do we have surjectivity of $\exp : \mathfrak{g}\to G$ ? and what about injectivity? Is $\mathfrak{g}$ equal to the algebra of all upper triangular matrices?,,"['linear-algebra', 'lie-groups', 'lie-algebras']"
20,Is there a way to update the inverse of a sum of two matrices following a rescaling of one of them?,Is there a way to update the inverse of a sum of two matrices following a rescaling of one of them?,,"Suppose I have two matrices $A$ and $B$ (let's assume that both $A$ and $B$ are invertible, as is their sum), and a scalar $g$. I am interested in the matrix $$M^{-1} = (A + gB)^{-1}$$ I am aware of various expressions for computing this inverse in general , but I am interested in whether, if I calculate $M^{-1}$ for some value of $g$, is there a way to quickly update $M^{-1}$ following an update to the value of $g$? I am specifically interested in whether this can be done without performing any additional inversions after updating $g$, i.e. if I can just store $A$, $B$, $A^{-1}$, $B^{-1}$ (or some factorizations of them) and the previous value of $g$ in memory, and then somehow update $M^{-1}$ as a function of these? I've just found this , which suggests a solution if $B=I$, but I fear I may be out of luck for the more general case where $B\neq I$. I would also potentially be interested in solutions which rely on sparsity of either $A$ or $B$, as I may have some cases in which this is true.","Suppose I have two matrices $A$ and $B$ (let's assume that both $A$ and $B$ are invertible, as is their sum), and a scalar $g$. I am interested in the matrix $$M^{-1} = (A + gB)^{-1}$$ I am aware of various expressions for computing this inverse in general , but I am interested in whether, if I calculate $M^{-1}$ for some value of $g$, is there a way to quickly update $M^{-1}$ following an update to the value of $g$? I am specifically interested in whether this can be done without performing any additional inversions after updating $g$, i.e. if I can just store $A$, $B$, $A^{-1}$, $B^{-1}$ (or some factorizations of them) and the previous value of $g$ in memory, and then somehow update $M^{-1}$ as a function of these? I've just found this , which suggests a solution if $B=I$, but I fear I may be out of luck for the more general case where $B\neq I$. I would also potentially be interested in solutions which rely on sparsity of either $A$ or $B$, as I may have some cases in which this is true.",,"['linear-algebra', 'matrices', 'inverse']"
21,$A^2$ $B=A^2-B$ then $AB=BA$,then,A^2 B=A^2-B AB=BA,"If for $2$ real $n$ by $n$ matrices we have $A^2B=A^2-B$ then prove that the two matrices commute. This is a problem from a competition. I've tried several manipulations but none of them work. Can't come up with a counter example, as well.","If for $2$ real $n$ by $n$ matrices we have $A^2B=A^2-B$ then prove that the two matrices commute. This is a problem from a competition. I've tried several manipulations but none of them work. Can't come up with a counter example, as well.",,"['linear-algebra', 'contest-math', 'recreational-mathematics']"
22,Uniqueness of Smith normal form in Z (ring of integers),Uniqueness of Smith normal form in Z (ring of integers),,"It is a very well known fact that Smith Normal Form has proven useful when dealing with the development of the structure theorem of finitely generated abelian groups. In this context, there is an approach that takes advantage of the next result, which indeed is a very particular case of a much more general theorem related with a special kind of rings. If $A$ is a $m\times n$ matrix with integer coefficients, then there exist two matrices $P$ of size $m\times m$ and $Q$ of size $n\times n$, both having integer entries and $\det =\pm 1$, such that $PAQ$ is a diagonal matrix with diagonal entries $d_1,d_2,\ldots,d_k$ ($k<\min(m,n)$) such that $d_1\mid d_2\mid \ldots\mid d_k$, and each $d_i$ is a positive integer. Furthermore, $d_1\mid \ldots\mid d_k$ are unique. I have no problem with the proof of the ""existence"" part of the last theorem. However, I can't manage to give a proof of the ""uniqueness"" part; at most, I can only show that if $d'_1\mid\ldots\mid d'_{\ell}$ have the same property, then $d'_1 = d_1$ (they are both the gcd of the entries of $A$). The idea is not to give a proof using structure theorems, but only any kind of ""very elemental"" proof (dealing, if possible, just with $\mathbb{Z}$ properties, and not talking about general rings/modules).","It is a very well known fact that Smith Normal Form has proven useful when dealing with the development of the structure theorem of finitely generated abelian groups. In this context, there is an approach that takes advantage of the next result, which indeed is a very particular case of a much more general theorem related with a special kind of rings. If $A$ is a $m\times n$ matrix with integer coefficients, then there exist two matrices $P$ of size $m\times m$ and $Q$ of size $n\times n$, both having integer entries and $\det =\pm 1$, such that $PAQ$ is a diagonal matrix with diagonal entries $d_1,d_2,\ldots,d_k$ ($k<\min(m,n)$) such that $d_1\mid d_2\mid \ldots\mid d_k$, and each $d_i$ is a positive integer. Furthermore, $d_1\mid \ldots\mid d_k$ are unique. I have no problem with the proof of the ""existence"" part of the last theorem. However, I can't manage to give a proof of the ""uniqueness"" part; at most, I can only show that if $d'_1\mid\ldots\mid d'_{\ell}$ have the same property, then $d'_1 = d_1$ (they are both the gcd of the entries of $A$). The idea is not to give a proof using structure theorems, but only any kind of ""very elemental"" proof (dealing, if possible, just with $\mathbb{Z}$ properties, and not talking about general rings/modules).",,"['linear-algebra', 'abstract-algebra']"
23,Linear functions and intersections of null subspaces,Linear functions and intersections of null subspaces,,"Let $V$ be a vector space of a finite dimension $n$ over the field $K$. Let $\phi, \psi$ be two non-zero functionals on $V$. Assume that there is no non-zero element $c \in K$ such that $\psi= c \phi$. Show that $\ker \phi \cap \ker \psi$ has dimension $n -2$. There is a coordinate-based argument I know, which I am not writing out here, but I do not like these as they are a little hard to generalize to other situations. If you have a nicer coordinate free argument you may share it. Here is my coordinate-free argument: Define $f: V \rightarrow K \times K: f(v) = (\phi (v), \psi (v))$. Then $f$ is non-zero since both $\phi$ and $\psi$ are not and also the dimension of $f(V)$ cannot be $1$ since there exists no non-zero $c$ such that $\psi = c\phi$. Hence, $\dim f(V) = 2$ and $ \ker f = \ker \phi \cap \ker \psi $ so that by the Rank-Nullity Theorem we get the result.","Let $V$ be a vector space of a finite dimension $n$ over the field $K$. Let $\phi, \psi$ be two non-zero functionals on $V$. Assume that there is no non-zero element $c \in K$ such that $\psi= c \phi$. Show that $\ker \phi \cap \ker \psi$ has dimension $n -2$. There is a coordinate-based argument I know, which I am not writing out here, but I do not like these as they are a little hard to generalize to other situations. If you have a nicer coordinate free argument you may share it. Here is my coordinate-free argument: Define $f: V \rightarrow K \times K: f(v) = (\phi (v), \psi (v))$. Then $f$ is non-zero since both $\phi$ and $\psi$ are not and also the dimension of $f(V)$ cannot be $1$ since there exists no non-zero $c$ such that $\psi = c\phi$. Hence, $\dim f(V) = 2$ and $ \ker f = \ker \phi \cap \ker \psi $ so that by the Rank-Nullity Theorem we get the result.",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'matrix-rank', 'duality-theorems']"
24,A is a positive definite matrix iff its leading principal minors are positive,A is a positive definite matrix iff its leading principal minors are positive,,"I am to prove that the a symmetric matrix $A$ is positive definite iff the leading principal minors of $A$ are positive. The forward implication is clear. Since the eigenvalues of a SPD matrix are positive and real, and $\det(A)$ is the product of eigenvalues, it must follow that these are positive too. However, can someone please help me with the backwards implication? I just don't know how to handle it.","I am to prove that the a symmetric matrix $A$ is positive definite iff the leading principal minors of $A$ are positive. The forward implication is clear. Since the eigenvalues of a SPD matrix are positive and real, and $\det(A)$ is the product of eigenvalues, it must follow that these are positive too. However, can someone please help me with the backwards implication? I just don't know how to handle it.",,"['linear-algebra', 'matrices', 'determinant', 'positive-definite']"
25,Which graphs do have invertible adjacency matrices?,Which graphs do have invertible adjacency matrices?,,I would like to know if there is any class of graphs for which the adjacency matrices are invertible. At this moment I am aware of o nly the class of graphs $n K_2$ which is the disjoint union of $n$ number of $K_2$ matrices where the adjacency matrices are self-inverse. Are there any other classes?,I would like to know if there is any class of graphs for which the adjacency matrices are invertible. At this moment I am aware of o nly the class of graphs $n K_2$ which is the disjoint union of $n$ number of $K_2$ matrices where the adjacency matrices are self-inverse. Are there any other classes?,,"['linear-algebra', 'matrices', 'graph-theory', 'inverse', 'spectral-graph-theory']"
26,Algebra (Matrix Theory) Linear Maps,Algebra (Matrix Theory) Linear Maps,,"I had a problem in my booked i tried to prove. Here is the problem ""Let $x_1,\ldots,x_n$ be different real numbers and $y_1,\ldots,y_n,s_1,\ldots,s_n$ some real numbers. Prove that there exists a polynomial $p(x)$ of a degree less than $2n$ such that $p(x_i)=y_i$ and $p'(x_i)=s_i$ for every $i=1,2,\ldots,n.$"" Here is my attempt: Let $V= \{ p(x)\in \mathbb{R}[x] \mid \deg(p(x))<2n\}$ $\Rightarrow$ $\exists$ $h(x)\in V$ s.t $\deg(h(x))<n<2n$ from here we now can apply Lagrange Interpolation theorem. $\Rightarrow$ $h(x_i)=y_i$. Let $Q= \{ g(x)\in \mathbb{R}[x] \mid g(x)=p'(x),\ p(x) \in V, \deg(p(x)) < n \}$ where $\dim(Q)=n-1$. Now let $g(x)\in Q \Rightarrow \deg(g(x))<n-1$ Let $A: Q\rightarrow \mathbb{R}^n$ $g(x) \rightarrow (g(x_1),\ldots,g(x_n))$ Now assume $g(x)\in \ker(A) \Rightarrow g(x_i) = 0$, $n$ zeros $\Rightarrow \ g(x)=0 \ \Rightarrow \ker(A) = \{0\} \Rightarrow A$ injective $+$surjective $\Rightarrow g(x_i)=s_i$ But since $\deg(h(x))<n \Rightarrow h'(x) \in Q \Rightarrow g(x)=h'(x) \Rightarrow g(x_i)=h'(x_i)=s_i$ My Professor had i quick look and stated that this was a good attempt but it was not correct. He said that my proof does not with certainty show that this $h'(x_i)=s_i$ ill get in the end will fullfill $h(x_i)=y_i$ I didnt understand him, because in my opinion im sure that the derivitive of $h(x)$ which fullfill $h(x_i)=y_i$ lies in $Q$ and therefor i can let my $g(x)$ be equal to $h'(x)$ I would be grateful if someone can explain this?","I had a problem in my booked i tried to prove. Here is the problem ""Let $x_1,\ldots,x_n$ be different real numbers and $y_1,\ldots,y_n,s_1,\ldots,s_n$ some real numbers. Prove that there exists a polynomial $p(x)$ of a degree less than $2n$ such that $p(x_i)=y_i$ and $p'(x_i)=s_i$ for every $i=1,2,\ldots,n.$"" Here is my attempt: Let $V= \{ p(x)\in \mathbb{R}[x] \mid \deg(p(x))<2n\}$ $\Rightarrow$ $\exists$ $h(x)\in V$ s.t $\deg(h(x))<n<2n$ from here we now can apply Lagrange Interpolation theorem. $\Rightarrow$ $h(x_i)=y_i$. Let $Q= \{ g(x)\in \mathbb{R}[x] \mid g(x)=p'(x),\ p(x) \in V, \deg(p(x)) < n \}$ where $\dim(Q)=n-1$. Now let $g(x)\in Q \Rightarrow \deg(g(x))<n-1$ Let $A: Q\rightarrow \mathbb{R}^n$ $g(x) \rightarrow (g(x_1),\ldots,g(x_n))$ Now assume $g(x)\in \ker(A) \Rightarrow g(x_i) = 0$, $n$ zeros $\Rightarrow \ g(x)=0 \ \Rightarrow \ker(A) = \{0\} \Rightarrow A$ injective $+$surjective $\Rightarrow g(x_i)=s_i$ But since $\deg(h(x))<n \Rightarrow h'(x) \in Q \Rightarrow g(x)=h'(x) \Rightarrow g(x_i)=h'(x_i)=s_i$ My Professor had i quick look and stated that this was a good attempt but it was not correct. He said that my proof does not with certainty show that this $h'(x_i)=s_i$ ill get in the end will fullfill $h(x_i)=y_i$ I didnt understand him, because in my opinion im sure that the derivitive of $h(x)$ which fullfill $h(x_i)=y_i$ lies in $Q$ and therefor i can let my $g(x)$ be equal to $h'(x)$ I would be grateful if someone can explain this?",,"['linear-algebra', 'polynomials', 'numerical-methods']"
27,Prove that the sum of all of AB's $r$-th principal minors is equal to that of those of BA's,Prove that the sum of all of AB's -th principal minors is equal to that of those of BA's,r,"Said more specifically, suppose $A,B\in M_n(K)$, $K$ a commutative ring. An $r$-th principal minor of a square matrix is the determinant $$\det\begin{bmatrix}a_{k_1k_1} & \cdots & a_{k_1k_r}\\ \vdots & \ddots & \vdots \\ a_{k_rk_1} & \cdots & a_{k_rk_r}\end{bmatrix}$$ where $1\le k_1<\cdots<k_r\le n$, $A$ is an $n\times n, n>r$ square matrix. Prove that the sum of all of $AB$'s $r$-th principal minors is equal to that of those of $BA$'s. Hint: use Cauchy-Binet formula . I simply have no idea what use to be made of the hint. I just can't represent the terms in the way Cauchy-Binet formula does , anyway for me it seems impossible to construct any of $P_r(AB)$ from sub-blocks of $A,B$. Any help? Edit: I'm sorry I made a mistake. It should be principal minor , no leading .","Said more specifically, suppose $A,B\in M_n(K)$, $K$ a commutative ring. An $r$-th principal minor of a square matrix is the determinant $$\det\begin{bmatrix}a_{k_1k_1} & \cdots & a_{k_1k_r}\\ \vdots & \ddots & \vdots \\ a_{k_rk_1} & \cdots & a_{k_rk_r}\end{bmatrix}$$ where $1\le k_1<\cdots<k_r\le n$, $A$ is an $n\times n, n>r$ square matrix. Prove that the sum of all of $AB$'s $r$-th principal minors is equal to that of those of $BA$'s. Hint: use Cauchy-Binet formula . I simply have no idea what use to be made of the hint. I just can't represent the terms in the way Cauchy-Binet formula does , anyway for me it seems impossible to construct any of $P_r(AB)$ from sub-blocks of $A,B$. Any help? Edit: I'm sorry I made a mistake. It should be principal minor , no leading .",,"['linear-algebra', 'matrices', 'determinant']"
28,"Verification for a block-determinant evaluation, and some further thoughts","Verification for a block-determinant evaluation, and some further thoughts",,"First, I want some verification for the validity of my approach for this det evaluation question: If $A,B\in M_n(K)$, $K$ is a number field (in the sense that $\Bbb Q$ is the smallest possible one), and $AB=BA$, prove this determinat equality   $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix} =|A^2+B^2| $$ My approach: Case 1: when $A=(a_{ij})$ is non-singular Consider $$ \begin{bmatrix}   I & O \\ -B & A \end{bmatrix} \begin{bmatrix}   A & -B \\ B & A \end{bmatrix}= \begin{bmatrix}   A & -B \\ O & A^2+B^2 \end{bmatrix} $$ therefore $$ \det \begin{bmatrix}   I & O \\ -B & A \end{bmatrix} \det \begin{bmatrix}   A & -B \\ B & A \end{bmatrix}= \det \begin{bmatrix}   A & -B \\ O & A^2+B^2 \end{bmatrix} $$ by Laplace expansion, it is clear that $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix}=\frac1{|A|}\cdot|A|\cdot |A^2+B^2|=|A^2+B^2| $$ Case 2: when $A$ is singular Consider $\require{cancel} A(t)\xcancel{=(a_{ij}+t)}=A+It$, since $A$ is singular, we have $A(0)=0$. That's to say, $0$ is a root of $A(t)$, which we regard now as a (non-zero) polynomial w.r.t. the parameter $t$. Since any non-zero polynomial has only finitely many roots in $\Bbb C$, it is clear that there exists a positive $\delta$ such that $t\in (-\delta,0)\cup(0,\delta)$ implies $A(t)\ne 0$, otherwise there would be an infinite sequence consisting of $A(t)$'s roots that converges to $0$. So if we pick any $t\in (-\delta,0)\cup(0,\delta)$ and replace $A$ by $A(t)$ in case 1, we'll obtain $$ \begin{vmatrix} A(t) & -B \\ B & A(t) \end{vmatrix}=|A^2(t)+B^2| $$ However, this is also a real polynomial function and therefore continuous at $t=0$, hence $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix}= \lim_{t\to 0}\begin{vmatrix} A(t) & -B \\ B & A(t) \end{vmatrix}= |A^2(0)+B^2|=|A^2+B^2| $$ Second, if my previous deduction is sound , will it be also proper to extend similar convert-singular-to-non-singular tricks elsewhere (except when the underlying field $K$ is not a number field in the usual sense, say, $K$ is a finite field)? For instance, there is an exercise marked as ""difficult"" in my textbook For $n\times n$ matrices $A,B$, prove   $$(AB)^*=B^*A^*$$   $(A^*)$ denotes the adjoint matrix (transpose of the cofactor matrix) of $A$. I know that it is marked as ""difficult"" because of the possibility that $A$ or $B$ can be singular, for if they are both non-singular, the proof will be extremely easy. However, if I apply my previous tricks here -- like making some $A(u),B(v)$ stuff, and noticing that every entry of $(AB)^*$ is continuously dependent on all the entries of $A(u),B(v)$, which are respectively continuously dependent on $u$ and $v$ due to the way I construct $A(u),B(v)$ -- will it not be extremely easy to extend the ""non-singular"" case to the ""singular"" case and thus complete the whole proof? Further yet, if we have successfully proven a matrix/det equality for a bunch of non-singular matrices like $A,B,C\cdots$, and this equality only involves things (like $\det$, but not $\text{rank}$ of course) that are continuously dependent on each entry of these matrices, will it be natural to extend the result to all cases, whether the involved matrices are singular or not? Any rectification/inspiration/clarification on my thoughts is welcomed. Best regards! EIDT @user1551 has pointed out that in my approach if $A(t)\equiv 0$ there'd be a fallacy, and has suggested that it can be fixed if I replace my $A(t)$ by $A+It$, which will in no case be a 0 polynomial. EDIT Or, is there any such matrix equations (both sides only include matrix addition and multiplication but not there inverses)  where invertibility makes all the difference?","First, I want some verification for the validity of my approach for this det evaluation question: If $A,B\in M_n(K)$, $K$ is a number field (in the sense that $\Bbb Q$ is the smallest possible one), and $AB=BA$, prove this determinat equality   $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix} =|A^2+B^2| $$ My approach: Case 1: when $A=(a_{ij})$ is non-singular Consider $$ \begin{bmatrix}   I & O \\ -B & A \end{bmatrix} \begin{bmatrix}   A & -B \\ B & A \end{bmatrix}= \begin{bmatrix}   A & -B \\ O & A^2+B^2 \end{bmatrix} $$ therefore $$ \det \begin{bmatrix}   I & O \\ -B & A \end{bmatrix} \det \begin{bmatrix}   A & -B \\ B & A \end{bmatrix}= \det \begin{bmatrix}   A & -B \\ O & A^2+B^2 \end{bmatrix} $$ by Laplace expansion, it is clear that $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix}=\frac1{|A|}\cdot|A|\cdot |A^2+B^2|=|A^2+B^2| $$ Case 2: when $A$ is singular Consider $\require{cancel} A(t)\xcancel{=(a_{ij}+t)}=A+It$, since $A$ is singular, we have $A(0)=0$. That's to say, $0$ is a root of $A(t)$, which we regard now as a (non-zero) polynomial w.r.t. the parameter $t$. Since any non-zero polynomial has only finitely many roots in $\Bbb C$, it is clear that there exists a positive $\delta$ such that $t\in (-\delta,0)\cup(0,\delta)$ implies $A(t)\ne 0$, otherwise there would be an infinite sequence consisting of $A(t)$'s roots that converges to $0$. So if we pick any $t\in (-\delta,0)\cup(0,\delta)$ and replace $A$ by $A(t)$ in case 1, we'll obtain $$ \begin{vmatrix} A(t) & -B \\ B & A(t) \end{vmatrix}=|A^2(t)+B^2| $$ However, this is also a real polynomial function and therefore continuous at $t=0$, hence $$ \begin{vmatrix} A & -B \\ B & A \end{vmatrix}= \lim_{t\to 0}\begin{vmatrix} A(t) & -B \\ B & A(t) \end{vmatrix}= |A^2(0)+B^2|=|A^2+B^2| $$ Second, if my previous deduction is sound , will it be also proper to extend similar convert-singular-to-non-singular tricks elsewhere (except when the underlying field $K$ is not a number field in the usual sense, say, $K$ is a finite field)? For instance, there is an exercise marked as ""difficult"" in my textbook For $n\times n$ matrices $A,B$, prove   $$(AB)^*=B^*A^*$$   $(A^*)$ denotes the adjoint matrix (transpose of the cofactor matrix) of $A$. I know that it is marked as ""difficult"" because of the possibility that $A$ or $B$ can be singular, for if they are both non-singular, the proof will be extremely easy. However, if I apply my previous tricks here -- like making some $A(u),B(v)$ stuff, and noticing that every entry of $(AB)^*$ is continuously dependent on all the entries of $A(u),B(v)$, which are respectively continuously dependent on $u$ and $v$ due to the way I construct $A(u),B(v)$ -- will it not be extremely easy to extend the ""non-singular"" case to the ""singular"" case and thus complete the whole proof? Further yet, if we have successfully proven a matrix/det equality for a bunch of non-singular matrices like $A,B,C\cdots$, and this equality only involves things (like $\det$, but not $\text{rank}$ of course) that are continuously dependent on each entry of these matrices, will it be natural to extend the result to all cases, whether the involved matrices are singular or not? Any rectification/inspiration/clarification on my thoughts is welcomed. Best regards! EIDT @user1551 has pointed out that in my approach if $A(t)\equiv 0$ there'd be a fallacy, and has suggested that it can be fixed if I replace my $A(t)$ by $A+It$, which will in no case be a 0 polynomial. EDIT Or, is there any such matrix equations (both sides only include matrix addition and multiplication but not there inverses)  where invertibility makes all the difference?",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
29,Finding an explicit eigenvector,Finding an explicit eigenvector,,"Let $A$ be an $n\times n$ matrix over a field and let $\operatorname{adj}(A)$ denote its classical adjoint. Suppose all column sums of $A$ are zero so that $A$ is singular. If $\operatorname{rank}(A) = n-1$, the adjoint is nonzero. Hence any nonzero column of $\operatorname{adj}(A)$ provides an explicit (right) eigenvector of $A$ in the null space because $A\operatorname{adj}(A)=0$. Unfortunately, if $\operatorname{rank}(A)\le n-2$, the adjoint is identically zero so this fails to yield an eigenvector. Here is the question: Is there a way to modify the above to get an explicit eigenvector in the null space when $\operatorname{rank}(A)\le n-2$? (What is desired is a formula rather than an algorithm.) Thank you","Let $A$ be an $n\times n$ matrix over a field and let $\operatorname{adj}(A)$ denote its classical adjoint. Suppose all column sums of $A$ are zero so that $A$ is singular. If $\operatorname{rank}(A) = n-1$, the adjoint is nonzero. Hence any nonzero column of $\operatorname{adj}(A)$ provides an explicit (right) eigenvector of $A$ in the null space because $A\operatorname{adj}(A)=0$. Unfortunately, if $\operatorname{rank}(A)\le n-2$, the adjoint is identically zero so this fails to yield an eigenvector. Here is the question: Is there a way to modify the above to get an explicit eigenvector in the null space when $\operatorname{rank}(A)\le n-2$? (What is desired is a formula rather than an algorithm.) Thank you",,"['linear-algebra', 'matrices']"
30,Is there any interesting relationship between a Hermitian matrix and its corresponding entrywise absolute?,Is there any interesting relationship between a Hermitian matrix and its corresponding entrywise absolute?,,"In general, a Hermitian matrix can have complex off-diagonal terms. Given any Hermitian matrix $[A]_{n,m}$, I can construct another matrix $[\vert A\vert ]_{n,m} =\vert A_{n,m} \vert$. I would like to know if there are any known relationships between two such matrices. Some more specific questions that I was thinking about but don't have an answer to are the following: Is it possible to transform using a unitary $UAU^{\dagger}$ or using a convex combination of unitary matrices $\sum_i p_i U_iAU_i^{\dagger}$ where $\sum_i p_i =1$? It seems to be possible for $2 \times 2$ positive semi-definite matrices. Are there any inequalities relative their trace norms $\operatorname{tr}(\sqrt{A^\dagger A})$ and $\operatorname{tr}(\sqrt{\vert A \vert^\dagger \vert A \vert})$? Such as if one is always bigger than the other. Any information will be helpful for general Hermitian matrices will be helpful, but if you would like to know, I am looking at two special cases matrices right now: a) $A$ is a positive definite matrix. b) $A$ is a hollow (zero main diagonal), banded, Toeplitz and Hermitian matrix  that looks like the following: \begin{bmatrix} 0 & a_1 & a_2 & \ldots & a_n \\ a_1^* & 0 & a_1 &\ldots & \vdots \\ a_2^* &a_1^* & 0 & \ldots &\vdots \\  \vdots & & & \ddots & \vdots\\ a_n^* & a_{n-1}^* & a_{n-2}^* &\ldots &0 \end{bmatrix} Thanks for all your help.","In general, a Hermitian matrix can have complex off-diagonal terms. Given any Hermitian matrix $[A]_{n,m}$, I can construct another matrix $[\vert A\vert ]_{n,m} =\vert A_{n,m} \vert$. I would like to know if there are any known relationships between two such matrices. Some more specific questions that I was thinking about but don't have an answer to are the following: Is it possible to transform using a unitary $UAU^{\dagger}$ or using a convex combination of unitary matrices $\sum_i p_i U_iAU_i^{\dagger}$ where $\sum_i p_i =1$? It seems to be possible for $2 \times 2$ positive semi-definite matrices. Are there any inequalities relative their trace norms $\operatorname{tr}(\sqrt{A^\dagger A})$ and $\operatorname{tr}(\sqrt{\vert A \vert^\dagger \vert A \vert})$? Such as if one is always bigger than the other. Any information will be helpful for general Hermitian matrices will be helpful, but if you would like to know, I am looking at two special cases matrices right now: a) $A$ is a positive definite matrix. b) $A$ is a hollow (zero main diagonal), banded, Toeplitz and Hermitian matrix  that looks like the following: \begin{bmatrix} 0 & a_1 & a_2 & \ldots & a_n \\ a_1^* & 0 & a_1 &\ldots & \vdots \\ a_2^* &a_1^* & 0 & \ldots &\vdots \\  \vdots & & & \ddots & \vdots\\ a_n^* & a_{n-1}^* & a_{n-2}^* &\ldots &0 \end{bmatrix} Thanks for all your help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'normed-spaces']"
31,A question about polynomial in two variables [duplicate],A question about polynomial in two variables [duplicate],,"This question already has answers here : Real polynomial in two variables (3 answers) Closed 8 years ago . Let $f:\mathbb R^2 \to \mathbb R$ be a function such that for any $b \in \mathbb R$ , the function $f_b : \mathbb R \to \mathbb R$ defined as $f_b(x):=f(x,b) , \forall x \in \mathbb R$ , is a polynomial in $x$ and for any $a \in \mathbb R$ , the function $f_a : \mathbb R \to \mathbb R$ defined as $f_a(y):=f(a,y) , \forall y \in \mathbb R$ , is a polynomial in $y$ . Then is the function $f(x,y)$ a polynomial in $x,y$ i.e. is $f \in \mathbb R [x,y]$ ?","This question already has answers here : Real polynomial in two variables (3 answers) Closed 8 years ago . Let $f:\mathbb R^2 \to \mathbb R$ be a function such that for any $b \in \mathbb R$ , the function $f_b : \mathbb R \to \mathbb R$ defined as $f_b(x):=f(x,b) , \forall x \in \mathbb R$ , is a polynomial in $x$ and for any $a \in \mathbb R$ , the function $f_a : \mathbb R \to \mathbb R$ defined as $f_a(y):=f(a,y) , \forall y \in \mathbb R$ , is a polynomial in $y$ . Then is the function $f(x,y)$ a polynomial in $x,y$ i.e. is $f \in \mathbb R [x,y]$ ?",,['linear-algebra']
32,Determinant of a Certain Block Structured Positive Definite Matrix,Determinant of a Certain Block Structured Positive Definite Matrix,,PLEASE FIND THE EDITED VERSION OF THIS QUESTION HERE: Asymptotic behavior of the minimum eigenvalue of a certain Gram matrix with linear independence I WILL ALSO PUT UP A BOUNTY FOR THE EDITED VERSION. Is there a lower bound for the determinant or minimum eigenvalue of the following $d$ by $d$ matrix in terms of $d$? $$\Gamma=\left( {\begin{array}{cc} 		I & B \\ 		B^{*} & I \\ 		\end{array} } \right)$$ Where $I$ is the identity matrix and the the moduli of entries of $B$ and those of its conjugate $B^{*}$ are all equal to $\frac{1}{\sqrt{d}}$. Also the blocks are all $\frac{d}{2}$by$\frac{d}{2}$. It is a Gram matrix and further assume that the rows and columns are linearly independent. Hence we know that the lower bound is larger than zero but can we say anything more? For simplicity we can assume the field of the matrix is real. Hence the entries of the off-diagonal blocks ($B$ and $B^{T}$) are $\pm\frac{1}{\sqrt{d}}$. I appreciate any input very much!,PLEASE FIND THE EDITED VERSION OF THIS QUESTION HERE: Asymptotic behavior of the minimum eigenvalue of a certain Gram matrix with linear independence I WILL ALSO PUT UP A BOUNTY FOR THE EDITED VERSION. Is there a lower bound for the determinant or minimum eigenvalue of the following $d$ by $d$ matrix in terms of $d$? $$\Gamma=\left( {\begin{array}{cc} 		I & B \\ 		B^{*} & I \\ 		\end{array} } \right)$$ Where $I$ is the identity matrix and the the moduli of entries of $B$ and those of its conjugate $B^{*}$ are all equal to $\frac{1}{\sqrt{d}}$. Also the blocks are all $\frac{d}{2}$by$\frac{d}{2}$. It is a Gram matrix and further assume that the rows and columns are linearly independent. Hence we know that the lower bound is larger than zero but can we say anything more? For simplicity we can assume the field of the matrix is real. Hence the entries of the off-diagonal blocks ($B$ and $B^{T}$) are $\pm\frac{1}{\sqrt{d}}$. I appreciate any input very much!,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
33,"Order $n^2$ different reals, such that they form a $\mathbb{R^n}$ basis","Order  different reals, such that they form a  basis",n^2 \mathbb{R^n},"I've been trying to solve this linear algebra problem: You are given $n^2 > 1$ pairwise different real numbers. Show that it's always possible to construct with them a basis for $\mathbb{R^n}$. The problem seems to be intuitive enough, but I couldn't come up with a solution. I tried using the Leibniz formula for determinants, but I can't argument why I should be able to always arrange the numbers in such a way so that $det \ne 0$. I also thought about first ordering the $n^2$ numbers, and then filling up a $n \times n$ matrix in a specific pattern, but I also couldn't close that argument. Anyway, any help in the right direction would be appreciated :)!","I've been trying to solve this linear algebra problem: You are given $n^2 > 1$ pairwise different real numbers. Show that it's always possible to construct with them a basis for $\mathbb{R^n}$. The problem seems to be intuitive enough, but I couldn't come up with a solution. I tried using the Leibniz formula for determinants, but I can't argument why I should be able to always arrange the numbers in such a way so that $det \ne 0$. I also thought about first ordering the $n^2$ numbers, and then filling up a $n \times n$ matrix in a specific pattern, but I also couldn't close that argument. Anyway, any help in the right direction would be appreciated :)!",,"['linear-algebra', 'matrices', 'determinant']"
34,"Matrices over $\mathbb{Q}[x,y,z]$ which are not equivalent",Matrices over  which are not equivalent,"\mathbb{Q}[x,y,z]","I have some problems solving the following task: Let $R = \mathbb{Q}[x,y,z]$ and:   $$A = \begin{pmatrix} x & y \\ 0 & z \end{pmatrix} \in M_{2,2}(R) \qquad B = \begin{pmatrix} x & 0 \\ y & z \end{pmatrix} \in M_{2,2}(R).$$   Show that $A$ is not equivalent to $B$, that is, there are no invertible matrices $C,D \in M_{2,2}(R)$ such that $B=CAD$. I've already tried substituting $C = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ and the same for $D$ and then drawing conclusions from the products of the matrices. But I wasn't able to find something helpful.","I have some problems solving the following task: Let $R = \mathbb{Q}[x,y,z]$ and:   $$A = \begin{pmatrix} x & y \\ 0 & z \end{pmatrix} \in M_{2,2}(R) \qquad B = \begin{pmatrix} x & 0 \\ y & z \end{pmatrix} \in M_{2,2}(R).$$   Show that $A$ is not equivalent to $B$, that is, there are no invertible matrices $C,D \in M_{2,2}(R)$ such that $B=CAD$. I've already tried substituting $C = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ and the same for $D$ and then drawing conclusions from the products of the matrices. But I wasn't able to find something helpful.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
35,Are symmetric matrices necessarily positive-definite / positive semi-definite?,Are symmetric matrices necessarily positive-definite / positive semi-definite?,,"I am trying to prove this just to be clear about this but I don't have enough conditions to force this idea to be true, so I doubt it is. Are symmetric matrices always at least positive semi-definite? I know the other way around, by convention, positive-definite / positive semi-definite matrices are always symmetric. I currently have on my piece of paper a 1x1 block, after using the definition of positive-definiteness, that consists of a polynomial that is quadratic in the variables x, y, z, which are the components of my chosen vector.  But the coefficients attached are coming from the symmetric matrix, and there's no reason why a symmetric matrix needs to have all positive entries. Thanks,","I am trying to prove this just to be clear about this but I don't have enough conditions to force this idea to be true, so I doubt it is. Are symmetric matrices always at least positive semi-definite? I know the other way around, by convention, positive-definite / positive semi-definite matrices are always symmetric. I currently have on my piece of paper a 1x1 block, after using the definition of positive-definiteness, that consists of a polynomial that is quadratic in the variables x, y, z, which are the components of my chosen vector.  But the coefficients attached are coming from the symmetric matrix, and there's no reason why a symmetric matrix needs to have all positive entries. Thanks,",,"['linear-algebra', 'operator-theory', 'symmetry']"
36,Prove: $(\det(A-B)+\det(A+B) )^2 \ge 4\det(A^2-B^2 )$,Prove:,(\det(A-B)+\det(A+B) )^2 \ge 4\det(A^2-B^2 ),"Let $A,B \in  \mathcal{M}_n (\mathbb{R})$ two matrices so that: a) $AB^2=B^2 A$ and $BA^2=A^2 B$ b) $\text{rank}(AB-BA)=1$. Prove: $$(\det(A-B)+\det(A+B) )^2≥4\det(A^2-B^2 )$$ This is a solution: Denote $C=AB-BA$. Then $rank(C)=1$ so $C=p\cdot q^T$ where $p,q$ are   column vectors and $tr(C)=0$. This proves that    $C^2=p\cdot q^T\cdot p\cdot q^T=tr(C)C=0.$ If $D=A^2-B^2$ then a) implies that $CD=DC$ and $D$ commutes with   $A,B$. As a consequence $tr(CD^{-1})=0$. If $\det(D)=0$ we have nothing to prove. Else $D$ is invertible and   $(CD^{-1})^2=0$. We have $(A-B)(A+B)=A^2-B^2+AB-BA=D+C$. We would like to prove that   $\det(A-B)(B-A)=\det(A^2-B^2)$. For this define $ f(t)=\det(AB-BA+t(A^2-B^2))$ and see that $f(t)=\det(D)\det(CD^{-1}+tI)=\det(A^2-B^2)\cdot t^n$. Replace $t=1$ in the previous relation to get    $\det(AB-BA+A^2-B^2)=\det(A^2-B^2)$ and we are done. But i don't understand why  $tr(CD^{-1})=0$ ??","Let $A,B \in  \mathcal{M}_n (\mathbb{R})$ two matrices so that: a) $AB^2=B^2 A$ and $BA^2=A^2 B$ b) $\text{rank}(AB-BA)=1$. Prove: $$(\det(A-B)+\det(A+B) )^2≥4\det(A^2-B^2 )$$ This is a solution: Denote $C=AB-BA$. Then $rank(C)=1$ so $C=p\cdot q^T$ where $p,q$ are   column vectors and $tr(C)=0$. This proves that    $C^2=p\cdot q^T\cdot p\cdot q^T=tr(C)C=0.$ If $D=A^2-B^2$ then a) implies that $CD=DC$ and $D$ commutes with   $A,B$. As a consequence $tr(CD^{-1})=0$. If $\det(D)=0$ we have nothing to prove. Else $D$ is invertible and   $(CD^{-1})^2=0$. We have $(A-B)(A+B)=A^2-B^2+AB-BA=D+C$. We would like to prove that   $\det(A-B)(B-A)=\det(A^2-B^2)$. For this define $ f(t)=\det(AB-BA+t(A^2-B^2))$ and see that $f(t)=\det(D)\det(CD^{-1}+tI)=\det(A^2-B^2)\cdot t^n$. Replace $t=1$ in the previous relation to get    $\det(AB-BA+A^2-B^2)=\det(A^2-B^2)$ and we are done. But i don't understand why  $tr(CD^{-1})=0$ ??",,"['linear-algebra', 'matrices', 'determinant']"
37,Cycle structure of affine transformation,Cycle structure of affine transformation,,"Consider the ring $\mathbb{Z}_n$ of remainders modulo $n$ for some number $n.$ Let $a,b \in \mathbb{Z}_n$ and consider the map $$f_{a,b}(x) = ax+b.$$ If $a$ is invertible then the above map is bijective and it can therefore be viewed as a permutation. My question is Is it possible to determine the cycle structure of $f_{a,b}(x)$ as a permutation? If so, how?","Consider the ring $\mathbb{Z}_n$ of remainders modulo $n$ for some number $n.$ Let $a,b \in \mathbb{Z}_n$ and consider the map $$f_{a,b}(x) = ax+b.$$ If $a$ is invertible then the above map is bijective and it can therefore be viewed as a permutation. My question is Is it possible to determine the cycle structure of $f_{a,b}(x)$ as a permutation? If so, how?",,"['linear-algebra', 'abstract-algebra', 'permutations']"
38,Prove the distributive property of the dot product using its geometric definition?,Prove the distributive property of the dot product using its geometric definition?,,"The geometric definition of the dot product: $$\mathbf{a} \cdot \mathbf{b} = a \cdot b \cos \theta\qquad\forall\mathbf{a} , \mathbf{b} \in \mathbb{R} ^n.$$ The distributive property of the dot product: $$\mathbf{a} \cdot ( \mathbf{b} + \mathbf{c} ) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c} \qquad\forall\mathbf{a} , \mathbf{b},\mathbf{c}  \in \mathbb{R} ^n.$$ I don't understand how the concepts 'length' and 'angle' make sense in $n \geq 4$. They seem to be defined using the dot product. But we're defining the dot product with them so that's a circular definition. Maybe the geometric definition isn't valid for those higher dimensions. Anyway, I want to prove the distributive property of the dot product using its geometric definition for all $n$ or at least for $n = 3$ using basic things like geometry, trigonometric identities etc. I've proven it for $n = 2$ by defining the vectors $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$ as vectors that are of unit length and are perpendicular to each other. Then I assumed that all vectors in $\mathbb{R} ^2$ can be written as a linear combination of these unit vectors since you can reach anywhere on a plane by going forwards/backwards and left/right. And then I rewrote the scalars of the linear combinations in polar coordinates. And finally by using trigonometric identities the distributive property can then be demonstrated to be true. I'm not sure how to prove the case $n = 3$ since the geometry is more complicated. And I'm not even sure if geometry works for $n \geq 4$ as I've mentioned above. This would be a lot simpler using the algebraic definition of the dot product but the proof that the two definitions are equivalent on Wikipedia requires the distributive property.","The geometric definition of the dot product: $$\mathbf{a} \cdot \mathbf{b} = a \cdot b \cos \theta\qquad\forall\mathbf{a} , \mathbf{b} \in \mathbb{R} ^n.$$ The distributive property of the dot product: $$\mathbf{a} \cdot ( \mathbf{b} + \mathbf{c} ) = \mathbf{a} \cdot \mathbf{b} + \mathbf{a} \cdot \mathbf{c} \qquad\forall\mathbf{a} , \mathbf{b},\mathbf{c}  \in \mathbb{R} ^n.$$ I don't understand how the concepts 'length' and 'angle' make sense in $n \geq 4$. They seem to be defined using the dot product. But we're defining the dot product with them so that's a circular definition. Maybe the geometric definition isn't valid for those higher dimensions. Anyway, I want to prove the distributive property of the dot product using its geometric definition for all $n$ or at least for $n = 3$ using basic things like geometry, trigonometric identities etc. I've proven it for $n = 2$ by defining the vectors $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$ as vectors that are of unit length and are perpendicular to each other. Then I assumed that all vectors in $\mathbb{R} ^2$ can be written as a linear combination of these unit vectors since you can reach anywhere on a plane by going forwards/backwards and left/right. And then I rewrote the scalars of the linear combinations in polar coordinates. And finally by using trigonometric identities the distributive property can then be demonstrated to be true. I'm not sure how to prove the case $n = 3$ since the geometry is more complicated. And I'm not even sure if geometry works for $n \geq 4$ as I've mentioned above. This would be a lot simpler using the algebraic definition of the dot product but the proof that the two definitions are equivalent on Wikipedia requires the distributive property.",,"['linear-algebra', 'geometry']"
39,Three dimensional rotation of equations.,Three dimensional rotation of equations.,,"I have a set of equations that describe a wire in (100) direction. I want to rotate the wire such that it's in the direction (111). My initial plan (which failed) was to use Euler coordinates and first apply a rotation $R_y(\pi/4)$ and then $R_z(\pi/4)$ where the rotation matrices are defined as follows: $$\begin{alignat}{1} R_x(\theta) &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos \theta &  -\sin \theta \\[3pt] 0 & \sin \theta  &  \cos \theta \\[3pt] \end{bmatrix} \\[6pt] R_y(\theta) &= \begin{bmatrix} \cos \theta & 0 & \sin \theta \\[3pt] 0 & 1 & 0 \\[3pt] -\sin \theta & 0 & \cos \theta \\ \end{bmatrix} \\[6pt] R_z(\theta) &= \begin{bmatrix} \cos \theta &  -\sin \theta & 0 \\[3pt] \sin \theta & \cos \theta & 0\\[3pt] 0 & 0 & 1\\ \end{bmatrix} \end{alignat}$$ If I now peform the operation $R_z(\pi/4) R_y(\pi/4) [x, y, z]^T=\left[\begin{matrix}0.5 x - 0.5 y + 0.5\sqrt{2} z\\0.5\sqrt{2} x + 0.5\sqrt{2} y\\- 0.5 x + 0.5 y + 0.5\sqrt{2} z\end{matrix}\right]$ and fill in $[x,y,z]=[1,0,0]$  I get $\left[\begin{matrix}0.5 \\0.5\sqrt{2} \\- 0.5 \end{matrix}\right]$, while I want $\left[\begin{matrix}1/\sqrt{3} \\1/\sqrt{3} \\1/\sqrt{3} \end{matrix}\right]$. What in my reasoning is incorrect, and how can I solve it?","I have a set of equations that describe a wire in (100) direction. I want to rotate the wire such that it's in the direction (111). My initial plan (which failed) was to use Euler coordinates and first apply a rotation $R_y(\pi/4)$ and then $R_z(\pi/4)$ where the rotation matrices are defined as follows: $$\begin{alignat}{1} R_x(\theta) &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos \theta &  -\sin \theta \\[3pt] 0 & \sin \theta  &  \cos \theta \\[3pt] \end{bmatrix} \\[6pt] R_y(\theta) &= \begin{bmatrix} \cos \theta & 0 & \sin \theta \\[3pt] 0 & 1 & 0 \\[3pt] -\sin \theta & 0 & \cos \theta \\ \end{bmatrix} \\[6pt] R_z(\theta) &= \begin{bmatrix} \cos \theta &  -\sin \theta & 0 \\[3pt] \sin \theta & \cos \theta & 0\\[3pt] 0 & 0 & 1\\ \end{bmatrix} \end{alignat}$$ If I now peform the operation $R_z(\pi/4) R_y(\pi/4) [x, y, z]^T=\left[\begin{matrix}0.5 x - 0.5 y + 0.5\sqrt{2} z\\0.5\sqrt{2} x + 0.5\sqrt{2} y\\- 0.5 x + 0.5 y + 0.5\sqrt{2} z\end{matrix}\right]$ and fill in $[x,y,z]=[1,0,0]$  I get $\left[\begin{matrix}0.5 \\0.5\sqrt{2} \\- 0.5 \end{matrix}\right]$, while I want $\left[\begin{matrix}1/\sqrt{3} \\1/\sqrt{3} \\1/\sqrt{3} \end{matrix}\right]$. What in my reasoning is incorrect, and how can I solve it?",,"['linear-algebra', 'coordinate-systems', '3d', 'rotations']"
40,Does PSD imply on average diagonal dominant?,Does PSD imply on average diagonal dominant?,,"Suppose $A$ is a $N \times N$ positive semidefinite matrix.   This does not necessarily imply that $A$ is diagonally dominant.  But does it imply the following ""average diagonal dominance"" i.e. $$\frac{1}{N} \sum_{i}A_{i,i}\ge \frac{2}{(N-1)N} \sum_{j=1}^{N}\sum_{i<j}A_{i,j}\text{ ?}$$","Suppose $A$ is a $N \times N$ positive semidefinite matrix.   This does not necessarily imply that $A$ is diagonally dominant.  But does it imply the following ""average diagonal dominance"" i.e. $$\frac{1}{N} \sum_{i}A_{i,i}\ge \frac{2}{(N-1)N} \sum_{j=1}^{N}\sum_{i<j}A_{i,j}\text{ ?}$$",,"['linear-algebra', 'matrices']"
41,Tensor products- balanced maps versus bilinear,Tensor products- balanced maps versus bilinear,,"When defining tensor products $M\otimes_R N$ over a commutative ring $R$ one can use a universal property with respect to bilinear maps $M\times N\rightarrow P$. On the other hand, in the general case, for noncommutative rings one has to use balanced maps $M \times N \rightarrow Z$ instead of bilinear. Of course, in the first case $P$ is an $R$-module while in the second case $Z$ is just an abelian group. Remember $f$ is biliniar if $f(mr, n)=rf(m, n)=f(m, nr)$ while $f$ is balanced if and only if $f(mr,n)=f(m, rn)$. I have the following two natural questions: Why the two definitions coincide? Is there an example of a balanced map $M\times N \rightarrow P$ which is not bilinear? I cannot construct one by myself. Here I assume R commutative so I can speak about bilinear maps.","When defining tensor products $M\otimes_R N$ over a commutative ring $R$ one can use a universal property with respect to bilinear maps $M\times N\rightarrow P$. On the other hand, in the general case, for noncommutative rings one has to use balanced maps $M \times N \rightarrow Z$ instead of bilinear. Of course, in the first case $P$ is an $R$-module while in the second case $Z$ is just an abelian group. Remember $f$ is biliniar if $f(mr, n)=rf(m, n)=f(m, nr)$ while $f$ is balanced if and only if $f(mr,n)=f(m, rn)$. I have the following two natural questions: Why the two definitions coincide? Is there an example of a balanced map $M\times N \rightarrow P$ which is not bilinear? I cannot construct one by myself. Here I assume R commutative so I can speak about bilinear maps.",,"['linear-algebra', 'tensor-products']"
42,How big is a tetrahedron?,How big is a tetrahedron?,,"Let $T$ be a tetrahedron with volume $vol(T)$ and edge lengths $a,b,c,d,e,f$ and let $sum(T) = a^3 + b^3 + ... + f^3$.  We wish to compare $vol(T)$ with $sum(T)$.  [ IMO (1961 #2 ) handles the case of triangles. ] Either by approaching it directly or by expressing the volume as a determinant, invoking Hadamard's bound, and applying the AM-GM inequality, one finds that $vol(T) < sum(T)/36$.  However, this hardly seems best possible. Question: What is the sharpest upper bound for $vol(T)$ in terms of $sum(T)$? Extra: Generalize to $n$ dimensions.","Let $T$ be a tetrahedron with volume $vol(T)$ and edge lengths $a,b,c,d,e,f$ and let $sum(T) = a^3 + b^3 + ... + f^3$.  We wish to compare $vol(T)$ with $sum(T)$.  [ IMO (1961 #2 ) handles the case of triangles. ] Either by approaching it directly or by expressing the volume as a determinant, invoking Hadamard's bound, and applying the AM-GM inequality, one finds that $vol(T) < sum(T)/36$.  However, this hardly seems best possible. Question: What is the sharpest upper bound for $vol(T)$ in terms of $sum(T)$? Extra: Generalize to $n$ dimensions.",,"['linear-algebra', 'geometry', 'inequality', 'analytic-geometry', 'simplex']"
43,Determinant value of a square matrix whose each entry is the g.c.d. of row and column position,Determinant value of a square matrix whose each entry is the g.c.d. of row and column position,,"Let $A=(a_{ij})$ be a $n \times n$ matrix with $a_{ij}=\gcd(i,j) , \forall i,j=1,2, \cdots, n$ , then how do we prove $\det A=\prod_{i=1}^n \phi(i)$ ? , where $\phi$ is the Euler's phi function","Let $A=(a_{ij})$ be a $n \times n$ matrix with $a_{ij}=\gcd(i,j) , \forall i,j=1,2, \cdots, n$ , then how do we prove $\det A=\prod_{i=1}^n \phi(i)$ ? , where $\phi$ is the Euler's phi function",,"['linear-algebra', 'matrices', 'number-theory']"
44,"Show that $f\in A_{n-1}(V)$ or $f\in A_n(V)$ is decomposable (Tensors, or k-linear forms)","Show that  or  is decomposable (Tensors, or k-linear forms)",f\in A_{n-1}(V) f\in A_n(V),"Show that $f\in A_{n-1}(V)$ or $f\in A_n(V)$ is decomposable. $f\in A_k(V)$ is decomposable if there exists a $a_1,...,a_k\in V^\wedge$ such that $f=a_1\wedge...\wedge a_k$ In this case ""let $n=dim(V)$"" and the -1 follows. I have seen many pages from this site before and I know that it is almost required of me to show what I have done, unfortunately in this case I have done nothing, I am not sure how to approach this at all, it is my first question on decomposing tensors. I do however know it to be true ( http://mathworld.wolfram.com/Decomposable.html is the most recent tab) but I'm on page 10 of search-engine results, so I finally registered. Thanks","Show that $f\in A_{n-1}(V)$ or $f\in A_n(V)$ is decomposable. $f\in A_k(V)$ is decomposable if there exists a $a_1,...,a_k\in V^\wedge$ such that $f=a_1\wedge...\wedge a_k$ In this case ""let $n=dim(V)$"" and the -1 follows. I have seen many pages from this site before and I know that it is almost required of me to show what I have done, unfortunately in this case I have done nothing, I am not sure how to approach this at all, it is my first question on decomposing tensors. I do however know it to be true ( http://mathworld.wolfram.com/Decomposable.html is the most recent tab) but I'm on page 10 of search-engine results, so I finally registered. Thanks",,['linear-algebra']
45,Novel approaches to linear algebra and geometry,Novel approaches to linear algebra and geometry,,"I'll be studying Brannan's Geometry and Lang's Introduction to Linear Algebra for one university course. I would like to know if you can you suggest some books that offer a unique perspective on the material covered by such books and that can be therefore used as effective companions. This unique perspective may include, for example, the emphasis of the geometric side of linear algebra","I'll be studying Brannan's Geometry and Lang's Introduction to Linear Algebra for one university course. I would like to know if you can you suggest some books that offer a unique perspective on the material covered by such books and that can be therefore used as effective companions. This unique perspective may include, for example, the emphasis of the geometric side of linear algebra",,"['linear-algebra', 'geometry', 'reference-request', 'soft-question', 'book-recommendation']"
46,Is there an elementary proof of Farkas Lemma that does not use convex analysis or hyperplane separation theorem?,Is there an elementary proof of Farkas Lemma that does not use convex analysis or hyperplane separation theorem?,,"Is there any elementary proof of Farkas lemma which does not use convex analysis and hyperplane separation theorem? What about special case below: If the Matrix $A$ is invertible, then there is obviously a unique vector $x$, such that $Ax=b$, is it hard to show that either $x$ is non-negative or there is a vector $y$ such that $y^tA \geq 0$ and $y^t b<0$?","Is there any elementary proof of Farkas lemma which does not use convex analysis and hyperplane separation theorem? What about special case below: If the Matrix $A$ is invertible, then there is obviously a unique vector $x$, such that $Ax=b$, is it hard to show that either $x$ is non-negative or there is a vector $y$ such that $y^tA \geq 0$ and $y^t b<0$?",,"['linear-algebra', 'matrices', 'convex-optimization', 'linear-programming']"
47,Equation over $\mathbb F_2$,Equation over,\mathbb F_2,"Given equation over $\mathbb F_2$: $$x_1x_2x_3+x_4x_5x_6=0$$ It has $50$ solutions. Let $N$ be set of solutions. If we give some linear dependencies to variables we will get cosets (linear subspace + a vector) that are inside  $N$. For example $x_1=0,\  x_4+x_5=1$ will give coset of dimension $4$ that lies on $N$. I have checked using computer that there is not coset of dimension $4$ that is in $N$ and contains $111111$ vector. I need to prove it formally. Any help for proving this or some books, articles about such problems will be appreciated.","Given equation over $\mathbb F_2$: $$x_1x_2x_3+x_4x_5x_6=0$$ It has $50$ solutions. Let $N$ be set of solutions. If we give some linear dependencies to variables we will get cosets (linear subspace + a vector) that are inside  $N$. For example $x_1=0,\  x_4+x_5=1$ will give coset of dimension $4$ that lies on $N$. I have checked using computer that there is not coset of dimension $4$ that is in $N$ and contains $111111$ vector. I need to prove it formally. Any help for proving this or some books, articles about such problems will be appreciated.",,"['linear-algebra', 'finite-fields']"
48,Invertibility of uncertain matrix,Invertibility of uncertain matrix,,"Given that we start with some $n\times n$ square matrix $A_0$ that is non-singular. We add some perturbation to it, $\Delta A$, so that our new matrix is $A = A_0 + \Delta A$. The question is whether we can guarantee invertibility of our new $A$ matrix if $|\Delta A_{ij}|<\rho$. We can refer to this $\rho$ as the radius of invertibility or radius of non-singularity. There seems to be a paper that talks about this exact problem, the additive case: http://www.eecs.berkeley.edu/~elghaoui/Pubs/InvErr_LAA02 . It says that $\rho$ is the smallest singular value of $A$. My question is, if we generate some random $A$ matrix that invertible, can we experimentally converge on $\rho$ through numerous trials? I suppose this is more of a question about possible algorithms than linear algebra.","Given that we start with some $n\times n$ square matrix $A_0$ that is non-singular. We add some perturbation to it, $\Delta A$, so that our new matrix is $A = A_0 + \Delta A$. The question is whether we can guarantee invertibility of our new $A$ matrix if $|\Delta A_{ij}|<\rho$. We can refer to this $\rho$ as the radius of invertibility or radius of non-singularity. There seems to be a paper that talks about this exact problem, the additive case: http://www.eecs.berkeley.edu/~elghaoui/Pubs/InvErr_LAA02 . It says that $\rho$ is the smallest singular value of $A$. My question is, if we generate some random $A$ matrix that invertible, can we experimentally converge on $\rho$ through numerous trials? I suppose this is more of a question about possible algorithms than linear algebra.",,['linear-algebra']
49,What property of a matrix causes $\|e^{tA}\|_2$ to oscillate as $t\rightarrow\infty$?,What property of a matrix causes  to oscillate as ?,\|e^{tA}\|_2 t\rightarrow\infty,"What property of a matrix causes $\|e^{tA}\|_2$ to oscillate as $t\rightarrow\infty$? The best I can come up with is that $A=bi\cdot M$ for $b$ a non-zero real number and $M$ a non-zero idempotent matrix, since in that case we have: $$\|e^{tA}\|_2 =  \left\|\sum_{i=0}^{\infty}\frac{(tbi)^k\cdot M^k}{k!}\right\|_2= \left\|I+M\sum_{i=1}^{\infty}\frac{(tbi)^k}{k!}\right\|_2= \|I+Me^{tbi}\|_2,$$ which quite clearly oscillates.  However it's certainly not clear to me that this is the only way to obtain oscillation, and if it is, I don't see how to go about proving it.","What property of a matrix causes $\|e^{tA}\|_2$ to oscillate as $t\rightarrow\infty$? The best I can come up with is that $A=bi\cdot M$ for $b$ a non-zero real number and $M$ a non-zero idempotent matrix, since in that case we have: $$\|e^{tA}\|_2 =  \left\|\sum_{i=0}^{\infty}\frac{(tbi)^k\cdot M^k}{k!}\right\|_2= \left\|I+M\sum_{i=1}^{\infty}\frac{(tbi)^k}{k!}\right\|_2= \|I+Me^{tbi}\|_2,$$ which quite clearly oscillates.  However it's certainly not clear to me that this is the only way to obtain oscillation, and if it is, I don't see how to go about proving it.",,"['linear-algebra', 'matrices', 'limits']"
50,Matrix graph and irreducibility,Matrix graph and irreducibility,,"How do I prove that if $A\in\mathbb C^{n\times n}$ is a matrix then it is irreducible if and only if its associated graph (defined as at Graph of a matrix ) is strongly connected? Update : Seeing as no-one answered for over a week, I tried to do it by myself. The first thing I did was try to show column or row permutation didn't change the strong connectedness of the graph. I didn't manage, and actually proved the opposite. On the way, though, I managed to show transposition doesn't. The argument is that transposition affects the graph in that it inverts all the arrows, but if there is a loop through all nodes then inverting the arrows means you go through it the other way round, so it's still there, and the graph stays strongly connectedness, and if there isn't, well, transposing can't make one appear, as otherwise transposing back would make it disappear, which we have proved impossible. This result may not be useful, but since I've done it I thought I might well write it down. Then I tried to think of what permuting both rows and columns does to the graph. Why that? Let's recall the notion of irreducible matrix: A matrix $A\in\mathbb{C}^{n\times n}$ is said to be reducible if   there exists a permutation matrix $\Pi$ for which $\Pi\cdot A\cdot > \Pi^T$ is a block upper triangular matrix, i.e. has a block of zeroes   in the bottom-left corner. So if this operation does not alter the graph's strong connectedness, then I can work on the reduced matrix to show its graph is not strongly connected and prove one implication. Now such multiplications as in the definition of a reducible matrix, with $\Pi$ a matrix that swaps line $i$ with line $j$ - what do they do to the graph? Swapping the lines makes all arrows that go out of $i$ go out of $j$ and viceversa; swapping the columns does the same for arrows leaving $i$ (or $j$). So imagine we have a loop. Say it starts from a node other than $i$ and $j$. At a certain point it reaches, say, $i$. Before that, everything is unchanged. When the original loop reaches $i$, the new loop will reach $j$ and go out of it to the same node as it went out from $i$ to before the permutation, if that node wasn't $j$, in which case it will go to $i$. When the original loop enters $j$, the new loop enters $i$, and same as before. So basically the result is just that $i$ and $j$ swap names, and the loop is the same as before taking the name swap into account. So this kind of operations do not alter the strong connectedness of the graph. Suppose $A$ is as follows: $$A=\left(\begin{array}{c|c}\Huge{A_{11}} & \Huge{A_{12}} \\\hline \Huge{0} & \Huge{A_{22}}\end{array}\right).$$ Suppose the $\Huge{0}$ is $m\times m$ with $m\geq\frac{n}{2}$. Then we have $m$ nodes that are unconnected to other $m$ nodes, going out. But we don't have $2m$ nodes, or have exactly that many, so those $m$ nodes are cut off from all the other $n-m$, going out. So suppose there is a loop. If it starts at one of the $m$ nodes, it can never reach the other $n-m$, and if it starts at one of those, it can reach the $m$ nodes but never get back, so maybe we have a path through all the nodes, but it can't be a loop, i.e. a closed path. So the graph is not strongly connected. Now the definition doesn't say anything about the size of those blocks, so the problem I still have is that if $m<\frac{n}{2}$, the argument above fails because we have at least one node besides the $m$ nodes and the other $m$ nodes that can't be reached from the first $m$, and that node could be the missing link. Of course, when I said ""can't be reached"" up till now, I meant ""be reached directly "", i.e. not passing through other nodes. Of course, if the above is concluded, I have proved that reducibility implies non-strong-connectedness of the graph, so that a strongly connected graph implies irreducibility. But the converse I haven't even tried. So the questions are: how do I finish the above at points 3-4 and how do I prove the converse? Or maybe I'm missing something in the definition, in which case what is it? Update 2: I think I am missing something, as a $3\times3$ matrix with a 0 in the bottom-left corner and no other zeroes does have a strongly connected graph, since the only missing arrow is $3\to1$, but we have the loop $1\to3\to2\to1$. So when is a matrix reducible? Update 3: Browsing the web, I have found some things. I bumped first into this link . Now if that is the definition of reducible matrix, then either I misunderstand the definition of the block triangular form, or the if and only if there doesn't hold, since a matrix with a square block of zeroes bottom-left definitely doesn't satisfy the disjoint set condition but definitely does satisfy the permutation condition, with no permutation at all. Maybe the first condition is equivalent to the non-strong-connection of the graph. Yes, because in that case there are $\mu$ nodes from which you can't reach the remaining $\nu$, so the graph is not strongly connected. So at least that condition implies the non-strong-connectedness of the graph. The converse seems a bit trickier. Looking for that definition, I bumped into this link . Note that no matrix there has a lone corner zero (which would be top-right as the link deals with lower triangular matrixes), and all of them satisfy the disjoint set condition in the link above. So what is the definition of block triangular matrix? If it is that there must be square blocks whose diagonal coincides with part of the original matrix's diagonal and below them there must only be zeroes, then I have finished, since the if and only if in the link above is valid, so reducibility implies non-strong-connectedness of the graph, and whoops, I'm not done yet, I still need the converse, so can someone finally come help me on that ? And if it isn't, then what the bleep is it and how do I make this damn proof?","How do I prove that if $A\in\mathbb C^{n\times n}$ is a matrix then it is irreducible if and only if its associated graph (defined as at Graph of a matrix ) is strongly connected? Update : Seeing as no-one answered for over a week, I tried to do it by myself. The first thing I did was try to show column or row permutation didn't change the strong connectedness of the graph. I didn't manage, and actually proved the opposite. On the way, though, I managed to show transposition doesn't. The argument is that transposition affects the graph in that it inverts all the arrows, but if there is a loop through all nodes then inverting the arrows means you go through it the other way round, so it's still there, and the graph stays strongly connectedness, and if there isn't, well, transposing can't make one appear, as otherwise transposing back would make it disappear, which we have proved impossible. This result may not be useful, but since I've done it I thought I might well write it down. Then I tried to think of what permuting both rows and columns does to the graph. Why that? Let's recall the notion of irreducible matrix: A matrix $A\in\mathbb{C}^{n\times n}$ is said to be reducible if   there exists a permutation matrix $\Pi$ for which $\Pi\cdot A\cdot > \Pi^T$ is a block upper triangular matrix, i.e. has a block of zeroes   in the bottom-left corner. So if this operation does not alter the graph's strong connectedness, then I can work on the reduced matrix to show its graph is not strongly connected and prove one implication. Now such multiplications as in the definition of a reducible matrix, with $\Pi$ a matrix that swaps line $i$ with line $j$ - what do they do to the graph? Swapping the lines makes all arrows that go out of $i$ go out of $j$ and viceversa; swapping the columns does the same for arrows leaving $i$ (or $j$). So imagine we have a loop. Say it starts from a node other than $i$ and $j$. At a certain point it reaches, say, $i$. Before that, everything is unchanged. When the original loop reaches $i$, the new loop will reach $j$ and go out of it to the same node as it went out from $i$ to before the permutation, if that node wasn't $j$, in which case it will go to $i$. When the original loop enters $j$, the new loop enters $i$, and same as before. So basically the result is just that $i$ and $j$ swap names, and the loop is the same as before taking the name swap into account. So this kind of operations do not alter the strong connectedness of the graph. Suppose $A$ is as follows: $$A=\left(\begin{array}{c|c}\Huge{A_{11}} & \Huge{A_{12}} \\\hline \Huge{0} & \Huge{A_{22}}\end{array}\right).$$ Suppose the $\Huge{0}$ is $m\times m$ with $m\geq\frac{n}{2}$. Then we have $m$ nodes that are unconnected to other $m$ nodes, going out. But we don't have $2m$ nodes, or have exactly that many, so those $m$ nodes are cut off from all the other $n-m$, going out. So suppose there is a loop. If it starts at one of the $m$ nodes, it can never reach the other $n-m$, and if it starts at one of those, it can reach the $m$ nodes but never get back, so maybe we have a path through all the nodes, but it can't be a loop, i.e. a closed path. So the graph is not strongly connected. Now the definition doesn't say anything about the size of those blocks, so the problem I still have is that if $m<\frac{n}{2}$, the argument above fails because we have at least one node besides the $m$ nodes and the other $m$ nodes that can't be reached from the first $m$, and that node could be the missing link. Of course, when I said ""can't be reached"" up till now, I meant ""be reached directly "", i.e. not passing through other nodes. Of course, if the above is concluded, I have proved that reducibility implies non-strong-connectedness of the graph, so that a strongly connected graph implies irreducibility. But the converse I haven't even tried. So the questions are: how do I finish the above at points 3-4 and how do I prove the converse? Or maybe I'm missing something in the definition, in which case what is it? Update 2: I think I am missing something, as a $3\times3$ matrix with a 0 in the bottom-left corner and no other zeroes does have a strongly connected graph, since the only missing arrow is $3\to1$, but we have the loop $1\to3\to2\to1$. So when is a matrix reducible? Update 3: Browsing the web, I have found some things. I bumped first into this link . Now if that is the definition of reducible matrix, then either I misunderstand the definition of the block triangular form, or the if and only if there doesn't hold, since a matrix with a square block of zeroes bottom-left definitely doesn't satisfy the disjoint set condition but definitely does satisfy the permutation condition, with no permutation at all. Maybe the first condition is equivalent to the non-strong-connection of the graph. Yes, because in that case there are $\mu$ nodes from which you can't reach the remaining $\nu$, so the graph is not strongly connected. So at least that condition implies the non-strong-connectedness of the graph. The converse seems a bit trickier. Looking for that definition, I bumped into this link . Note that no matrix there has a lone corner zero (which would be top-right as the link deals with lower triangular matrixes), and all of them satisfy the disjoint set condition in the link above. So what is the definition of block triangular matrix? If it is that there must be square blocks whose diagonal coincides with part of the original matrix's diagonal and below them there must only be zeroes, then I have finished, since the if and only if in the link above is valid, so reducibility implies non-strong-connectedness of the graph, and whoops, I'm not done yet, I still need the converse, so can someone finally come help me on that ? And if it isn't, then what the bleep is it and how do I make this damn proof?",,"['linear-algebra', 'matrices', 'graph-theory']"
51,Determinant (and invertibility) of generalized Vandermonde matrix,Determinant (and invertibility) of generalized Vandermonde matrix,,"I have stumbled upon the following generalization of Vandermonde matrix when solving some problem in linear algebra related to Jordan normal form. Let us consider some number $\lambda$ and we assign to this number an $n\times m$ matrix $V_m(\lambda)$ such that the first column is of the form $(1,\lambda,\lambda^2,\dots,\lambda^{n-1})^T$, the second column is of the form $(0,1,2\lambda,\dots,(n-1)\lambda^{n-2})^T$ etc. I.e., the $m$-th column will be $(0,\dots,0,1,\binom{m}{m-1}\lambda,\dots,\binom{n-1}{m-1}\lambda^{n-m})$, i.e. $$V_m(\lambda)= \begin{pmatrix} 1             & 0                  & 0                        & \ldots & 0 \\ \lambda       & 1                  & 0                        & \ldots & 0 \\ \lambda^2     & 2\lambda           & 1                        & \ldots & 0 \\ \lambda^3     & 3\lambda^2         & 3\lambda                 & \ldots & 0 \\ \vdots        & \vdots             & \vdots                   &        & \vdots \\ \lambda^{n-1} & (n-1)\lambda^{n-2} & \binom{n-1}2\lambda^{n-3}  & \ldots & \binom{n-1}{m-1}\lambda^{n-m} \end{pmatrix}$$ In the other words, the entry in $k$-th row and $l$-th column is $\binom{k-1}{l-1}x^{k-l}$. Now if we have some numbers $m_1,\dots,m_k$ such that $m_1+\dots+m_k=n$, we can define an $n\times n$-matrix $$V_{m_1,\dots,m_k}(\lambda_1,\dots,\lambda_k)= \begin{pmatrix}V_{m_1}(\lambda_1) & V_{m_2}(\lambda_2) & \dots & V_{m_k}(\lambda_k) \end{pmatrix}.$$ For example, $$V_{3,2}(x,y)= \begin{pmatrix}   1 & 0 & 0 & 1 & 0 \\   x & 1 & 0 & y & 1 \\   x^2 & 2x & 1 & y^2 & 2y \\   x^3 & 3x^2 & 3x & y^3 & 3y^2 \\   x^4 & 4x^3 & 6x & y^4 & 4y^3 \end{pmatrix} $$ Such matrix is indeed called generalized Vandermonde matrix by some authors, for example here or here . (Although the term generalized Vandermonde matrix is also used in different meanings, for example here .) The determinant of generalized Vandermonde matrix is $$\prod_{i<j} (\lambda_j-\lambda_i)^{m_im_j}.$$ We already have at this site several questions about the usual Vandermonde matrix, for example Vandermonde Determinant , Vandermonde determinant by induction , Proof determinant of transpose Vandermonde matrix is $\prod_{1\le i\lt j\le n}(\alpha_i-\alpha_j)$ , Why are Vandermonde matrices invertible? Various derivations of determinant of Vandermonde matrix and also some proofs of the fact that it is invertible (for distinct $\lambda_i$'s) are given in those questions. I am wondering about the same question for generalized Vandermonde matrix. How can we show that generalized Vandermonde matrix is invertible when $\lambda_i\ne\lambda_j$? How can we evaluate the determinant of generalized Vandermonde matrix?","I have stumbled upon the following generalization of Vandermonde matrix when solving some problem in linear algebra related to Jordan normal form. Let us consider some number $\lambda$ and we assign to this number an $n\times m$ matrix $V_m(\lambda)$ such that the first column is of the form $(1,\lambda,\lambda^2,\dots,\lambda^{n-1})^T$, the second column is of the form $(0,1,2\lambda,\dots,(n-1)\lambda^{n-2})^T$ etc. I.e., the $m$-th column will be $(0,\dots,0,1,\binom{m}{m-1}\lambda,\dots,\binom{n-1}{m-1}\lambda^{n-m})$, i.e. $$V_m(\lambda)= \begin{pmatrix} 1             & 0                  & 0                        & \ldots & 0 \\ \lambda       & 1                  & 0                        & \ldots & 0 \\ \lambda^2     & 2\lambda           & 1                        & \ldots & 0 \\ \lambda^3     & 3\lambda^2         & 3\lambda                 & \ldots & 0 \\ \vdots        & \vdots             & \vdots                   &        & \vdots \\ \lambda^{n-1} & (n-1)\lambda^{n-2} & \binom{n-1}2\lambda^{n-3}  & \ldots & \binom{n-1}{m-1}\lambda^{n-m} \end{pmatrix}$$ In the other words, the entry in $k$-th row and $l$-th column is $\binom{k-1}{l-1}x^{k-l}$. Now if we have some numbers $m_1,\dots,m_k$ such that $m_1+\dots+m_k=n$, we can define an $n\times n$-matrix $$V_{m_1,\dots,m_k}(\lambda_1,\dots,\lambda_k)= \begin{pmatrix}V_{m_1}(\lambda_1) & V_{m_2}(\lambda_2) & \dots & V_{m_k}(\lambda_k) \end{pmatrix}.$$ For example, $$V_{3,2}(x,y)= \begin{pmatrix}   1 & 0 & 0 & 1 & 0 \\   x & 1 & 0 & y & 1 \\   x^2 & 2x & 1 & y^2 & 2y \\   x^3 & 3x^2 & 3x & y^3 & 3y^2 \\   x^4 & 4x^3 & 6x & y^4 & 4y^3 \end{pmatrix} $$ Such matrix is indeed called generalized Vandermonde matrix by some authors, for example here or here . (Although the term generalized Vandermonde matrix is also used in different meanings, for example here .) The determinant of generalized Vandermonde matrix is $$\prod_{i<j} (\lambda_j-\lambda_i)^{m_im_j}.$$ We already have at this site several questions about the usual Vandermonde matrix, for example Vandermonde Determinant , Vandermonde determinant by induction , Proof determinant of transpose Vandermonde matrix is $\prod_{1\le i\lt j\le n}(\alpha_i-\alpha_j)$ , Why are Vandermonde matrices invertible? Various derivations of determinant of Vandermonde matrix and also some proofs of the fact that it is invertible (for distinct $\lambda_i$'s) are given in those questions. I am wondering about the same question for generalized Vandermonde matrix. How can we show that generalized Vandermonde matrix is invertible when $\lambda_i\ne\lambda_j$? How can we evaluate the determinant of generalized Vandermonde matrix?",,"['linear-algebra', 'determinant']"
52,Coordinate Transformations,Coordinate Transformations,,"I am physics student. My mathematical background is quite weak.  I just want to know the similarities (if there are any) between coordinate transformation of two kinds : Rotation of coordinate (and hence new transformed coordinate system) or translation and so on. (Most of which are symmetry associated) Transformation to a different system of coordinates - like cartesian to cylindrical or spherical to parabolic and so on. In this regard I would like to know the difference between these two in the linear algebra language. I can see in both cases the inner product is preserved. Secondly, I am also interested in knowing if we can associate symmetry to the second kind of transformation and hence some generator of transformation. Thanks","I am physics student. My mathematical background is quite weak.  I just want to know the similarities (if there are any) between coordinate transformation of two kinds : Rotation of coordinate (and hence new transformed coordinate system) or translation and so on. (Most of which are symmetry associated) Transformation to a different system of coordinates - like cartesian to cylindrical or spherical to parabolic and so on. In this regard I would like to know the difference between these two in the linear algebra language. I can see in both cases the inner product is preserved. Secondly, I am also interested in knowing if we can associate symmetry to the second kind of transformation and hence some generator of transformation. Thanks",,"['linear-algebra', 'analytic-geometry', 'coordinate-systems']"
53,Determinants and cofactors?,Determinants and cofactors?,,"My professor gave us this definition for determinants for a $n \times n$ matrix $A$: $$\det(A) = a_{11}C_{11} + a_{12}C_{12} ... + a_{1n}C_{1n} $$ where $C_{1j}$ is the cofactor of $A$ on $a_{ij}$. He also said that this can be generalized to: $$\det(A) = a_{q1}C_{q1} + a_{q2}C_{q2} ... + a_{qn}C_{qn} $$ for any row or column $q$. He didn't give us a proof of the above statement, because he said it was too complicated. Out of curiosity, could someone give me a proof of the above statement? I would appreciate a proof instead of a tip , because of me being completely new to this and the proof being way too advanced for my skills. What I know so far: Row Reduction, Basis ( not change of basis however), Linear Independence, Span, Transformations, Inverses, Subspaces and Dimensions. Please try to keep proofs within my what I know. Thank you.","My professor gave us this definition for determinants for a $n \times n$ matrix $A$: $$\det(A) = a_{11}C_{11} + a_{12}C_{12} ... + a_{1n}C_{1n} $$ where $C_{1j}$ is the cofactor of $A$ on $a_{ij}$. He also said that this can be generalized to: $$\det(A) = a_{q1}C_{q1} + a_{q2}C_{q2} ... + a_{qn}C_{qn} $$ for any row or column $q$. He didn't give us a proof of the above statement, because he said it was too complicated. Out of curiosity, could someone give me a proof of the above statement? I would appreciate a proof instead of a tip , because of me being completely new to this and the proof being way too advanced for my skills. What I know so far: Row Reduction, Basis ( not change of basis however), Linear Independence, Span, Transformations, Inverses, Subspaces and Dimensions. Please try to keep proofs within my what I know. Thank you.",,"['linear-algebra', 'matrices', 'determinant']"
54,Why is there n-1 different objects in a n by n matrix game like Bejeweled?,Why is there n-1 different objects in a n by n matrix game like Bejeweled?,,"For games that consists of a grid, and is similar to the concept like bejeweled: has an n by n matrix and n-1 different objects. What is the reason for this? Why not have more than n-1 different objects, or have less than n-1 objects. What is the logic behind this? Some Examples: DOTS have a 6 by 6 matrix, with 5 different colors of circles Bejeweled has a 8 by 8 matrix with 7 different jewels ANIPANG has a 7 by 7 matrix with 6 different animals","For games that consists of a grid, and is similar to the concept like bejeweled: has an n by n matrix and n-1 different objects. What is the reason for this? Why not have more than n-1 different objects, or have less than n-1 objects. What is the logic behind this? Some Examples: DOTS have a 6 by 6 matrix, with 5 different colors of circles Bejeweled has a 8 by 8 matrix with 7 different jewels ANIPANG has a 7 by 7 matrix with 6 different animals",,"['linear-algebra', 'matrices', 'logic', 'linear-programming']"
55,SVD by QR and Choleski decomposition - What is going on?,SVD by QR and Choleski decomposition - What is going on?,,"Here's an algorithm I found that performs Singular Value Decomposition. I preferred this algorithm because it can be parallelized, and I don't have to calculate the huge $AA^T$ matrix when the number of rows are very large: Let A be the matrix for which SVD has to be performed. $A = U{\Sigma}V^T$ 1) First perform $QR$ decomposition of $A$. $A=QR$ We calculate this by performing the 2nd and the 3rd step. 2) To calculate the $R$ matrix, calculate $A^TA$ and find the Cholesky decomposition of $A^TA$. $R = Cholesky(A^TA)$ 3) Calculate the $Q$ matrix: $Q = AR^{-1}$ 4) Also, perform SVD on the $R$ matrix (a small matrix when compared to A). $U_R{\Sigma}V^T = R$ The $\Sigma$ and $V$ matrix for $R$ will be the same as for the SVD for $A$. Now calculate the $U$ matrix: $U = QU_R$ Now, I know the concept of SVD through eigenvectors and eigenvalues of $AA^T$ and $A^TA$, but what is going on here? Is there an intuitive explanation?","Here's an algorithm I found that performs Singular Value Decomposition. I preferred this algorithm because it can be parallelized, and I don't have to calculate the huge $AA^T$ matrix when the number of rows are very large: Let A be the matrix for which SVD has to be performed. $A = U{\Sigma}V^T$ 1) First perform $QR$ decomposition of $A$. $A=QR$ We calculate this by performing the 2nd and the 3rd step. 2) To calculate the $R$ matrix, calculate $A^TA$ and find the Cholesky decomposition of $A^TA$. $R = Cholesky(A^TA)$ 3) Calculate the $Q$ matrix: $Q = AR^{-1}$ 4) Also, perform SVD on the $R$ matrix (a small matrix when compared to A). $U_R{\Sigma}V^T = R$ The $\Sigma$ and $V$ matrix for $R$ will be the same as for the SVD for $A$. Now calculate the $U$ matrix: $U = QU_R$ Now, I know the concept of SVD through eigenvectors and eigenvalues of $AA^T$ and $A^TA$, but what is going on here? Is there an intuitive explanation?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'cholesky-decomposition']"
56,Column Space of AA' is equal to column of A. [duplicate],Column Space of AA' is equal to column of A. [duplicate],,This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . This is my question. How to show that the column space of matrix A is just equal to the column space of AA'?.. A' represents the transpose of A. I know that the column space of AA' is a subset of the column space of A which is just trivial. But the other way around I still used inclusion  but it seemed that it is going nowhere. Maybe it can be done by some manipulation or the barbaric way of doing this. I just want to obtain a simple proof. Anyone?,This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . This is my question. How to show that the column space of matrix A is just equal to the column space of AA'?.. A' represents the transpose of A. I know that the column space of AA' is a subset of the column space of A which is just trivial. But the other way around I still used inclusion  but it seemed that it is going nowhere. Maybe it can be done by some manipulation or the barbaric way of doing this. I just want to obtain a simple proof. Anyone?,,['linear-algebra']
57,"$v_1,v_2$ are eigenvectors of $A$. Is it true that $v_1-v_2$ is eigenvector of $A$?",are eigenvectors of . Is it true that  is eigenvector of ?,"v_1,v_2 A v_1-v_2 A","Let $A \in \mathcal{M}_{12 \times 12}$ and let $v_1,v_2$ be eigenvectors of $A$ such that $Av_1 = v_1$ and $Av_2 = 2v_2$. Is it true that vector $v_1 - v_2$ is not eigenvector of $A$? My answer: We have $A(v_1 - v_2) = Av_1 - Av_2 = v_1 - 2v_2$. Let suppose that $v_1-v_2$ is eigenvector of $A$. So exists $\lambda \neq 0 \in \mathbb{R} $ such that $v_1 - 2v_2 = \lambda (v_1 - v_2)$ hence $(1- \lambda)v_1 + (\lambda - 2)v_2 = 0$. Of course, $v_1,v_2$ are linear independenc so $\lambda =1$ and $\lambda=2$. We have conflict, so the answer is FALSE. But answer in my book is TRUE. Could you tell me why?","Let $A \in \mathcal{M}_{12 \times 12}$ and let $v_1,v_2$ be eigenvectors of $A$ such that $Av_1 = v_1$ and $Av_2 = 2v_2$. Is it true that vector $v_1 - v_2$ is not eigenvector of $A$? My answer: We have $A(v_1 - v_2) = Av_1 - Av_2 = v_1 - 2v_2$. Let suppose that $v_1-v_2$ is eigenvector of $A$. So exists $\lambda \neq 0 \in \mathbb{R} $ such that $v_1 - 2v_2 = \lambda (v_1 - v_2)$ hence $(1- \lambda)v_1 + (\lambda - 2)v_2 = 0$. Of course, $v_1,v_2$ are linear independenc so $\lambda =1$ and $\lambda=2$. We have conflict, so the answer is FALSE. But answer in my book is TRUE. Could you tell me why?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'solution-verification']"
58,Interpreting a singular value in a specific problem,Interpreting a singular value in a specific problem,,"In a similar spirit to this post , I pose the following: Contextual Problem A PhD student in Applied Mathematics is defending his dissertation and needs to make 10 gallon keg consisting of vodka and beer to placate his thesis committee.  Suppose that all committee members, being stubborn people, refuse to sign his dissertation paperwork until the next day.   Since all committee members will be driving home immediately after his defense, he wants to make sure that they all drive home safely.  To do so, he must ensure that his mixture doesn't contain too much alcohol in it! Therefore, his goal is to make a 10 liter mixture of vodka and beer such that the total alcohol content of the mixture is only $12$ percent.  Suppose that beer has $8\%$ alcohol while vodka has $40\%$.  If $x$ is the volume of beer and $y$ is the volume of vodka needed, then clearly the system of equations is \begin{equation} x+y=10 \\ 0.08 x +0.4 y = 0.12\times 10  \end{equation} My Question The singular value decomposition of the corresponding matrix \begin{equation} A=\left[ \begin{array}{cc} 1 & 1\\ 0.08 & 0.4 \end{array} \right] \end{equation} is $$A=U\Sigma V^T$$ with \begin{equation} U=\left[ \begin{array}{cc} -0.9711 & -0.2388\\ -0.2388 & 0.9711 \end{array} \right] \end{equation} \begin{equation} \Sigma=\left[ \begin{array}{cc} 1.4554 & 0\\ 0 & 0.2199 \end{array} \right] \end{equation} \begin{equation} V=\left[ \begin{array}{cc} -0.6804 &-0.7329\\ -0.7329 & 0.6804 \end{array} \right] \end{equation} How do I interpret their physical meaning of the singular values and the columns of the two unitary matrices in the context of this particular problem?  That is, what insight do these quantities give me about the nature of the problem itself or perturbations thereof?","In a similar spirit to this post , I pose the following: Contextual Problem A PhD student in Applied Mathematics is defending his dissertation and needs to make 10 gallon keg consisting of vodka and beer to placate his thesis committee.  Suppose that all committee members, being stubborn people, refuse to sign his dissertation paperwork until the next day.   Since all committee members will be driving home immediately after his defense, he wants to make sure that they all drive home safely.  To do so, he must ensure that his mixture doesn't contain too much alcohol in it! Therefore, his goal is to make a 10 liter mixture of vodka and beer such that the total alcohol content of the mixture is only $12$ percent.  Suppose that beer has $8\%$ alcohol while vodka has $40\%$.  If $x$ is the volume of beer and $y$ is the volume of vodka needed, then clearly the system of equations is \begin{equation} x+y=10 \\ 0.08 x +0.4 y = 0.12\times 10  \end{equation} My Question The singular value decomposition of the corresponding matrix \begin{equation} A=\left[ \begin{array}{cc} 1 & 1\\ 0.08 & 0.4 \end{array} \right] \end{equation} is $$A=U\Sigma V^T$$ with \begin{equation} U=\left[ \begin{array}{cc} -0.9711 & -0.2388\\ -0.2388 & 0.9711 \end{array} \right] \end{equation} \begin{equation} \Sigma=\left[ \begin{array}{cc} 1.4554 & 0\\ 0 & 0.2199 \end{array} \right] \end{equation} \begin{equation} V=\left[ \begin{array}{cc} -0.6804 &-0.7329\\ -0.7329 & 0.6804 \end{array} \right] \end{equation} How do I interpret their physical meaning of the singular values and the columns of the two unitary matrices in the context of this particular problem?  That is, what insight do these quantities give me about the nature of the problem itself or perturbations thereof?",,['linear-algebra']
59,Is this proof that the vectors are colinear correct?,Is this proof that the vectors are colinear correct?,,"I was solving the following exercise: ""Let $x,y \in \mathbb{R}^n$ be nonzero such that if $z$ is orthogonal to $x$ then $z$ is orthogonal to $y$. Prove that $x$ and $y$ are colinear"". My idea was: since $x$ is nonzero, we know that we can write $y =\alpha x + z$ where $z$ is orthogonal to $x$. But, by hypothesis, being orthogonal to $x$ implies being orthogonal to $y$, so that taking the inner product on both sides with $z$ we have: $$\left\langle y,z \right\rangle=\alpha\left\langle x,z \right\rangle + \left\langle z,z \right\rangle = 0,$$ but since $\left\langle x,z\right\rangle = 0$ this implies $|z|^2 = 0$ so that by properties of norm, we must have $z = 0$ proving that $y = \alpha x$. Is this proof ok? I felt it was too easy to prove it and the answer in the book is a little different, so I thought there might be some error in this thought. Thanks very much in advance!","I was solving the following exercise: ""Let $x,y \in \mathbb{R}^n$ be nonzero such that if $z$ is orthogonal to $x$ then $z$ is orthogonal to $y$. Prove that $x$ and $y$ are colinear"". My idea was: since $x$ is nonzero, we know that we can write $y =\alpha x + z$ where $z$ is orthogonal to $x$. But, by hypothesis, being orthogonal to $x$ implies being orthogonal to $y$, so that taking the inner product on both sides with $z$ we have: $$\left\langle y,z \right\rangle=\alpha\left\langle x,z \right\rangle + \left\langle z,z \right\rangle = 0,$$ but since $\left\langle x,z\right\rangle = 0$ this implies $|z|^2 = 0$ so that by properties of norm, we must have $z = 0$ proving that $y = \alpha x$. Is this proof ok? I felt it was too easy to prove it and the answer in the book is a little different, so I thought there might be some error in this thought. Thanks very much in advance!",,"['linear-algebra', 'multivariable-calculus', 'proof-verification']"
60,Images in a short exact sequence,Images in a short exact sequence,,"Suppose $$ 0\to V\to W\to X\to 0\\ \downarrow\quad\quad\downarrow\quad\quad\downarrow\\ 0\to V'\to W'\to X'\to 0\\ $$ is a commutative diagram of vector spaces, with the top and bottom rows short exact sequences.  When is it true that I have an exact sequence $$ 0\to im(V\to V')\to im(W\to W')\to im(X\to X')\to 0? $$ I think I get from the snake lemma that this occurs if and only if the connecting morphism from the snake lemma is the trivial map, but I'm not feeling particularly confident in my answer, or maybe there's something simpler.","Suppose $$ 0\to V\to W\to X\to 0\\ \downarrow\quad\quad\downarrow\quad\quad\downarrow\\ 0\to V'\to W'\to X'\to 0\\ $$ is a commutative diagram of vector spaces, with the top and bottom rows short exact sequences.  When is it true that I have an exact sequence $$ 0\to im(V\to V')\to im(W\to W')\to im(X\to X')\to 0? $$ I think I get from the snake lemma that this occurs if and only if the connecting morphism from the snake lemma is the trivial map, but I'm not feeling particularly confident in my answer, or maybe there's something simpler.",,"['linear-algebra', 'commutative-algebra', 'vector-spaces', 'homological-algebra']"
61,"Dimension of $\operatorname{Hom}(V, W)$",Dimension of,"\operatorname{Hom}(V, W)","What is the dimension of $\operatorname{Hom}(V, W)$ if at least one of the two vector spaces $V, W$ is infinite dimensional? In the sense of cardinal numbers. Thanks","What is the dimension of $\operatorname{Hom}(V, W)$ if at least one of the two vector spaces $V, W$ is infinite dimensional? In the sense of cardinal numbers. Thanks",,"['linear-algebra', 'abstract-algebra', 'elementary-set-theory', 'vector-spaces', 'cardinals']"
62,How to show annihilator has dimension m-n (with Proof),How to show annihilator has dimension m-n (with Proof),,"I would like to show the following: Given a vector spaces $V$, a subspace $S \subset V$ and an the dual space $V^*$ to $V$. Show that: $$\dim(N)+\dim(S) = \dim(V) = \dim(V^*)$$, where $N \subset V^*$ is the annihilator to $S$. Assume only finite dimensional vector spaces. I tried a proof but I am not sure if this is correct: Proof: I give a try :-) Assume that $\dim(S) = m$, $\dim(V) = \dim(V^*) = n$ The annihilator is given as: $$N:= \{ \boldsymbol{\alpha} \mid   \langle \boldsymbol{\alpha},\mathbf{x} \rangle =0 \quad ,\quad \forall \mathbf{x} \in S \}$$ Where $ \langle \boldsymbol{\alpha},x \rangle$ is the duality pairing, it is a bilinear form as: $$ \begin{aligned} \textrm{B}(  \boldsymbol{\alpha},\mathbf{x} ) :   V^* \times V &\rightarrow \mathbb{R} \\ \boldsymbol{\alpha},\mathbf{x} &\mapsto \langle \boldsymbol{\alpha} ,\mathbf{x} \rangle \end{aligned} $$ We fix basis vectors for the following spaces: Basis for $V$:   $\{\mathbf{e}_i\}$ with $i \in \{1,n\}$ Basis for $S$: $\{\mathbf{e}_i\}$ with $i \in \{1,m\}$ Basis for $V^*$: $\{\boldsymbol{\alpha}^i\}$ with $i \in \{1,n\}$ Now with $\boldsymbol{\beta} =\beta_j \boldsymbol{\alpha}^j \in V^*$ and $\mathbf{x} = x^i \mathbf{e}_i  \in V$, it follows that $$ \langle \boldsymbol{\beta} ,\mathbf{x} \rangle = \langle \beta_j  \boldsymbol{\alpha}^j ,x^i \mathbf{e}_i\rangle = \beta_j \langle \boldsymbol{\alpha}^j , \mathbf{e}_i \rangle  x^i  = \beta_j \ B^j_{\ i} \ x^i \quad i,j \in \{1,n\}$$ We want now for every $\mathbf{x} \in S$ and a corresponding $\boldsymbol{\alpha} \in N$ that: $$\begin{aligned} \langle \boldsymbol{\beta} ,\mathbf{x} \rangle = 0 &= (\beta_1, \cdots, \beta_n)\left( \begin{array}{ccc}  B^1_{\ 1} & \cdots  & B^1_{\ m} \\ \vdots & \cdots  & \vdots \\ \vdots & \cdots  & \vdots \\ B^n_{\ 1} & \cdots  & B^n_{\ m}  \end{array} \right) \left(\begin{array}{c} x^1 \\ \vdots \\ x^m\end{array} \right)  \end{aligned} = [\boldsymbol{\beta}]^\top\mathbf{B}[\mathbf{x}] =[\mathbf{x}]^\top\mathbf{B}^\top [\boldsymbol{\beta}], \quad \forall [\mathbf{x}] $$ From where follows: $$\mathbf{B}^\top [\boldsymbol{\beta}] = \mathbf{0}$$ If we assume that $\boldsymbol{\alpha}^i$ is the dual basis vector to $\mathbf{e}_i$ with the property: $$ \langle \boldsymbol{\alpha}^i, \mathbf{e}_j \rangle = \delta^i_{\ j} $$, where $\delta^i_{\ j}$ is the Kroenecker Delta. It follows that $\delta^j_{\ i} = B^j_{\ i}$. Thus $[\boldsymbol{\beta}]$ lies in the Nullspace of $N$, $[\boldsymbol{\beta}] \in \text{Null}(\mathbf{B}^\top)$. The dimension of $\text{dim}(\text{Null}(\mathbf{B}^\top)) = n-m$ which means what now? and how can I construct a basis from the given ones for $N$? Thanks for the inputs!","I would like to show the following: Given a vector spaces $V$, a subspace $S \subset V$ and an the dual space $V^*$ to $V$. Show that: $$\dim(N)+\dim(S) = \dim(V) = \dim(V^*)$$, where $N \subset V^*$ is the annihilator to $S$. Assume only finite dimensional vector spaces. I tried a proof but I am not sure if this is correct: Proof: I give a try :-) Assume that $\dim(S) = m$, $\dim(V) = \dim(V^*) = n$ The annihilator is given as: $$N:= \{ \boldsymbol{\alpha} \mid   \langle \boldsymbol{\alpha},\mathbf{x} \rangle =0 \quad ,\quad \forall \mathbf{x} \in S \}$$ Where $ \langle \boldsymbol{\alpha},x \rangle$ is the duality pairing, it is a bilinear form as: $$ \begin{aligned} \textrm{B}(  \boldsymbol{\alpha},\mathbf{x} ) :   V^* \times V &\rightarrow \mathbb{R} \\ \boldsymbol{\alpha},\mathbf{x} &\mapsto \langle \boldsymbol{\alpha} ,\mathbf{x} \rangle \end{aligned} $$ We fix basis vectors for the following spaces: Basis for $V$:   $\{\mathbf{e}_i\}$ with $i \in \{1,n\}$ Basis for $S$: $\{\mathbf{e}_i\}$ with $i \in \{1,m\}$ Basis for $V^*$: $\{\boldsymbol{\alpha}^i\}$ with $i \in \{1,n\}$ Now with $\boldsymbol{\beta} =\beta_j \boldsymbol{\alpha}^j \in V^*$ and $\mathbf{x} = x^i \mathbf{e}_i  \in V$, it follows that $$ \langle \boldsymbol{\beta} ,\mathbf{x} \rangle = \langle \beta_j  \boldsymbol{\alpha}^j ,x^i \mathbf{e}_i\rangle = \beta_j \langle \boldsymbol{\alpha}^j , \mathbf{e}_i \rangle  x^i  = \beta_j \ B^j_{\ i} \ x^i \quad i,j \in \{1,n\}$$ We want now for every $\mathbf{x} \in S$ and a corresponding $\boldsymbol{\alpha} \in N$ that: $$\begin{aligned} \langle \boldsymbol{\beta} ,\mathbf{x} \rangle = 0 &= (\beta_1, \cdots, \beta_n)\left( \begin{array}{ccc}  B^1_{\ 1} & \cdots  & B^1_{\ m} \\ \vdots & \cdots  & \vdots \\ \vdots & \cdots  & \vdots \\ B^n_{\ 1} & \cdots  & B^n_{\ m}  \end{array} \right) \left(\begin{array}{c} x^1 \\ \vdots \\ x^m\end{array} \right)  \end{aligned} = [\boldsymbol{\beta}]^\top\mathbf{B}[\mathbf{x}] =[\mathbf{x}]^\top\mathbf{B}^\top [\boldsymbol{\beta}], \quad \forall [\mathbf{x}] $$ From where follows: $$\mathbf{B}^\top [\boldsymbol{\beta}] = \mathbf{0}$$ If we assume that $\boldsymbol{\alpha}^i$ is the dual basis vector to $\mathbf{e}_i$ with the property: $$ \langle \boldsymbol{\alpha}^i, \mathbf{e}_j \rangle = \delta^i_{\ j} $$, where $\delta^i_{\ j}$ is the Kroenecker Delta. It follows that $\delta^j_{\ i} = B^j_{\ i}$. Thus $[\boldsymbol{\beta}]$ lies in the Nullspace of $N$, $[\boldsymbol{\beta}] \in \text{Null}(\mathbf{B}^\top)$. The dimension of $\text{dim}(\text{Null}(\mathbf{B}^\top)) = n-m$ which means what now? and how can I construct a basis from the given ones for $N$? Thanks for the inputs!",,"['linear-algebra', 'vector-spaces']"
63,"Prove that if $Q^tQ = I$ and $A = QR$, then $\|Ax - b\| = \|Rx - Q^tb\|$","Prove that if  and , then",Q^tQ = I A = QR \|Ax - b\| = \|Rx - Q^tb\|,"I have a linear algebra final tomorrow and was practicing a few proofs. I want to make sure this proof is correct. Prove that: If $Q^tQ = I$ and $A = QR$, then $\|Ax - b\| = \|Rx - Q^tb\|$ $$\begin{align*} A &= QR\\[0.1cm] Ax &= QRx\\[0.1cm] Ax - b &= QRx - b\\[0.1cm] \qquad Ax - b &= QRx - QQ^tb \quad\text{(since $QQ^t = I$)}\\[0.1cm] Ax - b &= Q(Rx - Q^tb)\\[0.1cm] \|Ax - b\| &= \|Q(Rx - Q^tb)\|\\[0.1cm] \end{align*}$$   Since the orthogonal transformation preserves length, $\|Q(Rx - Q^tb)\| = \|(Rx - Q^tb)\|$. This completes the proof: $\|Ax - b\| = \|Rx - Q^tb\|$","I have a linear algebra final tomorrow and was practicing a few proofs. I want to make sure this proof is correct. Prove that: If $Q^tQ = I$ and $A = QR$, then $\|Ax - b\| = \|Rx - Q^tb\|$ $$\begin{align*} A &= QR\\[0.1cm] Ax &= QRx\\[0.1cm] Ax - b &= QRx - b\\[0.1cm] \qquad Ax - b &= QRx - QQ^tb \quad\text{(since $QQ^t = I$)}\\[0.1cm] Ax - b &= Q(Rx - Q^tb)\\[0.1cm] \|Ax - b\| &= \|Q(Rx - Q^tb)\|\\[0.1cm] \end{align*}$$   Since the orthogonal transformation preserves length, $\|Q(Rx - Q^tb)\| = \|(Rx - Q^tb)\|$. This completes the proof: $\|Ax - b\| = \|Rx - Q^tb\|$",,['linear-algebra']
64,"My proof that if for a k degree polynomial $P(x)$, for the matrix $A$, $P(A)=0$ then $A$ is invertible","My proof that if for a k degree polynomial , for the matrix ,  then  is invertible",P(x) A P(A)=0 A,"Let $P(x)$ be a $k$-degree polynomial with with non-zero free coefficient. Prove that if for matrix $A$, $P(A)$=0, then $A$ is invertible and $A^{-1}$ is $k-1$ degree $A$ polynomial. Here's my proof, I would like to know if that's ok: Let $P(x)$ be defined: $P(x)=a_1x+a_2x^2+ \dots +a_kx^k+a_{k+1}$. We know $P(A)=0$, so that: $$P(A)=a_1A+a_2A^2+\dots +a_kA^k+a_{k+1}=0$$  $$A(a_1+a_2A+ \dots +a_kA^{k-1})=-a_{k+1}$$ $-a_{k+1}I=-a_{k+1}$, so: $$A(a_1+a_2A+ \dots +a_kA^{k-1})=-a_{k+1}I$$ $$A{(a_1+a_2A+ \dots+a_kA^{k-1})\over -a_{k+1}}=I.$$ Lets define: $A^{-1}={(a_1+a_2A+\dots +a_kA^{k-1})\over -a_{k+1}}$, and then we found that $AA^{-1}=I$. Thus $A$ is invertible, and we can see $A^{-1}$ is a $k-1$ degree polynomial. QED Is this proof valid? Thanks in advance. [Translated from another language]","Let $P(x)$ be a $k$-degree polynomial with with non-zero free coefficient. Prove that if for matrix $A$, $P(A)$=0, then $A$ is invertible and $A^{-1}$ is $k-1$ degree $A$ polynomial. Here's my proof, I would like to know if that's ok: Let $P(x)$ be defined: $P(x)=a_1x+a_2x^2+ \dots +a_kx^k+a_{k+1}$. We know $P(A)=0$, so that: $$P(A)=a_1A+a_2A^2+\dots +a_kA^k+a_{k+1}=0$$  $$A(a_1+a_2A+ \dots +a_kA^{k-1})=-a_{k+1}$$ $-a_{k+1}I=-a_{k+1}$, so: $$A(a_1+a_2A+ \dots +a_kA^{k-1})=-a_{k+1}I$$ $$A{(a_1+a_2A+ \dots+a_kA^{k-1})\over -a_{k+1}}=I.$$ Lets define: $A^{-1}={(a_1+a_2A+\dots +a_kA^{k-1})\over -a_{k+1}}$, and then we found that $AA^{-1}=I$. Thus $A$ is invertible, and we can see $A^{-1}$ is a $k-1$ degree polynomial. QED Is this proof valid? Thanks in advance. [Translated from another language]",,"['linear-algebra', 'matrices', 'proof-verification', 'polynomials', 'inverse']"
65,How to make two vectors orthogonal?,How to make two vectors orthogonal?,,"From my question on CV, I need to find a way to make my two vectors orthogonal. They are: $${\bf{v}} = \pmatrix{1 \\ 1 \\ 1 \\ 1 \\ 1 } \hspace{1.5cm} {\bf{w}} = \pmatrix{0.25 \\ 0.0625 \\ 0 \\ 0.0625 \\ 0.25}$$ What I (think) I need is the step that makes these two vectors orthogonal and I think that will give me my orthogonal regression model. But how do I do this? It's not Gram-Schmidt is it? EDIT: 1) Fixed spelling of Gram-Schmidt thanks to the comment. 2) Basically, for my regression model to be orthogonal, I need all the sum of all the linear terms to be 0 and the sum of all the quadratic terms to be 0. The vector ${\bf{w}}$ is all the $x_i^2$ terms in matrix form. What I need is effectively the dot product of ${\bf{v \cdot w' }} = 0$, where ${\bf{w'}}$ is basically ${\bf{w}}$ transformed in some way such that the sum of all these new terms equal $0$. Does that make sense?","From my question on CV, I need to find a way to make my two vectors orthogonal. They are: $${\bf{v}} = \pmatrix{1 \\ 1 \\ 1 \\ 1 \\ 1 } \hspace{1.5cm} {\bf{w}} = \pmatrix{0.25 \\ 0.0625 \\ 0 \\ 0.0625 \\ 0.25}$$ What I (think) I need is the step that makes these two vectors orthogonal and I think that will give me my orthogonal regression model. But how do I do this? It's not Gram-Schmidt is it? EDIT: 1) Fixed spelling of Gram-Schmidt thanks to the comment. 2) Basically, for my regression model to be orthogonal, I need all the sum of all the linear terms to be 0 and the sum of all the quadratic terms to be 0. The vector ${\bf{w}}$ is all the $x_i^2$ terms in matrix form. What I need is effectively the dot product of ${\bf{v \cdot w' }} = 0$, where ${\bf{w'}}$ is basically ${\bf{w}}$ transformed in some way such that the sum of all these new terms equal $0$. Does that make sense?",,['linear-algebra']
66,"dimension of the space of all symmetric matrices with trace $0$ and $a_{11}=0$,","dimension of the space of all symmetric matrices with trace  and ,",0 a_{11}=0,"I want to know the dimension of the space of all symmetric matrices with trace $0$ and $a_{11}=0$, I can show that the dimension of space of all symmetric matrices $S$ is $n(n+1)/2$, now I give a linear map $T\colon S\rightarrow\mathbb{R}^2$ by $T(A)=(a_{11},\operatorname{trace}(A))$ so the kernel is exactly the space I want, so now it is enough to show the map is surjective so that I can apply rank nullity theorem. am I in right path? in that case $\dim\ker(T)=n(n+1)/2 -2$.","I want to know the dimension of the space of all symmetric matrices with trace $0$ and $a_{11}=0$, I can show that the dimension of space of all symmetric matrices $S$ is $n(n+1)/2$, now I give a linear map $T\colon S\rightarrow\mathbb{R}^2$ by $T(A)=(a_{11},\operatorname{trace}(A))$ so the kernel is exactly the space I want, so now it is enough to show the map is surjective so that I can apply rank nullity theorem. am I in right path? in that case $\dim\ker(T)=n(n+1)/2 -2$.",,"['linear-algebra', 'vector-spaces']"
67,Characterizing a real symmetric matrix $A$ as $A = XX^T - YY^T$,Characterizing a real symmetric matrix  as,A A = XX^T - YY^T,"In my personal research and quest to better understand the subject, I have noticed something concerning the Cholesky factorization of symmetric matrices. Everything I have read states that a symmetric real matrix has such a factorization only if it is positive definite. Assuming first that the matrix may be factored in LU form (lower triangular times an upper triangular) which is possible for ""most"" matrices (since it requires all principal minors to have non-zero determinant). Writing $A=LU$ gives the possibility for the Cholesky factorization with use of a diagonal matrix with the purpose of setting the diagonals of both the L and U matrices to be equal: $$A=L \underbrace{DD^{-1}}_I U$$ And from the original symmetry the two factors are magically transposes of each other. Now I do notice that if the mattrix $A$ is not positive definite (and I do not mind forcing it to be full rank as well) then $U$ has negative elements along the diagonal, and as such $D$ then needs complex values to work. I am fully aware that if such $D$ is chosen that the $A=W^TW \ne W^*W$, in other words it is not factored as a matrix and its conjugate -transpose, but for the formula $A = XX^T - YY^T$ it is not needed to be as such. Since the columns of $LD$ are either a pure real or a pure imaginary value (if this isn't too obvious, try some simple examples first maybe), the columns can further be ordered to the form of $$LD = \pmatrix{X & iY}$$ Which then gives (with the same and symmetric row ordering or the other factor): $$A=(LD)(LD)^T=\pmatrix{ X & iY} \pmatrix{X^T \\ iY^T}=XX^T - YY^T$$ which is the title formula of my question. This seems like a natural separation of the ""positive part"" and the ""negative part"" since all values in the final form are strictly real. It separates the symmetric matrix into positive definite symmetric and negative definite symmetric. I have not seen this before, and as I stated in the first paragraph, I always hear that the Cholesky factorization just does not exist unless the matrix is positive definite. Without considering the cases that do not have LU factorization for now, is this formula even at all known? Is there an error in my argument that it exists? I will be interested in any specific and good examples or references (please if ""search for such and such term"" is your answer, keep it short as I have done much searching already, and if your suggestion is new, then I would like a good explanation or just a short ""search for such and such"".) I will post this question and not look at answers for 6 hours from posting (work time), so feel free to take your time with your response, and thank you for reading this far. -- EDIT What interests me about the equation is the possibility of separating the negative eigenvalues from the positive ones. The answers so far are pointing out the obvious separation, but I like that the formula is one that does not calculate eigenvalues first. It looks to me like it separates the eigenvalue problem into two smaller sets, a sort of binary search on all eigenvalues at once, with just the computational cost of LU factoring. It also looks like a good possibility for a norm. This actually may reduce to another norm, or need adjustment (like adding a power 2 or something similar) but consider: $$\sqrt{|XX^T|+|YY^T|}$$ I submit that this is a norm (without proof at the moment) since it is the natural extension of the ""complex square root"" of the matrix $A$ as $\pmatrix{ X & iY}$ This all seems like logical directions to me, and would like to know if anything is looking familiar to anyone as if it has been done before? --- EDIT #2 I see now that $XY^T=\mathbf 0$ is necessary for the Cholesky form to be the same as the eigenvalue form of this equation. This is patently untrue in the Cholesky factoring. I was hoping the separation of two positive semi-definite symmetric matrices was unique, since then I would be guaranteed that it was the eigen-decomposition. The Cholesky finds a unique one within its range of solutions, but I now see it is not the same. I will be crediting user dineshdileep within a few days if no better answer comes along.","In my personal research and quest to better understand the subject, I have noticed something concerning the Cholesky factorization of symmetric matrices. Everything I have read states that a symmetric real matrix has such a factorization only if it is positive definite. Assuming first that the matrix may be factored in LU form (lower triangular times an upper triangular) which is possible for ""most"" matrices (since it requires all principal minors to have non-zero determinant). Writing $A=LU$ gives the possibility for the Cholesky factorization with use of a diagonal matrix with the purpose of setting the diagonals of both the L and U matrices to be equal: $$A=L \underbrace{DD^{-1}}_I U$$ And from the original symmetry the two factors are magically transposes of each other. Now I do notice that if the mattrix $A$ is not positive definite (and I do not mind forcing it to be full rank as well) then $U$ has negative elements along the diagonal, and as such $D$ then needs complex values to work. I am fully aware that if such $D$ is chosen that the $A=W^TW \ne W^*W$, in other words it is not factored as a matrix and its conjugate -transpose, but for the formula $A = XX^T - YY^T$ it is not needed to be as such. Since the columns of $LD$ are either a pure real or a pure imaginary value (if this isn't too obvious, try some simple examples first maybe), the columns can further be ordered to the form of $$LD = \pmatrix{X & iY}$$ Which then gives (with the same and symmetric row ordering or the other factor): $$A=(LD)(LD)^T=\pmatrix{ X & iY} \pmatrix{X^T \\ iY^T}=XX^T - YY^T$$ which is the title formula of my question. This seems like a natural separation of the ""positive part"" and the ""negative part"" since all values in the final form are strictly real. It separates the symmetric matrix into positive definite symmetric and negative definite symmetric. I have not seen this before, and as I stated in the first paragraph, I always hear that the Cholesky factorization just does not exist unless the matrix is positive definite. Without considering the cases that do not have LU factorization for now, is this formula even at all known? Is there an error in my argument that it exists? I will be interested in any specific and good examples or references (please if ""search for such and such term"" is your answer, keep it short as I have done much searching already, and if your suggestion is new, then I would like a good explanation or just a short ""search for such and such"".) I will post this question and not look at answers for 6 hours from posting (work time), so feel free to take your time with your response, and thank you for reading this far. -- EDIT What interests me about the equation is the possibility of separating the negative eigenvalues from the positive ones. The answers so far are pointing out the obvious separation, but I like that the formula is one that does not calculate eigenvalues first. It looks to me like it separates the eigenvalue problem into two smaller sets, a sort of binary search on all eigenvalues at once, with just the computational cost of LU factoring. It also looks like a good possibility for a norm. This actually may reduce to another norm, or need adjustment (like adding a power 2 or something similar) but consider: $$\sqrt{|XX^T|+|YY^T|}$$ I submit that this is a norm (without proof at the moment) since it is the natural extension of the ""complex square root"" of the matrix $A$ as $\pmatrix{ X & iY}$ This all seems like logical directions to me, and would like to know if anything is looking familiar to anyone as if it has been done before? --- EDIT #2 I see now that $XY^T=\mathbf 0$ is necessary for the Cholesky form to be the same as the eigenvalue form of this equation. This is patently untrue in the Cholesky factoring. I was hoping the separation of two positive semi-definite symmetric matrices was unique, since then I would be guaranteed that it was the eigen-decomposition. The Cholesky finds a unique one within its range of solutions, but I now see it is not the same. I will be crediting user dineshdileep within a few days if no better answer comes along.",,"['linear-algebra', 'matrices', 'algorithms', 'eigenvalues-eigenvectors']"
68,"Sufficient conditions for symmetry of arbitrary product of real, symmetric, positive semidefinite matrices","Sufficient conditions for symmetry of arbitrary product of real, symmetric, positive semidefinite matrices",,"It is straightforward to prove that the product $A_1 A_2\cdots A_k$ of $k$ (different) symmetric, real, positive semidefinite matrices is also symmetric if $A_i A_j=A_j A_i$ for all $i,j$. Moreover, it is well-known that for the case $k=2$, this pairwise commutativity condition is also a sufficient condition for symmetry of the matrix product. My question is the following: Is there a result for $k>2$ concerning sufficient conditions for the symmetry of the product $A_1 A_2\cdots A_k$ of $k$ symmetric, real, positive semidefinite matrices? I have a set of $k$ matrices whose product I know is symmetric, but I would like to know if there's a result in the literature placing any restrictions on the individual matrices $A_i$. I suspect it has to do with pairwise commutativity, but have not been able to figure it out. Thanks in advance for any insights!","It is straightforward to prove that the product $A_1 A_2\cdots A_k$ of $k$ (different) symmetric, real, positive semidefinite matrices is also symmetric if $A_i A_j=A_j A_i$ for all $i,j$. Moreover, it is well-known that for the case $k=2$, this pairwise commutativity condition is also a sufficient condition for symmetry of the matrix product. My question is the following: Is there a result for $k>2$ concerning sufficient conditions for the symmetry of the product $A_1 A_2\cdots A_k$ of $k$ symmetric, real, positive semidefinite matrices? I have a set of $k$ matrices whose product I know is symmetric, but I would like to know if there's a result in the literature placing any restrictions on the individual matrices $A_i$. I suspect it has to do with pairwise commutativity, but have not been able to figure it out. Thanks in advance for any insights!",,"['linear-algebra', 'matrices']"
69,Matrix groups generated by translation and inversion in the unit sphere,Matrix groups generated by translation and inversion in the unit sphere,,"Let $\alpha$ be algebraic over $\mathbb{Q}$, and consider the subgroup $G$ of $\mathrm{SL}_2(\mathbb{C})$ generated by inversion in the unit sphere and translation by $\alpha$.  That is, consider $G=\langle S, T \rangle$, in which $$ S:= \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right) \quad \text{and} \quad T:= \left( \begin{smallmatrix} 1 & \alpha \\ 0 & 1 \end{smallmatrix}\right).$$ For which $\alpha$ is the structure of $G$ known?  Of particular interest to me is the following: for which $\alpha$ do we obtain $\mathrm{SL}_2(\mathbb{Z}) \subset G$? Perhaps this occurs iff $1/\alpha$ is an algebraic integer?","Let $\alpha$ be algebraic over $\mathbb{Q}$, and consider the subgroup $G$ of $\mathrm{SL}_2(\mathbb{C})$ generated by inversion in the unit sphere and translation by $\alpha$.  That is, consider $G=\langle S, T \rangle$, in which $$ S:= \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right) \quad \text{and} \quad T:= \left( \begin{smallmatrix} 1 & \alpha \\ 0 & 1 \end{smallmatrix}\right).$$ For which $\alpha$ is the structure of $G$ known?  Of particular interest to me is the following: for which $\alpha$ do we obtain $\mathrm{SL}_2(\mathbb{Z}) \subset G$? Perhaps this occurs iff $1/\alpha$ is an algebraic integer?",,"['linear-algebra', 'abstract-algebra', 'algebraic-number-theory']"
70,"Why is $U(n,n)= GL(2n, \mathbb{C}) \cap O(2n,2n)$?",Why is ?,"U(n,n)= GL(2n, \mathbb{C}) \cap O(2n,2n)","I have read the statement above. I tried to prove it but I could not. For me, $O(2n,2n)$ already a subgroup of $GL(2n,\mathbb{C})$.","I have read the statement above. I tried to prove it but I could not. For me, $O(2n,2n)$ already a subgroup of $GL(2n,\mathbb{C})$.",,"['linear-algebra', 'lie-groups']"
71,"Given the error in the cg-method, calculate a lower bound for the condition number","Given the error in the cg-method, calculate a lower bound for the condition number",,"Edit: If any information is missing, please tell me and I'll edit the question. Thanks again! The conjugate gradient (cg) method was applied to a positive definite Matrix $A$. It is only known that $||e||_A=1$ and $||e^{10}||_A=2^{-9}$ (where $e$ is the error $||e^k||_A= ||x-x^k||_A$). Calculate with this information a lower bound for $κ(A)$ (where $κ$ is the condition number) and compare it with the equation   $$k \geq \frac{1}{2}(\sqrt{κ(A)}\ln(2/ε))$$   where ε is the factor by which the error is reduced, defined as   $$||e^k||_A= ||x-x^k||_A \leq ε||e^0||_A$$ Here's what I have so far. If I have understood it correctly $ε=2^{-9}/1=2^{-9}$. I however don't understand how only from that can I calculate the condition number. Doesn't the condition number require knowing the matrix and it's inverse? $κ=||A||||A^{-1}||$ I have calculated what $κ$ should be using the equation $k \geq \frac{1}{2}(\sqrt{κ(A)}\ln(2/ε))$ and I got $$10 \geq \frac{6.93}{2}(\sqrt{κ(A)})$$ $$2.89 \geq \sqrt{κ(A)}$$ $$8.33 \geq κ(A)$$ How can I move forward? Thanks in advance!","Edit: If any information is missing, please tell me and I'll edit the question. Thanks again! The conjugate gradient (cg) method was applied to a positive definite Matrix $A$. It is only known that $||e||_A=1$ and $||e^{10}||_A=2^{-9}$ (where $e$ is the error $||e^k||_A= ||x-x^k||_A$). Calculate with this information a lower bound for $κ(A)$ (where $κ$ is the condition number) and compare it with the equation   $$k \geq \frac{1}{2}(\sqrt{κ(A)}\ln(2/ε))$$   where ε is the factor by which the error is reduced, defined as   $$||e^k||_A= ||x-x^k||_A \leq ε||e^0||_A$$ Here's what I have so far. If I have understood it correctly $ε=2^{-9}/1=2^{-9}$. I however don't understand how only from that can I calculate the condition number. Doesn't the condition number require knowing the matrix and it's inverse? $κ=||A||||A^{-1}||$ I have calculated what $κ$ should be using the equation $k \geq \frac{1}{2}(\sqrt{κ(A)}\ln(2/ε))$ and I got $$10 \geq \frac{6.93}{2}(\sqrt{κ(A)})$$ $$2.89 \geq \sqrt{κ(A)}$$ $$8.33 \geq κ(A)$$ How can I move forward? Thanks in advance!",,"['linear-algebra', 'numerical-methods']"
72,Kernel of the Lie bracket,Kernel of the Lie bracket,,"Let $\mathfrak{g}$ be a dimension 3 Lie algebra and $[\quad,\quad]$ be a rank 1 map from $\bigwedge^{2}\mathfrak{g} \rightarrow \mathfrak{g}$. In this case, the kernel of $[\quad,\quad]$ is $3 - 1 = 2$ dimensional. Why does this mean that for some $X \in \mathfrak{g}$, the kernel consists of all vectors of the form $X \wedge Y$ with $Y$ ranging over all of $\mathfrak{g}$?","Let $\mathfrak{g}$ be a dimension 3 Lie algebra and $[\quad,\quad]$ be a rank 1 map from $\bigwedge^{2}\mathfrak{g} \rightarrow \mathfrak{g}$. In this case, the kernel of $[\quad,\quad]$ is $3 - 1 = 2$ dimensional. Why does this mean that for some $X \in \mathfrak{g}$, the kernel consists of all vectors of the form $X \wedge Y$ with $Y$ ranging over all of $\mathfrak{g}$?",,"['linear-algebra', 'lie-algebras']"
73,How to compute isomorphism $V \simeq V^{**}$ (in Haskell)?,How to compute isomorphism  (in Haskell)?,V \simeq V^{**},"Since there is a canonical isomorphism between vector space $V$ and his dual dual space $V^{**}$, $\dim V \in \mathbb N \;$, I want to write it as a Haskell function. This function is going to have a type of $$ F : \left( \left(V \to \mathbb K \right) \to \mathbb K \right) \to V $$ Is it possible at all?","Since there is a canonical isomorphism between vector space $V$ and his dual dual space $V^{**}$, $\dim V \in \mathbb N \;$, I want to write it as a Haskell function. This function is going to have a type of $$ F : \left( \left(V \to \mathbb K \right) \to \mathbb K \right) \to V $$ Is it possible at all?",,"['linear-algebra', 'computability', 'constructive-mathematics']"
74,Map preserving indefinite scalar product must be linear,Map preserving indefinite scalar product must be linear,,"Let $V$ be a finite dimensional real vector space and $\langle\cdot,\cdot \rangle$ be a positive definite scalar product in $V$. It is well know that if a map $T:V \to V$ preserves $\langle\cdot,\cdot \rangle$ (i.e. $\langle Tv,Tw \rangle = \langle v,w \rangle$ for all $v,w \in V$) then $T$ must be linear. One way of seing this is by computing $||x+y-z||^2 = ||x||^2 + ||y||^2 + ||z||^2 + 2\langle x,y\rangle - 2\langle x,z\rangle - 2\langle y,z\rangle$ $||Tx+Ty-Tz||^2 = ||Tx||^2 + ||Ty||^2 + ||Tz||^2 + 2\langle Tx,Ty\rangle - 2\langle Tx,Tz\rangle - 2\langle Ty,Tz\rangle$. Comparing both sides we get $||Tx+Ty-Tz|| = ||x+y-z||$ for all $x,y,z \in V$ and setting $z=x+y$ we obtain $T(x+y)=Tx + Ty$. By continuity its not hard to show that $T(\lambda x) = \lambda Tx$ for $\lambda \in \mathbb R, x \in V$. Now let $g:V \times V \to \mathbb R$ be an indefinite scalar product (that is, a non-degenrate symmetric bilinear form) and $T:V \to V$  a map preserving $g$, i.e., $g (Tv,Tw)  = g(v,w)$ for all $v,w \in V$. Is it true that $T$ must be linear in this case? The above computation no longer aplies, since vectors with zero norm need not to be zero anymore. The only thing one can say is that $T(x+y)-Tx - Ty$ is light-like for every $x,y \in V$. I think I managed to write a proof using the fact that $T$ is $C^1$ and surjective. Does anyone know if these conditions are necessary? I wasn't able to find any counter example.","Let $V$ be a finite dimensional real vector space and $\langle\cdot,\cdot \rangle$ be a positive definite scalar product in $V$. It is well know that if a map $T:V \to V$ preserves $\langle\cdot,\cdot \rangle$ (i.e. $\langle Tv,Tw \rangle = \langle v,w \rangle$ for all $v,w \in V$) then $T$ must be linear. One way of seing this is by computing $||x+y-z||^2 = ||x||^2 + ||y||^2 + ||z||^2 + 2\langle x,y\rangle - 2\langle x,z\rangle - 2\langle y,z\rangle$ $||Tx+Ty-Tz||^2 = ||Tx||^2 + ||Ty||^2 + ||Tz||^2 + 2\langle Tx,Ty\rangle - 2\langle Tx,Tz\rangle - 2\langle Ty,Tz\rangle$. Comparing both sides we get $||Tx+Ty-Tz|| = ||x+y-z||$ for all $x,y,z \in V$ and setting $z=x+y$ we obtain $T(x+y)=Tx + Ty$. By continuity its not hard to show that $T(\lambda x) = \lambda Tx$ for $\lambda \in \mathbb R, x \in V$. Now let $g:V \times V \to \mathbb R$ be an indefinite scalar product (that is, a non-degenrate symmetric bilinear form) and $T:V \to V$  a map preserving $g$, i.e., $g (Tv,Tw)  = g(v,w)$ for all $v,w \in V$. Is it true that $T$ must be linear in this case? The above computation no longer aplies, since vectors with zero norm need not to be zero anymore. The only thing one can say is that $T(x+y)-Tx - Ty$ is light-like for every $x,y \in V$. I think I managed to write a proof using the fact that $T$ is $C^1$ and surjective. Does anyone know if these conditions are necessary? I wasn't able to find any counter example.",,"['linear-algebra', 'inner-products', 'quadratic-forms']"
75,When does there exist an isometry that switches two subspaces?,When does there exist an isometry that switches two subspaces?,,"Let $V$ be a real vector space of finite dimension and let  $\langle \cdot, \cdot \rangle$ be a non-degenerate symmetric  bilinear form on $V$. Let $U, W \subseteq V$ be linear  subspaces such that the bilinear forms $\langle \cdot, \cdot  \rangle \vert_U$ and $\langle \cdot, \cdot \rangle \vert_W$  are isometric and non-degenerate. It is quite easy to prove that there exists an isometry $f$  of $V$ such that $f(U) = W$. I would like to know whether  there are some sufficient conditions such that there exists  an isometry $f$ of $V$ such that $f(U) = W$ and $f(W) = U$. Thanks to all! EDIT. Since I have not received any answer, I want to  say that I'm interested in a very particular case: $V =  \mathbb{R}^{n+1}$, $\langle \cdot , \cdot \rangle$ is the  Lorentzian scalar product on $\mathbb{R}^{n+1}$, i.e.  $\langle x, y \rangle = \sum_{i=1}^n x_i y_i - x_{n+1}  y_{n+1}$, and $U$ and $W$ are $2$-dimensional subspaces of  $\mathbb R^{n+1}$ such that $\langle \cdot, \cdot \rangle \vert_U$ and $\langle \cdot, \cdot \rangle \vert_W$  have signature $(1,1)$.","Let $V$ be a real vector space of finite dimension and let  $\langle \cdot, \cdot \rangle$ be a non-degenerate symmetric  bilinear form on $V$. Let $U, W \subseteq V$ be linear  subspaces such that the bilinear forms $\langle \cdot, \cdot  \rangle \vert_U$ and $\langle \cdot, \cdot \rangle \vert_W$  are isometric and non-degenerate. It is quite easy to prove that there exists an isometry $f$  of $V$ such that $f(U) = W$. I would like to know whether  there are some sufficient conditions such that there exists  an isometry $f$ of $V$ such that $f(U) = W$ and $f(W) = U$. Thanks to all! EDIT. Since I have not received any answer, I want to  say that I'm interested in a very particular case: $V =  \mathbb{R}^{n+1}$, $\langle \cdot , \cdot \rangle$ is the  Lorentzian scalar product on $\mathbb{R}^{n+1}$, i.e.  $\langle x, y \rangle = \sum_{i=1}^n x_i y_i - x_{n+1}  y_{n+1}$, and $U$ and $W$ are $2$-dimensional subspaces of  $\mathbb R^{n+1}$ such that $\langle \cdot, \cdot \rangle \vert_U$ and $\langle \cdot, \cdot \rangle \vert_W$  have signature $(1,1)$.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'hyperbolic-geometry']"
76,Correspondence between two matrices,Correspondence between two matrices,,"Suppose $B$ is a positive definite matrix with determinant $1 $ and  $$ A = \frac{1}{2} \int_0^\infty \frac{(B+sI)^{-1}}{\sqrt{\mbox{det}(B+sI)}} ds $$ Then, how does one prove that this provides a one to one onto correspondence between positive definite matrices $B$ with determinant $1$ and positive definite matrices $A$ with trace $1$. Thank you very much.","Suppose $B$ is a positive definite matrix with determinant $1 $ and  $$ A = \frac{1}{2} \int_0^\infty \frac{(B+sI)^{-1}}{\sqrt{\mbox{det}(B+sI)}} ds $$ Then, how does one prove that this provides a one to one onto correspondence between positive definite matrices $B$ with determinant $1$ and positive definite matrices $A$ with trace $1$. Thank you very much.",,['linear-algebra']
77,Symmetric matrix decomposition with orthonormal basis of non-eigenvectors,Symmetric matrix decomposition with orthonormal basis of non-eigenvectors,,"I like to understand the following transformation found in documentation for deriving Kalman filter . Abstract Formulation : Given 2 symmetric matrices $A$ ,$B$ $\in$  $\mathbb R^{3,3}$ with $A \ne B$ and a set of orthonormal Eigenvectors ($u_1$, $u_2$, $u_3$) from some other matrix $B$ (not $A$!). Because the matrices are symmetric it is clear that $B$ can be decomposed to $B = U\Lambda U^t$.  Now there is stated that A can be written as: $A$ = ($u_1^t$A$u_1$)$u_1$$u_1^t$ + ($u_2^t$A$u_2$)$u_2$$u_2^t$ + ($u_3^t$A$u_3$)$u_3$$u_3^t$ i.e. with the ""foreign"" Eigenvectors. Concrete Situation : In the original equation the above mentioned $A$ is defined as $H_kP_k^-H_k^t$ + $R_a$, where $P^-$ is the a priori estimation error covariance and $R_a$ is the sensor noise error covariance matrix. $H_k$ has 3x9 dimension and contains some ""more abstract"" content with rotation matrix of a quaternion multiplied with cross product operator of gravity vector (0,0,g). As far as I can see, the term ($H_kP_k^-H_k^t$ + $R_a$) does not lead to a diagonal matrix and this seems to be irrelevant. What I called $B$ is actually the signal's overall error error covariance named $U_k$ From original paper: Because $U_k$ covariance matrix cannot be obtained at this point in time (a priori estimation), it is approximated by the average of the last M steps i.e. from k-M to k-1. The signal itself might be fluctuating considerably because sometimes there is external acceleration at other times there isn't thus sensor noise is the only thing be measured. Assumption (thanks to Calle's and joriki's comments): The eigendecomposition of $U_k$ is related to PCA Principal component analysis (an easier one here ). The most interesting cases are all those measurements with strong accelerations i.e. $U_k$ is much greater than the remaining term. So this decomposition of the 2nd term transforms it (approximatively?) towards the direction of the strongest signal. Thus $\lambda - \mu$ helps to detect these situations respectively distinguish them from phases with no signal aside from noise. Does this explanation makes sense? Can this procedure of approximating with ""wrong"" eigenvectors and -values be applied and compares like with like? What is the name of this matrix decomposition taking not their own eigenvectors? What is about the error? Thanks for helping Kay PS: Title changed from ""Symmetric matrix multiplied with kind of orthonormal basis""","I like to understand the following transformation found in documentation for deriving Kalman filter . Abstract Formulation : Given 2 symmetric matrices $A$ ,$B$ $\in$  $\mathbb R^{3,3}$ with $A \ne B$ and a set of orthonormal Eigenvectors ($u_1$, $u_2$, $u_3$) from some other matrix $B$ (not $A$!). Because the matrices are symmetric it is clear that $B$ can be decomposed to $B = U\Lambda U^t$.  Now there is stated that A can be written as: $A$ = ($u_1^t$A$u_1$)$u_1$$u_1^t$ + ($u_2^t$A$u_2$)$u_2$$u_2^t$ + ($u_3^t$A$u_3$)$u_3$$u_3^t$ i.e. with the ""foreign"" Eigenvectors. Concrete Situation : In the original equation the above mentioned $A$ is defined as $H_kP_k^-H_k^t$ + $R_a$, where $P^-$ is the a priori estimation error covariance and $R_a$ is the sensor noise error covariance matrix. $H_k$ has 3x9 dimension and contains some ""more abstract"" content with rotation matrix of a quaternion multiplied with cross product operator of gravity vector (0,0,g). As far as I can see, the term ($H_kP_k^-H_k^t$ + $R_a$) does not lead to a diagonal matrix and this seems to be irrelevant. What I called $B$ is actually the signal's overall error error covariance named $U_k$ From original paper: Because $U_k$ covariance matrix cannot be obtained at this point in time (a priori estimation), it is approximated by the average of the last M steps i.e. from k-M to k-1. The signal itself might be fluctuating considerably because sometimes there is external acceleration at other times there isn't thus sensor noise is the only thing be measured. Assumption (thanks to Calle's and joriki's comments): The eigendecomposition of $U_k$ is related to PCA Principal component analysis (an easier one here ). The most interesting cases are all those measurements with strong accelerations i.e. $U_k$ is much greater than the remaining term. So this decomposition of the 2nd term transforms it (approximatively?) towards the direction of the strongest signal. Thus $\lambda - \mu$ helps to detect these situations respectively distinguish them from phases with no signal aside from noise. Does this explanation makes sense? Can this procedure of approximating with ""wrong"" eigenvectors and -values be applied and compares like with like? What is the name of this matrix decomposition taking not their own eigenvectors? What is about the error? Thanks for helping Kay PS: Title changed from ""Symmetric matrix multiplied with kind of orthonormal basis""",,"['linear-algebra', 'statistics', 'matrices', 'eigenvalues-eigenvectors']"
78,Spectral decomposition of a normal matrix,Spectral decomposition of a normal matrix,,"I'd like to find the spectral decomposition of $A$: $$A = \begin{pmatrix} 2-i & -1 & 0\\  -1 & 1-i & 1\\  0 & 1 & 2-i \end{pmatrix}$$ i.e. $A=\sum_{i}\lambda_i P_i$ where $P_i$ are the coordinate matrices (in the standard basis) of the corresponding orthogonal transformations in the spectral decomposition of $T_A$ and $\lambda_i$ are the eigenvalues. I started off by showing that $A$ is normal, piece of cake. Then found the eigenvalues of $A$, those are: $\lambda_1 = 2-i, \lambda_2 = 3-i, \lambda_3 = -i$. I tried using these known facts from the spectral theorem: $A=(2-i)P_1+(3-i)P_2-iP_3$ $I=P_1+P_2+P_3$ $\forall i\neq j, P_i P_j=0$ $P^*_i=P_i$ The only example I have in my book uses these but I couldn't get it to work here. The terms don't cancel out it seems. What else can I try?","I'd like to find the spectral decomposition of $A$: $$A = \begin{pmatrix} 2-i & -1 & 0\\  -1 & 1-i & 1\\  0 & 1 & 2-i \end{pmatrix}$$ i.e. $A=\sum_{i}\lambda_i P_i$ where $P_i$ are the coordinate matrices (in the standard basis) of the corresponding orthogonal transformations in the spectral decomposition of $T_A$ and $\lambda_i$ are the eigenvalues. I started off by showing that $A$ is normal, piece of cake. Then found the eigenvalues of $A$, those are: $\lambda_1 = 2-i, \lambda_2 = 3-i, \lambda_3 = -i$. I tried using these known facts from the spectral theorem: $A=(2-i)P_1+(3-i)P_2-iP_3$ $I=P_1+P_2+P_3$ $\forall i\neq j, P_i P_j=0$ $P^*_i=P_i$ The only example I have in my book uses these but I couldn't get it to work here. The terms don't cancel out it seems. What else can I try?",,"['linear-algebra', 'spectral-theory']"
79,eigen decomposition of an interesting matrix,eigen decomposition of an interesting matrix,,"Lets define: $U=\left \{  u_j\right \} , 1 \leq j\leq N= 2^{L},$ the set of all different binary sequences of length $L$. $V=\left \{  v_i\right \} , 1 \leq i\leq M=\binom{L}{k}2^{k},$ the set of all different gaped binary sequences with $k$ known bits and $L-k$ gaps. $A_{M*N}=[a_{i,j}]$ is a binary matrix defined as following: $$a_{i,j} = \left\{\begin{matrix} 1 & \text{if } v_i \text{ matches } u_j \\  0 & \text{otherwise } \end{matrix}\right.$$ now, the question is: What are the eigenvectors and eigenvalues of the matrix $S_{M*M}=AA^{T}$? Here is an example for $L=2, k=1$: $$U = \left \{  00,01,10,11\right \} $$ $$V = \left \{  0.,1.,.0,.1\right \}  ^*$$ $$ A = \begin{bmatrix} 1 & 1 & 0 &0 \\  0 & 0 & 1 &1 \\  1 & 0 & 1 &0 \\  0 & 1 & 0 &1  \end{bmatrix}$$ $$ S = \begin{bmatrix} 2 & 0 & 1 &1 \\  0 & 2 & 1 &1 \\  1 & 1 & 2 &0 \\  1 & 1 & 0 &2  \end{bmatrix}$$ For the special case $k=1$, is has been previously solved by joriki and the solution can be found here . See the same reference for a graph analogy of matrix $S$. Furthermore, it has been shown here by joriki that: $$\text{rank}(AA^{T})=\text{rank}(A)=\sum_{m=0}^k\left({L\atop m}\right)\;\;$$ As for the eigenvalues, numerical values suggest that $AA^{T}$ has $\binom{L}{m}$ eigenvalues equal to $\binom{L-m}{k-m}2^{g}, m=0,..,k$, where $g=L-k$ is the number of gaps. any comments or suggestion is appreciated. $^{*}$ here dots denote gaps. a gap can take any value, and each gaped sequence with $k$ known bits and $(L−K)$ gaps in $V$, exactly matches to $2^{L−k}$ sequences in U, hence the sum of elements in each row of $A$ is $2^{L−k}$.","Lets define: $U=\left \{  u_j\right \} , 1 \leq j\leq N= 2^{L},$ the set of all different binary sequences of length $L$. $V=\left \{  v_i\right \} , 1 \leq i\leq M=\binom{L}{k}2^{k},$ the set of all different gaped binary sequences with $k$ known bits and $L-k$ gaps. $A_{M*N}=[a_{i,j}]$ is a binary matrix defined as following: $$a_{i,j} = \left\{\begin{matrix} 1 & \text{if } v_i \text{ matches } u_j \\  0 & \text{otherwise } \end{matrix}\right.$$ now, the question is: What are the eigenvectors and eigenvalues of the matrix $S_{M*M}=AA^{T}$? Here is an example for $L=2, k=1$: $$U = \left \{  00,01,10,11\right \} $$ $$V = \left \{  0.,1.,.0,.1\right \}  ^*$$ $$ A = \begin{bmatrix} 1 & 1 & 0 &0 \\  0 & 0 & 1 &1 \\  1 & 0 & 1 &0 \\  0 & 1 & 0 &1  \end{bmatrix}$$ $$ S = \begin{bmatrix} 2 & 0 & 1 &1 \\  0 & 2 & 1 &1 \\  1 & 1 & 2 &0 \\  1 & 1 & 0 &2  \end{bmatrix}$$ For the special case $k=1$, is has been previously solved by joriki and the solution can be found here . See the same reference for a graph analogy of matrix $S$. Furthermore, it has been shown here by joriki that: $$\text{rank}(AA^{T})=\text{rank}(A)=\sum_{m=0}^k\left({L\atop m}\right)\;\;$$ As for the eigenvalues, numerical values suggest that $AA^{T}$ has $\binom{L}{m}$ eigenvalues equal to $\binom{L-m}{k-m}2^{g}, m=0,..,k$, where $g=L-k$ is the number of gaps. any comments or suggestion is appreciated. $^{*}$ here dots denote gaps. a gap can take any value, and each gaped sequence with $k$ known bits and $(L−K)$ gaps in $V$, exactly matches to $2^{L−k}$ sequences in U, hence the sum of elements in each row of $A$ is $2^{L−k}$.",,"['linear-algebra', 'combinatorics', 'matrices', 'graph-theory', 'eigenvalues-eigenvectors']"
80,On multiplying quaternion matrices,On multiplying quaternion matrices,,"Both matrix multiplication and quaternion multiplication are non-commutative; hence the use of terms like ""premultiplication"" and ""postmultiplication"". After encountering the concept of ""quaternion matrices"", I am a bit puzzled as to how one may multiply two of these things, since there are at least four ways to do this. Some searching has netted this paper , but not having any access to it, I have no way towards enlightenment except to ask this question here. If there are indeed these four ways to multiply quaternion matrices, how does one figure out which one to use in a situation, and what shorthand might be used to talk about a particular version of a multiplication?","Both matrix multiplication and quaternion multiplication are non-commutative; hence the use of terms like ""premultiplication"" and ""postmultiplication"". After encountering the concept of ""quaternion matrices"", I am a bit puzzled as to how one may multiply two of these things, since there are at least four ways to do this. Some searching has netted this paper , but not having any access to it, I have no way towards enlightenment except to ask this question here. If there are indeed these four ways to multiply quaternion matrices, how does one figure out which one to use in a situation, and what shorthand might be used to talk about a particular version of a multiplication?",,"['linear-algebra', 'quaternions', 'matrices']"
81,"If $\varphi : \mathcal{L}(V) \to \mathbb{F}$ such that $\varphi(ST) = \varphi(S)\varphi(T)$ for all $S, T \in \mathcal{L}(V)$, then $\varphi = 0$.","If  such that  for all , then .","\varphi : \mathcal{L}(V) \to \mathbb{F} \varphi(ST) = \varphi(S)\varphi(T) S, T \in \mathcal{L}(V) \varphi = 0","Exercise. Suppose $V$ is finite-dimensional with $\text{dim } V > 1$ . Show that if $\varphi : \mathcal{L}(V) \to \mathbb{F}$ is a linear map such that $\varphi(ST) = \varphi(S)\varphi(T)$ for all $S, T \in \mathcal{L}(V)$ , then $\varphi = 0$ . Source. Linear Algebra Done Right, 4th edition, by Sheldon Axler. Chapter 3, section 3B, exercise 32. Notation used. $\mathcal{L}(V)$ is the set of all linear transformations from a vector space $V$ to $V$ . $\mathbb{F}$ represents either the set of real numbers, $\mathbb{R}$ , or the set of complex numbers, $\mathbb{C}$ . What I've tried. Axler suggests to use the definition of a two-sided ideal which goes as follows: A subspace $\mathcal{E}$ of $\mathcal{L}(V)$ is called a two-sided ideal of $\mathcal{L}(V)$ if $TE \in \mathcal{E}$ and $ET \in \mathcal{E}$ for all $E \in \mathcal{E}$ and all $T \in \mathcal{L}(V)$ . I have shown before that $\mathcal{L}(V)$ is a two-sided ideal of $\mathcal{L}(V)$ . This implies that for any $S,T \in \mathcal{L}(V)$ , we have $ST \in \mathcal{L}(V)$ and $TS \in \mathcal{L}(V)$ . This implies that $\varphi(TS)$ is well-defined. Since $\varphi(ST) = \varphi(S)\varphi(T)$ , we must have then $\varphi(TS) = \varphi(T)\varphi(S)$ . It is easy to show that $$\varphi(S)\varphi(T) = \varphi(T)\varphi(S)$$ This implies that $\varphi(S)\varphi(T) - \varphi(T)\varphi(S) = 0$ which, by the distributive property of linear maps, grants us $$\varphi(S)[\varphi(T) - \varphi(T)] = 0$$ Question. Obviously, this will not lead me to the conclusion $\varphi = 0$ . Is this the direction I am meant to go in? Could someone give me a hint? The only place where I have used the fact that $V$ is finite-dimensional is when showing $\varphi(S)\varphi(T) = \varphi(T)\varphi(S)$ (I omitted this here, but have shown it on paper). Am I supposed to use it elsewhere to help me?","Exercise. Suppose is finite-dimensional with . Show that if is a linear map such that for all , then . Source. Linear Algebra Done Right, 4th edition, by Sheldon Axler. Chapter 3, section 3B, exercise 32. Notation used. is the set of all linear transformations from a vector space to . represents either the set of real numbers, , or the set of complex numbers, . What I've tried. Axler suggests to use the definition of a two-sided ideal which goes as follows: A subspace of is called a two-sided ideal of if and for all and all . I have shown before that is a two-sided ideal of . This implies that for any , we have and . This implies that is well-defined. Since , we must have then . It is easy to show that This implies that which, by the distributive property of linear maps, grants us Question. Obviously, this will not lead me to the conclusion . Is this the direction I am meant to go in? Could someone give me a hint? The only place where I have used the fact that is finite-dimensional is when showing (I omitted this here, but have shown it on paper). Am I supposed to use it elsewhere to help me?","V \text{dim } V > 1 \varphi : \mathcal{L}(V) \to \mathbb{F} \varphi(ST) = \varphi(S)\varphi(T) S, T \in \mathcal{L}(V) \varphi = 0 \mathcal{L}(V) V V \mathbb{F} \mathbb{R} \mathbb{C} \mathcal{E} \mathcal{L}(V) \mathcal{L}(V) TE \in \mathcal{E} ET \in
\mathcal{E} E \in \mathcal{E} T \in \mathcal{L}(V) \mathcal{L}(V) \mathcal{L}(V) S,T \in \mathcal{L}(V) ST \in \mathcal{L}(V) TS \in \mathcal{L}(V) \varphi(TS) \varphi(ST) = \varphi(S)\varphi(T) \varphi(TS) = \varphi(T)\varphi(S) \varphi(S)\varphi(T) = \varphi(T)\varphi(S) \varphi(S)\varphi(T) - \varphi(T)\varphi(S) = 0 \varphi(S)[\varphi(T) - \varphi(T)] = 0 \varphi = 0 V \varphi(S)\varphi(T) = \varphi(T)\varphi(S)","['linear-algebra', 'vector-spaces', 'linear-transformations']"
82,"Can a sufficiently large spanning set represent every vector, using fewer than n vectors?","Can a sufficiently large spanning set represent every vector, using fewer than n vectors?",,"Let $V$ be a $n$ -dimensional vector space and suppose we have a set $A$ of vectors which span $V$ . Let $q$ be the smallest number such that for all $x \in V$ , we can always write $x$ as a linear combination of $q$ vectors in $A$ . What is the relationship between $|A|$ and $q$ ? For example, here are the two examples: By basic linear algebra, we can have $|A| = n$ when $q = n$ . Let $\mathbb{F} = \mathbb{Z}/p\mathbb{Z}$ be a finite field and $V = \mathbb{F}^n$ . For any $q$ , I can construct $A$ with size $|A| = p^{n-q} + q - 1$ . Here it is: $$ A = \{(1, 0, \dots, 0, x_{q+1}, \dots, x_n) \mid x_{q+1}, \dots, x_n \in \mathbb{F}\} \cup \{e_2, \dots, e_q\},$$ where $e_2, \dots, e_q$ are standard basis vectors. This works because for any target $y = (y_1, \dots, y_n)$ , we have $$ y = y_1(1, 0, \dots, 0, y_{q+1}y_1^{-1}, \dots, y_ny_1^{-1}) + y_2e_2 + \dots + y_qe_q.$$ I have no clue if a smaller set $A$ exists. (edit: may not work when $y_1 = 0$ , thank you @jackson for pointing it out) An easier question might be: Can the set $A$ be finite if we desire $q < n$ , when $|\mathbb{F}| = \infty$ ? A possibly harder question that I am also interested in is: What if we have $\mathbb{Z}$ instead of a field?","Let be a -dimensional vector space and suppose we have a set of vectors which span . Let be the smallest number such that for all , we can always write as a linear combination of vectors in . What is the relationship between and ? For example, here are the two examples: By basic linear algebra, we can have when . Let be a finite field and . For any , I can construct with size . Here it is: where are standard basis vectors. This works because for any target , we have I have no clue if a smaller set exists. (edit: may not work when , thank you @jackson for pointing it out) An easier question might be: Can the set be finite if we desire , when ? A possibly harder question that I am also interested in is: What if we have instead of a field?","V n A V q x \in V x q A |A| q |A| = n q = n \mathbb{F} = \mathbb{Z}/p\mathbb{Z} V = \mathbb{F}^n q A |A| = p^{n-q} + q - 1  A = \{(1, 0, \dots, 0, x_{q+1}, \dots, x_n) \mid x_{q+1}, \dots, x_n \in \mathbb{F}\} \cup \{e_2, \dots, e_q\}, e_2, \dots, e_q y = (y_1, \dots, y_n)  y = y_1(1, 0, \dots, 0, y_{q+1}y_1^{-1}, \dots, y_ny_1^{-1}) + y_2e_2 + \dots + y_qe_q. A y_1 = 0 A q < n |\mathbb{F}| = \infty \mathbb{Z}","['linear-algebra', 'abstract-algebra']"
83,Are expressions considered Mathematical objects?,Are expressions considered Mathematical objects?,,"In linear algebra we learn about the idea of a set of 'polynomials' would this set be equivalent to a normal set such as the set of real numbers? The idea of sets (in my understanding) is that they can contain Mathematical objects (numbers, functions, other sets etc). Do we consider expressions as Mathematical objects? In which case is it more correct to say the following in terms of equality: 'The mathematical object that $x+1$ represents is the same as the mathematical object that $x+2-1$ represents'? Instead of: ' $x+1$ and $x+2-1$ are the same mathematical object' If expressions are objects in their own right? I would generally consider them 'syntactic objects' but the fact we can form sets with them suggests they are Mathematical Objects. Edit: I have learnt that in dealing with Polynomials we are dealing with mappings based on indeterminates, if we can have an expression as part of a set, something like ' $x+1$ is an element of A' is ambiguous, as are we referring to the expression or it's value? Is it possible to put an expression in a set?","In linear algebra we learn about the idea of a set of 'polynomials' would this set be equivalent to a normal set such as the set of real numbers? The idea of sets (in my understanding) is that they can contain Mathematical objects (numbers, functions, other sets etc). Do we consider expressions as Mathematical objects? In which case is it more correct to say the following in terms of equality: 'The mathematical object that represents is the same as the mathematical object that represents'? Instead of: ' and are the same mathematical object' If expressions are objects in their own right? I would generally consider them 'syntactic objects' but the fact we can form sets with them suggests they are Mathematical Objects. Edit: I have learnt that in dealing with Polynomials we are dealing with mappings based on indeterminates, if we can have an expression as part of a set, something like ' is an element of A' is ambiguous, as are we referring to the expression or it's value? Is it possible to put an expression in a set?",x+1 x+2-1 x+1 x+2-1 x+1,"['linear-algebra', 'algebra-precalculus', 'elementary-set-theory']"
84,Find all non-similar solutions of matrix equation,Find all non-similar solutions of matrix equation,,"Find all unique $($ not conjugate to each other by an element of $GL_2(\mathbb{Z}))$ matrices $A \in M_2(\mathbb{Z})$ , such that $A ^ 2 - 4A - I = 0$ , where $I$ is the identity matrix. From the equation it can be easily seen that matrix is a solution iff it has trace equal to $4$ and determinant equal to $-1$ . The main problem is to find all conjugacy classes. Since $\mathbb{Z}$ is not a field, Jordan, Frobenius and Smith normal forms are not applicable. I also tried to brute-force the problem and solve the equation $CA = BC$ , where $A$ and $B$ are two distinct solutions and $C \in GL_2(\mathbb{Z})$ , but this boils down to the system of 3 diophantine equations with 4 variables, one of which is not even linear (as in this paper, page 3) and I have no idea how to check existence of solutions for that. Another approach that I tried (again, without luck) was to decompose $A$ and $B$ into a product of $GL_2(\mathbb{Z})$ generators and check that they differ by a cyclic permutation (as listed here , point 6) but I was not able to do that in general for this family of matrices. I have also found a theorem (in this presentation by Svetlana Katok, slide 10), which is very cryptic to me, but I think it is impossible to use in practice. I feel that the answer is $\pmatrix{1 & 2 \\ 2 & 3}$ and $\pmatrix{0 & 1 \\ 1 & 4}$ because I could not find any counterexamples. Is there any simple way to find all conjugacy classes?","Find all unique not conjugate to each other by an element of matrices , such that , where is the identity matrix. From the equation it can be easily seen that matrix is a solution iff it has trace equal to and determinant equal to . The main problem is to find all conjugacy classes. Since is not a field, Jordan, Frobenius and Smith normal forms are not applicable. I also tried to brute-force the problem and solve the equation , where and are two distinct solutions and , but this boils down to the system of 3 diophantine equations with 4 variables, one of which is not even linear (as in this paper, page 3) and I have no idea how to check existence of solutions for that. Another approach that I tried (again, without luck) was to decompose and into a product of generators and check that they differ by a cyclic permutation (as listed here , point 6) but I was not able to do that in general for this family of matrices. I have also found a theorem (in this presentation by Svetlana Katok, slide 10), which is very cryptic to me, but I think it is impossible to use in practice. I feel that the answer is and because I could not find any counterexamples. Is there any simple way to find all conjugacy classes?",( GL_2(\mathbb{Z})) A \in M_2(\mathbb{Z}) A ^ 2 - 4A - I = 0 I 4 -1 \mathbb{Z} CA = BC A B C \in GL_2(\mathbb{Z}) A B GL_2(\mathbb{Z}) \pmatrix{1 & 2 \\ 2 & 3} \pmatrix{0 & 1 \\ 1 & 4},['linear-algebra']
85,Trying to understand the idea behind $T$-conductor,Trying to understand the idea behind -conductor,T,"I was reading Linear Algebra by Hoffman Kunze and I found a strange definition Let $W$ be an invariant subspace for $T\in \mathcal L(V)$ and let $\alpha$ be a vector in $V$ . The $T$ -conductor of $\alpha$ into $W$ is the set $S_T(\alpha; W)$ which consists of all polynomials $g$ (over the scalar field) such that $g(T)\alpha$ is in $W$ . The book also added In the special case $W=\{0\}$ the conductor is called the $T$ -annihilator of $\alpha$ . Now, my problem is I don't understand what's going on here. What's the idea behind these two definitions? What's the algebraic (or geometric) picture behind these ideas? I understand the definiton and the intuition behind all the terms used in defining a $T$ -conductor. The problem is I just can't connect them together. Please help me to do so. Also, just after these two definitions mentioned, they proved the lemma If $W$ is an invariant subspace for $T$ , then $W$ is invariant under every polynomial in $T$ . Thus, for each $\alpha$ in $V$ , the conductor $S(\alpha; W)$ is an ideal in the polynomial algebra $F[X]$ . and went on to say The unique monic generator of the ideal $S(\alpha;W)$ is also called the $T$ -conductor of $\alpha$ into $W$ (the $T$ -annihilator in case $W=\{0\}$ ). How come these two definitions are equivalent? This is one more connection I can't get. Please help me here. It seems that there are a couple of questions on MSE that looks like being similar to this one, but actually, they aren't. In these questions, the OP seems to have an idea of what's going on, whereas I don't. One of these questions even had an example, but that didn't help either.","I was reading Linear Algebra by Hoffman Kunze and I found a strange definition Let be an invariant subspace for and let be a vector in . The -conductor of into is the set which consists of all polynomials (over the scalar field) such that is in . The book also added In the special case the conductor is called the -annihilator of . Now, my problem is I don't understand what's going on here. What's the idea behind these two definitions? What's the algebraic (or geometric) picture behind these ideas? I understand the definiton and the intuition behind all the terms used in defining a -conductor. The problem is I just can't connect them together. Please help me to do so. Also, just after these two definitions mentioned, they proved the lemma If is an invariant subspace for , then is invariant under every polynomial in . Thus, for each in , the conductor is an ideal in the polynomial algebra . and went on to say The unique monic generator of the ideal is also called the -conductor of into (the -annihilator in case ). How come these two definitions are equivalent? This is one more connection I can't get. Please help me here. It seems that there are a couple of questions on MSE that looks like being similar to this one, but actually, they aren't. In these questions, the OP seems to have an idea of what's going on, whereas I don't. One of these questions even had an example, but that didn't help either.",W T\in \mathcal L(V) \alpha V T \alpha W S_T(\alpha; W) g g(T)\alpha W W=\{0\} T \alpha T W T W T \alpha V S(\alpha; W) F[X] S(\alpha;W) T \alpha W T W=\{0\},"['linear-algebra', 'geometry', 'linear-transformations', 'intuition', 'big-picture']"
86,Find all $x\in\mathbb{C^n}$ such that $||Ax||_2=1$ and $||x||_2=1$,Find all  such that  and,x\in\mathbb{C^n} ||Ax||_2=1 ||x||_2=1,"Given $A\in\mathbb{C}^{n\times n}$ , find all $x\in\mathbb{C^n}$ such that $||Ax||_2=1$ and $||x||_2=1$ . Lets do SVD: $A=U\Sigma V^*$ , where $\Sigma=\mathrm{diag}\{\sigma_1,\ldots,\sigma_n\}$ . We do change of a coordinates $x=Vy,$ then we need to find $y\in\mathbb{C^n}$ such that $||\Sigma y||_2=1$ and $||y||_2=1$ .","Given , find all such that and . Lets do SVD: , where . We do change of a coordinates then we need to find such that and .","A\in\mathbb{C}^{n\times n} x\in\mathbb{C^n} ||Ax||_2=1 ||x||_2=1 A=U\Sigma V^* \Sigma=\mathrm{diag}\{\sigma_1,\ldots,\sigma_n\} x=Vy, y\in\mathbb{C^n} ||\Sigma y||_2=1 ||y||_2=1","['linear-algebra', 'normed-spaces', 'svd', 'singular-values']"
87,Numerical analysis / Linear algebra resource to practice problems,Numerical analysis / Linear algebra resource to practice problems,,I am studying for a college course on numerical analysis. The current module is numerical solutions to linear equations. I am looking for resources that have lots of practice problems. I have already studied the theory and just want to practice different questions. The topics I wish to emphasise on: matrix norms spectral radius singular values iterated powers of matrices condition numbers iterative methods (and any related topics). I don't mind if the resources are books or online references. I just want many difficult questions to practice,I am studying for a college course on numerical analysis. The current module is numerical solutions to linear equations. I am looking for resources that have lots of practice problems. I have already studied the theory and just want to practice different questions. The topics I wish to emphasise on: matrix norms spectral radius singular values iterated powers of matrices condition numbers iterative methods (and any related topics). I don't mind if the resources are books or online references. I just want many difficult questions to practice,,"['linear-algebra', 'reference-request', 'numerical-linear-algebra', 'book-recommendation', 'online-resources']"
88,Invertibility of $I-A^s$ for each $s \geq 1$ implying nilpotence,Invertibility of  for each  implying nilpotence,I-A^s s \geq 1,"If $R$ is a unital ring and $x \in R$ a nilpotent element, then $1-x$ is a unit, and so is $1-x^s$ for any $s \geq 1 $ since powers of $x$ are again nilpotent. My question concerns the converse of this statement in the case in which $R = M_n(\mathbb{C})$ , that is: Question 1: if $A \in M_n(\mathbb{C})$ is a square matrix with complex entries and $I-A^s$ is invertible for all $s \geq 1$ , what conditions can we impose on $A$ to guarantee that it is nilpotent? Some condition has to be in place, as the general statement is already false in dimension $1$ . In that case, the matrices $1-a^n$ will be always invertible whenever $a$ is not a root of unity, and the only nilpotent element is zero. More generally, any diagonal matrix $\mathrm{diag}(a_1, \ldots, a_n)$ of non-zero elements that are not roots of unity will do the trick. Here's a partial result, If $\{\det(I-A^n)\}_n \subset \mathbb{R} \setminus \{0\}$ converges, then either the spectrum of $A$ is contained in $B_1(0)$ . In particular, if $A$ has integer coefficients then it is nilpotent. A proof follows: let $c_n := \det(I-A^s)$ and $\lambda_1, \ldots, \lambda_n \in \mathbb{C}$ be the eigenvalues of $A$ , counted with multiplicity. Then $$ (1-\lambda_1^s) \cdots (1-\lambda_n^s) = c_s  $$ is a convergent subsequence. Noting $d_s = \prod_{i \ : \ |\lambda_i| \neq 1}(1-\lambda_i^s)$ , we then have $$ \prod_{i \ : \ |\lambda_i| = 1}(1-\lambda_i^s) = c_s/d_s \to L. $$ with either $L = 0$ or $L = \lim_s c_s$ , depending on whether there are eigenvalues with absolute value greater than $1$ . If has some eigenvalue outside the unit disc, the right hand side converges and moreover the left hand side has some non-trival factor. Observe that since $c_n$ is never zero, no $\lambda_i$ can be a root of unity; and so we are left with showing that if $k \geq 1$ and $\alpha_1, \ldots, \alpha_k \in [0,1)$ are irrational then $$ a_s := \prod_{j = 1}^k(1-e^{i2\pi\alpha_j s})  $$ does not converge to $0$ nor $\lim_s c_s$ . By Dirichlet's approximation theorem, for each $k \geq 1$ we can find a natural number $s_k$ and an integer $p_k$ for which $|s_k\alpha_j - p_k| < 1/k$ , and thus there is a subsequence $(s_k)_k$ for which $e^{i2\pi\alpha_j s_k} \to 1$ for all $j$ , and $a_{s_k}\to 0$ . But then taking $s'_k = s_k+1$ yields $e^{i2\pi\alpha_j s_k'} \to e^{i2\pi\alpha_j s}$ and $a_{s_k'} \not \to 0$ . Hence $(a_s)$ never converges. Are there results in this direction whose proof is purely algebraic? In particular, can we recover the former partial results without doing analysis ? It seems to me like there is some relation between the spectrum of $A$ and the fact that $I-A^s$ being invertible implies the invertibility of $\sum_{i = 0}^s A^i$ , the limit of the latter - were to exist- being the inverse of $I-A$ . Is there a reference for this statement, at least in the case in which $\det(I-A^s) = \pm 1$ for each $s$ and $A$ has integer coefficients? I suspect this specific case can be proved in a purely algebraic fashion but so far I haven't been able to come up with a proof. Here's what I've done so far: Suppose that $f = \chi_A = X^m g, g = (X-\lambda_1) \cdots (X-\lambda_k)$ with $X \not \mid g$ . Note $p^{(m)}$ the polynomial whose roots are the $m$ -th powers of $p$ 's roots. Then our hypothesis is precisely that $f^{(m)}(1)$ is always a unit in $\mathbb{Z}$ . Thus, the same holds for $g$ , write $g = p_1 \cdots p_r$ where $p_j \in \mathbb{Z}[X]$ are irreducible. Since no eigenvalue $\lambda_i$ is a root of unity, once again by Kronecker's  theorem each $p_j$ has a - possibly complex - root of absolute value greater than $1$ . Moreover the proof notes that in general if $p \in \mathbb{Z}[X]$ then $p^{(m)} \in \mathbb{Z}[X]$ , so in any case, we have $$ \pm 1 = g^{(m)}(1) = p_1^{(m)}(1) \cdots p_r^{(m)}(1) $$ and so $p_j^{(m)}(1) = \pm 1$ . This allows us to reduce the original problem to: Attempted Subproblem 1. Let $f = \prod_{i=1}^k (X-\alpha_i) \in \mathbb{Z}[X]$ be an irreducible monic polynomial. If $f^{(m)}(1) = \pm 1$ for each $m \geq 1$ , where $f^{(m)} = \prod_{i=1}^k (X-\alpha_i^m)$ , then $f = X$ . Since $\det(I-A^s) = \pm 1$ for all $s$ , dividing by $\det(I-A)$ gives $\det(\sum_{i=0}^s A^i) = \pm  1$ for all $s$ . This yields a pretty large sum of all monomials $x_{i_1}^{j_1} \ldots x_{i_r}^{j_r}$ where $\{i_1, \ldots, i_r\} \subset \{1,\ldots, k\}$ and $j_t \in \{0,\ldots, s\}$ , being equal to $\pm 1$ . Maybe there's some underlying interpretation in terms of symmetric polynomials.","If is a unital ring and a nilpotent element, then is a unit, and so is for any since powers of are again nilpotent. My question concerns the converse of this statement in the case in which , that is: Question 1: if is a square matrix with complex entries and is invertible for all , what conditions can we impose on to guarantee that it is nilpotent? Some condition has to be in place, as the general statement is already false in dimension . In that case, the matrices will be always invertible whenever is not a root of unity, and the only nilpotent element is zero. More generally, any diagonal matrix of non-zero elements that are not roots of unity will do the trick. Here's a partial result, If converges, then either the spectrum of is contained in . In particular, if has integer coefficients then it is nilpotent. A proof follows: let and be the eigenvalues of , counted with multiplicity. Then is a convergent subsequence. Noting , we then have with either or , depending on whether there are eigenvalues with absolute value greater than . If has some eigenvalue outside the unit disc, the right hand side converges and moreover the left hand side has some non-trival factor. Observe that since is never zero, no can be a root of unity; and so we are left with showing that if and are irrational then does not converge to nor . By Dirichlet's approximation theorem, for each we can find a natural number and an integer for which , and thus there is a subsequence for which for all , and . But then taking yields and . Hence never converges. Are there results in this direction whose proof is purely algebraic? In particular, can we recover the former partial results without doing analysis ? It seems to me like there is some relation between the spectrum of and the fact that being invertible implies the invertibility of , the limit of the latter - were to exist- being the inverse of . Is there a reference for this statement, at least in the case in which for each and has integer coefficients? I suspect this specific case can be proved in a purely algebraic fashion but so far I haven't been able to come up with a proof. Here's what I've done so far: Suppose that with . Note the polynomial whose roots are the -th powers of 's roots. Then our hypothesis is precisely that is always a unit in . Thus, the same holds for , write where are irreducible. Since no eigenvalue is a root of unity, once again by Kronecker's  theorem each has a - possibly complex - root of absolute value greater than . Moreover the proof notes that in general if then , so in any case, we have and so . This allows us to reduce the original problem to: Attempted Subproblem 1. Let be an irreducible monic polynomial. If for each , where , then . Since for all , dividing by gives for all . This yields a pretty large sum of all monomials where and , being equal to . Maybe there's some underlying interpretation in terms of symmetric polynomials.","R x \in R 1-x 1-x^s s \geq 1  x R = M_n(\mathbb{C}) A \in M_n(\mathbb{C}) I-A^s s \geq 1 A 1 1-a^n a \mathrm{diag}(a_1, \ldots, a_n) \{\det(I-A^n)\}_n \subset \mathbb{R} \setminus \{0\} A B_1(0) A c_n := \det(I-A^s) \lambda_1, \ldots, \lambda_n \in \mathbb{C} A 
(1-\lambda_1^s) \cdots (1-\lambda_n^s) = c_s 
 d_s = \prod_{i \ : \ |\lambda_i| \neq 1}(1-\lambda_i^s) 
\prod_{i \ : \ |\lambda_i| = 1}(1-\lambda_i^s) = c_s/d_s \to L.
 L = 0 L = \lim_s c_s 1 c_n \lambda_i k \geq 1 \alpha_1, \ldots, \alpha_k \in [0,1) 
a_s := \prod_{j = 1}^k(1-e^{i2\pi\alpha_j s}) 
 0 \lim_s c_s k \geq 1 s_k p_k |s_k\alpha_j - p_k| < 1/k (s_k)_k e^{i2\pi\alpha_j s_k} \to 1 j a_{s_k}\to 0 s'_k = s_k+1 e^{i2\pi\alpha_j s_k'} \to e^{i2\pi\alpha_j s} a_{s_k'} \not \to 0 (a_s) A I-A^s \sum_{i = 0}^s A^i I-A \det(I-A^s) = \pm 1 s A f = \chi_A = X^m g, g = (X-\lambda_1) \cdots (X-\lambda_k) X \not \mid g p^{(m)} m p f^{(m)}(1) \mathbb{Z} g g = p_1 \cdots p_r p_j \in \mathbb{Z}[X] \lambda_i p_j 1 p \in \mathbb{Z}[X] p^{(m)} \in \mathbb{Z}[X] 
\pm 1 = g^{(m)}(1) = p_1^{(m)}(1) \cdots p_r^{(m)}(1)
 p_j^{(m)}(1) = \pm 1 f = \prod_{i=1}^k (X-\alpha_i) \in \mathbb{Z}[X] f^{(m)}(1) = \pm 1 m \geq 1 f^{(m)} = \prod_{i=1}^k (X-\alpha_i^m) f = X \det(I-A^s) = \pm 1 s \det(I-A) \det(\sum_{i=0}^s A^i) = \pm  1 s x_{i_1}^{j_1} \ldots x_{i_r}^{j_r} \{i_1, \ldots, i_r\} \subset \{1,\ldots, k\} j_t \in \{0,\ldots, s\} \pm 1","['linear-algebra', 'algebraic-number-theory', 'alternative-proof', 'nilpotence']"
89,Does the sequence $v_{n+1} = T(v_n)/|T(v_n)|$ converge to an eigenvector?,Does the sequence  converge to an eigenvector?,v_{n+1} = T(v_n)/|T(v_n)|,"Let $\varphi:[-1,1]\times \mathbb{R}\to\mathbb{R}$ the map $\varphi(x,w)=2\omega +1$ and $\mathcal{C}^0([-1,1])=\{f:[-1,1]\to\mathbb{R};\ f \ \text{is continuous}\}$ . I am interested in studying the following bounded linear map \begin{align*} T:\left(\mathcal{C}^0([-1,1]),\|\cdot\|_\infty\right)&\to \left(\mathcal{C}^0([-1,1]),\|\cdot\|_\infty\right)\\ f&\mapsto \left(x\mapsto \frac{1}{2}\int_{-1}^1 f(2x+\omega)\cdot\mathbf{1}_{[-1,1]}(2x+\omega) \ \mathrm{d}\omega \right), \end{align*} where $\mathbf{1}_{[-1,1]}$ is the indicator function of the interval $[-1,1]$ . Using the same techniques used in the links Proving that there exists only one non-negative eigenfuction for the following operator. , Is possible to show that the linear operator $T(\varphi)(x) = \int_{V_x\cap M} \varphi(y)\text{d}y$ has spectral radius $>0$. , and Krein-Rutman theorem we can ""easily"" conclude that $T$ is a compact linear map. $T$ has positive radius (actually, $r(T) =1/2$ ). $T$ has a unique eigenvector on the cone $\mathcal{C}^0_+([-1,1]) =\{f:[-1,1]\to\mathbb{R};\ f \ \text{is continuous and }f\geq 0\}$ and its eigenvalue is equal to $1/2$ . I have done some numerical simulations and I think that the sequence the recursive sequence: \begin{align*} v_0 &= 1\\ v_n&= \frac{T(v_{n-1})}{\|T(v_{n-1})\|_{\infty}}, \end{align*} converges to the unique eigenvector. This believe comes in light of some numerical simulations. In the picture below the red line represents the function $T(1)$ , the green line $T^2(1)$ , the orange one $T^3(1)$ and, finally, the blue one is $T^4(1)$ . My Question: Does anyone know if $\{v_n\}_{n\in\mathbb{N}}$ (or a subsequence of $\{v_n\}_{n\in\mathbb{N}}$ )  converges to an eigenvalue? Moreover (this is just out of curiosity the important question is the first one), is there some well-known theorem that gives conditions to the sequence $\{v_n\}$ converges to an eigenvalue in the infinite-dimensional case? It is worth mentioning that this paper gives us a result that under the ""strong"" Krein-Rutman theorem assumptions: The sequence $\{v_n\}_n$ converges to an eigenvector. But, I was looking for a theorem that I could apply in my linear operator. Can anyone please help me?","Let the map and . I am interested in studying the following bounded linear map where is the indicator function of the interval . Using the same techniques used in the links Proving that there exists only one non-negative eigenfuction for the following operator. , Is possible to show that the linear operator $T(\varphi)(x) = \int_{V_x\cap M} \varphi(y)\text{d}y$ has spectral radius $>0$. , and Krein-Rutman theorem we can ""easily"" conclude that is a compact linear map. has positive radius (actually, ). has a unique eigenvector on the cone and its eigenvalue is equal to . I have done some numerical simulations and I think that the sequence the recursive sequence: converges to the unique eigenvector. This believe comes in light of some numerical simulations. In the picture below the red line represents the function , the green line , the orange one and, finally, the blue one is . My Question: Does anyone know if (or a subsequence of )  converges to an eigenvalue? Moreover (this is just out of curiosity the important question is the first one), is there some well-known theorem that gives conditions to the sequence converges to an eigenvalue in the infinite-dimensional case? It is worth mentioning that this paper gives us a result that under the ""strong"" Krein-Rutman theorem assumptions: The sequence converges to an eigenvector. But, I was looking for a theorem that I could apply in my linear operator. Can anyone please help me?","\varphi:[-1,1]\times \mathbb{R}\to\mathbb{R} \varphi(x,w)=2\omega +1 \mathcal{C}^0([-1,1])=\{f:[-1,1]\to\mathbb{R};\ f \ \text{is continuous}\} \begin{align*}
T:\left(\mathcal{C}^0([-1,1]),\|\cdot\|_\infty\right)&\to \left(\mathcal{C}^0([-1,1]),\|\cdot\|_\infty\right)\\
f&\mapsto \left(x\mapsto \frac{1}{2}\int_{-1}^1 f(2x+\omega)\cdot\mathbf{1}_{[-1,1]}(2x+\omega) \ \mathrm{d}\omega \right),
\end{align*} \mathbf{1}_{[-1,1]} [-1,1] T T r(T) =1/2 T \mathcal{C}^0_+([-1,1]) =\{f:[-1,1]\to\mathbb{R};\ f \ \text{is continuous and }f\geq 0\} 1/2 \begin{align*}
v_0 &= 1\\
v_n&= \frac{T(v_{n-1})}{\|T(v_{n-1})\|_{\infty}},
\end{align*} T(1) T^2(1) T^3(1) T^4(1) \{v_n\}_{n\in\mathbb{N}} \{v_n\}_{n\in\mathbb{N}} \{v_n\} \{v_n\}_n","['linear-algebra', 'functional-analysis', 'eigenvalues-eigenvectors', 'banach-spaces', 'compact-operators']"
90,Finding perpendicular lines in $\mathbb R^4$,Finding perpendicular lines in,\mathbb R^4,"Let $$g=\begin{pmatrix}2\\-5\\-3\\-3\end{pmatrix}+\mathbb R\begin{pmatrix}1\\2\\3\\4\end{pmatrix}$$ and $$h=\begin{pmatrix}1\\-3\\0\\-1\end{pmatrix}+\mathbb R\begin{pmatrix}2\\3\\4\\5\end{pmatrix}.$$ Find all lines that are perpendicular to both $g$ and $h$ . Find the smallest affine subspace in $\mathbb R^4$ that contains both $g$ and $h$ . As for 1: One can easily see that the two lines are skew. Now, if $v_g$ and $v_h$ are the direction vectors of the lines I am first interested in a base of $U^\perp$ where $U=\langle v_g,v_h\rangle$ . I got $$U^\perp=\left\langle\begin{pmatrix}2\\-3\\0\\1\end{pmatrix},\begin{pmatrix}-1\\1\\1\\-1\end{pmatrix}\right\rangle=:\langle v_1,v_2\rangle.$$ So now we should get two perpendecular lines $$l_1=p_1+\mathbb R v_1\quad\text{ and }\quad l_2=p_2+\mathbb R v_2$$ and need to find $p_1$ and $p_2$ . We can parametrize $g$ via $$ \vec{P}_{\lambda}=\left(\begin{array}{c} 2+\lambda\\ -5+2\lambda\\ -3+3\lambda\\ -3+4\lambda \end{array}\right) $$ and $h$ via $$ \vec{G}_{\mu}=\left(\begin{array}{c} 1+2\mu\\ -3+3\mu\\ 4\mu\\ -1+5\mu \end{array}\right). $$ So the connection of $g$ and $h$ has the direction vector $$ v=\overrightarrow{P_{\lambda}G_{\mu}}=\left(\begin{array}{c} -1+2\mu-\lambda\\ 2+3\mu-2\lambda\\ 3+4\mu-3\lambda\\ 2+5\mu-4\lambda \end{array}\right). $$ The condition $v\perp g$ and $v\perp h$ yields $$ \left\langle \left(\begin{array}{c} -1+2\mu-\lambda\\ 2+3\mu-2\lambda\\ 3+4\mu-3\lambda\\ 2+5\mu-4\lambda \end{array}\right),\left(\begin{array}{c} 1\\ 2\\ 3\\ 4 \end{array}\right)\right\rangle =0=\left\langle \left(\begin{array}{c} -1+2\mu-\lambda\\ 2+3\mu-2\lambda\\ 3+4\mu-3\lambda\\ 2+5\mu-4\lambda \end{array}\right),\left(\begin{array}{c} 2\\ 3\\ 4\\ 5 \end{array}\right)\right\rangle  $$ and thus, $$ 20+40\mu-30\lambda=0\,\,\,\,\,\,\,\,\,\,\text{and}\,\,\,\,\,\,\,\,\,26+54\mu-40\lambda=0. $$ The solution of this system of linear equations is given by $\mu=1$ and $\lambda=2.$ With that, we find \begin{align*} l_{1} & =\vec{P}_{2}+\mathbb{R}\overrightarrow{P_{2}G_{1}}\\  & =\left(\begin{array}{c} 4\\ -1\\ 3\\ 5 \end{array}\right)+\mathbb{R}\left(\begin{array}{c} -1\\ 1\\ 1\\ -1 \end{array}\right). \end{align*} Is this correct so far? But how do I get the second one? As for 2: For the smallest subspace that contains both $g$ and $h$ I would take $g+v$ where $v$ is the direction vector between $g$ and $h$ as mentioned above. Does this make sense?","Let and Find all lines that are perpendicular to both and . Find the smallest affine subspace in that contains both and . As for 1: One can easily see that the two lines are skew. Now, if and are the direction vectors of the lines I am first interested in a base of where . I got So now we should get two perpendecular lines and need to find and . We can parametrize via and via So the connection of and has the direction vector The condition and yields and thus, The solution of this system of linear equations is given by and With that, we find Is this correct so far? But how do I get the second one? As for 2: For the smallest subspace that contains both and I would take where is the direction vector between and as mentioned above. Does this make sense?","g=\begin{pmatrix}2\\-5\\-3\\-3\end{pmatrix}+\mathbb R\begin{pmatrix}1\\2\\3\\4\end{pmatrix} h=\begin{pmatrix}1\\-3\\0\\-1\end{pmatrix}+\mathbb R\begin{pmatrix}2\\3\\4\\5\end{pmatrix}. g h \mathbb R^4 g h v_g v_h U^\perp U=\langle v_g,v_h\rangle U^\perp=\left\langle\begin{pmatrix}2\\-3\\0\\1\end{pmatrix},\begin{pmatrix}-1\\1\\1\\-1\end{pmatrix}\right\rangle=:\langle v_1,v_2\rangle. l_1=p_1+\mathbb R v_1\quad\text{ and }\quad l_2=p_2+\mathbb R v_2 p_1 p_2 g 
\vec{P}_{\lambda}=\left(\begin{array}{c}
2+\lambda\\
-5+2\lambda\\
-3+3\lambda\\
-3+4\lambda
\end{array}\right)
 h 
\vec{G}_{\mu}=\left(\begin{array}{c}
1+2\mu\\
-3+3\mu\\
4\mu\\
-1+5\mu
\end{array}\right).
 g h 
v=\overrightarrow{P_{\lambda}G_{\mu}}=\left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right).
 v\perp g v\perp h 
\left\langle \left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right),\left(\begin{array}{c}
1\\
2\\
3\\
4
\end{array}\right)\right\rangle =0=\left\langle \left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right),\left(\begin{array}{c}
2\\
3\\
4\\
5
\end{array}\right)\right\rangle 
 
20+40\mu-30\lambda=0\,\,\,\,\,\,\,\,\,\,\text{and}\,\,\,\,\,\,\,\,\,26+54\mu-40\lambda=0.
 \mu=1 \lambda=2. \begin{align*}
l_{1} & =\vec{P}_{2}+\mathbb{R}\overrightarrow{P_{2}G_{1}}\\
 & =\left(\begin{array}{c}
4\\
-1\\
3\\
5
\end{array}\right)+\mathbb{R}\left(\begin{array}{c}
-1\\
1\\
1\\
-1
\end{array}\right).
\end{align*} g h g+v v g h","['linear-algebra', 'analytic-geometry', 'orthogonality']"
91,On the joint numerical range of a pair of symmetric matrices,On the joint numerical range of a pair of symmetric matrices,,"Proposition 13.4 of Alexander Barvinok's A Course in Convexity shows the existence of the following result: Let $n\ge 3$ . For two $n\times n$ symmetric matrices $A$ and $B$ , and a PSD matrix $X$ with $\mbox{trace}(X) = 1$ , there exists a unit vector $x$ such that $x^T A x = \mbox{trace}(AX)$ and $x^T B x = \mbox{trace}(BX)$ . This book does not show how to construct such a vector! In Boyd & Vandenberghe's Convex Optimization , on page 656, there exists a constructive method for a looser version of above (basically, no constraints on the trace of $X$ and consequently, the magnitude of $x$ ). I could not so far use their proof for the above stronger version result and basically construct such $x$ . Any proof, idea, or help?","Proposition 13.4 of Alexander Barvinok's A Course in Convexity shows the existence of the following result: Let . For two symmetric matrices and , and a PSD matrix with , there exists a unit vector such that and . This book does not show how to construct such a vector! In Boyd & Vandenberghe's Convex Optimization , on page 656, there exists a constructive method for a looser version of above (basically, no constraints on the trace of and consequently, the magnitude of ). I could not so far use their proof for the above stronger version result and basically construct such . Any proof, idea, or help?",n\ge 3 n\times n A B X \mbox{trace}(X) = 1 x x^T A x = \mbox{trace}(AX) x^T B x = \mbox{trace}(BX) X x x,"['linear-algebra', 'optimization', 'matrix-decomposition', 'positive-semidefinite', 'common-root']"
92,Find the affine transformation to minimize the distance between two sets of points,Find the affine transformation to minimize the distance between two sets of points,,"Definition: Let $A=\{a_1,\cdots,a_n\}$ and $B=\{b_1,\cdots,b_n\}$ two sets with the same cardinality. Then define the distance between two sets as $$d(A,B):=\min_{\sigma}\sum_i||a_i-b_{\sigma(i)}||,\text{where }\sigma \text{ is a bijection from }\{1,2,\cdots, n\} \text{ to itself}.$$ Problem: Given two sets of points $A$ and $B$ in the Euclidean space with the same cardinality, find the affine transformation ${\bf M}$ such that distance between the sets ${\bf M}(A)$ and $B$ is minimized, where ${\bf M}(A) = \{{\bf M} {\bf p}:{\bf p}\in A\}$ .","Definition: Let and two sets with the same cardinality. Then define the distance between two sets as Problem: Given two sets of points and in the Euclidean space with the same cardinality, find the affine transformation such that distance between the sets and is minimized, where .","A=\{a_1,\cdots,a_n\} B=\{b_1,\cdots,b_n\} d(A,B):=\min_{\sigma}\sum_i||a_i-b_{\sigma(i)}||,\text{where }\sigma \text{ is a bijection from }\{1,2,\cdots, n\} \text{ to itself}. A B {\bf M} {\bf M}(A) B {\bf M}(A) = \{{\bf M} {\bf p}:{\bf p}\in A\}","['linear-algebra', 'metric-spaces', 'computational-complexity', 'nonlinear-optimization', 'computational-geometry']"
93,"Implementing QR Algorithm from Golub & Van Loan's ""Matrix Computations"" - notation and operation assistance","Implementing QR Algorithm from Golub & Van Loan's ""Matrix Computations"" - notation and operation assistance",,"I am implementing the Francis QR Algorithm from Golub & Van Loan's ""Matrix Computations"" (see algorithms below). I am unsure about the meaning of the following step: ""Find the largest nonnegative $q$ and the smallest non-negative $p$ such that..."" EDIT 4/30/2020: I have made some progress, the notation being used is ""Block Matrix"" notation, however I have noticed more difficulties. To simplify the question I am removing components that are no longer relevant. Francis Step requires a Matrix of minimum size $3\times 3$ . Does this mean the algorithm is only viable for matrices of minimum size $5\times 5$ ( $p=q=1$ )? Are there workarounds for $3\times3$ & $4\times4$ matrices? When partitioning the block matrix, I have not been able to find dimensions that satisfy the given conditions. For example, using the following $8\times8$ matrix $A$ (larger size so it's a bit easier to demonstrate), 1  2  3  4  5  6  7  8 9 10 11 12 13 14 15 16 0 18 19 20 21 22 23 24 0  0 27 28 29 30 31 32 0  0  0 36 37 38 39 40 0  0  0  0 45 46 47 48 0  0  0  0  0 54 55 56 0  0  0  0  0  0 63 64 There are a number of ways to partition it (while maintaining an $H_{22}$ of at least size $3\times3$ ), for example: $p = 1 ;\; q = 4$ 1   2 3 4      5 6 7 8        9   10 11 12   13 14 15 16   0   18 19 20   21 22 23 24   0    0 27 28   29 30 31 32    0   0 0 36     37 38 39 40   0   0 0  0     45 46 47 48   0   0 0  0      0 54 55 56   0   0 0  0      0  0 63 64 $p = 2 ;\; q = 3$ 1  2    3  4  5    6  7  8   9 10   11 12 13   14 15 16    0 18   19 20 21   22 23 24   0  0   27 28 29   30 31 32   0  0    0 36 37   38 39 40    0 0    0 0 45     46 47 48   0 0    0 0  0     54 55 56   0 0    0 0  0      0 63 64 $p = 3 ;\; q = 2$ 1  2  3    4  5  6    7  8   9 10 11   12 13 14   15 16   0 18 19   20 21 22   23 24    0 0 27    28 29 30   31 32   0 0  0    36 37 38   39 40   0 0  0     0 45 46   47 48    0 0 0     0 0 54     55 56   0 0 0     0 0  0     63 64 $p = 4 ;\; q = 1$ 1  2  3  4    5  6  7    8   9 10 11 12   13 14 15   16   0 18 19 20   21 22 23   24   0  0 27 28   29 30 31   32    0 0 0 36     37 38 39   40   0 0 0  0     45 46 47   48   0 0 0  0      0 54 55   56    0 0 0 0      0 0 63     64 $p = 1 ;\; q = 3$ 1   2 3 4 5       6 7 8       9   10 11 12 13   14 15 16   0   18 19 20 21   22 23 24   0    0 27 28 29   30 31 32   0    0  0 36 37   38 39 40    0   0 0 0 45      46 47 48   0   0 0 0  0      54 55 56   0   0 0 0  0       0 63 64 and on and on. Essentially, I have not been able to find combinations that result in a zero matrix in the $H_{12}$ spot. Every time it shifts it picks up a new subdiagonal non-zero scalar. What am I doing wrong?","I am implementing the Francis QR Algorithm from Golub & Van Loan's ""Matrix Computations"" (see algorithms below). I am unsure about the meaning of the following step: ""Find the largest nonnegative and the smallest non-negative such that..."" EDIT 4/30/2020: I have made some progress, the notation being used is ""Block Matrix"" notation, however I have noticed more difficulties. To simplify the question I am removing components that are no longer relevant. Francis Step requires a Matrix of minimum size . Does this mean the algorithm is only viable for matrices of minimum size ( )? Are there workarounds for & matrices? When partitioning the block matrix, I have not been able to find dimensions that satisfy the given conditions. For example, using the following matrix (larger size so it's a bit easier to demonstrate), 1  2  3  4  5  6  7  8 9 10 11 12 13 14 15 16 0 18 19 20 21 22 23 24 0  0 27 28 29 30 31 32 0  0  0 36 37 38 39 40 0  0  0  0 45 46 47 48 0  0  0  0  0 54 55 56 0  0  0  0  0  0 63 64 There are a number of ways to partition it (while maintaining an of at least size ), for example: 1   2 3 4      5 6 7 8        9   10 11 12   13 14 15 16   0   18 19 20   21 22 23 24   0    0 27 28   29 30 31 32    0   0 0 36     37 38 39 40   0   0 0  0     45 46 47 48   0   0 0  0      0 54 55 56   0   0 0  0      0  0 63 64 1  2    3  4  5    6  7  8   9 10   11 12 13   14 15 16    0 18   19 20 21   22 23 24   0  0   27 28 29   30 31 32   0  0    0 36 37   38 39 40    0 0    0 0 45     46 47 48   0 0    0 0  0     54 55 56   0 0    0 0  0      0 63 64 1  2  3    4  5  6    7  8   9 10 11   12 13 14   15 16   0 18 19   20 21 22   23 24    0 0 27    28 29 30   31 32   0 0  0    36 37 38   39 40   0 0  0     0 45 46   47 48    0 0 0     0 0 54     55 56   0 0 0     0 0  0     63 64 1  2  3  4    5  6  7    8   9 10 11 12   13 14 15   16   0 18 19 20   21 22 23   24   0  0 27 28   29 30 31   32    0 0 0 36     37 38 39   40   0 0 0  0     45 46 47   48   0 0 0  0      0 54 55   56    0 0 0 0      0 0 63     64 1   2 3 4 5       6 7 8       9   10 11 12 13   14 15 16   0   18 19 20 21   22 23 24   0    0 27 28 29   30 31 32   0    0  0 36 37   38 39 40    0   0 0 0 45      46 47 48   0   0 0 0  0      54 55 56   0   0 0 0  0       0 63 64 and on and on. Essentially, I have not been able to find combinations that result in a zero matrix in the spot. Every time it shifts it picks up a new subdiagonal non-zero scalar. What am I doing wrong?",q p 3\times 3 5\times 5 p=q=1 3\times3 4\times4 8\times8 A H_{22} 3\times3 p = 1 ;\; q = 4 p = 2 ;\; q = 3 p = 3 ;\; q = 2 p = 4 ;\; q = 1 p = 1 ;\; q = 3 H_{12},"['linear-algebra', 'eigenvalues-eigenvectors', 'algorithms', 'matrix-decomposition', 'computational-mathematics']"
94,Jacobian for polar decomposition,Jacobian for polar decomposition,,"Let $f:\mathrm{Mat}_n(\mathbb{C})\to\mathbb{C}$ be some function and let us suppose we want to make a change of variables in the integral $$ \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} $$ from $A$ to $|A| U$ , i.e., the polar decomposition of $A$ , where $|A|\equiv\sqrt{A^\ast A}$ and $U$ is the unique partial isometry with kernel equal to that of $A$ (there is a theorem saying it exists). What is the Jacobian matrix of the transformation $A \mapsto (|A|, U$ )? I.e., what is $J$ such that the following equation holds: $$ \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} = \int_{P\geq0,U^\ast U\,\mathrm{idempotent}}f(P U)|\det(J(P,U))|\mathrm{d}{P}\mathrm{d}{U}$$ I tried to calcualte it but I'm not getting anything simple. In particular, I've written $A = A_R + i A_I$ with $A_R = \frac{1}{2}(A+A^\ast); A_I = \frac{1}{2i}(A-A^\ast)$ so that $A$ is parametrized by two self-adjoint matrices. In turn, we may write $|A| = \exp(H_1) ; U = \exp(i H_2)$ for two self-adjoint matrices $H_1,H_2$ (assuming for a moment that $A$ is invertible so that $U$ is actually unitary). Hence we want to calculate the Jacobian of the transformation $(H_1,H_2)\mapsto(A_R,A_I)$ from $\mathrm{Herm}_n(\mathbb{C})^2\to \mathrm{Herm}_n(\mathbb{C})^2$ . This, however, starts to get ugly, with the differential of the exponential map for example being given by functional calculus of the adjoint super operator ( https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map ) and having to use the determinant of a block matrix formula. Is there an easier way out? Possible solution: In Edelman's PhD thesis there are given Jacobians to get from a matrix A to its LQ decomposition , and from its LQ decomposition to its Cholesky decomposition (Theorem 3.1). This possibly solves the problem as follows: \begin{align} \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} &= \int_{L\text{ lower triangular},\,U\text{ unitary}} f(LU)\prod_{i=1}^{n}L_{ii}^{2n-2i+1}\mathrm{d}{L}\mathrm{d}{U} \\ &=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(\sqrt{P}U)\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)|\det(P\otimes I+I\otimes P^\ast)|^2\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)\prod_{1\leq i,j\leq n}(\lambda_i(P)+\lambda_j(P))^2\mathrm{d}{P}\mathrm{d}{U}\end{align} with the usual abuse of notation that $\mathrm{d}{L}$ integrates only over the non-zero elements of $L$ , $\mathrm{d}{U}$ is the volume element within the unitary group, and $\mathrm{d}{P}$ the volume element on self-adjoint matrices (so only $n$ real and $\frac{1}{2}n(n-1)$ complex matrix elements). $\lambda_j(P)$ is the $j$ th eigenvalue of the matrix $P$ . Remaining question : Why is the LQ-decomposition change of variables valid for complex matrices? A complex unitary $n\times n$ matrix is $n^2$ real parameters, whereas a lower triangular matrix is $n(n+1)$ real parameters. On the other hand, a complex matrix is $2n^2$ real parameters, so there seem to be $n$ real parameters too many in this decomposition? (This is not a problem if matrices have real entries). Note that for the Cholesky decomposition this is not an issue since then the lower triangular matrix has positive entries on its diagonal. Could it be possible to make an LQ decomposition for complex matrices where the lower triangular has positive entries on the diagonal? Is this what Edelman is referring to? Unfortunately, precisely for the complex LQ decomposition he does not give a reference nor a proof.","Let be some function and let us suppose we want to make a change of variables in the integral from to , i.e., the polar decomposition of , where and is the unique partial isometry with kernel equal to that of (there is a theorem saying it exists). What is the Jacobian matrix of the transformation )? I.e., what is such that the following equation holds: I tried to calcualte it but I'm not getting anything simple. In particular, I've written with so that is parametrized by two self-adjoint matrices. In turn, we may write for two self-adjoint matrices (assuming for a moment that is invertible so that is actually unitary). Hence we want to calculate the Jacobian of the transformation from . This, however, starts to get ugly, with the differential of the exponential map for example being given by functional calculus of the adjoint super operator ( https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map ) and having to use the determinant of a block matrix formula. Is there an easier way out? Possible solution: In Edelman's PhD thesis there are given Jacobians to get from a matrix A to its LQ decomposition , and from its LQ decomposition to its Cholesky decomposition (Theorem 3.1). This possibly solves the problem as follows: with the usual abuse of notation that integrates only over the non-zero elements of , is the volume element within the unitary group, and the volume element on self-adjoint matrices (so only real and complex matrix elements). is the th eigenvalue of the matrix . Remaining question : Why is the LQ-decomposition change of variables valid for complex matrices? A complex unitary matrix is real parameters, whereas a lower triangular matrix is real parameters. On the other hand, a complex matrix is real parameters, so there seem to be real parameters too many in this decomposition? (This is not a problem if matrices have real entries). Note that for the Cholesky decomposition this is not an issue since then the lower triangular matrix has positive entries on its diagonal. Could it be possible to make an LQ decomposition for complex matrices where the lower triangular has positive entries on the diagonal? Is this what Edelman is referring to? Unfortunately, precisely for the complex LQ decomposition he does not give a reference nor a proof.","f:\mathrm{Mat}_n(\mathbb{C})\to\mathbb{C}  \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A}  A |A| U A |A|\equiv\sqrt{A^\ast A} U A A \mapsto (|A|, U J  \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} = \int_{P\geq0,U^\ast U\,\mathrm{idempotent}}f(P U)|\det(J(P,U))|\mathrm{d}{P}\mathrm{d}{U} A = A_R + i A_I A_R = \frac{1}{2}(A+A^\ast); A_I = \frac{1}{2i}(A-A^\ast) A |A| = \exp(H_1) ; U = \exp(i H_2) H_1,H_2 A U (H_1,H_2)\mapsto(A_R,A_I) \mathrm{Herm}_n(\mathbb{C})^2\to \mathrm{Herm}_n(\mathbb{C})^2 \begin{align} \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} &= \int_{L\text{ lower triangular},\,U\text{ unitary}} f(LU)\prod_{i=1}^{n}L_{ii}^{2n-2i+1}\mathrm{d}{L}\mathrm{d}{U} \\ &=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(\sqrt{P}U)\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)|\det(P\otimes I+I\otimes P^\ast)|^2\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)\prod_{1\leq i,j\leq n}(\lambda_i(P)+\lambda_j(P))^2\mathrm{d}{P}\mathrm{d}{U}\end{align} \mathrm{d}{L} L \mathrm{d}{U} \mathrm{d}{P} n \frac{1}{2}n(n-1) \lambda_j(P) j P n\times n n^2 n(n+1) 2n^2 n","['linear-algebra', 'functional-analysis', 'reference-request', 'jacobian', 'change-of-variable']"
95,Prove that a linear map is an isomorphism iff the image set of the basis of V is a basis of W.,Prove that a linear map is an isomorphism iff the image set of the basis of V is a basis of W.,,"So, here's the result that I'm trying to prove: Let $V$ and $W$ be vector spaces over $F$ and let $(v_1,v_2,...,v_n)$ be a basis of $V$ . Then, a linear map $f:V \to W$ is an isomorphism if   and only if the list $(f(v_1),f(v_2),...,f(v_n))$ is a basis of $W$ . Proof Attempt: Let $A = (v_1,v_2,....,v_n)$ and $B = (f(v_1),f(v_2),....,f(v_n))$ . Then, we have to show that the list B is linearly independent and generates $W$ . Consider the following linear combination: $$\sum_{k=1}^{n}\alpha_k f(v_k) = 0\\ \implies \sum_{k=1}^{n} f(\alpha_k v_k) = 0\\ \implies f[\sum_{k=1}^{n} \alpha_k v_k] = 0\\ \implies \sum_{k=1}^{n} \alpha_k v_k = 0\\ \implies \forall k \in \{1,2,...,n\}: \alpha_k = 0$$ In the above computation, we have used the linearity of $f$ and the fact that $f(0) = 0$ , alongside the bijectivity of $f$ . This proves the linear independence of the list of vectors in $B$ . Then, we need to show that it generates $W$ . Clearly, $L(B) \subset W$ . Let $w \in W$ . Then, there exists a unique element $v \in V$ such that $f(v) = w$ . So, we have: $$w = f(v) = f[\sum_{k=1}^{n} \alpha_k v_k]\\ \implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\ \implies w = \sum_{k=1}^{n} \alpha_k f(v_k)\\ \implies w \in L(B).$$ This proves that $W \subset L(B)$ and, thus, proves that $L(B) = W$ . This shows that the list $B$ is a basis of $W$ . Now, suppose that B is a basis of W. Then, we have to show that $f:V \to W$ is bijective. Let $u_1,u_2 \in V$ such that: $f(u_1) = f(u_2)$ Then, $u_1 = \sum_{k=1}^{n} \alpha_k v_k$ and $u_2 = \sum_{k=1}^{n} \beta_k v_k$ . So, we have: $f(u_1)-f(u_2) = f(u_1-u_2) = 0$ $\implies f[\sum_{k=1}^{n} (\alpha_k - \beta_k)v_k)] = 0$ $\implies \sum_{k=1}^{n} (\alpha_k-\beta_k)f(v_k) =0$ $\implies \forall k \in \{1,2,...,n\}: \alpha_k = \beta_k$ Hence, that proves that $u_1 = u_2$ . This proves the injectivity of $f$ . Then, consider $f(V)$ . Clearly, $f(V) \subset W$ . Let $w \in W$ . Then, we have: $$w = \sum_{k=1}^{n} \alpha_k f(v_k)\\ \implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\ \implies w = f[\sum_{k=1}^{n} \alpha_k v_k]\\ \implies w \in f(V)$$ . This proves that $W \subset f(V)$ and, therefore, $f(V) = W$ . That proves that f is surjective. Since f is injective and surjective, it follows that it is bijective and that proves that it is an isomorphism. I'd like some serious criticism on my proof above. Is it correct? Is my proof writing okay? How can I improve it?","So, here's the result that I'm trying to prove: Let and be vector spaces over and let be a basis of . Then, a linear map is an isomorphism if   and only if the list is a basis of . Proof Attempt: Let and . Then, we have to show that the list B is linearly independent and generates . Consider the following linear combination: In the above computation, we have used the linearity of and the fact that , alongside the bijectivity of . This proves the linear independence of the list of vectors in . Then, we need to show that it generates . Clearly, . Let . Then, there exists a unique element such that . So, we have: This proves that and, thus, proves that . This shows that the list is a basis of . Now, suppose that B is a basis of W. Then, we have to show that is bijective. Let such that: Then, and . So, we have: Hence, that proves that . This proves the injectivity of . Then, consider . Clearly, . Let . Then, we have: . This proves that and, therefore, . That proves that f is surjective. Since f is injective and surjective, it follows that it is bijective and that proves that it is an isomorphism. I'd like some serious criticism on my proof above. Is it correct? Is my proof writing okay? How can I improve it?","V W F (v_1,v_2,...,v_n) V f:V \to W (f(v_1),f(v_2),...,f(v_n)) W A = (v_1,v_2,....,v_n) B = (f(v_1),f(v_2),....,f(v_n)) W \sum_{k=1}^{n}\alpha_k f(v_k) = 0\\
\implies \sum_{k=1}^{n} f(\alpha_k v_k) = 0\\
\implies f[\sum_{k=1}^{n} \alpha_k v_k] = 0\\
\implies \sum_{k=1}^{n} \alpha_k v_k = 0\\
\implies \forall k \in \{1,2,...,n\}: \alpha_k = 0 f f(0) = 0 f B W L(B) \subset W w \in W v \in V f(v) = w w = f(v) = f[\sum_{k=1}^{n} \alpha_k v_k]\\
\implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\
\implies w = \sum_{k=1}^{n} \alpha_k f(v_k)\\
\implies w \in L(B). W \subset L(B) L(B) = W B W f:V \to W u_1,u_2 \in V f(u_1) = f(u_2) u_1 = \sum_{k=1}^{n} \alpha_k v_k u_2 = \sum_{k=1}^{n} \beta_k v_k f(u_1)-f(u_2) = f(u_1-u_2) = 0 \implies f[\sum_{k=1}^{n} (\alpha_k - \beta_k)v_k)] = 0 \implies \sum_{k=1}^{n} (\alpha_k-\beta_k)f(v_k) =0 \implies \forall k \in \{1,2,...,n\}: \alpha_k = \beta_k u_1 = u_2 f f(V) f(V) \subset W w \in W w = \sum_{k=1}^{n} \alpha_k f(v_k)\\
\implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\
\implies w = f[\sum_{k=1}^{n} \alpha_k v_k]\\
\implies w \in f(V) W \subset f(V) f(V) = W","['linear-algebra', 'solution-verification']"
96,Conditions under which a $2\times2$ block matrix has complex eigenvalues,Conditions under which a  block matrix has complex eigenvalues,2\times2,"Consider a matrix $A$ in $\mathbb{R}^2$ : \begin{equation} A=\begin{bmatrix} 0 & -c \\ 1 & -b \end{bmatrix} \end{equation} Then it can be shown that the matrix has complex eigenvalues if $b^2-4c < 0$ . Can a similar relation be derived also for the elements of a similarly shaped 2x2 block matrix, ie: \begin{equation} M=\begin{bmatrix} 0_n & -C \\ I_n & -B \end{bmatrix} \end{equation} where $B, C$ are matrices in $\mathbb{R}^n$ , such that $\lambda_M$ are complex?","Consider a matrix in : Then it can be shown that the matrix has complex eigenvalues if . Can a similar relation be derived also for the elements of a similarly shaped 2x2 block matrix, ie: where are matrices in , such that are complex?","A \mathbb{R}^2 \begin{equation}
A=\begin{bmatrix}
0 & -c \\
1 & -b
\end{bmatrix}
\end{equation} b^2-4c < 0 \begin{equation}
M=\begin{bmatrix}
0_n & -C \\
I_n & -B
\end{bmatrix}
\end{equation} B, C \mathbb{R}^n \lambda_M","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
97,Determinants through dot products,Determinants through dot products,,"Thanks for reading! So, in Paulo Buchsbaum's answer to this question on Quora... https://www.quora.com/What-is-the-mathematical-intuition-behind-the-determinant-of-a-matrix-How-was-its-definition-conceived-and-why-is-it-important-What-does-it-mean-intuitively ...he defines a beautiful way to calculate the determinant of a matrix through dot products. One I hadn't seen before. He goes through the process of calculating this determinant for a $(3,3)$ matrix. I know this is a long request, but I was wondering if someone could take me through the process of doing it for a $(4,4)$ dimensional matrix, as (as you can see from my comment to his answer) I'm getting really confused. I would really appreciate it! Thank you! Edit: After reading @MatthewTowers comment (thanks), here is my explanation of what he's done for the $(3,3)$ case: Let's say we've got a $(3,3)$ matrix with a left column vector $\vec{a}$ , a middle column vector $\vec{b}$ , and a right column vector $\vec{c}$ . These three vectors create a parallelepiped. The volume of that parallelepiped is the determinant of the matrix. As you read, here's a picture (taken from Paulo's answer) for reference: Each vector has three components since they exist in three-dimensional space (aka the matrix has three rows) . $\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3 \end{bmatrix} \vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ b_3 \end{bmatrix} \vec{c} = \begin{bmatrix}c_1 \\ c_2 \\ c_3 \end{bmatrix}$ Note that $\vec{b}$ and $\vec{c}$ create a parallelogram. This parallelogram is the ""base"" of the parallelepiped created by all three column-vectors, and the component of $\vec{a}$ perpendicular to this parallelogram is the height of the parallelepiped. Let's ignore $\vec{a}$ for now. The first step is to find a vector $\vec{n}$ that's orthogonal to both $\vec{b}$ and $\vec{c}$ . We set $\vec{n} \bullet \vec{b}=0$ and $\vec{n} \bullet \vec{c}=0$ . Or, in other words, $n_1b_1 + n_2b_2 + n_3b_3 = 0$ and $n_1c_1 + n_2c_2 + n_3c_3 = 0$ That's three unknowns and only two equations. However, we can choose $n_1$ to be whatever we want, which allows us to solve for $\vec{n}$ . The second step is to find a vector $\vec{o}$ that's orthogonal to $\vec{b}$ and $\vec{n}$ . Notice that $\vec{o}$ will be in the same plane as $\vec{b}$ and $\vec{c}$ , but will be perpendicular to $\vec{b}$ . We set $\vec{o} \bullet \vec{b}=0$ and $\vec{o} \bullet \vec{n}=0$ . Once more, that gives us three unknowns and only two equations. However, we can choose $o_1$ to be whatever we want, which allows us to solve for $\vec{o}$ . We now set the magnitude of $\vec{o}$ to be equal to the magnitude of $\vec{b}$ . $\left \| \vec{o} \right \| := \left \| \vec{b} \right \|$ Note that this makes $\vec{o}$ a vector orthogonal to $\vec{b}$ but in the same plane as $\vec{b}$ and $\vec{c}$ , whose magnitude is the magnitude of $\vec{b}$ . If we take the dot product of $\vec{o}$ and $\vec{c}$ , that gives us the area of the parallelogram created by $\vec{b}$ and $\vec{c}$ . That's because $\vec{c} \bullet \vec{o}$ is the magnitude of the projection of $\vec{c}$ onto $\vec{o}$ multiplied by the magnitude of $\vec{o}$ . The projection of $\vec{c}$ onto $\vec{o}$ is the height of the parallelogram with $\vec{b}$ taken as the base, since $\vec{o}$ is orthogonal to $\vec{b}$ . And, the magnitude of $\vec{o}$ is equal to the magnitude of $\vec{b}$ . So, the dot product $\vec{c} \bullet \vec{o}$ is the height times the base, aka the area of the parallelogram created by $\vec{b}$ and $\vec{c}$ . Now, we set the magnitude of $\vec{n}$ equal to the area of that parallelogram. $\left \| \vec{n} \right \| :=  \vec{c} \bullet \vec{o} $ And finally, we compute $\vec{a} \bullet \vec{n} $ , and get the determinant of the matrix! Why does this last step work? For the same reason as the previous dot product - $\vec{n}$ is orthogonal to the parallelogram created by $\vec{b}$ and $\vec{c}$ . Taking the dot product of $\vec{a}$ and $\vec{n}$ is the same thing as taking the projection of $\vec{a}$ onto $\vec{n}$ and multiplying it by the magnitude of $\vec{n}$ . The projection of $\vec{a}$ onto $\vec{n}$ is the height of the parallelepiped, with the parallelogram created by $\vec{b}$ and $\vec{c}$ taken to be the base, and the magnitude of $\vec{n}$ is the area of the parallelogram, aka the area of the base. Once again, height times base equals volume. The volume of the parallelepiped is the determinant of the matrix! To conclude, I have two questions: I can see that what he's doing is getting a matrix with $n$ column vectors, calculating the cross-product of the last $n-1$ vectors, and then taking the dot product of the first vector with that cross product. However, how does this connect with the ""normal"" way to calculate determinants? Is there a way to derive the cross-product formula from his method? Secondly, if someone could just carry out the process for $n=4$ , I'd really appreciate it! I'm getting two equations with four unknowns... Thanks! Edit 2: This is just to show what I've done so far, in regards to my second question (the request for someone to take me through this guy's process for a $(4,4)$ matrix) : Calculate $det(\vec{a},\vec{b},\vec{c},\vec{d})$ Take out the $\vec{a}$ , and calculate $det(\vec{b},\vec{c},\vec{d})$ To do so, we take out the $\vec{b}$ , and calculate $det(\vec{c},\vec{d})$ To do so, we declare an $\vec{n}$ such that $\vec{n} \bullet \vec{c} = 0$ and $\vec{n} \bullet \vec{d} = 0$ . In other words, $\vec{n}$ is orthogonal to the parallelogram created by $\vec{c}$ and $\vec{d}$ . (And here’s where I'm stuck…should $\vec{n}$ be orthogonal to $\vec{a}$ as well? Right now, I have two equations, but four unknowns, and I’m not sure what to do…","Thanks for reading! So, in Paulo Buchsbaum's answer to this question on Quora... https://www.quora.com/What-is-the-mathematical-intuition-behind-the-determinant-of-a-matrix-How-was-its-definition-conceived-and-why-is-it-important-What-does-it-mean-intuitively ...he defines a beautiful way to calculate the determinant of a matrix through dot products. One I hadn't seen before. He goes through the process of calculating this determinant for a matrix. I know this is a long request, but I was wondering if someone could take me through the process of doing it for a dimensional matrix, as (as you can see from my comment to his answer) I'm getting really confused. I would really appreciate it! Thank you! Edit: After reading @MatthewTowers comment (thanks), here is my explanation of what he's done for the case: Let's say we've got a matrix with a left column vector , a middle column vector , and a right column vector . These three vectors create a parallelepiped. The volume of that parallelepiped is the determinant of the matrix. As you read, here's a picture (taken from Paulo's answer) for reference: Each vector has three components since they exist in three-dimensional space (aka the matrix has three rows) . Note that and create a parallelogram. This parallelogram is the ""base"" of the parallelepiped created by all three column-vectors, and the component of perpendicular to this parallelogram is the height of the parallelepiped. Let's ignore for now. The first step is to find a vector that's orthogonal to both and . We set and . Or, in other words, and That's three unknowns and only two equations. However, we can choose to be whatever we want, which allows us to solve for . The second step is to find a vector that's orthogonal to and . Notice that will be in the same plane as and , but will be perpendicular to . We set and . Once more, that gives us three unknowns and only two equations. However, we can choose to be whatever we want, which allows us to solve for . We now set the magnitude of to be equal to the magnitude of . Note that this makes a vector orthogonal to but in the same plane as and , whose magnitude is the magnitude of . If we take the dot product of and , that gives us the area of the parallelogram created by and . That's because is the magnitude of the projection of onto multiplied by the magnitude of . The projection of onto is the height of the parallelogram with taken as the base, since is orthogonal to . And, the magnitude of is equal to the magnitude of . So, the dot product is the height times the base, aka the area of the parallelogram created by and . Now, we set the magnitude of equal to the area of that parallelogram. And finally, we compute , and get the determinant of the matrix! Why does this last step work? For the same reason as the previous dot product - is orthogonal to the parallelogram created by and . Taking the dot product of and is the same thing as taking the projection of onto and multiplying it by the magnitude of . The projection of onto is the height of the parallelepiped, with the parallelogram created by and taken to be the base, and the magnitude of is the area of the parallelogram, aka the area of the base. Once again, height times base equals volume. The volume of the parallelepiped is the determinant of the matrix! To conclude, I have two questions: I can see that what he's doing is getting a matrix with column vectors, calculating the cross-product of the last vectors, and then taking the dot product of the first vector with that cross product. However, how does this connect with the ""normal"" way to calculate determinants? Is there a way to derive the cross-product formula from his method? Secondly, if someone could just carry out the process for , I'd really appreciate it! I'm getting two equations with four unknowns... Thanks! Edit 2: This is just to show what I've done so far, in regards to my second question (the request for someone to take me through this guy's process for a matrix) : Calculate Take out the , and calculate To do so, we take out the , and calculate To do so, we declare an such that and . In other words, is orthogonal to the parallelogram created by and . (And here’s where I'm stuck…should be orthogonal to as well? Right now, I have two equations, but four unknowns, and I’m not sure what to do…","(3,3) (4,4) (3,3) (3,3) \vec{a} \vec{b} \vec{c} \vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3 \end{bmatrix} \vec{b} = \begin{bmatrix}b_1 \\ b_2 \\ b_3 \end{bmatrix} \vec{c} = \begin{bmatrix}c_1 \\ c_2 \\ c_3 \end{bmatrix} \vec{b} \vec{c} \vec{a} \vec{a} \vec{n} \vec{b} \vec{c} \vec{n} \bullet \vec{b}=0 \vec{n} \bullet \vec{c}=0 n_1b_1 + n_2b_2 + n_3b_3 = 0 n_1c_1 + n_2c_2 + n_3c_3 = 0 n_1 \vec{n} \vec{o} \vec{b} \vec{n} \vec{o} \vec{b} \vec{c} \vec{b} \vec{o} \bullet \vec{b}=0 \vec{o} \bullet \vec{n}=0 o_1 \vec{o} \vec{o} \vec{b} \left \| \vec{o} \right \| := \left \| \vec{b} \right \| \vec{o} \vec{b} \vec{b} \vec{c} \vec{b} \vec{o} \vec{c} \vec{b} \vec{c} \vec{c} \bullet \vec{o} \vec{c} \vec{o} \vec{o} \vec{c} \vec{o} \vec{b} \vec{o} \vec{b} \vec{o} \vec{b} \vec{c} \bullet \vec{o} \vec{b} \vec{c} \vec{n} \left \| \vec{n} \right \| :=  \vec{c} \bullet \vec{o}  \vec{a} \bullet \vec{n}  \vec{n} \vec{b} \vec{c} \vec{a} \vec{n} \vec{a} \vec{n} \vec{n} \vec{a} \vec{n} \vec{b} \vec{c} \vec{n} n n-1 n=4 (4,4) det(\vec{a},\vec{b},\vec{c},\vec{d}) \vec{a} det(\vec{b},\vec{c},\vec{d}) \vec{b} det(\vec{c},\vec{d}) \vec{n} \vec{n} \bullet \vec{c} = 0 \vec{n} \bullet \vec{d} = 0 \vec{n} \vec{c} \vec{d} \vec{n} \vec{a}","['linear-algebra', 'vectors', 'determinant', 'intuition']"
98,Composition of matrices - eigenvectors,Composition of matrices - eigenvectors,,Say $\lambda$ is an eigenvalue of $\textsf{ST}$ . There exists $𝑥 \ne \textbf{0}$ such that $\textsf{ST}x=  \lambda x$ . Multiply both sides by $\textsf T$ : $$\textsf{TST}x=\textsf{T} (\lambda x)$$ $$\textsf{TS}(\textsf{T}x)=\lambda(\textsf{T}x)$$ Thus $\textsf{T}x$ is an eigenvector for $\textsf{TS}$ . Why is there such a potent relation between the eigenvectors of $\textsf{ST}$ and $\textsf{TS}$ ? How come one is just the transform of the other? Looking for geometric/intuitive explanation. Hints welcome.,Say is an eigenvalue of . There exists such that . Multiply both sides by : Thus is an eigenvector for . Why is there such a potent relation between the eigenvectors of and ? How come one is just the transform of the other? Looking for geometric/intuitive explanation. Hints welcome.,\lambda \textsf{ST} 𝑥 \ne \textbf{0} \textsf{ST}x=  \lambda x \textsf T \textsf{TST}x=\textsf{T} (\lambda x) \textsf{TS}(\textsf{T}x)=\lambda(\textsf{T}x) \textsf{T}x \textsf{TS} \textsf{ST} \textsf{TS},"['linear-algebra', 'abstract-algebra', 'matrices']"
99,How independent are the minors of a matrix?,How independent are the minors of a matrix?,,"Let $M$ be a $p\times q$ matrix (say, $p\leq q$ ) with formal coefficients $m_{i,j}$ . It has $pq$ entries, and ${q\choose p}$ minors of maximal size $p\times p$ , which gives us a map $$\varphi_{p,q}: \Bbb Z^{pq}\to \Bbb Z^{q\choose p}$$ I'm interested in the surjectivity of this map. It is easy to see that $\varphi_{3,4}$ , for instance, is surjective, meaning that we can adjust the coefficients of a $3\times 4$ matrix to give each of the $3\times 3$ minors a prescribed value. In most cases, ${q\choose p}$ is much bigger than $pq$ , so it feels unlikely that $\varphi$ will be surjective. But what if we keep only a relatively small amount of minors? Question 1: For what values of $p,q,r\in\Bbb N$ is there a map $f:\Bbb Z^{q\choose p}\to \Bbb Z^r$ defined by keeping only some $r$ coordinates, such that $f\circ \varphi_{p,q}$ is surjective? Question 2 (less general but the one I'm really interested in) : Take $p=3, q=6$ and forget about the three minors corresponding to columns $(1, 4, 5), (2, 4, 6), (3, 5, 6)$ . Is the resulting map $\Bbb Z^{18}\to \Bbb Z^{17}$ surjective?","Let be a matrix (say, ) with formal coefficients . It has entries, and minors of maximal size , which gives us a map I'm interested in the surjectivity of this map. It is easy to see that , for instance, is surjective, meaning that we can adjust the coefficients of a matrix to give each of the minors a prescribed value. In most cases, is much bigger than , so it feels unlikely that will be surjective. But what if we keep only a relatively small amount of minors? Question 1: For what values of is there a map defined by keeping only some coordinates, such that is surjective? Question 2 (less general but the one I'm really interested in) : Take and forget about the three minors corresponding to columns . Is the resulting map surjective?","M p\times q p\leq q m_{i,j} pq {q\choose p} p\times p \varphi_{p,q}: \Bbb Z^{pq}\to \Bbb Z^{q\choose p} \varphi_{3,4} 3\times 4 3\times 3 {q\choose p} pq \varphi p,q,r\in\Bbb N f:\Bbb Z^{q\choose p}\to \Bbb Z^r r f\circ \varphi_{p,q} p=3, q=6 (1, 4, 5), (2, 4, 6), (3, 5, 6) \Bbb Z^{18}\to \Bbb Z^{17}","['linear-algebra', 'determinant']"
